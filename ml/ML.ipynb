{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/sentiment-analysis-with-pyspark-bc8e83f80c35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/trom/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.embeddings import *\n",
    "import sparknlp\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv = './train/training.1600000.processed.noemoticon.csv' #file at https://1drv.ms/u/s!AqlC23XtB27BisoM5u56CMPeNOBQKw\n",
    "# df = pd.read_csv(csv, header=None, encoding=\"ISO-8859-1\", usecols=[0,5], names=['target', 'text'])\n",
    "# df = df[['text', 'target']]\n",
    "\n",
    "# df.dropna(inplace=True)\n",
    "# df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# df.loc[df.target == 0, 'target'] = -1\n",
    "# df.loc[df.target == 2, 'target'] = 0\n",
    "# df.loc[df.target == 4, 'target'] = 1\n",
    "\n",
    "# # df.to_csv('./train/clean_tweet.csv', index=False)\n",
    "\n",
    "# train_df, val_df = train_test_split(df, test_size=0.21)\n",
    "\n",
    "# train_df.to_csv('./train/clean_tweet_train.csv', index=False)\n",
    "# val_df.to_csv('./train/clean_tweet_val.csv', index=False)\n",
    "\n",
    "# train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f\n",
    "spark = sparknlp.start()\n",
    "sc = spark.sparkContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = sqlCtx.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('./train/clean_tweet_train.csv')\n",
    "train_set = sqlCtx.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('./train/clean_tweet_val.csv')\n",
    "\n",
    "train_set = train_set.dropna()\n",
    "val_set = val_set.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "|                text|target|               words|                  tf|            features|label|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "|           FUCK YOU!|    -1|[, , , , , , , , ...|(65536,[32974,405...|(65536,[32974,405...|  1.0|\n",
      "|          i want ...|    -1|[, , , , , , , , ...|(65536,[6350,1300...|(65536,[6350,1300...|  1.0|\n",
      "|        my head f...|    -1|[, , , , , , , , ...|(65536,[158,2548,...|(65536,[158,2548,...|  1.0|\n",
      "|       i really2 ...|    -1|[, , , , , , , i,...|(65536,[9346,1165...|(65536,[9346,1165...|  1.0|\n",
      "|      My current ...|    -1|[, , , , , , my, ...|(65536,[1540,2752...|(65536,[1540,2752...|  1.0|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "train_df = pipelineFit.transform(train_set)\n",
    "val_df = pipelineFit.transform(val_set)\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "lrModel = lr.fit(train_df)\n",
    "predictions = lrModel.transform(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8558556225034201"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel.save('./train/lr.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "sameModel = LogisticRegressionModel.load(\"./train/lr.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def score_text(text):\n",
    "test0 = pipelineFit.transform([Row(features=\"Some text\")]) # To fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
