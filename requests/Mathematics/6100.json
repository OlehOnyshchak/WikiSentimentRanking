{
  "pages": [
    {
      "title": "Pseudo-spectral method",
      "url": "https://en.wikipedia.org/wiki/Pseudo-spectral_method",
      "text": "{{Refimprove|date=December 2009}}\n\n'''Pseudo-spectral methods''',<ref>{{cite journal|last1=Orszag|first1=Steven A.|title=Comparison of Pseudospectral and Spectral Approximation|journal=Studies in Applied Mathematics|date=September 1972|volume=51|issue=3|pages=253–259|doi=10.1002/sapm1972513253}}</ref> also known as discrete variable representation (DVR) methods, are a class of [[numerical methods]] used in [[applied mathematics]] and [[scientific computing]] for the solution of [[partial differential equation]]s. They are closely related to [[spectral method]]s, but complement the [[Basis (linear algebra)|basis]] by an additional pseudo-spectral basis, which allows representation of functions on a quadrature grid. This simplifies the evaluation of certain operators, and can considerably speed up the calculation when using fast algorithms such as the [[fast Fourier transform]].\n\n==Motivation with a concrete example==\n\nTake the initial-value problem\n\n:<math>i \\frac{\\partial}{\\partial t} \\psi(x, t) = \\Bigl[-\\frac{\\partial^2}{\\partial x^2} + V(x) \\Bigr] \\psi(x,t), \\qquad\\qquad \\psi(t_0) = \\psi_0</math>\n\nwith periodic conditions <math>\\psi(x+1, t) = \\psi(x, t)</math>. This specific example is the [[Schrödinger equation]] for a particle in a potential <math>V(x)</math>, but the structure is more general. In many practical partial differential equations, one has a term that involves derivatives (such as a kinetic energy contribution), and a multiplication with a function (for example, a potential).\n\nIn the spectral method, the solution <math>\\psi</math> is expanded in a suitable set of basis functions, for example plane waves,\n\n:<math>\\psi(x,t) = \\frac{1}{\\sqrt{2\\pi}} \\sum_n c_n(t) e^{2\\pi i n x} .</math>\n\nInsertion and equating identical coefficients yields a set of [[ordinary differential equation]]s for the coefficients,\n\n:<math>i\\frac{d}{dt} c_n(t) = (2\\pi n)^2 c_n + \\sum_k V_{n-k} c_k,</math>\n\nwhere the elements <math>V_{nk}</math> are calculated through the explicit Fourier-transform\n\n:<math>V_{n-k} = \\int_0^{1} V(x) \\ e^{2\\pi i (k-n) x} dx .</math>\n\nThe solution would then be obtained by truncating the expansion to <math>N</math> basis functions, and finding a solution for the <math>c_n(t)</math>. In general, this is done by [[Numerical methods for ordinary differential equations|numerical methods]], such as [[Runge–Kutta methods]]. For the numerical solutions, the right-hand side of the ordinary differential equation has to be evaluated repeatedly at different time steps. At this point, the spectral method has a major problem with the potential term <math>V(x)</math>.\n\nIn the spectral representation, the multiplication with the function <math>V(x)</math> transforms into a vector-matrix multiplication, which scales as <math>N^2</math>. Also, the matrix elements <math>V_{nk}</math> need to be evaluated explicitly before the differential equation for the coefficients can be solved, which requires an additional step.\n\nIn the pseudo-spectral method, this term is evaluated differently. Given the coefficients <math>c_n(t)</math>, an inverse discrete Fourier transform yields the value of the function <math>\\psi</math> at discrete grid points <math>x_j = 2\\pi j/N</math>. At these grid points, the function is then multiplied, <math>\\psi'(x_i, t) = V(x_i) \\psi(x_i, t)</math>, and the result Fourier-transformed back. This yields a new set of coefficients <math>c'_n(t)</math> that are used instead of the matrix product <math>\\sum_k V_{n-k} c_k(t)</math>.\n\nIt can be shown that both methods have similar accuracy. However, the pseudo-spectral method allows the use of a fast Fourier transform, which scales as <math>O(N\\ln N)</math>, and is therefore significantly more efficient than the matrix multiplication. Also, the function <math>V(x)</math> can be used directly without evaluating any additional integrals.\n\n==Technical discussion==\n\nIn a more abstract way, the pseudo-spectral method deals with the multiplication of two functions <math>V(x)</math> and <math>f(x)</math> as part of a partial differential equation. To simplify the notation, the time-dependence is dropped. Conceptually, it consists of three steps:\n\n# <math>f(x), \\tilde{f}(x) = V(x)f(x)</math> are expanded in a finite set of basis functions (this is the [[spectral method]]).\n# For a given set of basis functions, a quadrature is sought that converts scalar products of these basis functions into a weighted sum over grid points.\n# The product is calculated by multiplying <math>V,f</math> at each grid point.\n\n\n===Expansion in a basis===\n\nThe functions <math>f, \\tilde f</math> can be expanded in a finite basis <math>\\{\\phi_n\\}_{n = 0,\\ldots,N}</math> as\n\n:<math>f(x) = \\sum_{n=0}^N c_n \\phi_n(x)</math>\n:<math>\\tilde f(x) = \\sum_{n=0}^N \\tilde c_n \\phi_n(x)</math>\n\nFor simplicity, let the basis be orthogonal and normalized, <math>\\langle \\phi_n, \\phi_m \\rangle = \\delta_{nm}</math> using the [[inner product]] <math>\\langle f, g \\rangle = \\int_a^b f(x) \\overline{g(x)} dx</math> with appropriate boundaries <math>a,b</math>. The coefficients are then obtained by\n\n:<math>c_n = \\langle f, \\phi_n \\rangle</math>\n:<math>\\tilde c_n = \\langle \\tilde f, \\phi_n \\rangle</math>\n\nA bit of calculus yields then\n\n:<math>\\tilde c_n = \\sum_{m=0}^N V_{n-m} c_m</math>\n\nwith <math>V_{n-m} = \\langle V\\phi_m, \\phi_n \\rangle</math>. This forms the basis of the spectral method. To distinguish the basis of the <math>\\phi_n</math> from the quadrature basis, the expansion is sometimes called Finite Basis Representation (FBR).\n\n\n===Quadrature===\n\nFor a given basis <math>\\{\\phi_n\\}</math> and number of <math>N+1</math> basis functions, one can try to find a quadrature, i.e., a set of <math>N+1</math> points and weights such that\n\n:<math>\\langle \\phi_n, \\phi_m \\rangle = \\sum_{i=0}^N w_i \\phi_n(x_i) \\overline{\\phi_m(x_i)} \\qquad\\qquad n,m = 0,\\ldots,N</math>\n\nSpecial examples are the [[Gaussian quadrature]] for polynomials and the [[Discrete Fourier Transform]] for plane waves. It should be stressed that the grid points and weights, <math>x_i,w_i</math> are a function of the basis ''and'' the number <math>N</math>.\n\nThe quadrature allows an alternative numerical representation of the function <math>f(x), \\tilde f(x)</math> through their value at the grid points. This representation is sometimes denoted Discrete Variable Representation (DVR), and is completely equivalent to the expansion in the basis.\n\n:<math>f(x_i) = \\sum_{n=0}^N c_n \\phi_n(x_i)</math>\n:<math>c_n = \\langle f, \\phi_n \\rangle = \\sum_{n=0}^{N} w_i f(x_i) \\overline{\\phi_n(x_i)}</math>\n\n\n===Multiplication===\n\nThe multiplication with the function <math>V(x)</math> is then done at each grid point,\n\n:<math>\\tilde f(x_i) = V(x_i) f(x_i).</math>\n\nThis generally introduces an additional approximation. To see this, we can calculate one of the coefficients <math>\\tilde c_n</math>:\n\n:<math>\\tilde c_n = \\langle \\tilde f, \\phi_n \\rangle = \\sum_i w_i \\tilde f(x_i) \\overline{\\phi_n(x_i)} = \\sum_i w_i V(x_i) f(x_i) \\overline{\\phi_n(x_i)}</math>\n\nHowever, using the spectral method, the same coefficient would be <math>\\tilde c_n = \\langle Vf, \\phi_n \\rangle</math>. The pseudo-spectral method thus introduces the additional approximation\n\n:<math>\\langle Vf, \\phi_n \\rangle \\approx \\sum_i w_i V(x_i) f(x_i) \\overline{\\phi_n(x_i)}.</math>\n\nIf the product <math>Vf</math> can be represented with the given finite set of basis functions, the above equation is exact due to the chosen quadrature.\n\n==Special pseudospectral schemes==\n\n===The Fourier method===\n\nIf periodic boundary conditions with period <math>[0,L]</math> are imposed on the system, the basis functions can be generated by plane waves,\n\n:<math>\\phi_n(x) = \\frac{1}{\\sqrt{L}} e^{-\\imath k_n x}</math>\n\nwith <math>k_n = (-1)^n \\lceil n/2 \\rceil 2\\pi/L</math>, where <math>\\lceil\\cdot\\rceil</math> is the [[ceiling function]].\n\nThe quadrature for a cut-off at <math>n_{\\text{max}} = N</math> is given by the [[discrete Fourier transform]]ation. The grid points are equally spaced, <math>x_i = i \\Delta x</math> with spacing <math>\\Delta x = L / (N+1)</math>, and the constant weights are <math>w_i = \\Delta x</math>.\n\nFor the discussion of the error, note that the product of two plane waves is again a plane wave, <math>\\phi_{a} + \\phi_b = \\phi_c</math> with <math>c \\leq a+b</math>. Thus, qualitatively, if the functions <math>f(x), V(x)</math> can be represented sufficiently accurately with <math>N_f, N_V</math> basis functions, the pseudo-spectral method gives accurate results if <math>N_f + N_V</math> basis functions are used.\n\nAn expansion in plane waves often has a poor quality and needs many basis functions to converge. However, the transformation between the basis expansion and the grid representation can be done using a [[Fast Fourier transform]], which scales favorably as <math>N \\ln N</math>. As a consequence, plane waves are one of the most common expansion that is encountered with pseudo-spectral methods.\n\n===Polynomials===\n\nAnother common expansion is into classical polynomials. Here, the [[Gaussian quadrature]] is used, which states that one can always find weights <math>w_i</math> and points <math>x_i</math> such that\n\n:<math>\\int_a^b w(x) p(x) dx = \\sum_{i=0}^N w_i p(x_i)</math>\n\nholds for any polynomial <math>p(x)</math> of degree <math>2N+1</math> or less. Typically, the weight function <math>w(x)</math> and ranges <math>a,b</math> are chosen for a specific problem, and leads to one of the different forms of the quadrature. To apply this to the pseudo-spectral method, we choose basis functions <math>\\phi_n(x) = \\sqrt{w(x)} P_n(x)</math>, with <math>P_n</math> being a polynomial of degree <math>n</math> with the property\n\n:<math>\\int_a^b w(x) P_n(x) P_m(x) dx = \\delta_{mn}.</math>\n\nUnder these conditions, the <math>\\phi_n</math> form an orthonormal basis with respect to the scalar product <math>\\langle f, g \\rangle = \\int_a^b f(x) \\overline{g(x)} dx</math>. This basis, together with the quadrature points can then be used for the pseudo-spectral method.\n\nFor the discussion of the error, note that if <math>f</math> is well represented by <math>N_f</math> basis functions and <math>V</math> is well represented by a polynomial of degree <math>N_V</math>, their product can be expanded in the first <math>N_f+N_V</math> basis functions, and the pseudo-spectral method will give accurate results for that many basis functions.\n\nSuch polynomials occur naturally in several standard problems. For example, the quantum harmonic oscillator is ideally expanded in Hermite polynomials, and Jacobi-polynomials can be used to define the associated Legendre functions typically appearing in rotational problems.\n\n==References==\n{{Reflist}}\n* {{cite journal|last1=Orszag|first1=Steven A.|title=Numerical Methods for the Simulation of Turbulence|journal=Physics of Fluids|date=1969|volume=12|issue=12|pages=II-250|doi=10.1063/1.1692445}}\n* {{cite book|last1=Gottlieb|first1=David|last2=Orszag|first2=Steven A.|title=Numerical analysis of spectral methods : theory and applications|date=1989|publisher=Society for Industrial and Applied Mathematics|location=Philadelphia, Pa.|isbn=978-0898710236|edition=5. print.}}\n* {{cite book|last1=Hesthaven|first1=Jan S.|last2=Gottlieb|first2=Sigal|author2-link=Sigal Gottlieb|last3=Gottlieb|first3=David|title=Spectral methods for time-dependent problems|date=2007|publisher=Cambridge Univ. Press|location=Cambridge [u.a.]|isbn=9780521792110|edition=1. publ.}}\n* {{cite book|last1=Trefethen|first1=Lloyd N.|title=Spectral methods in MATLAB|date=2000|publisher=SIAM|location=Philadelphia, Pa|isbn=978-0-89871-465-4|edition=3rd. repr.}}\n* {{cite book|last1=Fornberg|first1=Bengt|title=A Practical Guide to Pseudospectral Methods|date=1996|publisher=Cambridge University Press|location=Cambridge|isbn=9780511626357}}\n* {{cite book|last1=Boyd|first1=John P.|title=Chebyshev and Fourier spectral methods|date=2001|publisher=Dover Publications|location=Mineola, N.Y.|isbn=978-0486411835|edition=2nd ed., rev.|url=http://www-personal.umich.edu/~jpboyd/BOOK_Spectral2000.html}}\n* {{cite book|last1=Funaro|first1=Daniele|title=Polynomial approximation of differential equations|date=1992|publisher=Springer-Verlag|location=Berlin|isbn=978-3-540-46783-0|url=http://cdm.unimo.it/home/matematica/funaro.daniele/bube.htm}}\n* {{cite journal|last1=de Frutos|first1=Javier|last2=Novo|first2=Julia|title=A Spectral Element Method for the Navier--Stokes Equations with Improved Accuracy|journal=SIAM Journal on Numerical Analysis|date=January 2000|volume=38|issue=3|pages=799–819|doi=10.1137/S0036142999351984}}\n* {{cite book|last1=Claudio|first1=Canuto|last2=M. Yousuff|first2=Hussaini|last3=Alfio|first3=Quarteroni|last4=Thomas A.|first4=Zang|title=Spectral methods fundamentals in single domains|date=2006|publisher=Springer-Verlag|location=Berlin|isbn=978-3-540-30726-6}}\n*{{Cite book | last1=Press | first1=WH | last2=Teukolsky | first2=SA | last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press |  publication-place=New York | isbn=978-0-521-88068-8 | chapter=Section 20.7. Spectral Methods | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=1083}}\n\n{{DEFAULTSORT:Pseudo-Spectral Method}}\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Bellman pseudospectral method",
      "url": "https://en.wikipedia.org/wiki/Bellman_pseudospectral_method",
      "text": "The '''Bellman pseudospectral method''' is a [[pseudospectral method]] for [[optimal control]] based on [[Bellman equation|Bellman's principle of optimality]]. It is part of the larger theory of [[pseudospectral optimal control]], a term coined by [[I. Michael Ross|Ross]].<ref name=\"RK\">{{cite journal | last1 = Ross | first1 = I. M. | last2 = Karpenko | first2 = M. | year = 2012 | title = A Review of Pseudospectral Optimal Control: From Theory to Flight | url = http://www.sciencedirect.com/science/article/pii/S1367578812000375 | journal = Annual Reviews in Control | volume = 36 | issue = | pages = 182–197 | doi=10.1016/j.arcontrol.2012.09.002}}</ref>   The method is named after [[Richard E. Bellman]].  It was introduced by [[I. Michael Ross|Ross]] et al.<ref name=\"Ross1\">{{cite journal | last1 = Ross | first1 = I. M. | last2 = Gong | first2 = Q. | last3 = Sekhavat | first3 = P. | year = 2007 | title = Low-Thrust, High-Accuracy Trajectory Optimization | url = | journal = Journal of Guidance, Control and Dynamics | volume = 30 | issue = 4| pages = 921–933 | doi=10.2514/1.23181}}</ref><ref name=\"Ross2\">I. M. Ross, Q. Gong and P. Sekhavat, The Bellman pseudospectral method, AIAA/AAS Astrodynamics Specialist Conference and Exhibit, Honolulu, Hawaii, AIAA-2008-6448, August 18–21, 2008.</ref>\nfirst as a means to solve multiscale optimal control problems, and later expanded to obtain suboptimal solutions for general optimal control problems.\n\n==Theoretical foundations==\nThe multiscale version of the Bellman pseudospectral method is based on the spectral convergence property of the [[Ross–Fahroo pseudospectral method]]s. That is, because the Ross–Fahroo pseudospectral method converges at an exponentially fast rate, pointwise convergence to a solution is obtained at very low number of nodes even when the solution has high-frequency components.  This [[aliasing]] phenomenon in optimal control was first discovered by Ross et al.<ref name = \"Ross1\" />  Rather than use signal processing techniques to anti-alias the solution, Ross et al. proposed that Bellman's principle of optimality can be applied to the converged solution to extract information between the nodes.  Because the Gauss–Lobatto nodes cluster at the boundary points, Ross et al. suggested that if the node density around the initial conditions satisfy the [[Nyquist–Shannon sampling theorem]], then the complete solution can be recovered by solving the optimal control problem in a recursive fashion over piecewise segments known as Bellman segments.<ref name=\"Ross1\" />\n\nIn an expanded version of the method, Ross et al.,<ref name = \"Ross2\" /> proposed that method could also be used to generate feasible solutions that were not necessarily optimal.  In this version, one can apply the Bellman pseudospectral method at even lower number of nodes even under the knowledge that the solution may not have converged to the optimal one.  In this situation, one obtains a feasible solution.\n\nA remarkable feature of the Bellman pseudospectral method is that it automatically determines several measures of suboptimality based on the original pseudospectral cost and the cost generated by the sum of the Bellman segments.<ref name=\"Ross1\" /><ref name=\"Ross2\" />\n\n==Computational efficiency==\nOne of the computational advantages of the Bellman pseudospectral method is that it allows one to escape Gaussian rules in the distribution of node points.  That is, in a standard pseudospectral method, the distribution of node points are Gaussian (typically Gauss-Lobatto for finite horizon and Gauss-Radau for infinite horizon).  The Gaussian points are sparse in the middle of the interval (middle is defined in a shifted sense for infinite-horizon problems) and dense at the boundaries. The second-order accumulation of points near the boundaries have the effect of wasting nodes.  The Bellman pseudospectral method takes advantage of the node accumulation at the initial point to anti-alias the solution and discards the remainder of the nodes.  Thus the final distribution of nodes is non-Gaussian and dense while the computational method retains a sparse structure.\n\n==Applications==\nThe Bellman pseudospectral method was first applied by Ross et al.<ref name=\"Ross1\" /> to solve the challenging problem of very low thrust trajectory optimization.  It has been successfully applied to solve a practical problem of generating very high accuracy solutions to a trans-Earth-injection problem of bringing a space capsule from a lunar orbit to a pin-pointed Earth-interface condition for successful reentry.<ref name=\"Yan1\">{{cite journal | last1 = Yan | first1 = H. | last2 = Gong | first2 = Q. | last3 = Park | first3 = C. | last4 = Ross | first4 = I. M. | last5 = D'Souza | first5 = C. N. | year = 2011 | title = High Accuracy Trajectory Optimization for a Trans-Earth Lunar Mission | url = | journal = Journal of Guidance, Control and Dynamics | volume = 34 | issue = 4| pages = 1219–1227 | doi=10.2514/1.49237}}</ref><ref name =\"Yan2\">H. Yan, Q. Gong, C. D. Park, I. M. Ross and C. N. D'Souza, High-Accuracy Moon to Earth trajectory optimization, AIAA Guidance, Navigation, and Control Conference, 2010.</ref>\n\nThe Bellman pseudospectral method is most commonly used as an additional check on the optimality of a pseudospectral solution generated by the Ross–Fahroo pseudospectral methods.  That is, in addition to the use of [[Pontryagin's minimum principle]] in conjunction with the solutions obtained by the Ross–Fahroo pseudospectral methods, the Bellman pseudospectral method is used as a primal-only test on the optimality of the computed solution.<ref name=\"F1\">{{cite journal | last1 = Fleming | first1 = A. | last2 = Sekhavat | first2 = P. | last3 = Ross | first3 = I. M. | year = 2010 | title = Minimum-Time Reorientation of a Rigid Body | url = | journal = Journal of Guidance, Control and Dynamics | volume = 33 | issue = 1| pages = 160–170 | doi=10.2514/1.43549}}</ref><ref name =\"Ross3\">{{cite journal | last1 = Ross | first1 = I. M. | last2 = Sekhavat | first2 = P. | last3 = Fleming | first3 = A. | last4 = Gong | first4 = Q. | year = 2008 | title = Optimal feedback control: foundations, examples, and experimental results for a new approach | url = | journal = Journal of Guidance, Control, and Dynamics | volume = 31 | issue = 2| pages = 307–321 | doi=10.2514/1.29532}}</ref>\n\n==See also==\n*[[Legendre pseudospectral method]]\n*[[Chebyshev pseudospectral method]]\n*[[Pseudospectral knotting method]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Pseudospectral Optimal Control}}\n[[Category:Optimal control]]\n[[Category:Numerical analysis]]\n[[Category:Control theory]]"
    },
    {
      "title": "Chebyshev pseudospectral method",
      "url": "https://en.wikipedia.org/wiki/Chebyshev_pseudospectral_method",
      "text": "The '''Chebyshev pseudospectral method''' for [[optimal control]] problems is based on [[Chebyshev polynomials|Chebyshev polynomials of the first kind]]. It is part of the larger theory of [[pseudospectral optimal control]], a term coined by [[I. Michael Ross|Ross]].<ref name=\"RK\">{{cite journal | last1 = Ross | first1 = I. M. | last2 = Karpenko | first2 = M. | year = 2012 | title = A Review of Pseudospectral Optimal Control: From Theory to Flight | url = http://www.sciencedirect.com/science/article/pii/S1367578812000375 | journal = Annual Reviews in Control | volume = 36 | issue = 2| pages = 182–197 | doi=10.1016/j.arcontrol.2012.09.002}}</ref> Unlike the [[Legendre pseudospectral method]], the Chebyshev pseudospectral (PS) method does not immediately offer high-accuracy quadrature solutions. Consequently, two different versions of the method have been proposed: one by Elnagar et al.,<ref name=\"EK1\">{{cite journal | last1 = Elnagar | first1 = G. | last2 = Kazemi | first2 = M. A. | year = 1998 | title = Pseudospectral Chebyshev Optimal Control of Constrained Nonlinear Dynamical Systems | url = | journal = Computational Optimization and Applications | volume = 11 | issue = 2| pages = 195–217 | doi = 10.1023/A:1018694111831 }}</ref> and another by Fahroo and Ross.<ref name=\"FR1\">{{cite journal | last1 = Fahroo | first1 = F. | last2 = Ross | first2 = I. M. | year = 2002 | title = Direct trajectory optimization by a Chebyshev pseudospectral method | url = | journal = Journal of Guidance, Control, and Dynamics | volume = 25 | issue = 1| pages = 160–166 | doi=10.2514/2.4862| bibcode = 2002JGCD...25..160F }}</ref>  The two versions differ in their quadrature techniques.  The [[Ross–Fahroo pseudospectral method|Fahroo–Ross method]] is more commonly used today due to the ease in implementation of the [[Clenshaw–Curtis quadrature]] technique (in contrast to Elnagar–Kazemi's cell-averaging method). In 2008, Trefethen showed that the Clenshaw–Curtis method was nearly as accurate as [[Gauss quadrature]].\n<ref name =\"LNT1\">{{cite journal | last1 = Trefethen | first1 = Lloyd N. | authorlink = Lloyd N. Trefethen | year = 2008 | title = Is Gauss quadrature better than Clenshaw–Curtis? | url = | journal = SIAM Review | volume = 50 | issue = 1| pages = 67–87 | doi=10.1137/060659831| bibcode = 2008SIAMR..50...67T | citeseerx = 10.1.1.468.1193 }}</ref>  This breakthrough result opened the door for a covector mapping theorem for Chebyshev PS methods.<ref name=\"GRF1\">{{cite journal | last1 = Gong | first1 = Q. | last2 = Ross | first2 = I. M. | last3 = Fahroo | first3 = F. | year = 2010 | title = Costate Computation by a Chebyshev Pseudospectral Method | url = | journal = Journal of Guidance, Control, and Dynamics | volume = 33 | issue = 2| pages = 623–628 | doi=10.2514/1.45154| bibcode = 2010JGCD...33..623G }}</ref>  A complete mathematical theory for Chebyshev PS methods was finally developed in 2009 by Gong, Ross and Fahroo.<ref name=\"GRF2\">Q. Gong, I. M. Ross and F. Fahroo, A Chebyshev Pseudospectral Method for Nonlinear Constrained Optimal\nControl Problems, Joint 48th IEEE Conference on Decision and Control and\n28th Chinese Control Conference\nShanghai, P.R. China, December 16–18, 2009</ref>\n\n==Other Chebyshev methods==\nThe Chebyshev PS method is frequently confused with other Chebyshev methods.  Prior to the advent of PS methods, many authors<ref name=\"VD\">{{cite journal | last1 = Vlassenbroeck | first1 = J. | last2 = Dooren | first2 = R. V. | year = 1988 | title = A Chebyshev technique for solving nonlinear optimal control problems | url = | journal = IEEE Transactions on Automatic Control | volume = 33 | issue = 4| pages = 333–340 | doi=10.1109/9.192187}}</ref> proposed using [[Chebyshev polynomials]] to solve [[optimal control]] problems; however, none of these methods belong to the class of [[pseudospectral method]]s.\n\n==See also==\n*[[Legendre pseudospectral method]]\n*[[Ross–Fahroo pseudospectral method]]s\n*[[Ross–Fahroo lemma]]\n*[[Bellman pseudospectral method]]\n*[[DIDO (optimal control)|DIDO]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Pseudospectral Optimal Control}}\n[[Category:Optimal control]]\n[[Category:Numerical analysis]]\n[[Category:Control theory]]"
    },
    {
      "title": "Flat pseudospectral method",
      "url": "https://en.wikipedia.org/wiki/Flat_pseudospectral_method",
      "text": "The '''flat pseudospectral method''' is part of the family of the [[Ross–Fahroo pseudospectral method]]s introduced by [[I. Michael Ross|Ross]] and [[Fariba Fahroo|Fahroo]].<ref name =\"rf-tac\"> Ross, I. M. and Fahroo, F., “Pseudospectral Methods for the Optimal Motion Planning of Differentially Flat Systems,” IEEE Transactions on Automatic Control, Vol.49, No.8, pp. 1410–1413, August 2004.</ref><ref name=\"rf-cdc-2003\"> Ross, I. M. and Fahroo, F., “A Unified Framework for Real-Time Optimal Control,” Proceedings of the IEEE Conference on Decision and Control, Maui, HI, December, 2003.</ref>  The method combines the concept of [[Flatness (systems theory)|differential flatness]] with [[pseudospectral optimal control]]  to generate outputs in the so-called flat space.<ref name=\"fliess-95\"> Fliess, M., Lévine, J.,  Martin, Ph., and Rouchon, P., “Flatness and defect of nonlinear systems: Introductory theory and examples,” International Journal of  Control, vol. 61, no. 6, pp. 1327–1361, 1995. </ref><ref name=\"RMM-siam98\"> Rathinam, M. and Murray, R. M., “Configuration flatness of Lagrangian systems underactuated by one control” SIAM Journal on Control and Optimization, 36, 164,1998. </ref> \n\n==Concept==\nBecause the differentiation matrix, <math>D </math>, in a pseudospectral method is square, higher-order derivatives of any polynomial, <math> y </math>, can be obtained by powers of <math>D </math>,\n: <math>\n\\begin{align}\n\\dot y &= D Y \\\\ \n\\ddot y & = D^2 Y \\\\\n&{} \\  \\vdots \\\\\ny^{(\\beta)} &= D^\\beta Y\n\\end{align}</math>\n\nwhere <math> Y </math> is the pseudospectral variable and <math> \\beta </math> is a finite positive integer.  \nBy differential flatness, there exists functions <math> a </math>  and <math> b </math> such that the state and control variables can be written as,\n\n: <math>\n\\begin{align}\nx & = a(y, \\dot y, \\ldots, y^{(\\beta)}) \\\\ \nu & = b(y, \\dot y, \\ldots, y^{(\\beta + 1)})\n\\end{align}\n</math>\n\nThe combination of these concepts generates the flat pseudospectral method; that is, x and u are written as,\n:<math> x = a(Y, D Y, \\ldots, D^\\beta Y) </math>\n:<math> u = b(Y, D Y, \\ldots, D^{\\beta + 1}Y) </math>\nThus, an optimal control problem can be quickly and easily transformed to a problem with just the Y pseudospectral variable.<ref name = \"rf-tac\" />\n\n==See also==\n*[[Ross' π lemma]]\n*[[Ross–Fahroo lemma]]\n*[[Bellman pseudospectral method]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Pseudospectral Optimal Control}}\n[[Category:Optimal control]]\n[[Category:Numerical analysis]]\n[[Category:Control theory]]"
    },
    {
      "title": "Legendre pseudospectral method",
      "url": "https://en.wikipedia.org/wiki/Legendre_pseudospectral_method",
      "text": "The '''Legendre pseudospectral method''' for [[optimal control]] problems is based on [[Legendre polynomials]].  It is part of the larger theory of [[pseudospectral optimal control]], a term coined by [[I. Michael Ross|Ross]].<ref name=\"RK\">{{cite journal | last1 = Ross | first1 = I. M. | last2 = Karpenko | first2 = M. | year = 2012 | title = A Review of Pseudospectral Optimal Control: From Theory to Flight | url = http://www.sciencedirect.com/science/article/pii/S1367578812000375 | journal = Annual Reviews in Control | volume = 36 | issue = 2| pages = 182–197 | doi=10.1016/j.arcontrol.2012.09.002}}</ref>  A basic version of the Legendre pseudospectral was originally proposed by Elnagar and his coworkers in 1995.<ref name=\"Elnagar1\">G. Elnagar, M. A. Kazemi, and M. Razzaghi, \"The Pseudospectral Legendre Method for Discretizing Optimal Control Problems,\" ''IEEE Transactions on Automatic Control,'' 40:1793–1796, 1995.</ref> Since then, Ross, [[Fariba Fahroo|Fahroo]] and their coworkers<ref>Ross, I. M. and Fahroo, F., “Legendre Pseudospectral Approximations of Optimal Control Problems,” ''Lecture Notes in Control and Information Sciences,'' Vol. 295, Springer-Verlag, New York, 2003, pp 327-342</ref><ref name=\":0\" /><ref name=\":1\" /><ref name = \"Kang1\">{{cite journal | last1 = Kang | first1 = W. | last2 = Gong | first2 = Q. | last3 = Ross | first3 = I. M. | last4 = Fahroo | first4 = F. | year = | title = On the Convergence of Nonlinear Optimal Control Using Pseudospectral Methods for Feedback Linearizable Systems | url = | journal = International Journal of Robust and Nonlinear Control | volume = 17 | issue = 1251–1277| page = 2007 }}</ref><ref name =\"RF1\">{{cite journal | last1 = Ross | first1 = I. M. | last2 = Fahroo | first2 = F. | year = | title = Pseudospectral Knotting Methods for Solving Nonsmooth Optimal Control Problems | url = | journal = Journal of Guidance Control and Dynamics | volume = 27 | issue = 397–405| page = 2004 }}</ref> have extended, generalized and applied the method for a large range of problems.<ref name=\"Gong1\">Q. Gong, W. Kang, N. Bedrossian, F. Fahroo, P. Sekhavat and K. Bollino, \"Pseudospectral Optimal Control for Military and Industrial Applications,\" ''46th IEEE Conference on Decision and Control,'' New Orleans, LA, pp. 4128–4142, Dec. 2007.</ref> An application that has received wide publicity<ref name=\"Kang2\">{{cite journal | last1 = Kang | first1 = W. | last2 = Bedrossian | first2 = N. | year = | title = Pseudospectral Optimal Control Theory Makes Debut Flight, Saves NASA $1M in Under Three Hours | url = | journal = SIAM News | volume = 40 | issue = | page = 2007 }}</ref><ref>Bedrossian, N. S., Bhatt, S., Kang, W. and Ross, I. M., “Zero-Propellant Maneuver Guidance,” ''IEEE Control Systems Magazine'', Vol.29, No.5, October 2009, pp 53-73; Cover Story.</ref> is the use of their method for generating real time trajectories for the [[International Space Station]].\n\n==Fundamentals==\nThere are three basic types of Legendre pseudospectral methods:<ref name=\"RK\" />\n# One based on Gauss-Lobatto points\n## First proposed by Elnagar et al<ref name=\"Elnagar1\" /> and subsequently extended by Fahroo and Ross<ref name=\":0\">Fahroo, F. and Ross, I. M., “Costate Estimation by a Legendre Pseudospectral Method,” ''Journal of Guidance, Control and Dynamics'', Vol.24, No.2, March–April 2001, pp.270-277.</ref> to incorporate the [[Covector mapping principle|covector mapping theorem]].\n## Forms the basis for solving general nonlinear finite-horizon optimal control problems.<ref name=\"RK\" /><ref name=\":2\" /><ref name=\":3\">{{Cite book|title=A Primer on Pontryagin's Principle in Optimal Control|last=Ross|first=Isaac|publisher=Collegiate Publishers|year=2015|isbn=|location=San Francisco|pages=|quote=|via=}}</ref>\n## Incorporated in several software products\n##* [[DIDO (software)|DIDO]], [https://otis.grc.nasa.gov/ OTIS], [http://www.psopt.org/ PSOPT]\n# One based on Gauss-Radau points\n## First proposed by Fahroo and Ross<ref>Fahroo, F. and Ross, I. M., “Pseudospectral Methods for Infinite Horizon Nonlinear Optimal Control Problems,” ''AIAA Guidance, Navigation and Control Conference'', August 15–18, 2005, San Francisco, CA</ref> and subsequently extended (by Fahroo and Ross) to incorporate a [[Covector mapping principle|covector mapping theorem]].<ref name=\":1\">Fahroo, F. and Ross, I. M., “Pseudospectral Methods for Infinite-Horizon Optimal Control Problems,” ''Journal of Guidance, Control and Dynamics'', Vol. 31, No. 4, pp. 927-936, 2008.</ref>\n## Forms the basis for solving general nonlinear infinite-horizon optimal control problems.<ref name=\"RK\" /><ref name=\":3\" />\n## Forms the basis for solving general nonlinear finite-horizon problems with one free endpoint.<ref name=\"RK\" /><ref name=\":2\" /><ref name=\":3\" />\n# One based on Gauss points\n## First proposed by Reddien<ref>Reddien, G.W., \"Collocation at Gauss Points as a Discretization in Optimal Control,\" ''SIAM Journal on Control and Optimization'', Vol. 17, No. 2, March 1979.</ref>\n## Forms the basis for solving finite-horizon problems with free endpoints<ref name=\":2\">Fahroo F., and Ross, I. M.,  \"Advances in Pseudospectral Methods for Optimal Control,\" ''AIAA Guidance, Navigation, and Control     Conference'', AIAA Paper 2008-7309, Honolulu, Hawaii, August 2008.</ref><ref name=\":3\" />\n## Incorporated in several software products\n##* [[GPOPS-II|GPOPS]], [[PROPT]]\n\n==Software==\nThe first software to implement the Legendre pseudospectral method was [[DIDO (optimal control)|DIDO]] in 2001.<ref name=\":3\" /><ref name=\"rea-mit\">J. R. Rea, ''A Legendre Pseudospectral Method for Rapid Optimization of Launch Vehicle Trajectories,'' S.M. Thesis, Dept. of Aeronautics and Astronautics, Massachusetts Institute of Technology, 2001. http://dspace.mit.edu/handle/1721.1/8608</ref>  Subsequently, the method was incorporated in the NASA code OTIS.<ref>{{Cite web|url=https://otis.grc.nasa.gov/|title=[ OTIS ] Optimal Trajectories by Implicit Simulation|website=otis.grc.nasa.gov|access-date=2016-12-08}}</ref>  Years later, many other software products emerged at an increasing pace, such as PSOPT, [[PROPT]] and [[GPOPS-II|GPOPS]].\n\n==Flight implementations==\nThe Legendre pseudospectral method (based on Gauss-Lobatto points) has been implemented in flight<ref name=\"RK\" /> by [[NASA]] on several spacecraft through the use of the software, [[DIDO (software)|DIDO]]. The first flight implementation was on November 5, 2006, when [[NASA]] used [[DIDO (software)|DIDO]] to maneuver the [[International Space Station]] to perform the [https://web.archive.org/web/20081005205928/http://www.nasa.gov/mission_pages/station/science/experiments/ZPM.html '''Zero Propellant Maneuver''']. The [[Zero Propellant Maneuver]] was discovered by Nazareth Bedrossian using [[DIDO (optimal control)|DIDO]]. [https://www.youtube.com/watch?v=MIp27Ea9_2I Watch a video] of this historic maneuver.\n\n==See also==\n*[[DIDO (optimal control)|DIDO]] \n*[[Chebyshev pseudospectral method]]\n*[[Ross–Fahroo pseudospectral method]]s\n*[[Ross–Fahroo lemma]]\n*[[Covector mapping principle]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Pseudospectral Optimal Control}}\n[[Category:Optimal control]]\n[[Category:Numerical analysis]]\n[[Category:Control theory]]"
    },
    {
      "title": "Pseudospectral knotting method",
      "url": "https://en.wikipedia.org/wiki/Pseudospectral_knotting_method",
      "text": "In [[applied mathematics]], the '''pseudospectral knotting method''' is a generalization and enhancement of a standard [[pseudospectral optimal control|pseudospectral method for optimal control]]. The concept was introduced by [[I. Michael Ross]] and [[Fariba Fahroo|F. Fahroo]] in 2004, and forms part of the collection of the [[Ross–Fahroo pseudospectral method]]s.<ref name=\"RF1\">Ross, I. M. and Fahroo, F., Pseudospectral Knotting Methods for Solving Optimal Control Problems, ''Journal of Guidance, Control and Dynamics,'' Vol. 27, No. 3, pp. 397–405, 2004.</ref>\n\n==Definition==\nAccording to Ross and Fahroo a pseudospectral (PS) knot is a double Lobatto point; i.e. two boundary points on top of one another.<ref name=\"RF1\" /> At this point, information (such as discontinuities, jumps, dimension changes etc.) is exchanged between two standard PS methods.  This information exchange is used to solve some of the most difficult problems in [[optimal control]] known as hybrid optimal control problems.<ref name=\"RD1\">Ross, I. M. and D’Souza, C. N., A Hybrid Optimal Control Framework for Mission Planning, ''Journal of Guidance, Control and Dynamics,'' Vol. 28, No. 4, July–August 2005, pp. 686–697.</ref>\n\nIn a hybrid optimal control problem, an optimal control problem is intertwined with a [[graph theory|graph problem]]. A standard [[pseudospectral optimal control]] method is incapable of solving such problems; however, through the use of pseudospectral knots, the information of the graph can be encoded at the double Lobatto points thereby allowing a hybrid optimal control problem to be discretized and solved using powerful software such as [[DIDO (optimal control)|DIDO]].\n\n==Applications==\nPS knots have found applications in aerospace problems such as the ascent guidance of a launch vehicles, and advancing the Aldrin Cycler through the use of solar sails.<ref name=\"SR1\">Stevens, R. and Ross, I. M., Preliminary Design of Earth–Mars Cyclers Using Solar Sails, ''Journal of Spacecraft and Rockets,'' Vol. 41, No. 4, 2004.</ref><ref name=\"SRM\"> Stevens, R., Ross, I. M. and Matousek, S. E., \"Earth-Mars Return Trajectories Using Solar Sails,\" 55th International Astronautical Congress, Vancouver, Canada, IAC-04-A.2.08, October 4–8, 2004.</ref>\nPS knots have also been used for anti-aliasing of PS optimal control solutions and for capturing critical information in switches in solving bang-bang-type [[optimal control]] problems.<ref name=\"GFR1\">Gong, Q., Fahroo, F. and Ross, I. M., A Spectral Algorithm for Pseudospectral Methods in Optimal Control, ''Journal of Guidance, Control and Dynamics,'' Vol. 31, No. 3, pp. 460–471, 2008.</ref>\n\n==Software==\nThe PS knotting method was first implemented in the [https://www.mathworks.com/products/connections/product_detail/product_61633.html MATLAB optimal control] software package, [[DIDO (optimal control)|DIDO]].\n\n==See also==\n*[[Legendre pseudospectral method]]\n*[[Chebyshev pseudospectral method]]\n*[[Ross–Fahroo lemma]]\n*[[Ross' π lemma]]\n*[[Ross–Fahroo pseudospectral method]]s\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Pseudospectral Optimal Control}}\n[[Category:Optimal control]]\n[[Category:Numerical analysis]]\n[[Category:Control theory]]"
    },
    {
      "title": "Ross–Fahroo lemma",
      "url": "https://en.wikipedia.org/wiki/Ross%E2%80%93Fahroo_lemma",
      "text": "Named after [[I. Michael Ross]] and [[Fariba Fahroo|F. Fahroo]], the '''Ross–Fahroo lemma''' is a fundamental result in [[optimal control]] theory.<ref name=\"RF01\">\nI. M. Ross and F. Fahroo, A Pseudospectral Transformation of the Covectors of Optimal Control Systems, Proceedings of the First IFAC Symposium on System Structure and Control, Prague, Czech Republic, 29–31 August 2001.</ref><ref name=\"RF1\">{{cite journal | last1 = Ross | first1 = I. M. | last2 = Fahroo | first2 = F. | year = 2003 | title = Legendre Pseudospectral Approximations of Optimal Control Problems | url = | journal = Lecture Notes in Control and Information Sciences | volume = 295 | issue = | page = }}</ref><ref name=\"RF3\">\nI. M. Ross and F. Fahroo, Discrete Verification of Necessary Conditions for Switched Nonlinear Optimal Control Systems, ''Proceedings of the American Control Conference, Invited Paper'', June 2004, Boston, MA.</ref><ref name = \"IEEESpectrum\">N. Bedrossian,  M. Karpenko,  and S. Bhatt, \"Overclock My Satellite: Sophisticated Algorithms Boost Satellite Performance on the Cheap\", ''[[IEEE Spectrum]]'', November 2012.</ref>\n\nIt states that dualization and [[discretization]] are, in general, non-commutative operations. The operations can be made commutative by an application of the [[covector mapping principle]].<ref name=\"RK\">{{cite journal | last1 = Ross | first1 = I. M. | last2 = Karpenko | first2 = M. | year = 2012 | title = A Review of Pseudospectral Optimal Control: From Theory to Flight | url = http://www.sciencedirect.com/science/article/pii/S1367578812000375 | journal = Annual Reviews in Control | volume = 36 | issue = 2| pages = 182–197 | doi=10.1016/j.arcontrol.2012.09.002}}</ref>\n\n==Description of the theory==\nA continuous-time optimal control problem is information rich. A number of interesting properties of a given problem can be derived by applying the [[Pontryagin's minimum principle]] or the [[Hamilton–Jacobi–Bellman equation]]s.  These theories implicitly use the continuity of time in their derivation.<ref name =\"mord\" >B. S. Mordukhovich, Variational Analysis and Generalized Differentiation: Basic Theory, Vol.330 of Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences] Series, Springer, Berlin, 2005.</ref> \nWhen an optimal control problem is discretized, the Ross–Fahroo lemma asserts that there is a fundamental loss of information. This loss of information can be in the primal variables as in the value of the control at one or both of the boundary points or in the dual variables as in the value of the Hamiltonian over the time horizon.<ref name=\"FR1\">F. Fahroo and I. M. Ross, Pseudospectral Methods for Infinite Horizon Nonlinear Optimal Control Problems, AIAA Guidance, Navigation and Control Conference, August 15–18, 2005, San Francisco, CA.</ref><ref name=\"FR2\">{{cite journal | last1 = Fahroo | first1 = F. | last2 = Ross | first2 = I. M. | year = 2008 | title = Pseudospectral Methods for Infinite-Horizon Optimal Control Problems | url = | journal = Journal of Guidance, Control and Dynamics | volume = 31 | issue = 4| pages = 927–936 | doi=10.2514/1.33117}}</ref> To address the information loss, Ross and Fahroo introduced the concept of closure conditions which allow the known information loss to be put back in. This is done by an application of the [[covector mapping principle]].<ref name=\"RK\" />\n\n==Applications to pseudospectral optimal control==\nWhen pseudospectral methods are applied to discretize optimal control problems, the implications of the Ross–Fahroo lemma appear in the form of the discrete covectors seemingly being discretized by the transpose of the differentiation matrix.<ref name = \"RF01\" /><ref name = \"RF1\" /><ref name = \"RF3\" />\n\nWhen the [[covector mapping principle]] is applied, it reveals the proper transformation for the adjoints.  Application of the transformation generates the [[Ross–Fahroo pseudospectral method]]s.<ref name=\"hawkins-mit\">[http://dspace.mit.edu/handle/1721.1/32431 A. M. Hawkins, ''Constrained Trajectory Optimization of a Soft Lunar Landing From a Parking Orbit,'' S.M. Thesis, Dept. of Aeronautics and Astronautics, Massachusetts Institute of Technology, 2005.]</ref><ref name=\"rea-mit\">[http://dspace.mit.edu/handle/1721.1/8608 J. R. Rea, ''A Legendre Pseudospectral Method for Rapid Optimization of Launch Vehicle Trajectories,'' S.M. Thesis, Dept. of Aeronautics and Astronautics, Massachusetts Institute of Technology, 2001.]</ref>\n\n==See also==\n*[[Ross' π lemma]]\n*[[Ross–Fahroo pseudospectral method]]s\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Pseudospectral Optimal Control}}\n[[Category:Optimal control]]\n[[Category:Numerical analysis]]\n[[Category:Control theory]]"
    },
    {
      "title": "Ross–Fahroo pseudospectral method",
      "url": "https://en.wikipedia.org/wiki/Ross%E2%80%93Fahroo_pseudospectral_method",
      "text": "Introduced by [[I. Michael Ross]] and [[Fariba Fahroo|F. Fahroo]], the '''Ross–Fahroo pseudospectral methods''' are a broad collection of [[Pseudospectral optimal control|pseudospectral methods for optimal control]].<ref name = \"IEEESpectrum\">N. Bedrossian,  M. Karpenko,  and S. Bhatt,\n\"Overclock My Satellite: Sophisticated Algorithms Boost Satellite Performance on the Cheap\",\n''[[IEEE Spectrum]]'', November 2012.</ref><ref name =\"NAS\">{{cite journal | last1 = Jr- | last2 = Li | first2 = S | last3 = Ruths | first3 = J. | last4 = Yu | first4 = T-Y | last5 = Arthanari | first5 = H. | last6 = Wagner | first6 = G. | year = 2011 | title = Optimal Pulse Design in Quantum Control: A Unified Computational Method | journal = Proceedings of the National Academy of Sciences | volume = 108 | issue = 5| pages = 1879–1884 | doi=10.1073/pnas.1009797108 | pmid=21245345 | pmc=3033291}}</ref><ref name = \"converge\">{{cite journal | last1 = Kang | first1 = W. | year = 2010 | title = Rate of Convergence for the Legendre Pseudospectral Optimal Control of Feedback Linearizable Systems | url = | journal = Journal of Control Theory and Application | volume = 8 | issue = 4| pages = 391–405 | doi=10.1007/s11768-010-9104-0}}</ref><ref name=\"survey\">{{cite journal | last1 = Conway | first1 = B. A. | year = 2012 | title = A Survey of Methods Available for the Numerical Optimization of Continuous Dynamic Systems | url = | journal = Journal of Optimization Theory Applications | volume = 152 | issue = 2| pages = 271–306 | doi=10.1007/s10957-011-9918-z}}</ref><ref name=\"RF01\">\nI. M. Ross and F. Fahroo, A Pseudospectral Transformation of the Covectors of Optimal Control Systems, Proceedings of the First IFAC Symposium on System Structure and Control, Prague, Czech Republic, 29–31 August 2001.</ref><ref name=\"RF1\">\nI. M. Ross and F. Fahroo, Legendre Pseudospectral Approximations of Optimal Control Problems, ''Lecture Notes in Control and Information Sciences'', Vol. 295, Springer-Verlag, 2003.</ref><ref name=\"RF2\">{{cite journal | last1 = Ross | first1 = I. M. | last2 = Fahroo | first2 = F. | year = 2004 | title = Pseudospectral Knotting Methods for Solving Optimal Control Problems | url = | journal = Journal of Guidance, Control and Dynamics | volume = 27 | issue = 3| pages = 397–405| doi=10.2514/1.3426}}</ref><ref name=\"RF3\">\nI. M. Ross and F. Fahroo, Discrete Verification of Necessary Conditions for Switched Nonlinear Optimal Control Systems, Proceedings of the American Control Conference, Invited Paper, June 2004, Boston, MA.</ref><ref name=\"RF4\">{{cite journal | last1 = Ross | first1 = I. M. | last2 = Fahroo | first2 = F. | year = 2004 | title = Pseudospectral Methods for the Optimal Motion Planning of Differentially Flat Systems | url = | journal = IEEE Transactions on Automatic Control | volume = 49 | issue = 8| pages = 1410–1413 | doi=10.1109/tac.2004.832972| hdl = 10945/29675 }}</ref>  Examples of the Ross–Fahroo pseudospectral methods are the [[pseudospectral knotting method]], the [[flat pseudospectral method]], the Legendre-Gauss-Radau pseudospectral method<ref name=\"advances\">F. Fahroo and I. M. Ross, \"Advances in Pseudospectral Methods for Optimal Control,\" ''Proceedings of the AIAA Guidance, Navigation and Control Conference,'' AIAA 2008-7309. \n[http://www.elissarglobal.com/wp-content/uploads/2012/04/Advances-in-Pseudospectral-Methods-for-Optimal-Control.pdf]</ref><ref name=\"wen\">{{cite journal | last1 = Wen | first1 = H. | last2 = Jin | first2 = D. | last3 = Hu | first3 = H. | year = 2008 | title = Infinite-Horizon Control for Retrieving a Tethered Subsatellite via an Elastic Tether | url = | journal = Journal of Guidance, Control and Dynamics | volume = 31 | issue = 4| pages = 889–906 | doi=10.2514/1.33224}}</ref> and pseudospectral methods for infinite-horizon optimal control.<ref name=\"FR1\">\nF. Fahroo and I. M. Ross, Pseudospectral Methods for Infinite Horizon Nonlinear Optimal Control Problems, AIAA Guidance, Navigation and Control Conference, August 15–18, 2005, San Francisco, CA.</ref>\n<ref name=\"FR2\">{{cite journal | last1 = Fahroo | first1 = F. | last2 = Ross | first2 = I. M. | year = 2008 | title = Pseudospectral Methods for Infinite-Horizon Optimal Control Problems | url = | journal = Journal of Guidance, Control and Dynamics | volume = 31 | issue = 4| pages = 927–936 | doi=10.2514/1.33117}}</ref>\n\n==Overview==\nThe Ross–Fahroo methods are based on shifted Gaussian pseudospectral node points.  The shifts are obtained by means of a linear or nonlinear transformation while the Gaussian pseudospectral points are chosen from a collection of [[Gaussian quadrature|Gauss-Lobatto]] or [[Gaussian quadrature|Gauss-Radau]] distribution arising from [[Legendre polynomials|Legendre]] or [[Chebyshev polynomials]].  The Gauss-Lobatto pseudospectral points are used for finite-horizon [[optimal control]] problems while the Gauss-Radau pseudospectral points are used for infinite-horizon optimal control problems.<ref name=\"RK\">{{cite journal | last1 = Ross | first1 = I. M. | last2 = Karpenko | first2 = M. | year = 2012 | title = A Review of Pseudospectral Optimal Control: From Theory to Flight | url = http://www.sciencedirect.com/science/article/pii/S1367578812000375 | journal = Annual Reviews in Control | volume = 36 | issue = 2| pages = 182–197 | doi=10.1016/j.arcontrol.2012.09.002}}</ref>\n\n==Mathematical applications==\nThe Ross–Fahroo methods are founded on the [[Ross–Fahroo lemma]]; they can be applied to optimal control problems governed by [[differential equation]]s, [[differential algebraic equation|differential-algebraic equations]], [[differential inclusion]]s, and differentially-flat systems. They can also be applied to infinite-horizon optimal control problems by a simple domain transformation technique.<ref name=\"FR1\"/>\n<ref name=\"FR2\"/>  The Ross–Fahroo pseudospectral methods also form the foundations for the [[Bellman pseudospectral method]].\n\n==Flight applications and awards==\nThe Ross–Fahroo methods have been implemented in many practical applications and laboratories around the world. In 2006, NASA used the Ross–Fahroo method to implement the \"zero propellant maneuver\" on board the [[International Space Station]].<ref name=\"zpmIEEE\">N. S. Bedrossian, S. Bhatt, W. Kang, and I. M. Ross, Zero-Propellant Maneuver Guidance, IEEE Control Systems Magazine, October 2009 (Feature Article), pp 53–73.</ref>\nIn recognition of all these advances, the AIAA presented Ross and Fahroo, the 2010 Mechanics and Control of Flight Award, for \"... changing the landscape of flight mechanics.\" Ross was also elected AAS Fellow for \"his pioneering contributions to pseudospectral optimal control.\"\n\n==Distinctive features==\nA remarkable feature of the Ross–Fahroo methods is that it does away with the prior notions of \"direct\" and \"indirect\" methods. That is, through a collection of theorems put forth by Ross and Fahroo,<ref name = \"RF01\" /><ref name = \"RF1\" /><ref name = \"RF3\" />\n<ref name = \"FR00\">\nF. Fahroo and I. M. Ross, Trajectory Optimization by Indirect Spectral Collocation Methods, Proceedings of the AIAA/AAS Astrodynamics Conference, August 2000, Denver, CO. AIAA Paper 2000–4028</ref>\nthey showed that it was possible to design pseudospectral methods for optimal control that were equivalent in both the direct and indirect forms. This implied that one could use their methods as simply as a \"direct\" method while automatically generating accurate duals as in \"indirect\" methods. This revolutionized solving optimal control problems leading to widespread use of the Ross–Fahroo techniques.<ref name=\"Gong1\">Q. Gong, W. Kang, N. Bedrossian, F. Fahroo, P. Sekhavat and K. Bollino, Pseudospectral Optimal Control for Military and Industrial Applications, 46th IEEE Conference on Decision and Control, New Orleans, LA, pp. 4128–4142, Dec. 2007.</ref>\n\n==Software implementation==\nThe Ross–Fahroo methods are implemented in the MATLAB optimal control solver, [[DIDO (software)|DIDO]].\n\n==See also==\n*[[Bellman pseudospectral method]]\n*[http://www.mathworks.com/products/connections/product_detail/product_61633.html DIDO - MATLAB tool for optimal control] named after [[DIDO (optimal control)|Dido]], the first [[Dido (Queen of Carthage)|queen of Carthage]]\n*[[Ross' π lemma]]\n*[[Ross–Fahroo lemma]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Pseudospectral Optimal Control}}\n[[Category:Optimal control]]\n[[Category:Numerical analysis]]\n[[Category:Control theory]]"
    },
    {
      "title": "Ross' π lemma",
      "url": "https://en.wikipedia.org/wiki/Ross%27_%CF%80_lemma",
      "text": "'''Ross' {{pi}} lemma''', named after [[I. Michael Ross]],<ref name =\"mor-2005\" > B. S. Mordukhovich, Variational Analysis and Generalized Differentiation, I: Basic Theory, Vol. 330 of Grundlehren der\nMathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences] Series, Springer, Berlin, 2005. </ref><ref name = \"kang-2010\"> W. Kang, \"Rate of Convergence for the Legendre Pseudospectral Optimal Control of Feedback Linearizable Systems\", Journal of Control Theory and Application, Vol.8, No.4, 2010. pp. 391-405. </ref><ref name =\"li-2011\">  Jr-S Li, J. Ruths, T.-Y. Yu, H. Arthanari and G. Wagner, \"[http://www.pnas.org/content/108/5/1879.full Optimal Pulse Design in Quantum Control: A Unified Computational Method]\",  Proceedings of the National Academy of Sciences, Vol.108, No.5, Feb 2011, pp.1879-1884. </ref> is a result in computational [[optimal control]].  Based on generating [[Caratheodory-π solution|Carathéodory-{{pi}} solutions]] for [[feedback control]], Ross' {{pi}}-lemma states that there is fundamental [[time constant]] within which a control solution must be computed for [[controllability]] and [[stability theory|stability]].  This time constant, known as Ross' time constant,<ref name = \"IEEESpectrum-2012\"> N. Bedrossian,  M. Karpenko,  and S. Bhatt,\n\"[http://calhoun.nps.edu/bitstream/handle/10945/41128/Karpenko_Overclock_2012.pdf?sequence=1 Overclock My Satellite: Sophisticated Algorithms Boost Satellite Performance on the Cheap]\"\nIEEE Spectrum, November 2012. </ref><ref name =\"res-2008\"> R. E. Stevens and W. Wiesel, \"Large Time Scale Optimal Control of an Electrodynamic Tether Satellite\", Journal of Guidance, Control and Dynamics, Vol. 32, No. 6, pp. 1716–1727, 2008. </ref> is proportional to the inverse of the [[Lipschitz constant]] of the [[vector field]] that governs the dynamics of a [[nonlinear control system]].<ref name =\"Ross1\">I. M. Ross, P. Sekhavat, A. Fleming and Q. Gong, \"[http://www.elissarglobal.com/wp-content/uploads/2012/03/Optimal-Feedback-Control-Foundations-Examples-and-Experimental-Results.pdf Optimal Feedback Control: Foundations, Examples, and Experimental Results for a New Approach]\", ''Journal of Guidance, Control, and Dynamics'', vol. 31 no. 2, pp. 307–321, 2008.\n</ref><ref name =\"Ross2\">I. M. Ross, Q. Gong,  F. Fahroo, and W. Kang, \"[https://pdfs.semanticscholar.org/67b3/453d24cdce3dd00e07d7e7d64ac2efbf1522.pdf Practical Stabilization\nThrough Real-Time Optimal Control]\", 2006 American Control\nConference, [[IEEE|Inst. of Electrical and Electronics Engineers]], Piscataway,\nNJ, 14–16 June 2006.</ref>\n\n==Theoretical implications==\nThe proportionality factor in the definition of Ross' time constant is dependent upon the magnitude of the disturbance on the plant and the specifications for feedback control. When there are no disturbances, Ross' {{pi}}-lemma shows that the open-loop optimal solution is the same as the closed-loop one. In the presence of disturbances, the proportionality factor can be written in terms of the [[Lambert W-function]].\n\n==Practical applications==\nIn practical applications, Ross' time constant can be found by numerical experimentation using [[DIDO (optimal control)|DIDO]].  Ross ''et al'' showed that this time constant is connected to the practical implementation of a Caratheodory-{{pi}} solution.<ref name=\"Ross1\"/>  That is, Ross ''et al'' showed that if feedback solutions are obtained by [[zero-order hold]]s only, then a significantly faster [[sampling rate]] is needed to achieve controllability and stability.  On the other hand, if a feedback solution is implemented by way of a Caratheodory-{{pi}} technique, then a larger sampling rate can be accommodated.  This implies that the computational burden on generating feedback solutions is significantly less than the standard implementations. These concepts have been used to generate collision-avoidance manevuers in [[robotics]] in the presence of uncertain and incomplete information of the static and dynamic obstacles.<ref>\nM. Hurni, P. Sekhavat, and I. M. Ross, \"[http://ebooks.narotama.ac.id/files/Dynamics%20of%20Information%20(Systems%20Theory%20and%20Applications)/Chapter%2011%20An%20Info-Centric%20Trajectory%20Planner%20for%20Unmanned%20Ground%20Vehicles.pdf An Info-Centric Trajectory Planner for Unmanned Ground Vehicles]\", Chapter 11 in ''Dynamics of Information Systems: Theory and Applications'', Springer, 2010, pp. 213–232.</ref>\n\n==See also==\n*[[Ross–Fahroo lemma]]\n*[[Ross–Fahroo pseudospectral method]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Pseudospectral Optimal Control}}\n[[Category:Optimal control| ]]\n[[Category:Numerical analysis]]\n[[Category:Control theory]]\n\n[[ar:هندسة اتصالات]]\n[[de:Optimale Steuerung]]\n[[es:Control óptimo]]\n[[fr:Commande optimale]]\n[[it:Controllo ottimo]]\n[[he:בקרה אופטימלית]]\n[[ru:Оптимальное управление]]"
    },
    {
      "title": "Quantification of margins and uncertainties",
      "url": "https://en.wikipedia.org/wiki/Quantification_of_margins_and_uncertainties",
      "text": "{{underlinked|date=March 2016}}\n'''Quantification of Margins and Uncertainty (QMU)''' is a decision-support methodology for complex technical decisions.  QMU focuses on the identification, characterization, and analysis of performance thresholds and their associated margins for engineering systems that are evaluated under conditions of uncertainty, particularly when portions of those results are generated using computational modeling and simulation.<ref>{{cite web | url=http://prod.sandia.gov/techlib/access-control.cgi/2006/065001.pdf | title=Ideas Underlying Quantification of Margins and Uncertainties (QMU): A white paper | publisher=Sandia National Laboratories report SAND2006-5001 |author1=Martin Pilch |author2=Timothy G. Trucano |author3=Jon C. Helton  |last-author-amp=yes | date=September 2006 }}</ref> QMU has traditionally been applied to complex systems where comprehensive experimental test data is not readily available and cannot be easily generated for either end-to-end system execution or for specific subsystems of interest.  Examples of systems where QMU has been applied include nuclear weapons performance, qualification, and stockpile assessment.  QMU focuses on characterizing in detail the various sources of uncertainty that exist in a model, thus allowing the uncertainty in the system response output variables to be well quantified.  These sources are frequently described in terms of probability distributions to account for the stochastic nature of complex engineering systems.  The characterization of uncertainty supports comparisons of design margins for key system performance metrics to the uncertainty associated with their calculation by the model.  QMU supports risk-informed decision-making processes where computational simulation results provide one of several inputs to the decision-making authority.  There is currently no standardized methodology across the simulation community for conducting QMU;<ref>{{cite journal| url=http://www.fas.org/irp/agency/dod/jason/margins.pdf | title=Quantification of Margins and Uncertainties | publisher=JASON – The Mitre Corporation JASON report JSR-04-330| date=2005-03-25 | author=D. Eardley|display-authors=etal}}</ref> the term is applied to a variety of different modeling and simulation techniques that focus on rigorously quantifying model uncertainty in order to support comparison to design margins.\n\n==History==\n\nThe fundamental concepts of QMU were originally developed concurrently at several national laboratories supporting nuclear weapons programs in the late 1990s, including [[Lawrence Livermore National Laboratory]], [[Sandia National Laboratory]], and [[Los Alamos National Laboratory]].  The original focus of the methodology was to support nuclear stockpile decision-making, an area where full experimental test data could no longer be generated for validation due to bans on nuclear weapons testing.<ref>{{cite journal | url=http://www.fas.org/sgp/othergov/doe/lanl/pubs/las28/sharp.pdf | title=QMU and Nuclear Weapons Certification—What’s under the Hood?  |author1=David H. Sharp  |author2=Merri M. Wood-Schultz  |lastauthoramp=yes | journal=[[Los Alamos Science]] | volume=28 | pages = 47–53 |year=2003}}</ref> The methodology has since been applied in other applications where safety or mission critical decisions for complex projects must be made using results based on modeling and simulation.  Examples outside of the nuclear weapons field include applications at NASA for interplanetary spacecraft and rover development,<ref>{{cite journal | url= http://www.kiss.caltech.edu/workshops/xterramechanics2011/presentations/peterson.pdf | title=Quantification of Margins and Uncertainty (QMU): Turning Models and Test Data into Mission Confidence  | author=Lee Peterson | journal= Keck Institute for Space xTerraMechanics Workshop|date=23 June 2011}}</ref> missile six-degree-of-freedom (6DOF) simulation results,<ref>{{cite journal | title=Estimation of Total Uncertainty in Modeling and Simulation | author=William L. Oberkampf| journal= Sandia Report SAND2000-0824 |date=April 2000|display-authors=etal}}</ref> and characterization of material properties in terminal ballistic encounters.<ref>{{cite journal | url= http://www.rosakis.caltech.edu/publications/pdf/2012/182_Rigorous_Model-Based.pdf  | title= Rigorous model-based uncertainty quantification with application to terminal ballistics, part I: Systems with controllable inputs and small scatter | author=A. Kidane| journal=Journal of the Mechanics of Physics and Solids | volume=60 | pages = 983–1001 |year=2012 | doi=10.1016/j.jmps.2011.12.001|display-authors=etal}}</ref>\n\n==Overview==\n\nQMU focuses on quantification of the ratio of design margin to model output uncertainty.  The process begins with the identification of the key performance thresholds for the system, which can frequently be found in the systems requirements documents.  These thresholds (also referred to as performance gates) can specify an upper bound of performance, a lower bound of performance, or both in the case where the metric must remain within the specified range.  For each of these performance thresholds, the associated performance margin must be identified.  The margin represents the targeted range the system is being designed to operate in to safely avoid the upper and lower performance bounds.  These margins account for aspects such as the design safety factor the system is being developed to as well as the confidence level in that safety factor.  QMU focuses on determining the quantified uncertainty of the simulation results as they relate to the performance threshold margins.  This total uncertainty includes all forms of uncertainty related to the computational model as well as the uncertainty in the threshold and margin values.  The identification and characterization of these values allows the ratios of margin-to-uncertainty (M/U) to be calculated for the system.  These M/U values can serve as quantified inputs that can help authorities make risk-informed decisions regarding how to interpret and act upon results based on simulations.\n\n[[File:QMU Diagram.tif|thumb|center|upright=3.5|alt=General Overview of QMU Process.|Overview of General QMU Process.]]\n\nQMU recognizes that there are multiple types of uncertainty that propagate through a model of a complex system.  The simulation in the QMU process produces output results for the key performance thresholds of interest, known as the Best Estimate Plus Uncertainty (BE+U).  The best estimate component of BE+U represents the core information that is known and understood about the model response variables.  The basis that allows high confidence in these estimates is usually ample experimental test data regarding the process of interest which allows the simulation model to be thoroughly validated.\n\nThe types of uncertainty that contribute to the value of the BE+U can be broken down into several categories:<ref>{{cite journal| url=http://prod.sandia.gov/techlib/access-control.cgi/2009/093055.pdf| title=Conceptual and Computational Basis for the Quantification of Margins and Uncertainty | publisher=Sandia National Laboratories technical report SAND2009-3055| year=2009 | author=Jon C. Helton|display-authors=etal}}</ref>\n\n* ''Aleatory uncertainty'': This type of uncertainty is naturally present in the system being modeled and is sometimes known as “irreducible uncertainty” and “stochastic variability.”  Examples include processes that are naturally stochastic such as wind gust parameters and manufacturing tolerances.\n* ''Epistemic uncertainty'': This type of uncertainty is due to a lack of knowledge about the system being modeled and is also known as “reducible uncertainty.”  Epistemic uncertainty can result from uncertainty about the correct underlying equations of the model, incomplete knowledge of the full set of scenarios to be encountered, and lack of experimental test data defining the key model input parameters.\n\nThe system may also suffer from requirements uncertainty related to the specified thresholds and margins associated with the system requirements.  QMU acknowledges that in some situations, the system designer may have high confidence in what the correct value for a specific metric may be, while at other times, the selected value may itself suffer from uncertainty due to lack of experience operating in this particular regime.  QMU attempts to separate these uncertainty values and quantify each of them as part of the overall inputs to the process.\n\nQMU can also factor in human error in the ability to identify the unknown unknowns that can affect a system.  These errors can be quantified to some degree by looking at the limited experimental data that may be available for previous system tests and identifying what percentage of tests resulted in system thresholds being exceeded in an unexpected manner.  This approach attempts to predict future events based on the past occurrences of unexpected outcomes.\n\nThe underlying parameters that serve as inputs to the models are frequently modeled as samples from a probability distribution.  The input parameter model distributions as well as the model propagation equations determine the distribution of the output parameter values.  The distribution of a specific output value must be considered when determining what is an acceptable M/U ratio for that performance variable.  If the uncertainty limit for U includes a finite upper bound due to the particular distribution of that variable, a lower M/U ratio may be acceptable.  However, if U is modeled as a normal or exponential distribution which can potentially include outliers from the far tails of the distribution, a larger value may be required in order to reduce system risk to an acceptable level.\n\nRatios of acceptable M/U for safety critical systems can vary from application to application.  Studies have cited acceptable M/U ratios as being in the 2:1 to 10:1 range for nuclear weapons stockpile decision-making.  Intuitively, the larger the value of M/U, the less of the available performance margin is being consumed by uncertainty in the simulation outputs.  A ratio of 1:1 could result in a simulation run where the simulated performance threshold is not exceeded when in actuality the entire design margin may have been consumed. It is important to note that rigorous QMU does not ensure that the system itself is capable of meeting its performance margin; rather, it serves to ensure that the decision-making authority can make judgments based on accurately characterized results.\n\nThe underlying objective of QMU is to present information to decision-makers that fully characterizes the results in light of the uncertainty as understood by the model developers.  This presentation of results allows decision makers an opportunity to make informed decisions while understanding what sensitivities exist in the results due to the current understanding of uncertainty.  Advocates of QMU recognize that decisions for complex systems cannot be made strictly based on the quantified M/U metrics.  Subject matter expert (SME) judgment and other external factors such as stakeholder opinions and regulatory issues must also be considered by the decision-making authority before a final outcome is decided.<ref>{{cite journal| title= Probabilistic Risk Assessment Practices in the USA for Nuclear Power Plants | journal= Safety Science | volume=40| pages = 177–201 |year=2002 |author1=B.J. Garrick  |author2=R.F. Christie  |lastauthoramp=yes | doi=10.1016/s0925-7535(01)00036-4}}</ref>\n\n==Verification and validation==\n[[Verification and validation]] (V&nbsp;&&nbsp;V) of a model is closely interrelated with QMU.<ref>{{cite journal | title=Assessing the Reliability of Complex Models | author=National Research Council of the National Academies |year=2012}}</ref>  Verification is broadly acknowledged as the process of determining if a model was built correctly; validation activities focus on determining if the correct model was built.<ref>{{cite journal| url=http://prod.sandia.gov/techlib/access-control.cgi/2003/033769.pdf| title= Verification, Validation, and Predictive Capability in Computational Engineering and Physics | journal= Applied Mechanics Reviews | volume=57| number= 5| pages = 345–384 |year=2004 |author1=W. L. Oberkampf |author2=T. G. Trucano |author3=C. Hirsch  |last-author-amp=yes | doi=10.1115/1.1767847}}</ref>  V&V against available experimental test data is an important aspect of accurately characterizing the overall uncertainty of the system response variables.  V&V seeks to make maximum use of component and subsystem-level experimental test data to accurately characterize model input parameters and the physics-based models associated with particular sub-elements of the system.  The use of QMU in the simulation process helps to ensure that the stochastic nature of the input variables (due to both aleatory and epistemic uncertainties) as well as the underlying uncertainty in the model are properly accounted for when determining the simulation runs required to establish model credibility prior to accreditation.\n\n==Advantages and disadvantages==\nQMU has the potential to support improved decision-making for programs that must rely heavily on modeling and simulation.  Modeling and simulation results are being used more often during the acquisition, development, design, and testing of complex engineering systems.<ref>{{cite journal| url= https://www.nsf.gov/pubs/reports/sbes_final_report.pdf| title= Simulation-Based Engineering Science: Revolutionizing Engineering Science Through Simulation. | journal=National Science Foundation Technical Report| year=2006 | author= Blue Ribbon Panel on Simulation-based Engineering Science}}</ref>  One of the major challenges of developing simulations is to know how much fidelity should be built into each element of the model.  The pursuit of higher fidelity can significantly increase development time and total cost of the simulation development effort.  QMU provides a formal method for describing the required fidelity relative to the design threshold margins for key performance variables.  This information can also be used to prioritize areas of future investment for the simulation.  Analysis of the various M/U ratios for the key performance variables can help identify model components that are in need of fidelity upgrades to order to increase simulation effectiveness.\n\nA variety of potential issues related to the use of QMU have also been identified.  QMU can lead to longer development schedules and increased development costs relative to traditional simulation projects due to the additional rigor being applied.  Proponents of QMU state that the level of uncertainty quantification required is driven by certification requirements for the intended application of the simulation.  Simulations used for capability planning or system trade analyses must generally model the overall performance trends of the systems and components being analyzed.  However, for safety-critical systems where experimental test data is lacking, simulation results provide a critical input to the decision-making process.  Another potential risk related to the use of QMU is a false sense of confidence regarding protection from unknown risks.  The use of quantified results for key simulation parameters can lead decision makers to believe all possible risks have been fully accounted for, which is particularly challenging for complex systems.  Proponents of QMU advocate for a risk-informed decision-making process to counter this risk; in this paradigm, M/U results as well as SME judgment and other external factors are always factored into the final decision.\n\n==See also==\n* [[Uncertainty quantification]]\n* [[Sandia National Laboratory]]\n* [[Los Alamos National Laboratory]]\n* [[Lawrence Livermore National Laboratory]]\n* [[Verification and Validation]]\n\n==References==\n<references />\n\n[[Category:Nuclear stockpile stewardship]]\n[[Category:Numerical analysis]]\n[[Category:Decision-making]]"
    },
    {
      "title": "Radial basis function",
      "url": "https://en.wikipedia.org/wiki/Radial_basis_function",
      "text": "A '''radial basis function''' ('''RBF''') is a [[real-valued function]] <math display=\"inline\">\\varphi</math> whose value depends only on the distance from the [[Origin (mathematics)|origin]], so that <math display=\"inline\">\\varphi(\\mathbf{x}) = \\varphi(\\left\\|\\mathbf{x}\\right\\|)</math>; or alternatively on the distance from some other point <math display=\"inline\">\\mathbf{c}</math>, called a ''center'', so that <math display=\"inline\">\\varphi(\\mathbf{x}, \\mathbf{c}) = \\varphi(\\left\\|\\mathbf{x}-\\mathbf{c}\\right\\|)</math>. Any function <math display=\"inline\">\\varphi</math> that satisfies the property <math display=\"inline\">\\varphi(\\mathbf{x}) = \\varphi(\\left\\|\\mathbf{x}\\right\\|)</math> is a [[radial function]]. The norm is usually [[Euclidean distance]], although other [[distance function]]s are also possible.\n\nSums of radial basis functions are typically used to [[function approximation|approximate given functions]]. This approximation process can also be interpreted as a simple kind of [[artificial neural network|neural network]]; this was the context in which they originally surfaced, in work by [[David Broomhead]] and David Lowe in 1988,<ref>[http://www.anc.ed.ac.uk/rbf/intro/node8.html Radial Basis Function networks] {{webarchive|url=https://web.archive.org/web/20140423232029/http://www.anc.ed.ac.uk/rbf/intro/node8.html |date=2014-04-23 }}</ref><ref>{{cite journal |first = David H. |last = Broomhead |first2 = David |last2 = Lowe |title = Multivariable Functional Interpolation and Adaptive Networks |journal = Complex Systems |volume = 2 |pages = 321–355 |year = 1988 |url = https://www.complex-systems.com/pdf/02-3-5.pdf |archiveurl = https://web.archive.org/web/20140714173428/https://www.complex-systems.com/pdf/02-3-5.pdf |archivedate = 2014-07-14 |ref = harv }}</ref> which stemmed from [[Michael J. D. Powell]]'s seminal research from 1977.<ref>{{cite journal |title = Restart procedures for the conjugate gradient method |author = [[Michael J. D. Powell]] |journal = [[Mathematical Programming]] |volume = 12 |number = 1 |pages = 241–254 |year = 1977 |url = https://link.springer.com/content/pdf/10.1007/BF01593790.pdf |doi=10.1007/bf01593790}}</ref><ref>{{cite thesis |type = M.Sc. |first = Ferat |last = Sahin |title = A Radial Basis Function Approach to a Color Image Classification Problem in a Real Time Industrial Application |publisher = [[Virginia Tech]] |year = 1997 |quote = Radial basis functions were first introduced by Powell to solve the real multivariate interpolation problem. |page = 26 |url = http://scholar.lib.vt.edu/theses/available/etd-6197-223641/unrestricted/Ch3.pdf }}</ref><ref name=\"CITEREFBroomheadLowe1988\">{{Harvnb|Broomhead|Lowe|1988|p=347}}: \"We would like to thank Professor M.J.D. Powell at the Department of Applied Mathematics and Theoretical Physics at Cambridge University for providing the initial stimulus for this work.\"</ref><!--this doesn't seem to be working, probably a bug with {{sfn}}: <ref>{{sfn|Broomhead|Lowe|1988|p=347}}: \"We would like to thank Professor M.J.D. Powell at the Department of Applied Mathematics and Theoretical Physics at Cambridge University for providing the initial stimulus for this work.\"</ref>-->\nRBFs are also used as a [[Radial basis function kernel|kernel]] in [[support vector machine|support vector classification]].<ref>{{cite web |url=https://beta.oreilly.com/learning/intro-to-svm |title=Introduction to Support Vector Machines |last=VanderPlas |first=Jake |publisher=[O'Reilly] |date=6 May 2015 |website= |access-date=14 May 2015}}</ref> The RBF implementation is ready enough to have the RBF exploited in different engineering applications.<ref>{{Cite book|title=Radial basis functions : theory and implementations|first=Martin Dietrich|last=Buhmann|date=2003|publisher=Cambridge University Press|isbn=978-0511040207|oclc=56352083}}</ref><ref>{{Cite book|title=Fast radial basis functions for engineering applications|last=Biancolini|first=Marco Evangelos|date=2018|isbn=9783319750118|publisher=Springer International Publishing|oclc=1030746230}}</ref>\n\n== Definition ==\nA radial function is a function <math display=\"inline\">\\varphi:[0,\\infty) \\to \\mathbb{R}</math>. When paired with a metric on a vector space <math display=\"inline\"> \\|\\cdot\\|:V \\to [0,\\infty)</math> a function <math display=\"inline\"> \\varphi_\\mathbf{c} = \\varphi(\\|\\mathbf{x}-\\mathbf{c}\\|) </math> is said to be a radial kernel centered at <math display=\"inline\"> \\mathbf{c} </math>. A Radial function and the associated radial kernels are said to be radial basis functions if, for any set of nodes <math>\\{\\mathbf{x}_k\\}_{k=1}^n</math>\n\n* The kernels <math>\\varphi_{\\mathbf{x}_1}, \\varphi_{\\mathbf{x}_2}, \\dots, \\varphi_{\\mathbf{x}_n}</math> are linearly independent (for example <math>\\varphi(r)=r^2</math> in <math>V=\\mathbb{R}</math> is not a radial basis function)\n\n* The kernels <math>\\varphi_{\\mathbf{x}_1}, \\varphi_{\\mathbf{x}_2}, \\dots, \\varphi_{\\mathbf{x}_n}</math> form a basis for a Haar Space, meaning that the [[radial basis function interpolation|interpolation matrix]]\n<math display=\"block\">\n\\begin{bmatrix}\n\\varphi(\\|\\mathbf{x}_1 - \\mathbf{x}_1\\|) & \\varphi(\\|\\mathbf{x}_2 - \\mathbf{x}_1\\|) & \\dots & \\varphi(\\|\\mathbf{x}_n - \\mathbf{x}_1\\|) \\\\\n\\varphi(\\|\\mathbf{x}_1 - \\mathbf{x}_2\\|) & \\varphi(\\|\\mathbf{x}_2 - \\mathbf{x}_2\\|) & \\dots & \\varphi(\\|\\mathbf{x}_n - \\mathbf{x}_2\\|) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\varphi(\\|\\mathbf{x}_1 - \\mathbf{x}_n\\|) & \\varphi(\\|\\mathbf{x}_2 - \\mathbf{x}_n\\|) & \\dots & \\varphi(\\|\\mathbf{x}_n - \\mathbf{x}_n\\|) \\\\\n\\end{bmatrix}\n</math>\nis nonsingular. \n<ref>{{cite book |last1=Fasshauer |first1=Gregory E. |title=Meshfree Approximation Methods with MATLAB |date=2007 |publisher=World Scientific Publishing Co. Pte. Ltd. |location=Singapore |isbn=9789812706331 |pages=17-25}}</ref>\n<ref name=\"wendland2005\">{{cite book |last1=Wendland |first1=Holger |title=Scattered Data Approximation |date=2005 |publisher=Cambridge University Press |location=Cambridge |isbn=0521843359 |pages=11, 18-23,64-66}}</ref>\n\n=== Examples ===\nCommonly used types of radial basis functions include (writing <math display=\"inline\">r = \\left\\|\\mathbf{x} - \\mathbf{x}_i\\right\\|</math> and using <math display=\"inline\">\\varepsilon </math> to indicate a ''shape parameter'' that can be used to scale the input of the radial kernel<ref>{{cite book |last1=Fasshauer |first1=Gregory E. |title=Meshfree Approximation Methods with MATLAB |date=2007 |publisher=World Scientific Publishing Co. Pte. Ltd. |location=Singapore |isbn=9789812706331 |page=37}}</ref>):\n\n* Infinitely Smooth RBFs\nThese radial basis functions are from <math>C^\\infty(\\mathbb{R})</math> and are strictly positive definite functions<ref>{{cite book |last1=Fasshauer |first1=Gregory E. |title=Meshfree Approximation Methods with MATLAB |date=2007 |publisher=World Scientific Publishing Co. Pte. Ltd. |location=Singapore |isbn=9789812706331 |pages=37-45}}</ref> that require tuning a shape parameter <math>\\varepsilon</math>\n:* [[Gaussian function|Gaussian]]:<math display=\"block\">\\varphi(r) = e^{-(\\varepsilon r)^2}</math>\n[[File:Gaussian function shape parameter.png|thumb|right|A [[Gaussian function]] for several choices of <math>\\varepsilon</math>.]]\n[[File:Bump function shape.png|thumb|A plot of the scaled [[Bump function]] with several choices of <math>\\varepsilon</math>.]]\n\n:* [[Multiquadric]]:<math display=\"block\">\\varphi(r) = \\sqrt{1 + (\\varepsilon r)^2} </math>\n\n:* [[Inverse quadratic]]:<math display=\"block\">\\varphi(r) = \\dfrac{1}{1+(\\varepsilon r)^2} </math>\n\n:* [[Inverse multiquadric]]:<math display=\"block\">\\varphi(r) = \\dfrac{1}{\\sqrt{1 + (\\varepsilon r)^2}} </math>\n\n* [[Polyharmonic spline]]:<math display=\"block\">\\begin{aligned}\n\\varphi(r) &= r^k,& k&=1,3,5,\\dotsc\n\\\\\n\\varphi(r) &= r^k \\ln(r),& k&=2,4,6,\\dotsc\n\\end{aligned}</math>''*For even-degree polyharmonic splines'' <math>(k = 2,4,6,\\dotsc)</math>'', to avoid numerical problems at <math>r = 0</math> where <math>\\ln(0) = -\\infty</math>, the computational implementation is often written as <math>\\varphi(r) = r^{k-1}\\ln(r^r)</math>.''\n\n* [[Thin plate spline]] (a special polyharmonic spline):<math display=\"block\">\\varphi(r) = r^2 \\ln(r)</math>\n\n* Compactly Supported RBFs\nThese RBFs are compactly supported and thus are non-zero only within a radius of <math>1/\\varepsilon</math>, and thus have sparse differentiation matrices\n\n:* [[Bump function]]:\n<math display=\"block\">\\varphi(r) = \n\\begin{cases}\n\\exp\\left( -\\frac{1}{1 - (\\varepsilon r)^2}\\right) & \\mbox{ for } r<\\frac{1}{\\varepsilon} \\\\\n0 & \\mbox{ otherwise} \n\\end{cases}\n</math>\n\n\n==Approximation==\n{{main|Kernel smoothing}}\n{{main|Radial basis function interpolation}}\n\nRadial basis functions are typically used to build up [[function approximation]]s of the form<math display=\"block\">y(\\mathbf{x}) = \\sum_{i=1}^N w_i \\, \\varphi(\\left\\|\\mathbf{x} - \\mathbf{x}_i\\right\\|),</math>\n\nwhere the approximating function <math display=\"inline\">y(\\mathbf{x})</math> is represented as a sum of <math>N</math> radial basis functions, each associated with a different center <math display=\"inline\">\\mathbf{x}_i</math>, and weighted by an appropriate coefficient <math display=\"inline\">w_i.</math> The weights <math display=\"inline\">w_i</math> can be estimated using the matrix methods of [[Weighted least squares|linear least squares]], because the approximating function is ''linear'' in the weights ''<math display=\"inline\">w_i</math>''.\n\nApproximation schemes of this kind have been particularly used{{citation needed|date=July 2013}} in [[time series prediction]] and [[Control theory|control]] of [[nonlinear systems]] exhibiting sufficiently simple [[chaos theory|chaotic]] behaviour, 3D reconstruction in [[computer graphics]] (for example, [[hierarchical RBF]] and [[Pose Space Deformation]]).\n\n==RBF Network==\n{{main|radial basis function network}}\n\n[[File:Unnormalized radial basis functions.svg|thumb|350px|right|Two unnormalized Gaussian radial basis functions in one input dimension. The basis function centers are located at <math display=\"inline\">x_1 = 0.75</math> and <math display=\"inline\">x_2 = 3.25</math>.]]\n\nThe sum<math display=\"block\">y(\\mathbf{x}) = \\sum_{i=1}^N w_i \\, \\varphi(\\left\\|\\mathbf{x} - \\mathbf{x}_i\\right\\|),</math>can also be interpreted as a rather simple single-layer type of [[artificial neural network]] called a [[radial basis function network]], with the radial basis functions taking on the role of the activation functions of the network.  It can be shown that any continuous function on a [[Compact space|compact]] interval can in principle be interpolated with arbitrary accuracy by a sum of this form, if a sufficiently large number ''<math display=\"inline\">N</math>'' of radial basis functions is used. \n\nThe approximant <math display=\"inline\">y(\\mathbf{x})</math> is differentiable with respect to the weights ''<math display=\"inline\">w_i</math>''. The weights could thus be learned using any of the standard iterative methods for neural networks.\n\nUsing radial basis functions in this manner yields a reasonable interpolation approach provided that the fitting set has been chosen such that it covers the entire range systematically (equidistant data points are ideal). However, without a polynomial term that is orthogonal to the radial basis functions, estimates outside the fitting set tend to perform poorly. {{citation needed|date=February 2019}}\n\n==See also==\n* [[Matérn covariance function]]\n* [[Radial basis function interpolation]]\n\n== References ==\n{{Reflist}}\n\n== Further reading ==\n{{more footnotes|date=June 2013}}\n*{{cite journal|last1=Hardy|first1=R.L.|year=1971|title=Multiquadric equations of topography and other irregular surfaces|url=|journal=Journal of Geophysical Research|volume=76|issue=8|pages=1905–1915|doi=10.1029/jb076i008p01905|bibcode=1971JGR....76.1905H}}\n* {{cite journal | last1 = Hardy | first1 = R.L. | year = 1990 | title = Theory and applications of the multiquadric-biharmonic method, 20 years of Discovery, 1968 1988 | url = | journal = Comp. Math Applic | volume = 19 | issue = 8/9| pages = 163–208 | doi=10.1016/0898-1221(90)90272-l}}\n* {{Citation |last1 = Press |first1 = WH |last2 = Teukolsky |first2 = SA |last3 = Vetterling |first3 = WT |last4 = Flannery |first4 = BP |year = 2007 |title = Numerical Recipes: The Art of Scientific Computing |edition = 3rd |publisher = Cambridge University Press| location=New York |isbn = 978-0-521-88068-8 |chapter = Section 3.7.1. Radial Basis Function Interpolation|chapter-url=http://apps.nrbook.com/empanel/index.html?pg=139 }}\n* Sirayanone, S., 1988, Comparative studies of kriging, multiquadric-biharmonic, and other methods for solving mineral resource problems, PhD. Dissertation, Dept. of Earth Sciences, Iowa State University, Ames, Iowa.\n* {{cite journal | last1 = Sirayanone | first1 = S. | last2 = Hardy | first2 = R.L. | year = 1995 | title = The Multiquadric-biharmonic Method as Used for Mineral Resources, Meteorological, and Other Applications | url = | journal = Journal of Applied Sciences and Computations | volume = 1 | issue = | pages = 437–475 }}\n\n[[Category:Artificial neural networks]]\n[[Category:Interpolation]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Rate of convergence",
      "url": "https://en.wikipedia.org/wiki/Rate_of_convergence",
      "text": "{{cleanup|reason=There are no references in-text. Convergence order uses ''p'' and ''q''. <math>e_\\text{new}</math> and <math>e_\\text{old}</math> are not introduced.|date=February 2017}}\nIn [[numerical analysis]], the speed at which a [[limit of a sequence|convergent sequence]] approaches its limit is called the '''rate of convergence'''. Although strictly speaking,  a limit does not give information about any finite first part of the sequence, the concept of rate of convergence is of practical importance when working with a sequence of successive approximations for an [[iterative method]], as then typically fewer iterations are needed to yield a useful approximation if the rate of convergence is higher. This may even make the difference between needing ten or a million iterations.\n\nSimilar concepts are used for [[discretization]] methods. The solution of the discretized problem converges to the solution of the continuous problem as the grid size goes to zero, and the speed of convergence is one of the factors of the efficiency of the method. However, the terminology in this case is different from the terminology for iterative methods.\n\n[[Series acceleration]] is a collection of techniques for improving the rate of convergence of a series discretization. Such acceleration is commonly accomplished with [[sequence transformation]]s.\n\n==Convergence speed for iterative methods==\n\n===Basic definition===\n\nSuppose that the [[sequence]] <math>(x_k)_k</math> converges to the number <math>L</math>.\n\nThis sequence is said to ''converge linearly'' to <math>L</math>, if there exists a number <math>\\mu \\in (0, 1)</math> such that\n:<math> \\lim_{k \\to \\infty} \\frac{|x_{k+1} - L|}{|x_k - L|} = \\mu.</math>\n\nThe number <math>\\mu</math> is called the ''rate of convergence''.\n\nIf the sequence converges, and\n* <math>\\mu_k</math> varies from step to step with <math>\\mu_k \\to 0</math> for <math>k \\to \\infty</math>, then the sequence is said to ''converge superlinearly''.\n* <math>\\mu_k</math> varies from step to step with <math>\\mu_k \\to 1</math> for <math>k \\to \\infty</math>, then the sequence is said to ''converge sublinearly''.\n\nIf the sequence ''converges sublinearly'' and additionally \n:<math>\\lim_{k \\to \\infty} \\frac{|x_{k+2} - x_{k+1}|}{|x_{k+1} - x_k|} = 1,</math>\n\nthen it is said that the sequence <math>(x_k)_k</math> ''converges logarithmically'' to <math>L</math>.\n\n{{anchor|quadratic convergence|cubic convergence}}\nThe next definition is used to distinguish superlinear rates of convergence. The sequence ''converges with order <math>q</math> to <math>L</math>'' for <math>q > 1</math><ref><math>q</math> may be non-integer. For example, the [[secant method]] has, in the case of convergence to a regular, simple root, convergence order [[golden ratio|φ]] ≈ 1.618.</ref> if \n:<math>\\lim_{k \\to \\infty} \\frac{|x_{k+1} - L|}{|x_k - L|^q} < M</math>\nfor some positive constant <math>M</math> (not necessarily less than 1). In particular, convergence with order\n* <math>q = 2</math> is called ''quadratic convergence'',\n* <math>q = 3</math> is called ''cubic convergence'',\n* etc.\n\nThis is sometimes called ''Q-linear convergence'', ''Q-quadratic convergence'', etc., to distinguish it from the definition below. The Q stands for \"quotient\", because the definition uses the quotient between two successive terms. A sequence that has a quadratic convergence implies that it has a superlinear rates of convergence.\n\nA practical method to calculate the order of convergence for a sequence is to calculate the following sequence, which is converging to <math>q</math>\n\n:<math>q \\approx \\frac{\\log \\left|\\frac{x_{n+1} - x_n}{x_n - x_{n-1}}\\right|}{\\log \\left|\\frac{x_n - x_{n-1}}{x_{n-1} - x_{n-2}}\\right|}.</math>\n\n===Extended definition===\n\nThe drawback of the above definitions is that these do not catch some sequences which still converge reasonably fast, but whose rate is variable, such as the sequence <math> \\left( b_k\\right)_k  </math> below. \nTherefore, the definition of rate of convergence is sometimes extended as follows.\n\nUnder the new definition, the sequence <math> (x_k)_k  </math> converges with at least order <math> q </math> if there exists a sequence <math> (\\varepsilon_k)_k  </math> such that\n:<math>\n|x_k - L|\\le\\varepsilon_k\\quad\\text{for all }k \\,,\n</math>\nand the sequence <math> (\\varepsilon_k)_k  </math> converges to zero with order <math> q  </math> according to the above \"simple\" definition. To distinguish it from that definition, this is sometimes called ''R-linear convergence'', ''R-quadratic convergence'', etc. (with the R standing for \"root\").\n\n===Examples===\n\nConsider the following sequences:\n:<math>\\begin{align}\n  a_0 &= 1 ,\\,       &&a_1 = \\frac12 ,\\, &&a_2 = \\frac14 ,\\,    &&a_3 = \\frac18 ,\\,     &&a_4 = \\frac1{16} ,\\,    &&a_5 = \\frac1{32} ,\\, &&\\ldots ,\\, &&a_k = \\frac1{2^k} ,\\,                           &&\\ldots \\\\\n  b_0 &= 1 ,\\,       &&b_1 = 1 ,\\,  &&b_2 = \\frac14 ,\\,    &&b_3 = \\frac14 ,\\,     &&b_4 = \\frac1{16} ,\\,    &&b_5 = \\frac1{16} ,\\, &&\\ldots ,\\, &&b_k = \\frac1{4^{\\left\\lfloor \\frac{k}{2} \\right\\rfloor}} ,\\, &&\\ldots \\\\\n  c_0 &= \\frac12 ,\\, &&c_1 = \\frac14 ,\\, &&c_2 = \\frac1{16} ,\\, &&c_3 = \\frac1{256} ,\\, &&c_4 = \\frac1{65\\,536} ,\\,                      &&&&\\ldots ,\\, &&c_k = \\frac1{2^{2^k}} ,\\,                       &&\\ldots \\\\\n  d_0 &= 1 ,\\,       &&d_1 = \\frac12 ,\\, &&d_2 = \\frac13 ,\\,    &&d_3 = \\frac14 ,\\,     &&d_4 = \\frac15 ,\\,       &&d_5 = \\frac16 ,\\,    &&\\ldots ,\\, &&d_k = \\frac1{k+1} ,\\,                           &&\\ldots\n\\end{align}</math>\n\nThe sequence {''a''<sub>''k''</sub>} converges linearly to 0 with rate 1/2. More generally, the sequence ''Cμ''<sup>''k''</sup> converges linearly with rate μ if |μ| < 1. The sequence {''b''<sub>''k''</sub>} also converges linearly to 0 with rate 1/2 under the extended definition, but not under the simple definition. The sequence {''c''<sub>''k''</sub>} converges superlinearly. In fact, it is quadratically convergent. Finally, the sequence {''d''<sub>''k''</sub>} converges sublinearly and logarithmically.\n[[Image:ConvergencePlots.png|thumb|alt=Plot showing the different rates of convergence for the sequences ''a''<sub>''k''</sub>, ''b''<sub>''k''</sub>, ''c''<sub>''k''</sub> and ''d''<sub>''k''</sub>.|Linear, linear, superlinear (quadratic), and sublinear rates of convergence|400px|center]]\n\n==Convergence speed for discretization methods==\n\nA similar situation exists for discretization methods. The important parameter here for the convergence speed is not the iteration number ''k'', but the number of grid points and grid spacing. In this case, the number of grid points ''n'' in a discretization process is inversely proportional to the grid spacing.\n\nIn this case, a sequence <math>x_n</math> is said to converge to ''L'' with order ''p'' if there exists a constant ''C'' such that\n:<math> |x_n - L| < C n^{-p} \\text{ for all } n. </math>\nThis is written as <math>|x_n - L | = \\mathcal{O}(n^{-p})</math> using [[big O notation]].\n\nThis is the relevant definition when discussing methods for [[numerical quadrature]] or the [[numerical ordinary differential equations|solution of ordinary differential equations]].\nA practical method to calculate the rate of convergence for a discretization method is to implement the following formula:\n\n:<math>p \\approx \\frac{\\log(e_\\text{new}/e_\\text{old})}{\\log(h_\\text{new}/h_\\text{old})},</math>\n\nwhere <math>e_\\text{new}</math> and <math>e_\\text{old}</math> denote the errors w.r.t. the new and old step sizes <math>h_\\text{new}</math> and <math>h_\\text{old}</math> respectively.\n\n=== Examples (continued)===\n\nThe sequence {''d''<sub>''k''</sub>} with ''d''<sub>''k''</sub> = 1 / (''k'' + 1) was introduced above. This sequence converges with order 1 according to the convention for discretization methods.\n\nThe sequence {''a''<sub>''k''</sub>} with ''a''<sub>''k''</sub> = 2<sup>−''k''</sup>, which was also introduced above, converges with order ''p'' for every number ''p''. It is said to converge exponentially using the convention for discretization methods. However, it only converges linearly (that is, with order 1) using the convention for iterative methods.\n\nThe order of convergence of a discretization method is related to its [[Truncation error (numerical integration)|global truncation error (GTE)]].\n\n==Acceleration of convergence==\nMany methods exist to increase the rate of convergence of a given sequence, \ni.e. to [[sequence transformation|transform a given sequence]] into one converging faster to the same limit.  Such techniques are in general known as \"[[series acceleration]]\". The goal of the transformed sequence is to reduce the [[computational cost]] of the calculation. One example of series acceleration is [[Aitken's delta-squared process]].\n\n==References==\n<references />\n\n==Literature==\nThe simple definition is used in\n* [[Michelle Schatzman]] (2002), ''Numerical analysis: a mathematical introduction'', Clarendon Press, Oxford. {{isbn|0-19-850279-6}}.\n\nThe extended definition is used in\n*http://web.mit.edu/rudin/www/MukherjeeRuSc11COLT.pdf\n* Walter Gautschi (1997), ''Numerical analysis: an introduction,'' Birkhäuser, Boston. {{isbn|0-8176-3895-4}}.\n* [[Endre Süli]] and David Mayers (2003), ''An introduction to numerical analysis,'' Cambridge University Press. {{isbn|0-521-00794-1}}.\n\nLogarithmic convergence is used in\n* {{cite journal | last = Van Tuyl | first = Andrew H. | year = 1994 | title = Acceleration of convergence of a family of logarithmically convergent sequences | journal = Mathematics of Computation | volume = 63 | issue = 207 | pages = 229–246 | url = http://www.ams.org/journals/mcom/1994-63-207/S0025-5718-1994-1234428-2/S0025-5718-1994-1234428-2.pdf | doi=10.2307/2153571}}\n\nThe Big O definition is used in\n*Richard L. Burden and J. Douglas Faires (2001), ''Numerical Analysis'' (7th ed.), Brooks/Cole. {{isbn|0-534-38216-9}}\n\nThe terms ''Q-linear'' and ''R-linear'' are used in; The Big O definition when using Taylor series is used in\n* {{Cite book | last1=Nocedal | first1=Jorge | last2=Wright | first2=Stephen J. | title=Numerical Optimization | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=2nd | isbn=978-0-387-30303-1 | year=2006 | pages=619+620 | postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}}}.\n\nOne may also study the following paper for Q-linear and R-linear: \n* {{cite journal | last = Potra | first = F. A. | year = 1989 | title = On Q-order and R-order of convergence | journal = J. Optim. Th. Appl. | volume = 63 | issue = 3 | pages = 415–431 | doi = 10.1007/BF00939805}}\n\n[[Category:Numerical analysis]]\n[[Category:Rates|Convergence]]"
    },
    {
      "title": "Regularized meshless method",
      "url": "https://en.wikipedia.org/wiki/Regularized_meshless_method",
      "text": "In numerical mathematics, the '''regularized meshless method (RMM)''', also known as the '''singular meshless method''' or '''desingularized meshless method''', is a meshless boundary collocation method designed to solve certain [[partial differential equations]] whose [[fundamental solution]] is explicitly known. The RMM is a strong-form [[collocation method]] with merits being meshless, integration-free, easy-to-implement, and high stability. Until now this method has been successfully applied to some typical problems, such as potential, acoustics, water wave, and [[inverse problems]] of bounded and unbounded domains.\n\n==Description==\nThe RMM employs the [[double layer potential]]s from the potential theory as its basis/kernel functions. Like the [[method of fundamental solution]]s (MFS),<ref>A.K. G. Fairweather, The method of fundamental solutions for elliptic boundary value problems, ''Advances in Computational Mathematics''. 9 (1998) 69–95.</ref><ref>M.A. Golberg, C.S. Chen, The theory of radial basis functions applied to the BEM for inhomogeneous partial differential equations, ''Boundary Elements Communications''. 5 (1994) 57–61.</ref> the numerical solution is approximated by a linear combination of double layer kernel functions with respect to different source points. Unlike the MFS, the collocation and source points of the RMM, however, are coincident and placed on the physical boundary without the need of a fictitious boundary in the MFS. Thus, the RMM overcomes the major bottleneck in the MFS applications to the real world problems. \n\nUpon the coincidence of the collocation and source points, the double layer kernel functions will present various orders of singularity. Thus, a subtracting and adding-back regularizing technique <ref name=\"RMM\">D.L. Young, K.H. Chen, C.W. Lee. Novel meshless method for solving the potential problems with arbitrary domains. ''Journal of Computational Physics'' 2005; 209(1): 290–321.</ref> is introduced and, hence, removes or cancels such singularities.\n\n==History and recent development==\nThese days the [[finite element method]] (FEM), [[finite difference method]] (FDM), [[finite volume method]] (FVM), and [[boundary element method]] (BEM) are dominant numerical techniques in numerical modelings of many fields of engineering and sciences. Mesh generation is tedious and even very challenging problems in their solution of high-dimensional moving or complex-shaped boundary problems and is computationally costly and often mathematically troublesome. \n\nThe BEM has long been claimed to alleviate such drawbacks thanks to the boundary-only discretizations and its semi-analytical nature. Despite these merits, the BEM, however, involves quite sophisticated mathematics and some tricky singular integrals. Moreover, surface meshing in a three-dimensional domain remains to be a nontrivial task. Over the past decades, considerable efforts have been devoted to alleviating or eliminating these difficulties, leading to the development of meshless/meshfree boundary collocation methods which require neither domain nor boundary meshing. Among these methods, the MFS is the most popular with the merit of easy programming, mathematical simplicity, high accuracy, and fast convergence. \n\nIn the MFS, a fictitious boundary outside the problem domain is required in order to avoid the singularity of the fundamental solution. However, determining the optimal location of the fictitious boundary is a nontrivial task to be studied. Dramatic efforts have ever since been made to remove this long perplexing issue. Recent advances include, for example, [[boundary knot method]] (BKM),<ref>W. Chen and M. Tanaka, \"[http://em.hhu.edu.cn/chenwen/papers/rbf/CMA_BKM.pdf A meshfree, exponential convergence, integration-free, and boundary-only RBF technique] {{webarchive|url=https://web.archive.org/web/20160304031518/http://em.hhu.edu.cn/chenwen/papers/rbf/CMA_BKM.pdf |date=2016-03-04 }}\", ''Computers and Mathematics with Applications'', 43, 379–391, 2002.</ref><ref>W. Chen and Y.C. Hon, \"[http://em.hhu.edu.cn/chenwen/papers/rbf/Chen&Hon.pdf Numerical convergence of boundary knot method in the analysis of Helmholtz, modified Helmholtz, and convection-diffusion problems] {{webarchive|url=https://web.archive.org/web/20150620223119/http://em.hhu.edu.cn/chenwen/papers/rbf/Chen%26Hon.pdf |date=2015-06-20 }}\", ''Computer Methods in Applied Mechanics and Engineering'', 192, 1859–1875, 2003.</ref> regularized meshless method (RMM),<ref name=\"RMM\"/> modified MFS (MMFS),<ref>B. Sarler, \"Solution of potential flow problems by the modified method of fundamental solutions: Formulations with the single layer and the double layer fundamental solutions\", ''Eng Anal Bound Elem'' 2009;33(12): 1374–82.</ref> and [[singular boundary method]] (SBM) <ref>W. Chen, F.Z. Wang, \"[http://em.hhu.edu.cn/chenwen/papers/rbf/EABE-SBM.pdf A method of fundamental solutions without fictitious boundary] {{webarchive|url=https://web.archive.org/web/20150606193008/http://em.hhu.edu.cn/chenwen/papers/rbf/EABE-SBM.pdf |date=2015-06-06 }}\", ''Eng Anal Bound Elem'' 2010;34(5): 530–32.</ref> \n\nThe methodology of the RMM was firstly proposed by Young and his collaborators in 2005. The key idea is to introduce a subtracting and adding-back regularizing technique to remove the singularity of the double layer kernel function at the origin, so that the source points can be placed directly on the real boundary. Up to now, the RMM has successfully been applied to a variety of physical problems, such as potential,<ref name=\"RMM\"/> exterior acoustics <ref>D.L. Young, K.H. Chen, C.W. Lee. Singular meshless method using double layer potentials for exterior acoustics.''Journal of the Acoustical Society of America'' 2006;119(1):96–107.</ref> antiplane piezo-electricity,<ref>K.H. Chen, J.H. Kao, J.T. Chen. Regularized meshless method for antiplane piezo- electricity problems with multiple inclusions.'' Computers, Materials, & Con- tinua'' 2009;9(3):253–79.</ref> acoustic eigenproblem with multiply-connected domain,<ref>K.H. Chen, J.T. Chen, J.H. Kao. Regularized meshless method for solving acoustic eigenproblem with multiply-connected domain.'' Computer Modeling in Engineering & Sciences'' 2006;16(1):27–39.</ref> inverse problem,<ref>K.H. Chen, J.H. Kao, J.T. Chen, K.L. Wu. Desingularized meshless method for solving Laplace equation with over-specified boundary conditions using regularization techniques. ''Computational Mechanics'' 2009;43:827–37</ref> possion’ equation <ref>W. Chen, J. Lin, F.Z. Wang, \"[http://em.hhu.edu.cn/chenwen/papers/softmatter/EABE-RMM.pdf Regularized meshless method for nonhomogeneous problems] {{webarchive|url=https://web.archive.org/web/20150606191121/http://em.hhu.edu.cn/chenwen/papers/softmatter/EABE-RMM.pdf |date=2015-06-06 }}\", ''Eng. Anal. Bound. Elem.'' 35 (2011) 253–257.</ref> and water wave problems.<ref>K.H. Chen, M.C. Lu, H.M. Hsu, Regularized meshless method analysis of the problem of obliquely incident water wave, ''Eng. Anal. Bound. Elem''. 35 (2011) 355–362.</ref> Furthermore, some improved formulations have been made aiming to further improve the feasibility and efficiency of this method, see, for example, the weighted RMM for irregular domain problems <ref>R.C. Song, W. Chen,\"[http://em.hhu.edu.cn/chenwen/papers/rbf/RMM-CMES.pdf An investigation on the regularized meshless method for irregular domain problems]{{dead link|date=April 2018 |bot=InternetArchiveBot |fix-attempted=yes }}\",'' CMES-Comput. Model. Eng. Sci''. 42 (2009) 59–70.</ref> and analytical RMM for 2D Laplace problems.<ref>W. Chen, R.C. Song, Analytical diagonal elements of regularized meshless method for regular domains of 2D Dirichlet Laplace problems, ''Eng. Anal. Bound. Elem.'' 34 (2010) 2–8.</ref>\n\n==See also==\n*[[Radial basis function]]\n*[[Boundary element method]]\n*[[Method of fundamental solutions]]\n*[[Boundary knot method]]\n*[[Boundary particle method]]\n*[[Singular boundary method]]\n\n==References==\n{{reflist}}\n\n{{Numerical PDE}}\n\n[[Category:Numerical analysis]]\n[[Category:Numerical differential equations]]"
    },
    {
      "title": "Relative change and difference",
      "url": "https://en.wikipedia.org/wiki/Relative_change_and_difference",
      "text": "In any [[quantitative science]], the terms '''relative change''' and '''relative difference''' are used to compare two [[quantities]] while taking into account the \"sizes\" of the things being compared. The comparison is expressed as a [[ratio]] and is a [[unitless]] [[number]]. By multiplying these ratios by 100 they can be expressed as [[percentage]]s so the terms '''percentage change''', '''percent(age) difference''', or '''relative percentage difference''' are also commonly used. The distinction between \"change\" and \"difference\" depends on whether or not one of the quantities being compared is considered a ''standard'' or ''reference'' or ''starting'' value. When this occurs, the term ''relative change'' (with respect to the reference value) is used and otherwise the term ''relative difference'' is preferred. Relative difference is often used as a quantitative indicator of [[quality assurance]] and [[quality control]] for repeated measurements where the outcomes are expected to be the same. A special case of percent change (relative change expressed as a percentage) called ''percent error'' occurs in measuring situations where the reference value is the accepted or actual value (perhaps theoretically determined) and the value being compared to it is experimentally determined (by measurement).\n\n==Definitions==\nGiven two numerical quantities, ''x'' and ''y'', their ''difference'', {{nowrap|1=Δ = ''x'' − ''y''}}, can be called their ''actual difference''. When ''y'' is a ''reference value'' (a theoretical/actual/correct/accepted/optimal/starting, etc. value; the value that ''x'' is being compared to) then Δ is called their ''actual change''. When there is no reference value, the sign of Δ has little meaning in the comparison of the two values since it doesn't matter which of the two values is written first, so one often works with {{nowrap|1={{abs|Δ}} = {{abs|''x'' − ''y''}}}}, the [[absolute difference]] instead of Δ, in these situations. Even when there is a reference value, if it doesn't matter whether the compared value is larger or smaller than the reference value, the absolute difference can be considered in place of the actual change.\n\nThe absolute difference between two values is not always a good way to compare the numbers. For instance, the absolute difference of 1 between 6 and 5 is more significant than the same absolute difference between 100,000,001 and 100,000,000. We can adjust the comparison to take into account the \"size\" of the quantities involved, by defining, for positive values of ''x''<sub>''reference''</sub>:\n\n::<math> \\text{Relative change}(x, x_\\text{reference}) = \\frac{\\text{Actual change}}{x_\\text{reference}} = \\frac{\\Delta}{x_\\text{reference}} = \\frac{x - x_\\text{reference}}{x_\\text{reference}}.</math>\n\nThe relative change is not defined if the reference value (''x''<sub>''reference''</sub>) is zero.\n\nFor values greater than the reference value, the relative change should be a positive number and for values that are smaller, the relative change should be negative. The formula given above behaves in this way only if ''x''<sub>''reference''</sub> is positive, and reverses this behavior if ''x''<sub>''reference''</sub> is negative. For example, if we are calibrating a thermometer which reads −6&nbsp;°C when it should read −10&nbsp;°C, this formula for relative change (which would be called ''relative error'' in this application) gives {{nowrap|1=((−6) − (−10)) / (−10) = 4 / −10 = −0.4}}, yet the reading is too high. To fix this problem we alter the definition of relative change so that it works correctly for all nonzero values of ''x''<sub>''reference''</sub>:\n\n::<math> \\text{Relative change}(x, x_\\text{reference}) = \\frac{\\text{Actual change}}{|x_\\text{reference}|} = \\frac{\\Delta}{|x_\\text{reference}|} = \\frac{x - x_\\text{reference}}{|x_\\text{reference}|}.</math>\n\nIf the relationship of the value with respect to the reference value (that is, larger or smaller) does not matter in a particular application, the absolute difference may be used in place of the actual change in the above formula to produce a value for the relative change which is always non-negative.\n\nDefining relative difference is not as easy as defining relative change since there is no \"correct\" value to scale the absolute difference with. As a result, there are many options for how to define relative difference and which one is used depends on what the comparison is being used for. In general we can say that the absolute difference {{abs|Δ}} is being scaled by some function of the values ''x'' and ''y'', say {{nowrap|''f''(''x'', ''y'')}}.<ref>{{harvnb|Törnqvist et al.|1985}}</ref>\n\n::<math> \\text{Relative difference}(x, y) = \\frac{\\text{Absolute difference}}{|f(x,y)|} = \\frac{|\\Delta|}{|f(x,y)|} = \\left |\\frac{x - y}{f(x,y)} \\right |.</math>\n\nAs with relative change, the relative difference is undefined if {{nowrap|''f''(''x'', ''y'')}} is zero.\n\nSeveral common choices for the function {{nowrap|''f''(''x'', ''y'')}} would be:\n* max({{abs|''x''}}, {{abs|''y''}}),\n* max(''x'', ''y''),\n* min({{abs|''x''}}, {{abs|''y''}}),\n* min (''x'', ''y''),\n* (''x'' + ''y'')/2, and\n* ({{abs|''x''}} + {{abs|''y''}})/2.\n\n==Formulae==\nMeasures of relative difference are [[unitless]] numbers expressed as a [[fraction (mathematics)|fraction]]. Corresponding values of percent difference would be obtained by multiplying these values by 100 (and appending the % sign to indicate that the value is a percentage).\n\nOne way to define the relative difference of two numbers is to take their [[absolute difference]] divided by the [[maximum]] absolute value of the two numbers.\n\n:<math>\nd_r=\\frac{|x-y|}{\\max(|x|,|y|)}\\,\n</math>\n\nif at least one of the values does not equal zero. This approach is especially useful when comparing [[floating point]] values in [[programming language]]s for [[Equality (mathematics)|equality]] with a certain tolerance.<ref>[http://c-faq.com/fp/fpequal.html What's a good way to check for ''close enough'' floating-point equality]</ref> Another application is in the computation of [[approximation error]]s when the relative error of a measurement is required.\n\nAnother way to define the relative difference of two numbers is to take their [[absolute value|absolute]] [[Subtraction|difference]] [[ratio|divided]] by some functional value of the two numbers, for example, the absolute value of their [[arithmetic mean]]:\n\n:<math>\nd_r=\\frac{|x-y|}{\\left(\\frac{|x+y|}{2}\\right)}\\, .\n</math>\n\nThis approach is often used when the two numbers reflect a change in some single underlying entity.{{citation needed|date=February 2012}} A problem with the above approach arises when the functional value is zero. In this example, if x and y have the same magnitude but opposite sign, then\n:<math>\n\\frac{|x+y|}{2} = 0 ,\n</math>\nwhich causes division by 0.  So it may be better to replace the denominator with the average of the absolute values of ''x'' and&nbsp;''y'':{{citation needed|date=February 2012}}\n\n:<math>\nd_r=\\frac{|x-y|}{\\left(\\frac{|x|+|y|}{2}\\right)}\\, .\n</math>\n\n==Percent error==\n\n'''Percent Error''' is a special case of the percentage form of relative change calculated from the absolute change between the experimental (measured) and theoretical (accepted) values, and dividing by the theoretical (accepted) value.\n\n: <math>\\%\\text{ Error} = \\frac{|\\text{Experimental}-\\text{Theoretical}|}{|\\text{Theoretical}|}\\times100</math>.\n\nThe terms \"Experimental\" and \"Theoretical\" used in the equation above are commonly replaced with similar terms. Other terms used for ''experimental'' could be \"measured,\" \"calculated,\" or \"actual\" and another term used for ''theoretical'' could be \"accepted.\"  Experimental value is what has been derived by use of calculation and/or measurement and is having its accuracy tested against the theoretical value, a value that is accepted by the scientific community or a value that could be seen as a goal for a successful result.\n\nAlthough it is common practice to use the absolute value version of relative change when discussing percent error, in some situations, it can be beneficial to remove the absolute values to provide more information about the result. Thus, if an experimental value is less than the theoretical value, the percent error will be negative. This negative result provides additional information about the experimental result. For example, experimentally calculating the speed of light and coming up with a negative percent error says that the experimental value is a velocity that is less than the speed of light. This is a big difference from getting a positive percent error, which means the experimental value is a velocity that is greater than the speed of light (violating the [[theory of relativity]]) and is a newsworthy result.\n\nThe percent error equation, when rewritten by removing the absolute values, becomes:\n\n: <math>\\%\\text{ Error} = \\frac{\\text{Experimental}-\\text{Theoretical}}{|\\text{Theoretical}|}\\times100.</math>\n\nIt is important to note that the two values in the [[numerator]] do not [[commutative|commute]]. Therefore, it is vital to preserve the order as above: subtract the theoretical value from the experimental value and not vice versa.\n\n==Percentage change==\n\nA '''percentage change''' is a way to express a change in a variable. It represents the relative change between the old value and the new one.\n\nFor example, if a house is worth $100,000 today and the year after its value goes up to $110,000, the percentage change of its value can be expressed as\n\n: <math> \\frac{110000-100000}{100000} = 0.1 = 10\\%.</math>\n\nIt can then be said that the worth of the house went up by 10%.\n\nMore generally, if ''V''<sub>1</sub> represents the old value and ''V''<sub>2</sub> the new one,\n\n: <math>\\text{Percentage change} = \\frac{\\Delta V}{V_1} = \\frac{V_2 - V_1}{V_1} \\times100 .</math>\n\nSome calculators directly support this via a {{kbd|%CH}} or {{kbd|Δ%}} function.\n\nWhen the variable in question is a percentage itself, it is better to talk about its change by using [[percentage point]]s, to avoid confusion between [[relative difference]] and [[absolute difference]].\n\n===Example of percentages of percentages===\nIf a bank were to raise the interest rate on a savings account from 3% to 4%, the statement that \"the interest rate was increased by 1%\" is ambiguous and should be avoided. The absolute change in this situation is 1 percentage point (4% − 3%), but the relative change in the interest rate is:\n::<math>\\frac{4\\% - 3\\%}{3\\%} = 0.333\\ldots = 33\\frac{1}{3}\\%.</math>\nSo, one should say either that the interest rate was increased by 1 percentage point, or that the interest rate was increased by <math>33\\frac{1}{3}\\%.</math>\n\nIn general, the term \"percentage point(s)\" indicates an absolute change or difference of percentages, while the percent sign or the word \"percentage\" refers to the relative change or difference.<ref>{{harvnb|Bennett|Briggs|2005|loc=p. 141}}</ref>\n\n==Examples==\n\n===Comparisons===\nCar ''M'' costs $50,000 and car ''L'' costs $40,000. We wish to compare these costs.<ref>{{harvnb|Bennett|Briggs|2005|loc=pp. 137&ndash;139}}</ref> With respect to car ''L'', the absolute difference is {{nowrap|1=$10,000 = $50,000 − $40,000}}. That is, car ''M'' costs $10,000 more than car ''L''. The relative difference is,\n::<math>\\frac{\\$10,000}{\\$40,000} = 0.25 = 25\\%,</math>\nand we say that car ''M'' costs 25% ''more than'' car ''L''. It is also common to express the comparison as a ratio, which in this example is,\n::<math>\\frac{\\$50,000}{\\$40,000} = 1.25 = 125\\%,</math>\nand we say that car ''M'' costs 125% ''of'' the cost of car ''L''.\n\nIn this example the cost of car ''L'' was considered the reference value, but we could have made the choice the other way and considered the cost of car ''M'' as the reference value. The absolute difference is now {{nowrap|1=−$10,000 = $40,000 − $50,000}} since car ''L'' costs $10,000 less than car ''M''. The relative difference,\n::<math>\\frac{-\\$10,000}{\\$50,000} = -0.20 = -20\\%</math>\nis also negative since car ''L'' costs 20% ''less than'' car ''M''. The ratio form of the comparison,\n::<math>\\frac{\\$40,000}{\\$50,000} = 0.8 = 80\\%</math>\nsays that car ''L'' costs 80% ''of'' what car ''M'' costs.\n\nIt is the use of the words \"of\" and \"less/more than\" that distinguish between ratios and relative differences.<ref>{{harvnb|Bennett|Briggs|2005|loc=p.140}}</ref>  \n<!--\n===Comparing different groups===\n\nLet's look at a group of people, some males and some females.  Here are the percentages of each subgroup that enjoy watching a particular TV show:\n\n:  Males 35%\n:  Females 75%\n\nThe absolute difference is 40 [[percentage points]] (75%&nbsp;&minus;&nbsp;35%). The relative differences can be stated several ways:\n\n:  47%  = 35%/75%\n:  Males enjoy the TV show 47% as frequently as, or 53% less frequently than, females.\n:  Or, about half as often as females.\n\n:  214% = 75%/35%\n:  Females enjoy the TV show 214% as frequently as, or 114% more frequently than, males.\n:  Or, over twice as often as males.\n\nAs ratios:\n\n:  0.47 = 35/75 Males relative to females.\n:  2.14 = 75/35 Females relative to males.\n\nAs percentages:\n\n:   47% = 35/75 Males relative to females.\n:  214% = 75/35 Females relative to males.\n\n===Characterizing change===\n\nNow let's look at a group of [[genderless]] [[Martians]], before and after watching that TV show. The percentages of the group favorable to [[Earthling]]s are\n\n:  Before 75%\n:  After 35%\n\nThe absolute difference is &minus;0.4 or &minus;40% (35%&nbsp;&minus;&nbsp;75%), e.g., Martian favorability to Earthlings decreased by 40 percentage points after watching the show. The relative difference (&minus;0.4/0.75) can also be stated as a ratio (&minus;0.53) or as a percentage (&minus;53%), e.g., Martian favorability to Earthlings decreased 53% after watching the show.\n-->\n\n==Logarithmic scale==\n\nChange in a quantity can also be expressed logarithmically.  Using the [[natural logarithm]] (ln) and normalization with a factor of 100, as done for [[percent]] aligns with the definition for percentage change for very small changes (called \"log change\" in the tables below):\n:<math> D = 100 \\cdot \\ln\\frac{V_2}{V_1} \\approx 100 \\cdot \\frac{V_2 - V_1}{V_1} = \\text{Percentage change} \\text{ when }\\left | \\frac{V_2 - V_1}{V_1} \\right | << 1 </math>\n\nUsing a logarithmic scale has advantages.  First, the magnitude of the change expressed in this way is the same whether ''V''<sub>1</sub> or ''V''<sub>2</sub> is chosen as the reference, since <math>\\ln\\frac{V_2}{V_1} = -1 \\cdot \\ln\\frac{V_1}{V_2}</math>.  In contrast, <math>\\frac{V_2 - V_1}{V_1} \\approx -1 \\cdot \\frac{V_1 - V_2}{V_2}</math>, with the approximation error becoming more significant as ''V''<sub>2</sub> and ''V''<sub>1</sub> diverge.  For example:\n{| class=\"wikitable\"\n|-\n! ''V''<sub>1</sub> !! ''V''<sub>2</sub> !! Log change !! Change (%)\n|-\n| 10 || 9 || −10.5 || −10.0\n|-\n| 9 || 10 || +10.5 || +11.1\n|}\n\nAnother advantage is that the total change after a series of changes equals the sum of the changes when expressed logarithmically.  With percent, summing the changes is only an approximation, with larger error for larger changes.  For example:\n{| class=\"wikitable\"\n|-\n! Log change 1 !! Log change 2 !! Total log change !! Change 1 (%) !! Change 2 (%) !! Total Change (%)\n|-\n| 10 || 5 || 15 || 10 || 5 || 15.5\n|-\n| 10 || −5 || 5 || 10 || −5 || 4.5\n|-\n| 10 || 10 || 20 || 10 || 10 || 21\n|-\n| 10 || −10 || 0 || 10 || −10 || −1\n|-\n| 50 || 50 || 100 || 50 || 50 || 125\n|-\n| 50 || −50 || 0 || 50 || −50 || −25\n|}\n\n==See also==\n\n*[[Approximation error]]\n*[[Errors and residuals in statistics]]\n*[[Relative standard deviation]]\n*[[Logarithmic scale]]\n\n{{More footnotes|date=March 2011}}\n\n==Notes==\n\n{{reflist}}\n\n==References==\n* {{citation|last1=Bennett|first1=Jeffrey|last2=Briggs|first2=William|title=Using and Understanding Mathematics: A Quantitative Reasoning Approach|edition=3rd|publisher=Pearson|place=Boston|year=2005|isbn=0-321-22773-5}}\n* {{cite web| last =| first =| authorlink =| vauthors =| title =Understanding Measurement and Graphing| work =| publisher =[[North Carolina State University]]| date =2008-08-20| url =http://www.physics.ncsu.edu/courses/pylabs/205N_208N_MeasurementandErrors.pdf| format =| doi =| accessdate =2010-05-05| deadurl =yes| archiveurl =https://web.archive.org/web/20100615192147/http://www.physics.ncsu.edu/courses/pylabs/205N_208N_MeasurementandErrors.pdf| archivedate =2010-06-15| df =}}\n* {{cite web\n  | title = Percent Difference &ndash; Percent Error\n  | work =\n  | publisher = Illinois State University, Dept of Physics\n  | date = 2004-07-20\n  | url = http://www.phy.ilstu.edu/slh/Percent%20Difference%20Error.pdf\n  | format =\n  | doi =\n  | accessdate = 2010-05-05 }}\n* {{citation|last1=Törnqvist|first1=Leo|last2=Vartia|first2=Pentti|last3=Vartia|first3=Yrjö|title=How Should Relative Changes Be Measured?|year=1985|journal=The American Statistician|volume=39|issue=1|pages=43–46|doi=10.2307/2683905}}\n\n==External links==\n*http://www.acponline.org/clinical_information/journals_publications/ecp/janfeb00/primer.htm\n\n{{DEFAULTSORT:Relative Change and Difference}}\n[[Category:Measurement]]\n[[Category:Numerical analysis]]\n[[Category:Statistical ratios]]\n[[Category:Subtraction]]"
    },
    {
      "title": "Residual (numerical analysis)",
      "url": "https://en.wikipedia.org/wiki/Residual_%28numerical_analysis%29",
      "text": "{{Other uses|Residual (disambiguation){{!}}Residual}}\n\nLoosely speaking, a '''residual''' is the [[Error#Experimental_science|error]] in a result. To be precise, suppose we want to find ''x'' such that \n\n: <math>f(x)=b.\\,</math>\n\nGiven an approximation ''x''<sub>0</sub> of ''x'', the residual is \n\n: <math>b - f(x_0)\\,</math>\n\nwhereas the error is\n\n: <math>x - x_0\\,</math>\n\nIf the exact value of ''x'' is not known, the residual can be computed, whereas the error cannot.\n\n==Residual of the approximation of a function==\nSimilar terminology is used dealing with \n[[differential equation|differential]], \n[[integral equation|integral]] and \n[[functional equation]]s.\nFor the approximation\n<math>~f_{\\rm a}~</math> of the solution \n<math>~f~</math> of the equation\n:<math> T(f)(x)=g(x) </math> ,\nthe residual can either be the function\n: <math>~g(x)~ - ~T(f_{\\rm a})(x)</math>\nor can be said to be the maximum of the norm of this difference\n: <math>\\max_{x\\in \\mathcal X} |g(x)-T(f_{\\rm a})(x)| </math>\nover the domain <math>\\mathcal X</math>, where the function \n<math>~f_{\\rm a}~</math>\nis expected to approximate the solution <math>~f~</math>,\nor some integral of a function of the difference, for example:\n\n: <math>~\\int_{\\mathcal X} |g(x)-T(f_{\\rm a})(x)|^2~{\\rm d} x.</math>\n\nIn many cases, the smallness of the residual means that the approximation is close to the solution, i.e., \n\n: <math>~\\left|\\frac{f_{\\rm a}(x) - f(x)}{f(x)}\\right| \\ll  1.~</math>\n\nIn these cases, the initial equation is considered as [[well-posed]]; and the residual can be considered as a measure of deviation of the approximation from the exact solution.\n\n==Use of residuals==\n\nWhen one does not know the exact solution, one may look for the approximation with small residual.\n\nResiduals appear in many areas in mathematics, including [[iterative solver]]s such as the [[generalized minimal residual method]], which seeks solutions to equations by systematically minimizing the residual.\n\n==External links==\n* Jonathan Richard Shewchuk. ''[http://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf An Introduction to the Conjugate Gradient Method Without the Agonizing Pain]'', p. 6.\n\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Richardson extrapolation",
      "url": "https://en.wikipedia.org/wiki/Richardson_extrapolation",
      "text": "In [[numerical analysis]], '''Richardson extrapolation''' is a [[Series acceleration|sequence acceleration]] method, used to improve the [[rate of convergence]] of a [[sequence]]. It is named after [[Lewis Fry Richardson]], who introduced the technique in the early 20th century.<ref>{{cite journal\n | last=Richardson | first=L. F. | authorlink=Lewis Fry Richardson\n | title=The approximate arithmetical solution by finite differences of physical problems including differential equations, with an application to the stresses in a masonry dam\n | journal=Philosophical Transactions of the Royal Society A\n | year=1911 | volume=210\n | issue=459-470 | pages=307–357\n | doi=10.1098/rsta.1911.0009\n}}</ref><ref>{{cite journal\n | last=Richardson | first=L. F. | authorlink=Lewis Fry Richardson\n | title=The deferred approach to the limit\n | journal=Philosophical Transactions of the Royal Society A\n | year=1927 | volume=226\n | issue=636-646 | pages=299–349\n | doi=10.1098/rsta.1927.0008\n | last2=Gaunt\n | first2=J. A.\n}}</ref>  In the words of [[Garrett Birkhoff|Birkhoff]] and [[Gian-Carlo Rota|Rota]], \"its usefulness for practical computations can hardly be overestimated.\"<ref>Page 126 of {{cite book | last=Birkhoff | first=Garrett | authorlink=Garrett Birkhoff |author2=[[Gian-Carlo Rota]]  | title=Ordinary differential equations | publisher=John Wiley and sons | year=1978 | edition=3rd | isbn=0-471-07411-X | oclc= 4379402}}</ref>\n\nPractical applications of Richardson extrapolation include [[Romberg integration]], which applies Richardson extrapolation to the [[trapezoid rule]], and the [[Bulirsch–Stoer algorithm]] for solving ordinary differential equations.\n\n==Example of Richardson extrapolation==\nSuppose that we wish to approximate <math>A^*</math>, and we have a method <math>A(h)</math> that depends on a small parameter <math>h</math> in such a way that\n\n<math>A(h) = A^\\ast + C h^n + O(h^{n+1}).</math>\n\nLet us define a new function\n\n<math> R(h,t) := \\frac{ t^n A(h/t) - A(h)}{t^n-1} </math>\nwhere <math>h</math> and <math>\\frac{h}{t}</math> are two distinct step sizes.\n\nThen\n\n<math>  R(h, t) = \\frac{ t^n ( A^* + C \\left(\\frac{h}{t}\\right)^n + O(h^{n+1}) ) - ( A^* + C h^n + O(h^{n+1}) ) }{ t^n - 1} = A^* + O(h^{n+1}). </math>\n\n<math> R(h,t) </math> is called the Richardson [[extrapolation]] of ''A''(''h''), and has a higher-order\nerror estimate <math> O(h^{n+1}) </math> compared to <math> A(h) </math>.\n\nVery often, it is much easier to obtain a given precision by using ''R(h)'' rather\nthan ''A(h')'' with a much smaller '' h' '', which can cause problems due to limited precision ([[rounding error]]s) and/or due to the increasing [[Computational cost|number of calculations]] needed (see examples below).\n\n==General formula==\nLet ''<math>A(h)</math>'' be an approximation of ''<math>A^*</math>''(exact value) that depends on a positive step size ''h'' with an [[Approximation error|error]] formula of the form \n:<math> A^* = A(h)+a_0h^{k_0} + a_1h^{k_1} + a_2h^{k_2} + \\cdots </math>\nwhere the ''a<sub>i</sub>'' are unknown constants and the ''k<sub>i</sub>'' are known constants such that ''h<sup>k<sub>i</sub></sup>'' > ''h<sup>k<sub>i+1</sub></sup>''.\n\n''k''<sub>0</sub> is the leading order step size behavior of [[Truncation error]] as <math>A^*=A(h)+O(h^{k_0})</math>\n\nThe exact value sought can be given by\n:<math> A^* = A(h) + a_0h^{k_0} + a_1h^{k_1} + a_2h^{k_2} + \\cdots </math>\nwhich can be simplified with [[Big O notation]] to be\n:<math> A^* = A(h)+ a_0h^{k_0} + O(h^{k_1}).  \\,\\!</math>\n\nUsing the step sizes ''h'' and ''h / t'' for some ''t'', the two formulas for ''A'' are:\n:<math> A^* = A(h)+ a_0h^{k_0} + O(h^{k_1})  \\,\\!</math>\n:<math> A^* = A\\!\\left(\\frac{h}{t}\\right) + a_0\\left(\\frac{h}{t}\\right)^{k_0} + O(h^{k_1}) .</math>\n\nMultiplying the second equation by ''t''<sup>''k''<sub>0</sub></sup> and subtracting the first equation gives\n:<math> (t^{k_0}-1)A^* = t^{k_0}A\\left(\\frac{h}{t}\\right) - A(h) + O(h^{k_1}) </math>\nwhich can be solved for <math>A^*</math> to give\n:<math>A^* = \\frac{t^{k_0}A\\left(\\frac{h}{t}\\right) - A(h)}{t^{k_0}-1} + O(h^{k_1}) .</math>\n\nTherefore, using <math>R(h,t)=\\frac{t^{k_0}A\\left(\\frac{h}{t}\\right)-A(h)}{t^{k_0}-1}</math> the truncation error has been reduced to \n<math>O(h^{k_1}) </math>. This is in contrast to <math>A(h)</math> where the [[truncation error]] is <math>O(h^{k_0}) </math> for the same step size <math>h</math>\n\nBy this process, we have achieved a better approximation of ''A'' by subtracting the largest term in the error which was ''O''(''h''<sup>''k''<sub>0</sub></sup>).  This process can be repeated to remove more error terms to get even better approximations.\n\nA general [[recurrence relation]] beginning with <math>A_0=A(h)</math> can be defined for the approximations by\n:<math> A_{i+1}(h) = \\frac{t^{k_i}A_i\\left(\\frac{h}{t}\\right) - A_i(h)}{t^{k_i}-1} </math>\nwhere <math>k_{i+1}</math> satisfies\n:<math> A^* = A_{i+1}(h) + O(h^{k_{i+1}}) </math>.\n\nThe Richardson extrapolation can be considered as a linear [[sequence transformation]].\n\nAdditionally, the general formula can be used to estimate ''k''<sub>0</sub> (leading order step size behavior of [[Truncation error]]) when neither its value nor ''A''<sup>*</sup> (exact value) is known ''a priori''.  Such a technique can be useful for quantifying an unknown [[rate of convergence]].  Given approximations of ''A'' from three distinct step sizes ''h'', ''h / t'', and ''h / s'', the exact relationship\n:<math>A^*=\\frac{t^{k_0}A\\left(\\frac{h}{t}\\right) - A(h)}{t^{k_0}-1} + O(h^{k_1}) = \\frac{s^{k_0}A\\left(\\frac{h}{s}\\right) - A(h)}{s^{k_0}-1} + O(h^{k_1})</math>\nyields an approximate relationship (please note that the notation here may cause a bit of confusion, the two O appearing in the equation above only indicates the leading order step size behavior but their explicit forms are different and hence cancelling out of the two O terms is approximately valid)\n:<math>A\\left(\\frac{h}{t}\\right) + \\frac{A\\left(\\frac{h}{t}\\right) - A(h)}{t^{k_0}-1} \\approx A\\left(\\frac{h}{s}\\right) +\\frac{A\\left(\\frac{h}{s}\\right) - A(h)}{s^{k_0}-1}</math>\nwhich can be solved numerically to estimate ''k''<sub>0</sub>.\n\n==Example pseudocode code for Richardson extrapolation==\n\nThe following pseudocode in MATLAB style demonstrates Richardson extrapolation to help solve the ODE <math>y'(t) = -y^2</math>, <math>y(0) = 1</math> with the [[Trapezoidal method]]. In this example we halve the step size <math>h</math> each iteration and so in the discussion above we'd have that <math>t = 2</math>. The error of the Trapezoidal method can be expressed in terms of odd powers so that the error over multiple steps can be expressed in even powers; this leads us to raise <math>t</math> to the second power and to take powers of <math>4 = 2^2 = t^2</math> in the pseudocode. We want to find the value of <math>y(5)</math>, which has the exact solution of <math>\\frac{1}{5 + 1} = \\frac{1}{6} = 0.1666...</math> since the exact solution of the ODE is <math>y(t) = \\frac{1}{1 + t}</math>. This pseudocode assumes that a function called <code>Trapezoidal(f, tStart, tEnd, h, y0)</code> exists which attempts to computes <code>y(tEnd)</code> by performing the trapezoidal method on the function <code>f</code>, with starting point <code>y0</code> and <code>tStart</code> and step size <code>h</code>.\n\nNote that starting with too small an initial step size can potentially introduce error into the final solution. Although there are methods designed to help pick the best initial step size, one option is to start with a large step size and then to allow the Richardson extrapolation to reduce the step size each iteration until the error reaches the desired tolerance.\n\n<source lang=\"matlab\">\ntStart = 0          %Starting time\ntEnd = 5            %Ending time\nf = -y^2            %The derivative of y, so y' = f(t, y(t)) = -y^2\n                    % The solution to this ODE is y = 1/(1 + t)\ny0 = 1              %The initial position (i.e. y0 = y(tStart) = y(0) = 1)\ntolerance = 10^-11  %10 digit accuracy is desired\n\nmaxRows = 20                %Don't allow the iteration to continue indefinitely\ninitialH = tStart - tEnd    %Pick an initial step size\nhaveWeFoundSolution = false %Were we able to find the solution to within the desired tolerance? not yet.\n\nh = initialH\n\n%Create a 2D matrix of size maxRows by maxRows to hold the Richardson extrapolates\n%Note that this will be a lower triangular matrix and that at most two rows are actually\n% needed at any time in the computation.\nA = zeroMatrix(maxRows, maxRows)\n\n%Compute the top left element of the matrix\nA(1, 1) = Trapezoidal(f, tStart, tEnd, h, y0)\n\n%Each row of the matrix requires one call to Trapezoidal\n%This loops starts by filling the second row of the matrix, since the first row was computed above\nfor i = 1 : maxRows - 1 %Starting at i = 1, iterate at most maxRows - 1 times\n    h = h/2             %Half the previous value of h since this is the start of a new row\n    \n    %Call the Trapezoidal function with this new smaller step size\n    A(i + 1, 1) = Trapezoidal(f, tStart, tEnd, h, y0)\n    \n    for j = 1 : i         %Go across the row until the diagonal is reached\n        %Use the value just computed (i.e. A(i + 1, j)) and the element from the\n        % row above it (i.e. A(i, j)) to compute the next Richardson extrapolate\n     \n        A(i + 1, j + 1) = ((4^j).*A(i + 1, j) - A(i, j))/(4^j - 1);\n    end\n    \n    %After leaving the above inner loop, the diagonal element of row i + 1 has been computed\n    % This diagonal element is the latest Richardson extrapolate to be computed  \n    %The difference between this extrapolate and the last extrapolate of row i is a good\n    % indication of the error\n    if(absoluteValue(A(i + 1, i + 1) - A(i, i)) < tolerance)   %If the result is within tolerance\n        print(\"y(5) = \", A(i + 1, i + 1))                      %Display the result of the Richardson extrapolation\n        haveWeFoundSolution = true\n        break                                                  %Done, so leave the loop\n    end\nend\n\nif(haveWeFoundSolution == false)   %If we weren't able to find a solution to within the desired tolerance\n    print(\"Warning: Not able to find solution to within the desired tolerance of \", tolerance);\n    print(\"The last computed extrapolate was \", A(maxRows, maxRows))\nend\n</source>\n\n==See also==\n* [[Aitken's delta-squared process]]\n* [[Takebe Kenko]]\n* [[Richardson iteration]]\n\n==References==\n<references/>\n*''Extrapolation Methods. Theory and Practice'' by C. Brezinski and M. Redivo Zaglia, North-Holland, 1991.\n*Ivan Dimov, Zahari Zlatev, Istvan Farago, Agnes Havasi: Richardson Extrapolation: Practical Aspects and Applications'', Walter de Gruyter GmbH & Co KG, {{ISBN|9783110533002}} (2017).\n\n==External links==\n*[http://web.mit.edu/ehliu/Public/Spring2006/18.304/extrapolation.pdf Fundamental Methods of Numerical Extrapolation With Applications], mit.edu\n*[http://www.math.ubc.ca/~feldman/m256/richard.pdf Richardson-Extrapolation]\n*[http://www.math.ubc.ca/~israel/m215/rich/rich.html  Richardson extrapolation on a website of Robert Israel (University of British Columbia) ]\n\n[[Category:Numerical analysis]]\n[[Category:Asymptotic analysis]]\n[[Category:Articles with example MATLAB/Octave code]]"
    },
    {
      "title": "Riemann solver",
      "url": "https://en.wikipedia.org/wiki/Riemann_solver",
      "text": "{{Computational physics}}\nA '''Riemann solver''' is a [[numerical method]] used to solve a [[Riemann problem]]. They are heavily used in [[computational fluid dynamics]] and [[computational magnetohydrodynamics]].\n\n==Exact solvers==\n[[Sergei K. Godunov|Godunov]] is credited with introducing the first exact Riemann solver for the Euler equations,<ref>{{Citation| last = Godunov| first = S. K.| title = A difference scheme for numerical computation of discontinuous solution of hyperbolic equation | journal = Math. Sbornik| volume = 47 | pages = 271&ndash;306 | year = 1959}}</ref> by extending the previous CIR (Courant-Isaacson-Rees) method to non-linear systems of hyperbolic conservation laws. Modern solvers are able to simulate relativistic effects and magnetic fields.\n\nFor the hydrodynamic case latest research results showed the possibility to avoid the iterations to calculate the exact solution for the\nEuler equations.<ref>{{Citation| last1 = Wu | first1 = Y.Y.| last2 = Cheung | first2 = K.F. |title = Explicit solution to the exact Riemann problem and application in nonlinear shallow-water equations | journal = Int. J. Numer. Meth. Fluids | volume = 57 | pages = 1649&ndash;1668| year = 2008 |doi = 10.1002/fld.1696 | issue = 11 |bibcode = 2008IJNMF..57.1649W }}</ref>\n\n==Approximate solvers==\nAs iterative solutions are too costly, especially in magnetohydrodynamics, some approximations have to be made. The most popular solvers are:\n\n===Roe solver===\n{{main|Roe solver}}\n[[Philip L. Roe|Roe]] used the linearisation of the Jacobian, which he then solves exactly.<ref>{{Citation| last = Roe| first = P. L. | title = Approximate Riemann solvers, parameter vectors and difference schemes| journal = J. Comput. Phys.| volume = 43 | pages = 357&ndash;372| year = 1981| doi = 10.1016/0021-9991(81)90128-5|bibcode = 1981JCoPh..43..357R| issue = 2 }}</ref>\n\n===HLLE solver===\nThe HLLE<ref>{{Citation| last1 = Einfeldt | first1 = B.  |title = On Godunov-type methods for gas dynamics | journal = SIAM J. Numer. Anal. | volume = 25 | pages = 294&ndash;318| year = 1988| doi = 10.1137/0725021|bibcode = 1988SJNA...25..294E| issue = 2 }}</ref> ([[Ami Harten|Harten]], [[Peter Lax|Lax]], [[Bram van Leer|van Leer]] and  Einfeldt) solver is an approximate solution to the Riemann problem,  which is only based on the integral form of the conservation laws and the largest and smallest signal velocities at the interface.  The stability and robustness of the HLLE solver is closely related to the signal velocities and a single central average state, as proposed by Einfeldt in the original paper. The description of the HLLE scheme in the book mentioned below is incomplete and partially wrong{{Citation needed|reason=This is a bold claim that needs proof. |date=October 2017}}. The reader is referred to the original paper. Actually, the HLLE scheme is based on a new stability theory for discontinuities in fluids, which was never published.<ref>Einfeldt, B. [http://discontinuous-flow.blogspot.de/2012/06/notes-on-proposal.html \"Notes on a Proposal\"], Blog entry, 2 June 2012.</ref>\n\n===HLLC solver===\nThe HLLC (Harten-Lax-van Leer-Contact) solver was introduced by Toro.<ref>{{Citation| last1 = Toro | first1 = E. F. | last2 = Spruce | first2 = M. | last3 = Speares | first3 = W. |title = Restoration of the contact surface in the HLL-Riemann solver | journal = Shock Waves | volume = 4 | issue = 1 | pages = 25&ndash;34| year = 1994| doi = 10.1007/BF01414629|bibcode = 1994ShWav...4...25T }}</ref> It restores the missing Rarefaction wave by some estimates, like linearisations, these can be simple but also more advanced exists like using the Roe average velocity for the middle wave speed. They are quite robust and efficient but somewhat more diffusive.<ref>{{Citation| last = Quirk| first = J. J. | title = A contribution to the great Riemann solver debate| journal = Int. J. Numer. Meth. Fluids | volume = 18 | pages = 555&ndash;574| year = 1994 |doi = 10.1002/fld.1650180603| postscript = .|bibcode = 1994IJNMF..18..555Q| issue = 6 }}</ref>\n\n===Rotated-hybrid Riemann solvers===\nThese solvers were introduced by [[Hiroaki Nishikawa|Nishikawa]] and Kitamura,<ref>{{Citation| last1 = Nishikawa | first1 = H.| last2 = Kitamura | first2 = K. |title = Very simple, carbuncle-free, boundary-layer-resolving, rotated-hybrid Riemann solvers | journal = J. Comput. Phys.| volume = 227 | pages = 2560&ndash;2581| year = 2008 |doi = 10.1016/j.jcp.2007.11.003|bibcode = 2008JCoPh.227.2560N| issue = 4 }}</ref> in order to overcome the carbuncle problems \nof the Roe solver and the excessive diffusion of the HLLE solver at the same time. They developed robust and accurate Riemann solvers by combining the Roe solver and the HLLE/Rusanov solvers: they show that being applied in two orthogonal directions the two Riemann solvers can be combined into a single Roe-type solver (the Roe solver with modified wave speeds). In particular, the one derived from the Roe and HLLE solvers, called Rotated-RHLL solver, is extremely robust (carbuncle-free for all possible test cases on both structured and unstructured grids) and accurate (as accurate as the Roe solver for the boundary layer calculation).\n\n==Notes==\n{{Reflist|2}}\n\n==See also==\n* [[Godunov's scheme]]\n* [[Computational fluid dynamics]]\n* [[Computational magnetohydrodynamics]]\n\n==References==\n* {{Citation | first=Eleuterio F.| last=Toro| year=1999 | title=Riemann Solvers and Numerical Methods for Fluid Dynamics| publisher=Springer Verlag|location=Berlin | isbn=978-3-540-65966-2}}\n\n==External links==\n\n[[Category:Numerical analysis]]\n[[Category:Computational fluid dynamics]]\n[[Category:Conservation equations]]\n[[Category:Bernhard Riemann]]"
    },
    {
      "title": "Rounding Errors in Algebraic Processes",
      "url": "https://en.wikipedia.org/wiki/Rounding_Errors_in_Algebraic_Processes",
      "text": "#REDIRECT [[James H. Wilkinson#REAP]]\n\n{{Redirect category shell|1=\n{{R to related topic}}\n{{R with possibilities}}\n}}\n\n[[Category:1963 books]]\n[[Category:Mathematics books]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Round-off error",
      "url": "https://en.wikipedia.org/wiki/Round-off_error",
      "text": "{{for|the acrobatic movement, roundoff|Roundoff}}\n\nA '''roundoff error''',<ref>{{citation|title=Introduction to Numerical Analysis Using MATLAB|first=Rizwan|last=Butt|publisher=Jones & Bartlett Learning|year=2009|isbn=9780763773762|pages=11–18|url=https://books.google.com/books?id=QWub-UVGxqkC&pg=PA11}}.</ref> also called '''rounding error''',<ref>{{citation|title=Numerical Computation 1: Methods, Software, and Analysis|first=Christoph W.|last=Ueberhuber|publisher=Springer|year=1997|isbn=9783540620587|url=https://books.google.com/books?id=JH9I7EJh3JUC&pg=PA139|pages=139–146}}.</ref> is the difference between the result produced by a given [[algorithm]] using exact arithmetic and the result produced by the same algorithm using finite-precision, rounded arithmetic.<ref name=\"Forrester_2018\">{{cite book|title= Math/Comp241 Numerical Methods (lecture notes)|first=Dick|last=Forrester|publisher=[[Dickinson College]]|year=2018}}.</ref> Rounding errors are due to inexactness in the representation of real numbers and the arithmetic operations done with them. This is a form of [[quantization error]].<ref>{{citation|title=Information Technology in Theory|first1=Pelin|last1=Aksoy|first2=Laura|last2=DeNardis|publisher=Cengage Learning|year=2007|isbn=9781423901402|page=134|url=https://books.google.com/books?id=KGS5IcixljwC&pg=PA134}}.</ref> When using approximation [[equation]]s or [[algorithm]]s, especially when using finitely many digits to represent real numbers (which in theory have infinitely many digits), one of the goals of [[numerical analysis]] is to [[error analysis (mathematics)|estimate]] computation errors.<ref>{{citation|title=A First Course in Numerical Analysis|edition=2nd|series=Dover Books on Mathematics|first1=Anthony|last1=Ralston|first2=Philip|last2=Rabinowitz|publisher=Courier Dover Publications|year=2012|isbn=9780486140292|url=https://books.google.com/books?id=TVq8AQAAQBAJ&pg=PA2|pages=2–4}}.</ref> Computation errors, also called [[numerical error]]s, include both [[truncation error]]s and roundoff errors. \n\nWhen a sequence of calculations with an input involving roundoff error are made, errors may accumulate, sometimes dominating the calculation. In [[ill-conditioned]] problems, significant error may accumulate.<ref>{{citation|title=MATLAB Programming with Applications for Engineers|first=Stephen|last=Chapman|publisher=Cengage Learning|year=2012|isbn=9781285402796|url=https://books.google.com/books?id=of8KAAAAQBAJ&pg=PA454|page=454}}.</ref>\n\nIn short, there are two major facets of roundoff errors involved in numerical calculations<ref name=\"Chapra_2012\">{{cite book |last1=Chapra |first1=Steven |title=Applied Numerical Methods with MATLAB for Engineers and Scientists |publisher=The McGraw-Hill Companies, Inc.|year=2012|isbn=9780073401102 |edition=3rd}}</ref>:\n# Digital computers have magnitude and precision limits on their ability to represent numbers.\n# Certain numerical manipulations are highly sensitive to roundoff errors. This can result from both mathematical considerations as well as from the way in which computers perform arithmetic operations.\n\n== Representation error ==\n\nThe error introduced by attempting to represent a number using a finite string of digits is a form of roundoff error called '''representation error'''.<ref name=\"Laplante_2000\">{{cite book |title=Dictionary of Computer Science, Engineering and Technology |first=Philip A. |last=Laplante |publisher=[[CRC Press]] |year=2000 |isbn=9780849326912 |page=420 |url=https://books.google.com/books?id=U1M3clUwCfEC&pg=PA420}}.</ref> Here are some examples of representation error in decimal representations:\n\n{| class=\"wikitable\" style=\"margin:1em auto\"\n! Notation\n! Representation\n! Approximation\n! Error\n|-\n|1/7 || 0.{{overline|142&nbsp;857}} || 0.142&nbsp;857 || 0.000&nbsp;000&nbsp;{{overline|142&nbsp;857}}</span>\n|-\n|[[Natural logarithm|ln 2]] || 0.693&nbsp;147&nbsp;180&nbsp;559&nbsp;945&nbsp;309&nbsp;41... &nbsp; || 0.693&nbsp;147 || 0.000&nbsp;000&nbsp;180&nbsp;559&nbsp;945&nbsp;309&nbsp;41...\n|-\n|[[Logarithm|log<sub>10</sub> 2]] || 0.301&nbsp;029&nbsp;995&nbsp;663&nbsp;981&nbsp;195&nbsp;21... &nbsp; || 0.3010 || 0.000&nbsp;029&nbsp;995&nbsp;663&nbsp;981&nbsp;195&nbsp;21...\n|-\n|[[cube root|{{radic|2|3}}]] || 1.259&nbsp;921&nbsp;049&nbsp;894&nbsp;873&nbsp;164&nbsp;76... &nbsp; || 1.25992 || 0.000&nbsp;001&nbsp;049&nbsp;894&nbsp;873&nbsp;164&nbsp;76...\n|-\n|[[square root|{{radic|2}}]] || 1.414&nbsp;213&nbsp;562&nbsp;373&nbsp;095&nbsp;048&nbsp;80... &nbsp; || 1.41421 || 0.000&nbsp;003&nbsp;562&nbsp;373&nbsp;095&nbsp;048&nbsp;80...\n|-\n|[[E (mathematical constant)|''e'']] || 2.718&nbsp;281&nbsp;828&nbsp;459&nbsp;045&nbsp;235&nbsp;36... &nbsp; || 2.718&nbsp;281&nbsp;828&nbsp;459&nbsp;045 &nbsp; || 0.000&nbsp;000&nbsp;000&nbsp;000&nbsp;000&nbsp;235&nbsp;36...\n|-\n|[[Pi|''π'']] || 3.141&nbsp;592&nbsp;653&nbsp;589&nbsp;793&nbsp;238&nbsp;46... &nbsp; || 3.141&nbsp;592&nbsp;653&nbsp;589&nbsp;793 || 0.000&nbsp;000&nbsp;000&nbsp;000&nbsp;000&nbsp;238&nbsp;46...\n|}\n\nIncreasing the number of digits allowed in a representation reduces the magnitude of possible roundoff errors, but any representation limited to finitely many digits will still cause some degree of roundoff error for [[Countable|uncountably many]] real numbers. Additional digits used for intermediary steps of a calculation are known as [[guard digit]]s.<ref name=\"Higham_2002\">{{cite book |title=Accuracy and Stability of Numerical Algorithms |edition=2 |first=Nicholas John |author-link=Nicholas John Higham |last=Higham |publisher=[[Society for Industrial and Applied Mathematics]] (SIAM) |year=2002 |isbn=9780898715217 |pages=43–44 |url=https://books.google.com/books?id=epilvM5MMxwC&pg=PA43}}.</ref>\n\nRounding multiple times can cause error to accumulate.<ref name=\"Volkov_1990\">{{cite book |title=Numerical Methods |first=E. A. |last=Volkov |publisher=[[Taylor & Francis]] |year=1990 |isbn=9781560320111 |page=24 |url=https://books.google.com/books?id=ubfrNN8GGOIC&pg=PA24}}.</ref> For example, if 9.945309 is rounded to two decimal places (9.95), then rounded again to one decimal place (10.0), the total error is 0.054691. Rounding 9.945309 to one decimal place (9.9) in a single step introduces less error (0.045309). This commonly occurs when performing arithmetic operations (See [[Loss of significance|Loss of Significance]]).\n\n== Floating-point number system ==\n\nCompared with the [[fixed-point arithmetic|fixed-point number system]], the [[floating-point arithmetic | floating-point number system]] is more efficient in representing real numbers so it is widely used in modern computers. While the real numbers <math>\\mathbb{R}</math> are infinite and continuous, a floating-point number system <math>F</math> is finite and discrete. Thus, representation error, which leads to roundoff error, occurs under the floating-point number system. \n\n=== Notation of floating-point number system ===\nA floating-point number system <math>F</math> is characterized by <math>4</math> integers:\n:<math> \\beta </math>: base or radix\n:<math>p</math>: precision\n:<math> [L, U] </math>: exponent range, where <math>L</math> is the lower bound and <math>U</math> is the upper bound\n\n* Any <math>x \\in F</math> has the following form: \n:<math> x = \\pm (\\underbrace{d_{0}.d_{1}d_{2}\\ldots d_{p-1}}_\\text{mantissa})_{\\beta}  \\times \\beta ^{\\overbrace{E}^\\text{exponent}} = \\pm d_{0}\\times \\beta ^{E}+d_{1}\\times \\beta ^{E-1}+\\ldots+ d_{p-1}\\times \\beta ^{E-(p-1)}</math>\n:where <math>d_{i}</math> is an integer such that <math>0 \\leq d_{i} \\leq \\beta-1</math> for <math>i=0, 1, \\ldots, p-1</math>, and <math>E</math> is an integer such that <math>L \\leq E \\leq U</math>.\n\n=== Normalized floating-number system ===\n\n* A floating-point number system is normalized if the leading digit <math>d_{0}</math> is always nonzero unless the number is zero. <ref name=\"Forrester_2018\"/> Since the mantissa is <math>d_{0}.d_{1}d_{2}\\ldots d_{p-1}</math>, the mantissa of a nonzero number in a normalized system satisfies <math>1 \\leq \\text{ mantissa } < \\beta</math>. Thus, the normalized form of a nonzero [[Institute of Electrical and Electronics Engineers|IEEE]] floating-point number is <math>\\pm 1.bb \\ldots b \\times 2^{E}</math> where <math>b \\in {0, 1}</math>. In binary, the leading digit is always <math>1</math> so it is not written out and is called the implicit bit. Then we can get an extra bit of precision so that the roundoff error caused by representation error is reduced. \n\n* Since floating-point number system <math>F</math> is finite and discrete, it cannot represent all real numbers which means infinite real numbers can only be approximated by some finite numbers through [[rounding|rounding rule]]s. We denote the floating-point approximation of a given real number <math>x</math> by <math>fl(x)</math>.\n** The total number of normalized floating-point numbers is \n::<math>2(\\beta -1)\\beta^{p-1} (U-L+1)+1</math>, where\n::: <math>2</math> counts choice of sign, being positive or negative\n::: <math>(\\beta -1)</math> counts choice of the leading digit\n::: <math>\\beta^{p-1}</math> counts remaining mantissa\n::: <math>U-L+1</math> counts choice of exponents\n::: <math>1</math> counts the case when the number is <math>0</math>.\n\n=== IEEE standard ===\n\nWe{{who|date=April 2019}} will focus on the [[Institute of Electrical and Electronics Engineers|IEEE]] standard since it is adopted universally after it was established in 1985. In this standard, the base is binary, i.e. <math>\\beta = 2</math>, and normalization is used. The [[Institute of Electrical and Electronics Engineers|IEEE]] standard stores the sign, exponent, and mantissa in separate fields of a floating point word, each of which has a fixed width (number of bits). The two most commonly used levels of precision for floating-point numbers are single precision and double precision. \n{| class=\"wikitable\" style=\"margin:1em auto\"\n! Precision\n! Sign (bits)\n! Exponent (bits)\n! Mantissa (bits)\n|-\n|Single || 1 || 8 || 23 \n|-\n|Double || 1 || 11 || 52\n|}\n\n== Machine epsilon ==\n\n[[Machine epsilon]] can be used to measure the level of roundoff error in the floating-point number system. Here are two different definitions. <ref name=\"Forrester_2018\"/>\n\n* The [[Machine epsilon]], denoted <math>\\epsilon_{mach}</math>, is the maximum possible [[approximation error|absolute relative error]] in representing a nonzero real number <math>x</math> in a floating-point number system. \n:<math>\\epsilon_{mach} = \\max_{x} \\frac{|x-fl(x)|}{|x|}</math>\n\n* The [[Machine epsilon]], denoted <math>\\epsilon_{mach}</math>, is the smallest number <math>\\epsilon</math> such that <math>fl(1+\\epsilon) > 1</math>. Thus, <math>fl(1+\\delta)=fl(1)=1</math> whenever <math>|\\delta| < \\epsilon_{mach}</math>. \n\n== Roundoff error under different rounding rules ==\n\nThere are two common rounding rules, round-by-chop and round-to-nearest. The [[Institute of Electrical and Electronics Engineers|IEEE]] standard uses round-to-nearest. \n\n* '''Round-by-chop''': The base-<math>\\beta</math> expansion of <math>x</math> is truncated after the <math>(p-1)^{th}</math> digit. \n** This rounding rule is biased because it always moves the result toward zero.\n\n* '''Round-to-nearest''': We set <math>fl(x)</math> to the nearest floating-point number to <math>x</math>. When there is a tie, we use the floating-point number whose last stored digit is even. \n** For [[Institute of Electrical and Electronics Engineers|IEEE]] standard where the base <math>\\beta</math> is <math>2</math>, this means when there is a tie we round so that the last digit is equal to <math>0</math>. \n** This rounding rule is more accurate but more computationally expensive. \n** Rounding so that the last stored digit is even when there is a tie ensures that we do not round up or down systematically. This is to try to avoid the possibility of an unwanted slow drift in long calculations due simply to a biased rounding. \n\n* The following example illustrates the level of roundoff error under the two rounding rules. <ref name=\"Forrester_2018\"/> The rounding rule, round-to-nearest, leads to less roundoff error in general. \n{| class=\"wikitable\" style=\"margin:1em auto\"\n! x\n! Round-by-chop\n! Roundoff Error\n! Round-to-nearest\n! Roundoff Error\n|-\n|1.649 || 1.6 || 0.049 || 1.6 || 0.049\n|-\n|1.650 || 1.6 || 0.050 || 1.6 || 0.050\n|-\n|1.651 || 1.6 || 0.051 || 1.7 || -0.049 \n|-\n|1.699 || 1.6 || 0.099 || 1.7 || -0.001\n|-\n|1.749 || 1.7 || 0.049 || 1.7 || 0.049\n|-\n|1.750 || 1.7 || 0.050 || 1.8 || -0.050\n|}\n\n=== Calculating roundoff error in IEEE standard ===\n\nSuppose we use round-to-nearest and IEEE double precision.\n \n* Example: the decimal number <math>(9.4)_{10}=(1001.{\\overline{0110}})_{2}</math> can be rearranged into \n:<math>+1.\\underbrace{0010110011001100110011001100110011001100110011001100}_\\text{52 bits}110 \\ldots \\times 2^{3}</math> \nSince the <math>53^{rd}</math> bit to the right of the binary point is a <math>1</math> and is followed by other nonzero bits, the round-to-nearest rule requires rounding up, that is, add <math>1</math> bit to the <math>52^{nd}</math> bit. Thus, the normalized floating-point representation in [[Institute of Electrical and Electronics Engineers|IEEE]] standard of <math>9.4</math> is \n:<math>fl(9.4)=1.0010110011001100110011001100110011001100110011001101 \\times 2^{3}</math>. \n\n* Now we can calculate the roundoff error when representing <math>9.4</math> with <math>fl(9.4)</math>. \nWe derived this representation by discarding the infinite tail \n:<math>0.{\\overline{1100}} \\times 2^{-52}\\times 2^{3} = 0.{\\overline{0110}} \\times 2^{-51} \\times 2^{3}=0.4 \\times 2^{-48}</math> \nfrom the right tail and then added <math>1 \\times 2^{-52} \\times 2^{3}=2^{-49}</math> in the rounding step. \n:Then <math>fl(9.4) = 9.4-0.4 \\times 2^{-48} + 2^{-49} = 9.4+(0.2)_{10} \\times 2^{-49}</math>. \n:Thus, the roundoff error is <math>(0.2 \\times 2^{-49})_{10}</math>.\n\n=== Measuring roundoff error by using machine epsilon ===\n\nWe can use [[machine epsilon]] <math>\\epsilon_{mach}</math> to measure the level of roundoff error when using the two rounding rules above. Below are the formulas and corresponding proof <ref name=\"Forrester_2018\"/>. The first definition of [[machine epsilon]] is used here. \n\n==== Theorem ====\n# Round-by-chop: <math>\\epsilon_{mach}=\\beta^{1-p}</math>\n# Round-to-nearest: <math>\\epsilon_{mach}=\\frac{1}{2}\\beta^{1-p}</math>\n\n==== Proof ====\nLet <math>x=d_{0}.d_{1}d_{2} \\ldots d_{p-1}d_{p} \\ldots \\times \\beta^{n} \\in \\mathbb{R}</math> where <math>n \\in [L, U]</math>, and let <math>fl(x)</math> be the floating-point representation of <math>x</math>.  \nSince we are using round-by-chop, we have that\n<math> \\begin{align}\n\\frac{|x-fl(x)|}{|x|} &= \\frac{|d_{0}.d_{1}d_{2}\\ldots d_{p-1}d_{p}d_{p+1}\\ldots \\times \\beta^{n} - d_{0}.d_{1}d_{2}\\ldots d_{p-1} \\times \\beta^{n}|}{|d_{0}.d_{1}d_{2}\\ldots \\times \\beta^{n}|}\\\\\n&= \\frac{|d_{p}.d_{p+1} \\ldots \\times \\beta^{n-p}|}{|d_{0}.d_{1}d_{2}\\ldots \\times \\beta^{n}|}\\\\\n&= \\frac{|d_{p}.d_{p+1}d_{p+2}\\ldots|}{|d_{0}.d_{1}d_{2}\\ldots|} \\times \\beta^{-p}\n\\end{align} </math>\n* In order to determine the maximum of this quantity, we need to find the maximum of the numerator and the minimum of the denominator. Since <math>d_{0}\\neq 0</math> (normalized system), the minimum value of the denominator is <math>1</math>. The numerator is bounded above by <math>(\\beta-1).(\\beta-1){\\overline{(\\beta-1)}}=\\beta </math>. Thus, <math>\\frac{|x-fl(x)|}{|x|} \\leq \\frac{\\beta}{1} \\times \\beta^{-p} = \\beta^{1-p}</math>. Therefore, <math>\\epsilon=\\beta^{1-p}</math> for round-by-chop.\nThe proof for round-to-nearest is similar.\n* Note that the first definition of [[machine epsilon]] is not quite equivalent to the second definition when using the round-to-nearest rule but it is equivalent for round-by-chop.\n\n== Roundoff error caused by floating-point arithmetic ==\n\nEven if some numbers can be represented exactly by floating-point numbers and such numbers are called '''machine numbers''', performing floating-point arithmetic may lead to roundoff error in the final result. \n\n=== Addition ===\n\nMachine addition consists of lining up the decimal points of the two numbers to be added, adding them, and then storing the result again as a floating-point number. The addition itself can be done in higher precision but the result must be rounded back to the specified precision, which may lead to roundoff error. <ref name=\"Forrester_2018\"/>\n\nFor example, adding <math>1</math> to <math>2^{-53}</math> in [[Institute of Electrical and Electronics Engineers|IEEE]] double precision as follows, \n\n<math>\\begin{align}\n1.00\\ldots 0 \\times 2^{0} + 1.00\\ldots 0 \\times 2^{-53} &= 1.\\underbrace{00\\ldots 0}_\\text{52 bits} \\times 2^{0} + 0.\\underbrace{00\\ldots 0}_\\text{52 bits}1 \\times 2^{0}\\\\\n&= 1.\\underbrace{00\\ldots 0}_\\text{52 bits}1\\times 2^{0}\n\\end{align}</math>\n* This is saved as <math>1.\\underbrace{00\\ldots 0}_\\text{52 bits}\\times 2^{0}</math> since round-to-nearest is used in [[Institute of Electrical and Electronics Engineers|IEEE]] standard. Therefore, <math>1+2^{-53}</math> is equal to <math>1</math> in [[Institute of Electrical and Electronics Engineers|IEEE]] double precision and the roundoff error is <math>2^{-53}</math>. \n\nFrom this example, we can see that roundoff error can be introduced when doing the addition of a large number and a small number because the shifting of decimal points in the mantissas to make the exponents match may cause the loss of some digits.\n\n=== Multiplication ===\n\nIn general, the product of <math>2</math> <math>p</math>-digit mantissas contains up to <math>2p</math> digits, so the result might not fit in the mantissa. <ref name=\"Forrester_2018\"/> Thus roundoff error will be involved in the result.\n* For example, consider a normalized floating-point number system with the base <math>\\beta=10</math> and the mantissa digits are at most <math>2</math>. Then <math>fl(77) = 7.7 \\times 10</math> and <math>fl(88) = 8.8 \\times 10</math>. Note that <math>77 \\times 88=6776</math> but <math>fl(6776) = 6.7 \\times 10^{3}</math> since there at most <math>2</math> mantissa digits. The roundoff error would be <math>6776 - fl(6776)  = 6776 - 6.7 \\times 10^{3}=76</math>.  \n\n=== Division ===\n\nIn general, the quotient of <math>2</math> <math>p</math>-digit mantissas may contain more than <math>p</math>-digits. <ref name=\"Forrester_2018\"/> Thus roundoff error will be involved in the result.\n* For example, if we still use the normalized floating-point number system above, then <math>1/3=0.333 \\ldots</math> but <math>fl(1/3)=fl(0.333 \\ldots)=3.3 \\times 10^{-1}</math>. So, the tail <math>0.333 \\ldots - 3.3 \\times 10^{-1}=0.00333 \\ldots </math> is cut off.\n\n=== Subtractive Cancellation ===\n\nThe subtracting of two nearly equal numbers is called '''subtractive cancellation'''. <ref name=\"Forrester_2018\"/> \n* When the leading digits are cancelled, the result may be too small to be represented exactly and it will just be represented as <math>0</math>. \n** For example, let <math>|\\epsilon| < \\epsilon_{mach}</math> and the second definition of [[machine epsilon]] is used here. What is the solution to <math>(1+\\epsilon) - (1-\\epsilon)</math>? <br /> We know that <math>1+\\epsilon</math> and <math>1-\\epsilon</math> are nearly equal numbers, and <math>(1+\\epsilon) - (1-\\epsilon)=1+\\epsilon-1+\\epsilon=2\\epsilon</math>. However, in the floating-point number system, <math>fl((1+\\epsilon) - (1-\\epsilon))=fl(1+\\epsilon)-fl(1-\\epsilon)=1-1=0</math>. We can see that <math>2\\epsilon</math> is too small so it is represented as <math>0</math>.\n* Even if the result is representable, the result is still regarded as \"garbage\". We do not have much faith in this value because the most uncertainty in any floating-point number is the digits on the far right. \n** For example, <math>1.99999 \\times 10 ^{2}- 1.99998 \\times 10^{2} = 0.00001\\times10^{2} =1 \\times 10^{-5}\\times 10^{2}=1\\times10^{-3}</math>. The result <math>1\\times10^{-3}</math> is clearly representable, but we do not have much faith in it.\n\n== Accumulation of roundoff error == \n\nErrors can be magnified or accumulated when a sequence of calculations is applied on an initial input with roundoff error due to inexact representation. \n\n=== Unstable algorithms ===\n\nAn algorithm or numerical process is called '''stable''' if small changes in the input only produce small changes in the output and it is called '''unstable''' if large changes in the output\nare produced. <ref name=\"Collins_2005\">{{cite web |last1=Collins |first1=Charles |title=Condition and Stability |url=https://www.math.utk.edu/~ccollins/M577/Handouts/cond_stab.pdf |website=Department of Mathematics in University of Tennessee |year=2005|accessdate=28 October 2018}}</ref>\n\nA sequence of calculations normally occur when running some [[algorithm]]. The amount of error in the result depends on the [[Numerical stability|stability of the algorithm]]. Roundoff error will be magnified by unstable algorithms. \n\nFor example, <math>y_{n}=\\int_0^1 \\, \\frac{x^{n}}{x+5} dx</math> for <math>n = 1, 2, \\ldots, 8</math> with <math>y_{0}</math> given. It is easy to show that <math>y_{n}=\\frac{1}{n}-5y_{n-1}</math>. Suppose <math>y_{0}</math> is our initial value and has a small representation error <math>\\epsilon</math>, which means the initial input to this algorithm is <math>y_{0}+\\epsilon</math> instead of <math>y_{0}</math>. Then the algorithm does the following sequence of calculations. \n:<math>\\begin{align}\n y_{1} &= 1-5(y_{0}+\\epsilon) = 1-5y_{0}-5\\epsilon\\\\\n y_{2} &= \\frac{1}{2}-5(1-5y_{0}-5\\epsilon) = \\frac{1}{2}-5+25y_{0}+5^{2}\\epsilon\\\\\n \\vdots\\\\\n y_{n} &= \\ldots + 5^{n}\\epsilon\n\\end{align}</math>\n\nThe roundoff error is amplified in succeeding calculations so this algorithm is unstable.\n\n=== Ill-conditioned problems ===\n\n[[File:Comparison1.jpg|thumb|Comparison1]]\n[[File:Comparison 2.jpg|thumb|Comparison 2]]\n\nEven if a stable [[algorithm]] is used, the solution to a problem is still inaccurate due to the accumulation of roundoff error when the problem itself is '''ill-conditioned'''. \n\nThe [[condition number]] of a problem is the ratio of the relative change in the solution to the relative change in the input. <ref name=\"Forrester_2018\"/> A problem is '''well-conditioned''' if small relative changes in input result in small relative changes in the solution. Otherwise. the problem is called '''ill-conditioned'''. <ref name=\"Forrester_2018\"/> In other words, a problem is called '''ill-conditioned''' if its condition number is \"much larger\" than <math>1</math>. \n\nThe [[condition number]] is introduced as a measure of the roundoff errors that can result when solving ill-conditioned problems. <ref name=\"Chapra_2012\"/>\n\nFor example, higher-order polynomials tend to be very '''ill-conditioned''', that is, they tend to be highly sensitive to roundoff error. <ref name=\"Chapra_2012\"/>\n\nIn 1901, [[Carl Runge]] published a study on the dangers of higher-order polynomial interpolation. He looked at the following simple-looking function:\n:<math>f(x) = \\frac{1}{1+25x^{2}}</math>\nwhich is now called [[Runge's phenomenon|Runge's function]]. He took equidistantly spaced data points from this function over the interval <math>[-1, 1]</math>. He then used interpolating polynomials of increasing order and found that as he took more points, the polynomials and the original curve differed considerably as illustrated in Figure “Comparison1” and Figure “Comparison 2”. Further, the situation deteriorated greatly as the order was increased. As shown in Figure “Comparison 2”, the fit has gotten even worse, particularly at the ends of the interval. \n\nClick on the figures in order to see the full descriptions.\n\n=== Real world example: Patriot Missile Failure due to magnification of roundoff error ===\n\n[[File:Patriot Missile.png|thumb|American Patriot Missile]]\n\nOn February 25, 1991, during the Gulf War, an American Patriot Missile battery in Dharan, Saudi Arabia, failed to intercept an incoming Iraqi Scud missile. The Scud struck an American Army barracks and killed 28 soldiers. A report of the General Accounting office, GAO/IMTEC-92-26, entitled Patriot Missile Defense: Software Problem Led to System Failure at Dhahran, Saudi Arabia reported on the cause of the failure. It turns out that the cause was an inaccurate calculation of the time since boot due to computer arithmetic errors. Specifically, the time in tenths of second as measured by the system's internal clock was multiplied by 1/10 to produce the time in seconds. This calculation was performed using a 24-bit fixed point register. In particular, the value 1/10, which has a non-terminating binary expansion, was chopped at 24 bits after the radix point. The small chopping error, when multiplied by the large number giving the time in tenths of a second, lead to a significant error. Indeed, the Patriot battery had been up around 100 hours, and an easy calculation shows that the resulting time error due to the magnified chopping error was about 0.34 seconds. (The number 1/10 equals <math>1/2^{4}+1/2^{5}+1/2^{8}+1/2^{9}+1/2^{12}+1/2^{13}+\\ldots</math>. In other words, the binary expansion of 1/10 is <math>0.0001100110011001100110011001100 \\ldots</math>. Now the 24 bit register in the Patriot stored instead <math>0.00011001100110011001100</math> introducing an error of <math>0.0000000000000000000000011001100 \\ldots</math> binary, or about <math>0.000000095</math> decimal. Multiplying by the number of tenths of a second in <math>100</math> hours gives <math>0.000000095 \\times 100 \\times 60 \\times 60 \\times 10=0.34</math>). A Scud travels at about 1,676 meters per second, and so travels more than half a kilometer in this time. This was far enough that the incoming Scud was outside the \"range gate\" that the Patriot tracked. Ironically, the fact that the bad time calculation had been improved in some parts of the code, but not all, contributed to the problem, since it meant that the inaccuracies did not cancel.<ref name=\"Arnold_2000\">{{cite web |last1=Arnold |first1=Douglas |title=The Patriot Missile Failure |url=http://ta.twi.tudelft.nl/users/vuik/wi211/disasters.html |accessdate=29 October 2018}}</ref>\n\n== See also ==\n*[[Precision (arithmetic)]]\n*[[Truncation]]\n*[[Rounding]]\n*[[Loss of significance]]\n*[[Floating point]]\n*[[Kahan summation algorithm]]\n*[[Machine epsilon]]\n*[[Wilkinson's polynomial]]\n\n==References==\n{{reflist}}\n\n== External links ==\n* [http://mathworld.wolfram.com/RoundoffError.html Roundoff Error] at MathWorld.\n*{{cite journal |first=David |last=Goldberg |author-link=David Goldberg (PARC) |title=What Every Computer Scientist Should Know About Floating-Point Arithmetic |journal=[[ACM Computing Surveys]] |date=March 1991 |volume=23 |issue=1 |pages=5–48 |doi=10.1145/103162.103163 |url=http://perso.ens-lyon.fr/jean-michel.muller/goldberg.pdf |access-date=2016-01-20}} ([http://www.validlab.com/goldberg/paper.pdf], [http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html])\n* [http://www.devtopics.com/20-famous-software-disasters/ 20 Famous Software Disasters]\n\n{{DEFAULTSORT:RoundOff Error}}\n[[Category:Numerical analysis]]\n\n[[sv:Avrundningsfel]]"
    },
    {
      "title": "Runge–Kutta methods",
      "url": "https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods",
      "text": "In [[numerical analysis]], the '''Runge–Kutta methods'''  are a family of [[Explicit and implicit methods|implicit and explicit]] iterative methods, which include the well-known routine called the [[Euler method|Euler Method]], used in [[temporal discretization]] for the approximate solutions of [[ordinary differential equation]]s.<ref>DEVRIES, Paul L. ; HASBUN, Javier E. A first course in computational physics. Second edition. Jones and Bartlett Publishers: 2011. p. 215.</ref> These methods were developed around 1900 by the German mathematicians [[Carl Runge]] and [[Wilhelm Kutta]].\n\nSee the article on [[numerical ordinary differential equations|numerical methods for ordinary differential equations]] for more background and other methods. See also [[List of Runge–Kutta methods]].\n[[File:Runge-kutta.svg|thumb|right|Comparison of the Runge-Kutta methods for the differential equation y'=sin(t)^2*y ( red is the exact solution)]]\n\n==The Runge–Kutta method==\n\n[[File:Runge-Kutta slopes.svg|thumb|Slopes used by the classical Runge-Kutta method|alt=|300x300px]]\nThe most widely known member of the Runge–Kutta family is generally referred to as \"RK4\", the \"classical Runge–Kutta method\" or simply as \"the Runge–Kutta method\".\n\nLet an [[initial value problem]] be specified as follows:\n\n:<math> \\dot{y} = f(t, y), \\quad y(t_0) = y_0. </math>\n\nHere <math>y</math> is an unknown function (scalar or vector) of time <math>t</math>, which we would like to approximate; we are told that <math>\\dot{y}</math>, the rate at which <math>y</math> changes, is a function of <math>t</math> and of <math>y</math> itself. At the initial time <math>t_0</math> the corresponding <math>y</math> value is <math>y_0</math>. The function <math>f</math> and the data <math>t_0</math>,  <math>y_0</math> are given.\n\nNow pick a step-size ''h'' &gt; 0 and define\n\n:<math>\\begin{align}\ny_{n+1} &= y_n + \\tfrac{1}{6}\\left(k_1 + 2k_2 + 2k_3 + k_4 \\right),\\\\\nt_{n+1} &= t_n + h \\\\\n\\end{align}</math>\n\nfor ''n'' = 0, 1, 2, 3, ..., using<ref>{{harvnb|Press|Teukolsky|Vetterling|Flannery|2007|p=908}}; {{harvnb|Süli|Mayers|2003|p=328}}</ref>\n:<math>\n\\begin{align}\n k_1 &= h\\ f(t_n, y_n), \\\\\n k_2 &= h\\ f\\left(t_n + \\frac{h}{2}, y_n + \\frac{k_1}{2}\\right), \\\\ \n k_3 &= h\\ f\\left(t_n + \\frac{h}{2}, y_n + \\frac{k_2}{2}\\right), \\\\\n k_4 &= h\\ f\\left(t_n + h, y_n + k_3\\right).\n\\end{align}\n</math>\n:''(Note: the above equations have different but equivalent definitions in different texts).''<ref name=notation>{{harvtxt|Atkinson|1989|p=423}}, {{harvtxt|Hairer|Nørsett|Wanner|1993|p=134}}, {{harvtxt|Kaw|Kalu|2008|loc=§8.4}} and {{harvtxt|Stoer|Bulirsch|2002|p=476}} leave out the factor ''h'' in the definition of the stages. {{harvtxt|Ascher|Petzold|1998|p=81}}, {{harvtxt|Butcher|2008|p=93}} and {{harvtxt|Iserles|1996|p=38}} use the ''y'' values as stages.</ref>\n\nHere <math>y_{n+1}</math> is the RK4 approximation of <math>y(t_{n+1})</math>, and the next value (<math>y_{n+1}</math>) is determined by the present value (<math>y_n</math>) plus the [[weighted average]] of four increments, where each increment is the product of the size of the interval, ''h'', and an estimated slope specified by function ''f'' on the right-hand side of the differential equation.\n* <math>k_1</math> is the increment based on the slope at the beginning of the interval, using <math> y </math> ([[Euler's method]]);\n* <math>k_2</math> is the increment based on the slope at the midpoint of the interval, using <math> y </math> and <math> k_1 </math>;\n* <math>k_3</math> is again the increment based on the slope at the midpoint, but now using <math> y </math> and <math> k_2 </math>;\n* <math>k_4</math> is the increment based on the slope at the end of the interval, using <math> y </math> and <math> k_3 </math>.\n\nIn averaging the four increments, greater weight is given to the increments at the midpoint. If <math>f</math> is independent of <math>y</math>, so that the differential equation is equivalent to a simple integral, then RK4 is [[Simpson's rule]].<ref name=\"Süli 2003 328\">{{harvnb|Süli|Mayers|2003|p=328}}</ref>\n\nThe RK4 method is a fourth-order method, meaning that the [[Truncation error (numerical integration)|local truncation error]] is [[Big O notation|on the order of]] <math>O(h^5)</math>, while the  [[Truncation error (numerical integration)|total accumulated error]] is on the order of <math>O(h^4)</math>.\n\nIn many practical applications the function <math>f</math> is independent of <math>t</math> (so called [[Autonomous system (mathematics)|autonomous system]], or time-invariant system, especially in physics), and their increments are not computed at all and not passed to function <math>f</math>, with only the final formula for <math>t_{n+1}</math> used.\n\n==Explicit Runge–Kutta methods==\nThe family of [[Explicit and implicit methods|explicit]] Runge–Kutta methods is a generalization of the RK4 method mentioned above. It is given by\n:<math> y_{n+1} = y_n + h \\sum_{i=1}^s b_i k_i, </math>\nwhere<ref>{{harvnb|Press|Teukolsky|Vetterling|Flannery|2007|p=907}}</ref>\n:<math>\n\\begin{align}\n k_1 & = f(t_n, y_n), \\\\\n k_2 & = f(t_n+c_2h, y_n+h(a_{21}k_1)), \\\\\n k_3 & = f(t_n+c_3h, y_n+h(a_{31}k_1+a_{32}k_2)), \\\\\n     & \\ \\ \\vdots \\\\\n k_s & = f(t_n+c_sh, y_n+h(a_{s1}k_1+a_{s2}k_2+\\cdots+a_{s,s-1}k_{s-1})).\n\\end{align}\n</math>\n:''(Note: the above equations may have different but equivalent definitions in some texts).''<ref name=notation />\n\nTo specify a particular method, one needs to provide the integer ''s'' (the number of stages), and the coefficients ''a<sub>ij</sub>'' (for 1 ≤ ''j'' < ''i'' ≤ ''s''), ''b<sub>i</sub>'' (for ''i'' = 1, 2, ..., ''s'') and ''c<sub>i</sub>'' (for ''i'' = 2, 3, ..., ''s''). The matrix [''a<sub>ij</sub>''] is called the ''Runge–Kutta matrix'', while the ''b<sub>i</sub>'' and ''c<sub>i</sub>'' are known as the ''weights'' and the ''nodes''.<ref>{{harvnb|Iserles|1996|p=38}}</ref> These data are usually arranged in a mnemonic device, known as a ''Butcher tableau'' (after [[John C. Butcher]]):\n\n:{| style=\"text-align: center\" cellspacing=\"0\" cellpadding=\"3\"\n| style=\"border-right:1px solid;\" | <math> 0 </math>\n|-\n| style=\"border-right:1px solid;\" | <math> c_2 </math> || <math> a_{21} </math>\n|-\n| style=\"border-right:1px solid;\" | <math> c_3 </math> || <math> a_{31} </math> || <math> a_{32} </math>\n|-\n| style=\"border-right:1px solid;\" | <math> \\vdots </math> || <math> \\vdots </math> || || <math> \\ddots </math>\n|-\n| style=\"border-right:1px solid; border-bottom:1px solid;\" | <math> c_s </math>\n| style=\"border-bottom:1px solid;\" | <math> a_{s1} </math>\n| style=\"border-bottom:1px solid;\" | <math> a_{s2} </math>\n| style=\"border-bottom:1px solid;\" | <math> \\cdots </math>\n| style=\"border-bottom:1px solid;\" | <math> a_{s,s-1} </math> || style=\"border-bottom:1px solid;\" |\n|-\n| style=\"border-right:1px solid;\" | || <math> b_1 </math> || <math> b_2 </math> || <math> \\cdots </math> || <math> b_{s-1} </math> || <math> b_s </math>\n|}\n\nThe Runge–Kutta method is consistent if\n:<math>\\sum_{j=1}^{i-1} a_{ij} = c_i \\text{ for } i=2, \\ldots, s.</math>\nThere are also accompanying requirements if one requires the method to have a certain order ''p'', meaning that the local truncation error is O(''h<sup>p</sup>''<sup>+1</sup>). These can be derived from the definition of the truncation error itself. For example, a two-stage method has order 2 if ''b''<sub>1</sub> + ''b''<sub>2</sub> = 1, ''b''<sub>2</sub>''c''<sub>2</sub> = 1/2, and ''a''<sub>21</sub> = ''c''<sub>2</sub>.<ref>{{harvnb|Iserles|1996|p=39}}</ref>\n\nIn general, if an explicit <math>s</math>-stage Runge–Kutta method has order <math>p</math>, then it can be proven that the number of stages must satisfy <math>s \\ge p</math>, and if <math>p \\ge 5</math>, then <math>s \\ge p+1</math>.<ref>{{harvnb|Butcher|2008|p=187}}</ref>\nHowever, it is not known whether these bounds are <i>sharp</i> in all cases; for example, all known methods of order 8 have at least 11 stages, though it is possible that there are methods with fewer stages. (The bound above suggests that there could be a method with 9 stages; but it could also be that the bound is simply not sharp.) Indeed, it is an open problem\nwhat the precise minimum number of stages <math>s</math> is for an explicit Runge–Kutta method to have order <math>p</math> in those cases where no methods have yet been discovered that satisfy the bounds above with equality. Some values which are known are:<ref>{{harvnb|Butcher|2008|pp=187–196}}</ref>\n:<math>\n\\begin{array}{c|cccccccc}\np & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\\\\n\\hline\n\\min s & 1 & 2 & 3 & 4 & 6 & 7 & 9 & 11 \n\\end{array} \n</math>\nThe provable bounds above then imply that we can not find methods of orders <math>p=1, 2, \\ldots, 6</math> that require fewer stages than the methods we already know for these orders. However, it is conceivable that we might find a method of order <math>p=7</math> that has only 8 stages, whereas the only ones known today have at least 9 stages as shown in the table.\n\n===Examples===\nThe RK4 method falls in this framework. Its tableau is<ref name=\"Süli 2003 352\">{{harvnb|Süli|Mayers|2003|p=352}}</ref>\n\n:{| style=\"text-align: center\" cellspacing=\"0\" cellpadding=\"3\"\n| style=\"border-right:1px solid;\" | 0\n|-\n| style=\"border-right:1px solid;\" | 1/2 || 1/2\n|-\n| style=\"border-right:1px solid;\" | 1/2 || 0 || 1/2\n|-\n| style=\"border-right:1px solid; border-bottom:1px solid;\" | 1 || style=\"border-bottom:1px solid;\" | 0\n| style=\"border-bottom:1px solid;\" | 0 || style=\"border-bottom:1px solid;\" | 1\n| style=\"border-bottom:1px solid;\" |\n|-\n| style=\"border-right:1px solid;\" | || 1/6 || 1/3 || 1/3 || 1/6\n|}\n\nA slight variation of \"the\" Runge–Kutta method is also due to Kutta in 1901 and is called the 3/8-rule.<ref>{{harvtxt|Hairer|Nørsett|Wanner|1993|p=138}} refer to {{harvtxt|Kutta|1901}}.</ref> The primary advantage this method has is that almost all of the error coefficients are smaller than in the popular method, but it requires slightly more FLOPs (floating-point operations) per time step. Its Butcher tableau is\n\n:{| style=\"text-align: center\" cellspacing=\"0\" cellpadding=\"3\"\n| style=\"border-right:1px solid;\" | 0\n|-\n| style=\"border-right:1px solid;\" | 1/3 || 1/3\n|-\n| style=\"border-right:1px solid;\" | 2/3 || -1/3 || 1\n|-\n| style=\"border-right:1px solid; border-bottom:1px solid;\" | 1 || style=\"border-bottom:1px solid;\" | 1\n| style=\"border-bottom:1px solid;\" | −1 || style=\"border-bottom:1px solid;\" | 1\n| style=\"border-bottom:1px solid;\" |\n|-\n| style=\"border-right:1px solid;\" | || 1/8 || 3/8 || 3/8 || 1/8\n|}\n\nHowever, the simplest Runge–Kutta method is the (forward) [[Euler method]], given by the formula <math> y_{n+1} = y_n + hf(t_n, y_n) </math>. This is the only consistent explicit Runge–Kutta method with one stage. The corresponding tableau is\n\n:{| style=\"text-align: center\" cellspacing=\"0\" cellpadding=\"3\"\n| width=\"10\" style=\"border-right:1px solid; border-bottom:1px solid;\" | 0\n| width=\"10\" style=\"border-bottom:1px solid;\" |\n|-\n| style=\"border-right:1px solid;\" | || 1\n|}\n\n===Second-order methods with two stages===\nAn example of a second-order method with two stages is provided by the [[midpoint method]]:\n:<math> y_{n+1} = y_n + hf\\left(t_n+\\frac{1}{2}h, y_n+\\frac{1}{2}hf(t_n,\\ y_n)\\right). </math>\nThe corresponding tableau is\n\n:{| style=\"text-align: center\" cellspacing=\"0\" cellpadding=\"3\"\n| style=\"border-right:1px solid;\" | 0\n|-\n| style=\"border-right:1px solid; border-bottom:1px solid;\" | 1/2 || style=\"border-bottom:1px solid;\" | 1/2 || style=\"border-bottom:1px solid;\" |\n|-\n| style=\"border-right:1px solid;\" | || 0 || 1\n|}\n\nThe midpoint method is not the only second-order Runge–Kutta method with two stages; there is a family of such methods, parameterized by α and given by the formula<ref>{{harvnb|Süli|Mayers|2003|p=327}}</ref>\n\n:<math> y_{n+1} = y_n + h\\bigl( (1-\\tfrac1{2\\alpha}) f(t_n, y_n) + \\tfrac1{2\\alpha} f(t_n + \\alpha h, y_n + \\alpha h f(t_n, y_n)) \\bigr). </math> \n\nIts Butcher tableau is\n\n:{| style=\"text-align: center\" cellspacing=\"0\" cellpadding=\"8\"\n| style=\"border-right:1px solid;\" | 0\n|-\n| style=\"border-right:1px solid; border-bottom:1px solid;\" | <math>\\alpha</math> || style=\"border-bottom:1px solid; text-align: center;\" | <math>\\alpha</math> || style=\"border-bottom:1px solid;\" |\n|-\n| style=\"border-right:1px solid;\" | || <math>(1-\\tfrac1{2\\alpha})</math> || <math>\\tfrac1{2\\alpha}</math>\n|}\n\nIn this family, <math>\\alpha=\\tfrac12</math> gives the midpoint method, and <math>\\alpha=1</math> is [[Heun's method]].<ref name=\"Süli 2003 328\"/>\n\n==Use==\nAs an example, consider the two-stage second-order Runge–Kutta method with α = 2/3, also known as [[Heun's method#Runge.E2.80.93Kutta method|Ralston method]]. It is given by the tableau\n\n{| cellspacing=\"0\" cellpadding=\"3\"\n| width=\"20\" |  || style=\"border-right:1px solid;\" | 0\n|-\n||| style=\"border-right:1px solid; border-bottom:1px solid;\" | 2/3 || style=\"border-bottom:1px solid;\" | 2/3\n| style=\"border-bottom:1px solid;\" |\n|-\n||| style=\"border-right:1px solid;\" | || 1/4 || 3/4\n|}\n\nwith the corresponding equations\n\n:<math> \\begin{align}\nk_1 &= f(t_n,\\ y_n), \\\\\nk_2 &= f(t_n + \\tfrac{2}{3}h,\\ y_n + \\tfrac{2}{3}h k_1), \\\\\ny_{n+1} &= y_n + h\\left(\\tfrac{1}{4}k_1+\\tfrac{3}{4}k_2\\right).\n\\end{align} </math>\n\nThis method is used to solve the initial-value problem\n:<math> y' = \\tan(y)+1,\\quad y_0=1,\\ t\\in [1, 1.1]</math>\nwith step size ''h'' = 0.025, so the method needs to take four steps.\n\nThe method proceeds as follows:\n\n{| cellpadding=\"8\"\n|-\n| <math>t_0=1 \\colon</math> || ||\n|-\n| || <math>y_0=1</math>\n|-\n| <math>t_1=1.025 \\colon</math>\n|-\n| || <math>y_0 = 1</math> || <math>k_1=2.557407725</math> || <math>k_2 = f(t_0 + \\tfrac23h ,\\ y_0 + \\tfrac23hk_1) = 2.7138981184</math>\n|-\n| ||  colspan=3 |<math>y_1=y_0+h(\\tfrac14k_1 + \\tfrac34k_2)=\\underline{1.066869388}</math>\n|-\n| <math>t_2=1.05 \\colon</math>\n|-\n| || <math>y_1 = 1.066869388</math> || <math>k_1=2.813524695</math> || <math>k_2 = f(t_1 + \\tfrac23h ,\\ y_1 + \\tfrac23hk_1)</math>\n|-\n| ||  colspan=3 | <math>y_2=y_1+h(\\tfrac14k_1 + \\tfrac34k_2)=\\underline{1.141332181}</math>\n|-\n| <math>t_3=1.075 \\colon</math>\n|-\n| || <math>y_2 = 1.141332181</math> || <math>k_1=3.183536647</math> || <math>k_2 = f(t_2 + \\tfrac23h ,\\ y_2 + \\tfrac23hk_1)</math>\n|-\n| ||  colspan=3 | <math>y_3=y_2+h(\\tfrac14k_1 + \\tfrac34k_2)=\\underline{1.227417567}</math>\n|-\n| <math>t_4=1.1 \\colon</math>\n|-\n| || <math>y_3 = 1.227417567 </math> || <math>k_1=3.796866512</math> || <math>k_2 = f(t_3 + \\tfrac23h ,\\ y_3 + \\tfrac23hk_1)</math>\n|-\n| ||  colspan=3 | <math>y_4=y_3+h(\\tfrac14k_1 + \\tfrac34k_2)=\\underline{1.335079087}.</math>\n|}\n\nThe numerical solutions correspond to the underlined values.\n\n==Adaptive Runge–Kutta methods==\nAdaptive methods are designed to produce an estimate of the local truncation error of a single Runge–Kutta step. This is done by having two methods, one with order <math>p</math> and one with order <math>p - 1</math>. These methods are interwoven, i.e., they have common intermediate steps. Thanks to this, estimating the error has little or negligible computational cost compared to a step with the higher-order method.\n\nDuring the integration, the step size is adapted such that the estimated error stays below a user-defined threshold: If the error is too high, a step is repeated with a lower step size; if the error is much smaller, the step size is increased to save time. This results in an (almost) optimal step size, which saves computation time. Moreover, the user does not have to spend time on finding an appropriate step size.\n\nThe lower-order step is given by\n:<math> y^*_{n+1} = y_n + h\\sum_{i=1}^s b^*_i k_i, </math>\nwhere <math>k_i</math> are the same as for the higher-order method. Then the error is\n:<math> e_{n+1} = y_{n+1} - y^*_{n+1} = h\\sum_{i=1}^s (b_i - b^*_i) k_i, </math>\nwhich is <math>O(h^p)</math>. \nThe Butcher tableau for this kind of method is extended to give the values of <math>b^*_i</math>:\n\n{| cellspacing=\"0\" cellpadding=\"3\"\n| width=\"20\" |  || style=\"border-right:1px solid;\" | 0\n|-\n||| style=\"border-right:1px solid;\" | <math> c_2 </math> || <math> a_{21} </math>\n|-\n||| style=\"border-right:1px solid;\" | <math> c_3 </math> || <math> a_{31} </math> || <math> a_{32} </math>\n|-\n||| style=\"border-right:1px solid;\" | <math> \\vdots </math> || <math> \\vdots </math> || || <math> \\ddots </math>\n|-\n||| style=\"border-right:1px solid; border-bottom:1px solid;\" | <math> c_s </math>\n| style=\"border-bottom:1px solid;\" | <math> a_{s1} </math>\n| style=\"border-bottom:1px solid;\" | <math> a_{s2} </math>\n| style=\"border-bottom:1px solid;\" | <math> \\cdots </math>\n| style=\"border-bottom:1px solid;\" | <math> a_{s,s-1} </math> || style=\"border-bottom:1px solid;\" |\n|-\n||| style=\"border-right:1px solid;\" | || <math> b_1 </math> || <math> b_2 </math> || <math> \\cdots </math> || <math> b_{s-1} </math> || <math> b_s </math>\n|-\n||| style=\"border-right:1px solid;\" | || <math> b^*_1 </math> || <math> b^*_2 </math> || <math> \\cdots </math> || <math> b^*_{s-1} </math> || <math> b^*_s </math>\n|}\n\nThe [[Runge–Kutta–Fehlberg method]] has two methods of orders 5 and 4. Its extended Butcher tableau is:\n\n{| cellspacing=\"0\" cellpadding=\"3\"\n| width=\"20\" |  || style=\"border-right:1px solid;\" | 0\n|-\n||| style=\"border-right:1px solid;\" | 1/4 || 1/4\n|-\n||| style=\"border-right:1px solid;\" | 3/8 || 3/32 || 9/32\n|-\n||| style=\"border-right:1px solid;\" | 12/13 || 1932/2197 || −7200/2197 || 7296/2197\n|-\n||| style=\"border-right:1px solid;\" | 1 || 439/216 || −8 || 3680/513 || -845/4104\n|-\n||| style=\"border-right:1px solid; border-bottom:1px solid;\" | 1/2 || style=\"border-bottom:1px solid;\" | −8/27 || style=\"border-bottom:1px solid;\" | 2 || style=\"border-bottom:1px solid;\" | −3544/2565 || style=\"border-bottom:1px solid;\" | 1859/4104 || style=\"border-bottom:1px solid;\" | −11/40 || style=\"border-bottom:1px solid;\" |\n|-\n||| style=\"border-right:1px solid;\" | || 16/135 || 0 || 6656/12825 || 28561/56430 || −9/50 || 2/55\n|-\n||| style=\"border-right:1px solid;\" | || 25/216 || 0 || 1408/2565 || 2197/4104 || −1/5 || 0\n|}\n\nHowever, the simplest adaptive Runge–Kutta method involves combining [[Heun's method]], which is order 2, with the [[Euler method]], which is order 1. Its extended Butcher tableau is:\n\n{| cellspacing=\"0\" cellpadding=\"3\"\n| width=\"20\" |  || style=\"border-right:1px solid;\" | 0\n|-\n||| style=\"border-right:1px solid; border-bottom:1px solid;\" | 1 || style=\"border-bottom:1px solid;\" | 1 || style=\"border-bottom:1px solid;\" |\n|-\n||| style=\"border-right:1px solid;\" | || 1/2 || 1/2\n|-\n||| style=\"border-right:1px solid;\" | || 1 || 0\n|}\n\nOther adaptive Runge–Kutta methods are the [[Bogacki–Shampine method]] (orders 3 and 2), the [[Cash–Karp method]] and the [[Dormand–Prince method]] (both with orders 5 and 4).\n\n==Nonconfluent Runge–Kutta methods==\nA Runge–Kutta method is said to be ''nonconfluent'' <ref>{{harvnb|Lambert|1991|p=278}}</ref> if all the <math>c_i,\\,i=1,2,\\ldots,s</math> are distinct.\n\n==Runge–Kutta-Nyström methods==\n\nRunge-Kutta Nyström methods are specialized Runge-Kutta methods that are optimized for second-order differential equations of the form:<ref>{{cite journal |last1=Dormand |first1=J. R. |last2=Prince |first2=P. J. |title=New Runge-Kutta Algorithms for Numerical Simulation in Dynamical Astronomy |journal=Celestial Mechanics |date=October 1978 |volume=18 |issue=3 |pages=223-232}}</ref>\n:<math> \\frac{d^2 y}{d t^2}= f(y,t)</math>\nOn the other hand, a general Runge-Kutta Nyström method is optimized for second-order differential equations of the form:<ref>{{cite report | last=Fehlberg | first=E. | date = October 1974 | title = Classical seventh-, sixth-, and fifth-order Runge-Kutta-Nyström formulas with stepsize control for general second-order differential equations | publisher   = National Aeronautics and Space Administration | edition =NASA TR R-432 | location    =Marshall Space Flight Center, AL\n}}</ref>\n:<math> \\frac{d^2 y}{d t^2}= f(y,\\dot{y},t)</math>\n\n==Implicit Runge–Kutta methods==\nAll Runge–Kutta methods mentioned up to now are [[explicit and implicit methods|explicit methods]]. Explicit Runge–Kutta methods are generally unsuitable for the solution of [[stiff equation]]s because their region of absolute stability is small; in particular, it is bounded.<ref>{{harvnb|Süli|Mayers|2003|pp=349–351}}</ref>\nThis issue is especially important in the solution of [[Numerical partial differential equations|partial differential equations]].\n\nThe instability of explicit Runge–Kutta methods motivates the development of implicit methods. An implicit Runge–Kutta method has the form\n\n:<math> y_{n+1} = y_n + h \\sum_{i=1}^s b_i k_i, </math>\nwhere\n:<math> k_i = f\\left( t_n + c_i h,\\ y_{n} + h \\sum_{j=1}^s a_{ij} k_j \\right), \\quad i = 1, \\ldots, s.</math> <ref>{{harvnb|Iserles|1996|p=41}}; {{harvnb|Süli|Mayers|2003|pp=351–352}}</ref>\n\nThe difference with an explicit method is that in an explicit method, the sum over ''j'' only goes up to ''i'' − 1. This also shows up in the Butcher tableau: the coefficient matrix <math>a_{ij}</math> of an explicit method is lower triangular. In an implicit method, the sum over ''j'' goes up to ''s'' and the coefficient matrix is not triangular, yielding a Butcher tableau of the form<ref name=\"Süli 2003 352\"/> \n\n:<math>\n\\begin{array}{c|cccc}\nc_1    & a_{11} & a_{12}& \\dots & a_{1s}\\\\\nc_2    & a_{21} & a_{22}& \\dots & a_{2s}\\\\\n\\vdots & \\vdots & \\vdots& \\ddots& \\vdots\\\\\nc_s    & a_{s1} & a_{s2}& \\dots & a_{ss} \\\\\n\\hline\n       & b_1    & b_2   & \\dots & b_s\\\\\n       & b^*_1  & b^*_2 & \\dots & b^*_s\\\\\n\\end{array} =\n\n\\begin{array}{c|c}\n\\mathbf{c}& A\\\\\n\\hline\n          & \\mathbf{b^T} \\\\\n\\end{array}\n</math>\nSee [[Runge–Kutta methods#Adaptive_Runge.E2.80.93Kutta_methods|Adaptive Runge-Kutta methods above]] for the explanation of the <math>b^*</math> row.\n\nThe consequence of this difference is that at every step, a system of algebraic equations has to be solved. This increases the computational cost considerably. If a method with ''s'' stages is used to solve a differential equation with ''m'' components, then the system of algebraic equations has ''ms'' components. This can be contrasted with implicit [[linear multistep method]]s (the other big family of methods for ODEs): an implicit ''s''-step linear multistep method needs to solve a system of algebraic equations with only ''m'' components, so the size of the system does not increase as the number of steps increases.<ref name=\"Süli 2003 353\">{{harvnb|Süli|Mayers|2003|p=353}}</ref>\n\n===Examples===\nThe simplest example of an implicit Runge–Kutta method is the [[backward Euler method]]:\n\n:<math>y_{n + 1} = y_n + h f(t_n + h,\\ y_{n + 1}). \\,</math>\n\nThe Butcher tableau for this is simply:\n\n:<math>\n\\begin{array}{c|c}\n1 & 1 \\\\\n\\hline\n  & 1 \\\\\n\\end{array}\n</math>\n\nThis Butcher tableau corresponds to the formulae\n\n:<math> k_1 = f(t_n + h,\\ y_n + h k_1) \\quad\\text{and}\\quad y_{n+1} = y_n + h k_1, </math>\n\nwhich can be re-arranged to get the formula for the backward Euler method listed above.\n\nAnother example for an implicit Runge–Kutta method is the [[trapezoidal rule (differential equations)|trapezoidal rule]]. Its Butcher tableau is:\n\n:<math>\n\\begin{array}{c|cc}\n0 & 0& 0\\\\\n1 & \\frac{1}{2}& \\frac{1}{2}\\\\\n\\hline\n  &  \\frac{1}{2}&\\frac{1}{2}\\\\\n  & 1 & 0 \\\\\n\\end{array}\n</math>\n\nThe trapezoidal rule is a [[collocation method]] (as discussed in that article). All collocation methods are implicit Runge–Kutta methods, but not all implicit Runge–Kutta methods are collocation methods.<ref>{{harvnb|Iserles|1996|pp=43–44}}</ref>\n\nThe [[Gauss–Legendre method]]s form a family of collocation methods based on [[Gauss quadrature]]. A Gauss–Legendre method with ''s'' stages has order 2''s'' (thus, methods with arbitrarily high order can be constructed).<ref>{{harvnb|Iserles|1996|p=47}}</ref> The method with two stages (and thus order four) has Butcher tableau:\n\n:<math>\n\\begin{array}{c|cc}\n\\frac12 - \\frac16 \\sqrt3 & \\frac14                  & \\frac14 - \\frac16 \\sqrt3 \\\\\n\\frac12 + \\frac16 \\sqrt3 & \\frac14 + \\frac16 \\sqrt3 & \\frac14 \\\\\n\\hline\n                         & \\frac12                  & \\frac12 \\\\\n                         & \\frac12+\\frac12 \\sqrt3   & \\frac12-\\frac12 \\sqrt3\n\\end{array}\n</math> <ref name=\"Süli 2003 353\"/>\n\n===Stability===\nThe advantage of implicit Runge–Kutta methods over explicit ones is their greater stability, especially when applied to [[stiff equation]]s. Consider the linear test equation ''y''' = λ''y''. A Runge–Kutta method applied to this equation reduces to the iteration <math> y_{n+1} = r(h\\lambda) \\, y_n </math>, with ''r'' given by\n\n:<math> r(z) = 1 + z b^T (I-zA)^{-1} e = \\frac{\\det(I-zA+zeb^T)}{\\det(I-zA)}, </math> <ref>{{harvnb|Hairer|Wanner|1996|pp=40–41}}</ref>\n\nwhere ''e'' stands for the vector of ones. The function ''r'' is called the ''stability function''.<ref>{{harvnb|Hairer|Wanner|1996|p=40}}</ref> It follows from the formula that ''r'' is the quotient of two polynomials of degree ''s'' if the method has ''s'' stages. Explicit methods have a strictly lower triangular matrix ''A'', which implies that det(''I'' − ''zA'') = 1 and that the stability function is a polynomial.<ref name=\"Iserles 1996 60\">{{harvnb|Iserles|1996|p=60}}</ref>\n\nThe numerical solution to the linear test equation decays to zero if | ''r''(''z'') | < 1 with ''z'' = ''h''λ. The set of such ''z'' is called the ''domain of absolute stability''. In particular, the method is said to be [[Stiff equation#A-stability|absolute stable]] if all ''z'' with Re(''z'') < 0 are in the domain of absolute stability. The stability function of an explicit Runge–Kutta method is a polynomial, so explicit Runge–Kutta methods can never be A-stable.<ref name=\"Iserles 1996 60\"/>\n\nIf the method has order ''p'', then the stability function satisfies <math> r(z) = \\textrm{e}^z + O(z^{p+1}) </math> as <math> z \\to 0 </math>. Thus, it is of interest to study quotients of polynomials of given degrees that approximate the exponential function the best. These are known as [[Padé approximant]]s. A Padé approximant with numerator of degree ''m'' and denominator of degree ''n'' is A-stable if and only if ''m'' ≤ ''n'' ≤ ''m'' + 2.<ref>{{harvnb|Iserles|1996|pp=62–63}}</ref>\n\nThe Gauss–Legendre method with ''s'' stages has order 2''s'', so its stability function is the Padé approximant with ''m'' = ''n'' = ''s''. It follows that the method is A-stable.<ref>{{harvnb|Iserles|1996|p=63}}</ref> This shows that A-stable Runge–Kutta can have arbitrarily high order. In contrast, the order of A-stable [[linear multistep method]]s cannot exceed two.<ref>This result is due to {{harvtxt|Dahlquist|1963}}.</ref>\n\n==B-stability==\nThe ''A-stability'' concept for the solution of differential equations is related to the linear autonomous equation <math>y'=\\lambda y</math>. Dahlquist proposed the investigation of stability of numerical schemes when applied to nonlinear systems that satisfy a monotonicity condition. The corresponding concepts were defined as ''G-stability'' for multistep methods (and the related one-leg methods) and ''B-stability'' (Butcher, 1975) for Runge–Kutta methods. A Runge–Kutta method applied to the non-linear system <math>y'=f(y)</math>, which verifies <math>\\langle f(y)-f(z),\\ y-z \\rangle<0</math>, is called ''B-stable'', if this condition implies <math>\\|y_{n+1}-z_{n+1}\\|\\leq\\|y_{n}-z_{n}\\|</math> for two numerical solutions.\n\nLet <math>B</math>, <math>M</math> and <math>Q</math> be three <math>s\\times s</math> matrices defined by\n\n: <math>B=\\operatorname{diag}(b_1,b_2,\\ldots,b_s),\\, M=BA+A^TB-bb^T,\\, Q=BA^{-1}+A^{-T}B-A^{-T}bb^TA^{-1}.</math>\n\nA Runge–Kutta method is said to be ''algebraically stable'' <ref>{{harvnb|Lambert|1991|p=275}}</ref> if the matrices <math>B</math> and <math>M</math> are both non-negative definite. A sufficient condition for ''B-stability'' <ref>{{harvnb|Lambert|1991|p=274}}</ref> is: <math>B</math> and <math>Q</math> are non-negative definite.\n\n==Derivation of the Runge–Kutta fourth-order method==\nIn general a Runge–Kutta method of order <math>s</math> can be written as:\n:<math>y_{t + h} = y_t + h \\cdot \\sum_{i=1}^s a_i k_i +\\mathcal{O}(h^{s+1}),</math>\nwhere:\n:<math>k_i = f\\left(y_t + h \\cdot \\sum_{j = 1}^s \\beta_{ij} k_j,\\ t_n + \\alpha_i h \\right)</math>\nare increments obtained evaluating the derivatives of <math>y_t</math> at the <math>i</math>-th order.\n\nWe develop the derivation<ref>[http://www.ss.ncu.edu.tw/~lyu/lecture_files_en/lyu_NSSP_Notes/Lyu_NSSP_AppendixC.pdf PDF] reporting this derivation</ref> for the Runge–Kutta fourth-order method using the general formula with <math>s=4</math> evaluated, as explained above, at the starting point, the midpoint and the end point of any interval <math>(t,\\ t +h)</math>; thus, we choose:\n\n: <math>\n\\begin{align}\n&\\alpha_i & &\\beta_{ij} \\\\\n\\alpha_1 &= 0 & \\beta_{21} &= \\frac{1}{2} \\\\\n\\alpha_2 &= \\frac{1}{2} &  \\beta_{32} &= \\frac{1}{2} \\\\\n\\alpha_3 &= \\frac{1}{2} &  \\beta_{43} &= 1 \\\\\n\\alpha_4 &= 1 & &\\\\\n\\end{align}\n</math>\n\nand <math>\\beta_{ij} = 0</math> otherwise. We begin by defining the following quantities:\n:<math>\\begin{align}\ny^1_{t+h} &= y_t + hf\\left(y_t,\\ t\\right) \\\\\ny^2_{t+h} &= y_t + hf\\left(y^1_{t+h/2},\\ t+\\frac{h}{2}\\right) \\\\\ny^3_{t+h} &= y_t + hf\\left(y^2_{t+h/2},\\ t+\\frac{h}{2}\\right)\n\\end{align}</math>\nwhere <math>y^1_{t+h/2} = \\dfrac{y_t + y^1_{t+h}}{2}</math> and <math>y^2_{t+h/2} = \\dfrac{y_t + y^2_{t+h}}{2}</math>.\nIf we define:\n:<math>\\begin{align}\nk_1 &= f(y_t,\\ t) \\\\\nk_2 &= f\\left(y^1_{t+h/2},\\ t + \\frac{h}{2}\\right) = f\\left(y_t + \\frac{h}{2} k_1,\\ t + \\frac{h}{2}\\right) \\\\\nk_3 &= f\\left(y^2_{t+h/2},\\ t + \\frac{h}{2}\\right) = f\\left(y_t + \\frac{h}{2} k_2,\\ t + \\frac{h}{2}\\right) \\\\\nk_4 &= f\\left(y^3_{t+h},\\ t + h\\right) = f\\left(y_t + h k_3,\\ t + h\\right)\n\\end{align}</math>\nand for the previous relations we can show that the following equalities hold up to <math>\\mathcal{O}(h^2)</math>:\n:<math>\\begin{align}\nk_2 &= f\\left(y^1_{t+h/2},\\ t + \\frac{h}{2}\\right) = f\\left(y_t + \\frac{h}{2} k_1,\\ t + \\frac{h}{2}\\right) \\\\\n&= f\\left(y_t,\\ t\\right) + \\frac{h}{2} \\frac{d}{dt}f\\left(y_t,\\ t\\right) \\\\\nk_3 &= f\\left(y^2_{t+h/2},\\ t + \\frac{h}{2}\\right) = f\\left(y_t + \\frac{h}{2} f\\left(y_t + \\frac{h}{2} k_1,\\ t + \\frac{h}{2}\\right),\\ t + \\frac{h}{2}\\right) \\\\\n&= f\\left(y_t,\\ t\\right) + \\frac{h}{2} \\frac{d}{dt} \\left[ f\\left(y_t,\\ t\\right) + \\frac{h}{2} \\frac{d}{dt}f\\left(y_t,\\ t\\right) \\right] \\\\\nk_4 &= f\\left(y^3_{t+h},\\ t + h\\right) = f\\left(y_t + h f\\left(y_t + \\frac{h}{2} k_2,\\ t + \\frac{h}{2}\\right),\\ t + h\\right) \\\\\n&= f\\left(y_t + h f\\left(y_t + \\frac{h}{2} f\\left(y_t + \\frac{h}{2} f\\left(y_t,\\ t\\right),\\ t + \\frac{h}{2}\\right),\\ t + \\frac{h}{2}\\right),\\ t + h\\right)  \\\\\n&= f\\left(y_t,\\ t\\right) + h \\frac{d}{dt} \\left[ f\\left(y_t,\\ t\\right) + \\frac{h}{2} \\frac{d}{dt}\\left[ f\\left(y_t,\\ t\\right) + \\frac{h}{2} \\frac{d}{dt}f\\left(y_t,\\ t\\right) \\right]\\right]\n\\end{align}</math>\nwhere:\n:<math>\\frac{d}{dt} f(y_t,\\ t) = \\frac{\\partial}{\\partial y} f(y_t,\\ t) \\dot y_t + \\frac{\\partial}{\\partial t} f(y_t,\\ t) = f_y(y_t,\\ t) \\dot y + f_t(y_t,\\ t) := \\ddot y_t</math>\nis the total derivative of <math>f</math> with respect to time.\n\nIf we now express the general formula using what we just derived we obtain:\n:<math>\\begin{align}\ny_{t+h} = {} & y_t + h  \\left\\lbrace a \\cdot f(y_t,\\ t) + b \\cdot \\left[ f(y_t,\\ t) + \\frac{h}{2} \\frac{d}{dt}f(y_t,\\ t) \\right] \\right.+ \\\\\n& {}+ c \\cdot \\left[ f(y_t,\\ t) + \\frac{h}{2} \\frac{d}{dt} \\left[ f\\left(y_t,\\ t\\right) + \\frac{h}{2} \\frac{d}{dt}f(y_t,\\ t) \\right] \\right] + \\\\\n&{}+ d \\cdot \\left[f(y_t,\\ t) + h \\frac{d}{dt} \\left[ f(y_t,\\ t) + \\frac{h}{2} \\frac{d}{dt}\\left[ f(y_t,\\ t)\n+ \\left. \\frac{h}{2} \\frac{d}{dt}f(y_t,\\ t) \\right]\\right]\\right]\\right\\rbrace + \\mathcal{O}(h^5) \\\\\n= {} & y_t + a \\cdot h f_t + b \\cdot h f_t + b \\cdot \\frac{h^2}{2} \\frac{df_t}{dt}   + c \\cdot h f_t+ c \\cdot \\frac{h^2}{2} \\frac{df_t}{dt} + \\\\\n&{}+ c \\cdot \\frac{h^3}{4} \\frac{d^2f_t}{dt^2} + d \\cdot h f_t + d \\cdot h^2 \\frac{df_t}{dt} + d \\cdot \\frac{h^3}{2} \\frac{d^2f_t}{dt^2} + d \\cdot \\frac{h^4}{4} \\frac{d^3f_t}{dt^3} + \\mathcal{O}(h^5)\n\\end{align}</math>\n\nand comparing this with the [[Taylor series]] of <math>y_{t+h}</math> around <math>y_t</math>:\n\n:<math>\\begin{align}\n y_{t+h} &= y_t + h \\dot y_t + \\frac{h^2}{2} \\ddot y_t + \\frac{h^3}{6} y^{(3)}_t + \\frac{h^4}{24} y^{(4)}_t + \\mathcal{O}(h^5) = \\\\\n&= y_t + h f(y_t,\\ t) + \\frac{h^2}{2} \\frac{d}{dt}f(y_t,\\ t) + \\frac{h^3}{6} \\frac{d^2}{dt^2}f(y_t,\\ t) + \\frac{h^4}{24} \\frac{d^3}{dt^3}f(y_t,\\ t)\n\\end{align}</math>\n\nwe obtain a system of constraints on the coefficients:\n\n:<math>\n \\begin{cases}\n & a + b + c + d = 1 \\\\[6pt]\n & \\frac{1}{2} b + \\frac{1}{2} c  + d = \\frac{1}{2} \\\\[6pt]\n & \\frac{1}{4} c + \\frac{1}{2} d = \\frac{1}{6} \\\\[6pt]\n & \\frac{1}{4} d = \\frac{1}{24}\n \\end{cases}</math>\nwhich when solved gives <math>a = \\frac{1}{6}, b = \\frac{1}{3}, c = \\frac{1}{3}, d = \\frac{1}{6}</math> as stated above.\n\n==See also==\n* [[Euler's method]]\n* [[List of Runge–Kutta methods]]\n* [[Numerical ordinary differential equations|Numerical methods for ordinary differential equations]]\n* [[Runge–Kutta method (SDE)]]\n* [[General linear methods]]\n* [[Lie group integrator]]\n\n==Notes==\n{{reflist|2}}\n\n==References==\n* {{Citation | last1=Runge | first1=Carl David Tolmé | author1-link=Carl_David_Tolmé_Runge | title=Über die numerische Auflösung von Differentialgleichungen | publisher=[[Springer Science+Business Media|Springer]] | journal=Mathematische Annalen | volume=46 | number=2 | pages=167-178 | year=1895 |doi=10.1007/BF01446807}}.\n* {{Citation | last1=Kutta | first1=Martin | author1-link= Martin Kutta | title=Beitrag zur näherungweisen Integration totaler Differentialgleichungen | year=1901}}.\n* {{Citation | last1=Ascher | first1=Uri M. | last2=Petzold | first2=Linda R.|author2-link=Linda Petzold | title=Computer Methods for Ordinary Differential Equations and Differential-Algebraic Equations | publisher=[[Society for Industrial and Applied Mathematics]] | location=Philadelphia | isbn=978-0-89871-412-8 | year=1998}}.\n* {{Citation | last1=Atkinson | first1=Kendall A. | title=An Introduction to Numerical Analysis | publisher=[[John Wiley & Sons]] | location=New York | edition=2nd | isbn=978-0-471-50023-0 | year=1989}}.\n* {{Citation| last1 = Butcher | first1 = John C. | author1-link=John C. Butcher | title=Coefficients for the study of Runge-Kutta integration processes | url=http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=4922056 | doi=10.1017/S1446788700027932 | volume=3 | issue=2 |date=May 1963 | pages=185–201}}.\n* {{Citation | last1=Butcher | first1=John C. | author1-link=John C. Butcher | title= A stability property of implicit Runge-Kutta methods| year=1975 |journal= BIT| volume= 15 |pages= 358–361 | doi=10.1007/bf01931672}}.\n* {{Citation | last1=Butcher | first1=John C. | author1-link=John C. Butcher | title=Numerical Methods for Ordinary Differential Equations | publisher=[[John Wiley & Sons]] | location=New York | isbn=978-0-470-72335-7 | year=2008}}.\n* {{Citation | last1=Cellier | first1=F. | last2=Kofman | first2=E. | title=Continuous System Simulation | publisher=[[Springer Verlag]] | isbn=0-387-26102-8 | year=2006}}.\n* {{Citation | last1=Dahlquist | first1=Germund | author1-link=Germund Dahlquist | title=A special stability problem for linear multistep methods | doi=10.1007/BF01963532 | year=1963 | journal=BIT | issn=0006-3835 | volume=3 | pages=27–43}}.\n* {{Citation | last1=Forsythe | first1=George E. | last2=Malcolm | first2=Michael A. | last3=Moler | first3=Cleve B. | title=Computer Methods for Mathematical Computations | publisher=[[Prentice-Hall]] | year = 1977}} (see Chapter 6).\n* {{Citation | last1=Hairer | first1=Ernst | last2=Nørsett | first2=Syvert Paul | last3=Wanner | first3=Gerhard | title=Solving ordinary differential equations I: Nonstiff problems | publisher=[[Springer-Verlag]] | location=Berlin, New York | isbn=978-3-540-56670-0 | year=1993}}.\n* {{Citation | last1=Hairer | first1=Ernst | last2=Wanner | first2=Gerhard | title=Solving ordinary differential equations II: Stiff and differential-algebraic problems | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=2nd | isbn=978-3-540-60452-5 | year=1996}}.\n* {{Citation | last1=Iserles | first1=Arieh | author1-link=Arieh Iserles | title=A First Course in the Numerical Analysis of Differential Equations | publisher=[[Cambridge University Press]] | isbn=978-0-521-55655-2 | year=1996}}.\n* {{Citation|last1=Lambert|first1=J.D| title=Numerical Methods for Ordinary Differential Systems. The Initial Value Problem | publisher=[[John Wiley & Sons]]|year=1991 | isbn=0-471-92990-5}}\n* {{Citation | last1=Kaw | first1=Autar | last2=Kalu | first2=Egwu | title=Numerical Methods with Applications | url=http://numericalmethods.eng.usf.edu/topics/textbook_index.html | publisher=autarkaw.com | edition=1st | year = 2008}}.\n* {{Citation | last1=Kutta | first1=Martin | author1-link=Martin Kutta | title=Beitrag zur näherungsweisen Integration totaler Differentialgleichungen | journal=Zeitschrift für Mathematik und Physik | volume=46 | pages=435–453 | year=1901}}.\n* {{Citation | last1=Press | first1=William H. | last2=Flannery | first2=Brian P. | last3=Teukolsky | first3=Saul A. | author3-link=Saul Teukolsky | last4=Vetterling | first4=William T. | title=[[Numerical Recipes|Numerical Recipes: The Art of Scientific Computing]] | chapter=Section 17.1 Runge-Kutta Method | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=907 | publisher=[[Cambridge University Press]] | edition=3rd | isbn=978-0-521-88068-8 | year=2007}}. Also, [http://apps.nrbook.com/empanel/index.html#pg=910 Section 17.2. Adaptive Stepsize Control for Runge-Kutta].\n* {{Citation | last1=Stoer | first1=Josef | last2=Bulirsch | first2=Roland | title=Introduction to Numerical Analysis | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=3rd | isbn=978-0-387-95452-3 | year=2002}}.\n* {{Citation | last1=Süli | first1=Endre | last2=Mayers | first2=David | title=An Introduction to Numerical Analysis | publisher=[[Cambridge University Press]] | isbn=0-521-00794-1 | year=2003}}.\n* {{Citation | last1=Tan | first1=Delin | last2=Chen | first2=Zheng | title=On A General Formula of Fourth Order Runge-Kutta Method | url=http://msme.us/2012-2-1.pdf | journal=Journal of Mathematical Science & Mathematics Education | volume=7.2 |pages = 1–10 |year = 2012}}.\n\n==External links==\n* {{springer|title=Runge-Kutta method|id=p/r082810}}\n* [http://numericalmethods.eng.usf.edu/topics/runge_kutta_4th_method.html Runge–Kutta 4th-Order Method]\n* [http://www.dotnumerics.com/NumericalLibraries/DifferentialEquations/ DotNumerics: Ordinary Differential Equations for C# and VB.NET] — Initial-value problem for nonstiff and stiff ordinary differential equations (explicit Runge–Kutta, implicit Runge–Kutta, Gear's BDF and Adams–Moulton).\n* [https://github.com/USNavalResearchLaboratory/TrackerComponentLibrary/tree/master/Mathematical%20Functions/Differential%20Equations Tracker Component Library Implementation in Matlab] — Implements 32 embedded Runge Kutta algorithms in <code>RungeKStep</code>, 24 embedded Runge-Kutta Nyström algorithms in <code>RungeKNystroemSStep</code> and 4 general Runge-Kutta Nyström algorithms in <code>RungeKNystroemGStep</code>.\n\n{{Numerical integrators}}\n\n{{DEFAULTSORT:Runge-Kutta Methods}}\n[[Category:Numerical differential equations]]\n[[Category:Runge–Kutta methods]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Runge–Kutta–Fehlberg method",
      "url": "https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta%E2%80%93Fehlberg_method",
      "text": "In [[mathematics]], the '''Runge–Kutta–Fehlberg method''' (or '''Fehlberg method''') is an [[algorithm]] in [[numerical analysis]] for the [[numerical ordinary differential equations|numerical solution of ordinary differential equations]]. It was developed by the German mathematician [[Erwin Fehlberg]] and is based on the large class of [[Runge–Kutta methods]]. \n\nThe novelty of Fehlberg's method is that it is an embedded method{{Definition needed|date=April 2019}} from the  [[Runge–Kutta methods|Runge–Kutta family]], meaning that identical function evaluations are used in conjunction with each other to create methods of varying order and similar error constants.  The method presented in Fehlberg's 1969 paper has been dubbed the '''RKF45''' method, and is a method of order O(''h''<sup>4</sup>) with an error estimator of order O(''h''<sup>5</sup>).<ref>According to Hairer et al. (1993, §II.4), the method was originally proposed in Fehlberg (1969); Fehlberg (1970) is an extract of the latter publication.</ref> By performing one extra calculation, the error in the solution can be estimated and controlled by using the higher-order embedded method that allows for an [[adaptive stepsize]] to be determined automatically.\n\n== Butcher tableau for Fehlberg's 4(5) method ==\n\nAny [[Runge–Kutta methods|Runge–Kutta method]] is uniquely identified by its [[Butcher tableau]]. The embedded pair proposed by Fehlberg<ref>{{harvtxt|Hairer|Nørsett|Wanner|1993|p=177}} refer to {{harvtxt|Fehlberg|1969}}</ref>\n\n{| cellpadding=3px cellspacing=0px\n|width=\"20px\"| || style=\"border-right:1px solid;\" | 0\n|- \n||| style=\"border-right:1px solid;\" | 1/4 || 1/4\n|- \n||| style=\"border-right:1px solid;\" | 3/8 || 3/32 || 9/32\n|- \n||| style=\"border-right:1px solid;\" | 12/13  || 1932/2197 || −7200/2197 || 7296/2197\n|- \n||| style=\"border-right:1px solid;\" | 1  || 439/216 || −8 || 3680/513 || −845/4104\n|- \n||| style=\"border-right:1px solid; border-bottom:1px solid;\" | 1/2 || style=\"border-bottom:1px solid;\" | −8/27 || style=\"border-bottom:1px solid;\" | 2 || style=\"border-bottom:1px solid;\" | −3544/2565 || style=\"border-bottom:1px solid;\" | 1859/4104 || style=\"border-bottom:1px solid;\" | −11/40 || style=\"border-bottom:1px solid;\" |\n|- \n||| style=\"border-right:1px solid;\" | || 16/135 || 0 || 6656/12825 || 28561/56430 || −9/50 || 2/55 \n|-\n||| style=\"border-right:1px solid;\" | || 25/216 || 0 || 1408/2565 || 2197/4104 || −1/5 || 0\n|}\n\nThe first row of coefficients at the bottom of the table gives the fifth-order accurate method, and the second row gives the fourth-order accurate method.\n\n== See also ==\n* [[List of Runge–Kutta methods]]\n* [[Numerical methods for ordinary differential equations]]\n* [[Runge–Kutta methods]]\n\n== Notes ==\n<references/>\n\n== References ==\n* [[Free software]] implementation in [[GNU Octave]]: http://octave.sourceforge.net/odepkg/function/ode45.html\n* Erwin Fehlberg (1969). ''Low-order classical Runge-Kutta formulas with step size control and their application to some heat transfer problems''. NASA Technical Report 315.\n* Erwin Fehlberg (1970). \"Klassische Runge-Kutta-Formeln vierter und niedrigerer Ordnung mit Schrittweiten-Kontrolle und ihre Anwendung auf Wärmeleitungsprobleme,\" ''Computing (Arch. Elektron. Rechnen)'', vol. 6, pp.&nbsp;61–71. {{doi|10.1007/BF02241732}}\n* Ernst Hairer, Syvert Nørsett, and Gerhard Wanner (1993). ''Solving Ordinary Differential Equations I: Nonstiff Problems'', second edition, Springer-Verlag, Berlin. {{isbn|3-540-56670-8}}.\n\n{{DEFAULTSORT:Runge-Kutta-Fehlberg method}}\n[[Category:Numerical differential equations]]\n[[Category:Runge–Kutta methods]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Scale co-occurrence matrix",
      "url": "https://en.wikipedia.org/wiki/Scale_co-occurrence_matrix",
      "text": "{{multiple issues|\n{{Orphan|date=December 2014}}\n{{Underlinked|date=December 2014}}\n}}\n\n'''Scale co-occurrence matrix (SCM)''' is a method for image feature extraction within scale space after [[wavelet transform]]ation, proposed by Wu Jun and Zhao Zhongming (Institute of Remote Sensing Application, [[China]]). In practice, we first do discrete wavelet transformation for one gray image and get sub images with different scales. Then we construct a series of scale based concurrent matrixes, every matrix describing the gray level variation between two adjacent scales. Last we use selected functions (such as Harris statistical approach) to calculate measurements with SCM and do feature extraction and classification. \nOne basis of the method is the fact: way texture information changes from one scale to another can represent that texture in some extent thus it can be used as a criterion for feature extraction. The matrix captures the relation of features between different scales rather than the features within a single scale space, which can represent the scale property of texture better. Also, there are several experiments showing that it can get more accurate results for texture classification than the traditional texture classification.<ref>{{cite journal|last1=Wu|first1=Jun|last2=Zhao|first2=Zhongming|title=Scale Co-occurrence Matrix for Texture Analysis using Wavelet Transformation|url=http://caod.oriprobe.com/articles/3275502/Scale_Co_occurrence_Matrix_for_Texture_Analysis_Using_Wavelet_Transfor.htm|journal=Journal of Remote Sensing|date=Mar 2001|volume=5|issue=2|page=100}}</ref>\n\n== Background ==\nTexture can be regarded as a similarity grouping in an image. Traditional texture analysis can be divided into four major issues: feature extraction, texture discrimination, texture classification and shape from texture(to reconstruct 3D surface geometry from texture information). For tradition feature extraction, approaches are usually categorized into structural, statistical, model based and transform.<ref>{{cite book|last1=Duda|first1=R.O.|title=Pattern Classification and Scene Analysis|isbn=978-0471223610|date=1973-02-09}}</ref> \nWavelet transformation is a popular method in numerical analysis and functional analysis, which captures both frequency and location information. Gray level co-occurrence matrix provides an important basis for SCM construction. \nSCM based on discrete wavelet frame transformation make use of both correlations and feature information so that it combines structural and statistical benefits.\n\n=== [[Discrete wavelet transform|Discrete wavelet]] frame (DWF) ===\nIn order to do SCM we have to use discrete wavelet frame (DWF) transformation first to get a series of sub images. The discrete wavelet frames is nearly identical to the standard wavelet transform,<ref>{{cite journal|last1=Kevin|first1=Lund|last2=Curt|first2=Burgess|title=Producing high-dimensional semantic spaces from lexical co-occurrence|journal=Behavior Research Methods|date=June 1996|volume=28|issue=2|pages=203–208}}</ref> except that one upsamples the filters, rather than downsamples the image. Given an image, the DWF decomposes its channel using the same method as the wavelet transform, but without the subsampling process. This results in four filtered images with the same size as the input image. The decomposition is then continued in the LL channels only as in the wavelet transform, but since the image is not subsampled, the filter has to be upsampled by inserting zeros in between its coefficients. The number of channels, hence the number of features for DWF is given by 3&nbsp;×&nbsp;l&nbsp;−&nbsp;1.<ref>{{cite journal|last1=Mallat|first1=S.G.|title=A theory for multiresolution signal decomposition: The wavelet representation|journal=IEEE Transactions on Pattern Analysis and Machine Intelligence|volume=11|issue=7|date=1989|pages=674–693|doi=10.1109/34.192463}}</ref>\nOne dimension discrete wavelet frame decompose the image in this way:\n\n: <math> d_i(k) = [ [g_i]^T x], \\quad (i=1,\\ldots,N) </math>\n\n== Example ==\nIf there are two sub images ''X''<sub>1</sub> and ''X''<sub>0</sub> from the parent image ''X'' (in practice ''X''&nbsp;=&nbsp;''X''<sub>0</sub>), ''X''<sub>1</sub>&nbsp;=&nbsp;[1 1;1 2], ''X''<sub>2</sub>&nbsp;=&nbsp;[1 1;1 4],the grayscale is 4 so that we can get ''k''&nbsp;=&nbsp;1, ''G''&nbsp;=&nbsp;4.\n''X''<sub>1</sub>(1,1), (1,2) and (2,1) are 1, while ''X''<sub>0</sub>(1,1), (1,2) and (2,1) are 1, thus Φ<sub>1</sub>(1,1)&nbsp;=&nbsp;3; Similarly, Φ<sub>1</sub>(2,4)&nbsp;=&nbsp;1.\nThe SCM is as following:\n\n{| class=\"wikitable\"\n|-\n! G=4 !! Gray level 0 !! Gray level 1 !! Gray level 2 !! Gray level 3 !! Gray level 4\n|-\n| Gray level 0 || 0 || 0 || 0 || 0 || 0 \n|-\n| Gray level 1 || 3 || 0 || 0 || 0 || 0 \n|-\n| Gray level 2 || 0 || 0 || 0 || 0 || 0 \n|-\n| Gray level 3 || 0 || 0 || 0 || 0 || 0 \n|-\n| Gray level 4 || 0 || 0 || 1 || 0 || 0 \n|}\n\n== External links ==\n*{{cite web|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=738911&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D738911|title=IEEE Xplore Abstract - Discrete wavelet frame representations of color texture features for image query|publisher=ieeexplore.ieee.org|accessdate=2016-03-25}}\n*[http://www.mathworks.com/matlabcentral/fileexchange/11727-cooccurrence-matrix co-occurrence-matrix MATLAB tutorial]\n*[http://www.mathworks.com/matlabcentral/fileexchange/11727-cooccurrence-matrix Co-occurrence Matrix]\n\n== References ==\n{{Reflist}}\n\n[[Category:Feature detection (computer vision)]]\n[[Category:Functional analysis]]\n[[Category:Image compression]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Scarborough criterion",
      "url": "https://en.wikipedia.org/wiki/Scarborough_criterion",
      "text": "{{Orphan|date=April 2017}}\n\nThe '''Scarborough criterion''' is used for satisfying convergence of a solution while solving [[linear equations]] using an [[iterative method]].\n\n==Introduction==\nAnalytical solutions for certain systems of equations can be difficult or impossible to obtain. A well known example are the Navier-Stokes equations describing the flow of Newtonian fluids. Solutions of such equations can be obtained [[numerically]], at discrete points of the solution domain (e.g. at discrete time points and points in space). Numerical solutions based on the integration of the equations at discrete control volumes of the solution domain (for example the [[Finite volume method|Finite Volume Method]]) result in a system of algebraic equations, one for each ''nodal point'' (corresponding to a particular control volume). These algebraic equations are usually referred to as ''discretised equations''. The '''Scarborough criterion''' formulated by Scarborough (1958), can be expressed in terms of the values of the coefficients of the discretised equations:<ref name=\"Scarborough1955\">{{cite book|author=James Blaine Scarborough|title=Numerical Mathematical Analysis|url=https://books.google.com/books?id=zpaVLfzPgfIC|year=1955|publisher=Johns Hopkins Press}}</ref><ref name=\"Pearson Education Limited\">{{cite book|author1=Henk Kaarle Versteeg|author2=Weeratunge Malalasekera|title=An Introduction to Computational Fluid Dynamics: The Finite Volume Method|url=https://books.google.com/books?id=RvBZ-UMpGzIC|date=1 January 2007|publisher=Pearson Education Limited|isbn=978-0-13-127498-3}}</ref>\n:<math> \\frac{\\sum |a_{nb}|}{|a'_{p}|} \\begin{cases}\n    \\leq 1, & \\text{at all nodes}\\\\\n    <1, & \\text{at one node at least}\n  \\end{cases} </math>\nHere {{math|<var>a'<sub>p</sub></var>}} is the net coefficient of a random central node ''P'' and the summation in the numerator is taken over all the neighbouring nodes. For a one, two and three-dimensional problem there will be two (east & west), four (east, west, south & north), and six (east, west, south north, top & bottom) neighbours for each node, respectively.\n\n==Comments==\n* This is a sufficient condition, not a necessary one. This means that we can get convergence, even if, at times, we violate the criterion.<ref name=\"Patankar1980\">{{cite book|author=Suhas Patankar|title=Numerical Heat Transfer and Fluid Flow|url=https://books.google.com/books?id=5JMYZMX3OVcC&pg=PA64|date=1 January 1980|publisher=CRC Press|isbn=978-0-89116-522-4|pages=64–}}</ref>\n* The satisfaction of this criterion ensures that the equations will be converged by at least one iterative method.<ref name=\"Patankar1980\"/>\n\n==Gauss–Seidel method==\nIf Scarborough criterion is not satisfied then [[Gauss–Seidel method|Gauss–Seidel method iterative procedure]] is not guaranteed to converge a solution. This criterion is a sufficient condition,<ref name=\"Patankar1980\"/> not a necessary one. If this criterion is satisfied then it means equation will be converged by at least one [[iterative method]]. The Scarborough criterion is used as a sufficient condition for convergent iterative method.  The [[finite volume method]] uses this criterion for obtaining a convergent solution and implementing [[boundary condition]]s.\n\n==Diagonal dominance==\nIf the differencing scheme produces coefficients that satisfy the above criterion the resulting matrix of coefficients is [[diagonally dominant]].<ref name=\"Minkowycz1988\">{{cite book|author=W. J. Minkowycz|title=Handbook of Numerical Heat Transfer|url=https://books.google.com/books?id=0AUHXswbSbYC|date=28 March 1988|publisher=Wiley|isbn=978-0-471-83093-1}}</ref> To achieve diagonal dominance we need large values of net coefficient so the linearisation practice of source terms should ensure that ''S''<sub>''P''</sub> is always negative. If this is the case –''S''<sub>''P''</sub> is always positive and adds to ''a''<sub>''P''</sub>. Diagonal dominance is a desirable feature for satisfying the [[bounded poset|boundedness]] criterion. This states that in the absence of sources the internal nodal values of the property ''ф'' should be bounded by its boundary values. Hence in a steady state conduction problem without sources and with boundary temperatures of 500&nbsp;°C and 200&nbsp;°C all interior values of ''T'' should be less than 500&nbsp;°C and greater than 200&nbsp;°C.<ref name=\"Pearson Education Limited\"/>\n\n==See also==\n* [[Computational fluid dynamics]]\n* [[Linear equation]]\n\n==References==\n{{reflist}}\n\n==External links ==\n* [https://web.archive.org/web/20120303230200/http://nptel.iitm.ac.in/courses/112105045/  Introduction to Computational Fluid Dynamics and Principles of Conservation - video lecture]\n* [https://engineering.purdue.edu/ME608/webpage/l5.pdf  Overview of Numerical Methods]\n* [http://web.iitd.ac.in/~prabal/BC-FVM-lecturenotes-9-10.pdf Implementation of BC in FVM]\n\n[[Category:Computational fluid dynamics]]\n[[Category:Numerical analysis]]\n[[Category:Applied mathematics]]\n[[Category:Functional analysis]]\n[[Category:Convergence (mathematics)]]"
    },
    {
      "title": "Series acceleration",
      "url": "https://en.wikipedia.org/wiki/Series_acceleration",
      "text": "In [[mathematics]], '''series acceleration''' is one of a collection of [[sequence transformation]]s for improving the [[rate of convergence]] of a [[series (mathematics)|series]]. Techniques for series acceleration are often applied in [[numerical analysis]], where they are used to improve the speed of [[numerical integration]]. Series acceleration techniques may also be used, for example, to obtain a variety of identities on [[special functions]]. Thus, the [[Euler transform]] applied to the [[hypergeometric series]] gives some of the classic, well-known hypergeometric series identities.\n\n== Definition ==\nGiven a [[sequence]] \n\n:<math>S=\\{ s_n \\}_{n\\in\\N}</math>\n\nhaving a limit\n\n:<math>\\lim_{n\\to\\infty} s_n = \\ell,</math>\n\nan accelerated series is a second sequence \n\n:<math>S'=\\{ s'_n \\}_{n\\in\\N}</math>\n\nwhich '''converges faster''' to <math>\\ell</math> than the original sequence, in the sense that \n\n:<math>\\lim_{n\\to\\infty} \\frac{s'_n-\\ell}{s_n-\\ell} = 0.</math>\n\nIf the original sequence is [[Divergent series|divergent]], the [[sequence transformation]] acts as an [[extrapolation method]] to the [[antilimit]] <math>\\ell</math>.\n\nThe mappings from the original to the transformed series may be linear (as defined in the article [[sequence transformation]]s), or non-linear. In general, the non-linear sequence transformations tend to be more powerful.\n\n== Overview ==\nTwo classical techniques for series acceleration are [[Euler's transformation of series]]<ref>{{AS ref|3, eqn 3.6.27|16}}</ref> and [[Kummer's transformation of series]].<ref>{{AS ref|3, eqn 3.6.26|16}}</ref> A variety of much more rapidly convergent and special-case tools have been developed in the 20th century, including [[Richardson extrapolation]], introduced by [[Lewis Fry Richardson]] in the early 20th century but also known and used by [[Takebe Kenko|Katahiro Takebe]] in 1722; the [[Aitken delta-squared process]], introduced by [[Alexander Aitken]] in 1926 but also known and used by [[Takakazu Seki]] in the 18th century; the [http://mathworld.wolfram.com/WynnsEpsilonMethod.html epsilon method] given by [[Peter Wynn (mathematician)|Peter Wynn]] in 1956; the [[Levin u-transform]]; and the Wilf-Zeilberger-Ekhad method or [[WZ theory|WZ method]].\n\nFor alternating series, several powerful techniques, offering convergence rates  from <math>5.828^{-n}</math> all the way to <math>17.93^{-n}</math> for a summation of <math>n</math> terms, are described by Cohen ''et al.''.<ref>[[Henri Cohen (number theorist)|Henri Cohen]], Fernando Rodriguez Villegas, and [[Don Zagier]],\n\"[http://people.mpim-bonn.mpg.de/zagier/files/exp-math-9/fulltext.pdf Convergence Acceleration of Alternating Series]\", ''Experimental Mathematics'', '''9''':1 (2000) page 3.</ref>\n\n==Euler's transform==\nA basic example of a [[linear sequence transformation]], offering improved convergence, is Euler's transform. It is intended to be applied to an alternating series; it is given by \n\n:<math>\\sum_{n=0}^\\infty (-1)^n a_n = \\sum_{n=0}^\\infty (-1)^n \n\\frac {\\Delta^n a_0} {2^{n+1}}</math>\n\nwhere <math>\\Delta</math> is the [[forward difference operator]]:\n\n:<math>\\Delta^n a_0 = \\sum_{k=0}^n (-1)^k {n \\choose k} a_{n-k}.</math>\n\nIf the original series, on the left hand side, is only slowly converging, the forward differences will tend to become small quite rapidly; the additional power of two further improves the rate at which the right hand side converges.\n\nA particularly efficient numerical implementation of the Euler transform is the [[van Wijngaarden transformation]].<ref>William H. Press, ''et al.'', ''Numerical Recipes in C'', (1987) Cambridge University Press, {{isbn|0-521-43108-5}} (See section 5.1).</ref>\n\n==Conformal mappings==\nA series\n\n:<math>S=\\sum_{n=0}^{\\infty} a_n</math>\n\ncan be written as f(1), where the function f(z) is defined as\n\n:<math>f(z) = \\sum_{n=0}^{\\infty} a_n z^{n}</math>\n\nThe function f(z) can have singularities in the complex plane (branch point singularities, poles or essential singularities), which limit the radius of convergence of the series. If the point z = 1 is close to or on the boundary of the disk of convergence, the series for S will converge very slowly. One can then improve the convergence of the series by means of a conformal mapping that moves the singularities such that the point that is mapped to z = 1, ends up deeper in the new disk of convergence.\n\nThe conformal transform <math>z = \\Phi(w)</math> needs to be chosen such that <math>\\Phi(0)=0</math>, and one usually chooses a function that has a finite derivative at w = 0. One can assume that <math>\\Phi(1)=1</math> without loss of generality, as one can always rescale w to redefine <math>\\Phi</math>. We then consider the function\n\n:<math>g(w)= f\\left(\\Phi(w)\\right)</math>\n\nSince <math>\\Phi(1)=1</math>, we have f(1) = g(1). We can obtain the series expansion of g(w) by putting <math>z=\\Phi(w)</math> in the series expansion of f(z) because <math>\\Phi(0)=0</math>; the first n terms of the series expansion for f(z) will yield the first n terms of the series expansion for g(w) if <math>\\Phi'(0)\\neq 0</math>. Putting w = 1 in that series expansion will thus yield a series such that if it converges, it will converge to the same value as the original series.\n\n==Non-linear sequence transformations==\n\nExamples of such nonlinear sequence transformations are [[Padé approximant]]s, the [[Shanks transformation]], and [[Levin-type sequence transformation]]s.\n\nEspecially nonlinear sequence transformations often provide  powerful numerical methods for the [[summation]] of [[divergent series]] or [[asymptotic series]] that arise for instance in [[perturbation theory]], and may be used as  highly effective [[extrapolation method]]s.\n\n===Aitken method===\n{{main article|Aitken's delta-squared process}}\nA simple nonlinear sequence transformation is the Aitken extrapolation or delta-squared method,\n\n:<math>\\mathbb{A} : S \\to S'=\\mathbb{A}(S) = {(s'_n)}_{n\\in\\N}</math>\n\ndefined by \n\n:<math>s'_n = s_{n+2} - \\frac{(s_{n+2}-s_{n+1})^2}{s_{n+2}-2s_{n+1}+s_n}.</math>\n\nThis transformation is commonly used to improve the [[rate of convergence]] of a slowly converging sequence; heuristically, it eliminates the largest part of the [[absolute error]].\n\n== See also ==\n* [[Shanks transformation]]\n* [[Minimum polynomial extrapolation]]\n* [[Van Wijngaarden transformation]]\n\n==External Links==\n* [http://numbers.computation.free.fr/Constants/Miscellaneous/seriesacceleration.html Convergence acceleration of series]\n* [https://www.gnu.org/software/gsl/manual/html_node/Series-Acceleration.html GNU Scientific Library, Series Acceleration]\n* [http://dlmf.nist.gov/3.9 Digital Library of Mathematical Functions]\n==References==\n<references/>\n* C. Brezinski and M. Redivo Zaglia, ''Extrapolation Methods. Theory and Practice'', North-Holland, 1991.\n* G. A. Baker, Jr. and P. Graves-Morris, ''Padé  Approximants'', Cambridge U.P., 1996.\n* {{mathworld|urlname=ConvergenceImprovement|title=Convergence Improvement}}\n* Herbert H. H. Homeier, ''Scalar Levin-Type Sequence Transformations'', Journal of Computational and Applied Mathematics, vol. 122, no. 1-2, p 81 (2000). {{Cite journal | last1 = Homeier | first1 = H. H. H. | doi = 10.1016/S0377-0427(00)00359-9 | title = Scalar Levin-type sequence transformations | journal = Journal of Computational and Applied Mathematics | volume = 122 | pages = 81 | year = 2000 | pmid =  | pmc = | arxiv = math/0005209 | bibcode = 2000JCoAM.122...81H }}, {{arxiv|math/0005209}}.\n\n[[Category:Numerical analysis]]\n[[Category:Asymptotic analysis]]\n[[Category:Summability methods]]\n[[Category:Perturbation theory]]"
    },
    {
      "title": "Shanks transformation",
      "url": "https://en.wikipedia.org/wiki/Shanks_transformation",
      "text": "In [[numerical analysis]], the '''Shanks transformation''' is a [[non-linear]] [[series acceleration]] method to increase the [[rate of convergence]] of a [[sequence]]. This method is named after [[Daniel Shanks]], who rediscovered this sequence transformation in 1955. It was first derived and published by R. Schmidt in 1941.<ref>Weniger (2003).</ref>\n\n{{Quote box\n | quote=One can calculate only a few terms of a [[perturbation theory|perturbation expansion]], usually no more than two or three, and almost never more than seven. The resulting series is often slowly convergent, or even divergent. Yet those few terms contain a remarkable amount of information, which the investigator should do his best to extract.<br> This viewpoint has been persuasively set forth in a delightful paper by Shanks (1955), who displays a number of amazing examples, including several from [[fluid mechanics]]. \n | source= [[Milton Van Dyke|Milton D. Van Dyke]] (1975) ''Perturbation methods in fluid mechanics'', p. 202.\n | width= 60%\n | align= right\n}}\n\n==Formulation==\n\nFor a sequence <math>\\left\\{a_m\\right\\}_{m\\in\\mathbb{N}}</math> the series\n\n:<math>A = \\sum_{m=0}^\\infty a_m\\,</math>\n\nis to be determined. First, the partial sum <math>A_n</math> is defined as:\n\n:<math>A_n = \\sum_{m=0}^n a_m\\,</math>\n\nand forms a new sequence <math>\\left\\{A_n\\right\\}_{n\\in\\mathbb{N}}</math>. Provided the series converges, <math>A_n</math> will also approach the limit <math>A</math> as <math>n\\to\\infty.</math>\nThe Shanks transformation <math>S(A_n)</math> of the sequence <math>A_n</math> is the new sequence defined by<ref name=BenderOrszag368>Bender & Orszag (1999), pp. 368–375.</ref><ref name=VanDyke>Van Dyke (1975), pp. 202–205.</ref>\n\n:<math>S(A_n) = \\frac{A_{n+1}\\, A_{n-1}\\, -\\, A_n^2}{A_{n+1}-2A_n+A_{n-1}} = A_{n+1} - \\frac{(A_{n+1}-A_{n})^2}{(A_{n+1}-A_{n})-(A_{n}-A_{n-1})}</math>\n\nwhere this sequence <math>S(A_n)</math> often converges more rapidly than the sequence <math>A_n.</math>   \nFurther speed-up may be obtained by repeated use of the Shanks transformation, by computing <math>S^2(A_n)=S(S(A_n)),</math> <math>S^3(A_n)=S(S(S(A_n))),</math> etc.\n\nNote that the non-linear transformation as used in the Shanks transformation is essentially the same as used in [[Aitken's delta-squared process]] so that as with Aitken's method, the right-most expression in <math>S(A_n)</math>'s definition (i.e. <math>S(A_n) = A_{n+1} - \\frac{(A_{n+1}-A_{n})^2}{(A_{n+1}-A_{n})-(A_{n}-A_{n-1})}</math>) is more numerically stable than the expression to its left (i.e. <math>S(A_n) = \\frac{A_{n+1}\\, A_{n-1}\\, -\\, A_n^2}{A_{n+1}-2A_n+A_{n-1}}</math>). Both  Aitken's method and the Shanks transformation operate on a sequence, but the sequence the Shanks transformation operates on is usually thought of as being a sequence of partial sums, although any sequence may be viewed as a sequence of partial sums.\n\n==Example==\n\n[[Image:Shanks transformation.svg|thumb|400px|right|Absolute error as a function of <math>n</math> in the partial sums <math>A_n</math> and after applying the Shanks transformation once or several times: <math>S(A_n),</math> <math>S^2(A_n)</math> and <math>S^3(A_n).</math> The series used is <math>\\scriptstyle 4\\left(1-\\frac13+\\frac15-\\frac17+\\frac19-\\cdots\\right),</math> which has the exact sum <math>\\pi.</math>]]\nAs an example, consider the slowly convergent series<ref name=VanDyke/>\n\n:<math> 4 \\sum_{k=0}^\\infty (-1)^k \\frac{1}{2k+1} = 4 \\left( 1 - \\frac{1}{3} + \\frac{1}{5} - \\frac{1}{7} + \\cdots \\right) </math>\n\nwhich has the exact sum ''π''&nbsp;≈&nbsp;3.14159265. The partial sum <math>A_6</math> has only one digit accuracy, while six-figure accuracy requires summing about 400,000 terms.\n\nIn the table below, the partial sums <math>A_n</math>, the Shanks transformation <math>S(A_n)</math> on them, as well as the repeated Shanks transformations <math>S^2(A_n)</math> and <math>S^3(A_n)</math> are given for <math>n</math> up to 12. The figure to the right shows the absolute error for the partial sums and Shanks transformation results, clearly showing the improved accuracy and convergence rate.\n \n{| class=\"wikitable\" style=\"text-align:center; width:40%\"\n|-\n! width=\"10% | <math>n</math>\n! width=\"20% | <math>A_n</math>\n! width=\"20% | <math>S(A_n)</math>\n! width=\"20% | <math>S^2(A_n)</math>\n! width=\"20% | <math>S^3(A_n)</math>\n|-\n|  0 || 4.00000000 || —          || —          || — \n|-\n|  1 || 2.66666667 || 3.16666667 || —          || — \n|-\n|  2 || 3.46666667 || 3.13333333 || 3.14210526 || — \n|-\n|  3 || 2.89523810 || 3.14523810 || 3.14145022 || 3.14159936 \n|-\n|  4 || 3.33968254 || 3.13968254 || 3.14164332 || 3.14159086 \n|-\n|  5 || 2.97604618 || 3.14271284 || 3.14157129 || 3.14159323 \n|-\n|  6 || 3.28373848 || 3.14088134 || 3.14160284 || 3.14159244 \n|-\n|  7 || 3.01707182 || 3.14207182 || 3.14158732 || 3.14159274 \n|-\n|  8 || 3.25236593 || 3.14125482 || 3.14159566 || 3.14159261 \n|-\n|  9 || 3.04183962 || 3.14183962 || 3.14159086 || 3.14159267 \n|-\n| 10 || 3.23231581 || 3.14140672 || 3.14159377 || 3.14159264 \n|-\n| 11 || 3.05840277 || 3.14173610 || 3.14159192 || 3.14159266 \n|-\n| 12 || 3.21840277 || 3.14147969 || 3.14159314 || 3.14159265 \n|}\n\nThe Shanks transformation <math>S(A_1)</math> already has two-digit accuracy, while the original partial sums only establish the same accuracy at <math>A_{24}.</math> Remarkably, <math>S^3(A_3)</math> has six digits accuracy, obtained from repeated Shank transformations applied to the first seven terms <math>A_0, \\ldots, A_6.</math> As said before, <math>A_n</math> only obtains 6-digit accuracy after about summing 400,000 terms.\n\n==Motivation==\n\nThe Shanks transformation is motivated by the observation that — for larger <math>n</math> — the partial sum <math>A_n</math> quite often behaves approximately as<ref name=BenderOrszag368/>\n\n:<math>A_n = A + \\alpha q^n, \\,</math>\n\nwith <math>|q|<1</math> so that the sequence converges [[transient (oscillation)|transient]]ly to the series result <math>A</math> for <math>n\\to\\infty.</math>\nSo for <math>n-1,</math> <math>n</math> and <math>n+1</math> the respective partial sums are:\n\n:<math>A_{n-1} = A + \\alpha q^{n-1} \\quad , \\qquad A_n = A + \\alpha q^n \\qquad \\text{and} \\qquad A_{n+1} = A + \\alpha q^{n+1}.</math>\n\nThese three equations contain three unknowns: <math>A,</math> <math>\\alpha</math> and <math>q.</math> Solving for <math>A</math> gives<ref name=BenderOrszag368/>\n\n:<math>A = \\frac{A_{n+1}\\, A_{n-1}\\, -\\, A_n^2}{A_{n+1}-2A_n+A_{n-1}}.</math>\n\nIn the (exceptional) case that the denominator is equal to zero: then <math>A_n=A</math> for all <math>n.</math>\n\n==Generalized Shanks transformation==\n\nThe generalized ''k''th-order Shanks transformation is given as the ratio of the [[determinant]]s:<ref name=BenderOrszag389>Bender & Orszag (1999), pp. 389–392.</ref>\n:<math>\n  S_k(A_n) \n  = \\frac{\n      \\begin{vmatrix}\n        A_{n-k}          & \\cdots & A_{n-1}          & A_n              \\\\\n        \\Delta A_{n-k}   & \\cdots & \\Delta A_{n-1}   & \\Delta A_{n}     \\\\\n        \\Delta A_{n-k+1} & \\cdots & \\Delta A_{n}     & \\Delta A_{n+1}   \\\\\n        \\vdots           &        & \\vdots           & \\vdots           \\\\\n        \\Delta A_{n-1}   & \\cdots & \\Delta A_{n+k-2} & \\Delta A_{n+k-1} \\\\\n      \\end{vmatrix}\n    }{\n      \\begin{vmatrix}\n        1                & \\cdots & 1                & 1                \\\\\n        \\Delta A_{n-k}   & \\cdots & \\Delta A_{n-1}   & \\Delta A_{n}     \\\\\n        \\Delta A_{n-k+1} & \\cdots & \\Delta A_{n}     & \\Delta A_{n+1}   \\\\\n        \\vdots           &        & \\vdots           & \\vdots           \\\\\n        \\Delta A_{n-1}   & \\cdots & \\Delta A_{n+k-2} & \\Delta A_{n+k-1} \\\\\n      \\end{vmatrix}\n    },\n</math>\nwith <math>\\Delta A_p = A_{p+1} - A_p.</math> It is the solution of a model for the convergence behaviour of the partial sums <math>A_n</math> with <math>k</math> distinct transients:\n\n:<math>A_n = A + \\sum_{p=1}^k \\alpha_p q_p^n.</math>\n\nThis model for the convergence behaviour contains <math>2k+1</math> unknowns. By evaluating the above equation at the elements <math>A_{n-k}, A_{n-k+1}, \\ldots, A_{n+k}</math> and solving for <math>A,</math> the above expression for the ''k''th-order Shanks transformation is obtained. The first-order generalized Shanks transformation is equal to the ordinary Shanks transformation: <math>S_1(A_n)=S(A_n).</math>\n\nThe generalized Shanks transformation is closely related to [[Padé approximant]]s and [[Padé table]]s.<ref name=BenderOrszag389/>\n\n==See also==\n*[[Aitken's delta-squared process]]\n*[[Rate of convergence]]\n*[[Richardson extrapolation]]\n*[[Sequence transformation]]\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{citation | first=D. | last=Shanks | authorlink=Daniel Shanks | year=1955 | title=Non-linear transformation of divergent and slowly convergent sequences | journal=Journal of Mathematics and Physics | volume = 34 | pages=1–42 }}\n*{{citation | first=R. | last=Schmidt | title=On the numerical solution of linear simultaneous equations by an iterative method | journal=Philosophical Magazine | volume=32 | year=1941 | pages=369–383 }}\n*{{citation | first=M.D. | last=Van Dyke | authorlink=Milton Van Dyke | title=Perturbation methods in fluid mechanics | publisher=Parabolic Press | year=1975 | edition=annotated | isbn=0-915760-01-0 }}\n*{{citation | first1=C.M. | last1=Bender | authorlink1=Carl M. Bender | first2=S.A. | last2=Orszag | authorlink2=Steven A. Orszag | title=Advanced mathematical methods for scientists and engineers | publisher=Springer | year=1999 | isbn=0-387-98931-5 }}\n*{{Cite journal| last=Weniger |first=E.J. | title=Nonlinear sequence transformations for the acceleration of convergence and the summation of divergent series| journal=Computer Physics Reports | volume=10 | issue=5–6 | pages=189–371 | arxiv=math.NA/0306302 | year=1989 | bibcode=1989CoPhR..10..189W | doi=10.1016/0167-7977(89)90011-7 }}\n\n[[Category:Numerical analysis]]\n[[Category:Asymptotic analysis]]"
    },
    {
      "title": "Sigma approximation",
      "url": "https://en.wikipedia.org/wiki/Sigma_approximation",
      "text": "In [[mathematics]], '''σ-approximation''' adjusts a [[Fourier series|Fourier summation]] to greatly reduce the [[Gibbs phenomenon]], which would otherwise occur at [[Discontinuity (mathematics)|discontinuities]].  \n\nA σ-approximated summation for a series of period ''T'' can be written as follows:\n\n:<math>s(\\theta) = \\frac{1}{2} a_0 + \\sum_{k=1}^{m-1} \\operatorname{sinc} \\frac{k}{m} \\cdot \\left[a_{k} \\cos \\left( \\frac{2 \\pi k}{T} \\theta \\right) + b_k \\sin \\left( \\frac{2 \\pi k}{T} \\theta \\right) \\right],</math> \n\nin terms of the normalized [[sinc function]] \n\n:<math> \\operatorname{sinc} x = \\frac{\\sin \\pi x}{\\pi x}.</math>\n\nThe term \n\n:<math>\\operatorname{sinc} \\frac{k}{m}</math> \n\nis the '''Lanczos σ factor''', which is responsible for eliminating most of the Gibbs phenomenon. It does not do so entirely, however, but one can square or even cube the expression to serially attenuate Gibbs phenomenon in the most extreme cases.\n\n== See also ==\n* [[Lanczos resampling]]\n\n==References==\n{{Unreferenced|date=January 2007}}\n\n[[Category:Fourier series]]\n[[Category:Numerical analysis]]\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Significance arithmetic",
      "url": "https://en.wikipedia.org/wiki/Significance_arithmetic",
      "text": "'''Significance arithmetic''' is a set of rules (sometimes called '''significant figure rules''') for approximating the [[propagation of uncertainty]] in scientific or statistical calculations. These rules can be used to find the appropriate number of [[significant figures]] to use to represent the result of a calculation.  If a calculation is done without analysis of the uncertainty involved, a result that is written with too many significant figures can be taken to imply a higher [[Arithmetic precision|precision]] than is known, and a result that is written with too few significant figures results in an avoidable loss of precision.  Understanding these rules requires a good understanding of the concept of [[significant figures|significant and insignificant figures]].\n\nThe rules of significance arithmetic are an approximation based on statistical rules for dealing with probability distributions.  See the article on [[propagation of uncertainty]] for these more advanced and precise rules.  Significance arithmetic rules rely on the assumption that the number of significant figures in the [[operand]]s gives accurate information about the uncertainty of the operands and hence the uncertainty of the result. For an alternatives see [[interval arithmetic]] and [[floating point error mitigation]].\n\nAn important caveat is that significant figures apply only to ''measured'' values. Values known to be exact should be ignored for determining the number of significant figures that belong in the result. Examples of such values include:\n* [[integer]] counts (e.g., the number of oranges in a bag)\n* definitions of one unit in terms of another (e.g. a minute is 60 seconds)\n* actual prices asked or offered, and quantities given in requirement specifications\n* legally defined conversions, such as international currency exchange\n* scalar operations, such as \"tripling\" or \"halving\"\n* mathematical constants, such as [[π]] and [[e (mathematical constant)|e]]\n\nPhysical constants such as the [[gravitational constant]], however, have a limited number of significant digits, because these constants are known to us only by measurement. On the other hand, c ([[speed of light]]) is exactly 299,792,458&nbsp;m/s by definition.\n\n==Multiplication and division using significance arithmetic==\nWhen multiplying or dividing numbers, the result is [[rounding|rounded]] to the ''number'' of significant figures in the factor with the least significant figures. Here, the ''quantity'' of significant figures in each of the factors is important—not the ''position'' of the significant figures.  For instance, using significance arithmetic rules:\n\n*8 × 8 ≈ 6 &times; 10<sup>1</sup>\n*8 × 8.0 ≈ 6 &times; 10<sup>1</sup>\n*8.0 × 8.0 ≈ 64\n*8.02 × 8.02 ≈ 64.3\n*8 / 2.0 ≈ 4\n*8.6 /2.0012 ≈ 4.3\n*2 × 0.8 ≈ 2\n\nIf, in the above, the numbers are assumed to be measurements (and therefore probably inexact) then \"8\" above represents an inexact measurement with only one significant digit.  Therefore, the result of \"8 × 8\" is rounded to a result with only one significant digit, i.e., \"6 &times; 10<sup>1</sup>\" instead of the unrounded \"64\" that one might expect.  In many cases, the rounded result is less accurate than the non-rounded result; a measurement of \"8\" has an actual underlying quantity between 7.5 and 8.5. The true square would be in the range between 56.25 and 72.25. So 6 &times; 10<sup>1</sup> is the best one can give, as other possible answers give a false sense of accuracy.  Further, the 6 &times; 10<sup>1</sup> is itself confusing (as it might be considered to imply 60 ±5, which is over-optimistic; more accurate would be 64 ±8).\n\n==Addition and subtraction using significance arithmetic==\nWhen adding or subtracting using significant figures rules, results are rounded to the ''position'' of the least significant digit in the most uncertain of the numbers being summed (or subtracted). That is, the result is rounded to the last digit that is significant in ''each'' of the numbers being summed. Here the ''position'' of the significant figures is important, but the ''quantity'' of significant figures is irrelevant.  Some examples using these rules:\n\n:{|cellspacing=0 cellpadding=2px\n| || || ||1\n|-\n|style=\"border-bottom: 1px solid black;\"|+ ||style=\"border-bottom: 1px solid black;\"| ||style=\"border-bottom: 1px solid black;\"| ||style=\"border-bottom: 1px solid black;\"|1.1\n|-\n| || || ||2\n|}\n* 1 is significant to the ones place, 1.1 is significant to the tenths place.  Of the two, the least precise is the ones place.  The answer cannot have any significant figures past the ones place.\n:{|cellspacing=0 cellpadding=2px\n| || || ||1.0\n|-\n|style=\"border-bottom: 1px solid black;\"|+ ||style=\"border-bottom: 1px solid black;\"| ||style=\"border-bottom: 1px solid black;\"| ||style=\"border-bottom: 1px solid black;\"|1.1\n|-\n| || || ||2.1\n|}\n* 1.0 and 1.1 are significant to the tenths place, so the answer will also have a number in the tenths place.\n*: 100 + 110 ≈ 200\n* We see the answer is 200, given the significance to the hundredths place of the 100.  The answer maintains a single digits of significance in the hundreds place, just like the first term in the arithmetic.\n*: 100. + 110. = 210.\n* 100. and 110. are both significant to the ones place (as indicated by the decimal), so the answer is also significant to the ones place.\n*: 1×10<sup>2</sup> + 1.1×10<sup>2</sup> ≈ 2×10<sup>2</sup>\n* 100 is significant up to the hundreds place, while 110 is up to the tens place.  Of the two, the least accurate is the hundreds place.  The answer should not have significant digits past the hundreds place.\n*: 1.0×10<sup>2</sup> + 111 = 2.1×10<sup>2</sup>\n* 1.0×10<sup>2</sup> is significant up to the tens place while 111 has numbers up until the ones place.  The answer will have no significant figures past the tens place.\n*: 123.25 + 46.0 + 86.26 ≈ 255.5\n* 123.25 and 86.26 are significant until the hundredths place while 46.0 is only significant until the tenths place.  The answer will be significant up until the tenths place.\n*: 100 - 1 ≈ 100\n* We see the answer is 100, given the significance to the hundredths place of the 100.  It may seem counter-intuitive, but giving the nature of significant digits dictating precision, we can see how this follows from the standard rules.\n\n==Transcendental functions==\n[[Transcendental function]]s have a complicated method for determining the significance of the result. These include the [[logarithm]] function, the [[exponential function]] and the [[trigonometric functions]]. The significance of the result depends on the [[condition number]]. In general, the number of significant figures for the result is equal to the number of significant figures for the input minus the [[order of magnitude]] of the condition number.\n\nThe condition number of a differentiable function ''f'' at a point ''x'' is <math>\\left|\\frac{xf'(x)}{f(x)}\\right|;</math>  see [[Condition number#One variable|Condition number: One variable]] for details. Note that if a function has a zero at a point, its condition number at the point is infinite, as infinitesimal changes in the input can change the output from zero to non-zero, yielding a ratio with zero in the denominator, hence an infinite relative change. The condition number of the mostly used functions are as follows;<ref>http://www.cl.cam.ac.uk/~jrh13/papers/transcendentals.pdf{{full citation needed|date=January 2013}}</ref> these can be used to compute significant figures for all [[elementary function]]s:\n\n{{aligned table|cols=3|class=wikitable\n|col1style=text-align:right;vertical-align:middle;\n|col2style=text-align:right;vertical-align:middle;\n| Exponential function | <math>e^x</math> | <math>|x|</math>\n| Natural logarithm function | <math>\\ln(x)</math> | <math>\\left|\\frac{1}{\\ln(x)}\\right|</math>\n| Sine function | <math>\\sin(x)</math> | <math>\\left|x\\cot(x)\\right|</math>\n| Cosine function | <math>\\cos(x)</math> | <math>\\left|x\\tan(x)\\right|</math>\n| Tangent function | <math>\\tan(x)</math> | <math>\\left|x(\\tan(x)+\\cot(x))\\right|</math>\n| Inverse sine function | <math>\\arcsin(x)</math> | <math>\\left|\\frac{x}{\\sqrt{1-x^2}\\arcsin(x)}\\right|</math>\n| Inverse cosine function | <math>\\arccos(x)</math> | <math>\\left|\\frac{x}{\\sqrt{1-x^2}\\arccos(x)}\\right|</math>\n| Inverse tangent function | <math>\\arctan(x)</math> | <math>\\left|\\frac{x}{(1+x^2)\\arctan(x)}\\right|</math>\n}}\n\nThe fact that the number of significant figures for the result is equal to the number of significant figures for the input minus the logarithm of the condition number can be easily derived from first principles. Let <math>\\hat{x}</math> and <math>f(\\hat{x})</math> be the true values and let <math>x</math> and <math>f(x)</math> be approximate values with errors <math>\\delta x</math> and <math>\\delta f</math> respectively. Then\nwe have <math> \\hat{x} = x \\pm \\delta x </math>, <math> f( \\hat{x} ) = f(x) \\pm \\delta f </math>, and \n<math> \\pm \\delta f = f(\\hat{x}) - f(x) = f(x \\pm \\delta x) - f(x) = \\frac{f(x \\pm \\delta x) - f(x)} { \\pm \\delta x } \\cdot (\\pm \\delta x) \\approx \\pm  \\frac{df(x)}{dx} \\delta x </math>\n\nThe significant figures of a number is related to the uncertain error of the number by <math>\\delta x \\approx x \\cdot 10^{1-{\\rm(significant ~ figures ~ of ~ x)}}</math>. Substituting this into the above equation gives:\n<math> f(x) \\cdot 10^{1-{\\rm(significant ~ figures ~ of ~ f(x))}} \\approx \\frac{df(x)}{dx} x \\cdot 10^{1-{\\rm(significant ~ figures ~ of ~ x)}} </math>\n\n<math> 1-{\\rm(significant ~ figures ~ of ~ f(x))} \\approx \\log_{10} \\left ( \\frac{df(x)}{dx} \\frac{x}{f(x)} \\cdot 10^{1-{\\rm(significant ~ figures ~ of ~ x)}} \\right ) = 1-{\\rm(significant ~ figures ~ of ~ x)} + \\log_{10} \\left ( \\frac{df(x)}{dx} \\frac{x}{f(x)} \\right ) </math>\n\n<math> {\\rm(significant ~ figures ~ of ~ f(x))} \\approx {\\rm(significant ~ figures ~ of ~ x)} - \\log_{10} \\left ( \\frac{df(x)}{dx} \\frac{x}{f(x)} \\right ) </math>\n\n==Rounding rules==\nBecause significance arithmetic involves rounding, it is useful to understand a specific rounding rule that is often used when doing scientific calculations: the [[Rounding#Round half to even|round-to-even rule]] (also called ''banker's rounding'').  It is especially useful when dealing with large data sets.\n\nThis rule helps to eliminate the upwards skewing of data when using traditional rounding rules.  Whereas traditional rounding always rounds up when the following digit is 5, bankers sometimes round down to eliminate this upwards bias.\n\n''See the article on [[rounding]] for more information on rounding rules and a detailed explanation of the round-to-even rule.''\n\n==Disagreements about importance==\nSignificant figures are used extensively in high school and undergraduate courses as a shorthand for the precision with which a measurement is known. However, significant figures are ''not'' a perfect representation of uncertainty, and are not meant to be. Instead, they are a useful tool for avoiding expressing more information than the experimenter actually knows, and for avoiding rounding numbers in such a way as to lose precision.\n\nFor example, here are some important differences between significant figure rules and uncertainty:\n* Uncertainty is not the same as a mistake.  If the outcome of a particular experiment is reported as 1.234±0.056 it does not mean the observer made a mistake;  it may be that the outcome is inherently statistical, and is best described by the expression indicating a value showing only those digits that are significant, ie the known digits plus one uncertain digit, in this case  1.23±0.06.  To describe that outcome as 1.234 would be incorrect under these circumstances, even though it expresses ''less'' uncertainty.\n* Uncertainty is not the same as insignificance, and vice versa. An uncertain number may be highly significant (example: [http://www.av8n.com/physics/uncertainty.htm#sec-extracting signal averaging]). Conversely, a completely certain number may be insignificant.\n* Significance is not the same as significant ''digits''. Digit-counting is not as rigorous a way to represent significance as specifying the uncertainty separately and explicitly (such as 1.234±0.056).\n* Manual, algebraic [[propagation of uncertainty]]—the nominal topic of this article—is possible, but challenging. Alternative methods include the [http://www.av8n.com/physics/uncertainty.htm#sec-crank3 crank three times] method and the [[Monte Carlo method]]. Another option is [[interval arithmetic]], which can provide a strict upper bound on the uncertainty, but generally it is not a tight upper bound (i.e. it does not provide a ''best estimate'' of the uncertainty).  For most purposes, Monte Carlo is more useful than interval arithmetic {{Citation needed|date=March 2012}}. [[William Kahan|Kahan]] considers significance arithmetic to be unreliable as a form of automated error analysis.<ref name=JavaHurt>{{cite web|url=http://www.cs.berkeley.edu/~wkahan/JAVAhurt.pdf|title=How JAVA's Floating-Point Hurts Everyone Everywhere| author=William Kahan |date=1 March 1998|pages=37–39}}</ref>\n\nIn order to explicitly express the uncertainty in any uncertain result, the uncertainty should be given separately, with an uncertainty interval, and a confidence interval.  The expression 1.23 U95 = 0.06 implies that the true (unknowable) value of the variable is expected to lie in the interval from 1.17 to 1.29 with at least 95% confidence.  If the confidence interval is not specified it has traditionally been assumed to be 95% corresponding to two standard deviations from the mean.  Confidence intervals at one standard deviation (68%) and three standard deviations (99%) are also commonly used.\n\n==See also==\n* [[Rounding]]\n* [[Propagation of uncertainty]]\n* [[Significant figures]]\n* [[Accuracy and precision]]\n* [[MANIAC III]]\n* [[Loss of significance]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n*{{cite journal |first1=D. B. |last1=Delury |year=1958 |title=Computations with approximate numbers |journal=The Mathematics Teacher |volume=51 |issue=7 |pages=521–30 |jstor=27955748}}\n*{{cite journal |first1=E. A. |last1=Bond |year=1931 |title=Significant Digits in Computation with Approximate Numbers |journal=The Mathematics Teacher |volume=24 |issue=4 |pages=208–12 |jstor=27951340}}\n* [[ASTM]] E29-06b, Standard Practice for Using Significant Digits in Test Data to Determine Conformance with Specifications\n\n==External links==\n* [http://speleotrove.com/decimal/decifaq4.html#signif The Decimal Arithmetic FAQ — Is the decimal arithmetic ‘significance’ arithmetic?]\n* [http://www.av8n.com/physics/uncertainty.htm Advanced methods for handling uncertainty] and some explanations of the shortcomings of significance arithmetic and significant figures.\n* [http://ostermiller.org/calc/sigfig.html Significant Figures Calculator] – Displays a number with the desired number of significant digits.\n*[http://www.av8n.com/physics/uncertainty.htm Measurements and Uncertainties versus Significant Digits or Significant Figures] – Proper methods for expressing uncertainty, including a detailed discussion of the problems with any notion of significant digits.\n\n[[Category:Numerical analysis]]\n[[Category:Elementary arithmetic]]"
    },
    {
      "title": "Significant figures",
      "url": "https://en.wikipedia.org/wiki/Significant_figures",
      "text": "{{More citations needed|date=July 2013|talk=Problems}}\n{{Use dmy dates|date=May 2019|cs1-dates=y}}\n{{Order-of-approx}}\nThe '''significant figures''' (also known as the '''significant digits''') of a number are [[Numerical digit|digits]] that carry meaning contributing to its [[Accuracy and precision|measurement resolution]]. This includes all digits ''except'':<ref>''Chemistry in the Community''; Kendall-Hunt:Dubuque, IA 1988</ref>\n*  All [[leading zeros]]. For example, \"013\" has 2 significant figures: 1 and 3;\n* [[Trailing zeros]] when they are merely placeholders to indicate the scale of the number (exact rules are explained at [[#Identifying significant figures|identifying significant figures]]); and\n* [[wikt:spurious|Spurious]] digits introduced, for example, by calculations carried out to greater precision than that of the original data, or measurements reported to a greater precision than the equipment supports.\n\n[[Significance arithmetic]] is a set of approximate rules for roughly maintaining significance throughout a computation.  The more sophisticated scientific rules are known as [[propagation of uncertainty]].\n\nNumbers are often [[rounding|rounded]] to avoid reporting insignificant figures. For example, it would create [[false precision]] to express a measurement as 12.34525&nbsp;kg (which has seven significant figures) if the scales only measured to the nearest gram and gave a reading of 12.345&nbsp;kg (which has five significant figures). Numbers can also be rounded merely for simplicity rather than to indicate a given precision of measurement, for example, to make them faster to pronounce in news broadcasts.\n\n==Identifying significant figures==\n\n===Concise rules===\n[[File:Sigfigs.png|thumb|Digits in red are significant figures; those in black are not]]\n*All non-zero digits are significant: 1, 2, 3, 4, 5, 6, 7, 8, 9.\n*Zeros between non-zero digits are significant: 102, 2005, 50009.\n*Leading zeros are never significant: 0.02, 001.887, 0.000515.\n*In a number ''with'' or '' without'' a decimal point, trailing zeros (those to the right of the last non-zero digit) are significant provided they are justified by the precision of their derivation: 389,000; 2.02000; 5.400; 57.5400. More information through additional graphical symbols or explicit information on errors is needed to clarify the significance of trailing zeros.\n\n===Significant figures rules explained===\nSpecifically, the rules for identifying significant figures when writing or interpreting numbers are as follows:<ref>Giving a precise definition for the number of correct significant digits is surprisingly subtle, see {{cite book |first= Nicholas |last= Higham |title= Accuracy and Stability of Numerical Algorithms |edition= 2nd |publisher= SIAM |year= 2002 |pages = 3–5  | url=http://ftp.demec.ufpr.br/CFD/bibliografia/Higham_2002_Accuracy%20and%20Stability%20of%20Numerical%20Algorithms.pdf}}</ref>\n*All non-zero digits are considered significant. For example, 91 has two significant figures (9 and 1), while 123.45 has five significant figures (1, 2, 3, 4 and 5).\n*Zeros appearing anywhere between two non-zero digits are significant: 101.1203 has seven significant figures: 1, 0, 1, 1, 2, 0 and 3.\n* Zeros to the left of the significant figures  are not significant. For example, 0.00052 has two significant figures: 5 and 2.\nZeros to the right of the significant figures are significant if and only if they are justified by the precision of their derivation. For example, 12.2300 may have six significant figures: 1, 2, 2, 3, 0 and 0. The number 0.000122300 still has only six significant figures (the zeros before the 1 are not significant). In addition, 120.00 has five significant figures since it has three trailing zeros. However, in some contexts it may be understood that trailing zeros are only shown if they are significant: for example, if a measurement precise to four decimal places (0.0001) is given as 12.23 then it might be understood that only two decimal places of precision are available.{{Fix|text=Please clarify. This contradicts itself.}} Stating the result as 12.2300 then makes clear that it is precise to four decimal places (in this case, six significant figures).\n*The significance of trailing zeros in a number not containing a decimal point can be ambiguous. For example, it may not always be clear if a number like 1300 is precise to the nearest unit (and just happens coincidentally to be an exact multiple of a hundred) or if it is only shown to the nearest hundred due to rounding or uncertainty. Many conventions exist to address this issue, but these conventions are mostly esoteric and not understood by those who are not specialists in the subject:\n\n:*An [[overline]], sometimes also called an overbar, or less accurately, a [[Vinculum (symbol)|vinculum]], may be placed over the last significant figure; any trailing zeros following this are insignificant. For example, 13{{overline|0}}0 has three significant figures (and hence indicates that the number is precise to the nearest ten).\n\n:*Less often, using a closely related convention, the last significant figure of a number may be [[underline]]d; for example, \"2<span style=\"text-decoration: underline;\">0</span>00\" has two significant figures.\n\n:*A decimal point may be placed after the number; for example \"100.\" indicates specifically that three significant figures are meant.<ref name=\"Chemistry Significant Figures\">{{cite book |last= Myers |first= R. Thomas |last2= Oldham |first2= Keith B. |last3= Tocci |first3= Salvatore |title= Chemistry |year= 2000 |publisher= Holt Rinehart Winston |location= Austin, Texas  |isbn= 0-03-052002-9 |page=59 }}</ref>\n\n:*In the combination of a number and a [[unit of measurement]], the ambiguity can be avoided by choosing a suitable [[unit prefix]]. For example, the number of significant figures in a mass specified as 1300&nbsp;g is ambiguous, while if stated as 1.3&nbsp;kg it is not.\n\n:*The number can be expressed in Scientific Notation (see below).\n\nAs these conventions are not in general use, it is often necessary to determine from context whether such trailing zeros are intended to be significant. If all else fails, the level of rounding can be specified explicitly. The abbreviation s.f. is sometimes used, for example \"20&nbsp;000 to 2&nbsp;s.f.\" or \"20&nbsp;000 (2&nbsp;sf)\". Alternatively, the uncertainty can be stated separately and explicitly with a [[plus-minus sign]], as in 20&nbsp;000&nbsp;±&nbsp;1%, so that significant-figures rules do not apply.  This also allows specifying a precision in-between powers of ten.\n\n===Scientific notation===\nIn most cases, the same rules apply to numbers expressed in [[scientific notation]]. However, in the normalized form of that notation, placeholder leading and trailing digits do not occur, so all digits are significant. For example, {{val|0.00012}} (two significant figures) becomes {{val|1.2|e=-4}}, and {{val|0.00122300}} (six significant figures) becomes {{val|1.22300|e=-3}}. In particular, the potential ambiguity about the significance of trailing zeros is eliminated. For example, {{val|1300}} to four significant figures is written as {{val|1.300|e=3}}, while {{val|1300}} to two significant figures is written as {{val|1.3|e=3}}.\n\nThe part of the representation that contains the significant figures (as opposed to the base or the exponent) is known as the [[significand]] or mantissa.\n\n==Rounding and decimal places==\n\nThe basic concept of significant figures is often used in connection with [[rounding]]. Rounding to significant figures is a more general-purpose technique than rounding to ''n'' decimal places, since it handles numbers of different scales in a uniform way. For example, the population of a city might only be known to the nearest thousand and be stated as 52,000, while the population of a country might only be known to the nearest million and be stated as 52,000,000. The former might be in error by hundreds, and the latter might be in error by hundreds of thousands, but both have two significant figures (5 and 2). This reflects the fact that the significance of the error is the same in both cases, relative to the size of the quantity being measured.\n\nTo round to ''n'' significant figures:<ref>{{cite web\n|title = Rounding Decimal Numbers to a Designated Precision\n|author = Engelbrecht, Nancy|display-authors=etal\n|year = 1990\n|publisher = U.S. Department of Education\n|location = Washington, D.C.\n|url = https://archive.org/download/ERIC_ED327701/ERIC_ED327701.pdf\n|format = PDF}}</ref><ref name=\"Numerical Mathematics and Computing, by Cheney and Kincaid\">[https://books.google.com/books?id=ZUfVZELlrMEC&pg=PA321&lpg=PA321&dq=Condition+Number+Rule+of+Thumb&source=bl&ots=kMuMoeATcB&sig=22t9ml1TcXKbve-nAkkTJ-qAf1g&hl=en&ei=A5abTIvGOsaMnQe_17TpDw&sa=X&oi=book_result&ct=result&resnum=3&ved=0CBsQ6AEwAg#v=onepage&q=Condition%20Number%20Rule%20of%20Thumb&f=false Numerical Mathematics and Computing, by Cheney and Kincaid].</ref>\n\n* Identify the significant figures before rounding.  These are the ''n'' consecutive digits beginning with the first non-zero digit.\n* If the digit immediately to the right of the last significant figure is greater than 5 or is a 5 followed by other non-zero digits, add 1 to the last significant figure. For example, 1.2459 as the result of a calculation or measurement that only allows for 3 significant figures should be written 1.25.\n* If the digit immediately to the right of the last significant figure is a 5 not followed by any other digits or followed only by zeros, rounding requires a [[Rounding#Tie-breaking|tie-breaking]] rule. For example, to round 1.25 to 2 significant figures:\n**[[Rounding#Round half away from zero|Round half away from zero]] (also known as \"5/4\"){{Citation needed|date=August 2018|reason=By who?}} rounds up to 1.3. This is the default rounding method implied in many disciplines{{Citation needed|date=August 2018|reason=How many, and which, disciplines?}} if not specified.\n** [[Rounding#Round half to even|Round half to even]], which rounds to the nearest even number, rounds down to 1.2 in this case. The same strategy applied to 1.35 would instead round up to 1.4.\n* Replace non-significant figures in front of the decimal point by zeros.\n* Drop all the digits after the decimal point to the right of the significant figures (do not replace them with zeros).\n\nIn financial calculations, a number is often rounded to a given number of places (for example, to two places after the [[decimal separator]] for many world currencies). This is done because greater precision is immaterial, and usually it is not possible to settle a debt of less than the smallest currency unit.\n\nIn UK personal tax returns income is rounded down to the nearest pound, whilst tax paid is calculated to the nearest penny.\n\nAs an illustration, the [[decimal]] quantity '''12.345''' can be expressed with various numbers of significant digits or decimal places.  If insufficient precision is available then the number is [[rounding|rounded]] in some manner to fit the available precision.  The following table shows the results for various total precisions and decimal places.\n{| class=\"wikitable\"\n|-\n! &nbsp;<br>Precision\n! Rounded to<br>significant figures\n! Rounded to<br>decimal places\n|-\n| align=center |6\n| align=left | 12.3450\n| align=left | 12.345000\n|-\n| align=center |5\n| align=left | 12.345\n| align=left | 12.34500\n|-\n| align=center |4\n| align=left | 12.34 or 12.35\n| align=left | 12.3450\n|-\n| align=center |3\n| align=left | 12.3\n\n| align=left | 12.345\n|-\n| align=center |2\n| align=left | 12\n| align=left | 12.34 or 12.35\n|-\n| align=center |1\n| align=left | 10\n| align=left | 12.3\n|-\n| align=center |0\n| align=left {{n/a}}\n| align=left | 12\n|}\n\nAnother example for '''0.012345''':\n\n{| class=\"wikitable\"\n|-\n! &nbsp;<br>Precision\n! Rounded to<br>significant figures\n! Rounded to<br>decimal places\n|-\n| align=center |7\n| align=left | 0.01234500\n| align=left | 0.0123450\n|-\n| align=center |6\n| align=left | 0.0123450\n| align=left | 0.012345\n|-\n| align=center |5\n| align=left | 0.012345\n| align=left | 0.01234 or 0.01235\n|-\n| align=center |4\n| align=left | 0.01234 or 0.01235\n| align=left | 0.0123\n|-\n| align=center |3\n| align=left | 0.0123\n| align=left | 0.012\n|-\n| align=center |2\n| align=left | 0.012\n| align=left | 0.01\n|-\n| align=center |1\n| align=left | 0.01\n| align=left | 0.0\n|-\n| align=center |0\n| align=left {{n/a}}\n| align=left | 0\n|}\n\nThe representation of a positive number ''x'' to a precision of ''p'' significant digits has a numerical value that is given by the formula:{{Citation needed|date=July 2017}}\n:<math>10^n \\cdot \\operatorname{round}\\left(\\frac{|x|}{10^n}\\right)</math>\n:where\n:<math>n=\\lfloor \\log_{10} (x) \\rfloor + 1 - p</math>\nwhich may need to be written with a specific marking as detailed [[Significant figures#Significant figures rules explained|above]] to specify the number of significant trailing zeros.\n\n==Arithmetic==\n{{Main|Significance arithmetic}}\nAs there are rules for determining the number of significant figures in directly ''measured'' quantities, there are rules for determining the number of significant figures in quantities ''calculated'' from these ''measured'' quantities.\n\nOnly ''measured'' quantities figure into the determination of the number of significant figures in ''calculated quantities''.  Exact mathematical quantities like the {{math|π}} in the formula for the [[area of a disk|area of a circle]] with radius {{math|''r''}}, {{math|π''r''<sup>2</sup>}} has no effect on the number of significant figures in the final calculated area.  Similarly the {{math|½}} in the formula for the [[kinetic energy]] of a mass {{math|''m''}} with velocity {{math|''v''}}, {{math|½''mv''<sup>2</sup>}}, has no bearing on the number of significant figures in the final calculated kinetic energy. The constants {{math|π}} and {{math|½}} are considered for this purpose to have an ''infinite'' number of significant figures.\n\nFor quantities created from measured quantities by '''multiplication''' and '''division''', the calculated result should have as many significant figures as the ''measured'' number with the ''least'' number of significant figures.  For example, \n:1.234 × 2.0 =  2.{{overline|4}}68… ≈ 2.5, \nwith only ''two'' significant figures. The first factor has four significant figures and the second has two significant figures. The factor with the least number of significant figures is the second one with only two, so the final calculated result should also have a total of two significant figures. However see below regarding intermediate results.\n\nFor quantities created from measured quantities by '''addition''' and '''subtraction''', the last significant [[Positional notation|''decimal place'']] (hundreds, tens, ones, tenths, and so forth) in the calculated result should be the same as the ''leftmost'' or largest ''decimal place'' of the last significant figure out of all the ''measured'' quantities in the terms of the sum.  For example, \n:100.0 + 1.234 = 101.{{overline|2}}34… ≈ 101.2 \nwith the last significant figure in the ''tenths'' place. The first term has its last significant figure in the tenths place and the second term has its last significant figure in the thousandths place. The leftmost of the decimal places of the last significant figure out of all the terms of the sum is the tenths place from the first term, so the calculated result should also have its last significant figure in the tenths place.\n\nThe rules for calculating significant figures for multiplication and division are opposite to the rules for addition and subtraction. For multiplication and division, only the total number of significant figures in each of the factors matters; the decimal place of the last significant figure in each factor is irrelevant. For addition and subtraction, only the decimal place of the last significant figure in each of the terms matters; the total number of significant figures in each term is irrelevant. However, greater accuracy will often be obtained if some non-significant digits are maintained in intermediate results which are used in subsequent calculations.\n\n<!-- see [[Significance arithmetic#Addition and subtraction using significance arithmetic]] for details -->\nIn a [[Base (exponentiation)|base]] 10 [[logarithm]] of a [[normalized number]], the result should be rounded to the number of significant figures in the normalized number. For example, log<sub>10</sub>(3.000×10<sup>4</sup>) = log<sub>10</sub>(10<sup>4</sup>) + log<sub>10</sub>(3.000) ≈ 4 + 0.47712125472, should be rounded to 4.4771.\n\nWhen taking antilogarithms, the resulting number should have as many significant figures as the [[Common logarithm|mantissa]] in the logarithm.\n\nWhen performing a calculation, do not follow these guidelines for intermediate results; keep as many digits as is practical (at least 1 more than implied by the precision of the final result) until the end of calculation to avoid cumulative rounding errors.<ref>{{cite web |url= http://www.ligo.caltech.edu/~vsanni/ph3/SignificantFiguresAndMeasurements/SignificantFiguresAndMeasurements.pdf |format= PDF |archiveurl= https://web.archive.org/web/20130618184216/http://www.ligo.caltech.edu/~vsanni/ph3/SignificantFiguresAndMeasurements/SignificantFiguresAndMeasurements.pdf |archivedate= June 18, 2013 |title= Measurements and Significant Figures (Draft) |first= Virgínio |last= de Oliveira Sannibale |year= 2001 |work= Freshman Physics Laboratory |publisher= California Institute of Technology, Physics Mathematics And Astronomy Division }}</ref>\n\n==Estimating tenths==\nWhen using a ruler, initially use the smallest mark as the first estimated digit. For example, if a ruler's smallest mark is 0.1&nbsp;cm, and 4.5&nbsp;cm is read, it is 4.5 (±0.1&nbsp;cm) or 4.4&nbsp;– 4.6&nbsp;cm. However, in practice a measurement can usually be estimated by eye to closer than the interval between the ruler's smallest mark, e.g. in the above case it might be estimated as between 4.51&nbsp;cm and 4.53&nbsp;cm (see below).\n\nIt is also possible that the overall length of a ruler may not be accurate to the degree of the smallest mark, and the marks may be imperfectly spaced within each unit. However assuming a normal good quality ruler, it should be possible to estimate tenths between the nearest two marks to achieve an extra decimal place of accuracy.<ref name=\"Weston\">{{cite book|title=Experimental Electrical Testing|url=https://books.google.com/books?id=JzRGAQAAMAAJ&pg=PA72&dq=%22Experimental+Electrical+Testing.%22&hl=en&sa=X&ved=0ahUKEwjR4cPKt-3fAhU-IjQIHXHZBBgQ6AEIKDAA#v=onepage&q=%22Experimental%20Electrical%20Testing.%22&f=false|date=1914|publisher=Weston Electrical Instruments Co.|location=Newark, NJ|page=9|accessdate=14 January 2019}}</ref> Failing to do this adds the error in reading the ruler to any error in the calibration of the ruler.<ref name=\"UMmeasurements\">{{cite web|title=Measurements|url=http://slc.umd.umich.edu/slconline/SIGF/page7.html|website=slc.umd.umich.edu|publisher=University of Michigan|accessdate=3 July 2017}}</ref>\n\n==Estimation==\n{{Main|Estimation}}\nWhen estimating the proportion of individuals carrying some particular characteristic in a population, from a random sample of that population, the number of significant figures should not exceed the maximum precision allowed by that sample size.\n\n==Relationship to accuracy and precision in measurement==\n{{main|Accuracy and precision}}\n\nTraditionally, in various technical fields, \"accuracy\" refers to the closeness of a given measurement to its true value; \"precision\" refers to the stability of that measurement when repeated many times.  Hoping to reflect the way the term \"accuracy\" is actually used in the scientific community, there is a more recent standard, ISO 5725, which keeps the same definition of precision but defines the term \"trueness\" as the closeness of a given measurement to its true value and uses the term \"accuracy\" as the combination of trueness and precision.  (See the [[Accuracy and precision]] article for a fuller discussion.)  In either case, the number of significant figures roughly corresponds to ''precision'', not to either use of the word accuracy or to the newer concept of trueness. <!--The smaller digits are not significant because they effectively random noise generated by the measurement process having little to do with the true value; they, they can be omitted for some reporting purposes. (This approach ignores techniques such as averaging to produce a higher-precision result.) *****NOTE: Aside from being broken grammatically, the previous blanket statement is unlikely to be true—it depends on the precision of the instruments and the exact situation. -->\n\n==In computing==\n{{main|Floating point}}\nComputer representations of [[floating point numbers]] typically use a form of rounding to significant figures, but with [[binary number]]s. The number of correct significant figures is closely related to the notion of [[Approximation error|relative error]] (which has the advantage of being a more accurate measure of precision, and is independent of the radix, also known as the base, of the number system used).\n\n==See also==\n*[[Accuracy and precision]]\n*[[Benford's Law]] (First Digit Law)\n*[[Engineering notation]]\n*[[Error bar]]\n*[[False precision]]\n*[[IEEE754]] (IEEE floating point standard)\n*[[Interval arithmetic]]\n*[[Kahan summation algorithm]]\n*[[Precision (computer science)]]\n*[[Round-off error]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://www.khanacademy.org/video/significant-figures?playlist=Pre-algebra Significant Figures Video by Khan academy]\n\n[[Category:Arithmetic]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Sinc numerical methods",
      "url": "https://en.wikipedia.org/wiki/Sinc_numerical_methods",
      "text": "In [[numerical analysis]] and [[applied mathematics]], '''sinc numerical methods''' are numerical techniques<ref>{{Cite journal | last1 = Stenger | first1 = F. | doi = 10.1016/S0377-0427(00)00348-4 | title = Summary of sinc numerical methods | journal = Journal of Computational and Applied Mathematics | volume = 121 | pages = 379–420 | year = 2000 | pmid =  | pmc = }}</ref> for finding approximate solutions of [[partial differential equations]] and [[integral equations]] based on the translates of [[sinc]] function and Cardinal function C(f,h) which is an expansion of f defined by\n:<math>C(f,h)(x)=\\sum_{k=-\\infty}^\\infty f(kh) \\, \\textrm{sinc} \\left(\\dfrac{x}{h}-k \\right)</math>\nwhere the step size h>0 and where the sinc function is defined by\n:<math>\\textrm{sinc}(x)=\\frac{\\sin(\\pi x)}{\\pi x}</math>\nSinc approximation methods excel for problems whose solutions may have singularities, or infinite domains, or boundary layers.\n\nThe truncated Sinc expansion of f is defined by the following series:\n:<math> C_{M,N}(f,h)(x)  = \\displaystyle \\sum_{k=-M}^{N} f(kh) \\, \\textrm{sinc} \\left(\\dfrac{x}{h}-k \\right)  </math> .\n\n==Sinc numerical methods cover==\n*function approximation,\n*approximation of [[derivative]]s,\n*approximate definite and indefinite [[integral|integration]],\n*approximate solution of initial and boundary value ordinary [[differential equation]] (ODE) problems,\n*approximation and inversion of [[Fourier transform|Fourier]] and [[Laplace transform|Laplace]] transforms,\n*approximation of [[Hilbert transforms]],\n*approximation of definite and indefinite [[convolution]],\n*approximate solution of partial differential equations,\n*approximate solution of [[integral equations]],\n*construction of conformal maps.\nIndeed, Sinc are ubiquitous for approximating every operation of calculus\n\nIn the standard setup of the sinc numerical methods, the errors (in [[big O notation]]) are known to be <math>O\\left(e^{-c\\sqrt{n}}\\right)</math> with some c>0, where n is the number of nodes or bases used in the methods. However, Sugihara<ref>{{Cite journal | last1 = Sugihara | first1 = M. | last2 = Matsuo | first2 = T. | doi = 10.1016/j.cam.2003.09.016 | title = Recent developments of the Sinc numerical methods | journal = Journal of Computational and Applied Mathematics | volume = 164-165 | pages = 673 | year = 2004 | pmid =  | pmc = }}</ref> has recently found that the errors in the Sinc numerical methods based on double exponential transformation are <math>O\\left(e^{-\\frac{k n}{\\ln n}}\\right)</math> with some k>0, in a setup that is also meaningful both theoretically and practically and are found to be best possible in a certain mathematical sense.\n\n==Reading==\n*{{cite book\n |title=\n\nHandbook of Sinc Numerical Methods\n\n |last1=Stenger |first1=Frank |authorlink1=\n |coauthors=\n |editor1-last= |editor1-first= |editor1-link=\n |year= 2011\n |publisher=CRC Press\n |location=Boca Raton, Florida\n |isbn=9781439821596\n |url=\n |page=\n |pages=\n |ref=\n}}\n*{{cite book\n |title=Sinc Methods for Quadrature and Differential Equations\n |last1=Lund |first1=John |authorlink1=\n |last2=Bowers | first2=Kenneth\n |coauthors=\n |editor1-last= |editor1-first= |editor1-link=\n |year= 1992\n |publisher=Society for Industrial and Applied Mathematics (SIAM)\n |location=Philadelphia\n |isbn=9780898712988\n |url=\n |page=\n |pages=\n |ref=\n}}\n\n==References==\n{{Reflist}}\n\n[[Category:Numerical analysis]]\n\n\n{{mathapplied-stub}}"
    },
    {
      "title": "Singular boundary method",
      "url": "https://en.wikipedia.org/wiki/Singular_boundary_method",
      "text": "[[File:SBM and MFS 01.PNG|thumb|right|'''Fig. 1.''' Problem sketch and nodes distribution using the MFS: (a) interior problems, (b) exterior problems (''please click to see big pictures'')]]\n[[File:MFS and SBM 02.PNG|thumb|right|'''Fig. 2.''' Problem sketch and nodes distribution using the SBM: (c) interior problems, (d) exterior problems (''please click to see big pictures'')]]\n\nIn [[numerical analysis]], the '''singular boundary method (SBM)''' belongs to a family of [[meshfree method|meshless]] boundary [[collocation method|collocation techniques]] which include the [[method of fundamental solutions]] (MFS),<ref>[http://www.arl.wustl.edu/~rama/mfs.htm method of fundamental solutions (MFS)]</ref><ref>Golberg MA, Chen CS, Ganesh M, \"Particular solutions of 3D Helmholtz-type equations using compactly supported radial basis functions\", ''Eng Anal Bound Elem'' 2000;24(7–8): 539–47.</ref><ref>Fairweather G, Karageorghis A, \"The method of fundamental solutions for elliptic boundary value problems\", ''Adv Comput Math'' 1998;9(1): 69–95.</ref> [[boundary knot method]] (BKM),<ref>Chen W, Tanaka M, \"[http://em.hhu.edu.cn/chenwen/papers/rbf/CMA_BKM.pdf A meshless, integration-free, and boundary-only RBF technique] {{webarchive|url=https://web.archive.org/web/20160304031518/http://em.hhu.edu.cn/chenwen/papers/rbf/CMA_BKM.pdf |date=2016-03-04 }}\", ''Comput Math Appl '' 2002;43(3–5): 379–91.</ref> [[regularized meshless method]] (RMM),<ref>D.L. Young, K.H. Chen, C.W. Lee, \"Novel meshless method for solving the potential problems with arbitrary domain\", ''J Comput Phys'' 2005;209(1): 290–321.</ref> [[boundary particle method]] (BPM),<ref>[http://em.hhu.edu.cn/chenwen/html/BPM.htm boundary particle method (BPM)]</ref> modified MFS,<ref>Sarler B, \"Solution of potential flow problems by the modified method of fundamental solutions: Formulations with the single layer and the double layer fundamental solutions\", ''Eng Anal Bound Elem'' 2009;33(12): 1374–82.</ref> and so on. This family of strong-form collocation methods is designed to avoid singular numerical integration and mesh generation in the traditional [[boundary element method]] (BEM) in the numerical solution of boundary value problems with boundary nodes, in which a fundamental solution of the governing equation is explicitly known. \n\nThe salient feature of the SBM is to overcome the fictitious boundary in the method of fundamental solution, while keeping all merits of the latter. The method offers several advantages over the classical domain or boundary discretization methods, among which are:\n\n* '''meshless.''' The method requires neither domain nor boundary meshing but boundary-only discretization points;\n* '''integration-free.''' The numerical integration of singular or nearly singular kernels could be otherwise troublesome, expensive, and complicated, as in the case, for example, the boundary element method;\n* '''boundary-only discretization for homogeneous problems.''' The SBM shares all the advantages of the BEM over domain discretization methods such as the finite element or finite difference methods;\n* '''to overcome the perplexing fictitious boundary in the method of fundamental solutions''' (see Figs. 1 and 2), thanks to the introduction of the concept of the origin intensity factor, which isolates the singularity of the fundamental solutions.\n\nThe SBM provides a significant and promising alternative to popular boundary-type methods such as the BEM and MFS, in particular, for infinite domain, wave, thin-walled structures, and inverse problems.  \n\n== History of the singular boundary method ==\nThe methodology of the SBM was firstly proposed by Chen and his collaborators in 2009.<ref>Chen W, \"[http://www.ccms.ac.cn/guy/SBM%20%20Acta%20Mech%20Solid%20Sin%2009.pdf Singular boundary method: A novel, simple, meshfree, boundary collocation numerical method]\", ''Chin J Solid Mech'' 2009;30(6): 592–9.</ref><ref>Chen W, Wang FZ, \"[http://em.hhu.edu.cn/chenwen/papers/rbf/EABE-SBM.pdf A method of fundamental solutions without fictitious boundary] {{webarchive|url=https://web.archive.org/web/20150606193008/http://em.hhu.edu.cn/chenwen/papers/rbf/EABE-SBM.pdf |date=2015-06-06 }}\", ''Eng Anal Bound Elem'' 2010;34(5): 530–32.</ref> The basic idea is to introduce a concept of the origin intensity factor to isolate the singularity of the fundamental solutions so that the source points can be placed directly on the real boundary. In comparison, the method of fundamental solutions requires a fictitious boundary for placing the source points to avoid the singularity of fundamental solution. The SBM has since been successfully applied to a variety of physical problems, such as potential problems,<ref>Wei X, Chen W, Fu ZJ, \"Solving inhomogeneous problems by singular boundary method\", ''J Mar SCI Tech'' 2012; 20(5).</ref><ref>Chen W, Fu ZJ, Wei X, \"[http://www.ccms.ac.cn/guy/CMES%20chen%20&%20fu%202009.pdf Potential Problems by Singular Boundary Method Satisfying Moment Condition]\", ''Comput Model Eng Sci'' 2009;54(1): 65–85.</ref> infinite domain problem,<ref>Chen W, Fu Z, \"[http://www.ccms.ac.cn/guy/Chin%20Sci%20Bull%20Chen%20&%20Fu%202010.pdf A novel numerical method for infinite domain potential problems]\", ''Chin Sci Bull'' 2010;55(16): 1598–603.</ref> Helmholtz problem,<ref>Fu ZJ, Chen W, \"A novel boundary meshless method for radiation and scattering problems\", ''Advances in Boundary Element Techniques XI, Proceedings of the 11th international Conference'', 12–14 July 2010, 83–90, Published by EC Ltd, United Kingdom ({{isbn|978-0-9547783-7-8}})</ref> and plane elasticity problem.<ref>Gu Y, Chen W, Zhang CZ., \"[http://www.ccms.ac.cn/guy/IJSS%202011.pdf Singular boundary method for solving plane strain elastostatic problems]\", ''Int J Solids Struct'' 2011;48(18): 2549–56.</ref>\n\nThere are the two techniques to evaluate the origin intensity factor. The first approach is to place a cluster of sample nodes inside the problem domain and to calculate the algebraic equations. The strategy leads to extra computational costs and makes the method is not as efficient as expected compared to the MFS. The second approach<ref>Chen W, Gu Y, \"[http://www.ccms.ac.cn/guy/Chen%20Wen%20Tai%20Wan%20Conference.pdf Recent advances on singular boundary method]\", ''Joint International Workshop on Trefftz Method VI and Method of Fundamental Solution II'', Taiwan 2011.</ref><ref>Gu Y, Chen, W, \"[http://www.ccms.ac.cn/guy/lixuexuebao%20SBM%20for%203D.pdf Improved singular boundary method for three dimensional potential problems]\", ''Chinese Journal of Theoretical and Applied Mechanics'', 2012, 44(2): 351-360 (in Chinese)</ref> is to employ a regularization technique to cancel the singularities of the fundamental solution and its derivatives. Consequently, the origin intensity factors can be determined directly without using any sample nodes. This scheme makes the method more stable, accurate, efficient, and extends its applicability.\n\n== Recent developments ==\n\n=== Boundary layer effect problems ===\n\nLike all the other boundary-type numerical methods, also it is observed that the SBM encounters a dramatic drop of solution accuracy at the region nearby boundary. Unlike singularity at origin, the fundamental solution at near-boundary regions remains finite. However, instead of being a flat function, the interpolation function develops a sharp peak as the field point approaches the boundary. Consequently, the kernels become “nearly singular” and can not accurately be calculated. This is similar to the so-called boundary layer effect encountered in the BEM-based methods.\n\nA nonlinear transformation, based on the [[hyperbolic sine|sinh function]], can be employed to remove or damp out the rapid variations of the nearly singular kernels.<ref>Gu Y, Chen W, Zhang J, \"[http://www.ccms.ac.cn/guy/EABE%20gu%20&%20chen%202012.pdf Investigation on near-boundary solutions by singular boundary method]\", ''Eng Anal Bound Elem'' 2012;36(8): 117–82.</ref> As a result, the troublesome boundary layer effect in the SBM has been successfully remedied. The implementation of this transformation is straightforward and can easily be embedded in existing SBM programs. For the test problems studied, very promising results are obtained even when the distance between the field point and the boundary is as small as 1{{e|-10}}.\n\n=== Large-scale problems ===\n\nLike the MFS and BEM, the SBM will produce dense coefficient matrices, whose operation count and the memory requirements for matrix equation buildup are of the order of ''O''(''N''<sup>2</sup>) which is computationally too expensive to simulate large-scale problems. \n\nThe [[fast multipole method]] (FMM) can reduce both CPU time and memory requirement from ''O''(''N''<sup>2</sup>) to ''O''(''N'') or ''O''(''N''log''N''). With the help of FMM, the SBM can be fully capable of solving a large scale problem of several million unknowns on a desktop. This fast algorithm dramatically expands the applicable territory of the SBM to far greater problems than were previously possible.\n\n== See also ==\n* [[Meshfree methods]]\n* [[Radial basis function]]\n* [[Trefftz method]]\n\n== References  ==\n{{Reflist}}\n\n== External links ==\n* [http://em.hhu.edu.cn/chenwen/html/KDF.htm Kernel distance functions and radial basis functions]\n* [http://www.ccms.ac.cn/guy/sbm.html Singular Boundary Method]\n\n{{Numerical PDE}}\n\n{{DEFAULTSORT:Singular boundary method}}\n[[Category:Numerical analysis]]\n[[Category:Numerical differential equations]]"
    },
    {
      "title": "Sparse grid",
      "url": "https://en.wikipedia.org/wiki/Sparse_grid",
      "text": "'''Sparse grids''' are numerical techniques to represent, integrate or interpolate high [[dimension]]al functions. They were originally developed by the [[Russia]]n [[mathematician]] [[Sergey A. Smolyak]], a student of [[Lazar Lyusternik]], and are based on a sparse tensor product construction. Computer algorithms for efficient implementations of such grids were later developed by [[Michael Griebel]] and [[Christoph Zenger]].\n\n== [[Curse of dimensionality]] ==\nThe standard way of representing multidimensional functions are tensor or full grids. The number of basis functions or nodes (grid points) that have to be stored and processed [[exponential function|depend exponentially]] on the number of dimensions. Even with today's computational power it is not possible to process functions with more than 4 or 5 dimensions.\n\nThe curse of dimension is expressed in the order of the integration error that is made by a quadrature of level <math>l</math>, with <math>N_{l}</math> points. The function has regularity <math>r</math>, i.e. is <math>r</math> times differentiable. The number of dimensions is <math>d</math>.\n\n<math>|E_l| = O(N_l^{-\\frac{r}{d}})</math>\n\n== Smolyak's quadrature rule ==\nSmolyak found a computationally more efficient method of integrating multidimensional functions based on a univariate quadrature rule <math>Q^{(1)}</math>. The <math>d</math>-dimensional Smolyak integral <math>Q^{(d)}</math> of a function <math>f</math> can be written as a recursion formula with the [[tensor product]].\n\n<math>Q_l^{(d)} f = \\left(\\sum_{i=1}^l \\left(Q_i^{(1)}-Q_{i-1}^{(1)}\\right)\\otimes Q_{l-i+1}^{(d-1)}\\right)f</math>\n\nThe index to <math>Q</math> is the level of the discretization. A <math>1-d</math> integration on level <math>i</math> is computed by the evaluation of <math>O(2^{i})</math> points. The error estimate for a function of regularity <math>r</math> is:\n\n<math>|E_l| = O\\left(N_l^{-r}\\left(\\log N_l\\right)^{(d-1)(r+1)}\\right)</math>\n\n== References ==\n* [http://www.lrr.in.tum.de/~murarasu/ppopp027s-murarasu.pdf A memory efficient data structure for regular sparse grids]\n* [http://sparse-grids.de/ Code to generate (and pre-generated) nodes and weights for quadrature]\n* [http://wissrech.iam.uni-bonn.de/research/projects/zumbusch/fd.html Finite difference scheme on sparse grids]\n* [https://web.archive.org/web/20120219044130/http://cumbia.informatik.uni-stuttgart.de/ger/research/fields/recent/sparse/ Visualization on sparse grids]\n* [http://wissrech.iam.uni-bonn.de/research/pub/garcke/kdd.pdf Datamining on sparse grids,  J.Garcke, M.Griebel (pdf)]\n\n*[http://garcke.ins.uni-bonn.de/research/pub/sparse_grids_nutshell_code.pdf  Jochen Garcke:\"Sparse Grids in a Nutshell\" (pdf)]\n*[https://web.stanford.edu/~paulcon/slides/Oxford_2012.pdf  Paul Constantine: \"Experiences with Sparse Grids and Smolyak-type Approximations\" (pdf)]\n* [https://www5.in.tum.de/pub/zenger91sg.pdf Christoph Zenger: \"Sparse Grids\" (pdf)]\n* Garcke, Jochen(Ed.) and Griebel, Michael (Ed.) : \"Sparse Grids and Applications\", Springer, {{ISBN|978-3-642-31702-6}} (2013).\n*[https://bfi.uchicago.edu/sites/default/files/file_uploads/Scheidegger_slides.pdf J. Brumm and S. Scheidegger:\"Using Adaptive Sparse Grids to Solve High-Dimensional Dynamic Models\",(2013) (pdf)]\n*[http://www.sparse-grids.de/ \"Quadrature on sparse grids\"]\n\n{{Mathanalysis-stub}}\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Spectral method",
      "url": "https://en.wikipedia.org/wiki/Spectral_method",
      "text": "{{More footnotes|date=August 2013}}\n\n'''Spectral methods''' are a class of techniques used in [[applied mathematics]] and [[scientific computing]] to numerically solve certain [[differential equation]]s, potentially involving the use of the [[fast Fourier transform]]. The idea is to write the solution of the differential equation as a sum of certain \"[[basis function|basis functions]]\" (for example, as a [[Fourier series]] which is a sum of [[Sine wave|sinusoid]]s) and then to choose the coefficients in the sum in order to satisfy the differential equation as well as possible.\n\nSpectral methods and [[finite element method]]s are closely related and built on the same ideas; the main difference between them is that spectral methods use basis functions that are nonzero over the whole domain, while finite element methods use basis functions that are nonzero only on small subdomains. In other words, spectral methods take on a ''global approach'' while finite element methods use a ''local approach''. Partially for this reason, spectral methods have excellent error properties, with the so-called \"exponential convergence\" being the fastest possible, when the solution is [[Smooth function|smooth]]. However, there are no known three-dimensional single domain spectral [[shock capturing]] results (shock waves are not smooth).<ref name=\"CHQZ\">[https://books.google.com/books?id=7COgEw5_EBQC pp 235, Spectral Methods]: evolution to complex geometries and applications to fluid dynamics, By Canuto, Hussaini, Quarteroni and Zang, Springer, 2007.</ref> In the finite element community, a method where the degree of the elements is very high or increases as the grid parameter ''h'' decreases to zero is sometimes called a [[spectral element method]].\n\nSpectral methods can be used to solve [[ordinary differential equations]] (ODEs), [[partial differential equations]] (PDEs) and [[eigenvalue]] problems involving differential equations. When applying spectral methods to time-dependent PDEs, the solution is typically written as a sum of basis functions with time-dependent coefficients; substituting this in the PDE yields a system of ODEs in the coefficients which can be solved using any [[numerical methods for ordinary differential equations|numerical method for ODEs]]. Eigenvalue problems for ODEs are similarly converted to matrix eigenvalue problems {{Citation needed|date=August 2013}}.\n\nSpectral methods were developed in a long series of papers by [[Steven Orszag]] starting in 1969 including, but not limited to, Fourier series methods for periodic geometry problems, polynomial spectral methods for finite and unbounded geometry problems, pseudospectral methods for highly nonlinear problems, and spectral iteration methods for fast solution of steady state problems. The implementation of the spectral method is normally accomplished either with [[collocation method|collocation]] or a [[Galerkin method|Galerkin]] or a [[Tau method|Tau]] approach.\n\nSpectral methods are computationally less expensive than finite element methods, but become less accurate for problems with complex geometries and discontinuous coefficients. This increase in error is a consequence of the [[Gibbs phenomenon]].\n\n==Examples of spectral methods==\n\n===A concrete, linear example===\n\nHere we presume an understanding of basic multivariate [[calculus]] and [[Fourier series]]. If <math>g(x,y)</math> is a known, complex-valued function of two real variables, and g is periodic in x and y (that is, <math>g(x,y)=g(x+2\\pi,y)=g(x,y+2\\pi)</math>) then we are interested in finding a function f(x,y) so that\n\n:<math>\\left(\\frac{\\partial^2}{\\partial x^2}+\\frac{\\partial^2}{\\partial y^2}\\right)f(x,y)=g(x,y)\\quad \\text{for all } x,y</math>\n<!--math>f_{xx}(x,y)+f_{yy}(x,y)=g(x,y)\\quad \\text{for all} x,y</math-->\n\nwhere the expression on the left denotes the second partial derivatives of f in x and y, respectively. This is the [[Poisson equation]], and can be physically interpreted as some sort of heat conduction problem, or a problem in potential theory, among other possibilities.\n\nIf we write f and g in Fourier series:\n\n:<math>f=:\\sum a_{j,k}e^{i(jx+ky)}</math>\n:<math>g=:\\sum b_{j,k}e^{i(jx+ky)}</math>\n\nand substitute into the differential equation, we obtain this equation:\n\n:<math>\\sum -a_{j,k}(j^2+k^2)e^{i(jx+ky)}=\\sum b_{j,k}e^{i(jx+ky)}</math>\n\nWe have exchanged partial differentiation with an infinite sum, which is legitimate if we assume for instance that ''f'' has a continuous second derivative. By the uniqueness theorem for Fourier expansions, we must then equate the Fourier coefficients term by term, giving\n\n:(*) <math>a_{j,k}=-\\frac{b_{j,k}}{j^2+k^2}</math>\n\nwhich is an explicit formula for the Fourier coefficients ''a''<sub>''j'',''k''</sub>.\n\nWith periodic boundary conditions, the [[Poisson equation]] possesses a solution only if ''b''<sub>''0'',''0''</sub> = ''0''. Therefore,\nwe can freely choose ''a''<sub>''0'',''0''</sub> which will be equal to the mean of the resolution. This corresponds to choosing the\nintegration constant.\n\nTo turn this into an algorithm, only finitely many frequencies are solved for. This introduces an error which can be shown to be proportional to <math>h^n</math>, where <math>h:=1/n</math> and <math>n</math> is the highest frequency treated.\n\n==== Algorithm ====\n\n# Compute the Fourier transform (''b<sub>j,k''</sub>) of ''g''.\n# Compute the Fourier transform (''a<sub>j,k</sub>'') of ''f'' via the formula (*).\n# Compute ''f'' by taking an inverse Fourier transform of (''a<sub>j,k''</sub>).\n\nSince we're only interested in a finite window of frequencies (of size ''n'', say) this can be done using a [[Fast Fourier Transform]] algorithm. Therefore, globally the algorithm runs in time ''O''(''n'' log ''n'').\n\n===Nonlinear example===\n\nWe wish to solve the forced, transient, nonlinear [[Burgers' equation]] using a spectral approach.\n\nGiven <math>u(x,0)</math> on the periodic domain\n<math>x\\in\\left[0,2\\pi\\right)</math>, find <math>u \\in \\mathcal{U}</math> such that\n:<math>\\partial_{t} u + u \\partial_{x} u = \\rho \\partial_{xx} u + f \\quad \\forall x\\in\\left[0,2\\pi\\right), \\forall t>0</math>\nwhere &rho; is the [[viscosity]] coefficient. In weak conservative form this becomes\n:<math>\\left\\langle \\partial_{t} u , v \\right\\rangle = \\left\\langle  \\partial_x \\left(-\\frac{1}{2} u^2 + \\rho \\partial_{x} u\\right) , v \\right\\rangle + \\left\\langle f, v \\right\\rangle \\quad \\forall v\\in \\mathcal{V}, \\forall t>0</math>\nwhere <math>\\langle f, g \\rangle := \\int_{0}^{2\\pi} f(x)\n  \\overline{g(x)}\\,dx</math> following [[inner product space|inner product]] notation. [[integration by parts|Integrating by parts]] and using periodicity grants\n:<math>\\langle \\partial_{t} u , v \\rangle = \\left\\langle   \\frac{1}{2} u^2 - \\rho \\partial_{x} u  ,  \\partial_x v\\right\\rangle+\\left\\langle f, v \\right\\rangle \\quad \\forall v\\in \\mathcal{V}, \\forall t>0.</math>\n\nTo apply the Fourier-[[Galerkin method]], choose both\n:<math>\\mathcal{U}^N := \\left\\{ u : u(x,t)=\\sum_{k=-N/2}^{N/2-1} \\hat{u}_{k}(t) e^{i k x}\\right\\}</math>\nand\n:<math>\\mathcal{V}^N :=\\text{ span}\\left\\{ e^{i k x} : k\\in -N/2,\\dots,N/2-1\\right\\}</math>\nwhere <math>\\hat{u}_k(t):=\\frac{1}{2\\pi}\\langle u(x,t), e^{i k x} \\rangle</math>. This reduces the problem to finding <math>u\\in\\mathcal{U}^N</math> such that\n:<math>\\langle \\partial_{t} u , e^{i k x} \\rangle = \\left\\langle  \\frac{1}{2} u^2 - \\rho \\partial_{x} u  ,  \\partial_x e^{i k x}  \\right\\rangle + \\left\\langle f, e^{i k x} \\right\\rangle \\quad \\forall k\\in \\left\\{ -N/2,\\dots,N/2-1 \\right\\}, \\forall t>0.</math>\n\nUsing the [[orthogonality]] relation <math>\\langle e^{i l x}, e^{i k x} \\rangle = 2 \\pi \\delta_{lk}</math> where <math>\\delta_{lk}</math> is the [[Kronecker delta]], we simplify the above three terms for each <math>k</math> to see\n:<math>\n\\begin{align}\n\\left\\langle \\partial_{t} u , e^{i k x}\\right\\rangle &= \\left\\langle     \\partial_{t} \\sum_{l} \\hat{u}_{l} e^{i l x}     ,      e^{i k x} \\right\\rangle  = \\left\\langle    \\sum_{l} \\partial_{t} \\hat{u}_{l} e^{i l x}    ,     e^{i k x} \\right\\rangle = 2 \\pi \\partial_t \\hat{u}_k,\n\\\\\n\\left\\langle f , e^{i k x} \\right\\rangle &= \\left\\langle    \\sum_{l} \\hat{f}_{l} e^{i l x}    ,     e^{i k x}\\right\\rangle= 2 \\pi \\hat{f}_k, \\text{ and}\n\\\\\n\\left\\langle\n  \\frac{1}{2} u^2 - \\rho \\partial_{x} u\n  ,\n  \\partial_x e^{i k x}\n\\right\\rangle\n&=\n\\left\\langle\n    \\frac{1}{2}\n    \\left(\\sum_{p} \\hat{u}_p e^{i p x}\\right)\n    \\left(\\sum_{q} \\hat{u}_q e^{i q x}\\right)\n    - \\rho \\partial_x \\sum_{l} \\hat{u}_l e^{i l x}\n    ,\n    \\partial_x e^{i k x}\n\\right\\rangle\n\\\\\n&=\n\\left\\langle\n    \\frac{1}{2}\n    \\sum_{p} \\sum_{q} \\hat{u}_p \\hat{u}_q e^{i \\left(p+q\\right) x}\n    ,\n    i k e^{i k x}\n\\right\\rangle\n-\n\\left\\langle\n    \\rho i \\sum_{l} l \\hat{u}_l e^{i l x}\n    ,\n    i k e^{i k x}\n\\right\\rangle\n\\\\\n&=\n-\\frac{i k}{2}\n\\left\\langle\n    \\sum_{p} \\sum_{q} \\hat{u}_p \\hat{u}_q e^{i \\left(p+q\\right) x}\n    ,\n    e^{i k x}\n\\right\\rangle\n- \\rho k\n\\left\\langle\n    \\sum_{l} l \\hat{u}_l e^{i l x}\n    ,\n    e^{i k x}\n\\right\\rangle\n\\\\\n&=\n- i \\pi k \\sum_{p+q=k} \\hat{u}_p \\hat{u}_q - 2\\pi\\rho{}k^2\\hat{u}_k.\n\\end{align}\n</math>\n\nAssemble the three terms for each <math>k</math> to obtain\n:<math>\n2 \\pi \\partial_t \\hat{u}_k\n=\n- i \\pi k \\sum_{p+q=k} \\hat{u}_p \\hat{u}_q \n- 2\\pi\\rho{}k^2\\hat{u}_k\n+ 2 \\pi \\hat{f}_k\n\\quad k\\in\\left\\{ -N/2,\\dots,N/2-1 \\right\\}, \\forall t>0.\n</math>\nDividing through by <math>2\\pi</math>, we finally arrive at\n:<math>\n\\partial_t \\hat{u}_k\n=\n- \\frac{i k}{2} \\sum_{p+q=k} \\hat{u}_p \\hat{u}_q \n- \\rho{}k^2\\hat{u}_k\n+ \\hat{f}_k\n\\quad k\\in\\left\\{ -N/2,\\dots,N/2-1 \\right\\}, \\forall t>0.\n</math>\nWith Fourier transformed initial conditions <math>\\hat{u}_{k}(0)</math> and forcing <math>\\hat{f}_{k}(t)</math>, this coupled system of ordinary differential equations may be integrated in time (using, e.g., a [[Runge Kutta]] technique) to find a solution. The nonlinear term is a [[convolution]], and there are several transform-based techniques for evaluating it efficiently. See the references by Boyd and Canuto et al. for more details.\n\n== A relationship with the spectral element method ==\n\nOne can show that if <math>g</math> is infinitely differentiable, then the numerical algorithm using Fast Fourier Transforms will converge faster than any polynomial in the grid size h. That is, for any n>0, there is a <math>C_n<\\infty</math> such that the error is less than <math>C_nh^n</math> for all sufficiently small values of <math>h</math>. We say that the spectral method is of order <math>n</math>, for every n>0.\n\nBecause a [[spectral element method]] is a [[finite element method]] of very high order, there is a similarity in the convergence properties. However, whereas the spectral method is based on the eigendecomposition of the particular boundary value problem, the finite element method does not use that information and works for arbitrary [[elliptic boundary value problem]]s.\n\n== See also ==\n* [[Finite element method]]\n* [[Gaussian grid]]\n* [[Pseudo-spectral method]]\n* [[Spectral element method]]\n* [[Galerkin method]]\n* [[Collocation method]]\n\n{{Numerical PDE}}\n\n== References ==\n{{Reflist}}\n* Bengt Fornberg (1996) ''A Practical Guide to Pseudospectral Methods.'' Cambridge University Press, Cambridge, UK\n* [http://www-personal.umich.edu/~jpboyd/BOOK_Spectral2000.html Chebyshev and Fourier Spectral Methods] by John P. Boyd.\n* Canuto C., [[M. Yousuff Hussaini|Hussaini M. Y.]], Quarteroni A., and Zang T.A. (2006) ''Spectral Methods. Fundamentals in Single Domains.'' Springer-Verlag, Berlin Heidelberg\n* Javier de Frutos, Julia Novo: [http://epubs.siam.org/sam-bin/dbq/article/35198 A Spectral Element Method for the Navier--Stokes Equations with Improved Accuracy]\n* [http://cdm.unimo.it/home/matematica/funaro.daniele/bube.htm Polynomial Approximation of Differential Equations], by Daniele Funaro, Lecture Notes in Physics, Volume 8, Springer-Verlag, Heidelberg 1992\n* D. Gottlieb and S. Orzag (1977) \"Numerical Analysis of Spectral Methods : Theory and Applications\", SIAM, Philadelphia, PA\n* J. Hesthaven, S. Gottlieb and D. Gottlieb (2007) \"Spectral methods for time-dependent problems\", Cambridge UP, Cambridge, UK\n* Steven A. Orszag (1969) ''Numerical Methods for the Simulation of Turbulence'', Phys. Fluids Supp. II, 12, 250-257\n*{{Cite book | last1=Press | first1=WH | last2=Teukolsky | first2=SA | last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press |  location=New York | isbn=978-0-521-88068-8 | chapter=Section 20.7. Spectral Methods | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=1083}}\n* Lloyd N. Trefethen (2000) ''Spectral Methods in MATLAB.'' SIAM, Philadelphia, PA\n\n{{DEFAULTSORT:Spectral Method}}\n[[Category:Numerical analysis]]\n[[Category:Numerical differential equations]]"
    },
    {
      "title": "Stiffness matrix",
      "url": "https://en.wikipedia.org/wiki/Stiffness_matrix",
      "text": ":''For the stiffness tensor in solid mechanics, see [[Hooke's law#Matrix representation (stiffness tensor)]].''\n\nIn the [[finite element method]] for the numerical solution of elliptic [[partial differential equations]], the '''stiffness matrix''' represents the system of linear equations that must be solved in order to ascertain an approximate solution to the differential equation.\n\n==The stiffness matrix for the Poisson problem==\nFor simplicity, we will first consider the [[Poisson problem]]\n\n:<math> -\\nabla^2 u = f</math>\n\non some domain Ω, subject to the boundary condition ''u'' = 0 on the boundary of Ω. To discretize this equation by the finite element method, one chooses a set of ''basis functions'' {''φ''<sub>1</sub>, ..., ''φ''<sub>''n''</sub>} defined on Ω which also vanish on the boundary. One then approximates\n\n:<math> u \\approx u^h = u_1\\varphi_1+\\cdots+u_n\\varphi_n.</math>\n\nThe coefficients ''u''<sub>1</sub>, ..., ''u''<sub>''n''</sub> are determined so that the error in the approximation is orthogonal to each basis function ''φ''<sub>''i''</sub>:\n\n:<math> \\int_\\Omega \\varphi_i\\cdot f \\, dx = -\\int_\\Omega \\varphi_i\\nabla^2u^h \\, dx = -\\sum_j\\left(\\int_\\Omega \\varphi_i\\nabla^2\\varphi_j\\,dx\\right)\\, u_j = \\sum_j\\left(\\int_\\Omega \\nabla\\varphi_i\\cdot\\nabla\\varphi_j\\, dx\\right)u_j.</math>\n\nThe '''stiffness matrix''' is the n-element square matrix A defined by\n\n:<math> A_{ij} = \\int_\\Omega\\nabla\\varphi_i\\cdot\\nabla\\varphi_j\\, dx.</math>\n\nBy defining the vector ''F'' with components ''F''<sub>''i''</sub> = <math>\\int_\\Omega\\varphi_i f\\,dx</math>, the coefficients ''u''<sub>''i''</sub> are determined by the linear system ''AU'' = ''F''. The stiffness matrix is symmetric, i.e. ''A''<sub>''ij''</sub> = ''A''<sub>''ji''</sub>, so all its eigenvalues are real. Moreover, it is a strictly [[positive-definite matrix]], so that the system ''AU'' = ''F'' always has a unique solution. (For other problems, these nice properties will be lost.)\n\nNote that the stiffness matrix will be different depending on the computational grid used for the domain and what type of finite element is used. For example, the stiffness matrix when piecewise quadratic finite elements are used will have more degrees of freedom than piecewise linear elements.\n\n==The stiffness matrix for other problems==\nDetermining the stiffness matrix for other PDE follows essentially the same procedure, but it can be complicated by the choice of boundary conditions. As a more complex example, consider the elliptic equation\n\n:<math> -\\sum_{k,l}\\frac{\\partial}{\\partial x_k}\\left(a^{kl}\\frac{\\partial u}{\\partial x_l}\\right) = f</math>\n\nwhere ''A''(''x'') = ''a''<sup>''kl''</sup>(''x'') is a positive-definite matrix defined for each point ''x'' in the domain. We impose  the [[Robin boundary condition]]\n\n:<math> -\\sum_{k,l}\\nu_k a^{kl}\\frac{\\partial u}{\\partial x_l} = c(u-g),</math>\n\nwhere ''ν''<sub>''k''</sub> is the component of the unit outward normal vector ''ν'' in the ''k''-th direction. The system to be solved is\n\n:<math> \\sum_j\\left(\\sum_{k,l}\\int_\\Omega a^{kl}\\frac{\\partial\\varphi_i}{\\partial x_k}\\frac{\\partial\\varphi_j}{\\partial x_l}dx+\\int_{\\partial\\Omega}c\\varphi_i\\varphi_j\\, ds\\right)u_j = \\int_\\Omega\\varphi_i f\\, dx+\\int_{\\partial\\Omega}c\\varphi_i g\\, ds,</math>\n\nas can be shown using an analogue of Green's identity. The coefficients ''u''<sub>''i''</sub> are still found by solving a system of linear equations, but the matrix representing the system is markedly different from that for the ordinary Poisson problem.\n\nIn general, to each scalar elliptic operator ''L'' of order 2''k'', there is associated a bilinear form ''B'' on the [[Sobolev space]] ''H''<sup>''k''</sup>, so that the [[weak formulation]] of the equation ''Lu'' = ''f'' is\n\n:<math> B[u,v] = (f,v)</math>\n\nfor all functions ''v'' in ''H''<sup>''k''</sup>. Then the stiffness matrix for this problem is\n\n:<math> A_{ij} = B[\\varphi_j,\\varphi_i].</math>\n\n==Practical assembly of the stiffness matrix==\nIn order to implement the finite element method on a computer, one must first choose a set of basis functions and then compute the integrals defining the stiffness matrix. Usually, the domain Ω is discretized by some form of [[mesh generation]], wherein it is divided into non-overlapping triangles or quadrilaterals, which are generally referred to as elements. The basis functions are then chosen to be polynomials of some order within each element, and continuous across element boundaries. The simplest choices are piecewise linear for triangular elements and piecewise bilinear for rectangular elements.\n\nThe '''element stiffness matrix''' ''A''<sup>[''k'']</sup> for element ''T''<sub>''k''</sub> is the matrix\n\n:<math> A^{[k]}_{ij} = \\int_{T_k}\\nabla\\varphi_i\\cdot\\nabla\\varphi_j\\, dx.</math>\n\nThe element stiffness matrix is zero for most values of i and j, for which the corresponding basis functions are zero within ''T''<sub>''k''</sub>. The full stiffness matrix ''A'' is the sum of the element stiffness matrices. In particular, for basis functions that are only supported locally, the stiffness matrix is [[sparse matrix|sparse]].\n\nFor many standard choices of basis functions, i.e. piecewise linear basis functions on triangles, there are simple formulas for the element stiffness matrices. For example, for piecewise linear elements, consider a triangle with vertices (''x''<sub>1</sub>,''y''<sub>1</sub>), (''x''<sub>2</sub>,''y''<sub>2</sub>), (''x''<sub>3</sub>,''y''<sub>3</sub>), and define the 2×3 matrix\n\n:<math> D = \\left[\\begin{matrix}x_3 - x_2 & x_1 - x_3 & x_2 - x_1 \\\\ y_3 - y_2 & y_1 - y_3 & y_2 - y_1\\end{matrix}\\right].</math>\n\nThen the element stiffness matrix is\n\n:<math> A^{[k]} = D^\\mathsf{T} D/(4 \\operatorname{area}(T)).</math>\n\nWhen the differential equation is more complicated, say by having an inhomogeneous diffusion coefficient, the integral defining the element stiffness matrix can be evaluated by [[Gaussian quadrature]].\n\nThe [[condition number]] of the stiffness matrix depends strongly on the quality of the numerical grid. In particular, triangles with small angles in the finite element mesh induce large eigenvalues of the stiffness matrix, degrading the solution quality.\n\n==References==\n* {{citation |first1=A. |last1=Ern |first2=J.-L. |last2=Guermond |year=2004 |title=Theory and Practice of Finite Elements |publisher=Springer-Verlag |location=New York, NY |isbn=0387205748 }}\n* {{citation |first=M.S. |last=Gockenbach |year=2006 |title=Understanding and Implementing the Finite Element Method |publisher=SIAM |location=Philadelphia, PA |isbn=0898716144 }}\n* {{citation |first1=C. |last1=Grossmann |first2=H.-G. |last2=Roos |first3=M. |last3=Stynes |year=2007 |title=Numerical Treatment of Partial Differential Equations |publisher=Springer-Verlag |location=Berlin, Germany |isbn=978-3-540-71584-9 }}\n* {{citation |first=C. |last=Johnson |year=2009 |title=Numerical Solution of Partial Differential Equations by the Finite Element Method |publisher=Dover |isbn=978-0486469003 }}\n* {{citation |first1=O.C. |last1=Zienkiewicz |author1-link=Olgierd Zienkiewicz |first2=R.L. |last2=Taylor |first3=J.Z. |last3=Zhu |year=2005 |title=The Finite Element Method: Its Basis and Fundamentals |publisher=Elsevier Butterworth-Heinemann |edition=6th |location=Oxford, UK |isbn=978-0750663205 }}\n\n[[Category:Applied mathematics]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Successive parabolic interpolation",
      "url": "https://en.wikipedia.org/wiki/Successive_parabolic_interpolation",
      "text": "'''Successive parabolic interpolation''' is a technique for finding the [[extremum]] (minimum or maximum) of a continuous [[unimodal function]] by successively fitting [[parabola]]s ([[polynomials]] of degree two) to a function of one variable at three unique points or, in general, a function of ''n'' variables at ''1+n(n+3)/2'' points, and at each iteration replacing the \"oldest\" point with the extremum of the fitted parabola.\n\n== Advantages ==\nOnly function values are used, and when this method converges to an extremum, it does so with an [[order of convergence]] of approximately ''1.325''. The superlinear rate of convergence is superior to that of other methods with only linear convergence (such as [[line search]]). Moreover, not requiring the computation or approximation of function [[derivative]]s makes successive parabolic interpolation a popular alternative to other methods that do require them (such as [[gradient descent]] and [[Newton's method in optimization|Newton's method]]).\n\n== Disadvantages ==\nOn the other hand, convergence (even to a local extremum) is not guaranteed when using this method in isolation. For example, if the three points are [[line (mathematics)|collinear]], the resulting parabola is [[degeneracy (mathematics)|degenerate]] and thus does not provide a new candidate point. Furthermore, if function derivatives are available, Newton's method is applicable and exhibits quadratic convergence.\n\n== Improvements ==\nAlternating the parabolic iterations with a more robust method ([[golden section search]] is a popular choice) to choose candidates can greatly increase the probability of convergence without hampering the convergence rate.\n\n== See also==\n* [[Inverse quadratic interpolation]] is a related method that uses parabolas to find [[root of a function|root]]s rather than extrema.\n* [[Simpson's rule]] uses parabolas to approximate definite integrals.\n\n== References ==\n{{cite book |author=[[Michael Heath (computer scientist)|Michael Heath]] |title=Scientific Computing: An Introductory Survey |publisher=McGraw-Hill |location=New York |year=2002 |pages= |edition=2nd| isbn=0-07-239910-4 |oclc= |doi=}}\n\n{{Optimization algorithms}}\n\n[[Category:Numerical analysis]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Superconvergence",
      "url": "https://en.wikipedia.org/wiki/Superconvergence",
      "text": "In [[numerical analysis]], a '''superconvergent''' or '''supraconvergent''' method is one which converges faster than generally expected (''superconvergence'' or ''supraconvergence''). For example, in the [[Finite Element Method]] approximation to [[Poisson's equation]] in two dimensions, using piecewise linear elements, the average error in the [[gradient]] is [[First order approximation|first order]]. However under certain conditions it's possible to recover the gradient at certain locations within each element to [[Second order approximation|second order]].\n\n== References ==\n\n* {{Citation | last1=Barbeiro | first1=S. | last2=Ferreira | first2=J.&nbsp;A. | last3=Grigorieff | first3=R.&nbsp;D. | title=Supraconvergence of a finite difference scheme for solutions in ''H''<sup>''s''</sup>(0, ''L'') | journal=IMA J Numer Anal | volume=25 | issue=4 | pages=797–811 | year=2005 | doi=10.1093/imanum/dri018 | url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.108.7189&rep=rep1&type=pdf}}\n* {{Citation | last1=Ferreira | first1=J.&nbsp;A. | last2=Grigorieff | first2=R.&nbsp;D. | title=On the supraconvergence of elliptic finite difference methods | journal=Applied Numerical Mathematics | volume=28 | issue= | pages=275–292 | year=1998 | doi= | url=https://estudogeral.sib.uc.pt/bitstream/10316/4663/1/filee39b5d5b0989429899f94ddf76537ee1.pdf}}\n* {{Citation | last1=Levine | first1=N.&nbsp;D. | title=Superconvergent Recovery of the Gradient from Piecewise Linear Finite-element Approximations | journal=IMA J Numer Anal | volume=5 | issue= | pages=407–427 | year=1985 | doi= | url=http://www.nicklevine.org/sums/SuperconvergentRecoveryoftheGradientfromPiecewiseLinearFinite-elementApproximations.pdf}}\n\n[[Category:Finite element method]]\n[[Category:Numerical analysis]]\n\n\n{{mathapplied-stub}}"
    },
    {
      "title": "Surrogate model",
      "url": "https://en.wikipedia.org/wiki/Surrogate_model",
      "text": "A '''surrogate model''' is an engineering method used when an outcome of interest cannot be easily directly measured, so a model of the outcome is used instead. Most engineering design problems require experiments and/or simulations to evaluate design objective and constraint functions as a function of design variables.  For example, in order to find the optimal airfoil shape for an aircraft wing, an engineer simulates the airflow around the wing for different shape variables (length, curvature, material, ..).  For many real-world problems, however, a single simulation can take many minutes, hours, or even days to complete.  As a result, routine tasks such as design optimization, design space exploration, sensitivity analysis and ''what-if'' analysis become impossible since they require thousands or even millions of simulation evaluations.\n\nOne way of alleviating this burden is by constructing approximation models, known as '''surrogate models''', [[Response surface methodology|response surface models]], ''metamodels'' or ''emulators'', that mimic the behavior of the simulation model as closely as possible while being computationally cheap(er) to evaluate. Surrogate models are constructed using a data-driven, bottom-up approach. The exact, inner working of the simulation code is not assumed to be known (or even understood), solely the input-output behavior is important. A model is constructed based on modeling the response of the simulator to a limited number of intelligently chosen data points. This approach is also known as behavioral modeling or black-box modeling, though the terminology is not always consistent.  When only a single design variable is involved, the process is known as [[curve fitting]].\n\nThough using surrogate models in lieu of experiments and simulations in engineering design is more common, surrogate modeling may be used in many other areas of science where there are expensive experiments and/or function evaluations.\n\n==Goals==\n\nThe scientific challenge of surrogate modeling is the generation of a surrogate that is as accurate as possible, using as few simulation evaluations as possible. The process comprises three major steps which may be interleaved iteratively:\n\n* Sample selection (also known as sequential design, optimal experimental design (OED) or active learning)\n* Construction of the surrogate model and optimizing the model parameters (bias-variance trade-off)\n* Appraisal of the accuracy of the surrogate.\n\nThe accuracy of the surrogate depends on the number and location of samples (expensive experiments or simulations) in the design space. Various [[design of experiments]] (DOE) techniques cater to different sources of errors, in particular, errors due to noise in the data or errors due to an improper surrogate model.\n\n==Types of surrogate models==\n\nThe most popular surrogate models are polynomial [[response surface]]s, [[kriging]], [[Gradient-Enhanced Kriging (GEK)]], [[radial basis function]], [[support vector machine]]s, [[space mapping]],<ref name=\"space mapping\">[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1262727 J.W. Bandler, Q. Cheng, S.A. Dakroury, A.S. Mohamed, M.H. Bakr, K. Madsen and J. Søndergaard, \"Space mapping: the state of the art,\" IEEE Trans. Microwave Theory Tech., vol. 52, no. 1, pp. 337-361, Jan. 2004.]</ref> and [[artificial neural networks]]. The use of [[Bayesian networks]] has been also considered<ref>{{cite journal|last1= Cardenas |first1=IC|title= On the use of Bayesian networks as a meta-modeling approach to analyse uncertainties in slope stability analysis|journal =Georisk: Assessment and Management of Risk for Engineered Systems and Geohazards|date=2019|volume=13|issue=1|pages=53–65|doi=10.1080/17499518.2018.1498524}}</ref>. Recently, [[Random Forests]] method has also been  explored\n<ref>{{cite conference\n| first = S.K.\n| last = Dasari |author2=P. Andersson |author3=A. Cheddad\n| title = Random Forest Surrogate Models to Support Design Space Exploration in Aerospace Use-Case\n| booktitle = Artificial Intelligence Applications and Innovations (AIAI 2019)\n| pages = 532-544\n| publisher = Springer\n| date = 2019\n| location = \n| url = https://www.springerprofessional.de/en/random-forest-surrogate-models-to-support-design-space-explorati/16724106\n| accessdate = 2019-06-02\n| id = \n}}</ref>.For some problems, the nature of true function is not known a priori so it is not clear which surrogate model will be most accurate. In addition, there is no consensus on how to obtain the most reliable estimates of the accuracy of a given surrogate.\nMany other problems have known physics properties. In these cases, physics-based surrogates such as [[space-mapping]] based models are the most efficient.<ref name=\"space mapping\" />\n\nA recent survey of surrogate-assisted evolutionary optimization techniques can be found in.<ref>Jin Y (2011). [http://www.soft-computing.de/SECPublished.pdf Surrogate-assisted evolutionary computation: Recent advances and future challenges]. Swarm and Evolutionary Computation, 1(2):61–70.</ref>\n\nSpanning two decades of development and engineering applications, Rayas-Sanchez reviews aggressive [[space mapping]] exploiting surrogate models.<ref>J.E. Rayas-Sanchez,[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7423860&action=search&sortType=&rowsPerPage=&searchField=Search_All&matchBoolean=true&queryText=(%22Document%20Title%22:simplicity%20in%20asm) \"Power in simplicity with ASM: tracing the aggressive space mapping algorithm over two decades of development and engineering applications\"], IEEE Microwave Magazine, vol. 17, no. 4, pp. 64-76, April 2016.</ref> Recently, Razavi et al. have published a state-of-the-art review of surrogate models used in water resources management field. <ref> Razavi, S., B. A.Tolson, and D. H.Burn (2012), [https://onlinelibrary.wiley.com/doi/pdf/10.1029/2011WR011527 Review of surrogate modeling in water resources], Water Resour. Res., 48, W07401, doi: 10.1029/2011WR011527. </ref>\n\n==Invariance properties==\nRecently proposed comparison-based surrogate models (e.g. ranking [[support vector machine]]) for [[evolutionary algorithms]], such as [[CMA-ES]], allow to preserve some invariance properties of surrogate-assisted optimizers:\n<ref>{{cite conference\n| first = I.\n| last = Loshchilov |author2=M. Schoenauer |author3=M. Sebag\n| title = Comparison-Based Optimizers Need Comparison-Based Surrogates\n| booktitle = Parallel Problem Solving from Nature (PPSN XI)\n| pages = 364–1373\n| publisher = Springer\n| date = 2010\n| location = \n| url = https://hal.inria.fr/file/index/docid/493921/filename/ACM-ES.pdf\n| accessdate = \n| id = \n}}</ref>\n*1. Invariance with respect to monotonous transformations of the function (scaling)\n*2. Invariance with respect to [[orthogonal transform]]ations of the search space (rotation).\n\n==Applications==\n\nAn important distinction can be made between two different applications of surrogate models: design optimization and design space approximation (also known as emulation).\n\nIn surrogate model based optimization, an initial surrogate is constructed using some of the available budgets of expensive experiments and/or simulations. The remaining experiments/simulations are run for designs which the surrogate model predicts may have promising performance. The process usually takes the form of the following search/update procedure.\n\n*1. Initial sample selection (the experiments and/or simulations to be run)\n*2. Construct surrogate model\n*3. Search surrogate model (the model can be searched extensively, e.g. using a [[genetic algorithm]], as it is cheap to evaluate)\n*4. Run and update experiment/simulation at a new location(s) found by search and add to sample\n*5. Iterate steps 2 to 4 until out of time or design 'good enough'\n\nDepending on the type of surrogate used and the complexity of the problem, the process may converge on a local or global optimum, or perhaps none at all.<ref>Jones, D.R (2001), \"[http://www.ressources-actuarielles.net/EXT/ISFA/1226.nsf/9c8e3fd4d8874d60c1257052003eced6/e7dc33e4da12c5a9c12576d8002e442b/$FILE/Jones01.pdf A taxonomy of global optimization methods based on response surfaces],\" Journal of Global Optimization, 21:345–383.</ref>\n\nIn design space approximation, one is not interested in finding the optimal parameter vector but rather in the global behavior of the system. Here the surrogate is tuned to mimic the underlying model as closely as needed over the complete design space. Such surrogates are a useful, cheap way to gain insight into the global behavior of the system. Optimization can still occur as a post-processing step, although with no update procedure (see above) the optimum found cannot be validated.\n\n== Surrogate modeling software ==\n* Surrogate Modeling Toolbox (SMT: https://github.com/SMTorg/smt): is a Python package that contains a collection of surrogate modeling methods, sampling techniques, and benchmarking functions. This package provides a library of surrogate models that is simple to use and facilitates the implementation of additional methods. SMT is different from existing surrogate modeling libraries because of its emphasis on derivatives, including training derivatives used for gradient-enhanced modeling, prediction derivatives, and derivatives with respect to the training data. It also includes new surrogate models that are not available elsewhere: kriging by partial-least squares reduction and energy-minimizing spline interpolation.<ref name = bouhlel2019>{{cite journal | last1 = Bouhlel | first1 = M.A. | last2 = Hwang | first2 = J.H. | last3 = Bartoli | first3 = Nathalie | last4 = Lafage |first4 = R. | last5 = Morlier | first5 = J. | last6 = Martins | first6 = J.R.R.A. | year = 2019 | title = A Python surrogate modeling framework with derivatives | journal = Advances in Engineering Software | doi =10.1016/j.advengsoft.2019.03.005 | url = http://mdolab.engin.umich.edu/content/python-surrogate-modeling-framework-derivatives }}</ref>\n\n==See also==\n*[[Linear approximation]]\n*[[Response surface methodology]]\n*[[Kriging]]\n*[[Radial basis function|Radial Basis Functions]]\n*[[Gradient-Enhanced Kriging (GEK)]]\n*[[OptiY]]\n*[[Space mapping]]\n*[[Surrogate endpoint]]\n*[[Surrogate data]]\n*[[Fitness approximation]]\n*[[Computer experiment]]\n*[[Conceptual model|Model]]\n\n==References==\n<references/>\n\n==Reading==\n* Queipo, N.V., Haftka, R.T., [[Wei Shyy|Shyy, W.]], Goel, T., Vaidyanathan, R., Tucker, P.K. (2005), “[https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20050186653.pdf Surrogate-based analysis and optimization],” Progress in Aerospace Sciences, 41, 1–28.\n* D. Gorissen, I. Couckuyt, P. Demeester, T. Dhaene, K. Crombecq, (2010), “[http://jmlr.csail.mit.edu/papers/volume11/gorissen10a/gorissen10a.pdf A Surrogate Modeling and Adaptive Sampling Toolbox for Computer Based Design],\" Journal of Machine Learning Research, Vol. 11, pp.&nbsp;2051−2055, July 2010.\n* T-Q. Pham, A. Kamusella, H. Neubert, “[http://www.ep.liu.se/ecp/063/074/ecp11063074.pdf Auto-Extraction of Modelica Code from Finite Element Analysis or Measurement Data],\" 8th International Modelica Conference, 20–22 March 2011 in Dresden.\n* Forrester, Alexander, Andras Sobester, and Andy Keane, ''[https://books.google.com/books?hl=en&lr=&id=ulMHmeMnRCcC&oi=fnd&pg=PR5&dq=%22Engineering+design+via+surrogate+modelling:+a+practical+guide%22&ots=geJJ6nEa6C&sig=wrm4uWCZ6DNVnW4t1gJlNFO8SKE#v=onepage&q=%22Engineering%20design%20via%20surrogate%20modelling%3A%20a%20practical%20guide%22&f=false Engineering design via surrogate modelling: a practical guide]'', John Wiley & Sons, 2008.\n* Bouhlel, M. A. and Bartoli, N. and Otsmane, A. and Morlier, J. (2016) \"[https://hal.archives-ouvertes.fr/hal-01232938/file/KPLS_paper2015.pdf Improving kriging surrogates of high-dimensional design models by Partial Least Squares dimension reduction]\", Structural and Multidisciplinary Optimization 53 (5), 935-952\n* Bouhlel, M. A. and Bartoli, N. and Otsmane, A. and Morlier, J. (2016) \"[http://downloads.hindawi.com/journals/mpe/2016/6723410.pdf An improved approach for estimating the hyperparameters of the kriging model for high-dimensional problems through the partial least squares method]\", Mathematical Problems in Engineering\n\n==External links==\n* [http://www.wiley.com//legacy/wileychi/forrester/terms.html Matlab code for surrogate modelling]\n* [http://sumowiki.intec.ugent.be Matlab '''SU'''rrogate '''MO'''deling Toolbox – Matlab SUMO Toolbox]\n* [https://github.com/SMTorg/SMT Surrogate Modeling Toolbox -- Python]\n\n[[Category:Design of experiments]]\n[[Category:Numerical analysis]]\n[[Category:Scientific modeling]]\n[[Category:Mathematical modeling]]"
    },
    {
      "title": "Symbolic-numeric computation",
      "url": "https://en.wikipedia.org/wiki/Symbolic-numeric_computation",
      "text": "In [[mathematics]] and [[computer science]], '''symbolic-numeric computation''' is the use of [[software]] that combines [[Symbolic computation|symbolic]] and [[numerical analysis|numeric]] methods to solve problems.\n\n== Background ==\n\n\n\n== Computational Algebraic Geometry ==\n\n\n==References==\n* {{cite book | url = https://books.google.com/books?id=BlRlhmwSPwMC | title = Symbolic-numeric Computation | first1 = Dongming | last1 = Wang | first2 = Lihong | last2 = Zhi | publisher = Springer | year = 2007 | isbn = 3-7643-7983-9 }}\n* {{cite book | citeseerx = 10.1.1.135.1680 | chapter = SYNAPS: A Library for Dedicated Applications in Symbolic Numeric Computing | first1 = Bernard | last1 = Mourrain | first2 = Jean-Pascal | last2 = Pavone | first3 = Philippe | last3 = Trebuchet | first4 = Elias P. | last4 = Tsigaridas | first5 = Julien | last5 = Wintz | title = Software for Algebraic Geometry | series = The IMA Volumes in Mathematics and its Applications | year = 2008 | volume = 148 | pages = 81–109 | doi = 10.1007/978-0-387-78133-4_6 }}\n* {{cite book | chapterurl = http://www4.ncsu.edu/~kaltofen/bibliography/01/symnum.pdf | chapter = Hybrid methods | url = https://books.google.com/books?id=-U_j6VoUvPAC | title = Computer algebra handbook: foundations, applications, systems, Volume 1 | editor1-first = Johannes | editor1-last = Grabmeier | editor2-first = Erich | editor2-last = Kaltofen | editor3-first = Volker | editor3-last = Weispfenning | publisher = Springer | year = 2003 | isbn = 978-3-540-65466-7 }}\n* {{cite book | url = https://books.google.com/books?id=dWhYcTGakBcC | title = Approximate Commutative Algebra | first1 = Lorenzo | last1 = Robbiano | first2 = John | last2 = Abbott | publisher = Springer | year = 2009 | isbn = 978-3-211-99313-2 }}\n* {{cite book | url = https://www.springer.com/mathematics/computational+science+%26+engineering/book/978-3-7091-0793-5 | title = Numerical and Symbolic Scientific Computing | editor1-last = Langer | editor1-first = Ulrich | editor2-last = Paule | editor2-first = Peter|editor2-link=Peter Paule | publisher = Springer | year = 2011 | isbn = 978-3-7091-0793-5 }}\n\n==External links==\n* {{cite web | url = http://www.cargo.wlu.ca/SNC2011/ | title = The Fourth International Workshop on Symbolic-Numeric Computation (SNC2011) | date = June 7–9, 2011 | location = San Jose, California }}\n'''Professional organizations'''\n* [http://www.sigsam.org ACM SIGSAM: Special Interest Group in Symbolic and Algebraic Manipulation]\n\n[[Category:Computer algebra]]\n[[Category:Numerical analysis]]\n[[Category:Computational science]]\n\n\n{{algorithm-stub}}"
    },
    {
      "title": "Timeline of numerical analysis after 1945",
      "url": "https://en.wikipedia.org/wiki/Timeline_of_numerical_analysis_after_1945",
      "text": "{{Use dmy dates|date=October 2013}}\nThe following is a '''timeline of numerical analysis after 1945''', and deals with developments after the invention of the modern [[electronic computer]], which  began during [[Second World War]]. For a fuller history of the subject before this period, see [[timeline of mathematics|timeline]] and [[history of mathematics]].\n\n==1940s==\n* Monte Carlo simulation (voted one of the top 10 [[algorithm]]s of the 20th century) invented at Los Alamos by von Neumann, Ulam and Metropolis.<ref>{{cite journal|last=Metropolis|first=N.|title=The Beginning of the Monte Carlo method|journal=[[Los Alamos Science]]|year=1987|volume=No. 15, Page 125|url=http://library.lanl.gov/cgi-bin/getfile?15-12.pdf}}. Accessed 5 may 2012.</ref><ref>S. Ulam, R. D. Richtmyer, and J. von Neumann(1947). [http://library.lanl.gov/cgi-bin/getfile?00329286.pdf  Statistical methods in neutron diffusion]. Los Alamos Scientific Laboratory report LAMS–551.</ref><ref>{{cite journal | last1 = Metropolis | first1 = N. | last2 = Ulam | first2 = S. | year = 1949 | title = The Monte Carlo method | url = | journal = Journal of the American Statistical Association | volume = 44 | issue = 247| pages = 335–341 | doi=10.1080/01621459.1949.10483310}}</ref>\n* [[Crank–Nicolson method]]  was developed by Crank and Nicolson.<ref>{{Cite journal \n | title = A practical method for numerical evaluation of solutions of partial differential equations of the heat conduction type\n | journal = Proc. Camb. Phil. Soc.\n | volume = 43 \n | issue = 1\n | year = 1947\n | pages = 50&ndash;67\n | doi = 10.1007/BF02127704 \n | last1 = Crank \n | first1 = J. (John) \n | last2 = Nicolson \n | first2 = P. (Phyllis) \n }}</ref>\n* Dantzig introduces the [[simplex method]] (voted one of the top 10 algorithms of the 20th century) in 1947.<ref>{{cite web|title=SIAM News, November 1994.|url=http://www.stanford.edu/group/SOL/dantzig.html|accessdate=6 June 2012}} Hosted at [http://www.stanford.edu/group/SOL/ Systems Optimization Laboratory], [[Stanford University]], [http://soe-oldwebserver.stanford.edu/visit/huang_center/index.html Huang Engineering Center] {{Webarchive|url=https://web.archive.org/web/20121112155545/http://soe-oldwebserver.stanford.edu/visit/huang_center/index.html |date=12 November 2012 }}.</ref>\n* Turing formulated the LU decomposition method.<ref>A. M. Turing, Rounding-off errors in matrix processes. Quart. J Mech. Appl. Math. 1 (1948), 287–308 (according to Poole, David (2006), Linear Algebra: A Modern Introduction (2nd ed.), Canada: Thomson Brooks/Cole, {{ISBN|0-534-99845-3}}.) .</ref>\n\n==1950s==\n* [[Successive over-relaxation]] was devised simultaneously by D.M. Young, Jr.<ref>{{citation\n |last=Young |first=David M. |authorlink=David M. Young\n |title=Iterative methods for solving partial difference equations of elliptical type\n |url=http://www.ma.utexas.edu/CNA/DMY/david_young_thesis.pdf\n |date=May 1, 1950\n |series=PhD thesis, Harvard University\n |accessdate=15 June 2009\n}}</ref> and by H. Frankel in 1950. \n* [[Magnus Hestenes|Hestenes]], [[Eduard Stiefel|Stiefel]], and [[Cornelius Lanczos|Lanczos]], all from the Institute for Numerical Analysis at the [[National Bureau of Standards]], initiate the development of [[Iterative method|Krylov subspace iteration method]]s.<ref>Magnus R. Hestenes and Eduard Stiefel, Methods of Conjugate Gradients for Solving Linear Systems, J. Res. Natl. Bur. Stand. 49, 409–436 (1952).</ref><ref>Eduard Stiefel,U¨ ber einige Methoden der Relaxationsrechnung (in German), Z. Angew. Math. Phys. 3, 1–33 (1952).</ref><ref>Cornelius Lanczos, Solution of Systems of Linear Equations by Minimized Iterations, J. Res. Natl. Bur. Stand. 49, 33–53 (1952).</ref><ref>Cornelius Lanczos, An Iteration Method for the Solution of the Eigenvalue Problem of Linear Differential and Integral Operators, J. Res. Natl. Bur. Stand. 45, 255–282 (1950).</ref> Voted one of the top 10 algorithms of the 20th century.\n* ''[[Equations of State Calculations by Fast Computing Machines]]'' introduces the [[Metropolis–Hastings algorithm]].<ref>{{cite journal | last1 = Metropolis | first1 = N. | last2 = Rosenbluth | first2 = A.W. | last3 = Rosenbluth | first3 = M.N. | last4 = Teller | first4 = A.H. | last5 = Teller | first5 = E. | year = 1953 | title =  Equation of State Calculations by Fast Computing Machines| url = | journal = Journal of Chemical Physics | volume = 21 | issue = 6| pages = 1087–1092 | doi = 10.1063/1.1699114 | bibcode=1953JChPh..21.1087M}}</ref>\n* In numerical differential equations, Lax and Friedrichs invent the Lax-Friedrichs method.<ref>{{cite journal | last1 = Lax | first1 = PD | year = 1954 | title = Weak solutions of nonlinear hyperbolic equations and their numerical approximation | url = | journal = Comm. Pure Appl. Math. | volume = 7 | issue = | pages = 159–193 | doi=10.1002/cpa.3160070112}}</ref><ref>{{cite journal | last1 = Friedrichs | first1 = KO | year = 1954 | title = Symmetric hyperbolic linear differential equations | url = | journal = Comm. Pure Appl. Math. | volume = 7 | issue = 2| pages = 345–392 | doi=10.1002/cpa.3160070206}}</ref>\n* Householder invents his [[Householder matrix|eponymous matrices]] and [[Householder transformation|transformation method]] (voted one of the top 10 algorithms of the 20th century).<ref>{{cite journal|first=A. S. |last=Householder |title=Unitary Triangularization of a Nonsymmetric Matrix|journal=[[Journal of the ACM]]\n |volume=5 |issue=4 |year=1958 |pages=339&ndash;342|doi=10.1145/320941.320947 |mr=0111128}}</ref>\n*[[Romberg integration]]<ref>1955</ref>\n* [[John G.F. Francis]]<ref>J.G.F. Francis, \"The QR Transformation, I\", ''The Computer Journal'', 4(3), pages 265–271 (1961, received October 1959) online at oxfordjournals.org;J.G.F. Francis, \"The QR Transformation, II\" ''The Computer Journal'', 4(4), pages 332–345 (1962) online at oxfordjournals.org.</ref> and [[Vera Kublanovskaya]]<ref>Vera N. Kublanovskaya (1961), \"On some algorithms for the solution of the complete eigenvalue problem,\" ''USSR Computational Mathematics and Mathematical Physics'', 1(3), pages 637–657 (1963, received Feb 1961). Also published in: Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki [Journal of Computational Mathematics and Mathematical Physics], 1(4), pages 555–570 (1961).</ref> invent [[QR factorization]] (voted one of the top 10 algorithms of the 20th century).\n\n==1960s==\n* [[finite element method#History|First recorded use]] of the term \"finite element method\" by [[Ray W. Clough|Ray Clough]],<ref>RW Clough, “The Finite Element Method in Plane\nStress Analysis,” Proceedings of 2nd ASCE Conference on Electronic Computation, Pittsburgh, PA, Sept. 8, 9, 1960.</ref> to describe the methods of Courant, Hrenikoff, Galerkin and Zienkiewicz, among others. See also [[Structural analysis#Timeline|here]].\n*Exponential integration by Certaine and Pope.\n* In computational fluid dynamics and numerical differential equations, Lax and Wendroff invent the [[Lax-Wendroff method]].<ref>{{ cite journal | author = P.D Lax |author2=B. Wendroff  | year = 1960 | title = Systems of conservation laws | journal = Commun. Pure Appl. Math. | volume = 13 | pages = 217–237 | doi = 10.1002/cpa.3160130205 | issue = 2 | url = http://www.dtic.mil/get-tr-doc/pdf?AD=ADA385056 }}</ref>\n* Fast Fourier Transform (voted one of the top 10 [[algorithms]] of the 20th century) invented by Cooley and Tukey.<ref>{{cite journal | last1 = Cooley | first1 = James W. | last2 = Tukey | first2 = John W. | year = 1965 | title = An algorithm for the machine calculation of complex Fourier series | url = http://attach3.bdwm.net/attach/0Announce/groups/GROUP_3/MathTools/D6714701A/D69595345/M.1089260001.A/CooleyJ_AlgMCC.pdf | journal = Math. Comput. | volume = 19 | issue = 90| pages = 297–301 | doi=10.1090/s0025-5718-1965-0178586-1}}</ref>\n* First edition of ''[[Abramowitz and Stegun|Handbook of Mathematical Functions]]'' by Abramowitz and Stegun, both of the U.S.[[National Bureau of Standards]].<ref>M Abramowitz and I Stegun, Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. Publisher: Dover Publications. Publication date: 1964; {{ISBN|0-486-61272-4}};[[Online Computer Library Center|OCLC]] Number:[http://worldcat.org/oclc/18003605 18003605] .</ref>\n* Broyden does new quasi-Newton method for finding roots in 1965.\n* The [[MacCormack method]], for the numerical solution of [[hyperbolic partial differential equation]]s in computational fluid dynamics, is introduced by  MacCormack in 1969.<ref>MacCormack, R. W., The Effect of viscosity in hypervelocity impact cratering, AIAA Paper, 69-354 (1969).</ref>\n* Verlet (re)discovers a numerical integration algorithm, (first used in 1791 by Delambre, by Cowell and Crommelin in 1909, and by Carl Fredrik Störmer in 1907, hence the alternative names Störmer's method or the Verlet-Störmer method) for dynamics.\n\n==1970s==\nCreation of [[LINPACK]] and [[LINPACK benchmarks|associated benchmark]] by Dongarra et al.<ref>{{cite journal|author1=J. Bunch|author2=G. W. Stewart.|author3=Cleve Moler|author4=Jack J. Dongarra|title=LINPACK User's Guide |publisher=SIAM |location=Philadelphia, PA |year=1979}}</ref><ref>[http://www.netlib.org/utk/people/JackDongarra/PAPERS/hpl.pdf The LINPACK Benchmark:Past,Present,and Future.] Jack J. Dongarra,Piotr Luszczeky, and Antoine Petitetz. December 2001.</ref>\n\n==1980s==\n* Progress in digital [[wavelet theory]] throughout  the decade, led by Daubechies et. al.\n*Creation of [[MINPACK]] \n* [[Fast multipole method]] (voted one of the top 10 [[algorithms]] of the 20th century) invented by Rokhlin and Greengard.<ref>L. Greengard, The Rapid Evaluation of Potential Fields in Particle Systems, MIT, Cambridge, (1987).</ref><ref>Rokhlin, Vladimir (1985). \"Rapid Solution of Integral Equations of Classic Potential Theory.\" J. Computational Physics Vol. 60, pp. 187–207.</ref><ref>{{cite journal | last1 = Greengard | first1 = L. | last2 = Rokhlin | first2 = V. | year = 1987 | title = A fast algorithm for particle simulations | journal = J. Comput. Phys. | volume = 73 | issue = 2| pages = 325–348 | doi=10.1016/0021-9991(87)90140-9}}</ref>\n* First edition of ''[[Numerical Recipes]]''  by  Press, Teukolsky, et al.<ref>Press, William H.; Teukolsky, Saul A.; Vetterling, William T.; Flannery, Brian P. (1986).  Numerical Recipes: The Art of Scientific Computing. New York: Cambridge University Press. {{ISBN|0-521-30811-9}}.</ref>\n* In numerical linear algebra, the [[GMRES]] algorithm invented in 1986.<ref>{{cite journal | last1 = Saad | first1 = Y. | last2 = Schultz | first2 = M.H. | year = 1986 | title = GMRES: A generalized minimal residual algorithm for solving nonsymmetric linear systems | url = | journal = SIAM J. Sci. Stat. Comput. | volume = 7 | issue = 3| pages = 856–869 | doi = 10.1137/0907058 | citeseerx = 10.1.1.476.951 }}</ref>\n\n==See also==\n* [[Scientific computing]]\n* [[History of numerical solution of differential equations using computers]]\n* [[Numerical analysis]]\n* [[Timeline of computational mathematics]]\n\n{{subject bar|portal2=Mathematics}}\n\n==References==\n{{Reflist}}\n\n==Further reading==\n* {{cite web |author-last=Cipra |author-first=Barry Arthur |author-link=Barry Arthur Cipra |date=2000 |title=Top 10 Algorithms of the 20th Century |publisher=[[Society for Industrial and Applied Mathematics]] (SIAM) |work=[[SIAM News]] |url=http://www.siam.org/news/news.php?id=637 |access-date=2012-12-01}}\n\n==External links==\n* [http://history.siam.org/ The History of Numerical Analysis and Scientific Computing] @ SIAM (Society for Industrial and Applied Mathematics)\n* {{cite journal | doi = 10.1038/440399a | volume=440 | issue=7083 | title=2020 computing: Milestones in scientific computing | year=2006 | journal=Nature | pages=399–405 | last1 = Ruttimann | first1 = Jacqueline | pmid=16554772}}\n* [https://web.archive.org/web/20140822045448/http://home.gwu.edu/~stroud/mc-classics.html The Monte Carlo Method: Classic Papers]\n* [http://scienze-como.uninsubria.it/bressanini/montecarlo-history/ Monte Carlo Landmark Papers]\n*[http://mathoverflow.net/questions/102413/must-read-papers-in-numerical-analysis “Must read” papers in numerical analysis.] Discussion at [[Math Overflow]] based upon a selected reading list on [[Lloyd N. Trefethen]]'s [http://people.maths.ox.ac.uk/trefethen/classic_papers.txt personal site].\n\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Trajectory (fluid mechanics)",
      "url": "https://en.wikipedia.org/wiki/Trajectory_%28fluid_mechanics%29",
      "text": "In [[fluid mechanics]], [[meteorology]] and [[oceanography]], a '''trajectory''' traces the motion of a single point, often called a [[fluid parcel|parcel]], in the flow.\n\nTrajectories are useful for tracking atmospheric contaminants, such as smoke plumes, and as constituents to [[Lagrangian coordinates|Lagrangian]] simulations, such as [[contour advection]] or [[semi-Lagrangian scheme]]s.\n\nSuppose we have a time-varying flow field, <math>\\vec v(\\vec x,~t)</math>. The motion of a fluid parcel, or trajectory, is given by the following system of [[ordinary differential equations]]:\n:<math>\n\\frac{d \\vec x}{dt} = \\vec v(\\vec x, ~t)\n</math>\n\nWhile the equation looks simple, there are at least three concerns when attempting to solve it [[numerically]].  The first is the [[numerical ordinary differential equations|integration scheme]]. This is typically a [[Runge-Kutta]],<ref name=\"Press_etal1992\">{{cite book\n  |title=Numerical Recipes in C: the Art of Scientific Computing\n  |author1=William H. Press\n  |author2=Brian P. Flannery\n  |author3=Saul A. Teukolsky\n  |author4=William T. Vetterling\n  |year=1992\n  |publisher=Cambridge University Press\n  |edition=2nd\n}}</ref> although others can be useful as well, such as a [[leapfrog integration|leapfrog]].  The second is the method of determining the velocity vector, <math>\\vec v</math> at a given position, <math>\\vec x</math>, and time, ''t''. Normally, it is not known at all positions and times, therefore some method of [[interpolation]] is required.  If the velocities are gridded in space and time, then [[bilinear interpolation|bilinear]], [[trilinear interpolation|trilinear]] or higher-dimensional linear interpolation is appropriate. [[Bicubic interpolation|Bicubic]], [[Tricubic interpolation|tricubic]], etc., interpolation is used as well, but is probably not worth the extra [[overhead (computing)|computational overhead]].\n\nVelocity fields can be determined by measurement, e.g. from [[weather balloons]], from numerical models or especially from a combination of the two, e.g. [[data assimilation|assimilation models]].\n\nThe final concern is metric corrections.  These are necessary for geophysical fluid flows on a spherical Earth.  The differential equations for tracing a two-dimensional, atmospheric trajectory in longitude-latitude coordinates are as follows:\n:<math>\n\\frac{d \\theta}{dt} = \\frac{u}{r \\cos \\phi}\n</math>\n\n:<math>\n\\frac{d \\phi}{dt} = \\frac{v}{r}\n</math>\n\nwhere, <math>\\theta</math> and <math>\\phi</math> are, respectively, the longitude and latitude in [[radian]]s, ''r'' is the [[radius of the Earth]], ''u'' is the zonal wind and ''v'' is the meridional wind.\n\nOne problem with this formulation is the polar singularity: notice how the denominator in the first equation goes to zero when the latitude is 90 degrees—plus or minus.  One means of overcoming this is to use a locally [[Cartesian coordinate]] system close to the poles.  Another is to perform the integration on a pair of [[Azimuthal equidistant projection]]s—one for the N. Hemisphere and one for the S. Hemisphere.<ref name=\"Mills2012\" >{{cite arxiv|eprint=1202.1999\n|last1=Mills\n|first1=Peter\n|title=Principal component proxy tracer analysis\n|class=physics.ao-ph\n|year=2012\n}}</ref>\n\nTrajectories can be validated by [[balloons]] in the [[atmosphere]] and [[buoys]] in the [[ocean]].\n\n== External links ==\n* [http://ctraj.sourceforge.net ctraj]: A trajectory integrator written in C++.\n\n== References ==\n{{Reflist}}\n\n[[Category:Fluid dynamics]]\n[[Category:Continuum mechanics]]\n[[Category:Meteorological concepts]]\n[[Category:Numerical analysis]]\n[[Category:Numerical climate and weather models]]"
    },
    {
      "title": "Transfer matrix",
      "url": "https://en.wikipedia.org/wiki/Transfer_matrix",
      "text": "{{about|the transfer matrix in wavelet theory|the transfer matrix in control systems|Transfer function matrix|the transfer matrix method in statistical physics|Transfer-matrix method|the transfer matrix method in optics|Transfer-matrix method (optics)}}\n\nIn [[applied mathematics]], the '''transfer matrix''' is a formulation in terms of a [[block-Toeplitz matrix]] of the two-scale equation, which characterizes [[refinable function]]s. Refinable functions play an important role in [[wavelet]] theory and [[finite element]] theory.\n\nFor the mask <math>h</math>, which is a vector with component indexes from <math>a</math> to <math>b</math>,\nthe transfer matrix of <math>h</math>, we call it <math>T_h</math> here, is defined as\n:<math>\n(T_h)_{j,k} = h_{2\\cdot j-k}.\n</math>\nMore verbosely\n:<math>\nT_h =\n\\begin{pmatrix}\nh_{a  } &         &         &         &         &   \\\\\nh_{a+2} & h_{a+1} & h_{a  } &         &         &   \\\\\nh_{a+4} & h_{a+3} & h_{a+2} & h_{a+1} & h_{a  } &   \\\\\n\\ddots  & \\ddots  & \\ddots  & \\ddots  & \\ddots  & \\ddots \\\\\n  & h_{b  } & h_{b-1} & h_{b-2} & h_{b-3} & h_{b-4} \\\\\n  &         &         & h_{b  } & h_{b-1} & h_{b-2} \\\\\n  &         &         &         &         & h_{b  }\n\\end{pmatrix}.\n</math>\nThe effect of <math>T_h</math> can be expressed in terms of the [[downsampling]] operator \"<math>\\downarrow</math>\":\n:<math>T_h\\cdot x = (h*x)\\downarrow 2.</math>\n\n==Properties==\n\n* <math>T_h\\cdot x = T_x\\cdot h</math>.\n* If you drop the first and the last column and move the odd-indexed columns to the left and the even-indexed columns to the right, then you obtain a transposed [[Sylvester matrix]].\n* The determinant of a transfer matrix is essentially a resultant.\n:More precisely:\n:Let <math>h_{\\mathrm{e}}</math> be the even-indexed coefficients of <math>h</math> (<math>(h_{\\mathrm{e}})_k = h_{2k}</math>) and let <math>h_{\\mathrm{o}}</math> be the odd-indexed coefficients of <math>h</math> (<math>(h_{\\mathrm{o}})_k = h_{2k+1}</math>).\n:Then <math>\\det T_h = (-1)^{\\lfloor\\frac{b-a+1}{4}\\rfloor}\\cdot h_a\\cdot h_b\\cdot\\mathrm{res}(h_{\\mathrm{e}},h_{\\mathrm{o}})</math>, where <math>\\mathrm{res}</math> is the [[resultant]].\n:This connection allows for fast computation using the [[Euclidean algorithm]].\n* For the [[Trace (linear algebra)|trace]] of the transfer matrix of [[convolution|convolved]] masks holds\n:<math>\\mathrm{tr}~T_{g*h} = \\mathrm{tr}~T_g \\cdot \\mathrm{tr}~T_h</math>\n* For the [[determinant]] of the transfer matrix of convolved mask holds\n:<math>\\det T_{g*h} = \\det T_g \\cdot \\det T_h \\cdot \\mathrm{res}(g_-,h)</math>\n:where <math>g_-</math> denotes the mask with alternating signs, i.e. <math>(g_-)_k = (-1)^k \\cdot g_k</math>.\n* If <math>T_{h}\\cdot x = 0</math>, then <math>T_{g*h}\\cdot (g_-*x) = 0</math>.\n: This is a concretion of the determinant property above. From the determinant property one knows that <math>T_{g*h}</math> is [[Singular matrix|singular]] whenever <math>T_{h}</math> is singular. This property also tells, how vectors from the [[null space]] of <math>T_{h}</math> can be converted to null space vectors of <math>T_{g*h}</math>.\n* If <math>x</math> is an eigenvector of <math>T_{h}</math> with respect to the eigenvalue <math>\\lambda</math>, i.e.\n: <math>T_{h}\\cdot x = \\lambda\\cdot x</math>,\n:then <math>x*(1,-1)</math> is an eigenvector of <math>T_{h*(1,1)}</math> with respect to the same eigenvalue, i.e.\n: <math>T_{h*(1,1)}\\cdot (x*(1,-1)) = \\lambda\\cdot (x*(1,-1))</math>.\n* Let <math>\\lambda_a,\\dots,\\lambda_b</math> be the eigenvalues of <math>T_h</math>, which implies <math>\\lambda_a+\\dots+\\lambda_b = \\mathrm{tr}~T_h</math> and more generally <math>\\lambda_a^n+\\dots+\\lambda_b^n = \\mathrm{tr}(T_h^n)</math>. This sum is useful for estimating the [[spectral radius]] of <math>T_h</math>. There is an alternative possibility for computing the sum of eigenvalue powers, which is faster for small <math>n</math>.\n:Let <math>C_k h</math> be the periodization of <math>h</math> with respect to period <math>2^k-1</math>. That is <math>C_k h</math> is a circular filter, which means that the component indexes are [[Modular arithmetic#Ring of congruence classes|residue class]]es with respect to the modulus <math>2^k-1</math>. Then with the [[upsampling]] operator <math>\\uparrow</math> it holds\n:<math>\\mathrm{tr}(T_h^n) = \\left(C_k h * (C_k h\\uparrow 2) * (C_k h\\uparrow 2^2) * \\cdots * (C_k h\\uparrow 2^{n-1})\\right)_{[0]_{2^n-1}}</math>\n:Actually not <math>n-2</math> convolutions are necessary, but only <math>2\\cdot \\log_2 n</math> ones, when applying the strategy of efficient computation of powers. Even more the approach can be further sped up using the [[Fast Fourier transform]].\n* From the previous statement we can derive an estimate of the [[spectral radius]] of <math>\\varrho(T_h)</math>. It holds\n:<math>\\varrho(T_h) \\ge \\frac{a}{\\sqrt{\\# h}} \\ge \\frac{1}{\\sqrt{3\\cdot \\# h}}</math>\n:where <math>\\# h</math> is the size of the filter and if all eigenvalues are real, it is also true that\n:<math>\\varrho(T_h) \\le a</math>,\n:where <math>a = \\Vert C_2 h \\Vert_2</math>.\n\n==See also==\n* [[Hurwitz determinant]]\n\n==References==\n* {{cite article\n|first=Gilbert|last=Strang\n|author-link=Gilbert Strang\n|title=Eigenvalues of <math>(\\downarrow 2){H}</math> and convergence of the cascade algorithm\n|journal=IEEE Transactions on Signal Processing\n|volume=44\n|pages=233–238\n|year=1996\n}}\n* {{cite thesis\n|first=Henning\n|last=Thielemann\n|url=http://nbn-resolving.de/urn:nbn:de:gbv:46-diss000103131\n|title=Optimally matched wavelets\n|type=PhD thesis\n|year=2006\n}} (contains proofs of the above properties)\n\n[[Category:Wavelets]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Truncated power function",
      "url": "https://en.wikipedia.org/wiki/Truncated_power_function",
      "text": "In mathematics, the '''truncated power function'''<ref>{{cite book\n|title=Interpolation and Approximation with Splines and Fractals\n|first=Peter|last=Massopust\n|publisher= Oxford University Press, USA\n|year=2010\n|isbn=0-19-533654-2\n|page=46\n}}</ref> with exponent <math>n</math> is defined as\n\n:<math>x_+^n = \n\\begin{cases} \nx^n &:\\ x > 0 \\\\\n0 &:\\ x \\le 0.\n\\end{cases}\n</math>\n\nIn particular,\n:<math>x_+ = \n\\begin{cases} \nx &:\\ x > 0 \\\\\n0 &:\\ x \\le 0.\n\\end{cases}\n</math>\nand interpret the exponent as conventional [[power function|power]].\n\n==Relations==\n* Truncated power functions can be used for construction of [[B-spline]]s.\n* <math>x \\mapsto x_+^0</math> is the [[Heaviside function]].\n* <math>\\chi_{[a,b)}(x) = (b-x)_+^0 - (a-x)_+^0</math> where <math>\\chi</math> is the [[indicator function]].\n* Truncated power functions are [[refinable function|refinable]].\n\n== See also ==\n* [[Macaulay brackets]]\n\n==External links==\n*[http://mathworld.wolfram.com/TruncatedPowerFunction.html Truncated Power Function on MathWorld]\n\n==References==\n<references/>\n\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Truncation",
      "url": "https://en.wikipedia.org/wiki/Truncation",
      "text": "{{Other uses}}\n\nIn [[mathematics]] and [[computer science]], '''truncation''' is limiting the number of [[numerical digit|digit]]s right of the [[decimal point]].\n\n== Truncation and floor function ==\n{{main|Floor and ceiling functions}}\nTruncation of positive real numbers can be done using the [[floor function]]. Given a number <math>x \\in \\mathbb{R}_+</math> to be truncated and <math>n \\in \\mathbb{N}_0</math>, the number of elements to be kept behind the decimal point, the truncated value of x is\n:<math>\\operatorname{trunc}(x,n) = \\frac{\\lfloor 10^n \\cdot x \\rfloor}{10^n}.</math>\n\nHowever, for negative numbers truncation does not round in the same direction as the floor function: truncation always rounds toward zero, the floor function rounds towards negative infinity. For a given number <math>x \\in \\mathbb{R}_-</math>, the function\n:<math>\\operatorname{trunc}(x,n) = \\frac{\\lceil 10^n \\cdot x \\rceil}{10^n}</math>\n\nis used instead.\n\n== Causes of truncation ==\nWith computers, truncation can occur when a decimal number is [[type conversion|typecast]] as an [[integer]]; it is truncated to zero decimal digits because integers cannot store non-integer [[real numbers]].\n\n== In algebra ==\nAn analogue of truncation can be applied to [[polynomial]]s. In this case, the truncation of a polynomial ''P'' to degree ''n'' can be defined as the sum of all terms of ''P'' of degree ''n'' or less. Polynomial truncations arise in the study of [[Taylor polynomial]]s, for example.<ref>{{cite book|first=Michael|last=Spivak|title=Calculus|edition=4th|year=2008|isbn=978-0-914098-91-1|page=434}}</ref>\n\n== See also ==\n* [[Arithmetic precision]]\n* [[Floor function]]\n* [[Quantization (signal processing)]]\n* [[Precision (computer science)]]\n* [[Truncation (statistics)]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://to-campos.planetaclix.pt/fractal/walle.html Wall paper applet] that visualizes errors due to finite precision\n\n[[Category:Numerical analysis]]\n\n[[ja:端数処理]]"
    },
    {
      "title": "Truncation error",
      "url": "https://en.wikipedia.org/wiki/Truncation_error",
      "text": "{{Other uses|Truncation error (numerical integration)}}\nIn [[numerical analysis]] and [[scientific computing]], '''truncation error''' is the error made by truncating an infinite sum and approximating it by a finite sum. For instance, if we approximate the [[sine]] function by the first two non-zero term of its Taylor series, as in <math>\\sin(x) \\approx x - \\tfrac16 x^3</math> for small <math>x</math>, the resulting error is a truncation error. It is present even with infinite-precision arithmetic, because it is caused by truncation of the infinite [[Taylor series]] to form the algorithm. \n\nOften, truncation error also includes [[discretization error]], which is the error that arises from taking a finite number of steps in a computation to approximate an infinite process. For example, in [[numerical methods for ordinary differential equations]], the continuously varying function that is the solution of the differential equation is approximated by a process that progresses step by step, and the error that this entails is a discretization or truncation error. See [[Truncation error (numerical integration)]] for more on this.\n\nOccasionally, [[round-off error]] (the consequence of using finite precision [[Floating point | floating point numbers]] on computers) is also called truncation error, especially if the number is rounded by [[truncation]].\n\n== References ==\n* {{Citation | last1=Atkinson | first1=Kendall A. | title=An Introduction to Numerical Analysis | publisher=[[John Wiley & Sons]] | location=New York | edition=2nd | isbn=978-0-471-50023-0 | year=1989 | page=20 }}\n* {{Citation | last1=Stoer | first1=Josef | last2=Bulirsch | first2=Roland | title=Introduction to Numerical Analysis | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=3rd | isbn=978-0-387-95452-3 | year=2002 | page=1 }}.\n\n[[Category:Numerical analysis]]"
    },
    {
      "title": "List of uncertainty propagation software",
      "url": "https://en.wikipedia.org/wiki/List_of_uncertainty_propagation_software",
      "text": "'''List of uncertainty propagation software''' used to perform [[propagation of uncertainty]] calculations:\n\n==Software==\n* [http://aue.erc.monash.edu.au/webMathematica/Asue/Evaluate.jsp ASUE] Potent web interface powered by [[Wolfram Mathematica|webMathematica]] to evaluate uncertainty symbolically using GUM. Webpage also allows symbolic uncertainty evaluation via ASUE framework (with reference), which is an extension to GUM framework\n* [https://dakota.sandia.gov/ Dakota] is a comprehensive suite of tools for sampling-based optimization and UQ developed by Sandia National Laboratories.\n* [http://www.udue.de/DSI Dempster Shafer with Intervals (DSI) Toolbox] is a [[MATLAB]] toolbox for verified computing under [[Dempster–Shafer theory]]. It provides aggregation rules, fast (non) monotonic function propagation, plots of basic probability assignments, verified [[fault tree analysis]] (FTA), and much more.\n* [http://www.futureskill.com/index.php?option=com_content&view=article&id=116&Itemid=111 EasyGraph] is a graphing package that supports error propagation directly into the error bars.\n* [http://epc.sourceforge.net/ epc] error propagating calculator (epc) is an open source script-based tool that calculates the propagation of errors in variables. To quote the text on the Epc web page \"This is done by repeated calculation of the expression using variable-values which are generated using a random number generator whose mean and standard-deviation match the values specified for the variable\".\n* [http://sourceforge.net/projects/errorcalc/?source=updater Error Propagation Calculator] Free cross-platform calculator ([[macOS]], [[Microsoft Windows|Windows]], [[Linux]]) written in [[Python (programming language)|Python]]. Essentially a GUI interface for the Python Uncertainties library. Easy to use and install.\n* [http://sites.google.com/site/ErrCalc/ ErrorCalc] is a scientific calculator app for iPhone or iPad that performs error propagation (Supports Algebraic and RPN modes of entry)\n* [https://pypi.python.org/pypi/FuncDesigner FuncDesigner]\n*[https://quodata.de/en/software/for-determination-of-measurement-uncertainty.html GUMsim] is a Monte Carlo simulator and uncertainty estimator for Windows\n*[https://www.measurement.govt.nz/resources GUM Tree] is a design pattern for propagating measurement uncertainty. An implementation exists in [[R (programming language)|R]] and add-ons for [[Microsoft Excel|Excel]] (real and complex numbers).\n* [https://www.measurement.govt.nz/resources GUM Tree Calculator] is a programmable Windows command-line tool with full support for uncertainty calculations involving real and complex quantities.\n*[http://www.metrodata.de/index_en.html GUM Workbench] implements a systematic way to analyze an uncertainty problem for single and multiple results. GUM + Monte Carlo. Free restricted educational version available.\n* The Gustavus propagator is an open source calculator that supports error propagation developed by Thomas Huber.\n*[https://pypi.org/project/gvar/ gvar] is a Python library for first order uncertainty propagation with correlations. Features transparent handling of arrays, dictionaries and dictionaries of arrays; numerical computation with uncertainty propagation of splines, matrix operations, differential equations, integrals, power series and equations.\n*The [https://web.archive.org/web/20160419182625/http://laffers.net/blog/2010/11/15/error-propagation-calculator/ laffers.net propagator] is a web-based tool for propagating errors in data. The tool uses the standard methods for propagation.\n* [http://www.lne.fr/fr/logiciels/MCM/logiciel-lne-mcm.asp LNE-MCM] is a free software for Windows dedicated to the evaluation of measurement uncertainty using Monte Carlo simulations according to Supplement 1 to the GUM. Moreover, additional features are implemented like the case of multivariate models, sensitivity analysis to provide an uncertainty budget and a goodness-of-fit test for the samples of the output quantities. \n* [http://mathosproject.com/updates/custom-functions-in-the-uncertainty-module/ Mathos Core Library Uncertainty package] Open source (.NET targeting library).\n* [http://metgen.pagesperso-orange.fr/metrologieen26.htm MC-Ed] is a native Windows software to perform uncertainty calculations according to the Supplement 1 to the Guide to the expression of uncertainty in measurement using Monte-Carlo method.\n* [https://github.com/giordano/Measurements.jl Measurements.jl] is a free and open-source error propagation calculator and library.  It supports real and complex numbers with uncertainty, [[arbitrary-precision arithmetic]] calculations, functional correlation between variables, mathematical and linear algebra operations with matrices and arrays, and numerical integration using [[Gauss–Kronrod quadrature formula|Gauss–Kronrod quadrature]].\n*[http://www.metas.ch/unclib Metas.UncLib] is a [[C Sharp (programming language)|C#]] software library.  A MATLAB wrapper exists. It supports: multivariate uncertainties, complex values, correlations, vector, and matrix algebra.\n* [https://cran.r-project.org/package=metRology metRology package for R] metRology is an R package to support metrological applications. Among other functions for metrology, it includes support for measurement uncertainty evaluation using algebraic and numerical differentiation and Monte Carlo methods.<ref>{{cite web |url=https://CRAN.R-project.org/package=metRology\n |title=metRology: Support for Metrological Applications |last=Ellison |first=Stephen L. R. |date=2017 |website=[[CRAN (R programming language)]] |access-date=2018-02-20 |ref=metRology}}</ref> \n*MUSE Measurement Uncertainty Simulation and Evaluation using the monte carlo method.\n* [http://muq.mit.edu/ MUQ] is an MIT developed collection of UQ tools for Markov Chain Monte Carlo sampling, Polynomial Chaos construction, transport maps, and many other operations. It has both C++ and Python interfaces.\n* [http://www.cossan.co.uk/software/open-cossan-engine.php OpenCOSSAN] is a [[MATLAB]] toolbox for uncertainty propagation, reliability analysis, model updating, sensitivity and robust design optimization. Allows interacting with 3rd party solvers. Interfaces with HPC through GridEngine and OpenLava.\n* [https://uncertainty.nist.gov NIST Uncertainty Machine] is an uncertainty calculator that uses Gauss' formula and Monte Carlo methods. Users access it through a web browser, but it runs in the [[R programming language]] on the server.<ref>{{Cite journal|last=Lafarge, T. and Possolo, A|date=2015|title=The NIST Uncertainty Machine|url=|journal=NCSLI Measure Journal of Measurement Science|volume=10|issue=3|pages=20–27|doi=10.1080/19315775.2015.11721732}}</ref>\n*[http://www.openturns.org OpenTURNS] is a [[C++]] and [[Python (programming language)|Python]] framework for probabilistic modelling and uncertainty management developed by the OpenTURNS consortium ([http://www.airbus.com/ Airbus], [https://www.edf.fr/en/meta-home EDF R&D], [http://imacs.polytechnique.fr/ IMACS], [http://www.phimeca.com/?lang=en Phimeca]). It contains state of the art algorithms for univariate, multivariate and infinite dimensional probabilistic modelling (arithmetic of independent random variables, copulas, Bayesian models, stochastic processes and random fields), Monte Carlo simulation, surrogate modelling (Kriging, functional chaos decomposition, low rank tensor approximation, Karhunen-Loeve decomposition, mixture of experts), rare event estimation (variance reduction, FORM/SORM reliability methods), robust optimization, global sensitivity analysis (ANCOVA, Sobol' indices). It can interact with third party software through a generic Python interface, which also allows to connect HPC facilities.\n*[http://www.qsyst.com/ QMSys GUM] is a potent commercial tool for measurement uncertainty analysis including Monte Carlo simulation for Windows (free restricted educational version available).\n*[http://www.ramas.com/riskcalc.htm Risk Calc] supports probability bounds analysis, standard fuzzy arithmetic, and classical interval analysis for conducting distribution-free or nonparametric risk analyses.\n*[https://www.smartuq.com/ SmartUQ] is a commercial uncertainty quantification and analytics software package. Capabilities include DOE generation, emulator construction, uncertainty propagation, sensitivity analysis, statistical calibration, and inverse analysis. \n*[https://pypi.python.org/pypi/soerp/ SOERP] implements second-order error propagation as a free Python library. Calculations are carried out naturally in calculator format and correlations are maintained.\n*[[SCaViS]] is a free data-analyais program written in Java and supports Python and Groovy.\n*[https://scram-pra.org SCRAM] is free [[Fault tree analysis|fault tree]] and [[Event tree analysis|event tree]] analysis software that employs Monte Carlo simulation for uncertainty analysis in probability expressions.\n* The [http://denethor.wlu.ca/data/xc.shtml Uncertainty Calculator] is a [[JavaScript]] browser-based calculator that performs error propagation calculations.\n* [https://www.av8n.com/physics/uncertainty-calculator.html Uncertainty Calculator] runs [[JavaScript]] in the browser.  [https://www.av8n.com/physics/uncertainty-calculator-doc.html Simple version]: uses Crank Three Times to provide numerical answers.  [https://www.av8n.com/physics/uncertainty-calculator-fancy.html Fancy version]: uses Monte Carlo to provide additional information including graphs of probability density and cumulative probability.  Warns users of potential issues that other methods don't warn about.  Handles correlations that arise during multi-step calculations.  Numerous interactive pushbutton demos.\n*[http://packages.python.org/uncertainties/ Uncertainties] is a potent free calculator and Python software library for transparently performing calculations with uncertainties and correlations.\n* [http://labs.mathosproject.com/Module/Uncertainty.aspx Mathos Laboratory Uncertainty Calculator] This is a web interface for uncertainty calculations.\n* [http://www.uqlab.com UQLab] is a software framework for uncertainty quantification developed at [http://www.sudret.ibk.ethz.ch/ ETH Zurich]. It is a general-purpose software running in MATLAB which contains state-of-the-art methods for Monte Carlo simulation, dependence modelling (copula theory), surrogate modelling (polynomial chaos expansions, Kriging (a.k.a. Gaussian process modelling), low-rank tensor approximations, global sensitivity analysis (ANOVA, Sobol’ indices, distribution-based indices), rare event simulation (a.k.a. reliability methods).<ref name=\"UQLab2014\">Marelli, S. and Sudret, B., [http://e-collection.library.ethz.ch/view/eth:14488 ''UQLab: A framework for uncertainty quantification in Matlab''], Proc. 2nd Int. Conf. on Vulnerability, Risk Analysis and Management (ICVRAM2014), Liverpool, United Kingdom, 2014</ref>\n* [http://pythonhosted.org/UncertaintyWrapper/ UncertaintyWrapper] is a free and open source software Python package that propagates uncertainty using 1st order linear combinations. Covariance  is also propagated. It approximates sensitivity with finite central differences. UncertaintyWrapper wraps any Python code even C extensions. It is vetted against Uncertainties, ALGOPY, Numdifftools and SymPy.\n* [http://www.sandia.gov/UQToolkit UQTk] is a set of tools for forward and inverse uncertainty quantification on computational models. The functionality can be accessed through C++, command line apps, or Python. The uncertainty quantification approaches in UQTk rely extensively on [https://en.wikipedia.org/wiki/Polynomial_chaos Polynomial Chaos methods] for representing random variables.\n\n==Comparison==\n{{cleanup|section|date=November 2018|reason=too technical for people to understand.}}\n\n{| class=\"wikitable sortable\" style=\"font-size: 90%; text-align: center; width: auto;\"\n|-\n! Name\n! Creator\n! [[Software license|License]]\n! [[Programming language]]\n! Handles [[Correlation and dependence|correlations]]?\n! [[Cross-platform]]?\n! [[Software calculator|Calculator]]?\n!Complex numbers calculation?\n!VISA data acquisition possible ?\n! [[Library (computing)|Library]]?\n! Remarks\n|-\n! [http://aue.erc.monash.edu.au/webMathematica/Asue/Evaluate.jsp ASUE]\n| [http://tc32.ieee-ims.org/content/analytic-standard-uncertainty-evaluation-toolbox TC-32 - Fault Tolerant Measurement Systems]\n| {{free}}\n| [[Wolfram Mathematica|Mathematica]]\n| {{no}}\n| {{yes}}\n| {{yes}}\n|\n| {{no}}\n| Potent web interface powered by [[Wolfram Mathematica|webMathematica]] to evaluate uncertainty symbolically using GUM. Webpage also allows symbolic uncertainty evaluation via ASUE framework (with reference), which is an extension to the GUM uncertainty propagation framework.\n| rowspan=\"5\" {{n/a}}\n|-\n! [http://sourceforge.net/projects/errorcalc/?source=updater Error Propagation Calculator]\n| R. Paul Nobrega \n| {{free}}\n| [[Python (programming language)|Python]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n|\n| {{no}}\n| Desktop calculator (Windows, macOS, Linux), handles up to 26 variable and error pairs per computation. Evaluates native python expressions. No prior knowledge of python language is required for use. Windows installer includes python dependencies.\n|-\n! [https://web.archive.org/web/20121002123731/http://www.eecs.berkeley.edu/~megens/abacus/abacus_description.html Abacus]\n| Mischa Megens \n| {{free}}\n| [[C (programming language)|C]], [[Win32]]\n| {{yes}}\n| {{no}}\n| {{yes}}\n|\n| {{no}}\n| Desktop calculator (Windows), handles multiple expressions\n|-\n! [https://metacpan.org/module/App::ErrorCalculator App::ErrorCalculator]\n| Steffen Müller \n| {{GPL-lic}}\n| [[Perl]]\n| {{no}}\n|\n|\n|\n| {{yes}}\n| Library and script to process tabular values\n|-\n! [http://www.colby.edu/chemistry/PChem/scripts/error.html Colby College Uncertainty Calculator]\n| T. W. Shattuck\n| {{unk}}\n| [[JavaScript]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n|\n| {{no}}\n| Web browser-based. Formula input, then asks for values of variables.\n|-\n! [http://www.udue.de/DSI Dempster Shafer with Intervals (DSI) Toolbox]\n| Gabor Rebner\n| {{free|Free for private and academic use}}\n| [[MATLAB]]\n| {{no}}\n| {{yes}}\n|\n|\n|\n|\n| Verified computation of basic probability assignments and [[fault tree analysis]] under [[Dempster–Shafer theory]]. Fast evaluation of (non) monotonic system functions and aggregation rules.\n|-\n! [http://www.upscale.utoronto.ca/GeneralInterest/Harrison/ErrProp.html Experimental Data Analyst (EDA)]\n| David Harrison\n| {{proprietary}}\n| [[Wolfram Mathematica|Mathematica]]\n| {{yes}}\n|\n| {{no}}\n|\n| {{yes}}\n| Library\n| {{n/a}}\n|-\n! [http://epc.sourceforge.net/ EPC: error-propagating calculator]\n| Dan Kelley\n| {{GPL-lic}}\n| [[Perl]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n|\n|\n|\n| [[Perl]] Script.  Monte Carlo evaluation of an expression.\n|-\n! [http://sites.google.com/site/ErrCalc/ ErrorCalc iPhone/iPad calculator app]\n| Thomas Huber\n| {{proprietary}}\n| {{unk}}\n| {{no}}\n| {{no}}\n| {{yes}}\n|\n| {{no}}\n| Scientific calculator app for iPhone or iPad that performs error propagation (Algebraic and RPN Entry modes)\n| rowspan=\"6\" {{n/a}}\n|-\n| FuncDesigner\n| Dmitrey\n| {{BSD-lic}}\n| [[Python (programming language)|Python]]\n| {{no}}\n|\n| {{yes}}\n|\n| {{yes}}\n| Library and stand-alone (via the Python shell). Involves [[automatic differentiation]], possibly large-scale sparse\n|-\n! [http://www.aoc.nrao.edu/~sbhatnag/softwares.html#FUSSY fussy]\n| S. Bhatnagar\n| {{free|Free but copyrighted}}\n| [[C (programming language)|C]]\n| {{yes}}\n| {{no}}\n| {{yes}}\n|\n| {{yes}}\n| Scripting language called '[http://www.aoc.nrao.edu/~sbhatnag/Softwares/fussy/fussy fussy]', similar to C.\n|-\n! [http://www.metrodata.de/index_en.html GUM Workbench]\n| Metrodata GmbH\n| {{proprietary}}\n| [[Object Pascal]] ([[Delphi (programming language)|Delphi]])\n| {{yes}}\n|\n| {{yes}}\n|\n| {{no}}\n| Standalone. Detailed consequences of a model equation. GUM + Monte Carlo. Free restricted educational version available.\n|-\n! [http://jeanmarie.biansan.free.fr/gum_mc.html GUM_MC]\n| Jean-Marie Biansan\n| {{GPL-lic}}\n| [[Lazarus (IDE)|Lazarus]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n|\n| {{no}}\n| Standalone. Gum framework and Monte Carlo method.\n|-\n! [https://quodata.de/en/software/for-determination-of-measurement-uncertainty.html GUMsim]\n|QuoData GmbH\n| {{proprietary}}\n| [[Object Pascal]] ([[Delphi (programming language)|Delphi]])\n| {{yes}}\n| {{no}}\n| {{yes}}\n|\n| {{no}}\n| Standalone; detailed consequences of a model equation\n|-\n! [https://www.measurement.govt.nz/resources GUM Tree Calculator (GTC)]\n| Callaghan Innovation\n| {{proprietary}}, [[freeware]] single-user, nontransferable\n| [[Python (programming language)|Python]]\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| A programmable command-line calculator for Windows. Suitable for calculations involving real and complex quantities. Programmable in Python. An IDE with syntax highlighting and on-line help is included.\n|-\n! [http://physics.gac.edu/~huber/error_calc/ Gustavus Adolphus error propagation calculator]\n| Thomas Huber\n| {{GPL-lic}}\n| {{unk}}\n| {{no}}\n| {{no}}\n| {{yes}}\n|\n| {{no}}\n| Executable only. Desk calculator style  (with no expression parentheses).\n| {{n/a}}\n|-\n! [https://pypi.org/project/gvar/ gvar]\n| G. Peter Lepage\n| {{GPL-lic}}\n| [[Python (programming language)|Python]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| Library\n| Not trivial to install on Windows, a compiled binary is provided [https://www.lfd.uci.edu/~gohlke/pythonlibs/#lsqfit here].\n|-\n! [https://web.archive.org/web/20100324023821/http://laffers.net/tools/error-propagation-calculator.php laffers.net error propagation calculator]\n| Richard Laffers\n| {{free|[[Creative Commons license|Creative Commons]]}}\n| [[JavaScript]]\n| {{no}}\n| {{yes}}\n| {{yes}}\n|\n| {{no}}\n| Web browser-based, desk calculator style\n| rowspan=\"11\" {{n/a}}\n|-\n! [http://www.lne.fr/fr/logiciels/MCM/logiciel-lne-mcm.asp LNE-MCM]\n| [http://www.lne.fr/index.asp LNE]\n| {{free}}\n| MATLAB\n| {{yes}}\n| {{no}}\n| {{yes}}\n|\n| {{no}}\n| Standalone. MATLAB Runtime required. Monte Carlo method, Supplement 1 to the GUM. Sensitivity analysis, Multivariate models, Goodness-of-fit test.\n|-\n! [https://www.measurement.govt.nz/resources Measurement Software Toolkit]\n| Callaghan Innovation\n| {{proprietary}}, [[freeware]] noncommercial use\n| [[R (programming language)|R]], [[Microsoft Excel|Excel]] plug-in \n| {{yes}}\n|\n| {{yes}}\n|\n| {{yes}}\n| Library and plug-in\n|-\n! [https://github.com/giordano/Measurements.jl Measurements.jl]\n| Mosè Giordano\n| {{free|[[MIT License|MIT]]}}\n| [[Julia (programming language)|Julia]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| Supports real and complex numbers with uncertainty, arbitrary-precision calculations, functional correlation between variables, mathematical and linear algebra operations with matrices and arrays, and numerical integration.\n|-\n! [https://CRAN.R-project.org/package=metRology metRology package for R]\n| S. Ellison\n| {{free}}, [[GNU General Public License|GPL]]\n| [[R (programming language)|R]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n|\n| {{no}}\n| Includes first-order algebraic and numerical differentiation, including finite-difference with specified step size and Kragten's method,<ref>{{cite journal |last1=Kragten |first1=J. |date=1994 |title=Calculating standard deviations and confidence intervals with a universally applicable spreadsheet technique |journal=Analyst |volume=119 |issue=10 |pages=2161–2166 |doi=10.1039/AN9941902161 }}</ref> as well as Monte Carlo simulation. Evaluation can be applied to R expressions, formulae and functions.\n|-\n! [http://www.mu.ethz.ch/muse/ MUSE]\n| Measurement Uncertainty Research Group, [[ETH Zürich]]\n| {{proprietary}}, [[freeware]] noncommercial use\n| [[C++]]\n| {{yes}}  \n|\n| {{yes}}\n|\n| {{no}}\n| Standalone. Monte-Carlo sampling. Interprets an [[XML]] model description file.\n|-\n! [https://archive.is/20130115173804/http://www.metas.ch/unclib/ Metas.UncLib]\n| Michael Wollensack, METAS\n| {{proprietary}} [[freeware]], no redistribution\n| [[C Sharp (programming language)|C#]], [[MATLAB]] wrapper\n| {{yes}}\n| {{no}}\n| {{yes}}\n|\n| {{yes}}\n| Library, and command-line calculator, via MATLAB\n|-\n! [https://uncertainty.nist.gov NIST Uncertainty Machine]\n| Thomas Lafarge, Antonio Possolo, National Institute of Standards and Technology \n| {{free| public domain}}\n| [[R]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n|\n| {{yes}}\n| Uses Gauss' method and Monte Carlo.  Web interface to a server-side R language calculator.  Complete [https://uncertainty.nist.gov/NISTUncertaintyMachine-UserManual.pdf documentation].\n|-\n! [https://metacpan.org/module/Number::WithError Number::WithError]\n| Steffen Müller \n| {{GPL-lic}}\n| [[Perl]]\n| {{no}}\n|\n| {{no}}\n|\n| {{yes}}\n| Library\n|-\n! [http://denethor.wlu.ca/data/xc.shtml Uncertainty Calculator; Wilfrid Laurier University ]\n| Terry Sturtevant\n| {{unk}}\n| [[JavaScript]]\n| {{no}}\n| {{yes}}\n| {{yes}}\n|\n| {{no}}\n| Web browser-based. Desk calculator style.\n|-\n! [https://cran.r-project.org/web/packages/propagate/index.html propagate]\n| Andrej-Nikolai Spiess \n| {{GPL-lic}}\n| [[R (programming language)|R]]\n| {{yes}}\n| {{yes}}\n| {{no}}\n|\n| {{yes}}\n| An R package that conducts error propagation by first- and second-order Taylor approximation (GUM 2008) and Monte-Carlo simulation (GUM 2008 S1), using full covariance structure. \n|-\n! [http://www.ramas.com/riskcalc.htm Risk Calc]\n| Scott Ferson\n| {{proprietary}}\n| [[C++]]\n| {{yes}}\n|\n| {{yes}}\n|\n|\n|\n| Probabilistic and interval uncertainty. Also handles uncertainty about correlations.\n|-\n! [[SCaViS]]\n| JWork.ORG\n| {{GPL-lic}}\n| [[Java (programming language)|Java]], [[Python (programming language)|Python]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n|\n| {{yes}}\n| Standalone. Conducts error propagation by first- and second-order Taylor approximation and using a Monte Carlo approach for complex functions.\n| rowspan=\"4\" {{n/a}}\n|-\n! [https://pypi.python.org/pypi/soerp/ soerp]\n| Abraham Lee\n| {{BSD-lic}}\n| [[Python (programming language)|Python]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n|\n| {{yes}}\n| Free [[Python (programming language)|Python]] library and command-line calculator. Fully transparent second-order calculations with correlations. Automatically calculates all the first and second derivatives of an expression using the free Python package [https://pypi.python.org/pypi/ad ad]\n|-\n! [http://web.mst.edu/~gbert/JAVA/uncertainty.HTML S&T Missouri Uncertainty Calculator]\n| Gary L. Bertrand\n| {{unk}}\n| [[JavaScript]]\n| {{no}}\n| {{yes}}\n| {{yes}}\n|\n| {{no}}\n| Web browser-based. Desk calculator style.\n|-\n! [https://www.av8n.com/physics/uncertainty-calculator.html Uncertainty Calculator]\n| John Denker\n| {{free|Free for non-commercial use}}\n| [[JavaScript]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n|\n| {{yes}}\n| [https://www.av8n.com/physics/uncertainty-calculator-doc.html Simple version]: uses Crank Three Times to provide numerical answers.  [https://www.av8n.com/physics/uncertainty-calculator-fancy.html Fancy version]: uses Monte Carlo to provide additional information including graphs of probability density and cumulative probability.  Warns users of potential issues that other methods don't warn about.  Handles correlations that arise during multi-step calculations.  Numerous interactive pushbutton demos.\n|-\n! [http://www.qsyst.com/ QMSys GUM]\n|Qualisyst Ltd.\n| {{proprietary}}\n| {{unk}}\n| {{yes}}\n| {{no}}\n|\n|\n|\n|\n| Standalone. Linear/nonlinear models, Monte Carlo method. (free restricted educational version available)\n|-\n! [http://metgen.pagesperso-orange.fr/metrologieen26.htm MC-Ed]\n| Florian Platel (MetGen) \n| {{free}}\n| [[Object Pascal]] ([[Delphi (programming language)|Delphi]])\n| {{no}}\n| {{no}}\n| {{yes}}\n|\n| {{no}}\n| Desktop calculator (Win32 native application). Monte-Carlo simulations.\n| rowspan=\"8\" {{n/a}}\n|-\n! [http://www.cossan.co.uk/software/open-cossan-engine.php OpenCOSSAN]\n| [http://www.liv.ac.uk/risk-and-uncertainty/ Institute of Risk and Uncertainty], [http://www.liv.ac.uk/ University of Liverpool]\n| {{GPL-lic}}\n| [[MATLAB]]\n| {{yes}}\n| {{yes}}\n| {{no}}\n|\n| {{yes}}\n| Provides an object-oriented programming interface to advanced algorithms and solution sequences.\n|-\n! [http://www.openturns.org/ OpenTURNS]\n| [http://www.airbus.com/ Airbus], [https://www.edf.fr/en/meta-home EDF R&D], [http://imacs.polytechnique.fr/ IMACS], [http://www.phimeca.com/?lang=en Phimeca]\n| {{LGPL-lic}}\n| [[C++]], [[Python (programming language)|Python]]\n| {{yes}}\n| {{yes}}\n| {{no}}\n|\n| {{yes}}\n| State of the art advanced probabilistic modelling (copulas, Bayesian models, stochastic processes, random fields), state of the art algorithms, Windows-Linux-macOS, C++ and Python APIs\n|-\n! [https://www.smartuq.com/software/ SmartUQ]\n| [https://www.smartuq.com/about/ SmartUQ LLC]\n| {{proprietary}}\n| [[C++]]\n| {{yes}}\n| {{yes}}\n| {{no}}\n|\n| {{no}}\n| Standalone application for Windows & Linux. [[MATLAB]] and [[Python (programming language)|Python]] APIs\n|-\n! [http://pythonhosted.org/uncertainties/ uncertainties]\n| Eric O. Lebigot (EOL)\n| {{BSD-lic}}\n| [[Python (programming language)|Python]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| Library or stand-alone command-line calculator (via the [[Python (programming language)|Python]] shell). Fully transparent, analytic calculations with correlations. Also handles matrices with uncertainties. Automatically calculates all the derivatives of an expression\n|-\n! [http://www.uqlab.com UQLab]<ref name=\"UQLab2014\" />\n| [https://www.ethz.ch ETH Zürich], [http://www.sudret.ibk.ethz.ch/ Chair of Risk, Safety and Uncertainty Quantification]\n| {{free|[[BSD licenses|BSD]] scientific modules. Free for academic use}}\n| [[MATLAB]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n|\n| {{no}}\n| General purpose software including copula modeling, surrogate models (polynomial chaos expansions, Kriging (a.k.a. Gaussian process modeling), low-rank tensor approximations), global sensitivity analysis (Sobol’ indices), rare event simulation (FORM/SORM, importance sampling, subset simulation). Easy to link with third party codes, fast learning curve.\n|-\n! [http://pythonhosted.org/UncertaintyWrapper/index.html UncertaintyWrapper]\n| [https://Mikofski.github.io Mark Mikofski]\n| {{BSD-lic}}\n| [[Python (programming language)|Python]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n|\n| {{yes}}\n| Python decorators to wrap any method including C extensions. Propagates 1st order uncertainties using finite central difference approximation of Jacobian matrix. Sensitivity also propagated. Any number of arguments and return values. Calculates multiple observations simultaneously. Validated with Uncertainties, ALGOPY, Numdifftools and SymPy.\n|-\n!MCM Alchimia\n|\n|\n| {{unk}}\n| {{yes}}\n|\n|\n|\n|\n| {{no}}\n|}\n\n==See also==\n*[[Automatic differentiation#Software]], also can be used to obtain uncertainties\n\n==References==\n{{Reflist}}\n* Y. C. Kuang, A. Rajan, M. P.-L. Ooi, and T. C. Ong (2014), \"[http://www.sciencedirect.com/science/article/pii/S0263224114004023 Standard uncertainty evaluation of multivariate polynomial],\" Measurement, vol. 58, pp.&nbsp;483–494, Dec. 2014\n* A. Rajan, M. P. Ooi, Y. C. Kuang, and S. N. Demidenko, \"[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7064781 Analytical Standard Uncertainty Evaluation Using Mellin Transform],\" Access, IEEE, vol. 3, pp.&nbsp;209–222, 2015.\n* Auer, E., Luther, W., Rebner, G., Limbourg, P. (2010) [http://www.scg.inf.uni-due.de/fileadmin/Projekte/Dempster/literature/A_Verified_MATLAB_Toolbox_for_the_Dempster-Shafer_Theory.pdf A Verified MATLAB Toolbox for the Dempster-Shafer Theory]. Proceedings of the Workshop on the Theory of Belief Functions\n* Bevington, P.R. and Robinson, D.K. (2002) ''Data Reduction and Error Analysis for the Physical Sciences'', 3rd Ed., McGraw-Hill {{ISBN|0-07-119926-8}}\n* Ferson, S., Kreinovich, V., Hajagos, J., Oberkampf, W. and Ginzburg, L. (2007) [http://prod.sandia.gov/techlib/access-control.cgi/2007/070939.pdf \"Experimental Uncertainty Estimation and Statistics for Data Having Interval Uncertainty\"]. Sandia National Laboratories Report: SAND2007-0939.\n* Patelli, E., [https://www.researchgate.net/profile/Edoardo_Patelli/publication/258030454_An_open_efficient_computational_framework_for_reliability_based_optimization/file/ef31753022db2ee741.pdf?ev=pub_ext_doc_dl&origin=publication_detail&inViewer=true An Open Computational Framework for Reliability Based Optimization], In proceeding of: The Eleventh International Conference on Computational Structures Technology, Dubrovnik, Croatia 4–7 September 2012\n\n{{DEFAULTSORT:Uncertainty Propagation Software}}\n[[Category:Lists of software]]\n[[Category:Numerical analysis]]\n[[Category:Numerical software]]"
    },
    {
      "title": "Unrestricted algorithm",
      "url": "https://en.wikipedia.org/wiki/Unrestricted_algorithm",
      "text": "An '''unrestricted algorithm''' is an [[algorithm]] for the computation of a [[mathematical function]] that puts no restrictions on the range of the [[argument of a function|argument]] or on the precision that may be demanded in the result.<ref name=\"Clenshaw\">{{cite journal|last1=C.W. Clenshaw and F. W. J. Olver|title=An unrestricted algorithm for the exponential function|journal=SIAM Journal on Numerical Analysis|date=April 1980|volume= 17|issue=2|pages=310–331|jstor=2156615}}</ref> The idea of such an algorithm was put forward by C. W. Clenshaw and F. W. J. Olver in a paper published in 1980.<ref name=Clenshaw/><ref name=\"Brent\">{{cite book|last1=Richard P Brent|chapter=Unrestricted algorithms for elementary and special functions|title=Information Processing |volume=80  |editor=S. H. Lavington |publisher=North-Holland, Amsterdam|date=1980|pages=613–619|arxiv=1004.3621}}</ref>\n\nIn the problem of developing algorithms for computing, as regards the values of a [[real-valued function]] of a [[Function of a real variable|real variable]] (e.g., ''g''[''x''] in \"restricted\" algorithms), the error that can be tolerated in the result is specified in advance. An interval on the [[real line]] would also be specified for values when the values of a function are to be evaluated. Different algorithms may have to be applied for evaluating functions outside the interval. An unrestricted algorithm envisages a situation in which a user may stipulate the value of ''x'' and also the precision required in ''g''(''x'') quite arbitrarily. The algorithm should then produce an acceptable result without failure.<ref name = Clenshaw/>\n\n==References==\n{{reflist}}\n\n[[Category:Numerical analysis]]\n[[Category:Algorithms|*]]\n[[Category:Theoretical computer science]]"
    },
    {
      "title": "Uzawa iteration",
      "url": "https://en.wikipedia.org/wiki/Uzawa_iteration",
      "text": "In [[numerical mathematics]], the '''Uzawa iteration''' is an algorithm for solving [[saddle point]] problems. It is named after [[Hirofumi Uzawa]] <!--https://www.amazon.com/Studies-Linear-Non-Linear-Programming-Mathematical/dp/1258444062/ref=sr_1_1?s=books&ie=UTF8&qid=1386043803&sr=1-1--> and was originally introduced in the context of concave programming.<ref name=\"UZ58\">{{cite book |first=H. |last=Uzawa |chapter=Iterative methods for concave programming |editor1-first=K. J. |editor1-last=Arrow |editor2-first=L. |editor2-last=Hurwicz |editor3-first=H. |editor3-last=Uzawa |title=Studies in linear and nonlinear programming |location= |publisher=Stanford University Press |year=1958 |isbn= }}</ref>\n\n== Basic idea ==\nWe consider a saddle point problem of the form\n\n: <math> \\begin{pmatrix} A & B\\\\ B^* & \\end{pmatrix} \\begin{pmatrix} x_1\\\\ x_2 \\end{pmatrix}\n         = \\begin{pmatrix} b_1\\\\ b_2 \\end{pmatrix},</math>\n\nwhere <math>A</math> is a symmetric [[positive-definite matrix]].\nMultiplying the first row by <math>B^* A^{-1}</math> and subtracting from the second row yields the upper-triangular system\n\n: <math> \\begin{pmatrix} A & B\\\\ & -S \\end{pmatrix} \\begin{pmatrix} x_1\\\\ x_2 \\end{pmatrix}\n         = \\begin{pmatrix} b_1\\\\ b_2 - B^* A^{-1} b_1 \\end{pmatrix},</math>\n\nwhere <math>S := B^* A^{-1} B</math> denotes the [[Schur complement]].\nSince <math>S</math> is symmetric positive-definite, we can apply standard iterative methods like the [[gradient descent]]\nmethod or the [[conjugate gradient method]] to\n\n: <math> S x_2 = B^* A^{-1} b_1 - b_2</math>\n\nin order to compute <math>x_2</math>.\nThe vector <math>x_1</math> can be reconstructed by solving\n\n: <math> A x_1 = b_1 - B x_2. \\, </math>\n\nIt is possible to update <math>x_1</math> alongside <math>x_2</math> during the iteration for the Schur complement system and thus obtain an efficient algorithm.\n\n== Implementation ==\n\nWe start the conjugate gradient iteration by computing the residual\n\n: <math> r_2 := B^* A^{-1} b_1 - b_2 - S x_2 = B^* A^{-1} (b_1 - B x_2) - b_2 = B^* x_1 - b_2,</math>\n\nof the Schur complement system, where\n\n: <math> x_1 := A^{-1} (b_1 - B x_2) </math>\n\ndenotes the upper half of the solution vector matching the initial guess <math>x_2</math> for its lower half. We complete the initialization by choosing the first search direction\n\n: <math> p_2 := r_2.\\, </math>\n\nIn each step, we compute\n\n: <math> a_2 := S p_2 = B^* A^{-1} B p_2 = B^* p_1 </math>\n\nand keep the intermediate result\n\n: <math> p_1 := A^{-1} B p_2 </math>\n\nfor later.\nThe scaling factor is given by\n\n: <math> \\alpha := p_2^* a_2 /p_2^* r_2  </math>\n\nand leads to the updates\n\n: <math> x_2 := x_2 + \\alpha p_2, \\quad r_2 := r_2 - \\alpha a_2. </math>\n\nUsing the intermediate result <math>p_1</math> saved earlier, we can also update the upper part of the solution vector\n\n: <math> x_1 := x_1 - \\alpha p_1.\\, </math>\n\nNow we only have to construct the new search direction by the [[Gram–Schmidt process]], i.e.,\n\n: <math> \\beta := r_2^* a_2 / p_2^* a_2,\\quad p_2 := r_2 - \\beta p_2. </math>\n\nThe iteration terminates if the residual <math>r_2</math> has become sufficiently small or if the norm of <math>p_2</math> is significantly smaller than <math>r_2</math> indicating that the [[Krylov subspace]] has been almost exhausted.\n\n== Modifications and extensions ==\nIf solving the linear system <math>A x=b</math> exactly is not feasible, inexact solvers can be applied.<ref name=\"ELGO94\">{{cite journal |first=H. C. |last=Elman |first2=G. H. |last2=Golub |authorlink2=Gene H. Golub |title=Inexact and preconditioned Uzawa algorithms for saddle point problems |journal=[[SIAM Journal on Numerical Analysis|SIAM J. Numer. Anal.]] |volume=31 |issue=6 |pages=1645–1661 |year=1994 |doi=10.1137/0731085 |citeseerx=10.1.1.307.8178 }}</ref><ref>{{cite journal |first=J. H. |last=Bramble |authorlink=James H. Bramble |first2=J. E. |last2=Pasciak |first3=A. T. |last3=Vassilev |title=Analysis of the inexact Uzawa algorithm for saddle point problems |journal=SIAM J. Numer. Anal. |volume=34 |issue=3 |pages=1072–1982 |year=1997 |doi=10.1137/S0036142994273343 |citeseerx=10.1.1.52.9559 }}</ref><ref>{{cite journal |first=W. |last=Zulehner |title=Analysis of iterative methods for saddle point problems. A unified approach |journal=Math. Comp. |volume=71 |issue= 238|pages=479–505 |year=1998 |doi=10.1090/S0025-5718-01-01324-2 |jstor= }}</ref>\n\nIf the Schur complement system is ill-conditioned, preconditioners can be employed to improve the speed of convergence of the underlying gradient method.<ref name=\"ELGO94\"/><ref name=\"GRKO07\">{{cite book |first=C. |last=Gräser |first2=R. |last2=Kornhuber |chapter=On Preconditioned Uzawa-type Iterations for a Saddle Point Problem with Inequality Constraints |title=Domain Decomposition Methods in Science and Engineering XVI |series=Lec. Not. Comp. Sci. Eng. |volume=55 |issue= |pages=91–102 |year=2007 |doi=10.1007/978-3-540-34469-8_8 |isbn=978-3-540-34468-1 |citeseerx=10.1.1.72.9238 }}</ref>\n\nInequality constraints can be incorporated, e.g., in order to handle obstacle problems.<ref name=\"GRKO07\"/>\n\n== References ==\n{{Reflist}}\n\n== Further reading ==\n* {{cite book |last=Chen |first=Zhangxin |chapter=Linear System Solution Techniques |title=Finite Element Methods and Their Applications |location=Berlin |publisher=Springer |year=2006 |isbn=978-3-540-28078-1 |chapterurl={{Google books |plainurl=yes |id=GvvMfd1chfkC |page=145 }} |pages=145–154 }}\n\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Validated numerics",
      "url": "https://en.wikipedia.org/wiki/Validated_numerics",
      "text": "'''Validated numerics''', or '''rigorous computation''', '''verified computation''', '''reliable computation''', '''numerical verification''' is numerics including mathematically strict error (rounding error, truncation error, discretization error) evaluation, and it is one field of [[numerical analysis]]. For computation, [[interval arithmetic]] is used, and all results are represented by intervals. Validated numerics were used by [[Warwick Tucker]] in order to solve the 14th of [[Smale's problems]].<ref>Tucker, W. (1999). \"The Lorenz attractor exists.\" ''Comptes Rendus de l'Académie des Sciences-Series I-Mathematics'', 328(12), 1197–1202.</ref>, and today it is recognized as a powerful tool for the study of [[dynamical systems]]<ref>ZIN ARAI, HIROSHI KOKUBU, AND PAWEÃL PILARCZYK. [https://www.math.kyoto-u.ac.jp/~kokubu/preprints/rigdynam.pdf RECENT DEVELOPMENT IN RIGOROUS COMPUTATIONAL METHODS IN DYNAMICAL SYSTEMS].</ref>\n\n==Importance==\nComputation without verification may cause unfortunate results. Below are some examples.\n\n===Rump's example===\nIn the 1980s, Rump made an example<ref>Rump, S. M. (1988). \"Algorithms for verified inclusions: Theory and practice.\" In ''Reliability in computing'' (pp. 109–126). Academic Press.</ref>. He made a complicated function and tried to obtain its value. Single precision, double precision, extended precision results seemed to be correct, but its plus-minus sign was different from the true value.\n\n===Phantom solution===\nBreuer–Plum–McKenna used the spectrum method to solve the boundary value problem of the Emden equation, and reported that an asymmetric solution was obtained<ref>Breuer, B., Plum, M., & McKenna, P. J. (2001). \"Inclusions and existence proofs for solutions of a nonlinear boundary value problem by spectral numerical methods.\" In ''Topics in Numerical Analysis'' (pp. 61–77). Springer, Vienna.</ref>. This result to the study conflicted to the theoretical study by Gidas–Ni–Nirenberg which claimed that there is no asymmetric solution<ref>Gidas, B., Ni, W. M., & Nirenberg, L. (1979). \"Symmetry and related properties via the maximum principle.\" ''Communications in Mathematical Physics'', 68(3), 209–243.</ref>. The solution obtained by Breuer–Plum–McKenna was a phantom solution caused by discretization error. This is a rare case, but it tells us that when we want to strictly discuss differential equations, numerical solutions must be verified.\n\n===Accidents caused by numerical errors===\nThe following examples are known as accidents caused by numerical errors.\n*Failure of intercepting missiles in the [[Gulf War]] (1991)<ref>http://www-users.math.umn.edu/~arnold//disasters/patriot.html</ref>\n*Failure of the [[Ariane 5]] rocket (1996)<ref>ARIANE 5 Flight 501 Failure, http://sunnyday.mit.edu/nasa-class/Ariane5-report.html</ref>\n\n==Main topics==\nThe study of validated numerics is divided into the following fields.\n*Verification in [[numerical linear algebra]]\n*Verification of [[special functions]] (such as the [[gamma function]] and the [[Bessel function]])\n*Verification of [[numerical quadrature]]\n*Verification of nonlinear equations (The [[Kantorovich theorem]], Krawczyk method, interval Newton method, and the Durand–Kerner–Aberth method are studied)\n*Verification for solutions of ODEs, PDEs (for PDEs, knowledge of [[functional analysis]] are used)\n*Verification of [[linear programming]] \n*Verification of [[computational geometry]]\n*Verification at high-performance computing environment\n\n==Tools==\n*[http://www.ti3.tu-harburg.de/rump/intlab/ INTLAB] Library made by [[MATLAB]]/Octave\n*[http://verifiedby.me/kv/index-e.html kv] Library made by [[C++]].\n*[http://arblib.org/ Arb] Library made by C. It is capable to rigorously compute various special functions.\n*[http://capd.ii.uj.edu.pl/ CAPD] A collection of flexible C++ modules which are mainly designed to computation of homology of sets and maps and nonrigorous and validated numerics for dynamical systems.\n\n==Further reading==\n* Tucker, W. (2011). Validated Numerics: A Short Introduction to Rigorous Computations. Princeton University Press.\n* Moore, R. E., Kearfott, R. B., & Cloud, M. J. (2009). Introduction to Interval Analysis. SIAM.\n\n==External links==\n* [http://www2.math.uu.se/~warwick/main/papers/ECM04Tucker.pdf Validated Numerics for Pedestrians]\n\n==See also==\n*[[computer-assisted proof]]\n*[[interval arithmetic]]\n*[[affine arithmetic]]\n*[[Kantorovich theorem]]\n\n== References ==\n<!-- Inline citations added to your article will automatically display here. See en.wikipedia.org/wiki/WP:REFB for instructions on how to add citations. -->\n{{reflist}}\n\n[[Category:Numerical analysis]]\n[[Category:Computational science]]"
    },
    {
      "title": "Van Wijngaarden transformation",
      "url": "https://en.wikipedia.org/wiki/Van_Wijngaarden_transformation",
      "text": "In [[mathematics]] and [[numerical analysis]], in order to accelerate convergence of an [[alternating series]], [[Euler transform|Euler's transform]] can be computed as follows.\n\nCompute a row of partial sums :\n:<math>s_{0,k} = \\sum_{n=0}^k(-1)^n a_n </math>\nand form rows of averages between neighbors, \n:<math> \\, s_{j+1,k} = \\frac{s_{j,k}+s_{j,k+1}}2 </math>\nThe first column <math>\\scriptstyle s_{j,0}</math> then contains the partial sums of the Euler transform.\n\n[[Adriaan van Wijngaarden]]'s contribution was to point out that it is better not to carry this procedure through to the very end, but to stop two-thirds of the way.<ref>[[Adriaan van Wijngaarden|A. van Wijngaarden]], in:  Cursus:  Wetenschappelijk Rekenen B, Proces Analyse, Stichting Mathematisch Centrum, (Amsterdam, 1965) pp. 51-60</ref> If  <math>\\scriptstyle  a_0,a_1, \\ldots, a_{12} </math> are available, then <math> \\scriptstyle s_{8,4} </math> is almost always a better approximation to the sum than <math>\\scriptstyle  s\\, _{12,0}. </math>\n\n[[Leibniz formula for pi]], <math>\\scriptstyle 1 - \\frac 1 3 + \\frac  1 5 -  \\frac 1 7 + \\cdots = \\frac \\pi 4 = 0.7853981\\ldots </math>, gives the partial sum <math>\\scriptstyle  \\,s_{0,12} = 0.8046006... (+2.4\\%)</math>, the Euler transform partial sum <math>\\scriptstyle  \\,s_{12,0} = 0.7854002... (+2.6 \\times 10^{-6})</math> and the van Wijngaarden result <math>\\scriptstyle  \\,s_{8,4} = 0.7853982... (+4.7 \\times 10^{-8})</math> (relative errors are in round brackets).\n\n<small> \n 1.00000000 0.66666667 0.86666667 0.72380952 0.83492063 0.74401154 0.82093462 0.75426795 0.81309148 0.76045990 0.80807895 0.76460069 '''0.80460069'''\n 0.83333333 0.76666667 0.79523810 0.77936508 0.78946609 0.78247308 0.78760129 0.78367972 0.78677569 0.78426943 0.78633982 0.78460069 \n 0.80000000 0.78095238 0.78730159 0.78441558 0.78596959 0.78503719 0.78564050 0.78522771 0.78552256 0.78530463 0.78547026 \n 0.79047619 0.78412698 0.78585859 0.78519259 0.78550339 0.78533884 0.78543410 0.78537513 0.78541359 0.78538744 \n 0.78730159 0.78499278 0.78552559 0.78534799 0.78542111 0.78538647 0.78540462 0.78539436 0.78540052 \n 0.78614719 0.78525919 0.78543679 0.78538455 0.78540379 0.78539555 0.78539949 0.78539744 \n 0.78570319 0.78534799 0.78541067 0.78539417 0.78539967 0.78539752 0.78539847 \n 0.78552559 0.78537933 0.78540242 0.78539692 0.78539860 0.78539799 \n 0.78545246 0.78539087 0.78539967 0.78539776 '''0.78539829''' \n 0.78542166 0.78539527 0.78539871 0.78539803 \n 0.78540847 0.78539699 0.78539837 \n 0.78540273 0.78539768     \n '''0.78540021''' \n</small>                   \nThis table results from the [[J (programming language)|J]] formula  'b11.8'8!:2-:&(}:+}.)^:n+/\\(_1^n)*%1+2*n=.i.13 In many cases the diagonal terms do not converge in one cycle so process of averaging is to be repeated with diagonal terms by bringing them in a row. This will be needed in a geometric series with ratio -4. This process of successive averaging of the average of partial sum can be replaced by using formula to calculate the diagonal term.\n\n== References ==\n{{reflist}}\n\n== See also==\n[[Euler summation]]\n\n{{DEFAULTSORT:Van Wijngaarden Transformation}}\n[[Category:Mathematical series]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Vector field reconstruction",
      "url": "https://en.wikipedia.org/wiki/Vector_field_reconstruction",
      "text": "{{Multiple issues|\n{{original research|date=January 2008}}\n{{cleanup|date=May 2011}}\n{{essay|date=March 2014}}\n}}\n\n'''Vector field reconstruction'''<ref>[http://prola.aps.org/pdf/PRE/v51/i5/p4262_1 Global Vector Field Reconstruction from a Chaotic Experimental Signal in Copper Electrodissolution.] Letellier C, Le Sceller L , Maréchal E, Dutertre P, Maheu B, Gouesbet G, Fei Z, Hudson JL. Physical Review E, 1995 May;51(5):4262-4266</ref> is a method of creating a [[vector field]] from experimental or computer generated data, usually with the goal of finding a [[differential equation]] [[Mathematical model|model]] of the system.\n\nA [[differential equation]] [[Mathematical model|model]] is one that describes the value of [[dependent variables]] as they evolve in time or space by giving equations involving those variables and their [[derivative]]s with respect to some [[independent variables]], usually time and/or space. An [[ordinary differential equation]] is one in which the system's dependent variables are functions of only one independent variable. Many physical, chemical, biological and electrical systems are well described by ordinary differential equations. Frequently we assume a system is governed by differential equations, but we do not have exact knowledge of the influence of various factors on the state of the system. For instance, we may have an electrical circuit that in theory is described by a system of ordinary differential equations, but due to the tolerance of [[resistors]], variations of the supply [[voltage]] or interference from outside influences we do not know the exact [[parameters]] of the system. For some systems, especially those that support [[Chaos theory|chaos]], a small change in parameter values can cause a large change in the behavior of the system, so an accurate model is extremely important. Therefore, it may be necessary to construct more exact differential equations by building them up based on the actual system performance rather than a theoretical model. Ideally, one would measure all the dynamical variables involved over an extended period of time, using many different [[initial conditions]], then build or fine tune a differential equation model based on these measurements.\n\nIn some cases we may not even know enough about the processes involved in a system to even formulate a model. In other cases, we may have access to only one dynamical variable for our measurements, i.e., we have a scalar [[time series]]. If we only have a scalar time series, we need to use the method of time [[delay embedding]] or [[derivative coordinates]] to get a large enough set of dynamical variables to describe the system.\n\nIn a nutshell, once we have a set of measurements of the system state over some period of time, we find the derivatives of these measurements, which gives us a local vector field, then determine a global vector field consistent with this local field. This is usually done by a [[least squares]] fit to the derivative data.\n\n==Formulation==\n\nIn the best possible case, one has data streams of  measurements of all the system variables, equally spaced in time, say\n\n:s<sub>1</sub>(t), s<sub>2</sub>(t), ... , s<sub>k</sub>(t)\n\nfor\n\n: ''t'' = ''t''<sub>1</sub>, ''t''<sub>2</sub>,..., ''t''<sub>''n''</sub>,\n\nbeginning at several different initial conditions. Then the task of finding a vector field, and thus a differential equation model consists of fitting functions, for instance, a [[cubic spline]], to the data to obtain a set of continuous time functions\n\n:x<sub>1</sub>(t), x<sub>2</sub>(t), ... , x<sub>k</sub>(t),\n\ncomputing time derivatives dx<sub>1</sub>/dt, dx<sub>2</sub>/dt,...,dx<sub>k</sub>/dt of the functions, then making a [[least squares]] fit using some sort of orthogonal basis functions ([[orthogonal polynomials]], [[radial basis functions]], etc.) to each component of the tangent vectors to find a global vector field. A differential equation then can be read off the global vector field.\n\nThere are various methods of creating the basis functions for the least squares fit.  The most common method is the [[Gram–Schmidt process]]. Which creates a set of orthogonal basis vectors, which can then easily be normalized.  This method begins by first selecting any standard basis β={v<sub>1</sub>, v<sub>2</sub>,...,v<sub>n</sub>}.  Next, set the first vector v<sub>1</sub>=u<sub>1</sub>. Then, we set u<sub>2</sub>=v<sub>2</sub>-proj<sub>u<sub>1</sub></sub>v<sub>2</sub>.  This process is repeated to for k vectors, with the final vector being u<sub>k</sub>= v<sub>k</sub>-∑<sub>(j=1)</sub><sup>(k-1)</sup>proj<sub>u<sub>k</sub></sub>v<sub>k</sub>. This then creates a set of orthogonal standard basis vectors.\n\nThe reason for using a standard orthogonal basis rather than a standard basis arises from the creation of the least squares fitting done next.  Creating a least-squares fit begins by assuming some function, in the case of the reconstruction an n<sup>th</sup> degree polynomial, and fitting the curve to the data using constants.  The accuracy of the fit can be increased by increasing the degree of the polynomial being used to fit the data.  If a set of non-orthogonal standard basis functions was used, it becomes necessary to recalculate the constant coefficients of the function describing the fit.  However, by using the orthogonal set of basis functions, it is not necessary to recalculate the constant coefficients.\n\n==Applications==\n\nVector field reconstruction has several applications, and many different approaches.  Some mathematicians have not only used radial basis functions and polynomials to reconstruct a vector field, but they have used [[Lyapunov exponent]]s and [[singular value decomposition]].<ref>Global vector-field reconstruction of nonlinear dynamical system from a time series with SVD method and validation with Lyapunov exponent. Wei-Dong L, Ren F K, Meunier-Guttin-Cluzel S., Gouesbet G. Chin. Phys. Soc, 2003 December; Vol 12 No 12:1366-1373</ref>  Gouesbet and Letellier used a multivariate polynomial approximation and least squares to reconstruct their vector field. This method was applied to the [[Rössler system]], and the [[Lorenz system]], as well as [[thermal lens oscillations]].\n\nThe Rossler system, Lorenz system and Thermal lens oscillation follows the differential equations in standard system as\n\n:X'=Y, Y'=Z and Z'=F(X,Y,Z)\n\nwhere F(X,Y,Z) is known as the standard function.<ref>Global vector field reconstruction by using a multivariate polynomial ''L''<sub>2</sub> approximation on nets. Gouesbet G. and Letellier C. Physical Review E, 1994 June; Vol 49, No 6: 4955-4972</ref>\n\n==Implementation issues==\n\nIn some situation the model is not very efficient and difficulties can arise if the model has a large number of coefficients and demonstrates a divergent solution.  For example, nonautonomous differential equations give the previously described results.<ref>Constructing nonautonomous differential equations from experimental time series. Bezruchko B.P and Smirnov D.A. Physical Review E, 2000; Vol 63, 016207:1-7</ref> In this case the modification of the standard approach in application gives a better way of further development of global vector reconstruction.\n\nUsually the system being modeled in this way is a [[chaotic dynamical system]], because chaotic systems explore a large part of the [[phase space]] and the estimate of the global dynamics based on the local dynamics will be better than with a system exploring only a small part of the space.\n\nFrequently, one has only a single scalar time series measurement from a system known to have more than one [[Degrees of freedom (physics and chemistry)|degree of freedom]]. The time series may not even be from a system variable, but may be instead of a function of all the variables, such as temperature in a stirred tank reactor using several chemical species. In this case, one must use the technique of [[delay coordinate embedding]],<ref>Embedology, Tim Sauer, James A. Yorke, and Martin Casdagli, Santa Fe Institute working paper</ref> where a state vector consisting of the data at time t and several delayed versions of the data is constructed.\n\nA comprehensive review of the topic is available from <ref>G. Gouesbet, S. Meunier-Guttin-Cluzel and O. Ménard, editors. Chaos and its reconstruction. Novascience Publishers, New-York (2003)</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Vector calculus]]\n[[Category:Mathematical modeling]]\n[[Category:Numerical analysis]]\n[[Category:inverse problems]]"
    },
    {
      "title": "Von Neumann stability analysis",
      "url": "https://en.wikipedia.org/wiki/Von_Neumann_stability_analysis",
      "text": "In [[numerical analysis]], '''von Neumann stability analysis''' (also known as Fourier stability analysis) is a procedure used to check the [[Numerical stability|stability]] of [[finite difference scheme]]s as applied to linear [[partial differential equation]]s.<ref>[https://books.google.com/books?id=y77n2ySMJHUC&pg=PA523&dq=von+Neumann+stability+analysis#PPA523,M1 Analysis of Numerical Methods by E. Isaacson, H. B. Keller]</ref> The analysis is based on the [[Fourier decomposition]] of [[numerical error]] and was developed at [[Los Alamos National Laboratory]] after having been briefly described in a 1947 article by [[British people|British]] researchers [[John Crank|Crank]] and [[Phyllis Nicolson|Nicolson]].<ref> \n\n{{Citation\n | last = Crank | first = J. | last2 = Nicolson | first2 = P.\n | title = A Practical Method for Numerical Evaluation of Solutions of Partial Differential Equations of Heat Conduction Type\n | journal = Proc. Camb. Phil. Soc.\n | volume = 43 | year = 1947 | pages = 50&ndash;67\n | doi = 10.1007/BF02127704 \n}}</ref>\nThis method is an example of [[temporal discretization|explicit time integration]] where the function that defines governing equation is evaluated at the current time.\nLater, the method was given a more rigorous treatment in an article<ref>\n{{Citation \n | last = Charney | first = J. G. | last2 = Fjørtoft | first2 = R.\n | last3 = von Neumann | first3 = J.\n | title = Numerical Integration of the Barotropic Vorticity Equation\n | journal = Tellus | volume = 2 | year = 1950 | pages = 237&ndash;254\n | doi=10.1111/j.2153-3490.1950.tb00336.x\n}}</ref> co-authored by [[John von Neumann]].\n\n==Numerical stability==\nThe [[Numerical stability|stability of numerical schemes]] is closely associated with [[numerical error]]. A finite difference scheme is stable if the errors made at one time step of the calculation do not cause the errors to be magnified as the computations are continued. A ''neutrally stable scheme'' is one in which errors remain constant as the computations are carried forward. If the errors decay and eventually damp out, the numerical scheme is said to be stable. If, on the contrary, the errors grow with time the numerical scheme is said to be unstable. The stability of numerical schemes can be investigated by performing von Neumann stability analysis. For time-dependent problems, stability guarantees that the numerical method produces a bounded solution whenever the solution of the exact differential equation is bounded. Stability, in general, can be difficult to investigate, especially when the equation under consideration is [[nonlinear partial differential equation|nonlinear]].\n\nIn certain cases, von Neumann stability is necessary and sufficient for stability in the sense of Lax–Richtmyer (as used in the [[Lax equivalence theorem]]): The PDE and the finite difference scheme models are linear; the PDE is constant-coefficient with [[periodic boundary conditions]] and has only two independent variables; and the scheme uses no more than two time levels.<ref>\n{{Citation \n | last = Smith | first = G. D.\n | title = Numerical Solution of Partial Differential Equations: Finite Difference Methods, 3rd ed.\n | year = 1985\n | pages = 67&ndash;68\n}}</ref> Von Neumann stability is necessary in a much wider variety of cases. It is often used in place of a more detailed stability analysis to provide a good guess at the restrictions (if any) on the step sizes used in the scheme because of its relative simplicity.\n\n==Illustration of the method==\nThe von Neumann method is based on the decomposition of the errors into [[Fourier series]]. To illustrate the procedure, consider the one-dimensional [[heat equation]] \n:<math>\n  \\frac{\\partial u}{\\partial t} = \\alpha \\frac{\\partial^2 u}{\\partial x^2}\n</math>\ndefined on the spatial interval <math>L</math>, which can be discretized<ref>in this case, using the [[FTCS|FTCS discretization scheme]]</ref> as\n:<math>\n  \\quad (1) \\qquad u_j^{n + 1} = u_j^{n} + r \\left(u_{j + 1}^n - 2 u_j^n + u_{j - 1}^n \\right)\n</math>\nwhere\n:<math>r = \\frac{\\alpha\\, \\Delta t}{\\left( \\Delta x \\right)^2}</math>\nand the solution <math>u_j^{n}</math> of the discrete equation approximates the analytical solution <math>u(x,t)</math> of the PDE on the grid.\n\nDefine the [[round-off error]] <math>\\epsilon_j^n</math> as\n:<math>\n  \\epsilon_j^n = N_j^n - u_j^n\n</math>\nwhere <math>u_j^n</math> is the solution of the discretized equation (1) that would be computed in the absence of round-off error, and <math>N_j^n</math> is the numerical solution obtained in [[Floating point|finite precision arithmetic]]. Since the exact solution <math>u_j^n</math> must satisfy the discretized equation exactly, the error <math>\\epsilon_j^n</math> must also satisfy the discretized equation.<ref>{{cite book | title = Computational Fluid Dynamics: The Basics with Applications | author = Anderson, J. D., Jr. | authorlink=John D. Anderson|edition = | publisher = [[McGraw Hill]] | year = 1994 | isbn = }}</ref> Here we assumed that <math>N_j^n</math> satisfies the equation, too (this is only true in machine precision).\nThus\n:<math>\n  \\quad (2) \\qquad \\epsilon_j^{n + 1} = \\epsilon_j^n + r \\left(\\epsilon_{j + 1}^n - 2 \\epsilon_j^n + \\epsilon_{j - 1}^n \\right)\n</math>\nis a recurrence relation for the error. Equations (1) and (2) show that both the error and the numerical solution have the same growth or decay behavior with respect to time. For linear differential equations with periodic boundary condition, the spatial variation of error may be expanded in a finite Fourier series, in the interval <math>L</math>, as\n:<math>\n  \\quad (3) \\qquad \\epsilon(x) = \\sum_{m=1}^{M} A_m e^{ik_m x}\n</math>\nwhere the [[wavenumber]] <math>k_m = \\frac{\\pi m}{L}</math> with <math>m = 1,2,\\ldots,M</math> and <math>M = L/\\Delta x</math>. The time dependence of the error is included by assuming that the amplitude of error <math>A_m</math> is a function of time. Since the error tends to grow or decay exponentially with time, it is reasonable to assume that the amplitude varies exponentially with time; hence\n:<math>\n  \\quad (4) \\qquad \\epsilon(x,t) = \\sum_{m=1}^{M} e^{at} e^{ik_m x}\n</math>\nwhere <math>a</math> is a constant.\n\nSince the difference equation for error is linear (the behavior of each term of the series is the same as series itself), it is enough to consider the growth of error of a typical term:\n:<math>\n  \\quad (5) \\qquad \\epsilon_m(x,t) = e^{at} e^{ik_m x}\n</math>\nThe stability characteristics can be studied using just this form for the error with no loss in generality. To find out how error varies in steps of time, substitute equation (5) into equation (2), after noting that\n:<math>\n\\begin{align}\n \\epsilon_j^n & = e^{at} e^{ik_m x} \\\\\n \\epsilon_j^{n+1} & = e^{a(t+\\Delta t)} e^{ik_m x} \\\\\n \\epsilon_{j+1}^n & = e^{at} e^{ik_m (x+\\Delta x)} \\\\\n \\epsilon_{j-1}^n & = e^{at} e^{ik_m (x-\\Delta x)},\n\\end{align}\n</math>\nto yield (after simplification)\n:<math>\n  \\quad (6) \\qquad e^{a\\Delta t} = 1 + r \\left(e^{ik_m \\Delta x} + e^{-ik_m \\Delta x} -  2\\right).\n</math>\n\nUsing the identities\n:<math>\n  \\qquad \\sin\\left(\\frac{k_m \\Delta x}{2}\\right)= \\frac{e^{ik_m \\Delta x/2} - e^{-ik_m \\Delta x/2}}{2i} \\qquad \\rightarrow \\qquad \\sin^2\\left(\\frac{k_m \\Delta x}{2}\\right) = -\\frac{ e^{ik_m \\Delta x} + e^{-ik_m \\Delta x} -  2 }{4}\n</math>\nequation (6) may be written as\n:<math>\n  \\quad (7) \\qquad e^{a\\Delta t} = 1 - 4 r \\sin^2 (k_m \\Delta x/2)\n</math>\nDefine the amplification factor\n:<math>\n  G \\equiv \\frac{\\epsilon_j^{n+1}}{\\epsilon_j^n}\n</math>\nThe necessary and sufficient condition for the error to remain bounded is that <math>\\vert G \\vert \\leq 1.</math> \nHowever,\n:<math>\n  \\quad (8) \\qquad G = \\frac{e^{a(t+\\Delta t)} e^{ik_m x}}{e^{at} e^{ik_m x}} = e^{a\\Delta t}\n</math>\nThus, from equations (7) and (8), the condition for stability is given by\n:<math>\n  \\quad (9) \\qquad \\left\\vert 1 - 4 r \\sin^2 (k_m \\Delta x/2) \\right\\vert \\leq 1\n</math>\nNote that the term <math>4 r \\sin^2 (k_m \\Delta x/2)</math> is always positive.  Thus, to satisfy Equation (9):\n:<math>\n  \\quad (10) \\qquad  4 r \\sin^2 (k_m \\Delta x/2) \\leq 2\n</math>\nFor the above condition to hold for all <math>m</math>  (and therefore all <math>\\sin^2 (k_m \\Delta x/2)</math>), we have\n:<math>\n  \\quad (11) \\qquad r=\\frac{\\alpha \\Delta t}{\\left( \\Delta x \\right)^2} \\leq \\frac{1}{2}  \n</math>\nEquation (11) gives the stability requirement for the [[FTCS scheme]] as applied to one-dimensional heat equation. It says that for a given <math>\\Delta x</math>, the allowed value of <math>\\Delta t</math> must be small enough to satisfy equation (10).\n\n==References==\n{{reflist}}\n\n[[Category:Numerical analysis]]\n[[Category:Fourier analysis]]"
    },
    {
      "title": "Weakened weak form",
      "url": "https://en.wikipedia.org/wiki/Weakened_weak_form",
      "text": "{{POV|date=May 2016}}\n\n'''Weakened weak form'''  (or '''W2 form''')<ref>G.R. Liu. \"A G space theory and a weakened weak (W2) form for a unified formulation of compatible and incompatible methods: Part I theory and Part II applications to solid mechanics problems\". ''International Journal for Numerical Methods in Engineering'', 81: 1093–1126, 2010</ref> is used in the formulation of general numerical methods based on  [[meshfree methods]] and/or [[finite element method]] settings. These numerical methods are applicable to [[solid mechanics]] as well as [[fluid dynamics]] problems.\n\n==Description==\nFor simplicity we choose elasticity problems (2nd order PDE) for our discussion.<ref name=\"auto\">Liu, G.R. 2nd edn: 2009 ''Mesh Free Methods'',  CRC Press.  978-1-4200-8209-9</ref>  Our discussion is also most convenient in reference to the well-known [[weak formulation|weak and strong form]]. In a strong formulation for an approximate solution, we need to assume displacement functions that are 2nd order differentiable. In a weak formulation, we create linear and bilinear forms and then search for a particular function (an approximate solution) that satisfy the weak statement.  The bilinear form uses gradient of the functions that has only 1st order differentiation. Therefore, the requirement on the continuity of assumed displacement functions is weaker than in the strong formulation.  In a discrete form (such as the [[Finite element method]], or FEM), a sufficient requirement for an assumed displacement function is piecewise continuous over the entire problems domain. This allows us to construct the function using elements (but making sure it is continuous a long all element interfaces), leading to the powerful FEM.\n\nNow, in a weakened weak (W2) formulation, we further reduce the requirement. We form a bilinear form using only the assumed function (not even the gradient).  This is done by using the so-called generalized gradient smoothing technique,<ref>Liu GR, \"A Generalized Gradient Smoothing Technique and the Smoothed Bilinear Form for Galerkin Formulation of a Wide Class of Computational Methods\", ''[[International Journal of Computational Methods]]''   Vol.5   Issue: 2, 199–236, 2008</ref> with which one can approximate the gradient of displacement functions for certain class of discontinuous functions, as long as they are in a proper [[G space]].<ref>Liu GR, \"On G Space Theory\", ''International Journal of Computational Methods'', Vol. 6 Issue: 2, 257–289, 2009</ref>  Since we do not have to actually perform even the 1st differentiation to the assumed displacement functions, the requirement on the consistence of the functions are further reduced, and hence the weakened weak or W2 formulation.\n\n==History ==\nThe development of systematic theory of the weakened weak form started from the works on meshfree methods.<ref name=\"auto\"/> It is relatively new, but had very rapid development in the past few years.{{when|date=March 2014}}\n\n==Features of W2 formulations ==\n\n#The W2 formulation offers possibilities for formulate various (uniformly) \"soft\" models that works well with triangular meshes. Because triangular mesh can be generated automatically, it becomes much easier in re-meshing and hence automation in modeling and simulation.  This is very important for our long-term goal of development of fully automated computational methods.\n#In addition, W2 models can be made soft enough (in uniform fashion) to produce upper bound solutions (for force-driving problems). Together with stiff models (such as the fully compatible FEM models), one can conveniently bound the solution from both sides.  This allows easy error estimation for generally complicated problems, as long as a triangular mesh can be generated. This is important for producing so-called certified solutions.\n#W2 models can be built free from volumetric locking, and possibly free from other types of locking phenomena.\n#W2 models provide the freedom to assume separately the displacement gradient of the displacement functions, offering opportunities for ultra-accurate and super-convergent models.  It may be possible to construct linear models with energy convergence rate of 2.\n#W2 models are often found less sensitive to mesh distortion.\n#W2 models are found effective for low order methods\n\n==Existing W2 models ==\n\nTypical W2 models are the smoothed point interpolation methods (or S-PIM).<ref>Liu, G.R. 2nd edn: 2009 ''Mesh Free Methods'', CRC Press.  978-1-4200-8209-9</ref> The S-PIM can be node-based (known as NS-PIM or LC-PIM),<ref>Liu GR, Zhang GY, Dai KY, Wang YY, Zhong ZH, Li GY and Han X, \"A linearly conforming point interpolation method (LC-PIM) for 2D solid mechanics problems\", ''International Journal of Computational Methods'', 2(4): 645–665, 2005.</ref> edge-based (ES-PIM),<ref>G.R. Liu, G.R. Zhang. \"Edge-based Smoothed Point Interpolation Methods\". ''International Journal of Computational Methods'', 5(4): 621–646, 2008</ref> and cell-based (CS-PIM).<ref>G.R. Liu, G.R. Zhang. \"A normed G space and weakened weak (W2) formulation of a cell-based Smoothed Point Interpolation Method\". ''International Journal of Computational Methods'', 6(1): 147–179, 2009</ref> The NS-PIM was developed using the so-called SCNI technique.<ref>Chen, J. S., Wu, C. T., Yoon, S. and You, Y. (2001). \"A stabilized conforming nodal integration for Galerkin mesh-free methods\". ''International Journal for Numerical Methods in Engineering''. 50: 435–466.</ref> It was then discovered that NS-PIM is capable of producing upper bound solution and volumetric locking free.<ref>G. R. Liu and G. Y. Zhang. Upper bound solution to elasticity problems: A unique property of the linearly conforming point interpolation method (LC-PIM). ''International Journal for Numerical Methods in Engineering'', 74: 1128–1161, 2008.</ref> The ES-PIM is found superior in accuracy, and CS-PIM behaves in between the NS-PIM and ES-PIM. Moreover, W2 formulations allow the use of polynomial and radial basis functions in the creation of shape functions (it accommodates the discontinuous displacement functions, as long as it is in G1 space), which opens further rooms for future developments.  \nThe S-FEM is largely the linear version of S-PIM, but with most of the properties of the S-PIM and much simpler.  It has also variations of NS-FEM, ES-FEM and CS-FEM.  The major property of S-PIM can be found also in S-FEM.<ref>Zhang ZQ, Liu GR, \"Upper and lower bounds for natural frequencies: A property of the smoothed finite element methods\", ''International Journal for Numerical Methods in Engineering''   Vol. 84   Issue: 2, 149–178, 2010</ref> The S-FEM models are: \n* [[Node-based Smoothed FEM]] (NS-FEM)<ref>Liu GR, Nguyen-Thoi T, Nguyen-Xuan H, Lam KY (2009) \"A node-based smoothed finite element method (NS-FEM) for upper bound solutions to solid mechanics problems\". ''Computers and Structures''; 87: 14–26.</ref>\n* [[Edge-based Smoothed FEM]] (NS-FEM)<ref>Liu GR, Nguyen-Thoi T, Lam KY (2009) \"An edge-based smoothed finite element method (ES-FEM) for static, free and forced vibration analyses in solids\". ''Journal of Sound and Vibration''; 320: 1100–1130.</ref>  \n* [[Face-based Smoothed FEM]] (NS-FEM)<ref>Nguyen-Thoi T, Liu GR, Lam KY, GY Zhang (2009) \"A Face-based Smoothed Finite Element Method (FS-FEM) for 3D linear and nonlinear solid mechanics problems using 4-node tetrahedral elements\". ''International Journal for Numerical Methods in Engineering''; 78: 324–353</ref>\n* [[Cell-based Smoothed FEM]] (NS-FEM)<ref>Liu GR, Dai KY, Nguyen-Thoi T (2007) \"A smoothed finite element method for mechanics problems\". ''Computational Mechanics''; 39: 859–877</ref><ref>Dai KY, Liu GR (2007) \"Free and forced vibration analysis using the smoothed finite element method (SFEM)\". ''Journal of Sound and Vibration''; 301: 803–820.</ref><ref>Dai KY, Liu GR, Nguyen-Thoi T (2007) \"An n-sided polygonal smoothed finite element method (nSFEM) for solid mechanics\". ''Finite Elements in Analysis and Design''; 43: 847－860.</ref> \n* [[Edge/node-based Smoothed FEM]] (NS/ES-FEM)<ref name=\"auto1\">Li Y, Liu GR, Zhang GY, \"An adaptive NS/ES-FEM approach for 2D contact problems using triangular elements\", ''Finite Elements in Analysis and Design''   Vol.47   Issue: 3, 256–275, 2011</ref> \n* [[Alpha FEM]] method (Alpha FEM)<ref>Liu GR, Nguyen-Thoi T, Lam KY (2009) \"A novel FEM by scaling the gradient of strains with factor α (αFEM)\". ''Computational Mechanics''; 43: 369–391</ref><ref>Liu GR, Nguyen-Xuan H, Nguyen-Thoi T, Xu X (2009) \"A novel weak form and a superconvergent alpha finite element method (SαFEM) for mechanics problems using triangular meshes\". ''Journal of Computational Physics''; 228: 4055–4087</ref>\n* [[Beta FEM]] method (Beta FEM)<ref>Zeng W, Liu GR, Li D, Dong XW (2016) A smoothing technique based beta finite element method (βFEM) for crystal plasticity modeling. Computers and Structures; 162: 48-67</ref>\n\n==Applications==\n\nSome of the applications of W2 models are:\n\n#Mechanics for solids, structures and piezoelectrics;<ref>Cui XY, Liu GR, Li GY, et al. A thin plate formulation without rotation DOFs based on the radial point interpolation method and triangular cells, ''International Journal for Numerical Methods in Engineering''   Vol.85   Issue: 8 , 958–986, 2011</ref><ref>Liu GR, Nguyen-Xuan H, Nguyen-Thoi T, A theoretical study on the smoothed FEM (S-FEM) models: Properties, accuracy and convergence rates, ''International Journal for Numerical Methods in Engineering''   Vol. 84   Issue: 10, 1222–1256, 2010</ref>\n#Fracture mechanics and crack propagation;<ref>Liu GR, Nourbakhshnia N, Zhang YW, A novel singular ES-FEM method for simulating singular stress fields near the crack tips for linear fracture problems, ''Engineering Fracture Mechanics''   Vol.78   Issue: 6   Pages: 863–876, 2011</ref><ref>Liu GR, Chen L, Nguyen-Thoi T, et al. A novel singular node-based smoothed finite element method (NS-FEM) for upper bound solutions of fracture problems, ''International Journal for Numerical Methods in Engineering''   Vol.83   Issue: 11, 1466–1497, 2010</ref><ref>Liu GR, Nourbakhshnia N, Chen L, et al. \"A Novel General Formulation for Singular Stress Field Using the Es-Fem Method for the Analysis of Mixed-Mode Cracks\", ''International Journal of Computational Methods''   Vol. 7   Issue: 1, 191–214, 2010</ref><ref>Zeng W, Liu GR, Jiang C, Dong XW, Chen HD, Bao Y, Jiang Y. \"An effective fracture analysis method based on the virtual crack closure-integral technique implemented in CS-FEM\", ''Applied Mathematical Modelling''   Vol. 40,   Issue: 5-6, 3783-3800, 2016</ref>\n#Heat transfer;<ref>Zhang ZB, Wu SC, Liu GR, et al. \"Nonlinear Transient Heat Transfer Problems using the Meshfree ES-PIM\", ''International Journal of Nonlinear Sciences and Numerical Simulation''   Vol.11   Issue: 12, 1077–1091, 2010</ref><ref>Wu SC, Liu GR, Cui XY, et al. \"An edge-based smoothed point interpolation method (ES-PIM) for heat transfer analysis of rapid manufacturing system\", ''International Journal of Heat and Mass Transfer''   Vol.53   Issue: 9-10, 1938–1950,   2010</ref>\n#Structural acoustics;<ref>He ZC, Cheng AG, Zhang GY, et al. \"Dispersion error reduction for acoustic problems using the edge-based smoothed finite element method (ES-FEM)\", ''International Journal for Numerical Methods in Engineering''   Vol. 86   Issue: 11   Pages: 1322–1338, 2011</ref><ref>He ZC, Liu GR, Zhong ZH, et al. \"A coupled ES-FEM/BEM method for fluid-structure interaction problems\", ''Engineering Analysis With Boundary Elements''   Vol. 35   Issue: 1, 140–147, 2011</ref><ref>Zhang ZQ, Liu GR, \"Upper and lower bounds for natural frequencies: A property of the smoothed finite element methods\", ''International Journal for Numerical Methods in Engineering''   Vol.84   Issue: 2, 149–178, 2010</ref>\n# Nonlinear and contact problems;<ref>Zhang ZQ, Liu GR, \"An edge-based smoothed finite element method (ES-FEM) using 3-node triangular elements for 3D non-linear analysis of spatial membrane structures\", ''International Journal for Numerical Methods in Engineering'',  Vol. 86   Issue: 2    135–154, 2011</ref><ref>Jiang C, Liu GR, Han X, Zhang ZQ, Zeng W, A smoothed finite element method for analysis of anisotropic large deformation of passive rabbit ventricles in diastole, ''International Journal for Numerical Methods in Biomedical Engineering'',  Vol. 31   Issue: 1,1-25, 2015</ref>\n# Stochastic analysis;<ref>Liu GR, Zeng W, Nguyen-Xuan H. Generalized stochastic cell-based smoothed finite element method (GS_CS-FEM) for solid mechanics, ''Finite Elements in Analysis and Design''   Vol.63, 51-61, 2013</ref>\n# Adaptive Analysis;<ref>Nguyen-Thoi T, Liu GR, Nguyen-Xuan H, et al. \"Adaptive analysis using the node-based smoothed finite element method (NS-FEM)\", ''International Journal for Numerical Methods in Biomedical Engineering''   Vol. 27   Issue: 2, 198–218, 2011</ref><ref name=\"auto1\"/>\n# Phase change problem;<ref>Li E, Liu GR, Tan V, et al. \"An efficient algorithm for phase change problem in tumor treatment using alpha FEM\", ''International Journal of Thermal Sciences''   Vol.49   Issue: 10, 1954–1967, 2010</ref>\n#Crystal plasticity modeling.<ref>Zeng W, Larsen JM, Liu GR. Smoothing technique based crystal plasticity finite element modeling of crystalline materials, ''International Journal of Plasticity''   Vol.65, 250-268, 2015</ref>\n# Limited analysis.<ref>Tran TN, Liu GR, Nguyen-Xuan H, et al. \"An edge-based smoothed finite element method for primal-dual shakedown analysis of structures\", ''International Journal for Numerical Methods in Engineering''   Vol.82   Issue: 7, 917–938, 2010</ref>\n\n==See also==\n* [[Finite element method]]\n* [[Meshfree methods]]\n* [[Smoothed finite element method]]\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://www.nus.edu.sg/ACES]\n\n{{Numerical PDE}}\n\n[[Category:Numerical analysis]]\n[[Category:Numerical differential equations]]\n[[Category:Computational fluid dynamics]]\n\n[[ja:メッシュフリー法]]"
    },
    {
      "title": "Well-posed problem",
      "url": "https://en.wikipedia.org/wiki/Well-posed_problem",
      "text": "The [[mathematics|mathematical]] term '''well-posed problem''' stems from a definition given by [[Jacques Hadamard]]. He believed that mathematical models of physical phenomena should have the properties that:\n# a solution exists,\n# the solution is unique,\n# the solution's behavior changes continuously with the initial conditions.\n\nExamples of [[archetypal]] well-posed problems include the [[Laplace's_equation#Boundary_conditions|Dirichlet problem for Laplace's equation]], and the [[heat equation]] with specified initial conditions. These might be regarded as 'natural' problems in that there are physical processes modelled by these problems.\n\nProblems that are not well-posed in the sense of Hadamard are termed '''ill-posed'''. [[Inverse problem]]s are often ill-posed. For example, the inverse heat equation, deducing a previous distribution of temperature from final data, is not well-posed in that the solution is highly sensitive to changes in the final data.\n\nContinuum models must often be [[discretization|discretized]] in order to obtain a numerical solution. While solutions may be continuous with respect to the initial conditions, they may suffer from [[numerical instability]] when solved with finite precision, or with errors in the data. Even if a problem is well-posed, it may still be '''ill-conditioned''', meaning that a small error in the initial data can result in much larger errors in the answers. Problems in nonlinear [[complex systems]] (so called chaotic systems) provide well-known examples of instability. An ill-conditioned problem is indicated by a large [[condition number]].\n\nIf the problem is well-posed, then it stands a good chance of solution on a computer using a [[numerical stability|stable algorithm]]. If it is not well-posed, it needs to be re-formulated for numerical treatment. Typically this involves including additional assumptions, such as smoothness of solution. This process is known as ''[[Regularization (mathematics)|regularization]]''. [[Tikhonov regularization]] is one of the most commonly used for regularization of linear ill-posed problems.\n\n== See also ==\n*[[Total absorption spectroscopy]] – an example of an inverse problem or ill-posed problem in a real-life situation that is solved by means of the [[expectation–maximization algorithm]]\n\n== References ==\n*{{Cite book |first=Jacques |last=Hadamard |year=1902 |title=Sur les problèmes aux dérivées partielles et leur signification physique |work=Princeton University Bulletin |pages=49–52 }}\n*{{Cite book |title=McGraw-Hill Dictionary of Scientific and Technical Terms |edition=4th |origyear=1974 |year=1989 |editor-first=Sybil B. |editor-last=Parker |publisher=McGraw-Hill |location=New York |isbn=0-07-045270-9 }}\n*{{Cite book |first=A. N. |last=Tikhonov |first2=V. Y. |last2=Arsenin |title=Solutions of ill-Posed Problems |publisher=Winston |location=New York |year=1977 |isbn=0-470-99124-0 }}\n\n{{Authority control}}\n[[Category:Numerical analysis]]\n[[Category:Partial differential equations]]"
    },
    {
      "title": "Wilkinson's polynomial",
      "url": "https://en.wikipedia.org/wiki/Wilkinson%27s_polynomial",
      "text": "{{multiple image\n|align=right\n|direction=vertical\n|width=300\n|image1=Wilkinson polynomial.svg\n|caption1=Plot of Wilkinson's polynomial\n|image2=log Wilkinson polynomial.svg\n|caption2=Plot of sgn(''w''(''x'')) log(1 + &#x7c;''w''(''x'')&#x7c;)\n}}\n\nIn [[numerical analysis]], '''Wilkinson's polynomial''' is a specific [[polynomial]] which was used by [[James H. Wilkinson]] in 1963 to illustrate a difficulty when [[root-finding algorithm|finding the root]] of a polynomial: the location of the roots can be very sensitive to perturbations in the coefficients of the polynomial.\n\nThe polynomial is\n:<math> w(x) = \\prod_{i=1}^{20} (x - i) = (x-1)(x-2) \\cdots (x-20). </math>\nSometimes, the term ''Wilkinson's polynomial'' is also used to refer to some other polynomials appearing in Wilkinson's discussion.\n\n==Background==\nWilkinson's polynomial arose in the study of algorithms for finding the roots of a polynomial\n:<math> p(x) = \\sum_{i=0}^n c_i x^i. </math>\nIt is a natural question in numerical analysis to ask whether the problem of finding the roots of ''p'' from the coefficients ''c''<sub>''i''</sub> is [[condition number|well-conditioned]].  That is, we hope that a small change in the coefficients will lead to a small change in the roots.  Unfortunately, this is not the case here.\n\nThe problem is ill-conditioned when the polynomial has a multiple root. For instance, the polynomial ''x''<sup>2</sup> has a double root at&nbsp;''x''&nbsp;=&nbsp;0. However, the polynomial ''x''<sup>2</sup>&nbsp;&minus;&nbsp;''ε'' (a perturbation of size&nbsp;''ε'') has roots at ±√''ε'', which is much bigger than ''ε'' when ''ε'' is small.\n\nIt is therefore natural to expect that ill-conditioning also occurs when the polynomial has zeros which are very close. However, the problem may also be extremely ill-conditioned for polynomials with well-separated zeros. Wilkinson used the polynomial ''w''(''x'') to illustrate this point (Wilkinson 1963).\n\nIn 1984, he described the personal impact of this discovery:\n\n:''Speaking for myself I regard it as the most traumatic experience in my career as a numerical analyst.''<ref>\n{{Cite book|last=Wilkinson |first=James H. |authorlink=James H. Wilkinson |editor=Gene H. Golub |title= Studies in Numerical Analysis |year=1984 |publisher=Mathematical Association of America |isbn=978-0-88385-126-5 |pages=3 |chapter=The perfidious polynomial}}</ref>\n\nWilkinson's polynomial is often used to illustrate the undesirability of naively computing [[eigenvalue]]s of a matrix by first calculating the coefficients of the matrix's [[characteristic polynomial]] and then finding its roots, since using the coefficients as an intermediate step may introduce an extreme ill-conditioning even if the original problem was well conditioned.<ref name=TrefethenBau>{{Citation|first1=Lloyd N. |last1=Trefethen |first2= David|last2= Bau|title=Numerical Linear Algebra|publisher=SIAM|year=1997}}</ref>\n\n==Conditioning of Wilkinson's polynomial==\nWilkinson's polynomial\n\n:<math> w(x) = \\prod_{i=1}^{20} (x - i) = (x-1)(x-2) \\cdots (x-20) </math>\n\nclearly has 20 roots, located at ''x'' = 1, 2, ..., 20. These roots are far apart. However, the polynomial is still very ill-conditioned.\n\nExpanding the polynomial, one finds\n\n: <math>\n\\begin{align}\nw(x) = {} & x^{20}-210 x^{19}+20615 x^{18}-1256850x^{17}+53327946 x^{16} \\\\\n& {}-1672280820x^{15}+40171771630 x^{14}-756111184500x^{13} \\\\\n& {}+11310276995381x^{12}-135585182899530x^{11} \\\\\n& {}+1307535010540395x^{10}-10142299865511450x^9 \\\\\n& {}+63030812099294896x^8-311333643161390640x^7 \\\\\n& {}+1206647803780373360x^6-3599979517947607200x^5 \\\\\n& {}+8037811822645051776x^4-12870931245150988800x^3 \\\\\n& {}+13803759753640704000x^2-8752948036761600000x \\\\ \n& {}+2432902008176640000.\n\\end{align}\n</math>\n\nIf the coefficient of ''x''<sup>19</sup> is decreased from −210 by 2<sup>&minus;23</sup> to −210.0000001192, then the polynomial value ''w''(20) decreases from 0 to &minus;2<sup>&minus;23</sup>20<sup>19</sup>&nbsp;= &minus;6.25&times;10<sup>17</sup>, and the root at ''x''&nbsp;= 20 grows to ''x''&nbsp;≈ 20.8 . The roots at ''x''&nbsp;= 18 and ''x''&nbsp;= 19 collide into a double root at  ''x'' ≈ 18.62 which turns into a pair of complex conjugate roots at ''x'' ≈ 19.5&nbsp;±&nbsp;1.9''i'' as the perturbation increases further. The 20 roots become (to 5 decimals)\n\n: <math>\n\\begin{array}{rrrrr}\n1.00000 & 2.00000 & 3.00000 & 4.00000 & 5.00000 \\\\[8pt]\n6.00001 & 6.99970 & 8.00727 & 8.91725 & 20.84691 \\\\[8pt]\n10.09527\\pm {} & 11.79363 \\pm {} & 13.99236\\pm{} & 16.73074\\pm{} & 19.50244 \\pm {} \\\\[-3pt]\n0.64350i & 1.65233i & 2.51883i & 2.81262i & 1.94033i\n\\end{array}\n</math>\n\nSome of the roots are greatly displaced, even though the change to the coefficient is tiny and the original roots seem widely spaced. Wilkinson showed by the stability analysis discussed in the next section that this behavior is related to the fact that some roots ''α'' (such as ''α''&nbsp;= 15) have many roots ''β'' that are \"close\" in the sense that |''α''&nbsp;−&nbsp;''β''| is smaller than&nbsp;|''α''|.\n\nWilkinson chose the perturbation of 2<sup>&minus;23</sup> because his [[Pilot ACE]] computer had 30-bit [[floating point]] [[significand]]s, so for numbers around 210, 2<sup>&minus;23</sup> was an error in the first bit position not represented in the computer. The two real numbers, &minus;210 and &minus;210 &minus; 2<sup>&minus;23</sup>, are represented by the same floating point number, which means that 2<sup>&minus;23</sup> is the ''unavoidable'' error in representing a real coefficient close to &minus;210 by a floating point number on that computer. The perturbation analysis shows that 30-bit coefficient [[precision (arithmetic)|precision]] is insufficient for separating the roots of Wilkinson's polynomial.\n\n==Stability analysis==\nSuppose that we perturb a polynomial ''p''(''x'')&nbsp;= Π (''x''&nbsp;&minus;&nbsp;''α''<sub>''j''</sub>)\nwith roots α<sub>''j''</sub> by adding a small multiple ''t''·''c''(''x'') of a polynomial ''c''(''x''), and ask how this affects the roots α<sub>''j''</sub>. To first order, the change in the roots will be controlled by the derivative\n:<math>{d\\alpha_j \\over dt} = -{c(\\alpha_j)\\over p^\\prime(\\alpha_j)}. </math>\nWhen the derivative is large, the roots will be less stable under variations of ''t'', and conversely if this derivative is small the roots will be stable. In particular,\nif α<sub>''j''</sub> is a multiple root, then the denominator vanishes. In this case, α<sub>''j''</sub> is usually not differentiable with respect to ''t'' (unless ''c'' happens to vanish there), and the roots will be extremely unstable.\n\nFor small values of ''t'' the perturbed root is given by the power series expansion in ''t''\n\n:<math> \\alpha_j + {d\\alpha_j \\over dt}t +{d^2\\alpha_j \\over dt^2}{t^2\\over 2!} + \\cdots </math>\n\nand one expects problems when |''t''| is larger than the radius of convergence of this power series, which is given by the smallest value of |''t''| such that the root α<sub>''j''</sub> becomes multiple. A very crude estimate for this radius takes half the distance from α<sub>''j''</sub> to the nearest root, and divides by the derivative above.\n\nIn the example of Wilkinson's polynomial of degree 20, the roots are given by α<sub>''j''</sub>&nbsp;= ''j'' for ''j''&nbsp;=&nbsp;1,&nbsp;...,&nbsp;20, and ''c''(''x'') is equal to ''x''<sup>19</sup>.\nSo the derivative is given by\n:<math>{d\\alpha_j \\over dt} = -{\\alpha_j^{19}\\over \\prod_{k\\ne j}(\\alpha_j-\\alpha_k)} = -\\prod_{k\\ne j}{\\alpha_j\\over \\alpha_j-\\alpha_k} . \\,\\!</math>\nThis shows that the root α<sub>''j''</sub> will be less stable if there are many roots\nα<sub>''k''</sub> close to α<sub>''j''</sub>, in the sense that the distance\n|α<sub>''j''</sub>&nbsp;&minus;&nbsp;α<sub>''k''</sub>| between them is smaller than |α<sub>''j''</sub>|.\n\n'''Example'''. For the root α<sub>1</sub>&nbsp;= 1, the derivative is equal to\n1/19! which is very small; this root is stable even for large changes in ''t''. This is because all the other roots ''β'' are a long way from it, in the sense that |''α''<sub>1</sub>&nbsp;&minus;&nbsp;''β''| = 1, 2, 3, ..., 19 is larger than |''α''<sub>1</sub>|&nbsp;= 1.\nFor example, even if ''t'' is as large as –10000000000, the root ''α''<sub>1</sub> only changes from 1 to about  0.99999991779380 (which is very close to the first order approximation 1&nbsp;+&nbsp;''t''/19! ≈ 0.99999991779365). Similarly, the other small roots of Wilkinson's polynomial are insensitive to changes in&nbsp;''t''.\n\n'''Example'''. On the other hand, for the root ''α''<sub>20</sub>&nbsp;= 20, the derivative is equal to &minus;20<sup>19</sup>/19!  which is huge (about 43000000), so this root is very sensitive to small changes in ''t''. The other roots ''β'' are close to ''α''<sub>20</sub>, in the sense that |''β''&nbsp;&minus;&nbsp;''α''<sub>20</sub>| = 1, 2, 3, ..., 19 is less than |''α''<sub>20</sub>| = 20. For ''t'' = &minus;2<sup>&nbsp;&minus;&nbsp;23</sup> the first-order approximation 20&nbsp;&minus;&nbsp;''t''·20<sup>19</sup>/19! = 25.137... to the perturbed root 20.84... is terrible; this is even more obvious for the root ''α''<sub>19</sub> where the perturbed root has a large imaginary part but the first-order approximation (and for that matter all higher-order approximations) are real. The reason for this discrepancy is that |''t''|  ≈ 0.000000119 is greater than the radius of convergence of the power series mentioned above (which is about 0.0000000029, somewhat smaller than the value  0.00000001 given by the crude estimate) so the linearized theory does not apply. For a value such as ''t'' = 0.000000001 that is significantly smaller than this radius of convergence, the first-order approximation  19.9569... is reasonably close to the root  19.9509...\n\nAt first sight the roots ''α''<sub>1</sub> = 1 and ''α''<sub>20</sub> = 20 of Wilkinson's polynomial appear to be similar, as they are on opposite ends of a symmetric line of roots, and have the same set of distances 1, 2, 3, ..., 19 from other roots. However the analysis above shows that this is grossly misleading: the root ''α''<sub>20</sub> = 20 is less stable than ''α''<sub>1</sub> = 1  (to small perturbations in the coefficient of ''x''<sup>19</sup>) by a factor of 20<sup>19</sup> = 5242880000000000000000000.\n\n==Wilkinson's second example==\nThe second example considered by Wilkinson is\n:<math> w_2(x) = \\prod_{i=1}^{20} (x - 2^{-i}) = (x-2^{-1})(x-2^{-2}) \\cdots (x-2^{-20}). </math>\nThe twenty zeros of this polynomial are in a geometric progression with common ratio 2, and hence the quotient\n:<math> \\alpha_j\\over \\alpha_j-\\alpha_k </math>\ncannot be large. Indeed, the zeros of ''w''<sub>2</sub> are quite stable to large ''relative'' changes in the coefficients.\n\n==The effect of the basis==\nThe expansion\n:<math> p(x) = \\sum_{i=0}^n c_i x^i </math>\nexpresses the polynomial in a particular basis, namely that of the monomials. If the polynomial is expressed in another basis, then the problem of finding its roots may cease to be ill-conditioned. For example, in a [[Lagrange polynomial|Lagrange form]], a small change in one (or several) coefficients need not change the roots too much.  Indeed, the basis polynomials for interpolation at the points 0, 1, 2, ..., 20 are\n\n:<math> \\ell_k(x) = \\prod_{i \\in \\{0,\\ldots,20\\} \\setminus \\{k\\}} \\frac{x - i}{k - i}, \\qquad\\text{for}\\quad  k=0,\\ldots,20. </math>\n\nEvery polynomial (of degree 20 or less) can be expressed in this basis:\n\n: <math> p(x) = \\sum_{i=0}^{20} d_i \\ell_i(x). </math>\n\nFor Wilkinson's polynomial, we find\n\n: <math> w(x) = (20!) \\ell_0(x) = \\sum_{i=0}^{20} d_i \\ell_i(x) \\quad\\text{with}\\quad d_0=(20!) ,\\, d_1=d_2= \\cdots =d_{20}=0. </math>\n\nGiven the definition of the Lagrange basis polynomial ℓ<sub>0</sub>(''x''), a change in the coefficient ''d''<sub>0</sub> will produce no change in the roots of ''w''.  However, a perturbation in the other coefficients (all equal to zero) will slightly change the roots. Therefore, Wilkinson's polynomial is well-conditioned in this basis.\n\n==Notes==\n<references/>\n\n==References==\nWilkinson discussed \"his\" polynomial in\n* J. H. Wilkinson (1959). The evaluation of the zeros of ill-conditioned polynomials. Part I. ''Numerische Mathematik'' '''1''':150–166.\n* J. H. Wilkinson (1963). ''Rounding Errors in Algebraic Processes''.  Englewood Cliffs, New Jersey: Prentice Hall.\nIt is mentioned in standard text books in numerical analysis, like\n*F. S. Acton, ''Numerical methods that work'', {{ISBN|978-0-88385-450-1}}, page 201.\nOther references:\n* Ronald G. Mosier (July 1986). Root neighborhoods of a polynomial. ''Mathematics of Computation'' '''47'''(175):265–273.\n* J. H. Wilkinson (1984). [http://www.maa.org/sites/default/files/pdf/upload_library/22/Chauvenet/Wilkinson.pdf The perfidious polynomial.] ''Studies in Numerical Analysis'', ed. by G. H. Golub, pp.&nbsp;1–28. (Studies in Mathematics, vol. 24). Washington, D.C.: Mathematical Association of America.\nA high-precision numerical computation is presented in:\n* Ray Buvel, [http://calcrpnpy.sourceforge.net/ratfunManual.html Polynomials And Rational Functions], part of the ''RPN Calculator User Manual'' (for Python), retrieved on 29 July 2006.\n{{Use dmy dates|date=September 2010}}\n\n{{DEFAULTSORT:Wilkinson's Polynomial}}\n[[Category:Numerical analysis]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Numerical dispersion",
      "url": "https://en.wikipedia.org/wiki/Numerical_dispersion",
      "text": "In applied computational mathematics, '''numerical dispersion''' is a difficulty with computer simulations of continua (such as fluids) wherein the simulated medium exhibits a higher dispersivity than the true medium. This phenomenon can be particularly egregious when the system should not be dispersive at all, for example a fluid acquiring some spurious dispersion in a numerical model.\n\nIt occurs whenever the [[dispersion relation]] for the finite difference approximation is nonlinear.<ref>[http://glossary.ametsoc.org/wiki/Numerical_dispersion numerical dispersion.]  Glossary of the American Meteorological Society; page last modified on 26 January 2012, at 19:36.</ref><ref>http://www.mathematik.uni-dortmund.de/~kuzmin/cfdintro/lecture10.pdf</ref> For these reasons, it is often seen as a [[numerical error]].\n\nNumerical dispersion is often identified, linked and compared with [[numerical diffusion]],<ref>[http://people.maths.ox.ac.uk/trefethen/5all.pdf CHAPTER 5: Dissipation, Dispersion, and Group Velocity] TREFETHEN</ref> another [[Artifact (error)|artifact]] of similar origin.\n\n==Explanation==\nIn simulations, time and space are divided into discrete grids and the continuous differential equations of motion (such as the Navier–Stokes equation) are discretized into finite-difference equations;<ref>https://ccrma.stanford.edu/~bilbao/booktop/node100.html</ref> these  discrete equations are in general unidentical to the original differential equations, so the simulated system behaves differently than the intended physical system. The amount and character of the difference depends on the system being simulated and the type of discretization that is used.\n\n==See also==\n*[[Numerical diffusion]]\n*[[Von Neumann stability analysis]]\n\n==References==\n{{Reflist}}\n\n[[Category:Numerical artefacts|dispersion]]\n[[Category:Numerical differential equations]]\n\n\n{{mathapplied-stub}}"
    },
    {
      "title": "Flying ice cube",
      "url": "https://en.wikipedia.org/wiki/Flying_ice_cube",
      "text": "In [[molecular dynamics]] (MD) simulations, the '''flying ice cube''' effect is an artifact in which the energy of high-[[frequency]] [[fundamental mode]]s is drained into low-frequency modes, particularly into zero-frequency motions such as overall [[translation (physics)|translation]] and [[rotation]] of the system. The artifact derives its name from a particularly noticeable manifestation that arises in simulations of particles in [[vacuum]], where the system being simulated acquires high linear [[momentum]] and experiences extremely damped internal motions, freezing the system into a single conformation reminiscent of an [[ice cube]] or other [[rigid body]] flying through space. The artifact is entirely a consequence of molecular dynamics [[algorithm]]s and is wholly unphysical, since it violates the principle of [[Equipartition theorem|equipartition of energy]].<ref name=\"harvey\">{{cite journal|last1=Harvey|first1=Stephen C.|last2=Tan|first2=Robert K.-Z.|last3=Cheatham|first3=Thomas E.|title=The flying ice cube: Velocity rescaling in molecular dynamics leads to violation of energy equipartition|journal=Journal of Computational Chemistry|date=May 1998|volume=19|issue=7|pages=726–740|doi=10.1002/(SICI)1096-987X(199805)19:7<726::AID-JCC4>3.0.CO;2-S}}</ref>\n\n==Origin and avoidance==\nThe flying ice cube artifact arises from repeated rescalings of the [[velocity|velocities]] of the particles in the simulation system. Velocity rescaling is a means of imposing a [[thermostat]] on the system by multiplying the velocities of a system's particles by a factor after an integration timestep is completed, as is done by the [[Berendsen thermostat]] and the Bussi–Donadio–Parrinello thermostat. These schemes fail when the rescaling is done to a kinetic energy distribution of an ensemble that is not invariant under [[Molecular dynamics#Microcanonical ensemble (NVE)|microcanonical molecular dynamics]]; thus, the [[Berendsen thermostat]] (which rescales to the isokinetic ensemble) exhibits the artifact, while the Bussi–Donadio–Parrinello thermostat (which rescales to the canonical ensemble) does not exhibits the artifact. Rescaling to an ensemble that is not invariant under microcanonical molecular dynamics results in a violation of the balance condition that is a requirement of Monte Carlo simulations (molecular dynamics simulations with velocity rescaling thermostats can be thought of as Monte Carlo simulations with molecular dynamics moves and velocity rescaling moves), which is the artifact's underlying reason.<ref name=\"braun\">{{cite journal | first=E. | last=Braun | author2=Moosavi, S. M.| author3= Smit, B.| journal=Journal of Chemical Theory and Computation | title=Anomalous Effects of Velocity Rescaling Algorithms: The Flying Ice Cube Effect Revisited | year=2018 | volume=14 | issue=10 | pages=5262–5272 | doi=10.1021/acs.jctc.8b00446| arxiv=1805.02295 }}</ref>\n\nWhen the flying ice cube problem was first found, the Bussi–Donadio–Parrinello thermostat had not yet been developed, and it was desired to continue using the Berendsen thermostat due to the efficiency with which velocity rescaling thermostats relax systems to desired temperatures. Thus, suggestions were given to avoid the flying ice cube effect under the Berendsen thermostat, such as periodically removing the center-of-mass motions and using a longer temperature coupling time.<ref name=\"harvey\"/> However, more recently it has been recommended that the better practice is to discontinue use of the Berendsen thermostat entirely in favor of the Bussi–Donadio–Parrinello thermostat, as it has been shown that the latter thermostat does not exhibit the flying ice cube effect.<ref name=\"braun\"/>\n\n==References==\n{{reflist}}\n\n[[Category:Molecular dynamics]]\n[[Category:Numerical artefacts]]"
    },
    {
      "title": "Gibbs phenomenon",
      "url": "https://en.wikipedia.org/wiki/Gibbs_phenomenon",
      "text": "In [[mathematics]], the '''Gibbs phenomenon,''' discovered by {{harvs|txt|authorlink=Henry Wilbraham|first=Henry|last= Wilbraham|year=1848}}<ref name=\"Hewitt 1979 129–160\">{{cite journal|last=Hewitt|first=Edwin|author2=Hewitt, Robert E.|title=The Gibbs-Wilbraham phenomenon: An episode in Fourier analysis|journal=Archive for History of Exact Sciences|year=1979|volume=21|issue=2|pages=129–160|doi=10.1007/BF00330404}} Available on-line at:  [http://ocw.nctu.edu.tw/course/fourier/supplement/hewitt-hewitt1979.pdf National Chiao Tung University:  Open Course Ware:  Hewitt & Hewitt, 1979.]</ref>  and rediscovered by {{harvs|txt|authorlink=Willard Gibbs|first=J. Willard |last=Gibbs|year=1899}},<ref name=dimarogonas>{{cite book|title=Vibration for engineers|author=Andrew Dimarogonas|isbn=978-0-13-462938-4|year=1996}}</ref> is the peculiar manner in which the [[Fourier series]] of a [[piecewise]] continuously differentiable [[periodic function]] behaves at a [[jump discontinuity]]. The ''n''th [[partial sum]] of the Fourier series has large oscillations near the jump, which might increase the maximum of the partial sum above that of the function itself.  The overshoot does not die out as ''n'' increases, but approaches a finite limit.<ref name=Carslaw>\n{{cite book\n|author=H. S. Carslaw\n|title=Introduction to the theory of Fourier's series and integrals\n|chapter=Chapter IX\n|year= 1930\n|edition=Third\n|publisher=Dover Publications Inc.\n|location=New York\n|chapter-url=https://books.google.com/books?id=JNVAAAAAIAAJ&printsec=frontcover&dq=intitle:Introduction+intitle:to+intitle:the+intitle:theory+intitle:of+intitle:Fourier%27s+intitle:series+intitle:and+intitle:integrals+inauthor:carslaw#PPA264,M1}}\n</ref> This sort of behavior was also observed by experimental physicists, but was believed to be due to imperfections in the measuring apparatuses.<ref>{{harvnb|Vretblad|2000}} Section 4.7.</ref>\n\nThis is one cause of [[ringing artifacts]] in [[signal processing]].\n\n==Description==\n[[Image:Gibbs phenomenon 10.svg|thumb|200px|right|Functional approximation of square wave using 5 harmonics]]\n[[Image:Gibbs phenomenon 50.svg|thumb|200px|right|Functional approximation of square wave using 25 harmonics]]\n[[Image:Gibbs phenomenon 250.svg|thumb|200px|right|Functional approximation of square wave using 125 harmonics]]\n\nThe Gibbs phenomenon involves both the fact that Fourier sums overshoot at a [[jump discontinuity]], and that this overshoot does not die out as more terms are added to the sum.\n\nThe three pictures on the right demonstrate the phenomenon for a [[square wave]] (of height <math>\\pi/4</math>) whose Fourier expansion is\n\n:<math> \\sin(x)+\\frac{1}{3}\\sin(3x)+\\frac{1}{5}\\sin(5x)+\\dotsb.</math>\n\nMore precisely, this is the function ''f'' which equals <math>\\pi/4</math> between <math>2n\\pi</math> and <math>(2n+1)\\pi</math> and <math>-\\pi/4</math> between <math>(2n+1)\\pi</math> and <math>(2n+2)\\pi</math> for every [[integer]] ''n''; thus this square wave has a jump discontinuity of height <math>\\pi/2</math> at every integer multiple of <math>\\pi</math>.\n\nAs can be seen, as the number of terms rises, the error of the approximation is reduced in width and energy, but converges to a fixed height. A calculation for the square wave (see Zygmund, chap. 8.5., or the computations at the end of this article) gives an explicit formula for the limit of the height of the error. It turns out that the Fourier series exceeds the height <math>\\pi/4</math> of the square wave by\n\n:<math>\\frac{1}{2}\\int_0^\\pi \\frac{\\sin t}{t}\\, dt - \\frac{\\pi}{4} = \\frac{\\pi}{2}\\cdot (0.089489872236\\dots)</math>({{OEIS2C|A243268}})\n\nor about 9 percent of the jump. More generally, at any jump point of a piecewise continuously differentiable function with a jump of ''a'', the ''n''th partial Fourier series will (for ''n'' very large) overshoot this jump by approximately <math>a \\cdot (0.089489872236\\dots)</math> at one end and undershoot it by the same amount at the other end; thus the \"jump\" in the partial Fourier series will be about 18% larger than the jump in the original function.  At the location of the discontinuity itself, the partial Fourier series will converge to the midpoint of the jump (regardless of what the actual value of the original function is at this point).  The quantity\n:<math>\\int_0^\\pi \\frac{\\sin t}{t}\\ dt = (1.851937051982\\dots) = \\frac{\\pi}{2} + \\pi \\cdot (0.089489872236\\dots)</math>({{OEIS2C|A036792}})\nis sometimes known as the ''[[Henry Wilbraham|Wilbraham]]–Gibbs constant''.\n\n=== History ===\nThe Gibbs phenomenon was first noticed and analyzed by [[Henry Wilbraham]] in an 1848 paper.<ref>Wilbraham, Henry (1848) [https://www.google.com/books?id=JrQ4AAAAMAAJ&pg=PA198#v=onepage&q&f=false \"On a certain periodic function,\"] ''The Cambridge and Dublin Mathematical Journal'', '''3''' : 198–201.</ref> The paper attracted little attention until 1914 when it was mentioned in [[Heinrich Burkhardt]]'s review of mathematical analysis in [[Klein's encyclopedia]].<ref>{{cite book|title=Encyklopädie der Mathematischen Wissenschaften mit Einschluss ihrer Anwendungen|volume=Vol II T. 1 H 1|date=1914|publisher=Vieweg+Teubner Verlag|location=Wiesbaden|page=1049|url=http://gdz.sub.uni-goettingen.de/pdfcache/PPN360506208/PPN360506208___LOG_0158.pdf|accessdate=14 September 2016}}</ref> In 1898, [[Albert A. Michelson]] developed a device that could compute and re-synthesize the Fourier series.<ref>{{cite book|last1=Hammack|first1=Bill|last2=Kranz|first2=Steve|last3=Carpenter|first3=Bruce|title=Albert Michelson's Harmonic Analyzer: A Visual Tour of a Nineteenth Century Machine that Performs Fourier Analysis|publisher=Articulate Noise Books|isbn=9780983966173|url=http://www.engineerguy.com/fourier/|accessdate=14 September 2016|language=English|date=2014-10-29}}</ref><ref>{{cite book|last=Wolfram|first=Stephen|title=A New Kind of Science|publisher=Wolfram Media, Inc.|year=2002|page=899|isbn=978-1-57955-008-0}}</ref> A widespread myth says that when the Fourier coefficients for a square wave were input to the machine, the graph would oscillate at the discontinuities, and that because it was a physical device subject to manufacturing flaws, Michelson was convinced that the overshoot was caused by errors in the machine. In fact the graphs produced by the machine were not good enough to exhibit the Gibbs phenomenon clearly, and Michelson may not have noticed it as he made no mention of this effect in his paper {{harv|Michelson|Stratton|1898}} about his machine or his later letters to ''[[Nature (journal)|Nature]]''.<ref name=\"Hewitt 1979 129–160\"/>  Inspired by some correspondence in ''Nature'' between Michelson and Love about the convergence of the Fourier series of the square wave function, in 1898 [[Willard Gibbs|J. Willard Gibbs]] published a short note in which he considered what today would be called a [[sawtooth wave]] and pointed out the important distinction between the limit of the graphs of the partial sums of the Fourier series, and the graph of the function that is the limit of those partial sums.  In his first letter Gibbs failed to notice the Gibbs phenomenon, and the limit that he described for the graphs of the partial sums was inaccurate.  In 1899 he published a correction in which he described the overshoot at the point of discontinuity (''Nature'': April 27, 1899, p.&nbsp;606). In 1906, [[Maxime Bôcher]] gave a detailed mathematical analysis of that overshoot, coining the term \"Gibbs phenomenon\"<ref>Bôcher, Maxime (April 1906) [https://www.jstor.org/stable/1967238?seq=1 \"Introduction to the theory of Fourier's series\"], ''Annals of Mathethematics'', second series, '''7''' (3) : 81–152.  The Gibbs phenomenon is discussed on pages 123–132; Gibbs's role is mentioned on page 129.</ref> and bringing the term into widespread use.<ref name=\"Hewitt 1979 129–160\"/>\n\nAfter the existence of [[Henry Wilbraham]]'s paper became widely known, in 1925 [[Horatio Scott Carslaw]] remarked \"We may still call this property of Fourier's series (and certain other series) Gibbs's phenomenon; but we must no longer claim that the property was first discovered by Gibbs.\"<ref>{{cite journal|last1=Carslaw|first1=H. S.|title=A historical note on Gibbs' phenomenon in Fourier's series and integrals|journal=Bulletin of the American Mathematical Society|date=1 October 1925|volume=31|issue=8|pages=420–424|url=https://projecteuclid.org/euclid.bams/1183486614|accessdate=14 September 2016|language=EN|issn=0002-9904|doi=10.1090/s0002-9904-1925-04081-1}}</ref>\n\n=== Explanation ===\nInformally, the Gibbs phenomenon reflects the difficulty inherent in approximating a [[discontinuous function]] by a ''finite'' series of [[continuous function|continuous]] sine and cosine waves. It is important to put emphasis on the word ''finite'' because even though every partial sum of the Fourier series overshoots the function it is approximating, the limit of the partial sums does not.  The value of ''x'' where the maximum overshoot is achieved moves closer and closer to the discontinuity as the number of terms summed increases so, again informally, once the overshoot has passed by a particular ''x'', convergence at that value of ''x'' is possible.\n\nThere is no contradiction in the overshoot converging to a non-zero amount, but the limit of the partial sums having no overshoot, because the location of that overshoot moves.  We have [[pointwise convergence]], but not [[uniform convergence]].  For a piecewise ''C''<sup>1</sup> function the Fourier series converges to the function at ''every point'' except at the jump discontinuities.  At the jump discontinuities themselves the limit will converge to the average of the values of the function on either side of the jump.  This is a consequence of the [[Dirichlet conditions|Dirichlet theorem]].<ref name=Pinksky>\n{{cite book\n|author=M. Pinsky\n|title=Introduction to Fourier Analysis and Wavelets\n|page=27\n|year= 2002\n|publisher=Brooks/Cole\n|location=United states of America\n}}\n</ref>\n\nThe Gibbs phenomenon is also closely related to the principle that the decay of the Fourier coefficients of a function at infinity is controlled by the smoothness of that function; very smooth functions will have very rapidly decaying Fourier coefficients (resulting in the rapid convergence of the Fourier series), whereas discontinuous functions will have very slowly decaying Fourier coefficients (causing the Fourier series to converge very slowly).  Note for instance that the Fourier coefficients 1,&nbsp;−1/3,&nbsp;1/5,&nbsp;... of the discontinuous square wave described above decay only as fast as the [[harmonic series (mathematics)|harmonic series]], which is not [[absolutely convergent]]; indeed, the above Fourier series turns out to be only conditionally convergent for [[almost everywhere|almost every]] value of&nbsp;''x''.  This provides a partial explanation of the Gibbs phenomenon, since Fourier series with absolutely convergent Fourier coefficients would be [[uniformly convergent]] by the [[Weierstrass M-test]] and would thus be unable to exhibit the above oscillatory behavior.  By the same token, it\nis impossible for a discontinuous function to have absolutely convergent Fourier coefficients, since the function would thus be the uniform limit of continuous functions and therefore be continuous, a contradiction.  See [[Convergence of Fourier series#Absolute convergence|more about absolute convergence of Fourier series]].\n\n=== Solutions ===\nIn practice, the difficulties associated with the Gibbs phenomenon can be ameliorated by using a smoother method of Fourier series summation, such as [[Fejér summation]] or [[Riesz summation]], or by using [[sigma-approximation]]. Using a continuous [[wavelet]] transform, the wavelet Gibbs phenomenon never exceeds the Fourier Gibbs phenomenon.<ref>Rasmussen, Henrik O. \"The Wavelet Gibbs Phenomenon.\" In \"''Wavelets, Fractals and Fourier Transforms''\", Eds [[Marie Farge|M. Farge]] ''et al''., Clarendon Press, Oxford, 1993.</ref> Also, using the discrete wavelet transform with [[Haar basis functions]], the Gibbs phenomenon does not occur at all in the case of continuous data at jump discontinuities,<ref>Kelly, Susan E. \"Gibbs Phenomenon for Wavelets.\" Applied and Computational Harmonic Analysis 3, 1995. {{cite web |url=http://www.uwlax.edu/faculty/kelly/Publications/GibbsJan.pdf |title=Archived copy |accessdate=2012-03-31 |deadurl=yes |archiveurl=https://web.archive.org/web/20130909200315/http://www.uwlax.edu/faculty/kelly/Publications/GibbsJan.pdf |archivedate=2013-09-09 |df= }}</ref> and is minimal in the discrete case at large change points. In wavelet analysis, this is commonly referred to as the [[Longo phenomenon]].\n\n== Formal mathematical description of the phenomenon ==\nLet <math>f: {\\mathbb R} \\to {\\mathbb R}</math> be a piecewise continuously differentiable function which is periodic with some period <math>L > 0</math>.  Suppose that at some point <math>x_0</math>, the left limit <math>f(x_0^-)</math> and right limit <math>f(x_0^+)</math> of the function <math>f</math> differ by a non-zero gap <math>a</math>:\n\n: <math> f(x_0^+) - f(x_0^-) = a \\neq 0.</math>\n\nFor each positive integer ''N'' ≥ 1, let ''S''<sub>''N''</sub>&nbsp;''f'' be the ''N''th partial Fourier series\n\n: <math> S_N f(x) := \\sum_{-N \\leq n \\leq N} \\widehat f(n) e^{\\frac{2i\\pi  n x}{L}}\n= \\frac{1}{2} a_0 + \\sum_{n=1}^N \\left( a_n \\cos\\left(\\frac{2\\pi nx}{L}\\right) + b_n \\sin\\left(\\frac{2\\pi nx}{L}\\right) \\right),</math>\n\nwhere the Fourier coefficients <math>\\widehat f(n), a_n, b_n</math> are given by the usual formulae\n\n: <math> \\widehat f(n) := \\frac{1}{L} \\int_0^L f(x) e^{-2i\\pi  n x/L}\\, dx</math>\n\n: <math> a_n := \\frac{2}{L} \\int_0^L f(x) \\cos\\left(\\frac{2\\pi nx}{L}\\right)\\, dx</math>\n\n: <math> b_n := \\frac{2}{L} \\int_0^L f(x) \\sin\\left(\\frac{2\\pi nx}{L}\\right)\\, dx.</math>\n\nThen we have\n\n: <math> \\lim_{N \\to \\infty} S_N f\\left(x_0 + \\frac{L}{2N}\\right) = f(x_0^+) + a\\cdot (0.089489872236\\dots)</math>\n\nand\n\n: <math> \\lim_{N \\to \\infty} S_N f\\left(x_0 - \\frac{L}{2N}\\right) = f(x_0^-) - a\\cdot (0.089489872236\\dots)</math>\n\nbut\n\n: <math> \\lim_{N \\to \\infty} S_N f(x_0) = \\frac{f(x_0^-) + f(x_0^+)}{2}.</math>\nMore generally, if <math>x_N</math> is any sequence of real numbers which converges to <math>x_0</math> as <math>N \\to \\infty</math>, and if the gap ''a'' is positive then\n: <math> \\limsup_{N \\to \\infty} S_N f(x_N) \\leq f(x_0^+) + a\\cdot (0.089489872236\\dots)</math>\nand\n: <math> \\liminf_{N \\to \\infty} S_N f(x_N) \\geq f(x_0^-) - a\\cdot (0.089489872236\\dots).</math>\nIf instead the gap ''a'' is negative, one needs to interchange [[limit superior]] with [[limit inferior]], and also interchange\nthe ≤ and ≥ signs, in the above two inequalities.\n\n== Signal processing explanation ==\n{{details|Ringing artifacts}}\n[[File:Sinc function (both).svg|thumb|The [[sinc function]], the [[impulse response]] of an ideal [[low-pass filter]]. Scaling narrows the function, and correspondingly increases magnitude (which is not shown here), but does not reduce the magnitude of the undershoot, which is the integral of the tail.]]\nFrom a [[signal processing]] point of view, the Gibbs phenomenon is the [[step response]] of a [[low-pass filter]], and the oscillations are called [[ringing (signal)|ringing]] or [[ringing artifacts]]. Truncating the [[Fourier transform]] of a signal on the real line, or the Fourier series of a periodic signal (equivalently, a signal on the circle) corresponds to filtering out the higher frequencies by an ideal ([[brick-wall filter|brick-wall]]) low-pass/high-cut filter. This can be represented as [[convolution]] of the original signal with the [[impulse response]] of the filter (also known as the [[Convolution kernel|kernel]]), which is the [[sinc function]]. Thus the Gibbs phenomenon can be seen as the result of convolving a [[Heaviside step function]] (if periodicity is not required) or a [[square wave]] (if periodic) with a sinc function: the oscillations in the sinc function cause the ripples in the output.\n\n[[File:Sine integral.svg|thumb|The [[sine integral]], exhibiting the Gibbs phenomenon for a step function on the real line.]]\nIn the case of convolving with a Heaviside step function, the resulting function is exactly the integral of the sinc function, the [[sine integral]]; for a square wave the description is not as simply stated. For the step function, the magnitude of the undershoot is thus exactly the integral of the (left) tail, integrating to the first negative zero: for the normalized sinc of unit sampling period, this is <math>\\int_{-\\infty}^{-1} \\frac{\\sin(\\pi x)}{\\pi x}\\,dx.</math> The overshoot is accordingly of the same magnitude: the integral of the right tail, or, which amounts to the same thing, the difference between the integral from negative infinity to the first positive zero, minus 1 (the non-overshooting value).\n\nThe overshoot and undershoot can be understood thus: kernels are generally normalized to have integral 1, so they result in a mapping of constant functions to constant functions – otherwise they have [[Gain (electronics)|gain]]. The value of a convolution at a point is a [[linear combination]] of the input signal, with coefficients (weights) the values of the kernel.\nIf a kernel is non-negative, such as for a [[Gaussian kernel]], then the value of the filtered signal will be a [[convex combination]] of the input values (the coefficients (the kernel) integrate to 1, and are non-negative), and will thus fall between the minimum and maximum of the input signal – it will not undershoot or overshoot. If, on the other hand, the kernel assumes negative values, such as the sinc function, then the value of the filtered signal will instead be an [[affine combination]] of the input values, and may fall outside of the minimum and maximum of the input signal, resulting in undershoot and overshoot, as in the Gibbs phenomenon.\n\nTaking a longer expansion – cutting at a higher frequency – corresponds in the frequency domain to widening the brick-wall, which in the time domain corresponds to narrowing the sinc function and increasing its height by the same factor, leaving the integrals between corresponding points unchanged. This is a general feature of the Fourier transform: widening in one domain corresponds to narrowing and increasing height in the other. This results in the oscillations in sinc being narrower and taller and, in the filtered function (after convolution), yields oscillations that are narrower and thus have less ''area,'' but does ''not'' reduce the ''magnitude:'' cutting off at any finite frequency results in a sinc function, however narrow, with the same tail integrals. This explains the persistence of the overshoot and undershoot.\n\n<gallery style=\"align: center\" widths=\"285px\" heights=\"285px\">\nImage:Gibbs phenomenon 10.svg|Oscillations can be interpreted as convolution with a sinc.\nImage:Gibbs phenomenon 50.svg|Higher cutoff makes the sinc narrower but taller, with the same magnitude tail integrals, yielding higher frequency oscillations, but whose magnitude does not vanish.\n</gallery>\n\nThus the features of the Gibbs phenomenon are interpreted as follows:\n* the undershoot is due to the impulse response having a negative tail integral, which is possible because the function takes negative values;\n* the overshoot offsets this, by symmetry (the overall integral does not change under filtering);\n* the persistence of the oscillations is because increasing the cutoff narrows the impulse response, but does not reduce its integral – the oscillations thus move towards the discontinuity, but do not decrease in magnitude.\n\n== The square wave example ==\n[[Image:squareWave.gif|thumb|450px|right|Animation of the additive synthesis of a square wave with an increasing number of harmonics. The Gibbs phenomenon is visible especially when the number of harmonics is large.]]\n\nIn the square wave case the period ''L'' is <math>2\\pi</math>, the discontinuity <math>x_0</math> is at zero, and the jump ''a'' is equal to <math>\\pi/2</math>.\nFor simplicity let us just deal with the case when ''N'' is even (the case of odd ''N'' is very similar).  Then\nwe have\n\n:<math>S_N f(x) = \\sin(x) + \\frac{1}{3} \\sin(3x) + \\cdots + \\frac{1}{N-1} \\sin((N-1)x).</math>\n\nSubstituting <math>x=0</math>, we obtain\n\n:<math>S_N f(0) = 0 = \\frac{-\\frac{\\pi}{4} + \\frac{\\pi}{4}}{2} = \\frac{f(0^-) + f(0^+)}{2}</math>\n\nas claimed above.  Next, we compute\n\n:<math>S_N f\\left(\\frac{2\\pi}{2N}\\right) = \\sin\\left(\\frac{\\pi}{N}\\right) + \\frac{1}{3} \\sin\\left(\\frac{3\\pi}{N}\\right)\n+ \\cdots + \\frac{1}{N-1} \\sin\\left( \\frac{(N-1)\\pi}{N} \\right).</math>\n\nIf we introduce the normalized [[sinc function]], <math>\\operatorname{sinc}(x)\\,</math>, we can rewrite this as\n\n:<math>S_N f\\left(\\frac{2\\pi}{2N}\\right) = \\frac{\\pi}{2} \\left[ \\frac{2}{N} \\operatorname{sinc}\\left(\\frac{1}{N}\\right) + \\frac{2}{N} \\operatorname{sinc}\\left(\\frac{3}{N}\\right)\n+ \\cdots + \\frac{2}{N} \\operatorname{sinc}\\left( \\frac{(N-1)}{N} \\right) \\right].</math>\n\nBut the expression in square brackets is a [[Riemann sum]] approximation to the integral <math>\\int_0^1 \\operatorname{sinc}(x)\\ dx</math> (more precisely, it is a [[midpoint rule]] approximation with spacing <math>2/N</math>).  Since the sinc function is continuous, this approximation converges to the actual integral\nas <math>N \\to \\infty</math>.  Thus we have\n\n: <math>\n\\begin{align}\n\\lim_{N \\to \\infty} S_N f\\left(\\frac{2\\pi}{2N}\\right)\n& = \\frac{\\pi}{2} \\int_0^1 \\operatorname{sinc}(x)\\, dx \\\\[8pt]\n& = \\frac{1}{2} \\int_{x=0}^1 \\frac{\\sin(\\pi x)}{\\pi x}\\, d(\\pi x) \\\\[8pt]\n& = \\frac{1}{2} \\int_0^\\pi \\frac{\\sin(t)}{t}\\ dt \\quad = \\quad \\frac{\\pi}{4} + \\frac{\\pi}{2} \\cdot (0.089489872236\\dots),\n\\end{align}\n</math>\n\nwhich was what was claimed in the previous section.  A similar computation shows\n\n:<math>\\lim_{N \\to \\infty} S_N f\\left(-\\frac{2\\pi}{2N}\\right) = -\\frac{\\pi}{2} \\int_0^1 \\operatorname{sinc}(x)\\ dx = -\\frac{\\pi}{4} -\n\\frac{\\pi}{2} \\cdot (0.089489872236\\dots).</math>\n\n==Consequences==\nIn signal processing, the Gibbs phenomenon is undesirable because it causes artifacts, namely [[Clipping (audio)|clipping]] from the overshoot and undershoot, and [[ringing artifacts]] from the oscillations. In the case of low-pass filtering, these can be reduced or eliminated by using different low-pass filters.\n\nIn [[MRI]], the Gibbs phenomenon causes artifacts in the presence of adjacent regions of markedly differing signal intensity. This is most commonly encountered in spinal MR imaging, where the Gibbs phenomenon may simulate the appearance of [[syringomyelia]].\n\nThe Gibbs phenomenon manifests as a cross pattern artifact in the [[discrete Fourier transform]] of an image,<ref>{{cite journal|author=R. Hovden, Y. Jiang, H.L. Xin, L.F. Kourkoutis|title=Periodic Artifact Reduction in Fourier Transforms of Full Field Atomic Resolution Images|journal=Microscopy and Microanalysis|year=2015|volume=21|issue=2|pages=436–441\n|doi=10.1017/S1431927614014639|url=https://www.cambridge.org/core/journals/microscopy-and-microanalysis/article/div-classtitleperiodic-artifact-reduction-in-fourier-transforms-of-full-field-atomic-resolution-imagesdiv/80D0E226F0B4B16627AA0B6B9BD24F24}}</ref> where most images (e.g. [[micrographs]] or photographs) have a sharp discontinuity between boundaries at the top / bottom and left / right of an image. When periodic boundary conditions are imposed in the Fourier transform, this jump discontinuity is represented by continuum of frequencies along the axes in reciprocal space (i.e. a cross pattern of intensity in the Fourier Transform).\n\n==See also==\n* [[Sigma approximation|σ-approximation]] which adjusts a Fourier summation to eliminate the Gibbs phenomenon which would otherwise occur at discontinuities\n* [[Pinsky phenomenon]]\n* Compare with [[Runge's phenomenon]] for polynomial approximations\n* [[Sine integral]]\n* [[Mach bands]]\n\n==Notes==\n{{reflist}}\n\n==References==\n\n*{{Citation | last1=Gibbs | first1=J. Willard | author1-link=J. Willard Gibbs | title=Fourier's Series | doi=10.1038/059200b0 | year=1898 | journal=[[Nature (journal)|Nature]] | issn=0028-0836 | volume=59 | issue=1522 | pages=200| url=https://zenodo.org/record/1429384 }}\n*{{Citation | last1=Gibbs | first1=J. Willard | author1-link=J. Willard Gibbs | title=Fourier's Series | doi=10.1038/059606a0 | year=1899 | journal=[[Nature (journal)|Nature]] | issn=0028-0836 | volume=59 | issue=1539 | pages=606}}\n*{{Citation | last1=Michelson | first1=A. A. | last2=Stratton | first2=S. W. | title=A new harmonic analyser | year=1898 | journal= Philosophical Magazine | volume=5 | issue=45 | pages=85–91}}\n* [[Antoni Zygmund]], ''[[Trigonometric series|Trigonometrical series]]'', Dover publications, 1955.\n*{{Citation | last1=Wilbraham | first1=Henry | title=On a certain periodic function | url=https://www.google.com/books?id=JrQ4AAAAMAAJ&pg=PA198#v=onepage&q&f=false | year=1848 | journal=[[The Cambridge and Dublin Mathematical Journal]] | volume=3 | pages=198–201}}\n* [[Paul J. Nahin]], ''Dr. Euler's Fabulous Formula,'' Princeton University Press, 2006.  Ch. 4, Sect. 4.\n* {{citation|last=Vretblad|first=Anders|title=Fourier Analysis and its Applications|year=2000|isbn=978-0-387-00836-3|publisher=[[Springer Publishing]]|series=Graduate Texts in Mathematics|volume=223|pages=93|location=New York}}\n\n==External links==\n* {{springer|title=Gibbs phenomenon|id=p/g044410}}\n* Weisstein, Eric W., \"''[http://mathworld.wolfram.com/GibbsPhenomenon.html Gibbs Phenomenon]''\". From MathWorld—A Wolfram Web Resource.\n* Prandoni, Paolo, \"''[http://www.sp4comm.org/gibbs/gibbs.html Gibbs Phenomenon]''\".\n* Radaelli-Sanchez, Ricardo, and Richard Baraniuk, \"''[https://web.archive.org/web/20040506042335/http://cnx.rice.edu/content/m10092/latest/ Gibbs Phenomenon]''\". The Connexions Project. (Creative Commons Attribution License)\n*[https://archive.org/details/introductiontot00unkngoog Horatio S Carslaw : Introduction to the theory of Fourier's series and integrals.pdf (introductiontot00unkngoog.pdf )] at [[archive.org]]\n\n{{DEFAULTSORT:Gibbs Phenomenon}}\n[[Category:Real analysis]]\n[[Category:Fourier series]]\n[[Category:Numerical artefacts]]"
    },
    {
      "title": "Numerical diffusion",
      "url": "https://en.wikipedia.org/wiki/Numerical_diffusion",
      "text": "{{Refimprove|date=March 2017}}\n'''Numerical diffusion''' is a difficulty with [[computer simulation]]s of continua (such as [[fluid]]s) wherein the simulated medium exhibits a higher [[diffusivity]] than the true medium. This phenomenon can be particularly egregious when the system should not be diffusive at all, for example an ideal fluid acquiring some spurious viscosity in a numerical model.\n\n== Explanation ==\n\nIn [[Eulerian method|Eulerian simulations]], time and space are divided into a discrete grid and the continuous [[differential equation]]s of motion (such as the [[Navier–Stokes equation]]) are [[discretization|discretized]] into [[finite-difference equation]]s.<ref>http://www.mathematik.uni-dortmund.de/~kuzmin/cfdintro/lecture10.pdf</ref>  The discrete equations are in general more [[diffusion|diffusive]] than the original differential equations, so that the simulated system behaves differently than the intended physical system.<ref>http://people.maths.ox.ac.uk/trefethen/5all.pdf</ref>  The amount and character of the difference depends on the system being simulated and the type of discretization that is used.  Most fluid dynamics or [[Magnetohydrodynamics|magnetohydrodynamic]] simulations seek to reduce numerical diffusion to the minimum possible, to achieve high fidelity — but under certain circumstances diffusion is added deliberately into the system to avoid [[mathematical singularity|singularities]].  For example, [[shock wave]]s in fluids and [[current sheet]]s in [[plasma (physics)|plasma]]s are in some approximations infinitely thin; this can cause difficulty for numerical codes.  A simple way to avoid the difficulty is to add diffusion that smooths out the shock or current sheet.  Higher order numerical methods (including spectral methods) tend to have less numerical diffusion than low order methods.\n\n== Example ==\n\nAs an example of numerical diffusion, consider an Eulerian simulation using an explicit time-advance of a drop of green dye diffusing through water.  If the water is flowing diagonally through the simulation grid, then it is impossible to move the dye in the exact direction of the flow: at each time step the simulation can at best transfer some dye in each of the vertical and horizontal directions.  After a few time steps, the dye will have spread out through the grid due to this sideways transfer.  This numerical effect takes the form of an extra high diffusion rate.\n\nWhen numerical diffusion applies to the components of the [[momentum]] vector, it is called [[numerical viscosity]]; when it applies to a magnetic field, it is called [[numerical resistivity]].\n\n[[File:Numerical_diffusion_01_by_N.KAID_2012_UB_BECHAR.jpg]]\n\n[[File:Animation(2).webm|thumb|Caption text|right|Phasefield Simulation of a airbubble within a phase of water]] Consider a [[Phase_field_models|Phasefield-problem]] with a high pressure loaded airbubble(blue) within a phase of water. Since there are no chemical or thermodynamical reactions during expansion of air in water there is no possibility to come up with another (i.e. non red or blue) phase during the simulation. These inaccuracies between single phases are based on numerical diffusion and can be decreased by [[Polygon_mesh|mesh]] refining.\n\n==See also==\n*[[Numerical dispersion]]\n*[[Numerical error]]\n\n==References==\n{{Reflist}}\n\n[[Category:Numerical artefacts]]\n[[Category:Numerical differential equations]]\n\n\n{{mathapplied-stub}}"
    },
    {
      "title": "Numerical sign problem",
      "url": "https://en.wikipedia.org/wiki/Numerical_sign_problem",
      "text": "In [[applied mathematics]], the '''numerical sign problem''' is the problem of numerically evaluating the [[integral]] of a highly [[Oscillation|oscillatory]] [[Function (mathematics)|function]] of a large number of variables. [[Numerical methods]] fail because of the near-cancellation of the positive and negative contributions to the integral. Each has to be integrated to very high [[Accuracy and precision|precision]] in order for their difference to be obtained with useful [[Accuracy and precision|accuracy]].\n\nThe sign problem is one of the major unsolved problems in the physics of [[many-particle system]]s. It often arises in calculations of the properties of a [[quantum mechanical]] system with large number of strongly interacting [[fermion]]s, or in field theories involving a non-zero density of strongly interacting fermions.\n\n==Overview<!--'Complex action problem' redirects here-->==\n\nIn physics the sign problem is typically (but not exclusively) encountered in calculations of the properties of a quantum mechanical system with large number of strongly interacting fermions, or in field theories involving a non-zero density of strongly interacting fermions. Because the particles are strongly interacting, [[perturbation theory]] is inapplicable, and one is forced to use brute-force numerical methods. Because the particles are fermions, their [[wavefunction]] changes sign when any two fermions are interchanged (due to the anti-symmetry of the wave function, see [[Pauli principle]]). So unless there are cancellations arising from some symmetry of the system, the quantum-mechanical sum over all multi-particle states involves an integral over a function that is highly oscillatory, hence hard to evaluate numerically, particularly in high dimension. Since the dimension of the integral is given by the number of particles, the sign problem becomes severe in the [[thermodynamic limit]]. The field-theoretic manifestation of the sign problem is discussed below.\n\nThe sign problem is one of the major unsolved problems in the physics of many-particle systems, impeding progress in many areas:\n* [[Condensed matter physics]] — It prevents the numerical solution of systems with a high density of strongly correlated electrons, such as the [[Hubbard model]].<ref>{{cite journal |doi=10.1103/PhysRevB.41.9301 |bibcode=1990PhRvB..41.9301L |title=Sign problem in the numerical simulation of many-electron systems |journal=Physical Review B |volume=41 |issue=13 |pages=9301–9307 |year=1990 |last1=Loh |first1=E. Y. |last2=Gubernatis |first2=J. E. |last3=Scalettar |first3=R. T. |last4=White |first4=S. R. |last5=Scalapino |first5=D. J. |last6=Sugar |first6=R. L.}}</ref>\n* [[Nuclear physics]] — It prevents the ''[[ab initio]]'' calculation of properties of [[nuclear matter]] and hence limits our understanding of [[atomic nucleus|nuclei]] and [[neutron star]]s.\n* [[Quantum field theory]] — It prevents the use of [[lattice QCD]]<ref>{{Cite journal |author=de Forcrand, Philippe |title=Simulating QCD at finite density |journal=Pos Lat |volume=010 |year=2010 |arxiv=1005.0539 |bibcode=2010arXiv1005.0539D}}</ref> to predict the phases and properties of [[quark matter]].<ref name='Philipsen'>{{cite journal |last=Philipsen |first=O. |year=2008 |title=Lattice calculations at non-zero chemical potential: The QCD phase diagram |url=http://pos.sissa.it//archive/conferences/077/011/Confinement8_011.pdf |journal=Proceedings of Science |volume=77 |pages=011 |doi=10.22323/1.077.0011}}</ref> (In [[lattice field theory]], the problem is also known as the '''complex action problem'''<!--boldface per WP:R#PLA-->.)<ref>{{cite journal |doi=10.1103/PhysRevD.66.106008 |arxiv=hep-th/0108041 |bibcode=2002PhRvD..66j6008A |title=New approach to the complex-action problem and its application to a nonperturbative study of superstring theory |journal=Physical Review D |volume=66 |issue=10 |year=2002 |last1=Anagnostopoulos |first1=K. N. |last2=Nishimura |first2=J.}}</ref>\n\n==The sign problem in field theory==\nSources:<ref name='Wiese-cluster'/><ref name='Kieu'/>\n\nIn a field theory approach to multi-particle systems, the fermion density is controlled by the value of the fermion [[chemical potential]] <math>\\mu</math>. One evaluates the [[Partition function (quantum field theory)|partition function]] <math>Z</math> by summing over all classical field configurations, weighted by <math>\\exp(-S)</math> where <math>S</math> is the [[Action (physics)|action]] of the configuration. The sum over fermion fields can be performed analytically, and one is left with a sum over the [[boson]]ic fields <math>\\sigma</math> (which may have been originally part of the theory, or have been produced by a [[Hubbard–Stratonovich transformation]] to make the fermion action quadratic)\n\n:<math>Z = \\int D \\sigma \\; \\rho[\\sigma]</math>\n\nwhere <math>D \\sigma</math> represents the measure for the sum over all configurations <math>\\sigma(x)</math> of the bosonic fields, weighted by\n\n:<math>\\rho[\\sigma]=\\det(M(\\mu,\\sigma))\\exp(-S[\\sigma])</math>\n\nwhere <math>S</math> is now the action of the bosonic fields, and <math>M(\\mu,\\sigma)</math> is a matrix that encodes how the fermions were coupled to the bosons. The expectation value of an observable <math>A[\\sigma]</math> is therefore an average over all configurations weighted by <math>\\rho[\\sigma]</math>\n\n:<math>\n\\langle A \\rangle_\\rho = \\frac{\\int D \\sigma \\; A[\\sigma] \\; \\rho[\\sigma]}{\\int D \\sigma \\; \\rho[\\sigma]} .\n</math>\n\nIf <math>\\rho[\\sigma]</math> is positive, then it can be interpreted as a probability measure, and <math>\\langle A \\rangle_\\rho</math> can be calculated by performing the sum over field configurations numerically, using standard techniques such as [[Monte Carlo integration|Monte Carlo importance sampling]].\n\nThe sign problem arises when <math>\\rho[\\sigma]</math> is non-positive. This typically occurs in theories of fermions when the fermion chemical potential <math>\\mu</math> is nonzero, i.e. when there is a nonzero background density of fermions. If <math>\\mu\\neq 0</math> there is no particle-antiparticle symmetry, and <math>\\det(M(\\mu,\\sigma))</math>, and hence the weight <math>\\rho(\\sigma)</math>, is in general a complex number, so Monte Carlo importance sampling cannot be used to evaluate the integral.\n\n=== Reweighting procedure ===\n\nA field theory with a non-positive weight can be transformed to one with a positive weight, by incorporating the non-positive part (sign or complex phase) of the weight into the observable. For example, one could decompose the weighting function into its modulus and phase,\n:<math>\\rho[\\sigma] = p[\\sigma]\\, \\exp(i\\theta[\\sigma])</math>\nwhere <math>p[\\sigma]</math> is real and positive, so\n:<math> \\langle A \\rangle_\\rho \n= \\frac{ \\int D\\sigma A[\\sigma] \\exp(i\\theta[\\sigma])\\; p[\\sigma]}{\\int D\\sigma \\exp(i\\theta[\\sigma])\\; p[\\sigma]}\n= \\frac{ \\langle A[\\sigma] \\exp(i\\theta[\\sigma]) \\rangle_p}{ \\langle \\exp(i\\theta[\\sigma]) \\rangle_p} </math>\n\nNote that the desired expectation value is now a ratio where the numerator and denominator are expectation values that both use a positive weighting function, <math>p[\\sigma]</math>. However, the phase <math>\\exp(i\\theta[\\sigma])</math> is a highly oscillatory function in the configuration space, so if one uses Monte Carlo methods to evaluate the numerator and denominator, each of them will evaluate to a very small number, whose exact value is swamped by the noise inherent in the Monte Carlo sampling process. The \"badness\" of the sign problem is measured by the smallness of the denominator <math>\\langle \\exp(i\\theta[\\sigma]) \\rangle_p</math>: if it is much less than 1 then the sign problem is severe.\nIt can be shown (e.g.<ref name='Wiese-cluster'/>) that\n:<math>\\langle \\exp(i\\theta[\\sigma]) \\rangle_p \\propto \\exp(-f V/T)</math>\nwhere <math>V</math> is the volume of the system, <math>T</math> is the temperature, and <math>f</math> is an energy density. The number of Monte Carlo sampling points needed to obtain an accurate result therefore rises exponentially as the volume of the system becomes large, and as the temperature goes to zero.\n\nThe decomposition of the weighting function into modulus and phase is just one example (although it has been advocated as the optimal choice since it minimizes the variance of the denominator <ref name='Kieu'>{{cite journal |doi=10.1103/PhysRevE.49.3855 |arxiv=hep-lat/9311072 |bibcode=1994PhRvE..49.3855K |title=Monte Carlo simulations with indefinite and complex-valued measures |journal=Physical Review E |volume=49 |issue=5 |pages=3855–3859 |year=1994 |last1=Kieu |first1=T. D. |last2=Griffin |first2=C. J.}}</ref>). In general one could write\n:<math>\\rho[\\sigma] = p[\\sigma] \\frac{\\rho[\\sigma]}{p[\\sigma]}</math>\nwhere <math>p[\\sigma]</math> can be any positive weighting function (for example, the weighting function of the <math>\\mu=0</math> theory.)<ref>{{Cite journal |arxiv=hep-lat/9705042 |last1=Barbour |first1=I. M. |title=Results on Finite Density QCD |journal=Nuclear Physics B - Proceedings Supplements |volume=60 |issue=1998 |pages=220–233 |last2=Morrison |first2=S. E. |last3=Klepfish |first3=E. G. |last4=Kogut |first4=J. B. |last5=Lombardo |first5=M.-P. |doi=10.1016/S0920-5632(97)00484-2 |year=1998|bibcode=1998NuPhS..60..220B }}</ref> The badness of the sign problem is then measured by\n:<math>\\left\\langle \\frac{\\rho[\\sigma]}{p[\\sigma]}\\right\\rangle_p \\propto \\exp(-f V/T)</math>\nwhich again goes to zero exponentially in the large-volume limit.\n\n==Methods for reducing the sign problem==\n\nThe sign problem is [[NP-hard]], implying that a full and generic solution of the sign problem would also solve all problems in the complexity class NP in polynomial time.<ref>{{Cite journal |arxiv=cond-mat/0408370 |doi=10.1103/PhysRevLett.94.170201 |pmid=15904269 |bibcode=2005PhRvL..94q0201T |title=Computational Complexity and Fundamental Limitations to Fermionic Quantum Monte Carlo Simulations |journal=Physical Review Letters |volume=94 |issue=17 |pages=170201 |year=2005 |last1=Troyer |first1=Matthias |last2=Wiese |first2=Uwe-Jens }}</ref> If (as is generally suspected) there are no polynomial-time solutions to NP problems (see [[P versus NP problem]]), then there is no ''generic'' solution to the sign problem. This leaves open the possibility that there may be solutions that work in specific cases, where the oscillations of the integrand have a structure that can be exploited to reduce the numerical errors.\n\nIn systems with a moderate sign problem, such as field theories at a sufficiently high temperature or in a sufficiently small volume, the sign problem is not too severe and useful results can be obtained by various methods, such as more carefully tuned reweighting, analytic continuation from imaginary <math>\\mu</math> to real <math>\\mu</math>, or Taylor expansion in powers of <math>\\mu</math>.<ref name='Philipsen'/><ref>{{Cite journal |arxiv=hep-lat/0610116 |last1=Schmidt |first1=Christian |title=Lattice QCD at Finite Density |journal=Pos Lat |volume=021 |year=2006|bibcode=2006slft.confE..21S }}</ref>\n\nThere are various proposals for solving systems with a severe sign problem:\n\n* [[Meron (physics)|Meron]]-cluster algorithms. These achieve an exponential speed-up by decomposing the fermion world lines into clusters that contribute independently. Cluster algorithms have been developed for certain theories,<ref name='Wiese-cluster'>{{cite journal |doi=10.1103/PhysRevLett.83.3116 |arxiv=cond-mat/9902128 |bibcode=1999PhRvL..83.3116C |title=Meron-Cluster Solution of Fermion Sign Problems |journal=Physical Review Letters |volume=83 |issue=16 |pages=3116–3119 |year=1999 |last1=Chandrasekharan |first1=Shailesh |last2=Wiese |first2=Uwe-Jens}}</ref> but not for the Hubbard model of electrons, nor for [[Quantum chromodynamics|QCD]], the theory of quarks.\n* [[Stochastic quantization]]. The sum over configurations is obtained as the equilibrium distribution of states explored by a complex [[Langevin equation]]. So far, the algorithm has been found to evade the sign problem in test models that have a sign problem but do not involve fermions.<ref>{{cite journal |doi=10.1103/PhysRevLett.102.131601 |pmid=19392346 |arxiv=0810.2089 |bibcode=2009PhRvL.102m1601A |title=Can Stochastic Quantization Evade the Sign Problem? The Relativistic Bose Gas at Finite Chemical Potential |journal=Physical Review Letters |volume=102 |issue=13 |pages=131601 |year=2009 |last1=Aarts |first1=Gert}}</ref>\n* Fixed-node method. One fixes the location of nodes (zeros) of the multiparticle wavefunction, and uses Monte Carlo methods to obtain an estimate of the energy of the ground state, subject to that constraint.<ref>{{cite journal |doi=10.1103/PhysRevLett.72.2442 |pmid=10055881 |bibcode=1994PhRvL..72.2442V |title=Fixed-Node Quantum Monte Carlo Method for Lattice Fermions |journal=Physical Review Letters |volume=72 |issue=15 |pages=2442–2445 |year=1994 |last1=Van Bemmel |first1=H. J. M. |last2=Ten Haaf |first2=D. F. B. |last3=Van Saarloos |first3=W. |last4=Van Leeuwen |first4=J. M. J. |last5=An |first5=G. |hdl=1887/5478}}</ref>\n* Majorana algorithms. Using Majorana fermion representation to perform Hubbard-Stratonovich transformations can help to solve the fermion sign problem of a class of fermionic many-body models.<ref>{{cite journal |doi=10.1103/PhysRevB.91.241117 |arxiv=1408.2269 |bibcode=2015PhRvB..91x1117L |title=Solving the fermion sign problem in quantum Monte Carlo simulations by Majorana representation |journal=Physical Review B |volume=91 |issue=24 |year=2015 |last1=Li |first1=Zi-Xiang |last2=Jiang |first2=Yi-Fan |last3=Yao |first3=Hong}}</ref><ref>{{Cite journal |doi=10.1103/PhysRevLett.117.267002 |pmid=28059531 |arxiv=1601.05780 |bibcode=2016PhRvL.117z7002L |title=Majorana-Time-Reversal Symmetries: A Fundamental Principle for Sign-Problem-Free Quantum Monte Carlo Simulations |journal=Physical Review Letters |volume=117 |issue=26 |pages=267002 |year=2016 |last1=Li |first1=Zi-Xiang |last2=Jiang |first2=Yi-Fan |last3=Yao |first3=Hong}}</ref>\n\n==See also==\n* [[Method of stationary phase]]\n* [[Oscillatory integral]]\n\n==References==\n<references/>\n\n{{DEFAULTSORT:Numerical Sign Problem}}\n[[Category:Statistical mechanics]]\n[[Category:Numerical artefacts]]\n[[Category:Unsolved problems in physics]]"
    },
    {
      "title": "Runge's phenomenon",
      "url": "https://en.wikipedia.org/wiki/Runge%27s_phenomenon",
      "text": "[[Image:Runge_phenomenon.svg|right|thumb|300px|The red curve is {{red|the Runge function}}.<br/> The blue curve is {{blue|a 5th-order interpolating polynomial}} (using six equally spaced interpolating points).<br/>\nThe green curve is {{green|a 9th-order interpolating polynomial}} (using ten equally spaced interpolating points).<br/>At the interpolating points, the error between the function and the interpolating polynomial is (by definition) zero. Between the interpolating points (especially in the region close to the endpoints 1 and −1), the error between the function and the interpolating polynomial gets worse for higher-order polynomials.]]\n\nIn the [[mathematics|mathematical]] field of [[numerical analysis]], '''Runge's phenomenon''' ({{IPA-de|ˈʁʊŋə|lang}}) is a problem of oscillation at the edges of an interval that occurs when using [[polynomial interpolation]] with polynomials of high degree over a set of equispaced interpolation points. It was discovered by [[Carl David Tolmé Runge]] (1901) when exploring the behavior of errors when using polynomial interpolation to approximate certain functions.<ref>{{ Citation\n| first = Carl\n| last = Runge\n| author-link = Carl David Tolmé Runge\n| year = 1901\n| title = Über empirische Funktionen und die Interpolation zwischen äquidistanten Ordinaten\n| journal = Zeitschrift für Mathematik und Physik\n| volume = 46\n| pages = 224–243\n| postscript = . }} available at [https://archive.org/details/zeitschriftfrma12runggoog www.archive.org]</ref>\nThe discovery was important because it shows that going to higher degrees does not always improve accuracy. The phenomenon is similar to the [[Gibbs phenomenon]] in Fourier series approximations.\n\n==Introduction==\nThe [[Weierstrass approximation theorem]] states that for every [[continuous function]] ''f''(''x'') defined on an [[interval (mathematics)|interval]] [''a'',''b''], there exists a set of [[polynomial]] functions ''P''<sub>''n''</sub>(''x'') for ''n''=0, 1, 2, &hellip;, each of degree at most ''n'', that approximates ''f''(''x'') with [[uniform convergence]] over [''a'',''b''] as ''n'' tends to infinity, that is,\n:<math>\\lim_{n \\rightarrow \\infty} \\left( \\max_{a \\leq x \\leq b} \\left| f(x) -P_n(x)\\right| \\right) = 0.</math>\n\nConsider the case where one desires to [[Interpolation|interpolate]] through ''n''+1 equispaced points of a function ''f''(''x'') using the ''n''-degree polynomial ''P''<sub>''n''</sub>(''x'') that passes through those points. Naturally, one might expect from Weierstrass' theorem that using more points would lead to a more accurate reconstruction of ''f''(''x''). However, this ''particular'' set of polynomial functions ''P''<sub>''n''</sub>(''x'') is not guaranteed to have the property of uniform convergence; the theorem only states that a set of polynomial functions exists, without providing [[Bernstein polynomial|a general method of finding one]].\n\nThe ''P''<sub>''n''</sub>(''x'') produced in this manner may in fact diverge away from ''f''(''x'') as ''n'' increases; this typically occurs in an oscillating pattern that magnifies near the ends of the interpolation points. This phenomenon is attributed to Runge.<ref>{{cite journal|author=Epperson, James|title=On the Runge example|journal=Amer. Math. Monthly|volume=94|year=1987|pages=329–341|url=http://www.maa.org/programs/maa-awards/writing-awards/on-the-runge-example|doi=10.2307/2323093}}</ref>\n\n==Problem==\n\nConsider the '''Runge function'''\n:<math>f(x) = \\frac{1}{1+25x^2}\\,</math>\n(a scaled version of the [[Witch of Agnesi]]).\nRunge found that if this function is [[interpolation|interpolated]] at equidistant points ''x''<sub>''i''</sub> between &minus;1 and 1 such that:\n\n:<math>x_i = \\frac{2i}{n} - 1,\\quad i \\in \\left\\{ 0, 1, \\dots, n \\right\\}</math>\n\nwith a [[polynomial]] ''P''<sub>''n''</sub>(''x'') of degree &le;&nbsp;''n'', the resulting interpolation oscillates toward the end of the interval, i.e. close to &minus;1 and 1. It can even be proven that the interpolation error increases (without bound) when the degree of the polynomial is increased:\n\n:<math>\\lim_{n \\rightarrow \\infty} \\left( \\max_{-1 \\leq x \\leq 1} | f(x) -P_n(x)| \\right) = +\\infty.</math>\n\nThis shows that high-degree polynomial interpolation at equidistant points can be troublesome.\n\n===Reason===\nRunge's phenomenon is the consequence of two properties of this problem. \n* The magnitude of the ''n''-th order derivatives of this particular function grows quickly when ''n'' increases.\n* The equidistance between points leads to a [[Lebesgue constant (interpolation)|Lebesgue constant]] that increases quickly when ''n'' increases.\nThe phenomenon is graphically obvious because both properties combine to increase the magnitude of the oscillations.\n\nThe error between the generating function and the interpolating polynomial of order ''n'' is given by\n:<math>f(x) - P_n(x) = \\frac{f^{(n + 1)}(\\xi)}{(n + 1)!} \\prod_{i=0}^{n} (x - x_i) </math>\n\nfor some <math>\\xi</math> in (&minus;1,&nbsp;1). Thus, \n:<math>\n  \\max_{-1 \\leq x \\leq 1} |f(x) - P_n(x)| \\leq \n    \\max_{-1 \\leq x \\leq 1} \\frac{\\left|f^{(n + 1)}(x)\\right|}{(n + 1)!} \n    \\max_{-1 \\leq x \\leq 1} \\prod_{i=0}^n |x - x_i|\n</math>.\n\nDenote by <math>w_n(x)</math> the nodal function\n:<math>w_n(x) = (x - x_0)(x - x_1)\\cdots(x - x_n)</math>\n\nand let <math>W_n</math> be the maximum of the <math>w_n</math> function:\n:<math>W_n=\\max_{-1 \\leq x \\leq 1} w_n(x)</math>.\n\nThen it can be proved that, if equidistant nodes are used,<ref>{{cite book|last1=Heath|first1=Michael|authorlink=Michael Heath (computer scientist)|title=Scientific Computing|date=2000|publisher=McGraw-Hill|isbn=0072399104|page=324}}</ref> then\n:<math>W_n \\leq h^n \\frac{(n - 1)!}{4} </math>\n\nwhere <math>h = 2/(n - 1)</math> is the step size.\n\nMoreover, assume that the ''n''-th derivative of <math>f</math> is bounded, i.e.\n:<math>\\max_{-1 \\leq x \\leq 1} f^{(n)}(x) \\leq M_n</math>.\n\nTherefore, \n:<math>\\max_{-1 \\leq x \\leq 1} |f(x) - P_{n-1}(x)| \\leq M_n \\frac{h^n}{4n}</math>.\n\nBut the magnitude of the n-th derivative of Runge's function increases when n increases, and very fast. The result is that the product in the previous equation tends to infinity when ''n'' tends to infinity.\n\nAlthough often used to explain the Runge phenomenon, the fact that the upper bound of the error goes to infinity does not necessarily \nimply, of course, that the error itself also diverges with ''n.''\n\n==Mitigations to the problem==\n\n===Change of interpolation points===\nThe oscillation can be minimized by using nodes that are distributed more densely towards the edges of the interval, specifically, with asymptotic density (on the interval [−1,1]) given by the formula<ref>{{Citation | last1=Berrut | first1=Jean-Paul | last2=Trefethen | first2=Lloyd N. | author2-link=Lloyd N. Trefethen | title=Barycentric Lagrange interpolation | doi=10.1137/S0036144502417715 | year=2004 | journal=SIAM Review | issn=1095-7200 | volume=46 | pages=501–517 | issue=3| citeseerx=10.1.1.15.5097 }}</ref>\n<math>\n1/\\sqrt{1-x^2}\n</math>.\nA standard example of such a set of nodes is [[Chebyshev nodes]], for which the maximum error in approximating the Runge function is guaranteed to diminish with increasing polynomial order. The phenomenon demonstrates that high degree polynomials are generally unsuitable for interpolation with equidistant nodes.\n\n===Use of piecewise polynomials===\nThe problem can be avoided by using [[spline curve]]s which are piecewise polynomials. When trying to decrease the interpolation error one can increase the number of polynomial pieces which are used to construct the spline instead of increasing the degree of the polynomials used.\n\n===Constrained minimization===\nOne can also fit a polynomial of higher degree (for instance, with <math>n</math> points use a polynomial of order <math>N = n^2</math> instead of <math>n+1</math>), and fit an interpolating polynomial whose first (or second) derivative has minimal [[Lp space|<math>L^2</math> norm]].\n\nA similar approach is to minimize a constrained version of the <math>L^p</math> distance between the polynomial's <math>m^{\\mathrm{th}}</math> derivative and the mean value of it's <math>m^{\\mathrm{th}}</math> derivative. Explicitly, to minimize\n:<math>\nV_p = \\int_a^b \\left|\\frac{\\operatorname{d}^m P_N(x)}{\\operatorname{d} x^m} - \\frac{1}{b-a} \\int_a^b  \\frac{\\operatorname{d}^m P_N(z)}{\\operatorname{d} z^m} \\operatorname{d}z\\right|^p \\operatorname{d} x - \\sum_{i=1}^n \\lambda_i \\, \\left(P_N(x_i) - f(x_i)\\right),\n</math>\nwhere <math> N \\ge n - 1</math> and <math> m < N </math>, with respect to the polynomial coefficients and the [[Lagrange multiplier]]s, <math>\\lambda_i</math>. When <math>N = n - 1</math>, the constraint equations generated by the Lagrange multipliers reduce <math>P_N(x)</math> to the minimum polynomial that passes through all <math>n</math> points. At the opposite end, <math>\\lim_{N \\rightarrow \\infty} P_N(x)</math> will approach a form very similar to a piecewise polynomials approximation. When <math>m=1</math>, in particular, <math>\\lim_{N \\rightarrow \\infty} P_N(x)</math> approaches the linear piecewise polynomials, i.e. connecting the interpolation points with straight lines.\n\nThe role played by <math>p</math> in the process of minimizing <math>V_p</math> is to control the importance of the size of the fluctuations away from the mean value. The larger <math>p</math> is, the more large fluctuations are penalized compared to small ones. The greatest advantage of the Euclidean norm, <math>p=2</math>, is that it allows for analytic solutions and it guarantees that <math>V_p</math> will only have a single minimum. When <math>p\\neq 2</math> there can be multiple minima in <math>V_p</math>, making it difficult to ensure that a particular minimum found will be the [[Maxima and minima|global minimum]] instead of a local one.\n\n===Least squares fitting===\nAnother method is fitting a polynomial of lower degree using the method of [[least squares]]. Generally, when using <math>m</math> equidistant points, if <math>N<2\\sqrt{m}</math> then least squares approximation <math>P_N(x)</math> is well-conditioned.<ref>{{Citation | last1=Dahlquist | first1=Germund | last2=Björk | first2=Åke | author1-link=Germund Dahlquist | title=Numerical Methods | year=1974 | isbn=0-13-627315-7 | chapter=4.3.4. Equidistant Interpolation and the Runge Phenomenon | pages=101–103}}</ref>\n\n===Bernstein polynomial===\nUsing [[Bernstein polynomial]]s, one can uniformly approximate every continuous function in a closed interval, although this method is rather computationally expensive.\n\n==Related statements from the [[approximation theory]]==\nFor every predefined table of interpolation nodes there is a continuous function for which the sequence of interpolation polynomials on those nodes diverges.<ref>{{citation | last1 = Cheney | first1 = Ward | last2 = Light | first2 = Will | title = A Course in Approximation Theory | page = 19 | publisher = Brooks/Cole | year = 2000 | isbn = 0-534-36224-9 | url = http://www.ams.org/bookstore-getitem/item=gsm-101}}</ref> For every continuous function there is a table of nodes on which the interpolation process converges. {{Citation needed|date=March 2014}} Chebyshev interpolation (i.e., on [[Chebyshev nodes]]) converges uniformly for every absolutely continuous function.\n\n==See also==\n* Compare with the [[Gibbs phenomenon]] for sinusoidal basis functions\n* [[Taylor series]]\n* [[Chebyshev nodes]]\n* [[Stone–Weierstrass theorem]]\n\n==References==\n<references />\n\n[[Category:Interpolation]]\n[[Category:Continuous mappings]]\n[[Category:Numerical artefacts]]\n\n[[de:Polynominterpolation#Runges Phänomen]]"
    },
    {
      "title": "Alpha to coverage",
      "url": "https://en.wikipedia.org/wiki/Alpha_to_coverage",
      "text": "{{refimprove|date=February 2013}}\n'''Alpha to coverage''' is a [[Multisample anti-aliasing|multisampling]] [[computer graphics]] technique, which uses the [[Alpha compositing|alpha channel]] of [[Texture mapping|textures]] as a [[Image mask|coverage mask]] for [[aliasing|anti-aliasing]]. This particular technique is useful for situations where dense foliage or grass must be rendered in a [[video game]].<ref>{{cite web\n| url=http://msdn.microsoft.com/en-us/library/windows/desktop/bb205072%28v=vs.85%29.aspx#Alpha_To_Coverage\n| title=Configuring Blending Functionality (Windows)\n| publisher=[[Microsoft Developer Network]]\n| quote=''Alpha-to-coverage is a multisampling technique that is most useful for situations such as dense foliage where there are several overlapping polygons that use alpha transparency to define edges within the surface''\n| accessdate=2013-01-27}}</ref>\n\nAlpha to coverage multisampling is based on regular multisampling, except that the [[Alpha compositing|alpha]] coverage mask is  [[Bitwise AND#AND|ANDed]] with the multisample mask. Alpha-to-coverage converts the [[Alpha compositing|alpha]] component output from the [[pixel shader]] to a coverage mask. When the [[Multisample anti-aliasing|multisampling]] is applied each output fragment gets a transparency of 0 or 1 depending on alpha coverage and the [[Multisample anti-aliasing|multisampling]] result.\n\n==See also==\n* [[Spatial anti-aliasing]]\n* [[Multisample anti-aliasing]]\n\n==References==\n{{reflist}}\n\n==External links==\n* [https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch04.html GPU Gems 3 ‒ Chapter 4. Next-Generation SpeedTree Rendering]\n* [http://www.pcgameshardware.com/aid,685997/Street-Fighter-IV-PC-explained-in-detail/News/ Street Fighter IV PC explained in detail]\n\n[[Category:Image processing]]\n[[Category:Computer graphic artifacts]]\n\n\n{{compu-graphics-stub}}"
    },
    {
      "title": "Anti-Grain Geometry",
      "url": "https://en.wikipedia.org/wiki/Anti-Grain_Geometry",
      "text": "{{Refimprove|date=October 2008}}\n{{Infobox software\n| developer              = Maxim Shemanarev\n| latest release version = 2.5.0\n| latest release date    = {{release date and age|2006|10|01}}\n| programming language   = [[C++]]\n| operating system       = [[Cross-platform]]\n| genre                  = [[Graphics library]]\n| license                = [[GNU General Public License|GPL]]\n}}\n\n'''Anti-Grain Geometry''' ('''AGG''') is a high-quality 2D [[rendering (computer graphics)|rendering library]] written in [[C++]]. It features [[Spatial anti-aliasing|anti-aliasing]] and [[sub-pixel resolution]]. It is not a [[graphics library]], per se, but rather a framework to build a graphics library upon.\n\nThe library is [[Cross-platform|operating system independent]] and renders to an abstract memory object. It comes with examples interfaced to the [[X Window System]], [[Microsoft Windows]], [[Mac OS X]], [[AmigaOS]], [[BeOS]], [[Simple DirectMedia Layer|SDL]]. The examples also include an [[SVG]] viewer.\n\nThe design of AGG uses C++ templates only at a very high level, rather than extensively, to achieve the flexibility to plug custom classes into the rendering pipeline, without requiring a rigid class hierarchy, and allows the compiler to inline many of the method calls for high performance. For a library of its complexity, it is remarkably lightweight: it has no dependencies above the standard C++ libraries and it avoids the [[C++ STL]] in the implementation of the basic algorithms. The implicit interfaces are not well documented, however, and this can make the learning process quite cumbersome.\n\nWhile AGG version 2.5 is licensed under the [[GNU General Public License]], version 2 or greater, AGG version 2.4 is still available under the [[BSD licenses|3-clause BSD license]] and is virtually the same as version 2.5.\n\n== History ==\nActive development of the AGG codebase stalled in 2006, around the time of the v2.5 release, due to shifting priorities of its primary developer Maxim Shemanarev. Shemanarev remained active in the community until his sudden death in November, 2013.<ref>{{cite web|url=http://rsdn.ru/forum/life/5377743.flat |title=Максим Шеманарев aka McSeem2, 1966-2013 - О жизни - RSDN |date=4 March 2016 |publisher= |deadurl=bot: unknown |archiveurl=https://web.archive.org/web/20160304023601/http://rsdn.ru/forum/life/5377743.flat |archivedate=4 March 2016 |df= }}</ref> Development has continued on a fork of the more liberally licensed v2.4 on SourceForge.net.<ref>{{cite web|url=http://sourceforge.net/projects/agg|title=Anti-Grain Geometry Library|publisher=}}</ref>\n\n== Usage ==\n* The [[Haiku (operating system)|Haiku operating system]] uses AGG in its windowing system.\n* It is one of the renderers available for use in [[GNU]]'s [[Gnash (software)|Gnash]] Flash player.\n* Graphical version of [[Rebol]] language interpreter is using AGG for scalable vector graphics DRAW dialect.\n* [[Hilti]] uses it in some of their rebar detection tools, like the PS 1000.\n* [[Matplotlib]] uses AGG as its back-end rendering engine.<ref>{{cite web\n| title = Matplotlib 1.1 documentation\n| url = http://matplotlib.sourceforge.net/users/installing.html\n| accessdate = 2011-04-08\n| archiveurl= https://web.archive.org/web/20110515162632/http://matplotlib.sourceforge.net/users/installing.html| archivedate= 15 May 2011 <!--DASHBot-->| deadurl= no}}</ref>\n* [[fpGUI|fpGUI Toolkit]] has an optional AggPas back-end rendering engine.<ref>{{cite web\n| title = fpGUI Toolkit news\n| url = http://fpgui.sourceforge.net/\n| accessdate = 2013-05-14\n| archiveurl= https://web.archive.org/web/20120423125505/http://fpgui.sourceforge.net/| archivedate= 23 April 2012 <!--DASHBot-->| deadurl= no}}</ref> Work is being done to make AggPas the default or sole rendering engine for fpGUI.\n* [[Mapnik]], the toolkit that renders the maps on the [[OpenStreetMap]] website, uses AGG for all its map rendering.\n* [[HTTPhotos]] uses AGG to scale photos.\n* Pdfium, the PDF rendering engine used by [[Google Chrome]] makes use of AGG<ref>{{cite web|title=Pdfium Source|url=https://pdfium.googlesource.com/pdfium/+/master/core/fxge/agg/fx_agg_driver.h|accessdate=23 Dec 2016}}</ref>\n* Graphics Mill, the .NET imaging SDK uses AGG as its drawing engine.<ref>{{cite web|title=Graphics Mill Documentation|url=http://www.graphicsmill.com/docs/gm/copyright-notices.htm|accessdate=16 August 2015}}</ref>\n* [[FL_Studio|Image-Line FL Studio]], a digital audio workstation, since version 10.8 released on September 30, 2012, uses AGG for drawing.<ref>{{cite web\n| title = History - FL Studio 10.8 (beta)\n| url = https://www.image-line.com/flstudio/history.php?entry_id=1363185606\n| accessdate = 2019-05-04\n| archiveurl= https://web.archive.org/web/20190503185134/https://www.image-line.com/flstudio/history.php?entry_id=1363185606| archivedate= 4 May 2019 <!--DASHBot-->| deadurl= no}}</ref>\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* {{Official website|antigrain.com}}\n* [http://agg.sourceforge.net/antigrain.com/index.html AGG at sourceforge] a copy of original website (created due to discontinuation of original site)\n* [http://sourceforge.net/projects/agg/ Updated fork of v2.4] that maintains the original BSD license.\n* [https://github.com/MatterHackers/agg-sharp agg-sharp] Is a c# port of the c++ library.\n* [http://www.aggpas.org AggPas] is an Object Pascal port of the c++ library.\n* [http://sourceforge.net/projects/aggpasmod AggPasMod] is a modernized Object Pascal port of the c++ library.\n* [https://github.com/dotmorten/AntiGrainRT AntiGrainRT] is a WinRT wrapper for Windows Store and Windows Phone.\n\n[[Category:Anti-aliasing]]\n[[Category:Computer graphic artifacts]]\n[[Category:Graphics software]]"
    },
    {
      "title": "Attribute clash",
      "url": "https://en.wikipedia.org/wiki/Attribute_clash",
      "text": "{{original research|date=February 2011}}\n{{No footnotes|date=December 2016}}\n[[File:MSX Computer Color Limit.gif|thumb|The effect of attribute clash on MSX 1 systems when using the 256×192 Highres mode of MSX 1 (in this example blocks of 8×8 background pixels share the same colour, so the effect is similar to a ZX Spectrum)]]\n\n'''Attribute clash''' (also known as '''colour clash''' or '''bleeding''') is a display [[Visual artifact|artifact]] caused by limits in the graphics circuitry of some colour 8-bit [[home computer]]s, most notably the [[Sinclair Research|Sinclair]] [[ZX Spectrum]], where it meant that only two colours could be used in any 8×8 tile of pixels. The effect was also noticeable on [[MSX]] software and in some [[Commodore 64]] titles. Workarounds to prevent this limit from becoming apparent have since been considered an element of Spectrum programmer culture.\n\nThis problem also happens with the \"semigraphic modes\" (text modes with graphics features) of the [[TRS-80 Color Computer|Color Computer]] and [[Dragon 32/64|Dragon]], but those computers also have non-attributed graphics and with better resolution. Several video game consoles of the era had such video modes that caused such limitations, but usually allowed more than two colours per tile: the NES (Famicom) had only one mode, which was also \"semigraphic\", and allowed four colours per 16×16 \"block\" (group of four 8×8 tile) but 16 per screen. The Super NES allowed 16 colours per tile but 256 per screen (among other improvements), and this made the artefact much harder to notice, if at all (except for those who had to program the device).\n\n==Causes==\nAttribute clash on the ZX Spectrum was caused by its idiosyncratic display memory layout, designed in such a way as to minimise memory use of the [[frame buffer]], and optimise for text display instead of graphics. Rather than limit the colour palette to conserve memory, Sinclair's design stored [[pixel]] [[bitmap]] and colour information in separate areas of memory. While the bitmap specified the state of individual pixels (either on or off), the colour information (or \"attributes\") corresponded to the text character matrix—24 rows of 32 columns—with one [[byte]] per 8x8 pixel character cell. This byte encoded two 3-bit values, known as ''INK'' (foreground colour) and ''PAPER'' (background colour) after the [[BASIC]] instructions used to define the colour values. Two other binary values were included in an attribute; a ''BRIGHT'' bit indicating one of two brightness levels for the two colours, and a ''FLASH'' bit, which, when set, caused the two colours to be swapped at regular intervals. This scheme provided 15 different colours: the eight combinations of red, green and blue at two brightness levels (except for black, which appeared the same at both brightness). Thus, each 8x8 pixel block could only contain 2 colours from the 15 available, which must both be from either the BRIGHT or non-BRIGHT halves of the palette. Trying to add a third colour in an 8x8 pixel area would result in the three colours being reduced to two by the ZX Spectrum. \n\nThe ZX Spectrum used 6144 bytes for pixel information, with one byte representing a row of eight pixels, and 768 bytes used for the colour attributes, thus giving a total of 6912 bytes for the entire graphics display, a relatively small total for a computer of the Spectrum's era with \"colour\" capabilities. This graphics architecture was retained right through to Sinclair and [[Amstrad]]'s later redesigns of the Spectrum, up until Amstrad's final model, the ZX Spectrum +3, despite subsequent models having contained 128&nbsp;[[KiB]] of RAM, reducing the need to save memory in this manner. The architecture was retained to prevent loss of [[backward compatibility]].\n\nAttributes were used by a variety of other computers and consoles, including the [[Commodore 64]], the [[MSX]] and [[Nintendo Entertainment System|NES]], although the size of the attribute blocks and the number of colours per block varied. However, with the use of hardware [[Sprite (computer graphics)|sprites]] and [[scrolling]], attribute clash could be avoided.\n\nThe [[Thomson MO5]] and [[Thomson TO7|TO7]] microcomputers, the [[Tangerine Computer Systems#Oric-1|Oric 1]], the [[MSX#MSX 1|MSX 1]] [[Computer architecture|architecture]], and other systems based on the [[Texas Instruments TMS9918]] [[video display controller]] display a very similar constraint: for each group of eight pixels horizontally, only two colours out of 16 are available. giving a similar but less severe effect than with the Spectrum. The MSX 1 did not have just one single colour attribute byte available for a whole 8x8 pixel area, as was the case with the Sinclair Spectrum, but eight, one attribute byte for each 8×1 pixel area. Thus, while the Spectrum was limited to one colour pair for a square area of 8x8 pixels, the MSX 1 was only limited to one colour pair for a \"line\" of eight adjacent pixels. In addition MSX1 could use sprites which were not bound to any attribute clash problems (although MSX 1 sprites did have their own limitations, such as being monochrome). \n\nIn practice this technical advantage often did not help MSX 1 systems to produce better pictures. The problem for the MSX 1 was that many European software companies who converted Spectrum games to MSX 1 ignored all the improvements the MSX 1 had over the Spectrum, and thus the resulting MSX 1 versions had the same amount of attribute clash as the original Spectrum games. ([[Jack the Nipper II: In Coconut Capers]] is one example of this.) To ease conversion, the software developers simply copied the single attribute byte value of the Spectrum to all eight corresponding attribute bytes of the MSX 1. For the same reason, the software companies also ignored the sprite capabilities of the MSX 1, and because the video display capabilities were otherwise quite similar (256×192 resolution, 16 colours), both systems produced virtually identical displays for the same game. In contrast, Japanese MSX 1 games did use all the capabilities of MSX 1, often resulting in better looking games.\n\n==Effects==\nTo avoid attribute clash, static graphic displays had to be constructed with care. Finely-detailed colour graphics were impossible, as colour could only be applied in 8×8 pixel blocks. Careful design could achieve impressive results, as could synchronising colour changes to the refresh rate of the display—usually a television set.\n\nHowever, animated displays were more difficult—a distinct drawback in a machine whose primary use was playing [[video game]]s. If just one pixel in an 8×8 block was recoloured because a moving part of the display touched it, the entire block would change colour. Thus detailed moving graphics caused large ugly fringes of rapidly changing colours to follow them around.\n\n==Workarounds==\nEarly software simply ignored the problem. Later, the standard workaround was to use colour for static display elements—such as a decorative border around the edges of the screen, which might include score displays and so on, or some form of instrumentation—with a smaller central [[monochrome]] area containing all the animated graphics. This also made graphics faster, as less of the screen had to be updated—both a smaller region, plus only changing pixel information and leaving the colour area untouched.\n\nSome late Spectrum software, such as FTL's ''[[Light Force (video game)|Light Force]]'', used extremely careful graphics design to achieve full-colour moving graphics, essentially by limiting both the design of the onscreen elements and their paths of motion to 8×8 colour resolution boundaries. The moving elements were thus relatively large and rather blocky or squarish, and their movement was constrained, but this was not visually obvious and the sight of moving full-colour graphics was hugely impressive to Spectrum owners.\n\nNo mainstream developers were able to find a suitable all-round fix for the attribute clash problem, instead preferring to use the monochrome graphics method when fast, clear graphics were needed, and full-colour graphics when the situation permitted.\n\nIt was possible by paying careful attention to timing to modify the attribute area of RAM at certain specific times as the display was drawn - let the display hardware draw one line of the display, then change the attribute RAM before the next line is drawn to give the effect of different attributes for each individual line. These changes had to be done in software and were time-consuming to program, meaning that this technique was usually limited to special effects. This technique was also very popular in the [[demoscene]].\n\n==The problem and solutions==\nMost  games before 1987 ignored attribute clash. Some later games, such as ''[[Knight Tyme]]'' and ''[[Three Weeks in Paradise]]'' allowed players to select between two modes of attribute clash: one which ignored main character attributes, blending the character into the background and vice versa, prioritising the characters colour scheme over the background imagery.\n\nAnother workaround was to simply render the graphics in two colours, otherwise known as monochrome, as done with the Spectrum version of ''[[Knight Lore]]'' in 1984.\n\nMany games used full-colour backgrounds and \"character scrolling\" (where the environment was scrolled eight pixels at a time), but monochrome sprites that were effectively transparent, as in ''[[Double Dragon (arcade game)|Double Dragon]]'', were drawn in such a way so they stand out, avoiding dependence on colour. Many games used this method with smooth pixel-by-pixel scrolling, but the attribute clash as elements of one character block were \"passed\" to the next were clearly visible.\n\nA prominent (and less successful) example of the use of full-colour graphics was the Spectrum conversion of ''[[Altered Beast]]''. The game suffers from considerable attribute clash.\n\nProgrammer [[Don Priestley]] developed a distinctive style for several of his games by using large, cartoon-like sprites which were carefully designed to span whole character blocks without appearing unduly square. A disadvantage of this technique was that the gameplay had to be designed around the graphics, and so it was not useful for [[Porting|ports]] from other platforms. Games that used this technique included ''Popeye'', ''[[The Trap Door (game)|The Trap Door]]'', ''Through the Trapdoor'', and ''[[Flunky (computer game)|Flunky]]''. Other developers who used a similar technique included [[Mike Singleton]], with ''[[Dark Sceptre]]'', and Gang of Five, with ''[[Dan Dare: Pilot of the Future]]''.\n\nIn 1994 programmer Igor Maznitsa developed a multi-CPU concept platform \"ZX-Poly\" based on ZX-Spectrum-128, the platform allows to avoid attribute clash and even colourise many old games without changes in executable code.\n\n==References==\n{{Reflist}}\n\n==Sources==\n* {{cite web |url=http://www.worldofspectrum.org/faq/reference/reference.htm |title=FAQ: Reference |website=WorldOfSpectrum.org}}\n* {{cite web |url=https://www.academia.edu/12429493/Arcade_Colour_Illustration_and_Attribute_Clash_1979_-_89 |title=Arcade Colour, Illustration and Attribute Clash 1979 - 89 |work=Academia.edu |first=David |last=Surman}}\n* {{cite news |url=https://www.theregister.co.uk/Print/2012/04/23/retro_week_sinclair_zx_spectrum_at_30/ |title=Happy 30th Birthday, Sinclair ZX Spectrum |work=The Register |date=2012-04-23 |first=Tony |last=Smith}}\n* [https://github.com/raydac/zxpoly Sources of the ZX-Poly emulator and description of the platform]\n\n[[Category:Computer graphic artifacts]]\n[[Category:ZX Spectrum]]"
    },
    {
      "title": "Clipping (computer graphics)",
      "url": "https://en.wikipedia.org/wiki/Clipping_%28computer_graphics%29",
      "text": "{{original research|date=August 2015}}\n{{refimprove|date=August 2015}}\n'''Clipping''', in the context of [[computer graphics]], is a method to selectively enable or disable [[rendering (computer graphics)|rendering operations]] within a defined [[region of interest]].  Mathematically, clipping can be described using the terminology of [[constructive geometry]].  A [[rendering algorithm]] only draws [[pixel]]s in the [[intersection]] between the clip region and the scene model.  Lines and surfaces outside the [[view volume]] (aka. frustum) are removed.<ref name=\"bertoline-graphics-comm\">{{cite book \n|last1=Bertoline |first1=Gary \n|last2=Wiebe |first2=Eric\n|title=Fundamentals of Graphics Communication \n|page=G-3\n|date=2002 |edition=3rd |isbn=0-07-232209-8\n|url=http://mhhe.com/engcs/drawgr/bertoline/index.htm\n|publisher=McGraw-Hill\n|accessdate=2015-01-04 }}</ref>\n\nClip regions are commonly specified to improve render performance.  A well-chosen clip allows the renderer to save time and energy by skipping calculations related to pixels that the user cannot see.  Pixels that will be drawn are said to be within the clip region.  Pixels that will not be drawn are outside the clip region.  More informally, pixels that will not be drawn are said to be \"clipped.\"\n\n== Clipping in 2D graphics ==\nIn two-dimensional graphics, a clip region may be defined so that pixels are only drawn within the boundaries of a [[window (computing)|window]] or frame.  Clip regions can also be used to selectively control pixel rendering for aesthetic or artistic purposes.  In many implementations, the final clip region is the composite (or intersection) of one or more application-defined shapes, as well as any system hardware constraints\n\nIn one example application, consider an image editing program.  A user application may render the image into a viewport.  As the user zooms and scrolls to view a smaller portion of the image, the application can set a clip boundary so that pixels outside the viewport are not rendered.  In addition, [[GUI widget]]s, overlays, and other [[windowing system|windows or frames]] may obscure some pixels from the original image.  In this sense, the clip region is the composite of the application-defined \"user clip\" and the \"device clip\" enforced by the system's software and hardware implementation.<ref name=\"java-awt-graphics-javadoc\"/>  Application software can take advantage of this clip information to save computation time, energy, and memory, avoiding work related to pixels that aren't visible.\n\n== Clipping in 3D graphics ==\n[[Image:ViewFrustum.svg|thumb|right|A view frustum, with near- and far- clip planes.  Only the shaded volume is rendered.]]\nIn three-dimensional graphics, the terminology of clipping can be used to describe many related features.  Typically, \"clipping\" refers to operations in the plane that work with rectangular shapes, and \"culling\" refers to more general methods to selectively process scene model elements.  This terminology is not rigid, and exact usage varies among many sources.\n\nScene model elements include geometric primitives: points or vertices; line segments or edges; polygons or faces; and more abstract model objects such as curves, splines, surfaces, and even text.  In complicated scene models, individual elements may be selectively disabled (clipped) for reasons including visibility within the viewport ([[frustum culling]]); orientation ([[backface culling]]), obscuration by other scene or model elements ([[occlusion culling]], depth- or \"z\" clipping).  Sophisticated algorithms exist to efficiently detect and perform such clipping.  Many optimized clipping methods rely on specific hardware acceleration logic provided by a [[graphics processing unit]] (GPU).\n\nThe concept of clipping can be extended to higher dimensionality using methods of abstract [[algebraic geometry]].\n\n=== Near clipping ===\n\nBeyond projection of vertices & 2D clipping, near clipping is required to correctly rasterise 3D primitives; this is because vertices may have been projected behind the eye. Near clipping ensures that all the vertices used have valid 2D coordinates. Together with '''far-clipping''' it also helps prevent overflow of depth-buffer values. Some early [[texture mapping hardware]] (using [[forward texture mapping]]) in video games suffered from complications associated with near clipping and [[UV coordinates]].\n\n=== Occlusion clipping (Z- or depth clipping) ===\n{{main|depth buffer|occlusion culling}}\nIn 3D computer graphics, \"Z\" often refers to the depth axis in the system of coordinates centered at the viewport origin: \"Z\" is used interchangeably with \"depth\", and conceptually corresponds to the distance \"into the virtual screen.\"  In this coordinate system, \"X\" and \"Y\" therefore refer to a conventional [[cartesian coordinate]] system laid out on the user's screen or [[viewport]].  This viewport is defined by the geometry of the [[viewing frustum]], and parameterizes the [[field of view]].\n\nZ-clipping, or depth clipping, refers to techniques that selectively render certain scene objects based on their depth relative to the screen.  Most graphics toolkits allow the programmer to specify a \"near\" and \"far\" clip depth, and only portions of objects between those two planes are displayed.  A creative application programmer can use this method to render visualizations of the interior of a 3D object in the scene.  For example, a [[medical imaging]] application could use this technique to render the organs inside a human body.  A video game programmer can use clipping information to accelerate game logic.<ref name=\"gpu-gems-1-29\"/>  For example, a tall wall or building that occludes other game entities can save GPU time that would otherwise be spent transforming and texturing items in the rear areas of the scene; and a tightly integrated software program can use this same information to save CPU time by optimizing out game logic for objects that aren't seen by the player.<ref name=\"gpu-gems-1-29\"/>\n\n==Importance of clipping in video games==\n{{unreferenced-section|date=August 2015}}\nGood clipping strategy is important in the development of [[video games]] in order to maximize the game's [[frame rate]] and visual quality. Despite [[graphics processing unit|GPU chips]] that are faster every year, it remains computationally expensive to [[transformation (mathematics)|transform]], [[texture mapping|texture]], and [[shader|shade]] polygons, especially with the multiple texture and shading passes common today. Hence, [[video game developer|game developer]]s must live within a certain \"budget\" of polygons that can be drawn each video frame.\n\nTo maximize the game's visual quality, developers prefer to let aesthetic choices, rather than hardware limitation, dictate the polygon budget.  Optimizations that save performance therefore or take advantage of graphics pipeline acceleration improve the player's experience. \n\nClipping [[optimization (computer science)|optimization]] can speed up the rendering of the current scene, economizing the use of renderer time and memory within the hardware's capability. Programmers often devise clever [[heuristic (computer science)|heuristic]]s to speed up the clipper, as it is sometimes computationally prohibitive to use line casting or [[Ray tracing (graphics)|ray tracing]] to determine with 100% accuracy which polygons are not within the camera's [[field of view]].  Spatially aware data structures, such as [[octree]]s, [[R* tree]]s, and [[bounding volume hierarchies]] can be used to partition scenes into rendered and non-rendered areas (allowing the renderer to reject or accept entire tree nodes where appropriate).\n\nOcclusion optimizations based on viewpoint geometry may introduce artifacts if the scene contains reflective surfaces.  A common technique, [[reflection mapping]], can optionally use existing occlusion estimates from the viewpoint of the main view frustum; or, if performance allows, a new occlusion map can be computed from a separate camera position.\n\nFor historical reasons, some video games used [[collision detection]] optimizations with identical logic and hardware acceleration as the occlusion test.  As a result, non-specialists have incorrectly used the term \"clip\" (and its antonym \"[[noclip|no clipping]]\") to refer to collision detection.\n\n==Algorithms==\n\n* '''[[Line clipping]] algorithms''':\n** [[Cohen–Sutherland]]\n** [[Liang–Barsky]]\n** [[Line clipping#Fast clipping|Fast-clipping]]\n** [[Line clipping#Cyrus–Beck|Cyrus–Beck]]\n** [[Nicholl–Lee–Nicholl]]\n** [[Line_clipping#Skala|Skala]]\n** [[Line_clipping#O(lg N) algorithm|O(lg N) Algorithm]]\n* '''Polygon clipping algorithms''':\n** [[Greiner-Hormann clipping algorithm|Greiner-Hormann]]\n** [[Sutherland–Hodgman]]\n** [[Weiler–Atherton]]\n** [[Vatti_clipping_algorithm|Vatti]]\n*Rendering Methodologies\n**[[Painter's algorithm]]\n\n==See also==\n*[[Boolean operations on polygons]]\n*[[Bounding volume]]\n*[[Clip Space]]\n*[[Guard-band clipping]]\n*[[Hidden surface determination]]\n*[[Pruning (decision trees)]]\n*[[Visibility (geometry)]]\n\n== Further reading ==\n* GPU Gems: Efficient Occlusion Culling <ref name=\"gpu-gems-1-29\">{{cite book |last=Sekulic |first=Dean |date=2004 |title=GPU Gems|url=https://developer.nvidia.com/content/gpu-gems |chapter=Efficient Occlusion Culling|chapterurl=http://http.developer.nvidia.com/GPUGems/gpugems_ch29.html |location= |publisher=Pearson |page= |isbn= |accessdate=2015-01-02 }}</ref>\n* Clipping in [[Java AWT]]: [http://docs.oracle.com/javase/8/docs/api/java/awt/Graphics.html#clipRect-int-int-int-int- <tt>java.awt.Graphics.clipRect</tt> JavaDoc] <ref name=\"java-awt-graphics-javadoc\">{{cite web |url=http://docs.oracle.com/javase/8/docs/api/java/awt/Graphics.html#clipRect-int-int-int-int- |title=java.awt.Graphics.clipRect |publisher=Oracle | date=2014}}</ref>\n* Clipping in UIKit for [[iOS]] (2D): [https://developer.apple.com/library/ios/documentation/UIKit/Reference/UIKitFunctionReference/index.html#//apple_ref/c/func/UIRectClip <tt>UIRectClip</tt>]\n* Clipping in SceneKit for iOS (3D): [https://developer.apple.com/library/mac/documentation/SceneKit/Reference/SCNCamera_Class/index.html#//apple_ref/occ/instp/SCNCamera/zNear <tt>SCNCamera</tt> ''(Adjusting Camera Perspective)'']\n* Clipping in [[OpenGL]]: [https://www.opengl.org/archives/resources/faq/technical/clipping.htm OpenGL Technical FAQs: ''Clipping, Culling, and Visibility Testing'']<ref name=\"OpenGL-tech-faq-10\">{{cite web |url=https://www.opengl.org/archives/resources/faq/technical/clipping.htm |title=Clipping, Culling, and Visibility Testing|author=Paul Martz |date=2001 |website=OpenGL.org|publisher= |accessdate=2015-01-02}}\n</ref>\n\n==References==\n<references />\n\n[[Category:Clipping (computer graphics)|*]]\n[[Category:Computer graphics]]\n[[Category:Computer graphic artifacts]]"
    },
    {
      "title": "Colour banding",
      "url": "https://en.wikipedia.org/wiki/Colour_banding",
      "text": "{{refimprove|date=May 2013}}\n[[File:Colour_banding_example01.png|thumb|right|418px|An illustration of colour banding<br>(disable browser zoom to see image without rescaling)]]\n[[File:The Foston by pass on the A1 North - geograph.org.uk - 1068397.jpg|thumb|right|418px|An example of colour banding, visible in the sky in this photograph]]\n\n'''Colour banding''', or '''color banding''' (American English) is a problem of inaccurate colour presentation in [[computer graphics]]. In 24-bit colour modes, 8 bits per channel is usually considered sufficient to render images in [[Rec. 709]] or [[sRGB]]. However, in some cases there is a risk of producing abrupt changes between shades of the same colour. For instance, displaying natural [[gradient]]s (like sunsets, dawns or clear blue skies) can show minor banding.\n\nColour banding is more noticeable with fewer [[bits per pixel]] (BPP) at 16&ndash;256 colours (4&ndash;8&nbsp;BPP), where not every shade can be shown without [[dithering]].\n\nPossible solutions include the introduction of [[Dither#Digital photography and image processing|dithering]] and increasing the number of bits per colour channel. [[Gaussian blur|Blurring]] does ''not'' fix this, unless one actually increases the number of levels available so that the blur can render color in intermediate levels.\n\n==See also==\n* [[Posterization]]\n* [[Quantization (signal processing)]]\n\n==External links==\n*[http://www.scantips.com/basics14.html Dynamic range 24 vs 36 bit ]\n\n{{color topics}}\n[[Category:Computer graphics]]\n[[Category:Computer graphic artifacts]]\n[[Category:Visual artifacts]]"
    },
    {
      "title": "Composite artifact colors",
      "url": "https://en.wikipedia.org/wiki/Composite_artifact_colors",
      "text": "{{redirect|Artifacting|other uses of the term|Artifact (disambiguation){{!}}Artifact}}\n[[File:CGA CompVsRGB Text.png|thumb|CGA 80-column text on RGB (left) vs. composite monitor (right)]]\n[[Image:Atari8.png|thumb|Atari 8 bit Moiré pattern in 320 horizontal pixel graphics mode. The colors are artifacts of displaying hi-res pixels which are half the size of the NTSC color clock.]]\n[[Image:Mystery House - Apple II render emulation - 2.png|thumb|Screenshot of the game ''[[Mystery House]]'' running on an Apple II. The color white was represented by combining green and purple, which produced white in the middle, but bleeding of the other two colors on the edges]]\n[[Image:OpenEmulator Rendering.jpg|thumb|Example of artwork created with the intent of having individual pixel values horizontally averaged over composite video]]\n\n'''Composite artifact colors'''  is a designation commonly used to address several graphic modes of some 1970s and 1980s [[home computer]]s. \nWith some machines, when connected to an [[NTSC]] TV or monitor over [[composite video]] outputs, the video signal encoding allowed for extra colors to be displayed, by manipulating the pixel position on screen, not being limited by each machine's [[List of 8-bit computer hardware palettes|hardware color palette]] (though, on modern TVs it might not work as well).\n\nThis mode was used mainly for games, since it limited the display's horizontal resolution more than normal. It was mostly used on the IBM PC (with [[CGA graphics]]),<ref>{{cite web|url=http://sourceforge.net/p/ultima-exodus/wiki/CGA%20Composite/ |title=Exodus Project / Wiki / CGA Composite |website=Sourceforge.net |date= |accessdate=2016-08-07}}</ref> [[TRS-80 Color Computer]]<ref>{{cite web|url=http://www.coco3.com/community/2009/02/256-color-mode-composite-mode-artifacting/ |title=256 color mode (composite mode artifacting) - The TRS-80/Tandy Color Computer COCO SuperSite! |website=Coco3.com |date= |accessdate=2016-08-07}}</ref> and [[Apple II]]<ref name=\"nerdlypleasures.blogspot.pt\">{{cite web|url=http://nerdlypleasures.blogspot.pt/2013/09/the-overlooked-artifact-color.html |title=Nerdly Pleasures: The Overlooked Artifact Color Capabilities of non-Apple II Computers |website=Nerdlypleasures.blogspot.pt |date=2013-09-24 |accessdate=2016-08-07}}</ref> computers, but also possible on [[Atari 8-bit]].<ref name=\"nerdlypleasures.blogspot.pt\"/>\n\nThe limitations of composite video regarding horizontal resolution were also exploited on other systems.  Adjacent pixel values got averaged horizontally, producing solid colors or generating transparency effects.\n\nOn [[PAL]] displays (or NTSC 4.43 ones) this effect doesn't generate new colors, but a rather a mix of adjacent horizontal pixel values. However, depending on the PAL system used, results will vary. If PAL M or PAL N are used color artifacts seen on NTSC might also be possible. \nIf a higher resolution video connection is used, the graphics are displayed as [[dither]] patterns.\nMachines such as the [[ZX Spectrum]] or [[Mega Drive]] took advantage of this situation.\n\n==Hardware support==\n\n===CGA===\nWhen using IBM's [[Color Graphics Adapter]] (CGA) with [[NTSC]] TV-out the separation between luminance and chrominance is imperfect, yielding cross-color artifacts, or color \"smearing\". This is especially a problem with 80-column text.\n\nIt is for this reason that each of the text and graphics modes described above exists twice: Once as the normal \"color\" version and once as a \"monochrome\" version. The \"monochrome\" version of each mode turns off the NTSC color decoding in the viewing monitor completely, resulting in a black-and-white picture, but also no color bleeding, hence, a sharper picture.\nOn RGBI monitors, the two versions of each mode are identical, with the exception of the 320×200 graphics mode, where the \"monochrome\" version produces the third palette, as described above.\n\nHowever, programmers learned that this flaw could be turned into an asset, as distinct patterns of high-resolution dots would \"smear\" into consistent areas of solid colors, thus allowing the display of completely new colors. Since these new colors are the result of cross-color artifacting, they are often called \"artifact colors\". Both the standard 320&times;200 four-color and the 640&times;200 color-on-black graphics modes could be used with this technique.\n\nThe resulting screens would have a usable resolution of 160×200 with 16 colors:<ref>{{cite web|url=http://www.seasip.info/VintagePC/cga.html#ccomp |title=Colour Graphics Adapter Notes |website=Seasip.info |date=2006-12-06 |accessdate=2016-08-07}}</ref>\n*Black\n*Dark green\n*Blue\n*Cyan\n*Crimson\n*Dark brownish grey\n*Magenta\n*Violet\n*Dark grey\n*Bright Green\n*Light brownish grey\n*Bright Cyan\n*Scarlet\n*Yellow\n*Hot Pink\n*White\n\n===TRS-80 Color Computer===\nThe [[TRS-80 Color Computer]] 256×192 two color graphics mode uses four colors due to a quirk in the NTSC television system. It is not possible to reliably display 256 dots across the screen due to the limitations of the NTSC signal and the phase relationship between the VDG clock and colorburst frequency. In the first colorset, where green and black dots are available, alternating columns of green and black are not distinct and appear as a muddy green color. However, when one switches to the white and black colorset, instead of a muddy gray as expected, the result is either orange or blue. Reversing the order of the alternating dots will give the opposite color. In effect this mode becomes a 128×192 4 color graphics mode where black, orange, blue, and white are available (the [[Apple II]] created color graphics by exploiting a similar effect). Most CoCo games used this mode as the colors available are more useful than the ones provided in the hardware 4 color modes. Unfortunately the VDG internally can power up on either the rising or falling edge of the clock, so the bit patterns that represent orange and blue are not predictable. Most CoCo games would start up with a title screen and invited the user to press the reset button until the colors were correct. The CoCo 3 fixed the clock-edge problem so it was always the same; a user would hold the F1 key during reset to choose the other color set. On a CoCo 3 with an analog RGB monitor, the black and white dot patterns do not artifact; to see them one would have to use a TV or composite monitor, or patch the games to use the hardware 128×192 four color mode in which the GIME chip allows the color choices to be mapped. Users in [[PAL]] countries saw green and purple stripes instead of solid red and blue colors.\n\nReaders of ''[[The Rainbow (magazine)|The Rainbow]]'' or ''Hot CoCo'' magazine learned that they could use some POKE commands to switch the 6847 VDG into one of the artifact modes, while Extended Color Basic continued to operate as though it were still displaying one of the 128x192 four-color modes. Thus, the entire set of Extended Color Basic graphics commands could be used with the artifact colors. Some users went on to develop a set of 16 artifact colors{{how|date=October 2012}} using a 4×2 pixel matrix. Use of POKE commands also made these colors available to the graphics commands, although the colors had to be drawn one horizontal line at a time. Some interesting artworks were produced from these effects, especially since the CoCo Max art package provided them in its palette of colors.\n\nThe resulting 16 color palette is:\n*black\n*dark cyan\n*brick red\n*light violet\n*dark blue\n*azure\n*olive green\n*brown\n*purple\n*light blue\n*orange\n*yellow\n*light gray\n*blue-white\n*pink-white\n*white\n\n===Apple II===\n[[Apple II graphics|Color graphics]] on the [[Apple II]] series took advantage of a quirk of the NTSC television signal standard, which made color display relatively easy and inexpensive to implement.\n\nThe Apple II display provided two pixels per NTSC subcarrier cycle. When the [[color burst]] reference signal was turned on and the computer attached to a color display, it could display green by showing one alternating pattern of pixels, magenta with an opposite pattern of alternating pixels, and white by placing two pixels next to each other. Later, blue and orange became available by tweaking the offset of the pixels by half a pixel-width in relation to the color-burst signal. The high-resolution display offered more colors simply by compressing more, narrower pixels into each subcarrier cycle.\n\nThe coarse, low-resolution graphics display mode worked differently, as it could output a ''pattern'' of dots per pixel to offer more color options. These patterns were stored in the character generator ROM and replaced the text character bit patterns when the computer was switched to low-res graphics mode. The text mode and low-res graphics mode used the same memory region and the same circuitry was used for both.\n\n===Atari 8-bit===\nThe graphics 8 mode on [[Atari 8-bit]] computers using the [[Color Television Interface Adaptor]] (CTIA) chip is designed to display black or white at a resolution of 320×192. Programmers found that by using artifact colors blue and red can also be used in the mode, and software such as [[On-Line Systems]]' ''[[The Wizard and the Princess]]'' use the feature to display four colors. After Atari began shipping computers with the improved [[Graphic Television Interface Adaptor]] (GTIA) users found that such programs displayed incorrect colors and needed updates.<ref name=\"creativeatari1983\">{{cite book | chapterurl=http://www.atariarchives.org/creativeatari/The_Wizard_the_Princess_and_the_Atari.php | title=The Creative Atari | publisher=Creative Computing Press | chapter=The Wizard, the Princess, and the Atari | year=1983 | isbn=0916688348 |editor1=Small, David |editor2=Small, Sandy |editor3=Blank, George }}</ref>\n\n==Software support==\nMany of the more high-profile game titles offered graphics optimized for composite color monitors.<ref>{{cite web|url=http://nerdlypleasures.blogspot.pt/2013/11/ibm-pc-color-composite-graphics.html |title=Nerdly Pleasures: IBM PC Color Composite Graphics |website=Nerdlypleasures.blogspot.pt |date=2013-11-02 |accessdate=2016-08-07}}</ref>\n\n''[[Ultima II]]'', the first game in the game series to be ported to IBM PC, used CGA composite graphics. ''[[King's Quest I]]'' was also innovative in its use of 16-color graphics. Other titles include ''[[Microsoft Decathlon]]'', ''[[King's Quest II]]'' and ''[[King's Quest III]]''.\n\n==Other machines and the PAL system==\nUsing a composite connection with the [[PAL]] TV system will not generate new colors, but instead a horizontal blurring effect. Given the different bandwidths of the [[PAL#PAL_broadcast_systems|PAL broadcast systems]] ([[PAL-M]], [[PAL-N]], [[PAL-B]], etc..), the actual effect intensity will vary.\n\nThis effect will be more pronounced if higher display resolutions are used, and as such was exploited by game artists on some machines using [[dithering]] patterns. One notable example of this is the [[Mega Drive]], which takes full advantage of it to simulate transparency effects.\n\nAn earlier machine, the [[ZX Spectrum]], used the effect to blend colors and simulate half bright variations of its basic 8 color palette. Other home computers like the [[Atari ST]] or the [[Commodore Amiga]] also had graphics prepared with dithering techniques to take advantage of composite TV connections. Regarding the Amiga, the special [[Hold-And-Modify]] was particularly suited for displaying \"high color\" TV-like images, taking full advantage of horizontal blurring.\n\n==References==\n{{Reflist}}\n\n[[Category:Computer graphic artifacts]]\n[[Category:Computer graphic techniques]]\n[[Category:Computer display standards]]\n[[Category:IBM PC compatibles]]\n[[Category:Apple II computers]]\n[[Category:Composite video formats]]"
    },
    {
      "title": "Compression artifact",
      "url": "https://en.wikipedia.org/wiki/Compression_artifact",
      "text": "{{Redirect|Mosquito noise|the anti-loitering technology|The Mosquito|other uses|mosquito (disambiguation)}}\n{{Refimprove|date=September 2007}}\n[[File:Sego lily cm.jpg|thumb|150px|right|Original image, with good color grade]]\n[[File:Sego lily cm-150.jpg|thumb|right|Loss of edge clarity and tone \"fuzziness\" in heavy [[JPEG]] compression]]\n\nA '''compression artifact''' (or '''artefact''') is a noticeable distortion of media (including [[image]]s, [[Sound recording|audio]], and [[video]]) caused by the application of [[lossy compression]].\n\nLossy data compression involves discarding some of the media's data so that it becomes small enough to be stored within the desired [[File size|disk space]] or transmitted (''streamed'') within the available bandwidth (known as the data rate or [[bit rate]]). If the compressor can not store enough data in the compressed version, the result is a loss of quality, or introduction of artifacts. The [[compression algorithm]] may not be intelligent enough to discriminate between distortions of little subjective importance and those objectionable to the user.\n\nCompression artifacts occur in many common media such as [[DVD]]s, common computer file formats such as [[JPEG]], [[MP3]], or [[MPEG]] files, and some alternatives to the compact disc, such as [[Sony|Sony's]] [[MiniDisc]] format. Uncompressed media (such as on [[Laserdisc]]s, [[Red Book (audio CD standard)|Audio CDs]], and [[WAV]] files) or [[lossless data compression|losslessly compressed]] media (such as [[FLAC]] or [[Portable Network Graphics|PNG]]) do not suffer from compression artifacts.\n\nThe minimization of perceivable artifacts is a key goal in implementing a lossy compression algorithm. However, artifacts are occasionally ''intentionally'' produced for artistic purposes, a style known as [[glitch art]]<ref>{{cite web|last=Geere |first=Duncan |url=https://www.wired.co.uk/news/archive/2010-08/17/glitch-art-databending |title=Glitch art created by 'databending' |publisher=Wired |date=2011-12-13 |accessdate=2011-12-23}}</ref> or datamoshing.<ref>{{cite web|last=Baker-Smith |first=Ben |url=http://bitsynthesis.com/2009/04/tutorial-datamoshing-the-beauty-of-glitch/ |title=Datamoshing&nbsp;– The Beauty of Glitch |publisher=Bitsynthesis.com |date=2009-04-28 |accessdate=2009-04-28}}</ref>\n\nTechnically speaking, a compression artifact is a particular class of data error that is usually the consequence of [[Quantization (signal processing)|quantization]] in lossy data compression. Where [[transform coding]] is used, it typically assumes the form of one of the [[basis function]]s of the coder's transform space.\n\n== Images ==\n<!-- deleted file removed [[File:Difference 60 x5.jpg|150px|thumb|This image shows the [[Residual (numerical analysis)|residual]] (with amplification) between [[:Image:60 save for webx5.jpg|a JPEG image]] and [[:Image:Original crop.jpg|the original file]]. Note especially the changes apparent on sharp edges.]] -->\n[[File:Jpeg-text-artifacts.gif|thumb|right|320px|Illustration of the effect of JPEG compression on a slightly noisy image with a mixture of text and whitespace. Text is a screen capture from a Wikipedia conversation with noise added (intensity 10 in Paint.NET). One frame of the animation was saved as a JPEG (quality 90) and reloaded. Both frames were then zoomed by a factor of 4 (nearest neighbor interpolation).]]\n\nWhen performing block-based coding for [[Quantization (signal processing)|quantization]], as in JPEG-compressed images, several types of artefacts can appear.\n* [[Ringing artifacts#JPEG|Ringing]]\n* Contouring\n* [[Posterizing]]\n* Staircase noise ([[aliasing]]) along curving edges\n* Blockiness in \"busy\" regions (block boundary artefacts, sometimes called macroblocking, quilting, or checkerboarding)\n\nOther lossy algorithms, which use pattern matching to deduplicate similar symbols, are prone to introducing hard to detect errors in printed text. For example, the numbers \"6\" and \"8\" may get replaced. This has been observed to happen with [[JBIG2]] in certain photocopier machines.<ref name=\"kriesel\">{{cite web|url=http://www.dkriesel.com/en/blog/2013/0802_xerox-workcentres_are_switching_written_numbers_when_scanning|title=Xerox scanners/photocopiers randomly alter numbers in scanned documents|date=2013-08-02|accessdate=2013-08-04}}</ref><ref>{{cite news|url=https://www.bbc.co.uk/news/technology-23588202 |title=Confused Xerox copiers rewrite documents, expert finds |publisher=BBC News |date=2013-08-06 |accessdate=2013-08-06}}</ref>\n\n=== Block boundary artefacts ===\n[[File:The macroblocking effect (JPEG).png|344px|right|thumb|Block coding artefacts in a JPEG image. Flat blocks are caused by coarse quantization. Discontinuities at transform block boundaries are visible.]]\nAt low bit rates, any [[lossy]] block-based coding scheme introduces visible artefacts in pixel blocks and at block boundaries. These boundaries can transform block boundaries, prediction block boundaries, or both, and may coincide with [[macroblock]] boundaries. The term ''macroblocking'' is commonly used regardless of the artefact's cause. Other names include tiling,<ref>{{cite book|url=https://books.google.com/books?id=STFAx4jqU5IC&pg=PA343&lpg=PA343&dq=tiling+mpeg&source=web&ots=orZJnUBtk5&sig=IxPYZSsRy48ZxKbjYWe2LOUx3FE&hl=en&sa=X&oi=book_result&resnum=8&ct=result|title=The MPEG handbook by John Watkinson}}</ref> mosaicing, pixelating, quilting, and checkerboarding.\n\nBlock-artefacts are a result of the very principle of [[Transform coding#Digital|block transform]] coding. The transform (for example the discrete cosine transform) is applied to a block of pixels, and to achieve lossy compression, the transform coefficients of each block are [[Quantization (image processing)|quantized]]. The lower the bit rate, the more coarsely the coefficients are represented and the more coefficients are quantized to zero. Statistically, images have more low-[[Spatial frequency|frequency]] than high-frequency content, so it is the low-frequency content that remains after quantization, which results in blurry, low-resolution blocks. In the most extreme case only the DC-coefficient, that is the coefficient which represents the average color of a block, is retained, and the transform block is only a single color after reconstruction.\n\nBecause this quantization process is applied individually in each block, neighboring blocks quantize coefficients differently. This leads to discontinuities at the block boundaries. These are most visible in flat areas, where there is little detail to mask the effect.\n\n=== Image artifact reduction ===\n{{Main article|Deblocking filter}}\nVarious approaches have been proposed to reduce image compression effects, but to use standardized compression/decompression techniques and retain the benefits of compression (for instance, lower transmission and storage costs), many of these methods focus on \"post-processing\"—that is, processing images when received or viewed. No post-processing technique has been shown to improve image quality in all cases; consequently, none has garnered widespread acceptance, though some have been implemented and are in use in proprietary systems. Many photo editing programs, for instance, have proprietary JPEG artefact reduction algorithms built-in. Consumer equipment often calls this post-processing \"MPEG Noise Reduction\".<ref>{{cite web|url=https://www.pcmag.com/encyclopedia_term/0,2542,t=blocking+artefacts&i=56319,00.asp|title=PC Magazine, Definition of blocking artefacts}}</ref>\n\n== Video ==\n[[File:Macroblocking error.png|thumb|350px|Example of image with artifacts due to a transmission error]]\nWhen motion prediction is used, as in [[MPEG-1]], [[MPEG-2]] or [[MPEG-4]], compression artifacts tend to remain on several generations of decompressed frames, and move with the [[optic flow]] of the image, leading to a peculiar effect, part way between a painting effect and \"grime\" that moves with objects in the scene.\n\nData errors in the compressed bit-stream, possibly due to transmission errors, can lead to errors similar to large quantization errors, or can disrupt the parsing of the data stream entirely for a short time, leading to \"break-up\" of the picture.  Where gross errors have occurred in the bit-stream, decoders continue to apply updates to the damaged picture for a short interval, creating a \"ghost image\" effect, until receiving the next independently compressed frame. In MPEG picture coding, these are known as \"[[I-frame]]s\", with the 'I' standing for \"intra\". Until the next I-frame arrives, the decoder can perform [[error concealment]].\n\n=== Motion compensation block boundary artifacts ===\nBlock boundary discontinuities can occur at edges of [[motion compensation]] prediction blocks. In motion compensated video compression, the current picture is predicted by shifting blocks (macroblocks, partitions, or prediction units) of pixels from previously decoded frames. If two neighboring blocks use different motion vectors, there will be a discontinuity at the edge between the blocks.\n\n=== Mosquito noise ===\nVideo compression artifacts include cumulative results of compression of the comprising still images, for instance [[Ringing artifacts#JPEG|ringing]] or other edge busyness in successive still images appear in sequence as a shimmering blur of dots around edges, called '''mosquito noise''', as they resemble mosquitoes swarming around the object.<ref name=\"dinhpatry\">{{cite web|last1=Le Dinh|first1=Phuc-Tue|last2=Patry|first2=Jacques|title=Video compression artifacts and MPEG noise reduction|url=http://www.embedded.com/print/4013028|website=Embedded|accessdate=19 February 2016}}</ref><ref>\"'''3.9 mosquito noise:''' Form of edge busyness distortion sometimes associated with movement, characterized by moving artifacts and/or blotchy noise patterns superimposed over the objects (resembling a mosquito flying around a person's head and shoulders).\"\n[http://www.itu.int/rec/T-REC-P.930-199608-I ITU-T Rec. P.930 (08/96) Principles of a reference impairment system for video]</ref>\n\n=== Video artifact reduction ===\n{{Main article|Deblocking filter}}\nThe artifacts at block boundaries can be reduced by applying a [[deblocking filter]]. As in still image coding, it is possible to apply a deblocking filter to the decoder output as post-processing.\n\nIn motion-predicted video coding with a closed prediction loop, the encoder uses the decoder output as the prediction reference from which future frames are predicted. To that end, the encoder conceptually integrates a decoder. If this \"decoder\" performs a deblocking, the deblocked picture is then used as a reference picture for motion compensation, which improves coding efficiency by preventing a propagation of block artifacts across frames. This is referred to as an in-loop deblocking filter. Standards which specify an in-loop deblocking filter include [[VC-1]], [[H.263]] Annex J, [[H.264/AVC]], and [[H.265/HEVC]].\n\n== Audio ==\nLossy audio compression typically works with a psychoacoustic model—a model of human hearing perception. Lossy audio formats typically involve the use of a time/frequency domain transform, such as a [[modified discrete cosine transform]]. With the psychoacoustic model, masking effects such as frequency masking and temporal masking are exploited, so that sounds that should be imperceptible are not recorded. For example, in general, human beings are unable to perceive a quiet tone played simultaneously with a similar but louder tone. A lossy compression technique might identify this quiet tone and attempt to remove it. Also, quantization noise can be \"hidden\" where they would be masked by more prominent sounds. With low compression, a conservative psy-model is used with small block sizes.\n\nWhen the psychoacoustic model is inaccurate, when the transform block size is restrained, or when aggressive compression is used, this may result in compression artifacts. Compression artifacts in compressed audio typically show up as ringing, [[pre-echo]], \"birdie artifacts\", drop-outs, rattling, warbling, metallic ringing, an underwater feeling, hissing, or \"graininess\".\n\nAn example of compression artifacts in audio is applause in a relatively highly compressed audio file (e.g. 96 kbit/sec MP3). In general, musical tones have repeating waveforms and more predictable variations in volume, whereas applause is essentially random, therefore hard to compress. A highly compressed track of applause may have \"metallic ringing\" and other compression artifacts.\n\n== Artistic use ==\n[[File:Glitch video.ogg|thumb|right|250px|Video glitch art]]\nCompression artifacts may intentionally be used as a visual style, sometimes known as ''[[glitch art]]''.\n\nIn still images, an example is ''Jpegs'' by German photographer [[Thomas Ruff]], which uses intentional [[JPEG]] artifacts as the basis of the picture's style.<ref>''jpegs'', [[Thomas Ruff]], ''Aperture'', May 31, 2009, 132 pp., {{ISBN|978-1-59711-093-8}}</ref><ref>[http://jmcolberg.com/weblog/2009/04/review_jpegs_by_thomas_ruff/ Review: jpegs by Thomas Ruff], by [[Jörg Colberg]], April 17, 2009</ref>\n\nIn [[video art]], one technique is ''datamoshing'', where two videos are interleaved so intermediate frames are interpolated from two separate sources. Another technique involves simply transcoding from one lossy video format to another, which exploits the difference in how the separate video codecs process motion and color information.<ref>{{cite web|author=Anoniem zei |url=http://rosa-menkman.blogspot.com/2009/02/from-compression-artifact-to-filter.html |title=From compression artifact to filter |publisher=Rosa-menkman.blogspot.com |date=2009-02-19 |accessdate=2011-12-23}}</ref> The technique was pioneered by artists [[Bertrand Planes]] in collaboration with [[Christian Jacquemin]] in 2006 with [[DivXPrime]],<ref>{{cite web |last=Jacquemin |first=Christian | title =Le bug dans l'oeuvre DivXPrime de Bertrand Planes: Invention et mutation. In, Ivan Toulouse and Daniel Danétis, editors, Eurêka: Le moment de l'invention, un dialogue entre art et science, L'Harmattan, Paris |pages=245–256 | year =2008 | url =http://perso.limsi.fr/jacquemi/FTP/jacquemin_Eureka.pdf | accessdate = 5 November 2012}}</ref> [[Sven König]], [[Takeshi Murata]], [[Jacques Perconte]] and [[Paul B. Davis (artist)|Paul B. Davis]] in collaboration with [[Paperrad]], and more recently used by [[David OReilly (artist)|David OReilly]] and within [[music video]]s for [[Chairlift (band)|Chairlift]] and [[Kanye West]].<ref>[http://rhizome.org/editorial/2380 Pixel Bleed], by John Michael Boling. [[Rhizome (organization)|Rhizome]]. February 25, 2009.</ref><ref>{{cite web|last=Rodriguez |first=Jayson |url=http://www.mtv.com/news/articles/1605281/20090218/west_kanye.jhtml |title=Kanye West Rushes New Video Onto His Web Site&nbsp;– MTV News |publisher=Mtv.com |date=2009-02-18 |accessdate=2011-12-23}}</ref>\n\n== See also ==\n{{commons category|Compression artifacts}}\n{{colbegin}}\n* [[Artifact (error)]]\n* [[Databending]]\n* [[Digital artifact]]\n* [[Generation loss]]\n* [[JPEG]]\n* [[JPEG 2000]]\n* [[Lossy compression]]\n* [[Noise print]]\n* [[Transparency (data compression)]]\n{{colend}}\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://onoffar2.free.fr/HTML/divxp.htm DivXPrime]  First known experiments of datamoshing video software by Bertrand Planes & Christian Jacquemin (based on Xvid algorithm)\n* [https://vimeo.com/20081410 Teaser for \"Sonic birth\"] a short movie directed by Jérome Blanquet, datamoshing effect by David Olivari, produced by [Metronomic]. Full movie: [https://vimeo.com/57857765 \"Sonic birth\"]\n* [https://launchpad.net/datamosher datamosher] A GPL video datamoshing software.\n* [http://vimeo.com/2564771 Example of heavy video compression artefacts].\n* [https://web.archive.org/web/20080206181840/http://www.jpegtutor.co.uk/ JPEG Tutor], an interactive applet allowing you to investigate the effects of changing the quantization matrix.\n* [http://www.cs.tut.fi/~foi/SA-DCT/#ref_software JPEG deringing and deblocking: Matlab software and Photoshop plug-in]\n\n{{Compression Methods}}\n\n{{DEFAULTSORT:Compression Artifact}}\n[[Category:Audio engineering]]\n[[Category:Computer graphic artifacts]]\n[[Category:Data compression]]\n[[Category:Articles containing video clips]]"
    },
    {
      "title": "Digital artifact",
      "url": "https://en.wikipedia.org/wiki/Digital_artifact",
      "text": "{{short description|Undesired or unintended alteration in data introduced in a digital process by an involved technique and/or technology}}\n{{distinguish|Virtual artifact}}\n{{refimprove|date=April 2013}}\n[[File:HyperGridDigitalArtifacts.jpg|thumb|A complicated grid pattern is insufficiently processed by a [[smartphone]] camera.]]\n[[File:Colonel Crawford Burn Site Monument drawing.jpg|thumb|200px|A scan of a drawing with large areas of whitespace; the diamond pattern is a scanning artifact.]]\n\n'''Digital artifact''' in information science, is any undesired or unintended alteration in data introduced in a digital process by an involved technique and/or technology. In anthropology and archeology a digital artifact is an artifact that is of a digital nature or creation. For example, a gif is such an artifact.  \n\nDigital artifact can be of any content types including text, audio, video, image, animation or a combination.\n\n== Information science ==\nIn information science, digital artifacts result from:\n*Hardware malfunction: In computer graphics, [[visual artifact]]s may be generated whenever a hardware component such as the processor, memory chip, cabling malfunctions, etc., corrupts data. Examples of malfunctions include physical damage, overheating, insufficient voltage and [[GPU]] [[overclocking]]. Common types of hardware artifacts are [[Texture mapping|texture]] corruption and [[T-vertices]] in 3D graphics, and [[pixelization]] in MPEG compressed video.\n*Software malfunction: Artifacts may be caused by algorithm flaws such as decoding/encoding audio or video, or a poor pseudo-random number generator that would introduce artifacts distinguishable from the desired noise into statistical models.\n*[[Compression artifact|Compression]]: Controlled amounts of unwanted information may be generated as a result of the use of [[lossy compression]] techniques. One example is the artifacts seen in [[JPEG]] and [[MPEG]] compression algorithms that produce [[compression artifact|compression artifacts]].\n*[[Aliasing]]: Digital imprecision generated in the process of converting analog information into digital space is due to the limited granularity of digital numbering space. In computer graphics, aliasing is seen as [[pixelation]].\n*[[Rolling shutter]], the line scanning of an object that is moving too fast for the image sensor to capture a unitary image.\n*[[Error diffusion]]: poorly-weighted kernel coefficients result in undesirable visual artifacts.\n\n== References ==\n{{reflist|30em}}\n\n== External links ==\n*[https://web.archive.org/web/20110610173132/http://dpreview.com/learn/?%2FGlossary%2FDigital_Imaging%2FArtifacts_01.htm DPReview: Glossary: Artifacts]\n\n[[Category:Anthropology]]\n[[Category:Archaeology]]\n[[Category:Information science]]\n[[Category:Error]]\n[[Category:Computer graphic artifacts]]\n[[Category:Digital photography]]"
    },
    {
      "title": "Dither",
      "url": "https://en.wikipedia.org/wiki/Dither",
      "text": "{{Other uses}}\n{{Use dmy dates|date=February 2013}}\n\n[[Image:Michelangelo's David - Floyd-Steinberg.png|frame|A [[grayscale]] image represented in 1-bit [[black-and-white]] space with dithering]]\n\n'''Dither''' is an intentionally applied form of [[image noise|noise]] used to randomize [[quantization error]], preventing large-scale patterns such as [[color banding]] in images. Dither is routinely used in processing of both [[digital audio]] and [[digital video|video]] data, and is often one of the last stages of [[Audio mastering|mastering]] audio to a [[compact disc|CD]].\n\nA common use of dither is converting a [[greyscale]] image to black and white, such that the density of black dots in the new image approximates the average grey level in the original.\n\n== Etymology ==\n\n{{quote\n|…[O]ne of the earliest [applications] of dither came in World War II. Airplane bombers used [[mechanical computers]] to perform navigation and bomb trajectory calculations. Curiously, these computers (boxes filled with hundreds of gears and cogs) performed more accurately when flying on board the aircraft, and less well on ground. Engineers realized that the vibration from the aircraft reduced the error from sticky moving parts. Instead of moving in short jerks, they moved more continuously. Small vibrating motors were built into the computers, and their vibration was called dither from the Middle English verb \"didderen,\" meaning \"to tremble.\" Today, when you tap a mechanical meter to increase its accuracy, you are applying dither, and modern dictionaries define dither as a highly nervous, confused, or agitated state. In minute quantities, dither successfully makes a digitization system a little more analog in the good sense of the word.\n|Ken Pohlmann, ''Principles of Digital Audio''<ref>{{cite book | title = Principles of Digital Audio | author = Ken C. Pohlmann | publisher = McGraw-Hill Professional | year = 2005 | url = https://books.google.com/books?id=VZw6z9a03ikC&pg=PA49&dq=didderen+dither+intitle:Principles+intitle:of+intitle:Digital+intitle:Audio | isbn = 978-0-07-144156-8 }}</ref>\n}}\n\nThe term ''dither'' was published in books on analog computation and hydraulically controlled guns shortly after [[World War II]].<ref>{{cite book | title = Ordnance Field Guide: Restricted | author = William C. Farmer | publisher = Military service publishing company | year = 1945 | url = https://books.google.com/books?id=15ffO4UVw8QC&q=dither }}</ref><ref>{{cite book | title = Electronic Analog Computers: (d–c Analog Computers) | author = Granino Arthur Korn and Theresa M. Korn | publisher = McGraw-Hill | year = 1952 | url = https://books.google.com/books?id=dwsuAAAAIAAJ&q=dither }}</ref> Though he did not use the term ''dither'', the concept of dithering to reduce quantization patterns was first applied by [[Lawrence G. Roberts]]<ref>{{cite book | title = Data Compression: Techniques and Applications | author = Thomas J. Lynch | publisher = Lifetime Learning Publications | year = 1985 | url = https://books.google.com/books?id=E7EmAAAAMAAJ&q=first+suggested+by+Roberts+in+1962&dq=first+suggested+by+Roberts+in+1962 | isbn = 978-0-534-03418-4 }}</ref> in his 1961 [[MIT]] master's thesis<ref>Lawrence G. Roberts, ''Picture Coding Using Pseudo-Random Noise'', MIT, S.M. thesis, 1961 [http://www.packet.cc/files/pic-code-noise.html online]</ref> and 1962 article.<ref>{{cite journal | title = Picture Coding Using Pseudo-Random Noise | author = Lawrence G. Roberts | journal = IEEE Transactions on Information Theory | volume = 8 | issue = 2 |date=February 1962 | pages = 145–154 | url = http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1057702 | format = abstract | doi = 10.1109/TIT.1962.1057702}}</ref> By 1964 dither was being used in the modern sense described in this article.<ref>{{cite journal | title = Dither Signals and Their Effect on Quantization Noise | author = L. Schuchman | journal = IEEE Trans. Commun. | volume = 12 | issue = 4 |date=December 1964 | pages = 162–165 | url = http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1088973 | doi = 10.1109/TCOM.1964.1088973 | format = abstract}}</ref>\n\n==In digital processing and waveform analysis==\nDither is utilized in many different fields where digital processing and analysis are used. These uses include systems using [[digital signal processing]], such as [[digital audio]], [[digital video]], [[digital photography]], [[seismology]], [[radar]] and [[weather forecasting]] systems.\n\n[[Quantization (signal processing)|Quantization]] yields error. If that error is ''[[correlation|correlated]]'' to the signal, the result is potentially cyclical or predictable. In some fields, especially where the receptor is sensitive to such artifacts, cyclical errors yield undesirable artifacts. In these fields introducing dither converts the error to random noise. The field of audio is a primary example of this. The human [[ear]] functions much like a [[Fourier transform]], wherein it hears individual frequencies.<ref name=\"Deutsch1999\">{{cite book|last=Deutsch|first=Diana|title=The psychology of music|url=https://books.google.com/books?id=A3jkobk4yMMC&pg=PA153|accessdate=24 May 2011|year=1999|publisher=Gulf Professional Publishing|isbn=978-0-12-213565-1|page=153}}</ref><ref name=\"Hauser1998\">{{cite book|last=Hauser|first=Marc D.|title=The evolution of communication|url=https://books.google.com/books?id=QbunyscCJBoC&pg=PA190|accessdate=24 May 2011|year=1998|publisher=MIT Press|isbn=978-0-262-58155-4|page=190}}</ref> The ear is therefore very sensitive to [[distortion]], or additional frequency content, but far less sensitive to additional random noise at all frequencies such as found in a dithered signal.<ref>{{cite web|last=Montgomery|first=Christopher (Monty)|title=Digital Show and Tell|url=https://wiki.xiph.org/Videos/Digital_Show_and_Tell#Dither|publisher=[[Xiph.Org]] / [[Red Hat]], Inc.|accessdate=27 February 2013|year=2012–2013|quote=Dither is specially-constructed noise that substitutes for the noise produced by simple quantization. Dither doesn't drown out or mask quantization noise, it replaces it with noise characteristics of our choosing that aren't influenced by the input.}}</ref>{{failed verification|reason=We're looking for a citation that says ears are more sensitive to quantization error than to dither noise|date=March 2013}}\n\n== Digital audio ==\n\n<blockquote>In an analog system, the signal is ''continuous'', but in a [[Pulse-code modulation|PCM]] digital system, the amplitude of the signal out of the digital system is limited to one of a set of fixed values or numbers. This process is called [[Quantization (sound processing)|quantization]]. Each coded value is a discrete step... if a signal is quantized without using dither, there will be quantization distortion related to the original input signal... In order to prevent this, the signal is \"dithered\", a process that mathematically removes the harmonics or other highly undesirable distortions entirely, and that replaces it with a constant, fixed noise level.<ref>Mastering Audio: The Art and the Science by [[Bob Katz]], pages 49–50, {{ISBN|978-0-240-80545-0}}</ref></blockquote>\n\nThe final version of audio that goes onto a [[compact disc]] contains only 16 bits per sample, but throughout the production process, a greater number of bits are typically used to represent the sample. In the end, the digital data must be reduced to 16 bits for pressing onto a CD and distributing.\n\nThere are multiple ways to do this. One can, for example, simply discard the excess bits – called ''truncation.'' One can also ''round'' the excess bits to the nearest value. Each of these methods, however, results in predictable and determinable errors in the result. Using dither replaces these errors with a constant, fixed noise level.\n\n=== Examples ===\n[[Image:Reducing amplitude resolution plot.png|frame]]\n{{listen\n| header = 6-bit truncation example audio samples\n| filename = 16bit sine.ogg\n| title = 16-bit sine wave\n| description =\n| format = [[Ogg]]\n| filename2 = 6bit sine truncated.ogg\n| title2 = truncated to 6 bits\n| description2 =\n| format2 = [[Ogg]]\n| filename3 =\n| title3 = dithered to 6 bits\n| description3 =\n| format3 = [[Ogg]]\n}}\n\nTake, for example, a [[waveform]] that consists of the following values:\n\n  1 2 3 4 5 6 7 8\n\nIf the waveform is reduced by 20%, then the following are the new values:\n\n  0.8 1.6 2.4 3.2 4.0 4.8 5.6 6.4\n\nIf these values are truncated it results in the following data:\n\n  0 1 2 3 4 4 5 6\n\nIf these values are rounded instead it results in the following data:\n\n  1 2 2 3 4 5 6 6\n\nFor any original waveform, the process of reducing the waveform amplitude by 20% results in regular errors. Take for example a sine wave that, for some portion, matches the values above. Every time the sine wave's value hit 3.2, the truncated result would be off by 0.2, as in the sample data above. Every time the sine wave's value hit 4.0, there would be no error since the truncated result would be off by 0.0, also shown above. The magnitude of this error changes regularly and repeatedly throughout the sine wave's cycle. It is precisely this error which manifests itself as [[distortion]]. What the ear hears as distortion is the additional content at discrete frequencies created by the regular and repeated quantization error.\n\nA plausible solution would be to take the 2 digit number (say, 4.8) and round it one direction or the other. For example, it could be rounded to 5 one time and then 4 the next time. This would make the long-term average 4.5 instead of 4, so that over the long-term the value is closer to its actual value. This, on the other hand, still results in determinable (though more complicated) error. Every other time the value 4.8 comes up the result is an error of 0.2, and the other times it is −0.8. This still results in a repeating, quantifiable error.\n\nAnother plausible solution would be to take 4.8 and round it so that the first four times out of five it is rounded up to 5, and the fifth time it is rounded to 4. This would average out to exactly 4.8 over the long term. Unfortunately, however, it still results in repeatable and determinable errors, and those errors still manifest themselves as distortion to the ear.\n\nThis leads to the ''dither'' solution. Rather than predictably rounding up or down in a repeating pattern, it is possible to round up or down in a random pattern. If a series of random numbers between 0.0 and 0.9 (ex: 0.6, 0.1, 0.3, 0.6, 0.9, etc.) are calculated and added to the results of the equation, two times out of ten the result will truncate back to 4 (if 0.0 or 0.1 are added to 4.8) and eight times out of ten it will truncate to 5. Each given situation has a random 20% chance of rounding to 4 or 80% chance of rounding to 5. Over the long haul this will result in results that average to 4.8 and a quantization error that is random noise. This noise result is less offensive to the ear than the determinable distortion that would result otherwise.<!--[[User:Kvng/RTH]]-->\n\n=== Usage ===\nDither should be added to any low-amplitude or highly periodic signal before any quantization or re-quantization process, in order to de-correlate the quantization noise from the input signal and to prevent non-linear behavior (distortion); the lesser the bit depth, the greater the dither must be. The result of the process still yields distortion, but the distortion is of a random nature so the resulting noise is, effectively, de-correlated from the intended signal. Any bit-reduction process should add dither to the waveform before the reduction is performed.\n\nIn a seminal paper published in the [[Audio Engineering Society|AES]] Journal, Lipshitz and Vanderkooy pointed out that different noise types, with different [[probability density function]]s (PDFs) behave differently when used as dither signals, and suggested optimal levels of dither signal for audio.<ref>{{Cite journal\n| last1 = Lipshitz\n| first1 = Stanley P\n| last2 = Vanderkooy\n| first2 = John\n| last3 = Wannamaker\n| first3 = Robert A.\n| title = Minimally Audible Noise Shaping\n| journal = J. Audio Eng. Soc.\n| volume = 39\n| issue = 11\n| pages = 836–852\n| date = November 1991\n| url = http://www.aes.org/e-lib/browse.cfm?elib=5956\n| accessdate = 28 October 2009}}\n</ref><ref name=vanderkooy87>\n{{Cite journal\n| last1 = Vanderkooy\n| first1 = John\n| last2 = Lipshitz\n| first2 = Stanley P\n| title = Dither in Digital Audio\n| journal = J. Audio Eng. Soc.\n| volume = 35\n| issue = 12\n| pages = 966–975\n| date = December 1987\n| url = http://www.aes.org/e-lib/browse.cfm?elib=5173\n| accessdate = 28 October 2009}}\n</ref> [[Gaussian noise]] requires a higher level of added noise for full elimination of distortion than noise with [[rectangular distribution|rectangular]] or [[triangular distribution]]. Triangular distributed noise also minimizes ''noise modulation''{{snd}}audible changes in the volume level of residual noise behind quiet music that draw attention to the noise.\n\nDither can be useful to break up periodic [[limit cycle]]s, which are a common problem in digital filters. Random noise is typically less objectionable than the harmonic tones produced by limit cycles.\n\n=== Different types ===\n'''RPDF''' stands for \"Rectangular Probability Density Function,\" equivalent to a roll of a [[dice|die]]. Any number has the same random [[probability]] of surfacing.\n\n'''TPDF''' stands for \"[[Triangular distribution|Triangular Probability Density Function]],\" equivalent to a roll of two dice (the sum of two independent samples of RPDF).\n\n'''Gaussian PDF''' is equivalent to a roll of a large number of dice. The relationship of probabilities of results follows a bell-shaped, or [[Gaussian curve]], typical of dither generated by analog sources such as microphone preamplifiers. If the bit depth of a recording is sufficiently great, that [[preamplifier]] noise will be sufficient to dither the recording.\n\n'''Colored dither''' is sometimes mentioned as dither that has been filtered to be different from [[white noise]]. Some dither algorithms use noise that has more energy in the higher frequencies so as to lower the energy in the critical audio band.\n\n'''[[Noise shaping]]''' is a filtering process that shapes the spectral energy of quantisation error, typically to either de-emphasise frequencies to which the ear is most sensitive or separate the signal and noise bands completely. If dither is used, its final spectrum depends on whether it is added inside or outside the feedback loop of the noise shaper: if inside, the dither is treated as part of the error signal and shaped along with actual quantisation error; if outside, the dither is treated as part of the original signal and linearises quantisation without being shaped itself. In this case, the final noise floor is the sum of the flat dither spectrum and the shaped quantisation noise. While real-world noise shaping usually includes in-loop dithering, it is also possible to use it without adding dither at all, in which case the usual harmonic-distortion effects still appear at low signal levels.\n\n==== Which types to use ====\nIf the signal being dithered is to undergo further processing, then it should be processed with a triangular-type dither that has an amplitude of two quantisation steps; for example, so that the dither values computed range from, say, −1 to +1, or 0 to 2.<ref name=vanderkooy87/> This is the \"lowest power ideal\" dither, in that it does not introduce noise modulation (which would manifest as a constant noise floor), and completely eliminates the harmonic distortion from quantisation. If a ''colored'' dither is used instead at these intermediate processing stages, then frequency content may \"bleed\" into other frequency ranges that are more noticeable, which could become distractingly audible.\n\nIf the signal being dithered is to undergo no further processing — if it is being dithered to its final result for distribution — then a \"colored\" dither or noise shaping is appropriate. This can effectively lower the audible noise level, by putting most of that noise in a frequency range where it is less critical.\n\n== Digital photography and image processing ==\n[[Image:Dithering example red blue.svg|frame|left|An illustration of dithering. Red and blue are the only colors used but, as the red and blue squares are made smaller, the patch appears purple.]]\n\n[[Image:256colortestthing.png|thumb|right|256 color dithering with [[IrfanView]]]]\n'''Dithering''' is used in [[computer graphics]] to create the illusion of \"[[color]] depth\" in images with a limited [[palette (computing)|color palette]] - a technique also known as [[color quantization]]. In a dithered image, colors that are not available in the palette are approximated by a diffusion of colored [[pixel]]s from within the available palette. The human eye perceives the diffusion as a mixture of the colors within it (see [[color vision]]).  Dithered images, particularly those with relatively few colors, can often be distinguished by a characteristic graininess or speckled appearance.\n\nBy its nature, dithering introduces pattern into an image - the theory being that the image will be viewed from such a distance that the pattern is not discernible to the human eye.  Unfortunately this is often not the case, and often the patterning is visible - for example, often with images found on the web.  In these circumstances it has been shown that a [[Blue noise#Blue noise|blue noise]] dither pattern is the least unsightly and distracting.<ref name=dithernoise>{{cite web\n|url = http://home.comcast.net/~ulichney/CV/papers/1994-freq-characterization.pdf\n|archiveurl=https://web.archive.org/web/20140214164527/http://home.comcast.net/~ulichney/CV/papers/1993-void-cluster.pdf\n|archivedate=2014-02-14\n|deadurl=no\n|title = Halftone Characterization in the Frequency Domain\n|accessdate = 2013-08-12\n|last = Ulichney\n|first = Robert A\n|year = 1994\n}}</ref>  The error diffusion techniques were some of the first methods to generate blue noise dithering patterns. However, other techniques such as [[ordered dithering]] can also generate blue noise dithering without the tendency to degenerate into areas with artifacts.\n\n=== Examples ===\n[[Image:Color dithering on a towel.jpg|thumb|Color dithering on a towel]]\nReducing the color depth of an image can often have significant visual side-effects. If the original image is a photograph, it is likely to have thousands, or even millions of distinct colors. The process of constraining the available colors to a specific ''color palette'' effectively throws away a certain amount of color information.\n\nA number of factors can affect the resulting quality of a color-reduced image. Perhaps most significant is the color palette that will be used in the reduced image. For example, an original image (''Figure 1'') may be reduced to the 216-color \"[[web-safe]]\" color palette. If the original pixel colors are simply translated into the closest available color from the palette, no dithering will occur (''Figure 2''). However, typically this approach will result in flat areas (contours) and a loss of detail, and may produce patches of color that are significantly different from the original. Shaded or gradient areas may appear as ''color bands'', which may be distracting. The application of dithering can help to minimize such visual artifacts, and usually results in a better representation of the original (''Figure 3''). Dithering helps to reduce [[color banding]] and flatness.\n\nOne of the problems associated with using a fixed color palette is that many of the needed colors may not be available in the palette, and many of the available colors may not be needed; a fixed palette containing mostly shades of green would not be well-suited for images that do not contain many shades of green, for instance. The use of an optimized color palette can be of benefit in such cases. An optimized color palette is one in which the available colors are chosen based on how frequently they are used in the original source image. If the image is reduced based on an optimized palette the result is often much closer to the original (''Figure 4'').\n\nThe number of colors available in the palette is also a contributing factor. If, for example, the palette is limited to only 16 colors then the resulting image could suffer from additional loss of detail, resulting in even more pronounced problems with flatness and color banding (''Figure 5''). Once again, dithering can help to minimize such artifacts (''Figure 6'').\n\n{{Gallery\n| lines = 6\n| width = 260\n|File:Dithering example undithered.png|Figure 1. Original photo; note the smoothness in the detail.\n|File:Dithering example undithered web palette.png|Figure 2. Original image using the [[web-safe color palette]] with no dithering applied. Note the large flat areas and loss of detail.\n|File:Dithering example dithered web palette.png|Figure 3. Original image using the web-safe color palette with [[Floyd–Steinberg dithering]]. Note that even though the same palette is used, the application of dithering gives a better representation of the original.\n|File:Dithering example dithered 256color.png|Figure 4. Here, the original has been reduced to a 256-color optimized palette with [[Floyd–Steinberg dithering]] applied. The use of an optimized palette, rather than a fixed palette, allows the result to better represent the colors in the original image.\n|File:Dithering example undithered 16color.png|Figure 5. Depth is reduced to a 16-color optimized palette in this image, with no dithering. Colors appear muted, and color banding is pronounced.\n|File:Dithering example dithered 16color.png|Figure 6. This image also uses the 16-color optimized palette, but the use of dithering helps to reduce banding.\n}}\n\n=== Applications ===\nMany different kinds of display hardware, including early computer [[graphics card|video adapters]] as well as many modern [[liquid crystal display|LCDs]] used in [[mobile phone]]s and inexpensive [[digital camera]]s, show a much smaller color range than more advanced displays. One common application of dithering is to more accurately display graphics containing a greater range of colors than the hardware is capable of showing. For example, dithering might be used in order to display a photographic image containing [[True Color|millions of colors]] on video hardware that is only capable of showing 256 colors at a time. The 256 available colors would be used to generate a dithered approximation of the original image. Without dithering, the colors in the original image might simply be \"rounded off\" to the closest available color, resulting in a new image that is a poor representation of the original. Dithering takes advantage of the human eye's tendency to \"mix\" two colors in close proximity to one another.\n\nSome LCDs may use [[temporal dithering]] to achieve a similar effect. By alternating each pixel's color value rapidly between two approximate colors in the panel's color space (also known as [[Frame Rate Control]]), a display panel which natively supports only 18-bit color (6 bits per channel) can represent a 24-bit \"true\" color image (8 bits per channel).<ref>6-Bit vs. 8-Bit... PVA/MVA vs. TN+Film\nAre Things Changing? [http://www.tftcentral.co.uk/articles/6bit_8bit.htm]</ref>\n\nDithering such as this, in which the computer's display hardware is the primary limitation on [[color depth]], is commonly employed in software such as [[web browser]]s. Since a web browser may be retrieving graphical elements from an external source, it may be necessary for the browser to perform dithering on images with too many colors for the available display. It was due to problems with dithering that a color palette known as the \"[[Web-safe color|web-safe color palette]]\" was identified, for use in choosing colors that would not be dithered on displays with only 256 colors available.\n\nBut even when the total number of available colors in the display hardware is high enough to \"properly\" render full color digital photographs (such as those using 15- and 16-bit RGB [[Highcolor|Hicolor]] 32,768/65,536 color modes), banding may still be evident to the eye, especially in large areas of smooth shade transitions (although the original image file has no banding at all). Dithering the 32 or 64 RGB levels will result in a pretty good \"pseudo [[True Color|truecolor]]\" display approximation, which the eye will not resolve as ''grainy''. Furthermore, images displayed on 24-bit RGB hardware (8 bits per RGB primary) can be dithered to simulate somewhat higher bit depth, and/or to minimize the loss of hues available after a [[gamma correction]]. High-end still image processing software commonly uses these techniques for improved display.\n\nAnother useful application of dithering is for situations in which the [[graphics file format|graphic file format]] is the limiting factor. In particular, the commonly used [[GIF]] format is restricted to the use of 256 or fewer colors in many graphics editing programs. Images in other file formats, such as [[Portable Network Graphics|PNG]], may also have such a restriction imposed on them for the sake of a reduction in file size. Images such as these have a fixed color palette defining all the colors that the image may use. For such situations, [[bitmap graphics editor|graphical editing software]] may be responsible for dithering images prior to saving them in such restrictive formats.\n\nDithering is analogous to the [[halftone]] technique used in [[printing]].  The recent widespread adoption of [[inkjet printer]]s and their ability to print isolated dots has increased the use of dithering in printing.  For this reason the term ''dithering'' is sometimes used interchangeably with the term ''halftoning'', particularly in association with [[digital printing]].\n\nA typical desktop inkjet printer can print just 16 colors (the combination of dot or no dot from cyan, magenta, yellow and black print heads). Some of these ink combinations are not useful though, because when the black ink is used it typically obscures any of the other colors.  To reproduce a large range of colors, dithering is used.  In densely printed areas, where the color is dark the dithering is often not visible because the dots of ink merge producing a more uniform print.  However, a close inspection of the light areas of a print where the dithering has placed dots much further apart reveals the tell-tale dots of dithering.\n\n=== Algorithms ===\nThere are several [[algorithm]]s designed to perform dithering. One of the earliest, and still one of the most popular, is the [[Floyd–Steinberg dithering]] algorithm, which was developed in 1975. One of the strengths of this algorithm is that it minimizes visual artifacts through an [[Error diffusion|error-diffusion]] process; error-diffusion algorithms typically produce images that more closely represent the original than simpler dithering algorithms.<ref name=dhalf>{{cite web\n|url = http://www.efg2.com/Lab/Library/ImageProcessing/DHALF.TXT\n|title = Digital Halftoning\n|accessdate = 2007-09-10\n|last = Crocker\n|first = Lee Daniel\n|authorlink = Lee Daniel Crocker |author2=Boulay, Paul |author3=Morra, Mike\n|date = 20 June 1991\n|work = Computer Lab and Reference Library\n}} ''Note: this article contains a minor mistake: “(To fully reproduce our 256-level image, we would need to use an''\n'''8x8''' ''pattern.)” The bold part should read “16x16”.''</ref>\n\nDithering methods include:\n* ''Thresholding'' (also average dithering<ref>{{cite web\n|url = http://www.visgraf.impa.br/Courses/ip00/proj/Dithering1/\n|title = Average Dithering\n|accessdate = 2007-09-10\n|last = Silva\n|first = Aristófanes Correia |author2=Lucena, Paula Salgado |author3=Figuerola, Wilfredo Blanco\n|date = 13 December 2000\n|work = Image Based Artistic Dithering\n|publisher = Visgraf Lab\n}}</ref>): each pixel value is compared against a fixed threshold. This may be the simplest dithering algorithm there is, but it results in immense loss of detail and contouring.<ref name=dhalf />\n* ''Random dithering'' was the first attempt (at least as early as 1951) to remedy the drawbacks of thresholding. Each pixel value is compared against a random threshold, resulting in a staticky image. Although this method doesn't generate patterned artifacts, the noise tends to swamp the detail of the image. It is analogous to the practice of [[mezzotint]]ing.<ref name=dhalf />\n* ''Patterning'' dithers using a fixed pattern.  For each of the input values a fixed pattern is placed in the output image.  The biggest disadvantage of this technique is that the output image is larger (by a factor of the fixed pattern size) than the input pattern.<ref name=dhalf />\n* ''[[Ordered dithering]]'' dithers using a \"dither matrix\".  For every pixel in the image the value of the pattern at the corresponding location is used as a threshold. Neighboring pixels do not affect each other, making this form of dithering suitable for use in animations. Different patterns can generate completely different dithering effects.  Though simple to implement, this dithering algorithm is not easily changed to work with free-form, arbitrary palettes.\n** A ''[[halftone]] dithering'' matrix produces a look similar to that of halftone screening in newspapers. This is a form of clustered dithering, in that dots tend to cluster together. This can help hide the adverse effects of blurry pixels found on some older output devices.  The primary use for this method is in [[offset printing]] and [[laser printer]]s. In both these devices the ink or toner prefers to clump together and will not form the isolated dots generated by the other dithering methods. \n** A ''Bayer matrix''<ref name=dhalf /> produces a very distinctive cross-hatch pattern.\n** A matrix tuned for ''[[blue noise]]'', such as those generated by the \"void-and-cluster\" method,<ref name=voidcluster>{{cite web\n|url = http://cv.ulichney.com/papers/1993-void-cluster.pdf\n|title = The void-and-cluster method for dither array generation\n|accessdate = 2014-02-11\n|last = Ulichney\n|first = Robert A\n|year = 1993\n}}</ref> produces a look closer to that of an error diffusion dither method.  \n{|\n|-\n!(Original)\n!Threshold\n!Random\n|-\n|[[Image:Michelangelo's David - 63 grijswaarden.png]]\n|[[Image:Michelangelo's David - drempel.png]]\n|[[Image:Michelangelo's David - ruis.png]]\n|-\n!Halftone\n!Ordered (Bayer)\n!Ordered (void-and-cluster)\n|-\n|[[Image:Michelangelo's David - halftoon.png]]\n|[[Image:Michelangelo's David - Bayer.png]]\n|[[File:Michelangelo's David - Void-and-Cluster.png]]\n|}\n* ''[[Error diffusion|Error-diffusion]] dithering'' is a feedback process that diffuses the quantization error to neighboring pixels.\n**[[Floyd–Steinberg dithering|Floyd–Steinberg (FS) dithering]] only diffuses the error to neighboring pixels. This results in very fine-grained dithering.\n**[[Minimized average error dithering]] by Jarvis, Judice, and Ninke diffuses the error also to pixels one step further away. The dithering is coarser, but has fewer visual artifacts. However, it is slower than Floyd–Steinberg dithering, because it distributes errors among 12 nearby pixels instead of 4 nearby pixels for Floyd–Steinberg.\n**[[Stucki dithering]] is based on the above, but is slightly faster. Its output tends to be clean and sharp.\n**[[Burkes dithering]] is a simplified form of Stucki dithering that is faster, but is less clean than Stucki dithering.\n{|\n|-\n!Floyd–Steinberg\n!Jarvis, Judice & Ninke\n!Stucki\n!Burkes\n|-\n|[[Image:Michelangelo's David - Floyd-Steinberg.png]]\n|[[Image:Michelangelo's David - Jarvis, Judice & Ninke.png]]\n|[[Image:Michelangelo's David - Stucki.png]]\n|[[Image:Michelangelo's David - Burkes.png]]\n|}\n* Error-diffusion dithering (continued):\n**[[Sierra dithering]] is based on Jarvis dithering, but it's faster while giving similar results.\n**''Two-row Sierra'' is the above method, but was modified by Sierra to improve its speed.\n**''Filter Lite'' is an algorithm by Sierra that is much simpler and faster than Floyd–Steinberg, while still yielding similar results.\n**[[Atkinson dithering]] was developed by Apple programmer [[Bill Atkinson]], and resembles Jarvis dithering and Sierra dithering, but it's faster. Another difference is that it doesn't diffuse the entire quantization error, but only three quarters. It tends to preserve detail well, but very light and dark areas may appear blown out.\n**[[Gradient-based error-diffusion dithering]] was developed in 2016 <ref>{{cite journal | title = Simple gradient-based error-diffusion method | author = Xiangyu Y. Hu| journal = Journal of Electronic Imaging | volume = 25 | issue = 4 |date= 2016| pages = 043029 | url = http://electronicimaging.spiedigitallibrary.org/article.aspx?articleid=2547014 | format = abstract | doi = 10.1117/1.JEI.25.4.043029}}</ref> to remove the structural artifact produced in the original FS algorithm by a modulated randomization, and to enhance the structures by a gradient-based diffusion modulation.\n\n{|\n|-\n!Sierra\n!Two-row Sierra\n!Sierra Lite\n!Atkinson\n!Gradient-based\n|-\n|[[Image:Michelangelo's David - Sierra.png]]\n|[[Image:Michelangelo's David - tweerijig Sierra.png]]\n|[[Image:Michelangelo's David - Sierra's Filter Lite.png]]\n|[[Image:Michelangelo's David - Atkinson.png]]\n|[[File:David-Gradient based.png]]\n|}\n\n== Other applications ==\nStimulated [[Brillouin scattering|Brillouin Scattering]] (SBS) is a [[Nonlinear optics|nonlinear optical effect]] that limits the launched optical power in [[fiber optic]] systems. This power limit can be increased by dithering the transmit optical center frequency, typically implemented by modulating the laser's bias input. See also [[polarization scrambling]].\n\nAn artificial jitter (dither) can be used in electronics for reducing quantization errors in A/D-Elements.<ref>Analog Devices: A Technical Tutorial on Digital Signal Synthesis. 1999. http://www.analog.com/static/imported-files/tutorials/450968421DDS_Tutorial_rev12-2-99.pdf</ref> Another common application is to get through EMC tests by smearing out single frequency peaks.<ref>Lauder, D., Moritz, M.,: Investigation into possible effects resulting from dithered clock oscillators on EMC measurements and interference to radio transmission systems, University of Hertfordshire, 2000. http://www.ofcom.org.uk/static/archive/ra/topics/research/topics/emc/ay3377/invest.htm</ref>\n\nAnother type of temporal dithering has recently been introduced in [[financial market]]s, in order to reduce the incentive to engage in [[high-frequency trading]].  ParFX, a London [[foreign exchange market]] that began trading in 2013, imposes brief random delays on all incoming orders; other currency exchanges are reportedly experimenting with the technique.  The use of such temporal buffering or dithering has been advocated more broadly in financial trading of equities, commodities, and derivatives.<ref>[https://regulatorystudies.columbian.gwu.edu/sites/g/files/zaxdzs1866/f/downloads/Mannix%20Races%20Rushes%20and%20Runs.pdf \"Races, Rushes, and Runs:  Taming the Turbulence in Financial Trading\"], Brian F. Mannix, January 2013.</ref>\n\n== See also ==\n* [[Anti-aliasing (disambiguation)]]\n* [[Digital audio]]\n* [[Jitter]]\n* [[Lossy data compression]]\n* [[Quantization (signal processing)]]\n* [[Stippling]]\n\n== References ==\n{{reflist}}\n\n== External links ==\n*[https://web.archive.org/web/20040206151053/http://www.prosoundweb.com/studyhall/rane/dd/dd2.php \"Dither – Not All Noise Is Bad\"]\n*[http://www.hifi-writer.com/he/dvdaudio/dither.htm What is Dither?] Article previously published in ''Australian HI-FI'' with visual examples of how audio dither sharply reduces high order harmonic distortion.\n\nOther well-written papers on the subject at a more elementary level are available by:\n*Aldrich, Nika. \"[https://web.archive.org/web/20160204184453/http://www.users.qwest.net/~volt42/cadenzarecording/DitherExplained.pdf Dither Explained]\"\n*[http://www.efg2.com/Lab/Library/ImageProcessing/DHALF.TXT DHALF] Explains a lot about dithering, and also includes sufficient detail to implement several dithering algorithms.\n\nBooks with far more comprehensive explanations:\n*[http://www.vibrationdata.com/Newsletters/May2006_NL.pdf Dither Vibration Example]\n\nMore recent research in the field of dither for audio was done by Lipshitz, Vanderkooy, and Wannamaker at the [[University of Waterloo]]:\n*[https://web.archive.org/web/20061210150331/http://audiolab.uwaterloo.ca/stan.htm Stan Lipshitz]\n\n[[Category:Audio engineering]]\n[[Category:Digital signal processing]]\n[[Category:Computer graphic artifacts]]"
    },
    {
      "title": "Jaggies",
      "url": "https://en.wikipedia.org/wiki/Jaggies",
      "text": "[[Image:Test nn.gif|right|frame|This image was scaled up using [[nearest-neighbor interpolation]], and thus the jaggies on the edges of the symbols became more prominent.]]\n'''\"Jaggies\"''' is the informal name for artifacts in [[raster image]]s, most frequently from [[aliasing]],<ref>[http://www.mentallandscape.com/Publications.htm Mitchell, Don],\n\"[http://www.mentallandscape.com/Papers_siggraph90tutorial.pdf The Antialiasing Problem in Ray Tracing]\", Advanced Topics in Ray Tracing, Course Notes, SIGGRAPH 90.</ref> which in turn is often caused by non-linear mixing effects producing high-frequency components or missing or poor [[anti-aliasing filter]]ing prior to sampling.\n\nJaggies are stairlike lines that appear where there should be smooth straight lines or curves. For example, when a nominally straight, un-aliased line steps across one pixel, a dogleg occurs halfway through the line, where it crosses the threshold from one pixel to the other.\n\nJaggies should not be confused with most [[compression artifact]]s, which are a different phenomenon.\n\n== Causes ==\nJaggies occur due to the \"staircase effect\". This is because a line represented in raster mode is approximated by a sequence of pixels. Jaggies can occur for a variety of reasons, the most common being that the output device ([[Computer display|display monitor]] or [[Computer printer|printer]]) does not have enough [[Display resolution|resolution]] to portray a smooth line.<ref>{{cite magazine|last= |first= |title=The Next Generation 1996 Lexicon A to Z: Jaggies|magazine=[[Next Generation (magazine)|Next Generation]]|issue=15 |publisher=[[Imagine Media]]|date=March 1996|page=35}}</ref> In addition, jaggies often occur when a bit-mapped image is converted to a different resolution. This is one of the advantages that [[vector graphics]] has over bit-mapped graphics – the output looks the same regardless of the resolution of the output device.\n\n== Solutions ==\nThe effect of jaggies can be reduced somewhat by a graphics technique known as [[spatial anti-aliasing]]. Anti-aliasing smooths out jagged lines by surrounding the jaggies with transparent [[pixel]]s to simulate the appearance of fractionally-filled pixels. The downside of anti-aliasing is that it reduces contrast – rather than sharp black/white transitions, there are shades of gray – and the resulting image is fuzzy. This is an inescapable trade-off: if the resolution is insufficient to display the desired detail, the output will either be jagged or fuzzy, or some combination thereof.\n\nIn addition, jaggies often occur when a bit mapped image is converted to a different resolution. They can occur for variety of reasons, the most common being that the output device (display monitor or printer) does not have enough resolution to portray a smooth line.\n\nIn realtime computer graphics, especially gaming, anti-aliasing is used to remove jaggies created by the edges of polygons and other lines entirely. Some video game consoles, such as the [[Xbox 360]] and [[PlayStation 3]], have publishing policies which mandated the use of anti-aliasing in some games released for them. Some computer graphics on newer video games are not anti-aliased on video game consoles (Xbox 360 and PlayStation 3), because their hardware can not run graphics smoothly (30 [[Frame rate|frames per second]]) if they are anti-aliased. On [[History of video game consoles (eighth generation)|eighth generation]] video game consoles, such as the [[PlayStation 4]] and [[Xbox One]], anti-aliasing and frame rate has been heavily improved. Jaggies in bitmaps, such as sprites and surface materials, are most often dealt with by separate [[texture filtering]] routines, which are far easier to perform than anti-aliasing filtering. Texture filtering became ubiquitous on PCs after the introduction of [[3Dfx]]'s Voodoo GPU.\n\n== Notable uses of the term ==\nIn the [[Atari 8-bit family|Atari 8-bit]] game ''[[Rescue on Fractalus!]]'', developed by [[LucasArts|Lucasfilm Games]] and published in 1985, the graphics depicting the cockpit of the player's spacecraft contains two window struts, which are not anti-aliased and are therefore very \"jagged\". The developers made fun of this and named the in-game enemies \"Jaggi\", and also initially titled the game ''Behind Jaggi Lines!''. The latter idea was scrapped by the marketing department before release.<ref>[http://www.dadgum.com/halcyon/BOOK/FOX.HTM Interview with David Fox] (from: [[James Hague]]: [[Halcyon Days (book)|''Halcyon Days: Interviews with Classic Computer and Video Game Programmers'']])</ref>\n\n==References==\n{{reflist}}\n\n== See also ==\n* [[Posterization]]\n\n[[Category:Computer graphic artifacts]]\n[[Category:Image processing]]"
    },
    {
      "title": "Morphological antialiasing",
      "url": "https://en.wikipedia.org/wiki/Morphological_antialiasing",
      "text": "'''Morphological antialiasing''', or '''MLAA''', is a technique for minimizing the distortion artifacts known as [[aliasing]] when representing a high-resolution image at a lower resolution. \n\nContrary to [[multisample anti-aliasing]] (MSAA), which does not work for [[Deferred shading|deferred rendering]], MLAA is a [[Video post-processing#Uses in 3D rendering|post-process filtering]] which detects borders in the resulting image and then finds specific patterns in these. Anti-aliasing is achieved by blending pixels in these borders, according to the pattern they belong to and their position within the pattern.<ref name=\"intel\">{{cite web\n| url=https://software.intel.com/sites/default/files/m/d/4/1/d/8/MLAA.pdf\n| title=MLAA: Efficiently Moving Antialiasing from the GPU to the CPU\n| publisher=[[Intel]]\n| accessdate=2018-12-02}}</ref><ref>{{cite web\n| url=http://igm.univ-mlv.fr/~biri/mlaa-gpu/TMLAA.pdf\n| title=MORPHOLOGICAL ANTIALIASING AND TOPOLOGICAL RECONSTRUCTION\n| publisher=Institut d'électronique et d'informatique Gaspard-Monge (IGM)\n| accessdate=2018-12-02}}</ref><ref name=\"comparison\">{{cite web\n| url=https://www.eurogamer.net/articles/digital-foundry-future-of-anti-aliasing\n| title=Digital Foundry: The Future of Anti-Aliasing \n| publisher=[[Eurogamer]]\n| date=2011-07-16\n| accessdate=2018-12-02}}</ref>\n\nEnhanced subpixel morphological antialiasing, or SMAA, is an [[Image-based modeling and rendering|image-based]] GPU-based implementation of MLAA<ref>{{cite web\n| url=https://github.com/iryoku/smaa\n| title=iryoku/smaa: SMAA is a very efficient GPU-based MLAA implementation\n| accessdate=2018-12-13}}</ref> developed by [[Universidad de Zaragoza]] and [[Crytek]]<ref>{{cite journal\n| url=http://www.iryoku.com/smaa/\n| title=SMAA: Enhanced Subpixel Morphological Antialiasing\n| journal=Computer Graphics Forum (Proc. EUROGRAPHICS 2012)\n| author=Jorge Jimenez and Jose I. Echevarria and Tiago Sousa and Diego Gutierrez\n| id=JIMENEZ2012_CGF\n| year=2012\n| volume=31\n| number=2\n| accessdate=2018-12-13}}</ref>.\n\n==See also==\n* [[Fast approximate anti-aliasing]]\n* [[Multisample anti-aliasing]]\n* [[Anisotropic filtering]]\n* [[Temporal anti-aliasing]]\n* [[Spatial anti-aliasing]]\n\n==References==\n{{Reflist}}\n\n[[Category:Image processing]]\n[[Category:Computer graphic artifacts]]\n[[Category:Anti-aliasing algorithms]]\n\n{{compu-stub}}"
    },
    {
      "title": "Pixelation",
      "url": "https://en.wikipedia.org/wiki/Pixelation",
      "text": "{{about|the graphics artifact|the stop motion animation technique|Pixilation|the graphical editing and censorship technique|Pixelization}}\n{{Unreferenced|date=April 2008}}\n[[Image:Dithering example undithered.png|right|frame|An example of pixelation. The image looks smooth when zoomed out, but when a small section is viewed more closely, the eye can distinguish individual pixels.]]\nIn [[computer graphics]], '''pixelation''' (or '''pixellation''' in [[British English]]) is caused by displaying a [[bitmap]] or a section of a bitmap at such a large size that individual [[pixel]]s, small single-colored square display elements that comprise the bitmap, are visible. Such an image is said to be '''[[Wiktionary:pixelated|pixelated]]''' ('''[[Wiktionary:pixellated|pixellated]]''' in the UK).\n\n[[Image:Diamond anti-aliasing demo.png|left|frame|A [[Rhombus|diamond]] without&nbsp;(left) and with&nbsp;(right) [[anti-aliasing]] ]]\nEarly graphical applications such as [[video game]]s ran at very low [[Image resolution|resolution]]s with a small number of colors, resulting in easily visible pixels. The resulting sharp edges gave curved objects and diagonal lines an unnatural appearance. However, when the number of available colors increased to 256, it was possible to gainfully employ [[spatial anti-aliasing|anti-aliasing]] to smooth the appearance of low-resolution objects, not eliminating pixelation but making it less jarring to the eye. Higher resolutions would soon make this type of pixelation all but invisible on the screen, but pixelation is still visible if a low-resolution image is printed on paper. \n\nIn the realm of real-time [[3D computer graphics]], pixelation can be a problem. Here, bitmaps are applied to polygons as [[texture mapping|texture]]s. As a camera approaches a textured polygon, simplistic [[nearest neighbor interpolation algorithm|nearest neighbor]] [[texture filtering]] would simply zoom in on the bitmap, creating drastic pixelation. The most common solution is a technique called ''pixel interpolation'' that smoothly blends or [[interpolate]]s the color of one pixel into the color of the next adjacent pixel at high levels of zoom. This creates a more organic, but also much blurrier image. There are a number of ways of doing this; see ''[[texture filtering]]'' for details.\n\nPixelation is a problem unique to bitmaps. Alternatives such as [[vector graphics]] or purely geometric polygon models can scale to any level of detail. This is one reason vector graphics are popular for printing{{snd}} most modern computer monitors have a resolution of about 100 dots per inch, and at 300 dots per inch printed documents have about nine times as many pixels per unit of area as a screen. Another solution sometimes used is [[procedural texture]]s, textures such as [[fractal]]s that can be generated on-the-fly at arbitrary levels of detail.  \n\n[[Image:Pixel interpolation.png|right|frame|The zoomed portion of the cat image above, resized using nearest neighbor ''(left)'' and with [[Adobe Photoshop]]'s [[bicubic interpolation|bicubic resampling]], which uses pixel interpolation ''(right)''. The interpolated image has no sharp edges, but is considerably blurrier.]]\n\n<gallery>\nImagepixeledanon.png \n</gallery>\n\n== Deliberate pixelation ==\nIn some cases, the resolution of an image or a portion of an image is lowered to introduce pixelation deliberately. This effect is commonly used on television news shows to obscure a person's face or to censor [[nudity]] or [[vulgarity|vulgar]] gestures, and is also used for artistic effect. This effect is called ''[[pixelization]]''. Making pixels easily visible is also a main feature in [[pixel art]] which is where the graphics are made in low resolutions for effect.\n\n{{-}}\n\n== See also ==\n* [[Colour banding]]\n* [[Macroblocking]]\n* [[Posterization]]\n* [[Pixel art]]\n\n== External links ==\n* [http://www.windowsphotostory.com/Guides/NoPixelZoom/ Zooming Without Pixelation], digital camera advice by Mark Coffman\n* [http://demonstrations.wolfram.com/PixelizationOfAFont/ Pixelization of a Font] by [[Stephen Wolfram]], [[The Wolfram Demonstrations Project]].\n\n[[Category:Computer graphic artifacts]]"
    },
    {
      "title": "Posterization",
      "url": "https://en.wikipedia.org/wiki/Posterization",
      "text": "{{refimprove|date=January 2011}}\n[[Image:Posterization example.jpg|thumb|300px|Example of a photograph in [[JPEG]] format (24-bit color or 16.7 million colors) before posterization, contrasting the result of saving to [[GIF]] format (256 colors). Posterization occurs across the image, but is most obvious in areas of subtle variation in tone.]]\n[[Image:Lucidity I.jpg|thumb|300px|Posterized photo of a [[hibiscus]].]]\n\n'''Posterization''' or '''posterisation''' of an image entails conversion of a continuous gradation of tone to several regions of fewer tones, with abrupt changes from one tone to another.  This was originally done with photographic processes to create [[poster]]s.  It can now be done photographically or with digital image processing, and may be deliberate or an unintended artifact of [[color quantization]].\n\n==Cause==\nThe effect may be created deliberately, or happen accidentally.  For artistic effect, most [[image editing]] programs provide a posterization feature, or photographic processes may be used. \n\nUnwanted posterization, also known as [[color banding|banding]], may occur when the [[color depth]], sometimes called bit depth, is insufficient to accurately sample a continuous gradation of color tone. As a result, a continuous gradient appears as a series of discrete steps or bands of color — hence the name. When discussing [[fixed pixel display]]s, such as LCD and plasma televisions, this effect is referred to as false contouring.<ref>{{Cite web |url=http://www.cnet.com/4520-7874_1-5107912-7.html#falsecontouring |title=HDTV World Glossary |accessdate=2007-06-06 |year=2007 |publisher=CNET Networks}}</ref> Additionally, [[Compression artifact|compression]] in image formats such as [[JPEG]] can also result in posterization when a smooth gradient of colour or luminosity is compressed into discrete quantized blocks with stepped gradients. The result may be compounded further by an [[optical illusion]], called the [[Mach bands|Mach band illusion]], in which each band appears to have an intensity gradient in the direction opposing the overall gradient. This problem may be resolved, in part, with [[Dither#Digital photography and image processing|dithering]].\n\n== Photographic process ==\nPosterization is a process in photograph development which converts normal photographs into an image consisting of distinct, but flat, areas of different tones or colors. A posterized image often has the same general appearance, but portions of the original image that presented gradual transitions are replaced by abrupt changes in shading and gradation from one area of tone to another. Printing posterization from black and white requires density separations, which one then prints on the same piece of paper to create the whole image. Separations may be made by density or color, using different exposures. Density Separations may be created by printing three prints of the same picture, each at a different exposure time that will be combined for the final image.\n\n== Applications ==\nTypically, posterization is used for tracing [[contour lines]] and [[raster to vector|vectorizing photo-realistic images]]. This tracing process starts with 1 bit per channel and advances to 4 bits per channel. As the bits per channel increases, the number of levels of lightness a color can display increases.\n\nA visual artist, faced with [[line art]] that has been damaged through JPEG compression, may consider posterizing the image as a first step to remove artifacts on the edges of the image.\n\n==Posterizing time {{anchor|Temporal posterization}} ==\nTemporal posterization is the [[visual effect]] of reducing the [[frame rate|number of frames]] of [[video]], while not reducing the speed at which it actually plays.  This compares to regular posterization, where the number of individual color variations is reduced, while the overall range of colors is not.  The motion effect is similar to the effect of a flashing strobe light, but without the contrast of bright and dark.  Unlike a [[Telecine|pulldown]], the unused frames are simply discarded, and it is intended to be apparent (longer than the [[persistence of vision]] that [[video]] and [[motion picture]]s normally depend on). An [[animated GIF]] often looks posterized because of its normally-low frame rate.\n\nMore formally, this is [[downsampling]] in the time dimension, as it is reducing the resolution (precision of the ''input''), not the bit rate (precision of the ''output,'' as in posterization).\n\nThe resulting stop-go motion is a temporal form of [[jaggies]]; formally, a form of [[aliasing]]. This effect may be the intention, but to reduce the frame rate without introducing this effect, one may use [[temporal anti-aliasing]], which yields [[motion blur]].\n\nCompare with [[Slow motion#Time stretching|time stretching]], which ''adds'' frames.\n\n==See also==\n* [[Downsampling]]\n* [[Quantization error]]\n* [[Discretization error]]\n* [[Color quantization]]\n\n==References==\n* Langford, Michael. ''The Darkroom Handbook''. New York: Dorling Kindersley Limited, 1981. 245-249.\n* [[Jasc Software]]. ''Paint Shop Pro'' Help, 1998.\n\n==Notes==\n{{reflist}}\n\n==External links==\n{{Commons category}}\n* https://web.archive.org/web/20060106051841/http://desktoppub.about.com/cs/graphicstips/f/posterization.htm\n* https://web.archive.org/web/20060202015057/http://www.sphoto.com/techinfo/wdtech.html\n\n[[Category:Artistic techniques]]\n[[Category:Posters]]\n[[Category:Computer graphic artifacts]]\n[[Category:Digital photography]]"
    },
    {
      "title": "Quantization (signal processing)",
      "url": "https://en.wikipedia.org/wiki/Quantization_%28signal_processing%29",
      "text": "{{Use American English|date=April 2019}}\n\n[[File:Quantization error.png|thumb|500px|The simplest way to quantize a signal is to choose the digital amplitude value closest to the original analog amplitude. This example shows the original analog signal (green), the quantized signal (black dots), the [[Signal reconstruction|signal reconstructed]] from the quantized signal (yellow) and the difference between the original signal and the reconstructed signal (red). The difference between the original signal and the reconstructed signal is the quantization error and, in this simple quantization scheme, is a deterministic function of the input signal.]]\n\n'''Quantization''', in mathematics and [[digital signal processing]], is the process of mapping input values from a large set (often a continuous set) to output values in a (countable) smaller set, often with a finite [[Cardinality|number of elements]].  [[Rounding]] and [[truncation]] are typical examples of quantization processes.  Quantization is involved to some degree in nearly all digital signal processing, as the process of representing a signal in digital form ordinarily involves rounding.  Quantization also forms the core of essentially all [[lossy compression]] algorithms.\n\nThe difference between an input value and its quantized value (such as [[round-off error]]) is referred to as '''quantization error'''.  A device or [[algorithm function|algorithmic function]] that performs quantization is called a '''quantizer'''.  An [[analog-to-digital converter]] is an example of a quantizer.\n\n==Mathematical properties==\nBecause quantization is a many-to-few mapping, it is an inherently [[Nonlinear system|non-linear]] and irreversible process (i.e., because the same output value is shared by multiple input values, it is impossible, in general, to recover the exact input value when given only the output value).\n\nThe set of possible input values may be infinitely large, and may possibly be continuous and therefore [[uncountable]] (such as the set of all [[real number]]s, or all real numbers within some limited range). The set of possible output values may be [[finite set|finite]] or [[Countable set|countably infinite]].<ref name=GrayNeuhoff/> The input and output sets involved in quantization can be defined in a rather general way. For example, [[vector quantization]] is the application of quantization to multi-dimensional (vector-valued) input data.<ref>{{cite book |author1=[[Allen Gersho]] |author2=Robert M. Gray |author-link2=Robert M. Gray |url=https://books.google.com/books/about/Vector_Quantization_and_Signal_Compressi.html?id=DwcDm6xgItUC |title=Vector Quantization and Signal Compression |publisher=[[Springer Science+Business Media|Springer]] |isbn=978-0-7923-9181-4 |date=1991}}</ref>\n\n==Basic types of quantization==\n[[File:2-bit resolution analog comparison.png|thumbnail|2-bit resolution with four levels of quantization compared to analog.<ref>Hodgson, Jay (2010). ''Understanding Records'', p.56. {{ISBN|978-1-4411-5607-5}}. Adapted from Franz, David (2004). ''Recording and Producing in the Home Studio'', p.38-9. Berklee Press.</ref>]]\n[[File:3-bit resolution analog comparison.png|thumbnail|3-bit resolution with eight levels.]]\n\n===Analog-to-digital converter===\nAn [[analog-to-digital converter]] (ADC) can be modeled as two processes: [[Sampling (signal processing)|sampling]] and quantization.  Sampling converts a time-varying voltage signal into a [[discrete-time signal]], a sequence of [[real number]]s.  Quantization replaces each real number with an approximation from a finite set of discrete values.  Most commonly, these discrete values are represented as fixed-point words. Though any number of quantization levels is possible, common word-lengths are [[8-bit]] (256 levels), [[16-bit]] (65,536 levels) and [[24-bit]] (16.8&nbsp;million levels). Quantizing a sequence of numbers produces a sequence of quantization errors which is sometimes modeled as an additive random signal called '''quantization noise''' because of its [[stochastic]] behavior.  The more levels a quantizer uses, the lower is its quantization noise power.\n\n===Rate–distortion optimization===\n''[[Rate–distortion theory|Rate–distortion optimized]]'' quantization is encountered in [[source coding]] for lossy data compression algorithms, where the purpose is to manage distortion within the limits of the bit rate supported by a communication channel or storage medium. The analysis of quantization in this context involves studying the amount of data (typically measured in digits or bits or bit ''rate'') that is used to represent the output of the quantizer, and studying the loss of precision that is introduced by the quantization process (which is referred to as the ''distortion'').\n\n== Rounding example ==\nAs an example, [[Rounding#Round half up|rounding]] a [[real number]] <math>x</math> to the nearest integer value forms a very basic type of quantizer – a ''uniform'' one.  A typical (''mid-tread'') uniform quantizer with a quantization ''step size'' equal to some value <math>\\Delta</math> can be expressed as\n\n:<math>Q(x) = \\Delta \\cdot  \\left\\lfloor \\frac{x}{\\Delta} + \\frac{1}{2} \\right\\rfloor =  \\Delta \\cdot \\operatorname{floor}\\left( \\frac{x}{\\Delta} + \\frac{1}{2} \\right)</math>,\n\nwhere the notation <math> \\lfloor \\ \\rfloor </math> or <math>\\operatorname{floor}( \\ )</math> depicts the [[floor function]].\n\nThe essential property of a quantizer is that it has a countable set of possible output values that has fewer members than the set of possible input values. The members of the set of output values may have integer, rational, or real values. For simple rounding to the nearest integer, the step size <math>\\Delta</math> is equal to 1. With <math>\\Delta = 1</math> or with <math>\\Delta</math> equal to any other integer value, this quantizer has real-valued inputs and integer-valued outputs.\n\nWhen the quantization step size (Δ) is small relative to the variation in the signal being quantized, it is relatively simple to show that the [[mean squared error]] produced by such a rounding operation will be approximately <math>\\Delta^2/ 12</math>.<ref name=Sheppard>[[William Fleetwood Sheppard]], \"On the Calculation of the Most Probable Values of Frequency Constants for data arranged according to Equidistant Divisions of a Scale\", ''[[Proceedings of the London Mathematical Society]]'', Vol. 29, pp. 353&ndash;80, 1898.{{doi|10.1112/plms/s1-29.1.353}}</ref><ref name=Bennett>W. R. Bennett, \"[http://www.alcatel-lucent.com/bstj/vol27-1948/articles/bstj27-3-446.pdf Spectra of Quantized Signals]\", ''[[Bell System Technical Journal]]'', Vol. 27, pp. 446–472, July 1948.</ref><ref name=OliverPierceShannon>B. M. Oliver, J. R. Pierce, and [[Claude Shannon|Claude E. Shannon]], \"The Philosophy of PCM\", ''[[Proceedings of the IEEE|Proceedings of the IRE]]'', Vol. 36, pp. 1324–1331, Nov. 1948. {{doi|10.1109/JRPROC.1948.231941}}</ref><ref name=Stein>Seymour Stein and J. Jay Jones, ''[https://books.google.com/books/about/Modern_communication_principles.html?id=jBc3AQAAIAAJ Modern Communication Principles]'', [[McGraw–Hill]], {{ISBN|978-0-07-061003-3}}, 1967 (p. 196).</ref><ref name=GishPierce>Herbert Gish and John N. Pierce, \"Asymptotically Efficient Quantizing\", ''[[IEEE Transactions on Information Theory]]'', Vol. IT-14, No. 5, pp. 676–683, Sept. 1968. {{doi|10.1109/TIT.1968.1054193}}</ref><ref name=GrayNeuhoff>[[Robert M. Gray]] and David L. Neuhoff, \"Quantization\", ''[[IEEE Transactions on Information Theory]]'', Vol. IT-44, No. 6, pp. 2325–2383, Oct. 1998. {{doi|10.1109/18.720541}}</ref> Mean squared error is also called the quantization ''noise power''.  Adding one bit to the quantizer halves the value of Δ, which reduces the noise power by the factor ¼. In terms of [[decibels]], the noise power change is <math>\\scriptstyle 10\\cdot \\log_{10}\\left(\\tfrac{1}{4}\\right)\\ \\approx\\ -6\\ \\mathrm{dB}.</math>\n\nBecause the set of possible output values of a quantizer is countable, any quantizer can be decomposed into two distinct stages, which can be referred to as the ''classification'' stage (or ''forward quantization'' stage) and the ''reconstruction'' stage (or ''inverse quantization'' stage), where the classification stage maps the input value to an integer ''quantization index'' <math>k</math> and the reconstruction stage maps the index <math>k</math> to the ''reconstruction value'' <math>y_k</math> that is the output approximation of the input value.  For the example uniform quantizer described above, the forward quantization stage can be expressed as\n:<math>k = \\left\\lfloor \\frac{x}{\\Delta} + \\frac{1}{2}\\right\\rfloor</math>,\nand the reconstruction stage for this example quantizer is simply \n:<math>y_k = k \\cdot \\Delta</math>.\n\nThis decomposition is useful for the design and analysis of quantization behavior, and it illustrates how the quantized data can be communicated over a communication channel – a ''source encoder'' can perform the forward quantization stage and send the index information through a communication channel, and a ''decoder'' can perform the reconstruction stage to produce the output approximation of the original input data. In general, the forward quantization stage may use any function that maps the input data to the integer space of the quantization index data, and the inverse quantization stage can conceptually (or literally) be a table look-up operation to map each quantization index to a corresponding reconstruction value. This two-stage decomposition applies equally well to [[vector quantization|vector]] as well as scalar quantizers.\n\n== Mid-riser and mid-tread uniform quantizers ==\nMost uniform quantizers for signed input data can be classified as being of one of two types: ''mid-riser'' and ''mid-tread''. The terminology is based on what happens in the region around the value 0, and uses the analogy of viewing the input-output function of the quantizer as a [[stairway]]. Mid-tread quantizers have a zero-valued reconstruction level (corresponding to a ''tread'' of a stairway), while mid-riser quantizers have a zero-valued classification threshold (corresponding to a ''[[Stair riser|riser]]'' of a stairway).<ref name=Gersho77>[[Allen Gersho]], \"Quantization\", ''[[IEEE Communications Magazine|IEEE Communications Society Magazine]]'', pp. 16–28, Sept. 1977. {{doi|10.1109/MCOM.1977.1089500}}</ref>\n\nMid-tread quantization involves rounding. The formulas for mid-tread uniform quantization are provided in the previous section.\n\nMid-riser quantization involves truncation. The input-output formula for a mid-riser uniform quantizer is given by:\n:<math>Q(x) = \\Delta\\cdot\\left(\\left\\lfloor \\frac{x}{\\Delta}\\right\\rfloor + \\frac1{2}\\right)</math>,\nwhere the classification rule is given by\n:<math>k = \\left\\lfloor \\frac{x}{\\Delta} \\right\\rfloor</math>\nand the reconstruction rule is\n:<math>y_k = \\Delta\\cdot\\left(k+\\tfrac1{2}\\right)</math>.\n\nNote that mid-riser uniform quantizers do not have a zero output value – their minimum output magnitude is half the step size. In contrast, mid-tread quantizers do have a zero output level. For some applications, having a zero output signal representation may be a necessity.\n\nIn general, a mid-riser or mid-tread quantizer may not actually be a ''uniform'' quantizer – i.e., the size of the quantizer's classification intervals may not all be the same, or the spacing between its possible output values may not all be the same. The distinguishing characteristic of a mid-riser quantizer is that it has a classification threshold value that is exactly zero, and the distinguishing characteristic of a mid-tread quantizer is that is it has a reconstruction value that is exactly zero.<ref name=Gersho77/>\n\n== Dead-zone quantizers ==\nA '''dead-zone quantizer''' is a type of mid-tread quantizer with symmetric behavior around 0. The region around the zero output value of such a quantizer is referred to as the ''dead zone'' or ''[[deadband]]''. The dead zone can sometimes serve the same purpose as a [[noise gate]] or [[squelch]] function. Especially for compression applications, the dead-zone may be given a different width than that for the other steps. For an otherwise-uniform quantizer, the dead-zone width can be set to any value <math>w</math> by using the forward quantization rule<ref>{{cite book| first1=Majid |last1=Rabbani |first2=Rajan L. |last2=Joshi |first3=Paul W. |last3=Jones |editor1-first=Peter |editor1-last=Schelkens |editor2-first=Athanassios |editor2-last=Skodras |editor3-first=Touradj |editor3-last=Ebrahimi |title=The JPEG 2000 Suite |publisher=[[John Wiley & Sons]] |date=2009 |isbn=978-0-470-72147-6 |chapter=Section 1.2.3: Quantization, in Chapter 1: JPEG 2000 Core Coding System (Part 1) |pages=22–24}}</ref><ref>{{cite book| first1=David S. |last1=Taubman |first2=Michael W. |last2=Marcellin |title=JPEG2000: Image Compression Fundamentals, Standards and Practice |publisher=[[Kluwer Academic Publishers]] |date=2002 |isbn=0-7923-7519-X |chapter=Chapter 3: Quantization |page=107}}</ref><ref name=SullivanIT/>\n:<math>k = \\sgn(x) \\cdot \\max\\left(0, \\left\\lfloor \\frac{\\left| x \\right|-w/2}{\\Delta}+1\\right\\rfloor\\right)</math>,\nwhere the function {{no break|<math>\\sgn</math>(&nbsp;)}} is the [[sign function]] (also known as the ''signum'' function). The general reconstruction rule for such a dead-zone quantizer is given by\n:<math>y_k = \\sgn(k) \\cdot\\left(\\frac{w}{2}+\\Delta\\cdot (|k|-1+r_k)\\right)</math>,\nwhere <math>r_k</math> is a reconstruction offset value in the range of 0 to 1 as a fraction of the step size. Ordinarily, <math>0 \\le r_k \\le \\tfrac1{2}</math> when quantizing input data with a typical pdf that is symmetric around zero and reaches its peak value at zero (such as a [[Gaussian distribution|Gaussian]], [[Laplacian distribution|Laplacian]], or [[generalized Gaussian distribution|generalized Gaussian]] pdf). Although <math>r_k</math> may depend on <math>k</math> in general, and can be chosen to fulfill the optimality condition described below, it is often simply set to a constant, such as <math>\\tfrac1{2}</math>. (Note that in this definition, <math>y_0 = 0</math> due to the definition of the {{no break|<math>\\sgn</math>(&nbsp;)}} function, so <math>r_0</math> has no effect.)\n\nA very commonly used special case (e.g., the scheme typically used in financial accounting and elementary mathematics) is to set <math>w=\\Delta</math> and <math>r_k=\\tfrac1{2}</math> for all <math>k</math>. In this case, the dead-zone quantizer is also a uniform quantizer, since the central dead-zone of this quantizer has the same width as all of its other steps, and all of its reconstruction values are equally spaced as well.\n\n==Granular distortion and overload distortion==\nOften the design of a quantizer involves supporting only a limited range of possible output values and performing clipping to limit the output to this range whenever the input exceeds the supported range. The error introduced by this clipping is referred to as ''overload'' distortion.  Within the extreme limits of the supported range, the amount of spacing between the selectable output values of a quantizer is referred to as its ''granularity'', and the error introduced by this spacing is referred to as ''granular'' distortion.  It is common for the design of a quantizer to involve determining the proper balance between granular distortion and overload distortion. For a given supported number of possible output values, reducing the average granular distortion may involve increasing the average overload distortion, and vice versa.  A technique for controlling the amplitude of the signal (or, equivalently, the quantization step size <math>\\Delta</math>) to achieve the appropriate balance is the use of ''[[automatic gain control]]'' (AGC). However, in some quantizer designs, the concepts of granular error and overload error may not apply (e.g., for a quantizer with a limited range of input data or with a countably infinite set of selectable output values).<ref name=GrayNeuhoff/>\n\n==The additive noise model for quantization error==\nA common assumption for the analysis of [[quantization error]] is that it affects a signal processing system in a similar manner to that of additive [[white noise]] – having negligible correlation with the signal and an approximately flat [[power spectral density]].<ref name=Bennett/><ref name=GrayNeuhoff/><ref name=Widrow1>[[Bernard Widrow]], \"A study of rough amplitude quantization by means of Nyquist sampling theory\", ''IRE Trans. Circuit Theory'', Vol. CT-3, pp. 266–276, 1956. {{doi|10.1109/TCT.1956.1086334}}</ref><ref name=Widrow2>[[Bernard Widrow]], \"[http://www-isl.stanford.edu/~widrow/papers/j1961statisticalanalysis.pdf Statistical analysis of amplitude quantized sampled data systems]\", ''Trans. AIEE Pt. II: Appl. Ind.'', Vol. 79, pp. 555–568, Jan. 1961.</ref> The additive noise model is commonly used for the analysis of quantization error effects in digital filtering systems, and it can be very useful in such analysis. It has been shown to be a valid model in cases of high resolution quantization (small <math>\\Delta</math> relative to the signal strength) with smooth probability density functions.<ref name=Bennett/><ref name=MarcoNeuhoff>Daniel Marco and David L. Neuhoff, \"The Validity of the Additive Noise Model for Uniform Scalar Quantizers\", ''[[IEEE Transactions on Information Theory]]'', Vol. IT-51, No. 5, pp. 1739–1755, May 2005. {{doi|10.1109/TIT.2005.846397}}</ref> \n\nAdditive noise behavior is not always a valid assumption. Quantization error (for quantizers defined as described here) is deterministically related to the signal and not entirely independent of it. Thus, periodic signals can create periodic quantization noise.  And in some cases it can even cause [[limit cycle]]s to appear in digital signal processing systems. One way to ensure effective independence of the quantization error from the source signal is to perform ''[[dither]]ed quantization'' (sometimes with ''[[noise shaping]]''), which involves adding random (or [[pseudo-random]]) noise to the signal prior to quantization.<ref name=GrayNeuhoff/><ref name=Widrow2/><!--[[User:Kvng/RTH]]-->\n\n==Quantization error models==\nIn the typical case, the original signal is much larger than one [[least significant bit|least significant bit (LSB)]]. When this is the case, the quantization error is not significantly correlated with the signal, and has an approximately [[uniform distribution (continuous)|uniform distribution]]. In the rounding case, the quantization error has a mean of zero and the [[root mean square|RMS]] value is the [[standard deviation]] of this distribution, given by <math>\\scriptstyle {\\frac{1}{\\sqrt{12}}}\\mathrm{LSB}\\ \\approx\\ 0.289\\,\\mathrm{LSB}</math>. In the truncation case the error has a non-zero mean of <math>\\scriptstyle {\\frac{1}{2}}\\mathrm{LSB}</math> and the RMS value is <math>\\scriptstyle {\\frac{1}{\\sqrt{3}}}\\mathrm{LSB}</math>.  In either case, the standard deviation, as a percentage of the full signal range, changes by a factor of 2 for each 1-bit change in the number of quantizer bits.  The potential signal-to-quantization-noise power ratio therefore changes by 4, or &nbsp;<math>\\scriptstyle 10\\cdot \\log_{10}(4)\\ =\\ 6.02</math>&nbsp; ''decibels per bit''.\n\nAt lower amplitudes the quantization error becomes dependent on the input signal, resulting in distortion. This distortion is created after the anti-aliasing filter, and if these distortions are above 1/2 the sample rate they will alias back into the band of interest. In order to make the quantization error independent of the input signal, noise with an amplitude of 2 least significant bits is added to the signal. This slightly reduces signal to noise ratio, but, ideally, completely eliminates the distortion.  It is known as [[dither]].\n\n==Quantization noise model==\n[[File:quanterr.png|thumb|300px|Quantization noise for a 2-bit ADC operating at infinite [[sample rate]]. The difference between the blue and red signals in the upper graph is the quantization error, which is \"added\" to the quantized signal and is the source of noise.]]\n[[File:Frequency spectrum of a sinusoid and its quantization noise floor.gif|thumb|300px|Comparison of quantizing a sinusoid to 64 levels (6 bits) and 256 levels (8 bits).  The additive noise created by 6-bit quantization is 12 dB greater than the noise created by 8-bit quantization.  When the spectral distribution is flat, as in this example, the 12 dB difference manifests as a measurable difference in the noise floors.]]\n\nQuantization noise is a [[Model (abstract)|model]] of quantization error introduced by quantization in the [[Analog-to-digital converter|analog-to-digital conversion]] (ADC) in\n[[Communications system|telecommunication systems]] and [[Digital signal processing|signal processing]]. It is a rounding error between the analog input voltage to the ADC and the output digitized value. The noise is non-linear and signal-dependent. It can be modelled in several different ways.\n\nIn an ideal analog-to-digital converter, where the quantization error is uniformly distributed between −1/2 LSB and +1/2 LSB, and the signal has a uniform distribution covering all quantization levels, the [[Signal-to-quantization-noise ratio]] (SQNR) can be calculated from\n\n:<math>\\mathrm{SQNR} = 20 \\log_{10}(2^Q) \\approx 6.02 \\cdot Q\\ \\mathrm{dB} \\,\\!</math>\n\nWhere Q is the number of quantization bits.\n\nThe most common test signals that fulfill this are full amplitude [[triangle wave]]s and [[sawtooth wave]]s.\n\nFor example, a [[16-bit]] ADC has a maximum signal-to-noise ratio of 6.02 × 16 = 96.3&nbsp;dB.\n\nWhen the input signal is a full-amplitude [[sine wave]] the distribution of the signal is no longer uniform, and the corresponding equation is instead\n\n:<math> \\mathrm{SQNR} \\approx  1.761 + 6.02 \\cdot Q \\ \\mathrm{dB} \\,\\!</math>\n\nHere, the quantization noise is once again ''assumed'' to be uniformly distributed.  When the input signal has a high amplitude and a wide frequency spectrum this is the case.<ref>{{cite book\n  | last = Pohlman\n  | first =Ken C.\n  | title = Principles of Digital Audio 2nd Edition\n  | publisher = SAMS\n  | date = 1989\n  | page = 60\n  | url = https://books.google.com/books?id=VZw6z9a03ikC&pg=PA37&source=gbs_selected_pages&cad=0_1}}</ref>  In this case a 16-bit ADC has a maximum signal-to-noise ratio of 98.09&nbsp;dB.  The 1.761 difference in signal-to-noise only occurs due to the signal being a full-scale sine wave instead of a triangle/sawtooth.\n\nQuantization noise power can be derived  from\n\n:<math>\\mathrm{N} = \\frac {(\\delta \\mathrm{v})^2} { 12 } \\mathrm{W} \\,\\!</math>\n\nwhere <math>\\delta \\mathrm{v}</math> is the voltage of the level.\n\n(Typical real-life values are worse than this theoretical minimum, due to the addition of [[dither]] to reduce the objectionable effects of quantization, and to imperfections of the ADC circuitry.  Also see [[noise shaping]].)\n\nFor complex signals in high-resolution ADCs this is an accurate model. For low-resolution ADCs, low-level signals in high-resolution ADCs, and for simple waveforms the quantization noise is not uniformly distributed, making this model inaccurate.<ref>{{cite book\n  | last = Watkinson\n  | first = John\n  | title = The Art of Digital Audio 3rd Edition\n  | publisher = [[Focal Press]]\n  | date = 2001\n  | isbn = 0-240-51587-0}}</ref> In these cases the quantization noise distribution is strongly affected by the exact amplitude of the signal.\n\nThe calculations above, however, assume a completely filled input channel. If this is not the case - if the input signal is small - the relative quantization distortion can be very large. To circumvent this issue, analog [[dynamic range compression|compressors and expanders]] can be used, but these introduce large amounts of distortion as well, especially if the compressor does not match the expander. The application of such compressors and expanders is also known as [[companding]].\n\n== Rate–distortion quantizer design ==\nA scalar quantizer, which performs a quantization operation, can ordinarily be decomposed into two stages:\n* '''Classification:''' A process that classifies the input signal range into <math>M</math> non-overlapping '''intervals''' <math>\\{I_k\\}_{k=1}^{M}</math>, by defining <math>M-1</math> '''boundary (decision)''' values <math> \\{b_k\\}_{k=1}^{M-1} </math>, such that <math> I_k = [b_{k-1}~,~b_k)</math> for <math>k = 1,2,\\ldots,M</math>, with the extreme limits defined by <math> b_0 = -\\infty</math> and <math> b_M = \\infty</math>. All the inputs <math>x</math> that fall in a given interval range <math>I_k</math> are associated with the same quantization index <math>k</math>.\n* '''Reconstruction:''' Each interval <math> I_k </math> is represented by a '''reconstruction value''' <math> y_k </math> which implements the mapping <math> x \\in I_k \\Rightarrow y = y_k </math>.\n\nThese two stages together comprise the mathematical operation of <math>y = Q(x)</math>.\n\n[[Entropy coding]] techniques can be applied to communicate the quantization indices from a source encoder that performs the classification stage to a decoder that performs the reconstruction stage. One way to do this is to associate each quantization index <math>k</math> with a binary codeword <math>c_k</math>. An important consideration is the number of bits used for each codeword, denoted here by <math>\\mathrm{length}(c_k)</math>.\n\nAs a result, the design of an <math>M</math>-level quantizer and an associated set of codewords for communicating its index values requires finding the values of <math> \\{b_k\\}_{k=1}^{M-1} </math>, <math>\\{c_k\\}_{k=1}^{M} </math> and <math> \\{y_k\\}_{k=1}^{M} </math> which optimally satisfy a selected set of design constraints such as the '''bit rate''' <math>R</math> and '''distortion''' <math>D</math>.\n\nAssuming that an information source <math>S</math> produces random variables <math>X</math> with an associated [[probability density function]] <math>f(x)</math>, the probability <math>p_k</math> that the random variable falls within a particular quantization interval <math>I_k</math> is given by\n:<math> p_k = P[x \\in I_k] = \\int_{b_{k-1}}^{b_k} f(x)dx </math>.\n\nThe resulting bit rate <math>R</math>, in units of average bits per quantized value, for this quantizer can be derived as follows:\n:<math> R = \\sum_{k=1}^{M} p_k \\cdot \\mathrm{length}(c_{k}) = \\sum_{k=1}^{M} \\mathrm{length}(c_k) \\int_{b_{k-1}}^{b_k} f(x)dx </math>.\n\nIf it is assumed that distortion is measured by [[mean squared error]], the distortion '''D''', is given by:\n:<math> D = E[(x-Q(x))^2] = \\int_{-\\infty}^{\\infty} (x-Q(x))^2f(x)dx = \\sum_{k=1}^{M} \\int_{b_{k-1}}^{b_k} (x-y_k)^2 f(x)dx </math>.\n\nNote that other distortion measures can also be considered, although mean squared error is a popular one.\n\nA key observation is that rate <math>R</math> depends on the decision boundaries <math>\\{b_k\\}_{k=1}^{M-1}</math> and the codeword lengths <math>\\{\\mathrm{length}(c_k)\\}_{k=1}^{M}</math>, whereas the distortion <math>D</math> depends on the decision boundaries <math>\\{b_k\\}_{k=1}^{M-1}</math> and the reconstruction levels <math>\\{y_k\\}_{k=1}^{M}</math>.\n\nAfter defining these two performance metrics for the quantizer, a typical Rate–Distortion formulation for a quantizer design problem can be expressed in one of two ways:\n# Given a maximum distortion constraint <math>D \\le D_\\max</math>, minimize the bit rate <math>R</math>\n# Given a maximum bit rate constraint <math>R \\le R_\\max</math>, minimize the distortion <math>D</math>\n\nOften the solution to these problems can be equivalently (or approximately) expressed and solved by converting the formulation to the unconstrained problem <math>\\min\\left\\{ D + \\lambda \\cdot R \\right\\}</math> where the [[Lagrange multiplier]] <math>\\lambda</math> is a non-negative constant that establishes the appropriate balance between rate and distortion. Solving the unconstrained problem is equivalent to finding a point on the [[convex hull]] of the family of solutions to an equivalent constrained formulation of the problem. However, finding a solution – especially a [[Closed-form expression|closed-form]] solution – to any of these three problem formulations can be difficult. Solutions that do not require multi-dimensional iterative optimization techniques have been published for only three probability distribution functions: the [[Uniform distribution (continuous)|uniform]],<ref>[[Nariman Farvardin]] and James W. Modestino, \"Optimum Quantizer Performance for a Class of Non-Gaussian Memoryless Sources\", ''[[IEEE Transactions on Information Theory]]'', Vol. IT-30, No. 3, pp. 485–497, May 1982 (Section VI.C and Appendix B). {{doi|10.1109/TIT.1984.1056920}}</ref> [[Exponential distribution|exponential]],<ref name=SullivanIT>[[Gary Sullivan (engineer)|Gary J. Sullivan]], \"Efficient Scalar Quantization of Exponential and Laplacian Random Variables\", ''[[IEEE Transactions on Information Theory]]'', Vol. IT-42, No. 5, pp. 1365–1374, Sept. 1996. {{doi|10.1109/18.532878}}</ref> and [[Laplace distribution|Laplacian]]<ref name=SullivanIT/> distributions. Iterative optimization approaches can be used to find solutions in other cases.<ref name=GrayNeuhoff/><ref name=Berger72>[[Toby Berger]], \"Optimum Quantizers and Permutation Codes\", ''[[IEEE Transactions on Information Theory]]'', Vol. IT-18, No. 6, pp. 759–765, Nov. 1972. {{doi|10.1109/TIT.1972.1054906}}</ref><ref name=Berger82>[[Toby Berger]], \"Minimum Entropy Quantizers and Permutation Codes\", ''[[IEEE Transactions on Information Theory]]'', Vol. IT-28, No. 2, pp. 149–157, Mar. 1982. {{doi|10.1109/TIT.1982.1056456}}</ref>\n\nNote that the reconstruction values <math>\\{y_k\\}_{k=1}^{M}</math> affect only the distortion – they do not affect the bit rate – and that each individual <math>y_k</math> makes a separate contribution <math> d_k </math> to the total distortion as shown below:\n:<math> D = \\sum_{k=1}^{M} d_k </math>\nwhere\n:<math> d_k = \\int_{b_{k-1}}^{b_k} (x-y_k)^2 f(x)dx </math>\nThis observation can be used to ease the analysis – given the set of <math>\\{b_k\\}_{k=1}^{M-1}</math> values, the value of each <math>y_k</math> can be optimized separately to minimize its contribution to the distortion <math>D</math>.\n\nFor the mean-square error distortion criterion, it can be easily shown that the optimal set of reconstruction values <math>\\{y^*_k\\}_{k=1}^{M}</math> is given by setting the reconstruction value <math>y_k</math> within each interval <math>I_k</math> to the conditional expected value (also referred to as the ''[[centroid]]'') within the interval, as given by:\n:<math>y^*_k = \\frac1{p_k} \\int_{b_{k-1}}^{b_k} x f(x)dx</math>.\n\nThe use of sufficiently well-designed entropy coding techniques can result in the use of a bit rate that is close to the true information content of the indices <math>\\{k\\}_{k=1}^{M}</math>, such that effectively\n:<math> \\mathrm{length}(c_k) \\approx -\\log_2\\left(p_k\\right)</math>\nand therefore\n:<math> R = \\sum_{k=1}^{M} -p_k \\cdot \\log_2\\left(p_k\\right) </math>.\n\nThe use of this approximation can allow the entropy coding design problem to be separated from the design of the quantizer itself. Modern entropy coding techniques such as [[arithmetic coding]] can achieve bit rates that are very close to the true entropy of a source, given a set of known (or adaptively estimated) probabilities <math>\\{p_k\\}_{k=1}^{M}</math>.\n\nIn some designs, rather than optimizing for a particular number of classification regions <math>M</math>, the quantizer design problem may include optimization of the value of <math>M</math> as well.  For some probabilistic source models, the best performance may be achieved when <math>M</math> approaches infinity.\n\n== Neglecting the entropy constraint: Lloyd–Max quantization ==\n\nIn the above formulation, if the bit rate constraint is neglected by setting <math>\\lambda</math> equal to 0, or equivalently if it is assumed that a fixed-length code (FLC) will be used to represent the quantized data instead of a [[variable-length code]] (or some other [[entropy coding]] technology such as [[arithmetic coding]] that is better than an FLC in the rate–distortion sense), the optimization problem reduces to minimization of distortion <math>D</math> alone.\n\nThe indices produced by an <math>M</math>-level quantizer can be coded using a fixed-length code using <math> R = \\lceil \\log_2 M \\rceil </math> bits/symbol. For example, when <math>M=</math>256 levels, the FLC bit rate <math>R</math> is 8 bits/symbol. For this reason, such a quantizer has sometimes been called an 8-bit quantizer. However using an FLC eliminates the compression improvement that can be obtained by use of better entropy coding.\n\nAssuming an FLC with <math>M</math> levels, the Rate–Distortion minimization problem can be reduced to distortion minimization alone.\nThe reduced problem can be stated as follows: given a source <math>X</math> with [[probability density function|pdf]] <math>f(x)</math> and the constraint that the quantizer must use only <math>M</math> classification regions, find the decision boundaries <math>\\{b_k\\}_{k=1}^{M-1} </math> and reconstruction levels <math>\\{y_k\\}_{k=1}^M</math> to minimize the resulting distortion \n:<math> D=E[(x-Q(x))^2] = \\int_{-\\infty}^{\\infty} (x-Q(x))^2f(x)dx = \\sum_{k=1}^{M} \\int_{b_{k-1}}^{b_k} (x-y_k)^2 f(x)dx =\\sum_{k=1}^{M} d_k </math>.\n\nFinding an optimal solution to the above problem results in a quantizer sometimes called a MMSQE (minimum mean-square quantization error) solution, and the resulting pdf-optimized (non-uniform) quantizer is referred to as a ''Lloyd–Max'' quantizer, named after two people who independently developed iterative methods<ref name=GrayNeuhoff/><ref>Stuart P. Lloyd, \"Least Squares Quantization in PCM\", ''[[IEEE Transactions on Information Theory]]'', Vol. IT-28, pp. 129–137, No. 2, March 1982 {{doi|10.1109/TIT.1982.1056489}} (work documented in a manuscript circulated for comments at [[Bell Labs|Bell Laboratories]] with a department log date of 31 July 1957 and also presented at the 1957 meeting of the [[Institute of Mathematical Statistics]], although not formally published until 1982).</ref><ref>Joel Max, \"Quantizing for Minimum Distortion\", ''[[IEEE Transactions on Information Theory|IRE Transactions on Information Theory]]'', Vol. IT-6, pp. 7–12, March 1960. {{doi|10.1109/TIT.1960.1057548}}</ref> to solve the two sets of simultaneous equations resulting from <math> {\\partial D / \\partial b_k} = 0 </math> and <math>{\\partial D/ \\partial y_k} = 0 </math>, as follows:\n:<math> {\\partial D \\over\\partial b_k} = 0 \\Rightarrow b_k = {y_k + y_{k+1} \\over 2} </math>,\nwhich places each threshold at the midpoint between each pair of reconstruction values, and\n:<math> {\\partial D \\over\\partial y_k} = 0 \\Rightarrow y_k = { \\int_{b_{k-1}}^{b_k} x f(x) dx \\over \\int_{b_{k-1}}^{b_k} f(x)dx } = \\frac1{p_k} \\int_{b_{k-1}}^{b_k} x f(x) dx </math>\nwhich places each reconstruction value at the centroid (conditional expected value) of its associated classification interval.\n\n[[Lloyd's algorithm|Lloyd's Method I algorithm]], originally described in 1957, can be generalized in a straightforward way for application to [[vector quantization|vector]] data. This generalization results in the [[Linde–Buzo–Gray algorithm|Linde–Buzo–Gray (LBG)]] or [[k-means clustering|k-means]] classifier optimization methods. Moreover, the technique can be further generalized in a straightforward way to also include an entropy constraint for vector data.<ref name=ChouLookabaughGray>Philip A. Chou, Tom Lookabaugh, and [[Robert M. Gray]], \"Entropy-Constrained Vector Quantization\", ''IEEE Transactions on Acoustics, Speech, and Signal Processing'', Vol. ASSP-37, No. 1, Jan. 1989. {{doi|10.1109/29.17498}}</ref>\n\n== Uniform quantization and the 6 dB/bit approximation ==\n\nThe Lloyd–Max quantizer is actually a uniform quantizer when the input [[probability density function|pdf]] is uniformly distributed over the range <math>[y_1-\\Delta/2,~y_M+\\Delta/2)</math>. However, for a source that does not have a uniform distribution, the minimum-distortion quantizer may not be a uniform quantizer.\n\nThe analysis of a uniform quantizer applied to a uniformly distributed source can be summarized in what follows:\n\nA symmetric source X can be modelled with <math> f(x)= \\tfrac1{2X_{\\max}}</math>, for <math>x \\in [-X_{\\max} , X_{\\max}]</math> and 0 elsewhere.\nThe step size <math>\\Delta = \\tfrac {2X_{\\max}} {M} </math> and the ''signal to quantization noise ratio'' (SQNR) of the quantizer is\n:<math>{\\rm SQNR}= 10\\log_{10}{\\frac {\\sigma_x^2}{\\sigma_q^2}} = 10\\log_{10}{\\frac {(M\\Delta)^2/12}{\\Delta^2/12}}= 10\\log_{10}M^2= 20\\log_{10}M</math>.\n\nFor a fixed-length code using <math>N</math> bits, <math>M=2^N</math>, resulting in\n<math>{\\rm SQNR}= 20\\log_{10}{2^N} = N\\cdot(20\\log_{10}2) = N\\cdot 6.0206\\,\\rm{dB}</math>,\n\nor approximately 6&nbsp;dB per bit. For example, for <math>N</math>=8 bits, <math>M</math>=256 levels and SQNR = 8*6 = 48&nbsp;dB; and for <math>N</math>=16 bits, <math>M</math>=65536 and SQNR = 16*6 = 96&nbsp;dB. The property of 6&nbsp;dB improvement in SQNR for each extra bit used in quantization is a well-known figure of merit. However, it must be used with care: this derivation is only for a uniform quantizer applied to a uniform source.\n\nFor other source pdfs and other quantizer designs, the SQNR may be somewhat different from that predicted by 6&nbsp;dB/bit, depending on the type of pdf, the type of source, the type of quantizer, and the bit rate range of operation.\n\nHowever, it is common to assume that for many sources, the slope of a quantizer SQNR function can be approximated as 6&nbsp;dB/bit when operating at a sufficiently high bit rate. At asymptotically high bit rates, cutting the step size in half increases the bit rate by approximately 1 bit per sample (because 1 bit is needed to indicate whether the value is in the left or right half of the prior double-sized interval) and reduces the mean squared error by a factor of 4 (i.e., 6&nbsp;dB) based on the <math>\\Delta^2/12</math> approximation.\n\nAt asymptotically high bit rates, the 6&nbsp;dB/bit approximation is supported for many source pdfs by rigorous theoretical analysis.<ref name=Bennett/><ref name=OliverPierceShannon/><ref name=GishPierce/><ref name=GrayNeuhoff/> Moreover, the structure of the optimal scalar quantizer (in the rate–distortion sense) approaches that of a uniform quantizer under these conditions.<ref name=GishPierce/><ref name=GrayNeuhoff/>\n<!-- I don't think that was proved by anyone else before it was done by Gish & Pearce in '68. For example, was it done by Koshelev in '63? (I don't think so) Zador in '66? (I don't know - probably not) Goblick & Holsinger in '67? (I don't see it in that paper.) -->\n\n== Other fields ==\nMany physical quantities are actually quantized by physical entities. Examples of fields where this limitation applies include [[electronics]] (due to [[electrons]]), [[optics]] (due to [[photons]]), [[biology]] (due to [[DNA]]), [[physics]] (due to [[Planck limits]]) and [[chemistry]] (due to [[molecules]]). This limitation is sometimes known in these fields as the \"quantum noise limit\".\n\n==See also==\n* [[Analog-to-digital converter]]\n* [[Beta encoder]]\n* [[Data binning]]\n* [[Discretization]]\n* [[Discretization error]]\n* [[Posterization]]\n* [[Pulse code modulation]]\n* [[Quantile]]\n* [[Regression dilution]] – a bias in parameter estimates caused by errors such as quantization in the explanatory or independent variable\n\n==Notes==\n{{Reflist|2}}\n\n== References ==\n*{{Citation |last=Sayood |first= Khalid|last2=|first2=|year= 2005 |title= Introduction to Data Compression, Third Edition |publisher= Morgan Kaufmann |isbn= 978-0-12-620862-7|doi=}}\n*{{Citation |last=Jayant |first= Nikil S.|last2=Noll|first2=Peter|year= 1984 |title= Digital Coding of Waveforms: Principles and Applications to Speech and Video |publisher= Prentice–Hall |isbn=978-0-13-211913-9|doi=}}\n*{{Citation |last=Gregg|first= W. David |year= 1977 |title= Analog & Digital Communication |publisher= John Wiley |isbn=978-0-471-32661-8\n|doi=}}\n*{{Citation |last=Stein |first= Seymour|last2= Jones|first2= J. Jay |year= 1967 |title= Modern Communication Principles |publisher= [[McGraw–Hill]] |isbn=978-0-07-061003-3|doi=}}\n\n== External links ==\n* [http://www.mit.bme.hu/books/quantization/ Quantization noise in Digital Computation, Signal Processing, and Control], Bernard Widrow and István Kollár, 2007.\n* [https://web.archive.org/web/20060522134626/http://www.techonline.com/community/related_content/20771  The Relationship of Dynamic Range to Data Word Size in Digital Audio Processing]\n* [http://ccrma.stanford.edu/~jos/mdft/Round_Off_Error_Variance.html Round-Off Error Variance] – derivation of noise power of <math>\\Delta^2/ 12</math> for round-off error\n* [http://www.ieee.li/pdf/essay/dynamic_evaluation_dac.pdf Dynamic Evaluation of High-Speed, High Resolution D/A Converters] Outlines HD, IMD and NPR measurements, also includes a derivation of quantization noise\n* [http://www.dsplog.com/2007/03/19/signal-to-quantization-noise-in-quantized-sinusoidal/ Signal to quantization noise in quantized sinusoidal]\n\n{{DSP}}\n{{Compression Methods}}\n\n{{Noise}}\n\n{{DEFAULTSORT:Quantization (Signal Processing)}}\n[[Category:Digital signal processing]]\n[[Category:Computer graphic artifacts]]\n[[Category:Digital audio]]\n[[Category:Noise (electronics)]]\n[[Category:Signal processing]]\n[[Category:Telecommunication theory]]"
    },
    {
      "title": "Ringing artifacts",
      "url": "https://en.wikipedia.org/wiki/Ringing_artifacts",
      "text": "{{about|ringing artifacts in signal processing, particularly image processing|ringing in electronics and signals generally|ringing (signal)}}\n[[File:Ringing artifact example.png|thumb|240px|Image showing ringing artifacts.\n\n3 levels on each side of transition: overshoot, first ring, and (faint) second ring.]]\n[[File:Ringing artifact example - original.png|thumb|240px|Same image without ringing artifacts.]]\n\nIn [[signal processing]], particularly [[digital image processing]], '''ringing artifacts''' are [[Artifact (error)|artifacts]] that appear as spurious signals near sharp transitions in a signal. Visually, they appear as bands or \"ghosts\" near edges; audibly, they appear as \"echos\" near [[Transient (acoustics)|transients]], particularly sounds from [[percussion instrument]]s; most noticeable are the [[pre-echo]]s. The term \"ringing\" is because the output signal oscillates at a fading rate around a sharp transition in the input, similar to a [[Bell (instrument)|bell]] after being struck. As with other artifacts, their minimization is a criterion in [[filter design]].\n\n== Introduction ==\n[[File:High accuracy settling time measurements figure 1.png|thumb|The main cause of ringing artifacts is [[overshoot (signal)|overshoot]] and oscillations in the [[step response]] of a filter.]]\nThe main cause of ringing artifacts is due to a signal being [[bandlimited]] (specifically, not having high frequencies) or passed through a [[low-pass filter]]; this is the [[frequency domain]] description.\nIn terms of the [[time domain]], the cause of this type of ringing is the ripples in the [[sinc function]],<ref name=\"Bankman\">{{citation\n|title=Handbook of medical imaging\n|first=Isaac N.\n|last=Bankman\n|publisher=Academic Press\n|year=2000\n|isbn=978-0-12-077790-7\n|url=https://books.google.com/books?id=UHkkPBnhT-MC\n}}, section I.6, Enhancement: Frequency Domain Techniques, [https://books.google.com/books?id=UHkkPBnhT-MC&pg=RA3-PA16#PRA3-PA16,M1 p. 16]</ref> which is the [[impulse response]] (time domain representation) of a perfect low-pass filter. Mathematically, this is called the [[Gibbs phenomenon]].\n\nOne may distinguish [[overshoot (signal)|overshoot]] (and undershoot), which occurs when transitions are accentuated – the output is higher than the input – from ringing, where ''after'' an overshoot, the signal overcorrects and is now below the target value; these phenomena often occur together, and are thus often conflated and jointly referred to as \"ringing\".\n\nThe term \"ringing\" is most often used for ripples in the ''time'' domain, though it is also sometimes used for ''frequency'' domain effects:<ref name=\"Chitode\">[https://books.google.com/books?id=LbUFjehqSdwC Digital Signal Processing], by J.S.Chitode, Technical Publications, 2008, {{ISBN|978-81-8431-346-8}}, [https://books.google.com/books?id=LbUFjehqSdwC&pg=RA3-PA1-IA104#PRA3-PA1-IA103,M1 4&nbsp;-&nbsp;70]</ref>\nwindowing a filter in the time domain by a rectangular function causes ripples in the ''frequency'' domain for the same reason as a brick-wall low pass filter (rectangular function in the ''frequency'' domain) causes ripples in the ''time'' domain, in each case the Fourier transform of the rectangular function being the sinc function.\n\nThere are related artifacts caused by other [[frequency domain]] effects,\nand [[#Similar phenomena|similar artifacts]] due to unrelated causes.\n\n== Causes ==\n=== Description ===\n[[File:Sinc function (normalized).svg|thumb|The [[sinc function]], the [[impulse response]] for an ideal [[low-pass filter]], illustrating ringing for an impulse.]]\n[[File:Gibbs phenomenon 10.svg|thumb|The [[Gibbs phenomenon]], illustrating ringing for a [[step function]].]]\nBy definition, ringing occurs when a non-oscillating input yields an oscillating output: formally, when an input signal which is [[monotonic function|monotonic]] on an interval has output response which is not monotonic. This occurs most severely when the [[impulse response]] or [[step response]] of a [[Filter (signal processing)|filter]] has oscillations – less formally, if for a spike input, respectively a step input (a sharp transition), the output has bumps. Ringing most commonly refers to step ringing, and that will be the focus.\n\nRinging is closely related to [[overshoot (signal)|overshoot]] and undershoot, which is when the output takes on values higher than the maximum (respectively, lower than the minimum) input value: one can have one without the other, but in important cases, such as a [[low-pass filter]], one first has overshoot, then the response bounces back below the steady-state level, causing the first ring, and then oscillates back and forth above and below the steady-state level. Thus overshoot is the first step of the phenomenon, while ringing is the second and subsequent steps. Due to this close connection, the terms are often conflated, with \"ringing\" referring to both the initial overshoot and the subsequent rings.\n\nIf one has a [[linear time invariant]] (LTI) filter, then one can understand the filter and ringing in terms of the impulse response (the time domain view), or in terms of its Fourier transform, the [[frequency response]] (the frequency domain view). Ringing is a ''time'' domain artifact, and in [[filter design]] is traded off with desired frequency domain characteristics: the desired frequency response may cause ringing, while reducing or eliminating ringing may worsen the frequency response.\n\n=== sinc filter ===\n{{main|sinc filter}} <!-- lower-case intentional, as this is a function name -->\n[[File:Sine integral.svg|thumb|The [[Sine integral]] for positive values, exhibiting oscillation.]]\nThe central example, and often what is meant by \"ringing artifacts\", is the ideal ([[Brick-wall filter|brick-wall]]) [[low-pass filter]], the [[sinc filter]]. This has an oscillatory impulse response function, as illustrated above, and the step response – its integral, the [[sine integral]] – thus also features oscillations, as illustrated at right.\n\nThese ringing artifacts are not results of imperfect implementation or windowing: the ideal low-pass filter, while possessing the desired frequency response, necessarily causes ringing artifacts in the ''time'' domain.\n\n=== Time domain ===\nIn terms of impulse response, the correspondence between these artifacts and the behavior of the function is as follows:\n* impulse undershoot is equivalent to the impulse response having negative values,\n* impulse ringing (ringing near a point) is precisely equivalent to the impulse response having oscillations, which is equivalent to the derivative of the impulse response alternating between negative and positive values,\n* and there is no notion of impulse overshoot, as the unit impulse is assumed to have infinite height (and integral 1 – a [[Dirac delta function]]), and thus cannot be overshot.\n\nTurning to step response,\nthe step response is the integral of the [[impulse response]]; formally, the value of the step response at time ''a'' is the integral <math>\\int_{-\\infty}^a</math> of the impulse response. Thus values of the step response can be understood in terms of ''tail'' integrals of the impulse response.\n\nAssume that the overall integral of the impulse response is 1, so it sends constant input to the same constant as output – otherwise the filter has [[Gain (electronics)|gain]], and scaling by gain gives an integral of 1.\n* Step undershoot is equivalent to a tail integral being negative, in which case the magnitude of the undershoot is the value of the tail integral.\n* Step overshoot is equivalent to a tail integral being greater than 1, in which case the magnitude of the overshoot is the amount by which the tail integral exceeds 1 – or equivalently the value of the tail in the other direction, <math>\\int_a^\\infty,</math> since these add up to 1.\n* Step ringing is equivalent to tail integrals alternating between increasing and decreasing – taking derivatives, this is equivalent to the impulse response alternating between positive and negative values.<ref>{{citation\n|title=Principles of Digital Image Synthesis\n|url=https://books.google.com/books?id=6KUsFm7L-LQC\n|first=Andrew S\n|last=Glassner\n|edition=2\n|publisher=Morgan Kaufmann\n|year=2004\n|isbn =978-1-55860-276-2\n\n}}, [https://books.google.com/books?id=6KUsFm7L-LQC&pg=PA518#PPA518,M1 p. 518]</ref> Regions where an impulse response are below or above the ''x''-axis (formally, regions between zeros) are called '''lobes,''' and the magnitude of an oscillation (from peak to trough) equals the integral of the corresponding lobe.\n\nThe impulse response may have many negative lobes, and thus many oscillations, each yielding a ring, though these decay for practical filters, and thus one generally only sees a few rings, with the first generally being most pronounced.\n\nNote that if the impulse response has small negative lobes and larger positive lobes, then it will exhibit ringing but not undershoot or overshoot: the tail integral will always be between 0 and 1, but will oscillate down at each negative lobe. However, in the sinc filter, the lobes monotonically decrease in magnitude and alternate in sign, as in the [[alternating harmonic series]], and thus tail integrals alternate in sign as well, so it exhibits overshoot as well as ringing.\n\nConversely, if the impulse response is always nonnegative, so it has no negative lobes – the function is a [[probability distribution]] – then the step response will exhibit neither ringing nor overshoot or undershoot – it will be a monotonic function growing from 0 to 1, like a [[cumulative distribution function]]. Thus the basic solution from the time domain perspective is to use filters with nonnegative impulse response.\n\n=== Frequency domain ===\nThe frequency domain perspective is that ringing is caused by the sharp cut-off in the rectangular [[passband]] in the frequency domain, and thus is reduced by smoother [[roll-off]], as discussed below.<ref name=\"Bankman\" /><ref name=\"MIP\">[https://books.google.com/books?id=uGWmR0f_350C Microscope Image Processing], by Qiang Wu,  Fatima Merchant,  Kenneth Castleman, {{ISBN|978-0-12-372578-3}} [https://books.google.com/books?id=uGWmR0f_350C&pg=RA1-PA71 p. 71]</ref>\n\n== Solutions ==\nSolutions depend on the parameters of the problem: if the cause is a low-pass filter, one may choose a different filter design, which reduces artifacts at the expense of worse frequency domain performance. On the other hand, if the cause is a band-limited signal, as in JPEG, one cannot simply replace a filter, and ringing artifacts may prove hard to fix – they are present in [[JPEG 2000]] and many audio compression codecs (in the form of [[pre-echo]]), as discussed in the [[#Examples|examples]].\n\n=== Low-pass filter ===\n[[File:DisNormal01.svg|thumb|The [[Gaussian function]] is non-negative and non-oscillating, hence causes no overshoot or ringing.]]\n\nIf the cause is the use of a brick-wall low-pass filter, one may replace the filter with one that reduces the time domain artifacts, at the cost of frequency domain performance. This can be analyzed from the time domain or frequency domain perspective.\n\nIn the time domain, the cause is an impulse response that oscillates, assuming negative values. This can be resolved by using a filter whose impulse response is non-negative and does not oscillate, but shares desired traits. For example, for a low-pass filter, the [[Gaussian filter]] is non-negative and non-oscillatory, hence causes no ringing. However, it is not as good as a low-pass filter: it rolls off in the passband, and leaks in the [[stopband]]: in image terms, a Gaussian filter \"blurs\" the signal, which reflects the attenuation of desired higher frequency signals in the passband.\n\nA general solution is to use a [[window function]] on the sinc filter, which cuts off or reduces the negative lobes: these respectively eliminate and reduce overshoot and ringing. Note that truncating some but not all of the lobes eliminates the ringing beyond that point, but does not reduce the amplitude of the ringing that is not truncated (because this is determined by the size of the lobe), and increases the magnitude of the overshoot if the last non-cut lobe is negative, since the magnitude of the overshoot is the integral of the ''tail,'' which is no longer canceled by positive lobes.\n\nFurther, in practical implementations one at least truncates sinc, otherwise one must use infinitely many data points (or rather, all points of the signal) to compute every point of the output – truncation corresponds to a rectangular window, and makes the filter practically implementable, but the frequency response is no longer perfect.<ref>{{Harv|Allen|Mills|2004}} Section 9.3.1.1 Ideal Filters: Low pass, [https://books.google.com/books?id=ZmyKvXQmQwIC&pg=PA621 p. 621]</ref>\nIn fact, if one takes a brick wall low-pass filter (sinc in time domain, rectangular in frequency domain) and truncates it (multiplies with a rectangular function in the time domain), this convolves the frequency domain with sinc (Fourier transform of the rectangular function) and causes ringing in the ''frequency'' domain,<ref name=\"Chitode\" /> which is referred to as ''[[ripple (electrical)#Frequency-domain ripple|ripple]].'' In symbols, <math>\\mathcal{F}(\\mathrm{sinc}\\cdot \\mathrm{rect}) = \\mathrm{rect} * \\mathrm{sinc}.</math> The frequency ringing in the stopband is also referred to as [[side lobe]]s. Flat response in the passband is desirable, so one windows with functions whose Fourier transform has fewer oscillations, so the frequency domain behavior is better.\n\nMultiplication in the time domain corresponds to convolution in the frequency domain, so multiplying a filter by a window function corresponds to convolving the Fourier transform of the original filter by the Fourier transform of the window, which has a smoothing effect – thus windowing in the time domain corresponds to smoothing in the frequency domain, and reduces or eliminates overshoot and ringing.<ref>{{Harv|Allen|Mills|2004}} [https://books.google.com/books?id=ZmyKvXQmQwIC&pg=PA623 p. 623]</ref>\n\nIn the [[frequency domain]], the cause can be interpreted as due to the sharp (brick-wall) cut-off, and ringing reduced by using a filter with smoother roll-off.<ref name=\"Bankman\" /> This is the case for the Gaussian filter, whose magnitude [[Bode plot]] is a downward opening parabola (quadratic roll-off), as its Fourier transform is again a Gaussian, hence (up to scale) <math>e^{-x^2}</math> – taking logarithms yields <math>-x^2.</math>\n\n{{external media\n|image1=[https://books.google.com/books?id=dunqt1rt4sAC&pg=RA4-PA332 Butterworth filter impulse response and frequency response graphs]<ref name=\"OpAmp\">[https://books.google.com/books?id=dunqt1rt4sAC Op Amp applications handbook],\nby Walter G. Jung, Newnes, 2004,\n{{ISBN|978-0-7506-7844-5}}, [https://books.google.com/books?id=dunqt1rt4sAC&pg=RA4-PA332 p. 332]</ref>\n}}\nIn [[electronic filter]]s, the trade-off between frequency domain response and time domain ringing artifacts is well-illustrated by the [[Butterworth filter]]: the frequency response of a Butterworth filter slopes down linearly on the log scale, with a first-order filter having slope of −6 [[decibel|dB]] per [[octave]], a second-order filter –12&nbsp;dB per octave, and an ''n''th order filter having slope of <math>-6n</math> dB per octave – in the limit, this approaches a brick-wall filter. Thus, among these the, first-order filter rolls off slowest, and hence exhibits the fewest time domain artifacts, but leaks the most in the stopband, while as order increases, the leakage decreases, but artifacts increase.<ref name=\"MIP\" />\n\n== Benefits ==\n[[File:Accutance.svg|thumb|Artificially added overshoot around the left bar increases [[acutance]].]]\nWhile ringing artifacts are generally considered undesirable, the initial overshoot (haloing) at transitions increases [[acutance]] (apparent sharpness) by increasing the derivative across the transition, and thus can be considered as an enhancement.<ref name=\"mitchell\">{{cite conference\n|title=Reconstruction filters in computer-graphics\n|first=Don P.\n|last=Mitchell\n|author2=Netravali, Arun N.\n |url=http://www.mentallandscape.com/Papers_siggraph88.pdf\n|doi=10.1145/54852.378514\n|conference=ACM SIGGRAPH International Conference on Computer Graphics and Interactive Techniques\n|conferenceurl=http://portal.acm.org/toc.cfm?id=54852&type=proceeding&coll=GUIDE&dl=GUIDE,ACM&CFID=30538218&CFTOKEN=95411512\n|pages=221–228\n|date=August 1988\n|volume=22\n|number=4\n|ISBN=0-89791-275-6 \n}}</ref>\n\n== Related phenomena ==\n=== Overshoot ===\n[[File:Sinc function (normalized).svg|thumb|The sinc function has negative tail integrals, hence has overshoot.]]\n[[File:Lanczos-kernel.svg|thumb|The Lanczos 2-lobed filter exhibits only overshoot, while the 3-lobed filter exhibits overshoot and ringing.]]\n{{main|Overshoot (signal)}}\nAnother artifact is [[overshoot (signal)|overshoot]] (and undershoot), which manifests itself not as rings, but as an increased jump at the transition.   It is related to ringing, and often occurs in combination with it.\n\nOvershoot and undershoot are caused by a negative tail – in the sinc, the integral from the first zero to infinity, including the first negative lobe.  While ringing is caused by a following ''positive'' tail – in sinc, the integral from the second zero to infinity, including the first non-central positive lobe.\nThus overshoot is ''necessary'' for ringing,{{Dubious|date=July 2013}} but can occur separately: for example, the 2-lobed [[Lanczos filter]] has only a single negative lobe on each side, with no following positive lobe, and thus exhibits overshoot but no ringing, while the 3-lobed Lanczos filter exhibits both overshoot and ringing, though the windowing reduces this compared to the sinc filter or the truncated sinc filter.\n\nSimilarly, the convolution kernel used in [[bicubic interpolation]] is similar to a 2-lobe windowed sinc, taking on negative values, and thus produces overshoot artifacts, which appear as halos at transitions.\n\n=== Clipping ===\n{{main|Clipping (audio)}}\nFollowing from overshoot and undershoot is [[Clipping (audio)|clipping]].\nIf the signal is bounded, for instance an 8-bit or 16-bit integer, this overshoot and undershoot can exceed the range of permissible values, thus causing clipping.\n\nStrictly speaking, the clipping is caused by the combination of overshoot and limited numerical accuracy, but it is closely associated with ringing, and often occurs in combination with it.\n\nClipping can also occur for unrelated reasons, from a signal simply exceeding the range of a channel.\n\n=== Ringing and ripple ===\n{{main|Ringing (signal)|Ripple (filters)}}\n[[File:Chebyscheff5.svg|thumb|Frequency response of a 5th order [[Chebyshev filter]], exhibiting [[ripple (electrical)#Frequency-domain ripple|ripple]].]]\n\nIn signal processing and related fields, the general phenomenon of time domain oscillation is called '''[[Ringing (signal)|ringing]],''' while frequency domain oscillations are generally called '''[[ripple (electrical)#Frequency-domain ripple|ripple]],''' though generally not \"rippling\".\n\nA key source of ripple in digital signal processing is the use of [[window function]]s: if one takes an [[infinite impulse response]] (IIR) filter, such as the sinc filter, and windows it to make it have [[finite impulse response]], as in the [[window design method]], then the frequency response of the resulting filter is the convolution of the frequency response of the IIR filter with the frequency response of the window function. Notably, the frequency response of the rectangular filter is the sinc function (the rectangular function and the sinc function are [[Conjugate variables|Fourier dual]] to each other), and thus truncation of a filter in the time domain corresponds to multiplication by the rectangular filter, thus convolution by the sinc filter in the frequency domain, causing ripple. In symbols, the frequency response of <math>\\mathrm{rect}(t) \\cdot h(t)</math> is <math>\\mathrm{sinc}(t) * \\hat h(t).</math> In particular, truncating the sinc function itself yields <math>\\mathrm{rect}(t) \\cdot \\mathrm{sinc}(t)</math> in the time domain, and <math>\\mathrm{sinc}(t) * \\mathrm{rect}(t)</math> in the frequency domain, so just as low-pass filtering (truncating in the frequency domain) causes ''ringing'' in the time domain, truncating in the time domain (windowing by a rectangular filter) causes ''ripple'' in the frequency domain.\n\n== Examples ==\n=== JPEG ===\n[[File:Asterisk with jpg-artefacts.png|thumb|Extreme example of JPEG artifacts, including ringing: cyan (= white minus red) rings around a red star.]]\n[[File:Dctjpeg.png|thumb|[[Discrete cosine transform]] basis functions.]]\n[[JPEG]] compression can introduce ringing artifacts at sharp transitions, which are particularly visible in text.\n\nThis is a due to loss of high frequency components, as in step response ringing.\n[[JPEG#Block splitting|JPEG uses 8×8 blocks]], on which the [[discrete cosine transform]] (DCT) is performed. The DCT is a [[Fourier-related transform]], and ringing occurs because of loss of high frequency components or loss of precision in high frequency components.\n\nThey can also occur at the edge of an image: since JPEG splits images into 8×8 blocks, if an image is not an integer number of blocks, the edge cannot easily be encoded, and solutions such as filling with a black border create a sharp transition in the source, hence ringing artifacts in the encoded image.\n\nRinging also occurs in the [[wavelet]]-based [[JPEG 2000]].\n\nJPEG and JPEG 2000 have other artifacts, as illustrated above, such as blocking (\"[[jaggies]]\") and edge busyness (\"[[mosquito noise]]\"), though these are due to specifics of the formats, and are not ringing as discussed here.\n\nSome illustrations:\n* [http://www.stat.columbia.edu/~jakulin/jpeg/artifacts.htm  Baseline JPEG and JPEG2000 Artifacts Illustrated]\n{| class=\"wikitable\" style=\"text-align:center\"\n! Image !! Lossless compression !! Lossy compression \n|-\n! Original\n| [[File:Lossless-circle.png|100px]] || [[File:Lossy-circle.jpg|100px]] \n|-\n! Processed by<br/>[[Canny edge detector]],<br/>highlighting artifacts.\n| [[File:Lossless-circle-canny.png|100px]] || [[File:Lossy-circle-canny.png|100px]] \n|}\n\n=== Pre-echo ===\n[[File:2006-07-06 Crash Zildjian 14.jpg|thumb|[[Pre-echo]] occurs in percussions such as cymbals.]]\n{{main|Pre-echo}}\nIn [[audio signal processing]], ringing can cause echoes to occur before and after [[transient (acoustics)|transients]], such as the impulsive sound from [[percussion instrument]]s, such as [[cymbal]]s (this is ''impulse'' ringing). The ([[Causal filter|causal]]) echo after the transient is not heard, because it is masked by the \ntransient, an effect called [[temporal masking]]. Thus only the ([[Anti-causal filter|anti-causal]]) echo before the transient is heard, and the phenomenon is called [[pre-echo]].\n\nThis phenomenon occurs as a [[compression artifact]] in audio compression algorithms that use [[Fourier-related transforms]], such as [[MP3]], [[Advanced Audio Coding|AAC]], and [[Vorbis]].\n\n== Similar phenomena ==\nOther phenomena have similar symptoms to ringing, but are otherwise distinct in their causes. In cases where these cause circular artifacts around point sources, these may be referred to as \"rings\" due to the round shape (formally, an [[Annulus (mathematics)|annulus]]), which is unrelated to the \"ringing\" (oscillatory decay) frequency phenomenon discussed on this page.\n\n=== Edge enhancement ===\n{{main|Edge enhancement}}\n\n[[Edge enhancement]], which aims to increase edges, may cause ringing phenomena, particularly under repeated application, such as by a DVD player followed by a television. This may be done by ''high''-pass filtering, rather than low-pass filtering.<ref name=\"MIP\" />\n\n=== Special functions ===\n[[File:Airy-pattern.svg|thumb|The [[Airy pattern]], caused by [[Fraunhofer diffraction]].]]\nMany [[special functions]] exhibit oscillatory decay, and thus convolving with such a function yields ringing in the output; one may consider these ringing, or restrict the term to unintended artifacts in frequency domain signal processing.\n\n[[Fraunhofer diffraction]] yields the [[Airy disk]] as [[point spread function]], which has a ringing pattern.\n\n[[File:Bessel Functions (1st Kind, n=0,1,2).svg|thumb|A few [[Bessel function]]s of the first kind, showing oscillatory decay.]]\nThe [[Bessel function]] of the first kind, <math>J_0,</math> which is related to the [[Airy function]], exhibits such decay.\n\n[[File:Spherical-aberration-disk.jpg|thumb|Combinations of [[defocus]] and [[spherical aberration]] exhibit ring artifacts.]]\nIn cameras, a combination of [[defocus]] and [[spherical aberration]] can yield circular artifacts (\"ring\" patterns). However, the pattern of these artifacts need not be similar to ringing (as discussed on this page) – they may exhibit oscillatory decay (circles of decreasing intensity), or other intensity patterns, such as a single bright band.\n\n=== Interference ===\n[[Ghosting (television)|Ghosting]] is a form of [[television interference]] where an image is repeated. Though this is not ringing, it can be interpreted as convolution with a function, which is 1 at the origin and ε (the intensity of the ghost) at some distance, which is formally similar to the above functions (a single discrete peak, rather than continuous oscillation).\n\n=== Lens flare ===\n{{details|Lens flare}}\n\n[[File:Lens Flare.jpg|thumb|[[Lens flare]].]]\nIn photography, [[lens flare]] is a defect where various circles can appear around highlights, and with ghosts throughout a photo, due to undesired light, such as reflection and scattering off elements in the lens.\n\n=== Visual illusions ===\n[[File:Mach band.svg|thumb|[[Mach bands]]]]\nVisual illusions can occur at transitions, as in [[Mach bands]], which perceptually exhibit a similar undershoot/overshoot to the Gibbs phenomenon.\n\n== See also ==\n{{colbegin}}\n* [[Artifact (error)]]\n* [[Digital artifact]]\n* [[sinc filter]]<!-- intentionally lowercase, as a function name -->\n* [[Brick-wall filter]]\n* [[Chromatic aberration]]\n* [[Ghosting (television)]]\n* [[Gibbs phenomenon]]\n* [[Low-pass filter]]\n* [[Pre-echo]]\n* [[Purple fringing]]\n{{colend}}\n\n== References ==\n{{reflist}}\n* {{citation\n|title=Signal analysis: time, frequency, scale, and structure\n|first1=Ronald L.\n|last1=Allen\n|first2=Duncan W.\n|last2=Mills\n|publisher=Wiley-IEEE\n|year=2004\n|url=https://books.google.com/books?id=ZmyKvXQmQwIC\n|isbn=978-0-471-23441-8\n}}\n\n[[Category:Signal processing]]\n[[Category:Computer graphic artifacts]]"
    },
    {
      "title": "Spatial anti-aliasing",
      "url": "https://en.wikipedia.org/wiki/Spatial_anti-aliasing",
      "text": "{{More footnotes|date=January 2009}}\n\nIn [[digital signal processing]], '''spatial anti-aliasing''' is a technique for minimizing the distortion artifacts known as [[aliasing]] when representing a high-resolution image at a lower resolution. Anti-aliasing is used in [[digital photography]], [[computer graphics]], [[digital audio]], and many other applications.\n\nAnti-aliasing means removing signal components that have a higher [[frequency]] than is able to be properly resolved by the recording (or sampling) device.  This removal is done before (re)sampling at a lower resolution. When sampling is performed without removing this part of the signal, it causes undesirable artifacts such as the black-and-white noise near the top of figure 1-a [[#Figure1|below]].\n\nIn signal acquisition and audio, anti-aliasing is often done using an analogue [[anti-aliasing filter]] to remove the out-of-band component of the input signal prior to sampling with an [[analogue-to-digital converter]]. In digital photography, optical anti-aliasing filters made of [[birefringent]] materials smooth the signal in the spatial optical domain. The anti-aliasing filter essentially blurs the image slightly in order to reduce the resolution to or below that achievable by the digital sensor (the larger the [[pixel pitch]], the lower the achievable resolution at the sensor level).\n\n==Examples==\n\n{|  style=\"margin-left:5px;\"\n|{{anchor|Figure1}}[[Image:aliased.png|An aliased picture of a checker-board from an angle looks random in the distance, and has jagged lines in the foreground.]]<center>(a)</center>\n|[[Image:antialiased.png|The same picture, anti-aliased, blurs into a grey in the distance, and has smoother lines in the foreground.]]<center>(b)</center>\n|[[Image:antialiased-lanczos.png|There are different algorithms for anti-aliasing, creating a slightly different appearance]]<center>(c)</center>\n|-\n|\n|<center>Figure 1</center>\n|}\n\nIn computer graphics, anti-aliasing improves the appearance of polygon edges, so they are not \"jagged\" but are smoothed out on the screen. However, it incurs a performance cost for the graphics card and uses more video memory. The level of anti-aliasing determines how smooth polygon edges are (and how much video memory it consumes).\n\nFigure 1-a illustrates the visual distortion that occurs when anti-aliasing is not used. Near the top of the image, where the checker-board is very small, the image is both difficult to recognise and not aesthetically appealing. In contrast, Figure 1-b shows an anti-aliased version of the scene. The checker-board near the top blends into grey, which is usually the desired effect when the [[Image resolution|resolution]] is insufficient to show the detail. Even near the bottom of the image, the edges appear much smoother in the anti-aliased image. Figure 1-c shows another anti-aliasing [[algorithm]], based on the [[sinc filter]], which is considered better than the algorithm used in 1-b.<ref name = refA>{{cite journal |doi=10.1145/965105.807509 |author=Leler, William J. |title=Human Vision, Anti-aliasing, and the Cheap 4000 Line Display |journal=ACM SIGGRAPH Computer Graphics |volume=14 |issue=3 |pages=308–313 |date=July 1980 }}\n</ref>\n\n[[Image:antialiased-zoom.png|framed|Figure 2]]\nFigure 2 shows magnified portions ([[interpolation|interpolated]] using the [[Nearest-neighbor interpolation|nearest neighbor algorithm]]) of Figure 1-a (left) and 1-c (right) for comparison. In Figure 1-c, anti-aliasing has interpolated the brightness of the pixels at the boundaries to produce grey [[pixel]]s since the space is occupied by both black and white tiles. These help make Figure 1-c appear much smoother than Figure 1-a at the original magnification.\n\n{{clear}}\n{| \n|valign=\"top\"|<div class=\"thumb\"><div style=\"width: 100px;\">[[Image:Anti-aliased-diamonds.png]]<div class=\"thumbcaption\">Above left: an aliased version of a simple shape; above right: an anti-aliased version of the same shape; right: The anti-aliased graphic at 5x magnification</div></div></div>\n|rowspan=\"2\"|<div class=\"thumb\" style=\"margin-bottom: 0;\"><div style=\"width:109px;\">[[Image:Anti-aliased diamond enlarged.png]]</div></div>\n|-\n|valign=\"bottom\" span style=\"font-size:94%; padding:.3em 0 .1em;\"|Figure 3\n|}\n\nIn Figure 3, anti-aliasing was used to blend the boundary pixels of a sample graphic; this reduced the aesthetically jarring effect of the sharp, step-like boundaries that appear in the aliased graphic at the left. Anti-aliasing is often applied in rendering text on a computer screen, to suggest smooth contours that better emulate the appearance of text produced by conventional ink-and-paper printing.\n\nParticularly with [[Computer font|fonts]] displayed on typical LCD screens, it is common to use [[subpixel rendering]] techniques like [[ClearType]]. Sub-pixel rendering requires special colour-balanced anti-aliasing filters to turn what would be severe colour distortion into barely-noticeable colour fringes. Equivalent results can be had by making individual sub-pixels addressable as if they were full pixels, and supplying a hardware-based anti-aliasing filter as is done in the [[OLPC XO-1]] laptop's display controller. [[Pixel geometry]] affects all of this, whether the anti-aliasing and sub-pixel addressing are done in software or hardware.\n\n==Simplest approach to anti-aliasing==\nThe most basic approach to anti-aliasing a pixel is determining what percentage of the pixel is occupied by a given region in the vector graphic - in this case a pixel-sized square, possibly transposed over several pixels - and using that percentage as the colour.\n\nA very basic plot of a single, white-on-black anti-aliased point using that method can be done as follows:\n\n Define function PlotAntiAliasedPoint ( number x, number y )\n     For roundedx = floor ( x ) to ceil ( x ) do\n          For roundedy = floor ( y ) to ceil ( y ) do\n               percent_x = 1 - abs ( x - roundedx )\n               percent_y = 1 - abs ( y - roundedy )\n               percent = percent_x * percent_y\n               DrawPixel ( coordinates roundedx, roundedy, color percent (range 0-1) )\n\nThis method is generally best suited for simple graphics, such as basic lines or curves, and applications that would otherwise have to convert absolute coordinates to pixel-constrained coordinates, such as 3-D graphics. It is a fairly fast function, but it is relatively low-quality, and gets slower as the complexity of the shape increases. For purposes requiring very high-quality graphics or very complex vector shapes, this will probably not be the best approach.\n\nNote: The DrawPixel routine above cannot blindly set the colour value to the percent calculated.  It must '''add''' the new value to the existing value at that location up to a maximum of 1.  Otherwise, the brightness of each pixel will be equal to the darkest value calculated in time for that location which produces a very bad result.  For example, if one point sets a brightness level of 0.90 for a given pixel and another point calculated later barely touches that pixel and has a brightness of 0.05, the final value set for that pixel should be 0.95, not 0.05.\n\nFor more sophisticated shapes, the algorithm may be generalized as rendering the shape to a pixel grid with higher resolution than the target display surface (usually a multiple that is a power of 2 to reduce distortion), then using [[bicubic interpolation]] to determine the average intensity of each real pixel on the display surface.\n\n==Signal processing approach to anti-aliasing==\nIn this approach, the ideal image is regarded as a ''signal''.  The image displayed on the screen is taken as samples, at each (''x,y'') pixel position, of a filtered version of the signal. Ideally, one would understand how the human brain would process the original signal, and provide an on-screen image that will yield the most similar response by the brain.\n\nThe most widely accepted analytic tool for such problems is the [[Fourier transform]]; this decomposes a signal into [[basis function]]s of different frequencies, known as frequency components, and gives us the [[amplitude]] of each frequency component in the signal. The waves are of the form:\n\n:<math>\\ \\cos (2j \\pi x) \\cos (2k \\pi y)</math>\n\nwhere ''j'' and ''k'' are arbitrary non-negative [[integer]]s. There are also frequency components involving the [[sine]] functions in one or both dimensions, but for the purpose of this discussion, the [[cosine]] will suffice.\n\nThe numbers ''j'' and ''k'' together are the ''frequency'' of the component: ''j'' is the frequency in the ''x'' direction, and ''k'' is the frequency in the ''y'' direction.\n\nThe goal of an anti-aliasing filter is to greatly reduce frequencies above a certain limit, known as the [[Nyquist frequency]], so that the signal will be accurately represented by its samples, or nearly so, in accordance with the [[sampling theorem]]; there are many different choices of detailed algorithm, with different filter [[transfer function]]s. Current knowledge of [[human visual perception]] is not sufficient, in general, to say what approach will look best.\n\n==Two dimensional considerations==\n[[Image:Sinc(x) x sinc(y) plot.jpg|thumb|Sinc function, with separate X and Y]]\nThe previous discussion assumes that the rectangular mesh sampling is the dominant part of the problem. The filter usually considered optimal is not rotationally symmetrical, as shown in this first figure; this is because the data is sampled on a [[square lattice]], not using a continuous image.  This sampling pattern is the justification for doing signal processing along each axis, as it is traditionally done on one dimensional data. [[Lanczos resampling]] is based on convolution of the data with a discrete representation of the sinc function.\n\nIf the resolution is not limited by the rectangular sampling rate of either the source or target image, then one should ideally use rotationally symmetrical filter or interpolation functions, as though the data were a two dimensional function of continuous x and y. The sinc function of the radius has too long a tail to make a good filter (it is not even [[square-integrable]]).  A more appropriate analogue to the one-dimensional sinc is the two-dimensional [[Airy disc]] amplitude, the 2D Fourier transform of a circular region in 2D frequency space, as opposed to a square region.\n\n[[Image:Gaussian plus its own curvature.jpg|thumb|Gaussian plus differential function]]\nOne might consider a Gaussian plus enough of its second derivative to flatten the top (in the frequency domain) or sharpen it up (in the spatial domain), as shown. Functions based on the Gaussian function are natural choices, because convolution with a Gaussian gives another Gaussian whether applied to x and y or to the radius. Similarly to wavelets, another of its properties is that it is halfway between being localized in the configuration (x and y) and in the spectral (j and k) representation. As an interpolation function, a Gaussian alone seems too spread out to preserve the maximum possible detail, and thus the second derivative is added.\n\nAs an example, when printing a photographic negative with plentiful processing capability and on a printer with a hexagonal pattern, there is no reason to use sinc function interpolation. Such interpolation would treat diagonal lines differently from horizontal and vertical lines, which is like a weak form of aliasing.\n\n==Practical real-time anti-aliasing approximations==\n\nThere are only a handful of [[rendering primitive|primitives]] used at the lowest level in a [[real-time rendering]] engine (either software or hardware accelerated). These include \"points\", \"lines\" and \"triangles\". If one is to draw such a primitive in white against a black background, it is possible to design such a primitive to have fuzzy edges, achieving some sort of anti-aliasing. However, this approach has difficulty dealing with adjacent primitives (such as triangles that share an edge).\n\nTo approximate the uniform averaging algorithm, one may use an extra buffer for sub-pixel data. The initial (and least memory-hungry) approach used 16 extra bits per pixel, in a 4 × 4 grid. If one renders the primitives in a careful order, such as front-to-back, it is possible to create a reasonable image.\n\nSince this requires that the primitives be in some order, and hence interacts poorly with an application programming interface such as [[OpenGL]], the latest methods simply have two or more full sub-pixels per pixel, including full color information for each sub-pixel. Some information may be shared between the sub-pixels (such as the [[Z-buffer]].)\n\n===Mipmapping===\n{{Main article|Mipmap}}\n\nThere is also an approach specialised for [[texture mapping]] called [[mipmap]]ping, which works by creating lower resolution, pre-filtered versions of the texture map. When rendering the image, the appropriate-resolution mipmap is chosen and hence the texture pixels (texels) are already filtered when they arrive on the screen. Mipmapping is generally combined with various forms of [[texture filtering]] in order to improve the final result.\n\n==An example of an image with extreme pseudo-random aliasing==\nBecause [[fractals]] have unlimited detail and no noise other than arithmetic round-off error, they illustrate aliasing more clearly than do photographs or other measured data. The [[Mandelbrot set#Escape time algorithm|escape times]], which are converted to colours at the exact centres of the pixels, go to infinity at the border of the set, so colours from centres near borders are unpredictable, due to aliasing. This example has edges in about half of its pixels, so it shows much aliasing. The first image is uploaded at its original sampling rate. (Since most modern software anti-aliases, one may have to download the full-size version to see all of the aliasing.) The second image is calculated at five times the sampling rate and [[downsampling|down-sampled]] with anti-aliasing. Assuming that one would really like something like the average colour over each pixel, this one is getting closer. It is clearly more orderly than the first.\n\nIn order to properly compare these images, viewing them at full-scale is necessary.\n<gallery>\nImage:Mandelbrot_\"Turbine\"_desk_shape.jpg|1. As calculated with the program \"MandelZot\"\nImage:Mandelbrot_Turbine_big_all_samples.jpg|2. Anti-aliased by blurring and down-sampling by a factor of five\nImage:Mandelbrot_Budding_turbines.jpg|3. Edge points interpolated, then anti-aliased and down-sampled\nImage:Mandelbrot_Turbine_Chaff.jpg|4. An enhancement of the points removed from the previous image\nImage:Mandelbrot Budding Turbines downsampled.jpg|5. Down-sampled again, without anti-aliasing\n</gallery>\n\nIt happens that, in this case, there is additional information that can be used. By re-calculating with a \"distance estimator\" algorithm, points were identified that are very close to the edge of the set, so that unusually fine detail is aliased in from the rapidly changing escape times near the edge of the set. The colours derived from these calculated points have been identified as unusually unrepresentative of their pixels. The set changes more rapidly there, so a single point sample is less representative of the whole pixel. Those points were replaced, in the third image, by interpolating the points around them. This reduces the noisiness of the image but has the side effect of brightening the colours. So this image is not exactly the same that would be obtained with an even larger set of calculated points. To show what was discarded, the rejected points, blended into a grey background, are shown in the fourth image.\n\nFinally, \"Budding Turbines\" is so regular that systematic (Moiré) aliasing can clearly be seen near the main \"turbine axis\" when it is downsized by taking the nearest pixel. The aliasing in the first image appears random because it comes from all levels of detail, below the pixel size. When the lower level aliasing is suppressed, to make the third image and then that is down-sampled once more, without anti-aliasing, to make the fifth image, the order on the scale of the third image appears as systematic aliasing in the fifth image.\n\nPure down-sampling of an image has the following effect (viewing at full-scale is recommended):\n\n<gallery>\nImage:Mandelbrot-spiral-original.png|1) A picture of a particular spiral feature of the [[Mandelbrot set]].\nImage:Mandelbrot-spiral-antialiased-4-samples.png|2) 4 samples per pixel.\nImage:Mandelbrot-spiral-antialiased-25-samples.png|3) 25 samples per pixel.\nImage:Mandelbrot-spiral-antialiased-400-samples.png|4) 400 samples per pixel.\n</gallery>\n\n==Super sampling / full-scene anti-aliasing==\n\n[[Supersampling|Super sampling anti-aliasing (SSAA)]],<ref>{{cite web |url=http://www.anandtech.com/show/2841/14 |title=AMD's Radeon HD 5870: Bringing About the Next Generation Of GPUs |publisher=AnandTech.com }}</ref> also called full-scene anti-aliasing (FSAA),<ref>\n{{cite book\n | title = Game Engine Architecture\n | author = Jason Gregory, [[Jeff Lander]]\n | publisher = A K Peters, Ltd.\n | year = 2009\n | isbn = 978-1-56881-413-1\n | page = 39\n | url = https://books.google.com/books?id=LJ20tsePKk4C&pg=PA442\n }}</ref> is used to avoid aliasing (or \"[[jaggies]]\") on full-screen images.<ref>\n{{cite journal\n | journal = Crossroads, the ACM Student Magazine\n | title = Graphic libraries for Windows programming\n | author = M. Carmen Juan Lizandra\n | volume = 6\n | issue = 4\n | pages = 14–18\n | publisher = ACM\n | doi = 10.1145/333424.333433\n | date = June 2000\n | url = http://dl.acm.org/citation.cfm?id=333433\n }}</ref> SSAA was the first type of anti-aliasing available with early video cards. But due to its tremendous computational cost and the advent of [[multisample anti-aliasing]] (MSAA) support on GPUs, it is no longer widely used in real time applications. MSAA provides somewhat lower graphic quality, but also tremendous savings in computational power.\n\nThe resulting image of SSAA may seem softer, and should also appear more realistic. However, while useful for photo-like images, a simple anti-aliasing approach (such as super-sampling and then averaging) may actually worsen the appearance of some types of line art or diagrams (making the image appear fuzzy), especially where most lines are horizontal or vertical.  In these cases, a prior grid-fitting step may be useful (see [[hinting]]).\n\nIn general, super-sampling is a technique of collecting data points at a greater resolution (usually by a power of two) than the final data resolution. These data points are then combined (down-sampled) to the desired resolution, often just by a simple [[average]]. The combined data points have less visible aliasing artifacts (or [[moiré pattern]]s).\n\nFull-scene anti-aliasing by super-sampling usually means that each full frame is rendered at double (2x) or quadruple (4x) the [[computer display|display]] resolution, and then down-sampled to match the display resolution. Thus, a 2x FSAA would render 4 super-sampled [[pixels]] for each single pixel of each frame. Rendering at larger resolutions will produce better results; however, more processor power is needed, which can degrade performance and frame rate. Sometimes FSAA is implemented in hardware in such a way that a graphical application is unaware the images are being super-sampled and then down-sampled before being displayed.\n\n==Object-based anti-aliasing==\n\nA graphics rendering system creates an image based on objects constructed of polygonal primitives; the aliasing effects in the image can be reduced by applying an anti-aliasing scheme only to the areas of the image representing silhouette edges of the objects. The silhouette edges are anti-aliased by creating anti-aliasing primitives which vary in opacity. These anti-aliasing primitives are joined to the [[silhouette edge|silhouetted edges]], and create a region in the image where the objects appear to blend into the background. The method has some important advantages over classical methods based on the [[accumulation buffer]]{{clarify|date=March 2012}} since it generates full-scene anti-aliasing in only two passes and does not require the use of additional memory required by the accumulation buffer. Object-based anti-aliasing was first developed at [[Silicon Graphics]] for their [[SGI Indy|Indy]] workstation.\n\n==Anti-aliasing and gamma compression==\n\nDigital images are usually stored in a [[Gamma compression|gamma-compressed]] format, but most optical anti-aliasing filters are linear. So to down-sample an image in a way that would match optical blurring, one should first convert it to a linear format, then apply the anti-aliasing filter, and finally convert it back to a gamma compressed format. Using linear arithmetic on a gamma-compressed image results in values which are slightly different from the ideal filter. This error is larger when dealing with high contrast areas, causing high contrast areas to become dimmer: bright details (such as a cat's whiskers) become visually thinner, and dark details (such as tree branches) become thicker, relative to the optically anti-aliased image.<ref>{{cite web |first=Eric |last=Brasseur |url=http://www.4p8.com/eric.brasseur/gamma.html |title=Gamma error in picture scaling |website=www.4p8.com |accessdate=2012-12-14 |deadurl=yes |archiveurl=https://web.archive.org/web/20121223021721/http://www.4p8.com/eric.brasseur/gamma.html |archivedate=2012-12-23 |df= }}</ref>\nBecause the conversion to and from a linear format greatly slows down the process, and because the differences are usually subtle, almost all [[image editing software]], including [[Final Cut Pro]], [[Adobe Photoshop]] and [[GIMP]], process images in the gamma-compressed domain.\n\nMost modern [[GPU]]s support storing [[Texture mapping|textures]] in memory in [[sRGB]] format, and can perform transformation to linear space and back transparently, with essentially no loss in performance.\n\n==History==\nImportant early works in the history of anti-aliasing include:\n\n*{{cite journal |doi=10.1145/356625.356627 |author=Freeman, H. |authorlink=Herbert Freeman |title=Computer processing of line drawing images |journal=ACM Computing Surveys |volume=6 |issue=1 |pages=57–97 |date=March 1974 }}\n*{{cite journal |doi=10.1145/359863.359869 |author=Crow, Franklin C. |authorlink=Franklin C. Crow |title=The aliasing problem in computer-generated shaded images |journal=[[Communications of the ACM]] |volume=20 |issue=11 |pages=799–805 |date=November 1977 }}\n*{{cite conference |title=A hidden-surface algorithm with anti-aliasing |author=Catmull, Edwin |authorlink=Edwin Catmull |date=August 23–25, 1978 |publisher= |booktitle=Proceedings of the 5th annual conference on Computer graphics and interactive techniques |pages=6–11 }}\n\n==See also==\n* [[Alpha to coverage]]\n* [[Anisotropic filtering]]\n* [[Font rasterization]]\n* [[Sampling (signal processing)]]\n* [[Temporal anti-aliasing]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://lunaloca.com/tutorials/antialiasing/ Antialiasing and Transparency Tutorial]: Explains interaction between anti-aliasing and transparency, especially when dealing with web graphics\n* [https://web.archive.org/web/20050408053948/http://home.no.net/dmaurer/~dersch/gamma/gamma.html Interpolation and Gamma Correction] In most real-world systems, [[gamma correction]] is required to linearize the response curve of the sensor and display systems. If this is not taken into account, the resultant non-linear distortion will defeat the purpose of anti-aliasing calculations based on the assumption of a linear system response.\n* [http://www.eurogamer.net/articles/digital-foundry-future-of-anti-aliasing The Future of Anti-Aliasing]: A comparison of the different algorithms MSAA, MLAA, DLAA and FXAA\n* {{fr icon}} [http://www.astrosurf.com/luxorion/apn-anti-aliasing.htm Le rôle du filtre anti-aliasing dans les APN (the function of anti-aliasing filter in dSLR)]\n*{{cite web |url=http://ati.amd.com:80/products/pdf/smoothvision.pdf |title=SMOOTHVISION™ |publisher=ATI Technologies |archive-url=https://web.archive.org/web/20070118033410/http://ati.amd.com:80/products/pdf/smoothvision.pdf |archive-date=January 18, 2007 }}\n\n{{DEFAULTSORT:Spatial Anti-Aliasing}}\n[[Category:Image processing]]\n[[Category:Digital typography]]\n[[Category:Computer graphic artifacts]]\n[[Category:Anti-aliasing algorithms]]"
    },
    {
      "title": "Visual artifact",
      "url": "https://en.wikipedia.org/wiki/Visual_artifact",
      "text": "[[File:Winxp estk artifact.png|thumb|A screenshot of a Microsoft [[Windows XP]] application displaying a visual artifact with repeated frames.]]\n\n'''Visual artifacts''' (also '''artefacts''') are [[artifact (error)|anomalies]] apparent during visual representation as in [[digital graphics]] and other forms of [[image]]ry, especially [[photography]] and [[microscopy]].\n\n==Examples in digital graphics==\n[[File:Retinography.jpg|thumb|A retinography. The gray spot in the center is a shadow artifact.]]\n* [[Image quality#Image quality factors|Image quality factors]], different types of visual artifacts\n* [[Compression artifact]]s\n* [[Digital artifact]]s, visual artifacts resulting from digital image processing\n* [[Image noise|Noise]]\n* [[Screen-door effect]], also known as fixed-pattern noise (FPN), a visual artifact of digital projection technology\n*[[Ghosting (television)]]\n*[[Screen burn-in]]\n* [[Distortion]]\n* [[Silk screen effect]]\n* [[Rainbow effect]]\n* [[Screen tearing]]\n* [[Purple fringing]]\n* [[Chromatic aberration]]\n* [[Moiré pattern]]\n* [[Color banding]]\n\n==Occurrences in video entertainment==\nMany people who use their computers as a hobby experience artifacting due to a hardware malfunction. The cases can differ but the usual causes are:\n\n* Fan issues, such as failure of cooling fan.\n* Unsuited video card drivers.\n* Drivers that have values that the graphics card is not suited with.\n* Overclocking beyond the capabilities of the particular video card.\n\nThe differing cases of visual artifacting can also differ between scheduled task(s).\n\n[[File:Anther of thale cress (Arabidopsis thaliana), an artefact.jpg|thumb|Confocal laser scanning fluorescence micrograph of [[Arabidopsis thaliana|thale cress]] anther (part of [[stamen]]). The picture shows among other things a nice red flowing collar-like structure just below the anther. However, an intact thale cress stamen does not have such collar, this is a fixation artifact: the stamen has been cut below the picture frame, and [[Epidermis (botany)|epidermis]] (upper layer of cells) of stamen stalk has peeled off, forming a non-characteristic structure. Photo: Heiti Paves from [[Tallinn University of Technology]].]]\n\n==In microscopy==\nIn [[microscopy]], an artifact is an apparent structural detail that is caused by the processing of the specimen and is thus not a legitimate feature of the specimen. In [[optical microscope|light microscopy]], arteficts may be produced by air bubbles trapped under the [[microscope slide|slide]]'s cover slip.<ref name=\"Kent\">{{cite book |last1=Kent |first1=Michael |title=Advanced Biology |date=2000 |publisher=[[Oxford University Press]] |location=Oxford |isbn=0199141959 |page=64 |edition=Repr.}}</ref>\n\nIn [[electron microscope|electron microscopy]], distortions may be produced in the drying out of the specimen. [[Staining]] can cause the appearance of solid chemical deposits that may be seen as structures inside the cell. Different techniques including [[electron microscope#Sample preparation |freeze-fracturing]] and [[cell fractionation]] may be used to overcome the problems of artifacts.<ref name=\"Kent\"/>\n\nA ''crush artifact'' is an artificial elongation and distortion seen in [[histopathology]] and [[cytopathology]] studies. Distortion can be caused by the slightest compression of tissue and can provide difficulties in diagnosis.<ref name=\"Chatterjee\">{{cite journal |last1=Chatterjee |first1=S. |title=Artefacts in histopathology. |work=Journal of oral and maxillofacial pathology (JOMFP) |date=September 2014 |volume=18 |issue=Suppl 1 |pages=S111-6 |doi=10.4103/0973-029X.141346 |pmid=25364159}}</ref><ref>{{cite journal |vauthors=Komanduri S, Swanson G, Keefer L, Jakate S |title=Use of a new jumbo forceps improves tissue acquisition of Barrett's esophagus surveillance biopsies |journal=Gastrointest. Endosc. |volume=70 |issue=6 |pages=1072–8.e1 |date=December 2009 |pmid=19595312 |doi=10.1016/j.gie.2009.04.009 |url=}}</ref>\n\n==References==\n{{Reflist}}\n\n[[Category:Computer graphic artifacts]]\n[[Category:Visual artifacts]]"
    },
    {
      "title": "Fireflies (computer graphics)",
      "url": "https://en.wikipedia.org/wiki/Fireflies_%28computer_graphics%29",
      "text": "[[File:UFO Moonbase Exterior + Fireflies.png|thumb|Example image with fireflies (click to enlarge and examine closely).]]\n[[File:UFO Moonbase Exterior - Fireflies.png|thumb|Image with fireflies removed through postprocessing.]]\n\n'''Fireflies''' are [[Rendering (computer graphics)|rendering]] artifacts resulting from [[Numerical stability|numerical instabilities]] in solving the [[rendering equation]]. They manifest themselves as anomalously-bright single pixels scattered over parts of the image.\n\nFireflies need to be distinguished from ''[[image noise|noise]]'' (overall graininess in the image), which can be reduced by simply increasing the number of rendering samples (amount of computation) per pixel. Fireflies tend to be harder to get rid of.\n\nFireflies tend to be confined to particular parts of the image, where they are caused by interactions between particular material and lighting settings that only affect certain objects in the scene.\n\n== Curing Fireflies ==\nSometimes fireflies can be reduced by various tweaks to renderer settings, for example clamping the maximum intermediate amplitude during pixel calculations, or disabling the calculation of [[Caustic (optics)|caustics]] if these are not needed. Another option is application of a [[despeckle]] filter as part of rendering post-processing, or manually removing the fireflies with the brush or clone tool in an image editor.\n\n{{Clear}}\n\n== References ==\n* [http://www.indigorenderer.com/why-are-there-bright-little-specks-still-my-render Why are there bright little specks still in my render?], indigorenderer.com\n* [http://wiki.blender.org/index.php/Doc:2.6/Manual/Render/Cycles/Reducing_Noise Reducing noise in Blender Cycles], wiki.blender.org\n* [[wikibooks:Blender 3D: Noob to Pro/Advanced Tutorials/Cycles Fireflies|Dealing with Fireflies in Cycles]], ''Blender 3D: Noob to Pro'' Wikibook\n* [http://www.luxrender.net/wiki/New_User_Tips_and_Tricks#Fireflies Fireflies], LuxRender wiki\n\n[[Category:3D graphic artifacts]]"
    },
    {
      "title": "Popping (computer graphics)",
      "url": "https://en.wikipedia.org/wiki/Popping_%28computer_graphics%29",
      "text": "[[File:LOD Example.png|thumb|This is an exaggerated example of a 3D object's geometrically being reduced using a level of detail technique. LOD0 is the highest detail version of the object and each subsequent LOD reduces the quality of the object. A change without intermediate steps from LOD1 to LOD2 will be obvious to the viewer.]]\nIn [[3D computer graphics]], '''popping''' refers to an undesirable visual effect that occurs when the transition of a 3D object to a different pre-calculated [[level of detail]] (LOD) is abrupt and obvious to the viewer. <ref name=\"J1\">M. Chover, J. Gumbau, A. Puig-Centelles, O. Ripolles, F. Ramos (June 2009)  \"Rendering continuous level-of-detail meshes by Masking Strips\" ''Graphical Models'' pp.185</ref> The LOD-ing algorithm reduces the geometrical complexity of a 3D object the further it is from the viewer and returns that lost complexity as the viewer gets closer to the 3D object, causing it to ''pop'' as it becomes suddenly more detailed. The LOD-ing algorithms can depend on more factors than just distance from the viewer, but it is often the primary factor that is considered. Popping is most obvious when switching between different LODs directly without intermediate steps. Techniques like '''geomorphing''' and '''LOD blending''' can reduce visual popping significantly by making the transitions more gradual.\n\n== LOD Blending ==\n[[File:Level of Detial Blending.png|thumb|An exaggerated example of LOD blending to illustrate how apparent the ghosting effect can be.]]\nAlso known as '''alpha blending''', this technique reduces popping by displaying both LODs of a 3D model simultaneously and blending them together over a small transition period. \n\nDuring the blending process an alpha value is specified for each LOD which determine transparency of objects. At the beginning of the transition, the initial LOD would have an alpha value of 1.0 (fully opaque) and the new LOD would be have an alpha value of 0.0 (fully transparent). As the viewer approaches the 3D object and reaches the distance when the LOD change would normally occur, the LOD alpha values would gradually switch until the new LOD has an alpha value of 1.0, at which point the initial LOD would no longer be rendered. <ref name=\"B1\">D. Luebke, M. Reddy, J. D. Cohen, A. Varshney, B. Watson, R. Huebner: Level of Detail for 3D Graphics, Morgan Kaufmann, 2002, {{ISBN|0-321-19496-9}}</ref>\n\nIt is important to stress that LOD blending only occurs at the distance that a LOD would normally change and only over a small distance.  So if during a simulation the LOD change would occur at the 100 units of distance then the LOD blending process would begin at the 95 units of distance and be complete by 105 units of distance. \n\nLOD blending has two major disadvantages. It is expensive in terms of computing power since both LODs are rendered simultaneously for the blend to occur and can be counter productive since the reason to use LOD-ing algorithms is to reduce the expense of rendering scenes. The technique is not effective when the viewer is close to the 3D object since the blending process will be obviously apparent and result in a visible ''ghosting'' effect.\n\n==Geomorphing==\n[[File:Geomorphing Example.png|thumb|Geomorphing creates a smoother transition between LOD0 and LOD1 by creating approximated meshes to act as intermediate steps.]]\nGeomorphing is a technique that reduces popping during LOD changes by adding approximations of the 3D model to serve as intermediate steps between two LODs to create a smooth transition. Edge collapses (removing vertices) and vertex splits (adding vertices) are the primary operations to modify the 3D model using this method.\n\nTraditional geomorphing creates a sequence of 3D models between two LODs. The sequence cannot be interrupted once it has begun and no modifications can be done to it until LOD change is complete. Due to this restriction, traditional geomorphing is not suited to interactive simulations because the process cannot be quickly reversed if conditions change unexpectedly. \n\nReal-time geomorphing directly modifies individual vertices of the 3D model to adjust its LOD. This allows changes made to the geomorph during any frame, either to halt ongoing morphs or initiate further morphs of the 3D model. Since multiple vertices can be triggered to morph independently one of another, certain vertices need to be temporarily locked to ensure a smooth transition occurs which could result in a delay in a LOD change. The flexibility of real-time geomorphing makes it an effective solution for interactive simulations. <ref name=\"J2\"> K. Jeong, S. Lee, L. Markosian, A. Ni (September 2005)  \"Detail control in line drawings of 3D meshes\" ''Springer-Verlag'' pp.700</ref>\n\n== External links ==\n* [https://www.youtube.com/watch?v=KfeFcZDjCRg Example of LOD popping]\n\n== References ==\n{{Reflist}}\n\n[[Category:3D graphic artifacts]]"
    },
    {
      "title": "Z-fighting",
      "url": "https://en.wikipedia.org/wiki/Z-fighting",
      "text": "{{Refimprove|date=September 2017}}\n\n[[file:ZfightingCB.png|thumb|Demonstration of z-fighting with multiple colors and textures over a grey background]]\n\n'''Z-fighting''', also called '''stitching''', is a phenomenon in [[Rendering (computer graphics)|3D rendering]] that occurs when two or more [[Primitive (geometry)|primitive]]s have similar or identical values in the [[z-buffer]].  It is particularly prevalent with [[coplanar]] polygons, where two faces occupy essentially the same space, with neither in front. Affected pixels are rendered with [[Fragment (computer graphics)|fragment]]s from one polygon or the other arbitrarily, in a manner determined by the precision of the z-buffer.  It can also vary as the scene or camera is changed, causing one polygon to \"win\" the z test, then another, and so on. The overall effect is a flickering, noisy rasterization of two polygons which \"fight\" to color the screen pixels.  This problem is usually caused by limited sub-pixel precision and [[floating point]] and [[Fixed-point arithmetic|fixed point]] [[round-off error]]s.\n\nThe more z-buffer precision one uses, the less likely it is that z-fighting will be encountered.  But for coplanar polygons, the problem is inevitable unless corrective action is taken.\n\nAs the distance between [[near and far clip plane]]s increases and in particular the near plane is selected near the eye, the greater the likelihood exists that z-fighting between primitives will occur. With large virtual environments inevitably there is an inherent conflict between the need to resolve visibility in the distance and in the foreground, so for example in a space flight simulator if you draw a distant galaxy to scale, you will not have the precision to resolve visibility on any cockpit geometry in the foreground (although even a numerical representation would present problems prior to z-buffered rendering). To mitigate these problems, z-buffer precision is weighted towards the near clip plane, but this is not the case with all visibility schemes and it is insufficient to eliminate all z-fighting issues.\n\n== Mitigation ==\n[[file:Z-fighting.png|thumb|The effect seen on two co-planar polygons.]]\n\nZ-fighting can be reduced through the use of a higher resolution [[depth buffer]], by [[z-buffering]] in some scenarios, or by simply moving the polygons further apart. Z-fighting, which cannot be entirely eliminated, in this manner is often resolved by the use of a [[stencil buffer]], or by applying a post transformation screen space z-buffer offset to one polygon which does not affect the projected shape on screen, but does affect the z-buffer value to eliminate the overlap during pixel interpolation and comparison. Where z-fighting is caused by different transformation paths in hardware for the same geometry (for example in a multi-pass rendering scheme) it can sometimes be resolved by requesting that the hardware uses invariant vertex transformation.\n\nZ-fighting which is caused by insufficient precision in the depth buffer can be resolved by simply reducing the visible distance in the world. This reduces the distance between the near and far planes, and solves the precision issue. However, in certain virtual environments, such as a space simulator, or a flight simulator, this is not possible. Alternative techniques exist in these cases. One of these techniques is to \"simulate\" the distance of objects far from the user without actually changing their position. For example, if the maximum safe view distance (beyond which z-fighting occurs) is 10,000 units, and an object to be rendered is 15,000 units away, that object could instead be rendered at 10,000 units but it could be scaled down in proportion to the distance that it was moved. So, an object that has been scaled down by half will look like it is twice as far as it actually is. If this is done only for objects that are already close to, or at, the maximum view distance, and objects close to the user are rendered normally, this technique should not be noticeable. Another technique that is utilized to reduce or completely eliminate Z-fighting is switching to a logarithmic Z-buffer, reversing Z. This technique is seen in the game ''[[Grand Theft Auto V]]''. Due to the way they are encoded, floating point numbers have much more precision when closer to 0. Here, reversing Z leads to more precision when storing the depth of very distant objects, hence greatly reducing Z-fighting.<ref>{{cite web|url=http://www.adriancourreges.com/blog/2015/11/02/gta-v-graphics-study/|title=GTA V - Graphics Study|first=Adrian|last=Courrèges|date=2 November 2015|website=AdrianCourreges.com|accessdate=20 June 2018}}</ref>\n\n== References ==\n{{Reflist}}\n\n[[Category:3D graphic artifacts]]\n[[Category:3D rendering]]"
    },
    {
      "title": "3D Print Canal House",
      "url": "https://en.wikipedia.org/wiki/3D_Print_Canal_House",
      "text": "The '''3D Print Canal House''' is a three-year, publicly accessible \"Research & Design by Doing\" project in which an international team of partners from various sectors works together on [[3D printing]] a [[canal house]] in [[Amsterdam]].<ref>{{cite web|url=http://www.archdaily.com/491666/first-3d-printed-house-to-be-built-in-amsterdam|title=First 3D Printed House to Be Built In Amsterdam|work=ArchDaily|accessdate=22 August 2015}}</ref>\n\nBy building the house, all parties research the possibilities of 3D printing architecture and form connections between design, science, culture, building, software, communities and the city.  The project serves as both an exhibition of 3D printing technology, as well as a research site into 3D printing architecture. The project is initiated by DUS architects and the site, in Amsterdam North, opened to the public on March 1, 2014.<ref>{{cite web|url=http://www.opp.today/how-3d-printing-by-robots-is-set-to-transform-building/|title=OPP.Today - How 3D printing by robots is set to transform building|work=OPP.Today|accessdate=22 August 2015}}</ref><ref>{{cite web|url=https://www.theglobeandmail.com/life/home-and-garden/architecture/the-printed-house-coming-soon/article24432799/|title=The printed house, coming soon: Futurists see 3-D technology radically changing the way houses are built|work=The Globe and Mail|accessdate=22 August 2015}}</ref>\n\n== Kamermaker ==\nThe house is constructed by a [[fused deposition modeling]] printer developed by DUS: the Kamermaker (\"Room builder\"), able to print elements of up to 2.2×2.2×3.5 metres. It is a movable pavilion with the size of a shipping container. The machine itself is 6 meters tall. The Kamermaker can be moved by truck or by ship.<ref>{{cite web|url=http://www.iamsterdam.com/en/visiting/what-to-do/attractions-and-sights/overview-of-attractions/3d-print-canal-house|title=3D Print Canal House|work=iamsterdam.com|accessdate=22 August 2015}}</ref>\n\n==See also==\n* [[Construction 3D printing]]\n\n==References==\n{{Reflist}}\n\n==External links==\n*{{official website|http://3dprintcanalhouse.com}}\n\n{{coord|52.3932|4.9023|region:NL_type:landmark|display=title}}\n\n[[Category:3D printed objects|Canal house]]\n[[Category:Houses in the Netherlands]]\n[[Category:Buildings and structures in Amsterdam]]\n[[Category:2014 establishments in the Netherlands]]\n[[Category:Building research]]\n[[Category:Building engineering]]\n\n{{Netherlands-struct-stub}}\n{{robotics-stub}}"
    },
    {
      "title": "3D-printed extraterrestrial structures",
      "url": "https://en.wikipedia.org/wiki/3D-printed_extraterrestrial_structures",
      "text": "#REDIRECT [[Colonization of the Moon#Surface colonies]]\n\n[[Category:3D printed objects|Extraterrestrial structures]]"
    },
    {
      "title": "Grecia (toucan)",
      "url": "https://en.wikipedia.org/wiki/Grecia_%28toucan%29",
      "text": "{{Infobox animal\n| name         = Grecia\n| image        = Grecia tucan.jpg\n| caption      = Grecia, in his environment at [[Zoo Ave]]\n| image_size   =\n| species      = ''[[Ramphastos ambiguus swainsonii]]''\n| gender       = Male\n| hatch_date   = approx. March 2014\n| hatch_place  = [[Alajuela Province]], [[Costa Rica]]\n| death_date   = \n| death_place  = \n| relativeage  =\n| employer     =\n| nationality  = [[Costa Rica]]\n| known_for = First toucan prosthetic beak\n| role         = \n| years_active =\n| weight       =\n| website      = \n}}\n\n'''Grecia''' (hatched around March, 2014) is a [[chestnut-mandibled toucan]] widely known as the first [[toucan]] to receive a prosthetic 3D printed beak.\n\n==Incident==\n\nGrecia was born in the wild in or around [[Grecia (canton)|Grecia]], one of the [[Cantons of Costa Rica]]. The toucan was beaten by youth and its top beak broke off.<ref>{{cite web|title=Maimed Toucan Set To Receive 3D Printed Beak|url=http://www.iflscience.com/plants-and-animals/maimed-toucan-attacked-youths-set-receive-3d-printed-beak/|website=www.iflscience.com|accessdate=14 September 2016}}</ref> Government officials transported the bird to animal rescue center [[Zoo Ave]], west of the city of Alajuela.<ref>{{cite web\n | url =http://www.ticotimes.net/2016/08/11/grecia-toucan-new-cage\n | title =Grecia, the toucan with the prosthetic beak, now receiving visitors\n | last =Aias\n | first =L\n | date =11 Aug 2016 \n | website =\n | publisher =[[The Tico Times]]\n | access-date =14 Sep 2016\n | quote = }}</ref> A few [[3D printing]] companies joined efforts to create a prosthetic beak which was successfully attached to Grecia. The bird received its name from the town of [[Grecia, Costa Rica|Grecia]] where it was picked up by city officials.<ref>{{cite news|author1=Staff Writer|title=Mutilated Costa Rican toucan 'to get prosthetic beak|url=https://www.bbc.com/news/world-latin-america-31409916|accessdate=14 September 2016|publisher=BBC News|date=11 February 2015}}</ref>\n\n== See also ==\n* [[Victoria (goose)]]\n\n== References ==\n{{Reflist}}\n\n[[Category:Individual birds]]\n[[Category:2014 animal births]]\n[[Category:Animal cruelty incidents]]\n[[Category:3D printed objects]]\n[[Category:Prosthetics]]"
    },
    {
      "title": "Local Motors",
      "url": "https://en.wikipedia.org/wiki/Local_Motors",
      "text": "{{Infobox company\n| name = Local Motors\n| logo = Local Motors logo.png\n| image = File:Rally Fighter Local Motors 1.jpg\n| image_caption = Local Motors Rally Fighter\n| type = [[Private company|Private]]\n| industry = Motor vehicle manufacturing\n| location = [[Phoenix, Arizona]]\n| founded = {{start date and age|2007}}\n| key_people = Jay Rogers ([[CEO]])\n| website = {{url|https://localmotors.com}}\n}}\n\n'''Local Motors''' is an American motor vehicle manufacturing company focused on low-volume manufacturing of [[Open-source car|open-source motor vehicle]] designs using multiple [[Microfactory|microfactories]]. It was founded in 2007 with headquarters in [[Phoenix, Arizona]].<ref>“[https://localmotors.com/about/ About Local Motors]”, Local Motors Website</ref> The company’s current vehicles include the [[Rally Fighter]] and their 3D-printed [[Strati (automobile)|Strati]] and Swim vehicles. The company develops vehicles using [[3D printing|3D Printing]] and utilizes vehicle designs provided by the online community. In 2016, the company introduced an autonomous electric-powered shuttle named Olli.\n\n==Community==\nLocal Motors' website is a community focusing on vehicle innovation. The content is created by the users who discuss designing, engineering, and building innovative vehicles. Members contribute their own ideas and projects which are discussed with the community.<ref>{{cite web|url=http://www.azcentral.com/story/money/business/2014/10/25/local-motors-printing-car-chandler/17941815/|title=Local Motors makes crowd-sourced and 3-D cars|access-date=2016-07-20}}</ref>\n\nCo-creation is a technique used by Local Motors, [[General Electric]], and [[Lego|LEGO]] to enhance new product development.<ref>{{cite web|url=https://www.forbes.com/sites/joannmuller/2015/07/01/big-ideas-small-scale-how-ge-is-using-the-wisdom-of-crowds-to-design-better-appliances/#10903f7b76dd|title=Big Ideas, Small Scale: How GE Is Using The Wisdom Of Crowds To Design Better Appliances|last=Muller|first=Joann|access-date=2016-07-20}}</ref><ref>{{cite web|url=http://leaderlab.com/lego-open-innovation-co-creation-and-mass-customization/|title=Network meeting 25/11: LEGO engaging consumers in Open innovation, co-creation and mass-customization. {{!}} Leaderlab|website=leaderlab.com|access-date=2016-07-20}}</ref> Select organizations have partnered with the company to facilitate co-creation of their products including [[United States Army|US Army]],<ref>{{cite web|url=http://www.ref.army.mil/news3_cocreate.html|title=REF Rapid Equipping Force - United States Army|website=www.ref.army.mil|access-date=2016-07-20}}</ref> [[Domino's Pizza|Domino’s]],<ref>{{cite web|url=https://www.forbes.com/sites/dalebuss/2015/10/21/dominos-gets-into-the-car-business-with-dpx-pizza-delivery-vehicle/#39e77a6e4724|title=Domino's Gets Into the Car Business With DXP Pizza-Delivery Vehicle|last=Buss|first=Dale|access-date=2016-07-20}}</ref> and [[Airbus Group, Inc.|Airbus]].<ref>{{cite web|url=http://www.popsci.com/airbus-drone-design-contest-winners|title=Look At These Wild Drone Concepts Airbus Thinks Are The Future|access-date=2016-07-20}}</ref> Local Motors uses Co-designing type of customer co-creation in which the selective process is made by its community and some features such as frame and structure are scoped by the company. Firstly, users create drawing designs and the decorative ideas on their own style. Although the users are novices or experts, all users can participate in this step. After that users present their designs on the website, the best design selected by people in the community will be developed by the company. Finally, the company will launch the co-designing car into the market. Using Co-creation method, company gains customer’s engagement and loyalty.<ref>{{cite web|url=http://cattaleeya.com/2017/01/04/local-motors/|title=\" Local Motors \" New generation of product co-creation – Cattaleeya|access-date=2017-08-11}}</ref>\n\nOne of the biggest community driven competitions was hosted by Local Motors in collaboration with [[Airbus]].<ref>{{cite web|url=https://cocreate.localmotors.com/blog/post/meet-the-judges-for-the-airbus-cargo-drone-challenge/2044/|title=Meet the judges for the Airbus Cargo Drone Challenge {{!}} Local Motors|date=2016-06-06|language=en-us|access-date=2016-07-05}}</ref>\n\n==Propositions==\nThe propositions that Local Motors offer are:\n\n- The car is produced and designed using the co-designing type of co-creation, which is called crowdsourcing.\n- The car was produced in 18 months, which is faster than the usual process by five times.\n- They are road legal cars.\n- The wheels are strong off-road wheels and are grade 8, which are used in the military.\n- About $ 3 million was spent to develop the car, which is less than the amount spent on commercial models by the automakers. The company achieved to spend about 3 million dollars by rethinking the car’s features, so they designed a five-point seat belt, which costs $10, instead of developing an airbag, which will cost them $6 million.\n\n== Rally Fighter ==\n{{Main article|Rally Fighter}}The [[Rally Fighter]] <ref>{{cite web|url=http://rallyfighter.com/the-rally-fighter/|title=The Rally Fighter – Local Motors Rally Fighter|website=rallyfighter.com|accessdate=28 September 2017}}</ref> was the first model produced by Local Motors and is an open sourced vehicle. The car was introduced in 2009 after 18 months of development, which constitutes a record Time to Market compared to the automobile industry standards, by applying innovative technologies and crowd sourcing techniques.<ref>{{cite web|url=http://www.azcentral.com/story/money/business/2014/10/25/local-motors-printing-car-chandler/17941815/|title=Local Motors makes crowd-sourced and 3-D cars|publisher=|accessdate=28 September 2017}}</ref> As the company describes it, the Rally Fighter is \"a fully capable off-road prerunner, with the amenities and luxuries of an every-day on road vehicle\".<ref>{{cite web|url=https://launchforth.io/localmotors/rally-fighter/latest/|title=Rally Fighter - Local Motors|date=20 May 2008|publisher=|accessdate=28 September 2017}}</ref>\n\n==Strati==\n{{Main article|Strati (automobile)}}\n\nIn collaboration with Cincinnati Incorporated and [[Oak Ridge National Laboratory]] Local Motors manufactured Strati, the world's first [[3D printing|3D printed]] [[electric car]].<ref>{{cite news|last1=Gastelu|first1=Gary|title=Local Motors 3D-printed car could lead an American manufacturing revolution|url=http://www.foxnews.com/leisure/2014/07/03/local-motors-3d-printed-car-could-lead-american-manufacturing-revolution/|accessdate=22 September 2014|publisher=Fox News|date=3 July 2014|deadurl=yes|archiveurl=https://web.archive.org/web/20140908015042/http://www.foxnews.com/leisure/2014/07/03/local-motors-3d-printed-car-could-lead-american-manufacturing-revolution/|archivedate=8 September 2014|df=}}</ref> The printing took 44 hours to complete, and was witnessed by a live audience at the 2014 [[International Manufacturing Technology Show]] in [[McCormick Place]], [[Chicago]].<ref>{{cite news|last1=Franklin|first1=Dallas|title=Made in Chicago: World’s First 3D Printed Electric Car|url=http://kfor.com/2014/09/15/made-in-chicago-worlds-first-3d-printed-electric-car/|accessdate=22 September 2014|publisher=KFOR-TV|date=15 September 2014}}</ref> The car consists of 50 individual parts, far less than a traditional vehicle (which is manufactured with roughly 30,000 parts).<ref>{{cite web|url=https://www.forbes.com/sites/eshachhabra/2015/12/30/the-3d-printed-car-that-could-transform-the-auto-industry-on-sale-in-2016/#27b12914502c|title=The 3D Printed Car That Could Transform The Auto Industry: On Sale In 2016|last=Chhabra|first=Esha|access-date=2016-07-20}}</ref> The Strati was designed by Michele Anoè,<ref>{{cite web|url=http://www.imts.com/show/newsletter/insider/article-details.cfm?articleid=20|title=The 3D-Printed Car Is All the Buzz|website=www.imts.com|access-date=2016-07-20}}</ref> a member of the Local Motors community, and is produced in small quantities to serve strategic partnerships, such as with [[NXP Semiconductors]].<ref>{{cite web|url=https://cocreate.localmotors.com/blog/post/local-motors-nxp-unveil-3d-printed-car-with-self-driving-iot-technology/2032/|title=Local Motors, NXP unveil 3D-printed car with self-driving, IoT technology {{!}} Local Motors|date=2016-05-18|language=en-us|access-date=2016-07-20}}</ref>\n\n== LM3D Swim ==\n\nIn 2015, the company debuted a 3D-printed car named the LM3D Swim.<ref>{{cite web|url=http://www.autoblog.com/2015/11/06/the-lm3d-swim-from-local-motors-is-the-first-3d-printed-car-you/|title=The LM3D Swim from Local Motors is the first 3D-printed car you can buy [w/video]|last=Bruce|first=Chris|website=Autoblog|access-date=2016-07-20}}</ref> It was designed by Kevin Lo, a member of the Local Motors community.<ref>{{cite web|url=http://www.nbcnews.com/business/autos/startup-plans-start-selling-first-3d-printed-cars-next-year-n388766|title=Startup Plans to Begin Selling First 3D-Printed Cars Next Year|access-date=2016-07-20}}</ref> The materials used are 80 percent ABS plastic and 20 percent carbon fiber. The vehicle uses technology provided by [[IBM]] that offered IoT connectivity.<ref>{{cite web|url=http://www.ibmbigdatahub.com/blog/connecting-road-octoblu-ibm-and-local-motors|title=Connecting the road with Octoblu, IBM and Local Motors|access-date=2016-07-20}}</ref> The Swim is currently on display at the company’s location in [[National Harbor, Maryland|National Harbor]], [[Maryland]].\n\n== Olli ==\n[[File:Goodyear Olli Genf 2019 1Y7A5023.jpg|thumb|Olli]]\nIn 2016, the company unveiled an autonomous, electric-powered bus.<ref>{{cite web|url=https://www.theverge.com/2016/6/16/11952072/local-motors-3d-printed-self-driving-bus-washington-dc-launch|title=This autonomous, 3D-printed bus starts giving rides in Washington, DC today|last=Warren|first=Tamara|date=2016-06-16|website=The Verge|access-date=2016-07-20}}</ref><ref>{{cite web|url=http://www.autoblog.com/2016/06/16/local-motors-olli-autonomous-shuttle/|title=The Local Motors Olli is a driverless EV minibus with IBM Watson inside|last=Counts|first=Reese|website=Autoblog|access-date=2016-07-20}}</ref> The vehicle was designed by Edgar Sarmiento, initially named the \"Berlino\" from the Urban Mobility Challenge: Berlin 2030.<ref>{{cite web|url=https://cocreate.localmotors.com/eddie_mauro/berlino-30-smart-mini-bus-system/|title=Berlino 3.0 - Smart mini bus system {{!}} Local Motors|date=2015-05-01|language=en-us|access-date=2016-07-20}}</ref> The vehicle was built by Local Motors and has [[Watson (computer)|IBM Watson]] technology installed to provide a personalized experience for riders. The vehicle was demonstrated live to their online audience on Facebook Live<ref>{{cite web|url=https://www.facebook.com/localmotors/videos/10154144618226597/|title=Local Motors - Timeline {{!}} Facebook|website=www.facebook.com|access-date=2016-07-20}}</ref> at a media event in National Harbor. The vehicle is still undergoing development. On January 2, 2018, Local Motors received a pledge of up to a $1 billion in financing and operational support to customers of Olli from Florida-based Elite Transportation Services (ETS) with additional funding of $20 million from Texas-based Xcelerate. <ref>{{Cite web|url=https://www.bizjournals.com/southflorida/prnewswire/press_releases/Florida/2018/01/02/LA79087|title=https://www.bizjournals.com/southflorida/prnewswire/press_releases/Florida/2018/01/02/LA79087|website=www.bizjournals.com|access-date=2018-02-09}}</ref>\n\nOlli was manufactured in [[Chandler, Arizona|Chandler]], [[Arizona]] using additive manufacturing techniques, including 3D Printing.<ref>{{cite web|url=https://cocreate.localmotors.com/blog/post/building-olli-why-second-degree-ddm-is-critical-to-the-process/2052/|title=Building Olli: Why \"Second-degree DDM\" is critical to the process {{!}} Local Motors|date=2016-06-24|language=en-us|access-date=2016-07-20}}</ref> The company has not announced pricing yet.\n\n[[Miami-Dade County, Florida|Miami-Dade County]], the State of [[Nevada]] and the Danish [[Vesthimmerland Municipality]] expressed interest in using Olli on their roadways.<ref>{{cite web|url=http://www.bizjournals.com/washington/blog/2016/06/local-motors-unveils-olli-its-new-self-driving-car.html|title=Local Motors unveils Olli, its new self-driving car, in National Harbor (Video) - Washington Business Journal|website=Washington Business Journal|access-date=2016-07-20}}</ref><ref>{{cite web|url=http://lasvegassun.com/news/2016/jun/16/self-driving-minibus-to-hit-streets-in-las-vegas/|title=Self-driving minibus to hit streets in Las Vegas|date=2016-06-16|access-date=2016-07-20}}</ref><ref>{{Cite news\n | author = [[Sebastian Stryhn Kjeldtoft]]\n | title = Førerløse busser skal spare tid og penge\n | publisher = Information\n | date = 2016-07-27\n | url = https://www.information.dk/indland/2016/07/foererloese-busser-spare-tid-penge\n}}</ref>\n\n== Locations ==\nLocal Motors operates facilities in [[Phoenix, Arizona|Phoenix]], [[Knoxville, Tennessee|Knoxville]], and [[National Harbor, Maryland|National Harbor]]. The company also hosts educational events that are focused on technology, science, and manufacturing topics. People can visit their retail locations in Knoxville and National Harbor to purchase sustainable and locally-sourced merchandise.\n\nAs of February 2017, Local Motors has closed its [[Las Vegas]] location. The Knoxville location is still open <ref>http://www.bizjournals.com/phoenix/news/2017/02/01/exclusive-local-motors-lays-off-employees-shifts.html</ref>\n\n==See also==\n* [[Open design]]\n* [[Microfactory]]\n* [[Open innovation]]\n* [[Creative Commons]]\n\n== References ==\n\n<references />\n\n== External links ==\n* [https://localmotors.com Local Motors Website]\n* [http://automobilityla.com/speaker/john-b-rogers-jr/ About John B. Rogers Jr.]\n{{Automotive industry in the United States}}\n\n[[Category:2007 establishments in Arizona]]\n[[Category:Motor vehicle manufacturers of the United States]]\n[[Category:Manufacturing companies based in Phoenix, Arizona]]\n[[Category:Electric vehicles]]\n[[Category:Open hardware vehicles]]\n[[Category:3D printed objects]]\n[[Category:Manufacturing companies established in 2007]]\n[[Category:American companies established in 2007]]"
    },
    {
      "title": "Perdix (drone)",
      "url": "https://en.wikipedia.org/wiki/Perdix_%28drone%29",
      "text": "{|{{Infobox aircraft begin\n| name           = Perdix\n| image          = File:Perdix-drone.jpg\n| caption        = Perdix UAV in testing\n}}{{Infobox aircraft type\n|type            = [[Micro air vehicle|Unmanned micro-air vehicle]]\n|national origin = [[United States]]\n|manufacturer   = [[MIT Lincoln Laboratory]]\n|designer       = [[Massachusetts Institute of Technology|MIT]]\n|first flight   = September 2014<ref name=\"defsys\">{{cite web|title=DoD ramps micro-drones after successful 'swarm' test|url=https://defensesystems.com/articles/2017/01/13/swarmleopold.aspx|website=www.defensesystems.com|publisher=Defense Systems|accessdate=3 September 2017|archive-url=https://web.archive.org/web/20170904013741/https://defensesystems.com/articles/2017/01/13/swarmleopold.aspx|archive-date=2017-09-04|dead-url=no|df=}}</ref>\n|status         = In testing\n|primary user   = [[United States Department of Defense]]\n|produced       = 2013 - present\n|number built   = 670\n|program cost   = $20 million<ref name=\"technologyreview\">{{cite web|title=A 100-drone swarm dropped from jets plans its own moves|url=https://www.technologyreview.com/s/603337/a-100-drone-swarm-dropped-from-jets-plans-its-own-moves|website=www.technologyreview.com|publisher=Technology Review|accessdate=14 January 2017}}</ref>\n}}\n|}\n'''Perdix''' drones are the main subject of an experimental project conducted by the Strategic Capabilities Office of the [[United States Department of Defense]] which aims to develop autonomous micro-drones to be used for unmanned [[aerial surveillance]].<ref name=\"factsheet\">{{cite web|title=Microsoft Word - Perdix Fact Sheet (01062017 Final)|url=https://www.defense.gov/Portals/1/Documents/pubs/Perdix%20Fact%20Sheet.pdf|website=www.defense.gov|publisher=United States Department of Defense|accessdate=14 January 2017|archive-url=https://web.archive.org/web/20170110204117/https://www.defense.gov/Portals/1/Documents/pubs/Perdix%20Fact%20Sheet.pdf|archive-date=2017-01-10|dead-url=no|df=}}</ref><ref name=\"bbc\" /><ref name=\"defense news\" />\n\n==Origin==\nThe idea of intelligent [[micro air vehicle|micro-drones]] which could communicate with each other was pioneered by a group of students studying at the Aeronautics and Astronautics Department of the [[Massachusetts Institute of Technology]] in 2011.<ref name=\"beaver\">{{cite web|title=Project Perdix|url=https://beaverworks.ll.mit.edu/CMS/bw/projectperdixcapstone|website=www.mit.edu|publisher=Beaver Works|accessdate=14 January 2017|archive-url=https://web.archive.org/web/20170112172111/https://beaverworks.ll.mit.edu/CMS/bw/projectperdixcapstone|archive-date=2017-01-12|dead-url=no|df=}}</ref>  They were subsequently modified for military use in 2013 under the direction of the [[United States Department of Defense]] Strategic Capabilities Office.<ref name=\"military times\">{{cite web|title=Defense Department successfully tests world's largest micro-drone swarm|url=http://www.militarytimes.com/articles/pentagon-successfully-tests-worlds-largest-micro-drone-swarm|website=www.militarytimes.com|publisher=Military Times|accessdate=14 January 2017|archive-url=https://web.archive.org/web/20170113012038/http://www.militarytimes.com/articles/pentagon-successfully-tests-worlds-largest-micro-drone-swarm|archive-date=2017-01-13|dead-url=no|df=}}</ref><ref name=\"national interest\">{{cite web|title=U.S. Military Successfully Tested Its Latest Super Weapon: \"The Swarm\"|url=http://nationalinterest.org/blog/the-buzz/us-military-successfully-tested-its-latest-super-weapon-%E2%80%98the-19002|website=www.nationalinterest.org|publisher=National Interest|accessdate=14 January 2017|archive-url=https://web.archive.org/web/20170114213649/http://nationalinterest.org/blog/the-buzz/us-military-successfully-tested-its-latest-super-weapon-%E2%80%98the-19002|archive-date=2017-01-14|dead-url=no|df=}}</ref>\n\nThe drone system was named after the [[Perdix (mythology)|character in Greek mythology of the same name]].<ref name=\"factsheet\" />\n\n==Autonomy==\nEach individual drone is not controlled in itself but instead it shares a collective, distributed \"brain,\" travelling in leaderless \"swarms,\" members of which can adapt to changes in drone numbers and remain co-ordinated with their counterparts.<ref name=\"national interest\" /><ref name=\"factsheet\" />  Having multiple micro-drones carrying out surveillance is tactically advantageous to simply having one large drone because it is easier for the micro-drones to dodge [[anti-aircraft warfare|air defense systems]].<ref name=\"bbc\" />  The drones have the ability to collectively determine whether they have completed a mission, leading some commentators to argue that Perdix drones are [[artificial intelligence|artificially intelligent]].<ref name=\"popular-mechanics\">{{cite web|title=The Pentagon's Autonomous Swarming Drones Are the Most Unsettling Thing You'll See Today|url=http://www.popularmechanics.com/military/aviation/a24675/pentagon-autonomous-swarming-drones/|website=www.popularmechanics.com|publisher=Popular Mechanics|accessdate=3 September 2017|archive-url=https://web.archive.org/web/20170904015009/http://www.popularmechanics.com/military/aviation/a24675/pentagon-autonomous-swarming-drones/|archive-date=2017-09-04|dead-url=no|df=}}</ref>\n\n==Testing==\n[[File:DOD 103983712-1024x576-1769k.webm|thumb|F/A18 jets drop Perdix drones over California in a test exercise.]]\nThe first operational test of the militarized Perdix drones was conducted by the [[U.S. Air Force Test Pilot School]] in September 2014 over [[Edwards Air Force Base]].<ref name=\"cbs-rev\" /><ref name=\"wash\" /><ref name=\"naval drones\">{{cite web|title=Perdix|url=http://www.navaldrones.com/perdix.html|website=www.navaldrones.com|publisher=Naval Drones|accessdate=3 September 2017|archive-url=https://web.archive.org/web/20170904015246/http://www.navaldrones.com/perdix.html|archive-date=2017-09-04|dead-url=no|df=}}</ref>  The drones were placed in the [[flare (countermeasure)|flare canisters]] of [[F-16 Fighting Falcon]] and deployed to operate at a lower altitude.<ref name=\"ihls\">{{cite web|title=F-16 Launching Perdix Drone Swarm|url=https://i-hls.com/archives/68848|website=www.i-hls.com|publisher=iHLS|accessdate=3 September 2017|archive-url=https://web.archive.org/web/20170904014955/https://i-hls.com/archives/68848|archive-date=2017-09-04|dead-url=no|df=}}</ref>  A year later, in September 2015, 90 Perdix missions were flown over [[Alaska]] to test maritime surveillance capabilities.<ref name=\"factsheet\" />\n\nIn October 2016, 103 Perdix drones were dropped from three [[F/A-18 Super Hornet]] fighter jets in a joint effort with the US Naval Air Systems Command over their base at [[China Lake, Kern County, California|China Lake, California]].<ref name=\"aviationist\">{{cite web|title=U.S. F/A-18 Hornets Unleash Swarm of Mini-Drones in First Test|url=https://theaviationist.com/2017/01/11/watch-u-s-fa-18-hornets-unleash-swarm-of-mini-drones-in-first-test/|website=www.theaviationist.com|publisher=The Aviationist|accessdate=3 September 2017|archive-url=https://web.archive.org/web/20170904020737/https://theaviationist.com/2017/01/11/watch-u-s-fa-18-hornets-unleash-swarm-of-mini-drones-in-first-test/|archive-date=2017-09-04|dead-url=no|df=}}</ref>  As with earlier tests, the drones were packed into [[Flare (countermeasure)|flare canisters]] for the jets to eject.<ref name=\"d1\">{{cite web|title=These Swarming Drones Launch from a Fighter Jet’s Flare Dispensers|url=http://www.defenseone.com/technology/2016/09/these-swarming-drones-launch-fighter-jets-flare-dispensers/131414/|website=www.defenseone.com|publisher=Defense One|accessdate=3 September 2017|archive-url=https://web.archive.org/web/20170904061831/http://www.defenseone.com/technology/2016/09/these-swarming-drones-launch-fighter-jets-flare-dispensers/131414/|archive-date=2017-09-04|dead-url=no|df=}}</ref><ref name=\"inverse\">{{cite web|title=Watch U.S. Fighter Jets Drop a Massive Swarm of 103 Micro-Drones|url=https://www.inverse.com/article/26211-watch-perdix-micro-drone-swarm|website=www.inverse.com|publisher=Inverse|accessdate=3 September 2017|archive-url=https://web.archive.org/web/20170904012241/https://www.inverse.com/article/26211-watch-perdix-micro-drone-swarm|archive-date=2017-09-04|dead-url=no|df=}}</ref><ref name=\"engadget\">{{cite web|title=DoD shows off its first successful micro-drone swarm launch|url=https://www.engadget.com/2017/01/10/dod-shows-off-its-first-successful-micro-drone-swarm-launch/|website=www.engadget.com|publisher=Engadget|accessdate=3 September 2017|archive-url=https://web.archive.org/web/20170904062129/https://www.engadget.com/2017/01/10/dod-shows-off-its-first-successful-micro-drone-swarm-launch/|archive-date=2017-09-04|dead-url=no|df=}}</ref>  The test was a success and elicited significant media coverage when announced on 9 January 2017.<ref name=\"network world\">{{cite web|title=Pentagon tested world's largest swarm of autonomous micro-drones|url=http://www.networkworld.com/article/3156594/security/pentagon-tested-worlds-largest-swarm-of-autonomous-micro-drones.html|website=www.networkworld.com|publisher=Network World|accessdate=14 January 2017|archive-url=https://web.archive.org/web/20170112175954/http://www.networkworld.com/article/3156594/security/pentagon-tested-worlds-largest-swarm-of-autonomous-micro-drones.html|archive-date=2017-01-12|dead-url=no|df=}}</ref><ref name=\"bbc\">{{cite web|title=US military tests swarm of mini-drones launched from jets|url=https://www.bbc.co.uk/news/technology-38569027|website=www.bbc.co.uk|publisher=BBC|accessdate=14 January 2017|archive-url=https://web.archive.org/web/20170113113617/http://www.bbc.co.uk/news/technology-38569027|archive-date=2017-01-13|dead-url=no|df=}}</ref><ref name=\"wash\">{{cite web|title=Watch Perdix - the secretive Pentagon program dropping tiny drones from jets|url=https://www.washingtonpost.com/news/checkpoint/wp/2016/03/08/watch-perdix-the-secretive-pentagon-program-dropping-tiny-drones-from-jets/|website=www.washingtonpost.com|publisher=Washington Post|accessdate=14 January 2017|archive-url=https://web.archive.org/web/20170112083607/https://www.washingtonpost.com/news/checkpoint/wp/2016/03/08/watch-perdix-the-secretive-pentagon-program-dropping-tiny-drones-from-jets/|archive-date=2017-01-12|dead-url=no|df=}}</ref>\n\nThese tests conclude that the drones can be safely launched at a speed of Mach 0.6 and in temperatures as low as {{convert|-10|C|F}}.<ref name=\"dod\" /><ref name=\"newatlas\">{{cite web|title=Super Hornets drop world's largest swarm of micro-drones|url=http://newatlas.com/perdix-micro-drones-super-hornets/47333/|website=www.newatlas.com|publisher=NewAtlas|accessdate=14 January 2017|archive-url=https://web.archive.org/web/20170113152904/http://newatlas.com/perdix-micro-drones-super-hornets/47333/|archive-date=2017-01-13|dead-url=no|df=}}</ref>\n\nPhotographers shooting a feature of the drones for [[CBS]] television program [[60 Minutes]] reportedly almost abandoned attempts to film the drones as their size and speed made getting a focussed image difficult.<ref name=\"cbs-rev\">{{cite web|title=Autonomous drones set to revolutionize military technology|url=https://www.cbsnews.com/news/60-minutes-autonomous-drones-set-to-revolutionize-military-technology/|website=www.cbsnews.com|publisher=CBS|accessdate=3 September 2017|archive-url=https://web.archive.org/web/20170904061754/https://www.cbsnews.com/news/60-minutes-autonomous-drones-set-to-revolutionize-military-technology/|archive-date=2017-09-04|dead-url=no|df=}}</ref><ref name=\"cbs-capture\">{{cite web|title=Capturing the swarm|url=https://www.cbsnews.com/news/60-minutes-capturing-the-perdix-drone-swarm/|website=www.cbsnews.com|publisher=CBS|accessdate=3 September 2017|archive-url=https://web.archive.org/web/20170904011540/https://www.cbsnews.com/news/60-minutes-capturing-the-perdix-drone-swarm/|archive-date=2017-09-04|dead-url=no|df=}}</ref><ref name=\"stripes\">{{cite web|title=Pentagon unveils Perdix micro-drone swarm|url=https://www.stripes.com/news/pentagon-unveils-perdix-micro-drone-swarm-1.448124#.WaxbiMiGPIU|website=www.stripes.com|publisher=Stripes|accessdate=3 September 2017|archive-url=https://web.archive.org/web/20170904015405/https://www.stripes.com/news/pentagon-unveils-perdix-micro-drone-swarm-1.448124#.WaxbiMiGPIU#.WaxbiMiGPIU|archive-date=2017-09-04|dead-url=no|df=}}</ref><ref name=\"popsci\">{{cite web|title=The Pentagon's new drone swarm heralds a future of autonomous war machines|url=http://www.popsci.com/pentagon-drone-swarm-autonomous-war-machines|website=www.popsci.com|publisher=Popular Science|accessdate=3 September 2017}}</ref>\n\n==Design==\nPerdix drones have two sets of [[wings]] which are straddled by a plastic body containing a [[lithium battery]] and a small camera.<ref name=\"dtrends\">{{cite web|title=The Sound Of 103 Micro Drones Launched From An F/A-18 Will Give You Nightmares|url=https://www.digitaltrends.com/cool-tech/perdix-drone-swarm/|website=www.digitaltrends.com|publisher=Digital Trends|accessdate=3 September 2017|archive-url=https://web.archive.org/web/20170904013739/https://www.digitaltrends.com/cool-tech/perdix-drone-swarm/|archive-date=2017-09-04|dead-url=no|df=}}</ref><ref name=\"defense news\">{{cite web|title=Pentagon Launches 103 Unit Drone Swarm|url=http://www.defensenews.com/articles/pentagon-launches-103-unit-drone-swarm|website=www.defensenews.com|publisher=Defense News|accessdate=14 January 2017}}</ref><ref name=\"beaver\" />  Propulsion is provided by a {{convert|2.6|in|mm}} [[propeller]] at the rear.<ref name=\"factsheet\" />  [[3D printing]] is used to create Perdix drones' bodies while the onboard software can be updated to enable refinements and improvements to be made without having to manufacture a new drone.<ref name=\"factsheet\" />  The Perdix software is currently in its sixth generation and the Department of Defense aims to have the capability to produce the drones in batches of 1,000 in the near future.<ref name=\"dod\">{{cite web|title=Department of Defense Announces Successful Micro-Drone Demonstration|url=https://www.defense.gov/News/News-Releases/News-Release-View/Article/1044811/department-of-defense-announces-successful-micro-drone-demonstration|website=www.defense.gov|publisher=U.S. Department of Defense|accessdate=14 January 2017|archive-url=https://web.archive.org/web/20170112191930/https://www.defense.gov/News/News-Releases/News-Release-View/Article/1044811/department-of-defense-announces-successful-micro-drone-demonstration|archive-date=2017-01-12|dead-url=no|df=}}</ref>\n\n==Specifications==\nThe published specifications<ref name=\"factsheet\" /> of Perdix drones are listed below.\n\n===General characteristics===\n* '''Length:''' 6.5 inches / 165mm\n* '''[[Wingspan]]:''' 11.8 inches / 300mm\n* '''Weight:''' 290 grams\n* '''Propeller diameter:''' 2.6 inches / 66mm\n\n===Performance===\n* '''[[V speeds#Regulatory V speeds|Maximum speed]]:''' 70&nbsp;mph / 113&nbsp;kph\n* '''[[Endurance (aeronautics)|Endurance]]:''' 20 minutes\n\n==See also==\n*[[Massachusetts Institute of Technology#Discoveries and innovation|MIT discoveries and innovation]]\n*[[DIUx|Defense Innovation Unit Experimental]]\n*[[Micro air vehicle]]\n\n==References==\n{{reflist}}\n\n[[Category:Unmanned aerial vehicles of the United States]]\n[[Category:Micro air vehicles]]\n[[Category:3D printed objects]]"
    },
    {
      "title": "Piccolissimo",
      "url": "https://en.wikipedia.org/wiki/Piccolissimo",
      "text": "'''Piccolissimo''' is a [[3D printing|3D printed]] single-motor [[micro drone]] that is the size of a coin created by engineers at the [[University of Pennsylvania]] and named after its creator Matt Piccoli.<ref name=techcrunch1>{{cite web|last1=Coldewey|first1=Devin|title=Piccolissimo joins the ranks of ultra-tiny flying robots|url=https://techcrunch.com/2016/10/31/piccolissimo-joins-the-ranks-of-ultra-tiny-flying-robots/|publisher=TechCrunch|accessdate=3 January 2017}}</ref><ref>{{cite web|title=UPENN students 3D print Piccolissimo, world's smallest self-powered flying robot|url=http://www.3ders.org/articles/20161101-upenn-students-3d-print-worlds-smallest-self-powered-remote-controlled-flying-droid.html|website=3ders.org|accessdate=3 January 2017}}</ref><ref>{{cite web|title=Découvrez Piccolissimo, le plus petit drone volant au monde !|url=http://www.lci.fr/high-tech/decouvrez-piccolissimo-le-plus-petit-drone-volant-au-monde-qui-vous-espionnera-peut-etre-un-jour-2010973.html |publisher=[[La Chaîne Info]] |accessdate=3 January 2017|language=fr-FR}}</ref><ref>{{cite web|title=Pocket-sized possibilities: Meet the smallest self-powered controllable drone|url=http://www.digitaltrends.com/cool-tech/piccolissimo-tiny-drone/|publisher=Digital Trends|accessdate=3 January 2017|date=2 November 2016}}</ref><ref name=popularmechanics1>{{cite web|title=Here's the World's Smallest Drone Spinning Itself Into the Air|url=http://www.popularmechanics.com/flight/a23691/worlds-smallest-drone-video/|publisher=Popular Mechanics|accessdate=3 January 2017|date=3 November 2016}}</ref><ref>{{cite web|last1=Mott|first1=Nathaniel|title=UPenn 3D-Printed the World's Smallest Self-Powered Drone|url=https://www.inverse.com/article/23064-upenn-smallest-drone-piccolissimo|publisher=Inverse|accessdate=3 January 2017}}</ref><ref>{{cite web|last1=Olsen|first1=Erik|title=Meet Piccolissimo, the world's smallest self-powered flying robot|url=http://qz.com/826970/meet-piccolissimo-the-worlds-smallest-self-powered-flying-robot/|publisher=Quartz|accessdate=3 January 2017}}</ref><ref>{{cite web|title=Meet Piccolissimo: The World’s Smallest Self-powered Controllable Flying Vehicle|url=http://www.upenn.edu/spotlights/meet-piccolissimo-worlds-smallest-self-powered-controllable-flying-vehicle|accessdate=3 January 2017|publisher=University of Pennsylvania}}</ref>\n\n==The drone==\nPiccolissimo—meaning \"smallest\" in Italian and a pun on the creator's surname—is claimed to be the world's smallest self-powered, controllable flying robot. The size of a quarter, it has just two moving parts: the propeller and the [[3D printing|3D-printed]] body, each of which spins at a different speed. It weighs 2.5 grams and has a payload limit of one gram.<ref name=techcrunch1/> A slightly larger and heavier model that is steerable has been developed.<ref name=popularmechanics1/>\n\nResearchers hope that their drones can be used [[Swarm robotics|in swarms]] for [[search-and-rescue]] operations.<ref name=techcrunch1/><ref name=popularmechanics1/>\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://modlabupenn.org/piccolissimo/ Piccolissimo website at the University of Pennsylvania]\n\n[[Category:Micro air vehicles]]\n[[Category:University of Pennsylvania]]\n[[Category:3D printed objects]]"
    },
    {
      "title": "3D-printed spacecraft",
      "url": "https://en.wikipedia.org/wiki/3D-printed_spacecraft",
      "text": "[[3D printing]] began to be used in production versions of spaceflight hardware in early 2014, when [[SpaceX]] first flew a flight-critical propulsion system assembly on an operational [[Falcon 9]] flight.\nA number of other 3D-printed spacecraft assemblies have been ground-tested, including high-temperature, high-pressure rocket engine combustion chambers<ref name=sdc20140529/> and the entire mechanical spaceframe and integral propellant tanks for a [[small satellite]].<ref name=pr20140626/>\n\nA 3D printed rocket engine successfully launched a rocket to [[outer space|space]] in 2017,<ref name=independent20170606/> and to [[orbit]] in 2018.<ref name=\"spaceflightnow.com\">https://spaceflightnow.com/2018/01/21/rocket-lab-delivers-nanosatellites-to-orbit-on-first-successful-test-launch/</ref>\n\n== History ==\n\n[[3D printing]] began to be used in production versions of spaceflight hardware in early 2014.  In January of that year, [[SpaceX]] first flew a \"[[Falcon 9]] rocket with a 3D-printed [[Cryogenic rocket engine|Main Oxidizer Valve]] (MOV) body in one of the nine [[Merlin 1D]] engines\". The valve is used to control flow of [[cryogenic]] [[liquid oxygen]] to the engine in a high-pressure, low-temperature, high-vibration physical environment.<ref name=sz20140801/>\n\nIn 2015–2016, other 3D-printed spacecraft assemblies were ground-tested, including high-temperature, high-pressure rocket engine combustion chambers and the entire mechanical spaceframe and propellant tanks for a small satellite of a few hundred kilograms.<ref name=pr20140626/>\n\nIn June 2014, [[Aerojet Rocketdyne]] (AJR) announced that they had \"manufactured and successfully tested an engine which had been entirely 3D printed.\"  The ''Baby Banton'' engine is a {{convert|5000|lbf|kN|abbr=on|disp=flip}} thrust engine that runs on [[LOX]]/[[RP-1|kerosene]] propellant.<ref name=3dp20140626>\n{{cite news|title=Aerojet Rocketdyne 3D Prints An Entire Engine in Just Three Parts |url=http://3dprint.com/7355/3d-printed-engine/|accessdate=2014-08-08 |publisher=3dprint.com |date=2014-06-26 }}</ref>\nBy March 2015, AJR had completed a series of hot-fire tests for additively manufactured components for its full-size [[AR-1 (rocket engine)|AR-1]] booster engine.<ref name=ajr20150315>{{cite web|title=Aerojet Rocketdyne Hot-Fire Tests Additive Manufactured Components for the AR1 Engine to Maintain 2019 Delivery|url=https://www.rocket.com/article/aerojet-rocketdyne-hot-fire-tests-additive-manufactured-components-ar1-engine-maintain-2019|publisher=Aerojet Rocketdyne|accessdate=5 June 2015|date=2015-03-15}}</ref>\n\nThe new [[United Launch Alliance]] [[Vulcan (rocket)|Vulcan]] [[launch vehicle]]—with first launch no earlier than 2019—is evaluating 3D printing for over 150 parts: 100 polymer and more than 50 metal parts.<ref name=ibt20150421>{{cite news |last1=Stone |first1=Jeff |title=Vulcan Rocket: 3D Printing Launch Plan Includes More Than 100 Components |url=http://www.ibtimes.com/vulcan-rocket-3d-printing-launch-plan-includes-more-100-components-1890292 |accessdate=22 April 2015 |work=International Business Times |date=2015-04-21}}</ref>\n\nBy 2017, a 3D printed rocket engine had successfully launched a rocket to [[outer space|space]]. when on 25 May 2017, an [[Electron (rocket)|Electron]] rocket launched to space from [[New Zealand]] that was the first to be powered by a main stage rocket \"engine made almost entirely using 3D printing.\"<ref name=independent20170606>[https://www.independent.co.uk/news/science/a-3d-printed-rocket-engine-just-launched-a-new-era-of-space-exploration-a7765496.html https://www.independent.co.uk/news/science/a-3d-printed-rocket-engine-just-launched-a-new-era-of-space-exploration-a7765496.html]</ref> The Electron's first successful orbital launch was on 21 January 2018.<ref name=\"spaceflightnow.com\"/>\n\n==Applications==\n\n=== Rocket engines ===\nThe [[SuperDraco]] engine that provides [[launch escape system]] and propulsive-landing thrust for the [[Dragon V2]] passenger-carrying [[space capsule]] is fully printed, and was the first fully printed [[rocket engine]].  In particular, the engine combustion chamber is printed of [[Inconel]], an alloy of nickel and iron, using a process of [[direct metal laser sintering]], and operates at a [[chamber pressure]] {{convert|1000|psi|kPa|disp=flip}} at a very high temperature.  The engines are contained in a printed protective nacelle to prevent fault propagation in the event of an engine failure.<ref name=aw20140530>\n{{cite news |last=Norris|first=Guy |title=SpaceX Unveils ‘Step Change’ Dragon ‘V2’ |url=http://aviationweek.com/space/spacex-unveils-step-change-dragon-v2|accessdate=2014-05-30 |newspaper=Aviation Week |date=2014-05-30 }}</ref><ref name=sdc20140529>\n{{cite news |last=Kramer|first=Miriam |title=SpaceX Unveils Dragon V2 Spaceship, a Manned Space Taxi for Astronauts — Meet Dragon V2: SpaceX's Manned Space Taxi for Astronaut Trips |url=http://www.space.com/26063-spacex-unveils-dragon-v2-manned-spaceship.html |accessdate=2014-05-30 |newspaper=space.com |date=2014-05-30 }}</ref><ref name=nsf20140530>\n{{cite news |last=Bergin|first=Chris |title=SpaceX lifts the lid on the Dragon V2 crew spacecraft |url=http://www.nasaspaceflight.com/2014/05/spacex-lifts-the-lid-dragon-v2-crew-spacecraft/ |accessdate=2014-05-30 |newspaper=NASAspaceflight.com |date=2014-05-30 }}</ref>\nThe SuperDraco engine produces {{convert|16400|lbf|kN|disp=flip}} of thrust.<ref name=DragonFlyEAI201403>\n{{Citation |last1=James|first1=Michael|last2=Salton|first2=Alexandria| last3=Downing|first3=Micah |title=Draft Environmental Assessment for Issuing an Experimental Permit to SpaceX for Operation of the Dragon Fly Vehicle at the McGregor Test Site, Texas, May 2014 – Appendices| publisher = Blue Ridge Research and Consulting, LCC |pages=12 |date=November 12, 2013 |accessdate=August 8, 2014 |url=http://www.faa.gov/about/office_org/headquarters_offices/ast/media/20140513_DragonFly_DraftEA_Appendices%28reduced%29.pdf }}</ref>\nThe engine completed a full [[Verification and validation|qualification]] test in May 2014, and is slated to make its first [[orbital spaceflight]] in 2018 or 2019.<ref name=sz20140801>\n{{cite web |title=SpaceX Launches 3D-Printed Part to Space, Creates Printed Engine Chamber for Crewed Spaceflight |url=http://www.spacex.com/news/2014/07/31/spacex-launches-3d-printed-part-space-creates-printed-engine-chamber-crewed |publisher=SpaceX |accessdate=2014-08-01 |quote=''Compared with a traditionally cast part, a printed valve body has superior strength, ductility, and fracture resistance, with a lower variability in materials properties. The MOV body was printed in less than two days, compared with a typical castings cycle measured in months. The valve’s extensive test program – including a rigorous series of engine firings, component level qualification testing and materials testing – has since qualified the printed MOV body to fly interchangeably with cast parts on all Falcon 9 flights going forward.''}}</ref><ref name=nsf20140530/><!-- this source also has an excellent-quality photo of the printed SuperDraco rocket engine combustion chamber, but I am unsure how Fair Use works and whether it might be possible to use the image on Wikipedia. -->\n\nThe ability to 3D print the complex parts was key to achieving the low-mass objective of the engine.  It is a very complex engine, and it was very difficult to form all the cooling channels, the injector head, and the throttling mechanism. ... [The ability] \"to print very high strength advanced alloys ... was crucial to being able to create the SuperDraco engine.\"<ref name=nsj20140530>{{cite news|last=Foust|first=Jeff |title=SpaceX unveils its \"21st century spaceship\" |url=http://www.newspacejournal.com/2014/05/30/spacex-unveils-its-21st-century-spaceship/|accessdate=2014-05-31|newspaper=NewSpace Journal|date=2014-05-30}}</ref>\n\nThe rocket engine for the [[Electron (rocket)|Electron]] [[launch vehicle]] is made nearly entirely using 3D printing.<ref name=independent20170606/>\n\n=== Spacecraft structure ===\n[[File:PlanetaryResources 3D printed satellite--201402-cropped.jpg|thumb|right|3D-printed satellite mechanical structure, [[Arkyd-300]], February 2014.  The [[satellite bus|torus]] holds the propellant and provides the structural frame for the satellite. ]]\nBy 2014, 3D printing had begun to be used to print the entire mechanical [[spacecraft bus|structure]] and integral propellant tanks of a [[smallsat|small]] spacecraft.<ref name=pr20140626>\n{{cite web|last1=Diamandis|first1=Peter|title=Update from Planetary Resources|url=https://www.youtube.com/watch?v=O7x-OC9DBuA|website=Peter H. Diamandis channel|publisher=Planetary Resources|accessdate=2014-07-30|date=2014-06-26}}</ref>\n\n==References==\n{{Reflist}}\n\n[[Category:3D printed objects|Spacecraft]]\n[[Category:Spaceflight technologies]]\n[[Category:Spacecraft]]"
    },
    {
      "title": "3D-printed stethoscope",
      "url": "https://en.wikipedia.org/wiki/3D-printed_stethoscope",
      "text": "#redirect [[Stethoscope#3D-printed stethoscope]]\n{{R from merge}}\n\n[[Category:3D printed objects|Stethoscope]]"
    },
    {
      "title": "Strati (automobile)",
      "url": "https://en.wikipedia.org/wiki/Strati_%28automobile%29",
      "text": "[[File:Strati overall.jpg|thumb|250px|Overall view of Strati<ref name=Robarts/>]]\n\n'''Strati''' is an [[electric car]] developed by [[Local Motors]] and manufactured in collaboration with [[Cincinnati Incorporated]] and [[Oak Ridge National Laboratory]].<ref>{{cite news|last1=Gastelu|first1=Gary|title=Local Motors 3D-printed car could lead an American manufacturing revolution|url=http://www.foxnews.com/leisure/2014/07/03/local-motors-3d-printed-car-could-lead-american-manufacturing-revolution/|accessdate=22 September 2014|publisher=Fox News|date=3 July 2014|deadurl=yes|archiveurl=https://web.archive.org/web/20140908015042/http://www.foxnews.com/leisure/2014/07/03/local-motors-3d-printed-car-could-lead-american-manufacturing-revolution/|archivedate=8 September 2014|df=}}</ref> It is the world's first electric car to heavily utilize 3D-printing during the production process.<ref name=\"Russon\"/> The car was manufactured using a Large Scale 3D Printer developed by [[ORNL]] and Cincinnati Inc. The car took just 44 hours to print during the 2014 [[International Manufacturing Technology Show]] in [[Chicago, Illinois]]. The printing was followed by three days of milling and assembling, with the completed car first test-driven on September 13, 2014. Strati is claimed to be the world's first 3D-Printed electric car.<ref name=Robarts>{{cite news|last1=Robarts|first1=Stu|title=\"World's first\" 3D printed car created and driven by Local Motors|url=http://www.gizmag.com/local-motors-strati-imts/33846/|accessdate=22 September 2014|work=Gizmag|date=17 September 2014}}</ref><ref>{{cite news|last1=Franklin|first1=Dallas|title=Made in Chicago: World's First 3D Printed Electric Car|url=http://kfor.com/2014/09/15/made-in-chicago-worlds-first-3d-printed-electric-car/|accessdate=22 September 2014|publisher=KFOR-TV|date=15 September 2014}}</ref>\n\n==Design==\nIn April 2014, Local Motors organized the 3D Printed Car Design Challenge [[crowdsourcing]] to assist in the production of a full-body 3D-printed car. Seven finalists were selected from more than 200 submissions. In June 2014, Local Motors announced that the challenge was won by Michele Anoé of Italy, who was awarded the $5,000 prize.<ref>{{cite news|last1=Jeffrey|first1=Colin|title=Strati wins 3D printed car challenge|url=http://www.gizmag.com/2014-3d-printed-car-challenge-winners/32416/|accessdate=22 September 2014|work=Gizmag|date=9 June 2014}}</ref> After the contest, Local Motors took the design and made several modifications so that the car could be manufactured through 3D-Printing.\n\n==Specifications==\nThe two-seat Strati is considered to be a \"neighborhood\" electric car. Depending on the configuration of the battery packs, the range of the car can be {{Convert|100|to|120|mi|abbr=on}} with top speeds of {{Convert|40|mph|abbr=on}}. The car is not designed to be used on highways, as it does not meet the required safety test requirements. Production is planned by the end of 2015, with prices between $18,000 and $30,000.<ref name=Pyper>{{cite news|last1=Pyper|first1=Julia|title=World's First Three-Dimensional Printed Car Made in Chicago|url=http://www.scientificamerican.com/article/world-s-first-three-dimensional-printed-car-made-in-chicago/|accessdate=22 September 2014|work=Scientific American|date=12 September 2014}}</ref>\n\n<div align=center><gallery widths=\"220px\" heights=\"200px\">\nFile:Strati front.jpg|Front view with steering details exposed\nFile:Strati passenger side.jpg|Passenger side\nFile:Strati rear view.jpg|Rear view\n</gallery></div>\n\n==Manufacturing==\n[[File:Strati printing details.jpg|thumb|Details of the printed body of a Strati<ref>{{cite news|last1=France|first1=Anna Kaziunas|title=First Fused-Filament, Fully-Electric Vehicle|url=http://makezine.com/2014/09/20/the-first-fused-filament-fully-electric-vehicle/|accessdate=22 September 2014|work=Makezine|date=20 September 2014}}</ref>]]\n\nFollowing the design competition, Local Motors handed off the design to the engineers at ORNL who perfected the process of Large Scale 3D Printing such that the Local Motors design could actually be manufactured. ORNL worked with Cincinnati Incorporated to develop the printer that would allow for the printing of the entire car. With the printer, ORNL and Cincinnati Inc. manufactured all body parts of the car and allowed for easy mounting of the mechanical parts, such as the electric motors and batteries.\n\nStrati is printed from [[thermoplastic]] using a [[big area additive manufacturing]] (BAAM)<ref>[http://www.e-ci.com/baam BAAM].</ref> machine (a large FDM 3D-printer). This material is fully recyclable, which can be chopped and reprocessed to be used in printing another car. After the car is printed, the mechanical and electrical parts such as battery, motors, and suspension are manually assembled.<ref name=\"Pyper\"/>\n\nThe printing process has been improved by ORNL since July 2014, bringing the printing time of 140 hours down to less than 45 hours in September. Since IMTS, ORNL has brought the printing time of the Strati to less than 24 hours and is continuing their research efforts with the hope of printing the car in less than 10 hours.\n\n==The world's first title==\nDisputes exist over the title of the world's first 3D-printed car. In 2010, a hybrid car \"Urbee\"<ref>[http://korecologic.com/  Urbee].</ref> was 3D-printed using an additive manufacturing process for the entire body.<ref>{{cite news|last1=Quick|first1=Darren|title=The Urbee hybrid: the world's first 3D printed car|url=http://www.gizmag.com/urbee-3d-printed-car/16795/|accessdate=22 September 2014|work=Gizmag|date=2 November 2010}}</ref> Local Motors claimed that Urbee's manufacturer only 3D-printed the panels and other exterior parts, but used standard parts for the internal structure. For Strati, the company claimed that 3D printing was used for all except the parts that are \"mechanically involved\". Strati claims to be the world's first 3D-printed electric car.<ref name=\"Robarts\"/><ref name=\"Russon\">{{cite news|last1=Russon|first1=Mary-Ann|title=The Strati: World's First 3D-Printed Electric Car Built in Just 44 Hours|url=https://uk.news.yahoo.com/strati-worlds-first-3d-printed-electric-car-built-121616691.html|accessdate=22 September 2014|work=IB Times|date=16 September 2014}}</ref>\n\n== See also ==\n* [[Local Motors]]\n* [[3D printing]]\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n*{{Commons category-inline|Strati automobiles}}\n* [https://localmotors.com Local Motors Website]\n* [http://www.e-ci.com/ Cincinnati Incorporated]\n\n[[Category:Electric cars]]\n[[Category:Open hardware vehicles]]\n[[Category:3D printed objects]]\n[[Category:Cars introduced in 2014]]"
    },
    {
      "title": "Victoria (goose)",
      "url": "https://en.wikipedia.org/wiki/Victoria_%28goose%29",
      "text": "{{refimprove|date=September 2016}}\n\n{{Infobox animal\n| name         = Victoria\n| image        = File:Vitoria_goose_2.jpg\n| caption      = \n| image_size   =\n| species      = [[Goose]]\n| gender       = female\n| hatch_date   = \n| hatch_place  = Brazil\n| death_date   = \n| death_place  = \n| relativeage  =\n| employer     =\n| known_for = First goose prosthetic beak\n| role         = \n| years_active =\n| weight       =\n| website      = \n}}\n\n'''Victoria''' is a goose widely known as the first goose to receive a prosthetic 3D printed beak.<ref>{{cite web\n | url =https://all3dp.com/animal-avengers-victoria-goose/\n | title =Animal Avengers Use 3D Printing to Save Victoria the Goose\n | last =Watkin\n | first =Hannah\n | date =5 Aug 2016\n | website =\n | publisher =All3Dp.com\n | access-date =15 Sep 2016\n | quote = }}</ref> Victoria was found on the São Paulo coast in late 2015, without most of her beak. She was transferred to the care of the [[Friend of the Sea]] in collaboration with [[Animal Avengers (charity)|Animal Avengers]], when [[Cícero Moraes|Cicero Moraes]] was still part of the team.<ref>{{cite news|author1=Janet Tappin Coelho|title=The Animal Avengers saving injured creatures with futuristic technology|url=https://www.mirror.co.uk/science/animal-avengers-saving-injured-creatures-8015912|accessdate=15 Sep 2016|publisher=[[Daily Mirror]]|date=20 May 2016}}</ref> It isn't clear how Victoria lost her beak. A 3D printed prosthetic beak was attached by veterinarians of the University of São Paulo.<ref>{{cite web\n | url =http://www.3ders.org/articles/20160731-victoria-the-goose-gets-2nd-3d-printed-beak-prosthesis-after-failing-to-adapt-to-first-model.html\n | title =Victoria the goose gets 2nd 3D printed beak prosthesis after failing to adapt to first model\n | last =Staff writer\n | first =\n | date =31 Jul 2016\n | website =\n | publisher =3Ders.org\n | access-date =15 Sep 2016\n | quote = }}</ref><ref>{{cite news|author1=Luke Dormehl|title=Mutilated Costa Rican toucan 'to get prosthetic beak|url=http://www.digitaltrends.com/cool-tech/3d-printed-goose-beak-brazil-animal-avengers/|accessdate=15 Sep 2016|publisher=DigitalTrends.|date=2 Aug 2016}}</ref>\n\n<gallery>\nVitoria goose 1.jpg| Victoria  \nVitoria 3D up beak.png| 3D beak modeled  \nVitoria beak 3D.stl| 3D object of the modeled and printed beak\n</gallery>\n== References ==\n{{Reflist}}\n\n[[Category:Individual waterfowl]]\n[[Category:3D printed objects]]\n[[Category:Prosthetics]]\n[[Category:Geese]]\n[[Category:Individual animals in Brazil]]\n\n{{goose-stub}}"
    },
    {
      "title": "3D printed firearms",
      "url": "https://en.wikipedia.org/wiki/3D_printed_firearms",
      "text": "{{Use mdy dates|date=August 2018}}\n{{summarize|from|3D printing#Firearms|date=September 2014}}\n[[File:DD Liberator.png|thumb|The '[[Liberator (gun)|Liberator]]' is a physible, 3D-printable single shot handgun, the first such printable firearm design made widely available online.]]\nIn 2012, the U.S.-based group [[Defense Distributed]] disclosed plans to design a working plastic [[gun]] that could be downloaded and reproduced by anybody with a [[3D printer]].<ref name=\"f20120823\">{{cite news|last=Greenberg|first=Andy|title='Wiki Weapon Project' Aims To Create A Gun Anyone Can 3D-Print At Home|url=https://www.forbes.com/sites/andygreenberg/2012/08/23/wiki-weapon-project-aims-to-create-a-gun-anyone-can-3d-print-at-home/|accessdate=August 27, 2012|newspaper=[[Forbes]]|date=August 23, 2012}}</ref><ref name=\"pcm20120824\">{{cite news|last=Poeter|first=Damon|title=Could a 'Printable Gun' Change the World?|url=https://www.pcmag.com/article2/0,2817,2408899,00.asp|accessdate=August 27, 2012|newspaper=PC Magazine|date=August 24, 2012}}</ref> Defense Distributed has also designed a 3D printable [[AR-15]] type rifle lower receiver (capable of lasting more than 650 rounds) and a variety of magazines, including for the AK-47.<ref>{{cite web|url=https://arstechnica.com/tech-policy/2013/03/download-this-gun-3d-printed-semi-automatic-fires-over-600-rounds/|title=\"Download this gun\": 3D-printed semi-automatic fires over 600 rounds|author=Farivar, Cyrus|work=Ars Technica|date=March 1, 2013|accessdate=February 5, 2015}}</ref> In May 2013, Defense Distributed completed design of the first working blueprint to produce a plastic gun with a 3D printer. The [[United States Department of State]] demanded removal of the instructions from the Defense Distributed website, deeming them a violation of the [[Arms Export Control Act]].<ref>{{cite web|url=http://www.statesman.com/news/news/blueprints-for-3-d-printer-gun-pulled-off-website/nXnbG/ |title=Blueprints for 3-D printer gun pulled off website |publisher=www.statesman.com |date= |accessdate=November 10, 2013}}</ref><ref>{{Cite web|url=https://harvardlawreview.org/2017/04/defense-distributed-v-united-states-department-of-state/|title=Defense Distributed v. United States Department of State|website=harvardlawreview.org|language=en-US|access-date=October 1, 2017}}</ref> In 2015, Defense Distributed founder Cody Wilson sued the United States government on free speech grounds and in 2018 the Department of Justice settled, acknowledging Wilson's right to publish instructions for the production of 3D printed firearms.<ref>{{Cite news|url=https://reason.com/volokh/2018/07/10/us-government-drops-prohibition-on-files|title=US government drops prohibition on files for 3D printed arms|last=Kopel|first=David|date=July 10, 2018|work=Reason.com|access-date=July 13, 2018|archive-url=https://www.webcitation.org/70sRv2q27?url=https://reason.com/volokh/2018/07/10/us-government-drops-prohibition-on-files|archive-date=July 13, 2018|dead-url=no|language=en|df=mdy-all}}</ref><ref>{{Cite news|url=https://www.wired.com/story/a-landmark-legal-shift-opens-pandoras-box-for-diy-guns/|title=A Landmark Legal Shift Opens Pandora’s Box for DIY Guns|last=Greenberg|first=Andy|date=July 10, 2018|work=Wired.com|access-date=July 13, 2018|archive-url=https://web.archive.org/web/20180710175551/https://www.wired.com/story/a-landmark-legal-shift-opens-pandoras-box-for-diy-guns/|archive-date=July 10, 2018|dead-url=no|language=en-US}}</ref>\n\nIn 2013 a Texas company, [[Solid Concepts]], demonstrated a 3D printed version of an [[M1911 pistol]] made of metal, using an industrial 3D printer.<ref>{{cite news|last=Gross|first=Doug|title=Texas company makes metal gun with 3-D printer|url=http://www.cnn.com/2013/11/08/tech/innovation/3d-printed-metal-gun/index.html|publisher=CNN|accessdate=November 9, 2013|date=November 9, 2013}}</ref>\n\n== Effect on gun control ==\nAfter Defense Distributed released their plans, questions were raised regarding the effects that 3D printing and widespread consumer-level [[CNC]] machining<ref>{{cite web|author= |url=https://www.guns.com/news/2013/05/23/3d-printers-meet-othermill-a-cnc-machine-for-your-home-office/ |title=3D Printers, Meet Othermill: A CNC machine for your home office (VIDEO) |publisher=Guns.com |date= |accessdate=November 10, 2013}}</ref><ref>{{cite web|url=http://www.popehat.com/2011/10/06/the-third-wave-cnc-stereolithography-and-the-end-of-gun-control/|title=The Third Wave, CNC, Stereolithography, and the end of gun control|last=|first=|date=October 6, 2011|website=|publisher=PopeHat.com|access-date=|author=Clark}}</ref> may have on [[gun control]] effectiveness.<ref>{{cite news|title=Weapons made with 3-D printers could test gun-control efforts|url=https://www.washingtonpost.com/local/weapons-made-with-3-d-printers-could-test-gun-control-efforts/2013/02/18/9ad8b45e-779b-11e2-95e4-6148e45d7adb_story.html?hpid=z1|newspaper=Washington Post | first=Michael S.|last=Rosenwald|date=February 25, 2013}}</ref><ref>{{cite news|author= |url=https://www.economist.com/news/united-states/21571910-regulatory-and-legal-challenges-posed-3d-printing-gun-parts-ready-print-fire |title=Making guns at home: Ready, print, fire |publisher=The Economist |date=February 16, 2013 |accessdate=November 10, 2013}}</ref><ref>{{cite news|last=Rayner|first=Alex|title=3D-printable guns are just the start, says Cody Wilson|url=https://www.theguardian.com/world/shortcuts/2013/may/06/3d-printable-guns-cody-wilson|newspaper=The Guardian|date=May 6, 2013|location=London}}</ref><ref>{{cite web|last=Manjoo |first=Farhad |url=http://www.slate.com/articles/technology/technology/2013/05/_3_d_printed_gun_yes_it_will_be_possible_to_make_weapons_with_3_d_printers.single.html |title=3-D-printed gun: Yes, it will be possible to make weapons with 3-D printers. No, that doesn’t make gun control futile |publisher=Slate.com |date=May 8, 2013 |accessdate=November 10, 2013}}</ref>\n\nThe U.S. [[Department of Homeland Security]] and the [[Joint Regional Intelligence Center]] released a memo stating \"Significant advances in three-dimensional (3D) printing capabilities, availability of free digital 3D printer files for firearms components, and difficulty regulating file sharing may present public safety risks from unqualified gun seekers who obtain or manufacture 3D printed guns,\" and that \"proposed legislation to ban 3D printing of weapons may deter, but cannot completely prevent their production. Even if the practice is prohibited by new legislation, online distribution of these digital files will be as difficult to control as any other illegally traded music, movie or software files.\"<ref>{{cite news|url=http://www.foxnews.com/us/2013/05/23/govt-memo-warns-3d-printed-guns-may-be-impossible-to-stop/ |title=Homeland Security bulletin warns 3D-printed guns may be 'impossible' to stop |publisher=Fox News |date=May 23, 2013 |accessdate=November 10, 2013}}</ref>\n\nInternationally, where gun controls are generally tighter than in the United States, some commentators have said the impact may be more strongly felt, as alternative firearms are not as easily obtainable.<ref>{{cite web|last=Cochrane |first=Peter |url=http://www.techrepublic.com/blog/european-technology/peter-cochranes-blog-beyond-3d-printed-guns/1728 |title=Peter Cochrane's Blog: Beyond 3D Printed Guns |publisher=TechRepublic |date=May 21, 2013 |accessdate=November 10, 2013}}</ref>  European officials have noted that producing a 3D printed gun would be illegal under their gun control laws,<ref>{{cite web|last=Gilani |first=Nadia |url=http://metro.co.uk/2013/05/06/gun-factory-fears-as-3d-blueprints-available-online-3714514/ |title=Gun factory fears as 3D blueprints put online by Defense Distributed &#124; Metro News |publisher=Metro.co.uk |date=May 6, 2013 |accessdate=November 10, 2013}}</ref> and that criminals have access to other sources of weapons, but noted that as the technology improved the risks of an effect would increase.<ref>{{cite web|url=http://digitaljournal.com/article/349588 |title=Liberator: First 3D-printed gun sparks gun control controversy |publisher=Digitaljournal.com |date= |accessdate=November 10, 2013}}</ref><ref>{{cite web|url=http://www.ibtimes.co.uk/articles/465236/20130507/3d-printed-gun-test-fire-defense-distributed.htm |title=First 3D Printed Gun 'The Liberator' Successfully Fired - IBTimes UK |publisher=Ibtimes.co.uk |date=May 7, 2013 |accessdate=November 10, 2013 |deadurl=yes |archiveurl=https://web.archive.org/web/20131029204738/http://www.ibtimes.co.uk/articles/465236/20130507/3d-printed-gun-test-fire-defense-distributed.htm |archivedate=October 29, 2013 }}</ref> Downloads of the plans from the UK, Germany, Spain, and Brazil were heavy.<ref>{{cite web |url=http://www.neurope.eu/article/us-demands-removal-3d-printed-gun-blueprints |title=US demands removal of 3D printed gun blueprints |publisher=neurope.eu |date= |accessdate=November 10, 2013 |archive-url=https://web.archive.org/web/20131030015133/http://www.neurope.eu/article/us-demands-removal-3d-printed-gun-blueprints |archive-date=October 30, 2013 |dead-url=yes |df=mdy-all }}</ref><ref>{{cite web|url=http://economia.elpais.com/economia/2013/05/09/agencias/1368130430_552019.html |title=España y EE.UU. lideran las descargas de los planos de la pistola de impresión casera &#124; Economía &#124; EL PAÍS |publisher=Economia.elpais.com |date=May 9, 2013 |accessdate=November 10, 2013}}</ref>\n\nAttempting to restrict the distribution over the Internet of gun plans has been likened to the futility of preventing the widespread distribution of [[DeCSS]] which enabled [[DVD]] [[ripping]].<ref>{{cite web|url=http://quietbabylon.com/2013/controlled-by-guns/ |title=Controlled by Guns |publisher=Quiet Babylon |date=May 7, 2013 |accessdate=November 10, 2013}}</ref><ref>{{cite web|url=http://www.joncamfield.com/tags/3dprinting |title=3dprinting &#124; Jon Camfield dot com |publisher=Joncamfield.com |date= |accessdate=November 10, 2013}}</ref><ref>{{cite web|url=http://news.antiwar.com/2013/05/10/state-dept-censors-3d-gun-plans-citing-national-security/ |title=State Dept Censors 3D Gun Plans, Citing ‘National Security’ - News from Antiwar.com |publisher=News.antiwar.com |date=May 10, 2013 |accessdate=November 10, 2013}}</ref><ref>{{cite web|url=http://reason.com/blog/2013/05/08/wishful-thinking-is-control-freaks-last |title=Wishful Thinking Is Control Freaks' Last Defense Against 3D-Printed Guns - Hit & Run |publisher=Reason.com |date=May 8, 2013 |accessdate=November 10, 2013}}</ref> After the US government had Defense Distributed take down the plans, they were still widely available via [[The Pirate Bay]] and other file sharing sites.<ref>{{cite web|author= |url=http://www.salon.com/2013/05/10/the_pirate_bay_steps_in_to_distribute_3d_gun_designs/ |title=The Pirate Bay steps in to distribute 3-D gun designs |publisher=Salon.com |date=May 10, 2013 |accessdate=November 10, 2013}}</ref> Some US legislators have proposed regulations on 3D printers to prevent their use for printing guns.<ref>{{cite web|url=http://sacramento.cbslocal.com/2013/05/08/sen-leland-yee-proposes-regulations-on-3-d-printers-after-gun-test/ |title=Sen. Leland Yee Proposes Regulating Guns From 3-D Printers « CBS Sacramento |publisher=Sacramento.cbslocal.com |date=May 8, 2013 |accessdate=November 10, 2013}}</ref><ref>{{cite web|url=http://newyork.cbslocal.com/2013/05/05/schumer-announces-support-for-measure-to-make-3d-printed-guns-illegal/ |title=Schumer Announces Support For Measure To Make 3D Printed Guns Illegal « CBS New York |publisher=Newyork.cbslocal.com |date=May 5, 2013 |accessdate=November 10, 2013}}</ref> 3D printing advocates have suggested that such regulations would be futile, could cripple the 3D printing industry, and could infringe on free speech rights.<ref>{{cite news|last=Ball|first=James|title=US government attempts to stifle 3D-printer gun designs will ultimately fail|url=https://www.theguardian.com/commentisfree/2013/may/10/3d-printing-gun-blueprint-state-department-ban|newspaper=The Guardian|date=May 10, 2013|location=London}}</ref><ref>{{cite web|author= |url=https://techcrunch.com/2013/01/18/like-it-or-not-i-think-3d-printing-is-about-to-get-legislated/ |title=Like It Or Not, 3D Printing Will Probably Be Legislated |publisher=TechCrunch |date=January 18, 2013 |accessdate=November 10, 2013}}</ref><ref>{{cite news|last=Beckhusen |first=Robert |url=https://www.wired.com/dangerroom/2013/02/gunpowder-regulation/ |title=3-D Printing Pioneer Wants Government to Restrict Gunpowder, Not Printable Guns &#124; Danger Room |publisher=Wired.com |date=February 15, 2013 |accessdate=November 10, 2013}}</ref><ref>{{cite web|url=http://www.theatlanticwire.com/technology/2013/05/how-defense-distributed-already-upended-world/65126/ |title=How Defense Distributed Already Upended the World - Philip Bump |publisher=The Atlantic Wire |date=May 10, 2013 |accessdate=November 10, 2013}}</ref><ref>{{cite web|url=http://www.europeanplasticsnews.com/subscriber/headlines2.html?cat=1&id=2961|title=Plastic gun draws eyes to 3-D printing|last=Putrich|first=Gayle S.|date=May 13, 2013|website=|publisher=European Plastics News|access-date=}}</ref>\n\n== Legal status ==\n\n=== Australia ===\nIn [[Australia]], the state law of [[New South Wales]] criminalizes the possession of the digital plans and files to 3D print firearms under Section 51F of the Firearms Act 1996.<ref>{{cite web|author=New South Wales Legislation|title=Section 51F of Firearms Act 1996 (NSW)|url=http://www.legislation.nsw.gov.au/#/view/act/1996/46/whole#/part6/sec51f|website=New South Wales legislation|accessdate=March 10, 2017}}</ref> In one case in 2015, a loaded 3D printed firearm was found during a police raid on a meth lab.<ref>{{cite news|title=Mudgeeraba acreage raided, 3D-printed gun and drug lab allegedly found|url=http://www.abc.net.au/news/2015-12-10/mudgeeraba-lone-wolves-drug-lab-police-raids-3d-gun/7017264|accessdate=November 15, 2016|agency=ABC News AU}}</ref>\n\nIn another case in February 2017, Sicen Sun was arrested on charges related to 3D printable guns. During trial in December 2017 he pleaded guilty to charges including possessing a digital blueprint for the manufacture of firearms, manufacturing a pistol without a licence permit, and possessing an unauthorised pistol. In sentence hearing on August 6, 2018, he told the court he initially wanted to replicate a gun from the videogame [[Halo (franchise)|Halo]] and when he started searching blueprints online he downloaded plans for other guns which looked \"cool.\"<ref>https://www.smh.com.au/national/nsw/silly-naive-fanboy-faces-jail-over-3d-printed-guns-20180806-p4zvu2.html</ref> Sun had previously posted an advertisement to the internet to sell one of his imitation weapons for \"$1 million negotiable\" on a [[Facebook]] buy, swap and sell group, which set off the investigation.\n\n=== United Kingdom ===\nIn the [[United Kingdom]], the [[Firearms Act 1968]] bans the manufacturing of guns and gun parts without government approval.<ref>{{cite web|author=UK Legislation|title=Firearms Act 1968|url=http://www.legislation.gov.uk/ukpga/1968/27/contents|website=UK Legislation|accessdate=November 15, 2016}}</ref> Hence, 3D printed weapons are de facto banned because the law bans all manufacturing, regardless of method. However, the [[Home Office]] updated its Guide on Firearms Licensing Law to specifically mention the ban on 3D printed weapons.<ref>{{cite web|author=UK Home Office|title=Guide on Firearms Licensing Law|url=https://www.gov.uk/government/uploads/system/uploads/attachment_data/file/518193/Guidance_on_Firearms_Licensing_Law_April_2016_v20.pdf|website=UK Government|publisher=UK Home Office|accessdate=November 15, 2016}}</ref> In 2013, a police raid on a Manchester gang resulted in seizures in what are believed to be 3D printed gun parts. The Greater Manchester police believe they found a trigger and a magazine along with a quantity of [[gunpowder]].<ref>{{cite news|author=Press Association|title=Suspected 3D-printed gun found in Manchester gang raid, say police|url=https://www.theguardian.com/uk-news/2013/oct/25/suspected-3d-printer-gun-found-manchester|accessdate=November 15, 2016|newspaper=[[The Guardian]]|date=October 25, 2013}}</ref>\n\n=== United States ===\nUnder the [[Undetectable Firearms Act]] any firearm that cannot be detected by a metal detector is illegal to manufacture, so legal designs for firearms such as the [[Liberator (gun)|Liberator]] require a metal plate to be inserted into the printed body.  The act had a sunset provision to expire December 9, 2013. Senator [[Charles Schumer]] proposed renewing the law, and expanding the type of guns that would be prohibited.<ref>{{cite web|url=https://www.theguardian.com/technology/2013/nov/17/3d-printing-guns-ban-senate|title=Senator seeks to extend ban on 'undetectable' 3D-printed guns|work=the Guardian|accessdate=February 15, 2015}}</ref> Proposed renewals and expansions of the current Undetectable Firearms Act ({{USBill|113|HR|1474}}, {{USBill|113|S|1149}}) include provisions to criminalize individual production of firearm receivers and magazines that do not include arbitrary amounts of metal, measures outside the scope of the original UFA and not extended to cover commercial manufacture.<ref>{{USBill|113|HR|1474}}</ref><ref>{{USBill|113|S|1149}}</ref>\n\nOn December 3, 2013, the [[United States House of Representatives]] passed the bill [[To extend the Undetectable Firearms Act of 1988 for 10 years (H.R. 3626; 113th Congress)]].<ref name=3626allactions>{{cite web|title=H.R. 3626 - All Actions|url=http://beta.congress.gov/bill/113th-congress/house-bill/3626/all-actions/|publisher=United States Congress|accessdate=December 5, 2013}}</ref> The bill extended the Act, but did not change any of the law's provisions.<ref name=foxnewsHousevotes>{{cite news|title=House votes to renew ban on plastic firearms|url=http://www.foxnews.com/politics/2013/12/03/house-to-vote-on-banning-plastic-firearms/?intcmp=latestnews|accessdate=December 5, 2013|newspaper=Foxnews.com|date=December 3, 2013}}</ref>\n\nOn August 27, 2018, a [[United States federal judge]] blocked the Defense Distributed and its founder, Cody Wilson, from posting 3D-printed gun blueprints online. [[Robert S. Lasnik|Judge Lasnik]] first imposed a temporary restraining order on Wilson, but that was due to expire, so he mandated a [[preliminary injunction]] that blocks online distribution in the United States while the legal proceedings are ongoing.<ref>{{cite news |last=Vanian |first=Jonathan |url=http://fortune.com/2018/08/27/3d-printed-gun-blueprints-judge-ruling/ |title=3D-Printed Gun Blueprints Aren't Allowed Online, Federal Judge Rules |work=[[Fortune (magazine)|Fortune]] |date=2018-08-27 |accessdate=2018-08-28 }}</ref>\n\n=== Japan ===\nIn [[Japan]], in May 2014, Yoshitomo Imura was the first person to be arrested for possessing printed guns.  Imura had five guns, two of which were capable of being fired, but had no ammunition.  Imura had previously posted blueprints and video of his guns to the Internet, which set off the investigation.<ref name=\"Shanghai\">{{cite web|url=http://www.shanghaidaily.com/world/Japanese-man-arrested-for-possessing-3D-printer-guns/shdaily.shtml|title=Japanese man arrested for possessing 3-D printer guns|publisher=|accessdate=February 15, 2015}}</ref>\n\n== See also ==\n* [[3D printing]]\n* [[Defense Distributed]]\n* [[Ghost gun]]\n* [[Gun control]]\n* [[Gun politics in the United States]]\n* [[Improvised firearm]]\n* [[List of 3D printed weapons and parts]]\n\n== References ==\n{{Reflist|30em}}\n\n== External links ==\n* [https://github.com/maduce/fosscad-repo Official FOSSCAD Library Repository]\n* [https://www.wired.com/2014/05/3d-printed-guns/ How 3-D Printed Guns Evolved Into Serious Weapons in Just One Year], ''Wired'', May 2014.\n* [http://www.popularmechanics.com/technology/gadgets/news/should-we-be-afraid-of-the-3d-printed-gun-16700086 Should We Be Afraid of the 3D Printed Gun?], ''Popular Mechanics'', May 2014.\n\n[[Category:3D printed firearms| ]]"
    },
    {
      "title": "List of 3D printed weapons and parts",
      "url": "https://en.wikipedia.org/wiki/List_of_3D_printed_weapons_and_parts",
      "text": "This is a '''list of notable 3D printed weapons and parts'''. The table below lists noteworthy [[3D printing|3D printed]] weapons and parts of weapons as well as items with a defense-related background. It includes 3D printed weapons and parts created using plastic producing printers as well as metal producing printers.\n\nThe [[Liberator (gun)|Liberator .380]] was the first 3D printed plastic gun. It was a single shot pistol made using a [[Stratasys]] Dimension SST 3-D printer.<ref name=\"Greenberg\">{{cite web|last=Greenberg|first=Andy|url=https://www.forbes.com/sites/andygreenberg/2013/05/05/meet-the-liberator-test-firing-the-worlds-first-fully-3d-printed-gun/|title=Meet The 'Liberator': Test-Firing The World's First Fully 3D-Printed Gun|publisher=[[Forbes]]|date=May 5, 2013|accessdate=May 7, 2013}}</ref><ref name=5gunrefs>[http://3dprint.com/14636/3d-prnted-guns/ 5 Different 3D Printed Gun Models Have Been Fired Since May, 2013 – Here They Are], 3D Print, September 10, 2014. ([https://www.webcitation.org/6SVllm7Dl?url=http://3dprint.com/14636/3d-prnted-guns/ archive])</ref>\n\nThe [[Solid Concepts]] replica of [[Browning 1911]] was the first 3D printed metal gun<ref name=\"theguardian.com\">[https://www.theguardian.com/technology/2013/nov/08/metal-3d-printed-gun-50-shots First metal 3D printed gun is capable of firing 50 shots], The Guardian, November 8, 2013. ([https://www.webcitation.org/6SRgjjCsK?url=http://www.theguardian.com/technology/2013/nov/08/metal-3d-printed-gun-50-shots archive])</ref><ref name=\"theguardian.com\"/><ref name=\"cnsnews.com\">[http://www.cnsnews.com/news/article/barbara-hollingsworth/world-s-first-3d-printed-metal-gun-successfully-fires-600-rounds World’s First 3D Printed Metal Gun Successfully Fires 600+ Rounds], CNS News, November 13, 2013. ([https://www.webcitation.org/6SRgztc8i?url=http://www.cnsnews.com/news/article/barbara-hollingsworth/world-s-first-3d-printed-metal-gun-successfully-fires-600-rounds archive])</ref><ref name=\"cnsnews.com\"/> created using more than 38 printed parts;<ref name=\"ReferenceC\">[https://www.guns.com/news/2013/11/20/solid-concepts-3d-printed-1911-gets-version-2-0/ Solid Concepts 3D-printed 1911 gets version 2.0], Guns.com, November 20, 2013. ([https://www.webcitation.org/6SRhGR1eo?url=https://www.guns.com/news/2013/11/20/solid-concepts-3d-printed-1911-gets-version-2-0/ archive])</ref> it successfully fired more than 600 bullets without damaging the gun.<ref name=\"cnsnews.com\"/> The metal printer used to create the weapon cost between US$500,000 to $1 million at the time the gun was created (November 2013).<ref name=\"cnsnews.com\"/>\n\n== List of weapons and parts ==\n; Key/Legend\n{{legend2|#EEEEEE|[[Plastic]] -Weapon/Part used plastic 3D-printer|border=1px solid #AAAAAA}}\n{{legend2|pink|[[Metal]] -Weapon/Part used metal 3D-printer|border=1px solid #AAAAAA}}\n{{legend2|orange|Both -Weapon/Part uses both metal and plastic 3D-printers|border=1px solid #AAAAAA}}\n\n{| class=\"wikitable sortable\" style=\"text-align: left;\"\n! Name\n! Date made public\n! Type of weapon/part or other\n! Printing method\n! Printer used \n! Creator\n! class=\"unsortable\" | Noteworthy facts\n\n|-\n! style=\"background: #EEEEEE;\" | Raptor Grip M500<ref name=ME1>{{cite web|last=Ewer|first=Marty|title=Shockwave Technologies Raptor Grip Test Parts Molded and Approved!|url= http://shockwavetechnologies.com/site/?p=325}}</ref>\n| <span style=\"display: none;\" >2011-12</span>December 15, 2011<ref name=ME2>{{cite news |last=Ewer|first=Marty |title=The 14\" 12-Gauge That Doesn't Require a Tax Stamp |url= http://shockwavetechnologies.com/site/?p=224}}</ref><ref name=ME6>{{cite news |last=6|first=Dutchman |title=Bees Living in Their Heads |url= http://sipseystreetirregulars.blogspot.com/2011/12/calling-atf-on-insane-rule-manufacturer.html}}</ref><ref name=ME7>{{cite news |last=Uncle |first=Say|title=Another NFA Hack |url= http://www.saysuncle.com/2011/12/29/another-nfa-hack-2/}}</ref><ref name=ME8>{{cite news |last=Johnson |first=Steve |title=Grip Raptor Grip (Birdshead) Grip for Mossberg 500 |url= http://www.thefirearmblog.com/blog/2012/01/11/raptor-grip-birdshead-grip-for-mossberg-500/}}</ref><ref name=ME9>{{cite news |title=Mossberg 500 Raptor Grip |url= http://savethegun.wordpress.com/2011/12/29/mossberg-500-raptor-grip/}}</ref>\n\n| '''Part:''' [[Mossberg 500]] birdshead-style grip\n| [[Fused deposition modeling|Fused deposition modeling (FDM)]] method\n| [[Makerbot Cupcake]] #1460 from Batch #14, which began shipping in May 2010.<ref name=ME3>{{cite web|title=Makerbot Lineage: Batch #14 - Ship Date May 2010|url= http://makerbot.wikidot.com/lineage#toc15}}</ref>\n| Shockwave Technologies\n| Prototypes of the grip were 3D printed and tested by firing. Production grips are [[injection moulded]] in a [[glass-filled polymer]].\n*The Raptor Grip M500 allows an individual to start with a pistol-grip-only (PGO) 12- or 20-gauge Mossberg 500 or Maverick 88 firearm and legally install a 14-inch barrel, without prior ATF approval or any additional tax paid, as an OAL of 26.5\" is maintained.<ref name=ME4>{{cite web|title=ATF Tech Branch letter to Mr. Len Savage|url= http://www.nfaoa.org/documents/PistolGrippedShotgunLike.pdf}}</ref>\n*The 3D-printed Raptor Grip M500 prototype survived 400 rounds of #8 birdshot, 220 rounds of 00 buckshot, and 40 slug rounds. It never failed. It still exists to this day in perfect condition in the Shockwave Technologies Museum in [[Salt Lake City]], [[Utah]].\n*The production version of the Raptor Grip M500 went on sale on December 27, 2011.<ref name=ME5>{{cite web|last=Ewer|first=Marty|title=Raptor Grips are IN!!!|url= http://shockwavetechnologies.com/site/?p=409}}</ref>\n\n|-\n! style=\"background: #EEEEEE;\" | [[The Cuomo Mag]]<ref name=FAB1>{{cite web|last=Branson |first=Michael |title=Defense Distributed Releases Printable AK Magazine |url=http://www.thefirearmblog.com/blog/2013/04/08/defense-distributed-releases-printable-ak-magazine/ |accessdate=April 12, 2013 |journal=The Firearm Blog |date=April 8, 2013 |deadurl=bot: unknown |archiveurl=https://www.webcitation.org/6SiJDDXnV?url=http://www.thefirearmblog.com/blog/2013/04/08/defense-distributed-releases-printable-ak-magazine/ |archivedate=September 19, 2014 |df= }}. ()</ref>\n| <span style=\"display: none;\" >2013-01</span>January 2013<ref name=Forbesmag>{{cite news|last=Greenberg |first=Andy |title=Gunsmiths 3D-Print High Capacity Ammo Clips To Thwart Proposed Gun Laws |url=https://www.forbes.com/sites/andygreenberg/2013/01/14/gunsmiths-3d-print-high-capacity-ammo-clips-to-thwart-proposed-gun-laws/ |accessdate=April 12, 2013 |journal=[[Forbes|Forbes Online]] |date=January 14, 2013 |deadurl=bot: unknown |archiveurl=https://www.webcitation.org/6SQYe3oOu?url=http://www.forbes.com/sites/andygreenberg/2013/01/14/gunsmiths-3d-print-high-capacity-ammo-clips-to-thwart-proposed-gun-laws/ |archivedate=September  8, 2014 |df= }} ()</ref>\n| '''Part:''' [[AR-15]] rifle [[STANAG magazine]]<ref name=\"FAB1\"/>\n| [[Fused deposition modeling|Fused deposition modeling (FDM)]] method<ref>[http://www.ammoland.com/2013/03/3d-printed-ar15-magazine/#axzz3ChRscSHd A Printed AR-15 Magazine], Ammoland.com, March 05 2013. ([https://www.webcitation.org/6SQZ6Vk5X?url=http://www.ammoland.com/2013/03/3d-printed-ar15-magazine/ archive])</ref>\n| [[Stratasys]] Dimension SST 3-D printer<ref name=\"archive\">[https://www.forbes.com/sites/andygreenberg/2013/05/08/3d-printed-guns-blueprints-downloaded-100000-times-in-two-days-with-some-help-from-kim-dotcom/ 3D-Printed Gun's Blueprints Downloaded 100,000 Times In Two Days (With Some Help From Kim Dotcom)], forbes.com, August 5, 2013. ([https://www.webcitation.org/6SQZHYIw5?url=http://www.forbes.com/sites/andygreenberg/2013/05/08/3d-printed-guns-blueprints-downloaded-100000-times-in-two-days-with-some-help-from-kim-dotcom/ archive])</ref>\n| [[Defense Distributed]]<ref name=\"Forbesmag\"/>\n| \n*The magazine holds 30 rounds<ref name=\"Forbesmag\"/><ref name=TPM1>{{cite news |last=Franzen|first=Carl |title=Defense Distributed Unveils New 3D Printed Gun Magazine ‘Cuomo’ (video) |url=http://idealab.talkingpointsmemo.com/2013/02/defense-distributed-unveils-new-3d-printed-gun-magazine-cuomo-video.php |accessdate=April 12, 2013 |journal=[[Talking Points Memo]] |date=February 7, 2013}}</ref><ref name=Wired5>{{cite news |last=Beckhusen|first=Robert |title=New 3-D Printed Rifle Magazine Lets You Fire Hundreds of Rounds |url= https://www.wired.com/dangerroom/2013/02/printed-magazine/ |accessdate=April 12, 2013 |journal=[[Wired (magazine)|Wired Danger Room]]|date=February 8, 2013}}</ref>\n*The initial prototype was created using an [[Objet Geometries|Objet]] Connex26 using VeroClear printing material (a transparent material) in order to show the magazine's round count and feeding action<ref>[http://www.extremetech.com/extreme/145664-3d-printed-30-round-ar-magazine-brings-us-ever-closer-to-a-fully-3d-printed-gun 3D-printed 30-round AR magazine brings us ever closer to a fully 3D-printed gun], Extreme Tech, January 14, 2014. ([https://www.webcitation.org/6SiMRYK29?url=http://www.extremetech.com/extreme/145664-3d-printed-30-round-ar-magazine-brings-us-ever-closer-to-a-fully-3d-printed-gun archive])</ref>\n*It was able to handle enough stress to fire 342 rounds and can fire 227+ rounds in quick succession<ref name=Wired5/>\n\n|-\n! style=\"background: #EEEEEE;\" | [[AR Lower V5]]<ref name=TC600>{{cite news|last=Biggs |first=John |title=Defense Distributed Prints An AR-15 Receiver That Has Fired More Than 600 Rounds |url=https://techcrunch.com/2013/03/01/defense-distributed-prints-an-ar-15-receiver-that-has-fired-more-than-600-rounds/ |accessdate=April 12, 2013 |journal=[[TechCrunch]] |date=March 1, 2013 |deadurl=bot: unknown |archiveurl=https://www.webcitation.org/6SQVl1PlV?url=https://techcrunch.com/2013/03/01/defense-distributed-prints-an-ar-15-receiver-that-has-fired-more-than-600-rounds/ |archivedate=September  8, 2014 |df= }} ()</ref>\n| <span style=\"display: none;\" >2013-03</span>March 2013<ref name=\"TC600\"/>\n|  '''Part:''' [[AR-15]] rifle [[lower receiver]]<ref name=\"TC600\"/>\n| Fused deposition modeling (FDM)<ref name=\"Printed AR Lower v5 Review\"/>\n| Stratasys Dimension SST 3-D printer<ref name=\"Printed AR Lower v5 Review\">[http://defdist.tumblr.com/post/44209819568/printed-ar-lower-v5-review Printed AR Lower v5 Review], Defense Distributed official tumblr blog ([https://www.webcitation.org/6SQWPktGE?url=http://defdist.tumblr.com/post/44209819568/printed-ar-lower-v5-review archive])</ref>\n| [[Defense Distributed]]<ref name=\"TC600\"/>\n| The receiver was able to handle enough stress to fire more than 600 rounds<ref name=\"TC600\"/>\n\n|-\n! style=\"background: #EEEEEE;\" | [[Feinstein AK Mag]]<ref name=\"FAB1\"/><ref name=BizIns2>{{cite news|last=Ingersoll |first=Geoffrey |title=3D Printing Company Names AK-47 Magazine After Gun Control Congresswoman |url=http://www.businessinsider.com/defense-distributed-feinstein-ak-mag-2013-3 |accessdate=April 12, 2013 |journal=[[Business Insider]] |date=March 8, 2013 |deadurl=bot: unknown |archiveurl=https://www.webcitation.org/6SRO3NSdI?url=http://www.businessinsider.com/defense-distributed-feinstein-ak-mag-2013-3 |archivedate=September  8, 2014 |df= }} ()</ref><ref name=huffein>[http://www.huffingtonpost.com/2013/03/08/feinstein-ak-magazine_n_2838366.html Defense Distributed's 'Feinstein AK Magazine' Named After Gun Control Advocate Sen. Dianne Feinstein], Huffington Post, August 8, 2013. ([https://www.webcitation.org/6SiJzWMmz?url=http://www.huffingtonpost.com/2013/03/08/feinstein-ak-magazine_n_2838366.html archive])</ref>\n| <span style=\"display: none;\" >2013-03</span>March 2013<ref name=\"FAB1\"/><ref name=\"BizIns2\"/>\n| '''Part:''' [[AK-47]] rifle magazine\n| Fused deposition modeling (FDM)<ref name=\"archive\"/>\n| [[Stratasys]] Dimension SST 3-D printer<ref name=\"archive\"/>\n| [[Defense Distributed]]<ref name=\"FAB1\"/><ref name=\"BizIns2\"/>\n| It is a 30-round 7.62×39 AK-47 magazine<ref name=\"FAB1\"/>\n|-\n! style=\"background: #EEEEEE;\" | [[Liberator (gun)|Liberator .380]]<ref name=\"Greenberg\"/><ref name=\"5gunrefs\"/>\n| <span style=\"display: none;\" >2013-05</span>May 2013<ref name=\"Greenberg\"/><ref name=\"Morelle\">{{cite web|last=Morelle |first=Rebecca |title=Working gun made with 3D printer |url=http://www.bbc.co.uk/news/science-environment-22421185 |work=BBC News |date=May 6, 2013 |accessdate=28 July 2013 |deadurl=bot: unknown |archiveurl=https://www.webcitation.org/6SQVCsc1n?url=http://www.bbc.co.uk/news/science-environment-22421185 |archivedate= 8 September 2014 |df= }} ()</ref><ref name=\"Hutchinson\">{{cite web|last=Hutchinson|first=Lee|title=The first entirely 3D-printed handgun is here|url=https://arstechnica.com/gadgets/2013/05/the-first-entirely-3d-printed-handgun-is-here/|work=Ars Technica|accessdate=13 May 2013}}</ref>\n| '''Weapon:''' [[Pistol]]\n| Fused deposition modeling (FDM)<ref>[http://www.inhale3d.com/tag/liberator/ 3D Printing Guns off the Download – Fact or Fiction?], inhale3d, May 7, 2013. ([https://www.webcitation.org/6SQXPDuUh?url=http://www.inhale3d.com/tag/liberator/ archive])</ref>\n| [[Stratasys]] Dimension SST 3-D printer<ref name=\"cbsnews.com\">[http://www.cbsnews.com/news/liberator-gun-made-with-3d-printer-fires-first-successful-shot/ \"Liberator\" gun made with 3D printer fires first successful shot], CBS NEWS, May 6, 2013. \"...Stratasys Dimension SST 3D printer\" ([https://www.webcitation.org/6SQVAHLIM?url=http://www.cbsnews.com/news/liberator-gun-made-with-3d-printer-fires-first-successful-shot/ archive])</ref>\n| [[Defense Distributed]]<ref name=\"cbsnews.com\"/>\n|\n* It is a single shot pistol<ref name=\"Greenberg\"/><ref name=\"Morelle\"/><ref name=\"Hutchinson\"/>\n* Uses [[.380 ACP]] cartridges\n\n|-\n! style=\"background: #EEEEEE;\" | [[Red rocket (shotgun slug)|Red Rocket]] [[shotgun slug]]<ref name=\"dailymail.co.uk\">[http://www.dailymail.co.uk/sciencetech/article-2329105/Is-3D-printed-BULLET-YouTube-video-shows-homemade-ammunition-fired.html Is this the first 3D-printed Bullet?], DailyMail, May 22, 2013. ([https://www.webcitation.org/6SRQHtugM?url=http://www.dailymail.co.uk/sciencetech/article-2329105/Is-3D-printed-BULLET-YouTube-video-shows-homemade-ammunition-fired.html archive])</ref>\n| <span style=\"display: none;\" >2013-05</span>May 2013<ref name=\"dailymail.co.uk\"/>\n| '''Part:''' 12 gauge Mossberg 590 shotgun slug<ref name=\"dailymail.co.uk\"/>\n| \n| [[Solidoodle]] 3 3-D printer<ref name=\"wired.com\">[https://www.wired.com/2013/05/3d-printed-bullets/ 3-D Printed Shotgun Slugs Blow Away Their Targets], Wired, May 22, 2013. ([https://www.webcitation.org/6SRR2wYqe?url=http://www.wired.com/2013/05/3d-printed-bullets/ archive])</ref>\n| Jeef Hesszel<ref>[http://www.huffingtonpost.com/2013/05/23/3d-printed-bullets_n_3322370.html 3D-Printed Bullets Exist, And They're Terrifyingly Easy To Make], huffingtonpost, May 23, 2013. ([https://www.webcitation.org/6SRQoTSbA?url=http://www.huffingtonpost.com/2013/05/23/3d-printed-bullets_n_3322370.html archive])</ref>\n| \n*The printer retails around US$800 as of September 2014, and it took the printer about an hour to produce the bullet<ref name=\"wired.com\"/>\n*ABS thermoplastic material was used<ref name=\"wired.com\"/>\n*During testing, the bullet blasted through a 2×12 piece of pine wood, and also created a hole in a wire reel<ref name=\"wired.com\"/>\n\n|-\n! style=\"background: #EEEEEE;\" | [[Charon (gun)|Charon]]<ref name=\"outdoorhub.com\">[http://www.outdoorhub.com/news/2013/05/22/3d-printed-hybrid-ar-15fn-p90-lower-and-12-gauge-slugs-make-web-debut/ 3D-printed Hybrid AR-15/FN P90 Lower and 12 Gauge Slugs Make Web Debut], outdoorhub, May 22, 2013. ([https://www.webcitation.org/6SRYaP5jC?url=http://www.outdoorhub.com/news/2013/05/22/3d-printed-hybrid-ar-15fn-p90-lower-and-12-gauge-slugs-make-web-debut/ archive])</ref><ref name=\"guns.com\">Slowik, Max, [http://www.guns.com/2013/06/03/meet-the-charon-family-of-3d-printable-ar-lowers-photos/ Meet the Charon Family of 3D-Printable AR Lowers (Photos)], 3 June 2013.</ref><ref name=\"Slowik 2013\">Slowik, Max, [http://www.guns.com/2013/07/01/3d-printing-community-updates-liberator-with-rifle-pepperbox-and-glock-powered-shuty-9 \"3D Printing Community Updates Liberator with Rifle, Pepperbox and Glock-Powered ‘Shuty-9′\"], 1 July 2013.</ref>\n| <span style=\"display: none;\" >2013-05</span>May 2013<ref name=\"outdoorhub.com\"/>\n| '''Part:''' [[AR-15]] rifle [[Receiver (firearms)|lower receiver]]<ref name=\"outdoorhub.com\"/><ref name=\"guns.com\"/><ref name=\"Slowik 2013\"/>\n| Fused deposition modeling (FDM) method<ref name=\"Charon V3\">[http://grabcad.com/library/charon-v3-1 Charon V3], grabcad, September 3, 2013. ([https://www.webcitation.org/6SRZ5hiDG?url=http://grabcad.com/library/charon-v3-1 archive])</ref>\n| [[LulzBot]] Taz<ref name=\"Guns 2013\">[https://www.guns.com/news/2013/05/21/introducing-the-warfairy-p-15-3d-printed-ar-stock/ Introducing the WarFairy P-15 3D-Printed AR Stock], Guns.com, May 21, 2013. ([https://www.webcitation.org/6SRZoLls5?url=https://www.guns.com/news/2013/05/21/introducing-the-warfairy-p-15-3d-printed-ar-stock/ archive])</ref>\n| [[WarFairy]]<ref name=\"guns.com\"/><ref name=\"Slowik 2013\"/>\n| Charon V3 weighs 0.2 pounds and showed no signs of strain even after 96 rounds of 5.56 AR-15 ammo was fired.<ref name=\"Charon V3\"/>\n\n|-\n! style=\"background: #EEEEEE;\" | [[WarFairy P-15]]<ref name=\"outdoorhub.com\"/>\n| <span style=\"display: none;\" >2013-05</span>May 2013<ref name=\"outdoorhub.com\"/>\n| '''Part:''' [[Fabrique Nationale P90]] stock<ref name=\"outdoorhub.com\"/>\n| Fused deposition modeling (FDM) method<ref name=\"Charon V3\"/>\n| LulzBot Taz<ref name=\"Guns 2013\"/>\n| WarFairy<ref name=\"guns.com\"/><ref name=\"Slowik 2013\"/>\n| The stock works a lower receiver for the FN-P90 but would work with any standard AR.<ref name=\"Guns 2013\"/>\n\n|-\n! style=\"background: pink;\" | [[3DX (muzzle brake)|3DX]]<ref name=\"gizmag.com\">[http://www.gizmag.com/sintercore-auxetik-3d-printed-muzzle-brake-inconel/28489/ Sintercore creates first commercial 3D printed metallic firearm component], Gizmag, July 30, 2013. ([https://www.webcitation.org/6SRU7dqAy?url=http://www.gizmag.com/sintercore-auxetik-3d-printed-muzzle-brake-inconel/28489/ archive])</ref><ref name=\"Sintercore 3DX Muzzle Brake\">[http://www.thefirearmblog.com/blog/2014/08/26/sintercore-3dx-muzzle-brake/ Sintercore 3DX Muzzle Brake], thefirearmblog, August 26, 2014.([https://www.webcitation.org/6SRV0XN9X?url=http://www.thefirearmblog.com/blog/2014/08/26/sintercore-3dx-muzzle-brake/ archive])</ref> muzzle brake\n| <span style=\"display: none;\" >2013-07</span>July 2013<ref name=\"gizmag.com\"/>\n| '''Part:''' AR-15 rifle [[Muzzle brake]]<ref name=\"gizmag.com\"/>\n| [[Direct metal laser sintering]] (DMLS) method<ref name=\"gizmag.com\"/>\n|\n| [[Sintercore]]<ref name=\"gizmag.com\"/>\n| \n*It is designed to tame the recoil and muzzle rise of the [[AR-15]] semi automatic rifle chambered for 5.56×45mm NATO (.223) rounds.<ref name=\"gizmag.com\"/> It uses metal [[Inconel]] material<ref>[https://www.guns.com/news/2014/02/20/quick-review-sintercore-auxetik-3d-printed-inconel-brake-video/ Quick review of the Sintercore Auxetik 3D-printed Inconel brake (VIDEO)], Guns.com, February 20, 2014. ([https://www.webcitation.org/6StIx5g91?url=https://www.guns.com/news/2014/02/20/quick-review-sintercore-auxetik-3d-printed-inconel-brake-video/ archive])</ref>\n*It fired 7900 rounds during testing on semi-auto.<ref name=\"Sintercore 3DX Muzzle Brake\"/> During a test on full auto, 10 magazines of 62 grain green tip 5.56 rounds were all fired without any issues.<ref name=\"Sintercore 3DX Muzzle Brake\"/>\n*US Special Operation Command's ([[USSOCOM]]) Science and Technology Directorate invited Neal (the owner of Sintercore) to demonstrate the 3DX muzzle brake for possible use by its elite troops. They tested it on August 5, 2014.<ref name=\"Sintercore 3DX Muzzle Brake\"/>\n|-\n! style=\"background: pink;\" | NERO 556 Muzzle brake<ref name=\":0\">{{Cite web|url=https://www.personaldefenseworld.com/2018/12/nero-tactical-muzzle-brake/|title=First Look: Walker Defense Research NERO Tactical Muzzle Brake|date=2018-12-06|website=Personal Defense World|language=en-US|access-date=2019-02-04}}</ref><ref name=\":1\">{{Cite web|url=http://walkerdr.com/nero556.html|title=NERO 556 {{!}} AR15 Muzzle Brake, 3D Printed Inconel|website=Walker Defense Research LLC|access-date=2019-02-04}}</ref>\n|2018<ref name=\":0\" />\n|'''Part:''' AR-15 Muzzle brake\n|Direct Metal Laser Sintering (DMLS) method<ref name=\":1\" />\n|\n|Walker Defense Research<ref>{{Cite web|url=https://www.americanrifleman.org/articles/2018/11/29/product-preview-walker-defense-research-nero-556-tactical-muzzle-brake/|title=Product Preview: Walker Defense Research NERO 556 Tactical Muzzle Brake|website=www.americanrifleman.org|language=en|access-date=2019-02-04}}</ref><ref name=\":1\" />\n|\n* Made from Inconel<ref>{{Cite web|url=https://www.tactical-life.com/gear/suppressors/walker-defense-research-nero-556-brake/|title=VIDEO: The Walker Defense NERO 556 Brake Rocks in Full-Auto|date=2018-05-24|website=Tactical Life Gun Magazine: Gun News and Gun Reviews|language=en-US|access-date=2019-02-04}}</ref>\n* \"The idea was to create a muzzle brake with no equal; one that turns the recoil of an AR-15 into a slight push with zero muzzle rise.\"<ref name=\":0\" />\n* Produced for 5.56mm (.223 Rem) or [[.224 Valkyrie]]<ref name=\":1\" /> rifles with 1/2x28 threads<ref name=\":0\" />. \n* Each muzzle brake is serialized<ref>{{Cite web|url=https://www.thefirearmblog.com/blog/2018/05/21/walker-defense-research-nero-556-muzzle-device/|title=Walker Defense Research NERO 556 Muzzle Device -|date=2018-05-21|website=The Firearm Blog|language=en-US|access-date=2019-02-04}}</ref>\n|-\n! style=\"background: #EEEEEE;\" |[[Grizzly (.22-caliber rifle)|Grizzly]],<ref name=5gunrefs/><ref name=\"nbcnews.com\">[http://www.nbcnews.com/tech/innovation/first-3-d-printed-rifle-fires-bullet-then-breaks-f8C10752930 First 3-D printed rifle fires bullet, then breaks], NBC News, July 26, 2013. ([https://www.webcitation.org/6SRSLyjdT?url=http://www.nbcnews.com/tech/innovation/first-3-d-printed-rifle-fires-bullet-then-breaks-f8C10752930 archive])</ref>\nGrizzly 2.0 (.22-caliber rifle)<ref name=\"Gun-advocates-expect-flood-3D-printed-guns-result-U-S-law-changes--Canadian-designed-plastic-rifle-successfully-fires-14-rounds 2013\">[http://www.dailymail.co.uk/news/article-2387624/Gun-advocates-expect-flood-3D-printed-guns-result-U-S-law-changes--Canadian-designed-plastic-rifle-successfully-fires-14-rounds.html 3D printed plastic rifle successfully fires 14 rounds - as gun advocates predict it will force changes in the law], DailyMail, 9 August 2013. ([https://www.webcitation.org/6SRSn0LE9?url=http://www.dailymail.co.uk/news/article-2387624/Gun-advocates-expect-flood-3D-printed-guns-result-U-S-law-changes--Canadian-designed-plastic-rifle-successfully-fires-14-rounds.html archive])</ref>\n| <span style=\"display: none;\" >2013-08</span>August 2013<ref name=\"nbcnews.com\"/>\n| '''Weapon:''' [[.22 Long Rifle]]<ref name=5gunrefs/>\n| \n| [[Stratasys]] Dimension 1200es<ref name=\"nbcnews.com\"/>\n| \"Matthew\" (pseudonym)<ref name=\"nbcnews.com\"/><ref name=\"theverge.com\">[https://www.theverge.com/2013/8/4/4588162/worlds-first-3d-printed-rifle-the-grizzly-updated World's first 3D-printed rifle gets update, fires 14 shots], The Verge, August 4, 2013.([https://www.webcitation.org/6SRScZvop?url=http://www.theverge.com/2013/8/4/4588162/worlds-first-3d-printed-rifle-the-grizzly-updated archive])</ref>\n| \n*The original Grizzly fired 1 shot then broke.<ref name=\"nbcnews.com\"/> Grizzly 2.0 fired 14 bullets before getting damaged due to the strain.<ref name=\"theverge.com\"/>\n*According to the [[Daily Mail]], the Grizzly 2.0 performed so well that the inventor \"Matthew\" was able to put it to his shoulder and shot off three rounds with the rifle pressed against his cheek without hurting him.<ref name=\"Gun-advocates-expect-flood-3D-printed-guns-result-U-S-law-changes--Canadian-designed-plastic-rifle-successfully-fires-14-rounds 2013\"/>\n*The printer costs US$10,000 as of August 2013<ref name=\"Gun-advocates-expect-flood-3D-printed-guns-result-U-S-law-changes--Canadian-designed-plastic-rifle-successfully-fires-14-rounds 2013\"/>\n\n|-\n! style=\"background: pink\" | Americas Cup sailor's knife<ref name=knife/><ref name=knife2/>\n| <span style=\"display: none;\" >2013-08</span>August 2013<ref name=knife>[http://www.nzherald.co.nz/bay-of-plenty-times/business/news/article.cfm?c_id=1503347&objectid=11107028 3D printer at cutting edge], New Zealand Herald, August 8, 2013. ([https://www.webcitation.org/6SShn1VVR?url=http://www.nzherald.co.nz/bay-of-plenty-times/business/news/article.cfm?c_id%3D1503347%26objectid%3D11107028 archive])</ref><ref name=knife2>[http://www.metal-am.com/articles/002735.html New Zealand leads the way in Titanium Additive Manufacturing], Metal Additive Manufacturing, May 12, 2014. ([https://www.webcitation.org/6SSiizkGk?url=http://www.metal-am.com/articles/002735.html archive])</ref>\n| '''Weapon:''' Titanium [[Knife]]<ref name=knife/><ref name=knife2/>\n| [[Selective laser melting]] method (using Titanium Powder processing)<ref name=knife/><ref name=knife2/>\n| Unknown NZ$1.2 million printer.<ref name=knife/><ref name=knife2/>\n| Rapid Advanced Manufacturing<ref name=knife/><ref name=knife2/>\n| According to the New Zealand Herald: \"They threw it off a building on to asphalt from approximately 14m and they did that about 20 times, and then they threw it against a brick wall about 10 times and then they ran it over with a forklift 10 times. That was their acceptance testing and it survived, it didn't get broken\".<ref name=knife/><ref name=knife2/>\n\n|-\n! style=\"background: #EEEEEE;\" | [[Reprringer]]<ref name=5gunrefs/><ref name=\"D-printed Reprringer Pepperbox 2013\">[https://www.guns.com/news/2013/09/13/introducing-3d-printed-reprringer-pepperbox-video/ Introducing the 3D-printed Reprringer Pepperbox (VIDEO)], guns.com, September 13, 2013. ([https://www.webcitation.org/6SRc8adlR?url=http://www.thefirearmblog.com/blog/2013/10/08/3d-printed-pepperbox-pistol/ archive])</ref><ref name=\"3D Printed Pepperbox Pistol\">[http://www.thefirearmblog.com/blog/2013/10/08/3d-printed-pepperbox-pistol/ 3D Printed Pepperbox Pistol], firearmblog, October 8, 2013. ([https://www.webcitation.org/6SRcXdCdQ?url=http://www.thefirearmblog.com/blog/2013/10/08/3d-printed-pepperbox-pistol/ archive])</ref><ref name=\"wired.co.uk\">[https://www.wired.co.uk/news/archive/2014-05/16/3d-printed-guns 3D printed guns a year on: from prototype to serious weapons], Wired, 16 May 2014. ([https://www.webcitation.org/6SRXdTEJs?url=http://www.wired.co.uk/news/archive/2014-05/16/3d-printed-guns archive])</ref>\n| <span style=\"display: none;\" >2013-09</span>September 2013<ref name=\"D-printed Reprringer Pepperbox 2013\"/>\n| '''Weapon:''' [[Pepper-box]] .22-Caliber revolver<ref name=5gunrefs/><ref name=\"D-printed Reprringer Pepperbox 2013\"/>\n|\n| Many plastic 3D printers<ref name=\"D-printed Reprringer Pepperbox 2013\"/>\n| Hexen<ref name=\"D-printed Reprringer Pepperbox 2013\"/><ref name=\"3D Printed Pepperbox Pistol\"/>\n| \n*It can hold 5 bullets and is chambered in .22 Caliber caps.<ref name=\"D-printed Reprringer Pepperbox 2013\"/><ref name=\"3D Printed Pepperbox Pistol\"/>\n*Unlike the many early 3D-printed firearm designs, which are usually massively overbuilt in order to withstand the pressures and strain on the material from modern gunpowder cartridges, the Reprringer is small and only slightly larger than the equivalent gun made from steel.<ref name=\"D-printed Reprringer Pepperbox 2013\"/>\n\n|-\n! style=\"background: pink\" | [[5.56×45mm NATO|5.56×45mm]]/[[.223 Remington|.223]] rifle [[suppressor]]s: \n*556-45 Samson (Samson suppressor replica) \n*556-SBR\n*556-45 Suppressor<ref name=\"556 Suppressors\">[http://oceania-defence.com/index.php?main_page=index&cPath=41_42 556 Suppressors] {{webarchive|url=https://web.archive.org/web/20140911014912/https://oceania-defence.com/index.php?main_page=index |date=2014-09-11 }}, Oceania Defence. ([https://www.webcitation.org/6SSwqyp1w?url=http://oceania-defence.com/index.php?main_page%3Dindex%26cPath%3D41_42 archive])</ref>\n| <span style=\"display: none;\" >2013-11</span>November 2013<ref name=sup2>{{cite web|url=http://www.nzherald.co.nz/bay-of-plenty-times/business/news/article.cfm?c_id=1503347&objectid=11160159 |title=Hi-tech printers at home in Bay |newspaper=[[New Zealand Herald]] |date=20 November 2013 |archive-url=https://www.webcitation.org/6SSx9BNYG?url=http://www.nzherald.co.nz/bay-of-plenty-times/business/news/article.cfm?c_id%3D1503347%26objectid%3D11160159 |archive-date= 9 September 2014 |deadurl=yes |df= }}</ref>\n| '''Part:''' Titanium 5.56mm/.223 rifle [[suppressor]] and works for smaller rifles<ref name=\"556 Suppressors\"/>\n| [[Selective laser melting]] method (using Titanium Powder processing)<ref name=knife2/>\n| Unknown NZ$1.2 million printer.<ref name=\"sup2\"/>\n| Oceania Defence Ltd.<ref name=knife2/><ref name=\"sup2\"/><ref name=sup1>[http://3dprintingsystems.com/silencing-the-sound-layer-by-layer-video/ Silencing the sound, Layer by layer Video], 3d Printing Systems. ([https://www.webcitation.org/6SSwbP5oa?url=http://3dprintingsystems.com/silencing-the-sound-layer-by-layer-video/ archive])</ref> and Rapid Advanced Manufacturing<ref name=supram>[http://www.tida.co.nz/upload/files/TiDA-Talk-July-2013-final-1.pdf TiDA Talk], Titanium Industry Development Association, July 2013. ([https://www.webcitation.org/6SSxRW7kC?url=http://www.tida.co.nz/upload/files/TiDA-Talk-July-2013-final-1.pdf archive])</ref>\n| \n*It is an additively manufactured titanium weapon suppressor. It is 50% lighter than conventional steel weapons<ref name=knife2/>\n*Oceania Defence has made three variations of the 5.56mm/.223 suppressor so far. They are: 556-45 Samson (an AR-15 suppressor designed to operate on semi auto [[Short-barreled rifle]] to 12.5\" barrels), 556-SBR (designed for hard use on 10.5\" barrel for AR-15 firearms in 5.56mm/.223 ammunition) and 556-45 Suppressor (direct thread on suppressor which overlaps the barrel designed to reduce a baffle strike)<ref name=\"556 Suppressors\"/>\n\n|-\n! style=\"background: pink\" | [[7.62×51mm NATO|7.62×51mm]]/[[.308 Winchester|.308]] rifle [[suppressor]]s:\n*300 BLK Long\n*762-AR10\n*762-G3<ref name=\"762 Suppressor\">[http://oceania-defence.com/index.php?main_page=index&cPath=41_43 762 Suppressor] {{webarchive|url=https://web.archive.org/web/20140910195256/http://oceania-defence.com/index.php?main_page=index&cPath=41_43 |date=2014-09-10 }}, Oceania Defence. ([https://www.webcitation.org/6ST06EFYV?url=http://oceania-defence.com/index.php?main_page%3Dindex%26cPath%3D41_43 archive])</ref>\n| <span style=\"display: none;\" >2013-11</span>November 2013<ref name=\"sup2\"/>\n| '''Part:''' Titanium suppressor for 7.62mm rifles<ref name=\"762 Suppressor\"/>\n| Selective laser melting  using titanium powder<ref name=knife2/>\n| Unknown NZ$1.2 million printer.<ref name=\"sup2\"/>\n| Oceania Defence Ltd.<ref name=knife2/><ref name=\"sup2\"/><ref name=\"sup1\"/> and Rapid Advanced Manufacturing<ref name=\"supram\"/>\n| \n*It is an additively manufactured titanium weapon suppressor. It is 50% lighter than conventional steel weapons<ref name=knife2/>\n*Oceania Defence made 3 variations of the 7.62mm rifle suppressor. They are: 300 BLK Long (designed for use with the [[7.62×35mm|300 AAC Blackout]] system in both subsonic and supersonic and fits under the rail of a Samson or similar rail system on the [[AR-15]]-type rifle, but can to perform acceptably on bolt action 7.62×51mm/.308 rifles as well), 762-AR10 Suppressor (designed for the [[AR-10]]/[[AR-10#ArmaLite AR-10B rifle series|LAR-8]] 7.62mm/.308 rifle but will also work with any bolt-action rifle in .30 caliber or less) and 762-G3 Suppressor (designed for the [[Heckler & Koch G3]] and fits the [[Heckler & Koch HK41#HK91|HK91]] series of 7.62mm/.308 rifles, as well as the [[Heckler & Koch HK33|HK33]] and [[Heckler & Koch HK43#HK93|HK93]] series of 5.56×45mm rifles).<ref name=\"762 Suppressor\"/>\n\n|-\n! style=\"background: pink\" | Small-caliber [[suppressor]]s:\n* Long version for [[9×19mm Parabellum|9mm]] [[carbine]]s<ref name=\"9mm Carbine Long\">[http://oceania-defence.com/index.php?main_page=product_info&cPath=41_47&products_id=70 9mm Carbine Long] {{webarchive|url=https://web.archive.org/web/20140910195609/http://oceania-defence.com/index.php?main_page=product_info&cPath=41_47&products_id=70 |date=2014-09-10 }}, Oceania Defence. ([https://www.webcitation.org/6ST0BwNQ6?url=http://oceania-defence.com/index.php?main_page%3Dproduct_info%26cPath%3D41_47%26products_id%3D70 archive])</ref>\n* UTU for pistols, including [[M1911]] and 9mm<ref name=\"oceania-defence.com\">[http://oceania-defence.com/index.php?main_page=product_info&cPath=41_48&products_id=73 UTU 9mm Titanium Pistol Suppressor with Neilsen] {{webarchive|url=https://web.archive.org/web/20140910195254/http://oceania-defence.com/index.php?main_page=product_info&cPath=41_48&products_id=73 |date=2014-09-10 }}, Oceania Defence. ([https://www.webcitation.org/6ST0eZrRH?url=http://oceania-defence.com/index.php?main_page%3Dproduct_info%26cPath%3D41_48%26products_id%3D73 archive])</ref>\n| <span style=\"display: none;\" >2013-11</span>November 2013<ref name=\"sup2\"/>\n| '''Part:''' 9mm and [[.45 ACP]] suppressor<ref name=\"9mm Carbine Long\"/>\n| Selective laser melting using titanium powder<ref name=knife2/>\n| Unknown NZ$1.2 million printer.<ref name=sup2/>\n| Oceania Defence Ltd.<ref name=knife2/><ref name=sup2/><ref name=\"sup1\"/> and Rapid Advanced Manufacturing<ref name=\"supram\"/>\n| \n*Long version is designed to operate on [[9×19mm Parabellum]] carbines firing fully automatic.<ref name=\"9mm Carbine Long\"/>\n**It fits under the rail of a Samson or similar rail system on the AR-15-type rifle.<ref name=\"9mm Carbine Long\"/>\n**This is a sealed can and is only suitable for use with jacketed bullets.<ref name=\"9mm Carbine Long\"/>\n*Pistol version is designed with a Neilsen which allows it to be used with most of the common John Browning tilting-barrel designs including the swinging-linked [[M1911]] and the cam-lock system operated [[Glock]] pistols. The Neilsen is an assembly in the aft end of the suppressor that allows the gasses to push the suppressor forward while allowing the unimpeded rearward movement of the barrel and slide assembly using a stainless steel spring and titanium piston.<ref name=\"oceania-defence.com\"/>\n**It is able to function well with a wide range of ammo, although its sound reduction performs best with subsonic bullets heavier than 124 grains, with 147 or 158 grain bullets being the quietest.<ref name=\"oceania-defence.com\"/>\n**It is designed to be run wet or dry, averaging 127.7–128.4 dBA with 147 grain or 127.6–134.2 dBA with 124 grain when dry and 123.1 dBA with 147 grain or 129.1 dBA with 124 grain when wet.<ref name=\"oceania-defence.com\"/>\n*It is an additively manufactured titanium weapon suppressor. It is 50% lighter than conventional steel weapons<ref name=knife2/>\n|-\n! style=\"background: pink\" | [[Solid Concepts 1911 DMLS]]<ref name=\"theguardian.com\"/><ref name=\"cnsnews.com\"/>\n| <span style=\"display: none;\" >2013-11</span>November 2013<ref name=\"theguardian.com\"/><ref name=\"cnsnews.com\"/>\n| '''Weapon:''' [[Browning 1911]] handgun<ref name=5gunrefs/><ref name=\"theguardian.com\"/><ref name=\"cnsnews.com\"/>\n| Direct metal laser sintering (DMLS)<ref name=\"theguardian.com\"/><ref name=\"cnsnews.com\"/>\n| EOSINT M270 Direct Metal 3D Printer<ref>[http://www.thetruthaboutguns.com/2013/12/robert-farago/gun-review-solid-concepts-1911-dmls-direct-metal-laser-sintering/ Gun Review: Solid Concepts 1911 DMLS], Truth about guns, December 10, 2013. ([https://www.webcitation.org/6SuwL2cWJ?url=http://www.thetruthaboutguns.com/2013/12/robert-farago/gun-review-solid-concepts-1911-dmls-direct-metal-laser-sintering/ archive])</ref>\n| [[Solid Concepts]]<ref name=5gunrefs/><ref name=\"theguardian.com\"/><ref name=\"cnsnews.com\"/>\n| \n*Fired more than 600 bullets without any damage to the gun.<ref name=\"cnsnews.com\"/>\n*The metal printer used to create the weapon cost between US$500,000 to $1 million at the time the gun was created (November 2013).<ref name=\"cnsnews.com\"/>\n*The gun is made up of 34 3D-printed components.<ref name=\"ReferenceC\"/>\n\n|-\n! style=\"background: #EEEEEE;\" | The Israel [[drum magazine]]<ref name=\"ReferenceB\">[http://www.guns.com/2013/12/23/3d-printed-75-round-drum-mags-meet-israel-yee-photosvideo/ 3D-printed 75-round drum mags: Meet the ‘Israel’ and the ‘Yee’ (Photos + Video)], guns.com, December 23, 2013. ([https://www.webcitation.org/6SRdQcsag?url=http://www.guns.com/2013/12/23/3d-printed-75-round-drum-mags-meet-israel-yee-photosvideo/ archive])</ref><ref name=\"thetruthaboutguns.com\">[http://www.thetruthaboutguns.com/2013/12/foghorn/fosscad-unveils-3d-printable-75-round-drum-magazine-ar-15-rifles/ FOSSCAD Unveils 3D Printable 75 Round Drum Magazine for AR-15 Rifles], the truth about guns, December 22, 2013. ([https://www.webcitation.org/6SRdasMF2?url=http://www.thetruthaboutguns.com/2013/12/foghorn/fosscad-unveils-3d-printable-75-round-drum-magazine-ar-15-rifles/ archive])</ref>\n| <span style=\"display: none;\" >2013-12</span>December 2013<ref name=\"ReferenceB\"/><ref name=\"thetruthaboutguns.com\"/>\n| '''Part:''' [[AR-15]] rifle 75-round [[STANAG magazine|STANAG]] [[drum magazine]]<ref name=\"ReferenceB\"/><ref name=\"thetruthaboutguns.com\"/>\n|\n| Many plastic 3D printers<ref name=\"ReferenceB\"/>\n| FOSSCAD members<ref name=\"ReferenceB\"/><ref name=\"thetruthaboutguns.com\"/>\n| It is a 75-round drum magazine for .223 Remington/5.56 NATO AR-15 rifles. It can be installed into a [[Charon (gun)|Charon]] 3D-printed lower receiver.<ref name=\"ReferenceB\"/><ref name=\"thetruthaboutguns.com\"/>\n\n|-\n! style=\"background: #EEEEEE;\" | The Yee [[drum magazine]]<ref name=\"ReferenceB\" /><ref name=\"thetruthaboutguns.com\" />\n| <span style=\"display: none;\" >2013-12</span>December 2013<ref name=\"ReferenceB\" /><ref name=\"thetruthaboutguns.com\" />\n| '''Part:''' AK-family (e.g. AK-47, AK-102, AK-104 and compatible variants) 75-round [[drum magazine]]<ref name=\"ReferenceB\" /><ref name=\"thetruthaboutguns.com\" />\n| \n|\n| FOSSCAD members<ref name=\"ReferenceB\" /><ref name=\"thetruthaboutguns.com\" />\n| 75-round drum magazine for 7.62×39mm AK-pattern rifles<ref name=\"ReferenceB\" /><ref name=\"thetruthaboutguns.com\" />\n\n|-\n! style=\"background: pink;\" | [[Panavia Tornado|Tornado]] multi-role aircraft spare and repair parts:\n*[[Landing gear]] guard\n*Covers for cockpit radios\n*Air intake door support [[strut]]s<ref name=tornadoyahoo>{{cite web|title=3D-printed components flown in British fighter jet |url=https://uk.finance.yahoo.com/news/3d-printed-components-flown-british-034921661.html |date=5 January 2014 |publisher=Yahoo! News |deadurl=bot: unknown |archiveurl=https://www.webcitation.org/6SXMUDIoL?url=https://uk.finance.yahoo.com/news/3d-printed-components-flown-british-034921661.html |archivedate=12 September 2014 |df= }} ()</ref>\n*[[Power take-off]] shaft protective guards<ref name=tornadoyahoo/>\n| <span style=\"display: none;\" >2014-01</span>January 2014<ref name=tornadoyahoo/>\n| '''Part:''' [[Panavia Tornado|Tornado]] parts<ref name=tornadoyahoo/>\n| [[Direct metal laser sintering]] (DMLS)<ref name=tornado3dexpo>[http://2014.3d-expo.ru/en/british-tested-fighter-printed-parts British tested fighter with \"printed\" parts], 3D-Expo & Lenta.ru, September 6, 2014. Note: [http://lenta.ru/news/2014/01/06/tornado/ original article] in russian, translation provided by 3d-expo.ru. ([https://www.webcitation.org/6SXODxGgS?url=http://2014.3d-expo.ru/en/british-tested-fighter-printed-parts archive english]) ([https://www.webcitation.org/6SXOM9Iy9?url=http://lenta.ru/news/2014/01/06/tornado/ archive russian])</ref>\n|\n| [[BAE Systems]]<ref name=tornadoyahoo/>\n| \n*It is the first fighter jet to fly with 3D printed parts<ref name=tornadoyahoo/>\n*The company claims that with 3D printing some of the parts costing less than £100 per piece to manufacture, resulting in savings of more than £300,000 and will offer further potential cost savings of more than £1.2 million between 2014 and 2017.<ref name=tornadopocket>{{cite web|url=http://www.pocket-lint.com/news/126185-printing-planes-bae-systems-now-using-3d-printed-parts-in-fighter-jets |title=Printing planes: BAE Systems now using 3D printed parts in fighter jets |author=Stuart Miles |publisher=pocket-lint.com |date=5 January 2014 |accessdate=5 January 2014 |archive-url=https://www.webcitation.org/6SXNZ470F?url=http://www.pocket-lint.com/news/126185-printing-planes-bae-systems-now-using-3d-printed-parts-in-fighter-jets |archive-date=12 September 2014 |deadurl=no |df= }}</ref>\n\n|-\n! style=\"background: #EEEEEE;\" | [[Zig zag revolver]]<ref name=5gunrefs/><ref name=\"techcrunch.com\">[https://techcrunch.com/2014/05/08/japanese-man-arrested-for-printing-his-own-revolvers/ Japanese Man Arrested For Printing His Own Revolvers], Tech Crunch, May 8, 2014. ([https://www.webcitation.org/6SRXO2gnh?url=https://techcrunch.com/2014/05/08/japanese-man-arrested-for-printing-his-own-revolvers/ archive])</ref>\n| <span style=\"display: none;\" >2014-05</span>May 2014<ref name=\"techcrunch.com\"/>\n| '''Weapon:''' .38 Caliber [[Revolver]]<ref name=5gunrefs/>\n| Fused deposition modeling (FDM)<ref name=5gunrefs/>\n| Unknown US$500 plastic 3D-printer used<ref name=\"techcrunch.com\"/>\n| Yoshitomo Imura<ref name=\"techcrunch.com\"/>\n| \n*It was named after the German [[Mauser Zig-Zag]] revolver.<ref name=\"wired.co.uk\"/>\n*It holds a capacity of six bullets and can fire .38 caliber bullets.<ref name=\"wired.co.uk\"/>\n*The printer he used cost US$500 at the time.<ref name=\"techcrunch.com\"/>\n\n|-\n! style=\"background: #EEEEEE;\" | [[Hanuman AR-15 Bullpup]]<ref name=\"thefirearmblog.com\">[http://www.thefirearmblog.com/blog/2014/05/27/warfairys-3d-printable-ar-15-bullpup/ WarFairy’s 3D Printable AR-15 Bullpup], firearmblog, May 27, 2014. ([https://www.webcitation.org/6SRaIWcr6?url=http://www.thefirearmblog.com/blog/2014/05/27/warfairys-3d-printable-ar-15-bullpup/ archive])</ref><ref name=\"Guns 2014\">[http://www.guns.com/2014/05/25/check-out-this-3d-printable-bullpup-for-ar-pattern-uppers/ Check out this 3D-printable bullpup for AR-pattern uppers], Guns.com, May 27, 2014. ([https://www.webcitation.org/6SRail3Ac?url=http://www.guns.com/2014/05/25/check-out-this-3d-printable-bullpup-for-ar-pattern-uppers/ archive])</ref>\n| <span style=\"display: none;\" >2014-05</span>May 2014<ref name=\"thefirearmblog.com\"/><ref name=\"Guns 2014\"/>\n| '''Part:''' [[AR-15]] rifle [[bullpup]] lower receiver<ref name=\"thefirearmblog.com\"/><ref name=\"Guns 2014\"/>\n|\n| Many plastic 3D-printers\n| WarFairy<ref name=\"thefirearmblog.com\"/><ref name=\"Guns 2014\"/>\n| \n*According to the creators \"It requires a bufferless upper to function, such as the ARAK-21 or Rock River Arms PDS Carbine, or a regular upper with a CMMG Style .22LR Conversion installed.\"<ref name=\"thefirearmblog.com\"/>\n*It is designed to be printed using ABS plastic.<ref name=\"Guns 2014\"/>\n\n|-\n! style=\"background: #EEEEEE;\" | [[SKS]] grip and stock<ref name=\"wired.co.uk\"/><ref name=\"natocouncil.ca\">[http://natocouncil.ca/a-dark-side-of-3d-printers-printed-guns/ A Dark Side of 3D Printers: Printed Guns], natocouncil, August 30, 2014. ([https://www.webcitation.org/6SRbRH3Ov?url=http://natocouncil.ca/a-dark-side-of-3d-printers-printed-guns/ archive])</ref>\n| <span style=\"display: none;\" >2014-05</span>May 2014<ref name=\"wired.co.uk\"/>\n| '''Part:''' [[SKS]] rifle grip and stock<ref name=\"wired.co.uk\"/>\n|\n| Many plastic 3D-printers\n| FOSSCAD members<ref name=\"wired.co.uk\"/>\n|\n|-\n! style=\"background: #EEEEEE;\" | [[Škorpion vz. 61]] grip and stock<ref name=\"wired.co.uk\"/><ref name=\"natocouncil.ca\"/>\n| <span style=\"display: none;\" >2014-05</span>May 2014<ref name=\"wired.co.uk\"/>\n| '''Part:''' [[Škorpion vz. 61]] sub machinegun grip and stock<ref name=\"wired.co.uk\"/>\n|\n| Many plastic 3D-printers<ref name=\"natocouncil.ca\"/>\n| FOSSCAD members<ref name=\"wired.co.uk\"/>\n|\n|-\n! style=\"background: #EEEEEE;\" | [[Improvised Explosive Device]]\n| <span style=\"display: none;\" >2014-06</span>June 2014<ref name=\"fool.com\">[http://www.fool.com/investing/general/2014/06/28/the-fbis-latest-tool-to-fight-terrorism-a-stratasy.aspx The FBI's Latest Tool To Fight Terrorism: A Stratasys 3-D Printer], the fool, June 28, 2014. ([https://www.webcitation.org/6SReY2hvZ?url=http://www.fool.com/investing/general/2014/06/28/the-fbis-latest-tool-to-fight-terrorism-a-stratasy.aspx archive])</ref><ref name=\"3dprinterworld.com\">[http://www.3dprinterworld.com/article/fbi-use-3d-printing-for-bomb-research FBI to Use 3D Printing for Bomb Research], 3D printer world, June 24, 2014. ([https://www.webcitation.org/6SRefwxm6?url=http://www.3dprinterworld.com/article/fbi-use-3d-printing-for-bomb-research archive])</ref><ref name=\"news.sky.com\">[http://news.sky.com/story/1285534/fbi-looking-into-3d-printed-bomb-threat FBI Looking Into '3D-Printed Bomb' Threat], Sky News UK, 19 June 2014. ([https://www.webcitation.org/6SRemio5T?url=http://news.sky.com/story/1285534/fbi-looking-into-3d-printed-bomb-threat archive])</ref>\n| '''Weapon:''' [[Bomb]]<ref name=\"fool.com\"/><ref name=\"3dprinterworld.com\"/><ref name=\"news.sky.com\"/>\n|\n| Object24 3D-printer<ref name=\"fool.com\"/><ref name=\"3dprinterworld.com\"/><ref name=\"news.sky.com\"/>\n| [[FBI]]<ref name=\"fool.com\"/><ref name=\"3dprinterworld.com\"/><ref name=\"news.sky.com\"/>\n| The FBI is testing IED devices created using 3D printers to determine its feasibility and risk. They will also use it for training purposes. The FBI spokesman Ann Todd said: \"The 3D printer is cutting-edge technology that will be used by the Terrorist Explosive Device Analytical Center to enhance their capabilities in exploiting improvised explosive devices.\"<ref name=\"fool.com\"/><ref name=\"3dprinterworld.com\"/><ref name=\"news.sky.com\"/>\n\n|-\n! style=\"background: #EEEEEE;\" | Rocket\n| <span style=\"display: none;\" >2014-07</span>July 2014.<ref name=\"3ders.org\">[http://www.3ders.org/articles/20140706-university-of-arizona-students-successfully-launch-3d-printed-rocket.html University of Arizona students successfully launch 3D printed rocket], 3ders, July 6, 2014. ([https://www.webcitation.org/6SRg7pXbu?url=http://www.3ders.org/articles/20140706-university-of-arizona-students-successfully-launch-3d-printed-rocket.html archive])</ref>\n| '''Weapon:''' Rocket\n| Fused deposition modeling (FDM)<ref name=\"3ders.org\"/>\n|\n| [[University of Arizona]]<ref name=\"3ders.org\"/>\n| \n*The rocket was made with a very low budget.<ref name=\"3ders.org\"/> The cost was undisclosed.\n*The circuit boards were also 3D printed.<ref name=\"3ders.org\"/>\n*The design produced via 3D printing reduced drag and increased efficiency by 85%.<ref name=\"3ders.org\"/>\n\n|-\n! style=\"background: orange;\" | [[Ruger Charger (3D printed)|Ruger Charger]]<ref name=3dprintrugger>[http://3dprint.com/8398/3d-printed-gun-semi-automatic/ 3D Printed Semi-automatic Ruger Charger Pistol is Assembled and Fired – ‘If you take my gun, I’ll print another!’], 3Dprint, July 4, 2014. ([https://www.webcitation.org/6StSBJoxR?url=http://3dprint.com/8398/3d-printed-gun-semi-automatic/ archive])</ref><ref name=reason>[http://reason.com/blog/2014/07/07/3d-printed-semiautomatic-22-debuts-if-yo 3D-Printed Semiautomatic .22 Debuts. \"If you take my gun, I will simply print another one.\"], Reason, July 2014. ([https://www.webcitation.org/6StQuzZ7S?url=http://reason.com/blog/2014/07/07/3d-printed-semiautomatic-22-debuts-if-yo archive])</ref><ref name=ammolandrugger>[http://www.ammoland.com/2014/07/3d-printed-ruger-charger-style-pistol-by-buck-o-fama/#axzz3EUU0yfrM 3D Printed Ruger Style Pistol Demo by Buck O’ Fama ~ Video], Ammoland, July 4, 2014. ([https://www.webcitation.org/6StRzQuTO?url=http://www.ammoland.com/2014/07/3d-printed-ruger-charger-style-pistol-by-buck-o-fama/ archive])</ref>\n| <span style=\"display: none;\" >2014-07</span>July 2014<ref name=reason/>\n| '''Weapon:''' [[Ruger 10/22]] semi-automatic pistol<ref name=reason/>\n| Fused deposition modeling (FDM)<ref>[http://news.softpedia.com/news/3D-Printing-Lets-Man-Assemble-Ruger-Charger-Pistol-Without-Legal-Paperwork-449881.shtml 3D Printing Lets Man Assemble Ruger Charger Pistol Without Legal Paperwork], Softpedia, July 8, 2014. ([https://www.webcitation.org/6StSe5XZ5?url=http://news.softpedia.com/news/3D-Printing-Lets-Man-Assemble-Ruger-Charger-Pistol-Without-Legal-Paperwork-449881.shtml archive])</ref>\n| Unknown small format 3D printer<ref name=3dprintrugger/>\n| \"Buck-o-Fama\" (pseudonym)<ref name=reason/>\n| \n*It is the  pistol version of the popular [[Ruger 10/22]] rifle, and comes standard with 10-round flush magazines and can  also accept high-capacity magazines including 30 rounds or more.<ref name=3dprintrugger/><ref name=reason/>\n*\"Buck O’Fama\" claims that the receiver was printed using an inexpensive, small format 3D printer, in 2 sections, and then those sections were crazy-glued together.<ref name=3dprintrugger/>\n\n|-\n! style=\"background: orange;\" | Imura Revolver<ref name=3dersimura>[http://www.3ders.org/articles/20140924-new-3d-printed-revolver-dedicated-to-yoshitomo-imura-in-development.html New 3D printed revolver dedicated to Yoshitomo Imura in development], 3Ders, Sep 24 2014. ([https://www.webcitation.org/6SwXcBW29?url=http://www.3ders.org/articles/20140924-new-3d-printed-revolver-dedicated-to-yoshitomo-imura-in-development.html archive])</ref><ref name=techcrunchimurarevolver>[https://techcrunch.com/2014/09/24/designer-builds-a-3d-printable-imura-revolver-in-honor-of-arrested-japanese-maker/ Designer Builds A 3D-Printable \"Imura Revolver\" In Honor Of Arrested Japanese Maker], TechCrunch, Sep 24 2014. ([https://www.webcitation.org/6SwXQGsH4?url=https://techcrunch.com/2014/09/24/designer-builds-a-3d-printable-imura-revolver-in-honor-of-arrested-japanese-maker/ archive])</ref><ref name=3dprintimura>[http://3dprint.com/15556/3d-printable-gun-revolver/ A New 3D Printable Gun, The ‘Imura Revolver’ is Being Designed], September 22, 2014. ([https://www.webcitation.org/6SwYBDDCM?url=http://3dprint.com/15556/3d-printable-gun-revolver/ archive])</ref>\n| <span style=\"display: none;\" >2014-09</span>September 2014<ref name=techcrunchimurarevolver/>\n| '''Weapon:''' .38 Caliber [[Revolver]]<ref name=3dersimura/><ref name=techcrunchimurarevolver/>\n|\n| \n| FOSSCAD members: WarFairy, Frostbyte and others<ref name=3dprintimura/>\n| \n*It was named in honor of Yoshitomo Imura who was arrested by Japanese police for creating the [[Zig zag revolver]].<ref name=3dprintimura/>\n*It is a double-action revolver<ref name=3dprintimura/> and holds six shots<ref>[http://versus.com/en/2014/09/26/5-alarming-3d-printed-guns 5 Alarming 3D Printed Guns], Versus, September 26, 2014. ([https://www.webcitation.org/6SwYQIc69?url=http://versus.com/en/2014/09/26/5-alarming-3d-printed-guns archive])</ref>\n*It is mostly plastic but includes a steel barrel liner and chamber sleeves<ref name=3dprintimura/> to increase tensile strength.<ref>[http://www.thetruthaboutguns.com/2014/09/dean-weingarten/the-next-phase-3d-printed-imura-revolver-hybrid-development/ The Next Phase: 3D Printed Imura Revolver Hybrid Development], Truth about Guns, September 15, 2014. ([https://www.webcitation.org/6SwZTgWRP?url=http://www.thetruthaboutguns.com/2014/09/dean-weingarten/the-next-phase-3d-printed-imura-revolver-hybrid-development/ archive])</ref>\n\n|-\n! style=\"background: #EEEEEE;\" | Wounded soldier raptor<ref name=3dprintraptor>[http://3dprint.com/16976/wounded-solder-prosthetic/ Wounded Soldier’ 3D Printed Prosthetic Hand – Giving Utility to Veterans & Active Adults for Under $100], 3D print, September 29, 2014. ([https://www.webcitation.org/6SyS5O5TE?url=http://3dprint.com/16976/wounded-solder-prosthetic/ archive])</ref>\n| <span style=\"display: none;\" >2014-09</span>September 2014<ref name=3dprintraptor/>\n| '''Other:''' Customisable [[Prosthesis|Prosthetic hand]] including KA-BAR pistol Bayonet and flashlight mount<ref name=3dprintraptor/>\n|\n|\n| Aaron Brown<ref name=3dprintraptor/>\n| \n*It is designed to be easily 3D printed and constructed by anyone, includes documentation and there is no need for the screws.<ref name=3dprintraptor/>\n*It is equipped with a Picatinny rail system, which allows for any tools or accessories to be attached as an add on. The creator said: \"the simplicity of these rails makes the idea of mounting nearly ANYTHING a possibility. It would be very easy to print a bracket for anything from a marker, toothbrush, spoon, paintbrush, and nearly anything we can imagine\"<ref name=3dprintraptor/>\n*The creator Aaron Brown stated he put a mount for a KA-BAR pistol [[bayonet]] \"right into the handle\" and included a flashlight mount<ref name=3dprintraptor/>\n*The device is also colored using a camouflaged color scheme.<ref name=3dprintraptor/>\n*It cost less than $100 to print including the price of the flashlight and bayonet.<ref name=3dprintraptor/>\n\n|-\n! style=\"background: pink\" | Reason ([[M1911 pistol]] chambered in [[10mm Auto]])<ref name=\"3DPrint.com\">{{cite web|url=http://3dprint.com/21109/3d-print-metal-gun-reason/|title=Solid Concepts 3D Prints Another Metal Gun, ‘Reason’, a 10mm Auto 1911|work=3DPrint.com|accessdate=5 December 2014}}</ref>\n| <span style=\"display: none;\" >2013-11</span>October 2014 \n| '''Weapon:''' [[10mm Auto]]\n|  Direct metal laser sintering (DMLS)<ref name=\"TechCrunch\">{{cite web|url=https://techcrunch.com/2014/10/27/solid-concepts-announces-another-3d-printed-metal-gun/|title=Solid Concepts Announces Another 3D-Printed Metal Gun|publisher=AOL|work=TechCrunch|accessdate=5 December 2014}}</ref>\n| EOSINT M280 Direct Metal 3D Printer<ref name=\"ReferenceA\">{{cite web|url=http://www.3ders.org/articles/20141029-the-world-second-3d-printed-metal-gun-revealed-the-reason.html |title=Archived copy |accessdate=2014-11-16 |deadurl=yes |archiveurl=https://web.archive.org/web/20141101031912/http://www.3ders.org/articles/20141029-the-world-second-3d-printed-metal-gun-revealed-the-reason.html |archivedate=2014-11-01 |df= }}</ref>\n| [[Solid Concepts]]<ref name=\"ReferenceA\"/>\n| \n*Created roughly after a year the first known metal 3D printed gun was produced.<ref name=\"3DPrint.com\"/>\n*Has the word \"Reason\" etched on it, along with an excerpt from the [[Declaration of Independence]] on the barrel.<ref name=\"TechCrunch\"/>\n\n|-\n! style=\"background: #EEEEEE;\" | CM901<ref>[https://www.engadget.com/2015/03/26/great-now-3d-printed-rifles-can-fire-7-62mm-nato-rounds/ Great, now 3D-printed rifles can fire larger, deadlier rounds]</ref>\n| <span style=\"display: none;\" >2015-03</span>March 2015\n| '''Weapon:''' 7.62×51mm NATO\n| Fused deposition modeling (FDM)\n| Da Vinci 3D\n| Printed Firearm<ref>{{tr icon}} [http://shiftdelete.net/3d-yazicidan-uretilen-olumcul-tufek-59354 3D Yazıcıdan Üretilen Ölümcül Tüfek!]</ref>\n| \n* Based on [[Colt CM901]].\n\n|-\n! style=\"background: #EEEEEE;\" | Shuty V2<ref>[https://www.youtube.com/watch?v=REGeDjuADXI 3D Printed 9mm Semi-Auto! Shuty V2 Test-Fire!]</ref>\n| <span style=\"display: none;\" >2015-05</span>May 2015\n| '''Weapon:''' [[9×19mm Parabellum]]\n| Fused deposition modeling (FDM)\n| Fusion3 F306\n| Derwood\n| \n* [[Semi-automatic pistol]]\n\n|-\n! style=\"background: #EEEEEE;\" | Washbear<ref>{{cite web|url=http://www.jamesrpatrick.com/2015/09/washbear1.html|title=PM522 Washbear 3D Printed .22 Revolver Concept |date=September 5, 2015|accessdate=5 January 2016}}</ref>\n| <span style=\"display: none;\" >2015-09</span>September 2015\n| '''Weapon:''' [[Pepper-box]] .22LR revolver\n| Fused deposition modeling (FDM)\n| [[RepRap]]\n| James R. Patrick\n| \n\n|-\n! style=\"background: #EEEEEE;\" | XPR-1<ref>{{cite web|url=http://bgr.com/2015/10/19/handheld-railgun-video-3d-printing/|title=3D printing used to make first real handheld railgun, which fires plasma projectiles at 560 mph |date=October 19, 2015|accessdate=5 August 2016}}</ref>\n| <span style=\"display: none;\" >2015-10</span>October 2015\n| '''Weapon:''' Plasma armature [[Railgun]]\n| Fused deposition modeling (FDM)\n| [[Delta robot|Kossel]]\n| David Wirth<ref>{{cite web|url=https://www.engadget.com/2015/10/19/3d-printed-handheld-railgun/|title=Guy creates handheld railgun with a 3D-printer |date=October 19, 2015|accessdate=5 August 2016}}</ref>\n| \n* First 3D printed railgun structure\n* Closed source - designs and code released to research institutions only\n* Employed a mixture of traditional and additive manufacturing.  Rails were traditionally machined, as well as the fasteners and electronics.\n\n|-\n! style=\"background: #EEEEEE;\" | Shuty MP-1<ref>[https://3dprint.com/118279/shuty-mp-1-semi-automatic/ The Shuty MP-1 is the Latest 3D Printed Working Semi-Automatic Handgun | 3DPrint.com]</ref>\n| <span style=\"display: none;\" >2016-01</span>January 2016\n| '''Weapon:''' [[9×19mm Parabellum]]<ref name=Shuty>[https://www.youtube.com/watch?v=Kjco5lhddqQ Shuty MP-1 3d-printed 9mm semiauto!]</ref>\n| Fused deposition modeling (FDM)\n| Fusion3 F306<ref name=Shuty></ref>\n| Derwood\n| \n* [[Semi-automatic pistol]]\n\n|-\n! style=\"background: #EEEEEE;\" | Shuty WTF-9<ref>[https://www.youtube.com/watch?v=kn9V1enlBxY 3D Printed Shuty WTF-9 Experimental semi-auto 9mm]</ref>\n| <span style=\"display: none;\" >2017-04</span>April 2017\n| '''Weapon:''' [[9×19mm Parabellum]]\n| Fused deposition modeling (FDM)\n| Fusion3 F306\n| Derwood\n| \n* [[Semi-automatic pistol]]\n\n|-\n! style=\"background: #EEEEEE;\" | Shuty AP-9<ref>[https://www.youtube.com/watch?v=6bSLiFx36lk 3d Printed Shuty AP- 9 AR-15 9mm pistol] </ref>\n| <span style=\"display: none;\" >2017-04</span>April 2017\n| '''Weapon:''' [[9×19mm Parabellum]]\n| Fused deposition modeling (FDM)\n| Fusion3 F306\n| Derwood\n| \n* [[Semi-automatic pistol]]\n\n|-\n! style=\"background: #EEEEEE;\" | EMG-01A<ref>[https://www.youtube.com/watch?v=9ZlOHUYGBGk Full-Auto 3D Printed Coilgun] </ref>\n| <span style=\"display: none;\" >2018-07</span>July 2018\n| '''Weapon:''' [[Coilgun]]\n| Fused deposition modeling (FDM)\n| Unknown 3D printer\n| [[Arcflash Labs]]\n| \n* First commercial handheld coilgun<ref>{{cite web|url=https://hackaday.com/2018/07/12/you-can-now-buy-a-practical-gauss-gun/|title=You Can Now Buy a Practical Gauss Gun|date=July 12, 2018|accessdate=March 6, 2019}}</ref>\n|}\n\n== See also ==\n* [[3D printing]]\n* [[3D printed firearms]]\n* [[Defense Distributed]]\n* [[Gun control]]\n* [[Gun politics in the United States]]\n* [[Improvised firearm]]\n\n== References ==\n{{reflist|colwidth=35em}}\n\n[[Category:3D printed firearms| ]]\n[[Category:Weapon development]]\n[[Category:Homemade firearms]]\n[[Category:Lists of firearms|3D printed]]"
    },
    {
      "title": "3DX",
      "url": "https://en.wikipedia.org/wiki/3DX",
      "text": "{{refimprove|date=December 2014}}\nThe '''3DX''', also known as the '''Auxetik''', was the first 3D printed metal [[muzzle brake]] and the first 3D printed metallic component for a firearm.<ref name=\"gizmag.com\">[http://www.gizmag.com/sintercore-auxetik-3d-printed-muzzle-brake-inconel/28489/ Sintercore creates first commercial 3D printed metallic firearm component], Gizmag, July 30, 2013. ([https://www.webcitation.org/6SRU7dqAy archive])</ref><ref name=\"Sintercore 3DX Muzzle Brake\">[http://www.thefirearmblog.com/blog/2014/08/26/sintercore-3dx-muzzle-brake/ Sintercore 3DX Muzzle Brake], thefirearmblog, August 26, 2014.([https://www.webcitation.org/6SRV0XN9X archive])</ref> It is meant for the highly customisable [[AR-15]] rifle. The design was made public around July 2013.<ref name=\"gizmag.com\"/> The printer used to print it is unknown but the brake was created using the [[Direct metal laser sintering]] (DMLS) method<ref name=\"gizmag.com\"/> by [[Sintercore]] (a weapons manufacturing startup company).<ref name=\"gizmag.com\"/> It is  designed to tame the recoil and muzzle rise of AR-15 pistols chambered for .223 caliber (5.56×45mm) NATO rounds.<ref name=\"gizmag.com\"/> The Auxetik was renamed to 3DX by Sintercore.<ref name=\"Sintercore 3DX Muzzle Brake\"/>\n\nIt uses metal [[Inconel]] material <ref name=\"archive\">[http://www.guns.com/2014/02/20/quick-review-sintercore-auxetik-3d-printed-inconel-brake-video/ Quick review of the Sintercore Auxetik 3D-printed Inconel brake (VIDEO)], Guns.com, February 20, 2014. ([https://www.webcitation.org/6StIx5g91 archive])</ref> and as of 2014 the company was selling each one for around $300.<ref name=\"archive\"/>\n\n==Specifications==\n[[File:Sintercore 3DX no coating.jpg|thumb|right|Early Sintercore 3DX (without Ionbond Diamondblack coating added in later iterations).]]\nThe 3DX is the first 3D printed muzzle brake available for commercial sale with muzzle control on semi-automatic and fully automatic. It uses 100% Inconel superalloy construction and Ionbond Diamondblack coating. The threading is 1/2×28RH for [[5.56×45mm NATO]], [[.223 Remington]], and smaller calibers.  The brake comes with installation instructions and a crush washer.<ref name=\"Sintercore 3DX Muzzle Brake\"/>\n\n==Performance==\nIt survived the firing of 7900 rounds during testing on semi-auto.<ref name=\"Sintercore 3DX Muzzle Brake\"/> During a test on full auto, 10 magazines of 62 grain green tip 5.56 rounds were all fired without any issues.<ref name=\"Sintercore 3DX Muzzle Brake\"/>\n\nThefirearmblog tested the item on the first, sixth, and 12th month of their experiment, then checked the interior using a [[USB microscope]]. They claim that there was no \"discernible difference in performance over that year, there was no muzzle rise to speak of and no increase in report when firing\".<ref name=\"Sintercore 3DX Muzzle Brake\"/>\n\n==United States military interest==\nUS Special Operation Command’s ([[USSOCOM]]) Science and Technology Directorate at MacDill Air Force Base invited Neal Brace (the owner of Sintercore<ref>[http=\"https://www.linkedin.com/in/neal-brace-872a2b49\"]</ref>) to demonstrate the newly named 3DX muzzle brake for possible use by its elite troops using an \"ARES Defense AMG-1 and AMG-2 belt fed machine gun with a 13-inch barrel feeding from a 200-round box magazine\". Testing was done on August the 5th 2014 by special operations troops on a closed range.  All the testing was carried out with the ARES AMG-1 and 2.\n\n==See also==\n*[[List of notable 3D printed weapons and parts]]\n\n==References==\n{{Reflist}}\n\n[[Category:3D printed firearms]]\n[[Category:Firearm components]]"
    },
    {
      "title": "AR Lower V5",
      "url": "https://en.wikipedia.org/wiki/AR_Lower_V5",
      "text": "'''The AR Lower V5''' is a 3D printed [[lower receiver]] for the [[AR-15]] rifle.<ref name=TC600>{{cite news\n |last=Biggs \n |first=John \n |title=Defense Distributed Prints An AR-15 Receiver That Has Fired More Than 600 Rounds \n |url=https://techcrunch.com/2013/03/01/defense-distributed-prints-an-ar-15-receiver-that-has-fired-more-than-600-rounds/ \n |accessdate=April 12, 2013 \n |journal=[[TechCrunch]] \n |date=March 1, 2013 \n |deadurl=yes \n |archiveurl=https://www.webcitation.org/6SQVl1PlV?url=http://techcrunch.com/2013/03/01/defense-distributed-prints-an-ar-15-receiver-that-has-fired-more-than-600-rounds/ \n |archivedate=September 8, 2014 \n |df= \n}}</ref> It was created in March 2013 [[Defense Distributed]] printed using the [[Stratasys]] Dimension SST 3-D printer<ref name=TC600 /><ref name=\"Printed AR Lower v5 Review\">[http://defdist.tumblr.com/post/44209819568/printed-ar-lower-v5-review Printed AR Lower v5 Review], Defense Distributed official tumblr blog ([https://www.webcitation.org/6SQWPktGE?url=http://defdist.tumblr.com/post/44209819568/printed-ar-lower-v5-review archive])</ref> using the [[fused deposition modeling]] (FDM) method.<ref name=\"Printed AR Lower v5 Review\"/>\n\nThe receiver was able to handle enough stress to fire more than 600 rounds.<ref name=TC600 /> Defense Distributed stated \"actual count of the new SLA lower was 660+ on day 1 with the SLA lower. The test ended when we ran out of ammunition, but this lower could easily withstand 1,000 rounds.\"<ref>[http://www.3ders.org/articles/20130228-3rd-generation-of-3d-printed-ar-lowers-ready-for-download.html 3D printed AR lowers holds up 600+ rounds, file ready for download], 3Ders, Feb 28 2013. ([https://www.webcitation.org/6SsDS4yBZ?url=http://www.3ders.org/articles/20130228-3rd-generation-of-3d-printed-ar-lowers-ready-for-download.html archive])</ref>\n\n==See also==\n*[[List of notable 3D printed weapons and parts]]\n\n==References==\n{{Reflist}}\n\n[[Category:3D printed firearms]]\n[[Category:Firearm components]]\n[[Category:AR Rifle Components]]\n[[Category:Fused filament fabrication]]\n[[Category:ArmaLite AR-10 derivatives]]\n\n==External links==\n*[https://github.com/maduce/fosscad-repo/tree/master/Rifles/AR-15_DefDist_Lower_Receiver_v5 AR-15_DefDist_Lower_Receiver_v5]"
    },
    {
      "title": "Charon (gun)",
      "url": "https://en.wikipedia.org/wiki/Charon_%28gun%29",
      "text": "[[File:Charon Line of 3D Printable AR-15 Lower Receivers by Shanrilivan.jpg|thumb|An image of the DEFCAD Charon AR-15 family.]]\n\nThe [[DEFCAD]] '''Charon''' is an open source<ref>Shanrilivan, [http://defcad.com/forums/showthread.php?655-WarFairy-Charon-V0-2B-released!&p=3686&viewfull=1#post3686 \"WarFairy Charon V0.2B released!′\"], 29 Apr 2013.</ref> [[3D printing|3D-printable]] [[AR-15]] [[lower receiver]] project that was partially inspired by the [[Fabrique Nationale P90]]. It began as a design exercise by a DEFCAD user to explore [[Fused deposition modeling|FDM]] [[additive manufacturing]] technology as a means of integrating the P90's ergonomics into a stock for the [[AR-15]], resulting in the [[WarFairy P-15]] stock set.<ref>Leghorn, Nick, [http://www.thetruthaboutguns.com/2013/05/foghorn/coming-soon-3d-printable-ar-15-lower-with-p-90-style-stock/ \"Coming Soon: 3D Printable AR-15 Lower with P-90 Style Stock\"], 21 May 2013.</ref><ref>Slowik,Max, [http://www.guns.com/2013/06/03/meet-the-charon-family-of-3d-printable-ar-lowers-photos/ Meet the Charon Family of 3D-Printable AR Lowers (PHOTOS) \"Meet the Charon Family of 3D-Printable AR Lowers (PHOTOS)′\"], 3 June 2013.</ref><ref>Slowik,Max, [http://www.guns.com/2013/07/01/3d-printing-community-updates-liberator-with-rifle-pepperbox-and-glock-powered-shuty-9 \"3D Printing Community Updates Liberator with Rifle, Pepperbox and Glock-Powered ‘Shuty-9′\"], 1 July 2013.</ref>\n\nThe additive manufacturing process permits curvilinear designs that are too expensive or impossible to do by subtractive methods.<ref>Johnson,Steve, [http://www.thefirearmblog.com/blog/2013/05/30/warfairys-latest-additions-charon-line-printable-ar-15s/ \"WarFairy’s Latest Additions To The Charon Line Of Printable AR-15s′\"], 30 May 2013.</ref> From the P-15, it was integrated with the DefDist V5 lower receiver, resulting in the WarFairy Charon V0.1. Further advantage was taken of the nature of [[Fused deposition modeling|FDM]] polymer printing to make it possible for printers with small work envelopes to produce large items, i.e., the Charon was designed to be printed in sections and then assembled with solvent cement.\n\nThe Charon pistol version was successfully test fired in July 2013 with no sign of damage.  {{Citation needed|date=December 2016}}\n\n==References==\n{{reflist}}\n\n[[Category:3D printed firearms]]\n[[Category:Firearm components]]\n[[Category:Fused filament fabrication]]\n\n{{firearms-stub}}\n\n==External links==\n*[https://github.com/maduce/fosscad-repo/tree/master/Rifles/AR-15_WarFairy_Charon_Lower_Receiver_v4_Warfairy AR-15_WarFairy_Charon_Lower_Receiver_v4_Warfairy]"
    },
    {
      "title": "DEFCAD",
      "url": "https://en.wikipedia.org/wiki/DEFCAD",
      "text": "{{Infobox company\n| name      =  DEFCAD, Inc\n| logo      =  [[File:DEFCAD logo.png|logo of DEFCAD, Inc.|200px]]\n| type      =  [[Private company|Private]]\n| foundation        =  [[Austin, TX]] (2013)\n| founder           =  [[Cody Wilson]]\n| location          =  [[Austin, TX]]\n| industry          =  [[Internet]], [[3D Printing]]  \n| products          =  See \"Products and Solutions\"\n| homepage          =  [http://www.defcad.com/ defcad.com]\n}}\n\n'''DEFCAD, Inc.''' is an Austin-based startup that has created a [[search engine]] and [[web portal]] for designers and hobbyists to find and develop [[3D printing|3D printable]] and other [[Computer-aided design|CAD]] models online launched by [[Defense Distributed]].\n\n==History==\n\n===Founding===\n\nWhen [[Makerbot| Makerbot Industries]]' removed [[3D printed firearms|firearms-related]] 3D Printable files at the public repository [[Thingiverse]] in December 2012,<ref name=\"WiredThing\">{{cite web|url=https://www.wired.com/design/2012/12/thingiverse-removes-gun-parts/|title=Thingiverse Removes (Most) Printable Gun Parts|first=Tim|last=Maly|publisher=[[Wired (magazine)|Wired]]|date=December 19, 2012|accessdate=January 14, 2013}}</ref><ref name=\"BBCthing\">{{cite web|url=https://www.bbc.co.uk/news/technology-20797207|title=MakerBot pulls 3D gun-parts blueprints after Sandy Hook|publisher=[[BBC News]]|date=December 20, 2012|accessdate=January 14, 2013}}</ref><ref name=\"CNNM\">{{cite web|url=http://money.cnn.com/2012/12/20/technology/makerbot-gun-parts/|title=3-D printer MakerBot cracks down on blueprints for gun parts|first=Julianne|last=Pepitone|publisher=[[CNN Money]]|date=December 20, 2012|accessdate=January 14, 2013}}</ref> the open source software firm [[Defense Distributed]] launched DEFCAD as a companion site to publicly host the removed files and its own.<ref name=\"Giz2\">{{cite web|url=https://gizmodo.com/5970483/theres-a-new-site-just-for-3d+printed-gun-designs|title=There’s a New Site Just for 3D-Printed Gun Designs|first=Eric|last=Limer|publisher=[[Gizmodo]]|date=December 21, 2012|accessdate=January 14, 2013}}</ref><ref name=\"VB1\">{{cite web|url=https://venturebeat.com/2012/12/21/defcad-gun-3d-printing-censorship/|title=Fighting ‘censorship,’ 3D-printed gun designs find a new home|first=Ricardo|last=Bilton|publisher=[[VentureBeat]]|date=December 21, 2012|accessdate=January 14, 2013}}</ref><ref name=\"verge1\">{{cite web|url=https://www.theverge.com/2012/12/21/3791616/defense-distributed-launches-3d-printed-gun-part-site|title=3D printed gun enthusiasts build site for firearm files after MakerBot crackdown|first=Adi|last=Robertson|publisher=[[The Verge]]|date=December 21, 2012|accessdate=January 14, 2013}}</ref>\nPublic and community submissions to DEFCAD rose quickly,<ref name=VBDC2>{{cite news|last=Bilton|first=Ricardo|title=3D-printing gun site DEFCAD now attracting 3K visitors an hour, 250K downloads since launch |url=https://venturebeat.com/2013/02/19/defcad-gun-traffic-growing/|accessdate=February 19, 2013|journal=[[VentureBeat]]|date=August 3, 2013}}</ref> and in March 2013, at the [[SXSW]] Interactive festival, [[Cody Wilson]] announced a repurposed and expanded site and company that would serve as a 3D search engine and development hub.<ref>{{cite news|last=Greenberg|first=Andy|title=3D-Printable Gun Project Announces Plans For A For-Profit Search Engine Startup|url=https://www.forbes.com/sites/andygreenberg/2013/03/11/3d-printable-gun-makers-announce-plans-for-a-for-profit-search-engine-startup/|accessdate=April 12, 2013|journal=[[Forbes|Forbes Online]]|date=March 11, 2013}}</ref><ref>{{cite news|last=Farivar|first=Cyrus|title=3D printing gunmaker forms company to flout copyright law, à la the Pirate Bay|url=https://arstechnica.com/tech-policy/2013/03/3d-printing-gunmaker-forms-company-to-flout-copyright-law-a-la-the-pirate-bay/|accessdate=April 12, 2013|journal=[[Ars Technica]]|date=March 11, 2013}}</ref><ref name=VBDC3/>\n\nDEFCAD has been called \"The Pirate Bay of 3D Printing\"<ref>{{cite news|title='Pirate Bay' for 3D printing launched|url=https://www.bbc.co.uk/news/technology-21754915|accessdate=April 12, 2013|journal=[[BBC News]]|date=March 12, 2013}}</ref> and \"the anti-Makerbot\".<ref name=VBDC3>{{cite news|last=Bilton|first=Ricardo|title=Expanding beyond 3D printed guns, DEFCAD is officially the anti-MakerBot|url=https://venturebeat.com/2013/03/11/defcad-anti-makerbot/|accessdate=April 12, 2013|journal=[[VentureBeat]]|date=March 11, 2013}}</ref>\n\n==Products and Solutions==\n\n===Community===\n\nDEFCAD began as a repository where users could upload and download CAD models, but quickly became a community with the addition of an [[Internet Relay Chat|IRC]] channel and public forums. The site has over 2,500 community users and offers access to over 100,000 models.<ref>{{cite web|url=http://forums.defcad.com/|title=DEFCAD Forums|date=August 3, 2013}}</ref>\n\n===Search===\n\nIn August 2013, DEFCAD released the public alpha of its 3D search engine which indexes public object repositories and allows users to add their own objects. \n\n== See also ==\n* [[3D printed firearms]]\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://defcad.com DEFCAD Website]\n* [http://defcad.com/forums DEFCAD Forums]\n* [http://webchat.oftc.net/?randomnick=0&channels=defcad&prompt=1&uio=d4 DEFCAD IRC]\n\n[[Category:3D printed firearms]]\n[[Category:Weapon development]]\n[[Category:Online companies]]\n[[Category:Free software companies]]"
    },
    {
      "title": "Feinstein AK Mag",
      "url": "https://en.wikipedia.org/wiki/Feinstein_AK_Mag",
      "text": "{{notability|date=April 2016}}\nThe '''Feinstein AK Mag''' is a [[3D print]]ed magazine for the [[AK-47]] rifle.<ref name=BizIns2>{{cite news\n |last=Ingersoll \n |first=Geoffrey \n |title=3D Printing Company Names AK-47 Magazine After Gun Control Congresswoman \n |url=http://www.businessinsider.com/defense-distributed-feinstein-ak-mag-2013-3 \n |accessdate=April 12, 2013 \n |journal=[[Business Insider]] \n |date=March 8, 2013 \n |deadurl=bot: unknown \n |archiveurl=https://www.webcitation.org/6SRO3NSdI?url=http://www.businessinsider.com/defense-distributed-feinstein-ak-mag-2013-3 \n |archivedate=September 8, 2014 \n |df= \n}} ()</ref><ref name=FAB1>{{cite web\n |last=Branson \n |first=Michael \n |title=Defense Distributed Releases Printable AK Magazine \n |url=http://www.thefirearmblog.com/blog/2013/04/08/defense-distributed-releases-printable-ak-magazine/ \n |accessdate=April 12, 2013 \n |journal=The Firearm Blog \n |date=April 8, 2013 \n |deadurl=bot: unknown \n |archiveurl=https://www.webcitation.org/6SiJDDXnV?url=http://www.thefirearmblog.com/blog/2013/04/08/defense-distributed-releases-printable-ak-magazine/ \n |archivedate=September 19, 2014 \n |df= \n}}. ()</ref> It was created by [[Defense Distributed]] and made public on March 2013.<ref name=\"BizIns2\" /><ref name=\"FAB1\" /> The magazine was created using a [[Stratasys]] Dimension SST 3-D printer via the [[fused deposition modeling]] (FDM) method.<ref name=\"archive\" >[https://www.forbes.com/sites/andygreenberg/2013/05/08/3d-printed-guns-blueprints-downloaded-100000-times-in-two-days-with-some-help-from-kim-dotcom/ 3D-Printed Gun's Blueprints Downloaded 100,000 Times In Two Days (With Some Help From Kim Dotcom)], forbes.com, August 5, 2013. ([https://www.webcitation.org/6SQZHYIw5?url=http://www.forbes.com/sites/andygreenberg/2013/05/08/3d-printed-guns-blueprints-downloaded-100000-times-in-two-days-with-some-help-from-kim-dotcom/ archive])</ref>\n\nIt is a 30-round 7.62×39 AK-47 magazine.<ref name=\"FAB1\"/> It is named after the anti-gun congresswoman [[Dianne Feinstein]].<ref name=\"BizIns2\"/><ref name=huffein>[http://www.huffingtonpost.com/2013/03/08/feinstein-ak-magazine_n_2838366.html Defense Distributed's 'Feinstein AK Magazine' Named After Gun Control Advocate Sen. Dianne Feinstein], Huffington Post, August 8, 2013. ([https://www.webcitation.org/6SiJzWMmz?url=http://www.huffingtonpost.com/2013/03/08/feinstein-ak-magazine_n_2838366.html archive])</ref> [[Cody Wilson]] (founder of Defense Distributed), said the magazine’s name is a symbol of what’s happening in congress and reflects Defense Distributed's belief the proposed ban on assault weapons by Dianne Feinstein will fail.<ref name=\"huffein\"/> The original prototype was able to withstand 60 rounds before it began to crack.<ref>[http://www.networkworld.com/article/2164302/smb/at-sxsw--3d-printed-guns-get-their-turn-in-the-spotlight--too.html SXSW 3D printed guns get their spotlight], Network World, March 11, 2013. ([https://www.webcitation.org/6SiKiuEVA?url=http://www.networkworld.com/article/2164302/smb/at-sxsw--3d-printed-guns-get-their-turn-in-the-spotlight--too.html archive])</ref>\n\n==See also==\n* [[List of notable 3D printed weapons and parts]]\n\n==References==\n{{Reflist}}\n\n\n\n[[Category:3D printed firearms]]\n[[Category:Magazines (firearms)]]\n[[Category:Fused filament fabrication]]\n\n==External links==\n*[https://github.com/maduce/fosscad-repo/tree/master/Rifles/AK-47_DefDist_Feinstein_magazine_v1.0 AK-47_DefDist_Feinstein_magazine_v1.0]"
    },
    {
      "title": "Ghost gun",
      "url": "https://en.wikipedia.org/wiki/Ghost_gun",
      "text": "{{Use mdy dates|date=November 2017}}\n{{Use American English|date=November 2017}}\n\nA '''ghost gun''' is a [[firearm]] without [[Serial code|serial numbers]]. The term is used by [[gun control]] advocates, [[Gun politics in the United States|gun rights]] advocates,  law enforcement, and some in the firearm industry.<ref name=\"Greenberg\" /><ref name=\"Steele\" /><ref name=\"Melendez\" /><ref name=\"Lee\" /> By making the gun themselves, owners may legally bypass [[background check]]s and [[Firearms license|registration regulations]].<ref name=\"Greenberg\" /><ref name=\"NatGeo\" /> Under [[Law of the United States|U.S. federal law]], the creation and possession of ghost guns is allowed, but a license is required to manufacture firearms for sale or distribution.<ref>{{Cite web|url=https://www.atf.gov/firearms/qa/does-individual-need-license-make-firearm-personal-use|title=Does an individual need a license to make a firearm for personal use? {{!}} Bureau of Alcohol, Tobacco, Firearms and Explosives|website=www.atf.gov|language=en|access-date=October 3, 2017|deadurl=no|archiveurl=https://web.archive.org/web/20171001122915/https://www.atf.gov/firearms/qa/does-individual-need-license-make-firearm-personal-use|archivedate=October 1, 2017|df=mdy-all}}</ref>\n\n== Production ==\n\nThe [[lower receiver]], which in the United States is the only part legally considered a \"gun\" (other components may be unregulated), can be completed from an \"80% receiver\", one which is 80% completed before being sold legally without background or identity checks. The remaining 20% of the work may then be done using common [[drill press]] or [[Dremel]] machine tools or available hand tools.<ref name=\"Stanton\"/><ref name=\"Blackman\"/> Companies sell kits including drill bits, stencils, and jigs to aid the process, but some proficiency with the equipment may be required.<ref name=\"Greenberg\"/> More recently it has been possible to produce the receiver from scratch using plastic or more durable metal in a [[3D printer]], though the variety of materials and methods can create challenges in ensuring the resulting receiver is suitable for use.<ref name=\"Greenberg\"/> More recently [[Defense Distributed]] introduced a [[Numerical control|CNC]] mill called the \"Ghost Gunner\" which is optimized for the purpose of carving the lower receiver from an aluminium unfinished receiver.<ref name=\"Greenberg\"/>\n\nSome ghost guns are [[AR-15]] style firearms.<ref name=\"Greenberg\"/> AR-15s are modular firearms and the serial number is applied to the [[lower receiver]], which holds the trigger group.<ref name=\"Greenberg\"/> Once an individual has an AR-15 lower receiver, they can assemble a complete weapon using widely available components, such as barrels, stocks, magazines, and upper receivers.<ref name=\"Greenberg\"/> Other ghost guns include pistols and AK-47 style semi-automatic rifles.<ref name=\"CBS\"/> The Philippines is a center of ghost gun production, especially .45 caliber [[semi-automatic pistol]]s.<ref name=\"NatGeo\"/>\n\n== Political issues ==\nDue to their lack of serial numbers, tracing ghost guns used in crimes is much harder than tracing serialized weapons.<ref name=\"Stanton\" /> There are no manufacturer or sales records to check.<ref name=\"Horwitz\" /> The difficulty means local law enforcement officers often do not even attempt traces of ghost guns.<ref name=\"Horwitz\" />\n\nCalifornia, especially Sacramento, has been a hub of ghost gun production.<ref name=\"CBS2\" /> The ATF speculated in 2014 that there are tens of thousands of ghost guns in California alone.<ref name=\"Horwitz\" /> Four noted crimes in California were committed with ghost guns: a murder-suicide involving college students in Walnut Creek, a shootout between hostage-taking bank robbers and Stockton police officers, a [[2013 Santa Monica shooting|mass shooting at Santa Monica College]] in 2013 by a student who was prohibited from owning a gun, and a [[Rancho Tehama Reserve shootings|shooting spree at Rancho Tehama Reserve]] in 2017 by a man who was served a restraining order that barred him from possessing guns.<ref name=\"Melendez\" /><ref name=\"Hurd2\" /><ref name=\"Hurd\" /><ref name=\"CBS-News2\">{{cite news |url = https://www.cbsnews.com/news/ghost-guns-shooting-rancho-tehama-california/ |title = Shooting rampage in California highlights \"ghost guns\" and their dangers |work = [[CBS News]] |date = November 16, 2017 |accessdate = November 16, 2017 |deadurl = no |archiveurl = https://web.archive.org/web/20171117073353/https://www.cbsnews.com/news/ghost-guns-shooting-rancho-tehama-california/ |archivedate = November 17, 2017 |df = mdy-all }}</ref> \n\nProponents of ghost guns include gun rights activists and anarchists.<ref name=\"Moody\" /> They say that making weapons is the right of every American which maintains the privacy of gun owners.<ref name=\"Horwitz\" /> Individuals have organized \"build parties\" where equipment and expertise are shared to help create ghost guns.  Advocates say that ghost guns are used in crime rarely despite widespread ownership.<ref name=\"Hurd\" /> Gun rights advocates and law enforcement say that, because of the cost and effort needed to create ghost guns, criminals are more likely to use commercial weapons instead.<ref name=\"Hurd2\" />\n\nTwo U.S. shipping companies, Federal Express and [[United Parcel Service]], have refused to transport Ghost Gunner branded [[computer numerical control]] (CNC) milling machines.<ref name=\"Greenberg2\" />\n\n== Legal issues ==\n=== United States federal law ===\n\nUnder U.S. federal law owning a ghost gun is allowed, assuming that no other impediments exist.<ref name=\"Greenberg\" /> The U.S. [[Bureau of Alcohol, Tobacco, Firearms and Explosives]] (ATF) officials characterize this as a loophole.<ref name=\"Horwitz\" /> The [[United States Department of State|U.S. State Department]] has sued to take computer files to control 3D-printers off the internet under the grounds their publication constituted export of a munition under the [[International Traffic in Arms Regulations]].<ref name=\"Moody\" />  With a legal case pending United States Supreme Court action,<ref>{{Cite news|url=https://reason.com/blog/2016/11/07/defense-distributed-of-3d-printed-gun-fa|title=Defense Distributed, of 3D-Printed Gun Fame, Requests Rehearing on Denial of Its Injunction Against the State Department for Crushing Its Free Speech Rights|date=2016-11-07|work=Reason.com|access-date=2017-08-14|language=en}}</ref><ref>{{Cite news|url=https://reason.com/blog/2017/08/14/supreme-court-asked-can-a-court-arbitrar|title=Can a Court Arbitrarily Conclude That 'Security' Overrules the First Amendment?|date=2017-08-14|work=Reason.com|access-date=2017-08-14|language=en}}</ref> [[Defense Distributed]] removed the files, but the censored blueprints remain accessible via [[The Pirate Bay]]'s \"Physibles\" section.<ref>{{cite web|url=https://torrentfreak.com/pirate-bay-takes-over-distribution-of-censored-3d-printable-gun-130510/|title=\nPirate Bay Takes Over Distribution of Censored 3D Printable Gun|author=Ernesto|date=2013-05-10|publisher=TorrentFreak}}</ref><ref>{{cite web|url=https://thepiratebay.org/browse/605/1/3|title=Physibles|publisher=The Pirate Bay}}</ref>  The U.S. [[Federal Bureau of Investigation]] (FBI) reported in 2013 that it had seized hundreds of ghost guns, including a machine gun,<ref Name=\"CBS\" /> and unregistered [[Suppressor|silencers]].<ref name=\"Stanton\" /> The FBI does not generally track the use of homemade firearms.<ref name=\"Hurd2\" /> ATF agents say that ghost guns are sold at a $1,000 premium due to being untraceable.<ref name=\"Luery\" /> According to the FBI, the popularity of ghost guns grew following the [[Sandy Hook Elementary School shooting]] in 2012, which sparked fears of new gun control measures.<ref name=\"Stanton\" />\n\nIn a 2014 raid of Ares Armor, the [[Bureau of Alcohol, Tobacco, and Firearms]] confiscated 6,000 receiver blanks which they said were too close to finished units.<ref name=\"Horwitz\" />  After a lawsuit, all but 18 of the seized guns were returned and placed for sale to purchasers in 47 states.<ref name=\"Eger2\">{{cite web|url=http://www.guns.com/2017/01/30/controversial-once-seized-80-percent-lowers-now-up-for-grabs-video/|title=Controversial once-seized 80 percent lowers now up for grabs (VIDEO)|date=2017-01-30|publisher=Guns.com|author=Chris Eger}}</ref>  In a similar case, EG Armory of California was raided, but agreed to forfeit 3800 lower receivers without admission of any wrongdoing.<ref name=\"Eger2\"/>  In Sacramento the owner of C&G Tool Inc. pled guilty to illegal manufacture of firearms.  Prosecutors argued that he \"advertised his shop as a place where people could make guns in 20 minutes by pressing a few buttons on a computerized machine\", rejecting his position that buyers created their own guns.<ref Name=\"CBS\" />\n\n=== California state law ===\nIn 2014, California attempted to enact a law to require serial numbers on unfinished receivers and all other firearms, including antique guns,<ref name=\"Eger\" /> but it was vetoed by the governor.<ref>{{Cite web |url = https://www.calffl.org/press-releases/california-governor-jerry-brown-vetoes-ghost-gun-ban-signs-three-gun-control-bills/ |title = California Governor Jerry Brown Vetoes “Ghost Gun” Ban, Signs Three Other Gun Control Bills |website = www.calffl.org |language = en-US |access-date = October 1, 2017 |deadurl = no |archiveurl = https://web.archive.org/web/20171001122132/https://www.calffl.org/press-releases/california-governor-jerry-brown-vetoes-ghost-gun-ban-signs-three-gun-control-bills/ |archivedate = October 1, 2017 |df = mdy-all }}</ref> However, in 2016, it passed a measure requiring anyone planning to build a homemade firearm to obtain a serial number from the state (''de facto'' registration) and pass a background check.<ref name=\"Reuters\" />\n\n=== New York State law ===\nIn 2015, during the state of New York's first prosecution for sale of ghost guns, Attorney General [[Eric Schneiderman]] said it is \"easy\" for \"criminals to make completely untraceable, military-grade firearms.\"<ref name=\"Bolger\" />\n\n=== New Jersey State law ===\n\nS-2465 enacted in 11/2018 prevents the manufacturing, and purchasing of guns or parts that is or can become an untraceable firearm.  Multiple arrests were made within months of this law going into effect.  Attorney General Gurbir Grewal has been aggressively prosecuting infractions of this law.  New Jersey has filed a lawsuit against U.S. Patriot Armory a company that has allegedly sold AR-15 build kits to NJ residents.\n\n=== Connecticut State law ===\nAs of October 1, 2019, all manufactured guns will be required to have a serial number obtained from the [[Connecticut Department of Emergency Services and Public Protection|Department of Emergency Services and Public Protection]] engraved. [[3D printed firearms|Plastic, undetectable guns]] are also banned.<ref>{{Cite web|url=https://www.cga.ct.gov/asp/cgabillstatus/cgabillstatus.asp?selBillType=Bill&which_year=2019&bill_num=7219|title=Connecticut General Assembly|last=|first=|date=|website=Connecticut General Assembly|language=en-US|archive-url=|archive-date=|dead-url=|access-date=2019-06-11}}</ref>\n\n== Further reading ==\n\n* Beyer, Katherine E. ''[http://heinonline.org/HOL/LandingPage?handle=hein.journals/kentlj103&div=25&id=&page= Busting the Ghost Guns A Technical, Statutory, and Practical Approach to the 3-D Printed Weapon Problem]'' 103 Ky. L.J. 433 (2014–2015)\n\n== See also ==\n*[[3D printed firearms]]\n*[[Gun control]]\n*[[Gun politics in the United States]]\n*[[Improvised firearm]]\n*[[List of notable 3D printed weapons and parts]]\n*[[Right to keep and bear arms]]\n\n== References ==\n\n{{reflist|30em|refs=\n<ref name=\"Greenberg\">{{cite news |title = I Made an Untraceable AR-15 Ghost Gun in My Office And It Was Easy |first1 = Andy |last1 = Greenberg |work = Wired |date = June 3, 2015 |url = https://www.wired.com/2015/06/i-made-an-untraceable-ar-15-ghost-gun/ |accessdate = October 17, 2016 |deadurl = no |archiveurl = https://web.archive.org/web/20161020040839/https://www.wired.com/2015/06/i-made-an-untraceable-ar-15-ghost-gun/ |archivedate = October 20, 2016 |df = mdy-all }}</ref>\n\n<ref name=\"Greenberg2\">{{cite news |url = https://www.wired.com/2015/02/fedex-mill-untraceable-firearms/ |title = FedEx And UPS Refuse to Ship a Digital Mill That Can Make Untraceable Guns |work = Wired |first1 = Andy |last1 = Greenberg |accessdate = October 17, 2016 |deadurl = no |archiveurl = https://web.archive.org/web/20161020040103/https://www.wired.com/2015/02/fedex-mill-untraceable-firearms/ |archivedate = October 20, 2016 |df = mdy-all }}</ref>\n\n<ref name=\"Stanton\">{{cite news |title = California black market surges for ghost guns |date = December 19, 2015 |author1 = Sam Stanton |author2 = Denny Walsh |work = Sacramento Bee |accessdate = }}</ref>\n\n<ref name=\"Eger\">{{cite news |title = California Ghost Gun Bill creeps onto governor's desk |last1 = Eger |first1 = Chris |date = September 14, 2014 |work = Guns.com |url = http://www.guns.com/2014/09/02/california-ghost-gun-bill-creeps-onto-governors-desk/ |accessdate = October 17, 2016 |deadurl = no |archiveurl = https://web.archive.org/web/20161020041524/http://www.guns.com/2014/09/02/california-ghost-gun-bill-creeps-onto-governors-desk/ |archivedate = October 20, 2016 |df = mdy-all }}</ref>\n\n<ref name=\"NatGeo\">{{cite web |url = http://channel.nationalgeographic.com/underworld-inc/episodes/ghost-guns/ |title = GHOST GUNS |work = National Geographic |deadurl = no |archiveurl = https://web.archive.org/web/20161020104533/http://channel.nationalgeographic.com/underworld-inc/episodes/ghost-guns/ |archivedate = October 20, 2016 |df = mdy-all }}</ref>\n\n<ref name=\"Reuters\">{{cite news |url = https://www.reuters.com/article/us-california-ghostguns-idUSKCN1022MB |title = California governor signs bill to require registration of 'ghost guns' |work = Reuters |date = July 23, 2016 |first1 = Alex |last1 = Dobuzinski |accessdate = October 17, 2016 |deadurl = no |archiveurl = https://web.archive.org/web/20161020103945/http://www.reuters.com/article/us-california-ghostguns-idUSKCN1022MB |archivedate = October 20, 2016 |df = mdy-all }}</ref>\n\n<ref name=\"Horwitz\">{{cite news |title = Unfinished receivers, a gun part that is sold separately, lets some get around the law |first1 = Sara |last1 = Horwitz |date = May 13, 2014 |url = https://www.washingtonpost.com/world/national-security/unfinished-receivers-that-can-be-used-to-build-guns-pose-problems-for-law-enforcement/2014/05/13/8ec39e9e-da51-11e3-bda1-9b46b2066796_story.html?postshare=6861473266526376&tid=ss_tw |accessdate = October 17, 2016 |work = Washington Post |deadurl = no |archiveurl = https://web.archive.org/web/20161020041216/https://www.washingtonpost.com/world/national-security/unfinished-receivers-that-can-be-used-to-build-guns-pose-problems-for-law-enforcement/2014/05/13/8ec39e9e-da51-11e3-bda1-9b46b2066796_story.html?postshare=6861473266526376&tid=ss_tw |archivedate = October 20, 2016 |df = mdy-all }}</ref>\n\n<ref name=\"Moody\">{{cite news |title = Anarchist will supply kit to build your own assault rifle |last1 = Moody |first1 = Oliver |work = The Times [London (UK)] |date = June 25, 2016 }}</ref>\n\n<ref name=\"Hurd\">{{cite news |title = Police Eye 'Ghost Gun' In Recent Slaying: With The Rise Of Homemade Firearms, Legislation Sought To Make It Easier To Trace Them |last1 = Hurd |first1 = Rick |work = San Jose Mercury News |date = August 7, 2015 |page = A1 }}</ref>\n\n<ref name=\"Steele\">{{cite news |title = 'Ghost Gunner' Makes Untraceable Guns Using a PC |last1 = Steele |first1 = Chandra |work = PCmag.com |date = October 1, 2014 }}</ref>\n\n<ref Name=\"CBS\">{{cite news |title = Illegal Firearm Maker Dr. Death Helped Create Untraceable Ghost Guns |url = http://sanfrancisco.cbslocal.com/2016/05/19/illegal-firearm-maker-doctor-death-helped-creates-untraceable-ghost-guns/ |work = CBS SF Bay Area |date = May 19, 2016 |accessdate = October 17, 2016 |deadurl = no |archiveurl = https://web.archive.org/web/20161020103924/http://sanfrancisco.cbslocal.com/2016/05/19/illegal-firearm-maker-doctor-death-helped-creates-untraceable-ghost-guns/ |archivedate = October 20, 2016 |df = mdy-all }}</ref>\n\n<ref name=\"Blackman\">{{cite news |title = The 1st Amendment, 2nd Amendment, And 3d Printed Guns |first1 = Josh |last1 = Blackman |work = 81 Tennessee Law Review 479 (2014) |date = June 14, 2014 |page = 511 |ssrn = 2450663 }}</ref>\n\n<ref name=\"Bolger\">{{cite news |title = Long Island Trio Charged in NY's First Ghost Gun Bust |first1 = Timothy |last1 = Bolger |date = June 26, 2015 |work = Long Island Press |url = https://www.longislandpress.com/2015/06/26/long-island-trio-charged-in-nys-first-ghost-gun-bust/ |accessdate = October 17, 2016 |deadurl = no |archiveurl = https://web.archive.org/web/20161020103908/https://www.longislandpress.com/2015/06/26/long-island-trio-charged-in-nys-first-ghost-gun-bust/ |archivedate = October 20, 2016 |df = mdy-all }}</ref>\n\n<ref name=\"Luery \">{{cite news |title = 8 Northern California men indicted for making 'ghost guns': More than 230 illegal guns seized by federal agents |first1 = Mike |last1 = Luery |date = October 15, 2015 |url = http://www.kcra.com/news/8-northern-california-men-indicted-for-making-ghost-guns/35868668 |work = KCRA |accessdate = October 17, 2016 |deadurl = no |archiveurl = https://web.archive.org/web/20161001180513/http://www.kcra.com/news/8-northern-california-men-indicted-for-making-ghost-guns/35868668 |archivedate = October 1, 2016 |df = mdy-all }}</ref>\n\n<ref name=\"Melendez\">{{cite news |title = Walnut Creek Police Say 'Ghost Gun' Used In Murder-Suicide |first1 = Lyanne |last1 = Melendez |date = August 4, 2015 |url = http://abc7news.com/news/walnut-creek-police-say-ghost-gun-used-in-murder-suicide/903250/ |work = KGO-TV San Francisco, ABC News |deadurl = no |archiveurl = https://web.archive.org/web/20161020043254/http://abc7news.com/news/walnut-creek-police-say-ghost-gun-used-in-murder-suicide/903250/ |archivedate = October 20, 2016 |df = mdy-all }}</ref>\n\n<ref name=\"Lee\">{{cite news |title = Walnut Creek murder-suicide suspect used \"ghost guns,\" police say |url = http://www.sfgate.com/bayarea/article/Cops-Walnut-Creek-murder-suicide-suspect-used-6424702.php |first1 = Henry K. |last1 = Lee |date = August 4, 2015 |work = SFGATE.COM |accessdate = October 17, 2016 |deadurl = no |archiveurl = https://web.archive.org/web/20161020103857/http://www.sfgate.com/bayarea/article/Cops-Walnut-Creek-murder-suicide-suspect-used-6424702.php |archivedate = October 20, 2016 |df = mdy-all }}</ref>\n\n<ref name=\"Hurd2\">{{cite news |url = http://www.mercurynews.com/2015/08/06/homemade-gun-in-stanford-students-murder-suicide-spurs-question-on-ghost-guns/ |title = Homemade gun in Stanford student’s murder-suicide spurs question on 'ghost guns' |work = The Mercury News |last1 = Hurd |first1 = Rick |date = August 12, 2016 |accessdate = October 17, 2016 |deadurl = no |archiveurl = https://web.archive.org/web/20161020041043/http://www.mercurynews.com/2015/08/06/homemade-gun-in-stanford-students-murder-suicide-spurs-question-on-ghost-guns/ |archivedate = October 20, 2016 |df = mdy-all }}</ref>\n\n<ref name=\"CBS2\">{{cite news |title = Sacramento At Center Of Untraceable 'Ghost Gun' Surge |url = http://sacramento.cbslocal.com/2016/05/16/sacramento-at-center-of-untraceable-ghost-gun-surge/ |date = May 16, 2016 |work = CBS Sacramento |accessdate = October 17, 2016 |deadurl = no |archiveurl = https://web.archive.org/web/20161020040412/http://sacramento.cbslocal.com/2016/05/16/sacramento-at-center-of-untraceable-ghost-gun-surge/ |archivedate = October 20, 2016 |df = mdy-all }}</ref>\n\n}}\n\n[[Category:3D printed firearms]]\n[[Category:Firearm construction]]\n[[Category:Homemade firearms]]"
    },
    {
      "title": "Grizzly (.22-caliber rifle)",
      "url": "https://en.wikipedia.org/wiki/Grizzly_%28.22-caliber_rifle%29",
      "text": "The '''Grizzly''' and the Grizzly 2.0 is a 3D printed .22-caliber rifle created around August 2013.<ref name=\"Gun-advocates-expect-flood-3D-printed-guns-result-U-S-law-changes--Canadian-designed-plastic-rifle-successfully-fires-14-rounds 2013\">[http://www.dailymail.co.uk/news/article-2387624/Gun-advocates-expect-flood-3D-printed-guns-result-U-S-law-changes--Canadian-designed-plastic-rifle-successfully-fires-14-rounds.html 3D printed plastic rifle successfully fires 14 rounds - as gun advocates predict it will force changes in the law], Daily Mail, 9 August 2013. ([https://www.webcitation.org/6SRSn0LE9 archive])</ref> It was created using a [[Stratasys]] Dimension 1200es printer.<ref name=\"nbcnews.com\"/> It was created by a Canadian only known by the pseudo name \"Matthew\" and told The Verge that he is in his late 20s, and his main job is making tools for the construction industry.<ref name=\"nbcnews.com\">[http://www.nbcnews.com/tech/innovation/first-3-d-printed-rifle-fires-bullet-then-breaks-f8C10752930 First 3-D printed rifle fires bullet, then breaks], NBC News, July 26, 2013. ([https://www.webcitation.org/6SRSLyjdT archive])</ref><ref name=\"theverge.com\">[https://www.theverge.com/2013/8/4/4588162/worlds-first-3d-printed-rifle-the-grizzly-updated World's first 3D-printed rifle gets update, fires 14 shots], The Verge, August 4, 2013.([https://www.webcitation.org/6SRScZvop archive])</ref>\n\nThe original Grizzly fired 1 shot then broke<ref name=\"nbcnews.com\"/> Grizzly 2.0 fired 14 bullets before getting damaged due to the strain.<ref name=\"theverge.com\"/> According to the [[Daily Mail]], the Grizzly 2.0 performed so well that the inventor \"Matthew\" was able to put it to his shoulder and shot off three rounds with the rifle pressed against his cheek without hurting him.<ref name=\"Gun-advocates-expect-flood-3D-printed-guns-result-U-S-law-changes--Canadian-designed-plastic-rifle-successfully-fires-14-rounds 2013\"/>\n\n==Specification==\nThe “Grizzly 2.0″ has 50 percent larger barrel with a rifled bore, a larger receiver and new hammer.<ref>[http://www.theblaze.com/stories/2013/08/05/3d-printed-rifle-shoots-14-rounds-before-breaking/ 3D-Printed Rifle Shoots 14 Rounds Before Breaking], The Blaze, August 5, 2013. ([https://www.webcitation.org/6SXi40h4V archive])</ref> In a video posted online by the creator, the Grizzly seemed to have successfully fired a Winchester Dynapoint .22-caliber bullet, like the [[Liberator (gun)|Liberator .380]]. The only metal in the Grizzly is a 1-inch roofing nail plus whatever metal is in the cartridge.<ref>[http://mashable.com/2013/07/25/3d-printed-rifle/ World's Possibly First 3D-Printed Rifle Is Fired on YouTube], Mashable, August 2013. ([https://www.webcitation.org/6SXi2H2ry archive])</ref>\n\n==Printer==\nThe printer used to make the rifle was a [[Stratasys]] Dimension 1200es printer<ref name=\"nbcnews.com\"/> costing $10,000 as of August 2013.<ref name=\"Gun-advocates-expect-flood-3D-printed-guns-result-U-S-law-changes--Canadian-designed-plastic-rifle-successfully-fires-14-rounds 2013\"/> It was made using [[ABS plastic]].<ref name=\"theverge.com\"/>\n\n\"Matthew\" said the rifle had taken 3 days to build, and about 27 hours to print it. He revealed it took him 13 hours to print the receiver, 6.5 hours to print the barrel, 5 hours to print the stock and 2 hours to print the rifle's internal parts.<ref name=\"nbcnews.com\"/>\n\n==See also==\n*[[List of notable 3D printed weapons and parts]]\n\n==References==\n{{reflist}}\n\n\n\n[[Category:.22 LR rifles]]\n[[Category:3D printed firearms]]\n\n==External links==\n*[https://github.com/maduce/fosscad-repo/tree/master/Firearms/Grizzly_Handgun-Canadiangunnut Grizzly_Handgun-Canadiangunnut]"
    },
    {
      "title": "Hanuman AR-15 Bullpup",
      "url": "https://en.wikipedia.org/wiki/Hanuman_AR-15_Bullpup",
      "text": "The '''Hanuman AR-15 Bullpup''' <ref name=\"thefirearmblog.com\">[http://www.thefirearmblog.com/blog/2014/05/27/warfairys-3d-printable-ar-15-bullpup/ WarFairy’s 3D Printable AR-15 Bullpup], firearmblog, May 27, 2014. ([https://www.webcitation.org/6SRaIWcr6 archive])</ref><ref name=\"Guns 2014\">[http://www.guns.com/2014/05/25/check-out-this-3d-printable-bullpup-for-ar-pattern-uppers/ Check out this 3D-printable bullpup for AR-pattern uppers], Guns.com, May 27, 2014. ([https://www.webcitation.org/6SRail3Ac archive])</ref>\nwhich was made public in May 2014 <ref name=\"thefirearmblog.com\"/><ref name=\"Guns 2014\"/> is a prototype [[AR-15]] rifle [[Bullpup]] <ref name=\"thefirearmblog.com\"/><ref name=\"Guns 2014\"/> created by WarFairy<ref name=\"thefirearmblog.com\"/><ref name=\"Guns 2014\"/>\n\nAccording to the creators \"It requires a bufferless upper to function, such as the ARAK-21 or Rock River Arms PDS Carbine, or a regular upper with a CMMG Style .22LR Conversion installed.\"<ref name=\"thefirearmblog.com\"/> Portions of the finished rifle are designed to be printed using ABS plastic.<ref name=\"Guns 2014\"/>\n\n==Specification==\nThe configuration at the top is with a SBR upper with a length of 20 inches with the configuration below consisting of a 16 inch upper with a length of 26.5 inches.<ref name=\"thefirearmblog.com\"/> The initial a prototype lacks a safety and adjustable outside of interchangeable butt pads, but the creators say \"it should work without issue\".<ref name=\"thefirearmblog.com\"/>\n\nThe lower receiver weighs 1.45 pounds when created using [[ABS plastic]] material, the creators estimate the \"material cost will be between 20 and 40 dollars depending on the cost of your material\".<ref name=\"thefirearmblog.com\"/>\n\nWarFairy stated that the later versions will include a safety. Like the [[Charon (gun)|Charon]], the Hanuman’s components were designed and meant to be printed individually, to allow those with small 3D printers to print the device.<ref name=\"archive\">[http://www.outdoorhub.com/news/2014/05/27/designs-3d-printable-ar-15-bullpup-lowers-released/ Designs for 3D-printable AR-15 Bullpup Lower Released], Outdoor hub, May 27, 2014. ([https://www.webcitation.org/6SyBjR3LH archive])</ref>\n\n==Name==\nThe creators decided to name the lower receiver after the Hindu deity [[Hanuman]]. The CAD files needed to print the Hanuman were made available at defcad.com, a website created by [[Cody Wilson]] founder of [[Defense Distributed]].<ref name=\"archive\"/>\n\n==Test fire==\nWarFairy claims that test receivers have fired over 600 rounds without breaking and still function, and that it showed no signs of stress.<ref name=\"archive\"/>\n\n==See also==\n*[[List of notable 3D printed weapons and parts]]\n\n==References==\n{{Reflist}}\n\n\n\n[[Category:3D printed firearms]]\n[[Category:Trial and research firearms]]\n\n==External links==\n*[https://github.com/maduce/fosscad-repo/tree/master/Rifles/AR-15_Hanuman_Bullpup_v1.1-WarFairy AR-15_Hanuman_Bullpup_v1.1-WarFairy]"
    },
    {
      "title": "Liberator (gun)",
      "url": "https://en.wikipedia.org/wiki/Liberator_%28gun%29",
      "text": "{{About||the World War II single shot pistol|FP-45 Liberator|the shotgun|Winchester Liberator}}\n{{Infobox weapon\n|name=Liberator .380\n|image=File:DD Liberator.png\n|caption=\n|origin={{flag|United States}}\n|type=[[Single-shot]] [[handgun|pistol]]\n<!-- Type selection -->\n|is_ranged=yes\n<!-- Service history -->\n|service=\n|used_by=\n|wars=\n<!-- Production history -->\n|designer=[[Defense Distributed]]\n|design_date=April 2013{{Citation needed|date=September 2018}}\n|manufacturer=\n|unit_cost=\n|production_date=2013&ndash;present<ref name=\"greenberg\"/>\n|number=\n|variants=\n<!-- General specifications -->\n|spec_label=\n|weight=\n|length=216 mm (8.5 in)\n|part_length=64 mm (2.5 in)\n|width=\n|height=160 mm (6.3 in)\n<!-- Ranged weapon specifications -->\n|cartridge=[[.380 ACP]]\n|action=Single-shot\n|rate=\n|velocity=\n|range=\n|max_range=\n|feed= \n|sights=\n}}\n\n'''The Liberator''' is a [[wikt:physible|physible]], [[3D printing|3D-printable]] single shot  [[handgun]], the first such printable [[firearm]] design made widely available online.<ref>{{cite web|title=US government orders removal of Defcad 3D-gun designs|url=https://www.bbc.co.uk/news/technology-22478310|work=BBC News|accessdate=13 May 2013}}</ref><ref>{{cite web|last=Biggs|first=John|title=What You Need To Know About The Liberator 3D-Printed Pistol|url=https://techcrunch.com/2013/05/06/what-you-need-to-know-about-the-liberator-3d-printed-pistol/|work=TechCrunch|accessdate=13 May 2013}}</ref><ref>{{cite web|last=Hutchinson|first=Lee|title=The first entirely 3D-printed handgun is here|url=https://arstechnica.com/gadgets/2013/05/the-first-entirely-3d-printed-handgun-is-here/|work=Ars Technica|accessdate=13 May 2013}}</ref> The [[Open-source hardware|open source]] firm [[Defense Distributed]] designed the gun and released the plans on the Internet on May 6, 2013. The plans were downloaded over 100,000 times in the two days before the [[United States Department of State]] demanded that Defense Distributed retract the plans.<ref name=\"greenberg\">{{cite web|last=Greenberg|first=Andy|title=3D-Printed Gun's Blueprints Downloaded 100,000 Times In Two Days (With Some Help From Kim Dotcom)|url=https://www.forbes.com/sites/andygreenberg/2013/05/08/3d-printed-guns-blueprints-downloaded-100000-times-in-two-days-with-some-help-from-kim-dotcom/|work=Forbes|accessdate=13 May 2013}}</ref>\n\nThe plans for the gun remain hosted across the Internet and are available at [[file sharing]] websites like [[The Pirate Bay]]<ref>{{cite web|title=Defiant Pirate Bay to continue hosting banned 3D printer gun designs|url=http://rt.com/news/liberator-gun-defcad-pirate-bay-122/|work=RT.com|date=10 May 2013|accessdate=4 August 2013}}</ref><ref name=\"TFreak1\">{{cite web|last=Ernesto|title=Pirate Bay Takes Over Distribution of Censored 3D Printable Gun|url=https://torrentfreak.com/pirate-bay-takes-over-distribution-of-censored-3d-printable-gun-130510/|work=TorrentFreak|accessdate=13 May 2013}}</ref> and [[GitHub]].<ref>{{cite web|title=The 3D printed gun scare never actualized|url=https://theoutline.com/post/963/3d-printed-gun-scare|date=27 January 2017|access-date=February 25, 2018}}</ref>\n\nOn July 19, 2018 the [[United States Department of Justice]] reached a settlement with [[Defense Distributed]], allowing the sale of plans for 3D-printed firearms online, beginning August 1, 2018.<ref>{{cite web|last=Uria|first=Daniel|title=Justice Department settlement allows sale of 3-D printed gun plans|url=https://www.upi.com/Top_News/US/2018/07/19/Justice-Department-settlement-allows-sale-of-3-D-printed-gun-plans/4651532046827/|accessdate=July 20, 2018}}</ref> \n\nOn July 31, 2018 President of the United States [[Donald Trump]] posted on [[Twitter]] about the decision to allow the online publication of the Liberator's files: “I am looking into 3-D Plastic Guns being sold to the public. Already spoke to [[NRA]], doesn’t seem to make much sense!”.<ref>{{cite web|title=Trump queries 3D printed guns – which administration helped make available to public|url=https://www.theguardian.com/us-news/2018/jul/31/3d-guns-trump-nra-blueprints-tweet-confusion}}</ref>\n\nOn the same day the tweet was posted, a federal judge stopped the release of blueprints to make the Liberator due to it being an untraceable and undetectable 3D-printed plastic gun, citing safety concerns.<ref>{{cite web |title=Judge blocks the release of blueprints for 3D-printed guns |url=https://www.cnbc.com/2018/08/01/judge-blocks-release-of-blueprints-for-3d-printed-guns.html |website=CNBC |accessdate=4 August 2018 |date=1 August 2018}}</ref>\n\n==Namesake and concept==\n\nThe pistol is named after the [[FP-45 Liberator]], a single-shot pistol that [[George Hyde (gun designer)|George Hyde]] designed and that the Inland Manufacturing Division of the [[General Motors|General Motors Corporation]] mass-produced for the United States [[Office of Strategic Services]] (OSS) in [[World War II]]. The OSS intended to air drop the gun into occupied Europe for [[Resistance during World War II|resistance forces]] to use.<ref name=\"LPHagan\">{{cite book|last = Hagan|first = Ralph| title = The Liberator Pistol| publisher = Target Sales| year = 1996| isbn = 978-0965449632}}</ref><ref name=\"OSSe\">{{cite book|last = Melton|first = H.| title =OSS Special Weapons & Equipment| publisher = Sterling Pub Co Inc.| year = 1991| isbn =978-0806982380}}</ref><ref name=\"Forbes1\">{{cite web|last=Greenberg|first=Andy|title=Meet The 'Liberator': Test-Firing The World's First Fully 3D-Printed Gun|url=https://www.forbes.com/sites/andygreenberg/2013/05/05/meet-the-liberator-test-firing-the-worlds-first-fully-3d-printed-gun/|work=Forbes|accessdate=13 May 2013}}</ref> A project of the OSS (which would later become the [[CIA]]), it is thought the Liberator was equally purposed as a tool of [[psychological warfare]].<ref name=\"LPHagan\"/> Occupying forces in Europe would have to weigh evidence of distributed pistols as a factor in planning against civilian resistance, which would complicate their strategy and affect morale. However, though used in France, there is little proof that the pistols were ever dropped into occupied Europe in large quantities.<ref name=\"LPHagan\"/>\n\nThe [[wikt:physible|physible]] Liberator's release to the Internet can be understood as Defense Distributed's attempt to more successfully execute the historical psychological operation, and as a symbolic act supporting resistance to world governments.<ref name=\"Forbes1\"/><ref>{{cite web|last=Slowik|first=Max|title=3D Printing Community Updates Liberator with Rifle, Pepperbox and Glock-Powered ‘Shuty-9′|url=http://www.guns.com/2013/07/01/3d-printing-community-updates-liberator-with-rifle-pepperbox-and-glock-powered-shuty-9/|work=Guns.com|accessdate=28 July 2013}}</ref>\n\n==Withdrawal of plans and The Pirate Bay hosting==\n[[File:DDLiberator2.3.jpg|thumb|left|Digital Liberator pistol by [[Defense Distributed]]]]\n\nDays after their publication, the [[United States Department of State]]'s Office of Defense Trade Controls issued a letter to Defense Distributed demanding that it retract the Liberator plans from public availability.<ref>{{cite web|last=Greenberg|first=Andy|title=State Department Demands Takedown Of 3D-Printable Gun Files For Possible Export Control Violations|url=https://www.forbes.com/sites/andygreenberg/2013/05/09/state-department-demands-takedown-of-3d-printable-gun-for-possible-export-control-violation|work=Forbes|accessdate=13 May 2013}}</ref> The State Department justified this demand by asserting the right to regulate the flow of technical data related to arms, and its role in enforcing the [[Arms Export Control Act]] of 1976.\n\nHowever, soon thereafter the design appeared on [[The Pirate Bay]] (TPB), which publicly stated its defense of the information. Quoted on [[TorrentFreak]]: \"TPB has for close to 10 years been operating without taking down one single torrent due to pressure from the outside. And it will never start doing that.\"<ref name=\"TFreak1\"/>\n\nThe site would go on to issue a statement on its Facebook page:\n{{quote|text=So apparently there are some 3D prints of guns in the physibles section at TPB. Prints that the US government now claim ownership of. Our position is, as always, to not delete any torrents as long as its contents are as stated in the torrents description. Printable guns [are] a very serious matter that will be up for debate for a long time from now. We don't condone gun violence. We believe that the world needs less guns, not more of them. We believe however that these prints will stay on the internets regardless of blocks and censorship, since that's how the internets works. If there's a lunatic out there who wants to print guns to kill people, he or she will do it. With or without TPB. Better to have these prints out in the open internets (TPB) and up for peer review (the comment threads), than semi hidden in the darker parts of the internet.|sign=''The Pirate Bay'', May 10, 2013}}\n\n==Reception==\n\nOriginal copies of the Liberator have been permanently acquired by the [[Victoria and Albert Museum]],<ref name=\"dezeen\">{{cite web|url=http://www.dezeen.com/2013/09/15/va-museum-acquires-first-3d-printed-gun/|title=V&A museum acquires first 3D-printed gun|publisher=[[dezeen]]|date=September 15, 2013|accessdate=December 31, 2013}}</ref><ref name=\"BBCVA\">{{cite web|url=https://www.bbc.co.uk/news/entertainment-arts-24099293|title=V&A museum to display printed gun|publisher=[[BBC Online]]|date=September 15, 2013|accessdate=December 31, 2013}}</ref><ref name=\"NYTVA\">{{cite news |last=Lee |first=Felicia |date=September 16, 2013 |title=3-D Printed Gun Goes on Display at London Museum |url=http://artsbeat.blogs.nytimes.com/2013/09/16/3-d-printed-gun-goes-on-display-at-london-museum/ |newspaper=[[The New York Times]]|accessdate=December 31, 2013 }}</ref> and a copy of the gun is on display at London's [[The Science Museum|Science Museum]].\n\nWriting in ''[[The Register]]'', Lewis Page ridiculed the Liberator, stating \"it isn't any more a gun than any other very short piece of plastic pipe is a \"gun\"\", and comparing it with a 1950s [[improvised firearm|zip gun]].<ref name=\"REG\">{{cite web|last1=Page|first1=Lewis|title='Liberator': Proof that you CAN'T make a working gun in a 3D printer|url=https://www.theregister.co.uk/2013/05/10/oh_no_its_the_plastic_3d_gun/|website=The Register|date=May 10, 2013|accessdate=12 July 2017}}</ref>\n\n==Usage history==\n[[File:ATF test of 3-D printed firearm using ABS material (Side View).webm|thumb|right|[[Bureau of Alcohol, Tobacco, Firearms and Explosives|ATF]] test of 3-D printed firearm using ABS material]]\n[[File:ATF test of 3-D printed firearm using VisiJet material (Side View).webm|thumb|right|ATF test of 3-D printed firearm using VisiJet material]]\n\nIn May 2013, Finnish [[Yle TV2]] current affairs programme ''[[Ajankohtainen kakkonen]]'' produced a Liberator handgun under the supervision of a licensed gunsmith and fired it under controlled conditions. During the experiment, the weapon shattered.<ref>{{cite web| url = http://yle.fi/uutiset/toimiko_3d-pistooli_katso_video/6641646 | title = Toimiko 3D-pistooli? Katso video | last = Richt | first = Jyrki | publisher = Yle Uutiset | date = 2013-05-14 | accessdate = 2015-09-19 | language = Finnish }}</ref><ref>{{cite web| url= http://yle.fi/uutiset/liberator_3d-printed_handgun_fails_after_single_shot_in_finnish_test/6643536 | title = 'Liberator' 3D-printed handgun fails after single shot in Finnish test | date = 2013-05-15 | accessdate=2015-09-19 | last = Richt | first = Jyrki | website = Yle Uutiset}}</ref>\n\nIsraeli [[Channel 10 (Israel)|Channel 10]] reporters built and tested a Liberator with a 9&nbsp;mm cartridge, successfully hitting a target at a distance of several meters. On June 24, 2013, the reporters smuggled the gun (without barrel and ammunition) into the Israeli [[Knesset|house of parliament]], coming within a short distance of Israeli PM [[Benjamin Netanyahu]].<ref>{{Cite video| title = תחקיר חדשות 10: אקדח יורה מטרים ספורים מראש הממשלה| accessdate = 2013-07-03| url = http://news.nana10.co.il/Article/?ArticleID=988880}}</ref>\n\nA Japanese man built five copies of the Liberator, and on or about April 12, 2014, he uploaded video evidence of his possession of the weapons to the internet. Authorities arrested him on May 8, 2014, and found that at least two of the copies possessed lethal power.<ref>{{Cite web| title = Man busted for possessing handguns made with 3-D printer| accessdate = 2014-05-09| url = http://www.asahi.com/english/articles/AJ201405090053.html}}</ref> [[Cody Wilson]], a founder of [[Defense Distributed]], stated on the incident that the man \"performed his work in the open, without suspicion, fear or dishonor\".<ref>{{Cite web| title = WikiWep DevBlog| accessdate = 2014-05-09| url = http://defdist.tumblr.com/post/85127166199/i-have-often-been-asked-who-the-first-person-to-be}}</ref>\n\n==See also==\n* [[List of notable 3D printed weapons and parts]]\n\n==References==\n\n{{Reflist}}\n\n==External links==\n{{commons category|Defense Distributed}}\n{{wikisourcecat|Defense Distributed}}\n* [https://web.archive.org/web/20130423212120/http://defensedistributed.com/ Defense Distributed's official website]\n* [http://defcad.org/ DEFCAD]\n* [http://www.defdist.tumblr.com/ The Wiki Weapon development blog]\n* [https://twitter.com/defcadIRC Defcad IRC channel twitter account]\n* [https://github.com/maduce/fosscad-repo/tree/master/Firearms/Liberators/ Fosscad Liberator repository]\n{{Portal bar|Anarchism|Criminal justice|Freedom of speech|Human rights|Internet|Law|Libertarianism|Texas|Austin|University of Texas at Austin|Arkansas|United States|War}}\n\n[[Category:Personal weapons]]\n[[Category:Handguns]]\n[[Category:3D printed firearms]]\n[[Category:Weapons and ammunition introduced in 2013]]\n[[Category:Fused filament fabrication]]"
    }
  ]
}