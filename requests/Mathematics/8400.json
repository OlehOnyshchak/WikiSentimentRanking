{
  "pages": [
    {
      "title": "Minimum-cost flow problem",
      "url": "https://en.wikipedia.org/wiki/Minimum-cost_flow_problem",
      "text": "The '''minimum-cost flow problem''' ('''MCFP''') is an [[optimization]] and [[decision problem]] to find the cheapest possible way of sending a certain amount of flow through a [[flow network]]. A typical application of this problem involves finding the best delivery route from a factory to a warehouse where the road network has some capacity and cost associated. The minimum cost flow problem is one of the most fundamental among all flow and circulation problems because most other such problems can be cast as a minimum cost flow problem and also that it can be solved efficiently using the [[network simplex algorithm]].\n\n== Definition ==\n\nA flow network is a [[directed graph]] <math>G=(V,E)</math> with a source vertex <math>s \\in V</math> and a sink vertex <math>t \\in V</math>, where each edge <math>(u,v) \\in E</math> has capacity <math>c(u,v) > 0</math>, flow <math>f(u,v) \\ge 0</math> and cost <math>a(u,v)</math>, with most minimum-cost flow algorithms supporting edges with negative costs. The cost of sending this flow along an edge <math>(u,v)</math> is <math>f(u,v)\\cdot a(u,v)</math>. The problem requires an amount of flow <math>d</math> to be sent from source <math>s</math> to sink <math>t</math>.\n\nThe definition of the problem is to minimize the '''total cost''' of the flow over all edges:\n\n:<math>\\sum_{(u,v) \\in E} a(u,v) \\cdot f(u,v)</math>\n\nwith the constraints\n:{|\n|-\n| '''Capacity constraints''': || <math>\\,f(u,v) \\le c(u,v)</math>\n|-\n| '''Skew symmetry''': || <math>\\,f(u,v) = - f(v,u)</math>\n|-\n| '''Flow conservation''': || <math>\\,\\sum_{w \\in V} f(u,w) = 0 \\text{ for all } u \\neq s, t</math>\n|-\n| '''Required flow''': || <math>\\,\\sum_{w \\in V} f(s,w) = d \\text{ and } \\sum_{w \\in V} f(w,t) = d</math>\n|}\n\n== Relation to other problems ==\n\nA variation of this problem is to find a flow which is maximum, but has the lowest cost among the maximum flow solutions. This could be called a minimum-cost maximum-flow problem and is useful for finding minimum cost maximum [[Matching (graph theory)|matching]]s.\n\nWith some solutions, finding the minimum cost maximum flow instead is straightforward. If not, one can find the maximum flow by performing a [[binary search]] on <math>d</math>.\n\nA related problem is the [[minimum cost circulation problem]], which can be used for solving minimum cost flow. This is achieved by setting the lower bound on all edges to zero, and then making an extra edge from the sink <math>t</math> to the source <math>s</math>, with capacity <math>c(t,s)=d</math> and lower bound <math>l(t,s)=d</math>, forcing the total flow from <math>s</math> to <math>t</math> to also be <math>d</math>.\n\nThe problem can be specialized into two other problemsÔºö\n* if the capacity constraint is removed, the problem is reduced to the [[shortest path problem]],\n* if the costs are all set equal to zero, the problem is reduced to the [[maximum flow problem]].\n\n== Solutions ==\n\nThe minimum cost flow problem can be solved by [[linear programming]], since we optimize a linear function, and all constraints are linear.\n\nApart from that, many combinatorial algorithms exist, for a comprehensive survey, see {{ref|AMO93}}. Some of them are generalizations of [[maximum flow|maximum flow algorithms]], others use entirely different approaches.\n\nWell-known fundamental algorithms (they have many variations):\n\n* ''Cycle canceling'': a general primal method.{{ref|K67}}\n* ''Minimum mean cycle canceling'': a simple [[strongly polynomial]] algorithm.{{ref|GT89}}\n* ''Successive shortest path'' and ''capacity scaling'': dual methods, which can be viewed as the generalizations of the [[Ford‚ÄìFulkerson algorithm]].{{ref|EK72}}\n* ''Cost scaling'': a primal-dual approach, which can be viewed as the generalization of the [[push-relabel algorithm]].{{ref|GT90}}\n* ''[[Network simplex algorithm]]'': a specialized version of the [[linear programming]] [[simplex method]].{{ref|O97}}\n* ''[[Out-of-kilter algorithm]]'' by [[D. R. Fulkerson]]\n\n== Application ==\n\n=== Minimum weight bipartite matching ===\n[[File:Minimum weight bipartite matching.pdf|thumb|Reducing Minimum weight bipartite matching to minimum cost max flow problem]]\nGiven a [[bipartite graph]] ''G'' = (''A'' ‚à™ ''B'', ''E''), the goal is to find the maximum cardinality matching in ''G'' that has minimum cost. Let ''w'': ''E'' ‚Üí ''R'' be a weight function on the edges of ''E''. The minimum weight bipartite matching problem or assignment problem is to find a \nperfect matching ''M'' ‚äÜ ''E'' whose total weight is minimized. The idea is to reduce this problem to a network flow problem.\n\nLet ''G&prime;'' = (''V&prime;'' = ''A'' ‚à™ ''B'', ''E&prime;'' = ''E''). Assign the capacity of all the edges in ''E&prime;'' to 1. Add a source vertex ''s'' and connect it to all the vertices in ''A&prime;'' and add a sink \nvertex ''t'' and connect all vertices inside group ''B&prime;'' to this vertex. The capacity of all the new edges is 1 and their costs is 0. It is proved that there is minimum weight perfect bipartite matching in ''G'' if and only if there a minimum cost flow in ''G&prime;''. {{ref|AMO93}}{{Dead link|date=January 2018}}\n\n== See also ==\n*[[LEMON (C++ library)]]\n*[[GNU Linear Programming Kit]]\n\n== References ==\n\n# {{Note|AMO93}}{{cite book\n | author = Ravindra K. Ahuja\n | author-link = Ravindra K. Ahuja\n | author2 = Thomas L. Magnanti\n | author2-link = Thomas L. Magnanti\n | author3 = James B. Orlin\n | author3-link = James B. Orlin\n | last-author-amp = yes\n | title = Network Flows: Theory, Algorithms, and Applications\n | year = 1993\n | publisher = Prentice-Hall, Inc.\n | isbn = 978-0-13-617549-0\n}}\n# {{Note|K67}}{{cite journal\n | author = Morton Klein\n | title = A primal method for minimal cost flows with applications to the assignment and transportation problems\n | journal = Management Science\n | volume = 14\n | issue = 3\n | pages = 205‚Äì220\n | year = 1967\n | doi=10.1287/mnsc.14.3.205\n| citeseerx = 10.1.1.228.7696\n }}\n# {{Note|GT89}}{{cite journal\n | author = Andrew V. Goldberg\n | author-link = Andrew V. Goldberg\n \n | author2 = Robert E. Tarjan\n | author2-link = Robert E. Tarjan\n | last-author-amp = yes\n | title = Finding minimum-cost circulations by canceling negative cycles\n | journal = Journal of the ACM\n | volume = 36\n | number = 4\n | pages = 873‚Äì886\n | year = 1989\n | doi=10.1145/76359.76368\n}}\n# {{Note|EK72}}{{cite journal\n | author = Jack Edmonds\n | author-link = Jack Edmonds\n \n | author2 = Richard M. Karp\n | author2-link = Richard M. Karp\n | last-author-amp = yes\n | title = Theoretical improvements in algorithmic efficiency for network flow problems\n | journal = Journal of the ACM\n | volume = 19\n | number = 2\n | pages = 248‚Äì264\n | year = 1972\n | doi=10.1145/321694.321699\n}}\n# {{Note|GT90}}{{cite journal\n | author = Andrew V. Goldberg\n | author-link = Andrew V. Goldberg\n \n | author2 = Robert E. Tarjan\n | author2-link = Robert E. Tarjan\n | last-author-amp = yes\n | title = Finding minimum-cost circulations by successive approximation\n | journal = Math. Oper. Res.\n | volume = 15\n | number = 3\n | pages = 430‚Äì466\n | year = 1990\n | doi=10.1287/moor.15.3.430\n}}\n# {{Note|O97}}{{cite journal\n | author = James B. Orlin\n | author-link = James B. Orlin\n | title = A polynomial time primal network simplex algorithm for minimum cost flows\n | journal = Mathematical Programming\n | volume = 78\n | issue = 2\n | pages = 109‚Äì129\n | year = 1997\n | doi=10.1007/bf02614365\n| hdl = 1721.1/2584\n }}\n\n== External links ==\n* [http://lemon.cs.elte.hu/ LEMON C++ library with several maximum flow and minimum cost circulation algorithms]\n* [http://www.di.unipi.it/optimize/Software/MCF.html The MCFClass C++ project library with some minimum cost flow and shortest path algorithms]\n\n[[Category:Network flow problem]]\n[[Category:Mathematical problems]]"
    },
    {
      "title": "Multi-commodity flow problem",
      "url": "https://en.wikipedia.org/wiki/Multi-commodity_flow_problem",
      "text": "The '''multi-commodity flow problem''' is a [[flow network|network flow]] problem with multiple commodities (flow demands) between different source and sink nodes.\n\n==Definition==\n\nGiven a flow network <math>\\,G(V,E)</math>, where edge <math>(u,v) \\in E</math> has capacity <math>\\,c(u,v)</math>. There are <math>\\,k</math> commodities <math>K_1,K_2,\\dots,K_k</math>, defined by <math>\\,K_i=(s_i,t_i,d_i)</math>, where <math>\\,s_i</math> and <math>\\,t_i</math> is the '''source''' and '''sink''' of commodity <math>\\,i</math>, and <math>\\,d_i</math> is its demand. The variable <math>\\,f_i(u,v)</math> defines the fraction of flow <math>\\,i</math> along edge <math>\\,(u,v)</math>, where <math>\\,f_i(u,v) \\in [0,1]</math> in case the flow can be split among multiple paths, and <math>\\,f_i(u,v) \\in \\{0,1\\}</math> otherwise (i.e. \"single path routing\"). Find an assignment of all flow variables which satisfies the following four constraints:\n\n'''(1) Link capacity:''' The sum of all flows routed over a link does not exceed its capacity.\n:<math>\\forall (u,v)\\in E:\\,\\sum_{i=1}^{k} f_i(u,v)\\cdot d_i \\leq c(u,v)</math>\n\n'''(2) Flow conservation on transit nodes:''' The amount of a flow entering an intermediate node <math>u</math> is the same that exits the node.\n:<math>\\,\\sum_{w \\in V} f_i(u,w) - \\sum_{w \\in V} f_i(w,u) = 0 \\quad \\mathrm{when} \\quad u \\neq s_i, t_i </math>\n\n'''(3) Flow conservation at the source:''' A flow must exit its source node completely.\n:<math>\\,\\sum_{w \\in V} f_i(s_i,w) - \\sum_{w \\in V} f_i(w,s_i) = 1</math>\n\n'''(4) Flow conservation at the destination:''' A flow must enter its sink node completely.\n:<math>\\,\\sum_{w \\in V} f_i(w,t_i) - \\sum_{w \\in V} f_i(t_i,w) = 1</math>\n\n==Corresponding optimization problems==\n\n'''Load balancing''' is the attempt to route flows such that the utilization <math>U(u,v)</math> of all links <math>(u,v)\\in E</math> is even, where\n\n:<math>U(u,v)=\\frac{\\sum_{i=1}^{k} f_i(u,v)\\cdot d_i}{c(u,v)}</math>\n\nThe problem can be solved e.g. by minimizing <math>\\sum_{u,v\\in V} (U(u,v))^2</math>. A common linearization of this problem is the minimization of the maximum utilization <math>U_{max}</math>, where\n\n:<math>\\forall (u,v)\\in E:\\, U_{max} \\geq U(u,v)</math>\n\nIn the '''minimum cost multi-commodity flow problem''', there is a cost <math>a(u,v) \\cdot f(u,v)</math> for sending a flow on <math>\\,(u,v)</math>. You then need to minimize \n\n:<math>\\sum_{(u,v) \\in E} \\left( a(u,v) \\sum_{i=1}^{k} f_i(u,v) \\right)</math>\n\nIn the '''maximum multi-commodity flow problem''', the demand of each commodity is not fixed, and the total throughput is maximized by maximizing the sum of all demands <math>\\sum_{i=1}^{k} d_i</math>\n\n==Relation to other problems==\n\nThe minimum cost variant is a generalisation of the [[minimum cost flow problem]]. Variants of the [[circulation problem]] are generalisations of all flow problems.\n\n==Usage==\n\n[[Routing and wavelength assignment]] (RWA) in [[optical burst switching]] of [[SONET|Optical Network]] would be approached via multi-commodity flow formulas.\n\n==Solutions==\n\nIn the decision version of problems, the problem of producing an integer flow satisfying all demands is [[NP-complete]],<ref name=\"EIS76\">{{cite journal | author = S. Even and A. Itai and A. Shamir | title = On the Complexity of Timetable and Multicommodity Flow Problems | publisher = SIAM | year = 1976 | journal = SIAM Journal on Computing | volume = 5 | pages = 691‚Äì703 | url = http://ieeexplore.ieee.org/document/4567876/?arnumber=4567876&tag=1 | doi = 10.1137/0205048 | issue = 4 | accessdate=2016-10-05}}</ref> even for only two commodities and unit capacities (making the problem [[strongly NP-complete]] in this case). \n\nIf fractional flows are allowed, the problem can be solved in polynomial time through [[linear programming]].<ref>{{cite book | author = [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]] | title = Introduction to Algorithms | edition = 3rd | year = 2009 | publisher = MIT Press and McGraw‚ÄìHill | pages = 862 | chapter = 29 | isbn = 978-0-262-03384-8| title-link = Introduction to Algorithms }}</ref> Or through (typically much faster) [[fully polynomial time approximation scheme]]s.<ref name=\"\">{{cite conference | author = George Karakostas | title = Faster approximation schemes for fractional multicommodity flow problems | booktitle = Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms | year = 2002 | isbn = 0-89871-513-X | pages = 166‚Äì173}}</ref>\n\n<!-- A combinatorial algorithm recently has been proposed for the Multi-commodity Flow Problem, which is based on a new conception‚Äî‚Äîequilibrium pseudo-flow.<ref>[https://arxiv.org/abs/1904.09397 Liu, P. (2019). A Combinatorial Algorithm for the Multi-commodity Flow Problem. arXiv preprint arXiv:1904.09397.]</ref> -->\n\n==External resources==\n* Papers by Clifford Stein about this problem: http://www.columbia.edu/~cs2035/papers/#mcf\n* Software solving the problem: https://web.archive.org/web/20130306031532/http://typo.zib.de/opt-long_projects/Software/Mcf/\n\n==References==\n<references/>\n\n{{DEFAULTSORT:Multi-Commodity Flow Problem}}\n[[Category:Network flow problem]]\n\nAdd: Jean-Patrice Netter, Meshings: a primal type of approach to the maximum flow in a muti-commodity network (integer or not), dissertation Johns Hopkins University, 1971"
    },
    {
      "title": "Nowhere-zero flow",
      "url": "https://en.wikipedia.org/wiki/Nowhere-zero_flow",
      "text": "In [[graph theory]], a '''nowhere-zero flow''' or '''NZ flow''' is a [[Flow network|network flow]] that is nowhere zero. It is intimately connected (by duality) to [[graph coloring|coloring]] [[planar graphs]].\n\n== Definitions ==\nLet ''G'' = (''V'',''E'') be a [[directed graph|digraph]] and let ''M'' be an [[abelian group]]. A map ''œÜ'': ''E'' ‚Üí ''M'' is an '''M-circulation''' if for every [[vertex (graph theory)|vertex]] ''v'' ‚àà ''V'' \n\n:<math>\\sum_{e \\in \\delta^+(v)} \\varphi(e) = \\sum_{e \\in \\delta^-(v)} \\varphi(e),</math>\nwhere ''Œ¥''<sup>+</sup>(''v'') denotes the set of edges out of ''v'' and ''Œ¥''<sup>‚àí</sup>(''v'') denotes the set of edges into ''v''. Sometimes, this condition is referred to as [[Kirchhoff's circuit laws|Kirchhoff's law]]. \n\nIf ''œÜ''(''e'') ‚â† 0 for every ''e'' ‚àà ''E'', we call ''œÜ'' a '''nowhere-zero flow,''' an '''M-flow,''' or an '''NZ-flow.''' If ''k'' is an integer and {{nowrap begin}}0 < |''œÜ''(''e'')| < ''k''{{nowrap end}} then ''œÜ'' is a '''''k''-flow.'''<ref name=\":0\">{{Cite book|url=http://worldcat.org/oclc/1048203362|title=Graph theory|last=Diestel, Reinhard, 1959- Verfasser.|isbn=9783662536216|oclc=1048203362}}</ref>\n\n=== Other notions ===\nLet ''G'' = (''V'',''E'') be an [[undirected graph]]. An orientation of ''E'' is a '''modular''' ''k''-'''flow''' if\n\n:<math>|\\delta^+(v)| \\equiv |\\delta^-(v)| \\pmod k</math>\nfor every vertex&nbsp;''v''&nbsp;‚àà&nbsp;''V''.<!-- why is this useful? -->\n\n== Properties ==\n\n* The set of M-flows does not necessarily form a group as the sum of two flows on one edge may add to 0.\n*(Tutte 1950) A graph G has an M-flow iff it has a |M|-flow. As a consequence, a <math>\\mathbb{Z}_k</math> flow exists iff a k-flow exists.<ref name=\":0\" /> As a consequence G admits a k-flow then it admits an h-flow where <math>h \\geq k</math>.\n*Orientation independence. Modify a nowhere-zero flow ''œÜ'' on a graph ''G'' by choosing an edge ''e'', reversing it, and then replacing ''œÜ''(''e'') with ‚àí''œÜ''(''e'').  After this adjustment, ''œÜ'' is still a nowhere-zero flow. Furthermore, if œÜ was originally a ''k''-flow, then the resulting ''œÜ'' is also a ''k''-flow.  Thus, the existence of a nowhere-zero ''M''-flow or a nowhere-zero ''k''-flow is independent of the orientation of the graph. Thus, an undirected graph ''G'' is said to have a nowhere-zero ''M''-flow or nowhere-zero ''k''-flow if some (and thus every) orientation of ''G'' has such a flow.\n\n== Flow polynomial ==\n{{Main|Tutte polynomial|Deletion‚Äìcontraction formula}}\nLet <math>N_M(G)</math> be the number of ''M''-flows on ''G''. It satisfies the [[deletion‚Äìcontraction formula]] ''N''(''G'') =&nbsp;''N''(''G''&nbsp;/&nbsp;''e'')&nbsp;‚àí&nbsp;''N''(''G''&nbsp;\\&nbsp;''e'').<ref name=\":0\" />\n\nUsing this and induction, it can be shown that ''N''(''G'') is a polynomial in <math>|M| - 1</math> where |M| is the [[Order (group theory)|order]] of the group M. We call N(G) the '''[[flow polynomial]]''' of ''G'' and abelian group ''M''.\n\nThe above implies that two groups of equal order have an equal number of NZ flows. The order is the only group parameter that matters, not the structure of M. In particular <math>N_{M_1}(G) = N_{M_2}(G)</math> if <math>|M_1| = |M_2|</math>.\n\nThe above results were proved by [[Tutte]] in 1953 when he was studying the [[Tutte polynomial]], a generalization of the flow polynomial.<ref>{{cite journal\n | last = Tutte\n | first = William Thomas\n | authorlink = W. T. Tutte\n | year = 1953\n | title = A contribution to the theory of chromatic polynomials\n | url = http://cms.math.ca/cjm/a144778#\n}}</ref>\n\n== Flow-coloring duality ==\n{{Main|Graph coloring}}\nThere is a duality between region-[[Graph coloring|colorings]] and M-flows, as well as the duality between k-region colorings and k-flows / Zk flows.\n\nLet ''G'' be a directed [[Bridge (graph theory)|bridgeless]] [[planar graph]], and assume that the regions of this drawing are properly ''k''-colored with the colors {0, 1, 2, .., ''k'' ‚Äì 1}. \n\nConstruct a map ''œÜ'': ''E''(''G'') ‚Üí {‚Äì(''k'' ‚Äì 1), ..., ‚Äì1, 0, 1, ..., ''k'' ‚Äì 1} by the following rule: if the edge ''e'' has a region of color ''x'' to the left and a region of color ''y'' to the right, then let ''œÜ''(''e'') = ''x'' ‚Äì ''y''. Then ''œÜ'' is a (NZ) ''k''-flow since x and y must be different colors. \n\nSo if ''G'' and ''G*'' are planar [[Dual graph|dual graphs]] and ''G*'' is ''k''-colorable (there is a coloring of the faces of G), then ''G'' has a NZ ''k''-flow. Tutte proved that the converse is also true (use induction on |''E''(''G'')| ). This can be expressed concisely by the relation: <ref name=\":0\" />\n\n<math>\\chi(G^*) = \\phi(G)</math>\n\nwhere the RHS is the '''flow number''', the smallest ''k'' for which ''G'' permits a ''k''-flow.\n\n=== In general ===\n\n* Let <math>c(r) \\in M</math> for each region ''r'' be the coloring function\n* Define  <math>\\phi_c(e) = c(r_1) - c(r_2)</math> where ''r''<sub>1</sub> is the region to the left of ''e'' and ''r''<sub>2</sub> is to the right\n* For every ''M''-circulation <math>\\varphi</math> there is a coloring function ''c'' such that <math>\\varphi = \\varphi_c</math> (prove by induction)\n* ''c'' is a |''E''(''G'')|-region-coloring iff <math>\\varphi_c</math> is a NZ ''M''-flow (straightforward)\n\nThe duality follows by combining the last two points. We can specialize to <math>M = \\mathbb{Z}_k</math> to obtain the similar results for ''k''-flows discussed above. Given this duality between NZ flows and colorings, and since we can define NZ flows to arbitrary graphs (not just planar), we can use this to extend coloring theory to non-planar graphs.<ref name=\":0\" />\n\n== Applications ==\n\n* ''G'' is 2-region-colorable iff every vertex has even degree (consider NZ '''Z'''<sub>2</sub> flows).<ref name=\":0\" />\n* Let <math>K = \\mathbb{Z}_2 \\times \\mathbb{Z}_2</math> be the [[Klein-4 group]]. Then a [[cubic graph]] has a ''K''-flow iff it is three-[[Edge coloring|edge-colorable]]. As a corollary a cubic graph that is 3-edge colorable is 4-face colorable.<ref name=\":0\" />\n*A graph is 4-face colorable iff it permits a NZ '''Z'''<sub>4</sub> flow (see [[Four color theorem]]). The [[Petersen graph]] does not have a NZ '''Z'''<sub>4</sub> flow, and this led to the 4-flow conjecture (see below).\n* If ''G'' is a triangulation then ''G'' is 3-(vertex)colorable iff every vertex has even degree. By the first bullet, the dual graph ''G''* is 2-colorable and thus [[Bipartite graph|bipartite]] and planar cubic. So ''G''* has a NZ '''Z'''<sup>3</sup> flow and is thus 3-region colorable, making ''G'' 3-vertex colorable.<ref name=\":0\" />\n* Just as no graph with a [[glossary of graph theory|loop]] edge has a proper (vertex) coloring, no graph with a [[glossary of graph theory|bridge]] can have a NZ M-flow for any group M. Conversely, every [[bridgeless graph]] has a NZ '''Z'''-flow (a form of [[Robbins theorem]]).{{Citation needed|date=May 2019}}\n\n== Existence of ''k''-flows ==\n{{unsolved|mathematics|Does every bridgeless graph have a nowhere zero 5-flow? Does every bridgeless graph that does not have the Petersen graph as a minor have a nowhere zero 4-flow?}}\nInteresting questions arise when trying to find nowhere-zero ''k''-flows for small values of ''k''. The following have been proven:\n\n* Jaeger's 4-flow theorem: every 4-[[glossary of graph theory|edge-connected]] graph has a 4-flow<ref>F. Jaeger, Flows and generalized coloring theorems in graphs, J. Comb. Theory Set. B, 26 (1979), 205‚Äì216.</ref> \n* [[Paul_Seymour_(mathematician)|Seymour]]'s 6-flow theorem: every bridgeless graph has a 6-flow (1981).<ref>P. D. Seymour, Nowhere-zero 6-flows, J. Comb. Theory Ser B, 30 (1981), 130‚Äì135.</ref>\n\n=== 4-flow and 5-flow conjectures ===\nAs of 2019, the following are currently unsolved (due to [[W. T. Tutte|Tutte]]):\n\n*'''5-flow conjecture:''' Every bridgeless graph has a 5-flow.<ref>[http://garden.irmacs.sfu.ca/?q=op/5_flow_conjecture 5-flow conjecture], Open Problem Garden.</ref>\n*'''4-flow conjecture:''' Every bridgeless graph that does not have the [[Petersen graph]] as a [[minor (graph theory)|minor]] has a 4-flow.<ref>[http://garden.irmacs.sfu.ca/?q=op/4_flow_conjecture 4-flow conjecture], Open Problem Garden.</ref> The converse does not hold since the [[complete graph]] K11 contains a Petersen graph AND a 4-flow.<ref name=\":0\" /> For bridgeless [[cubic graph]]s with no Petersen minor, 4-flows exist by the [[Snark (graph theory)|snark theorem]] (Seymour, et al 1998, not yet published). The [[four color theorem]] is equivalent to the statement that no snark is planar.<ref name=\":0\" />\n\n== See also ==\n* algebraic flow theory\n*[[Cycle space]]\n*[[Cycle double cover|Cycle double cover conjecture]]\n*[[Four color theorem]]\n*[[Graph coloring]]\n*[[Edge coloring]]\n*[[Tutte polynomial]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n* {{cite book\n| last                  = Zhang\n| first                 = Cun-Quan\n| title                 = Integer Flows and Cycle Covers of Graphs\n| url                   = http://www.math.wvu.edu/~cqzhang/book.html\n| series                = Chapman & Hall/CRC Pure and Applied Mathematics Series\n| year                  = 1997\n| publisher             = Marcel Dekker, Inc.\n| isbn                  = 9780824797904\n| lccn                  = 96037152\n}}\n*{{cite book\n | last = Zhang | first = Cun-Quan\n | isbn = 978-0-5212-8235-2\n | publisher = Cambridge University Press\n | title = Circuit Double Cover of Graphs\n | year = 2012}}\n*{{cite book\n | last1 = Jensen | first1 = T. R.\n | last2 = Toft | first2 = B.\n | series = Wiley-Interscience Serires in Discrete Mathematics and Optimization\n | chapter = 13 Orientations and Flows\n | pages = 209‚Äì219\n | title = Graph Coloring Problems\n | year = 1995}}\n*https://arxiv.org/pdf/1009.4062.pdf\n\n[[Category:Network flow problem]]"
    },
    {
      "title": "Out-of-kilter algorithm",
      "url": "https://en.wikipedia.org/wiki/Out-of-kilter_algorithm",
      "text": "The '''out-of-kilter algorithm''' is an [[algorithm]] that computes the solution to the [[minimum-cost flow problem]] in a [[flow network]]. It was published in 1961 by [[D. R. Fulkerson]]{{nnbsp}}<ref>{{cite journal |title=An Out-of-Kilter Method for Minimal-Cost Flow Problems |author=D. R. Fulkerson |journal=[[Journal of the Society for Industrial and Applied Mathematics]] |volume=9|issue=1 |date=March 1961|pages=18‚Äì27 |jstor=2099013}}</ref> and is described here.<ref name=\"durbin-and-kroenke-1967\">{{cite book\n | last1 = Durbin | first1 = EP\n | last2 = Kroenke | first2 = DM\n | title = The out-of-kilter algorithm: a primer ‚Äî Memorandum RM-5472-PR\n | date = December 1967\n | publisher = The Rand Corporation\n | location = Santa Monica, California, USA\n | url = https://www.rand.org/content/dam/rand/pubs/research_memoranda/2008/RM5472.pdf\n | access-date = 2018-01-16\n}}\n</ref>  The analog of steady state flow in a network of nodes and arcs may describe a variety of processes.  Examples include transportation systems & personnel assignment actions.  Arcs generally have cost & capacity parameters.  A recurring problem is trying to determine the minimum cost route between two points in a capacitated network.  The idea of the algorithm is to identify out-of-kilter arcs and modify the flow network until all arcs are in-kilter and a minimum cost flow has been reached.  The algorithm can be used to minimize the total cost of a constrained flow in an oriented network.\n\n== Algorithm ==\nTo begin, the algorithm takes a single cycle and a set of node numbers. It then searches for out-of-kilter arcs. If none are found the algorithm is complete. If the flow needs to be increased or decreased to bring an arc into kilter, the algorithm will look for a path that increases or decreases the flow respectively. If no paths are found to improve the system then there is no feasible flow. This is done until all arcs are in-kilter, at which point the algorithm is complete.\n\nSuppose that the network has n nodes and m oriented arcs.  We write <math>j ~ (i,i^1)</math> if arc <math>j</math> has initial node <math>i</math> and terminal node <math>i^1</math>.  Let <math>x(j)</math> be the flow along arc <math>j</math> (from node <math>i</math> to node <math>i^1</math>).  Define <math>c^-(j)</math> and <math>c^+(j)</math> to be the lower and upper capacity bounds on the flow in arc <math>j</math>.  The capacities may be either finite, or infinite on some or all arcs for either the lower or upper bounds.  The problem that is at hand to solve is to minimize: <math>\\sum_{j=1}^md(j) x(j)</math> subject to:\n\n<math>\\sum_{j:j~(i,i^1)}x(j)   -\\sum_{j:j~(i^1,i)}x(j) = 0 </math> for each <math>i = 1,....,n</math> (1)\n\n, and:\n\n<math>c^-(j)\\leq x(j)\\leq c^+(j) </math> for each <math>j = 1,....,n</math> (2)\n\nIf a given flow x satisfies (1), then the flow is conserved at each node and we call the flow a circulation.  If the flow x satisfies (2) we say it is feasible.\n\n== Complexity ==\nRuntime:\n\n* The algorithm terminates within O(mU) iterations\n* Dominant computation is shortest path computation\n* Total runtime is: O(m^2 U+mUnlogn)\n\n==References==\n\n{{reflist|30em}}<ref>{{Cite web|url=https://www.maths.cam.ac.uk/sites/www.maths.cam.ac.uk/files/pre2014/undergrad/catam/committee/STATS/8pt4.pdf|title=Out of Kilter Algorithm|last=Cambridge|first=University of|date=July 2012|website=https://www.maths.cam.ac.uk|archive-url=|archive-date=April 22, 2018|dead-url=|access-date=}}</ref>\n\n==External links==\n* {{YouTube|id=JaDnsMbeUkE|title=Algoritmo Out-of-Kilter}} (in Spanish)\n\n[[Category:Network flow problem]]"
    },
    {
      "title": "Push‚Äìrelabel maximum flow algorithm",
      "url": "https://en.wikipedia.org/wiki/Push%E2%80%93relabel_maximum_flow_algorithm",
      "text": "In [[mathematical optimization]], the '''push‚Äìrelabel algorithm''' (alternatively, '''preflow‚Äìpush algorithm''') is an algorithm for computing [[maximum flow]]s. The name \"push‚Äìrelabel\" comes from the two basic operations used in the algorithm. Throughout its execution, the algorithm maintains a \"preflow\" and gradually converts it into a maximum flow by moving flow locally between neighboring nodes using ''push'' operations under the guidance of an admissible network maintained by ''relabel'' operations. In comparison, the [[Ford‚ÄìFulkerson algorithm]] performs global augmentations that send flow following paths from the source all the way to the sink.<ref name=\"clrs26\"/>\n\nThe push‚Äìrelabel algorithm is considered one of the most efficient maximum flow algorithms. The generic algorithm has a [[strongly polynomial]] {{math|''O''(''V''<sup>&nbsp;2</sup>''E'')}} time complexity, which is asymptotically more efficient than the {{math|''O''(''VE''<sup>&nbsp;2</sup>)}} [[Edmonds‚ÄìKarp algorithm]].<ref name=\"goldberg86\"/> Specific variants of the algorithms achieve even lower time complexities. The variant based on the highest label node selection rule has {{math|''O''(''V''<sup>&nbsp;2</sup>{{sqrt|''E''}})}} time complexity and is generally regarded as the benchmark for maximum flow algorithms.<ref name=\"ahuja97\"/><ref name=\"goldberg08\"/> Subcubic {{math|''O''(''VE''log(''V''<sup>&nbsp;2</sup>/''E''))}} time complexity can be achieved using [[Link-cut tree|dynamic trees]], although in practice it is less efficient.<ref name=\"goldberg86\"/>\n\nThe push‚Äìrelabel algorithm has been extended to compute [[minimum cost flow]]s.<ref name=\"goldberg97\"/> The idea of distance labels has led to a more efficient augmenting path algorithm, which in turn can be incorporated back into the push‚Äìrelabel algorithm to create a variant with even higher empirical performance.<ref name=\"goldberg08\"/><ref name=\"ahuja91\"/>\n\n==History==\n\nThe concept of a preflow was originally designed by [[Alexander V. Karzanov]] and was published in 1974 in Soviet Mathematical Dokladi 15. This pre-flow algorithm also used a push operation; however, it used distances in the auxiliary network to determine where to push the flow instead of a labeling system.<ref name=\"goldberg86\"/><ref name=\"goldberg2014\"/>\n\nThe push-relabel algorithm was designed by [[Andrew V. Goldberg]] and [[Robert Tarjan]]. The algorithm was initially presented in November 1986 in STOC '86: Proceedings of the eighteenth annual ACM symposium on Theory of computing, and then officially in October 1988 as an article in the Journal of the ACM. Both papers detail a generic form of the algorithm terminating in {{math|''O''(''V''<sup>&nbsp;2</sup>''E'')}} along with a {{math|''O''(''V''<sup>&nbsp;3</sup>)}} sequential implementation, a {{math|''O''(''VE''&nbsp;log(''V''<sup>&nbsp;2</sup>/''E''))}} implementation using dynamic trees, and parallel/distributed implementation.<ref name=\"goldberg86\"/><ref name=\"goldberg88\"/>. A explained in <ref name=\"amo93\"/> Goldberg-Tarjan introduced distance labels by incorporating them into the parallel maximum flow algorithm of Yossi Shiloach and [[Uzi Vishkin]] <ref name=\"sv82\"/>.\n\n==Concepts==\n\n===Definitions and notations===\n{{main|Flow network}}\n\nLet:\n*{{math|''G'' {{=}} (''V'', ''E'')}} be a ''network'' with ''capacity function'' {{math|''c'': ''V'' √ó ''V'' ‚Üí ‚Ñù<sub>‚àû</sub>}},\n*{{math|''F'' {{=}} (''G'', ''c'', ''s'', ''t'')}} a ''flow network'', where {{math|''s'' ‚àà ''V''}} and {{math|''t'' ‚àà ''V''}} are chosen ''source'' and ''sink'' vertices respectively,\n*{{math|''f'' : ''V'' √ó ''V'' ‚Üí ‚Ñù}} denote a [[Flow_network#Flows|''pre-flow'']] in {{mvar|F}},\n*{{math|''x''<sub>''f''</sub> : ''V'' ‚Üí ‚Ñù}} denote the ''excess'' function with respect to the flow {{mvar|f}}, defined by {{math|''x''<sub>''f''</sub> (''u'') {{=}} ‚àë<sub>''v'' ‚àà ''V''</sub> ''f'' (''v'', ''u'') ‚àí ‚àë<sub>''v'' ‚àà ''V''</sub> ''f'' (''u'', ''v'')}},\n*{{math|''c''<sub>''f''</sub> : ''V'' √ó ''V'' ‚Üí ‚Ñù<sub>‚àû</sub>}} denote the ''residual capacity function'' with respect to the flow {{mvar|f}}, defined by {{math|''c''<sub>''f''</sub>&nbsp;(''e'') {{=}} ''c''(''e'') ‚àí ''f''&nbsp;(''e'')}},\nand\n*{{math|''G''<sub>''f''</sub> (''V'', ''E''<sub>''f''&nbsp;</sub>)}} denote the [[Flow_network#Residuals|''residual network'']] of {{mvar|G}} with respect to the flow {{mvar|f}}.\n\n\nThe push‚Äìrelabel algorithm uses a nonnegative integer valid '''labeling function''' which makes use of ''distance labels'', or ''heights'', on nodes to determine which arcs should be selected for the push operation. This labeling function is denoted by {{math|ùìÅ : ''V'' ‚Üí ‚Ñï}}. This function must satisfy the following conditions in order to be considered valid:\n\n:'''Valid labeling''':\n::{{math|ùìÅ(''u'') ‚â§ ùìÅ(''v'') + 1}} for all {{math|(''u'', ''v'') ‚àà ''E''<sub>''f''</sub>}}\n:'''Source condition''':\n::{{math|ùìÅ(''s'') {{=}} {{!}}&nbsp;''V''&nbsp;{{!}}}}\n:'''Sink conservation''':\n::{{math|ùìÅ(''t'') {{=}} 0}}\n\nIn the algorithm, the label values of {{mvar|s}} and {{mvar|t}} are fixed. {{math|ùìÅ(''u'')}} is a lower bound of the unweighted distance from {{mvar|u}} to {{mvar|t}} in {{math|''G''<sub>''f''</sub>}}&nbsp; if {{mvar|t}} is reachable from {{mvar|u}}. If {{mvar|u}} has been disconnected from {{mvar|t}}, then {{math|ùìÅ(''u'') ‚àí {{!}}&nbsp;''V''&nbsp;{{!}}}} is a lower bound of the unweighted distance from {{mvar|u}} to {{mvar|s}}. As a result, if a valid labeling function exists, there are no {{math|''s''-''t''}} paths in {{math|''G''<sub>''f''</sub>}}&nbsp; because no such paths can be longer than {{math|{{!}}&nbsp;''V''&nbsp;{{!}} ‚àí 1}}.\n\nAn arc {{math|(''u'', ''v'') ‚àà ''E''<sub>''f''</sub>}}&nbsp; is called '''admissible''' if {{math|ùìÅ(''u'') {{=}} ùìÅ(''v'') + 1}}. The '''admissible network''' {{math|''GÃÉ''<sub>''f''</sub> (''V'', ''·∫º''<sub>''f''</sub>&nbsp;)}} is composed of the set of arcs {{math|''e'' ‚àà ''E''<sub>''f''</sub>}}&nbsp;  that are admissible. The admissible network is acyclic.\n\n===Operations===\n\n====Initialization====\n\nThe algorithm starts by creating a residual graph, initializing the preflow values to zero and performing a set of saturating push operations on residual arcs exiting the source, {{math|(''s'', ''v'') where ''v'' ‚àà ''V'' \\ {{(}}''s''{{)}}}}. Similarly, the labels are initialized such that the label at the source is the number of nodes in the graph, {{math|ùìÅ(''s'') {{=}} {{!}}&nbsp;''V''&nbsp;{{!}}}}, and all other nodes are given a label of zero. Once the initialization is complete the algorithm repeatedly performs either the push or relabel operations against active nodes until no applicable operation can be performed.\n\n====Push====\nThe push operation applies on an admissible out-arc {{math|(''u'', ''v'')}} of an active node {{mvar|u}} in {{math|''G''<sub>''f''</sub>}}. It moves {{math|min{{(}}''x''<sub>''f''</sub> (''u''), ''c''<sub>''f''</sub> (''u'',''v''){{)}}}} units of flow from {{mvar|u}} to {{mvar|v}}.\n\n push(u, v):\n     assert x<sub>f</sub>[u] > 0 and ùìÅ[u] == ùìÅ[v] + 1\n     Œî = min(x<sub>f</sub>[u], c[u][v] - f[u][v])\n     f[u][v] += Œî\n     f[v][u] -= Œî\n     x<sub>f</sub>[u] -= Œî\n     x<sub>f</sub>[v] += Œî\n\nA push operation that causes {{math|''f''&nbsp;(''u'', ''v'')}} to reach {{math|''c''(''u'', ''v'')}} is called a '''saturating push''' since it uses up all the available capacity of the residual arc. Otherwise, all of the excess at the node is pushed across the residual arc. This is called an '''unsaturating''' or '''non-saturating push'''.\n\n====Relabel====\nThe relabel operation applies on an active node {{mvar|u}} without any admissible out-arcs in {{math|''G<sub>f</sub>''}}. It modifies {{math|ùìÅ(''u'')}} to be the minimum value such that an admissible out-arc is created. Note that this always increases {{math|ùìÅ(''u'')}} and never creates a steep arc, which is an arc {{math|(''u'', ''v'')}} such that {{math|''c''<sub>''f''</sub>&nbsp;(''u'', ''v'') > 0}}, and {{math|ùìÅ(''u'') > ùìÅ(''v'') + 1}}.\n\n relabel(u):\n     assert x<sub>f</sub>[u] > 0 and ùìÅ[u] <= ùìÅ[v] for all v such that c[u][v] - f[u][v] > 0\n     ùìÅ[u] = min(ùìÅ[v] for all v such that c[u][v] - f[u][v] > 0) + 1\n\n====Effects of push and relabel====\nAfter a push or relabel operation, {{math|ùìÅ}} remains a valid labeling function with respect to {{mvar|f}}.\n\nFor a push operation on an admissible arc {{math|(''u'', ''v'')}}, it may add an arc {{math|(''v'', ''u'')}} to {{math|''E<sub>f</sub>''}}, where {{math|ùìÅ(''v'') {{=}} ùìÅ(''u'') ‚àí 1 ‚â§ ùìÅ(''u'') + 1}}; it may also remove the arc {{math|(''u'', ''v'')}} from {{math|''E''<sub>''f''</sub>}}, where it effectively removes the constraint {{math|ùìÅ(''u'') ‚â§ ùìÅ(''v'') + 1}}.\n\nTo see that a relabel operation on node {{mvar|u}} preserves the validity of {{math|ùìÅ(''u'')}}, notice that this is trivially guaranteed by definition for the out-arcs of ''u'' in {{math|''G''<sub>''f''</sub>}}. For the in-arcs of {{mvar|u}} in {{math|''G''<sub>''f''</sub>}}, the increased {{math|ùìÅ(''u'')}} can only satisfy the constraints less tightly, not violate them.\n\n==The generic push‚Äìrelabel algorithm==\n\nThe generic push‚Äìrelabel algorithm is used as a proof of concept only and does not contain implementation details on how to select an active node for the push and relabel operations. This generic version of the algorithm will terminate in {{math|''O''(''V''<sup>2</sup>''E'')}}.\n\nSince {{math|ùìÅ(''s'') {{=}} {{!}}&nbsp;''V''&nbsp;{{!}}}}, {{math|ùìÅ(''t'') {{=}} 0}}, and there are no paths longer than {{math|{{!}}&nbsp;''V''&nbsp;{{!}} ‚àí 1}} in {{math|''G''<sub>''f''</sub>}}, in order for {{math|ùìÅ(''s'')}} to satisfy the valid labeling condition {{mvar|s}} must be disconnected from {{mvar|t}}. At initialisation, the algorithm fulfills this requirement by creating a pre-flow {{mvar|f}} that saturates all out-arcs of {{mvar|s}}, after which {{math|ùìÅ(''v'') {{=}} 0}} is trivially valid for all {{math|''v'' ‚àà ''V'' \\ {{(}}''s'', ''t''{{)}}}}. After initialisation, the algorithm repeatedly executes an applicable push or relabel operation until no such operations apply, at which point the pre-flow has been converted into a maximum flow.\n\n generic-push-relabel(G, c, s, t):\n     create a pre-flow f that saturates all out-arcs of s\n     let ùìÅ[s] = |V|\n     let ùìÅ[v] = 0 for all v ‚àà V \\ {s}\n     while there is an applicable push or relabel operation\n         execute the operation\n\n===Correctness===\nThe algorithm maintains the condition that {{math|ùìÅ}} is a valid labeling during its execution. This can be proven true by examining the effects of the push and relabel operations on the label function {{math|ùìÅ}}. The relabel operation increases the label value by the associated minimum plus one which will always satisfy the {{math|ùìÅ(''u'') ‚â§ ùìÅ(''v'') + 1}} constraint. The push operation can send flow from {{mvar|u}} to {{mvar|v}} if {{math|ùìÅ(''u'') {{=}} ùìÅ(''v'') + 1}}. This may add {{math|(''v'', ''u'')}} to {{math|''G''<sub>''f''</sub>&nbsp;}} and may delete {{math|(''u'', ''v'')}} from {{math|''G''<sub>''f''</sub>&nbsp;}}. The addition of {{math|(''v'', ''u'')}} to {{math|''G''<sub>''f''</sub>&nbsp;}} will not affect the valid labeling since {{math|ùìÅ(''v'') {{=}} ùìÅ(''u'') ‚àí 1}}.  The deletion of {{math|(''u'', ''v'')}} from {{math|''G''<sub>''f''</sub>&nbsp;}} removes the corresponding constraint since the valid labeling property {{math|ùìÅ(''u'') ‚â§ ùìÅ(''v'') + 1}} only applies to residual arcs in {{math|''G''<sub>''f''</sub>&nbsp;}}.<ref name=\"goldberg88\"/>\n\nIf a preflow {{mvar|f}} and a valid labeling {{math|ùìÅ}} for {{mvar|f}} exists then there is no augmenting path from {{mvar|s}} to {{mvar|t}} in the residual graph {{math|''G''<sub>''f''</sub>&nbsp;}}. This can be proven by contradiction based on inequalities which arise in the labeling function when supposing that an augmenting path does exist. If the algorithm terminates, then all nodes in {{math|''V'' \\ {{(}}''s'', ''t''{{)}}}} are not active. This means all {{math|''v'' ‚àà ''V'' \\ {{(}}''s'', ''t''{{)}}}} have no excess flow, and with no excess the preflow {{mvar|f}} obeys the flow conservation constraint and can be considered a normal flow. This flow is the maximum flow according to the [[max-flow min-cut theorem]] since there is no augmenting path from {{mvar|s}} to {{mvar|t}}.<ref name=\"goldberg88\"/>\n\nTherefore, the algorithm will return the maximum flow upon termination.\n\n===Time complexity===\nIn order to bound the time complexity of the algorithm, we must analyze the number of push and relabel operations which occur within the main loop. The numbers of relabel, saturating push and nonsaturating push operations are analyzed separately.\n\nIn the algorithm, the relabel operation can be performed at most {{math|(2{{!}}&nbsp;''V''&nbsp;{{!}} ‚àí 1)({{!}}&nbsp;''V''&nbsp;{{!}} ‚àí 2) < 2{{!}}&nbsp;''V''&nbsp;{{!}}<sup>2</sup>}} times. This is because the labeling {{math|ùìÅ(''u'')}} value for any node ''u'' can never decrease, and the maximum label value is at most {{math|2{{!}}&nbsp;''V''&nbsp;{{!}} ‚àí 1}} for all nodes. This means the relabel operation could potentially be performed {{math|2{{!}}&nbsp;''V''&nbsp;{{!}} ‚àí 1 }} times for all nodes {{math|''V'' \\ {{(}}''s'', ''t''{{)}}}} (i.e.  {{math|{{!}}&nbsp;''V''&nbsp;{{!}} ‚àí 2}}). This results in a bound of {{math|''O''(''V''<sup>&nbsp;2</sup>)}} for the relabel operation.\n\nEach saturating push on an admissible arc {{math|(''u'', ''v'')}} removes the arc from {{math|''G''<sub>''f''</sub>&nbsp;}}. For the arc to be reinserted into {{math|''G''<sub>''f''</sub>&nbsp;}} for another saturating push, {{mvar|v}} must first be relabeled, followed by a push on the arc {{math|(''v'', ''u'')}}, then {{mvar|u}} must be relabeled. In the process, {{math|ùìÅ(''u'')}} increases by at least two. Therefore, there are {{math|''O''(''V'')}} saturating pushes on {{math|(''u'', ''v'')}}, and the total number of saturating pushes is at most {{math|2{{!}}&nbsp;''V''&nbsp;{{!}}{{!}}&nbsp;''E''&nbsp;{{!}}}}. This results in a time bound of {{math|''O''(''VE'')}} for the saturating push operations.\n\nBounding the number of nonsaturating pushes can be achieved via a [[Potential method|potential argument]]. We use the potential function {{math|Œ¶ {{=}} ‚àë<sub>[''u'' ‚àà ''V'' ‚àß ''x''<sub>''f''</sub>&nbsp;(''u'') > 0]</sub> ùìÅ(''u'')}} (i.e. {{math|Œ¶}} is the sum of the labels of all active nodes). It is obvious that {{math|Œ¶}} is {{math|0}} initially and stays nonnegative throughout the execution of the algorithm. Both relabels and saturating pushes can increase {{math|Œ¶}}. However, the value of {{math|Œ¶}} must be equal to 0 at termination since there cannot be any remaining active nodes at the end of the algorithm's execution. This means that over the execution of the algorithm, the nonsaturating pushes must make up the difference of the relabel and saturating push operations in order for {{math|Œ¶}} to terminate with a value of 0.\nThe relabel operation can increase {{math|Œ¶}} by at most {{math|(2{{!}}&nbsp;''V''&nbsp;{{!}} ‚àí 1)({{!}}&nbsp;''V''&nbsp;{{!}} ‚àí 2)}}. A saturating push on {{math|(''u'', ''v'')}} activates {{mvar|v}} if it was inactive before the push, increasing {{math|Œ¶}} by at most {{math|2{{!}}&nbsp;''V''&nbsp;{{!}} ‚àí 1}}. Hence, the total contribution of all saturating pushes operations to {{math|Œ¶}} is at most {{math|(2{{!}}&nbsp;''V''&nbsp;{{!}} ‚àí 1)(2{{!}}&nbsp;''V''&nbsp;{{!}}{{!}}&nbsp;''E''&nbsp;{{!}})}}. A nonsaturating push on {{math|(''u'', ''v'')}} always deactivates {{mvar|u}}, but it can also activate {{mvar|v}} as in a saturating push. As a result, it decreases {{math|Œ¶}} by at least {{math|ùìÅ(''u'') ‚àí ùìÅ(''v'') {{=}} 1}}. Since relabels and saturating pushes increase {{math|Œ¶}}, the total number of nonsaturating pushes must make up the difference of {{math|(2{{!}}&nbsp;''V''&nbsp;{{!}} ‚àí 1)({{!}}&nbsp;''V''&nbsp;{{!}} ‚àí 2) + (2{{!}}&nbsp;''V''&nbsp;{{!}} ‚àí 1)(2{{!}}&nbsp;''V''&nbsp;{{!}}{{!}}&nbsp;''E''&nbsp;{{!}}) ‚â§ 4{{!}}&nbsp;''V''&nbsp;{{!}}<sup>2</sup>{{!}}&nbsp;''E''&nbsp;{{!}}}}. This results in a time bound of {{math|''O''(''V''<sup>&nbsp;2</sup>''E'')}} for the nonsaturating push operations.\n\nIn sum, the algorithm executes {{math|''O''(''V''<sup>&nbsp;2</sup>)}} relabels, {{math|''O''(''VE'')}} saturating pushes and {{math|''O''(''V''<sup>&nbsp;2</sup>''E'')}} nonsaturating pushes. Data structures can be designed to pick and execute an applicable operation in {{math|''O''(1)}} time. Therefore, the time complexity of the algorithm is {{math|''O''(''V''<sup>&nbsp;2</sup>''E'')}}.<ref name=\"clrs26\"/><ref name=\"goldberg88\"/>\n\n===Example===\nThe following is a sample execution of the generic push-relabel algorithm, as defined above, on the following simple network flow graph diagram.\n\n{{multiple image\n | align = center\n | width = 350\n | image1 = Push Relabel Algoritm Example - Initial Graph.svg\n | alt1 = Initial flow network graph\n | caption1 = Initial flow network graph\n | image2 = Push-Relabel Algorithm Example - Final Network Graph.svg\n | alt2 = Final maximum flow network graph\n | caption2 = Final maximum flow network graph\n }}\n\n{{clear}}\n \nIn the example, the {{nowrap|''h''}} and {{nowrap|''e''}} values denote the label {{math|ùìÅ}} and excess {{math|''x''<sub>''f''</sub>&nbsp;}}, respectively, of the node during the execution of the algorithm. Each residual graph in the example only contains the residual arcs with a capacity larger than zero. Each residual graph may contain multiple iterations of the {{nowrap|''perform operation''}} loop.\n \n{{clear}}\n\n{| class=\"wikitable\"\n|-\n! Algorithm Operation(s) !! Residual Graph\n|-\n| Initialise the residual graph by setting the preflow to values 0 and initialising the labeling. || [[File:Push-Relabel Algorithm Example - Step 1.svg|Step 1|350px]]\n|-\n| Initial saturating push is performed across all preflow arcs out of the source, {{mvar|s}}. || [[File:Push-Relabel Algorithm Example - Step 2.svg|Step 2|350px]]\n|-\n|rowspan=\"2\"| Node {{mvar|a}} is relabeled in order to push its excess flow towards the sink, {{mvar|t}}.\nThe excess at {{mvar|a}} is then pushed to {{mvar|b}} then {{mvar|d}} in two subsequent saturating pushes; which still leaves {{mvar|a}} with some excess.\n|-\n| [[File:Push-Relabel Algorithm Example - Step 3.svg|Step 3|350px]] \n|-\n|rowspan=\"2\"| Once again, {{mvar|a}} is relabeled in order to push its excess along its last remaining positive residual (i.e. push the excess back to {{mvar|s}}).\nThe node {{mvar|a}} is then removed from the set of active nodes. \n|-\n| [[File:Push-Relabel Algorithm Example - Step 4.svg|Step 4|350px]] \n|-\n| Relabel {{mvar|b}} and then push its excess to {{mvar|t}} and {{mvar|c}}. || [[File:Push-Relabel Algorithm Example - Step 5.svg|Step 5|350px]] \n|-\n| Relabel {{mvar|c}} and then push its excess to {{mvar|d}}. || [[File:Push-Relabel Algorithm Example - Step 6.svg|Step 6|350px]] \n|-\n| Relabel {{mvar|d}} and then push its excess to {{mvar|t}}. || [[File:Push-Relabel Algorithm Example - Step 7.svg|Step 7|350px]] \n|-\n|rowspan=\"2\"| This leaves the node {{mvar|b}} as the only remaining active node, but it cannot push its excess flow towards the sink.\nRelabel {{mvar|b}} and then push its excess towards the source, {{mvar|s}}, via the node {{mvar|a}}.\n|-\n| [[File:Push-Relabel Algorithm Example - Step 8.svg|Step 8|350px]] \n|-\n|rowspan=\"2\"| Push the last bit of excess at {{mvar|a}} back to the source, {{mvar|s}}.\nThere are no remaining active nodes. The algorithm terminates and returns the maximum flow of the graph (as seen above). \n|-\n| [[File:Push-Relabel Algorithm Example - Step 9.svg|Step 9|350px]] \n|}\n\nThe example (but with initial flow of 0) can be run [http://www.adrian-haarbach.de/idp-graph-algorithms/implementation/maxflow-push-relabel/index_en.html here] interactively.\n\n==Practical implementations==\nWhile the generic push‚Äìrelabel algorithm has {{math|''O''(''V''<sup>&nbsp;2</sup>''E'')}} time complexity, efficient implementations achieve {{math|''O''(''V''<sup>&nbsp;3</sup>)}} or lower time complexity by enforcing appropriate rules in selecting applicable push and relabel operations. The empirical performance can be further improved by heuristics.\n\n===\"Current-arc\" data structure and discharge operation===\nThe \"current-arc\" data structure is a mechanism for visiting the in- and out-neighbors of a node in the flow network in a static circular order. If a singly linked list of neighbors is created for a node, the data structure can be as simple as a pointer into the list that steps through the list and rewinds to the head when it runs off the end.\n\nBased on the \"current-arc\" data structure, the discharge operation can be defined. A discharge operation applies on an active node and repeatedly pushes flow from the node until it becomes inactive, relabeling it as necessary to create admissible arcs in the process.\n\n discharge(u):\n     while x<sub>f</sub>[u] > 0\n         if current-arc[u] has run off the end of neighbors[u]\n             relabel(u)\n             rewind current-arc[u]\n         else\n             let (u, v) = current-arc[u]\n             if (u, v) is admissible\n                 push(u, v)\n             let current-arc[u] point to the next neighbor of u\n\n===Active node selection rules===\nDefinition of the discharge operation reduces the push‚Äìrelabel algorithm to repeatedly selecting an active node to discharge. Depending on the selection rule, the algorithm exhibits different time complexities. For the sake of brevity, we ignore {{mvar|s}} and {{mvar|t}} when referring to the nodes in the following discussion.\n\n====FIFO selection rule====\nThe [[FIFO (computing and electronics)|FIFO]] push‚Äìrelabel algorithm<ref name=\"goldberg86\"/> organizes the active nodes into a queue. The initial active nodes can be inserted in arbitrary order. The algorithm always removes the node at the front of the queue for discharging. Whenever an inactive node becomes active, it is appended to the back of the queue.\n\nThe algorithm has {{math|''O''(''V''<sup>&nbsp;3</sup>)}} time complexity.\n\n====Relabel-to-front selection rule====\nThe relabel-to-front push‚Äìrelabel algorithm<ref name=\"clrs26\"/> organizes all nodes into a linked list and maintains the invariant that the list is [[Topological sorting|topologically sorted]] with respect to the admissible network. The algorithm scans the list from front to back and performs a discharge operation on the current node if it is active. If the node is relabeled, it is moved to the front of the list, and the scan is restarted from the front.\n\nThe algorithm also has {{math|''O''(''V''<sup>&nbsp;3</sup>)}} time complexity.\n\n====Highest label selection rule====\nThe highest-label push‚Äìrelabel algorithm<ref name=\"cheriyan88\"/> organizes all nodes into buckets indexed by their labels. The algorithm always selects an active node with the largest label to discharge.\n\nThe algorithm has {{math|''O''(''V''<sup>&nbsp;2</sup>{{sqrt|''E''}})}} time complexity. If the lowest-label selection rule is used instead, the time complexity becomes {{math|''O''(''V''<sup>&nbsp;2</sup>''E'')}}.<ref name=\"ahuja97\"/>\n\n===Implementation techniques===\nAlthough in the description of the generic push‚Äìrelabel algorithm above, {{math|ùìÅ(''u'')}} is set to zero for each node ''u'' other than {{mvar|s}} and {{mvar|t}} at the beginning, it is preferable to perform a backward [[breadth-first search]] from {{mvar|t}} to compute exact labels.<ref name=\"goldberg86\"/>\n\nThe algorithm is typically separated into two phases. Phase one computes a maximum pre-flow by discharging only active nodes whose labels are below {{mvar|n}}. Phase two converts the maximum preflow into a maximum flow by returning excess flow that cannot reach {{mvar|t}} to {{mvar|s}}. It can be shown that phase two has {{math|''O''(''VE'')}} time complexity regardless of the order of push and relabel operations and is therefore dominated by phase one. Alternatively, it can be implemented using flow decomposition.<ref name=\"amo93\"/>\n\nHeuristics are crucial to improving the empirical performance of the algorithm.<ref name=\"cherkassky95\"/> Two commonly used heuristics are the gap heuristic and the global relabeling heuristic.<ref name=\"goldberg86\"/><ref name=\"derigs89\"/> The gap heuristic detects gaps in the labeling function. If there is a label {{nowrap|0 < ùìÅ''{{'}}'' < {{!}}&nbsp;''V''&nbsp;{{!}}}} for which there is no node {{mvar|u}} such that {{math|ùìÅ(''u'') {{=}} ùìÅ''{{'}}''}}, then any node {{mvar|u}} with {{math|ùìÅ''{{'}}'' < ùìÅ(''u'') < {{!}}&nbsp;''V''&nbsp;{{!}}}} has been disconnected from {{mvar|t}} and can be relabeled to {{math|({{!}}&nbsp;''V''&nbsp;{{!}} + 1)}} immediately. The global relabeling heuristic periodically performs backward breadth-first search from {{mvar|t}} in {{math|''G''<sub>''f''</sub>&nbsp;}} to compute the exact labels of the nodes. Both heuristics skip unhelpful relabel operations, which are a bottleneck of the algorithm and contribute to the ineffectiveness of dynamic trees.<ref name=\"goldberg08\"/>\n\n==Sample implementations==\n{{hidden begin|border=1px #aaa solid|titlestyle=text-align:center|title=[[C (programming language)|C]] implementation}}\n<source lang=\"c\">\n#include <stdlib.h>\n#include <stdio.h>\n\n#define NODES 6\n#define MIN(X,Y) ((X) < (Y) ? (X) : (Y))\n#define INFINITE 10000000\n\nvoid push(const int * const * C, int ** F, int *excess, int u, int v) {\n  int send = MIN(excess[u], C[u][v] - F[u][v]);\n  F[u][v] += send;\n  F[v][u] -= send;\n  excess[u] -= send;\n  excess[v] += send;\n}\n\nvoid relabel(const int * const * C, const int * const * F, int *height, int u) {\n  int v;\n  int min_height = INFINITE;\n  for (v = 0; v < NODES; v++) {\n    if (C[u][v] - F[u][v] > 0) {\n      min_height = MIN(min_height, height[v]);\n      height[u] = min_height + 1;\n    }\n  }\n};\n\nvoid discharge(const int * const * C, int ** F, int *excess, int *height, int *seen, int u) {\n  while (excess[u] > 0) {\n    if (seen[u] < NODES) {\n      int v = seen[u];\n      if ((C[u][v] - F[u][v] > 0) && (height[u] > height[v])) {\n        push(C, F, excess, u, v);\n      } else {\n        seen[u] += 1;\n      }\n    } else {\n      relabel(C, F, height, u);\n      seen[u] = 0;\n    }\n  }\n}\n\nvoid moveToFront(int i, int *A) {\n  int temp = A[i];\n  int n;\n  for (n = i; n > 0; n--) {\n    A[n] = A[n-1];\n  }\n  A[0] = temp;\n}\n\nint pushRelabel(const int * const * C, int ** F, int source, int sink) {\n  int *excess, *height, *list, *seen, i, p;\n\n  excess = (int *) calloc(NODES, sizeof(int));\n  height = (int *) calloc(NODES, sizeof(int));\n  seen = (int *) calloc(NODES, sizeof(int));\n\n  list = (int *) calloc((NODES-2), sizeof(int));\n\n  for (i = 0, p = 0; i < NODES; i++){\n    if((i != source) && (i != sink)) {\n      list[p] = i;\n      p++;\n    }\n  }\n\n  height[source] = NODES;\n  excess[source] = INFINITE;\n  for (i = 0; i < NODES; i++)\n    push(C, F, excess, source, i);\n\n  p = 0;\n  while (p < NODES - 2) {\n    int u = list[p];\n    int old_height = height[u];\n    discharge(C, F, excess, height, seen, u);\n    if (height[u] > old_height) {\n      moveToFront(p, list);\n      p = 0;\n    } else {\n      p += 1;\n    }\n  }\n  int maxflow = 0;\n  for (i = 0; i < NODES; i++)\n    maxflow += F[source][i];\n\n  free(list);\n\n  free(seen);\n  free(height);\n  free(excess);\n\n  return maxflow;\n}\n\nvoid printMatrix(const int * const * M) {\n  int i,j;\n  for (i = 0; i < NODES; i++) {\n    for (j = 0; j < NODES; j++)\n      printf(\"%d\\t\",M[i][j]);\n    printf(\"\\n\");\n  }\n}\n\nint main(void) {\n  int **flow, **capacities, i;\n  flow = (int **) calloc(NODES, sizeof(int*));\n  capacities = (int **) calloc(NODES, sizeof(int*));\n  for (i = 0; i < NODES; i++) {\n    flow[i] = (int *) calloc(NODES, sizeof(int));\n    capacities[i] = (int *) calloc(NODES, sizeof(int));\n  }\n\n  //Sample graph\n  capacities[0][1] = 2;\n  capacities[0][2] = 9;\n  capacities[1][2] = 1;\n  capacities[1][3] = 0;\n  capacities[1][4] = 0;\n  capacities[2][4] = 7;\n  capacities[3][5] = 7;\n  capacities[4][5] = 4;\n\n  printf(\"Capacity:\\n\");\n  printMatrix(capacities);\n\n  printf(\"Max Flow:\\n%d\\n\", pushRelabel(capacities, flow, 0, 5));\n\n  printf(\"Flows:\\n\");\n  printMatrix(flow);\n\n  return 0;\n}\n</source>\n{{hidden end}}\n\n{{hidden begin|border=1px #aaa solid|titlestyle=text-align:center|title=[[Python (programming language)|Python]] implementation}}\n<source lang=\"python\">\n  def relabel_to_front(C, source, sink):\n     n = len(C) # C is the capacity matrix\n     F = [[0] * n for _ in xrange(n)]\n     # residual capacity from u to v is C[u][v] - F[u][v]\n\n     height = [0] * n # height of node\n     excess = [0] * n # flow into node minus flow from node\n     seen   = [0] * n # neighbours seen since last relabel\n     # node \"queue\"\n     nodelist = [i for i in xrange(n) if i != source and i != sink]\n\n     def push(u, v):\n         send = min(excess[u], C[u][v] - F[u][v])\n         F[u][v] += send\n         F[v][u] -= send\n         excess[u] -= send\n         excess[v] += send\n\n     def relabel(u):\n         # find smallest new height making a push possible,\n         # if such a push is possible at all\n         min_height = ‚àû\n         for v in xrange(n):\n             if C[u][v] - F[u][v] > 0:\n                 min_height = min(min_height, height[v])\n                 height[u] = min_height + 1\n\n     def discharge(u):\n         while excess[u] > 0:\n             if seen[u] < n: # check next neighbour\n                 v = seen[u]\n                 if C[u][v] - F[u][v] > 0 and height[u] > height[v]:\n                     push(u, v)\n                 else:\n                     seen[u] += 1\n             else: # we have checked all neighbours. must relabel\n                 relabel(u)\n                 seen[u] = 0\n\n     height[source] = n # longest path from source to sink is less than n long\n     excess[source] = ‚àû # send as much flow as possible to neighbours of source\n     for v in xrange(n):\n         push(source, v)\n\n     p = 0\n     while p < len(nodelist):\n         u = nodelist[p]\n         old_height = height[u]\n         discharge(u)\n         if height[u] > old_height:\n             nodelist.insert(0, nodelist.pop(p)) # move to front of list\n             p = 0 # start from front of list\n         else:\n             p += 1\n\n     return sum(F[source])\n</source>\n{{hidden end}}\n\n== References ==\n{{reflist|2|refs=\n<ref name=\"clrs26\">{{Cite book | isbn = 978-0262032933 | title = Introduction to Algorithms | edition = 2nd | last1 = Cormen | first1 = T. H. | authorlink1 = Thomas H. Cormen | year = 2001 | publisher = The MIT Press | last2 = Leiserson | first2 = C. E. | authorlink2 = Charles E. Leiserson | last3 = Rivest | first3 = R. L. | authorlink3 = Ron Rivest | last4 = Stein | first4 = C. | authorlink4 = Clifford Stein | chapter = ¬ß26 Maximum flow | pages = 643‚Äì698| title-link = Introduction to Algorithms }}</ref>\n<ref name=\"goldberg86\">{{cite book|doi=10.1145/12130.12144|chapter=A new approach to the maximum flow problem|title=Proceedings of the eighteenth annual ACM symposium on Theory of computing ‚Äì STOC '86|pages=136|year=1986|last1=Goldberg|first1=A V|last2=Tarjan|first2=R E|isbn=978-0897911931}}</ref>\n<ref name=\"goldberg88\">{{cite journal|doi=10.1145/48014.61051|title=A new approach to the maximum-flow problem|journal=Journal of the ACM|volume=35|issue=4|pages=921|year=1988|last1=Goldberg|first1=Andrew V.|last2=Tarjan|first2=Robert E.}}</ref>\n<ref name=\"sv82\">{{cite journal|doi=10.1016/0196-6774(82)90013-X|title=An O(n2log n) parallel max-flow algorithm|journal=Journal of Algorithms|volume=3|issue=2|pages=128‚Äì146|year=1982|last1=Shiloach|first1=Yossi|last2=Vishkin|first2=Uzi}}</ref>\n<ref name=\"goldberg88\">{{cite journal|doi=10.1145/48014.61051|title=A new approach to the maximum-flow problem|journal=Journal of the ACM|volume=35|issue=4|pages=921|year=1988|last1=Goldberg|first1=Andrew V.|last2=Tarjan|first2=Robert E.}}</ref>\n<ref name=\"cheriyan88\">{{cite book|doi=10.1007/3-540-50517-2_69|chapter=Analysis of preflow push algorithms for maximum network flow|title=Foundations of Software Technology and Theoretical Computer Science|volume=338|pages=30|series=Lecture Notes in Computer Science|year=1988|last1=Cheriyan|first1=J.|last2=Maheshwari|first2=S. N.|isbn=978-3-540-50517-4}}</ref>\n<ref name=\"derigs89\">{{cite journal|doi=10.1007/BF01415937|title=Implementing Goldberg's max-flow-algorithm ? A computational investigation|journal=ZOR Zeitschrift f√ºr Operations Research Methods and Models of Operations Research|volume=33|issue=6|pages=383|year=1989|last1=Derigs|first1=U.|last2=Meier|first2=W.}}</ref>\n<ref name=\"ahuja91\">{{cite journal|doi=10.1002/1520-6750(199106)38:3<413::AID-NAV3220380310>3.0.CO;2-J|title=Distance-directed augmenting path algorithms for maximum flow and parametric maximum flow problems|journal=Naval Research Logistics|volume=38|issue=3|pages=413|year=1991|last1=Ahuja|first1=Ravindra K.|last2=Orlin|first2=James B.|citeseerx=10.1.1.297.5698}}</ref>\n<ref name=\"amo93\">{{Cite book | isbn = 978-0136175490 | title = Network Flows: Theory, Algorithms, and Applications | edition = 1st | last1 = Ahuja | first1 = R. K. | year = 1993 | publisher = Prentice Hall | last2 = Magnanti | first2 = T. L. | last3 = Orlin | first3 = J. B. | pages = }}</ref>\n<ref name=\"cherkassky95\">{{cite book|doi=10.1007/3-540-59408-6_49|chapter=On implementing push-relabel method for the maximum flow problem|title=Integer Programming and Combinatorial Optimization|volume=920|pages=157|series=Lecture Notes in Computer Science|year=1995|last1=Cherkassky|first1=Boris V.|last2=Goldberg|first2=Andrew V.|isbn=978-3-540-59408-6|citeseerx=10.1.1.150.3609}}</ref>\n<ref name=\"ahuja97\">{{cite journal|doi=10.1016/S0377-2217(96)00269-X|title=Computational investigations of maximum flow algorithms|journal=European Journal of Operational Research|volume=97|issue=3|pages=509|year=1997|last1=Ahuja|first1=Ravindra K.|last2=Kodialam|first2=Murali|last3=Mishra|first3=Ajay K.|last4=Orlin|first4=James B.|citeseerx=10.1.1.297.2945}}</ref>\n<ref name=\"goldberg97\">{{cite journal|doi=10.1006/jagm.1995.0805|title=An Efficient Implementation of a Scaling Minimum-Cost Flow Algorithm|journal=Journal of Algorithms|volume=22|pages=1‚Äì29|year=1997|last1=Goldberg|first1=Andrew V}}</ref>\n<ref name=\"goldberg08\">{{cite book|doi=10.1007/978-3-540-87744-8_39|chapter=The Partial Augment‚ÄìRelabel Algorithm for the Maximum Flow Problem|title=Algorithms ‚Äì ESA 2008|volume=5193|pages=466‚Äì477|series=Lecture Notes in Computer Science|year=2008|last1=Goldberg|first1=Andrew V.|isbn=978-3-540-87743-1|citeseerx=10.1.1.150.5103}}</ref>\n<ref name=\"goldberg2014\">{{cite journal|doi=10.1145/2628036|title=Efficient maximum flow algorithms|journal=Communications of the ACM|volume=57|issue=8|pages=82|year=2014|last1=Goldberg|first1=Andrew V.|last2=Tarjan|first2=Robert E.}}</ref>\n}}\n\n{{DEFAULTSORT:Push-relabel maximum flow algorithm}}\n[[Category:Network flow problem]]\n[[Category:Graph algorithms]]"
    },
    {
      "title": "Barrier function",
      "url": "https://en.wikipedia.org/wiki/Barrier_function",
      "text": "In constrained [[optimization (mathematics)|optimization]], a field of [[mathematics]],  a '''barrier function''' is a [[continuous function]] whose value on a point increases to infinity as the point approaches the boundary of the [[Candidate solution|feasible region]] of an optimization problem.<ref>{{cite book|title = Numerical Optimization | first=Jorge| last = Nocedal |first2=Stephen |last2=Wright| year=1999 | publisher=Springer | location=New York, NY| isbn=0-387-98793-2}}{{page number needed|date=March 2016}}</ref> Such functions are used to replace inequality [[Constraint (mathematics)|constraints]] by a penalizing term in the objective function that is easier to handle.\n\nThe two most common types of barrier functions are [[inverse barrier function]]s and logarithmic barrier functions. Resumption of interest in logarithmic barrier functions was motivated by their connection with primal-dual [[interior point method]]s.\n\n==Motivation==\nConsider the following constrained optimization problem:\n\n:minimize {{math|''f''(''x'')}}\n:subject to {{math|''x'' ‚â• ''b''}}\n\nwhere {{mvar|b}} is some constant. If one wishes to remove the inequality constraint, the problem can be re-formulated as\n\n:minimize {{math|''f''(''x'') + ''c''(''x'')}},\n:where {{math|''c''(''x'') {{=}} ‚àû}} if {{math|''x'' < ''b''}}, and zero otherwise.\n\nThis problem is equivalent to the first. It gets rid of the inequality, but introduces the issue that the penalty function {{mvar|c}}, and therefore the objective function {{math|''f''(''x'') + ''c''(''x'')}}, is [[discontinuous]], preventing the use of [[calculus]] to solve it.\n\nA barrier function, now, is a continuous approximation {{mvar|g}} to {{mvar|c}} that tends to infinity as {{mvar|x}} approaches {{mvar|b}} from above. Using such a function, a new optimization problem is formulated, viz.\n\n:minimize {{math|''f''(''x'') + ''Œº g''(''x'')}}\n\nwhere {{math|''Œº'' > 0}} is a free parameter. This problem is not equivalent to the original, but as {{mvar|Œº}} approaches zero, it becomes an ever-better approximation.<ref>{{cite book |first=Robert J. |last=Vanderbei |authorlink=Robert J. Vanderbei |title=Linear Programming: Foundations and Extensions |year=2001 |publisher=Kluwer |pages=277‚Äì279}}</ref>\n\n==Logarithmic barrier function==\n\nFor logarithmic barrier functions, <math>g(x,b)</math> is defined as <math>-\\log(b-x)</math> when <math>x < b</math> and <math>\\infty</math> otherwise (in 1 dimension. See below for a definition in higher dimensions). This essentially relies on the fact that <math>\\log(t)</math> tends to negative infinity as <math>t</math> tends to 0.\n\nThis introduces a gradient to the function being optimized which favors less extreme values of <math>x</math> (in this case values lower than <math>b</math>), while having relatively low impact on the function away from these extremes.\n\nLogarithmic barrier functions may be favored over less computationally expensive [[inverse barrier functions]] depending on the function being optimized.\n\n===Higher dimensions===\n\nExtending to higher dimensions is simple, provided each dimension is independent. For each variable <math>x_i</math> which should be limited to be strictly lower than <math>b_i</math>, add <math>-\\log(b_i-x_i)</math>.\n\n===Formal definition===\nMinimize <math> \\mathbf c^Tx</math> subject to <math>\\mathbf a_i^T x \\le b_i, i = 1,\\ldots,m</math>\n\nAssume strictly feasible: <math> \\{\\mathbf x|A x < b\\}\\ne\\emptyset </math>\n\nDefine '''logarithmic barrier''' <math>\\Phi(x) = \\begin{cases}\n\n\\sum_{i=1}^m -\\log(b_i - a_i^Tx) & \\text{for } Ax<b \\\\\n+\\infty & \\text{otherwise}\n\\end{cases}</math>\n\n==See also==\n* [[Penalty method]]\n* [[Augmented Lagrangian method]]\n\n==Notes==\n\n{{Empty section|date=February 2016}}\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://www.seas.ucla.edu/~vandenbe/ee236a/lectures/barrier.pdf Lecture 14: Barrier method] from Professor Lieven Vandenberghe of [[UCLA]]\n\n{{Commons category|Newton Method}}\n{{optimization algorithms}}\n\n[[Category:Constraint programming]]\n[[Category:Convex optimization]]\n[[Category:Types of functions]]\n\n\n{{mathapplied-stub}}"
    },
    {
      "title": "Biconvex optimization",
      "url": "https://en.wikipedia.org/wiki/Biconvex_optimization",
      "text": "'''Biconvex optimization''' is a generalization of [[convex optimization]] where the objective function and the constraint set can be biconvex. There are methods that can find the global optimum of these problems.<ref name=Survey>{{cite journal|last=Gorski|first=Jochen|author2=Pfeuffer, Frank |author3=Klamroth, Kathrin |title=Biconvex sets and optimization with biconvex functions: a survey and extensions|journal=Mathematical Methods of Operations Research|date=22 June 2007|volume=66|issue=3|pages=373‚Äì407|doi=10.1007/s00186-007-0161-1|url=http://www2.math.uni-wuppertal.de/~klamroth/publications/gopfkl07.pdf}}</ref><ref>{{cite book|last=Floudas|first=Christodoulos A.|title=Deterministic global optimization : theory, methods, and applications|year=2000|publisher=Kluwer Academic Publ.|location=Dordrecht [u.a.]|isbn=978-0-7923-6014-8|url=https://www.springer.com/mathematics/book/978-0-7923-6014-8}}</ref> \n\nA set <math>B \\subset X\\times Y </math> is called a biconvex set on <math>X\\times Y </math> if for every fixed <math>y\\in Y </math>, <math>B_y = \\{x \\in X: (x,y) \\in B\\}</math> is a convex set in <math>X </math> and for every fixed <math>x\\in X </math>, <math>B_x = \\{y \\in Y: (x,y) \\in B\\}</math> is a convex set in <math>Y </math>.\n\nA function <math>f(x, y): B \\to \\mathbb{R} </math> is called a biconvex function if fixing <math>x</math>, <math> f_x(y) = f(x, y)</math> is convex over <math> Y </math> and fixing <math>y</math>, <math> f_y(x) = f(x, y)</math> is convex over <math> X </math>.\n\nA common practice for solving a biconvex problem (which does not guarantee global optimality of the solution) is alternatively updating <math>x, y</math> by fixing one of them and solving the corresponding convex optimization problem.<ref name=Survey />\n\nThe generalization to functions of more than two arguments\nis called a '''block multi-convex''' function.\nA function\n<math>\nf(x_1,\\ldots,x_K) \\to \\mathbb{R}\n</math>\nis block multi-convex\niff it is convex with respect to each of the individual arguments\nwhile holding all others fixed.\n<ref>{{cite journal\n|last=Chen\n|first=Caihua\n|title=\"The direct extension of ADMM for multi-block convex minimization problems is not necessarily convergent\"\n|journal=\"Math. Prof.\"\n|volume=155\n|pages=57-59\n|doi=10.1007/s10107-014-0826-5\n|year=2016\n}}</ref>\n\n== References ==\n{{reflist}}\n{{Optimization algorithms}}\n[[Category:Convex optimization]]\n[[Category:Generalized convexity]]\n\n\n{{mathapplied-stub}}"
    },
    {
      "title": "Conic optimization",
      "url": "https://en.wikipedia.org/wiki/Conic_optimization",
      "text": "{{More footnotes|date=October 2011}}\n\n'''Conic optimization''' is a subfield of [[convex optimization]] that studies problems consisting of minimizing a [[convex function]] over the intersection of an [[affine subspace]] and a [[convex cone]].\n\nThe class of conic optimization problems includes some of the most well known classes of convex optimization problems, namely [[linear programming|linear]] and [[semidefinite programming]].\n\n==Definition==\n\nGiven a [[real number|real]] [[vector space]] ''X'', a [[convex function|convex]], real-valued [[function (mathematics)|function]]\n\n:<math>f:C \\to \\mathbb R</math>\n\ndefined on a [[convex cone]] <math>C \\subset X</math>, and an affine subspace <math>\\mathcal{H}</math> defined by a set of [[Affine transformation|affine]] constraints <math>h_i(x) = 0 \\ </math>, a conic optimization problem is to find the point <math>x</math> in <math>C \\cap \\mathcal{H} </math> for which the number <math>f(x)</math> is smallest.\n\nExamples of <math> C </math> include the positive [[orthant]] <math>\\mathbb{R}_+^n = \\left\\{ x \\in \\mathbb{R}^n : \\, x \\geq \\mathbf{0}\\right\\} </math>, [[Positive-semidefinite matrix|positive semidefinite]] matrices <math>\\mathbb{S}^n_{+}</math>, and the '''second-order cone''' <math>\\left \\{ (x,t) \\in \\mathbb{R}^{n}\\times \\mathbb{R} : \\lVert x \\rVert \\leq t \\right \\} </math>.  Often <math>f \\ </math> is a linear function, in which case the conic optimization problem reduces to a [[linear program]], a [[semidefinite programming|semidefinite program]], and a [[second order cone programming|second order cone program]], respectively.\n\n==Duality==\nCertain special cases of conic optimization problems have notable closed-form expressions of their dual problems.\n\n===Conic LP===\nThe dual of the conic linear program\n\n:minimize <math>c^T x \\ </math>\n:subject to <math>Ax = b, x \\in C \\ </math>\n\nis\n\n:maximize <math>b^T y \\ </math>\n:subject to <math>A^T y + s= c, s \\in C^* \\ </math>\n\nwhere <math>C^*</math> denotes the [[dual cone]] of <math>C \\ </math>.\n\nWhilst weak duality holds in conic linear programming, strong duality does not necessarily hold.<ref name=\"ConicDuality\" />\n\n===Semidefinite Program===\nThe dual of a semidefinite program in inequality form\n\n: minimize <math>c^T x \\ </math> \n: subject to <math>x_1 F_1 + \\cdots + x_n F_n + G \\leq 0</math>\n\nis given by\n\n: maximize <math>\\mathrm{tr}\\ (GZ)\\ </math> \n: subject to <math>\\mathrm{tr}\\ (F_i Z) +c_i =0,\\quad i=1,\\dots,n</math>\n: <math>Z \\geq0</math>\n\n==References==\n{{Reflist|refs=\n<ref name=\"ConicDuality\">{{cite web|title=Duality in Conic Programming|url=https://people.smp.uq.edu.au/YoniNazarathy/teaching_projects/studentWork/Duality.pdf}}</ref>\n}}\n\n==External links==\n* {{cite book|title=Convex Optimization|first1=Stephen P.|last1=Boyd|first2=Lieven|last2=Vandenberghe|year=2004|publisher=Cambridge University Press|isbn=978-0-521-83378-3|url=http://www.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf|format=pdf|accessdate=October 15, 2011}}\n* [http://www.mosek.com MOSEK] Software capable of solving conic optimization problems.\n\n[[Category:Convex optimization]]"
    },
    {
      "title": "Danskin's theorem",
      "url": "https://en.wikipedia.org/wiki/Danskin%27s_theorem",
      "text": "In [[convex analysis]], '''Danskin's theorem''' is a [[theorem]] which provides information about the [[derivative]]s of a [[function (mathematics)|function]] of the form\n\n:<math>f(x) = \\max_{z \\in Z} \\phi(x,z).</math>\n\nThe theorem has applications in [[optimization (mathematics)|optimization]], where it sometimes is used to solve [[minimax]] problems. The original theorem by J. M. Danskin, given in his 1967, monograph \"The Theory of Max-Min and its Applications to Weapons Allocation Problems,\" Springer, NY, provides a formula for the directional derivative of the maximum of a (not necessarily convex) directionally differentiable function. When adapted to the case of a convex function, this formula yields the following theorem given in somewhat more general form as Proposition A.22 in the 1971 Ph.D. Thesis by D. P. Bertsekas, \"Control of Uncertain Systems with a Set-Membership Description of the Uncertainty\". A proof of the following version can be found in the 1999 book \"Nonlinear Programming\" by Bertsekas (Section B.5).\n\n== Statement ==\nThe theorem applies to the following situation. Suppose <math>\\phi(x,z)</math> is a [[continuous function]] of two arguments,\n:<math>\\phi: {\\mathbb R}^n \\times Z \\rightarrow {\\mathbb R}</math>\nwhere <math>Z \\subset {\\mathbb R}^m</math> is a [[compact set]]. Further assume that <math>\\phi(x,z)</math> is [[convex function|convex]] in <math>x</math> for every <math>z \\in Z</math>.\n\nUnder these conditions, Danskin's theorem provides conclusions regarding the [[differentiability]] of the function\n:<math>f(x) = \\max_{z \\in Z} \\phi(x,z).</math>\nTo state these results, we define the set of maximizing points <math>Z_0(x)</math> as\n:<math>Z_0(x) = \\left\\{ \\overline{z} : \\phi(x,\\overline{z}) = \\max_{z \\in Z} \\phi(x,z)\\right\\}.</math>\n\nDanskin's theorem then provides the following results.\n\n;Convexity\n: <math>f(x)</math> is [[convex function|convex]].\n;Directional derivatives\n: The [[directional derivative]] of <math>f(x)</math> in the direction <math>y</math>, denoted <math>D_y\\ f(x)</math>, is given by\n::<math>D_y f(x) = \\max_{z \\in Z_0(x)} \\phi'(x,z;y),</math>\n: where <math>\\phi'(x,z;y)</math> is the directional derivative of the function <math>\\phi(\\cdot,z)</math> at <math>x</math> in the direction <math>y</math>.\n;Derivative\n: <math>f(x)</math> is [[Differentiable function|differentiable]] at <math>x</math> if <math>Z_0(x)</math> consists of a single element <math>\\overline{z}</math>. In this case, the [[derivative]] of <math>f(x)</math> (or the [[gradient]] of <math>f(x)</math> if <math>x</math> is a vector) is given by\n:: <math>\\frac{\\partial f}{\\partial x} = \\frac{\\partial \\phi(x,\\overline{z})}{\\partial x}.</math>\n;Subdifferential\n:If <math>\\phi(x,z)</math> is differentiable with respect to <math>x</math> for all <math>z \\in Z</math>, and if <math>\\partial \\phi/\\partial x</math> is continuous with respect to <math>z</math> for all <math>x</math>, then the [[subdifferential]] of <math>f(x)</math> is given by\n:: <math>\\partial f(x) = \\mathrm{conv} \\left\\{ \\frac{\\partial \\phi(x,z)}{\\partial x} : z \\in Z_0(x) \\right\\}</math>\n: where <math>\\mathrm{conv}</math> indicates the [[convex hull]] operation.\n;Extension\nThe 1971 Ph.D. Thesis by Bertsekas <ref>[http://web.mit.edu/dimitrib/www/phdthesis.pdf Ph.D. Thesis by D. P. Bertsekas]</ref> (Proposition A.22) proves a more general result, which does not require that <math>\\phi(\\cdot,z)</math> is differentiable. Instead it assumes that <math>\\phi(\\cdot,z)</math> is an extended real-valued closed proper convex function for each <math>z</math> in the compact set <math>Z</math>, that <math>int(dom(f))</math>, the interior of the effective domain of <math>f</math>, is nonempty, and that <math>\\phi</math> is continuous on the set <math>int(dom(f))\\times Z</math>. Then  for all <math>x</math> in <math>int(dom(f))</math>, the subdifferential of <math>f</math> at <math>x</math> is given by\n:: <math>\\partial f(x) = \\mathrm{conv} \\left\\{ \\partial \\phi(x,z) : z \\in Z_0(x) \\right\\}</math>\nwhere <math>\\partial \\phi(x,z)</math> is the subdifferential of <math>\\phi(\\cdot,z)</math> at <math>x</math> for any <math>z</math> in <math>Z</math>.\n\n== See also ==\n* [[Maximum theorem]]\n* [[Envelope theorem]]\n* [[Hotellings Lemma]]\n\n== References ==\n{{Reflist}}\n* {{cite book\n| last = Danskin\n| first = John M.\n| title = The Theory of Max-Min and its Applications to Weapons Allocation Problems\n| publisher = Springer\n| date = 1967\n| location = NY}}\n* {{cite book\n| last = Bertsekas\n| first = Dimitri P.\n| title = Control of Uncertain Systems with a Set-Membership Description of Uncertainty\n| publisher = PhD Thesis, MIT\n| date = 1971\n| location = Cambridge, MA}}\n* {{cite book\n| last = Bertsekas\n| first = Dimitri P.\n| title = Nonlinear Programming\n| publisher = Athena Scientific\n| date = 1999\n| pages = 737\n| location = Belmont, MA\n| isbn = 1-886529-00-0 }}\n\n[[Category:Theorems in analysis]]\n[[Category:Convex optimization]]"
    },
    {
      "title": "Entropy maximization",
      "url": "https://en.wikipedia.org/wiki/Entropy_maximization",
      "text": "{{Expert-subject|Mathematics|date=November 2008}}\n\nAn '''[[entropy]] maximization problem''' is a [[convex optimization]] problem of the form\n\n:maximize <math>f_0(\\vec{x}) = - \\sum_{i=1}^n x_i \\log x_i </math>\n:subject to <math>A\\vec{x} \\leq b, \\quad \\mathbf{1}^T \\vec{x}  = |\\vec{x}|_1 =1</math>\n\nwhere <math>\\vec{x} \\in \\mathbb{R}^n_{++}</math> is the optimization variable, <math>A\\in\\mathbb{R}^{m\\times n} </math> and <math> b \\in\\mathbb{R}^m </math> are problem parameters, and <math>\\mathbf{1}</math> denotes a vector whose components are all 1.\n\n==See also==\n* [[Principle of maximum entropy]]\n\n==External links==\n* {{cite book\n        |last=Boyd\n        |first=Stephen\n        |author2=Lieven Vandenberghe\n        |title=Convex Optimization\n        |publisher=[[Cambridge University Press]]\n        |date=2004\n        |pages=362\n        |isbn=0-521-83378-7\n        |url=http://www.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf\n        |accessdate=2008-08-24\n}}\n\n[[Category:Convex optimization]]\n\n\n{{mathapplied-stub}}"
    },
    {
      "title": "Fenchel's duality theorem",
      "url": "https://en.wikipedia.org/wiki/Fenchel%27s_duality_theorem",
      "text": "In mathematics, '''Fenchel's duality theorem''' is a result in the theory of convex functions named after [[Werner Fenchel]].\n\nLet ''&fnof;'' be a [[proper convex function]] on '''R'''<sup>''n''</sup> and let ''g'' be a proper concave function on '''R'''<sup>''n''</sup>. Then, if regularity conditions are satisfied,\n\n:<math>\\inf_x ( f(x)-g(x) ) = \\sup_p ( g_\\star(p)-f^\\star(p)).</math>\n\nwhere ''&fnof;''<sup>&nbsp;*</sup> is the [[convex conjugate]] of ''&fnof;'' (also referred to as the Fenchel&ndash;Legendre transform) and ''g''<sub>&nbsp;*</sub> is the [[concave conjugate]] of ''g''. That is,\n\n:<math>f^{\\star} \\left( x^{*} \\right) := \\sup \\left \\{ \\left. \\left\\langle x^{*} , x \\right\\rangle - f \\left( x \\right) \\right| x \\in \\mathbb{R}^n \\right\\}</math>\n\n:<math>g_{\\star} \\left( x^{*} \\right) := \\inf \\left \\{ \\left. \\left\\langle x^{*} , x \\right\\rangle - g \\left( x \\right) \\right| x \\in \\mathbb{R}^n \\right\\}</math>\n\n==Mathematical theorem==\nLet ''X'' and ''Y'' be [[Banach spaces]], <math>f: X \\to \\mathbb{R} \\cup \\{+\\infty\\}</math> and <math>g: Y \\to \\mathbb{R} \\cup \\{+\\infty\\}</math> be convex functions and <math>A: X \\to Y</math> be a [[bounded operator|bounded]] [[linear map]].  Then the Fenchel problems:\n:<math>p^* = \\inf_{x \\in X} \\{f(x) + g(Ax)\\}</math>\n:<math>d^* = \\sup_{y^* \\in Y^*} \\{-f^*(A^*y^*) - g^*(-y^*)\\}</math>\nsatisfy [[weak duality]], i.e. <math>p^* \\geq d^*</math>.  Note that <math>f^*,g^*</math> are the convex conjugates of ''f'',''g'' respectively, and <math>A^*</math> is the [[adjoint operator]].  The [[perturbation function]] for this [[dual problem]] is given by <math>F(x,y) = f(x) + g(Ax - y)</math>.\n\nSuppose that ''f'',''g'', and ''A'' satisfy either\n# ''f'' and ''g'' are [[lower semi-continuous]] and <math>0 \\in \\operatorname{core}(\\operatorname{dom}g - A \\operatorname{dom}f)</math> where <math>\\operatorname{core}</math> is the [[algebraic interior]] and <math>\\operatorname{dom}h</math>, where ''h'' is some function, is the set <math>\\{z: h(z) < +\\infty\\}</math>, or\n# <math>A \\operatorname{dom}f \\cap \\operatorname{cont}g \\neq \\emptyset</math> where <math>\\operatorname{cont}</math> are the points where the function is [[continuous function|continuous]].\nThen [[strong duality]] holds, i.e. <math>p^* = d^*</math>.  If <math>d^* \\in \\mathbb{R}</math> then [[supremum]] is attained.<ref>{{cite book|title=Techniques of Variational Analysis|last1=Borwein|first1=Jonathan|last2=Zhu|first2=Qiji|year=2005|publisher=Springer|isbn=978-1-4419-2026-3|pages=135‚Äì137}}</ref>\n\n==One-dimensional illustration==\n\nIn the following figure, the minimization problem on the left side of the equation is illustrated.  One seeks to vary ''x'' such that the vertical distance between the convex and concave curves at ''x'' is as small as possible.  The position of the vertical line in the figure is the (approximate) optimum.\n\n[[File:FencheDual02.png]]\n\nThe next figure illustrates the maximization problem on the right hand side of the above equation.  Tangents are drawn to each of the two curves such that both tangents have the same slope ''p''. The problem is to adjust ''p'' in such a way that the two tangents are as far away from each other as possible (more precisely, such that the points where they intersect the y-axis are as far from each other as possible).  Imagine the two tangents as metal bars with vertical springs between them that push them apart and against the two parabolas that are fixed in place.\n\n[[File:FenchelDual01.png]]\n\nFenchel's theorem states that the two problems have the same solution. The points having the minimum vertical separation are also the tangency points for the maximally separated parallel tangents.\n\n==See also==\n*[[Legendre transformation]]\n*[[Convex conjugate]]\n*[[Moreau's theorem]]\n*[[Wolfe duality]]\n*[[Werner Fenchel]]\n\n==References==\n{{Reflist}}\n* {{cite book | authorlink=R. Tyrrell Rockafellar|last=Rockafellar|first=Ralph Tyrrell | title=Convex Analysis | publisher=Princeton University Press | year=1996 | isbn=0-691-01586-4 |page=327 }}\n\n[[Category:Theorems in analysis]]\n[[Category:Convex optimization]]"
    },
    {
      "title": "Geodesic convexity",
      "url": "https://en.wikipedia.org/wiki/Geodesic_convexity",
      "text": "{{Disputed|date=April 2009}}\n\nIn [[mathematics]] &mdash; specifically, in [[Riemannian geometry]] &mdash; '''geodesic convexity''' is a natural generalization of [[convex set|convexity for sets]] and [[convex function|functions]] to [[Riemannian manifold]]s. It is common to drop the prefix \"geodesic\" and refer simply to \"convexity\" of a set or function.\n\n==Definitions==\n\nLet (''M'',&nbsp;''g'') be a Riemannian manifold.\n\n* A subset ''C'' of ''M'' is said to be a '''geodesically convex set''' if, given any two points in ''C'', there is a unique minimizing [[geodesic]]  contained within ''C'' that joins those two points.\n* Let ''C'' be a geodesically convex subset of ''M''. A function <math>f:C\\to\\mathbf{R}</math> is said to be a ('''strictly''') '''geodesically convex function''' if the composition\n\n::<math>f \\circ \\gamma : [0, T] \\to \\mathbf{R}</math>\n\n: is a (strictly) convex function in the usual sense for every unit speed geodesic arc ''&gamma;''&nbsp;:&nbsp;[0,&nbsp;''T'']&nbsp;&rarr;&nbsp;''M'' contained within ''C''.\n\n==Properties==\n\n* A geodesically convex (subset of a) Riemannian manifold is also a [[convex metric space]] with respect to the geodesic distance.\n\n==Examples==\n\n* A subset of ''n''-dimensional [[Euclidean space]] '''E'''<sup>''n''</sup> with its usual flat metric is geodesically convex [[if and only if]] it is convex in the usual sense, and similarly for functions.\n* The \"northern hemisphere\" of the 2-dimensional sphere '''S'''<sup>2</sup> with its usual metric is geodesically convex. However, the subset ''A'' of '''S'''<sup>2</sup> consisting of those points with [[latitude]] further north than 45¬∞ south is ''not'' geodesically convex, since the geodesic ([[great circle]]) joining two points on the southern boundary of ''A'' may well leave ''A'' (e.g. in the case of two points 180¬∞ apart in [[longitude]], in which case the geodesic arc passes over the south pole).\n\n==References==\n\n* {{cite book\n|     last = Rapcs√°k\n|    first = Tam√°s\n|    title = Smooth nonlinear optimization in R<sup>n</sup>\n|   series = Nonconvex Optimization and its Applications | volume = 19\n|publisher = Kluwer Academic Publishers\n| location = Dordrecht\n|     year = 1997\n|     isbn = 0-7923-4680-7\n|       mr = 1480415}}\n\n* {{cite book\n|     last = Udriste\n|    first = Constantin\n|    title = Convex functions and optimization methods on Riemannian manifolds\n|   series = Mathematics and its Applications | volume = 297\n|publisher = Kluwer Academic Publishers\n| location = Dordrecht\n|     year = 1994\n|     isbn = 0-7923-3002-1\n}}\n\n[[Category:Convex optimization]]\n[[Category:Riemannian manifolds]]\n[[Category:Geodesic (mathematics)]]"
    },
    {
      "title": "Geometric programming",
      "url": "https://en.wikipedia.org/wiki/Geometric_programming",
      "text": "A '''geometric program''' ('''GP''') is an [[optimization (mathematics)|optimization]] problem of the form\n:<math>\n\\begin{array}{ll}\n\\mbox{minimize} & f_0(x) \\\\\n\\mbox{subject to} & f_i(x) \\leq 1, \\quad i=1, \\ldots, m\\\\\n& g_i(x) = 1, \\quad i=1, \\ldots, p,\n\\end{array}\n</math>\nwhere <math>f_0,\\dots,f_m</math> are [[posynomials]] and <math>g_1,\\dots,g_p</math> are monomials. In the context of geometric programming (unlike standard mathematics), a monomial is a function from <math>\\mathbb{R}_{++}^n</math> to <math>\\mathbb{R}</math> defined as\n\n:<math>x \\mapsto c x_1^{a_1} x_2^{a_2} \\cdots x_n^{a_n} </math>\n\nwhere <math> c > 0 \\ </math> and <math>a_i \\in \\mathbb{R} </math>. A posynomial is any sum of monomials. <ref name=\"duffin\">{{cite book\n | author     = Richard J. Duffin\n |author2=Elmor L. Peterson |author3=Clarence Zener\n  | title      = Geometric Programming\n | publisher  = John Wiley and Sons\n | year       = 1967\n | pages      = 278\n | isbn       = 0-471-22370-0\n}}</ref><ref name=\"tutorial\">S. Boyd, S. J. Kim, L. Vandenberghe, and A. Hassibi. ''[http://www.stanford.edu/~boyd/gp_tutorial.html A Tutorial on Geometric Programming].'' Retrieved 8 January 2019.</ref>\n\nGeometric programming is\nclosely related to [[convex optimization]]: any GP can be made convex by means of a change of variables. <ref name=\"tutorial\"/> GPs have numerous applications, including component sizing in [[Integrated circuit|IC]] design<ref>M. Hershenson, S. Boyd, and T. Lee. ''[http://www.stanford.edu/~boyd/papers/opamp.html Optimal Design of a CMOS Op-amp via Geometric Programming].'' Retrieved 8 January 2019.</ref><ref> S. Boyd, S. J. Kim, D. Patil, and M. Horowitz. ''[http://www.stanford.edu/~boyd/gp_digital_ckt.html Digital Circuit Optimization via Geometric Programming].'' Retrieved 8 January 2019.</ref>, aircraft design<ref>W. Hoburg and P. Abbeel. ''[https://people.eecs.berkeley.edu/~pabbeel/papers/2014-AIAA-GP-aircraft-design.pdf Geometric programming for aircraft design optimization].'' AIAA Journal 52.11 (2014): 2414-2426.</ref>, and [[maximum likelihood estimation]] for [[logistic regression]] in [[statistics]]. \n\n==Convex form==\nGeometric programs are not in general convex optimization problems, but they can be transformed to convex problems by a change of variables and a transformation of the objective and constraint functions.  In particular, after performing the change of variables <math>y_i = \\log(x_i)</math> and taking the log of the objective and constraint functions, the functions <math>f_i</math>, i.e., the posynomials, are transformed  into [[LogSumExp | log-sum-exp]] functions, which are convex, and the functions <math>g_i</math>, i.e., the monomials, become [[affine transformation | affine]]. Hence, this transformation transforms every GP into an equivalent convex program. <ref name=\"tutorial\"/> In fact, this log-log transformation can be used to convert a larger class of problems, known as log-log convex programming (LLCP), into an equivalent convex form. <ref name=\"dgp\">A. Agrawal, S. Diamond, and S. Boyd. ''[https://arxiv.org/abs/1812.04074 Disciplined Geometric Programming.]'' Retrieved 8 January 2019.</ref> \n\n==Software==\nSeveral software packages exist to assist with formulating and solving geometric programs.\n* [https://www.mosek.com/ MOSEK] is a commercial solver capable of solving geometric programs as well as other non-linear optimization problems.\n* [http://cvxopt.org/ CVXOPT] is an open-source solver for convex optimization problems.\n* [https://github.com/convexengineering/gpkit GPkit] is a Python package for cleanly defining and manipulating geometric programming models. There are a number of example GP models written with this package [https://github.com/convexengineering/gplibrary here].\n*[https://web.stanford.edu/~boyd/ggplab/ GGPLAB] is a MATLAB toolbox for specifying and solving geometric programs (GPs) and generalized geometric programs (GGPs).\n* [https://www.cvxpy.org/tutorial/dgp/index.html CVXPY] is a Python-embedded modeling language for specifying and solving convex optimization problems, including GPs, GGPs, and LLCPs. <ref name=\"dgp\"/>\n\n==See also==\n*[[Signomial]]\n*[[Clarence Zener]]\n\n==References==\n{{reflist}}\n\n[[Category:Convex optimization]]"
    },
    {
      "title": "Lagrangian relaxation",
      "url": "https://en.wikipedia.org/wiki/Lagrangian_relaxation",
      "text": "In the field of [[mathematical optimization]], '''Lagrangian relaxation''' is a [[relaxation (approximation)|relaxation method]] which [[approximation theory|approximates]] a difficult problem of [[constrained optimization]] by a simpler problem. A solution to the relaxed problem is an approximate solution to the original problem, and provides useful information.\n\nThe method penalizes violations of inequality constraints using a [[Lagrange multiplier]], which imposes a cost on violations. These added costs are used instead of the strict inequality constraints in the optimization. In practice, this relaxed problem can often be solved more easily than the original problem.\n\nThe problem of maximizing the Lagrangian function of the dual variables (the Lagrangian multipliers) is the Lagrangian [[dual problem]].\n\n==Mathematical description==\nSuppose we are given a [[LP problem|linear programming problem]], with <math>x\\in \\mathbb{R}^n</math> and <math>A\\in \\mathbb{R}^{m,n}</math>, of the following form:\n\n:{| border=\"0\" cellpadding=\"1\" cellspacing=\"0\"\n|-\n|max\n|\n|<math>c^T x</math>\n|-\n|s.t.\n|-\n|\n|\n|<math>Ax \\le b</math>\n|}\n\nIf we split the constraints in <math>A</math> such that <math>A_1\\in \\mathbb{R}^{m_1,n}</math>,\n<math>A_2\\in \\mathbb{R}^{m_2,n}</math> and <math>m_1+m_2=m</math> we may write the system:\n\n:{| border=\"0\" cellpadding=\"1\" cellspacing=\"0\"\n|-\n|max\n|\n|<math>c^T x</math>\n|-\n|s.t.\n|-\n|(1)\n|\n|<math>A_1 x \\le b_1</math>\n|-\n|(2)\n|\n|<math>A_2 x \\le b_2</math>\n|}\n\nWe may introduce the constraint (2) into the objective:\n\n:{| border=\"0\" cellpadding=\"1\" cellspacing=\"0\"\n|-\n|max\n|\n|<math>c^T x+\\lambda^T(b_2-A_2x)</math>\n|-\n|s.t.\n|-\n|(1)\n|\n|<math>A_1 x \\le b_1</math>\n|}\n\nIf we let <math>\\lambda=(\\lambda_1,\\ldots,\\lambda_{m_2})</math> be nonnegative\nweights, we get penalized if we violate the constraint (2), and we are also rewarded if we satisfy the constraint strictly. The above\nsystem is called the Lagrangian relaxation of our original problem.\n\n==The LR solution as a bound==\n\nOf particular use is the property that for any fixed set of <math>\\tilde{\\lambda}</math> values, the optimal result to the Lagrangian relaxation problem will be no smaller than the optimal result to the original problem. To see this, let <math>\\hat{x}</math> be the optimal solution to the original problem, and let <math>\\bar{x}</math> be the optimal solution to the Lagrangian relaxation. We can then see that  \n:{| border=\"0\" cellpadding=\"1\" cellspacing=\"0\"\n|-\n|<math>c^T \\hat{x} \\leq c^T  \\hat{x} +\\tilde{\\lambda}^T(b_2-A_2 \\hat{x} ) \\leq c^T  \\bar{x} +\\tilde{\\lambda}^T(b_2-A_2 \\bar{x} ) </math>\n|}\n\nThe first inequality is true because <math>\\hat{x}</math> is feasible in the original problem and the second inequality is true because <math>\\bar{x}</math> is the optimal solution to the Lagrangian relaxation.\n\n==Iterating towards a solution of the original problem==\n\nThe above inequality tells us that if we minimize the maximum value we obtain from the relaxed problem, we obtain a tighter limit on the objective value of our original problem. Thus we can address the original problem by instead exploring the partially dualized problem\n\n:{| border=\"0\" cellpadding=\"1\" cellspacing=\"0\"\n|-\n|min \n|\n| <math>P(\\lambda)</math>\n|\n|s.t.\n|\n| <math>\\lambda \\geq 0 </math>\n|}\n\nwhere we define <math>P(\\lambda)</math> as\n\n:{| border=\"0\" cellpadding=\"1\" cellspacing=\"0\"\n|-\n|max\n|\n|<math>c^T x+\\lambda^T(b_2-A_2x)</math>\n|-\n|s.t.\n|-\n|(1)\n|\n|<math>A_1 x \\le b_1</math>\n|}\n\nA Lagrangian relaxation algorithm thus proceeds to explore the range of feasible <math>\\lambda</math> values while seeking to minimize the result returned by the inner <math>P</math> problem. Each value returned by <math>P</math> is a candidate upper bound to the problem, the smallest of which is kept as the best upper bound. If we additionally employ a heuristic, probably seeded by the <math>\\bar{x}</math> values returned by <math>P</math>, to find feasible solutions to the original problem, then we can iterate until the best upper bound and the cost of the best feasible solution converge to a desired tolerance.\n\n== Related methods ==\nThe [[augmented Lagrangian method]] is quite similar in spirit to the Lagrangian relaxation method, but adds an extra term, and updates the dual parameters <math>\\lambda</math> in a more principled manner. It was introduced in the 1970s and has been used extensively.\n\nThe [[penalty method]] does not use dual variables but rather removes the constraints and instead penalizes deviations from the constraint. The method is conceptually simple but usually augmented Lagrangian methods are preferred in practice since the penalty method suffers from ill-conditioning issues.\n\n==References==\n\n===Books===\n* {{cite book | author=[[Ravindra K. Ahuja]], [[Thomas L. Magnanti]], and [[James B. Orlin]] | title= Network Flows: Theory, Algorithms and Applications | publisher=Prentice Hall | year=1993 | isbn=0-13-617549-X }}\n* Bertsekas, Dimitri P. (1999). ''Nonlinear Programming: 2nd Edition.'' Athena Scientific. {{isbn|1-886529-00-0}}.\n* {{cite book|last1=Bonnans|first1=J.&nbsp;Fr√©d√©ric|last2=Gilbert|first2=J.&nbsp;Charles|last3=Lemar√©chal|first3=Claude|last4=Sagastiz√°bal|first4=Claudia&nbsp;A.|author4-link= Claudia Sagastiz√°bal |title=Numerical optimization: Theoretical and practical aspects|url=https://www.springer.com/mathematics/applications/book/978-3-540-35445-1|edition=Second revised ed. of  translation of 1997 <!-- ''Optimisation num√©rique: Aspects th√©oriques et pratiques'' --> French| series=Universitext|publisher=Springer-Verlag|location=Berlin|year=2006|pages=xiv+490|isbn=3-540-35445-X|doi=10.1007/978-3-540-35447-5|mr=2265882|authorlink3=Claude Lemar√©chal}}\n* {{cite book|last1=Hiriart-Urruty|first1=Jean-Baptiste|last2=Lemar√©chal|first2=Claude|title=Convex analysis and minimization algorithms, Volume&nbsp;I: Fundamentals|series=Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences]|volume=305|publisher=Springer-Verlag|location=Berlin|year=1993|pages=xviii+417|isbn=3-540-56850-6|mr=1261420|authorlink2=Claude Lemar√©chal}}\n*{{cite book|last1=Hiriart-Urruty|first1=Jean-Baptiste|last2=Lemar√©chal|first2=Claude|chapter=14 Duality for Practitioners|title=Convex analysis and minimization algorithms, Volume&nbsp;II: Advanced theory and bundle methods|series=Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences]|volume=306|publisher=Springer-Verlag|location=Berlin|year=1993|pages=xviii+346|isbn=3-540-56852-2|authorlink2=Claude Lemar√©chal}}\n*{{cite book|last=Lasdon|first=Leon&nbsp;S.|title=Optimization theory for large systems|publisher=Dover Publications, Inc.|location=Mineola, New York|year=2002|edition=reprint of the 1970 Macmillan|pages=xiii+523|mr=1888251|ref=harv}}\n* {{cite book|last=Lemar√©chal|first=Claude|chapter=Lagrangian relaxation|pages=112‚Äì156|title=Computational combinatorial optimization: Papers from the Spring School held in Schlo√ü Dagstuhl, May&nbsp;15‚Äì19,&nbsp;2000|editor=Michael J√ºnger and Denis Naddef|series=Lecture Notes in Computer Science|volume=2241|publisher=Springer-Verlag|location=Berlin|year=2001|isbn=3-540-42877-1|mr=1900016|doi=10.1007/3-540-45586-8_4|authorlink=Claude Lemar√©chal|ref=harv}}\n* {{cite book|last=Minoux|first=M.|authorlink=Michel Minoux|title=Mathematical programming: Theory and algorithms|others=Egon Balas (foreword)|edition=Translated  by Steven Vajda from the (1983 Paris: Dunod) French|publisher=A Wiley-Interscience Publication. John Wiley & Sons, Ltd.|location=Chichester|year=1986|pages=xxviii+489|isbn=0-471-90170-9|mr=868279|ref=harv|id=(2008 Second ed., in French: ''Programmation math√©matique: Th√©orie et algorithmes''. Editions Tec & Doc, Paris,  2008. xxx+711 pp. {{isbn|978-2-7430-1000-3}} {{MR|2571910}})}}\n\n===Articles===\n* {{cite journal|last=Everett |first=Hugh, III |year=1963 |title=Generalized Lagrange multiplier method for solving problems of optimum allocation of resources |url=http://or.journal.informs.org/cgi/reprint/11/3/399 |journal=Operations Research |volume=11 |issue=3 |pages=399‚Äì417 |doi=10.1287/opre.11.3.399 |jstor=168028 |mr=152360 |ref=harv |via= |authorlink=Hugh Everett |deadurl=yes |archiveurl=https://web.archive.org/web/20110724151508/http://or.journal.informs.org/cgi/reprint/11/3/399 |archivedate=2011-07-24 |df= }}\n*{{cite journal|last1=Kiwiel|first1=Krzysztof&nbsp;C.|last2=Larsson |first2=Torbj√∂rn|last3=Lindberg|first3=P.&nbsp;O.|title=Lagrangian relaxation via ballstep subgradient methods|mr=2348241|journal=Mathematics of Operations Research|volume=32|date=August 2007|url=http://mor.journal.informs.org/cgi/content/abstract/32/3/669|issue=3|pages=669‚Äì686|doi=10.1287/moor.1070.0261|ref=harv}}\n\n{{DEFAULTSORT:Lagrangian Relaxation}}\n[[Category:Convex optimization]]\n[[Category:Relaxation (approximation)]]"
    },
    {
      "title": "Linear matrix inequality",
      "url": "https://en.wikipedia.org/wiki/Linear_matrix_inequality",
      "text": "In [[convex optimization]], a '''linear matrix inequality''' ('''LMI''') is an expression of the form\n: <math>\\operatorname{LMI}(y):=A_0+y_1A_1+y_2A_2+\\cdots+y_m A_m\\succeq 0\\,</math>\nwhere\n* <math>y=[y_i\\,,~i\\!=\\!1,\\dots, m]</math> is a real vector,\n* <math>A_0, A_1, A_2,\\dots,A_m</math> are <math>n\\times n</math> [[symmetric matrix|symmetric matrices]] <math>\\mathbb{S}^n</math>,\n* <math>B\\succeq0 </math> is a generalized inequality meaning <math>B</math> is a [[positive semidefinite matrix]] belonging to the positive semidefinite cone <math>\\mathbb{S}_+</math> in the subspace of symmetric matrices <math>\\mathbb{S}</math>.\n\nThis linear matrix inequality specifies a [[convex set|convex]] constraint on&nbsp;''y''.\n\n== Applications ==\nThere are efficient numerical methods to determine whether an LMI is feasible (''e.g.'', whether there exists a vector ''y'' such that LMI(''y'')&nbsp;‚â•&nbsp;0), or to solve a [[convex optimization]] problem with LMI constraints.\nMany optimization problems in [[control theory]], [[system identification]] and [[signal processing]] can be formulated using LMIs. Also LMIs find application in [[Polynomial SOS|Polynomial Sum-Of-Squares]]. The prototypical primal and dual [[semidefinite programming|semidefinite program]] is a minimization of a real linear function respectively subject to the primal and dual [[convex cone]]s governing this LMI.\n\n== Solving LMIs ==\n\nA major breakthrough in convex optimization lies in the introduction of [[interior-point method]]s. These methods were developed in a series of papers and became of true interest in the context of LMI problems in the work of Yurii Nesterov and Arkadii Nemirovskii.\n\n== References ==\n\n* Y. Nesterov and A. Nemirovsky, ''Interior Point Polynomial Methods in Convex Programming.'' SIAM, 1994.\n\n== External links ==\n* S. Boyd, L. El Ghaoui, E. Feron, and V. Balakrishnan, [http://www.stanford.edu/~boyd/lmibook/ Linear Matrix Inequalities in System and Control Theory]  (book in pdf)\n* C. Scherer and S. Weiland, [http://www.st.ewi.tudelft.nl/roos/courses/WI4218/lmi052.pdf Linear Matrix Inequalities in Control]\n\n[[Category:Convex optimization]]"
    },
    {
      "title": "Pseudoconvex function",
      "url": "https://en.wikipedia.org/wiki/Pseudoconvex_function",
      "text": "{{about|the notion in convex analysis|the notion in several complex variables|pseudoconvex domain}}\nIn [[convex analysis]] and the [[calculus of variations]], branches of [[mathematics]], a '''pseudoconvex function''' is a [[function (mathematics)|function]] that behaves like a [[convex function]] with respect to finding its [[local extrema|local minima]], but need not actually be convex.  Informally, a differentiable function is pseudoconvex if it is increasing in any direction where it has a positive [[directional derivative]].\n\n==Formal definition==\nFormally, a real-valued differentiable function ''&fnof;'' defined on a (nonempty) [[convex set|convex]] [[open set]] ''X'' in the finite-dimensional [[Euclidean space]] '''R'''<sup>''n''</sup> is said to be '''pseudoconvex''' if, for all {{nowrap|''x'', ''y'' &isin; ''X''}} such that <math>\\nabla f(x)\\cdot(y-x) \\ge 0</math>, we have <math>f(y)\\ge f(x)</math>.<ref>{{harvnb|Mangasarian|1965}}</ref>  Here ‚àá''&fnof;'' is the [[gradient]] of ''&fnof;'', defined by\n:<math>\\nabla f = \\left(\\frac{\\partial f}{\\partial x_1},\\dots,\\frac{\\partial f}{\\partial x_n}\\right).</math>\n\n==Properties==\nEvery convex function is pseudoconvex, but the converse is not true.  For example, the function {{nowrap|''&fnof;''(''x'') {{=}} ''x'' + ''x''<sup>3</sup>}} is pseudoconvex but not convex.  Any pseudoconvex function is [[quasiconvex function|quasiconvex]], but the converse is not true since the function {{nowrap|''&fnof;''(''x'') {{=}} ''x''<sup>3</sup>}} is quasiconvex but not pseudoconvex. Pseudoconvexity is primarily of interest because a point ''x''* is a local minimum of a pseudoconvex function ''&fnof;'' if and only if it is a [[stationary point]] of ''&fnof;'', which is to say that the [[gradient]] of ''&fnof;'' vanishes at ''x''*:\n:<math>\\nabla f(x^*) = 0.</math><ref>{{harvnb|Mangasarian|1965}}</ref>\n\n==Generalization to nondifferentiable functions==\nThe notion of pseudoconvexity can be generalized to nondifferentiable functions as follows.<ref>{{harvnb|Floudas|Pardalos|2001}}</ref>  Given any function {{nowrap|''&fnof;'' : ''X'' &rarr; '''R'''}} we can define the upper [[Dini derivative]] of ''&fnof;'' by\n:<math>f^+(x,u) = \\limsup_{h\\to 0^+} \\frac{f(x+hu) - f(x)}{h}</math>\nwhere ''u'' is any [[unit vector]].  The function is said to be pseudoconvex if it is increasing in any direction where the upper Dini derivative is positive.  More precisely, this is characterized in terms of the [[subdifferential]] ‚àÇ''&fnof;'' as follows:\n*For all {{nowrap|''x'', ''y'' &isin; ''X''}}, if there exists an {{nowrap|''x''* &isin; ‚àÇ''&fnof;''(''x'')}} such that <math> \\langle x^* , y - x \\rangle \\ge 0 \\,,</math> then {{nowrap|''&fnof;''(''x'') &le; ''&fnof;''(''z'')}} for all ''z'' on the line segment adjoining ''x'' and ''y''.\n\n==Related notions==\nA '''{{visible anchor|pseudoconcave function}}''' is a function whose negative is pseudoconvex.  A '''{{visible anchor|pseudolinear function}}''' is a function that is both pseudoconvex and pseudoconcave.<ref>{{harvnb|Rapcsak|1991}}</ref> For example, [[linear-fractional programming|linear‚Äìfractional program]]s have pseudolinear [[objective function]]s and [[linear programming|linear‚Äìinequality constraints]]: These properties allow fractional‚Äìlinear problems to be solved by a variant of the [[simplex algorithm]] (of [[George B. Dantzig]]).<ref>\nChapter five: {{cite book| last=Craven|first=B. D.|title=Fractional programming|series=Sigma Series in Applied Mathematics|volume=4|publisher=Heldermann Verlag|location=Berlin|year=1988|pages=145|isbn=3-88538-404-3 |mr=949209}}</ref><ref>{{cite news | last1=Kruk | first1=Serge|last2=Wolkowicz|first2=Henry|title=Pseudolinear programming |journal=[[SIAM Review]]|volume=41 |year=1999 |number=4 |pages=795‚Äì805 |mr=1723002 | jstor = 2653207 |doi=10.1137/S0036144598335259}}\n</ref><ref>{{cite news | last1=Mathis|first1=Frank H.|last2=Mathis|first2=Lenora Jane|title=A nonlinear programming algorithm for hospital management |journal=[[SIAM Review]]|volume=37 |year=1995 |number=2 |pages=230‚Äì234|mr=1343214 | jstor = 2132826 |doi=10.1137/1037046}}\n</ref>\n\n==See also==\n* [[Pseudoconvexity]]\n* [[Convex function]]\n* [[Quasiconvex function]]\n\n==Notes==\n{{reflist}}\n\n==References==\n* {{citation|first1=Christodoulos A.|last1=Floudas|first2=Panos M.|last2=Pardalos|title=Encyclopedia of Optimization|chapter=Generalized monotone multivalued maps|publisher=Springer|year=2001|isbn=978-0-7923-6932-5|page=227}}.\n* {{cite journal|ref=harv|title=Pseudo-Convex Functions|journal=Journal of the Society for Industrial and Applied Mathematics Series A Control|volume=3|issue=2|pages=281&ndash;290 |date=January 1965|doi=10.1137/0303020|first=O. L.|last=Mangasarian|issn=0363-0129}}.\n* {{cite journal|ref=harv|first=T.|last=Rapcsak|title=On pseudolinear functions|journal=European Journal of Operational Research|volume=50|issue=3|date=1991-02-15|pages=353&ndash;360|issn=0377-2217|doi=10.1016/0377-2217(91)90267-Y}}\n\n[[Category:Convex analysis]]\n[[Category:Convex optimization]]\n[[Category:Types of functions]]\n[[Category:Generalized convexity]]"
    },
    {
      "title": "Quasiconvex function",
      "url": "https://en.wikipedia.org/wiki/Quasiconvex_function",
      "text": "[[File:Quasiconvex function.png|right|thumb|A quasiconvex function that is not convex]] \n[[File:Nonquasiconvex function.png|right|thumb|A function that is not quasiconvex: the set of points in the domain of the function for which the function values are below the dashed red line is the union of the two red intervals, which is not a convex set.]]\n[[File:standard deviation diagram.svg|325px|thumb|The [[probability density function]] of the [[normal distribution]] is quasiconcave but not concave.]]\n[[File:Multivariate Gaussian.png|thumb|right|300px|The [[bivariate normal]] [[Joint probability distribution#Density function or mass function|joint density]] is quasiconcave.]]\n\nIn [[mathematics]], a '''quasiconvex function''' is a [[real number|real]]-valued [[function (mathematics)|function]] defined on an [[interval (mathematics)|interval]] or on a [[convex set|convex subset]] of a real [[vector space]] such that the [[inverse image]] of any set of the form <math>(-\\infty,a)</math> is a [[convex set]].  For a function of a single variable, along any stretch of the curve the highest point is one of the endpoints. The negative of a quasiconvex function is said to be '''quasiconcave'''.\n\nAll [[convex function]]s are also quasiconvex, but not all quasiconvex functions are convex, so quasiconvexity is a generalization of convexity. Quasiconvexity and quasiconcavity extend to functions with multiple [[argument of a function|arguments]] the notion of [[Unimodality#Unimodal function|unimodality]] of functions with a single real argument.\n\n==Definition and properties==\n\nA function <math>f:S \\to \\mathbb{R}</math> defined on a convex subset  ''S'' of a real vector space is quasiconvex if for all <math>x, y \\in S</math> and <math>\\lambda \\in [0,1]</math> we have\n\n: <math>f(\\lambda x + (1 - \\lambda)y)\\leq\\max\\big\\{f(x),f(y)\\big\\}.</math>\n\nIn words, if ''f'' is such that it is always true that a point directly between two other points does not give a higher value of the function than both of the other points do, then ''f'' is quasiconvex. Note that the points ''x'' and ''y'', and the point directly between them, can be points on a line or more generally points in ''n''-dimensional space.\n\n[[File:Monotonicity example2.png|right|thumb|A quasilinear function is both quasiconvex and quasiconcave.]]\n\n[[File:Quasi-concave-function-graph.png|right|thumb|The graph of a function that is both concave and quasi-convex on the nonnegative real numbers.]]\n\nAn alternative way (see introduction) of defining a quasi-convex function <math>f(x)</math> is to require that each sublevel set\n<math>S_\\alpha(f) = \\{x\\mid f(x) \\leq \\alpha\\}</math>\nis a convex set.\n\nIf furthermore\n\n: <math>f(\\lambda x + (1 - \\lambda)y)<\\max\\big\\{f(x),f(y)\\big\\}</math>\n\nfor all <math>x \\neq y</math> and <math>\\lambda \\in (0,1)</math>, then <math>f</math> is '''strictly quasiconvex'''. That is, strict quasiconvexity requires that a point directly between two other points must give a lower value of the function than one of the other points does.\n\nA '''quasiconcave function''' is a function whose negative is quasiconvex, and a '''strictly quasiconcave function''' is a function whose negative is strictly quasiconvex. Equivalently a function <math>f</math> is quasiconcave if\n\n: <math>f(\\lambda x + (1 - \\lambda)y)\\geq\\min\\big\\{f(x),f(y)\\big\\}.</math>\n\nand strictly quasiconcave if\n\n: <math>f(\\lambda x + (1 - \\lambda)y)>\\min\\big\\{f(x),f(y)\\big\\}</math>\n\nA (strictly) quasiconvex function has (strictly) convex [[lower contour set]]s, while a (strictly) quasiconcave function has (strictly) convex [[upper contour set]]s.\n\nA function that is both quasiconvex and quasiconcave is '''quasilinear'''.\n\nA particular case of quasi-concavity, if <math>S \\subset \\mathbb{R}</math>, is [[Unimodality#Unimodal function|unimodality]], in which there is a locally maximal value.\n\n==Applications==\nQuasiconvex functions have applications in [[mathematical analysis]], in [[mathematical optimization]], and in [[game theory]] and [[economics]].\n\n===Mathematical optimization===\nIn [[nonlinear programming|nonlinear optimization]], quasiconvex programming studies [[iterative method]]s that converge to a minimum (if one exists) for quasiconvex functions.  Quasiconvex programming is a generalization of  [[convex programming]].<ref>{{harvtxt|Di&nbsp;Guglielmo|1977|pp=287‚Äì288}}: {{cite journal|ref=harv|last=Di&nbsp;Guglielmo|first=F.|title=Nonconvex duality in multiobjective optimization|doi=10.1287/moor.2.3.285|volume=2|year=1977|number=3|pages=285‚Äì291|journal=Mathematics of Operations Research|mr=484418|jstor=3689518}}</ref> Quasiconvex programming is used in the solution of \"surrogate\" [[dual problem]]s, whose biduals provide quasiconvex closures of the primal problem, which therefore provide tighter bounds than do the convex closures provided by Lagrangian [[Lagrange duality|dual problems]].<ref>{{cite book|last=Di&nbsp;Guglielmo|first=F.|chapter=Estimates of the duality&nbsp;gap for discrete&nbsp;and&nbsp;quasiconvex optimization&nbsp;problems|title=Generalized concavity in optimization and economics: Proceedings of the NATO Advanced&nbsp;Study Institute held at the University of British&nbsp;Columbia, Vancouver,&nbsp;B.C., August&nbsp;4‚Äì15,&nbsp;1980\n|editor1-first=Siegfried|editor1-last=Schaible|editor2-first=William&nbsp;T.|editor2-last=Ziemba|publisher=Academic Press,&nbsp;Inc. [Harcourt Brace Jovanovich, Publishers]|location=New&nbsp;York|year=1981|pages=281‚Äì298|isbn=0-12-621120-5|mr=652702}}</ref> In [[Computational complexity theory|theory]], quasiconvex programming and convex programming problems can be solved in reasonable amount of time, where the number of iterations grows like a polynomial in the dimension of the problem (and in the reciprocal of the approximation error tolerated);<ref>{{cite news|last=Kiwiel|first=Krzysztof C.|title=Convergence and efficiency of subgradient methods for quasiconvex minimization|journal=Mathematical Programming, Series A|publisher=Springer|location=Berlin, Heidelberg|issn=0025-5610|pages=1‚Äì25|volume=90|issue=1|doi=10.1007/PL00011414|year=2001|mr=1819784}} Kiwiel acknowledges that [[Yuri Nesterov (mathematician)|Yuri Nesterov]] first established that quasiconvex minimization problems can be solved efficiently.</ref> however, such theoretically \"efficient\" methods use \"divergent-series\" [[gradient descent#Stepsize rules|stepsize rule]]s, which were first developed for classical [[subgradient method]]s. Classical subgradient methods using divergent-series rules are much slower than modern methods of convex minimization, such as subgradient projection methods, [[bundle method]]s of descent, and nonsmooth [[filter method]]s.\n\n===Economics and partial differential equations: Minimax <!-- and fixed-point  -->theorems===\n<div id='economics'></div>\nIn [[microeconomics]], quasiconcave [[utility function]]s imply that consumers have [[convex preferences]]. Quasiconvex functions are important\nalso in [[game theory]], [[industrial organization]], and  [[general equilibrium theory]], particularly for applications of [[Sion's minimax theorem]]. Generalizing a [[minimax theorem]] of [[John von Neumann]], Sion's theorem is also used in the theory of [[partial differential equation]]s. <!-- CHECK! Quasiconvex functions are also used in many [[fixed-point theorem]]s, for example, theorems by [[Kakutani]] and [[Ky Fan]]. -->\n\n==Preservation of quasiconvexity==\n\n===Operations preserving quasiconvexity===\n* maximum of quasiconvex functions (i.e. <math>f = \\max \\left\\lbrace f_1 , \\ldots , f_n \\right\\rbrace</math> ). This is also true for strict quasiconvex functions (maximum of strict quasiconvex functions is also strict quasiconvex).<ref>{{cite journal|last1=Johansson|first1=Edvard|last2=Petersson|first2=David|title=Parameter Optimization for Equilibrium Solutions of Mass Action Systems|date=2016|pages=13‚Äì14|url=https://lup.lub.lu.se/student-papers/search/publication/8892543|accessdate=26 October 2016}}</ref>\n* composition with a non-decreasing function (i.e. <math>g : \\mathbb{R}^{n} \\rightarrow \\mathbb{R}</math> quasiconvex, <math>h : \\mathbb{R} \\rightarrow \\mathbb{R}</math> non-decreasing, then <math>f = h \\circ g</math> is quasiconvex)\n* minimization (i.e. <math>f(x,y)</math> quasiconvex, <math>C</math> convex set, then <math>h(x) = \\inf_{y \\in C} f(x,y)</math> is quasiconvex)\n\n===Operations not preserving quasiconvexity===\n* The sum of quasiconvex functions defined on ''the same domain'' need not be quasiconvex: In other words, if <math>f(x), g(x)</math> are quasiconvex, then <math>(f+g)(x) = f(x) + g(x)</math> need not be quasiconvex.\n* The sum of quasiconvex functions defined on ''different'' domains (i.e. if <math>f(x), g(y)</math> are quasiconvex, <math>h(x,y) = f(x) + g(y)</math>) need not be quasiconvex. Such functions are called \"additively decomposed\" in economics and \"separable\" in [[mathematical optimization]].\n\n==Examples==\n* Every convex function is quasiconvex.\n* A concave function can be quasiconvex function. For example <math>x \\mapsto \\log(x)</math> is concave, and it is quasiconvex.\n* Any [[monotonic function]] is both quasiconvex and quasiconcave. More generally, a function which decreases up to a point and increases from that point on is quasiconvex (compare [[unimodality]]).\n*The [[floor function]]  <math>x\\mapsto \\lfloor x\\rfloor</math> is an example of a quasiconvex function that is neither convex nor continuous.\n\n==See also==\n* [[Convex function]]\n* [[Concave function]]\n* [[Logarithmically concave function]]\n* [[Pseudoconvexity]] in the sense of several complex variables (not generalized convexity)\n* [[Pseudoconvex function]]\n* [[Invex function]]\n* [[Concavification]]\n\n==References==\n<references/>\n\n* Avriel, M., Diewert, W.E., Schaible, S. and Zang, I., ''Generalized Concavity'', Plenum Press, 1988.\n* {{cite book|last=Crouzeix|first=J.-P.|chapter=Quasi-concavity|title=The New&nbsp;Palgrave Dictionary of Economics|editor-first=Steven&nbsp;N.|editor-last=Durlauf|editor2-first=Lawrence&nbsp;E<!-- . -->|editor2-last=Blume|publisher=Palgrave Macmillan|year=2008|edition=Second|pages=|url=http://www.dictionaryofeconomics.com/article?id=pde2008_Q000008|doi=10.1057/9780230226203.1375|ref=harv}}\n* Singer, Ivan ''Abstract convex analysis''. Canadian Mathematical Society Series of Monographs and Advanced Texts. A Wiley-Interscience Publication. John Wiley & Sons, Inc., New York, 1997. xxii+491 pp.&nbsp;{{ISBN|0-471-16015-6}} <!-- MR1461544 -->\n\n==External links==\n* [http://projecteuclid.org/euclid.pjm/1103040253 SION, M., \"On general minimax theorems\", Pacific J. Math. 8 (1958), 171-176.]\n* [http://glossary.computing.society.informs.org/second.php Mathematical programming glossary]\n* [http://homepages.nyu.edu/~caw1/UMath/Handouts/ums11h22convexsetsandfunctions.pdf Concave and Quasi-Concave Functions] - by Charles Wilson, [[NYU]] Department of Economics\n* [http://www.economics.utoronto.ca/osborne/MathTutorial/QCC.HTM Quasiconcavity and quasiconvexity] - by Martin J. Osborne, [[University of Toronto]] Department of Economics\n\n[[Category:Real analysis]]\n[[Category:Convex optimization]]\n[[Category:Types of functions]]\n[[Category:Convex analysis]]\n[[Category:Generalized convexity]]"
    },
    {
      "title": "Slater's condition",
      "url": "https://en.wikipedia.org/wiki/Slater%27s_condition",
      "text": "In [[mathematics]], '''Slater's condition''' (or '''Slater condition''') is a [[sufficient condition]] for [[strong duality]] to hold for a [[convex optimization|convex optimization problem]], named after Morton L. Slater.<ref>{{cite report |author=Slater, Morton |title=Lagrange Multipliers Revisited |date=1950 |work=Cowles Commission Discussion Paper No. 403 |url=http://cowles.yale.edu/sites/default/files/files/pub/d00/d0080.pdf }} Reprinted in {{cite book |editor-first=Giorgio |editor-last=Giorgi |editor2-first=Tinne Hoff |editor2-last=Kjeldsen |title=Traces and Emergence of Nonlinear Programming |location=Basel |publisher=Birkh√§user |year=2014 |isbn=978-3-0348-0438-7 |pages=293‚Äì306 |url=https://books.google.com/books?id=q4PBBAAAQBAJ&pg=PA293 }}</ref>  Informally, Slater's condition states that the feasible region must have an [[Interior (topology)|interior point]] (see technical details below).\n\nSlater's condition is a specific example of a [[constraint qualification]].<ref>{{cite book |first=Akira |last=Takayama |title=Mathematical Economics |location=New York |publisher=Cambridge University Press |year=1985 |isbn=0-521-25707-7 |pages=66‚Äì76 }}</ref>  In particular, if Slater's condition holds for the [[primal problem]], then the [[duality gap]] is 0, and if the dual value is finite then it is attained.<ref>{{cite book |last1=Borwein |first1=Jonathan |last2=Lewis |first2=Adrian |title=Convex Analysis and Nonlinear Optimization: Theory and Examples| edition=2nd |year=2006 |publisher=Springer |isbn=0-387-29570-4 }}</ref>\n\n==Formulation==\nConsider the [[mathematical optimization|optimization problem]]\n:<math> \\text{Minimize }\\; f_0(x) </math>\n:<math> \\text{subject to: }\\ </math>\n::<math> f_i(x) \\le 0 , i = 1,\\ldots,m</math>\n::<math> Ax = b</math>\nwhere <math>f_0,\\ldots,f_m</math> are [[convex function|convex functions]]. This is an instance of [[Convex_optimization|convex programming]].\n\nIn words, Slater's condition for convex programming states that strong duality holds if there exists an <math>x^*</math> such that <math>x^*</math> is strictly [[Feasible region#Candidate solution|feasible]] (i.e. all constraints are satisfied and the nonlinear constraints are satisfied with strict inequalities).\n\nMathematically, Slater's condition states that strong duality holds if there exists an <math>x^* \\in \\operatorname{relint}(D)</math> (where relint denotes the [[relative interior]] of the convex set\n<math>D := \\cap_{i = 0}^m \\operatorname{dom}(f_i)</math>) such that\n:<math>f_i(x^*) < 0, i = 1,\\ldots,m,</math> (the convex, nonlinear constraints)\n:<math>Ax^* = b.\\,</math><ref name=\"boyd\">{{cite book |last1=Boyd |first1=Stephen |last2=Vandenberghe |first2=Lieven |title=Convex Optimization |publisher=Cambridge University Press |year=2004 |isbn=978-0-521-83378-3 |url=http://www.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf |format=pdf |accessdate=October 3, 2011}}</ref>\n\n==Generalized Inequalities==\nGiven the problem\n:<math> \\text{Minimize }\\; f_0(x) </math>\n:<math> \\text{subject to: }\\ </math>\n::<math> f_i(x) \\le_{K_i} 0 , i = 1,\\ldots,m</math>\n::<math> Ax = b</math>\nwhere <math>f_0</math> is convex and <math>f_i</math> is <math>K_i</math>-convex for each <math>i</math>.  Then Slater's condition says that if there exists an <math>x^* \\in \\operatorname{relint}(D)</math> such that\n:<math>f_i(x^*) <_{K_i} 0, i = 1,\\ldots,m</math> and\n:<math>Ax^* = b</math>\nthen strong duality holds.<ref name=\"boyd\" />\n\n==References==\n{{Reflist}}\n\n[[Category:Convex optimization]]"
    },
    {
      "title": "Subderivative",
      "url": "https://en.wikipedia.org/wiki/Subderivative",
      "text": "[[File:Subderivative illustration.png|right|thumb|A convex function (blue) and \"subtangent lines\" at ''x''<sub>0</sub> (red).]]\nIn [[mathematics]], the '''subderivative''', '''subgradient''', and '''subdifferential''' generalize the [[derivative]] to convex functions which are not necessarily [[Differentiable function|differentiable]]. Subderivatives arise in [[convex analysis]], the study of [[convex functions]], often in connection to [[convex optimization]].\n\nLet ''f'':''I''‚Üí'''R''' be a [[real number|real]]-valued convex function  defined on an [[open interval]] of the real line. Such a function need not be differentiable at all points: For example, the [[absolute value]] function ''f''(''x'')=|''x''| is nondifferentiable when ''x''=0. However, as seen in the graph on the right (where ''f(x)'' in blue has non-differentiable kinks similar to the absolute value function), for any ''x''<sub>0</sub> in the domain of the function one can draw a line which goes through the point (''x''<sub>0</sub>, ''f''(''x''<sub>0</sub>)) and which is everywhere either touching or below the graph of ''f''. The [[slope]] of such a line is called a ''subderivative'' (because the line is under the graph of ''f'').\n\n== Definition ==\nRigorously, a ''subderivative'' of a convex function ''f'':''I''‚Üí'''R''' at a point ''x''<sub>0</sub> in the open interval ''I'' is a real number ''c'' such that \n:<math>f(x)-f(x_0)\\ge c(x-x_0)</math>\nfor all ''x'' in ''I''. One may show that the [[Set (mathematics)|set]] of subderivatives at ''x''<sub>0</sub> for a convex function is a [[empty set|nonempty]] [[closed interval]] [''a'', ''b''], where ''a'' and ''b'' are the [[one-sided limit]]s\n\n:<math>a=\\lim_{x\\to x_0^-}\\frac{f(x)-f(x_0)}{x-x_0}</math>\n\n:<math>b=\\lim_{x\\to x_0^+}\\frac{f(x)-f(x_0)}{x-x_0}</math>\n\nwhich are guaranteed to exist and satisfy ''a'' ‚â§ ''b''.\n\nThe set [''a'', ''b''] of all subderivatives is called the '''subdifferential''' of the function ''f'' at ''x''<sub>0</sub>. Since ''f'' is convex, if its subdifferential at <math>x_0</math> contains exactly one subderivative, then ''f'' is differentiable at <math>x_0</math>.<ref>[[R. T. Rockafellar]] ''Convex analysis'' 1970. Theorem 25.1, p.242</ref>\n\n== Examples ==\n\nConsider the function ''f''(''x'')=|''x''| which is convex. Then, the subdifferential at the origin is the interval [&minus;1, 1]. The subdifferential at any point ''x''<sub>0</sub><0 is the [[singleton set]] {‚àí1}, while the subdifferential at any point ''x''<sub>0</sub>>0  is the singleton set {1}. This is similar to the [[sign function]], but is not a single-valued function at 0, instead including all possible subderivatives.\n\n== Properties ==\n\n* A convex function ''f'':''I''‚Üí'''R''' is differentiable at ''x''<sub>0</sub> [[if and only if]] the subdifferential is made up of only one point, which is the derivative at ''x''<sub>0</sub>.\n* A point ''x''<sub>0</sub> is a [[global minimum]] of a convex function ''f'' if and only if zero is contained in the subdifferential, that is, in the figure above, one may draw a horizontal \"subtangent line\" to the graph of ''f'' at (''x''<sub>0</sub>, ''f''(''x''<sub>0</sub>)). This last property is a generalization of the fact that the derivative of a function differentiable at a local minimum is zero.\n* If <math>f</math> and <math>g</math> are convex functions with subdifferentials <math>\\partial f(x)</math> and <math>\\partial g(x)</math>, then the subdifferential of <math>f + g</math> is <math>\\partial(f + g)(x) = \\partial f(x) + \\partial g(x)</math> (where the addition operator denotes the [[Minkowski sum]]). This reads as \"the subdifferential of a sum is the sum of the subdifferentials.\" <ref>{{cite book|last1=Lemar√©chal|first1=Claude|last2=Hiriart-Urruty|first2=Jean-Baptiste|title=Fundamentals of Convex Analysis|date=2001|publisher=Springer-Verlag Berlin Heidelberg|isbn=978-3-642-56468-0|page=183}}</ref>\n\n== The subgradient ==\nThe concepts of subderivative and subdifferential can be generalized to functions of several variables. If ''f'':''U''‚Üí '''R''' is a real-valued convex function defined on a [[convex set|convex]] [[open set]] in the [[Euclidean space]] '''R'''<sup>''n''</sup>, a vector <math> v</math> in that space is called a '''subgradient''' at a point ''x''<sub>0</sub> in ''U'' if for any ''x'' in ''U'' one has\n:<math>f(x)-f(x_0)\\ge v\\cdot (x-x_0)</math>\nwhere the dot denotes the [[dot product]]. \nThe set of all subgradients at ''x''<sub>0</sub> is called the '''subdifferential''' at ''x''<sub>0</sub> and is denoted ‚àÇ''f''(''x''<sub>0</sub>). The subdifferential is always a nonempty convex [[compact set]].\n\nThese concepts generalize further to convex functions ''f'':''U''‚Üí '''R''' on a [[convex set]] in a [[locally convex space]] ''V''. A functional <math>v</math><sup>‚àó</sup> in the [[dual space]] V<sup>‚àó</sup> is called ''subgradient'' at ''x''<sub>0</sub> in ''U'' if for any ''x'' in ''U''\n:<math>f(x)-f(x_0)\\ge v^*(x-x_0).</math>\nThe set of all subgradients at ''x''<sub>0</sub> is called the subdifferential at ''x''<sub>0</sub> and is again denoted ‚àÇ''f''(''x''<sub>0</sub>). The subdifferential is always a convex [[closed set]]. It can be an empty set; consider for example an [[unbounded operator]], which is convex, but has no subgradient. If ''f'' is continuous, the subdifferential is nonempty.\n\n== History ==\n\nThe subdifferential on convex functions was introduced by [[Jean Jacques Moreau]] and [[R. Tyrrell Rockafellar]] in the early 1960s. The ''generalized subdifferential'' for nonconvex functions was introduced by F.H. Clarke and R.T. Rockafellar in the early 1980s.<ref>\n {{cite book|last=Clarke|first=Frank H.|title=Optimization and nonsmooth analysis|publisher=[[John Wiley & Sons]]|location=New York|year=1983|pages=xiii+308|isbn=0-471-87504-X |mr=0709590}}</ref>\n<!-- this reference is not recognized by the optimization community\n<ref>\nGeorgios Stavroulakis, \"Quasidifferentiable optimization\" in Christodoulos A. Floudas, P.M. Pardalos, eds., ''Encyclopedia of Optimization'' 2001, {{isbn|0-7923-6932-7}}, [http://books.google.com/books?id=gtoTkL7heS0C&pg=PA452#v=snippet&q=subdifferential%20moreau&f=false p. 452''ff'']</ref>\n-->\n\n== See also ==\n* [[Weak derivative]]\n* [[Subgradient method]]\n\n== References ==\n<references/>\n* Jean-Baptiste Hiriart-Urruty, [[Claude Lemar√©chal]], ''Fundamentals of Convex Analysis'', Springer, 2001. {{isbn|3-540-42205-6}}.\n* {{cite book|last=ZƒÉlinescu|first=C.|title=Convex analysis in general vector spaces|publisher=World Scientific Publishing&nbsp; Co.,&nbsp;Inc|year=2002|pages=xx+367|isbn=981-238-067-1|mr=1921556}}\n\n[[Category:Generalizations of the derivative]]\n[[Category:Convex optimization]]\n[[Category:Variational analysis]]"
    },
    {
      "title": "Test functions for optimization",
      "url": "https://en.wikipedia.org/wiki/Test_functions_for_optimization",
      "text": "In applied mathematics, '''test functions''', known as '''artificial landscapes''', are useful to evaluate characteristics of optimization algorithms, such as:\n* Convergence rate.\n* Precision.\n* Robustness.\n* General performance.\n\nHere some test functions are presented with the aim of giving an idea about the different situations that optimization algorithms have to face when coping with these kinds of problems. In the first part, some objective functions for single-objective optimization cases are presented. In the second part, test functions with their respective Pareto fronts for [[multi-objective optimization]] problems (MOP) are given.\n\nThe artificial landscapes presented herein for single-objective optimization problems are taken from B√§ck,<ref>{{cite book|last=B√§ck|first=Thomas|title=Evolutionary algorithms in theory and practice : evolution strategies, evolutionary programming, genetic algorithms|year=1995|publisher=Oxford University Press|location=Oxford|isbn=978-0-19-509971-3|page=328}}</ref> Haupt et al.<ref>{{cite book|last=Haupt|first=Randy L. Haupt, Sue Ellen|title=Practical genetic algorithms with CD-Rom|year=2004|publisher=J. Wiley|location=New York|isbn=978-0-471-45565-3|edition=2nd}}</ref> and from Rody Oldenhuis software.<ref>{{cite web|last=Oldenhuis|first=Rody|title=Many test functions for global optimizers|url=http://www.mathworks.com/matlabcentral/fileexchange/23147-many-testfunctions-for-global-optimizers|publisher=Mathworks|accessdate=1 November 2012}}</ref> Given the number of problems (55 in total), just a few are presented here. The complete list of test functions is found on the Mathworks website.<ref>{{cite web|last=Ortiz|first=Gilberto A.|title=Evolution Strategies (ES)|url=http://www.mathworks.com/matlabcentral/fileexchange/35801-evolution-strategies-es|publisher=Mathworks|accessdate=1 November 2012}}</ref>\n\nThe test functions used to evaluate the algorithms for MOP were taken from Deb,<ref name=\"Deb:2002\">Deb, Kalyanmoy (2002) Multiobjective optimization using evolutionary algorithms (Repr. ed.). Chichester [u.a.]: Wiley. {{isbn|0-471-87339-X}}.</ref> Binh et al.<ref name=\"Binh97\">Binh T. and Korn U. (1997) MOBES: A Multiobjective Evolution Strategy for Constrained Optimization Problems. In: Proceedings of the Third International Conference on Genetic Algorithms. Czech Republic. pp. 176‚Äì182</ref> and Binh.<ref name=\"Binh99\">Binh T. (1999) [https://www.researchgate.net/profile/Thanh_Binh_To/publication/2446107_A_Multiobjective_Evolutionary_Algorithm_The_Study_Cases/links/53eb422f0cf28f342f45251d.pdf A multiobjective evolutionary algorithm. The study cases.] Technical report. Institute for Automation and Communication. Barleben, Germany</ref> You can download the software developed by Deb,<ref name=\"Deb_nsga\">Deb K. (2011) Software for multi-objective NSGA-II code in C. Available at URL:http://www.iitk.ac.in/kangal/codes.shtml. Revision 1.1.6</ref> which implements the NSGA-II procedure with GAs, or the program posted on Internet,<ref>{{cite web|last=Ortiz|first=Gilberto A.|title=Multi-objective optimization using ES as Evolutionary Algorithm.|url=http://www.mathworks.com/matlabcentral/fileexchange/35824-multi-objective-optimization-using-evolution-strategies-es-as-evolutionary-algorithm-ea|publisher=Mathworks|accessdate=1 November 2012}}</ref> which implements the NSGA-II procedure with ES.\n\nJust a general form of the equation, a plot of the objective function, boundaries of the object variables and the coordinates of global minima are given herein.\n\n==Test functions for single-objective optimization==\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n! Name !! Plot                               !! Formula !! Global minimum !! Search domain\n|-\n| [[Rastrigin function]]\n|| [[File:Rastrigin_function.png|200px|Rastrigin function for n=2]]\n||<math>f(\\mathbf{x}) = A n + \\sum_{i=1}^n \\left[x_i^2 - A\\cos(2 \\pi x_i)\\right]</math>\n<math>\\text{where: }  A=10</math>\n||<math>f(0, \\dots, 0) = 0</math>\n||<math>-5.12\\le x_{i} \\le 5.12 </math>\n|-\n| [[Ackley function]]\n|| [[File:Ackley's function.pdf|200px|Ackley's function for n=2]]\n||<math>f(x,y) = -20\\exp\\left[-0.2\\sqrt{0.5\\left(x^{2}+y^{2}\\right)}\\right]</math>\n<math>-\\exp\\left[0.5\\left(\\cos 2\\pi x + \\cos 2\\pi y \\right)\\right] + e + 20</math>\n||<math>f(0,0) = 0</math>\n||<math>-5\\le x,y \\le 5</math>\n|-\n| Sphere function \n|| [[File:Sphere function in 3D.pdf|200px|Sphere function for n=2]]\n|| <math>f(\\boldsymbol{x}) = \\sum_{i=1}^{n} x_{i}^{2}</math>\n|| <math>f(x_{1}, \\dots, x_{n}) = f(0, \\dots, 0) = 0</math> \n|| <math>-\\infty \\le x_{i} \\le \\infty</math>, <math>1 \\le i \\le n</math> \n|-\n| [[Rosenbrock function]] \n|| [[File:Rosenbrock's function in 3D.pdf|200px|Rosenbrock's function for n=2]]\n|| <math>f(\\boldsymbol{x}) = \\sum_{i=1}^{n-1} \\left[ 100 \\left(x_{i+1} - x_{i}^{2}\\right)^{2} + \\left(1 - x_{i}\\right)^{2}\\right]</math>  \n|| <math>\\text{Min} =\n\\begin{cases}\nn=2 & \\rightarrow \\quad f(1,1) = 0, \\\\\nn=3 & \\rightarrow \\quad f(1,1,1) = 0, \\\\\nn>3 & \\rightarrow \\quad f(\\underbrace{1,\\dots,1}_{n \\text{ times}}) = 0 \\\\\n\\end{cases}\n</math>\n|| <math>-\\infty \\le x_{i} \\le \\infty</math>, <math>1 \\le i \\le n</math> \n|-\n| [[Beale function]]\n|| [[File:Beale's function.pdf|200px|Beale's function]]\n|| <math>f(x,y) = \\left( 1.5 - x + xy \\right)^{2} + \\left( 2.25 - x + xy^{2}\\right)^{2}</math> \n<math>+ \\left(2.625 - x+ xy^{3}\\right)^{2}</math>\n|| <math>f(3, 0.5) = 0</math>\n|| <math>-4.5 \\le x,y \\le 4.5</math> \n|-\n| [[Goldstein‚ÄìPrice function]]\n|| [[File:Goldstein Price function.pdf|200px|Goldstein‚ÄìPrice function]]\n|| <math>f(x,y) = \\left[1+\\left(x+y+1\\right)^{2}\\left(19-14x+3x^{2}-14y+6xy+3y^{2}\\right)\\right]</math> \n<math>\\left[30+\\left(2x-3y\\right)^{2}\\left(18-32x+12x^{2}+48y-36xy+27y^{2}\\right)\\right]</math>\n|| <math>f(0, -1) = 3</math>\n|| <math>-2 \\le x,y \\le 2</math>\n|-\n| [[Booth function]]\n|| [[File:Booth's function.pdf|200px|Booth's function]]\n||<math>f(x,y) = \\left( x + 2y -7\\right)^{2} + \\left(2x +y - 5\\right)^{2}</math>\n||<math>f(1,3) = 0</math>\n||<math>-10 \\le x,y \\le 10</math>\n|-\n| Bukin function N.6\n|| [[File:Bukin function 6.pdf|200px|Bukin function N.6]]\n|| <math>f(x,y) = 100\\sqrt{\\left|y - 0.01x^{2}\\right|} + 0.01 \\left|x+10 \\right|.\\quad</math>\n|| <math>f(-10,1) = 0</math>\n|| <math>-15\\le x \\le -5</math>, <math>-3\\le y \\le 3</math>\n|-\n| [[Matyas function]]\n|| [[File:Matyas function.pdf|200px|Matyas function]]\n|| <math>f(x,y) = 0.26 \\left( x^{2} + y^{2}\\right) - 0.48 xy</math>\n|| <math>f(0,0) = 0</math>\n|| <math>-10\\le x,y \\le 10</math>\n|-\n| L√©vi function N.13\n||[[File:Levi function 13.pdf|200px|L√©vi function N.13]]\n|| <math>f(x,y) = \\sin^{2} 3\\pi x + \\left(x-1\\right)^{2}\\left(1+\\sin^{2} 3\\pi y\\right)</math>\n<math>+\\left(y-1\\right)^{2}\\left(1+\\sin^{2} 2\\pi y\\right)</math>\n|| <math>f(1,1) = 0</math>\n|| <math>-10\\le x,y \\le 10</math>\n|-\n| [[Himmelblau's function]]\n||[[File:Himmelblau function.svg|200px|Himmelblau's function]]\n|| <math>f(x, y) = (x^2+y-11)^2 + (x+y^2-7)^2.\\quad</math>\n|| <math>\\text{Min} =\n\\begin{cases}\n      f\\left(3.0,  2.0\\right) & = 0.0 \\\\\n      f\\left(-2.805118, 3.131312\\right) & = 0.0 \\\\\n      f\\left(-3.779310, -3.283186\\right) & = 0.0 \\\\\n      f\\left(3.584428, -1.848126\\right) & = 0.0 \\\\\n\\end{cases}\n</math>\n|| <math>-5\\le x,y \\le 5</math>\n|-\n| Three-hump camel function\n||[[File:Three Hump Camel function.pdf|200px|Three Hump Camel function]]\n|| <math>f(x,y) = 2x^{2} - 1.05x^{4} + \\frac{x^{6}}{6} + xy + y^{2}</math>\n|| <math>f(0,0) = 0</math>\n|| <math>-5\\le x,y \\le 5</math>\n|-\n| [[Easom function]]\n|| [[File:Easom function.pdf|200px|Easom function]]\n|| <math>f(x,y) = -\\cos \\left(x\\right)\\cos \\left(y\\right) \\exp\\left(-\\left(\\left(x-\\pi\\right)^{2} + \\left(y-\\pi\\right)^{2}\\right)\\right)</math>\n|| <math>f(\\pi , \\pi) = -1</math>\n|| <math>-100\\le x,y \\le 100</math>\n|-\n| Cross-in-tray function\n|| [[File:Cross-in-tray function.pdf|200px|Cross-in-tray function]]\n|| <math>f(x,y) = -0.0001 \\left[ \\left| \\sin x \\sin y \\exp \\left(\\left|100 - \\frac{\\sqrt{x^{2} + y^{2}}}{\\pi} \\right|\\right)\\right| + 1 \\right]^{0.1}</math>\n|| <math>\\text{Min} =\n\\begin{cases}\n      f\\left(1.34941, -1.34941\\right) & = -2.06261 \\\\\n      f\\left(1.34941,  1.34941\\right) & = -2.06261 \\\\\n      f\\left(-1.34941, 1.34941\\right) & = -2.06261 \\\\\n      f\\left(-1.34941,-1.34941\\right) & = -2.06261 \\\\\n\\end{cases}\n</math>\n|| <math>-10\\le x,y \\le 10</math>\n|-\n| [[Eggholder function]] <ref name=\"Vanaret2014\">Vanaret C., Gotteland J-B., Durand N., Alliot J-M. (2014) [https://hal-enac.archives-ouvertes.fr/hal-00996713/document Certified Global Minima for a Benchmark of Difficult Optimization Problems.] Technical report. Ecole Nationale de l'Aviation Civile. Toulouse, France.</ref>\n|| [[File:Eggholder function.pdf|200px|Eggholder function]]\n|| <math>f(x,y) = - \\left(y+47\\right) \\sin \\sqrt{\\left|\\frac{x}{2}+\\left(y+47\\right)\\right|} - x \\sin \\sqrt{\\left|x - \\left(y + 47 \\right)\\right|}</math>\n|| <math>f(512, 404.2319) = -959.6407</math>\n|| <math>-512\\le x,y \\le 512</math>\n|-\n| [[H√∂lder table function]]\n|| [[File:Holder table function.pdf|200px|Holder table function]]\n|| <math>f(x,y) = - \\left|\\sin x \\cos y \\exp \\left(\\left|1 - \\frac{\\sqrt{x^{2} + y^{2}}}{\\pi} \\right|\\right)\\right|</math> \n|| <math>\\text{Min} =\n\\begin{cases}\n      f\\left(8.05502,  9.66459\\right) & = -19.2085 \\\\\n      f\\left(-8.05502,  9.66459\\right) & = -19.2085 \\\\\n      f\\left(8.05502,-9.66459\\right) & = -19.2085 \\\\\n      f\\left(-8.05502,-9.66459\\right) & = -19.2085\n\\end{cases}\n</math> \n|| <math>-10\\le x,y \\le 10</math>\n|-\n| [[McCormick function]]\n|| [[File:McCormick function.pdf|200px|McCormick function]]\n|| <math>f(x,y) = \\sin \\left(x+y\\right) + \\left(x-y\\right)^{2} - 1.5x + 2.5y + 1</math>\n|| <math>f(-0.54719,-1.54719) = -1.9133</math>\n|| <math>-1.5\\le x \\le 4</math>, <math>-3\\le y \\le 4</math>\n|-\n| Schaffer function N. 2\n|| [[File:Schaffer function 2.pdf|200px|Schaffer function N.2]]\n|| <math>f(x,y) = 0.5 + \\frac{\\sin^{2}\\left(x^{2} - y^{2}\\right) - 0.5}{\\left[1 + 0.001\\left(x^{2} + y^{2}\\right) \\right]^{2}}</math>\n|| <math>f(0, 0) = 0</math>\n|| <math>-100\\le x,y \\le 100</math>\n|-\n| Schaffer function N. 4\n|| [[File:Schaffer function 4.pdf|200px|Schaffer function N.4]]\n|| <math>f(x,y) = 0.5 + \\frac{\\cos^{2}\\left[\\sin \\left( \\left|x^{2} - y^{2}\\right|\\right)\\right] - 0.5}{\\left[1 + 0.001\\left(x^{2} + y^{2}\\right) \\right]^{2}}</math>\n|| <math>\\text{Min} =\n\\begin{cases}\n    f\\left(0,1.25313\\right) & = 0.292579 \\\\\n    f\\left(0,-1.25313\\right) & = 0.292579\n\\end{cases}\n</math>\n|| <math>-100\\le x,y \\le 100</math>\n|-\n| [[Styblinski‚ÄìTang function]]\n|| [[File:Styblinski-Tang function.pdf|200px|Styblinski-Tang function]]\n|| <math>f(\\boldsymbol{x}) = \\frac{\\sum_{i=1}^{n} x_{i}^{4} - 16x_{i}^{2} + 5x_{i}}{2}</math>\n|| <math>-39.16617n < f(\\underbrace{-2.903534, \\ldots, -2.903534}_{n \\text{ times}} ) < -39.16616n</math>\n|| <math>-5\\le x_{i} \\le 5</math>, <math>1\\le i \\le n</math>..\n|}\n\n==Test functions for constrained optimization==\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n! Name !! Plot           !! Formula !! Global minimum !! Search domain\n|-\n| Rosenbrock function constrained with a cubic and a line<ref>{{cite conference |author1=Simionescu, P.A. |author2=Beale, D. |title=New Concepts in Graphic Visualization of Objective Functions |conference=ASME 2002 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference |location=Montreal, Canada |date=September 29 ‚Äì October 2, 2002|pages=891‚Äì897 |url=http://faculty.tamucc.edu/psimionescu/PDFs/DETC02-DAC-34129.pdf |accessdate=7 January 2017 }}</ref>\n|| [[File:ConstrTestFunc04.png|200px|Rosenbrock function constrained with a cubic and a line]]\n|| <math>f(x,y) = (1-x)^2 + 100(y-x^2)^2</math>,\n\nsubjected to: <math> (x-1)^3 - y + 1 \\le 0 \\text{  and  } x + y - 2 \\le 0 </math>\n|| <math>f(1.0,1.0) = 0</math>\n|| <math>-1.5\\le x \\le 1.5</math>, <math>-0.5\\le y \\le 2.5</math>\n|-\n| Rosenbrock function constrained to a disk<ref>{{Cite web|url=https://www.mathworks.com/help/optim/ug/example-nonlinear-constrained-minimization.html?requestedDomain=www.mathworks.com|title=Solve a Constrained Nonlinear Problem - MATLAB & Simulink|website=www.mathworks.com|access-date=2017-08-29}}</ref>\n|| [[File:ConstrTestFunc03.png|200px|Rosenbrock function constrained to a disk]]\n|| <math>f(x,y) = (1-x)^2 + 100(y-x^2)^2</math>,\n\nsubjected to: <math> x^2 + y^2 \\le 2 </math>\n|| <math>f(1.0,1.0) = 0</math>\n|| <math>-1.5\\le x \\le 1.5</math>, <math>-1.5\\le y \\le 1.5</math>\n|-\n| Mishra's Bird function - constrained<ref>{{Cite web|url=http://www.phoenix-int.com/software/benchmark_report/bird_constrained.php|title=Bird Problem (Constrained) {{!}} Phoenix Integration|access-date=2017-08-29|deadurl=bot: unknown|archiveurl=https://web.archive.org/web/20161229032528/http://www.phoenix-int.com/software/benchmark_report/bird_constrained.php|archivedate=2016-12-29|df=}}</ref><ref>{{Cite journal|last=Mishra|first=Sudhanshu|date=2006|title=Some new test functions for global optimization and performance of repulsive particle swarm method|url=https://mpra.ub.uni-muenchen.de/2718/|journal=MPRA Paper|volume=|pages=|via=}}</ref>\n|| [[File:ConstrTestFunc01.png|200px|Bird function (constrained)]]\n|| <math>f(x,y) = \\sin(y) e^{\\left [(1-\\cos x)^2\\right]} + \\cos(x) e^{\\left [(1-\\sin y)^2 \\right]} + (x-y)^2</math>,\nsubjected to: <math> (x+5)^2 + (y+5)^2 < 25 </math>\n|| <math>f(-3.1302468,-1.5821422) = -106.7645367</math>\n|| <math>-10\\le x \\le 0</math>, <math>-6.5\\le y \\le 0</math>\n|-\n| Townsend function (modified)<ref>{{Cite web|url=http://www.chebfun.org/examples/opt/ConstrainedOptimization.html|title=Constrained optimization in Chebfun|last=Townsend|first=Alex|date=January 2014|website=chebfun.org|access-date=2017-08-29}}</ref>\n|| [[File:ConstrTestFunc02.png|200px|Heart constrained multimodal function]]\n|| <math>f(x,y) = -[\\cos((x-0.1)y)]^2 - x \\sin(3x+y)</math>,\n\nsubjected to:<math>x^2+y^2 < \\left[2\\cos t - \\frac 1 2 \\cos 2t - \\frac 1 4 \\cos 3t - \\frac 1 8 \\cos 4t\\right]^2 + [2\\sin t]^2 </math>\nwhere: {{Math|1=''t'' = Atan2(x,y)}}\n|| <math>f(2.0052938,1.1944509) = -2.0239884</math>\n|| <math>-2.25\\le x \\le 2.5</math>, <math>-2.5\\le y \\le 1.75</math>\n|-\n| [[Simionescu function]]<ref>{{cite book|last=Simionescu|first=P.A.|title=Computer Aided Graphing and Simulation Tools for AutoCAD Users|year=2014|publisher=CRC Press|location=Boca Raton, FL|isbn=978-1-4822-5290-3|edition=1st}}</ref>\n|| [[File:Simionescu's function.svg|200px|Simionescu function]]\n|| <math>f(x,y) = 0.1xy</math>,\nsubjected to: <math> x^2+y^2\\le\\left[r_{T}+r_{S}\\cos\\left(n \\arctan \\frac{x}{y} \\right)\\right]^2</math>\n<math>\\text{where: }  r_{T}=1, r_{S}=0.2 \\text{ and } n = 8</math>\n|| <math>f(\\pm 0.84852813,\\mp 0.84852813) = -0.072</math>\n|| <math>-1.25\\le x,y \\le 1.25</math>\n|}\n\n==Test functions for multi-objective optimization==\n\n{{explain|reason=What does it mean to minimize two objective functions?|date=September 2016}}\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n! Name !! Plot           !! Functions !! Constraints !! Search domain\n|-\n| [[Binh and Korn function]]:\n|| [[File:Binh and Korn function.pdf|200px|Binh and Korn function]]\n|| <math>\\text{Minimize} =\n\\begin{cases}\n      f_{1}\\left(x,y\\right) = 4x^{2} + 4y^{2} \\\\\n      f_{2}\\left(x,y\\right) = \\left(x - 5\\right)^{2} + \\left(y - 5\\right)^{2} \\\\\n\\end{cases}\n</math>\n||<math>\\text{s.t.} =\n\\begin{cases}\n      g_{1}\\left(x,y\\right) = \\left(x - 5\\right)^{2} + y^{2} \\leq 25 \\\\\n      g_{2}\\left(x,y\\right) = \\left(x - 8\\right)^{2} + \\left(y + 3\\right)^{2} \\geq 7.7 \\\\\n\\end{cases}\n</math>\n|| <math>0\\le x \\le 5</math>, <math>0\\le y \\le 3</math>\n|-\n| [[Chakong and Haimes function]]:\n|| [[File:Chakong and Haimes function.pdf|200px|Chakong and Haimes function]]\n|| <math>\\text{Minimize} =\n\\begin{cases}\n      f_{1}\\left(x,y\\right) = 2 + \\left(x-2\\right)^{2} + \\left(y-1\\right)^{2} \\\\\n      f_{2}\\left(x,y\\right) = 9x - \\left(y - 1\\right)^{2} \\\\\n\\end{cases}\n</math>\n|| <math>\\text{s.t.} =\n\\begin{cases}\n      g_{1}\\left(x,y\\right) = x^{2} + y^{2} \\leq 225 \\\\\n      g_{2}\\left(x,y\\right) = x - 3y + 10 \\leq 0 \\\\\n\\end{cases}\n</math>\n|| <math>-20\\le x,y \\le 20</math>\n|-\n| [[Fonseca‚ÄìFleming function]]:<ref name=\"FonzecaFleming:1995\">{{cite journal |first=C. M. |last=Fonseca |first2=P. J. |last2=Fleming |title=An Overview of Evolutionary Algorithms in Multiobjective Optimization |journal=[[Evolutionary Computation (journal)|Evol Comput]] |volume=3 |issue=1 |pages=1‚Äì16 |year=1995 |doi=10.1162/evco.1995.3.1.1 |citeseerx=10.1.1.50.7779 }}</ref>\n|| [[File:Fonseca and Fleming function.pdf|200px|Fonseca and Fleming function]]\n|| <math>\\text{Minimize} =\n\\begin{cases}\n      f_{1}\\left(\\boldsymbol{x}\\right) = 1 - \\exp \\left[-\\sum_{i=1}^{n} \\left(x_{i} - \\frac{1}{\\sqrt{n}} \\right)^{2} \\right] \\\\\n      f_{2}\\left(\\boldsymbol{x}\\right) = 1 - \\exp \\left[-\\sum_{i=1}^{n} \\left(x_{i} + \\frac{1}{\\sqrt{n}} \\right)^{2} \\right] \\\\\n\\end{cases}\n</math>\n||\n|| <math>-4\\le x_{i} \\le 4</math>, <math>1\\le i \\le n</math>\n|-\n| Test function 4:<ref name=\"Binh99\"/>\n|| [[File:Test function 4 - Binh.pdf|200px|Test function 4.<ref name=\"Binh99\" />]]\n|| <math>\\text{Minimize} =\n\\begin{cases}\n      f_{1}\\left(x,y\\right) = x^{2} - y \\\\\n      f_{2}\\left(x,y\\right) = -0.5x - y - 1 \\\\\n\\end{cases}\n</math>\n|| <math>\\text{s.t.} =\n\\begin{cases}\n      g_{1}\\left(x,y\\right) = 6.5 - \\frac{x}{6} - y \\geq 0 \\\\\n      g_{2}\\left(x,y\\right) = 7.5  - 0.5x - y \\geq 0 \\\\\n      g_{3}\\left(x,y\\right) = 30  - 5x - y \\geq 0 \\\\\n\\end{cases}\n</math>\n|| <math>-7\\le x,y \\le 4</math>\n|-\n| [[Kursawe function]]:<ref name=\"Kursawe:1991\">F. Kursawe, ‚Äú[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.47.8050&rep=rep1&type=pdf A variant of evolution strategies for vector optimization],‚Äù in PPSN I, Vol 496 Lect Notes in Comput Sc. Springer-Verlag, 1991, pp.&nbsp;193‚Äì197.</ref>\n|| [[File:Kursawe function.pdf|200px|Kursawe function]]\n|| <math>\\text{Minimize} =\n\\begin{cases}\n      f_{1}\\left(\\boldsymbol{x}\\right) = \\sum_{i=1}^{2} \\left[-10 \\exp \\left(-0.2 \\sqrt{x_{i}^{2} + x_{i+1}^{2}} \\right) \\right] \\\\\n      & \\\\\n      f_{2}\\left(\\boldsymbol{x}\\right) = \\sum_{i=1}^{3} \\left[\\left|x_{i}\\right|^{0.8}  + 5 \\sin \\left(x_{i}^{3} \\right) \\right] \\\\\n\\end{cases}\n</math>\n||\n||<math>-5\\le x_{i} \\le 5</math>, <math>1\\le i \\le 3</math>.\n|-\n| Schaffer function N. 1:<ref name=\"Schaffer:1984\">{{cite thesis |type=PhD |last=Schaffer |first=J. David |date=1984 |title=Some experiments in machine learning using vector evaluated genetic algorithms (artificial intelligence, optimization, adaptation, pattern recognition) |publisher=Vanderbilt University |oclc=20004572 }}</ref>\n|| [[File:Schaffer function 1.pdf|200px|Schaffer function N.1]]\n|| <math>\\text{Minimize} =\n\\begin{cases}\n      f_{1}\\left(x\\right) = x^{2} \\\\\n      f_{2}\\left(x\\right) = \\left(x-2\\right)^{2} \\\\\n\\end{cases}\n</math>\n||\n|| <math>-A\\le x \\le A</math>. Values of <math>A</math> from <math>10</math> to <math>10^{5}</math> have been used successfully. Higher values of <math>A</math> increase the difficulty of the problem.\n|-\n| Schaffer function N. 2:\n|| [[File:Schaffer function 2 - multi-objective.pdf|200px|Schaffer function N.2]]\n|| <math>\\text{Minimize} =\n\\begin{cases}\n      f_{1}\\left(x\\right) = \\begin{cases}\n                                -x,   & \\text{if } x \\le 1 \\\\\n                                 x-2, & \\text{if } 1 < x \\le 3 \\\\\n                                 4-x, & \\text{if } 3 < x \\le 4 \\\\\n                                 x-4, & \\text{if } x > 4 \\\\\n                              \\end{cases} \\\\\n      f_{2}\\left(x\\right) = \\left(x-5\\right)^{2} \\\\\n\\end{cases}\n</math>\n||\n||<math>-5\\le x \\le 10</math>.\n|-\n| Poloni's two objective function:\n|| [[File:Poloni's two objective function.pdf|200px|Poloni's two objective function]]\n|| <math>\\text{Minimize} =\n\\begin{cases}\n      f_{1}\\left(x,y\\right) = \\left[1 + \\left(A_{1} - B_{1}\\left(x,y\\right) \\right)^{2} + \\left(A_{2} - B_{2}\\left(x,y\\right) \\right)^{2} \\right] \\\\\n      f_{2}\\left(x,y\\right) = \\left(x + 3\\right)^{2} + \\left(y + 1 \\right)^{2} \\\\\n\\end{cases}\n</math>\n<math>\\text{where} =\n\\begin{cases}\n      A_{1} = 0.5 \\sin \\left(1\\right) - 2 \\cos \\left(1\\right) + \\sin \\left(2\\right) - 1.5 \\cos \\left(2\\right)  \\\\\n      A_{2} = 1.5 \\sin \\left(1\\right) - \\cos \\left(1\\right) + 2 \\sin \\left(2\\right) - 0.5 \\cos \\left(2\\right)  \\\\\n      B_{1}\\left(x,y\\right) = 0.5 \\sin \\left(x\\right) - 2 \\cos \\left(x\\right) + \\sin \\left(y\\right) - 1.5 \\cos \\left(y\\right)  \\\\\n      B_{2}\\left(x,y\\right) = 1.5 \\sin \\left(x\\right) - \\cos \\left(x\\right) + 2 \\sin \\left(y\\right) - 0.5 \\cos \\left(y\\right)\n\\end{cases}\n</math>\n||\n||<math>-\\pi\\le x,y \\le \\pi</math>\n|-\n| Zitzler‚ÄìDeb‚ÄìThiele's function N. 1:\n|| [[File:Zitzler-Deb-Thiele's function 1.pdf|200px|Zitzler-Deb-Thiele's function N.1]]\n|| <math>\\text{Minimize} =\n\\begin{cases}\n      f_{1}\\left(\\boldsymbol{x}\\right) = x_{1} \\\\\n      f_{2}\\left(\\boldsymbol{x}\\right) = g\\left(\\boldsymbol{x}\\right) h \\left(f_{1}\\left(\\boldsymbol{x}\\right),g\\left(\\boldsymbol{x}\\right)\\right) \\\\\n      g\\left(\\boldsymbol{x}\\right) = 1 + \\frac{9}{29} \\sum_{i=2}^{30} x_{i} \\\\\n      h \\left(f_{1}\\left(\\boldsymbol{x}\\right),g\\left(\\boldsymbol{x}\\right)\\right) = 1 - \\sqrt{\\frac{f_{1}\\left(\\boldsymbol{x}\\right)}{g\\left(\\boldsymbol{x}\\right)}} \\\\\n\\end{cases}\n</math>\n||\n||<math>0\\le x_{i} \\le 1</math>, <math>1\\le i \\le 30</math>.\n|-\n| Zitzler‚ÄìDeb‚ÄìThiele's function N. 2:\n|| [[File:Zitzler-Deb-Thiele's function 2.pdf|200px|Zitzler-Deb-Thiele's function N.2]]\n|| <math>\\text{Minimize} =\n\\begin{cases}\n      f_{1}\\left(\\boldsymbol{x}\\right) = x_{1} \\\\\n      f_{2}\\left(\\boldsymbol{x}\\right) = g\\left(\\boldsymbol{x}\\right) h \\left(f_{1}\\left(\\boldsymbol{x}\\right),g\\left(\\boldsymbol{x}\\right)\\right) \\\\\n      g\\left(\\boldsymbol{x}\\right) = 1 + \\frac{9}{29} \\sum_{i=2}^{30} x_{i} \\\\\n      h \\left(f_{1}\\left(\\boldsymbol{x}\\right),g\\left(\\boldsymbol{x}\\right)\\right) = 1 - \\left(\\frac{f_{1}\\left(\\boldsymbol{x}\\right)}{g\\left(\\boldsymbol{x}\\right)}\\right)^{2} \\\\\n\\end{cases}\n</math>\n||\n|| <math>0\\le x_{i} \\le 1</math>, <math>1\\le i \\le 30</math>.\n|-\n| Zitzler‚ÄìDeb‚ÄìThiele's function N. 3:\n|| [[File:Zitzler-Deb-Thiele's function 3.pdf|200px|Zitzler-Deb-Thiele's function N.3]]\n||<math>\\text{Minimize} =\n\\begin{cases}\n      f_{1}\\left(\\boldsymbol{x}\\right) = x_{1} \\\\\n      f_{2}\\left(\\boldsymbol{x}\\right) = g\\left(\\boldsymbol{x}\\right) h \\left(f_{1}\\left(\\boldsymbol{x}\\right),g\\left(\\boldsymbol{x}\\right)\\right) \\\\\n      g\\left(\\boldsymbol{x}\\right) = 1 + \\frac{9}{29} \\sum_{i=2}^{30} x_{i} \\\\\n      h \\left(f_{1}\\left(\\boldsymbol{x}\\right),g\\left(\\boldsymbol{x}\\right)\\right) = 1 - \\sqrt{\\frac{f_{1}\\left(\\boldsymbol{x}\\right)}{g\\left(\\boldsymbol{x} \\right)}} - \\left(\\frac{f_{1}\\left(\\boldsymbol{x}\\right)}{g\\left(\\boldsymbol{x}\\right)} \\right) \\sin \\left(10 \\pi f_{1} \\left(\\boldsymbol{x} \\right) \\right)\n\\end{cases}\n</math>\n||\n||<math>0\\le x_{i} \\le 1</math>, <math>1\\le i \\le 30</math>.\n|-\n| Zitzler‚ÄìDeb‚ÄìThiele's function N. 4:\n|| [[File:Zitzler-Deb-Thiele's function 4.pdf|200px|Zitzler-Deb-Thiele's function N.4]]\n|| <math>\\text{Minimize} =\n\\begin{cases}\n      f_{1}\\left(\\boldsymbol{x}\\right) = x_{1} \\\\\n      f_{2}\\left(\\boldsymbol{x}\\right) = g\\left(\\boldsymbol{x}\\right) h \\left(f_{1}\\left(\\boldsymbol{x}\\right),g\\left(\\boldsymbol{x}\\right)\\right) \\\\\n      g\\left(\\boldsymbol{x}\\right) = 91 + \\sum_{i=2}^{10} \\left(x_{i}^{2} - 10 \\cos \\left(4 \\pi x_{i}\\right) \\right) \\\\\n      h \\left(f_{1}\\left(\\boldsymbol{x}\\right),g\\left(\\boldsymbol{x}\\right)\\right) = 1 - \\sqrt{\\frac{f_{1}\\left(\\boldsymbol{x}\\right)}{g\\left(\\boldsymbol{x} \\right)}}\n\\end{cases}\n</math>\n||\n||<math>0\\le x_{1} \\le 1</math>, <math>-5\\le x_{i} \\le 5</math>, <math>2\\le i \\le 10</math>\n|-\n| Zitzler‚ÄìDeb‚ÄìThiele's function N. 6:\n|| [[File:Zitzler-Deb-Thiele's function 6.pdf|200px|Zitzler-Deb-Thiele's function N.6]]\n||<math>\\text{Minimize} =\n\\begin{cases}\n      f_{1}\\left(\\boldsymbol{x}\\right) = 1 - \\exp \\left(-4x_{1}\\right)\\sin^{6}\\left(6 \\pi x_{1} \\right) \\\\\n      f_{2}\\left(\\boldsymbol{x}\\right) = g\\left(\\boldsymbol{x}\\right) h \\left(f_{1}\\left(\\boldsymbol{x}\\right),g\\left(\\boldsymbol{x}\\right)\\right) \\\\\n      g\\left(\\boldsymbol{x}\\right) = 1 + 9 \\left[\\frac{\\sum_{i=2}^{10} x_{i}}{9}\\right]^{0.25} \\\\\n      h \\left(f_{1}\\left(\\boldsymbol{x}\\right),g\\left(\\boldsymbol{x}\\right)\\right) = 1 - \\left(\\frac{f_{1}\\left(\\boldsymbol{x}\\right)}{g\\left(\\boldsymbol{x} \\right)}\\right)^{2} \\\\\n\\end{cases}\n</math>\n||\n||<math>0\\le x_{i} \\le 1</math>, <math>1\\le i \\le 10</math>.\n|-\n| Osyczka and Kundu function:\n|| [[File:Osyczka and Kundu function.pdf|200px|Osyczka and Kundu function]]\n||<math>\\text{Minimize} =\n\\begin{cases}\n      f_{1}\\left(\\boldsymbol{x}\\right) = -25 \\left(x_{1}-2\\right)^{2} - \\left(x_{2}-2\\right)^{2} - \\left(x_{3}-1\\right)^{2}\n- \\left(x_{4}-4\\right)^{2} - \\left(x_{5}-1\\right)^{2} \\\\\n      f_{2}\\left(\\boldsymbol{x}\\right) = \\sum_{i=1}^{6} x_{i}^{2} \\\\\n\\end{cases}\n</math>\n||<math>\\text{s.t.} =\n\\begin{cases}\n      g_{1}\\left(\\boldsymbol{x}\\right) = x_{1} + x_{2} - 2 \\geq 0 \\\\\n      g_{2}\\left(\\boldsymbol{x}\\right) = 6 - x_{1} - x_{2} \\geq 0 \\\\\n      g_{3}\\left(\\boldsymbol{x}\\right) = 2 - x_{2} + x_{1} \\geq 0 \\\\\n      g_{4}\\left(\\boldsymbol{x}\\right) = 2 - x_{1} + 3x_{2} \\geq 0 \\\\\n      g_{5}\\left(\\boldsymbol{x}\\right) = 4 - \\left(x_{3}-3\\right)^{2} - x_{4} \\geq 0 \\\\\n      g_{6}\\left(\\boldsymbol{x}\\right) = \\left(x_{5} - 3\\right)^{2} + x_{6} - 4 \\geq 0\n\\end{cases}\n</math>\n|| <math>0\\le x_{1},x_{2},x_{6} \\le 10</math>, <math>1\\le x_{3},x_{5} \\le 5</math>, <math>0\\le x_{4} \\le 6</math>.\n|-\n| CTP1 function (2 variables):<ref name=\"Deb:2002\"/>\n|| [[File:CTP1 function (2 variables).pdf|200px|CTP1 function (2 variables).<ref name=\"Deb:2002\" />]]\n|| <math>\\text{Minimize} =\n\\begin{cases}\n      f_{1}\\left(x,y\\right) = x \\\\\n      f_{2}\\left(x,y\\right) = \\left(1 + y\\right) \\exp \\left(-\\frac{x}{1+y} \\right)\n\\end{cases}\n</math>\n||<math>\\text{s.t.} =\n\\begin{cases}\n      g_{1}\\left(x,y\\right) = \\frac{f_{2}\\left(x,y\\right)}{0.858 \\exp \\left(-0.541 f_{1}\\left(x,y\\right)\\right)} \\geq 1 \\\\\n      g_{2}\\left(x,y\\right) = \\frac{f_{2}\\left(x,y\\right)}{0.728 \\exp \\left(-0.295 f_{1}\\left(x,y\\right)\\right)} \\geq 1\n\\end{cases}\n</math>\n|| <math>0\\le x,y \\le 1</math>.\n|-\n| Constr-Ex problem:<ref name=\"Deb:2002\"/>\n|| [[File:Constr-Ex problem.pdf|200px|Constr-Ex problem.<ref name=\"Deb:2002\" />]]\n|| <math>\\text{Minimize} =\n\\begin{cases}\n      f_{1}\\left(x,y\\right) = x \\\\\n      f_{2}\\left(x,y\\right) = \\frac{1 + y}{x} \\\\\n\\end{cases}\n</math>\n|| <math>\\text{s.t.} =\n\\begin{cases}\n      g_{1}\\left(x,y\\right) = y + 9x \\geq 6 \\\\\n      g_{1}\\left(x,y\\right) = -y + 9x \\geq 1 \\\\\n\\end{cases}\n</math>\n|| <math>0.1\\le x \\le 1</math>, <math>0\\le y \\le 5</math>\n|-\n| Viennet function:\n|| [[File:Viennet function.pdf|200px|Viennet function]]\n|| <math>\\text{Minimize} =\n\\begin{cases}\n      f_{1}\\left(x,y\\right) = 0.5\\left(x^{2} + y^{2}\\right) + \\sin\\left(x^{2} + y^{2} \\right) \\\\\n      f_{2}\\left(x,y\\right) = \\frac{\\left(3x - 2y + 4\\right)^{2}}{8} +  \\frac{\\left(x - y + 1\\right)^{2}}{27} + 15 \\\\\n      f_{3}\\left(x,y\\right) = \\frac{1}{x^{2} + y^{2} + 1} - 1.1 \\exp \\left(- \\left(x^{2} + y^{2} \\right) \\right) \\\\\n\\end{cases}\n</math>\n||\n||<math>-3\\le x,y \\le 3</math>.\n|}\n\n== See also ==\n{{commons category|Test functions (mathematical optimization)}}\n* [[Ackley function]]\n* [[Himmelblau's function]]\n* [[Rastrigin function]]\n* [[Rosenbrock function]]\n* [[Shekel function]]\n\n==References==\n\n<references/>\n\n==External links==\n*[https://www.sfu.ca/~ssurjano/index.html Virtual Library of Simulation Experiments: Test Functions and Datasets]\n*[http://benchmarkfcns.xyz/fcns Benchmarkfcns] - Categorized collection of optimization benchmark functions and source code\n*[http://infinity77.net/global_optimization/test_functions.html Test Functions Index] - with an estimate of \"hardness\" of the problem\n*[https://www.cs.unm.edu/~neal.holts/dga/benchmarkFunction/index.html Benchmark functions] - Categorized list\n*[http://www-optima.amp.i.kyoto-u.ac.jp/member/student/hedar/Hedar_files/TestGO.htm Global Optimization Test Problems] - Constrained and unconstrained\n*[https://deap.readthedocs.io/en/master/api/benchmarks.html DEAP Benchmarks]\n\n{{DEFAULTSORT:Test functions for optimization}}\n[[Category:Constraint programming]]\n[[Category:Convex optimization]]\n[[Category:Types of functions]]"
    },
    {
      "title": "Wolfe duality",
      "url": "https://en.wikipedia.org/wiki/Wolfe_duality",
      "text": "{{Refimprove|date=May 2012}}\n\nIn [[mathematical optimization]], '''Wolfe duality''', named after [[Philip Wolfe (mathematician)|Philip Wolfe]], is type of [[Duality (optimization)|dual problem]] in which the [[objective function]] and constraints are all [[differentiable function]]s.  Using this concept a lower bound for a minimization problem can be found because of the [[weak duality]] principle.<ref name=\"Wolfe\">{{cite journal|author=Philip Wolfe|title=A duality theorem for non-linear programming|journal=Quarterly of Applied Mathematics|volume=19|year=1961|pages=239‚Äì244}}</ref>\n\n== Mathematical formulation ==\nFor a minimization problem with inequality constraints,\n\n: <math>\\begin{align}\n&\\underset{x}{\\operatorname{minimize}}& & f(x) \\\\\n&\\operatorname{subject\\;to}\n& &g_i(x) \\leq 0, \\quad i = 1,\\dots,m\n\\end{align}</math>\n\nthe [[Lagrangian duality|Lagrangian dual problem]] is\n\n: <math>\\begin{align}\n&\\underset{u}{\\operatorname{maximize}}& & \\inf_x \\left(f(x) + \\sum_{j=1}^m u_j g_j(x)\\right) \\\\\n&\\operatorname{subject\\;to}\n& &u_i \\geq 0, \\quad i = 1,\\dots,m\n\\end{align}</math>\n\nwhere the objective function is the Lagrange dual function. Provided that the functions <math>f</math> and <math>g_1, \\ldots, g_m</math> are convex and continuously differentiable, the [[Infimum and supremum|infimum]] occurs where the gradient is equal to zero. The problem  \n\n: <math>\\begin{align}\n&\\underset{x, u}{\\operatorname{maximize}}& & f(x) + \\sum_{j=1}^m u_j g_j(x) \\\\\n&\\operatorname{subject\\;to}\n& & \\nabla f(x) + \\sum_{j=1}^m u_j \\nabla g_j(x) = 0 \\\\\n&&&u_i \\geq 0, \\quad i = 1,\\dots,m\n\\end{align}</math>\n\nis called the Wolfe dual problem.<ref>{{cite web|title=Chapter 3. Duality in convex optimization|date=October 30, 2011|url=http://wwwhome.math.utwente.nl/~stillgj/conopt/chap3.pdf|format=pdf|accessdate=May 20, 2012}}</ref>  This problem employs the [[KKT conditions]] as a constraint.  Also, the equality constraint <math>\\nabla f(x) + \\sum_{j=1}^m u_j \\nabla g_j(x)</math> is nonlinear in general, so the Wolfe dual problem may be a nonconvex optimization problem. In any case, weak duality holds.<ref>{{cite journal |last1=Geoffrion |first1=Arthur M. |title=Duality in Nonlinear Programming: A Simplified Applications-Oriented Development | jstor=2028848 |journal=SIAM Review |volume=13 |year=1971 |pages=1‚Äì37 |issue=1 |doi=10.1137/1013001}}</ref>\n\n== See also ==\n* [[Lagrangian duality]]\n* [[Fenchel duality]]\n\n== References ==\n{{Reflist}}\n\n[[Category:Convex optimization]]\n\n{{applied-math-stub}}"
    },
    {
      "title": "ŒëŒíŒí",
      "url": "https://en.wikipedia.org/wiki/%CE%91%CE%92%CE%92",
      "text": "{{Redirect|Alpha BB|other uses|ABB (disambiguation)}}\n{{DISPLAYTITLE:Œ±ŒíŒí}}\n\n'''Œ±ŒíŒí''' is a second-order [[deterministic global optimization]] algorithm for finding the optima of general, twice continuously differentiable functions.<ref>\"[https://www.researchgate.net/profile/Costas_Maranas/publication/2731734_A_Global_Optimization_Approach_for_Lennard-Jones_Microclusters/links/0fcfd50a3ced593b65000000.pdf A global optimization approach for Lennard-Jones microclusters].\" ''Journal of Chemical Physics'', 1992, 97(10), 7667-7677</ref><ref>\"[https://www.researchgate.net/profile/Costas_Maranas/publication/225827466_aBB_A_Global_Optimization_Method_for_General_Constrained_Nonconvex_Problems/links/09e4150a259d2132a7000000.pdf Œ±BB: A global optimization method for general constrained nonconvex problems].\" ''Journal of Global Optimization'', 1995, 7(4), 337-363</ref> The algorithm is based around creating a [[relaxation (approximation)|relaxation]] for nonlinear functions of general form by superposing them with a quadratic of sufficient magnitude, called Œ±, such that the resulting superposition is enough to overcome the worst-case scenario of non-convexity of the original function. Since a quadratic has a diagonal [[Hessian matrix]], this superposition essentially adds a number to all diagonal elements of the original Hessian, such that the resulting Hessian is [[positive-semidefinite matrix|positive-semidefinite]]. Thus, the resulting relaxation is a [[convex function]].\n\n== Theory ==\n\nLet a function <math>{f(\\boldsymbol{x}) \\in C^2}</math> be a function of general non-linear non-convex structure, defined in a finite box \n<math>X=\\{\\boldsymbol{x}\\in \\mathbb{R}^n:\\boldsymbol{x}^L\\leq\\boldsymbol{x}\\leq\\boldsymbol{x}^U\\}</math>.\nThen, a convex underestimation (relaxation) <math>L(\\boldsymbol{x})</math> of this function can be constructed over <math>X</math> by superposing \na sum of univariate quadratics, each of sufficient magnitude to overcome the non-convexity of \n<math>{f(\\boldsymbol{x})}</math> everywhere in <math>X</math>, as follows:\n\n:<math>L(\\boldsymbol{x})=f(\\boldsymbol{x})-\\sum_{i=1}^{i=n}\\alpha_i(x_i-x_i^L)(x_i-x_i^U)</math>\n\n<math>L(\\boldsymbol{x})</math> is called the <math>\\alpha \\text{BB}</math> underestimator for general functional forms. \nIf all <math>\\alpha_i</math> are sufficiently large, the new function <math>L(\\boldsymbol{x})</math> is convex everywhere in <math>X</math>. \nThus, local minimization of <math>L(\\boldsymbol{x})</math> yields a rigorous lower bound on the value of <math>{f(\\boldsymbol{x})}</math> in that domain.\n\n== Calculation of <math>\\boldsymbol{\\alpha}</math> ==\n\nThere are numerous methods to calculate the values of the <math>\\boldsymbol{\\alpha}</math> vector.\nIt is proven that when <math>\\alpha_i=\\max\\{0,-\\frac{1}{2}\\lambda_i^{\\min}\\}</math>, where <math>\\lambda_i^{\\min}</math> is a valid lower bound on the <math>i</math>-th eigenvalue of the Hessian matrix of <math>{f(\\boldsymbol{x})}</math>, the <math>L(\\boldsymbol{x})</math> underestimator is guaranteed to be convex.\n\nOne of the most popular methods to get these valid bounds on eigenvalues is by use of the Scaled Gerschgorin theorem. Let <math>H(X)</math> be the interval Hessian matrix of <math>{f(X)}</math> over the interval <math>X</math>. Then, <math>\\forall d_i>0</math> a valid lower bound on eigenvalue <math>\\lambda_i</math> may be derived from the <math>i</math>-th row of <math>H(X)</math> as follows:\n\n:<math>\\lambda_i^{\\min}=\\underline{h_{ii}}-\\sum_{i\\neq j}(\\max(|\\underline{h_{ij}}|,|\\overline{h_{ij}}|\\frac{d_j}{d_i})</math>\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:AlphaBB}}\n[[Category:Deterministic global optimization]]"
    },
    {
      "title": "Global optimization",
      "url": "https://en.wikipedia.org/wiki/Global_optimization",
      "text": "{{more footnotes|date=December 2013}}\n\n'''Global optimization''' is a branch of [[applied mathematics]] and [[numerical analysis]] that attempts to find the global minima or maxima of a function or a set of functions on a given set. It is usually described as a minimization problem because the maximization of the real-valued function <math>g(x)</math> is obviously equivalent to the minimization of the function <math>f(x):=(-1)\\cdot g(x)</math>. \n\nGiven a possibly nonlinear and non-convex continuous function <math>f:\\Omega\\subset\\mathbb{R}^n\\to\\mathbb{R}</math> with the global minima <math>f^*</math> and the set of all global minimizers <math>X^*</math> in <math>\\Omega</math>, the standard minimization problem can be given as \n:<math>\\min_{x\\in\\Omega}f(x),</math>\nthat is, finding <math>f^*</math> and a global minimizer in <math>X^*</math>; where <math>\\Omega</math> is a (not necessarily convex) compact set defined by inequalities <math>g_i(x)\\geqslant0, i=1,\\ldots,r</math>.\n\nGlobal optimization is distinguished from local optimization by its focus on finding the minima or maxima over the given set, as opposed to finding ''local'' minima or maxima. Finding an arbitrary local minima is relatively straightforward by using classical ''local optimization'' methods. Finding the global minima of a function is far more difficult: analytical methods are frequently not applicable, and the use of numerical solution strategies often leads to very hard challenges.\n\n== General theory ==\n\nA recent approach to the global optimization problem is via '''minima distribution''' \n<ref>{{cite journal\n |author = Xiaopeng Luo\n |year = 2018\n |title = Minima distribution for global optimization\n |arxiv = 1812.03457}}</ref>. In this work, a relationship between any continuous function <math>f</math> on a compact set <math>\\Omega\\subset\\mathbb{R}^n</math> and its global minima <math>f^*</math> has been strictly established. As a typical case, it follows that\n:<math>\n\\lim_{k\\to\\infty}\\int_\\Omega f(x)m^{(k)}(x) \\, \\mathrm{d}x=f^*,~~\\textrm{where}~~m^{(k)}(x)=\\frac{e^{-kf(x)}}{\\int_\\Omega e^{-kf(x)} \\, \\mathrm{d}x};\n</math>\nmeanwhile, \n:<math>\n\\lim_{k\\to\\infty}m^{(k)}(x)\n=\\left\\{\\begin{array}{cl}\n  \\frac{1}{\\mu(X^*)}, & x\\in X^*, \\\\\n  0, & x\\in\\Omega-X^*,\n\\end{array}\\right.\n</math>\nwhere <math>\\mu(X^*)</math> is the <math>n</math>-dimensional Lebesgue measure of the set of minimizers <math>X^*\\in\\Omega</math>. And if <math>f</math> is not a constant on <math>\\Omega</math>, the monotonic relationship\n:<math>\n\\int_\\Omega f(x)m^{(k)}(x)\\,\\mathrm{d}x>\n\\int_\\Omega f(x)m^{(k+\\Delta k)}(x)\\,\\mathrm{d}x>f^*\n</math>\nholds for all <math>k\\in\\mathbb{R}</math> and <math>\\Delta k>0</math>, which implies a series of monotonous containment relationships, and one of them is, for examples,\n:<math>\n\\Omega\\supset D_f^{(k)}\\supset D_f^{(k+\\Delta k)}\\supset X^*, \\text{ where } D_f^{(k)}=\\left\\{ x \\in \\Omega : f(x)\\leqslant \\int_\\Omega f(t)m^{(k)}(t) \\, \\mathrm{d}t\\right\\}.\n</math>\nAnd we define a '''minima distribution''' to be a weak limit <math>m_{f,\\Omega}</math> such that the identity\n:<math>\n\\int_\\Omega m_{f,\\Omega}(x)\\varphi(x) \\, \\mathrm{d}x = \\lim_{k\\to\\infty} \\int_\\Omega m^{(k)}(x) \\varphi(x) \\, \\mathrm{d}x\n</math>\nholds for every smooth function <math>\\varphi</math> with compact support in <math>\\Omega</math>. Here are two immediate properties of <math>m_{f,\\Omega}</math>:\n: (1) <math>m_{f,\\Omega}</math> satisfies the identity <math>\\int_\\Omega m_{f,\\Omega}(x) \\, \\mathrm{d}x = 1</math>.\n: (2) If <math>f</math> is continuous on <math>\\Omega</math>, then <math>f^*=\\int_\\Omega f(x)m_{f,\\Omega}(x) \\, \\mathrm{d}x</math>.\n\nAs a comparison, the well-known relationship between any differentiable convex function and its minima is strictly established by the gradient. If <math>f</math> is differentiable on a convex set <math>D</math>, then <math>f</math> is convex if and only if\n:<math>\nf(y)\\geqslant f(x)+\\nabla f(x)(y-x),~~\\forall x,y\\in D;\n</math>\nthus, <math>\\nabla f(x^*)=0</math> implies that <math>f(y)\\geqslant f(x^*)</math> holds for all <math>y\\in D</math>, i.e., <math>x^*</math> is a global minimizer of <math>f</math> on <math>D</math>.\n\n== Applications ==\nTypical examples of global optimization applications include:\n* [[Protein structure prediction]] (minimize the energy/free energy function)\n* [[Computational phylogenetics]] (e.g., minimize the number of character transformations in the tree)\n* [[Traveling salesman problem]] and electrical circuit design (minimize the path length)\n* [[Chemical engineering]] (e.g., analyzing the [[Gibbs free energy|Gibbs energy]])\n* Safety verification, [[safety engineering]] (e.g., of mechanical structures, buildings)\n* [[Worst case|Worst-case analysis]]\n* Mathematical problems (e.g., the [[Kepler conjecture]])\n* Object packing (configuration design) problems\n* The starting point of several [[molecular dynamics]] simulations consists of an initial optimization of the energy of the system to be simulated.\n* [[Spin glass]]es\n* Calibration of [[radio propagation models]] and of many other models in the sciences and engineering\n* [[Curve fitting]] like [[non-linear least squares]] analysis and other generalizations, used in fitting model parameters to experimental data in chemistry, physics, biology, economics, finance, medicine, astronomy, engineering.\n\n== Deterministic methods ==\n{{main article|Deterministic global optimization}}\nThe most successful general exact strategies are:\n\n===Inner and outer approximation===\nIn both of these strategies, the set over which a function is to be optimized is approximated by polyhedra. In inner approximation, the polyhedra are contained in the set, while in outer approximation, the polyhedra contain the set.\n\n===Cutting-plane methods===\n{{main article|Cutting plane}}\nThe '''cutting-plane method''' is an umbrella term for optimization methods which iteratively refine a [[feasible set]] or objective function by means of linear inequalities, termed ''cuts''.  Such procedures are popularly used to find [[integer]] solutions to [[mixed integer linear programming]] (MILP) problems, as well as to solve general, not necessarily differentiable [[convex optimization]] problems.  The use of cutting planes to solve MILP was introduced by [[Ralph E. Gomory]] and [[V√°clav Chv√°tal]].\n\n===Branch and bound methods===\n{{main article|Branch and bound}}\n'''Branch and bound''' ('''BB''' or '''B&B''') is an [[algorithm]] design paradigm for [[discrete optimization|discrete]] and [[combinatorial optimization]] problems. A branch-and-bound algorithm consists of a systematic enumeration of candidate solutions by means of [[state space search]]: the set of candidate solutions is thought of as forming a [[Tree (graph theory)|rooted tree]] with the full set at the root. The algorithm explores ''branches'' of this tree, which represent subsets of the solution set. Before enumerating the candidate solutions of a branch, the branch is checked against upper and lower estimated ''bounds'' on the optimal solution, and is discarded if it cannot produce a better solution than the best one found so far by the algorithm.\n\n===Interval methods===\n{{main article|Interval arithmetic|Set inversion}}\n'''Interval arithmetic''', '''interval mathematics''', '''interval analysis''', or '''interval computation''', is a method developed by mathematicians since the 1950s and 1960s as an approach to putting bounds on [[rounding error]]s and [[measurement error]]s in [[numerical analysis|mathematical computation]] and thus developing [[numerical methods]] that yield reliable results. Interval arithmetic helps find reliable and guaranteed solutions to equations and optimization problems.\n\n===Methods based on real algebraic geometry===\n{{main article|Real algebraic geometry}}\n'''Real algebra''' is the part of algebra which is relevant to real algebraic (and semialgebraic) geometry. It is mostly concerned with the study of [[ordered field]]s and [[ordered ring]]s (in particular [[real closed field]]s) and their applications to the study of [[positive polynomial]]s and [[Polynomial SOS|sums-of-squares of polynomials]]. It can be used in [[convex optimization]]\n\n== Stochastic methods ==\n{{Main article|Stochastic optimization}}\n\nSeveral exact or inexact Monte-Carlo-based algorithms exist:\n\n===Direct Monte-Carlo sampling===\n{{Main article|Monte Carlo method}}\nIn this method, random simulations are used to find an approximate solution.\n\nExample: The [[traveling salesman problem]] is what is called a conventional optimization problem. That is, all the facts (distances between each destination point) needed to determine the optimal path to follow are known with certainty and the goal is to run through the possible travel choices to come up with the one with the lowest total distance. However, let's assume that instead of wanting to minimize the total distance traveled to visit each desired destination, we wanted to minimize the total time needed to reach each destination. This goes beyond conventional optimization since travel time is inherently uncertain (traffic jams, time of day, etc.). As a result, to determine our optimal path we would want to use simulation - optimization to first understand the range of potential times it could take to go from one point to another (represented by a probability distribution in this case rather than a specific distance) and then optimize our travel decisions to identify the best path to follow taking that uncertainty into account.\n\n===Stochastic tunneling===\n{{Main article|Stochastic tunneling}}\n'''Stochastic tunneling''' (STUN) is an approach to global optimization based on the [[Monte Carlo method]]-[[Sampling (signal processing)|sampling]] of the function to be objectively minimized in which the function is nonlinearly transformed to allow for easier tunneling among regions containing function minima. Easier tunneling allows for faster exploration of sample space and faster convergence to a good solution.\n\n===Parallel tempering===\n{{main article|Parallel tempering}}\n'''Parallel tempering''', also known as '''replica exchange MCMC sampling''', is a [[simulation]] method aimed at improving the dynamic properties of  [[Monte Carlo method]] simulations of physical systems, and of [[Markov chain Monte Carlo]] (MCMC) sampling methods more generally.  The replica exchange method was originally devised by Swendsen,<ref>Swendsen RH and Wang JS (1986) [https://www.researchgate.net/profile/Robert_Swendsen/publication/13255490_Replica_Monte_Carlo_Simulation_of_Spin-Glasses/links/0046352309b5f54715000000.pdf Replica Monte Carlo simulation of spin glasses] Physical Review Letters 57 : 2607‚Äì2609</ref> then extended by Geyer<ref>C. J. Geyer, (1991) in ''Computing Science and Statistics'', Proceedings of the 23rd Symposium on the Interface, American Statistical Association, New York, p. 156.</ref> and later developed, among others, by [[Giorgio Parisi]].,<ref>{{cite journal\n |author = Marco Falcioni and Michael W. Deem\n |year=1999\n |title = A Biased Monte Carlo Scheme for Zeolite Structure Solution\n |journal = J. Chem. Phys.\n |volume = 110 |issue = 3 |pages = 1754\n |doi=10.1063/1.477812\n|arxiv = cond-mat/9809085|bibcode = 1999JChPh.110.1754F}}</ref><ref>David J. Earl and Michael W. Deem (2005) [http://www.rsc.org/Publishing/Journals/CP/article.asp?doi=b509983h \"Parallel tempering: Theory, applications, and new perspectives\"],  ''Phys. Chem. Chem. Phys.'',  7, 3910</ref>\nSugita and Okamoto formulated a [[molecular dynamics]] version of parallel tempering:<ref>{{cite journal\n |author = Y. Sugita and Y. Okamoto\n |year=1999\n |title = Replica-exchange molecular dynamics method for protein folding\n |journal = Chemical Physics Letters\n |volume = 314 |pages = 141‚Äì151\n |doi=10.1016/S0009-2614(99)01123-9\n|bibcode=1999CPL...314..141S}}</ref> this is usually known as replica-exchange molecular dynamics or REMD.\n\nEssentially, one runs ''N'' copies of the system, randomly initialized, at different temperatures. Then, based on the Metropolis criterion  one exchanges configurations at different temperatures. The idea of this method\nis to make configurations at high temperatures available to the simulations at low temperatures and vice versa.\nThis results in a very robust ensemble which is able to sample both low and high energy configurations.\nIn this way, thermodynamical properties such as the specific heat, which is in general not well computed in the canonical ensemble, can be computed with great precision.\n\n==Heuristics and metaheuristics ==\n:''Main page: [[Metaheuristic]]''\n\nOther approaches include heuristic strategies to search the search space in a more or less intelligent way, including:\n* [[Ant colony optimization algorithms|Ant colony optimization]] (ACO)\n* [[Simulated annealing]], a generic probabilistic metaheuristic\n* [[Tabu search]], an extension of [[Local search (optimization)|local search]] capable of escaping from local minima\n* [[Evolutionary algorithm]]s (e.g., [[genetic algorithms]] and [[evolution strategies]])\n* [[Differential evolution]], a method that [[optimization (mathematics)|optimizes]] a problem by [[iterative method|iteratively]] trying to improve a [[candidate solution]] with regard to a given measure of quality\n* [[Swarm intelligence|Swarm-based optimization algorithms]] (e.g., [[particle swarm optimization]], [[social cognitive optimization]], [[multi-swarm optimization]] and [[ant colony optimization]])\n* [[Memetic algorithm]]s, combining global and local search strategies\n* Reactive search optimization (i.e. integration of sub-symbolic machine learning techniques into search heuristics)\n* [[Graduated optimization]], a technique that attempts to solve a difficult optimization problem by initially solving a greatly simplified problem, and progressively transforming that problem (while optimizing) until it is equivalent to the difficult optimization problem.<ref>{{cite book |first1=Neil |last1=Thacker |first2=Tim |last2=Cootes |chapterurl=http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/BMVA96Tut/node29.html |chapter=Graduated Non-Convexity and Multi-Resolution Optimization Methods |title=Vision Through Optimization |year=1996}}</ref><ref>{{cite book |first1=Andrew |last1=Blake |first2=Andrew |last2=Zisserman |url=http://research.microsoft.com/en-us/um/people/ablake/papers/VisualReconstruction/ |title=Visual Reconstruction |publisher=MIT Press |year=1987 |isbn=0-262-02271-0}}{{page needed|date=October 2011}}</ref><ref name=\"mobahi2015\">Hossein Mobahi, John W. Fisher III. \n[http://people.csail.mit.edu/hmobahi/pubs/gaussian_convenv_2015.pdf On the Link Between Gaussian Homotopy Continuation and Convex Envelopes], In Lecture Notes in Computer Science (EMMCVPR 2015), Springer, 2015.</ref>\n\n== Response surface methodology-based approaches ==\n* [[IOSO]] Indirect Optimization based on Self-Organization\n* [[Bayesian optimization]], a sequential design strategy for global [[optimization]] of black-box functions using [[Bayesian statistics]]<ref>Jonas Mockus (2013). [https://books.google.com/books?hl=en&lr=&id=VuKoCAAAQBAJ&oi=fnd&pg=PR11&dq=%22Bayesian+approach+to+global+optimization:+theory+and+applications%22&ots=_FHs41Ts93&sig=mSnIqX_IU2fFNE68zQ4T1Iz9HxU#v=onepage&q=%22Bayesian%20approach%20to%20global%20optimization%3A%20theory%20and%20applications%22&f=false Bayesian approach to global optimization: theory and applications]. Kluwer Academic.</ref>\n\n==See also==\n* [[Multidisciplinary design optimization]]\n* [[Multiobjective optimization]]\n* [[Optimization (mathematics)]]\n\n==Footnotes==\n{{reflist}}\n\n== References ==\n{{refbegin}}\nDeterministic global optimization:\n* R. Horst, H. Tuy, [https://books.google.com/books?hl=en&lr=&id=Pe_1CAAAQBAJ&oi=fnd&pg=PA2&dq=%22Global+Optimization:+Deterministic+Approaches%22&ots=cwxbZCCoNY&sig=L6nXjOxlCdqO7V31V8GbmbpE3UA#v=onepage&q=%22Global%20Optimization%3A%20Deterministic%20Approaches%22&f=false Global Optimization: Deterministic Approaches], Springer, 1996.\n* R. Horst, [[Panos M. Pardalos|P.M. Pardalos]] and N.V. Thoai, [https://books.google.com/books?hl=en&lr=&id=dbu02-1JbLIC&oi=fnd&pg=PR11&dq=%22Introduction+to+Global+Optimization%22&ots=x6eVm3MuH2&sig=-6O0NB548pudhSq5hKfAxjwYnvU#v=onepage&q=%22Introduction%20to%20Global%20Optimization%22&f=false Introduction to Global Optimization], Second Edition.  Kluwer Academic Publishers, 2000.\n*[http://www.mat.univie.ac.at/~neum/ms/glopt03.pdf A.Neumaier, Complete Search in Continuous Global Optimization and Constraint Satisfaction, pp. 271‚Äì369 in: Acta Numerica 2004 (A. Iserles, ed.), Cambridge University Press 2004.]\n* M. Mongeau, H. Karsenty, V. Rouz√© and J.-B. Hiriart-Urruty, [https://www.tandfonline.com/doi/abs/10.1080/10556780008805783 Comparison of public-domain software for black box global optimization]. Optimization Methods & Software 13(3), pp.&nbsp;203‚Äì226, 2000.\n* J.D. Pint√©r, [https://books.google.com/books?hl=en&lr=&id=uv7lBwAAQBAJ&oi=fnd&pg=PR18&dq=%22+Continuous+and+Lipschitz+Optimization%22&ots=fgmBS9IlnJ&sig=hymKlMTwfBTJoY97wYVvSVBuyx8 Global Optimization in Action - Continuous and Lipschitz Optimization: Algorithms, Implementations and Applications]. Kluwer Academic Publishers, Dordrecht, 1996. Now distributed by Springer Science and Business Media, New York. This book also discusses stochastic global optimization methods.\n* L. Jaulin, M. Kieffer, O. Didrit, E. Walter (2001). Applied Interval Analysis. Berlin: Springer.\n* E.R. Hansen (1992), Global Optimization using Interval Analysis, Marcel Dekker, New York.\n* R.G. Strongin, Ya.D. Sergeyev  (2000,2013,2014) [https://pdfs.semanticscholar.org/a6dc/009f2c4e5d22ed69fffa500c72a06cc71951.pdf Global optimization with non-convex constraints: Sequential and parallel algorithms], Kluwer Academic Publishers, Dordrecht.\n* Ya.D. Sergeyev, R.G. Strongin, D. Lera (2013) [https://books.google.com/books?hl=en&lr=&id=ZXM8AAAAQBAJ&oi=fnd&pg=PR5&dq=%22Introduction+to+global+optimization+exploiting+space-filling+curves%22&ots=PdJp0--i7l&sig=eQImrC8Bih5ZzbipVUxxcIwPUuY Introduction to global optimization exploiting space-filling curves], Springer, NY.\n* Ya.D. Sergeyev, D.E. Kvasov (2017) [https://books.google.com/books?hl=en&lr=&id=r38oDwAAQBAJ&oi=fnd&pg=PP5&dq=%22Deterministic+Global+Optimization:+An+introduction+to+the+diagonal+approach%22&ots=JuOPGIr54A&sig=SLKyqRSxSujxcS0xoqdPLyd4-W4 Deterministic Global Optimization: An introduction to the diagonal approach], Springer, NY. \n\nFor simulated annealing:\n* S. Kirkpatrick, C.D. Gelatt, and M.P. Vecchi. ''Science'', 220:671&ndash;680, 1983.\nFor reactive search optimization:\n* [[Roberto Battiti]], M. Brunato and F. Mascia, Reactive Search and Intelligent Optimization, Operations Research/Computer Science Interfaces Series, Vol. 45, Springer, November 2008. {{ISBN|978-0-387-09623-0}}\nFor stochastic methods:\n* [[Anatoly Zhigljavsky|A. Zhigljavsky]].  Theory of Global Random Search.  Mathematics and its applications. Kluwer Academic Publishers. 1991.\n* K. Hamacher. [https://iopscience.iop.org/article/10.1209/epl/i2006-10058-0/meta Adaptation in Stochastic Tunneling Global Optimization of Complex Potential Energy Landscapes], ''Europhys.Lett.'' 74(6):944, 2006.\n* K. Hamacher and W. Wenzel. [https://arxiv.org/pdf/physics/9810035 The Scaling Behaviour of Stochastic Minimization Algorithms in a Perfect Funnel Landscape]. ''Phys. Rev. E'', 59(1):938‚Äì941, 1999.\n* W. Wenzel and K. Hamacher. [https://arxiv.org/pdf/physics/9903008 A Stochastic tunneling approach for global minimization]. ''Phys. Rev. Lett.'', 82(15):3003‚Äì3007, 1999.\nFor parallel tempering:\n* U. H. E. Hansmann. ''Chem.Phys.Lett.'', 281:140, 1997.\nFor continuation methods:\n* Zhijun Wu.  [https://www.osti.gov/servlets/purl/395617 The effective energy transformation scheme as a special continuation approach to global optimization with application to molecular conformation].  Technical Report, Argonne National Lab., IL (United States), November 1996.\nFor general considerations on the dimensionality of the domain of definition of the objective function:\n* K. Hamacher. [https://www.sciencedirect.com/science/article/pii/S0378437105001834 On Stochastic Global Optimization of one-dimensional functions]. ''Physica A'' 354:547‚Äì557, 2005.\nFor strategies allowing one to compare deterministic and stochastic global optimization methods\n*[https://www.nature.com/articles/s41598-017-18940-4.pdf Ya.D. Sergeyev,  M.S. Mukhametzhanov, D.E. Kvasov (2018) On the efficiency of nature-inspired metaheuristics in expensive global optimization with limited budget, ''Scientific Reports'', 8, 453.]\n{{refend}}\n\n== External links ==\n*[http://www.globaloptimization.org/ The International Society of Global Optimization]\n*[http://www.mat.univie.ac.at/~neum/glopt.html A. Neumaier‚Äôs page on Global Optimization]\n*[http://www.lix.polytechnique.fr/~liberti/teaching/dix/inf572-09/nonconvex_optimization.pdf Introduction to global optimization by L. Liberti]\n*[http://www.it-weise.de/projects/book.pdf Free e-book by Thomas Weise]\n\n[[Category:Deterministic global optimization]]"
    },
    {
      "title": "List of optimization software",
      "url": "https://en.wikipedia.org/wiki/List_of_optimization_software",
      "text": "{{unreferenced|date=August 2013}}\nGiven a transformation between input and output values, described by a [[Function (mathematics)|mathematical function]] ''f'', [[Mathematical optimization|optimization]] deals with generating and selecting a best solution from some set of available alternatives, by systematically choosing input values from within an allowed set, computing the output of the function, and recording the best output values found during the process. Many real-world problems can be modeled in this way. For example, the inputs can be design parameters of a motor, the output can be the power consumption, or the inputs can be business choices and the output can be the obtained profit.\n\nAn [[optimization problem]], in this case a minimization problem, can be represented in the following way\n:''Given:'' a [[function (mathematics)|function]] ''f'' : ''A'' <math>\\to</math> '''R''' from some [[Set (mathematics)|set]] ''A'' to the [[real number]]s\n:''Search for:'' an element ''x''<sub>0</sub> in ''A'' such that ''f''(''x''<sub>0</sub>) ‚â§ ''f''(''x'') for all ''x'' in ''A''.\n\nIn continuous optimization, ''A'' is some [[subset]] of the [[Euclidean space]] '''R'''<sup>''n''</sup>, often specified by a set of ''[[constraint (mathematics)|constraint]]s'', equalities or inequalities that the members of ''A'' have to satisfy. In combinatorial optimization, ''A'' is some [[subset]] of a discrete space, like binary strings, permutations, or sets of integers.\n\nThe use of '''optimization software''' requires that the function ''f'' is defined in a suitable programming language and connected at compile or run time to the optimization software. The optimization software will deliver input values in ''A'', the software module realizing ''f'' will deliver the computed value ''f''(''x'') and, in some cases, additional information about the function like derivatives.\n\nIn this manner, a clear separation of concerns is obtained: different optimization software modules can be easily tested on the same function ''f'', or a given optimization software can be used for different functions ''f''.\n\nThe following tables provide a list of notable optimization software organized according to license and business model type.\n\n==Free and open-source software==\n;Applications\n:{| class=\"wikitable\"\n|-\n! Name\n! License\n! Description\n|-\n| [[ADMB]] || [[BSD]]\n| a [[Nonlinear programming|nonlinear optimization]] framework, using [[automatic differentiation]].\n|-\n| [[ASCEND]] || [[GPL]]\n| a [[mathematical model]]ling chemical process modelling system.\n|-\n| [[CUTEr]] || GPL\n| a testing environment for [[Mathematical optimization|optimization]] and [[linear algebra]] [[solver]]s.\n|-\n| [[GNU_Octave]] || GPL\n| a software package featuring a high-level programming language, primarily intended for numerical computations; well recognized free alternative to [[MATLAB]].\n|-\n| [[Scilab]] || [[CeCILL]]\n| a cross-platform numerical computational package and a high-level, numerically oriented programming language with free numerical optimization framework.\n|}\n\n;Software libraries\n:{| class=\"wikitable\"\n|-\n! Name\n! License\n! Description\n|-\n| [[ALGLIB]] || GPL\n| dual licensed (GPL/commercial) nonlinear optimization library (unconstrained, box, linearly, nonlinearly-constrained, nonlinear and QP problems), optionally using [[automatic differentiation]]. Cross-language: C++, C#.\n|-\n| [[COIN-OR SYMPHONY]] || [[Eclipse Public License|EPL 1.0]]\n| integer programming\n|-\n| [[Dlib]] || [[Boost Software License]]\n| Unconstrained/box-constrained nonlinear/QP optimization library written in [[C++]].\n|-\n| [[Gekko (optimization software)|GEKKO]] || [[MIT License]]\n| machine learning and optimization of mixed-integer and differential algebraic equations in Python.\n|-\n| [[GLPK]] || GPL\n| GNU Linear Programming Kit, C API.\n|-\n| [[IPOPT]] || [[Common Public License|CPL]]\n| a large scale nonlinear optimizer for continuous systems (requires gradient), C++ (formerly [[Fortran]] and [[C (programming language)|C]]).\n|- \n| [[Michael J. D. Powell|J. D. Powell]]'s<br />optimization suite || LGPL\n| a set of [[Fortran 95]] algorithms for (derivative-free, DFO) optimization subject to box and linear constraints: [[BOBYQA]] ‚Äì DFO, box; [[COBYLA]] ‚Äì DFO, nonlinearly constrained; [[LINCOA]] ‚Äì DFO, linearly constrained; [[NEWUOA]] and [[UOBYQA]] ‚Äì DFO, unconstrained; [[TOLMIN (optimization software)|TOLMIN]] ‚Äì linearly constrained.\n|-\n| [[MIDACO]] || Dual (Commercial, BY-NC-ND)\n| a lightweight software tool for single- and multi-objective [[Mathematical optimization|optimization]] based on [[Evolutionary computation|evolutionary computing]]. Written in C/C++ and Fortran with gateways to Excel, VBA, Java, Python, Matlab, Octave, R, C# and Julia.\n|-\n| [[MINUIT]]&nbsp;(now&nbsp;MINUIT2) || LGPL\n| an unconstrained optimizer internally developed at [[CERN]].\n|-\n| [[OpenMDAO]] || {{nowrap|Apache License}}\n| a [[Multidisciplinary design optimization|Multidisciplinary Design, Analysis, and Optimization (MDAO)]] framework, written in [[Python (programming language)|Python]]. The development is led out of the [[Glenn Research Center|NASA Glenn Research Center]], with support from the [[Langley Research Center|NASA Langley Research Center]].\n|-\n| [[OptaPlanner]] || Apache License\n| a lightweight, embeddable planning engine written in [[Java (programming language)|Java]]. It solves constraint satisfaction problems with construction heuristics and [[metaheuristic]] algorithms.\n|-\n| [[SciPy]] || BSD\n| a general numeric package for Python, with some support for optimization.\n|}\n\n==Proprietary software==\n* [[AIMMS]] ‚Äì optimization modeling system, including GUI building facilities.\n* [[ALGLIB]] ‚Äì dual licensed (GPL/commercial) constrained quadratic and nonlinear optimization library with C++ and C# interfaces.\n* [[Altair Engineering|Altair HyperStudy]] ‚Äì design of experiments and multi-disciplinary design optimization.\n* [[AMPL]] ‚Äì modelling language for large-scale linear, mixed integer and nonlinear optimization.\n* [[APMonitor]] ‚Äì modeling language and optimization suite for large-scale, nonlinear, mixed integer, differential and algebraic equations with interfaces to MATLAB, Python, and Julia.\n* [[Artelys Knitro]] ‚Äì large scale nonlinear optimization for continuous and mixed-integer programming.\n* [[ASTOS]] ‚Äì AeroSpace Trajectory Optimization Software for launcher, re-entry and generic aerospace problems.\n* [[BARON]] ‚Äì optimization of algebraic nonlinear and mixed-integer nonlinear problems.\n* [[COMSOL Multiphysics]] ‚Äì a cross-platform [[Finite element method|finite element]] analysis, solver and [[multiphysics]] [[simulation software]].\n* [[CPLEX]] ‚Äì integer, linear and quadratic programming.\n* [[FEATool Multiphysics]] ‚Äì FEA GUI Toolbox for MATLAB\n* [[FICO Xpress]] ‚Äì integer, linear and quadratic and nonlinear programming.\n* [[FortMP]] ‚Äì integer, linear and quadratic programming.\n* [[FortSP]] ‚Äì stochastic programming.\n* [[General Algebraic Modeling System|GAMS]] ‚Äì General Algebraic Modeling System.\n* [[Gurobi]] ‚Äì integer, linear and quadratic programming.\n* [[Red Cedar Technology#HEEDS MDO|HEEDS MDO]] ‚Äì multidisciplinary design optimization using SHERPA, a hybrid, adaptive optimization algorithm.\n* [[IMSL Numerical Libraries]] ‚Äì linear, quadratic, nonlinear, and sparse QP and LP optimization algorithms implemented in standard programming languages C, Java, C# .NET, Fortran, and Python.\n* [[IOSO]] ‚Äì (Indirect Optimization on the basis of Self-Organization) a multiobjective, multidimensional nonlinear optimization technology.\n* [[Kimeme]] ‚Äì an open platform for multi-objective optimization and multidisciplinary design optimization.\n* [[LINDO]] - (Linear, Interactive, and Discrete Optimizer) a software package for linear programming, integer programming, [[nonlinear programming]], stochastic programming, and global optimization. The \"What's Best!\" Excel add-in performs linear, integer, and nonlinear optimization using LINDO.\n* [[LIONsolver]] ‚Äì an integrated software for [[data mining]], [[analytics]], [[Modeling and simulation|modeling]] '''L'''earning and '''I'''ntelligent '''O'''ptimizatio'''N''' and reactive [[business intelligence]] approach.\n* [[modeFRONTIER]] ‚Äì an integration platform for multi-objective and multi-disciplinary optimization, which provides a seamless coupling with third party engineering tools, enables the automation of the design simulation process, and facilitates analytic decision making.\n* [[Maple (software)|Maple]] ‚Äì linear, quadratic, and nonlinear, continuous and integer optimization. Constrained and unconstrained. Global optimization with add-on toolbox.\n* [[MATLAB]] ‚Äì linear, integer, quadratic, and nonlinear problems with [[Optimization Toolbox]]; multiple maxima, multiple minima, and non-smooth optimization problems; estimation and optimization of model parameters.\n* [[MIDACO]] a lightweight software tool for single- and multi-objective [[Mathematical optimization|optimization]] based on [[Evolutionary computation|evolutionary computing]]. Written in C/C++ and Fortran with gateways to Excel, VBA, Java, Python, Matlab, Octave, R, C# and Julia.\n* [[Wolfram Mathematica|Mathematica]] ‚Äì large-scale multivariate constrained and unconstrained, linear and nonlinear, continuous and integer optimization.\n* [[ModelCenter]] ‚Äì a graphical environment for integration, automation, and design optimization.\n* [[MOSEK]] ‚Äì linear, quadratic, conic and convex nonlinear, continuous and integer optimization.\n* [[NAG Numerical Library|NAG]] ‚Äì linear, quadratic, nonlinear, sums of squares of linear or nonlinear functions; linear, sparse linear, nonlinear, bounded or no constraints; local and global optimization; continuous or integer problems.\n* [[NMath]] ‚Äì linear, quadratic and nonlinear programming.\n* [[OptimJ]] ‚Äì Java-based modeling language. Premium Edition includes support for Gurobi, Mosek and CPLEX solvers.\n* [[Optimus platform]] ‚Äì a process integration and design optimization platform developed by Noesis Solutions.\n* [[optiSLang]] ‚Äì software solutions for CAE-based sensitivity analysis, optimization and robustness evaluation.\n* [[OptiY]] - a design environment providing modern optimization strategies and state of the art probabilistic algorithms for uncertainty, reliability, robustness, sensitivity analysis, data-mining and meta-modeling.\n* [[OptiStruct]] ‚Äì award-winning CAE technology for conceptual design synthesis and structural optimization.\n* [[PottersWheel]] ‚Äì parameter estimation in ordinary differential equations (MATLAB toolbox, free for academic use).\n* [[pSeven]] ‚Äî software platform for automation of engineering simulation and analysis, multidisciplinary optimization and data mining, developed by [[DATADVANCE]].\n* [[SmartDO]] ‚Äì multidisciplinary global design optimization, specialized in computer-aided engineering (CAE). using the direct global search approaches. \n* [[SNOPT]] ‚Äì large-scale optimization problems.\n* [[The Unscrambler]] X ‚Äì product formulation and process optimization software.\n* [[TOMLAB]] ‚Äì supports global optimization, integer programming, all types of least squares, linear, quadratic and unconstrained programming for [[MATLAB]]. TOMLAB supports solvers like [[Gurobi]], [[CPLEX]], [[SNOPT]], [[Artelys Knitro|KNITRO]] and [[MIDACO]].\n* [[VisSim]] ‚Äì a visual [[block diagram]] language for simulation and optimization of [[dynamical system]]s.\n* [[WORHP]] ‚Äì a large-scale sparse solver for continuous nonlinear optimization.\n\n==Freeware/free for academic use{{anchor|Freeware}}==\n* [[AIMMS]]\n* [[AMPL]]\n* [[APMonitor]] ‚Äì free for academic and commercial use alike, with [[Julia (programming language)|Julia]], [[Python (programming language)|Python]] and [[MATLAB]] integrations.\n* [[ASTOS]]\n* [[CPLEX]]\n* [[FICO Xpress]]\n* [[Galahad library]]\n* [[Gekko (optimization software)|GEKKO Python]]\n* [[Gurobi]]\n* [[LIONsolver]]\n* [[MIDACO]] ‚Äì a software package for numerical [[Mathematical optimization|optimization]] based on [[Evolutionary computation|evolutionary computing]].\n* [[MINTO]] ‚Äì [[integer programming]] solver using branch and bound algorithm; freeware for personal use.\n* [[MOSEK]] ‚Äì a large scale optimization software. Solves linear, quadratic, conic and convex nonlinear, continuous and integer optimization.\n* [[OptimJ]] ‚Äì Java-based modeling language; the free edition includes support for lp_solve, [[GNU Linear Programming Kit|GLPK]] and [[Linear programming|LP]] or [[MPS (format)|MPS]] file formats.\n* [[PottersWheel]] ‚Äì parameter estimation in ordinary differential equations (free MATLAB toolbox for academic use).\n* [[SCIP (optimization software)|SCIP]] ‚Äì free to members of non-commercial and academic institutions, for research purposes.\n* [[WORHP]]\n\n==See also==\n* [[Comparison of optimization software]]\n* [[List of computer algebra systems]]\n* [[List of constraint programming languages]]\n* [[List of numerical libraries]]\n* [[List of optimization algorithms]]\n* [[List of SMT solvers]]\n\n{{Mathematical optimization software}}\n\n[[Category:Mathematical optimization software|*]]\n[[Category:Lists of software|Optimization software]]"
    },
    {
      "title": "AIMMS",
      "url": "https://en.wikipedia.org/wiki/AIMMS",
      "text": "'''AIMMS''' (acronym for '''Advanced Interactive Multidimensional Modeling System''') is a prescriptive analytics software company with offices in the Netherlands, United States, China and Singapore. \n\nIt has two main product offerings that provide modeling and optimization capabilities across a variety of industries. The [https://aimms.com/english/software-solutions/software/aimmsprescriptiveanalytics/ AIMMS Prescriptive Analytics Platform] allows advanced users to develop optimization-based applications and deploy them to business users. [https://aimms.com/english/software-solutions/software/scnavigator/ AIMMS SC Navigator], launched in 2017, is built on the AIMMS Prescriptive Analytics Platform and provides configurable Apps for supply chain teams. SC Navigator provides supply chain analytics to non-advanced users. \n\n{{Infobox programming language\n| name = AIMMS\n| logo = https://aimms.com/\n| designer = Johannes J. Bisschop <br> Marcel Roelofs\n| developer = [http://www.aimms.com AIMMS B.V.] (formerly named Paragon Decision Technology B.V.<ref>''\"We are moving forward, from now on you can call us AIMMS\"'', {{cite web|url=http://business.aimms.com/moving-forward-now-can-call-us-aimms/ |title=Archived copy |accessdate=2013-10-23 |deadurl=yes |archiveurl=https://web.archive.org/web/20131029190618/http://business.aimms.com/moving-forward-now-can-call-us-aimms/ |archivedate=2013-10-29 |df= }}</ref>)\n| website = [http://www.aimms.com AIMMS home page]\n| Year Started = 1993\n}}\n\n== History ==\nAIMMS B.V. was founded in 1989 by mathematician Johannes Bisschop under the name of Paragon Decision Technology. His vision was to make optimization more approachable by building models rather than programming. In Bisschop‚Äôs view, modeling was able to build the bridge between the people who had problems and the people helping them solve those problems. \n\n'''AIMMS''' began as a software system designed for modeling and solving large-scale optimization and scheduling-type problems.<ref>{{cite book\n| title = Modeling Languages in Mathematical Optimization\n| last = Kallrath\n| first = Joseph\n| year = 2004\n| publisher = Kluwer Academic Publishing\n| isbn = 978-1-4020-7547-6\n| url = https://books.google.com/books?id=wJYART7VYe8C&lr=&source=gbs_navlinks_s}}\n</ref><ref>{{cite book\n| title = AIMMS Language Reference\n| last = Roelofs\n| first = Marcel\n| year = 2010\n| publisher = lulu.com\n| isbn = 978-0-557-42456-6\n| url = http://www.aimms.com/aimms/download/manuals/aimms_ref.pdf}}\n</ref>  \n\nAIMMS is considered to be one of the five most important algebraic modeling languages. Bisschop was awarded with [[INFORMS]] Impact Prize for his work in this language.<ref>{{Cite web |url=http://www.informs.org/Blogs/E-News-Blog/INFORMS-Impact-Prize |title=Archived copy |access-date=2013-10-22 |archive-url=https://web.archive.org/web/20131022091250/https://www.informs.org/Blogs/E-News-Blog/INFORMS-Impact-Prize |archive-date=2013-10-22 |dead-url=yes |df= }}</ref>\n\nIn 2003, AIMMS was acquired by a small private equity firm. This led to the creation of a partnership program, further technical investment and the evolution of the platform. In 2011, the company launched AIMMS PRO, a way to deploy applications to end-users who do not have a technical background. This was quickly followed by the ability to publish and customize applications using a browser so that decision support applications are available on any device. \n\nThe company grew and was in 2017 recognized as a top B2B technology in the Netherlands.<ref>{{Cite news|url=https://blog.g2crowd.com/blog/technology-research/b2b-netherlands-tech-2017/|title=The State of the Netherland's B2B Tech Scene in 2017|date=2017-12-14|work=G2 Crowd|access-date=2018-04-12|language=en-US}}</ref> and was named one of the fastest growing companies in the Netherlands for the second consecutive year.<ref>{{Cite web|url=https://aimms.com/english/news-archive/aimms-named-one-fastest-companies-netherlands-second-consecutive-year/|title=AIMMS :: AIMMS named one of the fastest growing companies in the Netherlands for the second consecutive year|website=AIMMS|language=en|access-date=2018-04-12}}</ref>\n\n== AIMMS SC Navigator Platform ==\nAlong with a growing interest in embedded advanced analytics for [[supply chain management]], AIMSS developed the AIMMS SC Navigator Platform to allow for supply chain analytics.  It was launched in October 2017 with three initial cloud-based Apps: Supply Chain Network Design, Sales & Operations Planning and Data Navigator. In 2018 they added Center of Gravity and Product Lifecycle. \n\n==AIMMS Prescriptive Analytics Platform==\n\nThe AIMMS Prescriptive Analytics Platform consists of an [[algebraic modeling language]], an [[integrated development environment]] for both editing models and creating a [[graphical user interface]] around these models, and a graphical end-user environment.<ref>{{cite book|url=http://www.aimms.com/aimms/download/manuals/aimms_user.pdf|title=AIMMS User's Guide|last=Roelofs|first=Marcel|publisher=lulu.com|year=2010|isbn=978-0-557-06360-4}}\n</ref>\nAIMMS is linked to multiple [[solver]]s through the AIMMS Open Solver Interface.<ref>{{cite web|url=http://download.aimms.com/aimms/AimmsOSI/frames.html?frmname=topic&frmfile=index.html|title=AIMMS Open Solver Interface API|author=Paragon Decision Technology|year=2009}}\n</ref>\nSupported solvers include [[CPLEX]], [[MOSEK]], [[FICO Xpress]], CBC, Conopt, [[MINOS (optimization software)|MINOS]], [[IPOPT]], [[SNOPT]], [[KNITRO]] and [http://www.ilog.com/products/cpoptimizer/ CP Optimizer].\n\nAIMMS features a mixture of [[declarative programming|declarative]] and [[imperative programming|imperative]] programming styles. Formulation of optimization models takes place through declarative language elements such as sets and indices, as well as scalar and multidimensional parameters, variables and constraints, which are common to all [[algebraic modeling language]]s, and allow for a concise description of most problems in the domain of mathematical optimization. [[Units of measurement]] are natively supported in the language, and compile- and runtime unit analysis may be employed to detect modeling errors.\n\nProcedures and [[control flow]] statements are available in AIMMS for \n* the exchange of data with external data sources such as [[spreadsheet]]s, [[database]]s, [[XML]] and text files\n* data pre- and post-processing tasks around optimization models\n* user interface event handling\n* the construction of hybrid algorithms for problem types for which no direct efficient solvers are available. \nTo support the re-use of common modeling components, AIMMS allows modelers to organize their model in user model [[library (computing)|libraries]].\n\nAIMMS supports a wide range of mathematical optimization problem types:\n* [[Linear programming]]\n* [[Quadratic programming]]\n* [[Nonlinear programming]]\n* [[Linear programming#Integer unknowns|Mixed-integer programming]]\n* Mixed-integer nonlinear programming\n* [[Global optimization]]\n* [[Complementarity theory|Complementarity problems]] (MPECs)\n* [[Stochastic programming]]\n* [[Robust optimization]]\n* [[Constraint programming]]\n[[Uncertainty]] can be taken into account in [[deterministic system|deterministic]] linear and mixed integer optimization models in AIMMS through the specification of additional attributes, such that [[stochastic programming|stochastic]] or [[robust optimization|robust]] optimization techniques can be applied alongside the existing deterministic solution techniques.\n\nCustom hybrid and decomposition algorithms can be constructed using the GMP system library which makes available at the modeling level many of the basic building blocks used internally by the higher level solution methods present in AIMMS, matrix modification methods, as well as specialized steps for customizing solution algorithms for specific problem types.\n\nOptimization solutions created with AIMMS can be used either as a standalone [[desktop application]] or can be embedded as a [[software component]] in other applications.\n\n==Use in industry==\n\nAIMMS Prescriptive Analytics Platform is used in a wide range of industries including retail, consumer products, healthcare, oil and chemicals, steel production and agribusiness.<ref>\n{{Cite journal\n |first=Winston \n |last=Lasschuit \n |author2=Thijssen, Nort \n |title=Supporting supply chain planning and scheduling decisions in the oil and chemical industry \n |journal=Computers & Chemical Engineering \n |issue=Volume 28, Issues 6-7, FOCAPO 2003 Special issue \n |pages=863‚Äì870 \n |date=15 June 2004 \n |url=http://www.aimms.com/aimms/download/case_studies/shell_elsevier_article.pdf \n |doi=10.1016/j.compchemeng.2003.09.026 \n |deadurl=yes \n |archiveurl=https://web.archive.org/web/20110903124517/http://www.aimms.com/aimms/download/case_studies/shell_elsevier_article.pdf \n |archivedate=3 September 2011 \n}}\n</ref><ref>\n{{Cite press release\n |title=Integration and Optimisation of Crude Planning and Scheduling in the Hydrocarbon Supply Chain \n |publisher=Shell Global Solutions \n |date=January 17, 2011 \n |url=http://www.shell.com/home/content/globalsolutions/media_centre/industry_commentary/feature_articles/feature_articles/news_integration_planning.html\n}}{{dead link|date=June 2017 |bot=InternetArchiveBot |fix-attempted=yes }}</ref><ref>\n{{cite web \n |    title = 25 years of O.R. in Brazil\n |    first = Eduardo \n |     last = Medeiros Milanez\n |     work = OR/MS Today\n |date=April 2010 \n |      url = http://www.lionhrtpub.com/orms/orms-4-10/frorinbrazil.html}}\n</ref>\n\n[[Alstom|GE Grid]] uses AIMMS as the modeling and optimization engine of its [[energy market]] [[market clearing|clearing]] software.<ref>\n{{Cite conference\n |first=D. \n |last=Streiffert \n |author2=Philbrick, R. \n |author3=Ott, A. \n |title=A mixed integer programming solution for market clearing and reliability analysis \n |booktitle=Power Engineering Society General Meeting, 2005. IEEE \n |pages=2724‚Äì2731 Vol. 3 \n |date=August 1, 2005 \n |url=http://class.ece.iastate.edu/ee458/PJM-Areva.pdf \n |doi=10.1109/PES.2005.1489108 \n |deadurl=yes \n |archiveurl=https://web.archive.org/web/20110813164042/http://class.ece.iastate.edu/ee458/PJM-Areva.pdf \n |archivedate=August 13, 2011 \n}}\n</ref>\nTogether with [[Alstom|GE Grid]], AIMMS was part of the analytics team of [[Midcontinent Independent System Operator|Midwest ISO]] that won the [[Franz Edelman Award for Achievement in Operations Research and the Management Sciences]] of 2011 for successfully applying [[operations research]] in the Midwest ISO energy market.<ref>\n{{Cite press release\n  | title = Midwest ISO Wins INFORMS Edelman Award\n  | publisher = INFORMS\n  | date = April 11, 2011\n  | url = http://www.informs.org/About-INFORMS/News-Room/Press-Releases/Edelman-Winner-2011}}\n</ref> In 2012, TNT Express, an AIMMS customer won the Franz Edleman Award for modernizing its operations and reducing its carbon footprint. <ref>{{Cite web|url=https://www.informs.org/About-INFORMS/News-Room/Press-Releases/TNT-2012-Edelman-Winner|title=TNT Express Wins 2012 INFORMS Edelman Award, Super Bowl of Analytics, Operations Research|last=INFORMS|website=INFORMS|language=en-US|access-date=2018-04-12}}</ref> The AIMMS platform was also used by the Dutch Delta team to develop and implement a new method for calculating the most efficient levels of flood protection for the Netherlands and won the Edelman prize in 2013<ref>{{Cite web|url=https://www.informs.org/ORMS-Today/Public-Articles/June-Volume-40-Number-3/Dutch-Delta-team-earns-Edelman|title=Dutch Delta team earns Edelman|last=INFORMS|website=INFORMS|language=en-US|access-date=2018-04-12}}</ref>. \n\n \n\n==See also==\n* [[Algebraic modeling language]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://www.aimms.com/ AIMMS home page] \n* [https://groups.google.com/group/aimms AIMMS user forum]\n* [http://blog.aimms.com AIMMS blog: How-to, tips & tricks for AIMMS users]\n* [http://download.aimms.com/aimms/AimmsOSI/frames.html?frmname=topic&frmfile=index.html AIMMS Open Solver Interface]\n* [http://www.lionhrtpub.com/orms/orms-8-00/swr.html AIMMS 3 Software Review]\n* [http://support.dce.felk.cvut.cz/mediawiki/images/0/0b/Bp_2010_podhradsky_michal.pdf Comparison of modeling languages for optimization]\n\n<!--\n<ref>\n{{cite web \n |    title = AIMMS 3\n |    first = Dan\n |     last = Streiffert\n |     work = OR/MS Today\n |date=August 2000\n |      url = http://www.lionhrtpub.com/orms/orms-8-00/swr.html}}\n</ref>\n-->\n\n{{Mathematical optimization software}}\n\n<!--- Categories --->\n[[Category:Computer algebra systems]]\n[[Category:Mathematical optimization software]]\n[[Category:Numerical programming languages]]\n[[Category:Mathematical modeling]]\n[[Category:Algebraic modeling languages]]\n[[Category:Supply chain analytics]]\n[[Category:Supply chain management]]\n[[Category:Supply chain software companies]]"
    },
    {
      "title": "Algebraic modeling language",
      "url": "https://en.wikipedia.org/wiki/Algebraic_modeling_language",
      "text": "'''Algebraic modeling languages''' ('''AML''') are high-level [[computer]] [[programming languages]] for describing and solving high complexity problems for large scale [[mathematical]] computation (i.e. large scale [[Optimization (mathematics)|optimization]] type problems).<ref name=\"kallrath\">{{cite book | url=https://books.google.com/books?id=wJYART7VYe8C&lr=&source=gbs_navlinks_s | title=Modeling Languages in Mathematical Optimization | publisher=Kluwer Academic Publishing | last=Kallrath | first=Joseph | year=2004 | isbn=978-1-4020-7547-6}}</ref> One particular advantage of some algebraic modeling languages like [[AIMMS]],<ref name=\"kallrath\"/> [[AMPL]],<ref>\n{{Cite journal\n  | author1 = Robert Fourer\n  | author2 = David M. Gay\n  | author3 = Brian W. Kernighan\n  | title = A Modeling Language for Mathematical Programming\n  | journal = Management Science\n  | volume = 36\n  | pages = 519‚Äì554‚Äì83\n  | year = 1990\n  | doi=10.1287/mnsc.36.5.519\n}}\n</ref> [[General Algebraic Modeling System|GAMS]],<ref name=\"kallrath\"/>\n[[MathProg]],\n[[FICO_Xpress|Mosel]],<ref name=\"kallrath\"/><ref name=\"gueret\">\n{{cite book\n  | url = https://books.google.com/books?id=2hDrPQAACAA&lr=&source=gbs_navlinks_s\n  | title = Applications of Optimization with Xpress-MP\n  | publisher = Dash Optimization Limited\n  | last1 = Gueret | first1 = Christelle\n  | last2 = Prins | first2 = Christian\n  |last3 = Sevaux | first3 = Marc\n  | year = 2002\n  | isbn=0-9543503-0-8\n}}\n</ref> \nand\n[[Optimization_Programming_Language|OPL]]\nis the similarity of their syntax to the mathematical notation of optimization problems. This allows for a very concise and readable definition of problems in the domain of optimization, which is supported by certain language elements like sets, indices, algebraic expressions, powerful sparse index and data handling variables, constraints with arbitrary names. The algebraic formulation of a model does not contain any hints how to process it.\n\nAn AML does not solve those problems directly; instead, it calls appropriate external algorithms to obtain a solution. These algorithms are called [[solver]]s and can handle certain kind of [[mathematical problem]]s like:\n\n* linear problems\n* integer problems\n* (mixed integer) quadratic problems\n* [[mixed complementarity problem]]s\n* [[mathematical programs with equilibrium constraints]]\n* constrained nonlinear systems\n* general nonlinear problems\n* non-linear programs with discontinuous derivatives\n* nonlinear integer problems\n* global optimization problems\n* stochastic optimization problems\n\n== Core elements ==\nThe core elements of an AML are:\n\n* a modeling language interpreter (the AML itself)\n* solver links\n* [[user interfaces]] (UI)\n* data exchange facilities\n\n== Design principles ==\nMost AML follow certain design principles:\n\n* a balanced mix of declarative and procedural elements\n* [[open architecture]] and interfaces to other systems\n* different layers with separation of:\n** model and data\n** model and solution methods\n** model and [[operating system]]\n** model and interface\n\n=== Data driven model generation ===\n\nMost modelling languages exploit the similarities between structured models and relational databases <ref name=Mitra1995/> by providing a database access layer, which enables the modelling system to directly access data from external data sources (e.g. these<ref>[http://ampl.com/resources/database-and-spreadsheet-table-handlers] Database and spreadsheet table handlers for AMPL</ref>table handlers for AMPL). \nWith the refinement of analytic technologies applied to business processes, optimisation models are becoming an integral part of decision support systems; optimisation models can be structured and layered to represent and support complex business processes. In such applications, the multi-dimensional data structure typical of OLAP systems can be directly mapped to the optimisation models and typical MDDB operations can be translated into aggregation and disaggregation operations on the underlying model <ref>{{cite journal\n| last1  = Koutsoukis\n| first1 = N.\n| last2  = Mitra\n| first2 = G.\n| last3  = Lucas\n| first3 = C.\n| title   = Adapting on-line analytical processing for decision modelling: the interaction of information and decision technologies\n| journal = Decision support systems\n| volume  = 26\n| issue   = 1\n| pages   = 1‚Äì30\n| url     =  http://optirisk-systems.com/publications.asp#modelling\n| accessdate = November 22, 2017\n| year    = 1999\n}}</ref>\n\n==History==\nAlgebraic modelling languages find their roots in matrix-generator and report-writer programs (MGRW), developed in the late seventies. Some of these are MAGEN, MGRW (IBM), GAMMA.3, DATAFORM and MGG/RWG. These systems simplified the communication of problem instances to the solution algorithms and the generation of a readable report of the results.\n\nAn early matrix-generator for LP was developed around 1969 at the Mathematisch Centrum (now CWI), Amsterdam.<ref>Jac. M. Anthonisse, An input system for linear programming problems, Statistica Neerlandica '''24''' (1970), 143-153.</ref>\nIts syntax was very close to the usual mathematical notation, using subscripts en sigmas. Input for the generator consisted of separate sections for the model and the data. It found users at universities and in industry. The main industrial user was the steel maker Hoogovens (now Tata Steel) where it was used for nearly 25 years.\n\nA big step towards the modern modelling languages is found in UIMP<ref>\n{{Cite journal\n  | author1 = Francis D Ellison\n  | author2 = Gautam Mitra\n  | title = UIMP: user interface for mathematical programming\n  | journal = ACM Transactions on Mathematical Software\n  | volume = 8\n  | number = 3\n  | pages = 229‚Äì255\n  | year = 1982\n  | doi=10.1145/356004.356005\n  | url=http://www.optirisk-systems.com/papers/Ellison-Mitra-1982.pdf}}\n</ref>\n, where the structure of the [[Mathematical optimization|mathematical programming]] models taken from real life is analysed for the first time, to highlight the natural grouping of variables and constraints arising from such models. This led to data-structure features, which supported structured modelling; in this paradigm, all the input and output tables, together with the decision variables, are defined in terms of these structures, in a way comparable to the use of subscripts and sets.\nThis is probably the single most notable feature common to all modern AMLs and enabled, in time, a separation between the model structure and its data, and a correspondence between the entities in a MP model and data in relational databases. So, a model could be finally instantiated and solved over different datasets, just by modifying its datasets.\n\nThe correspondence between modelling entities and [[Relational database|relational data models]],<ref name=Mitra1995>\n{{Cite journal\n  | author1 = Gautam Mitra\n  | author2 = Cormac Lucas\n  | author3 = Shirley Moody\n  | author4 = Bjarni Kristjansson\n  | title = Sets and indices in linear programming modelling and their integration with relational data models\n  | journal = Computational Optimization and Applications\n  | volume = 4\n  | number = 3\n  | pages = 262‚Äì283\n  | year = 1995\n  | url=http://www.optirisk-systems.com/publications.asp#whitepaper}}\n</ref> made then possible to seamlessly generate model instances by fetching data from corporate databases. \nThis feature accounts now for a lot of the usability of optimisation in real life applications, and is supported by most well-known modelling languages.\n\n==References==\n<references/>\n\n== See also ==\n* [[AIMMS]]\n* [[AMPL]] - a popular modeling language for large-scale linear, mixed integer and nonlinear optimization.\n* [[APMonitor]]\n* [[ASCEND]]\n* [https://github.com/JuliaOpt/JuMP.jl JuMP.jl] - a [[Julia (programming language)|Julia]]-based modeling library. \n* [http://support.sas.com/documentation/cdl/en/ormpug/67517/HTML/default/viewer.htm#ormpug_optmodel_overview.htm OPTMODEL] - part of the [[SAS (software)|SAS]] system.\n* [[Pyomo]] - a [[Python (programming language)|Python]]-based modeling library. \n* [[General Algebraic Modeling System|GAMS]]\n* [[OptimJ]] - a Java-based modeling language.\n* [[SAMPL]] - a set of extension to [[AMPL]] to support Stochastic Programming and Robust Optimisation problems, and the definition of (Integrated) Chance Constraints.\n* [[FICO_Xpress|Xpress-Mosel]] - The modelling language in [[FICO_Xpress|Xpress]]\n\n{{DEFAULTSORT:Algebraic Modeling Language}}\n[[Category:Computer algebra systems]]\n[[Category:Mathematical optimization software]]\n[[Category:Specification languages]]"
    },
    {
      "title": "AMPL",
      "url": "https://en.wikipedia.org/wiki/AMPL",
      "text": "{{Infobox programming language\n| name = AMPL\n| logo = [[File:AMPL (textbook cover).jpg]]\n| caption = \n| designers = [[Robert Fourer]]<br />David Gay<br />[[Brian Kernighan]]<br/>[[Bell Labs]]\n| paradigm = [[Multi-paradigm programming language|Multi-paradigm]]: [[Declarative programming|declarative]], [[Imperative programming|imperative]]\n| developer = AMPL Optimization, Inc.\n| released = {{Start date and age|1985}}\n| latest release version = 20131012\n| latest release date = {{Start date and age|2013|10|12}}\n| latest test version = \n| latest test date = \n| typing = \n| implementations = \n| dialects = \n| influenced by = [[AWK]], [[C (programming language)|C]]\n| influenced = [[Coopr]]\n| operating system = [[Cross-platform]]: [[Linux]], [[OS X]], some [[Unix]], [[Microsoft Windows|Windows]]\n| license = [[Proprietary software|Proprietary]] (translator),<br />[[Free and open source software|free and open-source]] (AMPL Solver Library)\n| genre = [[Algebraic modeling language]] (AML)\n| website = {{URL|www.ampl.com}}\n| file ext = .mod, .dat, .run\n}}\n\n'''A Mathematical Programming Language''' ('''AMPL''') is an [[algebraic modeling language]] to describe and solve high-complexity problems for large-scale mathematical computing (i.e., large-scale optimization and [[automated planning and scheduling|scheduling]]-type problems).<ref name=\"ampl-book\">\n{{cite book\n| title = AMPL: A Modeling Language for Mathematical Programming\n| last = Fourer\n| first = Robert\n| authorlink = Robert Fourer\n| author2 = Brian W. Kernighan\n| year = 2002\n| publisher = Duxbury Press\n| isbn = 978-0-534-38809-6| authorlink2 = Brian Kernighan\n}}\n</ref>\nIt was developed by [[Robert Fourer]], David Gay, and [[Brian Kernighan]] at [[Bell Labs|Bell Laboratories]].\nAMPL supports dozens of [[solver]]s, both [[Open-source software|open source]] and [[commercial software]], including CBC, [[CPLEX]], [[FortMP]], [[Gurobi]], [[MINOS (optimization software)|MINOS]], [[IPOPT]], [[SNOPT]], [[KNITRO]], and LGO. Problems are passed to solvers as [[nl (format)|nl]] files.\nAMPL is used by more than 100 corporate clients, and by government agencies and academic institutions.<ref>\n {{cite web\n  | title=Position Available\n  | url=http://www.ampl.com/OPENINGS/2011July.html#Product\n  | accessdate=2011-07-29}}\n</ref>\n\nOne advantage of AMPL is the similarity of its syntax to the mathematical notation of [[Optimization (mathematics)|optimization]] problems. This allows for a very concise and readable definition of problems in the domain of [[mathematical programming|optimization]]. Many modern solvers available on the [[NEOS Server]] (formerly hosted at the [[Argonne National Laboratory]], currently hosted at the [[University of Wisconsin, Madison]]<ref name=\"neos-uwm\">{{cite web|url=http://neos-guide.org/About/|title=About|publisher=|accessdate=11 August 2015}}</ref>) accept AMPL input. According to the NEOS statistics AMPL is the most popular format for representing mathematical programming problems.\n\n==Features==\nAMPL features a mix of [[declarative programming|declarative]] and [[imperative programming|imperative]] programming styles. Formulating optimization models occurs via declarative language elements such as sets, scalar and multidimensional parameters, decision variables, objectives and [[constraint (mathematics)|constraints]], which allow for concise description of most problems in the domain of mathematical optimization.\n\nProcedures and [[control flow]] statements are available in AMPL for\n* the exchange of data with external data sources such as [[spreadsheet]]s, [[database]]s, [[XML]] and text files\n* data pre- and post-processing tasks around optimization models\n* the construction of hybrid algorithms for problem types for which no direct efficient solvers are available.\n\nTo support re-use and simplify construction of large-scale optimization problems, AMPL allows separation of model and data.\n\nAMPL supports a wide range of problem types, among them:\n* [[Linear programming]]\n* [[Quadratic programming]]\n* [[Nonlinear programming]]\n* [[Linear programming#Integer unknowns|Mixed-integer programming]]\n* Mixed-integer quadratic programming with or without [[Convex function|convex]] quadratic constraints\n* Mixed-integer nonlinear programming\n* [[Second-order cone programming]]\n* [[Global optimization]]\n* [[Semidefinite programming]] problems with [[Bilinear form|bilinear]] matrix inequalities\n* [[Complementarity theory]] problems (MPECs) in discrete or continuous variables\n* [[Constraint programming]]<ref name=\"cp-support\">\n{{Cite journal\n  |authorlink = Robert Fourer\n  | authorlink2 = David M. Gay\n  | title = Extending an Algebraic Modeling Language to Support Constraint Programming\n  | journal = INFORMS Journal on Computing\n  | volume = 14\n  | issue = 4\n  | pages = 322‚Äì344\n  | year = 2002\n  | url = http://joc.journal.informs.org/content/14/4/322\n  | doi=10.1287/ijoc.14.4.322.2825| last1 = Fourer\n  | first1 = Robert\n  | last2 = Gay\n  | first2 = David M.\n  | citeseerx = 10.1.1.8.9699\n  }}\n</ref>\n\nAMPL invokes a solver in a separate process which has these advantages:\n* User can interrupt the solution process at any time\n* Solver errors do not affect the interpreter\n* 32-bit version of AMPL can be used with a 64-bit solver and vice versa\nInteraction with the solver is done through a well-defined [[nl (format)|nl interface]].\n\n==Availability==\n[[File:Neos-stats-2011-01.png|right|thumb|352px|[[Argonne National Laboratory#User facilities|NEOS]] input statistics for January 2011.]]\nAMPL is available for many popular 32- and 64-bit [[operating system]]s including [[Linux]], Mac [[OS X]], some [[Unix]], and [[Microsoft Windows|Windows]].<ref>[http://ampl.com/products/ampl/ AMPL page at AMPL Optimization Inc.]</ref>\nThe translator is proprietary software maintained by AMPL Optimization LLC. However, several online services exist, providing free modeling and solving facilities using AMPL.<ref name=\"neos\">{{cite web|url=http://www.neos-server.org/neos/|title=NEOS Server for Optimization|publisher=|accessdate=11 August 2015}}</ref><ref>{{cite web|url=http://www.ampl.com/TRYAMPL/|title=Try AMPL!|publisher=|accessdate=11 August 2015}}</ref>  A free student version with limited functionality and a free full-featured version for academic courses are also available.<ref>{{cite web|url=http://www.ampl.com/DOWNLOADS/index.html|title=AMPL Downloads|publisher=|accessdate=11 August 2015|archive-url=https://web.archive.org/web/20150526013237/http://www.ampl.com/DOWNLOADS/index.html|archive-date=26 May 2015|dead-url=yes|df=dmy-all}}</ref>\n\nAMPL can be used from within [[Microsoft Excel]] via the [[SolverStudio]] Excel add-in.\n\nThe AMPL Solver Library (ASL), which allows reading nl files and provides the automatic differentiation, is open-source. It is used in many solvers to implement AMPL connection.\n\n==Status history==\nThis table present significant steps in AMPL history.\n{| class=\"wikitable\"\n|-\n! Year\n! Highlights\n|-\n| 1985\n| AMPL was designed and implemented<ref name=\"ampl-book\"/>\n|-\n| 1990\n| Paper describing the AMPL modeling language was published in [[Management Science: A Journal of the Institute for Operations Research and the Management Sciences|Management Science]]<ref>\n{{Cite journal\n  | authorlink1 = Robert Fourer\n  | authorlink2 = David M. Gay\n  | authorlink3 = Brian W. Kernighan\n  | title = A Modeling Language for Mathematical Programming\n  | journal = Management Science\n  | volume = 36\n  | issue = 5\n  | pages = 519‚Äì554‚Äì83\n  | year = 1990\n  | url = http://www.ampl.com/REFS/amplmod.pdf\n  | doi=10.1287/mnsc.36.5.519| last1 = Fourer\n  | first1 = Robert\n  | last2 = Gay\n  | first2 = David M.\n  | last3 = Kernighan\n  | first3 = Brian W.\n  }}\n</ref>\n|-\n| 1991\n| AMPL supports [[nonlinear programming]] and [[automatic differentiation]]\n|-\n| 1993\n| [[Robert Fourer]], David Gay and [[Brian Kernighan]] were awarded ORSA/CSTS Prize<ref>{{cite web|url=http://computing.society.informs.org/pdf/GreenbergHistory.pdf|title=ICS - INFORMS|author=INFORMS|publisher=|accessdate=11 August 2015}}</ref> by the [[Operations Research Society of America]], for writings on the design of mathematical programming systems and the AMPL modeling language\n|-\n| 1995\n| Extensions for representing [[piecewise linear function|piecewise-linear]] and network structures\n|-\n| 1995\n| Scripting constructs\n|-\n| 1997\n| Enhanced support for nonlinear solvers\n|-\n| 1998\n| AMPL supports [[complementarity theory]] problems\n|-\n| 2000\n| Relational database and spreadsheet access\n|-\n| 2002\n| Support for constraint programming<ref name=\"cp-support\" />\n|-\n| 2003\n| AMPL Optimization LLC was founded by the inventors of AMPL, Robert Fourer, David Gay, and Brian Kernighan. The new company took over the development and support of the AMPL modeling language from [[Lucent|Lucent Technologies, Inc]].\n|-\n| 2005\n| AMPL Modeling Language Google group opened<ref>{{Cite web | url=https://groups.google.com/group/ampl | title=Google Groups}}</ref>\n|-\n| 2008\n| Kestrel: An AMPL Interface to the NEOS Server introduced\n|-\n| 2012\n| [[Robert Fourer]], David Gay, and [[Brian Kernighan]] were awarded the 2012 INFORMS Impact Prize as the originators of one of the most important algebraic modeling languages.<ref>{{cite web|url=http://www.informs.org/Blogs/E-News-Blog/INFORMS-Impact-Prize|title=INFORMS Impact Prize|author=INFORMS|publisher=|accessdate=11 August 2015|archive-url=https://web.archive.org/web/20131022091250/https://www.informs.org/Blogs/E-News-Blog/INFORMS-Impact-Prize|archive-date=22 October 2013|dead-url=yes|df=dmy-all}}</ref>\n|-\n| 2012\n| AMPL book becomes freely available online\n|-\n| 2013\n| A new cross-platform [[integrated development environment]] (IDE) for AMPL becomes available<ref>{{cite web|url=https://groups.google.com/forum/#!topic/ampl/y1FJcYZz-_Q|title=Google Groups|publisher=|accessdate=11 August 2015}}</ref>\n|}\n\n=={{anchor|example}}A sample model==\nA transportation problem from [[George Dantzig]] is used to provide a sample AMPL model. This problem finds the least cost shipping schedule that meets requirements at markets and supplies at factories.<ref>\nDantzig, G B, chapter 3.3 in ''Linear Programming and Extensions'', Princeton University Press, Princeton, New Jersey, 1963.\n</ref>\n<!-- See http://pygments.org/docs/lexers/#lexers-for-the-ampl-language -->\n<source lang=\"ampl\">\n set Plants;\n set Markets;\n\n # Capacity of plant p in cases\n param Capacity{p in Plants};\n\n # Demand at market m in cases\n param Demand{m in Markets};\n\n # Distance in thousands of miles\n param Distance{Plants, Markets};\n\n # Freight in dollars per case per thousand miles\n param Freight;\n\n # Transport cost in thousands of dollars per case\n param TransportCost{p in Plants, m in Markets} :=\n     Freight * Distance[p, m] / 1000;\n\n # Shipment quantities in cases\n var shipment{Plants, Markets} >= 0;\n\n # Total transportation costs in thousands of dollars\n minimize cost:\n     sum{p in Plants, m in Markets} TransportCost[p, m] * shipment[p, m];\n\n # Observe supply limit at plant p\n s.t. supply{p in Plants}: sum{m in Markets} shipment[p, m] <= Capacity[p];\n\n # Satisfy demand at market m\n s.t. demand{m in Markets}: sum{p in Plants} shipment[p, m] >= Demand[m];\n\n data;\n\n set Plants := seattle san-diego;\n set Markets := new-york chicago topeka;\n\n param Capacity :=\n     seattle   350\n     san-diego 600;\n\n param Demand :=\n     new-york 325\n     chicago  300\n     topeka   275;\n\n param Distance : new-york chicago topeka :=\n     seattle        2.5      1.7     1.8\n     san-diego      2.5      1.8     1.4;\n\n param Freight := 90;\n</source>\n\n==Solvers==\nHere is a partial list of [[solver]]s supported by AMPL:<ref>{{cite web|url=http://www.ampl.com/solvers.html|title=Solvers - AMPL|publisher=|accessdate=21 January 2018}}</ref>\n\n{| class=\"wikitable sortable\"\n|-\n! width=20%| Solver\n! width=80%| Supported problem types\n|-\n! [[APOPT]]\n| mixed integer [[nonlinear programming]]\n|-\n! [[Artelys Knitro]]\n| linear, quadratic and nonlinear programming\n|-\n! Bonmin\n| mixed integer [[nonlinear programming]]\n|-\n! BPMPD\n| [[Linear programming|linear]] and [[quadratic programming]]\n|-\n! [[COIN-OR]] CBC\n| [[Linear programming#Integer unknowns|mixed integer programming]]\n|-\n! [[COIN-OR#CLP|COIN-OR CLP]]\n| linear programming\n|-\n! CONOPT\n| nonlinear programming\n|-\n! Couenne<ref>{{cite web|url=https://projects.coin-or.org/Couenne |title=Couenne |accessdate=2013-10-27 |deadurl=yes |archiveurl=https://web.archive.org/web/20131029190415/https://projects.coin-or.org/Couenne |archivedate=2013-10-29 |df= }}</ref>\n| mixed integer nonlinear programming (MINLP)\n|-\n! [[CPLEX]]\n| linear, quadratic, [[Second-order cone programming|second-order cone]] and mixed integer programming\n|-\n! CPLEX CP Optimizer<ref>{{cite web|url=https://github.com/ampl/mp/tree/master/solvers/ilogcp|title=mp/solvers/ilogcp at master ¬∑ ampl/mp ¬∑ GitHub|work=GitHub|accessdate=11 August 2015}}</ref>\n| [[constraint programming]]\n|-\n! FILTER\n| nonlinear programming\n|-\n! [[FortMP]]\n| linear, quadratic and mixed integer programming\n|-\n! [[Gecode]]<ref>{{cite web|url=https://github.com/ampl/mp/tree/master/solvers/gecode|title=mp/solvers/gecode at master ¬∑ ampl/mp ¬∑ GitHub|work=GitHub|accessdate=11 August 2015}}</ref>\n| constraint programming\n|-\n! [[Gurobi]]\n| linear, quadratic, second-order cone and mixed integer programming\n|-\n! [[IPOPT]]\n| nonlinear programming\n|-\n! [[JaCoP (solver)|JaCoP]]<ref>{{cite web|url=https://github.com/ampl/mp/tree/master/solvers/jacop|title=mp/solvers/jacop at master ¬∑ ampl/mp ¬∑ GitHub|work=GitHub|accessdate=11 August 2015}}</ref>\n| constraint programming\n|-\n! LGO<ref>{{cite web|url=http://ampl.com/products/solvers/solvers-we-sell/lgo/|title=LGO - AMPL|publisher=|accessdate=11 August 2015}}</ref>\n| global and local nonlinear optimization\n|-\n! LocalSolver<ref>{{cite web|url=https://github.com/ampl/mp/tree/master/solvers/localsolver|title=mp/solvers/localsolver at master ¬∑ ampl/mp ¬∑ GitHub|work=GitHub|accessdate=11 August 2015}}</ref>\n| mixed integer nonlinear programming\n|-\n! lp_solve<ref>{{cite web|url=http://www.ampl.com/SOLVERS/GUIDE.lpsolve.html|title=Using lpsolve from AMPL|publisher=|accessdate=11 August 2015}}</ref>\n| linear and mixed integer programming\n|-\n! [[MINOS (optimization software)|MINOS]]\n| linear and nonlinear programming\n|-\n! [[MINTO]]\n| mixed integer programming\n|-\n! [[MOSEK]]\n| linear, mixed integer linear, quadratic, mixed integer quadratic, [[Quadratically constrained quadratic program|quadratically constrained]], conic and convex nonlinear programming\n|-\n! [[SCIP (optimization software)|SCIP]]\n| mixed integer programming\n|-\n! [[SNOPT]]\n| nonlinear programming\n|-\n! Sulum<ref>{{cite web|url=https://github.com/ampl/mp/tree/master/solvers/sulum|title=mp/solvers/sulum at master ¬∑ ampl/mp ¬∑ GitHub|work=GitHub|accessdate=11 August 2015}}</ref>\n| linear and mixed integer programming\n|-\n! [[WORHP]]\n| nonlinear programming\n|-\n! XA\n| linear and mixed integer programming\n|-\n! [[FICO Xpress|Xpress]]\n| linear and convex [[Quadratically constrained quadratic program|quadratic optimization]] and their mixed integer counterparts\n|}\n\n==See also==\n* [[sol (format)]]\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n* {{Official website|www.ampl.com}}\n* [https://web.archive.org/web/20061127232734/http://iems.northwestern.edu/~4er/ Prof. Fourer's home page] at [[Northwestern University]]\n\n{{Mathematical optimization software}}\n\n{{Use dmy dates|date=June 2017}}\n\n{{DEFAULTSORT:Ampl}}\n[[Category:1990 software]]\n[[Category:Computer algebra systems]]\n[[Category:Mathematical modeling]]\n[[Category:Mathematical optimization software]]\n[[Category:Numerical programming languages]]\n[[Category:Scripting languages]]\n[[Category:Text-oriented programming languages]]"
    },
    {
      "title": "Analytica (software)",
      "url": "https://en.wikipedia.org/wiki/Analytica_%28software%29",
      "text": "{{Infobox software\n| name                 = Analytica\n| developer            = Lumina Decision Systems\n| released             = {{Start date and age|1992|01|16}}\n| programming language = [[C++ (programming language)|C++]]\n| operating system     = [[Microsoft Windows|Windows]]\n| platform             = [[IA-32]], [[x64]]\n| language             = English\n| genre                = [[Decision-making software]]\n| license              = [[Commercial software|Commercial]] [[proprietary software]]\n| website              = {{URL|www.analytica.com}}\n}}\n\n{{Advert|date=June 2019}}\n\n'''Analytica''' is a visual software package developed by Lumina Decision Systems for creating, analyzing and communicating quantitative decision models.<ref>Granger Morgan and Max Henrion (1998), [http://www.lumina.com/software/ch10.9.PDF Analytica:A Software Tool for Uncertainty Analysis and Model Communication] {{webarchive |url=https://web.archive.org/web/20070630125258/http://www.lumina.com/software/ch10.9.PDF |date=June 30, 2007 }}, Chapter 10 of ''Uncertainty: A Guide to Dealing with Uncertainty in Quantitative Risk and Policy Analysis'', second edition, Cambridge University Press, New York.</ref>  As a modeling environment, it is interesting in the way it combines hierarchical [[influence diagram]]s for visual creation and view of models, intelligent arrays for working with multidimensional data, [[Monte Carlo simulation]] for analyzing risk and uncertainty, and [[optimization]], including linear and nonlinear programming. Its design, especially its influence diagrams and treatment of uncertainty, is based on ideas from the field of [[decision analysis]]. As a computer language, it is notable in combining a declarative (non-procedural) structure for referential transparency, array abstraction, and automatic dependency maintenance for efficient sequencing of computation.\n\n== Hierarchical influence diagrams ==\nAnalytica models are organized as [[influence diagram]]s.  Variables (and other objects) appear as nodes of various shapes on a diagram, connected by arrows that provide a visual representation of dependencies.  Analytica influence diagrams may be hierarchical, in which a single ''module'' node on a diagram represents an entire submodel.\n\nHierarchical influence diagrams in Analytica serve as a key organizational tool.  Because the visual layout of an influence diagram matches these natural human abilities both spatially and in the level of abstraction, people are able to take in far more information about a model's structure and organization at a glance than is possible with less visual paradigms, such as [[spreadsheet]]s and [[mathematical expression]]s.  Managing the structure and organization of a large model can be a significant part of the modeling process, but is substantially aided by the visualization of influence diagrams.\n\nInfluence diagrams also serve as a tool for communication.  Once a quantitative model has been created and its final results computed, it is often the case that an understanding of how the results are obtained, and how various assumptions impact the results, is far more important than the specific numbers computed.  The ability of a target audience to understand these aspects is critical to the modeling enterprise.  The visual representation of an influence diagram quickly communicates an understanding at a level of abstraction that is normally more appropriate than detailed representations such as mathematical expressions or cell formulae.  When more detail is desired, users can drill down to increasing levels of detail, speeded by the visual depiction of the model's structure.\n\nThe existence of an easily understandable and transparent model supports communication and debate within an organization, and this effect is one of the primary benefits of investing in quantitative model building.  When all interested parties are able to understand a common model structure, debates and discussions will often focus more directly on specific assumptions, can cut down on \"cross-talk\", and therefore lead to more productive interactions within the organization.  The influence diagram serves as a graphical representation that can help to make models accessible to people at different levels.\n\n== Intelligent multidimensional arrays ==\n\nAnalytica uses index objects to track the dimensions of multidimensional arrays. An index object has a name and a list of elements.  When two multidimensional values are combined, for example in an expression such as\n\n:<code>Profit = Revenue ‚àí Expenses</code>\n\nwhere ''Revenue'' and ''Expenses'' are each multidimensional, Analytica repeats the profit calculation over each dimension, but recognizes when same dimension occurs in both values and treats it as the same dimension during the calculation, in a process called ''intelligent array abstraction''.  Unlike most programming languages, there is no inherent ordering to the dimensions in a multidimensional array. This avoids duplicated formulas and explicit FOR loops, both common sources of modeling errors. The simplified expressions made possible by intelligent array abstraction allow the model to be more accessible, interpretable, and transparent.\n\nAnother consequence of intelligent array abstraction is that new dimensions can be introduced or removed from an existing model, without requiring changes to the model structure or changes to variable definitions.  For example, while creating a model, the model builder might assume a particular variable, for example ''[[discounted cash flow|discount_rate]]'', contains a single number.  Later, after constructing a model, a user might replace the single number with a table of numbers, perhaps ''discount_rate'' broken down by ''Country'' and by ''Economic_scenario''.  These new divisions may reflect the fact that the effective discount rate is not the same for international divisions of a company, and that different rates are applicable to different hypothetical scenarios.  Analytica automatically propagates these new dimensions to any results that depend upon ''discount_rate'', so for example, the result for ''[[Net present value]]'' will become multidimensional and contain these new dimensions.  In essence, Analytica repeats the same calculation using the discount rate for each possible combination of ''Country'' and ''Economic_scenario''.\n\nThis flexibility is important when exploring computation tradeoffs between the level of detail, computation time, available data, and overall size or dimensionality of parametric spaces. Such adjustments are common after models have been fully constructed as a way of exploring ''[[What-if analysis|what-if]]'' scenarios and overall relationships between variables.\n\n== Uncertainty analysis ==\n\nIncorporating uncertainty into model outputs helps to provide more realistic and informative projections. Uncertain quantities in Analytica can be specified using a [[probability distribution|distribution function]]. When evaluated, distributions are sampled using either [[Latin hypercube sampling|Latin hypercube]] or [[Monte Carlo simulation|Monte Carlo]] sampling, and the samples are propagated through the computations to the results.  The sampled result distribution and summary statistics can then be viewed directly ([[mean]], [[quantile|fractile bands]], [[probability density function]] (PDF), [[cumulative distribution function]] (CDF)), Analytica supports collaborative decision analysis and [[probability management]] through the use of the SIPMath(tm) standard.<ref>The SIPmath<sup>TM</sup> Standard\nhttp://probabilitymanagement.org/standards.html\n</ref><ref>Paul D. Kaplan and Sam Savage (2011), [http://probabilitymanagement.org/library/IWM11MarApr_MonteCarlo.pdf Monte Carlo, A Lightbulb for Illuminating Uncertainty] {{Webarchive|url=https://web.archive.org/web/20170307150357/http://probabilitymanagement.org/library/IWM11MarApr_MonteCarlo.pdf |date=2017-03-07 }}, in Investments & Wealth Monitor</ref>\n\n== Systems dynamics modeling ==\n\n[[System dynamics]] is an approach to simulating the behaviour of complex systems over time. It deals with feedback loops and time delays on the behaviour of the entire system. The Dynamic() function in Analytica allows definition of variables with cyclic dependencies, such as feedback loops. It expands the [[influence diagram]] notation, which does not normally allow cycles. At least one link in each cycle includes a time lag, depicted as a gray influence arrow to distinguish it from standard black arrows without time lags.\n\n== As a programming language ==\n\nAnalytica includes a general language of operators and functions for expressing mathematical relationships among variables. Users can define functions and libraries to extend the language.\n\nAnalytica has several features as a [[programming language]] designed to make it easy to use for quantitative modeling: It is a [[visual programming languages|visual programming language]], where users view programs (or \"models\") as [[influence diagrams]], which they create and edit visually by adding and linking nodes. It is a [[declarative programming language|declarative language]], meaning that a model declares a definition for each variable without specifying an execution sequence as required by conventional [[imperative programming language|imperative languages]]. Analytica determines a correct and efficient execution sequence using the dependency graph. It is a [[referential transparency (computer science)|referentially transparent]] [[functional programming language|functional language]], in that execution of functions and variables have no side effects i.e. changing other variables. Analytica is an [[array programming]] language, where operations and functions generalize to work on multidimensional arrays.\n\n== Applications of Analytica ==\n\nAnalytica has been used for [[policy analysis]], [[Business Analysis|business modeling]], and [[Risk analysis (business)|risk analysi]]s.<ref>Jun Long, Baruch Fischhoff (2000), [http://www.blackwell-synergy.com/doi/abs/10.1111/0272-4332.203033 ''Setting Risk Priorities: A Formal Model Risk Analysis''], Risk Analysis 20(3):339‚Äì352.</ref>  Areas in which Analytica has been applied include energy,<ref>Stadler M., Marnay C., Azevedo I.L., Komiyama R., Lai J. (2009), [http://eetd.lbl.gov/ea/emp/reports/lbnl-1884e.pdf '' The Open Source Stochastic Building Simulation Tool SLBM and Its Capabilities to Capture Uncertainty of Policymaking in the U.S. Building Sector''] {{webarchive |url=https://web.archive.org/web/20110927151032/http://eetd.lbl.gov/ea/emp/reports/lbnl-1884e.pdf |date=September 27, 2011 }}</ref><ref>Ye Li and H. Keith Florig (Sept. 2006), [https://web.archive.org/web/20110928162008/https://wpweb2.tepper.cmu.edu/ceic/pdfs/CEIC_06_10.pdf ''Modeling the Operation and Maintenance Costs of a Large Scale Tidal Current Turbine Farm''], [http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=4098949 Oceans (2006)]:1-6\n</ref><ref>L.F.Miller, Brian Thomas, J.McConn, J. Hou, J.Preston, T.Anderson, and M.Humberstone (2007), [http://hou.jia.googlepages.com/2007_ANS_Summer_Abstract.doc ''Uncertainty Analysis Methods for Equilibrium Fuel Cycles''], ANS Summer Abstract.</ref><ref>Gregory A. Norris and Peter Yost (Fall 2001), [http://www.mitpressjournals.org/doi/abs/10.1162/10881980160084015 ''Journal of Industrial Ecology''] 5(4):15‚Äì28, MIT Press Journals.</ref><ref>Jouni T Tuomisto and Marko Tainio (2005),\n[http://www.biomedcentral.com/1471-2458/5/123 ''An economic way of reducing health, environmental, and other pressures of urban traffic: a decision analysis on trip aggregation''], BMC Public Health 5:123. {{doi|10.1186/1471-2458-5-123}}</ref><ref>Yurika Nishioka, Jonathan I. Levy, Gregory A. Norris, Andrew Wilson, Patrick Hofstetter, John D. Spengler (Oct 2002),\n[http://www.blackwell-synergy.com/doi/abs/10.1111/1539-6924.00266 ''Integrating Risk Assessment and Life Cycle Assessment: A Case Study of Insulation''], Risk Analysis 22(5):1003‚Äì1017.</ref> [[Health care|health]] and pharmaceuticals,<ref>Igor Linkov, Richard Wilson and George M., Gray (1998), [http://toxsci.oxfordjournals.org/cgi/content/abstract/43/1/1 ''Anticarcinogenic Responses in Rodent Cancer Bioassays Are Not Explained by Random Effects''], [[Toxicological Sciences]] 43(1), Oxford University Press.</ref><ref>M. Loane and R. Wootton (Oct 2001), [http://www.ingentaconnect.com/content/rsm/jtt/2001/00000007/A00105s1/art00009 ''A simulation model for analysing patient activity in dermatology''], Journal of Telemedicine and Telecare 7(1):23‚Äì25(3), Royal Society of Medicine Press.</ref><ref>Davis Bu, Eric Pan, Janice Walker, Julia Adler-Milstein, David Kendrick, Julie M. Hook, Caitlin M. Cusack, David W. Bates, and Blackford Middleton (2007), [http://care.diabetesjournals.org/cgi/content/abstract/30/5/1137 ''Benefits of Information Technology‚ÄìEnabled Diabetes Management''], Diabetes Care 30:1137‚Äì1142, American Diabetes Association.</ref><ref>Julia Adler-Milstein, Davis Bu, Eric Pan, Janice Walker, David Kendrick, Julie M. Hook, David W. Bates, Blackford Middleton. [http://www.liebertonline.com/doi/abs/10.1089/dis.2007.103640?cookieSet=1&journalCode=dis ''The Cost of Information Technology-Enabled Diabetes Management''], Disease Management. June 1, 2007, 10(3): 115‚Äì128. {{doi|10.1089/dis.2007.103640}}.\n</ref><ref>E. Ekaette, R.C. Lee, K-L Kelly, P. Dunscombe (Aug 2006),\n[http://www.palgrave-journals.com/jors/journal/v58/n2/full/2602269a.html ''A Monte Carlo simulation approach to the characterization of uncertainties in cancer staging and radiation treatment decisions''], Journal of the Operational Research Society 58:177‚Äì185.\n</ref><ref>Lyon, Joseph L.; Alder, Stephen C.; Stone, Mary Bishop; Scholl, Alan; Reading, James C.; Holubkov, Richard; Sheng, Xiaoming; White, George L. Jr; Hegmann, Kurt T.; Anspaugh, Lynn; Hoffman, F Owen; Simon, Steven L.; Thomas, Brian; Carroll, Raymond; Meikle, A Wayne (Nov 2006),[http://www.epidem.com/pt/re/epidemiology/abstract.00001648-200611000-00004.htm ''Thyroid Disease Associated With Exposure to the Nevada Nuclear Weapons Test Site Radiation: A Reevaluation Based on Corrected Dosimetry and Examination Data''], Epidemiology 17(6):604‚Äì614.\n</ref><ref>Negar Elmieh, [[Hadi Dowlatabadi]], Liz Casman (Jan 2006), [http://www.cher.ubc.ca/westnile/pdfs/elmieh_jan06.pdf ''A model for Probabilistic Assessment of Malathion Spray Exposures (PAMSE) in British Columbia''] {{webarchive |url=https://web.archive.org/web/20110929115256/http://www.cher.ubc.ca/westnile/pdfs/elmieh_jan06.pdf |date=September 29, 2011 }}, CMU EEP.\n</ref><ref>Detlofvon Winterfeldt, Thomas Eppel, John Adams, Raymond Neutra, and Vincent Del Pizzo (2004),\n[http://www.blackwell-synergy.com/doi/abs/10.1111/j.0272-4332.2004.00544.x ''Managing Potential Health Risks from Electric Powerlines: A Decision Analysis Caught in Controversy''], Risk Analysis 24(6):1487‚Äì1502.\n</ref><ref>Rebecca Montville, Yuhuan Chen and Donald W. Schaffner (March 2002), [https://dx.doi.org/10.1016/S0168-1605(01)00666-3 ''Risk assessment of hand washing efficacy using literature and experimental data''], International Journal of Food Microbiology 73(2‚Äì3):305‚Äì313.\n</ref><ref>DC Kendrick, D Bu, E Pan, B Middleton (2007), ''Crossing the Evidence Chasm: Building Evidence Bridges from Process Changes to Clinical Outcomes'', Journal of the American Medical Informatics Association, Elsevier.\n</ref><ref>Louis Anthony (Tony) Cox, Jr. (May 2005), \n[https://dx.doi.org/10.1016/j.envint.2004.10.012 ''Potential human health benefits of antibiotics used in food animals: a case study of virginiamycin''], Environment International 31(4):549‚Äì563.\n</ref><ref>Jan Walker, Eric Pan, Douglas Johnston, Julia Adler-Milstein, David W. Bates, and Blackford Middleton (19 Jan 2005), [http://content.healthaffairs.org/cgi/reprint/hlthaff.w5.10v1.pdf ''The Value Of Health Care Information Exchange And Interoperability''], Health Affairs.\n</ref><ref>Doug Johnston, Eric Pan, Blackford Middleton, [http://www.citl.org/research/articles.htm ''Finding the Value in Healthcare Information Technologies''] {{webarchive |url=https://web.archive.org/web/20080706120811/http://www.citl.org/research/articles.htm |date=July 6, 2008 }}'', Center for Information Technology Leadership (C!TL) whitepaper.''\n</ref><ref>Chrisman, L., Langley, P., Bay, S., and Pohorille, A. (Jan 2003), [http://chrisman.org/Lonnie/psb2003/index.htm \"Incorporating biological knowledge into evaluation of causal regulatory hypotheses\"], Pacific Symposium on Biocomputing (PSB).\n</ref><ref>Jan Walker, Eric Pan, Douglas Johnson, Julia Adler-Milstein, David W. Bates and Blackford Middleton (2005), [http://content.healthaffairs.org/content/early/2005/01/19/hlthaff.w5.10.short The Value of Health Care Information and Exchange And Interoperability\"] Health Affairs.\n</ref><ref>Steve Lohr, [https://www.nytimes.com/2005/01/19/technology/19health.html?ex=1107134972&ei=1&en=4657166ae8ef8c9f Road Map to a Digital System of Health Records], New York Times, January 29, 2005\n</ref> \nenvironmental risk and emissions policy analysis,<ref>C. Bloyd, J. Camp, G. Conzelmann, J. Formento, J. Molburg, J. Shannon, M. Henrion, R. Sonnenblick, K. Soo Hoo, J. Kalagnanam, S. Siegel, R. Sinha, M. Small, T. Sullivan, R. Marnicio, P. Ryan, R. Turner, D. Austin, D. Burtraw, D. Farrell, T. Green, A. Krupnick, and E. Mansur (Dec 1996), [http://lumina.com/taf/taflist/dltaf/TAF.pdf ''Tracking and Analysis Framework (TAF) Model Documentation and User‚Äôs Guide: An Interaction Model for Integrated Assessment of Title IV of the Clean Air Act Amendments''] {{webarchive |url=https://web.archive.org/web/20090105162907/http://lumina.com/taf/taflist/dltaf/TAF.pdf |date=January 5, 2009 }}, Decision and Information Sciences Division, Argonne National Laboratory.\n</ref><ref>Max Henrion, Richard Sonnenblick, Cary Bloyd (Jan 1997), [https://www.lumina.com/taf/taflist/dltaf/Innovations.PDF ''Innovations in Integrated Assessment: The Tracking and Analysis Framework (TAF)''] {{webarchive |url=https://web.archive.org/web/20090105174404/https://www.lumina.com/taf/taflist/dltaf/Innovations.PDF |date=January 5, 2009 }}, Air and Waste Management Conference on Acid Rain and Electric Utilities, Scottsdale, AZ.\n</ref><ref>Richard Sonnenblick and Max Henrion (Jan 1997), [https://www.lumina.com/taf/taflist/dltaf/Uncertainty.PDF ''Uncertainty in the Tracking and Analysis Framework Integrated Assessment: The Value of Knowing How Little You Know''] {{webarchive |url=https://web.archive.org/web/20090105175211/https://www.lumina.com/taf/taflist/dltaf/Uncertainty.PDF |date=January 5, 2009 }}, Air and Waste Management Conference\non Acid Rain and Electric Utilities, Scottsdale, Arizona.\n</ref><ref>R. Sinha, M. J. Small, P. F. Ryan, T. J. Sullivan and B. J. Cosby (July 1998), \n[http://www.springerlink.com/content/tq8127805181x57k/ ''Reduced-Form Modelling of Surface Water and Soil Chemistry for the Tracking and Analysis Framework''], Water, Air, &amp; Soil Pollution 105 (3‚Äì4).\n</ref><ref>Dallas Burtraw and Erin Mansur (Mar 1999), \n[http://www.rff.org/documents/RFF-DP-99-25.pdf ''The Effects of Trading and Banking in the SO2 Allowance Market''], Discussion paper 99‚Äì25, [http://rff.org Resources for the Future].\n</ref><ref>Galen mcKinley, Miriam Zuk, Morten H√∂jer, Montserrat Avalos, Isabel Gonz√°lez, Rodolfo Iniestra, Israel Laguna, Miguel A. Mart√≠nez, Patricia Osnaya, Luz M. Reynales, Raydel Vald√©s, and Julia Mart√≠nez (2005), [http://www.aos.wisc.edu/~galen/Downloads/McKinley_EST05.pdf ''Quantification of Local and Global Benefits from Air Pollution Control in Mexico City''] {{webarchive |url=https://web.archive.org/web/20110929000949/http://www.aos.wisc.edu/~galen/Downloads/McKinley_EST05.pdf |date=September 29, 2011 }}, Environ. Sci. Technol. 39:1954‚Äì1961.\n</ref><ref>Luis A. CIFUENTES, Enzo SAUMA, Hector JORQUERA and Felipe SOTO (2000),\n[http://www.airimpacts.org/documents/local/M00007481.pdf ''Preliminary Estimation of the Potential Ancillary Benefits for Chile''] {{Webarchive|url=https://web.archive.org/web/20120423160550/http://www.airimpacts.org/documents/local/M00007481.pdf |date=2012-04-23 }}, Ancillary Benefits and Costs of Greenhouse Gas Mitigation.</ref><ref>Marko Tainio, Jouni T Tuomisto, Otto H√§nninen, Juhani Ruuskanen, Matti J Jantunen, and Juha Pekkanen (2007),\n[http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2000460 ''Parameter and model uncertainty in a life-table model for fine particles (PM2.5): a statistical modeling study''], Environ Health 6(24).\n</ref><ref>L. Basson and J.G. Petrie (Feb 2007),\n[https://dx.doi.org/10.1016/j.envsoft.2005.07.026 ''An integrated approach for the consideration of uncertainty in decision making supported by Life Cycle Assessment''], Environmental Modeling &amp; Software 22(2):167‚Äì176, Environmental Decision Support Systems, Elsevier.\n</ref> [[wildlife management]],<ref>Matthew F. Bingham, Zhimin Li, Kristy E. Mathews, Colleen M. Spagnardi, Jennifer S. Whaley, Sara G. Veale and Jason C. Kinnell (2011), [http://www.informaworld.com/smpp/content~db=all~content=a936752464~frm=titlelink ''An Application of Behavioral Modeling to Characterize Urban Angling Decisions and Values''], North American Journal of Fisheries Management 31:257‚Äì268.\n</ref><ref>Peter B. Woodbury, James E. Smith, David A. Weinstein and John A. Laurence (Aug 1998), [https://dx.doi.org/10.1016/S0378-1127(97)00323-X ''Assessing potential climate change effects on loblolly pine growth: A probabilistic regional modeling approach''], Forest Ecology and Management 107 (1‚Äì3), 99‚Äì116.\n</ref><ref>P.R. Richard, M. Power, M. Hammill, and W. Doidge(2003). [http://www.dfo-mpo.gc.ca/csas/Csas/DocREC/2003/RES2003_086_E.pdf ''Eastern Hudson Bay Beluga Precautionary Approach Case Study: Risk analysis models for co-management''] {{webarchive |url=https://web.archive.org/web/20120403034050/http://www.dfo-mpo.gc.ca/csas/Csas/DocREC/2003/RES2003_086_E.pdf |date=April 3, 2012 }}, Canadian Science Advisory Secretariat Research Document.\n</ref><ref>P.R. Richard (2003), [http://www.dfo-mpo.gc.ca/csas/Csas/DocREC/2003/RES2003_087_E.pdf ''Incorporating Uncertainty in Population Assessments''] {{webarchive |url=https://web.archive.org/web/20120403034057/http://www.dfo-mpo.gc.ca/csas/Csas/DocREC/2003/RES2003_087_E.pdf |date=April 3, 2012 }}, Canadian Science Advisory Secretariat Research Document.\n</ref>\necology,<ref>O'Ryan R., Diaz M. (2008), [http://www.informaworld.com/smpp/section?content=a793817831&fulltext=713240928 ''The Use of Probabilistic Analysis to Improve Decision-Making in Environmental Regulation in a Developing Context: The Case of Arsenic Regulation in Chile''], Human and Ecological Risk Assessment: An International Journal, Vol 14, Issue 3, pg: 623‚Äì640.\n</ref><ref>Andrew Gronewold and Mark Borsuk, \"A probabilistic modeling tool for assessing water quality standard compliance\", submitted to EMS Oct 2008.\n</ref><ref>Mark E. Borsuk, Peter Reichert, Armin Peter, Eva Schager and Patricia Burkhardt-Holm (feb 2006), \n[https://dx.doi.org/10.1016/j.ecolmodel.2005.07.006 ''Assessing the decline of brown trout (Salmo trutta) in Swiss rivers using a Bayesian probability network''], Ecological Modelling 192 (1‚Äì2):224‚Äì244.\n</ref><ref>Mark E. Borsuk, Craig A. Stow1 and Kenneth H. Reckhow (Apr 2004), \n[https://dx.doi.org/10.1016/j.ecolmodel.2003.08.020 ''A Bayesian network of eutrophication models for synthesis, prediction, and uncertainty analysis''], Ecological Modelling 173 (2‚Äì3):219‚Äì239.\n</ref><ref>Mark E. Borsuk, Sean P. Powers, and Charles H. Peterson (2002), \n[http://article.pubs.nrc-cnrc.gc.ca/ppv/RPViewDoc?_handler_=HandleInitialGet&journal=cjfas&volume=59&calyLang=eng&articleFile=f02-093.pdf ''A survival model of the effects of bottom-water hypoxia on the population density of an estuarine clam (Macoma balthica)'']  {{webarchive|url=https://www.collectionscanada.gc.ca/webarchives/20060201102418/http://article.pubs.nrc-cnrc.gc.ca/ppv/rpviewdoc?_handler_=handleinitialget&journal=cjfas&volume=59&calylang=eng&articlefile=f02-093.pdf |date=February 1, 2006 }}, Canadian Journal of Fisheries and Aquatic Sciences (59):1266‚Äì1274.\n</ref><ref>Rebecca Montville and Donald Schaffner (Feb 2005),\n[http://aem.highwire.org/cgi/content/abstract/71/2/746 ''Monte Carlo Simulation of Pathogen Behavior during the Sprout Production Process''] {{webarchive |url=https://web.archive.org/web/20111002093013/http://aem.highwire.org/cgi/content/abstract/71/2/746 |date=October 2, 2011 }}, Applied and Environmental Microbiology 71(2):746‚Äì753.\n</ref><ref>S. K. J. Rasmussen, T. Ross, J. Olley and T. McMeekin (2002),\n[https://dx.doi.org/10.1016/S0168-1605(01)00687-0 ''A process risk model for the shelf life of Atlantic salmon fillets''], International Journal of Food Microbiology 73(1):47‚Äì60.\n</ref>\nclimate change,<ref>David G. Groves and Robert J. Lempert (Feb 2007), [https://dx.doi.org/10.1016/j.gloenvcha.2006.11.006 ''A new analytic method for finding policy-relevant scenarios''], Global Environmental Change 17(1):73‚Äì85.\n</ref><ref>Maged Senbel, Timothy McDaniels,  and Hadi Dowlatabadi (July 2003), \n[https://dx.doi.org/10.1016/S0959-3780(03)00009-8 ''The ecological footprint: a non-monetary metric of human consumption applied to North America''], Global Environmental Change 13(2):83‚Äì100.\n</ref><ref>Dowlatabadi, H. (1998). ''Sensitivity of Climate Change Mitigation Estimates to Assumptions About Technical Change.'' Energy Economics 20: 473‚Äì93.\n</ref><ref>West, J. J. and H. Dowlatabadi (1998). ''On assessing the economic impacts of sea level rise on developed coasts.'' Climate, change and risk. London, Routledge. 205‚Äì20.\n</ref><ref>Leiss, W., H. Dowlatabadi, and Greg Paoli (2001). ''Who's Afraid of Climate Change? A guide for the perplexed.'' Isuma 2(4): 95‚Äì103.\n</ref><ref>Morgan, M. G., M. Kandlikar, J. Risbey and H. Dowlatabadi (1999). ''Why conventional tools for policy analysis are often inadequate for problems of global change.'' Climatic Change 41: 271‚Äì81.\n</ref><ref>Casman, E. A., M. G. Morgan and H. Dowlatabadi (1999). ''Mixed Levels of Uncertainty in Complex Policy Models.'' Risk Analysis 19(1): 33‚Äì42. \n</ref><ref>Dowlatabadi, H. (2003). ''Scale and Scope In Integrated Assessment: lessons from ten years with ICAM. Scaling in Integrated Assessment.'' J. Rotmans and D. S. Rothman. Lisse, Swetz & Zeitlinger: 55‚Äì72. \n</ref><ref>Dowlatabadi, H. (2000). ''Bumping against a gas ceiling.'' Climatic Change 46(3): 391‚Äì407. \n</ref><ref>Morgan, M. G. and H. Dowlatabadi (1996). ''Learning From Integrated Assessment of Climate Change.'' Climatic Change 34: 337‚Äì368.\n</ref> technology and defense,<ref>Henry Heimeier (1996), A New Paradigm For Modeling The Precision Strike Process, published in MILCOM96. \n</ref><ref>Russell F. Richards, Henry A. Neimeier, W. L. Hamm, and D. L. Alexander,\n\"Analytical Modeling in Support of C4ISR Mission Assessment (CMA),\" Third\nInternational Symposium on Command and Control Research and Technology,\nNational Defense University, Fort McNair, Washington, DC, June 17‚Äì20, 1997, pp. 626‚Äì\n639.\n</ref><ref>Henry Neimeier and C. McGowan (1996), \"Analyzing Processes with HANQ\", Proceedings of the International Council on Systems Engineering '96.\n</ref><ref>Kenneth P. Kuskey and Susan K. Parker (2000), \"The Architecture of CAPE Models\", MITRE technical paper.  See [http://www.mitre.org/work/tech_papers/tech_papers_00/kuskey_architecture/index.html Abstract].\n</ref><ref>Henry Neimeier (1994), \"Analytic Queuing Network\", Conference Proceedings of the 12th International Conference on the System Dynamics Society, in Stirling, Scotland.\n</ref><ref>Henry Neimeier (1996), \"Analytic Uncertainty Modeling Versus Discrete Event Simulation\", PHALANX.\n</ref><ref>Rahul Tongia, \"Can broadband over powerline carrier (PLC) compete?\".  The author uses Analytica to model the economic viability of the introduction of a PLC service.\n</ref><ref>Promises and False Promises of PowerLine Carrier (PLC) Broadband Communications ‚Äì A Techno-Economic Analysis {{cite web\n |url=http://tprc.org/papers/2003/246/Tongia-PLC.pdf \n |title=Archived copy \n |accessdate=2011-07-08 \n |deadurl=yes \n |archiveurl=https://web.archive.org/web/20070211022443/http://tprc.org/papers/2003/246/Tongia-PLC.pdf \n |archivedate=2007-02-11 \n |df= \n}}\n</ref><ref>Kanchana Wanichkorn and Marvin Sirbu (1998), [http://www2.tepper.cmu.edu/afs/andrew/gsia/45-879/Readings/ip_pbx.pdf ''The Economics of Premises Internet Telephony'']{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}, CMU-EPP.\n</ref><ref>E.L. Kyser, E.R. Hnatek, M.H. Roettgering (2001), \n[http://cat.inist.fr/?aModele=afficheN&cpsidt=973960 ''The politics of accelerated stress testing''], Sound and Vibration 35(3):24‚Äì29.\n</ref><ref>Kevin J. Soo Hoo (June 2000), \n[http://iis-db.stanford.edu/pubs/11900/soohoo.pdf ''How Much Is Enough? A Risk-Management Approach to Computer Security''] {{webarchive |url=https://web.archive.org/web/20110921004719/http://iis-db.stanford.edu/pubs/11900/soohoo.pdf |date=September 21, 2011 }}, Working Paper, Consortium for Research on Information Security and Policy (CRISP), Stanford University.\n</ref><ref>M. Steinbach and S. Giles of MITRE (2005), [http://www.aiaa.org/content.cfm?pageid=406&gTable=Paper&gID=37642 ''A Model for Joint Infrastructure Investment''] {{webarchive|url=https://web.archive.org/web/20110924165023/http://www.aiaa.org/content.cfm?pageid=406 |date=2011-09-24 }}, AIAA-2005-7309, in AIAA 5th ATIO and 16th Lighter-than-air sys tech and balloon systems conferences, Arlington VA, Sep 26‚Äì28, 2005.\n</ref><ref>Bloomfield, R., Guerra, S. (2002), [http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=1028892 ''Process modelling to support dependability arguments''], Proceedings. International Conference on Dependable Systems and Networks, pg. 113‚Äì122. DSN 2002. \n</ref><ref>Christopher L Weber and Sanath K Kalidas (Fall 2004), \n[http://www.cmu.edu/greenpractices/green_initiatives/new_house_images/NewHouseCBA_final.pdf ''Cost-Benefit Analysis of LEED Silver Certification for New House Residence Hall at Carnegie Mellon University''], Civil Systems Investment Planning and Pricing Project, Dept. of Civil &amp; Environmental Engineering, Carnegie Mellon University.\n</ref><ref>J. McMahon, X. Liu, I. Turiel (Jun 2000), [http://www.osti.gov/energycitations/product.biblio.jsp?osti_id=767554 ''Uncertainty and sensitivity analyses of ballast life-cycle cost and payback period''], Technical Report LBNL‚Äì44450, Lawrence Berkeley Labs, Berkeley CA.\n</ref><ref>Paul K. Davis (2000), [http://portal.acm.org/citation.cfm?id=510378.510428 ''Dealing with complexity: exploratory analysis enabled by multiresolultion, multiperspective modeling''], Proceedings of the 32nd Conference on Winter Simulation, pg. 293‚Äì302.\n</ref><ref>Paul K. Davis (2000), [http://www.informs-sim.org/wsc00papers/043.PDF ''Exploratory Analysis Enabled by Multiresolution, Multiperspective Modeling''], Proceedings of the 2000 Winter Simulation Conference\nJ. A. Joines, R. R. Barton, K. Kang, and P. A. Fishwick, eds.\n</ref><ref>NASA (1994), [http://sbir.gsfc.nasa.gov/SBIR/successes/ss/10-018text.html Schedule and Cost Risk Analysis Modeling (SCRAM) System], NASA SBIR Successes.\n</ref>\nstrategic financial planning,<ref>{{cite web|url=http://cubeplan.com/cubeplan/case-studies/ |title=Cubeplan case studies |publisher=Cubeplan.com |date=|accessdate=2011-07-12}}</ref><ref>{{cite web|url=http://novix.com/ |title=Novix consulting services |publisher=Novix.com |date=|accessdate=2011-07-12}}</ref> \nR&D planning and portfolio management,<ref>Enrich Consulting, [http://enrichconsulting.com/news.html publications on Portfolio Management] {{webarchive |url=https://web.archive.org/web/20110713120746/http://enrichconsulting.com/news.html |date=July 13, 2011 }}</ref><ref>{{cite web|url=http://www.bicore.nl/ |title=Bicore, Inc |publisher=Bicore.nl |date=|accessdate=2011-07-12}}</ref><ref>{{cite web |url=http://www.lumina.com/case-studies/w-l-gore-and-associates/ |title=R&D evaluation tools at W.L. Gore |publisher=Lumina |archiveurl=https://web.archive.org/web/20131017035957/http://www.lumina.com/case-studies/w-l-gore-and-associates/ |archivedate=October 17, 2013}}\n</ref> \n[[financial services]], \n[[aerospace]],<ref>[http://www.lumina.com/case-studies/nasa/ Speeding turnaround of the Space Shuttle] {{webarchive |url=https://web.archive.org/web/20120315223805/http://www.lumina.com/case-studies/nasa/ |date=March 15, 2012 }}, Lumina case studies</ref> manufacturing<ref>[http://www.lumina.com/case-studies/auto-co/ Auto maker saves $250M on warranty costs] {{webarchive|url=https://web.archive.org/web/20101212031800/http://www.lumina.com/case-studies/auto-co/ |date=2010-12-12 }}, Lumina case studies\n</ref> and environmental health impact assessment.<ref>James Grellier, Paolo Ravazzani, and Elisabeth Cardis (2014),[http://www.sciencedirect.com/science/article/pii/S0160412013002110 ''Potential health impacts of residential exposures to extremely low frequency magnetic fields in Europe''], Environment International 62, p55-63. {{doi|10.1016/j.envint.2013.09.017}}</ref>\n\n== Editions ==\n\nThe Analytica software runs on [[Microsoft Windows]] operating systems.  Three editions (Professional, Enterprise, Optimizer) each with more functions and cost, are purchased by users interested in building models. A free edition is available, called Analytica Free 101, which allows you to build medium to moderate sized models of up to 101 user objects.. Free 101 also allows you to view models with more than 101 objects, change inputs, and compute results, which enables free sharing of models for review. A more capable but non-free Power Player enables users to save inputs and utilize database connections. The Analytica Cloud Player allows you to share models over the web and lets users access and run via a web browser.\n\nThe most recent release of Analytica is version 5.1, released in May 2018.\n\n== History ==\n\nAnalytica's predecessor, called ''Demos'',<ref>\nNeil Wishbow and Max Henrion, \"Demos User's Manual\", Department of Engineering and Public Policy, Carnegie Mellon University, 1987.</ref> grew from the research on tools for [[policy analysis]] by Max Henrion as a PhD student and later professor at [[Carnegie Mellon University]] between 1979 and 1990.  Henrion founded Lumina Decision Systems in 1991 with Brian Arnold.  Lumina continued to develop the software and apply it to environmental and public policy analysis applications.  Lumina first released Analytica as a product in 1996.\n\n==References==\n{{reflist|colwidth=30em}}\n\n==External links==\n*{{official website}}\n\n\n{{DEFAULTSORT:Analytica (software)}}\n[[Category:Mathematical modeling]]\n[[Category:Mathematical optimization software]]\n[[Category:Science software]]\n[[Category:Statistical programming languages]]\n[[Category:Visual programming languages]]\n[[Category:Numerical programming languages]]\n[[Category:Numerical software]]\n[[Category:Array programming languages]]\n[[Category:Science software for Windows]]\n[[Category:Computer algebra systems]]\n[[Category:Plotting software]]"
    },
    {
      "title": "APMonitor",
      "url": "https://en.wikipedia.org/wiki/APMonitor",
      "text": "{{Infobox Software\n| name                   = APMonitor\n| logo                   = APMonitor_Logo2.png\n| logo size              = 300px\n| screenshot             =<!-- Deleted image removed: [[Image:APMonitor trend.png|200px]] -->\n| caption                = APMonitor Web-based Interface\n| developer              = APMonitor\n| latest_release_version = v0.7.6\n| latest_release_date    = {{release date|2018|01|31}}\n| operating_system       = [[Cross-platform]]\n| genre                  = [[List of numerical analysis software|Technical computing]]\n| license                = [[Proprietary software|Proprietary]], [[BSD license|BSD]]\n| website                = [http://www.apmonitor.com APMonitor product page]\n}}\n\n'''Advanced process monitor (APMonitor)''' is a modeling language for [[differential equation|differential]] [[algebraic equation|algebraic]] ([[differential algebraic equation|DAE]]) equations.<ref>{{cite journal|last=J.D. Hedengren |author2=R. Asgharzadeh Shishavan |author3=K.M. Powell |author4=T.F. Edgar |title=Nonlinear modeling, estimation and predictive control in APMonitor|journal=Computers & Chemical Engineering|year=2014|volume=70|issue=5|pages=133‚Äì148|doi=10.1016/j.compchemeng.2014.04.013}}</ref>  It is a free web-service or local server for solving representations of physical systems in the form of implicit DAE models.  APMonitor is suited for large-scale problems and solves [[linear programming]], [[integer programming]], [[nonlinear programming]], nonlinear mixed integer programming, dynamic simulation,<ref>{{cite journal | last=Hedengren | first=J. | title=A Nonlinear Model Library for Dynamics and Control | journal=CACHE (Computer Aids for Chemical Engineering) News | year=2008 | url=http://www.hedengren.net/research/Publications/Cache_2008/NonlinearModelLibrary.pdf}}</ref> [[moving horizon estimation]],<ref>{{cite journal | last=Spivey | first=B. | title=Monitoring of Process Fouling Using First-Principles Modeling and Moving Horizon Estimation | journal=Proc. Applications of Computer Algebra (ACA) Conference | year=2009}}</ref> and nonlinear [[model predictive control]].<ref>{{cite journal | last=Ramlal | first=J. | title=Moving Horizon Estimation for an Industrial Gas Phase Polymerization Reactor | journal=IFAC Symposium on Nonlinear Control Systems Design (NOLCOS) | year=2007 | url=http://apmonitor.com/Documents/mhe.pdf | access-date=2010-03-29 | archive-url=https://web.archive.org/web/20090920070424/http://apmonitor.com/Documents/mhe.pdf | archive-date=2009-09-20 | dead-url=yes | df= }}</ref>  APMonitor does not solve the problems directly, but calls [[nonlinear programming]] solvers such as [[APOPT]], [[BPOPT]], [[IPOPT]], [[MINOS (optimization software)|MINOS]], and [[SNOPT]]. The APMonitor API provides exact first and second derivatives of continuous functions to the solvers through [[automatic differentiation]] and in [[sparse matrix]] form.\n\n==Programming Language Integration==\n\n[[Julia (programming language)|Julia]], [[MATLAB]], [[Python (programming language)|Python]] are mathematical programming languages that have APMonitor integration through web-service APIs. The [[Gekko_(optimization_software)|GEKKO Optimization Suite]] is a recent extension of APMonitor with complete Python integration. The interfaces are built-in optimization toolboxes or modules to both load and process solutions of optimization problems. APMonitor is an [[object-oriented]] [[modeling language]] and optimization suite that relies on programming languages to load, run, and retrieve solutions. APMonitor models and data are compiled at run-time and translated into objects that are solved by an optimization engine such as [[APOPT]] or [[IPOPT]]. The optimization engine is not specified by APMonitor, allowing several different optimization engines to be switched out. The simulation or optimization mode is also configurable to reconfigure the model for [[dynamic simulation]], nonlinear [[model predictive control]], [[moving horizon estimation]] or general problems in [[mathematical optimization]].\n\nAs a first step in solving the problem, a mathematical model is expressed in terms of variables and equations such as the Hock & Schittkowski Benchmark Problem #71<ref>W. Hock and K. Schittkowski, Test Examples for Nonlinear Programming Codes, Lecture Notes in Economics and Mathematical Systems, Vol. 187, Springer 1981.</ref> used to test the performance of [[nonlinear programming]] solvers. This particular optimization problem has an objective function <math>\\min_{x\\in\\mathbb R}\\; x_1 x_4 (x_1+x_2+x_3)+x_3</math> and subject to the inequality constraint <math>x_1 x_2 x_3 x_4 \\ge 25</math> and equality constraint <math>{x_1}^2 + {x_2}^2 + {x_3}^2 + {x_4}^2=40</math>. The four variables must be between a lower bound of 1 and an upper bound of 5. The initial guess values are <math>x_1 = 1, x_2=5, x_3=5, x_4=1</math>. This mathematical model is translated into the APMonitor modeling language in the following text file.\n\n<source lang=\"python\">\n! file saved as hs71.apm\nVariables\n  x1 = 1, >=1, <=5\n  x2 = 5, >=1, <=5\n  x3 = 5, >=1, <=5\n  x4 = 1, >=1, <=5\nEnd Variables\n\nEquations\n  minimize x1*x4*(x1+x2+x3) + x3\n\n  x1*x2*x3*x4 > 25\n  x1^2 + x2^2 + x3^2 + x4^2 = 40\nEnd Equations\n</source>\n\nThe problem is then solved in Python by first installing the APMonitor package with '''pip install APMonitor''' or from the following Python code.\n\n<source lang=\"python\">\n# Install APMonitor\nimport pip\npip.main(['install','APMonitor'])\n</source>\n\nInstalling a Python is only required once for any module. Once the APMonitor package is installed, it is imported and the '''apm_solve''' function solves the optimization problem. The solution is returned to the programming language for further processing and analysis.\n\n<source lang=\"python\">\n# Python example for solving an optimization problem\nfrom APMonitor.apm import *\n\n# Solve optimization problem\nsol = apm_solve('hs71',3)\n\n# Access solution\nx1 = sol['x1']\nx2 = sol['x2']\n</source>\n\nSimilar interfaces are available for [[MATLAB]] and [[Julia (programming language)|Julia]] with minor differences from the above syntax. Extending the capability of a modeling language is important because significant pre- or post-processing of data or solutions is often required when solving complex optimization, dynamic simulation, estimation, or control problems.\n\n==High Index DAEs==\n\nThe highest order of a derivative that is necessary to return a DAE to ODE form is called the ''differentiation index''. A standard way for dealing with high-index DAEs is to differentiate the equations to put them in index-1 DAE or ODE form (see [[Pantelides algorithm]]). However, this approach can cause a number of undesirable numerical issues such as instability.  While the syntax is similar to other modeling languages such as gProms, APMonitor solves DAEs of any index without rearrangement or differentiation.<ref>{{cite journal | last=Harney | first=D. | title=Numerical evaluation of the stability of stationary points of index-2 differential-algebraic equations: Applications to reactive flash and reactive distillation systems | journal=Computers & Chemical Engineering | year=2013 | doi=10.1016/j.compchemeng.2012.09.021 | volume=49 | pages=61‚Äì69}}</ref>  As an example, an index-3 DAE is shown below for the pendulum motion equations and lower index rearrangements can return this system of equations to ODE form (see [http://apmonitor.com/wiki/index.php/Apps/PendulumMotion Index 0 to 3 Pendulum example]).\n\n===Pendulum motion (index-3 DAE form)===\n<source lang=\"fortran\">\nModel pendulum\n  Parameters\n    m = 1\n    g = 9.81\n    s = 1\n  End Parameters\n\n  Variables\n    x = 0\n    y = -s\n    v = 1\n    w = 0\n    lam = m*(1+s*g)/2*s^2\n  End Variables\n\n  Equations\n    x^2 + y^2 = s^2\n    $x = v\n    $y = w\n    m*$v = -2*x*lam\n    m*$w = -m*g - 2*y*lam\n  End Equations\nEnd Model\n</source>\n\n==Applications in APMonitor Modeling Language==\n\nMany physical systems are naturally expressed by [[differential algebraic equation]].  Some of these include:\n\n* [[cell culture]]s\n* [[chemical reactor]]s\n* [[Cogeneration|cogeneration (power and heat)]]<ref>{{cite journal | last=Mojica | first=J. | title=Optimal combined long-term facility design and short-term operational strategy for CHP capacity investments | journal=Energy | year=2017 | url=https://authors.elsevier.com/a/1UEQ7_8CgI-dD6 | doi=10.1016/j.energy.2016.12.009 | volume=118 | pages=97‚Äì115}}</ref>\n* [[Fractionating column|distillation columns]]\n* [[Oil well|drilling automation]]<ref>{{cite journal | last=Eaton | first=A. | title=Real time model identification using multi-fidelity models in managed pressure drilling | journal=Computers & Chemical Engineering | year=2017 | url=http://www.sciencedirect.com/science/article/pii/S0098135416303428 | doi=10.1016/j.compchemeng.2016.11.008 | volume=97 | pages=76‚Äì84}}</ref>\n* [[essential oil]] steam distillation<ref>{{cite journal | last=Valderrama | first=F. | title=An optimal control approach to steam distillation of essential oils from aromatic plants | journal=Computers & Chemical Engineering | year=2018 | url=https://doi.org/10.1016/j.compchemeng.2018.05.009}}</ref>\n* [[friction stir welding]]<ref>{{cite thesis |degree=M.Sc. |first=Isak |last=Nielsen |title=Modeling and Control of Friction Stir Welding in 5 cm thick Copper Canisters |publisher=Link√∂ping University |year=2012 |url=http://liu.diva-portal.org/smash/record.jsf?pid=diva2:535544&rvn=4}}</ref>\n* hydrate formation in deep-sea pipelines<ref>{{cite journal | last=Brower | first=D. | title=Fiber Optic Monitoring of Subsea Equipment | journal=OMAE 2012 Proceedings, Rio de Janeiro, Brazil | year=2012 | url=http://hedengren.net/research/Publications/OMAE_2012/Brower_Hedengren_OMAE_2012.pdf}}</ref>\n* [[infectious disease dynamics|infectious disease spread]]\n* [[Van der Pol oscillator|oscillators]]\n* severe slugging control<ref>{{cite journal | last=Eaton | first=A. | title=Post-installed fiber optic pressure sensors on subsea production risers for severe slugging control | journal=OMAE 2015 Proceedings, St. John's, Canada | year=2015 | url=http://apm.byu.edu/prism/uploads/Projects/Eaton_OMAE15.pdf}}</ref>\n* solar thermal energy production<ref>{{cite journal | last=Powell | first=K. | title=Dynamic Optimization of a Hybrid Solar Thermal and Fossil Fuel System | journal=Solar Energy | year=2014 | url=http://www.sciencedirect.com/science/article/pii/S0038092X14003429 | doi=10.1016/j.solener.2014.07.004 | volume=108 | pages=210‚Äì218}}</ref>\n* [[solid oxide fuel cell]]s<ref>{{cite journal | last=Spivey | first=B. | title=Dynamic Modeling of Reliability Constraints in Solid Oxide Fuel Cells and Implications for Advanced Control | journal=AIChE Annual Meeting Proceedings, Salt Lake City, Utah | year=2010 | url=http://apmonitor.com/wiki/uploads/Apps/sofc.pdf}}</ref><ref>{{cite journal | last=Spivey | first=B. | title=Dynamic modeling, simulation, and MIMO predictive control of a tubular solid oxide fuel cell | journal=Journal of Process Control | year=2012 | doi=10.1016/j.jprocont.2012.01.015 | volume=22 | pages=1502‚Äì1520}}</ref>\n* [[Space Shuttle|space shuttle launch simulation]]\n* [[Unmanned aerial vehicle|Unmanned Aerial Vehicles (UAVs)]]<ref>{{cite journal | last=Sun | first=L. | title=Optimal Trajectory Generation using Model Predictive Control for Aerially Towed Cable Systems | journal=Journal of Guidance, Control, and Dynamics | year=2013 | url=http://apm.byu.edu/prism/uploads/Members/sun_2013.pdf}}</ref>\n\nModels for a direct current (DC) motor and blood glucose response of an insulin dependent patient are listed below. They are representative of differential and algebraic equations encountered in many branches of science and engineering.\n\n===Direct current (DC) motor===\n<source lang=\"fortran\">\nParameters\n  ! motor parameters (dc motor)\n  v   = 36        ! input voltage to the motor (volts)\n  rm  = 0.1       ! motor resistance (ohms)\n  lm  = 0.01      ! motor inductance (henrys)\n  kb  = 6.5e-4    ! back emf constant (volt¬∑s/rad)\n  kt  = 0.1       ! torque constant (N¬∑m/a)\n  jm  = 1.0e-4    ! rotor inertia (kg m¬≤)\n  bm  = 1.0e-5    ! mechanical damping (linear model of friction: bm * dth)\n\n  ! load parameters\n  jl = 1000*jm    ! load inertia (1000 times the rotor)\n  bl = 1.0e-3     ! load damping (friction)\n  k = 1.0e2       ! spring constant for motor shaft to load\n  b = 0.1         ! spring damping for motor shaft to load\nEnd Parameters\n\nVariables\n  i     = 0       ! motor electric current (amperes)\n  dth_m = 0       ! rotor angular velocity sometimes called omega (radians/sec)\n  th_m  = 0       ! rotor angle, theta (radians)\n  dth_l = 0       ! wheel angular velocity (rad/s)\n  th_l  = 0       ! wheel angle (radians)\nEnd Variables\n\nEquations\n  lm*$i - v = -rm*i -    kb *$th_m\n  jm*$dth_m =  kt*i - (bm+b)*$th_m - k*th_m +     b *$th_l + k*th_l\n  jl*$dth_l =             b *$th_m + k*th_m - (b+bl)*$th_l - k*th_l\n  dth_m = $th_m\n  dth_l = $th_l \nEnd Equations\n</source>\n\n===Blood glucose response of an insulin dependent patient===\n<source lang=\"fortran\">\n! Model source:\n! A. Roy and R.S. Parker. ‚ÄúDynamic Modeling of Free Fatty \n!   Acids, Glucose, and Insulin: An Extended Minimal Model,‚Äù\n!   Diabetes Technology and Therapeutics 8(6), 617-626, 2006.\nParameters\n  p1 = 0.068       ! 1/min\n  p2 = 0.037       ! 1/min\n  p3 = 0.000012    ! 1/min\n  p4 = 1.3         ! mL/(min¬∑¬µU)\n  p5 = 0.000568    ! 1/mL\n  p6 = 0.00006     ! 1/(min¬∑¬µmol)\n  p7 = 0.03        ! 1/min\n  p8 = 4.5         ! mL/(min¬∑¬µU)\n  k1 = 0.02        ! 1/min\n  k2 = 0.03        ! 1/min\n  pF2 = 0.17       ! 1/min\n  pF3 = 0.00001    ! 1/min\n  n = 0.142        ! 1/min\n  VolG = 117       ! dL\n  VolF = 11.7      ! L\n  ! basal parameters for Type-I diabetic\n  Ib = 0           ! Insulin (¬µU/mL)\n  Xb = 0           ! Remote insulin (¬µU/mL)\n  Gb = 98          ! Blood Glucose (mg/dL)\n  Yb = 0           ! Insulin for Lipogenesis (¬µU/mL)\n  Fb = 380         ! Plasma Free Fatty Acid (¬µmol/L)\n  Zb = 380         ! Remote Free Fatty Acid (¬µmol/L)\n  ! insulin infusion rate\n  u1 = 3           ! ¬µU/min\n  ! glucose uptake rate\n  u2 = 300         ! mg/min\n  ! external lipid infusion\n  u3 = 0           ! mg/min\nEnd parameters\n\nIntermediates\n  p9 = 0.00021 * exp(-0.0055*G)  ! dL/(min*mg)\nEnd Intermediates\n\nVariables\n  I = Ib\n  X = Xb\n  G = Gb\n  Y = Yb\n  F = Fb\n  Z = Zb\nEnd variables\n\nEquations\n  ! Insulin dynamics\n  $I = -n*I  + p5*u1\n  ! Remote insulin compartment dynamics\n  $X = -p2*X + p3*I\n  ! Glucose dynamics\n  $G = -p1*G - p4*X*G + p6*G*Z + p1*Gb - p6*Gb*Zb + u2/VolG\n  ! Insulin dynamics for lipogenesis\n  $Y = -pF2*Y + pF3*I\n  ! Plasma-free fatty acid (FFA) dynamics\n  $F = -p7*(F-Fb) - p8*Y*F + p9 * (F*G-Fb*Gb) + u3/VolF\n  ! Remote FFA dynamics\n  $Z = -k2*(Z-Zb) + k1*(F-Fb)\nEnd Equations\n</source>\n\n==See also==\n* [[APOPT]]\n* [[ASCEND]]\n* [[EMSO simulator|EMSO]]\n* [[Gekko_(optimization_software)|GEKKO]]\n* [[MATLAB]]\n* [[Modelica]]\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://apmonitor.com APMonitor home page]\n* [http://apmonitor.com/do Dynamic optimization course] with APMonitor\n* [http://apmonitor.com/wiki APMonitor documentation]\n* [http://apmonitor.com/wiki/index.php/Main/APMonitorReferences APMonitor citations]\n* [http://apmonitor.com/online/view_pass.php Online solution engine] with IPOPT\n* [http://apmonitor.com/compare.htm Comparison] of popular modeling language syntax\n* Download [http://apmonitor.com/wiki/index.php/Main/MATLAB APM MATLAB], [http://apmonitor.com/wiki/index.php/Main/PythonApp APM Python], or [http://apmonitor.com/wiki/index.php/Main/JuliaOpt APM Julia] client for APMonitor\n* Download [http://apmonitor.com/wiki/index.php/Main/APMonitorServer APMonitor Server (Windows)] \n* Download [http://apmonitor.com/wiki/index.php/Main/APMonitorServerLinux APMonitor Server (Linux)]\n\n{{Mathematical optimization software}}\n\n{{DEFAULTSORT:Apmonitor}}\n[[Category:Numerical programming languages]]\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "APOPT",
      "url": "https://en.wikipedia.org/wiki/APOPT",
      "text": "{{Infobox Software\n| name                   = APOPT \n| developer              = {{URL|apopt.com|Advanced Process Solutions, LLC}}\n| latest_release_version = 1.0.4\n| latest_release_date    = {{Start date and age|2016|08|01}}\n| operating_system       = {{URL|apopt.com/developers.php|Cross-Platform}}\n| genre                  = [[List of numerical analysis software|Technical computing]]\n| license                = [[Proprietary software|Proprietary]]\n| website                = {{URL|apopt.com}}\n}}\n\n'''APOPT''' (for '''Advanced Process OPTimizer''') is a software package for solving large-scale [[Optimization (mathematics)|optimization]] problems of any of these forms:\n\n* [[Linear programming]] (LP)\n* [[Quadratic programming]] (QP)\n* [[Quadratically constrained quadratic program]] (QCQP)\n* [[Nonlinear programming]] (NLP)\n* [[Mixed integer programming]] (MIP)\n* [[Mixed integer linear programming]] (MILP)\n* [[Mixed integer nonlinear programming]] (MINLP)\n\nApplications of the APOPT include [[chemical reactor]]s,<ref>{{ cite patent | country = WO | number = 2012005740 | status = patent | title = Method for Controlling Bubble Formation in Polymerization Reactors | pubdate = 2012-01-12 | inventor = Lawson, K. W., Hedengren, J. D., Smith, L. C.}}</ref><ref>{{cite journal | last=Spivey | first=B. | title=Constrained Nonlinear Estimation for Industrial Process Fouling | journal=Industrial & Engineering Chemistry Research | volume=49 | issue=17 | pages=7824‚Äì7831 | year=2010 | doi=10.1021/ie9018116}}</ref> \n[[friction stir welding]],<ref>{{cite thesis |degree=M.Sc. |first=Isak |last=Nielsen |title=Modeling and Control of Friction Stir Welding in 5 cm thick Copper Canisters |publisher=Link√∂ping University |year=2012 |url=http://liu.diva-portal.org/smash/record.jsf?pid=diva2:535544&rvn=4}}</ref> prevention of hydrate formation in deep-sea pipelines,<ref>{{cite journal | last=Brower | first=D. | title=Fiber Optic Monitoring of Subsea Equipment | journal=OMAE 2012 Proceedings, Rio de Janeiro, Brazil | year=2012 | url=http://apm.byu.edu/prism/uploads/Members/hedengren_omae2012.pdf}}</ref><ref>{{cite journal | last=Brower | first=D. | title=Advanced Deepwater Monitoring System | journal=OMAE 2013 Proceedings, Nantes, France | year=2013 | url=http://apm.byu.edu/prism/uploads/Members/hedengren_omae2013.pdf}}</ref> [[computational biology]],<ref>{{cite journal | last=Abbott | first=C. | title=New Capabilities for Large-Scale Models in Computational Biology | journal=AIChE Annual Meeting Proceedings, Pittsburgh, PA | year=2012 | url=http://apm.byu.edu/prism/uploads/Members/abbott_aiche12.pdf}}</ref> [[solid oxide fuel cell]]s,<ref>{{cite journal | last=Spivey | first=B. | title=Dynamic Modeling of Reliability Constraints in Solid Oxide Fuel Cells and Implications for Advanced Control | journal=AIChE Annual Meeting Proceedings, Salt Lake City, Utah | year=2010 | url=http://apm.byu.edu/prism/uploads/Members/spivey_ppt_acc2012.pdf}}</ref><ref>{{cite journal | last=Jacobsen | first=L. | title=Model Predictive Control with a Rigorous Model of a Solid Oxide Fuel Cell | journal=American Control Conference (ACC), Washington, DC | year=2013 | url=http://apm.byu.edu/prism/uploads/Members/powell_acc2013.pdf}}</ref> and flight controls for [[Unmanned aerial vehicle|Unmanned Aerial Vehicles (UAVs)]].<ref>{{cite journal | last=Sun | first=L. | title=Optimal Trajectory Generation using Model Predictive Control for Aerially Towed Cable Systems | journal=Journal of Guidance, Control, and Dynamics | year=2013 | url=http://apm.byu.edu/prism/uploads/Members/sun_2013.pdf}}</ref>\n\n== See also ==\n* APOPT is supported in [[AMPL]], [[APMonitor]], [[Gekko_(optimization_software)|Gekko]], [[Julia (programming language)|Julia]], [[MATLAB]], [[Pyomo]], and [[Python (programming language)|Python]].\n\n==References==\n{{reflist}}\n\n== External links ==\n* {{Official website|apopt.com}}\n* [http://apmonitor.com/online/view_pass.php Web interface to solve optimization problems] with the APOPT solver\n* [http://apopt.com/download.php Download APOPT for AMPL, MATLAB, Julia, Python, or APMonitor]\n\n\n{{Mathematical optimization software}}\n{{DEFAULTSORT:Apopt}}\n[[Category:Numerical software]]\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "Artelys Knitro",
      "url": "https://en.wikipedia.org/wiki/Artelys_Knitro",
      "text": "{{Infobox programming language\n| name                   = Artelys Knitro\n| designer               = {{ubl|Richard Waltz|Jorge Nocedal|Todd Plantenga|Richard Byrd}}\n| developer              = Artelys\n| license                = [[Proprietary software|Proprietary]]\n| website                = {{URL|https://www.artelys.com/en/optimization-tools/knitro|Artelys Knitro}}\n| year                   = {{Start date|2001}}\n| latest_release_version = 12\n| latest_release_date    = {{Start date and age|2019|05|31}}\n| operating_system       = [[Cross-platform]]\n| genre                  = [[Algebraic modeling language|Algebraic Modeling Language (AML)]]\n}}\n\n'''Artelys Knitro''' <ref>[https://www.artelys.com/en/optimization-tools/knitro Artelys Knitro Website]</ref>  is a [[Commercial software|commercial]] [[Software package (installation)|software package]] for solving large scale nonlinear [[mathematical programming|mathematical optimization]] problems.\n\n'''KNITRO''' ‚Äì (the original solver name) short for \"'''N'''onlinear '''I'''nterior point '''T'''rust '''R'''egion '''O'''ptimization\" (the \"K\" is silent) ‚Äì was co-created by Richard Waltz, [[Jorge Nocedal]], Todd Plantenga and Richard Byrd. It was first introduced in 2001, as a derivative of academic research at [[Northwestern University]] (through Ziena Optimization LLC), and has since been continually improved by developers at Artelys.\n\nOptimization problems must be presented to Knitro in mathematical form, and should provide a way of computing function derivatives using [[sparse matrix|sparse matrices]] (Knitro can compute derivatives approximation but in most cases providing the exact derivatives is beneficial). An often easier approach is to develop the optimization problem in an [[algebraic modeling language]]. The modeling environment computes function derivatives, and Knitro is called as a \"solver\" from within the environment.\n\n==Problem classes solved by Artelys Knitro==\n\nKnitro is specialized for [[nonlinear optimization]] but also solves a wide range of optimization problems:\n\n* General nonlinear problems (NLP), including non-convex \n* Systems of nonlinear equations \n* Linear problems (LP) \n* Quadratic problems (QP/QCQP/SOCP), both convex and non-convex \n* Least squares problems / regression, both linear and nonlinear \n* Mathematical programs with complementarity constraints (MPCC/MPEC) \n* Mixed-integer nonlinear problems (MIP/MINLP) \n* Derivative-free optimization problems (DFO)\n\n==Algorithms==\n\nArtelys Knitro contains a wide range of optimization algorithms. \n\n===NonLinear Programming (NLP) solver===\nKnitro offers four different optimization [[algorithm]]s for solving optimization problems<ref>[https://www.artelys.com/tools/knitro_doc/2_userGuide/algorithms.html Artelys Knitro NLP algorithms]</ref>. Two algorithms are of the [[interior point method|interior point]] type, and two are of the [[active set method|active set]] type. These algorithms are known to have fundamentally different characteristics; for example, interior point methods follow a path through the interior of the [[feasible region]] while active set methods tend to stay at the boundaries. Knitro provides both types of algorithm for greater flexibility in solving problems, and allows crossover during the solution process from one algorithm to another. The code also provides a multistart option for promoting the computation of the [[global minimum]].\n\n* Interior/Direct algorithm\n* Interior/Conjugate Gradient algorithm\n* Active Set algorithm\n* Sequential Quadratic Programming (SQP) algorithm\n\n===Mixed-Integer NonLinear Programming (MINLP) solver===\nKnitro provides tools for solving optimization models (both linear and nonlinear) with binary or integer variables. The Knitro mixed integer programming (MIP) code offers three algorithms for mixed-integer nonlinear programming (MINLP)<ref>[https://www.artelys.com/tools/knitro_doc/2_userGuide/minlp.html Artelys Knitro MINLP algorithms]</ref>:\n\n* Nonlinear Branch and Bound\n* Quesada Grossman algorithm\n* Mixed-Integer Sequential Quadratic Programming (MISQP)\n\n==Features==\nArtelys Knitro supports a variety of programming and modeling languages including<ref>[https://www.artelys.com/en/optimization-tools/knitro#features-tab Artelys Knitro Features]</ref>.\n* Object-oriented interfaces for C++, C#, Java and Python\n* Matrix-oriented interfaces for C, Fortran, MATLAB, and R\n* Links to modeling languages: AIMMS, AMPL, GAMS, and MPL\n* Links to Excel through Frontline Solvers\n\nArtelys Knitro also includes a number of key features:\n* A large set of well-documented user options<ref>[https://www.artelys.com/tools/knitro_doc/ Artelys Knitro User's Manual]</ref> and automatic tuner\n* (Parallel) multi-start for global optimization\n* Derivatives approximation and checker\n* Internal presolver\n\n==References==\n* {{cite book |last=Nocedal |first=Jorge |last2=Wright |first2=Stephen J. |title=Numerical Optimization |edition=2nd |date=2006 |publisher=[[Springer Publishing]] |isbn=0-387-30303-0}}\n* {{cite journal |last=Byrd |first=Richard H. |last2=Nocedal |first2=Jorge |last3=Waltz |first3=Richard A. |date=2006 |title=Knitro: An Integrated Package for Nonlinear Optimization |url=https://www.artelys.com/downloads/pdf/composants-numeriques/knitro/papers/integratedpackage.pdf |format=PDF |accessdate=November 17, 2017}}\n\n==External links==\n{{reflist}}\n* [http://users.iems.northwestern.edu/~nocedal/ Jorge Nocedal], profile at EECS department of [[Robert R. McCormick School of Engineering and Applied Science|McCormick School of Engineering ]]\n\n{{Mathematical optimization software}}\n\n[[Category:Numerical software]]\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "ASCEND",
      "url": "https://en.wikipedia.org/wiki/ASCEND",
      "text": "{{Other uses|Ascend (disambiguation)}}\n{{Infobox Software\n|logo =\n|screenshot =\n|caption =\n|developer = the ASCEND team\n|latest_release_version = 0.9.8\n|latest_release_date =  {{Start date and age|2012|04|30}}\n|operating_system = [[Linux]], [[Microsoft Windows|Windows]] (and partial support for [[Mac OS X]])\n|programming language = [[C (programming language)|C]], [[Python (programming language)|Python]], [[Tcl/Tk]], [[C++]]\n|genre = mathematical modelling\n|license = [[GNU General Public License|GPL]] ([[free software]])\n|website = {{url|http://ascend4.org}}\n}}\n'''ASCEND''' is an [[Open-source software|open source]], [[mathematical modelling]] chemical process modelling system developed at [[Carnegie Mellon University]] since late 1978.<ref>{{cite book | last1 = Piela | first1 = McKelvey | last2 = Westerberg | year = 1992| title = An introduction to ASCEND: its language and interactive environment | doi = 10.1109/HICSS.1992.183516 | journal = Proceedings of the Twenty-Fifth Hawaii International Conference on System Sciences| volume =  | issue = | pages = 449‚Äì461 vol.3 | isbn = 978-0-8186-2420-9 }}</ref><ref>[http://ascend4.org/History History of ASCEND] from the ASCEND website</ref> ASCEND is an acronym which stands for Advanced System for Computations in Engineering Design. Its main uses have been in the field of [[chemical process modelling]] although its capabilities are general.<ref>[http://ascendwiki.cheme.cmu.edu/Publications ASCEND bibliography] {{webarchive |url=https://web.archive.org/web/20101014235434/http://ascendwiki.cheme.cmu.edu/Publications |date=October 14, 2010 }}</ref> \n\nASCEND includes nonlinear algebraic [[solver]]s, differential/algebraic equation solvers, nonlinear [[Optimization (mathematics)|optimization]] and modelling of multi-region 'conditional models'. Its matrix operations are supported by an efficient [[sparse matrix]] solver called ''mtx''.\n\nASCEND differs from earlier modelling systems because it separates the solving strategy from model building. So domain experts (people writing the models) and computational engineers (people writing the solver code) can work separately in developing ASCEND. Together with a number of other early modelling tools, its architecture helped to inspire newer languages such as [[Modelica]].<ref>{{cite book | last1 = Elmqvist | first1 = Mattsson | last2 = Otter | year = 1999 | title = Modelica-a language for physical system modeling, visualization and interaction | doi = 10.1109/CACSD.1999.808720 | journal = Proceedings of the 1999 IEEE International Symposium on Computer Aided Control System Design (Cat. No.99TH8404)| volume =  | issue = | pages = 630‚Äì639 | isbn = 978-0-7803-5500-2 }}</ref><ref>Karl Johan √Östr√∂m'', 2001 Control of complex systems'', Springer</ref> It was recognised for its flexible use of variables and parameters, which it always treats as solvable, if desired<ref>{{cite journal | last1 = Sinha | first1 = R. | last2 = Liang | first2 = V.C. | last3 = Paredis | first3 = C.J.J. | last4 = Khosla | first4 = P.K. | year = 2001 | title = Modeling and Simulation Methods for Design of Engineering Systems | url = | journal = Journal of Computing and Information Science in Engineering | volume = 1 | issue = | pages = 84‚Äì91 | doi=10.1115/1.1344877| citeseerx = 10.1.1.64.4463 }}</ref>\n\nThe software remains as an active open-source software project, and has been part of the Google Summer of Code programme in 2009, 2010, 2011, 2012, 2013 (under the Python Software Foundation) and has been accepted for the 2015 programme as well.<ref>{{Cite web | url=http://www.google-melange.com/gsoc/projects/list/google/gsoc2013 | title=Google Summer of Code 2013}}</ref>\n\n==See also==\n* [[Art Westerberg]]\n* [[AMPL]]\n* [[APMonitor]]\n* [[EMSO simulator|EMSO]]\n* [[JModelica.org]]\n* [[Modelica]]\n* [[List of chemical process simulators]]\n\n==References==\n{{reflist}}\n\n==External links==\n* {{official website|http://www.ascend4.org/}}\n\n\n{{DEFAULTSORT:Ascend}}\n[[Category:Simulation programming languages]]\n[[Category:Mathematical optimization software]]\n[[Category:Free simulation software]]\n[[Category:Free software programmed in Tcl]]\n[[Category:Declarative programming languages]]\n[[Category:Object-oriented programming]]\n[[Category:Free software programmed in Python]]\n[[Category:Software that uses Tk]]\n\n\n{{science-software-stub}}\n{{free-software-stub}}"
    },
    {
      "title": "ASTOS",
      "url": "https://en.wikipedia.org/wiki/ASTOS",
      "text": "{{Infobox Software\n| name                   = ASTOS\n| screenshot             = ASTOS screenshot.png\n| screenshot size        = 300px\n| caption                = ASTOS running on Windows 7 showing GRACE mission\n| developer              = Astos Solutions GmbH\n| latest_release_version = 8.0.6\n| latest_release_date    = {{release date|2015|12|07}}\n| operating_system       = [[Cross-platform]]\n| genre                  = [[List of numerical analysis software|Technical computing]]\n| license                = [[Proprietary software|Proprietary]]\n| website                = [http://www.astos.de/products/astos ASTOS product website]\n}}\n\n'''ASTOS''' is a tool dedicated to mission analysis, [[Trajectory optimization]], vehicle design and [[Computer simulation|simulation]] for space scenarios, i.e. [[Launch vehicle|launch]], [[Atmospheric reentry|re-entry]] missions, [[Orbital maneuver|orbit transfers]], Earth observation, navigation, coverage and re-entry safety assessments. It solves [[Aerospace]] problems with a data driven interface and automatic initial guesses. Since 1989, with the support of the [[European Space Agency]], it has developed, and improved this trajectory optimization environment to compute optimal trajectories for a variety of complex multi-phase [[Optimal control]] problems. ASTOS is being extensively used at ESA and aerospace industry community to calculate mission analysis, optimal launch and entry trajectories and was one of the tools used by ESA to assess the risk due to the [[Automated Transfer Vehicle|ATV]] [[Jules Verne ATV|'Jules Verne']] re-entry. ASTOS is compatible with [[Microsoft Windows|Windows]] and [[Linux]] platforms and is maintained and commercialized by Astos Solutions GmbH.\n\n==History==\nThe development of ASTOS (formerly named ALTOS) started in 1989 at the DLR in Oberpfaffenhofen and MBB (now Astrium).\nIn 1991 the Institute of Flight Mechanics and Control (IFR) at the [[University of Stuttgart]] under the head of Prof. Klaus Well took the responsibility for the development of ASTOS. In 1999 the commercialization of ASTOS began. In the period 2001-2006 ASTOS was sold by Technology Transfer Initiative of the [[University of Stuttgart]] (TTI). Since September 2006, the newly founded company Astos Solutions GmbH is responsible for development and sales of ASTOS.\n\n==Projects==\nASTOS is being extensively used in aerospace agencies and industry since 1998, hereafter a not complete list of project is presented where the software was involved during the design or accomplishment of the space mission.\n* Performance map of conventional launch vehicle: [[Ariane 6]], [[Ariane 5]], [[Vega (rocket)|Vega]], [[Soyuz (rocket family)|Soyuz]] from [[Guiana Space Centre]], several [[FLPP]] concepts, [[VLM (rocket)|VLM]]\n* Feasibility study of reusable launch vehicle: [[Hopper (spacecraft)|Hopper]], [[Skylon (spacecraft)|Skylon]], [[SpaceLiner]], [[Fast 20XX]] ALPHA. \n* Earth Atmospheric re-entry: [[X-38]], [[Atmospheric Reentry Demonstrator]], [[IXV]], EXPERT.\n* Safety aspect related to the [[Automated Transfer Vehicle|ATV]] re-entry.\n* Planetary re-entry: [[Beagle 2]], [[ExoMars]], [[Huygens (spacecraft)|Huygens]].\n* Orbit transfer and [[Space rendezvous]]: [[ConeXpress]], [[German Aerospace Center|DLR]] DEOS, [[OHB SE]] Electra.\n* [[Sounding rocket]]: [[Sharp Edge Flight Experiment|SHEFEX]] II and III, [[Maser (rocket)|Maser]]11.\n* Mission Analysis: [[STE-QUEST]],\n\n==See also==\n* [[Trajectory optimization]]\n* [[General Mission Analysis Tool]]\n\n==External links==\n* [http://www.astos.de Astos Solutions website]\n* [https://web.archive.org/web/20081201120944/http://www.ifr.uni-stuttgart.de/index_en.html Website of the Institute of Flight Mechanics and Control, University of Stuttgart]\n\n{{DEFAULTSORT:Astos}}\n[[Category:Astronomy software]]\n[[Category:Mathematical software]]\n[[Category:Physics software]]\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "BARON",
      "url": "https://en.wikipedia.org/wiki/BARON",
      "text": "'''BARON''' is a computational system for solving non-convex [[optimization]] problems to [[Global optimization|global optimality]].  Purely continuous, purely integer, and mixed-integer nonlinear problems can be solved with the software.  BARON is available under the [[AIMMS]], [[AMPL]], and [[General Algebraic Modeling System|GAMS]] [[modeling language]]s on a variety of platforms.  The GAMS/BARON solver is also available on the NEOS Server.<ref>[http://www.neos-server.org/neos/solvers/index.html \"BARON on the NEOS Server\"]</ref>\n\nThe development of the BARON [[algorithm]]s and [[software]] has been recognized by the 2004 [https://web.archive.org/web/20101020144637/http://www.informs.org/Community/ICS/Prizes/ICS-Prize INFORMS Computing Society Prize] and the 2006 [https://web.archive.org/web/20110521175811/http://www.mathprog.org/?nav=boh_2006 Beale-Orchard-Hays Prize] for excellence in [[Computational mathematics|computational mathematical programming]] from the [[Mathematical Optimization Society]].\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://www.gams.com/help/topic/gams.doc/solvers/baron/index.html GAMS/BARON manual]\n* [http://archimedes.cheme.cmu.edu/ Nick Sahinidis's Homepage]\n* [http://web.ics.purdue.edu/~mtawarma/ Mohit Tawarmalani's Homepage]\n\n\n{{Mathematical optimization software}}\n{{DEFAULTSORT:Baron}}\n[[Category:Numerical software]]\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "CasADi",
      "url": "https://en.wikipedia.org/wiki/CasADi",
      "text": "{{Infobox software\n| name                   = CasADi\n| title                  = \n| logo                   = <!-- [[File: ]] -->\n| logo caption           = \n| screenshot             = <!-- [[File: ]] -->\n| caption                = \n| collapsible            = \n| author                 = \n| developer              = Optimization in Engineering Center (OPTEC),<ref>{{cite web \n| url=http://www.kuleuven.be/optec/\n| title = Optimization in Engineering Center (OPTEC)}}</ref> [[Katholieke Universiteit Leuven|K.U. Leuven]] \n| released               = <!-- {{Start date|YYYY|MM|DD|df=yes/no}} -->\n| discontinued           = \n| latest release version = <!-- {{Start date|YYYY|MM|DD|df=yes/no}} -->\n\n| latest release date    = {{Start date and age|2012|02|04|df=yes/no}}\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| programming language   = [[C++]]. Interfaces to [[Python (programming language)|Python]] and [[Octave]]\n| operating system       = [[Linux]], [[Microsoft Windows|Windows]] and [[Mac OS X]]\n| platform               = \n| size                   = \n| language               = \n| status                 = \n| genre                  = Automatic differentiation and mathematical optimization\n| license                = [[GNU Lesser General Public License]] ([[free software]])\n| alexa                  = \n| website                = {{URL|http://www.casadi.org}}\n}}\n'''CasADi''' is a free and open source symbolic framework for [[automatic differentiation]] and [[optimal control]].<ref>Joel Andersson, Johan √Ökesson, Moritz Diehl: \"CasADi - A symbolic package for automatic differentiation and optimal control\". Recent Advances in Algorithmic Differentiation. 2012.</ref>\n\n==See also==\n*[[Automatic differentiation]]\n*[[JModelica.org]]\n\n==References==\n{{Reflist}}\n\n[[Category:Mathematical optimization software]]\n[[Category:Free software programmed in Python]]"
    },
    {
      "title": "COIN-OR",
      "url": "https://en.wikipedia.org/wiki/COIN-OR",
      "text": "{{third-party|date=September 2018}}\n{{Infobox Organization\n| name         = COIN-OR\n| image        = COIN_OR_LOGO.png\n| image_border = \n| size         = \n| caption      = \n| formation    = \n| type         = \n| headquarters = \n| location     = \n| membership   = \n| language     = \n| leader_title = \n| leader_name  = \n| key_people   = \n| num_staff    = \n| budget       = \n| website      = {{URL|www.coin-or.org}}\n}}\n\n'''Computational Infrastructure for Operations Research''' ('''COIN-OR'''), is a project that aims to \"create for mathematical [[software]] what the open literature is for mathematical [[theory]].\"  The open literature (e.g., a research journal) provides the [[operations research]] (OR) community with a peer-review process and an archive. Papers in operations research journals on mathematical theory often contain supporting numerical results from computational studies. The software implementations, models, and data used to produce the numerical results are typically not published. The status quo impeded researchers needing to reproduce computational results, make fair comparisons, and extend the state of the art.\n\nThe success of [[Linux]], [[Apache HTTP Server|Apache]], and other projects popularized the [[open-source model]] of software development and distribution. A group at [[IBM]] Research proposed open source as an analogous yet viable means to ''publish'' software, models, and data. COIN-OR was conceived as an initiative to promote open source in the computational operations research community and to provide the on-line resources and hosting services required to enable others to run their own [[open-source software]] projects.\n\nThe COIN-OR website was launched as an experiment in 2000, in conjunction with 17th International Symposium on Math Programming in Atlanta, Georgia.  In 2007, COIN-OR had 25 application projects,<ref>[http://www.coin-or.org/coin-or-foundation/annual_report2007.pdf COIN-OR Annual Report, 2007]</ref> including tools for [[linear programming]] (e.g., [[#CLP|COIN-OR CLP]]), [[nonlinear programming]] (e.g., [[IPOPT]]), [[integer programming]] (e.g., CBC, Bcp and [[#SYMPHONY|COIN-OR SYMPHONY]]), [[algebraic modeling language]]s (e.g., [[Coopr]]) and more. By 2011, this had grown to 48 projects.<ref>[http://www.coin-or.org/coin-or-foundation/annual_report2011.pdf COIN-OR Annual Report, 2011]</ref> COIN-OR is hosted by the Institute for Operations Research and the Management Sciences, [[INFORMS]], and run by the educational, non-profit COIN-OR Foundation.\n\n== Projects ==\n\n=== CLP ===\nCOIN-OR LP (CLP or Clp) is an open-source [[linear programming]] [[solver]] written in [[C++]]. It is published under the [[Common Public License]] so it can be used in [[proprietary software]] with none of the restrictions of the [[GNU General Public License]].  CLP is primarily meant to be used as a callable library, although a stand-alone executable version can be built.  It is designed to be as reliable as any commercial solver (if a bit slower) and to be able to tackle very large problems.\n\nCLP is designed to solve linear programming problems such as :\n\n:: minimize <math>c_1 x_1 + c_2 x_2\\,</math>\n* subject to '''problem constraints''' of the following form\n::  <math>a_{11} x_1 + a_{12} x_2 \\le b_1</math>\n::  <math>a_{21} x_1 + a_{22} x_2 \\le b_2</math>\n::  <math>a_{31} x_1 + a_{32} x_2 \\le b_3</math>\n* and '''non-negative variables'''\n::  <math>x_1 \\ge 0</math>\n::  <math>x_2 \\ge 0</math>\n\nwith up to millions of variables and/or constraints.  Its main algorithm is the [[simplex algorithm]].\n\nCLP is used in other COIN-OR projects such as [[COIN-OR#SYMPHONY|SYMPHONY]], Branch Cut and Price (BCP), COIN-OR Branch and Cut ([[COIN-OR#CBC|CBC]]), and others.\n\n=== CBC ===\nCOIN-OR [[branch and cut]] (CBC or Cbc) is an open-source [[Linear programming#Integer unknowns|mixed integer programming]] solver written in [[C++]]. It can be used as both a stand-alone executable and as a callable library (through ''A Mathematical Programming Language'' ([[AMPL]]) [natively], ''[[General Algebraic Modeling System]]'' (GAMS) [using the links provided by the ''COIN-OR Optimization Services'' (OS)  and ''GAMSlinks'' projects], [[Mathematical Programming Language|MPL]] [through the ''CoinMP'' project], [[AIMMS]] [through the ''AIMMSlinks'' project], or [[COIN-OR#PuLP|PuLP]]).\n\n=== SYMPHONY ===\n<!-- SYMPHONY is a program for solving a class of mathematical problems called [[integer programming]] (IP) problems and its variants. A [[linear programming]] problem is an [[optimization (mathematics)|optimization]] problem in which we want to maximize or minimize a [[linear]] objective function over a set of [[linear]] constraints. A Pure Integer Programming problem is a linear programming problem in which all the variables are allowed to assume only [[integer]] values. A Mixed Integer Programming (MIP) problem is similar to a Pure IP Problem, but only some of the variables are constrained to be integers. Other variables can assume non-integral values.  MIPs are useful in modelling a lot of real life problems in logistics, scheduling, production planning, finance and management sciences. They are also extensively used in theoretical research like combinatorics, statistics, physics and computational biology. MIPs are therefore, an important tool in the field of OR, which is, roughly, the analysis and optimization of [[business]] and other decisions using [[mathematics]]. -->\n\nSingle- or [[Parallel computing|multi-process]] [[optimization (mathematics)|optimization]] over [[network theory|networks]] (SYMPHONY) is an open source [[branch and cut]] framework for solving  [[Linear programming|mixed integer programs]] (MIPs) over heterogeneous networks.<ref>[http://projects.coin-or.org/SYMPHONY SYMPHONY]</ref> It can use [[#CLP|CLP]], [[CPLEX]], XPRESS or other [[linear programming]] solvers to solve the underlying linear programs.\n\nSYMPHONY is a callable library which implements both sequential and parallel versions of branch, cut and price to solve MILPs. A branch, cut and price algorithm is similar to a [[branch and bound]] algorithm but additionally includes [[cutting-plane method]]s and pricing algorithms. The user of the library can customize the algorithm in any number of ways by supplying application-specific subroutines for reading in custom data files, generating application-specific cutting planes, or applying custom branching rules, resulting in a customized branch and cut algorithm. Most components of the algorithm, e.g., search tree management, management of linear programming solution, cut pool management, and communication management, are internal to the library and need not be touched by the user. The executables can be built in any number of configurations ranging from completely sequential to fully parallel with independently functioning cut generators, cut pools, and LP solvers. The distributed version currently runs in any environment supported by the [[PVM]] message passing protocol. The same source code can also be compiled for shared-memory architectures using any [[OpenMP]] compliant compiler.\n\nSYMPHONY reads [[MPS (format)|MPS]] (through the COIN-OR MPS reader) and [[GNU Linear Programming Kit|GNU MathProg]] files. SYMPHONY does not have an LP-Solver of its own, but can be used with solvers like Clp, Cplex, Xpress through the Osi-interface. Cuts are generated using COIN's cut generation library: CGL. SYMPHONY also has structure specific implementations for problems like the [[traveling salesman problem]], [[vehicle routing problem]], [[set partitioning problem]], [[mixed postman problem]], etc. SYMPHONY also has an interactive shell where the user can enter commands to execute and control the program.\n\n=== PuLP ===\nPuLP is an LP/IP modeler written in [[Python (programming language)|Python]].<ref>[http://projects.coin-or.org/PuLP PuLP]</ref> It can generate [[MPS (format)|MPS]] or LP files and call [[GNU Linear Programming Kit|GLPK]], [[COIN-OR#CLP|CLP]]/[[COIN-OR#CBC|CBC]], [[CPLEX]], and [[Gurobi]] to solve linear problems. PuLP is the default optimization tool in [[SolverStudio]] for [[Microsoft Excel|Excel]].\n\n=== SMI ===\nSMI is a [[stochastic programming]] modeler and solver written in C++.<ref>{{Cite web |url=http://www.coin-or.org/projects/Smi.xml |title=SMI |access-date=2014-01-03 |archive-url=https://web.archive.org/web/20141015160242/http://www.coin-or.org/projects/Smi.xml |archive-date=2014-10-15 |dead-url=yes }}</ref> It can read Stochastic MPS and offers direct interfaces for constructing stochastic programs.  It generates the deterministic equivalent linear program, solves it, and provides interfaces to access the scenario solutions.\n\n== See also ==\n* COIN-OR solvers are available in the [[AIMMS]], [[AMPL]] and [[General Algebraic Modeling System|GAMS]] modeling systems, and in the [[FortSP]] solver. They can also be used from within [[Microsoft Excel|Excel]] via the [[OpenSolver]] and [[SolverStudio]] add-ins.\n\n== References ==\n<references/>\n\n== Further reading ==\n* J.T. Linderoth and T.K. Ralphs: ''[http://coral.ie.lehigh.edu/pubs/files/jtl3_noncomm.pdf Noncommercial Software for Mixed-Integer Linear Programming]''. In: ''Integer Programming: Theory and Practice'', John Karlof (ed.), CRC Press Operations Research Series, 2005, 253-303. (Working paper version)\n* T. Ralphs: ''[https://coral.ie.lehigh.edu/~ted/files/talks/COIN-OptimizationDays13.pdf An Introduction to the COIN-OR Optimization Suite: Open Source Tools for Building and Solving Optimization Models]''. Optimization Days, Montreal, May 7, 2013. (Presentation slides)\n\n== External links ==\n* {{Official website|coin-or.org}} COIN-OR, Computational Infrastructure for Operations Research\n\n{{Mathematical optimization software}}\n\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "Couenne",
      "url": "https://en.wikipedia.org/wiki/Couenne",
      "text": "{{Infobox software\n| name                   = Couenne\n| latest release version = 0.5.2\n| programming language   = [[C++]]\n| operating system       = [[Cross-platform]]\n| website                = {{URL|https://projects.coin-or.org/Couenne}}\n}}\n\n'''Convex Over and Under ENvelopes  for Nonlinear Estimation''' ('''Couenne''') is an [[open-source software|open-source]] library for solving [[global optimization]] problems, also termed mixed integer nonlinear optimization problems.<ref>P. Belotti, C. Kirches, S. Leyffer, J. Linderoth, J. Luedtke and A. Mahajan (2013). Mixed-integer nonlinear optimization. Acta Numerica, 22, pp 1-131. doi:10.1017/S0962492913000032. http://journals.cambridge.org/abstract_S0962492913000032</ref> A global optimization problem requires to minimize a [[Function (mathematics)|function]], called [[objective function]], subject to a set of constraints. Both the objective function and the constraints might be nonlinear and nonconvex. For solving these problems, Couenne uses a reformulation procedure<ref>M. Tawarmalani, N.V. Sahinidis. Convexification and global optimization in continuous and mixed-integer nonlinear programming: theory, algorithms, software, and applications. Vol. 65. Springer Science & Business Media, 2002.</ref> and provides a [[linear programming]] approximation of any nonconvex optimization problem.<ref>P. Belotti, J. Lee, L. Liberti, F. Margot, & A. W√§chter (2009), Branching and bounds tightening techniques for non-convex MINLP. Optimization Methods & Software, 24(4-5), 597-634.</ref>\n\nCouenne is an implementation of a [[branch-and-bound]] where every subproblem is solved by constructing a [[linear programming]] relaxation to obtain a lower bound. Branching may occur at both continuous and integer variables, which is necessary in global optimization problems. It requires the input to be specified in A Mathematical Programming Language ([[AMPL]]) [[nl (format)|.nl format]], so as to be used from AMPL, and writes as an output a file .sol containing the best solution found until that moment (if the optimization is interrupted) or the global optimum if it completes without interruption.\n\nThe development of Couenne began in 2006 within a collaboration between [[IBM]] and [[Carnegie Mellon University]]. It is [[open-source software]] and is currently released under the [[Eclipse Public License]] v1.0.\n\nThe source code is available for download in the Computational Infrastructure for Operations Research [[COIN-OR]] repository and on Github. Couenne uses other packages both in COIN-OR ([[COIN-OR#CBC|CBC]], [[COIN-OR CLP|CLP]], COIN-OR OSI, COIN-OR Bonmin, COIN-OR Cgl, Interior Point OPTimizer ([[IPOPT]])) and outside ([[Lapack|LAPACK]], [[Basic Linear Algebra Subprograms]] (BLAS), MUltifrontal Massively Parallel sparse direct Solver ([[MUMPS (software)|MUMPS]]), Nauty, Solving Constraint Integer Programs ([[SCIP (optimization software)|SCIP]]), SoPlex).\n\n==See also==\n* [[BARON]] ‚Äì a commercial solver for MINLP developed by Nick Sahinidis and others\n* [[LINDO]] ‚Äì a suite comprising LindoGlobal for solving global optimization problems\n* [[SCIP (optimization software)|SCIP]] ‚Äì a freely available solver for MILP, MIQCQP, and [[global optimization]] problems\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* {{Official website|https://www.coin-or.org/Couenne/}}\n* [https://projects.coin-or.org/Couenne/browser/trunk Source code (trunk)]\n* [https://web.archive.org/web/20131029190415/https://projects.coin-or.org/Couenne Project page]\n* [https://projects.coin-or.org/Couenne/export/974/trunk/Couenne/doc/couenne-user-manual.pdf User manual]\n\n{{Mathematical optimization software}}\n\n[[Category:Mathematical optimization software]]\n[[Category:Numerical software]]"
    },
    {
      "title": "CPLEX",
      "url": "https://en.wikipedia.org/wiki/CPLEX",
      "text": "{{primary sources|date=May 2016}}\n{{Infobox Software\n| name                   = CPLEX \n| developer              = IBM\n| latest_release_version = 12.9.0<ref>{{cite web|url=https://developer.ibm.com/docloud/blog/2019/03/12/cplex-optimization-studio-12-9-is-available/|title=CPLEX Optimization Studio 12.9 is available\n}}</ref>\n| genre                  = [[List of numerical analysis software|Technical computing]]\n| license                = [[Proprietary software|Proprietary]]\n| website                = {{URL| 1=http://www.ibm.com/analytics/cplex-optimizer}}\n}}\n\n'''IBM ILOG CPLEX Optimization Studio''' (often informally referred to simply as '''CPLEX''') is an [[Optimization (mathematics)|optimization]] software package.  In 2004, the work on CPLEX earned the first [[INFORMS]] Impact Prize.\n\n== History ==\nThe CPLEX Optimizer was named for the [[simplex method]] as implemented in the [[C programming language]], although today it also supports other types of [[mathematical optimization]] and offers interfaces other than C.  It was originally developed by [[Robert E. Bixby]] and sold commercially from 1988 by CPLEX Optimization Inc. This was acquired by [[ILOG]] in 1997 and ILOG was subsequently acquired by IBM in January 2009.<ref>{{citation | url = https://www-304.ibm.com/jct03002c/press/us/en/pressrelease/26403.wss | title = IBM Completes Acquisition of ILOG | date = 6 Jan 2009 | access-date = 13 January 2011 | archive-url = https://web.archive.org/web/20120927094255/https://www-304.ibm.com/jct03002c/press/us/en/pressrelease/26403.wss | archive-date = 27 September 2012 | dead-url = yes }}</ref>  CPLEX continues to be actively developed by IBM.\n\n== Features ==\nThe IBM ILOG CPLEX Optimizer solves [[integer programming]] problems, very large<ref>{{citation | title = Recent Benchmarks of Optimization Software | first = H. D. | last = Mittelmann | date = 10 July 2007 | location = EURO XXII Prague, Czech Republic | publisher = Dept of Math and Stats Arizona State University|work=22nd European Conference on Operational Research}}</ref> [[linear programming]] problems using either primal or dual variants of the [[simplex method]] or the barrier [[interior point method]], convex and non-convex [[quadratic programming]] problems, and convex quadratically constrained problems (solved via [[second-order cone programming]], or SOCP).\n\nThe CPLEX Optimizer has a modeling layer called Concert that provides interfaces to the [[C++]], [[C Sharp (programming language)|C#]], and [[Java programming language|Java]] languages.  There is a [[Python programming language|Python]] language interface based on the C interface.  Additionally, connectors to [[Microsoft Excel]] and [[MATLAB]] are provided.  Finally, a stand-alone Interactive Optimizer executable is provided for debugging and other purposes.\n\nThe CPLEX Optimizer is accessible through independent modeling systems such as [[AIMMS]], [[AMPL]], [[General Algebraic Modeling System|GAMS]], [[OptimJ]] and [[TOMLAB]]. In addition to that AMPL provides an interface to the CPLEX CP Optimizer.\n\nThe full IBM ILOG CPLEX Optimization Studio consists of the CPLEX Optimizer for mathematical programming, the CP Optimizer for constraint programming<ref name=\"CPOptimizer2018\">{{cite journal|vauthors=Laborie P, Rogerie J, Shaw P, Vilim P|date=2018|title=IBM ILOG CP optimizer for scheduling|journal=Constraints|volume=23|issue=2|pages=210‚Äì250|doi=10.1007/s10601-018-9281-x}}</ref>, the [[Optimization Programming Language]] (OPL), and a tightly integrated IDE.\n\n== See also ==\n* [[GLPK]]\n* [[Gurobi]]\n* [[SCIP (optimization software)|SCIP]]\n\n==References==\n{{reflist}}\n\n==External links==\n\n* {{Official website| 1=http://www.ibm.com/analytics/cplex-optimizer |name=IBM ILOG CPLEX Optimizer home page}}\n* {{Official website| 1=http://www.ibm.com/analytics/cplex-cp-optimizer |name=IBM ILOG CPLEX CP Optimizer home page}}\n* {{Official website| 1=http://www.ibm.com/products/ilog-cplex-optimization-studio/ |name=Optimization Programming Language (OPL)}}\n* [http://www.ibm.com/software/websphere/products/optimization/cplex-studio-preview-edition/ Free preview edition of IBM ILOG CPLEX Optimization Studio]\n* [http://ibm.biz/COS_Forums IBM ILOG Optimization Forums]\n\n{{Mathematical optimization software}}\n\n[[Category:Mathematical optimization software]]\n[[Category:IBM software]]"
    },
    {
      "title": "CUTEr",
      "url": "https://en.wikipedia.org/wiki/CUTEr",
      "text": "CUTEr ('''C'''onstrained and '''U'''nconstrained '''T'''esting '''E'''nvironment, '''r'''evisited) is an [[open source]] testing environment for [[Optimization (mathematics)|optimization]] and [[linear algebra]] [[Solver (computer science)|solver]]s. CUTEr provides a collection of test problems along with a set of tools to help developers design, compare, and improve new and existing test problem solvers.\n\nCUTEr is the successor of the original Constrained and Unconstrained Testing Environment (CUTE) of Bongartz, Conn, Gould and Toint.<ref>I. Bongartz, A.R. Conn, N. I. M. Gould and Ph. L. Toint, ''CUTE: Constrained and Unconstrained Testing Environment'', ACM Transactions on Mathematical Software, 21:1, pp. 123-160, 1995.</ref> It provides support for a larger number of platforms and operating systems as well as a more convenient optimization toolbox.\n\nThe test problems provided in CUTEr are written in Standard Input Format (SIF).<ref>[http://www.numerical.rl.ac.uk/lancelot/sif/sifhtml.html Standard Input Format (SIF)]</ref> A decoder to convert from this format into well-defined subroutines and data files is available as a separate package. Once translated, these files may be manipulated to provide tools suitable for testing optimization packages. Ready-to-use interfaces to existing packages, such as [[IPOPT]], [http://www.sbsi-sol-optimize.com/asp/sol_product_minos.htm MINOS], [http://www.sbsi-sol-optimize.com/asp/sol_product_snopt.htm SNOPT], [http://neos.mcs.anl.gov/neos/solvers/nco:filter/AMPL.html filterSQP], [http://www.ziena.com Knitro] and more are provided. The problems in the CUTE subset are also available in the [[AMPL]] format. <ref>http://orfe.princeton.edu/~rvdb/ampl/nlmodels/cute/</ref>\n\nMore than 1000 problems are available in the collection, including problems in:\n*[[linear programming]],\n*convex and nonconvex [[quadratic programming]],\n*linear and nonlinear [[least squares]], and\n*more general convex and nonconvex large-scale and sparse equality and inequality-constrained [[nonlinear programming]].\n\nOver time, the CUTEr test set has become the ''de facto'' standard benchmark for research and production-level optimization solvers, and is used and cited in numerous published research articles.{{Citation needed|date=November 2010}}\n\nThe SIF is a [[subset|superset]] of the original [[MPS (format)|MPS format]] for [[linear programming]] and of its extension QPS for [[quadratic programming]]. Therefore, access to problem collections such as the [http://www.netlib.org Netlib] linear programs and the [http://www.doc.ic.ac.uk/~im/ Maros and Meszaros] convex quadratic programs is possible. Moreover, the collection covers the Argonne test set,<ref>J. J. Mor√©, B. S. Garbow and K. E. Hillstr√∂m, ''Testing Unconstrained Optimization Software'', ACM Transactions on Mathematical Software, 7:1, pp 17-41, 1981.</ref> the Hock and Schittkowski collection,<ref>W. Hock and K. Schittkowski, Test Examples for Nonlinear Programming Codes, Lecture Notes in Economics and Mathematical Systems, Vol. 187, Springer 1981.</ref> the Dembo network problems, the Gould QPs, and others.\n\nCUTEr is available on a variety of [[UNIX]] platforms, including [[Linux]] and [[Mac OS X]] and is designed to be accessible and easily manageable on heterogeneous networks.\n\n== References ==\n{{Reflist}}\n;Notes\n*N. I. M. Gould, D. Orban and Ph. L. Toint, ''CUTEr (and SifDec): a Constrained and Unconstrained Testing Environment, revisited'', ACM Transactions on Mathematical Software, 29:4, pp 373‚Äì394, 2003.\n\n== External links ==\n* The [http://cuter.rl.ac.uk/ official CUTEr website]\n*[http://cuter.rl.ac.uk/cuter-www/Doc/general/node55.html CUTEr license]\n\n{{DEFAULTSORT:Cuter}}\n[[Category:Numerical software]]\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "DIDO (software)",
      "url": "https://en.wikipedia.org/wiki/DIDO_%28software%29",
      "text": "'''DIDO''' ({{IPAc-en|Àà|d|a…™|d|o ä}} {{respell|DY|doh}}) is a software product for solving general-purpose [[optimal control]] problems.<ref name=\"R1\">Ross, I. M. ''A Primer on Pontryagin's Principle in Optimal Control'', Second Edition, Collegiate Publishers, San Francisco, 2015.</ref><ref name=\"RK\" /><ref name=\":4\">Eren, H., \"Optimal Control and the Software,\" ''Measurements, Instrumentation, and Sensors Handbook'', Second Edition, CRC Press, 2014, pp.92-1-16.</ref><ref name=\"RD1\">{{cite journal | last1 = Ross | first1 = I. M. | authorlink = I. Michael Ross | last2 = D'Souza | first2 = C. N. | year = 2005 | title = A Hybrid Optimal Control Framework for Mission Planning | url = | journal = Journal of Guidance, Control and Dynamics | volume = 28 | issue = 4| pages = 686‚Äì697 | doi=10.2514/1.8285}}</ref> It is widely used in academia,<ref name=\":3\" /><ref name=\":1\" /><ref name=\"hawkins-mit\" /> industry,<ref name=\"RK\" /><ref name=\"Gong+\" /> and NASA.<ref name=\"NASA Fact Sheet\" /><ref name=\"Kang2\" /><ref name=\"LR\" /> Hailed as a breakthrough software,<ref name=\"Honegger1\" /><ref name=\":5\">{{Cite book|title=Modeling Languages in Mathematical Optimization|last=Kallrath|first=Josef|publisher=Kluwer Academic Publishers|year=2004|isbn=|location=Dordrecht, The Netherlands|pages=379‚Äì403|quote=|via=}}</ref> DIDO is based on the [[pseudospectral optimal control]] theory of [[I. Michael Ross|Ross]] and [[Fariba Fahroo|Fahroo]].<ref name=\"RF1\">{{cite journal | last1 = Ross | first1 = I. M. | last2 = Fahroo | first2 = F. | year = 2004 | title = Pseudospectral Knotting Methods for Solving Optimal Control Problems | url = | journal = Journal of Guidance, Control and Dynamics | volume = 27 | issue = 3| pages = 397‚Äì405 | doi=10.2514/1.3426}}</ref>\n\n==Usage==\nDIDO utilizes trademarked expressions and objects<ref name=\"R1\" /> that facilitate a user to quickly formulate and solve [[optimal control]] problems.<ref name=\"hawkins-mit\">A. M. Hawkins, ''Constrained Trajectory Optimization of a Soft Lunar Landing From a Parking Orbit,'' S.M. Thesis, Dept. of Aeronautics and Astronautics, Massachusetts Institute of Technology, 2005. http://dspace.mit.edu/handle/1721.1/32431\n</ref><ref name=\"rea-mit\" /><ref name=\"JR1\" /><ref>{{cite journal|last=Infeld|first=S. I.|year=2005|title=Optimization of Mission Design for Constrained Libration Point Space Missions|url=http://www.stanford.edu/group/SOL/dissertations/samantha-thesis.pdf|publisher=Stanford University}}</ref> Rapidity in formulation is achieved through a set of DIDO expressions which are based on variables commonly used in optimal control theory.<ref name=\"R1\" /> For example, the '''state''', '''control''' and '''time''' variables are formatted as:<ref name=\"R1\" />\n* primal.'''states''',\n* primal.'''controls''', and \n* primal.'''time'''\nThe entire problem is codified using the key words, '''cost''', '''dynamics''', '''events''' and '''path''':<ref name=\"R1\" />\n* problem.'''cost'''\n* problem.'''dynamics'''\n* problem.'''events''', and\n* problem.'''path'''\nA user runs DIDO using the one-line command:\n\n<code>[cost, primal, dual] = dido(problem, algorithm)</code>,\n\nwhere the object defined by <code>algorithm</code> allows a user to choose various options.  In addition to the cost value and the primal solution, DIDO automatically outputs all the dual variables that are necessary to verify and validate a computational solution.<ref name=\"R1\" /> The output <code>dual</code> is computed by an application of the [[covector mapping principle]].\n\n==Theory==\nDIDO implements a spectral algorithm<ref name=\"RF1\" /><ref name=\"GFR1\" /> based on [[pseudospectral optimal control]] theory founded by [[I. Michael Ross|Ross]] and his associates.<ref name=\"RK\">{{cite journal | last1 = Ross | first1 = I. M. | last2 = Karpenko | first2 = M. | year = 2012 | title = A Review of Pseudospectral Optimal Control: From Theory to Flight | url = http://www.sciencedirect.com/science/article/pii/S1367578812000375 | journal = Annual Reviews in Control | volume = 36 | issue = 2| pages = 182‚Äì197 | doi=10.1016/j.arcontrol.2012.09.002}}</ref> The [[covector mapping principle]] of [[I. Michael Ross|Ross]] and [[Fariba Fahroo|Fahroo]] eliminates the curse of sensitivity<ref name=\"R1\" /> associated in solving for the [[costate equations|costates]] in [[optimal control]] problems. [[DIDO (optimal control)|DIDO]] generates spectrally accurate solutions <ref name=\"GFR1\">{{cite journal | last1 = Gong | first1 = Q. | authorlink2 = Fariba Fahroo | authorlink3 = I. Michael Ross | last2 = Fahroo | first2 = F. | last3 = Ross | first3 = I. M. | year = 2008 | title = A Spectral Algorithm for Pseudospectral Methods in Optimal Control | url = | journal = Journal of Guidance, Control and Dynamics | volume = 31 | issue = 3| pages = 460‚Äì471 | doi=10.2514/1.32908}}</ref>  whose extremality can be verified using [[Pontryagin's Minimum Principle]].  Because no knowledge of pseudospectral methods is necessary to use it, DIDO is often used<ref name=\":1\" /><ref name=\"hawkins-mit\" /><ref name=\"Gong+\" /><ref name=\":0\" /> as a fundamental mathematical tool for solving [[optimal control]] problems. That is, a solution obtained from DIDO is treated as a candidate solution for the application of [[Pontryagin's minimum principle]] as a [[Necessary and sufficient conditions|necessary condition]] for optimality.\n\n==Applications==\nDIDO is used world wide in academia, industry and government laboratories.<ref name=\"Gong+\">Q. Gong, W. Kang, N. Bedrossian, F. Fahroo, P. Sekhavat and K. Bollino, Pseudospectral Optimal Control for Military and Industrial Applications, ''46th IEEE Conference on Decision and Control,'' New Orleans, LA, pp. 4128-4142, Dec. 2007.</ref>  Thanks to [[NASA]], DIDO was flight-proven in 2006.<ref name=\"RK\" />  On November 5, 2006, [[NASA]] used DIDO to maneuver the [[International Space Station]] to perform the [[zero-propellant maneuver]].\n\nSince this flight demonstration, DIDO was used for the International Space Station and other NASA spacecraft.<ref name=\"LR\">\nL. Keesey, \"TRACE Spacecraft's New Slewing Procedure.\" NASA's Goddard Space Flight Center. National Aeronautics and Space Administration. Dec. 20, 2010. (Sept. 11, 2011) http://www.nasa.gov/mission_pages/sunearth/news/trace-slew.html.</ref>  It is also used in other industries.<ref name=\"R1\"/><ref name=\"Gong+\"/><ref name=\":0\" /><ref name=\":2\" />\n\n== MATLAB optimal control toolbox ==\nDIDO is also available as a [[MATLAB]] \"toolbox\" product.<ref>{{Cite web |title= DIDO: Optimal control software |work= Promotional web page |publisher= Mathworks |url= https://www.mathworks.com/products/connections/product_detail/product_61633.html }}</ref>  It does not require the MATLAB [[Optimization Toolbox]] or any other third-party software like [[SNOPT]] or [[IPOPT]] or other [[nonlinear programming]] solvers.\n\nThe MATLAB/DIDO toolbox does not require a \"guess\" to run the algorithm.  This and other distinguishing features have made DIDO a popular tool to solve optimal control problems.<ref name=\":4\" /><ref name=\":1\" /><ref name=\":5\" />\n\nThe MATLAB optimal control toolbox has been used to solve problems in aerospace,<ref name=\"Kang2\" /> [[robotics]] and [[search theory]].\n\n==History==\nThe  optimal control toolbox is named after [[Dido (Queen of Carthage)|Dido]], the legendary founder and first queen of [[Carthage]] who is famous in mathematics for her remarkable solution to a constrained [[optimal control]] [[Isoperimetric inequality|problem]] even before the invention of [[calculus]]. Invented by [[I. Michael Ross|Ross]], DIDO was first produced in 2001.<ref name=\"R1\" /><ref name=\":3\" /><ref name=\"rea-mit\">J. R. Rea, ''A Legendre Pseudospectral Method for Rapid Optimization of Launch Vehicle Trajectories,'' S.M. Thesis, Dept. of Aeronautics and Astronautics, Massachusetts Institute of Technology, 2001. http://dspace.mit.edu/handle/1721.1/8608</ref>  The software is widely cited<ref name=\":3\">{{Cite journal|last=Rao|first=A. V.|year=2014|title=Trajectory Optimization: A Survey|url=|journal=Optimization and Optimal Control of Automotive Systems|volume=LNCIS 455|pages=3‚Äì21|via=|doi=10.1007/978-3-319-05371-4_1|series=Lecture Notes in Control and Information Sciences|isbn=978-3-319-05370-7}}</ref><ref name=\":1\">{{Cite journal|last=Conway|first=B. A.|year=2012|title=A Survey of Methods Available for the Numerical Optimization of Continuous Dynamical Systems|url=|journal=Journal of Optimization Theory and Applications|volume=152|issue=2|pages=271‚Äì306|via=|doi=10.1007/s10957-011-9918-z}}</ref><ref name=\":0\">D. Delahaye, S. Puechmorel, P. Tsiotras, and E. Feron, \"Mathematical Models for Aircraft Trajectory Design : A Survey\" Lecture notes in Electrical Engineering, 2014, Lecture Notes in Electrical Engineering, 290 (Part V), pp 205-247</ref><ref name=\":2\">S. E. Li, K. Deng, X. Zang, and Q. Zhang, \"Pseudospectral Optimal Control of Constrained Nonlinear Systems,\" Ch 8, in Automotive Air Conditioning: Optimization, Control and Diagnosis, edited by Q. Zhang, S. E. Li and K. Deng, Springer 2016, pp. 145-166.</ref> and has many firsts to its credit:<ref name=\"NASA Fact Sheet\">\nNational Aeronautics and Space Administration. \"Fact Sheet: International Space Station Zero-Propellant Maneuver (ZPM) Demonstration.\" June 10, 2011. (Sept. 13, 2011) http://www.nasa.gov/mission_pages/station/research/experiments/ZPM.html</ref>\n<ref name=\"Kang2\">W. Kang and N. Bedrossian, \"Pseudospectral Optimal Control Theory Makes Debut Flight, Saves nasa $1m in Under Three Hours,\" SIAM News, 40, 2007.</ref>\n<ref name=\"LR\"/>\n<ref name=\"Honegger1\">B. Honegger, \"NPS Professor's Software Breakthrough Allows Zero-Propellant Maneuvers in Space.\" Navy.mil. United States Navy. April 20, 2007. (Sept. 11, 2011) http://www.elissarglobal.com/wp-content/uploads/2011/07/Navy_News.pdf.</ref>\n<ref name =\"RF1\" />\n<ref name=\"JR1\">{{cite journal | last1 = Josselyn | first1 = S. | last2 = Ross | first2 = I. M. | year = 2003 | title = A Rapid Verification Method for the Trajectory Optimization of Reentry Vehicles | url = | journal = Journal of Guidance, Control and Dynamics | volume = 26 | issue = 3| pages = 505‚Äì508 | doi=10.2514/2.5074}}</ref>\n<ref name=\"FDN\">{{cite journal | last1 = Fahroo | first1 = F. | last2 = Doman | first2 = D. B. | last3 = Ngo | first3 = A. D. | year = 2003 | title = Modeling Issues in Footprint Generation of Resuable Launch Vehicles | url = | journal = Proceedings of the IEEE Aerospace Conference | volume = 6 | issue = | pages = 2791‚Äì2799 | doi=10.1109/aero.2003.1235205| isbn = 978-0-7803-7651-9 }}</ref>\n\n* First general-purpose object-oriented optimal control software\n* First general-purpose pseudospectral optimal control software\n* First flight-proven general-purpose optimal control software\n* First embedded general-purpose optimal control solver\n* First guess-free general-purpose optimal control solver\n\n==Versions==\nSeveral different versions of DIDO are available from Elissar Global.<ref>{{Cite web |url= http://www.ElissarGlobal.com/ |title= Elissar Global |work= web site }} distributes the software.</ref>\n\n==See also==\n\n*[[Bellman pseudospectral method]]\n*[[Chebyshev pseudospectral method]]\n*[[Covector mapping principle]]\n*[[Fariba Fahroo]]\n*[[Flat pseudospectral method]]s\n*[[I. Michael Ross]]\n*[[Legendre pseudospectral method]]\n*[[Ross‚ÄìFahroo lemma]]\n*[[Ross' œÄ lemma]]\n*[[Ross‚ÄìFahroo pseudospectral method]]s\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n\n* {{cite journal | last = Ross | first = I. Michael |author2=Fahroo, Fariba  | title = Legendre Pseudospectral Approximations of Optimal Control Problems| year = 2003 | publisher = Springer Verlag| url = http://www.elissarglobal.com/wp-content/uploads/2012/03/Legendre-Pseudospectral-Approximations-of-Optimal-Control-Problems.pdf}}\n* {{cite journal | last = Bollino| first = K. |author2=Lewis, L. R. |author3=Sekhavat, P. |author4=Ross, I. M. | title = Pseudospectral Optimal Control: A Clear Road for Autonomous Intelligent Path Planning | year = 2007 | publisher = AIAA| url = http://www.elissarglobal.com/wp-content/uploads/2012/04/Pseudospectral-Optimal-Control-A-Clear-Road-for-Autonomous-Intelligent-Path-Planning.pdf}}\n* {{cite journal | last = Kang| first = W. |author2=Ross, I. M. |author3=Gong, Q.  | title = Pseudospectral Optimal Control and Its Convergence Theorems | year = 2007 | publisher = Springer Berlin Heidelberg| url = http://www.springerlink.com/content/v1874282v4541275/}}\n* {{cite book | last = Ross| first = I. M. | title = A Primer on Pontryagin's Principle in Optimal Control | year = 2009 | publisher = Collegiate Publishers |isbn=978-0-9843571-0-9}}\n\n==External links==\n* [http://computer.howstuffworks.com/dido.htm How DIDO Works at How Stuff Works]\n* [https://web.archive.org/web/20081005205928/http://www.nasa.gov/mission_pages/station/science/experiments/ZPM.html  NASA News ]\n* [https://www.youtube.com/watch?v=faQeCI1IgoQ Pseudospectral optimal control: Part 1]\n* [https://www.youtube.com/watch?v=jRmJwQI_JZw Pseudospectral optimal control: Part 2]\n\n[[Category:Numerical software]]\n[[Category:Mathematical optimization software]]\n[[Category:Optimal control]]"
    },
    {
      "title": "EMSO simulator",
      "url": "https://en.wikipedia.org/wiki/EMSO_simulator",
      "text": "{{Infobox software\n|logo =\n|screenshot =\n|caption =\n|developer = the ALSOC Project\n|latest_release_version = 0.9.60\n|latest_release_date = {{Start date and age|2009|09|13}}\n|operating_system = [[Linux]], [[Microsoft Windows|Windows]]\n|programming language = [[C++]]\n|genre = process simulation\n|license = ALSOC Open source License\n|website = {{URL|www.enq.ufrgs.br/trac/alsoc/wiki/EMSO}}\n}}\n\n'''EMSO simulator''' is an equation-oriented process simulator with a graphical interface for modeling complex dynamic or steady-state processes.  It is CAPE-OPEN compliant.  EMSO stands for Environment for Modeling, Simulation, and Optimization.<ref>R. P. Soares and A. R. Secchi, ''EMSO: A new environment for modelling, simulation and optimisation'', Computer Aided Chemical Engineering, '''14''', (2003), 947-952. DOI: [https://dx.doi.org/10.1016/S1570-7946(03)80239-0 10.1016/S1570-7946(03)80239-0]</ref>  The ALSOC Project - a [[Portuguese language|Portuguese]] acronym for [[Free software|Free]] Environment for Simulation, Optimization and Control of Processes -, which is based at the [[Universidade Federal do Rio Grande do Sul|UFRGS]], develops, maintains and distributes this object-oriented software.  Pre-built models are available in the EMSO Modeling Library (EML).<ref>http://www.vrtech.com.br/rps/emso.html, EMSO page, Retrieved 7/4/2010</ref>  New models can be written in the EMSO modeling language or a user can embed models coded in [[C (programming language)|C]], [[C++]] or [[Fortran]] into the simulation environment.\n\n==See also==\n* [[AMPL]]\n* [[APMonitor]]\n* [[ASCEND]]\n* [[General Algebraic Modeling System]]\n* [[JModelica.org]]\n* [[MATLAB]]\n* [[Modelica]]\n* [[List of chemical process simulators]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://www.enq.ufrgs.br/alsoc ALSOC Project]\n* [http://chengineer.com/index.php/tag/emso Blog posts regarding EMSO]\n\n{{DEFAULTSORT:Emso Simulator}}\n[[Category:Simulation software]]\n[[Category:Computer-aided engineering software]]\n[[Category:Simulation programming languages]]\n[[Category:Mathematical optimization software]]\n[[Category:Object-oriented programming]]"
    },
    {
      "title": "FICO Xpress",
      "url": "https://en.wikipedia.org/wiki/FICO_Xpress",
      "text": "{{Infobox software\n| name                   = FICO Xpress\n| developer              = [[FICO]]\n| latest_release_version = 8.6<ref name=\"documentation\">{{cite web|url=https://www.fico.com/fico-xpress-optimization/docs/latest/overview.html|title=FICO Xpress Optimization|date=May 27, 2019}}</ref>\n| genre                  = [[Operations Research]], [[Optimization (mathematics)|Mathematical optimization]]\n| platform               = [[Cross-platform]]\n| license                = [[Proprietary software|Proprietary]]\n| website                = {{URL|https://www.fico.com/en/products/fico-xpress-optimization}}\n| released               = {{Start date and age|1983}}\n}}\nThe '''FICO Xpress''' optimizer is a commercial [[Optimization (mathematics)|optimization]] [[solver]] for [[linear programming]] (LP), [[mixed integer linear programming]] (MILP), convex [[quadratic programming]] (QP), convex [[quadratically constrained quadratic program]]ming (QCQP), [[second-order cone programming]] (SOCP) and their mixed integer counterparts.<ref name=\"parallel\">{{cite journal |last1=Berthold |first1=T. |last2=Farmer |first2=J. |last3=Heinz |first3=S. |last4=Perregaard |first4=M. |date=15 Jun 2017 |title=Parallelization of the FICO Xpress-Optimizer |journal=Optimization Methods and Software |volume= 33|issue= 3|pages=518‚Äì529 |doi=10.1080/10556788.2017.1333612}}</ref> Xpress includes a general purpose non-linear solver, Xpress NonLinear, including a successive linear programming algorithm (SLP, first-order method), and [[Artelys Knitro]] (second-order methods).\n\nXpress was originally developed by Dash Optimization, and was acquired by [[FICO]] in 2008.<ref>\n[http://www.wikinvest.com/stock/Fair,_Isaac_and_Company_(FICO)/Acquisition_Dash_Optimization_Limited \"Dash Optimization acquired by FICO\"] Jan 22, 2008.</ref>\nIts initial authors were Bob Daniel and Robert Ashford. The first version of Xpress could only solve LPs; support for MIPs was added in 1986.\nBeing released in 1983, Xpress was the first commercial LP and MIP solver running on [[Personal computer|PCs]].<ref>{{cite journal |last1=Ashford |first1=R. |date=Feb 2007 |title=Mixed integer programming: A historical perspective with Xpress-MP |journal=Annals of Operations Research |volume=149 |issue=1 |pages=5‚Äì17 |doi=10.1007/s10479-006-0092-x}}</ref>\nIn 1992, an Xpress version for parallel computing was published, which was extended to distributed computing five years later.<ref>{{cite book |last1=Laundy |first1=R. |date=1999 |title=Implementation of Parallel Branch-and-bound Algorithms in XPRESS-MP |journal=Operational Research in Industry |volume= |issue= |pages=25‚Äì41 |doi=10.1057/9780230372924_2|isbn=9780230372924}}</ref>\nXpress was the first MIP solver to cross the billion decision variable threshold by introducing 64-bit indexing in 2010.<ref>{{cite report |author=O. Bastert |date=2011 |title=FICO Xpress Optimization Suite |url=https://www.msi-jp.com/xpress/overview/20110921_MSI_Webinar_en.pdf |access-date=Jan 23, 2019}}</ref>\nSince 2014, Xpress features the first commercial implementation of a parallel dual [[simplex method]].<ref name=\"parallel\" />\n\n== Technology ==\nLinear and quadratic programs can be solved via the primal simplex method, the dual simplex method or the barrier [[interior point method]]. All mixed integer programming variants are solved by a combination of the [[branch and bound]] method and the [[cutting-plane method]]. Infeasible problems can be analyzed via the IIS ([[Irreducibility (mathematics)|irreducible]] infeasible subset) method. Xpress provides a built-in tuner for automatic tuning of control settings.\n<ref name=\"documentation\" />\n\nXpress includes its modelling language Xpress Mosel<ref name=\"Modelli\">{{cite book|first1=Christelle|last1=Gu√©ret|first2=Christian|last2=Prins|first3=Marc|last3=Sevaux|title=Applications of Optimization with Xpress-MP |year=2002|isbn= 9780954350307}}</ref> and the integrated development environment Xpress Workbench<ref>{{cite web|url=http://www.fico.com/en/products/fico-xpress-workbench|title=FICO Xpress Workbench|date=Nov 12, 2017}}</ref>.\nMosel includes [[distributed computing]] features to solve multiple scenarios of an optimization problem in parallel. Uncertainty in the input data can be handled via [[robust optimization]] methods.<ref>{{cite report |author=P. Belotti |date=2014 |title=Robust Optimization with Xpress |url=https://www.msi-jp.com/xpress/doc/Xpress-Rubost-WhitePaper.pdf |access-date=Oct 28, 2018}}</ref>\n\nXpress has a modeling module called BCL (Builder Component Library) that interfaces to the [[C (programming language)|C]], [[C++]], [[Java programming language|Java]] programming languages, and to the [[.NET framework]].<ref>[https://www.fico.com/fico-xpress-optimization/docs/latest/bcl/dhtml/ \"BCL Reference Manual\"] Nov 13, 2018.</ref> Independent of BCL, there are [[Python programming language|Python]] and [[MATLAB]] interfaces. Next to Mosel, Xpress connects to other standard modeling languages, such as [[AIMMS]], [[AMPL]], and [[General Algebraic Modeling System|GAMS]].\n\nThe FICO Xpress Executor<ref>[https://www.fico.com/en/products/fico-xpress-executor \"FICO Xpress Executor\"] Nov 13, 2018.</ref> executes and deploys Mosel models, using [[SOAP]] or [[Representational state transfer | REST]] interfaces. It can be used from external applications or from the [https://www.fico.com/en/products/fico-decision-management-platform FICO Decision Management Platform].\n\n== References ==\n{{reflist}}\n\n== External links ==\n* [http://plato.asu.edu/bench.html Benchmark for optimization software]\n\n{{Mathematical optimization software}}\n\n[[Category:Numerical software]]\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "FortMP",
      "url": "https://en.wikipedia.org/wiki/FortMP",
      "text": "{{Infobox Software\n| name                   = FortMP\n| developer              = OptiRisk Systems\n| latest_release_version = 3.2\n| platform               = [[Cross-platform]]\n| genre                  = [[Operations Research|Operations Research Tool]], [[List of numerical analysis software|Numerical Software]]\n| status                 = Active\n| license                = [[Proprietary software|Proprietary]]\n| website                = [http://www.optirisk-systems.com/products_fortmp.asp FortMP home page]\n}}\n\n'''FortMP''' is a software package for solving large-scale [[Optimization (mathematics)|optimization]] problems. It solves [[linear programming]] problems, [[quadratic programming]] problems and [[mixed integer programming]] problems (both linear and quadratic). Its robustness has been explored and published in the [[Mathematical Programming Society#Journals and prizes|Mathematical Programming]] journal.<ref>\n{{cite journal\n  | last = Neumaier\n  | first = Arnold\n  |author2=Oleg Shcherbina\n  | title = Safe bounds in linear and mixed-integer linear programming\n  | journal = Mathematical Programming\n  | volume = 99\n  | issue = 2\n  | pages = 283‚Äì296\n  | date = March 2004\n  | issn = 0025-5610\n  | doi = 10.1007/s10107-003-0433-3| citeseerx = 10.1.1.373.508\n  }}\n</ref>\nFortMP is available as a standalone executable that accepts input in [[MPS (format)|MPS format]] and as a library with interfaces in [[C programming language|C]] and [[Fortran]]. It is also supported in the [[AMPL]] modeling system.\n\nThe main algorithms implemented in FortMP are the primal and dual [[simplex algorithm]]s using [[sparse matrix|sparse matrices]]. These are supplemented for large problems and quadratic programming problems by [[interior point methods]]. Mixed integer programming problems are solved using [[branch and bound]] algorithm.\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://www.optiriskindia.com/fortmpoverview.php FortMP Overview]\n* [http://www.optirisk-systems.com/products_fortmp.asp FortMP home page]\n* [http://www.optirisk-systems.com OptiRisk Systems home page]\n\n\n{{Mathematical optimization software}}\n{{DEFAULTSORT:Fortmp}}\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "FortSP",
      "url": "https://en.wikipedia.org/wiki/FortSP",
      "text": "{{Infobox Software\n| name                   = FortSP\n| developer              = OptiRisk Systems\n| platform               = [[Cross-platform]]\n| genre                  = [[Operations Research|Operations Research Tool]], [[List of numerical analysis software|Numerical Software]]\n| status                 = Active\n| license                = [[Proprietary software|Proprietary]]\n| website                = {{URL|www.optirisk-systems.com/products_fortsp.asp}}\n}}\n\n'''FortSP''' is a software package for solving [[stochastic programming]] (SP) problems. It solves scenario-based SP problems with recourse as well as problems with chance constraints and integrated chance constraints. FortSP is available as a standalone executable that accepts input in SMPS format and as a library with an interface in the [[C programming language]].\n\nThe solution algorithms provided by FortSP include [[Benders' decomposition]] and a variant of level decomposition for two-stage problems, nested Benders' decomposition for multistage problems and reformulation of the problem as a deterministic equivalent. There is also an implementation of a cutting-plane algorithm for integrated chance constraints.\n\nFortSP supports external [[linear programming]] [[solver (computer_science)|solvers]] such as [[CPLEX]], [[FortMP]] and [[Gurobi]] through their library interfaces or [[nl (format)|nl files]]. These solvers are used to optimize the deterministic equivalent problem and also the subproblems in the decomposition methods.\n\n==External links==\n* [http://www.optirisk-systems.com OptiRisk Systems home page]\n* [http://www.optirisk-systems.com/products_fortsp.asp FortSP home page]\n\n[[Category:Numerical software]]\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "Galahad library",
      "url": "https://en.wikipedia.org/wiki/Galahad_library",
      "text": "{{inline|date=September 2016}}\nThe '''Galahad library''' is a [[Thread safety|thread-safe]] [[library (computing)|library]] of packages for the solution of [[mathematical optimization]] problems. The areas covered by the library are unconstrained and [[Constrained optimization|bound-constrained optimization]], [[quadratic programming]], [[nonlinear programming]], systems of nonlinear equations and inequalities, and [[non-linear least squares]] problems. The library is mostly written in the [[Fortran|Fortran 90]] programming language.\n\nThe name of the library originates from its major package for general [[nonlinear programming]], LANCELOT-B, the successor of the original [[Augmented Lagrangian method|augmented Lagrangian]] package LANCELOT of Conn, Gould and Toint.<ref>{{cite book |first=A. R. |last=Conn |first2=N. I. M. |last2=Gould |first3=Ph. L. |last3=Toint |title=LANCELOT: A Fortran Package for Nonlinear Optimization (Release A) |series=Springer Series in Computational Mathematics |volume=vol. 17 |publisher=Springer-Verlag |year=1992 |isbn=0-387-55470-X }}</ref>\n\nOther packages in the library include:\n* a filter-based method for systems of linear and nonlinear equations and inequalities,\n* an active-set method for nonconvex [[quadratic programming]],\n* a primal-dual interior-point method for nonconvex [[quadratic programming]],\n* a presolver for quadratic programs,\n* a [[Lanczos algorithm|Lanczos method]] for trust-region subproblems,\n* an interior-point method to solve [[linear programming|linear programs]] or separable [[convex optimization|convex programs]] or alternatively, to compute the analytic center of a set defined by such constraints, if it exists.\n\nPackages in the GALAHAD library accept problems modeled in either the [[Standard Input Format]] (SIF),<ref>{{cite web |title=The SIF Reference Document |first=Andrew R. |last=Conn |first2=Nicholas I. M. |last2=Gould |first3=Philippe L. |last3=Toint |url=http://www.numerical.rl.ac.uk/lancelot/sif/sifhtml.html }}</ref> or the [[AMPL|AMPL modeling language]]. For problems modeled in the SIF, the GALAHAD library naturally relies upon the [[CUTEr]] package, an optimization toolbox providing all low-level functionalities required by solvers.\n\nThe library is available on several popular computing platforms, including Compaq (DEC) Alpha, Cray, HP, IBM RS/6000, Intel-like PCs, SGI and Sun. It is designed to be easily adapted to other platforms. Support is provided for many operating systems, including [[Tru64]], [[Linux]], [[HP-UX]], [[AIX (operating system)|AIX]], [[Irix|IRIX]] and [[Solaris (operating system)|Solaris]], and for a variety of popular [[Fortran|Fortran 90]] compilers on these platforms and operating systems.\n\nThe GALAHAD Library is authored and maintained by N.I.M. Gould, D. Orban and Ph.L. Toint.<ref>{{cite journal |first=N. I. M. |last=Gould |first2=D. |last2=Orban |first3=Ph. L. |last3=Toint |title=GALAHAD, a library of thread-safe Fortran 90 packages for large-scale nonlinear optimization |journal=ACM Transactions on Mathematical Software |volume=29 |issue=4 |pages=353‚Äì372 |year=2003 |doi=10.1145/962437.962438 }}</ref>\n\n== References ==\n{{reflist}}\n\n== External links ==\n* The [http://galahad.rl.ac.uk official GALAHAD website].\n\n{{Mathematical optimization software}}\n\n[[Category:Numerical software]]\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "GAUSS (software)",
      "url": "https://en.wikipedia.org/wiki/GAUSS_%28software%29",
      "text": "{{Infobox Software\n| name = GAUSS\n| developer = [[Aptech Systems]]\n| released = {{Start date and age|1984|df=yes}}\n| latest_release_version = 19\n| latest_release_date = {{Start date and age|2019|01|14|df=yes}}\n| operating_system = [[Linux]], [[macOS]], [[Microsoft Windows|Windows]]\n| genre = [[programming language]]\n| license = [[proprietary software|proprietary]]\n| website = [https://www.aptech.com/ www.aptech.com]\n}}\n\n'''GAUSS''' is a matrix [[programming language]] for [[mathematics]] and [[statistics]], developed and marketed by Aptech Systems. Its primary purpose is the solution of numerical problems in statistics, [[econometrics]], [[time-series]], optimization and 2D- and 3D-[[Visualization (computer graphics)|visualization]]. It was first published in 1984 for [[MS-DOS]] and is currently also available for [[Linux]], [[macOS]] and [[Microsoft Windows|Windows]].\n\n== Examples of Functions Included in Run-Time Library ==\n* GAUSS has several Application Modules as well as functions in its [[Runtime library|Run-Time Library]] (i.e., functions that come with GAUSS without extra cost)\n** Qprog ‚Äì [[Quadratic programming]]\n** SqpSolvemt ‚Äì [[Sequential quadratic programming]]\n** QNewton - [[Quasi-Newton method|Quasi-Newton]] unconstrained optimization\n** EQsolve - Nonlinear equations solver\n\n== GAUSS Applications ==\nA range of toolboxes are available for GAUSS at additional cost. See https://www.aptech.com/products/gauss-applications/ for complete listing of products.\n\n{| class=\"wikitable\" style=\"text-align:center\"; border=\"5\"\n|-\n|[https://www.aptech.com/products/gauss-applications/algorithmic-derivatives/ Algorithmic Derivatives] || A program for generating GAUSS procedures for computing algorithmic derivatives.\n|-\n|[https://www.aptech.com/products/gauss-applications/constrained-maximum-likelihood-mt/ Constrained Maximum Likelihood MT] || Solves the general maximum likelihood problem subject to general constraints on the parameters.\n|-\n|[https://www.aptech.com/products/gauss-applications/constrained-optimization-mt/ Constrained Optimization] || Solves the [[nonlinear programming]] problem subject to general constraints on the parameters.\n|-\n|[https://www.aptech.com/products/gauss-applications/curvefit/ CurveFit] || Nonlinear [[curve fitting]].\n|-\n|[https://web.archive.org/web/20081122011154/http://www.aptech.com/ga_ds.html Descriptive Statistics] || Basic sample statistics including means, frequencies and [[Contingency table|crosstabs]]. This application is backwards compatible with programs written with Descriptive Statistics 3.1\n|-\n|[https://www.aptech.com/products/gauss-applications/descriptive-statistics-mt/ Descriptive Statistics MT] || Basic sample statistics including means, frequencies and crosstabs. This application is thread-safe and takes advantage of structures.\n|-\n|[https://www.aptech.com/products/gauss-applications/discrete-choice/ Discrete Choice] || A statistical package for estimating [[discrete choice]] and other models in which the dependent variable is qualitative in some way.\n|-\n|[https://www.aptech.com/products/gauss-applications/fanpac-mt/ FANPAC MT] || Comprehensive suite of [[Autoregressive conditional heteroskedasticity#GARCH|GARCH]] (Generalized AutoRegressive Conditional Heteroskedastic) models for estimating volatility.\n|-\n|[https://www.aptech.com/products/gauss-applications/linear-programming-mt/ Linear Programming MT] || Solves small and large scale [[linear programming]] problems\n|-\n|[https://www.aptech.com/products/gauss-applications/linear-regression-mt/ Linear Regression MT] || [[Least squares]] estimation.\n|-\n|[https://www.aptech.com/products/gauss-applications/loglinear-analysis-mt/ Loglinear Analysis MT] || Analysis of [[Categorical variable|categorical data]] using [[log-linear analysis]].\n|-\n|[https://www.aptech.com/products/gauss-applications/maximum-likelihood-mt/ Maximum Likelihood MT]|| [[Maximum likelihood estimation]] of the parameters of statistical models.\n|-\n|[https://www.aptech.com/products/gauss-applications/nonlinear-equations-mt/ Nonlinear Equations MT] || Solves [[System of polynomial equations|systems of nonlinear equations]] having as many equations as unknowns.\n|-\n|[https://www.aptech.com/products/gauss-applications/optimization-mt/ Optimization] || Unconstrained optimization.\n|-\n|[https://www.aptech.com/products/gauss-applications/time-series-mt/ Time Series MT] || Exact ML estimation of VARMAX, VARMA, ARIMAX, [[Autoregressive integrated moving average|ARIMA]], and [[Error correction model|ECM]] models subject to general constraints on the parameters. [[Panel data]] estimation. [[Cointegration]] and [[unit root test]]s.\n|}\n\n== See also ==\n* [[List of numerical analysis software]]\n* [[Comparison of numerical analysis software]]\n\n== External links ==\n*[http://www.aptech.com/ International homepage]\n*[https://web.archive.org/web/20080214092434/http://www.aptech.com/gaussians.html GAUSS Mailing List]\n*[http://www.deskeng.com/articles/aaabjg.htm Review of version 7.0]\n*[http://www.aae.wisc.edu/aae637/gausscode.htm Some more links]\n\n{{Numerical analysis software}}\n{{Statistical software}}\n\n[[Category:Econometrics software]]\n[[Category:Mathematical optimization software]]\n[[Category:Numerical programming languages]]\n[[Category:Statistical programming languages]]\n[[Category:Proprietary commercial software for Linux]]"
    },
    {
      "title": "Gekko (optimization software)",
      "url": "https://en.wikipedia.org/wiki/Gekko_%28optimization_software%29",
      "text": "{{Infobox Software\n| name                   = GEKKO \n| logo                   = gekko_logo.png\n| logo size              = 300px\n| developer              = Logan Beal\n| latest_release_version = 0.2.0\n| latest_release_date    = {{Start date and age|2019|05|02}}\n| operating_system       = {{URL|https://github.com/BYU-PRISM/GEKKO|Cross-Platform}}\n| genre                  = [[List of numerical analysis software|Technical computing]]\n| license                = [[MIT license|MIT]]\n| website                = {{URL|gekko.readthedocs.io/en/latest/}}\n}}\n\nThe '''GEKKO''' Python package<ref>{{cite journal | last=Beal | first=L. | title=GEKKO Optimization Suite | journal=Processes | year=2018 | doi=10.3390/pr6080106  | volume=6 | number=8 | pages=106}}</ref> solves large-scale mixed-integer and differential algebraic equations with nonlinear programming solvers ([[IPOPT]], [[APOPT]], BPOPT, [[SNOPT]], [[MINOS_(optimization_software)|MINOS]]). Modes of operation include machine learning, data reconciliation, real-time optimization, dynamic simulation, and nonlinear [[Model_predictive_control|model predictive control]]. In addition, the package solves [[Linear programming]] (LP), [[Quadratic programming]] (QP), [[Quadratically constrained quadratic program]] (QCQP), [[Nonlinear programming]] (NLP), [[Mixed integer programming]] (MIP), and [[Mixed integer linear programming]] (MILP). GEKKO is available in Python and installed with pip from PyPI of the Python Software Foundation.\n\n<source lang=\"python\">\npip install gekko\n</source>\n    \nGEKKO works on all platforms (Windows, MacOS, Linux, ARM processors) and with Python 2.7 and 3+. By default, the problem is sent to a public server where the solution is computed and returned to Python. There is a Windows and Linux option to solve without an Internet connection. GEKKO is an extension of the [[APMonitor|APMonitor Optimization Suite]] but has integrated the modeling and solution visualization directly within Python. A mathematical model is expressed in terms of variables and equations such as the Hock & Schittkowski Benchmark Problem #71<ref>W. Hock and K. Schittkowski, Test Examples for Nonlinear Programming Codes, Lecture Notes in Economics and Mathematical Systems, Vol. 187, Springer 1981.</ref> used to test the performance of [[nonlinear programming]] solvers. This particular optimization problem has an objective function <math>\\min_{x\\in\\mathbb R}\\; x_1 x_4 (x_1+x_2+x_3)+x_3</math> and subject to the inequality constraint <math>x_1 x_2 x_3 x_4 \\ge 25</math> and equality constraint <math>{x_1}^2 + {x_2}^2 + {x_3}^2 + {x_4}^2=40</math>. The four variables must be between a lower bound of 1 and an upper bound of 5. The initial guess values are <math>x_1 = 1, x_2=5, x_3=5, x_4=1</math>. This optimization problem is solved with GEKKO as shown below.\n\n<source lang=\"python\">\nfrom gekko import GEKKO\nm = GEKKO() # Initialize gekko\n# Initialize variables\nx1 = m.Var(value=1,lb=1,ub=5)\nx2 = m.Var(value=5,lb=1,ub=5)\nx3 = m.Var(value=5,lb=1,ub=5)\nx4 = m.Var(value=1,lb=1,ub=5)\n# Equations\nm.Equation(x1*x2*x3*x4>=25)\nm.Equation(x1**2+x2**2+x3**2+x4**2==40)\nm.Obj(x1*x4*(x1+x2+x3)+x3) # Objective\nm.solve(disp=False) # Solve\nprint('x1: ' + str(x1.value))\nprint('x2: ' + str(x2.value))\nprint('x3: ' + str(x3.value))\nprint('x4: ' + str(x4.value))\nprint('Objective: ' + str(m.options.objfcnval))\n</source>\n\n==Applications of GEKKO==\n\nApplications include [[Cogeneration|cogeneration (power and heat)]]<ref>{{cite journal | last=Mojica | first=J. | title=Optimal combined long-term facility design and short-term operational strategy for CHP capacity investments | journal=Energy | year=2017 | doi=10.1016/j.energy.2016.12.009 | volume=118 | pages=97‚Äì115}}</ref>, [[Oil well|drilling automation]]<ref>{{cite journal | last=Eaton | first=A. | title=Real time model identification using multi-fidelity models in managed pressure drilling | journal=Computers & Chemical Engineering | year=2017 | doi=10.1016/j.compchemeng.2016.11.008 | volume=97 | pages=76‚Äì84}}</ref>, severe slugging control<ref>{{cite journal | last=Eaton | first=A. | title=Post-installed fiber optic pressure sensors on subsea production risers for severe slugging control | journal=OMAE 2015 Proceedings, St. John's, Canada | year=2015 | url=http://apm.byu.edu/prism/uploads/Projects/Eaton_OMAE15.pdf}}</ref>, solar thermal energy production<ref>{{cite journal | last=Powell | first=K. | title=Dynamic Optimization of a Hybrid Solar Thermal and Fossil Fuel System | journal=Solar Energy | year=2014 | doi=10.1016/j.solener.2014.07.004 | volume=108 | pages=210‚Äì218}}</ref>, [[solid oxide fuel cell]]s<ref>{{cite journal | last=Spivey | first=B. | title=Dynamic Modeling of Reliability Constraints in Solid Oxide Fuel Cells and Implications for Advanced Control | journal=AIChE Annual Meeting Proceedings, Salt Lake City, Utah | year=2010 | url=http://apmonitor.com/wiki/uploads/Apps/sofc.pdf}}</ref><ref>{{cite journal | last=Spivey | first=B. | title=Dynamic modeling, simulation, and MIMO predictive control of a tubular solid oxide fuel cell | journal=Journal of Process Control | year=2012 | doi=10.1016/j.jprocont.2012.01.015 | volume=22 | issue=8 | pages=1502‚Äì1520}}</ref>, flow assurance <ref>{{cite journal | last=Hedengren | first=J. | title=New flow assurance system with high speed subsea fiber optic monitoring of pressure and temperature | journal=ASME 37th International Conference on Ocean, Offshore and Arctic Engineering, OMAE2018/78079, Madrid, Spain | pages=V005T04A034 | year=2018| doi=10.1115/OMAE2018-78079 | isbn=978-0-7918-5124-1 }}</ref>, [[Enhanced oil recovery]] <ref>{{cite journal | last=Udy | first=J. | title=Reduced order modeling for reservoir injection optimization and forecasting | journal=FOCAPO / CPC 2017, Tucson, AZ | year=2017 | url=https://apm.byu.edu/prism/uploads/Members/udy2017_eor.pdf}}</ref>, [[Essential oil]] extraction<ref>{{cite journal | last=Valderrama | first=F. | title=An optimal control approach to steam distillation of essential oils from aromatic plants | journal=Computers & Chemical Engineering | volume=117 | pages=25‚Äì31 | year=2018 | doi=10.1016/j.compchemeng.2018.05.009 }}</ref>, and [[Unmanned aerial vehicle|Unmanned Aerial Vehicles (UAVs)]]<ref>{{cite journal | last=Sun | first=L. | title=Optimal Trajectory Generation using Model Predictive Control for Aerially Towed Cable Systems | journal=Journal of Guidance, Control, and Dynamics | year=2013 | url=http://apm.byu.edu/prism/uploads/Members/sun_2013.pdf}}</ref>. There are many other references to [http://apmonitor.com/wiki/index.php/Main/APMonitorReferences APMonitor and GEKKO] as a sample of the types of applications that can be solved. GEKKO is developed from the National Science Foundation (NSF) research grant #1547110 <ref>{{cite journal | last=Beal | first=L. | title=Integrated scheduling and control in discrete-time with dynamic parameters and constraints | journal=Computers & Chemical Engineering | volume=115 | pages=361‚Äì376 | year=2018 | doi=10.1016/j.compchemeng.2018.04.010}}</ref><ref>{{cite journal | last=Beal | first=L. | title=Combined model predictive control and scheduling with dominant time constant compensation | journal=Computers & Chemical Engineering | volume=104 | pages=271‚Äì282 | year=2017 | url=https://scholarsarchive.byu.edu/facpub/1905/ | doi=10.1016/j.compchemeng.2017.04.024}}</ref><ref>{{cite journal | last=Beal | first=L. | title=Economic benefit from progressive integration of scheduling and control for continuous chemical processes | journal=Processes | year=2017 | doi=10.3390/pr5040084 | volume=5| issue=4 | pages=84 }}</ref><ref>{{cite journal | last=Petersen | first=D. | title=Combined noncyclic scheduling and advanced control for continuous chemical processes | journal=Processes | year=2017 | doi=10.3390/pr5040083 | volume=5| issue=4 | pages=83 }}</ref> and is detailed in a Special Issue collection on combined scheduling and control<ref>{{cite journal | last=Hedengren | first=J. | title=Special issue: combined scheduling and control | journal=Processes | volume=6 | issue=3 | pages=24 | year=2018 | url=http://www.mdpi.com/journal/processes/special_issues/Combined_Scheduling | doi=10.3390/pr6030024}}</ref>. Other notable mentions of GEKKO are the listing in the Decision Tree for Optimization Software<ref>{{cite web |url=http://plato.asu.edu/sub/tools.html |title=Decision Tree for Optimization Software |last=Mittleman |first=Hans |date=1 May 2018 |website=Plato |publisher=Arizona State University |access-date=1 May 2018\n |quote=Object-oriented python library for mixed-integer and differential-algebraic equations}}</ref>, added support for [[APOPT]] and BPOPT solvers<ref>{{cite web\n |url=https://apopt.com |title=Solver Solutions  |publisher=Advanced Process Solutions, LLC |access-date=1 May 2018 |quote=GEKKO Python with APOPT or BPOPT Solvers}}</ref>, projects reports of the online Dynamic Optimization course from international participants<ref>{{cite web |url=http://apmonitor.com/do/index.php/Main/ProjectLab |title=Dynamic Optimization Projects |last=Everton |first=Colling |website=Petrobras |publisher=Petrobras, Statoil, Facebook |access-date=1 May 2018 |quote=Example Presentation: Everton Colling of Petrobras shares his experience with GEKKO for modeling and nonlinear control of distillation}}</ref>. GEKKO is a topic in online forums where users are solving optimization and optimal control problems<ref>{{cite web |url=https://groups.google.com/forum/#!searchin/apmonitor/gekko |title=APMonitor Google Group: GEKKO |website=Google |access-date=1 May 2018}}</ref><ref>{{cite web |url=https://scicomp.stackexchange.com/questions/83/is-there-a-high-quality-nonlinear-programming-solver-for-python |title=Computational Science: Is there a high quality nonlinear programming solver for Python? |website=SciComp |access-date=1 May 2018}}</ref>. GEKKO is used for advanced control in the Temperature Control Lab (TCLab)<ref>{{cite web |url=https://media.readthedocs.org/pdf/tclab/stable/tclab.pdf |title=TCLab Documentation |last=Kantor |first=Jeff |date=2 May 2018 |website=ReadTheDocs |publisher=University of Notre Dame |access-date=2 May 2018\n |quote=pip install tclab}}</ref> for process control education at 20 universities<ref>{{cite web |url=https://jckantor.github.io/CBE30338/ |title=Chemical Process Control |last=Kantor |first=Jeff |date=2 May 2018 |website=GitHub |publisher=University of Notre Dame |access-date=2 May 2018\n |quote=Using the Temperature Control Lab (TCLab)}}</ref><ref>{{cite web |url=http://apmonitor.com/do/index.php/Main/AdvancedTemperatureControl |title=Advanced Temperature Control Lab |last=Hedengren |first=John |date=2 May 2018 |website=Dynamic Optimization Course |publisher=Brigham Young University |access-date=2 May 2018 |quote=Hands-on applications of advanced temperature control}}</ref><ref>{{cite web |url=https://github.com/alchemyst/Dynamics-and-Control |title=Jupyter notebooks for Dynamics and Control |last=Sandrock |first=Carl |date=2 May 2018 |website=GitHub |publisher=University of Pretoria, South Africa |access-date=2 May 2018 |quote=CPN321 (Process Dynamics), and CPB421 (Process Control) at the Chemical Engineering department of the University of Pretoria}}</ref><ref>{{cite web |url=http://cache.org/files/winter-2018-dynamic-simulation.pdf |title=CACHE News (Winter 2018): Incorporating Dynamic Simulation into Chemical Engineering Curricula |date=2 May 2018 |website=CACHE: Computer Aids for Chemical Engineering |publisher=University of Texas at Austin |access-date=2 May 2018 |quote=Short Course at the ASEE 2017 Summer School hosted at SCSU by Hedengren (BYU), Grover (Georgia Tech), and Badgwell (ExxonMobil)}}</ref>.\n\n=== Machine Learning ===\n\n[[File:Artificial Neural Network Example.png|thumb|Artificial Neural Network]]\n\nOne application of [[machine learning]] is to perform regression from training data to build a correlation. In this example a regression model is generated from training data generated with the function <math>1-\\cos(x)</math>. An [[artificial neural network]] with three layers is used for this example. The first layer is linear, the second layer has a hyperbolic tangent activation function, and the third layer is linear. The above program produces parameter weights for <math>w_1</math>, <math>w_{2a}</math>, <math>w_{2b}</math>, and <math>w_3</math> that minimize the sum of squared errors between the ten measured data points and the neural network predictions at those points. GEKKO uses gradient-based optimizers to determine the optimal weight values instead of standard methods such as [[backpropagation]]. The gradients are determined by automatic differentiation, similar to other popular packages. The problem is solved as a constrained optimization problem and is converged when the solver satisfies [[Karush‚ÄìKuhn‚ÄìTucker conditions]]. Using a gradient-based optimizer allows additional constraints that may be imposed with domain knowledge of the data or system.\n\n<source lang=\"python\">\nfrom gekko import GEKKO\nimport numpy as np\nimport matplotlib.pyplot as plt\n# generate training data\nx = np.linspace(-np.pi,3*np.pi,10)\ny = 1.0 - np.cos(x)\n# neural network structure\nn1 = 3   # hidden layer 1 (linear)\nn2 = 5   # hidden layer 2 (nonlinear)\nn3 = 3   # hidden layer 3 (linear)\n# Initialize gekko\nm = GEKKO()\n# input(s)\nm.inpt = m.Param(x)\n# layer 1\nm.w1 = m.Array(m.FV, (1,n1), value=1,lb=0.1)\nm.l1 = [m.Intermediate(m.w1[0,i]*m.inpt) for i in range(n1)]\n# layer 2\nm.w2a = m.Array(m.FV, (n1,n2), value=1.0)\nm.w2b = m.Array(m.FV, (n1,n2), value=1.0)\nm.l2 = [m.Intermediate(sum([m.tanh(m.w2a[j,i]+m.w2b[j,i]*m.l1[j]) \\\n                        for j in range(n1)])) for i in range(n2)]\n# layer 3\nm.w3 = m.Array(m.FV, (n2,n3), value=1.0)\nm.l3 = [m.Intermediate(sum([m.w3[j,i]*m.l2[j] for j in range(n2)])) for i in range(n3)]\n# output(s)\nm.outpt = m.CV(y)\nm.Equation(m.outpt==sum([m.l3[i] for i in range(n3)]))\n# flatten matrices\nm.w1 = m.w1.flatten()\nm.w2a = m.w2a.flatten()\nm.w2b = m.w2b.flatten()\nm.w3 = m.w3.flatten()\n# Fit parameter weights\nm.outpt.FSTATUS = 1\nfor i in range(len(m.w1)):\n    m.w1[i].STATUS=1\nfor i in range(len(m.w2a)):\n    m.w2a[i].STATUS=1\n    m.w2b[i].STATUS=1\nfor i in range(len(m.w3)):\n    m.w3[i].STATUS=1\nm.options.IMODE = 2\nm.options.EV_TYPE = 2\nm.solve(disp=False)\n</source>\n\nThe neural network model is tested across the range of training data as well as for extrapolation to demonstrate poor predictions outside of the training data. Predictions outside the training data set are improved with hybrid machine learning that uses fundamental principles (if available) to impose a structure that is valid over a wider range of conditions. In the example above, the hyperbolic tangent activation function (hidden layer 2) could be replaced with a sine function to improve extrapolation. The final part of the script displays the neural network model, the original function, and the sampled data points used for fitting.\n\n<source lang=\"python\">\n# Test sample points\nfor i in range(len(m.w1)):\n    m.w1[i].STATUS=0\nfor i in range(len(m.w2a)):\n    m.w2a[i].STATUS=0\n    m.w2b[i].STATUS=0\nfor i in range(len(m.w3)):\n    m.w3[i].STATUS=0\nx_test = np.linspace(-2*np.pi,4*np.pi,100)\nm.inpt.value=x_test\nm.options.IMODE = 2\nm.solve(disp=False)\n# Plot ANN results\nplt.figure()\nplt.plot(x_test,1.0-np.cos(x_test),'b--',label=r'$1.0-\\cos(x)$')\nplt.plot(x,y,'bo',label='training data')\nplt.plot(m.inpt.value,m.outpt.value, 'r-',label='neural network')\nplt.ylabel('Output (y)')\nplt.xlabel('Input (x)')\nplt.legend()\nplt.show()\n</source>\n\n=== Optimal Control ===\n\n[[File:Optimal Control Luus.png|thumb|Optimal control problem benchmark (Luus) with an integral objective, inequality, and differential constraint.]]\n\n[[Optimal control]] is the use of [[mathematical optimization]] to obtain a policy that is constrained by differential <math>\\left(\\frac{d\\,x_1}{d\\,t}=u\\right)</math>, equality <math>\\left(x_1(0) = 1\\right)</math>, or inequality <math>\\left(-1 \\le u(t) \\le 1\\right)</math> equations and minimizes an objective/reward function <math>\\left(\\min_u \\frac{1}{2} \\int_0^2 x_1^2(t) \\, dt\\right)</math>. The basic optimal control is solved with GEKKO by integrating the objective and transcribing the differential equation into algebraic form with orthogonal collocation on finite elements.\n\n<source lang=\"python\">\nfrom gekko import GEKKO\nimport numpy as np\nimport matplotlib.pyplot as plt\nm = GEKKO() # initialize gekko\nnt = 101\nm.time = np.linspace(0,2,nt)\n# Variables\nx1 = m.Var(value=1)\nx2 = m.Var(value=0)\nu = m.Var(value=0,lb=-1,ub=1)\np = np.zeros(nt) # mark final time point\np[-1] = 1.0\nfinal = m.Param(value=p)\n# Equations\nm.Equation(x1.dt()==u)\nm.Equation(x2.dt()==0.5*x1**2)\nm.Obj(x2*final) # Objective function\nm.options.IMODE = 6 # optimal control mode\nm.solve() # solve\nplt.figure(1) # plot results\nplt.plot(m.time,x1.value,'k-',label=r'$x_1$')\nplt.plot(m.time,x2.value,'b-',label=r'$x_2$')\nplt.plot(m.time,u.value,'r--',label=r'$u$')\nplt.legend(loc='best')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.show()\n</source>\n\n== See also ==\n* [[APMonitor]] and [[Python (programming language)|Python]].\n\n==References==\n{{reflist}}\n\n== External links ==\n* [https://gekko.readthedocs.io/en/latest/ GEKKO Documentation]\n* [https://github.com/BYU-PRISM/GEKKO GEKKO Source Code]\n* [https://pypi.org/project/gekko GEKKO on PyPI] for Python pip install\n* GEKKO is open-source product of [https://www.nsf.gov/awardsearch/showAward?AWD_ID=1547110 National Science Foundation (NSF) research grant 1547110]\n* [http://apmonitor.com/wiki/index.php/Main/APMonitorReferences References to APMonitor and GEKKO] in the literature\n* [https://apmonitor.com/wiki/index.php/Main/GekkoPythonOptimization 18 examples of GEKKO]: machine learning, optimal control, data regression\n\n{{Mathematical optimization software}}\n{{DEFAULTSORT:Gekko}}\n[[Category:Numerical programming languages]]\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "General Algebraic Modeling System",
      "url": "https://en.wikipedia.org/wiki/General_Algebraic_Modeling_System",
      "text": "{{Infobox software\n| name                   = GAMS\n| developer              = GAMS Development Corporation\n| latest_release_version = 27.2.0\n| latest release date    = {{start date and age|df=yes|paren=yes|2019|05|24}}\n| platform               = [[Cross-platform]]\n| status                 = Active\n| license                = [[Proprietary software|Proprietary]]\n| genre                  = [[Algebraic modeling language|Algebraic Modeling Language (AML)]]\n| website                = {{url|https://www.gams.com}}\n}}\n\nThe '''General Algebraic Modeling System''' ('''GAMS''') is a high-level [[computer model|modeling]] system for [[Optimization (mathematics)|mathematical optimization]]. GAMS is designed for modeling and solving [[Linear optimization|linear]], [[Nonlinear optimization|nonlinear]], and [[Mixed integer programming|mixed-integer optimization]] problems. The system is tailored for complex, large-scale modeling applications and allows the user to build large maintainable models that can be adapted to new situations. The system is available for use on various computer platforms. Models are [[Software portability|portable]] from one platform to another.\n\nGAMS was the first [[algebraic modeling language]] (AML)<ref>{{cite book|last1=Kallrath|first1=Josef|title=Modeling Languages in Mathematical Optimization|date=2004|publisher=Kluer Academic Publishers|location=Norwell, USA|isbn=978-1-4613-7945-4|page=241|edition=First}}</ref> and is formally similar to commonly used [[fourth-generation programming language]]s. GAMS contains an [[integrated development environment]] (IDE) and is connected to a group of third-party optimization [[solver]]s. Among these [[solver]]s are BARON, [[COIN-OR]] solvers, CONOPT, [[CPLEX]], DICOPT, [[Gurobi]], [[MOSEK]], [[SNOPT]], SULUM, and [[FICO Xpress | XPRESS]].\n\nGAMS allows the users to implement a sort of [[hybrid algorithm]] combining different solvers. Models are described in concise, human-readable algebraic statements. GAMS is among the most popular input formats for the [[NEOS Server]].{{cn|date=January 2014}} Although initially designed for applications related to [[economics]] and [[management science]], it has a community of users from various backgrounds of [[engineering]] and [[science]].\n\n==Timeline==\n* 1976 GAMS idea is presented at the International Symposium on Mathematical Programming (ISMP), Budapest<ref>\n{{cite conference\n | title = Toward a General Algebraic Modelling System\n | conference = IX. International Symposium on Mathematical Programming\n | url = http://www.math.uwaterloo.ca/~bico/ismp/ismp1976_abstracts.pdf\n | year = 1976\n | pages = 185\n | location = Budapest, Hungary\n }}</ref>\n* 1978 Phase I: GAMS supports [[linear programming]]. Supported platforms: Mainframes and Unix Workstations\n* 1979 Phase II: GAMS supports [[nonlinear programming]].\n* 1987 GAMS becomes a commercial product\n* 1988 First PC System (16 bit)\n* 1988 Alex Meeraus, the initiator of GAMS and founder of [http://www.gams.com/ GAMS Development Corporation], is awarded [http://computing.society.informs.org/ INFORMS Computing Society] [http://computing.society.informs.org/prize.php Prize]\n* 1990 32 bit Dos Extender\n* 1990 GAMS moves to [[Georgetown, Washington, D.C.]]\n* 1991 Mixed Integer Non-Linear Programs capability (DICOPT)\n* 1994 GAMS supports [[mixed complementarity problem]]s\n* 1995 MPSGE language is added for CGE modeling\n* 1996 European branch opens in Germany\n* 1998 32 bit native Windows\n* 1998 [[Stochastic programming]] capability (OSL/SE, DECIS)\n* 1999 Introduction of the GAMS [[Integrated development environment]] (IDE)\n* 2000 [http://www.gamsworld.org/ GAMS World] initiative started\n* 2001 GAMS Data Exchange (GDX) is introduced\n* 2002 GAMS is listed in OR/MS 50th Anniversary list of milestones\n* 2003 [[Conic programming]] is added\n* 2003 [[Global optimization]] in GAMS\n* 2004 Quality assurance initiative starts\n* 2004 Support for Quadratic Constrained programs\n* 2005 Support for 64 bit PC Operating systems\n* 2006 GAMS supports parallel [[grid computing]]\n* 2007 GAMS supports open-source [[solver]]s from [[COIN-OR]]\n* 2008 Support for 32 and 64 bit [[Mac OS X]]\n* 2009 GAMS available on the [[Amazon Elastic Compute Cloud]]\n* 2009 GAMS supports extended mathematical programs ([[Extended Mathematical Programming (EMP)|EMP]])\n* 2010 GAMS is awarded the [http://gor.uni-paderborn.de/40_Preise/UntPreis/ company award] of the German Society of Operations Research (GOR)\n* 2010 [https://www.gams.com/latest/docs/T_GDXMRW.html GDXMRW] interface between GAMS and Matlab\n* 2011 Support for [https://www.gams.com/latest/docs/UG_ExtrinsicFunctions.html Extrinsic Function Libraries]\n* 2012 The Winners of the 2012 INFORMS Impact Prize included Alexander Meeraus. The prize was awarded to the originators of the five most important algebraic modeling languages [http://www.informs.org/Blogs/E-News-Blog/INFORMS-Impact-Prize].\n* 2012 Introduction of [https://www.gams.com/latest/docs/API_MAIN.html#GAMS_OOAPIS Object Oriented API for .NET, Java, and Python]\n* 2012 The winners of the 2012 [http://www.coin-or.org/coinCup/coinCup2012Winner.html Coin OR Cup]  included Michael Bussieck, Steven Dirkse, & Stefan Vigerske for [https://projects.coin-or.org/GAMSlinks GAMSlinks]\n* 2013 Support for distributed MIP (Cplex/Gurobi)\n* 2013 [https://www.gams.com/latest/docs/UG_EMP_SP.html Stochastic programming extension] of GAMS EMP\n* 2013 [https://www.gams.com/latest/docs/T_GDXRRW.html GDXRRW] interface between GAMS and R \n* 2014 Local search solver LocalSolver added to solver portfolio\n* 2015 LaTeX documentation from GAMS source ([https://www.gams.com/latest/docs/T_MODEL2TEX.html Model2TeX])\n* 2016 [https://www.gams.com/about-the-company/ New Management Team]\n* 2017 [https://www.gams.com/latest/docs/UG_EmbeddedCode.html EmbeddedCode Facility]\n* 2017 [https://www.gams.com/latest/docs/API_CPP_OVERVIEW.html C++ API]\n* 2018 [https://www.gams.com/latest/docs/T_STUDIO.html GAMS Studio (Beta)]\n* 2019 [https://www.gams.com/miro/ GAMS MIRO - Model Interface with Rapid Orchestration (Beta)]\n\n==Background==\nThe driving force behind the development of GAMS were the users of [[mathematical programming]] who believed in [[Optimization (mathematics)|optimization]] as a powerful and elegant framework for solving real life problems in science and engineering. At the same time, these users were frustrated by high costs, skill requirements, and an overall low reliability of applying optimization tools. Most of the system's initiatives and support for new development arose in response to problems in the fields of [[economics]], [[finance]], and [[chemical engineering]], since these disciplines view and understand the world as a mathematical program.\n\nGAMS‚Äôs impetus for development arose from the frustrating experience of a large economic modeling group at the [[World Bank]]. In hindsight, one may call it a historic accident that in the 1970s mathematical economists and statisticians were assembled to address problems of development. They used the best techniques available at that time to solve multi-sector economy-wide models and large simulation and optimization models in agriculture, steel, fertilizer, power, water use, and other sectors. Although the group produced impressive research, initial success was difficult to reproduce outside their well functioning research environment. The existing techniques to construct, manipulate, and solve such models required several manual, time-consuming, and error-prone translations into different, problem-specific representations required by each solution method. During seminar presentations, modelers had to defend the existing versions of their models, sometimes quite irrationally, because of time and [[money]] considerations. Their models just could not be moved to other environments, because special programming knowledge was needed, and data formats and solution methods were not portable.\n\nThe idea of an algebraic approach to represent, manipulate, and solve large-scale mathematical models fused old and new paradigms into a [[consistent]] and computationally tractable system. Using [[generator matrix|generator matrices]] for [[linear program]]s revealed the importance of naming rows and columns in a consistent manner. The connection to the emerging relational data model became evident. Experience using traditional [[programming language]]s to manage those name spaces naturally lead one to think in terms of [[Set (mathematics)|set]]s and [[tuple]]s, and this led to the relational data model.\n\nCombining multi-dimensional algebraic notation with the relational data model was the obvious answer. Compiler writing techniques were by now widespread, and languages like GAMS could be implemented relatively quickly. However, translating this rigorous mathematical representation into the algorithm-specific format required the computation of [[partial derivative]]s on very large systems. In the 1970s, [[TRW Inc.|TRW]] developed a system called [[PROSE modeling language|PROSE]] that took the ideas of chemical engineers to compute point derivatives that were exact [[derivative]]s at a given point, and to embed them in a consistent, Fortran-style calculus [[modeling language]]. The resulting system allowed the user to use automatically generated exact first and second order derivatives. This was a pioneering system and an important demonstration of a concept. However, [[PROSE modeling language|PROSE]] had a number of shortcomings: it could not handle large systems, problem representation was tied to an array-type data structure that required address calculations, and the system did not provide access to state-of-the art solution methods. From linear programming, GAMS learned that exploitation of [[sparsity]] was key to solving large problems. Thus, the final piece of the puzzle was the use of sparse data structures.\n\n==A sample model==\nA transportation problem from [[George Dantzig]] is used to provide a sample GAMS model.<ref>\n{{cite manual\n |     author = R E Rosenthal\n |      title = GAMS: A User's Guide\n |    section = Chapter 2: A GAMS Tutorial\n |  publisher = The Scientific Press, Redwood City, California\n |       year = 1988\n }}</ref> This model is part of the model library which contains many more complete GAMS models. This problem finds a least cost shipping schedule that meets requirements at markets and supplies at factories.\n\n[[George Dantzig|Dantzig]], G B, Chapter 3.3. In Linear Programming and Extensions. Princeton University Press, Princeton, New Jersey, 1963.\n\n  Sets\n       i   canning plants   / seattle, san-diego /\n       j   markets          / new-york, Chicago, topeka / ;\n  Parameters\n       a(i)  capacity of plant i in cases\n         /    seattle     350\n              san-diego   600  /\n       b(j)  demand at market j in cases\n         /    new-york    325\n              Chicago     300\n              topeka      275  / ;\n  Table d(i,j)  distance in thousands of miles\n                    new-york       Chicago      topeka\n      seattle          2.5           1.7          1.8\n      san-diego        2.5           1.8          1.4  ;\n  Scalar f  freight in dollars per case per thousand miles  /90/ ;\n  Parameter c(i,j)  transport cost in thousands of dollars per case ;\n            c(i,j) = f * d(i,j) / 1000 ;\n  Variables\n       x(i,j)  shipment quantities in cases\n       z       total transportation costs in thousands of dollars ;\n  Positive Variable x ;\n  Equations\n       cost        define objective function\n       supply(i)   observe supply limit at plant i\n       demand(j)   satisfy demand at market j ;\n  cost ..        z  =e=  sum((i,j), c(i,j)*x(i,j)) ;\n  supply(i) ..   sum(j, x(i,j))  =l=  a(i) ;\n  demand(j) ..   sum(i, x(i,j))  =g=  b(j) ;\n  Model transport /all/ ;\n  Solve transport using lp minimizing z ;\n  Display x.l, x.m ;\n\n==Subsystems==\nThe Mathematical Programming System for General Equilibrium analysis (MPSGE) is a language used for formulating and solving [[Arrow‚ÄìDebreu model|Arrow‚ÄìDebreu]] economic equilibrium models and exists as a subsystem within GAMS.<ref>{{Cite journal | last1 = Rutherford | first1 = T. F. | journal = Computational Economics | volume = 14 | pages = 1‚Äì4 |title=Applied General Equilibrium Modeling with MPSGE as a GAMS Subsystem: An Overview of the Modeling Framework and Syntax| doi = 10.1023/A:1008655831209 | year = 1999 | pmid =  | pmc = }}</ref>\n\n== See also ==\n\n* [[Extended Mathematical Programming (EMP)]] ‚Äì an extension to mathematical programming languages available within GAMS\n* [[GNU Linear Programming Kit|GNU MathProg]] ‚Äì an open-source mathematical programming language based on AMPL\n\n==References==\n{{reflist|30em}}\n\n==External links==\n* [https://www.gams.com/ GAMS Development Corporation]\n* [http://www.gams.de/ GAMS Software GmbH]\n* [http://www.gamsworld.org/ GAMS World]\n\n{{Mathematical optimization software}}\n[[Category:Computer algebra systems]]\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "GNU Linear Programming Kit",
      "url": "https://en.wikipedia.org/wiki/GNU_Linear_Programming_Kit",
      "text": "{{Infobox software\n| name                   = GNU Linear Programming Kit\n| logo                   =\n| screenshot             =\n| caption                =\n| author                 = Andrew O. Makhorin\n| developer              = [[GNU Project]]\n| released               =\n| latest release version = 4.65\n| latest release date    = {{start date and age|df=yes|paren=yes|2018|02|16}} \n| latest preview version =\n| latest preview date    =\n| programming language   = [[C (programming language)|C]]\n| operating system       = [[Cross-platform]]\n| language               = English\n| genre                  =\n| license                = [[GNU General Public License|GPLv3]]\n| website                = {{URL|https://www.gnu.org/software/glpk/}}\n}}\n\nThe '''GNU Linear Programming Kit''' ('''GLPK''') is a [[Software package (installation)|software package]] intended for solving large-scale [[linear programming]] (LP), [[mixed integer programming]] (MIP), and other related problems. It is a set of routines written in [[ANSI C]] and organized in the form of a callable [[library (computer science)|library]]. The package is part of the [[GNU Project]] and is released under the [[GNU General Public License]].\n\nProblems can be modeled in the language '''GNU MathProg''' (previously known as GMPL) which shares many parts of the syntax with [[AMPL]] and solved with standalone solver GLPSOL.\n\nGLPK can also be used as a [[C (programming language)|C]] [[Library (computing)|library]].\n\nGLPK uses the [[Simplex algorithm#Implementation|revised simplex method]] and the primal-dual [[interior point method]] for non-integer problems and the [[Branch and bound|branch-and-bound]] algorithm together with [[Cutting-plane method#Gomory.27s cut|Gomory's mixed integer cuts]] for (mixed) integer problems.\n\nGLPK is supported in the free edition of the [[OptimJ]] modeling system\n\nAn independent project provides a [[Java (Sun)|Java]]-based interface to GLPK (via JNI).<ref>http://glpk-java.sourceforge.net</ref> This allows Java applications to call out to GLPK in a relatively transparent manner.\n\n== History ==\nGLPK was developed by Andrew O. Makhorin (–ê–Ω–¥—Ä–µ–π –û–ª–µ–≥–æ–≤–∏—á –ú–∞—Ö–æ—Ä–∏–Ω) of the [[Moscow Aviation Institute]]. The first public release was in October 2000.\n* Version 1.1.1 contained a library for a revised primal and dual simplex algorithm.\n* Version 2.0 introduced an implementation of the primal-dual interior point method.\n* Version 2.2 added branch and bound solving of mixed integer problems.\n* Version 2.4 added a first implementation of the GLPK/L modeling language.\n* Version 4.0 replaced GLPK/L by the GNU MathProg modeling language, which is a subset of the [[AMPL]] modeling language.\n\n==References==\n{{reflist}}\n\n== Further reading ==\n* {{cite book|author=Eiji Oki|title=Linear Programming and Algorithms for Communication Networks: A Practical Guide to Network Design, Control, and Management|year=2012|publisher=CRC Press|isbn=978-1-4665-5264-7}} The book uses GLPK exclusively and contains numerous examples.\n\n==External links==\n{{Portal|Free and open-source software}}\n{{Wikibooks|GLPK}}\n* [https://www.gnu.org/software/glpk/ GLPK official site]\n* [[en:wikibooks:GLPK | GLPK Wikibook]]\n\n{{Mathematical optimization software}}\n\n[[Category:GNU Project software|Linear Programming Kit]]\n[[Category:Mathematical optimization software]]\n[[Category:Free mathematics software]]\n[[Category:Free software programmed in C]]\n[[Category:Mathematics software for Linux]]"
    },
    {
      "title": "GPOPS-II",
      "url": "https://en.wikipedia.org/wiki/GPOPS-II",
      "text": "{{Infobox software\n| name                   = GPOPS-II\n| logo                   = Logo_for_Optimal_Control_Software_GPOPS-II.png\n| screenshot             =\n| caption                =\n| developer              = Michael Patterson,<ref name=people>{{Cite web | url=http://vdol.mae.ufl.edu/People/People.html | title=People &#124;}}</ref> Anil V. Rao<ref name=people />\n| released               = {{Start date and age|df=yes|2013|01}}\n| latest release version = 2.0\n| latest release date    = {{Start date and age|2015|09|1|df=yes}}\n| programming language   = [[MATLAB]]\n| operating system       = [[Mac OS X]], [[Linux]], [[Windows]]\n| language               = [[English language|English]]\n| genre                  = [[Numerical optimization software]]\n| license                = [[Proprietary software|Proprietary]], Free-of-charge for K - 12 or classroom use.  Licensing fees apply for all academic, not-for profit, and commercial use (outside of classroom use) \n| website                = {{URL|gpops2.com}}\n}}\n\n'''GPOPS-II''' (pronounced \"GPOPS 2\") is a general-purpose MATLAB software for solving continuous optimal control problems using hp-adaptive Gaussian quadrature collocation and sparse nonlinear programming.  The acronym GPOPS stands for \"'''G'''eneral '''P'''urpose '''OP'''timal Control '''S'''oftware\", and the Roman numeral \"II\" refers to the fact that GPOPS-II is the second software of its type (that employs Gaussian quadrature integration).\n\n== Problem Formulation ==\nGPOPS-II<ref name=GPOPS-II-Article>{{cite journal|last1=Patterson|first1=M. A.|last2=Rao|first2=A. V.|title=GPOPS-II: A MATLAB Software for Solving Multiple-Phase Optimal Control Problems Using hp-Adaptive Gaussian Quadrature Collocation Methods and Sparse Nonlinear Programming|journal=ACM Transactions on Mathematical Software|date=2014|volume=41|issue=1|pages=1:1‚Äì1:37|doi=10.1145/2558904|url=http://dl.acm.org/citation.cfm?id=2558904}}</ref> is designed to solve multiple-phase optimal control problems of the following mathematical form (where <math>P</math> is the number of phases):\n::: <math>\\min J = \\phi(\\mathbf{e}^{(1)},\\ldots,\\mathbf{e}^{(P)})</math>\n:subject to the dynamic constraints\n::: <math>\\dot{\\mathbf{y}}^{(p)}(t)=\\mathbf{a}^{(p)}(\\mathbf{y}^{(p)}(t),\\mathbf{u}^{(p)}(t),t,\\mathbf{s}),\\quad (p=1,\\ldots,P),</math>\n:the event constraints\n::: <math>\\mathbf{b}_{\\min}\\leq\\mathbf{b}(\\mathbf{e}^{(1)},\\ldots,\\mathbf{e}^{(P)},\\mathbf{s})\\leq\\mathbf{b}_{\\max},</math>\n:the inequality path constraints\n::: <math>\\mathbf{c}_{\\min}^{(p)}\\leq\\mathbf{c}(\\mathbf{y}^{(p)}(t),\\mathbf{u}^{(p)}(t),t,\\mathbf{s})\\leq\\mathbf{c}_{\\max}^{(p)},\\quad (p=1,\\ldots,P),</math>\n:the static parameter constraints\n::: <math>\\mathbf{s}_{\\min}\\leq\\mathbf{s}\\leq\\mathbf{s}_{\\max},</math>\n:and the integral constraints\n::: <math>\\mathbf{q}_{\\min}^{(p)}\\leq\\mathbf{q}^{(p)}\\leq\\mathbf{q}_{\\max}^{(p)},\\quad (p=1,\\ldots,P),</math>\n:where\n::: <math>\\mathbf{e}^{(p)}=\\left[\\mathbf{y}^{(p)}(t_0^{(p)}),t_0^{(p)},\\mathbf{y}^{(p)}(t_f^{(p)}),t_f^{(p)},\\mathbf{q}^{(p)}\\right],\\quad (p=1,\\ldots,P),</math>\n:and the integrals in each phase are defined as\n::: <math>q_i^{(p)}=\\int_{t_0^{(p)}}^{t_f^{(p)}} g_i^{(p)}(\\mathbf{y}^{(p)}(t),\\mathbf{u}^{(p)}(t),t,\\mathbf{s})dt,\\quad (i=1,\\ldots,n_q^{(p)},\\; p=1,\\ldots,P).</math>\nIt is important to note that the event constraints can contain any functions that relate information at the start and/or terminus of any phase (including relationships that include both static parameters and integrals) and that the phases themselves need not be sequential.  It is noted that the approach to linking phases is based on well-known formulations in the literature.<ref name=Betts-Practical>{{cite book|last1=Betts|first1=John T.|title=Practical Methods for Optimal Control and Estimation Using Nonlinear Programming|date=2010|publisher=SIAM Press|location=Philadelphia|isbn=9780898718577|doi=10.1137/1.9780898718577}}</ref>\n\n== Method Employed by GPOPS-II ==\nGPOPS-II uses a class of methods referred to as <math>hp</math>-adaptive Gaussian quadrature collocation where the collocation points are the nodes of a Gauss quadrature (in this case, the Legendre-Gauss-Radau [LGR] points).  The mesh consists of intervals into which the total time interval <math>t^{(p)}\\in[t_0^{(p)},t_f^{(p)}]</math> in each phase is divided, and LGR collocation is performed in each interval.  Because the mesh can be adapted such that both the degree of the polynomial used to approximate the state <math>\\mathbf{y}^{(p)}(t)</math> and the width of each mesh interval can be different from interval to interval, the method is referred to as an <math>hp</math>-adaptive method (where \"<math>h</math>\" refers to the width of each mesh interval, while \"<math>p</math>\" refers to the polynomial degree in each mesh interval).  The LGR collocation method has been developed rigorously in Refs.,<ref name=Garg-Unified>{{cite journal|last1=Garg|first1=D.|last2=Patterson|first2=M. A.|last3=Hager|first3=W. W.|last4=Rao|first4=A. V.|last5=Benson|first5=D. A.|last6=Huntington|first6=G. T.|title=A Unified Framework for the Numerical Solution of Optimal Control Problems Using Pseudospectral Methods|journal=Automatica|date=2010|volume=46|issue=11|pages=1843‚Äì1851|doi=10.1016/j.automatica.2010.06.048}}</ref><ref name=Garg-Infinite>{{cite journal|last1=Garg|first1=D.|last2=Hager|first2=W. W.|last3=Rao|first3=A. V.|title=Pseudospectral Methods for Solving Infinite-Horizon Optimal Control Problems|journal=Automatica|date=2011|volume=47|issue=4|pages=829‚Äì837|doi=10.1016/j.automatica.2011.01.085|display-authors=etal}}</ref><ref name=Garg-Radau>{{cite journal|last1=Garg|first1=D.|last2=Patterson|first2=M. A.|last3=Darby|first3=C. L.|last4=Francolin|first4=C.|last5=Huntington|first5=G. T.|last6=Hager|first6=W. W.|last7=Rao|first7=A. V.|title=Direct Trajectory Optimization and Costate Estimation of Finite-Horizon and Infinite-Horizon Optimal Control Problems Using a Radau Pseudospectral Method|journal=Computational Optimization and Applications|date=2011|volume=49|issue=2|pages=335‚Äì358|display-authors=etal|doi=10.1007/s10589-009-9291-0|citeseerx=10.1.1.663.4215}}</ref> while <math>hp</math>-adaptive mesh refinement methods based on the LGR collocation method can be found in Refs., .<ref name=Darby-hp1>{{cite journal|last1=Darby|first1=C. L.|last2=Hager|first2=W. W.|last3=Rao|first3=A. V.|title=An hp-Adaptive Pseudospectral Method for Solving Optimal Control Problems|journal=Optimal Control Applications and Methods|date=2011|volume=32|issue=4|pages=476‚Äì502|doi=10.1002/oca.957|display-authors=etal}}</ref><ref name=Darby-hp2>{{cite journal|last1=Darby|first1=C. L.|last2=Hager|first2=W. W.|last3=Rao|first3=A. V.|title=Direct Trajectory Optimization Using a Variable Low-Order Adaptive Pseudospectral Method|journal=Journal of Spacecraft and Rockets|date=2011|volume=48|issue=3|pages=433‚Äì445|doi=10.2514/1.52136|display-authors=etal|citeseerx=10.1.1.367.7092}}</ref><ref name=Patterson-ph>{{cite journal|last1=Patterson|first1=M. A.|last2=Hager|first2=W. W.|last3=Rao|first3=A. V.|title=A ph Mesh Refinement Method for Optimal Control|journal=Optimal Control Applications and Methods|date=2011|volume=36|issue=4|pages=398‚Äì421|doi=10.1002/oca.2114}}</ref><ref name=Liu-hp>{{cite journal|last1=Liu|first1=F.|last2=Hager|first2=W. W.|last3=Rao|first3=A. V.|title=Adaptive Mesh Refinement for Optimal Control Using Nonsmoothness Detection and Mesh Size Reduction|journal=Journal of the Franklin Institute - Engineering and Applied Mathematics|date=2015|volume=352|issue=10|pages=4081‚Äì4106|doi=10.1016/j.jfranklin.2015.05.028}}</ref>\n\n== Development ==\nThe development of GPOPS-II began in 2007.  The code development name for the software was ''OptimalPrime'', but was changed to GPOPS-II in late 2012 in order to keep with the lineage of the original version of GPOPS <ref name=GPOPS>{{cite journal|last1=Rao|first1=A. V.|last2=Benson|first2=D. A.|last3=Darby|first3=C. L.|last4=Patterson|first4=M. A.|last5=Francolin|first5=C.|last6=Sanders|first6=I.|last7=Huntington|first7=G. T.|title=GPOPS: A MATLAB Software for Solving Multiple-Phase Optimal Control Problems Using the Gauss Pseudospectral Method|journal=ACM Transactions on Mathematical Software|date=2010|volume=37|issue=2|pages=22:1‚Äì22:39|doi=10.1145/1731022.1731032}}</ref> which implemented global collocation using the [[Gauss pseudospectral method]].  The development of GPOPS-II continues today, with improvements that include the open-source algorithmic differentiation package ADiGator <ref>{{cite web|last1=Weinstein|first1=M. J.|last2=Rao|first2=A. V.|title=ADiGator: A MATLAB Toolbox for Algorithmic Differentiation Using Source Transformation via Operator Overloading|url=http://sourceforge.net/projects/adigator|website=ADiGator}}</ref> and continued development of <math>hp</math>-adaptive mesh refinement methods for optimal control.\n\n== Applications of GPOPS-II ==\nGPOPS-II has been used extensively throughout the world both in academia and industry.  Published academic research where GPOPS-II has been used includes Refs.,<ref name=Perantoni-Part-I>{{cite journal|last1=Perantoni|first1=G.|last2=Limebeer|first2=D. J. N.|title=Optimal Control of a Formula One Car on a Three-Dimensional Track. Part 1: Track Modelling and Identification|journal=ASME Journal of Dynamic Systems, Measurement, and Control|date=2015|volume=In Press|issue=2|doi=10.1115/1.4028253|pages=021010}}</ref><ref name=Limebeer-Part-II>{{cite journal|last1=Limebeer|first1=D. J. N.|last2=Perantoni|first2=G.|title=Optimal Control of a Formula One Car on a Three-Dimensional Track Part 2: Optimal Control|journal=ASME Journal of Dynamic Systems, Measurement, and Control|date=2015|volume=In Press|issue=5|doi=10.1115/1.4029466|pages=051019}}</ref><ref name=Limebeer-KERS>{{cite journal|last1=Limebeer|first1=D. J. N.|last2=Perantoni|first2=G.|last3=Rao|first3=A. V.|title=Optimal Control of Formula One Car Energy Recovery Systems|journal=International Journal of Control|date=2014|volume=87|issue=10|pages=2065‚Äì2080|doi=10.1080/00207179.2014.900705}}</ref> where the software has been used in applications such as performance optimization of Formula One race cars, Ref.<ref>{{cite journal|last1=Graham|first1=K. F.|last2=Rao|first2=A. V.|title=Minimum-Time Trajectory Optimization of Many Revolution Low-Thrust Earth-Orbit Transfers|journal=Journal of Spacecraft and Rockets|date=2015|volume=52|issue=3|pages=711‚Äì727|doi=10.2514/1.a33187}}</ref> where the software has been used for minimum-time optimization of low-thrust orbital transfers,<ref>{{cite journal|last1=Dahmen|first1=T.|last2=Saupeand|first2=D.|title=Optimal pacing strategy for a race of two competing cyclists|journal=Journal of Science and Cycling|date=2014|volume=3|issue=2}}</ref> where the software has been used for human performance in cycling, Ref.<ref name=Moon-Kwon>{{cite journal|last1=Moon|first1=Y|last2=Kwon|first2=S|title=Lunar Soft Landing with Minimum-Mass Propulsion System Using H2O2/Kerosene Bipropellant Rocket System|journal=Acta Astronautica|volume=99|issue=May - June|pages=153‚Äì157|doi=10.1016/j.actaastro.2014.02.003|year=2014}}</ref> where the software has been used for soft lunar landing, and Ref.<ref name=Haberland>{{cite journal|last1=Haberland|first1=M.|last2=McClelland|first2=H.|last3=Kim|first3=S.|last4=Hong|first4=D.|title=The Effect of Mass Distribution on Bipedal Robot Efficiency|journal=International Journal of Robotics Research|volume=25|issue=11|pages=1087‚Äì1098|doi=10.1177/0278364906072449|year=2006}}</ref> where the software has been used to optimize the motion of a bipedal robot.\n\n== References ==\n{{Reflist|30em}}\n\n== External links ==\n* [http://www.gpops2.com GPOPS-II home page]\n* [http://dl.acm.org/citation.cfm?id=2558904 GPOPS-II Journal Article Appearing in the ACM Transactions on Mathematical Software]\n* {{sourceforge|adigator}}\n\n{{Mathematical optimization software}}\n\n[[Category:Mathematical optimization software]]\n[[Category:Optimal control]]\n[[Category:Mathematical software]]\n[[Category:Numerical software]]"
    },
    {
      "title": "Gurobi",
      "url": "https://en.wikipedia.org/wiki/Gurobi",
      "text": "{{Infobox Software\n| name                   = Gurobi \n| developer              = Gurobi Optimization \n| latest_release_version = 8.1<ref>[http://www.gurobi.com/products/whats-new/whats-new-in-the-latest-version Current Release Enhancements - Gurobi Optimization]</ref>\n| genre                  = [[Operations Research]], [[Optimization (mathematics)|Mathematical optimization]]\n| license                = [[Proprietary software|Proprietary]]\n| website                = {{URL|gurobi.com}}\n| AsOf                   = 2019-05-07\n}}\n\nThe '''Gurobi''' Optimizer is a commercial [[Optimization (mathematics)|optimization]] [[solver]] for [[linear programming]] (LP), [[quadratic programming]] (QP), quadratically constrained programming (QCP), [[mixed integer linear programming]] (MILP), mixed-integer quadratic programming (MIQP), and mixed-integer quadratically constrained programming (MIQCP). \n\nGurobi was founded in 2008<ref>[http://www.gurobi.com/company/management-team (Gurobi) \"Management Team\"]</ref> and is named for its founders: Zonghao '''Gu''', Edward '''Ro'''thberg and Robert '''Bi'''xby. Bixby was also the founder of [[CPLEX]], while Rothberg and Gu led the CPLEX development team for nearly a decade.<ref>[http://www.gurobi.com/company/management-team \"Gurobi Management Team Overview\"]</ref>\n\n== Features ==\nThe Gurobi Optimizer supports a variety of programming and modeling languages including:<ref>[http://www.gurobi.com/products/gurobi-optimizer/gurobi-overview \"Gurobi Optimizer Product Overview\"]</ref>\n\n* Object-oriented interfaces for [[C++]], [[Java (programming language)|Java]], [[.NET Framework|.NET]], and [[Python (programming language)|Python]]\n* Matrix-oriented interfaces for [[C (programming language)|C]], [[MATLAB]], and [[R (programming language)|R]]\n* Links to standard modeling languages: [[AIMMS]], [[AMPL]], [[General Algebraic Modeling System|GAMS]], and [[Mathematical Programming Language|MPL]]\n* Links to [[Microsoft Excel|Excel]] through their [https://www.solver.com/premium-solver-platform Analytic Solver] and [https://www.solver.com/solver-sdk-platform Solver SDK] products\n\nThe Gurobi Optimizer also includes a number of features to support the building of optimization models including support for:<ref>[http://www.gurobi.com/products/features-benefits \"Gurobi Optimizer Features Overview\"]</ref>\n\n* Multiple objectives with flexibility in how they are prioritized\n* General constraints such as MIN/MAX, ABS, AND/OR, and indicator constraints help avoid having to turn commonly occurring constraints in linear constraints\n* Models with convex, piecewise-linear objective functions, to capture certain non-linear problems\n* Arbitrary piecewise-linear objective functions, to make it easier to express this common modeling feature\n* Distributed tuning, to speed the exploration of parameter settings to speed solve times\n\nThe Gurobi Optimizer also has options to deploy on the cloud <ref>[http://www.gurobi.com/products/gurobi-cloud \"Gurobi Cloud\"]</ref> and for client-server computing.<ref>[http://www.gurobi.com/products/gurobi-compute-server/gurobi-compute-server \"Gurobi Compute Server\"]</ref>\n\n==See also==\n* [[CPLEX]]\n* [[GLPK]]\n* [[SCIP (optimization software)]]\n* [[FICO Xpress]]\n\n== References ==\n{{reflist}}\n\n== External links ==\n* {{Official website|www.gurobi.com}}\n* [http://www.gurobi.com/products/licensing-and-pricing/licensing-overview Licensing options]\n\n\n{{Mathematical optimization software}}\n[[Category:Numerical software]]\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "JModelica.org",
      "url": "https://en.wikipedia.org/wiki/JModelica.org",
      "text": "{{Infobox software\n| name = JModelica.org\n| developer = Modelon AB\n| latest release version = 2.4\n| latest release date = {{Start date and age|2018|09|24|df=yes/no}}<ref name=\"\nChangeset 11652\">{{cite web |url= https://trac.jmodelica.org/changeset/11652|title=Tagging JModelica release 2.4|publisher=Christian Andersson|accessdate=24 December 2018}}</ref>\n| repo = {{URL|https://trac.jmodelica.org/wiki}}\n| programming language = [[C (programming language)|C]], [[Python (programming language)|Python]], [[C++]], [[Java (programming language)|Java]]\n| operating system = [[Linux]], [[Microsoft Windows|Windows]] and [[OS X]]\n| genre = Dynamic simulation and optimization\n| license = [[GNU General Public License|GPL]] ([[free software]])\n| website = {{URL|www.jmodelica.org}}\n}}\n'''JModelica.org''' is a free and [[open source software]] platform based on the [[Modelica]] modeling language for modeling, simulating, optimizing and analyzing complex dynamic systems.<ref>Johan √Ökesson, Karl-Erik √Örz√©n, Magnus G√§fvert, Tove Bergdahl, Hubertus Tummescheit: [http://www.control.lth.se/Publication/ake+10cace.html \"Modeling and Optimization with Optimica and JModelica.org‚ÄîLanguages and Tools for Solving Large-Scale Dynamic Optimization Problem\"]. Computers and Chemical Engineering, 34:11, pp. 1737-1749, November 2010.</ref> The platform is maintained and developed by Modelon AB in collaboration with academic and industrial institutions, notably [[Lund University]] and the Lund Center for Control of Complex Systems (LCCC).<ref>{{cite web \n| url=http://www.lccc.lth.se | title = Lund Center for Control of Complex Systems (LCCC)}}</ref> The platform has been used in industrial projects with applications in robotics,<ref>Bj√∂rn Olofsson, Henrik Nilsson, Anders Robertsson, Johan √Ökesson:[http://www.control.lth.se/Publication/bo+IFAC2011.html \"Optimal Tracking and Identification of Paths for Industrial Robots\"]. In Proc. 18th World Congress of the International Federation of Automatic Control (IFAC), Milano, Italy, August 2011.</ref> vehicle systems,<ref>Tomas Gustafsson: [http://www.vehicular.isy.liu.se/Publications/MSc/08_EX_4074_TG.html \"Computing the Ideal Racing Line Using Optimal Control\"]. Link√∂ping University, 2008</ref> energy systems,<ref>Francesco Casella, Filippo Donida, Johan √Ökesson: [http://www.control.lth.se/Publication/cas+11ifac.html \"Object-Oriented Modeling and Optimal Control: A Case Study in Power Plant Start-Up\"]. In Proc. of 18th World Congress of the International Federation of Automatic Control (IFAC), August 2011.</ref> CO2 separation<ref>Johan √Ökesson, R Faber, Carl Laird, Katrin Pr√∂lss, Hubertus Tummescheit, St√©phane Velut, Yu Zhu: [http://www.control.lth.se/Publication/ake+11mod11.html \"Models of a post-combustion absorption unit for simulation, optimization and non-linear model predictive control schemes\"]. In 8th International Modelica Conference, March 2011.</ref> and polyethylene production.<ref>Per-Ola Larsson, Johan √Ökesson, Staffan Haugwitz, Niklas Andersson: [http://www.control.lth.se/Publication/pol+11IFAC.html \"Modeling and Optimization of Grade Changes for Multistage Polyethylene Reactors\"]. In Proc. of 18th World Congress of the International Federation of Automatic Control (IFAC), September 2011.</ref> \n\nThe key components of the platform are:\n* A Modelica compiler for translating Modelica source code into C or XML code. The compiler also generates models compliant with the [[Functional Mock-up Interface]] standard.\n* A [[Python (programming language)|Python]] package for simulation of dynamic models, Assimulo. Assimulo provides interfaces to several state of the art integrators and is used as a simulation engine in JModelica.org.\n* Algorithms for solving large scale dynamic optimization problems implementing local  [[collocation method]]s on finite elements and [[Gauss pseudospectral method|pseudospectral]] [[collocation method]]s.\n* A Python package for user interaction. All parts of the platform are accessed from Python, including compiling and loading models, simulating and optimizing.\n* An Eclipse plug-in for editing of Modelica source code.\n\nJModelica.org supports the Modelica modeling language for modeling of physical systems. Modelica provides high-level descriptions of hybrid dynamic systems, which are used as a basis for different kinds of computations in JModelica.org including simulation, sensitivity analysis and optimization.\n\nDynamic optimization problems, including [[optimal control]], [[trajectory optimization]], parameter optimization and model calibration can be formulated and solved using JModelica.org. The Optimica extension<ref>Johan √Ökesson: [http://www.control.lth.se/Publication/ake08mod08.html \"Optimica‚ÄîAn Extension of Modelica Supporting Dynamic Optimization\"]. In In 6th International Modelica Conference 2008, Modelica Association, March 2008.</ref> enables high-level formulation of dynamic optimization problems based on Modelica models. The mintOC project<ref>{{cite web \n| url= http://mintoc.de/index.php/Main_Page\n| title = The mintOC project}}</ref> provides a number of benchmark problems encoded in Optimica.\n\nThe platform promotes open interfaces for integration with numerical packages. The Sundials<ref>{{cite web \n| url=https://computation.llnl.gov/casc/sundials/main.html\n| title = The Sundials project}}</ref> ODE/DAE integrator suite, the NLP solver [[IPOPT]] and the AD package [[CasADi]] are examples of packages that are integrated into the JModelica.org platform.\n\nJModelica.org is compliant with the [[Functional Mock-up Interface]] (FMI) standard and Functional Mock-up Units (FMUs), generated by JModelica.org or by another FMI-compliant tool, can be simulated in the Python environment.\n\nAn independent comparison between JModelica.org and the optimization systems ACADO Toolkit,<ref>{{cite web \n| url= http://www.acadotoolkit.org\n| title = The ACADO Toolkit project}}</ref> IPOPT, and CppAD, is provided in the report Open-Source Software for Nonlinear Constrained Optimization of Dynamic Systems.<ref>Rune Brus:\n[http://etd.dtu.dk/thesis/262234/ep10_26_net_new.pdf \"Open-Source Software for Nonlinear Constrained Optimization of Dynamic Systems\"]. Technical University of Denmark, Department of Informatics and Mathematical Modeling, Scientific Computing. 2010.</ref>\n\n==See also==\n*[[AMESim]]\n*[[AMPL]]\n*[[APMonitor]]\n*[[ASCEND]]\n*[[Dymola]]\n*[[General Algebraic Modeling System]] (GAMS)\n*[[MapleSim]]\n*[[Wolfram SystemModeler]]\n*[[Openmodelica]]\n*[[SimulationX]]\n*[[PROPT]]\n\n\n==References==\n{{Reflist|2}}\n\n[[Category:Simulation software]]\n[[Category:Simulation programming languages]]\n[[Category:Mathematical optimization software]]\n[[Category:Free simulation software]]\n[[Category:Declarative programming languages]]\n[[Category:Object-oriented programming]]\n[[Category:Free software programmed in Python]]"
    },
    {
      "title": "LIONsolver",
      "url": "https://en.wikipedia.org/wiki/LIONsolver",
      "text": "{{COI|date=December 2014}}\n{{Advert|date=March 2019}}\n{{Infobox Software\n| name                   = LIONsolver\n| screenshot             = \n| caption                = \n| developer              = Reactive Search srl\n| latest release version = 2.0.198\n| latest release date    = {{release date and age|2011|10|9}}\n| latest preview version = \n| latest preview date    = \n| operating system       = [[Microsoft Windows|Windows]] , [[Mac OS X Lion|Mac OS X]], [[Unix]]\n| language               = [[English language|English]]\n| genre                  = [[Business intelligence software]]\n| license                = [[Proprietary software]],  free for academic use\n| website                = {{url|http://lionoso.com/}}\n}}\n\n'''LIONsolver''' is an integrated software for [[data mining]], [[business intelligence]], [[analytics]], and [[Modeling and simulation|modeling]] \n'''Learning and Intelligent OptimizatioN'''<ref>{{cite book\n|title=The LION way. Machine Learning plus Intelligent Optimization. \n|last=Battiti\n|first=Roberto\n|authorlink=\n|author2=Mauro Brunato\n |year=2014\n|publisher=LIONlab, University of Trento\n|location= Trento, Italy\n|isbn=978-14-960340-2-1\n|url=http://intelligent-optimization.org/LIONbook/\n}}\n</ref> and [[reactive business intelligence]] approach.<ref>{{cite book\n|title=Reactive Search and Intelligent Optimization\n|last=Battiti\n|first=Roberto\n|authorlink=\n|author2=Mauro Brunato |author3=Franco Mascia\n |year=2008\n|publisher=[[Springer Verlag]]\n|location=\n|isbn=978-0-387-09623-0\n}}\n</ref> A non-profit version is available as '''LIONoso'''.\n\nLIONsolver can be used to build models, visualize them, and improve business and engineering processes. \nIt is a tool for decision making based on data and quantitative models, it can be connected to most databases\nand external programs, it is fully integrated with the [[Grapheur]] business intelligence  software and intended for more advanced users, interested in designing business logic and processes and not only in simple analytics and visualization tasks.\n\n==Overview==\n\nLIONsolver originates from research principles in Reactive Search Optimization<ref>{{cite journal\n| last        =Battiti\n| first       =Roberto\n|author2=Gianpietro Tecchiolli\n| year        =1994\n| title       =The reactive tabu search.\n| journal     =ORSA Journal on Computing\n| volume      =6\n| issue       =2\n| pages       =126‚Äì140\n| doi         =10.1287/ijoc.6.2.126\n| url         =http://rtm.science.unitn.it/~battiti/archive/TheReactiveTabuSearch.PDF\n| format      =PDF\n}}\n</ref> advocating the use of self-tuning schemes acting while a software\nsystem is running. '''Learning and Intelligent OptimizatioN''' refers to the integration\nof online [[machine learning]] schemes into the optimization software, so that\nit becomes capable of learning from its previous runs and from human feedback. \nA related approach is that of Programming by Optimization,<ref>{{cite journal\n| last        =Holger\n| first       =Hoos\n| year        =2012\n| title       =Programming by optimization.\n| journal     =Communications of the ACM\n| volume      =55\n| issue       =2\n| pages       =70‚Äì80\n| doi         = 10.1145/2076450.2076469\n| url         =http://doi.acm.org/10.1145/2076450.2076469\n| format      =PDF\n}}\n</ref>\nwhich provides a direct way of defining design spaces involving Reactive Search Optimization, and\nof Autonomous Search\n<ref>{{cite book\n|title=Autonomous Search\n|last=Youssef\n|first=Hamadi\n|authorlink= |author2=E. Monfroy |author3=F. Saubion\n|year=2012\n|publisher=[[Springer Verlag]]\n|location=New York\n|isbn=978-3-642-21433-2\n}}\n</ref> advocating adapting problem-solving algorithms.\n\nVersion 2.0 of the software was released on Oct 1, 2011, covering also the Unix and Mac OS X operating\nsystems in addition to Windows.\n\nThe modeling components include neural networks, polynomials, locally weighted Bayesian regression, k-means clustering, and self-organizing maps. A free academic license for non-commercial use and class use is available.\n\nThe software architecture of LIONsolver<ref>{{cite journal\n| last        =Battiti\n| first       =Roberto\n|author2=Mauro Brunato\n| year        =2010\n| title       =Grapheur: A Software Architecture for Reactive and Interactive Optimization .\n|trans-title=Proceedings Learning and Intelligent OptimizatioN LION 4, Jan 18-22, 2010, Venice, Italy.\n| journal     =Lecture Notes in Computer Science\n| volume      =6073\n| pages       =232‚Äì246\n| doi         = 10.1007/978-3-642-13800-3\n| url         =http://rtm.science.unitn.it/~battiti/archive/grapheur-lion4.pdf\n| format      =PDF\n}}\n</ref> \npermits interactive [[multi-objective optimization]], with a user interface for visualizing the results and facilitating\nthe solution analysis and decision making process.\nThe architecture allows for problem-specific extensions, and it is\napplicable as a post-processing tool for all optimization schemes with a number of\ndifferent potential solutions. When the architecture is tightly coupled to a specific\nproblem-solving or optimization method, effective interactive schemes where the\nfinal decision maker is in the loop can be developed.<ref>\n{{cite journal\n| last        =Battiti\n| first       =Roberto\n|author2=Andrea Passerini \n| year        =2010\n| title       =Brain-Computer Evolutionary Multi-Objective Optimization (BC-EMO): a genetic algorithm adapting to the decision maker.\n| journal     =IEEE Transactions on Evolutionary Computation\n| volume      =14\n| issue       =15\n| pages       =671‚Äì687\n| doi         =10.1109/TEVC.2010.2058118 \n| url         =http://rtm.science.unitn.it/~battiti/archive/bcemo.pdf\n| format      =PDF\n}}\n</ref>\n\nOn Apr 24, 2013 LIONsolver received the first prize of the [[The Michael J. Fox Foundation|Michael J. Fox Foundation]] ‚Äì\n[[Kaggle]] Parkinson's Data Challenge, a contest leveraging \"the wisdom of the crowd\" to benefit people with [[Parkinson's disease]].<ref>{{cite web|title=\"Machine Learning Approach\" to Smartphone Data Garners $10,000 First Prize in The Michael J. Fox Foundation Parkinson's Data Challenge|date=April 24, 2013|url=https://www.michaeljfox.org/foundation/publication-detail.html?id=473&category=7|publisher=MJFF}}</ref>\n\n== See also ==\n* [[Multi-objective optimization]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://lionoso.com/ LIONsolver official non-profit site]\n\n[[Category:Time series software]]\n[[Category:Data analysis software]]\n[[Category:Data visualization software]]\n[[Category:Mathematical optimization software]]\n[[Category:Numerical software]]"
    },
    {
      "title": "Maple (software)",
      "url": "https://en.wikipedia.org/wiki/Maple_%28software%29",
      "text": "{{Other uses|Maple (disambiguation)}}\n{{Infobox software\n|                   name = Maple\n|                   logo = \n|             screenshot = Maple 2016 Core Screenshots.jpg\n|                caption = Maple interface\n|              developer = [[Waterloo Maple]] (Maplesoft)\n|               released = 1982\n| latest release version = {{Latest stable software release/Maple}}\n|   programming language = [[C (programming language)|C]], [[Java (programming language)|Java]], Maple\n|                  genre = [[Computer algebra system]], [[Numeric computation]]\n|                license = [[Proprietary software|Proprietary]] [[commercial software]]\n|               platform = [[Microsoft Windows|Windows]] (7, 8 and 10), [[macOS]], [[Linux]]\n|               language = [[English language|English]], [[Japanese language|Japanese]], and limited support in additional languages<ref>{{cite web|url=https://www.maplesoft.com/products/languages/|title=International Language Support in Maple|publisher=[[Maplesoft]]|accessdate=2 June 2016}}</ref>\n|                website = {{URL|www.maplesoft.com/products/maple/}}}}\n\n'''Maple''' is a [[Symbolic computation|symbolic]] and [[Numerical analysis|numeric]] computing environment, and is also a [[Programming paradigm#Multi-paradigm|multi-paradigm programming language]].\n\nDeveloped by [[Maplesoft]], Maple also covers other aspects of technical computing, including visualization, data analysis, matrix computation, and connectivity.\n\nA toolbox, [[MapleSim]], adds functionality for multidomain physical modeling and code generation.\n\n==Overview==\n===Core functionality===\nUsers can enter mathematics in traditional [[mathematical notation]].  Custom user interfaces can also be created. There is support for numeric computations, to arbitrary precision, as well as symbolic computation and visualization.  Examples of symbolic computations are given below.\n\nMaple incorporates a [[Type system#Dynamic typing|dynamically typed]] imperative-style [[programming language]] which resembles [[Pascal (programming language)|Pascal]].<ref>[http://www.bitwisemag.com/copy/reviews/software/maths/maple10_mathematica52.html Power of two] Bitwise Magazine</ref> The language permits variables of lexical [[scope (programming)|scope]].   There are also interfaces to other languages ([[C (programming language)|C]], [[C Sharp (programming language)|C#]], [[Fortran]], [[Java (programming language)|Java]], [[MATLAB]], and [[Visual Basic]]).  There is also an interface to [[Microsoft Excel|Excel]].\n\nMaple supports [[MathML]] 2.0, a W3C format for representing and interpreting mathematical expressions, including their display in Web pages.<ref>http://www.maplesoft.com/standards/MathML/info.html</ref>\n\n===Architecture===\nMaple is based on a small [[Kernel (computing)|kernel]], written in [[C (programming language)|C]], which provides the Maple language. Most functionality is provided by libraries, which come from a variety of sources. Most of the libraries are written in the Maple language; these have viewable source code.  Many numerical computations are performed by the [[NAG Numerical Libraries]], [[Automatically Tuned Linear Algebra Software|ATLAS]] libraries, or [[GNU Multi-Precision Library|GMP]] libraries.\n\nDifferent functionality in Maple requires numerical data in different formats. Symbolic expressions are stored in memory as [[directed acyclic graph]]s. The standard interface and calculator interface are written in [[Java (programming language)|Java]].\n\n==History==\n<!----This section was created by a COI editor and may not be a neutral point of view --->\n\nThe first concept of Maple arose from a meeting in November 1980 at the [[University of Waterloo]]. Researchers at the university wished to purchase a computer powerful enough to run [[Macsyma]]. Instead, it was decided that they would develop their own computer algebra system that would be able to run on lower cost computers. The first limited version appearing in December 1980 with Maple demonstrated first at conferences beginning in 1982. The name is a reference to Maple's [[Flag of Canada|Canadian heritage]]. By the end of 1983, over 50 universities had copies of Maple installed on their machines.\n\nIn 1984, the research group arranged with [[Watcom|Watcom Products Inc]] to license and distribute the first commercially available version, Maple 3.3.<ref>[http://zakuski.math.utsa.edu/~gokhman/ftp/mirrors/maple/mplhist.htm History of Maple] Alexander F. Walz, 1998</ref> In 1988 [[Waterloo Maple]] Inc. was founded. The company‚Äôs original goal was to manage the distribution of the software. Eventually, the company evolved to have an R&D department where most of Maple's development is done today with the rest done at university research labs worldwide including: the Symbolic Computation Laboratory at the [[University of Waterloo]] and the Ontario Research Centre for Computer Algebra at the [[University of Western Ontario]]{{Who|date=August 2011}}.\n\n<!----End of questionable neutrality section--->\n<!-- Deleted image removed: [[Image:Maple morphos.png|thumb|300px|Maple V Release 3 for [[Amiga]] (1995), running under [[MorphOS]] ]] -->\nIn 1989, the first graphical user interface for Maple was developed and included with version 4.3 for the [[Macintosh]]. X11 and Windows versions of the new interface followed in 1990 with Maple V. In 1992, Maple V Release 2 introduced the Maple \"worksheet\" that combined text, graphics, and input and typeset output.<ref>[http://www.maplesoft.com/support/help/Maple/view.aspx?path=updates/v52 Maple V Release 2 Notes] Maplesoft</ref> In 1994 a special issue of a newsletter created by Maple developers called ''MapleTech'' was published.<ref>[http://web.mit.edu/maple/www/plibrary/mtn/mtn-si94.html MapleTech Special Issue, Birkh√§user-Boston, (1994)]</ref>\n\nIn 1999, with the release of Maple 6, Maple included some of the [[NAG Numerical Libraries]].<ref>[http://www.macworld.com/article/1870/2001/02/21reviewsmaple.html Maple 6.0] Macworld, Feb 2001</ref>  In 2003, the current \"standard\" interface was introduced with Maple 9. This interface is primarily written in [[Java (programming language)|Java]] (although portions, such as the rules for typesetting mathematical formulae, are written in the Maple language). The Java interface was criticized for being slow;<ref>[http://www.scientific-computing.com/scwoctnov05review.html Capturing knowledge with pure maths], Scientific Computing World.</ref> improvements have been made in later versions, although the Maple&nbsp;11 documentation<ref>[http://www.maplesoft.com/documentation_center/maple11/Install.html Maple 11 Installation Guide] Maplesoft</ref> recommends the previous (\"classic\") interface for users with less than 500&nbsp;MB of physical memory.\n\nBetween the mid 1995 and 2005 Maple lost significant market share to competitors due to a weaker user interface.<ref>[http://history.siam.org/oralhistories/gonnet.htm Interview with Gaston Gonnet, co-creator of Maple] {{webarchive|url=https://web.archive.org/web/20071229044836/http://history.siam.org/oralhistories/gonnet.htm |date=2007-12-29 }}, SIAM History of Numerical Analysis and Computing, 16 March 2005</ref> In 2005, Maple 10 introduced a new \"document mode\", as part of the standard interface that it has been further developed over the following years.\n\nIn September 2009 Maple and Maplesoft were acquired by the Japanese software retailer [[Cybernet Systems]].\n\n==Version history==\n{{Div col|colwidth=20em}}\n*    Maple 1.0: January, 1982\n*    Maple 1.1: January, 1982\n*    Maple 2.0: May, 1982\n*    Maple 2.1: June, 1982\n*    Maple 2.15: August, 1982\n*    Maple 2.2: December, 1982\n*    Maple 3.0: May, 1983\n*    Maple 3.1: October, 1983\n*    Maple 3.2: April, 1984\n*    Maple 3.3: March, 1985 (first public available version)\n*    Maple 4.0: April, 1986\n*    Maple 4.1: May, 1987\n*    Maple 4.2: December, 1987\n*    Maple 4.3: March, 1989\n*    Maple V: August, 1990\n*    Maple V R2: November 1992\n*    Maple V R3: March 15, 1994\n*    Maple V R4: January, 1996\n*    Maple V R5: November 1, 1997\n*    Maple 6: December 6, 1999\n*    Maple 7: July 1, 2001\n*    Maple 8: April 16, 2002\n*    Maple 9: June 30, 2003\n*    Maple 9.5: April 15, 2004\n*    Maple 10: May 10, 2005\n*    Maple 11: February 21, 2007\n*    Maple 11.01: July, 2007\n*    Maple 11.02: November, 2007\n*    Maple 12: May, 2008\n*    Maple 12.01: October, 2008\n*    Maple 12.02: December, 2008\n*    Maple 13: April 28, 2009<ref>{{cite web|url=https://www.mapleprimes.com/posts/37445-Maple-13-And-MapleSim-2-Now-Available|title=MaplePrimes Blog - Maple 13 and MapleSim 2 now available|access-date=28 Apr 2009}}</ref>\n*    Maple 13.01: July, 2009\n*    Maple 13.02: October, 2009\n*    Maple 14: April 29, 2010<ref>{{cite web|url=https://www.mapleprimes.com/posts/80867-Announcing-Maple-14-And-MapleSim-4-|title=MaplePrimes Blog - Announcing Maple 14 and MapleSim 4|access-date=29 Apr 2010}}</ref>\n*    Maple 14.01: October 28, 2010\n*    Maple 15: April 13, 2011<ref>{{cite web|url=https://www.mapleprimes.com/maplesoftblog/103907-Introducing-Maple-15|title=MaplePrimes Blog - Introducing Maple 15|access-date=11 Apr 2011}}</ref>\n*    Maple 15.01: June 21, 2011\n*    Maple 16: March 28, 2012<ref>{{cite web|url=https://www.mapleprimes.com/maplesoftblog/132249-Maple-16-Is-Here|title=MaplePrimes Blog - Maple 16 is here|access-date=28 Mar 2012}}</ref>\n*    Maple 16.01: May 16, 2012\n*    Maple 17: March 13, 2013<ref>{{cite web|url=https://www.mapleprimes.com/maplesoftblog/144580-Introducing-Maple-17|title=MaplePrimes Blog - Introducing Maple 17|access-date=13 Mar 2013}}</ref>\n*    Maple 17.01: July, 2013\n*    Maple 18: Mar 5, 2014<ref>{{cite web|url=https://www.mapleprimes.com/maplesoftblog/200200-Announcing-Maple-18|title=MaplePrimes Blog - Announcing Maple 18|access-date=5 Mar 2014}}</ref>\n*    Maple 18.01: May, 2014\n*    Maple 18.01a: July, 2014\n*    Maple 18.02: Nov, 2014\n*    Maple 2015.0: Mar 4, 2015<ref>{{cite web|url=https://www.mapleprimes.com/maplesoftblog/200713-Maple-2015-Is-Now-Available|title=MaplePrimes Blog - Maple 2015 is now available!|access-date=4 Mar 2015}}</ref>\n*    Maple 2015.1: Nov, 2015\n*    Maple 2016.0: March 2, 2016<ref>{{cite web|url=https://www.mapleprimes.com/maplesoftblog/202794-Announcing-Maple-2016|title=MaplePrimes Blog - Announcing Maple 2016|access-date=2 Mar 2016}}</ref>\n*    Maple 2016.1: April 20, 2016\n*    Maple 2016.1a: April 27, 2016\n*    Maple 2017.0: May 25, 2017<ref>{{cite web|url=https://www.mapleprimes.com/maplesoftblog/208276-Announcing-Maple-2017|title=MaplePrimes Blog - Announcing Maple 2017|access-date=25 May 2017}}</ref>\n*    Maple 2017.1: June 28, 2017\n*    Maple 2017.2: August 2, 2017\n*    Maple 2017.3: October 3, 2017\n*    Maple 2018.0: March 21, 2018<ref>{{cite web|url=https://www.mapleprimes.com/maplesoftblog/209095-Maple-2018-Is-Here-|title=MaplePrimes Blog - Maple 2018 is here!|access-date=21 Mar 2018}}</ref>\n*    Maple 2019.0: March 14, 2019<ref>{{cite web|url=https://www.mapleprimes.com/maplesoftblog/210286-Announcing-Maple-2019|title=MaplePrimes Blog - Announcing Maple 2019|access-date=14 Mar 2019}}</ref>\n{{div col end}}\n\n==Features==\nFeatures of Maple include:<ref>[https://www.maplesoft.com/products/Maple/features/ Maple Product Features Page]</ref>\n* Support for symbolic and numeric computation with [[Arbitrary-precision arithmetic|arbitrary precision]]\n* [[Elementary function|Elementary]] and [[Special functions|special mathematical function]] libraries\n* [[Complex number]]s and interval arithmetic\n* Arithmetic, [[greatest common divisor]]s and [[factorization]] for [[multivariate polynomial]]s over the rationals, [[finite field]]s, [[algebraic number field]]s, and [[algebraic function field]]s\n* Limits, series and [[asymptotic expansion]]s\n* [[Gr√∂bner basis|Groebner bases]]\n* Differential Algebra\n* [[Matrix operation#Basic operations|Matrix manipulation]] tools including support for [[sparse array]]s\n* Mathematical [[function graphing]] and animation tools\n* Solvers for [[systems of equations]], [[diophantine equation]]s, [[Ordinary differential equation|ODEs]], [[Partial differential equation|PDEs]], [[Differential algebraic equation|DAEs]], [[Delay differential equation|DDEs]] and [[recurrence relation]]s \n* Numeric and symbolic tools for discrete and continuous [[calculus]] including definite and [[indefinite integration]], definite and [[indefinite sum]]mation, automatic differentiation and continuous and discrete [[integral transform]]s\n* Constrained and unconstrained local and global [[Mathematical optimization|optimization]]\n* [[Statistics]] including model fitting, [[hypothesis testing]], and [[probability distribution]]s\n* Tools for data manipulation, visualization and analysis\n* Tools for probability and [[combinatoric]] problems\n* Support for time-series and unit based data\n* Connection to online collection of financial and economic data\n* Tools for financial calculations including bonds, annuities, derivatives, options etc.\n* Calculations and simulations on random processes\n* Tools for [[text mining]] including [[regular expressions]]\n* Tools for [[signal processing]] and linear and non-linear [[control system]]s\n* [[Discrete math]] tools including [[number theory]]\n* Tools for visualizing and analysing directed and undirected [[graph (discrete mathematics)|graphs]]\n* Group theory including permutation and finitely presented groups\n* Symbolic tensor functions\n* Import and export filters for data, image, sound, [[computer-aided design|CAD]], and document formats\n* Technical word processing including [[formula editor|formula editing]] \n* Programming language supporting [[Procedural programming|procedural]], [[Functional programming|functional]] and [[Object-oriented programming|object-oriented]] constructs\n* Tools for adding [[user interface]]s to calculations and applications\n* Tools for connecting to [[SQL]], [[Java (software platform)|Java]], [[.NET Framework|.NET]], [[C++]], [[Fortran]] and [[Hypertext Transfer Protocol|http]]\n* Tools for generating code for [[C (programming language)|C]], [[C Sharp (programming language)|C#]], [[Fortran]], [[Java (programming language)|Java]], [[JavaScript]], [[Julia (programming language)|Julia]], [[Matlab]], [[Perl]], [[Python (programming language)|Python]], [[R (programming language)|R]], and [[Visual Basic]]\n* Tools for [[parallel programming]]\n\n==Examples of Maple code==\nSample [[imperative programming]] constructs:<!-- <source lang=\"maple\"> not implemented, yet... -->\n<source lang=\"mupad\">\nmyfac := proc(n::nonnegint)\n   local out, i;\n   out := 1;\n   for i from 2 to n do\n       out := out * i\n   end do;\n   out\nend proc;\n</source>\nSimple functions can also be defined using the \"maps to\" arrow notation:\n<source lang=\"mupad\">\n myfac := n -> product( i, i=1..n );\n</source>\n\n===Integration===\nFind\n:<math>\\int\\cos\\left(\\frac{x}{a}\\right)dx</math>.\n int(cos(x/a), x);\nAnswer:\n:<math>a \\sin\\left(\\frac{x}{a}\\right)</math>\n----\n\n===Determinant===\nCompute the determinant of a matrix.\n<source lang=\"mupad\">\n M := Matrix([[1,2,3], [a,b,c], [x,y,z]]);  # example Matrix\n</source>\n: <math>\n  \\begin{bmatrix}\n    1 & 2 & 3 \\\\\n    a & b & c \\\\\n    x & y & z\n  \\end{bmatrix}\n</math>\n LinearAlgebra:-Determinant(M);\n: <math>bz-cy+3ay-2az+2xc-3xb</math>\n\n===Series expansion===\n series(tanh(x),x=0,15)\n\n:<math>x-\\frac{1}{3}\\,x^3+\\frac{2}{15}\\,x^5-\\frac{17}{315}\\,x^7</math>\n:<math>{}+\\frac{62}{2835}\\,x^9-\\frac{1382}{155925}\\,x^{11}+\\frac{21844}{6081075}\\,x^{13}+\\mathcal{O}\\left(x^{15}\\right)</math>\n\n===Solve equation numerically===\nHigh order polynomial equation\n<source lang=\"mupad\">\n f := x^53-88*x^5-3*x-5 = 0\n\n fsolve(f)\n\n -1.097486315, -.5226535640, 1.099074017\n</source>\n\n===Solve equation set===\n<source lang=\"mupad\">\n f := (cos(x+y))^2 + exp(x)*y+cot(x-y)+cosh(z+x) = 0:\n\n g := x^5 - 8*y = 2:\n\n h := x+3*y-77*z=55;\n                    \n fsolve( {f,g,h} );\n\n {x = -1.543352313, y = -1.344549481, z = -.7867142955}\n</source>\n\n===Plotting of function of single variable===\n* Plot <math>x \\cdot \\sin(x)</math> with <math>x</math> ranging from -10 to 10\n plot(x*sin(x),x=-10..10);\n[[Image:Maple1DPlot.PNG|thumb|center|300px]]\n\n===Plotting of function of two variables===\n* Plot <math>x^2+y^2</math> with <math>x</math> and <math>y</math> ranging from -1 to 1\n plot3d(x^2+y^2, x=-1..1, y=-1..1);\n[[Image:Maple163DPlot.jpg|thumb|center|300px]]\n\n===Animation of functions===\n* animation of function of two variables\n:<math>f := 2 \\cdot \\frac{k^2}{\\cosh\\left(k \\cdot \\left(x - 4 \\cdot k^2 \\cdot t\\right)\\right)^2}</math>\n plots:-animate(subs(k = .5, f), x=-30..30, t=-10..10, numpoints=200, frames=50, color=red, thickness=3);\n[[File:Bellsoliton2.gif|thumb|center|300px|2D bell solution]]\n\n* animation of functions of three variables\n plots:-animate3d(cos(t*x)*sin(3*t*y), x=-Pi..Pi, y=-Pi..Pi, t=1..2);\n[[File:3dsincos animation.gif|thumb|center|300px|3D animation of function]]\n\n* Fly-through animation of 3-D plots.<ref>[http://www.maplesoft.com/applications/view.aspx?SID=33073&view=html Using the New Fly-through Feature in Maple 13] Maplesoft</ref>\n M := Matrix([[400,400,200], [100,100,-400], [1,1,1]], datatype=float[8]):\n plot3d(1, x=0..2*Pi, y=0..Pi, axes=none, coords=spherical, viewpoint=[path=M]);\n[[File:Maple plot3D flythrough.gif|thumb|center|300px|Maple plot3D fly-through]]\n\n===Laplace transform===\n* [[Laplace transform]]\n\n f := (1+A*t+B*t^2)*exp(c*t);\n\n:<math> \\left(1 + A \\cdot t + B \\cdot t^2\\right) \\cdot e^{c \\cdot t}</math>\n\n inttrans:-laplace(f, t, s);\n\n:<math>\\frac{1}{s-c}+\\frac{A}{(s-c)^2}+\\frac{2B}{(s-c)^3}</math>\n\n* inverse Laplace transform\n\n inttrans:-invlaplace(1/(s-a),s,x)\n\n: <math>e^{ax}</math>\n\n===Fourier transform===\n<source lang=\"mupad\">\n inttrans:-fourier(sin(x),x,w)\n</source>\n:<math>\\mathrm{I}\\pi\\,(\\mathrm{Dirac}(w+1)-\\mathrm{Dirac}(w-1))</math>\n\n===Integral equations===\nFind functions <math>f</math> that satisfy the [[integral equation]]\n:<math>f(x)-3\\int_{-1}^1(xy+x^2y^2)f(y)dy = h(x)</math>.\n<source lang=\"mupad\">\n\n eqn:= f(x)-3*Int((x*y+x^2*y^2)*f(y), y=-1..1) = h(x):\n intsolve(eqn,f(x));\n</source>\n:<math>f \\left( x \\right) =\\int _{-1}^{1}\\! \\left( -15\\,{x}^{2}{y}^{2}-3\\,xy \\right) h \\left( y \\right) {dy}+h \\left( x \\right)\n</math>\n\n==Use of the Maple engine==\nThe Maple engine is used within several other products from [[Maplesoft]]:\n* Moebius, DigitalEd‚Äôs online testing suite, uses Maple to algorithmically generate questions and grade student responses.\n* MapleNet allows users to create [[JavaServer Pages|JSP]] pages and [[Java (programming language)|Java]] [[Applets]]. MapleNet 12 and above also allow users to upload and work with [[Notebook interface|Maple worksheets]] containing interactive components.\n* [[MapleSim]], an engineering simulation tool.<ref>{{cite journal|last1=Mahmud|first1=Khizir|last2=Town|first2=Graham E.|title=A review of computer tools for modeling electric vehicle energy requirements and their impact on power distribution networks|journal=Applied Energy|date=June 2016|volume=172|pages=337‚Äì359|doi=10.1016/j.apenergy.2016.03.100}}</ref>\n* Maple Quantum Chemistry Toolbox from RDMChem computes and visualizes the electronic energies and properties of molecules.<ref>{{cite web|url=https://www.mapleprimes.com/maplesoftblog/210517-Introducing-The-Maple-Quantum-Chemistry-Toolbox|title=MaplePrimes Blog - Introducing the Maple Quantum Chemistry Toolbox|access-date=6 May 2019}}</ref>\n\nListed below are third-party commercial products that no longer use the Maple engine:\n* Versions of [[Mathcad]] released between 1994 and 2006 included a Maple-derived algebra engine (MKM, aka [[Mathsoft]] Kernel Maple), though subsequent versions use [[MuPAD]].\n* Symbolic Math Toolbox in [[MATLAB]] contained a portion of the Maple 10 engine, but now uses [[MuPAD]] (starting with MATLAB R2007b+ release).<ref>{{cite web|title=Release Notes for Symbolic Math Toolbox|url=http://www.mathworks.com/help/symbolic/release-notes.html#brqy3xk|publisher=MathWorks|accessdate=10 July 2014}}</ref>\n* Older versions of the mathematical editor [[Scientific Workplace]] included Maple as a computational engine, though current versions include [[MuPAD]].\n\n==See also==\n{{div col|colwidth=30em}}\n* [[Comparison of computer algebra systems]]\n* [[Comparison of numerical analysis software]]\n* [[Comparison of programming languages]]\n* [[Comparison of statistical packages]]\n* [[List of computer algebra systems]]\n* [[List of computer simulation software]]\n* [[List of graphing software]]\n* [[List of numerical analysis software]]\n* [[Mathematical software]]\n* [[SageMath|SageMath (an open source algebra program)]]\n{{div col end}}\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n{{Commons category}}\n{{Wikibooks|Maple}}\n* [https://www.maplesoft.com Maplesoft, division of Waterloo Maple, Inc.] ‚Äì official website\n* [https://www.maplesoft.com/support/help/ Maple Online Help] ‚Äì online documentation\n* [https://www.mapleprimes.com MaplePrimes] ‚Äì a community website for Maple users\n* [https://maple.cloud MapleCloud] ‚Äì an online Maple application viewer\n\n{{Computer algebra systems}}\n{{Numerical analysis software}}\n{{Statistical software}}\n{{Deep_Learning_Software}}\n{{Fractal software}}\n{{Graph Analysis Software}}\n\n[[Category:C software]]\n[[Category:Computational notebook]]\n[[Category:Computer algebra system software for Linux]]\n[[Category:Computer algebra system software for MacOS]]\n[[Category:Computer algebra system software for Windows]]\n[[Category:Computer algebra systems]]\n[[Category:Cross-platform software]]\n[[Category:Data mining and machine learning software]]\n[[Category:Data visualization software]]\n[[Category:Data-centric programming languages]]\n[[Category:Econometrics software]]\n[[Category:Functional languages]]\n[[Category:Interactive geometry software]]\n[[Category:IRIX software]]\n[[Category:Linear algebra]]\n[[Category:Maplesoft]]\n[[Category:Mathematical optimization software]]\n[[Category:Mathematical software]]\n[[Category:Numerical analysis software for Linux]]\n[[Category:Numerical analysis software for MacOS]]\n[[Category:Numerical analysis software for Windows]]\n[[Category:Numerical programming languages]]\n[[Category:Numerical software]]\n[[Category:Parallel computing]]\n[[Category:Physics software]]\n[[Category:Plotting software]]\n[[Category:Products introduced in 1982]]\n[[Category:Proprietary commercial software for Linux]]\n[[Category:Proprietary cross-platform software]]\n[[Category:Regression and curve fitting software]]\n[[Category:Simulation programming languages]]\n[[Category:Software modeling language]]\n[[Category:Statistical programming languages]]\n[[Category:Theorem proving software systems]]\n[[Category:Time series software]]"
    },
    {
      "title": "MapleSim",
      "url": "https://en.wikipedia.org/wiki/MapleSim",
      "text": "{{Use mdy dates|date=September 2012}}\n{{Infobox Software\n| name                   = MapleSim\n| logo                   = \n| screenshot             = MapleSim_2016.2_Core_Screenshot.jpg\n| screenshot size        = 300px\n| caption                = Modeling and Simulation with MapleSim\n| developer              = [[Maplesoft]]\n| released               = {{Start date and age|2008|December|15}}<ref>http://www.maplesoft.com/products/maplesim/index.aspx</ref>\n| latest_release_version = 2019.1\n| programming language   = \n| operating system       = \n*[[Windows XP]] or later\n*[[Mac OS X 10.5]] or later\n*[[Red Hat Enterprise Linux]] 5 or later\n*[[OpenSUSE]] 10.3 or later\n*[[SUSE Linux Enterprise]] 10 or later\n*[[Ubuntu (operating system)|Ubuntu]] 10.04 or later\n| platform               = \n* [[IA-32|Intel x86 32-bit]], [[x86-64]], [[PowerPC G4]] and [[PowerPC G5]]\n* [[Maple (software)]]\n| language               = English and Japanese\n| genre                  = [[Mathematical model]]ing and [[Computer Simulation]]\n| license                = [[Proprietary software|Proprietary]] [[commercial software]]\n| website                = {{URL|http://www.maplesoft.com/products/maplesim/index.aspx}}\n}}\n\n'''MapleSim''' is a [[Modelica]]-based, multi-domain [[Mathematical model|modeling]] and [[Computer Simulation|simulation]] tool developed by [[Maplesoft]]. MapleSim generates model equations, runs simulations, and performs analyses using the symbolic and numeric mathematical engine of [[Maple (software)|Maple]]. Models are created by [[Drag-and-drop|dragging-and-dropping]] components from a library into a central workspace, resulting in a model that represents the physical system in a graphical form.\nMaplesoft began development of MapleSim partly in response to a request from [[Toyota Canada Inc.|Toyota]] to produce physical modeling tools to aid in their new model-based development process.<ref>[http://www.proe.com/feature_full.php?cpfeatureid=30189 The Simulation Landscape: Products and New Releases in Simulation Software], ProE Community, Sep 15, 2008</ref><ref>http://www.solidedgecommunity.com/feature_full.php?cpfeatureid=30190 {{Webarchive|url=https://web.archive.org/web/20110716102850/http://www.solidedgecommunity.com/feature_full.php?cpfeatureid=30190 |date=July 16, 2011 }} A first look at MapleSim</ref>\n\nThe MapleSim library includes many components that can be connected together to model a system. These components are from areas of science and engineering such as [[Electronics|electrical]], [[Mechanical engineering|mechanical]],<ref>{{cite journal|last1=Mahmud|first1=Khizir|last2=Town|first2=Graham E.|title=A review of computer tools for modeling electric vehicle energy requirements and their impact on power distribution networks|journal=Applied Energy|date=June 2016|volume=172|pages=337‚Äì359|doi=10.1016/j.apenergy.2016.03.100}}</ref> and [[Thermodynamics|thermal]] engineering fields. MapleSim also includes traditional signal flow components that can be combined with other physical components in the workspace. Thus, MapleSim is able to combine [[Causal#Engineering|causal]] modeling methods with [[Anticausal system|acausal]] techniques that do not require specification of signal flow direction between all components.<ref>[http://www.scientific-computing.com/features/feature.php?feature_id=208 European Models Promote Fidelity], Scientific Computing World, August 2008</ref>\n\nThe use of Maple underneath MapleSim allows all of the system equations to be generated and simplified automatically. The user can explore their system in various ways, such as viewing the equations behind their model and performing parameter optimization.<ref>[http://www.deskeng.com/articles/aaakrm.htm Modelica Aims for Effective Model-based Simulation], Desktop Engineering Online, Sep 1, 2008</ref><ref>[http://www.deskeng.com/articles/aaanpf.htm Editor's Pick: MapleSim Handles Multidomain Modeling & Simulation], Desktop Engineering Online, Jan 21, 2009,</ref> The use of the Maple mathematics engine also allows for MapleSim to incorporate such features as units management and solving of high-order [[differential algebraic equation|DAEs]] that are typically encountered in complex acausal models.<ref>[http://www.controleng.com/index.asp?layout=article&articleid=CA6579839&article_prefix=CA&article_id=6579839&q=Maplesoft Mechatronics: Next generation tool for modeling and simulation], Control Engineering, July 28, 2008</ref>\n\n== Release history ==\n\n{| class=\"wikitable\" style=\"font-size: 90%; text-align: left; \"\n|-\n! Name/Version !! Date\n|-\n| MapleSim 1.0\n| December 2008\n|-\n| MapleSim 2.0\n| April 2009\n|-\n| MapleSim 3.0\n| October 2009\n|-\n| MapleSim 4.0\n| April 2010\n|-\n| MapleSim 4.5\n| October 2010\n|-\n| MapleSim 5.0\n| June 2011\n|-\n| MapleSim 5.01\n| October 2011\n|-\n| MapleSim 5.02\n| January 2012\n|-\n| MapleSim 6.0\n| September 2012\n|-\n| MapleSim 6.1\n| April 2013\n|-\n| MapleSim 6.2\n| September 2013\n|-\n| MapleSim 6.3\n| December 2013\n|-\n| MapleSim 6.4\n| March 2014\n|-\n| MapleSim 7\n| December 2014\n|-\n\n| MapleSim 2015\n| May 2015\n|-\n| MapleSim 2016\n| April 2016\n|-\n| MapleSim 2016.2\n| January 2017\n|-\n| MapleSim 2017.0\n| May 2017\n|-\n| MapleSim 2017.1\n| June 2017\n|-\n| MapleSim 2017.2\n| August 2017\n|-\n| MapleSim 2017.3\n| September 2017\n|-\n|MapleSim 2018.1\n|June 2018\n|-\n|MapleSim 2019.1\n|May 2019\n|}\n\n== Add-on Libraries & Tools ==\n\n* ''MapleSim Connector''\n** ANSI C base [[Simulink]] S-function code generation\n* ''MapleSim Connector for FMI''\n** FMU generation based on [[Functional Mock-up Interface|FMI]] Standard\n*''B&R MapleSim Connector''\n**Integration tool for [[B&R]] Automation Studio and MapleSim models\n*''MapleSim Connector for LabVIEW and NI Veristand''\n** Code generation for NI [[LabVIEW]] Software\n* ''MapleSim Connector for JMAG-RT''\n** Import [[JMAG]]-RT file into MapleSim model\n* ''MapleSim CAD Toolbox''\n** Import various CAD models into MapleSim then automatically recreating the model components in MapleSim\n* ''MapleSim Tire Library''\n** Industry standard tire component library which includes Fiala, Calspan and Pacejka 2002 types.\n* ''MapleSim Driveline Library''\n** Component library for powertrain modeling in automotive engineering such as differential, wheels and road loads. \n* ''MapleSim Battery Library''\n** Supports electrochemical and equivalent-circuit models for battery system modeling\n* ''MapleSim Hydraulics Library from Modelon''\n** Third-party version for Hydraulics component models\n* ''MapleSim Pneumatics Library from Modelon''\n** Third-party version for Pneumatics component models\n*''MapleSim Engine Dynamics Library from Modelon''\n**Third-party version of Engine Dynamics Library which can be used for modeling and simulation for combustion engine in automotive applications.\n*''MapleSim Heat Transfer Library from CYBERNET''\n** System-level simulation for Heat Transfer effects in MapleSim model based on automatically generated discretization approach.\n* ''MapleSim Control Design Toolbox''\n** Provides set of commands for controller design such as PID working with plant models designed by MapleSim. These commands are used in [[Maple (software)|Maple]].\n* ''MapleSim Explorer''\n** Viewer version of MapleSim that can run simulation of MapleSim models.\n* ''MapleSim Server''\n** Web deployment option that can serve MapleSim models on web browser or tablets.\n\n== See also ==\n* [[AMESim]]\n* [[APMonitor]]\n* [[Computer simulation]]\n* [[Control engineering]]\n* [[Dymola]]\n* [[EcosimPro]]\n* [[EMSO simulator]]\n* [[Hardware-in-the-loop simulation]]\n* [[Maple (software)]]\n* [[Mechatronics]]\n* [[Model-based design]]\n* [[Modelica]]\n* [[SimulationX]]\n* [[Vehicle dynamics]]\n* [[Wolfram SystemModeler]]\n\n== References ==\n{{reflist}}\n\n== External links ==\n* [http://www.maplesoft.com/products/maplesim/index.aspx MapleSim home page]\n*[https://www.maplesoft.com/products/maplesim/ModelGallery/ MapleSim Model Gallery]\n*[https://www.maplesoft.com/whitepapers/technical.aspx List of research papers that Maplesoft products are used]\n\n[[Category:Maplesoft]]\n[[Category:Plotting software]]\n[[Category:Mathematical optimization software]]\n[[Category:Computer algebra system software for Linux]]\n[[Category:Computer algebra system software for Windows]]\n[[Category:Computer algebra system software for MacOS]]\n[[Category:Cross-platform software]]\n[[Category:Simulation software]]"
    },
    {
      "title": "MATLAB",
      "url": "https://en.wikipedia.org/wiki/MATLAB",
      "text": "{{Use mdy dates|date=September 2018}}\n{{for|the region in Bangladesh|Matlab (Bangladesh)}}\n{{distinguish|MATHLAB}}\n{{Infobox software\n| name = MATLAB\n| logo = [[File:Matlab Logo.png|100px]]\n| logo caption = L-shaped membrane logo<ref>{{cite web|title=The L-Shaped Membrane|url=http://www.mathworks.com/company/newsletters/articles/the-l-shaped-membrane.html|publisher=MathWorks|accessdate=February 7, 2014|year=2003}}</ref>\n| screenshot = [[File:MATLAB R2013a Win8 screenshot.png|320px]]\n| caption = MATLAB R2013a running on [[Windows 8]]\n| developer = [[MathWorks]]\n| released = {{Start date and age|1984}}\n| latest release version = R2019a\n| latest release date = {{Start date and age|2019|03|20}}\n| latest preview version = \n| latest preview date = \n| status = Active\n| programming language = [[C (programming language)|C]], [[C++]], [[Java (programming language)|Java]]\n| operating system = [[Microsoft Windows|Windows]], [[macOS]], and [[Linux]]<ref>{{cite web|url=http://www.mathworks.com/products/availability/index.html#ML|title=System Requirements and Platform Availability|publisher=MathWorks|accessdate=August 14, 2013}}</ref>\n| platform = [[IA-32]], [[x86-64]]\n| genre = [[List of numerical analysis software|Numerical computing]]\n| license = [[Proprietary software|Proprietary]] [[commercial software]]\n| website = {{URL|https://www.mathworks.com/products/matlab.html|mathworks.com}}\n}}\n{{Infobox programming language\n| name                   = MATLAB\n| paradigm               = [[Multi-paradigm programming language|multi-paradigm]]: [[Functional programming|functional]], [[Imperative programming|imperative]], [[Procedural programming|procedural]], [[Object-oriented programming|object-oriented]], [[Array programming|array]]\n| family                 =\n| year                   = late 1970s\n| designer               = [[Cleve Moler]]\n| developer              = [[MathWorks]]\n| latest release version = 9.5 (R2018b)\n| latest release date    = {{Start date and age|2018|09|12}}\n| latest preview version =\n| latest preview date    =\n| typing                 = [[dynamic typing|dynamic]], [[weak typing|weak]]\n| turing-complete        = Yes\n| scope                  =\n| implementations        =\n| dialects               =\n| influenced by          = {{startflatlist}}\n*[[APL (programming language)|APL]]\n*[[EISPACK]]\n*[[LINPACK]]\n*[[PL/0]]\n*[[Speakeasy (computational environment)|Speakeasy]]<ref>{{cite web |url=http://archive.computerhistory.org/resources/access/text/2013/12/102746804-05-01-acc.pdf |title=An interview with CLEVE MOLER Conducted by Thomas Haigh On 8 and 9 March, 2004 Santa Barbara, California |publisher=Computer History Museum |quote=So APL, Speakeasy, LINPACK, EISPACK, and PL0 were the predecessors to MATLAB. |accessdate=December 6, 2016}}</ref>\n{{endflatlist}}\n| influenced             = {{startflatlist}}\n*[[Julia (programming language)|Julia]]<ref name=\"Julia\">{{cite web |url=http://julialang.org/blog/2012/02/why-we-created-julia |first1=Jeff |last1=Bezanson |first2=Stefan |last2=Karpinski |first3=Viral |last3=Shah |first4=Alan |last4=Edelman |title=Why We Created Julia  |publisher=Julia Language |date=February 14, 2012 |accessdate=December 1, 2016}}</ref>\n*[[GNU Octave|Octave]]<ref name=\"Octave\">{{cite web |url=http://jbrwww.che.wisc.edu/tech-reports/twmcc-2001-03.pdf |first=John W. |last=Eaton |title=Octave: Past, Present, and Future |work=Texas-Wisconsin Modeling and Control Consortium |date=May 21, 2001 |accessdate=December 1, 2016}}</ref> \n*[[Scilab]]<ref name=\"Scilab\">{{cite web |url=https://www.scilab.org/scilab/history |title=History |publisher=Scilab |accessdate=December 1, 2016}}</ref>\n{{endflatlist}}\n| operating system       =\n| license                =\n| file ext               = .m\n| website                = {{URL|https://www.mathworks.com/products/matlab.html|mathworks.com}}\n| wikibooks              = MATLAB Programming\n}}\n\n'''MATLAB''' (''matrix laboratory'') is a [[multi-paradigm programming language|multi-paradigm]] [[numerical analysis|numerical computing]] environment and [[proprietary programming language]] developed by [[MathWorks]]. MATLAB allows [[matrix (mathematics)|matrix]] manipulations, plotting of [[function (mathematics)|functions]] and data, implementation of [[algorithm]]s, creation of [[user interface]]s, and interfacing with programs written in other languages, including [[C (programming language)|C]], [[C++]], [[C Sharp (programming language)|C#]], [[Java (programming language)|Java]], [[Fortran]] and [[Python (programming language)|Python]].\n\nAlthough MATLAB is intended primarily for numerical computing, an optional toolbox uses the [[MuPAD]] [[computer algebra system|symbolic engine]], allowing access to [[symbolic computing]] abilities. An additional package, [[Simulink]], adds graphical multi-domain simulation and [[model-based design]] for [[dynamical system|dynamic]] and [[embedded system]]s.\n\nAs of 2018, MATLAB has more than 3 million users worldwide.<ref name=\"mathworksCompanyOverview\">{{cite web |url=https://www.mathworks.com/content/dam/mathworks/tag-team/Objects/c/company-fact-sheet-8282v18.pdf |title=Company Overview |author = The MathWorks |date=April 2018}}</ref> MATLAB users come from various backgrounds of [[engineering]], [[science]], and [[economics]].\n\n== History ==\n[[Cleve Moler]], the chairman of the [[computer science]] department at the [[University of New Mexico]], started developing MATLAB in the late 1970s.<ref name=\"origins\">{{cite web | url = http://www.mathworks.com/company/newsletters/articles/the-origins-of-matlab.html | title = The Origins of MATLAB| author = Cleve Moler | accessdate = April 15, 2007 |date=December 2004 }}</ref> He designed it to give his students access to [[LINPACK]] and [[EISPACK]] without them having to learn [[Fortran]]. It soon spread to other universities and found a strong audience within the [[applied mathematics]] community. [[John N. Little|Jack Little]], an engineer, was exposed to it during a visit Moler made to [[Stanford University]] in 1983. Recognizing its commercial potential, he joined with Moler and Steve Bangert. They rewrote MATLAB in [[C (programming language)|C]] and founded [[MathWorks]] in 1984 to continue its development. These rewritten libraries were known as JACKPAC.<ref>{{cite web|url=http://www.altiusdirectory.com/Computers/matlab-programming-language.php|title=MATLAB Programming Language|publisher=Altius Directory|accessdate=December 17, 2010}}</ref> In 2000, MATLAB was rewritten to use a newer set of libraries for matrix manipulation, [[LAPACK]].<ref>{{cite web|title=MATLAB Incorporates LAPACK|url=http://www.mathworks.com/company/newsletters/articles/matlab-incorporates-lapack.html|work=Cleve's Corner|publisher=MathWorks|accessdate=December 20, 2008|first=Cleve |last=Moler|date=January 2000}}</ref>\n\nMATLAB was first adopted by researchers and practitioners in [[control engineering]], Little's specialty, but quickly spread to many other domains. It is now also used in education, in particular the teaching of [[linear algebra]] and [[numerical analysis]], and is popular amongst scientists involved in [[image processing]].<ref name=\"origins\" />\n\n== Syntax ==\nThe MATLAB application is built around the MATLAB scripting language. Common usage of the MATLAB application involves using the Command Window as an interactive mathematical [[command line interface|shell]] or executing text files containing MATLAB code.<ref>{{cite web|url=http://www.mathworks.com/help/matlab/index.html|title=MATLAB Documentation |publisher=MathWorks|accessdate=August 14, 2013}}</ref>\n\n=== Variables ===\nVariables are defined using the assignment operator, <code>=</code>. MATLAB is a [[Strong and weak typing|weakly typed]] programming language because types are implicitly converted.<ref>{{cite web|title=Comparing MATLAB with Other OO Languages|url=http://www.mathworks.com/help/matlab/matlab_oop/matlab-vs-other-oo-languages.html|work=MATLAB|publisher=MathWorks|accessdate=August 14, 2013}}</ref>  It is an inferred typed language because variables can be assigned without declaring their type, except if they are to be treated as symbolic objects,<ref>{{cite web|title=Create Symbolic Variables and Expressions|url=http://www.mathworks.com/help/symbolic/creating-symbolic-variables-and-expressions.html|work=Symbolic Math Toolbox|publisher=MathWorks|accessdate=August 14, 2013}}</ref> and that their type can change. Values can come from [[constant (computer science)|constant]]s, from computation involving values of other variables, or from the output of a function. For example:\n<source lang=\"matlabsession\">\n>> x = 17\nx =\n 17\n\n>> x = 'hat'\nx =\nhat\n\n>> x = [3*4, pi/2]\nx =\n   12.0000    1.5708\n\n>> y = 3*sin(x)\ny =\n   -1.6097    3.0000\n</source>\n\n=== Vectors and matrices ===\nA simple array is defined using the colon syntax: ''initial''<code>:</code>''increment''<code>:</code>''terminator''. For instance:\n<source lang=\"matlab\">\n>> array = 1:2:9\narray =\n 1 3 5 7 9\n</source>\ndefines a variable named <code>array</code> (or assigns a new value to an existing variable with the name <code>array</code>) which is an array consisting of the values 1, 3, 5, 7, and 9. That is, the array starts at 1 (the ''initial'' value), increments with each step from the previous value by 2 (the ''increment'' value), and stops once it reaches (or to avoid exceeding) 9 (the ''terminator'' value).\n<source lang=\"matlab\">\n>> array = 1:3:9\narray =\n 1 4 7\n</source>\nthe ''increment'' value can actually be left out of this syntax (along with one of the colons), to use a default value of 1.\n<source lang=\"matlab\">\n>> ari = 1:5\nari =\n 1 2 3 4 5\n</source>\nassigns to the variable named <code>ari</code> an array with the values 1, 2, 3, 4, and 5, since the default value of 1 is used as the incrementer.\n\n[[One-based indexing|Indexing]] is one-based,<ref>{{cite web|title=Matrix Indexing|url=http://www.mathworks.com/help/matlab/math/matrix-indexing.html|publisher=MathWorks|accessdate=August 14, 2013}}</ref> which is the usual convention for [[matrix (mathematics)|matrices]] in mathematics, although not for some programming languages such as C, C++, and Java.\n\nMatrices can be defined by separating the elements of a row with blank space or comma and using a semicolon to terminate each row. The list of elements should be surrounded by square brackets: []. Parentheses: () are used to access elements and subarrays (they are also used to denote a function argument list).\n\n<source lang=\"matlab\">\n>> A = [16 3 2 13; 5 10 11 8; 9 6 7 12; 4 15 14 1]\nA =\n 16  3  2 13\n  5 10 11  8\n  9  6  7 12\n  4 15 14  1\n\n>> A(2,3)\nans =\n 11\n</source>\n\nSets of indices can be specified by expressions such as \"2:4\", which evaluates to [2, 3, 4].  For example, a submatrix taken from rows 2 through 4 and columns 3 through 4 can be written as:\n<source lang=\"matlab\">\n>> A(2:4,3:4)\nans =\n 11 8\n 7 12\n 14 1\n</source>\nA square [[identity matrix]] of size ''n'' can be generated using the function ''eye'', and matrices of any size with zeros or ones can be generated with the functions ''zeros'' and ''ones'', respectively.\n<source lang=\"matlab\">\n>> eye(3,3)\nans =\n 1 0 0\n 0 1 0\n 0 0 1\n\n>> zeros(2,3)\nans =\n 0 0 0\n 0 0 0\n\n>> ones(2,3)\nans =\n 1 1 1\n 1 1 1\n</source>\n\n[[Transpose|Transposing]] a vector or a matrix is done either by the function ''transpose'' or by adding prime after a dot to the matrix. Without the dot MATLAB will perform [[conjugate transpose]].\n<source lang=\"matlab\">\n>> A = [1 ; 2],  B = A.', C = transpose(A)\nA =\n     1\n     2\nB =\n     1     2\nC =\n     1     2\n\n>> D = [0 3 ; 1 5], D.'\nD =\n     0     3\n     1     5\nans =\n     0     1\n     3     5\n</source>\n\nMost MATLAB functions can accept matrices and will apply themselves to each element. For example, <code>mod(2*J,n)</code> will multiply every element in \"J\" by 2, and then reduce each element modulo \"n\". MATLAB does include standard \"for\" and \"while\" loops, but (as in other similar applications such as [[R (programming language)|R]]), using the [[Array programming|vectorized]] notation often produces code that is faster to execute. This code, excerpted from the function ''magic.m'', creates a [[magic square]] ''M'' for odd values of ''n'' (MATLAB function <code>meshgrid</code> is used here to generate square matrices I and J containing 1:n).\n\n<source lang=\"matlab\">\n[J,I] = meshgrid(1:n);\nA = mod(I + J - (n + 3) / 2, n);\nB = mod(I + 2 * J - 2, n);\nM = n * A + B + 1;\n</source>\n\n=== Structures ===\nMATLAB has structure data types.<ref>{{cite web|title=Structures|url=http://www.mathworks.com/help/matlab/structures.html|publisher=MathWorks|accessdate=August 14, 2013}}</ref> Since all variables in MATLAB are arrays, a more adequate name is \"structure array\", where each element of the array has the same field names. In addition, MATLAB supports dynamic field names<ref>{{cite web|title=Generate Field Names from Variables|url=http://www.mathworks.com/help/matlab/matlab_prog/generate-field-names-from-variables.html|publisher=MathWorks|accessdate=August 14, 2013}}</ref> (field look-ups by name, field manipulations, etc.). Unfortunately, MATLAB JIT does not support MATLAB structures, therefore just a simple bundling of various variables into a structure will come at a cost.<ref>[http://blogs.mathworks.com/loren/2012/03/26/considering-performance-in-object-oriented-matlab-code/ Considering Performance in Object-Oriented MATLAB Code], Loren Shure, MATLAB Central, March 26, 2012: \"function calls on structs, cells, and function handles will not benefit from JIT optimization of the function call and can be many times slower than function calls on purely numeric arguments\"</ref>\n\n=== Functions ===\nWhen creating a MATLAB function, the name of the file should match the name of the first function in the file. Valid function names begin with an alphabetic character, and can contain letters, numbers, or underscores.  Functions are often case sensitive.\n\n=== Function handles ===\nMATLAB supports elements of [[lambda calculus]] by introducing function handles,<ref>{{cite web|title=Function Handles|url=http://www.mathworks.com/help/matlab/function-handles.html|publisher=MathWorks|accessdate=August 14, 2013}}</ref> or function references, which are implemented either in .m files or anonymous<ref>{{cite web|title=Anonymous Functions|url=http://www.mathworks.com/help/matlab/matlab_prog/anonymous-functions.html|publisher=MathWorks|accessdate=August 14, 2013}}</ref>/nested functions.<ref>{{cite web|title=Nested Functions|url=http://www.mathworks.com/help/matlab/matlab_prog/nested-functions.html|publisher=MathWorks.}}</ref>\n\n=== Classes and object-oriented programming ===\nMATLAB supports [[object-oriented programming]] including classes, inheritance, virtual dispatch, packages, pass-by-value semantics, and pass-by-reference semantics.<ref>{{cite web|url=http://www.mathworks.com/help/matlab/object-oriented-programming.html |title=Object-Oriented Programming|publisher=MathWorks|accessdate=August 14, 2013}}</ref> However, the syntax and calling conventions are significantly different from other languages. MATLAB has value classes and reference classes, depending on whether the class has ''handle'' as a super-class (for reference classes) or not (for value classes).<ref>{{cite web|title=Comparing Handle and Value Classes|url=http://www.mathworks.com/help/matlab/matlab_oop/comparing-handle-and-value-classes.html|publisher=MathWorks}}</ref>\n\nMethod call behavior is different between value and reference classes. For example, a call to a method\n<source lang=\"matlab\">\nobject.method();\n</source>\ncan alter any member of ''object'' only if ''object'' is an instance of a reference class.\n\nAn example of a simple class is provided below.\n\n<source lang=\"matlab\">\nclassdef hello\n    methods\n        function greet(this)\n            disp('Hello!')\n        end\n    end\nend\n</source>\n\nWhen put into a file named <tt>hello.m</tt>, this can be executed with the following commands:\n<source lang=\"matlabsession\">\n>> x = hello;\n>> x.greet();\nHello!\n</source>\n\n== Graphics and graphical user interface programming ==\nMATLAB supports developing applications with [[graphical user interface]] (GUI) features. MATLAB includes GUIDE<ref>{{cite web|title=Create a Simple GUIDE GUI|url=http://www.mathworks.com/help/matlab/creating_guis/about-the-simple-guide-gui-example.html|publisher=MathWorks|accessdate=August 14, 2014}}</ref> (GUI development environment) for graphically designing GUIs.<ref>{{cite web| url=http://www.mathworks.com/discovery/matlab-gui.html | title=MATLAB GUI | publisher=MathWorks | date=April 30, 2011 | accessdate=August 14, 2013}}</ref> It also has tightly integrated graph-plotting features. For example, the function ''plot'' can be used to produce a graph from two vectors ''x'' and ''y''. The code:\n<source lang=\"matlab\">\nx = 0:pi/100:2*pi;\ny = sin(x);\nplot(x,y)\n</source>\nproduces the following figure of the [[sine wave|sine function]]:\n\n[[File:Matlab plot sin.svg|350px]]\n\nA MATLAB program can produce three-dimensional graphics using the functions ''surf'', ''plot3'' or ''mesh''.\n{|\n|-\n| valign=\"top\" |<source lang=\"matlab\">[X,Y] = meshgrid(-10:0.25:10,-10:0.25:10);\nf = sinc(sqrt((X/pi).^2+(Y/pi).^2));\nmesh(X,Y,f);\naxis([-10 10 -10 10 -0.3 1])\nxlabel('{\\bfx}')\nylabel('{\\bfy}')\nzlabel('{\\bfsinc} ({\\bfR})')\nhidden off\n</source>\n| &nbsp;&nbsp;&nbsp;\n| valign=\"top\" |<source lang=\"matlab\">\n[X,Y] = meshgrid(-10:0.25:10,-10:0.25:10);\nf = sinc(sqrt((X/pi).^2+(Y/pi).^2));\nsurf(X,Y,f);\naxis([-10 10 -10 10 -0.3 1])\nxlabel('{\\bfx}')\nylabel('{\\bfy}')\nzlabel('{\\bfsinc} ({\\bfR})')\n</source>\n|-\n| This code produces a '''[[wire frame model|wireframe]]''' 3D plot of the two-dimensional unnormalized [[sinc function]]:\n| &nbsp;&nbsp;&nbsp;\n| This code produces a '''surface''' 3D plot of the two-dimensional unnormalized [[sinc function]]:\n|-\n| style=\"text-align:center;\"|[[File:MATLAB mesh sinc3D.svg]]\n| &nbsp;&nbsp;&nbsp;\n| style=\"text-align:center;\"|[[File:MATLAB surf sinc3D.svg]]\n|}\n\nIn MATLAB, graphical user interfaces can be programmed with the GUI design environment (GUIDE) tool.<ref>{{cite book | title=MATLAB: Advanced GUI Development | publisher=Dog Ear Publishing | last=Smith |first=S. T. | year=2006 | isbn=978-1-59858-181-2}}</ref>\n\n== Interfacing with other languages ==\nMATLAB can call functions and subroutines written in the programming languages [[C (programming language)|C]] or [[Fortran]].<ref>{{cite web|title=Application Programming Interfaces to MATLAB|url=http://www.mathworks.com/help/matlab/programming-interfaces-for-c-c-fortran-com.html|publisher=MathWorks|accessdate=August 14, 2013}}</ref> A wrapper function is created allowing MATLAB data types to be passed and returned. [[MEX file]]s (MATLAB executables) are the dynamically loadable object files created by compiling such functions.<ref>{{cite web|title=Create MEX-Files|url=http://www.mathworks.com/help/matlab/create-mex-files.html|publisher=MathWorks|accessdate=August 14, 2013}}</ref><ref>{{cite web|title=Connecting C and Matlab | last=Spielman | first=Dan | publisher=Yale University, Computer Science Department | date=February 10, 2004 | url=http://www.cs.yale.edu/homes/spielman/ECC/cMatlab.html | accessdate=May 20, 2008}}</ref> Since 2014 increasing two-way interfacing with [[Python (programming language)|Python]] was being added.<ref>{{cite web|title=MATLAB Engine for Python|url=http://www.mathworks.com/help/matlab/matlab-engine-for-python.html|publisher=MathWorks|accessdate=June 13, 2015}}</ref><ref>{{cite web|title=Call Python Libraries|url=http://www.mathworks.com/help/matlab/call-python-libraries.html|publisher=MathWorks|accessdate=June 13, 2015}}</ref>\n\nLibraries written in [[Perl]], [[Java (programming language)|Java]], [[ActiveX]] or [[.NET Framework|.NET]] can be directly called from MATLAB,<ref>{{cite web|title=External Programming Language Interfaces|url=http://www.mathworks.com/help/matlab/external-interfaces.html|publisher=MathWorks|accessdate=August 14, 2013}}</ref><ref>{{cite web|title=Call Perl script using appropriate operating system executable|url=http://www.mathworks.com/help/matlab/ref/perl.html|publisher=MathWorks|accessdate=November 7, 2013}}</ref> and many MATLAB libraries (for example [[XML]] or [[SQL]] support) are implemented as wrappers around Java or ActiveX libraries. Calling MATLAB from Java is more complicated, but can be done with a MATLAB toolbox<ref>{{cite web|url=http://www.mathworks.com/products/javabuilder/ |title=MATLAB Builder JA |publisher=MathWorks |accessdate=June 7, 2010}}</ref> which is sold separately by [[MathWorks]], or using an undocumented mechanism called JMI (Java-to-MATLAB  Interface),<ref>{{cite web|url=http://undocumentedmatlab.com/blog/jmi-java-to-matlab-interface/|first=Yair |last=Altman |title=Java-to-Matlab Interface |publisher=Undocumented Matlab |date=April 14, 2010 |accessdate=June 7, 2010}}</ref><ref>{{cite web|title=matlabcontrol JMI|url=https://code.google.com/p/matlabcontrol/wiki/JMI|first=Joshua |last=Kaplan}}</ref> (which should not be confused with the unrelated [[Java Metadata Interface]] that is also called JMI). Official MATLAB API for Java was added in 2016.<ref name=\"MATLAB Engine API for Java\">{{cite web|title=MATLAB Engine API for Java|url=http://www.mathworks.com/help/matlab/matlab-engine-api-for-java.html|publisher=MathWorks|accessdate=September 15, 2016}}</ref>\n\nAs alternatives to the [[MuPAD]] based Symbolic Math Toolbox available from MathWorks, MATLAB can be connected to [[Maple (software)|Maple]] or [[Mathematica]].<ref>{{cite web|title=MaMa: Calling MATLAB from Mathematica with MathLink|url=http://library.wolfram.com/infocenter/MathSource/618/|publisher=Wolfram Library Archive|first=Roger |last=Germundsson |work=[[Wolfram Research]] |date=September 30, 1998}}</ref><ref>{{cite web|title=MATLink:   Communicate with MATLAB from Mathematica|url=http://matlink.org/|accessdate=August 14, 2013|author1=rsmenon |author2=szhorvat|year=2013}}</ref>\n\nLibraries also exist to import and export [[MathML]].<ref>{{cite web |first=Michael |last=Weitzel |url=http://www.mathworks.com/matlabcentral/fileexchange/7709-mathml-importexport |title=MathML import/export |publisher=MathWorks - File Exchange |date=September 1, 2006 |accessdate=August 14, 2013}}</ref>\n\n== License ==\nMATLAB is a [[Proprietary software|proprietary]] product of MathWorks, so users are subject to [[vendor lock-in]].<ref name=\"eetimes2004\">{{cite news |url=http://www.eetimes.com/document.asp?doc_id=1151422 |title=Matlab edges closer to electronic design automation world |work=EE Times |first=Richard |last=Goering |date=October 4, 2004}}</ref><ref>{{cite web|title=The Wrong Choice: Locked in by license restrictions|url=http://searchenterpriselinux.techtarget.com/news/902076/The-Wrong-Choice-Locked-in-by-license-restrictions|publisher=SearchOpenSource.com|accessdate=August 14, 2013|first=Jan |last=Stafford|date=May 21, 2003}}</ref>  Although MATLAB Builder products can deploy MATLAB functions as library files which can be used with [[.NET Framework|.NET]]<ref>{{cite web|title=MATLAB Builder NE|url=http://www.mathworks.com/products/netbuilder/|publisher=MathWorks|accessdate=August 14, 2013}}</ref> or [[Java (software platform)|Java]]<ref>{{cite web|title=MATLAB Builder JA|url=http://www.mathworks.com/products/javabuilder/|publisher=MathWorks|accessdate=August 14, 2013}}</ref> application building environment, future development will still be tied to the MATLAB language.\n\nEach toolbox is purchased separately. If an evaluation license is requested, the MathWorks sales department requires detailed information about the project for which MATLAB is to be evaluated.{{citation needed|date=May 2019}} If granted (which it often is), the evaluation license is valid for two to four weeks.{{citation needed|date=May 2019}} A student version of MATLAB is available as is a home-use license for MATLAB, Simulink, and a subset of Mathwork's Toolboxes at substantially reduced prices.{{citation needed|date=May 2019}}\n\nIt has been reported that [[European Union]] (EU) competition regulators are investigating whether MathWorks refused to sell licenses to a competitor.<ref>{{cite web|title=MathWorks Software Licenses Probed by EU Antitrust Regulators|url=https://www.bloomberg.com/news/2012-03-01/mathworks-software-licenses-probed-by-eu-antitrust-regulators.html|publisher=Bloomberg news|date=March 1, 2012}}</ref>  The regulators dropped the investigation after the complainant withdrew its accusation and no evidence of wrongdoing was found.<ref>{{cite news|title=EU regulators scrap antitrust case against MathWorks|url=https://www.reuters.com/article/2014/09/02/us-eu-mathworks-antitrust-idUSKBN0GX14V20140902|work=Reuters|date=September 2, 2014}}</ref>\n\n== Alternatives ==\n{{See also|list of numerical analysis software|comparison of numerical analysis software}}\n\nMATLAB has a number of competitors.<ref>{{cite web|title=Comparison of mathematical programs for data analysis|url=http://www.scientificweb.de/ncrunch/|first=Stefan |last=Steinhaus|date=February 24, 2008}}</ref> Commercial competitors include [[Mathematica]], [[TK Solver]], [[Maple software|Maple]], and [[IDL (programming language)|IDL]]. There are also [[free software|free]] [[open source software|open source]] alternatives to MATLAB, in particular [[GNU Octave]], [[Scilab]], [[FreeMat]], and [[SageMath]], which are intended to be mostly compatible with the MATLAB language; the [[Julia (programming language)|Julia]] programming language also initially used MATLAB-like syntax. Among other languages that treat arrays as basic entities (array programming languages) are [[APL (programming language)|APL]], [[Fortran]] 90 and higher, [[S-Lang]], as well as the statistical languages [[R (programming language)|R]] and [[S (programming language)|S]]. There are also libraries to add similar functionality to existing languages, such as [[IT++]] for [[C++]], [[Perl Data Language]] for [[Perl]], [[ILNumerics.Net|ILNumerics]] for [[.NET Framework|.NET]], [[NumPy]]/[[SciPy]]/[[matplotlib]] for [[Python (programming language)|Python]], SciLua/[[Torch (machine learning)|Torch]] for [[Lua (programming language)|Lua]], SciRuby for [[Ruby (programming language)|Ruby]], and Numeric.js for [[JavaScript]].\n\n[[GNU Octave]] is unique from other alternatives because it treats incompatibility with MATLAB as a bug (see [[GNU Octave#MATLAB compatibility|MATLAB Compatibility of GNU Octave]]), therefore, making [[GNU Octave]] a superset of the MATLAB language.\n\n== Release history ==\n{| class=\"wikitable\"\n|-\n! Version<ref name=\"growth\">{{cite web|title=The Growth of MATLAB and The MathWorks over Two Decades|url=http://www.mathworks.com/company/newsletters/articles/the-growth-of-matlab-and-the-mathworks-over-two-decades.html|work=News & Notes Newsletter|publisher=MathWorks|accessdate=August 14, 2013|first=Cleve |last=Moler|date=January 2006}}</ref> !! Release name !! Number !! Bundled [[Java virtual machine|JVM]] !! Year !! Release date !! Notes\n|-\n| MATLAB 1.0\n|\n|\n|\n| 1984\n|\n|\n|-\n| MATLAB  2\n|\n|\n|\n| 1986\n|\n|\n|-\n| MATLAB  3\n|\n|\n|\n| 1987\n|\n|\n|-\n| MATLAB  3.5\n|\n|\n|\n| 1990\n|\n| Ran on [[DOS]] but needed at least a [[Intel 80386|386]] processor; version 3.5m needed [[80387|math coprocessor]]\n|-\n| MATLAB  4\n|\n|\n|\n| 1992\n|\n| Ran on [[Windows 3.1x]] and Macintosh\n|-\n| MATLAB  4.2c\n|\n|\n|\n| 1994\n|\n| Ran on Windows 3.1x, needed a [[math coprocessor]]\n|-\n| MATLAB 5.0\n| Volume 8\n|\n|\n| 1996\n| December 1996\n| Unified releases across all platforms\n|-\n| MATLAB 5.1\n| Volume 9\n|\n|\n| rowspan=2| 1997\n| May 1997\n|\n|-\n| MATLAB 5.1.1\n| R9.1\n|\n|\n|\n|\n|-\n| MATLAB 5.2\n| R10\n|\n|\n| rowspan=2| 1998\n| March 1998\n| Last version working on classic Macs\n|-\n| MATLAB 5.2.1\n| R10.1\n|\n|\n|\n|\n|-\n| MATLAB 5.3\n| R11\n|\n|\n| rowspan=2| 1999\n| January 1999\n|\n|-\n| MATLAB 5.3.1\n| R11.1\n|\n|\n| November 1999\n|\n|-\n| MATLAB 6.0\n| R12\n| rowspan=2|12\n| 1.1.8\n| 2000\n| November 2000\n| First release with bundled Java virtual machine (JVM)\n|-\n| MATLAB 6.1\n| R12.1\n| 1.3.0\n| 2001\n| June 2001\n|Last release for Windows 95\n|-\n| MATLAB 6.5\n| R13\n| rowspan=3|13\n| 1.3.1\n| 2002\n| July 2002\n|\n|-\n| MATLAB 6.5.1\n| R13SP1\n|\n| rowspan=2| 2003\n|\n|\n|-\n| MATLAB 6.5.2\n| R13SP2\n|\n|\n| Last release for Windows 98, Windows ME, IBM/AIX, Alpha/TRU64, and SGI/IRIX<ref>{{cite web|title=MATLAB System Requirements - Release 13|url=http://www.mathworks.com/support/sysreq/release13/unix.html|publisher=MathWorks|accessdate=October 6, 2015}}</ref>\n|-\n| MATLAB 7\n| R14\n| rowspan=4| 14\n| 1.4.2\n| rowspan=2| 2004\n| June 2004\n| Introduced anonymous and nested functions<ref>{{cite web|title=Dynamic Function Creation with Anonymous and Nested Functions|url=http://www.mathworks.com/company/newsletters/articles/dynamic-function-creation-with-anonymous-and-nested-functions.html|publisher=MathWorks|accessdate=January 15, 2016}}</ref><br />\nRe-introduced for Mac (under Mac OS X)\n|-\n| MATLAB 7.0.1\n| R14SP1\n|\n| October 2004\n|\n|-\n| MATLAB 7.0.4\n| R14SP2\n| 1.5.0\n| rowspan=2| 2005\n| March 7, 2005\n| Support for memory-mapped files<ref>{{cite web|title=Memory Mapping|url=http://www.mathworks.com/help/matlab/memory-mapping.html|publisher=MathWorks|accessdate=January 22, 2014}}</ref> \n|-\n| MATLAB 7.1\n| R14SP3\n| 1.5.0\n| September 1, 2005\n|First 64-bit version available for Windows XP 64-bit\n|-\n| MATLAB 7.2\n| R2006a\n| 15\n| 1.5.0\n| rowspan=2| 2006\n| March 1, 2006\n|\n|-\n| MATLAB 7.3\n| R2006b\n| 16\n| 1.5.0\n| September 1, 2006\n| [[Hierarchical Data Format|HDF5]]-based MAT-file support\n|-\n| MATLAB 7.4\n| R2007a\n| 17\n| 1.5.0_07\n| rowspan=2| 2007\n| March 1, 2007\n| New <code>bsxfun</code> function to apply element-by-element binary operation with singleton expansion enabled<ref>{{cite web|title=MATLAB bsxfun|url=http://www.mathworks.com/help/matlab/ref/bsxfun.html|publisher=MathWorks|accessdate=January 22, 2014}}</ref> \n|-\n| MATLAB 7.5\n| R2007b\n| 18\n| 1.6.0\n| September 1, 2007\n| Last release for Windows 2000 and [[PowerPC]] Mac; License Server support for Windows Vista;<ref name=\"matsol\">{{cite web|title=Do MATLAB versions prior to R2007a run under Windows Vista?|url=http://www.mathworks.com/support/solutions/en/data/1-43EHE5/|publisher=MathWorks|accessdate=February 8, 2011|date=September 3, 2010}}</ref> new internal format for P-code\n|-\n| MATLAB 7.6\n| R2008a\n| 19\n| 1.6.0\n| rowspan=2| 2008\n| March 1, 2008\n| Major enhancements to object-oriented programming abilities with a new class definition syntax,<ref>{{cite web |url=http://www.mathworks.com/help/matlab/matlab_oop/compatibility-with-previous-versions-.html |title=OOP Compatibility with Previous Versions |publisher=MathWorks |accessdate=March 11, 2013}}</ref> and ability to manage namespaces with packages<ref>{{cite web|title=Packages Create Namespaces|url=http://www.mathworks.com/help/matlab/matlab_oop/scoping-classes-with-packages.html|publisher=MathWorks|accessdate=January 22, 2014}}</ref>\n|-\n| MATLAB 7.7\n| R2008b\n| 20\n| 1.6.0_04\n| October 9, 2008\n| Last release for processors w/o SSE2. New Map data structure:<ref>{{cite web|title=Map Containers|url=http://www.mathworks.com/help/matlab/map-containers.html|publisher=MathWorks|accessdate=January 22, 2014}}</ref> upgrades to random number generators<ref>{{cite web|title=Creating and Controlling a Random Number Stream|url=http://www.mathworks.com/help/matlab/math/creating-and-controlling-a-random-number-stream.html|publisher=MathWorks|accessdate=January 22, 2014}}</ref>\n|-\n| MATLAB 7.8\n| R2009a\n| 21\n| 1.6.0_04\n| rowspan=2| 2009\n| March 6, 2009\n| First release for Microsoft 32-bit & 64-bit Windows 7, new external interface to .NET Framework<ref>{{cite web|title=New MATLAB External Interfacing Features in R2009a|url=http://www.mathworks.com/support/2013b/matlab/8.2/demos/New-MATLAB-External-Interfacing-Features-in-R2009a.html|publisher=MathWorks|accessdate=January 22, 2014}}</ref> \n|-\n| MATLAB 7.9\n| R2009b\n| rowspan=2| 22\n| 1.6.0_12\n| September 4, 2009\n| First release for [[Mac OS X Snow Leopard#64-bit architecture|Intel 64-bit Mac]], and last for [[Solaris (operating system)|Solaris]] [[SPARC]]; new use for the tilde operator (<code>~</code>) to ignore arguments in function calls<ref>{{cite web|title=Ignore Function Outputs|url=http://www.mathworks.com/help/matlab/matlab_prog/ignore-function-outputs.html|publisher=MathWorks|accessdate=January 22, 2014}}</ref><ref>{{cite web|title=Ignore Function Inputs|url=http://www.mathworks.com/help/matlab/matlab_prog/ignore-function-inputs.html|publisher=MathWorks|accessdate=January 22, 2014}}</ref>\n|-\n| MATLAB 7.9.1\n| R2009bSP1\n| 1.6.0_12\n| rowspan=3| 2010\n| April 1, 2010\n| bug fixes.\n|-\n| MATLAB 7.10\n| R2010a\n| 23\n| 1.6.0_12\n| March 5, 2010\n| Last release for [[Apple‚ÄìIntel architecture|Intel 32-bit Mac]]\n|-\n| MATLAB 7.11\n| R2010b\n| rowspan=3|24\n| 1.6.0_17\n| September 3, 2010\n| Add support for enumerations<ref>{{cite web|title=Working with Enumerations|url=http://www.mathworks.com/help/matlab/matlab_oop/enumerations.html|publisher=MathWorks|accessdate=January 22, 2014}}</ref> \n|-\n| MATLAB 7.11.1\n| R2010bSP1\n| 1.6.0_17\n| rowspan=4| 2011\n| March 17, 2011\n| bug fixes and updates\n|-\n| MATLAB 7.11.2\n| R2010bSP2\n| 1.6.0_17\n| April 5, 2012<ref>{{cite web|title=What's New in Release 2010b|url=http://www.mathworks.com/products/new_products/release2010b.html|publisher=MathWorks|accessdate=January 22, 2014}}</ref>\n| bug fixes\n|-\n| MATLAB 7.12\n| R2011a\n| 25\n| 1.6.0_17\n| April 8, 2011\n| New <code>rng</code> function to control random number generation<ref>{{cite web|title=New RNG Function for Controlling Random Number Generation in Release 2011a|url=http://www.mathworks.com/support/2013b/matlab/8.2/demos/new-rng-function-in-r2011a.html|publisher=MathWorks|accessdate=January 22, 2014}}</ref><ref>{{cite web|title=MATLAB rng|url=http://www.mathworks.com/help/matlab/ref/rng.html|publisher=MathWorks|accessdate=January 22, 2014}}</ref><ref>{{cite web|title=Replace Discouraged Syntaxes of rand and randn|url=http://www.mathworks.com/help/matlab/math/updating-your-random-number-generator-syntax.html|publisher=MathWorks|accessdate=January 22, 2014}}</ref>\n|-\n| MATLAB 7.13\n| R2011b\n| 26\n| 1.6.0_17\n| September 1, 2011\n| Access-change parts of variables directly in MAT-files, without loading into memory;<ref>{{cite web|title=MATLAB matfile|url=http://www.mathworks.com/help/matlab/ref/matfile.html|publisher=MathWorks|accessdate=January 22, 2014}}</ref> increased maximum local workers with Parallel Computing Toolbox from 8 to 12<ref>{{cite web|title=MATLAB max workers|url=http://www.mathworks.com/matlabcentral/answers/25987|accessdate=January 22, 2014}}</ref>\n|-\n| MATLAB 7.14\n| R2012a\n| 27\n| 1.6.0_17\n| rowspan=2| 2012\n| March 1, 2012\n| Last version with 32-bit Linux support.<ref>{{cite web |url=https://www.mathworks.com/matlabcentral/answers/222489-is-matlab-supported-on-32-bit-linux|title=Is MATLAB supported on 32-bit Linux? |author=MathWorks Support Team |date=June 4, 2015 |quote=Versions of MATLAB prior to R2012a are fully supported on 32-bit Linux. After R2012a, MATLAB is no longer supported on 32-bit Linux.}}</ref>\n|-\n| MATLAB 8\n| R2012b\n| 28\n| 1.6.0_17\n| September 11, 2012\n| First release with [[Ribbon (computing)|Toolstrip]] interface;<ref>{{cite web |url=http://blogs.mathworks.com/loren/2012/09/12/the-matlab-r2012b-desktop-part-1-introduction-to-the-toolstrip/ |title=The MATLAB R2012b Desktop ‚Äì Part 1: Introduction to the Toolstrip |first=Loren |last=Shure |date=September 2012}}</ref> MATLAB Apps.<ref>{{cite web |url=http://www.mathworks.com/discovery/matlab-apps.html |title=MATLAB Apps |publisher=MathWorks |accessdate=August 14, 2013}}</ref> redesigned documentation system\n|-\n| MATLAB 8.1\n| R2013a\n| 29\n| 1.6.0_17\n| rowspan=2| 2013\n| March 7, 2013\n| New [[unit testing]] framework<ref>{{cite web |url=http://www.mathworks.com/help/matlab/matlab-unit-test-framework.html |title=MATLAB Unit Testing Framework |publisher=MathWorks |accessdate=August 14, 2013}}</ref>\n|-\n| MATLAB 8.2\n| R2013b\n| 30\n| 1.7.0_11\n| September 6, 2013<ref>{{cite web | url = http://www.mathworks.com/company/newsroom/mathworks-announces-release-2013b-of-the-matlab-and-simulink-product-families.html | title = MathWorks Announces Release 2013b of the MATLAB and Simulink Product Families | publisher=MathWorks |date=September 2013 }}</ref>\n| Built in Java Runtime Environment (JRE) updated to version 7;<ref>{{cite web|title=R2013b Release Notes|url=https://www.mathworks.com/help/matlab/release-notes.html?rntext=&startrelease=R2013b&endrelease=R2013b&category=desktop|publisher=MathWorks|accessdate=September 17, 2018}}</ref> New table data type<ref>{{cite web|title=MATLAB Tables|url=http://www.mathworks.com/help/matlab/tables.html|publisher=MathWorks|accessdate=September 14, 2013}}</ref> \n|-\n| MATLAB 8.3\n| R2014a\n| 31\n| 1.7.0_11\n| rowspan=2| 2014\n| March 7, 2014<ref>{{cite web|title=MathWorks Announces Release 2014a of the MATLAB and Simulink Product Families|url=http://www.mathworks.com/company/newsroom/mathworks-announces-release-2014a-of-the-matlab-and-simulink-product-families.html|publisher=MathWorks|accessdate=March 11, 2014}}</ref>\n| Simplified compiler setup for building MEX-files; USB Webcams support in core MATLAB; number of local workers no longer limited to 12 with Parallel Computing Toolbox\n|-\n| MATLAB 8.4\n| R2014b\n| 32\n| 1.7.0_11\n| October 3, 2014\n| New class-based graphics engine (a.k.a. HG2);<ref>{{cite web|title=Graphics Changes in R2014b|url=http://www.mathworks.com/help/matlab/graphics-changes-in-r2014b.html|publisher=MathWorks|accessdate=October 3, 2014}}</ref> tabbing function in GUI;<ref>{{cite web|title=uitab: Create tabbed panel|url=http://www.mathworks.com/help/matlab/ref/uitab.html|publisher=MathWorks|accessdate=October 3, 2014}}</ref> improved user toolbox packaging and help files;<ref>{{cite web|title=Create and Share Toolboxes|url=http://www.mathworks.com/help/matlab/matlab_prog/create-and-share-custom-matlab-toolboxes.html|publisher=MathWorks|accessdate=October 3, 2014}}</ref> new objects for time-date manipulations;<ref>{{cite web|title=Dates and Time|url=http://www.mathworks.com/help/matlab/date-and-time-operations.html|publisher=MathWorks|accessdate=October 3, 2014}}</ref> [[Git (software)|Git]]-[[Apache Subversion|Subversion]] integration in IDE;<ref>{{cite web|title=Source Control Integration|url=http://www.mathworks.com/help/matlab/source-control.html|publisher=MathWorks|accessdate=October 3, 2014}}</ref> [[big data]] abilities with [[MapReduce]] (scalable to [[Apache Hadoop|Hadoop]]);<ref>{{cite web|title=MATLAB MapReduce and Hadoop|url=http://www.mathworks.com/discovery/matlab-mapreduce-hadoop.html|publisher=MathWorks|accessdate=October 3, 2014}}</ref> new <code>py</code> package for using [[Python (programming language)|Python]] from inside MATLAB,<ref>{{cite web|title=Call Python Libraries|url=http://www.mathworks.com/help/matlab/call-python-libraries.html|publisher=MathWorks|accessdate=October 3, 2014}}</ref> new engine interface to call MATLAB from Python;<ref>{{cite web|title=MATLAB Engine for Python|url=http://www.mathworks.com/help/matlab/matlab-engine-for-python.html|publisher=MathWorks|accessdate=October 3, 2014}}</ref> several new and improved functions: <code>webread</code> (RESTful web services with JSON/XML support), <code>tcpclient</code> (socket-based connections), <code>histcounts</code>, <code>histogram</code>, <code>animatedline</code>, and others\n|-\n| MATLAB 8.5\n| R2015a\n| rowspan=2 | 33\n| 1.7.0_60\n| rowspan=3 | 2015\n| March 5, 2015\n| Last release supporting Windows XP and Windows Vista\n|-\n| MATLAB 8.5\n| R2015aSP1\n| 1.7.0_60\n| October 14, 2015\n| \n|-\n| MATLAB 8.6\n| R2015b\n| 34\n| 1.7.0_60\n| September 3, 2015\n| New MATLAB execution engine (a.k.a. LXE);<ref>{{cite web|title=MATLAB Execution Engine|url=http://www.mathworks.com/products/matlab/matlab-execution-engine/|publisher=MathWorks|accessdate=September 15, 2016}}</ref> <code>graph</code> and <code>digraph</code> classes to work with graphs and networks;<ref>{{cite web|title=Graph and Network Algorithms|url=http://www.mathworks.com/help/matlab/graph-and-network-algorithms.html|publisher=MathWorks|accessdate=September 15, 2016}}</ref> MinGW-w64 as supported compiler on Windows;<ref>{{cite web|title=Install MinGW-w64 Compiler|url=http://www.mathworks.com/help/matlab/matlab_external/install-mingw-support-package.html|publisher=MathWorks|accessdate=September 15, 2016}}</ref> Last version with 32-bit support\n|-\n| MATLAB 9.0\n| R2016a\n| 35\n| 1.7.0_60\n| rowspan=2 | 2016\n| March 3, 2016\n| Live Scripts: interactive documents that combine text, code, and output (in the style of [[Literate programming]]);<ref>{{cite web|title=What Is a Live Script?|url=http://www.mathworks.com/help/matlab/matlab_prog/what-is-a-live-script.html|publisher=MathWorks|accessdate=September 15, 2016}}</ref>  App Designer: a new development environment for building apps (with new kind of UI figures, axes, and components);<ref>{{cite web|title=MATLAB App Designer|url=http://www.mathworks.com/products/matlab/app-designer/|publisher=MathWorks|accessdate=September 15, 2016}}</ref> pause execution of running programs using a Pause Button\n|-\n| MATLAB 9.1\n| R2016b\n| 36\n| 1.7.0_60\n| September 15, 2016\n| define local functions in scripts;<ref>{{cite web|title=Add Functions to Scripts|url=http://www.mathworks.com/help/matlab/matlab_prog/local-functions-in-scripts.html|publisher=MathWorks|accessdate=September 15, 2016}}</ref> automatic expansion of dimensions (previously provided via explicit call to <code>bsxfun</code>); <code>tall</code> arrays for [[Big data]];<ref>{{cite web|title=Tall Arrays|url=http://www.mathworks.com/help/matlab/tall-arrays.html|publisher=MathWorks|accessdate=September 15, 2016}}</ref> new <code>string</code> type;<ref>{{cite web|title=Create String Arrays|url=http://www.mathworks.com/help/matlab/matlab_prog/create-string-arrays.html|publisher=MathWorks|accessdate=September 15, 2016}}</ref> new functions to encode/decode [[JSON]];<ref>{{Cite web|url=http://mathworks.com/help/matlab/json-format.html|title=JSON Format - MATLAB & Simulink|website=mathworks.com|access-date=August 20, 2017}}</ref> official MATLAB Engine API for Java<ref name=\"MATLAB Engine API for Java\"/>\n|-\n| MATLAB 9.2\n| R2017a\n| 37\n| 1.7.0_60\n| rowspan=2 | 2017\n| March 9, 2017\n| MATLAB Online: cloud-based MATLAB desktop accessed in a web browser;<ref>{{cite web|title=MATLAB Online|url=https://www.mathworks.com/products/matlab-online.html|publisher=MathWorks|accessdate=April 10, 2017}}</ref>  double-quoted strings; new <code>memoize</code> function for [[Memoization]]; expanded object properties validation;<ref>{{cite web|title=Validate Property Values|url=https://www.mathworks.com/help/matlab/matlab_oop/validate-property-values.html|publisher=MathWorks|accessdate=April 10, 2017}}</ref>  [[Mock object|mocking]] framework for unit testing;<ref>{{cite web|title=Mocking Framework|url=https://www.mathworks.com/help/matlab/mocking-framework.html|publisher=MathWorks|accessdate=April 10, 2017}}</ref>  MEX targets 64-bit by default;  new <code>heatmap</code> function for creating [[Heat map|heatmap charts]]<ref>{{cite web|title=Create Heatmap from Tabular Data|url=https://www.mathworks.com/help/matlab/creating_plots/create-heatmap-from-tabular-data.html|publisher=MathWorks|accessdate=April 10, 2017}}</ref>\n|-\n| MATLAB 9.3\n| R2017b\n| 38\n| 1.8.0_121\n| September 21, 2017\n| \n|-\n| MATLAB 9.4\n| R2018a\n| 39\n| 1.8.0_144\n| rowspan=2 | 2018\n| March 15, 2018<ref>{{cite web|title=MathWorks Announces Release 2018a of the MATLAB and Simulink Product Families|url=https://www.mathworks.com/company/newsroom/mathworks-announces-release-2018a-of-the-matlab-and-simulink-product-families.html|publisher=MathWorks|accessdate=April 5, 2018}}</ref>\n|\n|-\n| MATLAB 9.5\n| R2018b\n| 40\n| 1.8.0_152\n| September 12, 2018\n|\n|-\n|MATLAB 9.6\n|R2019a\n|41\n|1.8.0_181\n|2019\n|March 20, 2019\n|MATLAB Projects.\n|}\n\nThe number (or release number) is the version reported by Concurrent License Manager program [[FlexNet Publisher|FLEXlm]].\n\nFor a complete list of changes of both MATLAB and official toolboxes, consult the MATLAB release notes.<ref>{{cite web|title=MATLAB Release Notes|url=http://www.mathworks.com/help/relnotes/index.html|publisher=MathWorks|accessdate=January 25, 2014}}</ref>\n\n== File extensions ==\n\n=== MATLAB ===\n; .m : MATLAB code (function, script, or class)\n; .mat : MATLAB data (binary file for storing variables)\n; .mex* (.mexw32, .mexw64, .mexglx, .mexa64, .mexmaci64, ...) : MATLAB executable MEX-files<ref>{{cite web |title = Introducing MEX-Files |url = http://www.mathworks.com/help/matlab/matlab_external/introducing-mex-files.html |publisher = MathWorks |accessdate = August 14, 2013 }}</ref> (platform specific, e.g. \".mexmac\" for the [[Macintosh|Mac]], \".mexglx\" for [[Linux]], etc.<ref>{{cite web |title = Binary MEX-File Extensions |url = http://www.mathworks.com/help/matlab/matlab_external/using-mex-files-to-call-c-c-and-fortran-programs.html#bra56dy-1 |publisher = MathWorks |accessdate = August 14, 2013 }}</ref>)\n; .p : MATLAB content-obscured .m file (P-code<ref>{{cite web |title = Protect Your Source Code |url = http://www.mathworks.com/help/matlab/matlab_prog/protect-your-source-code.html |publisher = MathWorks |accessdate = August 14, 2013 }}</ref>)\n; .mlx : MATLAB live script<ref>{{cite web|title=What Is a Live Script?|url=http://www.mathworks.com/help/matlab/matlab_prog/what-is-a-live-script.html|publisher=MathWorks|accessdate=August 21, 2016}}</ref><ref>{{cite web|title=Live Script File Format (.mlx)|url=http://www.mathworks.com/help/matlab/matlab_prog/live-script-file-format.html|publisher=MathWorks|accessdate=August 21, 2016}}</ref>\n; .fig : MATLAB figures (created with GUIDE)\n; .mlapp : MATLAB apps (created with App Designer<ref>{{cite web|title=MATLAB App Designer|url=http://www.mathworks.com/products/matlab/app-designer/|publisher=MathWorks|accessdate=August 21, 2016}}</ref>)\n; .mlappinstall : MATLAB packaged App Installer<ref>{{cite web |title = MATLAB App Installer File |url = http://www.mathworks.com/help/matlab/creating_guis/what-is-an-app.html |publisher = MathWorks |accessdate = August 14, 2013 }}</ref>\n; .mlpkginstall: support package installer (add-on for third-party hardware)<ref>{{cite web |title = Support Package Installation |url = http://www.mathworks.com/help/matlab/matlab_external/support-package-installation.html |publisher = MathWorks |accessdate = October 3, 2014 }}</ref>\n; .mltx, .mltbx: packaged custom toolbox<ref>{{cite web |title = Manage Toolboxes |url = http://www.mathworks.com/help/matlab/matlab_prog/manage-toolboxes.html |publisher = MathWorks |accessdate = October 3, 2014 }}</ref><ref>{{cite web |title = Toolbox Distribution |url = http://www.mathworks.com/help/matlab/creating-help.html |publisher = MathWorks |accessdate = August 6, 2016 }}</ref><ref>{{cite web |title = What are MATLAB toolboxes? |url = https://www.lynda.com/MATLAB-tutorials/What-MATLAB-toolboxes/124067/138192-4.html |publisher = [[Lynda.com]] |accessdate = August 6, 2016 }}</ref>\n; .prj: project file used by various solutions (packaged app/toolbox projects, MATLAB Compiler/Coder projects, Simulink projects)\n; .rpt: report setup file created by MATLAB Report Generator<ref>{{cite web |title = MATLAB Report Generator |url = http://www.mathworks.com/products/ML_reportgenerator/ |publisher = MathWorks |accessdate = October 3, 2014 }}</ref>\n\n=== Simulink ===\n; .mdl : Simulink Model\n; .mdlp : Simulink Protected Model\n; .slx : Simulink Model (SLX format)\n; .slxp : Simulink Protected Model (SLX format)\n\n=== Simscape ===\n; .ssc : Simscape<ref>{{cite web|title=Simscape|url=http://www.mathworks.com/products/simscape/|publisher=MathWorks|accessdate=August 14, 2013}}</ref> Model\n\n=== MuPAD ===\n; .mn : MuPAD [[Notebook interface|Notebook]]\n; .mu : MuPAD Code\n; .xvc, .xvz : MuPAD Graphics\n\n=== Third-party ===\n; .jkt : GPU Cache file generated by Jacket for MATLAB (AccelerEyes)\n; .mum : MATLAB CAPE-OPEN Unit Operation Model File (AmsterCHEM)\n\n== Easter eggs ==\nSeveral [[Easter egg (media)|easter eggs]] exist in MATLAB.<ref>{{cite web|url=http://www.mathworks.com/matlabcentral/answers/2001-what-matlab-easter-eggs-do-you-know |title=What MATLAB Easter eggs do you know? |publisher=MathWorks - MATLAB Answers |date=February 25, 2011|accessdate=August 14, 2013}}</ref> These include hidden pictures,<ref>{{cite web|title=The Story Behind the MATLAB Default Image|url=http://blogs.mathworks.com/steve/2006/10/17/the-story-behind-the-matlab-default-image/|accessdate=August 14, 2013|first=Steve |last=Eddins|date=October 17, 2006}}</ref> and jokes. For example, typing in \"spy\" used to generate a picture of the spies from [[Spy vs Spy]], but now displays an image of a dog. Typing in \"why\" randomly outputs a philosophical answer. Other commands include \"penny\", \"toilet\", \"image\", and \"life\".  Not every Easter egg appears in every version of MATLAB.\n\n{{clear}}\n\n== See also ==\n* [[Comparison of numerical analysis software]]\n* [[List of numerical analysis software]]\n\n== Notes ==\n{{Reflist|30em}}\n\n== References ==\n{{Refbegin}}\n* {{cite book |last=Gilat |first=Amos |authorlink= |title=MATLAB: An Introduction with Applications 2nd Edition |year=2004 |publisher=John Wiley & Sons |location= |isbn= 978-0-471-69420-5 }}\n* {{cite book |last=Quarteroni |first=Alfio |authorlink= |first2=Fausto |last2=Saleri |title=Scientific Computing with MATLAB and Octave |year=2006 |publisher=Springer |location= |isbn= 978-3-540-32612-0 }}\n* {{cite book |last=Ferreira |first=A.J.M. |authorlink= |title=MATLAB Codes for Finite Element Analysis |year=2009 |publisher=Springer |location= |isbn= 978-1-4020-9199-5 }}\n* {{cite book |last=Lynch |first=Stephen |authorlink= |title=Dynamical Systems with Applications using MATLAB |year=2004 |publisher=Birkh√§user |location= |isbn=978-0-8176-4321-8 }}\n{{Refend}}\n\n== External links ==\n{{Wikibooks|MATLAB Programming}}\n{{Commons category|MATLAB}}\n{{Wikiversity|MATLAB essential}}\n* {{Official website|https://www.mathworks.com/products/matlab.html}}\n*{{Dmoz|Science/Math/Software/MATLAB|MATLAB}}\n\n\n<!-- Please do not add any links to NI and/or LabVIEW. Instead, argue your case on the talk page -->\n\n{{Numerical analysis software}}\n{{Computer algebra systems}}\n{{Statistical software}}\n{{Image Processing Software}}\n{{Linear algebra}}\n{{Programming languages}}\n\n{{DEFAULTSORT:Matlab}}\n[[Category:Array programming languages]]\n[[Category:Articles with example MATLAB/Octave code]]\n[[Category:C software]]\n[[Category:Computer algebra system software for Linux]]\n[[Category:Computer algebra system software for MacOS]]\n[[Category:Computer algebra system software for Windows]]\n[[Category:Computer algebra systems]]\n[[Category:Computer vision software]]\n[[Category:Cross-platform software]]\n[[Category:Data mining and machine learning software]]\n[[Category:Data visualization software]]\n[[Category:Data-centric programming languages]]\n[[Category:Dynamically typed programming languages]]\n[[Category:Econometrics software]]\n[[Category:High-level programming languages]]\n[[Category:IRIX software]]\n[[Category:Linear algebra]]\n[[Category:Mathematical optimization software]]\n[[Category:Numerical analysis software for Linux]]\n[[Category:Numerical analysis software for MacOS]]\n[[Category:Numerical analysis software for Windows]]\n[[Category:Numerical linear algebra]]\n[[Category:Numerical programming languages]]\n[[Category:Numerical software]]\n[[Category:Parallel computing]]\n[[Category:Plotting software]]\n[[Category:Proprietary commercial software for Linux]]\n[[Category:Proprietary cross-platform software]]\n[[Category:Regression and curve fitting software]]\n[[Category:Software modeling language]]\n[[Category:Statistical programming languages]]\n[[Category:Time series software]]"
    },
    {
      "title": "MCSim",
      "url": "https://en.wikipedia.org/wiki/MCSim",
      "text": "{{Use dmy dates|date=December 2012}}\n{{Infobox Software \n| name                   = MCSim\n| logo                   = mcsimlogo.png\n| logo size              = 100px\n| developer              = [[GNU Project]] \n| latest_release_version = 6.1.0\n| latest_release_date    = {{release date|df=yes|2019|02|19}}\n| programming language   = [[C (programming language)|C]]\n| operating_system       = [[Cross-platform]] \n| genre                  = [[Numerical Analysis]] \n| license                = [[GNU General Public License]] \n| website                = https://www.gnu.org/software/mcsim\n}}\n\n'''GNU MCSim''' is a suite of simulation software.  It allows one to design one's own statistical or simulation models,\nperform [[Monte Carlo method|Monte Carlo]]  simulations, and [[Bayesian inference]] through [[Markov chain Monte Carlo]] simulations. The latest version allow the use of simulated tempering MCMC simulations.\n\n==Description==\nGNU MCSim is a simulation and statistical inference tool for algebraic or [[differential equation]] systems, optimized for performing Monte Carlo analysis. The software comprises a model generator and a simulation engine:\n\n* The model generator facilitates structural model definition and maintenance, while keeping execution time short. The model is coded using a simple grammar, and the generator translates it into C code. Starting with version 5.3.0, models coded in [[SBML]] can also be used.\n* The simulation engine is a set of routines that are linked to the model in order to produce executable code. The result is that one can run simulations of the structural model under a variety of conditions.\n\nInternally, the software uses the [[GNU Scientific Library]] for some of its numerical calculations.\n\n==History==\nThe project began in 1991 in Berkeley when Don Maszle and [[Frederic Y. Bois]] translated in C and reorganized a program that Bois had developed at Harvard for his PhD thesis. The primary motivation for the work was to be able to quickly develop and easily maintain [[PBPK]] models. However, the syntax was defined with enough generality that many algebraic and first-order ordinary [[differential equations]] can be solved. The capability to perform efficient [[Monte Carlo simulations]] was added early on, for the research needs of the group. The code was made freely available from a server at UC Berkeley. Discussions with Stuart Beal at [[University of California, San Francisco|UCSF]] School of Pharmacy, led the team to investigate the use of Markov chain Monte Carlo techniques for PBPK models' [[Calibration (statistics)|calibration]]. The corresponding code was developed by Maszle, during a project in collaboration with [[Andrew Gelman]], then professor at [[University of California, Berkeley|UC Berkeley]] Statistics Department. Additional code written by Ken Revzan allowed the definition and Bayesian calibration of hierarchical (multilevel) statistical models. At the time of these developments (around 1996) those capabilities were unique for a freely distributed, easily accessible, efficient and quite versatile software.\n\n===Released versions===\n* 6.1.0 (19 February 2019)\n* 6.0.1 (05 May 2018)\n* 6.0.0 (24 February 2018)\n* 5.6.6 (21 January 2017)\n* 5.6.5 (27 February 2016)\n* 5.6.4 (30 January 2016)\n* 5.6.3 (1 January 2016)\n* 5.6.2 (24 December 2015)\n* 5.6.1 (21 December 2015)\n* 5.6.0 (16 December 2015)\n* 5.5.0 (17 March 2013)\n* 5.4.0 (18 January 2011)\n* 5.3.1 (3 March 2009)\n* 5.3.0 (12 January 2009)\n* 5.2 beta (29 January 2008)\n* 5.1beta (18 September 2006)\n* 5.0.0 (4 January 2005)\n* 4.2.0 (15 October 2001)\n* 4.1.0 (1 August 1997)\n* 4.0.0 (24 March 1997)\n* 3.6.0\n* 3.3.2\n\n==Licensing==\nGNU MCSim is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3 of the License, or (at your option) any later version.\n\n==Platform Availability==\nThe C source code is provided and can be compiled on any machine disposing of a C compiler. The [[GNU Scientific Library]] needs to be also available on the target platform to use a few extra distributions in statistical models. To take advantage of the SBML translation capabilities, the LibSBLM library should be installed. Starting with version 6.0.0 the Sundials Cvodes integrator is also used.\n\n==See also==\n* [[Comparison of numerical analysis software]]\n* [[List of numerical analysis software]]\n\n==References==\nBois F., Maszle D., 1997, MCSim: A simulation program, Journal of Statistical Software, 2(9):http://www.stat.ucla.edu/journals/jss/v02/i09.\n\nJonsson F., Johanson G., 2003, The Bayesian population approach to physiological toxicokinetic-toxicodynamic models - An example using the MCSim software, Toxicology Letters 138:143-150.\n\nBois F., 2009, GNU MCSim: Bayesian statistical inference for SBML-coded systems biology models, Bioinformatics, 25:1453-1454, doi: 10.1093/bioinformatics/btp162.\n\nAllen B.C., Hack E.C., Clewell H.J., 2007, Use of Markov chain Monte Carlo analysis with a physiologically-based pharmacokinetic model of methylmercury to estimate exposures in u.s. women of childbearing age, Risk Analysis, 27:947-959.\n\nCovington T.R., Gentry P.R., et al., 2007, The use of Markov chain Monte Carlo uncertainty analysis to support a Public Health Goal for perchloroethylene, [[Regulatory Toxicology and Pharmacology]], 47:1-18.\n\nDavid R.M., Clewell H.J., et al., 2006, Revised assessment of cancer risk to dichloromethane II. Application of probabilistic methods to cancer risk determinations. Regulatory Toxicology and Pharmacology 45: 55-65.\n\nFranks S.J., Spendiff M.K., et al., 2006, Physiologically based pharmacokinetic modelling of human exposure to 2-butoxyethanol, [[Toxicology Letters]] 162:164-173.\n\nHack E.C., 2006, Bayesian analysis of physiologically based toxicokinetic and toxicodynamic models, Toxicology, 221:241-248.\n\nHack E.C., Chiu W.A, et al., 2006, Bayesian population analysis of a harmonized physiologically based pharmacokinetic model of trichloroethylene and its metabolites, Regulatory Toxicology and Pharmacology, 46:63-83.\n\nLyons M.A., Yang R.S.H, Mayeno A.N., Reisfeld B. 2008, Computational toxicology of chloroform: reverse dosimetry using Bayesian inference, Markov chain Monte Carlo simulation, and human biomonitoring data, Environmental Health Perspectives, 116:1040-1046.\n\nMarino, D. J., H. Clewell, et al., 2006, Revised assessment of cancer risk to dichloromethane: part I Bayesian PBPK and dose-response modeling in mice, Regulatory Toxicology and Pharmacology 45:44-54.\n\nMezzetti M., Ibrahim J.G., et al., 2003, A Bayesian compartmental model for the evaluation of 1,3-butadiene metabolism, Journal of the Royal Statistical Society, Series C, 52:291-305.\n\n==External links==\n* [https://www.gnu.org/software/mcsim/ GNU MCSim home page]\n\n{{GNU}}\n\n[[Category:Free software programmed in C]]\n[[Category:Free Bayesian statistics software]]\n[[Category:Numerical software]]\n[[Category:GNU Project software]]\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "MIDACO",
      "url": "https://en.wikipedia.org/wiki/MIDACO",
      "text": "'''MIDACO''' ([[mixed integer programming|Mixed Integer]] Distributed [[ant colony optimization|Ant Colony Optimization]]) is a software package for numerical [[Mathematical optimization|optimization]] based on [[Evolutionary computation|evolutionary computing]].\nMIDACO was created in collaboration of\n[[ESA|European Space Agency]] and [[EADS Astrium]] to solve constrained mixed-integer non-linear (MINLP) space applications.<ref>{{cite journal |author1=M. Schlueter |author2=S. Erb |author3=M. Gerdts |author4=S. Kemble |author5=J.J. Rueckmann | title = MIDACO on MINLP Space Applications | url = http://www.sciencedirect.com/science/article/pii/S0273117712006898 | journal = Advances in Space Research | volume = 51 | pages = 1116‚Äì1131 | date = 2013 | doi=10.1016/j.asr.2012.11.006 | bibcode=2013AdSpR..51.1116S}}</ref><ref>[https://web.archive.org/web/20160911010247/https://www.highbeam.com/doc/1G1-333486307.html Newspaper article on MIDACO space applications in ''Defense & Aerospace Week'' (June 2013)]</ref>\nMIDACO holds several record solutions on [[interplanetary spaceflight]] trajectory design problems<ref>{{cite journal |author1=M. Schlueter | title = MIDACO software performance on interplanetary trajectory benchmarks | url =http://www.sciencedirect.com/science/article/pii/S0273117714002786 | journal = Advances in Space Research | volume = 54 | pages = 744‚Äì754 | date = 2014 | doi=10.1016/j.asr.2014.05.002 | bibcode=2014AdSpR..54..744S}}</ref><ref>[http://www.esa.int/gsp/ACT/inf/projects/gtop/gtoc1.html List of record solutions on  ESA GTOC1 benchmark]</ref><ref>[http://www.esa.int/gsp/ACT/inf/projects/gtop/messenger_full.html List of record solutions on ESA Messenger (full version) benchmark]</ref><ref>[http://www.esa.int/gsp/ACT/inf/projects/gtop/cassini2.html List of record solutions on ESA Cassini2 benchmark]</ref> made publicly available by [[European Space Agency]]. MIDACO is included in software packages like [[TOMLAB]],<ref>[http://tomopt.com/tomlab/products/midaco/ TOMLAB MIDACO webpage]</ref> [[Astos]],<ref>[https://www.astos.de/products/lotos/optimization Astos Lotos product page]</ref> and [[SigmaXL]].<ref>[http://www.sigmaxl.com/DSimReleaseInfo.shtml SigmaXL DiscoverSim product page]</ref>\n\n==References==\n{{reflist}}\n\n== External links ==\n* {{Official website|http://www.midaco-solver.com/}}\n\n{{Mathematical optimization software}}\n\n[[Category:Numerical software]]\n[[Category:Mathematical optimization software]]\n\n\n{{software-stub}}"
    },
    {
      "title": "MINOS (optimization software)",
      "url": "https://en.wikipedia.org/wiki/MINOS_%28optimization_software%29",
      "text": "__NOTOC__\n'''MINOS''' is a [[Fortran]] software package for solving linear and nonlinear mathematical [[Optimization (mathematics)|optimization]] problems.   MINOS (Modular In-core Nonlinear Optimization  System) may be used for linear programming, quadratic programming, and more general objective functions and constraints, and for finding a feasible point for a set of linear or nonlinear equalities and inequalities.<ref>{{cite paper \n| author = B.A. Murtagh, M.A. Saunders \n| title = MINOS 5.51 User's Guide \n| date = 2003 \n| url = http://web.stanford.edu/group/SOL/guides/minos551.pdf \n| format = PDF }}</ref>\n\nMINOS was first developed by Bruce Murtagh and [[Michael Saunders (academic)|Michael Saunders]], mostly at the Systems Optimization Laboratory in the Department of Operations Research at Stanford University.<ref>{{cite paper | author = B.A. Murtagh, M.A. Saunders | title = Large-scale linearly constrained optimization| journal = Mathematical Programming| volume = 14| pages = 41‚Äì72| date = 1978 | url = http://web.stanford.edu/group/SOL/guides/minos-math-prog-1978.pdf | format = PDF }}</ref>   In 1985, Saunders was awarded the inaugural Orchard-Hays prize <ref>[http://www.mathopt.org/?nav=boh#winners Beale-Orchard-Hays Prize winners]</ref> by the Mathematical Programming Society (now the [[Mathematical Optimization Society]]) for his work on MINOS.  Despite being one of the first general-purpose constrained optimization solvers to emerge, the package remains heavily used.  MINOS is supported in the [[AIMMS]], [[AMPL]], [[APMonitor]], [[General Algebraic Modeling System|GAMS]], and [[TOMLAB]] modeling systems.  In addition, it remains one of the top-used solvers on the NEOS Server<ref>[http://www.neos-server.org/neos/ NEOS Server]</ref><ref>{{cite conference | first = Michael | last = Saunders | date = 2013 | title = Optimization Algorithms and Software at SOL | url = http://web.stanford.edu/group/SOL/talks/saunders-CLAODE.pdf}}</ref> and in [[General Algebraic Modeling System|GAMS]].<ref>[http://www.gams.com/help/topic/gams.doc/solvers/minos/index.html GAMS/MINOS Solver guide]</ref>\n\n==Operation==\nIdeally, the user should provide gradients of the nonlinear functions.  (This is automatic in most of the modeling systems mentioned above.)  If some or all of the gradients are not provided, MINOS will approximate the missing ones by finite differences, but this could be slow and less reliable.  If the objective function is convex and the constraints are linear, the solution obtained will be a global minimizer. Otherwise, the solution obtained may be a local minimizer.\n\nFor linear programs, a two-phase primal [[simplex method]] is used. The first phase minimizes the sum of infeasibilities. For problems with linear constraints and a nonlinear objective, a reduced-gradient method is used.  A quasi-Newton approximation to the reduced Hessian is maintained to obtain search directions. The method is most efficient when many constraints or bounds are active at the solution.\n\nFor problems with nonlinear constraints, a linearly constrained Lagrangian method is used.<ref>{{cite book |last1=More |first1=Jorge J.  |last2=Wright |first2=Stephen J. |date=1993 |title=Optimization Software Guide|series=Frontiers in Applied Mathematics |chapter= Chapter 8: Constrained Optimization |doi=10.1137/1.9781611970951.ch8}}</ref> This involves a sequence of major iterations, each of which solves (perhaps approximately) a linearly constrained subproblem. The subproblem objective is an [[Augmented Lagrangian method|augmented Lagrangian]], and the subproblem constraints are linearizations of the nonlinear constraints at the current point.\n\nMINOS is intended for large sparse problems. There is no fixed limit on problem size. Most working storage is contained in one double-precision array (which should be sufficiently large). The source code is suitable for all scientific machines with a Fortran compiler.\n\n== References ==\n{{reflist}}\n\n== Further reading ==\n* {{cite paper \n| author = B.A. Murtagh, M.A. Saunders \n| title = A projected Lagrangian algorithm and its implementation for sparse nonlinear constraints \n| journal = Mathematical Programming\n| volume = 16\n| pages = 84‚Äì117\n| date = 1982 \n| url = http://web.stanford.edu/group/SOL/guides/minos-MPStudy-1982.pdf \n| format = PDF }}\n\n== External links ==\n* [http://web.stanford.edu/group/SOL Systems Optimization Laboratory, Stanford University] Systems Optimization Laboratory (SOL).\n* [http://www.sbsi-sol-optimize.com/asp/sol_products_minos_desc.htm MINOS 5.5 - Description] - Distributors of the software.\n\n{{Mathematical optimization software}}\n\n[[Category:Mathematical optimization software]]\n\n\n{{Software-stub}}"
    },
    {
      "title": "MINTO",
      "url": "https://en.wikipedia.org/wiki/MINTO",
      "text": "'''MINTO''' (''Mixed Integer Optimizer'') is an [[integer programming]] solver which uses [[branch and bound]] algorithm.\n\nMINTO is a software system that solves [[mixed integer programming]] problem by a [[branch and bound]] algorithm with [[linear programming]] relaxations. It also provides automatic constraint classification, preprocessing, primal heuristics and constraint generation. It also has inbuilt cut generation and can create [[knapsack cuts]], [[GUB cuts]], [[clique cuts]], [[implication cuts]], [[flow cuts]], [[mixed integer rounding]] and [[Gomory cuts]]. Moreover, the user can enrich the basic algorithm by providing a variety of specialized application routines that can customize MINTO to achieve higher efficiency for a problem class.\n\nMINTO does not have a [[linear programming]] (LP) solver of its own. It can use most of the LP solvers, like CLP, [[CPLEX]], XPRESS through the OSI interface of [[COIN-OR]]. MINTO can read files in [[MPS (format)|MPS]] and can also be called as a solver from [[AMPL]]. It can run on both [[Linux]] and Windows operating system. MINTO is a non-commercial solver and the executables are available for free download from its home page at COR@L.\n\n== References ==\n* J.T. Linderoth and T.K. Ralphs, ''Noncommercial Software for Mixed-Integer Linear Programming, Integer Programming: Theory and Practice'', John Karlof (ed.), CRC Press Operations Research Series, 2005, 253-303. [http://coral.ie.lehigh.edu/pubs/files/jtl3_noncomm.pdf (Working Paper Version PDF)]\n\n== See also ==\n* [[COIN-OR|COIN-OR (Computational Infrastructure for Operations Research)]]\n\n==External links==\n* [http://coral.ie.lehigh.edu/minto/ MINTO Homepage]\n\n{{Mathematical optimization software}}\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "ModeFRONTIER",
      "url": "https://en.wikipedia.org/wiki/ModeFRONTIER",
      "text": "{{Multiple issues|\n{{Advert|date=December 2016}}\n{{notability|date=June 2017}}\n{{more citations needed|date=June 2017}}\n}}\n\n{{Infobox software\n| name                   = modeFRONTIER \n| developer              = Esteco s.p.a\n| latest_release_version = 2017R4 (5.6.0.1)\n| latest_release_date    = June 14, 2017\n| operating_system       = [[Cross-platform]]\n| genre                  = [[List of numerical analysis software|Technical computing]]\n| status                 = Active\n| license                = [[Proprietary software|Proprietary]]\n| website                = {{URL|http://www.esteco.com/}}\n}}\n\n'''modeFRONTIER''' is a [[multidisciplinary design optimization]] (MDO) platform developed by the Italian software house ESTECO SpA. Its workflow based environment, and '''multi-objective optimization algorithms''' are used for streamlining the engineering design process to cut time and cost while obtaining improved results.\n\n== History ==\nmodeFRONTIER was developed as a spin-off of the European research project on \"Design Optimization\", called FRONTIER. The project started in 1996 thanks to the collaboration of the following partners: [[British Aerospace]] (UK), Parallab (Norway), [[University of Trieste]], [[Newcastle University|University of Newcastle]] (UK), [[DASA|Daimler-Benz Aerospace (DASA)]] (Germany), [[Defence Evaluation and Research Agency|Defense Evaluation and Research Agency]] (UK), [[Electrolux]]-[[Zanussi]] (Italy) and Calortecnica (Italy). The project‚Äôs main objective was to develop a technology for design optimization based on the cornerstone of \"design analysis\".\n\nIn 1999, following the end of the project, ESTECO was founded with the aim of further developing a commercial version of the FRONTIER software, which later became modeFRONTIER. In 2000, the commercial software, FRONTIER 2.0 was released (following FRONTIER 1.0, released in 1998). In 2002, the 2.5 version is released under the new name, modeFRONTIER, still in use today.\n\n== Functionality ==\nmodeFRONTIER functionalities and features include third party software integration, process automation within a workflow-based environment, [[design space exploration]], real and RSM based optimization, RSM model creation and training, robust design and reliability, as well as a set of post processing tools for data analytics and visualization and decision making.\n\nmodeFRONTIER encompasses tools for process automation and integration, design space exploration, multidisciplinary optimization, RSM modeling and virtual optimization, robustness and reliability. modeFRONTIER includes a set of advanced optimization algorithms covering deterministic, stochastic and heuristic methods for both single and multi-objective problems. The platform comes with analytics and visualization tools that facilitate decision making.\n\n== Design Space Exploration ==\nSeveral design of experiments (DOE) modes are available in modeFRONTIER:\n* Space filler DOEs - serve as the starting point for a subsequent optimization process or a database for response surface training.\n* Statistical DOEs - are useful to create samplings for the sensitivity analysis thus allowing in-depth understanding of the problem by identifying the sources of variation.\n* Robustness and reliability DOEs - help create a set of stochastic points for robustness evaluation.\n* Optimal designs DOEs - are special purpose techniques used for reducing the dataset volume.\n\n== Multidisciplinary optimization ==\nmodeFRONTIER allows the user to choose the optimization strategy based on the design space boundaries and on the required reliability and robustness. The platform includes advanced algorithms for RSM-based and direct optimization, that cover deterministic, stochastic and heuristic methods for both single and multiobjective problems. It is possible to run algorithms within modeFRONTIER in manual, self-initializing or autonomous mode. In particular, its multi-strategy algorithms (Hybrid, FAST, pilOPT, SANGEA) are able to combine different optimization methods to further reduce time and effort needed to complete a design cycle.\n\n== Modularity ==\nIt is possible to access modeFRONTIER features through different standalone modules within the same installation, or directly in modeFRONTIER, according to the profile of the user, allowing for efficient streamlining of teamwork within multidisciplinary engineering processes.\n\n== modeSPACE ==\nAvailable as a standalone application, modeSPACE allows for role management within teams. The module includes a set of tools for data analysis and investigation of the characteristics of the engineering problem at hand, both in the post-processing phase and prior to optimization.\n\n== modePROCESS ==\nAn independent desktop application for visualizing processes in the form of graphical workflows specifying which parameters and simulations are required to solve an engineering design problem at hand.\n\n== modeFRONTIER for Calibration (mfC) ==\nmodeFRONTIER for Calibration toolbox is a dedicated solution for fast, automated model-based engine calibration, included in the modeFRONTIER platform. It presents a simplified modeFRONTIER interface tailored to the needs of engine calibration, while exploiting its optimization technology\n\nmFC automates the generation of tasks required at the engine test bench, generating optimal Engine Control Unit (ECU) maps.\n\n== Fields of application (commercial) ==\nmodeFRONTIER industrial and academic applications include automotive, aerospace, architecture, biotechnology, pharmaceutical sector, consumer goods, electronics, energy and environment, marine and offshore.\n\n== Academic applications ==\nUniversities that used modeFRONTIER for their academic projects and research include the University of Michigan, Georgia Institute of Technology, Tohoku University, Yokohama National University, Purdue University, Polytechnic University of Milan, University of Trieste, Beijing University, Massachusetts Institute of Technology, Rutgers University, Wayne State University.\n\n== EU-funded projects ==\nEuropean projects that have used modeFRONTIER include Gasvessel, Composelector, Utopiae, Umrida, Nodesim-CFD, Mulicube.\n\n==References==\n[https://www.engineering.com/BIM/ArticleID/12696/modeFRONTIER-2016-Adds-User-Profiles-to-Improve-CAE-Optimization-Workflows.aspx modeFRONTIER 2016 Adds User Profiles to Improve CAE Optimization Workflows]\n\n==External links==\n* [http://www.esteco.com/modefrontier modeFRONTIER] at Esteco website\n* [https://www.esteco.com ESTECO SpA website]\n\n[[Category:Computer system optimization software]]\n[[Category:Computer-aided design software]]\n[[Category:Computer-aided engineering software]]\n[[Category:Mathematical optimization software]]\n[[Category:Simulation software]]"
    },
    {
      "title": "LINDO",
      "url": "https://en.wikipedia.org/wiki/LINDO",
      "text": "{{Infobox Software\n| name                   = LINDO\n| developer              = LINDO SYSTEMS INC.\n| latest_release_version = 10.0\n| genre                  = [[Optimization (mathematics)|Mathematical optimization]]\n| status                 = Active\n| license                = [[Proprietary software|Proprietary]]\n| website                = {{Official website|http://www.lindo.com/|name=lindo.com}}\n}}\n\n'''LINDO''' ('''L'''inear, '''In'''teractive, and '''D'''iscrete '''O'''ptimizer) is a software package for [[linear programming]], [[integer programming]], [[nonlinear programming]], [[stochastic programming]] and [[global optimization]].<ref>Linus E. Schrage, Linear, Integer, and Quadratic Programming with Lindo, Scientific Press, 1986, {{ISBN|0894260901}}</ref>\n\nLindo also creates \"What'sBest!\" which is an add-in for linear, integer and nonlinear optimization. First released for Lotus 1-2-3<ref>{{cite magazine |last=Nash|first=John C.|date=1991-04-16|title=Optimizing Add-Ins: The Educated Guess, What'sBest!|magazine=PC Magazine|publisher= Ziff Davis|issn=0888-8507|volume=10|number=7|pages=130, 132}}</ref> and later also for Microsoft Excel.<ref>{{cite magazine |last=Arnett|first=Nick|date=1988-08-29|title=Spreadsheet Optimizer Ported to Macintosh|magazine=InfoWorld|publisher= IDG|issn=0199-6649|volume=10|number=35|page=24}}</ref>\n\n==References==\n{{reflist}}\n\n==External links==\n* Official website [http://www.lindo.com/]\n\n{{Mathematical optimization software}}\n\n{{DEFAULTSORT:Mosek}}\n[[Category:Mathematical optimization software]]\n[[Category:Numerical software]]\n\n\n{{compsci-stub}}"
    },
    {
      "title": "MOSEK",
      "url": "https://en.wikipedia.org/wiki/MOSEK",
      "text": "{{multiple issues|\n{{COI|date=May 2019}}\n{{third-party|date=May 2019}}\n}}\n{{Infobox Software\n| name                   = MOSEK\n| developer              = MOSEK ApS\n| latest_release_version = 9.0.x\n| genre                  = [[Optimization (mathematics)|Mathematical optimization]]\n| status                 = Active\n| license                = [[Proprietary software|Proprietary]]\n| website                = {{Official website|https://www.mosek.com|name=www.mosek.com}}\n|logo=[[File:mosek_logo.png|250px]]}}\n\n'''MOSEK''' is a software package for the solution of linear, mixed-integer linear, quadratic, mixed-integer quadratic, quadratically constraint, conic and convex nonlinear mathematical optimization problems. The emphasis in MOSEK is on solving large scale sparse problems, particularly the interior-point optimizer for linear, conic quadratic (a.k.a. [[Second-order cone programming]]) and semi-definite (aka. [[semidefinite programming]]). MOSEK is very efficient solving the latter set of problems.\n\nA special feature of the MOSEK interior-point optimizer is that it is based on the so-called homogeneous model. This implies that MOSEK can reliably detect a primal and/or dual infeasible status as documented in several published papers.<ref>E. D. Andersen and Y. Ye. A computational study of the homogeneous algorithm for large-scale convex optimization. Computational Optimization  and Applications, 10:243‚Äì269, 1998</ref><ref>E. D. Andersen and K. D. Andersen. The MOSEK interior point optimizer for linear programming: an implementation of the homogeneous algorithm.In H. Frenk, K. Roos, T. Terlaky, and S. Zhang, editors, High Performance Optimization, pages 197‚Äì232. Kluwer Academic Publishers, 2000</ref><ref>E. D. Andersen, C. Roos, and T. Terlaky. On implementing a primal-dual interior-point method for conic quadratic optimization. Math. Programming, 95(2), February 2003</ref>\n\nThe software is developed by Mosek ApS, a Danish company established in 1997. It has its office located in [[Copenhagen]], the capital of [[Denmark]].\n\nIn addition to the interior-point optimizer MOSEK includes:\n* Primal and dual simplex optimizer for linear problems.\n* Mixed-integer optimizer for linear, quadratic and conic problems.\n\nIn version 9, Mosek introduced support for exponential and power cones in its sover. The software also provides interfaces<ref>https://www.mosek.com/documentation/</ref> to the [[C (programming language)|C]], [[C Sharp (programming language)|C#]], [[Java programming language|Java]] and [[Python programming language|Python]] languages. Most major modeling systems are made compatible with MOSEK, examples are: [[AMPL]], and [[General Algebraic Modeling System|GAMS]].\nMOSEK can also be used from popular tools such as [[MATLAB]] and the [[R]] programming language / software environment. With the latter, an outdated version of package Rmosek is available from the CRAN server, the up-to-date version is provided by Mosek ApS<ref>http://docs.mosek.com/9.0/rmosek/index.html</ref>), [[CVX (optimization software)|CVX]], and [[YALMIP]].<ref>[http://users.isy.liu.se/johanl/yalmip/pmwiki.php?n=Solvers.MOSEK MOSEK @ Yalmip homepage]</ref>\n\n==References==\n{{reflist}}\n\n\n{{Mathematical optimization software}}\n\n{{DEFAULTSORT:Mosek}}\n[[Category:Mathematical optimization software]]\n[[Category:Numerical software]]\n\n\n{{compsci-stub}}"
    },
    {
      "title": "NEOS Server",
      "url": "https://en.wikipedia.org/wiki/NEOS_Server",
      "text": "The '''NEOS Server''' is an Internet-based [[client-server]] application that provides free access to a library of [[Solver|optimization solvers]]. Its library of solvers includes more than 60 commercial, free and open source solvers, which can be applied to [[mathematical optimization]] problems of more than 12 different types, including [[Linear optimization|linear programming]], [[integer programming]] and [[Nonlinear programming|nonlinear optimization]].\n\nThe server is managed by the Wisconsin Institute for Discovery at the [[University of Wisconsin-Madison]]. Most of the solvers are hosted by the University of Wisconsin in Madison, where jobs run on a cluster of high-performance machines managed by the [[HTCondor]] software. A smaller number of solvers are hosted by partner organizations: [[Arizona State University]], the [[University of Klagenfurt]] in Austria, and the [[University of Minho]] in Portugal. The  server was developed in 1996 by the Optimization Technology Center of  Argonne National Laboratory and [[Northwestern University]].\n\n[[File:Overview of the NEOS Server.jpg|thumb|Graphical depiction of the structure of the NEOS Server]]\n\n== Structure ==\nThe NEOS (Network-Enabled Optimization System) project<ref>{{cite web|last1=Savage|first1=Sam|title=NEOS Reaches New Milestone|url=http://www.redorbit.com/news/technology/1825088/neos_reaches_new_milestone/|website=redOrbit|accessdate=19 April 2016|date=February 18, 2010}}</ref> was launched in  at Argonne National Laboratory and Northwestern University to develop a method to share optimization software resources over the Internet.<ref name=czyzyk1997>{{cite journal|last1=Czyzyk|first1=Joseph|last2=Owen|first2=Jonathan H.|last3=Wright|first3=Stephen J.|title=Optimization on the Internet|journal=OR/MS Today|date=1997|volume=24|issue=5|pages=48‚Äì51|url=http://www.orms-today.org/orms-10-97/NEOS.html}}</ref><ref>{{cite journal|last1=Czyzyk|first1=Joseph|last2=Mesnier|first2=Michael P.|last3=Mor√©|first3=Jorge J.|title=The NEOS Server|journal=IEEE Journal on Computational Science and Engineering|date=1998|volume=5|issue=3|pages=68‚Äì75|doi=10.1109/99.714603}}\n</ref><ref>{{cite journal|last1=Dolan|first1=Elizabeth D.|last2=Fourer|first2=Robert|last3=Mor√©|first3=Jorge J.|last4=Munson|first4=Todd S.|title=Optimization on the NEOS Server|journal=SIAM News|date=2002|volume=35|issue=6|pages=8‚Äì9|url=https://www.siam.org/pdf/news/457.pdf}}</ref><ref>{{cite web|last1=Puget|first1=JeanFrancois|title=Computing the Really Optimal Tour Across the USA on the Cloud with Python|url=https://www.ibm.com/developerworks/community/blogs/jfp/entry/computing_the_really_optimal_tour_acrosss_the_usa_on_the_cloud_with_python?lang=en|website=IBM developerWorks|accessdate=19 April 2016|date=April 7, 2015}}</ref><ref>{{cite journal|last1=Gill|first1=Philip E.|last2=Murray|first2=Walter|last3=Saunders|first3=Michael A.|last4=Tomlin|first4=John A.|last5=Wright|first5=Margaret H.|title=George B. Dantzig and systems optimization|journal=Discrete Optimization|date=May 2008|volume=5|issue=2|pages=151‚Äì158|doi=10.1016/j.disopt.2007.01.002}}</ref> The server went live in 1996, one of the first examples of [[software as a service]].\n\nThe NEOS Server is an Internet-based client-server application that provides access to a library of optimization solvers. The server\naccepts optimization models described in modeling languages, programming languages, and problem-specific formats. Most of the linear programming, integer programming and nonlinear programming solvers accept input from [[AMPL]] and/or [[General Algebraic Modeling System|GAMS]]. Jobs can be submitted via a web page, email, [[XML RPC]], Kestrel<ref>{{cite journal|last1=Dolan|first1=Elizabeth D.|last2=Fourer|first2=Robert|last3=Goux|first3=Jean-Pierre|last4=Munson|first4=Todd S.|last5=Sarich|first5=Jason|title=Kestrel: An Interface from Optimization Modeling Systems to the NEOS Server|journal=INFORMS Journal on Computing|date=2008|volume=20|issue=4|pages=525‚Äì538|url=http://www.ampl.com/REFS/kestrel.pdf|doi=10.1287/ijoc.1080.0264}}</ref> or indirectly via third party submission tools  SolverStudio for Excel, OpenSolver, [[Pyomo]], [[JuMP]] (through the Julia package NEOS<ref>https://github.com/odow/NEOS.jl</ref>) and the [[R_(programming_language)#Packages|R package]] rneos. NEOS uses the HTCondor software to manage the workload on a dedicated cluster of computers.<ref>{{cite journal|last1=Ferris|first1=Michael C.|last2=Mesnier|first2=Michael P.|last3=Mor√©|first3=Jorge J.|title=NEOS and Condor: Solving Nonlinear Optimization Problems over the Internet|journal=ACM Transactions on Mathematical Software|date=2000|volume=26|pages=1‚Äì18|doi=10.1145/347837.347842|citeseerx=10.1.1.52.7788}}</ref>\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://www.neos-server.org NEOS Server]: official site of the NEOS Server\n* [http://www.neos-guide.org NEOS Guide]: official site of the NEOS Guide\n\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "Nl (format)",
      "url": "https://en.wikipedia.org/wiki/Nl_%28format%29",
      "text": "{{Infobox file format\n| icon =\n| logo =\n| screenshot =\n| extension = .nl\n| mime = \n| type code =\n| uniform type = \n| magic =\n| owner = [[Robert Fourer]]<br/> David Gay<br/> [[Brian Kernighan]]<br/>[[Bell Labs]]\n| genre = [[mathematical programming]]\n| container for =\n| contained by =\n| extended from = \n| extended to = \n| standard = \n| free = \n}}\n\n{{lowercase|nl}}\n'''nl''' is a file format for presenting and archiving [[mathematical programming]] problems.<ref>\n{{cite techreport\n | author=David Gay\n | title=Writing .nl Files\n | institution=[[Sandia National Laboratories]]\n | year=2005\n | location=Albuquerque, NM\n | url = https://cfwebprod.sandia.gov/cfdocs/CompResearch/docs/nlwrite20051130.pdf\n | citeseerx = 10.1.1.60.9659\n }}</ref> Initially this format has been invented for connecting solvers to [[AMPL]].<ref>\n{{cite techreport\n | author=David Gay\n | title=Hooking Your Solver to AMPL\n | institution=[[Bell Laboratories]]\n | year=1993\n | number=97-4-06\n | location=Murray Hill, NJ\n | url = http://www.ampl.com/REFS/hooking2.pdf\n }}</ref> It has also been adopted by other systems such as [[COIN-OR]] (as one of the input formats), [[FortSP]] (for interacting with external solvers), and [[Coopr]] (as one of its output formats).\n\nThe nl format supports a wide range of problem types, among them:\n* [[Linear programming]]\n* [[Quadratic programming]]\n* [[Nonlinear programming]]\n* [[Linear_programming#Integer_unknowns|Mixed-integer programming]]\n* Mixed-integer quadratic programming with or without [[Convex function|convex]] quadratic constraints\n* Mixed-integer nonlinear programming\n* [[Second-order cone programming]]\n* [[Global optimization]]\n* [[Semidefinite programming]] problems with [[Bilinear form|bilinear]] matrix inequalities\n* [[Complementarity theory|Complementarity problems]] (MPECs) in discrete or continuous variables\n* [[Constraint programming]]<ref name=\"cp-support\">\n{{Cite journal\n  | author1 = Robert Fourer\n  | author2 = David M. Gay\n  | title = Extending an Algebraic Modeling Language to Support Constraint Programming\n  | journal = INFORMS Journal on Computing\n  | volume = 14\n  | issue = 4\n  | pages = 322‚Äì344\n  | year = 2002\n  | url = http://joc.journal.informs.org/content/14/4/322\n  | doi=10.1287/ijoc.14.4.322.2825| citeseerx = 10.1.1.8.9699\n  }}\n</ref>\n\nThe nl format is low-level and is designed for compactness, not for readability. It has both binary and textual representation.\nMost commercial and academic solvers accept this format either directly or through special driver programs.\n\nThe open-source AMPL Solver Library (ASL) distributed via [[Netlib]] <ref>http://www.netlib.org/ampl/</ref> and AMPL/MP library <ref>https://github.com/ampl/mp</ref> provide nl [[Parsing|parsers]] that are used in many solvers.\n\n==See also==\n*[[sol (format)]] - a file format for presenting solutions of mathematical programming problems\n\n==References==\n{{Reflist}}\n\n\n{{Mathematical optimization software}}\n[[Category:Mathematical optimization software]]\n[[Category:Computer file formats]]"
    },
    {
      "title": "NMath",
      "url": "https://en.wikipedia.org/wiki/NMath",
      "text": "{{Infobox software\n| name = NMath\n| logo =\n| screenshot = <!-- Deleted image removed: [[File:NMath Screenshot.JPG|240px|NMath Stats]] -->\n| caption = NMath in Visual Studio environment\n| developer = [[CenterSpace Software]]\n| latest release version = 6.0\n| latest release date = {{Start date and age|2014|08|df=yes}}\n| operating system = [[Microsoft Windows|Windows]]\n| genre = [[List of numerical libraries|Numerical component libraries]]\n| license = [[Proprietary software|Proprietary]]\n| website = {{URL|http://www.centerspace.net/}}\n}}\n\n'''NMath''' is a numerical package for the [[Microsoft]] [[.NET Framework]]. It is developed by [[CenterSpace Software]]. Version 1.0 was released in March, 2003 as '''NMath Core'''. The current version is called NMath 6.0, released in August, 2014.\n\nNMath is built on [[Math Kernel Library|MKL]], a numerical library from [[Intel Corporation|Intel]].<ref name=\"MKL forums\">{{cite web |url=http://software.intel.com/en-us/forums/showthread.php?t=48812|title=Microsoft Visual C# and Intel MKL|publisher=Intel|date=|accessdate=16 February 2010}}</ref><ref name=\"MathFSharp\">{{cite web | url=http://fsharp.org/guides/math-and-statistics/ |title=Guide - Math and Statistics Programming with F# | last= | first= | work=| publisher=fsharp.org|date=| archive-url=https://web.archive.org/web/20160425104519/http://fsharp.org/guides/math-and-statistics/ | archive-date=2016-04-25|access-date=2016-04-25 }}</ref>\n\n==Features==\nNMath contains vector and matrix classes, complex numbers, factorizations, decompositions, linear programming, minimization, root-finding, structured and sparse matrix, least squares, polynomials, simulated annealing, curve fitting, numerical integration and differentiationing.<ref name=\"Description of features from CenterSpace web site\">{{cite web |url=http://www.centerspace.net/products/nmath|title=.NET Math Library|publisher=CenterSpace Software|date=3 March 2009|accessdate=16 February 2010}}</ref><ref name=\"MathForumNmath\">{{cite web | url=http://mathforum.org/library/topics/num_analysis/?start_at=101 | \ntitle=List of Mathematical Libraries| last= | first= | work=| publisher=Math Forum |date=| archive-url=https://web.archive.org/web/20160425102956/http://mathforum.org/library/topics/num_analysis/?start_at=101 | archive-date=2016-04-25|access-date=2016-04-25 }}</ref><ref name=\"FinancialAnalysis\">{{cite web | url=http://www.asymptotix.eu/content/quantitative-libraries-financial-predictive-analytics |title=Quantitative Libraries for Financial Predictive Analytics | last= | first= | work=| publisher=Asymptotix|date=31 August 2010| archive-url= https://web.archive.org/web/20160425111448/http://www.asymptotix.eu/content/quantitative-libraries-financial-predictive-analytics| archive-date=2016-04-25|access-date=2016-04-25 }}</ref>\n\n==See also==\n* [[Comparison of numerical analysis software]]\n* [[List of numerical analysis software]]\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://www.centerspace.net/ CenterSpace Software]\n\n{{DEFAULTSORT:Nmath}}\n[[Category:.NET programming tools]]\n[[Category:C Sharp libraries]]\n[[Category:C Sharp software]]\n[[Category:Component-based software engineering]]\n[[Category:Econometrics software]]\n[[Category:Mathematical optimization software]]\n[[Category:Numerical software]]\n[[Category:Programming tools for Windows]]\n[[Category:Windows-only software]]"
    },
    {
      "title": "OpenMDAO",
      "url": "https://en.wikipedia.org/wiki/OpenMDAO",
      "text": "{{Infobox Software\n| logo                   = OpenMDAO_logo.png\n| logo size              = 300px\n| name                   = OpenMDAO\n| developer              = [[Glenn Research Center|NASA Glenn Research Center]]\n| latest_release_version = 2.3.0\n| operating_system       = [[Cross-platform]]\n| genre                  = [[List of optimization software|Technical computing]]\n| status                 = Active\n| license                = [[Apache License]] 2.0\n| website                = {{URL|http://openmdao.org/}}\n}}\n\n'''OpenMDAO''' is an open-source high-performance computing platform for systems analysis and multidisciplinary optimization written in the [[Python (programming language)|Python]] programming language. \n\nThe OpenMDAO project is primarily focused on supporting gradient based optimization with analytic derivatives to allow you to explore large design spaces with hundreds or thousands of design variables, but the framework also has a number of parallel computing features that can work with gradient-free optimization, mixed-integer nonlinear programming, and traditional design space exploration.\n\nThe OpenMDAO framework is designed to aid in linking together separate pieces of software for the purpose of combined analyses. It allows users to combine analysis tools (or design codes) from multiple disciplines, at multiple levels of fidelity, and to manage the interaction between them. OpenMDAO is specifically designed to manage the dataflow (the actual data) and the workflow (what code is run when) in conjunction with optimization algorithms and other advanced solution techniques.<ref> J. S. Gray, J. T. Hwang, J. R. R. A. Martins, K. T. Moore, and B. A. Naylor. [https://link.springer.com/article/10.1007%2Fs00158-019-02211-z OpenMDAO: An open- source framework for multidisciplinary design, analysis, and optimization.] Structural and Multidisciplinary Optimization, 2019. [https://link.springer.com/article/10.1007%2Fs00158-019-02211-z doi:10.1007/s00158-019-02211-z.]</ref>\n\nThe development of OpenMDAO is led out of the [[Glenn Research Center|NASA Glenn Research Center]].\n\n==Features==\n* Library of built-in solvers and optimizers\n* Tools for [[metamodeling]]\n* Data recording capabilities\n* Support for analytic derivatives\n* Support for high-performance computer clusters and [[distributed computing]]\n* Extensible plugin library\n\n==Applications==\n[[NASA]]‚Äôs motivation in supporting the OpenMDAO project stems from the demands of unconventional aircraft concepts like Turbo-Electric [[distributed propulsion]]. Although NASA‚Äôs focus is on analyzing aerospace applications, the framework itself is general and is not specific to any discipline.\n\n==Framework structure==\n\nOpenMDAO is designed to separate the flow of information (dataflow) from the process in which analyses are executed (workflow). It does that by using four specific constructs: Component, Assembly, Driver, and Workflow.\n\nThe construction of system models begins with wrapping (or writing from scratch) various analysis codes as Components. A group of components is linked together inside an Assembly, specifying the dataflow between them.  Once the dataflow is in place, one can select specific Drivers (optimizers, solvers, design of experiments, etc.) and set up a Workflow to determine exactly how the problem should be solved.\n\nOpenMDAO also includes a web-browser-based graphical user interface (GUI) for visual construction, execution, and optimization of models.\n\n==See also==\n{{Portal|Free and open-source software|Python}}\n* [[ModelCenter]]\n* [[Simulink]]\n* [[Multidisciplinary design optimization]]\n\n==External links==\n* {{Official website|http://openmdao.org/}}\n\n==Notes==\n{{reflist}}\n\n[[Category:Mathematical optimization software]]\n[[Category:Free software programmed in Python]]\n[[Category:Cross-platform software]]\n[[Category:Software using the Apache license]]"
    },
    {
      "title": "OpenModelica",
      "url": "https://en.wikipedia.org/wiki/OpenModelica",
      "text": "{{Infobox software\n| name = OpenModelica\n| developer = Open Source Modelica Consortium (OSMC)\n| released = <!-- {{Start date|YYYY|MM|DD|df=yes/no}} -->\n| latest release version = 1.13.0\n| latest release date = {{Start date and age|2018|12|20|df=yes/no}}\n| repo = {{URL|https://github.com/OpenModelica/OpenModelica}}\n| programming language = [[C (programming language)|C]], [[C++]], [[MetaModelica]]\n| operating system = [[Linux]], [[Microsoft Windows|Windows]] and [[OS X]]\n| genre = Dynamic simulation and optimization\n| license = OSMC Public License, [[Eclipse Public License|EPL]], [[GNU General Public License|GPL]] ([[free software]])\n| website = {{URL|www.openmodelica.org}}\n}}\n\n'''OpenModelica'''<ref>{{Cite web|url=https://openmodelica.org|title=Welcome to OpenModelica - OpenModelica|last=Administrator|website=openmodelica.org|language=en-gb|access-date=2017-05-24}}</ref><ref>{{Cite web|url=https://github.com/OpenModelica/OpenModelica|title=OpenModelica/OpenModelica|website=GitHub|language=en|access-date=2017-05-24}}</ref> is a [[Free software|free]] and [[Open-source software|open source]] environment based on the [[Modelica]] modeling language for modeling, simulating, optimizing and analyzing complex dynamic systems. This software is actively developed by Open Source Modelica Consortium,<ref>[https://openmodelica.org/home/consortium  \"OSMC Home page\"]</ref> a non-profit, non-governmental organization. The Open Source Modelica Consortium is run as a project of [https://www.sics.se/groups/rise-sics-east RISE SICS East AB]  in collaboration with [[Link√∂ping University]].\n\nOpenModelica is used in academic and industrial environments. Industrial applications include the use of OpenModelica along with proprietary software in the fields of power plant optimization,<ref>[http://new.abb.com/power-generation/power-plant-optimization \"ABB Power Plant optimization\"]</ref> automotive<ref>[http://www.wolfram.com/system-modeler/ \"Wolfram modeler\"]</ref> and water treatment.<ref>[http://www.mikepoweredbydhi.com/products/mike-operations \" Mike operations\"]</ref>\n\n==Tools and Applications==\n\n===OpenModelica Compiler (OMC)===\n[https://openmodelica.org/?id=51:open-modelica-compiler-omc&catid=10:main-category OpenModelica Compiler] (OMC) is a [[Modelica]] compiler, translating Modelica to C code, with a symbol table containing definitions of classes, functions, and variables. Such definitions can be predefined, user-defined, or obtained from libraries. The compiler also includes a Modelica interpreter for interactive usage and constant expression evaluation. The subsystem also includes facilities for building simulation executables linked with selected numerical ODE or DAE solvers. The OMC is written in MetaModelica,<ref>{{Cite book|last=Pop|first=Adrian|last2=Fritzson|first2=Peter|date=2006-09-13|title=MetaModelica: A Unified Equation-Based Semantical and Mathematical Modeling Language|journal=Modular Programming Languages|volume=4228|language=en|pages=211‚Äì229|doi=10.1007/11860990_14|series=Lecture Notes in Computer Science|isbn=978-3-540-40927-4}}</ref> a unified equation-based semantical and mathematical modeling language and is [[bootstrapped]].\n\n===OpenModelica Connection Editor (OMEdit)===\n'''OpenModelica Connection Editor'''<ref>{{Cite journal|last=Adeel|first=Asghar, Syed|last2=Sonia|first2=Tariq|date=2010|title=Design and Implementation of a User Friendly OpenModelica Graphical Connection Editor|url=http://liu.diva-portal.org/smash/record.jsf?searchId=2&pid=diva2:399755&dswid=-6197}}</ref><ref>{{Cite web|url=https://openmodelica.org/?id=78:omconnectioneditoromedit&catid=10:main-category|title=OpenModelica Connection Editor (OMEdit) - OpenModelica|last=Administrator|website=openmodelica.org|language=en-gb|access-date=2017-05-24}}</ref> is an open source [[graphical user interface]] for creating, editing and simulating Modelica models in textual and graphical modes. OMEdit communicates with OMC through an interactive API, requests model information and creates models/connection diagrams based on the Modelica annotations. The implementation is based on C++ and the [[Qt (software)|Qt library]].\n\n=== OpenModelica Shell (OMShell) ===\nOpenModelica Shell (OMShell) is an interactive [[Command Line Interface]] that parses and interprets commands and Modelica expressions for evaluation, simulation, plotting, etc. The session handler also contains simple history facilities, and completion of file names and certain identifiers in commands.\n\n===OpenModelica Notebook (OMNotebook)===\nOpenModelica Notebook (OMNotebook), is a light-weight [[Mathematica]]-style editor for Modelica that implements interactive <abbr>WYSIWYG</abbr> realization of Literate Programming, a form of programming where programs are integrated with documentation in the same document. \n\nOMNotebook is primarily used for teaching and allows to mix hierarchically structured text with cells containing Modelica models and expressions. These can be evaluated, simulated and plotted with the results displayed directly in the OMNotebook. \n\n===OpenModelica Python Interface (OMPython)===\nOMPython is a Python interface enabling users to access the modeling and simulation capabilities of OpenModelica from Python. It uses [[Corba|CORBA]] (omniORB) or [[ZeroMQ|ZEROMQ]] to communicate with the OpenModelica scripting API.\n\n===Modelica Development Tooling (MDT)===\nMDT is an [[Eclipse (software)|Eclipse]] plugin that integrates the OpenModelica compiler with Eclipse. It provides and editor for advanced text based model editing with code assistance. MDT interacts with the OpenModelica Compiler through an existing [[Corba|CORBA]] based [[API]] and is used primarily in the development of the OpenModelica compiler.\n\n== See also ==\n* [[Modelica]]\n* [[Dymola]]\n* [[JModelica.org]]\n* [[Wolfram SystemModeler]]\n* [[SimulationX]]\n* [[Simulink]]\n\n==References==\n{{Reflist|2}}\n\n[[Category:Simulation software]]\n[[Category:Simulation programming languages]]\n[[Category:Mathematical optimization software]]\n[[Category:Free simulation software]]\n[[Category:Declarative programming languages]]\n[[Category:Object-oriented programming]]"
    },
    {
      "title": "OptaPlanner",
      "url": "https://en.wikipedia.org/wiki/OptaPlanner",
      "text": "{{Infobox Software \n| name                   = OptaPlanner\n| logo                   = [[Image:OptaPlannerLogo.png|215px]]\n| caption                =\n| developer              = [[Red Hat]]\n| latest release version = 7.9.0.Final\n| latest release date    = {{release date and age|2018|07|27}}\n| latest preview version =\n| latest preview date    =\n| operating system       = [[Cross-platform]]\n| programming language   = [[Java (programming language)|Java]]\n| genre                  = [[Mathematical optimization]]\n| license                = [[Apache License|ASL 2]]\n| website                = http://www.optaplanner.org\n}}\n\n'''OptaPlanner''' is an [[Open-source software|Open Source]] [[mathematical optimization|Constraint Solver]] written in [[Java_(programming_language)|Java]]. It solves [[constraint satisfaction problem]]s with construction heuristics and [[metaheuristic]] algorithms.\n\nIt's sponsored by [[Red Hat]], part of the [[List_of_JBoss_software|JBoss community]] and closely related to the [[Drools]] and [[jBPM]] projects in the KIE group.\n\n==History==\nIt was founded by Geoffrey De Smet in 2006 under the name Taseree.<ref>[https://www.optaplanner.org/blog/2016/08/07/ADecadeOfOptaPlanner.html A decade of OptaPlanner]</ref> In 2007, it joined the [[Drools]] project as Drools Solver. In 2009 it renamed to Drools Planner.\nIn March 2013, it [http://www.optaplanner.org/community/droolsPlannerRenamed.html graduated] from Drools project and finally renamed to OptaPlanner.\nIt's under continuous development by a dedicated core team (employed by [[Red Hat]]) and external community contributors.<ref>[https://www.optaplanner.org/community/team.html OptaPlanner - Team]</ref>\n\nRed Hat's support product for OptaPlanner is called ''Red Hat Business Optimizer'' (before 2018 it was called ''Red Hat JBoss Business Resource Planner''). Between March 2014 and March 2015, Red Hat's BRMS and BPM Suite 6.0 subscriptions included it as Tech Preview. As of April 2015, BRMS and BPM Suite 6.1 and higher include it as Full Support.<ref>[https://www.redhat.com/en/about/press-releases/red-hat-helps-enterprises-optimize-complex-planning-and-scheduling-challenges-new-business-resource-planner Red Hat Helps Enterprises Optimize Complex Planning and Scheduling Challenges with New Business Resource Planner]</ref>\n\n==Research competitions results==\nOptaPlanner contributors regularly compete against academic researchers in research competitions. Their results include:\n* [http://iconchallenge.insight-centre.org/ ICON Challenge on Algorithm Selection (2014)] (2nd place)\n* [http://www.cs.qub.ac.uk/itc2007/ International Timetabling Competition (2007)] (4th place on Track 1)\n\n==References==\n{{Reflist}}\n\n==External links==\n* {{Official website}}\n\n{{Mathematical optimization software}}\n\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "Optimization Programming Language",
      "url": "https://en.wikipedia.org/wiki/Optimization_Programming_Language",
      "text": "'''Optimization Programming Language''' ('''OPL''') is an [[algebraic modeling language]] for mathematical optimization models, which makes the coding easier and shorter than with a general-purpose programming language.<ref>http://www-01.ibm.com/software/commerce/optimization/modeling/index.html</ref> It is part of the [[CPLEX]] software package and therefore tailored for the IBM ILOG CPLEX and IBM ILOG CPLEX CP Optimizers. The original author of OPL is [[Pascal Van Hentenryck]].\n\n== References ==\n{{Reflist}}\n\n\n[[Category:Mathematical optimization software]]\n[[Category:Algebraic modeling languages]]"
    },
    {
      "title": "Optimization Toolbox",
      "url": "https://en.wikipedia.org/wiki/Optimization_Toolbox",
      "text": "{{Infobox Software\n| name                   = Optimization Toolbox\n| caption                = MATLAB plot showing steps of Optimization Toolbox solver fmincon on a nonlinear function\n| developer              = [[MathWorks]]\n| latest_release_version = R2018a\n| latest_release_date    = {{release date|2018|03|16}}\n| operating_system       = [[Cross-platform]]<ref>[http://www.mathworks.com/products/availability/#OP?s_cid=wiki_optimizationtoolbox_1 MathWorks - Optimization Toolbox - Requirements]</ref>\n| genre                  = [[List of optimization software]]\n| license                = [[Proprietary software|Proprietary]]\n| website                = [http://www.mathworks.com/products/optimization/ Optimization Toolbox]\n}}\n\n'''Optimization Toolbox''' is an [[List of optimization software|optimization software]] package developed by MathWorks.  It is an add-on product to [[MATLAB]], and provides a library of solvers that can be used from the MATLAB environment.  The toolbox was first released for MATLAB in 1990.\n\n== Optimization algorithms ==\n\nOptimization Toolbox has algorithms for:\n* [[Linear programming]]\n* [[Integer programming|Mixed-integer linear programming]]\n* [[Quadratic programming]]\n* [[Nonlinear programming]]\n* [[Linear least squares (mathematics)|Linear least squares]]\n* [[Non-linear least squares|Nonlinear least squares]]\n* Nonlinear equation solving\n* [[Multi-objective optimization]]\n\n== Applications ==\n\n=== Engineering Optimization ===\n\nOptimization Toolbox solvers are used for engineering applications in MATLAB, such as [[optimal control]] and optimal mechanical designs. \n<ref>{{cite web|title=Comparative Study of Numerical Methods for Optimal Control of a Biomechanical System | last=Dragain| first=Andreas | publisher= Chalmers University of Technology | year=2009 | url=http://publications.lib.chalmers.se/records/fulltext/134033.pdf| accessdate=2013-07-01}}</ref>\n<ref>{{cite book|last=Rao|first=Singiresu|title=Engineering Optimization: Theory and Practice|year=2009|publisher=John Wiley & Sons|location=Hoboken, New Jersey|isbn=978-0-470-18352-6|page=36}}</ref>\n\n=== Parameter Estimation ===\n\nOptimization can help with fitting a model to data, where the goal is to identify the model parameters that minimize the difference between simulated and experimental data.  Common parameter estimation problems that are solved with Optimization Toolbox include estimating material parameters and estimating coefficients of [[ordinary differential equations]].\n<ref>{{cite journal|last=Banks|first=H.T.|title=Material parameter estimation and hypothesis testing on a 1D viscoelastic stenosis model: Methodology|journal=Journal of Inverse and Ill-Posed Problems|year=2013|volume=21|issue=1|pages=25‚Äì57|doi=10.1515/jip-2012-0081|display-authors=etal}}</ref> \n<ref>{{cite journal|last=Collins Licata|first=A.|title=A Physiologically Based Pharmacokinetic Model for Methyl tert-Butyl Ether in Humans: Implementing Sensitivity and Variability Analyses|journal=Toxicological Sciences|year=2001|volume=62|pages=191‚Äì204|doi=10.1093/toxsci/62.2.191|pmid=11452131|issue=2|display-authors=etal}}</ref>\n\n=== Computational Finance ===\n\n[[Portfolio optimization]], [[cashflow matching]], and other computational finance problems are solved with Optimization Toolbox.\n<ref>{{cite book|last=Pachamanova|first=D.|title=Simulation and Optimization in Finance + Web Site|year=2010|publisher=John Wiley & Sons|location=Hoboken, New Jersey|isbn=978-0-470-37189-3}}</ref>\n\n=== Utilities and Energy ===\n\nOptimization Toolbox solvers are used for security constrained optimal power flow and power systems analysis.\n<ref>{{cite journal|last=Cartina|first=G.|title=Power System Analysis using MATLAB Toolboxes|journal=6th International Conference on Electromechanical and Power Systems|year=2007|pages=305‚Äì308|display-authors=etal}}</ref>\n\n== See also ==\n\n* [[Mathematical optimization]]\n\n== References ==\n{{reflist|2}}\n\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "OptimJ",
      "url": "https://en.wikipedia.org/wiki/OptimJ",
      "text": "{{Infobox programming language\n | name                   = OptimJ\n | paradigm               = [[object-oriented programming|object-oriented]]\n | released               = {{Start date|2006}}\n | designer               = [[Ateji]]\n | influenced by          = [[Java (programming language)|Java]]\n | free download          = [http://www.ateji.com/optimj/freeoptimjdownload.html]\n | website                = [http://www.ateji.com/ www.Ateji.com]\n}}\n'''OptimJ''' is an extension of the [[Java (programming language)|Java]] with language support for writing optimization models and abstractions for bulk data processing. The extensions and the proprietary product implementing the extensions were developed by Ateji which went out of business in September 2011.<ref>\n {{cite web\n  | title=Ateji is closed\n  | url=http://www.ateji.com/blog/ateji-is-closed/\n  | accessdate=2012-01-11}}\n</ref>\nOptimJ aims at providing a clear and concise algebraic notation for optimization modeling, removing compatibility barriers between optimization modeling and application programming tools, and bringing software engineering techniques such as object-orientation and modern IDE support to optimization experts.\n\nOptimJ models are directly compatible with Java source code, existing Java libraries such as database access, Excel connection or graphical interfaces. OptimJ is compatible with development tools such as Eclipse, CVS, JUnit or JavaDoc. OptimJ is available free with the following solvers: lp_solve, glpk, LP or MPS file formats and also supports the following commercial solvers: [[Gurobi]], [[MOSEK]], IBM ILOG CPLEX Optimization Studio.\n\n==Language concepts==\n\nOptimJ combines concepts from object-oriented imperative languages with concepts from [[algebraic modeling language]]s for optimization problems. Here we will review the optimization concepts added to Java, starting with a concrete example.\n\n==The example of map coloring==\n\nThe goal of a map coloring problem is to color a map so that regions sharing a common border have different colors. It can be expressed in OptimJ as follows.\n\n<source lang=\"java\">\npackage examples;\n\n// a simple model for the map-coloring problem\npublic model SimpleColoring solver lpsolve\n{\n  // maximum number of colors\n  int nbColors = 4;\n\n  // decision variables hold the color of each country\n  var int belgium in 1 .. nbColors;\n  var int denmark in 1 .. nbColors;\n  var int germany in 1 .. nbColors;\n\n  // neighbouring countries must have a different color\n  constraints {\n    belgium != germany;\n    germany != denmark;\n  }\n\n  // a main entry point to test our model\n  public static void main(String[] args)\n  {\n    // instantiate the model\n    SimpleColoring m = new SimpleColoring();\n\n    // solve it\n    m.extract();\n    m.solve();\n\n    // print solutions\n    System.out.println(\"Belgium: \" + m.value(m.belgium));\n    System.out.println(\"Denmark: \" + m.value(m.denmark));\n    System.out.println(\"Germany: \" + m.value(m.germany));\n  }\n}\n</source>\n\nReaders familiar with Java will notice a strong similarity with this language. Indeed, OptimJ is a [[conservative extension]] of Java: every valid Java program is also a valid OptimJ program and has the same behavior.\n\nThis map coloring example also shows features specific to optimization that have no direct equivalent in Java, introduced by the keywords <code>model</code>, <code>var</code>, <code>constraints</code>.\n\n==OR-specific concepts==\n\n===Models===\n\nA model is an extension of a Java class that can contain not only fields and methods but also constraints and an objective function. It is introduced by the <code>model</code> keyword and follows the same rules as class declarations. A non-abstract model must be linked to a solver, introduced by the keyword <code>solver</code>. The capabilities of the solver will determine what kind of constraints can be expressed in the model, for instance a linear solver such as [[lp solve]] will only allow linear constraints.\n\n<source lang=\"java\">\npublic model SimpleColoring solver lpsolve\n</source>\n\n===Decision variables===\n\nImperative languages such as Java provide a notion of [[Imperative programming|imperative variable]]s, which basically represent memory locations that can be written to and read from.\n\nOptimJ also introduces the notion of a decision variable, which basically represents an unknown quantity whose value one is searching. A solution to an optimization problem is a set of values for all its decision variables that respects the constraints of the problem‚Äîwithout decision variables, it would not possible to express optimization problems. The term \"decision variable\" comes from the optimization community, but decision variables in OptimJ are the same concept as logical variables in logical languages such as Prolog.\n\nDecision variables have special types introduced by the keyword <code>var</code>. There is a <code>var</code> type for each possible Java type.\n\n<source lang=\"java\">\n  // a var type for a Java primitive type\n  var int x;\n\n  // a var type for a user-defined class\n  var MyClass y;\n</source>\n\nIn the map coloring example, decision variables were introduced together with the range of values they may take.\n\n<source lang=\"java\">\n  var int germany in 1 .. nbColors;\n</source>\n\nThis is just a shorthand equivalent to putting a constraint on the variable.\n\n===Constraints===\n\nConstraints express conditions that must be true in any solution of the problem. A constraint can be any Java boolean expression involving decision variables.\n\nIn the map coloring example, this set of constraints states that in any solution to the map coloring problem, the color of Belgium must be different from the color of Germany, and the color of Germany must be different from the color of Denmark.\n\n<source lang=\"java\">\n  constraints {\n    belgium != germany;\n    germany != denmark;\n  }\n</source>\n\nThe operator <code>!=</code> is the standard Java not-equal operator.\n\nConstraints typically come in batches and can be quantified with the <code>forall</code> operator. For instance, instead of listing all countries and their neighbors explicitly in the source code, one may have an array of countries, an array of decision variables representing the color of each country, and an array <code>boolean[][] neighboring</code> or a predicate (a boolean function) <code>boolean isNeighbor()</code>. \n\n<source lang=\"java\">\nconstraints {\n  forall(Country c1 : countries, Country c2 : countries, :isNeighbor(c1,c2)) {\n    color[c1] != color[c2];\n  }\n}\n</source>\n\n<code>Country c1 : countries</code> is a generator: it iterates <code>c1</code> over all the values in the collection <code>countries</code>.\n\n<code>:isNeighbor(c1,c2)</code> is a filter: it keeps only the generated values for which the predicate is true (the symbol <code>:</code> may be read as \"if\").\n\nAssuming that the array <code>countries</code> contains <code>belgium</code>, <code>germany</code> and <code>denmark</code>, and that the predicate <code>isNeighbor</code> returns <code>true</code> for the couples (<code>Belgium </code>, <code>Germany</code>) and (<code>Germany</code>, <code>Denmark</code>), then this code is equivalent to the constraints block of the original map coloring example.\n\n===Objectives===\n\nOptionally, when a model describes an optimization problem, an objective function to be minimized or maximized can be stated in the model.\n\n==Generalist concepts==\n\nGeneralist concepts are programming concepts that are not specific to OR problems and would make sense for any kind of application development. The generalist concepts added to Java by OptimJ make the expression of OR models easier or more concise. They are often present in older modeling languages and thus provide OR experts with a familiar way of expressing their models.\n\n===Associative arrays===\n\nWhile Java arrays can only be indexed by 0-based integers, OptimJ arrays can be indexed by values of any type. Such arrays are typically called [[associative array]]s or maps. In this example, the array <code>age</code> contains the age of persons, identified by their name:\n\n<source lang=\"java\">\n  int[String] age;\n</source>\n\nThe type <code>int[String]</code> denoting an array of <code>int</code> indexed by <code>String</code>. Accessing OptimJ arrays using the standard Java syntax:\n\n<source lang=\"java\">\n  age[\"Stephan\"] = 37;\n  x = age[\"Lynda\"];\n</source>\n\nTraditionally, associative arrays are heavily used in the expression of optimization problems. OptimJ associative arrays are very handy when associated to their specific initialization syntax. Initial values can be given in [[intention|intensional definition]], as in:\n\n<source lang=\"java\">\nint[String] age = { \n  \"Stephan\" -> 37,\n  \"Lynda\"   -> 29 \n};\n</source>\n\nor can be given in [[Extensibility|extensional definition]], as in:\n\n<source lang=\"java\">\n  int[String] length[String name : names] = name.length();\n</source>\n\nHere each of the entries <code>length[i]</code> is initialized with <code>names[i].length()</code>.\n\n===Extended initialization===\n\n===Tuples===\n\n[[Tuple]]s are ubiquitous in computing, but absent from most mainstream languages including Java. OptimJ provides a notion of tuple at the language level that can be very useful as indexes in combination with associative arrays.\n\n<source lang=\"java\">\n  (: int, String :) myTuple = new (: 3, \"Three\" :);\n  String s = myTuple#1;\n</source>\n\nTuple types and tuple values are both written between <code>(:</code> and <code>:)</code>.\n\n===Ranges===\n\n===Comprehensions===\n\n[[List comprehension|Comprehension]]s, also called aggregates operations or reductions, are OptimJ expressions that extend a given binary operation over a collection of values. A common example is the sum:\n\n<source lang=\"java\">\n  // the sum of all integers from 1 to 10\n  int k = sum { i | int i in 1 .. 10};\n</source>\n\nThis construction is very similar to the big-sigma [[summation]] notation used in mathematics, with a syntax compatible with the Java language.\n\nComprehensions can also be used to build collections, such as lists, sets, multisets or maps:\n\n<source lang=\"java\">\n  // the set of all integers from 1 to 10\n  HashSet<Integer> s = `hashSet(){ i | int i in 1 .. 10};\n</source>\n\nComprehension expressions can have an arbitrary expression as target, as in:\n\n<source lang=\"java\">\n  // the sum of all squares of integers from 1 to 10\n  int k = sum { i*i | int i in 1 .. 10};\n</source>\n\nThey can also have an arbitrary number of generators and filters:\n\n<source lang=\"java\">\n  // the sum of all f(i,j), for 0<=i<10, 1<=j<=10 and i!=j \n  int k = sum{ f(i,j) | int i : 10, int j : 1 .. 10, :i!=j }\n</source>\n\nComprehension need not apply only to numeric values. Set or multiset-building comprehensions, especially in combination with tuples of strings, make it possible to express queries very similar to SQL database queries:\n\n<source lang=\"java\">\n  // select name from persons where age > 18\n  `multiSet(){ p.name | Person p : persons, :p.age > 18 }\n</source>\n\nIn the context of optimization models, comprehension expressions provide a concise and expressive way to pre-process and clean the input data, and format the output data.\n\n==Development environment==\n\nOptimJ is available as an Eclipse plug-in. The compiler implements a [[Source-to-source compiler|source-to-source translation]] from OptimJ to standard Java, thus providing immediate compatibility with most development tools of the Java ecosystem.\n\n==OptimJ GUI and Rapid Prototyping==\n\nSince the OptimJ compiler knows about the structure of all data used in models, it is able to generate a structured graphical view of this data at compile-time. This is especially relevant in the case of associative arrays where the compiler knows the collections used for indexing the various dimensions.\n\nThe basic graphical view generated by the compiler is reminiscent of an [[OLAP cube]]. It can then be customized in many different ways, from simple coloring up to providing new widgets for displaying data elements.\n\nThe compiler-generated OptimJ GUI saves the OR expert from writing all the glue code required when mapping graphical libraries to data. It enables rapid prototyping, by providing immediate visual hints about the structure of data.\n\nAnother part of the OptimJ GUI reports in real time performance statistics from the solver. This information can be used for understanding performance problems and improving solving time. At this time, it is available only for lp_solve.\n\n==Supported solvers==\n\nOptimJ is available for free with the following solvers lp_solve, glpk, LP or MPS file formats and also supports the following commercial solvers: [[Gurobi]], Mosek, IBM ILOG CPLEX Optimization Studio.\n\n==External links==\n*[https://fr.slideshare.net/PatrickViry/object-oriented-modeling-with-optim-j Object-oriented Modeling with OptimJ]\n*[https://fr.slideshare.net/PatrickViry/the-optimj-manual The OptimJ language manual]\n*[https://web.archive.org/web/20151220001557/http://www.ateji.com/optimj/optimj-gui.html OptimJ GUI]\n\n==References==\n{{Reflist}}\n*[http://www.rostudel.com/docs/rostudel-optimj-lisbon2010.pdf Rapid application development with OPTIMJ, a practitioner's experience report. David Gravot, Patrick Viry. EURO 2010 (Lisbon)]\n*[http://www.in-ter-trans.eu/resources/Zesch_Hellingrath_2010_Integrated+Production-Distribution+Planning.pdf OptimJ used in an optimization model for mixed-model assembly lines, University of M√ºnster]\n*[http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/viewFile/1769/2076 OptimJ used in an Approximate Subgame-Perfect Equilibrium Computation Technique for Repeated Games, Laval University]\n\n\n{{Mathematical optimization software}}\n[[Category:Object-oriented programming languages]]\n[[Category:Mathematical optimization software]]\n[[Category:Mathematical modeling]]\n[[Category:Discontinued programming languages]]\n[[Category:Programming languages created in 2006]]"
    },
    {
      "title": "Optimus platform",
      "url": "https://en.wikipedia.org/wiki/Optimus_platform",
      "text": "{{Multiple issues|\n{{notability|Products|date=May 2017}}\n{{primary sources|date=May 2017}}\n}}\n\n{{Infobox Software\n| name                   = Optimus\n| logo                   = Optimus Logo.gif\n| developer              = Noesis Solutions\n| latest_release_version = 2018.1\n| latest_release_date    = July, 2018\n| operating_system       = [[Cross-platform]]\n| genre                  = [[List of numerical analysis software|Technical computing]]\n| status                 = Active\n| license                = [[Proprietary software|Proprietary]]\n| website                = [http://www.noesissolutions.com]\n}}\n\n'''Optimus''' is a Process Integration and [[Multidisciplinary design optimization|Design Optimization]] ([[PIDO]]) platform developed by Noesis Solutions.  Noesis Solutions takes part in key research projects, such as MEGaFIT (zero-defect manufacturing of complex high-precision metal parts),<ref name=\"megafit-project\">{{cite web|title=MEGaFIT project facts|url=http://www.megafit-project.eu/}}</ref> JTI CleanSky Green Rotorcraft<ref name=\"cleansky\">{{cite web|title=Green Rotorcraft project page|url=http://www.cleansky.eu/content/page/green-rotorcraft}}</ref><ref>{{cite journal|last=d‚ÄôIppolito|first=R.|author2=Stevens, J. |author3=Pachidis, V. |author4=Berta, A. |author5=Goulos, I. |author6=Rizzi, C. |title=A Multidisciplinary Simulation Framework for Optimization of Rotorcraft Operations and Environmental Impac|journal=Proceedings of the 2nd International Conference on Engineering Optimization EngOpt 2010|date=06-09-2010}}</ref>   and iProd (Integrated management of product heterogeneous data).<ref name=\"iprod-project\">{{cite web|title=iProd fact sheet|url=http://www.iprod-project.eu/contenidoAction?ACTION=FIND&id_contenido=19&mimetype=application/pdf}}</ref><ref name=\"iprod-project_a\">{{cite web|title=iProd project partners|url=http://www.iprod-project.eu/partners}}</ref>\n\nOptimus allows the integration of multiple engineering software tools ([[Computer-aided design|CAD]], [[Multibody dynamics]], [[finite elements]], [[computational fluid dynamics]], ...) into a single and automated workflow.   Once a simulation process is captured in a workflow, Optimus will direct the simulations to explore the design space and to optimize product designs for improved functional performance and lower cost, while also minimizing the time required for the overall design process.\n\n==Process integration==\nThe Optimus GUI enables the creation of a graphical simulation workflow. A set of functions supports the integration of both commercial and in-house software. A simple workflow can cover a single simulation program, whereas more advanced workflows can include multiple simulation programs. These workflows may contain multiple branches, each with one or more simulation programs, and may include special statements that define looping and conditional branching.\n\nOptimus‚Äô workflow execution mechanism can range from a step-by-step review of the simulation process up to deployment on a large (and non-heterogeneous) computation cluster.  Optimus is integrated with several resource management systems to support parallel execution on a [[computer cluster|computational cluster]].\n\n==Design optimization==\nOptimus includes a wide range of methods and models to help solve design optimization problems:\t\n* [[Design of Experiments]] ([[design of experiments|DOE]])\n* [[response surface methodology|Response Surface Modeling]] ([[response surface methodology|RSM]])\n* [[Numerical optimization]], based on local or global algorithms, both for single or multiple objectives with continuous and/or discrete design variables\n\n===Design of Experiments (DOE)===\n\n: Design of Experiments (DOE) defines an optimal set of experiments in the design space in order to obtain the most relevant and accurate design information at minimal cost.  Optimus supports the following DOE methods:\n: * [[factorial experiment|Full Factorial]] (2-level & 3-level)\n: * Adjustable Full Factorial\n: * Fractional Factorial\n: * [[Plackett-Burman design|Plackett-Burman]]\n: * [[central composite design|Central composite]]\n: * Random\n: * [[latin hypercube|Latin-Hypercube]]\n: * Starpoints\n: * Diagonal\n: * Minimax & Maximin\n: * [[Optimal design]] (I-, D- & A-optimal)\n: * User-defined\n\n===Response Surface Modeling (RSM)===\n\n[[response surface methodology|Response Surface Modeling]] ([[response surface methodology|RSM]]) is a collection of mathematical and statistical techniques that are useful to model and analyze problems in which a design response of interest is influenced by several design parameters. [[design of experiments|DOE]] methods in combination with RSM can predict design response values for combinations of input design parameters that were not previously calculated, with very little simulation effort.  RSM thus allows further post-processing of DOE results.\n\nOptimus‚Äô Response Surface Modeling range from classical [[Least squares|Least Squares]] methods to advanced Stochastic Interpolation methods, including [[Kriging]], [[Neural Network]], [[radial basis function|Radial Basis Functions]]  and [[Gaussian Process]] models. To maximize RSM accuracy, Optimus can also generate the best RSM automatically ‚Äì drawing from a large set of RSM algorithms and optimizing the RSM using a cross-validation approach.\n\n===Numerical Optimization===\n\nOptimus supports a range of single-objective and multi-objective methods.  Multi-objective methods include NLPQL (gradient-based optimization algorithm) and NSEA+ (Non-dominant Sorting Evolution Algorithm).  Multi-objective optimization methods usually generate a so-called ‚ÄûPareto front‚Äú or use a weighting function to generate a single Pareto point.\n \nBased on the search methods, Optimus optimization methods can be categorized into:\n* local optimization methods - searching for an optimum based on local information of the optimization problem (such as gradient information). Methods include\n: * [[Sequential quadratic programming|SQP]] ([[Sequential quadratic programming|Sequential Quadratic Programming]])\n: * NLPQL\n: * Generalized Reduced Gradient\n\n* [[global optimization]] methods - searching for the optimum based on global information of the optimization problem. These are usually probability-based searching methods. Methods include\n: * [[Genetic algorithm]]s ([[Differential evolution|Differential Evolution]], Self-adaptive Evolution, ...)\n: * [[Simulated Annealing]]\n\n* hybrid optimization methods, e.g. Efficient Global Optimization, combining the local and the global approach into one approach which usually relies on response surface modeling to find a global optimum.\n\n==Robust design optimization==\nIn order to assess the influence of real-world uncertainties and tolerances on a given design, Optimus contains [[Monte Carlo simulation|Monte Carlo Simulation]] as well as a [[First-order second-moment method|First-Order Second Moment method]] to estimate and improve the robustness of a design. Optimus calculates and optimizes the probability of failure using advanced reliability methods, including First-Order and Second-Order Reliability Methods.\n\nOptimus also includes a dedicated set of functionalities to set up a [[Genichi Taguchi|Taguchi]] study through the definition of control factors, noise factors and signal factors in case of a dynamic study.  [[Genichi Taguchi]], a Japanese engineer, published his first book on [[experimental design]] in 1958. The aim of the [[Taguchi methods|Taguchi design]] is to make a product or process more stable in the face of variations over which there is little or no control (for example, ensuring reliable performance of a car engine for different ambient temperatures).\n\n==Applications==\nThe use of Optimus covers a wide range of applications, including \n* [http://www.noesissolutions.com/Noesis/center-wing-box-factory-optimization optimization of the production process of a center wing box (CWB) factory in function of production rate variations]\n* [http://www.noesissolutions.com/Noesis/food-supplement-tablet-shape identification of the best possible design trade-off between ease of swallowing and durability, based on finite element based analysis of food supplement tablet hardness and punch strength simulations]\n* [http://www.noesissolutions.com/Noesis/industries/automotive/hev-prototype engineering of a hybrid electric vehicle (HEV) prototype for fuel economy]<ref>{{cite journal|last=Carello|first=M.|author2=Filippo, N. |author3=d'Ippolito, R. |title=Performance optimization for the XAM hybrid electric vehicle prototype|journal=Proceedings of the SAE World Congress SAE 2012|date=2012-04-24}}</ref>\n\n==External links==\n* [http://www.noesissolutions.com Noesis Solutions website]\n* [http://www.noesissolutions.com/Noesis/software Optimus integration with engineering software]\n* [http://www.noesissolutions.com/Noesis/process-optimization/automate-execution/parallel-execution Optimus integration with resource management systems]\n* [http://www.noesissolutions.com/Noesis/industries Optimus industry applications]\n\n==References==\n{{reflist}}\n\n[[Category:Computer system optimization software]]\n[[Category:Computer-aided engineering software]]\n[[Category:Computer-aided design software]]\n[[Category:Mathematical optimization software]]\n[[Category:Simulation software]]"
    },
    {
      "title": "OptiSLang",
      "url": "https://en.wikipedia.org/wiki/OptiSLang",
      "text": "{{Multiple issues|\n{{notability|Products|date=May 2015}}\n{{refimprove|date=March 2016}}\n}}\n\n{{Infobox Software\n| name                   = optiSLang\n| logo                   = Kugel_Dynardo_OSL_P_RGB.jpg\n| screenshot             = OptiSLangScreenShot.png\n| caption                = \n| developer              = Dynardo GmbH\n| latest_release_version = 7.4.0<ref>''[https://www.dynardo.de/en/software/optislang/changelog-ansys-optislang.html ANSYS optiSLang changelog]'', Dynardo, May 2019</ref>\n| latest_release_date    = May 2019\n| status                 = Active\n| operating_system       = [[Cross-platform]]\n| platform               = [[IA-32|Intel x86 32-bit]], [[x86-64]]\n| language               = English\n| genre               = [[Simulation software]]\n| license                = [[Proprietary software|Proprietary]] [[commercial software]]\n| website                = [http://www.dynardo.de/en/software/optislang.html optiSLang product page]\n}}\n\n'''optiSLang''' is a software platform for [[Computer-aided engineering|CAE]]-based [[sensitivity analysis]], [[Multidisciplinary design optimization|multi-disciplinary optimization (MDO)]] and robustness evaluation. It is developed by Dynardo GmbH and provides a framework for numerical Robust Design Optimization (RDO) and stochastic analysis by identifying variables which contribute most to a predefined optimization goal. This includes also the evaluation of robustness, i.e. the sensitivity towards scatter of design variables or random fluctuations of parameters.<ref name=\"OSL_broschure\" />\n\n== Methodology ==\n\n'''[[Sensitivity analysis]]:'''\n<br/>\nRepresenting continuous optimization variables by uniform distributions without variable interactions, variance based sensitivity analysis quantifies the contribution of the optimization variables for a possible improvement of the model responses. In contrast to local derivative based sensitivity methods, the variance based approach quantifies the contribution with respect to the defined variable ranges.\n\n'''Coefficient of Prognosis (CoP)'''<ref name=\"Paper_Most_2011\"/>\n<br/>\nThe CoP is a model independent measure to assess the model quality and is defined as follows:\n\n<math> CoP = 1 - \\frac{SS_E^{\\text{pred}}}{SS_T}</math>\n\nWhere <math>SS_E^{\\text{pred}}</math> is the sum of squared prediction errors. These errors are estimated based on [[Cross-validation (statistics)|cross validation]]. In the cross validation procedure, the set of support points is mapped to <math>q</math> subsets. Then the approximation model is built by removing subset <math>i</math> from the support points and approximating the subset model output <math>y_i</math> using the remaining point set. This means that the model quality is estimated only at those points which are not used to build the approximation model. Since the prediction error is used instead of the fit, this approach applies to regression and even interpolation models.\n\n'''Metamodel of Optimal Prognosis (MOP):'''<ref name=\"Paper_Most_2011\"/>\n<br/>\nThe prediction quality of an approximation model may be improved if unimportant variables are removed from the model. This idea is adopted in the Metamodel of Optimal Prognosis (MOP) which is based on the search for the optimal input variable set and the most appropriate approximation model (polynomial or Moving Least Squares with linear or quadratic basis). Due to the model independence and objectivity of the CoP measure, it is well suited to compare the different models in the different subspaces.\n\n'''[[Multidisciplinary design optimization|Multi-disciplinary optimization]]:'''\n<br/>\nThe optimal variable subspace and approximation model found by a CoP/MOP procedure can also be used for a pre-optimization before global optimizers (evolutionary algorithms, Adaptive Response Surface Methods, Gradient-based methods, biological-based methods) are used for a direct single-objective optimization. After conducting a sensitivity analysis using MOP/CoP, also a multi-objective optimization can be performed to determine the optimization potential within opposing objectives and to derive suitable weighting factors for a following single-objective optimization. Finally this single-objective optimization determines an optimal design.\n\n'''Robustness evaluation:'''\n<br/>\nIn variance-based robustness analysis, the variations of the critical model responses are investigated. In '''optiSLang''', random sampling methods are used to generate discrete samples of the joined probability density function of the given random variables. Based on these samples, which are evaluated by the solver similarly as in the sensitivity analysis, the statistical properties of the model responses as mean value, standard deviation, quantiles and higher order stochastic moments are estimated.\n\n'''Reliability analysis:'''\n<br/>\nWithin the framework of probabilistic safety assessment or reliability analysis, the scattering influences are modelled as random variables, which are defined by distribution type, stochastic moments and mutual correlations. The result of the analysis is the complementary of reliability, the probability of failure, which can be represented on a logarithmic scale.\n\n==Process integration==\noptiSLang is designed to use several solvers to investigate mechanical, mathematical, technical and any other quantifiable problems. Herein optiSLang provides direct interfaces for external programs:\n* [[Ansys|ANSYS]]\n* [[MATLAB]]\n* [[GNU Octave]]\n* [[Microsoft Excel|Excel]]\n* [[OpenOffice Calc]]\n* [[Python (programming language)|Python]]\n* [[Abaqus]]\n* [[SimulationX]]\n* [[CATIA]]\n* [[LS-DYNA]]\n* multiPlas\n* any software with text-based input definition\n\n== History ==\nSince the 1980s, research teams at the University of Innsbruck and Bauhaus-Universit√§t Weimar had been developing algorithms for optimization and reliability analysis in conjunction with [[Finite element method|finite element]] simulations. As a result, the software \"Structural Language (SLang)\" was created. In 2000, [[Computer-aided engineering|CAE]] engineers first applied it to conducted optimization and robustness analysis in the automotive industry. In 2001, the Dynardo GmbH was founded in 2003. Based on SLang, the software optiSLang was launched as an industrial solution for CAE-based [[sensitivity analysis]], [[optimization]], robustness evaluation and reliability analysis. In 2013, the current version optiSLang 4 was completely restructured with a new graphical user interface and extended interfaces to external CAE processes.\n<ref name=\"OSL_broschure\"/>\n\n== References ==\n{{Reflist|\nrefs=\n<ref name=\"OSL_broschure\">[http://www.optislang.com Product website]</ref>\n<ref name=Paper_Most_2011>{{cite journal|last1=Most|first1=Thomas|last2=Will|first2=Johannes|title=Sensitivity analysis using the Metamodel of Optimal Prognosis (MOP)|journal=Proceedings of WOST|date=2011|volume=8|url=http://www.dynardo.de/fileadmin/Material_Dynardo/bibliothek/WOST_8.0/Paper_Most.pdf}}</ref>\n}}\n\n== External links ==\n{{Refbegin}}\n* [http://www.dynardo.com Dynardo GmbH Website]\n*[http://www.cadfem.de/en/products.html Cadfem Products]\n*[http://www.carhs.de/downloads/gzpk13caec44/ajsdko908p/automotive_CAE_Companion_2013.pdf#page=71 Automotive CAE Companion 2013]\n*[http://resource.ansys.com/staticassets/ANSYS/staticassets/resourcelibrary/article/AA-V7-I2-Avoiding-Small-Mistakes-and-Huge-Costs.pdf ANSYS Advantage Magazine 02_2013]\n*[http://www.konstruktionspraxis.vogel.de/themen/digitale_konstruktion/berechnen/articles/397644/ Konstruktionspraxis.de 03_2013]\n*[http://www.konstruktionspraxis.vogel.de/themen/digitale_konstruktion/berechnen/articles/382127/index2.html Konstruktionspraxis.de 10_2012]\n*[http://www.konstruktionspraxis.vogel.de/themen/digitale_konstruktion/berechnen/articles/369037/ Konstruktionspraxis.de 06_2012]\n*[http://www.konstruktionspraxis.vogel.de/cad-software/articles/330013/ Konstruktionspraxis.de 09_2011]\n{{Refend}}\n\n[[Category:Computer system optimization software]]\n[[Category:Computer-aided design software]]\n[[Category:Computer-aided engineering software]]\n[[Category:Mathematical optimization software]]\n[[Category:Simulation software]]"
    },
    {
      "title": "PROPT",
      "url": "https://en.wikipedia.org/wiki/PROPT",
      "text": "{{Infobox Software\n| name                   = PROPT \n| developer              = [http://tomopt.com/ Tomlab Optimization Inc.]\n| latest_release_version = 7.8\n| latest_release_date    = {{release date|2011|12|16}}\n| operating_system       = [http://tomopt.com/tomlab/about/ TOMLAB - OS Support]\n| genre                  = [[List of numerical analysis software|Technical computing]]\n| license                = [[Proprietary software|Proprietary]]\n| website                = [http://tomdyn.com/ PROPT product page]\n}}\n\nThe '''PROPT'''<ref>{{cite book | title = PROPT - Matlab Optimal Control Software | first = Per | last = Rutquist |author2=M. M. Edvall  | date = June 2008 | url = http://tomopt.com/docs/TOMLAB_PROPT.pdf | location = 1260 SE Bishop Blvd Ste E, Pullman, WA 99163, USA | publisher = Tomlab Optimization Inc.}}</ref> [[MATLAB]] [[optimal control|Optimal Control]] Software is a new generation platform for solving applied optimal control (with [[Ordinary differential equation|ODE]] or [[Differential algebraic equation|DAE]] formulation) and [[Estimation theory|parameters estimation]] problems.\n\nThe platform was developed by MATLAB Programming Contest Winner, [http://se.mathworks.com/matlabcentral/contest/contests/26/winners Per Rutquist] in 2008. The most recent version has support for binary and integer variables as well as an automated scaling module.\n\n== Description ==\nPROPT is a combined [[Mathematical model|modeling]], [[Code generation (compiler)|compilation]] and solver engine, built upon the [[TomSym]] modeling class, for generation of highly complex optimal control problems. PROPT uses a [[Gauss pseudospectral method|pseudospectral]] [[Collocation method]] (with Gauss or Chebyshev points) for solving optimal control problems. This means that the solution takes the form of a [[Polynomial]], and this polynomial satisfies the DAE and the path [[Constraint (mathematics)|constraints]] at the collocation points.\n\nIn general PROPT has the following main functions:\n\n* Computation of the constant [[Matrix (mathematics)|matrices]] used for the [[Derivative|differentiation]] and [[Integral|integration]] of the polynomials used to approximate the solution to the [[Trajectory optimization]] problem.\n* Source transformation to turn user-supplied [[Expression (mathematics)|expressions]] into MATLAB code for the cost function <math> f </math> and constraint function <math> c </math> that are passed to a [[Nonlinear programming]] solver in [[TOMLAB]]. The source transformation package TomSym automatically generates first and second order derivatives.\n* Functionality for plotting and computing a variety of information for the solution to the problem.\n* Automatic detection of the following:\n** Linear and quadratic objective.\n** Simple bounds, linear and nonlinear constraints.\n** Non-optimized expressions.\n* Integrated support for [[Smooth function|non-smooth]]<ref>{{cite journal | title = Dynamic optimization of bioprocesses: efficient and robust numerical strategies | first = J. R. | last = Banga |author2=Balsa-Canto, E. |author3=Moles, C. G. |author4=Alonso, A. A. | date = 2003 | publisher = Journal of Biotechnology}}</ref> (hybrid) optimal control problems.\n* Module for automatic scaling of difficult space related problem.\n* Support for binary and integer variables, controls or states.\n\n== Modeling ==\n\nThe PROPT system uses the TomSym symbolic source transformation engine to model optimal control problems. It is possible to define [[Dependent and independent variables|independent]] variables, dependent functions, scalars and constant parameters:\n\n<source lang=\"matlab\">\n toms tf\n toms t\n p = tomPhase('p', t, 0, tf, 30);\n x0 = {tf == 20};\n cbox = {10 <= tf <= 40};\n\n toms z1\n cbox = {cbox; 0 <= z1 <= 500};\n x0 = {x0; z1 == 0};\n\n ki0 = [1e3; 1e7; 10; 1e-3];\n</source>\n\n=== States and controls ===\n\nStates and controls only differ in the sense that states need be continuous between phases.\n\n<source lang=\"matlab\">\n tomStates x1\n x0 = {icollocate({x1 == 0})};\n\n tomControls u1\n cbox = {-2 <= collocate(u1) <= 1};\n x0 = {x0; collocate(u1 == -0.01)};\n</source>\n\n=== Boundary, path, event and integral constraints ===\n\nA variety of boundary, path, event and integral constraints are shown below:\n\n<source lang=\"matlab\">\n cbnd = initial(x1 == 1);       % Starting point for x1\n cbnd = final(x1 == 1);         % End point for x1\n cbnd = final(x2 == 2);         % End point for x2\n pathc = collocate(x3 >= 0.5);  % Path constraint for x3\n intc  = {integrate(x2) == 1};  % Integral constraint for x2\n cbnd = final(x3 >= 0.5);       % Final event constraint for x3\n cbnd = initial(x1 <= 2.0);     % Initial event constraint x1\n</source>\n\n== Single-phase optimal control example ==\n'''Van der Pol Oscillator''' <ref>[http://tomdyn.com/examples/vanDerPol.html \"Van Der Pol Oscillator - Matlab Solution\", ''PROPT Home Page''] June, 2008.</ref>\n\nMinimize:\n\n<math>\n\\begin{matrix}\n  J_{x,t} & = & x_3(t_f) \\\\\n\\end{matrix}\n</math>\n\nSubject to:\n\n<math>\n\\begin{cases}\n  \\frac{dx_1}{dt} = (1-x_2^2)*x_1-x_2+u \\\\\n  \\frac{dx_2}{dt} = x_1 \\\\\n  \\frac{dx_3}{dt} = x_1^2+x_2^2+u^2 \\\\\n  x(t_0) = [0 \\ 1 \\ 0] \\\\\n  t_f = 5 \\\\\n  -0.3 \\le u \\le 1.0 \\\\\n\\end{cases}\n</math>\n\nTo solve the problem with PROPT the following code can be used (with 60 collocation points):\n\n<source lang=\"matlab\">\ntoms t\np = tomPhase('p', t, 0, 5, 60);\nsetPhase(p);\n\ntomStates x1 x2 x3\ntomControls u\n\n% Initial guess\nx0 = {icollocate({x1 == 0; x2 == 1; x3 == 0})\n    collocate(u == -0.01)};\n\n% Box constraints\ncbox = {-10  <= icollocate(x1) <= 10\n    -10  <= icollocate(x2) <= 10\n    -10  <= icollocate(x3) <= 10\n    -0.3 <= collocate(u)   <= 1};\n\n% Boundary constraints\ncbnd = initial({x1 == 0; x2 == 1; x3 == 0});\n\n% ODEs and path constraints\nceq = collocate({dot(x1) == (1-x2.^2).*x1-x2+u\n    dot(x2) == x1; dot(x3) == x1.^2+x2.^2+u.^2});\n\n% Objective\nobjective = final(x3);\n\n% Solve the problem\noptions = struct;\noptions.name = 'Van Der Pol';\nsolution = ezsolve(objective, {cbox, cbnd, ceq}, x0, options);\n</source>\n\n== Multi-phase optimal control example ==\n'''One-dimensional rocket''' <ref>[http://tomdyn.com/examples/onedimRocket.html \"One Dimensional Rocket Launch (2 Free Time)\", ''PROPT Home Page''] June, 2008.</ref> with free end time and undetermined phase shift\n\nMinimize:\n\n<math>\n\\begin{matrix}\n  J_{x,t} & = & tCut \\\\\n\\end{matrix}\n</math>\n\nSubject to:\n\n<math>\n\\begin{cases}\n  \\frac{dx_1}{dt} = x_2 \\\\\n  \\frac{dx_2}{dt} = a-g \\ (0 < t <= tCut) \\\\\n  \\frac{dx_2}{dt} = -g \\ (tCut < t < t_f) \\\\\n  x(t_0) = [0 \\ 0] \\\\\n  g = 1 \\\\\n  a = 2 \\\\\n  x_1(t_f) = 100 \\\\\n\\end{cases}\n</math>\n\nThe problem is solved with PROPT by creating two phases and connecting them:\n\n<source lang=\"matlab\">\ntoms t\ntoms tCut tp2\np1 = tomPhase('p1', t, 0, tCut, 20);\np2 = tomPhase('p2', t, tCut, tp2, 20);\n\ntf = tCut+tp2;\n\nx1p1 = tomState(p1,'x1p1');\nx2p1 = tomState(p1,'x2p1');\nx1p2 = tomState(p2,'x1p2');\nx2p2 = tomState(p2,'x2p2');\n\n% Initial guess\nx0 = {tCut==10\n    tf==15\n    icollocate(p1,{x1p1 == 50*tCut/10;x2p1 == 0;})\n    icollocate(p2,{x1p2 == 50+50*t/100;x2p2 == 0;})};\n\n% Box constraints\ncbox = {\n    1  <= tCut <= tf-0.00001\n    tf <= 100\n    0  <= icollocate(p1,x1p1)\n    0  <= icollocate(p1,x2p1)\n    0  <= icollocate(p2,x1p2)\n    0  <= icollocate(p2,x2p2)};\n\n% Boundary constraints\ncbnd = {initial(p1,{x1p1 == 0;x2p1 == 0;})\n    final(p2,x1p2 == 100)};\n\n% ODEs and path constraints\na = 2; g = 1;\nceq = {collocate(p1,{\n    dot(p1,x1p1) == x2p1\n    dot(p1,x2p1) == a-g})\n    collocate(p2,{\n    dot(p2,x1p2) == x2p2\n    dot(p2,x2p2) == -g})};\n\n% Objective\nobjective = tCut;\n\n% Link phase\nlink = {final(p1,x1p1) == initial(p2,x1p2)\n    final(p1,x2p1) == initial(p2,x2p2)};\n\n%% Solve the problem\noptions = struct;\noptions.name = 'One Dim Rocket';\nconstr = {cbox, cbnd, ceq, link};\nsolution = ezsolve(objective, constr, x0, options);\n</source>\n\n== Parameter estimation example ==\n'''Parameter estimation problem''' <ref>[http://tomdyn.com/examples/parameterEstimation.html \"Matlab Dynamic Parameter Estimation with PROPT\", ''PROPT Home Page''] June, 2008.</ref>\n\nMinimize:\n\n<math>\n\\begin{matrix}\n  J_{p} & = & \\sum_{i=1,2,3,5}{(x_1(t_i) - x_1^m(t_i))^2} \\\\\n\\end{matrix}\n</math>\n\nSubject to:\n\n<math>\n\\begin{cases}\n  \\frac{dx_1}{dt} = x_2 \\\\\n  \\frac{dx_2}{dt} = 1-2*x_2-x_1 \\\\\n  x_0 = [p_1 \\ p_2] \\\\\n  t_i = [1 \\ 2 \\ 3 \\ 5] \\\\\n  x_1^m(t_i) = [0.264 \\ 0.594 \\ 0.801 \\ 0.959] \\\\\n  |p_{1:2}| <= 1.5 \\\\\n\\end{cases}\n</math>\n\nIn the code below the problem is solved with a fine grid (10 collocation points). This solution is subsequently fine-tuned using 40 collocation points:\n\n<source lang=\"matlab\">\ntoms t p1 p2\nx1meas = [0.264;0.594;0.801;0.959];\ntmeas  = [1;2;3;5];\n\n% Box constraints\ncbox = {-1.5 <= p1 <= 1.5\n    -1.5 <= p2 <= 1.5};\n\n%% Solve the problem, using a successively larger number collocation points\nfor n=[10 40]\n    p = tomPhase('p', t, 0, 6, n);\n    setPhase(p);\n    tomStates x1 x2\n\n    % Initial guess\n    if n == 10\n        x0 = {p1 == 0; p2 == 0};\n    else\n        x0 = {p1 == p1opt; p2 == p2opt\n            icollocate({x1 == x1opt; x2 == x2opt})};\n    end\n\n    % Boundary constraints\n    cbnd = initial({x1 == p1; x2 == p2});\n\n    % ODEs and path constraints\n    x1err = sum((atPoints(tmeas,x1) - x1meas).^2);\n    ceq = collocate({dot(x1) == x2; dot(x2) == 1-2*x2-x1});\n\n    % Objective\n    objective = x1err;\n\n    %% Solve the problem\n    options = struct;\n    options.name   = 'Parameter Estimation';\n    options.solver = 'snopt';\n    solution = ezsolve(objective, {cbox, cbnd, ceq}, x0, options);\n\n    % Optimal x, p for starting point\n    x1opt = subs(x1, solution);\n    x2opt = subs(x2, solution);\n    p1opt = subs(p1, solution);\n    p2opt = subs(p2, solution);\nend\n</source>\n\n== Optimal control problems supported ==\n* Aerodynamic trajectory control<ref>{{cite journal | title = SOCS Release 6.5.0 | first = J. | last = Betts | date = 2007 | publisher = THE BOEING COMPANY}}</ref>\n* [[Bang-bang control]]<ref>{{cite journal | title = Solving Tough Optimal Control Problems by Network Enabled Optimization Server (NEOS) | first = J. | last = Liang |author2=Meng, M. |author3=Chen, Y. |author4=Fullmer, R. | date = 2003 | publisher = School of Engineering, Utah State University USA, Chinene University of Hong Kong China}}</ref>\n* [[Chemical engineering]]<ref>{{cite journal | title = A HYBRID METHOD FOR THE OPTIMAL CONTROL OF CHEMICAL PROCESSES | first = E. F. | last = Carrasco |author2=Banga, J. R.  | date = September 1998 | location = University of Wales, Swansea, UK | publisher = UKACC International Conference on CONTROL 98}}</ref>\n* [[Dynamical system|Dynamic systems]]<ref>{{cite journal | title = Second-order sensitivities of general dynamic systems with application to optimal control problems | first = V. S. | last = Vassiliadis |author2=Banga, J. R. |author3=Balsa-Canto, E. | date = 1999 | publisher = Chemical Engineering Science | volume = 54 | pages = 3851‚Äì3860}}</ref>\n* General optimal control\n* Large-scale linear control<ref>{{cite book | title = Iterative dynamic programming | first = R. | last = Luus | date = 2002 | publisher = Chapman and Hall/CRC}}</ref>\n* Multi-phase system control<ref>{{cite journal | title = A Java Application for the Solution of Optimal Control Problems | first = B. C. | last = Fabien  | date = 1998 | location = Stevens Way, Box 352600 Seattle, WA 98195, USA | publisher = Mechanical Engineering, University of Washington}}</ref>\n* [[Mechanical engineering]] design<ref>{{cite journal | title = MISER3: Optimal Control Toolbox User Manual, Matlab Beta Version 2.0 | first = L. S. | last = Jennings  |author2=Fisher, M. E.  | date = 2002 | location = Nedlands, WA 6907, Australia | publisher = Department of Mathematics, The University of Western Australia}}</ref>\n* Nondifferentiable control<ref>{{Cite book | title = Global Optimization of Chemical Processes using Stochastic Algorithms - State of the Art in Global Optimization: Computational Methods and Applications | first = J. R. | last = Banga |author2=Seider, W. D.  | editor-last = Floudas | editor-first = C. A. | editor2-last = Pardalos | editor2-first = P. M.\n| date = 1996 | location = Dordrecht, The Netherlands | publisher = Kluwer Academic Publishers | isbn = 0-7923-3838-3 | pages = 563‚Äì583 | postscript = <!--None-->}}</ref>\n* Parameters estimation for dynamic systems<ref>{{cite journal | title = Benchmarking Optimization Software with COPS | first = E. D. | last = Dolan |author2=More, J. J.  | date = January 2001 | location = 9700 South Cass Avenue, Argonne, Illinois 60439 | publisher = ARGONNE NATIONAL LABORATORY}}</ref>\n* [[Singular control]]\n\n== References ==\n{{reflist}}\n\n== External links ==\n* [http://tomopt.com/ TOMLAB] - Developer and distributor of the software.\n* [http://tomsym.com/ TomSym] - Source transformation engine used in software.\n* [http://tomdyn.com/ PROPT] - Home page for PROPT.\n* [http://tomopt.com/tomlab/products/snopt/solvers/SNOPT.php SNOPT] - Default solver used in PROPT.\n\n[[Category:Numerical software]]\n[[Category:Mathematical optimization software]]\n[[Category:Optimal control]]"
    },
    {
      "title": "Pyomo",
      "url": "https://en.wikipedia.org/wiki/Pyomo",
      "text": "{{Infobox programming language\n| name = Pyomo\n| logo = Pyomo Logo Without Text.png\n| caption = \n| paradigm = \n| year = {{Start date and age|2008}}\n| designer = Gabriel Hackebeil<br/>William E. Hart<br/>Carl Laird<br/>Bethany Nicholson<br/>John Siirola<br/>Jean-Paul Watson<br/>David Woodruff\n| developer = \n| latest_release_version = 5.6.1\n| latest_release_date = {{Start date and age|2019|01|18}}\n| latest_test_version = \n| latest_test_date = \n| turing-complete = \n| typing = \n| implementations = \n| dialects = \n| influenced_by = [[Python (programming language)|Python]], [[AMPL]], [[General Algebraic Modeling System]]\n| influenced = \n| operating_system = [[Cross-platform]]: [[Linux]], [[Mac OS X]] and [[Windows]]\n| license = [[BSD license]]\n| genre = [[Algebraic modeling language|Algebraic Modeling Language (AML)]]\n| website = {{URL|www.pyomo.org}}\n| file_ext = .py\n}}\n\n'''Pyomo''' is a collection of [[Python (programming language)|Python]] software packages for formulating optimization models.<ref name=\"HartLaird2017\">{{cite book |author1=William E. Hart |author2=Carl D. Laird |author3=Jean-Paul Watson |author4=David L. Woodruff |author5=Gabriel A. Hackebeil |author6=Bethany L. Nicholson |author7=John D. Siirola |title=Pyomo ‚Äî Optimization Modeling in Python |url=https://books.google.com/books?id=P4olDwAAQBAJ&printsec=frontcover#v=onepage&q=optimization&f=false |date=26 May 2017|publisher=Springer|isbn=978-3-319-58821-6}}</ref><ref name=\"pyomo-article\">\n{{cite news\n| title = Pyomo: modeling and solving mathematical programs in python\n| last = Hart\n| first = William |author2=Jean-Paul Watson |author3=David L. Woodruff\n| year = 2011\n| journal = Mathematical Programming Computation\n| number = 3\n| volume = 3\n| url = http://mpc.zib.de/index.php/mpc/article/viewfile/59/30}}\n</ref>\n\nPyomo was developed by William Hart and Jean-Paul Watson at [[Sandia National Laboratories]] and David Woodruff at [[University of California, Davis]].  Significant extensions to Pyomo were developed by Bethany Nicholson and John Siirola at [[Sandia National Laboratories]], Carl Laird at [[Purdue University]], and Gabriel Hackebeil.  Pyomo is an open-source project that is freely available, and it is licensed with the [[BSD_license|BSD]] license.  Pyomo is developed as part of the [[COIN-OR]] project.  Pyomo is a popular open-source software package that is used by a variety of government agencies and academic institutions.\n\n==Features==\n\nPyomo allows users to formulate [[Optimization (mathematics)|optimization]] problems in Python in a manner that is similar to the notation commonly used in mathematical optimization. Pyomo supports an object-oriented style of formulating optimization models, which are defined with a variety of modeling components:  sets, scalar and multidimensional parameters, decision variables, objectives, constraints, equations, disjunctions and more.  Optimization models can be initialized with python data, and external data sources can be defined using [[spreadsheet]]s, [[database]]s, various formats of text files.  Pyomo supports both abstract models, which are defined without data, and concrete models, which are defined with data.  In both cases, Pyomo allows for the separation of model and data.\n\nPyomo supports dozens of [[solver]]s, both open source and commercial, including many solvers supported by [[AMPL]], PICO, [[COIN-OR#CBC|CBC]], [[CPLEX]], [[IPOPT]], [[Gurobi]] and [[GLPK]]. Pyomo can either invoke the solver directly or asynchronous with a solver manager.  Solver managers support remote, asynchronous execution of solvers, which supports parallel execution of Pyomo scripts.  Solver interaction is performed with a variety of solver interfaces, depending on the solver being used.  A very generic\nsolver interface is supported with AMPL's [[nl (format)]].\n\n==Related software==\nThe following software packages integrate Pyomo as a library to support optimization modeling and analysis:\n\n* [[SolverStudio]] lets you use Excel to edit, save and solve optimisation models built using a variety of modeling languages, including Pyomo.<ref name=\"solverstudio-article\">\n{{cite news\n| title = SolverStudio: A New Tool for Better Optimisation and Simulation Modelling in Excel\n| first = Andrew\n| last = Mason\n| journal = INFORMS Transactions on Education\n| year = 2013\n| volume = 14\n| number = 1\n| pages = 45‚Äì52\n| url = https://pubsonline.informs.org/doi/pdf/10.1287/ited.2013.0112}}\n</ref> Pyomo is bundled with the [http://solverstudio.org/ SolverStudio] software.\n* [http://www.temoaproject.org/ TEMOA] (Tools for Energy Model Optimization and Assessment) is an open source modeling framework for conducting energy system analysis.<ref name=\"temoa-article\">\n{{cite news\n| title = The TEMOA Project: Tools for Energy Model Optimization and Analysis\n| first = Joseph\n| last = DeCarolis\n|author2=Kevin Hunter |author3=Sarat Sreepathi\n | conference = International Energy Workshop\n| year = 2010\n| location = Stockholm, Sweden\n| url = http://www.temoaproject.org/publications/DeCarolis_IEW2010_paper.pdf\n}}\n</ref> The core component of TEMOA is an energy economy optimization model. This model is formulated and optimized using Pyomo.\n* [https://adamgreenhall.github.io/minpower/ MinPower] is an open source toolkit for students and researchers in power systems. It is designed to make working with standard power system models simple and intuitive.<ref name=\"minpower-article\">\n{{cite news\n| title = Minpower: A power systems optimization toolkit\n| first = Adam\n| last = Greenhall |author2=Rich Christie |author3=Jean-Paul Watson\n| conference = Power and Energy Society General Meeting\n| year = 2012\n| url = http://adamgreenhall.github.io/minpower/minpower.pdf\n}}\n</ref> MinPower uses Pyomo to formulate and optimize these power system models.\n\n==See also==\n* [[Algebraic modeling language]]\n\n==References==\n{{reflist}}\n<!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. --->\n\n==External links==\n* Articles from IBM's developerWorks:\n** [http://www.ibm.com/developerworks/cloud/library/cl-optimizepythoncloud1/ Linear optimization in Python, Part 1: Solve complex problems in the cloud with Pyomo]\n** [http://www.ibm.com/developerworks/cloud/library/cl-optimizepythoncloud2/index.html Linear optimization in Python, Part 2: Build a scalable architecture in the cloud]\n* [https://thedatabarista.wordpress.com/2015/01/27/pyomo-meets-fantasy-football/ Pyomo Meets Fantasy Football]\n* [[APOPT]] Solver for [https://github.com/APMonitor/apopt LP, QP, MILP, NLP, and MINLP solutions in Pyomo]\n\n{{Mathematical optimization software}}\n\n[[Category:Python software]]\n[[Category:Mathematical optimization software]]\n[[Category:Software using the BSD license]]"
    },
    {
      "title": "SAMPL",
      "url": "https://en.wikipedia.org/wiki/SAMPL",
      "text": "{{Infobox programming language\n| name                   = SAMPL\n| logo                   = \n| caption                = \n| year                   = {{Start date and age|2001}}\n| designer               = Gautam Mitra, Enza Messina, Valente Patrick\n| paradigm               = [[Multi-paradigm programming language|multi-paradigm]]: [[Declarative programming|declarative]], [[Imperative programming|imperative]]\n| developer              = \n| latest_release_version = 20120523\n| latest_release_date    = {{Start date and age|2013|05|23}}\n| latest_test_version    = \n| latest_test_date       =  \n| turing-complete        = \n| typing                 = \n| implementations        = \n| dialects               = \n| influenced_by          = [[AMPL]]\n| influenced             = \n| operating_system       = [[Cross-platform|Cross-platform (multi-platform)]]\n| license                = [[Proprietary software|Proprietary]] | genre                  = [[Algebraic modeling language|Algebraic Modeling Language (AML)]]\n| website                = {{URL|www.optirisk-systems.com}}\n| file_ext               = .mod .dat .run .sampl\n}}\n\n'''SAMPL''', which stands for '''\"Stochastic [[AMPL]]\"''', is an [[algebraic modeling language]] resulting by expanding the well-known language [[AMPL]] with extended syntax and keywords. It is designed specifically for representing [[stochastic programming]] problems<ref name=\"ExtendingAlgebraicLanguages\">\n{{cite journal | author=Christian Valente, Gautam Mitra, Mustapha Sadki and Robert Fourer| title=Extending algebraic modelling languages for stochastic programming| journal=INFORMS Journal on Computing| year=2009| volume=21| issue=1| pages=107‚Äì122| url=http://joc.journal.informs.org/content/21/1/107.abstract| doi=10.1287/ijoc.1080.0282}}\n</ref> and, through recent extensions, problems with chance constraints, integrated chance constraints and [[robust optimization]] problems. \nIt can generate the deterministic equivalent version of the instances, using all the solvers AMPL connects to,<ref>http://www.ampl.com/solvers.html</ref> or generate an [[MPS (format)#extensions|SMPS]] representation and use specialized decomposition based solvers, like [[FortSP]].\n\n== Language Features ==\n\nSAMPL shares all language features with AMPL, and adds some constructs specifically designed for expressing scenario based [[stochastic programming]] and [[robust optimization]].\n\n=== Stochastic programming features and constructs ===\n\nTo express scenario-based SP problems, additional constructs describe the tree structure and group the decision variable into stages. Moreover, it is possible to specify which parameter stores the probabilities for each branch of the tree and which set represents the scenario set. Other constructs to easily define chance constraints and integrated chance constraint in an SP problem are available as well.\nUsing these language constructs allows to retain the structure of the problem, hence making it available to the solvers, which might exploit it using specialized decomposition methods like [[Benders' decomposition]] to speed-up the solution.\n\n=== Robust optimization constructs ===\n\nSAMPL supports constructs to describe three types of robust optimization formulations:\n* Soyster<ref name=\"Soyster\">{{cite journal | author=Allen L Soyster| title=Technical Note‚ÄîConvex Programming with Set-Inclusive Constraints and Applications to Inexact Linear Programming| journal=[[Operations Research (journal)|Operations Research]]| year=1974| volume=21| issue=5| pages=1154‚Äì1157| doi=10.1287/opre.21.5.1154}}</ref>\n* Bertsimas and Sim<ref>{{cite journal|last=Bertsimas|first=Dimitris|author2=Sim, Melvyn |title=The Price of Robustness|journal=Operations Research|year=2004|volume=52|issue=1|pages=35‚Äì53|doi=10.1287/opre.1030.0065}}</ref>\n* Ben-Tal and Nemirovski<ref name=\"BenTal\">{{cite journal |author1=Aharon Ben-Tal  |author2=Arkadi Nemirovski |lastauthoramp=yes | title=Robust convex optimization| journal=Mathematics of Operations Research| year=1998| volume=23| issue=4| pages=769‚Äì805| doi=10.1287/moor.23.4.769|citeseerx=10.1.1.135.798 }}</ref>\n\n== Availability ==\n\nSAMPL is currently available as a part of the software AMPLDev (distributed by [http://www.optirisk-systems.com www.optirisk-systems.com]). It supports many popular 32- and 64-bit platforms including [[Windows]], [[Linux]] and [[Mac OS X]]. A free evaluation version with limited functionality is available.<ref>http://optirisk-systems.com/products_ampldevSP.asp</ref>\n\n== A stochastic programming sample model ==\n\nThe following is the SAMPL version of a simple problem (Dakota<ref>{{cite journal| author=Higle, Julia L, Wallace, Stein W| title=Sensitivity analysis and uncertainty in linear programming| journal=Interfaces| year=2003| volume=33| number=4| pages=53‚Äì60| doi=10.1287/inte.33.4.53.16370}}</ref>), to show the SP related constructs. It does not include the data file, which follows the normal AMPL syntax (see the [[AMPL#example|example]] provided in the AMPL Wikipedia page for further reference).\n \n set Prod;\n set Resource;\n \n ''# Scenarios (future possible realizations)''\n '''scenarioset''' Scen;\n \n ''# Definition of the problem as a two-stage problem''\n '''tree''' Tree := '''twostage''';\n \n ''# Demand for each product in each scenario''\n '''random param''' Demand{Prod, Scen};\n \n ''# Probability of each scenario''\n '''probability''' P{Scen};\t\n \n ''# Cost of each unit of resource''\n param Cost{Resource};\n \n ''# Requirement in terms of resources units to produce one unit of each product''\n param ProdReq{Resource,Prod};\n \n ''# Selling price of each product''\n param Price{Prod};\n \n ''# Initial budget''\n param Budget;\n \n ''# Amount of resources to buy''\n var buy{r in Resource} >= 0, '''suffix stage''' 1;\n \n ''# Amount of each product to produce''\n var amountprod{p in Prod, s in Scen} >= 0, '''suffix stage''' 2;\n \n ''# Amount of each product to sell''\n var amountsell{p in Prod, s in Scen} >= 0, '''suffix stage''' 2;\n \n ''# Total final wealth, as expected total income from sales minus costs for the resources''\n maximize wealth: sum{s in Scen} P[s] * \n  (sum{p in Prod} Price[p] * amountsell[p,s] - sum{r in Resource} Cost[r] * buy[r]);\n \n subject to \n ''# Make sure you have enough resources to produce what we intend to''\n balance{r in Resource, s in Scen}: \n  buy[r] >= sum{p in Prod} ProdReq[r,p] * amountprod[p, s];\n ''# Make sure we do not sell what we did not produce''\n production{p in Prod, s in Scen}: amountsell[p,s] <= amountprod[p,s];\n ''# Make sure we do not sell more than the market demand''\n sales{p in Prod, s in Scen}: amountsell[p,s] <= Demand[p,s];\n ''# Respect initial budget''\n budgetres: sum{r in Resource} Cost[r] * buy[r] <= Budget;\n\n== Solvers connectivity ==\n\nSAMPL instance level format for SP problems is SMPS, and therefore the problem can be solved by any solver which supports that standard. One of such solvers (FortSP) is included in the standard SAMPL distribution. Regarding robust optimization problems, the needed solver depend on the specific formulation used, as Ben-Tal and Nemirovski formulation need a [[Second-order cone programming|second-order cone]] capable solver.\n\n== See also ==\n\n* [[Algebraic modeling language]]\n* [[AIMMS]]\n* [[AMPL]]\n* [[FortSP]]\n* [[General Algebraic Modeling System|GAMS]] ‚Äì General Algebraic Modeling System\n* [[GLPK]] ‚Äì free open source system based on a subset of AMPL\n* [[MPS (format)]]\n* [[Robust optimization]]\n* [[Stochastic programming]]\n\n== References ==\n{{reflist|30em}}\n\n== External links ==\n* [http://www.ampl.com AMPL home page]\n* [http://www.optirisk-systems.com OptiRisk Systems home page]\n\n[[Category:Computer algebra systems]]\n[[Category:Mathematical optimization software]]\n[[Category:Numerical programming languages]]\n[[Category:Mathematical modeling]]\n[[Category:Text-oriented programming languages]]\n[[Category:1990 software]]"
    },
    {
      "title": "SAS (software)",
      "url": "https://en.wikipedia.org/wiki/SAS_%28software%29",
      "text": "{{About|the software suite||SAS (disambiguation)}}\n{{Infobox software\n| name                   = SAS\n| logo                   = ‡¶∏‡ßç‡¶Ø‡¶æ‡¶∏ ‡¶≤‡ßã‡¶ó‡ßã.png\n| screenshot             = SAS 9 on Microsoft Windows.png\n| caption                = SAS 9 on Microsoft Windows\n| developer              = [[SAS Institute]]\n| programming language   = [[C (programming language)|C]]\n| released               = {{start date and age|1976}}\n| latest release version = 9.4\n| latest release date    = {{Start date and age|2013|07|10}}\n| operating system       = [[Microsoft Windows|Windows]], [[IBM mainframe]], [[Unix]]/[[Linux]], [[OpenVMS|OpenVMS Alpha]]\n| genre                  = [[Numerical analysis]]\n| license                = [[Proprietary software|Proprietary]]\n| website                = {{URL|www.sas.com/en_us/home.html}}\n}}\n\n'''SAS''' (previously \"'''Statistical Analysis System'''\")<ref>{{cite web|url=https://www.sas.com/en_us/company-information.html#history|title=About SAS|last=|first=|date=|website=|accessdate=5 July 2017}}</ref> is a software suite developed by [[SAS Institute]] for advanced analytics, [[multivariate analysis]], [[business intelligence]], [[data management]], and [[predictive analytics]].\n\nSAS was developed at [[North Carolina State University]] from 1966 until 1976, when SAS Institute was incorporated. SAS was further developed in the 1980s and 1990s with the addition of new statistical procedures, additional components and the introduction of [[JMP (statistical software)|JMP]]. A point-and-click interface was added in version 9 in 2004. A [[social media analytics]] product was added in 2010.\n\n==Technical overview and terminology==\nSAS is a software suite that can mine, alter, manage and retrieve data from a variety of sources and perform statistical analysis on it.<ref name=\"encycl\"/> SAS provides a graphical point-and-click user interface for non-technical users and more advanced options through the [[SAS language]].<ref name=\"encycl\">\n{{Cite book | title = Encyclopedia of Research Design Encyclopedia of research design | doi = 10.4135/9781412961288 | year = 2010 | isbn = 9781412961271 | pmid =  | pmc = | last1 = Salkind | first1 = Neil }}</ref>\n\nSAS programs have DATA steps, which retrieve and manipulate data, and PROC steps, which analyze the data.<ref name=\"court\"/> Each step consists of a series of statements.<ref name=\"DelwicheSlaughter2012\">\n{{cite book\n| author1=Lora D. Delwiche\n| author2=Susan J. Slaughter\n| title=The Little SAS Book: A Primer : a Programming Approach\n| url=https://books.google.com/books?id=WtZZ6sYA2_QC&pg=PA6|year=2012\n| publisher=SAS Institute\n| isbn=978-1-61290-400-9\n| pages=6}}</ref>\n\nThe DATA step has executable statements that result in the software taking an action, and declarative statements that provide instructions to read a data set or alter the data's appearance.<ref name=\"court\"/> The DATA step has two phases: compilation and execution. In the compilation phase, declarative statements are processed and syntax errors are identified. Afterwards, the execution phase processes each executable statement sequentially.<ref name=\"Li2013\">\n{{cite book\n| author=Arthur Li\n| title=Handbook of SAS DATA Step Programming\n| url=https://books.google.com/books?id=kBL_aEB6RX0C&pg=PA149\n| date=10 April 2013\n| publisher=CRC Press\n| isbn=978-1-4665-5238-8|page=149}}</ref> Data sets are organized into tables with rows called \"observations\" and columns called \"variables\". Additionally, each piece of data has a descriptor and a value.<ref name=\"court\">\n{{cite court\n| litigants =SAS Institute Inc. and World Programming Limited\n| court =England and Wales High Court (Chancery Division)\n| date =July 23, 2010\n| url=http://www.bailii.org/ew/cases/EWHC/Ch/2010/1829.html#para36 }}</ref><ref>\n{{cite web\n| first =Debbie\n| last =Buck\n| title =A Hands-On Introduction to SAS DATA Step Programming\n| publisher =SAS Institute\n| location =SUGI 30\n| url =http://www2.sas.com/proceedings/sugi30/134-30.pdf\n| accessdate =October 2, 2013}}</ref>\n\nThe PROC step consists of PROC statements that call upon named procedures. Procedures perform analysis and reporting on data sets to produce statistics, analyses, and graphics. There are more than 300 procedures and each one contains a substantial body of programming and statistical work.<ref name=\"court\"/> PROC statements can also display results, sort data or perform other operations.<ref name=\"DelwicheSlaughter2012\"/>\n\nSAS macros are pieces of code or variables that are coded once and referenced to perform repetitive tasks.<ref name=\"BassSolutions2007\">\n{{cite book\n| author1=N. Jyoti Bass\n| author2=K. Madhavi Lata & Kogent Solutions\n| title=Base Sas Programming Black Book, 2007 Ed\n| url=https://books.google.com/books?id=o9nVu8Xsd6kC&pg=PA365\n| date=1 September 2007\n| publisher=Dreamtech Press\n| isbn=978-81-7722-769-7\n| pages=365‚Äì}}</ref>\n\nSAS data can be published in HTML, PDF, Excel and other formats using the Output Delivery System, which was first introduced in 2007.<ref>\n{{cite journal\n| last =Tolbert\n| first =William\n| title =How to Win Friends and Influence People with the SAS Output Delivery System\n| journal =Clinical Medicine & Research\n| volume =8\n| issue =3‚Äì4\n| pages =189‚Äì190\n| date =December 1, 2010\n| url =http://www.clinmedres.org/content/8/3-4/189.3\n| accessdate =October 2, 2013\n| doi =10.3121/cmr.2010.943.c-c1-04 | pmc =3006529\n}}</ref> The SAS Enterprise Guide is SAS's point-and-click interface. It generates code to manipulate data or perform analysis automatically and does not require SAS programming experience to use.<ref>\n{{cite journal\n| last =Der\n| first =G.\n| author2=B. S. Everittt\n| title =Basic Statistics using SAS Enterprise Guide\n| journal =Journal of the Royal Statistical Society, Series A\n| volume =172\n| issue =2\n| pages =530\n| date =March 10, 2009\n| doi =10.1111/j.1467-985X.2009.00588_2.x }}</ref>\n\nThe SAS software suite has more than 200<ref name=\"Schermerhorn2011\">{{cite book|author=John R. Schermerhorn|title=Exploring Management|url=https://books.google.com/books?id=qNVP2L6iKi0C&pg=SL3-PA22|date=11 October 2011|publisher=John Wiley & Sons|isbn=978-0-470-87821-7|pages=3}}</ref> components<ref name=\"berk\">\n{{cite web\n| publisher=University of California, Berkeley\n| title=An Introduction to the SAS System\n| first=Phil|last=Spector\n| url=https://www.stat.berkeley.edu/classes/s100/sas.pdf\n| format=PDF\n| archiveurl=https://web.archive.org/web/20131012065352/http://www.stat.berkeley.edu/classes/s100/sas.pdf\n| archivedate=October 12, 2013\n| accessdate=October 4, 2013}}</ref><ref>\n{{cite web\n| url=http://www.nesug.org/Proceedings/nesug12/ma/ma10.pdf\n| title=Determine what SAS Version and Components are available\n| first=David\n| last=Chapman\n| publisher=NESUG\n| format=PDF\n| year=2012}}</ref> Some of the SAS components include:<ref name=\"encycl\"/><ref name=\"berk\"/><ref>\n{{Cite journal | last1 = Hallahan | first1 = C. | title = Data Analysis Using SAS | doi = 10.1177/0049124195023003006 | journal = Sociological Methods & Research | volume = 23 | issue = 3 | pages = 373‚Äì391 | year = 1995 | pmid =  | pmc = }}</ref>\n{{columns-list|colwidth=30em|\n* Base SAS ‚Äì Basic procedures and data management\n* SAS/STAT ‚Äì Statistical analysis\n* SAS/GRAPH ‚Äì Graphics and presentation\n* SAS/OR ‚Äì Operations research\n* SAS/ETS ‚Äì [[Econometrics]] and [[Time Series Analysis]]\n* SAS/IML ‚Äì Interactive matrix language\n* SAS/AF ‚Äì Applications facility\n* SAS/QC ‚Äì Quality control\n* SAS/INSIGHT ‚Äì [[Data mining]]\n* SAS/PH ‚Äì [[Clinical trial]] analysis\n* Enterprise Miner ‚Äì data mining\n* Enterprise Guide - GUI based code editor & project manager\n* SAS EBI - Suite of [[Business Intelligence]] Applications \n* SAS Grid Manager - Manager of SAS grid computing environment\n}}\n\n== History ==\n\n===Origins===\nThe development of SAS began in 1966 after [[North Carolina State University]] re-hired [[Anthony James Barr|Anthony Barr]]<ref name=\"NourseGreenberg1978\">{{cite journal|last1=Nourse|first1=E. Shepley|last2=Greenberg|first2=Bernard G.|last3=Cox|first3=Gertrude M.|last4=Mason|first4=David D.|last5=Grizzle|first5=James E.|last6=Johnson|first6=Norman L.|last7=Jones|first7=Lyle V.|last8=Monroe|first8=John|last9=Simons|first9=Gordon D.|title=Statistical Training and Research: The University of North Carolina System|journal=International Statistical Review / Revue Internationale de Statistique|volume=46|issue=2|year=1978|pages=171|issn=0306-7734|doi=10.2307/1402812|jstor=1402812}}</ref> to program his analysis of variance and regression software so that it would run on [[IBM System/360]] computers.<ref name=\"AgrestiMeng2012\">{{cite book|author1=Alan Agresti|author2=Xiao-Li Meng|title=Strength in Numbers: The Rising of Academic Statistics Departments in the U. S.: The Rising of Academic Statistics Departments in the U.S.|url=https://books.google.com/books?id=kPGJUiUCJZkC&pg=PA177|date=2 November 2012|publisher=Springer|isbn=978-1-4614-3649-2|pages=177}}</ref> The project was funded by the [[National Institute of Health]]<ref name=\"fda\"/> and was originally intended to analyze agricultural data<ref name=\"berk\"/><ref name=\"‚Äúlittle\">{{cite news|title=Little-known software giant to raise its profile|first=Emery|last=Dalesio|url=https://news.google.com/newspapers?nid=1916&dat=20010505&id=nPpIAAAAIBAJ&sjid=oAUNAAAAIBAJ&pg=3476,727402|date=May 5, 2001|publisher=Associated Press|accessdate=April 8, 2014}}</ref> to improve crop yields.<ref name=\"seventyeight\">{{cite news|first=David|last=Kaplan|newspaper=Fortune|url=http://money.cnn.com/2010/01/21/technology/sas_best_companies.fortune/|title=SAS: A new no. 1 best employer|date=January 22, 2010|accessdate=April 8, 2014|deadurl=yes|archiveurl=https://web.archive.org/web/20111129010236/http://money.cnn.com/2010/01/21/technology/sas_best_companies.fortune/|archivedate=November 29, 2011|df=}}</ref> Barr was joined by student [[James Goodnight]], who developed the software's statistical routines, and the two became project leaders.<ref name=\"NourseGreenberg1978\"/><ref name=\"AgrestiMeng2012\"/><ref name=\"timeline\"/> In 1968, Barr and Goodnight integrated new [[multiple regression]] and [[analysis of variance]] routines.<ref name=\"Attr76\">{{cite journal|first=Anthony|last=Barr|author2=James Goodnight |title=The SAS Staff|year=1976|quote=SAS 72 and SAS 76 are attributed to Barr, Goodnight, Service, Perkins, and Helwig}}</ref><ref>(Barr & Goodnight et al. 1979:front matter) Attribution of the development of various parts of the system to Barr, Goodnight, and Sall.</ref> In 1972, after issuing the first release of SAS, the project lost its funding.<ref name=\"fda\">{{citation|title=SAS Institute FDA Intellectual Partnership for Efficient Regulated Research Data Archival and Analyses|url=http://www.fda.gov/ohrms/dockets/dockets/00n0001/ts00016.pdf|publisher=Presented at Duke University|date=April 12, 2000|accessdate=September 28, 2011}}</ref> According to Goodnight, this was because NIH only wanted to fund projects with medical applications.<ref name=\"intervieww\">{{citation|title=Oral History Interview with Jim Goodnight|date=July 22, 1999|url=http://docsouth.unc.edu/sohp/I-0073/excerpts/excerpt_976.html|accessdate=April 8, 2014|publisher=Oral Histories of the American South}}</ref> Goodnight continued teaching at the university for a salary of $1 and access to mainframe computers for use with the project,<ref name=\"fda\"/> until it was funded by the [[University Statisticians of the Southern Experiment Stations]] the following year.<ref name=\"AgrestiMeng2012\"/><ref name=\"intervieww\"/> [[John Sall]] joined the project in 1973 and contributed to the software's econometrics, time series, and matrix algebra. Another early participant, Caroll G. Perkins, contributed to SAS' early programming. Jolayne W. Service and Jane T. Helwig created SAS' first documentation.<ref name=\"Attr76\"/>\n\nThe first versions of SAS were named after the year in which they were released.<ref name=\"expert\">{{cite web\n| title=History of SAS version\n| url=http://www.globalstatements.com/sas/differences/\n| publisher=Global Statements|first=Rick|last=Aster\n| accessdate=October 4, 2013}}</ref> In 1971, SAS 71 was published as a limited release.<ref name=\"encycl\"/><ref>\n{{cite book\n| first=Anthony|last=Barr |author2=James Goodnight |author3=James Howard\n| publisher=North Carolina State University|year=1971\n| title=Statistical analysis system\n|oclc=5728643 }}</ref> It was used only on IBM mainframes and had the main elements of SAS programming, such as the DATA step and the most common procedures in the PROC step.<ref name=\"expert\"/> The following year a full version was released as SAS 72, which introduced the MERGE statement and added features for handling missing data or combining data sets.<ref>\n{{cite book\n| last=Service\n| first=Jolayne\n| title=A User's Guide to the Statistical Analysis System\n| publisher=North Carolina State University\n| year=1972\n| url=http://www.worldcatlibraries.org/oclc/1325510}}</ref> In 1976, Barr, Goodnight, Sall, and Helwig removed the project from North Carolina State and incorporated it into [[SAS Institute|SAS Institute, Inc.]]<ref>\n{{cite news\n| first=Mary\n| last=Shacklett\n| date=September 5, 2013\n| url=http://www.techrepublic.com/blog/big-data-analytics/see-if-the-r-language-fits-in-your-big-data-toolkit/\n| publisher=The New Republic|accessdate=October 3, 2013\n| title=See if the R language fits in your big data toolkit}}</ref>\n\n===Development===\nSAS was re-designed in SAS 76 with an [[open architecture]] that allowed for [[compilers]] and procedures. The INPUT and INFILE statements were improved so they could read most data formats used by IBM mainframes. Generating reports was also added through the PUT and FILE statements. The ability to analyze [[general linear model]]s was also added<ref>\n{{cite book\n| last=Barr\n| first=Anthony\n| author2=James Goodnight\n| author3=James Sall\n| author4=John Helwig\n| author5=Jane T\n| title=SAS Programmer's Guide, 1979 Edition\n| publisher=SAS Institute, Inc.\n| year=1979\n| url=http://www.worldcatlibraries.org/oclc/4984363\n| oclc=4984363}}</ref> as was the FORMAT procedure, which allowed developers to customize the appearance of data.<ref name=\"expert\"/> In 1979, SAS 79 added support for the [[Conversational Monitor System|CMS operating system]] and introduced the DATASETS procedure. Three years later, SAS 82 introduced an early macro language and the APPEND procedure.<ref name=\"expert\"/>\n\nSAS version 4 had limited features, but made SAS more accessible. Version 5 introduced a complete macro language, array subscripts, and a full-screen interactive user interface called Display Manager.<ref name=\"expert\"/> In 1985, SAS was rewritten in the [[C programming language]]. This allowed for the SAS' Multivendor Architecture that allows the software to run on [[UNIX]], [[MS-DOS]], and [[Windows]]. It was previously written in [[PL/I]], [[Fortran]], and [[assembly language]].<ref name=\"timeline\"/><ref name=\"expert\"/>\n\nIn the 1980s and 1990s, SAS released a number of components to complement Base SAS. SAS/GRAPH, which produces graphics, was released in 1980, as well as the SAS/ETS component, which supports econometric and time series analysis. A component intended for pharmaceutical users, SAS/PH-Clinical, was released in the 1990s. The [[Food and Drug Administration]] standardized on SAS/PH-Clinical for new drug applications in 2002.<ref name=\"timeline\">\n{{cite news\n| url=http://www.wral.com/business/story/9211429/\n| title=SAS corporate timeline|date=March 3, 2011\n| accessdate=October 17, 2011}}</ref> Vertical products like SAS Financial Management and SAS Human Capital Management (then called CFO Vision and HR Vision respectively) were also introduced.<ref>\n{{cite web\n| url=http://www.sas.com/company/about/history.html#s1=5\n| title=SAS history\n| publisher=SAS Institute\n| accessdate=October 4, 2013}}</ref> [[JMP (statistical software)|JMP]] was developed by SAS co-founder [[John Sall]] and a team of developers to take advantage of the graphical user interface introduced in the 1984 [[Apple Macintosh]]<ref name=\"CoxGaudard2009\">\n{{cite book\n| author1=Ian Cox|author2=Marie A. Gaudard\n| author3=Philip J. Ramsey |author4=Mia L. Stephens |author5=Leo Wright\n| title=Visual Six Sigma: Making Data Analysis Lean\n| url=https://books.google.com/books?id=xdg9nkBFh1UC&pg=PA23\n| accessdate=16 November 2012|date=21 December 2009\n| publisher=John Wiley & Sons\n| isbn=978-0-470-50691-2\n| pages=23‚Äì}}</ref> and shipped for the first time in 1989.<ref name=\"CoxGaudard2009\"/> Updated versions of JMP were released continuously after 2002 with the most recent release being from 2016.<ref name=\"jmpone\">\n{{cite news\n| url=http://www.jmp.com/landing/foreword_pdf/JMPForeward_72dpi.pdf\n| title=Growing up|pp=5\n| accessdate=October 4, 2013\n| newspaper=JMP Forward\n| publisher=JMP}}</ref><ref name=\"jumptwo\">\n{{cite news\n| first=John\n| last=Saul\n| newspaper=JMPer Cable\n| url=http://www.jmp.com/about/newsletters/jmpercable/pdf/26_winter_2010.pdf\n| title=JMP is 20 Years Old\n| date=Winter 2010\n| accessdate=October 13, 2011}}</ref><ref>\n{{cite news\n| url=http://www.google.com/translate?hl=en&ie=UTF8&sl=auto&tl=en&u=http%3A%2F%2Fwww.diarioti.com%2Fnoticia%2FSAS_lanza_JMP_8_para_Mac%2F24733\n| title=Launches SAS JMP 8 for Mac and Linux\n| date=April 11, 2009\n| accessdate=December 30, 2012\n| newspaper=Ti Journal}}</ref><ref>\n{{cite web\n| title=New Features in JMP 9\n| url=http://www.jmp.com/support/downloads/pdf/jmp9/jmp9_new_features.pdf|publisher=JMP\n| accessdate=December 30, 2012}}</ref><ref>\n{{cite news\n| first=Adriian\n| url=http://www.drdobbs.com/tools/228200027?queryText=SAS%2BJMP\n| last=Bridgewater|newspaper=Dr. Dobb's Journal\n| title=JMP Genomics 5: Data Visualization & Exploration|date=November 3, 2010\n| accessdate=May 31, 2012}}</ref><ref>\n{{cite web\n| url=http://www.pharmasug.org/proceedings/2012/DG/PharmaSUG-2012-DG01.pdf\n| work=PharmaSUG 2012\n| title=Proficiency in JMP Visualization\n| first=Charles\n| last=Shipp\n| author2=Kirk Paul Lafler\n| accessdate=December 30, 2012}}</ref><ref name=\"thirtyeight\">\n{{cite news\n| first=James\n| last=Taylor\n| publisher=JTonEDM\n| title=First Look ‚Äì JMP Pro\n| date=August 10, 2011\n| accessdate=May 31, 2012\n| url=http://jtonedm.com/2011/08/10/first-look-jmp-pro/}}</ref>\n\nSAS version 6 was used throughout the 1990s and was available on a wider range of operating systems, including [[Macintosh]], [[OS/2]], [[Silicon Graphics]], and [[Primos]]. SAS introduced new features through dot-releases. From 6.06 to 6.09, a user interface based on the windows paradigm was introduced and support for SQL <ref>\n{{cite book\n| first=Kirk Paul\n| last=Lafler\n| title=PROC SQL: Beyond the Basics Using SAS, Second Edition\n| url=https://www.sas.com/store/books/categories/usage-and-reference/proc-sql-beyond-the-basics-using-sas-second-edition/prodBK_62432_en.html\n| publisher=SAS Institute\n| isbn=978-1-61290-027-8| year=2013\n}}</ref> was added.<ref name=\"expert\"/> Version 7 introduced the Output Delivery System (ODS) and an improved text editor. ODS was improved upon in successive releases. For example, more output options were added in version 8. The number of operating systems that were supported was reduced to [[UNIX]], [[Windows]] and [[z/OS]], and [[Linux]] was added.<ref name=\"expert\"/><ref name=\"supported systems\">\n{{cite web\n| url=http://support.sas.com/supportos/list\n| title=Supported Operating Systems\n| publisher=SAS Institute\n| accessdate=October 6, 2012}}</ref> SAS version 8 and SAS Enterprise Miner were released in 1999.<ref name=\"timeline\"/>\n\n===Recent history===\nIn 2002, the Text Miner software was introduced. Text Miner [[text mining|analyzes text data]] like emails for patterns in Business Intelligence applications.<ref>{{cite news|first=Emery|last=Dalesio|title=Text Miner program to bolster business intelligence|url=https://news.google.com/newspapers?nid=1734&dat=20020205&id=ljUgAAAAIBAJ&sjid=GFMEAAAAIBAJ&pg=4836,3215963|date=February 5, 2002|accessdate=April 8, 2014|publisher=Associated Press}}</ref> In 2004, SAS Version 9.0 was released, which was dubbed \"Project Mercury\" and was designed to make SAS accessible to a broader range of business users.<ref name=\"sixtyfour\">\n{{cite news\n| first=Dave\n| last=Steven\n| publisher=Pennsylvania State University\n| url=http://css.its.psu.edu/news/nlsu02/sas.html\n| title=SAS is Starting to Look Even Better...\n| date=July 29, 2002\n| accessdate=October 17, 2011}}</ref><ref name=\"sixtyfive\">\n{{cite news\n| first=Rick\n| last=Whiting\n| newspaper=InformationWeek\n| url=http://informationweek.com/news/18700087?queryText=SAS+announced\n| title=SAS Extends Business Intelligence to the Masses\n| date=March 31, 2004\n| accessdate=October 17, 2011}}</ref> Version 9.0 added custom user interfaces based on the user's role and established the point-and-click user interface of SAS Enterprise Guide as the software's primary [[graphical user interface]] (GUI).<ref name=\"sixtyfour\"/> The [[Customer Relationship Management]] (CRM) features were improved in 2004 with SAS Interaction Management.<ref name=\"sixtythree\">\n{{cite news\n| first=Dennis\n| last=Callaghan\n| newspaper=eWeek\n| url=http://www.eweek.com/c/a/Finance-IT/SAS-to-Add-to-Analytical-CRM-Arsenal/\n| title=SAS to Add to Analytical CRM Arsenal\n| date=September 26, 2002\n| accessdate=October 17, 2011}}</ref> In 2008 SAS announced Project Unity, designed to integrate data quality, data integration and master data management.<ref name=\"sixtysix\">\n{{cite news\n| first=Antone\n| last=Gonsalves\n| newspaper=InformationWeek\n| url=http://informationweek.com/news/software/bi/211100027?queryText=SAS+announced\n| title=SAS, DataFlux Unveil 'Project Unity'\n| date=October 10, 2008\n| accessdate=October 17, 2011}}</ref>\n\nSAS [[SAS Institute lawsuit with World Programming|sued World Programming]], the developers of a competing implementation, [[World Programming System]], alleging that they had infringed SAS's copyright in part by implementing the same functionality. This case was referred from the United Kingdom's [[High Court of Justice]] to the [[European Court of Justice]] on 11 August 2010.<ref name=\"ecj reference\">\n{{cite web\n| url=http://curia.europa.eu/juris/document/document.jsf?docid=82474&doclang=en&mode=&part=1\n| title=Reference for a preliminary ruling from High Court of Justice (Chancery Division) (England and Wales) made on 11 August 2010 ‚Äì SAS Institute Inc. v World Programming Ltd\n| publisher=European Court of Justice\n| accessdate=May 19, 2012}}</ref> In May 2012, the [[European Court of Justice]] ruled in favor of World Programming, finding that \"the functionality of a computer program and the programming language cannot be protected by copyright.\"<ref>\n{{cite web\n| url=http://curia.europa.eu/jcms/upload/docs/application/pdf/2012-05/cp120053en.pdf\n| title=The functionality of a computer program and the programming language cannot be protected by copyright\n| publisher=European Court of Justice\n| accessdate=May 19, 2012}}</ref>\n\nA free version was introduced for students in 2010.<ref name=\"fortyeight\">{{cite news|first=Quentin|last=Hardy|newspaper=Forbes|url=https://www.forbes.com/sites/quentinhardy/2011/06/09/sas-ibms-bad-culture-how-well-win/|title=SAS-We Spurned IBM, Now to Win|date=June 9, 2011|accessdate=October 17, 2011}}</ref> SAS Social Media Analytics, a tool for social media monitoring, engagement and [[sentiment analysis]], was also released that year.<ref name=\"twentyseven\">\n{{cite news\n| first=Paul\n| last=Greenberg\n| publisher=ZDNet\n| url=http://www.zdnet.com/blog/crm/the-crm-watchlist-part-ii-the-usual-suspects/2419?tag=mantle_skin;content\n| title=The CRM Watchlist Part II: The Usual Suspects\n| date=December 31, 2010\n| accessdate=October 4, 2013}}</ref><ref name=\"ninetyfour\"/> SAS Rapid Predictive Modeler (RPM), which creates basic analytical models using [[Microsoft Excel]], was introduced that same year.<ref name=\"ninetyfour\">\n{{cite web\n| publisher=UCLA Academic Technology Services\n| url=http://www.ats.ucla.edu/stat/sas/seminars/sas_macros_introduction/default.htm\n| title=Computing Seminars: Introduction to SAS Macro Language\n| accessdate=October 4, 2013}}</ref><ref name=\"sixtyseven\">\n{{cite news\n| first=Cindi\n| last=Howson\n| newspaper=InformationWeek\n| url=http://informationweek.com/blog/software/228900553?queryText=SAS+announced\n| title=SAS Takes Predictive Analytics Mainstream|date=September 7, 2010\n| accessdate=October 4, 2013}}</ref> JMP 9 in 2010 added a new interface for using the [[R (programming language)|R programming language]] from JMP and an add-in for Excel.<ref>{{citation|title=New Features in JMP 9|url=http://www.jmp.com/support/downloads/pdf/jmp9/jmp9_new_features.pdf|publisher=JMP|accessdate=December 30, 2012}}</ref><ref>{{cite news|first=Adriian|url=http://www.drdobbs.com/tools/228200027?queryText=SAS%2BJMP|last=Bridgewater|newspaper=Dr. Dobb's Journal|title=JMP Genomics 5: Data Visualization & Exploration|date=November 3, 2010|accessdate=May 31, 2012}}</ref> The following year, a [[High Performance Computing]] appliance was made available in a partnership with [[Teradata]] and [[EMC Greenplum]].<ref>\n{{cite news\n| first=Nicole\n| last=Laskowski\n| publisher=SearchBusinessAnalytics\n| url=http://searchbusinessanalytics.techtarget.com/news/2240102699/SAS-ups-big-data-ante-with-high-performance-computing-platform\n| title=SAS ups 'big data' ante with high-performance computing platform\n| date=October 26, 2011\n| accessdate=October 4, 2013}}</ref><ref>\n{{cite web\n| first=Madan\n| last=Sheina\n|author2=Surya Mukherjee\n | publisher=Ovum\n| url=http://ovum.com/2011/10/17/sas-adds-in-memory-to-high-performance-computing/\n| title=SAS adds in-memory to high-performance computing\n| date=October 17, 2011}}</ref> In 2011, the company released Enterprise Miner 7.1.<ref>\n{{cite news\n| first=James\n| last=Taylor\n| title=First Look ‚Äì SAS Enterprise Miner 7.1\n| url=http://jtonedm.com/2011/11/11/first-look-sas-enterprise-miner-7-1/\n| date=November 11, 2011\n| accessdate=October 4, 2013}}</ref> The company introduced 27 data management products from October 2013 to October 2014 and updates to 160 others.<ref>{{cite news|title=SAS expands cloud analytics business|first=Nestor|last=Arellano|date=October 22, 2014|url=http://www.itworldcanada.com/article/sas-expands-cloud-analytics-business/98450#ixzz3IzvMFJ9l|newspaper=IT World Canada|accessdate=November 13, 2014}}</ref> At the 2015 SAS Global Forum, it announced several new products that were specialized for different industries, as well as new training software.<ref>{{cite news|title=SAS enlarges its palette for big data analysis|first=Joab|last=Jackson|date=April 27, 2015|url=http://www.itworld.com/article/2915374/sas-enlarges-its-palette-for-big-data-analysis.html|publisher=IT World|accessdate=May 22, 2015}}</ref>\n\n=== Releases date ===\nSAS had many releases since 1972.<ref>{{Cite news|url=https://blogs.sas.com/content/iml/2013/08/02/how-old-is-your-version-of-sas-release-dates-for-sas-software.html|title=How old is your version of SAS? Release dates for SAS software|work=The DO Loop|access-date=2018-03-27|language=en-US}}</ref> Since release 9.3, SAS/STAT has its own release numbering.\n{| class=\"wikitable\"\n|+\n!Release\n!Date\n!Comment\n|-\n|72\n|January 1972\n|\n|-\n|76\n|July 1976\n|\n|-\n|79.5\n|April 1981\n|\n|-\n|82.4\n|January 1983\n|\n|-\n|4.06\n|March 1984\n|\n|-\n|5.03\n|July 1986\n|\n|-\n|6.01\n|January 1985\n|PC-DOS\n|-\n|6.03\n|March 1988\n|\n|-\n|6.06\n|March 1990\n|\n|-\n|6.07\n|April 1991\n|\n|-\n|6.08\n|March 1993\n|\n|-\n|6.09\n|October 1993\n|\n|-\n|6.10\n|October 1994\n|\n|-\n|6.11\n|October 1995\n|\n|-\n|6.12\n|November 1996\n|\n|-\n|7.0\n|October 1998\n|\n|-\n|8.0\n|November 1999\n|\n|-\n|8.1\n|July 2000\n|\n|-\n|8.2\n|March 2001\n|\n|-\n|9.0\n|October 2002\n|\n|-\n|9.1\n|December 2003\n|\n|-\n|9.1.3\n|August 2004\n|\n|-\n|9.2\n|March 2008\n|STAT 9.2\n|-\n|9.2m2\n|April 2010\n|STAT 9.22\n|-\n|9.3\n|July 2011\n|STAT 9.3\n|-\n|9.3m2\n|August 2012\n|STAT 12.1\n|-\n|9.4\n|July 2013\n|STAT 12.3\n|-\n|9.4m1\n|December 2013\n|STAT 13.1\n|-\n|9.4m2\n|August 2014\n|STAT 13.2\n|-\n|9.4m3\n|July 2015\n|STAT 14.1\n|-\n|9.4m4\n|November 2016\n|STAT 14.2\n|-\n|9.4m5\n|September 2017\n|STAT 14.3\n|-\n|9.4m6\n|November 2018\n|STAT 15.1\n|}\n\n==Software products==\nAs of 2011 SAS's largest set of products is its line for [[customer intelligence]]. Numerous SAS modules for web, social media and marketing analytics may be used to profile customers and prospects, predict their behaviors and manage and optimize communications.<ref name=\"thirtyeight\" /><ref>{{cite news|date=May 11, 2004|newspaper=InformationWeek|first=Rick|last=Whiting|url= http://www.informationweek.com/sas-ships-customer-intelligence-app/d/d-id/1025026?|title=SAS Ships Customer-Intelligence App|accessdate=January 7, 2014}}</ref> SAS also provides the SAS Fraud Framework. The framework's primary functionality is to monitor transactions across different applications, networks and partners and use analytics to identify anomalies that are indicative of fraud.<ref>{{cite news|title=Social network analysis, predictive coding enlisted to fight fraud|newspaper=Government Computer News|date=May 10, 2013|accessdate=December 11, 2013|url=http://gcn.com/Articles/2013/05/10/Social-media-analysis-predictive-coding-enlisted-to-fight-fraud.aspx?Page=3|first=Rutrell|last=Yasin}}</ref><ref>{{cite news|url=http://www.oregonlive.com/finance/index.ssf/2013/08/what_to_do_with_that_false_pos.html|newspaper=The Oregonian|title=Credit card blocked? Get used to false positive fraud alerts for now|first=Brent|last=Hunsberger|date=August 23, 2013|accessdate=December 11, 2013}}</ref><ref>{{cite news|title=IRS, States Call on IBM, LexisNexis, SAS to Fight Tax Fraud|first=Reed|last=Albergotti|date=July 22, 2013|url= https://online.wsj.com/news/articles/SB10001424127887324144304578619811891715262|newspaper=The Wall Street Journal}}</ref><ref>{{cite news|url=http://www.insurancenetworking.com/news/business_intelligence_analytics_cloud_computing_fraud_insurance_technology-12069-1.html|newspaper=Insurance Networking News|date=March 25, 2009|first=Bill|last=Kenealy|accessdate=December 11, 2013|title=Assessing Business Intelligence}}</ref> SAS Enterprise GRC (Governance, Risk and Compliance) provides risk modeling, scenario analysis and other functions<ref>{{cite news|title=SAS Upgrades GRC Software Quicker Compliance|url=http://www.information-management.com/news/SAS-GRC-compliance-software-Forrester-McClean-10022461-1.html|publisher=Information Management|date=May 10, 2012|first=Justin|last=Stephani}}</ref><ref>{{cite web|url=http://www.protiviti.com/en-US/Documents/About-Us/The-Forrester-Wave-Enterprise-Governance-Risk-and-Compliance-Platforms-Q4-2011.pdf|date=November 30, 2011|title=The Forrester Wave‚Ñ¢: Enterprise, Governance, Risk, And Compliance Platforms, Q4 2011|first= Chris|last=McClean|accessdate=December 12, 2013}}</ref> in order to manage and visualize risk, compliance and corporate policies.<ref>{{citation|url=http://www.gleanster.com/#/solution/sas-for-governance-risk-and-compliance|title=Solution Overview|publisher=Gleanster|accessdate=December 12, 2013}}</ref> There is also a SAS Enterprise Risk Management product-set designed primarily for banks and financial services organizations.<ref name=\"Editor\">{{cite book|author=Hui Pan, Editor|title=Iraq Telecom Monthly Newsletter November 2009|url=https://books.google.com/books?id=vTDCtLEo5UcC&pg=PA5|publisher=Information Gatekeepers Inc|pages=5‚Äì|id=GGKEY:ZPZ8BD13DR8}}</ref>\n\nSAS' products for monitoring and managing the operations of IT systems are collectively referred to as SAS IT Management Solutions.<ref>{{cite news|title=SAS Launches Suite Of Solutions For IT|publisher=ITManagementNews|date=September 22, 2004|accessdate=December 12, 2013|url=http://archive.itmanagementnews.com/itmanagementnews-54-20040922SASLaunchesSuiteofSolutionsforIT.html}}</ref> SAS collects data from various IT assets on performance and utilization, then creates reports and analyses.<ref name=\"CokinsSchubert2010\">{{cite book|author1=Gary Cokins|author2=Karl D. Schubert|author3=Michael H. Hugos |author4=Randy Betancourt |author5=Alyssa Farrell |author6=Bill Flemming |author7=Jonathan Hujsak|title=CIO Best Practices: Enabling Strategic Value With Information Technology|url=https://books.google.com/books?id=vjqSNRhEnQcC&pg=PA64|date=24 September 2010|publisher=John Wiley & Sons|isbn=978-0-470-91255-3|pages=64‚Äì}}</ref> SAS' Performance Management products consolidate and provide graphical displays for [[Performance indicator|key performance indicators]] (KPIs) at the employee, department and organizational level.<ref>{{citation|url=http://www.nesug.org/proceedings/nesug02/et/et004.pdf|title=Using SAS Strategically: A Case Study|first=Timothy|last=Brown|publisher=NESUG|accessdate=December 12, 2013}}</ref><ref>{{cite news|title=Product Review SAS SPM Strategic Performance Management|url=http://www.fsn.co.uk/channel_bi_bpm_cpm/pr_sas_spm_strategic_performance_management|date=October 8, 2006|accessdate=December 12, 2013|publisher=FSN}}</ref> The SAS Supply Chain Intelligence product suite is offered for supply chain needs, such as forecasting product demand, managing distribution and inventory and optimizing pricing.<ref>{{cite news|title=SAS to Extend Its Supply Chain Offerings|first=Dennis|last=Callaghan|url=http://www.eweek.com/c/a/Channel/SAS-to-Extend-Its-Supply-Chain-Offerings/#sthash.DcJjCKou.dpuf|newspaper=eWeek|accessdate=December 12, 2013}}</ref> There is also a \"SAS for Sustainability Management\" set of software to forecast environmental, social and economic effects and identify causal relationships between operations and an impact on the environment or ecosystem.<ref>{{cite news|title=Software for sustainability management unveiled|date=May 5, 2008|pp=41|first=Kang|last=Li|newspaper=New Straits Times}}</ref>\n\nSAS has product sets for specific industries, such as government, retail, telecommunications and aerospace and for marketing optimization or [[high-performance computing]].<ref>{{cite web|url=https://www.sas.com/products/|title=Products & Solutions Index|publisher=SAS|accessdate=December 12, 2013}}</ref>\n\n===Free University Edition===\nSAS also offers Free University Edition which can be downloaded by anyone and used for non commercial use. The first announcement regarding this Free University Edition seems to have appeared in newspapers on 28 May 2014.<ref>{{Cite web|url=https://www.businesswire.com/news/home/20140528005904/en/Free-SAS%C2%AE-Software-Higher-Education-Adult-Learners|title=Free SAS¬Æ Software for Higher Education, Adult Learners Now Available|date=2014-05-28|website=www.businesswire.com|language=en|access-date=2019-01-06}}</ref> Hence it can be said that the University Edition is available freely since then.<ref>{{Cite web|url=https://www.techwire.net/sponsored/get-started-sas-university-edition.html|title=Get Started with SAS University Edition|date=2015-01-20|website=Techwire.net|language=en|access-date=2019-01-06}}</ref><ref>{{Cite web|url=https://www.businessnewsdaily.com/10716-sas-certification-guide.html|title=SAS Certification Guide: Overview and Career Paths|website=www.businessnewsdaily.com|access-date=2019-01-06}}</ref><ref>{{Cite web|url=https://insidebigdata.com/2017/03/18/sas-enables-visually-impaired-visualize-data/|title=SAS Enables Visually Impaired to ‚ÄòVisualize‚Äô Data|last=Team|first=Editorial|date=2017-03-18|website=insideBIGDATA|language=en-US|access-date=2019-01-06}}</ref>\n\n===Comparison to other products===\n{{See also|Comparison of statistical packages}}\nIn a 2005 article for the ''[[Journal of Marriage and Family]]'' comparing statistical packages from SAS and its competitors [[Stata]] and [[SPSS]], Alan C. Acock wrote that SAS programs provide \"extraordinary range of data analysis and data management tasks,\" but were [[Usability|difficult to use]] and learn.<ref name=\"Acock\"/> SPSS and Stata, meanwhile, were both easier to learn (with better documentation) but had less capable analytic abilities, though these could be expanded with paid (in SPSS) or free (in Stata) add-ons. Acock concluded that SAS was best for [[power user]]s, while occasional users would benefit most from SPSS and Stata.<ref name=\"Acock\">{{cite journal|pages=1093‚Äì1095|title=SAS, Stata, SPSS: A Comparison |journal=Journal of Marriage and Family |doi=10.1111/j.1741-3737.2005.00196.x|volume=67|issue=4 |date=November 2005 | last=Acock |first=Alan C}}</ref> A comparison by the [[University of California, Los Angeles]], gave similar results.<ref>{{cite web|url=http://www.ats.ucla.edu/stat/mult_pkg/compare_packages.htm |title=Compare Packages |publisher=University of California, Los Angeles |accessdate=12 January 2014}}</ref>\n\nCompetitors such as [[Revolution Analytics]] and [[Alpine Data Labs]] advertise their products as considerably cheaper than SAS'. In a 2011 comparison, Doug Henschen of ''[[InformationWeek]]'' found that start-up fees for the three are similar, though he admitted that the starting fees were not necessarily the best basis for comparison.<ref>{{cite web| url=http://www.informationweek.com/software/information-management/low-cost-options-for-predictive-analytics-challenge-sas-ibm/d/d-id/1099191?page_number=1 |title=Low-Cost Options For Predictive Analytics Challenge SAS, IBM|first=Dough |last=Henschen |work=InformationWeek|date=July 26, 2011 |accessdate=January 12, 2014}}</ref> SAS' business model is not weighted as heavily on initial fees for its programs, instead focusing on revenue from annual subscription fees.<ref>{{cite news|title=SAS' revenue up 12% in 2011|work=The News & Observer |location= Raleigh, North Carolina |date=January 20, 2012 |url=http://www.newsobserver.com/2012/01/20/1790587/sas-revenue-up-12-in-2011.html|last=Ranii|first=David |accessdate=January 12, 2014}}</ref><ref name=\"SoftBus\">{{cite web|last=Turchin|first=Brian|title=SAS Profile -- Going Its Own Way|url=http://www.cabnr.unr.edu/gf/apst650/sassoftbusiness.pdf|publisher=Software Business Online}}</ref>\n\n==Adoption==\nAccording to IDC, SAS is the largest market-share holder in \"advanced analytics\" with 35.4 percent of the market as of 2013.<ref>{{cite web|url=http://www.clickz.com/clickz/column/2353618/just-how-big-is-the-big-data-market |title=Just How Big Is the Big Data Market? |publisher=ClickZ |accessdate=7 July 2014|date=2014-07-07 }}</ref> It is the fifth largest market-share holder for [[business intelligence|business intelligence (BI)]] software with a 6.9% share<ref>\n{{cite web\n| title=Worldwide Business Analytics Software 2013‚Äì2017 Forecast and 2012 Vendor Shares\n| url=http://idcdocserv.com/241689e_sas\n| first=Dan\n| last=Vesset |author2=David Schubmehl |author3=Brian McDonough |author4=Mary Wardley\n| publisher=IDC\n| date=June 2013\n| accessdate=October 2, 2013}}</ref> and the largest independent vendor. It competes in the BI market against conglomerates, such as [[SAP BusinessObjects]], [[IBM Cognos]], [[SPSS Modeler]], [[Oracle Hyperion]], and [[Power BI|Microsoft BI]].<ref>\n{{cite web\n| url=http://www.bi-verdict.com/fileadmin/FreeAnalyses/consolidations.htm\n| title=Consolidations in the BI industry\n| date=March 7, 2008\n| last=Pendse\n| first=Nigel\n| work=The OLAP Report}}</ref> SAS has been named in the Gartner Leader's Quadrant for Data Integration Tool and for Business Intelligence and Analytical Platforms.<ref>\n{{cite web\n| title=Magic Quadrant for Business Intelligence and Analytics Platforms\n| date=February 5, 2013\n| publisher=Gartner\n| url=http://www.gartner.com/technology/reprints.do?id=1-1DZLPEP&ct=130207&st=sb\n| accessdate=October 1, 2013\n| first=Kurt\n| last=Schlegel |author2=Rita Sallam |author3=Daniel Yuen |author4=Joao Tapadinhas}}</ref>\nA study published in 2011 in ''[[BMC Health Services Research]]'' found that SAS was used in 42.6 percent of data analyses in health service research, based on a sample of 1,139 articles drawn from three journals.<ref>{{Cite journal | last1 = Dembe | first1 = A. E. | last2 = Partridge | first2 = J. S. | last3 = Geist | first3 = L. C. | title = Statistical software applications used in health services research: Analysis of published studies in the U.S | doi = 10.1186/1472-6963-11-252 | journal = BMC Health Services Research | volume = 11 | pages = 252 | year = 2011 | pmid =  21977990| pmc =3205033 }}</ref>\n\n== See also ==\n* [[Comparison of numerical analysis software]]\n*[http://sas-or.com/ SAS for Operations Research]<ref>{{Cite book|title=Applied operational research with SAS|last=Ali.|first=Emrouznejad|date=2011|publisher=Chapman & Hall/CRC|others=Ho, William.|isbn=9781439841310|location=Boca Raton|oclc=778432733}}</ref>\n* [[Comparison of OLAP Servers]]\n* [[JMP (statistical software)]], also from [[SAS Institute Inc.]]\n* [[SAS language]]\n\n== References ==\n{{Reflist}}\n\n==Further reading==\n* {{Cite journal\n| last1=Greenberg\n| first1=Bernard G.\n| last2=Cox\n| first2=Gertrude M.\n| authorlink2=Gertrude Mary Cox\n| last3=Mason\n| first3=David D.\n| last4=Grizzle\n| first4=James E.\n| last5=Johnson\n| first5=Norman L.\n| last6=Jones\n| first6=Lyle V.\n| last7=Monroe\n| first7=John\n| last8=Simmons\n| first8=Gordon D., Jr.\n| editor=Nourse, E. Shepley\n| journal=International Statistical Review\n| title=Statistical Training and Research: The University of North Carolina System\n|volume=46\n| pages=171‚Äì207\n| year=1978\n| jstor=1402812\n| issue=2\n| doi=10.2307/1402812\n| postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}} }}\n\n* [http://www.sas.com SAS homepage]\n* [https://books.google.com/books?id=a0Fc9dJby7EC&pg=PA321&dq=sas+%22library+engine%22+%22remote+library+services%22&hl=en&sa=X&ei=pmZNUsnbI6SHygHt4oHYAQ&ved=0CEsQ6AEwAQ#v=onepage&q=Glossary&f=false A Glossary of SAS terminology]\n* [http://www.sascommunity.org/wiki/Main_Page The SAS customer community Wiki]\n* [[Wikiversity:Data Analysis using the SAS Language]]\n{{Statistical software}}\n{{Good article}}\n\n[[Category:4GL]]\n[[Category:Articles with example code]]\n[[Category:Business intelligence]]\n[[Category:C software]]\n[[Category:Data mining and machine learning software]]\n[[Category:Data warehousing]]\n[[Category:Extract, transform, load tools]]\n[[Category:Mathematical optimization software]]\n[[Category:Numerical software]]\n[[Category:Proprietary commercial software for Linux]]\n[[Category:Proprietary cross-platform software]]\n[[Category:Science software for Linux]]"
    },
    {
      "title": "SCIP (optimization software)",
      "url": "https://en.wikipedia.org/wiki/SCIP_%28optimization_software%29",
      "text": "{{Notability|Products|date=November 2013}}\n__NOTOC__\n{{Infobox Software\n| name                   = SCIP\n| latest_release_version = 6.0.0\n| latest_release_date = {{release date and age|2018|07|02}}\n| programming language   = [[C (programming language)|C]]\n| operating system       = [[Cross-platform]]\n| genre                  = [[Optimization (mathematics)|Mathematical optimization]]\n| status                 = Active\n| website                = {{URL|scip.zib.de}}\n|license=ZIB Academic License}}\n\n'''SCIP''' ('''Solving Constraint Integer Programs''') is a [[mixed integer programming]] solver and a framework for [[Branch and cut]] and [[Branch and price]], developed primarily at [[Zuse Institute Berlin]]. Unlike most commercial solvers, SCIP gives the user low-level control of and information about the solving process. Run as a standalone solver, it is one of the fastest non-commercial solvers for mixed integer programs.<ref>[http://plato.asu.edu/ftp/milpc.html Mixed Integer Linear Programming Benchmark] Mar 18, 2012.</ref>\n\nSCIP is implemented as [[C (programming language)|C]] [[Library (computing)|callable library]].\nFor user [[Plug-in (computing)|plugins]], [[C++]] wrapper classes are provided.\nThe solver for the LP relaxations is not a native component of SCIP, an open LP interface is provided instead.\nCurrently supported LP solvers are [[COIN-OR#CLP|CLP]], [[CPLEX]], [[Gurobi]], [[MOSEK]], [[QSopt]], SoPlex, and [[FICO Xpress | Xpress]].\nSCIP can be run on [[Linux]], [[Mac OS|Mac]], [[SunOS|Sun]], and [[Windows]] operating systems.\n\n==Features==\nThe design of SCIP is based on the notion of [[Constraint programming|constraints]]. It supports about 20 constraint types for mixed-integer linear programming, mixed-integer nonlinear programming, mixed-integer all-quadratic programming and Pseudo-Boolean <ref>[http://www.cril.univ-artois.fr/PB09/results/ranking.php?idev=28 Pseudo-Boolean challenge 2009] Feb 11, 2011.</ref> optimization. It can also solve [[Steiner Trees]] and [[multi-objective optimization]] problems.<ref>[https://opus4.kobv.de/opus4-zib/files/5781/MasterThesis.pdf A Generic Approach to Solving the Steiner Tree Problem and Variants] Nov 9, 2015.</ref><ref>[http://polyscip.zib.de/ PolySCIP] Aug 15, 2016.</ref>\n\n==Interfaces==\nThere are several native interface libraries available for SCIP. SCIP can be accessed through the modeling system of [[General Algebraic Modeling System|GAMS]]. Interfaces to [[MATLAB]] and [[AMPL]] are available within the standard distribution. There are also currently two externalized interfaces for [[Python (programming language)|Python]] and [[Java (programming language)|Java]].<ref>[https://github.com/SCIP-Interfaces/ SCIP-Interfaces] Aug 15, 2016.</ref>\n\n== References ==\n{{Reflist}}\n\n=== Further reading ===\n\n{{refbegin}}\n*{{Citation\n| last1 = Achterberg\n| first1 = Tobias\n| year = 2007\n| title = Constraint Integer Programming\n| url = http://opus4.kobv.de/opus4-tuberlin/frontdoor/index/index/docId/1541\n| isbn = 978-3-89963-892-9}}.\n{{refend}}\n\n== External links ==\n* {{Official website|scip.zib.de}}\n\n{{Mathematical optimization software}}\n\n[[Category:Mathematical optimization software]]\n[[Category:Numerical software]]"
    },
    {
      "title": "SmartDO",
      "url": "https://en.wikipedia.org/wiki/SmartDO",
      "text": "{{inline|date=July 2016}}\n{{notability|Products|date=November 2015}}\n{{Infobox Software\n| name                   = SmartDO \n| logo                   = SmartDO logo.gif\n| developer              = FEA-Opt Technology\n| released               = {{Start date and age|2006}}\n| latest_release_version = 5.0.4\n| latest_release_date    = {{Start date and age|2013|06}}\n| operating_system       = MS Windows\n| genre                  = [[List of numerical analysis software|Technical computing]]\n| status                 = Active\n| license                = [[Proprietary software|Proprietary]]\n| website                = {{URL|www.Smartdo.co}}\n}}\n'''SmartDO'''  is a [[multidisciplinary design optimization]] software, based on the '''Direct Global Search''' technology developed and marketed by FEA-Opt Technology. SmartDO specialized in the CAE-Based optimization, such as CAE ([[computer-aided engineering]]), FEA ([[finite element analysis]]), CAD ([[computer-aided design]]), CFD ([[Computational fluid dynamics]]) and [[automatic control]], with application on various physics phenomena. It is both GUI and scripting driven, allowed to be integrated with almost any kind of CAD/CAE and in-house codes.\n\n'''SmartDO''' focuses on the '''direct [[global optimization]]''' solver, which does not need much parametric study and tweaking on the solver parameter. Because of this, '''SmartDO''' has been frequently customized as the push-button expert system.\n\n==History==\nSmartDO was originated in 1995 by its founder (Dr. Shen-Yeh Chen) during his Ph.D. study in Civil Engineering Department of [[Arizona State University]]. During 1998 to 2004, SmartDO was continuously developed and applied on aerospace industry and CAE consulting application as an in-house code. In 2005, Dr. Chen established FEA-Opt Technology as a CAE consulting firm and software vendor.  The first commercialized version 1.0 was published in 2006 by FEA-Opt Technology. In 2012, FEA-Opt Technology signed partner agreement with both [[ANSYS]] and [[MSC Software]] base on SmartDO.\n\n==Process integration==\nSmartDO uses both GUI and scripting-based interface to integrate with the 3rd party software. The GUI includes general operation of SmartDO and package specific linking interface, called the SmartLink. Smartlink can link with 3rd party CAE software, such as [[ANSYS]] Workbench. The user can cross-link any parameters in [[ANSYS]] Workbench to any design parameters in SmartDO, such as design variables, objective function, and constraints, and SmartDO will usually solve the problem well with the default settings.\n\nThe scripting interface in SmartDO is based on [[Tcl]]/[[Tk]] shell. This makes SmartDO able to link with almost any kind of 3rd party software and in-house code. SmartDO comes with the SmartScripting GUI, for generating [[Tcl]]/[[Tk]] script automatically. The user can create script  by answering questions in the SmartScripting GUI, and SmartScripting will generate [[Tcl]]/[[Tk]] scripts for the user. The flexible scripting interface allows SmartDO to be customized as a push-button automatic design system.\n\n==Design optimization==\nSmartDO uses the '''Direct Global Search''' methodology to achieve [[global optimization]], including both Gradient-Based [[Nonlinear programming]] and [[Genetic Algorithm]] based [[stochastic programming]]. These two approaches can also be combined or mixed to solve specific problems.\n\nFor all the solvers in SmartDO, there is no theoretical and/or coding restriction on the number of design variables and/or constraints. SmartDO can start from an infeasible design point, pushing the design into the feasible domain first, and then proceed with optimization.\n\n===Gradient-Based Nonlinear Programming ===\n\n: SmartDO uses the Generalized Reduced Gradient Method and the Method of Feasible Directions as its foundation to solve the constrained [[nonlinear programming]] problem. To achieve global search capability, SmartDO also uses  [[Tunneling protocol|Tunneling]] and [[Hill climbing]] to escape from the local minimum. This also enables SmartDO to eliminate the numerical noise caused by meshing, discretization, and other phenomena during numerical analysis. Other unique technologies include\n\n* Automatic recognition of active constraints. \n* Smart Dynamic Search to automatically adjust search direction and step size.\n\n===Genetic Algorithm===\n\n: The Genetic Algorithm in SmartDO was part of the founder's Ph.D. dissertation, which is called the Robust Genetic Algorithms. It includes some special approaches to achieve stability and efficiency, for example, \n* Adaptive Penalty Function.\n* Automatic Schema Representation.\n* Automatic Population and Generation Number Calculation.\n* Adaptive and Automatic Cross-Over Probability Calculation.\n* Absolute Descent.\n\nBecause there are various types of design variables available in the Robust Genetic Algorithms, the users can perform Concurrent Sizing, Shaping and Topology Optimization with SmartDO.\n\n==Applications==\nSmartDO has been widely applied on the industry design and control since 1995. The disciplines and physics phenomena includes\n* Structure\n* CFD\n* Heat Flow\n* Heat Transfer\n* [[Crashworthiness]]\n* Structural/Thermal/Electronic Coupled\n* Automatic Control\n\nAnd the application includes\n* Life prolonging of semi-conductor component.\n* Keratotomy Surgeries.\n* Civil structure and resident roof optimization (sizing, shaping and topology).\n* Life prolonging and weight reduction for the components of gas turbine engines.\n* Enhancement for the performance of the fluid power system.\n* Weight reduction and strength increase of the nuclear heavy-duty lifting hook.\n* Performance optimization of the shock absorbing mechanism.\n* Weight reduction of the air cargo deck.\n* Performance optimization of the thermoelectric generator.\n* Weight reduction of the lower A-Arm of the armored tank.\n* Performance curve optimization for the keyboard rubber dome.\n* Performance curve optimization for the connectors.\n* Composite structure optimization.\n* Strength optimization of the circulation water pump in power plant.\n* Structural optimization for the wave energy converter.\n* Performance optimization of the jet nozzle.\n* Optimization of the O-Ring Sealing for the steel charger.\n* Performance Enhancement of the Golf Club Head.\n* Crashworthiness Optimization of The Crash Box.\n* Ceramic Gas Turbine Engines Rotor Disk Structural Optimization.\n\n==References==\n{{reflist}}\n;Notes\n{{refbegin}}\n* C-Y Tsai, 2010, \"Improving the O-ring Sealing of 8 gram Charger by Finite Element Analysis and Shape Optimization\", M.S. Thesis, Department of Mechanical Engineering, National Chiao Tung University, Taiwan.\n* H-C Tseng, Z-C. Wu, C Hung, M-H. Lee, C-C. Huang, 2009, \"Investigation of Optimum Process Parameters on the Sheet Hydroforming of Titanium/Aluminum Clad Metal for Battery Housing\", by 4th International Conference on Tube Hydroforming (TUBEHYDRO 2009), September 6‚Äì9, Kaohsiung, Taiwan.\n* S-Y. Chen, 2007, Gradient-Based Structural and CFD Global Shape Optimization with SmartDO and the Response Smoothing Technology, Proceedings of the 7th World Congresses of Structural and Multidisciplinary Optimization (WCSMO7), COEX Seoul, 21‚Äì25 May 2007, Korea\n* S-Y. Chen, J. W.C. Liao and V. Tsai, 2007 ‚ÄúImproving the Reliability and Usability of Structural Shaping Optimization ‚ÄìThe Contour Natural Shape Function‚Äù, Journal of Chinese Institute of Engineers, Vol. 30 (to be published).\n* S-Y. Chen, Nov 2002, \"Integrating ANSYS with Modern Numerical Optimization Techniques - Part I : Conjugate Feasible Direciton Method, 2002 Taiwan Area ANSYS Users Conference.\n* S-Y. Chen, Nov 2002, \"Integrating ANSYS with Modern Numerical Optimization Techniques - Part II : A Reverse Parametric Modeling Approach for Structural Shaping Optimization, 2002 Taiwan Area ANSYS Users Conference.\n* S-Y. Chen, March 2001, \"An Approach for Impact Structure Optimization Using The Robust Genetic Algorithm\", Finite Elements in Analysis and Design, Vol 37, No 5, pp431‚Äì446.\n* S-Y. Chen and S. D. Rajan, October 2000, \"A Robust Genetic Algorithm for Structural Optimization\", Structural Engineering & Mechanics Journal, Vol 10, No 4, pp313‚Äì336.\n* S-Y. Chen, Oct 2000, \"Integrating ANSYS with Modern Numerical Optimization Technologies\", ANSYS Solutions Magazine, Spring Issue, 2003.\n* S-Y. Chen and S. D. Rajan, May 1999, \" Using Genetic Algorithm as An Automatic Structural Design Tool\", Short Paper Proceedings of 3rd World Congress of Structural and Multidisciplinary Optimization, Vol. 1, pp263‚Äì265, Buffalo, NY.\n* B. Mobasher, S-Y.Chen, C. Young and S. D. Rajan, Oct. 1998, \"A Cost Based Approach To Design Of Residential Steel Roof Systems\", 14th International Specialty Conference, Recent Research and Developments in Cold-Formed Steel Design and Construction, University of Missori-Rolla, Edited By Wei-Wen Yu and R. LaBoube, pp613‚Äì625.\n* S-Y. Chen and S.D. Rajan, 1998, \"Improving the Efficiency of Genetic Algorithms for Frame Designs\", Engineering Optimization, Vol. 30, pp281‚Äì307.\n* S-Y. Chen, December 1997, \"Using Genetic Algorithms for the Optimal Design of Structural System\",Dissertation for Doctor of Philosophy, Department of Civil Engineering, Arizona State University.\n{{refend}}\n\n==External links==\n* [http://www.FEA-Optimization.com/index_e.htm FEA-Opt Technology company page].\n* [http://www.SmartDO.co Dedicated web page for SmartDO]\n\n[[Category:Computer system optimization software]]\n[[Category:Computer-aided engineering software]]\n[[Category:Computer-aided design software]]\n[[Category:Mathematical optimization software]]\n[[Category:Simulation software]]"
    },
    {
      "title": "SNOPT",
      "url": "https://en.wikipedia.org/wiki/SNOPT",
      "text": "{{third-party|date=May 2019}}\n{{Infobox software\n| name                   = SNOPT\n| developer             = Philip Gill<br> [[Michael Saunders (academic)|Michael Saunders]]<br> Walter Murray\n| latest_release_version = 7.6.0\n| operating_system       = [[Cross-platform]]\n| status                 = Active\n| programming language   = [[Fortran]]\n| license                = [[Proprietary software|Proprietary]]\n| website                = {{URL|ccom.ucsd.edu/~optimizers}}\n}}\n\n'''SNOPT''', for '''Sparse Nonlinear OPTimizer''', is a software package for solving large-scale [[nonlinear optimization]] problems written by Philip Gill, Walter Murray and [[Michael Saunders (academic)|Michael Saunders]].  SNOPT is mainly written in [[Fortran]], but interfaces to [[C (programming language)|C]], [[C++]], [[Python (programming language)|Python]] and [[MATLAB]] are available.\n\nIt employs a sparse [[sequential quadratic programming]] (SQP) algorithm with limited-memory quasi-Newton approximations to the Hessian of the Lagrangian.  It is especially effective for nonlinear problems with functions and gradients that are expensive to evaluate. The functions should be smooth but need not be convex.\n\nSNOPT is used in several trajectory optimization software packages, including Copernicus, AeroSpace Trajectory Optimization and Software ([[ASTOS]]), [[General Mission Analysis Tool]], and Optimal Trajectories by Implicit Simulation (OTIS).\n\nSNOPT is supported in the [[AIMMS]], [[AMPL]], [[APMonitor]], [[General Algebraic Modeling System]] (GAMS),and [[TOMLAB]] modeling systems.\n\n==References==\n* {{cite journal |author1=P.E. Gill |author2=W. Murray |author3=M.A. Saunders | title = SNOPT: An SQP algorithm for large-scale constrained optimization | date = 2005 | url = http://ccom.ucsd.edu/~optimizers/papers/siam_44609.pdf | format = PDF }}\n\n== External links ==\n* {{Official website|http://www.sbsi-sol-optimize.com/asp/sol_product_snopt.htm}}\n* [http://www.sbsi-sol-optimize.com/manuals/SNOPT%20Manual.pdf SNOPT manual (.pdf)]\n\n{{Mathematical optimization software}}\n\n[[Category:Numerical software]]\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "Sol (format)",
      "url": "https://en.wikipedia.org/wiki/Sol_%28format%29",
      "text": "{{Other uses|.sol (disambiguation){{!}}.sol}}\n{{Infobox file format\n| icon =\n| logo =\n| screenshot =\n| extension = .sol\n| mime = \n| type code =\n| uniform type = \n| magic =\n| owner = [[Robert Fourer]]<br/> David Gay<br/> [[Brian Kernighan]]<br/>[[Bell Labs]]\n| genre = [[mathematical programming]]\n| container for =\n| contained by =\n| extended from = \n| extended to = \n| standard x= \n| free = \n}}\n\n{{lowercase|nl}}\n'''sol''' is a file format for representing solutions of [[mathematical programming]] problems.<ref>\n{{cite techreport\n | author=David Gay\n | title=Writing .nl Files\n | institution=[[Sandia National Laboratories]]\n | year=2005\n | location=Albuquerque, NM\n | citeseerx = 10.1.1.60.9659\n }}</ref>  It is often used in conjunction with the [[nl (format)|nl format]] to return solutions from the solvers. Initially this format has been invented for connecting solvers to [[AMPL]]<ref>\n{{cite techreport\n | author=David Gay\n | title=Hooking Your Solver to AMPL\n | institution=[[Bell Laboratories]]\n | year=1993\n | number=97-4-06\n | location=Murray Hill, NJ\n | url = http://www.ampl.com/REFS/hooking2.pdf\n }}</ref> but then it has been adopted by other systems such as [[FortSP]] for interacting with external solvers.\n\nThe sol format is low-level and is designed for compactness not for readability. It has both binary and textual representation.\nMany solvers such as [[CPLEX]], [[Gurobi]] and [[MOSEK]] can produce files in this format either directly or through special driver programs.\n\nThe AMPL Solver Library (ASL) which allows among other things to read and write the sol files is open-source. It is used in many solvers to implement AMPL connection.\n\n==See also==\n* [[nl (format)]] ‚Äì a file format for presenting mathematical programming problems\n\n==References==\n{{Reflist}}\n\n\n{{Mathematical optimization software}}\n{{DEFAULTSORT:Sol (Format)}}\n[[Category:Mathematical optimization software]]\n[[Category:Computer file formats]]"
    },
    {
      "title": "SolverStudio",
      "url": "https://en.wikipedia.org/wiki/SolverStudio",
      "text": "{{Use dmy dates|date=June 2016}}\n{{Use New Zealand English|date=June 2016}}\n\n'''SolverStudio''' is a free [[Microsoft Excel|Excel]] plug-in developed at the University of Auckland<ref name=\"solverstudio-article\">\n{{cite news\n| title = SolverStudio: A New Tool for Better Optimisation and Simulation Modelling in Excel\n| first = Andrew\n| last = Mason\n| journal = INFORMS Transactions on Education\n| year = 2013\n| volume = 14\n| number = 1\n| pages = 45‚Äì52}}\n</ref>  that supports [[optimization]] and [[simulation]] modelling in a [[spreadsheet]] using an [[algebraic modeling language]]. It is popular in education,<ref>{{Cite web|url=http://coral.ie.lehigh.edu/~ted/files/coin-or/slides/COINModeling.pdf#page=10|title=The COIN-OR Optimization Suite: Open Source Tools for Optimization. Part 4: Modeling with COIN|last=Ralphs|first=Ted|date=10 Jan 2015|website=Computational Optimization Research at Lehigh|publisher=Lehigh University|access-date=9 April 2016}}</ref> the public sector<ref>{{Cite web|url=http://ifors.org/developing_countries/index.php?title=SolverStudio|title=SolverStudio|last=|first=|date=|website=|publisher=International Federation of Operational Research Societies|access-date=9 April 2016}}</ref> and industry for optimization users because it uses industry-standard modelling languages and is faster than traditional Excel optimisation approaches.<ref>{{cite web|title=SolverStudio+GAMS speedup: 2 hours to 2 minutes|url=http://solverstudio.org/solverstudiogams-speedup-2-hours-to-2-minutes/|website=SolverStudio User Feedback|accessdate=9 April 2016}}</ref>\n\nSolverStudio adds a text editor to Excel that is used to create a text-based optimization (or simulation) model using a modelling language such as [[COIN-OR#PuLP|PuLP]], [[AMPL]], [[General Algebraic Modeling System|GAMS]] or [[Julia (programming language)|Julia]]/JuMP. SolverStudio also provides a tool for naming data on a spreadsheet (and specifying indices for this data), allowing the data to be used in the model. When the model is run, the system automatically reads input data from the spreadsheet and provides it to the model, and then writes the model results back to the spreadsheet.\n\nSolverStudio works with a range of commercial and open source modelling systems. By default, it uses [[COIN-OR#PuLP|PuLP]], an open-source [[Python (programming language)|Python]] [[COIN-OR]] modelling language. A second open-source [[Python (programming language)|Python]] option is [[Pyomo]] which supports non-linear and stochastic programming and provides access to a larger range of solvers. Another supported linear and non-linear modelling option is [[Julia (programming language)|Julia]]/JuMP.\n\nSolverStudio also makes the two popular commercial modelling languages, [[AMPL]] and [[General Algebraic Modeling System|GAMS]] available to Excel users. SolverStudio allows models written using these languages to be solved on the user's own PC, or in the cloud using  NEOS.<ref name=\"neos\">{{cite web|url=http://www.neos-server.org/neos/|title=NEOS Server for Optimization|publisher=|accessdate=11 August 2015}}</ref>\n\nThe GNU clone of AMPL, GMPL ([[GNU Project|GNU]] MathProg Language) is included with SolverStudio. The commercial [[Gurobi]] optimizer can also be used via its [[Python (programming language)|Python]] interface.\n\nSolverStudio includes the open-source [[COIN-OR]] CMPL modelling language, and the [[Python (programming language)|Python]]-based SimPy  [[simulation]] language. SolverStudio supports general programming using both [[Python (programming language)|Python]] and [[IronPython]], allowing these programming languages to be used to script Excel using the standard [[Visual Basic for Applications|VBA]] interfaces.\n\n==References==\n<references/>\n\n== External links ==\n* {{Official website|solverstudio.org}} SolverStudio web site\n* {{Official website|coin-or.org}} COIN-OR, Computational Infrastructure for Operations Research\n\n[[Category:Mathematical optimization software]]\n[[Category:University of Auckland]]\n[[Category:Microsoft Office-related software]]"
    },
    {
      "title": "TOMLAB",
      "url": "https://en.wikipedia.org/wiki/TOMLAB",
      "text": "{{Use dmy dates|date=December 2013}}\n{{Infobox Software\n| name                   = TOMLAB\n| programming_language   = [[MATLAB]], [[C (programming language)|C]], [[Fortran]]\n| developer              = Tomlab Optimization Inc.\n| latest_release_version = 7.9\n| latest_release_date    = 23 August 2012\n| size                   = 89 MB ([[Microsoft Windows|Windows 32-bit]])\n| operating_system       = [[Microsoft Windows|Windows 32/64-bit]], [[Linux|Linux 32/64-bit]] and [[Mac OS X|Mac OS X (Intel)]]\n| genre                  = [[List of numerical analysis software|Technical computing]]\n| status                 = Active\n| license                = [[Proprietary software|Proprietary]]\n| website                = [http://tomopt.com/tomlab/ TOMLAB product page]\n}}\n\nThe '''TOMLAB'''<ref>{{cite book | last = Holmstr√∂m | first = Kenneth |author2=Quttineh, Nils-Hassan |author3=Edvall, Marcus M. | title = An adaptive radial basis algorithm {(ARBF)} for expensive black-box mixed-integer constrained global optimization | date = 2008-02-07 | publisher = Journal of Optimization and Engineering | doi = 10.1007/s11081-008-9037-3 |issn=1389-4420 }}</ref><ref>{{cite book | last = Kallrath | first = Josef |author2=Holmstr√∂m, Kenneth |author3=Edvall, Marcus M. | title = Modeling Languages in Mathematical Optimization (Applied Optimization)  | date = 2004-02-29 | publisher = Springer | isbn = 1-4020-7547-2 }}</ref><ref>{{cite journal | last = Holmstr√∂m | first = Kenneth |author2=Edvall, Marcus M. |author3=G√∂ran Anders O. | title = TOMLAB - for Large-Scale Robust Optimization | date = 2003-10-21 | publisher = Nordic MATLAB Conference 2003 | url = http://tomopt.com/docs/NordicMATLAB_TOMLAB.pdf | format = [[PDF]] }}</ref> [[mathematical optimization|Optimization]] Environment is a modeling platform for solving applied optimization problems in [[MATLAB]].\n\n==Description==\nTOMLAB is a general purpose development and modeling environment<ref>[http://tomopt.com/tomlab/about/ \"TOMLAB OPTIMIZATION\", ''TOMOPT Home Page''] Juli, 2014.</ref> in MATLAB for research, teaching and practical solution of optimization problems. It enables a wider range of problems to be solved in MATLAB and provides many additional solvers.\n\n==Optimization problems supported==\n* TOMLAB handles a wide range of problem types, among them:\n** [[Linear programming]]\n** [[Quadratic programming]]\n** [[Nonlinear programming]]\n** [[Integer_programming#Integer_unknowns|Mixed-integer programming]]\n** Mixed-integer quadratic programming with or without [[Convex function|convex]] quadratic constraints\n** Mixed-integer nonlinear programming\n** Linear and nonlinear [[least squares]] with [[Taxicab geometry|L1]], [[Lp space|L2]] and [[infinity norm]]\n** [[Curve fitting|Exponential data fitting]]\n** [[Global optimization]]\n** [[Semidefinite programming|Semi-definite programming]] problem with [[Bilinear form|bilinear]] matrix inequalities\n** Constrained [[Goal programming|goal attainment]]\n** [[Geometric programming]]\n** [[Genetic programming]]\n** Costly or expensive [[Black box|black-box]] global optimization<ref>{{cite book | last = Holmstr√∂m | first = Kenneth | title = An adaptive radial basis algorithm {(ARBF)} for expensive black-box global optimization  | date = 2007-11-07 | publisher = Journal of Global Optimization (JOGO) | doi = 10.1007/s10898-007-9256-8 |issn=0925-5001 }}</ref>\n** [[Mixed complementarity problem|Nonlinear complementarity problems]]\n\n==Additional features==\n* TOMLAB supports more areas than general optimization, for example:\n** [[Optimal control]] with [[PROPT]] using Gauss and Chebyshev collocation.<ref>[http://tomdyn.com/ \"PROPT - Matlab Optimal Control Software (DAE, ODE)\", ''PROPT Home Page''] April, 2009.</ref>\n** [[Automatic differentiation]] with MAD<ref>[http://matlabad.com/ \"Matlab Automatic Differentiation (MAD) - matlabAD\", ''MAD Home Page''] June, 2008.</ref>\n** Interface to [[AMPL]]\n\n== Further details ==\nTOMLAB supports solvers like [[Gurobi]], [[CPLEX]], [[SNOPT]], [[KNITRO]] and [[MIDACO]]. Each such solver can be called to solve one single model formulation. The supported solvers are appropriate for many problems, including [[linear programming]], [[integer programming]], and  [[global optimization]].\n\nAn interface to [[AMPL]] makes it possible to formulate the problem in an algebraic format. The [[MATLAB Compiler]] enables the user to build stand-alone solutions. Sister products are available for [[LabVIEW]] and [[Microsoft .NET]].\n\nModeling is mainly facilitated by the [[TomSym]] class.\n\n==References==\n{{reflist}}\n\n== External links ==\n* [http://tomopt.com/tomlab/ TOMLAB]\n* [http://matlabad.com/ MAD] ('''M'''ATLAB '''A'''utomatic '''D'''ifferentiation)\n* [http://tomdyn.com/ PROPT - MATLAB Optimal Control Software]\n\n\n{{Mathematical optimization software}}\n{{DEFAULTSORT:Tomlab}}\n[[Category:Numerical software]]\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "TOMNET",
      "url": "https://en.wikipedia.org/wiki/TOMNET",
      "text": "{{refimprove|date=June 2017}}\n{{notability|date=June 2017}}\n\n{{Infobox Software\n| name                   = TOMNET \n| programming_language   = [[.NET Framework|C++, C# and more]]\n| developer              = Tomlab Optimization Inc.\n| latest_release_version = 1.1\n| latest_release_date    = 20 September 2007\n| size                   = 10 MB ([[Microsoft Windows|Windows]])\n| operating_system       = [[Microsoft Windows|Windows 32-bit]]\n| genre                  = [[List of numerical analysis software|Technical computing]]\n| status                 = Active\n| license                = [[Proprietary software|Proprietary]]\n| website                = [http://tomopt.com/tomnet/ TOMNET product page]\n}}\n\nThe '''TOMNET''' [[mathematical optimization|optimization]] Environment is a platform for solving applied optimization problems in [[Microsoft .NET]]. It makes it possible to use solvers like [[SNOPT]], [[MINOS]] and [[CPLEX]] with one single model formulation. The solvers handle everything from [[linear programming]] and [[integer programming]] to [[global optimization]].\n\n== External links ==\n* [http://tomopt.com/tomnet/] (home page)\n\n[[Category:Numerical software]]\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "TomSym",
      "url": "https://en.wikipedia.org/wiki/TomSym",
      "text": "{{Infobox Software\n| name                   = TomSym \n| developer              = [http://tomsym.com/ Tomlab Optimization Inc.]\n| latest_release_version = 7.8\n| latest_release_date    = {{release date|2011|12|16}}\n| operating_system       = [http://tomopt.com/tomlab/about/ TOMLAB - OS Support]\n| genre                  = [[List of numerical analysis software|Technical computing]]\n| license                = [[Proprietary software|Proprietary]]\n| website                = [http://tomsym.com/ TomSym product page]\n}}\n\nThe '''TomSym'''<ref>{{cite book | title = User's Manual for TOMLAB | first = Per | last = Rutquist |author2=M. M. Edvall  | date = Nov 2008 | url = http://tomopt.com/docs/TOMLAB.pdf | location = 1260 SE Bishop Blvd Ste E, Pullman, WA 99163, USA | publisher = Tomlab Optimization Inc.}}</ref> [[MATLAB]] symbolic modeling engine is a platform for modeling applied optimization and optimal control problems.\n\n== Description ==\n\nTomSym is complete [[computer model|modeling]] environment in Matlab with support for most built-in mathematical [[Operation (mathematics)|operators]] in Matlab. It is a combined [[Mathematical model|modeling]], [[Code generation (compiler)|compilation]] and interface to the [[TOMLAB]] solvers. The [[Matrix (mathematics)|matrix derivative]] of a matrix function is a fourth rank [[tensor]] - that is, a matrix each of whose entries is a matrix. Rather than using four-dimensional matrices to represent this, TomSym continues to work in two dimensions. This makes it possible to take advantage of the very efficient handling of [[Sparse matrix|sparse]] matrices in Matlab, which is not available for higher-dimensional matrices.\n\nTomSym has a variety of functions, among them:\n\n* Ability to transform expressions and generate analytical first and second order [[derivative]]s, including sparsity patterns.\n* Interfaced and compatible with [[Automatic differentiation|MAD]], i.e. MAD can be used when symbolic modeling is not suitable.\n* Numerical differentiation can be used to parts of the model.\n* Functionality for plotting and computing a variety of information for the solution to the problem.\n* Support for if, then, else statements.\n* Ability to analyze [[P-code machine|p-coded]] Matlab files.\n* Automated code simplification for generated models, for example.\n** [[Multiplication]] by 1 or the [[identity matrix]] is eliminated: 1*A = A\n** [[Addition]]/[[subtraction]] of 0 is eliminated: 0+A = A\n** All-same matrices are reduced to [[Scalar (mathematics)|scalars]]: [3;3;3]+x = 3+x\n** Scalars are moved to the left in addition/subtraction: A-y = -y+A\n** Inverse operations cancel: sqrt(x)^2 = x\n\n== Modeling ==\n\nThe TomSym symbolic source transformation makes it possible to define any the set of decision variables (both [[Continuous function|continuous]] and [[integer]]) and any type of constraint as well as scalars and [[Constant (mathematics)|constant]] parameters.\n\n=== Linear programming ===\n\nAn example [[linear programming]] problem would look like this:\n\n<source lang=\"matlab\">\n c = [-7; -5];\n A = [ 1 2\n         4 1 ];\n b_U = [ 6; 12 ];\n x_L = [ 0; 0 ];\n\n toms 2x1 x\n\n solution = ezsolve(c'*x, {A*x<=b_U, x_L<=x});\n</source>\n\n=== Mixed-integer nonlinear programming ===\n\nA MINLP problem is defined just like a linear programming problem. This example also shows how to convert the model into a general TOMLAB problem.\n\n<source lang=\"matlab\">\n Name='minlp1Demo - Kocis/Grossman.';\n\n toms 2x1 x\n toms 3x1 integer y\n\n objective = [2 3 1.5 2 -0.5]*[x;y];\n\n constraints = { ...\n    x(1) >= 0, ...\n    x(2) >= 1e-8, ...\n    x <= 1e8, ...\n    0 <= y <=1, ...\n    [1 0 1 0 0]*[x;y] <= 1.6, ...\n    1.333*x(2) + y(2) <= 3, ...\n    [-1 -1 1]*y <= 0, ...\n    x(1)^2+y(1) == 1.25, ...\n    sqrt(x(2)^3)+1.5*y(2) == 3, ...\n };\n\n guess = struct('x',ones(size(x)),'y',ones(size(y)));\n options = struct;\n options.name = Name;\n Prob = sym2prob('minlp',objective,constraints,guess,options);\n\n Prob.DUNDEE.optPar(20) = 1;\n Result = tomRun('minlpBB',Prob,2);\n</source>\n\n=== Multi-index modeling ===\n\ntomSym makes it possible to build models with two or more variable indices in MATLAB.<ref>[http://tomsym.com/examples/tomsym_airlinehub.html \"Airline Hub Location\", ''TOMSYM Home Page''] April, 2009.</ref> The following example creates a variable 'flow' with four indices. The variable is then used to create a constraint over two of the indices and to sum the multiplication with a two-dimensional matrix.\n\n<source lang=\"matlab\">\n\n% Create the indices used in model\ni = tomArrayIdx('i',1:6);\nj = tomArrayIdx('j',1:6);\nk = tomArrayIdx('k',1:6);\nl = tomArrayIdx('l',1:6);\n\n% Create an integer variable of full length\nflow = tom('flow',6^4,1,'int');\n\n% Convert the variable to a matrix with four indices.\nflow = tomArray(flow,[6,6,6,6]);\n\n% Create a constraint valid for all i and j\ncons = {sum(sum(flow(i,j,k,l),k),l) == 1};\n\n% Create a scalar based on multi-index multiplications\ndistance = tomArray([   0   945   605   4667   4749    4394;...\n    945     0   866   3726   3806    3448;...\n    605   866     0   4471   4541    4152;...\n    4667  3726  4471      0    109     415;...\n    4749  3806  4541    109      0     431;...\n    4394  3448  4152    415    431       0]);\n\nsumtotal = sum(vec((distance(i,k)+distance(l,j)+...\n    distance(k,l)*.8).*flow(i,j,k,l)));\n</source>\n\n=== Automatic and numerical differentiation ===\n\nFor functions that cannot be interpreted by tomSym it is possible to use either [[automatic differentiation]] or numerical differentiation. In the following example a simple problem is solved using the two methods.\n\n<source lang=\"matlab\">\ntoms x1 x2\nalpha = 100;\n\n% USE MAD (AUTOMATIC DIFFERENTIATION) FOR ONE FUNCTION\n%\n% Create a wrapper function. In this case we use sin, but it could be any\n% MAD supported function.\n\ny = wrap(struct('fun','sin','n',1,'sz1',1,'sz2',1,'JFuns','MAD'),x1/x2);\nf = alpha*(x2-x1^2)^2 + (1-x1)^2 + y;\n\n% Setup and solve the problem\nc = -x1^2 - x2;\ncon = {-1000 <= c <= 0\n    -10 <= x1 <= 2\n    -10 <= x2 <= 2};\n\nx0 = {x1 == -1.2\n    x2 == 1};\n\nsolution1 = ezsolve(f,con,x0);\n\n% USE NUMERICAL DIFFERENTIATION FOR ONE FUNCTIONS\n% Create a new wrapper function. In this case we use sin, but it could be\n% any function since we use numerical derivatives.\n\ny = wrap(struct('fun','sin','n',1,'sz1',1,'sz2',1,'JFuns','FDJac'),x1/x2);\nf = alpha*(x2-x1^2)^2 + (1-x1)^2 + y;\n\nsolution2 = ezsolve(f,con,x0);\n</source>\n\n== References ==\n{{reflist}}\n\n== External links ==\n* [http://tomsym.com/ TomSym Home Page].\n\n{{DEFAULTSORT:Tomsym}}\n[[Category:Numerical software]]\n[[Category:Mathematical optimization software]]"
    },
    {
      "title": "TOMVIEW",
      "url": "https://en.wikipedia.org/wiki/TOMVIEW",
      "text": "{{Orphan|date=February 2009}}\n{{refimprove|date=June 2017}}\n\n{{Infobox Software\n| name                   = TOMVIEW\n| programming_language   = [[LabVIEW]] 7 and 8 ([[FORTRAN]], [[C (programming language)|C]])\n| developer              = Tomlab Optimization Inc.\n| latest_release_version = 2.1\n| latest_release_date    = 26 Mars 2007\n| size                   = 27 MB ([[Microsoft Windows|Windows]])\n| operating_system       = [[Microsoft Windows|Windows 32+bit]]\n| genre                  = [[List of numerical analysis software|Technical computing]]\n| status                 = Active\n| license                = [[Proprietary software|Proprietary]]\n| website                = [http://tomopt.com/tomview/ TOMVIEW product page]\n}}\n\nThe '''TOMVIEW'''<ref>{{cite book | title = USER‚ÄôS GUIDE FOR TOMVIEW - LabVIEW - The Base Module | first = Mats | last =  Backlund |author2=M. M. Edvall |author3=B. Holmstr√∂m  | date = November 2006 | url = http://tomopt.com/docs/TOMVIEW.pdf | location = 1260 SE Bishop Blvd Ste E, Pullman, WA 99163, USA | publisher = Tomlab Optimization Inc.}}</ref> [[mathematical optimization|Optimization]] Environment is a platform for solving applied optimization problems in [[LabVIEW]].\n\n==Description==\nTOMVIEW is a general purpose development environment in LabVIEW for research, teaching and practical solution of optimization problems. It enables a wider range of problems to be solved in LabVIEW and provides many proprietary solvers.\n\n==References==\n{{reflist}}\n\n== External links ==\n* [http://tomopt.com/tomview/ TOMVIEW] - Developers and distributors of the software.\n\n[[Category:Mathematical optimization software]]\n[[Category:Visual programming languages]]"
    },
    {
      "title": "WORHP",
      "url": "https://en.wikipedia.org/wiki/WORHP",
      "text": "{{Infobox software\n| name                   = WORHP\n| logo                   = [[File:Worhp banner final path.svg|250px|Logo and claim of WORHP.]]\n| screenshot             = Worhp 933x581.png\n| screenshot size        = 250px\n| caption                =\n| developer              = [http://www.math.uni-bremen.de/zetem/ChristofBueskens Christof B√ºskens], [http://www.unibw.de/lrt1/gerdts/mitarbeiter/prof.-gerdts Matthias Gerdts] et al.\n| released               = {{Start date and age|df=yes|2010|03}}\n| latest release version = 1.13\n| latest release date    = {{Start date and age|2018|11|16|df=yes}}\n| programming language   = [[ANSI C]], [[Fortran#FORTRAN 77|FORTRAN 77]], [[Fortran#Fortran 95|Fortran 95]] and [[Fortran#Fortran 2003|Fortran 2003]]\n| operating system       = [[Unix-like]], [[Windows XP]] and later\n| language               = [[English language|English]]\n| genre                  = [[Numerical software]]\n| license                = [[Proprietary software|Proprietary]], Free of charge for academic users.\n| website                = [http://www.worhp.de worhp.de]\n}}\n\n'''WORHP''' ({{IPAc-en|w|…îÀêr|p}} \"warp\"), also referred to as '''eNLP''' (European [[Nonlinear programming|NLP]] solver) by [[ESA]], is a mathematical software [[Library (computing)|library]] for solving continuous large scale [[nonlinear optimization]] problems numerically. The acronym WORHP is sometimes spelled out as \"'''W'''e '''O'''ptimize '''R'''eally '''H'''uge '''P'''roblems\", its primary intended application. WORHP is a hybrid [[Fortran#Fortran 2003|Fortran]] and [[C (programming language)|C]] implementation and can be used from C/[[C++]] and Fortran programs using different interfaces of varying complexity and flexibility. In addition interfaces for the modelling environments [[MATLAB]], [[CasADi]] and [[AMPL]] exist.<ref name=\"Interfaces\">{{cite web |url=http://worhp.de/content/interfaces |title=WORHP interfaces}}</ref>\n\n== Problem formulation ==\nWORHP is designed to solve problems of the form\n::: <math>\\min_{x \\in \\R^n} f(x)</math>\n:subject to\n::: <math>L \\leq \\begin{pmatrix} x \\\\ g(x) \\end{pmatrix} \\leq U </math>\nwith sufficiently smooth functions <math>f:\\R^n \\to \\R</math> (objective) and <math>g:\\R^n \\to \\R^m</math> (constraints) that may be nonlinear, and need not necessarily be convex. Even problems with large dimensions <math>n</math> and <math>m</math> can be solved efficiently, if the problem is sufficiently sparse.\nCases where objective and constraints cannot be evaluated separately, or where constraints can be evaluated element-wise can be exploited by WORHP to increase the computational efficiency.\n\n=== Derivatives ===\nWORHP requires the first [[derivative]] ([[Gradient]]) of <math>f</math> and of <math>g</math> ([[Jacobian matrix and determinant|Jacobian]]) and second derivatives ([[Hessian matrix]]) of the [[Lagrange function]]; in a modelling environment like AMPL, these are provided by [[automatic differentiation]] methods, but need to be provided by the caller in other environments. First and second derivatives can be approximated by WORHP using [[finite differences]]. To reduce the otherwise prohibitively high number of necessary function evaluations in large scale [[sparse matrix|sparse]] problems, [[graph colouring]] theory is used to group first and second partial derivatives. Second derivatives may also be approximated using variations of the classic [[BFGS method]], including block-diagonal or sparse BFGS matrices.\n\n== Structure ==\nThe NLP level of WORHP is based on [[Sequential quadratic programming|SQP]], while the quadratic subproblems are solved using an [[interior point method]]. This approach was chosen to benefit from the robustness of SQP methods and the reliable runtime complexity of IP methods, since traditional [[active set]] methods may be unsuitable for large-scale problems.\n\n== Development ==\nDevelopment of WORHP started in 2006 with funding from [[German Aerospace Center|DLR]] and was continued under the ''eNLP'' label after 2008 with support by ESA / [[European Space Research and Technology Centre|ESTEC]] together with the Interior-Point solver ipfilter<ref name=\"ipfilter\">\n{{cite web |url=http://www.mat.uc.pt/ipfilter |title=ipfilter ‚Äî An NLP Solver based on a primal-dual interior-point filter algorithm |author1=Luis Vicente |author2=Renata Silva |author3=Michael Ulbrich |author4=Stefan Ulbrich }}\n</ref>\n(whose inclusion in eNLP was discontinued after 2010) to develop a European NLP solver for use in trajectory optimisation, mission analysis and aerospace applications in general.<ref name=\"eNLP talk\">\n{{cite web |url=http://uma.ensta.fr/itn-sadco/?module=conf_aero&action=programme |title=eNLP: application-centric NLP-based optimization in the aerospace market  |location=ITN Sadco First Industrial Workshop |author=Sven Erb |date=2011-03-02}}\n</ref>\n\nThe development of WORHP is led by the [http://www.stw.de/su/1118 Steinbeis-Forschungszentrum Optimierung, Steuerung und Regelung] and scientists of the [http://www.math.uni-bremen.de/zetem/o2c/en Optimization and Optimal Control Group] at the [[University of Bremen]], and at the [[Bundeswehr University of Munich]].<ref name=\"Development Team\">{{cite web |url=https://worhp.de/content/developers |title=Development Team |accessdate=2018-01-09}}</ref>\nThe developers stress that WORHP, despite its academic roots, is intended as industrial-grade tool rather than an academic research platform.<ref name=\"The ESA NLP Solver WORHP\">\n{{cite book |author1=Christof B√ºskens |title = Modeling and Optimization in Space Engineering|volume = 73|pages = 85‚Äì110|author2=Dennis Wassel |doi=10.1007/978-1-4614-4469-5_4 |series = Springer Optimization and its Applications|year = 2012|isbn = 978-1-4614-4468-8}}\n</ref>\n\n== Applications ==\nWORHP has been integrated into trajectory analysis tools such as LOTNAV<ref name=\"LOTNAV\">\n{{cite journal |bibcode=2004ESASP.548..609C |title=Navigation and Guidance for Low-Thrust Trajectories, LOTNAV  |journal=18Th International Symposium on Space Flight Dynamics  |volume=548  |pages=609  |author1=J. L. Cano |author2=M. Bello |author3=J. Rodriguez-Canabal |year=2004}}\n</ref>\nand [[ASTOS]], and is being used at [[European Space Operations Centre|ESOC]] and [[European Space Research and Technology Centre|ESTEC]]. It can be used as optimiser in CasADi (since version 1.5.0beta)<ref name=\"CasADi\">\n{{cite web |url=https://github.com/casadi/casadi/wiki |title=CasADi wiki |accessdate=2013-05-27}}\n</ref>\nand as local optimiser in SVAGO MDO<ref name=\"SVAGO\">{{cite web |url=http://www.aero.polimi.it/~castellini |title=PRESTIGE MDO research, Research Achievements |author=Francesco Castellini |year=2009 |accessdate=2011-03-23}}</ref> tool developed at University of Bremen and [[Politecnico di Milano]] on [[Multidisciplinary design optimization]] through the ESA PRESTIGE program.<ref name=\"PRESTIGE\">\n{{cite web|url=http://www.esa.int/esaMI/Education/SEM3E4WX3RF_0.html |title=Universities selected for PRESTIGE programme |author=ESA education |year=2009 |accessdate=2011-03-23}}\n</ref>\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://www.worhp.de WORHP home page]\n* [https://link.springer.com/chapter/10.1007%2F978-1-4614-4469-5_4 WORHP overview chapter]\n\n{{Mathematical optimization software}}\n\n[[Category:Mathematical optimization software]]\n[[Category:Mathematical software]]\n[[Category:Numerical software]]"
    },
    {
      "title": "Ordinal optimization",
      "url": "https://en.wikipedia.org/wiki/Ordinal_optimization",
      "text": "In [[mathematical optimization]], '''ordinal optimization''' is the maximization of functions taking values in a [[partially ordered set]] (\"poset\").<ref>Dietrich, B. L.; Hoffman, A. J. On greedy algorithms, partially ordered sets, and submodular functions. ''IBM J. Res. Dev.'' 47 (2003), no. 1, 25‚Äì30.  <!-- MR1957350 (2003k:90102) --></ref><ref>Topkis, Donald M. ''Supermodularity and complementarity''. Frontiers of Economic Research. Princeton University Press, Princeton, NJ, 1998. xii+272 pp. {{isbn|0-691-03244-0}} <!-- MR1614637 (99i:90024) --></ref><ref>Singer, Ivan ''Abstract convex analysis''. Canadian Mathematical Society Series of Monographs and Advanced Texts. A Wiley-Interscience Publication. John Wiley & Sons, Inc., New York, 1997. xxii+491 pp. {{isbn|0-471-16015-6}} <!-- MR1461544 --></ref><ref>Bj√∂rner, Anders; Ziegler, G√ºnter M. Introduction to greedoids. ''Matroid applications'', 284‚Äì357, Encyclopedia Math. Appl., 40, Cambridge Univ. Press, Cambridge, 1992,<!-- MR1165545 (94a:05038) --></ref>  Ordinal optimization has applications in the theory of [[queuing theory|queuing]] [[flow network|networks]].\n\n== Mathematical foundations ==\n{{See also|Mathematical optimization|Partially ordered set|Lattice (order)|Greedoid|Antimatroid|Combinatorial optimization|Duality (mathematics)#Order-reversing dualities}}\n\n=== Definitions ===\n\nA '''partial order''' is a [[binary relation]] \"‚â§\" over  a [[Set (mathematics)|set]] ''P'' which is [[reflexive relation|reflexive]], [[antisymmetric relation|antisymmetric]], and [[transitive relation|transitive]], i.e., for all ''a'', ''b'', and ''c'' in ''P'', we have that:\n\n*''a ‚â§ a'' (reflexivity);\n*if ''a ‚â§ b'' and ''b ‚â§ a'' then ''a'' = ''b'' (antisymmetry);\n*if ''a ‚â§ b'' and ''b ‚â§ c'' then ''a ‚â§ c'' (transitivity).\n\nIn other words, a partial order is an antisymmetric [[preorder]].\n\nA set with a partial order is called a '''partially ordered set''' (also called a '''poset'''). The term ''ordered set'' is sometimes also used for posets, as long as it is clear from the context that no other kinds of orders are meant. In particular, totally ordered sets can also be referred to as \"ordered sets\", especially in areas where these structures are more common than posets.\n\nFor ''a, b'' distinct elements of a partially ordered set ''P'', if ''a ‚â§ b'' or ''b ‚â§ a'', then ''a'' and ''b'' are '''comparable'''. Otherwise they are '''incomparable'''. If every two elements of a poset are comparable, the poset is called a [[totally ordered set]] or '''chain''' (e.g. the natural numbers under order). A poset in which every two elements are incomparable is called an [[antichain]].\n\n=== Examples ===\n\nStandard examples of posets arising in mathematics include:\n\n* The [[real number]]s ordered by the standard ''less-than-or-equal'' relation ‚â§ (a totally ordered set as well).\n* The set of [[subset]]s of a given set (its [[power set]]) ordered by [[subset|inclusion]]\n* The set of subspaces of a [[vector space]] ordered by inclusion.\n* For a partially ordered set ''P'', the [[sequence space]] containing all [[sequence]]s of elements from ''P'', where sequence ''a'' precedes sequence ''b'' if every item in ''a'' precedes the corresponding item in ''b''. Formally, {{math|(''a''<sub>''n''</sub>)<sub>''n''‚àà‚Ñï</sub>&nbsp;‚â§&nbsp;(''b''<sub>''n''</sub>)<sub>''n''‚àà‚Ñï</sub>}} if and only if {{math|''a''<sub>''n''</sub>&nbsp;‚â§&nbsp;''b''<sub>''n''</sub>}} for all ''n'' in ‚Ñï.\n* For a set ''X'' and a partially ordered set ''P'', the [[function space]] containing all functions from ''X'' to ''P'', where ''f'' ‚â§ ''g'' if and only if ''f''(''x'') ‚â§ ''g''(''x'') for all ''x'' in ''X''.\n* The vertex set of a [[directed acyclic graph]] ordered by [[reachability]].\n* The set of [[natural numbers]] equipped with the relation of [[divisibility]].\n\n=== Extrema ===\n\nThere are several notions of \"greatest\" and \"least\" element in a poset ''P'', notably:\n\n* [[Greatest element]] and least element: An element ''g'' in ''P'' is a greatest element if for every element ''a'' in ''P'', ''a''&nbsp;‚â§&nbsp;''g''. An element ''m'' in ''P'' is a least element if for every element ''a'' in ''P'', ''a''&nbsp;‚â•&nbsp;''m''. A poset can only have one greatest or least element.\n* [[Maximal element]]s and minimal elements: An element ''g'' in P is a maximal element if there is no element ''a'' in ''P'' such that ''a''&nbsp;>&nbsp;''g''. Similarly, an element ''m'' in ''P'' is a minimal element if there is no element ''a'' in P such that ''a''&nbsp;<&nbsp;''m''. If a poset has a greatest element, it must be the unique maximal element, but otherwise there can be more than one maximal element, and similarly for least elements and minimal elements.\n* [[Upper and lower bounds]]: For a subset ''A'' of ''P'', an element ''x'' in ''P'' is an upper bound of ''A'' if ''a''&nbsp;‚â§&nbsp;''x'', for each element ''a'' in ''A''. In particular, ''x'' need not be in ''A'' to be an upper bound of ''A''. Similarly, an element ''x'' in ''P'' is a lower bound of ''A'' if ''a''&nbsp;‚â•&nbsp;''x'', for each element ''a'' in ''A''. A greatest element of ''P'' is an upper bound of ''P'' itself, and a least element is a lower bound of ''P''.\n\nFor example, consider the natural numbers, ordered by divisibility: 1 is a least element, as it divides all other elements, but this set does not have a greatest element nor does it have any maximal elements: any ''g'' divides 2''g'', so 2''g'' is greater than ''g'' and ''g'' cannot be maximal. If instead we consider only the natural numbers that are greater than 1, then the resulting poset does not have a least element, but any [[prime number]] is a minimal element. In this poset, 60 is an upper bound (though not the least upper bound) of {2,3,5} and 2 is a lower bound of {4,6,8,12}.\n\n=== Additional structure ===\n\nIn many such cases, the poset has additional structure: For example, the poset can be a [[lattice (order)|lattice]] or a [[ordered semigroup|partially ordered algebraic structure]].\n\n====Lattices====\n{{Main|Lattice (order)}}\nA [[Partially ordered set|poset]] (''L'', ‚â§) is a '''lattice''' if it satisfies the following two axioms.\n\n;Existence of binary joins:\n: For any two elements ''a'' and ''b'' of ''L'', the set {''a, b''} has a '''[[Join (mathematics)|join]]''': <math>a \\lor b</math> (also known as the least upper bound, or the supremum).\n;Existence of binary meets:\n: For any two elements ''a'' and ''b'' of ''L'', the set {''a, b''} has a '''[[meet (mathematics)|meet]]''': <math>a \\land b</math> (also known as the greatest lower bound, or the infimum).\n\nThe join and meet of ''a'' and ''b'' are denoted by <math>a \\lor b</math> and <math>a \\land b</math>, respectively.  This definition makes <math> \\lor </math> and <math> \\land </math> [[binary operation]]s. The first axiom says that ''L'' is a [[join-semilattice]]; the second says that ''L'' is a [[meet-semilattice]]. Both operations are monotone with respect to the order: ''a''<sub>1</sub>&nbsp;‚â§&nbsp;''a''<sub>2</sub> and ''b''<sub>1</sub>&nbsp;‚â§&nbsp;''b''<sub>2</sub> implies that a<sub>1</sub><math> \\lor </math> b<sub>1</sub> ‚â§ a<sub>2</sub> <math>\\lor </math> b<sub>2</sub> and a<sub>1</sub><math> \\land </math>b<sub>1</sub> ‚â§ a<sub>2</sub><math> \\land </math>b<sub>2</sub>.\n\nIt follows by an [[mathematical induction|induction]] argument that every non-empty finite subset of a lattice has a join (supremum) and a meet (infimum). With additional assumptions, further conclusions may be possible; ''see'' [[Completeness (order theory)]] for more discussion of this subject.\n\nA '''bounded lattice''' has a [[greatest element|greatest]] (or maximum) and [[least element|least]] (or minimum) element, denoted 1 and 0 by convention (also called '''top''' and '''bottom'''). Any lattice can be converted into a bounded lattice by adding a greatest and least element, and every non-empty finite lattice is bounded, by taking the join (resp., meet) of all elements, denoted by <math>\\bigvee A=a_1\\lor\\cdots\\lor a_n</math> (resp.<math>\\bigwedge A=a_1\\land\\cdots\\land a_n</math>) where <math>A=\\{a_1,\\ldots,a_n\\}</math>.\n\nA poset is a bounded lattice if and only if every finite set of elements (including the empty set) has a join and a meet. Here, the join of an empty set of elements is defined to be the least element  <math>\\bigvee\\varnothing=0</math>, and the meet of the empty set is defined to be the greatest element <math>\\bigwedge\\varnothing=1</math>. This convention is consistent with the associativity and commutativity of meet and join: the join of a union of finite sets is equal to the join of the joins of the sets, and dually, the meet of a union of finite sets is equal to the meet of the meets of the sets, i.e., for finite subsets ''A'' and ''B'' of a poset ''L'',\n\n:<math>\\bigvee \\left( A \\cup B \\right)= \\left( \\bigvee A \\right) \\vee \\left( \\bigvee B \\right)</math>\n\nand\n\n:<math>\\bigwedge \\left( A \\cup B \\right)= \\left(\\bigwedge A \\right) \\wedge \\left( \\bigwedge B \\right)</math>\n\nhold. Taking ''B'' to be the empty set,\n\n:<math>\\bigvee \\left( A \\cup \\emptyset \\right)\n= \\left( \\bigvee A \\right) \\vee \\left( \\bigvee \\emptyset \\right)\n= \\left( \\bigvee A \\right) \\vee 0\n= \\bigvee A</math>\n\nand\n\n:<math>\\bigwedge \\left( A \\cup \\emptyset \\right)\n= \\left( \\bigwedge A \\right) \\wedge \\left( \\bigwedge \\emptyset \\right)\n= \\left( \\bigwedge A \\right) \\wedge 1\n= \\bigwedge A</math>\n\nwhich is consistent with the fact that <math>A \\cup \\emptyset = A</math>.\n\n====Ordered algebraic structure====\n{{Main|Ordered semigroup}}\n{{See also|Ordered vector space|Riesz space}}\nThe poset can be a [[Ordered semigroup|partially ordered algebraic structure]].<ref>Fujishige, Satoru ''Submodular functions and optimization''. Second edition. Annals of Discrete Mathematics, 58. Elsevier B. V., Amsterdam, 2005. xiv+395 pp. {{isbn|0-444-52086-4}} <!-- MR2171629 (2006d:90098) --></ref><ref>Gondran, Michel; Minoux, Michel ''Graphs, dioids and semirings. New models and algorithms''. Operations Research/Computer Science Interfaces Series, 41. Springer, New York, 2008. xx+383 pp. {{isbn|978-0-387-75449-9}} <!-- MR2389137 (2009g:16066) --></ref><ref>Dietrich, B. L.; Hoffman, A. J. On greedy algorithms, partially ordered sets, and submodular functions. ''IBM J. Res. Dev.'' 47 (2003), no. 1, 25‚Äì30.  <!-- MR1957350 (2003k:90102) --></ref><ref>Murota, Kazuo ''Discrete convex analysis''. SIAM Monographs on Discrete Mathematics and Applications. Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA, 2003. xxii+389 pp. {{isbn|0-89871-540-7}} <!-- MR1997998 (2004f:90007) --></ref><ref>Topkis, Donald M. ''Supermodularity and complementarity''. Frontiers of Economic Research. Princeton University Press, Princeton, NJ, 1998. xii+272 pp. {{isbn|0-691-03244-0}} <!-- MR1614637 (99i:90024) --></ref><ref>Zimmermann, U. ''Linear and combinatorial optimization in ordered algebraic structures''. Ann. Discrete Math. 10 (1981), viii+380 pp. <!-- MR0609751 --></ref><ref>Cuninghame-Green, Raymond ''Minimax algebra''. Lecture Notes in Economics and Mathematical Systems, 166. Springer-Verlag, Berlin-New York, 1979. xi+258 pp. {{isbn|3-540-09113-0}} <!-- MR0580321 --></ref>\n\nIn [[algebra]], an ''ordered semigroup'' is a [[semigroup]] (''S'',‚Ä¢) together with a [[partial order]] ‚â§ that is ''compatible'' with the semigroup operation, meaning that ''x'' ‚â§ ''y'' implies z‚Ä¢x ‚â§ z‚Ä¢y and x‚Ä¢z ‚â§ y‚Ä¢z for all ''x'', ''y'', ''z'' in ''S''. If S is a [[Group (mathematics)|group]] and it is ordered as a semigroup, one obtains the notion of [[ordered group]], and similarly if S is a [[monoid]] it may be called ''ordered monoid''. [[Ordered vector space|Partially ordered vector space]]s and [[Riesz space|vector lattice]]s are important in [[multiobjective optimization|optimization with multiple objectives]].<ref>{{cite book|last=ZƒÉlinescu|first=C.|title=Convex analysis in general vector spaces|publisher=World Scientific Publishing&nbsp; Co.,&nbsp;Inc|location = River Edge,&nbsp;NJ|year= 2002|pages=xx+367|isbn=981-238-067-1|mr=1921556}}</ref>\n\n== Ordinal optimization in computer science and statistics ==\n{{See also|Selection algorithm}}\n\nProblems of ordinal optimization arise in many disciplines. [[Computer science|Computer scientists]] study [[selection algorithm]]s, which are simpler than [[sorting algorithm]]s.<ref>[[Donald Knuth]]. ''[[The Art of Computer Programming]]'', Volume 3: ''Sorting and Searching'', Third Edition. Addison-Wesley, 1997. {{isbn|0-201-89685-0}}. Section 5.3.3: Minimum-Comparison Selection, pp.207&ndash;219.</ref><ref>[[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. {{isbn|0-262-03293-7}}. Chapter 9: Medians and Order Statistics, pp.183&ndash;196. Section 14.1: Dynamic order statistics, pp.302&ndash;308.</ref>\n\n[[Statistical decision theory]] studies \"selection problems\" that require the identification of a \"best\" subpopulation or of identifying a \"near best\" subpopulation.<ref>[[Jean D. Gibbons|Gibbons, Jean Dickinson]]; [[Ingram Olkin|Olkin, Ingram]], and Sobel, Milton, ''Selecting and Ordering of Populations'', Wiley, (1977). (Republished as a Classic in Applied Mathematics by SIAM.)</ref><ref>{{cite book|last1=Gupta|first1=Shanti&nbsp;S.|last2=Panchapakesan|first2=S.|title=Multiple decision procedures: Theory and methodology of selecting and ranking populations|series=Wiley Series in Probability and Mathematical Statistics|publisher=John Wiley &&nbsp;Sons|location=New York|year=1979|pages=xxv+573|isbn=0-471-05177-2|mr=555416}} (Republished as a Classic in Applied Mathematics by SIAM.)</ref><ref>Santner, Thomas J., and Tamhane, A. C., ''Design of Experiments: Ranking and Selection'', M. Dekker, (1984).</ref><ref>Robert E. Bechhofer, Thomas J. Santner, David M. Goldsman. ''Design and Analysis of Experiments for Statistical Selection, Screening, and Multiple Comparisons''. John Wiley & Sons, 1995.</ref><ref>Friedrich Liese, Klaus-J. Miescke. 2008. ''Statistical Decision Theory: Estimation, Testing, and Selection''. Springer Verlag.</ref>\n\n== Applications ==\n{{See also|Antimatroid|Max-plus algebra|Flow network|Queuing theory|Discrete event simulation}}\n\nSince the 1960s, the field of ordinal optimization has expanded in theory and in applications. In particular, [[antimatroid]]s and the \"[[max-plus algebra]]\" have found application in [[flow network|network analysis]] and [[queuing theory]], particularly in queuing networks and [[discrete event simulation|discrete-event systems]].<ref>{{cite book| last1=Glasserman|first1=Paul|last2=Yao|first2=David D.|title=Monotone structure in discrete-event systems|series=Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics|publisher=John Wiley & Sons, Inc.|location=New York|year=1994|pages=xiv+297|isbn=0-471-58041-4|mr=1266839 }}</ref><ref>{{cite book|author1=Baccelli, Fran√ßois Louis |author2=Cohen, Guy |author3=Olsder, Geert Jan |author4=Quadrat, Jean-Pierre |title=Synchronization and linearity: An algebra for discrete event systems|series=Wiley Series in Probability and Mathematical Statistics: Probability and Mathematical Statistics|publisher=John Wiley & Sons, Ltd.|location=Chichester|year=1992|pages=xx+489|isbn=0-471-93609-X |mr=1204266 }}</ref><ref>{{cite book|author1=Heidergott, Bernd |author2=Oldser, Geert Jan |author3=van der Woude, Jacob |title=Max plus at work: Modeling and analysis of synchronized systems, a course on max-plus algebra and its applications|series=Princeton Series in Applied Mathematics|publisher=Princeton University Press|location=Princeton, NJ|year=2006|pages=xii+213|isbn=978-0-691-11763-8|mr=2188299 }}</ref>\n\n== See also ==\n\n* [[Stochastic optimization]]\n* [[Computational complexity theory]]\n* [[Heuristic (computer science)|Heuristics]]\n* [[Level of measurement]] (\"Ordinal data\")\n\n== References ==\n\n{{reflist|colwidth=30em}}\n\n== Further reading ==\n\n* Fujishige, Satoru ''Submodular functions and optimization''. Second edition. Annals of Discrete Mathematics, 58. Elsevier B. V., Amsterdam, 2005. xiv+395 pp. {{isbn|0-444-52086-4}} <!-- MR2171629 (2006d:90098) -->\n* Gondran, Michel; Minoux, Michel ''Graphs, dioids and semirings. New models and algorithms''. Operations Research/Computer Science Interfaces Series, 41. Springer, New York, 2008. xx+383 pp. {{isbn|978-0-387-75449-9}} <!-- MR2389137 (2009g:16066) -->\n* Dietrich, B. L.; Hoffman, A. J. On greedy algorithms, partially ordered sets, and submodular functions. ''IBM J. Res. Dev.'' 47 (2003), no. 1, 25‚Äì30.  <!-- MR1957350 (2003k:90102) -->\n* Murota, Kazuo ''Discrete convex analysis''. SIAM Monographs on Discrete Mathematics and Applications. Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA, 2003. xxii+389 pp. {{isbn|0-89871-540-7}} <!-- MR1997998 (2004f:90007) -->\n* Topkis, Donald M. ''Supermodularity and complementarity''. Frontiers of Economic Research. Princeton University Press, Princeton, NJ, 1998. xii+272 pp. {{isbn|0-691-03244-0}} <!-- MR1614637 (99i:90024) -->\n* Singer, Ivan ''Abstract convex analysis''. Canadian Mathematical Society Series of Monographs and Advanced Texts. A Wiley-Interscience Publication. John Wiley & Sons, Inc., New York, 1997. xxii+491 pp. {{isbn|0-471-16015-6}} <!-- MR1461544 -->\n* Bj√∂rner, Anders; Ziegler, G√ºnter M. Introduction to greedoids. ''Matroid applications'', 284‚Äì357, Encyclopedia Math. Appl., 40, Cambridge Univ. Press, Cambridge, 1992,<!-- MR1165545 (94a:05038) -->\n* Zimmermann, U. ''Linear and combinatorial optimization in ordered algebraic structures''. Ann. Discrete Math. 10 (1981), viii+380 pp. <!-- MR0609751 -->\n* Cuninghame-Green, Raymond ''Minimax algebra''. Lecture Notes in Economics and Mathematical Systems, 166. Springer-Verlag, Berlin-New York, 1979. xi+258 pp. {{isbn|3-540-09113-0}} <!-- MR0580321 -->\n* {{cite book|author1=Baccelli, Fran√ßois Louis |author2=Cohen, Guy |author3=Olsder, Geert Jan |author4=Quadrat, Jean-Pierre |title=Synchronization and linearity: An algebra for discrete event systems|series=Wiley Series in Probability and Mathematical Statistics: Probability and Mathematical Statistics|publisher=John Wiley & Sons, Ltd.|location=Chichester|year=1992|pages=xx+489|isbn=0-471-93609-X\n|mr=1204266}}\n* {{cite book| last1=Glasserman|first1=Paul|last2=Yao|first2=David D.|title=Monotone structure in discrete-event systems|series=Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics|publisher=John Wiley & Sons, Inc.|location=New York|year=1994|pages=xiv+297|isbn=0-471-58041-4|mr=1266839}}\n* {{cite book|author1=Heidergott, Bernd |author2=Oldser, Geert Jan |author3=van der Woude, Jacob |title=Max plus at work: Modeling and analysis of synchronized systems, a course on max-plus algebra and its applications|series=Princeton Series in Applied Mathematics|publisher=Princeton University Press|location=Princeton, NJ|year=2006|pages=xii+213|isbn=978-0-691-11763-8|mr=2188299}}\n* [[Yu-Chi Ho|Ho, Y.C.]], Sreenivas, R., Vakili, P.,\"Ordinal Optimization of Discrete Event Dynamic Systems\", J. of DEDS 2(2), 61-88, (1992).\n* Allen, Eric, and Marija D. Ilic. ''Price-Based Commitment Decisions in the Electricity Market''. Advances in industrial control. London: Springer, 1999. {{isbn|978-1-85233-069-9}}\n\n== External links ==\n* [http://people.deas.harvard.edu/~ho/DEDS/OO/Reference/OOReference.html Annotated bibliography on ordinal optimization] by [[Yu-Chi Ho]]\n\n{{DEFAULTSORT:Ordinal Optimization}}\n[[Category:Control theory]]\n[[Category:Order theory]]\n[[Category:Optimization of ordered sets]]"
    },
    {
      "title": "Supermodular function",
      "url": "https://en.wikipedia.org/wiki/Supermodular_function",
      "text": "In [[mathematics]], a function\n:<math>f\\colon \\mathbb{R}^k \\to \\mathbb{R}</math>\nis '''supermodular''' if\n:<math>\nf(x \\uparrow y) + f(x \\downarrow y) \\geq f(x) + f(y)\n</math>\nfor all <math>x</math>, <math>y \\isin \\mathbb{R}^{k}</math>, where <math>x \\uparrow y</math> denotes the componentwise maximum and <math>x \\downarrow y</math> the componentwise minimum of <math>x</math> and <math>y</math>.\n\nIf &minus;''f'' is supermodular then ''f'' is called '''submodular''', and if the inequality is changed to an equality the function is '''modular'''.\n\nIf ''f'' is twice continuously differentiable, then supermodularity is equivalent to the condition<ref>The equivalence between the definition of supermodularity and its calculus formulation is sometimes called [[Topkis's theorem|Topkis' characterization theorem]]. See {{cite journal |first=Paul |last=Milgrom |first2=John |last2=Roberts |year=1990 |title=Rationalizability, Learning, and Equilibrium in Games with Strategic Complementarities |journal=[[Econometrica]] |volume=58 |issue=6 |pages=1255‚Äì1277 [p. 1261] |jstor=2938316 |doi=10.2307/2938316 }}</ref>\n\n:<math> \\frac{\\partial ^2 f}{\\partial z_i\\, \\partial z_j} \\geq 0 \\mbox{ for all } i \\neq j.</math>\n\n==Supermodularity in economics and game theory==\nThe concept of supermodularity is used in the social sciences to analyze how one [[Agent (economics)|agent's]] decision affects the incentives of others.\n\nConsider a [[symmetric game]] with a smooth payoff function <math>\\,f</math> defined over actions <math>\\,z_i</math> of two or more players <math>i \\in {1,2,\\dots,N}</math>. Suppose the action space is continuous; for simplicity, suppose each action is chosen from an interval: <math>z_i \\in [a,b]</math>. In this context, supermodularity of <math>\\,f</math> implies that an increase in player <math>\\,i</math>'s choice <math>\\,z_i</math> increases the marginal payoff <math>df/dz_j</math> of action <math>\\,z_j</math> for all other players <math>\\,j</math>. That is, if any player <math>\\,i</math> chooses a higher <math>\\,z_i</math>, all other players <math>\\,j</math> have an incentive to raise their choices <math>\\,z_j</math> too. Following the terminology of Bulow, [[John Geanakoplos|Geanakoplos]], and [[Paul Klemperer|Klemperer]] (1985), economists call this situation [[strategic complements|strategic complementarity]], because players' strategies are complements to each other.<ref>{{cite journal |first=Jeremy I. |last=Bulow |first2=John D. |last2=Geanakoplos |first3=Paul D. |last3=Klemperer |year=1985 |title=Multimarket Oligopoly: Strategic Substitutes and Complements |journal=[[Journal of Political Economy]] |volume=93 |issue=3 |pages=488‚Äì511 |doi=10.1086/261312 |citeseerx=10.1.1.541.2368 }}</ref> This is the basic property underlying examples of [[General equilibrium#Uniqueness|multiple equilibria]] in [[coordination game]]s.<ref>{{cite journal |first=Russell |last=Cooper |first2=Andrew |last2=John |year=1988 |title=Coordinating coordination failures in Keynesian models |journal=[[Quarterly Journal of Economics]] |volume=103 |issue=3 |pages=441‚Äì463 |doi=10.2307/1885539 |jstor=1885539 }}</ref>\n\nThe opposite case of submodularity of <math>\\,f</math> corresponds to the situation of [[strategic complements|strategic substitutability]]. An increase in <math>\\,z_i</math> lowers the marginal payoff to all other player's choices <math>\\,z_j</math>, so strategies are substitutes. That is, if <math>\\,i</math> chooses a higher <math>\\,z_i</math>, other players have an incentive to pick a ''lower'' <math>\\,z_j</math>.\n\nFor example, Bulow et al. consider the interactions of many [[Imperfect competition|imperfectly competitive]] firms. When an increase in output by one firm raises the marginal revenues of the other firms, production decisions are strategic complements. When an increase in output by one firm lowers the marginal revenues of the other firms, production decisions are strategic substitutes.\n\n\nA supermodular [[utility function]] is often related to [[complementary goods]]. However, this view is disputed.<ref>{{Cite journal|doi=10.1016/j.jet.2008.06.004 |title=Supermodularity and preferences |journal=[[Journal of Economic Theory]] |volume=144 |issue=3 |pages=1004 |year=2009 |last1=Chambers |first1=Christopher P. |last2=Echenique |first2=Federico |citeseerx=10.1.1.122.6861 }}</ref>\n\n==Submodular functions of subsets==\nSupermodularity and submodularity are also defined for functions defined over subsets of a larger set.  Intuitively, a submodular function over the subsets demonstrates \"diminishing returns\".  There are specialized techniques for optimizing submodular functions.\n\nLet ''S'' be a finite set.  A function <math>f\\colon 2^S \\to \\mathbb{R}</math> is submodular if for any <math>A \\subset B \\subset S</math> and <math>x \\in S \\setminus B</math>, <math>f(A \\cup \\{x\\})-f(A) \\geq f(B \\cup \\{x\\})-f(B)</math>.  For supermodularity, the inequality is reversed.\n<!--\nA simple illustrative example motivates this definition of submodular.  Let S be a set of different foods, <math>M \\subset  S</math> a meal, and <math>f(M)</math> the \"goodness\" of that meal.  Then A above is one meal, and B is A but with even more options.  Let x be ice cream.  Adding ice cream to a meal is always good, but it is best if there is not already a dessert.  If A and B either both have a dessert or both do not, then adding ice cream to them is comparably good.  But if A does not have dessert and B does, then the effect of adding ice cream is more pronounced in A.\n-->\n\nThe definition of submodularity can equivalently be formulated as\n:<math> f(A)+f(B) \\geq f(A \\cap B) + f(A \\cup B) </math>\nfor all subsets ''A'' and ''B'' of ''S''.\n\n==See also==\n* [[Pseudo-Boolean function]]\n* [[Topkis's theorem]]\n* [[Submodular set function]]\n* [[Superadditive]]\n* [[Utility functions on indivisible goods]]\n\n==Notes and references==\n{{Reflist}}\n\n{{DEFAULTSORT:Supermodular Function}}\n[[Category:Order theory]]\n[[Category:Optimization of ordered sets]]\n[[Category:Generalized convexity]]\n[[Category:Supermodular functions]]"
    },
    {
      "title": "Topkis's theorem",
      "url": "https://en.wikipedia.org/wiki/Topkis%27s_theorem",
      "text": "{{Multiple issues|\n{{single source|date=May 2014}}\n{{primary sources|date=May 2014}}\n}}\nIn [[mathematical economics]], '''Topkis's theorem''' is a result that is useful for establishing [[comparative statics]].  The theorem allows researchers to understand how the optimal value for a choice variable changes when a feature of the environment changes.  The result states that if ''f'' is [[supermodular]] in (''x'',''Œ∏''), and ''D'' is a [[lattice (order)|lattice]], then <math>x^*(\\theta)=\\arg\\max_{x\\in D}f(x,\\theta)</math> is nondecreasing in ''Œ∏''.  The result is especially helpful for establishing comparative static results when the objective function is not differentiable.\n\n==An example==\nThis example will show how using Topkis's theorem gives the same result as using more standard tools.  The advantage of using Topkis's theorem is that it can be applied to a wider class of problems than can be studied with standard economics tools.\n\nA driver is driving down a highway and must choose a speed, ''s''. Going faster is desirable, but is more likely to result in a crash. There is some prevalence of potholes, ''p''.  The presence of potholes increases the probability of crashing.  Note that ''s'' is a choice variable and ''p'' is a parameter of the environment that is fixed from the perspective of the driver.  The driver seeks to <math>\\max_{s}U(s,p)</math>.\n\nWe would like to understand how the driver's speed (a choice variable) changes with the amount of potholes: \n\n:<math>\\frac{\\partial s^{\\ast }(p)}{\\partial p}.</math>\n\nIf one wanted to solve the problem with standard tools such as the [[implicit function theorem]], one would have to assume that the problem is well behaved: ''U''(.) is twice continuously differentiable, concave in ''s'', that the domain over which ''s'' is defined is convex, and that it there is a unique maximizer <math>s^{\\ast }(p)</math> for every value of ''p'' and that <math>s^{\\ast }(p)</math> is in the interior of the set over which ''s'' is defined.  Note that the optimal speed is a function of the amount of potholes.  Taking the first order condition, we know that at the optimum, <math>U_{s}(s^{\\ast }(p),p)=0</math>.  Differentiating the first order condition, with respect to ''p'' and using the implicit function theorem, we find that\n: <math>U_{ss}(s^{\\ast}(p),p)(\\partial s^{\\ast }(p)/(\\partial p))+U_{sp}(s^{\\ast }(p),p)=0</math> \nor that \n\n: <math>\\frac{\\partial s^{\\ast }(p)}{\\partial p}\n = \\underset{\\text{negative since we assumed }U(.)\\text{ was concave in }s}{\\underbrace{\\frac{-U_{sp}(s^{\\ast }(p),p)}{U_{ss}(s^{\\ast }(p),p)}}}.</math>\n\nSo, \n\n:<math>\\frac{\\partial s^{\\ast }(p)}{\\partial p}\\overset{\\text{sign}}{=}U_{sp}(s^{\\ast }(p),p). </math>\n\nIf ''s'' and ''p'' are substitutes, \n\n:<math>U_{sp}(s^\\ast (p),p)<0</math> \n\nand hence \n\n:<math>\\frac{\\partial s^{\\ast }(p)}{\\partial p}<0</math>\n\nand more potholes causes less speeding.  Clearly it is more reasonable to assume that they are substitutes.\n\nThe problem with the above approach is that it relies on the differentiability of the objective function and on concavity.  We could get at the same answer using Topkis's theorem in the following way.  We want to show that <math>U(s,p)</math> is submodular (the opposite of supermodular) in <math>\\left( s,p\\right) </math>.  Note that the choice set is clearly a lattice.  The cross partial of ''U'' being negative, <math>\\frac{\\partial^2 U}{\\partial s\\,\\partial p}<0</math>, is a sufficient condition.  Hence if <math> \\frac{\\partial^2 U}{\\partial s\\,\\partial p}<0,</math> we know that <math>\\frac{\\partial s^{\\ast }(p)}{\\partial p}<0</math>.\n\nHence using the [[implicit function theorem]] and Topkis's theorem gives the same result, but the latter does so with fewer assumptions.\n\n==Notes and references==\n{{Reflist}}\n*{{cite journal |first=Rabah |last=Amir |title=Supermodularity and Complementarity in Economics: An Elementary Survey |journal=[[Southern Economic Journal]] |volume=71 |issue=3 |year=2005 |pages=636‚Äì660 |jstor=20062066 |doi=10.2307/20062066 }}\n*{{cite journal |first=Donald M. |last=Topkis |year=1978 |title=Minimizing a Submodular Function on a Lattice |journal=[[Operations Research (journal)|Operations Research]] |volume=26 |issue=2 |pages=305‚Äì321 |doi=10.1287/opre.26.2.305 |citeseerx=10.1.1.557.5908 }}\n*{{cite book |first=Donald M. |last=Topkis |year=1998 |title=Supermodularity and Complementarity |location= |publisher=[[Princeton University Press]] |isbn=978-0-691-03244-3 }}\n\n{{DEFAULTSORT:Topkis's Theorem}}\n[[Category:Comparative statics]]\n[[Category:Economics theorems]]\n[[Category:Lattice theory]]\n[[Category:Order theory]]\n[[Category:Optimization of ordered sets]]\n[[Category:Supermodular functions]]"
    },
    {
      "title": "Allocative efficiency",
      "url": "https://en.wikipedia.org/wiki/Allocative_efficiency",
      "text": "{{short description|State of the economy in which production represents consumer preferences}}\n'''Allocative efficiency''' is a state of the economy in which production represents consumer preferences; in particular, every good or service is produced up to the point where the last unit provides a marginal benefit to consumers equal to the marginal cost of producing.\n\nIn [[contract theory]], allocative efficiency is achieved in a contract in which the skill demanded by the offering party and the skill of the agreeing party are the same.\n\nAlthough there are different standards of evaluation for the concept of allocative efficiency, the basic principle asserts that in any economic system, choices in resource allocation produce both \"winners\" and \"losers\" relative to the choice being evaluated. The principles of rational choice, individual maximization, [[utilitarianism]] and market theory further suppose that the outcomes for winners and losers can be identified, compared and measured. Under these basic premises, the goal of attaining allocative efficiency can be defined according to some principle where some allocations are subjectively better than others. For example, an economist might say that a change in policy is an allocative improvement as long as those who benefit from the change (winners) gain more than the losers lose (see [[Kaldor‚ÄìHicks efficiency]]).\n\nAn allocatively efficient economy produces an \"optimal mix\" of commodities. A firm is allocatively efficient when its price is equal to its [[marginal costs]] (that is, P = MC) in a perfect market. The [[demand]] curve coincides with the [[marginal utility]] curve, which measures the (private) benefit of the additional unit, while the [[Supply (economics)|supply]] curve coincides with the [[marginal cost]] curve, which measures the (private) cost of the additional unit. In a perfect market, there are no externalities, implying that the demand curve is also equal to the social benefit of the additional unit, while the supply curve measures the social cost of the additional unit. Therefore, the market equilibrium, where demand meets supply, is also where the marginal social benefit equals the marginal social costs. At this point, net social benefit is maximized, meaning this is the allocatively efficient outcome. When a market fails to allocate resources efficiently, there is said to be [[market failure]]. Market failure may occur because of imperfect knowledge, differentiated goods, concentrated [[market power]] (e.g., [[monopoly]] or [[oligopoly]]), or [[externalities]].\n\nIn the single-price model, at the point of allocative efficiency price is equal to marginal cost.<ref>{{cite book|title=Matters of Principle|last=Markovits|first=Richard|publisher=[[New York University Press]]|year=1998|isbn=978-0-8147-5513-6|location=[[New York City|New York]]}}</ref><ref>{{cite book|title=Truth or Economics|last=Markovits|first=Richard|publisher=[[Yale University Press]]|year=2008|isbn=978-0-300-11459-1|location=[[New Haven]]}}</ref>   At this point the social surplus is maximized with no [[deadweight loss]] (the latter being the value society puts on that level of output produced minus the value of resources used to achieve that level). Allocative efficiency is the main tool of welfare analysis to measure the impact of markets and public policy upon society and subgroups being made better or worse off.\n\nIt is possible to have [[Pareto efficiency]] without allocative efficiency: in such a situation, it is impossible to reallocate resources in such a way that someone gains and no one loses (hence we have Pareto efficiency), yet it would be possible to reallocate in such a way that gainers gain more than losers lose (hence with such a reallocation, we do not have allocative efficiency).<ref>Beardshaw, J., ''Economics: A Student's Guide'' ([[Upper Saddle River, NJ]]: [[FT Press]], 1984), p. 397.</ref>\n\nAlso, for an extensive discussion of various types of allocative (in)efficiency in production context and their estimations see Sickles and Zelenyuk (2019, Chapter 3, etc.). <ref>[https://assets.cambridge.org/97811070/36161/frontmatter/9781107036161_frontmatter.pdf  Sickles, R., & Zelenyuk, V. (2019). Measurement of Productivity and Efficiency: Theory and Practice. Cambridge: Cambridge University Press. doi:10.1017/9781139565981 ]</ref>\n\n==See also==\n\n*[[Financial market efficiency]]\n*[[Pareto efficiency]]\n*[[Production-possibility frontier]]\n*[[Productive efficiency]]\n*[[X-inefficiency]]\n\n==References==\n{{reflist}}\n\n[[Category:Pareto efficiency]]\n[[Category:Price controls]]"
    },
    {
      "title": "Distortion (economics)",
      "url": "https://en.wikipedia.org/wiki/Distortion_%28economics%29",
      "text": "{{merge|Market distortion|date=April 2018}}\nA '''distortion''' is \"any departure from the ideal of perfect competition that therefore interferes with economic agents maximizing social welfare when they maximize their own\".<ref>[[Alan Deardorff]]. \"[http://www-personal.umich.edu/~alandear/glossary/d.html#distortion Distortion]\",  Deardorff's Glossary of International Economics.</ref> A proportional wage-income tax, for instance, is distortionary, whereas a [[lump-sum tax]] is not.  In a [[competitive equilibrium]], a proportional wage income tax discourages work.<ref>Stephen D. Williamson (2010). \"Sources of Social Inefficiencies,\" ''Macroeconomics'', 3rd Edition.</ref>\n\nIn [[perfect competition]] with no externalities, there is zero distortion at market equilibrium of [[supply and demand]] where [[price]] equals [[marginal cost]] for each firm and product. More generally, a measure of distortion is the deviation between the market price of a good and its [[marginal cost|marginal]] [[social cost]], that is, the difference between the [[marginal rate of substitution]] in consumption and the [[marginal rate of transformation]] in production. Such a deviation may result from government [[regulation]], [[monopoly]] [[tariff]]s and [[import quota]]s, which in theory may give rise to [[rent seeking]].  Other sources of distortions are uncorrected [[externalities]],<ref>[[Agnar Sandmo]]  (2008). \"Pigouvian taxes.\" ''[[The New Palgrave Dictionary of Economics]]'', 2nd Edition. [http://www.dictionaryofeconomics.com/article?id=pde2008_P000351&q=distortions&topicid=&result_number=2 Abstract.]</ref> different tax rates on goods or income,<ref>‚Ä¢Louis Kaplow  (2008). \"optimal taxation,\" ''The New Palgrave Dictionary of Economics'', 2nd Edition. [http://www.dictionaryofeconomics.com/article?id=pde2008_O000034&q=distortion%20&topicid=&result_number=8 Abstract.]<br/>&nbsp;&nbsp; ‚Ä¢ Louis Kaplow  (2008). \"income taxation and optimal policies,\" ''The New Palgrave Dictionary of Economics'', 2nd Edition. [http://www.dictionaryofeconomics.com/article?id=pde2008_I000245  Abstract.]<br/>&nbsp;&nbsp; ‚Ä¢ Alan J. Auerbach  (2008). \"taxation of corporate profits,\" ''The New Palgrave Dictionary of Economics'', 2nd Edition. [http://www.dictionaryofeconomics.com/article?id=pde2008_T000023&q=Distortion%20&topicid=&result_number=2 Abstract.]</ref> [[Inflation#Effects|inflation]],<ref>S. Rao Aiyagari, R. Anton Braun, [[Zvi Eckstein]] (1998). \"Transaction Services, Inflation, and Welfare,\" ''Journal of Political Economy'', 106(6), pp. [http://www.econ.umn.edu/~eckstein/research/Aiya-Braun-Eckstein.pdf 1274-1301] {{webarchive |url=https://web.archive.org/web/20050521012313/http://www.econ.umn.edu/~eckstein/research/Aiya-Braun-Eckstein.pdf |date=May 21, 2005 }} (press '''+''').</ref> and [[incomplete information]]. Each of these may lead to a net loss in [[social surplus]].<ref>‚Ä¢ [[T. N. Srinivasan]] (1987). \"distortions,\" ''[[The New Palgrave: A Dictionary of Economics]]'', v. 1, pp. 865-67.<br/>&nbsp;&nbsp; ‚Ä¢ Joel Slemrod (1990). \"Optimal Taxation and Optimal Tax Systems,\" ''Journal of Economic Perspectives'', 4(1), p [https://www.jstor.org/stable/1942838 p. 157]-178.</ref>\n\n==See also==\n{{div col|colwidth=22em}}\n* [[Deadweight loss]]\n* [[Excess burden of taxation]]\n* [[Government failure]]\n* [[Imperfect competition]]\n* [[Land value tax]]\n* [[Lump-sum tax]]\n* [[Market failure]]\n* [[Optimal tax]]\n* [[Laffer Curve]]\n* [[Welfare economics]]\n{{div col end}}\n\n==Notes==\n<!-- See the user page User:Dmoss/Wikicite for an excellent citation tool! -->\n{{Reflist|2}}\n\n[[Category:Imperfect competition]]\n[[Category:Pareto efficiency]]\n\n\n{{microeconomics-stub}}"
    },
    {
      "title": "Efficient cake-cutting",
      "url": "https://en.wikipedia.org/wiki/Efficient_cake-cutting",
      "text": "'''Efficient cake-cutting''' is a problem in [[economics]] and [[computer science]]. It involves a ''heterogenous'' resource, such as a cake with different toppings or a land with different coverings, that is assumed to be ''divisible'' - it is possible to cut arbitrarily small pieces of it without destroying their value. The resource has to be divided among several partners who have different preferences over different parts of the cake, i.e., some people prefer the chocolate toppings, some prefer the cherries, some just want as large a piece as possible, etc. The division should be [[Pareto-efficient]].\n\nMost often, efficiency is studied in connection with [[fair cake-cutting|fairness]], and the goal is to find a division which satisfies both efficiency and fairness criteria.\n\n== Assumptions ==\nThere is a cake <math>C</math>. It is usually assumed to be either a finite 1-dimensional segment, a 2-dimensional polygon or a finite subset of the multidimensional Euclidean plane <math>\\mathbb{R}^d</math>.\n\nThere are <math>n</math> partners. Each partner <math>i</math> has a subjective value function <math>V_i</math> which maps subsets of <math>C</math> to numbers.\n\n<math>C</math> has to be divided to <math>n</math> disjoint subsets, such that each person receives a disjoint subset. The piece allocated to person <math>i</math> is called <math>X_i</math>, so that <math>C = X_1 \\sqcup ... \\sqcup X_n</math>.\n\n== Example cake ==\n\nIn the following lines we consider a cake with two parts: chocolate and vanilla, and two partners: Alice and George, with the following valuations:\n\n{| class=\"wikitable\"\n|-\n! Header text !! Chocolate !! Vanilla\n|-\n| Alice || <big>9</big> || <big>1</big>\n|-\n| George || <big>6</big> || <big>4</big>\n|}\n\n== Efficiency ==\nA division <math>Y</math> ''Pareto-dominates'' a division <math>X</math>, if at least one person feels that <math>Y</math> is better than <math>X</math>, and no person feels that <math>Y</math> is worse than <math>X</math>. In symbols:\n\n:<math>\\forall{i}:\\ V_i(Y_i)\\geq V_i(X_i)</math> and <math>\\exists{i}: V_i(Y_i)>V_i(X_i)</math>\n\nA [[Pareto efficient]] (PE) division is a division that is not Pareto-dominated by any other division, i.e., it cannot be improved without objection. In the example cake, many PE divisions are possible. For example, every division that gives the entire cake to a single person is PE, since every change in the division will raise the objection of that single person. Of course a PE division is not necessarily fair.\n\nA division that is both Pareto-efficient and [[proportional division|proportional]] will be called PEPR and a division that is both PE and [[envy-free cake-cutting|envy-free]] will be called PEEF for short.\n\n\n== Combining efficiency and fairness ==\n\n=== PEPR division ===\nA PE division can be achieved trivially by giving the entire cake to a single partner. But, a PE division which is also ''proportional'' cannot be found by a finite algorithm. The proof is essentially the same as for [[Utilitarian cake-cutting#Finding utilitarian-maximal divisions|utilitarian-maximal divisions]].\n\n=== PEEF division ===\nFor ''n'' partners with additive value functions, when pieces may be disconnected, a PEEF division always exists. This is [[Weller's theorem]].\n\nIf the cake is a 1-dimensional ''interval'' and each person must receive a connected interval, the following general result holds: if the value functions are strictly monotonic (i.e. each person strictly prefers a piece over all its proper subsets) then every EF division is also PE (note that this is not true if the agents may receive disconnected pieces). Hence, in this case, the [[Simmons‚ÄìSu protocols]] create a PEEF division.\n\nIf the cake is a 1-dimensional ''circle'' (i.e. an interval whose two endpoints are topologically identified) and each person must receive a connected arc, then the previous result does not hold: an EF division is not necessarily PE. Additionally, there are pairs of (non-additive) value functions for which no PEEF division exists. However, if there are 2 agents and at least one of them has an additive value function, then a PEEF division exists.<ref name=t07>{{Cite journal | doi = 10.1007/s00199-006-0109-3| title = Children Crying at Birthday Parties. Why?| journal = Economic Theory| volume = 31| issue = 3| pages = 501| year = 2006| last1 = Thomson | first1 = W. }}</ref>\n\n== See also ==\n* [[Utilitarian cake-cutting]]\n* [[Fair cake-cutting]]\n* [[Price of fairness]]\n\n== References ==\n{{reflist}}\n\n[[Category:Fair division]]\n[[Category:Pareto efficiency]]"
    },
    {
      "title": "Kaldor‚ÄìHicks efficiency",
      "url": "https://en.wikipedia.org/wiki/Kaldor%E2%80%93Hicks_efficiency",
      "text": "{{Use dmy dates|date=September 2015}}\n{{more footnotes|date=April 2012}}\nA '''Kaldor‚ÄìHicks improvement''', named for [[Nicholas Kaldor]] and [[John Hicks]], is an economic re-allocation of resources among people that captures some of the intuitive appeal of a [[Pareto efficiency|Pareto improvement]], but has less stringent criteria and is hence applicable to more circumstances. A re-allocation is a Kaldor‚ÄìHicks improvement if those that are made better off could [[compensation principle|hypothetically compensate]] those that are made worse off and lead to a Pareto-improving outcome. The compensation does not actually have to occur (there is no presumption in favor of status-quo) and thus, a Kaldor‚ÄìHicks improvement can in fact leave some people worse off.\n\nA situation is said to be '''Kaldor‚ÄìHicks efficient''', or equivalently is said to satisfy the '''Kaldor‚ÄìHicks criterion''', if no potential Kaldor‚ÄìHicks improvement from that situation exists.\n\n==Explanation==\nA reallocation is said to be a [[Pareto improvement]] if at least one person is made better off and nobody is made worse off. However in practice, it is almost impossible to take any social action, such as a change in economic policy, without making at least one person worse off. Even voluntary exchanges may not be Pareto improving if they make third parties worse off.\n\nUsing the criterion for Kaldor‚ÄìHicks improvement, an outcome is an improvement if those that are made better off could in principle compensate those that are made worse off, so that a Pareto improving outcome ''could'' (though does not have to) be achieved. For example, a voluntary exchange that creates pollution would be a ''Kaldor‚ÄìHicks improvement'' if the buyers and sellers are still willing to carry out the transaction even if they have to fully compensate the victims of the pollution. Kaldor‚ÄìHicks does not require compensation actually be paid, merely that the possibility for compensation exists, and thus need not leave each at least as well off. Under Kaldor‚ÄìHicks efficiency, an improvement can in fact leave some people worse off. Pareto-improvements require making every party involved better off (or at least none worse off).\n\nWhile every Pareto improvement is a Kaldor‚ÄìHicks improvement, most Kaldor‚ÄìHicks improvements are not Pareto improvements. This is because the set of Pareto improvements is a proper subset of Kaldor‚ÄìHicks improvements. This reflects the greater flexibility and applicability of the Kaldor‚ÄìHicks criterion relative to the Pareto criterion.\n\n==Use in policy-making==\nThe Kaldor‚ÄìHicks methods are typically used as tests of potential improvements rather than as efficiency goals themselves. They are used to determine whether an activity moves the economy toward Pareto efficiency. Any change usually makes some people better off and others worse off, so these tests consider what would happen if gainers were to compensate losers.\n\nThe Kaldor criterion is that an activity moves the economy closer to Pareto optimality if the maximum amount the gainers are [[willingness to pay|prepared to pay]] to the losers to agree to the change is greater than the minimum amount losers are prepared to accept; the Hicks criterion is that an activity moves the economy toward Pareto optimality if the maximum amount the losers would pay the gainers to forgo the change is less than the minimum amount the gainers would accept to so agree. Thus, the Kaldor test supposes that losers could prevent the arrangement and asks whether gainers value their gain so much they would and could pay losers to accept the arrangement, whereas the Hicks test supposes that gainers are able to proceed with the change and asks whether losers consider their loss to be worth less than what it would cost them to pay gainers to agree ''not'' to proceed with the change. After several technical problems with each separate criterion were discovered, they were combined into the [[Scitovsky]] criterion, more commonly known as the \"Kaldor‚ÄìHicks criterion\", which does not share the same flaws.\n\nThe Kaldor‚ÄìHicks criterion is widely applied in [[welfare economics]] and [[managerial economics]].  For example, it forms an underlying rationale for [[cost‚Äìbenefit analysis]].  In cost‚Äìbenefit analysis, a project (for example, a new airport) is evaluated by comparing the total costs, such as building costs and environmental costs, with the total benefits, such as airline profits and convenience for travelers. (However, as cost‚Äìbenefit analysis may also assign different social welfare weights to different individuals, e.g. more to the poor, the compensation criterion is not always invoked by cost‚Äìbenefit analysis.)\n\nThe project would typically be given the go-ahead if the benefits exceed the costs.  This is effectively an application of the Kaldor‚ÄìHicks criterion because it is equivalent to requiring that the benefits be enough that those that benefit could in theory compensate those that have lost out.  The criterion is used because it is argued that it is justifiable for society as a whole to make some worse off if this means a greater gain for others.\n\n==Criticisms==\n\nAt a more technical level, various versions of the Kaldor‚ÄìHicks criteria lack desirable formal properties. For instance, [[Tibor Scitovsky]] demonstrated that the Kaldor criterion alone is not [[Antisymmetric relation|antisymmetric]]: it's possible to have a situation where an outcome A is an improvement (according to the Kaldor criterion) over outcome B, but B is also an improvement over A. The combined Kaldor‚ÄìHicks criterion does not have this problem, but it can be [[Intransitivity|non-transitive]] (though A may be an improvement over B, and B over C, A is not thereby an improvement over C).<ref>{{cite journal |last=Scitovsky |first=T. |authorlink=Tibor Scitovsky|author2= |year=1941 |month= |title=A Note on Welfare Propositions in Economics |journal=Review of Economic Studies |volume=9 |issue=1 |pages=77‚Äì88 |doi=10.2307/2967640 |jstor= 2967640|publisher=The Review of Economic Studies, Vol. 9, No. 1 }}</ref><ref>{{cite web |url=http://cepa.newschool.edu/het/essays/paretian/paretosocial.htm#scitovsky |title=The Paretian System: Scitovsky Reversals and the Double Criteria |accessdate= |last=Fonseca |first=G. L. |work= |publisher= |date= |deadurl=yes |archiveurl=https://web.archive.org/web/20060215000915/http://cepa.newschool.edu/het/essays/paretian/paretosocial.htm#scitovsky |archivedate=15 February 2006 |df=dmy-all }}</ref>\n\n==See also==\n* [[Compensation principle]]\n* [[Pareto efficiency]]\n* [[Scitovsky paradox]]\n\n==References==\n{{reflist|30em}}\n\n==Further reading==\n*{{cite journal |last=Hicks |first=John |authorlink= |author2= |year=1939 |month= |title=The Foundations of Welfare Economics |journal=Economic Journal |volume=49 |issue=196 |pages=696‚Äì712 |doi=10.2307/2225023 |jstor= 2225023|publisher=The Economic Journal, Vol. 49, No. 196 }}\n*{{cite journal |last=Kaldor |first=Nicholas |authorlink= |author2= |year=1939 |month= |title=Welfare Propositions in Economics and Interpersonal Comparisons of Utility |journal=Economic Journal |volume=49 |issue=195 |pages=549‚Äì552  |jstor= 2224835|doi=10.2307/2224835 |publisher=The Economic Journal, Vol. 49, No. 195 }}\n*{{cite book |title=Economic Analysis of Law |last=Posner |first=Richard A. |authorlink= |edition=Seventh |year=2007 |publisher=Wolters Kluwer |location=Austin, TX |isbn=0-7355-6354-3 |pages= }}\n\n== External links==\n{{Wikiquote-inline}}\n\n{{DEFAULTSORT:Kaldor-Hicks efficiency}}\n[[Category:Law and economics]]\n[[Category:Welfare economics]]\n[[Category:Pareto efficiency]]"
    },
    {
      "title": "Stochastic optimization",
      "url": "https://en.wikipedia.org/wiki/Stochastic_optimization",
      "text": "{{about|[[iterative method]]s|the modeling (and optimization) of decisions under uncertainty|stochastic programming|the context of control theory|stochastic control}}\n'''Stochastic optimization''' ('''SO''') methods are [[optimization (mathematics)|optimization]] [[iterative method|method]]s that generate and use [[random variable]]s. For stochastic problems, the random variables appear in the formulation of the optimization problem itself, which involves random [[objective function]]s or random constraints. Stochastic optimization methods also include methods with random iterates. Some stochastic optimization methods use random iterates to solve stochastic problems, combining both meanings of stochastic optimization.<ref name=spall2003>\n{{Cite book\n  | author = Spall, J. C.\n  | title = Introduction to Stochastic Search and Optimization\n  | year = 2003\n  | publisher = Wiley\n  | url = http://www.jhuapl.edu/ISSO\n  | isbn = 978-0-471-33052-3 \n}}</ref>\nStochastic optimization methods generalize [[deterministic system (mathematics)|deterministic]] methods for deterministic problems.\n\n==Methods for stochastic functions==\nPartly random input data arise in such areas as real-time estimation and control, simulation-based optimization where [[Monte Carlo method|Monte Carlo simulations]] are run as estimates of an actual system,<ref name=fu2002>\n{{cite journal\n  | author = Fu, M. C.\n  | title = Optimization for Simulation: Theory vs. Practice\n  | journal = INFORMS Journal on Computing\n  | year = 2002\n  | volume = 14\n  | pages = 192‚Äì227\n  | doi = 10.1287/ijoc.14.3.192.113\n  | issue = 3\n}}</ref> \n<ref>M.C. Campi and S. Garatti. ''The Exact Feasibility of Randomized Solutions of Uncertain Convex Programs.'' SIAM J. on Optimization, 19, no.3: 1211&ndash;1230, 2008.[http://epubs.siam.org/siopt/resource/1/sjope8/v19/i3/p1211_s1]</ref> and problems where there is experimental (random) error in the measurements of the criterion.  In such cases, knowledge that the function values are contaminated by random \"noise\" leads naturally to algorithms that use [[statistics|statistical inference]] tools to estimate the \"true\" values of the function and/or make statistically optimal decisions about the next steps.  Methods of this class include:\n* [[stochastic approximation]] (SA), by [[Herbert Robbins|Robbins]] and Monro (1951)<ref name=RM1951>\n{{cite journal\n  | author = Robbins, H.\n  |author2=Monro, S.\n   | title = A Stochastic Approximation Method\n  | journal = Annals of Mathematical Statistics\n  | year = 1951\n  | volume = 22\n  | pages = 400‚Äì407\n  | doi = 10.1214/aoms/1177729586\n  | issue = 3\n}}</ref>\n* [[stochastic gradient descent]] <!--inventor and reference needed-->\n* [[finite-difference stochastic approximation|finite-difference SA]] by Kiefer and Wolfowitz (1952)<ref name=KW1952>{{cite journal\n  | author = J. Kiefer\n  | authorlink = Jack Kiefer (mathematician)\n  |author2=J. Wolfowitz\n   | title = Stochastic Estimation of the Maximum of a Regression Function\n  | journal = Annals of Mathematical Statistics\n  | year = 1952\n  | volume = 23\n  | pages = 462‚Äì466\n  | doi = 10.1214/aoms/1177729392\n  | issue = 3\n}}</ref>\n* [[simultaneous perturbation stochastic approximation|simultaneous perturbation SA]] by Spall (1992)<ref name=spall1992>\n{{cite journal\n  | author = Spall, J. C.\n  | title = Multivariate Stochastic Approximation Using a Simultaneous Perturbation Gradient Approximation\n  | journal = IEEE Transactions on Automatic Control\n  | year = 1992\n  | volume = 37\n  | pages = 332‚Äì341\n  | url = http://www.jhuapl.edu/SPSA\n  | doi = 10.1109/9.119632\n  | issue = 3\n| citeseerx = 10.1.1.19.4562\n  }}</ref>\n* [[scenario optimization]]\n\n==Randomized search methods==\n{{See also|Metaheuristic}}\nOn the other hand, even when the [[data set]] consists of precise measurements, some methods  introduce randomness into the search-process to accelerate progress.<ref>Holger H. Hoos and Thomas St√ºtzle, ''[http://www.sls-book.net/ Stochastic Local Search: Foundations and Applications]'', [[Morgan Kaufmann]] / [[Elsevier]], 2004.</ref> Such randomness can also make the method less sensitive to modeling errors.  Further, the injected randomness may enable the method to escape a local optimum and eventually to approach a global optimum.   Indeed, this [[randomized algorithm|randomization]] principle is known to be a simple and effective way to obtain algorithms with ''almost certain'' good performance uniformly across many data sets, for many sorts of problems.  Stochastic optimization methods of this kind include:\n* [[simulated annealing]] by S. Kirkpatrick, C. D. Gelatt and M. P. Vecchi (1983)<ref name=kirk1983>\n{{cite journal\n  | author = S. Kirkpatrick |author2=C. D. Gelatt |author3=M. P. Vecchi\n  | title = Optimization by Simulated Annealing\n  | journal = Science\n  | year = 1983\n  | volume =  220\n  | pages = 671‚Äì680\n  | url = http://citeseer.ist.psu.edu/kirkpatrick83optimization.html\n  | doi = 10.1126/science.220.4598.671\n  | pmid = 17813860\n  | issue = 4598\n|bibcode = 1983Sci...220..671K |citeseerx=10.1.1.123.7607 }}</ref>\n* [[quantum annealing]]\n* [[Probability Collectives]] by D.H. Wolpert, S.R. Bieniawski and D.G. Rajnarayan (2011)<ref name=wolp2011>\n{{cite journal\n  | author = D.H. Wolpert |author2=S.R. Bieniawski |author3=D.G. Rajnarayan\n  | title = Probability Collectives in Optimization\n  | year = 2011\n  | booktitle =  Handbook of Statistics\n  | editor = C.R. Rao; V. Govindaraju\n  | url = http://www.santafe.edu/research/working-papers/abstract/f752fdb9c2b41e4e04947d7531421d61/\n}}</ref>\n* [[Reactive Search Optimization|reactive search optimization (RSO)]] by [[Roberto Battiti]], G. Tecchiolli (1994),<ref name=BattitiTecchiolli94>\n{{cite journal\n| last        =Battiti\n| first       =Roberto\n|author2=Gianpietro Tecchiolli\n| year        =1994\n| title       =The reactive tabu search\n| journal     =ORSA Journal on Computing\n| volume      =6\n| issue       =2\n| pages       =126‚Äì140\n| doi         =10.1287/ijoc.6.2.126\n| url         =http://rtm.science.unitn.it/~battiti/archive/TheReactiveTabuSearch.PDF\n}}</ref> recently reviewed in the reference book <ref name=BattitiBrunatoMascia2008>\n{{cite book\n|title=Reactive Search and Intelligent Optimization\n|last=Battiti\n|first=Roberto\n|authorlink=\n|author2=Mauro Brunato |author3=Franco Mascia\n |year=2008\n|publisher=[[Springer Verlag]]\n|location=\n|isbn=978-0-387-09623-0\n}}\n</ref>\n* [[cross-entropy method]] by Rubinstein and Kroese (2004)<ref name=rubkro>\n{{Cite book\n  | author = Rubinstein, R. Y.\n  |author2=Kroese, D. P.\n   | title = The Cross-Entropy Method\n  | year = 2004\n  | publisher = Springer-Verlag\n  | isbn = 978-0-387-21240-1\n}}</ref>\n* [[random search]] by [[Anatoly Zhigljavsky]] (1991)<ref name=zhigl1991>\n{{Cite book\n  | author = Zhigljavsky, A. A.\n  | title = Theory of Global Random Search\n  | year = 1991\n  | publisher = Kluwer Academic\n  | isbn = 978-0-7923-1122-5\n}}</ref>\n* Informational search <ref>{{cite paper\n|url= http://www.eng.tau.ac.il/~bengal/GTA.pdf\n|title=A Group-Testing Algorithm with Online Informational Learning\n|author= Kagan E. and Ben-Gal I. \n|publisher= IIE Transactions, 46:2, 164-184\n|year=2014\n}}</ref> \n* [[stochastic tunneling]]<ref name=wenz1999>\n{{cite journal\n  | author = W. Wenzel\n  |author2=K. Hamacher\n   | title = Stochastic tunneling approach for global optimization of complex potential energy landscapes\n  | journal = Phys. Rev. Lett.\n  | volume = 82\n  | year = 1999\n  | pages = 3003\n  | doi = 10.1103/PhysRevLett.82.3003\n| bibcode=1999PhRvL..82.3003W\n  | issue = 15\n|arxiv = physics/9903008 }}</ref>\n* [[parallel tempering]] a.k.a. replica exchange<ref name=mari1992>\n{{cite journal\n  | author = E. Marinari\n  |author2=G. Parisi\n   | title = Simulated tempering: A new monte carlo scheme\n  | journal = Europhys. Lett.\n  | volume = 19\n  | year = 1992\n  | pages = 451‚Äì458\n  | doi = 10.1209/0295-5075/19/6/002\n  | issue = 6\n|arxiv = hep-lat/9205018 |bibcode = 1992EL.....19..451M }}</ref>\n* [[stochastic hill climbing]]\n* [[Swarm intelligence|swarm algorithms]]\n* [[evolutionary algorithms]]\n** [[genetic algorithms]] by Holland (1975)<ref name=gold1991>\n{{Cite book\n |author      = Goldberg, D. E.\n |title       = Genetic Algorithms in Search, Optimization, and Machine Learning\n |year        = 1989\n |publisher   = Addison-Wesley\n |url         = http://www-illigal.ge.uiuc.edu\n |isbn        = 978-0-201-15767-3\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20060719133933/http://www-illigal.ge.uiuc.edu/\n |archivedate = 2006-07-19\n |df          = \n}}</ref>\n** [[evolution strategies]]\n* cascade object optimization & modification algorithm (2016)<ref>\n{{cite journal\n  | author = Tavridovich, S. A.\n  | title = COOMA: an object-oriented stochastic optimization algorithm\n  | journal = International Journal of Advanced Studies\n  | volume = 7\n  | year = 2017\n  | pages = 26‚Äì47\n  | issue = 2\n  | url = http://journal-s.org/index.php/ijas/article/view/10121/pdf\n  | doi=10.12731/2227-930x-2017-2-26-47\n}}</ref>\n\nIn contrast, some authors have argued that randomization can only improve a deterministic algorithm if the deterministic algorithm was poorly designed in the first place.<ref>http://lesswrong.com/lw/vp/worse_than_random/</ref> \n[[Fred W. Glover]] <ref>\n{{cite journal\n  | author = Glover, F.\n  | year = 2007\n  | title = Tabu search‚Äîuncharted domains\n  | journal = Annals of Operations Research\n  | volume = 149\n  | pages = 89‚Äì98\n  | doi=10.1007/s10479-006-0113-9\n| citeseerx = 10.1.1.417.8223\n  }}</ref> argues that reliance on random elements may prevent the development of more intelligent and better deterministic components. The way in which results of stochastic optimization algorithms are usually presented (e.g., presenting only the average, or even the best, out of N runs without any mention of the spread), may also result in a positive bias towards randomness.\n\n==See also==\n* [[Global optimization]]\n* [[Machine learning]]\n* [[Scenario optimization]]\n* [[Gaussian process]]\n* [[State Space Model]]\n* [[Model predictive control]]\n* [[Nonlinear programming]]\n* [[Entropic value at risk]]\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n*Michalewicz, Z. and Fogel, D. B. (2000), ''[https://books.google.com/books?id=MpKqCAAAQBAJ&printsec=frontcover#v=onepage&q=%22stochastic%22&f=false How to Solve It: Modern Heuristics]'', Springer-Verlag, New York.\n* \"[https://arxiv.org/pdf/1709.09840.pdf PSA: A novel optimization algorithm based on survival rules of porcellio scaber]\", Y. Zhang and S. Li\n\n==External links==\n* [http://stoprog.org/ COSP]\n\n{{DEFAULTSORT:Stochastic Optimization}}\n[[Category:Monte Carlo methods]]\n[[Category:Stochastic optimization| ]]"
    },
    {
      "title": "Bayesian optimization",
      "url": "https://en.wikipedia.org/wiki/Bayesian_optimization",
      "text": "'''Bayesian optimization''' is a [[sequential analysis|sequential design]] strategy\nfor [[global optimization]] of [[Black box|black-box]] functions<ref>Jonas Mockus (2012). [https://books.google.com/books?id=VuKoCAAAQBAJ&printsec=frontcover#v=onepage&q=%22global%20optimization%22&f=false Bayesian approach to global optimization: theory and applications]. Kluwer Academic.</ref> that [[Derivative-free optimization|doesn't require derivatives]].\n\n==History==\nThe term is generally attributed to Jonas Mockus and is coined in his work from a series of publications on global optimization in the 1970s and 1980s.<ref>Jonas Mockus: [https://link.springer.com/content/pdf/10.1007/3-540-07165-2_55.pdf On Bayesian Methods for Seeking the Extremum]. Optimization Techniques 1974: 400-404</ref><ref>Jonas Mockus: On Bayesian Methods for Seeking the Extremum and their Application. IFIP Congress 1977: 195-200</ref><ref>J. Mockus, Bayesian Approach to Global Optimization. Kluwer Academic Publishers, Dordrecht, 1989</ref>\n\n==Strategy==\n\nSince the objective function is unknown, the Bayesian strategy is to treat it as a random function and place a [[Prior distribution|prior]] over it.\nThe prior captures beliefs about the behaviour of the function. After gathering the function evaluations, which are treated as data, the prior is updated to form the [[posterior distribution]] over the objective function. The posterior distribution, in turn, is used to construct an acquisition function (often also referred to as infill sampling criteria) that determines the next query point.\n\n==Examples==\nExamples of acquisition functions include probability of improvement, expected improvement, Bayesian expected losses, upper confidence bounds (UCB), [[Thompson sampling]] and hybrids of these.<ref>Matthew W. Hoffman, Eric Brochu, [[Nando de Freitas]]: Portfolio Allocation for Bayesian Optimization. Uncertainty in Artificial Intelligence (UAI): 327‚Äì336 (2011)</ref> They all trade-off exploration and exploitation so as to minimize the number of function queries. As such, Bayesian optimization is well suited for functions that are expensive to evaluate.\n\n==Solution methods==\nThe maximum of the acquisition function is typically found by resorting to discretization or by means of an auxiliary optimizer.\n\n==Applications==\nThe approach has been applied to solve a wide range of problems,<ref>Eric Brochu, Vlad M. Cora, Nando de Freitas: [https://arxiv.org/pdf/1012.2599.pdf?bcsi_scan_dd0fad490e5fad80=fwQqmV5CfHDAMm8dFLewPK+h1WGiAAAAkj1aUQ%3D%3D&bcsi_scan_filename=1012.2599.pdf&utm_content=buffered388&utm_medium=social&utm_source=plus.google.com&utm_campaign=buffer A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning]. CoRR abs/1012.2599 (2010)</ref> including learning to rank,<ref>Eric Brochu, Nando de Freitas, Abhijeet Ghosh: [http://papers.nips.cc/paper/3219-active-preference-learning-with-discrete-choice-data.pdf Active Preference Learning with Discrete Choice Data]. NIPS 2007</ref> interactive animation,<ref>Eric Brochu, Tyson Brochu, Nando de Freitas: [http://haikufactory.com/files/sca2010.pdf A Bayesian Interactive Optimization Approach to Procedural Animation Design]. Symposium on Computer Animation 2010: 103‚Äì112</ref> [[robotics]],<ref>Daniel J. Lizotte, Tao Wang, Michael H. Bowling, Dale Schuurmans: [https://www.aaai.org/Papers/IJCAI/2007/IJCAI07-152.pdf Automatic Gait Optimization with Gaussian Process Regression]. IJCAI 2007: 944‚Äì949</ref><ref>Ruben Martinez-Cantin, Nando de Freitas, Eric Brochu, Jose Castellanos and Arnaud Doucet. [https://link.springer.com/article/10.1007%2Fs10514-009-9130-2# A Bayesian exploration-exploitation approach for optimal online sensing and planning with a visually guided mobile robot]. Autonomous Robots. Volume 27, Issue 2, pp 93‚Äì103 (2009)</ref><ref>Scott Kuindersma, Roderic Grupen, and Andrew Barto. [http://ijr.sagepub.com/content/32/7/806.abstract# Variable Risk Control via Stochastic Optimization]. International Journal of Robotics Research, volume 32, number 7, pp 806‚Äì825 (2013)</ref><ref>    Roberto Calandra, Andr√© Seyfarth, Jan Peters, and Marc P. Deisenroth [https://link.springer.com/article/10.1007%2Fs10472-015-9463-9 Bayesian optimization for learning gaits under uncertainty]. Ann. Math. Artif. Intell. Volume 76, Issue 1, pp 5-23 (2016) DOI:10.1007/s10472-015-9463-9</ref> [[sensor networks]],<ref>Niranjan Srinivas, Andreas Krause, Sham M. Kakade, Matthias W. Seeger: [https://infoscience.epfl.ch/record/177246/files/srinivas_ieeeit2012.pdf Information-Theoretic Regret Bounds for Gaussian Process Optimization in the Bandit Setting]. IEEE Transactions on Information Theory 58(5):3250‚Äì3265 (2012)</ref><ref>Roman Garnett, Michael A. Osborne, Stephen J. Roberts: [http://www.academia.edu/download/30681076/ipsn673-garnett.pdf Bayesian optimization for sensor set selection]. IPSN 2010: 209‚Äì219</ref> automatic algorithm configuration,<ref>Frank Hutter, Holger Hoos, and Kevin Leyton-Brown (2011). [http://www.cs.ubc.ca/labs/beta/Projects/SMAC/papers/11-LION5-SMAC.pdf Sequential model-based optimization for general algorithm configuration], Learning and Intelligent Optimization</ref> automatic [[machine learning]] toolboxes,<ref>J. Bergstra, D. Yamins, D. D. Cox (2013).\n[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.704.3494&rep=rep1&type=pdf Hyperopt: A Python Library for Optimizing the Hyperparameters of Machine Learning Algorithms].\nProc. SciPy 2013.</ref><ref>Chris Thornton, Frank Hutter, Holger H. Hoos, Kevin Leyton-Brown: [https://arxiv.org/pdf/1208.3719 Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms]. KDD 2013: 847‚Äì855</ref><ref>Jasper Snoek, Hugo Larochelle and Ryan Prescott Adams. [https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf Practical Bayesian Optimization of Machine Learning Algorithms]. Neural Information Processing Systems, 2012</ref> [[reinforcement learning]], planning, visual attention, architecture configuration in [[deep learning]], static program analysis and experimental [[particle physics]].<ref>Philip Ilten, Mike Williams, Yunjie Yang. [https://arxiv.org/pdf/1610.08328 Event generator tuning using Bayesian optimization]. 2017 JINST 12 P04028. DOI: 10.1088/1748-0221/12/04/P04028</ref>\n\n==See also==\n* [[Multi-armed bandit]]\n* [[Thompson sampling]]\n* [[Global optimization]]\n* [[Bayesian experimental design]]\n\n==References==\n{{reflist}}\n\n==External links==\n* [https://botorch.org/ BoTorch], A modular and modern PyTorch-based open-source library for Bayesian optimization research with support for [http://gpytorch.ai GPyTorch].\n* [https://sheffieldml.github.io/GPyOpt/ GPyOpt], Python open-source library for Bayesian Optimization based on [https://github.com/SheffieldML/GPy GPy].\n* [https://rmcantin.bitbucket.io/html/ Bayesopt], an efficient implementation in C/C++ with support for Python, Matlab and Octave.\n* [https://github.com/HIPS/Spearmint Spearmint], a Python implementation focused on parallel and cluster computing.\n* [https://github.com/hyperopt/hyperopt Hyperopt], a Python implementation for hyperparameter optimization.\n* [http://www.cs.ubc.ca/labs/beta/Projects/SMAC/ SMAC], a Java implementation of random-forest-based Bayesian optimization for general algorithm configuration.\n* [https://github.com/mwhoffman/pybo pybo], a Python implementation of modular Bayesian optimization.\n* [https://bitbucket.org/mlcircus/bayesopt.m Bayesopt.m], a Matlab implementation of Bayesian optimization with or without constraints.\n* [https://github.com/yelp/MOE MOE] MOE is a Python/C++/CUDA implementation of Bayesian Global Optimization using Gaussian Processes.\n* [https://sigopt.com/ SigOpt] SigOpt offers Bayesian Global Optimization as a SaaS service focused on enterprise use cases.\n* [https://github.com/mila-udem/metaopt Metaopt], a Python implementation of hyperparameter optimization focused on parallel and cluster computing.\n* [https://mindfoundry.ai/optaas Mind Foundry] OPTaaS offers Bayesian Global Optimization via web-services with flexible parameter constraints.\n[[Category:Sequential methods]]\n[[Category:Sequential experiments]]\n[[Category:Stochastic optimization]]\n[[Category:Machine learning]]"
    },
    {
      "title": "BRST algorithm",
      "url": "https://en.wikipedia.org/wiki/BRST_algorithm",
      "text": "'''Boender-Rinnooy-Stougie-Timmer''' algorithm (BRST) is an optimization algorithm suitable for finding [[global optimum]] of [[black box]] functions. In their paper Boender ''et al.'' <ref name=\"boender\">{{cite journal | author1 = Boender, C.G.E.|author2= A.H.G. Rinnooy Kan|author3= L. Strougie|author4= G.T. Timmer | year = 1982 | title = A stochastic method for global optimization | journal = Mathematical Programming | volume = 22 | pages = 125‚Äì140 | doi = 10.1007/BF01581033}}</ref> describe their method as a stochastic method involving a combination of sampling, clustering and local search, terminating with a range of confidence intervals on the value of the global minimum.\n\nThe algorithm of Boender ''et al.'' has been modified by Timmer.<ref name=\"timmer\">{{cite journal | last = Timmer | first = G.T. | title = Global optimization: A stochastic approach | type =  Ph.D. Thesis | publisher = Erasmus University Rotterdam | year = 1984 }}</ref> Timmer considered several clustering methods. Based on experiments a method named \"multi level single linkage\" was deemed most accurate.\n\nCsendes' algorithms <ref name=\"csendes\">{{cite journal | last = Csendes | first = T. | title = Nonlinear parameter estimation by global optimization‚ÄîEfficiency and reliability | journal = Acta Cybernetica | volume = 8 | issue = 4 | year = 1988 | pages = 361‚Äì370 | url = http://www.inf.u-szeged.hu/actacybernetica/edb/vol08n4/Csendes_1988_ActaCybernetica.xml }}</ref> are implementations of the algorithm of [Boender ''et al.'']<ref name=\"boender\" /> and originated the [[public domain software]] product [[GLOBAL]]. The local algorithms used are a random direction, linear search algorithm also used by T√∂rn, and a quasi‚ÄîNewton algorithm not using the derivative of the function. The results show the dependence of the result on the auxiliary local algorithm used.\n\n== Background ==\nExtending the class of functions to include multimodal functions makes the global optimization problem unsolvable in general. In order to be solvable some smoothness condition on the function in addition to continuity must be known.\n\nThe existence of several local minima and unsolvability in general are important characteristics of global optimization. Unsolvability here means that a solution cannot be guaranteed in a finite number of steps.\nThere are two ways to deal with the unsolvability problem. First, \"a priori\" conditions on f and A are posed which turns the problem into a solvable one or at least makes it possible to tell for sure that a solution has been found. This restricts the function class that is considered.\nThe second approach which allows a larger class of objective functions to be considered is to give up the solvability requirement and only try to obtain an estimate of the global minimum. In this \"probabilistic\" approach it would be desirable also to obtain some results on the goodness of an obtained estimate. Some of the solvable problems may fall in this class because the number of steps required for a guaranteed solution might be prohibitively large.\n\nWhen relaxing the requirement on solvability it seems rational to require that the probability that a solution is obtained approaches 1 if the procedure is allowed to continue forever. An obvious probabilistic global search procedure is to use a local algorithm starting from several points distributed over the whole optimization region. This procedure is named \"Multistart\". Multistart is certainly one of the earliest global procedures used. It has even been used in local optimization for increasing the confidence in the obtained solution. One drawback of Multistart is that when many starting points are used the same minimum will eventually be determined several times. In order to improve the efficiency of Multistart this should be avoided.\n\nClustering methods are used to avoid this repeated determination of local minima. This is realized in three steps which may be iteratively used. The three steps are:\n\n* (a) Sample points in the region of interest.\n* (b) Transform the sample to obtain points grouped around the local minima.\n* (c) Use a clustering technique to recognize these groups (i.e. neighbourhoods of the local minima).\n\nIf the procedure employing these steps is successful then starting a single local optimization from each cluster would determine the local minima and thus also the global minimum. The advantage in using this approach is that the work spared by computing each minimum just once can be spent on computations in (a) and (b), which will increase the probability that the global minimum will be found.\n\nBeing a [[clustering method]], their effectiveness is higher for low-dimensional problems and become less effective for problems having a few hundred variables.\n\n== References ==\n<references />\n\n==External links==\n*[http://www.abo.fi/~atorn/Globopt.html http://www.abo.fi/~atorn/Globopt.html] With the author's permission, text has been verbatim copied.\n*[http://www.mat.univie.ac.at/~vpk/math/gopt_eng.html Janka] Compares various global optimization algorithms, of which BRST shows superior performance.\n*[http://www.mat.univie.ac.at/~vpk/math/dix_sze_eng.html Janka] Presents the number of function-evaluations performed on the testset of Dixon-Szeg√∂. Along with the [[MCS algorithm]], the BRST requires the lowest number of evaluations.\n\n[[Category:Stochastic optimization]]"
    },
    {
      "title": "Correlation gap",
      "url": "https://en.wikipedia.org/wiki/Correlation_gap",
      "text": "In [[stochastic programming]], the '''correlation gap''' is the worst-case ratio between the cost when the random variables are [[correlated]] to the cost when the random variables are [[Independence (probability theory)|independent]].<ref name=adsy10>{{cite conference|doi=10.1137/1.9781611973075.88 |chapter=Correlation Robust Stochastic Optimization |title=Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms |pages=1087 |year=2010 |last1=Agrawal |first1=Shipra |last2=Ding |first2=Yichuan |last3=Saberi |first3=Amin |last4=Ye |first4=Yinyu |isbn=978-0-89871-701-3 |arxiv=0902.1792 }}</ref>\n\nAs an example,<ref name=adsy10/>{{rp|6}} consider the following optimization problem. A teacher wants to know whether to come to class or not. There are ''n'' potential students. For each student, there is a probability of 1/''n'' that the student will attend the class. If at least one student attends, then the teacher must come and his cost is&nbsp;1. If no students attend, then the teacher can stay at home and his cost is&nbsp;0. The goal of the teacher is to minimize his cost. This is a stochastic-programming problem, because the constraints are not known in advance ‚Äì only their probabilities are known. Now, there are two cases regarding the correlation between the students:\n* Case #1: the students are uncorrelated: each student decides whether to come to class or not by tossing a coin with probability <math>1/n</math>, independently of the others. The expected cost in this case is <math>1-(1-1/n)^n \\approx 1-1/e</math>.{{clarification needed|reason=see talk page--where did this come from?|date=July 2016}}\n* Case #2: the students are correlated: one student is selected at random and comes to class, while the others stay at home. Note that the probability of each student to come is still <math>1/n</math>. However, now the cost is&nbsp;1.\nThe correlation gap is the cost in case #2 divided by the cost in case #1, which is <math>e/(e-1)</math>.\n\n<ref name=adsy10/> prove that the correlation gap is bounded in several cases. For example, when the cost function is a [[submodular set function]] (as in the above example), the correlation gap is at most <math>e/(e-1)</math> (so the above example is a worst-case).\n\nAn upper bound on the correlation gap implies an upper bound on the loss that results from ignoring the correlation. For example, suppose we have a stochastic programming problem with a submodular cost function. We know the marginal probabilities of the variables, but we do not know whether they are correlated or not. If we just ignore the correlation and solve the problem as if the variables are independent, the resulting solution is a <math>(e-1)/e</math>-approximation to the optimal solution.\n\n== Applications ==\n\nThe correlation gap was used to bound the loss of revenue when using a [[Bayesian-optimal pricing]] instead of a [[Bayesian-optimal auction]].<ref name=y10>{{cite conference|doi=10.1137/1.9781611973082.56|chapter=Mechanism Design via Correlation Gap|title=Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms|pages=710|year=2011|last1=Yan|first1=Qiqi|isbn=978-0-89871-993-2|arxiv=1008.1843}}</ref>\n\n== See also ==\n* [[Robust optimization]]\n* [[Info-gap decision theory]]\n\n== References ==\n{{reflist}}\n[[Category:Stochastic optimization]]"
    },
    {
      "title": "Estimation of distribution algorithm",
      "url": "https://en.wikipedia.org/wiki/Estimation_of_distribution_algorithm",
      "text": "[[Image:Eda mono-variant gauss iterations.svg|thumb|350px|Estimation of distribution algorithm. For each iteration ''i'', a random draw is performed for a population ''P'' in a distribution ''PDu''. The distribution parameters ''PDe'' are then estimated using the selected points ''PS''. The illustrated example optimizes a continuous objective function ''f(X)'' with a unique optimum ''O''. The sampling (following a normal distribution ''N'') concentrates around the optimum as one goes along unwinding algorithm.]]\n\n'''''Estimation of distribution algorithms''''' ('''EDAs'''), sometimes called '''''probabilistic model-building genetic algorithms''''' (PMBGAs),<ref>{{Citation|last=Pelikan|first=Martin|date=2005-02-21|pages=13‚Äì30|publisher=Springer Berlin Heidelberg|language=en|doi=10.1007/978-3-540-32373-0_2|isbn=9783540237747|title=Hierarchical Bayesian Optimization Algorithm|volume=170|series=Studies in Fuzziness and Soft Computing|chapter=Probabilistic Model-Building Genetic Algorithms}}</ref> are [[stochastic optimization]] methods that guide the search for the optimum by building and sampling explicit probabilistic models of promising candidate solutions. Optimization is viewed as a series of incremental updates of a probabilistic model, starting with the model encoding an uninformative prior over admissible solutions and ending with the model that generates only the global optima.<ref>{{cite book|author1=Pedro Larra√±aga|author2=Jose A. Lozano|title=Estimation of Distribution Algorithms a New Tool for Evolutionary Computation|date=2002|publisher=Springer US|location=Boston, MA|isbn=978-1-4615-1539-5}}</ref><ref>{{cite book|author1=Jose A. Lozano|author2=Larra√±aga, P.|author3=Inza, I.|author4=Bengoetxea, E.|title=Towards a new evolutionary computation advances in the estimation of distribution algorithms|date=2006|publisher=Springer|location=Berlin|isbn=978-3-540-32494-2}}</ref><ref>{{cite book|last1=Pelikan|first1=Martin|last2=Sastry|first2=Kumara|last3=Cant√∫-Paz|first3=Erick|title=Scalable optimization via probabilistic modeling : from algorithms to applications ; with 26 tables|date=2006|publisher=Springer|location=Berlin|isbn=978-3540349532}}</ref>\n\nEDAs belong to the class of [[evolutionary algorithms]]. The main difference between EDAs and most conventional evolutionary algorithms is that evolutionary algorithms generate new candidate solutions using an ''implicit'' distribution defined by one or more variation operators, whereas EDAs use an ''explicit'' probability distribution encoded by a [[Bayesian network]], a [[multivariate normal distribution]], or another model class. Similarly as other evolutionary algorithms, EDAs can be used to solve optimization problems defined over a number of representations from vectors to [[LISP]] style S expressions, and the quality of candidate solutions is often evaluated using one or more objective functions.\n\nThe general procedure of an EDA is outlined in the following:\n<source lang=python>\nt = 0\ninitialize model M(0) to represent uniform distribution over admissible solutions\nwhile (termination criteria not met)\n    P = generate N>0 candidate solutions by sampling M(t)\n    F = evaluate all candidate solutions in P\n    M(t+1) = adjust_model(P,F,M(t))\n    t = t + 1\n</source>\n\nUsing explicit probabilistic models in optimization allowed EDAs to feasibly solve optimization problems that were notoriously difficult for most conventional evolutionary algorithms and traditional optimization techniques, such as problems with high levels of [[epistasis]]{{Citation needed|date=September 2017}}.  Nonetheless, the advantage of EDAs is also that these algorithms provide an optimization practitioner with a series of probabilistic models that reveal a lot of information about the problem being solved. This information can in turn be used to design problem-specific neighborhood operators for local search, to bias future runs of EDAs on a similar problem, or to create an efficient computational model of the problem.\n\nFor example, if the population is represented by bit strings of length 4, the EDA can represent the population of promising solution using a single vector of four probabilities (p1, p2, p3, p4) where each component of p defines the probability of that position being a 1. Using this probability vector it is possible to create an arbitrary number of candidate solutions.\n\n==Estimation of distribution algorithms (EDAs)==\nThis section describes the models built by some well known EDAs of different levels of complexity. It is always assumed a population <math>P(t)</math> at the generation <math>t</math>, a selection operator <math>S</math>, a model-building operator <math>\\alpha</math> and a sampling operator <math>\\beta</math>.\n\n==Univariate factorizations==\nThe most simple EDAs assume that decision variables are independent, i.e. <math>p(X_1,X_2) = p(X_1)\\cdot p(X_2)</math>. Therefore, univariate EDAs rely only on univariate statistics and multivariate distributions must be factorized as the product of <math>N</math> univariate probability distributions,\n\n<math>\nD_\\text{Univariate} := p(X_1,\\dots,X_N) = \\prod_{i=1}^N p(X_i).\n</math>\n\nSuch factorizations are used in many different EDAs, next we describe some of them.\n\n===Univariate marginal distribution algorithm (UMDA)===\nThe UMDA<ref>{{cite journal|last1=M√ºhlenbein|first1=Heinz|title=The Equation for Response to Selection and Its Use for Prediction|journal=Evol. Computation|date=1 September 1997|volume=5|issue=3|pages=303‚Äì346|doi=10.1162/evco.1997.5.3.303|url=http://dl.acm.org/citation.cfm?id=1326756|issn=1063-6560}}</ref> is a simple EDA that uses an operator <math>\\alpha_{UMDA}</math> to estimate marginal probabilities from a selected population <math>S(P(t))</math>. By assuming <math>S(P(t))</math> contain <math>\\lambda</math> elements, <math>\\alpha_{UMDA}</math> produces probabilities:\n\n<math>\np_{t+1}(X_i) = \\dfrac{1}{\\lambda} \\sum_{x\\in S(P(t))} x_i,~\\forall i\\in 1,2,\\dots,N.\n</math>\n\nEvery UMDA step can be described as follows\n\n<math>\nD(t+1) = \\alpha_\\text{UMDA} \\circ S \\circ \\beta_{\\lambda}(D(t)).\n</math>\n\n===[[Population-based incremental learning]] (PBIL)===\nThe PBIL,<ref>{{cite journal|last1=Baluja|first1=Shummet|title=Population-Based Incremental Learning: A Method for Integrating Genetic Search Based Function Optimization and Competitive Learning|date=1 January 1994|url=http://dl.acm.org/citation.cfm?id=865123|publisher=Carnegie Mellon University}}</ref> represents the population implicitly by its model, from which it samples new solutions and updates the model. At each generation, <math>\\mu</math> individuals are sampled and <math>\\lambda\\leq \\mu</math> are selected. Such individuals are then used to update the model as follows\n\n<math>\np_{t+1}(X_i) = (1- \\gamma) p_{t}(X_i) + (\\gamma/\\lambda) \\sum_{x\\in S(P(t))} x_i,~\\forall i\\in 1,2,\\dots,N,\n</math>\n\nwhere <math>\\gamma\\in(0,1]</math> is a parameter defining the [[learning rate]], a small value determines that the previous model <math>p_t(X_i)</math> should be only slightly modified by the new solutions sampled. PBIL can be described as\n\n<math>\nD(t+1) = \\alpha_\\text{PIBIL} \\circ S \\circ \\beta_\\mu(D(t))\n</math>\n\n===Compact genetic algorithm (cGA)===\nThe CGA,<ref>{{cite journal|last1=Harik|first1=G.R.|last2=Lobo|first2=F.G.|last3=Goldberg|first3=D.E.|title=The compact genetic algorithm|journal=IEEE Transactions on Evolutionary Computation|date=1999|volume=3|issue=4|pages=287‚Äì297|doi=10.1109/4235.797971}}</ref> also relies on the implicit populations defined by univariate distributions. At each generation <math>t</math>, two individuals <math>x,y</math> are sampled, <math>P(t)=\\beta_2(D(t))</math>. The population <math>P(t)</math> is then sorted in decreasing order of fitness, <math>S_{\\text{Sort}(f)}(P(t))</math>, with <math>u</math> being the best and <math>v</math> being the worst solution. The CGA estimates univariate probabilities as follows\n\n<math>\np_{t+1}(X_i) = p_t(X_i) + \\gamma (u_i - v_i), \\quad\\forall i\\in 1,2,\\dots,N,\n</math>\n\nwhere, <math>\\gamma\\in(0,1]</math> is a constant defining the [[learning rate]], usually set to <math>\\gamma=1/N</math>. The CGA can be defined as\n\n<math>\nD(t+1) = \\alpha_\\text{CGA} \\circ S_{\\text{Sort}(f)} \\circ \\beta_2(D(t))\n</math>\n\n==Bivariate factorizations==\nAlthough univariate models can be computed efficiently, in many cases they are not representative enough to provide better performance than GAs. In order to overcome such a drawback, the use of bivariate factorizations was proposed in the EDA community, in which dependencies between pairs of variables could be modeled. A bivariate factorization can be defined as follows, where <math>\\pi_i</math> contains a possible variable dependent to <math>X_i</math>, i.e. <math>|\\pi_i|=1</math>.\n\n<math>\nD_\\text{Bivariate} := p(X_1,\\dots,X_N) = \\prod_{i=1}^{N} p(X_i|\\pi_i).\n</math>\n\nBivariate and multivariate distributions are usually represented as Probabilistic [[Graphical Models]] (graphs), in which edges denote statistical dependencies (or conditional probabilities) and vertices denote variables. To learn the structure of a PGM from data linkage-learning is employed.\n\n===Mutual information maximizing input clustering (MIMIC)===\nThe MIMIC<ref>{{cite journal|last1=Bonet|first1=Jeremy S. De|last2=Isbell|first2=Charles L.|last3=Viola|first3=Paul|title=MIMIC: Finding Optima by Estimating Probability Densities|journal=Advances in Neural Information Processing Systems|date=1 January 1996|pages=424|citeseerx=10.1.1.47.6497}}</ref> factorizes the [[joint probability distribution]] in a chain-like model representing successive dependencies between variables. It finds a permutation of the decision variables, <math>r : i \\mapsto j</math>, such that <math>x_{r(1)}x_{r(2)},\\dots,x_{r(N)}</math> minimizes the [[Kullback-Leibler divergence]] in relation to the true probability distribution, i.e. <math>\\pi_{r(i+1)} = \\{X_{r(i)}\\}</math>. MIMIC models a distribution\n\n<math>\np_{t+1}(X_1,\\dots,X_N) = p_t(X_{r(N)}) \\prod_{i=1}^{N-1} p_t(X_{r(i)}|X_{r(i+1)}).\n</math>\n\nNew solutions are sampled from the leftmost to the rightmost variable, the first is generated independently and the others according to conditional probabilities. Since the estimated distribution must be recomputed each generation, MIMIC uses concrete populations in the following way\n\n<math>\nP(t+1) = \\beta_\\mu \\circ \\alpha_\\text{MIMIC} \\circ S(P(t)).\n</math>\n\n===Bivariate marginal distribution algorithm (BMDA)===\nThe BMDA<ref>{{cite book|last1=Pelikan|first1=Martin|last2=Muehlenbein|first2=Heinz|title=The Bivariate Marginal Distribution Algorithm|journal=Advances in Soft Computing|date=1 January 1999|pages=521‚Äì535|doi=10.1007/978-1-4471-0819-1_39|isbn=978-1-85233-062-0|citeseerx=10.1.1.55.1151}}</ref> factorizes the joint probability distribution in bivariate distributions. First, a randomly chosen variable is added as a node in a graph, the most dependent variable to one of those in the graph is chosen among those not yet in the graph, this procedure is repeated until no remaining variable depends on any variable in the graph (verified according to a threshold value).\n\nThe resulting model is a forest with multiple trees rooted at nodes <math>\\Upsilon_t</math>. Considering <math>I_t</math> the non-root variables, BMDA estimates a factorized distribution in which the root variables can be sampled independently, whereas all the others must be conditioned to the parent variable <math>\\pi_i</math>.\n\n<math>\np_{t+1}(X_1,\\dots,X_N) = \\prod_{X_i\\in \\Upsilon_t} p_t(X_i) \\cdot \\prod_{X_i\\in I_t} p_t(X_i | \\pi_i).\n</math>\n\nEach step of BMDA is defined as follows\n\n<math>\nP(t+1) = \\beta_\\mu \\circ \\alpha_\\text{BMDA} \\circ S(P(t)).\n</math>\n\n==Multivariate factorizations==\nThe next stage of EDAs development was the use of multivariate factorizations. In this case, the joint probability distribution is usually factorized in a number of components of limited size <math>|\\pi_i| \\leq K,~\\forall i\\in 1,2,\\dots,N</math>.\n\n<math>\np(X_1,\\dots,X_N) = \\prod_{i=1}^{N} p(X_i|\\pi_i)\n</math>\n\nThe learning of PGMs encoding multivariate distributions is a computationally expensive task, therefore, it is usual for EDAs to estimate multivariate statistics from bivariate statistics. Such relaxation allows PGM to be built in polynomial time in <math>N</math>; however, it also limits the generality of such EDAs.\n\n===Extended compact genetic algorithm (eCGA)===\nThe ECGA<ref>{{cite book|last1=Harik|first1=Georges Raif|title=Learning Gene Linkage to Efficiently Solve Problems of Bounded Difficulty Using Genetic Algorithms|publisher=University of Michigan|url=http://dl.acm.org/citation.cfm?id=269517|year=1997}}</ref> was one of the first EDA to employ multivariate factorizations, in which high-order dependencies among decision variables can be modeled. Its approach factorizes the joint probability distribution in the product of multivariate marginal distributions. Assume <math>T_\\text{eCGA}=\\{\\tau_1,\\dots,\\tau_\\Psi\\}</math> is a set of subsets, in which every <math>\\tau\\in T_\\text{eCGA}</math> is a linkage set, containing <math>|\\tau|\\leq K</math> variables. The factorized joint probability distribution is represented as follows\n\n<math>\np(X_1,\\dots,X_N) = \\prod_{\\tau\\in T_\\text{eCGA}} p(\\tau).\n</math>\n\nThe ECGA popularized the term \"linkage-learning\" as denoting procedures that identify linkage sets. Its linkage-learning procedure relies on two measures: (1) the Model Complexity (MC) and (2) the Compressed Population Complexity (CPC). The MC quantifies the model representation size in terms of number of bits required to store all the marginal probabilities\n\n<math>\nMC = \\log_2 (\\lambda+1) \\sum_{\\tau\\in T_\\text{eCGA}} (2^{|\\tau|-1}),\n</math>\n\nThe CPC, on the other hand, quantifies the data compression in terms of entropy of the marginal distribution over all partitions, where <math>\\lambda</math> is the selected population size, <math>|\\tau|</math> is the number of decision variables in the linkage set <math>\\tau</math> and <math>H(\\tau)</math> is the joint entropy of the variables in <math>\\tau</math>\n\n<math>\nCPC = \\lambda \\sum_{\\tau\\in T_\\text{eCGA}} H(\\tau).\n</math>\n\nThe linkage-learning in ECGA works as follows: (1) Insert each variable in a cluster, (2) compute CCC = MC + CPC of the current linkage sets, (3) verify the increase on CCC provided by joining pairs of clusters, (4) effectively joins those clusters with highest CCC improvement. This procedure is repeated until no CCC improvements are possible and produces a linkage model <math>T_\\text{eCGA}</math>. The ECGA works with concrete populations, therefore, using the factorized distribution modeled by ECGA, it can be described as\n\n<math>\nP(t+1) = \\beta_\\mu \\circ \\alpha_\\text{eCGA} \\circ S(P(t))\n</math>\n\n===Bayesian optimization algorithm (BOA)===\nThe BOA<ref>{{cite journal|last1=Pelikan|first1=Martin|last2=Goldberg|first2=David E.|last3=Cantu-Paz|first3=Erick|title=BOA: The Bayesian Optimization Algorithm|date=1 January 1999|pages=525‚Äì532|publisher=Morgan Kaufmann|citeseerx=10.1.1.46.8131}}</ref><ref>{{cite book|last1=Pelikan|first1=Martin|title=Hierarchical Bayesian optimization algorithm : toward a new generation of evolutionary algorithms|date=2005|publisher=Springer|location=Berlin [u.a.]|isbn=978-3-540-23774-7|edition=1st}}</ref><ref>{{cite journal|last1=Wolpert|first1=David H.|last2=Rajnarayan|first2=Dev|title=Using Machine Learning to Improve Stochastic Optimization|journal=Proceedings of the 17th AAAI Conference on Late-Breaking Developments in the Field of Artificial Intelligence|date=1 January 2013|pages=146‚Äì148|url=http://dl.acm.org/citation.cfm?id=2908286.2908335|series=Aaaiws'13-17}}</ref> uses Bayesian networks to model and sample promising solutions. Bayesian networks are directed acyclic graphs, with nodes representing variables and edges representing conditional probabilities between pair of variables. The value of a variable <math>x_i</math> can be conditioned on a maximum of <math>K</math> other variables, defined in <math>\\pi_i</math>. BOA builds a PGM encoding a factorized joint distribution, in which the parameters of the network, i.e. the conditional probabilities, are estimated from the selected population using the maximum likelihood estimator.\n\n<math>\np(X_1,X_2,\\dots,X_N)=\\prod_{i=1}^{N}p(X_i|\\pi_{i}).\n</math>\n\nThe Bayesian network structure, on the other hand, must be built iteratively (linkage-learning). It starts with a network without edges and, at each step, adds the edge which better improves some scoring metric (e.g. Bayesian information criterion (BIC) or Bayesian-Dirichlet metric with likelihood equivalence (BDe)).<ref>{{cite journal|last1=Larra√±aga|first1=Pedro|last2=Karshenas|first2=Hossein|last3=Bielza|first3=Concha|last4=Santana|first4=Roberto|title=A review on probabilistic graphical models in evolutionary computation|journal=Journal of Heuristics|date=21 August 2012|volume=18|issue=5|pages=795‚Äì819|doi=10.1007/s10732-012-9208-4|url=http://oa.upm.es/15826/}}</ref> The scoring metric evaluates the network structure according to its accuracy in modeling the selected population. From the built network, BOA samples new promising solutions as follows: (1) it computes the ancestral ordering for each variable, each node being preceded by its parents; (2) each variable is sampled conditionally to its parents. Given such scenario, every BOA step can be defined as\n\n<math>\nP(t+1) = \\beta_\\mu \\circ \\alpha_\\text{BOA} \\circ S(P(t))\n</math>\n\n===Linkage-tree Genetic Algorithm (LTGA)===\nThe LTGA<ref>{{cite book|last1=Thierens|first1=Dirk|title=The Linkage Tree Genetic Algorithm|journal=Parallel Problem Solving from Nature, PPSN XI|date=11 September 2010|pages=264‚Äì273|doi=10.1007/978-3-642-15844-5_27|isbn=978-3-642-15843-8}}</ref> differs from most EDA in the sense it does not explicitly model a probability distribution but only a linkage model, called linkage-tree. A linkage <math>T</math> is a set of linkage sets with no probability distribution associated, therefore, there is no way to sample new solutions directly from <math>T</math>. The linkage model is a linkage-tree produced stored as a [[Family of sets]] (FOS).\n\n<math>\nT_\\text{LT}  = \\{ \\{x_1\\}, \\{x_2\\},\\{x_3\\},\\{x_4\\},\\{x_1,x_2\\},\\{x_3,x_4\\} \\}.\n</math>\n\nThe linkage-tree learning procedure is a [[hierarchical clustering]] algorithm, which work as follows. At each step the two ''closest'' clusters <math>i</math> and <math>j</math> are merged, this procedure repeats until only one cluster remains, each subtree is stored as a subset <math>\\tau\\in T_\\text{LT}</math>.\n\nThe LTGA uses <math>T_\\text{LT}</math> to guide an \"optimal mixing\" procedure which resembles a recombination operator but only accepts improving moves. We denote it as <math>R_\\text{LTGA}</math>, where the notation <math>x[\\tau]\\gets y[\\tau]</math> indicates the transfer of the genetic material indexed by <math>\\tau</math> from <math>y</math> to <math>x</math>.\n\n{{algorithm-begin|name=Gene-pool optimal mixing}}\n    Input: A family of subsets <math>T_\\text{LT}</math> and a population <math>P(t)</math>\n    Output: A population <math>P(t+1)</math>.\n    '''for each''' <math>x_i</math> '''in''' <math>P(t)</math> '''do'''\n        '''for each''' <math>\\tau</math> '''in''' <math>T_\\text{LT}</math> '''do'''\n            choose a random <math>x_j\\in P(t) : x_i\\neq x_j</math>\n            <math>f_{x_i}</math> := <math>f(x_i)</math>\n            <math>x_i[\\tau]</math>:= <math>x_j[\\tau]</math>\n            '''if''' <math>f(x_i) \\leq f_{x_i}</math> '''then'''\n                <math>x_i[\\tau]:= x_j[\\tau]</math> \n    '''return''' <math>P(t)</math>\n{{algorithm-end}}\n\nThe LTGA does not implement typical selection operators, instead, selection is performed during recombination. Similar ideas have been usually applied into local-search heuristics and, in this sense, the LTGA can be seen as an hybrid method. In summary, one step of the LTGA is defined as\n\n<math>\nP(t+1) = R_{\\text{LTGA}}(P(t)) \\circ \\alpha_\\text{LTGA} (P(t))\n</math>\n\n==Other==\n* Probability collectives (PC)<ref>{{cite journal|last1=WOLPERT|first1=DAVID H.|last2=STRAUSS|first2=CHARLIE E. M.|last3=RAJNARAYAN|first3=DEV|title=ADVANCES IN DISTRIBUTED OPTIMIZATION USING PROBABILITY COLLECTIVES|journal=Advances in Complex Systems|date=December 2006|volume=09|issue=4|pages=383‚Äì436|doi=10.1142/S0219525906000884|citeseerx=10.1.1.154.6395}}</ref><ref>{{cite journal|last1=Pelikan|first1=Martin|last2=Goldberg|first2=David E.|last3=Lobo|first3=Fernando G.|title=A Survey of Optimization by Building and Using Probabilistic Models|journal=Computational Optimization and Applications|date=2002|volume=21|issue=1|pages=5‚Äì20|doi=10.1023/A:1013500812258}}</ref>\n* Hill climbing with learning (HCwL)<ref>{{Cite journal|last=Rudlof|first=Stephan|last2=K√∂ppen|first2=Mario|date=1997|title=Stochastic Hill Climbing with Learning by Vectors of Normal Distributions|url=http://citeseerx.ist.psu.edu/viewdoc/similar?doi=10.1.1.19.3536&type=ab|language=en}}</ref>\n* Estimation of multivariate normal algorithm (EMNA){{Citation needed|date=June 2018}}\n* Estimation of Bayesian networks algorithm (EBNA){{Citation needed|date=June 2018}}\n* Stochastic hill climbing with learning by vectors of normal distributions (SHCLVND)<ref>{{Cite journal|last=Rudlof|first=Stephan|last2=K√∂ppen|first2=Mario|date=1997|title=Stochastic Hill Climbing with Learning by Vectors of Normal Distributions|pages=60‚Äì‚Äì70|citeseerx=10.1.1.19.3536}}</ref>\n* Real-coded PBIL{{Citation needed|date=June 2018}}\n* Selfish Gene Algorithm (SG)<ref>{{Cite book|last=Corno|first=Fulvio|last2=Reorda|first2=Matteo Sonza|last3=Squillero|first3=Giovanni|date=1998-02-27|title=The selfish gene algorithm: a new evolutionary optimization strategy|publisher=ACM|pages=349‚Äì355|doi=10.1145/330560.330838|isbn=978-0897919692}}</ref>\n* Compact Differential Evolution (cDE)<ref>{{Cite journal|last=Mininno|first=Ernesto|last2=Neri|first2=Ferrante|last3=Cupertino|first3=Francesco|last4=Naso|first4=David|date=2011|title=Compact Differential Evolution|journal=IEEE Transactions on Evolutionary Computation|language=en-US|volume=15|issue=1|pages=32‚Äì54|doi=10.1109/tevc.2010.2058120|issn=1089-778X}}</ref> and its variants<ref>{{Cite journal|last=Iacca|first=Giovanni|last2=Caraffini|first2=Fabio|last3=Neri|first3=Ferrante|date=2012|title=Compact Differential Evolution Light: High Performance Despite Limited Memory Requirement and Modest Computational Overhead|journal=Journal of Computer Science and Technology|language=en|volume=27|issue=5|pages=1056‚Äì1076|doi=10.1007/s11390-012-1284-2|issn=1000-9000}}</ref><ref>{{Citation|last=Iacca|first=Giovanni|title=Opposition-Based Learning in Compact Differential Evolution|date=2011|last2=Neri|first2=Ferrante|last3=Mininno|first3=Ernesto|work=Applications of Evolutionary Computation|pages=264‚Äì273|publisher=Springer Berlin Heidelberg|language=en|doi=10.1007/978-3-642-20525-5_27|isbn=9783642205248}}</ref><ref>{{Cite book|last=Mallipeddi|first=Rammohan|last2=Iacca|first2=Giovanni|last3=Suganthan|first3=Ponnuthurai Nagaratnam|last4=Neri|first4=Ferrante|last5=Mininno|first5=Ernesto|date=2011|title=Ensemble strategies in Compact Differential Evolution|journal=2011 IEEE Congress of Evolutionary Computation (CEC)|language=en-US|publisher=IEEE|volume=|pages=|doi=10.1109/cec.2011.5949857|isbn=9781424478347}}</ref><ref>{{Cite journal|last=Neri|first=Ferrante|last2=Iacca|first2=Giovanni|last3=Mininno|first3=Ernesto|date=2011|title=Disturbed Exploitation compact Differential Evolution for limited memory optimization problems|journal=Information Sciences|volume=181|issue=12|pages=2469‚Äì2487|doi=10.1016/j.ins.2011.02.004|issn=0020-0255}}</ref><ref>{{Cite book|last=Iacca|first=Giovanni|last2=Mallipeddi|first2=Rammohan|last3=Mininno|first3=Ernesto|last4=Neri|first4=Ferrante|last5=Suganthan|first5=Pannuthurai Nagaratnam|date=2011|title=Global supervision for compact Differential Evolution|journal=2011 IEEE Symposium on Differential Evolution (SDE)|language=en-US|publisher=IEEE|volume=|pages=|doi=10.1109/sde.2011.5952051|isbn=9781612840710}}</ref><ref>{{Cite book|last=Iacca|first=Giovanni|last2=Mallipeddi|first2=Rammohan|last3=Mininno|first3=Ernesto|last4=Neri|first4=Ferrante|last5=Suganthan|first5=Pannuthurai Nagaratnam|date=2011|title=Super-fit and population size reduction in compact Differential Evolution|journal=2011 IEEE Workshop on Memetic Computing (MC)|language=en-US|publisher=IEEE|volume=|pages=|doi=10.1109/mc.2011.5953633|isbn=9781612840659}}</ref>\n* Compact Particle Swarm Optimization (cPSO)<ref>{{Cite journal|last=Neri|first=Ferrante|last2=Mininno|first2=Ernesto|last3=Iacca|first3=Giovanni|date=2013|title=Compact Particle Swarm Optimization|journal=Information Sciences|volume=239|pages=96‚Äì121|doi=10.1016/j.ins.2013.03.026|issn=0020-0255}}</ref>\n* Compact Bacterial Foraging Optimization (cBFO)<ref>{{Citation|last=Iacca|first=Giovanni|title=Compact Bacterial Foraging Optimization|date=2012|last2=Neri|first2=Ferrante|last3=Mininno|first3=Ernesto|work=Swarm and Evolutionary Computation|pages=84‚Äì92|publisher=Springer Berlin Heidelberg|language=en|doi=10.1007/978-3-642-29353-5_10|isbn=9783642293528}}</ref>\n* Probabilistic incremental program evolution (PIPE)<ref>{{Cite journal|last=Salustowicz|first=null|last2=Schmidhuber|first2=null|date=1997|title=Probabilistic incremental program evolution|journal=Evolutionary Computation|volume=5|issue=2|pages=123‚Äì141|issn=1530-9304|pmid=10021756|doi=10.1162/evco.1997.5.2.123}}</ref>\n* Estimation of Gaussian networks algorithm (EGNA){{Citation needed|date=June 2018}}\n* Estimation multivariate normal algorithm with thresheld convergence<ref>{{Cite book|last=Tamayo-Vera|first=Dania|last2=Bolufe-Rohler|first2=Antonio|last3=Chen|first3=Stephen|date=2016|title=Estimation multivariate normal algorithm with thresheld convergence|journal=2016 IEEE Congress on Evolutionary Computation (CEC)|language=en-US|publisher=IEEE|volume=|pages=|doi=10.1109/cec.2016.7744223|isbn=9781509006236}}</ref>\n*Dependency Structure Matrix Genetic Algorithm (DSMGA)<ref>{{Citation|last=Yu|first=Tian-Li|title=Genetic Algorithm Design Inspired by Organizational Theory: Pilot Study of a Dependency Structure Matrix Driven Genetic Algorithm|date=2003|work=Genetic and Evolutionary Computation ‚Äî GECCO 2003|pages=1620‚Äì1621|publisher=Springer Berlin Heidelberg|language=en|doi=10.1007/3-540-45110-2_54|isbn=9783540406037|last2=Goldberg|first2=David E.|last3=Yassine|first3=Ali|last4=Chen|first4=Ying-Ping}}</ref><ref>{{Cite book|last=Hsu|first=Shih-Huan|last2=Yu|first2=Tian-Li|date=2015-07-11|title=Optimization by Pairwise Linkage Detection, Incremental Linkage Set, and Restricted / Back Mixing: DSMGA-II|publisher=ACM|pages=519‚Äì526|doi=10.1145/2739480.2754737|isbn=9781450334723|arxiv=1807.11669}}</ref>\n\n==Related==\n\n* [[CMA-ES]]\n* [[Cross-entropy method]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Estimation Of Distribution Algorithm}}\n[[Category:Evolutionary computation]]\n[[Category:Stochastic optimization]]"
    }
  ]
}