{
  "pages": [
    {
      "title": "Smart intelligent aircraft structure",
      "url": "https://en.wikipedia.org/wiki/Smart_intelligent_aircraft_structure",
      "text": "{{multiple issues|\n{{Orphan|date=February 2013}}\n{{cleanup rewrite|date=October 2012}}\n}}\n\nThe term \"smart structures\" is commonly used for structures which have the ability to adapt to environmental conditions according to the design requirements. As a rule, the adjustments are designed and performed in order to increase the efficiency or safety of the structure.  Combining \"smart structures\" with the \"sophistication\" achieved in [[materials science]], [[information technology]], measurement science, [[sensors]], [[actuators]], [[signal processing]], [[nanotechnology]], [[cybernetics]], [[artificial intelligence]], and [[biomimetics]],<ref>Wadhawan, V.K. (2005) Smart Structures and Materials. Resonance [online]. Available from: http://www.ias.ac.in/resonance/Nov2005/pdf/Nov2005p27-41.pdf [Accessed 30 July 2012].</ref> one can talk about Smart Intelligent Structures. In other words, structures which are able to sense their environment, self-diagnose their condition and adapt in such a way so as to make the design more useful and efficient.\n\nThe concept of Smart Intelligent Aircraft Structures offers significant improvements in aircraft total weight, [[manufacturing cost]] and, above all, operational cost by an integration of system tasks into the load carrying structure.<ref>Speckmann, H., Roesner, H. (2006). Structural Health Monitoring: A Contribution to the Intelligent Aircraft Structure, [online] ECNDT 2006 – Tu. 1.1.1, Airbus, Bremen, Germany. Available from: http://www.ndt.net/article/ecndt2006/doc/Tu.1.1.1.pdf [Accessed 30 July 2012].</ref> It also helps to improve the aircraft’s life cycle and reduce its maintenance.<ref>Dufault, C.F. and Akhras, G., (2008). Smart Structure Applications in Aircraft. The Canadian Air Force Journal, [online], p. 31-39. Available from: {{cite web |url=http://www.airforce.forces.gc.ca/CFAWC/eLibrary/Journal/Vol1-2008/Iss2-Summer/Sections/06-Smart_Structure_Applications_in_Aircraft_e.pdf |title=Archived copy |accessdate=2012-10-11 |deadurl=yes |archiveurl=https://web.archive.org/web/20130522211717/http://www.airforce.forces.gc.ca/CFAWC/eLibrary/Journal/Vol1-2008/Iss2-Summer/Sections/06-Smart_Structure_Applications_in_Aircraft_e.pdf |archivedate=2013-05-22 |df= }}  [Accessed 30 July 2012].</ref> Individual morphing concepts also have the ability to decrease airframe generated noise and hence reduce the effect of air traffic noise near airports. Furthermore, cruise drag reductions have a positive effect on [[fuel consumption]] and required take-off fuel load.\n\n== Morphing structures ==\nFixed geometry [[wing]]s are optimized for a single design point, identified through [[altitude]], [[Mach number]], [[weight]], etc. Their development is always a compromise between design and off-design points, referred to a typical mission. This is emphasised for civil aircraft where flight profiles are almost standard. Nevertheless, it may occur to fly at high speeds and low altitude with light weight over a short stretch or to fly at low speeds and high altitude with maximum load for a longer range. The [[lift coefficient]] would then range between 0.08 and 0.4,<ref>H. P. Monner, D. Sachau, E. Breitbach, \"Design Aspects of the Elastic Trailing Edge for an Adaptive Wing\", RTO AVT Specialists’ Meeting on \"Structural Aspects of Flexible Aircraft Control\", Ottawa (CAN), 18–20 October 1999, published in RTO MP 36</ref><ref>J. J. Spillman, \"The use of variable camber to reduce drag, weight and costs of transport aircraft\", Aeronautical Journal, Vol. 96, No. 951, pp. 1-9, 1992</ref> with the aircraft experiencing up to 30% weight reduction as the fuel is consumed.<ref>H. Ahrend, D. Heyland, W. Martin, \"Das Leitkonzept ‘Adaptiver Fltiuegel’ (ADIF) DGLR-Jahrestagung, DGLR-JT97-147, Muenchen 1997</ref> These changes could be compensated by [[Variable camber wing|wing camber variations]], to pursue optimal geometry for any flight condition, thus improving [[aerodynamic]] and structural performance\n\nExisting aircraft cannot change shape without aerodynamic gaps, something that can be solved with Smart Intelligent Structures. By ensuring the detailed consideration of structural needs throughout the entire lifetime of an aircraft and focusing on the structural integration of needed past capabilities, Smart Intelligent Aircraft Structures will allow aircraft designers to seriously consider conformal morphing technologies. The reduced drag during take-off, cruise and landing for future and ecologically improved civil aircraft wings can be achieved through naturally laminar wing technology, by incorporating a gapless and deformable [[leading edge device]] with lift providing capability. Such a morphing structure typically consists of a flexible outer skin and an internal driving mechanism (Figure 1).\nCurrent aircraft designs already employ [[winglets]] aimed at increasing the cruise flight efficiency by induced drag reduction. Smart intelligent Structures propose a state of the art technology that incorporates a [[wingtip]] active [[trailing edge]], which could be a means of reducing winglet and wing loads at key flight conditions.\n\n==Structural health monitoring==\n\nAnother component of an \"intelligent\" aircraft structure is the ability to sense and diagnose potential threats to its structural integrity. This differs from conventional [[Nondestructive testing|non-destructive testing (NDT)]] by the fact that [[Structural health monitoring|Structural Health Monitoring (SHM)]] <ref>Guzman E. (2014) \"A Novel Structural Health Monitoring Method for Full-Scale CFRP Structures\". EPFL PhD thesis {{doi|10.5075/epfl-thesis-6422}}</ref> uses sensors that are permanently bonded or embedded in the structure. [[Composite materials]], which are highly susceptible to hidden internal flaws which may occur during manufacturing and processing of the material or while the structure is subjected to service loads,  require a substantial amount of inspection and defect monitoring at regular intervals. Thus, the increasing use of composite materials for aircraft primary structure aircraft components increases substantially their [[Whole-life cost|life cycle cost]]. According to some estimates, over 25% of the life cycle cost of an aircraft or aerospace structure, which includes pre-production, production, and post-production costs, can be attributed to operation and support, involving inspection and maintenance. With sensing technology reducing in cost, size and weight, and sensor signal processing power continuously increasing, a variety of approaches have been developed allowing integration of such sensing options onto or into [[structural component]]s.\n\nAlthough available in principle, none of these SHM technologies have currently achieved a sufficient level of maturity such that SHM could be reliably applied to real engineering structures. A real reduction of life cycle costs related to maintenance and inspections can only be achieved by SHM systems designed as \"[[fail-safe]]\" components and included within a [[damage tolerance]] assessment scenario, able to reduce the inspection times (or their intervals) by investigating the structure quickly and reliably and avoiding the time consuming disassembly of structural parts.<ref>Guzman E., Cugnoni J. and Gmür T. (2015) \"Monitoring of composite structures using a network of integrated PVDF film transducers\" Smart Materials and Structures vol. 24, num. 5, p. 055017 {{doi|10.1088/0964-1726/24/5/055017}}.</ref>\n\n==Multifunctional materials==\n\nThe advantages of [[Carbon-fiber-reinforced polymer|carbon fibre reinforced polymers (CFRPs)]] over metallic materials in terms of specific [[stiffness]] and [[strength of materials|strength]] are well known. In the last few years, there has been a sharp increase in the demand for composite materials with integrated multifunctional capabilities for use in aeronautical structures.\n\nHowever, a major drawback with CFRPs for primary structural applications is their low toughness and damage tolerance. [[Epoxy resins]] are brittle and have poor impact strength and resistance to [[crack propagation]], resulting in unsatisfactory levels of robustness and reliability. This results in designs with large margins of safety and complex inspection operations. In addition, by increasing the relative fraction of composite components within new aircraft, challenges regarding [[electrical conductivity]] have arisen such as [[lightning strike]] protection, [[static discharge]], [[electrical bonding]] and [[Electrical grounding|grounding]], interference shielding and current return through the structure. These drawbacks can be solved by the use of emerging technologies such as [[nanocomposites]], which combine mechanical, electrical and thermal properties.<ref>Gibson, R.F., \"A review of recent research on mechanics of multifunctional composite materials and structures\", Composite Structures 92 (2010) 2793\"</ref>\n\nNanoparticle reinforced resins have been found to offer two distinct advantages over current resin systems,.<ref>Gojny F H., Wichmann M. H. G., Fiedler B., Bauhofer W., Schulte K., \"Influence of nano-modification on the mechanical and electrical properties of conventional fibre-reinforced composites\", Composites Part A: Applied Science and Manufacturing, 36 (2005) 1525-1535</ref><ref>Z. Spitalsky, D. Tasis, K. Papagenlis, C. Galiotis, \"Carbon nnotube-polymer composites: Chemistry, processing, mechanical and electrical properties\", Progress in Polymer Science 35 (2010) 357-401</ref><ref>G. Romhány, G. Szebényi, \"Interlaminar crack propagation in MWCNT/fiber reinforced hybrid composites\", eXPRESS Polymer Letters Vol. 3, Nº 3 (2009) 145-151</ref><ref>V. Kostopoulos, A. Baltopoulos, P. Karapappas, A. Vavoluliotis, A. Paipetis, \"Impact and after-impact properties of carbon fibre reinforced composites enhanced with multi-wall carbon nanotubes\", Composites Science and Technology 70 (2010) 553-563</ref><ref>L. Gorbatikh, Y. Ding, N. De Greef, D. Yvanov, M. Karahan, A. Godara, L. Mezzo, S. Lomov, I. Verpoest, \"Effect of carbon nanotubes on the damage development in fiber-reinforced composites\", 14th European Conference on Composites Materials, 7–10 June 2010, Budapest, Hungary</ref> First of all, they are able to provide an increase in fracture toughness of up to 50% for older liquid resin infusion (LRI) resins and 30% in more advanced systems. Secondly, percolated nanoparticles drastically improve resin conductivity, turning it from a perfect isolator into a [[semiconductor]]. While improved damage tolerance properties could directly lead to structural weight savings, the exploitation of electrical properties could also enable a simpler, and hence cheaper, Electrical Structure Network (ESN).\n\n==Running research activities to implement the above technologies to aircraft==\n\nDeveloping these technologies for future A/C, there is currently (2011 – 2015) a running project, partially funded by the [[European Commission]], called \"SARISTU\" (Smart Intelligent Aircraft Structures) with a total budget of €51,000,000. This initiative is coordinated by Airbus and brings together 64 partners from 16 European countries.<ref>SARISTU PROJECT www.saristu.eu</ref><ref>CORDIS {{cite web |url=http://cordis.europa.eu/projects/index.cfm?fuseaction=app.details&TXT=saristu&FRM=1&STP=10&SIC=&PGA=&CCY=&PCY=&SRC=&LNG=en&REF=100047 |title=CORDIS &#124; European Commission |accessdate=2012-10-11 |deadurl=yes |archiveurl=https://web.archive.org/web/20151223024814/http://cordis.europa.eu/project/rcn/100047_en.html |archivedate=2015-12-23 |df= }}</ref> SARISTU focuses on the cost reduction of air travel through a variety of individual applications as well as their combination. Specifically, the integration of different conformal morphing concepts in a laminar wing is intended to improve aircraft performance through a 6% drag reduction, with a positive effect on fuel consumption and required take-off fuel load. A side effect will be a decrease of up to 6&nbsp;dB(A) of the airframe generated noise, thus reducing the impact of air traffic noise in the vicinity of airports. Recent calculations and [[Computational Fluid Dynamics]] Analysis indicate that the target is likely to be exceeded but will still need to be offset against a possible weight penalty.\n\nAnother expected outcome is to limit the integration cost of Structural Health Monitoring (SHM) systems by moving the system integration as far forward in the manufacturing chain as possible. In this manner, SHM integration becomes a feasible concept to enable in-service inspection cost reductions of up to 1%. Structural Health Monitoring related trials indicate that specific aircraft inspections may gain higher benefits than originally anticipated.\n\nFinally, the incorporation of [[Carbon Nanotubes]] into aeronautical resins is expected to enable weight savings of up to 3% when compared to the unmodified skin/stringer/frame system, while a combination of technologies is expected to decrease Electrical Structure Network installation costs by up to 15%.\n\n==References==\n\n{{Reflist}}\n\n==External links==\n* [http://iopscience.iop.org/0964-1726 Journal of Intelligent Material Systems and Structures]\n* [http://jim.sagepub.com/ Smart Structures and Systems]\n* [http://technopress.kaist.ac.kr/?journal=sss International Journal of Conceptual Structures and Smart Applications (IJCSSA)]\n* [http://www.igi-global.com/journal/international-journal-conceptual-structures-smart/54917 Journal of Intelligent Material Systems and Structures]\n* [http://jim.sagepub.com/content/current International Journal of Structural Integrity International Journal of Structural Integrity]\n* [http://www.emeraldinsight.com/products/journals/journals.htm?id=AEAT Aircraft Engineering and Aerospace Technology]\n\n[[Category:Structural analysis]]\n[[Category:Aerospace engineering]]\n[[Category:Aircraft components]]\n[[Category:Aircraft configurations]]\n[[Category:Aerospace materials]]\n[[Category:Smart materials]]"
    },
    {
      "title": "Solution procedure for Indeterminate Structures",
      "url": "https://en.wikipedia.org/wiki/Solution_procedure_for_Indeterminate_Structures",
      "text": "{{orphan|date=January 2014}}\n\n==Introduction==\nSolving a structure means determining the unknown internal forces, reactions and displacements of the structure. When a structure can be solved by using the equations of static equilibrium alone, it is known as determinate structure. A structure can be termed as indeterminate structure if it can not be solved by using the equations of equilibrium alone. Some examples of indeterminate structures are fixed-fixed beam, continuous beam, propped cantilever etc.\n\n==Methods for Solving==\nTo solve an indeterminate structure it is necessary to satisfy equilibrium, compatibility and force-displacement requirements of the structure.<ref>{{cite web|url=http://astore.amazon.com/civilbooks-20/detail/013257053X|title=Structural Analysis by Hibbeler R.C.(8th Ed. 2011)|publisher=Prentice Hall|accessdate=2013-10-09}}</ref> The additional equations required to solve indeterminate structure are obtained by the conditions of compatibility and/or force-displacement relations. The number of additional equations required to solve an indeterminate structure is known as degree of indeterminacy. Based on the types of unknown, a structure can be termed as [[statically indeterminate]] or kinematically indeterminate.\n\nThe following methods are used to solve indeterminate structures:\n# [[Flexibility method]]\n# [[Slope deflection method]]\n# [[Moment distribution method]]\n# [[Direct stiffness method]]\n\n==See also==\n*[[Statically indeterminate]]\n*[[Static equilibrium]]\n\n==References==\n{{reflist}}\n<!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. --->\n*\n\n<!--- STOP! Be warned that by using this process instead of Articles for Creation, this article is subject to scrutiny. As an article in \"mainspace\", it will be DELETED if there are problems, not just declined. If you wish to use AfC, please return to the Wizard and continue from there. --->\n\n\n\n[[Category:Structural analysis]]"
    },
    {
      "title": "STAAD",
      "url": "https://en.wikipedia.org/wiki/STAAD",
      "text": "{{Refimprove|date=July 2016}}\n'''STAAD''' or ('''STAAD.Pro''') is a [[structural analysis]] and design software application originally developed by Research Engineers International in 1997. In late 2005, Research Engineers International was bought by [[Bentley Systems]].<ref name=\"bentley\">{{cite web|url=http://www.bentley.com/en-US/Products/STAAD.Pro/ |title=3D Structural Analysis and Design Software - STAAD.Pro |website=Bentley.com |date= |accessdate=2016-07-27}}</ref><ref>[https://archive.is/20120710184428/http://discussion.bentley.com/cgi-bin/dnewsweb.exe?cmd=article&group=bentley.announcements&item=448&utag=] </ref>\n\nSTAAD.Pro is one of the most widely used structural analysis and design [[software]] products worldwide. It supports over 90 international steel, concrete, timber & aluminium design codes.\n\nIt can make use of various forms of analysis from the traditional static analysis to more recent analysis methods like [[P-Delta Effect|p-delta]] analysis, geometric non-linear analysis, Pushover analysis (Static-Non Linear Analysis) or a [[buckling]] analysis. It can also make use of various forms of dynamic analysis methods from time history analysis to response spectrum analysis.The response spectrum analysis feature is supported for both user defined spectra as well as a number of international code specified spectra.   \n\nAdditionally, STAAD.Pro is interoperable with applications such as RAM Connection, AutoPIPE, SACS and many more engineering design and analysis applications to further improve collaboration between the different disciplines involved in a project. STAAD can be used for analysis and design of all types of structural projects from plants, buildings, and bridges to towers, tunnels, metro stations, water/wastewater treatment plants and more.\n\n\n=== Important Features ===\n\n==== Analytical Modeling ====\nAnalytical model can be created using the ribbon-based user interface, by editing the command file or by importing several other files types like dxf, cis/2 etc. The model geometry can even be generated from the data of macro-enabled applications (like Microsoft Excel, Microstation etc.) by using Macros.\n\n==== Physical Modeling ====\nPhysical modeling has been a significant feature included in the program. STAAD.Pro Physical Modeler takes advantage of physical modeling to simplify modeling of a structure, which in turn more accurately reflects the process of building a model. Beams and surfaces are placed in the model on the scale of which they would appear in the physical world. A column may span multiple floors and a surface represents an entire floor of a building, for example. A joint is then generated anywhere two physical objects meet in the model (as well as at the free ends of cantilevered members, for convenience).\n\n==== STAAD Building Planner ====\nSTAAD Building Planner is a module that enables seamless generation of building models that can be analyzed and designed thereafter in the program itself. Operations like defining geometry, making changes in the geometric specifications are matters of only few clicks in this workflow.\n\n==== Steel AutoDrafter ====\nSteel AutoDrafter workflow extracts planar drawings and material take-off from a structural steel model prepared in STAAD.Pro. It produces excellent quality plans at any level and sections in any of the orthogonal directions.\n\n==== STAAD.Beava ====\nThe general philosophy governing the design of bridges is that, subject to a set of loading rules and constraints, the worst effects due to load application should be established and designed against. The process of load application can be complex as governing rules can impose interdependent parameters such as loaded length on a lane, lane factors, and load intensity. To obtain the maximum design effects, engineers have to try many loading situations on a trial and error basis.\n\nThis leads to the generation of many live load application instances (and a large volume of output data) that then must be combined with dead load and other effects, as well. Bridge Deck is used to minimize the load application process while complying with national code requirements.\n\nThe program is based on the use of influence surfaces, which are generated by STAAD.Pro as part of the loading process. An influence surface for a given effect on a bridge deck relates its value to movement of a unit load over the point of interest. The influence surface is a three-dimensional form of an influence line for a single member (or, in other words, it is a 2D influence function).\n\nSTAAD.Pro will automatically generate influence surfaces for effects such as bending moments for elements, deflection in all the degrees of freedom of nodes, and support reactions. The user then instruct the program to utilize the relevant influence surfaces and, with due regards to code requirements, optimize load positions to obtain the maximum desired effects.\n\n\n==== Advanced Concrete Design ====\nThe Advanced Concrete Design workflow provides direct access for STAAD.Pro models to leverage the power of the RCDC application. This is a standalone application, which is operated outside the STAAD.Pro environment, but requires a model and results data from a suitable analysis.\nThe model should typically be formed from beams and columns (plates are currently not supported). RCDC can be used to design the following objects: Pile Caps, Footings, Columns and walls, Beams, Slabs. \n\nAs the projects progresses, each design created in RCDC is retained and displayed when RCDC is re-entered, so that previous designs can be recalled and/or continued. \nDetailed drawings and BBS of excellent quality can be generated as required and they are quite ready to be sent for execution.\n\n\n==== Advanced Slab Design ====\nThe STAAD.Pro Advanced Slab Design workflow is an integrated tool that works from within the STAAD.Pro environment. Concrete slabs can be defined, and the data can be transferred to RAM Concept.\nThe data passed into RAM Concept includes the geometry, section and material properties, loads and combination information, and analysis results.\n\n\n==== Earthquake Mode ====\nEurocode 8: Part 1 contains specific requirements and recommendations for building structures that are to be constructed in seismic regions. Essentially, these fundamental requirements have been provided to ensure that the structures can sustain the seismic loads without collapse and also – where required– avoid suffering unacceptable damage and can continue to function after an exposure to a seismic event.\nThis STAAD.Pro workflow is used to check if the structure conforms to the basic geometric recommendations made in Eurocode 8 (EC8). This workflow is in addition to the normal post-processing workflow which gives the various analysis results. These checks are intended to give you a \"feel\" for the structure and are not mandatory to proceed to the design phase.\n\n\n==== OpenSTAAD Macro Editor ====\nOpenSTAAD is a library of exposed functions enabling engineers access STAAD.Pro’s internal functions and routines as well as its graphical commands. With OpenSTAAD, one can use VBA macros to perform such tasks as automating repetitive modeling or post-processing tasks or embedding customized design routines. Following an open architecture paradigm, OpenSTAAD was built using ATL, COM, and COM+ standards as specified by Microsoft. This allows OpenSTAAD to be used in a macro application like Microsoft Excel or Autodesk AutoCAD. OpenSTAAD can also be used to link STAAD data to Web-based applications using ActiveX, HTML, and ASP. Through the in-built Macro Editor, one can leverage the functionalities of OpenSTAAD and automate the analysis and design workflows, thereby eliminating the chance of occurrence of potential errors due to manual intervention and reducing the required time for execution of the whole workflow (as compared to the manual execution time), to a large extent.\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://www.bentley.com/en-US/Products/STAAD.Pro Bentley.com]\n\n[[Category:Computer-aided engineering software]]\n[[Category:Structural analysis]]\n[[Category:Structural engineering]]"
    },
    {
      "title": "Statically indeterminate",
      "url": "https://en.wikipedia.org/wiki/Statically_indeterminate",
      "text": "{{merge|Statical determinacy|date=November 2018}}\nIn [[statics]], a structure is '''statically indeterminate''' (or '''hyperstatic''')<ref>{{cite book |last= Matheson |first= James Adam Louis |authorlink= Louis Matheson |title= Hyperstatic structures: an introduction to the theory of statically indeterminate structures |year= 1971 |publisher= Butterworths }}</ref> when the [[static equilibrium]] equations are insufficient for determining the internal forces and reactions on that structure.\n\nBased on [[Newton's laws of motion]], the equilibrium equations available for a two-dimensional body are:\n: <math> \\sum \\vec F = 0 </math>: the vectorial sum of the [[force]]s acting on the body equals zero. This translates to:\n:: &Sigma; ''H'' = 0: the sum of the horizontal components of the forces equals zero;\n:: &Sigma; ''V'' = 0: the sum of the vertical components of forces equals zero;\n: <math> \\sum \\vec M = 0 </math>: the sum of the [[moment (physics)|moment]]s (about an arbitrary point) of all forces equals zero.\n\n[[File:Statically Indeterminate Beam.svg|thumb|350px|right|[[Free body diagram]] of a statically indeterminate [[beam (structure)|beam]].]]\nIn the [[beam (structure)|beam]] construction on the right, the four unknown reactions are ''V''<sub>''A''</sub>, ''V''<sub>''B''</sub>, ''V''<sub>''C''</sub> and ''H''<sub>''A''</sub>. The equilibrium equations are:\n\n: Σ ''V'' = '''0''':\n::''V''<sub>''A''</sub> − ''F''<sub>''v''</sub> + ''V''<sub>''B''</sub> + ''V''<sub>''C''</sub> = 0\n: Σ ''H'' = '''0''':\n:: ''H''<sub>''A''</sub> = 0\n: Σ ''M''<sub>''A''</sub> = 0:\n:: ''F''<sub>''v''</sub> &sdot; ''a'' − ''V''<sub>''B''</sub> &sdot; (''a'' + ''b'') − ''V''<sub>''C''</sub> &sdot; (''a'' + ''b'' + ''c'') = 0.\n\nSince there are four unknown forces (or ''[[Variable (mathematics)|variables]]'') (''V''<sub>''A''</sub>, ''V''<sub>''B''</sub>, ''V''<sub>''C''</sub> and ''H''<sub>''A''</sub>) but only three equilibrium equations, this system of [[simultaneous equations]] does not have a unique solution. The structure is therefore classified as ''statically indeterminate''. Considerations in the material properties and compatibility in [[deformation (engineering)|deformation]]s are taken to solve statically indeterminate systems or structures.\n\n==Statically determinate==\nIf the support at ''B'' is removed, the reaction ''V''<sub>''B''</sub> cannot occur, and the system becomes '''statically determinate''' (or '''isostatic''').<ref>{{cite book \n|last= Carpinteri |first= Alberto |authorlink= Alberto Carpinteri \n|title= Structural Mechanics:  A Unified Approach |year= 1997 \n|publisher= Taylor & Francis |isbn= 0-419-19160-7}}</ref> Note that the system is ''completely constrained'' here.\nThe system becomes an [[exact constraint]] [[kinematic coupling]].\nThe solution to the problem is\n\n:<math>\\begin{align}\n  H_A &= F_h \\\\\n  V_C &= \\frac{F_v \\cdot a}{a + b + c} \\\\\n  V_A &= F_v - V_C\n\\end{align}</math>\n\nIf, in addition, the support at ''A'' is changed to a roller support, the number of reactions are reduced to three (without ''H''<sub>''A''</sub>), but the beam can now be moved horizontally; the system becomes ''unstable'' or ''partly constrained''—a [[mechanism (engineering)|mechanism]] rather than a structure. In order to distinguish between this and the situation when a system under equilibrium is perturbed and becomes unstable, it is preferable to use the phrase ''partly constrained'' here. In this case, the 2 unknowns ''V''<sub>''A''</sub> and ''V''<sub>''C''</sub> can be determined by resolving the vertical force equation and the moment equation simultaneously. The solution yields the same results as previously obtained. However, it is not possible to satisfy the horizontal force equation unless <math>F_h=0</math>.\n\nStatical indeterminacy is the existence of a non-trivial (non-zero) solution to the homogeneous system of equilibrium equations. It indicates the possibility of self-stress (stress in the absence of an external load) that may be induced by mechanical or thermal action. {{Disputed inline|date=August 2017}}\n\n==See also==\n*[[Christian Otto Mohr]]\n*[[Flexibility method]]\n*[[Moment distribution method]]\n*[[Overconstrained mechanism]]\n*[[Structural engineering]]\n\n== References ==\n{{reflist}}\n\n==External links==\n* [http://en.sopromat.org/2008/ Beam calculation online (Statically indeterminate)]\n* [https://civilengineer.webinfolist.com/mdcalc.htm Online calculator for solving indeterminate beams.]\n[http://www.civilengineerbuddy.com/introduction-structures-static-indeterminacy-beams-example/ Introduction to static indeterminacy and its difference with static determinacy]\n[[Category:Statics]]\n[[Category:Structural analysis]]"
    },
    {
      "title": "Stiffness",
      "url": "https://en.wikipedia.org/wiki/Stiffness",
      "text": "{{short description|Rigidity of an object}}\n{{about||pain and/or loss of range of motion of a joint|joint stiffness|the term regarding the stability of a differential equation|stiff equation}}\n{{redirect|Flexibility}}\n[[File:Stiffness of a coil spring.png|thumb|Stiffness of a coil spring|thumb|right|Extension of a coil spring, ''δ'', caused by an axial force, ''F'']]\n'''Stiffness''' is the extent to which an object resists [[deformation (mechanics)|deformation]] in response to an applied [[force]].<ref>{{cite journal\n| title = Stiffness--an unknown world of mechanical science?\n| journal = Injury\n| author = Baumgart F.\n| year = 2000\n| volume = 31\n| publisher = Elsevier\n| quote = “Stiffness” = “Load” divided by “Deformation”\n| url = http://www.sciencedirect.com/science/article/pii/S0020138300800406\n| accessdate = 2012-05-04\n| doi=10.1016/S0020-1383(00)80040-6}}</ref>\n\nThe complementary concept is '''flexibility''' or pliability: the more flexible an object is, the less stiff it is.<ref>{{citation |page=126 |chapter=Stiffness and flexibility |title=200 science investigations for young students |author=Martin Wenham |year=2001 |isbn=978-0-7619-6349-3}}</ref>\n\n==Calculations==\nThe stiffness, ''k'', of a body is a measure of the resistance offered by an elastic body to deformation. For an elastic body with a single [[Degrees of freedom (mechanics)|degree of freedom]] (DOF) (for example, stretching or compression of a rod), the stiffness is defined as\n\n:<math>k=\\frac {F} {\\delta} </math>\n\nwhere,\n\n:''F'' is the force on the body\n:<math> \\delta </math> is the [[displacement (vector)|displacement]] produced by the force along the same degree of freedom (for instance,  the change in length of a stretched spring)\n\nIn the [[International System of Units]], stiffness is typically measured in [[Newton (unit)|newton]]s per meter. \nIn Imperial units, stiffness is typically measured in [[Pound (force)|pound]]s(lbs) per inch.\n\nGenerally speaking, [[Deflection (engineering)|deflections]] (or motions) of an infinitesimal element (which is viewed as a point) in an elastic body can occur along multiple DOF (maximum of six DOF at a point). For example, a point on a horizontal [[Euler–Bernoulli beam equation|beam]] can undergo both a vertical [[Displacement (vector)|displacement]] and a rotation relative to its undeformed axis. When there are M degrees of freedom a M x M [[Matrix (mathematics)|matrix]] must be used to describe the stiffness at the point. The diagonal terms in the matrix are the direct-related stiffnesses (or simply stiffnesses) along the same degree of freedom and the off-diagonal terms are the coupling stiffnesses between two different degrees of freedom (either at the same or different points) or the same degree of freedom at two different points. In industry, the term '''influence coefficient''' is sometimes used to refer to the coupling stiffness.\n\nIt is noted that for a body with multiple DOF, the equation above generally does not apply since the applied force generates not only the deflection along its own direction (or degree of freedom) but also those along with other directions.\n\nFor a body with multiple DOF, in order to calculate a particular direct-related stiffness (the diagonal terms), the corresponding DOF is left free while the remaining should be constrained. Under such a condition, the above equation can be used to obtain the direct-related stiffness for the degree of freedom which is unconstrained. The ratios between the reaction forces (or moments) and the produced deflection are the coupling stiffnesses.\n\nA description including all possible stretch and shear parameters is given by the [[Hooke's law|elasticity tensor]].\n\n== Compliance ==\n\nThe [[Multiplicative inverse|inverse]] of stiffness is ''flexibility'' or ''compliance'', typically measured in units of metres per Newton. In rheology, it may be defined as the ratio of strain to stress,<ref>V. GOPALAKRISHNAN and CHARLES F. ZUKOSKI;  \"Delayed flow in thermo-reversible colloidal gels\";  Journal of Rheology;  Society of Rheology, U.S.A.;  July/August 2007;  51 (4): pp. 623–644.</ref> and so take the units of reciprocal stress, ''e.g''. 1/[[pascal (unit)|Pa]].\n\n== Rotational stiffness ==<!-- [[Torsional rigidity]] redirects here -->\n[[File:Angle torsion cylindre.svg|thumb|right|Twist, by angle ''α'', of a cylindrical bar, with length ''L'', caused by an axial moment, ''M'']]\n\nA body may also have a rotational stiffness, ''k'', given by\n\n:<math>k=\\frac {M} {\\theta} </math>\n\nwhere\n: ''M'' is the applied [[moment (physics)|moment]]\n: ''&theta;'' is the rotation\n\nIn the SI system, rotational stiffness is typically measured in [[newton-metre]]s per [[radian]].\n\nIn the SAE system, rotational stiffness is typically measured in inch-[[Pound (force)|pound]]s per [[degree (angle)|degree]].\n\nFurther measures of stiffness are derived on a similar basis, including:\n\n* shear stiffness - the ratio of applied [[shear stress|shear]] force to shear deformation\n* torsional stiffness - ratio of applied [[torsion (mechanics)|torsion]] moment to angle of twist\n\n== Relationship to elasticity ==\nThe [[elastic modulus]] of a material is not the same as the stiffness of a component made from that material.  Elastic modulus is a property of the constituent material; stiffness is a property of a structure or component of a structure, and hence it is dependent upon various physical dimensions that describe that component.  That is, the modulus is an [[intensive and extensive properties|intensive property]] of the material; stiffness, on the other hand, is an [[intensive and extensive properties|extensive property]] of the solid body that is dependent on the material ''and'' its shape and boundary conditions.  For example, for an element in [[tension (mechanics)|tension]] or [[compression (physical)|compression]], the axial stiffness is\n\n:<math>k=\\frac {AE} {L} </math>\n\nwhere\n:''A'' is the [[Cross section (geometry)#Area and volume|cross-sectional area]],\n:''E'' is the (tensile) elastic modulus (or [[Young's modulus]]),\n:''L'' is the [[length]] of the element.\n\nSimilarly, the torsional stiffness of a straight section is\n\n:<math>k=\\frac {GJ} {L} </math>\n\nwhere\n:''J'' is the [[torsion constant]] for the section,\n:''G'' is the [[rigidity modulus]] of the material.\n\nNote that in SI, these units yield <math>k : \\frac{\\mathrm{N} \\cdot \\mathrm{m}}{\\mathrm{rad}}</math>. For the special case of unconstrained uniaxial tension or compression, [[Young's modulus]] ''can'' be thought of as a measure of the stiffness of a structure.\n\n== Applications ==\nThe stiffness of a structure is of principal importance in many engineering applications, so the [[modulus of elasticity]] is often one of the primary properties considered when selecting a material. A high modulus of elasticity is sought when [[Deflection (engineering)|deflection]] is undesirable, while a low modulus of elasticity is required when flexibility is needed.\n\nIn biology, the stiffness of the [[extracellular matrix]] is important for guiding the migration of cells in a phenomenon called [[durotaxis]].\n\nAnother application of stiffness finds itself in [[skin]] biology. The skin maintains its structure due to its intrinsic tension, contributed to by [[collagen]], an extracellular protein which accounts for approximately 75% of its dry weight.<ref>{{cite journal|last1=Chattopadhyay|first1=S.|last2=Raines|first2=R.|title=Collagen-Based Biomaterials for Wound Healing|journal=Biopolymers|date=August 2014|volume=101|issue=8|pages=821–833|doi= 10.1002/bip.22486|pmc=4203321}}</ref> The pliability of skin is a parameter of interest that represents its firmness and extensibility, encompassing characteristics such as elasticity, stiffness, and adherence. These factors are of functional significance to patients.<ref>{{cite journal|last1=Ferriero|first1=Giorgio|last2=Di Carlo|first2=Silvia|last3=Salgovic|first3=Ludovit|last4=Bravini|first4=Elisabetta|last5=Sartorio|first5=Francesco|last6=Vercelli|first6=Stefano|title=Post-surgical scar assessment in rehabilitation: a systematic review|journal=Physical Therapy and Rehabilitation|date=2015|volume=2|issue=1|doi=10.7243/2055-2386-2-2|url=http://www.hoajonline.com/journals/pdf/2055-2386-2-2.pdf}}</ref> This is of significance to patients with traumatic injuries to the skin, whereby the pliability can be reduced due to the formation and replacement of healthy skin tissue by a pathological [[scar]]. This can be evaluated both subjectively, or objectively using a device such as the Cutometer. The Cutometer applies a vacuum to the skin and measures the extent to which it can be vertically distended. These measurements are able to distinguish between healthy skin, normal scarring, and pathological scarring,<ref>{{cite journal|last1=Nedelec|first1=Bernadette|last2=Correa|first2=José|last3=de Oliveira|first3=Ana|last4=LaSalle|first4=Leo|last5=Perrault|first5=Isabelle|title=Longitudinal burn scar quantification|journal=Burns|date=2014|doi= 10.1016/j.burns.2014.03.002}}</ref> and the method has been applied within clinical and industrial settings to monitor both pathophysiological sequelae, and the effects of treatments on skin.\n\n== See also ==\n{{columns-list|colwidth=30em|\n*[[Elasticity (physics)]]\n*[[Elastic modulus]]\n*[[Mechanical impedance]]\n*[[Hardness]]\n*[[Hooke's law]]\n*[[Moment of inertia]]\n*[[Stiffness (mathematics)]]\n*[[Young's modulus]]\n*[[Compliant mechanism]]\n*[[Shore durometer]]\n*[[Elastography]]\n*[[Tactile Imaging]]\n}}\n\n== References ==\n{{Reflist}}\n\n[[Category:Physical quantities]]\n[[Category:Continuum mechanics]]\n[[Category:Structural analysis]]"
    },
    {
      "title": "Stress–strain analysis",
      "url": "https://en.wikipedia.org/wiki/Stress%E2%80%93strain_analysis",
      "text": "'''Stress–strain analysis''' (or '''stress analysis''') is an [[engineering]] discipline that uses many methods to determine the [[stress (mechanics)|stresses]] and [[strain (mechanics)|strains]] in materials and structures subjected to [[force]]s. In [[continuum mechanics]], stress is a [[physical quantity]] that expresses the internal [[force]]s that neighboring [[particle]]s of a [[continuum mechanics|continuous material]] exert on each other, while strain is the measure of the deformation of the material.\n\nStress analysis is a primary task for [[civil engineering|civil]], [[mechanical engineering|mechanical]] and [[aerospace engineering|aerospace engineers]] involved in the design of structures of all sizes, such as [[tunnel]]s, [[bridge]]s and [[dam]]s, [[aircraft]] and [[rocket]] bodies, mechanical parts, and even [[plastic cutlery]] and [[staple (fastener)|staple]]s.  Stress analysis is also used in the maintenance of such structures, and to investigate the causes of structural failures.\n\nTypically, the starting point for stress analysis are a [[geometry|geometrical]] description of the structure, the properties of the materials used for its parts, how the parts are joined, and the maximum or typical forces that are expected to be applied to the structure. The output data is typically a quantitative description of how the applied forces spread throughout the structure, resulting in stresses, strains and the deflections of the entire structure and each component of that structure.  The analysis may consider forces that vary with time, such as [[engine]] vibrations or the load of moving vehicles. In that case, the stresses and deformations will also be functions of time and space.\n\nIn engineering, stress analysis is often a tool rather than a goal in itself; the ultimate goal being the design of structures and artifacts that can withstand a specified load, using the minimum amount of material or that satisfies some other optimality criterion.\n\nStress analysis may be performed through classical mathematical techniques, analytic mathematical modelling or computational simulation, experimental testing, or a combination of methods.\n\nThe term stress analysis is used throughout this article for the sake of brevity, but it should be understood that the strains, and deflections of structures are of equal importance and in fact, an analysis of a structure may begin with the calculation of deflections or strains and end with calculation of the stresses.\n \n==Scope==\n\n===General principles===\nStress analysis is specifically concerned with solid objects. The study of stresses in liquids and gases is the subject of [[fluid mechanics]].\n\nStress analysis adopts the macroscopic view of materials characteristic of [[continuum mechanics]], namely that all properties of materials are homogeneous at small enough scales.  Thus, even the smallest [[particle]] considered in stress analysis still contains an enormous number of atoms, and its properties are averages of the properties of those atoms.\n\nIn stress analysis one normally disregards the physical causes of forces or the precise nature of the materials.  Instead, one assumes that the stresses are related to [[deformation (mechanics)|strain]] of the material by known [[constitutive equations]].\n\nBy [[Newton's laws of motion]], any external forces that act on a system must be balanced by internal reaction forces,<ref name=Smith/> or cause the particles in the affected part to accelerate.  In a solid object, all particles must move substantially in concert in order to maintain the object's overall shape.  It follows that any force applied to one part of a solid object must give rise to internal reaction forces that propagate from particle to particle throughout an extended part of the system.  With very rare exceptions (such as [[ferromagnetism|ferromagnetic]] materials or planet-scale bodies), internal forces are due to very short range intermolecular interactions, and are therefore manifested as surface contact forces between adjacent particles — that is, as stress.<ref name=Liu/>\n\n===Fundamental problem===\nThe fundamental problem in stress analysis is to determine the distribution of internal stresses throughout the system, given the external forces that are acting on it. In principle, that means determining, implicitly or explicitly, the [[Cauchy stress tensor]] at every point.\n\nThe external forces may be [[body force]]s (such as gravity or magnetic attraction), that act throughout the volume of a material;<ref name=Irgens/> or concentrated loads (such as friction between an axle and a [[bearing (mechanical)|bearing]], or the weight of a train wheel on a rail), that are imagined to act over a two-dimensional area, or along a line, or at single point.  The same net external force will have a different effect on the local stress depending on whether it is concentrated or spread out.\n\n===Types of structures===\nIn civil engineering applications, one typically considers structures to be in [[static equilibrium]]: that is, are either unchanging with time, or are changing slowly enough for [[viscosity|viscous stresses]] to be unimportant (quasi-static). In mechanical and aerospace engineering, however, stress analysis must often be performed on parts that are far from equilibrium, such as vibrating plates or rapidly spinning wheels and axles. In those cases, the equations of motion must include terms that account for the acceleration of the particles.  In structural design applications, one usually tries to ensure the stresses are everywhere well below the [[yield strength]] of the material.  In the case of dynamic loads, the [[fatigue (material)|material fatigue]] must also be taken into account.  However, these concerns lie outside the scope of stress analysis proper, being covered in [[materials science]] under the names [[strength of materials]], [[fatigue (material)|fatigue]] analysis, stress corrosion, [[creep (deformation)|creep]] modeling, and other.\n\n==Experimental methods==\nStress analysis can be performed experimentally by applying forces to a test element or structure and then determining the resulting stress using [[sensor]]s. In this case the process would more properly be known as ''testing'' ([[destructive testing|destructive]] or [[non-destructive testing|non-destructive]]). Experimental methods may be used in cases where mathematical approaches are cumbersome or inaccurate. Special equipment appropriate to the experimental method is used to apply the static or dynamic loading.\n\nThere are a number of experimental methods which may be used:\n\n*[[Tensile testing]] is a fundamental [[materials science]] test in which a sample is subjected to [[uniaxial tension]] until failure. The results from the test are commonly used to select a material for an application, for [[quality control]], or to predict how a material will react under other types of forces. Properties that are directly measured via a tensile test are the [[ultimate tensile strength]], maximum elongation and reduction in [[cross section (geometry)|cross-section]] area. From these measurements, properties such as [[Young's modulus]], [[Poisson's ratio]], [[yield strength]], and the [[strain-hardening]] characteristics of the sample can be determined.\n*[[Strain gauge]]s can be used to experimentally determine the deformation of a physical part.  A commonly used type of strain gauge is a thin flat [[resistor]] that is affixed to the surface of a part, and which measures the strain in a given direction.  From the measurement of strain on a surface in three directions the stress state that developed in the part can be calculated.\n* Neutron diffraction is a technique that can be used to determine the subsurface strain in a part.\n\n[[File:Plastic Protractor Polarized 05375.jpg|thumb|Stress in plastic protractor causes [[birefringence]].]]\n*The [[Photoelasticity|photoelastic method]] relies on the fact that some materials exhibit  [[birefringence]] on the application of stress, and the magnitude of the refractive indices at each point in the material is directly related to the state of stress at that point. The stresses in a structure can be determined by making a model of the structure from such a photoelastic material.\n*[[Dynamic mechanical analysis]]  (DMA) is a technique used to study and characterize [[viscoelastic]] materials, particularly polymers.  The viscoelastic property of a polymer is studied by dynamic mechanical analysis where a sinusoidal force (stress) is applied to a material and the resulting displacement (strain) is measured. For a perfectly elastic solid, the resulting strains and the stresses will be perfectly in phase. For a purely viscous fluid, there will be a 90 degree phase lag of strain with respect to stress. Viscoelastic polymers have the characteristics in between where some phase lag will occur during DMA tests.\n\n==Mathematical methods==\nWhile experimental techniques are widely used, most stress analysis is done by mathematical methods, especially during design.\n\n===Differential formulation===\nThe basic stress analysis problem can be formulated by [[Euler's laws|Euler's equations of motion]] for continuous bodies (which are consequences of [[Newton's laws of motion|Newton's laws]] for conservation of [[linear momentum]] and [[angular momentum]]) and the [[Euler-Cauchy stress principle]], together with the appropriate constitutive equations.\n\nThese laws yield a system of [[partial differential equations]] that relate the stress tensor field to the [[strain tensor]] field as unknown functions to be determined.  Solving for either then allows one to solve for the other through another set of equations called constitutive equations.  Both the stress and strain tensor fields will normally be [[continuous function|continuous]] within each part of the system and that part can be regarded as a continuous medium with smoothly varying constitutive equations.\n\nThe external body forces will appear as the independent (\"right-hand side\") term in the differential equations, while the concentrated forces appear as boundary conditions.  An external (applied) surface force, such as ambient pressure or friction, can be incorporated as an imposed value of the stress tensor across that surface.  External forces that are specified as line loads (such as traction) or point loads (such as the weight of a person standing on a roof) introduce singularities in the stress field, and may be introduced by assuming that they are spread over small volume or surface area. The basic stress analysis problem is therefore a [[boundary-value problem]].\n\n===Elastic and linear cases===\nA system is said to be [[Elasticity (physics)|elastic]] if any deformations caused by applied  forces will spontaneously and completely disappear once the applied forces are removed.  The calculation of the stresses (stress analysis) that develop within such systems is based on the [[theory of elasticity]] and [[infinitesimal strain theory]].  When the applied loads cause permanent deformation, one must use more complicated constitutive equations, that can account for the physical processes involved ([[plasticity (physics)|plastic flow]], [[fracture]], [[phase transition|phase change]], etc.)\n\nEngineered structures are usually designed so that the maximum expected stresses are well within the realm of [[linear elasticity|linear elastic]] (the generalization of [[Hooke’s law]] for continuous media) behavior for the material from which the structure will be built. That is, the deformations caused by internal stresses are linearly related to the applied loads.  In this case the differential equations that define the stress tensor are also linear.  Linear equations are much better understood than non-linear ones; for one thing, their solution (the calculation of stress at any desired point within the structure) will also be a linear function of the applied forces.  For small enough applied loads, even non-linear systems can usually be assumed to be linear.\n\n===Built-in stress (preloaded) ===\n\n[[File:Hyperstatic_Stress_Field.jpg|240px|right|thumb|Example of a Hyperstatic Stress Field.]]\n\nA preloaded structure is one that has, internal forces, stresses and strains imposed within it by various means prior to application of externally applied forces. For example, a structure may have cables that are tightened, causing forces to develop in the structure, before any other loads are applied. Tempered glass is a commonly found example of a preloaded structure that has tensile forces and stresses that act on the plane of the glass and in the central plane of glass that causes compression forces to act on the external surfaces of that glass.\n\nThe mathematical problem represented is typically [[ill-posed problem|ill-posed]] because it has an infinitude of solutions. In fact, in any three-dimensional solid body one may have infinitely many (and infinitely complicated) non-zero stress tensor fields that are in stable equilibrium even in the absence of external forces.  These stress fields are often termed hyperstatic stress fields<ref>{{cite web|last1=Ramsay|first1=Angus|title=Hyperstatic Stress Fields|url=http://www.ramsay-maunder.co.uk/knowledge-base/glossary/hyperstatic-stress-fields/|website=www.ramsay-maunder.co.uk|accessdate=6 May 2017}}</ref> and they co-exist with the stress fields that balance the external forces.  In linear elasticity, their presence is required to satisfy the strain/displacement compatibility requirements and in limit analysis their presence is required to maximise the load carrying capacity of the structure or component.\n\n[[File:Hyperstatic_Moment_Field.jpg|240px|right|thumb|Example of a Hyperstatic Moment Field.]]\n\nSuch '''built-in stress''' may occur due to many physical causes, either during manufacture (in processes like [[extrusion]], [[casting]] or [[cold forming|cold working]]), or after the fact (for example because of uneven heating, or changes in moisture content or chemical composition).  However, if the system can be assumed to behave in a linear fashion with respect to the loading and response of the system, then effect of preload can be accounted for by adding the results of a preloaded structure and the same non-preloaded structure.\n\nIf linearity cannot be assumed, however, any built-in stress may affect the distribution of internal forces induced by applied loads (for example, by changing the effective stiffness of the material) or even cause an unexpected material failure.  For these reasons, a number of techniques have been developed to avoid or reduce built-in stress, such as [[annealing (metallurgy)|annealing]] of cold-worked glass and metal parts, [[expansion joint]]s in buildings, and [[roller joint]]s for bridges.\n\n===Simplifications===\n[[File:Loaded truss.svg|240px|right|thumb|Simplified modeling of a truss by unidimensional elements under uniaxial uniform stress.]]\nStress analysis is simplified when the physical dimensions and the distribution of loads allow the structure to be treated as one- or two-dimensional. In the analysis of a bridge, its three dimensional structure may be idealized  as a single planar structure, if all forces are acting in the plane of the trusses of the bridge. Further, each member of the truss structure might then be treated a uni-dimensional members with the forces acting along the axis of each member. In which case, the differential equations reduce to a finite set of equations  with finitely many unknowns.\n\nIf the stress distribution can be assumed to be uniform (or predictable, or unimportant) in one direction, then one may use the assumption of [[plane stress]] and [[plane strain]] behavior and the equations that describe the stress field are then a function of two coordinates only, instead of three.\n\nEven under the assumption of linear elastic behavior of the material, the relation between the stress and strain tensors is generally expressed by a fourth-order [[stiffness tensor]] with 21 independent coefficients (a symmetric 6 × 6 stiffness matrix). This complexity may be required for general anisotropic materials, but for many common materials it can be simplified. For [[orthotropic material]]s such as wood, whose stiffness is symmetric with respect to each of three orthogonal planes, nine coefficients suffice to express the stress–strain relationship.  For isotropic materials, these coefficients reduce to only two.\n\nOne may be able to determine a priori that, in some parts of the system, the stress will be of a certain type, such as uniaxial [[tension (physics)|tension]] or [[compression (physical)|compression]], simple [[shear stress|shear]], isotropic compression or tension, [[torsion (mechanics)|torsion]], [[bending]], etc.  In those parts, the stress field may then be represented by fewer than six numbers, and possibly just one.\n\n===Solving the equations===\nIn any case, for two- or three-dimensional domains one must solve a system of partial differential equations with specified boundary conditions. Analytical (closed-form) solutions to the differential equations can be obtained when the geometry, constitutive relations, and boundary conditions are simple enough. For more complicated problems one must generally resort to numerical approximations such as the [[finite element method]], the [[finite difference method]], and the [[boundary element method]].\n\n==Factor of safety==\n{{main article|Factor of safety}}\nThe ultimate purpose of any analysis is to allow the comparison of the developed stresses, strains, and deflections with those that are allowed by the design criteria. All structures, and components thereof, must obviously be designed to have a capacity greater than what is expected to develop during the structure's use to obviate failure. The stress that is calculated to develop in a member is compared to the strength of the material from which the member is made by calculating the ratio of the strength of the material to the calculated stress. The ratio must obviously be greater than 1.0 if the member is to not fail. However, the ratio of the allowable stress to the developed stress must be greater than 1.0 as a factor of safety (design factor) will be specified in the design requirement for the structure. All structures are designed to exceed the load those structures are expected to experience during their use. The design factor (a number greater than 1.0) represents the degree of uncertainty in the value of the loads, material strength, and consequences of failure. The stress (or load, or deflection) the structure is expected to experience are known as the working, the design or limit stress. The limit stress, for example, is chosen to be some fraction of the [[yield strength]] of the material from which the structure is made. The ratio of the ultimate strength of the material to the allowable stress is defined as the factor of safety against ultimate failure.\n\nLaboratory tests are usually performed on material samples in order to determine the yield and ultimate strengths of those materials. A statistical analysis of the strength of many samples of a material is performed to calculate the particular material strength of that material. The analysis allows for a rational method of defining the material strength and results in a value less than, for example, 99.99% of the values from samples tested. By that method, in a sense, a separate factor of safety has been applied over and above the design factor of safety applied to a particular design that uses said material.\n\nThe purpose of maintaining a factor of safety on yield strength is to prevent detrimental deformations that would impair the use of the structure. An aircraft with a permanently bent wing might not be able to move its control surfaces, and hence, is inoperable. While yielding of material of structure could render the structure unusable it would not necessarily lead to the collapse of the structure. The factor of safety on ultimate tensile strength is to prevent sudden fracture and collapse, which would result in greater economic loss and possible loss of life.\n\nAn aircraft wing might be designed with a factor of safety of 1.25 on the yield strength of the wing and a factor of safety of 1.5 on its ultimate strength. The test fixtures that apply those loads to the wing during the test might be designed with a factor of safety of 3.0 on ultimate strength, while the structure that shelters the test fixture might have an ultimate factor of safety of ten. These values reflect the degree of confidence the responsible authorities have in their understanding of the load environment, their certainty of the material strengths, the accuracy of the analytical techniques used in the analysis, the value of the structures, the value of the lives of those flying, those near the test fixtures, and those within the building.\n\nThe factor of safety is used to calculate a maximum allowable stress:\n:<math>\\text{maximum allowable stress} = \\frac{\\text{ultimate tensile strength}}{\\text{factor of safety}}</math>\n\n==Load transfer==\nThe evaluation of loads and stresses within structures is directed to finding the load transfer path. Loads will be transferred by physical contact between the various component parts and within structures.  The load transfer may be identified visually or by simple logic for simple structures.  For more complex structures more complex methods, such as theoretical [[solid mechanics]] or numerical methods may be required. Numerical methods include [[direct stiffness method]] which is also referred to as the [[finite element method]].\n\nThe object is to determine the critical stresses in each part, and compare them to the strength of the material (see [[strength of materials]]).\n\nFor parts that have broken in service, a [[forensic engineering]] or [[failure analysis]] is performed to identify weakness, where broken parts are analysed for the cause or causes of failure. The method seeks to identify the weakest component in the load path. If this is the part which actually failed, then it may corroborate independent evidence of the failure. If not, then another explanation has to be sought, such as a defective part with a lower [[tensile strength]] than it should for example.\n\n==Uniaxial stress==\nA linear element of a structure is one that is essentially one dimensional and is often subject to axial loading only. When a structural element is subjected to tension or compression its length will tend to elongate or shorten, and its cross-sectional area changes by an amount that depends on the [[Poisson's ratio]] of the material. In engineering applications, structural members experience small deformations and the reduction in cross-sectional area is very small and can be neglected, i.e., the cross-sectional area is assumed constant during deformation. For this case, the stress is called ''engineering stress'' or ''nominal stress'' and is calculated using the original cross section.\n\n:<math>\\sigma_\\mathrm{e} = \\tfrac{P}{A_o}</math>\n\nwhere P is the applied load, and Ao is the original cross-sectional area.\n\nIn some other cases, e.g., [[elastomer]]s and [[plasticity (physics)|plastic]] materials, the change in cross-sectional area is significant. For the case of materials where the volume is conserved (i.e. [[Poisson's ratio]] = 0.5), if the ''true stress'' is desired, it must be calculated using the true cross-sectional area instead of the initial cross-sectional area, as:\n\n:<math>\\sigma_\\mathrm{true} = (1 + \\varepsilon_\\mathrm e)(\\sigma_\\mathrm e)\\,\\!</math>,\n\nwhere\n\n:<math>\\varepsilon_\\mathrm e\\,\\!</math> is the nominal (engineering) [[strain (materials science)|strain]], and\n\n:<math>\\sigma_\\mathrm e\\,\\!</math> is nominal (engineering) stress.\n\nThe relationship between true strain and engineering strain is given by\n\n:<math>\\varepsilon_\\mathrm{true} = \\ln(1 + \\varepsilon_\\mathrm e)\\,\\!</math>.\n\nIn uniaxial tension, true stress is then greater than nominal stress. The converse holds in compression.\n\n==Graphical representation of stress at a point==\n''Mohr's circle'', ''Lame's stress ellipsoid'' (together with the ''stress director surface''), and ''Cauchy's stress quadric'' are two-dimensional graphical representations of the [[Stress (mechanics)#Cauchy's stress principle|state of stress at a point]]. They allow for the graphical determination of the magnitude of the stress tensor at a given point for all planes passing through that point. Mohr's circle is the most common graphical method.{{Main article|Mohr's circle}}\n''Mohr's circle'', named after [[Otto Mohr|Christian Otto Mohr]], is the locus of points that represent the state of stress on individual planes at all their orientations. The [[abscissa]], <math>\\sigma_\\mathrm{n}\\,\\!</math>, and [[ordinate]], <math>\\tau_\\mathrm{n}\\,\\!</math>, of each point on the [[circle]] are the normal stress and shear stress components, respectively, acting on a particular cut plane with a [[unit vector]] <math>\\mathbf n\\,\\!</math> with components <math>\\left(n_1, n_2, n_3 \\right)\\,\\!</math>.\n\n===Lame's stress ellipsoid===\n{{Main article|Lame's stress ellipsoid}}\nThe surface of the ellipsoid represents the locus of the endpoints of all stress vectors acting on all planes passing through a given point in the continuum body. In other words, the endpoints of all stress vectors at a given point in the continuum body lie on the stress ellipsoid surface, i.e., the radius-vector from the center of the ellipsoid, located at the material point in consideration, to a point on the surface of the ellipsoid is equal to the stress vector on some plane passing through the point. In two dimensions, the surface is represented by an [[ellipse]] (Figure coming).\n\n===Cauchy's stress quadric===\n[[File:Stress Trajectories in a Plate Membrane.jpg|thumbnail|Stress Trajectories in a Plate Membrane]]\n\nThe Cauchy's stress quadric, also called the ''stress surface'', is a surface of the second order that traces the variation of the normal stress vector <math>\\sigma_\\mathrm n \\,\\!</math> as the orientation of the planes passing through a given point is changed.\n\nThe complete state of stress in a body at a particular deformed configuration, i.e., at a particular time during the motion of the body, implies knowing the six independent components of the stress tensor <math>(\\sigma_{11}, \\sigma_{22}, \\sigma_{33}, \\sigma_{12}, \\sigma_{23}, \\sigma_{13})\\,\\!</math>, or the three principal stresses <math>(\\sigma_1, \\sigma_2, \\sigma_3)\\,\\!</math>, at each material point in the body at that time. However, numerical analysis and analytical methods allow only for the calculation of the stress tensor at a certain number of discrete material points. To graphically represent in two dimensions this partial picture of the stress field different sets of [[contour lines]] can be used:<ref name=Jaeger/>\n* ''Isobars'' are curves along which the principal stress, e.g., <math>\\sigma_1\\,\\!</math> is constant.\n* ''Isochromatics'' are curves along which the [[Stress (mechanics)#Maximum and minimum shear stresses|maximum shear stress]] is constant. These curves are directly determined using photoelasticity methods.\n* ''Isopachs'' are curves along which the [[Stress (mechanics)#Stress deviator tensor|mean normal stress]] is constant\n* ''Isostatics'' or ''stress trajectories''<ref>{{cite web|last1=Maunder|first1=Edward|title=Visualisation of stress fields - from stress trajectories to strut and tie models.|url=http://www.ramsay-maunder.co.uk/knowledge-base/publications/visualisation-of-stress-fields/|website=www.ramsay-maunder.co.uk|accessdate=15 April 2017}}</ref> are a system of curves which are at each material point tangent to the principal axes of stress - see figure <ref>{{cite web|last1=Ramsay|first1=Angus|title=Stress Trajectories|url=http://www.ramsay-maunder.co.uk/knowledge-base/projects/stress-trajectories/|website=Ramsay Maunder Associates|accessdate=15 April 2017}}</ref> \n* ''Isoclinics'' are curves on which the principal axes make a constant angle with a given fixed reference direction. These curves can also be obtained directly by photoelasticity methods.\n* ''[[Slip line field|Slip lines]]'' are curves on which the shear stress is a maximum.\n\n==See also==\n*[[Forensic engineering]]\n*[[Piping]]\n*[[Rockwell scale]]\n*[[Structural analysis]]\n*[[Stress (physics)|Stress]]\n*[[Worst case circuit analysis]]\n*[[List of finite element software packages]]\n*[[Stress–strain curve]]\n\n==References==\n<references>\n\n<ref name=Smith>\n  Donald Ray Smith and Clifford Truesdell (1993) [https://books.google.com/books?id=ZcWC7YVdb4wC&pg=PA97 \"An Introduction to Continuum Mechanics after Truesdell and Noll\". Springer.] {{ISBN|0-7923-2454-4}}</ref>\n\n<ref name=Jaeger>\n  John Conrad Jaeger, N. G. W. Cook, and R. W. Zimmerman (2007), [https://books.google.com/books?id=FqADDkunVNAC&pg=PA10 \"Fundamentals of Rock Mechanics\"] (4th edition) Wiley-Blackwell. {{ISBN|0-632-05759-9}}\n</ref>\n\n<ref name=Irgens>\n  Fridtjov Irgens (2008), [https://books.google.com/books?id=q5dB7Gf4bIoC&pg=PA46 \"Continuum Mechanics\"]. Springer. {{ISBN|3-540-74297-2}}\n</ref>\n\n<ref name=Liu>\n  I-Shih Liu (2002), [https://books.google.com/books?id=-gWqM4uMV6wC&pg=PA43 \"Continuum Mechanics\"]. Springer {{ISBN|3-540-43019-9}}\n</ref>\n\n</references>\n\n{{DEFAULTSORT:Stress Analysis}}\n[[Category:Structural analysis]]"
    },
    {
      "title": "Structural dynamics",
      "url": "https://en.wikipedia.org/wiki/Structural_dynamics",
      "text": "Structural analysis is mainly concerned with finding out the behavior of a physical structure when subjected to force. This action can be in the form of [[structural load|load]] due to the weight of things such as people, furniture, wind, snow, etc. or some other kind of excitation such as an earthquake, shaking of the ground due to a blast nearby, etc. In essence all these loads are dynamic, including the self-weight of the structure because at some point in time these loads were not there. The distinction is made between the dynamic and the static analysis on the basis of whether the applied action has enough acceleration in comparison to the structure's natural frequency. If a load is applied sufficiently slowly, the inertia forces ([[Newton's first law of motion]]) can be ignored and the analysis can be simplified as static analysis.\n'''Structural dynamics''', therefore, is a type of [[structural analysis]] which covers the behavior of [[structure]]s subjected to [[Dynamics (physics)|dynamic]] (actions having high acceleration) loading. Dynamic loads include people, wind, waves, traffic, [[earthquake]]s, and blasts. Any structure can be subjected to dynamic loading. Dynamic analysis can be used to find dynamic [[Displacement (vector)|displacements]], time history, and [[modal analysis]].\n\nA dynamic analysis is also related to the inertia forces developed by a structure when it is excited by means of dynamic loads applied suddenly (e.g., wind blasts, explosion, earthquake).\n\nA [[statics|static]] load is one which varies very slowly.  A dynamic load is one which changes with time fairly quickly in comparison to the structure's natural frequency. If it changes slowly, the structure's response may be determined with static analysis, but if it varies quickly (relative to the structure's ability to respond), the response must be determined with a dynamic analysis.\n\nDynamic analysis for simple structures can be carried out manually, but for complex structures [[finite element analysis]] can be used to calculate the mode shapes and frequencies.\n\n==Displacements==\n\nA dynamic load can have a significantly larger effect than a static load of the same magnitude due to the structure's inability to respond quickly to the loading (by deflecting). The increase in the effect of a dynamic load is given by the [[dynamic amplification factor]] (DAF) or dynamic load factor(DLF):\n\n:<math> DAF = DLF = \\frac{{u_{max}}}{{u_{static}}}</math>\n\nwhere u is the deflection of the structure due to the applied load.\n\nGraphs of dynamic amplification factors vs non-dimensional [[rise time]] (t<sub>r</sub>/T) exist for standard loading functions (for an explanation of rise time, see '''time history analysis''' below). Hence the DAF for a given loading can be read from the graph, the static deflection can be easily calculated for simple structures and the dynamic deflection found.\n\n==Time history analysis==\n\nA {{Not a typo|full time}} history will give the response of a structure over time during and after the application of a load. To find the {{Not a typo|full time}} history of a structure's response, you must solve the structure's [[equation of motion]].\n\n===Example===\n\n[[Image:Mass spring.svg|200px|right|Single degree of freedom system: simple mass spring model]]\n\nA simple single [[Degrees of freedom (mechanics)|degree of freedom]] [[system]] (a [[mass]], M, on a [[Spring (device)|spring]] of [[stiffness]] k, for example) has the following equation of motion:\n\n:<math>M{\\ddot{x}} + kx = F(t)</math>\n:\nwhere <math>\\ddot{x}</math> is the acceleration (the double [[derivative]] of the displacement) and x is the displacement.\n\nIf the loading F(t) is  a [[Heaviside step function]] (the sudden application of a constant load), the solution to the equation of motion is:\n\n:<math>x = \\frac{{F_0}}{{k}}[1 - cos{(\\omega t)}]</math>\n:\n\nwhere <math>\\omega = \\sqrt{\\frac{{k}}{{M}}}</math> and the fundamental natural frequency, <math>f = \\frac {\\omega}{2\\pi}</math>.\n\nThe static deflection of a single degree of freedom system is:\n\n:<math>x_{static} = \\frac{{F_0}}{{k}}</math>\n:\nso you can write, by combining the above formulae:\n\n:<math>x = x_{static}[1 - cos(\\omega t)]</math>\n\nThis gives the (theoretical) time history of the structure due to a load F(t), where the false assumption is made that there is no [[damping]].\n\nAlthough this is too simplistic to apply to a real structure, the Heaviside Step Function is a reasonable model for the application of many real loads, such as the sudden addition of a piece of furniture, or the removal of a prop to a newly cast concrete floor.  However, in reality loads are never applied instantaneously - they build up over a period of time (this may be very short indeed).  This time is called the [[rise time]].\n\nAs the number of degrees of freedom of a structure increases it very quickly becomes too difficult to calculate the time history manually - real structures are analysed using [[non-linear]] [[finite element analysis]] software.\n\n==Damping==\n\nAny real structure will dissipate energy (mainly through friction). This can be modelled by modifying the DAF\n\n:<math>DAF = 1 + e^{-c\\pi}</math>\n:\nwhere <math>c=\\frac{{\\text {Damping Coefficient}}}{{\\text{Critical Damping Coefficient}}}</math> and is typically 2%-10% depending on the type of construction:\n\n* Bolted steel ~6%\n* Reinforced concrete ~ 5%\n* Welded steel ~ 2%\n* Brick masonry ~ 10%\n\n'''Methods to increase damping'''\n\nOne of the widely used methods to increase damping is to attach a layer of material with a high Damping Coefficient, for example rubber, to a vibrating structure.\n\n==Modal analysis==\n\nA [[modal analysis]] calculates the frequency [[Normal mode|modes]] or natural frequencies of a given system, but not necessarily its full-time history response to a given input.  The natural frequency of a system is dependent only on the [[stiffness]] of the structure and the [[mass]] which participates with the structure (including self-weight). It is not dependent on the load function.\n\nIt is useful to know the modal frequencies of a structure as it allows you to ensure that the frequency of any applied periodic loading will not coincide with a modal frequency and hence cause [[resonance]], which leads to large [[oscillations]].\n\nThe method is:\n\n# Find the natural modes (the shape adopted by a structure) and natural frequencies\n# Calculate the response of each mode\n# Optionally superpose the response of each mode to find the full modal response to a given loading\n\n===Energy method===\n\nIt is possible to calculate the frequency of different mode shape of system manually by the [[Energy principles in structural mechanics|energy method]]. For a given mode shape of a multiple degree of freedom system you can find an \"equivalent\" mass, stiffness and applied force for a single degree of freedom system. For simple structures the basic mode shapes can be found by inspection, but it is not a conservative method. Rayleigh's principle states:\n\n\"The frequency ω of an arbitrary mode of vibration, calculated by the energy method, is always greater than - or equal to - the fundamental frequency ω<sub>n</sub>.\"\n\nFor an assumed mode shape <math>\\bar{u}(x)</math>, of a structural system with mass M; bending stiffness, EI ([[Young's modulus]], E, multiplied by the [[second moment of area]], I); and applied force, F(x):\n\n:<math>\\text{Equivalent mass, } M_{eq} = \\int{M\\bar{u}^2} du </math>\n:\n:<math>\\text{Equivalent stiffness, }k_{eq} = \\int{EI \\bigg(\\frac{{d^2\\bar{u}}}{{dx^2}} \\bigg)^2}dx</math>\n:\n:<math>\\text{Equivalent force, }F_{eq} = \\int{F\\bar{u}}dx</math>\n:\nthen, as above:\n\n:<math>\\omega = \\sqrt{\\frac{{k_{eq}}}{{M_{eq}}}}</math>\n\n===Modal response===\n\nThe complete modal response to a given load F(x,t) is <math>v(x,t)=\\sum{u_n(x,t)}</math>. The summation can be carried out by one of three common methods:\n\n* Superpose complete time histories of each mode (time consuming, but exact)\n* Superpose the maximum amplitudes of each mode (quick but conservative)\n* Superpose the square root of the sum of squares (good estimate for well-separated frequencies, but unsafe for closely spaced frequencies)\n\nTo superpose the individual modal responses manually, having calculated them by the energy method:\n\nAssuming that the rise time t<sub>r</sub> is known (T = 2π/ω), it is possible to read the DAF from a standard graph. The static displacement can be calculated with <math>u_{static}=\\frac{F_{1,eq}}{k_{1,eq}}</math>. The dynamic displacement for the chosen mode and applied force can then be found from:\n\n:<math>u_{max} = u_{static}DAF</math>\n\n==Modal participation factor==\n\nFor real systems there is often mass participating in the [[forcing function (differential equations)|forcing function]] (such as the mass of ground in an [[earthquake]]) and mass participating in [[inertia]] effects (the mass of the structure itself, M<sub>eq</sub>).  The [[modal participation factor]] Γ is a comparison of these two masses.  For a single degree of freedom system Γ = 1.\n\n:Γ <math> = \\frac{\\sum{M_n\\bar{u}_n}}{\\sum{M_n\\bar{u}_n^2}}</math>\n\n==External links==\n* [http://sites.google.com/site/dyssolve/ DYSSOLVE: Dynamic System Solver] - An encrypted-source, lightweight, free-of-charge software that can be used to solve basic structural dynamics problems. \n* [http://structdynviblab.mcgill.ca/ Structural Dynamics and Vibration Laboratory of McGill University]\n* [http://frame3dd.sourceforge.net Frame3DD open source 3D structural dynamics analysis program]\n* [http://www.noisestructure.com/products/FRF.php Frequency response function from modal parameters]\n* [http://vibrationdata.wordpress.com/category/structural-dynamics/ Structural Dynamics Tutorials & Matlab scripts]\n* [http://www.exploringstructuraldynamics.org/ AIAA Exploring Structural Dynamics]  (http://www.exploringstructuraldynamics.org/ ) - Structural Dynamics in Aerospace Engineering: Interactive Demos, Videos & Interviews with Practicing Engineers\n\n[[Category:Structural analysis]]\n[[Category:Dynamics (mechanics)]]"
    },
    {
      "title": "Structural element",
      "url": "https://en.wikipedia.org/wiki/Structural_element",
      "text": "'''Structural elements''' are used in [[structural analysis]] to split a complex structure into simple [[Wiktionary:element|elements]]. Within a structure, an element cannot be broken down (decomposed) into parts of different kinds (e.g., beam or column).<ref name=\"waddell\">{{cite book\n | last = Waddelln Alexander Low Waddell\n | title = Bridge Engineering - Volume 2\n | url = https://books.google.com/books?id=bxAkAAAAMAAJ\n | accessdate = 2008-08-19\n | year = 1916\n | publisher = John Wiley & Sons, Inc.\n | location = New York\n | pages = 1958\n}}</ref>\n\nStructural elements can be lines, surfaces or volumes.<ref>{{cite book| last1=Ryal| first1=M.J.| authorlink1=| last2=Parke| first2=G.A.R.| last3=Harding| first3=J.E.|editor1-first=| editor1-last=| editor1-link=| others=| title=The Manual of Bridge Engineering| url=https://books.google.com/books?id=8PGk81gtCywC| format=Google books (preview)| accessdate=2009-04-12| edition=| series=| volume=| date=| year=2000| month=|origyear=| publisher=Thomas Telford| location=London| language=| isbn=978-0-7277-2774-9| page=98| pages=| chapter=| chapterurl=| quote=| ref=}}</ref>\n\nLine elements:\n\n*[[Rod (geometry)|Rod]] - axial loads\n*[[Beam (structure)|Beam]] - axial and [[bending]] loads\n*[[Strut]]s or [[Compression member]]s- [[compressive stress|compressive loads]]\n*[[Tie (engineering)|Tie]]s, [[Tie rod]]s, [[eyebar]]s, [[guy-wire]]s, suspension cables, or [[wire rope]]s - [[Tension (physics)|tension]] loads\n\nSurface elements:\n\n*[[Membrane theory of shells|membrane]] - in-plane loads only\n*[[thin-shell structure|shell]] - in plane and bending moments\n**[[Concrete slab]]\n**[[Deck (bridge)|deck]]\n*[[shear wall|shear panel]] - [[shear stress|shear load]]s only\n\nVolumes:\n\n*Axial, shear and bending loads for all three dimensions\n\n==See also==\n* [[Load-bearing wall]]\n* [[Stressed member engine]]\n* [[Element Structures]]\n\n==References==\n{{Reflist}}\n\n[[Category:Structural analysis]]"
    },
    {
      "title": "Structural load",
      "url": "https://en.wikipedia.org/wiki/Structural_load",
      "text": "'''Structural loads''' or '''actions''' are [[force]]s, [[Deformation (engineering)|deformations]], or [[acceleration]]s applied to a [[structure]] or its [[Structural engineering#Structural elements|component]]s.<ref>{{cite book|title=ASCE/SEI 7-05 Minimum Design Loads for Buildings and Other Structures|year=2006|publisher=American Society of Civil Engineers|isbn=0-7844-0809-2|pages=1}}</ref><ref>{{cite book|title=Eurocode 0: Basis of structural design EN 1990 |year=2002|publisher=European Committee for Standardization|location=Bruxelles|chapter=1.5.3.1}}</ref> Loads cause [[stress (physics)|stresses]], [[deformation (engineering)|deformations]], and [[displacement (vector)|displacements]] in structures. Assessment of their effects is carried out by the methods of [[structural analysis]]. Excess load or overloading may cause [[structural failure]], and hence such possibility should be either considered in the design or strictly controlled. Mechanical structures, such as [[aircraft]], [[satellite]]s, [[rocket]]s, [[space station]]s, [[ship]]s, and [[submarine]]s, have their own particular structural loads and actions.<ref>{{cite book|title=Mark's Standard Handbook for Mechanical Engineers|publisher=McGraw-Hill|isbn=0-07-004997-1|edition=10th|editor=Avallone, E.A. |editor2= Baumeister, T.|pages=11–42}}</ref> Engineers often evaluate structural loads based upon published [[regulation]]s, [[contract]]s, or [[specification]]s.  Accepted [[technical standard]]s are used for [[acceptance testing]] and [[inspection]].\n\n==Types of loads==\nDead loads are static forces that are relatively constant for an extended time. They can be in [[tension (physics)|tension]] or [[compression (physics)|compression]]. The term can refer to a laboratory test method or to the normal usage of a material or structure.\n\nLive loads are usually variable or [[moving load]]s. These can have a significant dynamic element and may involve considerations such as [[impact (mechanics)|impact]], [[momentum]], [[vibration]], [[slosh dynamics]] of fluids, etc.\n\nAn impact load is one whose time of application on a material is less than one-third of the natural period of vibration of that material.\n\nCyclic loads on a structure can lead to [[Fatigue (material)|fatigue]] damage, cumulative damage, or failure. These loads can be repeated loadings on a structure or can be due to [[vibration]].\n\n==Loads on architectural and civil engineering structures==\nStructural loads are an important consideration in the design of [[building]]s. [[Building code]]s require that structures be designed and built to safely resist all actions that they are likely to face during their service life, while remaining fit for use.<ref>{{cite book|title=Eurocode 0: Basis of structural design EN 1990 |year=2002|publisher=European Committee for Standardization|location=Bruxelles|chapter=2.2.1(1)}}</ref>  Minimum loads or actions are specified in these building codes for types of structures, geographic locations, usage and materials of construction.<ref>{{cite book|title=International Building Code|year=2000|publisher=International Code Council|location=USA|isbn=1-892395-26-6|page=295|chapter=1604.2}}</ref>  Structural loads are split into categories by their originating cause.  In terms of the actual load on a structure, there is no difference between dead or live loading, but the split occurs for use in safety calculations or ease of analysis on complex models.\n\nTo meet the requirement that design strength be higher than maximum loads, [[building code]]s prescribe that, for structural design, loads are increased by load factors.  These [[Lrfd#Factor Development|load factors]] are, roughly, a ratio of the theoretical design strength to the maximum load expected in service.  They are developed to help achieve the desired level of reliability of a structure<ref>{{cite book|title=Eurocode 0: Basis of structural design EN 1990 |year=2002|publisher=European Committee for Standardization|location=Bruxelles|chapter=2.2.5(b)}}</ref>  based on probabilistic studies that take into account the load's originating cause, recurrence, distribution, and static or dynamic nature.<ref>{{cite book|last=Rao|first=Singiresu S.|title=Reliability Based Design|year=1992|publisher=McGraw-Hill|location=USA|isbn=0-07-051192-6|pages=214–227}}</ref>\n\n=== {{Visible anchor|Dead load}} ===\n[[File:DEAD_lOAD.jpg|thumb|Dead load]]\n[[File:IMPOSED lOAD.jpg|thumb|Imposed load (live load)]]\n[[File:SNOW LOAD.jpg|thumb|Live snow load]]\nThe dead load includes loads that are relatively constant over time, including the weight of the structure itself, and immovable fixtures such as walls, [[plasterboard]] or [[carpet]]. The roof is also a dead load. Dead loads are also known as permanent or static loads. Building materials are not dead loads until constructed in permanent position.<ref name=\"Reference A\">2006 International Building Code Section 1602.1.</ref><ref name=\"Reference B\">EN 1990 Euro code – Basis of structural design section 4.1.1</ref><ref name=\"Reference C\">EN 1991-1-1 Euro code 1: Actions on Structures – Part 1-1: General actions – densities, self-weight, imposed loads for buildings section 3.2</ref> IS875(part 1)-1987 give unit weight of building materials, parts, components.\n\n===Live load, imposed loads, transient load===\n<!--Imposed load redirects here-->\nLive loads, or imposed loads, are temporary, of short duration, or a [[moving load]]. These [[dynamics (mechanics)|dynamic]] loads may involve considerations such as [[impact (mechanics)|impact]], [[momentum]], [[vibration]], [[slosh dynamics]] of fluids and material [[fatigue (material)|fatigue]].\n\nLive loads, sometimes also referred to as probabilistic loads, include all the forces that are variable within the object's normal operation cycle not including construction or environmental loads.\n\nRoof and floor live loads are produced during maintenance by workers, equipment and materials, and during the life of the structure by movable objects, such as planters and people.\n\nBridge live loads are produced by vehicles traveling over the deck of the bridge.\n\n===Environmental loads===\nEnvironmental Loads are structural loads caused by natural forces such as wind, rain, snow, earthquake or extreme temperatures.\n*[[Wind engineering|Wind loads]]\n*[[Snow#Design of structures considering snow load|Snow]], rain and ice loads\n*[[Seismic loading|Seismic loads]]\n*[[Hydrostatic load]]s\n*[[Temperature]] changes leading to [[thermal expansion]] cause [[thermal load]]s\n*[[Ponding]] loads\n*[[Frost heaving]]\n*Lateral pressure of [[soil]], [[groundwater]] or bulk materials\n*Loads from fluids or [[flood]]s\n*[[Permafrost]] melting\n*Dust loads\n\n===Other loads===\nEngineers must also be aware of other actions that may affect a structure, such as:\n*[[Foundation (engineering)|Foundation]] [[consolidation (soil)|settlement]] or displacement\n*[[Structure fire|Fire]]\n*[[Corrosion]]\n*[[Explosion]]\n*[[Creep (deformation)|Creep]] or shrinkage\n*Impact from vehicles or machinery vibration \n*[[Construction]] loads\n\n===Load combinations===\nA load combination results when more than one load type acts on the structure. [[Building code]]s usually specify a variety of load combinations together with [[Lrfd#Factor Development|load factors]] (weightings) for each load type in order to ensure the safety of the structure under different maximum expected loading scenarios.  For example, in designing a [[staircase]], a dead load factor may be 1.2 times the weight of the structure, and a live load factor may be 1.6 times the maximum expected live load. These two \"factored loads\" are combined (added) to determine the \"required strength\" of the staircase.\n\nThe reason for the disparity between factors for dead load and live load, and thus the reason the loads are initially categorized as dead or live is because while it is not unreasonable to expect a large number of people ascending the staircase at once, it is less likely that the structure will experience much change in its permanent load.\n\n== Aircraft structural loads ==\nFor aircraft, loading is divided into two major categories: limit loads and ultimate loads.<ref name=Donaldson126>Bruce K. Donaldson, ''Analysis of Aircraft Structures: An Introduction'' (Cambridge; New York: Cambridge University Press, 2008), p. 126</ref> Limit loads are the [[maximum]] loads a component or structure may carry safely. Ultimate loads are the limit loads times a factor of 1.5 or the point beyond which the component or structure will fail.<ref name=Donaldson126/> Gust loads are determined [[Statistics|statistically]] and are provided by an agency such as the [[Federal Aviation Administration]]. Crash loads are loosely bounded by the ability of structures to survive the [[deceleration]] of a major [[Earth's surface|ground]] [[wikt:impact|impact]].<ref>''Experimental Mechanics: Advances in Design, Testing and Analysis'', Volume 1, ed. I. M. Allison (Rotterdam, Netherlands: A.A. Balkema Publishers, 1998), p. 379</ref> Other loads that may be critical are pressure loads (for pressurized, high-altitude aircraft) and ground loads. Loads on the ground can be from adverse braking or maneuvering during [[taxiing]]. Aircraft are constantly subjected to cyclic loading. These cyclic loads can cause [[Fatigue (material)|metal fatigue]].<ref name=Donaldson129>Bruce K. Donaldson, ''Analysis of Aircraft Structures: An Introduction'' (Cambridge; New York: Cambridge University Press, 2008), p. 129</ref>\n\n==See also==\n*[[Hotel New World disaster]] – caused by omitting the dead load of the building in load calculations\n*[[Influence line]]\n*[[Probabilistic design]]\n\n==References==\n{{reflist}}\n\n==External links==\n*[https://skyciv.com/resources/load-combinations/ Load Combinations Summary]\n*Luebkeman, Chris H., and Donald Petting \"Lecture 17: Primary Loads\". University of Oregon. 1996<sup>[https://web.archive.org/web/20110525142919/http://darkwing.uoregon.edu/~struct/courseware/461/461_lectures/461_lecture17/461_lecture17.html]</sup>\n*Fisette, Paul, and the American Wood Council. \"Understanding Loads and Using Span Tables\". 1997.<sup>[http://www.awc.org/technical/spantables/tutorial.php]</sup>\n\n[[Category:Civil engineering]]\n[[Category:Structural engineering]]\n[[Category:Building engineering]]\n[[Category:Mechanical engineering]]\n[[Category:Construction terminology]]\n[[Category:Structural analysis]]"
    },
    {
      "title": "Theorem of three moments",
      "url": "https://en.wikipedia.org/wiki/Theorem_of_three_moments",
      "text": "In [[civil engineering]] and [[structural analysis]] [[Benoît Paul Émile Clapeyron|Clapeyron]]'s '''theorem of three moments''' is a relationship among the bending moments at three consecutive supports of a horizontal beam.\n\nLet ''A,B,C'' be the three consecutive points of support, and denote by- '''''l''''' the length of ''AB'' and <math>l'</math> the length of ''BC'', by ''w'' and <math>w'</math> the weight per unit of length in these segments.  Then<ref>J. B. Wheeler: An Elementary Course of Civil Engineering, 1876, Page 118 [https://books.google.com/books?id=IRhDAAAAIAAJ&pg=PA118&lpg=PA118#v=onepage&q&f=false]</ref> the bending moments <math>M_A,\\, M_B,\\, M_C</math> at the three points are related by:\n\n:<math>M_A l + 2 M_B (l+l') +M_C l' = \\frac{1}{4} w l^3 + \\frac{1}{4} w' (l')^3.</math>\n\nThis equation can also be written as <ref>[https://books.google.com/books?id=W9ZuLZWldUoC&pg=PA73&lpg=PA73#v=onepage&q&f=false Srivastava and Gope: Strength of Materials, page 73]</ref>\n\n:<math>M_A l + 2 M_B (l+l') +M_C l' = \\frac{6 a_1 x_1}{l} + \\frac{6 a_2 x_2}{l'}</math>\n\nwhere ''a''<sub>1</sub> is the area on the [[Shear and moment diagram|bending moment diagram]] due to vertical loads on AB, ''a''<sub>2</sub> is the area due to loads on BC, ''x''<sub>1</sub> is the distance from A to the centroid of the bending moment diagram of beam AB, ''x''<sub>2</sub> is the distance from C to the centroid of the area of the bending moment diagram of beam BC.\n\nThe second equation is more general as it does not require that the weight of each segment be distributed uniformly.\n[[File:Figure 01 - Sample continuous beam section.png|thumb|Figure 01-Sample continuous beam section]]\n\n==Derivation of three moments equations ==\n[[Christian Otto Mohr|Mohr's]] theorem<ref>{{cite web|title=Mohr's Theorem|url=http://www.colincaprani.com/files/notes/SAIII/Mohrs%20Theorems.pdf}}</ref> can be used to derive the three moment theorem<ref>{{cite web|title=Three Moment Theorem|url=http://www.duke.edu/~hpgavin/ce130/three-moment.pdf}}</ref>  (TMT).\n\n===Mohr's first theorem===\nThe change in [[slope]] of a [[deflection (physics)|deflection]] curve between two points of a beam is equal to the area of the M/EI diagram between those two points.(Figure 02)\n[[File:Mohr's First Theorem.png|thumb|Figure 02-Mohr's First Theorem]]\n\n===Mohr's second theorem===\nConsider two points k1 and k2 on a [[beam (structure)|beam]]. The [[deflection (physics)|deflection]] of k1 and k2 relative to the point of intersection between tangent at k1 and k2 and vertical through k1 is equal to the moment of M/EI diagram between k1 and k2 about k1.(Figure 03)\n[[File:Mohr's Second Theorem.png|thumb|Figure03-Mohr's Second Theorem]]\n\nThe three moment equation expresses the relation between [[bending moment]]s at three successive supports of a continuous beam, subject to a loading on a two adjacent span with or without [[settlement (structural)|settlement]] of the supports.\n\n===The sign convention===\nAccording to the Figure 04,\n# The moment M1, M2, and M3 be positive if they cause [[Compression (physical)|compression]] in the upper part of the beam. ([:wikt:sagging|sagging]] positive)\n# The [[deflection (physics)|deflection]] downward positive. (Downward settlement positive)\n# Let ABC is a [[:wikt:continuous|continuous]] beam with support at A,B, and C. Then moment at A,B, and C are M1, M2, and M3, respectively.    \n# Let A' B' and C' be the final positions of the beam ABC due to support [[settlement (structural)|settlements]].\n[[File:Deflection Curve of a Continuous Beam.png|thumb|Figure 04-Deflection Curve of a Continuous Beam Under Settlement]]\n\n===Derivation of three moment theorem===\nPB'Q is a tangent drawn at B' for final [[Elasticity (physics)|Elastic]] Curve A'B'C' of the [[beam (structure)|beam]] ABC. RB'S is a horizontal line drawn through B'. \nConsider, Triangles RB'P and QB'S.\n\n:<math>\\dfrac{PR}{RB'} = \\dfrac{SQ}{B'S},</math>\n{{NumBlk|:|<math>\\dfrac{PR}{L1} = \\dfrac{SQ}{L2}</math>|{{EquationRef|1}}}}\n{{NumBlk|:|<math>PR = \\Delta B - \\Delta A + PA'</math>|{{EquationRef|2}}}}\n{{NumBlk|:|<math>SQ = \\Delta C - \\Delta B - QC'</math>|{{EquationRef|3}}}}\n\nFrom (1), (2), and (3),\n:<math>\\dfrac{\\Delta B - \\Delta A + PA'}{L1} = \\dfrac{\\Delta C - \\Delta B - QC'}{L2}</math>\n{{NumBlk|:|<math>\\dfrac{PA'}{L1} + \\dfrac{QC'}{L2} = \\dfrac{\\Delta A -\\Delta B}{L1} + \\dfrac{\\Delta C -\\Delta B}{L2}</math>|{{EquationRef|a}}}}\n\nDraw the M/EI diagram to find the PA' and QC'.\n[[File:Moment Diagram.png|thumb|Figure 05 - M / EI Diagram]]\n\n'''From Mohr's Second Theorem''' <br />\nPA' = First moment of area of M/EI diagram between A and B about A.\n:<math>PA' = \\left(\\frac{1}{2} \\times \\frac{M_1}{E_1 I_1} \\times L_1\\right)\\times L_1\\times \\frac{1}{3} + \\left(\\frac{1}{2} \\times \\frac{M_2}{E_2 I_2} \\times L_1\\right)\\times L_1\\times\\frac{2}{3}+ \\frac{A_1 X_1}{E_1 I_1}</math>\nQC' = First moment of area of M/EI diagram between B and C about C.\n:<math>QC' = \\left(\\frac{1}{2} \\times \\frac{M_3}{E_2 I_2} \\times L_2\\right)\\times L_2\\times\\frac{1}{3} + \\left(\\frac{1}{2} \\times \\frac{M_2}{E_2 I_2} \\times L_2\\right)\\times L_2\\times\\frac{2}{3}+ \\frac{A_2 X_2}{E_2 I_2}</math>\n\nSubstitute in PA' and QC' on equation (a), the Three Moment Theorem (TMT) can be obtained.\n\n==Three moment equation==\n<br />\n:<math>\\frac{M_1 L_1}{E_1 I_1}+ 2M_2\\left(\\frac{L_1}{E_1 I_1} + \\frac{L_2}{E_2 I_2}\\right)+\\frac{M_3 L_2}{E_2 I_2} = 6 [\\frac{\\Delta A - \\Delta B}{L_1} + \\frac{\\Delta C - \\Delta B}{L_2}] - 6 [\\frac{A_1 X_1}{E_1 I_1 L_1} + \\frac{A_2 X_2}{E_2 I_2 L_2}]</math>\n\n==Notes==\n{{reflist}}\n\n==External links==\n*[http://www.codecogs.com/library/engineering/materials/beams/multiple-continuous-beams.php CodeCogs: Continuous beams with more than one span]\n\n{{DEFAULTSORT:Theorem Of Three Moments}}\n[[Category:Structural analysis]]\n[[Category:Continuum mechanics]]\n[[Category:Physics theorems]]"
    },
    {
      "title": "Timoshenko beam theory",
      "url": "https://en.wikipedia.org/wiki/Timoshenko_beam_theory",
      "text": "<!--{{Continuum mechanics|cTopic=[[Solid mechanics]]}}-->\n[[File:TimoshenkoBeamPhoto_plain.svg|thumb|400px|Orientations of the line perpendicular to the mid-plane of a thick book under bending.]]\nThe '''Timoshenko beam theory''' was developed by [[Stephen Timoshenko]] early in the 20th century.<ref name=Timo1>Timoshenko, S. P., 1921, ''On the correction factor for shear of the differential equation for transverse vibrations of bars of uniform cross-section'', Philosophical Magazine, p. 744.</ref><ref name=Timo2>Timoshenko, S. P., 1922, ''On the transverse vibrations of bars of uniform cross-section'', Philosophical Magazine, p. 125.</ref> The model takes into account [[shearing (physics)|shear deformation]] and rotational [[bending]] effects, making it suitable for describing the behaviour of thick beams, [[sandwich-structured composite|sandwich composite beams]], or beams subject to high-[[frequency]] excitation when the [[wavelength]] approaches the thickness of the beam. The resulting equation is of 4th order but, unlike [[Euler–Bernoulli beam theory]], there is also a second-order partial derivative present. Physically, taking into account the added mechanisms of deformation effectively lowers the stiffness of the beam, while the result is a larger deflection under a static load and lower predicted [[eigenfrequency|eigenfrequencies]] for a given set of boundary conditions. The latter effect is more noticeable for higher frequencies as the wavelength becomes shorter (in principle comparable to the height of the beam or shorter), and thus the distance between opposing shear forces decreases.\n\nIf the [[shear modulus]] of the beam material approaches infinity—and thus the beam becomes rigid in shear—and if rotational inertia effects are neglected, Timoshenko beam theory converges towards ordinary beam theory.\n\n== Quasistatic Timoshenko beam ==\n[[Image:TimoshenkoBeam.svg|thumb|Deformation of a Timoshenko beam (blue) compared with that of an Euler-Bernoulli beam (red).]]\n[[Image:Plate theory.svg|thumb|Deformation of a Timoshenko beam. The normal rotates by an amount <math>\\theta_x = \\varphi(x)</math> which is not equal to <math>dw/dx</math>.]]\n\nIn [[statics|static]] Timoshenko beam theory without axial effects, the displacements of the beam are assumed to be given by\n:<math>\n  u_x(x,y,z) = -z~\\varphi(x) ~;~~ u_y(x,y,z) = 0 ~;~~ u_z(x,y) = w(x)\n</math>\nwhere <math>(x,y,z)</math> are the coordinates of a point in the beam, <math>u_x, u_y, u_z</math> are the components of the displacement vector in the three coordinate directions, <math>\\varphi</math> is the angle of rotation of the normal to the mid-surface of the beam, and <math>w</math> is the displacement of the mid-surface in the <math>z</math>-direction.\n\nThe governing equations are the following coupled system of [[ordinary differential equation]]s:\n:<math>\n \\begin{align}\n    & \\frac{\\mathrm{d}^2}{\\mathrm{d} x^2}\\left(EI\\frac{\\mathrm{d} \\varphi}{\\mathrm{d} x}\\right) = q(x) \\\\\n    & \\frac{\\mathrm{d} w}{\\mathrm{d} x} = \\varphi - \\frac{1}{\\kappa AG} \\frac{\\mathrm{d}}{\\mathrm{d} x}\\left(EI\\frac{\\mathrm{d} \\varphi}{\\mathrm{d} x}\\right).\n  \\end{align}\n</math>\n\nThe Timoshenko beam theory for the static case is equivalent to the [[Euler-Bernoulli beam equation|Euler-Bernoulli theory]] when the last term above is neglected, an approximation that is valid when\n:<math>\n\\frac{EI}{\\kappa L^2 A G} \\ll 1\n</math>\nwhere \n* <math>L</math> is the length of the beam.\n* <math>A</math> is the cross section area.\n* <math>E</math> is the [[elastic modulus]].\n* <math>G</math> is the [[shear modulus]].\n* <math>I</math> is the [[second moment of area]].\n* <math>\\kappa</math>, called the Timoshenko shear coefficient, depends on the geometry. Normally, <math>\\kappa = 5/6</math> for a rectangular section.\n* <math>q(x)</math> is a distributed load (force per length).\n\nCombining the two equations gives, for a homogeneous beam of constant cross-section,\n:<math>\n   EI~\\cfrac{\\mathrm{d}^4 w}{\\mathrm{d} x^4} = q(x) - \\cfrac{EI}{\\kappa A G}~\\cfrac{\\mathrm{d}^2 q}{\\mathrm{d} x^2}\n </math>\n\nThe bending moment <math>M_{xx}</math> and the shear force <math>Q_x</math> in the beam are related to the displacement <math>w</math> and the rotation <math>\\varphi</math>.  These relations, for a linear elastic Timoshenko beam, are:\n:<math>\n    M_{xx} = -EI~\\frac{\\partial \\varphi}{\\partial x} \\quad \\text{and} \\quad\n    Q_{x}  = \\kappa~AG~\\left(-\\varphi + \\frac{\\partial w}{\\partial x}\\right) \\,.\n</math>\n\n:{| class=\"toccolours collapsible collapsed\" width=\"60%\" style=\"text-align:left\"\n!Derivation of quasistatic Timoshenko beam equations\n|-\n|From the kinematic assumptions for a Timoshenko beam, the displacements of the beam are given by\n:<math>\n  u_x(x,y,z,t) = -z~\\varphi(x,t) ~;~~ u_y(x,y,z,t) = 0 ~;~~ u_z(x,y,z) = w(x,t)\n</math>\nThen, from the strain-displacement relations for small strains, the non-zero strains based on the Timoshenko assumptions are\n:<math>\n  \\varepsilon_{xx} = \\frac{\\partial u_x}{\\partial x} = -z~\\frac{\\partial \\varphi}{\\partial x} ~;~~\n  \\varepsilon_{xz} = \\frac{1}{2}\\left(\\frac{\\partial u_x}{\\partial z}+\\frac{\\partial u_z}{\\partial x}\\right)\n    = \\frac{1}{2}\\left(-\\varphi + \\frac{\\partial w}{\\partial x}\\right)\n</math>\nSince the actual shear strain in the beam is not constant over the cross section we introduce a correction factor <math>\\kappa</math> such that\n:<math>\n  \\varepsilon_{xz} = \\frac{1}{2}~\\kappa~\\left(-\\varphi + \\frac{\\partial w}{\\partial x}\\right)\n</math>\nThe variation in the internal energy of the beam is\n:<math>\n  \\delta U = \\int_L \\int_A (\\sigma_{xx}\\delta\\varepsilon_{xx} + 2\\sigma_{xz}\\delta\\varepsilon_{xz})~\\mathrm{d}A~\\mathrm{d}L \n   = \\int_L \\int_A \\left[-z~\\sigma_{xx}\\frac{\\partial (\\delta\\varphi)}{\\partial x} + \\sigma_{xz}~\\kappa\\left(-\\delta\\varphi + \\frac{\\partial (\\delta w)}{\\partial x}\\right)\\right]~\\mathrm{d}A~\\mathrm{d}L \n</math>\nDefine\n:<math>\n   M_{xx} := \\int_A z~\\sigma_{xx}~\\mathrm{d}A ~;~~ Q_x := \\kappa~\\int_A \\sigma_{xz}~\\mathrm{d}A\n</math>\nThen\n:<math>\n   \\delta U = \\int_L \\left[-M_{xx}\\frac{\\partial (\\delta\\varphi)}{\\partial x} + Q_{x}\\left(-\\delta\\varphi + \\frac{\\partial (\\delta w)}{\\partial x}\\right)\\right]~\\mathrm{d}L \n</math>\nIntegration by parts, and noting that because of the boundary conditions the variations are zero at the ends of the beam, leads to\n:<math>\n   \\delta U = \\int_L \\left[\\left(\\frac{\\partial M_{xx}}{\\partial x} - Q_x\\right)~\\delta\\varphi - \\frac{\\partial Q_{x}}{\\partial x}~\\delta w\\right]~\\mathrm{d}L \n</math>\nThe variation in the external work done on the beam by a transverse load <math>q(x,t)</math> per unit length is\n:<math>\n  \\delta W = \\int_L q~\\delta w~\\mathrm{d}L\n</math>\nThen, for a quasistatic beam, the principle of virtual work gives\n:<math>\n   \\delta U = \\delta W \\implies\n   \\int_L \\left[\\left(\\frac{\\partial M_{xx}}{\\partial x} - Q_x\\right)~\\delta\\varphi - \\left(\\frac{\\partial Q_{x}}{\\partial x} + q\\right)~\\delta w\\right]~\\mathrm{d}L = 0\n</math>\nThe governing equations for the beam are, from the fundamental theorem of variational calculus,\n:<math>\n   \\frac{\\partial M_{xx}}{\\partial x} - Q_x = 0 ~;~~ \\frac{\\partial Q_{x}}{\\partial x} + q = 0\n</math>\nFor a linear elastic beam \n:<math>\n  \\begin{align}\n    M_{xx} & = \\int_A z~\\sigma_{xx}~\\mathrm{d}A = \\int_A z~E~\\varepsilon_{xx}~\\mathrm{d}A = \n     -\\int_A z^2~E~\\frac{\\partial \\varphi}{\\partial x}~\\mathrm{d}A = -EI~\\frac{\\partial \\varphi}{\\partial x} \\\\\n    Q_{x} & = \\int_A \\sigma_{xz}~\\mathrm{d}A = \\int_A 2G~\\varepsilon_{xz}~\\mathrm{d}A = \n     \\int_A \\kappa~G~\\left(-\\varphi + \\frac{\\partial w}{\\partial x}\\right)~\\mathrm{d}A = \\kappa~AG~\\left(-\\varphi + \\frac{\\partial w}{\\partial x}\\right)\n  \\end{align}\n</math>\nTherefore the governing equations for the beam may be expressed as\n:<math>\n  \\begin{align}\n    \\frac{\\partial }{\\partial x}\\left(EI\\frac{\\partial \\varphi}{\\partial x}\\right) + \\kappa AG~\\left(\\frac{\\partial w}{\\partial x}-\\varphi\\right) & = 0 \\\\\n    \\frac{\\partial }{\\partial x}\\left[\\kappa AG\\left(\\frac{\\partial w}{\\partial x} - \\varphi\\right)\\right] + q & = 0\n  \\end{align}\n</math>\nCombining the two equations together gives\n:<math>\n  \\begin{align}\n    & \\frac{\\partial^2 }{\\partial x^2}\\left(EI\\frac{\\partial \\varphi}{\\partial x}\\right) = q \\\\\n    & \\frac{\\partial w}{\\partial x} = \\varphi - \\cfrac{1}{\\kappa AG}~\\frac{\\partial }{\\partial x}\\left(EI\\frac{\\partial \\varphi}{\\partial x}\\right) \n  \\end{align}\n</math>\n|}\n\n=== Boundary conditions ===\nThe two equations that describe the deformation of a Timoshenko beam have to be augmented with [[boundary condition]]s if they are to be solved.  Four boundary conditions are needed for the problem to be [[well-posed problem|well-posed]].  Typical boundary conditions are:\n* '''Simply supported beams''': The displacement <math>w</math> is zero at the locations of the two supports.  The [[bending moment]] <math>M_{xx}</math> applied to the beam also has to be specified.  The rotation <math>\\varphi</math> and the transverse shear force <math>Q_x</math> are not specified.\n* '''Clamped beams''': The displacement <math>w</math> and the rotation <math>\\varphi</math> are specified to be zero at the clamped end.  If one end is free, shear force <math>Q_x</math> and bending moment <math>M_{xx}</math> have to be specified at that end.\n\n=== Example: Cantilever beam ===\n[[File:TimoCantBeamPointLoad.svg|thumb|350px|A cantilever Timoshenko beam under a point load at the free end]]\n\nFor a [[cantilever beam]], one boundary is clamped while the other is free.  Let us use a [[Orientation (vector space)|right handed coordinate system]] where the <math>x</math> direction is positive towards right and the <math>z</math> direction is positive upward.  Following normal convention, we assume that positive forces act in the positive directions of the <math>x</math> and <math>z</math> axes and positive moments act in the clockwise direction.  We also assume that the sign convention of the [[stress resultants]] (<math>M_{xx}</math> and <math>Q_x</math>) is such that positive bending moments compress the material at the bottom of the beam (lower <math>z</math> coordinates) and positive shear forces rotate the beam in a counterclockwise direction.\n\nLet us assume that the clamped end is at <math>x=L</math> and the free end is at <math>x=0</math>.  If a point load <math>P</math> is applied to the free end in the positive <math>z</math> direction, a [[free body diagram]] of the beam gives us\n:<math>\n   -Px - M_{xx} = 0 \\implies M_{xx} = -Px \n </math>\nand\n:<math> P + Q_x = 0 \\implies Q_x = -P\\,.\n </math>\nTherefore, from the expressions for the bending moment and shear force, we have\n:<math>\n   Px = EI\\,\\frac{d\\varphi}{dx} \\qquad \\text{and} \\qquad -P = \\kappa AG\\left(-\\varphi + \\frac{dw}{dx}\\right) \\,.\n </math>\nIntegration of the first equation, and application of the boundary condition <math>\\varphi = 0</math> at <math>x = L</math>, leads to\n:<math>\n    \\varphi(x) = -\\frac{P}{2EI}\\,(L^2-x^2) \\,.\n </math>\nThe second equation can then be written as\n:<math>\n   \\frac{dw}{dx} = -\\frac{P}{\\kappa AG} - \\frac{P}{2EI}\\,(L^2-x^2)\\,.\n </math>\nIntegration and application of the boundary condition <math>w = 0</math> at <math>x = L</math> gives\n:<math>\n   w(x) = \\frac{P(L-x)}{\\kappa AG} - \\frac{Px}{2EI}\\,\\left(L^2-\\frac{x^2}{3}\\right) + \\frac{PL^3}{3EI} \\,.\n </math>\nThe axial stress is given by\n:<math>\n   \\sigma_{xx}(x,z) = E\\,\\varepsilon_{xx} = -E\\,z\\,\\frac{d\\varphi}{dx} = -\\frac{Pxz}{I} = \\frac{M_{xx}z}{I} \\,.\n </math>\n\n== Dynamic Timoshenko beam ==\nIn Timoshenko beam theory without axial effects, the displacements of the beam are assumed to be given by\n:<math>\n  u_x(x,y,z,t) = -z~\\varphi(x,t) ~;~~ u_y(x,y,z,t) = 0 ~;~~ u_z(x,y,z,t) = w(x,t)\n</math>\nwhere <math>(x,y,z)</math> are the coordinates of a point in the beam, <math>u_x, u_y, u_z</math> are the components of the displacement vector in the three coordinate directions, <math>\\varphi</math> is the angle of rotation of the normal to the mid-surface of the beam, and <math>w</math> is the displacement of the mid-surface in the <math>z</math>-direction.\n\nStarting from the above assumption, the Timoshenko beam theory, allowing for vibrations, may be described with the coupled linear [[partial differential equations]]:<ref>[http://ccrma.stanford.edu/~bilbao/master/node163.html Timoshenko's Beam Equations<!-- Bot generated title -->]</ref>\n\n:<math>\n\\rho A\\frac{\\partial^{2}w}{\\partial t^{2}} - q(x,t) = \\frac{\\partial}{\\partial x}\\left[ \\kappa AG \\left(\\frac{\\partial w}{\\partial x}-\\varphi\\right)\\right]\n</math>\n\n:<math>\n\\rho I\\frac{\\partial^{2}\\varphi}{\\partial t^{2}} = \\frac{\\partial}{\\partial x}\\left(EI\\frac{\\partial \\varphi}{\\partial x}\\right)+\\kappa AG\\left(\\frac{\\partial w}{\\partial x}-\\varphi\\right)\n</math>\n\nwhere the dependent variables are <math>w(x,t)</math>, the translational displacement of the beam, and <math>\\varphi(x,t)</math>, the angular displacement. Note that unlike the [[Euler-Bernoulli beam equation|Euler-Bernoulli]] theory, the angular deflection is another variable and not approximated by the slope of the deflection. Also,\n\n* <math>\\rho</math> is the [[density]] of the beam material (but not the [[linear density]]).\n* <math>A</math> is the cross section area.\n* <math>E</math> is the [[elastic modulus]].\n* <math>G</math> is the [[shear modulus]].\n* <math>I</math> is the [[second moment of area]].\n* <math>\\kappa</math>, called the Timoshenko shear coefficient, depends on the geometry. Normally, <math>\\kappa = 5/6</math> for a rectangular section.\n* <math>q(x,t)</math> is a distributed load (force per length).\n* <math>m := \\rho A</math>\n* <math>J := \\rho I</math>\n\nThese parameters are not necessarily constants.\n\nFor a linear elastic, isotropic, homogeneous beam of constant cross-section these two equations can be combined to give<ref name=Thomson>Thomson, W. T., 1981, ''Theory of Vibration with Applications'', second edition. Prentice-Hall, New Jersey.</ref><ref name=Rosinger>Rosinger, H. E. and Ritchie, I. G., 1977, ''On Timoshenko's correction for shear in vibrating isotropic beams'', J. Phys. D: Appl. Phys., vol. 10, pp. 1461-1466.</ref>\n:<math>\n   EI~\\cfrac{\\partial^4 w}{\\partial x^4} + m~\\cfrac{\\partial^2 w}{\\partial t^2} - \\left(J + \\cfrac{E I m}{\\kappa A G}\\right)\\cfrac{\\partial^4 w}{\\partial x^2~\\partial t^2} + \\cfrac{m J}{\\kappa A G}~\\cfrac{\\partial^4 w}{\\partial t^4} = q(x,t) + \\cfrac{J}{\\kappa A G}~\\cfrac{\\partial^2 q}{\\partial t^2} - \\cfrac{EI}{\\kappa A G}~\\cfrac{\\partial^2 q}{\\partial x^2}\n </math>\n:{| class=\"toccolours collapsible collapsed\" width=\"60%\" style=\"text-align:left\"\n!Derivation of combined Timoshenko beam equation\n|-\n|The equations governing the bending of a homogeneous Timoshenko beam of constant cross-section are\n:<math>\n  \\begin{align}\n    (1) & & \\quad m~\\frac{\\partial^2 w}{\\partial t^2} & = \\kappa AG~\\left(\\frac{\\partial^2 w}{\\partial x^2} - \\frac{\\partial \\varphi}{\\partial x}\\right) + q(x,t) ~;~~ m := \\rho A \\\\\n    (2) & & \\quad J~\\frac{\\partial^2 \\varphi}{\\partial t^2} & = EI~\\frac{\\partial^2 \\varphi}{\\partial x^2} + \\kappa AG~\\left(\\frac{\\partial w}{\\partial x} - \\varphi\\right) ~;~~ J := \\rho I\n  \\end{align}\n</math>\nFrom equation (1), assuming appropriate smoothness, we have\n:<math>\n  \\begin{align}\n    (3) & & \\quad \\frac{\\partial \\varphi}{\\partial x} & = -\\cfrac{m}{\\kappa AG}~\\frac{\\partial^2 w}{\\partial t^2} + \\frac{\\partial^2 w}{\\partial x^2} + \\cfrac{q}{\\kappa AG} \\\\\n    (4) & & \\quad \\frac{\\partial^2 q}{\\partial t^2} & = m~\\cfrac{\\partial^4 w}{\\partial t^4} - \\kappa AG~\\left(\\cfrac{\\partial^4 w}{\\partial x^2\\partial t^2} - \\cfrac{\\partial^3\\varphi}{\\partial x\\partial t^2}\\right)\n  \\end{align}\n</math>\nFrom (3), assuming appropriate smoothness, \n:<math>\n    (5) \\qquad \\cfrac{\\partial^3\\varphi}{\\partial x^3} = -\\cfrac{m}{\\kappa AG}~\\cfrac{\\partial^4 w}{\\partial x^2\\partial t^2} + \\cfrac{\\partial^4 w}{\\partial x^4} + \\cfrac{1}{\\kappa AG}~\\frac{\\partial^2 q}{\\partial x^2}\n</math>\nDifferentiating equation (2) gives\n:<math>\n    (6) \\qquad \\cfrac{\\partial^3\\varphi}{\\partial x \\partial t^2} = \\cfrac{EI}{J}~\\cfrac{\\partial^3 \\varphi}{\\partial x^3} + \\cfrac{\\kappa AG}{J}~\\left(\\frac{\\partial^2 w}{\\partial x^2} - \\frac{\\partial \\varphi}{\\partial x}\\right)\n</math>\nFrom equations (4) and (6)\n:<math>\n    (7) \\qquad \n    \\cfrac{1}{\\kappa AG}~\\frac{\\partial^2 q}{\\partial t^2} -\\cfrac{m}{\\kappa AG}~\\cfrac{\\partial^4 w}{\\partial t^4} + \\cfrac{\\partial^4 w}{\\partial x^2\\partial t^2} \n= \\cfrac{EI}{J}~\\cfrac{\\partial^3 \\varphi}{\\partial x^3} + \\cfrac{\\kappa AG}{J}~\\left(\\frac{\\partial^2 w}{\\partial x^2} - \\frac{\\partial \\varphi}{\\partial x}\\right)\n</math>\nFrom equations (3) and (7)\n:<math>\n    (8) \\qquad \n    \\cfrac{1}{\\kappa AG}~\\frac{\\partial^2 q}{\\partial t^2} -\\cfrac{m}{\\kappa AG}~\\cfrac{\\partial^4 w}{\\partial t^4} + \\cfrac{\\partial^4 w}{\\partial x^2\\partial t^2} \n= \\cfrac{EI}{J}~\\cfrac{\\partial^3 \\varphi}{\\partial x^3} + \\cfrac{m}{J}~\\frac{\\partial^2 w}{\\partial t^2} - \\cfrac{q}{J}\n</math>\nPlugging equation (5) into (8) gives\n:<math>\n    (9) \\qquad \n    \\cfrac{J}{\\kappa AG}~\\frac{\\partial^2 q}{\\partial t^2} -\\cfrac{mJ}{\\kappa AG}~\\cfrac{\\partial^4 w}{\\partial t^4} + J~\\cfrac{\\partial^4 w}{\\partial x^2\\partial t^2} \n= -\\cfrac{mEI}{\\kappa AG}~\\cfrac{\\partial^4 w}{\\partial x^2\\partial t^2} + EI~\\cfrac{\\partial^4 w}{\\partial x^4} + \\cfrac{EI}{\\kappa AG}~\\frac{\\partial^2 q}{\\partial x^2}\n + m~\\frac{\\partial^2 w}{\\partial t^2} - q\n</math>\nRearrange to get\n:<math>\n   EI~\\cfrac{\\partial^4 w}{\\partial x^4} + m~\\frac{\\partial^2 w}{\\partial t^2} - \\left(J+\\cfrac{mEI}{\\kappa AG}\\right)~\\cfrac{\\partial^4 w}{\\partial x^2 \\partial t^2} + \\cfrac{mJ}{\\kappa AG}~\\cfrac{\\partial^4 w}{\\partial t^4} = q + \\cfrac{J}{\\kappa AG}~\\frac{\\partial^2 q}{\\partial t^2} - \\cfrac{EI}{\\kappa A G}~\\frac{\\partial^2 q}{\\partial x^2}\\quad\\square\n</math>\n|}\nThe Timoshenko equation predicts a critical frequency \n<math>\n \\omega_C=2 \\pi f_c=\\sqrt{\\frac{\\kappa GA}{\\rho I}}.    \n</math>\nFor normal modes the Timoshenko equation can be solved. Being a fourth order equation, there are four independent solutions, two oscillatory and two evanescent for frequencies below <math>f_c</math>.   \nFor frequencies larger than <math>f_c</math> all solutions are oscillatory and, as consequence, a second spectrum appears.<ref>\"Experimental study of the Timoshenko beam theory predictions\", A. Díaz-de-Anda, J. Flores, L. Gutiérrez, R.A. Méndez-Sánchez, G. Monsivais, and A. Morales, Journal of Sound and Vibration, Volume 331, Issue 26, 17 December 2012, pp. 5732–5744.</ref>\n\n=== Axial effects ===\nIf the displacements of the beam are given by\n:<math>\n  u_x(x,y,z,t) = u_0(x,t)-z~\\varphi(x,t) ~;~~ u_y(x,y,z,t) = 0 ~;~~ u_z(x,y,z,t) = w(x,t)\n</math>\nwhere <math>u_0</math> is an additional displacement in the <math>x</math>-direction, then the governing equations of a Timoshenko beam take the form\n:<math>\n  \\begin{align}\nm \\frac{\\partial^{2}w}{\\partial t^{2}} & = \\frac{\\partial}{\\partial x}\\left[ \\kappa AG \\left(\\frac{\\partial w}{\\partial x}-\\varphi\\right)\\right] + q(x,t) \\\\\nJ \\frac{\\partial^{2}\\varphi}{\\partial t^{2}} & = N(x,t)~\\frac{\\partial w}{\\partial x} + \\frac{\\partial}{\\partial x}\\left(EI\\frac{\\partial \\varphi}{\\partial x}\\right)+\\kappa AG\\left(\\frac{\\partial w}{\\partial x}-\\varphi\\right)\n  \\end{align}\n</math>\nwhere <math>J = \\rho I</math> and <math>N(x,t)</math> is an externally applied axial force.  Any external axial force is balanced by the stress resultant\n:<math>\n   N_{xx}(x,t) = \\int_{-h}^{h} \\sigma_{xx}~dz\n </math>\nwhere <math>\\sigma_{xx}</math> is the axial stress and the thickness of the beam has been assumed to be <math>2h</math>.\n\nThe combined beam equation with axial force effects included is\n:<math>\n   EI~\\cfrac{\\partial^4 w}{\\partial x^4} + N~\\cfrac{\\partial^2 w}{\\partial x^2} + m~\\frac{\\partial^2 w}{\\partial t^2} - \\left(J+\\cfrac{mEI}{\\kappa AG}\\right)~\\cfrac{\\partial^4 w}{\\partial x^2 \\partial t^2} + \\cfrac{mJ}{\\kappa AG}~\\cfrac{\\partial^4 w}{\\partial t^4} = q + \\cfrac{J}{\\kappa AG}~\\frac{\\partial^2 q}{\\partial t^2} - \\cfrac{EI}{\\kappa A G}~\\frac{\\partial^2 q}{\\partial x^2}\n</math>\n\n=== Damping ===\nIf, in addition to axial forces, we assume a damping force that is proportional to the velocity with the form\n:<math>\n   \\eta(x)~\\cfrac{\\partial w}{\\partial t}\n </math>\nthe coupled governing equations for a Timoshenko beam take the form\n:<math>\nm \\frac{\\partial^{2}w}{\\partial t^{2}} + \\eta(x)~\\cfrac{\\partial w}{\\partial t} = \\frac{\\partial}{\\partial x}\\left[ \\kappa AG \\left(\\frac{\\partial w}{\\partial x}-\\varphi\\right)\\right] + q(x,t)\n</math>\n\n:<math>\nJ \\frac{\\partial^{2}\\varphi}{\\partial t^{2}} = N\\frac{\\partial w}{\\partial x} + \\frac{\\partial}{\\partial x}\\left(EI\\frac{\\partial \\varphi}{\\partial x}\\right)+\\kappa AG\\left(\\frac{\\partial w}{\\partial x}-\\varphi\\right)\n</math>\nand the combined equation becomes\n:<math>\n  \\begin{align}\n   EI~\\cfrac{\\partial^4 w}{\\partial x^4} & + N~\\cfrac{\\partial^2 w}{\\partial x^2} + m~\\frac{\\partial^2 w}{\\partial t^2} - \\left(J+\\cfrac{mEI}{\\kappa AG}\\right)~\\cfrac{\\partial^4 w}{\\partial x^2 \\partial t^2} + \\cfrac{mJ}{\\kappa AG}~\\cfrac{\\partial^4 w}{\\partial t^4} + \\cfrac{J \\eta(x)}{\\kappa AG}~\\cfrac{\\partial^3 w}{\\partial t^3} \\\\\n  & -\\cfrac{EI}{\\kappa AG}~\\cfrac{\\partial^2}{\\partial x^2}\\left(\\eta(x)\\cfrac{\\partial w}{\\partial t}\\right) + \\eta(x)\\cfrac{\\partial w}{\\partial t} = q + \\cfrac{J}{\\kappa AG}~\\frac{\\partial^2 q}{\\partial t^2} - \\cfrac{EI}{\\kappa A G}~\\frac{\\partial^2 q}{\\partial x^2}\n  \\end{align}\n</math>\n\nA caveat to this Ansatz damping force (resembling viscosity) is that, whereas viscosity leads to a frequency-dependent and amplitude-independent damping rate of beam oscillations, the empirically measured damping rates are frequency-insensitive, but depend on the amplitude of beam deflection.\n\n== Shear coefficient ==\nDetermining the shear coefficient is not straightforward (nor are the determined values widely accepted, i.e. there's more than one answer); generally it must satisfy:\n:<math>\\int_A \\tau dA = \\kappa A G \\varphi\\,</math> .\n\nThe shear coefficient depends on the Poisson's ratio. The attempts to provide precise expressions were made by many scientists, including [[Stephen Timoshenko]],<ref>Timoshenko, Stephen P., 1932, ''Schwingungsprobleme der Technik'', Julius Springer.</ref> [[Raymond D. Mindlin]],<ref>Mindlin, R. D., Deresiewicz, H., 1953, ''Timoshenko's Shear Coefficient for Flexural Vibrations of Beams'', Technical Report No. 10, ONR Project NR064-388, Department of Civil Engineering, Columbia University, New York, N.Y.</ref> G. R. Cowper,<ref>Cowper, G. R., 1966, \"The Shear Coefficient in Timoshenko’s Beam Theory\", J. Appl. Mech., Vol. 33, No.2, pp. 335–340.</ref> N. G. Stephen,<ref>Stephen, N. G., 1980. \"Timoshenko’s shear coefficient from a beam subjected to gravity loading\", Journal of Applied Mechanics, Vol. 47, No. 1, pp. 121–127.</ref> J. R. Hutchinson<ref>Hutchinson, J. R., 1981, \"Transverse vibration of beams, exact versus approximate solutions\", Journal of Applied Mechanics, Vol. 48, No. 12, pp. 923–928.</ref> etc. (see also the derivation of the Timoshenko beam theory as refined beam theory based on the variational-asymptotic method in the book by Khanh C. Le<ref>Le, Khanh C., 1999, ''Vibrations of shells and rods'', Springer.</ref> leading to the different shear coefficients in the static and dynamic cases). In engineering practice, the expressions by [[Stephen Timoshenko]]<ref>Stephen Timoshenko, James M. Gere. Mechanics of Materials. Van Nostrand Reinhold Co., 1972. pages 207.</ref> are sufficient in most cases. In 1975 Kaneko<ref>Kaneko, T., 1975, \"On Timoshenko's correction for shear in vibrating beams\", J. Phys. D: Appl. Phys., Vol. 8, pp. 1927–1936.</ref> published an excellent review of studies of the shear coefficient. More recently new experimental data show that the shear coefficient is underestimated.<ref>\"Experimental check on the accuracy of Timoshenko’s beam theory\", R. A. Méndez-Sáchez, A. Morales, J. Flores, Journal of Sound and Vibration 279 (2005) 508–512.</ref><ref>\"On the Accuracy of the Timoshenko Beam Theory Above the Critical Frequency: Best Shear Coefficient\", J. A. Franco-Villafañe and R. A. Méndez-Sánchez, Journal of Mechanics, January 2016, pp. 1–4. DOI: 10.1017/jmech.2015.104.</ref>\n\nAccording to Cowper (1966) for solid rectangular cross-section,\n:<math>\n\\kappa = \\cfrac{10(1+\\nu)}{12+11\\nu}\n</math>\n\nand for solid circular cross-section,\n:<math>\n\\kappa = \\cfrac{6(1+\\nu)}{7+6\\nu}\n</math>.\n\n==See also==\n* [[Plate theory]]\n* [[Sandwich theory]]\n\n==References==\n{{Reflist|30em}}\n\n[[Category:Continuum mechanics]]\n[[Category:Structural analysis]]"
    },
    {
      "title": "Torsion constant",
      "url": "https://en.wikipedia.org/wiki/Torsion_constant",
      "text": "The '''torsion constant''' is a geometrical property of a bar's cross-section which is involved in the relationship between angle of twist and applied torque along the axis of the bar, for a homogeneous linear-elastic bar. The torsion constant, together with material properties and length, describes a bar's torsional [[stiffness]]. The SI unit for torsion constant is m<sup>4</sup>.\n\n== History ==\nIn 1820, the French engineer A. Duleau derived analytically that the torsion constant of a beam is identical to the [[second moment of area]] normal to the section J<sub>zz</sub>, which has an exact analytic equation, by assuming that a plane section before twisting remains planar after twisting, and a diameter remains a straight line.\nUnfortunately, that assumption is correct only in beams with circular cross-sections, and is incorrect for any other shape where warping takes place.<ref>\nArchie Higdon et al.\n\"Mechanics of Materials, 4th edition\".\n</ref>\n\nFor non-circular cross-sections, there are no exact analytical equations for finding the torsion constant. However, approximate solutions have been found for many shapes.\nNon-circular cross-sections always have warping deformations that require numerical methods to allow for the exact calculation of the torsion constant.<ref name=\"David\">Advanced structural mechanics, 2nd Edition, David Johnson</ref>\n\nThe torsional stiffness of beams with non-circular cross sections is significantly increased if the warping of the end sections is restrained by, for example, stiff end blocks.<ref>[http://www.ramsay-maunder.co.uk/downloads/warping_article_web.pdf The Influence and Modelling of Warping Restraint on Beams]</ref>\n\n== Partial Derivation ==\nFor a beam of uniform cross-section along its length:\n:<math>\\theta = \\frac{TL}{GJ}</math>\nwhere\n:<math>\\theta</math> is the angle of twist in radians\n:''T'' is the applied torque\n:''L'' is the beam length\n:''G'' is the [[Modulus of rigidity]] (shear modulus) of the material\n:''J'' is the torsional constant\n[[File:TorsionConstantBar.svg]]\n\n== Torsional Rigidity (GJ) and Stiffness (GJ/L) ==\nInverting the previous relation, we can define two quantities: the torsional rigidity\n\n<math>GJ = \\frac{TL}{\\theta}</math> with dimensions <math>[GJ] =</math> N*m<sup>2</sup>/rad\n\nAnd the torsional stiffness:\n\n<math>\\frac{GJ}{L} = \\frac{T}{\\theta}</math> with dimensions <math>[GJ/L] =</math> N*m/rad\n\n\nwhere\n:<math>\\theta</math> is the angle of twist in radians\n:''T'' is the applied torque in N*m\n:''L'' is the beam length in m\n\n==Examples for specific uniform cross-sectional shapes==\n\n===Circle===\n:<math>J_{zz} = J_{xx}+J_{yy} = \\frac{\\pi r^4}{4} + \\frac{\\pi r^4}{4} = \\frac{\\pi r^4}{2}</math><ref name=\"Weisstein, Eric W.\">\"Area Moment of Inertia.\" From MathWorld--A Wolfram Web Resource. http://mathworld.wolfram.com/AreaMomentofInertia.html</ref>\nwhere\n:''r'' is the radius\nThis is identical to the [[second moment of area]] J<sub>zz</sub> and is exact.\n\nalternatively write: <math>J = \\frac{\\pi D^4}{32}</math><ref name=\"Weisstein, Eric W.\"/>\nwhere\n:''D'' is the Diameter\n\n===Ellipse===\n:<math>J \\approx \\frac{\\pi a^3 b^3}{a^2 + b^2}</math><ref name=\"Roark7\">Roark's Formulas for stress & Strain, 7th Edition, Warren C. Young & Richard G. Budynas</ref><ref name=\"Irjens\">Continuum Mechanics, Fridtjov Irjens, Springer 2008, p238, {{ISBN|978-3-540-74297-5}}</ref>\nwhere\n:''a'' is the major radius \n:''b'' is the minor radius\n\n===Square===\n:<math>J \\approx \\,2.25 a^4</math><ref name=\"RoyMech7\">Torsion Equations, Roy Beardmore, http://www.roymech.co.uk/Useful_Tables/Torsion/Torsion.html</ref>\nwhere\n:''a'' is half the side length\n\n===Rectangle===\n:<math>J \\approx\\beta a b^3</math>\nwhere\n:''a'' is the length of the long side\n:''b'' is the length of the short side\n:<math>\\beta</math> is found from the following table:\n{| class=\"wikitable\"\n|-\n! a/b\n! <math>\\beta</math>\n|-\n| 1.0\n| 0.141\n|-\n| 1.5\n| 0.196\n|-\n| 2.0\n| 0.229\n|-\n| 2.5\n| 0.249\n|-\n| 3.0\n| 0.263\n|-\n| 4.0\n| 0.281\n|-\n| 5.0\n| 0.291\n|-\n| 6.0\n| 0.299\n|-\n| 10.0\n| 0.312\n|-\n| <math>\\infty</math>\n| 0.333\n|}<ref>Advanced Strength and Applied Elasticity, Ugural & Fenster, Elsevier, {{ISBN|0-444-00160-3}}</ref>\n\nAlternatively the following equation can be used with an error of not greater than 4%:<br>\n:<math>J \\approx a b^3 \\left ( \\frac{16}{3}-3.36 \\frac{b}{a} \\left ( 1- \\frac{b^4}{12a^4} \\right ) \\right )</math><ref name=\"Roark7\" />\n\nIn the formula above, a and b are ''half'' the length of the long and short sides, respectively.\n\n===Thin walled open tube of uniform thickness===\n:<math>J = \\frac{1}{3}Ut^3</math><ref>Advanced Mechanics of Materials, Boresi, John Wiley & Sons, {{ISBN|0-471-55157-0}}</ref>\n:''t'' is the wall thickness\n:''U'' is the length of the median boundary (perimeter of median cross section)\n\n===Circular thin walled open tube of uniform thickness (approximation)===\nThis is a tube with a slit cut longitudinally through its wall.\n:<math>J = \\frac{2}{3} \\pi r t^3</math><ref name=\"Roark\">Roark's Formulas for stress & Strain, 6th Edition, Warren C. Young</ref>\n:''t'' is the wall thickness\n:''r'' is the mean radius\nThis is derived from the above equation for an arbitrary thin walled open tube of uniform thickness.\n\n==References==\n{{reflist}}\n\n==External links==\n* [https://www.fxsolver.com/browse/?q=torsion Torsion constant calculator]\n\n{{DEFAULTSORT:Torsion Constant}}\n[[Category:Continuum mechanics]]\n[[Category:Structural analysis]]"
    },
    {
      "title": "Transmissibility (structural dynamics)",
      "url": "https://en.wikipedia.org/wiki/Transmissibility_%28structural_dynamics%29",
      "text": "'''Transmissibility''', in the context of [[Structural Dynamics]], can be defined as the ratio of the maximum force (<math>f_{max}</math>) on the floor as a result of the vibration of a machine to the maximum machine force (<math>P_0</math>):\n\n:<math>TR = \\frac{f_{max}}{P_0} = R_d\\sqrt{1+(2\\zeta\\beta)^2}</math>\n\n:Where <math>\\zeta</math> is equal to the [[damping ratio]] and <math>\\beta</math> is equal to the frequency ratio. <math>R_d</math> is the ratio of the dynamic to static amplitude.\n\n==Further reading==\n* [http://personal.cityu.edu.hk/~bsapplec/transmis3.htm Vibration Control and Measurement]\n* [http://www.optimumg.com/docs/Springs&Dampers_Tech_Tip_4.pdf Tech Tip: Spring & Dampers, Episode Four]\n\n[[Category:Structural analysis]]"
    },
    {
      "title": "Unified framework",
      "url": "https://en.wikipedia.org/wiki/Unified_framework",
      "text": "'''Unified framework''' is a general formulation which yields ''n''<sup>th</sup> - order expressions giving mode shapes and natural frequencies for damaged elastic structures such as rods, beams, plates, and shells. The formulation is applicable to structures with any shape of damage or those having more than one area of damage. The formulation uses the geometric definition of the discontinuity at the damage location and perturbation to modes and natural frequencies of the undamaged structure to determine the mode shapes and natural frequencies of the damaged structure. The geometric discontinuity at the damage location manifests itself in terms of discontinuities in the cross-sectional properties, such as the depth of the structure, the cross-sectional area or the area moment of inertia. The change in cross-sectional properties in turn affects the stiffness and mass distribution. Considering the geometric discontinuity along with the perturbation of modes and natural frequencies, the initial homogeneous differential equation with nonconstant coefficients is changed to a series of non-homogeneous differential equations with constant coefficients. Solutions of this series of differential equations is obtained in this framework.\n\nThis framework is about using structural-dynamics based methods to address the existing challenges in the field of [[structural health monitoring]] (SHM).<ref>Akash Dixit - Damage Modeling and Damage Detection For Structures Using A Perurbation Method (May 2012)</ref> It makes no ad hoc assumptions regarding the physical behavior at the damage location such as adding fictitious springs or modeling changes in [[Young's modulus]].\n\n== Introduction ==\nStructural health monitoring (SHM) is a rapidly expanding field both in academia and research.{{cn|date=August 2017}} Most of the literature on SHM is based on experimental observations and physically expected models.{{cn|date=August 2017}} There are some mathematical models that give analytical theory to model the damage. Such mathematical models for structures with damage are useful in two ways. They allow understanding of the physics behind the problem, which helps in the explanation of experimental readings, and they allow prediction of response of the structure. These studies are also useful for the development of new experimental techniques.{{cn|date=August 2017}}\n\nExamples of models based on expected physical behavior of damage are by Ismail et al. (1990),<ref>Ismail, F., Ibrahim, A., Martin, H.K., 1990. Identification of fatigue cracks from vibration testing. Journal of Sound and Vibration 140, 305–317.</ref> who modeled the rectangular edge defect as a spring, by Ostachowicz and Krawczuk (1991),<ref>Ostachowicz, W., Krawczuk, M., 1991. Analysis of the effect of cracks on the natural frequencies of a cantilever beam. Journal of Sound and Vibrations 150,191–201.</ref> who modeled the damage as an elastic hinge and by Thompson (1949),<ref>Thompson, W.T., 1949. Vibration of slender bars with discontinuities in stiffness. Journal of Applied Mechanics 16, 203–207.</ref> who modeled the damage as a concentrated couple at the location of the damage. Other models based on expected physical behavior are by Joshi and Madhusudhan (1991),<ref>Joshi, A., Madhusudhan, B.S., 1991. A unified approach to free vibration of locally damaged beams having various homogeneous boundary conditions. Journal of Sound and Vibration 147, 475–488.</ref> who modeled the damage as a zone with reduced Young’s modulus and by Ballo (1999),<ref>Ballo, I., 1999. Non-linear effects of vibration of a continuous transverse cracked slender shaft. Journal of Sound and Vibration 217 (2), 321–333.</ref> who modeled it as spring with nonlinear stiffness. Krawczuk (2002)<ref>Krawczuk, M., 2002. Application of spectral beam finite element with a crack and iterative search technique for damage detection. Finite Elements in Analysis and Design 9–10, 991–1004.</ref> used an extensional spring at the damage location, with its flexibility determined using the stress intensity factors ''K<sub>I</sub>''. Approximate methods to model the crack are by Chondros et al. (1998),<ref>Chondros, T., Dimarogonas, A., Yao, J., 1998. A continuous cracked beams vibration theory. Journal of Sound and Vibration 215 (1), 17–34.</ref> who used a so-called crack function as an additional term in the axial displacement of [[Euler–Bernoulli beam theory|Euler–Bernoulli beams]]. The crack functions were determined using stress intensity factors ''K<sub>I</sub>'', ''K<sub>II</sub>'' and ''K<sub>III</sub>''. Christides and Barr (1984)<ref>Christides, S., Barr, A.D.S., 1984. One-dimensional theory of cracked Euler–Bernoulli\nbeams. International Journal of Mechanical Sciences 26 (11–12), 339–348.</ref> used the [[Rayleigh–Ritz method]], Shen and Pierre (1990)<ref>Shen, M.H., Pierre, C., 1990. Natural modes of [[Euler–Bernoulli beam theory|Euler–Bernoulli Beams]] with symmetric cracks. Journal of Sound and Vibration 138, 115–134.</ref> used the  [[Galerkin Method]], and Qian et al. (1991)<ref>Qian, G.L., Gu, S.N., Jiang, J.S., 1991. The dynamic behavior and crack detection of a\nbeam with a crack. Journal of Sound and Vibration 138, 233–243</ref> used a [[finite element method]] to predict the behavior of a beam with an edge crack. Law and Lu (2005)<ref>Law, S., Lu, Z.R., 2005. Crack identification in beam from dynamic responses. Journal of Sound and Vibration 285, 967–987.</ref> used assumed modes and modeled the crack mathematically as a Dirac delta function.\nWang and Qiao (2007)<ref>Wang, J., Qiao, P., 2007. Vibration of beams with arbitrary discontinuities and boundary conditions. Journal of Sound and Vibration 308 (1–2), 12–27.</ref> approximated the modal displacements using Heaviside’s function, which meant that modal displacements were discontinuous at the crack location.\n\n=== Application to SHM ===\nPrimary shortcomings of the above methods were that: \n# They have been developed mostly for [[Euler–Bernoulli beam theory]];{{cn|date=August 2017}}\n# They were developed in a few cases for [[Timoshenko beam theory]] or plate theories with expressions provided only for particular boundary conditions and beam or plate shapes{{cn|date=August 2017}}; \n# They did not include mass change when applicable{{cn|date=August 2017}}; and\n# Only few damage shapes were considered, such as V shaped or rectangular notches, even though damage can occur in a wide variety of shapes (for which stress intensity factors may not be readily available).{{cn|date=August 2017}}\n\nObservations in the literature survey regarding the different damage models are similar, i.e., they are not generic.{{cn|date=August 2017}} In spite of considerable progress in the damage identification using vibration\nbased methods, there is still lack of a fairly successful algorithm to detect damage as concluded in all the reviews since 1995. In 1995, in the review published by Dimarogonas (1996), ref>Dimarogonas, A.D., 1996. Vibration of cracked structures: a state of the art review. Engineering Fracture Mechanics 55 (5), 831–857.</ref> it is concluded “A consistent cracked beam vibration theory is yet to be developed”. In 2005, in another review about vibration based structural health monitoring, Carden and Fanning (2004)<ref>Carden, E., Fanning, P., 2004. Vibration based condition monitoring: a review. Structural Health Monitoring 3 (4), 355–377</ref> conclude, “There is no universal agreement as to the optimum method for using measured vibration data for damage detection, location or quantification”. Similarly in 2007, Montalvao et al. (2006)<ref>Montalvao, D., Maia, N.M.M., Ribeiro, A.M.R., 2006. A review of vibration-based structural\nhealth monitoring with special emphasis on composite materials. Shock and Vibration Digest 38 (4), 295–326.</ref> state as one of the conclusions, “There is no general algorithm that allows the resolution of all kinds of problems in all kinds of structures”. Similar trends regarding lack of generality of proposed models is seen in the latest review by Fan and Qiao (2010).<ref>Fan, W., Qiao, P.Z., 2010. Vibration-based damage identification methods: a review and comparative study. Structural Health Monitoring.</ref>\n\nThe lack of generality of damage models is addressed by proposing a ‘unified framework’ which is valid for self-adjoint systems using beam theories like Euler–Bernoulli beam theory, [[Timoshenko beam theory|Timoshenko]], [[Plate theory|plate theories]] like Kirchhoff and Mindlin and shell theories. The model was presented and verified for a damaged beam with notch type damage, using first-order perturbation only, for the Euler–Bernoulli beam theory in the paper by Dixit and Hanagud (2011)<ref>Dixit, A., Hanagud, S., 2011. Single beam analysis of damaged beams verified using a strain energy based damage measure. International Journal of Solid and Structures 48, 592–602.</ref> and using Timoshenko beam theory in the paper by Dixit and Hanagud (2009).<ref>Dixit, A., Hanagud, S., 2009. Comparison of strain energy based damage measure for Timoshenko and Euler–Bernoulli beams with notch like damages. In: Proceedings of the International Workshop on Structural Health Monitoring 2009.</ref> Since the results are given for nth order, a computer program can be developed which will give the results for mode shapes and natural frequencies to the desired accuracy, preempting the need to go through the mathematically arduous task of deriving the higher order expressions algebraically.{{cn|date=August 2017}}\n\n== Features ==\n{{unsourced|section|date=August 2017}}\nThis Unified Framework involves a general analytical procedure, which yields nth-order expressions governing mode shapes and natural frequencies and for damaged elastic structures\nsuch as rods, beams, plates and shells of any shape. Features of the procedure include the following:\n# Rather than modeling the damage as a fictitious elastic element or localized or global change in constitutive properties, it is modeled in a mathematically rigorous manner as a geometric discontinuity. \n# The inertia effect (kinetic energy), which, unlike the [[Strain energy|stiffness effect (strain energy)]], of the damage has been neglected by researchers, is included in it.\n# The framework is generic and is applicable to wide variety of engineering structures of different shapes with arbitrary boundary conditions which constitute self adjoint systems and also to a wide variety of damage profiles and even multiple areas of damage.\n\n== References ==\n{{reflist}}\n\n[[Category:Structural analysis]]"
    },
    {
      "title": "Unit dummy force method",
      "url": "https://en.wikipedia.org/wiki/Unit_dummy_force_method",
      "text": "{{Unreferenced|date=December 2009}}\nThe '''Unit dummy force method''' provides a convenient means for computing displacements in structural systems. It is applicable for both linear and non-linear material behaviours as well as for systems subject to environmental effects, and hence more general than [[Castigliano's method|Castigliano's second theorem]].\n\n==Discrete systems==\nConsider a discrete system such as trusses, beams or frames having members interconnected at the nodes. Let the consistent set of members' deformations be given by <math>\\mathbf{q}_{M \\times 1} </math>, which can be computed using the [[flexibility method|member flexibility relation]]. These member deformations give rise to the nodal displacements <math>\\mathbf{r}_{N \\times 1} </math>, which we want to determine.\n\nWe start by applying ''N'' virtual nodal forces <math>\\mathbf{R}^*_{N \\times 1} </math>, one for each wanted ''r'', and find the virtual member forces <math>\\mathbf{Q}^*_{M \\times 1} </math> that are in equilibrium with <math>\\mathbf{R}^*_{N \\times 1} </math>:\n\n:<math>\\mathbf{Q}^*_{M \\times 1}  =  \\mathbf{B}_{M \\times N} \\mathbf{R}^*_{N \\times 1} \\qquad \\qquad \\qquad \\mathrm{(1)} </math>\n\nIn the case of a statically indeterminate system, matrix '''B''' is not unique because the set of <math>\\mathbf{Q}^*_{M \\times 1} </math> that satisfies nodal equilibrium is infinite. It can be computed as the inverse of the nodal equilibrium matrix of any [[flexibility method|primary system]] derived from the original system.\n\nImagine that internal and external virtual forces undergo, respectively, the real deformations and displacements; the virtual work done can be expressed as:\n\n* External virtual work: <math>\\mathbf{R}^{*T} \\mathbf{r} </math>\n* Internal virtual work: <math>\\mathbf{Q}^{*T} \\mathbf{q} </math>\n\nAccording to the [[virtual work]] principle, the two work expressions are equal:\n\n:<math>\\mathbf{R}^{*T} \\mathbf{r} = \\mathbf{Q}^{*T} \\mathbf{q}  </math>\n\nSubstitution of (1) gives\n\n:<math>\\mathbf{R}^{*T} \\mathbf{r} = \\mathbf{R}^{*T} \\mathbf{B}^{T} \\mathbf{q}  </math>\n\nSince <math>\\mathbf{R}^{*}</math> contains arbitrary virtual forces, the above equation gives\n\n:<math> \\mathbf{r} = \\mathbf{B}^{T} \\mathbf{q}  \\qquad \\qquad \\qquad \\mathrm{(2)} </math>\n\nIt is remarkable that the computation in (2) does not involve any integration regardless of the complexity of the systems, and that the result is unique irrespective of the choice of primary system for '''B'''. It is thus far more convenient and general than the classical form of the dummy unit load method, which varies with the type of system as well as with the imposed external effects. On the other hand, it is important to note that Eq.(2) is for computing displacements or rotations of the nodes only. This is not a restriction because we can make any point into a node when desired.\n\nFinally, the name '''unit load''' arises from the interepretation that the coefficients <math> B_{i,j} </math> in matrix '''B''' are the member forces in equilibrium with the unit nodal force <math> R^*_j = 1 </math>, by virtue of Eq.(1).\n\n==General systems==\nFor a general system, the unit dummy force method also comes directly from the [[virtual work]] principle. Fig.(a) shows a system with known actual deformations <math> \\boldsymbol{\\epsilon} </math>. These deformations, supposedly consistent, give rise to displacements throughout the system. For example, a point A has moved to A', and we want to compute the displacement ''r'' of A in the direction shown. For this particular purpose, we choose the virtual force system in Fig.(b) which shows:\n* The unit force ''R''* is at A and in the direction of ''r'' so that the external virtual work done by ''R''* is, noting that the work done by the virtual reactions in (b) is zero because their displacements in (a) are zero:\n\n: <math> R^* \\times r = 1 \\times r </math>: The desired displacement\n\n* The internal virtual work done by the virtual stresses is\n:<math> \\int_{V}\\boldsymbol{\\epsilon}^T \\boldsymbol{\\sigma}^* dV </math>\n:where the virtual stresses <math> \\boldsymbol{\\sigma}^* </math> must satisfy equilibrium everywhere.\n\nEquating the two work expressions gives the desired displacement:\n\n:<math> 1 \\times r = \\int_{V}\\boldsymbol{\\epsilon}^T \\boldsymbol{\\sigma}^* dV </math>\n\n{{DEFAULTSORT:Unit Dummy Force Method}}\n[[Category:Structural analysis]]"
    },
    {
      "title": "Virtual work",
      "url": "https://en.wikipedia.org/wiki/Virtual_work",
      "text": "{{Classical mechanics|cTopic=Fundamental concepts}}\n'''Virtual work''' arises in the application of the ''[[principle of least action]]'' to the study of forces and movement of a [[mechanical system]].  The [[Work (physics)|work]] of a force acting on a particle as it moves along a displacement is different for different displacements.  Among all the possible displacements that a particle may follow, called [[virtual displacement]]s, one will minimize the action. This displacement is therefore the displacement followed by the particle according to the principle of least action.  ''The work of a force on a particle along a virtual displacement is known as the virtual work''.\n\nHistorically, virtual work and the associated [[calculus of variations]] were formulated to analyze systems of rigid bodies,<ref name=Lanczos>[https://books.google.com/books?id=ZWoYYr8wk2IC&printsec=frontcover&source=gbs_ge_summary_r&cad=0#v=onepage&q&f=false C. Lánczos, The Variational Principles of Mechanics, 4th Ed., General Publishing Co., Canada, 1970]</ref> but they have also been developed for the study of the mechanics of deformable bodies.<ref>Dym, C. L. and I. H. Shames, ''Solid Mechanics: A Variational Approach'', McGraw-Hill, 1973.</ref>\n\n== History ==\nThe '''principle of virtual work''' had always been used in some form since antiquity in the study of statics. It was used by the Greeks, medieval Arabs and Latins, and Renaissance Italians as \"the law of lever\".<ref name=capecchi>{{cite book | last=Capecchi | first=Danilo | title=History of Virtual Work Laws | publisher=Springer Milan | publication-place=Milano | year=2012 | isbn=978-88-470-2055-9 | doi=10.1007/978-88-470-2056-6 | ref=harv}}</ref> The idea of virtual work was invoked by many notable physicists of the 17th century, such as Galileo, Descartes, Torricelli, Wallis, and Huygens, in varying degrees of generality, when solving problems in statics.<ref name=capecchi/> Working with Leibnizian concepts,  [[Johann Bernoulli]] systematized the virtual work principle and made explicit the concept of infinitesimal displacement. He was able to solve problems for both rigid bodies as well as fluids. Bernoulli's version of virtual work law appeared in his letter to [[Pierre Varignon]] in 1715, which was later published in Varignon's second volume of ''Nouvelle mécanique ou Statique'' in 1725. This formulation of the principle is today known as the principle of virtual velocities and is commonly considered as the prototype of the contemporary virtual work principles.<ref name=capecchi/> In 1743 D'Alembert published his ''Traité de Dynamique'' where he applied the principle of virtual work, based on Bernoulli's work, to solve various problems in dynamics. His idea was to convert a dynamical problem into static problem by introducing ''inertial force''.<ref name=dugas>René Dugas, A History of Mechanics, Courier Corporation, 2012</ref> In 1768, [[Lagrange]] presented the virtual work principle in a more efficient form by introducing generalized coordinates and presented it as an alternative principle of mechanics by which all problems of equilibrium could be solved. A systematic exposition of Lagrange's program of applying this approach to all of mechanics, both static and dynamic, essentially [[D'Alembert's principle]], was given in his ''Mécanique Analytique'' of 1788.<ref name=capecchi/> Although Lagrange had presented his version of [[least action principle]] prior to this work, he recognized the virtual work principle to be more fundamental mainly because it could be assumed alone as the foundation for all mechanics, unlike the modern understanding that least action does not account for non-conservative forces.<ref name=capecchi/>\n\n==Overview==\nIf a force acts on a particle as it moves from  point ''A'' to point ''B'', then, for each possible trajectory that the particle may take, it is possible to compute the total work done by the force along the path.   The ''principle of virtual work'', which is the form of the principle of least action applied to these systems, states that the path actually followed by the particle is the one for which the difference between the work along this path and other nearby paths is zero (to first order). The formal procedure for computing the difference of functions evaluated on nearby paths is a generalization of the derivative known from differential calculus, and is termed ''the calculus of variations''.\n\nConsider a point particle that moves along a path which is described by a function '''r'''(''t'') from point ''A'', where '''r'''(''t'' = ''t''<sub>0</sub>), to point ''B'', where '''r'''(''t'' = ''t''<sub>1</sub>). It is possible that the particle moves from ''A'' to ''B'' along a nearby path described by '''r'''(''t'') + δ'''r'''(''t''), where δ'''r'''(''t'') is called the variation of '''r'''(''t''). The variation δ'''r'''(t) satisfies the requirement δ'''r'''(''t''<sub>0</sub>) = δ'''r'''(''t''<sub>1</sub>) = '''0'''. The components of the variation, δ''r''<sub>1</sub>(t), δ''r''<sub>2</sub>(t) and δ''r''<sub>3</sub>(t), are called virtual displacements. This can be generalized to an arbitrary mechanical system defined by the [[generalized coordinates]] ''q''<sub>''i'' </sub>, ''i'' = 1, ..., ''n''.  In which case, the variation of the trajectory ''q''<sub>''i'' </sub>(''t'') is defined by the virtual displacements ''δq''<sub>''i'' </sub>, ''i'' = 1, ..., ''n''.\n\nVirtual work is the total work done by the applied forces and the inertial forces of a mechanical system as it moves through a set of virtual displacements. When considering forces applied to a body in static equilibrium, the principle of least action requires the virtual work of these forces to be zero.\n\n<!-- \n'''Virtual work''' on a [[physical system|system]] is the [[mechanical work|work]] resulting from either virtual forces acting through a real [[Displacement (vector)|displacement]] or real [[forces]] acting through a [[virtual displacement]]. In this discussion, the term ''displacement'' may refer to a translation or a rotation, and the term ''force'' to a force or a moment. When the virtual quantities are [[independent variable]]s, they are also ''arbitrary''. Being arbitrary is an essential characteristic that enables one to draw important conclusions from mathematical relations. For example, in the matrix relation\n\n:<math>\\mathbf{R}^{*T} \\mathbf{r} = \\mathbf{R}^{*T} \\mathbf{B}^{T} \\mathbf{q}</math>,\n\nif <math>\\mathbf{R}^{*}</math> is an arbitrary vector, then one can conclude that <math> \\mathbf{r} = \\mathbf{B}^{T} \\mathbf{q} </math>. In this way, the arbitrary quantities disappear from the final useful results.\n-->\n\n== Introduction ==\nConsider a particle ''P'' that moves from a point ''A'' to a point ''B'' along a trajectory '''r'''(''t''), while a force '''F'''('''r'''(''t'')) is applied to it. The work done by the force '''F''' is given by the integral\n:<math> W = \\int_{\\mathbf{r}(t_0)=A}^{\\mathbf{r}(t_1)=B}\\mathbf{F}\\cdot d\\mathbf{r} = \\int_{t_0}^{t_1}\\mathbf{F}\\cdot \\frac{d\\mathbf{r}}{dt}~dt = \\int_{t_0}^{t_1}\\mathbf{F}\\cdot \\mathbf{v}~dt,</math>\nwhere ''d'''''r''' is the differential element along the curve that is the trajectory of ''P'', and '''v''' is its velocity.  It is important to notice that the value of the work ''W'' depends on the trajectory '''r'''(''t'').\n\nNow consider particle ''P'' that moves from point ''A'' to point ''B'' again, but this time it moves along the nearby trajectory that differs from '''r'''(''t'') by the variation ''δ'''''r'''(''t'')=''ε'''''h'''(''t''), where ''ε'' is a scaling constant that can be made as small as desired and '''h'''(''t'') is an arbitrary function that satisfies '''h'''(''t''<sub>0</sub>) = '''h'''(''t''<sub>1</sub>) = 0. Suppose the force '''F'''('''r'''(''t'')+''ε'''''h'''(''t'')) is the same as '''F'''('''r'''(''t'')). The work done by the force is given by the integral\n:<math>\\bar{W} = \\int_{\\mathbf{r}(t_0)=A}^{\\mathbf{r}(t_1)=B}\\mathbf{F}\\cdot d(\\mathbf{r}+\\epsilon \\mathbf{h})=\\int_{t_0}^{t_1}\\mathbf{F}\\cdot \\frac{d(\\mathbf{r}(t)+\\epsilon\\mathbf{h}(t))}{dt}~dt=\\int_{t_0}^{t_1}\\mathbf{F}\\cdot  (\\mathbf{v}+\\epsilon \\dot{\\mathbf{h}})~dt  .</math>\nThe variation of the work ''δW'' associated with this nearby path, known as the ''virtual work'', can be computed to be\n:<math> \\delta W = \\bar{W}-W = \\int_{t_0}^{t_1}(\\mathbf{F}\\cdot \\epsilon\\dot{\\mathbf{h}})~dt.</math>\n\nIf there are no constraints on the motion of ''P'', then 6 parameters are needed to completely describe ''P'''s position at any time ''t''. If there are ''k'' (''k'' ≤ 6) constraint forces, then ''n'' = (6 - ''k'') parameters are needed. Hence, we can define ''n'' generalized coordinates ''q''<sub>''i''</sub> (''t'') (''i'' = 1, 2, ..., ''n''), and express '''r'''(''t'') and ''δ'''''r'''=''ε'''''h'''(''t'') in terms of the generalized coordinates. That is,\n:<math>\\mathbf{r}(t) = \\mathbf{r}(q_1,q_2,...,q_n;t)</math>,\n:<math>\\mathbf{h}(t) = \\mathbf{h}(q_1,q_2,...,q_n;t)</math>.\nThen, the derivative of the variation ''δ'''''r'''=''ε'''''h'''(''t'') is given by\n:<math> \\frac{d}{dt}\\delta \\mathbf{r} = \\frac{d}{dt}\\epsilon\\mathbf{h} = \\sum_{i=1}^n\\frac{\\partial \\mathbf{h}}{\\partial q_i}\\epsilon\\dot{q}_i,</math>\nthen we have  \n:<math> \\delta W = \\int_{t_0}^{t_1}\\left(\\sum_{i=1}^n \\mathbf{F}\\cdot\\frac{\\partial\\mathbf{h}}{\\partial q_i}\\epsilon\\dot{q}_i\\right)dt = \\sum_{i=1}^n\\left(\\int_{t_0}^{t_1}\\mathbf{F}\\cdot\\frac{\\partial\\mathbf{h}}{\\partial q_i}\\epsilon\\dot{q}_i ~dt\\right).</math>\n \nThe requirement that the virtual work be zero for an arbitrary variation ''δ'''''r'''(''t'')=ε'''h'''(''t'') is equivalent to the set of requirements\n:<math> Q_i = \\mathbf{F}\\cdot \\frac{\\partial \\mathbf{h}}{\\partial q_i} = 0, \\quad i=1, \\ldots, n.</math>\nThe terms ''Q<sub>i</sub>'' are called the ''generalized forces'' associated with the virtual displacement δ'''r'''.\n\n== Static equilibrium ==\n[[Mechanical equilibrium|Static equilibrium]] is a state in which the net force and net torque acted upon the system is zero. In other words, both [[Momentum|linear momentum]] and [[angular momentum]] of the system are conserved. The principle of virtual work states that ''the virtual work of the applied forces is zero for all [[Virtual displacement|virtual movements]] of the system from [[Mechanical equilibrium|static equilibrium]]''. This principle can be generalized such that three dimensional [[rotation]]s are included: the virtual work of the applied forces and applied moments is zero for all [[Virtual displacement|virtual movements]] of the system from static equilibrium. That is\n:<math> \\delta W = \\sum_{i=1}^m \\mathbf{F}_i \\cdot \\delta\\mathbf{r}_i + \\sum_{j=1}^n \\mathbf{M}_j \\cdot \\delta\\mathbf{\\phi}_j = 0 ,</math>\nwhere '''F'''''<sub>i</sub>'' , ''i'' = 1, 2, ..., ''m'' and '''M'''''<sub>j</sub>'' , ''j'' = 1, 2, ..., ''n'' are the applied forces and applied moments, respectively, and ''δ'''''r'''''<sub>i</sub>'' , ''i'' = 1, 2, ..., ''m'' and ''δ'''''φ'''''<sub>j</sub>'' , ''j'' = 1, 2, ..., ''n'' are the [[virtual displacement]]s and [[Virtual displacement|virtual rotations]], respectively.\n\nSuppose the system consists of ''N'' particles, and it has ''f'' (''f'' ≤ 6''N'') [[Degrees of freedom (mechanics)|degrees of freedom]]. It is sufficient to use only ''f'' coordinates to give a complete description of the motion of the system, so ''f'' [[Generalized coordinates|generalized coordinates]] ''q<sub>k</sub>'' , ''k'' = 1, 2, ..., ''f'' are defined such that the [[Virtual displacement|virtual movements]] can be expressed in terms of these  [[Generalized coordinates|generalized coordinates]]. That is,\n:<math> \\delta \\mathbf{r}_i (q_1, q_2, ..., q_f; t), \\quad i = 1, 2, ..., m ; </math>\n:<math> \\delta\\phi_j (q_1, q_2, ..., q_f; t), \\quad j = 1, 2, ..., n . </math>\n\nThe virtual work can then be [[Parametrization (geometry)|reparametrized]] by the [[Generalized coordinates|generalized coordinates]]:\n:<math> \\delta W = \\sum_{k=1}^f \\left[ \\left(  \\sum_{i=1}^m \\mathbf{F}_i \\cdot \\frac{\\partial \\mathbf{r}_i}{\\partial q_k} + \\sum_{j=1}^n \\mathbf{M}_j \\cdot \\frac{\\partial \\mathbf{\\phi}_j}{\\partial q_k} \\right) \\delta q_k \\right] = \\sum_{k=1}^f Q_k \\delta q_k ,</math>\nwhere the [[Generalized forces|generalized forces]] ''Q<sub>k</sub>'' are defined as\n:<math> Q_k = \\sum_{i=1}^m \\mathbf{F}_i \\cdot \\frac{\\partial \\mathbf{r}_i}{\\partial q_k} + \\sum_{j=1}^n \\mathbf{M}_j \\cdot \\frac{\\partial \\mathbf{\\phi}_j}{\\partial q_k} , \\quad k = 1, 2, ..., f .</math>\nKane<ref>T. R. Kane and D. A. Levinson, Dynamics: theory and applications, McGraw-Hill, New York, 1985</ref> shows that these [[Generalized forces|generalized forces]] can also be formulated in terms of the ratio of time derivatives. That is,\n:<math> Q_k = \\sum_{i=1}^m \\mathbf{F}_i \\cdot \\frac{\\partial \\mathbf{v}_i}{\\partial \\dot{q}_k} + \\sum_{j=1}^n \\mathbf{M}_j \\cdot \\frac{\\partial \\mathbf{\\omega}_j}{\\partial \\dot{q}_k} , \\quad k = 1, 2, ..., f . </math>\n\nThe principle of virtual work requires that the virtual work done on a system by the forces '''F'''<sub>''i''</sub> and moments '''M'''<sub>''j''</sub> vanishes if it is in [[Mechanical equilibrium|equilibrium]]. Therefore, the generalized forces ''Q''<sub>''k''</sub> are zero, that is\n:<math> \\delta W=0 \\quad \\Rightarrow \\quad Q_k = 0 \\quad k =1, 2, ..., f . </math>\n\n=== Constraint forces ===\nAn important benefit of the principle of virtual work is that only forces that do work as the system moves through a [[virtual displacement]] are needed to determine the mechanics of the system.  There are many forces in a mechanical system that do no work during a [[virtual displacement]], which means that they need not be considered in this analysis.  The two important examples are (i) the internal forces in a [[rigid body]], and (ii) the constraint forces at an ideal [[Kinematic pair|joint]].\n\nLanczos<ref name=Lanczos/> presents this as the postulate:  \"The virtual work of the forces of reaction is always zero for any [[virtual displacement]] which is in harmony with the given kinematic constraints.\"  The argument is as follows.  The principle of virtual work states that in [[Mechanical equilibrium|equilibrium]] the virtual work of the forces applied to a system is zero.  [[Newton's laws of motion|Newton's laws]] state that at [[Mechanical equilibrium|equilibrium]] the applied forces are equal and opposite to the reaction, or constraint forces.  This means the virtual work of the constraint forces must be zero as well.\n\n<!-- removal of unnecessary part, lever and gear train in the next two sections is sufficient\n== One degree-of-freedom mechanisms ==\nIn this section, the principle of virtual work is used for the static analysis of one degree-of-freedom mechanical devices.  Specifically, we analyze  the lever, a pulley system, a gear train, and a four-bar linkage.  Each of these devices moves in the plane, therefore a force '''F'''=(''f<sub>x</sub>'', ''f<sub>y</sub>'') has two components and acts on a point with coordinates '''r'''&nbsp;=&nbsp;(''r<sub>x</sub>'', ''r<sub>y</sub>'') and velocity '''v'''&nbsp;=&nbsp;(''v<sub>x</sub>'', ''v<sub>y</sub>'').  A moment, also called a [[torque]], ''T'' acting on a body that moves in the plane has one component as does the angular velocity ''ω'' of the body.\n\nAssume the bodies in the mechanism are rigid and the joints are ideal so that the only change in virtual work is associated with the movement of the input and output forces and torques.\n\n=== Applied Forces ===\nConsider a mechanism such as a lever that operates so that an input force generates an output force.   Let ''A'' be the point where the input force '''F'''<sub>''A''</sub> is applied, and let ''B'' be the point where the output force '''F'''<sub>''B''</sub> is exerted.  Define the position and velocity of ''A'' and ''B'' by the vectors '''r'''<sub>''A''</sub>, '''v'''<sub>''A''</sub> and '''r'''<sub>''B''</sub>, '''v'''<sub>''B''</sub>, respectively.\n\nBecause the mechanism has one degree-of-freedom, there is a single generalized coordinate ''q'' that defines the position vectors '''r'''<sub>''A''</sub>(''q'') and '''r'''<sub>''B''</sub>(''q'') of the input and output points in the system.  The principle of virtual work requires that the generalized force associated with this coordinate be zero, thus\n:<math> Q =  \\mathbf{F}_A \\cdot \\frac{\\partial\\mathbf{v}_A}{\\partial\\dot{q}} - \\mathbf{F}_B \\cdot \\frac{\\partial\\mathbf{v}_B}{\\partial\\dot{q}}=0.</math>\n\nThe negative sign on the output force '''F'''<sub>''B''</sub> arises because the convention of virtual work assumes the forces are applied to the device.\n\n=== Applied Torque ===\nConsider a mechanism such as a gear train that operates so that an input torque generates an output torque.  Let body ''E''<sub>''A''</sub> have the input moment ''T''<sub>''A''</sub> applied to it, and let body ''E''<sub>''B''</sub> exert the output torque ''T''<sub>''B''</sub>.  Define the angular position and velocity of ''E''<sub>''A''</sub> and ''E''<sub>''B''</sub> by ''θ''<sub>''A''</sub>, ''ω''<sub>''A''</sub> and ''θ''<sub>''B''</sub>, ''ω''<sub>''B''</sub>, respectively.\n\nBecause the mechanism has one degree-of-freedom, there is a single generalized coordinate ''q'' that defines the angles ''θ<sub>A</sub>''(''q'') and ''θ<sub>B</sub>''(''q'') of the input and output of the system.  The principle of virtual work requires that the generalized force associated with this coordinate be zero, thus\n:<math> Q =  T_A  \\frac{\\partial\\mathbf{\\omega}_A}{\\partial\\dot{q}} - T_B \\frac{\\partial\\mathbf{\\omega}_B}{\\partial\\dot{q}}=0.</math>\n\nThe negative sign on the output torque ''T''<sub>''B''</sub> arises because the convention of virtual work assumes the torques are applied to the device.\n-->\n\n== Law of the lever ==\nA [[lever]] is modeled as a rigid bar connected to a ground frame by a hinged joint called a fulcrum. The lever is operated by applying an input force  '''F'''<sub>''A''</sub> at a point ''A'' located by the coordinate vector '''r'''<sub>''A''</sub> on the bar.  The lever then exerts an output force '''F'''<sub>''B''</sub> at the point ''B'' located by '''r'''<sub>''B''</sub>.   The rotation of the lever about the fulcrum ''P'' is defined by the rotation angle ''θ''.\n[[Image:Archimedes lever (Small).jpg|thumb|right|300px|This is an engraving from ''Mechanics Magazine'' published in London in 1824.]]\n\nLet the coordinate vector of the point ''P'' that defines the fulcrum be '''r'''<sub>''P''</sub>, and introduce the lengths \n:<math> a = |\\mathbf{r}_A -  \\mathbf{r}_P|, \\quad  b = |\\mathbf{r}_B -  \\mathbf{r}_P|, </math>\nwhich are the distances from the fulcrum to the input point ''A'' and to the output point ''B'', respectively.\n\nNow introduce the unit vectors '''e'''<sub>''A''</sub> and '''e'''<sub>''B''</sub> from the fulcrum to the point ''A'' and ''B'', so\n:<math> \\mathbf{r}_A  -  \\mathbf{r}_P = a\\mathbf{e}_A, \\quad \\mathbf{r}_B -  \\mathbf{r}_P = b\\mathbf{e}_B.</math>\nThis notation allows us to define the velocity of the points ''A'' and ''B'' as\n:<math> \\mathbf{v}_A = \\dot{\\theta} a \\mathbf{e}_A^\\perp, \\quad  \\mathbf{v}_B = \\dot{\\theta} b \\mathbf{e}_B^\\perp,</math>\nwhere '''e'''<sub>''A''</sub><sup>⊥</sup> and '''e'''<sub>''B''</sub><sup>⊥</sup> are unit vectors perpendicular to '''e'''<sub>''A''</sub> and '''e'''<sub>''B''</sub>, respectively.\n\nThe angle ''θ'' is the generalized coordinate that defines the configuration of the lever, therefore using the formula above for forces applied to a one degree-of-freedom mechanism, the generalized force is given by\n:<math> Q =  \\mathbf{F}_A \\cdot \\frac{\\partial\\mathbf{v}_A}{\\partial\\dot{\\theta}} - \\mathbf{F}_B \\cdot \\frac{\\partial\\mathbf{v}_B}{\\partial\\dot{\\theta}}= a(\\mathbf{F}_A \\cdot \\mathbf{e}_A^\\perp) - b(\\mathbf{F}_B \\cdot \\mathbf{e}_B^\\perp).</math>\n\nNow, denote as ''F''<sub>''A''</sub> and ''F''<sub>''B''</sub> the components of the forces that are perpendicular to the radial segments ''PA'' and ''PB''.   These forces are given by\n:<math> F_A = \\mathbf{F}_A \\cdot \\mathbf{e}_A^\\perp, \\quad  F_B = \\mathbf{F}_B \\cdot \\mathbf{e}_B^\\perp.</math>\nThis notation and the principle of virtual work yield the formula for the generalized force as\n:<math> Q = a F_A - b F_B = 0. </math>\n\nThe ratio of the output force ''F''<sub>''B''</sub> to the input force ''F''<sub>''A''</sub> is the [[mechanical advantage]] of the lever, and is obtained from the principle of virtual work as\n:<math> MA = \\frac{F_B}{F_A} = \\frac{a}{b}.</math>\n\nThis equation shows that if the distance ''a'' from the fulcrum to the point ''A'' where the input force is applied is greater than the distance ''b'' from fulcrum to the point ''B'' where the output force is applied, then the lever amplifies the input force.  If the opposite is true that the distance from the fulcrum to the input point ''A'' is less than from the fulcrum to the output point ''B'', then the lever reduces the magnitude of the input force.\n\nThis is the ''law of the lever'', which was proven by [[Archimedes]] using geometric reasoning.<ref name=\"Usher1954\">{{cite book|author=Usher, A. P.|authorlink=Abbott Payson Usher|title=A History of Mechanical Inventions|url=https://books.google.com/books?id=Zt4Aw9wKjm8C&pg=PA94|page=94|accessdate=7 April 2013|year=1929|publisher=Harvard University Press (reprinted by Dover Publications 1988)|isbn=978-0-486-14359-0|oclc=514178}}</ref>\n\n== Gear train ==\nA gear train is formed by mounting gears on a frame so that the teeth of the gears engage.  Gear teeth are designed to ensure the pitch circles of engaging gears roll on each other without slipping, this provides a smooth transmission of rotation from one gear to the next.   For this analysis, we consider a gear train that has one degree-of-freedom, which means the angular rotation of all the gears in the gear train are defined by the angle of the input gear.   \n[[Image:Transmission of motion by compund gear train (Army Service Corps Training, Mechanical Transport, 1911).jpg|thumb|right|300px|Illustration from Army Service Corps Training on Mechanical Transport, (1911), Fig. 112 Transmission of motion and force by gear wheels, compound train]]\n\nThe size of the gears and the sequence in which they engage define the ratio of the angular velocity ''ω<sub>A</sub>'' of the input gear to the angular velocity ''ω<sub>B</sub>'' of the output gear, known as the speed ratio, or [[gear ratio]], of the gear train.  Let ''R'' be the speed ratio, then\n:<math> \\frac{\\omega_A}{\\omega_B} = R.</math>\n\nThe input torque ''T''<sub>''A''</sub> acting on the input gear ''G''<sub>''A''</sub> is transformed by the gear train into the output torque ''T''<sub>''B''</sub> exerted by the output gear ''G''<sub>''B''</sub>.  If we assume, that the gears are rigid and that there are no losses in the engagement of the gear teeth, then the principle of virtual work can be used to analyze the static equilibrium of the gear train.\n\nLet the angle ''θ'' of the input gear be the generalized coordinate of the gear train, then the speed ratio ''R'' of the gear train defines the angular velocity of the output gear in terms of the input gear, that is\n:<math> \\omega_A = \\omega, \\quad \\omega_B = \\omega/R.</math>\n\nThe formula above for the principle of virtual work with applied torques yields the generalized force\n:<math> Q =  T_A  \\frac{\\partial\\omega_A}{\\partial\\omega} - T_B \\frac{\\partial \\omega_B}{\\partial\\omega}= T_A - T_B/R = 0.</math>\n\nThe [[mechanical advantage]] of the gear train is the ratio of the output torque ''T''<sub>''B''</sub> to the input torque ''T''<sub>''A''</sub>, and the above equation yields\n:<math> MA = \\frac{T_B}{T_A} = R.</math>\n\nThus, the speed ratio of a gear train also defines its mechanical advantage.  This shows that if the input gear rotates faster than the output gear, then the gear train amplifies the input torque.  And, if the input gear rotates slower than the output gear, then the gear train reduces the input torque.\n\n== Dynamic equilibrium for rigid bodies ==\nIf the principle of virtual work for applied forces is used on individual particles of a [[rigid body]], the principle can be generalized for a rigid body: ''When a rigid body that is in equilibrium is subject to virtual compatible displacements, the total virtual work of all external forces is zero; and conversely, if the total virtual work of all external forces acting on a rigid body is zero then the body is in equilibrium''.\n\nIf a system is not in static equilibrium, D'Alembert showed that by introducing the acceleration terms of Newton's laws as inertia forces, this approach is generalized to define dynamic equilibrium.  The result is D'Alembert's form of the principle of virtual work, which is used to derive the equations of motion for a mechanical system of rigid bodies.\n\nThe expression ''compatible displacements'' means that the particles remain in contact and displace together so that the work done by pairs of action/reaction inter-particle forces cancel out. Various forms of this principle have been credited to [[Johann Bernoulli|Johann (Jean) Bernoulli]] (1667–1748) and [[Daniel Bernoulli]] (1700–1782).\n\n<!-- totally the same as in static equilibirum\n\n===Generalized active forces===\nThe static equilibrium of a mechanical system of rigid bodies is defined by the condition that the virtual work of the applied forces is zero for any virtual displacement of the system.  This is known as the ''principle of virtual work.''<ref name=\"Torby1984\">{{cite book |last=Torby |first=Bruce |title=Advanced Dynamics for Engineers |series=HRW Series in Mechanical Engineering |year=1984 |publisher=CBS College Publishing |location=United States of America  |isbn=0-03-063366-4 |chapter=Energy Methods}}</ref>  This is equivalent to the requirement that the generalized forces for any virtual displacement are zero, that is ''Q''<sub>''i''</sub>&nbsp;=&nbsp;0.\n\nLet a mechanical system be constructed from ''n'' rigid bodies, ''B''<sub>''i'' </sub>, ''i'' = 1, ..., ''n'', and let the resultant of the applied forces on each body be the force–torque pairs, '''F'''<sub>''i''</sub> and '''T'''<sub>''i'' </sub>, ''i'' = 1, ..., ''n''.  Notice that these applied forces do not include the reaction forces where the bodies are connected.  Finally, assume that the velocity '''V'''<sub>''i''</sub> and angular velocities '''ω'''<sub>''i'' </sub>, ''i'' = 1, ..., ''n'', for each rigid body, are defined by a single generalized coordinate ''q''.  Such a system of rigid bodies is said to have one [[degree of freedom (mechanics)|degree of freedom]].\n\nThe virtual work of the forces and torques, '''F'''<sub>''i''</sub> and '''T'''<sub>''i'' </sub>, applied to this one degree of freedom system is given by\n:<math> \\delta W = \\sum_{i=1}^n \\left(\\mathbf{F}_i\\cdot \\frac{\\partial \\mathbf{V}_i}{\\partial \\dot{q}} + \\mathbf{T}_i\\cdot\\frac{\\partial \\vec{\\omega}_i}{\\partial \\dot{q}}\\right)\\delta q = Q\\,\\delta q,</math>\nwhere \n:<math> Q = \\sum_{i=1}^n \\left(\\mathbf{F}_i\\cdot \\frac{\\partial \\mathbf{V}_i}{\\partial \\dot{q}} + \\mathbf{T}_i\\cdot\\frac{\\partial \\vec{\\omega}_i}{\\partial \\dot{q}}\\right),</math>\nis the generalized force acting on this one degree of freedom system.\n\nIf the mechanical system is defined by ''m'' generalized coordinates, ''q''<sub>''j'' </sub>, ''j'' = 1, ..., ''m'', then the system has ''m'' degrees of freedom and the virtual work is given by,\n:<math> \\delta W = \\sum_{j=1}^m Q_j\\,\\delta q_j,</math>\nwhere\n:<math> Q_j = \\sum_{i=1}^n \\left(\\mathbf{F}_i\\cdot \\frac{\\partial \\mathbf{V}_i}{\\partial \\dot{q}_j} + \\mathbf{T}_i\\cdot\\frac{\\partial \\vec{\\omega}_i}{\\partial \\dot{q}_j}\\right),\\quad j=1, \\ldots, m.</math>\nis the generalized force associated with the generalized coordinate ''q''<sub>''j''</sub>.  The principle of virtual work states that static equilibrium occurs when these generalized forces acting on the system are zero, that is\n:<math> Q_j = 0,\\quad j=1, \\ldots, m.</math>\nThese ''m'' equations define the static equilibrium of the system of rigid bodies.\n-->\n\n===Generalized inertia forces===\nLet a mechanical system be constructed from n rigid bodies, B<sub>i</sub>, i=1,...,n, and let the resultant of the applied forces on each body be the force-torque pairs, '''F'''<sub>i</sub> and '''T'''<sub>i</sub>, i=1,...,n.  Notice that these applied forces do not include the reaction forces where the bodies are connected.  Finally, assume that the velocity '''V'''<sub>i</sub> and angular velocities '''ω'''<sub>i</sub>, i=,1...,n, for each rigid body, are defined by a single generalized coordinate q.  Such a system of rigid bodies is said to have one [[degree of freedom (mechanics)|degree of freedom]].\n\nConsider a single rigid body which moves under the action of a resultant force '''F''' and torque '''T''', with one degree of freedom defined by the generalized coordinate q.  Assume the reference point for the resultant force and torque is the center of mass of the body, then the generalized inertia force Q* associated with the generalized coordinate q is given by\n:<math> Q^* = -(M\\mathbf{A})\\cdot  \\frac{\\partial \\mathbf{V}}{\\partial \\dot{q}}   -   ([I_R]\\alpha+ \\omega\\times[I_R]\\omega)\\cdot \\frac{\\partial \\vec{\\omega}}{\\partial \\dot{q}}.</math>\nThis inertia force can be computed from the kinetic energy of the rigid body,\n:<math> T = \\frac{1}{2}M\\mathbf{V}\\cdot\\mathbf{V} + \\frac{1}{2}\\vec{\\omega}\\cdot [I_R]\\vec{\\omega},</math>\nby using the formula\n:<math> Q^* = -\\left(\\frac{d}{dt} \\frac{\\partial T}{\\partial \\dot{q}} -\\frac{\\partial T}{\\partial q}\\right).</math>\n\nA system of n rigid bodies with m generalized coordinates has the kinetic energy\n:<math>T = \\sum_{i=1}^n(\\frac{1}{2}M\\mathbf{V}_i\\cdot\\mathbf{V}_i + \\frac{1}{2}\\vec{\\omega}_i\\cdot [I_R]\\vec{\\omega}_i),</math>\nwhich can be used to calculate the m generalized inertia forces<ref>T. R. Kane and D. A. Levinson, [https://www.amazon.com/Dynamics-Theory-Applications-Mechanical-Engineering/dp/0070378460 Dynamics, Theory and Applications], McGraw-Hill, NY, 2005.</ref>\n:<math> Q^*_j = -\\left(\\frac{d}{dt} \\frac{\\partial T}{\\partial \\dot{q}_j} -\\frac{\\partial T}{\\partial q_j}\\right),\\quad j=1, \\ldots, m.</math>\n\n===D'Alembert's form of the principle of virtual work===\nD'Alembert's form of the principle of virtual work states that a system of rigid bodies is in dynamic equilibrium when the virtual work of the sum of the applied forces and the inertial forces is zero for any virtual displacement of the system.  Thus, dynamic equilibrium of a system of n rigid bodies with m generalized coordinates requires that\n:<math> \\delta W = (Q_1 + Q^*_1)\\delta q_1 + \\ldots + (Q_m + Q^*_m)\\delta q_m = 0,</math>\nfor any set of virtual displacements δq<sub>j</sub>.  This condition yields m equations,\n:<math> Q_j + Q^*_j = 0, \\quad j=1, \\ldots, m,</math>\nwhich can also be written as\n:<math>  \\frac{d}{dt} \\frac{\\partial T}{\\partial \\dot{q}_j} -\\frac{\\partial T}{\\partial q_j} = Q_j, \\quad j=1,\\ldots,m.</math>\nThe result is a set of m equations of motion that define the dynamics of the rigid body system.\n\nIf the generalized forces Q<sub>j</sub> are derivable from a potential energy V(q<sub>1</sub>,...,q<sub>m</sub>), then these equations of motion take the form\n:<math>  \\frac{d}{dt} \\frac{\\partial T}{\\partial \\dot{q}_j} -\\frac{\\partial T}{\\partial q_j} = -\\frac{\\partial V}{\\partial q_j}, \\quad j=1,\\ldots,m.</math>\n\nIn this case, introduce the [[Lagrangian mechanics|Lagrangian]], L=T-V, so these equations of motion become\n:<math>  \\frac{d}{dt} \\frac{\\partial L}{\\partial \\dot{q}_j} -\\frac{\\partial L}{\\partial q_j} =0 \\quad j=1,\\ldots,m.</math>\nThese are known as [[Lagrangian mechanics|Lagrange's equations of motion]].\n\n==Virtual work principle for a deformable body==\n\nConsider now the [[free body diagram]] of a [[deformable body]], which is composed of an infinite number of differential cubes. Let's define two unrelated states for the body:\n* The <math> \\boldsymbol{\\sigma} </math>-State <!-- No Fig.a in the page. -->: This shows external surface forces '''T''', body forces '''f''', and internal stresses <math> \\boldsymbol{\\sigma} </math> in equilibrium.\n* The <math> \\boldsymbol{\\epsilon} </math>-State <!-- No Fig.b in the page. -->: This shows continuous displacements <math> \\mathbf {u}^* </math> and consistent strains <math> \\boldsymbol{\\epsilon}^* </math>.\nThe superscript * emphasizes that the two states are unrelated. Other than the above stated conditions, there is no need to specify if any of the states are real or virtual.\n\nImagine now that the forces and stresses in the <math> \\boldsymbol{\\sigma} </math>-State  undergo the [[displacement (vector)|displacement]]s and [[deformation (engineering)|deformation]]s in the <math> \\boldsymbol{\\epsilon} </math>-State: We can compute the total virtual (imaginary) work done by '''''all forces acting on the faces of all cubes''''' in two different ways:\n\n* First, by summing the work done by forces such as <math> F_A </math> which act on individual common faces (Fig.c): Since the material experiences compatible [[displacement (vector)|displacement]]s, such work cancels out, leaving only the virtual work done by the surface forces '''T''' (which are equal to stresses on the cubes' faces, by equilibrium).\n* Second, by computing the net work done by stresses or forces such as <math> F_A </math>, <math> F_B </math> which act on an individual cube, e.g. for the one-dimensional case in Fig.(c):\n\n:<math> F_B \\left( u^* + \\frac{ \\partial u^*}{\\partial x} dx \\right ) - F_A u^* \\approx  \\frac{ \\partial u^* }{\\partial x}\n\\sigma dV +   u^* \\frac{ \\partial \\sigma }{\\partial x} dV = \\epsilon^* \\sigma dV - u^* f dV </math>\n\n:where the equilibrium relation <math> \\frac{ \\partial \\sigma }{\\partial x}+f=0 </math> has been used and the second order term has been neglected.\n\n:Integrating over the whole body gives:\n\n:<math>\\int_{V} \\boldsymbol{\\epsilon}^{*T} \\boldsymbol{\\sigma} \\, dV </math> – Work done by the body forces '''f'''.\n\nEquating the two results leads to the principle of virtual work for a deformable body:\n:<math>\\mbox{Total external virtual work} = \\int_{V} \\boldsymbol{\\epsilon}^{*T} \\boldsymbol{\\sigma} dV \\qquad \\mathrm{(d)} </math>\n\nwhere the total external virtual work is done by '''T''' and '''f'''. Thus,\n\n:<math> \\int_{S} \\mathbf{u}^{*T} \\mathbf{T} dS + \\int_{V} \\mathbf{u}^{*T} \\mathbf{f} dV = \\int_{V} \\boldsymbol{\\epsilon}^{*T} \\boldsymbol{\\sigma} dV \\qquad \\mathrm{(e)} </math>\n\nThe right-hand-side of (d,e) is often called the internal virtual work. The principle of virtual work then states: ''External virtual work is equal to internal virtual work when equilibrated forces and stresses undergo unrelated but consistent displacements and strains''. It includes the principle of virtual work for rigid bodies as a special case where the internal virtual work is zero.\n\n===Proof of equivalence between the principle of virtual work and the equilibrium equation===\n\nWe start by looking at the total work done by surface traction on the body going through the specified deformation:\n\n:<math> \\int_{S} \\mathbf{u \\cdot T} dS = \\int_{S} \\mathbf{u \\cdot \\boldsymbol {\\sigma} \\cdot n} dS </math>\n\nApplying divergence theorem to the right hand side yields:\n\n:<math> \\int_{S} \\mathbf{u \\cdot \\boldsymbol \\sigma \\cdot n} dS = \\int_V \\nabla \\cdot \\left( \\mathbf{u} \\cdot \\boldsymbol {\\sigma} \\right) dV </math>\n\nNow switch to indicial notation for the ease of derivation.\n\n:<math>\n\\begin{align}\n\\int_V \\nabla \\cdot \\left( \\mathbf{u} \\cdot \\boldsymbol {\\sigma} \\right) dV \n          &= \\int_V \\frac{\\partial}{\\partial x_j} \\left( u_i \\sigma_{ij} \\right) dV \\\\\n          &= \\int_V \\frac{\\partial u_i}{\\partial x_j} \\sigma_{ij} + u_i \\frac{\\partial \\sigma_{ij}}{\\partial x_j} dV\n\\end{align}\n</math>\n\nTo continue our derivation, we substitute in the equilibrium equation <math> \\frac{\\partial \\sigma_{ij}}{\\partial x_j} + f_i = 0 </math>. Then\n\n:<math>\n\\int_V \\frac{\\partial u_i}{\\partial x_j} \\sigma_{ij} + u_i \\frac{\\partial \\sigma_{ij}}{\\partial x_j} dV\n       = \\int_V \\frac{\\partial u_i}{\\partial x_j} \\sigma_{ij} - u_i f_i dV\n</math>\n\nThe first term on the right hand side needs to be broken into a symmetric part and a skew part as follows:\n\n:<math>\n\\begin{align}\n\\int_V \\frac{\\partial u_i}{\\partial x_j} \\sigma_{ij} - u_i f_i dV\n       &= \\int_V \\frac12 \\left[ \\left( \\frac{\\partial u_i}{\\partial x_j} + \\frac{\\partial u_j}{\\partial x_i} \\right) \n          + \\left( \\frac{\\partial u_i}{\\partial x_j} - \\frac{\\partial u_j}{\\partial x_i} \\right) \\right] \\sigma_{ij} - u_i f_i dV \\\\\n       &= \\int_V \\left[ \\epsilon_{ij} \n          + \\frac12 \\left( \\frac{\\partial u_i}{\\partial x_j} - \\frac{\\partial u_j}{\\partial x_i} \\right) \\right] \\sigma_{ij} - u_i f_i dV \\\\\n       &= \\int_V \\epsilon_{ij} \\sigma_{ij} - u_i f_i dV\\\\\n       &= \\int_V \\boldsymbol\\epsilon : \\boldsymbol\\sigma - \\mathbf u \\cdot \\mathbf f dV\n\\end{align}\n</math>\n\nwhere <math> \\boldsymbol\\epsilon </math> is the strain that is consistent with the specified displacement field. The 2nd to last equality comes from the fact that the stress matrix is symmetric and that the product of a skew matrix and a symmetric matrix is zero.\n\nNow recap. We have shown through the above derivation that\n\n:<math> \\int_{S} \\mathbf{u \\cdot T} dS = \\int_V \\boldsymbol\\epsilon : \\boldsymbol\\sigma dV - \\int_V \\mathbf u \\cdot \\mathbf f dV </math>\n\nMove the 2nd term on the right hand side of the equation to the left:\n\n:<math> \\int_{S} \\mathbf{u \\cdot T} dS + \\int_V \\mathbf u \\cdot \\mathbf f dV = \\int_V \\boldsymbol\\epsilon : \\boldsymbol\\sigma dV </math>\n\nThe physical interpretation of the above equation is, ''the External virtual work is equal to internal virtual work when equilibrated forces and stresses undergo unrelated but consistent displacements and strains''.\n\nFor practical applications:\n\n* In order to impose equilibrium on real stresses and forces, we use consistent virtual displacements and strains in the virtual work equation.\n* In order to impose consistent displacements and strains, we use equilibriated virtual stresses and forces in the virtual work equation.\n\nThese two general scenarios give rise to two often stated variational principles. They are valid irrespective of material behaviour.\n\n===Principle of virtual displacements===\nDepending on the purpose, we may specialize the virtual work equation. For example, to derive the principle of virtual displacements in variational notations for supported bodies, we specify:\n\n* Virtual displacements and strains as variations of the real displacements and strains using variational notation such as <math> \\delta\\ \\mathbf {u} \\equiv \\mathbf{u}^* </math> and <math> \\delta\\ \\boldsymbol {\\epsilon} \\equiv \\boldsymbol {\\epsilon}^* </math>\n* Virtual displacements be zero on the part of the surface that has prescribed displacements, and thus the work done by the reactions is zero. There remains only external surface forces on the part <math> S_t </math> that do work.\n\nThe virtual work equation then becomes the principle of virtual displacements:\n\n:<math> \\int_{S_t} \\delta\\ \\mathbf{u}^T \\mathbf{T} dS + \\int_{V} \\delta\\ \\mathbf{u}^T \\mathbf{f} dV = \\int_{V}\\delta\\boldsymbol{\\epsilon}^T \\boldsymbol{\\sigma} dV \\qquad \\mathrm{(f)} </math>\n\nThis relation is equivalent to the set of equilibrium equations written for a differential element in the deformable body as well as of the stress boundary conditions on the part <math> S_t </math> of the surface. Conversely, (f) can be reached, albeit in a non-trivial manner, by starting with the differential equilibrium equations and the stress boundary conditions on <math> S_t </math>, and proceeding in the manner similar to (a) and (b).\n\nSince virtual displacements are automatically compatible when they are expressed in terms of [[Continuous function|continuous]], [[single-valued function]]s, we often mention only the need for consistency between strains and displacements. The virtual work principle is also valid for large real displacements; however, Eq.(f) would then be written using more complex measures of stresses and strains.\n\n===Principle of virtual forces===\nHere, we specify:\n\n* Virtual forces and stresses as variations of the real forces and stresses.\n* Virtual forces be zero on the part <math> S_t </math> of the surface that has prescribed forces, and thus only surface (reaction) forces on <math> S_u </math> (where displacements are prescribed) would do work.\n\nThe virtual work equation becomes the principle of virtual forces:\n\n:<math> \\int_{S_u} \\mathbf{u}^T \\delta\\ \\mathbf{T} dS + \\int_{V} \\mathbf{u}^T \\delta\\ \\mathbf{f} dV = \\int_{V} \\boldsymbol{\\epsilon}^T \\delta \\boldsymbol{\\sigma} dV \\qquad \\mathrm{(g)} </math>\n\nThis relation is equivalent to the set of strain-compatibility equations as well as of the displacement boundary conditions on the part <math> S_u </math>. It has another name: the principle of complementary virtual work.\n\n==Alternative forms==\nA specialization of the principle of virtual forces is the [[unit dummy force method]], which is very useful for computing displacements in structural systems. According to [[D'Alembert's principle]], inclusion of inertial forces as additional body forces will give the virtual work equation applicable to dynamical systems. More generalized principles can be derived by:\n\n* allowing variations of all quantities.\n* using [[Lagrange multipliers]] to impose boundary conditions and/or to relax the conditions specified in the two states.\n\nThese are described in some of the references.\n\nAmong the many [[energy principles in structural mechanics]], the virtual work principle deserves a special place due to its generality that leads to powerful applications in [[structural analysis]], [[solid mechanics]], and [[finite element method in structural mechanics]].\n\n==See also==\n*[[Flexibility method]]\n*[[Unit dummy force method]]\n*[[Finite element method in structural mechanics]]\n*[[Calculus of variations]]\n*[[Lagrangian mechanics]]\n*[[Müller-Breslau's principle]]\n\n== External links ==\n* [http://sameradeeb.srv.ualberta.ca/variational-principles/the-principle-of-virtual-work/ Examples applications of the virtual work principle]\n\n==References==\n{{reflist}}\n\n==Bibliography==\n*[[Klaus-Jürgen Bathe|Bathe, K.J.]] \"Finite Element Procedures\", Prentice Hall, 1996. {{ISBN|0-13-301458-4}}\n*Charlton, T.M. ''Energy Principles in Theory of Structures'', Oxford University Press, 1973. {{ISBN|0-19-714102-1}}\n*Dym, C. L. and I. H. Shames, ''Solid Mechanics: A Variational Approach'', McGraw-Hill, 1973.\n*Greenwood, Donald T. ''Classical Dynamics'', Dover Publications Inc., 1977, {{ISBN|0-486-69690-1}}\n*Hu, H. ''Variational Principles of Theory of Elasticity With Applications'', Taylor & Francis, 1984. {{ISBN|0-677-31330-6}}\n*Langhaar, H. L. ''Energy Methods in Applied Mechanics'', Krieger, 1989.\n*[[J. N. Reddy|Reddy, J.N.]] ''Energy Principles and Variational Methods in Applied Mechanics'', John Wiley, 2002. {{ISBN|0-471-17985-X}}\n*Shames, I. H. and Dym, C. L. ''Energy and Finite Element Methods in Structural Mechanics'', Taylor & Francis, 1995, {{ISBN|0-89116-942-3}}\n*Tauchert, T.R. ''Energy Principles in Structural Mechanics'', McGraw-Hill, 1974. {{ISBN|0-07-062925-0}}\n*Washizu, K. ''Variational Methods in Elasticity and Plasticity'', Pergamon Pr, 1982.  {{ISBN|0-08-026723-8}}\n*Wunderlich, W. ''Mechanics of Structures: Variational and Computational Methods'', CRC, 2002. {{ISBN|0-8493-0700-7}}\n\n[[Category:Mechanics]]\n[[Category:Dynamical systems]]\n[[Category:Structural analysis]]\n[[Category:Linkages (mechanical)]]"
    },
    {
      "title": "Williot diagram",
      "url": "https://en.wikipedia.org/wiki/Williot_diagram",
      "text": "The '''Williot diagram''' is a graphical method to obtain an approximate value for displacement of a structure which submitted to a certain load. The method consists of, from a [[Graph (discrete mathematics)|graph]] representation of a [[structural system]], representing the structure's fixed [[vertex (graph theory)|vertices]] as a single, fixed starting point and from there sequentially adding the neighbouring vertices' relative displacements due to [[strain (materials science)|strain]].<ref>{{cite book|author1=C. Hartsuijker|author2=J.W. Welleman|title=Engineering Mechanics: Volume 2: Stresses, Strains, Displacements|url=https://books.google.com/books?id=RB7NcJb7r74C&pg=PA514|date=2007-11-21|publisher=Springer Science & Business Media|isbn=978-1-4020-5763-2|page=514}}</ref><ref>{{cite book|author=D.S. Prakash|title=Graphical Methods in Structural Analysis|url=https://books.google.com/books?id=_IOw1aiAxVAC&pg=PA67|date=1997|publisher=Universities Press|isbn=978-81-7371-046-9|page=67}}</ref>\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Williot Diagram}}\n[[Category:Structural analysis]]\n\n\n{{civil-engineering-stub}}"
    },
    {
      "title": "Wood method",
      "url": "https://en.wikipedia.org/wiki/Wood_method",
      "text": "The '''Wood method''', also known as the '''Merchant–Rankine–Wood method''', is a [[structural analysis]] method which was developed to determine estimates for the effective [[buckling]] length of a compressed member included in a building frames, both in sway and a non-sway buckling modes.<ref name=\"Wood74\"/><ref name=\"RS\"/>  It is named after R. H. Wood.\n\nAccording to this method, the ratio between the critical buckling length and the real length of a column is determined based on two redistribution coefficients, <math>\\eta_1</math> and <math>\\eta_2</math>, which are mapped to a ratio between the effective buckling length of a compressed member and its real length.\n\nThe redistribution coefficients are obtained through the following expressions:\n\n:<math>\\eta_i = \\frac{K_c + K_i}{K_c+K_i + K_i1 + K_i2},\\quad i = 1,2</math>\nwhere <math>K_i</math> are the stiffness coefficients for the adjacent length of columns.\n\nAlthough this method was included in ENV 1993-1-1:1992, it is absent from [[EN 1993-1-1]].\n\n==See also==\n* [[EN 1993]]\n* [[Merchant–Rankine method]]\n* [[Horne method]]\n\n==References==\n<references>\n<ref name=\"Wood74\">{{cite news|last=Wood|first= R. H.|year=1974|title=Effective Lengths of Columns in Multi-Storey Buildings. The Struct. Eng., Vol. 52, 7, |pages=341–346}}</ref>\n<ref name=\"RS\">{{cite book|title=Manual de Dimensionamento de Estruturas Metálicas |last=Simões |first=Rui |year=2007 |publisher=CMM – Associação Portuguesa de Construção Metálica e Mista |isbn=978-972-98376-9-2|page=119}}</ref>\n</references>\n\n{{DEFAULTSORT:Wood Method}}\n[[Category:Structural analysis]]\n\n\n{{technology-stub}}"
    },
    {
      "title": "Yoshimura buckling",
      "url": "https://en.wikipedia.org/wiki/Yoshimura_buckling",
      "text": "'''Yoshimura buckling''' is a triangular mesh [[buckling]] pattern found in thin-walled [[cylinder (geometry)|cylinder]]s under [[axial compression]].<ref>de Vries, J., [http://adsabs.harvard.edu/full/2005ESASP.581E..21D Research on the Yoshimura buckling pattern of small cylindrical thin walled shells], in Proceedings of the European Conference on Spacecraft Structures, Materials and Mechanical Testing 2005 (ESA SP-581). 10–12 May 2005, Noordwijk, The Netherlands. Edited by Karen Fletcher. Bibcode 2005ESASP.581E..21D</ref><ref>{{Cite book | isbn = 9780471974505 | title = Buckling Experiments, Shells, Built-up Structures, Composites and Additional Topics | last1 = Singer | first1 = J. | year = 2002 | publisher = John Wiley & Sons Ltd | last2 = Arbocz | first2 = J. | last3 = Weller | first3 = T. | volume = 2 | page = 640  }}</ref>\n\nThis buckling pattern is named after Yoshimaru Yoshimura, the Japanese researcher who first provided an explanation for its development in a paper first published in Japan in 1951,<ref>{{cite web|url = http://www.dtic.mil/get-tr-doc/pdf?AD=AD0631508|title = The Perplexing Behavior of Thin Circular Cylindrical Shells in Axial Compression|author = Nicholas J. Hoff|date = February 1966|publisher = Stanford University Department of Aeronautics and Astronautics}}</ref> and later republished in the United States in 1955.<ref>{{cite web|url=https://digital.library.unt.edu/ark:/67531/metadc62872/|title=On the mechanism of buckling of a circular cylindrical shell under axial compression|author=Yoshimaru Yoshimura|date=July 1955|publisher=National Advisory Committee For Aeronautics}}</ref>\n\n== References ==\n{{reflist}}\n\n[[Category:Materials science]]\n[[Category:Mechanical failure modes]]\n[[Category:Structural analysis]]\n[[Category:Mechanics]]\n\n\n{{engineering-stub}}"
    },
    {
      "title": "Zero force member",
      "url": "https://en.wikipedia.org/wiki/Zero_force_member",
      "text": "In the field of [[engineering mechanics]], a '''zero force member''' is a member (a single truss segment) in a [[truss]] which, given a specific [[structural load|load]], is at rest: neither in [[tension (mechanics)|tension]], nor in [[compression (physical)|compression]]. In a truss a zero force member is often found at pins (any connections within the truss) where no external load is applied and three or fewer truss members meet. Recognizing basic zero force members can be accomplished by analyzing the [[force]]s acting on an individual pin in a physical [[system]].\n\n''NOTE: If the pin has an external force or [[Moment (physics)|moment]] applied to it, then all of the members attached to that pin are not zero force members UNLESS the external force acts in a manner that fulfills one of the rules below:''\n\n* If two non-collinear members meet in an unloaded [[Joint (building)|joint]], both are zero-force members.\n* If three members meet in an unloaded joint of which two are collinear, then the third member is a zero-force member.\n\n'''Reasons for Zero-force members in a truss system'''\n*These members contribute to the stability of the structure, by providing buckling prevention for long slender members under compressive forces\n*These members can carry loads in the event that variations are introduced in the normal external loading configuration\n\n==See also==\n* [[Structural Engineering]]\n* [[Neutral plane]]\n\n==External links==\n* [https://web.archive.org/web/20060901183411/http://vcity.ou.edu/demoModules/analysis/truss/truss.htm Truss Overview]\n* [https://web.archive.org/web/20060113051049/http://em-ntserver.unl.edu/NEGAHBAN/EM223/note12/note12.htm Another Truss Overview]\n\n==Sources==\n* ''Engineering Mechanics Volume 1: Equilibrium'',  by C. Hartsuijker and J.W. Welleman\n\n{{DEFAULTSORT:Zero Force Member}}\n[[Category:Structural analysis]]\n[[Category:Statics]]"
    },
    {
      "title": "Wavelet",
      "url": "https://en.wikipedia.org/wiki/Wavelet",
      "text": "A '''wavelet''' is a [[wave]]-like [[oscillation]] with an [[amplitude]] that begins at zero, increases, and then decreases back to zero. It can typically be visualized as a \"brief oscillation\" like one recorded by a [[seismograph]] or [[heart monitor]]. Generally, wavelets are intentionally crafted to have specific properties that make them useful for [[signal processing]]. Using a \"reverse, shift, multiply and integrate\" technique called [[convolution]], wavelets can be combined with known portions of a damaged signal to extract information from the unknown portions.\n[[File:Seismic Wavelet.jpg|thumb|Seismic wavelet]]\nFor example, a wavelet could be created to have a frequency of [[Middle C]] and a short duration of roughly a [[32nd note]]. If this wavelet were to be convolved with a signal created from the recording of a song, then the resulting signal would be useful for determining when the Middle C note was being played in the song. Mathematically, the wavelet will correlate with the signal if the unknown signal contains information of similar frequency. This concept of [[correlation]] is at the core of many practical applications of wavelet theory.\n\nAs a mathematical tool, wavelets can be used to extract information from many different kinds of data, including – but not limited to – [[audio signal]]s and images. Sets of wavelets are generally needed to analyze data fully. A set of \"complementary\" wavelets will decompose data without gaps or overlap so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in [[Wavelet compression|wavelet based compression]]/decompression algorithms where it is desirable to recover the original information with minimal loss.\n\nIn formal terms, this representation is a [[wavelet series]] representation of a [[square-integrable function]] with respect to either a [[Complete orthogonal system#Incomplete orthogonal sets|complete]], [[orthonormal]] set of [[basis function]]s, or an [[Complete orthogonal system#Incomplete orthogonal sets|overcomplete]] set or [[frame of a vector space]], for the [[Hilbert space]] of square integrable functions. This is accomplished through [[Coherent states in mathematical physics#The group-theoretical approach|coherent states]].\n\n== Name ==\nThe word ''wavelet'' has been used for decades in digital signal processing and exploration geophysics.<ref>{{cite journal |last1=Ricker |first1=Norman |year=1953 |title=WAVELET CONTRACTION, WAVELET EXPANSION, AND THE CONTROL OF SEISMIC RESOLUTION |journal=Geophysics |volume= 18|issue= 4 |pages=769–792 |doi=10.1190/1.1437927|bibcode=1953Geop...18..769R }}</ref> The equivalent [[French language|French]] word ''ondelette'' meaning \"small wave\" was used by [[Jean Morlet|Morlet]] and [[Alex Grossman|Grossmann]] in the early 1980s.\n\n== Wavelet theory ==\n{{unreferenced section|date=November 2014}}\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of [[time-frequency representation]] for [[continuous-time]] (analog) signals and so are related to [[harmonic analysis]]. Almost all practically useful discrete wavelet transforms use [[discrete-time]] [[filterbank]]s. These filter banks are called the wavelet and scaling coefficients in wavelets nomenclature. These filterbanks may contain either [[finite impulse response]] (FIR) or [[infinite impulse response]] (IIR) filters. The wavelets forming a [[continuous wavelet transform]] (CWT) are subject to the [[Fourier uncertainty principle|uncertainty principle]] of Fourier analysis respective sampling theory: Given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the [[scaleogram]] of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\n\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n\n=== Continuous wavelet transforms (continuous shift and scale parameters) ===\nIn [[continuous wavelet transform]]s, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the [[Lp space|''L<sup>p</sup>'']] [[function space]] ''L''<sup>2</sup>('''R''') ). For instance the signal may be represented on every frequency band of the form [''f'', 2''f''] for all positive frequencies ''f'' > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\n\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in ''L''<sup>2</sup>('''R'''), the ''mother wavelet''. For the example of the scale one frequency band [1, 2] this function is\n:<math>\\psi(t)=2\\,\\operatorname{sinc}(2t)-\\,\\operatorname{sinc}(t)=\\frac{\\sin(2\\pi t)-\\sin(\\pi t)}{\\pi t}</math>\nwith the (normalized) [[sinc function]]. That, Meyer's, and two other examples of mother wavelets are:\n\n{|\n|-\n| [[File:MeyerMathematica.svg|thumb|360px|[[Meyer wavelet|Meyer]]]]\n|}\n{|\n|-\n| [[File:MorletWaveletMathematica.svg|thumb|360px|[[Morlet wavelet|Morlet]]]]\n|}\n{|\n|-\n| [[File:MexicanHatMathematica.svg|thumb|360px|[[Mexican hat wavelet|Mexican hat]]]]\n|}\n\nThe subspace of scale ''a'' or frequency band [1/''a'', 2/''a''] is generated by the functions (sometimes called ''child wavelets'')\n:<math>\\psi_{a,b} (t) = \\frac1{\\sqrt a }\\psi \\left( \\frac{t - b}{a} \\right),</math>\nwhere ''a'' is positive and defines the scale and ''b'' is any real number and defines the shift. The pair (''a'', ''b'') defines a point in the right halfplane '''R'''<sub>+</sub> × '''R'''.\n\nThe projection of a function ''x'' onto the subspace of scale ''a'' then has the form\n:<math>x_a(t)=\\int_\\R WT_\\psi\\{x\\}(a,b)\\cdot\\psi_{a,b}(t)\\,db</math>\nwith ''wavelet coefficients''\n:<math>WT_\\psi\\{x\\}(a,b)=\\langle x,\\psi_{a,b}\\rangle=\\int_\\R x(t){\\psi_{a,b}(t)}\\,dt.</math>\n\nFor the analysis of the signal ''x'', one can assemble the wavelet coefficients into a [[scaleogram]] of the signal.\n\nSee a list of some [[Continuous wavelets]].\n\n=== Discrete wavelet transforms (discrete shift and scale parameters) ===\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the [[wikt:affine|affine]] system for some real parameters ''a'' > 1, ''b'' > 0. The corresponding discrete subset of the halfplane consists of all the points (''a<sup>m</sup>'', ''na<sup>m</sup>b'') with ''m'', ''n'' in '''Z'''. The corresponding ''child wavelets'' are now given as\n:<math>\\psi_{m,n}(t) = \\frac1{\\sqrt{a^m}}\\psi\\left(\\frac{t - nb}{a^m}\\right).</math>\n\nA sufficient condition for the reconstruction of any signal ''x'' of finite energy by the formula\n:<math> x(t)=\\sum_{m\\in\\Z}\\sum_{n\\in\\Z}\\langle x,\\,\\psi_{m,n}\\rangle\\cdot\\psi_{m,n}(t)</math>\nis that the functions <math>\\{\\psi_{m,n}:m,n\\in\\Z\\}</math> form an [[orthonormal basis]] of ''L''<sup>2</sup>('''R''').\n\n=== Multiresolution based discrete wavelet transforms ===\n<!-- MRA:: probably Multi-resolution analysis based transforms... but\nI'm not sure content matches section header... weirdness.\n -->\n[[Image:Daubechies4-functions.svg|thumb|right|D4 wavelet]]\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a [[multiresolution analysis]]. This means that there has to exist an auxiliary function, the ''father wavelet'' φ in ''L''<sup>2</sup>('''R'''), and that ''a'' is an integer. A typical choice is ''a'' = 2 and ''b'' = 1. The most famous pair of father and mother wavelets is the [[Daubechies wavelets|Daubechies]] 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.<ref>{{cite journal |last1=Larson |first1=David R. |year=2007 |title= Wavelet Analysis and Applications (See: Unitary systems and wavelet sets) |series = Appl. Numer. Harmon. Anal. |publisher=Birkhäuser |pages=143–171 }}</ref>\n\nFrom the mother and father wavelets one constructs the subspaces\n:<math>V_m=\\operatorname{span}(\\phi_{m,n}:n\\in\\Z),\\text{ where }\\phi_{m,n}(t)=2^{-m/2}\\phi(2^{-m}t-n)</math>\n:<math>W_m=\\operatorname{span}(\\psi_{m,n}:n\\in\\Z),\\text{ where }\\psi_{m,n}(t)=2^{-m/2}\\psi(2^{-m}t-n).</math>\nThe father wavelet <math>V_{i}</math> keeps the time domain properties, while the mother wavelets <math>W_{i}</math> keeps the frequency domain properties.\n\nFrom these it is required that the sequence\n:<math>\\{0\\}\\subset\\dots\\subset V_{1}\\subset V_{0}\\subset V_{-1}\\subset V_{-2}\\subset\\dots\\subset L^2(\\R)</math>\nforms a [[multiresolution analysis]] of ''L<sup>2</sup>'' and that the subspaces <math>\\dots,W_1,W_0,W_{-1},\\dots\\dots</math> are the orthogonal \"differences\" of the above sequence, that is, ''W<sub>m</sub>'' is the orthogonal complement of ''V<sub>m</sub>'' inside the subspace ''V''<sub>''m''−1</sub>,\n:<math>V_m\\oplus W_m=V_{m-1}.</math>\n\nIn analogy to the [[sampling theorem]] one may conclude that the space ''V<sub>m</sub>'' with sampling distance 2<sup>''m''</sup> more or less covers the frequency baseband from 0 to 2<sup>−''m''-1</sup>. As orthogonal complement, ''W<sub>m</sub>'' roughly covers the band [2<sup>−''m''−1</sup>, 2<sup>−''m''</sup>].\n\nFrom those inclusions and orthogonality relations, especially <math>V_0\\oplus W_0=V_{-1}</math>, follows the existence of sequences <math>h=\\{h_n\\}_{n\\in\\Z}</math> and <math>g=\\{g_n\\}_{n\\in\\Z}</math> that satisfy the identities\n:<math>g_n=\\langle\\phi_{0,0},\\,\\phi_{-1,n}\\rangle</math> so that <math>\\phi(t)=\\sqrt2 \\sum_{n\\in\\Z} g_n\\phi(2t-n),</math> and\n:<math>h_n=\\langle\\psi_{0,0},\\,\\phi_{-1,n}\\rangle</math> so that <math>\\psi(t)=\\sqrt2 \\sum_{n\\in\\Z} h_n\\phi(2t-n).</math>\nThe second identity of the first pair is a [[Refinable function|refinement equation]] for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the [[fast wavelet transform]].\n\nFrom the multiresolution analysis derives the orthogonal decomposition of the space ''L''<sup>2</sup> as\n:<math>L^2=V_{j_0}\\oplus W_{j_0}\\oplus W_{j_0-1}\\oplus W_{j_0-2}\\oplus W_{j_0-3}\\oplus\\dots</math>\nFor any signal or function <math>S\\in L^2</math> this gives a representation in basis functions of the corresponding subspaces as\n:<math>S = \\sum_{k} c_{j_0,k}\\phi_{j_0,k} + \\sum_{j\\le j_0}\\sum_{k} d_{j,k}\\psi_{j,k}</math>\nwhere the coefficients are\n:<math>c_{j_0,k} = \\langle S,\\phi_{j_0,k}\\rangle </math> and\n:<math>d_{j,k} = \\langle S,\\psi_{j,k}\\rangle </math>.\n\n== Mother wavelet ==\nFor practical applications, and for efficiency reasons, one prefers continuously differentiable functions with compact support as mother (prototype) wavelet (functions). However, to satisfy analytical requirements (in the continuous WT) and in general for theoretical reasons, one chooses the wavelet functions from a subspace of the [[Lp space|space]] <math>L^1(\\R)\\cap L^2(\\R).</math> This is the space of [[Lebesgue integration|measurable functions]] that are absolutely and square [[integrable function|integrable]]:\n:<math>\\int_{-\\infty}^{\\infty} |\\psi (t)|\\, dt <\\infty</math> and <math>\\int_{-\\infty}^{\\infty} |\\psi (t)|^2 \\, dt <\\infty.</math>\n\nBeing in this space ensures that one can formulate the conditions of zero mean and square norm one:\n:<math>\\int_{-\\infty}^{\\infty} \\psi (t)\\, dt = 0</math> is the condition for zero mean, and\n:<math>\\int_{-\\infty}^{\\infty} |\\psi (t)|^2\\, dt = 1</math> is the condition for square norm one.\n\nFor ψ to be a wavelet for the [[continuous wavelet transform]] (see there for exact statement), the mother wavelet must satisfy an admissibility criterion (loosely speaking, a kind of half-differentiability) in order to get a stably invertible transform.\n\nFor the [[discrete wavelet transform]], one needs at least the condition that the [[wavelet series]] is a representation of the identity in the [[Lp space|space]] ''L''<sup>2</sup>('''R'''). Most constructions of discrete WT make use of the [[multiresolution analysis]], which defines the wavelet by a scaling function. This scaling function itself is a solution to a functional equation.\n\nIn most situations it is useful to restrict ψ to be a continuous function with a higher number ''M'' of vanishing moments, i.e. for all integer ''m'' < ''M''\n:<math>\\int_{-\\infty}^{\\infty} t^m\\,\\psi (t)\\, dt = 0.</math>\n\nThe mother wavelet is scaled (or dilated) by a factor of ''a'' and translated (or shifted) by a factor of ''b'' to give (under Morlet's original formulation):\n\n:<math>\\psi _{a,b} (t) = {1 \\over \\sqrt a}\\psi \\left( {t - b \\over a} \\right).</math>\n\nFor the continuous WT, the pair (''a'',''b'') varies over the full half-plane '''R'''<sub>+</sub> × '''R'''; for the discrete WT this pair varies over a discrete subset of it, which is also called ''affine group''.\n\nThese functions are often incorrectly referred to as the basis functions of the (continuous) transform. In fact, as in the continuous Fourier transform, there is no basis in the continuous wavelet transform. Time-frequency interpretation uses a subtly different formulation (after Delprat).\n\nRestriction：\n\n(1) <math> \\frac{1}{\\sqrt{a}} \\int_{-\\infty}^\\infty \\varphi_{a1,b1}(t)\\varphi\\left(\\frac{t-b}{a}\\right) \\, dt</math> when a1 = a and b1 = b,\n\n(2) <math>\\Psi (t)</math> has a finite time interval\n\n== Comparisons with Fourier transform (continuous-time) ==\nThe wavelet transform is often compared with the [[Fourier transform]], in which signals are represented as a sum of sinusoids. In fact, the [[Fourier transform]] can be viewed as a special case of the continuous wavelet transform with the choice of the mother wavelet\n<math>\\psi (t) = e^{-2 \\pi i t}</math>.\nThe main difference in general is that wavelets are localized in both time and frequency whereas the standard [[Fourier transform]] is only localized in [[frequency]]. The [[Short-time Fourier transform]] (STFT) is similar to the wavelet transform, in that it is also time and frequency localized, but there are issues with the frequency/time resolution trade-off.\n\nIn particular, assuming a rectangular window region, one may think of the STFT as a transform with a slightly different kernel\n\n:<math>\n\\psi (t) = g(t-u) e^{-2 \\pi i t}\n</math>\n\nwhere <math>g(t-u)</math> can often be written as <math>\\rm{rect}\\left(\\frac{t-u}{\\Delta_t}\\right)</math>, where <math>\\Delta_t</math> and u respectively denote the length and temporal offset of the windowing function. Using [[Parseval's theorem]], one may define the wavelet's energy as\n:<math>\nE = \\int_{-\\infty}^{\\infty} |\\psi (t)|^2\\, dt = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} |\\hat{\\psi}(\\omega)|^2 \\, d\\omega\n</math>\nFrom this, the square of the temporal support of the window offset by time u is given by\n\n:<math>\n\\sigma_t^2 = \\frac{1}{E}\\int |t-u|^2|\\psi(t)|^2 \\, dt\n</math>\n\nand the square of the spectral support of the window acting on a frequency <math>\\xi</math>\n\n:<math>\n\\sigma_\\omega^2 =\\frac{1}{2\\pi E} \\int |\\omega-\\xi|^2|\\hat{\\psi}(\\omega)|^2 \\, d\\omega\n</math>\n\nMultiplication with a rectangular window in the time domain corresponds to convolution with a <math>\\rm{sinc(\\Delta_t\\omega)}</math> function in the frequency domain, resulting in spurious [[ringing artifacts]] for short/localized temporal windows. With the continuous-time Fourier Transform, <math>\\Delta_t \\rightarrow \\infty</math> and this convolution is with a delta function in Fourier space, resulting in the true Fourier transform of the signal <math>x(t)</math>. The window function may be some other [[Apodizing|apodizing filter]], such as a [[Gaussian filter|Gaussian]]. The choice of windowing function will affect the approximation error relative to the true Fourier transform.\n\nA given resolution cell's time-bandwidth product may not be exceeded with the STFT. All STFT basis elements maintain a uniform spectral and temporal support for all temporal shifts or offsets, thereby attaining an equal resolution in time for lower and higher frequencies. The resolution is purely determined by the sampling width.\n\nIn contrast, the wavelet transform's [[Multiresolution analysis|multiresolutional]] properties enables large temporal supports for lower frequencies while maintaining short temporal widths for higher frequencies by the scaling properties of the wavelet transform. This property extends conventional time-frequency analysis into time-scale analysis.<ref>Mallat, Stephane. \"A wavelet tour of signal processing. 1998.\" 250-252.</ref> [[File:Time frequency atom resolution.png|thumb|STFT time-frequency atoms (left) and DWT time-scale atoms (right). The time-frequency atoms are four different basis functions used for the STFT (i.e. '''four separate Fourier transforms required'''). The time-scale atoms of the DWT achieve small temporal widths for high frequencies and good temporal widths for low frequencies with a '''single''' transform basis set.]]\n\nThe discrete wavelet transform is less computationally [[complexity|complex]], taking [[Big O notation|O(''N'')]] time as compared to O(''N''&nbsp;log&nbsp;''N'') for the [[fast Fourier transform]]. This computational advantage is not inherent to the transform, but reflects the choice of a logarithmic division of frequency, in contrast to the equally spaced frequency divisions of the FFT (Fast Fourier Transform) which uses the same basis functions as DFT (Discrete Fourier Transform).<ref>The Scientist and Engineer's Guide to Digital Signal Processing By Steven W. Smith, Ph.D. chapter 8 equation 8-1: http://www.dspguide.com/ch8/4.htm</ref> It is also important to note that this complexity only applies when the filter size has no relation to the signal size. A wavelet without [[compact support]] such as the [[Shannon wavelet]] would require O(''N''<sup>2</sup>). (For instance, a logarithmic Fourier Transform also exists with O(''N'') complexity, but the original signal must be sampled logarithmically in time, which is only useful for certain types of signals.<ref>http://homepages.dias.ie/~ajones/publications/28.pdf</ref>)\n\n== Definition of a wavelet ==\nThere are a number of ways of defining a wavelet (or a wavelet family).\n\n=== Scaling filter ===\nAn orthogonal wavelet is entirely defined by the scaling filter – a low-pass [[finite impulse response]] (FIR) filter of length 2''N'' and sum 1. In [[Biorthogonal system|biorthogonal]] wavelets, separate decomposition and reconstruction filters are defined.\n\nFor analysis with orthogonal wavelets the high pass filter is calculated as the [[quadrature mirror filter]] of the low pass, and reconstruction filters are the time reverse of the decomposition filters.\n\nDaubechies and Symlet wavelets can be defined by the scaling filter.\n\n=== Scaling function ===\nWavelets are defined by the wavelet function ψ(''t'') (i.e. the mother wavelet) and scaling function φ(''t'') (also called father wavelet) in the time domain.\n\nThe wavelet function is in effect a band-pass filter and scaling that for each level halves its bandwidth. This creates the problem that in order to cover the entire spectrum, an infinite number of levels would be required. The scaling function filters the lowest level of the transform and ensures all the spectrum is covered. See<ref>{{cite web|url=http://www.polyvalens.com/blog/?page_id=15#7.+The+scaling+function+%5B7%5D|title=A Really Friendly Guide To Wavelets – PolyValens|author=|date=|website=www.polyvalens.com}}</ref> for a detailed explanation.\n\nFor a wavelet with compact support, φ(''t'') can be considered finite in length and is equivalent to the scaling filter ''g''.\n\nMeyer wavelets can be defined by scaling functions\n\n=== Wavelet function ===\nThe wavelet only has a time domain representation as the wavelet function ψ(''t'').\n\nFor instance, [[Mexican hat wavelet]]s can be defined by a wavelet function. See a list of a few [[Continuous wavelets]].\n\n== History ==\nThe development of wavelets can be linked to several separate trains of thought, starting with [[Alfréd Haar|Haar]]'s work in the early 20th century. Later work by [[Dennis Gabor]] yielded [[Gabor atom]]s (1946), which are constructed similarly to wavelets, and applied to similar purposes. Notable contributions to wavelet theory can be attributed to [[George Zweig|Zweig]]’s discovery of the continuous wavelet transform in 1975 (originally called the cochlear transform and discovered while studying the reaction of the ear to sound),<ref>http://scienceworld.wolfram.com/biography/Zweig.html\n Zweig, George Biography on Scienceworld.wolfram.com</ref> Pierre Goupillaud, [[Alex Grossman|Grossmann]] and [[Jean Morlet|Morlet]]'s formulation of what is now known as the CWT (1982), Jan-Olov Strömberg's early work on [[Strömberg wavelet|discrete wavelets]] (1983), [[Ingrid Daubechies|Daubechies]]' orthogonal wavelets with compact support (1988), [[Stephane Mallat|Mallat]]'s multiresolution framework (1989), [[Ali Akansu|Akansu]]'s [[Binomial QMF]] (1990), Nathalie Delprat's time-frequency interpretation of the CWT (1991), Newland's [[harmonic wavelet transform]] (1993) and many others since.\n\n=== Timeline ===\n* First wavelet ([[Haar Wavelet]]) by [[Alfréd Haar]] (1909)\n* Since the 1970s: [[George Zweig]], [[Jean Morlet]], [[Alex Grossman]]n\n* Since the 1980s: [[Yves Meyer]], [[Stéphane Mallat]], [[Ingrid Daubechies]], [[Ronald Coifman]], [[Ali Akansu]], [[Victor Wickerhauser]]\n\n== Wavelet transforms ==\n{{main|Wavelet transform}}\nA wavelet is a mathematical function used to divide a given function or [[continuous signal|continuous-time signal]] into different scale components. Usually one can assign a frequency range to each scale component. Each scale component can then be studied with a resolution that matches its scale. A wavelet transform is the representation of a function by wavelets. The wavelets are [[Scaling (geometry)|scaled]] and [[Translation (geometry)|translated]] copies (known as \"daughter wavelets\") of a finite-length or fast-decaying oscillating waveform (known as the \"mother wavelet\"). Wavelet transforms have advantages over traditional [[Fourier transform]]s for representing functions that have discontinuities and sharp peaks, and for accurately deconstructing and reconstructing finite, non-[[Periodic function|periodic]] and/or non-[[stationary process|stationary]] signals.\n\nWavelet transforms are classified into [[discrete wavelet transform]]s (DWTs) and [[continuous wavelet transform]]s (CWTs). Note that both DWT and CWT are continuous-time (analog) transforms. They can be used to represent continuous-time (analog) signals. CWTs operate over every possible scale and translation whereas DWTs use a specific subset of scale and translation values or representation grid.\n\nThere are a large number of wavelet transforms each suitable for different applications. For a full list see [[list of wavelet-related transforms]] but the common ones are listed below:\n\n* [[Continuous wavelet transform]] (CWT)\n* [[Discrete wavelet transform]] (DWT)\n* [[Fast wavelet transform]] (FWT)\n* [[Lifting scheme]] & [[Generalized lifting|Generalized Lifting Scheme]]\n* [[Wavelet packet decomposition]] (WPD)\n* [[Stationary wavelet transform]] (SWT)\n* [[Fractional Fourier transform]] (FRFT)\n* [[Fractional wavelet transform]] (FRWT)\n\n=== Generalized transforms ===\nThere are a number of generalized transforms of which the wavelet transform is a special case. For example, Yosef Joseph {{Proper name|Segman}} introduced scale into the [[Heisenberg group]], giving rise to a continuous transform space that is a function of time, scale, and frequency. The CWT is a two-dimensional slice through the resulting 3d time-scale-frequency volume.\n\nAnother example of a generalized transform is the [[chirplet transform]] in which the CWT is also a two dimensional slice through the chirplet transform.\n\nAn important application area for generalized transforms involves systems in which high frequency resolution is crucial. For example, [[darkfield microscope|darkfield]] electron optical transforms intermediate between direct and [[reciprocal space]] have been widely used in the [[harmonic analysis]] of atom clustering, i.e. in the study of [[crystal]]s and [[crystal defect]]s.<ref>P. Hirsch, A. Howie, R. Nicholson, D. W. Pashley and M. J. Whelan (1965/1977) ''Electron microscopy of thin crystals'' (Butterworths, London/Krieger, Malabar FLA) {{isbn|0-88275-376-2}}</ref> Now that [[transmission electron microscope]]s are capable of providing digital images with picometer-scale information on atomic periodicity in [[nanostructure]] of all sorts, the range of [[pattern recognition]]<ref>P. Fraundorf, J. Wang, E. Mandell and M. Rose (2006) Digital darkfield tableaus, ''Microscopy and Microanalysis'' '''12''':S2, 1010–1011 (cf. [https://arxiv.org/abs/cond-mat/0403017 arXiv:cond-mat/0403017])</ref> and [[strain (materials science)|strain]]<ref>M. J. Hÿtch, E. Snoeck and R. Kilaas (1998) Quantitative measurement of displacement and strain fields from HRTEM micrographs, ''Ultramicroscopy'' '''74''':131-146.</ref>/[[metrology]]<ref>Martin Rose (2006) ''Spacing measurements of lattice fringes in HRTEM image using digital darkfield decomposition'' (M.S. Thesis in Physics, U. Missouri – St. Louis)</ref> applications for intermediate transforms with high frequency resolution (like brushlets<ref>F. G. Meyer and R. R. Coifman (1997) ''Applied and Computational Harmonic Analysis'' '''4''':147.</ref> and ridgelets<ref>A. G. Flesia, H. Hel-Or, A. Averbuch, [[Emmanuel Candès|E. J. Candes]], R. R. Coifman and [[David Donoho|D. L. Donoho]] (2001) ''Digital implementation of ridgelet packets'' (Academic Press, New York).</ref>) is growing rapidly.\n\nFractional wavelet transform (FRWT) is a generalization of the classical wavelet transform in the fractional Fourier transform domains. This transform is capable of providing the time- and fractional-domain information simultaneously and representing signals in the time-fractional-frequency plane.<ref>J. Shi, N.-T. Zhang, and X.-P. Liu, \"A novel fractional wavelet transform and its applications,\" Sci. China Inf. Sci., vol. 55, no. 6, pp. 1270–1279, June 2012. URL: http://www.springerlink.com/content/q01np2848m388647/</ref>\n\n== Applications of wavelet transform ==\nGenerally, an approximation to DWT is used for [[data compression]] if a signal is already sampled, and the CWT for [[signal analysis]].<ref>A.N. Akansu, W.A. Serdijn and I.W. Selesnick, [http://web.njit.edu/~akansu/PAPERS/ANA-IWS-WAS-ELSEVIER%20PHYSCOM%202010.pdf Emerging applications of wavelets: A review], Physical Communication, Elsevier, vol. 3, issue 1, pp. 1-18, March 2010.</ref> Thus, DWT approximation is commonly used in engineering and computer science, and the CWT in scientific research.\n\nLike some other transforms, wavelet transforms can be used to transform data, then encode the transformed data, resulting in effective compression. For example, [[JPEG 2000]] is an image compression standard that uses biorthogonal wavelets. This means that although the frame is overcomplete, it is a ''tight frame'' (see types of [[Frame of a vector space|frames of a vector space]]), and the same frame functions (except for conjugation in the case of complex wavelets) are used for both analysis and synthesis, i.e., in both the forward and inverse transform. For details see [[wavelet compression]].\n\nA related use is for smoothing/denoising data based on wavelet coefficient thresholding, also called wavelet shrinkage. By adaptively thresholding the wavelet coefficients that correspond to undesired frequency components smoothing and/or denoising operations can be performed.\n\nWavelet transforms are also starting to be used for communication applications. Wavelet [[OFDM]] is the basic modulation scheme used in HD-PLC (a [[power line communication]]s technology developed by [[Panasonic]]), and in one of the optional modes included in the [[IEEE 1901]] standard. Wavelet OFDM can achieve deeper notches than traditional [[Fast Fourier transform|FFT]] OFDM, and wavelet OFDM does not require a guard interval (which usually represents significant overhead in FFT OFDM systems).<ref name=\"galli\">{{cite journal |title= Recent Developments in the Standardization of Power Line Communications within the IEEE |authors= Stefano Galli, O. Logvinov |journal= IEEE Communications Magazine |date= July 2008 |volume= 46 |number= 7 |pages= 64–71 |doi= 10.1109/MCOM.2008.4557044 }} An overview of P1901 PHY/MAC proposal.</ref>\n\n=== As a representation of a signal ===\n\nOften, signals can be represented well as a sum of sinusoids. However, consider a non-continuous signal with an abrupt discontinuity; this signal can still be represented as a sum of sinusoids, but requires an infinite number, which is an observation known as [[Gibbs phenomenon]]. This, then, requires an infinite number of Fourier coefficients, which is not practical for many applications, such as compression. Wavelets are more useful for describing these signals with discontinuities because of their time-localized behavior (both Fourier and wavelet transforms are frequency-localized, but wavelets have an additional time-localization property). Because of this, many types of signals in practice may be non-sparse in the Fourier domain, but very sparse in the wavelet domain. This is particularly useful in signal reconstruction, especially in the recently popular field of [[compressed sensing]]. (Note that the [[short-time Fourier transform]] (STFT) is also localized in time and frequency, but there are often problems with the frequency-time resolution trade-off. Wavelets are better signal representations because of [[multiresolution analysis]].)\n\nThis motivates why wavelet transforms are now being adopted for a vast number of applications, often replacing the conventional [[Fourier Transform|Fourier transform]]. Many areas of physics have seen this paradigm shift, including [[molecular dynamics]], [[chaos theory]],<ref>{{cite journal|last1=Wotherspoon|first1=T.|last2=et.|first2=al.|title=Adaptation to the edge of chaos with random-wavelet feedback.|journal=J. Phys. Chem.|volume=113|issue=1|pages=19–22|date=2009|doi=10.1021/jp804420g|bibcode=2009JPCA..113...19W}}</ref> [[ab initio]] calculations, [[astrophysics]], [[gravitational wave]] transient data analysis,<ref>{{cite journal |collaboration=LIGO Scientific Collaboration and Virgo Collaboration |last1=Abbott |first1=Benjamin P. |title=Observing gravitational-wave transient GW150914 with minimal assumptions |journal=[[Phys. Rev. D]] |volume=93 |issue=12 |page=122004 |year=2016 |doi=10.1103/PhysRevD.93.122004 |arxiv=1602.03843 |bibcode=2016PhRvD..93l2004A |pmid=}}</ref><ref>{{cite journal |author=V Necula, S Klimenko and G Mitselmakher |title=Transient analysis with fast Wilson-Daubechies time-frequency transform |journal=Journal of Physics: Conference Series |volume=363 |issue= |page=012032 |year=2012 |doi=10.1088/1742-6596/363/1/012032 |bibcode= |pmid=}}</ref> [[Density matrix|density-matrix]] localisation, [[seismology]], [[optics]], [[turbulence]] and [[quantum mechanics]]. This change has also occurred in [[image processing]], [[Electroencephalography|EEG]], [[Electromyography|EMG]],<ref>J. Rafiee et al. Feature extraction of forearm EMG signals for prosthetics, Expert Systems with Applications 38 (2011) 4058–67.</ref> [[Electrocardiography|ECG]] analyses, [[neural oscillation|brain rhythms]], [[DNA]] analysis, [[protein]] analysis, [[climatology]], human sexual response analysis,<ref>J. Rafiee et al. Female sexual responses using signal processing techniques, The Journal of Sexual Medicine 6 (2009) 3086–96. [http://rafiee.us/files/JSM_2009.pdf (pdf)]</ref> general [[signal processing]], [[speech recognition]], acoustics, vibration signals,<ref>J. Rafiee and Peter W. Tse, Use of autocorrelation in wavelet coefficients for fault diagnosis, Mechanical Systems and Signal Processing 23 (2009) 1554–72.</ref> [[computer graphics]], [[multifractal analysis]], and [[sparse coding]]. In [[computer vision]] and [[image processing]], the notion of [[scale space]] representation and Gaussian derivative operators is regarded as a canonical multi-scale representation.\n\n=== Wavelet denoising ===\nSuppose we measure a noisy signal <math>x = s + v </math>. Assume s has a sparse representation in a certain wavelet bases, and <math>v \\ \\sim\\ \\mathcal{N}(0,\\,\\sigma^2I)</math>\n\nSo <math>y = W^Tx = W^Ts + W^Tv = p + z</math>.\n\nMost elements in p are 0 or close to 0, and <math>z \\ \\sim\\ \\ \\mathcal{N}(0,\\,\\sigma^2I)</math>\n\nSince W is orthogonal, the estimation problem amounts to recovery of a signal in iid [[Gaussian noise]]. As p is sparse, one method is to apply a Gaussian mixture model for p.\n\nAssume a prior <math>p \\ \\sim\\ a\\mathcal{N}(0,\\,\\sigma_1^2) +(1- a)\\mathcal{N}(0,\\,\\sigma_2^2)</math>, <math>\\sigma_1^2</math> is the variance of \"significant\" coefficients, and <math>\\sigma_2^2</math> is the variance of \"insignificant\" coefficients.\n\nThen <math>\\tilde p = E(p/y) = \\tau(y) y</math>, <math>\\tau(y)</math> is called the shrinkage factor, which depends on the prior variances <math>\\sigma_1^2</math> and <math>\\sigma_2^2</math>. The effect of the shrinkage factor is that small coefficients are set early to 0, and large coefficients are unaltered.\n\nSmall coefficients are mostly noises, and large coefficients contain actual signal.\n\nAt last, apply the inverse wavelet transform to obtain <math> \\tilde s = W \\tilde p</math>\n\n== List of wavelets ==\n\n=== Discrete wavelets ===\n* [[Beylkin]] (18)\n* [[BNC wavelets]]\n* [[Coiflet]] (6, 12, 18, 24, 30)\n* [[Cohen-Daubechies-Feauveau wavelet]] (Sometimes referred to as CDF N/P or Daubechies biorthogonal wavelets)\n* [[Daubechies wavelet]] (2, 4, 6, 8, 10, 12, 14, 16, 18, 20, etc.)\n* [[Binomial-QMF]] (Also referred to as Daubechies wavelet)\n* [[Haar wavelet]]\n* [[Mathieu wavelet]]\n* [[Legendre wavelet]]\n* [[Villasenor wavelet]]\n* [[Symlet]]<ref>''Matlab Toolbox'' – URL: http://matlab.izmiran.ru/help/toolbox/wavelet/ch06_a32.html</ref>\n\n=== Continuous wavelets ===\n\n==== Real-valued ====\n* [[Beta wavelet]]\n* [[Hermitian wavelet]]\n* [[Hermitian hat wavelet]]\n* [[Meyer wavelet]]\n* [[Mexican hat wavelet]]\n* [[Poisson wavelet]]\n* [[Shannon wavelet]]\n* [[Spline wavelet]]\n* [[Stromberg wavelet]]\n\n==== Complex-valued ====\n* [[Complex Mexican hat wavelet]]\n* [[fbsp wavelet]]\n* [[Morlet wavelet]]\n* [[Shannon wavelet]]\n* [[Modified Morlet wavelet]]\n\n== See also ==\n* [[Chirplet transform]]\n* [[Curvelet]]\n* [[Digital cinema]]\n* [[Filter bank]]s\n* [[Fractal compression]]\n* [[Fractional Fourier transform]]\n* [[JPEG 2000]]\n* [[Multiresolution analysis]]\n* [[Noiselet]]\n* [[Non-separable wavelet]]\n* [[Scale space]]\n* [[Scaled correlation]]\n* [[Shearlet]]\n* [[Short-time Fourier transform]]\n* [[Ultra wideband]] radio- transmits wavelets\n* [[Wave packet]]\n* [[Gabor wavelet#Wavelet space]]<ref>Erik Hjelmås  (1999-01-21) ''Gabor Wavelets'' URL: http://www.ansatt.hig.no/erikh/papers/scia99/node6.html</ref>\n* [[Dimension reduction]]\n* [[Fourier-related transforms]]\n* [[Spectrogram]]\n* [[Huygens–Fresnel principle]] (physical wavelets)\n\n== References ==\n=== Citations ===\n{{Reflist|30em}}\n\n=== Sources ===\n* Paul S. Addison, ''The Illustrated Wavelet Transform Handbook'', [[Institute of Physics]], 2002, {{isbn|0-7503-0692-0}}\n* [[Ali Akansu]] and Richard Haddad, ''Multiresolution Signal Decomposition: Transforms, Subbands, Wavelets'', Academic Press, 1992, {{isbn|0-12-047140-X}}\n* B. Boashash, editor, \"Time-Frequency Signal Analysis and Processing – A Comprehensive Reference\", Elsevier Science, Oxford, 2003, {{isbn|0-08-044335-4}}.\n* [[Tony F. Chan]] and [https://sites.google.com/view/jackieshen/ \"Jackie (Jianhong) Shen\"], ''Image Processing and Analysis – Variational, PDE, Wavelet, and Stochastic Methods'', Society of Applied Mathematics, {{isbn|0-89871-589-X}} (2005)\n* [[Ingrid Daubechies]], ''Ten Lectures on Wavelets'', Society for Industrial and Applied Mathematics, 1992, {{isbn|0-89871-274-2}}\n* Ramazan Gençay, Faruk Selçuk and Brandon Whitcher, ''An Introduction to Wavelets and Other Filtering Methods in Finance and Economics'', Academic Press, 2001, {{isbn|0-12-279670-5}}\n* Haar A., ''Zur Theorie der orthogonalen Funktionensysteme'', Mathematische Annalen, '''69''', pp 331–371, 1910.\n* Barbara Burke Hubbard, \"The World According to Wavelets: The Story of a Mathematical Technique in the Making\", AK Peters Ltd, 1998, {{isbn|1-56881-072-5}}, {{isbn|978-1-56881-072-0}}\n* Gerald Kaiser, ''A Friendly Guide to Wavelets'', Birkhauser, 1994, {{isbn|0-8176-3711-7}}\n* [[Stéphane Mallat]], \"A wavelet tour of signal processing\" 2nd Edition, Academic Press, 1999, {{isbn|0-12-466606-X}}\n* Donald B. Percival and Andrew T. Walden, ''Wavelet Methods for Time Series Analysis'', Cambridge University Press, 2000, {{isbn|0-521-68508-7}}\n*{{Citation | last1=Press | first1=WH | last2=Teukolsky | first2=SA | last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press |  location=New York | isbn=978-0-521-88068-8 | chapter=Section 13.10. Wavelet Transforms | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=699}}\n* [[P. P. Vaidyanathan]], ''Multirate Systems and Filter Banks'', Prentice Hall, 1993, {{isbn|0-13-605718-7}}\n* Mladen Victor Wickerhauser, ''Adapted Wavelet Analysis From Theory to Software'', A K Peters Ltd, 1994, {{isbn|1-56881-041-5}}\n* Martin Vetterli and Jelena Kovačević, \"Wavelets and Subband Coding\", Prentice Hall, 1995, {{isbn|0-13-097080-8}}\n\n== External links ==\n{{extlinks|section|date=July 2016}}\n{{Wiktionary|wavelet}}\n{{commons|Wavelet|Wavelet}}\n* [http://www.wavelet.org Wavelet Digest]\n* [http://wavelets.org/software.php Wavelets: Software] – a list of useful wavelet transform frameworks, libraries, and other software\n* {{springer|title=Wavelet analysis|id=p/w097160}}\n* [http://web.njit.edu/~ali/s1.htm 1st NJIT Symposium on Wavelets (April 30, 1990) (First Wavelets Conference in USA)]\n* [http://web.njit.edu/~ali/NJITSYMP1990/AkansuNJIT1STWAVELETSSYMPAPRIL301990.pdf Binomial-QMF Daubechies Wavelets]\n* [http://www-math.mit.edu/~gs/papers/amsci.pdf Wavelets] by Gilbert Strang, American Scientist 82 (1994) 250–255. (A very short and excellent introduction)\n* [http://wavelets.ens.fr/ENSEIGNEMENT/COURS/UCSB/index.html Course on Wavelets given at UC Santa Barbara, 2004]\n* [http://www.isye.gatech.edu/~brani/wp/kidsA.pdf Wavelets for Kids (PDF file)] (Introductory (for very smart kids!))\n* [http://www.relisoft.com/Science/Physics/sampling.html Very basic explanation of Wavelets and how FFT relates to it]\n* [http://www.laurent-duval.eu/siva-wits-where-is-the-starlet.html WITS: Where Is The Starlet?] A dictionary of tens of wavelets and wavelet-related terms ending in -let, from activelets to x-lets through bandlets, contourlets, curvelets, noiselets, wedgelets.\n* [http://bigwww.epfl.ch/publications/blu0001.pdf The Fractional Spline Wavelet Transform] describes a [[fractional wavelet transform]] based on fractional b-Splines.\n* [https://dx.doi.org/10.1016/j.sigpro.2011.04.025 A Panorama on Multiscale Geometric Representations, Intertwining Spatial, Directional and Frequency Selectivity] provides a tutorial on two-dimensional oriented wavelets and related geometric multiscale transforms.\n* [http://tx.technion.ac.il/~rc/SignalDenoisingUsingWavelets_RamiCohen.pdf Signal Denoising using Wavelets]\n* [http://www.vigyanprasar.gov.in/dream/mar2005/english.pdf Dream 2047, March 2005, page 29]\n* [https://www.scribd.com/document/370574131/Concise-Introduction-to-Wavelets Concise Introduction to Wavelets by René Puchinger]\n\n{{Statistics|analysis}}\n\n[[Category:Wavelets| ]]\n[[Category:Time–frequency analysis]]\n[[Category:Signal processing]]"
    },
    {
      "title": "Wavelet transform",
      "url": "https://en.wikipedia.org/wiki/Wavelet_transform",
      "text": "[[File:Jpeg2000 2-level wavelet transform-lichtenstein.png|thumb|300px|An example of the 2D [[discrete wavelet transform]] that is used in [[JPEG2000]].]]\n{{broader|Wavelet}}\nIn  [[mathematics]], a '''wavelet series''' is a representation of a [[square-integrable]] ([[real number|real]]- or [[complex number|complex]]-valued) [[function (mathematics)|function]] by a certain [[orthonormal]] [[series (mathematics)|series]] generated by a [[wavelet]]. This article provides a formal, mathematical definition of an '''orthonormal wavelet''' and of the '''integral wavelet transform'''. \n\n==Definition==\nA function <math>\\scriptstyle \\psi \\,\\in\\, L^2(\\mathbb{R})</math> is called an '''orthonormal wavelet''' if it can be used to define a [[Hilbert space#Orthonormal bases|Hilbert basis]], that is a [[complete space|complete]] [[orthonormality|orthonormal system]], for the [[Hilbert space]] <math>\\scriptstyle L^2\\left(\\mathbb{R}\\right)</math> of [[Square-integrable function|square integrable]] functions.\n\nThe Hilbert basis is constructed as the family of functions <math>\\scriptstyle  \\{\\psi_{jk}:\\, j,\\, k \\,\\in\\, \\Z\\}</math> by means of [[Dyadic transformation|dyadic]] [[translation (geometry)|translation]]s and [[dilation (operator theory)|dilation]]s of <math>\\scriptstyle \\psi\\,</math>,\n:<math>\\psi_{jk}(x) = 2^\\frac{j}{2} \\psi\\left(2^jx - k\\right)\\,</math>\n\nfor integers <math>\\scriptstyle j,\\, k \\,\\in\\, \\mathbb{Z}</math>.\n\nIf under the standard [[inner product]] on <math>\\scriptstyle L^2\\left(\\mathbb{R}\\right)</math>,\n:<math>\\langle f, g\\rangle = \\int_{-\\infty}^\\infty f(x)\\overline{g(x)}dx</math>\n\nthis family is orthonormal, it is an orthonormal system:\n:<math>\\begin{align}\n  \\langle\\psi_{jk},\\psi_{lm}\\rangle &= \\int_{-\\infty}^\\infty \\psi_{jk}(x)\\overline{\\psi_{lm}(x)}dx \\\\\n                                    &=\\delta_{jl}\\delta_{km}\n\\end{align}</math>\n\nwhere <math>\\scriptstyle \\delta_{jl}\\,</math> is the [[Kronecker delta]].\n\nCompleteness is satisfied if every function <math>\\scriptstyle f \\,\\in\\, L^2\\left(\\mathbb{R}\\right)</math> may be expanded in the basis as\n:<math>f(x) = \\sum_{j, k=-\\infty}^\\infty c_{jk} \\psi_{jk}(x)</math>\n\nwith convergence of the series understood to be [[norm (mathematics)#Properties|convergence in norm]]. Such a representation of ''f'' is known as a '''wavelet series'''.  This implies that an orthonormal wavelet is [[dual wavelet|self-dual]].\n\nThe '''integral wavelet transform''' is the [[integral transform]] defined as\n\n:<math>\\left[W_\\psi f\\right](a, b) = \\frac{1}{\\sqrt{|a|}} \\int_{-\\infty}^\\infty \\overline{\\psi\\left(\\frac{x-b}{a}\\right)}f(x)dx\\,</math>\n\nThe '''wavelet coefficients''' <math>\\scriptstyle c_{jk}</math> are then given by\n\n:<math>c_{jk} = \\left[W_\\psi f\\right]\\left(2^{-j}, k2^{-j}\\right)</math>\n\nHere, <math>\\scriptstyle a \\;=\\; 2^{-j}</math> is called the '''binary dilation''' or '''dyadic dilation''', and <math>\\scriptstyle b \\;=\\; k2^{-j}</math> is the '''binary''' or '''dyadic position'''.\n\n== Principle ==\n\nThe fundamental idea of wavelet transforms is that the transformation should allow only changes in time extension, but not shape.  This is affected by choosing suitable basis functions that allow for this.{{how|date=July 2013}} Changes in the time extension are expected to conform to the corresponding analysis frequency of the basis function. Based on the [[Uncertainty principle#Signal processing|uncertainty principle]] of signal processing,\n\n:<math>\\Delta t \\Delta \\omega \\geqq \\frac{1}{2}</math>\n\nwhere t represents time and ω angular frequency (ω = 2πf, where f is temporal frequency).\n\nThe higher the required resolution in time, the lower the resolution in frequency has to be. The larger the extension of the analysis windows is chosen, the larger is the value of <math>\\scriptstyle \\Delta t</math>{{how|date=May 2014}}.\n\n[[File:Basis function with compression factor.jpg|right|430px]]\n\nWhen Δt is large,\n# Bad time resolution\n# Good frequency resolution\n# Low frequency, large scaling factor\n\nWhen Δt is small\n# Good time resolution\n# Bad frequency resolution\n# High frequency, small scaling factor\n\nIn other words, the basis function Ψ can be regarded as an impulse response of a system with which the function x(t) has been filtered. The transformed signal provides information about the time and the frequency. Therefore, wavelet-transformation contains information similar to the [[Short-time Fourier transform|short-time-Fourier-transformation]], but with additional special properties of the wavelets, which show up at the resolution in time at higher analysis frequencies of the basis function. The difference in time resolution at ascending frequencies for the [[Fourier transform]] and the wavelet transform is shown below.\n\n[[File:STFT and WT.jpg|center|500px]]\n\nThis shows that wavelet transformation is good in time resolution of high frequencies, while for slowly varying functions, the frequency resolution is remarkable.\n\nAnother example: The analysis of three superposed sinusoidal signals <math>\\scriptstyle y(t) \\;=\\; \\sin(2 \\pi f_0 t) \\;+\\; \\sin(4 \\pi f_0 t) \\;+\\; \\sin(8 \\pi f_0 t)</math> with STFT and wavelet-transformation.\n\n[[File:Analysis of three superposed sinusoidal signals.jpg|center|500px]]\n\n==Wavelet compression==\n'''Wavelet compression''' is a form of [[data compression]] well suited for [[image compression]] (sometimes also [[video compression]] and [[audio compression (data)|audio compression]]). Notable implementations are [[JPEG 2000]], [[DjVu]] and [[ECW (file format)|ECW]] for still images, [[CineForm]], and the BBC's [[Dirac (codec)|Dirac]]. The goal is to store image data in as little space as possible in a [[Computer file|file]].  Wavelet compression can be either [[lossless data compression|lossless]] or [[lossy data compression|lossy]].<ref>[[JPEG 2000]], for example, may use a 5/3 wavelet for lossless (reversible) transform and a 9/7 wavelet for lossy (irreversible) transform.</ref>\n\nUsing a wavelet transform, the wavelet compression methods are adequate for representing [[Transient (acoustics)|transient]]s, such as percussion sounds in audio, or high-frequency components in two-dimensional images, for example an image of stars on a night sky. This means that the transient elements of a data signal can be represented by a smaller amount of information than would be the case if some other transform, such as the more widespread [[discrete cosine transform]], had been used.\n\nDiscrete wavelet transform has been successfully applied for the compression of electrocardiograph (ECG) signals<ref>A. G. Ramakrishnan and S. Saha, [http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=649997&url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F10%2F14156%2F00649997.pdf%3Farnumber%3D649997 \"ECG coding by wavelet-based linear prediction,\"] ''IEEE Trans. Biomed. Eng.'', Vol. 44, No. 12, pp. 1253-1261, 1977.</ref> In this work, the high correlation between the corresponding wavelet coefficients of signals of successive cardiac cycles is utilized employing linear prediction.\n \nWavelet compression is not good for all kinds of data: transient signal characteristics mean good wavelet compression, while smooth, periodic signals are better compressed by other methods, particularly traditional harmonic compression (frequency domain, as by Fourier transforms and related).\n\nSee [https://web.archive.org/web/20100228145846/http://x264dev.multimedia.cx/?p=317 Diary Of An x264 Developer: The problems with wavelets] (2010) for discussion of practical issues of current methods using wavelets for video compression.\n\n===Method===\nFirst a wavelet transform is applied.  This produces as many [[coefficient]]s as there are [[pixel]]s in the image (i.e., there is no compression yet since it is only a transform).  These [[coefficient]]s can then be compressed more easily because the information is statistically concentrated in just a few coefficients.  This principle is called [[transform coding]].  After that, the [[coefficient]]s are [[Quantization (signal processing)|quantized]] and the quantized values are [[entropy encoding|entropy encoded]] and/or [[run-length encoding|run length encoded]].\n\nA few 1D and 2D applications of wavelet compression use a technique called \"wavelet footprints\".<ref>\n N. Malmurugan, A. Shanmugam, S. Jayaraman and V. V. Dinesh Chander.\n [http://www.acadjournal.com/2005/V14/part6/p1/ \"A New and Novel Image Compression Algorithm Using Wavelet Footprints\"]\n</ref><ref>\n Ho Tatt Wei and Jeoti, V.\n \"A wavelet footprints-based compression scheme for ECG signals\".\n {{Cite book| doi = 10.1109/TENCON.2004.1414412| year = 2004| last1 = Ho Tatt Wei| volume = A| last2 = Jeoti | first2 = V.| pages = 283 | chapter = A wavelet footprints-based compression scheme for ECG signals| title = 2004 IEEE Region 10 Conference TENCON 2004| isbn = 0-7803-8560-8}}\n</ref>\n\n==Comparison with Fourier transform and time-frequency analysis==\n{| class=\"wikitable\"\n|-\n! Transform !! Representation !! Input\n|-\n| [[Fourier transform]] || <math> \\hat f(\\xi) = \\int_{-\\infty}^{\\infty} f(x)e^{-2 \\pi ix \\xi}\\, dx</math> || ''ξ'', frequency\n|-\n| [[Time-frequency analysis]] || <math>X(t, f)</math> || ''t'', time; ''f'', frequency\n|-\n| Wavelet transform || <math> X(a,b) = \\frac{1}{\\sqrt{a}}\\int_{-\\infty}^{\\infty}\\overline{\\Psi\\left(\\frac{t - b}{a}\\right)} x(t)\\, dt </math> || ''a'', scaling; ''b'', time\n|}\nWavelets have some slight benefits over Fourier transforms in reducing computations when examining specific frequencies. However, they are rarely more sensitive, and indeed, the common [[Morlet wavelet]] is mathematically identical to a [[short-time Fourier transform]] using a Gaussian window function.<ref>{{cite journal|last1=Bruns|first1=Andreas|title=Fourier-, Hilbert- and wavelet-based signal analysis: are they really different approaches?|journal=Journal of Neuroscience Methods|date=2004|volume=137|issue=2|pages=321–332|doi=10.1016/j.jneumeth.2004.03.002|pmid=15262077|url=https://www.ncbi.nlm.nih.gov/pubmed/?term=10.1016%2Fj.jneumeth.2004.03.002}}</ref> The exception is when searching for signals of a  known, non-sinusoidal shape (e.g., heartbeats); in that case, using matched wavelets can outperform standard STFT/Morlet analyses.<ref>{{cite book|last1=Krantz|first1=Steven G.|title=A Panorama of Harmonic Analysis|date=1999|publisher=Mathematical Association of America|isbn=0-88385-031-1}}</ref>\n\n==Other practical applications==\nThe wavelet transform can provide us with the frequency of the signals and the time associated to those frequencies, making it very convenient for its application in numerous fields. For instance, signal processing of accelerations for gait analysis,<ref>[https://www.youtube.com/watch?v=DTpEVQSEBBk \"Novel method for stride length estimation with body area network accelerometers\"], ''IEEE BioWireless 2011'', pp. 79-82</ref> for fault detection,<ref>{{ cite journal | title=Shannon wavelet spectrum analysis on truncated vibration signals for machine incipient fault detection| journal=Measurement Science and Technology| year=2012 | last=Liu | volume=23 | issue=5 | pages=1–11 | first1=Jie | doi=10.1088/0957-0233/23/5/055604}}</ref> for design of low power pacemakers and also in ultra-wideband (UWB) wireless communications.<ref>{{Cite journal | doi = 10.1016/j.phycom.2009.07.001| title = Emerging applications of wavelets: A review| journal = Physical Communication| volume = 3| pages = 1| year = 2010| last1 = Akansu | first1 = A. N. | last2 = Serdijn | first2 = W. A. | last3 = Selesnick | first3 = I. W. | url = http://web.njit.edu/~akansu/PAPERS/ANA-IWS-WAS-ELSEVIER%20PHYSCOM%202010.pdf}}</ref>\n\n{{ordered list\n| Discretizing of the c-τ-axis\n\nApplied the following discretization of frequency and time:\n:<math>\\begin{align}\n     c_n &= c_0^n \\\\\n  \\tau_m &= m \\cdot T \\cdot c_0^n\n\\end{align}</math>\n\nLeading to wavelets of the form, the discrete formula for the basis wavelet:\n:<math>\\Psi(k, n, m) = \\frac{1}{\\sqrt{c_0^n}}\\cdot\\Psi\\left[\\frac{k - m c_0^n}{c_0^n}T\\right] = \\frac{1}{\\sqrt{c_0^n}}\\cdot\\Psi\\left[\\left(\\frac{k}{c_0^n} - m\\right)T\\right]</math>\n\nSuch discrete wavelets can be used for the transformation:\n:<math>Y_{DW}(n, m) = \\frac{1}{\\sqrt{c_0^n}}\\cdot\\sum_{k=0}^{K - 1} y(k)\\cdot\\Psi\\left[\\left(\\frac{k}{c_0^n} - m\\right)T\\right]</math>\n\n| Implementation via the FFT (fast Fourier transform)\n\nAs apparent from wavelet-transformation representation (shown below) \n:<math>Y_W(c, \\tau) = \\frac{1}{\\sqrt{c}}\\cdot\\int_{-\\infty}^{\\infty} y(t) \\cdot \\Psi\\left(\\frac{t - \\tau}{c}\\right)\\, dt </math>\n\nwhere c is scaling factor, τ represents time shift factor\n\nand as already mentioned in this context, the wavelet-transformation corresponds to a convolution of a function y(t) and a wavelet-function. A convolution can be implemented as a multiplication in the frequency domain. With this the following approach of implementation results into:\n* Fourier-transformation of signal y(k) with the FFT\n* Selection of a discrete scaling factor <math>c_n</math>\n* Scaling of the wavelet-basis-function by this factor <math>c_n</math> and subsequent FFT of this function\n* Multiplication with the transformed signal YFFT of the first step\n* Inverse transformation of the product into the time domain results in ''Y<sub>W</sub><math>(c, \\tau)</math>'' for different discrete values of τ and a discrete value of <math>c_n</math>\n* Back to the second step, until all discrete scaling values for <math>c_n</math>are processed\n\nThere are many different types of wavelet transforms for specific purposes. See also a full [[list of wavelet-related transforms]] but the common ones are listed below: [[Mexican hat wavelet]], [[Haar Wavelet]], [[Daubechies wavelet]], triangular wavelet.\n}}\n\n==See also==\n* [[Continuous wavelet transform]]\n* [[Discrete wavelet transform]]\n* [[Complex wavelet transform]]\n* [[Stationary wavelet transform]]\n* [[Dual wavelet]]\n* [[Multiresolution analysis]]\n* [[MrSID]], the image format developed from original wavelet compression research at [[Los Alamos National Laboratory]] (LANL).\n* [[ECW (file format)|ECW]], a wavelet-based [[geospatial]] image format designed for speed and processing efficiency\n* [[JPEG 2000]], a wavelet-based [[image compression]] standard\n* [[DjVu]] format uses wavelet-based IW44 algorithm for image compression\n* [[scaleogram]]s, a type of [[spectrogram]] generated using wavelets instead of a [[short-time Fourier transform]].\n* [[Wavelet]]\n* [[Haar wavelet]]\n* [[Daubechies wavelet]]\n* [[Morlet wavelet]]\n* [[Gabor wavelet]]\n* [[Chirplet transform]]\n* [[Time-frequency representation]]\n* [[S transform]]\n* [[Short-time Fourier transform]]\n\n==References==\n* {{cite book |first=Yves |last=Meyer |title= Wavelets and Operators |year=1992 |publisher= Cambridge University Press |location=Cambridge |isbn=0-521-42000-8}}\n* {{cite book |first=Charles K. |last=Chui |title=An Introduction to Wavelets |year=1992 |publisher=Academic Press |location=San Diego |isbn=0-12-174584-8}}\n* {{cite book |first1=Ali N. |last1=Akansu |first2=Richard A. |last2=Haddad|title=Multiresolution Signal Decomposition: Transforms, Subbands, Wavelets |year=1992 |publisher=Academic Press |location=San Diego |isbn=978-0-12-047141-6}}\n{{Reflist}}\n\n==External links==\n{{Commons category|Wavelets}}\n* {{cite web\n| author = Amara Graps\n| date=  \n| title = An Introduction to Wavelets\n| url = http://dl.acm.org/citation.cfm?id=615342\n}}\n\n* {{cite web|author=Robi Polikar|first=|date=2001-01-12|website=|archive-url=|archive-date=|dead-url=|access-date=|title=The Wavelet Tutorial|url=http://users.rowan.edu/~polikar/WTtutorial.html}}\n\n* [https://www.scribd.com/document/370574131/Concise-Introduction-to-Wavelets Concise Introduction to Wavelets by René Puchinger]\n\n{{Authority control}}\n\n[[Category:Wavelets| ]]\n[[Category:Functional analysis]]\n[[Category:Signal processing]]\n[[Category:Image compression]]"
    },
    {
      "title": "Wavelet transform modulus maxima method",
      "url": "https://en.wikipedia.org/wiki/Wavelet_transform_modulus_maxima_method",
      "text": "The '''wavelet transform modulus maxima (WTMM)''' is a method for detecting the [[fractal dimension]] of a signal.\n\nMore than this, the WTMM is capable of partitioning the time and scale domain of a signal into fractal dimension regions, and the method is sometimes referred to as a \"mathematical microscope\" due to its ability to inspect the multi-scale dimensional characteristics of a signal and possibly inform about the sources of these characteristics.\n\nThe WTMM method uses [[continuous wavelet transform]] rather than [[Fourier transform]]s to detect singularities [[Mathematical singularity|singularity]] – that is discontinuities, areas in the signal that are not continuous at a particular derivative.\n\nIn particular, this method is useful when analyzing [[multifractal]] signals, that is, signals having multiple fractal dimensions.\n\n== Description ==\n\nConsider a signal that can be represented by the following equation:\n\n: <math>f(t) = a_0 + a_1 (t - t_i) + a_2(t - t_i)^2 + \\cdots + a_h(t - t_i)^{h_i} \\, </math>\n\nwhere <math> t </math> is close to <math> t_i </math> and <math> h_i </math> is a non-integer quantifying the local singularity.  (Compare this to a [[Taylor series]], where in practice only a limited number of low-order terms are used to approximate a continuous function.)\n\nGenerally, a [[continuous wavelet transform]] decomposes a signal as a function of time, rather than assuming the signal is stationary (For example, the Fourier transform).   Any continuous wavelet can be used, though the first derivative of the [[Gaussian distribution]] and the [[Mexican hat wavelet]] (2nd derivative of Gaussian) are common.  Choice of wavelet may depend on characteristics of the signal being investigated.\n\nBelow we see one possible wavelet basis given by the first derivative of the Gaussian:\n\n: <math>G' (t,a,b) = \\frac{a}{(2\\pi)^{-1/2}}(t - b) e^{\\left(\\frac{-(t-b)^2}{2a^2}\\right)} \\,</math>\n\nOnce a \"mother wavelet\" is chosen, the continuous wavelet transform is carried out as a continuous, [[square-integrable function]] that can be scaled and translated.  Let <math>a > 0</math> be the scaling constant and <math>b\\in\\mathbb{R}</math> be the translation of the wavelet along the signal:\n\n: <math>X_w(a,b)=\\frac{1}{\\sqrt{a}} \\int_{-\\infty}^\\infty x(t)\\psi^\\ast \\left(\\frac{t-b}{a}\\right)\\, dt</math>\n\nwhere <math>\\psi(t)</math>  is a continuous function in both the time domain and the frequency domain called the mother wavelet and <math>^{\\ast}</math> represents the operation of [[complex conjugate]].\n\nBy calculating <math>X_w(a,b) </math> for subsequent wavelets that are derivatives of the mother wavelet, singularities can be identified.  Successive derivative wavelets remove the contribution of lower order terms in the signal, allowing the maximum <math>h_i</math> to be detected.  (Recall that when taking derivatives, lower order terms become 0.)  This is the \"modulus maxima\".\n\nThus, this method identifies the singularity spectrum by convolving the signal with a wavelet at different scales and time offsets.\n\nThe WTMM is then capable of producing{{Vague|date=October 2013}} a \"skeleton\" that partitions the scale and time space by fractal dimension.\n\n== History ==\n\nThe WTMM was developed out of the larger field of continuous wavelet transforms, which arose in the 1980s, and its contemporary fractal dimension methods.\n\nAt its essence, it is a combination of fractal dimension \"box counting\" methods and continuous wavelet transforms, where wavelets at various scales are used instead of boxes.\n\nWTMM was originally developed by Mallat and Hwang in 1992 and used for image processing. [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=119727&isnumber=3425]\n\nBacry, Muzy, and Arneodo were early users of this methodology. [http://prl.aps.org/abstract/PRL/v67/i25/p3515_1][http://pre.aps.org/abstract/PRE/v47/i2/p875_1] It has subsequently been used in fields related to signal processing.\n\n== References ==\n\n* Alain Arneodo et al. (2008), [[Scholarpedia]], 3(3):4103. [http://www.scholarpedia.org/article/Wavelet-based_multifractal_analysis]\n* ''A Wavelet Tour of Signal Processing'', by Stéphane Mallat; {{isbn|012466606X}}; Academic Press, 1999 [http://www.ceremade.dauphine.fr/~peyre/wavelet-tour/]\n* Mallat, S.; Hwang, W.L.;, \"Singularity detection and processing with wavelets,\" ''IEEE Transactions on Information Theory'', volume 38, number 2, pages 617–643, Mar 1992 {{doi|10.1109/18.119727}} [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=119727&isnumber=3425]\n* Arneodo on Wavelets [http://www.iscpif.fr/tiki-index.php?page=CSSS'08+Arneodo&highlight=towards]\n* \"Wavelets and multifractal formalism for singular signals : application to turbulence data\", J.F. Muzy, E. Bacry and A. Arneodo, ''Physical Review Letters'' 67, 3515 (1991). [http://prl.aps.org/abstract/PRL/v67/i25/p3515_1]\n* \"Multifractal formalism for fractal signals: the structure function approach versus the wavelet transform modulus maxima method\", J.F. Muzy, E. Bacry and A. Arneodo, ''Phys. Rev. E'' 47, 875 [http://pre.aps.org/abstract/PRE/v47/i2/p875_1]\n\n[[Category:Wavelets| ]]"
    },
    {
      "title": "Bandelet (computer science)",
      "url": "https://en.wikipedia.org/wiki/Bandelet_%28computer_science%29",
      "text": "'''Bandelets''' are an [[orthonormal basis]] that is adapted to geometric boundaries.  Bandelets can be interpreted as a warped [[wavelet]] basis.  The motivation behind bandelets is to perform a transform on functions defined as smooth functions on smoothly bounded domains.  As bandelet construction utilizes [[wavelets]], many of the results follow.  Similar approaches to take account of geometric structure were taken for [[contourlet]]s and [[curvelet]]s.\n\n== See also ==\n*[[Wavelet]]\n*[[Multiresolution analysis]]\n*[[Scale space]]\n\n== References ==\n{{refbegin}}\n* {{cite journal| last1 = Le Pennec | first1 = E.| last2 = Mallat | first2 = S.| doi = 10.1109/TIP.2005.843753| title = Sparse geometric image representations with bandelets| url = http://www.cmap.polytechnique.fr/~mallat/papiers/PublBandIEEE.pdf| journal = IEEE Transactions on Image Processing| volume = 14| issue = 4| pages = 423–438| date=April 2005 | pmid =  15825478| pmc = | citeseerx = 10.1.1.2.5888}}\n* {{cite journal| last1 = Peyré| first1 = G.| last2 = Mallat| first2 = S. P.| doi = 10.1145/1073204.1073236| title = Surface compression with geometric bandelets| url = http://www.ceremade.dauphine.fr/~peyre/publications/PeyreMallatSIGGRAPH05.pdf| journal = [[ACM Transactions on Graphics]]| series = Proceedings of ACM SIGGRAPH 2005| volume = 24| issue = 3| pages = 601–608| date = July 2005| pmid = | pmc = | deadurl = yes| archiveurl = https://web.archive.org/web/20061129073215/http://www.ceremade.dauphine.fr/~peyre/publications/PeyreMallatSIGGRAPH05.pdf| archivedate = 2006-11-29| df = }}\n{{refend}}\n\n== External links ==\n* [http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=7914&objectType=FILE Bandelet toolbox] on MatLab Central\n\n[[Category:Wavelets]]"
    },
    {
      "title": "Cascade algorithm",
      "url": "https://en.wikipedia.org/wiki/Cascade_algorithm",
      "text": "In the [[mathematics|mathematical]] topic of [[wavelet]] theory, the '''cascade algorithm''' is a [[numerical method]] for calculating function values of the basic [[Wavelet#Scaling function|scaling]] and [[wavelet]] functions of a [[discrete wavelet transform]] using an iterative algorithm. It starts from values on a coarse sequence of sampling points and produces values for successively more densely spaced sequences of sampling points. Because it applies the same operation over and over to the output of the previous application, it is known as the ''cascade algorithm''.\n\n== Successive approximation ==\n\nThe iterative algorithm generates successive approximations to ψ(''t'') or φ(''t'') from {''h''} and {''g''} filter coefficients. If the algorithm converges to a fixed point, then that fixed point is the basic scaling function or wavelet.\n\nThe iterations are defined by\n\n: <math>\\varphi^{(k+1)}(t)=\\sum_{n=0}^{N-1} h[n] \\sqrt 2 \\varphi^{(k)} (2t-n)</math>\n\nFor the ''k''th iteration, where an initial φ<sup>(0)</sup>(''t'') must be given.\n\nThe frequency domain estimates of the basic scaling function is given by\n\n: <math>\\Phi^{(k+1)}(\\omega)= \\frac {1} {\\sqrt 2} H\\left( \\frac {\\omega} {2}\\right) \\Phi^{(k)}\\left(\\frac {\\omega} {2}\\right)</math>\n\nand the limit can be viewed as an infinite product in the form\n\n: <math>\\Phi^{(\\infty)}(\\omega)= \\prod_{k=1}^{\\infty} \\frac {1} {\\sqrt 2} H\\left( \\frac {\\omega} {2^k}\\right) \\Phi^{(\\infty)}(0).</math>\n\nIf such a limit exists, the spectrum of the scaling function is\n\n: <math>\\Phi(\\omega)= \\prod_{k=1}^\\infty \\frac {1} {\\sqrt 2} H\\left( \\frac {\\omega} {2^k}\\right) \\Phi^{(\\infty)}(0)</math>\n\nThe limit does not depends on the initial shape assume for φ<sup>(0)</sup>(''t'').  This algorithm converges reliably to φ(''t''), even if it is discontinuous.\n\nFrom this scaling function, the wavelet can be generated from\n\n: <math>\\psi(t)= \\sum_{n=- \\infty}^{\\infty} g[n]{\\sqrt 2} \\varphi^{(k)} (2t-n).</math>\n\nSuccessive approximation can also be derived in the frequency domain.\n== References ==\n* [[C. Sidney Burrus|C.S. Burrus]], R.A. Gopinath, H. Guo, ''Introduction to Wavelets and Wavelet Transforms: A Primer'', Prentice-Hall, 1988,  {{ISBN|0-13-489600-9}}.\n* http://cnx.org/content/m10486/latest/\n* https://web.archive.org/web/20070615055323/http://cm.bell-labs.com/cm/ms/who/wim/cascade/index.html\n\n[[Category:Wavelets]]"
    },
    {
      "title": "Coiflet",
      "url": "https://en.wikipedia.org/wiki/Coiflet",
      "text": "[[File:Wavelet Coif1.svg|thumb|right|Coiflet with two vanishing moments]]\n'''Coiflets''' are discrete [[wavelet]]s designed by [[Ingrid Daubechies]], at the request of [[Ronald Coifman]], to have scaling functions with vanishing moments.  The wavelet is near symmetric, their wavelet functions have <math>N/3</math> vanishing moments and scaling functions <math>N/3-1</math>, and has been used in many applications using [[Singular integral#Calder.C3.B3n-Zygmund kernels|Calderón-Zygmund Operators]].<ref name=bcr>G. Beylkin, R. Coifman, and V. Rokhlin (1991),''Fast wavelet transforms and numerical algorithms'', Comm. Pure Appl. Math., 44, pp. 141-183</ref><ref name=deb>Ingrid Daubechies, ''Ten Lectures on Wavelets'', Society for Industrial and Applied Mathematics, 1992, {{ISBN|0-89871-274-2}}</ref>\n\n==Theory==\nSome theories about Coiflet :<ref>{{cite web|title=COIFLET-TYPE WAVELETS: THEORY, DESIGN, AND APPLICATIONS|url=https://seagrant.mit.edu/ESRDC_library/Wei_Dong_PhD_Thesis.pdf}}</ref>\n\n===Theorem 1===\nFor a wavelet system {<math>\\phi,\\tilde{\\phi},\\psi,\\tilde{\\psi},h,\\tilde{h},g,\\tilde{g}</math>}, the following three\nequations are equivalent:\n\n<math>\n\\begin{array}{lcl}\n\\\\\n\\mathcal{M_\\tilde{\\psi}}(0,l] = 0   & \\mbox{for }l\\mbox{ =0,1,...,L-1} \\\\\n\\sum_{n} (-1)^n n^l h[n]=0    & \\mbox{for }l\\mbox{ =0,1,...,L-1} \\\\\nH^{(l)}(\\pi)=0 & \\mbox{for }l\\mbox{ =0,1,...,L-1} \\\\\n\\end{array}</math>\n<br />\nand similar equivalence holds between <math>\\psi </math> and <math> \\tilde{h}\n</math>\n\n===Theorem 2===\nFor a wavelet system  {<math>\\phi,\\tilde{\\phi},\\psi,\\tilde{\\psi},h,\\tilde{h},g,\\tilde{g}</math>}, the following six equations\nare equivalent:\n\n<math>\n\\begin{array}{lcl}\n\\\\\n\\mathcal{M_\\tilde{\\phi}}(t_0,l] = \\delta[l]   & \\mbox{for }l\\mbox{ =0,1,...,L-1} \\\\\n\\mathcal{M_\\tilde{\\phi}}(0,l] = t_0^l   & \\mbox{for }l\\mbox{ =0,1,...,L-1} \\\\\n\\hat{\\phi}^(l)(0)=(-jt_0)^t& \\mbox{for }l\\mbox{ =0,1,...,L-1} \\\\\n\\sum_{n} (n-t_0)^l h[n]= \\delta[l]    & \\mbox{for }l\\mbox{ =0,1,...,L-1} \\\\\n\\sum_{n} n^l h[n]=t_0^l    & \\mbox{for }l\\mbox{ =0,1,...,L-1} \\\\\nH^{(l)}(0)=(-jt_0)^t & \\mbox{for }l\\mbox{ =0,1,...,L-1} \\\\\n\\end{array}</math>\n<br />\nand similar equivalence holds between <math>\\tilde{\\psi} </math> and <math> \\tilde{h}\n</math>\n\n===Theorem 3===\nFor a biorthogonal wavelet system  {<math>\\phi,\\psi,\\tilde{\\phi},\\tilde{\\psi}</math>}, if either <math>\\tilde{\\psi}</math> or <math>\\psi</math>\npossesses a degree L of vanishing moments, then the following two equations\nare equivalent:\n<br />\n<math>\n\\begin{array}{lcl}\n\\\\\n\\mathcal{M_\\tilde{\\psi}}(t_0,l] = \\delta[l]   & \\mbox{for }l\\mbox{ =0,1,...,} \\bar{L}-1 \\\\\n\\mathcal{M_\\psi}(t_0,l] = \\delta[l]   & \\mbox{for }l\\mbox{ =0,1,..., }\\bar{L}-1 \\\\\n\\end{array}</math>\n<br />\nfor any <math>\\bar{L}</math> such that <math>\\bar{L}  \\ll  L</math>\n\n==Coiflet coefficients==\nBoth the scaling function (low-pass filter) and the wavelet function (High-Pass Filter) must be normalised by a factor <math>1/\\sqrt{2} </math>. Below are the coefficients for the [[Wavelet#Scaling function|scaling functions]] for C6-30. The wavelet coefficients are derived by reversing the order of the scaling function coefficients and then reversing the sign of every second one (i.e. C6 wavelet = {&minus;0.022140543057, 0.102859456942, 0.544281086116, &minus;1.205718913884, 0.477859456942, 0.102859456942}).\n\n[[mathematics|Mathematically]], this looks like \n<math> B_k = (-1)^{k} C_{N - 1 - k} </math> where ''k'' is the coefficient index, ''B'' is a wavelet coefficient and ''C'' a scaling function coefficient. ''N'' is the wavelet index, i.e. 6 for C6.\n\n{| class=\"wikitable\" border=\"1\" style=\"font-size: 70%; text-align: right;\"\n|+'''Coiflets coefficients  (normalized to have sum 2)'''\n!k\n!C6\n!C12\n!C18\n!C24\n!C30\n|----\n| -10\n| \n| \n| \n| \n| -0.0002999290456692\n|----\n| -9\n| \n| \n| \n| \n| 0.0005071055047161\n|----\n| -8\n| \n| \n| \n| 0.0012619224228619\n| 0.0030805734519904\n|----\n| -7\n| \n| \n| \n| -0.0023044502875399\n| -0.0058821563280714\n|----\n| -6\n| \n| \n| -0.0053648373418441\n| -0.0103890503269406\n| -0.0143282246988201\n|----\n| -5\n| \n| \n| 0.0110062534156628\n| 0.0227249229665297\n| 0.0331043666129858\n|----\n| -4\n| \n| 0.0231751934774337\n| 0.0331671209583407\n| 0.0377344771391261\n| 0.0398380343959686\n|----\n| -3\n| \n| -0.0586402759669371\n| -0.0930155289574539\n| -0.1149284838038540\n| -0.1299967565094460\n|----\n| -2\n| -0.1028594569415370\n| -0.0952791806220162\n| -0.0864415271204239\n| -0.0793053059248983\n| -0.0736051069489375\n|----\n| -1\n| 0.4778594569415370\n| 0.5460420930695330\n| 0.5730066705472950\n| 0.5873348100322010\n| 0.5961918029174380\n|----\n| 0\n| 1.2057189138830700\n| 1.1493647877137300\n| 1.1225705137406600\n| 1.1062529100791000\n| 1.0950165427080700\n|----\n| 1\n| 0.5442810861169260\n| 0.5897343873912380\n| 0.6059671435456480\n| 0.6143146193357710\n| 0.6194005181568410\n|----\n| 2\n| -0.1028594569415370\n| -0.1081712141834230\n| -0.1015402815097780\n| -0.0942254750477914\n| -0.0877346296564723\n|----\n| 3\n| -0.0221405430584631\n| -0.0840529609215432\n| -0.1163925015231710\n| -0.1360762293560410\n| -0.1492888402656790\n|----\n| 4\n| \n| 0.0334888203265590\n| 0.0488681886423339\n| 0.0556272739169390\n| 0.0583893855505615\n|----\n| 5\n| \n| 0.0079357672259240\n| 0.0224584819240757\n| 0.0354716628454062\n| 0.0462091445541337\n|----\n| 6\n| \n| -0.0025784067122813\n| -0.0127392020220977\n| -0.0215126323101745\n| -0.0279425853727641\n|----\n| 7\n| \n| -0.0010190107982153\n| -0.0036409178311325\n| -0.0080020216899011\n| -0.0129534995030117\n|----\n| 8\n| \n| \n| 0.0015804102019152\n| 0.0053053298270610\n| 0.0095622335982613\n|----\n| 9\n| \n| \n| 0.0006593303475864\n| 0.0017911878553906\n| 0.0034387669687710\n|----\n| 10\n| \n| \n| -0.0001003855491065\n| -0.0008330003901883\n| -0.0023498958688271\n|----\n| 11\n| \n| \n| -0.0000489314685106\n| -0.0003676592334273\n| -0.0009016444801393\n|----\n| 12\n| \n| \n| \n| 0.0000881604532320\n| 0.0004268915950172\n|----\n| 13\n| \n| \n| \n| 0.0000441656938246\n| 0.0001984938227975\n|----\n| 14\n| \n| \n| \n| -0.0000046098383254\n| -0.0000582936877724\n|----\n| 15\n| \n| \n| \n| -0.0000025243583600\n| -0.0000300806359640\n|----\n| 16\n| \n| \n| \n| \n| 0.0000052336193200\n|----\n| 17\n| \n| \n| \n| \n| 0.0000029150058427\n|----\n| 18\n| \n| \n| \n| \n| -0.0000002296399300\n|----\n| 19\n| \n| \n| \n| \n| -0.0000001358212135\n|----\n|}\n\n==Matlab function==\nF = coifwavf(W) returns the scaling filter associated with the Coiflet wavelet specified by the string W where W = 'coifN'. Possible values for N are 1, 2, 3, 4, or 5.\n<ref>{{cite web|title=coifwavf|url=http://www.mathworks.com/help/wavelet/ref/coifwavf.html|website=www.mathworks.com/|accessdate=22 January 2015}}</ref>\n\n== References ==\n\n<references/>\n\n[[Category:Orthogonal wavelets]]\n[[Category:Wavelets]]"
    },
    {
      "title": "Complex wavelet transform",
      "url": "https://en.wikipedia.org/wiki/Complex_wavelet_transform",
      "text": "The '''complex wavelet transform (CWT)''' is a [[Complex number|complex-valued]] extension to the standard [[discrete wavelet transform]] (DWT). It is a two-dimensional [[wavelet]] transform which provides [[multiresolution analysis|multiresolution]], sparse representation, and useful characterization of the structure of an image. Further, it purveys a high degree of shift-invariance in its magnitude, which was investigated in.<ref>{{cite journal | last1 = Barri | first1 = Adriaan | last2 = Dooms | first2 = Ann | last3 = Schelkens | first3 = Peter | year = 2012 | title = The near shift-invariance of the dual-tree complex wavelet transform revisited | url = http://www.sciencedirect.com/science/article/pii/S0022247X12000261 | journal = Journal of Mathematical Analysis and Applications | volume = 389 | issue = 2| pages = 1303–1314 | doi=10.1016/j.jmaa.2012.01.010| arxiv = 1304.7932 }}</ref> However, a drawback to this transform is that it exhibits <math>2^{d}</math> (where <math>d</math> is the dimension of the signal being transformed) redundancy compared to a separable (DWT).\n\nThe use of complex wavelets in image processing was originally set up in 1995 by J.M. Lina and L. Gagnon [http://www.crim.ca/perso/langis.gagnon/articles/spie95.pdf] in the framework of the Daubechies orthogonal filters banks [http://portal.acm.org/citation.cfm?id=258030&dl=GUIDE&coll=GUIDE&CFID=10476702&CFTOKEN=44762573]. It was then generalized in 1997 by [[Nick Kingsbury|Prof. Nick Kingsbury]] <ref>{{cite conference\n|author = N. G. Kingsbury\n|title = Image processing with complex wavelets\n|booktitle = Phil. Trans. Royal Society London\n|location = London\n|url = http://citeseer.ist.psu.edu/kingsbury97image.html\n|date = September 1999}}\n</ref><ref>{{cite journal\n|first = N G|last = Kingsbury\n|date=May 2001\n|volume = 10\n|issue = 3\n|pages = 234–253\n|journal = Journal of Applied and Computational Harmonic Analysis\n|title = Complex wavelets for shift invariant analysis and filtering of signals\n|url = http://www-sigproc.eng.cam.ac.uk/%7Engk/publications/ngk_ACHApap.pdf\n|doi = 10.1006/acha.2000.0343}}\n</ref><ref>{{cite journal\n|first = Ivan W.|last = Selesnick |author2=Baraniuk, Richard G. |author3=Kingsbury, Nick G.\n|title = The Dual-Tree Complex Wavelet Transform\n|date=November 2005\n|volume = 22\n|issue = 6\n|pages = 123–151\n|journal = IEEE Signal Processing Magazine\n|url = http://www-sigproc.eng.cam.ac.uk/%7Engk/publications/ngk_SPmag_nov05.pdf\n|doi = 10.1109/MSP.2005.1550194|bibcode=2005ISPM...22..123S}}\n</ref>\nof [[University of Cambridge|Cambridge University]].\n\nIn the area of computer vision, by exploiting the concept of visual contexts, one can quickly focus on candidate regions, where objects of interest may be found, and then compute additional features through the CWT for those regions only. These additional features, while not necessary for global regions, are useful in accurate detection and recognition of smaller objects. Similarly, the CWT may be applied to detect the activated voxels of cortex and additionally the [[temporal independent component analysis]] (tICA) may be utilized to extract the underlying independent sources whose number is determined by Bayesian information criterion [http://www.springerlink.com/(t0ojvoayxrkdyk55vru2g245)/app/home/contribution.asp?referrer=parent&backto=issue,51,56;journal,180,3824;linkingpublicationresults,1:105633,1].\n\n== Dual-tree complex wavelet transform ==\n\nThe '''Dual-tree complex wavelet transform''' (DTCWT) calculates the complex transform of a signal using two separate DWT decompositions (tree ''a'' and tree ''b'').  If the filters used in one are specifically designed different from those in the other it is possible for one DWT to produce the real coefficients and the other the imaginary.\n\n[[Image:Wavelets - DTCWT.png|frame|none|Block diagram for a 3-level DTCWT]]\n\nThis redundancy of two provides extra information for analysis but at the expense of extra computational power.  It also provides approximate [[Shift-invariant system|shift-invariance]] (unlike the DWT) yet still allows perfect reconstruction of the signal. \n \nThe design of the filters is particularly important for the transform to occur correctly and the necessary characteristics are:\n\n* The [[low-pass filter]]s in the two trees must differ by half a sample period\n* Reconstruction filters are the reverse of analysis\n* All filters from the same orthonormal set\n* Tree ''a'' filters are the reverse of tree ''b'' filters\n* Both trees have the same frequency response\n\n==See also==\n* [[Wavelet series]]\n* [[Continuous wavelet transform]]\n\n== References ==\n\n{{reflist}}\n\n==External links==\n* [http://www.wavelet.org/phpBB2/viewtopic.php?t=7584 An MPhil thesis: Complex wavelet transforms and their applications]\n* [http://eprints.soton.ac.uk/11007/ CWT for EMG analysis]\n* [https://web.archive.org/web/20041111015830/http://www.cvgpr.uni-mannheim.de/Publications/TR_13_03.pdf A paper on DTCWT]\n* [https://web.archive.org/web/20050816124215/http://www-iplab.ece.ucsb.edu/publications/99IPTexture.pdf Another full paper]\n* [http://www-sigproc.eng.cam.ac.uk/~ngk/publications/ngk_biosig04.pdf 3-D DT MRI data visualization]\n* [http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=1369333 Multidimensional, mapping-based complex wavelet transforms ]\n* [http://www-syscom.univ-mlv.fr/~chaux/articles/ieeeIPdouble.pdf  Image Analysis Using a Dual-Tree <math>M</math>-band Wavelet Transform (2006), preprint, Caroline Chaux, Laurent Duval, Jean-Christophe Pesquet]\n* [http://www-syscom.univ-mlv.fr/~chaux/articles/chauxpesquetduvalIT.pdf  Noise covariance properties in dual-tree wavelet decompositions (2007), preprint, Caroline Chaux, Laurent Duval, Jean-Christophe Pesquet]\n* [https://arxiv.org/pdf/0712.2317  A nonlinear Stein based estimator for multichannel image denoising (2007), preprint, Caroline Chaux, Laurent Duval, Amel Benazza-Benyahia, Jean-Christophe Pesquet]\n* [http://www-syscom.univ-mlv.fr/~chaux/ Caroline Chaux website (<math>M</math>-band dual-tree wavelets)]\n* [http://www.laurent-duval.eu/siva-signal-image-links.html#dual-tree-complex-wavelet Laurent Duval website (<math>M</math>-band dual-tree wavelets)]\n* [http://www.ece.msstate.edu/~fowler/ James E. Fowler (dual-tree wavelets for video and hyperspectral image compression)]\n* [http://www-sigproc.eng.cam.ac.uk/Main/NGK Nick Kingsbury website (dual-tree wavelets)]\n* [http://www-syscom.univ-mlv.fr/~pesquet/ Jean-Christophe Pesquet website (<math>M</math>-band dual-tree wavelets)]\n* [http://taco.poly.edu/selesi/ Ivan Selesnick (dual-tree wavelets)]\n\n[[Category:Wavelets]]"
    },
    {
      "title": "Contourlet",
      "url": "https://en.wikipedia.org/wiki/Contourlet",
      "text": "{{context|date=November 2014}}\n'''Contourlets''' form a multiresolution directional tight [[Frame of a vector space|frame]] designed to efficiently approximate images made of smooth regions separated by smooth boundaries. The contourlet transform has a fast implementation based on a [[Laplacian pyramid]] decomposition followed by directional [[filterbank]]s applied on each bandpass subband.\n\n== Contourlet transform ==\n\n===Introduction and motivation===\nIn the field of geometrical image transforms, there are many 1-D transforms designed for detecting or capturing the geometry of image information, such as the [[Fourier Transform|Fourier]] and [[wavelet transform]]. However, the ability of 1-D transform processing of the intrinsic geometrical structures, such as smoothness of curves, is limited in one direction, then more powerful representations are required in higher dimensions.\nThe contourlet transform which was proposed by Do and Vetterli in 2002, is a new two-dimensional transform method for image representations.The contourlet transform has properties of multiresolution, localization, directionality, critical sampling and anisotropy. Its basic functions are multiscale and multidimensional. The contours of original images, which are the dominant features in natural images, can be captured effectively with a few coefficients by using contourlet transform.\n\nThe contourlet transform is inspired by the human visual system and [[Curvelet]] transform which can capture the smoothness of the contour of images with different elongated shapes and in variety of directions.<ref name=\"Curv1\"/> However, it is difficult to sampling on a rectangular grid for Curvelet transform since Curvelet transform was developed in continuous domain and directions other than horizontal and vertical are very different on rectangular grid. Therefore, the contourlet transform was proposed initially as a directional multiresolution transform in the discrete domain.\n\n===Definition===\n[[File:Contourlet Transform Double Filter Bank.jpg|thumb|Contourlet transform double filter bank]]\nThe contourlet transform uses a double filter bank structure to get the smooth contours of images. In this double filter bank, the [[Laplacian pyramid]] (LP) is first used to capture the point discontinuities, and then a [[filter bank|directional filter bank]] (DFB) is used to form those point discontinuities into linear structures.<ref name=\"Thesis\"/>\n\nThe Laplacian pyramid (LP) decomposition only produce one bandpass image in a [[multidimensional signal processing]], that can avoid frequency scrambling. And directional filter bank (DFB) is only fit for high frequency since it will leak the low frequency of signals in its directional subbands. This is the reason to combine DFB with LP, which is multiscale decomposition and remove the low frequency. Therefore, image signals pass through LP subbands to get bandpass signals and pass those signals through DFB to capture the directional information of image. This double filter bank structure of combination of LP and DFB is also called as pyramid directional filter bank (PDFB), and this transform is approximate the original image by using basic contour, so it is also called discrete contourlet transform.<ref name=\"DoVet1\"/>\n\n===The properties of discrete contourlet transform <ref name=\"DoVet1\"/>===\n1). If perfect-reconstruction filters are used for both the LP decomposition and DFB, then the discrete contourlet transform can reconstruct the original image perfectly, which means it provides a frame operator.<br>\n2). If orthogonal filters are used for both the LP decomposition and DFB, then the discrete contourlet transform provides a tight frame which bounds equal to 1.<br>\n3). The upper bound for the redundancy ratio of the discrete contourlet transform is <math>4/3</math>.<br>\n4). If the <math>j</math> pyramidal level of LP applies to <math>l_j</math> level DFB, the basis images of the contourlet transform have the size of <math>width</math> ≈ <math>2^j</math> and <math>length</math> ≈ <math>2^{j+l_j-2}</math>. <br>\n5). When FIR is used, the computational complexity of the discrete contourlet transform is <math>O(N)</math> for ''N''-pixel images.\n\n== Nonsubsampled contourlet transform ==\n\n===Motivation and applications===\nThe contourlet transform has a number of useful features and qualities, but it also has its flaws. One of the more notable variations of the contourlet transform was developed and proposed by da Cunha, Zhou and Do in 2006. The nonsubsampled contourlet transform (NSCT) was developed mainly because the contourlet transform is not shift invariant.<ref name=\"NSCT\"/>  The reason for this lies in the up-sampling and down-sampling present in both the Laplacian Pyramid and the directional filter banks. The method used in this variation was inspired by the nonsubsampled wavelet transform or the stationary wavelet transform which were computed with the à trous algorithm.<ref name=\"NSCT\"/>\n\nThough the contourlet and this variant are relatively new, they have been used in many different applications including synthetic aperture radar despeckling,<ref name=\"Despec\"/> image enhancement<ref name=\"ImgEnhNSCT\"/> and texture classification.<ref name=\"TxtrCl\"/>\n\n===Basic concept===\n[[File:Nonsubsampled Contourlet Transform.jpg|thumb|Nonsubsampled contourlet transform]]\nTo retain the directional and multiscale properties of the transform, the Laplacian Pyramid was replaced with a nonsubsampled pyramid structure to retain the multiscale property, and a nonsubsampled directional filter bank for directionality. The first major notable difference is that upsampling and downsampling are removed from both processes. Instead the filters in both the Laplacian Pyramid and the directional filter banks are upsampled. Though this mitigates the shift invariance issue a new issue is now present with aliasing and the directional filter bank. When processing the coarser levels of the pyramid there is potential for aliasing and loss in resolution. This issue is avoided though by upsampling the directional filter bank filters as was done with the filters from the pyramidal filter bank.<ref name=\"NSCT\"/>\n\nThe next issue that lies with this transform is the design of the filters for both filter banks. According to the authors there were some properties that they desired with this transform such as: perfect reconstruction, a sharp frequency response, easy implementation and linear-phase filters.<ref name=\"NSCT\"/> These features were implemented by first removing the tight frame requirement and then using a mapping to design the filters and then  implementing a ladder type structure. These changes lead to a transform that is not only efficient but performs well in comparison to other similar and in some cases more advanced transforms when denoising and enhancing images.\n\n==Variations of the contourlet transform==\n\n===Wavelet-based contourlet transform===\n[[File:Wavelet-based contourlet packet.jpg|thumb|Wavelet-based contourlet packet using 3 dyadic wavelet levels and 8 directions at the finest level.]]\nAlthough the wavelet transform is not optimal in capturing the 2-D singularities of images, it can take the place of LP decomposition in the double filter bank structure to make the contourlet transform a non-redundant image transform.<ref name=\"WBCT\"/> The wavelet-based contourlet transform is similar to the original contourlet transform, and it also consists of two filter bank stages. In the first stage, the wavelet transform is used to do the sub-band decomposition instead of the Laplacian pyramid (LP) in the contourlet transform. And the second stage of the wavelet-based contourlet transform is still a directional filter bank (DFB) to provide the link of singular points. \nOne of the advantages to the wavelet-based contourlet transform is that the wavelet-based contourlet packets are similar to the wavelet packets which allows quad-tree decomposition of both low-pass and high-pass channels and then apply the DFB on each sub-band.\n\n===The hidden Markov tree (HMT) model for the contourlet transform===\nBased on the study of statistics of contourlet coefficients of natural images, the HMT model for the contourlet transform is proposed. The statistics show that the contourlet coefficients are highly non-Gaussian, high interaction dependent on all their eight neighbors and high inter-direction dependent on their cousins. Therefore, the HMT model, that captures the highly non-Gaussian property, is used to get the dependence on neighborhood through the links between the hidden states of the coefficients.<ref name=\"DirMult\"/> This HMT model of contourlet transform coefficients has better results than original contourlet transform and other HMT modeled transforms in denoising and texture retrieval, since it restores edges better visually.\n\n===Contourlet transform with sharp frequency localization===\nAn alternative or variation of the contourlet transform was proposed by Lu and Do in 2006. This new proposed method was intended as a remedy to fix non-localized basis images in frequency.<ref name=\"Sharp_Freq\"/> The issue with the original contourlet transform was that when the contourlet transform was used with imperfect filter bank filters aliasing occurs and the frequency domain resolution is affected. There are two contributing factors to the aliasing, the first is the periodicity of 2-D frequency spectra and the second is an inherent flaw in the critical sampling of the directional filter banks.<ref name=\"Sharp_Freq\"/> This new method mitigates these issues by changing the method of multiscale decomposition. As mentioned before, the original contourlet used the Laplacian Pyramid for multiscale decomposition. This new method as proposed by Lu and Do uses a multiscale pyramid that can be adjusted by applying low pass or high pass filters for the different levels.<ref name=\"Sharp_Freq\"/> This method fixes multiple issues, it reduces the amount of cross terms and localizes the basis images in frequency,  removes aliasing and has proven in some instances more effective in denoising images. Though it fixes all of those issues, this method requires more filters than the original contourlet transform and still has both the up-sampling and down-sampling operations meaning it is not shift-invariant.\n\n===Image enhancement based on nonsubsampled contourlet transform===\nIn prior studies the contourlet transform has proven effective in the denoising of images but in this method the researchers developed a method of image enhancement. When enhancing images preservation and the enhancement of important data is of paramount importance. The contourlet transform meets this criterion to some extent with its ability to denoise and detect edges.<ref name=\"DoVet1\"/> This transform first passes the image through the multiscale decomposition by way of the nonsubsampled laplacian pyramid. After that, the noise variance for each sub-band is calculated and relative to local statistics of the image it is classified as either noise, a weak edge or strong edge. The strong edges are retained, the weak edges are enhanced and the noise is discarded. This method of image enhancement significantly outperformed the nonsubsampled wavelet transform (NSWT) both qualitatively and quantitatively.<ref name=\"ImgEnhNSCT\"/> Though this method outperformed the NSWT there still lies the issue of the complexity of designing adequate filter banks and fine tuning the filters for specific applications of which further study will be required.<ref name=\"ImgEnhNSCT\"/>\n\n==Applications==\n[[Noise reduction#In images|Image Denoising]] <br> \n[[Image editing#Enhancing images|Image Enhancement]]<br> \n[[Image restoration|Image Restoration]] <br> \n[[Speckle noise#Speckle Noise Reduction|Image Despeckling]]\n\n== See also ==\n*[[Wavelet]]\n*[[Multiresolution analysis]]\n*[[Scale space]]\n*[[Bandelet (computer science)|Bandelets]]\n*[[Curvelet]]s\n*[[Multiscale decomposition]]\n*[[Directional decomposition]]\n*[[Pyramid Directional Filter Banks]]\n*[[Basis Functions]]\n\n== References ==\n{{reflist| refs=\n<ref name=\"DoVet1\">M. N. Do and M. Vetterli, \"The contourlet transform: an efficient directional multiresolution image representation\", IEEE Transactions on Image Processing, vol. 14, no. 12, pp.&nbsp;2091–2106, Dec. 2005.--> [http://www.ifp.uiuc.edu/~minhdo/publications/contourlet_txform.pdf]</ref>\n<ref name=\"Curv1\">E. J. Candès and D. L. Donoho, “Curvelets – a surprisingly effective nonadaptive representation for objects with edges,” in Curve and Surface Fitting, A. Cohen, C. Rabut, and L. L. Schumaker, Eds. Saint-Malo: Vanderbilt University Press, 1999. [http://statweb.stanford.edu/~donoho/Reports/1999/curveletsurprise.pdf]</ref>\n<ref name=\"NSCT\">L.da Cunha, Jianping Zhou, and Minh N. Do, “The nonsubsampled contourlet transform: theory, design, and applications,” IEEE Transactions on Image Processing, Vol. 15, No. 10, pp. 3089–3101, 2006. [http://www.ifp.illinois.edu/~minhdo/publications/nsct.pdf]</ref>\n<ref name=\"WBCT\">Ramin Eslami and Hayder Radha,\"wavelet-based contourlet transform and its application to image coding,” in Proceedings of the IEEE International Conference on Image Processing (ICIP’04), IEEE Signal Processing Society, Vol. 5,pp. 3189 - 3192</ref>\n<ref name=\"DirMult\">D. D.-Y. Po and M. N. Do, “Directional multiscale modeling of images using the contourlet transform,” IEEE Trans. Image Process., vol. 15, no. 6, pp. 1610–1620, Jun. 2006. [http://minhdo.ece.illinois.edu/publications/contourlet_modeling.pdf]</ref>\n<ref name=\"Sharp_Freq\">Y. Lu and M. N. Do, “A new contourlet transform with sharp frequency localization,” IEEE Int. Conf. Image Processing, Atlanta, GA, Oct. 2006., pp. 1–4 [http://minhdo.ece.illinois.edu/publications/ContourletSD.pdf]</ref>\n<ref name=\"ImgEnhNSCT\">Ma Y., Xie J., Luo J., \"Image Enhancement Based on Nonsubsampled Contourlet Transform\", International Conference on Information Assurance and Security, 2009., pp. 1–4</ref>\n<ref name=\"Thesis\">M. N. Do, Directional multiresolution image representations. PhD thesis, EPFL, Lausanne, Switzerland, Dec. 2001.[http://www.ifp.illinois.edu/~minhdo/publications/thesis.pdf]</ref>\n<ref name=\"Despec\">W. Ni, B. Guo, Y. Yan, and L. Yang, “Speckle suppression for SAR images based on adaptive shrinkage in contourlet domain,” in Proc. 8th World Congr. Intell. Control Autom., vol. 2. 2006, pp. 10017–10021.</ref>\n<ref name=\"TxtrCl\">Li.S, Fu.X, Yang.B, “Nonsubsampled Contourlet Transform for Texture Classifications using Support Vector Machines,” IEEE ICNSC, pp. 1654–\n1657, 2008.</ref>\n\n}}\n\n== External links ==\n* The [http://www.ifp.uiuc.edu/~minhdo/software/contourlet_toolbox.zip Contourlet Toolbox] (in [[Matlab]])\n\n[[Category:Wavelets]]"
    },
    {
      "title": "Curvelet",
      "url": "https://en.wikipedia.org/wiki/Curvelet",
      "text": "{{cleanup|reason=poor math and text formatting|date=May 2013}}\n\n'''Curvelets''' are a non-[[Adaptive-additive algorithm|adaptive]] technique for multi-scale [[Object (computer science)|object]] representation. Being an extension of the [[wavelet]] concept, they are becoming popular in similar fields, namely in [[image processing]] and [[scientific computing]].\n\nWavelets generalize the [[Fourier transform]] by using a basis that represents both location and spatial frequency. For 2D or 3D signals, directional wavelet transforms go further, by using basis functions that are also localized in ''orientation''. A curvelet transform differs from other directional wavelet transforms in that the degree of localisation in orientation varies with scale. In particular, fine-scale basis functions are long ridges; the shape of the basis functions at scale ''j'' is <math>2^{-j}</math> by <math>2^{-j/2}</math> so the fine-scale bases are skinny ridges with a precisely determined orientation.\n\nCurvelets are an appropriate basis for representing images (or other functions) which are smooth apart from singularities along smooth curves, ''where the curves have bounded curvature'', i.e. where objects in the image have a minimum length scale. This property holds for cartoons, geometrical diagrams, and text. As one zooms in on such images, the edges they contain appear increasingly straight. Curvelets take advantage of this property, by defining the higher resolution curvelets to be more elongated than the lower resolution curvelets. However, natural images (photographs) do not have this property; they have detail at every scale. Therefore, for natural images, it is preferable to use some sort of directional wavelet transform whose wavelets have the same aspect ratio at every scale.\n\nWhen the image is of the right type, curvelets provide a representation that is considerably sparser than other wavelet transforms. This can be quantified by considering the best approximation of a geometrical test image that can be represented using only <math>n</math> wavelets, and analysing the approximation error as a function of <math>n</math>. For a Fourier transform, the squared error decreases only as <math>O(1/\\sqrt{n})</math>. For a wide variety of wavelet transforms, including both directional and non-directional variants, the squared error decreases as <math>O(1/n)</math>. The extra assumption underlying the curvelet transform allows it to achieve <math>O({(\\log n)}^3/{n^2})</math>.\n\nEfficient numerical algorithms exist for computing the curvelet transform of discrete data. The computational cost of a curvelet transform is approximately 10–20 times that of an FFT, and has the same dependence of <math>O(n^2 \\log n)</math>  for an image of size <math>n \\times n</math>.\n\n== Curvelet construction ==\nTo construct a basic curvelet <math>\\phi</math> and provide a tiling of the 2-D frequency space, two main ideas should be followed:\n# Consider polar coordinates in frequency domain\n# Construct curvelet elements being locally supported near wedges\n\nThe number of wedges is <math> N_j = 4 \\cdot 2^{\\left \\lceil \\frac{j}{2} \\right \\rceil}</math> \nat the scale <math>2^{-j}</math>, i.e., it doubles in each second circular ring.<br />\n\nLet <math>\\boldsymbol{\\xi}=\\left (\\xi_1,\\xi_2 \\right )^T</math>\nbe the variable in frequency domain, and <math>r=\\sqrt{\\xi_1^2+\\xi_2^2}, \\omega=\\arctan \\frac{\\xi_1}{\\xi_2}</math> be the polar coordinates in the frequency domain.\n<br />\n\nWe use the [[ansatz]] for the ''dilated basic curvelets'' in polar coordinates:<br />\n<math>\\hat{\\phi}_{j,0,0}:=2^{\\frac{-3j}{4}}W(2^{-j}r)\\tilde{V}_{N_j}(\\omega), r\\ge 0, \\omega \\in [0,2\\pi), j \\in N_0</math><br /><br />\n\nTo construct a basic curvelet with compact support near a ″basic wedge″, the two windows <math>W</math> and <math>\\tilde{V}_{N_j}</math>need to have compact support.\nHere, we can simply take <math>W(r)</math> to cover <math>(0,\\infty)</math>with dilated curvelets and <math>\\tilde{V}_{N_j}</math> such that each circular ring is covered by the translations of <math>\\tilde{V}_{N_j}</math> .<br />\n\nThen the admissibility yields<br />\n<math>\\sum_{j=-\\infty}^{\\infty}\\left| W(2^{-j}r) \\right|^2=1, r \\in (0,\\infty).</math><sub>see ''[[Window function|Window Functions]]'' for more information</sub><br />\n<br />\nFor tiling a circular ring into <math>N</math> wedges, where <math>N</math> is an arbitrary positive integer, we need a <math>2\\pi</math>-periodic nonnegative window <math>\\tilde{V}_{N}</math> with support inside <math>\\left[ \\frac{-2\\pi}{N}, \\frac{2\\pi}{N} \\right]</math> such that<br />\n<math>\\sum_{l=0}^{N-1}\\tilde{V}^2_N(\\omega-\\frac{2\\pi l}{N})=1</math>, for all <math>\\omega \\in \\left[ 0, 2\\pi \\right)</math><br />\n<math>\\tilde{V}_N</math> can be simply constructed as <math>2\\pi</math>-periodizations of a scaled window <math>V(\\frac{N\\omega}{2\\pi})</math>.<br />\n<br />\nThen, it follows that<br />\n<math>\\sum_{l=0}^{N_j-1}\\left| 2^{\\frac{3j}{4}}\\hat{\\phi}_{j,0,0}(r, \\omega-\\frac{2\\pi l}{N_j}) \\right| ^2=\\left|W(2^{-j}r) \\right|^2\\sum_{l=0}^{N_j-1}\\tilde{V}^2_{N_j}(\\omega-\\frac{2\\pi l}{N})=\\left|W(2^{-j}r) \\right|^2</math>\n\nFor a complete covering of the frequency plane including the region around zero, we need to define a low pass element<br />\n<math>\\hat{\\phi}_{-1}:=W_0(\\left| \\xi \\right|) </math>  with <br /> \n<math>W_0^2(r)^2:=1-\\sum_{j=0}^{\\infty}W(2^{-j}r)^2</math><br />\nthat is supported on the unit circle, and where we do not consider any rotation.\n\n== Applications ==\n* [[Image Processing]]\n* Seismic Exploration\n* [[Fluid Mechanics]]\n* [[Partial differential equation|PDEs]] Solving\n* [[Compressed Sensing]]\n\n== See also ==\n* [[Shearlet|Shearlet transform]]\n* [[Bandelet|Bandelet transform]]\n* [[Contourlet|Contourlet transform]] \n* [[Fresnel transform|Fresnelet transform]]\n* [[Chirplet transform]]\n* [[Noiselet |Noiselet transform]]\n* [[Scale space]]\n\n== References ==\n*[[Emmanuel Candès|E. Candès]] and D. Donoho, \"Curvelets – a surprisingly effective nonadaptive representation for objects with edges.\" In: A. Cohen, C. Rabut and L. Schumaker, Editors, ''Curves and Surface Fitting'': Saint-Malo 1999, Vanderbilt University Press, Nashville (2000), pp. 105&ndash;120.\n* Majumdar Angshul [http://jprr.org/index.php/jprr/article/view/27 Bangla Basic Character Recognition using Digital Curvelet Transform] Journal of Pattern Recognition Research ([http://www.jprr.org JPRR]), Vol 2. (1) 2007 p.17-26\n* Emmanuel Candes, Laurent Demanet, David Donoho and Lexing Ying [http://www.curvelet.org/papers/FDCT.pdf Fast Discrete Curvelet Transforms]\n* Jianwei Ma, [[Gerlind Plonka]], ''The Curvelet Transform'': IEEE Signal Processing Magazine, 2010, 27 (2), 118-133. \n* Jean-Luc Starck, Emmanuel J. Candès, and David L. Donoho, ''The Curvelet Transform for Image Denoising,'': IEEE Transactions on Image Processing, Vol. 11, No. 6, June 2002\n\n== External links ==\n*[http://curvelet.org/ Curvelet.org homepage]\n[[Category:Image processing]]\n[[Category:Time–frequency analysis]]\n[[Category:Wavelets]]"
    },
    {
      "title": "Diffusion wavelets",
      "url": "https://en.wikipedia.org/wiki/Diffusion_wavelets",
      "text": "'''Diffusion wavelets''' are a fast multiscale framework for the analysis of functions on discrete (or discretized continuous) structures like graphs, manifolds, and point clouds in Euclidean space.  Diffusion wavelets are an extension of classical [[Wavelet|wavelet theory]] from [[harmonic analysis]].  Unlike classical wavelets whose basis functions are predetermined, diffusion wavelets are adapted to the geometry of a given diffusion operator <math>T</math> (e.g., a [[heat kernel]] or a [[random walk]]).  Moreover, the diffusion wavelet basis functions are constructed by dilation using the dyadic powers (powers of two) of <math>T</math>.  These dyadic powers of <math>T</math> diffusion over the space and propagate local relationships in the function throughout the space until they become global.  And if the rank of higher powers of <math>T</math> decrease (i.e., its spectrum decays), then these higher powers become compressible.  From these decaying dyadic powers of <math>T</math> comes a chain of decreasing subspaces.  These subspaces are the [[Wavelet#Scaling_function|scaling function]] approximation subspaces, and the differences in the subspace chain are the wavelet subspaces.\n\nDiffusion wavelets were first introduced in 2004 by [[Ronald Coifman]] and Mauro Maggioni at Yale University.<ref>{{cite journal|last=Coifman |first=Ronald |author2=Mauro Maggioni |title=Diffusion Wavelets |journal=Applied and Computational Harmonic Analysis |date=May 2008 |volume=24 |issue=3 |pages=329–353 |url=http://www.math.duke.edu/~mauro/Papers/DiffusionWavelets.pdf |deadurl=yes |archiveurl=https://web.archive.org/web/20120422151024/http://www.math.duke.edu/~mauro/Papers/DiffusionWavelets.pdf |archivedate=2012-04-22 |df= }}</ref>\n\n== Algorithm ==\nThis algorithm constructs the scaling basis functions and the wavelet basis functions along with the representations of the diffusion operator <math>T</math> at these scales.\n\nIn the algorithm below, the subscript notation <math>\\Phi_a</math> and <math>\\Psi_b</math> represents the scaling basis functions at scale <math>a</math> and the wavelet basis functions at scale <math>b</math> respectively.  The notation <math>[\\Phi_b]_{\\Phi_a}</math> denotes the matrix representation of the scaling basis <math>\\Phi_b</math> represented with respect to the basis <math>\\Phi_a</math>.  Lastly, the notation <math>[T]_{\\Phi_a}^{\\Phi_b}</math> denotes the matrix represents of the operator <math>T</math>, where the [[row space]] of <math>T</math> is represented with respect to the basis <math>\\Phi_a</math>, and the [[column space]] of <math>T</math> is represented with respect to the basis <math>\\Phi_b</math>.  Otherwise put, the domain of operator <math>T</math> is represented with respect to the basis <math>\\Phi_a</math> and the range is represented with respect to the basis <math>\\Phi_b</math>.  The function <math>QR</math> is a sparse [[QR decomposition]] with <math>\\epsilon</math> precision.<ref>{{cite conference|last=Maggioni|first=Mauro|author2=Mahadevan, Sridhar|title=Fast Direct Policy Evaluation using Multiscale Analysis of Markov  Diﬀusion Processes|conference=The 23rd International Conference on Machine Learning|year=2006|url=http://www.cs.umass.edu/~mahadeva/papers/icml2006.pdf}}</ref> \n\n   // Input:\n   //    <math>T</math> is the matrix representation of the diffusion operator.\n   //    <math>\\epsilon</math> is the precision of the QR decomposition, e.g., 1e-6.\n   //    <math>J</math> is the maximum number of scale levels (note: this is an <em>optional</em> upper bound, it may converge sooner.)\n   // Output:\n   //    <math>\\lbrace\\Phi_j\\rbrace</math> is the set of scaling basis functions indexed by scale <math>j</math>.\n   //    <math>\\lbrace\\Psi_j\\rbrace</math> is the set of wavelet basis functions indexed by scale <math>j</math>.\n   \n   <math>\\lbrace\\Phi_j\\rbrace, \\lbrace\\Psi_j\\rbrace \\leftarrow \\text{function DiffusionWaveletTree} ( T , \\epsilon , J ):</math>\n      <math>\\textbf{for } j\\leftarrow 0 \\text{ to } J-1</math>:\n         <math>[\\Phi_{j+1}]_{\\Phi_j}, [T^{2^j}]_{\\Phi_j}^{\\Phi_{j+1}} \\leftarrow QR\\left([T^{2^j}]_{\\Phi_j}^{\\Phi_{j}}, \\epsilon\\right)  </math>\n         <math>[T^{2^{j+1}}]_{\\Phi_{j+1}}^{\\Phi_{j+1}} \\leftarrow  \\left([T^{2^j}]_{\\Phi_j}^{\\Phi_{j+1}} [\\Phi_{j+1}]_{\\Phi_j}\\right)^2</math>\n         <math>[\\Psi_j]_{\\Phi_j} \\leftarrow QR\\left(I_{\\langle\\Phi_j\\rangle}-[\\Phi_{j+1}]_{\\Phi_j}\\left([\\Phi_{j+1}]_{\\Phi_j}\\right)^*, \\epsilon\\right)</math> \n      <math>\\textbf{end for}</math>\n\n== Applications ==\n===Mathematics===\nDiffusion wavelets are of general interest in mathematics.  Specifically, they allow for the direct calculation of the [[Green′s function]] and the inverse [[Laplacian matrix|graph Laplacian]].\n\n===Computer science===\nDiffusion wavelets have been used extensively in computer science, especially in machine learning.  They have been applied to the following fields:\n* solving [[Markov decision process]]es and [[Markov chains]] for machine learning,<ref>{{cite journal|last=Mahadevan|first=Sridhar|title=Learning Representation and Control in Markov Decision Processes|journal=Foundations and Trends in Machine Learning|year=2008|volume=1|issue=4}}</ref>\n* [[transfer learning]],<ref>{{cite journal|last=Wang|first=Chang|author2=Mahadevan, Sridhar|title=Multiscale Manifold Alignment|journal=Univ. of Massachusetts Technical Report|year=2010|issue=UM-CS-2010-049|url=http://www.cs.umass.edu/~mahadeva/papers/UM-CS-2010-049.pdf}}</ref>\n* value function approximation in [[reinforcement learning]],<ref>{{cite journal|last=Mahadevan|first=Sridhar|author2=Maggioni, Mauro|title=Value Function Approximation using Diffusion Wavelets and Laplacian Eigenfunctions|journal=Advances in Neural Information Processing Systems|year=2006|url=http://www.cs.umass.edu/~mahadeva/papers/nips-paper1-v5.pdf}}</ref>\n* [[dimensionality reduction]],<ref>{{cite journal|last=Wang|first=Chang|author2=Mahadevan, Sridhar|title=Multiscale Dimensionality Reduction with Diffusion Wavelets|journal=Univ. of Massachusetts Technical Report|year=2009|issue=UM-CS-2009-030|url=http://www.cs.umass.edu/~mahadeva/papers/TR-2009-DP.pdf}}</ref>\n* mesh compression for 3D graphics,<ref>{{cite conference|last=Mahadevan|first=Sridhar|title=Adaptive Mesh Compression in 3D Computer Graphics using Multiresolution Manifold Learning|conference=The 24th International Conference on Machine Learning|year=2007|url=http://www.cs.umass.edu/~mahadeva/papers/sridhar-icml07.pdf}}</ref>\n* [[topic model]] analysis of document corpora.<ref>{{cite conference|last=Wang |first=Chang |author2=Mahadevan, Sridhar |title=Multiscale Analysis of Document Corpora Based on Diffusion Models |conference=The 21st International Joint Conference on Artificial Intelligence |year=2009 |url=http://www.cs.umass.edu/~chwang/papers/IJCAI-2009-TD.pdf }}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>\n* relation extraction. <ref>{{cite conference|last=Wang |first=Chang |author2=James Fan |author3=Aditya A. Kalyanpur |author4=David Gondek |title=Relation Extraction with Relation Topics |conference=The 2011 Conference on Empirical Methods in Natural Language Processing |year=2011 |url=http://acl.eldoc.ub.rug.nl/mirror/D/D11/D11-1132.pdf }}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>\n\n== See also ==\n* [[Wavelets]]\n\n==References==\n<references/>\n\n==External links==\n*[https://web.archive.org/web/20120308014909/http://www.math.duke.edu/~mauro/code.html Mauro Maggioni's MATLAB code implementation]\n*[https://archive.is/20121212094349/http://www.cs.umass.edu/~chwang/dwt.html Chang Wang's diffusion wavelet page]\n\n[[Category:Wavelets]]"
    },
    {
      "title": "Dirac (video compression format)",
      "url": "https://en.wikipedia.org/wiki/Dirac_%28video_compression_format%29",
      "text": "{{short description|Video compression format}}\n{{Use dmy dates|date=July 2013}}\n{{Infobox file format\n| name                   = Dirac\n| icon                   =\n| logo                   =\n| extension              = drc\n| mime                   =\n| type code              =\n| uniform type           =\n| magic                  =\n| owner                  = [[BBC Research & Development]]\n| released               = <!-- {{Start date and age|YYYY|mm|dd|df=yes}} -->\n| latest release version = 2.2.3<ref name=\"dirac-specs\">{{cite web |url = http://diracvideo.org/specifications/ |title = Dirac Specifications |accessdate = 2011-01-04 |archiveurl              = https://web.archive.org/web/20150503015104/http://diracvideo.org/download/specification/dirac-spec-latest.pdf |archivedate = 2015-05-03 }}</ref>\n| latest release date    = {{Start date and age|2008|09|23|df=yes}}\n| genre                  = [[Video compression]] format\n| container for          =\n| contained by           = [[MPEG-TS]], [[Ogg]], [[Audio Video Interleave|AVI]], [[Matroska|MKV]], [[QuickTime File Format|MOV]], [[MPEG-4 Part 12]], etc.\n| extended from          =\n| extended to            = VC-2\n| standard               = SMPTE 2042-1-2009, SMPTE 2042-2-2009 (a sub-set of Dirac)\n| free                   =\n| url                    =\n}}\n\n'''Dirac''' is an open and [[royalty-free]] [[video compression format]],<ref>{{cite web |url = http://diracvideo.org/about-dirac/ |title = About Dirac |accessdate = 2012-07-15 }}</ref> specification and system developed by [[BBC Research & Development]].<ref>{{cite web |publisher = diracvideo.org |title = FAQ – Diracvideo |url = http://diracvideo.org/wiki/index.php/FAQ#Flavours_of_Dirac |accessdate = 2009-08-30 }}</ref><ref name=\"about\">{{cite web |publisher = diracvideo.org |title = About Dirac |url = http://diracvideo.org/wiki/index.php/FAQ#About_Dirac |accessdate = 2009-08-30 }}</ref><ref name=\"bbc-white-paper\">{{cite web |url = http://downloads.bbc.co.uk/rd/pubs/whp/whp-pdf-files/WHP159.pdf |format = PDF |title = BBC Research White Paper, WHP 159, November 2007, Open Technology Video Compression for Production and Post Production |author = Tim Borer |year = 2007 |accessdate = 2010-08-19 }}</ref><ref>{{cite web |url = http://www.bbc.co.uk/rd/projects/dirac/index.shtml |title = BBC R&D – Dirac |accessdate = 2010-08-19 }}</ref> '''Schrödinger''' and '''dirac-research''' (formerly just called \"Dirac\") are open and royalty-free software implementations ([[video codec]]s) of Dirac. Dirac format aims to provide high-quality video compression for [[Ultra High Definition Television|Ultra HDTV]] and beyond,<ref name=\"about\" /> and as such competes with existing formats such as [[H.264]] and [[VC-1]].\n\nThe specification was finalised in January 2008, and further developments are only bug fixes and constraints.<ref name=\"dirac-specs\"/> In September of that year, version 1.0.0 of an [[I-frame]] only subset known as ''Dirac Pro'' was released<ref>{{cite web |url=https://lwn.net/Articles/298755/ |title=Dirac 1.0.0 released. |last=Edge |first=Jake |date=September 17, 2008 |website=LWN.net |accessdate=July 7, 2017}}</ref> and has since been standardised by the [[Society of Motion Picture and Television Engineers|SMPTE]] as '''VC-2'''.<ref name=\"bbc-white-paper\" /><ref name=\"smpte-vc2\" >{{cite web |url = http://standards.smpte.org/content/978-1-61482-709-2/st-2042-1-2012/SEC1.abstract |title = SMPTE 2042-1:2012 |accessdate = 2015-09-09 }}</ref> Version 2.2.3 of the full Dirac specification, including [[motion compensation]] and inter-frame coding, was issued a few days later.<ref>{{cite journal |publisher = [[BBC]] |title = Dirac Specification, Version 2.2.3 |date = 23 September 2008 |url = http://diracvideo.org/download/specification/dirac-spec-latest.pdf |accessdate = 2009-07-05 |deadurl = yes |archiveurl = https://web.archive.org/web/20150503015104/http://diracvideo.org/download/specification/dirac-spec-latest.pdf |archivedate = 3 May 2015 |df = dmy-all }}</ref> Dirac Pro was used internally by the BBC to transmit HDTV pictures at the [[2008 Summer Olympics|Beijing Olympics]] in 2008.<ref>{{cite web |publisher = Broadcast Magazine ([[EMAP|East Midland Allied Press]]) |title = Dirac Pro to bolster BBC HD links |url = http://www.broadcastnow.co.uk/news/2008/07/dirac_pro_to_bolster_bbc_hd_links.html }}</ref><ref>[http://www.ibc.org/cgi-bin/ibc_dailynews_cms.cgi?story_no=25368&issue=4 BBC pushes Dirac to the forefront]</ref><ref>[http://www.videsignline.com/210601739 And now, Dirac from the Olympics, a new free codec!]</ref>\n\nThe format implementations are named in honour of the theoretical physicists [[Paul Dirac]] and [[Erwin Schrödinger]], who shared the 1933 [[Nobel Prize in physics]].\n\n==Technology==\nDirac supports resolutions of [[HDTV]] (1920×1080) and greater, and is claimed to provide significant savings in data rate and improvements in quality over video compression formats such as [[MPEG-2 Part 2]], [[MPEG-4 Part 2]] and its competitors, e.g. [[Theora]], and [[Windows Media Video|WMV]]. Dirac's implementers make the preliminary claim of \"''a two-fold reduction in bit rate over MPEG-2 for high definition video''\",<ref>{{cite web |url = http://www.bbc.co.uk/rd/projects/dirac/overview.shtml |title = Dirac Overview |publisher = BBC R&D |accessdate = 2009-01-14 }}</ref> which makes it comparable to standards such as [[H.264/MPEG-4 AVC]] and [[VC-1]].\n\nDirac supports both [[constant bit rate]] and [[variable bit rate]] operation. When the low delay syntax is used, the bit rate will be constant for each area (Dirac slice) in a picture to ensure constant latency. Dirac supports [[lossy]] and [[lossless]] compression modes.<ref>{{cite journal |url = http://diracvideo.org/download/specification/dirac-spec-latest.pdf |title = Dirac Specification |author = BBC Research |publisher = diracvideo.org |format = PDF |date = 23 September 2008 |accessdate = 2009-10-04|archive-url=https://web.archive.org/web/20160308225842/http://diracvideo.org/download/specification/dirac-spec-latest.pdf|archive-date=8 March 2016}}</ref>\n\nDirac employs [[wavelet compression]], like the [[JPEG 2000]] and [[Progressive Graphics File|PGF]] image formats and the [[Cineform]] professional video codec, instead of the [[discrete cosine transform]]s used in [[MPEG]] compression formats. Two of the specific wavelets Dirac can use are nearly identical to JPEG 2000's (known as the [[Cohen-Daubechies-Feauveau wavelet|5/3 and 9/7 wavelets]]), as well as two more derived from them.<ref>{{cite web |url = http://dirac.sourceforge.net/documentation/algorithm/algorithm/wlt_transform.xht |title = Dirac: Wavelet transform |first = Thomas |last = Davies |date = 2008-02-06 |accessdate = 2015-09-09 }}</ref>\n\nDirac can be used in [[Audio Video Interleave|AVI]], [[Ogg]] and [[Matroska]] [[container format (digital)|container formats]] and is also registered for use in the [[ISO base media file format|MPEG-4 file format]]<ref>[http://www.mp4ra.org/codecs.html MP4 Registration Authority – Dirac] {{Webarchive|url=https://web.archive.org/web/20090419074514/http://www.mp4ra.org/codecs.html |date=19 April 2009 }} Retrieved on 2009-07-05</ref> and [[MPEG transport stream|MPEG-2 transport streams]].<ref>[http://smpte-ra.org/mpegreg/drac.html SMPTE Registration Authority, LLC; Registration for format_identifier drac] {{webarchive|url=https://web.archive.org/web/20090421041153/http://smpte-ra.org/mpegreg/drac.html |date=21 April 2009 }} Retrieved on 2009-07-05</ref>\n\n==VC-2==\nDirac Pro was proposed to the SMPTE for standardisation.<ref name=\"bbc-white-paper\" /><ref name=\"vc2-dirac\">{{cite web |publisher = SMPTE |title = Advancements in Compression and Transcoding: 2008 and Beyond – Supporting the March to 8K with SMPTE VC2 / Dirac |url = http://www.smpte.org/events/smpte_annual_tech/schedule/06wedspm1/ |accessdate = 2009-08-30 |archive-url = https://web.archive.org/web/20110719180141/http://www.smpte.org/events/smpte_annual_tech/schedule/06wedspm1/ |archive-date = 19 July 2011 |dead-url = yes |df = dmy-all }}</ref><ref>{{cite web |url = http://diracvideo.org/worlds-first-high-performance-dirac-video-codec-implementation-available/ |title = Worlds first high performance Dirac video codec implementation available |date = 6 March 2008 |accessdate = 2010-08-18 }}</ref> The Dirac Pro specification defines an [[I-frame]] only subset of the main Dirac Specification, aimed for professional and studio use in high bitrate applications.<ref name=\"dirac-specs\" /><ref>{{cite web |url = http://www.bbc.co.uk/rd/projects/dirac/diracpro.shtml |title = Dirac Pro - WHAT IS DIRAC PRO? |author = BBC |accessdate = 2010-08-18 }}</ref> In 2010, the SMPTE standardised Dirac Pro as VC-2.<ref name=\"smpte-vc2\" /><ref>{{cite web |url = http://diracvideo.org/2010/03/schroedinger-1-0-9-released/ |title = Schroedinger-1.0.9 Released, Other news - SMPTE VC-2 |date = 4 March 2010 |accessdate = 2010-08-18 }}</ref>\n\n* SMPTE 2042-1:2009 VC-2 Video Compression<ref name=\"smpte-vc2\"/>\n* SMPTE 2042-2:2009 VC-2 Level Definitions\n* RP (Recommended Practices) 2047-1-2009 – VC-2 Mezzanine Level Compression of 1080P High Definition Video Sources\n* SMPTE 2047-2:2010  Carriage of VC-2 Compressed Video over HD-SDI\n* RP 2042-3:2010 – VC-2 Conformance Specification<ref>{{cite web |url = http://store.smpte.org/product-p/rp%202042-3-2010.htm |title = SMPTE RP 2042-3:2010, VC-2 Conformance Specification |accessdate = 2010-08-18 }}</ref>\n\nThe basic spec was updated in 2012, adding a new profile for lossless and near-lossless archiving.<ref>{{cite web |url = https://kws.smpte.org/kws/public/projects/project/details?project_id=21 |title = Revision of ST 2042-1 VC-2 -- New Profile |date = 2012-09-15 |accessdate = 2015-09-09 }}</ref>\n\n==Software implementations==\n{{Update|date=May 2010}}\n\n{{Infobox software\n| name                   = Schrödinger\n| developer              = David Schleef\n| latest_release_version = 1.0.11\n| latest_release_date    = {{Release date and age|df=yes|2012|01|23}}\n| genre                  = [[Video codec]]\n| license                = [[Mozilla Public Licence|MPL 1.1]], [[GNU General Public License|GNU GPL]] 2, [[GNU Lesser General Public License|GNU LGPL]] 2, [[MIT License]]\n| website                = <code>diracvideo.org</code> (Offline)\n}}\n\nTwo software implementations of the specification currently exist. The first is the BBC's reference implementation, formerly just called ''Dirac'' but renamed ''dirac-research'' to avoid confusion. It is written in C++ and released under the [[Mozilla Public License]], [[GNU GPL]] 2 and [[GNU LGPL]] [[free software license]]s. Version 1.0.0 of this implementation was released on 17 September 2008.\n\nA second implementation called ''Schrödinger'' was funded by the BBC and aims to provide high-performance, portable version of the codec whilst remaining 100% bitstream compatible. Schrödinger is written in ANSI [[C (programming language)|C]] and released under the same licenses as dirac-research, as well as the highly-permissive [[MIT License]]. The Schrödinger project also provides [[GStreamer]] plugins to enable the library to be used with that framework. On 22 February 2008, Schrödinger 1.0.0 was released.<ref>Diracvideo.org [http://diracvideo.org/download/schroedinger/ Download section – schroedinger-1.0.0.tar.gz, 22-Feb-2008 13:52, 739K], Retrieved on 2009-08-07</ref> This release was able to decode HD720/25p in real-time on a [[Core Duo]] laptop.\n\nAs of the release of Schrödinger-1.0.9, ''\"Schrödinger outperforms dirac-research in most encoding situations, both in terms of encoding speed and visual quality\"''.<ref>{{cite web |url = http://diracvideo.org/2010/03/schroedinger-1-0-9-released/ |title = ''Schrödinger-1.0.9 Released'' |accessdate = 2010-03-11 }}</ref> With that release, most of the encoding tools in dirac-research have been ported over to Schrödinger, giving Schrödinger the same as or better compression efficiency than dirac-research.\n\nAn encoder quality testing system has been put in place at BBC to check how well new encoding tools work and to make sure bugs that affect quality are quickly fixed.\n\n==Patents==\nThe BBC does not own any patents on Dirac. They previously had some patent applications with plans to irrevocably grant a royalty-free licence for their Dirac-related patents to everyone, but they let the applications lapse. In addition, the developers have said they will try to ensure that Dirac does not infringe on any third party patents, enabling the public to use Dirac for any purpose.<ref>{{cite web |url = http://www.diracvideo.org/wiki/FAQ#Do_you_infringe_any_patents.3F |title = ''Do you infringe any patents?'' in official FAQ |accessdate = 2009-01-14 |archiveurl = https://web.archive.org/web/20080804070759/http://diracvideo.org/wiki/FAQ#Do_you_infringe_any_patents.3F |archivedate = 4 August 2008 }}</ref>\n\n==Desktop playback and encoding==\nAs of November 2008, Dirac video playback is supported by [[VLC media player]] (version 0.9.2 or newer), and by applications using the [[GStreamer]] framework (such as [[Songbird (software)|Songbird]], [[Rhythmbox]] and [[Totem (media player)|Totem]]). Support has also been added to [[FFmpeg]].<ref>{{cite web |url = http://diracvideo.org/wiki/index.php/Dirac_Compatibility_Matrix |title = ''Dirac Compatibility Matrix'' in official wiki |accessdate = 2009-01-14 }}</ref>\n\nApplications which can encode to Dirac include [[MediaCoder]], [[LiVES]] and [[OggConvert]], as well as [[FFmpeg]].\n\n==Performance==\nThe algorithms in the Dirac specification have been designed with the intention to provide a competitive performance as compared to state-of-the-art international standards. Whether they succeeded is an open question; while at least one comparison exists which used implementations from the second quarter of 2008—it shows [[x264]] scoring higher than Dirac<ref name=etil>{{cite web |url=http://etill.net/projects/dirac_theora_evaluation/ |title=A performance assessment of the royalty-free and open video compression specifications Dirac, Dirac Pro, and Theora and their open-source implementations |date=March 2009 |format=PDF |archive-url=https://archive.is/20120707194336/http://etill.net/projects/dirac_theora_evaluation/ |archivedate=7 July 2012 |deadurl=yes |df=dmy-all }}</ref>—it is now somewhat out of date.<ref name=etil /> A study on the performances of the Dirac codec, dated from August 2009, finds that the quality obtained on SDTV is inferior to the H.264 output<ref>{{cite web |url=http://www-ee.uta.edu/Dip/Courses/EE5359/ArunaRavi_MSThesis.pdf |title=Performance analysis and comparison of Dirac video codec with H.264 / MPEG-4 Part 10 AVC (see conclusion §5) |accessdate=July 7, 2017}}</ref> and did not include HD content.\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n* {{Internet Archive film clip|id=LRL_USA_2008_Dirac|description=where David Schleef talks about Dirac}}\n* [http://www.bbc.co.uk/rd/projects/vc-2 BBC Research & Development page on VC-2]\n\n{{Portal|Free and open-source software}}\n{{Compression formats}}\n{{Compression Software Implementations}}\n\n[[Category:BBC Research & Development]]\n[[Category:Free video codecs]]\n[[Category:Lossless compression algorithms]]\n[[Category:SMPTE standards]]\n[[Category:Wavelets]]\n[[Category:Open formats]]"
    },
    {
      "title": "Dual wavelet",
      "url": "https://en.wikipedia.org/wiki/Dual_wavelet",
      "text": "{{refimprove|date=October 2010}}\nIn [[mathematics]], a '''dual wavelet''' is the [[dual space|dual]] to a [[wavelet]]. In general, the [[wavelet series]] generated by a [[square integrable]] [[function (mathematics)|function]] will have a dual series, in the sense of the [[Riesz representation theorem]]. However, the dual series is not itself in general representable by a square integrable function.\n\n==Definition==\nGiven a square integrable function <math>\\psi\\in L^2(\\mathbb{R})</math>, define the series <math>\\{\\psi_{jk}\\}</math> by\n\n:<math>\\psi_{jk}(x) = 2^{j/2}\\psi(2^jx-k)</math>\n\nfor integers <math>j,k\\in \\mathbb{Z}</math>.\n\nSuch a function is called an '''''R''-function''' if the linear span of <math>\\{\\psi_{jk}\\}</math> is [[dense set|dense]] in <math>L^2(\\mathbb{R})</math>, and if there exist positive constants ''A'', ''B'' with <math>0<A\\leq B < \\infty</math> such that\n\n:<math>A \\Vert c_{jk} \\Vert^2_{l^2} \\leq \n\\bigg\\Vert \\sum_{jk=-\\infty}^\\infty c_{jk}\\psi_{jk}\\bigg\\Vert^2_{L^2} \\leq \nB \\Vert c_{jk} \\Vert^2_{l^2}\\,</math>\n\nfor all bi-infinite [[square summable]] series <math>\\{c_{jk}\\}</math>.  Here, <math>\\Vert \\cdot \\Vert_{l^2}</math> denotes the square-sum norm:\n\n:<math>\\Vert c_{jk} \\Vert^2_{l^2} = \\sum_{jk=-\\infty}^\\infty \\vert c_{jk}\\vert^2</math>\n\nand <math>\\Vert \\cdot\\Vert_{L^2}</math> denotes the usual norm on <math>L^2(\\mathbb{R})</math>:\n\n:<math>\\Vert f\\Vert^2_{L^2}= \\int_{-\\infty}^\\infty \\vert f(x)\\vert^2 dx</math>\n\nBy the [[Riesz representation theorem]], there exists a unique dual basis <math>\\psi^{jk}</math> such that\n\n:<math>\\langle \\psi^{jk} \\vert \\psi_{lm} \\rangle = \\delta_{jl} \\delta_{km}</math>\n\nwhere <math>\\delta_{jk}</math> is the [[Kronecker delta]] and <math>\\langle f \\vert g \\rangle</math> is the usual [[inner product]] on <math>L^2(\\mathbb{R})</math>. Indeed, there exists a unique [[series representation]] for a square integrable function ''f'' expressed in this basis:\n\n:<math>f(x) = \\sum_{jk} \\langle \\psi^{jk} \\vert f \\rangle \\psi_{jk}(x)</math>\n\nIf there exists a function <math>\\tilde{\\psi} \\in L^2(\\mathbb{R})</math> such that\n\n:<math>\\tilde{\\psi}_{jk} = \\psi^{jk}</math>\n\nthen <math>\\tilde{\\psi}</math> is called the '''dual wavelet''' or the '''wavelet dual to &psi;'''. In general, for some given ''R''-function &psi;, the dual will not exist.  In the special case of <math>\\psi = \\tilde{\\psi}</math>, the wavelet is said to be an [[orthogonal wavelet]].\n\nAn example of an ''R''-function without a dual is easy to construct. Let <math>\\phi</math> be an orthogonal wavelet. Then define <math>\\psi(x) = \\phi(x) + z\\phi(2x)</math> for some complex number ''z''. It is straightforward to show that this &psi; does not have a wavelet dual.\n\n==See also==\n* [[Multiresolution analysis]]\n\n==References==\n* Charles K. Chui, ''An Introduction to Wavelets (Wavelet Analysis & Its Applications)'', (1992), Academic Press, San Diego, {{ISBN|0-12-174584-8}}\n\n[[Category:Wavelets]]\n[[Category:Duality theories|Wavelet]]"
    },
    {
      "title": "Dynamic link matching",
      "url": "https://en.wikipedia.org/wiki/Dynamic_link_matching",
      "text": "'''Dynamic link matching''' is a graph-based system for [[image recognition]].  It uses [[wavelet transformation]]s to encode incoming image data.<ref>[http://aicat.inf.ed.ac.uk/entry.php?id=656 \"Dynamic Link Matching\"] in ''Catalogue of Artificial Intelligence Techniques'' </ref>\n\n==References==\n{{reflist}}\n==External links==\n*[http://nn.cs.utexas.edu/web-pubs/htmlbook96/wiskott/ Original paper on Dynamic Link Matching]\n\n[[Category:Wavelets]]\n[[Category:Pattern recognition]]\n[[Category:Graph algorithms]]\n\n{{comp-sci-stub}}"
    },
    {
      "title": "Embedded Zerotrees of Wavelet transforms",
      "url": "https://en.wikipedia.org/wiki/Embedded_Zerotrees_of_Wavelet_transforms",
      "text": "'''Embedded Zerotrees''' of '''Wavelet transforms''' ('''EZW''') is a lossy [[image compression]] [[algorithm]]. At low bit rates, i.e. high compression ratios, most of the coefficients produced by a [[Sub-band coding|subband transform]] (such as the [[wavelet transform]])\nwill be zero, or very close to zero. This occurs because \"real world\" images tend to contain mostly low frequency information (highly correlated). However where high frequency information does occur (such as edges in the image) this is particularly important in terms of human perception of the image quality, and thus must be represented accurately in any high quality coding scheme.\n\nBy considering the transformed coefficients as a [[Tree (graph theory)|tree]] (or trees) with the lowest frequency coefficients at the root node and with the children of each tree node being the spatially related coefficients in the next higher frequency subband, there is a high probability that one or more subtrees will consist entirely of coefficients which are zero or nearly zero, such subtrees are called '''zerotrees'''. Due to this, we use the terms node and coefficient interchangeably, and when we refer to the children of a coefficient, we mean the child coefficients of the node in the tree where that coefficient is located. We use '''children''' to refer to directly connected nodes lower in the tree and '''descendants''' to refer to all nodes which are below a particular node in the tree, even if not directly connected.\n\nIn zerotree based image compression scheme such as EZW and [[SPIHT]], the intent is to use the statistical properties of the trees in order to efficiently code the locations of the significant coefficients. Since most of the coefficients will be zero or close to zero, the spatial locations of the significant coefficients make up a large portion of the total size of a typical compressed image. A coefficient (likewise a tree) is considered '''significant''' if its magnitude (or magnitudes of a node and all its descendants in the case of a tree) is above a particular threshold. By starting with a threshold which is close to the maximum coefficient magnitudes and iteratively decreasing the threshold, it is possible to create a compressed representation of an image which progressively adds finer detail. Due to the structure of the trees, it is very likely that if a coefficient in a particular frequency band is insignificant, then all its descendants (the spatially related higher frequency band coefficients) will also be insignificant.\n\nEZW uses four symbols to represent (a) a zerotree root, (b) an isolated zero (a coefficient which is insignificant, but which has significant descendants), (c) a significant positive coefficient and (d) a significant negative coefficient. The symbols may be thus represented by two binary bits. The compression algorithm consists\nof a number of iterations through a '''dominant pass''' and a '''subordinate pass''', the threshold is updated (reduced by a factor of two) after each iteration. The dominant pass encodes the significance of the coefficients which have not yet been found significant in earlier iterations, by scanning the trees and emitting one of the four symbols. The children of a coefficient are only scanned if the coefficient was found to be significant, or if the coefficient was an isolated zero. The subordinate pass emits one bit (the most significant bit of each coefficient not so far emitted) for each coefficient which has been found significant in the previous significance passes. The subordinate pass is therefore similar to bit-plane coding.\n\nThere are several important features to note. Firstly, it is possible to stop the compression algorithm at any time and obtain an approximation of the original image, the greater the number of bits received, the better the image. Secondly, due to the way in which the compression algorithm is structured as a series of decisions, the same algorithm can be run at the decoder to reconstruct the coefficients, but with the decisions being taken according to the incoming bit stream. In practical implementations, it would be usual to use an entropy code such as [[Arithmetic coding|arithmetic code]] to further improve the performance of the dominant pass. Bits from the subordinate pass are usually random enough that entropy coding provides no further coding gain.\n\nThe coding performance of EZW has since been exceeded by [[SPIHT]] and its many derivatives.\n\n== Introduction ==\n'''Embedded zerotree wavelet algorithm''' (EZW) as developed by J. Shapiro in 1993, enables scalable image transmission and decoding. It is based on four key concepts: first, it should be a discrete wavelet transform or hierarchical subband decomposition; second, it should predict the absence of significant information when exploring the [[self-similarity]] inherent in images; third, it has  entropy-coded successive-approximation quantization, and fourth, it is enabled to achieve universal lossless data compression via adaptive arithmetic coding.\n\nBesides, the EZW algorithm also contains the following features:\n\n(1) A discrete wavelet transform which can use a compact multiresolution representation in the image.\n\n(2) Zerotree coding which provides a compact multiresolution representation of significance maps.\n\n(3) Successive approximation for a compact multiprecision representation of the significant coefficients.\n\n(4) A prioritization protocol which the importance is determined by the precision, magnitude, scale, and spatial location of the wavelet coefficients in order.\n\n(5) Adaptive multilevel arithmetic coding which is a fast and efficient method for entropy coding strings of symbols.\n\n== Embedded Zerotree Wavelet Coding ==\n\n=== A. Encoding a coefficient of the significance map ===\nIn a significance map, the coefficients can be representing by the following four different symbols. With using these symbols to represent the image information, the coding will be less complication.\n\n==== 1. Zerotree root ====\nIf the magnitude of a coefficient is less than a threshold T, and all its descendants are less than T, then this coefficient is called zerotree root. And if a coefficient has been labeled as zerotree root, it means that all of its descendants are insignificant, so there is no need to label its descendants.\n\n==== 2. Isolated zero ====\nIf the magnitude of a coefficient that is less than a threshold T, but it still has some significant descendants, then this coefficient is called isolated zero.\n\n==== 3. Positive significant coefficient ====\nIf the magnitude of a coefficient is greater than a threshold T at level T, and also is positive, than it is a positive significant coefficient.\n\n==== 4. Negative significant coefficient ====\nIf the magnitude of a coefficient is greater than a threshold T at level T, and also is negative, than it is a negative significant coefficient.\n\n=== B. Defining threshold ===\nThe threshold using above can be defined as the type below.\n\n==== 1. Initial threshold T<sub>0</sub>: (Assume C<sub>max</sub> is the largest coefficient.) ====\n<span>[[File:Threshold-0119.png|126x126px]]</span> \n==== 2. Threshold T<sub>i</sub> is reduced to half of the value of the previous threshold. ====\n[[File:Threshold-01192.png|frameless|133x133px]] \n=== C. Scanning order for coefficients ===\n'''Raster scanning''' is the rectangular pattern of image capture and reconstruction. Using this scanning on EZW transform is to perform scanning the coefficients in such way that no child node is scanned before its parent node. Also, all positions in a given subband are scanned before it moves to the next subband.\n\n=== D. Two-pass bitplane coding ===\n\n==== (1) Refinement pass (or subordinate pass) ====\nThis determine that if the coefficient is the internal [Ti, 2Ti). And A refinement bit is coded for each significant coefficient.\n\nIn this method, it will visit the significant coefficients according to the magnitude and raster order within subbands.\n\n==== (2) Significant pass (or dominant pass) ====\nThis method will code a bit for each coefficient that is not yet be seen as significant. Once a determination of significance has been made, the significant coefficient is included in a list for further refinement in the refinement pass. And if any coefficient already known to be zero, it will not be coded again.\n\n==See also==\n* [[Set partitioning in hierarchical trees]] (SPIHT)\n\n==References==\n*Shapiro, J. M., {{doi-inline|10.1109/78.258085|EMBEDDED IMAGE CODING USING ZEROTREES OF WAVELET COEFFICIENTS}}. [[IEEE]] Transactions on Signal Processing, Vol. 41, No. 12 (1993), p.&nbsp;3445-3462.\n\n==External links==\n{{commons category|EZW}}\n<!-- *https://web.archive.org/web/20071013231046/http://perso.orange.fr/polyvalens/clemens/clemens.html -->\n\n*{{cite web |url=http://polyvalens.pagesperso-orange.fr/clemens/ezw/ezw.html |title=EZW encoding |author=Clemens Valens |date=2003-08-24 |access-date= |archive-url=https://web.archive.org/web/20090203181451/http://pagesperso-orange.fr:80/polyvalens/clemens/ezw/ezw.html |archive-date=2009-03-02}}\n\n{{Compression methods}}\n\n[[Category:Image compression]]\n[[Category:Lossless compression algorithms]]\n[[Category:Trees (data structures)]]\n[[Category:Wavelets]]"
    },
    {
      "title": "Fast wavelet transform",
      "url": "https://en.wikipedia.org/wiki/Fast_wavelet_transform",
      "text": "{{Multiple issues|\n{{Refimprove|date=January 2010}}\n{{More footnotes|date=February 2018}}\n}}\nThe '''Fast Wavelet Transform''' is a [[mathematics|mathematical]] [[algorithm]] designed to turn a [[waveform]] or signal in the [[time domain]] into a [[sequence]] of coefficients based on an [[orthogonal basis]] of small finite waves, or [[wavelets]]. The transform can be easily extended to multidimensional signals, such as images, where the time domain is replaced with the space domain. This algorithm was introduced in 1989 by [[Stéphane Mallat]].<ref>{{cite web |title=Fast Wavelet Transform (FWT) Algorithm |url=https://www.mathworks.com/help/wavelet/ug/fast-wavelet-transform-fwt-algorithm.html?requestedDomain=true |publisher=[[MathWorks]] |access-date=2018-02-20}}</ref>\n\nIt has as theoretical foundation the device of a finitely generated, orthogonal [[multiresolution analysis]] (MRA). In the terms given there, one selects a sampling scale ''J'' with [[sampling rate]] of 2<sup>J</sup> per unit interval, and projects the given signal ''f'' onto the space <math>V_J</math>; in theory by computing the [[dot product|scalar product]]s \n\n:<math>s^{(J)}_n:=2^J \\langle f(t),\\phi(2^J t-n) \\rangle,</math> \n\nwhere <math>\\phi</math> is the [[Wavelet#Scaling_function|scaling function]] of the chosen wavelet transform; in practice by any suitable sampling procedure under the condition that the signal is highly oversampled, so\n:<math> P_J[f](x):=\\sum_{n\\in\\Z} s^{(J)}_n\\,\\phi(2^Jx-n)</math>\nis the [[orthogonal projection]] or at least some good approximation of the original signal in <math>V_J</math>. \n\nThe MRA is characterised by its scaling sequence\n\n:<math>a=(a_{-N},\\dots,a_0,\\dots,a_N)</math> or, as [[Z-transform]], <math>a(z)=\\sum_{n=-N}^Na_nz^{-n}</math>\n\nand its wavelet sequence \n\n:<math>b=(b_{-N},\\dots,b_0,\\dots,b_N)</math> or <math>b(z)=\\sum_{n=-N}^Nb_nz^{-n}</math>\n\n(some coefficients might be zero). Those allow to compute the wavelet coefficients <math>d^{(k)}_n</math>, at least some range ''k=M,...,J-1'', without having to approximate the integrals in the corresponding scalar products. Instead, one can directly, with the help of convolution and decimation operators, compute those coefficients from the first approximation <math>s^{(J)}</math>.\n\n== Forward DWT ==\nFor the [[discrete wavelet transform]] (DWT), \none computes [[recursion|recursively]], starting with the coefficient sequence <math>s^{(J)}</math> and counting down from ''k=J-1'' to some ''M<J'',\n\n[[Image:Wavelets_-_DWT.png|thumb|390px|single application of a wavelet filter bank, with filters g=a<sup>*</sup>, h=b<sup>*</sup>]]\n:<math>\ns^{(k)}_n:=\\frac12 \\sum_{m=-N}^N a_m s^{(k+1)}_{2n+m}\n</math> or <math>\ns^{(k)}(z):=(\\downarrow 2)(a^*(z)\\cdot s^{(k+1)}(z))\n</math>\nand \n:<math>\nd^{(k)}_n:=\\frac12 \\sum_{m=-N}^N b_m s^{(k+1)}_{2n+m}\n</math> or <math>\nd^{(k)}(z):=(\\downarrow 2)(b^*(z)\\cdot s^{(k+1)}(z))\n</math>,\n\nfor ''k=J-1,J-2,...,M'' and all <math>n\\in\\Z</math>. In the Z-transform notation:\n[[Image:Wavelets_-_Filter_Bank.png|thumb|400px|recursive application of the filter bank]]\n:* The [[downsampling|downsampling operator]] <math>(\\downarrow 2)</math> reduces an infinite sequence, given by its [[Z-transform]], which is simply a [[Laurent series]], to the sequence of the coefficients with even indices, <math>(\\downarrow 2)(c(z))=\\sum_{k\\in\\Z}c_{2k}z^{-k}</math>. \n:* The starred Laurent-polynomial <math>a^*(z)</math> denotes the [[adjoint filter]], it has ''time-reversed'' adjoint coefficients, <math>a^*(z)=\\sum_{n=-N}^N a_{-n}^*z^{-n}</math>. (The adjoint of a real number being the number itself, of a complex number its conjugate, of a real matrix the transposed matrix, of a complex matrix its hermitian adjoint).\n:* Multiplication is polynomial multiplication, which is equivalent to the convolution of the coefficient sequences.\n\nIt follows that\n\n:<math>P_k[f](x):=\\sum_{n\\in\\Z} s^{(k)}_n\\,\\phi(2^kx-n)</math>\n\nis the orthogonal projection of the original signal ''f'' or at least of the first approximation <math>P_J[f](x)</math> onto the [[linear subspace|subspace]] <math>V_k</math>, that is, with sampling rate of 2<sup>k</sup> per unit interval. The difference to the first approximation is given by\n\n:<math>P_J[f](x)=P_k[f](x)+D_k[f](x)+\\dots+D_{J-1}[f](x)</math>,\n\nwhere the difference or detail signals are computed from the detail coefficients as\n\n:<math>D_k[f](x):=\\sum_{n\\in\\Z} d^{(k)}_n\\,\\psi(2^kx-n)</math>,\n\nwith <math>\\psi</math> denoting the ''mother wavelet'' of the wavelet transform.\n\n==Inverse DWT==\nGiven the coefficient sequence <math>s^{(M)}</math> for some ''M<J'' and all the difference sequences <math>d^{(k)}</math>, ''k=M,...,J-1'', one computes recursively\n:<math>\ns^{(k+1)}_n:=\\sum_{k=-N}^N a_k s^{(k)}_{2n-k}+\\sum_{k=-N}^N b_k d^{(k)}_{2n-k}\n</math> or <math>\ns^{(k+1)}(z)=a(z)\\cdot(\\uparrow 2)(s^{(k)}(z))+b(z)\\cdot(\\uparrow 2)(d^{(k)}(z))\n</math>\nfor ''k=J-1,J-2,...,M'' and all <math>n\\in\\Z</math>. In the Z-transform notation:\n:* The [[upsampling|upsampling operator]] <math>(\\uparrow 2)</math> creates zero-filled holes inside a given sequence. That is, every second element of the resulting sequence is an element of the given sequence, every other second element is zero or <math>(\\uparrow 2)(c(z)):=\\sum_{n\\in\\Z}c_nz^{-2n}</math>. This linear operator is, in the [[Hilbert space]] <math>\\ell^2(\\Z,\\R)</math>, the adjoint to the downsampling operator <math>(\\downarrow 2)</math>.\n\n==See also==\n* [[Lifting scheme]]\n\n==References==\n{{reflist}}\n* S.G. Mallat \"A Theory for Multiresolution Signal Decomposition: The Wavelet Representation\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 2, no. 7. July 1989.\n* A.N. Akansu ''Multiplierless Suboptimal PR-QMF Design'' Proc. SPIE 1818, Visual Communications and Image Processing, p. 723, November, 1992\n* A.N. Akansu ''Multiplierless 2-band Perfect Reconstruction Quadrature Mirror Filter (PR-QMF) Banks'' US Patent 5,420,891, 1995\n* A.N. Akansu ''Multiplierless PR Quadrature Mirror Filters for Subband Image Coding'' IEEE Trans. Image Processing, p. 1359, September 1996\n* M.J. Mohlenkamp, [[Cristina Pereyra|M.C. Pereyra]] ''Wavelets, Their Friends, and What They Can Do for You'' (2008 EMS) p. 38\n* B.B. Hubbard ''The World According to Wavelets: The Story of a Mathematical Technique in the Making (1998 Peters) p. 184\n* S.G. Mallat ''A Wavelet Tour of Signal Processing'' (1999 Academic Press) p. 255\n* A. Teolis ''Computational Signal Processing with Wavelets'' (1998 Birkhäuser) p. 116\n* Y. Nievergelt ''Wavelets Made Easy'' (1999 Springer) p. 95\n\n==Further reading==\nG. Beylkin, R. Coifman, V. Rokhlin, \"Fast wavelet transforms and numerical algorithms\"  ''Comm. Pure Appl. Math.'', 44 (1991) pp. 141–183\n\n[[Category:Wavelets]]\n[[Category:Discrete transforms]]"
    },
    {
      "title": "Filter bank",
      "url": "https://en.wikipedia.org/wiki/Filter_bank",
      "text": "{{tone|date=February 2015}}\nIn [[signal processing]], a '''filter bank''' is an array of [[bandpass filter|band-pass]] [[filter (signal processing)|filters]] that separates the input signal into multiple components, each one carrying a single [[frequency]] [[Sub-band coding|sub-band]] of the original signal.  One application of a filter bank is a [[graphic equalizer]], which can attenuate the components differently and recombine them into a modified version of the original signal.  The process of decomposition performed by the filter bank is called ''analysis'' (meaning analysis of the signal in terms of its components in each sub-band); the output of analysis is referred to as a subband signal with as many subbands as there are filters in the filter bank.  The reconstruction process is called ''synthesis'', meaning reconstitution of a complete signal resulting from the filtering process.\n\nIn [[digital signal processing]], the term '''filter bank''' is also commonly applied to a bank of receivers.  The difference is that receivers also [[Digital down converter|down-convert]] the subbands to a low center frequency that can be re-sampled at a reduced rate.  The same result can sometimes be achieved by [[undersampling]] the bandpass subbands.\n\nAnother application of filter banks is [[Signalling (telecommunication)|signal]] compression when some frequencies are more important than others.  After decomposition, the important frequencies can be coded with a fine resolution.  Small differences at these frequencies are significant and a [[coding theory|coding]] scheme that preserves these differences must be used. On the other hand, less important frequencies do not have to be exact. A coarser coding scheme can be used, even though some of the finer (but less important) details will be lost in the coding.\n\nThe [[vocoder]] uses a filter bank to determine the amplitude information of the subbands of a modulator signal (such as a voice) and uses them to control the amplitude of the subbands of a carrier signal (such as the output of a guitar or synthesizer), thus imposing the dynamic characteristics of the modulator on the carrier.\n\n== FFT filter banks ==\nA bank of receivers can be created by performing a sequence of [[Fast Fourier transform|FFT]]s on overlapping ''segments'' of the input data stream.  A weighting function (aka [[window function]]) is applied to each segment to control the shape of the [[frequency response]]s of the filters.  The wider the shape, the more often the FFTs have to be done to satisfy the [[Nyquist rate|Nyquist sampling criteria]].<ref group=\"note\">The term ''filter'' implies that it preserves the information within its passband, and suppresses the information (or noise) outside the passband.  When the FFT rate is not sufficient for that, the design is typically called ''spectrum analyzer''.  And in that case, it is not necessary for the segments to overlap.</ref>  For a fixed segment length, the amount of overlap determines how often the FFTs are done (and vice versa).  Also, the wider the shape of the filters, the fewer filters that are needed to span the input bandwidth.  Eliminating unnecessary filters (i.e. decimation in frequency) is efficiently done by treating each weighted segment as a sequence of smaller ''blocks'', and the FFT is performed on only the sum of the blocks.  This has been referred to as ''multi-block windowing'' and ''weighted pre-sum FFT'' (see [[Discrete-time Fourier transform#Sampling the DTFT|Sampling the DTFT]]).\n\nA special case occurs when, by design, the length of the blocks is an integer multiple of the interval between FFTs.  Then the FFT filter bank can be described in terms of one or more polyphase filter structures where the phases are recombined by an FFT instead of a simple summation.  The number of blocks per segment is the impulse response length (or ''depth'') of each filter.  The computational efficiencies of the FFT and polyphase structures, on a general purpose processor, are identical.\n\nSynthesis (i.e. recombining the outputs of multiple receivers) is basically a matter of [[upsampling]] each one at a rate commensurate with the total bandwidth to be created, translating each channel to its new center frequency, and summing the streams of samples.  In that context, the interpolation filter associated with upsampling is called ''synthesis filter''.  The net frequency response of each channel is the product of the synthesis filter with the frequency response of the filter bank (''analysis filter'').  Ideally, the frequency responses of adjacent channels sum to a constant value at every frequency between the channel centers.  That condition is known as ''perfect reconstruction''.\n\n==Filter banks as time-frequency distributions==\nIn time-frequency signal processing, a filter bank is a special quadratic time-frequency distribution (TFD) that represents the signal in a joint time-frequency domain. It is related to the [[Wigner-Ville distribution]] by a two-dimensional filtering that defines the class of [[Bilinear time-frequency distribution|quadratic (or bilinear) time-frequency distributions]].<ref>B. Boashash, editor, \"Time-Frequency Signal Analysis and Processing – A Comprehensive Reference\", Elsevier Science, Oxford, 2003; {{ISBN|0-08-044335-4}}</ref> The filter bank and the spectrogram are the two simplest ways of producing a quadratic TFD; they are in essence similar as one (the spectrogram) is obtained by dividing the time-domain in slices and then taking a Fourier transform, while the other (the filter bank) is obtained by dividing the frequency domain in slices forming bandpass filters that are excited by the signal under analysis.\n\n==Multirate Filter Bank==\nA multirate filter bank divides a signal into a number of subbands, which can be analysed at different rates corresponding to the bandwidth of the frequency bands.  The implementation makes use of [[Downsampling (signal processing)|downsampling (decimation)]] and [[Upsampling|upsampling (expansion)]].  See [[Discrete-time_Fourier_transform#Properties]] and [[Z-transform#Properties]] for additional insight into the effects of those operations in the transform domains.\n\n===Narrow lowpass filter===\nWe can define a narrow lowpass filter as a [[lowpass filter]] with a narrow passband.\nIn order to create a multirate narrow lowpass FIR filter, we need to replace the time invariant FIR filter with a lowpass antialiasing filter and use a decimator along with an interpolator and lowpass anti-imaging filter\n\nIn this way, the resulting multirate system is a time varying linear phase filter via the decimator and interpolator.\nThis process explained in block diagram form where Figure 2 (a) is replaced by Figure 2(b).\nThe lowpass filter consists of two polyphase filters, one for the decimator and one for the interpolator.<ref>{{cite book|last1=Parks|first1=TW|title=Digital Filter Design|date=1987|publisher=Wiley-Interscience}}</ref>\n\nA filter bank divides the input signal <math>x\\left(n\\right)</math> into a set of signals <math>x_{1}(n),x_{2}(n),x_{3}(n),...</math>. In this way each of the generated signals corresponds to a different region in the spectrum of <math>x\\left(n\\right)</math>.\nIn this process it can be possible for the regions overlap (or not, based on application).\nFigure 4 shows an example of a three-band filter bank.\nThe generated signals <math>x_{1}(n),x_{2}(n),x_{3}(n),...</math> can be generated via a collection of set of bandpass filters with bandwidths  <math>\\rm BW_{1},BW_{2},BW_{3},...</math> and center frequencies <math>f_{c1},f_{c2},f_{c3},...</math>(respectively).\nA multirate filter bank uses a single input signal and then produces multiple outputs of the signal by filtering and subsampling.\nIn order to split the input signal into two or more signals (see Figure 5) an analysis-synthesis system can be used.\nIn figure 5, only 4 sub-signals are used.\n\nThe signal would split with the help of four filters <math>H_{k}(z)</math> for ''k'' =0,1,2,3 into 4 bands of the same bandwidths (In the analysis bank) and then each sub-signal is decimated by a factor of 4.\nIn each band by dividing the signal in each band, we would have different signal characteristics.\n\nIn synthesis section the filter will reconstruct the original signal:\nFirst, upsampling the 4 sub-signals at the output of the processing unit by a factor of 4 and then filter by 4 synthesis filters <math>F_{k}(z)</math> for ''k'' = 0,1,2,3.\nFinally, the outputs of these four filters are added.\n\n==Multidimensional filter banks==\n[[Multidimensional filter design|Multidimensional Filtering]], [[downsampling]], and [[upsampling]] are the main parts of [[multirate systems]] and filter banks.\n\nA complete filter bank consists of the analysis and synthesis side.\nThe analysis filter bank divides an input signal to different subbands with different frequency spectrums.\nThe synthesis part reassembles the different subband signals and generates a reconstruction signal.\nTwo of the basic building blocks are the decimator and expander. For example, in Figure 6, the input divides into four directional sub bands that each of them covers one of the wedge-shaped frequency regions. In 1D systems, M-fold decimators keep only those samples that are multiples of M and discard the rest. while in multi-dimensional systems the decimators are ''D'' × ''D'' nonsingular integer matrix. it considers only those samples that are on the lattice generated by the decimator. Commonly used decimator is the quincunx decimator whose lattice is generated from the [[Quincunx matrix]] which is defined by <math>\\begin{bmatrix}1 & 1 \\\\-1 & 1 \\end{bmatrix}</math>\n[[File:Screenshot (80).png|thumb|The quincunx lattice]]\nThe quincunx lattice generated by Quincunx matrix is as shown. Synthesis part is dual to the analysis part.\nIt is important to analyze filter banks from a frequency domain perspective in terms of subband decomposition and reconstruction. However, equally important is [[Hilbert Spaces and Fourier analysis|hilbert space]] interpretation of filter banks, which plays a key role in geometrical signal representations.\nFor generic ''K''-channel filter bank, with analysis filters <math>\\left\\{ h_{k}[n]\\right\\} _{k=1}^{K}\n</math>, synthesis filters <math>\\left\\{ g_{k}[n]\\right\\} _{k=1}^{K}</math>, and sampling matrices <math>\\left\\{ M_{k}[n]\\right\\} _{k=1}^{K}\n</math>.\nIn the analysis side, we can define vectors in ''<math>\\ell^{2}(\\mathbf{Z}^{d})\n</math>'' as\n\n:<math>\\varphi_{k,m}[n]\\stackrel{\\rm def}{=}h_{k}^{*}[M_{k}m-n]</math>,\n\neach index by two parameters: <math>1\\leq k\\leq K</math> and <math>m\\in \\mathbf{Z}^{2}</math>.\n\nSimilarly, for the synthesis filters <math>g_{k}[n]</math> we can define <math>\\psi_{k,m}[n]\\stackrel{\\rm def}{=}g_{k}^{*}[M_{k}m-n]</math>.\n\nConsidering the definition of analysis/synthesis sides we can verify that <ref>{{cite journal|last1=Do|first1=Minh N|title=Multidimensional filter banks and multiscale geometric representations|journal=Signal Processing|date=2011|pages=157–264}}</ref> <math>c_{k}[m]=\\langle x[n],\\varphi_{k,m}[n] \\rangle</math> and for reconstruction part:\n\n:<math>\\hat{x}[n]=\\sum_{1\\leq k\\leq K,m\\in \\mathbf{Z}^{2}}c_{k}[m]\\psi_{k,m}[n]</math>.\n\nIn other words, the analysis filter bank calculate the inner product of the input signal and the vector from analysis set. Moreover, the reconstructed signal in the combination of the vectors from the synthesis set, and the combination coefficients of the computed inner products, meaning that\n\n:<math>\\hat{x}[n]=\\sum_{1\\leq k\\leq K,m\\in \\mathbf{Z}^{2}}\\langle x[n],\\varphi_{k,m}[n] \\rangle\\psi_{k,m}[n]</math>\n\nIf there is no loss in the decomposition and the subsequent reconstruction, the filter bank is called ''perfect reconstruction''. (in that case we would have <math>x[n]=\\hat{x[n]}</math>.<ref>{{cite book|last1=Mallat|first1=Stephane|title=A wavelet tour of signal processing: the sparse way|date=2008|publisher=Academic press}}</ref>\nFigure shows a general multidimensional filter bank with ''N'' channels and a common sampling matrix ''M''.\nThe analysis part transforms the input signal <math>x[n]</math> into ''N'' filtered\nand downsampled outputs <math>y_{j}[n],</math> <math>j=0,1,...,N-1</math>.\nThe synthesis part recovers the original signal from <math>y_{j}[n]</math> by upsampling and filtering.\nThis kind of set up is used in many applications such as [[subband coding]], multichannel acquisition, and [[wavelet transforms|discrete wavelet transforms]].\n\n===Perfect reconstruction filter banks===\nWe can use polyphase representation, so input signal <math>x[n]</math> can be represented by a vector of its polyphase components <math>x(z)\\stackrel{\\rm def}{=}(X_{0}(z),...,X_{|M|-1}(z))^{T}\n</math>. Denote <math>y(z)\\stackrel{\\rm def}{=}(Y_{0}(z),...,Y_{|N|-1}(z))^{T}.</math>\n<br />\nSo we would have <math>y(z)=H(z)x(z)</math>, where <math>H_{i,j}(z)</math> denotes the ''j''-th polyphase component of the filter <math>H_{i}(z)</math>.\n\nSimilarly, for the output signal we would have <math>\\hat{x}(z)=G(z)y(z)</math>, where <math>\\hat{x}(z)\\stackrel{\\rm def}{=}(\\hat{X}_{0}(z),...,\\hat{X}_{|M|-1}(z))^{T}\n</math>. Also G is a matrix where <math>G_{i,j}(z)</math> denotes ith polyphase component of the jth synthesis\nfilter Gj(z).\n\nThe filter bank has perfect reconstruction\nif <math>x(z)= \\hat{x}(z)</math> for any input, or equivalently <math>I_{|M|}=G(z)H(z)</math> which means that G(z) is a left inverse of H(z).\n\n==Multidimensional filter design==\n1-D filter banks have been well developed until today. However, many signals, such as image, video, 3D sound, radar, sonar, are multidimensional, and require the design of multidimensional filter banks.\n\nWith the fast development of communication technology, signal processing system needs more room to store data during the processing, transmission and reception. In order to reduce the data to be processed, save storage and lower the complexity, multirate sampling techniques were introduced to achieve these goals. Filter banks can be used in various areas, such as image coding, voice coding, radar and so on.\n\nMany 1D filter issues were well studied and researchers proposed many 1D filter bank design approaches. But there are still many multidimensional filter bank design problems that need to be solved.<ref>Chen, Tsuhan, and P. P. Vaidyanathan. \"Considerations in multidimensional filter bank design\" IEEE International Symposium on Circuits and Systems, pp.&nbsp;643–646., May, 1993.</ref> Some methods may not well reconstruct the signal, some methods are complex and hard to implement.\n[[File:1D Filter Bank.jpg|thumb|right|1D Filter Bank]]\nThe simplest approach to design a multi-dimensional filter bank is to cascade 1D filter banks in the form of a tree structure where the decimation matrix is diagonal and data is processed in each dimension separately. Such systems are referred to as separable systems. However, the region of support for the filter banks might not be separable. In that case designing of filter bank gets complex. In most cases we deal with non-separable systems.\n\n[[File:2D Filter Bank.jpg|thumb|right|2D Filter Bank]]\nA filter bank consists of an analysis stage and a synthesis stage. Each stage consists of a set of filters in parallel. The filter bank design is the design of the filters in the analysis and synthesis stages. The analysis filters divide the signal into overlapping or non-overlapping subbands depending on the application requirements. The synthesis filters should be designed to reconstruct the input signal back from the subbands when the outputs of these filters are combined together. Processing is typically performed after the analysis stage. These filter banks can be designed as [[Infinite impulse response]] (IIR) or [[Finite impulse response]] (FIR).\nIn order to reduce the data rate, downsampling and upsampling are performed in the analysis and synthesis stages, respectively.\n\n===Existing approaches===\nBelow are several approaches on the design of multidimensional filter banks. For more details, please check the '''ORIGINAL''' references.\n\n===2-Channel multidimensional perfect reconstruction (PR) filter banks===\n\nIn real life, we always want to reconstruct the divided signal back to the original one, which makes PR filter banks very important.\nLet H('''z''') be the transfer function of a filter. The size of the filter is defined as the order of corresponding polynomial in every dimension.  The symmetry or anti-symmetry of a polynomial determines the linear phase property of the corresponding filter and is related to its size.\nLike the 1D case, the aliasing term A(z) and transfer function T(z) for a 2 channel filter bank are:<ref>Zhang, Lei, and Anamitra Makur. \"Multidimensional perfect reconstruction filter banks: an approach of algebraic geometry.\" Multidimensional Systems and Signal Processing. Volume 20 Issue 1, pp.&nbsp;3–24. Mar. 2009</ref>\n\nA('''z''')=1/2(H<sub>0</sub>(-'''z''') F<sub>0</sub> ('''z''')+H<sub>1</sub> (-'''z''') F<sub>1</sub> ('''z'''));\nT('''z''')=1/2(H<sub>0</sub> ('''z''') F<sub>0</sub> ('''z''')+H<sub>1</sub> ('''z''') F<sub>1</sub> ('''z''')),\nwhere H<sub>0</sub> and H<sub>1</sub> are decomposition filters, and F<sub>0</sub> and F<sub>1</sub> are reconstruction filters.\n\nThe input signal can be perfectly reconstructed if the alias term is cancelled and T('''z''') equal to a monomial. So the necessary condition is that T'('''z''') is generally symmetric and of an odd-by-odd size.\nLinear phase PR filters are very useful for image processing.  This 2-Channel filter bank is relatively easy to implement. But 2 channels sometimes are not enough for use. 2-channel filter banks can be cascaded to generate multi-channel filter banks.\n\n===Multidimensional directional filter banks and surfacelets===\n[[File:Multidimensional Analysis Filter Banks.jpg|thumb|right|Multidimensional Analysis Filter Banks]]\n\nM-dimensional directional filter banks (MDFB) are a family of filter banks that can achieve the directional decomposition of arbitrary M-dimensional signals with a simple and efficient tree-structured construction. It has many distinctive properties like: directional decomposition, efficient tree construction, angular resolution and perfect reconstruction.\nIn the general M-dimensional case, the ideal frequency supports of the MDFB are hypercube-based hyperpyramids.  The first level of decomposition for MDFB is achieved by an N-channel undecimated filter bank, whose component filters are M-D \"hourglass\"-shaped filter aligned with the w<sub>1</sub>,...,w<sub>M</sub> respectively axes. After that, the input signal is further decomposed by a series of 2-D iteratively resampled checkerboard filter banks ''IRC''<sub>''li''</sub><sup>(''Li'')</sup>(i=2,3,...,M), where ''IRC''<sub>''li''</sub><sup>(''Li'')</sup>operates on 2-D slices of the input signal represented by the dimension pair (n<sub>1</sub>,n<sub>i</sub>) and superscript (Li) means the levels of decomposition for the ith level filter bank. Note that, starting from the second level, we attach an IRC filter bank to each output channel from the previous level, and hence the entire filter has a total of 2<sup>(''L''<sub>1</sub>+...+''L''<sub>N</sub>)</sup> output channels.<ref>Lu, Yue M., and Minh N. Do. \"Multidimensional directional filter banks and surfacelets\", IEEE Transactions on Image Processing. Volume 16 Issue 4, pp.&nbsp;918–931. April, 2007</ref>\n\n===Multidimensional oversampled filter banks===\n\n[[File:Multidimensional Synthesis Filter Banks.jpg|thumb|right|Multidimensional Synthesis Filter Banks]]\n\nOversampled filter banks are multirate filter banks where the number of output samples at the analysis stage is larger than the number of input samples. It is proposed for robust applications. One particular class of oversampled filter banks is nonsubsampled filter banks without downsampling or upsampling. The perfect reconstruction condition for an oversampled filter bank can be stated as a matrix inverse problem in the polyphase domain.<ref name=\"Zhou\">J. Zhou and M. N. Do, \"Multidimensional oversampled filter banks\" in Proc. SPIE Conf. Wavelet Applications Signal Image Processing XI, San Diego, CA, pp.&nbsp;591424–1-591424-12, July 2005</ref>\n\nFor IIR oversampled filter bank, perfect reconstruction have been studied in Wolovich<ref>\nWolovich, William A. Linear multivariable systems. New York: Springer-Verlag, 1974.\n</ref> and Kailath.<ref>\nKailath, Thomas. Linear systems. Vol. 1. Englewood Cliffs, NJ: Prentice-Hall, 1980.\n</ref>\nin the context of control theory. While for FIR oversampled filter bank we have to use different strategy for 1-D and M-D.\nFIR filter are more popular since it is easier to implement. For 1-D oversampled FIR filter banks, the Euclidean algorithm plays a key role in the matrix inverse problem.<ref>\nCvetkovic, Zoran, and Martin Vetterli. \"Oversampled filter banks\" IEEE Transactions on Signal Processing, Vol.46 Issue 5, pp.&nbsp;1245–1255. May, 1998.\n</ref>\nHowever, the Euclidean algorithm fails for multidimensional (MD) filters. For MD filter, we can convert the FIR representation into a polynomial representation.<ref name=\"Charo\"/>  And then use [[Algebraic geometry]] and Gröbner bases to get the framework and the reconstruction condition of the multidimensional oversampled filter banks.<ref name=\"Zhou\"/>\n\n=== Multidimentional nonsubsampled FIR filter banks===\n\nNonsubsampled filter banks are particular oversampled filter banks without downsampling or upsampling.\nThe perfect reconstruction condition for nonsubsampled FIR filter banks leads to a vector inverse problem: the\nanalysis filters <math>\\{H_{1},...,H_{N}\\}</math> are given and FIR, and the goal is to find a set of FIR synthesis filters <math>\\{G_{1},...,G_{N}\\}</math> satisfying.<ref name=\"Zhou\"/>\n\n===Using [[Gröbner Basis]]===\n\n[[File:Multidimensional M channel Filter Banks.jpg|thumb|right|Multidimensional M_channel Filter Banks]]\n\nAs multidimensional filter banks can be represented by multivariate rational matrices, this method is a very effective tool that can be used to deal with the multidimensional filter banks.<ref name=\"Charo\">Charoenlarpnopparut, Chalie, and N. K. Bose. \"Multidimensional FIR filter bank design using Gröbner bases\" IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing, Volume 46 Issue 12, pp.&nbsp;1475–1486, Dec, 1999</ref>\n\nIn Charo,<ref name=\"Charo\"/> a multivariate polynomial matrix-factorization algorithm is introduced and discussed. The most common problem is the multidimensional filter banks for perfect reconstruction. This paper talks about the method to achieve this goal that satisfies the constrained condition of linear phase.\n\nAccording to the description of the paper, some new results in factorization are discussed and being applied to issues of multidimensional linear phase perfect reconstruction finite-impulse response filter banks. The basic concept of Gröbner Bases is given in Adams.<ref>Adams, William W., and Philippe Loustaunau. \"An introduction to Gröbner bases, volume 3 of [[Graduate Studies in Mathematics]]\" American Mathematical Society, Providence, RI 24(47), 1994.</ref>\n\nThis approach based on multivariate matrix factorization can be used in different areas. The algorithmic theory of polynomial ideals and modules can be modified to address problems in processing, compression, transmission, and decoding of multidimensional signals.\n\nThe general multidimensional filter bank (Figure 7) can be represented by a pair of analysis and synthesis polyphase matrices <math>H(z)</math> and <math>G(z)</math> of size <math>N\\times M\n</math> and <math>M\\times N</math>,  where ''N'' is the number of channels and <math>M\\stackrel{\\rm def}{=}|M|\n</math> is the absolute value of the determinant of the sampling matrix. Also <math>H(z)</math> and <math>G(z)</math> are the z-transform of the polyphase components of the analysis and synthesis filters. Therefore, they are ''multivariate Laurent polynomials'', which have the general form:\n\n:<math>F(z)=\\sum_{k\\in \\mathbf{Z}^{d}}f[k]z^{k}=\\sum_{k\\in \\mathbf{Z}^{d}}f[k_{1},...,k_{d}]z_{1}^{k_{1}}...z_{d}^{k_{d}}</math>.\n\nLaurent polynomial matrix equation need to be solve to design perfect reconstruction filter banks:\n\n:<math>G(z)H(z)=I_{|M|}</math>.\n\nIn the multidimentonal case with multivariate polynomials we need to use the theory and algorithms of ''[[Gröbner bases]]''.<ref>{{cite journal|date=1985|title=An algorithmic method in polynomial ideal theory|url=|journal=Multidimensional systems theory|pages=|via=|last1=Buchberger|first1=Bruno}}</ref>\n\nGröbner bases can be used to characterizing perfect reconstruction multidimensional filter banks, but it first need to extend from polynomial matrices to [[Laurent polynomial]] matrices.<ref>{{cite journal|author=Park, Hyungju|author2=Kalker, Ton|last-author-amp=yes|author3=Vetterli, Martin|title=Gröbner bases and multidimensional FIR multirate systems|journal=Multidimensional Systems and Signal Processing|date=1997|issue=Springer}}</ref><ref>{{cite journal|last1=Hyung-Ju|first1=Park|title=A computational theory of Laurent polynomial rings and multidimensional FIR systems|date=1995|issue=University of California}}</ref>\n\nThe Gröbner basis computation can be considered equivalently as Gaussian elimination for solving the polynomial matrix equation  <math>G(z)H(z)=I_{|M|}</math>.\nIf we have set of polynomial vectors\n\n:<math>\\mathrm{Module}\\left\\{ h_{1}(z),...,h_{N}(z)\\right\\} \\stackrel{\\rm def}{=}\\{c_{1}(z)h_{1}(z)+...+c_{N}(z)h_{N}(z)\\}</math>\n\nwhere <math>c_{1}(z),...,c_{N}(z)</math> are polynomials.\n\nThe Module is analogous to the ''span'' of a set of vectors in linear algebra. The theory of Gröbner bases implies that the Module has a unique reduced Gröbner basis for a given order of power products in polynomials.\n\nIf we define the Gröbner basis as <math>\\left\\{ b_{1}(z),...,b_{N}(z)\\right\\}</math>, it can be\nobtained from <math>\\left\\{ h_{1}(z),...,h_{N}(z)\\right\\} </math> by a finite sequence of reduction\n(division) steps.\n\nUsing reverse engineering, we can compute the basis vectors <math>b_{i}(z)</math> in terms of the original vectors <math>h_{j}(z)</math> through a <math>K\\times N</math> transformation matrix <math>W_{ij}(z)</math> as:\n\n:<math>b_{i}(z)=\\sum_{j=1}^{N}W_{ij}(z)h_{j}(z),i=1,...,K</math>\n\n===Mapping-based multidimensional filter banks===\nDesigning filters with good frequency responses is challenging via Gröbner bases approach.<br />\nMapping based design in popularly used to design nonseparable multidimensional filter banks with good frequency responses.<ref>{{cite journal|last1=McClellan|first1=James|title=The design of two-dimensional digital filters by transformations|journal=Proc. 7th Annu. Princeton Conf. Information Sciences and Systems|date=1973}}</ref><ref>{{cite journal|last1=Kovacevic, Vetterli|first1=Jelena, Martin|title=Nonseparable multidimensional perfect reconstruction filter banks and wavelet bases for R^n|journal=IEEE Transactions on Information Theory|date=1992|issue=Institute of Electrical and Electronics Engineers|doi= 10.1109/18.119722 }}</ref>\n\nThe mapping approaches have certain restrictions on the kind of filters; However, It brings many important advantages, such as efficient implementation via lifting/ladder structures.\nHere we provide an example of two-channel filter banks in 2D with sampling matrix <br />\n<math>D_{1}=\\left[\\begin{array}{cc}\n2 & 0\\\\\n0 & 1\n\\end{array}\\right]</math>\nWe would have several possible choices of ideal frequency responses of the channel filter <math>H_{0}(\\xi)\n</math> and <math>G_{0}(\\xi)</math>. (Note that the other two filters <math>H_{1}(\\xi)\n</math> and <math>G_{1}(\\xi)</math> are supported on complementary regions.)<br />\nAll the frequency regions in Figure can be critically sampled by the rectangular lattice spanned by <math>D_1</math>.\n<br />\nSo imagine the filter bank achieves perfect reconstruction\nwith FIR filters. Then from the polyphase domain characterization it follows that the filters H1(z) and G1(z) are completely\nspecified by H0(z) and G0(z), respectively. Therefore, we need to design H0(x) and G0(z) which have desired frequency responses and satisfy the polyphase-domain conditions.\n<math>H_{0}(z_{1},z_{2})G_{0}(z_{1},z_{2})+H_{0}(-z_{1},z_{2})G_{0}(-z_{1},z_{2})=2</math><br />\nThere are different mapping technique that can be used to get above result.<ref>Tay, David BH, and Nick G. Kingsbury. \"Flexible design of multidimensional perfect reconstruction FIR 2-band filters using transformations of variables.\" Image Processing, IEEE Transactions on 2, no. 4 (1993): 466-480.</ref>\n\n===Filter banks design in the frequency domain===\nIf we do not want perfect reconstruction filter banks using FIR filters, the design problem can be simplified by working in frequency domain\ninstead of using FIR filters.<ref>Laligant, Olivier, and Frederic Truchetet. \"Discrete wavelet transform implementation in Fourier domain for multidimensional signal.\" Journal of Electronic Imaging 11.3 (2002): 338-346.</ref><ref>Woiselle, Arnaud, J-L. Starck, and J. Fadili. \"3D curvelet transforms and astronomical data restoration.\" Applied and Computational Harmonic Analysis 28.2 (2010): 171-188.</ref><br />\nNote that the frequency domain method is not limited to the design of nonsubsampled filter banks (read <ref>Feilner, Manuela, Dimitri Van De Ville, and Michael Unser. \"An orthogonal family of quincunx wavelets with continuously adjustable order.\" Image Processing, IEEE Transactions on 14.4 (2005): 499-510.</ref>).\n\n===Direct frequency-domain optimization===\n\nMany of the existing methods for designing 2-channel filter banks are based on transformation of variable technique. For example, McClellan transform can be used to design 1-D 2-channel filter banks. Though the 2-D filter banks have many similar properties with the 1-D prototype, but it is difficult to extend to more than 2-channel cases.<ref name=\"Nguyen\">\nNguyen, Truong T., and Soontorn Oraintara. \"Multidimensional filter banks design by direct optimization\" IEEE International Symposium onCircuits and Systems, pp.&nbsp;1090–1093. May, 2005.\n</ref>\n\nIn Nguyen,<ref name=\"Nguyen\"/> the authors talk about the design of multidimensional filter banks by direct optimization in the frequency domain. The method proposed here is mainly focused on the M-channel 2D filter banks design. The method is flexible towards frequency support configurations. 2D filter banks designed by optimization in the frequency domain has been used in Wei<ref>D. Wei and S. Guo, \"A new approach to the design of multidimensional nonseparable two-channel orthonormal filterbanks and wavelets\", IEEE Signal Processing Letters, vol. 7, no. 11, pp.&nbsp;327–330, Nov 2000.</ref> and Lu.<ref>W.-S. Lu, A. Antoniou, and H. Xu, \"A direct method for the design of 2-D nonseparable diamond-shaped filter banks\", IEEE Transactions on Circuits and Systems II, vol. 45, no. 8, pp.&nbsp;1146–1150, Aug 1998.</ref>  In Nguyen's paper,<ref name=\"Nguyen\"/> the proposed method is not limited to two-channel 2D filter banks design; the approach is generalized to M-channel filter banks with any critical subsampling matrix.  According to the implementation in the paper, it can be used to achieve up to 8-channel 2D filter banks design.\n\n'''(6)'''Reverse Jacket Matrix<ref name=\"Lee\">Lee, Moon Ho, and Ju Yong Park. \"The design of multidimensional filter bank using Reverse Jacket matrix\", TENCON 99. Proceedings of the IEEE Region 10 Conference. Vol.1 pp.&nbsp;637–641, Conference in 1999.</ref>\n\nIn Lee's 1999 paper,<ref name=\"Lee\"/> the authors talk about the multidimensional filter bank design using Reverse [[Jacket matrix]]. According to Wiki article, let ''H'' be a [[Hadamard matrix]] of order ''n'', the transpose of ''H'' is closely related to its inverse. The correct formula is: <math>HH^T=I_n</math>, where I<sub>n</sub> is the n×n identity matrix and ''H<sup>T</sup>'' is the transpose of ''H''. In the 1999 paper,<ref name=\"Lee\"/> the authors generalize the Reverse Jacket matrix [RJ]<sub>N</sub> using Hadamard matrices and Weighted Hadamard matrices.<ref>Lee, Seung-Rae, and Moon Ho Lee. \"On the Reverse Jacket matrix for weighted Hadamard transform.\" IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing,  Vol. 45 Issue 3, pp.&nbsp;436–441. Mar, 1998.</ref><ref>Moon Ho Lee, \"A New Reverse Jacket Matrix and Its Fast Algorithm\", Accepted IEEE Trans. on CAS-II, pp.&nbsp;39–47, Jan,2000.</ref>\n\nIn this paper, the authors proposed that the FIR filter with 128 tap is used as a basic filter and decimation factor is computed for RJ matrices. They did simulations based on different parameters and achieve a good quality performances in low decimation factor.\n\n==Directional filter banks==\n\nBamberger and Smith proposed a 2D directional filter bank (DFB).<ref>Bamberger, Roberto H., and Mark JT Smith. \"A filter bank for the directional decomposition of images: Theory and design.\" IEEE Transactions, Signal Processing 40.4 (1992): 882-893.</ref>\nThe DFB is efficiently implemented via an ''l''-level tree-structured decomposition that leads to <math>2^{l}</math> subbands with wedge-shaped frequency partition (see Figure ).\nThe original construction of the DFB involves modulating the input signal and using diamond-shaped filters.\nMoreover, in order to obtain the desired frequency partition, a complicated tree expanding rule has to be followed.<ref>{{cite journal|author=Park, Sang-Il|author2=Smith, Mark JT|last-author-amp=yes|author3=Mersereau, Russell M|title=A new directional filter bank for image analysis and classification|journal=IEEE International Conference, Acoustics, Speech, and Signal Processing, 1999. Proceedings., 1999 |date=1999|issue=IEEE}}</ref> As a result, the frequency regions\nfor the resulting subbands do not follow a simple ordering as shown in Figure 9 based on the channel indices.\n\nThe first advantage of DFB is that not only it is not a redundant transform but also it offers perfect reconstruction.\nAnother advantage of DFB is its directional-selectivity and efficient structure.\nThis advantage makes DFB an appropriate approach for many signal and image processing usage. (e.g., Laplacian pyramid, constructed the contourlets,<ref>Do, Minh N., and Martin Vetterli. \"The contourlet transform: an efficient directional multiresolution image representation.\" Image Processing, IEEE Transactions on 14.12 (2005): 2091-2106.</ref> sparse image representation, medical imageing<ref>Truc, Phan TH, et al. \"Vessel enhancement filter using directional filter bank.\" Computer Vision and Image Understanding 113.1 (2009): 101-112.</ref> etc.).\n\nDirectional Filter Banks can be developed to higher dimensions. It can be use in 3-D to achieve the frequency sectioning.\n\n==Conclusion and application==\n\nFilter banks play important roles in signal processing.\nThey are used in many areas, such as signal and image compression, and processing.\nThe main use of [[filter banks]] is to divide a signal or system in to several separate frequency domains. Different filter designs can be used depending on the purpose.\nIn this page we provide information regarding filter banks, multidimensional filter banks and different methods to design multidimensional filters.\nAlso we talked about NDFB, which is built upon an efficient tree-structured construction, which leads to a low redundancy ratio and refinable angular resolution.\nBy combining the NDFB with a new multiscale pyramid, we can construct the surfacelet transform, which has potential in efficiently capturing and representing surface-like singularities in multidimensional signals.\nAS mentioned above NDFB and surfacelet transform have applications in various areas that involve the processing of multidimensional volumetric data, including video processing, seismic image processing, and medical image analysis.\nSome other advantages of NDFB can be addressed as follow:\n'''Directional decomposition''', '''Construction''', '''Angular resolution''', '''Perfect reconstruction''', and '''Small redundancy'''.\n\n==Notes==\n{{reflist|group=note}}\n\n===Citations===\n{{reflist}}\n\n===Further reading===\n*{{Cite book\n | author=Harris, Fredric J.\n \n \n | title=Multirate signal processing for communication systems\n | year=2004\n | publisher=Prentice Hall PTR\n | location=Upper Saddle River, NJ\n | isbn=0-13-146511-2\n | pages=\n}}\n\n[[Category:Digital signal processing]]\n[[Category:Linear filters]]\n[[Category:Wavelets]]"
    },
    {
      "title": "Fractional wavelet transform",
      "url": "https://en.wikipedia.org/wiki/Fractional_wavelet_transform",
      "text": "'''Fractional wavelet transform''' (FRWT) is a generalization of the classical [[wavelet transform]] (WT). This transform is proposed in order to rectify the limitations of the WT and the [[fractional Fourier transform]] (FRFT). The FRWT inherits the advantages of [[multiresolution analysis]] of the WT and has the capability of signal representations in the fractional domain which is similar to the FRFT.\n\n==Definition==\nThe fractional Fourier transform (FRFT)<ref>H. M. Ozaktas, Z. Zalevsky, and M. A. Kutay, The Fractional Fourier Transform with Applications in Optics and Signal Processing. Wiley, New York, 2000.</ref>，a generalization of the Fourier transform (FT),\nserves a useful and powerful analyzing tool<ref>E. Sejdic, I. Djurovic, and L. Stankovic, \"Fractional Fourier transform as a signal processing tool: An overview of recent developments,\" Signal Process., vol. 91, pp. 1351--1369, 2011.</ref> in optics, communications, signal and image processing, etc.\nThis transform, however, has one major drawback due to using global kernel, i.e., the fractional Fourier representation only provides such FRFT spectral content with no indication about the time localization of the FRFT spectral components.\nTherefore, the analysis of non-stationary signals whose FRFT spectral characteristics change with time requires joint signal representations in both time and FRFT domains, rather than just a FRFT-domain representation.\n\nThe first modification to the FRFT to allow analysis of aforementioned non-stationary signals came as the short-time FRFT (STFRFT).<ref>L. Stankovic, T. Alieva, and M. J. Bastiaans, \"Time-frequency signal analysis based on the windowed fractional Fourier transform,\"Signal Process., vol. 83, pp. 2459--2468, 2003.</ref><ref>R. Tao, Y. Lei, and Y. Wang, \"Short-time fractional Fourier transform and its applications,\" IEEE Trans. Signal Process., vol. 58, pp. 2568--2580, 2010.</ref> The idea behind the STFRFT was segmenting the signal by using a time-localized window, and performing FRFT spectral analysis for each segment.\nSince the FRFT was computed for every windowed segment of the signal, the STFRFT was able to provide a true joint signal representation in both time and FRFT domains.\nHowever, the drawback is that the STFRFT has the limitation of a fixed window width which needs to be fixed a priori; this effectively means that it does not provide the requisite good resolution in both time and FRFT domains. In other words, the efficiency of the STFRFT techniques is limited by the fundamental uncertainty principle,<ref>J. Shi, X.-P. Liu, and N.-T. Zhang, \"On uncertainty principle for signal concentrations with fractional Fourier transform,\" Signal Process., vol. 92, pp. 2830--2836, 2012.</ref> which implies that narrow windows produce good time resolution but poor spectral resolution, whereas wide windows provide good spectral resolution but poor time resolution. Most of the signals of practical interest are such that they have high spectral components for short durations and low spectral components for long durations.\n\nAs a generalization of the wavelet transform, Mendlovic and David<ref>D. Mendlovic, Z. Zalevsky, D. Mas, J. Garcia, and C. Ferreira, \"Fractional wavelet transform,\" Appl. Opt., vol. 36, pp. 4801--4806, 1997.</ref> first introduced the fractional wavelet transform (FRWT) as a way to deal with optical signals, which was defined as a cascade of the FRFT and the ordinary wavelet transform (WT), i.e., \n:<math>\n\\begin{align}\nW^{\\alpha}(a,b)&=\\frac{1}{\\sqrt{a}}\\int\\limits_{\\mathbb{R}}\\int\\limits_{\\mathbb{R}}{\\mathcal K}_\\alpha(u,t)f(t)\\psi^{\\ast}\\left(\\frac{u-b}{a}\\right)dtdu\\\\\n&=\\frac{1}{\\sqrt{a}}\\int\\limits_{\\mathbb{R}}\\left(\\int\\limits_{\\mathbb{R}}f(t){\\mathcal K}_\\alpha(u,t)dt\\right)\\psi^{\\ast}\\left(\\frac{u-b}{a}\\right)du\\\\\n&=\\frac{1}{\\sqrt{a}}\\int\\limits_{\\mathbb{R}}F_{\\alpha}(u)\\psi^{\\ast}\\left(\\frac{u-b}{a}\\right)du\\\\\n\\end{align}\n</math>\nwhere the transform kernel <math>\\mathcal{K}_{\\alpha}(u,t)</math> is given by\n:<math>\n\\mathcal{K}_{\\alpha}(u,t)=\n  \\begin{cases}\n  A_{\\alpha}e^{j\\frac{u^2+t^2}{2}\\cot\\alpha-jut\\csc\\alpha},&\\alpha \\neq k\\pi\\\\\n  \\delta(t-u),&\\alpha = 2k\\pi \\\\\n  \\delta(t+u),&\\alpha = (2k-1)\\pi \\\\\n  \\end{cases}\n</math>\nwhere <math>A_\\alpha=\\sqrt{{(1-j\\cot\\alpha)}/{2\\pi}}</math>, and <math>F_{\\alpha}(u)</math> denotes the FRFT of <math>f(t)</math>. But it could not be regarded as a kind of joint time-FRFT-domain representation since time information is lost in this transform. Moreover, Prasad and Mahato<ref>A. Prasad and A. Mahato, \"The fractional wavelet transform on spaces of type S,\" Integral Transform Spec. Funct., vol. 23, no. 4, pp. 237--249, 2012.</ref> expressed the ordinary WT of a signal in terms of the FRFTs of the signal and mother wavelet, and also called the expression the FRWT. That is,\n:<math>\n\\begin{align}\n\\left(W_{\\psi}^{\\alpha}f\\right)(b,a)&=\\frac{1}{\\sqrt{a}}\\int\\limits_{\\mathbb{R}}f(t)\\psi^{\\ast}\\left(\\frac{t-b}{a}\\right)dt\\\\\n&=\\frac{\\csc\\alpha}{4\\pi^2}\\int\\limits_{\\mathbb{R}}F(u\\sin\\alpha)\\Psi^{\\ast}(au\\sin\\alpha)e^{j\\frac{u^2}{4}(1-a^2)\\sin2\\alpha-jbu}du\\\\\n\\end{align}\n</math>\nwhere <math>F(u\\sin\\alpha)</math> and <math>\\Psi(u\\sin\\alpha)</math> denote the FTs (with their arguments scaled by <math>\\sin\\alpha</math>) <math>f(t)</math> and <math>\\psi(t)</math>, respectively. Clearly, this so-called FRWT is identical to the ordinary WT.\n\nRecently, Shi et al. proposed a new definition<ref>{{cite journal | last1 = Shi | first1 = J. | last2 = Zhang | first2 = N.-T. | last3 = Liu | first3 = X.-P. | year = 2011| title = A novel fractional wavelet transform and its applications | url = http://www.springerlink.com/content/q01np2848m388647/ | journal = Sci. China Inf. Sci. | volume = 55 | issue = 6| pages = 1270–1279 | doi=10.1007/s11432-011-4320-x}}</ref> of the FRWT by introducing a new structure of the fractional convolution<ref>{{cite journal | last1 = Shi | first1 = J. | last2 = Chi | first2 = Y.-G. | last3 = Zhang | first3 = N.-T. | year = 2010 | title = Multichannel sampling and reconstruction of bandlimited signals in fractional Fourier domain | url = | journal = IEEE Signal Process. Lett. | volume = 17 | issue = 11| pages = 909–912 | doi=10.1109/lsp.2010.2071383}}</ref> associated with the FRFT. \nSpecifically, the FRWT of any function  <math>f(t)\\in L^{2}(\\mathbb{R})</math> is defined as [8]\n:<math>\nW_{f}^{\\alpha}(a,b)=\\mathcal{W}^{\\alpha}[f(t)](a,b)=\\int\\limits_{\\mathbb{R}} f(t)\\psi_{\\alpha,a,b}^{\\ast}(t)\\, dt\n</math>\nwhere <math>\\psi_{\\alpha,a,b}(t)</math> is a continuous affine transformation and chirp modulation of the mother wavelet <math>\\psi(t)</math>, i.e.,\n:<math>\n\\psi_{\\alpha,a,b}(t)=\\frac{1}{\\sqrt{a}}\\psi\\left(\\frac{t-b}{a}\\right)e^{-j\\frac{t^2-b^2}{2}\\cot\\alpha}\n</math>\nin which <math>a\\in \\mathbb{R^+}</math> and <math>b\\in \\mathbb{R}</math> are scaling and translation parameters, respectively.\nConversely, the inverse FRWT is given by\n:<math>\nf(t)=\\frac{1}{2\\pi C_{\\psi}}\\int\\limits_{\\mathbb{R}}\\int\\limits_{\\mathbb{R}^{+}}W_{f}^{\\alpha}(a,b)\\psi_{\\alpha,a,b}(t)\\frac{da}{a^2}db\n</math>\nwhere <math>C_{\\psi}</math> is a constant that depends on the wavelet used. The success of the reconstruction depends on this constant called, the admissibility constant, to satisfy the following admissibility condition:\n:<math>\nC_{\\psi}=\\int\\limits_{\\mathbb{R}}{\\frac{|\\Psi(\\Omega)|^2}{|\\Omega|}}\\,d\\Omega<\\infty\n</math>\nwhere <math>\\Psi(\\Omega)</math> denotes the FT of <math>\\psi(t)</math>. The admissibility condition implies that <math>\\Psi(0)=0</math>, which is <math>\\int_{\\mathbb{R}}\\psi(t)dt=0</math>. Consequently, continuous fractional wavelets must oscillate and behave as bandpass filters in the fractional Fourier domain. From this viewpoint, the FRWT of <math>f(t)</math> can be expressed in terms of the FRFT-domain representation as\n:<math>\nW_{f}^{\\alpha}(a,b)=\\int\\limits_{\\mathbb{R}}{\\sqrt{2\\pi a}F_{\\alpha}(u)\\Psi^{\\ast}(au\\csc\\alpha)}\\mathcal{K}^{\\ast}_{\\alpha}(u,b)du\n</math>\nwhere <math>F_{\\alpha}(u)</math> indicates the FRFT of <math>f(t)</math>, and <math>\\Psi(u\\csc\\alpha)</math> denotes the FT (with its argument scaled by <math>\\csc\\alpha</math>) of <math>\\psi(t)</math>. \nNote that when <math>\\alpha={\\pi}/{2}</math>, the FRWT reduces to the classical WT. For more details of this type of the FRWT, see [8] and.<ref>L. Debnath and F. A. Shah, Wavelet Transforms and Their Applications, 2nd Edition, 2015, pp.14-15. URL: https://www.springer.com/cn/book/9780817684174/</ref>\n\n==Multiresolution Analysis (MRA) Associated with Fractional Wavelet Transform==\n\nA comprehensive overview of MRA and orthogonal fractional wavelets associated with the FRWT can be found in the paper.<ref>{{cite journal | last1 = Shi | first1 = J. | last2 = Liu | first2 = X.-P. | last3 = Zhang | first3 = N.-T. | year = 2015 | title = Multiresolution analysis and orthogonal wavelets associated with fractional wavelet transform | journal = Signal, Image, Video Process | volume = 9 | issue = 1| pages = 211–220 | doi=10.1007/s11760-013-0498-2}}</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Wavelets]]\n[[Category:Fourier analysis]]"
    },
    {
      "title": "Gabor atom",
      "url": "https://en.wikipedia.org/wiki/Gabor_atom",
      "text": "In applied [[mathematics]], '''Gabor atoms''', or '''Gabor functions''', are [[function (mathematics)|functions]] used in the analysis proposed by [[Dennis Gabor]] in 1946 in which a family of functions is built from translations and modulations of a generating function.\n\n==Overview==\nIn 1946,<ref>{{Cite journal|last=Gabor|first=D.|title=Theory of communication. Part 1: The analysis of information|journal=Journal of the Institution of Electrical Engineers - Part III: Radio and Communication Engineering|volume=93|issue=26|pages=429–441|doi=10.1049/ji-3-2.1946.0074|year=1946}}</ref> [[Dennis Gabor]] suggested the idea of using a granular system to produce [[sound]]. In his work, Gabor discussed the problems with [[Fourier analysis]]. Although he found the mathematics to be correct, it did not reflect the behaviour of sound in the world, because sounds, such as the sound of a siren, have variable frequencies over time. Another problem was the underlying supposition, as we use sine waves analysis, that the signal under concern has infinite duration even though sounds in real life have limited duration – see [[time–frequency analysis]]. Gabor applied ideas from [[quantum physics]] to sound, allowing an analogy between sound and quanta. He proposed a mathematical method to reduce Fourier analysis into cells. His research aimed at the information transmission through communication channels. Gabor saw in his atoms a possibility to transmit the same information but using less data. Instead of transmitting the signal itself it would be possible to transmit only the coefficients which represent the same signal using his atoms.\n\n==Mathematical definition==\nThe Gabor function is defined by\n\n:<math>g_{\\ell,n}(x) = g(x - a\\ell)e^{2\\pi ibnx}, \\quad -\\infty < \\ell,n < \\infty,</math>\n\nwhere ''a'' and ''b'' are constants and ''g'' is a fixed function in [[Square-integrable function|''L''<sup>2</sup>('''R''')]], such that ||''g''||&nbsp;=&nbsp;1.  Depending on <math>a</math>, <math>b</math>, and <math>g</math>, a Gabor system may be a basis for ''L''<sup>2</sup>('''R'''), which is defined by translations and modulations.  This is similar to a wavelet system, which may form a basis through dilating and translating a mother wavelet.\n\n==See also==\n*[[Gabor filter]]\n*[[Gabor wavelet]]\n*[[Fourier analysis]]\n*[[Wavelet]]\n*[[Morlet wavelet]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n*Hans G. Feichtinger, Thomas Strohmer: \"Gabor Analysis and Algorithms\", Birkhäuser, 1998;   {{ISBN|0-8176-3959-4}}\n*Hans G. Feichtinger, Thomas Strohmer: \"Advances in Gabor Analysis\",  Birkhäuser,  2003; {{ISBN|0-8176-4239-0}}\n*Karlheinz Gröchenig:  \"Foundations of Time-Frequency Analysis\", Birkhäuser,  2001; {{ISBN|0-8176-4022-3}}\n\n==External links==\n*[http://www.nuhag.eu NuHAG homepage] [Numerical Harmonic Analysis Group]\n\n{{DEFAULTSORT:Gabor Atom}}\n[[Category:Wavelets]]\n[[Category:Fourier analysis]]"
    },
    {
      "title": "Gabor wavelet",
      "url": "https://en.wikipedia.org/wiki/Gabor_wavelet",
      "text": "'''Gabor wavelets''' are [[wavelet]]s invented by [[Dennis Gabor]] using complex functions constructed to serve as a basis for [[Fourier transform]]s in [[information theory]] applications. They are very similar to [[Morlet wavelet]]s. They are also closely related to [[Gabor filter#Wavelet space| Gabor filters]]. The important property of the [[wavelet]] is that it minimizes the product of its standard deviations in the time and frequency domain. Put another way, the [[Heisenberg's uncertainty principle|uncertainty]] in information carried by this wavelet is minimized. However they have the downside of being non-orthogonal, so efficient decomposition into the basis is difficult. Since their inception, various applications have appeared, from image processing to analyzing neurons in the human visual system.\n<ref name=\"imageRep\">{{Cite journal \n|last1=Lee \n|first1=Tai S. \n|url=http://www.cnbc.cmu.edu/~tai/papers/pami.pdf\n|title=Image Representation Using 2D Gabor wavelets \n|journal=IEEE Transactions on Pattern Analysis and Machine Intelligence \n|volume=18 \n|pages=959–971 \n|number=10 \n|date=October 1996 \n|doi=10.1109/34.541406\n}}</ref>\n<ref name=\"CVRef\">{{cite book\n | last = Daugman\n | first = John\n | title = Computer Vision Lecture Series\n | publisher = University of Cambridge\n | url =http://www.cl.cam.ac.uk/teaching/1314/CompVision/CompVisNotes2014.pdf\n}}</ref>\n\n== Minimal uncertainty property ==\n\nThe motivation for Gabor wavelets comes from finding some function <math> f(x) </math> which minimizes its standard deviation in the time and frequency domains. More  formally, the variance in the position domain is:\n\n:<math> (\\Delta x)^2 = \\frac {\\int_{-\\infty}^{\\infty} (x-\\mu)^2 f(x)f^{*}(x) \\,dx} {\\int_{-\\infty}^{\\infty} f(x)f^{*}(x) \\, dx} </math>\n\nwhere <math>f^{*}(x)</math> is the complex conjugate of <math> f(x) </math> and <math> \\mu </math> is the arithmetic mean, defined as:\n\n:<math> \\mu = \\frac {\\int_{-\\infty}^{\\infty} x f(x)f^{*}(x) \\,dx} {\\int_{-\\infty}^{\\infty} f(x)f^{*}(x)\\,dx} </math>\n\nThe variance in the [[wave number]] domain is:\n\n:<math> (\\Delta k)^2 = \\frac {\\int_{-\\infty}^{\\infty} (k-k_0)^2 F(k)F^{*}(k) \\, dk} {\\int_{-\\infty}^{\\infty} F(k)F^{*}(k) \\, dk} </math>\n\nWhere <math> k_0 </math> is the arithmetic mean of the Fourier Transform of <math> f(x) </math>, <math> F(x) </math>:\n\n:<math> k_0 = \\frac {\\int_{-\\infty}^{\\infty} k F(k)F^{*}(k) \\,dk} {\\int_{-\\infty}^{\\infty} F(k)F^{*}(k) \\,dk} </math>\n\nWith these defined, the uncertainty is written as:\n\n:<math> (\\Delta x)(\\Delta k)</math>\n\nThis quantity has been shown to have a lower bound of <math> \\frac12 </math>. The quantum mechanics view is to interpret <math> (\\Delta x) </math> as the uncertainty in position and <math> \\hbar (\\Delta k) </math> as uncertainty in momentum. A function <math> f(x) </math> that has the lowest theoretically possible uncertainty bound is the Gabor Wavelet.<ref name = \"infRef\">{{cite book\n | last = Daugman\n | first = John\n | title = Information Theory Lecture Series\n | publisher = University of Cambridge\n | url=http://www.cl.cam.ac.uk/Teaching/1314/InfoTheory/InfoTheoryNotes2013.pdf\n}}</ref>\n\n== Equation ==\n\nThe equation of a 1-D Gabor wavelet is a Gaussian modulated by a complex exponential, described as follows:<ref name=\"infRef\" />\n:<math> f(x) = e^{-(x - x_0)^2/a^2}e^{-i k_0(x-x_0)} </math>\nAs opposed to other functions commonly used as bases in Fourier Transforms such as <math> \\sin </math> and <math> \\cos </math>, Gabor wavelets have locality properties, meaning that as the distance from the center <math> x_0 </math> increases, the value of the function becomes exponentially suppressed. <math> a </math> controls the rate of this exponential drop-off and <math> k_0 </math> controls the rate of modulation.\n\nIt is also worth noting the Fourier transform of a Gabor wavelet, which is also a Gabor wavelet:\n:<math> F(k) = e^{-(k - k_0)^2 a^2}e^{-i x_0(k-k_0)} </math>\n\nAn example wavelet is given here:\n[[File:Gabor Wavelet, a=2, k=1.gif|frame|center|A Gabor wavelet with ''a''&nbsp;=&nbsp;2, ''x''<sub>0</sub>&nbsp;=&nbsp;0, and ''k''<sub>0</sub>&nbsp;=&nbsp;1]]\n\n==See also==\n* [[Gabor filter]]\n* [[Morlet wavelet]]\n\n== External links ==\n*[http://www.mathworks.com/matlabcentral/fileexchange/44630 MATLAB code for 2D Gabor wavelets and Gabor feature extraction]\n\n== References ==\n\n<ref name=\"imageRep\"/>\n\n{{reflist}}\n\n[[Category:Wavelets]]"
    },
    {
      "title": "Generalized lifting",
      "url": "https://en.wikipedia.org/wiki/Generalized_lifting",
      "text": "{{multiple issues|\n{{context|date=May 2017}}\n{{technical|date=May 2017}}\n{{notability|date=March 2009}}}}\n\n[[File:LiftingScheme.png|thumb|400px|alt=Lifting scheme|Block diagram of the (forward) lifting scheme transform]]\n\nThe '''generalized lifting scheme''' was developed by Joel Solé and Philippe Salembier and published in Solé's PhD dissertation.<ref>Ph.D. dissertation: ''[https://upcommons.upc.edu/bitstream/handle/2117/94208/04Jsr04de09.pdf Optimization and Generalization of Lifting Schemes: Application to Lossless Image Compression]''.</ref> It is based on the classical [[lifting scheme]] and generalizes it by breaking out a restriction hidden in the scheme structure. The classical lifting scheme has three kinds of operations:\n# A '''lazy wavelet transform''' splits signal <math>f_j[n]</math> in two new signals: the odd-samples signal denoted by <math>f_j^o[n]</math> and the even-samples signal denoted by <math>f_j^e[n]</math>.\n# A '''prediction step''' computes a prediction for the odd samples, based on the even samples (or vice versa). This prediction is subtracted from the odd samples, creating an error signal <math>g_{j+1}[n]</math>.\n# An '''update step''' recalibrates the low-frequency branch with some of the energy removed during subsampling. In the case of classical lifting, this is used in order to \"prepare\" the signal for the next prediction step. It uses the predicted odd samples <math>g_{j+1}[n]</math> to prepare the even ones <math>f_j^e[n]</math> (or vice versa). This update is subtracted from the even samples, producing the signal denoted by <math>f_{j+1}[n]</math>.\n\nThe scheme is invertible due to its structure. In the [[Receiver (information theory)|receiver]], the update step is computed first with its result added back to the even samples, and then it is possible to compute exactly the same prediction to add to the odd samples. In order to recover the original signal, the lazy wavelet transform has to be inverted. Generalized lifting scheme has the same three kinds of operations. However, this scheme avoids the addition-subtraction restriction that offered classical lifting, which has some consequences. For example, the design of all steps must guarantee the scheme invertibility (not guaranteed if the addition-subtraction restriction is avoided).\n\n== Definition ==\n[[File:GLScheme.png|thumb|400px|alt=Generalized Lifting Scheme.|The (forward) Generalized Lifting Scheme transform block diagram.]]\n\n''Generalized lifting scheme'' is a dyadic transform that follows these rules:\n# [[Interleaving (data)|Deinterleaves]] the input into a stream of even-numbered samples and another stream of odd-numbered samples. This is sometimes referred to as a '''Lazy Wavelet Transform'''.\n# Computes a '''Prediction [[Map (mathematics)|Mapping]]'''. This step tries to predict odd samples taking into account the even ones (or vice versa). There is a mapping from the space of the samples in <math>f_j^e[n]</math> to the space of the samples in <math>g_{j+1}[n]</math>. In this case the samples (from <math>f_j^e[n]</math>) chosen to be the reference for <math>f_j^o[n]</math> are called '''the context'''. It could be expressed as:\n#:<math> \\textstyle g_{j+1}[n] = P(f_j^o[n];f_j^e[n]) </math>\n# Computes an '''Update Mapping'''. This step tries to update the even samples taking into account the odd predicted samples. It would be a kind of preparation for the next prediction step, if any. It could be expressed as:\n#:<math> \\textstyle f_{j+1}[n] = U(f_j^e[n];g_{j+1}[n]) </math>\n\nObviously, these mappings cannot be any functions. In order to guarantee the invertibility of the scheme itself, all mappings involved in the transform must be invertible. In case that mappings arise and arrive on finite sets (discrete bounded value signals), this condition is equivalent to saying that mappings are [[Injective function|injective]] (one-to-one). Moreover, if a mapping goes from one set to a set of the same cardinality, it should be [[Bijection|bijective]].\n\nIn the Generalized Lifting Scheme the addition/subtraction restriction is avoided by including this step in the mapping. In this way the Classical Lifting Scheme is generalized.\n\n== Design ==\nSome designs have been developed for the prediction-step mapping. The update-step design has not been considered as thoroughly, because it remains to be answered how exactly the update step is useful. The main application of this technique is image compression. There some interesting references such as,<ref>{{cite conference | last1 = Rolon    | first1 = J. C. | last2 = Salembier | first2 = P. | title = Generalized Lifting for Sparse Image Representation and Coding  | booktitle = Picture Coding Symposiu, PCS 2007  | date = Nov 7–9, 2007|url=https://www.researchgate.net/profile/Philippe_Salembier/publication/228347925_Generalized_Lifting_for_Sparse_Image_Representation_and_Coding/links/09e4150bd01088a783000000.pdf}}</ref><ref>{{cite conference  | last1 = Rolon | first1 = J. C. | last2 = Salembier | first2 = P. | last3 = Alameda | first3 = X. | title = Image Compression with Generalized Lifting and partial knowledge of the signal pdf | booktitle = International Conference on Image Processing, ICIP'08 | date = Oct 12–15, 2008|url=https://www.academia.edu/download/40211740/cRolon08.pdf}}</ref><ref>{{cite conference | last1 = Rolon | first1 = J. C. | last2 = Ortega | first2 = A. | last3 = Salembier | first3 = P. | title = Modeling of Contours in Wavelet Domain for Generalized Lifting Image Compression | booktitle = ICASSP 2009 (submitted)|url=https://upcommons.upc.edu/bitstream/handle/2117/9008/modelingcontours.pdf}}</ref> and.<ref>{{cite conference | last1 = Rolon | first1 = J. C. | last2 = Mendonça | first2 = E. | last3 = Salembier     | first3 = P. | title = Generalized Lifting With Adaptive Local pdf estimation for Image Coding|url=https://upcommons.upc.edu/bitstream/handle/2117/8835/GeneralizedLifting.pdf}}</ref>\n\n== References ==\n{{Reflist}}\n\n{{DEFAULTSORT:Generalized Lifting}}\n[[Category:Wavelets]]"
    },
    {
      "title": "Harmonic wavelet transform",
      "url": "https://en.wikipedia.org/wiki/Harmonic_wavelet_transform",
      "text": "In the [[mathematics]] of [[signal processing]], the '''harmonic wavelet transform''', introduced by [[David Edward Newland]] in 1993, is a [[wavelet]]-based linear transformation of a given function into a [[time-frequency representation]].  It combines advantages of the [[short-time Fourier transform]] and the [[continuous wavelet transform]]. It can be expressed in terms of repeated [[Fourier transform]]s, and its discrete analogue can be computed efficiently using a [[fast Fourier transform]] algorithm.\n\n== Harmonic wavelets ==\n\nThe transform uses a family of \"harmonic\" wavelets indexed by two integers ''j'' (the \"level\" or \"order\") and ''k'' (the \"translation\"), given by <math>w(2^j t - k) \\!</math>, where\n\n:<math>w(t) = \\frac{e^{i4\\pi t} - e^{i 2\\pi t}}{i 2\\pi t} .</math>\n\nThese functions are orthogonal, and their Fourier transforms are a square [[window function]] (constant in a certain octave band and zero elsewhere).  In particular, they satisfy:\n\n:<math>\\int_{-\\infty}^\\infty w^*(2^j t - k) \\cdot w(2^{j'} t - k') \\, dt = \\frac{1}{2^j} \\delta_{j,j'} \\delta_{k,k'}</math>\n:<math>\\int_{-\\infty}^\\infty w(2^j t - k) \\cdot w(2^{j'} t - k') \\, dt = 0</math>\n\nwhere \"*\" denotes [[complex conjugation]] and <math>\\delta</math> is [[Kronecker's delta]].\n\nAs the order ''j'' increases, these wavelets become more localized in Fourier space (frequency) and in higher frequency bands, and conversely become less localized in time (''t'').  Hence, when they are used as a [[basis (linear algebra)|basis]] for expanding an arbitrary function, they represent behaviors of the function on different timescales (and at different time offsets for different ''k'').\n\nHowever, it is possible to combine all of the negative orders (''j'' &lt; 0) together into a single family of \"scaling\" functions <math>\\varphi(t - k)</math> where\n\n:<math>\\varphi(t) = \\frac{e^{i2\\pi t} - 1}{i 2\\pi t}.</math>\n\nThe function ''&phi;'' is orthogonal to itself for different ''k'' and is also orthogonal to the wavelet functions for non-negative ''j'':\n\n:<math>\\int_{-\\infty}^\\infty \\varphi^*(t - k) \\cdot \\varphi(t - k') \\, dt = \\delta_{k,k'}</math>\n:<math>\\int_{-\\infty}^\\infty w^*(2^j t - k) \\cdot \\varphi(t - k') \\, dt = 0\\text{ for }j \\geq 0</math>\n:<math>\\int_{-\\infty}^\\infty \\varphi(t - k) \\cdot \\varphi(t - k') \\, dt = 0</math>\n:<math>\\int_{-\\infty}^\\infty w(2^j t - k) \\cdot \\varphi(t - k') \\, dt = 0\\text{ for }j \\geq 0.</math>\n\nIn the harmonic wavelet transform, therefore, an arbitrary real- or complex-valued function <math>f(t)</math> (in [[Lp space|L2]]) is expanded in the basis of the harmonic wavelets (for all integers ''j'') and their complex conjugates:\n\n:<math>f(t) = \\sum_{j=-\\infty}^\\infty \\sum_{k=-\\infty}^\\infty \\left[ a_{j,k} w(2^j t - k) + \\tilde{a}_{j,k} w^*(2^j t - k)\\right],</math>\n\nor alternatively in the basis of the wavelets for non-negative ''j'' supplemented by the scaling functions ''&phi;'':\n\n:<math>f(t) = \\sum_{k=-\\infty}^\\infty \\left[ a_k \\varphi(t - k) + \\tilde{a}_k \\varphi^*(t - k) \\right] + \\sum_{j=0}^\\infty \\sum_{k=-\\infty}^\\infty \\left[ a_{j,k} w(2^j t - k) + \\tilde{a}_{j,k} w^*(2^j t - k)\\right] .</math>\n\nThe expansion coefficients can then, in principle, be computed using the orthogonality relationships:\n\n:<math>\n\\begin{align}\na_{j,k} & {} = 2^j \\int_{-\\infty}^\\infty f(t) \\cdot w^*(2^j t - k) \\, dt \\\\\n\\tilde{a}_{j,k} & {} = 2^j \\int_{-\\infty}^\\infty f(t) \\cdot w(2^j t - k) \\, dt \\\\\na_k & {} = \\int_{-\\infty}^\\infty f(t) \\cdot \\varphi^*(t - k) \\, dt \\\\\n\\tilde{a}_k & {} = \\int_{-\\infty}^\\infty f(t) \\cdot \\varphi(t - k) \\, dt.\n\\end{align}\n</math>\n\nFor a real-valued function ''f''(''t''), <math>\\tilde{a}_{j,k} = a_{j,k}^*</math> and <math>\\tilde{a}_k = a_k^*</math> so one can cut the number of independent expansion coefficients in half.\n\nThis expansion has the property, analogous to [[Parseval's theorem]], that:\n\n:<math>\n\\begin{align}\n& \\sum_{j=-\\infty}^\\infty \\sum_{k=-\\infty}^\\infty 2^{-j} \\left( |a_{j,k}|^2 + |\\tilde{a}_{j,k}|^2 \\right) \\\\\n& {} = \\sum_{k=-\\infty}^\\infty \\left( |a_k|^2 + |\\tilde{a}_k|^2 \\right) + \\sum_{j=0}^\\infty \\sum_{k=-\\infty}^\\infty 2^{-j} \\left( |a_{j,k}|^2 + |\\tilde{a}_{j,k}|^2 \\right) \\\\\n& {} = \\int_{-\\infty}^\\infty |f(x)|^2 \\, dx.\n\\end{align}\n</math>\n\nRather than computing the expansion coefficients directly from the orthogonality relationships, however, it is possible to do so using a sequence of Fourier transforms.  This is much more efficient in the discrete analogue of this transform (discrete ''t''), where it can exploit [[fast Fourier transform]] algorithms.\n\n== References ==\n\n* David E. Newland, \"Harmonic wavelet analysis,\" ''Proceedings of the Royal Society of London, Series A (Mathematical and Physical Sciences)'', vol. '''443''', no. 1917, p.&nbsp;203&ndash;225 (8 Oct. 1993).\n* ''Wavelets: the key to intermittent information'' by B. W. Silverman and J. C. Vassilicos, Oxford University Press, 2000. ({{ISBN|0-19-850716-X}})\n* B. Boashash, editor, “Time-Frequency Signal Analysis and Processing – A Comprehensive Reference”, Elsevier Science, Oxford, 2003.\n\n[[Category:Time–frequency analysis]]\n[[Category:Transforms]]\n[[Category:Wavelets]]"
    },
    {
      "title": "ICER",
      "url": "https://en.wikipedia.org/wiki/ICER",
      "text": "{{other uses}}\n'''ICER''' is a [[wavelet]]-based [[image compression]] file format used by the [[NASA]] [[Mars Rover]]s. ICER has both lossy and [[lossless compression]] modes.\n\nThe [[Mars]] Exploration Rovers “Spirit” ([[MER-A]]) and “Opportunity” ([[MER-B]]) both use ICER.  Onboard image compression is used extensively to make best use of the [[downlink]] resources. The [[Mars Science Lab]] supports the use of ICER for its [[Navcam|navigation cameras]] (but all other cameras use other file formats).\n\nMost of the MER images are compressed with the ICER image compression software. The remaining MER images that are compressed make use of modified Low Complexity Lossless Compression ('''LOCO''') software, a lossless submode of ICER.\n\nICER is a wavelet-based image compressor that allows for a graceful trade-off between the amount of compression (expressed in terms of compressed data volume in [[bit]]s/[[pixel]]) and the resulting degradation in image quality (distortion). ICER has some similarities to [[JPEG2000]], with respect to select wavelet operations.\n\nThe development of ICER was driven by the desire to achieve high compression performance while meeting the specialized needs of [[outer space|deep space]] applications.\n\n== Practical considerations ==\nTo control the image quality and amount of compression in ICER, the user specifies a byte quota (the nominal number of bytes to be used to store the compressed image) and a quality level parameter (which is essentially a quality goal).\n\n* ICER attempts to produce a compressed image that meets the quality level using as few compressed bytes as possible.\n* ICER stops producing compressed bytes once the quality level or byte quota is met, whichever comes first.\n\nThis arrangement provides added flexibility compared to compressors (like the JPEG compressor used on Mars Pathfinder) that provide only a single parameter to control image quality. Using ICER, when the primary concern is the bandwidth available to transmit the compressed image, one can set the quality goal to lossless and the given byte quota will determine the amount of compression obtained.\n\nAt the other extreme—when the only important consideration is a minimum acceptable image quality it is possible to specify sufficiently large byte quota and the amount of compression will be determined by the quality level specified.\n\nTo achieve error containment, ICER produces the compressed bitstream in separate pieces or segments that can be decoded independently. These segments represent rectangular regions of the original image, but are defined in the transform domain.\n\nIf the image were partitioned directly and the wavelet transform separately applied to each segment, under lossy compression the boundaries between segments would tend to be noticeable in the reconstructed image even when no\ncompressed data is lost.\n\nSince ICER provides a facility for automated flexibility in choosing the number of segments, compression effectiveness can be traded against packet loss protection, thereby accommodating different channel error rates.\n\nNote also that more segments are not always bad for compression effectiveness: many images are most effectively compressed using 4 to 6 segments (for megapixel images) because disparate regions of the image end up in different segments.\n\n== Design commonalities with the JPEG2000 compressor ==\n[[JPEG 2000]] has some design commonalities with the ICER image compression format that is used to send images back from the [[Mars]] rovers.\n\nICER (like JPEG 2000) is wavelet-based and provides\n* progressive compression.\n* lossless compression (using the LOCO compressor).\n* lossy compression.\n* image context error correction to limit the effects of data loss on the deep-space channel.\n\nICER overall provides lossy compression performance\ncompetitive with the JPEG2000 image compression standard.\n\nICER-JPEG 2000 Common Features\n* Both offer a variable number of image tiles to increase compression effectiveness over the deep space channel. Image tiles reduce demands on memory and processing time.\n* Both offer a 'byte' quota.\n* Both offer a 'quality' quota.\n\n== ICER-JPEG 2000 differences ==\nJPEG2000 and ICER have many important internal differences\n* JPEG 2000 uses floating point math, where ICER uses only integer math. Thus ICER will have good performance on integer only CPUs like the T414 [[Transputer]], whereas JPEG 2000 will not perform as well as it is forced into [[floating point emulation]].\n* ICER reverts to a separate internal LOCO (Low Complexity Lossless Compression) compressor for lossless image compression.\n* JPEG 2000 implements a low complexity symmetrical wavelet lossless compressor, but ICER uses an integer only non-wavelet lossless compressor.\n* ICER and JPEG 2000 encode color spaces differently.\n* ICER in its current form does compress monochrome images better than colour images due to its origins as an internal NASA [[Deep Space Network]] file format.\n* ICER is subject to less than 1% overshoot when byte and quality quotas are in effect. On the other hand, JPEG2000 codecs are typically designed never to overshoot their byte quotas.\n\n== When should one use ICER or ICER 3D ==\nICER was created for low end 32 bit [[CPU]]s (essentially [[embedded computer]]s) on spacecraft. It was finally used for the Mars Exploration Rovers. It has never been used for any real time application, only near real time.\n\nJPEG2000 has been used by many image processing applications in near real time and real-time (Digital Cinema, Broadcast). Main advantages of the codec is that it is License free (JPEG2000 PART1). The JPEG committee has stated: “It has always been a strong goal of the JPEG committee that its standards should be implementable in their baseline form without payment of royalty and license fees.[...] Agreements have been reached with over 20 large organizations holding many patents in this area to allow use of their intellectual property in connection with the standard without payment of license fees or royalties”.\n[[Hewlett-Packard]]'s [[Remote Graphics Software]] uses a video codec called [[HP3 (codec)]] which claims to derive from Mars Rover compression - this could be a real-time implementation of ICER.\n<ref>\n[http://www.augi.com/publications/hotnews.asp?page=1751 Autodesk User Group International: \"April 2008\"] \"HP Remote Graphics Software (RGS) ... [uses] ... an HP patented compression algorithm developed for the NASA Mars Rover program that compresses and encrypts graphics data\"\n</ref>\n\nICER offers a new mode called '''Spectral+ICER''' that makes possible lower rate distortion levels (aka grey level errors) with ICER images. This mode is only so far being used with the Mars Pathfinders, but may see wider implementation in the ICER standard [https://web.archive.org/web/20090706080635/http://web99.arc.nasa.gov/~vgulick/GSOM/LPSC2000/SPEC_Path.pdf].\n\n== ICER 3D ==\nError-containment segments in ICER-3D are defined spatially (in the wavelet transform domain) similarly to JPEG 2000. The wavelet-transformed data are partitioned in much the same way as in ICER, except that in ICER-3D the segments extend through all spectral bands. Error-containment segments in ICER and ICER-3D are defined using an unmodified form of the ICER rectangle partitioning algorithm.\n\nIn ICER-3D, contexts are defined based on two neighboring coefficients in the spectral dimension and no neighboring coefficients in the same spatial plane. This contrasts with the context modeling scheme used by ICER, which makes use of previously encoded information from spatially neighboring coefficients.\n\nICER-3D exploits 3D data dependencies in part by using a 3-D wavelet decomposition. The particular\ndecomposition used by ICER-3D includes additional spatial decomposition steps compared to a 3-D Mallat\ndecomposition. This modified decomposition provides benefits in the form of quantitatively improved\nrate-distortion performance and in the elimination of spectral [[ringing artifacts]].\n\nICER-3D takes advantage of the correlation properties of wavelet-transformed hyperspectral data by\nusing a context modeling procedure that emphasizes spectral (rather than spatial) dependencies in the\nwavelet-transformed data. This provides a significant gain over the alternative spatial context modeler considered.\n\nICER-3D also inherits most of the  important features of ICER, including progressive compression, the ability to perform lossless and lossy compression, and an effective error-containment scheme to limit the effects of data loss on the deep-space channel.\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://ipnpr.jpl.nasa.gov/progress_report/42-155/155J.pdf The ICER Progressive Wavelet Image Compressor]\n* [http://tmo.jpl.nasa.gov/progress_report/42-168/168D.pdf Lossy Image Compression from Mars Rovers]\n* [http://tmo.jpl.nasa.gov/progress_report/42-164/164A.pdf ICER 3D]\n\n{{CCSDS}}\n{{Compression formats}}\n{{Graphics file formats}}\n\n[[Category:Graphics file formats]]\n[[Category:Spaceflight technologies]]\n[[Category:ISO standards]]\n[[Category:Wavelets]]"
    },
    {
      "title": "JPEG 2000",
      "url": "https://en.wikipedia.org/wiki/JPEG_2000",
      "text": "{{Infobox file format\n| name = JPEG 2000\n| screenshot = JPEG JFIF and 2000 Comparison.png\n| caption = Comparison of JPEG 2000 with the original JPEG format.\n| extension = <tt>.jp2</tt>, <tt>.j2k</tt>, <tt>.jpf</tt>, <tt>.jpx</tt>, <tt>.jpm</tt>, <tt>.mj2</tt>\n| mime = <tt>image/jp2</tt>, <tt>image/jpx</tt>, <tt>image/jpm</tt>, <tt>video/mj2</tt>\n| uniform type = <tt>public.jpeg-2000</tt>\n| owner = [[Joint Photographic Experts Group]]\n| genre = [[graphics file format]]\n| containerfor =\n| containedby =\n| extendedfrom =\n| extendedto =\n| standard = ISO/IEC 15444\n}}\n\n'''JPEG 2000''' ('''JP2''') is an [[image compression]] standard and coding system. It was created by the [[Joint Photographic Experts Group]] committee in 2000 with the intention of superseding their original [[discrete cosine transform]]-based [[JPEG]] standard (created in 1992) with a newly designed, [[wavelet]]-based method. The standardized [[filename extension]] is '''.jp2''' for [[International Organization for Standardization|ISO]]/[[International Electrotechnical Commission|IEC]] 15444-1 conforming files and '''.jpx''' for the extended part-2 specifications, published as ISO/IEC 15444-2. The registered [[Internet media type|MIME types]] are defined in RFC 3745. For ISO/IEC 15444-1 it is '''image/jp2'''.\n\nJPEG 2000 code streams are [[Region of interest|regions of interest]] that offer several mechanisms to support spatial random access or region of interest access at varying degrees of granularity. It is possible to store different parts of the same picture using different quality.\n\n== Aims of the standard ==\nWhile there is a modest increase in compression performance of JPEG 2000 compared to JPEG, the main advantage offered by JPEG 2000 is the significant flexibility of the codestream. The codestream obtained after compression of an image with JPEG 2000 is scalable in nature, meaning that it can be decoded in a number of ways; for instance, by truncating the codestream at any point, one may obtain a representation of the image at a lower resolution, or [[signal-to-noise]] ratio – see [[Lossy data compression#Downsampling/compressed representation scalability|scalable compression]]. By ordering the codestream in various ways, applications can achieve significant performance increases. However, as a consequence of this flexibility, JPEG 2000 requires [[encoder]]s/[[Codec|decoders]] that are complex and computationally demanding. Another difference, in comparison with JPEG, is in terms of visual [[compression artifact|artifacts]]: JPEG 2000 only produces [[ringing artifacts]], manifested as blur and rings near edges in the image, while JPEG produces both ringing artifacts and 'blocking' artifacts, due to its [[JPEG#Block splitting|8×8 blocks]].\n\nJPEG 2000 has been published as an [[International Organization for Standardization|ISO]] standard, ISO/IEC 15444. The cost of obtaining all documents for the standard has been estimated to 2718 CHF (approximately 2700 USD).<ref>[http://www.diva-portal.org/smash/get/diva2:925474/FULLTEXT01.pdf Lundell, B., Gamalielsson, J. & Katz, A. (2015) On implementation of Open Standards in software: To what extent can ISO standards be implemented in open source software?] International Journal of Standardization Research, Vol. 13(1), pp. 47-73.</ref> {{As of|2017}}, JPEG 2000 is not widely supported in [[web browser]]s, and hence is not generally used on the [[Internet]].\n\n==Improvements over the 1992 JPEG standard==\n[[File:JPEG 2000 Artifacts Demonstration.png|thumb|128px|Top-to-bottom demonstration of the artifacts of JPEG 2000 compression. The numbers indicate the compression ratio used.]]\n\n===Superior compression ratio===\nAt high bit rates, artifacts become nearly imperceptible, so JPEG 2000 has a small machine-measured fidelity advantage over JPEG. At lower bit rates (e.g., less than 0.25 bits/pixel for grayscale images), JPEG 2000 has a significant advantage over certain modes of JPEG: artifacts are less visible and there is almost no blocking. The compression gains over JPEG are attributed to the use of [[Discrete wavelet transform|DWT]] and a more sophisticated entropy encoding scheme.\n\n=== Multiple resolution representation ===\nJPEG 2000 decomposes the image into a multiple resolution representation in the course of its compression process. This [[Pyramid (image processing)|pyramid representation]] can be put to use for other image presentation purposes beyond compression.\n\n=== Progressive transmission by pixel and resolution accuracy ===\nThese features are more commonly known as ''progressive decoding'' and ''signal-to-noise ratio (SNR) scalability''. JPEG 2000 provides efficient code-stream organizations which are progressive by pixel accuracy and by image resolution (or by image size). This way, after a smaller part of the whole file has been received, the viewer can see a lower quality version of the final picture. The quality then improves progressively through downloading more data bits from the source.\n\n=== Choice of lossless or lossy compression ===\nLike the [[Lossless JPEG]] standard,<ref>[http://www.cis.temple.edu/~vasilis/Courses/CIS750/Papers/doc_jpeg_c_5.pdf The JPEG Still Picture Compression Standard] pp.6-7</ref> the JPEG 2000 standard provides both [[Lossless compression|lossless]] and [[lossy compression]] in a single compression architecture. Lossless compression is provided by the use of a reversible integer wavelet transform in JPEG 2000.\n\n=== Error resilience ===\nLike JPEG 1992, JPEG 2000 is robust to bit errors introduced by noisy communication channels, due to the coding of data in relatively small independent blocks.\n\n=== Flexible file format ===\nThe JP2 and JPX file formats allow for handling of color-space information, metadata, and for interactivity in networked applications as developed in the JPEG Part 9 JPIP protocol.\n\n=== High dynamic range support ===\nJPEG 2000 supports any bit depth, such as 16- and 32-bit floating point pixel images, and any color space.\n\n=== Side channel spatial information ===\nFull support for transparency and alpha planes.\n{{Clear}}\n\n==JPEG 2000 image coding system - Parts==\n\nThe JPEG 2000 image coding system (ISO/IEC 15444) consists of following parts:\n\n{| class=\"wikitable sortable\" width=\"87%\"\n|+JPEG 2000 image coding system - Parts<ref name=\"jpeg2000-jpeg\">{{cite web | url=http://www.jpeg.org/jpeg2000/index.html | title=Joint Photographic Experts Group, JPEG2000 | author=JPEG | accessdate=2009-11-01}}</ref><ref name=\"jpeg2000-ign\">{{cite web | url=http://eden.ign.fr/std/JPEG2000/index_html | title=JPEG2000 (ISO 15444) | author=IGN Standardization Team | accessdate=2009-11-01}}</ref>\n|-\n! width=\"8%\" | Part\n! width=\"15%\" | Number\n! width=\"8%\" | First public release date (First edition)\n! width=\"8%\" | Latest public release date (edition)\n! width=\"8%\" | Latest amendment\n! width=\"8%\" | Identical ITU-T standard\n! width=\"30%\" | Title\n! Description\n|-\n| Part 1\n| [https://www.iso.org/standard/27687.html ISO/IEC 15444-1]\n| 2000\n| 2016<ref>{{cite web | url=https://www.iso.org/standard/70018.html | title=ISO/IEC 15444-1:2016 - Information technology -- JPEG 2000 image coding system: Core coding system | author=International Organization for Standardization | accessdate=2017-10-19 }}</ref>\n|\n| [http://www.itu.int/rec/T-REC-T.800 T.800]\n| Core coding system\n| the basic characteristics of JPEG 2000 compression (''.jp2'')\n|-\n| Part 2\n| [https://www.iso.org/standard/33160.html ISO/IEC 15444-2]\n| 2004\n| 2004\n| 2015<ref>{{cite web | url=https://www.iso.org/standard/33160.html | title=ISO/IEC 15444-2:2004 - Information technology -- JPEG 2000 image coding system: Extensions | author=International Organization for Standardization | accessdate=2017-10-19 }}</ref>\n| [http://www.itu.int/rec/T-REC-T.801 T.801]\n| Extensions\n| (''.jpx'', ''.jpf'', ''floating points'')\n|-\n| Part 3\n| [https://www.iso.org/standard/33875.html ISO/IEC 15444-3]\n| 2002\n| 2007\n| 2010<ref>{{cite web | url=https://www.iso.org/standard/41570.html | title=ISO/IEC 15444-3:2007 - Information technology -- JPEG 2000 image coding system: Motion JPEG 2000 | author=International Organization for Standardization | accessdate=2017-10-19 }}</ref>\n| [http://www.itu.int/rec/T-REC-T.802 T.802]\n| [[Motion JPEG 2000]]\n| (''.mj2'')\n|-\n| Part 4\n| [https://www.iso.org/standard/33876.html ISO/IEC 15444-4]\n| 2002\n| 2004<ref>{{cite web | url=https://www.iso.org/standard/39079.html | title=ISO/IEC 15444-4:2004 - Information technology -- JPEG 2000 image coding system: Conformance testing | author=International Organization for Standardization | accessdate=2017-10-19 }}</ref>\n|\n| [http://www.itu.int/rec/T-REC-T.803 T.803]\n| Conformance testing\n|\n|-\n| Part 5\n| [https://www.iso.org/standard/33877.html ISO/IEC 15444-5]\n| 2003\n| 2015<ref>{{cite web | url=https://www.iso.org/standard/69462.html | title=ISO/IEC 15444-5:2015 - Information technology -- JPEG 2000 image coding system: Reference software | author=International Organization for Standardization | accessdate=2017-10-19 }}</ref>\n|\n| [http://www.itu.int/rec/T-REC-T.804 T.804]\n| Reference software\n| Java and C implementations\n|-\n| Part 6\n| [https://www.iso.org/standard/35458.html ISO/IEC 15444-6]\n| 2003\n| 2016<ref>{{cite web | url=https://www.iso.org/standard/61124.html | title=ISO/IEC 15444-6:2013 - Information technology -- JPEG 2000 image coding system -- Part 6: Compound image file format | author=International Organization for Standardization | accessdate=2017-10-19 }}</ref>\n|\n| [http://www.itu.int/rec/T-REC-T.805 T.805]\n| Compound image file format\n| (''.jpm'') e.g. document imaging, for pre-press and fax-like applications\n|-\n| Part 7\n| abandoned<ref name=\"jpeg2000-jpeg\" />\n|\n|\n|\n|\n| Guideline of minimum support function of ISO/IEC 15444-1<ref>{{Cite web | url=http://kikaku.itscj.ipsj.or.jp/sc29/open/29view/29n39731.doc | title=JPEG, JBIG - Resolutions of 22nd WG1 New Orleans Meeting | format=DOC | author=International Organization for Standardization/IEC JTC 1/SC 29/WG 1 | date=2000-12-08 | accessdate=2009-11-01 | deadurl=yes | archiveurl=https://web.archive.org/web/20140512215828/http://kikaku.itscj.ipsj.or.jp/sc29/open/29view/29n39731.doc | archivedate=2014-05-12 | df= }}</ref>\n| (Technical Report on Minimum Support Functions<ref>{{Cite web | url=http://kikaku.itscj.ipsj.or.jp/sc29/open/29view/29n39741.doc | title=22nd WG1 New Orleans Meeting, Draft Meeting Report | format=DOC | date=2000-12-08 | accessdate=2009-11-01 | deadurl=yes | archiveurl=https://web.archive.org/web/20140512224331/http://kikaku.itscj.ipsj.or.jp/sc29/open/29view/29n39741.doc | archivedate=2014-05-12 | df= }}</ref>)\n|-\n| Part 8\n| [https://www.iso.org/standard/37382.html ISO/IEC 15444-8]\n| 2007\n| 2007\n| 2008<ref>{{cite web | url=https://www.iso.org/standard/37382.html | title=ISO/IEC 15444-8:2007 - Information technology -- JPEG 2000 image coding system: Secure JPEG 2000 | author=International Organization for Standardization | accessdate=2017-10-19 }}</ref>\n| [http://www.itu.int/rec/T-REC-T.807 T.807]\n| Secure JPEG 2000\n| JPSEC (security aspects)\n|-\n| Part 9\n| [https://www.iso.org/standard/39413.html ISO/IEC 15444-9]\n| 2005\n| 2005\n| 2014<ref>{{cite web | url=https://www.iso.org/standard/39413.html | title=ISO/IEC 15444-9:2005 - Information technology -- JPEG 2000 image coding system: Interactivity tools, APIs and protocols | author=International Organization for Standardization | accessdate=2017-10-19 }}</ref>\n| [http://www.itu.int/rec/T-REC-T.808 T.808]\n| Interactivity tools, APIs and protocols\n| [[JPIP]] (interactive protocols and API)\n|-\n| Part 10\n| [https://www.iso.org/standard/40024.html ISO/IEC 15444-10]\n| 2008\n| 2011<ref>{{cite web | url=https://www.iso.org/standard/61534.html | title=ISO/IEC 15444-10:2011 - Information technology -- JPEG 2000 image coding system: Extensions for three-dimensional data | author=International Organization for Standardization | accessdate=2017-10-19 }}</ref>\n|\n| [http://www.itu.int/rec/T-REC-T.809 T.809]\n| Extensions for three-dimensional data\n| JP3D (volumetric imaging)\n|-\n| Part 11\n| [https://www.iso.org/standard/40025.html ISO/IEC 15444-11]\n| 2007\n| 2007\n| 2013<ref>{{cite web | url=https://www.iso.org/standard/40025.html | title=ISO/IEC 15444-11:2007 - Information technology -- JPEG 2000 image coding system: Wireless | author=International Organization for Standardization | accessdate=2017-10-19 }}</ref>\n| [http://www.itu.int/rec/T-REC-T.810 T.810]\n| Wireless\n| JPWL (wireless applications)\n|-\n| Part 12\n| [https://www.iso.org/standard/38612.html ISO/IEC 15444-12]<br />(Withdrawn in 2017)\n| 2004\n| 2015<ref>{{cite web | url=https://www.iso.org/standard/68963.html | title=ISO/IEC 15444-12:2015 - Information technology -- JPEG 2000 image coding system -- Part 12: ISO base media file format | author=International Organization for Standardization | accessdate=2017-10-19 }}</ref>\n|\n|\n| [[ISO base media file format]]\n|\n|-\n| Part 13\n| [https://www.iso.org/standard/42271.html ISO/IEC 15444-13]\n| 2008\n| 2008<ref>{{cite web | url=https://www.iso.org/standard/42271.html | title=ISO/IEC 15444-13:2008 - Information technology -- JPEG 2000 image coding system: An entry level JPEG 2000 encoder | author=International Organization for Standardization | accessdate=2017-10-19 }}</ref>\n|\n| [http://www.itu.int/rec/T-REC-T.812 T.812]\n| An entry level JPEG 2000 encoder\n|\n|-\n| Part 14\n| [https://www.iso.org/standard/50410.html ISO/IEC 15444-14]\n| 2013<ref>{{cite web | url=https://www.iso.org/standard/50410.html | title=ISO/IEC 15444-14:2013 - Information technology -- JPEG 2000 image coding system -- Part 14: XML representation and reference | author=International Organization for Standardization | date=2007-07-01 | accessdate=2009-11-01 }}</ref>\n|\n|\n| [http://www.itu.int/rec/T-REC-T.813 T.813]\n| XML structural representation and reference\n| JPXML<ref>{{Cite web | url=http://kikaku.itscj.ipsj.or.jp/sc29/open/29view/29n83811.doc | title=Resolutions of 41st WG1 San Jose Meeting | format=DOC | date=2007-04-27 | accessdate=2009-11-01 | deadurl=yes | archiveurl=https://web.archive.org/web/20140512224659/http://kikaku.itscj.ipsj.or.jp/sc29/open/29view/29n83811.doc | archivedate=2014-05-12 | df= }}</ref>\n|}\n\n==Technical discussion==\nThe aim of JPEG 2000 is not only improving compression performance over JPEG but also adding (or improving) features such as scalability and editability. JPEG 2000's improvement in compression performance relative to the original JPEG standard is actually rather modest and should not ordinarily be the primary consideration for evaluating the design.  Very low and very high compression rates are supported in JPEG 2000. The ability of the design to handle a very large range of effective bit rates is one of the strengths of JPEG 2000. For example, to reduce the number of bits for a picture below a certain amount, the advisable thing to do with the first JPEG standard is to reduce the resolution of the input image before encoding it. That is unnecessary when using JPEG 2000, because JPEG 2000 already does this automatically through its multiresolution decomposition structure. The following sections describe the algorithm of JPEG 2000.\n\nAccording to KB, «the current JP2 format specification leaves room for multiple interpretations when it comes to the support of ICC profiles, and the handling of grid resolution information».<ref>{{cite journal|url=http://www.dlib.org/dlib/may11/vanderknijff/05vanderknijff.html|doi=10.1045/may2011-vanderknijff|title=JPEG 2000 for Long-term Preservation: JP2 as a Preservation Format|author= Johan van der Knijff}}</ref>\n\n===Color components transformation===\nInitially images have to be transformed from the RGB [[color space]] to another color space, leading to three ''components'' that are handled separately. There are two possible choices:\n# Irreversible Color Transform (ICT) uses the well known [[YCbCr|YC<sub>B</sub>C<sub>R</sub>]] color space. It is called \"irreversible\" because it has to be implemented in floating or fix-point and causes round-off errors.\n# Reversible Color Transform (RCT) uses a modified YUV color space that does not introduce quantization errors, so it is fully reversible. Proper implementation of the RCT requires that numbers are rounded as specified that cannot be expressed exactly in matrix form. The transformation is:\n:<math>\nY = \\left\\lfloor \\frac{R+2G+B}{4} \\right\\rfloor ;\nC_B = B - G ;\nC_R = R - G ;\n</math>\nand\n<math>\nG = Y - \\left\\lfloor \\frac{C_B + C_R}{4} \\right\\rfloor ;\nR = C_R + G ;\nB = C_B + G.\n</math>\n\nThe [[chrominance]] components can be, but do not necessarily have to be, down-scaled in resolution; in fact, since the wavelet transformation already separates images into scales, downsampling is more effectively handled by dropping the finest wavelet scale. This step is called ''multiple component transformation'' in the JPEG 2000 language since its usage is not restricted to the [[RGB color model]].\n\n===Tiling===\nAfter color transformation, the image is split into so-called ''tiles'', rectangular regions of the image that are transformed and encoded separately. Tiles can be any size, and it is also possible to consider the whole image as one single tile. Once the size is chosen, all the tiles will have the same size (except optionally those on the right and bottom borders). Dividing the image into tiles is advantageous in that the decoder will need less memory to decode the image and it can opt to decode only selected tiles to achieve a partial decoding of the image. The disadvantage of this approach is that the quality of the picture decreases due to a lower [[peak signal-to-noise ratio]]. Using many tiles can create a blocking effect similar to the older [[JPEG]] 1992 standard.\n\n===Wavelet transform===\n[[File:Wavelet Bior2.2.svg|thumb|right|CDF 5/3 wavelet used for lossless compression.]]\n[[File:Jpeg2000 2-level wavelet transform-lichtenstein.png|thumb|256px|An example of the wavelet transform that is used in JPEG 2000. This is a  2nd-level [[Cohen-Daubechies-Feauveau wavelet|CDF]] 9/7 [[wavelet transform]].]]\n\nThese tiles are then [[wavelet transform]]ed to an arbitrary depth, in contrast to JPEG 1992 which uses an 8×8 block-size [[discrete cosine transform]]. JPEG 2000 uses two different wavelet transforms:\n# ''irreversible'': the [[Cohen-Daubechies-Feauveau wavelet|CDF]] 9/7 [[wavelet transform]]. It is said to be \"irreversible\" because it introduces quantization noise that depends on the precision of the decoder.\n# ''reversible'': a rounded version of the  biorthogonal [[Cohen-Daubechies-Feauveau wavelet|CDF]] 5/3 [[wavelet]] transform. It uses only integer coefficients, so the output does not require rounding (quantization) and so it does not introduce any quantization noise. It is used in lossless coding.\nThe wavelet transforms are implemented by the [[lifting scheme]] or by [[convolution]].\n\n===Quantization===\nAfter the wavelet transform, the coefficients are scalar-[[Quantization (image processing)|quantized]] to reduce the number of bits to represent them, at the expense of quality. The output is a set of integer numbers which have to be encoded bit-by-bit. The parameter that can be changed to set the final quality is the quantization step: the greater the step, the greater is the compression and the loss of quality. With a quantization step that equals 1, no quantization is performed (it is used in lossless compression).\n\n===Coding=== <!-- Courtesy note per [[MOS:LINK2SECT]]: [[EBCOT]] redirects here -->\nThe result of the previous process is a collection of ''sub-bands'' which represent several approximation scales. A sub-band is a set of ''coefficients''—[[real numbers]] which represent aspects of the image associated with a certain frequency range as well as a spatial area of the image.\n\nThe quantized sub-bands are split further into ''precincts'', rectangular regions in the wavelet domain. They are typically sized so that they provide an efficient way to access only part of the (reconstructed) image, though this is not a requirement.\n\nPrecincts are split further into ''code blocks''. Code blocks are in a single sub-band and have equal sizes—except those located at the edges of the image. The encoder has to encode the bits of all quantized coefficients of a code block, starting with the most significant bits and progressing to less significant bits by a process called the ''EBCOT'' scheme. ''EBCOT'' here stands for ''Embedded Block Coding with Optimal Truncation''. In this encoding process, each [[bit plane]] of the code block gets encoded in three so-called ''coding passes'', first encoding bits (and signs) of insignificant coefficients with significant neighbors (i.e., with 1-bits in higher bit planes), then refinement bits of significant coefficients and finally coefficients without significant neighbors. The three passes are called ''Significance Propagation'', ''Magnitude Refinement'' and ''Cleanup'' pass, respectively.\n\nIn lossless mode all bit planes have to be encoded by the EBCOT, and no bit planes can be dropped.\n\nThe bits selected by these coding passes then get encoded by a context-driven binary [[arithmetic coding|arithmetic coder]], namely the binary MQ-coder. The context of a coefficient is formed by the state of its eight neighbors in the code block.\n\nThe result is a bit-stream that is split into ''packets'' where a ''packet'' groups selected passes of all code blocks from a precinct into one indivisible unit. Packets are the key to quality scalability (i.e., packets containing less significant bits can be discarded to achieve lower bit rates and higher distortion).\n\nPackets from all sub-bands are then collected in so-called ''layers''.\nThe way the packets are built up from the code-block coding passes, and thus which packets a layer will contain, is not defined by the JPEG 2000 standard, but in general a codec will try to build layers in such a way that the image quality will increase monotonically with each layer, and the image distortion will shrink from layer to layer. Thus, layers define the progression by image quality within the code stream.\n\nThe problem is now to find the optimal packet length for all code blocks which minimizes the overall distortion in a way that the generated target bitrate equals the demanded bit rate.\n\nWhile the standard does not define a procedure as to how to perform this form of [[rate–distortion optimization]], the general outline is given in one of its many appendices: For each bit encoded by the EBCOT coder, the improvement in image quality, defined as mean square error, gets measured; this can be implemented by an easy table-lookup algorithm. Furthermore, the length of the resulting code stream gets measured. This forms for each code block a graph in the rate–distortion plane, giving image quality over bitstream length. The optimal selection for the truncation points, thus for the packet-build-up points is then given by defining critical ''slopes'' of these curves, and picking all those coding passes whose curve in the rate–distortion graph is steeper than the given critical slope. This method can be seen as a special application of the method of ''[[Lagrange multiplier]]'' which is used for optimization problems under constraints. The [[Lagrange multiplier]], typically denoted by λ, turns out to be the critical slope, the constraint is the demanded target bitrate, and the value to optimize is the overall distortion.\n\nPackets can be reordered almost arbitrarily in the JPEG 2000 bit-stream; this gives the encoder as well as image servers a high degree of freedom.\n\nAlready encoded images can be sent over networks with arbitrary bit rates by using a layer-progressive encoding order.\nOn the other hand, color components can be moved back in the bit-stream; lower resolutions (corresponding to low-frequency sub-bands) could be sent first for image previewing.\nFinally, spatial browsing of large images is possible through appropriate tile and/or partition selection.\nAll these operations do not require any re-encoding but only byte-wise copy operations.\n\n===Compression ratio===\n[[File:Lichtenstein jpeg2000 difference.png|thumb|right|225px|This image shows the (accentuated) difference between an image saved as JPEG 2000 (quality 50%) and the original.]]\n[[File:Comparison between JPEG, JPEG 2000 and JPEG XR.png|thumb|right|225px|Comparison between JPEG 2000, [[JPEG XR]], and JPEG.]]\n\nCompared to the previous JPEG standard, JPEG 2000 delivers a typical compression gain in the range of 20%, depending on the image characteristics. Higher-resolution images tend to benefit more, where JPEG-2000's spatial-redundancy prediction can contribute more to the compression process. In very low-bitrate applications, studies have shown JPEG 2000 to be outperformed<ref>{{Cite web | last = Halbach, Till | title = Performance comparison: H.26L intra coding vs. JPEG2000 |date = July 2002| url = http://etill.net/papers/jvt-d039.pdf | accessdate = 2008-04-22 }}</ref> by the intra-frame coding mode of H.264. Good applications for JPEG 2000 are large images, images with low-contrast edges — e.g., medical images.\n\n===Computational complexity and performance===\nJPEG2000 is much more complicated in terms of computational complexity in comparison with JPEG standard. Tiling, color component transform, discrete wavelet transform, and quantization could be done pretty fast, though entropy codec is time consuming and quite complicated. EBCOT context modelling and arithmetic MQ-coder take most of the time of JPEG2000 codec.\n\nOn CPU the main idea of getting fast JPEG2000 encoding and decoding is closely connected with AVX/SSE and multithreading to process each tile in separate thread. The fastest JPEG2000 solutions utilize both CPU and GPU power to get high performance benchmarks.<ref>{{Cite web | last = Fastvideo | title = JPEG2000 performance benchmarks on GPU |date = September 2018| url = https://www.fastcompression.com/benchmarks/benchmarks-j2k.htm | accessdate = 2019-04-26 }}</ref><ref>{{Cite web | last = Comprimato | title = JPEG2000 performance specification |date = September 2016| url = http://comprimato.com/specifications/ | accessdate = 2016-09-01 }}</ref>\n\n==File format and code stream==\nSimilar to JPEG-1, JPEG 2000 defines both a file format and a code stream. Whereas JPEG 2000 entirely describes the image samples, JPEG-1 includes additional meta-information such as the resolution of the image or the color space that has been used to encode the image. JPEG 2000 images should — if stored as files — be boxed in the JPEG 2000 file format, where they get the '''.jp2''' extension. The part-2 extension to JPEG 2000, i.e., ISO/IEC 15444-2, also enriches this file format by including mechanisms for animation or composition of several code streams into one single image. Images in this extended file-format use the '''.jpx''' extension.\n\nThere is no standardized extension for code-stream data because code-stream data is not to be considered to be stored in files in the first place, though when done for testing purposes, the extension '''.jpc''' or '''.j2k''' appear frequently.\n\n==Metadata==\nFor traditional JPEG, additional [[metadata]], e.g. lighting and exposure conditions, is kept in an application marker in the [[Exchangeable image file format|Exif]] format specified by the JEITA. JPEG 2000 chooses a different route, encoding the same metadata in [[XML]] form. The reference between the Exif tags and the XML elements is standardized by the ISO TC42 committee in the standard 12234-1.4.\n\n[[Extensible Metadata Platform]] can also be embedded in  JPEG 2000.\n\n==Applications==\nSome markets and applications intended to be served by this standard are listed below:\n* Consumer applications such as multimedia devices (e.g., digital cameras, personal digital assistants, 3G mobile phones, color facsimile, printers, scanners, etc.)\n* Client/server communication (e.g., the Internet, Image database, Video streaming, video server, etc.)\n* Military/surveillance (e.g., HD satellite images, Motion detection, network distribution and storage, etc.)\n* Medical imagery, esp. the [[DICOM]] specifications for medical data interchange.\n* Biometrics.\n* [[Remote sensing]]\n* High-quality frame-based video recording, editing and storage.\n* Live HDTV feed contribution (I-frame only video compression with low transmission latency), such as live HDTV feed of a sport event linked to the TV station studio\n* [[Digital cinema]]\n* JPEG 2000 has many design commonalities with the [[ICER]] image compression format that is used to send images back from the [[Mars]] rovers.\n* Digitized Audio-visual contents and Images for Long term [[digital preservation]]\n* [[World Meteorological Organization]] has built JPEG 2000 Compression into the new GRIB2 file format.  The GRIB file structure is designed for global distribution of meteorological data.  The implementation of JPEG 2000 compression in GRIB2 has reduced file sizes up to 80%.<ref>[http://www.cpc.ncep.noaa.gov/products/wesley/wgrib2/ wgrib2 home page]</ref>\n\n==Comparison with PNG format==\nAlthough JPEG 2000 format supports lossless encoding, it is not intended to completely supersede today's dominant lossless image file formats.\n\nThe [[Portable Network Graphics|PNG]] (Portable Network Graphics) format is still more space-efficient in the case of images with many pixels of the same color {{Citation needed|date=August 2016}}, such as diagrams, and supports special compression features that JPEG 2000 does not.\n\n==Legal status==\nJPEG 2000 is covered by patents, but the contributing companies and organizations agreed that licenses for its first part—the core coding system—can be obtained free of charge from all contributors.\n\nThe JPEG committee has stated:\n{{bq|It has always been a strong goal of the JPEG committee that its standards should be implementable in their baseline form without payment of royalty and license fees... The up and coming JPEG 2000 standard has been prepared along these lines, and agreement reached with over 20 large organizations holding many patents in this area to allow use of their intellectual property in connection with the standard without payment of license fees or royalties.<ref>[http://www.jpeg.org/newsrel1.html JPEG 2000 Concerning recent patent claims] {{webarchive|url=https://web.archive.org/web/20070714232941/http://www.jpeg.org/newsrel1.html |date=2007-07-14 }}</ref>}}\n\nHowever, the JPEG committee has acknowledged that undeclared [[submarine patent]]s may still present a hazard:\n{{bq|It is of course still possible that other organizations or individuals may claim intellectual property rights that affect implementation of the standard, and any implementers are urged to carry out their own searches and investigations in this area.<ref>[http://www.jpeg.org/jpeg2000/CDs15444.html JPEG 2000 Committee Drafts] {{webarchive|url=https://web.archive.org/web/20060702065150/http://www.jpeg.org/jpeg2000/CDs15444.html |date=2006-07-02 }}</ref>}}\n\n==Related standards==\nSeveral additional parts of the JPEG 2000 standard exist;\nAmongst them are ISO/IEC 15444-2:2000, JPEG 2000 extensions defining the '''.jpx''' file format, featuring for example [[Trellis quantization]], an extended file format and additional [[color space]]s,<ref name=\"ISO-15444-2\">{{cite web\n | url = http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=33160\n | author = International Organization for Standardization\n | title = ISO/IEC 15444-2:2004, Information technology -- JPEG 2000 image coding system: Extensions\n | year = 2004\n | accessdate = 2009-06-11 }}</ref> ISO/IEC 15444-4:2000, the reference testing and ISO/IEC 15444-6:2000, the compound image file format ('''.jpm'''), allowing compression of compound text/image graphics.<ref name=\"ISO-15444-6\">{{cite web\n | url = http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=35458\n | author = International Organization for Standardization\n | title = ISO/IEC 15444-6:2003, Information technology -- JPEG 2000 image coding system -- Part 6: Compound image file format\n | year = 2003\n | accessdate = 2009-06-11 }}</ref>\n\nExtensions for secure image transfer, ''JPSEC'' (ISO/IEC 15444-8), enhanced error-correction schemes for wireless applications, ''JPWL'' (ISO/IEC 15444-11) and extensions for encoding of volumetric images, ''JP3D'' (ISO/IEC 15444-10) are also already available from the ISO.\n\n===JPIP protocol for streaming JPEG 2000 images===\nIn 2005, a JPEG 2000 based image browsing protocol, called [[JPIP]] has been published as ISO/IEC 15444-9.<ref name=\"ISO-15444-9\">{{cite web\n | url = http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=39413\n | author = International Organization for Standardization\n | title = ISO/IEC 15444-9:2005, Information technology -- JPEG 2000 image coding system: Interactivity tools, APIs and protocols\n | year = 2005\n | accessdate = 2009-06-11 }}</ref> Within this framework, only selected regions of potentially huge images have to be transmitted from an image server on the request of a client, thus reducing the required bandwidth.\n\nJPEG 2000 data may also be streamed using the ECWP and ECWPS protocols found within the ERDAS [[ECW (file format)|ECW]]/JP2 SDK.\n\n===Motion JPEG 2000===\n{{main|Motion JPEG 2000}}\n[[Motion JPEG 2000]], (MJ2), originally defined in Part 3 of the ISO Standard for JPEG2000 (ISO/IEC 15444-3:2002,) as a standalone document, has now been expressed by ISO/IEC 15444-3:2002/Amd 2:2003 in terms of the ISO Base format, ISO/IEC 15444-12 and in [[ITU-T]] Recommendation  T.802.<ref>{{cite web | url=http://www.itu.int/rec/T-REC-T.802/en | title=T.802 : Information technology - JPEG 2000 image coding system: Motion JPEG 2000 | date=January 2005 | accessdate=2009-11-01}}</ref> It specifies the use of the JPEG 2000 format for timed sequences of images (motion sequences), possibly combined with audio, and composed into an overall presentation.<ref name=\"ISO-15444-3\">{{cite web\n | url = http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=41570\n | author = International Organization for Standardization\n | title = ISO/IEC 15444-3:2007, Information technology -- JPEG 2000 image coding system: Motion JPEG 2000\n | year = 2007\n | accessdate = 2009-06-11 }}</ref><ref>{{cite web | url=http://www.jpeg.org/jpeg2000/j2kpart3.html | title=Motion JPEG 2000 (Part 3) | author=JPEG | year=2007 | accessdate=2009-11-01 | deadurl=yes | archiveurl=https://www.webcitation.org/6ArTdULNs?url=http://www.jpeg.org/jpeg2000/j2kpart3.html | archivedate=2012-09-22 | df= }}</ref> It also defines a file format,<ref>{{cite web |url=http://www.itu.int/dms_pubrec/itu-t/rec/t/T-REC-T.802-200501-I!!SUM-HTM-E.htm |title=T.802 : Information technology – JPEG 2000 image coding system: Motion JPEG 2000 - Summary |author=ITU-T |accessdate=2010-09-28}}</ref> based on ISO base media file format (ISO 15444-12). Filename extensions for Motion JPEG 2000 video files are '''.mj2''' and '''.mjp2''' according to RFC 3745.\n\nIt is an open [[International Organization for Standardization|ISO]] standard and an advanced update to [[MJPEG]] (or MJ), which was based on the legacy [[JPEG]] format. Unlike common video formats, such as [[MPEG-4 Part 2]], [[WMV]], and [[H.264/MPEG-4 AVC|H.264]], MJ2 does not employ temporal or inter-frame compression. Instead, each frame is an independent entity encoded by either a lossy or lossless variant of JPEG 2000. Its physical structure does not depend on time ordering, but it does employ a separate profile to complement the data. For audio, it supports [[LPCM]] encoding, as well as various MPEG-4 variants, as \"raw\" or complement data.<ref>[http://www.jpeg.org/jpeg2000/j2kpart3.html Motion JPEG 2000 (Part 3)] {{webarchive|url=https://www.webcitation.org/6ArTdULNs?url=http://www.jpeg.org/jpeg2000/j2kpart3.html |date=2012-09-22 }}</ref>\n\nMotion JPEG 2000 (often referenced as MJ2 or MJP2) was considered as a digital archival format<ref>[http://www.digitalpreservation.gov/formats/fdd/fdd000127.shtml Motion JPEG 2000 mj2 File Format]. Sustainability of Digital Formats Planning for Library of Congress Collections.</ref> by the [[Library of Congress]].\nIn June 2013, in an interview with Bertram Lyons from the Library of Congress for ''[[The New York Times Magazine]]'', about \"Tips on Archiving Family History\", codecs such as [[FFV1]], [[H264]] or [[ProRes|Apple ProRes]] are mentioned, but JPEG 2000 is not.<ref>[[The New York Times]]: [https://www.nytimes.com/2013/06/12/booming/tips-on-archiving-family-history-part-3.html Interview with Bert Lyons (LoC) about \"Tips on Archiving Family History\"], June 2013</ref>\n\n===ISO base media file format===\nISO/IEC 15444-12 is identical with ISO/IEC 14496-12 (MPEG-4 Part 12) and it defines [[ISO base media file format]]. For example, Motion JPEG 2000 file format, [[MP4]] file format or [[3GP]] file format are also based on this ISO base media file format.<ref name=\"mpeg4part12\">{{cite journal\n  | author = International Organization for Standardization\n  | title =  ISO Base Media File Format white paper - Proposal\n  | publisher = archive.org\n  | date = April 2006\n  | url = http://www.chiariglione.org/mpeg/technologies/mp04-ff/index.htm\n   | accessdate = 2009-12-26 |archiveurl = https://web.archive.org/web/20080714101745/http://www.chiariglione.org/mpeg/technologies/mp04-ff/index.htm |archivedate = 2008-07-14\n  | authorlink = International Organization for Standardization}}</ref><ref name=\"mpeg4part12--old\">{{cite journal\n  | author = International Organization for Standardization\n  | title =  MPEG-4 File Formats white paper - Proposal\n  | publisher = archive.org\n  | date = October 2005\n  | url = http://www.chiariglione.org/mpeg/technologies/mp04-ff/index.htm\n   | accessdate = 2009-12-26 |archiveurl = https://web.archive.org/web/20080115035235/http://www.chiariglione.org/mpeg/technologies/mp04-ff/index.htm |archivedate = 2008-01-15\n  | authorlink = International Organization for Standardization}}</ref><ref name=\"mpeg4part12--new\">{{cite journal\n  | author = International Organization for Standardization\n  | title =  ISO Base Media File Format white paper - Proposal\n  | publisher = chiariglione.org\n  | date = October 2009\n  | url = http://mpeg.chiariglione.org/technologies/mpeg-4/mp04-ff/index.htm\n  | accessdate = 2009-12-26\n  | authorlink = International Organization for Standardization }}</ref><ref name=\"ISO-14496-12\">{{cite web\n | url = http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=38539\n | author = International Organization for Standardization\n | title = ISO/IEC 14496-12:2004, Information technology -- Coding of audio-visual objects -- Part 12: ISO base media file format\n | year = 2004\n | accessdate = 2009-06-11 }}</ref><ref name=\"ISO-15444-12\">{{cite web\n | url = http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=51537\n | author = International Organization for Standardization\n | title = ISO/IEC 15444-12:2008, Information technology -- JPEG 2000 image coding system -- Part 12: ISO base media file format\n | year = 2008\n | accessdate = 2009-06-11 }}</ref>\n\n===GML JP2 georeferencing===\nThe [[Open Geospatial Consortium]] (OGC) has defined a [[metadata (computing)|metadata]] standard for [[georeferencing]] JPEG 2000 images with embedded [[XML]] using the [[Geography Markup Language]] (GML) format: ''GML in JPEG 2000 for Geographic Imagery Encoding (GMLJP2)'', version 1.0.0, dated 2006-01-18.<ref name=\"gmljp2\">Open Geospatial Consortium [http://www.opengeospatial.org/standards/gmljp2 GMLJP2 Home Page]</ref>  Version 2.0, entitled ''GML in JPEG 2000 (GMLJP2) Encoding Standard Part 1: Core'' was approved 2014-06-30.<ref name=\"gmljp2\"/>\n\nJP2 and JPX files containing GMLJP2 markup can be located and displayed in the correct position on the Earth's surface by a suitable [[Geographic Information System]] (GIS), in a similar way to [[GeoTIFF]] images.\n\n==Application support==\n\n===Applications===\n{| class=\"wikitable sortable\" style=\"margin: 1em auto 1em auto\"\n|+ Application support for JPEG 2000\n!rowspan=\"2\" | Program\n!colspan=\"2\" | Basic {{r|group=Note|SupportLevel}}\n!colspan=\"2\" | Advanced {{r|group=Note|SupportLevel}}\n!rowspan=\"2\" | License\n|-\n!Read\n!Write\n!Read\n!Write\n|-\n! {{rh}} | [[ACDSee]]\n|{{yes}}\n|{{yes}}\n|?\n|?\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[Adobe Photoshop]] {{r|group=Note|PhotoShopsupport}}\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[Adobe Lightroom]]\n|{{no}}\n|{{no}}\n|{{no}}\n|{{no}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | Apple [[iPhoto]]\n|{{yes}}\n|{{no}}\n|{{yes}}\n|{{no}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | Apple [[Preview (software)|Preview]]  {{r|group=Note|Previewsupport}}\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | Autodesk [[AutoCAD]]\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|{{dunno}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[BAE Systems]] CoMPASS\n|{{yes}}\n|{{no}}\n|{{yes}}\n|{{no}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[Blender (software)|Blender]]<ref>{{cite web|url=http://www.blender.org/development/release-logs/blender-249/ |title=Blender 2.49 |date=2009-05-30 |accessdate=2010-01-20 |deadurl=yes |archiveurl=https://web.archive.org/web/20090611183623/http://www.blender.org/development/release-logs/blender-249/ |archivedate=2009-06-11 |df= }}</ref>\n|{{yes}}\n|{{yes}}\n|{{dunno}}\n|{{dunno}}\n| [[GNU General Public License|GPL]]\n|-\n! {{rh}} | Phase One [[Capture One]]\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[Chasys Draw IES]]\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|[[Freeware]]\n|-\n! {{rh}} | [[CineAsset]]\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[Photodex|CompuPic Pro]]\n|{{yes}}\n|{{yes}}\n|{{dunno}}\n|{{dunno}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[Corel Photo-Paint]]\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[Daminion]]<ref name=\"daminion.net\">{{cite web|url=http://daminion.net|title=Daminion}}</ref>\n|{{yes}}\n|{{no}}\n|{{yes}}\n|{{no}}\n|[[Proprietary software|Proprietary]]\n\n|-\n! {{rh}} | [[darktable]]<ref name=\"darktable.org\">{{cite web|url=http://darktable.org|title=the darktable project}}</ref>\n|{{dunno}}\n|{{yes}}\n|{{dunno}}\n|{{dunno}}\n|[[GNU General Public License|GPL]]\n|-\n! {{rh}} | [[DBGallery]]\n|{{yes}}\n|{{no}}\n|{{yes}}\n|{{no}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[digiKam]]<ref name=\"kde-jpeg2000\">{{cite web |url=http://docs.kde.org/development/en/extragear-graphics/digikam/using-fileformatsupport.html |publisher=docs.kde.org |title=The digiKam Handbook - Supported File Formats |accessdate=2010-01-20 |deadurl=yes |archiveurl=https://web.archive.org/web/20090901120259/http://docs.kde.org/development/en/extragear-graphics/digikam/using-fileformatsupport.html |archivedate=2009-09-01 |df= }}</ref><ref>{{cite web |url=http://docs.kde.org/development/en/extragear-graphics/showfoto/using-fileformatsupport.html |title=The Showfoto Handbook - Supported File Formats |accessdate=2010-01-20 |deadurl=yes |archiveurl=https://web.archive.org/web/20110213230715/http://docs.kde.org/development/en/extragear-graphics/showfoto/using-fileformatsupport.html |archivedate=2011-02-13 |df= }}</ref> ([[KDE]]<ref name=\"kde3\">{{cite web |url=http://techbase.kde.org/Development/Architecture/KDE3/Imaging_and_Animation |title=Development/Architecture/KDE3/Imaging and Animation |accessdate=2010-01-20}}</ref>)\n|{{yes}}\n|{{yes}}\n|{{dunno}}\n|{{dunno}}\n|[[GNU General Public License|GPL]]\n|-\n! {{rh}} | [[ECognition]]\n|{{yes}}\n|{{yes}}\n|{{dunno}}\n|{{dunno}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[ENVI (software)|ENVI]]\n|{{yes}}\n|{{yes}}\n|{{dunno}}\n|{{dunno}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[ERDAS IMAGINE]]\n|{{yes}}\n|{{yes}}\n|{{dunno}}\n|{{dunno}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[Evince|evince (PDF 1.5 embedding)]]\n|{{yes}}\n|{{no}}\n|{{no}}\n|{{no}}\n|[[GPL v2]]\n|-\n! {{rh}} | [[FastStone Image Viewer]]\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|[[Freeware]]\n|-\n! {{rh}} | [[FastStone MaxView]]\n|{{yes}}\n|{{no}}\n|{{yes}}\n|{{no}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[Fotografix|FotoGrafix 2.0]]\n|{{no}}\n|{{no}}\n|{{no}}\n|{{no}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[FotoSketcher 2.70]]\n|{{no}}\n|{{no}}\n|{{no}}\n|{{no}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[GIMP|GIMP 2.10]]\n|{{yes}}<ref>{{cite web |url=http://www.gimp.org/release-notes/gimp-2.7.html |title=GIMP 2.7 RELEASE NOTES |author=The GIMP Team |date=2009-08-16 |accessdate=2009-11-17}}</ref>\n|{{no}}\n|{{dunno}}\n|{{no}}\n| [[GNU General Public License|GPL]]\n|-\n! {{rh}} | [[Global Mapper]]\n|{{yes}}\n|{{yes}}\n|{{no}}\n|{{no}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[GNOME Web]]\n|{{yes}}\n| -\n|{{dunno}}\n| -\n| [[GNU General Public License|GPL]]\n\n|-\n! {{rh}} | [[Google Chrome]]\n|{{no}}\n|{{no}}\n|{{no}}\n|{{no}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[GraphicConverter]]\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|{{dunno}}\n|[[Shareware]]\n|-\n! {{rh}} | [[Gwenview]] ([[KDE]]<ref name=\"kde3\" />)\n|{{yes}}\n|{{yes}}\n|{{dunno}}\n|{{dunno}}\n|[[GNU General Public License|GPL]]\n|-\n! {{rh}} | [[IDL (programming language)|IDL]]\n|{{yes}}\n|{{yes}}\n|{{dunno}}\n|{{dunno}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[ImageMagick]]\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|[http://www.imagemagick.org/script/license.php ImageMagick License]\n|-\n! {{rh}} | [[Imagine (image viewer)|Imagine]] (with a plugin)<ref>{{cite web |url=http://www.nyam.pe.kr/dev/imagine/ |title=Imagine: Freeware Image & Animation Viewer for Windows |author=Sejin Chun |accessdate=2018-05-02}}</ref>\n|{{yes}}\n|{{no}}\n|{{no}}\n|{{no}}\n|[[Freeware]]\n|-\n! {{rh}} | [[IrfanView]]\n|{{yes}}\n|{{yes}}\n|{{no}}\n|{{no}}\n|[[Freeware]]\n|-\n! {{rh}} | [[KolourPaint]] ([[KDE]]<ref name=\"kde3\" />)\n|{{yes}}\n|{{yes}}\n|{{dunno}}\n|{{dunno}}\n|2-clause [[BSD Licenses|BSD]]\n|-\n! {{rh}} | [[Mathematica]]\n|{{yes}}\n|{{yes}}\n|{{no}}\n|{{no}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[Matlab]]\n|via toolbox\n|via toolbox\n|via toolbox\n|via toolbox\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[Mozilla Firefox]]\n|{{no}} {{r|group=Note|FirefoxBug}}\n|{{no}}\n|{{no}}\n|{{no}}\n|[[Mozilla Public License|MPL]]\n|-\n! {{rh}} | [[Opera (web browser)|Opera]]\n|via [[QuickTime]]\n| -\n|{{dunno}}\n| -\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[Paint Shop Pro]]\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[PhotoFiltre 7.1]]\n|{{no}}\n|{{no}}\n|{{no}}\n|{{no}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[PhotoLine]]\n|{{yes}}\n|{{yes}}\n|{{dunno}}\n|{{dunno}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | Pixel image editor\n|{{yes}}\n|{{yes}}\n|{{dunno}}\n|{{dunno}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[Preview (macOS)|Preview]]\n|{{yes}}\n|{{yes}}\n|{{dunno}}\n|{{dunno}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[QGIS]] (with a plugin)\n|{{yes}}\n|{{yes}}\n|{{dunno}}\n|{{dunno}}\n|[[GPL]]\n|-\n! {{rh}} | [[Safari (web browser)|Safari]]\n|{{yes}}\n| -\n|{{dunno}}\n| -\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[SilverFast]]\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[XnView]]\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[Ziproxy]]\n|{{yes}}\n|{{yes}}\n|{{no}}\n|{{no}}\n|[[GPL]]\n|}\n{{reflist|group=Note|refs=\n<ref name=SupportLevel>'''basic''' and '''advanced''' support refer to conformance with, respectively, '''Part1''' and '''Part2''' of the JPEG 2000 Standard.</ref>\n<ref name=PhotoShopsupport>Adobe Photoshop CS2 and CS3's official JPEG 2000 plug-in package is not installed by default and must be manually copied from the install disk/folder to the Plug-Ins > File Formats folder.</ref>\n<ref name=Previewsupport>Tested with Preview.app 7.0 in Mac OS 10.9</ref>\n<ref name=FirefoxBug>Mozilla support for JPEG 2000 was requested in April 2000, but the report was closed as WONTFIX in August 2009.[https://bugzilla.mozilla.org/show_bug.cgi?id=36351] There is an extension that adds support to older versions of Firefox.[http://eschew.org/test/jp2/xpi/]</ref>\n}}\n\n===Libraries===\n{| class=\"wikitable sortable\" style=\"margin: 1em auto 1em auto\"\n|+ Library support for JPEG 2000\n!rowspan=\"2\" | Program\n!colspan=\"2\" | Basic\n!colspan=\"2\" | Advanced\n!rowspan=\"2\" | Language\n!rowspan=\"2\" | License\n|-\n!Read\n!Write\n!Read\n!Write\n|-\n! {{rh}} | [[Comprimato]]\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|[[C (programming language)|C]], [[C++]]\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[ECW (file format)|ERDAS ECW JPEG2000 SDK]]\n|{{yes}}\n|{{yes}}\n|{{dunno}}\n|{{dunno}}\n|[[C (programming language)|C]], [[C++]]\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[Fastvideo SDK]]\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|[[C (programming language)|C]], [[C++]]\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[FFmpeg]]\n|{{yes}}\n|{{yes}}\n|{{dunno}}\n|{{dunno}}\n|[[C (programming language)|C]]\n|[[LGPL]]\n|-\n! {{rh}} | [[Grok (JPEG 2000)|Grok]]\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|[[C (programming language)|C]], [[C++]]\n|[[Affero General Public License|AGPL]]\n|-\n! {{rh}} | [[J2K-Codec]]\n|{{yes}}\n|{{no}}\n|{{yes}}\n|{{no}}\n|[[C++]]\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[JasPer]]\n|{{yes}} {{r|group=Note|JasperSupport}}\n|{{yes}}\n|{{no}}\n|{{no}}\n|[[C (programming language)|C]]\n|[[MIT License]]-style\n|-\n! {{rh}} | [[Kakadu (software)|Kakadu]]\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|[[C++]]\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[LEADTOOLS]]\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|[[C++]], [[.NET Framework|.NET]]\n|[[Proprietary software|Proprietary]]\n|-\n! {{rh}} | [[OpenJPEG]]\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|{{yes}}\n|[[C (programming language)|C]]\n|[[BSD licenses|BSD]]\n|-\n! {{rh}} | [[BOI codec]]\n|{{yes}}\n|{{yes}}\n|{{no}}\n|{{no}}\n|[[Java (programming language)|Java]]\n|[[BOI License]]\n|}\n{{reflist|group=Note|refs=\n<ref name=JasperSupport>Jasper does not handle 16bits properly—see for example [http://bugs.debian.org/681234#17 Fwd: &#91;Dcmlib&#93; jpeg 2000].</ref>\n}}\n\n==See also==\n* [[Digital cinema]]\n* [[Comparison of graphics file formats]]\n* [[Video compression picture types]]\n* [[DjVu]]&nbsp;– a compression format that also uses wavelets and that is designed for use on the web.\n* [[ECW (file format)|ECW]]&nbsp;– a wavelet compression format that compares well to JPEG 2000.\n* [[High bit rate media transport]]\n* [[QuickTime]]&nbsp;– a multimedia framework, application and web browser plugin developed by Apple, capable of encoding, decoding and playing various multimedia files (including JPEG 2000 images by default).\n* [[MrSID]]&nbsp;– a wavelet compression format that compares well to JPEG 2000\n* [[Progressive Graphics File|PGF]]&nbsp;– a fast wavelet compression format that compares well to JPEG 2000\n* [[JPIP]]&nbsp;– JPEG 2000 Interactive Protocol\n* [[Wavelet]]\n* [[WebP]]&nbsp;– an image format related to [[WebM]], supporting lossy and lossless compression\n\n==Notes==\n{{Reflist|colwidth=30em}}\n\n==References==\n* [http://www.jpeg.org/jpeg2000/index.html Official JPEG 2000 page]\n* [https://web.archive.org/web/20060702065150/http://www.jpeg.org/jpeg2000/CDs15444.html Final Committee Drafts of JPEG 2000 standard] (as the official JPEG 2000 standard is not freely available, the final drafts are the most accurate freely available documentation about this standard)\n* [https://web.archive.org/web/20020811020128/http://www.crc.ricoh.com/~gormish/jpeg2000.html Gormish Notes on JPEG 2000]\n* [https://web.archive.org/web/20041108232228/http://www.rii.ricoh.com/%7egormish/pdf/dcc2000_jpeg2000_note.pdf Technical overview of JPEG 2000] ([[Portable Document Format|PDF]])\n* [https://www.intopix.com/Ressources/WPs_and_Sc_Pub/intoPIX%20-%20Pocket%20book%20about%20JPEG%202000.pdf Everything you always wanted to know about JPEG 2000 - published by intoPIX in 2008] ([[Portable Document Format|PDF]])\n\n== External links ==\n* {{Official website|https://jpeg.org/jpeg2000/}}\n\n{{Graphics file formats}}\n{{CCSDS}}\n{{Compression formats}}\n{{ISO standards}}\n{{List of International Electrotechnical Commission standards}}\n\n{{DEFAULTSORT:Jpeg 2000}}\n[[Category:JPEG|2000]]\n[[Category:ISO/IEC standards]]\n[[Category:ITU-T recommendations]]\n[[Category:Graphics file formats]]\n[[Category:Wavelets]]\n[[Category:Image compression]]\n[[Category:Open formats]]"
    },
    {
      "title": "Legendre wavelet",
      "url": "https://en.wikipedia.org/wiki/Legendre_wavelet",
      "text": "{{Short description|Type of wavelet}}\nIn [[functional analysis]], compactly supported [[wavelet]]s derived from [[Legendre polynomials]] are termed '''Legendre wavelets''' or spherical harmonic wavelets.<ref>Lira et al</ref> Legendre functions have widespread applications in which [[spherical coordinate system]] is appropriate.<ref name=\"Gradsh\">{{cite book |author-first1=Izrail Solomonovich |author-last1=Gradshteyn |author-link1=Izrail Solomonovich Gradshteyn |author-first2=Iosif Moiseevich |author-last2=Ryzhik |author-link2=Iosif Moiseevich Ryzhik |author-first3=Yuri Veniaminovich |author-last3=Geronimus |author-link3=Yuri Veniaminovich Geronimus |author-first4=Michail Yulyevich |author-last4=Tseytlin |author-link4=Michail Yulyevich Tseytlin |author-first5=Alan |author-last5=Jeffrey |editor-first1=Daniel |editor-last1=Zwillinger |editor-first2=Victor Hugo |editor-last2=Moll |translator=Scripta Technica, Inc. |title=Table of Integrals, Series, and Products |publisher=[[Academic Press, Inc.]] |date=2015 |orig-year=October 2014 |edition=8 |language=English |isbn=0-12-384933-0 |id={{isbn|978-0-12-384933-5}} |lccn=2014010276 <!-- |url=http://books.google.com/books?id=NjnLAwAAQBAJ |access-date=2016-02-21-->|title-link=Gradshteyn and Ryzhik}}</ref><ref name=\"Colomer\">Colomer and Colomer</ref><ref>Ramm and Zaslavsky</ref> As with many wavelets there is no nice analytical formula for describing these harmonic spherical wavelets. The [[low-pass filter]] associated to Legendre [[multiresolution analysis]] is a [[finite impulse response]] (FIR) filter.\n\nWavelets associated to FIR filters are commonly preferred in most applications.<ref name=Colomer/> An extra appealing feature is that the Legendre filters are ''linear phase'' FIR (i.e. multiresolution analysis associated with [[linear phase]] filters). These wavelets have been implemented on [[MATLAB]] (wavelet toolbox). Although being compactly supported wavelet, legdN are not orthogonal (but for ''N'' = 1).<ref>Herley and Vetterli</ref>\n\n== Legendre multiresolution filters  ==\nAssociated Legendre polynomials are the colatitudinal part of the spherical harmonics which are common to all separations of Laplace's equation in spherical polar coordinates.<ref name=Gradsh/>  The radial part of the solution varies from one potential to another, but the harmonics are always the same and are a consequence of spherical symmetry. Spherical harmonics <math>P_n(z)</math> are solutions of the Legendre <math>2^{nd}</math>-order differential equation, ''n'' integer:\n\n: <math>\\left (1-z^2 \\right ) \\frac {d^2y} {dz^2} - 2z \\frac {dy} {dz} + n(n+1)y=0.</math>\n\n<math>P_n(\\cos(\\theta))</math> polynomials can be used to define the smoothing filter <math>H(\\omega)</math> of a multiresolution analysis (MRA).<ref name=Mallat>Mallat</ref> Since the appropriate boundary conditions for an MRA are <math>|H(0)|=1</math> and <math>|H(\\pi)|=0</math>, the smoothing filter of an MRA can be defined so that the magnitude of the low-pass <math>|H(\\omega)|</math> can be associated to Legendre polynomials according to: <math>\\nu=2n+1.</math>\n\n: <math>|H_{\\nu}(\\omega)|= \\left | \\frac {P_{\\nu} \\left ( \\cos \\left ( \\frac{\\omega}{2} \\right ) \\right ) } {P_{\\nu} \\cos (0)} \\right |</math>\n\nIllustrative examples of filter transfer functions for a Legendre MRA are shown in figure 1, for <math>\\nu=1,3,5.</math> A low-pass behaviour is exhibited for the filter ''H'', as expected. The number of zeroes within <math>- \\pi < \\omega < \\pi</math> is equal to the degree of the Legendre polynomial. Therefore, the [[roll-off]] of side-lobes with frequency is easily controlled by the parameter <math>\\nu</math>.\n\n[[Image:Legendre MRA filter.svg|thumb|right|400px|'''Figure 1 - Magnitude of the transfer function for Legendre multiresolution smoothing filters. Filter <math>|H_{\\nu} (\\omega)|</math> for orders 1, 3, and 5.''']]\n\nThe low-pass filter transfer function is given by\n\n: <math>H_{\\nu} (\\omega)=-e^{-j \\nu \\frac {\\omega - \\pi} {2}} P_{\\nu}  \\left ( \\cos \\left ( \\tfrac{\\omega}{2} \\right ) \\right )</math>\n\nThe transfer function of the high-pass analysing filter <math>G_{\\nu} (\\omega)</math> is chosen according to [[Quadrature mirror filter]] condition,<ref name=Mallat/><ref>Vetterli and Herley</ref> yielding:\n\n: <math>H_{\\nu} (\\omega)=-e^{-j {(\\nu-2)} \\frac {\\omega} {2}} P_{\\nu} \\left ( \\sin \\left ( \\tfrac{\\omega}{2} \\right ) \\right )</math>\n\nIndeed, <math>|G_{\\nu}(0)|=0</math> and <math>|G_{\\nu}( \\pi)|=1</math>, as expected.\n\n== Legendre multiresolution filter coefficients  ==\n\nA suitable phase assignment is done so as to properly adjust the transfer function <math>H_{\\nu} (\\omega)</math> to the form\n\n:<math>H_{\\nu} (\\omega)= \\frac {1} {\\sqrt {2}} \\sum_{k \\in Z} h_k^{\\nu} e^{-j \\omega k}</math>\n\nThe filter coefficients <math>\\{ h_k \\}_{k \\in \\Z}</math> are given by:\n\n:<math>h_k^{\\nu}= - \\frac {\\sqrt {2}} {2^{2 \\nu}} \\binom{2k}{k} \\binom{2 \\nu -2k}{\\nu -k}</math>\n\nfrom which the symmetry:\n\n:<math>{h_k^{\\nu}}={h_{\\nu -k}^{\\nu}},</math>\n\nfollows. There are just <math>\\nu+1</math> non-zero filter coefficients on <math>H_n (\\omega)</math>, so that the Legendre wavelets have compact support for every odd integer <math>\\nu</math>.\n\n:::''Table I - Smoothing Legendre FIR filter coefficients for <math>\\nu=1,3,5</math> (<math>N</math> is the wavelet order.)'' \n{| border=\"1\" cellspacing=\"0\" cellpadding=\"5\" align=\"center\"\n|\n| <math>\\nu=1 (N =1 )</math> \n| <math>\\nu=3 (N=2)</math> \n| <math>\\nu=5 (N=3)</math> \n|-\n| <math>h_0</math>\n| <math>-   \\tfrac{\\sqrt {2}}{2}</math>\n| <math>- 5 \\tfrac{\\sqrt {2}}{16}</math>\n| <math>-63 \\tfrac{\\sqrt {2}}{256}</math>\n|-\n| <math>h_1</math>\n| <math>- \\tfrac{\\sqrt {2}}{2}</math>\n| <math>-3 \\tfrac{\\sqrt {2}}{16}</math>\n| <math>- 35 \\tfrac{\\sqrt {2}}{256}</math>\n|-\n| <math>h_2</math>\n| \n| <math>-3 \\tfrac{\\sqrt {2}}{16}</math>\n| <math>-30 \\tfrac{\\sqrt {2}}{256}</math>\n|-\n| <math>h_3</math>\n| \n| <math>- 5 \\tfrac{\\sqrt {2}}{16}</math>\n| <math>-30 \\tfrac{\\sqrt {2}}{256}</math>\n|-\n| <math>h_4</math>\n| \n| \n| <math>-35 \\tfrac{\\sqrt {2}}{256}</math>\n|-\n| <math>h_5</math>\n| \n| \n| <math>-63 \\tfrac{\\sqrt {2}}{256}</math>\n|-\n|}\n::: N.B. The minus signal can be suppressed.\n\n== MATLAB implementation of Legendre wavelets ==\n\nLegendre wavelets can be easily loaded into the [[MATLAB]] wavelet toolbox—The m-files to allow the computation of Legendre wavelet transform, details and filter are (freeware) available. The finite support width Legendre family is denoted by legd (short name). Wavelets: 'legdN'. The parameter ''N'' in the legdN family is found according to <math>2N = \\nu+1</math> (length of the MRA filters).\n\nLegendre wavelets can be derived from the low-pass reconstruction filter by an iterative procedure (the [[cascade algorithm]]). The wavelet has compact support and finite impulse response AMR filters (FIR) are used (table 1). The first wavelet of the Legendre's family is exactly the well-known [[Haar wavelet]]. Figure 2 shows an emerging pattern that progressively looks like the wavelet's shape.\n\n[[Image:Figura legd2.jpg|thumb|none|500px|'''Figure 2 - Shape of Legendre Wavelets of degree <math>\\nu=3</math> (legd2) derived after 4 and 8 iteration of the cascade algorithm, respectively. Shape of Legendre Wavelets of degree <math>\\nu=5</math> (legd3) derived by the cascade algorithm after 4 and 8 iterations of the cascade algorithm, respectively.''']]\n\nThe Legendre wavelet shape can be visualised using the wavemenu command of MATLAB. Figure 3 shows legd8 wavelet displayed using MATLAB<sup>TM</sup>. Legendre Polynomials are also associated with windows families.<ref>Jaskula</ref>\n\n[[Image:Figura legd3.jpg|thumb|none|300px|''' Figure 3 - legd8 wavelet display over MATLAB<sup>TM</sup> using the wavemenu command. ''']]\n\n== Legendre wavelet packets ==\n\n[[Wavelet packets]] (WP) systems derived from Legendre wavelets can also be easily accomplished. Figure 5 illustrates the WP functions derived from legd2. \n[[Image:Figura legd5.jpg|thumb|none|350px|'''Figure 5 - Legendre (legd2) Wavelet Packets W system functions: WP from 0 to 9.''']]\n\n== References ==\n{{reflist|32em}}\n\n==Bibliography==\n* M.M.S. Lira, H.M. de Oliveira, M.A. Carvalho Jr, R.M.C.Souza,  Compactly Supported Wavelets Derived from Legendre Polynomials: Spherical Harmonic Wavelets,  In: ''Computational Methods in Circuits and Systems Applications'', N.E. Mastorakis, I.A. Stahopulos, C. Manikopoulos, G.E. Antoniou, V.M. Mladenov, I.F. Gonos Eds., WSEAS press, pp.&nbsp;211–215, 2003. {{isbn|960-8052-88-2}}. Available at [http://www2.ee.ufpe.br/codec/Legendre_WSEAS.PDF ee.ufpe.br]\n* A. A. Colomer and A. A. Colomer, Adaptive ECG Data Compression Using Discrete Legendre Transform, ''Digital Signal Processing'', 7, 1997, pp.&nbsp;222–228.\n* A.G. Ramm, A.I. Zaslavsky, X-Ray Transform, the Legendre Transform, and Envelopes, ''J. of Math. Analysis and Appl''., 183, pp.&nbsp;528–546, 1994.\n* C. Herley, M. Vetterli, Orthogonalization of Compactly Supported Wavelet Bases, ''IEEE Digital Signal Process. Workshop'', 13-16 Sep., pp.&nbsp;1.7.1-1.7.2, 1992.\n* S. Mallat, A Theory for Multiresolution Signal Decomposition: The Wavelet Representation, ''IEEE Transactions on Pattern Analysis and Machine Intelligence'', 11, July pp.&nbsp;674–693, 1989.\n* M. Vetterli, C. Herly, Wavelets and Filter Banks: Theory and Design, ''IEEE Trans. on Acoustics, Speech, and Signal Processing'', 40, 9, p.&nbsp;2207, 1992.\n* M. Jaskula, New Windows Family Based on Modified Legendre Polynomials, ''IEEE Instrum. And Measurement Technol. Conf.'', Anchorage, AK, May, 2002, pp.&nbsp;553–556.\n\n[[Category:Wavelets]]\n[[Category:Functional analysis]]"
    },
    {
      "title": "List of wavelet-related transforms",
      "url": "https://en.wikipedia.org/wiki/List_of_wavelet-related_transforms",
      "text": "A list of [[wavelet]] related transforms:\n\n* [[Continuous wavelet transform]] (CWT)\n* [[Discrete wavelet transform]] (DWT)\n* [[Multiresolution analysis]] (MRA)\n* [[Lifting scheme]]\n* [[Binomial QMF]] (BQMF)\n* [[Fast wavelet transform]] (FWT)\n* [[Complex wavelet transform]]\n* Non or [[undecimated wavelet transform]], the downsampling is omitted\n* [[Newland transform]], an orthonormal basis of wavelets is formed from appropriately constructed top-hat filters in frequency space\n* [[Wavelet packet decomposition]] (WPD), detail coefficients are decomposed and a variable tree can be formed\n* [[Stationary wavelet transform]] (SWT), no downsampling and the filters at each level are different\n* [[e-decimated discrete wavelet transform]], depends on if the even or odd coefficients are selected in the downsampling\n* [[Second generation wavelet transform]] (SGWT), filters and wavelets are not created in the frequency domain\n* [[Dual-tree complex wavelet transform]] (DTCWT), two trees are used for decomposion to produce the real and complex coefficients\n* [http://www.laurent-duval.eu/siva-wits-where-is-the-starlet.html WITS: Where Is The Starlet], a collection of a hundredth of wavelet names in -let and associated multiscale, directional, geometric, representations, from activelets to x-lets through [[Bandelet (computer science)|bandelets]], [[Chirplet transform|chirplets]], [[contourlet]]s, [[curvelet]]s, [[noiselet]]s, wedgelets ...\n\n\n\n[[Category:Wavelets]]\n[[Category:Mathematics-related lists|Wavelet-related transforms]]"
    },
    {
      "title": "Mathieu wavelet",
      "url": "https://en.wikipedia.org/wiki/Mathieu_wavelet",
      "text": "The Mathieu equation is a linear [[second-order differential equation]] with periodic coefficients. The French mathematician, E. Léonard Mathieu, first introduced this family of differential equations, nowadays termed Mathieu equations, in his “Memoir on vibrations of an elliptic membrane” in 1868. \"Mathieu functions are applicable to a wide variety of physical phenomena, e.g., diffraction, amplitude distortion, inverted pendulum, stability of a floating body, radio frequency quadrupole, and vibration in a medium with modulated density\"<ref>L. Ruby, “Applications of the Mathieu Equation,” Am. J. Phys., vol. 64, pp. 39–44, Jan. 1996</ref>\n\n== Elliptic-cylinder wavelets ==\n\nThis is a wide family of wavelet system that provides a [[multiresolution analysis]]. The magnitude of the detail and smoothing filters corresponds to first-kind [[Mathieu function]]s with odd characteristic exponent. The number of notches of these filters can be easily designed by choosing the characteristic exponent.  Elliptic-cylinder wavelets derived by this method <ref>M.M.S. Lira, H.M. de Oiveira, R.J.S. Cintra. Elliptic-Cylindrical Wavelets: The Mathieu Wavelets,''IEEE Signal Processing Letters'', vol.11, n.1, January, pp.&nbsp;52&ndash;55, 2004.</ref> possess potential application in the fields of [[optics]] and [[electromagnetism]] due to its symmetry.\n\n== Mathieu differential equations ==\n\nMathieu's equation is related to the wave equation for the elliptic cylinder. In 1868, the French mathematician [[Émile Léonard Mathieu]] introduced a family of differential equations nowadays termed [[Mathieu equation]]s.<ref>É. Mathieu, Mémoire sur le mouvement vibratoire d'une membrane de forme elliptique, ''J. Math. Pures Appl''., vol.13, 1868, pp.&nbsp;137&ndash;203.</ref>\n\nGiven <math>a \\in \\mathbb{R}, q \\in \\mathbb{C}</math>, the Mathieu equation is given by\n\n: <math>\\frac {d^2 y} {dw^2} +(a-2q \\cos 2w )y=0.</math>\n\nThe Mathieu equation is a linear second-order differential equation with periodic coefficients. For ''q''&nbsp;=&nbsp;0, it reduces to the well-known harmonic oscillator, ''a'' being the square of the frequency.<ref>N.W. McLachlan, Theory and Application of Mathieu Functions, New York: Dover, 1964.</ref>\n\nThe solution of the Mathieu equation is the elliptic-cylinder harmonic, known as [[Mathieu functions]]. They have long been applied on a broad scope of wave-guide problems involving elliptical geometry, including:\n\n# analysis for weak guiding for step index elliptical core [[optical fibre]]s \n# power transport of elliptical [[wave guide]]s \n# evaluating radiated waves of elliptical [[horn antenna]]s \n# elliptical annular [[microstrip antenna]]s with arbitrary eccentricity <math>\\nu</math>) \n# scattering by a coated strip.\n\n== Mathieu functions: cosine-elliptic and sine-elliptic functions ==\n\nIn general, the solutions of Mathieu equation are not periodic. However, for a given ''q'', periodic solutions exist for infinitely many special values (eigenvalues) of ''a''. For several physically relevant solutions ''y'' must be periodic of period <math>\\pi</math> or <math>2\\pi</math>. It is convenient to distinguish even and odd periodic solutions, which are termed [[Mathieu function]]s of first kind.\n\nOne of four simpler types can be considered:  Periodic solution (<math>\\pi</math> or <math>2\\pi</math>) symmetry (even or odd).\n\nFor <math>q \\ne 0</math>, the only periodic solutions ''y'' corresponding to any characteristic value <math>a=a_r(q)</math> or <math>a=b_r(q)</math> have the following notations:\n\n''ce'' and ''se'' are abbreviations for cosine-elliptic and sine-elliptic, respectively. \n<blockquote>\n*Even periodic solution: \n:: <math>ce_r(\\omega,q)= \\sum_m A_{r,m} \\cos {m \\omega}\\text{ for }a=a_r(q)</math>\n\n*Odd periodic solution: \n:: <math>se_r(\\omega,q)= \\sum_m A_{r,m} \\sin {m \\omega}\\text{ for }a=b_r(q)</math>\n</blockquote>\nwhere the sums are taken over even (respectively odd) values of ''m'' if the period of ''y'' is <math>\\pi</math> (respectively <math>2\\pi</math>).\n\nGiven ''r'', we denote henceforth <math>A_{r,m}</math> by <math>A_m</math>, for short.\n\nInteresting relationships are found when <math>q \\to 0</math>, <math>r \\ne 0</math>:\n\n: <math>\\lim_{q \\to 0} ce_r(\\omega,q)= \\cos {r \\omega}</math>\n\n: <math>\\lim_{q \\to 0} se_r(\\omega,q)= \\sin {r \\omega}</math>\n\nFigure 1 shows two illustrative waveform of elliptic cosines, whose shape strongly depends on the parameters <math>\\nu</math> and ''q''.\n[[Image:Figura Mathieu1.PNG|thumb|600px|none|'''Figure 1. Some plots of <math>2\\pi</math>-periodic 1st kind even Mathieu functions. Elliptic cosines shape for the following set of parameters: a) <math>\\nu=1</math>=and ''q'' = 5 ;  b) <math>\\nu=5</math>=and ''q'' = 5.''']]\n\n== Multiresolution analysis filters and Mathieu's equation ==\n\n[[Wavelets]] are denoted by <math>\\psi(t)</math> and [[Wavelet#Scaling function|scaling functions]] by <math>\\phi(t)</math>, with corresponding spectra <math>\\Psi(\\omega)</math>  and  <math>\\Phi(\\omega)</math>, respectively.\n\nThe equation  <math>\\phi(t)= \\sqrt {2} \\sum_{n \\in Z} h_n \\phi(2t-n)</math>, which is known as the ''dilation'' or ''refinement equation'', is the chief relation determining a [[Multiresolution Analysis]] (MRA).\n\n<math>H(\\omega)= \\frac {1} {\\sqrt 2} \\sum_{k \\in Z} h_k e^{j \\omega k}</math> is the transfer function of the smoothing filter.\n\n<math>G(\\omega)= \\frac {1} {\\sqrt 2} \\sum_{k \\in Z} g_k e^{j \\omega k}</math> is the transfer function of the detail filter.\n\nThe transfer function of the \"detail filter\" of a Mathieu wavelet is\n\n: <math>G_{\\nu}(\\omega)=e^{j(\\nu-2)[ \\frac {\\omega - \\pi} {2}]}. \\frac {ce_{\\nu} ( \\frac {\\omega-\\pi} {2},q)} {{ce_{\\nu}(0,q)}}.</math>\n\nThe transfer function of the \"smoothing filter\" of a Mathieu wavelet is\n\n: <math>H_{\\nu}(\\omega)=-e^{j\\nu [ \\frac {\\omega} {2}]}. \\frac {ce_{\\nu}( \\frac {\\omega} {2},q)} {{ce_{\\nu}(0,q)}}.</math>\n\nThe characteristic exponent <math>\\nu</math> should be chosen so as to guarantee suitable initial conditions, i.e. <math>G_{\\nu}(0)=0</math> and <math>G_{\\nu}(\\pi)=1</math>, which are compatible with wavelet filter requirements. Therefore, <math>\\nu</math> must be odd.\n\nThe magnitude of the transfer function corresponds exactly to the modulus of an elliptic-sine:\n\nExamples of filter transfer function for a Mathieu MRA are shown in the figure 2. The value of ''a'' is adjusted to an ''eigenvalue'' in each case, leading to a periodic solution. Such solutions present a number of <math>\\nu</math> zeroes in the interval <math>0 \\le |\\omega | \\le \\pi</math>.\n\n[[Image:Figura Mathieu2.PNG|thumb|600px|none|'''Figure 2 - Magnitude of the transfer function for Mathieu multiresolution analysis filters. (smoothing filter <math>H_{\\nu}( \\omega)</math> and detail filter <math>G_{\\nu}( \\omega)</math> for a few Mathieu parameters.) (a) <math>\\nu=1</math>, ''q''=5, ''a'' = 1.85818754...;  (b) <math>\\nu=1</math>, ''q'' = 10, ''a'' = −2.3991424...; (c) <math>\\nu=5</math>, ''q'' = 10, ''a'' = 25.5499717...; (d) <math>\\nu=5</math>, ''q'' = 10, ''a'' = 27.70376873... ''']]\n\nThe ''G'' and ''H'' filter coefficients of Mathieu MRA can be expressed in terms of the values <math>\\{ A_{2 l +1} \\}_{l \\in Z}</math> of the Mathieu function as:\n\n: <math>\\frac {h_l} {\\sqrt{2}}=- \\frac {A_{|2l+1|}/2} {ce_{\\nu}(0,q)}</math>\n\n: <math>\\frac {g_l} {\\sqrt{2}}=(-1)^l \\frac {A_{|2l-3|}/2} {ce_{\\nu}(0,q)}</math>\n\nThere exist recurrence relations among the coefficients:\n\n: <math>(a-1-q)A_1-qA_3 =0</math>\n\n: <math>(a-m^2)A_m-q(A_{m-2}+A_{m+2} =0</math>\n\nfor <math>m \\ge 3</math>, ''m'' odd.\n\nIt is straightforward to show that   <math>h_{-l}=h_{|l|-1}</math>, <math> \\forall l>0</math>.\n\nNormalising conditions are  <math>\\sum_{k=- \\infty}^{k=+ \\infty} {h_k =-1}</math> and <math>\\sum_{k=- \\infty}^{k=+ \\infty} {(-1)^k h_k =0}</math>.\n\n== Waveform of Mathieu wavelets ==\n\nMathieu wavelets can be derived from the lowpass reconstruction filter by the [[cascade algorithm]]. Infinite Impulse Response filters ([[IIR filter]]) should be use since Mathieu wavelet has no [[compact support]]. Figure 3 shows emerging pattern that progressively looks like the wavelet's shape. Depending on the parameters ''a'' and ''q'' some waveforms (e.g. fig. 3b) can present a somewhat unusual shape.\n\n[[Image:Figura Mathieu3.PNG|thumb|600px|none|Figure 3: FIR-based approximation of Mathieu wavelets. Filter coefficients holding ''h''&nbsp;<&nbsp;10<sup>&minus;10</sup> were thrown away (20 retained coefficients per filter in both cases.) (a) Mathieu Wavelet with ''&nu;''&nbsp;=&nbsp;5 and ''q'' = 5 and (b) Mathieu wavelet with ''&nu;''&nbsp;=&nbsp;1 and ''q'' = 5.]]\n\n== References ==\n{{reflist}}\n\n{{DEFAULTSORT:Mathieu Wavelet}}\n[[Category:Wavelets]]"
    },
    {
      "title": "Meyer wavelet",
      "url": "https://en.wikipedia.org/wiki/Meyer_wavelet",
      "text": "[[File:Spectrum Meyer wavelet.png|thumb|Spectrum of the Meyer wavelet (numerically computed).]]\nThe '''Meyer wavelet''' is an orthogonal [[wavelet]] proposed by [[Yves Meyer]].<ref>{{cite book |last1=Meyer |first1=Yves |title=Ondelettes et opérateurs: Ondelettes |date=1990 |publisher=Hermann |isbn=9782705661250}}</ref> As a type of a [[continuous wavelet]], it has been applied in a number of cases, such as in [[adaptive filter]]s,<ref>{{cite journal |last1=Xu |first1=L. |last2=Zhang |first2=D. |last3=Wang |first3=K. |title=Wavelet-based cascaded adaptive filter for removing baseline drift in pulse waveforms |journal=IEEE Transactions on Biomedical Engineering |date=2005 |volume=52 |issue=11 |pages=1973–1975 |doi=10.1109/tbme.2005.856296 |pmid=16285403}}</ref> fractal [[random fields]],<ref>{{cite journal |last1=Elliott, Jr. |first1=F. W. |last2=Horntrop |first2=D. J. |last3=Majda |first3=A. J. |title=A Fourier-Wavelet Monte Carlo method for fractal random fields |journal=Journal of Computational Physics |date=1997 |volume=132 |issue=2 |pages=384–408 |doi=10.1006/jcph.1996.5647 |bibcode=1997JCoPh.132..384E}}</ref> and multi-fault classification.<ref>{{cite journal |last1=Abbasion |first1=S. |display-authors=etal |title=Rolling element bearings multi-fault classification based on the wavelet denoising and support vector machine |journal=Mechanical Systems and Signal Processing |date=2007 |volume=21 |issue=7 |pages=2933–2945 |doi=10.1016/j.ymssp.2007.02.003 |bibcode=2007MSSP...21.2933A}}</ref>\n\nThe Meyer wavelet is infinitely differentiable with infinite support and defined in frequency domain in terms of function <math> \\nu</math> as\n\n: <math> \\Psi(\\omega) := \\begin{cases}\n \\frac {1}{\\sqrt{2\\pi}} \\sin\\left(\\frac {\\pi}{2} \\nu \\left(\\frac{3|\\omega|}{2\\pi} -1\\right)\\right) e^{j\\omega/2} & \\text{if } 2 \\pi /3<|\\omega|< 4 \\pi /3, \\\\\n \\frac {1}{\\sqrt{2\\pi}} \\cos\\left(\\frac {\\pi}{2} \\nu \\left(\\frac{3| \\omega|}{4 \\pi}-1\\right)\\right) e^{j \\omega/2} & \\text{if } 4 \\pi /3<| \\omega|< 8 \\pi /3, \\\\\n 0 & \\text{otherwise},\n\\end{cases}</math>\n\nwhere\n: <math> \\nu (x) := \\begin{cases}\n 0 & \\text{if } x < 0, \\\\\n x & \\text{if } 0< x < 1, \\\\\n 1 & \\text{if } x > 1.\n\\end{cases}</math>\n\nThere are many different ways for defining this auxiliary function, which yields variants of the Meyer wavelet.\nFor instance, another standard implementation adopts\n: <math> \\nu (x) := \\begin{cases}\n x^4 (35 - 84x + 70x^2 - 20x^3) & \\text{if } 0 < x < 1, \\\\\n 0 & \\text{otherwise}.\n\\end{cases}</math>\n\n[[File:Spectrum Meyer scalefunction.png|thumb|Meyer scale function (numerically computed)| Meyer scale function]]\nThe Meyer scale function is given by\n\n: <math> \\Phi(\\omega) := \\begin{cases}\n \\frac{1}{\\sqrt{2\\pi}} & \\text{if } | \\omega| < 2 \\pi/3, \\\\\n \\frac{1}{\\sqrt{2\\pi}} \\cos\\left(\\frac{\\pi}{2} \\nu \\left(\\frac{3|\\omega|}{2\\pi} - 1\\right) \\right)  & \\text{if } 2\\pi/3 < |\\omega| < 4\\pi/3, \\\\\n 0 & \\text{otherwise}.\n\\end{cases}</math>\n\nIn the [[time domain]], the waveform of the Meyer mother-wavelet has the shape as shown in the following figure:\n[[File:Meyer wavelet.png|thumb|center|waveform of the Meyer wavelet (numerically computed)| Meyer wavelet]]\n\nIn 2015, Victor Vermehren Valenzuela and H. M. de Oliveira gave the explicit expressions of Meyer wavelet and scale functions:<ref>{{cite arxiv |last1=Valenzuela |first1=Victor Vermehren |last2=de Oliveira |first2=H. M. |title=Close expressions for Meyer Wavelet and Scale Function |eprint=1502.00161 |page=4 |class=stat.ME |year=2015}}</ref>\n\n: <math>\\phi(t) = \\begin{cases}\n \\frac{2}{3} + \\frac{4}{3\\pi} & t = 0, \\\\\n \\frac{\\sin(\\frac{2\\pi}{3}t) + \\frac{4}{3}t\\cos(\\frac{4\\pi}{3}t)}{\\pi t - \\frac{16\\pi}{9}t^3} & \\text{otherwise},\n\\end{cases}</math>\n\nand\n\n: <math>\\psi(t) = \\psi_1(t) + \\psi_2(t),</math>\n\nwhere\n\n: <math>\\psi_1(t) = \\frac{\\frac{4}{3\\pi}(t - \\frac12)\\cos[\\frac{2\\pi}{3}(t - \\frac12)] - \\frac{1}{\\pi}\\sin[\\frac{4\\pi}{3}(t - \\frac12)]}{(t - \\frac12) - \\frac{16}{9}(t - \\frac12)^3},</math>\n: <math>\\psi_2(t) = \\frac{\\frac{8}{3\\pi}(t - \\frac12)\\cos[\\frac{8\\pi}{3}(t - \\frac12)] + \\frac{1}{\\pi}\\sin[\\frac{4\\pi}{3}(t - \\frac12)]}{(t - \\frac12) - \\frac{64}{9}(t - \\frac12)^3}.</math>\n\n==References==\n{{reflist}}\n*{{cite book|last1=Daubechies|first1=Ingrid|title=Ten Lectures on Wavelets (CBMS-NSF conference series in applied mathematics)|date=September 1992|publisher=Springer-Verlag|isbn=978-0-89871-274-2|pages=117–119, 137–138, 152–155|edition=SIAM}}\n\n== External links ==\n{{Wiktionary|wavelet}}\n{{commons|Wavelet|Wavelet}}\n* [http://radio.feld.cvut.cz/matlab/toolbox/wavelet/ch06_a32.html wavelet toolbox]\n* [http://www.mathworks.com/help/wavelet/ref/meyer.html Matlab implementation]\n\n[[Category:Wavelets]]"
    },
    {
      "title": "Modified Morlet wavelet",
      "url": "https://en.wikipedia.org/wiki/Modified_Morlet_wavelet",
      "text": "'''Modified [[Mexican hat wavelet|Mexican hat]]''', '''Modified Morlet''' and '''Dark soliton''' or '''Darklet wavelets''' are derived from [[Hyperbolic function|hyperbolic]] (sech) (bright soliton) and hyperbolic tangent (tanh) (dark soliton) pulses. These functions are derived intuitively from the solutions of the nonlinear [[Schrödinger equation]] in the anomalous and normal dispersion regimes in a similar fashion to the way that the Morlet and the Mexican hat are derived.\nThe modified Morlet is defined as:\n<math>\\psi_2(t)=C_{\\psi_2}\\cos(\\omega_0 t){\\rm sech}(t)</math>\n\n[[Category:Wavelets]]\n\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Multiresolution analysis",
      "url": "https://en.wikipedia.org/wiki/Multiresolution_analysis",
      "text": "A '''multiresolution analysis''' ('''MRA''') or '''multiscale approximation''' ('''MSA''') is the design method of most of the practically relevant [[discrete wavelet transform]]s (DWT) and the justification for the [[algorithm]] of the [[fast wavelet transform]] (FWT). It was introduced in this context in 1988/89 by [[Stephane Mallat]] and [[Yves Meyer]] and has predecessors in the [[microlocal analysis]] in the theory of [[differential equation]]s (the ''ironing method'') and the [[pyramid (image processing)|pyramid method]]s of [[image processing]] as introduced in 1981/83 by Peter J. Burt, Edward H. Adelson and [http://www-prima.inrialpes.fr/Prima/Homepages/jlc/jlc.html James L. Crowley].\n\n==Definition==\nA multiresolution analysis of the [[Lp space|Lebesgue space]] <math>L^2(\\mathbb{R})</math> consists of a [[sequence]] of nested [[linear subspace|subspaces]]\n\n::<math>\\{0\\}\\dots\\subset V_1\\subset V_0\\subset V_{-1}\\subset\\dots\\subset V_{-n}\\subset V_{-(n+1)}\\subset\\dots\\subset L^2(\\R)</math>\n\nthat satisfies certain [[self-similarity]] relations in time-space and scale-frequency, as well as [[Complete metric space|completeness]] and regularity relations.\n\n* ''Self-similarity'' in ''time'' demands that each subspace ''V<sub>k</sub>'' is invariant under shifts by [[integer]] [[multiple (mathematics)|multiple]]s of ''2<sup>k</sup>''. That is, for each <math>f\\in V_k,\\; m\\in\\Z</math> the function ''g'' defined as <math>g(x)=f(x-m2^{k})</math> also contained in <math>V_k</math>.\n* ''Self-similarity'' in ''scale'' demands that all subspaces <math>V_k\\subset V_l,\\; k>l,</math> are time-scaled versions of each other, with [[Scaling (geometry)|scaling]] respectively [[Dilation (metric space)|dilation]] factor 2<sup>''k-l''</sup>. I.e., for each <math>f\\in V_k</math> there is a <math>g\\in V_l</math> with <math>\\forall x\\in\\R:\\;g(x)=f(2^{k-l}x)</math>.\n* In the sequence of subspaces, for ''k''>''l'' the space resolution 2<sup>''l''</sup> of the ''l''-th subspace is higher than the resolution 2<sup>''k''</sup> of the ''k''-th subspace. \n* ''Regularity'' demands that the model [[linear subspace|subspace]] ''V<sub>0</sub>'' be generated as the [[linear hull]] ([[algebraic closure|algebraically]] or even [[topologically closed]]) of the integer shifts of one or a finite number of generating functions <math>\\phi</math> or <math>\\phi_1,\\dots,\\phi_r</math>. Those integer shifts should at least form a frame for the subspace <math> V_0\\subset L^2(\\R) </math>, which imposes certain conditions on the decay at [[infinity]]. The generating functions are also known as '''[[Wavelet#Scaling function|scaling functions]]''' or '''[[father wavelets]]'''. In most cases one demands of those functions to be [[piecewise continuous]] with [[compact support]].\n* ''Completeness'' demands that those nested subspaces fill the whole space, i.e., their union should be [[dense set|dense]] in <math> L^2(\\R) </math>, and that they are not too redundant, i.e., their [[intersection]] should only contain the [[zero element]].\n\n== Important conclusions ==\nIn the case of one continuous (or at least with bounded variation) compactly supported scaling function with orthogonal shifts, one may make a number of deductions. The proof of existence of this class of functions is due to [[Ingrid Daubechies]].\n\nAssuming the scaling function has compact support, then <math>V_0\\subset V_{-1}</math> implies that there is a finite sequence of coefficients <math>a_k=2 \\langle\\phi(x),\\phi(2x-k)\\rangle</math> for <math>|k|\\leq N</math>, and <math>a_k=0</math> for <math>|k|>N</math>, such that\n \n:<math>\\phi(x)=\\sum_{k=-N}^N a_k\\phi(2x-k).</math>\n\nDefining another function, known as '''mother wavelet''' or just '''the wavelet'''\n:<math>\\psi(x):=\\sum_{k=-N}^N (-1)^k a_{1-k}\\phi(2x-k),</math>\none can show that the space <math>W_0\\subset V_{-1}</math>, which is defined as the (closed) linear hull of the mother wavelet's integer shifts, is the orthogonal complement to <math>V_0</math> inside <math>V_{-1}</math>.{{citation needed|date=April 2013}} Or put differently, <math>V_{-1}</math> is the [[orthogonal direct sum|orthogonal sum]] (denoted by <math>\\oplus</math>) of <math>W_0</math> and <math>V_0</math>. By self-similarity, there are scaled versions <math>W_k</math> of <math>W_0</math> and by completeness one has{{citation needed|date=April 2013}}\n:<math>L^2(\\R)=\\mbox{closure of }\\bigoplus_{k\\in\\Z}W_k,</math>\nthus the set\n:<math>\\{\\psi_{k,n}(x)=\\sqrt2^{-k}\\psi(2^{-k}x-n):\\;k,n\\in\\Z\\}</math>\nis a countable complete [[orthonormal wavelet]] basis in <math>L^2(\\R)</math>.\n\n==See also==\n*[[Multiscale modeling]]\n*[[Scale space]]\n*[[Time–frequency analysis]]\n*[[Wavelet]]\n\n== References ==\n* {{cite book|first=Charles K.|last=Chui|title=An Introduction to Wavelets|year=1992|publisher=Academic Press|location=San Diego|isbn=0-585-47090-1}}\n* {{cite book|author1-link=Ali Akansu|first1=A.N.|last1=Akansu|first2=R.A.|last2=Haddad|title=Multiresolution signal decomposition: transforms, subbands, and wavelets|publisher=Academic Press|year=1992|isbn=978-0-12-047141-6}}\n* Crowley, J. L., (1982). [http://www-prima.inrialpes.fr/Prima/Homepages/jlc/papers/Crowley-Thesis81.pdf A Representations for Visual Information], Doctoral Thesis, Carnegie-Mellon University, 1982.\n* {{cite book|author1-link=C. Sidney Burrus|first1=C.S.|last1=Burrus|first2=R.A.|last2=Gopinath|first3=H.|last3=Guo|title=Introduction to Wavelets and Wavelet Transforms: A Primer|publisher=Prentice-Hall|year=1997|isbn=0-13-489600-9}}\n* {{cite book|first=S.G.|last=Mallat|url=http://www.cmap.polytechnique.fr/~mallat/book.html|title=A Wavelet Tour of Signal Processing|publisher=Academic Press|year=1999|isbn=0-12-466606-X}}\n\n== External links ==\n\n[[Category:Time–frequency analysis]]\n[[Category:Wavelets]]"
    },
    {
      "title": "Non-linear multi-dimensional signal processing",
      "url": "https://en.wikipedia.org/wiki/Non-linear_multi-dimensional_signal_processing",
      "text": "{{Orphan|date=November 2015}}In [[signal processing]], '''nonlinear multidimensional signal processing (NMSP)''' covers all signal processing using nonlinear multidimensional signals and systems. Nonlinear [[multidimensional signal processing]] is a subset of signal processing ([[multidimensional signal processing]]). Nonlinear multi-dimensional systems can be used in a broad range such as imaging,<ref name=\":1\" /> teletraffic, communications, hydrology, geology, and economics. [[Nonlinear system]]s cannot be treated as [[linear system]]s, using [[Fourier transform]]ation and [[Wavelet|wavelet analysis]]. Nonlinear systems will have [[Chaos theory|chaotic behavior]], limit cycle, [[Steady State theory|steady state]], [[Bifurcation theory|bifurcation]], multi-stability and so on. Nonlinear systems do not have a canonical representation, like [[impulse response]] for linear systems. But there are some efforts  to characterize nonlinear systems, such as [[Volterra series|Volterra]] and [[Wiener series]] using [[polynomial]] integrals as the use of those methods naturally extend the signal into multi-dimensions.<ref name=\":0\">{{cite journal|title = Multi-dimensional signal processing for non-linear structural dynamics|url = http://www.sciencedirect.com/science/article/pii/088832709190015W|journal = Mechanical Systems and Signal Processing|date = 1991-01-01|pages = 61–80|volume = 5|issue = 1|doi = 10.1016/0888-3270(91)90015-W|first = H.|last = Liu|first2 = T.|last2 = Vinh}}</ref><ref>{{cite journal|title = Multidimensional Nonlinear Schur Parametrization of NonGaussian Stochastic Signals, Part Two: Generalized Schur Algorithm|journal = Multidimensional Systems and Signal Processing|date = 2004-07-01|issn = 0923-6082|pages = 243–275|volume = 15|issue = 3|doi = 10.1023/B:MULT.0000028008.93933.45|first = Jan|last = Zarzycki}}</ref> Another example is the [[Empirical mode decomposition]] method using [[Hilbert transform]] instead of [[Fourier transform|Fourier Transform]] for nonlinear multi-dimensional systems.<ref name=\":3\">{{cite journal|title = The multi-dimensional ensemble empirical mode decomposition method|journal = Advances in Adaptive Data Analysis|date = 2009-07-01|issn = 1793-5369|pages = 339–372|volume = 01|issue = 3|doi = 10.1142/S1793536909000187|first = Zhaohua|last = Wu|first2 = Norden E.|last2 = Huang|first3 = Xianyao|last3 = Chen}}</ref><ref>{{cite journal|title = Two-dimensional nonlinear geophysical data filtering using the multidimensional EEMD method|url = http://www.sciencedirect.com/science/article/pii/S0926985114003097|journal = Journal of Applied Geophysics|date = 2014-12-01|pages = 256–270|volume = 111|doi = 10.1016/j.jappgeo.2014.10.015|first = Chih-Sung|last = Chen|first2 = Yih|last2 = Jeng}}</ref> This method is an empirical method and can be directly applied to data sets. Multi-dimensional [[nonlinear filter]]s (MDNF) are also an important part of NMSP, MDNF are mainly used to filter noise in real data. There are nonlinear-type hybrid filters used in color image processing,<ref name=\":1\">{{cite journal|title = Generalised class of nonlinear-type hybrid filters|url = http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1137441|journal = Electronics Letters|date = 2002-12-01|issn = 0013-5194|pages = 1650–1651|volume = 38|issue = 25|doi = 10.1049/el:20021120|first = L.|last = Khriji|first2 = M.|last2 = Gabbouj}}</ref> nonlinear edge-preserving filters use in magnetic resonance image restoration. Those filters use both temporal and spatial information and combine the maximum likelihood estimate with the spatial smoothing algorithm.<ref name=\":2\">{{cite journal|title = A multidimensional nonlinear edge-preserving filter for magnetic resonance image restoration|url = http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=342189&tag=1|journal = IEEE Transactions on Image Processing|date = 1995-02-01|issn = 1057-7149|pages = 147–161|volume = 4|issue = 2|doi = 10.1109/83.342189|pmid = 18289967|first = H.|last = Soltanian-Zadeh|first2 = J.P.|last2 = Windham|first3 = A.E.|last3 = Yagle}}</ref>\n\n== Nonlinear analyser ==\n\nA linear frequency response function (FRF) can be extended to a nonlinear system by evaluation of higher order transfer functions and impulse response functions by [[Volterra series]].<ref name=\":0\" /> Suppose we have a time series <math>y(t)</math>, which is decomposed <math>y(t)</math> into components of various order<ref name=\":0\" />\n\n<math>y(t)=y_0+y_1(t)+y_2(t)+\\cdots+y_n(t).</math>\n\nEach component is defined as\n\n<math>y_n(t)=\\int_{-\\infty}^{+\\infty}\\cdots\\int_{-\\infty}^{+\\infty}h_n(\\tau_1,\\tau_2,\\cdots,\\tau_n)\\displaystyle\\prod_{i=1}^{n}x(t-\\tau_i)d\\tau_i</math> ,\n\nfor <math>n=1</math>, <math>y_1(t)</math> is the linear convolution.  <math>h_n(\\tau_1,\\tau_2,\\cdots,\\tau_n)</math> is the generalized impulse response of order <math>n</math>.\n\nThe 1D Fourier transform of <math>y_n(t)</math> is\n\n<math>Y_{(n)}(\\omega)=\\int_{-\\infin}^{+\\infin}[\\int_{-\\infin}^{+\\infin}\\cdots\\int_{-\\infin}^{+\\infin}h_n(\\tau_1,\\cdots,\\tau_n)\\prod_{i=1}^{n}x(t-\\tau_i)d\\tau_i]\\exp(-j\\omega t)dt.</math>\n\nSchetzen<ref>{{Cite book|title = The Volterra and Wiener theories of nonlinear systems|url = https://www.amazon.ca/exec/obidos/redirect?path=ASIN/0471044555|publisher = Wiley|date = 1980-04-09|isbn = 978-0471044550|first = Martin|last = Schetzen}}</ref> suggested the definition of <math>n</math>th output component as <math>n</math> time variables <math>y_n(t_1,\\cdots,t_n)</math> so as to permit the application of the <math>n</math>-dimensional Fourier transform,\n\n<math>Y_n(\\omega_1,\\cdots,\\omega_n)=\\int_{-\\infin}^{+\\infin}\\cdots\\int_{-\\infin}^{+\\infin}[\\int_{-\\infin}^{+\\infin}\\cdots\\int_{-\\infin}^{+\\infin}h_n(\\tau_1,\\cdots,\\tau_n)x(t-\\tau_1)\\cdots x(t-\\tau_n)d\\tau_1\\cdots d\\tau_n]\\times\\exp(-j\\omega_1t_1\\cdots-j\\omega_nt_n)dt_1\\cdots dt_n.</math>\n\nTaking the inverse Fourier transform of <math>Y_{(n)}(\\omega)</math> and <math>Y_n(\\omega_1,\\cdots,\\omega_n)</math> and equalizing <math>t_1=t_2=\\cdots=t_n=t</math>, we obtain the following equation,\n\n<math>Y_{(n)}(\\omega)=\\int_{-\\infin}^{+\\infin}\\cdots\\int_{-\\infin}^{+\\infin}Y_n(\\omega-\\omega_2-\\cdots-\\omega_n,\\omega_2,\\cdots,\\omega_n)d\\omega_2d\\omega_3\\cdots d\\omega_n.</math><ref name=\":0\" />\n\n=== Transfer function ===\n\nApplying the <math>n</math>th dimensional Fourier Transform to <math>h_n(\\tau_1,\\tau_2,\\cdots,\\tau_n)</math> obtain the transfer function\n\n<math>H_n(\\omega_1,\\cdots,\\omega_n)=\\int_{-\\infty}^{+\\infty}\\cdots\\int_{-\\infty}^{+\\infty}h_n(\\tau_1,\\cdots,\\tau_n)\\exp(-j\\omega_1\\tau_1-\\cdots-j\\omega_n\\tau_n)d\\tau_1\\cdots d\\tau_n</math>\n\n==  Multi-dimensional nonlinear filter ==\n\n=== Nonlinear-type hybrid filters ===\n\nOne example of nonlinear filters is the (generalized directional distance rational hybrid filter (GDDRHF)<ref name=\":1\" />) for multidimensional signal processing. This filter is a two-stage type hybrid filter: 1) the stage  <math>L_p</math> norm criteria and angular distance criteria to produce three output vectors with respect to the shape models; 2) the stage performs vector rational operation on the above three output vectors to produce the final output vectors.  The output vector <math>\\underline{y}(\\textbf{x}_i)</math> of the GDDRHF is the result of a vector rational function taking into account three input sub-function which form an input function set <math>\\{\\underline{y}_1,\\underline{y}_2,\\underline{y}_3\\}</math>,\n\n<math>\\underline{y}(\\textbf{x}_i)=\\underline{y}_2(\\textbf{x}_i)+\\frac{\\sum_{j=1}^{3}\\beta_j\\underline{y}_j(\\textbf{x}_i)}{h+kD[\\underline{y}_1(\\textbf{x}_i),\\underline{y}_3(\\textbf{x}_i)]},</math>\n\nwhere <math>D[\\cdot]</math> plays an important role as an edge sensing term, <math>\\beta=[\\beta_1,\\beta_2,\\beta_3]</math> characterizes the constant vector coefficient of the input sub-functions. <math>h</math> and <math>k</math> are some positive constants. The parameter <math>k</math> is used to control the amount of the nonlinear effect.<ref name=\":1\" />\n\n=== Multidimensional nonlinear edge-preserving filter ===\n\nThis kind of multidimensional filter has been used for MRI imaging processing.<ref name=\":2\" /> This filter uses MRI signal models to implement an approximate maximum likelihood or least squares estimate of each pixel gray level from the gray levels. It is also employs a trimmed mean spatial smoothing algorithm that uses a Euclidean distance discriminator to preserve partial volume and edge information; corresponds to using intra frame information .\n\n== Multi-dimensional ensemble empirical mode decomposition method ==\nA multi-dimensional ensemble empirical mode decomposition method was applied to multi-dimensional data including images and solid with variable density. The decomposition is based on the application of ensemble [[empirical mode decomposition]] (EEMD) to slices of data in each and every dimension involved. The final reconstruction of the corresponding intrinsic mode function is based on a comparable minimal scale combination principle.<ref>{{cite book|title = Hilbert–Huang Transform and Its Applications|url = https://books.google.com/books?id=jTG7CgAAQBAJ|publisher = World Scientific|date = 2014-04-22|isbn = 9789814508254|first = Norden E.|last = Huang|first2 = Samuel S. P.|last2 = Shen}}</ref>\n\nFor a two-dimensional signal <math>f(m,n)</math> using EEMD, the signal is first decomposed the y-direction to obtain <math>g_j(m,n)</math>, each row of <math>g_j(m,n)</math>  is decomposed using [[Empirical mode decomposition|EEMD]].\n\nLet <math>f(x,y)</math> be sampled as <math>f(m,n)=\\left(\\begin{array}{cccc}\nf_{1,1} & f_{2,1} & \\cdots & f_{M,1}\\\\\nf_{1,2} & f_{2,2} & \\cdots & f_{M,2}\\\\\n\\cdots  & \\cdots & \\cdots & \\cdots\\\\\nf_{1,n} & f_{2,N} & \\cdots & f_{M,N}\n\\end{array}\n\\right),</math>\n\nThe [[Empirical mode decomposition|EEMD]] decomposition of the <math>m</math>th column of <math>f(m,n)</math> is\n\n<math>f(m,\\sim)=\\sum_{j=1}^{J}C_j(m,\\sim)=\\sum_{j=1}^{J}\\left(\\begin{array}{c}\nc_{m,1,j}\\\\\nc_{m,2,j}\\\\\n\\cdots\\\\\nc_{m,N,j}\\\\\n\\end{array}\\right),</math>\n\nafter all the columns are decomposed we get <math>j</math> th matrix being\n\n<math>g_j(m,n)=\\left(\\begin{array}{cccc}\nc_{1,1,j} & c_{2,1,j} & \\cdots & c_{M,1,j}\\\\\nc_{1,2,j} & c_{2,2,j} & \\cdots & c_{M,2,j}\\\\\n\\cdots & \\cdots & \\cdots & \\cdots\\\\\nc_{1,N,j} & c_{2,N,j} & \\cdots & c_{M,N,j}\\\\\n\\end{array}\\right).</math>\n\nThis is the <math>j^{th}</math> component of the original data <math>f(m,n)</math>\n\n<math>n^{th}</math> row of <math>g_j(m,n)</math> decomposition using [[Empirical mode decomposition|EEMD]] is\n\n<math>g_j(\\sim,n)=\\sum_{k=1}^{K}D_{j,k}(\\sim,n)=\\sum_{k=1}^{K}\\left(\\begin{array}{cccc}\nd_{1,n,j,k}&\nd_{2,n,j,k}&\n\\cdots&\nd_{M,n,j,k}\\\\\n\\end{array}\\right),</math>\n\nrearrange the component as\n\n<math>h_{j,k}(m,n)=\\left(\\begin{array}{cccc}\nd_{1,1,j,k} & d_{2,1,j,k} & \\cdots & d_{M,1,j,k}\\\\\nd_{1,2,j,k} & d_{2,2,j,k} & \\cdots & d_{M,2,j,k}\\\\\n\\cdots & \\cdots & \\cdots & \\cdots\\\\\nd_{1,N,j,k} & d_{2,1,N,k} & \\cdots & d_{M,N,j,k}\\\\\n\\end{array}\\right),</math>\n\nSo <math>f(m,n)=\\sum_{k=1}^{K}\\sum_{j=1}^{J}h_{j,k}(m,n).</math> For a multi-dimension decomposition with an <math>n</math>-dimensional function we can use the same method above.<ref name=\":3\" />\n[[File:MDEEMD figure.jpg|thumb|The picture is download from Internet<ref>{{Cite web|url = http://fbwhatsappstatusdp.com/wp-content/uploads/2015/09/beautiful_beach_sunsets_wallpaper_hd_free.jpg|title = Beautiful_picture|date = |accessdate = |website = |last = |first = }}</ref> and code using is from Zhaohua, Wu et.al.<ref>{{Cite journal|title = The multi-dimensional ensemble empirical mode decomposition method|journal = Advances in Adaptive Data Analysis|date = 2009-07-01|issn = 1793-5369|pages = 339–372|volume = 01|issue = 3|doi = 10.1142/S1793536909000187|first = Zhaohua|last = Wu|first2 = Norden E.|last2 = Huang|first3 = Xianyao|last3 = Chen}}</ref>  The MDEEMD for a picture C1, C2, C3, C4, C5 are five mode components after decomposition.]]\n\n== References ==\n{{Reflist}}\n\n[[Category:Wavelets]]\n[[Category:Signal processing]]"
    },
    {
      "title": "Non-separable wavelet",
      "url": "https://en.wikipedia.org/wiki/Non-separable_wavelet",
      "text": "'''Non-separable wavelets''' are multi-dimensional [[wavelet]]s that are not directly implemented as [[tensor product]]s of wavelets on some lower-dimensional space.\nThey have been studied since 1992.<ref>J. Kovacevic and M. Vetterli, \"Nonseparable multidimensional perfect reconstruction filter banks and wavelet bases for Rn,\" IEEE Trans. Inf. Theory, vol. 38, no. 2, pp. 533–555, Mar. 1992.</ref>\nThey offer a few important advantage.\nNotably, using non-separable filters leads to more parameters in design, and consequently better filters.<ref>J. Kovacevic and M. Vetterli, \"Nonseparable two- and three-dimensional wavelets,\" IEEE Transactions on Signal Processing, vol. 43, no. 5, pp. 1269–1273, May 1995.</ref>\nThe main difference, when compared to the one-dimensional wavelets, is that [[multidimensional sampling|multi-dimensional sampling]] requires the use of [[lattice (group)|lattices]] (e.g., the quincunx lattice).\nThe wavelet filters themselves can be separable or non-separable regardless of the sampling lattice.\nThus, in some cases, the non-separable wavelets can be implemented in a separable fashion.\nUnlike separable wavelet, the non-separable wavelets are capable of detecting structures that are not only horizontal, vertical or diagonal (show less [[anisotropy]]).\n\n== Examples ==\n* Red-black wavelets<ref>G. Uytterhoeven and [[Adhemar Bultheel|A. Bultheel]], \"The Red-Black Wavelet Transform,\" in IEEE Signal Processing Symposium, pages 191–194, 1998.</ref>\n* [[Contourlet]]s<ref>M. N. Do and M. Vetterli, \"The contourlet transform: an efficient directional multiresolution image representation,\" IEEE Transactions on Image Processing, vol. 14, no. 12, pp. 2091–2106, Dec. 2005.</ref>\n* [[Shearlet]]s<ref>G. Kutyniok and D. Labate, \"Shearlets: Multiscale Analysis for Multivariate Data,\" 2012.</ref>\n* Directionlets<ref>V. Velisavljevic, B. Beferull-Lozano, M. Vetterli and P. L. Dragotti, \"Directionlets: anisotropic multi-directional representation with separable filtering,\" IEEE Trans. on Image Proc., Jul. 2006.</ref>\n* [[pyramid (image processing)#Steerable pyramid|Steerable pyramids]]<ref>E. P. Simoncelli and W. T. Freeman, \"The Steerable Pyramid: A Flexible Architecture for Multi-Scale Derivative Computation,\" in IEEE Second Int'l Conf on Image Processing. Oct. 1995.</ref>\n* Non-separable schemes for tensor-product wavelets<ref>D. Barina, M. Kula and P. Zemcik, \"Parallel wavelet schemes for images,\" J Real-Time Image Proc, 2016.</ref>\n\n== References ==\n<references />\n\n[[Category:Wavelets]]\n[[Category:Multidimensional signal processing]]\n[[Category:Image processing]]\n\n\n{{signal-processing-stub}}"
    },
    {
      "title": "Pixlet",
      "url": "https://en.wikipedia.org/wiki/Pixlet",
      "text": "'''Pixlet''' is a [[video codec]] created by [[Apple Inc.|Apple]] and based on [[wavelet]]s, designed to enable viewing of full-resolution, [[High-definition video|HD]] movies in real time at low [[DV]] data rates.{{Citation needed|date=March 2019}}  According to Apple's claims, it allows for a 20–25:1 [[compression ratio]]. Similar to DV, it does not use [[Inter frame|interframe compression]], making it suitable for previewing in production and special effects studios. It is designed to be an editing codec; however, low bitrates make it poorly suited to broadcast use.\n\nThe name ''Pixlet'' is a contraction of 'Pixar wavelet'.<ref>{{Cite book|url=https://books.google.com/books?id=kJUczzCvkdgC&pg=PA359&lpg=PA359&dq=%22pixlet%22+pixar&source=bl&ots=cc6k2FcBg5&sig=ACfU3U2f_HDZLyF1MbJf0AiD57W4uxjPuw&hl=en&sa=X&ved=2ahUKEwiqjvf67pbhAhVEO30KHZhFCykQ6AEwBnoECAkQAQ#v=onepage&q=%22pixlet%22%20pixar&f=false|title=A Practical Guide to Video and Audio Compression: From Sprockets and Rasters to Macroblocks|last=Wootton|first=Cliff|date=2005|publisher=Taylor & Francis|isbn=9780240806303|page=359|language=en}}</ref> When it was introduced by [[Steve Jobs]] at [[Worldwide Developers Conference]] 2003, it was said that the codec was developed at the request of animation company [[Pixar]].\n\nA [[Power Macintosh]] with at least a 1 [[GHz]] [[PowerPC G4]] processor is required for real-time playback of half-resolution high-definition video (960x540).\n\nPixlet, while part of the cross-platform [[QuickTime]], is only available on [[Apple Macintosh|Macs]] running [[Mac OS X v10.3]] or later. QuickTime X cannot decode Pixlet files.\n\n[[FFmpeg]] version 3.3 and later can decode Pixlet files.<ref>{{Cite web|url=http://git.videolan.org/?p=ffmpeg.git;a=commit;h=73651090ca1183f37753ee30a7e206ca4fb9f4f0|title=git.videolan.org Git - ffmpeg.git/commit|last=Mahol|first=Paul B.|date=December 22, 2016|website=git.videolan.org|language=en-US|access-date=2019-03-22}}</ref> On March 21, 2019, Apple announced that Pixlet is among the codecs that will no longer be supported in its own software after the successor to [[MacOS Mojave|macOS Mojave]] is released, due to the transition to 64-bit-only software. In the same announcement, Apple noted that third-party developers could maintain support by building support directly into their apps.<ref>{{Cite web|url=https://support.apple.com/en-us/HT209000|title=About incompatible media in Final Cut Pro X|date=March 21, 2019|website=Apple Support|language=en|access-date=2019-03-22}}</ref>\n\n== References ==\n<references />\n\n{{Compression formats}}\n\n[[Category:Video codecs]]\n[[Category:Wavelets]]\n[[Category:QuickTime]]\n[[Category:Computer-related introductions in 2003]]"
    },
    {
      "title": "Poisson wavelet",
      "url": "https://en.wikipedia.org/wiki/Poisson_wavelet",
      "text": "In mathematics, in functional analysis, several different [[wavelet]]s are known by the name '''Poisson wavelet'''.  In one context, the term \"Poisson wavelet\" is used to denote a family  of wavelets labeled by the set of [[positive integer]]s,  the members of which are associated with the  [[Poisson probability distribution]]. These wavelets were first defined  and studied by Karlene A. Kosanovich, Allan R. Moser and Michael J. Piovoso in 1995–96.<ref name=def>{{cite journal|last1=Karlene A. Kosanovich, Allan R. Moser and Michael J. Piovoso|title=The Poisson wavelet transform|journal=Chemical Engineering Communications| date=1996| volume=146|issue=1|pages= 131–138}}</ref><ref>{{cite journal|last1=Karlene A. Kosanovich, Allan R. Moser and Michael J. Piovoso|title=A new family of wavelets: the Poisson wavelet transform|journal=Computers in Chemical  Engineering|date=1997|volume=21|issue=6|pages=601–620}}</ref> In another context, the term refers to a certain wavelet which involves a form of  the Poisson integral kernel.<ref name=geo>{{cite book|last1=Roland Klees, Roger Haagmans (editors)|title=Wavelets in the Geosciences|date=2000|publisher=Springer|location=Berlin|pages=18–20}}</ref> In a still another context, the terminology is used to describe a family of complex wavelets indexed by positive integers which are connected with the derivatives of the Poisson integral kernel.<ref name=jerri>{{cite book|last1=Abdul J. Jerri|title=The Gibbs Phenomenon in Fourier Analysis, Splines and Wavelet Approximations|date=1998|publisher=Springer Science+Business Media|location=Dordrech|isbn=978-1-4419-4800-7|pages=222–224}}</ref>\n\n==Wavelets associated with Poisson probability distribution==\n\n===Definition===\n\n[[Image:PoissonWavelets.png|right|thumb|300px|Members of the family of Poisson wavelets corresponding to ''n'' = 1, 2, 3, 4.]]\n\nFor each positive integer ''n'' the Poisson wavelet <math>\\psi_n(t)</math> is defined by\n::<math> \\psi_n(t)  = \\begin{cases}  \\left(\\frac{t-n}{n!}\\right) t^{n-1}e^{-t} & \\text{ for } t\\ge 0 \\\\ 0 & \\text{ for } t<0.\\end{cases}\n</math>\n\nTo see the relation between the Poisson wavelet and the Poisson distribution let ''X'' be a discrete random variable having the Poisson distribution with parameter (mean) ''t'' and, for each non-negative integer ''n'', let Prob(''X'' = ''n'') = ''p''<sub>''n''</sub>(''t''). Then we have\n::<math>p_n(t) = \\frac{t^n}{n!}e^{-t}.</math>\n\nThe Poisson wavelet <math>\\psi_n(t)</math> is now given by\n\n::<math>\\psi_n(t) = -\\frac{d}{dt}p_n(t).</math>\n\n===Basic properties===\n*<math>\\psi_n(t)</math> is the backward difference of the values of the Poisson distribution: \n::<math>\\psi_n(t)=p_n(t)-p_{n-1}(t).</math>\n\n*The \"waviness\" of the members of this wavelet family follows from\n::<math>\\int_{-\\infty}^{\\infty}\\psi_n(t)\\,dt =0.</math>\n\n*The Fourier transform of <math>\\psi_n(t)</math> is given\n::<math>\\Psi(\\omega)=\\frac{-i\\omega}{(1+i\\omega)^{n+1}}.</math>\n\n*The admissibility constant associated with <math>\\psi_n(t)</math> is\n::<math>C_{\\psi_n} = \\int_{-\\infty}^{\\infty}\\frac{\\left|\\Psi_n(\\omega)\\right|^2}{|\\omega|}\\, d\\omega =\\frac{1}{n}.</math>\n\n*Poisson wavelet is  not an orthogonal family of wavelets.\n\n===Poisson wavelet transform===\nThe Poisson wavelet family can be used to construct the family of Poisson wavelet transforms of functions defined the time domain. Since the Poisson wavelets satisfy the admissibility condition also, functions in the time domain can be reconstructed from their Poisson wavelet transforms using the formula for inverse continuous-time wavelet transforms.\n\nIf ''f''(''t'') is a function in the time domain its ''n''-th Poisson wavelet transform is given by\n\n::<math>(W_nf)(a,b) = \\frac{1}{\\sqrt{|a|}}\\int_{-\\infty}^{\\infty} f(t)\\psi_n\\left(\\frac{t-b}{a}\\right)\\, dt</math>\n\nIn the reverse direction, given the ''n''-th Poisson wavelet transform <math>(W_nf)(a,b)</math> of a function ''f''(''t'') in the time domain, the function ''f''(''t'') can be reconstructed as follows:\n\n::<math>f(t)=\\frac{1}{C_{\\psi_n}}  \\int_{-\\infty}^{\\infty} \\left[  \\int_{-\\infty}^{\\infty} \\, \\left\\{(W_nf)(a,b)\\frac{1}{\\sqrt{|a|}} \\psi_n\\left(\\frac{t-b}{a}\\right)\\,\\right\\} db\\right] \\frac{da}{a^2}</math>\n\n===Applications===\nPoisson wavelet transforms have been applied in multi-resolution analysis, system identification, and parameter estimation. They are particularly useful in studying problems in which the functions in the time domain consist of linear combinations of decaying exponentials with time delay.\n\n==Wavelet associated with Poisson kernel==\n[[File:PoissonWaveletFromPoissonKernel.png|thumb|right|300px|Image of the wavelet associated with the Poisson kernel.]]\n\n[[File:FourierTransformOfPoissonWaveletFromPoissonKernel.png|thumb|right|300px|Image of the Fourier transform of the wavelet associated with the Poisson kernel.]]\n\n===Definition===\nThe Poisson wavelet is defined by the function<ref name=geo/>\n\n::<math>\\psi(t)=\\frac{1}{\\pi}\\frac{1-t^2}{(1+t^2)^2}</math>\n\nThis can be expressed in the form\n\n::<math>\\psi(t)=P(t) + t\\frac{d}{dt}P(t)</math> where <math> P(t)=\\frac{1}{\\pi}\\frac{1}{1+t^2}</math>.\n\n===Relation with Poisson kernel===\nThe function <math>P(t)</math> appears as an [[integral kernel]] in the solution of a certain [[initial value problem]] of the [[Laplace operator]].\n\nThis is the initial value problem: Given any <math>s(x)</math> in <math>L^p(\\mathbb R)</math>, find a harmonic function <math>\\phi(x,y)</math> defined in the [[upper half-plane]] satisfying the following conditions:\n\n:#<math>\\int_{-\\infty}^\\infty |\\phi(x,y)|^p\\, dx \\le c < \\infty</math>, and\n:#<math> \\phi(x,y)\\rightarrow s(x)</math> as <math>y\\rightarrow 0</math> in <math>L^p(\\mathbb R)</math>.\n\nThe problem has the following solution: There is exactly one function <math>\\phi(x,y)</math> satisfying the two conditions  and it is given by\n\n::<math>\\phi(t,y)= P_y(t)\\star s(t) </math>\n\nwhere <math>P_y(t)=\\frac{1}{y}P\\left(\\frac{t}{y}\\right)=\\frac{1}{\\pi}\\frac{y}{t^2+y^2}</math> and where \"<math>\\star</math>\" denotes the [[convolution operation]]. The function <math>P_y(t)</math> is the integral kernel for the function <math> \\phi(x,y)</math>. The function <math>\\phi(x,y)</math> is the harmonic continuation of <math>s(x)</math> into the upper half plane.\n\n===Properties===\n*The \"waviness\" of the function follows from\n::<math>\\int_{-\\infty}^\\infty \\psi(t)\\, dt=0</math>.\n\n*The Fourier transform of <math>\\psi(t)</math> is given by\n\n::<math> \\Psi(\\omega) = |\\omega|e^{-|\\omega|}</math>.\n\n*The admissibility constant is\n\n::<math>C_{\\psi} = \\int_{-\\infty}^{\\infty}\\frac{\\left|\\Psi(\\omega)\\right|^2}{|\\omega|}\\, d\\omega =2.</math>\n\n==A class of complex wavelets associated with the Poisson kernel==\n[[File:RealPartsOfPoissonWavelets.png|thumb|300px|right|The graphs of the real parts of the Poisson wavelet\n<math>\\psi_n(t)</math> for <math>n=1,2,3,4</math>.]]\n\n[[File:ImaginaryPartsOfPoissonWavelets.png|thumb|300px|right|The graphs of the imaginary parts of the Poisson wavelet\n<math>\\psi_n(t)</math> for <math>n=1,2,3,4</math>.]]\n\n===Definition===\nThe Poisson wavelet is a family of  complex valued functions indexed by the set of positive integers and defined by<ref name=jerri/><ref>{{cite book|last1=Wojbor A. Woyczynski|title=Distributions in the Physical and Engineering Sciences: Distributional and Fractal Calculus, Integral Transforms and Wavelets, Volume 1|date=1997|publisher=Springer Science & Business Media|isbn=9780817639242|page=223}}</ref>\n::<math>\\psi_n(t)=\\frac{1}{2\\pi}(1-it)^{-(n+1)}</math> where <math>n=1,2,3, \\ldots</math>\n\n===Relation with Poisson kernel===\nThe function <math>\\psi_n(t)</math> can be expressed as an ''n''-th derivative as follows:\n::<math>\\psi_n(t)= \\frac{1}{2\\pi}\\frac{1}{n!\\, i^n}\\frac{d^n}{dt^n}\\left((1-it)^{-1}\\right)</math>\nWriting the function  <math>(1-it)^{-1}</math> in terms of the Poisson integral kernel <math>P(t)=\\frac{1}{1+t^2}</math> as \n::<math>(1-it)^{-1}= P(t)+itP(t)</math>\nwe have\n::<math>\\psi_n(t)=\\frac{1}{2\\pi}\\frac{1}{n!\\,i^n}\\frac{d^n}{dt^n}P(t) + i \\left(\\frac{1}{2\\pi}\\frac{1}{n!\\, i^n}\\frac{d^n}{dt^n}\\left(tP(t)\\right)\\right)</math>\nThus <math>\\psi_n(t)</math> can be interpreted as a function proportional to the derivatives of the Poisson integral kernel.\n\n===Properties===\nThe Fourier transform  of <math>\\psi_n(t)</math> is given by\n::<math>\\Psi_n(\\omega) = \\frac{1}{\\Gamma(n+1)}\\omega^n e^{-\\omega}u(\\omega)</math>\nwhere <math>u(\\omega)</math> is the [[unit step function]].\n\n==References==\n{{reflist}}\n\n[[Category:Wavelets]]\n[[Category:Time–frequency analysis]]\n[[Category:Signal processing]]\n[[Category:Continuous wavelets]]\n[[Category:Poisson distribution]]"
    },
    {
      "title": "Polyphase matrix",
      "url": "https://en.wikipedia.org/wiki/Polyphase_matrix",
      "text": "In [[signal processing]], a '''polyphase matrix''' is a matrix whose elements are [[linear filter|filter mask]]s.  It represents a [[filter bank]] as it is used in [[sub-band coder]]s alias [[discrete wavelet transform]]s.<ref name=\"strang1997filterbanks\">\n{{cite book\n |first1=Gilbert|last1=Strang|author1-link=Gilbert Strang\n |first2=Truong|last2=Nguyen\n |title=Wavelets and Filter Banks\n |publisher=Wellesley-Cambridge Press\n |year=1997\n |isbn=0-9614088-7-1\n}}</ref>\n\nIf <math>\\scriptstyle h,\\, g</math> are two filters, then one level the traditional wavelet transform maps an input signal <math>\\scriptstyle a_0</math> to two output signals <math>\\scriptstyle a_1,\\, d_1</math>, each of the half length:\n:<math>\\begin{align}\n  a_1 &= (h \\cdot a_0) \\downarrow 2 \\\\\n  d_1 &= (g \\cdot a_0) \\downarrow 2\n\\end{align}</math>\n\nNote, that the dot means [[polynomial multiplication]]; i.e., [[convolution]] and <math>\\scriptstyle\\downarrow</math> means [[downsampling]].\n\nIf the above formula is implemented directly, you will compute values that are subsequently flushed by the down-sampling.  You can avoid their computation by splitting the filters and the signal into even and odd indexed values before the wavelet transformation:\n:<math>\\begin{align}\n  h_\\mbox{e} &= h \\downarrow 2                & a_{0,\\mbox{e}} &= a_0 \\downarrow 2 \\\\\n  h_\\mbox{o} &= (h \\leftarrow 1) \\downarrow 2 & a_{0,\\mbox{o}} &= (a_0 \\leftarrow 1) \\downarrow 2\n\\end{align}</math>\n\nThe arrows <math>\\scriptstyle\\leftarrow</math> and <math>\\scriptstyle\\rightarrow</math> denote left and right shifting, respectively.  They shall have the same [[operator precedence|precedence]] like convolution, because they are in fact convolutions with a shifted discrete [[Kronecker delta|delta impulse]].\n:<math>\\delta = (\\dots, 0, 0, \\underset{0-\\mbox{th position}}{1}, 0, 0, \\dots)</math>\n\nThe wavelet transformation reformulated to the split filters is:\n:<math>\\begin{align}\n  a_1 &= h_\\mbox{e} \\cdot a_{0,\\mbox{e}} +\n         h_\\mbox{o} \\cdot a_{0,\\mbox{o}} \\rightarrow 1 \\\\\n  d_1 &= g_\\mbox{e} \\cdot a_{0,\\mbox{e}} +\n         g_\\mbox{o} \\cdot a_{0,\\mbox{o}} \\rightarrow 1\n\\end{align}</math>\n\nThis can be written as [[matrix multiplication|matrix-vector-multiplication]]\n:<math>\\begin{align}\n  P &= \\begin{pmatrix}\n          h_\\mbox{e} & h_\\mbox{o} \\rightarrow 1 \\\\\n          g_\\mbox{e} & g_\\mbox{o} \\rightarrow 1\n        \\end{pmatrix} \\\\\n  \\begin{pmatrix} a_1 \\\\ d_1 \\end{pmatrix} &= P \\cdot\n        \\begin{pmatrix}\n          a_{0,\\mbox{e}} \\\\\n          a_{0,\\mbox{o}}\n        \\end{pmatrix}\n\\end{align}</math>\n\nThis matrix <math>\\scriptstyle P</math> is the polyphase matrix.\n\nOf course, a polyphase matrix can have any size, it need not to have square shape.  That is, the principle scales well to any [[filterbank]]s, [[multiwavelet]]s, wavelet transforms based on fractional [[refinable function|refinements]].\n\n== Properties ==\n\nThe representation of sub-band coding by the polyphase matrix is more than about write simplification.  It allows the adaptation of many results from [[Matrix (mathematics)|matrix theory]] and [[module theory]].  The following properties are explained for a <math>\\scriptstyle 2 \\,\\times\\, 2</math> matrix, but they scale equally to higher dimensions.\n\n=== Invertibility/perfect reconstruction ===\n\nThe case that a polyphase matrix allows reconstruction of a processed signal from the filtered data, is called [[perfect reconstruction]] property.  Mathematically this is equivalent to invertibility.  According to the theorem of [[inverse matrix|invertibility]] of a matrix over a ring, the polyphase matrix is invertible if and only if the [[determinant]] of the polyphase matrix is a [[Kronecker delta]], which is zero everywhere except for one value.\n:<math>\\begin{align}\n                \\det P &= h_{\\mbox{e}} \\cdot g_{\\mbox{o}} - h_{\\mbox{o}} \\cdot g_{\\mbox{e}} \\\\\n  \\exists A\\ A \\cdot P &= I \\iff \\exists c\\ \\exists k\\ \\det P = c \\cdot \\delta \\rightarrow k\n\\end{align}</math>\n<!--\n(\\dots,0,0,\\underset{i-\\mbox{th position}}{c},0,0,\\dots),\ndefined above\n-->\n\nBy [[Cramer's rule]] the inverse of <math>\\scriptstyle P</math> can be given immediately.\n:<math>P^{-1} \\cdot \\det P =\n  \\begin{pmatrix}\n     g_\\mbox{o} \\rightarrow 1 & - h_\\mbox{o} \\rightarrow 1 \\\\\n    -g_\\mbox{e}               &   h_\\mbox{e}\n  \\end{pmatrix}\n</math>\n\n=== Orthogonality ===\n\nOrthogonality means that the [[adjoint matrix]] <math>\\scriptstyle P^*</math> is also the inverse matrix of <math>\\scriptstyle P</math>.  The adjoint matrix is the [[transposed matrix]] with [[adjoint filter]]s.\n:<math>P^* = \\begin{pmatrix}\n    h_\\mbox{e}^*              & g_\\mbox{e}^* \\\\\n    h_\\mbox{o}^* \\leftarrow 1 & g_\\mbox{o}^* \\leftarrow 1\n  \\end{pmatrix}\n</math>\n\nIt implies, that the [[Euclidean norm]] of the input signals is preserved.  That is, the according wavelet transform is an [[isometry]].\n:<math>\\left\\|a_1\\right\\|_2^2 + \\left\\|d_1\\right\\|_2^2 = \\left\\|a_0\\right\\|_2^2</math>\n\nThe orthogonality condition\n:<math>P \\cdot P^* = I</math>\n\ncan be written out\n:<math>\\begin{align}\n  h_\\mbox{e}^* \\cdot h_\\mbox{e} + h_\\mbox{o}^* \\cdot h_\\mbox{o} &= \\delta \\\\\n  g_\\mbox{e}^* \\cdot g_\\mbox{e} + g_\\mbox{o}^* \\cdot g_\\mbox{o} &= \\delta \\\\\n  h_\\mbox{e}^* \\cdot g_\\mbox{e} + h_\\mbox{o}^* \\cdot g_\\mbox{o} &= 0\n\\end{align}</math>\n\n=== Operator norm ===\n\nFor non-orthogonal polyphase matrices the question arises what Euclidean norms the output can assume.  This can be bounded by the help of the [[operator norm]].\n:<math>\\forall x\\ \\left\\|P \\cdot x\\right\\|_2 \\in \\left[\\left\\|P^{-1}\\right\\|_2^{-1} \\cdot \\|x\\|_2, \\|P\\|_2 \\cdot \\|x\\|_2\\right]</math>\n\nFor the <math>\\scriptstyle 2 \\,\\times\\, 2</math> polyphase matrix the Euclidean operator norm can be given explicitly using the [[Frobenius norm]] <math>\\scriptstyle\\|\\cdot\\|_F</math> and the [[z transform]] <math>\\scriptstyle Z</math>:<ref name=\"thielemann2001adaptivewavelet\">\n{{cite thesis\n |first=Henning|last=Thielemann\n |title=Adaptive construction of wavelets for image compression\n |type=Diploma thesis\n |publisher=Martin-Luther-Universität Halle-Wittenberg, Fachbereich Mathematik/Informatik\n |year=2001\n |url=http://edoc.bibliothek.uni-halle.de/servlets/DocumentServlet?id=2134\n}}</ref>\n:<math>\\begin{align}\n                          p(z) &= \\frac{1}{2} \\cdot \\left\\|Z P(z)\\right\\|_F^2 \\\\\n                          q(z) &= \\left|\\det [Z P(z)]\\right|^2 \\\\\n                       \\|P\\|_2 &= \\max\\left\\{\\sqrt{p(z) + \\sqrt{p(z)^2 - q(z)}} : z\\in\\mathbb{C}\\ \\land\\ |z| = 1\\right\\} \\\\\n  \\left\\|P^{-1}\\right\\|_2^{-1} &= \\min\\left\\{\\sqrt{p(z) - \\sqrt{p(z)^2 - q(z)}} : z\\in\\mathbb{C}\\ \\land\\ |z| = 1\\right\\}\n\\end{align}</math>\n\nThis is a special case of the <math>n\\times n</math> matrix where the operator norm can be obtained via [[z transform]] and the [[spectral radius]] of a matrix or the according [[spectral norm]].\n:<math>\\begin{align}\n  \\left\\|P\\right\\|_2\n    &= \\sqrt{\\max\\left\\{\\lambda_\\text{max} \\left[Z P^*(z) \\cdot Z P(z)\\right] : z\\in\\mathbb{C}\\ \\land\\ |z| = 1\\right\\}} \\\\\n    &= \\max\\left\\{\\left\\|Z P(z)\\right\\|_2 : z\\in\\mathbb{C}\\ \\land\\ |z| = 1\\right\\} \\\\[3pt]\n  \\left\\|P^{-1}\\right\\|_2^{-1}\n    &= \\sqrt{\\min\\left\\{\\lambda_\\text{min}\\left[Z P^*(z) \\cdot Z P(z)\\right] : z\\in\\mathbb{C}\\ \\land\\ |z| = 1\\right\\}}\n\\end{align}</math>\n\nA signal, where these bounds are assumed can be derived from the eigenvector corresponding to the maximizing and minimizing eigenvalue.\n\n=== Lifting scheme ===\n\nThe concept of the polyphase matrix allows [[matrix decomposition]].  For instance the decomposition into [[triangular matrix|addition matrices]] leads to the [[lifting scheme]].<ref name=\"sweldens1998liftingfactor\">\n{{cite article\n |first1       = Ingrid\n |last1        = Daubechies\n |author1-link = Ingrid Daubechies\n |first2       = Wim\n |last2        = Sweldens\n |author2-link = Wim Sweldens\n |title        = Factoring wavelet transforms into lifting steps\n |journal      = J. Fourier Anal. Appl.\n |volume       = 4\n |issue        = 3\n |pages        = 245-267\n |year         = 1998\n |url          = http://cm.bell-labs.com/who/wim/papers/factor/index.html\n |deadurl      = yes\n |archiveurl   = https://web.archive.org/web/20061207025713/http://cm.bell-labs.com/who/wim/papers/factor/index.html\n |archivedate  = 2006-12-07\n |df           = \n}}</ref>  However, classical matrix decompositions like [[LU decomposition|LU]] and [[QR decomposition]] cannot be applied immediately, because the filters form a [[ring (algebra)|ring]] with respect to convolution, not a [[field (algebra)|field]].\n\n== References ==\n<references/>\n\n[[Category:Wavelets]]<!-- it is central part of wavelet theory -->\n[[Category:Digital signal processing]]<!-- but also of interest elsewhere -->"
    },
    {
      "title": "Progressive Graphics File",
      "url": "https://en.wikipedia.org/wiki/Progressive_Graphics_File",
      "text": "{{multiple issues|\n{{refimprove|date=October 2010}}\n{{third-party|date=March 2014}}\n}}\n\n{{Infobox file format\n| logo = [[File:LibPGF.PNG|frameless]]\n| name = PGF\n| extension = <tt>.pgf</tt>\n| mime = \n| owner =  xeraina GmbH\n| creatorcode =\n| magic = <code>504746</code>h ([[ASCII]] PGF) \n| type = [[wavelet]]-based [[Raster graphics|bitmapped]] [[Graphics file format|image format]]\n| released               = {{Start date and age|2000}} \n| latest release version = 7.15.25\n| latest release date    = {{Start date and age|2015}}\n| free                   = [[GNU Lesser General Public License|LGPLv2]]<ref name=\"SF\">{{cite web|url=http://libpgf.org/|title=PGF libPGF.org|work=[[SourceForge]] project [https://sourceforge.net/projects/libpgf libpgf]|year=2015|author=Christoph Stamm|accessdate=2015-09-14}}</ref>\n| extended from          = [[JPEG]], [[Portable Network Graphics|PNG]]\n}}\n\n'''PGF''' ('''Progressive Graphics File''') is a [[wavelet]]-based [[Raster graphics|bitmapped]] [[Graphics file format|image format]] that employs [[lossless data compression|lossless]] and [[lossy data compression]]. PGF was created to improve upon and replace the [[JPEG]] format. It was developed at the same time as [[JPEG 2000]] but with a focus on speed over [[data compression ratio|compression ratio]].{{citation needed|date=October 2011}}\n\nPGF can operate at higher compression ratios without taking more encoding/decoding time and without generating the characteristic \"blocky and blurry\" [[compression artifact|artifacts]] of the original [[Discrete cosine transform|DCT]]-based JPEG standard.<ref name=\"stamm\">{{cite web|url=http://www.libpgf.org/uploads/media/PGF_stamm_wscg02.pdf|archive-url=https://web.archive.org/web/20070307101234/http://www.libpgf.org/uploads/media/PGF_stamm_wscg02.pdf|dead-url=yes|archive-date=2007-03-07|title=PGF &ndash; A new progressive file format for lossy and lossless image compression|author=Christoph Stamm|accessdate=2014-03-13}}</ref> It also allows more sophisticated [[progressive download]]s.{{citation needed|date=October 2011}}\n\n== Color models ==\nPGF supports a wide variety of color models:{{cite web|url=http://www.libpgf.org/fileadmin/MEDIA/PGF_Facts.pdf|title=PGF Facts Sheet}}\n* [[Grayscale]] with 1, 8, 16, or 31 bits per pixel\n* [[Indexed color]] with palette size of 256\n* [[RGB color model|RGB]] color image with 12, 16 (red: 5 bits, green: 6 bits, blue: 5 bits), 24, or 48 bits per pixel\n* [[RGBA color space|ARGB]] color image with 32 bits per pixel\n* [[Lab color space|L*a*b]] color image with 24 or 48 bits per pixel\n* [[CMYK color model|CMYK]] color image with 32 or 64 bits per pixel\n\n== Technical discussion ==\nPGF claims to achieve an improved compression quality over JPEG adding or improving features such as scalability. Its compression performance is similar to the original JPEG standard. Very low and very high compression rates (including [[lossless compression]]) are also supported in PGF. The ability of the design to handle a very large range of effective bit rates is one of the strengths of PGF. For example, to reduce the number of bits for a picture below a certain amount, the advisable thing to do with the first JPEG standard is to reduce the resolution of the input image before encoding it — something that is ordinarily not necessary for that purpose when using PGF because of its wavelet scalability properties.\n\nThe PGF process chain contains the following four steps:\n# [[Color space]] transform (in case of color images)\n# [[Discrete wavelet transform|Discrete Wavelet Transform]]\n# [[Quantization (image processing)|Quantization]] (in case of lossy data compression)\n# Hierarchical [[bit-plane]] [[run-length encoding]]\n\n===Color components transformation===\n\nInitially, images have to be transformed from the RGB [[color space]] to another color space, leading to three ''components'' that are handled separately. PGF uses a fully reversible modified [[YUV]] color transform. The transformation matrices are:\n:<math>\n\\begin{bmatrix}\nY_r \\\\ U_r \\\\ V_r\n\\end{bmatrix} \n= \\begin{bmatrix}\n\\frac{1}{4} & \\frac{1}{2} & \\frac{1}{4} \\\\\n1 & -1 & 0 \\\\\n0 & -1 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\nR \\\\ G \\\\ B\n\\end{bmatrix}; \\qquad \\qquad\n\\begin{bmatrix}\nR \\\\ G \\\\ B\n\\end{bmatrix} \n= \\begin{bmatrix}\n1 & \\frac{3}{4} & -\\frac{1}{4} \\\\\n1 & -\\frac{1}{4} & -\\frac{1}{4} \\\\\n1 & -\\frac{1}{4} & \\frac{3}{4}\n\\end{bmatrix}\n\\begin{bmatrix}\nY_r \\\\ U_r \\\\ V_r\n\\end{bmatrix}\n</math>\n\nThe [[chrominance]] components can be, but do not necessarily have to be, down-scaled in resolution.\n\n===Wavelet transform===\n\nThe color components are then [[wavelet transform]]ed to an arbitrary depth.  In contrast to JPEG 1992 which uses an 8x8 block-size [[discrete cosine transform]], PGF uses one reversible wavelet transform: a rounded version of the  biorthogonal [[Cohen-Daubechies-Feauveau wavelet|CDF]] 5/3 [[wavelet]] transform. This wavelet filter bank is exactly the same as the reversible wavelet used in JPEG 2000. It uses only integer coefficients, so the output does not require rounding (quantization) and so it does not introduce any quantization noise.\n\n===Quantization===\n\nAfter the wavelet transform, the coefficients are scalar-[[Quantization (image processing)|quantized]] to reduce the amount of bits to represent them, at the expense of a loss of quality. The output is a set of integer numbers which have to be encoded bit-by-bit. The parameter that can be changed to set the final quality is the quantization step: the greater the step, the greater is the compression and the loss of quality. With a quantization step that equals 1, no quantization is performed (it is used in lossless compression). In contrast to JPEG 2000, PGF uses only powers of two, therefore the parameter value ''i'' represents a quantization step of 2<sup>''i''</sup>. Just using powers of two makes no need of integer multiplication and division operations.\n\n===Coding===\n\nThe result of the previous process is a collection of ''sub-bands'' which represent several approximation scales.\nA sub-band is a set of ''coefficients'' — [[integer|integer numbers]] which represent aspects of the image associated with a certain frequency range as well as a spatial area of the image.\n\nThe quantized sub-bands are split further into ''blocks'', rectangular regions in the wavelet domain. They are typically selected in a way that the coefficients within them across the sub-bands form approximately spatial blocks in the (reconstructed) image domain and collected in a fixed size ''macroblock''.\n\nThe encoder has to encode the bits of all quantized coefficients of a macroblock, starting with the most significant bits and progressing to less significant bits. In this encoding process, each [[bit-plane]] of the macroblock gets encoded in two so-called ''coding passes'', first encoding bits of significant coefficients, then refinement bits of significant coefficients. Clearly, in lossless mode all bit-planes have to be encoded, and no bit-planes can be dropped.\n\nOnly significant coefficients are compressed with an adaptive [[Run-length encoding|run-length/Rice]] (RLR) coder, because they contain long runs of zeros. The RLR coder with parameter ''k'' (logarithmic length of a run of zeros) is also known as the elementary [[Golomb code]] of order 2<sup>''k''</sup>.\n\n===Comparison with other file formats===\n* '''[[JPEG 2000]]''' is slightly more space-efficient in handling natural images. The [[PSNR]] for the same compression ratio is on average 3% better than the PSNR of PGF. It has a small advantage in compression ratio but longer encoding and decoding times.<ref name=\"stamm\" />\n* '''[[Portable Network Graphics|PNG]]''' (Portable Network Graphics) is more space-efficient in handling images with many pixels of the same color.\n\nThere are several self-proclaimed advantages of PGF over the ordinary JPEG standard:<ref name=\"stamm\" />\n* '''Superior compression performance''': The image quality (measured in [[Peak signal-to-noise ratio|PSNR]]) for the same compression ratio is on average 3% better than the PSNR of JPEG. At lower bit rates (e.g. less than 0.25 bits/pixel for gray-scale images), PGF has a much more significant advantage over certain modes of JPEG: artifacts are less visible and there is almost no blocking. The compression gains over JPEG are attributed to the use of [[Discrete wavelet transform|DWT]].\n* '''Multiple resolution representation''': PGF provides seamless compression of multiple image components, with each component carrying from 1 to 31 bits per component sample. With this feature there is no need for separately stored preview images ([[thumbnail]]s).\n* '''Progressive transmission''' by resolution accuracy, commonly referred to as progressive decoding: PGF provides efficient codestream organizations which are progressive by resolution. This way, after a smaller part of the whole file has been received, it is possible to see a lower quality of the final picture, the quality can be improved monotonically getting more data from the source.\n* '''Lossless and lossy compression''': PGF provides both lossless and lossy compression in a single compression architecture. Both lossy and lossless compression are provided by the use of a reversible (integer) wavelet transform.\n* '''Side channel spatial information''': Transparency and alpha planes are fully supported\n* '''ROI extraction''': Since version 5, PGF supports extraction of regions of interest ([[Region of interest|ROI]]) without decoding the whole image.\n\n==Available software==\nThe author published ''libPGF'' via a [[SourceForge]], under the [[GNU Lesser General Public License]] version 2.0.<ref name=\"SF\" /> Xeraina offers a free [[Photoshop plugin#Plugin types|Photoshop]] <code>.8bi</code> file format plugin, a [[Win32 console]] encoder and decoder, and PGF viewers based on [[Windows Imaging Component|WIC]] for 32bit and 64bit Windows platforms. Other WIC applications including [[Windows Photo Gallery|Photo Gallery]] are able to display PGF images after installing this viewer.<ref>{{cite web|url=http://www.xeraina.ch/pgf/pgfdownload.html|publisher=xeraina|title=PGF download|year=2013|accessdate=2014-03-14}}</ref>\n\n==See also==\n{{Portal|Free and open-source software}}\n*[[Comparison of graphics file formats]]\n*Related [[graphics file formats]]: [[ECW (file format)|ECW]], [[JPEG]], [[JPEG 2000]], [[JPEG XR]]\n*[[Image file formats]]\n*[[Image compression]]\n\n===File extension===\nFile extension <code>.pgf</code> and the [[Three-letter acronym|TLA]] [[PGF (disambiguation)|PGF]] are also used for unrelated purposes: \n* [[Adobe Illustrator Artwork|Adobe Illustrator]] used a ''Progressive Graphics Format'' before [[Encapsulated PostScript]].\n* [[PGF/TikZ]] uses a ''Portable Graphics Format'' in [[SourceForge]] project ''PGF''.\n* [[XnView]] and Konvertor associate file extension <code>.pgf</code> with ''Portfolio Graphics''.\n\n==References==\n{{reflist}}\n\n{{Graphics file formats}}\n{{Compression formats}}\n{{Use dmy dates|date=October 2010}}\n\n[[Category:Graphics file formats]]\n[[Category:Open formats]]\n[[Category:Image compression]]\n[[Category:Image processing]]\n[[Category:Wavelets]]"
    },
    {
      "title": "Quadrature mirror filter",
      "url": "https://en.wikipedia.org/wiki/Quadrature_mirror_filter",
      "text": "In [[digital signal processing]], a '''quadrature mirror filter''' is a filter whose magnitude response is the mirror image around <math>\\pi/2</math> of that of another filter. Together these filters are known as the Quadrature Mirror Filter pair.\n\nA filter <math>H_1(z)</math> will be the quadrature mirror filter of <math>H_0(z)</math> if <math>H_1(z) = H_0(-z)</math>\n\nThe filter responses are symmetric about <math>\\Omega = \\pi / 2</math>\n\n:<math>|H_1(e^{j\\Omega})| = |H_0(e^{j(\\pi - \\Omega)})|</math>\n\n\nIn audio/voice codecs, a quadrature mirror filter pair is often used to implement a [[filter bank]] that splits an input [[signal processing|signal]] into two bands.  The resulting high-pass and low-pass signals are often reduced by a factor of 2, giving a critically sampled two-channel representation of the original signal. The analysis filters are often related by the following formulae in addition to quadrate mirror property:\n:<math>|H_0(e^{j\\Omega})|^2 + |H_1(e^{j\\Omega})|^2 = 1</math> where <math>\\Omega</math> is the [[frequency]], and the sampling rate is normalized to <math>2\\pi</math>.\nThis is known as power complementary property.\nIn other words, the power sum of the high-pass and low-pass filters is equal to 1.  \n\nOrthogonal [[wavelet]]s -- the [[Haar wavelet]]s and related [[Daubechies wavelet]]s, [[Coiflet]]s, and some developed by [[Stéphane Mallat|Mallat]], are generated by [[Wavelet#Scaling_function|scaling functions]] which, with the wavelet, satisfy a quadrature mirror filter relationship.\n\n==Relationship to other filter banks==\nThe earliest wavelets were based on expanding a function in terms of rectangular steps, the Haar wavelets. This is usually a poor approximation, whereas Daubechies wavelets are among the simplest but most important families of wavelets. A linear filter that is zero for “smooth” signals, given a record of <math>N</math> points <math>x_n</math> is defined as:\n:<math>y_n = \\sum_{i=0}^{M-1} b_i x_{n-i}</math>\n\nIt is desirable to have it vanish for a constant, so taking the order <math>m = 4</math> for example:\n:<math>b_0 \\centerdot 1 + b_1 \\centerdot 1 + b_2 \\centerdot 1 + b_3 \\centerdot 1 = 0</math>\nAnd to have it vanish for a linear ramp so that:\n:<math>b_0 \\centerdot 0 + b_1 \\centerdot 1 + b_2 \\centerdot 2 + b_3 \\centerdot 3 = 0</math>\n\nA linear filter will vanish for any <math>x = \\alpha n + \\beta</math>, and this is all that can be done with a fourth order wavelet. Six terms will be needed to vanish a quadratic curve and so on given the other constraints to be included. Next an accompanying filter may be defined as:\n:<math>z_n = \\sum_{i=0}^{M-1} c_i x_{n-i}</math>\n\nThis filter responds in an exactly opposite manner, being large for smooth signals and small for non-smooth signals. A linear filter is just a convolution of the signal with the filter’s coefficients, so the series of the coefficients is the signal that the filter responds to maximally. Thus, the output of the second filter vanishes when the coefficients of the first one are input into it. The aim is to have:\n:<math>\\sum_{i=0}^{M-1} c_i b_i = 0</math>\n\nWhere the associated time series flips the order of the coefficients because the linear filter is a convolution, and so both have the same index in this sum. A pair of filters with this property are defined as quadrature mirror filters.<ref>\n{{Citation | last1=Gershenfeld\n| first1=Neil \n| title=The Nature of Mathematical Modeling\n| pages=132–135\n| publisher=Cambridge University Press\n| location=Cambridge, England\n| isbn=0521570956\n| year=1998}}.</ref>\nEven if the two resulting bands have been subsampled by a factor of 2, the relationship between the filters means that approximately [[perfect reconstruction]] is possible. That is, the two bands can then be upsampled, filtered again with the same filters and added together, to reproduce the original signal exactly (but with a small delay). (In practical implementations, numeric precision issues in floating-point arithmetic may affect the perfection of the reconstruction.)\n\n==Further reading==\n* Johnston, JD, A Filter Family Designed for use in Quadrature Mirror Filter Banks.  [http://www.info490b.ece.mcgill.ca/Data/Exp4/Johnston.pdf]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}, Acoustics, Speech and Signal Processing,IEEE International Conference, 5, 291-294, April, 1980.\n* Mohlenkamp, M. J, A Tutorial on Wavelets and Their Applications.  [http://www.ohiouniversityfaculty.com/mohlenka/20044/PASIII/waveletIPAM.pdf], University of Colorado, Boulder, Dept. of Applied Mathematics, 2004.\n* Polikar, R, Multiresolution Analysis: The Discrete Wavelet Transform.  [https://web.archive.org/web/20180430094742/http://users.rowan.edu/~polikar/WAVELETS/WTpart4.html], Rowan University, NJ, Dept. of Electrical and Computer Engineering\n\n==References==\n{{reflist}}\n\n[[Category:Digital signal processing]]\n[[Category:Filter theory]]\n[[Category:Wavelets]]"
    },
    {
      "title": "Refinable function",
      "url": "https://en.wikipedia.org/wiki/Refinable_function",
      "text": "In [[mathematics]], in the area of [[wavelet]] analysis, a '''refinable function''' is a function which fulfils some kind of [[self-similarity]]. A function <math>\\varphi</math> is called refinable with respect to the mask <math>h</math> if\n:<math>\\varphi(x)=2\\cdot\\sum_{k=0}^{N-1} h_k\\cdot\\varphi(2\\cdot x-k)</math>\nThis condition is called '''refinement equation''', '''dilation equation''' or '''two-scale equation'''.\n\nUsing the [[convolution]] (denoted by a star, *) of a function with a discrete mask and the dilation operator <math>D</math> one can write more concisely:\n:<math>\\varphi=2\\cdot D_{1/2} (h * \\varphi)</math>\nIt means that one obtains the function, again, if you convolve the function with a discrete mask and then scale it back.\nThere is a similarity to [[iterated function systems]] and [[de Rham curve]]s.\n\nThe operator <math>\\varphi\\mapsto 2\\cdot D_{1/2} (h * \\varphi)</math> is linear.\nA refinable function is an [[eigenfunction]] of that operator.\nIts absolute value is not uniquely defined.\nThat is, if <math>\\varphi</math> is a refinable function,\nthen for every <math>c</math> the function <math>c\\cdot\\varphi</math> is refinable, too.\n\nThese functions play a fundamental role in [[wavelet]] theory as [[Wavelet#Scaling_function|scaling functions]].\n\n==Properties==\n===Values at integral points===\n\nA refinable function is defined only implicitly.\nIt may also be that there are several functions which are refinable with respect to the same mask.\nIf <math>\\varphi</math> shall have finite support\nand the function values at integer arguments are wanted,\nthen the two scale equation becomes a system of [[simultaneous linear equations]].\n\nLet <math>a</math> be the minimum index and <math>b</math> be the maximum index\nof non-zero elements of <math>h</math>, then one obtains\n:<math>\n\\begin{pmatrix}\n\\varphi(a)\\\\\n\\varphi(a+1)\\\\\n\\vdots\\\\\n\\varphi(b)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nh_{a  } &         &         &         &         &   \\\\\nh_{a+2} & h_{a+1} & h_{a  } &         &         &   \\\\\nh_{a+4} & h_{a+3} & h_{a+2} & h_{a+1} & h_{a  } &   \\\\\n\\ddots  & \\ddots  & \\ddots  & \\ddots  & \\ddots  & \\ddots \\\\\n  & h_{b  } & h_{b-1} & h_{b-2} & h_{b-3} & h_{b-4} \\\\\n  &         &         & h_{b  } & h_{b-1} & h_{b-2} \\\\\n  &         &         &         &         & h_{b  }\n\\end{pmatrix}\n\\cdot\n\\begin{pmatrix}\n\\varphi(a)\\\\\n\\varphi(a+1)\\\\\n\\vdots\\\\\n\\varphi(b)\n\\end{pmatrix}.\n</math>\n\nUsing the [[discretization]] operator, call it <math>Q</math> here, and the [[transfer matrix]] of <math>h</math>, named <math>T_h</math>, this can be written concisely as\n\n:<math>Q\\varphi = T_h \\cdot Q\\varphi. \\,</math>\n\nThis is again a [[Fixed point (mathematics)|fixed-point equation]]. But this one can now be considered as an [[eigenvector]]-[[eigenvalue]] problem. That is, a finitely supported refinable function exists only (but not necessarily), if <math>T_h</math> has the eigenvalue&nbsp;1.\n\n===Values at dyadic points===\n\nFrom the values at integral points you can derive the values at dyadic points,\ni.e. points of the form <math>k\\cdot 2^{-j}</math>, with <math>k\\in\\mathbb{Z}</math> and <math>j\\in\\mathbb{N}</math>.\n:<math>\\varphi = D_{1/2} (2\\cdot (h * \\varphi))</math>\n:<math>D_2 \\varphi = 2\\cdot (h * \\varphi)</math>\n:<math>Q(D_2 \\varphi) = Q(2\\cdot (h * \\varphi)) = 2\\cdot (h * Q\\varphi)</math>\nThe star denotes the [[convolution]] of a discrete filter with a function.\nWith this step you can compute the values at points of the form <math>\\frac{k}{2}</math>.\nBy replacing iteratedly <math>\\varphi</math> by <math>D_2 \\varphi</math> you get the values at all finer scales.\n\n: <math>Q(D_{2^{j+1}}\\varphi) = 2\\cdot (h * Q(D_{2^j}\\varphi))</math>\n\n===Convolution===\n\nIf <math>\\varphi</math> is refinable with respect to <math>h</math>,\nand <math>\\psi</math> is refinable with respect to <math>g</math>,\nthen <math>\\varphi*\\psi</math> is refinable with respect to <math>h*g</math>.\n\n===Differentiation===\n\nIf <math>\\varphi</math> is refinable with respect to <math>h</math>,\nand the derivative <math>\\varphi'</math> exists,\nthen <math>\\varphi'</math> is refinable with respect to <math>2\\cdot h</math>.\nThis can be interpreted as a special case of the convolution property,\nwhere one of the convolution operands is a derivative of the [[Dirac delta function|Dirac impulse]].\n\n===Integration===\n\nIf <math>\\varphi</math> is refinable with respect to <math>h</math>,\nand there is an antiderivative <math>\\Phi</math> with\n<math>\\Phi(t) = \\int_0^{t}\\varphi(\\tau)\\mathrm{d}\\tau</math>,\nthen the antiderivative <math>t \\mapsto \\Phi(t) + c</math>\nis refinable with respect to mask <math>\\frac{1}{2}\\cdot h</math>\nwhere the constant <math>c</math> must fulfill\n<math>c\\cdot(1 - \\sum_j h_j) = \\sum_j h_j \\cdot \\Phi(-j)</math>.\n\nIf <math>\\varphi</math> has [[compact support|bounded support]],\nthen we can interpret integration as convolution with the [[Heaviside function]] and apply the convolution law.\n\n===Scalar products===\n\nComputing the scalar products of two refinable functions and their translates can be broken down to the two above properties.\nLet <math>T</math> be the translation operator. It holds\n:<math>\\langle \\varphi, T_k \\psi\\rangle = \\langle \\varphi * \\psi^*, T_k\\delta\\rangle = (\\varphi*\\psi^*)(k)</math>\nwhere <math>\\psi^*</math> is the [[adjoint filter|adjoint]] of <math>\\psi</math> with respect to [[convolution]],\ni.e. <math>\\psi^*</math> is the flipped and [[complex conjugate]]d version of <math>\\psi</math>,\ni.e. <math>\\psi^*(t) = \\overline{\\psi(-t)}</math>.\n\nBecause of the above property, <math>\\varphi*\\psi^*</math> is refinable with respect to <math>h*g^*</math>,\nand its values at integral arguments can be computed as eigenvectors of the transfer matrix.\nThis idea can be easily generalized to integrals of products of more than two refinable functions.<ref>{{Cite journal\n  | last1 = Dahmen  | first1 = Wolfgang\n  | last2 = Micchelli | first2 = Charles A.\n  | title = Using the refinement equation for evaluating integrals of wavelets\n  | journal = Journal Numerical Analysis\n  | volume = 30\n  | pages = 507–537\n  | publisher = SIAM\n  | year = 1993\n  | doi=10.1137/0730024\n }}\n</ref>\n\n===Smoothness===\n\nA refinable function usually has a fractal shape.\nThe design of continuous or smooth refinable functions is not obvious.\nBefore dealing with forcing smoothness it is necessary to measure smoothness of refinable functions.\nUsing the Villemoes machine<ref>\n{{Cite web\n |last        = Villemoes\n |first       = Lars\n |title       = Sobolev regularity of wavelets and stability of iterated filter banks\n |url         = http://www.math.kth.se/~larsv/paper3.ps.Z\n |format      = PostScript\n |accessdate  = 2006\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20020511071537/http://www.math.kth.se/~larsv/paper3.ps.Z\n |archivedate = 2002-05-11\n |df          = \n}}\n</ref>\none can compute the smoothness of refinable functions in terms of [[Sobolev space|Sobolev exponents]].\n\nIn a first step the refinement mask <math>h</math> is divided into a filter <math>b</math>, which is a power of the smoothness factor <math>(1,1)</math> (this is a binomial mask) and a rest <math>q</math>.\nRoughly spoken, the binomial mask <math>b</math> makes smoothness and\n<math>q</math> represents a fractal component, which reduces smoothness again.\nNow the Sobolev exponent is roughly\nthe order of <math>b</math> minus [[logarithm]] of the [[spectral radius]] of <math>T_{q*q^*}</math>.\n\n==Generalization==\n\nThe concept of refinable functions can be generalized to functions of more than one variable,\nthat is functions from <math>\\R^d \\to \\R</math>.\nThe most simple generalization is about [[tensor product]]s.\nIf <math>\\varphi</math> and <math>\\psi</math> are refinable with respect to <math>h</math> and <math>g</math>, respectively, then <math>\\varphi\\otimes\\psi</math>\nis refinable with respect to <math>h\\otimes g</math>.\n\nThe scheme can be generalized even more to different scaling factors with respect to different dimensions or even to mixing data between dimensions.<ref>\n{{Citation\n  | last1 = Berger | first = Marc A.\n  | last2 = Wang | first2 = Yang\n  | chapter = Multidimensional two-scale dilation equations (chapter IV)\n  | publisher = Academic Press, Inc.\n  | year = 1992\n  | volume = 2\n  | series = Wavelet Analysis and its Applications\n  | booktitle = Wavelets: A Tutorial in Theory and Applications\n  | editor-last = Chui | editor-first = Charles K.\n  | pages = 295–323 }}\n</ref>\nInstead of scaling by scalar factor like 2 the signal the coordinates are transformed by a matrix <math>M</math> of integers.\nIn order to let the scheme work, the absolute values of all eigenvalues of <math>M</math> must be larger than one.\n(Maybe it also suffices that <math>|\\det M|>1</math>.)\n\nFormally the two-scale equation does not change very much:\n:<math>\\varphi(x)=|\\det M|\\cdot\\sum_{k\\in\\Z^d} h_k\\cdot\\varphi(M\\cdot x-k)</math>\n\n:<math>\\varphi=|\\det M|\\cdot D_{M^{-1}} (h * \\varphi)</math>\n\n==Examples==\n\n* If the definition is extended to [[Distribution_(mathematics)|distributions]], then the [[Dirac delta function|Dirac impulse]] is refinable with respect to the unit vector <math>\\delta</math>, that is known as [[Kronecker delta]]. The <math>n</math>-th derivative of the Dirac distribution is refinable with respect to <math>2^n\\cdot\\delta</math>.\n* The [[Heaviside function]] is refinable with respect to <math>\\frac{1}{2}\\cdot\\delta</math>.\n* The [[truncated power function]]s with exponent <math>n</math> are refinable with respect to <math>\\frac{1}{2^{n+1}}\\cdot\\delta</math>.\n* The [[triangular function]] is a refinable function.<ref>\n{{Cite web\n |last        = Nathanael\n |first       = Berglund\n |title       = Reconstructing Refinable Functions\n |url         = http://www.math.gatech.edu/~berglund/Refinable/index.html\n |accessdate  = 2010-12-24\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20090404080724/http://www.math.gatech.edu/~berglund/Refinable/index.html\n |archivedate = 2009-04-04\n |df          = \n}}\n</ref> [[B-spline]] functions with successive integral nodes are refinable, because of the convolution theorem and the refinability of the [[indicator function|characteristic function]] for the interval <math>[0,1)</math> (a [[boxcar function]]).\n* All [[polynomial function]]s are refinable. For every refinement mask there is a polynomial that is uniquely defined up to a constant factor. For every polynomial of degree <math>n</math> there are many refinement masks that all differ by a mask of type <math>v * (1,-1)^{n+1}</math> for any mask <math>v</math> and the convolutional power <math>(1,-1)^{n+1}</math>.<ref>\n{{cite arxiv\n  | last = Thielemann | first = Henning\n  | title = How to refine polynomial functions\n  | date = 2012-01-29\n  | eprint = 1012.2453\n}}</ref>\n* A [[rational function]] <math>\\varphi</math> is refinable if and only if it can be represented using [[partial fraction]]s as <math>\\varphi(x) = \\sum_{i\\in\\mathbb{Z}} \\frac{s_i}{(x-i)^k}</math>, where <math>k</math> is a [[positive number|positive]] [[natural number]] and <math>s</math> is a real sequence with finitely many non-zero elements (a [[Laurent polynomial]]) such that <math>s | (s \\uparrow 2)</math> (read: <math>\\exists h(z)\\in\\mathbb{R}[z,z^{-1}]\\ h(z)\\cdot s(z) = s(z^2)</math>). The Laurent polynomial <math>2^{k-1}\\cdot h</math> is the associated refinement mask.<ref>\n{{Citation\n  | last1 = Gustafson | first1 =  Paul\n  | last2 = Savir | first2 = Nathan\n  | last3 = Spears | first3 =  Ely\n  | title = A Characterization of Refinable Rational Functions\n  | journal = American Journal of Undergraduate Research\n  | volume = 5\n  | issue = 3\n  | pages = 11–20\n  | date = 2006-11-14\n  \n  | url = http://www.uni.edu/ajur/v5n3/Gufstafson%20et%20al%20new%20pp%2011-20.pdf}}\n</ref>\n\n== References ==\n{{reflist}}\n\n==See also==\n* [[Subdivision surface|Subdivision scheme]]\n\n[[Category:Wavelets]]"
    },
    {
      "title": "Second-generation wavelet transform",
      "url": "https://en.wikipedia.org/wiki/Second-generation_wavelet_transform",
      "text": "In [[signal processing]], the '''second-generation wavelet transform (SGWT)''' is a [[wavelet]] transform where the [[filter (signal processing)|filters]] (or even the represented wavelets) are not designed explicitly, but the transform consists of the application of the [[Lifting scheme]].\nActually, the sequence of lifting steps could be converted to a regular [[discrete wavelet transform]], but this is unnecessary because both design and application is made via the lifting scheme.\nThis means that they are not designed in the [[frequency domain]], as they are usually in the ''classical'' (so to speak ''first generation'') transforms such as the [[discrete wavelet transform|DWT]] and [[continuous wavelet transform|CWT]]).\nThe idea of moving away from the [[Fourier analysis|Fourier]] domain was introduced independently by [[David Donoho]] and [[Harten]] in the early 1990s.\n\n== Calculating transform ==\n\nThe input signal <math>f</math> is split into odd <math>\\gamma _1</math> and even <math>\\lambda _1</math> samples using shifting and [[downsampling]].  The detail coefficients <math>\\gamma _2</math> are then interpolated using the values of <math>\\gamma _1</math> and the ''prediction operator'' on the even values:\n\n:<math>\\gamma _2  = \\gamma _1  - P(\\lambda _1 ) \\, </math>\n\nThe next stage (known as the ''updating operator'') alters the approximation coefficients using the detailed ones:\n\n:<math>\\lambda _2  = \\lambda _1  + U(\\gamma _2 ) \\, </math>\n\n[[Image:Second generation wavelet transform.svg|center|500px|alt=Block diagram of the SGWT]]\n\nThe functions prediction operator <math>P</math> and updating operator <math>U</math>\neffectively define the wavelet used for decomposition.\nFor certain wavelets the lifting steps (interpolating and updating) are repeated several times before the result is produced.\n\nThe idea can be expanded (as used in the DWT) to create a [[filter bank]] with a number of levels.\nThe variable tree used in [[wavelet packet decomposition]] can also be used.\n\n== Advantages ==\n\nThe SGWT has a number of advantages over the classical wavelet transform in that it is quicker to compute (by a factor of 2) and it can be used to generate a [[multiresolution analysis]] that does not fit a uniform grid.  Using a priori information the grid can be designed to allow the best analysis of the signal to be made.\nThe transform can be modified locally while preserving invertibility; it can even adapt to some extent to the transformed signal.\n\n== References ==\n\n* Wim Sweldens: [https://web.archive.org/web/20060220064101/http://ima.umn.edu/industrial/97_98/sweldens/fourth.html Second-Generation Wavelets: Theory and Application]\n\n[[Category:Wavelets]]"
    },
    {
      "title": "Set partitioning in hierarchical trees",
      "url": "https://en.wikipedia.org/wiki/Set_partitioning_in_hierarchical_trees",
      "text": "'''Set partitioning in hierarchical trees''' ('''SPIHT''')<ref>{{cite journal\n|last = Said|first = Amir|author2=Pearlman, William A.\n |title = A new fast and efficient image codec based on set partitioning in hierarchical trees\n|journal = IEEE Transactions on Circuits and Systems for Video Technology\n|volume = 6\n|pages = 243–250\n|date = June 1996\n|issn = 1051-8215\n|doi = 10.1109/76.499834\n|issue = 3}}\n</ref><ref>http://reference.kfupm.edu.sa/content/n/e/a_new_fast_and_e_cient_image_codec_based_661859.pdf{{dead link|date=January 2018 |bot=InternetArchiveBot |fix-attempted=yes }}</ref> is an image [[compression algorithm]] that exploits the inherent similarities across the subbands in a [[wavelet transform|wavelet decomposition]] of an image.\n\n== General description ==\nThe algorithm [[Codes and Keys|codes]] the most important wavelet transform [[coefficients]] first, and transmits the bits so that an increasingly refined copy of the original image can be obtained progressively.\n\n==See also==\n* [[EZW]]\n\n== References ==\n\n{{reflist}}\n\n== External links ==\n\n{{-}}\n{{Compression Methods}}\n\n[[Category:Image compression]]\n[[Category:Wavelets]]"
    },
    {
      "title": "Shearlet",
      "url": "https://en.wikipedia.org/wiki/Shearlet",
      "text": "In applied mathematical analysis, '''shearlets''' are a multiscale framework which allows efficient encoding of [[Anisotropy|anisotropic]] features in [[Multivariable calculus|multivariate]] problem classes. Originally, shearlets were introduced in 2006 <ref name=shearletsintroduction/> for the analysis and [[sparse approximation]] of functions  <math>f \\in L^2(\\R^2)</math>. They are a natural extension of [[wavelet]]s, to accommodate the fact that multivariate functions are typically governed by anisotropic features such as edges in images, since wavelets, as isotropic objects, are not capable of capturing such phenomena.\n\nShearlets are constructed by parabolic [[Scaling (geometry)|scaling]], shearing, and [[Translation (geometry)|translation]] applied to a few generating functions. At fine scales, they are essentially supported within skinny and directional ridges following the parabolic scaling law, which reads ''length² ≈ width''. Similar to wavelets, shearlets arise from the [[affine group]] and allow a unified treatment of the continuum and digital situation leading to faithful implementations. Although they do not constitute an [[orthonormal basis]] for <math>L^2(\\R^2)</math>, they still form a [[Frame of a vector space|frame]] allowing stable expansions of arbitrary functions <math>f \\in L^2(\\R^2)</math>.\n\nOne of the most important properties of shearlets is their ability to provide optimally sparse approximations (in the sense of optimality in <ref name=cartoonbenchmark/>) for [[cartoon-like function]]s <math>f</math>. In imaging sciences, ''cartoon-like functions'' serve as a model for anisotropic features and are compactly supported in <math>[0,1]^2</math> while being <math>C^2</math> apart from a closed piecewise <math>C^2</math> singularity curve with bounded curvature.  The decay rate of the <math>L^2</math>-error of the <math>N</math>-term shearlet approximation obtained by taking the <math>N</math> largest coefficients from the shearlet expansion is in fact optimal up to a log-factor:<ref name=shearletsparsebandlim/><ref name=shearletsparsecompact/>\n:<math>\\| f - f_N \\|_{L^2}^2 \\leq C N^{-2} (\\log N)^3, \\quad N \\to \\infty,</math>\nwhere the constant <math>C</math> depends only on the maximum curvature of the singularity curve and the maximum magnitudes of <math>f</math>, <math>f^{'}</math> and <math>f^{''}</math>. This approximation rate significantly improves the best <math>N</math>-term approximation rate of wavelets providing only <math>O(N^{-1})</math> for such class of functions.\n\nShearlets are to date the only directional representation system that provides sparse approximation of anisotropic features while providing a unified treatment of the continuum and digital realm that allows faithful implementation. Extensions of shearlet systems to <math>L^2(\\R^d), d \\ge 2</math> are also available. A comprehensive presentation of the theory and applications of shearlets can be found in <ref name=shearletbook/>.\n\n== Definition ==\n\n=== Continuous shearlet systems ===\n\n{{multiple image\n | direction = vertical\n | footer = Geometric effects of parabolic scaling and shearing with several parameters a and s.\n | image1 = shearletscaling.gif\n | alt1 = Parabolic scaling\n | image2 = shearletshearing.gif\n | alt2 = Shearing\n }}\n\nThe construction of continuous shearlet systems is based on ''parabolic scaling matrices''\n\n:<math> A_a = \\begin{bmatrix} a & 0 \\\\ 0 & a^{1/2} \\end{bmatrix}, \\quad a > 0 </math>\n\nas a mean to change the resolution, on ''shear matrices''\n\n:<math> S_s = \\begin{bmatrix} 1 & s \\\\ 0 & 1 \\end{bmatrix}, \\quad s \\in \\R </math>\n\nas a means to change the orientation, and finally on translations to change the positioning. \nIn comparison to [[curvelet]]s, shearlets use shearings instead of rotations, the advantage being that the shear operator <math>S_s</math> leaves the [[integer lattice]] invariant in case <math>s \\in \\Z</math>, i.e., <math> S_s \\Z^2 \\subseteq \\Z^2. </math> This indeed allows a unified treatment of the continuum and digital realm, thereby guaranteeing a faithful digital implementation.\n\nFor <math>\\psi \\in L^2(\\R^2)</math> the ''continuous shearlet system'' generated by <math>\\psi</math> is then defined as\n\n:<math> \\operatorname{SH}_{\\mathrm{cont}}(\\psi) = \\{ \\psi_{a,s,t} = a^{3/4} \\psi(S_s A_a (\\cdot - t)) \\mid a > 0, s \\in \\R, t \\in \\R^2 \\}, </math>\n\nand the corresponding ''continuous shearlet transform'' is given by the map\n\n:<math> f \\mapsto \\mathcal{SH}_\\psi f(a, s, t) = \\langle f, \\psi_{a,s,t} \\rangle, \\quad f \\in L^2(\\R^2), \\quad (a, s, t) \\in \\R_{>0} \\times \\R \\times \\R^2. </math>\n\n=== Discrete shearlet systems ===\n\nA discrete version of shearlet systems can be directly obtained from <math>\\operatorname{SH}_{\\mathrm{cont}}(\\psi)</math> by [[Discretization|discretizing]] the parameter set  <math>\\R_{>0} \\times \\R \\times \\R^2.</math> There are numerous approaches for this but the most popular one is given by\n\n:<math> \\{ (2^j, k, A_{2^j}^{-1} S_k^{-1} m) \\mid j \\in \\Z, k \\in \\Z, m \\in \\Z^2\\} \\subseteq \\R_{>0} \\times \\R \\times \\R^2. </math>\n\nFrom this, the ''discrete shearlet system'' associated with the shearlet generator <math>\\psi</math> is defined by\n\n:<math> \\operatorname{SH}(\\psi) = \\{ \\psi_{j,k,m} = 2^{3j/4} \\psi(S_k A_{2^j} \\cdot{} - m) \\mid j \\in \\Z, k \\in \\Z, m \\in \\Z^2 \\}, </math>\n\nand the associated ''discrete shearlet transform'' is defined by\n:<math> f \\mapsto \\mathcal{SH}_\\psi f(j, k, m) = \\langle f, \\psi_{j,k,m} \\rangle, \\quad f \\in L^2(\\R^2), \\quad (j, k, m) \\in \\Z \\times \\Z \\times \\Z^2. </math>\n\n== Examples ==\n\n{{multiple image\n | direction = horizontal\n | image1 = Classshearsupp.svg\n | alt1 = Classical shearlet frequency support\n | caption1 = Trapezoidal frequency support of the classical shearlet.\n | image2 = Classsheartiling.svg\n | alt2 = Classical shearlet frequency tiling\n | caption2 = Frequency tiling of the (discrete) classical shearlet system.\n }}\n\nLet <math>\\psi_1 \\in L^2(\\R)</math> be a function satisfying the ''discrete Calderón condition'', i.e.,\n\n:<math>\\sum_{j \\in \\Z} |\\hat\\psi_1(2^{-j} \\xi)|^2 = 1, \\quad \\xi \\in \\R,</math>\n\nwith <math>\\hat\\psi_1 \\in C^\\infty(\\R)</math> and <math>\\operatorname{supp}\\hat\\psi_1 \\subseteq [-\\tfrac{1}{2}, -\\tfrac{1}{16}] \\cup [\\tfrac{1}{16}, \\tfrac{1}{2}],</math> \nwhere <math>\\hat\\psi_1</math> denotes the [[Fourier transform]] of <math>\\psi_1.</math> For instance, one can choose <math>\\psi_1</math> to be a [[Meyer wavelet]].\nFurthermore, let <math>\\psi_2 \\in L^2(\\R)</math> be such that <math>\\hat\\psi_2 \\in C^\\infty(\\R),</math> <math>\\operatorname{supp}\\hat\\psi_2 \\subseteq [-1, 1]</math> and\n\n:<math>\\sum_{k \\in Z} |\\hat\\psi_2(\\xi + k)|^2 = 1, \\quad \\xi \\in \\R.</math>\n\nOne typically chooses <math>\\hat \\psi_2</math> to be a smooth [[bump function]]. Then <math>\\psi \\in L^2(\\R^2)</math> given by\n\n:<math>\\hat\\psi(\\xi) = \\hat\\psi_1(\\xi_1) \\hat\\psi_2\\left( \\tfrac{\\xi_2}{\\xi_1} \\right), \\quad \\xi = (\\xi_1, \\xi_2) \\in \\R^2,</math>\n\nis called a ''classical shearlet''. It can be shown that the corresponding discrete shearlet system <math>\\operatorname{SH}(\\psi)</math> constitutes a [[Parseval frame]] for <math>L^2(\\R^2)</math> consisting of [[bandlimited]] functions.<ref name=shearletbook/>\n\nAnother example are [[Compact space|compactly]] [[Support (mathematics)|supported]] shearlet systems, where a compactly supported function <math>\\psi \\in L^2(\\R^2)</math> can be chosen so that <math>\\operatorname{SH}(\\psi)</math> forms a [[Frame of a vector space|frame]] for <math>L^2(\\R^2)</math>.<ref name=shearletsparsecompact/><ref name=shearletcompact/><ref name=shearlets3dalpha/><ref name=VideoTextPur/> In this case, all shearlet elements in <math>\\operatorname{SH}(\\psi)</math> are compactly supported providing superior spatial localization compared to the classical shearlets, which are bandlimited. Although a compactly supported shearlet system does not generally form a Parseval frame, any function <math>f \\in L^2(\\R^2)</math> can be represented by the shearlet expansion due to its frame property.\n\n== Cone-adapted shearlets ==\n\nOne drawback of shearlets defined as above is the directional bias of shearlet elements associated with large shearing parameters.\nThis effect is already recognizable in the frequency tiling of classical shearlets (see Figure in Section [[#Examples]]), where the frequency support of a shearlet increasingly aligns along the <math>\\xi_2</math>-axis as the shearing parameter <math>s</math> goes to infinity. \nThis causes serious problems when analyzing a function whose Fourier transform is concentrated around the <math>\\xi_2</math>-axis.\n\n[[File:Conefreq.svg|thumb|alt=Decomposition of the frequency domain into cones|Decomposition of the frequency domain into cones.]]\nTo deal with this problem, the frequency domain is divided into a low-frequency part and two conic regions (see Figure):\n\n:<math> \\begin{align}\n\\mathcal{R} &= \\left\\{ (\\xi_1, \\xi_2) \\in \\R^2 \\mid |\\xi_1|, |\\xi_2| \\leq 1 \\right\\}, \\\\\n\\mathcal{C}_{\\mathrm{h}} &= \\left\\{ (\\xi_1, \\xi_2) \\in \\R^2 \\mid | \\xi_2 / \\xi_1 | \\leq 1, |\\xi_1| > 1 \\right\\}, \\\\\n\\mathcal{C}_{\\mathrm{v}} &= \\left\\{ (\\xi_1, \\xi_2) \\in \\R^2 \\mid | \\xi_1 / \\xi_2 | \\leq 1, |\\xi_2| > 1 \\right\\}.\n\\end{align} </math>\n\n[[File:Coneadaptedtiling.svg|thumb|alt=Frequency tiling of the cone-adapted shearlet system|Frequency tiling of the cone-adapted shearlet system generated by the classical shearlet.]]\nThe associated ''cone-adapted discrete shearlet system'' consists of three parts, each one corresponding to one of these frequency domains.\nIt is generated by three functions <math> \\phi, \\psi, \\tilde\\psi \\in L^2(\\R^2)</math> and a ''lattice [[Sampling (signal processing)|sampling]]'' factor <math>c = (c_1, c_2) \\in (\\R_{> 0})^2:</math>\n\n:<math> \\operatorname{SH}(\\phi, \\psi, \\tilde\\psi; c) = \\Phi(\\phi; c_1) \\cup \\Psi(\\psi; c) \\cup \\tilde\\Psi(\\tilde\\psi; c), </math>\n\nwhere\n\n:<math> \\begin{align}\n\\Phi(\\phi; c_1) &= \\{ \\phi_m = \\phi(\\cdot{} - c_1 m) \\mid m \\in \\Z^2 \\}, \\\\\n\\Psi(\\psi; c) &= \\{ \\psi_{j,k,m} = 2^{3j/4} \\psi(S_k A_{2^j} \\cdot{} - M_c m) \\mid j \\geq 0, |k| \\leq \\lceil 2^{j/2} \\rceil, m \\in \\Z^2 \\}, \\\\\n\\tilde\\Psi(\\tilde\\psi; c) &= \\{ \\tilde{\\psi}_{j,k,m} = 2^{3j/4} \\psi(\\tilde{S}_k \\tilde{A}_{2^j} \\cdot{} - \\tilde{M}_c m) \\mid j \\geq 0, |k| \\leq \\lceil 2^{j/2} \\rceil, m \\in \\Z^2 \\},\n\\end{align} </math>\n\nwith\n\n:<math> \n\\begin{align}\n& \\tilde{A}_a = \\begin{bmatrix} a^{1/2} & 0 \\\\ 0 & a \\end{bmatrix}, \\; a > 0, \\quad \n\\tilde{S}_s = \\begin{bmatrix} 1 & 0 \\\\ s & 1 \\end{bmatrix}, \\; s \\in \\R, \\quad\nM_c = \\begin{bmatrix} c_1 & 0 \\\\ 0 & c_2 \\end{bmatrix}, \\quad  \\text{and} \\quad \\tilde{M}_c = \\begin{bmatrix} c_2 & 0 \\\\ 0 & c_1 \\end{bmatrix}.\n\\end{align}</math>\n\nThe systems <math>\\Psi(\\psi)</math> and <math>\\tilde\\Psi(\\tilde\\psi)</math> basically differ in the reversed roles of <math>x_1</math> and <math>x_2</math>. \nThus, they correspond to the conic regions <math>\\mathcal{C}_{\\mathrm{h}}</math> and <math>\\mathcal{C}_{\\mathrm{v}}</math>, respectively. \nFinally, the ''scaling function'' <math>\\phi</math> is associated with the low-frequency part <math>\\mathcal{R}</math>.\n\n== Applications ==\n\n* [[Image processing]] and [[computer sciences]] <ref name=shearletbook/>\n** [[Image denoising#In images|Denoising]]\n** [[Inverse problems]]\n** [[Image enhancement]]\n** [[Edge detection]]\n** [[Inpainting]]\n** [[Image separation]]\n* [[Partial differential equation|PDEs]] <ref name=shearletbook/>\n** Resolution of the [[wavefront set]] \n** [[Transport equations]]\n* [[Coorbit theory]], characterization of [[Besov space|smoothness spaces]] <ref name=shearletbook/>\n* [[Differential geometry]]: [[manifold learning]]\n\n== Generalizations and extensions==\n\n* 3D-Shearlets <ref name=shearlets3dalpha/><ref name=shearletbandlim/>\n* <math>\\alpha</math>-Shearlets <ref name=shearlets3dalpha/>\n* Parabolic molecules <ref name=parabolicmole/>\n\n== See also ==\n\n* [[Wavelet|Wavelet transform]]\n* [[Curvelet|Curvelet transform]]\n* [[Contourlet|Contourlet transform]]\n* [[Bandelet|Bandelet transform]]\n* [[Chirplet transform]]\n* [[Noiselet|Noiselet transform]]\n\n== References ==\n\n{{reflist|refs=\n<ref name=shearletbook>Kutyniok, Gitta, and Demetrio Labate, eds. ''Shearlets: Multiscale analysis for multivariate data''. Springer, 2012, {{isbn|0-8176-8315-1}}</ref>\n<ref name=shearletbandlim>Guo, Kanghui, and Demetrio Labate. \"The construction of smooth Parseval frames of shearlets.\" Mathematical Modelling of Natural Phenomena '''8.01''' (2013): 82–105.\n{{cite web|url= http://www3.math.tu-berlin.de/numerik/mt/www.shearlet.org/papers/shear_construction.pdf |title=PDF }}</ref>\n<ref name=shearletcompact>Kittipoom, Pisamai, Gitta Kutyniok, and Wang-Q Lim. \"Construction of compactly supported shearlet frames.\" Constructive Approximation '''35.1''' (2012): 21–72.\n{{cite web|url= https://arxiv.org/pdf/1003.5481.pdf |title=PDF }}</ref>\n<ref name=shearletsparsebandlim>Guo, Kanghui, and Demetrio Labate. \"Optimally sparse multidimensional representation using shearlets.\" SIAM Journal on Mathematical Analysis '''39.1''' (2007): 298–318.\n{{cite web|url= http://www3.math.tu-berlin.de/numerik/mt/mt/www.shearlet.org/papers/OSMRuS.pdf |title=PDF }}</ref>\n<ref name=shearletsparsecompact>Kutyniok, Gitta, and Wang-Q Lim. \"Compactly supported shearlets are optimally sparse.\" Journal of Approximation Theory '''163.11''' (2011): 1564–1589.\n{{cite web|url= http://www.math.tu-berlin.de/fileadmin/i26_fg-kutyniok/wangQ/paper/JAT-D-10-00168_final.pdf |title=PDF }}</ref>\n<ref name=shearlets3dalpha>Kutyniok, Gitta, Jakob Lemvig, and Wang-Q Lim. \"Optimally sparse approximations of 3D functions by compactly supported shearlet frames.\" SIAM Journal on Mathematical Analysis '''44.4''' (2012): 2962–3017.\n{{cite web|url= https://arxiv.org/pdf/1109.5993.pdf |title=PDF }}</ref>\n<ref name=shearletsintroduction>Guo, Kanghui, Gitta Kutyniok, and Demetrio Labate. \"Sparse multidimensional representations using anisotropic dilation and shear operators.\" Wavelets and Splines (Athens, GA, 2005), G. Chen and MJ Lai, eds., Nashboro Press, Nashville, TN (2006): 189–201.\n{{cite web|url= http://www3.math.tu-berlin.de/numerik/mt/mt/www.shearlet.org/papers/SMRuADaSO.pdf |title=PDF }}</ref>\n<ref name=VideoTextPur>Purnendu Banerjee and B. B. Chaudhuri, “Video Text Localization using Wavelet and Shearlet Transforms”, In Proc. SPIE 9021, Document Recognition and Retrieval XXI, 2014 (doi:10.1117/12.2036077).{{cite web|url= https://arxiv.org/ftp/arxiv/papers/1307/1307.4990.pdf |title=PDF }}</ref>\n<ref name=cartoonbenchmark>Donoho, David Leigh. \"Sparse components of images and optimal atomic decompositions.\" Constructive Approximation '''17.3''' (2001): 353–382.\n{{cite web|url= http://stats.stanford.edu/~donoho/Reports/1998/SCA.pdf |title=PDF }}</ref>\n<ref name=parabolicmole>Grohs, Philipp and Kutyniok, Gitta. \"Parabolic molecules.\" Foundations of Computational Mathematics (to appear)\n{{cite web|url= https://arxiv.org/pdf/1206.1958.pdf |title=PDF }}</ref>\n}}\n\n== External links ==\n* [http://www.shearlet.org/ Shearlet homepage]\n* [http://www.shearlab.org/ Shearlab homepage]\n* [http://www.math.tu-berlin.de/fachgebiete_ag_modnumdiff/angewandtefunktionalanalysis/v-menue/mitarbeiter/kutyniok/v-menue/home/parameter/en/ Homepage of Gitta Kutyniok]\n* [http://www.math.uh.edu/~dlabate/ Homepage of Demetrio Labate]\n\n[[Category:Image processing]]\n[[Category:Time–frequency analysis]]\n[[Category:Signal processing]]\n[[Category:Wavelets]]"
    },
    {
      "title": "Stationary wavelet transform",
      "url": "https://en.wikipedia.org/wiki/Stationary_wavelet_transform",
      "text": "{{Context|date=October 2009}}\n\n\nThe '''Stationary wavelet transform''' (SWT)<ref>James E. Fowler: [http://ieeexplore.ieee.org/iel5/97/32130/01495429.pdf?arnumber=1495429 The Redundant Discrete Wavelet Transform and Additive Noise], contains an overview of different names for this transform.</ref> is a [[wavelet transform]] algorithm designed to overcome the lack of translation-invariance of the [[discrete wavelet transform]] (DWT). Translation-invariance is achieved by removing the downsamplers and upsamplers in the DWT and upsampling the filter coefficients by a factor of <math>2^{(j-1)}</math> in the <math>j</math>th level of the algorithm.<ref>A.N. Akansu and Y. Liu, On Signal Decomposition Techniques, Optical Engineering, pp. 912-920, July 1991.</ref><ref>M.J. Shensa, The Discrete Wavelet Transform: Wedding the A Trous and Mallat Algorithms, IEEE Transactions on Signal Processing, Vol 40, No 10, Oct. 1992.</ref><ref>M.V. Tazebay and A.N. Akansu, Progressive Optimality in Hierarchical Filter Banks, Proc. IEEE International Conference on Image Processing (ICIP), Vol 1, pp. 825-829, Nov. 1994.</ref><ref>M.V. Tazebay and A.N. Akansu, Adaptive Subband Transforms in Time-Frequency Excisers for DSSS Communications Systems , IEEE Transactions on Signal Processing, Vol 43, No 11, pp. 2776-2782, Nov. 1995.</ref> The SWT is an inherently redundant scheme as the output of each level of SWT contains the same number of samples as the input – so for a decomposition of N levels there is a redundancy of N in the wavelet coefficients. This algorithm is more famously known as \"''algorithme à trous''\" in French (word ''trous'' means holes in English) which refers to inserting zeros in the filters. It was introduced by Holschneider et al.<ref>M. Holschneider, R. Kronland-Martinet, J. Morlet and P. Tchamitchian. A real-time algorithm for signal analysis with the help of the wavelet transform. In ''Wavelets, Time-Frequency Methods and Phase Space'', pp. 289–297. Springer-Verlag, 1989.</ref>\n\n==Implementation==\nThe following block diagram depicts the digital implementation of SWT.\n\n[[Image:Wavelets - SWT Filter Bank.png|frame|none|A 3 level SWT filter bank]]\n \nIn the above diagram, filters in each level are up-sampled versions of the previous (see figure below).\n\n[[Image:Wavelets - SWT Filters.png|frame|none|SWT filters]]\n\nKIT\n\n==Applications==\nA few applications of SWT are specified below.\n\n* Signal denoising\n* Pattern recognition\n* Brain image classification <ref>{{cite journal|last1=Zhang|first1=Y.|title=Feature Extraction of Brain MRI by Stationary Wavelet Transform and its Applications|journal=Journal of Biological Systems|date=2010|volume=18|issue=s1|pages=115–132|doi=10.1142/S0218339010003652}}</ref>\n* Pathological brain detection<ref>{{cite journal|last1=Dong|first1=Z.|title=Magnetic Resonance Brain Image Classification via Stationary Wavelet Transform and Generalized Eigenvalue Proximal Support Vector Machine|journal=Journal of Medical Imaging and Health Informatics|date=2015|volume=5|issue=7|pages=1395–1403|url=http://www.ingentaconnect.com/content/asp/jmihi/2015/00000005/00000007/art00003|doi=10.1166/jmihi.2015.1542}}</ref>\n\n==Synonyms==\n* Redundant wavelet transform\n* Algorithme à trous\n* Quasi-continuous wavelet transform\n* Translation invariant wavelet transform\n* Shift invariant wavelet transform\n* Cycle spinning\n* Maximal overlap wavelet transform (MODWT)\n* Undecimated wavelet transform (UWT)\n\n==See also==\n* [[wavelet transform]]\n* wavelet entropy\n* [[wavelet packet decomposition]]\n\n==References==\n\n<references/>\n\n[[Category:Wavelets]]"
    },
    {
      "title": "Strömberg wavelet",
      "url": "https://en.wikipedia.org/wiki/Str%C3%B6mberg_wavelet",
      "text": "In [[mathematics]], the '''Strömberg wavelet''' is a certain [[orthonormal wavelet]] discovered by Jan-Olov Strömberg and presented in a paper published in 1983.<ref name=Stromberg>Janos-Olov Strömberg, ''A modified Franklin system and higher order spline systems on R<sup>n</sup> as unconditional bases for [[Hardy space]]s'', Conference on Harmonic Analysis in Honor of A. Zygmond, Vol. II, W. Beckner, et al (eds.) Wadsworth, 1983, pp.475-494</ref>  Even though the [[Haar wavelet]] was earlier known to be an orthonormal wavelet, Stromberg wavelet was the first smooth orthonormal wavelet to be discovered. The term  ''wavelet'' had not been coined at the time of publishing the discovery of Strömberg wavelet and Strömberg's motivation was to find an orthonormal basis for the [[Hardy space]]s.<ref name=Stromberg/>\n\n==Definition==\nLe ''m'' be any [[non-negative integer]]. Let ''V'' be any [[discrete subset]] of the set ''R'' of [[real number]]s. Then ''V'' splits ''R'' into non-overlapping [[Interval (mathematics)|interval]]s. For any ''r'' in ''V'', let ''I''<sub>''r''</sub> denote the interval determined by ''V'' with ''r'' as the left endpoint. Let ''P''<sup>(''m'')</sup>(''V'') denote the set of all [[Function (mathematics)|function]]s ''f''(''t'') over ''R'' satisfying the following conditions:\n:*''f''(''t'') is [[square integrable]].\n:*''f''(''t'') has [[Continuity (mathematics)|continuous]] [[derivative]]s of all orders up to ''m''.\n:*''f''(''t'') is a [[polynomial]] of degree ''m'' + 1 in each of the intervals ''I''<sub>''r''</sub>.\nIf ''A''<sub>0</sub> =  {. . . , -2, -3/2, -1, -1/2} &cup; {0} &cup; {1, 2, 3, . . .} and ''A''<sub>1</sub> = ''A''<sub>0</sub> &cup; { 1/2 } then the '''Strömberg wavelet''' of order ''m'' is a function ''S''<sup>''m''</sup>(''t'') satisfying the following conditions:<ref name=Stromberg/>\n:* <math>S^m(t)\\in P^{(m)} (A_1).</math>\n:*<math>\\Vert S^m(t)\\Vert=1</math>, that is, <math>\\int_R\\vert S^m(t)\\vert^2\\, dt = 1.</math>\n:* <math>S^m(t)</math> is [[orthogonal]] to <math>P^{(m)}(A_0)</math>,  that is, <math>\\int_R S^m(t)\\, f(t)\\, dt=0</math> for all <math>f(t)\\in P^{(m)}(A_0).</math>\n\n=== Properties of the set ''P''<sup>(''m'')</sup>(''V'') ===\nThe following are some of the properties of the set ''P''<sup>(''m'')</sup>(''V''):\n# Let the number of distinct elements in ''V'' be two. Then ''f''(''t'') &isin; ''P''<sup>(''m'')</sup>(''V'') if and only if ''f''(''t'') = 0 for all ''t''.\n# If the number of elements in ''V'' is three or more than ''P''<sup>(''m'')</sup>(''V'') contains nonzero functions.\n# If ''V''<sub>1</sub> and ''V''<sub>2</sub> are discrete subsets of ''R'' such that ''V''<sub>1</sub> &sub; ''V''<sub>2</sub> then ''P''<sup>(''m'')</sup>(''V''<sub>1</sub>) &sub; ''P''<sup>(''m'')</sup>(''V''<sub>2</sub>). In particular, ''P''<sup>(''m'')</sup>(''A''<sub>0</sub>) &sub; ''P''<sup>(''m'')</sup>(''A''<sub>1</sub>).\n# If ''f''(''t'') &isin; ''P''<sup>(''m'')</sup>(''A''<sub>1</sub>) then ''f''(''t'') = ''g''(''t'') + &alpha; &lambda;(''t'') where &alpha; is constant and ''g''(''t'') &isin; ''P''<sup>(''m'')</sup>(''A''<sub>0</sub>) is defined by ''g''(''r'') = ''f''(''r'') for ''r'' &isin; ''A''<sub>0</sub>.\n\n=== Strömberg wavelet as an orthonormal wavelet ===\nThe following result establishes the Strömberg wavelet as an [[orthonormal wavelet]].<ref name=Stromberg/>\n\n=== Theorem ===\nLet ''S''<sup>''m''</sup> be the Strömberg wavelet of order ''m''. Then the following set\n::<math>\\left\\{2^{j/2}S^m(2^jt-k):j,k \\text{ integers }\\right\\}</math>\nis a complete [[orthonormal]] system in the space of square integrable functions over ''R''.\n\n==Stromberg wavelets of order 0==\n[[File:StrombergWaveletOfOrderZero.png|thumb|400px|right|The graph of the Strömberg wavelet of order 0. The graph is scaled such that the value of the wavelet function at 1 is 1.]]\n\nIn the special case of Strömberg wavelets of order 0, the following facts may be observed:\n# If ''f''(''t'') &isin; ''P''<sup>0</sup>(''V'') then ''f''(''t'') is defined uniquely by the discrete subset {''f''(''r'') : ''r'' &isin; ''V''} of ''R''.\n# To each ''s'' &isin; ''A''<sub>0</sub>, a special function &lambda;<sub>''s''</sub> in ''A''<sub>0</sub> is associated: It is defined by &lambda;<sub>''s''</sub>(''r'') = 1 if ''r'' = ''s'' and &lambda;<sub>''s''</sub>(''r'') = 0 if  ''s'' &ne; ''r'' &isin; ''A''<sub>0</sub>. These special elements in ''P''(''A''<sub>0</sub>) are called ''simple tents''. The special simple tent &lambda;<sub>1/2</sub>(''t'') is denoted by &lambda;(''t'')\n\n=== Computation of the Strömberg wavelet of order 0 ===\nAs already observed, the Strömberg wavelet ''S''<sup>0</sup>(''t'') is completely determined by the set { ''S''<sup>0</sup>(''r'') : ''r'' &isin; ''A''<sub>1</sub> }. Using the defining properties of the Strömbeg wavelet, exact expressions for elements of this set can be computed and they are given below.<ref name=Woj>{{cite book|last1=P. Wojtaszczyk|title=A Mathematical Introduction to Wavelets|date=1997|publisher=Cambridge University Press|isbn=0521570204|pages=5–14}}</ref>\n::<math> S^0(k) = S^0(1)(\\sqrt{3}-2)^{k-1}</math> for <math> k=1,2,3, \\ldots</math>\n::<math>S^0(\\tfrac{1}{2})  =  -S^0(1)\\left(\\sqrt{3}+\\tfrac{1}{2}\\right)</math>\n::<math>S^0(0) = S^0(1)(2\\sqrt{3}-2)</math>\n::<math>S^0(-\\tfrac{k}{2}) = S^0(1)(2\\sqrt{3}-2)(\\sqrt{3}-2)^k</math> for <math>k=1,2,3, \\ldots</math>\nHere ''S''<sup>0</sup>(1) is constant such that ||''S''<sup>0</sup>(''t'')||&nbsp;=&nbsp;1.\n\n=== Some additional information about Strömberg wavelet of order 0 ===\nThe Strömberg wavelet of order 0 has the following properties.<ref name=Woj/>\n:*The Strömberg wavelet ''S''<sup>0</sup>(''t'') [[oscillation|oscillates]] about ''t''-axis.\n:*The Strömberg wavelet ''S''<sup>0</sup>(''t'') has [[exponential decay]]. \n:*The values of  ''S''<sup>0</sup>(''t'') for positive integral values of ''t'' and for negative half-integral values of ''t'' are related as follows: <math>S^0(-k/2)=(10-6\\sqrt{3})S^0(k)</math> for <math>k=1,2,3,\\ldots\\,.</math>\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Stromberg wavelet}}\n[[Category:Wavelets]]\n[[Category:Continuous wavelets]]"
    },
    {
      "title": "Symlet",
      "url": "https://en.wikipedia.org/wiki/Symlet",
      "text": "{{unreferenced|date=July 2012}}\n\nIn [[applied mathematics]], '''symlet wavelets''' are a family of wavelets. They are a modified version of [[Daubechies wavelet]]s with increased symmetry.\n\n\n\n\n[[Category:Wavelets]]\n\n\n{{applied-math-stub}}"
    },
    {
      "title": "WaveLab (mathematics software)",
      "url": "https://en.wikipedia.org/wiki/WaveLab_%28mathematics_software%29",
      "text": "{{For|the digital audio editor|WaveLab}}\n{{Multiple issues|\n{{refimprove|date=May 2016}}\n{{notability|Products|date=May 2016}}\n}}\n\n{{Infobox software|\n name = WaveLab |\n logo= |\n screenshot = |\n developer =  |\n latest_release_version = |\n latest_release_date = |\n operating_system = [[Cross-platform]] |\n genre =  |\n license =  |\n website = http://www-stat.stanford.edu/~wavelab |\n}}\n\n'''WaveLab''' is a collection of [[MATLAB]] functions for [[wavelet]] analysis. Following the success of WaveLab package, there is now the availability of CurveLab and ShearLab.\n\n[[Category:Wavelets]]\n\n\n{{Science-software-stub}}\n{{Mathapplied-stub}}"
    },
    {
      "title": "Wavelet for multidimensional signals analysis",
      "url": "https://en.wikipedia.org/wiki/Wavelet_for_multidimensional_signals_analysis",
      "text": "{{Orphan|date=November 2015}}\n\n[[Wavelet]]s are often used to analyse piece-wise smooth signals.<ref>{{cite book|last1=Mallat|first1=Stéphane|title=A Wavelet Tour of Signal Processing|date=2008|publisher=Academic Press}}</ref> Wavelet coefficients can efficiently represent a signal which has led to data compression algorithms using wavelets.<ref>{{cite book|last1=Devore|first1=Ronald|last2=Jawerth|first2=Bjorn|last3=Lucier|first3=Bradley|title=Data compression using wavelets: error, smoothness and quantization|journal=IEEE Data Compression Conference|date=8 April 1991|pages=186–195|doi=10.1109/DCC.1991.213386|isbn=978-0-8186-9202-4}}</ref> Wavelet analysis is extended for [[multidimensional signal processing]] as well. This article introduces a few methods for wavelet synthesis and analysis for multidimensional signals. There also occur challenges such as directivity in multidimensional case.\n\n== Multidimensional separable Discrete Wavelet Transform (DWT) ==\nThe [[Discrete wavelet transform]] is extended to the multidimensional case using the [[tensor product]] of well known 1-D wavelets.\nIn 2-D for example, the tensor product space for 2-D is decomposed into  four tensor product vector spaces<ref name=Tensor_products>{{cite journal|last1=Kugarajah|first1=Tharmarajah|last2=Zhang|first2=Qinghua|title=Multidimensional wavelet frames|journal=IEEE Transactions on Neural Networks|date=November 1995|volume=6|issue=6|pages=1552–1556|doi=10.1109/72.471353|pmid=18263450}}</ref> as\n\n{{math| ( &phi;(x) ⨁ &psi;(x) ) ⊗ ( &phi;(y) ⨁ &psi;(y) ) {{=}} { &phi;(x)&phi;(y), &phi;(x)&psi;(y), &psi;(x)&phi;(y), &psi;(x)&psi;(y) }}}\n                                                                                                                                                                                   \nThis leads to the concept of multidimensional separable DWT similar in principle to the multidimensional DFT.\n\n{{math|&phi;(x)&phi;(y)}} gives the approximation coefficients and other subbands:\n\n{{math|&phi;(x)&psi;(y)}} low-high (LH) subband,\n\n{{math|&psi;(x)&phi;(y)}} high-low (HL) subband,\n\n{{math|&psi;(x)&psi;(y)}} high-high (HH) subband,\n\ngive detail coefficients.\n\n===Implementation of multidimensional separable DWT  ===\n\nWavelet coefficients can be computed by passing the signal to be decomposed though a series of filters. In the case of 1-D, there are two filters at every level-one low pass for approximation and one high pass for the details. In the multidimensional case, the number of filters at each level depends on the number of tensor product vector spaces. For M-D, {{math|2<sup>M</sup>}} filters are necessary at every level. Each of these is called a subband. The subband with all low pass (LLL...) gives the approximation coefficients and all the rest give the detail coefficients at that level.\nFor example, for {{math|M{{=}}3}} and a signal of size  {{math| N1 &times; N2 &times; N3}} , a separable DWT can be implemented as follows:\n[[Image:Wiki figures mod.001.png|framed|none|The figure depicts 3-D separable DWT procedure by applying 1-D DWT for each dimension and splitting the data into chunks to obtain wavelets for different subbands]]\n\nApplying the 1-D DWT analysis filterbank in dimension  {{math|N1}}, it is now split into two chunks of  size {{math| {{frac|N1|2}} &times; N2 &times; N3}}. Applying 1-D DWT in  {{math|N2}}  dimension, each of these chunks is split into two more chunks of {{math|{{frac|N1|2}} &times; {{frac|N2|2}} &times; N3}}. This repeated in 3-D gives a total of 8 chunks of size  {{math| {{frac|N1|2}} &times; {{frac|N2|2}} &times; {{frac|N3|2}}}}.<ref>{{cite journal|last1=Cheng-Wu|first1=Po|last2=Gee-Chen|first2=Liang|title=An efficient architecture for two-dimensional discrete wavelet transform|journal=IEEE Transactions on Circuits and Systems for Video Technology|date=7 August 2002|volume=11|issue=4|pages=536–545|doi=10.1109/76.915359}}</ref>\n\n[[Image:Filterbank mod try 2.001.png|framed|none|The figure shows the 3-D analysis filterbank for 3-D separable DWT]]\n\n===Disadvantages of M-D separable DWT===\nThe wavelets generated by the separable DWT procedure are highly shift variant. A small shift in the input signal changes the wavelet coefficients to a large extent. Also, these wavelets are almost equal in their magnitude in all directions and thus do not reflect the orientation or directivity that could be present in the multidimensional signal. For example, there could be an edge discontinuity in an image or an object moving smoothly along a straight line in the space-time 4D dimension. A separable DWT does not fully capture the same.\nIn order to overcome these difficulties, a method of wavelet transform called [[Complex wavelet transform]] (CWT) was developed.\n\n== Multidimensional Complex Wavelet Transform==\nSimilar to the 1-D complex wavelet transform,<ref name=kingsbury>{{cite journal|last1=Kingsbury|first1=Nick|title=Complex Wavelets for Shift Invariant Analysis and Filtering of Signals|journal=Applied and Computational Harmonic Analysis|date=2001|volume=10|issue=3|pages=234–253|doi=10.1006/acha.2000.0343|url=http://www.idealibrary.com}}</ref> tensor products of complex wavelets are considered to produce complex wavelets for multidimensional signal analysis. With further analysis it is seen that these complex wavelets are oriented.<ref name=IEEEmag>{{cite journal|last1=Selesnick|first1=Ivan|last2=Baraniuk|first2=Richard|last3=Kingsbury|first3=Nick|title=The Dual-Tree Complex Wavelet Transform|journal=IEEE Signal Processing Magazine|date=2005|pages=123–151|url=http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1550194&tag=1}}</ref> This sort of orientation helps to resolve the directional ambiguity of the signal.\n\n===Implementation of multidimensional (M-D) dual tree CWT ===\nDual tree CWT in 1-D uses 2 real DWTs, where the first one gives the real part of CWT and the second DWT gives the imaginary part of the CWT. M-D dual tree CWT is analyzed in terms of tensor products. However, it is possible to implement M-D CWTs efficiently using separable M-D DWTs and considering sum and difference of subbands obtained. Additionally, these wavelets tend to be oriented in specific directions.\n\nTwo types of oriented M-D CWTs can be implemented. Considering only the real part of the tensor product of wavelets, real coefficients are obtained. All wavelets are oriented in different directions. This is 2<sup>m</sup> times as expansive where m is the dimensions.\n\nIf both real and imaginary parts of the tensor products of complex wavelets are considered, complex oriented dual tree CWT which is 2 times more expansive than real oriented dual tree CWT is obtained. So there are two wavelets oriented in each of the directions.\nAlthough implementing complex oriented dual tree structure takes more resources, it is used in order to ensure an approximate shift invariance property that a complex analytical wavelet can provide in 1-D. In the 1-D case, it is required that the real part of the wavelet and the imaginary part are [[Hilbert transform]] pairs for the wavelet to be analytical and to exhibit shift invariance. Similarly in the M-D case, the real and imaginary parts of tensor products are made to be approximate Hilbert transform pairs in order to be analytic and shift invariant.<ref name=IEEEmag /><ref>{{cite journal|last1=Selesnick|first1=I.W.|title=Hilbert transform pairs of wavelet bases|journal=IEEE Signal Processing Letters|date=June 2001|volume=8|issue=6|pages=170–173|doi=10.1109/97.923042|citeseerx=10.1.1.139.5369}}</ref>\n \nConsider an example for 2-D dual tree real oriented CWT:\n                                                              \nLet {{math| &psi;(x)}} and {{math| &psi;(y)}} be complex wavelets:\n\n{{math| &psi;(x) {{=}} &psi;(x)<sub>h</sub> + j &psi;(x)<sub>g</sub>}} and {{math| &psi;(y) {{=}} &psi;(y)<sub>h</sub> + j &psi;(y)<sub>g</sub>}}.\n\n{{math| &psi;(x,y) {{=}} [&psi;(x)<sub>h</sub> + j &psi;(x)<sub>g</sub>][ &psi;(y)<sub>h</sub> + j &psi;(y)<sub>g</sub>]\n{{=}} &psi;(x)<sub>h</sub>&psi;(y)<sub>h</sub> - &psi;(x)<sub>g</sub>&psi;(x)<sub>g</sub> + j [&psi;(x)<sub>h</sub>&psi;(y)<sub>g</sub> - &psi;(x)<sub>h</sub>&psi;(x)<sub>g</sub>]}}\n\nThe support of the Fourier spectrum of the wavelet above resides in the first quadrant. When just the real part is considered, {{math|Real(&psi;(x,y)) {{=}} &psi;(x)<sub>h</sub>&psi;(y)<sub>h</sub> - &psi;(x)<sub>g</sub>&psi;(x)<sub>g</sub>}} has support on opposite quadrants (see (a) in figure). Both {{math|&psi;(x)<sub>h</sub>&psi;(y)<sub>h</sub>}} and {{math|&psi;(x)<sub>g</sub>&psi;(y)<sub>g</sub>}} correspond to the HH subband of two different separable 2-D DWTs. This wavelet is oriented at {{math|-45<sup>o</sup>}}.\n\nSimilarly, by considering {{math| &psi;<sub>2</sub>(x,y) {{=}} &psi;(x)&psi;(y)<sup>*</sup>}}, a wavelet oriented at {{math|45<sup>o</sup>}} is obtained. To obtain 4 more oriented real wavelets, {{math|&phi;(x)&psi;(y)}}, {{math|&psi;(x)&phi;(y)}}, {{math|&phi;(x)&psi;(y)<sup>*</sup>}} and {{math|&psi;(x)&phi;(y)<sup>*</sup>}} are considered.\n\nThe implementation of complex oriented dual tree structure is done as follows: Two separable 2-D DWTs are implemented in parallel using the filterbank structure as in the previous section. Then, the appropriate sum and difference of different subbands (LL, LH, HL, HH) give oriented wavelets, a total of 6 in all.\n[[Image:Wavelet orientation.jpg|framed|none|The figure shows the Fourier support of all 6 oriented wavelets obtained by a 2-D real oriented dual tree CWT]]\nSimilarly, in 3-D, 4 separable 3-D DWTs in parallel are needed and a total of 28 oriented wavelets are obtained.\n\n===Disadvantage of M-D CWT===\nAlthough the M-D CWT provides one with oriented wavelets, these orientations are only appropriate to represent the orientation along the (m-1)<sup>th</sup> dimension of a signal with {{math|m}} dimensions. When singularities in [[Manifolds|manifold]]<ref>{{cite book|last1=Boothby|first1=W|title=An Introduction to Differentiable Manifolds and Riemannian Geometry|date=2003|publisher=Academic|location=San Diego}}</ref> of lower dimensions are considered, such as a bee moving in a straight line in the 4-D space-time, oriented wavelets that are smooth in the direction of the manifold and change rapidly in the direction normal to it are needed. A new transform, Hypercomplex Wavelet transform was developed in order to address this issue.\n\n==Hypercomplex Wavelet Transform==\nThe dual tree '''Hypercomplex  Wavelet Transform (HWT)''' developed in <ref name=DHWT>{{cite journal|last1=Lam Chan|first1=Wai|last2=Choi|first2=Hyeokho|last3=Baraniuk|first3=Richard|title=DIRECTIONAL HYPERCOMPLEX WAVELETS FOR MULTIDIMENSIONAL SIGNAL ANALYSIS AND PROCESSING|journal=IEEE Icassp|date=2004|volume=3|pages=996–999|url=http://citeseerx.ist.psu.edu/viewdoc/download?}}</ref> consists of a standard DWT tensor and {{math|2<sup>m -1</sup>}} wavelets obtained from combining the 1-D Hilbert transform of these wavelets along  the n-coordinates. In particular a 2-D HWT consists of the standard 2-D separable DWT  tensor and three additional components:\n\n{{math| H<sub>x</sub> {&psi;(x)<sub>h</sub>&psi;(y)<sub>h</sub>} {{=}} &psi;(x)<sub>g</sub>&psi;(y)<sub>h</sub> }}\n\n{{math| H<sub>y</sub> {&psi;(x)<sub>h</sub>&psi;(y)<sub>h</sub>} {{=}} &psi;(x)<sub>h</sub>&psi;(y)<sub>g</sub> }}\n\n{{math| H<sub>x</sub> H<sub>y</sub> {&psi;(x)<sub>h</sub>&psi;(y)<sub>h</sub>} {{=}} &psi;(x)<sub>g</sub>&psi;(y)<sub>g</sub> }}\n\nFor the 2-D case, this  is named dual tree '''[[quaternion]] Wavelet Transform (QWT)'''.<ref>{{cite journal|last1=Lam Chan|first1=Wai|last2=Choi|first2=Hyeokho|last3=Baraniuk|first3=Richard|title=Coherent Multiscale Image Processing Using Dual-Tree Quaternion Wavelets|journal=IEEE Transactions on Image Processing|volume=17|issue=7|pages=1069–1082|date=2008|doi=10.1109/TIP.2008.924282|pmid=18586616}}</ref>\nThe total redundancy in M-D is {{math|2<sup>m</sup>}} tight frame.\n\n==Directional Hypercomplex Wavelet Transform==\nThe hypercomplex transform described above serves as a building block to construct the '''Directional Hypercomplex  Wavelet Transform (DHWT)'''. A linear combination of the wavelets obtained using the hypercomplex transform give a wavelet oriented in a particular direction. For the 2-D DHWT, it is seen that these linear combinations correspond to the exact 2-D dual tree CWT case.\nFor 3-D, the DHWT can be considered in two dimensions, one DHWT for {{math|n {{=}} 1}} and another for {{math|n {{=}} 2}}. For {{math|n {{=}} 2}}, {{math|n {{=}} m-1}}, so, as in the 2-D case, this corresponds to 3-D dual tree CWT. But the case of {{math|n {{=}} 1}} gives rise to a new DHWT transform. The combination of 3-D HWT wavelets is done in a manner to ensure that the resultant wavelet is lowpass along 1-D and bandpass along 2-D.\nIn,<ref name=DHWT /> this was used to detect line singularities in 3-D space.\n\n==Challenges ahead==\nThe wavelet transforms for multidimensional signals are often computationally challenging which is the case with most multidimensional signals. Also, the methods of CWT and DHWT are redundant even though they offer directivity and shift invariance.\n\n== References ==\n{{Reflist}}\n\n== External links ==\n*[http://www.uio.no/studier/emner/matnat/math/MAT-INF2360/v12/tensorwavelet.pdf Tensor products in wavelet settings]\n*[http://eeweb.poly.edu/iselesni/WaveletSoftware/index.html Matlab implementation of wavelet transforms]\n*[https://arxiv.org/abs/1101.5320 A Panorama on Multiscale Geometric Representations, Intertwining Spatial, Directional and Frequency Selectivity], a review on 2D (two-dimensional) wavelet representations\n\n[[Category:Multidimensional signal processing]]\n[[Category:Wavelets]]"
    },
    {
      "title": "Wavelet modulation",
      "url": "https://en.wikipedia.org/wiki/Wavelet_modulation",
      "text": "{{Modulation techniques}}\n\n'''Wavelet modulation''', also known as '''fractal modulation''', is a [[modulation]] technique that makes use of [[Wavelet|wavelet transformations]] to represent the data being transmitted. One of the objectives of this type of modulation is to send data at multiple rates over a [[channel (communications)|channel]] that is unknown.<ref name=\"test\">[http://scholar.lib.vt.edu/theses/available/etd-08072001-093853/unrestricted/etdset.pdf Wavelet Modulation in Gaussian and Rayleigh Fading Channels, Manish J. Manglani], (Masters thesis)</ref> If the channel is not clear for one specific [[bit rate]], meaning that the signal will not be received, the signal can be sent at a different bit rate where the [[signal to noise ratio]] is higher.\n\n== See also ==\n* [[Wavelet]]\n\n== References ==\n{{reflist}}\n\n{{DEFAULTSORT:Wavelet Modulation}}\n[[Category:Quantized radio modulation modes]]\n[[Category:Wavelets]]\n\n\n{{Telecomm-stub}}"
    },
    {
      "title": "Wavelet packet decomposition",
      "url": "https://en.wikipedia.org/wiki/Wavelet_packet_decomposition",
      "text": "Originally known as '''Optimal Subband Tree Structuring''' (SB-TS) also called '''Wavelet Packet Decomposition''' (WPD)\n(sometimes known as just '''Wavelet Packets''' or '''Subband Tree''') is a [[wavelet]] transform where the discrete-time (sampled) signal is passed through more filters than the [[discrete wavelet transform]] (DWT).\n\n== Introduction ==\n\nIn the DWT, each level is calculated by passing only the previous wavelet approximation coefficients (cA<sub>j</sub>) through discrete-time low and high pass [[quadrature mirror filter]]s.<ref name=coifman1992/>  However, in the WPD, both the detail (cD<sub>j</sub> (in the 1-D case), cH<sub>j</sub>, cV<sub>j</sub>, cD<sub>j</sub> (in the 2-D case)) and approximation coefficients are decomposed to create the full binary tree.<ref name=Daub>Daubechies, I. (1992), Ten lectures on wavelets, SIAM</ref>\n<ref name=akansu1991/><ref name=caglar1991/><ref name=akansu1992/><ref name=benyassine1995/><ref name=akansu1995/>\n\n[[Image:Wavelets - WPD.png|frame|none|Wavelet Packet decomposition over 3 levels. g[n] is the low-pass approximation coefficients, h[n] is the high-pass detail coefficients]]\n\nFor n levels of decomposition the WPD produces 2<sup>n</sup> different sets of coefficients (or nodes) as opposed to (3n + 1) sets for the DWT.  However, due to the [[downsampling]] process the overall number of coefficients is still the same and there is no redundancy.\n\nFrom the point of view of compression, the standard wavelet transform may not produce the best result, since it is limited to wavelet bases that increase by a power of two towards the low frequencies. It could be that another combination of bases produce a more desirable representation for a particular signal. The best basis algorithm by Coifman and Wickerhauser<ref name=coifman1992>Coifman RR & Wickerhauser MV, 1992. [http://www.csee.wvu.edu/~xinl/library/papers/infor/coifman1992.pdf Entropy-Based Algorithms for Best Basis Selection], IEEE Transactions on Information Theory, 38(2).</ref> finds a set of bases that provide the most desirable representation of the data relative to a particular cost function (e.g. [[entropy]]).\n\nThere were relevant studies in signal processing and communications fields to address the selection of subband trees (orthogonal basis) of various kinds, e.g. regular, dyadic, irregular, with respect to performance metrics of interest including energy compaction ([[entropy]]), subband correlations and others.\n<ref name=akansu1991>A.N. Akansu and Y. Liu, [https://web.njit.edu/~akansu/PAPERS/Akansu-LiuOnSignalDecomposition-SPIE-OptEngJuly1991.pdf On Signal Decomposition Techniques], (Invited Paper), Optical Engineering Journal, special issue Visual Communications and Image Processing, vol.30, pp. 912-920, July 1991.</ref>\n<ref name=caglar1991>H. Caglar, Y. Liu and A.N. Akansu, [https://web.njit.edu/~akansu/PAPERS/Akansu-StatOptPR-QMF-SPIENov1991.pdf Statistically Optimized PR-QMF Design], Proc. SPIE Visual Communications and Image Processing, vol. 1605, pp. 86-94, 1991.</ref><ref name=akansu1992>A.N. Akansu and R.A. Haddad, [https://www.amazon.com/Multiresolution-Signal-Decomposition-Second-Transforms/dp/0120471418/ref=sr_1_1?ie=UTF8&qid=1325021689&sr=8-1 Multiresolution Signal Decomposition: Transforms, Subbands, and Wavelets.]\nBoston, MA: Academic Press, {{ISBN|978-0-12-047141-6}}, 1992.</ref>\n<ref name=benyassine1995>A. Benyassine and A.N. Akansu, [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=521408 Performance Analysis and Optimal Structuring of Subchannels for Discrete Multitone Transceivers ], Proc. IEEE \nProc. IEEE International Symposium on Circuits and Systems (ISCAS), pp. 1456-1459, April 1995.</ref>\n<ref name=akansu1995>M.V. Tazebay and A.N. Akansu, [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=482125 Adaptive Subband Transforms in Time-frequency Excisers for DSSS Communications Systems], IEEE Trans. Signal Process., vol. 43, pp. 2776-2782, Nov. 1995.</ref>\n\nDiscrete wavelet transform theory (continuous in the variable(s)) offers an approximation to transform discrete (sampled) signals. In contrast, the discrete subband transform theory provides a perfect representation of discrete signals.<ref name=akansu1992/>\n\n==Gallery==\n<gallery>\nImage:Daubechies12-packet-functions.png\nImage:Daubechies12-packet-spectrum.png\n</gallery>\n\n== Applications ==\n\nWavelet packets were successfully applied in preclinical diagnosis.<ref>{{cite journal|last1=Zhang|first1=Y.|last2=Dong|first2=Z.|title=Preclinical Diagnosis of Magnetic Resonance (MR) Brain Images via Discrete Wavelet Packet Transform with Tsallis Entropy and Generalized Eigenvalue Proximal Support Vector Machine (GEPSVM)|journal=Entropy|date=2015|volume=17|pages=1795–1813|url=http://www.mdpi.com/1099-4300/17/4/1795|doi=10.3390/e17041795}}</ref>\n\n== References==\n<references/>\n\n== External links ==\n\n* An implementation of wavelet packet decomposition can be found in MATLAB wavelet toolbox: [http://www.mathworks.com/access/helpdesk/help/toolbox/wavelet/index.html?/access/helpdesk/help/toolbox/wavelet/ch05_use.html].\n* An implementation for R can be found in the wavethresh package: [https://cran.r-project.org/web/packages/wavethresh/index.html].\n* An illustration and implementation of wavelet packets along with its code in C++ can be found at [http://www.bearcave.com/misl/misl_tech/wavelets/packet/index.html].\n* [https://github.com/cscheiblich/JWave JWave]: An implementation in Java for 1-D and 2-D wavelet packets using [[Haar wavelet|Haar]], [[Daubechies wavelet|Daubechies]], [[Coiflet]], and [[Legendre wavelet|Legendre]] wavelets.\n\n[[Category:Wavelets]]\n[[Category:Signal processing]]"
    },
    {
      "title": "Biorthogonal wavelet",
      "url": "https://en.wikipedia.org/wiki/Biorthogonal_wavelet",
      "text": "A '''Biorthogonal wavelet''' is a [[wavelet]] where the associated [[Discrete wavelet transform|wavelet transform]] is [[invertible]] but not necessarily [[Orthogonality|orthogonal]]. Designing biorthogonal wavelets allows more degrees of freedom than [[orthogonal wavelet]]s. One additional degree of freedom is the possibility to construct symmetric wavelet functions.\n\nIn the biorthogonal case, there are two [[Wavelet#Scaling_function|scaling functions]] <math>\\phi,\\tilde\\phi</math>, which may generate different [[Multiresolution analysis|multiresolution analyses]], and accordingly two different wavelet functions <math>\\psi,\\tilde\\psi</math>. So the numbers ''M'' and ''N'' of coefficients in the scaling sequences <math>a,\\tilde a</math> may differ. The scaling sequences must satisfy the following biorthogonality condition\n:<math>\\sum_{n\\in\\Z} a_n \\tilde a_{n+2m}=2\\cdot\\delta_{m,0}</math>.\nThen the wavelet sequences can be determined as <br>\n<math>b_n=(-1)^n \\tilde a_{M-1-n} \\quad \\quad (n=0,\\dots,N-1) </math><br>\n<math>\\tilde b_n=(-1)^n a_{M-1-n} \\quad \\quad (n=0,\\dots,N-1) </math>.<br>\n\n==References==\n* {{cite book| author = Stéphane G. Mallat| title = A Wavelet Tour of Signal Processing| year = 1999| publisher = Academic Press| isbn = 978-0-12-466606-1 }}\n\n[[Category:Biorthogonal wavelets| ]]\n\n\n{{signal-processing-stub}}"
    },
    {
      "title": "Cohen–Daubechies–Feauveau wavelet",
      "url": "https://en.wikipedia.org/wiki/Cohen%E2%80%93Daubechies%E2%80%93Feauveau_wavelet",
      "text": "[[Image:Jpeg2000 2-level wavelet transform-lichtenstein.png|thumb|256px|An example of the 2D wavelet transform that is used in [[JPEG2000]]]]\n\n'''Cohen–Daubechies–Feauveau wavelet''' are the historically first family of [[biorthogonal wavelet]]s, which was made popular by [[Ingrid Daubechies]].<ref>{{Cite journal | last1 = Cohen | first1 = A. | last2 = Daubechies | first2 = I. | last3 = Feauveau | first3 = J.-C. | doi = 10.1002/cpa.3160450502 | title = Biorthogonal bases of compactly supported wavelets | journal = Communications on Pure and Applied Mathematics | volume = 45 | issue = 5 | pages = 485–560 | year = 1992 | pmid =  | pmc = }}</ref><ref>\n{{cite book\n |first=Ingrid|last=Daubechies\n |title=Ten Lectures on wavelets\n |publisher=SIAM\n |year=1992\n |doi=10.1137/1.9781611970104\n |isbn=978-0-89871-274-2\n}}</ref> These are not the same as the orthogonal [[Daubechies wavelet]]s, and also not very similar in shape and properties. However, their construction idea is the same.\n\nThe [[JPEG 2000]] [[Image compression|compression]] standard uses the biorthogonal CDF 5/3 wavelet (also called the [[LeGall 5/3 wavelet]]) for lossless compression and a CDF 9/7 wavelet for [[lossy compression]].\n\n==Properties==\n\n* The [[primal generator]] is a [[B-spline]] if the simple factorization <math>q_\\text{prim}(X) = 1</math> (see below) is chosen.\n* The [[dual generator]] has the highest possible number of smoothness factors for its length.\n* All generators and wavelets in this family are symmetric.\n\n==Construction==\n\nFor every positive integer ''A'' there exists a unique polynomial <math>Q_A(X)</math> of degree ''A'' − 1 satisfying the identity\n:<math>\\left(1 - \\frac{X}{2}\\right)^A\\,Q_A(X) + \\left(\\frac{X}{2}\\right)^A\\,Q_A(2 - X) = 1.</math>\n\nThis is the same polynomial as used in the construction of the [[Daubechies wavelet]]s. But, instead of a spectral factorization, here we try to factor \n:<math>Q_A(X) = q_\\text{prim}(X)\\,q_\\text{dual}(X),</math>\n\nwhere the factors are polynomials with real coefficients and constant coefficient 1. Then\n:<math>a_\\text{prim}(Z) = 2 Z^d\\,\\left(\\frac{1 + Z}2\\right)^A\\,q_\\text{prim}\\left(1 - \\frac{Z + Z^{-1}}{2}\\right)</math>\n\nand\n:<math>a_\\text{dual}(Z) = 2 Z^d\\,\\left(\\frac{1 + Z}2\\right)^A\\,q_\\text{dual}\\left(1 - \\frac{Z + Z^{-1}}{2}\\right)</math>\n\nform a biorthogonal pair of scaling sequences. ''d'' is some integer used to center the symmetric sequences at zero or to make the corresponding discrete filters causal.\n\nDepending on the roots of <math>Q_A(X)</math>, there may be up to <math>2^{A-1}</math> different factorizations. A simple factorization is <math>q_\\text{prim}(X) = 1</math> and <math>q_\\text{dual}(X) = Q_A(X)</math>, then the primary scaling function is the [[B-spline]] of order ''A'' − 1. For ''A'' = 1 one obtains the orthogonal '''[[Haar wavelet]]'''.\n\n==Tables of coefficients==\n\n[[File:Wavelet Bior2.2.svg|thumb|right|Cohen–Daubechies–Feauveau wavelet 5/3 used in JPEG 2000 standard]]\nFor ''A'' = 2 one obtains in this way the '''LeGall 5/3-wavelet''':\n\n{|class=\"wikitable\"\n!''A''\n!''Q''<sub>''A''</sub>(''X'')\n!''q''<sub>prim</sub>(''X'')\n!''q''<sub>dual</sub>(''X'')\n!''a''<sub>prim</sub>(''Z'')\n!''a''<sub>dual</sub>(''Z'')\n|-\n|2\n|<math>1 + X</math>\n|1\n|<math>1 + X</math>\n|<math>\\frac12(1+Z)^2\\,Z</math>\n|<math>\\frac12(1+Z)^2\\,\\left(-\\frac12 + 2\\,Z - \\frac12\\,Z^2\\right)</math>\n|}\n\n----\nFor ''A'' = 4 one obtains the '''9/7-CDF-wavelet'''. One gets <math>Q_4(X) = 1 + 2X + \\tfrac52 X^2 + \\tfrac52 X^3</math>, this polynomial has exactly one real root, thus it is the product of a linear factor <math>1 - cX</math> and a quadratic factor. The coefficient ''c'', which is the inverse of the root, has an approximate value of −1.4603482098.\n\n{|class=\"wikitable\"\n!''A''\n!''Q''<sub>''A''</sub>(''X'')\n!''q''<sub>prim</sub>(''X'')\n!''q''<sub>dual</sub>(''X'')\n|-\n|4\n|<math>1 + 2X + \\tfrac52 X^2 + \\tfrac52 X^3</math>\n|<math>1-cX</math>\n|<math>1 + (c + 2) X + (c^2 + 2c + \\tfrac52) X^2</math>\n|}\n\nFor the coefficients of the centered scaling and wavelet sequences one gets numerical values in an implementation–friendly form\n\n{|class=\"wikitable\"\n!''k''\n!Analysis lowpass filter\n(1/2 ''a''<sub>dual</sub>)\n!Analysis highpass filter\n(''b''<sub>dual</sub>)\n!Synthesis lowpass filter\n(''a''<sub>prim</sub>)\n!Synthesis highpass filter\n(1/2 ''b''<sub>prim</sub>)\n|-\n| −4\n| 0.026748757411\n| 0\n| 0\n| 0.026748757411\n|-\n| −3\n| −0.016864118443\n| 0.091271763114\n| −0.091271763114\n| 0.016864118443\n|-\n| −2\n| −0.078223266529\n| −0.057543526229\n| −0.057543526229\n| −0.078223266529\n|-\n| −1\n| 0.266864118443\n| −0.591271763114\n| 0.591271763114\n| −0.266864118443\n|-\n| 0\n| 0.602949018236\n| 1.11508705\n| 1.11508705\n| 0.602949018236\n|-\n| 1\n| 0.266864118443\n| −0.591271763114\n| 0.591271763114\n| −0.266864118443\n|-\n| 2\n| −0.078223266529\n| −0.057543526229\n| −0.057543526229\n| −0.078223266529\n|-\n| 3\n| −0.016864118443\n| 0.091271763114\n| −0.091271763114\n| 0.016864118443\n|-\n| 4\n| 0.026748757411\n| 0\n| 0\n| 0.026748757411\n|-\n|}\n\n==Numbering==\n\nThere are two concurring numbering schemes for wavelets of the CDF family:\n* the number of smoothness factors of the lowpass filters, or equivalently the number of [[Moment (mathematics)|vanishing moments]] of the highpass filters, e.g. \"2, 2\";\n* the sizes of the lowpass filters, or equivalently the sizes of the highpass filters, e.g. \"5, 3\".\n\nThe first numbering was used in Daubechies' book ''Ten lectures on wavelets''.\nNeither of this numbering is unique. The number of vanishing moments does not tell about the chosen factorization. A filter bank with filter sizes 7 and 9 can have 6 and 2 vanishing moments when using the trivial factorization, or 4 and 4 vanishing moments as it is the case for the JPEG 2000 wavelet.  The same wavelet may therefore be referred to as \"CDF 9/7\" (based on the filter sizes) or \"biorthogonal 4, 4\" (based on the vanishing moments).  Similarly, the same wavelet may therefore be referred to as \"CDF 5/3\" (based on the filter sizes) or \"biorthogonal 2, 2\" (based on the vanishing moments).\n\n==Lifting decomposition==\n\nFor the trivially factorized filterbanks a [[Lifting scheme|lifting decomposition]] can be explicitly given.<ref>\n{{cite thesis\n |first=Henning\n |last=Thielemann\n |url=http://nbn-resolving.de/urn:nbn:de:gbv:46-diss000103131\n |title=Optimally matched wavelets\n |type=PhD thesis\n |year=2006\n |chapter=section 3.2.4\n}}</ref>\n\n===Even number of smoothness factors===\n\nLet <math>n</math> be the number of smoothness factors in the B-spline lowpass filter,\nwhich shall be even.\n\nThen define recursively\n: <math>\\begin{align}\n  a_0 &= \\frac{1}{n}, \\\\\n  a_m &= \\frac{1}{(n^2 - 4\\cdot m^2)\\cdot a_{m - 1}}.\n\\end{align}</math>\n\nThe lifting filters are\n: <math>s_{m}(z) = a_m\\cdot(2\\cdot m + 1)\\cdot(1 + z^{(-1)^m}).</math>\n\nConclusively, the interim results of the lifting are\n: <math>\\begin{align}\n     x_{-1}(z) &= z, \\\\\n      x_{0}(z) &= 1, \\\\\n  x_{m + 1}(z) &= x_{m - 1}(z) + a_m\\cdot(2\\cdot m + 1)\\cdot(z + z^{-1}) \\cdot z^{(-1)^m} \\cdot x_{m}(z),\n\\end{align}</math>\n\nwhich leads to\n: <math>x_{n/2}(z) = 2^{-n/2} \\cdot (1 + z)^n \\cdot z^{n/2 \\bmod 2 - n/2}.</math>\n\nThe filters <math>x_{n/2}</math> and <math>x_{n/2-1}</math> constitute the CDF-''n'',0 filterbank.\n\n===Odd number of smoothness factors===\n\nNow, let <math>n</math> be odd.\n\nThen define recursively\n: <math>\\begin{align}\n  a_0 &= \\frac{1}{n}, \\\\\n  a_m &= \\frac{1}{(n^2 - (2\\cdot m - 1)^2)\\cdot a_{m - 1}}.\n\\end{align}</math>\n\nThe lifting filters are\n: <math>s_{m}(z) = a_m\\cdot((2\\cdot m + 1) + (2\\cdot m - 1)\\cdot z) / z^{m \\bmod 2}.</math>\n\nConclusively, the interim results of the lifting are\n: <math>\\begin{align}\n     x_{-1}(z) &= z, \\\\\n      x_{0}(z) &= 1, \\\\\n      x_{1}(z) &= x_{-1}(z) + a_0\\cdot x_0(z), \\\\\n  x_{m + 1}(z) &= x_{m - 1}(z) + a_m\\cdot((2\\cdot m + 1)\\cdot z + (2\\cdot m - 1)\\cdot z^{-1}) \\cdot z^{(-1)^m} \\cdot x_{m}(z),\n\\end{align}</math>\n\nwhich leads to\n: <math>x_{(n + 1)/2}(z) \\sim (1 + z)^n,</math>\nwhere we neglect the translation and the constant factor.\n\nThe filters <math>x_{(n + 1)/2}</math> and <math>x_{(n - 1)/2}</math> constitute the CDF-''n'',1 filterbank.\n\n==Applications==\nThe Cohen–Daubechies–Feauveau wavelet and other biorthogonal wavelets have been used to compress [[fingerprint]] scans for the [[FBI]].<ref name=\"cipra94\">\n{{cite book\n |last= Cipra |first= Barry Arthur |author-link = Barry Arthur Cipra\n |title= What's Happening in the Mathematical Sciences (Vol. 2) Parlez-vous Wavelets?\n |publisher=American Mathematical Society\n |year= 1994\n |isbn=978-0821889985\n}}</ref> A standard for compressing fingerprints in this way was developed by Tom Hopper (FBI), Jonathan Bradley ([[Los Alamos National Laboratory]]) and Chris Brislawn (Los Alamos National Laboratory).<ref name=\"cipra94\"/> By using wavelets, a compression ratio of around 20 to 1 can be achieved, meaning a 10&nbsp;MB image could be reduced to as little as 500&nbsp;kB while still passing recognition tests.<ref name=\"cipra94\"/>\n==External links==\n* [http://faculty.gvsu.edu/aboufade/web/wavelets/student_work/EF/how-works.html JPEG 2000: How does it work?]\n* {{webarchive |url=https://web.archive.org/web/20120305164605/http://www.embl.de/~gpau/misc/dwt97.c |date=March 5, 2012 |title=Fast discrete CDF 9/7 wavelet transform source code in C language (lifting implementation) }}\n* [http://www.olhovsky.com/content/wavelet/2dwavelet97lift.py CDF 9/7 Wavelet Transform for 2D Signals via Lifting: Source code in Python]\n* [https://github.com/codeprof/TurboWavelets.Net Open Source 5/3-CDF-Wavelet implementation in C#, for arbitrary lengths]\n\n==References==\n\n<references/>\n\n{{DEFAULTSORT:Cohen-Daubechies-Feauveau wavelet}}\n[[Category:Biorthogonal wavelets]]"
    },
    {
      "title": "Beta wavelet",
      "url": "https://en.wikipedia.org/wiki/Beta_wavelet",
      "text": "[[Continuous wavelets]] of [[compact support]] can be built [1], which are related to the [[beta distribution]].  The process is derived from probability distributions using blur derivative.  These new wavelets have just one cycle, so they are termed unicycle wavelets. They can be viewed as a ''soft variety'' of [[Haar wavelet]]s whose shape is fine-tuned by two parameters <math>\\alpha</math> and <math>\\beta</math>.  Closed-form expressions for beta wavelets and scale functions as well as their spectra are derived.  Their importance is due to the [[Central Limit Theorem]] by Gnedenko and Kolmogorov applied for compactly supported signals [2].\n\n== Beta distribution ==\n\nThe [[beta distribution]] is a continuous probability distribution defined over the interval <math>0\\leq t\\leq 1</math>.  It is characterised by a couple of parameters, namely <math>\\alpha</math> and <math>\\beta</math>  according to:\n\n<math>P(t)=\\frac{1}{B(\\alpha ,\\beta )}t^{\\alpha -1}\\cdot (1-t)^{\\beta -1},\\quad 1\\leq \\alpha ,\\beta \\leq +\\infty </math>.\n\nThe normalising factor is <math>B(\\alpha ,\\beta )=\\frac{\\Gamma (\\alpha )\\cdot \\Gamma (\\beta )}{\\Gamma (\\alpha +\\beta )}</math>,\n\nwhere <math> \\Gamma (\\cdot )</math> is the generalised factorial function of Euler and <math>B(\\cdot ,\\cdot )</math> is the Beta function [4].\n\n== Gnedenko-Kolmogorov central limit theorem revisited ==\n\nLet <math>p_{i}(t)</math> be a probability density of the random variable <math>t_{i}</math>, <math>i=1,2,3..N</math> i.e.\n\n<math>p_{i}(t)\\ge 0</math>, <math>(\\forall t)</math> and <math>\\int_{-\\infty }^{+\\infty }p_{i}(t)dt=1</math>.\n\nSuppose that all variables are independent.\n\nThe mean and the variance of a given random variable <math>t_{i}</math> are, respectively\n\n<math>m_{i}=\\int_{-\\infty }^{+\\infty }\\tau \\cdot p_{i}(\\tau )d\\tau ,</math> <math>\\sigma _{i}^{2}=\\int_{-\\infty }^{+\\infty }(\\tau -m_{i})^{2}\\cdot p_{i}(\\tau )d\\tau </math>.\n\nThe mean and variance of <math>t</math> are therefore <math>m=\\sum_{i=1}^{N}m_{i}</math> and <math>\\sigma^2 =\\sum_{i=1}^{N}\\sigma _{i}^{2}</math>.\n\nThe density <math>p(t)</math> of the random variable corresponding to the sum <math>t=\\sum_{i=1}^{N}t_{i}</math> is given by the\n\n'''Central Limit Theorem for distributions of compact support (Gnedenko and Kolmogorov) [2].'''\n\nLet <math>\\{p_{i}(t)\\}</math> be distributions such that <math>Supp\\{(p_{i}(t))\\}=(a_{i},b_{i})(\\forall i)</math>.\n\nLet <math>a=\\sum_{i=1}^{N}a_{i}<+\\infty </math>, and <math>b=\\sum_{i=1}^{N}b_{i}<+\\infty</math>.\n\nWithout loss of generality assume that <math>a=0</math> and <math>b=1</math>.\n \nThe random variable <math>t</math> holds, as <math>N\\rightarrow \\infty </math>,\n<math>p(t)\\approx </math> <math>\\begin{cases} {k \\cdot t^{\\alpha }(1-t)^{\\beta}}, \\\\otherwise \\end{cases}</math>\n\nwhere <math>\\alpha =\\frac{m(m-m^{2}-\\sigma ^{2})}{\\sigma ^{2}},</math> and <math>\\beta =\\frac{(1-m)(\\alpha +1)}{m}.</math>\n\n== Beta wavelets ==\nSince <math>P(\\cdot |\\alpha ,\\beta )</math> is unimodal, the wavelet generated by\n\n<math>\\psi _{beta}(t|\\alpha ,\\beta )=(-1)\\frac{dP(t|\\alpha ,\\beta )}{dt}</math> \nhas only one-cycle (a negative half-cycle and a positive half-cycle).\n\nThe main features of beta wavelets of parameters <math>\\alpha</math>  and <math>\\beta</math> are:\n\n<math>Supp(\\psi )=[ -\\sqrt{\\frac{\\alpha}{\\beta}}\\sqrt{\\alpha + \\beta +1},\\sqrt{ \\frac{\\beta }{\\alpha }} \\sqrt{\\alpha +\\beta +1}]=[a,b].</math>\n\n<math>lengthSupp(\\psi )=T(\\alpha ,\\beta )=(\\alpha +\\beta )\\sqrt{\\frac{\\alpha +\\beta +1}{\\alpha \\beta }}.</math>\n\nThe parameter <math>R=b/|a| =\\beta / \\alpha</math> is referred to as “cyclic balance”, and is defined as the ratio between the lengths of the causal and non-causal piece of the wavelet. The instant of transition <math>t_{zerocross}</math> from the first to the second half cycle is given by\n\n<math>t_{zerocross}=\\frac{(\\alpha -\\beta )}{(\\alpha +\\beta -2)}\\sqrt{\\frac{\\alpha +\\beta +1}{\\alpha \\beta }}.</math>\n\nThe (unimodal) scale function associated with the wavelets is given by\n\n<math>\\phi _{beta}(t|\\alpha ,\\beta )=\\frac{1}{B(\\alpha ,\\beta )T^{\\alpha +\\beta -1}}\\cdot (t-a)^{\\alpha -1}\\cdot (b-t)^{\\beta -1},</math> <math>a\\leq t\\leq b </math>.\n\nA closed-form expression for first-order beta wavelets can easily be derived. Within their support,\n\n<math>\\psi_{beta}(t|\\alpha ,\\beta ) =\\frac{-1}{B(\\alpha ,\\beta )T^{\\alpha +\\beta -1}} \\cdot [\\frac{\\alpha -1}{t-a}-\\frac{\\beta -1}{b-t}] \\cdot(t-a)^{\\alpha -1} \\cdot(b-t)^{\\beta -1}</math>\n\n[[Image:Beta scale and wavelet.jpg|frame|right|'''Figure. Unicyclic beta scale function and wavelet for different parameters: a) <math>\\alpha =4</math>, <math>\\beta =3</math> b) <math>\\alpha =3</math>, <math>\\beta =7</math> c) <math>\\alpha =5</math>, <math>\\beta =17</math>.''']]\n\n== Beta wavelet spectrum ==\n\nThe beta wavelet spectrum can be derived in terms of the Kummer hypergeometric function [5].\n\nLet <math>\\psi _{beta}(t|\\alpha ,\\beta )\\leftrightarrow \\Psi _{BETA}(\\omega |\\alpha ,\\beta )</math> denote the Fourier transform pair associated with the wavelet.\n\nThis spectrum is also denoted by <math>\\Psi _{BETA}(\\omega)</math> for short. It can be proved by applying properties of the Fourier transform that\n\n<math>\\Psi _{BETA}(\\omega ) =-j\\omega \\cdot M(\\alpha ,\\alpha +\\beta ,-j\\omega (\\alpha +\\beta )\\sqrt{\\frac{\\alpha +\\beta +1}{\\alpha \\beta}})\\cdot exp\\{(j\\omega \\sqrt{\\frac{\\alpha (\\alpha +\\beta +1)}{\\beta }})\\}</math>\n\nwhere <math>M(\\alpha ,\\alpha +\\beta ,j\\nu )=\\frac{\\Gamma (\\alpha +\\beta )}{\\Gamma (\\alpha )\\cdot \\Gamma (\\beta )}\\cdot \\int_{0}^{1}e^{j\\nu t}t^{\\alpha -1}(1-t)^{\\beta -1}dt</math>.\n\nOnly symmetrical <math>(\\alpha =\\beta )</math> cases have zeroes in the spectrum. A few asymmetric <math>(\\alpha \\neq \\beta )</math> beta wavelets are shown in Fig. Inquisitively, they are parameter-symmetrical in the sense that they hold <math>|\\Psi _{BETA}(\\omega |\\alpha ,\\beta )|=|\\Psi _{BETA}(\\omega |\\beta ,\\alpha )|.</math>\n\nHigher derivatives may also generate further beta wavelets.  Higher order beta wavelets are defined by\n<math>\\psi _{beta}(t|\\alpha ,\\beta )=(-1)^{N}\\frac{d^{N}P(t|\\alpha ,\\beta )}{dt^{N}}.</math>\n\nThis is henceforth referred to as an <math>N</math>-order beta wavelet. They exist for order <math>N\\leq Min(\\alpha ,\\beta )-1</math>. After some algebraic handling, their closed-form expression can be found:\n\n<math>\\Psi _{beta}(t|\\alpha ,\\beta ) =\\frac{(-1)^{N}}{B(\\alpha ,\\beta ) \\cdot T^{\\alpha +\\beta -1}} \\sum_{n=0}^{N}sgn(2n-N)\\cdot \\frac{\\Gamma (\\alpha )}{\\Gamma (\\alpha -(N-n))}(t-a)^{\\alpha -1-(N-n)} \\cdot \\frac{\\Gamma (\\beta )}{\\Gamma (\\beta -n)}(b-t)^{\\beta -1-n}.</math>\n\n[[Image:Fig1a.jpg|40px|frame|right|'''Figure. Magnitude of the spectrum <math>\\Psi _{BETA}(\\omega )</math> of beta wavelets, <math>|\\Psi _{BETA}(\\omega \\alpha ,\\beta )|</math>  <math>\\times \\omega</math> for Symmetric beta wavelet <math>\\alpha = \\beta = 3</math>, <math>\\alpha = \\beta = 4</math>, <math>\\alpha = \\beta = 5</math>''']]\n\n[[Image:Fig1b.jpg|40px|frame|right|'''Figure. Magnitude of the spectrum <math>\\Psi _{BETA}(\\omega )</math> of beta wavelets, <math>|\\Psi _{BETA}(\\omega \\alpha ,\\beta )|</math>  <math>\\times \\omega</math> for: Asymmetric beta wavelet <math>\\alpha =3</math>, <math>\\beta =4</math>, <math>\\alpha =3</math>, <math>\\beta =5</math>.''']]\n\n== Application ==\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Almost all practically useful discrete wavelet transforms use discrete-time filter banks. Similarly, Beta wavelet [1][6] and its derivative are utilized in several real-time engineering applications such as image compression[6],bio-medical signal compression[7][8], image recognition [9] etc.\n\n== References ==\n* [1] H.M. de Oliveira, G.A.A. Araújo, Compactly Supported One-cyclic Wavelets Derived from Beta Distributions, ''Journal of Communication and Information Systems'', vol.20, n.3, pp.&nbsp;27–33, 2005.\n* [2] B.V. Gnedenko and A.N. Kolmogorov, Limit Distributions for Sums of Independent Random Variables, Reading, Ma: Addison-Wesley, 1954.\n* [3] W.B. Davenport, Probability and Random Processes, McGraw-Hill /Kogakusha, Tokyo, 1970.\n* [4] P.J. Davies, Gamma Function and Related Functions, in: M. Abramowitz; I. Segun (Eds.), Handbook of Mathematical Functions, New York: Dover, 1968.\n* [5] L.J. Slater, Confluent Hypergeometric Function, in: M. Abramowitz; I. Segun (Eds.), Handbook of Mathematical Functions, New York: Dover, 1968.\n* [6] B.C. Amar, M. Zaied, M.A. Alimi, \"Beta wavelet synthesis and application to lossy image compression\" Adv Eng Softw, 36 (2005), pp.&nbsp;459–474\n* [7] Ranjeet Kumar, A. Kumar and Rajesh K. Pandey, “Electrocardiogram Signal compression Using Beta Wavelets” Journal of Mathematical Modeling and Algorithms, Vol. 11, pp.&nbsp;235–248, 2012.\n* [8] Ranjeet Kumar, A. Kumar and Rajesh K Pandey “Beta Wavelet Based ECG Signal Compression using Loss-less Encoding with Modified Thresholding” Computers & Electrical Engineering, Vol. 39, Issue. 1, pp.&nbsp;130– 140, 2013.\n* [9] Zaied, M., Jemai, O., Ben Amar, C., \"Training of the Beta wavelet networks by the frames theory: Application to face recognition\", Image Processing Theory, Tools and Applications, 2008. DOI: 10.1109/IPTA.2008.4743756\n\n==External links==\n* http://www.iecom.org.br/ \n* http://www2.ee.ufpe.br/codec/WEBLET.html\n* http://www2.ee.ufpe.br/codec/beta.html\n[[Category:Continuous wavelets]]\n[[Category:Functional analysis]]"
    },
    {
      "title": "Complex Mexican hat wavelet",
      "url": "https://en.wikipedia.org/wiki/Complex_Mexican_hat_wavelet",
      "text": "In [[applied mathematics]], the '''complex Mexican hat wavelet''' is a low-oscillation, [[complex number|complex-valued]], [[wavelet]] for the [[continuous wavelet transform]]. This wavelet is formulated in terms of its [[Fourier transform]] as the Hilbert [[analytic signal]] of the conventional [[Mexican hat wavelet]]:\n\n:<math>\\hat{\\Psi}(\\omega)=\\begin{cases} 2\\sqrt{\\frac{2}{3}}\\pi^{-1/4}\\omega^2 e^{-\\frac{1}{2}\\omega^2} & \\omega\\geq0 \\\\[10pt]\n0 & \\omega\\leq 0. \\end{cases}</math>\n\nTemporally, this wavelet can be expressed in terms of the [[error function]],\nas:\n\n:<math>\\Psi(t)=\\frac{2}{\\sqrt{3}}\\pi^{-\\frac{1}{4}}\\left(\\sqrt{\\pi}(1-t^2)e^{-\\frac{1}{2}t^2}-\\left(\\sqrt{2}it+\\sqrt{\\pi}\\operatorname{erf}\\left[\\frac{i}{\\sqrt{2}}t\\right]\\left(1-t^2\\right)e^{-\\frac{1}{2}t^2}\\right)\\right).</math>\n\nThis wavelet has <math>O(|t|^{-3})</math> [[asymptotic]] temporal decay in <math>|\\Psi(t)|</math>,\ndominated by the [[Discontinuity (mathematics)|discontinuity]] of the second [[derivative]] of <math>\\hat{\\Psi}(\\omega)</math>\nat <math>\\omega=0</math>.\n\nThis wavelet was proposed in 2002 by Addison ''et al.''<ref>[http://sbe.napier.ac.uk/staff/paddison/wavelet.htm P. S. Addison, ''et al.'', ''The Journal of Sound and Vibration'', 2002] {{webarchive|url=https://archive.is/20000226173323/http://sbe.napier.ac.uk/staff/paddison/wavelet.htm |date=2000-02-26 }}</ref> for applications requiring high temporal precision [[time-frequency analysis]].\n\n== References ==\n{{reflist}}\n\n[[Category:Continuous wavelets]]"
    },
    {
      "title": "Difference of Gaussians",
      "url": "https://en.wikipedia.org/wiki/Difference_of_Gaussians",
      "text": "{{Redirect|DoG|the animal|Dog}}\nIn [[imaging science]], '''difference of Gaussians''' (DoG) is a [[feature (computer vision)|feature]] enhancement algorithm that involves the subtraction of one blurred version of an original image from another, less blurred version of the original. In the simple case of [[Grayscale|grayscale images]], the blurred images are obtained by [[Convolution|convolving]] the original [[Grayscale|grayscale images]] with Gaussian kernels having differing standard deviations. Blurring an image using a [[Gaussian blur|Gaussian]] [[Convolution kernel|kernel]] suppresses only [[spatial frequencies|high-frequency spatial]] information. Subtracting one image from the other preserves spatial information that lies between the range of frequencies that are preserved in the two blurred images. Thus, the difference of Gaussians is a [[band-pass filter]] that discards all but a handful of spatial frequencies that are present in the original grayscale image.<ref name=\"micro.magnet.fsu.edu\">[http://micro.magnet.fsu.edu/primer/java/digitalimaging/processing/diffgaussians/index.html \"Molecular Expressions Microscopy Primer: Digital Image Processing – Difference of Gaussians Edge Enhancement Algorithm\", ''Olympus America Inc., and Florida State University''] Michael W. Davidson, Mortimer Abramowitz</ref>\n\n==Mathematics of difference of Gaussians==\n\n[[Image:DOG vs MHF.png|thumb|150px|Comparison of difference of Gaussian with [[Mexican hat wavelet]]]]\n\nGiven an m-channel, n-dimensional image\n\n<math>\nI:\\{\\mathbb{X}\\subseteq\\mathbb{R}^n\\}\\rightarrow\\{\\mathbb{Y}\\subseteq\\mathbb{R}^m\\}\n</math>\n\nThe difference of Gaussians (DoG) of the image <math>I</math> is the function\n\n<math>\n\\Gamma_{\\sigma_1,\\sigma_2}:\\{\\mathbb{X}\\subseteq\\mathbb{R}^n\\}\\rightarrow\\{\\mathbb{Z}\\subseteq\\mathbb{R}\\}\n</math>\n\nobtained by subtracting the image <math>I</math> [[Convolution|convolved]] with the Gaussian of variance <math>\\sigma^2_2</math> from the image <math>I</math> [[Convolution|convolved]] with a Gaussian of narrower variance <math>\\sigma^2_1</math>, with <math>\\sigma_2 > \\sigma_1</math>. In one dimension, <math>\\Gamma</math> is defined as:\n\n<math>\n\\Gamma_{\\sigma_1,\\sigma_2}(x)\n=\nI*\\frac{1}{\\sigma_1\\sqrt{2\\pi}} \\, e^{-(x^2)/(2\\sigma^2_1)}-I*\\frac{1}{\\sigma_2\\sqrt{2\\pi}} \\, e^{-(x^2)/(2\\sigma_2^2)}.</math>\n\nand for the centered two-dimensional case :\n\n<math>\n\\Gamma_{\\sigma,K\\sigma}(x,y)\n=\nI*\\frac{1}{2\\pi \\sigma^2} e^{-(x^2 + y^2)/(2 \\sigma^2)} - I*\\frac{1}{2\\pi K^2 \\sigma^2}  e^{-(x^2 + y^2)/(2 K^2 \\sigma^2)}\n</math>\n\nwhich is formally equivalent to:\n\n<math>\n\\Gamma_{\\sigma,K\\sigma}(x,y)\n=\nI*(\\frac{1}{2\\pi \\sigma^2} e^{-(x^2 + y^2)/(2 \\sigma^2)} - \\frac{1}{2\\pi K^2 \\sigma^2}  e^{-(x^2 + y^2)/(2 K^2 \\sigma^2)})\n</math>\n\nwhich represents an image convoluted to the difference of two Gaussians, which approximates a [[Mexican hat wavelet|Mexican Hat]] function.\n\nThe relation between the difference of Gaussians operator and the Laplacian of the Gaussian operator (the [[Mexican hat wavelet]]) is explained in appendix A in Lindeberg (2015).<ref>[http://www.csc.kth.se/~tony/abstracts/Lin15-JMIV.html Lindeberg (2015) ``Image matching using generalized scale-space interest points\", Journal of Mathematical Imaging and Vision, volume 52, number 1, pages 3-36, 2015.]</ref>\n\n==Details and applications==\n[[Image:Flowers before difference of gaussians.jpg|thumb|right|150px|Example before difference of Gaussians]]\n[[Image:Flowers after difference of gaussians grayscale.jpg|thumb|right|150px|After difference of Gaussians filtering in black and white]]\n\nAs a [[feature (computer vision)|feature]] enhancement algorithm, the difference of Gaussians can be utilized to increase the visibility of edges and other detail present in a digital image. A wide variety of alternative [[edge detection|edge sharpening filters]] operate by enhancing high frequency detail, but because [[random noise]] also has a high spatial frequency, many of these sharpening filters tend to enhance noise, which can be an undesirable artifact. The difference of Gaussians algorithm removes high frequency detail that often includes random noise, rendering this approach one of the most suitable for processing images with a high degree of noise. A major drawback to application of the algorithm is an inherent reduction in overall image contrast produced by the operation.<ref name=\"micro.magnet.fsu.edu\"/>\n\nWhen utilized for image enhancement, the difference of Gaussians algorithm is typically applied when the size ratio of kernel (2) to kernel (1) is 4:1 or 5:1. In the example images to the right, the sizes of the Gaussian [[Convolution kernel|kernels]] employed to [[Smooth function|smooth]] the sample image were 10 pixels and 5 pixels. The algorithm can also be used to obtain an approximation of the [[Laplacian of Gaussian]] when the ratio of size 2 to size 1 is roughly equal to 1.6.<ref>\n{{cite journal |author1=D. Marr |author2=E. Hildreth |title=Theory of Edge Detection|journal=Proceedings of the Royal Society of London. Series B, Biological Sciences|volume=207|date=29 February 1980| issue=1167|pages=215–217|jstor=35407| doi=10.1098/rspb.1980.0020|pmid=6102765 |bibcode=1980RSPSB.207..187M}} — '''Note that''' a difference of Gaussians of any scale is an approximation to the laplacian of the Gaussian (see the entry for difference of Gaussians under [[Blob detection]]).  However, Marr and Hildreth recommend the ratio of 1.6 because of design considerations balancing bandwidth and sensitivity.  '''Note also that''' the url for this reference may only make the first page and abstract of the article available depending on if you are connecting through an academic institution or not.</ref> The Laplacian of Gaussian is useful for detecting edges that appear at various image scales or degrees of image focus. The exact values of sizes of the two kernels that are used to approximate the Laplacian of Gaussian will determine the scale of the difference image, which may appear blurry as a result.\n\nDifferences of Gaussians have also been used for [[blob detection]] in the [[scale-invariant feature transform]]. In fact, the DoG as the difference of two [[Multivariate normal distribution]] has always a total null sum and convolving it with a uniform signal generates no response.  It approximates well a second derivate of Gaussian ([[Laplacian of Gaussian]])  with K~1.6 and the receptive fields of ganglion cells in the [[retina]] with K~5. It may easily be used in recursive schemes and is used as an operator in real-time algorithms for blob detection and automatic scale selection.\n\n==More information==\n\nIn its operation, the difference of Gaussians algorithm is believed to mimic how neural processing in the retina of the eye extracts details from images destined for transmission to the brain.<ref>{{cite journal |author1=C. Enroth-Cugell |author2=J. G. Robson |title=The Contrast Sensitivity of Retinal Ganglion Cells of the Cat. |journal=Journal of Physiology |volume= 187 |issue=3 |pages=517–23 |year= 1966|doi=10.1113/jphysiol.1966.sp008107 }}</ref>\n<ref>{{cite journal |author1=Matthew J. McMahon |author2=Orin S. Packer |author3=Dennis M. Dacey |title=The Classical Receptive Field Surround of Primate Parasol Ganglion Cells Is Mediated Primarily by a Non-GABAergic Pathway |journal=Journal of Neuroscience | date= April 14, 2004}}</ref>\n<ref>{{cite journal |author=Young, Richard |title=The Gaussian derivative model for spatial vision: I. Retinal mechanisms |journal=Spatial Vision \n|volume=2 |issue=4 |pages=273–293(21) |year= 1987 |doi=10.1163/156856887X00222}}</ref>\n\n==See also==\n\n*[[Marr-Hildreth algorithm|Marr–Hildreth algorithm]]\n*Treatment of the [[blob detection#The difference of Gaussians approach|difference of Gaussians approach]] in blob detection.\n*[[Blob detection]]\n*[[Scale space]]\n*[[Scale-invariant feature transform]]\n*Notes by Bryan S. Morse on [http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/MORSE/edges.pdf Edge Detection and Gaussian related mathematics] from the University of Edinburgh.\n\n==References==\n{{reflist|colwidth=30em}}\n\n{{DEFAULTSORT:Difference Of Gaussians}}\n[[Category:Continuous wavelets]]\n[[Category:Feature detection (computer vision)]]"
    },
    {
      "title": "Fbsp wavelet",
      "url": "https://en.wikipedia.org/wiki/Fbsp_wavelet",
      "text": "{{cleanup|reason=Obviously, this is a mess.|date=March 2013}}\n\nIn applied mathematics, '''fbsp wavelets''' are '''frequency [[Spline wavelet|B-spline wavelet]]s'''.\n\n''fbsp m-fb-fc''\n\nThese frequency B-spline wavelets are complex wavelets whose spectrum are [[spline (mathematics)|spline]].\n\n: <math> fbsp^{(\\operatorname{m-fb-fc}) }(t) := {\\sqrt {fb}} .\\operatorname{sinc}^m \\left( \\frac {t} {fb^m} \\right). e^{j2 \\pi fc t}   </math>\n\nwhere [[sinc function]] that appears in [[Shannon sampling theorem]].\n\n* ''m'' > 1 is the order of the spline\n* fb is a bandwidth parameter\n* fc is the wavelet center frequency\n\nClearly, [[Shannon wavelet]] (sinc wavelet) is a particular case of fbsp.\n\n[[File:Complex fbsp wavelet.PNG|thumb|Frequency B-Spline wavelets: cubic spline fbsp 3-1-2 complex wavelet.]]\n\n== References ==\n\n* S.G. Mallat, ''A Wavelet Tour of Signal Processing'', Academic Press, 1999, {{ISBN|0-12-466606-X}}\n* [[C. Sidney Burrus|C.S. Burrus]], R.A. Gopinath, H. Guo, ''Introduction to Wavelets and Wavelet Transforms: A Primer'', Prentice-Hall, 1988,  {{ISBN|0-13-489600-9}}.\n* O. Cho, M-J. Lai, A Class of Compactly Supported Orthonormal B-Spline Wavelets in: ''Splines and Wavelets'', Athens 2005, G Chen and M-J Lai Editors pp.&nbsp;123–151.\n* M. Unser, Ten Good Reasons for Using Spline Wavelets, ''Proc. SPIE'', Vol.3169, Wavelets Applications in Signal and Image Processing, 1997, pp.&nbsp;422–431.\n\n== References ==\n\n* S.G. Mallat, ''A Wavelet Tour of Signal Processing'', Academic Press, 1999, {{ISBN|0-12-466606-X}}\n* [[C. Sidney Burrus|C.S. Burrus]], R.A. Gopinath, H. Guo, ''Introduction to Wavelets and Wavelet Transforms: A Primer'', Prentice–Hall, 1988,  {{ISBN|0-13-489600-9}}.\n* O. Cho, M-J. Lai, A Class of Compactly Supported Orthonormal B-Spline Wavelets in: ''Splines and Wavelets'', Athens 2005, G Chen and M-J Lai Editors pp.&nbsp;123–151.\n* M. Unser, Ten Good Reasons for Using Spline Wavelets, ''Proc. SPIE'', Vol.3169, Wavelets Applications in Signal and Image Processing, 1997, pp.&nbsp;422–431.\n\n[[Category:Continuous wavelets]]\n[[Category:Functional analysis]]"
    },
    {
      "title": "Hermitian hat wavelet",
      "url": "https://en.wikipedia.org/wiki/Hermitian_hat_wavelet",
      "text": "{{Unreferenced|date=December 2006}}\n\nThe '''Hermitian hat wavelet''' is a low-[[oscillation]], complex-valued [[wavelet]].\nThe real and imaginary parts of this wavelet are defined to be the\nsecond and first [[derivative]]s of a [[Gauss]]ian respectively:\n\n<math>\\Psi(t)=\\frac{2}{\\sqrt{5}}\\pi^{-\\frac{1}{4}}(1-t^{2}+it)e^{-\\frac{1}{2}t^{2}}.</math>\n\nThe [[Fourier transform]] of this wavelet is:\n\n<math>\\hat{\\Psi}(\\omega)=\\frac{2}{\\sqrt{5}}\\pi^{-\\frac{1}{4}}\\omega(1+\\omega)e^{-\\frac{1}{2}\\omega^{2}}.</math>\n\nThe Hermitian hat wavelet satisfies the admissibility criterion. The prefactor <math>C_{\\Psi}</math> in the resolution of the identity of the continuous wavelet transform is:\n\n<math>C_{\\Psi}=\\frac{16}{5}\\sqrt{\\pi}.</math>\n\nThis wavelet was formulated by Szu in 1997 for the numerical estimation of\nfunction derivatives in the presence of noise. The\ntechnique used to extract these derivative values exploits only the\nargument (phase) of the wavelet and, consequently, the relative weights\nof the real and imaginary parts are unimportant.\n\n[[Category:Continuous wavelets]]"
    },
    {
      "title": "Hermitian wavelet",
      "url": "https://en.wikipedia.org/wiki/Hermitian_wavelet",
      "text": "{{Unreferenced|date=December 2009}}\n'''Hermitian wavelets''' are a family of [[continuous wavelet]]s, used in the [[continuous wavelet transform]]. The <math>n^\\textrm{th}</math> Hermitian wavelet is defined as the <math>n^\\textrm{th}</math> derivative of a [[Gaussian distribution]]:\n\n<math>\\Psi_{n}(t)=(2n)^{-\\frac{n}{2}}c_{n}He_{n}\\left(t\\right)e^{-\\frac{1}{2n}t^{2}}</math>\n\nwhere <math>He_{n}\\left({x}\\right)</math> denotes the <math>n^\\textrm{th}</math> [[Hermite polynomial]].\n\nThe normalisation coefficient <math>c_{n}</math> is given by:\n\n<math>c_{n} = \\left(n^{\\frac{1}{2}-n}\\Gamma(n+\\frac{1}{2})\\right)^{-\\frac{1}{2}} = \\left(n^{\\frac{1}{2}-n}\\sqrt{\\pi}2^{-n}(2n-1)!!\\right)^{-\\frac{1}{2}}\\quad n\\in\\mathbb{Z}.</math>\n\nThe prefactor <math>C_{\\Psi}</math> in the resolution of the identity of the continuous wavelet transform for this wavelet is given by:\n\n<math>C_{\\Psi}=\\frac{4\\pi n}{2n-1}</math>\n\ni.e. Hermitian wavelets are admissible for all positive <math>n</math>.\n\nIn [[computer vision]] and [[image processing]], Gaussian derivative operators of different orders are frequently used as a basis for expressing various types of visual operations; see [[scale space]] and [[N-jet]].\n\n'''Examples of Hermitian wavelets:'''\nStarting from a [[Gaussian function]] with <math>\\mu=0, \\sigma=1</math>:\n\n<math>f(t) = \\pi^{-1/4}e^{(-t^2/2)}</math>\n\nthe first 3 derivatives read\n\n:<math>\\begin{align}\n            f'(t)  & = -\\pi^{-1/4}te^{(-t^2/2)} \\\\\n                          f''(t)          & = \\pi^{-1/4}(t^2 - 1)e^{(-t^2/2)}\\\\\nf^{(3)}(t) & = \\pi^{-1/4}(3t - t^3)e^{(-t^2/2)}\n       \\end{align}</math>\n\nand their <math>L^2</math> norms <math>||f'||=\\sqrt{2}/2, ||f''||=\\sqrt{3}/2, ||f^{(3)}||= \\sqrt{30}/4</math>\n\nSo the wavelets which are the negative normalized derivatives are:\n\n:<math>\\begin{align}\n\\Psi_{1}(t) &= \\sqrt{2}\\pi^{-1/4}te^{(-t^2/2)}\\\\\n\\Psi_{2}(t) &=\\frac{2}{3}\\sqrt{3}\\pi^{-1/4}(1-t^2)e^{(-t^2/2)}\\\\\n\\Psi_{3}(t) &= \\frac{2}{15}\\sqrt{30}\\pi^{-1/4}(t^3 - 3t)e^{(-t^2/2)}\n\\end{align}</math>\n\n\n\n{{DEFAULTSORT:Hermitian Wavelet}}\n[[Category:Continuous wavelets]]"
    },
    {
      "title": "Mexican hat wavelet",
      "url": "https://en.wikipedia.org/wiki/Mexican_hat_wavelet",
      "text": "[[File:MexicanHatMathematica.svg|thumb|250px|Mexican hat]]\n\nIn [[mathematics]] and [[numerical analysis]], the '''Ricker wavelet'''<ref>{{cite web |url=http://74.3.176.63/publications/recorder/1994/09sep/sep94-choice-of-wavelets.pdf |title=Archived copy |accessdate=2014-12-27 |deadurl=yes |archiveurl=https://web.archive.org/web/20141227215059/http://74.3.176.63/publications/recorder/1994/09sep/sep94-choice-of-wavelets.pdf |archivedate=2014-12-27 |df= }}</ref>\n:<math>\\psi(t) = \\frac{2}{\\sqrt{3\\sigma}\\pi^{1/4}} \\left(1 - \\left(\\frac{t}{\\sigma}\\right)^2 \\right) e^{-\\frac{t^2}{2\\sigma^2}}</math>\nis the '''negative''' [[normalizing constant|normalized]] '''second''' [[derivative]] of a [[Gaussian function]], i.e., up to scale and normalization, the second [[Hermite function]]. It is a special case of the family of [[continuous wavelet]]s ([[wavelet]]s used in a [[continuous wavelet transform]]) known as [[Hermitian wavelet]]s. The Ricker wavelet is frequently employed to model seismic data, and as a broad spectrum source term in computational electrodynamics. It is usually only referred to as the '''Mexican hat wavelet''' in the Americas, due to taking the shape of a [[sombrero]] when used as a 2D image processing kernel. It is also known as the '''Marr wavelet''' for [[David Marr (neuroscientist)|David Marr]].<ref>http://www2.isye.gatech.edu/~brani/isyebayes/bank/handout20.pdf</ref><ref>http://cxc.harvard.edu/ciao/download/doc/detect_manual/wav_theory.html#wav_theory_mh</ref>\n\n:<math>\n\\psi(x,y) = \\frac{1}{\\pi\\sigma^2}\\left(1-\\frac{1}{2} \\left(\\frac{x^2+y^2}{\\sigma^2}\\right)\\right) e^{-\\frac{x^2+y^2}{2\\sigma^2}}\n</math>\n[[File:Marr-wavelet2.jpg|thumb|3D view of 2D Mexican hat wavelet]]\nThe multidimensional generalization of this wavelet is called the [[Laplacian of Gaussian]] function. In practice, this wavelet is sometimes approximated by the [[difference of Gaussians]] function, because the DoG is separable<ref>{{cite web|last=Fisher, Perkins, Walker and Wolfart|title=Spatial Filters - Gaussian Smoothing|url=http://homepages.inf.ed.ac.uk/rbf/HIPR2/gsmooth.htm|accessdate=23 February 2014}}</ref> and can therefore save considerable computation time in two or more dimensions. The scale normalised Laplacian (in <math>L_1</math>-norm) is frequently used as a [[blob detection|blob detector]] and for automatic scale selection in [[computer vision]] applications; see [[Laplacian of Gaussian]] and [[scale space]]. The relation between this Laplacian of the Gaussian operator and the [[Difference of Gaussians|difference-of-Gaussians operator]] is explained in appendix A in Lindeberg (2015).<ref>[http://www.csc.kth.se/~tony/abstracts/Lin15-JMIV.html Lindeberg (2015) ``Image matching using generalized scale-space interest points\", Journal of Mathematical Imaging and Vision, volume 52, number 1, pages 3-36, 2015.]</ref> The Mexican hat wavelet can also be approximated by [[derivative]]s of [[B-spline#Cardinal B-spline|Cardinal B-Splines]].<ref>Brinks R: ''On the convergence of derivatives of B-splines to derivatives of the Gaussian function'', Comp. Appl. Math., 27, 1, 2008</ref>\n\n== See also ==\n\n==References==\n{{reflist}}\n\n[[Category:Continuous wavelets]]"
    },
    {
      "title": "Morlet wavelet",
      "url": "https://en.wikipedia.org/wiki/Morlet_wavelet",
      "text": "[[File:MorletWaveletMathematica.svg|thumb|Real-valued Morlet wavelet]]\n[[File:Wavelet Cmor.svg|thumb|Complex-valued Morlet wavelet]]\n\nIn [[mathematics]], the '''Morlet wavelet''' (or '''Gabor wavelet''')<ref name=\"Bernardino\">[http://homepages.inf.ed.ac.uk/rbf/CAVIAR/PAPERS/05-ibpria-alex.pdf A Real-Time Gabor Primal Sketch for Visual Attention] \"The Gabor kernel satisfies the admissibility condition for wavelets, thus being suited for multi-resolution analysis. Apart from a scale factor, it is also known as the Morlet Wavelet.\"</ref> is a [[wavelet]] composed of a [[complex exponential]] ([[carrier signal|carrier]]) multiplied by a [[Gaussian window]] (envelope).  This wavelet is closely related to human perception, both hearing<ref name=\"Mallat\"/> and vision.<ref>[[John Daugman|J. G. Daugman]]. Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by two-dimensional visual cortical filters. ''Journal of the Optical Society of America A'', 2(7):1160–1169, July 1985.</ref>\n\n==History==\nIn 1946, physicist [[Dennis Gabor]], applying ideas from [[quantum physics]], introduced the use of Gaussian-windowed sinusoids for time-frequency decomposition, which he referred to as ''[[Gabor atom|atoms]]'', and which provide the best trade-off between spatial and frequency resolution.<ref name=\"Bernardino\"/>  These are used in the [[Gabor transform]], a type of [[short-time Fourier transform]].<ref name=\"Mallat\">[http://cnx.org/content/m23074/latest/ Time-Frequency Dictionaries], Mallat</ref>  In 1984, [[Jean Morlet]] introduced Gabor's work to the seismology community and, with Goupillaud and Grossmann, modified it to keep the same wavelet shape over equal octave intervals, resulting in the first formalization of the [[continuous wavelet transform]].<ref>http://rocksolidimages.com/pdf/gabor.pdf</ref>  (See also [[Wavelet#History|Wavelet history]])\n\n==Definition==\nThe wavelet is defined as a constant <math>\\kappa_{\\sigma}</math> subtracted from a plane wave and then localised by a [[Gaussian]] [[Window function#Gaussian window|window]]:<ref name=\"Ashmead2012\">\n{{cite journal\n | author = John Ashmead\n | title  = Morlet Wavelets in Quantum Mechanics\n | journal = Quanta\n | year = 2012\n | volume = 1\n | issue = 1\n | pages = 58–70\n | doi = 10.12743/quanta.v1i1.5\n | url = http://quanta.ws/ojs/index.php/quanta/article/view/5\n| arxiv = 1001.0250\n }}\n</ref>\n\n:<math>\\Psi_{\\sigma}(t)=c_{\\sigma}\\pi^{-\\frac{1}{4}}e^{-\\frac{1}{2}t^{2}}(e^{i\\sigma t}-\\kappa_{\\sigma})</math>\n\nwhere <math>\\kappa_{\\sigma}=e^{-\\frac{1}{2}\\sigma^{2}}</math> is defined by the admissibility criterion,\nand the normalisation constant <math>c_{\\sigma}</math> is:\n\n:<math>c_{\\sigma}=\\left(1+e^{-\\sigma^{2}}-2e^{-\\frac{3}{4}\\sigma^{2}}\\right)^{-\\frac{1}{2}}</math>\n\nThe [[Fourier transform]] of the Morlet wavelet is:\n\n:<math>\\hat{\\Psi}_{\\sigma}(\\omega) = c_\\sigma \\pi^{-\\frac{1}{4}} \\left( e^{-\\frac{1}{2}(\\sigma-\\omega)^2} - \\kappa_\\sigma e^{-\\frac{1}{2}\\omega^{2}} \\right)</math>\n\nThe \"central frequency\" <math>\\omega_{\\Psi}</math> is the position of the global maximum of <math>\\hat{\\Psi}_{\\sigma}(\\omega)</math> which, in this case, is given by the positive solution to:\n\n:<math>\\omega_{\\Psi} = \\sigma \\frac{1}{1 - e^{-\\sigma \\omega_{\\Psi}}}</math>{{Citation needed|date=July 2018}}\n\nwhich can be solved by a [[fixed-point iteration]] starting at <math>\\omega_{\\Psi} = \\sigma</math> (the fixed point iterations converge to the unique positive solution for any initial <math>\\omega_{\\Psi}>0</math>){{Citation needed|date=July 2018}}. \n\nThe parameter <math>\\sigma</math> in the Morlet wavelet allows trade between\ntime and frequency resolutions. Conventionally, the restriction <math>\\sigma>5</math> is used to avoid problems with the Morlet wavelet at low <math>\\sigma</math> (high temporal resolution){{Citation needed|date=July 2018}}.\n\nFor signals containing only slowly varying frequency and amplitude\nmodulations (audio, for example) it is not necessary to use small\nvalues of <math>\\sigma</math>. In this case, <math>\\kappa_{\\sigma}</math> becomes very small (e.g. <math>\\sigma>5 \\quad \\Rightarrow \\quad \\kappa_{\\sigma}<10^{-5}\\,</math>) and is, therefore, often neglected. Under the restriction <math>\\sigma>5</math>, the frequency of the Morlet wavelet is conventionally taken to be <math>\\omega_{\\Psi}\\simeq\\sigma</math>{{Citation needed|date=July 2018}}.\n\nThe wavelet exists as a complex version or a purely real-valued version.  Some distinguish between the \"real Morlet\" vs the \"complex Morlet\".<ref>[http://www.mathworks.com/help/toolbox/wavelet/ug/f8-24282.html#f8-23908 Matlab Wavelet Families] - \"Morlet Wavelet: morl\" and \"Complex Morlet Wavelets: cmor\"</ref>  Others consider the complex version to be the \"Gabor wavelet\", while the real-valued version is the \"Morlet wavelet\".<ref>Mathematica documentation: [http://reference.wolfram.com/mathematica/ref/GaborWavelet.html GaborWavelet]</ref><ref>Mathematica documentation: [http://reference.wolfram.com/mathematica/ref/MorletWavelet.html MorletWavelet]</ref>\n\n==Uses==\n=== Use in medicine ===\nThe Morlet wavelet transform method presented offers an intuitive bridge between frequency and time information which can clarify interpretation of complex head trauma spectra obtained with [[Fourier transform]]. The Morlet wavelet transform, however, is not intended as a replacement for the Fourier transform, but rather a supplement that allows qualitative access to time related changes and takes advantage of the multiple dimensions available in a [[free induction decay]] analysis.<ref>http://cds.ismrm.org/ismrm-2001/PDF3/0822.pdf</ref>\n\nThe application of the Morlet wavelet analysis is also used to discriminate abnormal heartbeat behavior in the electrocardiogram (ECG). Since the variation of the abnormal heartbeat is a non-stationary signal, this signal is suitable for wavelet-based analysis.\n\n=== Use in music ===\nThe Morlet wavelet transform method is applied to music transcription. It produces very accurate results that were not possible using Fourier transform techniques. The Morlet wavelet transform is capable of capturing short bursts of repeating and alternating music notes with a clear start and end time for each note.{{cn|date=November 2018}}\n\n== See also ==\n* [[Constant-Q transform]]\n* [[Gabor wavelet]]\n\n== References ==\n<references/>\n* P. Goupillaud, A. Grossman, and J. Morlet. ''Cycle-Octave and Related Transforms in Seismic Signal Analysis''. Geoexploration, 23:85-102, 1984\n* N. Delprat, B. Escudié, P. Guillemain, R. Kronland-Martinet, P. Tchamitchian, and B. Torrésani. Asymptotic wavelet and Gabor analysis: extraction of instantaneous frequencies. IEEE Trans. Inf. Th., 38:644-664, 1992\n\n[[Category:Continuous wavelets]]"
    },
    {
      "title": "Shannon wavelet",
      "url": "https://en.wikipedia.org/wiki/Shannon_wavelet",
      "text": "{{Context|date=June 2017}}\nIn [[functional analysis]], a '''Shannon wavelet''' may be either of [[real number|real]] or [[complex number|complex]] type. \nSignal analysis by ideal [[bandpass filter]]s defines a decomposition known as Shannon wavelets (or '''sinc wavelets'''). The Haar and sinc systems are Fourier duals of each other.\n\n== Real Shannon wavelet ==\n\n[[File:Wavelet Shan.svg|thumb|right|Real Shannon wavelet]]\nThe [[Fourier transform]] of the Shannon mother wavelet is given by:\n\n: <math> \\Psi^{(\\operatorname{Sha}) }(w) = \\prod \\left( \\frac {w- 3 \\pi /2} {\\pi}\\right)+\\prod \\left( \\frac {w+ 3 \\pi /2} {\\pi}\\right). </math>\n\nwhere the (normalised) [[gate function]] is defined by\n\n: <math> \\prod ( x):= \n\\begin{cases}\n1, & \\mbox{if } {|x| \\le 1/2}, \\\\\n0 & \\mbox{if } \\mbox{otherwise}. \\\\\n\\end{cases} </math>\n\nThe analytical expression of the real Shannon wavelet can be found by taking the inverse [[Fourier transform]]:\n\n: <math> \\psi^{(\\operatorname{Sha}) }(t) = \\operatorname{sinc} \\left( \\frac {t} {2}\\right)\\cdot \\cos \\left( \\frac {3 \\pi t} {2}\\right)</math>\nor alternatively as\n\n: <math> \\psi^{(\\operatorname{Sha})}(t)=2 \\cdot \\operatorname{sinc}(2t - 1)-\\operatorname{sinc}(t), </math>\n\nwhere\n\n: <math>\\operatorname{sinc}(t):= \\frac {\\sin {\\pi t}} {\\pi t}</math>\n\nis the usual [[sinc function]] that appears in [[Shannon sampling theorem]].\n\nThis wavelet belongs to the <math>C^\\infty</math>-class of [[Smooth function#Differentiability classes|differentiability]], but it decreases slowly at infinity and has no [[Support (mathematics)#Compact support|bounded support]], since [[band-limited]] signals cannot be time-limited.\n\nThe [[Wavelet#Scaling function|scaling function]] for the Shannon MRA (or ''Sinc''-MRA) is given by the sample function:\n\n: <math>\\phi^{(Sha)}(t)= \\frac {\\sin \\pi t} {\\pi t} = \\operatorname{sinc}(t).</math>\n\n== Complex Shannon wavelet ==\n\nIn the case of [[complex number|complex]] continuous wavelet, the Shannon wavelet is defined by\n:<math> \\psi^{(CSha) }(t)=\\operatorname{sinc}(t) \\cdot e^{-j2 \\pi t}</math>,\n\n== References ==\n\n* S.G. Mallat, ''A Wavelet Tour of Signal Processing'', Academic Press, 1999, {{isbn|0-12-466606-X}}\n* [[C. Sidney Burrus|C.S. Burrus]], R.A. Gopinath, H. Guo, ''Introduction to Wavelets and Wavelet Transforms: A Primer'', Prentice-Hall, 1988,  {{isbn|0-13-489600-9}}.\n\n[[Category:Continuous wavelets]]\n[[Category:Functional analysis]]"
    },
    {
      "title": "Orthogonal wavelet",
      "url": "https://en.wikipedia.org/wiki/Orthogonal_wavelet",
      "text": "An '''orthogonal wavelet''' is a [[wavelet]] whose associated [[Discrete wavelet transform|wavelet transform]] is [[Orthogonality|orthogonal]].\nThat is, the inverse wavelet transform is the [[Adjoint of an operator|adjoint]] of the wavelet transform.\nIf this condition is weakened one may end up with [[biorthogonal wavelet]]s.\n\n== Basics ==\nThe [[Wavelet#Scaling_function|scaling function]] is a [[refinable function]].\nThat is, it is a [[fractal]] functional equation, called the '''refinement equation''' ('''twin-scale relation''' or '''dilation equation'''):\n:<math>\\phi(x)=\\sum_{k=0}^{N-1} a_k\\phi(2x-k)</math>,\nwhere the sequence <math>(a_0,\\dots, a_{N-1})</math> of [[real number]]s is called a scaling sequence or scaling mask.\nThe wavelet proper is obtained by a similar linear combination,\n:<math>\\psi(x)=\\sum_{k=0}^{M-1} b_k\\phi(2x-k)</math>,\nwhere the sequence <math>(b_0,\\dots, b_{M-1})</math> of real numbers is called a wavelet sequence or wavelet mask.\n\nA necessary condition for the ''orthogonality'' of the wavelets is that the scaling sequence is orthogonal to any shifts of it by an even number of coefficients:\n:<math>\\sum_{n\\in\\Z} a_n a_{n+2m}=2\\delta_{m,0}</math>,\n\nwhere <math>\\delta_{m,n}</math> is the [[Kronecker delta]].\n\nIn this case there is the same number ''M=N'' of coefficients in the scaling as in the wavelet sequence, the wavelet sequence can be determined as <math>b_n=(-1)^n a_{N-1-n}</math>. In some cases the opposite sign is chosen.\n\n== Vanishing moments, polynomial approximation and smoothness ==\n\nA necessary condition for the existence of a solution to the refinement equation is that there exists a positive integer ''A'' such that (see [[Z-transform]]):\n\n:<math>(1+Z)^A | a(Z):=a_0+a_1Z+\\dots+a_{N-1}Z^{N-1}.</math> \n\nThe maximally possible power ''A'' is called '''polynomial approximation order''' (or pol. app. power) or '''number of vanishing moments'''. It describes the ability to represent polynomials up to degree ''A''-1 with linear combinations of integer translates of the scaling function. \n\nIn the biorthogonal case, an approximation order ''A'' of <math>\\phi</math> corresponds to ''A'' '''vanishing moments''' of the dual wavelet <math>\\tilde\\psi</math>, that is, the [[dot product|scalar products]] of <math>\\tilde\\psi</math> with any polynomial up to degree ''A-1'' are zero. In the opposite direction, the approximation order ''Ã'' of <math>\\tilde\\phi</math> is equivalent to ''Ã'' vanishing moments of <math>\\psi</math>. In the orthogonal case, ''A'' and ''Ã'' coincide.\n\nA sufficient condition for the existence of a scaling function is the following: if one decomposes <math>a(Z)=2^{1-A}(1+Z)^Ap(Z)</math>, and the estimate\n\n:<math>1\\le\\sup_{t\\in[0,2\\pi]} \\left |p(e^{it}) \\right |<2^{A-1-n},</math>\n\nholds for some <math>n\\in\\N</math>, then the refinement equation has a ''n'' times continuously differentiable solution with compact support.\n\n===Examples===\n\n* Suppose <math>p(Z) =1</math> then <math>a(Z)=2^{1-A}(1+Z)^A</math>, and the estimate holds for ''n''=''A''-2. The solutions are Schoenbergs [[B-spline]]s of order ''A''-1, where the (''A''-1)-th derivative is piecewise constant, thus the (''A''-2)-th derivative is [[Lipschitz continuity|Lipschitz-continuous]]. ''A''=1 corresponds to the index function of the unit interval.\n\n*''A''=2 and ''p'' linear may be written as \n::<math>a(Z)=\\frac14(1+Z)^2((1+Z)+c(1-Z)).</math>\n:Expansion of this degree 3 polynomial and insertion of the 4 coefficients into the orthogonality condition results in <math>c^2=3.</math> The positive root gives the scaling sequence of the D4-wavelet, see below.\n\n==References==\n\n* [[Ingrid Daubechies]]: ''Ten Lectures on Wavelets'', SIAM 1992,\n\n[[Category:Orthogonal wavelets|*]]"
    },
    {
      "title": "Binomial QMF",
      "url": "https://en.wikipedia.org/wiki/Binomial_QMF",
      "text": "A '''binomial QMF''' – properly an '''orthonormal binomial quadrature mirror filter''' – is an [[orthogonal wavelet]] developed in 1990.\n\nThe binomial QMF bank with perfect reconstruction '''(PR)''' was designed by [[Ali Akansu]], et al. published in 1990, using the family of binomial polynomials for subband decomposition of discrete-time signals.<ref>A.N. Akansu, [http://web.njit.edu/~akansu/NJITSYMP1990/AkansuNJIT1STWAVELETSSYMPAPRIL301990.pdf  An Efficient QMF-Wavelet Structure] (Binomial-QMF Daubechies Wavelets), Proc. 1st NJIT Symposium on Wavelets, April 1990.</ref><ref>A.N. Akansu, R.A. Haddad and H. Caglar, [http://web.njit.edu/~akansu/PAPERS/Akansu-BinomialQMF-Wavelet-SPIE-VCIP-Sept1990.pdf Perfect Reconstruction Binomial QMF-Wavelet Transform], Proc. SPIE Visual Communications and Image Processing, pp. 609–618, vol. 1360, Lausanne, Sept. 1990.</ref><ref>A.N. Akansu, R.A. Haddad and H. Caglar, [http://web.njit.edu/~akansu/PAPERS/IEEE20TSPBinomialQMFJAN1993.pdf The Binomial QMF-Wavelet Transform for Multiresolution Signal Decomposition]{{dead link|date=July 2017 |bot=InternetArchiveBot |fix-attempted=yes }}, IEEE Trans. Signal Process., pp. 13–19, January 1993.</ref>\n\nAkansu and his fellow authors also showed that these binomial-QMF filters are identical to the [[wavelet]] filters designed independently by [[Ingrid Daubechies]] from compactly supported orthonormal [[Discrete wavelet transform|wavelet transform]] perspective in 1988 ([[Daubechies wavelet]]). Later, it was shown that the magnitude square functions of low-pass and high-pass binomial-QMF filters are the unique maximally flat functions in a two-band PR-QMF design framework.<ref>H. Caglar and A.N. Akansu, [http://web.njit.edu/~akansu/PAPERS/CaglarAkansuBernstein.pdf A Generalized Parametric PR-QMF Design Technique Based on Bernstein Polynomial Approximation], IEEE Trans. Signal Process., pp. 2314–2321, July 1993.</ref>\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://mathworld.wolfram.com/DaubechiesWaveletFilter.html Daubechies Wavelet Filter]\n* [http://web.njit.edu/~akansu/s1.htm 1st NJIT Symposium on Wavelets (April 30, 1990)] (First Wavelets Conference in USA)\n\n[[Category:Orthogonal wavelets]]"
    },
    {
      "title": "Daubechies wavelet",
      "url": "https://en.wikipedia.org/wiki/Daubechies_wavelet",
      "text": "{{Refimprove|date=August 2009}}\n\n[[Image:Daubechies20LowPassHighPass2DFilterM.png|right|thumb|Daubechies 20 2-d wavelet (Wavelet Fn X Scaling Fn)|406x406px]]\nThe '''Daubechies wavelets''', based on the work of [[Ingrid Daubechies]], are a family of [[orthogonal wavelet]]s defining a [[discrete wavelet transform]] and characterized by a maximal number of vanishing [[Moment (mathematics)|moments]] for some given [[Support (mathematics)|support]]. With each wavelet type of this class, there is a scaling function (called the ''father wavelet'') which generates an orthogonal [[multiresolution analysis]].\n\n==Properties==\n\nIn general the Daubechies wavelets are chosen to have the highest number ''A'' of vanishing moments, (this does not imply the best smoothness) for given support width 2''A''&nbsp;−&nbsp;1.<ref>I. Daubechies, Ten Lectures on Wavelets, SIAM, 1992, p. 194.</ref> There are two naming schemes in use, D''N'' using the length or number of taps, and db''A'' referring to the number of vanishing moments. So D4 and db2 are the same wavelet transform.\n\nAmong the 2<sup>''A''−1</sup> possible solutions of the algebraic equations for the moment and orthogonality conditions, the one is chosen whose scaling filter has extremal phase. The wavelet transform is also easy to put into practice using the [[fast wavelet transform]]. Daubechies wavelets are widely used in solving a broad range of problems, e.g. self-similarity properties of a signal or [[fractal]] problems, signal discontinuities, etc.\n\nThe Daubechies wavelets are not defined in terms of the resulting scaling and wavelet functions; in fact, they are not possible to write down in [[closed form expression|closed form]]. The graphs below are generated using the [[cascade algorithm]], a numeric technique consisting of simply inverse-transforming [1 0 0 0 0 ... ] an appropriate number of times.\n\n{| class=\"wikitable\"\n|scaling and wavelet functions\n|[[Image:Daubechies4-functions.svg|360px]]\n|[[Image:Daubechies12-functions.png|360px]]\n|[[Image:Daubechies20-functions.png|360px]]\n|-\n|amplitudes of the frequency spectra of the above functions\n|[[Image:Daubechies4-spectrum.svg|360px]]\n|[[Image:Daubechies12-spectrum.png|360px]]\n|[[Image:Daubechies20-spectrum.png|360px]]\n|}\nNote that the spectra shown here are not the frequency response of the high and low pass filters, but rather the amplitudes of the continuous Fourier transforms of the scaling (blue) and wavelet (red) functions.\n\nDaubechies orthogonal wavelets D2–D20 resp. db1–db10 are commonly used. The index number refers to the number ''N'' of coefficients. Each wavelet has a number of ''zero moments'' or ''vanishing moments'' equal to half the number of coefficients. For example, D2 (the [[Haar wavelet]]) has one vanishing moment, D4 has two, etc. A vanishing moment limits the wavelets ability to represent [[polynomial]] behaviour or information in a signal. For example, D2, with one vanishing moment, easily encodes polynomials of one coefficient, or constant signal components. D4 encodes polynomials with two coefficients, i.e. constant and linear signal components; and D6 encodes 3-polynomials, i.e. constant, linear and [[quadratic polynomial|quadratic]] signal components. This ability to encode signals is nonetheless subject to the phenomenon of ''scale leakage'', and the lack of shift-invariance, which raise from the discrete shifting operation (below) during application of the transform. Sub-sequences which represent linear, [[quadratic polynomial|quadratic]] (for example) signal components are treated differently by the transform depending on whether the points align with even- or odd-numbered locations in the sequence. The lack of the important property of [[translational invariance|shift-invariance]], has led to the development of several different versions of a [[shift invariant wavelet transform|shift-invariant (discrete) wavelet transform]].\n\n==Construction==\nBoth the scaling sequence (low-pass filter) and the wavelet sequence (band-pass filter) (see [[orthogonal wavelet]] for details of this construction) will here be normalized to have sum equal 2 and sum of squares equal 2. In some applications, they are normalised to have sum <math>\\sqrt{2}</math>, so that both sequences and all shifts of them by an even number of coefficients are orthonormal to each other.\n\nUsing the general representation for a scaling sequence of an orthogonal discrete wavelet transform with approximation order ''A'', \n\n:<math>a(Z)=2^{1-A}(1+Z)^A p(Z),</math> \n\nwith ''N'' = 2''A'', ''p'' having real coefficients, ''p''(1) = 1 and deg(''p'')&nbsp;=&nbsp;''A''&nbsp;−&nbsp;1, one can write the orthogonality condition as\n\n:<math>a(Z)a \\left (Z^{-1} \\right )+a(-Z)a \\left (-Z^{-1} \\right )=4,</math> \n\nor equally as \n\n:<math>(2-X)^A P(X)+X^A P(2-X)=2^A \\qquad (*),</math>\n\nwith the Laurent-polynomial \n\n:<math>X:= \\frac{1}{2}\\left (2-Z-Z^{-1} \\right )</math> \n\ngenerating all symmetric sequences and <math>X(-Z)=2-X(Z).</math> Further, ''P''(''X'') stands for the symmetric Laurent-polynomial \n\n:<math>P(X(Z))=p(Z)p \\left ( Z^{-1} \\right ).</math>\n\nSince \n\n:<math>X(e^{iw})=1-\\cos(w)</math> \n:<math>p(e^{iw})p(e^{-iw})=|p(e^{iw})|^2</math>\n\n''P'' takes nonnegative values on the segment [0,2].\n\nEquation (*) has one minimal solution for each ''A'', which can be obtained by division in the ring of truncated power series in ''X'',\n\n:<math>P_A(X)=\\sum_{k=0}^{A-1} \\binom{A+k-1}{A-1} 2^{-k}X^k.</math>\n\nObviously, this has positive values on (0,2).\n\nThe homogeneous equation for (*) is antisymmetric about ''X'' = 1 and has thus the general solution \n\n:<math>X^A(X-1)R \\left ((X-1)^2 \\right ),</math>\n\nwith ''R'' some polynomial with real coefficients. That the sum\n\n:<math>P(X)=P_A(X)+X^A(X-1)R \\left ((X-1)^2 \\right )</math>\n\nshall be nonnegative on the interval [0,2] translates into a set of linear restrictions on the coefficients of ''R''. The values of ''P'' on the interval [0,2] are bounded by some quantity <math>4^{A-r},</math> maximizing ''r'' results in a linear program with infinitely many inequality conditions.\n\nTo solve \n\n:<math>P(X(Z))=p(Z)p \\left (Z^{-1} \\right)</math> \n\nfor ''p'' one uses a technique called spectral factorization resp. Fejér-Riesz-algorithm. The polynomial ''P''(''X'') splits into linear factors \n\n:<math>P(X)=(X-\\mu_1)\\cdots(X-\\mu_N), \\qquad N=A+1+2\\deg(R).</math> \n\nEach linear factor represents a Laurent-polynomial \n\n:<math>X(Z)-\\mu =-\\frac{1}{2}Z+1-\\mu-\\frac12Z^{-1}</math> \n\nthat can be factored into two linear factors. One can assign either one of the two linear factors to ''p''(''Z''), thus one obtains 2<sup>''N''</sup> possible solutions. For extremal phase one chooses the one that has all complex roots of ''p''(''Z'') inside or on the unit circle and is thus real.\n\nFor Daubechies wavelet transform, a pair of linear filters is being used. This pair of filters should have a property which is called as quadrature mirror filter. Solving the coefficient of the linear filter <math>c_i</math> using the quadrature mirror filter property results in the below solution for the coefficient values for filter of order 4.\n\n:<math>c_0 = \\frac{1+\\sqrt{3}}{4\\sqrt{2}}, \\quad c_1 = \\frac{3+\\sqrt{3}}{4\\sqrt{2}}, \\quad c_2 = \\frac{3-\\sqrt{3}}{4\\sqrt{2}}, \\quad c_3 = \\frac{1-\\sqrt{3}}{4\\sqrt{2}}.</math>\n\n== The scaling sequences of lowest approximation order ==\n\nBelow are the coefficients for the scaling functions for D2-20. The wavelet coefficients are derived by reversing the order of the [[Wavelet#Scaling function|scaling function]] coefficients and then reversing the sign of every second one, (i.e., D4 wavelet = {−0.1830127, −0.3169873, 1.1830127, −0.6830127}). Mathematically, this looks like <math>b_k = (-1)^k a_{N-1-k} </math> where ''k'' is the coefficient index, ''b'' is a coefficient of the wavelet sequence and ''a'' a coefficient of the scaling sequence. ''N'' is the wavelet index, i.e., 2 for D2.\n\n<div style=\"font-size:83%\">\n{| class=\"wikitable\"\n|+'''Orthogonal Daubechies coefficients (normalized to have sum 2)'''\n!D2 ([[Haar wavelet|Haar]])\n!D4\n!D6\n!D8\n!D10\n!D12\n!D14\n!D16\n!D18\n!D20\n|----\n|1\n|0.6830127\n|0.47046721\n|0.32580343\n|0.22641898\n|0.15774243\n|0.11009943\n|0.07695562\n|0.05385035\n|0.03771716\n|----\n|1\n|1.1830127\n|1.14111692\n|1.01094572\n|0.85394354\n|0.69950381\n|0.56079128\n|0.44246725\n|0.34483430\n|0.26612218\n|----\n|\n|0.3169873\n|0.650365\n|0.89220014\n|1.02432694\n|1.06226376\n|1.03114849\n|0.95548615\n|0.85534906\n|0.74557507\n|----\n|\n| −0.1830127\n| −0.19093442\n| −0.03957503\n|0.19576696\n|0.44583132\n|0.66437248\n|0.82781653\n|0.92954571\n|0.97362811\n|----\n|\n|\n| −0.12083221\n| −0.26450717\n| −0.34265671\n| −0.31998660\n| −0.20351382\n| −0.02238574\n|0.18836955\n|0.39763774\n|----\n|\n|\n|0.0498175\n|0.0436163\n| −0.04560113\n| −0.18351806\n| −0.31683501\n| −0.40165863\n| −0.41475176\n| −0.35333620\n|----\n|\n|\n|\n|0.0465036\n|0.10970265\n|0.13788809\n|0.1008467\n|6.68194092&nbsp;×&nbsp;10<sup>−4</sup>\n| −0.13695355\n| −0.27710988\n|----\n|\n|\n|\n| −0.01498699\n| −0.00882680\n|0.03892321\n|0.11400345\n|0.18207636\n|0.21006834\n|0.18012745\n|----\n|\n|\n|\n|\n| −0.01779187\n| −0.04466375\n| −0.05378245\n| −0.02456390\n|0.043452675\n|0.13160299\n|----\n|\n|\n|\n|\n|4.71742793&nbsp;×&nbsp;10<sup>−3</sup>\n|7.83251152&nbsp;×&nbsp;10<sup>−4</sup>\n| −0.02343994\n| −0.06235021\n| −0.09564726\n| −0.10096657\n|----\n|\n|\n|\n|\n|\n|6.75606236&nbsp;×&nbsp;10<sup>−3</sup>\n|0.01774979\n|0.01977216\n|3.54892813&nbsp;×&nbsp;10<sup>−4</sup> \n| −0.04165925\n|----\n|\n|\n|\n|\n|\n| −1.52353381&nbsp;×&nbsp;10<sup>−3</sup>\n|6.07514995&nbsp;×&nbsp;10<sup>−4</sup>\n|0.01236884\n|0.03162417\n|0.04696981\n|----\n|\n|\n|\n|\n|\n|\n| −2.54790472&nbsp;×&nbsp;10<sup>−3</sup>\n| −6.88771926&nbsp;×&nbsp;10<sup>−3</sup>\n| −6.67962023&nbsp;×&nbsp;10<sup>−3</sup>\n|5.10043697&nbsp;×&nbsp;10<sup>−3</sup>\n|----\n|\n|\n|\n|\n|\n|\n| 5.00226853&nbsp;×&nbsp;10<sup>−4</sup>\n| −5.54004549&nbsp;×&nbsp;10<sup>−4</sup>\n| −6.05496058&nbsp;×&nbsp;10<sup>−3</sup>\n| −0.01517900\n|----\n|\n|\n|\n|\n|\n|\n|\n|9.55229711&nbsp;×&nbsp;10<sup>−4</sup>\n|2.61296728&nbsp;×&nbsp;10<sup>−3</sup>\n|1.97332536&nbsp;×&nbsp;10<sup>−3</sup>\n|----\n|\n|\n|\n|\n|\n|\n|\n| −1.66137261&nbsp;×&nbsp;10<sup>−4</sup>\n|3.25814671&nbsp;×&nbsp;10<sup>−4</sup>\n|2.81768659&nbsp;×&nbsp;10<sup>−3</sup>\n|----\n|\n|\n|\n|\n|\n|\n|\n|\n| −3.56329759&nbsp;×&nbsp;10<sup>−4</sup>\n| −9.69947840&nbsp;×&nbsp;10<sup>−4</sup>\n|----\n|\n|\n|\n|\n|\n|\n|\n|\n| 5.5645514&nbsp;×&nbsp;10<sup>−5</sup>\n| −1.64709006&nbsp;×&nbsp;10<sup>−4</sup>\n|----\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|1.32354367&nbsp;×&nbsp;10<sup>−4</sup>\n|----\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| −1.875841&nbsp;×&nbsp;10<sup>−5</sup>\n|}\n</div>\n\nParts of the construction are also used to derive the biorthogonal [[Cohen–Daubechies–Feauveau wavelet]]s (CDFs).\n\n==Implementation==\nWhile software such as [[Mathematica]] supports Daubechies wavelets directly<ref>\n[http://reference.wolfram.com/mathematica/ref/DaubechiesWavelet.html Daubechies Wavelet in Mathematica. Note that in there ''n'' is ''n''/2 from the text.]</ref> a basic implementation is simple in [[MATLAB]] (in this case, Daubechies 4). This implementation uses periodization to handle the problem of finite length signals. Other, more sophisticated methods are available, but often it is not necessary to use these as it only affects the very ends of the transformed signal. The periodization is accomplished in the forward transform directly in MATLAB vector notation, and the inverse transform by using the <tt>circshift()</tt> function:\n\n===Transform, D4===\nIt is assumed that ''S'', a column vector with an even number of elements, has been pre-defined as the signal to be analyzed. Note that the D4 coefficients are [1&nbsp;+&nbsp;{{radic|3}}, 3&nbsp;+&nbsp;{{radic|3}}, 3&nbsp;−&nbsp;{{radic|3}}, 1&nbsp;−&nbsp;{{radic|3}}]/4.\n\n<source lang=\"matlab\">\nN = length(S);\ns1 = S(1:2:N-1) + sqrt(3)*S(2:2:N);\nd1 = S(2:2:N) - sqrt(3)/4*s1 - (sqrt(3)-2)/4*[s1(N/2); s1(1:N/2-1)];\ns2 = s1 - [d1(2:N/2); d1(1)];\ns = (sqrt(3)-1)/sqrt(2) * s2;\nd = -(sqrt(3)+1)/sqrt(2) * d1;\n</source>\n\n===Inverse transform, D4===\n<source lang=\"matlab\">\nd1 = d * ((sqrt(3)-1)/sqrt(2));\ns2 = s * ((sqrt(3)+1)/sqrt(2));\ns1 = s2 + circshift(d1,-1);\nS(2:2:N) = d1 + sqrt(3)/4*s1 + (sqrt(3)-2)/4*circshift(s1,1);\nS(1:2:N-1) = s1 - sqrt(3)*S(2:2:N);\n</source>\n\n==See also==\n\n* [[Binomial-QMF]] (Daubechies Wavelet Filters)\n* [[Fast wavelet transform]]\n\n==References==\n{{reflist}}\n*{{cite book|last = Jensen | first = | authorlink = |author2=la Cour-Harbo | title = Ripples in Mathematics| publisher = Springer| date = 2001 | location = Berlin | pages = 157–160 | url = http://www.control.auc.dk/~alc/ripples.html | doi = | isbn = 3-540-41662-5 }}\n\n* [https://sites.google.com/site/jackieneoshen/ Jianhong (Jackie) Shen] and [[Gilbert Strang]], ''Applied and Computational Harmonic Analysis'', '''5'''(3), [https://doi.org/10.1006/acha.1997.0234 ''Asymptotics of Daubechies Filters, Scaling Functions, and Wavelets''].\n\n==External links==\n{{commons category|Daubechies wavelets}}\n* Ingrid Daubechies: ''Ten Lectures on Wavelets'', SIAM 1992\n* A.N. Akansu, [http://web.njit.edu/~ali/NJITSYMP1990/AkansuNJIT1STWAVELETSSYMPAPRIL301990.pdf An Efficient QMF-Wavelet Structure] (Binomial-QMF Daubechies Wavelets), Proc. 1st NJIT Symposium on Wavelets, April 1990\n* [http://web.njit.edu/~akansu/s1.htm Proc. 1st NJIT Symposium on Wavelets, Subbands and Transforms, April 1990]\n* A.N. Akansu, R.A. Haddad and H. Caglar, [https://web.njit.edu/~akansu/PAPERS/Akansu-BinomialQMF-Wavelet-SPIE-VCIP-Sept1990.pdf Perfect Reconstruction Binomial QMF-Wavelet Transform], Proc. SPIE Visual Communications and Image Processing, pp.&nbsp;609–618, Lausanne, Sept. 1990\n* [http://mate.dm.uba.ar/~hafg/ Carlos Cabrelli, Ursula Molter]: ''Generalized Self-similarity\", Journal of Mathematical Analysis and Applications, 230: 251–260, 1999.\n*[http://etd.lib.fsu.edu/theses/available/etd-11242003-185039/ Hardware implementation of wavelets]\n* {{springer|title=Daubechies wavelets|id=p/d130030}}\n* I. Kaplan, [http://www.bearcave.com/misl/misl_tech/wavelets/daubechies/index.html The Daubechies D4 Wavelet Transform].\n\n{{DEFAULTSORT:Daubechies Wavelet}}\n[[Category:Orthogonal wavelets]]\n[[Category:Articles with example MATLAB/Octave code]]"
    },
    {
      "title": "Haar wavelet",
      "url": "https://en.wikipedia.org/wiki/Haar_wavelet",
      "text": "{{Use dmy dates|date=July 2013}}\n[[Image:Haar wavelet.svg|thumb|right|The Haar wavelet]]\n[[Image:Lenna_Haar_Decomposition_2_iterations.png|thumb|300px|Two iterations of the 2D Haar wavelet decomposition on the [[Lenna]] image. The original image is high-pass filtered, yielding the three detail coefficients subimages (top right: horizontal, bottom left: vertical, and bottom right: diagonal). It is then low-pass filtered and downscaled, yielding an approximation coefficients subimage (top left); the filtering process is repeated once again on this approximation image.{{clarify|reason=This caption is seriously incomprehensible, or am I the only one?|date=February 2017}}]]\n\nIn mathematics, the '''Haar wavelet''' is a sequence of rescaled \"square-shaped\" functions which together form a [[wavelet]] family or basis. Wavelet analysis is similar to [[Fourier analysis]] in that it allows a target function over an interval to be represented in terms of an [[orthonormal basis]]. The Haar sequence is now recognised as the first known wavelet basis and extensively used as a teaching example.\n\nThe '''Haar sequence''' was proposed in 1909 by [[Alfréd Haar]].<ref>see p.&nbsp;361 in {{harvtxt|Haar|1910}}.</ref> \nHaar used these functions to give an example of an orthonormal system for the space of [[square-integrable function]]s on the [[unit interval]]&nbsp;[0,&nbsp;1].  The study of wavelets, and even the term \"wavelet\", did not come until much later. As a special case of the [[Daubechies wavelet]], the Haar wavelet is also known as '''Db1'''.\n\nThe Haar wavelet is also the simplest possible wavelet. The technical disadvantage of the Haar wavelet is that it is not [[continuous function|continuous]], and therefore not [[derivative|differentiable]]. This property can, however, be an advantage for the analysis of signals with sudden transitions, such as monitoring of tool failure in machines.<ref>{{cite journal |first=B. |last=Lee |first2=Y. S. |last2=Tarng |title=Application of the discrete wavelet transform to the monitoring of tool failure in end milling using the spindle motor current |journal=International Journal of Advanced Manufacturing Technology |year=1999 |volume=15 |issue=4 |pages=238–243 |doi=10.1007/s001700050062 }}</ref>\n\nThe Haar wavelet's mother wavelet function <math>\\psi(t)</math> can be described as\n: <math>\\psi(t) = \\begin{cases}\n  1 \\quad & 0 \\leq  t < \\frac{1}{2},\\\\\n -1 & \\frac{1}{2} \\leq t < 1,\\\\\n  0 &\\mbox{otherwise.}\n\\end{cases}</math>\n\nIts [[Father wavelets|scaling function]] <math>\\varphi(t)</math> can be described as\n: <math>\\varphi(t) = \\begin{cases}1 \\quad & 0 \\leq  t < 1,\\\\0 &\\mbox{otherwise.}\\end{cases}</math>\n\n== Haar functions and Haar system ==\nFor every pair ''n'', ''k'' of integers in '''Z''', the '''Haar function''' ''&psi;''<sub>''n'',&thinsp;''k''</sub> is defined on the [[real line]] '''R''' by the formula\n:<math> \\psi_{n,k}(t) = 2^{n / 2} \\psi(2^n t-k), \\quad t \\in \\mathbf{R}.</math>\nThis function is supported on the [[Semi-open interval|right-open interval]] {{nowrap| ''I''<sub>''n'', &thinsp;''k''</sub> {{=}}}} {{nowrap|[ ''k''&thinsp;2<sup>&minus;''n''</sup>, (''k''+1)&thinsp;2<sup>&minus;''n''&thinsp;</sup>)}}, ''i.e.'', it [[Zero of a function|vanishes]] outside that interval. It has integral 0 and norm&nbsp;1 in the [[Hilbert space]]&nbsp;[[Lp space|''L''<sup>2</sup>('''R''')]],\n:<math> \\int_{\\mathbf{R}} \\psi_{n, k}(t) \\, d t = 0, \\quad \\|\\psi_{n, k}\\|^2_{L^2(\\mathbf{R})} = \\int_{\\mathbf{R}} \\psi_{n, k}(t)^2 \\, d t = 1.</math>\nThe Haar functions are pairwise [[Orthogonality#Orthogonal functions|orthogonal]],\n:<math> \\int_{\\mathbf{R}} \\psi_{n_1, k_1}(t) \\psi_{n_2, k_2}(t) \\, d t = \\delta_{n_1, n_2} \\delta_{k_1, k_2}, </math>\nwhere ''δ''<sub>''i'',''j''</sub> represents the [[Kronecker delta]]. Here is the reason for orthogonality: when the two supporting intervals <math>I_{n_1, k_1}</math> and <math>I_{n_2, k_2}</math> are not equal, then they are either disjoint, or else, the smaller of the two supports, say <math>I_{n_1, k_1}</math>, is contained in the lower or in the upper half of the other interval, on which the function <math>\\psi_{n_2, k_2}</math> remains constant. It follows in this case that the product of these two Haar functions is a multiple of the first Haar function, hence the product has integral&nbsp;0.\n\nThe '''Haar system''' on the real line is the set of functions\n:<math>\\{ \\psi_{n,k}(t) \\; ; \\; n \\in \\mathbf{Z}, \\; k \\in \\mathbf{Z} \\}.</math>\nIt is [[Orthonormal basis|complete]] in ''L''<sup>2</sup>('''R'''): ''The Haar system on the line is an orthonormal basis in'' ''L''<sup>2</sup>('''R''').\n\n==Haar wavelet properties==\nThe Haar wavelet has several notable properties:\n\n#Any continuous real function with compact support can be approximated uniformly by [[linear combination]]s of <math>\\varphi(t),\\varphi(2t),\\varphi(4t),\\dots,\\varphi(2^n t),\\dots</math> and their shifted functions. This extends to those function spaces where any function therein can be approximated by continuous functions.\n#Any continuous real function on [0,&nbsp;1] can be approximated uniformly on [0,&nbsp;1] by linear combinations of the constant function&nbsp;'''1''', <math>\\psi(t),\\psi(2t),\\psi(4t),\\dots,\\psi(2^n t),\\dots</math> and their shifted functions.<ref>As opposed to the preceding statement, this fact is not obvious: see p.&nbsp;363 in {{harvtxt|Haar|1910}}.</ref><!--\n     (spacing trick in lists, see Help:List)   \n                                                -->\n#[[Orthogonality]] in the form\n\n&nbsp;&nbsp;&nbsp;&nbsp;<math>\n \\int_{-\\infty}^{\\infty}2^{(n+n_1)/2}\\psi(2^n t-k)\\psi(2^{n_1} t - k_1)\\, dt = \\delta_{n,n_1}\\delta_{k,k_1}.\n </math>\n\n<!--\n\n -->Here ''δ''<sub>''i'',''j''</sub> represents the [[Kronecker delta]]. The [[dual function]] of &psi;(''t'') is &psi;(''t'') itself.<!--\n     (spacing trick in lists, see Help:List)   \n                                                   -->\n# Wavelet/scaling functions with different scale ''n'' have a functional relationship:<ref>{{cite book |last=Vidakovic |first=Brani |title=Statistical Modeling by Wavelets |year=2010 |edition=2 |doi=10.1002/9780470317020 |pages=60, 63}}</ref> since\n::<math>\n\\begin{align}\n  \\varphi(t) &= \\varphi(2t)+\\varphi(2t-1)\\\\[.2em]\n  \\psi(t) &= \\varphi(2t)-\\varphi(2t-1),\n\\end{align}</math>\n:it follows that coefficients of scale ''n'' can be calculated by coefficients of scale ''n+1'':\n:If <math> \\chi_w(k, n)= 2^{n/2}\\int_{-\\infty}^\\infty x(t)\\varphi(2^nt-k)\\, dt</math>\n:and <math> \\Chi_w(k, n)= 2^{n/2}\\int_{-\\infty}^\\infty x(t)\\psi(2^nt-k)\\, dt</math>\n:then\n::<math> \\chi_w(k,n)= 2^{-1/2} \\bigl( \\chi_w(2k,n+1)+\\chi_w(2k+1,n+1) \\bigr)</math>\n::<math> \\Chi_w(k,n)= 2^{-1/2} \\bigl( \\chi_w(2k,n+1)-\\chi_w(2k+1,n+1) \\bigr).</math>\n<!--The structure of [[multiresolution analysis]] (MRA):\nImage with unknown copyright status removed: [[Image:Haar_Wavelet_20080121_1.png|thumb|center|]] -->\n\n== Haar system on the unit interval and related systems ==\nIn this section, the discussion is restricted to the [[unit interval]] [0,&nbsp;1] and to the Haar functions that are supported on [0,&nbsp;1]. The system of functions considered by Haar in 1910,<ref>p.&nbsp;361 in {{harvtxt|Haar|1910}}</ref>\ncalled the '''Haar system on [0,&nbsp;1]''' in this article, consists of the subset of Haar wavelets defined as\n:<math>\\{ t \\in [0, 1] \\mapsto \\psi_{n,k}(t) \\; ; \\; n \\in \\N \\cup \\{0\\}, \\; 0 \\leq k < 2^n\\},</math>\nwith the addition of the constant function '''1''' on [0,&nbsp;1].\n\nIn [[Hilbert space]] terms, this Haar system on [0,&nbsp;1] is a [[Orthonormal basis|complete orthonormal system]], ''i.e.'', an [[orthonormal basis]], for the space ''L''<sup>2</sup>([0,&nbsp;1]) of square integrable functions on the unit interval.\n\nThe Haar system on [0,&nbsp;1] &mdash;with the constant function '''1''' as first element, followed with the Haar functions ordered according to the [[Lexicographical order|lexicographic]] ordering of couples {{nowrap|(''n'', ''k'')}}&mdash; is further a [[Schauder basis#Properties|monotone]] [[Schauder basis]] for the space [[Lp space|''L''<sup>''p''</sup>([0,&nbsp;1])]] when {{nowrap|1 &le; ''p'' &lt; ∞}}.<ref name=\"L. Tzafriri, 1977\">see p.&nbsp;3 in [[Joram Lindenstrauss|J. Lindenstrauss]], L. Tzafriri, (1977), \"Classical Banach Spaces I, Sequence Spaces\", Ergebnisse der Mathematik und ihrer Grenzgebiete '''92''', Berlin: Springer-Verlag, {{ISBN|3-540-08072-4}}.</ref> \nThis basis is [[Schauder basis#Unconditionality|unconditional]]  when {{nowrap|1 &lt; ''p'' &lt; ∞}}.<ref>The result is due to [[Raymond Paley|R. E. Paley]], ''A remarkable series of orthogonal functions (I)'', Proc. London Math. Soc. '''34''' (1931) pp. 241-264. See also p.&nbsp;155 in J. Lindenstrauss, L. Tzafriri, (1979), \"Classical Banach spaces II, Function spaces\". Ergebnisse der Mathematik und ihrer Grenzgebiete '''97''', Berlin: Springer-Verlag, {{ISBN|3-540-08888-1}}.</ref>\n\nThere is a related [[Rademacher system]] consisting of sums of Haar functions, \n:<math>r_n(t) = 2^{-n/2} \\sum_{k=0}^{2^n - 1} \\psi_{n, k}(t), \\quad t \\in [0, 1], \\ n \\ge 0.</math>\nNotice that |''r''<sub>''n''</sub>(''t'')|&nbsp;= 1 on [0,&nbsp;1). This is an orthonormal system but it is not complete.<ref>{{cite web |url=http://eom.springer.de/O/o070380.htm |title=Orthogonal system |work=Encyclopaedia of Mathematics }}</ref><ref>{{cite book |first=Gilbert G. |last=Walter |first2=Xiaoping |last2=Shen |title=Wavelets and Other Orthogonal Systems |year=2001 |location=Boca Raton |publisher=Chapman |isbn=1-58488-227-1 }}</ref>\nIn the language of [[probability theory]], the Rademacher sequence is an instance of a sequence of [[Independence (probability theory)|independent]] [[Bernoulli distribution|Bernoulli]] [[random variables]] with [[mean]]&nbsp;0. The [[Khintchine inequality]] expresses the fact that in all the spaces ''L''<sup>''p''</sup>([0,&nbsp;1]), {{nowrap|1 &le; ''p'' &lt; ∞}}, the Rademacher sequence is [[Schauder basis#Definitions|equivalent]] to the unit vector basis in ℓ<sup>''2''</sup>.<ref>see for example p.&nbsp;66 in [[Joram Lindenstrauss|J. Lindenstrauss]], L. Tzafriri, (1977), \"Classical Banach Spaces I, Sequence Spaces\", Ergebnisse der Mathematik und ihrer Grenzgebiete '''92''', Berlin: Springer-Verlag, {{ISBN|3-540-08072-4}}.</ref> In particular, the [[Linear span#Closed linear span|closed linear span]] of the Rademacher sequence in ''L''<sup>''p''</sup>([0,&nbsp;1]), {{nowrap|1 &le; ''p'' &lt; ∞}}, is [[Banach space#Linear operators, isomorphisms|isomorphic]] to ℓ<sup>''2''</sup>.\n\n=== The Faber&ndash;Schauder system ===\nThe '''Faber&ndash;Schauder system'''<ref name=\"Faber\">Faber, Georg (1910), \"Über die Orthogonalfunktionen des Herrn Haar\", ''Deutsche Math.-Ver'' (in German) '''19''': 104&ndash;112. {{issn|0012-0456}}; \nhttp://www-gdz.sub.uni-goettingen.de/cgi-bin/digbib.cgi?PPN37721857X ; http://resolver.sub.uni-goettingen.de/purl?GDZPPN002122553</ref><ref>Schauder, Juliusz (1928), \"Eine Eigenschaft des Haarschen Orthogonalsystems\", ''Mathematische Zeitschrift'' '''28''': 317&ndash;320.</ref><ref>{{eom|id=f/f038020\n |title=Faber–Schauder system|first=B.I.|last= Golubov}}</ref> \nis the family of continuous functions on [0,&nbsp;1] consisting of the constant function&nbsp;'''1''', and of multiples of [[Antiderivative|indefinite integrals]] of the functions in the Haar system on&nbsp;[0,&nbsp;1], chosen to have norm&nbsp;1 in the [[Uniform norm|maximum norm]]. This system begins with ''s''<sub>0</sub>&nbsp;=&nbsp;'''1''', then {{nowrap| ''s''<sub>1</sub>(''t'') {{=}} ''t''}} is the indefinite integral vanishing at&nbsp;0 of the function&nbsp;'''1''', first element of the Haar system on [0,&nbsp;1].  Next, for every integer {{nowrap|''n'' &ge; 0}}, functions {{nowrap| ''s''<sub>''n'',&thinsp;''k''</sub>}} are defined by the formula\n:<math>\n s_{n, k}(t) = 2^{1 + n/2} \\int_0^t \\psi_{n, k}(u) \\, d u, \\quad t \\in [0, 1], \\ 0 \\le k < 2^n.</math>\nThese functions {{nowrap| ''s''<sub>''n'',&thinsp;''k''</sub>}} are continuous, [[Piecewise linear function|piecewise linear]], supported by the interval {{nowrap| ''I''<sub>''n'',&thinsp;''k''</sub>}} that also supports {{nowrap| &psi;<sub>''n'',&thinsp;''k''</sub>}}. The function {{nowrap| ''s''<sub>''n'',&thinsp;''k''</sub>}} is equal to&nbsp;1 at the midpoint {{nowrap| ''x''<sub>''n'',&thinsp;''k''</sub>}} of the interval&nbsp;{{nowrap| ''I''<sub>''n'',&thinsp;''k''</sub>}}, linear on both halves of that interval. It takes values between&nbsp;0 and&nbsp;1 everywhere.\n\nThe Faber&ndash;Schauder system is a [[Schauder basis]] for the space ''C''([0,&nbsp;1]) of continuous functions on [0,&nbsp;1].<ref name=\"L. Tzafriri, 1977\"/> \nFor every&nbsp;''f'' in ''C''([0,&nbsp;1]), the partial sum\n:<math> f_{n+1} = a_0 s_0 + a_1 s_1 + \\sum_{m = 0}^{n-1} \\Bigl( \\sum_{k=0}^{2^m - 1} a_{m,k} s_{m, k} \\Bigr) \\in C([0, 1])</math>\nof the [[series expansion]] of ''f'' in the Faber&ndash;Schauder system is the continuous piecewise linear function that agrees with&nbsp;''f'' at the {{nowrap|2<sup>''n''</sup>&thinsp;+&thinsp;1}} points {{nowrap|''k''&thinsp;2<sup>&minus;''n''</sup>}}, where {{nowrap| 0 &le; ''k'' &le; 2<sup>''n''</sup>}}. Next, the formula\n:<math> f_{n+2} - f_{n+1} = \\sum_{k=0}^{2^n - 1} \\bigl( f(x_{n,k}) - f_{n+1}(x_{n, k}) \\bigr) s_{n, k} = \\sum_{k=0}^{2^n - 1} a_{n, k} s_{n, k} </math>\ngives a way to compute the expansion of ''f'' step by step. Since ''f'' is [[Heine–Borel theorem|uniformly continuous]], the sequence {''f''<sub>''n''</sub>} converges uniformly to ''f''.  It follows that the Faber&ndash;Schauder series expansion of ''f'' converges in ''C''([0,&nbsp;1]), and the sum of this series is equal to&nbsp;''f''.\n\n=== The Franklin system ===\nThe '''Franklin system''' is obtained from the Faber&ndash;Schauder system by the [[Gram–Schmidt process|Gram&ndash;Schmidt orthonormalization procedure]].<ref>see Z. Ciesielski, ''Properties of the orthonormal Franklin system''. Studia Math. 23 1963 141–157.</ref><ref>Franklin system. B.I. Golubov (originator), Encyclopedia of Mathematics. URL: http://www.encyclopediaofmath.org/index.php?title=Franklin_system&oldid=16655</ref>\nSince the Franklin system has the same linear span as that of the Faber&ndash;Schauder system, this span is dense in ''C''([0,&nbsp;1]), hence in ''L''<sup>2</sup>([0,&nbsp;1]). The Franklin system is therefore an orthonormal basis for ''L''<sup>2</sup>([0,&nbsp;1]), consisting of continuous piecewise linear functions. P. Franklin proved in 1928 that this system is a Schauder basis for ''C''([0,&nbsp;1]).<ref>Philip Franklin, ''A set of continuous orthogonal functions'', Math. Ann. 100 (1928), 522-529.</ref> \nThe Franklin system is also an unconditional basis for the space ''L''<sup>''p''</sup>([0,&nbsp;1]) when {{nowrap|1 &lt; ''p'' &lt; ∞}}.<ref name=Bo>S. V. Bočkarev, ''Existence of a basis in the space of functions analytic in the disc, and some properties of Franklin's system''. Mat. Sb. '''95''' (1974), 3–18 (Russian). Translated in Math. USSR-Sb. '''24''' (1974), 1–16.</ref>\nThe Franklin system provides a Schauder basis in the [[disk algebra]] ''A''(''D'').<ref name=Bo />\nThis was proved in 1974 by Bočkarev, after the existence of a basis for the disk algebra had remained open for more than forty years.<ref>The question appears p.&nbsp;238, &sect;3 in Banach's book, {{citation|first=Stefan|last=Banach|authorlink=Stefan Banach|url=http://matwbn.icm.edu.pl/kstresc.php?tom=1&wyd=10|title=Théorie des opérations linéaires|publication-place=Warszawa|publisher=Subwencji Funduszu Kultury Narodowej|year=1932|series=Monografie Matematyczne|volume=1|zbl=0005.20901}}.  The disk algebra ''A''(''D'') appears as Example&nbsp;10, p.&nbsp;12 in Banach's book.</ref>\n\nBočkarev's construction of a Schauder basis in ''A''(''D'') goes as follows: let&nbsp;''f'' be a complex valued [[Lipschitz continuity|Lipschitz function]] on [0,&nbsp;&pi;]; then&nbsp;''f'' is the sum of a [[Fourier series|cosine series]] with [[Absolute convergence|absolutely summable]] coefficients. Let&nbsp;''T''(''f'') be the element of ''A''(''D'') defined by the complex [[power series]] with the same coefficients,\n\n:<math> \\left\\{ f : x \\in [0, \\pi] \\rightarrow \\sum_{n=0}^\\infty a_n \\cos(n x) \\right\\} \\longrightarrow \\left\\{ T(f) : z \\rightarrow \\sum_{n=0}^\\infty a_n z^n, \\quad |z| \\le 1 \\right\\}.</math>\nBočkarev's basis for ''A''(''D'') is formed by the images under&nbsp;''T'' of the functions in the Franklin system on&nbsp;[0,&nbsp;&pi;]. Bočkarev's equivalent description for the mapping&nbsp;''T'' starts by extending ''f'' to an [[Even and odd functions|even]] Lipschitz function&nbsp;''g''<sub>1</sub> on [&minus;&pi;,&nbsp;&pi;], identified with a Lipschitz function on the [[unit circle]]&nbsp;'''T'''. Next, let ''g''<sub>2</sub> be the [[Hardy space#Real-variable techniques on the unit circle|conjugate function]] of&nbsp;''g''<sub>1</sub>, and define ''T''(''f'') to be the function in&nbsp;''A''(''D'') whose value on the boundary '''T''' of&nbsp;''D'' is equal to&nbsp;{{nowrap|''g''<sub>1</sub> + i&thinsp;''g''<sub>2</sub>}}.\n\nWhen dealing with 1-periodic continuous functions, or rather with continuous functions ''f'' on [0,&nbsp;1] such that {{nowrap|''f''(0) {{=}} ''f''(1)}}, one removes the function {{nowrap| ''s''<sub>1</sub>(''t'') {{=}} ''t''}} from the Faber&ndash;Schauder system, in order to obtain the '''periodic Faber&ndash;Schauder system'''. The '''periodic Franklin system''' is obtained by orthonormalization from the periodic Faber&ndash;-Schauder system.<ref name=\"Prz\">See p.&nbsp;161, III.D.20 and p.&nbsp;192, III.E.17 in {{citation\n | last=Wojtaszczyk | first= Przemysław\n | title = Banach spaces for analysts\n | series = Cambridge Studies in Advanced Mathematics\n | volume = 25\n | publisher = Cambridge University Press \n | location = Cambridge\n | year= 1991\n | pages = xiv+382\n | ISBN = 0-521-35618-0 \n}}</ref>\nOne can prove Bočkarev's result on ''A''(''D'') by proving that the periodic Franklin system on [0,&nbsp;2&pi;] is a basis for a Banach space ''A''<sub>''r''</sub> isomorphic to ''A''(''D'').<ref name=\"Prz\" /> \nThe space ''A''<sub>''r''</sub> consists of complex continuous functions on the unit circle '''T''' whose [[Harmonic conjugate|conjugate function]] is also continuous.\n\n==Haar matrix==\nThe 2×2 Haar matrix that is associated with the Haar wavelet is\n: <math> H_2 = \\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix}.</math>\nUsing the [[discrete wavelet transform]], one can transform any sequence <math>(a_0,a_1,\\dots,a_{2n},a_{2n+1})</math> of even length into a sequence of two-component-vectors <math> \\left(\\left(a_0,a_1\\right),\\dots,\\left(a_{2n},a_{2n+1}\\right)\\right) </math>. If one right-multiplies each vector with the matrix <math> H_2 </math>, one gets the result <math>\\left(\\left(s_0,d_0\\right),\\dots,\\left(s_n,d_n\\right)\\right)</math> of one stage of the fast Haar-wavelet transform. Usually one separates the sequences ''s'' and ''d'' and continues with transforming the sequence ''s''. Sequence ''s'' is often referred to as the ''averages'' part, whereas ''d'' is known as the ''details'' part.<ref>{{cite book |first=David K. |last=Ruch |first2=Patrick J. |last2=Van Fleet |title=Wavelet Theory: An Elementary Approach with Applications |year=2009 |location= |publisher=John Wiley & Sons|isbn=978-0-470-38840-2 }}</ref>\n\nIf one has a sequence of length a multiple of four, one can build blocks of 4 elements and transform them in a similar manner with the 4×4 Haar matrix\n: <math> H_4 = \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 1 & -1 & -1 \\\\ 1 & -1 & 0 & 0\\\\ 0 & 0 & 1 & -1 \\end{bmatrix},</math>\nwhich combines two stages of the fast Haar-wavelet transform.\n\nCompare with a [[Walsh matrix]], which is a non-localized 1/–1 matrix.\n\nGenerally, the 2N×2N Haar matrix can be derived by the following equation.\n\n: <math> H_{2N} = \\begin{bmatrix} H_{N} \\otimes [1, 1] \\\\ I_{N} \\otimes [1, -1] \\end{bmatrix}</math>\n:where <math>I_{N} = \\begin{bmatrix} 1 & 0 & \\dots & 0 \\\\ 0 & 1 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & 1 \\end{bmatrix}</math> and <math>\\otimes</math> is the [[Kronecker product]].\n\nThe [[Kronecker product]] of <math>A \\otimes B</math>, where <math>A</math> is an m×n matrix and <math>B</math> is a p×q matrix, is expressed as\n\n: <math>A \\otimes B = \\begin{bmatrix} a_{11}B & \\dots & a_{1n}B \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{m1}B & \\dots & a_{mn}B\\end{bmatrix}.</math>\n\nAn un-normalized 8-point Haar matrix <math>H_8</math> is shown below\n\n: <math>H_{8} = \\begin{bmatrix} 1&1&1&1&1&1&1&1 \\\\ 1&1&1&1&-1&-1&-1&-1 \\\\ 1&1&-1&-1&0&0&0&0& \\\\ 0&0&0&0&1&1&-1&-1 \\\\ 1&-1&0&0&0&0&0&0& \\\\ 0&0&1&-1&0&0&0&0 \\\\ 0&0&0&0&1&-1&0&0& \\\\ 0&0&0&0&0&0&1&-1 \\end{bmatrix}.</math>\n\nNote that, the above matrix is an un-normalized Haar matrix. The Haar matrix required by the Haar transform should be normalized.\n\nFrom the definition of the Haar matrix <math>H</math>, one can observe that, unlike the Fourier transform, <math>H</math> has only real elements (i.e., 1, -1 or 0) and is non-symmetric.\n\nTake the 8-point Haar matrix <math>H_8</math> as an example. The first row of <math>H_8</math> measures the average value, and the second row of <math>H_8</math> measures a low frequency component of the input vector. The next two rows are sensitive to the first and second half of the input vector respectively, which corresponds to moderate frequency components. The remaining four rows are sensitive to the four section of the input vector, which corresponds to high frequency components.<ref>{{cite web|url=http://fourier.eng.hmc.edu/e161/lectures/Haar/index.html |title=haar |publisher=Fourier.eng.hmc.edu |date=2013-10-30 |accessdate=2013-11-23}}</ref>\n\n==Haar transform==\nThe '''Haar transform''' is the simplest of the [[wavelet transform]]s. This transform cross-multiplies a function against the Haar wavelet with various shifts and stretches, like the Fourier transform cross-multiplies a function against a sine wave with two phases and many stretches.<ref>[http://sepwww.stanford.edu/public/docs/sep75/ray2/paper_html/node4.html The Haar Transform<!-- Bot generated title -->]</ref>{{clarify|Is this comparing the kernels being integrated over, and decomposing exponentials into sine and cosine to treat the Fourier kernel as a space of sines, changing the parametrization accordingly? If so, we can give more specific, linkable language than \"cross-multiplies\", talk about inner products or projections and integrating them, and then lucidly compare that to a convolutional treatment.|date=June 2018}}\n\n=== Introduction ===\nThe '''Haar transform''' is one of the oldest transform functions, proposed in 1910 by the Hungarian mathematician [[Alfréd Haar]]. It is found effective in applications such as signal and image compression in electrical and computer engineering as it provides a simple and computationally efficient approach for analysing the local aspects of a signal.\n\nThe Haar transform is derived from the Haar matrix. An example of a 4x4 Haar transformation matrix is shown below.\n\n:<math>H_4 = \\frac{1}{2}\n\\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 1 & -1 & -1 \\\\ \\sqrt{2} & -\\sqrt{2} & 0 & 0 \\\\ 0 & 0 & \\sqrt{2} & -\\sqrt{2}\\end{bmatrix}\n</math>\n\nThe Haar transform can be thought of as a sampling process in which rows of the transformation matrix act as samples of finer and finer resolution.\n\nCompare with the [[Walsh transform]], which is also 1/–1, but is non-localized.\n\n=== Property ===\nThe Haar transform has the following properties\n\n: 1. No need for multiplications. It requires only additions and there are many elements with zero value in the Haar matrix, so the computation time is short. It is faster than [[Walsh transform]], whose matrix is composed of +1 and −1.\n: 2. Input and output length are the same. However, the length should be a power of 2, i.e. <math>N = 2^k,  k\\in \\mathbb{N}</math>.\n: 3. It can be used to analyse the localized feature of signals. Due to the [[orthogonal]] property of the Haar function, the frequency components of input signal can be analyzed.\n\n=== Haar transform and Inverse Haar transform ===\nThe Haar transform ''y''<sub>''n''</sub> of an n-input function ''x''<sub>''n''</sub> is\n\n: <math> y_n = H_n x_n</math>\n\nThe Haar transform matrix is real and orthogonal. Thus, the inverse Haar transform can be derived by the following equations.\n\n: <math> H = H^*, H^{-1} = H^T, \\text{ i.e. } HH^T = I </math>\n\n: where <math>I</math> is the identity matrix. For example, when n = 4\n\n: <math> H_4^{T}H_4 = \\frac{1}{2}\\begin{bmatrix} 1&1&\\sqrt{2}&0 \\\\ 1&1&-\\sqrt{2}&0 \\\\ 1&-1&0&\\sqrt{2} \\\\ 1&-1&0&-\\sqrt{2}\\end{bmatrix}\n\\cdot\\; \\frac{1}{2}\\begin{bmatrix} 1&1&1&1 \\\\ 1&1&-1&-1 \\\\ \\sqrt{2}&-\\sqrt{2}&0&0 \\\\ 0&0&\\sqrt{2}&-\\sqrt{2}\\end{bmatrix}\n= \\begin{bmatrix} 1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&1&0 \\\\ 0&0&0&1 \\end{bmatrix}\n</math>\n\nThus, the inverse Haar transform is\n\n: <math> x_{n} = H^{T}y_{n}</math>\n\n=== Example ===\nThe Haar transform coefficients of a n=4-point signal <math>x_{4} = [1,2,3,4]^{T}</math> can be found as\n\n: <math> y_{4} = H_4 x_4 = \n\\frac{1}{2}\\begin{bmatrix} 1&1&1&1 \\\\ 1&1&-1&-1 \\\\ \\sqrt{2}&-\\sqrt{2}&0&0 \\\\ 0&0&\\sqrt{2}&-\\sqrt{2}\\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4\\end{bmatrix}\n= \\begin{bmatrix} 5 \\\\ -2 \\\\ -1/\\sqrt{2} \\\\ -1/\\sqrt{2}\\end{bmatrix}\n</math>\n\nThe input signal can then be perfectly reconstructed by the inverse Haar transform\n\n: <math> \\hat{x_{4}} = H_{4}^{T}y_{4} = \n\\frac{1}{2}\\begin{bmatrix} 1&1&\\sqrt{2}&0 \\\\ 1&1&-\\sqrt{2}&0 \\\\ 1&-1&0&\\sqrt{2} \\\\ 1&-1&0&-\\sqrt{2}\\end{bmatrix} \\begin{bmatrix} 5 \\\\ -2 \\\\ -1/\\sqrt{2} \\\\ -1/\\sqrt{2}\\end{bmatrix}\n= \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}\n</math>\n\n=== Application ===\nModern cameras are capable of producing images with resolutions in the range of tens of megapixels. These images need to be [[Image compression|compressed]] before storage and transfer. The Haar transform can be used for image compression. The basic idea is to transfer the image into a matrix in which each element of the matrix represents a pixel in the image. For example, a 256×256 matrix is saved for a 256×256 image. [[JPEG]] image compression involves cutting the original image into 8×8 sub-images. Each sub-image is an 8×8 matrix.\n\nThe 2-D Haar transform is required. The equation of the Haar transform is <math>B_n = H_n A_n H_n^T</math>, where <math>A_n</math> is a ''n''&nbsp;×&nbsp;''n'' matrix and <math>H_n</math> is n-point Haar transform. The inverse Haar transform is <math>A_n = H_n^T B_n H_n</math>\n\n==See also==\n* [[Dimension reduction]]\n* [[Walsh matrix]]\n* [[Walsh transform]]\n* [[Wavelet]]\n* [[Signal (electrical engineering)|Signal]]\n* [[Haar-like feature]] \n* [[Strömberg wavelet]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n*{{citation\n | last = Haar |first = Alfréd | author-link = Alfréd Haar\n | title = Zur Theorie der orthogonalen Funktionensysteme\n | journal = [[Mathematische Annalen]] \n | volume = 69 \n | year = 1910 \n | issue = 3 \n | pages = 331–371 \n | doi=10.1007/BF01456326 }}\n* Charles K. Chui, ''An Introduction to Wavelets'', (1992), Academic Press, San Diego, {{ISBN|0-585-47090-1}}\n* English Translation of Haar's seminal article: https://www.uni-hohenheim.de/~gzim/Publications/haar.pdf\n\n==External links==\n{{Commons category}}\n\n* {{springer|title=Haar system|id=p/h046070}}\n* [http://www.tomgibara.com/computer-vision/haar-wavelet Free Haar wavelet filtering implementation and interactive demo]\n* [http://packages.debian.org/wzip Free Haar wavelet denoising and lossy signal compression]\n\n===Haar transform===\n* [http://wayback.archive-it.org/all/20070728170955/http://cnx.org/content/m11087/latest/]\n* [http://math.hws.edu/eck/math371/applets/Haar.html]\n* [https://web.archive.org/web/20110125080404/http://online.redwoods.cc.ca.us/instruct/darnold/laproj/Fall2002/ames/paper.pdf]\n* [https://web.archive.org/web/20080318071618/http://scien.stanford.edu/class/ee368/projects2000/project12/2.html]\n* [http://fourier.eng.hmc.edu/e161/lectures/Haar/index.html]\n\n{{DEFAULTSORT:Haar Wavelet}}\n[[Category:Orthogonal wavelets]]"
    },
    {
      "title": "Alpha beta filter",
      "url": "https://en.wikipedia.org/wiki/Alpha_beta_filter",
      "text": "{{Confusing|date=February 2009}}\nAn '''alpha beta filter''' (also called alpha-beta filter, f-g filter or g-h filter<ref name=\"Brookner\">Eli Brookner: Tracking and Kalman Filtering Made Easy. Wiley-Interscience, 1st edition, 4 1998.</ref>) is a simplified form of [[State observer|observer]] for estimation, data smoothing and control applications. It is closely related to [[Kalman filter]]s and to linear state observers used in [[control theory]]. Its principal advantage is that it does not require a detailed system model.\n\n==Filter equations==\nAn alpha beta filter presumes that a system is adequately approximated by a model having two internal states, where the first state is obtained by integrating the value of the second state over time. Measured system output values correspond to observations of the first model state, plus disturbances. This very low order approximation is adequate for many simple systems, for example, mechanical systems where position is obtained as the time integral of velocity. Based on a mechanical system analogy, the two states can be called ''position x'' and ''velocity v''. Assuming that velocity remains approximately constant over the small time interval ''ΔT'' between measurements, the position state is projected forward to predict its value at the next sampling time using equation 1.\n\n:<math> \\text{(1)} \\quad \\hat{\\textbf{x}}_{k} \\leftarrow \\hat{\\textbf{x}}_{k-1} + \\Delta \\textrm{T}\\ \\textbf{ } \\hat{\\textbf{v}}_{k-1} </math>\n\nSince velocity variable ''v'' is presumed constant, its projected value at the next sampling time equals the current value.\n\n:<math> \\text{(2)} \\quad \\hat{\\textbf{v}}_{k} \\leftarrow  \\hat{\\textbf{v}}_{k-1}  </math>\n\nIf additional information is known about how a driving function will change the ''v'' state during each time interval, equation 2 can be modified to include it.\n\nThe output measurement is expected to deviate from the prediction because of noise and dynamic effects not included in the simplified dynamic model. This prediction error ''r'' is also called the ''residual'' or ''innovation'', based on statistical or Kalman filtering interpretations\n\n:<math> \\text{(3)} \\quad \\hat{\\textbf{r}}_{k} \\leftarrow \\textbf{x}_{k} - \\hat{\\textbf{x}}_{k} </math>\n\nSuppose that residual ''r'' is positive. This could result because the previous ''x'' estimate was low, the previous ''v'' was low, or some combination of the two. The alpha beta filter takes selected ''alpha'' and ''beta'' constants (from which the filter gets its name), uses ''alpha'' times the deviation ''r'' to correct the position estimate, and uses ''beta'' times the deviation ''r'' to correct the velocity estimate. An extra ''ΔT'' factor conventionally serves to normalize magnitudes of the multipliers.\n\n:<math> \\text{(4)} \\quad \\hat{\\textbf{x}}_{k} \\leftarrow \\hat{\\textbf{x}}_{k} + (\\alpha)\\ \\hat{\\textbf{r}}_{k} </math>\n \n:<math> \\text{(5)}\\quad \\hat{\\textbf{v}}_{k} \\leftarrow \\hat{\\textbf{v}}_{k} + ( \\beta / [ \\Delta \\textrm{T} ] )\\ \\hat{\\textbf{r}}_{k} </math>\n\nThe corrections can be considered small steps along an estimate of the gradient direction. As these adjustments accumulate, error in the state estimates is reduced. For convergence and stability, the values of the ''alpha'' and ''beta'' multipliers should be positive and small:<ref>C. Frank Asquith: Weight selection in first-order linear filters. Technical report, Army Intertial Guidance and Control Laboratory Center, Redstone Arsenal, Alabama, 1969.</ref>\n\n:<math> \\quad 0 < \\alpha < 1 </math>\n:<math> \\quad 0 < \\beta \\leq 2 </math>\n:<math> \\quad 0 < 4 - 2\\alpha - \\beta </math>\n\nNoise is suppressed only if <math>0 < \\beta < 1</math>, otherwise the noise is amplified.\n\nValues of ''alpha'' and ''beta'' typically are adjusted experimentally. In general, larger ''alpha'' and ''beta'' gains tend to produce faster response for tracking transient changes, while smaller ''alpha'' and ''beta'' gains reduce the level of noise in the state estimates. If a good balance between accurate tracking and noise reduction is found, and the algorithm is effective, filtered estimates are more accurate than the direct measurements.  This motivates calling the alpha-beta process a ''filter''.\n\n===Algorithm summary===\n'''Initialize.'''  \n* Set the initial values of state estimates ''x'' and ''v'', using prior information or additional measurements; otherwise, set the initial state values to zero. \n* Select values of the ''alpha'' and ''beta'' correction gains.\n\n'''Update.''' Repeat for each time step ΔT:<pre>\n  Project state estimates x and v using equations 1 and 2\n  Obtain a current measurement of the output value\n  Compute the residual r using equation 3\n  Correct the state estimates using equations 4 and 5\n  Send updated x and optionally v as the filter outputs\n</pre>\n\n==Sample program==\nAlpha Beta filter can be implemented in [[C (programming language)|C]]<ref name=\"ReferenceA\">Tremor Cancellation in Handheld Microsurgical Devices, TC83 by Gaurav Mittal, Deepansh Sehgal and Harsimran Jit Singh, [[Punjab Engineering College]]</ref> as follows:\n\n<pre>\n#include <stdio.h>\n#include <stdlib.h>\n\nint main()\n{\n\tfloat dt = 0.5;\n\tfloat xk_1 = 0, vk_1 = 0, a = 0.85, b = 0.005;\n\n\tfloat xk, vk, rk;\n\tfloat xm;\n\n\twhile( 1 )\n\t{\n\t\txm = rand() % 100;// input signal\n\n\t\txk = xk_1 + ( vk_1 * dt );\n\t\tvk = vk_1;\n\n\t\trk = xm - xk;\n\n\t\txk += a * rk;\n\t\tvk += ( b * rk ) / dt;\n\n\t\txk_1 = xk;\n\t\tvk_1 = vk;\n\n\t\tprintf( \"%f \\t %f\\n\", xm, xk_1 );\n\t\tsleep( 1 );\n\t}\n} \n</pre>\n\n===Result===\nThe following images depict the outcome of the above program in graphical format. In each image, the blue trace is the input signal; the output is red in the first image, yellow in the second, and green in the third. For the first two images, the output signal is visibly smoother than the input signal and lacks extreme spikes seen in the input. Also, the output moves in an estimate of [[gradient]] direction of input.\n\nThe higher the alpha parameter, the higher is the effect of input x and the less damping is seen. A low value of beta is effective in controlling sudden surges in velocity. Also, as alpha increases beyond unity, the output becomes rougher and more uneven than the input.<ref name=\"ReferenceA\"/>\n\n{| cellpadding=\"2\" style=\"border: 1px solid darkgray;\"\n|- border=\"0\"\n| [[File:alpha beta filter 0.85-0.005.jpg|320px|left|thumb|Results for alpha = 0.85 and beta = 0.005]]\n| [[File:alpha beta filter 0.5-0.1.jpg|320px|center|thumb|Results for alpha = 0.5 and beta = 0.1]]\n| [[File:alpha beta filter 1.5-.5.jpg|320px|right|thumb|Results for alpha = 1.5 and beta = 0.5]]\n|- align=\"center\"\n|\n|}\n\n==Relationship to general state observers==\nMore general state observers, such as the [[State observer|Luenberger observer]] for linear control systems, use a rigorous system model.  Linear observers use a gain matrix to determine state estimate corrections from multiple deviations between measured variables and predicted outputs that are linear combinations of state variables. In the case of alpha beta filters, this gain matrix reduces to two terms. There is no general theory for determining the best observer gain terms, and typically gains are adjusted experimentally for both.\n\nThe linear Luenberger [[state observer|observer equations]] reduce to the alpha beta filter by applying the following specializations and simplifications.\n\n* The discrete state transition matrix '''A''' is a square matrix of dimension 2, with all main diagonal terms equal to 1, and the first super-diagonal terms equal to ''ΔT''.\n* The observation equation matrix '''C''' has one row that selects the value of the first state variable for output.\n* The filter correction gain matrix '''L''' has one column containing the alpha and beta gain values.\n* Any known driving signal for the second state term is represented as part of the input signal vector '''u''', otherwise the '''u''' vector is set to zero.\n* Input coupling matrix '''B''' has a non-zero gain term as its last element if vector '''u''' is non-zero.\n\n==Relationship to Kalman filters==\nA [[Kalman filter]] estimates the values of state variables and corrects them in a manner similar to an alpha beta filter or a state observer. However, a Kalman filter does this in a much more formal and rigorous manner. The principal differences between Kalman filters and alpha beta filters are the following.\n\n* Like state observers, Kalman filters use a detailed dynamic system model that is not restricted to two states.\n* Like state observers, Kalman filters in general use multiple observed variables to correct state variable estimates, and these do not have to be direct measurements of individual system states.\n* A Kalman filter uses covariance noise models for states and observations. Using these, a time-dependent estimate of state covariance is updated automatically, and from this the [[Kalman filter|Kalman gain]] matrix terms are calculated. Alpha beta filter gains are manually selected and static.\n* For certain classes of problems, a Kalman filter is [[Wiener filter|Wiener optimal]], while alpha beta filtering is in general suboptimal.\n\nA Kalman filter designed to track a moving object using a constant-velocity target dynamics (process) model (i.e., constant velocity between measurement updates) with process noise covariance and measurement covariance held constant will converge to the same structure as an alpha-beta filter.  However, a Kalman filter's gain is computed recursively at each time step using the assumed process and measurement error statistics, whereas the alpha-beta's gain is computed ad hoc.\n\n===Choice of parameters===\nThe alpha-beta filter becomes a steady-state Kalman filter if filter parameters are calculated from the sampling interval <math>T</math>, the process variance <math>\\sigma_w^2</math> and the noise variance <math>\\sigma_v^2</math> like this<ref name=\"Kalata\">Paul R. Kalata: The tracking index: A generalized parameter for α-β and α-β-γ target trackers. IEEE Transactions on Aerospace and Electronic Systems, AES-20(2):174–181, March 1984.</ref><ref name=\"GrayMurray\">J. E. Gray and W. J. Murray: A derivation of an analytic expression for the tracking index for the alpha-beta-gamma filter. IEEE Trans. on Aerospace and Electronic Systems, 29:1064–1065, 1993.</ref>\n\n:<math>\\lambda = \\frac{\\sigma_w T^2}{\\sigma_v}</math>\n:<math>r = \\frac{4 + \\lambda - \\sqrt{8\\lambda + \\lambda^2}}{4}</math>\n:<math>\\alpha = 1 - r^2</math>\n:<math>\\beta = 2\\left(2 - \\alpha\\right) - 4\\sqrt{1 - \\alpha}</math>\n\nThis choice of filter parameters minimizes the mean square error.\n\nThe steady state innovation variance  <math>s</math> can be expressed as:\n\n:<math>s = \\frac{\\sigma_v^2}{1-\\alpha^2}</math>\n\n==Variations==\n===Alpha filter===\nA simpler member of this family of filters is the alpha filter which observes only one state:\n\n:<math>\n\\hat{\\textbf{x}}_{k} \\leftarrow \\hat{\\textbf{x}}_{k} + (\\alpha)\\ \\hat{\\textbf{r}}_{k}\n</math>\n\nwith the optimal parameter calculated like this:<ref name=\"Kalata\" />\n\n:<math>\n\\begin{align}\n\\lambda & = \\frac{\\sigma_w T^2}{\\sigma_v} \\\\\n\\alpha & = \\frac{-\\lambda^2 + \\sqrt{\\lambda^4 + 16\\lambda^2}}{8}\n\\end{align}\n</math>\n\nThis calculation is identical for a [[moving average]] and a [[low-pass filter]].\n\n===Alpha beta gamma filter===\nWhen the second state variable varies quickly, i.e. when the acceleration of the first state is large, it can be useful to extend the states of the alpha beta filter by one level. In this extension, the second state variable ''v'' is obtained from integrating a third ''acceleration'' state, analogous to the way that the first state is obtained by integrating the second. An equation for the ''a'' state is added to the equation system.  A third multiplier, ''gamma'', is selected for applying corrections to the new ''a'' state estimates. This yields the ''alpha beta gamma'' update equations.<ref name=\"Brookner\" />\n\n:<math> \\hat{\\textbf{x}}_{k} \\leftarrow \\hat{\\textbf{x}}_{k} + ( \\alpha )\\ \\textbf{r}_{k} </math>\n\n:<math> \\hat{\\textbf{v}}_{k} \\leftarrow \\hat{\\textbf{v}}_{k} + ( \\beta / [ \\Delta \\textrm{T} ] )\\ \\textbf{r}_{k} </math>\n\n:<math> \\hat{\\textbf{a}}_{k} \\leftarrow \\hat{\\textbf{a}}_{k} + ( 2\\gamma / [ \\Delta \\textrm{T} ]^\\textrm{2} )\\ \\textbf{r}_{k} </math>\n\nSimilar extensions to additional higher orders are possible, but most systems of higher order tend to have significant interactions among the multiple states, {{citation needed|date=May 2013}} so approximating the system dynamics as a simple integrator chain is less likely to prove useful.\n\nCalculating optimal parameters for the alpha-beta-gamma filter is a bit more involved than for the alpha-beta filter:<ref name=\"GrayMurray\" />\n\n:<math>\n\\begin{align}\n\\lambda & = \\frac{\\sigma_w T^2}{\\sigma_v} \\\\[2ex]\nb & = \\frac{\\lambda}{2} - 3 \\\\\nc & = \\frac{\\lambda}{2} + 3 \\\\\nd & = -1 \\\\\np & = c - \\frac{b^2}{3} \\\\\nq & = \\frac{2b^3}{27} - \\frac{bc}{3} + d \\\\\nv & = \\sqrt{q^2 + \\frac{4p^3}{27}} \\\\\nz & = -\\sqrt[3]{q + \\frac{v}{2}} \\\\\ns & = z - \\frac{p}{3z} - \\frac{b}{3} \\\\[2ex]\n\\alpha & = 1 - s^2 \\\\\n\\beta & = 2(1 - s)^2 \\\\\n\\gamma & = \\frac{\\beta^2}{2\\alpha}\n\\end{align}\n</math>\n\n==See also==\n* [[Kalman filter]]\n* [[Control theory]]\n* [[State space (controls)]]\n* [[Moving average]]\n\n==References==\n{{reflist}}\n\n;Sources\n*[http://collaboration.cmc.ec.gc.ca/science/rpn/biblio/ddj/Website/articles/CUJ/1993/9307/penoyer/penoyer.htm \"The Alpha-Beta Filter\", Penoyer, Robert; C Users Journal, July 1993]\n*[http://www.mstarlabs.com/control/engspeed.html \"Engine Speed Monitoring: The Alpha-Beta Filter\", Microstar Laboratories]\n*[http://ieeexplore.ieee.org/Xplore/login.jsp?url=/iel4/7/2265/00062250.pdf?arnumber=62250 \"Reconciling steady-state Kalman and alpha-beta filter design\", by Painter, J.H.; Kerstetter, D.; Jowers, S. IEEE Transactions on Aerospace and Electronic Systems, Volume 26, Issue 6, Nov. 1990, pp. 986–991]\n*[http://ieeexplore.ieee.org/Xplore/login.jsp?url=/iel5/7/30189/01386894.pdf?temp=x Fixed-lag alpha-beta filter for target trajectory smoothing Ogle, T.L.; Blair, W.D. Aerospace and Electronic Systems, IEEE Transactions on Volume 40, Issue 4, Oct. 2004, pp. 1417–1421]\n*[http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=html&identifier=AD0759011 \"Description of an alpha-beta Filter in Cartesian Coordinates\", by Cantrell, Ben H., NAVAL RESEARCH LAB WASHINGTON DC, March 21, 1973]\n*[http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=html&identifier=ADA329021 \"Comparison of Four Filtering Options for a Radar Tracking Problem\", by Lawton, John A. ; Jesionowski, Robert J. ; Zarchan, Paul. NAVAL SURFACE WARFARE CENTER DAHLGREN DIV VA, 1979.]\n*[https://books.google.com/books?id=HxjDMcJWLYwC&pg=PA160 ''Mathematical Techniques in Multisensor Data Fusion''], By David Lee Hall, Sonya A. H., Artech House, 2004, {{ISBN|1-58053-335-3}}, section 4.4.4\n\n==External links==\n*[https://github.com/mikasjp/ins/blob/master/InertialNavigationSystem/AlphaBetaFilter.cs Alpha-Beta C# source code sample]\n*[http://adrianboeing.blogspot.com/2010/03/alpha-beta-filters.html Alpha-Beta C source code sample]\n*[http://www.advsolned.com/example_radar_tracking.html Tracking performance of the alpha-beta tracker versus the Kalman filter]\n\n[[Category:Filter theory]]\n[[Category:Signal processing]]\n[[Category:Statistical approximations]]"
    },
    {
      "title": "Approximate Bayesian computation",
      "url": "https://en.wikipedia.org/wiki/Approximate_Bayesian_computation",
      "text": "{{short description|Computational method in Bayesian statistics}}\n{{Bayesian statistics}}\n\n'''Approximate Bayesian computation''' ('''ABC''') constitutes a class of [[Computational science|computational methods]] rooted in [[Bayesian statistics]] that can be used to estimate the posterior distributions of model parameters. \n\nIn all model-based  [[statistical inference]], the [[likelihood|likelihood function]] is of central importance, since it expresses the probability of the observed data under a particular [[statistical model]], and thus quantifies the support data lend to particular values of parameters and to choices among different models. For simple models, an analytical formula for the likelihood function can typically be derived. However, for more complex models, an analytical formula might be elusive or the likelihood function might be computationally very costly to evaluate.\n\nABC methods bypass the evaluation of the likelihood function. In this way, ABC methods widen the realm of models for which statistical inference can be considered. ABC methods are mathematically well-founded, but they inevitably make assumptions and approximations whose impact needs to be carefully assessed. Furthermore, the wider application domain of ABC exacerbates the challenges of [[Estimation Theory|parameter estimation]] and [[model selection]].\n\nABC has rapidly gained popularity over the last years and in particular for the analysis of complex problems arising in [[Biology|biological sciences]], e.g. in [[population genetics]], [[ecology]], [[epidemiology]], and [[systems biology]].\n\n==History==\nThe first ABC-related ideas date back to the 1980s. [[Donald Rubin]], when discussing the interpretation of Bayesian statements in 1984,<ref name=\"Rubin\" /> described a hypothetical sampling mechanism that yields a sample from the [[Posterior probability|posterior distribution]]. This scheme was more of a conceptual [[thought experiment]] to demonstrate what type of manipulations are done when inferring the posterior distributions of parameters. The description of the sampling mechanism coincides exactly with that of the [[#The ABC rejection algorithm|ABC-rejection scheme]], and this article can be considered to be the first to describe approximate Bayesian computation. However, a two-stage [[quincunx]] was constructed by [[Francis Galton]] in the late 1800s that can be seen as a physical implementation of an [[#The ABC rejection algorithm|ABC-rejection scheme]] for a single unknown (parameter) and a single observation.<ref name=\"Stigler2010\">see figure 5 in {{cite journal|last1=Stigler|first1=Stephen M.|title=Darwin, Galton and the Statistical Enlightenment|journal=Journal of the Royal Statistical Society. Series A (Statistics in Society)|volume=173|issue=3|year=2010|pages=469–482|issn=0964-1998|doi=10.1111/j.1467-985X.2010.00643.x}}</ref> Another prescient point was made by Rubin when he argued that in Bayesian inference, applied statisticians should not settle for analytically tractable models only, but instead consider computational methods that allow them to estimate the posterior distribution of interest. This way, a wider range of models can be considered. These arguments are particularly relevant in the context of ABC.\n\nIn 1984, [[Peter Diggle]] and Richard Gratton<ref name=\"Diggle\" /> suggested using a systematic simulation scheme to approximate the likelihood function in situations where its analytic form is [[Intractability (complexity)|intractable]]. Their method was based on defining a grid in the parameter space and using it to approximate the likelihood by running several simulations for each grid point. The approximation was then improved by applying smoothing techniques to the outcomes of the simulations. While the idea of using simulation for hypothesis testing was not new,<ref name=\"Bartlett63\" /><ref name=\"Hoel71\" /> Diggle and Gratton seemingly introduced the first procedure using simulation to do statistical inference under a circumstance where the likelihood is intractable.\n\nAlthough Diggle and Gratton’s approach had opened a new frontier, their method was not yet exactly identical to what is now known as ABC, as it aimed at approximating the likelihood rather than the posterior distribution. An article of [[Simon Tavaré]] ''et al.''<ref name=\"Tavare\" /> was first to propose an ABC algorithm for posterior inference. In their seminal work, inference about the genealogy of DNA sequence data was considered, and in particular the problem of deciding the posterior distribution of the time to the [[most recent common ancestor]] of the sampled individuals. Such inference is analytically intractable for many demographic models, but the authors presented ways of simulating coalescent trees under the putative models. A sample from the posterior of model parameters was obtained by accepting/rejecting proposals based on comparing the number of segregating sites in the synthetic and real data.  This work was followed by an applied study on modeling the variation in human Y chromosome by [[Jonathan K. Pritchard]] ''et al.''<ref name=\"Pritchard1999\" /> using the ABC method. Finally, the term approximate Bayesian computation was established by Mark Beaumont ''et al.'',<ref name=\"Beaumont2002\" /> extending further the ABC methodology and discussing the suitability of the ABC-approach more specifically for problems in population genetics. Since then, ABC has spread to applications outside population genetics, such as systems biology, epidemiology, and [[phylogeography]].\n\n==Method==\n\n===Motivation===\nA common incarnation of [[Bayes' theorem|Bayes’ theorem]] relates the [[conditional probability]] (or density) of a particular parameter value <math>\\theta</math> given data <math>D</math> to the [[probability]] of <math>D</math> given <math>\\theta</math> by the rule\n\n:<math>p(\\theta|D) = \\frac{p(D|\\theta)p(\\theta)}{p(D)}</math>,\n\nwhere <math>p(\\theta|D)</math> denotes the posterior, <math>p(D|\\theta)</math> the likelihood, <math>p(\\theta)</math> the prior, and <math>p(D)</math> the evidence (also referred to as the [[marginal likelihood]] or the prior predictive probability of the data).\n\nThe prior represents beliefs about <math>\\theta</math> before <math>D</math> is available, and it is often specified by choosing a particular distribution among a set of well-known and tractable families of distributions, such that both the evaluation of prior probabilities and random generation of values of <math>\\theta</math> are relatively straightforward. For certain kinds of models, it is more pragmatic to specify the prior <math>p(\\theta)</math> using a factorization of the joint distribution of all the elements of <math>\\theta</math> in terms of a sequence of their conditional distributions. If one is only interested in the relative posterior plausibilities of different values of <math>\\theta</math>, the evidence <math>p(D)</math> can be ignored, as it constitutes a [[Normalizing constant|normalising constant]], which cancels for any ratio of posterior probabilities. It remains, however, necessary to evaluate the likelihood <math>p(D|\\theta)</math> and the prior <math>p(\\theta)</math>. For numerous applications, it is [[computationally expensive]], or even completely infeasible, to evaluate the likelihood,<ref name=\"Busetto2009a\" /> which motivates the use of ABC to circumvent this issue.\n\n===The ABC rejection algorithm===\nAll ABC-based methods approximate the likelihood function by simulations, the outcomes of which are compared with the observed data.<ref name=\"Beaumont2010\" /><ref name=\"Bertorelle\" /><ref name=\"Csillery\" /> More specifically, with the ABC rejection algorithm—the most basic form of ABC—a set of parameter points is first sampled from the prior distribution. Given a sampled parameter point <math>\\hat{\\theta}</math>, a data set <math>\\hat{D}</math> is then simulated under the statistical model <math>M</math> specified by <math>\\hat{\\theta}</math>. If the generated <math>\\hat{D}</math> is too different from the observed data <math>D</math>, the sampled parameter value is discarded. In precise terms, <math>\\hat{D}</math> is accepted with tolerance <math>\\epsilon \\ge 0</math> if:\n\n:<math>\\rho (\\hat{D},D)\\le\\epsilon</math>,\n\nwhere the distance measure <math>\\rho(\\hat{D},D)</math> determines the level of discrepancy between <math>\\hat{D}</math> and <math>D</math> based on a given [[Metric (mathematics)|metric]] (e.g. [[Euclidean distance]]). A strictly positive tolerance is usually necessary, since the probability that the simulation outcome coincides exactly with the data (event <math>\\hat{D}=D</math>) is negligible for all but trivial applications of ABC, which would in practice lead to rejection of nearly all sampled parameter points. The outcome of the ABC rejection algorithm is a sample of parameter values approximately distributed according to the desired posterior distribution, and, crucially, obtained without the need to explicitly evaluate the likelihood function.[[Image:Approximate Bayesian computation conceptual overview.svg|632px|thumb|center|Parameter estimation by approximate Bayesian computation: a conceptual overview.]]\n\n===Summary statistics===\nThe probability of generating a data set <math>\\hat{D}</math> with a small distance to <math>D</math> typically decreases as the dimensionality of the data increases. This leads to a substantial decrease in the computational efficiency of the above basic ABC rejection algorithm. A common approach to lessen this problem is to replace <math>D</math> with a set of lower-dimensional [[summary statistics]] <math>S(D)</math>, which are selected to capture the relevant information in <math>D</math>. The acceptance criterion in ABC rejection algorithm becomes:\n\n:<math>\\rho(S(\\hat{D}),S(D))\\le\\epsilon</math>.\n\nIf the summary statistics are [[Sufficient statistic|sufficient]] with respect to the model parameters <math>\\theta</math>, the efficiency increase obtained in this way does not introduce any error.<ref name=\"Didelot\" /> Indeed, by definition, sufficiency implies that all information in <math>D</math> about <math>\\theta</math> is captured by <math>S(D)</math>.\n\nAs [[#Choice and sufficiency of summary statistics|elaborated below]], it is typically impossible, outside the [[Exponential family|exponential family of distributions]], to identify a finite-dimensional set of sufficient statistics. Nevertheless, informative but possibly insufficient summary statistics are often used in applications where inference is performed with ABC methods.\n\n==Example==\n[[Image:A dynamic bistable hidden Markov model.svg|326px|thumb|right| A dynamic bistable hidden Markov model.]]\n\nAn illustrative example is a [[Bistability|bistable]] system that can be characterized by a [[Hidden Markov model|hidden Markov model (HMM)]] subject to measurement noise. Such models are employed for many biological systems: They have, for example, been used in development, [[cell signaling]], [[activation]]/deactivation, logical processing and [[non-equilibrium thermodynamics]]. For instance, the behavior of the [[Sonic hedgehog (protein)|Sonic hedgehog]] (Shh) transcription factor in ''[[Drosophila melanogaster]]'' can be modeled with an HMM.<ref name=\"Lai\" /> The (biological) dynamical model consists of two states: A and B. If the probability of a transition from one state to the other is defined as <math>\\theta</math> in both directions, then the probability to remain in the same state at each time step is <math>{1-\\theta}</math>. The probability to measure the state correctly is <math>\\gamma</math> (and conversely, the probability of an incorrect measurement is <math>{1-\\gamma}</math>).\n\nDue to the conditional dependencies between states at different time points, calculation of the likelihood of time series data is somewhat tedious, which illustrates the motivation to use ABC. A computational issue for basic ABC is the large dimensionality of the data in an application like this. The dimensionality can be reduced using the summary statistic <math>S</math>, which is the frequency of switches between the two states. The absolute difference is used as a distance measure <math>\\rho(\\cdot,\\cdot)</math> with tolerance <math>\\epsilon=2</math>. The posterior inference about the parameter <math>\\theta</math> can be done following the five steps presented in.\n\n'''Step 1:'''  Assume that the observed data form the state sequence AAAABAABBAAAAAABAAAA, which is generated using <math>\\theta=0.25</math> and <math>\\gamma=0.8</math>. The associated summary statistic—the number of switches between the states in the experimental data—is <math>\\omega_E=6</math>.\n\n'''Step 2:''' Assuming nothing is known about <math>\\theta</math>, a uniform prior in the interval <math>[0,1]</math> is employed. The parameter <math>\\gamma</math> is assumed to be known and fixed to the data-generating value <math>\\gamma=0.8</math>, but it could in general also be estimated from the observations. A total of <math>n</math> parameter points are drawn from the prior, and the model is simulated for each of the parameter points <math>\\theta_i: \\text{ } i = 1,\\ldots, n</math>, which results in <math>n</math> sequences of simulated data. In this example, <math>n=5</math>, with each drawn parameter and simulated dataset recorded in [[#table1|Table 1, columns 2-3]]. In practice, <math>n</math> would need to be much larger to obtain an appropriate approximation.\n\n{| class=\"sortable wikitable\" style=\"float: right; margin-left: 1em; text-align: center;\" id=\"table1\"\n|+Example of ABC rejection algorithm\n|-\n! i\n! <math>\\theta_i</math>\n! Simulated datasets (step 2)\n! Summary statistic <br/> <math>\\omega_{S,i}</math> (step 3)\n! Distance <math>\\rho(\\omega_{S,i}, \\omega_E)</math> <br />(step 4)\n! Outcome <br />(step 4)\n|-\n| 1\n| 0.08\n| AABAAAABAABAAABAAAAA\n| 8\n| 2\n| accepted\n|-\n| 2\n| 0.68\n| AABBABABAAABBABABBAB\n| 13\n| 7\n| rejected\n|-\n| 3\n| 0.87\n| BBBABBABBBBABABBBBBA\n| 9\n| 3\n| rejected\n|-\n| 4\n| 0.43\n| AABAAAAABBABBBBBBBBA\n| 6\n| 0\n| accepted\n|-\n| 5\n| 0.53\n| ABBBBBAABBABBABAABBB\n| 9\n| 3\n| rejected\n|}\n\n'''Step 3:''' The summary statistic is computed for each sequence of simulated data <math>\\omega_{S,i}: \\text{ } i = 1,\\ldots,n</math>.\n\n'''Step 4:''' The distance between the observed and simulated transition frequencies <math>\\rho(\\omega_{S,i}, \\omega_E) = |\\omega_{S,i} - \\omega_{E}|</math> is computed for all parameter points. Parameter points for which the distance is smaller than or equal to <math>\\epsilon</math> are accepted as approximate samples from the posterior.[[Image:Approximate Bayesian Computation example.svg|500px|thumb|right| Posterior of <math>\\theta</math> obtained in the example (red), compared to the true posterior distribution (black) and ABC simulations with large <math>n</math>. The use of the insufficient summary statistic <math>\\omega</math> introduces bias, even when requiring <math>\\epsilon=0</math> (light green).]]\n\n'''Step 5:''' The posterior distribution is approximated with the accepted parameter points. The posterior distribution should have a non-negligible probability for parameter values in a region around the true value of <math>\\theta</math> in the system if the data are sufficiently informative. In this example, the posterior probability mass is evenly split between the values 0.08 and 0.43.\n\nThe posterior probabilities are obtained via ABC with large <math>n</math> by utilizing the summary statistic (with <math>\\epsilon = 0 </math> and <math>\\epsilon = 2 </math>) and the full data sequence (with <math>\\epsilon = 0 </math>). These are compared with the true posterior, which can be computed exactly and efficiently using the [[Viterbi algorithm]]. The summary statistic utilized in this example is not sufficient, as the deviation from the theoretical posterior is significant even under the stringent requirement of <math>\\epsilon = 0 </math>. It should be noted that a much longer observed data sequence would be needed to obtain a posterior concentrated around <math>\\theta = 0.25</math>, the true value of <math>\\theta</math>.\n\nThis example application of ABC uses simplifications for illustrative purposes. More realistic applications of ABC are available in a growing number of peer-reviewed articles.<ref name=\"Beaumont2010\" /><ref name=\"Bertorelle\" /><ref name=\"Csillery\" /><ref name=\"Marin11\" />\n\n==Model comparison with ABC==\nOutside of parameter estimation, the ABC framework can be used to compute the posterior probabilities of different candidate models.<ref name=\"Wilkinson2007\" /><ref name=\"Grelaud\" /><ref name=\"Toni2010\" /> In such applications, one possibility is to use rejection sampling in a hierarchical manner. First, a model is sampled from the prior distribution for the models. Then, parameters are sampled from the prior distribution assigned to that model. Finally, a simulation is performed as in single-model ABC. The relative acceptance frequencies for the different models now approximate the posterior distribution for these models. Again, computational improvements for ABC in the space of models have been proposed, such as constructing a particle filter in the joint space of models and parameters.<ref name=\"Toni2010\" />\n\nOnce the posterior probabilities of the models have been estimated, one can make full use of the techniques of [[Bayesian model comparison]]. For instance, to compare the relative plausibilities of two models <math>M_1</math> and <math>M_2</math>, one can compute their posterior ratio, which is related to the [[Bayes factor]] <math>B_{1,2}</math>:\n\n:<math>\\frac{p(M_1 |D)}{p(M_2|D)}=\\frac{p(D|M_1)}{p(D|M_2)}\\frac{p(M_1)}{p(M_2)} = B_{1,2}\\frac{p(M_1)}{p(M_2)}</math>.\n\nIf the model priors are equal—that is, <math>p(M_1)=p(M_2)</math>—the Bayes factor equals the posterior ratio.\n\nIn practice, [[#Bayes factor with ABC and summary statistics|as discussed below]], these measures can be highly sensitive to the choice of parameter prior distributions and summary statistics, and thus conclusions of model comparison should be drawn with caution.\n\n==Pitfalls and remedies==\n\n{| class=\"sortable wikitable\" style=\"float: right; margin-left: 1em;\"  width=\"75%\" id=\"table2\"\n|+Potential risks and remedies in ABC-based statistical inference\n\n|-\n! Error source\n! Potential issue\n! Solution\n! Subsection\n|-\n| Nonzero tolerance <math>\\epsilon</math>\n| The inexactness introduces bias into the computed posterior distribution.\n| Theoretical/practical studies of the sensitivity of the posterior distribution to the tolerance. Noisy ABC.\n| [[#Approximation of the posterior]]\n|-\n| Insufficient summary statistics\n| The information loss causes inflated credible intervals.\n| Automatic selection/semi-automatic identification of sufficient statistics. Model validation checks (e.g., Templeton 2009<ref name=\"Templeton2009b\" />).\n| [[#Choice and sufficiency of summary statistics]]\n|-\n| Small number of models/incorrectly specified models\n| The investigated models are not representative/lack predictive power.\n| Careful selection of models. Evaluation of the predictive power.\n| [[#Small number of models]]\n|-\n| Priors and parameter ranges\n| Conclusions may be sensitive to the choice of priors. Model choice may be meaningless.\n| Check sensitivity of Bayes factors to the choice of priors. Some theoretical results regarding choice of priors are available. Use alternative methods for model validation.\n| [[#Prior distribution and parameter ranges]]\n|-\n| Curse of dimensionality\n| Low parameter acceptance rates. Model errors cannot be distinguished from an insufficient exploration of the parameter space.  Risk of overfitting.\n| Methods for model reduction if applicable. Methods to speed up the parameter exploration. Quality controls to detect overfitting.\n| [[#Curse of dimensionality]]\n|-\n| Model ranking with summary statistics\n| The computation of Bayes factors on summary statistics may not be related to the Bayes factors on the original data, which may therefore render the results meaningless.\n| Only use summary statistics that fulfill the necessary and sufficient conditions to produce a consistent Bayesian model choice. Use alternative methods for model validation.\n| [[#Bayes factor with ABC and summary statistics]]\n|-\n| Implementation\n| Low protection to common assumptions in the simulation and the inference process.\n| Sanity checks of results. Standardization of software.\n| [[#Indispensable quality controls]]\n|}\n\nAs for all statistical methods, a number of assumptions and approximations are inherently required for the application of ABC-based methods to real modeling problems. For example, setting the [[#The ABC rejection algorithm|tolerance parameter <math>\\epsilon </math>]] to zero ensures an exact result, but typically makes computations prohibitively expensive. Thus, values of <math>\\epsilon</math> larger than zero are used in practice, which introduces a bias. Likewise, sufficient statistics are typically not available and instead, other summary statistics are used, which introduces an additional bias due to the loss of information. Additional sources of bias- for example, in the context of model selection—may be more subtle.<ref name=\"Didelot\" /><ref name=\"Robert\" />\n\nAt the same time, some of the criticisms that have been directed at the ABC methods, in particular within the field of [[phylogeography]],<ref name=\"Templeton2009b\" /><ref name=\"Templeton2008\" /><ref name=\"Templeton2009a\" /> are not specific to ABC and apply to all Bayesian methods or even all statistical methods (e.g., the choice of prior distribution and parameter ranges).<ref name=\"Beaumont2010\" /><ref name=\"Berger\" /> However, because of the ability of ABC-methods to handle much more complex models, some of these general pitfalls are of particular relevance in the context of ABC analyses.\n\nThis section discusses these potential risks and reviews possible ways to address them.\n\n===Approximation of the posterior===\nA non-negligible <math>\\epsilon</math> comes with the price that one samples from <math>p(\\theta|\\rho(\\hat{D},D)\\le\\epsilon)</math> instead of the true posterior <math>p(\\theta|D)</math>. With a sufficiently small tolerance, and a sensible distance measure, the resulting distribution <math>p(\\theta|\\rho(\\hat{D},D)\\le\\epsilon)</math> should often approximate the actual target distribution <math>p(\\theta|D)</math> reasonably well. On the other hand, a tolerance that is large enough that every point in the parameter space becomes accepted will yield a replica of the prior distribution. There are empirical studies of the difference between <math>p(\\theta|\\rho(\\hat{D},D)\\le\\epsilon)</math> and <math>p(\\theta|D)</math> as a function of <math>\\epsilon</math>,<ref name=\"Sisson\" /> and theoretical results for an upper <math>\\epsilon</math>-dependent bound for the error in parameter estimates.<ref name=\"Dean\" /> The accuracy of the posterior (defined as the expected quadratic loss) delivered by ABC as a function of <math>\\epsilon</math> has also been investigated.<ref name=\"Fearnhead\" /> However, the convergence of the distributions when <math>\\epsilon</math> approaches zero, and how it depends on the distance measure used, is an important topic that has yet to be investigated in greater detail. In particular, it remains difficult to disentangle errors introduced by this approximation from errors due to model mis-specification.<ref name=\"Beaumont2010\" />\n\nAs an attempt to correct some of the error due to a non-zero <math>\\epsilon</math>, the usage of local linear weighted regression with ABC to reduce the variance of the posterior estimates has been suggested.<ref name=\"Beaumont2002\" /> The method assigns weights to the parameters according to how well simulated summaries adhere to the observed ones and performs linear regression between the summaries and the weighted parameters in the vicinity of observed summaries. The obtained regression coefficients are used to correct sampled parameters in the direction of observed summaries. An improvement was suggested in the form of nonlinear regression using a feed-forward neural network model.<ref name=\"Blum2010\" /> However, it has been shown that the posterior distributions obtained with these approaches are not always consistent with the prior distribution, which did lead to a reformulation of the regression adjustment that respects the prior distribution.<ref name=\"Leuenberger2009\" />\n\nFinally, statistical inference using ABC with a non-zero tolerance <math>\\epsilon</math> is not inherently flawed: under the assumption of measurement errors, the optimal <math>\\epsilon</math> can in fact be shown to be not zero.<ref name=\"Fearnhead\" /><ref name=\"Wilkinson\" /> Indeed, the bias caused by a non-zero tolerance can be characterized and compensated by introducing a specific form of noise to the summary statistics. Asymptotic consistency for such “noisy ABC”, has been established, together with formulas for the asymptotic variance of the parameter estimates for a fixed tolerance.<ref name=\"Fearnhead\" />\n\n===Choice and sufficiency of summary statistics===\nSummary statistics may be used to increase the acceptance rate of ABC for high-dimensional data. Low-dimensional sufficient statistics are optimal for this purpose, as they capture all relevant information present in the data in the simplest possible form.<ref name=\"Csillery\" /> However, low-dimensional sufficient statistics are typically unattainable for statistical models where ABC-based inference is most relevant, and consequently, some [[heuristic]] is usually necessary to identify useful low-dimensional summary statistics. The use of a set of poorly chosen summary statistics will often lead to inflated [[credible interval]]s due to the implied loss of information,<ref name=\"Csillery\" /> which can also bias the discrimination between models. A review of methods for choosing summary statistics is available,<ref name=\"Blum12\" /> which may provide valuable guidance in practice.\n\nOne approach to capture most of the information present in data would be to use many statistics, but the accuracy and stability of ABC appears to decrease rapidly with an increasing numbers of summary statistics.<ref name=\"Beaumont2010\" /><ref name=\"Csillery\" /> Instead, a better strategy is to focus on the relevant statistics only—relevancy depending on the whole inference problem, on the model used, and on the data at hand.<ref name=\"Nunes\" />\n\nAn algorithm has been proposed for identifying a representative subset of summary statistics, by iteratively assessing whether an additional statistic introduces a meaningful modification of the posterior.<ref name=\"Joyce\" /> One of the challenges here is that a large ABC approximation error may heavily influence the conclusions about the usefulness of a statistic at any stage of the procedure. Another method<ref name=\"Nunes\" /> decomposes into two main steps. First, a reference approximation of the posterior is constructed by minimizing the [[Entropy (statistical thermodynamics)|entropy]]. Sets of candidate summaries are then evaluated by comparing the ABC-approximated posteriors with the reference posterior.\n\nWith both of these strategies, a subset of statistics is selected from a large set of candidate statistics. Instead, the [[partial least squares regression]] approach uses information from all the candidate statistics, each being weighted appropriately.<ref name=\"Wegmann\" /> Recently, a method for constructing summaries in a semi-automatic manner has attained a considerable interest.<ref name=\"Fearnhead\" /> This method is based on the observation that the optimal choice of summary statistics, when minimizing the quadratic loss of the parameter point estimates, can be obtained through the posterior mean of the parameters, which is approximated by performing a linear regression based on the simulated data.\n\nMethods for the identification of summary statistics that could also simultaneously assess the influence on the approximation of the posterior would be of substantial value.<ref name=\"Marjoram\" /> This is because the choice of summary statistics and the choice of tolerance constitute two sources of error in the resulting posterior distribution. These errors may corrupt the ranking of models and may also lead to incorrect model predictions. Indeed, none of the methods above assesses the choice of summaries for the purpose of model selection.\n\n===Bayes factor with ABC and summary statistics===\nIt has been shown that the combination of insufficient summary statistics and ABC for model selection can be problematic.<ref name=\"Didelot\" /><ref name=\"Robert\" /> Indeed, if one lets the Bayes factor based on the summary statistic <math>S(D)</math> be denoted by <math>B_{1,2}^s</math>, the relation between <math>B_{1,2}</math> and <math>B_{1,2}^s</math> takes the form:<ref name=\"Didelot\" />\n\n:<math>B_{1,2}=\\frac{p(D|M_1)}{p(D|M_2)}=\\frac{p(D|S(D),M_1)}{p(D|S(D),M_2)} \\frac{p(S(D)|M_1)}{p(S(D)|M_2)}=\\frac{p(D|S(D),M_1)}{p(D|S(D),M_2)} B_{1,2}^s</math>.\n\nThus, a summary statistic <math>S(D)</math> is sufficient for comparing two models <math>M_1</math> and <math>M_2</math> if and only if:\n\n:<math>p(D|S(D),M_1)=p(D|S(D),M_2)</math>,\t\n\nwhich results in that <math>B_{1,2}=B_{1,2}^s</math>. It is also clear from the equation above that there might be a huge difference between <math>B_{1,2}</math> and <math>B_{1,2}^s</math> if the condition is not satisfied, as can be demonstrated by toy examples.<ref name=\"Didelot\" /><ref name=\"Grelaud\" /><ref name=\"Robert\" /> Crucially, it was shown that sufficiency for <math>M_1</math> or <math>M_2</math> alone, or for both models, does not guarantee sufficiency for ranking the models.<ref name=\"Didelot\" /> However, it was also shown that any [[Sufficient statistic|sufficient summary statistic]] for a model <math>M</math> in which both <math>M_1</math> and <math>M_2</math> are [[Multilevel model|nested]] is valid for ranking the [[Multilevel model|nested models]].<ref name=\"Didelot\" />\n\nThe computation of Bayes factors on <math>S(D)</math> may therefore be misleading for model selection purposes, unless the ratio between the Bayes factors on <math>D</math> and <math>S(D)</math> would be available, or at least could be approximated reasonably well. Alternatively, necessary and sufficient conditions on summary statistics for a consistent Bayesian model choice have recently been derived,<ref name=\"Marin\" /> which can provide useful guidance.\n\nHowever, this issue is only relevant for model selection when the dimension of the data has been reduced. ABC-based inference, in which the actual data sets are directly compared—as is the case for some systems biology applications (e.g., see <ref name=\"Toni\" />)—circumvents this problem.\n\n===Indispensable quality controls===\nAs the above discussion makes clear, any ABC analysis requires choices and trade-offs that can have a considerable impact on its outcomes. Specifically, the choice of competing models/hypotheses, the number of simulations, the choice of summary statistics, or the acceptance threshold cannot currently be based on general rules, but the effect of these choices should be evaluated and tested in each study.<ref name=\"Bertorelle\" />\n\nA number of [[Heuristic (computer science)|heuristic approaches]] to the quality control of ABC have been proposed, such as the quantification of the fraction of parameter variance explained by the summary statistics.<ref name=\"Bertorelle\" /> A common class of methods aims at assessing whether or not the inference yields valid results, regardless of the actually observed data. For instance, given a set of parameter values, which are typically drawn from the prior or the posterior distributions for a model, one can generate a large number of artificial datasets. In this way, the quality and robustness of ABC inference can be assessed in a controlled setting, by gauging how well the chosen ABC inference method recovers the true parameter values, and also models if multiple structurally different models are considered simultaneously.\n\nAnother class of methods assesses whether the inference was successful in light of the given observed data, for example, by comparing the posterior predictive distribution of summary statistics to the summary statistics observed.<ref name=\"Bertorelle\" /> Beyond that, [[Cross-validation (statistics)|cross-validation]] techniques<ref name=\"Arlot\" /> and [[Predictive analytics|predictive checks]]<ref name=\"Dawid\" /><ref name=\"Vehtari\" /> represent promising future strategies to evaluate the stability and out-of-sample predictive validity of ABC inferences. This is particularly important when modeling large data sets, because then the posterior support of a particular model can appear overwhelmingly conclusive, even if all proposed models in fact are poor representations of the stochastic system underlying the observation data. Out-of-sample predictive checks can reveal potential systematic biases within a model and provide clues on to how to improve its structure or parametrization.\n\nFundamentally novel approaches for model choice that incorporate quality control as an integral step in the process have recently been proposed. ABC allows, by construction, estimation of the discrepancies between the observed data and the model predictions, with respect to a comprehensive set of statistics. These statistics are not necessarily the same as those used in the acceptance criterion. The resulting discrepancy distributions have been used for selecting models that are in agreement with many aspects of the data simultaneously,<ref name=\"Ratmann\" /> and model inconsistency is detected from conflicting and co-dependent summaries. Another quality-control-based method for model selection employs ABC to approximate the effective number of model parameters and the deviance of the posterior predictive distributions of summaries and parameters.<ref name=\"Francois\" /> The deviance information criterion is then used as measure of model fit. It has also been shown that the models preferred based on this criterion can conflict with those supported by [[Bayes factor]]s. For this reason, it is useful to combine different methods for model selection to obtain correct conclusions.\n\nQuality controls are achievable and indeed performed in many ABC-based works, but for certain problems, the assessment of the impact of the method-related parameters can be challenging. However, the rapidly increasing use of ABC can be expected to provide a more thorough understanding of the limitations and applicability of the method.\n\n===General risks in statistical inference exacerbated in ABC===\nThis section reviews risks that are strictly speaking not specific to ABC, but also relevant for other statistical methods as well. However, the flexibility offered by ABC to analyze very complex models makes them highly relevant to discuss here.\n\n====Prior distribution and parameter ranges====\nThe specification of the range and the prior distribution of parameters strongly benefits from previous knowledge about the properties of the system. One criticism has been that in some studies the “parameter ranges and distributions are only guessed based upon the subjective opinion of the investigators”,<ref name=\"Templeton2010\" /> which is connected to classical objections of Bayesian approaches.<ref name=\"Beaumont2010b\" />\n\nWith any computational method, it is typically necessary to constrain the investigated parameter ranges. The parameter ranges should if possible be defined based on known properties of the studied system, but may for practical applications necessitate an educated guess. However, theoretical results regarding [[Prior distribution#Uninformative priors|objective priors]] are available, which may for example be based on the [[principle of indifference]] or the [[principle of maximum entropy]].<ref name=\"Jaynes\" /><ref name=\"Berger2006\" /> On the other hand, automated or semi-automated methods for choosing a prior distribution often yield [[Prior probability#Improper priors|improper densities]]. As most ABC procedures require generating samples from the prior, improper priors are not directly applicable to ABC.\n\nOne should also keep the purpose of the analysis in mind when choosing the prior distribution. In principle, uninformative and flat priors, that exaggerate our subjective ignorance about the parameters, may still yield reasonable parameter estimates. However, Bayes factors are highly sensitive to the prior distribution of parameters. Conclusions on model choice based on Bayes factor can be misleading unless the sensitivity of conclusions to the choice of priors is carefully considered.\n\n====Small number of models====\nModel-based methods have been criticized for not exhaustively covering the hypothesis space.<ref name=\"Templeton2009a\" /> Indeed, model-based studies often revolve around a small number of models, and due to the high computational cost to evaluate a single model in some instances, it may then be difficult to cover a large part of the hypothesis space.\n\nAn upper limit to the number of considered candidate models is typically set by the substantial effort required to define the models and to choose between many alternative options.<ref name=\"Bertorelle\" /> There is no commonly accepted ABC-specific procedure for model construction, so experience and prior knowledge are used instead.<ref name=\"Csillery\" /> Although more robust procedures for ''a priori'' model choice and formulation would be beneficial, there is no one-size-fits-all strategy for model development in statistics: sensible characterization of complex systems will always necessitate a great deal of detective work and use of expert knowledge from the problem domain.\n\nSome opponents of ABC contend that since only few models—subjectively chosen and probably all wrong—can be realistically considered, ABC analyses provide only limited insight.<ref name=\"Templeton2009a\" /> However, there is an important distinction between identifying a plausible null hypothesis and assessing the relative fit of alternative hypotheses.<ref name=\"Beaumont2010\" /> Since useful null hypotheses, that potentially hold true, can extremely seldom be put forward in the context of complex models, predictive ability of statistical models as explanations of complex phenomena is far more important than the test of a statistical null hypothesis in this context. It is also common to average over the investigated models, weighted based on their relative plausibility, to infer model features (e.g., parameter values) and to make predictions.\n\n====Large datasets====\nLarge data sets may constitute a computational bottleneck for model-based methods. It was, for example, pointed out that in some ABC-based analyses, part of the data have to be omitted.<ref name=\"Templeton2009a\" /> A number of authors have argued that large data sets are not a practical limitation,<ref name=\"Bertorelle\" /><ref name=\"Beaumont2010b\" /> although the severity of this issue depends strongly on the characteristics of the models. Several aspects of a modeling problem can contribute to the computational complexity, such as the sample size, number of observed variables or features, time or spatial resolution, etc. However, with increasing computing power, this issue will potentially be less important.\n\nInstead of sampling parameters for each simulation from the prior, it has been proposed alternatively to combine the [[Metropolis–Hastings algorithm|Metropolis-Hastings algorithm]] with ABC, which was reported to result in a higher acceptance rate than for plain ABC.<ref name=\"Marjoram\" /> Naturally, such an approach inherits the general burdens of MCMC methods, such as the difficulty to assess convergence, correlation among the samples from the posterior,<ref name=\"Sisson\" /> and relatively poor parallelizability.<ref name=\"Bertorelle\" />\n\nLikewise, the ideas of [[Particle filter|sequential Monte Carlo]] (SMC) and population Monte Carlo (PMC) methods have been adapted to the ABC setting.<ref name=\"Sisson\" /><ref name=\"Beaumont2009\" /> The general idea is to iteratively approach the posterior from the prior through a sequence of target distributions. An advantage of such methods, compared to ABC-MCMC, is that the samples from the resulting posterior are independent. In addition, with sequential methods the tolerance levels must not be specified prior to the analysis, but are adjusted adaptively.<ref name=\"DelMoral\" />\n\nIt is relatively straightforward to parallelize a number of steps in ABC algorithms based on rejection sampling and [[Particle filter|sequential Monte Carlo]] methods. It has also been demonstrated that parallel algorithms may yield significant speedups for MCMC-based inference in phylogenetics,<ref name=\"Feng\" /> which may be a tractable approach also for ABC-based methods. Yet an adequate model for a complex system is very likely to require intensive computation irrespectively of the chosen method of inference, and it is up to the user to select a method that is suitable for the particular application in question.\n\n====Curse of dimensionality====\nHigh-dimensional data sets and high-dimensional parameter spaces can require an extremely large number of parameter points to be simulated in ABC-based studies to obtain a reasonable level of accuracy for the posterior inferences. In such situations, the computational cost is severely increased and may in the worst case render the computational analysis intractable. These are examples of well-known phenomena, which are usually referred to with the umbrella term [[curse of dimensionality]].<ref name=\"Bellman\" />\n\nTo assess how severely the dimensionality of a data set affects the analysis within the context of ABC, analytical formulas have been derived for the error of the ABC estimators as functions of the dimension of the summary statistics.<ref name=\"Blum10\" /><ref name=\"Fearnhead12\" /> In addition, Blum and François have investigated how the dimension of the summary statistics is related to the mean squared error for different correction adjustments to the error of ABC estimators. It was also argued that dimension reduction techniques are useful to avoid the curse-of-dimensionality, due to a potentially lower-dimensional underlying structure of summary statistics.<ref name=\"Blum10\" /> Motivated by minimizing the quadratic loss of ABC estimators, Fearnhead and Prangle have proposed a scheme to project (possibly high-dimensional) data into estimates of the parameter posterior means; these means, now having the same dimension as the parameters, are then used as summary statistics for ABC.<ref name=\"Fearnhead12\" />\n\nABC can be used to infer problems in high-dimensional parameter spaces, although one should account for the possibility of overfitting (e.g., see the model selection methods in <ref name=\"Ratmann\" /> and <ref name=\"Francois\" />). However, the probability of accepting the simulated values for the parameters under a given tolerance with the ABC rejection algorithm typically decreases exponentially with increasing dimensionality of the parameter space (due to the global acceptance criterion).<ref name=\"Csillery\" /> Although no computational method (based on ABC or not) seems to be able to break the curse-of-dimensionality, methods have recently been developed to handle high-dimensional parameter spaces under certain assumptions (e.g., based on polynomial approximation on sparse grids,<ref name=\"Gerstner\" /> which could potentially heavily reduce the simulation times for ABC). However, the applicability of such methods is problem dependent, and the difficulty of exploring parameter spaces should in general not be underestimated. For example, the introduction of deterministic global parameter estimation led to reports that the global optima obtained in several previous studies of low-dimensional problems were incorrect.<ref name=\"Singer\" /> For certain problems, it might therefore be difficult to know whether the model is incorrect or, [[#Small number of models|as discussed above]], whether the explored region of the parameter space is inappropriate.<ref name=\"Templeton2009a\" /> A more pragmatic approach is to cut the scope of the problem through model reduction.<ref name=\"Csillery\" />\n\n==Software==\nA number of software packages are currently available for application of ABC to particular classes of statistical models.\n\n{| class=\"sortable wikitable\" style=\"float: right; margin-left: 1em; text-align: center;\" id=\"table3\"\n|+Software incorporating ABC\n|-\n! Software\n! Keywords and features\n! Reference\n|-\n| [http://github.com/icb-dcm/pyabc pyABC]\n| Python framework for efficient distributed ABC-SMC (Sequential Monte Carlo).\n| <ref name=\"Klinger2017\" />\n|-\n| [http://www1.montpellier.inra.fr/CBGP/diyabc/ DIY-ABC]\n| Software for fit of genetic data to complex situations. Comparison of competing models. Parameter estimation. Computation of bias and precision measures for a given model and known parameters values.\n| <ref name=\"Cornuet08\" />\n|-\n| [https://cran.r-project.org/web/packages/abc abc<br> R package]\n| Several ABC algorithms for performing parameter estimation and model selection. Nonlinear heteroscedastic regression methods for ABC. Cross-validation tool.\n| <ref name=\"Csillery12\" /><ref>{{cite web |url= https://cran.r-project.org/web/packages/abc/vignettes/abcvignette.pdf |title=Approximate Bayesian Computation (ABC) in R: A Vignette |last1=Csillery |first1=K |last2=Francois |first2=O |last3=Blum |first3=MGB |date=2012-02-21| accessdate=10 May 2013}}</ref>\n|-\n| [https://cran.r-project.org/web/packages/EasyABC/index.html EasyABC<br> R package]\n| Several algorithms for performing efficient ABC sampling schemes, including 4 sequential sampling schemes and 3 MCMC schemes.\n| <ref>{{cite journal |title=EasyABC: performing efficient approximate Bayesian computation sampling schemes using R. |last1=Jabot |first1=F |last2=Faure |first2=T |last3=Dumoulin |first3=N |doi=10.1111/2041-210X.12050 |volume=4 |issue = 7|journal=Methods in Ecology and Evolution |pages=684–687 |year=2013}}</ref><ref>{{cite web |url= https://cran.r-project.org/web/packages/EasyABC/vignettes/EasyABC.pdf |title=EasyABC: a vignette |last1=Jabot |first1=F |last2=Faure |first2=T |last3=Dumoulin |first3=N |date=2013-06-03}}</ref>\n|-\n| [http://www.theosysbio.bio.ic.ac.uk/resources/abc-sysbio ABC-SysBio]\n| Python package. Parameter inference and model selection for dynamical systems. Combines ABC rejection sampler, ABC SMC for parameter inference, and ABC SMC for model selection. Compatible with models written in Systems Biology Markup Language (SBML). Deterministic and stochastic models.\n| <ref name=\"Liepe10\" />\n|-\n| [https://archive.is/20130219204330/http://www.cmpg.iee.unibe.ch/content/softwares__services/computer_programs/abctoolbox ABCtoolbox]\n| Open source programs for various ABC algorithms including rejection sampling, MCMC without likelihood, a particle-based sampler, and ABC-GLM. Compatibility with most simulation and summary statistics computation programs.\n| <ref name=\"Wegmann2010\" />\n|-\n| [http://msbayes.sourceforge.net/ msBayes]\n| Open source software package consisting of several C and R programs that are run with a Perl \"front-end\". Hierarchical coalescent models. Population genetic data from multiple co-distributed species.\n| <ref name=\"Hickerson07\" />\n|-\n| [https://code.google.com/p/popabc/ PopABC]\n| Software package for inference of the pattern of demographic divergence. Coalescent simulation. Bayesian model choice.\n| <ref name=\"Lopes09\" />\n|-\n| [http://genomics.jun.alaska.edu ONeSAMP]\n| Web-based program to estimate the effective population size from a sample of microsatellite genotypes. Estimates of effective population size, together with 95% credible limits.\n| <ref name=\"Tallmon08\" />\n|-\n| [https://web.archive.org/web/20100429043632/http://www.etoology.net/index.php/software/genetics/112-abc4f.html ABC4F]\n| Software for estimation of F-statistics for dominant data.\n| <ref name=\"Foll08\" />\n|-\n| [https://web.archive.org/web/20130425104634/http://compbio.igc.gulbenkian.pt/pcg/pcg_software.html#2BAD 2BAD]\n| 2-event Bayesian ADmixture. Software allowing up to two independent admixture events with up to three parental populations. Estimation of several parameters (admixture, effective sizes, etc.). Comparison of pairs of admixture models.\n| <ref name=\"Bray10\" />\n|-\n| [https://elfi.readthedocs.io/en/latest/ ELFI]\n| Engine for Likelihood-Free Inference. ELFI is a statistical software package written in Python for Approximative Bayesian Computation (ABC), also known e.g. as likelihood-free inference, simulator-based inference, approximative Bayesian inference etc.\n| <ref name=\"Kangas16\" />\n|}\nThe suitability of individual software packages depends on the specific application at hand, the computer system environment, and the algorithms required.\n\n==See also==\n* [[Markov chain Monte Carlo]]\n* [[Empirical Bayes]]\n\n==References==\n{{Academic peer reviewed\n| journal = [[PLOS Computational Biology]]\n| doi     = 10.1371/journal.pcbi.1002803\n| review  = http://journals.plos.org/ploscompbiol/article/comments?id=10.1371/journal.pcbi.1002803\n | title   = Approximate Bayesian computation\n | vauthors = Sunnåker M, Busetto AG, Numminen E, Corander J, Foll M, Dessimoz C\n | date    = 2013\n | volume  = 9\n | issue   = 1\n | pages   = e1002803\n | pmid    = 23341757\n | pmc     = 3547661\n}}\n{{reflist|35em|refs=\n<ref name=\"Beaumont2010\">{{cite journal | last1 = Beaumont | first1 = MA | year = 2010 | title = Approximate Bayesian Computation in Evolution and Ecology | url = | journal = Annual Review of Ecology, Evolution, and Systematics | volume = 41 | issue = | pages = 379–406 | doi=10.1146/annurev-ecolsys-102209-144621}}</ref>\n<ref name=\"Bertorelle\">{{cite journal | last1 = Bertorelle | first1 = G | last2 = Benazzo | first2 = A | last3 = Mona | first3 = S | year = 2010 | title = ABC as a flexible framework to estimate demography over space and time: some cons, many pros | url = | journal = Molecular Ecology | volume = 19 | issue = 13| pages = 2609–2625 | doi=10.1111/j.1365-294x.2010.04690.x| pmid = 20561199 }}</ref>\n<ref name=\"Csillery\">{{cite journal | last1 = Csilléry | first1 = K | last2 = Blum | first2 = MGB | last3 = Gaggiotti | first3 = OE | last4 = François | first4 = O | year = 2010 | title = Approximate Bayesian Computation (ABC) in practice | url = | journal = Trends in Ecology & Evolution | volume = 25 | issue = 7| pages = 410–418 | doi=10.1016/j.tree.2010.04.001| pmid = 20488578 }}</ref>\n<ref name=\"Rubin\">{{cite journal | last1 = Rubin | first1 = DB | year = 1984 | title = Bayesianly Justifiable and Relevant Frequency Calculations for the Applied Statistician | url = | journal = The Annals of Statistics | volume = 12 | issue = 4| pages = 1151–1172 | doi=10.1214/aos/1176346785}}</ref>\n<ref name=\"Marjoram\">{{cite journal | last1 = Marjoram | first1 = P | last2 = Molitor | first2 = J | last3 = Plagnol | first3 = V | last4 = Tavare | first4 = S | year = 2003 | title = Markov chain Monte Carlo without likelihoods | url = | journal = Proc Natl Acad Sci U S A | volume = 100 | issue = 26| pages = 15324–15328 | doi=10.1073/pnas.0306899100| pmid = 14663152 | pmc = 307566 | bibcode = 2003PNAS..10015324M }}</ref>\n<ref name=\"Sisson\">{{cite journal | last1 = Sisson | first1 = SA | last2 = Fan | first2 = Y | last3 = Tanaka | first3 = MM | year = 2007 | title = Sequential Monte Carlo without likelihoods | url = | journal = Proc Natl Acad Sci U S A | volume = 104 | issue = 6| pages = 1760–1765 | doi=10.1073/pnas.0607208104| pmid = 17264216 | pmc = 1794282 | bibcode = 2007PNAS..104.1760S }}</ref>\n<ref name=\"Wegmann\">{{cite journal | last1 = Wegmann | first1 = D | last2 = Leuenberger | first2 = C | last3 = Excoffier | first3 = L | year = 2009 | title = Efficient approximate Bayesian computation coupled with Markov chain Monte Carlo without likelihood | url = | journal = Genetics | volume = 182 | issue = 4| pages = 1207–1218 | doi=10.1534/genetics.109.102509| pmid = 19506307 | pmc = 2728860 }}</ref>\n<ref name=\"Templeton2008\">{{cite journal | last1 = Templeton | first1 = AR | year = 2008 | title = Nested clade analysis: an extensively validated method for strong phylogeographic inference | url = | journal = Molecular Ecology | volume = 17 | issue = 8| pages = 1877–1880 | doi=10.1111/j.1365-294x.2008.03731.x| pmid = 18346121 | pmc = 2746708}}</ref>\n<ref name=\"Templeton2009a\">{{cite journal | last1 = Templeton | first1 = AR | year = 2009 | title = Statistical hypothesis testing in intraspecific phylogeography: nested clade phylogeographical analysis vs. approximate Bayesian computation | url = | journal = Molecular Ecology | volume = 18 | issue = 2| pages = 319–331 | doi=10.1111/j.1365-294x.2008.04026.x| pmid = 19192182 | pmc = 2696056}}</ref>\n<ref name=\"Templeton2009b\">{{cite journal | last1 = Templeton | first1 = AR | year = 2009 | title = Why does a method that fails continue to be used? The answer | url = | journal = Evolution | volume = 63 | issue = 4| pages = 807–812 | doi=10.1111/j.1558-5646.2008.00600.x| pmid = 19335340 | pmc = 2693665 }}</ref>\n<ref name=\"Berger\">{{cite journal | last1 = Berger | first1 = JO | last2 = Fienberg | first2 = SE | last3 = Raftery | first3 = AE | last4 = Robert | first4 = CP | year = 2010 | title = Incoherent phylogeographic inference | url = | journal = Proceedings of the National Academy of Sciences of the United States of America | volume = 107 | issue = 41| pages = E157 | doi=10.1073/pnas.1008762107| pmid = 20870964 | bibcode = 2010PNAS..107E.157B | pmc = 2955098 }}</ref>\n<ref name=\"Didelot\">{{cite journal | last1 = Didelot | first1 = X | last2 = Everitt | first2 = RG | last3 = Johansen | first3 = AM | last4 = Lawson | first4 = DJ | year = 2011 | title = Likelihood-free estimation of model evidence | url = | journal = Bayesian Analysis | volume = 6 | issue = | pages = 49–76 | doi=10.1214/11-ba602}}</ref>\n<ref name=\"Robert\">{{cite journal | last1 = Robert | first1 = CP | last2 = Cornuet | first2 = J-M | last3 = Marin | first3 = J-M | last4 = Pillai | first4 = NS | year = 2011 | title = Lack of confidence in approximate Bayesian computation model choice | url = | journal = Proc Natl Acad Sci U S A | volume = 108 | issue = 37| pages = 15112–15117 | doi=10.1073/pnas.1102900108| pmid = 21876135 | pmc = 3174657 | bibcode = 2011PNAS..10815112R }}</ref>\n<ref name=\"Busetto2009a\">Busetto A.G., Buhmann J. Stable Bayesian Parameter Estimation for Biological Dynamical Systems.; 2009. IEEE Computer Society Press pp. 148-157.</ref>\n<!--<ref name=\"Busetto2009b\">Busetto A, Ong C, Buhmann J. Optimized Expected Information Gain for Nonlinear Dynamical Systems. Int. Conf. Proc. Series; 2009. Association for Computing Machinery (ACM) pp. 97-104.</ref>-->\n<!--<ref name=\"Jeffreys\">Jeffreys H (1961) Theory of probability: Clarendon Press, Oxford.</ref>-->\n<!--<ref name=\"Kass\">{{cite journal | last1 = Kass | first1 = R | last2 = Raftery | first2 = AE | year = 1995 | title = Bayes factors | url = | journal = Journal of the American Statistical Association | volume = 90 | issue = | pages = 773–795 | doi=10.1080/01621459.1995.10476572}}</ref>-->\n<!--<ref name=\"Vyshemirsky\">{{cite journal | last1 = Vyshemirsky | first1 = V | last2 = Girolami | first2 = MA | year = 2008 | title = Bayesian ranking of biochemical system models | url = | journal = Bioinformatics | volume = 24 | issue = | pages = 833–839 | doi=10.1093/bioinformatics/btm607}}</ref>-->\n<ref name=\"Arlot\">{{cite journal | last1 = Arlot | first1 = S | last2 = Celisse | first2 = A | year = 2010 | title = A survey of cross-validation procedures for model selection | url = | journal = Statistics Surveys | volume = 4 | issue = | pages = 40–79 | doi=10.1214/09-ss054}}</ref>\n<ref name=\"Dawid\">{{cite journal | last1 = Dawid | first1 = A | year = | title = Present position and potential developments: Some personal views: Statistical theory: The prequential approach | url = | journal = Journal of the Royal Statistical Society, Series A | volume = 1984 | issue = | pages = 278–292 }}</ref>\n<ref name=\"Vehtari\">{{cite journal | last1 = Vehtari | first1 = A | last2 = Lampinen | first2 = J | year = 2002 | title = Bayesian model assessment and comparison using cross-validation predictive densities | url = | journal = Neural Computation | volume = 14 | issue = 10| pages = 2439–2468 | doi=10.1162/08997660260293292| pmid = 12396570 | citeseerx = 10.1.1.16.3206 }}</ref>\n<ref name=\"Ratmann\">{{cite journal | last1 = Ratmann | first1 = O | last2 = Andrieu | first2 = C | last3 = Wiuf | first3 = C | last4 = Richardson | first4 = S | year = 2009 | title = Model criticism based on likelihood-free inference, with an application to protein network evolution | url = | journal = Proceedings of the National Academy of Sciences of the United States of America | volume = 106 | issue = 26| pages = 10576–10581 | doi=10.1073/pnas.0807882106| pmid = 19525398 | pmc = 2695753 | bibcode = 2009PNAS..10610576R }}</ref>\n<ref name=\"Francois\">{{cite journal | last1 = Francois | first1 = O | last2 = Laval | first2 = G | year = 2011 | title = Deviance Information Criteria for Model Selection in Approximate Bayesian Computation | url = | journal = Stat Appl Genet Mol Biol | volume = 10 | issue = | page = Article 33 | doi=10.2202/1544-6115.1678}}</ref>\n<ref name=\"Beaumont2009\">{{cite journal | last1 = Beaumont | first1 = MA | last2 = Cornuet | first2 = J-M | last3 = Marin | first3 = J-M | last4 = Robert | first4 = CP | year = 2009 | title = Adaptive approximate Bayesian computation | url = | journal = Biometrika | volume = 96 | issue = 4| pages = 983–990 | doi=10.1093/biomet/asp052| arxiv = 0805.2256 }}</ref>\n<ref name=\"DelMoral\">Del Moral P, Doucet A, Jasra A (2011) An adaptive sequential Monte Carlo method for approximate Bayesian computation. Statistics and computing.</ref>\n<ref name=\"Beaumont2002\">{{cite journal | last1 = Beaumont | first1 = MA | last2 = Zhang | first2 = W | last3 = Balding | first3 = DJ | year = 2002 | title = Approximate Bayesian Computation in Population Genetics | url = | journal = Genetics | volume = 162 | issue = | pages = 2025–2035 }}</ref>\n<ref name=\"Blum2010\">{{cite journal | last1 = Blum | first1 = M | last2 = Francois | first2 = O | year = 2010 | title = Non-linear regression models for approximate Bayesian computation | url = | journal = Stat Comp | volume = 20 | issue = | pages = 63–73 | doi=10.1007/s11222-009-9116-0| arxiv = 0809.4178 }}</ref>\n<ref name=\"Leuenberger2009\">{{cite journal | last1 = Leuenberger | first1 = C | last2 = Wegmann | first2 = D | year = 2009 | title = Bayesian Computation and Model Selection Without Likelihoods | url = | journal = Genetics | volume = 184 | issue = 1| pages = 243–252 | doi=10.1534/genetics.109.109058| pmid = 19786619 | pmc = 2815920 }}</ref>\n<ref name=\"Beaumont2010b\">{{cite journal | last1 = Beaumont | first1 = MA | last2 = Nielsen | first2 = R | last3 = Robert | first3 = C | last4 = Hey | first4 = J | last5 = Gaggiotti | first5 = O |display-authors=et al | year = 2010 | title = In defence of model-based inference in phylogeography | url = | journal = Molecular Ecology | volume = 19 | issue = 3| pages = 436–446 | doi=10.1111/j.1365-294x.2009.04515.x| pmid = 29284924 | pmc = 5743441 }}</ref>\n<!-- <ref name=\"Csillery2010\">{{cite journal | last1 = Csilléry | first1 = K | last2 = Blum | first2 = MGB | last3 = Gaggiotti | first3 = OE | last4 = Francois | first4 = O | year = 2010 | title = Invalid arguments against ABC: Reply to AR Templeton | url = | journal = Trends in Ecology & Evolution | volume = 25 | issue = | pages = 490–491 | doi=10.1016/j.tree.2010.06.011}}</ref> -->\n<ref name=\"Templeton2010\">{{cite journal | last1 = Templeton | first1 = AR | year = 2010 | title = Coherent and incoherent inference in phylogeography and human evolution | url = | journal = Proceedings of the National Academy of Sciences of the United States of America | volume = 107 | issue = 14| pages = 6376–6381 | doi=10.1073/pnas.0910647107| pmid = 20308555 | pmc = 2851988 | bibcode = 2010PNAS..107.6376T}}</ref>\n<!--<ref name=\"Fagundes\">{{cite journal | last1 = Fagundes | first1 = NJR | last2 = Ray | first2 = N | last3 = Beaumont | first3 = M | last4 = Neuenschwander | first4 = S | last5 = Salzano | first5 = FM |display-authors=et al | year = 2007 | title = Statistical evaluation of alternative models of human evolution | url = | journal = Proceedings of the National Academy of Sciences of the United States of America | volume = 104 | issue = | pages = 17614–17619 | doi=10.1073/pnas.0708280104}}</ref>-->\n<!-- <ref name=\"Gelfand\">{{cite journal | last1 = Gelfand | first1 = AE | last2 = Dey | first2 = DK | year = 1994 | title = Bayesian model choice: Asymptotics and exact calculations | url = | journal = J R Statist Soc B | volume = 56 | issue = | pages = 501–514 }}</ref> -->\n<!-- <ref name=\"Bernardo\">Bernardo JM, Smith AFM (1994) Bayesian Theory: John Wiley.</ref> -->\n<!-- <ref name=\"Box\">Box G, Draper NR (1987) Empirical Model-Building and Response Surfaces: John Wiley and Sons, Oxford.</ref> -->\n<!-- <ref name=\"Excoffier\">{{cite journal | last1 = Excoffier | first1 = L | last2 = Foll | first2 = M | year = 2011 | title = fastsimcoal: a continuous-time coalescent simulator of genomic diversity under arbitrarily complex evolutionary scenarios | url = | journal = Bioinformatics | volume = 27 | issue = | pages = 1332–1334 | doi=10.1093/bioinformatics/btr124}}</ref> -->\n<ref name=\"Wegmann2010\">{{cite journal | last1 = Wegmann | first1 = D | last2 = Leuenberger | first2 = C | last3 = Neuenschwander | first3 = S | last4 = Excoffier | first4 = L | year = 2010 | title = ABCtoolbox: a versatile toolkit for approximate Bayesian computations | url = | journal = BMC Bioinformatics | volume = 11 | issue = | page = 116 | doi=10.1186/1471-2105-11-116| pmid = 20202215 | pmc = 2848233 }}</ref>\n<!-- <ref name=\"Cornuet\">{{cite journal | last1 = Cornuet | first1 = J-M | last2 = Santos | first2 = F | last3 = Beaumont | first3 = MA | last4 = Robert | first4 = CP | last5 = Marin | first5 = J-M |display-authors=et al | year = 2008 | title = Inferring population history with DIY ABC: a user-friendly approach to approximate Bayesian computation | url = | journal = Bioinformatics | volume = 24 | issue = | pages = 2713–2719 | doi=10.1093/bioinformatics/btn514}}</ref> -->\n<!-- <ref name=\"Templeton2010b\">{{cite journal | last1 = Templeton | first1 = AR | year = 2010 | title = Coalescent-based, maximum likelihood inference in phylogeography | url = | journal = Molecular Ecology | volume = 19 | issue = | pages = 431–435 | doi=10.1111/j.1365-294x.2009.04514.x}}</ref> -->\n<ref name=\"Jaynes\">Jaynes ET (1968) Prior Probabilities. IEEE Transactions on Systems Science and Cybernetics 4.</ref>\n<ref name=\"Feng\">{{cite journal | last1 = Feng | first1 = X | last2 = Buell | first2 = DA | last3 = Rose | first3 = JR | last4 = Waddellb | first4 = PJ | year = 2003 | title = Parallel Algorithms for Bayesian Phylogenetic Inference | url = | journal = Journal of Parallel and Distributed Computing | volume = 63 | issue = 7–8| pages = 707–718 | doi=10.1016/s0743-7315(03)00079-0| citeseerx = 10.1.1.109.7764 }}</ref>\n<ref name=\"Bellman\">Bellman R (1961) Adaptive Control Processes: A Guided Tour: Princeton University Press.</ref>\n<ref name=\"Gerstner\">{{cite journal | last1 = Gerstner | first1 = T | last2 = Griebel | first2 = M | year = 2003 | title = Dimension-Adaptive Tensor-Product Quadrature | url = | journal = Computing | volume = 71 | issue = | pages = 65–87 | doi=10.1007/s00607-003-0015-5| citeseerx = 10.1.1.16.2434 }}</ref>\n<ref name=\"Singer\">{{cite journal | last1 = Singer | first1 = AB | last2 = Taylor | first2 = JW | last3 = Barton | first3 = PI | last4 = Green | first4 = WH | year = 2006 | title = Global dynamic optimization for parameter estimation in chemical kinetics | url = | journal = J Phys Chem A | volume = 110 | issue = 3| pages = 971–976 | doi=10.1021/jp0548873| pmid = 16419997 | bibcode = 2006JPCA..110..971S }}</ref>\n<ref name=\"Dean\">Dean TA, Singh SS, Jasra A, Peters GW (2011) Parameter estimation for hidden markov models with intractable likelihoods. arXiv:11035399v1 [mathST] 28 Mar 2011.</ref>\n<ref name=\"Fearnhead\">Fearnhead P, Prangle D (2011) Constructing Summary Statistics for Approximate Bayesian Computation: Semi-automatic ABC. ArXiv:10041112v2 [statME] 13 Apr 2011.</ref>\n<ref name=\"Wilkinson\">Wilkinson RD (2009) Approximate Bayesian computation (ABC) gives exact results under the assumption of model error. arXiv:08113355.</ref>\n<ref name=\"Nunes\">{{cite journal | last1 = Nunes | first1 = MA | last2 = Balding | first2 = DJ | year = 2010 | title = On optimal selection of summary statistics for approximate Bayesian computation | url = | journal = Stat Appl Genet Mol Biol | volume = 9 | issue = | page = Article 34 | doi=10.2202/1544-6115.1576| pmid = 20887273 }}</ref>\n<ref name=\"Joyce\">{{cite journal | last1 = Joyce | first1 = P | last2 = Marjoram | first2 = P | year = 2008 | title = Approximately sufficient statistics and bayesian computation | url = | journal = Stat Appl Genet Mol Biol | volume = 7 | issue = 1| page = Article 26 | doi=10.2202/1544-6115.1389| pmid = 18764775 }}</ref>\n<ref name=\"Grelaud\">{{cite journal | last1 = Grelaud | first1 = A | last2 = Marin | first2 = J-M | last3 = Robert | first3 = C | last4 = Rodolphe | first4 = F | last5 = Tally | first5 = F | year = 2009 | title = Likelihood-free methods for model choice in Gibbs random fields | url = | journal = Bayesian Analysis | volume = 3 | issue = | pages = 427–442 }}</ref>\n<ref name=\"Marin\">Marin J-M, Pillai NS, Robert CP, Rousseau J (2011) Relevant statistics for Bayesian model choice. ArXiv:11104700v1 [mathST] 21 Oct 2011: 1-24.</ref>\n<ref name=\"Toni\">{{cite journal | last1 = Toni | first1 = T | last2 = Welch | first2 = D | last3 = Strelkowa | first3 = N | last4 = Ipsen | first4 = A | last5 = Stumpf | first5 = M | year = 2007 | title = Approximate Bayesian computation scheme for parameter inference and model selection in dynamical systems | url = | journal = J R Soc Interface | volume = 6 | issue = 31| pages = 187–202 | pmid = 19205079 | pmc = 2658655 | doi = 10.1098/rsif.2008.0172 }}</ref>\n<ref name=\"Tavare\">{{cite journal | last1 = Tavaré | first1 = S | last2 = Balding | first2 = DJ | last3 = Griffiths | first3 = RC | last4 = Donnelly | first4 = P | year = 1997 | title = Inferring Coalescence Times from DNA Sequence Data | url =  | journal = Genetics | volume = 145 | issue = 2 | pages = 505–518 | pmc = 1207814 | pmid=9071603}}</ref>\n<ref name=\"Toni2010\">Toni T, Stumpf MPH (2010). Simulation-based model selection for dynamical systems in systems and population biology, ''Bioinformatics' 26 (1):104–10.</ref>\n.<ref name=\"Pritchard1999\">{{cite journal | last1 = Pritchard | first1 = JK | last2 = Seielstad | first2 = MT | last3 = Perez-Lezaun | first3 = A |display-authors=et al | year = 1999 | title = Population Growth of Human Y Chromosomes: A Study of Y Chromosome Microsatellites | url = | journal = Molecular Biology and Evolution | volume = 16 | issue = 12| pages = 1791–1798 | doi=10.1093/oxfordjournals.molbev.a026091| pmid = 10605120 }}</ref>\n<ref name=\"Diggle\">{{cite journal | last1 = Diggle | first1 = PJ | year = 1984 | title = Monte Carlo Methods of Inference for Implicit Statistical Models | url = | journal = Journal of the Royal Statistical Society, Series B | volume = 46 | issue = | pages = 193–227 }}</ref>\n<ref name=\"Hoel71\">{{cite journal | last1 = Hoel | first1 = DG | last2 = Mitchell | first2 = TJ | year = 1971 | title = The simulation, fitting and testing of a stochastic cellular proliferation model | url = | journal = Biometrics | volume = 27 | issue = 1| pages = 191–199 | doi=10.2307/2528937| jstor = 2528937 }}</ref>\n<ref name=\"Lai\">{{cite journal | last1 = Lai | first1 = K | last2 = Robertson | first2 = MJ | last3 = Schaffer | first3 = DV | year = 2004 | title = The sonic hedgehog signaling system as a bistable genetic switch | url = | journal = Biophys. J. | volume = 86 | issue = 5| pages = 2748–2757 | doi=10.1016/s0006-3495(04)74328-3 | pmid = 15111393 | bibcode=2004BpJ....86.2748L}}</ref>\n<ref name=\"Bartlett63\">{{cite journal | last1 = Bartlett | first1 = MS | year = 1963 | title = The spectral analysis of point processes | url = | journal = Journal of the Royal Statistical Society, Series B | volume = 25 | issue = | pages = 264–296 }}</ref>\n<ref name=\"Blum12\">Blum MGB, Nunes MA, Prangle D, Sisson SA (2012) A comparative review of dimension reduction methods in approximate Bayesian computation. arxiv.org/abs/1202.3819</ref>\n<ref name=\"Fearnhead12\">{{cite journal | last1 = Fearnhead | first1 = P | last2 = Prangle | first2 = D | year = 2012 | title = Constructing summary statistics for approximate Bayesian computation: semi-automatic approximate Bayesian computation | url = | journal = Journal of the Royal Statistical Society, Series B | volume = 74 | issue = 3| pages = 419–474 | doi=10.1111/j.1467-9868.2011.01010.x| citeseerx = 10.1.1.760.7753 }}</ref>\n<ref name=\"Blum10\">Blum MGB (2010) Approximate Bayesian Computation: a nonparametric perspective, ''Journal of the American Statistical Association'' (105): 1178-1187</ref>\n<!--<ref name=\"Marin11\">Jean-Michel Marin, Pierre Pudlo, Christian P. Robert and Robin J. Ryder (2011) Approximate Bayesian computational methods, Statistics and Computing</ref>-->\n<ref name=\"Cornuet08\">{{cite journal | last1 = Cornuet | first1 = J-M | last2 = Santos | first2 = F | last3 = Beaumont | first3 = M |display-authors=et al | year = 2008 | title = Inferring population history with DIY ABC: a user-friendly approach to approximate Bayesian computation | url = | journal = Bioinformatics | volume = 24 | issue = 23| pages = 2713–2719 | doi=10.1093/bioinformatics/btn514| pmid = 18842597 | pmc = 2639274 }}</ref>\n<ref name=\"Csillery12\">{{cite journal | last1 = Csilléry | first1 = K | last2 = François | first2 = O | last3 = Blum | first3 = MGB | year = 2012 | title = abc: an R package for approximate Bayesian computation (ABC) | url = | journal = Methods in Ecology and Evolution | volume = 3 | issue = 3| pages = 475–479 | doi=10.1111/j.2041-210x.2011.00179.x| arxiv = 1106.2793 }}</ref>\n<ref name=\"Liepe10\">{{cite journal | last1 = Liepe | first1 = J | last2 = Barnes | first2 = C | last3 = Cule | first3 = E | last4 = Erguler | first4 = K | last5 = Kirk | first5 = P | last6 = Toni | first6 = T | last7 = Stumpf | first7 = MP | year = 2010 | title = ABC-SysBio—approximate Bayesian computation in Python with GPU support | url = | journal = Bioinformatics | volume = 26 | issue = 14| pages = 1797–1799 | doi=10.1093/bioinformatics/btq278| pmid = 20591907 | pmc = 2894518 }}</ref>\n<ref name=\"Hickerson07\">{{cite journal | last1 = Hickerson | first1 = MJ | last2 = Stahl | first2 = E | last3 = Takebayashi | first3 = N | year = 2007 | title = msBayes: Pipeline for testing comparative phylogeographic histories using hierarchical approximate Bayesian computation | url = | journal = BMC Bioinformatics | volume = 8 | issue = 268| pages = 1471–2105 | doi=10.1186/1471-2105-8-268| pmid = 17655753 | pmc = 1949838 }}</ref>\n<ref name=\"Lopes09\">{{cite journal | last1 = Lopes | first1 = JS | last2 = Balding | first2 = D | last3 = Beaumont | first3 = MA | year = 2009 | title = PopABC: a program to infer historical demographic parameters | url = | journal = Bioinformatics | volume = 25 | issue = 20| pages = 2747–2749 | doi=10.1093/bioinformatics/btp487| pmid = 19679678 }}</ref>\n<ref name=\"Tallmon08\">{{cite journal | last1 = Tallmon | first1 = DA | last2 = Koyuk | first2 = A | last3 = Luikart | first3 = G | last4 = Beaumont | first4 = MA | year = 2008 | title = COMPUTER PROGRAMS: onesamp: a program to estimate effective population size using approximate Bayesian computation | url = | journal = Molecular Ecology Resources | volume = 8 | issue = 2| pages = 299–301 | doi=10.1111/j.1471-8286.2007.01997.x| pmid = 21585773 }}</ref>\n<ref name=\"Foll08\">{{cite journal | last1 = Foll | first1 = M | last2 = Baumont | first2 = MA | last3 = Gaggiotti | first3 = OE | year = 2008 | title = An Approximate Bayesian Computation approach to overcome biases that arise when using AFLP markers to study population structure | url = | journal = Genetics | volume = 179 | issue = 2| pages = 927–939 | doi=10.1534/genetics.107.084541| pmid = 18505879 | pmc = 2429886 }}</ref>\n<ref name=\"Bray10\">{{cite journal | last1 = Bray | first1 = TC | last2 = Sousa | first2 = VC | last3 = Parreira | first3 = B | last4 = Bruford | first4 = MW | last5 = Chikhi | first5 = L | year = 2010 | title = 2BAD: an application to estimate the parental contributions during two independent admisture events | url = | journal = Molecular Ecology Resources | volume = 10 | issue = 3| pages = 538–541 | doi=10.1111/j.1755-0998.2009.02766.x| pmid = 21565053 }}</ref>\n<ref name=\"Marin11\">{{cite journal | last1 = Marin | first1 = JM | last2 = Pudlo | first2 = P | last3 = Robert | first3 = CP | last4 = Ryder | first4 = RJ | year = 2012 | title = Approximate Bayesian computational methods | url = | journal = Statistics and Computing | volume = 22 | issue = 6| pages = 1167–1180 | doi=10.1007/s11222-011-9288-2| arxiv = 1101.0955 }}</ref>\n<ref name=\"Wilkinson2007\">Wilkinson, R. G. (2007). Bayesian Estimation of Primate Divergence Times, Ph.D. thesis, University of Cambridge.</ref>\n<ref name=\"Berger2006\">{{cite journal |last= Berger |first= J.O. |year= 2006 |title= The case for objective Bayesian analysis |url= |journal= Bayesian Analysis |volume= 1 |issue= pages 385–402 and 457–464 |pages= 385–402 |doi= 10.1214/06-BA115 }}</ref>\n<ref name=\"Kangas16\">{{cite journal |last1= Kangasrääsiö |first1= Antti |last2= Lintusaari |first2= Jarno |last3= Skytén |first3= Kusti |last4= Järvenpää |first4= Marko |last5= Vuollekoski |first5= Henri |last6= Gutmann |first6= Michael |last7= Vehtari |first7= Aki |last8= Corander |first8= Jukka |last9= Kaski |first9= Samuel|year= 2016 |title= ELFI: Engine for Likelihood-Free Inference |url=http://approximateinference.org/accepted/KangasraasioEtAl2016.pdf |journal= NIPS 2016 Workshop on Advances in Approximate Bayesian Inference|bibcode= 2017arXiv170800707L |arxiv= 1708.00707 }}</ref>\n<ref name=\"Klinger2017\">Klinger, E.; Rickert, D.; Hasenauer, J. (2017). pyABC: distributed, likelihood-free inference.</ref>\n}}\n\n==External links==\n* {{cite web |url= http://darrenjw.wordpress.com/2013/03/31/introduction-to-approximate-bayesian-computation-abc/| title=Introduction to Approximate Bayesian Computation| author=Darren Wilkinson| date=March 31, 2013|accessdate=2013-03-31}}\n* {{cite web |url= http://www.sumsar.net/blog/2014/10/tiny-data-and-the-socks-of-karl-broman/| title=Tiny Data, Approximate Bayesian Computation and the Socks of Karl Broman| author= Rasmus Bååth| date=October 20, 2014|accessdate=2015-01-22}}\n\n{{DEFAULTSORT:Approximate Bayesian Computation}}\n[[Category:Bayesian statistics]]\n[[Category:Statistical approximations]]"
    },
    {
      "title": "Binomial proportion confidence interval",
      "url": "https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval",
      "text": "In [[statistics]], a '''binomial proportion confidence interval''' is a [[confidence interval]] for the probability of success calculated from the outcome of a series of success–failure experiments ([[Bernoulli trial|Bernoulli trials]]). In other words, a binomial proportion confidence interval is an interval estimate of a success probability ''p'' when only the number of experiments ''n'' and the number of successes ''n<sub>S</sub>'' are known. \n\nThere are several formulas for a binomial confidence interval, but all of them rely on the assumption of a [[binomial distribution]]. In general, a binomial distribution applies when an experiment is repeated a fixed number of times, each trial of the experiment has two possible outcomes (success and failure), the probability of success is the same for each trial, and the trials are [[statistically independent]]. Because the binomial distribution is a [[discrete probability distribution]] (i.e., not continuous) and difficult to calculate for large numbers of trials, a variety of approximations are used to calculate this confidence interval, all with their own tradeoffs in accuracy and computational intensity.\n\nA simple example of a binomial distribution is the set of various possible outcomes, and their probabilities, for the number of heads observed when a [[Coin flipping|coin is flipped]] ten times. The observed binomial proportion is the fraction of the flips that turn out to be heads. Given this observed proportion, the confidence interval for the true probability of the coin landing on heads is a range of possible proportions, which may or may not contain the true proportion. A 95% confidence interval for the proportion, for instance, will contain the true proportion 95% of the times that the procedure for constructing the confidence interval is employed.<ref name=\"Sullivan\">{{Cite web|url=http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Confidence_Intervals/|title=Confidence Intervals|last1=Sullivan|first1=Lisa|date=2017-10-27|website=Boston University School of Public Health|archive-url=|archive-date=|dead-url=|access-date=}}</ref>\n\n==Normal approximation interval==\n\nA commonly used formula for a binomial confidence interval relies on approximating the distribution of error about a binomially-distributed observation, <math>\\hat p</math>, with a [[normal distribution]].<ref name=Wallis2013>{{Cite journal\n | last1 = Wallis\n | first1 = Sean A.\n | title = Binomial confidence intervals and contingency tests: mathematical fundamentals and the evaluation of alternative methods\n | journal = Journal of Quantitative Linguistics\n | volume = 20\n | issue = 3\n | pages = 178–208\n | year = 2013\n | doi = 10.1080/09296174.2013.799918\n | url = http://www.ucl.ac.uk/english-usage/staff/sean/resources/binomialpoisson.pdf\n}}</ref> This approximation is based on the [[central limit theorem]] and is unreliable when the sample size is small or the success probability is close to 0 or 1.<ref name=\"Brown2001\">{{Cite journal\n | last1 = Brown\n | first1 = Lawrence D.\n | authorlink = Lawrence D. Brown\n | last2 = Cai\n | first2 = T. Tony\n | authorlink2 = T. Tony Cai\n | last3 = DasGupta\n | first3 = Anirban\n | title = Interval Estimation for a Binomial Proportion\n | journal = Statistical Science\n | volume = 16\n | issue = 2\n | pages = 101–133\n | year = 2001\n | doi = 10.1214/ss/1009213286\n | mr = 1861069\n | zbl = 1059.62533\n| citeseerx = 10.1.1.50.3025\n }}</ref>\n\nUsing the normal approximation, the success probability ''p'' is estimated as\n\n: <math>\\hat p \\pm z \\sqrt{\\frac{\\hat p \\left(1 - \\hat p\\right)}{n}},</math>\n\nor the equivalent\n\n: <math>\\frac{n_S}{n} \\pm \\frac{z}{n} \\sqrt{\\frac{n_S n_F}{n}},</math>\n\nwhere <math>\\hat p = n_S / n</math> is the proportion of successes in a [[Bernoulli trial]] process, measured with <math>n</math> trials yielding <math>n_S</math> successes and <math>n_F = n - n_S</math> failures, and <math>z</math> is the <math>1 - \\tfrac{\\alpha}{2}</math> [[quantile]] of a [[standard normal distribution]] (i.e., the [[probit]]) corresponding to the target error rate <math>\\alpha</math>. For a 95% confidence level, the error <math>\\alpha=1-0.95=0.05</math>, so <math>1 - \\tfrac \\alpha 2=0.975</math> and <math>z=1.96</math>.\n\nAn important theoretical derivation of this confidence interval involves the inversion of a hypothesis test. Under this formulation, the confidence interval represents those values of the population parameter that would have large ''p''-values if they were tested as a hypothesized [[population proportion]]. The collection of values, <math>\\theta</math>, for which the normal approximation is valid can be represented as\n\n: <math>\\left\\{ \\theta \\,\\,\\bigg|\\,\\, y \\le \\frac{\\hat p - \\theta}{\\sqrt{\\frac{1}{n} \\hat p \\left(1 - \\hat p\\right)}} \\le z_ \\tfrac \\alpha 2 \\right\\},</math>\n\nwhere <math>y</math> is the <math>\\tfrac \\alpha 2</math> [[quantile]] of a [[standard normal distribution]]. Since the test in the middle of the inequality is a [[Wald test]], the normal approximation interval is sometimes called the [[Abraham Wald|Wald]] interval, but it was first described by [[Pierre-Simon Laplace]] in 1812.<ref>{{Cite book|url=https://books.google.com/books?id=ooBLvgAACAAJ|title=Théorie analytique des probabilités|last=Laplace|first=Pierre Simon|year=1812|isbn=|location=|pages=283|language=fr}}</ref>\n\n==Wilson score interval==\n\nThe Wilson score interval is an improvement over the normal approximation interval in that the actual [[coverage probability]] is closer to the nominal value. It was developed by [[Edwin Bidwell Wilson]] (1927).<ref name=Wilson1927>\n{{Cite journal\n | last1 = Wilson\n | first1 = E. B.\n | authorlink1 = Edwin Bidwell Wilson\n | title = Probable inference, the law of succession, and statistical inference\n | journal = Journal of the American Statistical Association\n | volume = 22\n | issue = 158\n | pages = 209–212\n | year = 1927\n |jstor = 2276774\n | doi = 10.1080/01621459.1927.10502953\n}}</ref>\n\nWilson gave the confidence limits as solutions of both equations\n:<math>\np=\\hat{p}\\pm z\\sqrt{\\frac{p\\left(1-p\\right)}{n}}\\qquad\\Longrightarrow\\qquad\\left(p-\\hat{p}\\right)^{2}=z^{2}\\cdot\\frac{p\\left(1-p\\right)}{n}\n</math>\nafter transforming it into quadratic equations. Hence the success probability ''p'' is estimated as\n\n:<math>\n  \\frac{\\hat p+\\frac{z^2}{2n}}{1+\\frac{z^2}{n}}\n\\pm\n\\frac{z}{1+\\frac{z^2}{n}}\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}+\\frac{z^2}{4n^2}}\n</math>\n\nor the equivalent\n\n:<math>\n  \\frac{n_S+\\frac{z^2}{2}}{n+z^2}\n\\pm \\frac{z}{n + z^2}\n\\sqrt{\n  \\frac{n_S n_F}{n} +\n  \\frac{z^2}{4}\n}. \n</math>\n\nThis interval has good properties even for a small number of trials and/or an extreme probability.\n\nIntuitively, the center value of this interval is the weighted average of <math>\\hat{p}</math> and <math>\\tfrac{1}{2}</math>, with <math>\\hat{p}</math> receiving greater weight as the sample size increases. Formally, the center value corresponds to using a [[pseudocount]] of {{tmath|z^2/2}}, the number of standard deviations of the confidence interval: add this number to both the count of successes and of failures to yield the estimate of the ratio. For the common two standard deviations in each direction interval (approximately 95% coverage, which itself is approximately 1.96 standard deviations), this yields the estimate <math>(n_S+2)/(n+4)</math>, which is known as the \"plus four rule\".\n\nIn most cases Wilson's equations can be solved numerically using the fixed-point iteration\n:<math>\np_{n+1}=\\hat{p}\\pm z\\cdot\\sqrt{\\frac{p_{n}\\cdot\\left(1-p_{n}\\right)}{n}}\n</math>\nwith <math>p_0=\\hat{p}</math>.\n\nThe Wilson interval can be derived from [[Pearson's chi-squared test]] with two categories. The resulting interval,\n\n:<math>\\left\\{\n  \\theta \\,\\,\\bigg|\\,\\, y \\le\n  \\frac{\\hat{p} - \\theta}{\\sqrt{\\frac{1}{n} \\theta(1 - \\theta)}} \\le\n  z\n\\right\\},</math>\n\ncan then be solved for <math>\\theta</math> to produce the Wilson score interval. The test in the middle of the inequality is a [[score test]].\n\n===Wilson score interval with continuity correction===\n\nThe Wilson interval may be modified by employing a [[continuity correction]], in order to align the minimum [[coverage probability]], rather than the average probability, with the nominal value.\n\nJust as the Wilson interval mirrors [[Pearson's chi-squared test]], the Wilson interval with continuity correction mirrors the equivalent [[Yates's correction for continuity|Yates' chi-squared test]].\n\nThe following formulae for the lower and upper bounds of the Wilson score interval with continuity correction <math>( w^- , w^+ )</math> are derived from Newcombe (1998).<ref name=New/>\n\n:<math>\\begin{align}\n  w^- &= \\max\\left\\{\n           0, \\frac { 2n\\hat{p} + z^2 - \\left[z \\sqrt{z^2 - \\frac{1}{n} + 4n\\hat{p}(1 - \\hat{p}) + (4\\hat{p} - 2)} + 1\\right] } { 2(n + z^2) }\n         \\right\\} \\\\\n  w^+ &= \\min\\left\\{\n           1, \\frac { 2n\\hat{p} + z^2 + \\left[z \\sqrt{z^2 - \\frac{1}{n} + 4n\\hat{p}(1 - \\hat{p}) - (4\\hat{p} - 2)} + 1\\right] } { 2(n + z^2) }\n         \\right\\}\n\\end{align}</math>\n\nHowever, if ''p''&nbsp;=&nbsp;0, <math>w^-</math> must be taken as 0; if ''p''&nbsp;=&nbsp;1, <math>w^+</math> is then&nbsp;1.\n\n==Jeffreys interval==\nThe ''Jeffreys interval'' has a Bayesian derivation, but it has good frequentist properties.  In particular, it has coverage properties that are similar to those of the Wilson interval, but it is one of the few intervals with the advantage of being ''equal-tailed'' (e.g., for a 95% confidence interval, the probabilities of the interval lying above or below the true value are both close to 2.5%).  In contrast, the Wilson interval has a systematic bias such that it is centred too close to ''p'' = 0.5.<ref>{{cite journal | last1 = Cai | first1 = TT | year = 2005 | title = One-sided confidence intervals in discrete distributions | url = | journal = Journal of Statistical Planning and Inference | volume = 131 | issue = | pages = 63–88 | doi=10.1016/j.jspi.2004.01.005}}</ref>\n\nThe Jeffreys interval is the Bayesian [[credible interval]] obtained when using the [[non-informative prior|non-informative]] [[Jeffreys prior]] for the binomial proportion {{math|''p''}}. The [[Jeffreys prior#Bernoulli trial|Jeffreys prior for this problem]] is a [[Beta distribution]] with parameters {{math|(1/2,&nbsp;1/2)}}. After observing {{math|''x''}} successes in {{math|''n''}} trials, the [[posterior distribution]] for {{math|''p''}} is a Beta distribution with parameters {{math|(''x''&nbsp;+&nbsp;1/2,&nbsp;''n''&nbsp;–&nbsp;''x''&nbsp;+&nbsp;1/2)}}.\n\nWhen {{math|''x''&nbsp;≠0}} and {{math|''x''&nbsp;≠&nbsp;''n''}}, the Jeffreys interval is taken to be the {{math|100(1&nbsp;–&nbsp;''α'')%}} equal-tailed posterior probability interval, i.e., the {{math|''α''&thinsp;/&thinsp;2}} and {{math|1&nbsp;–&nbsp;''α''&thinsp;/&thinsp;2}} quantiles of a Beta distribution with parameters {{math|(''x''&nbsp;+&nbsp;1/2,&nbsp;''n''&nbsp;–&nbsp;''x''&nbsp;+&nbsp;1/2)}}. These quantiles need to be computed numerically, although this is reasonably simple with modern statistical software.\n\nIn order to avoid the coverage probability tending to zero when {{math|''p''&nbsp;→&nbsp;0}} or {{math|1}}, when {{math|''x''&nbsp;{{=}}&nbsp;0}} the upper limit is calculated as before but the lower limit is set to 0, and when {{math|''x''&nbsp;{{=}}&nbsp;''n''}} the lower limit is calculated as before but the upper limit is set to 1.<ref name=Brown2001/>\n\n==Clopper–Pearson interval==\n\nThe Clopper–Pearson interval is an early and very common method for calculating binomial confidence intervals.<ref>\n{{Cite journal\n | last1 = Clopper\n | first1 = C.\n | last2 = Pearson\n | first2 = E. S.\n | authorlink2 = Egon Pearson\n | title = The use of confidence or fiducial limits illustrated in the case of the binomial\n | journal = Biometrika\n | volume = 26\n | issue = 4\n | pages = 404–413\n | year = 1934\n | doi = 10.1093/biomet/26.4.404 \n}}</ref> This is often called an 'exact' method, but that is because it is based on the cumulative probabilities of the binomial distribution (i.e., exactly the correct distribution rather than an approximation), but the intervals are not exact in the way that one might assume: the discontinuous nature of the binomial distribution precludes any interval with exact coverage for all population proportions. The Clopper–Pearson interval can be written as\n\n:<math>\nS_{\\le} \\cap S_{\\ge}\n</math>\n\nor equivalently,\n:<math>\n( \\inf S_{\\ge}\\,,\\, \\sup S_{\\le} )\n</math>\n\nwith\n\n:<math>\nS_{\\le} := \\left\\{ \\theta \\,\\,\\Big|\\,\\, P \\left[ \\operatorname{Bin}\\left( n; \\theta \\right) \\le x \\right] > \\frac{\\alpha}{2} \\right\\}\n\\text{ and }\nS_{\\ge} := \\left\\{ \\theta \\,\\,\\Big|\\,\\, P \\left[ \\operatorname{Bin}\\left( n; \\theta \\right) \\ge x \\right] > \\frac{\\alpha}{2} \\right\\}, \n</math>\n\nwhere 0 ≤ ''x'' ≤ ''n'' is the number of successes observed in the sample and Bin(''n'';&nbsp;''θ'') is a binomial random variable with ''n'' trials and probability of success&nbsp;''θ''.\n\nEquivalently we can say that the Clopper–Pearson interval is <math>(x/n-\\varepsilon_1,\\ x/n+\\varepsilon_2)</math> with confidence level <math>1-\\alpha</math> if <math>\\varepsilon_i</math> is the infimum of those such that the following tests of hypothesis succeed with significance <math>\\alpha/2</math>:\n\n1.    H<sub>0</sub>: <math>\\theta=x/n-\\varepsilon_1</math> with H<sub>A</sub>: <math>\\theta>x/n-\\varepsilon_1</math>\n\n2.    H<sub>0</sub>: <math>\\theta=x/n+\\varepsilon_2</math> with H<sub>A</sub>: <math>\\theta<x/n+\\varepsilon_2</math>.\n<br>\n<br>\n\nBecause of a relationship between the binomial distribution and the [[beta distribution]], the Clopper–Pearson interval is sometimes presented in an alternate format that uses quantiles from the beta distribution.\n\n:<math>B\\left(\\frac{\\alpha}{2}; x, n - x + 1\\right) < \\theta <  B\\left(1 - \\frac{\\alpha}{2}; x + 1, n - x\\right)</math>\n\nwhere ''x'' is the number of successes, ''n'' is the number of trials, and ''B''(''p''; ''v'',''w'') is the ''p''th [[Cumulative distribution function#Inverse distribution function .28quantile function.29|quantile]] from a beta distribution with shape parameters ''v'' and ''w''.\n\nWhen <math>x</math> is either <math>0</math> or <math>n</math>, closed-form expressions for the interval bounds are available: when <math>x=0</math> the interval is <math>\\left(0, 1-\\left(\\frac{\\alpha}{2}\\right)^{1/n} \\right)</math> and when <math>x=n</math> it is <math>\\left(\\left(\\frac{\\alpha}{2}\\right)^{1/n} , 1\\right)</math>.<ref>{{Cite journal|last=Thulin|first=Måns|date=2014-01-01|title=The cost of using exact confidence intervals for a binomial proportion|journal=Electronic Journal of Statistics|language=EN|volume=8|issue=1|pages=817–840|doi=10.1214/14-EJS909|issn=1935-7524|arxiv=1303.1288}}</ref>\n\nThe beta distribution is, in turn, related to the [[F-distribution]] so a third formulation of the Clopper–Pearson interval can be written using F quantiles:\n\n:<math>\n  \\left( 1 + \\frac{n - x+1}{x\\,F\\!\\left[\\frac{\\alpha}{2}; 2x, 2(n - x+1)\\right]} \\right)^{-1}<\n  \\theta <\n  \\left( 1 + \\frac{n - x}{(x+1)\\,\\,F\\!\\left[1 - \\frac{\\alpha}{2}; 2(x+1), 2(n - x)\\right]} \\right)^{-1}\n\n</math>\n\nwhere ''x'' is the number of successes, ''n'' is the number of trials, and ''F''(''c''; ''d''<sub>1</sub>, ''d''<sub>2</sub>) is the ''c'' quantile from an F-distribution with ''d''<sub>1</sub> and ''d''<sub>2</sub> degrees of freedom.<ref name=AgrestiCoull1998 />\n\nThe Clopper–Pearson interval is an exact interval since it is based directly on the binomial distribution rather than any approximation to the binomial distribution. This interval never has less than the nominal coverage for any population proportion, but that means that it is usually conservative. For example, the true coverage rate of a 95% Clopper–Pearson interval may be well above 95%, depending on ''n'' and&nbsp;''θ''.  Thus the interval may be wider than it needs to be to achieve 95% confidence.  In contrast, it is worth noting that other confidence bounds may be narrower than their nominal confidence width, i.e., the normal approximation (or \"standard\") interval, Wilson interval,<ref name=Wilson1927/> Agresti–Coull interval,<ref name=AgrestiCoull1998>\n{{Cite journal\n | last1 = Agresti\n | first1 = Alan\n | last2 = Coull\n | first2 = Brent A.\n | title = Approximate is better than 'exact' for interval estimation of binomial proportions\n | journal = The American Statistician\n | volume = 52\n | issue = 2\n | pages = 119–126\n | year = 1998\n | mr = 1628435\n | jstor = 2685469\n | doi=10.2307/2685469\n}}</ref> etc., with a nominal coverage of 95% may in fact cover less than 95%.<ref name=Brown2001/>\n\n==Agresti–Coull interval==\n\nThe Agresti–Coull interval is also another approximate binomial confidence interval.<ref name=AgrestiCoull1998/>\n\nGiven <math>X</math> successes in <math>n</math> trials, define\n:<math>\\tilde{n} = n + z^2</math>\n\nand\n:<math>\\tilde{p} = \\frac{1}{\\tilde{n}}\\left(X + \\frac{z^2}{2}\\right)</math>\n\nThen, a confidence interval for <math>p</math> is given by\n\n:<math>\n  \\tilde{p} \\pm z\n    \\sqrt{\\frac{\\tilde{p}}{\\tilde{n}}\\left(1 - \\tilde{p} \\right)}\n</math>\n\nwhere <math>z</math> is the <math>1 - \\frac{\\alpha}{2}</math> quantile of a standard normal distribution, as before.  For example, for a 95% confidence interval, let <math>\\alpha = 0.05</math>, so <math>z = 1.96</math> and <math>z^2 = 3.84</math>. If we use 2 instead of 1.96 for <math>z</math>, this is the \"add 2 successes and 2 failures\" interval in Agresti and Coull's 1998 paper \"Approximate is Better than 'Exact' for Interval Estimation of Binomial Proportions.\" <ref name=AgrestiCoull1998/>\n\nThis interval can be summarised as employing the centre-point adjustment, <math>\\tilde{p}</math>, of the Wilson score interval, and then applying the Normal approximation to this point.<ref name=\"Wallis2013\" /><ref name=\"Brown2001\" /> \n\n:<math>\n  \\tilde{p} = \\frac{\\hat p+\\frac{z^2}{2n}}{1+\\frac{z^2}{n}}\n</math>\n\n==Arcsine transformation==\n{{details|Cohen's h}}\n\nLet ''X'' be the number of successes in ''n'' trials and let ''p'' = ''X''/''n''. The variance of ''p'' is\n\n: <math> \\operatorname{var}(p) = \\frac{p (1 - p)}{n} .</math>\n\nUsing the arc sine transform the variance of the arcsine of ''p''<sup>1/2</sup> is<ref name=Shao1998>Shao J (1998) Mathematical statistics. Springer. New York, New York, USA</ref>\n\n: <math> \\operatorname{var}\\left(\\arcsin(\\sqrt{p})\\right) \\approx \\frac{\\operatorname{var}(p)}{4p (1 - p)} = \\frac{p (1 - p)}{4n p(1 - p)} = \\frac{1}{4n} .</math>\n\nSo, the confidence interval itself has the following form:\n\n: <math> \\sin^2\\left(\\arcsin(\\sqrt{p}) - \\frac{z}{2\\sqrt{n}}\\right) < \\theta < \\sin^2\\left(\\arcsin(\\sqrt{p}) + \\frac{z}{2\\sqrt{n}}\\right) </math>\n\nwhere <math>z</math> is the <math>\\scriptstyle 1 \\,-\\, \\frac{\\alpha}{2}</math> quantile of a standard normal distribution.\n\nThis method may be used to estimate the variance of ''p'' but its use is problematic when ''p'' is close to 0 or&nbsp;1.\n\n==''t''<sub>''a''</sub> transform==\n{{unreferenced section|date=July 2017}}\nLet ''p'' be the proportion of successes.  For 0 ≤ ''a'' ≤ 2,\n\n: <math> t_a = \\log\\left( \\frac{p^a}{ (1 - p)^{2-a} } \\right) = a \\log(p) - (2-a) \\log(1-p) </math>\n\nThis family is a generalisation of the logit transform which is a special case with ''a'' = 1 and can be used to transform a proportional data distribution to an approximately [[normal distribution]]. The parameter ''a'' has to be estimated for the data set.\n\n==Special cases==\nThe [[Rule of three (statistics)|rule of three]] is  used to provide a simple way of stating an approximate 95% confidence interval for ''p'', in the special case that no successes (<math>\\hat p = 0</math>) have been observed.<ref>Steve Simon (2010) [http://www.pmean.com/01/zeroevents.html \"Confidence interval with zero events\"], The Children's Mercy Hospital, Kansas City, Mo. (website: \"Ask Professor Mean at [http://www.childrensmercy.org/stats/ Stats topics or Medical Research] {{webarchive |url=https://web.archive.org/web/20111015182854/http://www.childrensmercy.org/stats/ |date=October 15, 2011 }})</ref> The interval is {{nowrap|(0,3/''n'')}}.\n\nBy symmetry, one could expect for only successes (<math>\\hat p = 1</math>), the interval is {{nowrap|(1&nbsp;−&nbsp;3/''n'',1)}}.\n\n==Comparison of different intervals==\n\nThere are several research papers that compare these and other confidence intervals for the binomial proportion.<ref name=Wallis2013/><ref name=New/><ref name=Rei/><ref name=SL/>  Both Agresti and Coull (1998)<ref name=AgrestiCoull1998/> and Ross (2003)<ref name=Ross/> point out that exact methods such as the Clopper–Pearson interval may not work as well as certain approximations. \n\nOf the approximations listed above, Wilson score interval methods (with or without continuity correction) have been shown to be the most accurate and the most robust.<ref name=Wallis2013/><ref name=\"Brown2001\" /><ref name=New/>\n\nMany of these intervals can be calculated in [[R (programming language)|R]] using packages like {{cite web |url=https://cran.r-project.org/web/packages/binom/index.html |title=binom}}.\n\n==See also==\n* [[Estimation theory]]\n* [[Pseudocount]]\n\n==References==\n{{Reflist|refs=\n\n<ref name=New>{{Cite journal| pmc =   \n| title = Two-sided confidence intervals for the single proportion: comparison of seven methods\n| year = 1998\n| issue = 8 \n| pmid =  9595616 \n| doi = 10.1002/(SICI)1097-0258(19980430)17:8<857::AID-SIM777>3.0.CO;2-E \n| pages = 857–872\n| last1 = Newcombe | first1 = R. G.\n| volume = 17 \n|journal = [[Statistics in Medicine (journal)|Statistics in Medicine]] \n}}</ref>\n<ref name=Rei>{{cite journal | last1 = Reiczigel | first1 = J | year = 2003 | title = Confidence intervals for the binomial parameter: some new considerations | url = http://www.zoologia.hu/qp/Reiczigel_conf_int.pdf | journal = Statistics in Medicine | volume = 22 | issue = 4| pages = 611–621 | doi=10.1002/sim.1320}}</ref>\n<ref name=SL>Sauro J., Lewis J.R. (2005) [http://www.measuringusability.com/papers/sauro-lewisHFES.pdf \"Comparison of Wald, Adj-Wald, Exact and Wilson intervals Calculator\"] {{Webarchive|url=https://web.archive.org/web/20120618053914/http://www.measuringusability.com/papers/sauro-lewisHFES.pdf |date=2012-06-18 }}. ''Proceedings of the Human Factors and Ergonomics Society, 49th Annual Meeting (HFES 2005)'', Orlando, FL, pp. 2100–2104</ref>\n<ref name=Ross>{{Cite journal\n | last1 = Ross   | first1 = T. D.\n | title = Accurate confidence intervals for binomial proportion and Poisson rate estimation\n | journal = Computers in Biology and Medicine\n | volume = 33\n | issue = 6\n | pages = 509–531\n | year = 2003\n | doi = 10.1016/S0010-4825(03)00019-2 | url = https://zenodo.org/record/1259565\n }}\n</ref>\n\n}}\n\n{{DEFAULTSORT:Binomial Proportion Confidence Interval}}\n[[Category:Statistical approximations]]\n[[Category:Statistical intervals]]"
    },
    {
      "title": "Delta method",
      "url": "https://en.wikipedia.org/wiki/Delta_method",
      "text": "In [[statistics]], the '''delta method''' is a result concerning the approximate [[probability distribution]] for a [[function (mathematics)|function]] of an [[Asymptotic distribution|asymptotically normal]] statistical [[estimator]] from knowledge of the limiting [[variance]] of that estimator.\n\n== History ==\nThe delta method was derived from [[propagation of error]], and the idea behind was known in the early 19th century.<ref>{{cite journal \n|last=Portnoy\n|first=Stephen\n|year=2013\n|title=Letter to the Editor\n|journal=[[The American Statistician]] \n|volume=67 |number=3 |pages=190-190 |doi=10.1080/00031305.2013.820668}}</ref> Its statistical application can be traced as far back as 1928 by [[Truman Lee Kelley|T. L. Kelley]].<ref>{{cite book \n|last=Kelley\n|first=Truman L. \n|year=1928\n|title=Crossroads in the Mind of Man: A Study of Differentiable Mental Abilities \n|pages=49-50 |ISBN=978-1-4338-0048-1}}</ref> A formal description of the method was presented by [[J. L. Doob]] in 1935.<ref>{{cite journal \n|jstor=2957546\n|last=Doob\n|first=J. L. \n|year=1935\n|title=The Limiting Distributions of Certain Statistics\n|journal=[[Annals of Mathematical Statistics]] \n|volume=6 |pages=160-169}}</ref> [[Robert Dorfman]] also described a version of it in 1938.<ref>{{cite journal |jstor=23339471 |last=Ver Hoef |first=J. M. |year=2012 |title=Who invented the delta method? |journal=[[The American Statistician]] |volume=66 |issue=2 |pages=124–127 }}</ref>\n\n==Univariate delta method==\nWhile the delta method generalizes easily to a multivariate setting, careful motivation of the technique is more easily demonstrated in univariate terms. Roughly, if there is a [[sequence (mathematics)|sequence]] of random variables {{mvar|X<sub>n</sub>}} satisfying\n \n:<math>{\\sqrt{n}[X_n-\\theta]\\,\\xrightarrow{D}\\,\\mathcal{N}(0,\\sigma^2)},</math>\n\nwhere ''θ'' and ''σ''<sup>2</sup> are finite valued constants and <math>\\xrightarrow{D}</math> denotes [[convergence in distribution]], then\n\n:<math>{\\sqrt{n}[g(X_n)-g(\\theta)]\\,\\xrightarrow{D}\\,\\mathcal{N}(0,\\sigma^2\\cdot[g'(\\theta)]^2)}</math>\n\nfor any function ''g'' satisfying the property that {{math|''g′''(''θ'')}} exists and is non-zero valued.\n\n===Proof in the univariate case===\nDemonstration of this result is fairly straightforward under the assumption that {{math|''g′''(''θ'')}} is [[Continuous function|continuous]]. To begin, we use the [[mean value theorem]] (i.e.: the first order approximation of a [[Taylor series]] using [[Taylor's theorem]]):\n:<math>g(X_n)=g(\\theta)+g'(\\tilde{\\theta})(X_n-\\theta),</math>\nwhere <math>\\tilde{\\theta}</math> lies between {{mvar|X<sub>n</sub>}} and ''θ''.\nNote that since <math>X_n\\,\\xrightarrow{P}\\,\\theta</math> and <math>X_n <  \\tilde{\\theta} < \\theta  </math>, it must be that  <math>\\tilde{\\theta} \\,\\xrightarrow{P}\\,\\theta</math> and since {{math|''g′''(''θ'')}} is continuous, applying the [[continuous mapping theorem]] yields\n:<math>g'(\\tilde{\\theta})\\,\\xrightarrow{P}\\,g'(\\theta),</math>\nwhere <math>\\xrightarrow{P}</math> denotes [[convergence in probability]].\n\nRearranging the terms and multiplying by <math>\\sqrt{n}</math> gives\n:<math>\\sqrt{n}[g(X_n)-g(\\theta)]=g' \\left (\\tilde{\\theta} \\right )\\sqrt{n}[X_n-\\theta].</math>\nSince\n:<math>{\\sqrt{n}[X_n-\\theta] \\xrightarrow{D} \\mathcal{N}(0,\\sigma^2)}</math>\nby assumption, it follows immediately from appeal to [[Slutsky's theorem]] that\n:<math>{\\sqrt{n}[g(X_n)-g(\\theta)] \\xrightarrow{D} \\mathcal{N}(0,\\sigma^2[g'(\\theta)]^2)}.</math>\nThis concludes the proof.\n\n====Proof with an explicit order of approximation====\nAlternatively, one can add one more step at the end, to obtain the [[Big O in probability notation|order of approximation]]:\n:<math>\n\\begin{align}\n\\sqrt{n}[g(X_n)-g(\\theta)]&=g' \\left (\\tilde{\\theta} \\right )\\sqrt{n}[X_n-\\theta]=\\sqrt{n}[X_n-\\theta]\\left[ g'(\\tilde{\\theta} )+g'(\\theta)-g'(\\theta)\\right]\\\\\n&=\\sqrt{n}[X_n-\\theta]\\left[g'(\\theta)\\right]+\\sqrt{n}[X_n-\\theta]\\left[ g'(\\tilde{\\theta} )-g'(\\theta)\\right]\\\\\n&=\\sqrt{n}[X_n-\\theta]\\left[g'(\\theta)\\right]+O_p(1)\\cdot o_p(1)\\\\\n&=\\sqrt{n}[X_n-\\theta]\\left[g'(\\theta)\\right]+o_p(1)\n\\end{align}\n</math>\nThis suggests that the error in the approximation converges to 0 in probability.\n\n==Multivariate delta method==\nBy definition, a [[consistency (statistics)|consistent]] [[estimator]] ''B'' [[convergence in probability|converges in probability]] to its true value ''β'', and often a [[central limit theorem]] can be applied to obtain [[Estimator#Asymptotic normality|asymptotic normality]]:\n\n:<math>\\sqrt{n}\\left(B-\\beta\\right)\\,\\xrightarrow{D}\\,N\\left(0, \\Sigma \\right),</math>\n\nwhere ''n'' is the number of observations and Σ is a (symmetric positive semi-definite) covariance matrix. Suppose we want to estimate the variance of a scalar-valued function ''h'' of the estimator ''B''. Keeping only the first two terms of the [[Taylor series]], and using vector notation for the [[gradient]], we can estimate ''h(B)'' as\n\n:<math>h(B) \\approx h(\\beta) + \\nabla h(\\beta)^T \\cdot (B-\\beta)</math>\n\nwhich implies the variance of ''h(B)'' is approximately\n\n:<math>\\begin{align}\n\\operatorname{Var}\\left(h(B)\\right) & \\approx \\operatorname{Var}\\left(h(\\beta) + \\nabla h(\\beta)^T \\cdot (B-\\beta)\\right) \\\\\n & = \\operatorname{Var}\\left(h(\\beta) + \\nabla h(\\beta)^T \\cdot B - \\nabla h(\\beta)^T \\cdot \\beta\\right) \\\\\n & = \\operatorname{Var}\\left(\\nabla h(\\beta)^T \\cdot B\\right) \\\\\n & = \\nabla h(\\beta)^T \\cdot \\operatorname{Cov}(B) \\cdot \\nabla h(\\beta) \\\\\n & = \\nabla h(\\beta)^T \\cdot (\\Sigma / n) \\cdot \\nabla h(\\beta)\n\\end{align}</math>\n\nOne can use the [[mean value theorem]] (for real-valued functions of many variables) to see that this does not rely on taking first order approximation.\n\nThe delta method therefore implies that\n\n:<math>\\sqrt{n}\\left(h(B)-h(\\beta)\\right)\\,\\xrightarrow{D}\\,N\\left(0, \\nabla h(\\beta)^T \\cdot \\Sigma \\cdot \\nabla h(\\beta)\\right)</math>\n\nor in univariate terms,\n\n:<math>\\sqrt{n}\\left(h(B)-h(\\beta)\\right)\\,\\xrightarrow{D}\\,N\\left(0, \\sigma^2 \\cdot \\left(h^\\prime(\\beta)\\right)^2 \\right).</math>\n\n==Example: the binomial proportion ==\nSuppose ''X<sub>n</sub>'' is [[binomial distribution|binomial]] with parameters <math> p \\in (0,1] </math> and ''n''.  Since\n\n:<math>{\\sqrt{n} \\left[ \\frac{X_n}{n}-p \\right]\\,\\xrightarrow{D}\\,N(0,p (1-p))},</math>\n\nwe can apply the Delta method with {{math|''g''(''θ'') {{=}} log(''θ'')}} to see\n\n:<math>{\\sqrt{n} \\left[ \\log\\left( \\frac{X_n}{n}\\right)-\\log(p)\\right] \\,\\xrightarrow{D}\\,N(0,p (1-p) [1/p]^2)}</math>\n\nHence, even though for any finite ''n'', the variance of <math>\\log\\left(\\frac{X_n}{n}\\right)</math> does not actually exist (since ''X<sub>n</sub>'' can be zero), the asymptotic variance of <math> \\log \\left( \\frac{X_n}{n} \\right) </math> does exist and is equal to\n\n:<math> \\frac{1-p}{np}.</math>\n\nNote that since ''p>0'', <math> \\Pr \\left( \\frac{X_n}{n} > 0 \\right) \\rightarrow 1 </math> as <math> n \\rightarrow \\infty </math>, so with probability converging to one, <math> \\log\\left(\\frac{X_n}{n}\\right) </math> is finite for large ''n''.\n\nMoreover, if <math>\\hat p </math> and <math>\\hat q</math> are estimates of different group rates from independent samples of sizes ''n'' and ''m'' respectively, then the logarithm of the estimated [[relative risk]] <math>\\frac{\\hat p}{\\hat q} </math> has asymptotic variance equal to\n\n:<math> \\frac{1-p}{p \\, n}+\\frac{1-q}{q \\, m}.</math>\n\nThis is useful to construct a hypothesis test or to make a confidence interval for the relative risk.\n\n==Note==\nThe delta method is often used in a form that is essentially identical to that above, but without the assumption that {{mvar|X<sub>n</sub>}} or ''B'' is asymptotically normal. Often the only context is that the variance is \"small\". The results then just give approximations to the means and covariances of the transformed quantities. For example, the formulae presented in Klein (1953, p.&nbsp;258) are:<ref>{{cite book |last=Klein |first=L. R. |authorlink=Lawrence Klein |year=1953 |title=A Textbook of Econometrics |location= |publisher= |page=258 |url=https://books.google.com/books?id=uzwiAAAAMAAJ&pg=PA258 }}</ref>\n\n:<math>\\begin{align}\n\\operatorname{Var} \\left(h_r \\right) = & \\sum_i \\left( \\frac{\\partial h_r}{\\partial B_i} \\right)^2 \\operatorname{Var}\\left( B_i \\right) +  \\sum_i \\sum_{j \\neq i} \\left( \\frac{ \\partial h_r }{ \\partial B_i } \\right) \\left( \\frac{ \\partial h_r }{ \\partial B_j } \\right) \\operatorname{Cov}\\left( B_i, B_j \\right) \\\\\n\\operatorname{Cov}\\left( h_r, h_s \\right) = & \\sum_i \\left( \\frac{ \\partial h_r }{ \\partial B_i } \\right) \\left( \\frac{\\partial h_s }{ \\partial B_i } \\right) \\operatorname{Var}\\left( B_i \\right) + \\sum_i \\sum_{j \\neq i} \\left( \\frac{\\partial h_r}{\\partial B_i} \\right) \\left(\\frac{\\partial h_s}{\\partial B_j} \\right) \\operatorname{Cov}\\left( B_i, B_j \\right)\n\\end{align}</math>\n\nwhere {{mvar|h<sub>r</sub>}} is the ''r''th element of ''h''(''B'') and ''B<sub>i</sub>'' is the ''i''th element of ''B''.\n\n==See also==\n*[[Taylor expansions for the moments of functions of random variables]]\n*[[Variance-stabilizing transformation]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n* {{cite journal |last=Oehlert |first=G. W. |year=1992 |title=A Note on the Delta Method |journal=The American Statistician |volume=46 |issue=1 |pages=27–29 |jstor=2684406 |doi=10.1080/00031305.1992.10475842}}\n* {{cite book |first=Kirk M. |last=Wolter |chapter=Taylor Series Methods |title=Introduction to Variance Estimation |location=New York |publisher=Springer |year=1985 |isbn=0-387-96119-4 |pages=221–247 |chapterurl=https://books.google.com/books?id=EadxTw0t2dMC&pg=PA221 }}\n\n==External links==\n*{{cite web |first=Søren |last=Asmussen |publisher=Aarhus University |title=Some Applications of the Delta Method |url=https://data.math.au.dk/courses/advsimmethod/Fall05/notes/1209.pdf |work=Lecture notes |date=2005 }}\n*{{cite web |title=Explanation of the delta method |first=Alan H. |last=Feiveson |publisher=Stata Corp. |url=https://www.stata.com/support/faqs/statistics/delta-method/ }}\n*{{cite web |title=Using the Delta Method to Construct Confidence Intervals for Predicted Probabilities, Rates, and Discrete Changes |first=Jun |last=Xu |first2=J. Scott |last2=Long|author2-link=J. Scott Long |publisher=Indiana University |date=August 22, 2005 |work=Lecture notes |url=http://www.indiana.edu/~jslsoc/stata/ci_computations/spost_deltaci.pdf }}\n\n[[Category:Estimation methods]]\n[[Category:Statistical approximations]]\n[[Category:Articles containing proofs]]\n[[Category:Statistics articles needing expert attention]]\n\n[[de:Statistischer Test#Asymptotisches Verhalten des Tests]]"
    },
    {
      "title": "Edgeworth series",
      "url": "https://en.wikipedia.org/wiki/Edgeworth_series",
      "text": "The '''Gram–Charlier A series''' (named in honor of [[Jørgen Pedersen Gram]] and [[Carl Charlier]]), and the '''Edgeworth series''' (named in honor of [[Francis Ysidro Edgeworth]]) are [[series (mathematics)|series]] that approximate a [[probability distribution]] in terms of its [[cumulant]]s.<ref>Stuart, A., & Kendall, M. G. (1968). The advanced theory of statistics. Hafner Publishing Company.</ref> The series are the same; but, the arrangement of terms (and thus the accuracy of truncating the series) differ.<ref>Kolassa, J. E. (2006). Series approximation methods in statistics (Vol. 88). Springer Science & Business Media.</ref> The key idea of these expansions is to write the [[Characteristic function (probability theory)|characteristic function]] of the distribution whose [[probability density function]]  {{mvar|f}} is to be approximated in terms of the characteristic function of a distribution with known and suitable properties, and to recover {{mvar|f}} through the inverse [[Fourier transform]].\n\n==Gram–Charlier A series==\nWe examine a continuous random variable. Let <math>\\hat{f}</math> be the characteristic function of its distribution whose density function is {{mvar|f}}, and <math>\\kappa_r</math> its [[cumulant]]s. We expand in terms of a known distribution with probability density function {{math|ψ}}, characteristic function <math>\\hat{\\psi}</math>, and cumulants <math>\\gamma_r</math>. The density {{math|ψ}} is generally chosen to be that of the [[normal distribution]], but other choices are possible as well. By the definition of the cumulants, we have (see Wallace, 1958)<ref>{{cite journal |last=Wallace |first=D. L. |year=1958 |title=Asymptotic Approximations to Distributions |journal=Annals of Mathematical Statistics |volume=29 |issue=3 |pages=635–654 |jstor=2237255 }}</ref>\n:<math>\\hat{f}(t)= \\exp\\left[\\sum_{r=1}^\\infty\\kappa_r\\frac{(it)^r}{r!}\\right]</math> and \n:<math> \\hat{\\psi}(t)=\\exp\\left[\\sum_{r=1}^\\infty\\gamma_r\\frac{(it)^r}{r!}\\right],</math>\nwhich gives the following formal identity:\n:<math>\\hat{f}(t)=\\exp\\left[\\sum_{r=1}^\\infty(\\kappa_r-\\gamma_r)\\frac{(it)^r}{r!}\\right]\\hat{\\psi}(t)\\,.</math>\n\nBy the properties of the Fourier transform, <math>(it)^r \\hat{\\psi}(t)</math> is the Fourier transform of <math>(-1)^r[D^r\\psi](-x)</math>, where {{mvar|D}} is the [[differential operator]] with respect to {{mvar|x}}. Thus, after changing <math>x</math> with <math>-x</math> on both sides of the equation, we find for {{mvar|f}} the formal expansion\n\n:<math>f(x) = \\exp\\left[\\sum_{r=1}^\\infty(\\kappa_r - \\gamma_r)\\frac{(-D)^r}{r!}\\right]\\psi(x)\\,.</math>\n\nIf {{math|ψ}} is chosen as the normal density \n\n:<math>\\phi(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right]</math> \n\nwith mean and variance as given by {{mvar|f}}, that is, mean <math>\\mu = \\kappa_1</math> and variance <math>\\sigma^2 = \\kappa_2</math>, then the expansion becomes\n\n:<math>f(x) = \\exp\\left[\\sum_{r=3}^\\infty\\kappa_r\\frac{(-D)^r}{r!}\\right] \\phi(x),</math>\n\nsince <math> \\gamma_r=0</math> for all {{mvar|r}}  > 2, as higher cumulants of the normal distribution are 0. By expanding the exponential and collecting terms according to the order of the derivatives, we arrive at the Gram–Charlier A series. Such an expansion can be written compactly in terms of [[Bell polynomials]] as\n\n:<math>\\exp\\left[\\sum_{r=3}^\\infty\\kappa_r\\frac{(-D)^r}{r!}\\right] = \\sum_{n=0}^\\infty B_n(0,0,\\kappa_3,\\ldots,\\kappa_n)\\frac{(-D)^n}{n!}. </math>\n\nSince the n-th derivative of the Gaussian function <math>\\phi</math> is given in terms of [[Hermite polynomial]] as\n\n:<math>\\phi^{(n)}(x) = \\frac{(-1)^n}{\\sigma^n} He_n \\left( \\frac{x-\\mu}{\\sigma} \\right) \\phi(x),</math>\n\nthis gives us the final expression of the Gram-Charlier A series as\n\n:<math> f(x) = \\phi(x) \\sum_{n=0}^\\infty \\frac{1}{n! \\sigma^n} B_n(0,0,\\kappa_3,\\ldots,\\kappa_n)  He_n \\left( \\frac{x-\\mu}{\\sigma} \\right).</math>\n\nIntegrating the series gives us the [[cumulative distribution function]] \n\n:<math> F(x) = \\int_{-\\infty}^x f(u) du = \\Phi(x) - \\phi(x) \\sum_{n=3}^\\infty \\frac{1}{n! \\sigma^{n-1}} B_n(0,0,\\kappa_3,\\ldots,\\kappa_n) He_{n-1} \\left( \\frac{x-\\mu}{\\sigma} \\right), </math>\n\nwhere <math>\\Phi</math> is the CDF of the normal distribution. \n\nIf we include only the first two correction terms to the normal distribution, we obtain\n\n:<math> f(x) \\approx \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right]\\left[1+\\frac{\\kappa_3}{3!\\sigma^3}He_3\\left(\\frac{x-\\mu}{\\sigma}\\right)+\\frac{\\kappa_4}{4!\\sigma^4}He_4\\left(\\frac{x-\\mu}{\\sigma}\\right)\\right]\\,,</math>\n\nwith <math>He_3(x)=x^3-3x</math> and <math>He_4(x)=x^4 - 6x^2 + 3</math>. \n\nNote that this expression is not guaranteed to be positive, and is therefore not a valid probability distribution. The Gram–Charlier A series diverges in many cases of interest—it converges only if <math>f(x)</math> falls off faster than <math>\\exp(-(x^2)/4)</math> at infinity (Cramér 1957). When it does not converge, the series is also not a true [[asymptotic expansion]], because it is not possible to estimate the error of the expansion. For this reason, the Edgeworth series (see next section) is generally preferred over the Gram–Charlier A series.\n\n==The Edgeworth series==\nEdgeworth developed a similar expansion as an improvement to the [[central limit theorem]].<ref>Hall, P. (2013). The bootstrap and Edgeworth expansion. Springer Science & Business Media.</ref> The advantage of the Edgeworth series is that the error is controlled, so that it is a true [[asymptotic expansion]].\n\nLet <math>\\{Z_i\\}</math> be a sequence of [[independent and identically distributed]] random variables with mean <math>\\mu</math> and variance <math>\\sigma^2</math>, and let <math>X_n</math> be their standardized sums:\n\n:<math>X_n = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\frac{Z_i - \\mu}{\\sigma}.</math>\n\nLet <math>F_n</math> denote the [[cumulative distribution function]]s of the variables <math>X_n</math>. Then by the central limit theorem,\n: <math>\n    \\lim_{n\\to\\infty} F_n(x) = \\Phi(x) \\equiv \\int_{-\\infty}^x \\tfrac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}q^2}dq\n  </math>\n\nfor every <math>x</math>, as long as the mean and variance are finite.\n\nNow assume that, in addition to having mean <math>\\mu</math> and variance <math>\\sigma^2</math>, the i.i.d. random variables <math>Z_i</math> have higher cumulants <math> \\kappa_r</math>. From the additivity and homogeneity properties of cumulants, the cumulants of <math>X_n</math> in terms of the cumulants of <math>Z_i</math> are for <math>r \\geq 2</math>, \n\n:<math> \\kappa_r^{F_n} = \\frac{n \\kappa_r}{\\sigma^r n^{r/2}} = \\frac{\\lambda_r}{n^{r/2 - 1}} \\quad \\mathrm{where} \\quad \\lambda_r = \\frac{\\kappa_r}{\\sigma^r}.\n </math>\n\nIf we expand in terms of the standard normal distribution, that is, if we set\n\n:<math>\\phi(x)=\\frac{1}{\\sqrt{2\\pi}}\\exp(-\\tfrac{1}{2}x^2),</math>\n\nthen the cumulant differences in the formal expression of the characteristic function <math>\\hat{f}_n(t)</math> of <math>F_n</math> are\n\n:<math> \\kappa^{F_n}_1-\\gamma_1 = 0,</math>\n:<math> \\kappa^{F_n}_2-\\gamma_2 = 0,</math>\n:<math> \\kappa^{F_n}_r-\\gamma_r = \\frac{\\lambda_r}{n^{r/2-1}}; \\qquad r\\geq 3.</math>\n\nThe Gram-Charlier A series for the density function of <math>X_n</math> is now\n\n:<math> f_n(x) = \\phi(x) \\sum_{r=0}^\\infty \\frac{1}{r!} B_r \\left(0,0,\\frac{\\lambda_3}{n^{1/2}},\\ldots,\\frac{\\lambda_r}{n^{r/2-1}}\\right) He_r(x).</math>\n\nThe Edgeworth series is developed similarly to the Gram–Charlier A series, only that now terms are collected according to powers of <math>n</math>. The coefficients of ''n''<sup>-m/2</sup> term can be obtained by collecting the monomials of the Bell polynomials corresponding to the integer partitions of ''m''. Thus, we have the characteristic function as\n\n:<math> \\hat{f}_n(t)=\\left[1+\\sum_{j=1}^\\infty \\frac{P_j(it)}{n^{j/2}}\\right] \\exp(-t^2/2)\\,,</math>\n\nwhere <math>P_j(x)</math> is a [[polynomial]] of degree <math>3j</math>. Again, after inverse Fourier transform, the density function <math>f_n</math> follows as\n\n:<math> f_n(x) = \\phi(x) + \\sum_{j=1}^\\infty \\frac{P_j(-D)}{n^{j/2}} \\phi(x)\\,.</math>\n\nLikewise, integrating the series, we obtain the distribution function \n\n:<math> F_n(x) = \\Phi(x) + \\sum_{j=1}^\\infty \\frac{1}{n^{j/2}} \\frac{P_j(-D)}{D} \\phi(x)\\,. </math>\n\nWe can explicitly write the polynomial <math>P_m(-D)</math> as \n\n:<math> P_m(-D) = \\sum \\prod_i  \\frac{1}{k_i!} \\left(\\frac{\\lambda_{l_i}}{l_i!}\\right)^{k_i} (-D)^s,</math>\n\nwhere the summation is over all the integer partitions of ''m'' such that <math>\\sum_i i k_i = m</math> and <math>l_i = i+2</math> and <math>s = \\sum_i k_i l_i.</math> \n\nFor example, if ''m'' = 3, then there are three ways to partition this number: 1 + 1 + 1 = 2 + 1 = 3. As such we need to examine three cases:\n\n* 1 + 1 + 1 = 1 · ''k''<sub>1</sub>, so we have ''k''<sub>1</sub> = 3,  ''l''<sub>1</sub> = 3, and ''s'' = 9. \n* 1 + 2 = 1 · ''k''<sub>1</sub> + 2 · ''k''<sub>2</sub>, so we have ''k''<sub>1</sub> = 1, ''k''<sub>2</sub> = 1,  ''l''<sub>1</sub> = 3, ''l''<sub>2</sub> = 4, and ''s'' = 7. \n* 3 = 3 · ''k''<sub>3</sub>, so we have ''k''<sub>3</sub> = 1,  ''l''<sub>3</sub> = 5, and ''s'' = 5. \n\nThus, the required polynomial is\n\n:<math>\n\\begin{align}\nP_3(-D) &= \\frac{1}{3!} \\left(\\frac{\\lambda_3}{3!}\\right)^3 (-D)^9 + \\frac{1}{1! 1!} \\left(\\frac{\\lambda_3}{3!}\\right) \\left(\\frac{\\lambda_4}{4!}\\right) (-D)^7 + \\frac{1}{1!} \\left(\\frac{\\lambda_5}{5!}\\right) (-D)^5 \\\\\n&= \\frac{\\lambda_3^3}{1296} (-D)^9 + \\frac{\\lambda_3 \\lambda_4}{144} (-D)^7 + \\frac{\\lambda_5}{120} (-D)^5.\n\\end{align}\n</math>\n\nThe first five terms of the expansion are<ref>{{MathWorld |title=Edgeworth Series|urlname=EdgeworthSeries}}</ref>\n\n:<math>\\begin{align}\nf_n(x) &= \\phi(x) \\\\\n&\\quad -\\frac{1}{n^{\\frac{1}{2}}}\\left(\\tfrac{1}{6}\\lambda_3\\,\\phi^{(3)}(x) \\right) \\\\\n&\\quad +\\frac{1}{n}\\left(\\tfrac{1}{24}\\lambda_4\\,\\phi^{(4)}(x) + \\tfrac{1}{72}\\lambda_3^2\\,\\phi^{(6)}(x) \\right) \\\\\n&\\quad -\\frac{1}{n^{\\frac{3}{2}}}\\left(\\tfrac{1}{120}\\lambda_5\\,\\phi^{(5)}(x) + \\tfrac{1}{144}\\lambda_3\\lambda_4\\,\\phi^{(7)}(x) + \\tfrac{1}{1296}\\lambda_3^3\\,\\phi^{(9)}(x)\\right) \\\\\n&\\quad + \\frac{1}{n^2}\\left(\\tfrac{1}{720}\\lambda_6\\,\\phi^{(6)}(x) + \\left(\\tfrac{1}{1152}\\lambda_4^2 + \\tfrac{1}{720}\\lambda_3\\lambda_5\\right)\\phi^{(8)}(x) + \\tfrac{1}{1728}\\lambda_3^2\\lambda_4\\,\\phi^{(10)}(x) + \\tfrac{1}{31104}\\lambda_3^4\\,\\phi^{(12)}(x) \\right)\\\\\n&\\quad + O \\left (n^{-\\frac{5}{2}} \\right ).\n\\end{align}</math>\n\nHere, {{math|φ<sup>(''j'')</sup>(''x'')}} is the ''j''-th derivative of {{math|φ(·)}} at point ''x''. Remembering that the [[Normal distribution#Symmetries and derivatives|derivatives of the density of the normal distribution]] are related to the normal density by <math>\\phi^{(n)}(x) = (-1)^n He_n(x)\\phi(x)</math>, (where <math>He_n</math> is the [[Hermite polynomial]] of order ''n''), this explains the alternative representations in terms of the density function. Blinnikov and Moessner (1998) have given a simple algorithm to calculate higher-order terms of the expansion.\n\nNote that in case of a lattice distributions (which have discrete values), the Edgeworth expansion must be adjusted to account for the discontinuous jumps between lattice points.<ref>{{cite journal |last=Kolassa |first=John E. |first2=Peter |last2=McCullagh |title=Edgeworth series for lattice distributions |journal=[[Annals of Statistics]] |volume=18 |issue=2 |year=1990 |pages=981–985 |jstor=2242145 |doi=10.1214/aos/1176347637}}</ref>\n\n==Illustration: density of the sample mean of three <math> \\chi^2 </math>==\n\n[[File:Edgeworth expansion of the density of the sample mean of three Chi2 variables.png|thumb|Density of the sample mean of three chi2 variables. The chart compares the true density, the normal approximation, and two edgeworth expansions]]\n\nTake  <math> X_i \\sim \\chi^2(k=2) \\qquad i=1, 2, 3 </math> and the sample mean <math> \\bar X = \\frac{1}{3} \\sum_{i=1}^{3} X_i </math>.\n\nWe can use several distributions for <math> \\bar X </math>:\n* The exact distribution, which follows a [[gamma distribution]]: <math> \\bar X \\sim \\mathrm{Gamma}\\left(\\alpha=n\\cdot k /2, \\theta= 2/n \\right)</math> = <math>\\mathrm{Gamma}\\left(\\alpha=3, \\theta= 2/3 \\right)</math>\n* The asymptotic normal distribution: <math> \\bar X  \\xrightarrow{n \\to \\infty} N(k, 2\\cdot k /n ) = N(2, 4/3 )</math>\n* Two Edgeworth expansion, of degree 2 and 3\n\n==Disadvantages of the Edgeworth expansion==\n\nEdgeworth expansions can suffer from a few issues:\n* They are not guaranteed to be a proper [[probability distribution]] as:\n** The integral of the density need not integrate to 1\n** Probabilities can be negative\n* They can be inaccurate, especially in the tails, due to mainly two reasons:\n** They are obtained under a Taylor series around the mean\n** They guarantee (asymptotically) an [[Approximation error|absolute error]], not a relative one. This is an issue when one wants to approximate very small quantities, for which the absolute error might be small, but the relative error important.\n\n==See also==\n\n* [[Cornish–Fisher expansion]]\n* [[Edgeworth binomial tree]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n* [[Harald Cramér|H. Cramér]]. (1957). ''Mathematical Methods of Statistics''. Princeton University Press, Princeton.\n* {{cite journal | last1 = Wallace | first1 = D. L. | year = 1958 | title = Asymptotic approximations to distributions | url = | journal = Annals of Mathematical Statistics | volume = 29 | issue = | pages = 635–654 | doi=10.1214/aoms/1177706528}}\n* M. Kendall & A. Stuart. (1977), ''The advanced theory of statistics'', Vol 1: Distribution theory, 4th Edition, Macmillan, New York.\n* [[Peter McCullagh|P. McCullagh]] (1987). ''Tensor Methods in Statistics''.  Chapman and Hall, London.\n* [[David Cox (statistician)|D. R. Cox]] and [[Ole Barndorff-Nielsen|O. E. Barndorff-Nielsen]] (1989). ''Asymptotic Techniques for Use in Statistics''.  Chapman and Hall, London.\n* P. Hall (1992). ''The Bootstrap and Edgeworth Expansion''. Springer, New York.\n* {{springer|title=Edgeworth series|id=p/e035060}}\n* {{cite journal | last1 = Blinnikov | first1 = S. | last2 = Moessner | first2 = R. | year = 1998 | title = Expansions for nearly Gaussian distributions | url = http://aas.aanda.org/articles/aas/pdf/1998/10/h0596.pdf | format = PDF | journal = Astronomy and Astrophysics Supplement Series | volume = 130 | issue = | pages = 193–205 | doi=10.1051/aas:1998221| arxiv = astro-ph/9711239 | bibcode = 1998A&AS..130..193B }}\n* {{cite journal | last1 = Martin| first1 = Douglas | last2 = Arora| first2 = Rohit | year = 2017 | title = Inefficiency and bias of modified value-at-risk and expected shortfall | journal = Journal of Risk | volume = 19 | issue = 6 | pages = 59–84 | doi=10.21314/JOR.2017.365}}\n* J. E. Kolassa (2006). ''Series Approximation Methods in Statistics'' (3rd ed.). (Lecture Notes in Statistics #88). Springer, New York.\n\n{{DEFAULTSORT:Edgeworth Series}}\n[[Category:Mathematical series]]\n[[Category:Statistical approximations]]"
    },
    {
      "title": "Function approximation",
      "url": "https://en.wikipedia.org/wiki/Function_approximation",
      "text": "{{distinguish|Function fitting}}\nIn general, a function approximation problem asks us to select a [[function (mathematics)|function]] among a well-defined class{{Clarify|date=October 2017}} that closely matches (\"approximates\") a '''target function''' in a task-specific way. The need for '''function approximations''' arises in many branches{{Examples|date=October 2017}} of [[applied mathematics]], and [[computer science]] in particular {{why|date=October 2017}}.\n\nOne can distinguish two major classes of function approximation problems: \n\nFirst, for known target functions [[approximation theory]] is the branch of [[numerical analysis]] that investigates how certain known functions (for example, [[special function]]s) can be approximated by a specific class of functions (for example, [[polynomial]]s or [[rational function]]s) that often have desirable properties (inexpensive computation, continuity, integral and limit values, etc.).\n\nSecond, the target function, call it ''g'', may be unknown; instead of an explicit formula, only a set of points of the form (''x'', ''g''(''x'')) is provided.  Depending on the structure of the [[domain of a function|domain]] and [[codomain]] of ''g'', several techniques for approximating ''g'' may be applicable.  For example, if ''g'' is an operation on the [[real number]]s, techniques of [[interpolation]], [[extrapolation]], [[regression analysis]], and [[curve fitting]] can be used.  If the [[codomain]] (range or target set) of ''g'' is a finite set, one is dealing with a [[statistical classification|classification]] problem instead.\n\nTo some extent, the different problems (regression, classification, [[fitness approximation]]) have received a unified treatment in [[statistical learning theory]], where they are viewed as [[supervised learning]] problems.\n\n==See also==\n*[[Approximation theory]]\n*[[Fitness approximation]]\n*[[Kriging]]\n*[[Least squares (function approximation)]]\n*[[Radial basis function network]]\n\n{{DEFAULTSORT:Function Approximation}}\n[[Category:Regression analysis]]\n[[Category:Statistical approximations]]\n\n\n{{mathanalysis-stub}}\n{{statistics-stub}}"
    },
    {
      "title": "Imprecise probability",
      "url": "https://en.wikipedia.org/wiki/Imprecise_probability",
      "text": "'''Imprecise probability''' generalizes [[probability theory]] to allow for partial probability specifications, and is applicable when information is scarce, vague, or conflicting, in which case a unique [[probability distribution]] may be hard to identify. Thereby, the theory aims to represent the available knowledge more accurately.  Imprecision is useful for dealing with [[expert elicitation]], because:\n* People have a limited ability to determine their own subjective probabilities and might find that they can only provide an interval.\n* As an interval is compatible with a range of opinions, the analysis ought to be more convincing to a range of different people.\n\n== Introduction ==\n\nUncertainty is traditionally modelled by a [[probability]] distribution, as argued by [[Andrey Kolmogorov|Kolmogorov]],<ref name=\"KOLMOGOROV1950\">{{Cite book\n| publisher = Chelsea Publishing Company\n| last = Kolmogorov\n| first = A. N.\n| title = Foundations of the Theory of Probability\n| location = New York\n| year = 1950\n}}</ref> [[Pierre-Simon Laplace|Laplace]], [[Bruno de Finetti|de Finetti]],<ref name=\"DEFINETTI1974\">{{cite book | last= de Finetti| first= Bruno| year= 1974| title=Theory of Probability|location=New York|publisher=Wiley}}</ref> [[Frank P. Ramsey|Ramsey]], [[Cox's theorem|Cox]], [[Dennis Lindley|Lindley]], and many others. However, this has not been unanimously accepted by scientists, statisticians, and probabilists: it has been argued that some modification or broadening of probability theory is required, because one may not always be able to provide a probability for every event, particularly when only little information or data is available&mdash;an early example of such criticism is [[George Boole|Boole]]'s critique<ref name=\"BOOLE1854\">{{cite book\n| last = Boole \n| first = George \n| authorlink = \n| title = An investigation of the laws of thought on which are founded the mathematical theories of logic and probabilities \n| publisher = Walton and Maberly \n| year = 1854 \n| location = London \n| pages = \n| url = http://www.gutenberg.org/etext/15114 \n| doi = \n| isbn = }}</ref> of [[Pierre-Simon Laplace|Laplace]]'s work&mdash;, or when we wish to model probabilities that a group agrees with, rather than those of a single individual.\n\nPerhaps the most straightforward generalization is to replace a single probability specification with an interval specification. [[Upper and lower probabilities|Lower and upper probabilities]], denoted by <math>\\underline{P}(A)</math> and <math>\\overline{P}(A)</math>, or more generally, lower and upper expectations (previsions),<ref name=\"SMITH1961\">{{Cite journal\n| volume = B\n| issue = 23\n| pages = 1–37\n| last = Smith\n| first = Cedric A. B.\n| title = Consistency in statistical inference and decision\n| journal = Journal of the Royal Statistical Society\n| year = 1961\n}}</ref><ref name=\"WILLIAMS1970\">{{Cite conference\n| publisher = School of Math. and Phys. Sci., Univ. of Sussex\n| last = Williams\n| first = Peter M.\n| title = Notes on conditional previsions\n| year = 1975\n}}</ref><ref name=\"WILLIAMS2007\">{{Cite journal\n| volume = 44\n| issue = 3\n| pages = 366–383\n| last = Williams\n| first = Peter M.\n| title = Notes on conditional previsions\n| journal = International Journal of Approximate Reasoning\n| year = 2007\n| doi = 10.1016/j.ijar.2006.07.019\n}}</ref><ref name=\"WALLEY1991\">{{cite book\n| last = Walley\n| first = Peter\n| authorlink =\n| title = Statistical Reasoning with Imprecise Probabilities\n| publisher = Chapman and Hall\n| year = 1991\n| location = London\n| pages =\n| isbn = 978-0-412-28660-5 }}</ref> aim to fill this gap:\n*the special case with <math>\\underline{P}(A)=\\overline{P}(A)</math> for all events <math>A</math> provides precise probability, whilst\n*<math>\\underline{P}(A)=0</math> and <math>\\overline{P}(A)=1</math> represents no constraint at all on the specification of <math>P(A)</math>,\nwith a flexible continuum in between.\n\nSome approaches, summarized under the name ''nonadditive probabilities'',<ref name=\"DENNEBERG1994\">{{Cite book\n| publisher = Kluwer\n| last = Denneberg\n| first = Dieter\n| title = Non-additive Measure and Integral\n| location = Dordrecht\n| year = 1994\n}}</ref> directly use one of these [[set function]]s, assuming the other one to be naturally defined such that  <math>\\underline{P}(A^c)= 1-\\overline{P}(A)</math>, with <math>A^c</math> the complement of <math>A</math>. Other related concepts understand the corresponding intervals <math>[\\underline{P}(A), \\overline{P}(A)]</math> for all events as the basic entity.<ref name=\"WEICHSELBERGER2000\">{{cite journal\n|title=The theory of interval probability as a unifying concept for uncertainty\n|journal=International Journal of Approximate Reasoning\n|year=2000\n|first=Kurt\n|last=Weichselberger\n|volume=24\n|issue=2–3\n|pages=149–170\n|url=\n|doi=10.1016/S0888-613X(00)00032-3}}</ref><ref name=\"WEICHSELBERGER2001\">{{Cite book\n| publisher = Physica\n| last = Weichselberger\n| first = K.\n| title = Elementare Grundbegriffe einer allgemeineren Wahrscheinlichkeitsrechnung I - Intervallwahrscheinlichkeit als umfassendes Konzept\n| location = Heidelberg\n| year = 2001\n}}</ref>\n\n== History ==\nThe  idea to use imprecise probability has a long history. The first formal treatment dates back at least to the middle of the nineteenth century, by [[George Boole]],<ref name=\"BOOLE1854\"/> who aimed to reconcile the theories of logic (which can express complete ignorance) and probability. In the 1920s, in ''[[A Treatise on Probability]]'', [[John Maynard Keynes|Keynes]]<ref name=\"KEYNES1921\">{{cite book\n| last = Keynes \n| first = John Maynard\n| authorlink = \n| title = A Treatise on Probability \n| publisher = Macmillan And Co.\n| year = 1921 \n| location = London \n| pages = \n| url = https://archive.org/details/treatiseonprobab007528mbp\n| doi = \n| isbn = }}</ref> formulated and applied an explicit interval estimate approach to probability.\n\nSince the 1990s, the theory has gathered strong momentum, initiated by comprehensive foundations put forward by Walley,<ref name=\"WALLEY1991\"/> who coined the term ''imprecise probability'', by Kuznetsov,<ref name=\"KUZNETSOV1991\">{{Cite book\n| publisher = Radio i Svyaz Publ.\n| last = Kuznetsov\n| first = Vladimir P.\n| title = Interval Statistical Models\n| location = Moscow\n| year = 1991\n}}</ref> and by Weichselberger,<ref name=\"WEICHSELBERGER2000\"/><ref name=\"WEICHSELBERGER2001\"/> who uses the term ''interval probability''. Walley's theory extends the traditional subjective probability theory via buying and selling prices for gambles, whereas Weichselberger's approach generalizes [[Andrey Kolmogorov|Kolmogorov]]'s axioms without imposing an interpretation.\n\nUsually assumed consistency conditions relate imprecise probability assignments to non-empty closed convex sets of probability distributions. Therefore, as a welcome by-product, the theory also provides a formal framework for models used in [[robust statistics]]<ref name=\"RIOS2000\">{{Cite book\n| publisher = Springer\n| last = Ruggeri\n| first = Fabrizio\n| others = D. Ríos Insua\n| title = Robust Bayesian Analysis\n| location = New York\n| year = 2000\n}}</ref> and [[non-parametric statistics]].<ref>{{Cite journal | doi = 10.1016/j.jspi.2003.07.003 | title = Nonparametric predictive inference and interval probability | year = 2004 | last1 = Augustin | first1 = T. | last2 = Coolen | first2 = F. P. A.| journal = Journal of Statistical Planning and Inference | volume = 124 | issue = 2 | pages = 251–272}}</ref> Included are also concepts based on [[Choquet integral|Choquet integration]],<ref>{{Cite journal | doi = 10.1016/j.jmaa.2008.05.071 | title = n-Monotone exact functionals | year = 2008 | last1 = de Cooman | first1 = G. | last2 = Troffaes | first2 = M. C. M.| last3 = Miranda | first3 = E. | journal = Journal of Mathematical Analysis and Applications | volume = 347 | issue = 1 | pages = 143–156|bibcode = 2008JMAA..347..143D |arxiv = 0801.1962 }}</ref> and so-called two-monotone and totally monotone [[capacity (statistics)|capacities]],<ref name=\"HUBER1973\">{{Cite journal\n| volume = 1\n| pages = 251–263\n| last = Huber\n| first = P. J.\n|author2=V. Strassen\n| title = Minimax tests and the Neyman-Pearson lemma for capacities\n| journal = The Annals of Statistics\n| year = 1973\n| doi = 10.1214/aos/1176342363\n| issue = 2\n}}</ref> which have become very popular in [[artificial intelligence]] under the name [[Dempster–Shafer theory#Belief and plausibility|(Dempster-Shafer) belief functions]].<ref name=\"DEMPSTER1967\">{{cite journal\n|title=Upper and lower probabilities induced by a multivalued mapping\n|journal=The Annals of Mathematical Statistics\n|year=1967\n|first=A. P.\n|last=Dempster\n|volume=38\n|issue=2\n|pages=325–339\n|jstor=2239146\n|doi=10.1214/aoms/1177698950}}</ref><ref name=\"SHAFER1976\">{{cite book\n| last = Shafer \n| first = Glenn \n| authorlink = \n| title = A Mathematical Theory of Evidence \n| publisher = Princeton University Press\n| year = 1976 \n| location = \n| pages = \n| url = \n| doi = \n| isbn = }}</ref> Moreover, there is a strong connection<ref name=\"DECOOMAN2008\">{{Cite journal | doi = 10.1016/j.artint.2008.03.001 | title = Imprecise probability trees: Bridging two theories of imprecise probability | year = 2008 | last1 = de Cooman | first1 = G. | last2 = Hermans | first2 = F. | journal = Artificial Intelligence | volume = 172 | issue = 11 | pages = 1400–1427| arxiv = 0801.1196 }}</ref> to [[Glenn Shafer|Shafer]] and Vovk's notion of [[game-theoretic probability]].<ref name=\"SHAFER2001\">{{Cite book\n| publisher = Wiley\n| last = Shafer\n| first = Glenn\n|author2=Vladimir Vovk\n| title = Probability and Finance: It's Only a Game!\n| year = 2001\n}}</ref>\n\n== Mathematical models ==\nSo, the term imprecise probability&mdash;although an unfortunate misnomer as it enables more accurate quantification of uncertainty than precise probability&mdash;appears to have been established in the 1990s, and covers a wide range of extensions of the theory of [[probability]], including:\n* [[previsions]]<ref name=\"DEFINETTI1974\" />\n* [[Upper and lower probabilities|lower and upper probabilities, or interval probabilities]]<ref name=\"BOOLE1854\" /><ref name=\"WEICHSELBERGER2000\" /><ref name=\"KEYNES1921\" />\n* [[belief functions]]<ref name=\"DEMPSTER1967\" /><ref name=\"SHAFER1976\" />\n* [[Possibility theory|possibility and necessity measures]]<ref>{{cite journal|title=Fuzzy sets as a basis for a theory of possibility|journal=Fuzzy Sets and Systems|year=1978|first=L. A.|last=Zadeh|volume=1|issue=|pages=3–28 |url=|doi=10.1016/0165-0114(78)90029-5 }}</ref><ref>{{cite book | last = Dubois | first = Didier | authorlink = |author2=Henri Prade | title = Théorie des possibilité | publisher = Masson | year = 1985 | location = Paris | pages = | url = | doi =  | isbn = }}</ref><ref>{{cite book | last = Dubois | first = Didier | authorlink = |author2=Henri Prade | title = Possibility Theory - An Approach to Computerized Processing of Uncertainty | publisher = Plenum Press | year = 1988 | location = New York | pages = | url = | doi =  | isbn = }}</ref>\n* [[lower and upper previsions]]<ref name=\"WILLIAMS1970\" /><ref name=\"WILLIAMS2007\" /><ref name=\"WALLEY1991\"/>\n* comparative probability orderings<ref name=\"KEYNES1921\"/><ref>{{cite journal|title=Sul significato soggettivo della probabilità|journal=Fundamenta Mathematicae|year=1931|first=Bruno|last=de Finetti|volume=17|issue=|pages=298–329 |url=|doi=10.4064/fm-17-1-298-329}}</ref><ref>{{cite book | last = Fine | first = Terrence L. | authorlink = | title = Theories of Probability | publisher = Academic Press | year = 1973 | location = New York | pages = | url = | doi =  | isbn = }}</ref><ref>{{cite journal|title=The axioms of subjective probability|journal=Statistical Science|year=1986|first=P. C.|last=Fishburn|volume=1|issue=3|pages=335–358 |url=|doi=10.1214/ss/1177013611 }}</ref>\n* partial preference orderings\n* sets of desirable gambles<ref name=\"WILLIAMS1970\" /><ref name=\"WILLIAMS2007\" /><ref name=\"WALLEY1991\"/>\n* [[probability box|p-boxes]]<ref>{{cite web|url=http://www.ramas.com/unabridged.zip |title=Constructing Probability Boxes and Dempster-Shafer Structures |accessdate=2009-09-23 |last=Ferson |first=Scott |author2=[[Vladik Kreinovich]] |author3=Lev Ginzburg |author4=David S. Myers |author5=Kari Sentz |year=2003 |work=SAND2002-4015 |publisher=Sandia National Laboratories }}</ref>\n* [[robust Bayes analysis|robust Bayes methods]]<ref>{{Cite book| first=James O. | last=Berger| contribution=The robust Bayesian viewpoint| title=Robustness of Bayesian Analyses| editor-first=J. B.| editor-last=Kadane| publisher=Elsevier Science| place=| pages=63–144| year=1984 | contribution-url=| editors= }}</ref>\n\n==Interpretation of imprecise probabilities==\nA unification of many of the above-mentioned imprecise probability theories was proposed by Walley,<ref name=\"WALLEY1991\"/> although this is in no way the first attempt to formalize imprecise probabilities.  In terms of [[probability interpretations]], Walley's formulation of imprecise probabilities is based on the [[Bayesian probability|subjective variant of the Bayesian interpretation]] of probability. Walley defines upper and lower probabilities as special cases of upper and lower previsions and the gambling framework advanced by [[Bruno de Finetti]]. In simple terms, a decision maker's lower prevision is the highest price at which the decision maker is sure he or she would buy a gamble, and the upper prevision is the lowest price at which the decision maker is sure he or she would buy the opposite of the gamble (which is equivalent to selling the original gamble). If the upper and lower previsions are equal, then they jointly represent the decision maker's [[fair price]] for the gamble, the price at which the decision maker is willing to take either side of the gamble. The existence of a fair price leads to precise probabilities.\n\nThe allowance for imprecision, or a gap between a decision maker's upper and lower previsions, is the primary difference between precise and imprecise probability theories.  Such gaps arise naturally in [[prediction market|betting markets]] which happen to be financially [[illiquid]] due to [[asymmetric information]].  This gap is also given by [[Henry Kyburg]] repeatedly for his interval probabilities, though he and [[Isaac Levi]] also give other reasons for intervals, or sets of distributions, representing states of belief.\n\n==Issues with imprecise probabilities==\nOne issue with imprecise probabilities is that there is often an independent degree of caution or boldness inherent in the use of one interval, rather than a wider or narrower one.  This may be a degree of confidence, degree of fuzzy membership, or threshold of acceptance.  This is not as much of a problem for intervals that are lower and upper bounds derived from a set of probability distributions, e.g., a set of priors followed by conditionalization on each member of the set.  However, it can lead to the question why some distributions are included in the set of priors and some are not.\n\nAnother issue is why one can be precise about two numbers, a lower bound and an upper bound, rather than a single number, a point probability.  This issue may be merely rhetorical, as the robustness of a model with intervals is inherently greater than that of a model with point-valued probabilities.  It does raise concerns about inappropriate claims of precision at endpoints, as well as for point values.\n\nA more practical issue is what kind of decision theory can make use of imprecise probabilities.<ref>Seidenfeld, Teddy. \"Decisions with indeterminate probabilities.\" Behavioral and Brain Sciences 6, no. 2 (1983): 259-261.</ref>  For fuzzy measures, there is the work of Yager.<ref>Yager, R.R., 1978. Fuzzy decision making including unequal objectives. Fuzzy sets and systems, 1(2), pp.87-95.</ref>  For convex sets of distributions, Levi's works are instructive.<ref>Levi, I., 1990. Hard choices: Decision making under unresolved conflict. Cambridge University Press.</ref>  Another approach asks whether the threshold controlling the boldness of the interval matters more to a decision than simply taking the average or using a [[Hurwicz]] decision rule.<ref>Loui, R.P., 1986. Decisions with indeterminate probabilities. Theory and Decision, 21(3), pp.283-309.</ref>  Other approaches appear in the literature.<ref>Guo, P. and Tanaka, H., 2010. Decision making with interval probabilities. European Journal of Operational Research, 203(2), pp.444-454.</ref><ref>Caselton, W.F. and Luo, W., 1992. Decision making with imprecise probabilities: Dempster‐Shafer theory and application. Water Resources Research, 28(12), pp.3071-3083.</ref><ref>Breese, J.S. and Fertig, K.W., 2013. Decision making with interval influence diagrams. arXiv preprint arXiv:1304.1096.</ref><ref>Gärdenfors, P. and Sahlin, N.E., 1982. Unreliable probabilities, risk taking, and decision making. Synthese, 53(3), pp.361-386.</ref>\n\n== Bibliography ==\n<references/>\n\n==See also==\n*[[Ambiguity aversion]]\n*[[Robust decision making]]\n*[[Imprecise Dirichlet process]]\n\n== External links ==\n*[http://www.sipta.org/ The Society for Imprecise Probability: Theories and Applications]\n*What is imprecision? [http://www.maths.dur.ac.uk/users/matthias.troffaes/jstpip/improb.html Journal of Statistical Theory and Practice (call for papers)]\n*[http://www.idsia.ch/~giorgio/jncc2.html Open source implementation of a classifier based on Imprecise Probabilities]\n*[http://ipg.idsia.ch The imprecise probability group at IDSIA]\n\n{{DEFAULTSORT:Imprecise Probability}}\n[[Category:Probability theory]]\n[[Category:Statistical approximations]]"
    },
    {
      "title": "Kirkwood approximation",
      "url": "https://en.wikipedia.org/wiki/Kirkwood_approximation",
      "text": "The '''Kirkwood superposition approximation''' was introduced in 1935 by [[John Gamble Kirkwood|John G. Kirkwood]] as a means of representing a [[discrete probability distribution]].<ref>Kirkwood, J. G. ''Statistical Mechanics of Fluid Mixtures''. [[J. Chem. Phys.]] 3, 300, (1935)</ref>  The Kirkwood approximation for a discrete [[probability density function]] <math>P(x_{1},x_{2},\\ldots ,x_{n})</math> is given by\n\n:<math>\nP^{\\prime }(x_1,x_2,\\ldots ,x_n)=\\frac{\\frac{\\frac{\\prod_{\\mathcal{T}\n_{n-1}\\subseteq \\mathcal{V}}p(\\mathcal{T}_{n-1})}{\\prod_{\\mathcal{T}\n_{n-2}\\subseteq \\mathcal{V}}p(\\mathcal{T}_{n-2})}}{\\vdots }}{\\prod_{\\mathcal{\nT}_1\\subseteq \\mathcal{V}}p(\\mathcal{T}_1)}  \n</math>\n\nwhere\n\n: <math>\\prod_{\\mathcal{T}_i\\subseteq \\mathcal{V}}p(\\mathcal{T}_i)</math>\n\nis the product of probabilities over all subsets of variables of size ''i'' in variable set <math>\\scriptstyle\\mathcal{V}</math>. This kind of formula has been considered by Watanabe (1960) and, according to Watanabe, also by Robert Fano. For the three-variable case, it reduces to simply\n\n:<math>\nP^\\prime(x_1,x_2,x_3)=\\frac{p(x_1,x_2)p(x_2,x_3)p(x_1,x_3)}{p(x_1)p(x_{2})p(x_3)}\n</math>\n \nThe Kirkwood approximation does not generally produce a valid probability distribution (the normalization condition is violated). Watanabe claims that for this reason informational expressions of this type are not meaningful, and indeed there has been very little written about the properties of this measure. The Kirkwood approximation is the probabilistic counterpart of the [[interaction information]].\n\n[[Judea Pearl]] (1988 §3.2.4) indicates that an expression of this type can be exact in the case of a ''decomposable'' model, that is, a probability distribution that admits a [[Graph (discrete mathematics)|graph]] structure whose [[clique (graph theory)|cliques]] form a [[tree (graph theory)|tree]]. In such cases, the numerator contains the product of the intra-clique joint distributions and the denominator contains the product of the clique intersection distributions.\n\n== References ==\n{{reflist}}\n\n* Jakulin, A. & Bratko, I. (2004), Quantifying and visualizing attribute interactions: An approach based on entropy, ''Journal of Machine Learning Research'', (submitted) pp.&nbsp;38&ndash;43.\n* Matsuda, H. (2000), Physical nature of higher-order mutual information: Intrinsic correlations and frustration, ''Physical Review E'' '''62''', 3096&ndash;3102.\n* Pearl, J. (1988), ''Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference'', Morgan Kaufmann, San Mateo, CA.\n* Watanabe, S. (1960), Information theoretical analysis of multivariate correlation, ''IBM Journal of Research and Development'' '''4''', 66&ndash;82.\n\n[[Category:Discrete distributions]]\n[[Category:Statistical approximations]]"
    },
    {
      "title": "Morris method",
      "url": "https://en.wikipedia.org/wiki/Morris_method",
      "text": "In [[applied statistics]], the '''Morris method''' for [[global sensitivity analysis]] is a so-called [[one-step-at-a-time method]] (OAT), meaning that in each run only one input parameter is given a new value. It facilitates a global sensitivity analysis by making a number ''r'' of local changes at different points ''x''(1&nbsp;→&nbsp;''r'') of the possible range of input values.\n\n==Method's details==\n\n=== Elementary effects' distribution===\nThe finite distribution of elementary effects associated with the ith input factor, is obtained by randomly sampling different x from Ω, and is denoted by Fi<ref name=\"Campolongo 2004\">{{cite book|author1=Andrea Saltelli |author2=Stefano Tarantola |author3=Francesca Campolongo |author4=Marco Ratto |title=Sensitivity analysis in practice: a guide to assessing scientific models|publisher= John Willy & Sons, Ltd|pages= 94–120|year= 2004}}</ref>\n\n===Variations===\nIn the original work of Morris  the two sensitivity measures proposed were respectively the mean, µ,\nand the standard deviation, σ, of Fi. However, choosing Morris has the drawback that, if the distribution, Fi, contains negative elements, which occurs when the model is non-monotonic, when computing the mean some effects may cancel each other out. Thus, the measure µ on its own is not reliable for ranking factors in order\nof importance. It is necessary to consider at the same time the values of µ and σ, as a factor with elementary effects of different signs (that cancel each other out) would have a low value of µ but a\nconsiderable value of σ that avoids underestimating the factors The screening exercise importance.<ref name=\"Campolongo 2004\" />\n\n===µ*===\nIf the distribution, Fi, contains negative elements, which occurs when the model is non-monotonic, when\ncomputing the mean some effects may cancel each other out.When the goal is to rank factors in order of importance by making use of a single sensitivity measure, scientific advice is to use µ∗,which by making use of the absolute value, avoids the occurrence of effects of opposite signs.<ref name=\"Campolongo 2004\" />\n\nIn Revised Morris method µ* is used to detect input factors with an important overall influence on the output. σ is used to detect factors involved in interaction with other factors or whose effect is non-linear.<ref name=\"Campolongo 2004\" />\n\n==Method's steps==\nThe method starts by sampling a set of start values within the defined ranges of possible values for all input variables and calculating the subsequent model outcome. The second step changes the values for one variable (all other inputs remaining at their start values) and calculates the resulting change in model outcome compared to the first run. Next, the values for another variable are changed (the previous variable is kept at its changed value and all other ones kept at their start values) and the resulting change in model outcome compared to the second run is calculated. This goes on until all input variables are changed. This procedure is repeated ''r'' times (where ''r'' is usually taken between 5 and 15), each time with a different set of start values, which leads to a number of ''r''(''k''&nbsp;+&nbsp;1) runs, where ''k'' is the number of input variables. Such number is very efficient compared to more demanding methods for [[sensitivity analysis]].<ref name=\"Campolongo 2003\">{{cite journal | first1 = F. | first2 = J. | first3 = A. | last1 = Campolongo | last2 = Cariboni | last3 = Saltelli | url = http://library.lanl.gov/cgi-bin/getdoc?event=SAMO2004&document=samo04-52.pdf | title = Sensitivity analysis: the Morris method versus the variance based measures. | journal = ''Submitted To'' Technometrics |volume = |pages = |year = 2003}}</ref>\n\nA [[sensitivity analysis]] method widely used to screen factors in models of large dimensionality is the design proposed by Morris.<ref name=\"Factorial Sampling Plans pg. 33\">{{cite journal | first = M.D. | last = Morris | url = http://www.abe.ufl.edu/Faculty/jjones/ABE_5646/2010/Morris.1991%20SA%20paper.pdf | title = Factorial Sampling Plans for Preliminary Computational Experiments | journal = Technometrics | volume = 33 | issue = 2 | pages = 161–174 | year = 1991 | doi = 10.2307/1269043| jstor = 1269043 | citeseerx = 10.1.1.584.521 }}</ref> The Morris method deals efficiently with models containing hundreds of input factors without relying on strict assumptions about the model, such as for instance additivity or monotonicity of the model input-output relationship. The Morris method is simple to  understand and  implement, and  its  results are easily interpreted. Furthermore, it is economic in the sense that it requires a number of model evaluations that is linear in the number of model factors. The method can be regarded as global as the final measure is obtained by averaging a number of local measures (the elementary effects), computed at different points of the input space.<ref name=\"Campolongo 2003\" />\n\n==See also==\n*[[Monte Carlo method]]\n\n== References ==\n{{Reflist}}\n\n==External links==\n* [http://www.abe.ufl.edu/Faculty/jjones/ABE_5646/2010/Morris.1991%20SA%20paper.pdf Morris method paper]\n*{{cite journal|last=Campolongo, F., S. Tarantola and A. Saltelli. |title=Tackling quantitatively large dimensionality problems. |journal= Computer Physics Communications|volume= 1999|issue=1–2|pages= 75–85|year= 1999| ref=Campolongo|doi=10.1016/S0010-4655(98)00165-9|bibcode=1999CoPhC.117...75C}}\n\n[[Category:Statistical mechanics]]\n[[Category:Computational physics]]\n[[Category:Statistical approximations]]\n[[Category:Randomized algorithms]]"
    },
    {
      "title": "Rare disease assumption",
      "url": "https://en.wikipedia.org/wiki/Rare_disease_assumption",
      "text": "{{No footnotes|date=June 2015}}\nThe '''rare disease assumption''' is a mathematical [[wikt:assumption|assumption]] in [[epidemiology|epidemiologic]] [[case-control study|case-control studies]] where the [[hypothesis]] tests the association between an exposure and a disease. It is assumed that, if the [[prevalence]] of the disease is low, then the [[odds ratio]] approaches the [[relative risk]].\n\nCase control studies are relatively inexpensive and less time consuming than [[cohort study|cohort studies]].{{Citation needed|date=June 2015}} Since case control studies don't track patients over time, they can't establish [[relative risk]]. The case control study can, however, calculate the exposure-odds ratio, which, mathematically, is supposed to approach the relative risk as prevalence falls.\n\nSome authors {{who|date=June 2015}} state that if the prevalence is 10% or less, the disease can be considered rare enough to allow the rare disease assumption. Unfortunately, the magnitude of discrepancy between the odds ratio and the relative risk is dependent not only on the prevalence, but also, to a great degree, on two other factors.\n\nThe following example will illustrate this difficulty clearly. Consider a standard table showing the association between two binary variables with frequencies ''a'' = true positives = 49,005,929, ''b'' = false positives = 50,994,071, ''c'' = false negatives = 50,994,071 and ''d'' = true negatives = 849,005,929. In this case the odds ratio (OR) is equal to 16 and the relative risk (RR) is equal to 8.65. Although the prevalence in our example equals 10% it is very difficult to apply the rare disease assumption because OR and RR can hardly be considered to be approximately the same. However, in this example the disease is not particularly \"rare\"; a 10% prevalence value means 1 in 10 people would have it. As the prevalence drops lower and lower, OR approaches the RR much more closely. This is one of the most problematic aspects of the rare disease assumption, since there is no threshold prevalence below which a disease is considered \"rare\", and thus no strict guideline to determine when the assumption applies.\n\n{| class=\"wikitable\"\n|-\n|\n! scope=\"col\" | Positive\n! scope=\"col\" | Negative\n|-\n! scope=row | True\n| 49,005,929\n| 849,005,929\n|-\n! scope=row | False\n| 50,994,071\n| 50,994,071\n|}\n\n==References==\n* {{cite journal |vauthors=Greenland S, Thomas DC |title=On the need for the rare disease assumption in case-control studies |journal=Am. J. Epidemiol. |volume=116 |issue=3 |pages=547–53 |date=September 1982 |pmid=7124721 |doi= |url=http://aje.oxfordjournals.org/cgi/pmidlookup?view=long&pmid=7124721}}\n* {{cite journal |vauthors=Cummings P, Koepsell TD |title=On the need for the rare disease assumption in some case-control studies |journal=Inj. Prev. |volume=7 |issue=3 |pages=254 |date=September 2001 |pmid=11565997 |pmc=1730752 |doi= 10.1136/ip.7.3.254-a|url=http://ip.bmj.com/cgi/pmidlookup?view=long&pmid=11565997}}\n* {{cite journal |vauthors=Greenland S, Thomas DC, Morgenstern H |title=The rare-disease assumption revisited. A critique of \"estimators of relative risk for case-control studies\" |journal=Am. J. Epidemiol. |volume=124 |issue=6 |pages=869–83 |date=December 1986 |pmid=3776970 |doi= |url=http://aje.oxfordjournals.org/cgi/pmidlookup?view=long&pmid=3776970}}\n* {{cite journal |vauthors=Bjerre LM, LeLorier J |title=Expressing the magnitude of adverse effects in case-control studies: \"the number of patients needed to be treated for one additional patient to be harmed\" |journal=BMJ |volume=320 |issue=7233 |pages=503–6 |date=February 2000 |pmid=10678870 |pmc=1127536 |doi= 10.1136/bmj.320.7233.503|url=http://bmj.com/cgi/pmidlookup?view=long&pmid=10678870}}\n\n[[Category:Epidemiology]]\n[[Category:Medical statistics]]\n[[Category:Statistical hypothesis testing]]\n[[Category:Statistical approximations]]\n\n\n{{statistics-stub}}"
    },
    {
      "title": "Rule of three (statistics)",
      "url": "https://en.wikipedia.org/wiki/Rule_of_three_%28statistics%29",
      "text": "{{About|one meaning of \"rule of three\" in statistics|other meanings in mathematics and beyond|Rule of three (disambiguation){{!}}Rule of three}}\n[[File:Rule of three.svg|thumb|300px|Comparison of the rule of three to the exact binomial one-sided confidence interval with no positive samples]]\n\nIn [[statistical analysis]], the '''rule of three''' states that if a certain event did not occur in a sample with {{mvar|n}} [[Design of experiments|subjects]], the interval from 0 to 3/{{mvar|n}} is a 95% [[confidence interval]] for the rate of occurrences in the [[population (statistics)|population]]. When {{mvar|n}} is greater than 30, this is a good approximation of results from more sensitive tests. For example, a pain-relief drug is tested on 1500 [[Human subjects research|human subjects]], and no [[adverse event]] is recorded. From the rule of three, it can be concluded with 95% confidence that fewer than 1 person in 500 (or 3/1500) will experience an adverse event. By symmetry, one could expect for only successes, the 95% confidence interval is {{nowrap|[1−3/{{mvar|n}},1]}}.\n\nThe rule is useful in the interpretation of [[clinical trial]]s generally, particularly in [[Phases of clinical research#Phase II|phase&nbsp;II]] and phase&nbsp;III where often there are limitations in duration or [[statistical power]]. The rule of three applies well beyond medical research, to any trial done {{mvar|n}} times. If 300 parachutes are randomly tested and all open successfully, then it is concluded with 95% confidence that fewer than 1 in 100 parachutes with the same characteristics (3/300) will fail.<ref>There are other meanings of the term \"rule of three\" in mathematics, and a further distinct meaning within statistics:<blockquote>A century and a half ago Charles Darwin said he had \"no Faith in anything short of actual measurement and the [[Cross-multiplication#Rule_of_Three|Rule of Three]],\" by which he appeared to mean the peak of arithmetical accomplishment in a nineteenth-century gentleman, solving for {{mvar|x}} in \"6&nbsp;is to 3 as 9 is to&nbsp;{{mvar|x}}.\" Some decades later, in the early 1900s, Karl Pearson shifted the meaning of the rule of three&nbsp;– \"take 3σ <nowiki>[</nowiki>[[three-sigma rule|three standard deviations]]<nowiki>]</nowiki> as definitely significant\"&nbsp;– and claimed it for his new journal of significance testing, ''Biometrika''. Even Darwin late in life seems to have fallen into the confusion. (Ziliak and McCloskey, 2008, p.&nbsp;26; parenthetic gloss in original)</blockquote></ref>\n\n==Derivation==\nA 95% [[confidence interval]] is sought for the probability ''p'' of an event occurring for any randomly selected single individual in a population, given that it has not been observed to occur in ''n'' [[Bernoulli trial]]s. Denoting the number of events by ''X'', we therefore wish to find the values of the parameter ''p'' of a [[binomial distribution]] that give Pr(''X'' = 0) ≥ 0.05. The rule can then be derived<ref>\"Professor Mean\" (2010) [http://www.pmean.com/01/zeroevents.html \"Confidence interval with zero events\"], The Children's Mercy Hospital. Retrieved 2013-01-01.</ref> either from the [[binomial distribution#Poisson approximation|Poisson approximation to the binomial distribution]], or from the formula (1−''p'')<sup>''n''</sup> for the probability of zero events in the binomial distribution. In the latter case, the edge of the confidence interval is given by Pr(''X'' = 0) = 0.05 and hence (1−''p'')<sup>''n''</sup> = .05 so ''n'' [[natural logarithm|ln]](1–''p'') = ln .05 ≈ −2.996. Rounding the latter to −3 and using the approximation, for ''p'' close to 0, that ln(1−''p'') ≈ −''p'', we obtain the interval's boundary 3/''n''.\n\nBy a similar argument, the numerator values of 3.51, 4.61, and 5.3 may be used for the 97%, 99%, and 99.5% confidence intervals, respectively, and in general the upper end of the confidence interval can be given as <math>\\frac{-ln(\\alpha)}{n}</math>, where <math>1-\\alpha</math> is the desired confidence level.\n\n==Extension==\nThe [[Vysochanskij–Petunin inequality]] shows that the rule of three holds for [[Unimodal function|unimodal]] distributions with finite [[variance]] beyond just the binomial distribution, and gives a way to change the factor 3 if a different confidence is desired. [[Chebyshev's inequality]] removes the assumption of unimodality at the price of a higher multiplier (about 4.5 for 95% confidence). [[Cantelli's inequality]] is the one-tailed version of Chebyshev's inequality.\n\n==See also==\n*[[Binomial proportion confidence interval]]\n*[[Rule of succession]]\n\n==Notes==\n{{reflist}}\n\n==References==\n\n*{{Cite journal\n| volume = 311\n| issue = 7005\n| pages = 619–620\n| last = Eypasch\n| first = Ernst\n|author2=Rolf Lefering |author3=C. K. Kum |author4=Hans Troidl\n | title = Probability of adverse events that have not yet occurred: A statistical reminder\n| journal = BMJ\n| accessdate = 2008-04-15\n| date = 1995\n| url = http://www.bmj.com/cgi/content/full/311/7005/619\n| pmid = 7663258\n| pmc = 2550668\n| doi=10.1136/bmj.311.7005.619\n}}\n*{{Cite journal\n| volume = 249\n| issue = 13\n| pages = 1743–5\n| last = Hanley\n| first = J. A.\n|author2=A. Lippman-Hand\n | title = If nothing goes wrong, is everything alright?\n| journal = JAMA\n| date = 1983\n| pmid = 6827763\n| doi=10.1001/jama.1983.03330370053031 }}\n\n* Ziliak, S. T.; D. N. McCloskey (2008). ''The cult of statistical significance: How the standard error costs us jobs, justice, and lives''. University of Michigan Press. {{ISBN|0472050079}}\n\n[[Category:Clinical trials]]\n[[Category:Statistical approximations]]\n[[Category:Medical statistics]]\n[[Category:Nursing research]]"
    },
    {
      "title": "Target function",
      "url": "https://en.wikipedia.org/wiki/Target_function",
      "text": "#REDIRECT [[Function approximation]]\n[[Category:Regression analysis]]\n[[Category:Statistical approximations]]"
    },
    {
      "title": "Taylor expansions for the moments of functions of random variables",
      "url": "https://en.wikipedia.org/wiki/Taylor_expansions_for_the_moments_of_functions_of_random_variables",
      "text": "{{Refimprove|date=November 2014}}\n<!-- ==CDF method== will do this later -->\nIn [[probability theory]], it is possible to approximate the [[moment (mathematics)|moments]] of a function ''f'' of a [[random variable]] ''X'' using [[Taylor expansion]]s, provided that ''f'' is sufficiently differentiable and that the moments of ''X'' are finite.  \n<!--\n::{|\n|-\n|<math>\\mu</math>\n|<math> = \\operatorname{E}\\left[X\\right]</math>\n|-\n|<math>\\sigma^2</math> \n|<math> = \\operatorname{var}\\left[X\\right]</math>\n|}-->\n\n==First moment==\n: <math>\n\\begin{align}\n\\operatorname{E}\\left[f(X)\\right] & {} = \\operatorname{E}\\left[f\\left(\\mu_X + \\left(X - \\mu_X\\right)\\right)\\right] \\\\\n& {} \\approx \\operatorname{E}\\left[f(\\mu_X) + f'(\\mu_X)\\left(X-\\mu_X\\right) + \\frac{1}{2}f''(\\mu_X) \\left(X - \\mu_X\\right)^2 \\right].\n\\end{align}\n</math>\n\nSince <math>E[X-\\mu_X]=0,</math> the second term disappears. Also <math>E[(X-\\mu_X)^2]</math> is <math>\\sigma_X^2</math>. Therefore,\n\n:<math>\\operatorname{E}\\left[f(X)\\right]\\approx f(\\mu_X) +\\frac{f''(\\mu_X)}{2}\\sigma_X^2</math>\nwhere <math>\\mu_X</math> and <math>\\sigma^2_X</math> are the mean and variance of X respectively.<ref name=\"Benaroya\">Haym Benaroya, Seon Mi Han, and Mark Nagurka. ''Probability Models in Engineering and Science''. CRC Press, 2005.</ref>\n\nIt is possible to generalize this to functions of more than one variable using [[Taylor expansion#Taylor series in several variables|multivariate Taylor expansions]]. For example,\n\n:<math>\\operatorname{E}\\left[\\frac{X}{Y}\\right]\\approx\\frac{\\operatorname{E}\\left[X\\right]}{\\operatorname{E}\\left[Y\\right]} -\\frac{\\operatorname{cov}\\left[X,Y\\right]}{\\operatorname{E}\\left[Y\\right]^2}+\\frac{\\operatorname{E}\\left[X\\right]}{\\operatorname{E}\\left[Y\\right]^3}\\operatorname{var}\\left[Y\\right]</math>\n\n==Second moment==\nSimilarly,<ref name=\"Benaroya\" />\n\n:<math>\\operatorname{var}\\left[f(X)\\right]\\approx \\left(f'(\\operatorname{E}\\left[X\\right])\\right)^2\\operatorname{var}\\left[X\\right] = \\left(f'(\\mu_X)\\right)^2\\sigma^2_X</math>\n\nThe above is using a first order approximation unlike for the method used in estimating the first moment. It will be a poor approximation in cases where <math>f(X)</math> is highly non-linear. This is a special case of the [[delta method]]. For example,\n\n:<math>\\operatorname{var}\\left[\\frac{X}{Y}\\right]\\approx\\frac{\\operatorname{var}\\left[X\\right]}{\\operatorname{E}\\left[Y\\right]^2}-\\frac{2\\operatorname{E}\\left[X\\right]}{\\operatorname{E}\\left[Y\\right]^3}\\operatorname{cov}\\left[X,Y\\right]+\\frac{\\operatorname{E}\\left[X\\right]^2}{\\operatorname{E}\\left[Y\\right]^4}\\operatorname{var}\\left[Y\\right].</math>\n\nThe second order approximation is<ref name=\"HendebyGustafsson\">{{cite web|last1=Hendeby|first1=Gustaf|last2=Gustafsson|first2=Fredrik|title=ON NONLINEAR TRANSFORMATIONS OF GAUSSIAN DISTRIBUTIONS|url=http://users.isy.liu.se/en/rt/fredrik/reports/07SSPut.pdf|accessdate=5 October 2017}}</ref>:\n\n:<math>\\operatorname{var}\\left[f(X)\\right]\\approx \\left(f'(\\operatorname{E}\\left[X\\right])\\right)^2\\operatorname{var}\\left[X\\right] + \\frac{\\left(f''(\\operatorname{E}\\left[X\\right])\\right)^2}{2}\\left(\\operatorname{var}\\left[X\\right]\\right)^2 = \\left(f'(\\mu_X)\\right)^2\\sigma^2_X + \\frac{1}{2}\\left(f''(\\mu_X)\\right)^2\\sigma_X^4</math>\n\n==See also==\n*[[Propagation of uncertainty]]\n*[[WKB approximation]]\n*[[Delta method]]\n\n==Notes==\n{{reflist}}\n\n==Further reading==\n*{{cite book |first=Kirk M. |last=Wolter |chapter=Taylor Series Methods |title=Introduction to Variance Estimation |location=New York |publisher=Springer |year=1985 |isbn=0-387-96119-4 |pages=221–247 |chapterurl=https://books.google.com/books?id=EadxTw0t2dMC&pg=PA221 }}\n\n{{DEFAULTSORT:Taylor Expansions For The Moments Of Functions Of Random Variables}}\n[[Category:Statistical approximations]]\n[[Category:Algebra of random variables]]\n[[Category:Moment (mathematics)]]"
    },
    {
      "title": "Three-point estimation",
      "url": "https://en.wikipedia.org/wiki/Three-point_estimation",
      "text": "The '''three-point estimation''' technique is used in management and [[information systems]] applications for the construction of an approximate [[probability distribution]] representing the outcome of future events, based on very limited information. While the distribution used for the approximation might be a [[normal distribution]], this is not always so and, for example a [[triangular distribution]] might be used, depending on the application.\n\nIn three-point estimation, three figures are produced initially for every distribution that is required, based on prior experience or best-guesses:\n* ''a'' = the best-case estimate\n* ''m'' = the most likely estimate\n* ''b'' = the worst-case estimate\nThese are then combined to yield either a full probability distribution, for later combination with distributions obtained similarly for other variables, or summary descriptors of the distribution, such as the [[mean]], [[standard deviation]] or [[percentile|percentage points]] of the distribution. The accuracy attributed to the results derived can be no better than the accuracy inherent in the 3 initial points, and there are clear dangers in using an assumed form for an underlying distribution that itself has little basis.\n\n==Estimation==\nBased on the assumption that a double-[[triangular distribution]]{{What?|date=February 2017|reason=The provided link says nothing about the DOUBLE-triangular distribution.}} governs the data, several estimates are possible. These values are used to calculate an ''E'' value for the estimate and a [[standard deviation]] (SD) as [[L-estimator]]s, where:\n\n: ''E'' = (''a'' + 4''m'' + ''b'') / 6\n: SD = (''b''&nbsp;&minus;&nbsp;''a'') / 6\n\n''E'' is a [[weighted average]] which takes into account both the most optimistic and most pessimistic estimates provided. SD measures the variability or uncertainty in the estimate.\nIn Project Evaluation and Review Techniques ([[PERT]]) the three values are used to fit a [[PERT distribution]] for [[Monte Carlo Method|Monte Carlo]] simulations.\n\nThe [[triangular distribution]] is also commonly used. It differs from the double-triangular by its simple triangular shape and by the property that the mode does not have to coincide with the median. The mean ([[expected value]]) is then:\n\n: ''E'' = (''a'' + ''m'' + ''b'') / 3.\n\nIn some applications,<ref name=MOD2007>Ministry of Defence (2007) [http://www.aof.mod.uk/aofcontent/tactical/risk/downloads/3pepracgude.pdf \"Three point estimates and quantitative risk analysis\"] [http://www.aof.mod.uk/aofcontent/tactical/risk/content/tpe.htm Policy, information and guidance on the Risk Management aspects of UK MOD Defence Acquisition]</ref> the triangular distribution is used directly as an estimated [[probability distribution]], rather than for the derivation of estimated statistics.\n\n==Project management==\nTo produce a project estimate the project manager:\n* Decomposes the project into a list of estimable tasks, i.e. a [[work breakdown structure]]\n* Estimates the expected value E(task) and the [[standard error]] SE(task) of this estimate for each task time\n* Calculates the expected value for the total project work time as <math>\\operatorname{E}(\\text{project}) = \\sum{ \\operatorname{E}(\\text{task})}</math> \n* Calculates the value SE(project) for the standard error of the estimated total project work time as: <math> \\operatorname{SE}(\\text{project}) = \\sqrt{\\sum{\\operatorname{SE}(\\text{task})^2}}</math> under the assumption that the project work time estimates are [[correlation|uncorrelated]]\n\nThe E and SE values are then used to convert the project time estimates to [[confidence interval]]s as follows:\n\n* The 68% confidence interval for the true project work time is approximately E(project) ± SE(project)\n* The 90% confidence interval for the true project work time is approximately E(project) ± 1.645 &times; SE(project)\n* The 95% confidence interval for the true project work time is approximately E(project) ± 2 &times; SE(project)\n* The 99.7% confidence interval for the true project work time is approximately E(project) ± 3 &times; SE(project)\n* Information Systems typically uses the 95% confidence interval for all project and task estimates.<ref>[[68-95-99.7 rule]]</ref>\n\nThese confidence interval estimates assume that the data from all of the tasks combine to be approximately normal (see [[Asymptotic distribution#Asymptotic normality|asymptotic normality]]). Typically, there would need to be 20&ndash;30 tasks for this to be reasonable, and each of the estimates E for the individual tasks would have to be unbiased.\n\n==See also==\n* [[Five-number summary]]\n* [[Seven-number summary]]\n* [[Program Evaluation and Review Technique]] (PERT)\n\n{{More footnotes|date=September 2010}}\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://www.4pm.com/articles/PERT_program_evaluation_&_review_technique.pdf Risk and duration estimates: 3 point estimating] from www.4pm.com\n{{Project cost estimation methods}}\n{{DEFAULTSORT:Three-Point Estimation}}\n[[Category:Statistical approximations]]\n[[Category:Informal estimation]]"
    },
    {
      "title": "Welch–Satterthwaite equation",
      "url": "https://en.wikipedia.org/wiki/Welch%E2%80%93Satterthwaite_equation",
      "text": "In [[statistics]] and [[propagation of uncertainty|uncertainty analysis]], the '''Welch–Satterthwaite equation''' is used to calculate an approximation to the effective [[degrees of freedom (statistics)|degrees of freedom]] of a [[linear combination]] of independent [[sample variance]]s, also known as the '''pooled degrees of freedom''',<ref>[https://books.google.com/books?id=0W1mAQAAQBAJ&pg=PA174&dq=%22pooled+degrees+of+freedom%22&hl=en&sa=X&ei=arcPVLCgMa_CsATQ2YL4Ag&redir_esc=y#v=onepage&q=%22pooled%20degrees%20of%20freedom%22&f=false]</ref><ref>[https://books.google.com/books?id=wOxoFIa7NeYC&pg=PA68&dq=%22pooled+degrees+of+freedom%22&hl=en&sa=X&ei=arcPVLCgMa_CsATQ2YL4Ag&redir_esc=y#v=onepage&q=%22pooled%20degrees%20of%20freedom%22&f=false]</ref> corresponding to the [[pooled variance]].\n\nFor {{math|''n''}} sample variances {{math|''s''<sub>''i''</sub><sup>2</sup> (''i'' {{=}} 1, ..., ''n'')}}, each respectively having {{math|''ν''<sub>''i''</sub>}} degrees of freedom, often one computes the linear combination\n\n:<math>\n  \\chi' = \\sum_{i=1}^n k_i s_i^2.\n</math>\n\nwhere <math>k_i </math> is a real positive number, typically <math> k_i=\\frac{1}{\\nu_i+1}</math>. In general, the [[probability distribution]] of {{math|''χ'''}} cannot be expressed analytically.  However, its distribution can be approximated by another [[chi-squared distribution]], whose effective degrees of freedom are given by the '''Welch–Satterthwaite equation'''\n\n:<math>\n \\nu_{\\chi'} \\approx \\frac{\\displaystyle\\left(\\sum_{i=1}^n k_i s_i^2\\right)^2}\n                          {\\displaystyle\\sum_{i=1}^n \\frac{(k_i s_i^2)^2}\n                                               {\\nu_i}\n                          }\n</math>\n\nThere is ''no'' assumption that the underlying population variances {{math|''σ<sub>i</sub>''<sup>2</sup>}} are equal. This is known as the [[Behrens–Fisher problem]].\n\nThe result can be used to perform approximate [[statistical inference]] tests.  The simplest application of this equation is in performing [[Welch's t-test|Welch's ''t''-test]].\n\n==See also==\n* [[Pooled variance]]\n\n==References==\n<references />\n\n==Further reading==\n* {{Citation | last = Satterthwaite | first = F. E. | title = An Approximate Distribution of Estimates of Variance Components.| journal = Biometrics Bulletin | volume = 2 | pages = 110–114 | year = 1946 | doi = 10.2307/3002019 }}\n* {{Citation | last = Welch | first = B. L. | title = The generalization of \"student's\" problem when several different population variances are involved. | journal = Biometrika | volume = 34 | pages = 28–35 | year = 1947 | doi=10.2307/2332510}}  \n* {{cite book \n  | last = Neter\n  | first = John \n  |author2=John Neter |author3=William Wasserman |author4=Michael H. Kutner\n   | title = Applied Linear Statistical Models\n  | publisher = Richard D. Irwin, Inc.\n  | date = 1990\n <!-- | pages = 851 -->\n  | isbn = 0-256-08338-X }}\n* Michael Allwood (2008) \"The Satterthwaite Formula for Degrees of Freedom in the Two-Sample ''t''-Test\", ''AP Statistics'', Advanced Placement Program, The College Board. [http://apcentral.collegeboard.com/apc/public/repository/ap05_stats_allwood_fin4prod.pdf]\n\n{{DEFAULTSORT:Welch-Satterthwaite equation}}\n[[Category:Statistical theorems]]\n[[Category:Equations]]\n[[Category:Statistical approximations]]"
    },
    {
      "title": "Welch's t-test",
      "url": "https://en.wikipedia.org/wiki/Welch%27s_t-test",
      "text": "{{DISPLAYTITLE:Welch's ''t''-test}}\nIn [[statistics]], '''Welch's ''t''-test''', or '''unequal variances ''t''-test''', is a two-sample [[location test]] which is used to test the hypothesis that two [[population (statistics)|populations]] have equal means. It is named for its creator, [[Bernard Lewis Welch]], and is an adaptation of [[Student's t-test|Student's ''t''-test]],<ref name=Welch1947>{{Cite journal | last = Welch | first = B. L. | title = The generalization of \"Student's\" problem when several different population variances are involved | journal = [[Biometrika]] | volume = 34 |issue=1–2 | pages = 28–35 | year = 1947 |doi =10.1093/biomet/34.1-2.28 | mr = 19277 }}</ref> and is more reliable when the two samples have unequal variances and/or unequal sample sizes.<ref name=Ruxton2006>{{Cite journal | last = Ruxton | first = G. D. | title = The unequal variance t-test is an underused alternative to Student's t-test and the Mann–Whitney U test |journal = [[Behavioral Ecology (journal)|Behavioral Ecology]] | volume = 17 | pages = 688–690 | year = 2006 | doi = 10.1093/beheco/ark016}}</ref><ref name=\"WhyWelch\">{{cite journal|last1=Derrick|first1=B|last2=Toher|first2=D|last3=White|first3=P|title=Why Welchs test is Type I error robust|journal=The Quantitative Methods for Psychology|date=2016|volume=12|issue=1|pages=30-38|doi=10.20982/tqmp.12.1.p030|url=http://eprints.uwe.ac.uk/27232/27/p030.pdf}}</ref> These tests are often referred to as \"unpaired\" or \"independent samples\" ''t''-tests, as they are typically applied when the statistical units underlying the two samples being compared are non-overlapping. Given that Welch's ''t''-test has been less popular than Student's ''t''-test<ref name=Ruxton2006/> and may be less familiar to readers, a more informative name is \"Welch's unequal variances ''t''-test\" or \"unequal variances ''t''-test\" for brevity.<ref name=WhyWelch/>\n\n==Assumptions==\n\nStudent's ''t''-test assumes that the two populations have normal distributions with equal variances. Welch's ''t''-test is designed for unequal variances, but the assumption of normality is maintained.<ref name=Welch1947/> Welch's ''t''-test is an approximate solution to the [[Behrens–Fisher problem]].\n\n==Calculations==\n\nWelch's ''t''-test defines the statistic ''t'' by the following formula:\n\n:<math>\nt \\quad = \\quad {\\; \\overline{X}_1 - \\overline{X}_2 \\; \\over \\sqrt{ \\; {s_1^2 \\over N_1} \\; + \\; {s_2^2 \\over N_2} \\quad }}\\,</math>\n\nwhere <math>\\overline{X}_1</math>, <math>s_1^2</math> and <math>N_1</math> are the 1st [[mean|sample mean]], sample [[variance]] and [[sample size]], respectively.  Unlike in [[Student's t test|Student's ''t''-test]], the denominator is ''not'' based on a [[pooled variance]] estimate.\n\nThe [[degrees of freedom (statistics)|degrees of freedom]] <math>\\nu</math>&nbsp; associated with this variance estimate is approximated using the [[Welch–Satterthwaite equation]]:\n\n:<math>\n\\nu \\quad  \\approx \\quad\n {{\\left( \\; {s_1^2 \\over N_1} \\; + \\; {s_2^2 \\over N_2} \\; \\right)^2 } \\over\n { \\quad {s_1^4 \\over N_1^2 \\nu_1} \\; + \\; {s_2^4 \\over N_2^2 \\nu_2 } \\quad }}\n</math>\n\nHere <math>\\nu_1 = N_1-1</math>, the degrees of freedom associated with the first variance estimate. <math>\\nu_2 = N_2-1</math>, the degrees of freedom associated with the 2nd variance estimate.\n\n==Statistical test==\n\nOnce ''t'' and ''<math>\\nu</math>'' have been computed, these statistics can be used with the [[Student's t-distribution|''t''-distribution]] to test one of two possible [[null hypothesis|null hypotheses]]: \n* that the two population means are equal, in which a [[two-tailed test]] is applied; or \n* that one of the population means is greater than or equal to the other, in which a [[one-tailed test]] is applied. \n{{citation needed span|date=January 2019|text=The approximate degrees of freedom are rounded down to the nearest integer.}}\n\n==Advantages and limitations==\n\nWelch's ''t''-test is more robust than Student's ''t''-test and maintains [[Type I and type II errors|type I error rates]] close to nominal for unequal variances and for unequal sample sizes under normality. Furthermore, the [[Power (statistics)|power]] of Welch's ''t''-test comes close to that of Student's ''t''-test, even when the population variances are equal and sample sizes are balanced.<ref name=Ruxton2006/> Welch's ''t''-test can be generalized to more than 2-samples,<ref name=\"Welch1951\">{{cite journal|last1=Welch|first1=B. L.|title=On the Comparison of Several Mean Values: An Alternative Approach|journal=Biometrika|date=1951|volume=38|pages=330–336|doi=10.2307/2332579|jstor=2332579}}</ref> which is more robust than [[one-way analysis of variance]] (ANOVA).\n\nIt is ''not recommended'' to pre-test for equal variances and then choose between Student's ''t''-test or Welch's ''t''-test.<ref name=Zimmerman2004>{{Cite journal | last = Zimmerman | first = D. W. | title = A note on preliminary tests of equality of variances | journal = [[British Journal of Mathematical and Statistical Psychology]] | volume = 57 | pages = 173–181 | year = 2004 | doi = 10.1348/000711004849222}}</ref> Rather, Welch's ''t''-test can be applied directly and without any substantial disadvantages to Student's ''t''-test as noted above. Welch's ''t''-test remains robust for skewed distributions and large sample sizes.<ref name=Fagerland2012>{{Cite journal | last = Fagerland | first = M. W. | title = t-tests, non-parametric tests, and large studies—a paradox of statistical practice? | journal = [[BioMed Central]] Medical Research Methodology | volume = 12 | pages = 78 | year = 2012 | doi = 10.1186/1471-2288-12-78| pmc = 3445820 }}</ref> Reliability decreases for skewed distributions and smaller samples, where one could possibly perform Welch's ''t''-test.<ref name=Fagerland2009>{{Cite journal | last1 = Fagerland | first1 = M. W. | last2 = Sandvik | first2 = L. | title = Performance of five two-sample location tests for skewed distributions with unequal variances | journal = [[Contemporary Clinical Trials]] | volume = 30 | pages = 490–496 | year = 2009 | doi=10.1016/j.cct.2009.06.007}}</ref>\n\n==Examples==\n\nThe following three examples compare Welch's ''t''-test and Student's ''t''-test. Samples are from random normal distributions using the [[R (programming language)|R programming language]].\n\nFor all three examples, the population means were <math>\\mu_1 = 20</math> and <math>\\mu_2 = 22</math>.\n\nThe first example is for equal variances (<math>\\sigma_1^2 = \\sigma_2^2 = 4</math>) and equal sample sizes (<math>N_1 = N_2 = 15</math>). Let A1 and A2 denote two random samples:\n\n: <math>A_1 = \\{27.5, 21.0, 19.0, 23.6, 17.0, 17.9, 16.9, 20.1, 21.9, 22.6, 23.1, 19.6, 19.0, 21.7, 21.4\\}</math>\n\n: <math>A_2 = \\{27.1, 22.0, 20.8, 23.4, 23.4, 23.5, 25.8, 22.0, 24.8, 20.2, 21.9, 22.1, 22.9, 20.5, 24.4\\}</math>\n\nThe second example is for unequal variances (<math>\\sigma_1^2 = 16</math>, <math>\\sigma_2^2 = 1</math>) and unequal sample sizes (<math>N_1 = 10</math>, <math>N_2 = 20</math>). The smaller sample has the larger variance:\n\n: <math>\\begin{align}\nA_1 &= \\{17.2, 20.9, 22.6, 18.1, 21.7, 21.4, 23.5, 24.2, 14.7, 21.8\\}\n\\\\\nA_2 &= \\{21.5, 22.8, 21.0, 23.0, 21.6, 23.6, 22.5, 20.7, 23.4, 21.8, 20.7, 21.7, 21.5, 22.5, 23.6, 21.5, 22.5, 23.5, 21.5, 21.8\\}\n\\end{align}</math>\n\nThe third example is for unequal variances (<math>\\sigma_1^2 = 1</math>, <math>\\sigma_2^2 = 16</math>) and unequal sample sizes (<math>N_1 = 10</math>, <math>N_2 = 20</math>). The larger sample has the larger variance:\n\n: <math>\\begin{align}\nA_1 &= \\{19.8, 20.4, 19.6, 17.8, 18.5, 18.9, 18.3, 18.9, 19.5, 22.0\\}\n\\\\\nA_2 &= \\{28.2, 26.6, 20.1, 23.3, 25.2, 22.1, 17.7, 27.6, 20.6, 13.7, 23.2, 17.5, 20.6, 18.0, 23.9, 21.6, 24.3, 20.4, 24.0, 13.2\\}\n\\end{align}</math>\n\nReference p-values were obtained by simulating the distributions of the ''t'' statistics for the null hypothesis of equal population means (<math>\\mu_1 - \\mu_2 =0</math>). Results are summarised in the table below, with two-tailed p-values:\n\n{| border=\"1\" cellspacing=\"0\" cellpadding=\"5\" align=\"center\" class=\"wikitable\"\n|-\n! colspan=\"1\" |\n! colspan=\"3\" align=\"center\"  | Sample A1\n! colspan=\"3\" align=\"center\"  | Sample A2\n! colspan=\"4\" align=\"center\" | Student's ''t''-test\n! colspan=\"4\" align=\"center\" | Welch's ''t''-test\n|-\n! align=\"center\" | Example\n! align=\"center\" | <math>N_1</math> || align=\"center\" | <math>\\overline{X}_1</math> || align=\"center\" | <math>s_1^2</math>\n! align=\"center\" | <math>N_2</math> || align=\"center\" | <math>\\overline{X}_2</math> || align=\"center\" | <math>s_2^2</math>\n! align=\"center\" | {{tmath|t}} || align=\"center\" | {{tmath|\\nu}} || align=\"center\" | {{tmath|P}} || align=\"center\" | <math>P_\\mathrm{sim}</math>\n! align=\"center\" | {{tmath|t}} || align=\"center\" | {{tmath|\\nu}} || align=\"center\" | {{tmath|P}} || align=\"center\" | <math>P_\\mathrm{sim}</math>\n|-\n| 1 || 15 || 20.8 || 7.9 || 15 || 23.0 || 3.8 || −2.46 || 28 || 0.021 || 0.021 || −2.46 || 25.0 || 0.021 || 0.017\n|-\n| 2 || 10 || 20.6 || 9.0 || 20 || 22.1 || 0.9 || −2.10 || 28 || 0.045 || 0.150 || −1.57 || 9.9 || 0.149 || 0.144\n|-\n| 3 || 10 || 19.4 || 1.4 || 20 || 21.6 || 17.1 || −1.64 || 28 || 0.110 || 0.036 || −2.22 || 24.5 || 0.036 || 0.042\n|-\n|}\n\nWelch's ''t''-test and Student's ''t''-test gave identical results when the two samples have identical variances and sample sizes (Example 1). But note that if you sample data from populations with identical variances, the sample variances will differ, as will the results of the two t-tests. So with actual data, the two tests will almost always give somewhat different results.\n\nFor unequal variances, Student's ''t''-test gave a low p-value when the smaller sample had a larger variance (Example 2) and a high p-value when the larger sample had a larger variance (Example 3). For unequal variances, Welch's ''t''-test gave p-values close to simulated p-values.\n\n==Software implementations==\n{| class=\"wikitable sortable\"\n|-\n! Language/Program !! Function !! Notes\n|-\n| [[LibreOffice]] || <code>TTEST(''Data1; Data2; Mode; Type'')</code> || See [https://help.libreoffice.org/Calc/Statistical_Functions_Part_Five#TTEST]\n|-\n| [[MATLAB]] || <code>ttest2(data1, data2, 'Vartype', 'unequal')</code>  || See [http://uk.mathworks.com/help/stats/ttest2.html]\n|-\n| [[Microsoft Excel]] pre 2010 || <code>TTEST(''array1'', ''array2'', ''tails'', ''type'')</code> || See [http://office.microsoft.com/en-us/excel-help/ttest-HP005209325.aspx]\n|-\n| [[Microsoft Excel]] 2010 and later || <code>T.TEST(''array1'', ''array2'', ''tails'', ''type'')</code> || See [http://office.microsoft.com/en-us/excel-help/t-test-function-HA102753135.aspx]\n|-\n| [[Minitab]] || Access commands through menu:  see <ref>[http://support.minitab.com/en-us/minitab/18/help-and-how-to/statistics/basic-statistics/how-to/2-sample-t/before-you-start/example/ Example of 2-Sample t - Minitab:] — official documentation for Minitab version 18.  Accessed 2019-01-22.</ref> || <ref>[http://support.minitab.com/en-us/minitab/18/help-and-how-to/statistics/basic-statistics/how-to/2-sample-t/perform-the-analysis/select-the-analysis-options/#assume-equal-variances Select the analysis options for 2-Sample t - Minitab:] — official documentation for Minitab version 18.  Accessed 2019-01-22.</ref>\n|-\n| [[SAS (Software)]] || default output from <code>proc ttest</code> (labeled \"Satterthwaite\")\n|-\n| [[Python (programming language)|Python]] || <code>scipy.stats.ttest_ind(''a'', ''b'', ''axis=0'', ''equal_var=False'')</code> || See [http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html]\n|-\n| [[R (programming language)|R]] || <code>t.test(data1, data2, alternative=\"two.sided\", var.equal=FALSE)</code>  || See [https://stat.ethz.ch/R-manual/R-devel/library/stats/html/t.test.html]\n|-\n| [[Haskell (programming language)|Haskell]] || <code>Statistics.Test.StudentT.welchTTest SamplesDiffer data1 data2</code> || See [http://hackage.haskell.org/package/statistics-0.15.0.0/docs/Statistics-Test-StudentT.html]\n|-\n| [[JMP (statistical software)|JMP]] || <code> Oneway( Y( YColumn), X( XColumn), Unequal Variances( 1 ) );</code> || See [https://www.jmp.com/support/help/]\n|-\n| [[Julia (programming language)|Julia]] || <code> UnequalVarianceTTest(data1, data2)</code> || See [http://hypothesistestsjl.readthedocs.org/en/latest/index.html]\n|-\n|[[Stata]]\n|<code>'''ttest''' ''varname1'' '''==''' ''varname2''''',''' '''welch'''</code>\n|See [http://www.stata.com/help.cgi?ttest]\n|-\n|[[Google Docs, Sheets and Slides|Google Sheets]]\n|<code>TTEST(range1, range2, tails, type)</code>\n|See [https://support.google.com/docs/answer/6055837?hl=en]\n|-\n|[[GraphPad Prism]]\n|It is a choice on the t test dialog.\n|-\n|[[SPSS|IBM SPSS Statistics]] || An option in the menu || <ref>Jeremy Miles (https://stats.stackexchange.com/users/17072/jeremy-miles), Unequal variances t-test or U Mann-Whitney test?, URL (version: 2014-04-11): https://stats.stackexchange.com/q/93475</ref>.  ''Cf.'' <ref>[https://www.ibm.com/support/knowledgecenter/SSLVMB_24.0.0/spss/base/syn_t-test_examples.html#syn_t-test_examples ] — Official documentation for SPSS Statistics version 24.  Accessed 2019-01-22.</ref>\n|\n|}\n\n==See also==\n{{Portal|Statistics}}\n* [[Student's t-test|Student's ''t''-test]]\n* [[Z-test|''Z''-test]]\n* [[Factorial experiment]]\n* [[One-way analysis of variance]]\n* [[Hotelling's two-sample T-squared statistic]], a multivariate extension of Welch's ''t''-test\n\n==References==\n{{Reflist|30em}}\n\n[[Category:Statistical approximations]]\n[[Category:Statistical tests]]"
    },
    {
      "title": "List of mathematical identities",
      "url": "https://en.wikipedia.org/wiki/List_of_mathematical_identities",
      "text": "This page lists '''[[identity (mathematics)|mathematical identities]]''', that is, ''identically true relations'' holding in [[mathematics]].\n\n\n* [[Bézout's identity]] (despite its usual name, it is not, properly speaking, an identity)\n* [[Binomial inverse theorem]]\n* [[Binomial theorem|Binomial identity]]\n* [[Brahmagupta–Fibonacci identity|Brahmagupta–Fibonacci two-square identity]]\n* [[Giacomo Candido#Candido's identity|Candido's identity]]\n* [[Cassini and Catalan identities]]\n* [[Degen's eight-square identity]]\n* [[Difference of two squares]]\n* [[Euler's four-square identity]]\n* [[Euler's identity]]\n* Fibonacci's identity see [[Brahmagupta–Fibonacci identity]] or [[Cassini and Catalan identities]]\n* [[Heine's identity]]\n* [[Hermite's identity]]\n* [[Lagrange's identity]]\n* [[Lagrange's trigonometric identities]]\n* [[Enumerator polynomial#MacWilliams identity|MacWilliams identity]]\n* [[Matrix determinant lemma]]\n* [[Parseval's identity]]\n* [[Pfister's sixteen-square identity]]\n* [[Sherman–Morrison formula]]\n* [[Sun's curious identity]]\n* [[Sylvester's determinant identity]]\n* [[Vandermonde's identity]]\n* [[Woodbury matrix identity]]\n\n== Identities for classes of functions ==\n* [[Logarithmic identities]]\n* [[Trigonometric identity|Trigonometric identities]]\n* [[Hypergeometric function identities]]\n* [[Summation identities]]\n* Fibonacci identities: [[Fibonacci number#Combinatorial identities|Combinatorial Fibonacci identities]] and [[Fibonacci number#Other identities|Other Fibonacci identities]]\n\n==External links==\n* [http://encyclopedia-of-equation.webnode.jp/ Encyclopedia of Equation]   Online  encyclopedia of mathematical identities\n*[http://sites.google.com/site/tpiezas/Home A Collection of Algebraic Identities]\n*[http://www.ee.ic.ac.uk/hp/staff/dmb/matrix/identity.html Matrix Identities]\n\n==See also==\n*[[List of inequalities]]\n\n[[Category:Mathematical identities|*]]\n[[Category:Mathematics-related lists|Identities]]"
    },
    {
      "title": "Binet–Cauchy identity",
      "url": "https://en.wikipedia.org/wiki/Binet%E2%80%93Cauchy_identity",
      "text": "In [[algebra]], the '''Binet–Cauchy identity''', named after [[Jacques Philippe Marie Binet]] and [[Augustin-Louis Cauchy]], states that<ref name=Weisstein>\n\n{{cite book |title=CRC concise encyclopedia of mathematics |author=Eric W. Weisstein |page=228 |url=https://books.google.com/books?id=8LmCzWQYh_UC&pg=PA228 |chapter=Binet-Cauchy identity |isbn=1-58488-347-2 |year=2003 |edition=2nd |publisher=CRC Press}}\n\n</ref>\n\n: <math>\n\\biggl(\\sum_{i=1}^n a_i c_i\\biggr)\n\\biggl(\\sum_{j=1}^n b_j d_j\\biggr) = \n\\biggl(\\sum_{i=1}^n a_i d_i\\biggr)\n\\biggl(\\sum_{j=1}^n b_j c_j\\biggr) \n+ \\sum_{1\\le i < j \\le n} \n(a_i b_j - a_j b_i ) \n(c_i d_j - c_j d_i )\n</math>\n\nfor every choice of [[real number|real]] or [[complex number]]s (or more generally, elements of a [[commutative ring]]).\nSetting ''a<sub>i</sub>''&nbsp;=&nbsp;''c<sub>i</sub>'' and ''b<sub>j</sub>''&nbsp;=&nbsp;''d<sub>j</sub>'', it gives the [[Lagrange's identity]], which is a stronger version of the [[Cauchy–Schwarz inequality]] for the [[Euclidean space]] <math>\\textstyle\\mathbb{R}^n</math>.\n\n==The Binet–Cauchy identity and exterior algebra==\nWhen {{math|1=''n'' = 3}}, the first and second terms on the right hand side become the squared magnitudes of [[Dot product|dot]] and [[cross product]]s respectively; in {{math|''n''}} dimensions these become the magnitudes of the dot and [[wedge product]]s. We may write it\n\n:<math>(a \\cdot c)(b \\cdot d) = (a \\cdot d)(b \\cdot c) + (a \\wedge b) \\cdot (c \\wedge d)</math>\n\nwhere {{math|'''a'''}}, {{math|'''b'''}}, {{math|'''c'''}}, and {{math|'''d'''}} are vectors. It may also be written as a formula giving the dot product of two wedge products, as \n\n:<math>(a \\wedge b) \\cdot (c \\wedge d) = (a \\cdot c)(b \\cdot d) - (a \\cdot d)(b \\cdot c)\\,,</math>\nwhich can be written as \n:<math>(a \\times b) \\cdot (c \\times d) = (a \\cdot c)(b \\cdot d) - (a \\cdot d)(b \\cdot c)</math>\nin the {{math|1=''n'' = 3}} case.\n\nIn the special case {{math|1='''a''' = '''c'''}} and {{math|1='''b''' = '''d'''}}, the formula yields \n:<math>|a \\wedge b|^2 = |a|^2|b|^2 - |a \\cdot b|^2. </math>\n\nWhen both {{math|'''a'''}} and {{math|'''b'''}} are unit vectors, we obtain the usual relation\n:<math>\\sin^2 \\phi = 1 - \\cos^2 \\phi</math>\nwhere {{math|''φ''}} is the angle between the vectors.\n\n==Einstein notation==\nA relationship between the [[Levi-Civita symbol|Levi–Cevita symbol]]s and the generalized [[Kronecker delta]] is\n:<math>\\frac{1}{k!}\\varepsilon^{\\lambda_1\\cdots\\lambda_k\\mu_{k+1}\\cdots\\mu_{n}} \\varepsilon_{\\lambda_1\\cdots\\lambda_k\\nu_{k+1}\\cdots\\nu_{n}} = \\delta^{\\mu_{k+1}\\cdots\\mu_{n}}_{\\nu_{k+1}\\cdots\\nu_{n}}\\,.</math>\n\nThe <math>(a \\wedge b) \\cdot (c \\wedge d) = (a \\cdot c)(b \\cdot d) - (a \\cdot d)(b \\cdot c)</math> form of the Binet–Cauchy identity can be written as \n:<math>\\frac{1}{(n-2)!}\\left(\\varepsilon^{\\mu_1\\cdots\\mu_{n-2}\\alpha\\beta} ~ a_{\\alpha} ~ b_{\\beta} \\right)\\left( \\varepsilon_{\\mu_1\\cdots\\mu_{n-2}\\gamma\\delta} ~ c^{\\gamma} ~ d^{\\delta}\\right) = \\delta^{\\alpha\\beta}_{\\gamma\\delta} ~ a_{\\alpha} ~ b_{\\beta} ~ c^{\\gamma} ~ d^{\\delta}\\,.</math>\n\n==Proof==\nExpanding the last term,\n\n:<math>\n\\sum_{1\\le i < j \\le n} \n(a_i b_j - a_j b_i ) \n(c_i d_j - c_j d_i )\n</math>\n:<math>\n=\n\\sum_{1\\le i < j \\le n} \n(a_i c_i b_j d_j + a_j c_j b_i d_i)\n+\\sum_{i=1}^n a_i c_i b_i d_i\n-\n\\sum_{1\\le i < j \\le n} \n(a_i d_i b_j c_j + a_j d_j b_i c_i)\n-\n\\sum_{i=1}^n a_i d_i b_i c_i\n</math>\n\nwhere the second and fourth terms are the same and artificially added to complete the sums as follows:\n\n:<math>\n=\n\\sum_{i=1}^n \\sum_{j=1}^n\na_i c_i b_j d_j\n-\n\\sum_{i=1}^n \\sum_{j=1}^n\na_i d_i b_j c_j.\n</math>\n\nThis completes the proof after factoring out the terms indexed by ''i''.\n\n==Generalization==\nA general form, also known as the [[Cauchy–Binet formula]], states the following:\nSuppose ''A'' is an ''m''&times;''n'' [[matrix (mathematics)|matrix]] and ''B'' is an ''n''&times;''m'' matrix. If ''S'' is a [[subset]] of {1, ..., ''n''} with ''m'' elements, we write ''A<sub>S</sub>'' for the ''m''&times;''m'' matrix whose columns are those columns of ''A'' that have indices from ''S''. Similarly, we write ''B<sub>S</sub>'' for the ''m''&times;''m'' matrix whose ''rows'' are those rows of ''B'' that have indices from ''S''. \nThen the [[determinant]] of the [[matrix product]] of ''A'' and ''B'' satisfies the identity\n:<math>\\det(AB) = \\sum_{\\scriptstyle S\\subset\\{1,\\ldots,n\\}\\atop\\scriptstyle|S|=m} \\det(A_S)\\det(B_S),</math>\nwhere the sum extends over all possible subsets ''S'' of  {1, ..., ''n''} with ''m'' elements.\n\nWe get the original identity as special case by setting\n:<math>\nA=\\begin{pmatrix}a_1&\\dots&a_n\\\\b_1&\\dots& b_n\\end{pmatrix},\\quad\nB=\\begin{pmatrix}c_1&d_1\\\\\\vdots&\\vdots\\\\c_n&d_n\\end{pmatrix}.\n</math>\n\n==In-line notes and references==\n<references/>\n\n{{DEFAULTSORT:Binet-Cauchy Identity}}\n[[Category:Mathematical identities]]\n[[Category:Multilinear algebra]]\n[[Category:Articles containing proofs]]"
    }
  ]
}