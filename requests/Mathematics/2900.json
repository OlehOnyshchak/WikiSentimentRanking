{
  "pages": [
    {
      "title": "Gaussian process",
      "url": "https://en.wikipedia.org/wiki/Gaussian_process",
      "text": "In [[probability theory]] and [[statistics]], a '''Gaussian process''' is a [[stochastic process]] (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a [[multivariate normal distribution]], i.e. every finite [[linear combination]] of them is normally distributed. The distribution of a Gaussian process is the [[joint distribution]] of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.\n\nA machine-learning algorithm that involves a Gaussian process uses [[lazy learning]] and a measure of the similarity between points (the ''kernel function'') to predict the value for an unseen point from training data. The prediction is not just an estimate for that point, but also has uncertainty information—it is a one-dimensional Gaussian distribution (which is the [[marginal distribution]] at that point).<ref>{{cite web|url=http://platypusinnovation.blogspot.co.uk/2016/05/a-simple-intro-to-gaussian-processes.html|title=Platypus Innovation: A Simple Intro to Gaussian Processes (a great data modelling tool)|date=2016-05-10}}</ref>\n\nFor some kernel functions, matrix algebra can be used to calculate the predictions using the technique of [[kriging]]. When a parameterised kernel is used, optimisation software is typically used to fit a Gaussian process model.\n\nThe concept of Gaussian processes is named after [[Carl Friedrich Gauss]] because it is based on the notion of the Gaussian distribution ([[normal distribution]]). Gaussian processes can be seen as an infinite-dimensional generalization of multivariate normal distributions.\n\nGaussian processes are useful in [[statistical model]]ling, benefiting from properties inherited from the normal distribution. For example, if a [[random process]] is modelled as a Gaussian process, the distributions of various derived quantities can be obtained explicitly. Such quantities include the average value of the process over a range of times and the error in estimating the average using sample values at a small set of times.\n\n==Definition==\nA time continuous [[stochastic process]] <math>\\left\\{X_t ; t\\in T\\right\\}</math> is Gaussian [[if and only if]] for every [[finite set]] of [[indexed family|indices]] <math>t_1,\\ldots,t_k</math> in the index set <math>T</math>\n\n: <math> \\mathbf{X}_{t_1, \\ldots, t_k} = (X_{t_1}, \\ldots, X_{t_k}) </math>\n\nis a [[multivariate normal distribution|multivariate Gaussian]] [[random variable]].<ref name=DrMacKayGPNN>{{cite book| last=MacKay | first=David, J.C.| authorlink=David J.C. MacKay| title=Information Theory, Inference, and Learning Algorithms |year=2003 | pages=540|publisher=[[Cambridge University Press]]| isbn=9780521642989|url=http://www.inference.phy.cam.ac.uk/itprnn/book.pdf | quote=The probability distribution of a function <math>y(\\mathbf{x})</math> is a Gaussian processes if for any finite selection of points <math>\\mathbf{x}^{(1)},\\mathbf{x}^{(2)},\\ldots,\\mathbf{x}^{(N)}</math>, the density <math>P(y(\\mathbf{x}^{(1)}),y(\\mathbf{x}^{(2)}),\\ldots,y(\\mathbf{x}^{(N)}))</math>is a Gaussian}}</ref> That is the same as saying every linear combination of <math>\n (X_{t_1}, \\ldots, X_{t_k}) </math> has a univariate normal (or Gaussian) distribution.\n\nUsing [[Characteristic function (probability theory)|characteristic functions]] of random variables, the Gaussian property can be formulated as follows: <math>\\left\\{X_t ; t\\in T\\right\\}</math> is Gaussian if and only if, for every finite set of indices <math>t_1,\\ldots,t_k</math>, there are real-valued <math>\\sigma_{\\ell j}</math>, <math>\\mu_\\ell</math> with <math>\\sigma_{jj} > 0</math> such that the following equality holds for all <math>s_1,s_2,\\ldots,s_k\\in\\mathbb{R}</math>\n\n:<math> \\operatorname{E}\\left(\\exp\\left(i \\ \\sum_{\\ell=1}^k s_\\ell \\ \\mathbf{X}_{t_\\ell}\\right)\\right) = \\exp \\left(-\\frac{1}{2} \\, \\sum_{\\ell, j} \\sigma_{\\ell j} s_\\ell s_j + i \\sum_\\ell \\mu_\\ell s_\\ell\\right)</math>.\n\nwhere <math>i</math> denotes the [[imaginary unit]] such that <math>i^2 =-1</math>.\n\nThe numbers <math>\\sigma_{\\ell j}</math> and <math>\\mu_\\ell</math> can be shown to be the [[covariance]]s and [[mean (mathematics)|means]] of the variables in the process.<ref>{{cite book |last=Dudley |first=R.M. |title=Real Analysis and Probability |year=1989 |publisher=Wadsworth and Brooks/Cole}}</ref>\n\n==Variance==\nThe variance of a Gaussian process is finite at any time <math>t</math>, formally<ref name=\"Lapidoth2017\">{{cite book|author=Amos Lapidoth|title=A Foundation in Digital Communication|url=https://books.google.com/books?id=6oTuDQAAQBAJ&printsec=frontcover#v=onepage&q=independence&f=false|date=8 February 2017|publisher=Cambridge University Press|isbn=978-1-107-17732-1}}</ref>{{rp|p. 515}}\n:<math> \\operatorname{var}[X(t)] = \\operatorname{E}[|X(t)-\\operatorname{E}[X(t)]|^2] < \\infty \\quad \\text{for all } t \\in T</math>.\n\n==Stationarity==\nFor general stochastic processes [[Stationary process#strict-sense stationarity|strict-sense stationarity]] implies [[Stationary process#wide-sense stationarity|wide-sense stationarity]] but not every wide-sense stationary stochastic process is strict-sense stationary. However, for a Gaussian stochastic process the two concepts are equivalent.<ref name=\"Lapidoth2017\"/>{{rp|p. 518}}\n\nA Gaussian stochastic process is strict-sense stationary if, and only if, it is wide-sense stationary.\n\n==Covariance functions==\nA key fact of Gaussian processes is that they can be completely defined by their second-order statistics.<ref name=\"prml\">{{cite book |last=Bishop |first=C.M. |title= Pattern Recognition and Machine Learning |year=2006 |publisher=[[Springer Science+Business Media|Springer]] |isbn=978-0-387-31073-2}}</ref> Thus, if a Gaussian process is assumed to have mean zero, defining the [[covariance function]] completely defines the process' behaviour. Importantly the non-negative definiteness of  this function enables its spectral decomposition using the [[Karhunen–Loève theorem|Karhunen–Loève expansion]]. Basic aspects that can be defined through the covariance function are the process' [[stationary process|stationarity]], [[isotropy]], [[smoothness]] and [[periodic function|periodicity]].<ref name=\"brml\">{{cite book |last=Barber |first=David |title=Bayesian Reasoning and Machine Learning |url=http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.HomePage |year=2012 |publisher=[[Cambridge University Press]] |isbn=978-0-521-51814-7}}</ref><ref name=\"gpml\">{{cite book |last=Rasmussen |first=C.E. |author2=Williams, C.K.I |title=Gaussian Processes for Machine Learning |url=http://www.gaussianprocess.org/gpml/ |year=2006 |publisher=[[MIT Press]] |isbn=978-0-262-18253-9}}</ref>\n\n[[stationary process|Stationarity]] refers to the process' behaviour regarding the separation of any two points <math>x</math> and <math>x'</math>. If the process is stationary, it depends on their separation, <math>x-x'</math>, while if non-stationary it depends on the actual position of the points  <math>x</math> and <math>x'</math>. For example, the special case of an [[Ornstein–Uhlenbeck process]], a [[Brownian motion]] process, is stationary.\n\nIf the process depends only on <math>|x-x'|</math>, the Euclidean distance (not the direction) between <math>x</math> and <math>x'</math>,  then the process is considered isotropic. A process that is concurrently stationary and isotropic is considered to be [[homogeneous]];<ref name=\"PRP\">{{cite book |last=Grimmett  |first=Geoffrey |author2=David Stirzaker|title= Probability and Random Processes| year=2001 |publisher=[[Oxford University Press]] |isbn=978-0198572220}}</ref> in practice these properties reflect the differences (or rather the lack of them) in the behaviour of the process given the location of the observer.\n\nUltimately Gaussian processes translate as taking priors on functions and the smoothness of these priors can be induced by the covariance function.<ref name =\"brml\"/> If we expect that for \"near-by\" input points <math>x</math> and <math>x'</math> their corresponding output points <math>y</math> and <math>y'</math> to be \"near-by\" also, then the assumption of continuity is present. If we wish to allow for significant displacement then we might choose a rougher covariance function. Extreme examples of the behaviour is the Ornstein&ndash;Uhlenbeck covariance function and the squared exponential where the former is never differentiable and the latter infinitely differentiable.\n\nPeriodicity refers to inducing periodic patterns within the behaviour of the process. Formally, this is achieved by mapping the input <math>x</math> to a two dimensional vector <math>u(x) = \\left( \\cos(x), \\sin(x) \\right)</math>.\n\n===Usual covariance functions===\n[[File:Gaussian process draws from prior distribution.png|thumbnail|right|The effect of choosing different kernels on the prior function distribution of the Gaussian process. Left is a squared exponential kernel. Middle is Brownian. Right is quadratic.]]\nThere are a number of common covariance functions:<ref name=\"gpml\"/>\n*Constant : <math> K_\\operatorname{C}(x,x') = C </math>\n*Linear: <math> K_\\operatorname{L}(x,x') =  x^T x'</math>\n*white Gaussian noise: <math> K_\\operatorname{GN}(x,x') = \\sigma^2 \\delta_{x,x'}</math>\n*Squared exponential: <math> K_\\operatorname{SE}(x,x') = \\exp \\Big(-\\frac{|d|^2}{2\\ell^2} \\Big)</math>\n*Ornstein&ndash;Uhlenbeck: <math> K_\\operatorname{OU}(x,x') = \\exp \\left(-\\frac{|d|} \\ell \\right)</math>\n*Matérn: <math> K_\\operatorname{Matern}(x,x') = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\Big(\\frac{\\sqrt{2\\nu}|d|}{\\ell} \\Big)^\\nu K_\\nu \\Big(\\frac{\\sqrt{2\\nu}|d|}{\\ell} \\Big)</math>\n*Periodic: <math> K_\\operatorname{P}(x,x') = \\exp\\left(-\\frac{ 2\\sin^2\\left(\\frac d 2 \\right)}{\\ell^2} \\right)</math>\n*Rational quadratic: <math> K_\\operatorname{RQ}(x,x') =  (1+|d|^2)^{-\\alpha}, \\quad \\alpha \\geq 0</math>\n\nHere <math>d = x- x'</math>. The parameter ''ℓ'' is the characteristic length-scale of the process (practically, \"how close\" two points <math>x</math> and <math>x'</math> have to be to influence each other significantly), ''δ'' is the [[Kronecker delta]] and σ the [[standard deviation]] of the noise fluctuations. Moreover, <math>K_\\nu</math> is the [[modified Bessel function]] of order <math>\\nu</math> and <math>\\Gamma(\\nu)</math> is the [[gamma function]] evaluated at <math>\\nu</math>. Importantly, a complicated covariance function can be defined as a linear combination of other simpler covariance functions in order to incorporate different insights about the data-set at hand.\n\nClearly, the inferential results are dependent on the values of the hyperparameters ''θ'' (e.g. ''ℓ'' and ''σ'') defining the model's behaviour. A popular choice for ''θ'' is to provide ''[[maximum a posteriori]]'' (MAP) estimates of it with some chosen prior. If the prior is very near uniform, this is the same as maximizing the [[marginal likelihood]] of the process; the  marginalization being done over the observed process values <math>y</math>.<ref name= \"gpml\"/> This approach is also known as ''maximum likelihood II'', ''evidence maximization'', or ''[[empirical Bayes]]''.<ref name=\"seegerGPML\">{{cite journal |last1= Seeger| first1= Matthias |year= 2004 |title= Gaussian Processes for Machine Learning|journal= International Journal of Neural Systems|volume= 14|issue= 2|pages= 69–104 |doi=10.1142/s0129065704001899| pmid= 15112367 | citeseerx= 10.1.1.71.1079 }}</ref>\n\n==Continuity==\n\nFor a Gaussian process, [[Continuous stochastic process#Continuity in probability|continuity in probability]] is equivalent to [[Continuous stochastic process#Mean-square continuity|mean-square continuity]],\n<ref>{{cite book |chapterurl=https://www.mathunion.org/fileadmin/ICM/Proceedings/ICM1974.2/ICM1974.2.ocr.pdf |title=Proceedings of the International Congress of Mathematicians |editors= |first=R. M. |last=Dudley |authorlink=Richard M. Dudley |year=1975 |volume=2 |pages=143–146 |chapter=The Gaussian process and how to approach it}}</ref>{{rp|145}}\nand [[Continuous stochastic process#Continuity with probability one|continuity with probability one]] is equivalent to [[Continuous stochastic process#Sample continuity|sample continuity]].<ref>{{cite journal |first=R. M. |last=Dudley |authorlink=Richard M. Dudley |title=Sample functions of the Gaussian process |journal=[[Annals of Probability]] |volume=1 |issue=1 |pages=66–103 |year=1973 |quote=|doi=10.1007/978-1-4419-5821-1_13 |isbn=978-1-4419-5820-4 }}</ref>{{rp|91 \"Gaussian processes are discontinuous at fixed points.\"}}\nThe latter implies, but is not implied by, continuity in probability.\nContinuity in probability holds if and only if the [[autocovariance|mean and autocovariance]] are continuous functions. In contrast, sample continuity was challenging even for [[Gaussian process#Stationarity|stationary Gaussian processes]] (as probably noted first by [[Andrey Kolmogorov]]), and more challenging for more general processes.<ref>{{cite book |last=Talagrand  |first=Michel |author-link=Michel Talagrand |title=Upper and lower bounds for stochastic processes: modern methods and classical problems |year=2014 |publisher=Springer, Heidelberg |isbn=978-3-642-54074-5 |url=https://www.springer.com/gp/book/9783642540745}}</ref>{{rp|Sect. 2.8}}\n<ref>{{cite book |title=Lecture Notes in Mathematics |editors= |first=Michel |last=Ledoux |authorlink=Michel Ledoux |year=1994 |volume=1648 |publisher=Springer, Berlin |pages=165–294 |chapter=Isoperimetry and Gaussian analysis|doi=10.1007/BFb0095676 |title-link = Lecture Notes in Mathematics|isbn = 978-3-540-62055-6}}</ref>{{rp|69,81}}\n<ref>{{cite journal |last=Adler |first=Robert J. |author-link=Robert Adler |title=An introduction to continuity, extrema, and related topics for general Gaussian processes |journal=Lecture Notes-Monograph Series |volume=12 |pages=i–155 |year=1990 |publisher=Institute of Mathematical Statistics |isbn= |jstor=4355563 }}</ref>{{rp|80}}\n<ref>{{cite journal |title=Review of: Adler 1990 'An introduction to continuity...' |first=Simeon M. |last=Berman |date=1992 |journal=Mathematical Reviews|mr = 1088478}}</ref>\nAs usual, by a sample continuous process one means a process that admits a sample continuous [[Stochastic process#Modification|modification]].\n<ref name=Dudley67>{{cite journal |first=R. M. |last=Dudley |authorlink=Richard M. Dudley |title=The sizes of compact subsets of Hilbert space and continuity of Gaussian processes |journal=Journal of Functional Analysis |volume=1 |issue= 3|pages=290–330 |year=1967 |url=https://www.sciencedirect.com/science/article/pii/0022123667900171|doi=10.1016/0022-1236(67)90017-1 }}</ref>{{rp|292}}\n<ref name=MarcusShepp72>{{cite book |chapterurl=https://projecteuclid.org/euclid.bsmsp/1200514231 |title=Proceedings of the sixth Berkeley symposium on mathematical statistics and probability, vol. II: probability theory  |editors= |first1=M.B. |last1=Marcus |first2=Lawrence A. |last2=Shepp |authorlink2=Lawrence Shepp |year=1972 |volume= |publisher=Univ. California, Berkeley |pages=423–441 |chapter=Sample behavior of Gaussian processes}}</ref>{{rp|424}}\n\n===Stationary case===\n\nFor a stationary Gaussian process <math>X=(X_t)_{t\\in\\mathbb R},</math> some conditions on its spectrum are sufficient for sample continuity, but fail to be necessary. A necessary and sufficient condition, sometimes called Dudley-Fernique theorem, involves the function <math>\\sigma</math> defined by\n:<math> \\sigma(h) = \\sqrt{ \\mathbb E \\big( X(t+h) - X(t) \\big)^2 } </math>\n(the right-hand side does not depend on <math>t</math> due to stationarity). Continuity of <math>X</math> in probability is equivalent to continuity of <math>\\sigma</math> at <math>0.</math> When convergence of <math>\\sigma(h)</math> to <math>0</math> (as <math>h\\to 0</math>) is too slow, sample continuity of <math>X</math> may fail. Convergence of the following integrals matters:\n:<math> I(\\sigma) = \\int_0^1 \\frac{ \\sigma(h) }{ h \\sqrt{ \\log(1/h) } } \\, dh = \\int_0^\\infty 2\\sigma(\\mathbb e^{-x^2}) \\, dx ,</math>\nthese two integrals being equal according to [[integration by substitution]] <math> h = \\mathbb e^{-x^2}, </math> <math>\\textstyle x = \\sqrt{\\log(1/h)} .</math> The first integrand need not be bounded as <math>h\\to 0+,</math> thus the integral may converge (<math>I(\\sigma)<\\infty</math>) or diverge (<math>I(\\sigma)=\\infty</math>). Taking for example <math>\\sigma(\\mathbb e^{-x^2})=\\tfrac1{x^a}</math> for large <math>x,</math> that is, <math>\\sigma(h)=(\\log(1/h))^{-a/2}</math> for small <math>h,</math> one obtains <math>I(\\sigma)<\\infty</math> when <math>a>1,</math> and <math>I(\\sigma)=\\infty</math> when <math>0<a\\le 1.</math>\nIn these two cases the function <math>\\sigma</math> is increasing on <math>[0,\\infty),</math> but generally it is not. Moreover, the condition\n: <math>(*)</math> &nbsp; there exists <math>\\varepsilon>0</math> such that <math>\\sigma</math> is monotone on <math>[0,\\varepsilon]</math>\ndoes not follow from continuity of <math>\\sigma</math> and the evident relations <math>\\sigma(h)\\ge0</math> (for all <math>h</math>) and <math>\\sigma(0)=0.</math>\n\n'''Theorem 1.''' &nbsp; Let <math>\\sigma</math> be continuous and satisfy <math>(*).</math> Then the condition <math>I(\\sigma)<\\infty</math> is necessary and sufficient for sample continuity of <math>X.</math>\n\nSome history.<ref name=MarcusShepp72 />{{rp|424}}\nSufficiency was announced by [[Xavier Fernique]] in 1964, but the first proof was published by [[Richard M. Dudley]] in 1967.<ref name=Dudley67 />{{rp|Theorem 7.1}}\nNecessity was proved by Michael B. Marcus and [[Lawrence Shepp]] in 1970.<ref name=MarcusShepp70>{{cite journal |first1=Michael B. |last1=Marcus |first2=Lawrence A. |last2=Shepp |authorlink2=Lawrence Shepp |title=Continuity of Gaussian processes |journal=[[Transactions of the American Mathematical Society]] |volume=151 |issue= 2|pages=377–391 |year=1970 |doi=10.2307/1995502|jstor=1995502 }}</ref>{{rp|380}}\n\nThere exist sample continuous processes <math>X</math> such that <math>I(\\sigma)=\\infty;</math> they violate condition <math>(*).</math> An example found by Marcus and Shepp <ref name=MarcusShepp70 />{{rp|387}} is a random [[Lacunary function#Lacunary trigonometric series|lacunary Fourier series]]\n: <math> X_t = \\sum_{n=1}^\\infty c_n ( \\xi_n \\cos \\lambda_n t + \\eta_n \\sin \\lambda_n t ) ,</math>\nwhere <math>\\xi_1,\\eta_1,\\xi_2,\\eta_2,\\dots</math> are independent random variables with [[Normal distribution#Standard normal distribution|standard normal distribution]]; frequencies <math>0<\\lambda_1<\\lambda_2<\\dots</math> are a fast growing sequence; and coefficients <math>c_n>0</math> satisfy <math>\\textstyle \\sum_n c_n < \\infty.</math> The latter relation implies <math>\\textstyle\\mathbb E \\sum_n c_n ( |\\xi_n| + |\\eta_n| ) = \\sum_n c_n \\mathbb E ( |\\xi_n| + |\\eta_n| ) = \\text{const} \\cdot \\sum_n c_n < \\infty,</math> whence <math>\\sum_n c_n ( |\\xi_n| + |\\eta_n| ) < \\infty</math> almost surely, which ensures uniform convergence of the Fourier series almost surely, and sample continuity of <math>X.</math>\n[[File:Autocorrelation of a random lacunary Fourier series.svg|thumb|411px|Autocorrelation of a random lacunary Fourier series]]\nIts autocovariation function\n: <math> \\mathbb E X_t X_{t+h} = \\sum_{n=1}^\\infty c_n^2 \\cos \\lambda_n h </math>\nis nowhere monotone (see the picture), as well as the corresponding function <math>\\sigma,</math>\n: <math> \\sigma(h) = \\sqrt{ 2 \\mathbb E X_t X_t - 2 \\mathbb E X_t X_{t+h} } = 2 \\sqrt{ \\sum_{n=1}^\\infty c_n^2 \\sin^2 \\frac{\\lambda_n h}2 } .</math>\n\n==Brownian motion as the integral of Gaussian processes==\nA [[Wiener process]] (aka Brownian motion) is the integral of a [[White noise#Continuous-time white noise|white noise generalized Gaussian process]]. It is not [[stationary process|stationary]], but it has stationary increments.\n\nThe [[Ornstein–Uhlenbeck process]] is a [[stationary process|stationary]] Gaussian process.\n\nThe [[Brownian bridge]] is (like the Ornstein–Uhlenbeck process) an example of a Gaussian process whose increments are not [[statistical independence|independent]].\n\nThe [[fractional Brownian motion]] is a Gaussian process whose covariance function is a generalisation of that of the Wiener process.\n\n==Applications==\n[[Image:Regressions sine demo.svg|thumbnail|right|An example of Gaussian Process Regression (prediction) compared with other regression models.<ref>The documentation for [[scikit-learn]] also has similar [http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_compare_gpr_krr.html examples].</ref>]]\nA Gaussian process can be used as a [[prior probability distribution]] over [[Function (mathematics)|functions]] in [[Bayesian inference]].<ref name=\"gpml\"/><ref>{{cite book |last=Liu |first=W. |author2=Principe, J.C. |author3=Haykin, S. |title=Kernel Adaptive Filtering: A Comprehensive Introduction |url=http://www.cnel.ufl.edu/~weifeng/publication.htm |year=2010 |publisher=[[John Wiley & Sons|John Wiley]] |isbn=978-0-470-44753-6}}</ref> Given any set of ''N'' points in the desired domain of your functions, take a [[multivariate Gaussian]] whose covariance [[matrix (mathematics)|matrix]] parameter is the [[Gram matrix]] of your ''N'' points with some desired [[stochastic kernel|kernel]], and [[sampling (mathematics)|sample]] from that Gaussian.\n\nInference of continuous values with a Gaussian process prior is known as Gaussian process regression, or [[kriging]]; extending Gaussian process regression to [[Kernel methods for vector output|multiple target variables]] is known as ''cokriging''.<ref>{{cite book |last=Stein |first=M.L. |title=Interpolation of Spatial Data: Some Theory for Kriging |year=1999 |publisher = [[Springer Science+Business Media|Springer]]}}</ref> Gaussian processes are thus useful as a powerful non-linear multivariate [[interpolation]] tool. Gaussian process regression can be further extended to address learning tasks in both [[Supervised learning|supervised]] (e.g. probabilistic classification<ref name=\"gpml\"/>) and [[Unsupervised learning|unsupervised]] (e.g. [[manifold learning]]<ref name= \"prml\"/>) learning frameworks.\n\nGaussian processes can also be used in the context of mixture of experts models, for example.<ref>Emmanouil A. Platanios and Sotirios P. Chatzis, “Gaussian Process-Mixture Conditional Heteroscedasticity,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 5, pp. 888–900, May 2014. [http://ieeexplore.ieee.org/document/6613486/]</ref><ref>Sotirios P. Chatzis, “A Latent Variable Gaussian Process Model with Pitman-Yor Process Priors for Multiclass Classification,” Neurocomputing, vol. 120, pp. 482–489, Nov. 2013.\n [http://www.sciencedirect.com/science/article/pii/S0925231213005523]</ref> The underlying rationale of such a learning framework consists in the assumption that a given mapping cannot be well captured by a single Gaussian process model. Instead, the observation space is divided into subsets, each of which is characterized by a different mapping function; each of these is learned via a different Gaussian process component in the postulated mixture.\n\n===Gaussian process prediction, or kriging===\n[[File:Gaussian Process Regression.png|thumbnail|right|Gaussian Process Regression (prediction) with a squared exponential kernel. Left plot are draws from the prior function distribution. Middle are draws from the posterior. Right is mean prediction with one standard deviation shaded.]]\nWhen concerned with a general Gaussian process regression problem ([[kriging]]), it is assumed that for a Gaussian process ''f'' observed at coordinates ''x'', the vector of values {{tmath|f(x)}} is just one sample from a multivariate Gaussian distribution of dimension equal to number of observed coordinates {{tmath|n}}. Therefore, under the assumption of a zero-mean distribution, {{tmath|f (x) \\sim N (0, K(\\theta,x,x'))}}, where {{tmath|K(\\theta,x,x')}} is the covariance matrix between all possible pairs {{tmath|(x,x')}} for a given set of hyperparameters ''θ''.<ref name= \"gpml\"/>\nAs such the log marginal likelihood is:\n\n: <math>\\log p(f(x)|\\theta,x) =  -\\frac{1}{2}f(x)^T K(\\theta,x,x')^{-1} f(x) -\\frac 1 2 \\log \\det(K(\\theta,x,x')) - \\frac{n} 2 \\log 2\\pi </math>\n\nand maximizing this marginal likelihood towards ''θ'' provides the complete specification of the Gaussian process ''f''. One can briefly note at this point that the first term corresponds to a penalty term for a model's failure to fit observed values and the second term to a penalty term that increases proportionally to a model's complexity. Having specified ''θ'' making predictions about unobserved values {{tmath|f(x^*)}} at coordinates ''x''* is then only a matter of drawing samples from the predictive distribution <math>p(y^*\\mid x^*,f(x),x) = N(y^*\\mid A,B)</math> where the posterior mean estimate ''A'' is defined as\n\n:<math>A = K(\\theta,x^*,x) K(\\theta,x,x')^{-1} f(x)</math>\n\nand the posterior variance estimate ''B'' is defined as:\n\n:<math>B = K(\\theta,x^*,x^*) - K(\\theta,x^*,x)  K(\\theta,x,x')^{-1}  K(\\theta,x^*,x)^T </math>\n\nwhere {{tmath|K(\\theta,x^*,x)}} is the covariance between the new coordinate of estimation ''x''* and all other observed coordinates ''x'' for a given hyperparameter vector ''θ'', {{tmath|K(\\theta,x,x')}} and {{tmath|f(x)}} are defined as before and {{tmath|K(\\theta,x^*,x^*)}} is the variance at point ''x''* as dictated by ''θ''. It is important to note that practically the posterior mean estimate {{tmath|f(x^*)}} (the \"point estimate\") is just a linear combination of the observations {{tmath|f(x)}}; in a similar manner the variance of {{tmath|f(x^*)}} is actually independent of the observations {{tmath|f(x)}}. A known bottleneck in Gaussian process prediction is that the computational complexity of prediction is cubic in the number of points |''x''| and as such can become unfeasible for larger data sets.<ref name= \"brml\"/> Works on sparse Gaussian processes, that usually are based on the idea of building a ''representative set'' for the given process ''f'', try to circumvent this issue.<ref name=\"smolaSparse\">{{cite journal |last1= Smola| first1= A.J.| last2=Schoellkopf | first2= B. |year= 2000 |title= Sparse greedy matrix approximation for machine learning |journal= Proceedings of the Seventeenth International Conference on Machine Learning| pages=911–918}}</ref><ref name=\"CsatoSparse\">{{cite journal |last1= Csato| first1=L.| last2=Opper | first2= M. |year= 2002 |title= Sparse on-line Gaussian processes  |journal= Neural Computation |number=3| volume= 14 | pages=641–668 | doi=10.1162/089976602317250933| pmid=11860686| citeseerx=10.1.1.335.9713}}</ref>\n\n==See also==\n* [[Bayes linear statistics]]\n* [[Bayesian interpretation of regularization]]\n* [[Kriging]]\n* [[Gaussian free field]]\n* [[Gradient-Enhanced Kriging (GEK)]]\n\n== References ==\n{{Reflist}}\n\n==External links==\n* [http://www.GaussianProcess.org The Gaussian Processes Web Site, including the text of Rasmussen and Williams' Gaussian Processes for Machine Learning]\n* [https://arxiv.org/abs/1505.02965 A gentle introduction to Gaussian processes]\n* [http://publications.nr.no/917_Rapport.pdf A Review of Gaussian Random Fields and Correlation Functions]\n* [https://pdfs.semanticscholar.org/c9f2/1b84149991f4d547b3f0f625f710750ad8d9.pdf Efficient Reinforcement Learning using Gaussian Processes]\n===Software===\n* [http://sourceforge.net/projects/kriging STK: a Small (Matlab/Octave) Toolbox for Kriging and GP modeling]\n* [http://www.uqlab.com/ Kriging module in UQLab framework (Matlab)]\n* [http://au.mathworks.com/matlabcentral/fileexchange/38880: Matlab/Octave function for stationary Gaussian fields]\n* [https://github.com/Yelp/MOE Yelp MOE – A black box optimization engine using Gaussian process learning]\n* [http://www.sumo.intec.ugent.be/ooDACE ooDACE] – A flexible object-oriented Kriging matlab toolbox.\n* [http://becs.aalto.fi/en/research/bayes/gpstuff/ GPstuff – Gaussian process toolbox for Matlab and Octave]\n* [https://github.com/SheffieldML/GPy GPy – A Gaussian processes framework in Python]\n* [http://www.tmpl.fi/gp/ Interactive Gaussian process regression demo]\n* [https://github.com/ChristophJud/GPR Basic Gaussian process library written in C++11]\n* [http://scikit-learn.org scikit-learn] – A machine learning library for Python which includes Gaussian process regression and classification\n* [https://github.com/modsim/KriKit] - The Kriging toolKit (KriKit) is developed at the Institute of Bio- and Geosciences 1 (IBG-1) of Forschungszentrum Jülich (FZJ)\n\n===Video tutorials===\n* [http://videolectures.net/gpip06_mackay_gpb Gaussian Process Basics by David MacKay]\n* [http://videolectures.net/epsrcws08_rasmussen_lgp Learning with Gaussian Processes by Carl Edward Rasmussen]\n* [http://videolectures.net/mlss07_rasmussen_bigp Bayesian inference and Gaussian processes by Carl Edward Rasmussen]\n\n{{Stochastic processes}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Gaussian Process}}\n[[Category:Stochastic processes]]\n[[Category:Kernel methods for machine learning]]\n[[Category:Nonparametric Bayesian statistics]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Graphical lasso",
      "url": "https://en.wikipedia.org/wiki/Graphical_lasso",
      "text": "{{Multiple issues|\n{{Underlinked|date=July 2016}}\n{{Orphan|date=July 2016}}\n}}\n\nIn statistics, the '''graphical lasso''' is an [[algorithm]] to estimate the [[Precision_(statistics)|precision matrix]] (inverse of [[covariance matrix]]) from the observations from [[multivariate Gaussian distribution]].<ref name='friedman'>{{cite journal\n  | author = Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert\n  | title = Sparse inverse covariance estimation with the graphical lasso\n  | publisher = Biometrika Trust\n  | journal = Biostatistics\n  | year = 2008\n  | url = http://statweb.stanford.edu/~tibs/ftp/graph.pdf\n }}</ref>\n\n== Setting ==\n\nConsider observations <math>X_1, X_2, \\ldots, X_n</math> from multivariate Gaussian distribution <math>X \\sim N(0, \\Sigma)</math>. We are interested in estimating the [[precision matrix]] <math>\\Theta = \\Sigma^{-1}</math>.\n\nThe graphical lasso estimator is the <math>\\hat{\\Theta}</math> such that:\n\n:<math>\n\\hat{\\Theta} = \\operatorname{argmin}_{\\Theta \\ge 0} \\left(\\operatorname{tr}(S \\Theta) - \\log \\det(\\Theta) + \\lambda \\sum_{j \\ne k} |\\Theta_{jk}| \\right)</math>\n\nwhere <math>S</math> is the sample covariance, and <math>\\lambda</math> is the penalizing parameter.<ref name=\"friedman\" />\n\n== Application ==\n\nTo obtain the estimator in programs, users could use the R package [https://cran.r-project.org/web/packages/glasso/glasso.pdf glasso],<ref name='r-glasso'>{{cite book \n  | title = glasso: Graphical lasso- estimation of Gaussian graphical models\n  | url = https://cran.r-project.org/package=glasso\n  | year = 2014\n  |author1=Jerome Friedman |author2=Trevor Hastie |author3=Rob Tibshirani }}</ref>, [https://scikit-learn.org/stable/modules/generated/sklearn.covariance.GraphicalLasso.html GraphicalLasso() class] in Python Scikit-Learn package<ref name='sklearn'>{{cite journal\n  | author = Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.\n  | title = Scikit-learn: Machine Learning in Python\n  | journal = Journal of Machine Learning Research\n  | year = 2011\n  | url = http://scikit-learn.org/stable/about.html\n}}</ref>, or the [https://github.com/skggm/skggm skggm] Python package <ref name='skggm'>{{cite journal\n  | author1      = Jason Laska \n  | author2      = Manjari Narayan\n  | title        = skggm 0.2.7: A scikit-learn compatible package for Gaussian and related Graphical Models\n  | year         = 2017\n  | doi          = 10.5281/zenodo.830033\n  | url          = https://doi.org/10.5281/zenodo.830033}}</ref> (similar to scikit-learn).\n\n== References ==\n{{Reflist}}\n\n[[Category:Normal distribution]]\n[[Category:Graphical models]]\n[[Category:Markov networks]]"
    },
    {
      "title": "Half-normal distribution",
      "url": "https://en.wikipedia.org/wiki/Half-normal_distribution",
      "text": "{{unreferenced|date=August 2016}}\n{{Probability distribution\n  | name       = Half-normal distribution\n  | type       = density\n  | pdf_image  = [[File:Half normal pdf.svg|350px|Probability density function of the half-normal distribution <math>\\sigma=1</math>]]<br><math>\\sigma=1</math>\n  | cdf_image  = [[File:Half normal cdf.svg|350px|Cumulative distribution function of the half-normal distribution <math>\\sigma=1</math>]]<br><math>\\sigma=1</math>\n  | notation   = \n  | parameters = {{nowrap|<math>\\sigma>0</math>}} — ([[scale parameter|scale]])\n  | support    = {{nowrap|<math>x\\in[0,\\infty)</math>}} \n  | pdf        = <math>f(x; \\sigma) = \\frac{\\sqrt{2}}{\\sigma\\sqrt{\\pi}}\\exp  \\left( -\\frac{x^2}{2\\sigma^2} \\right) \\quad x>0</math>\n  | cdf        = <math>F(x; \\sigma) = \\operatorname{erf}\\left(\\frac{x}{\\sigma\\sqrt{2}}\\right)</math>\n  | quantile   = <math>Q(F;\\sigma)=\\sigma\\sqrt{2}\\operatorname{erf}^{-1}(F)</math>\n  | mean       = <math>\\frac{\\sigma\\sqrt{2}}{\\sqrt{\\pi}}</math>\n  | median     = <math>\\sigma\\sqrt{2}\\operatorname{erf}^{-1}(1/2)</math>\n  | mode       = <math>0</math>\n  | variance   = <math>\\sigma^2\\left(1 - \\frac 2 \\pi \\right) </math>\n  | skewness   = <math>\\frac{\\sqrt{2}(4-\\pi)}{(\\pi-2)^{3/2}} \\approx 0.9952717</math>\n  | kurtosis   = <math>\\frac{8(\\pi-3)}{(\\pi-2)^2}</math>\n  | entropy    = <math> \\frac{1}{2}  \\log \\left( \\frac{ \\pi \\sigma^2 }{2} \\right) + \\frac{1}{2} </math>\n  | mgf        = \n  | char       = \n  | fisher     = \n  }}\n\nIn probability theory and statistics, the '''half-normal distribution''' is a special case of the [[folded normal distribution]].\n\nLet <math>X</math> follow an ordinary [[normal distribution]], <math>N(0,\\sigma^2)</math>, then <math>Y=|X|</math> follows a half-normal distribution. Thus, the half-normal distribution is a fold at the mean of an ordinary normal distribution with mean zero.\n\n==Properties==\n\nUsing the <math>\\sigma</math> parametrization of the normal distribution, the [[probability density function]] (PDF) of the half-normal is given by\n\n: <math>f_Y(y; \\sigma) = \\frac{\\sqrt{2}}{\\sigma\\sqrt{\\pi}}\\exp \\left( -\\frac{y^2}{2\\sigma^2} \\right) \\quad y \\geq 0,</math>\n\nwhere <math>E[Y] = \\mu = \\frac{\\sigma\\sqrt{2}}{\\sqrt{\\pi}}</math>.\n\nAlternatively using a scaled precision (inverse of the variance) parametrization (to avoid issues if <math>\\sigma</math> is near zero), obtained by setting <math>\\theta=\\frac{\\sqrt{\\pi}}{\\sigma\\sqrt{2}}</math>, the [[probability density function]] is given by\n\n: <math>f_Y(y; \\theta) = \\frac{2\\theta}{\\pi}\\exp  \\left( -\\frac{y^2\\theta^2}{\\pi} \\right) \\quad y \\geq 0,</math>\n\nwhere <math>E[Y] = \\mu = \\frac{1}{\\theta}</math>.\n\nThe [[cumulative distribution function]] (CDF) is given by\n\n: <math>F_Y(y; \\sigma) = \\int_0^y \\frac{1}{\\sigma}\\sqrt{\\frac{2}{\\pi}} \\, \\exp \\left( -\\frac{x^2}{2\\sigma^2} \\right)\\, dx</math>\n\nUsing the change-of-variables <math>z = x/(\\sqrt{2}\\sigma)</math>, the CDF can be written as\n\n: <math>F_Y(y; \\sigma) = \\frac{2}{\\sqrt{\\pi}} \\,\\int_0^{y/(\\sqrt{2}\\sigma)}\\exp \\left(-z^2\\right)dz  = \\operatorname{erf}\\left(\\frac{y}{\\sqrt{2}\\sigma}\\right),\n</math>\nwhere erf is the [[error function]], a standard function in many mathematical software packages.\n\nThe quantile function (or inverse CDF) is written:\n:<math>Q(F;\\sigma)=\\sigma\\sqrt{2} \\operatorname{erf}^{-1}(F)</math>\nwhere <math>0\\le F \\le 1</math> and <math>\\operatorname{erf}^{-1}</math> is the [[Inverse error function#Inverse functions|inverse error function]]\n\nThe expectation is then given by\n\n: <math>E(Y) = \\sigma \\sqrt{2/\\pi},</math>\n\nThe variance is given by\n\n: <math>\\operatorname{var}(Y) = \\sigma^2\\left(1 - \\frac{2}{\\pi}\\right). </math>\n\nSince this is proportional to the variance σ<sup>2</sup> of ''X'', ''σ'' can be seen as a [[scale parameter]] of the new distribution.\n\nThe entropy of the half-normal distribution is exactly one bit less the entropy of a zero-mean normal distribution with the same second moment about&nbsp;0. This can be understood intuitively since the magnitude operator reduces information by one bit (if the probability distribution at its input is even). Alternatively, since a half-normal distribution is always positive, the one bit it would take to record whether a standard normal random variable were positive (say, a 1) or negative (say, a 0) is no longer necessary. Thus,\n\n: <math> H(Y) = \\frac{1}{2}  \\log \\left( \\frac{\\pi\\sigma^2}{2} \\right) + \\frac{1}{2}.</math>\n\n== Parameter estimation ==\nGiven numbers <math>\\{x_i\\}_{i=1}^n</math> drawn from a half-normal distribution, the unknown parameter <math>\\sigma</math> of that distribution can be estimated by the method of [[maximum likelihood]], giving\n\n: <math> \\hat \\sigma = \\sqrt{\\frac 1 n \\sum_{i=1}^n x_i^2}</math>\n\nThe bias is equal to \n: <math>\n    b \\equiv \\operatorname{E}\\bigg[\\;(\\hat\\sigma_\\mathrm{mle} - \\sigma)\\;\\bigg]\n        = - \\frac{\\sigma}{4n} \n  </math>\n\nwhich yields the '''[[Maximum_likelihood_estimation#Higher-order_properties|bias-corrected maximum likelihood estimator]]'''\n\n: <math>\n    \\hat{\\sigma\\,}^*_\\text{mle} = \\hat{\\sigma\\,}_\\text{mle} - \\hat{b\\,}.\n  </math>\n\n== Related distributions ==\n* The distribution is a special case of the [[folded normal distribution]] with ''μ''&nbsp;=&nbsp;0.\n* It also coincides with a zero-mean normal distribution truncated from below at zero (see [[truncated normal distribution]])\n* If ''Y'' has a half-normal distribution, then (''Y''/''σ'')<sup>2</sup> has a [[chi square distribution]] with 1 degree of freedom, i.e. ''Y''/''σ'' has a [[chi distribution]] with 1 degree of freedom.\n* The half-normal distribution is a special case of the [[generalized gamma distribution]] with ''d''&nbsp;=&nbsp;1, ''p''&nbsp;=&nbsp;2, ''a''&nbsp;=&nbsp;<math>\\sqrt{2}\\sigma</math>.\n\n==See also==\n* [[Half-t distribution|Half-''t'' distribution]]\n\n==References==\n\n{{Empty section|date=January 2017}}\n\n==External links==\n*[http://mathworld.wolfram.com/Half-NormalDistribution.html Half-Normal Distribution] at [[MathWorld]]\n:(note that MathWorld uses the parameter <math> \\theta = \\frac{1}{\\sigma}\\sqrt {\\pi/2} </math>)\n\n==Further reading==\n* {{citation|last1=Leone|first1=F. C.|last2=Nelson|first2=L. S.|last3=Nottingham|first3=R. B.|title=The folded normal distribution|journal=Technometrics|volume=3|issue=4|year=1961|pages=543-550|doi=10.2307/1266560|jstor=1266560}}\n\n{{ProbDistributions|continuous-semi-infinite}}\n\n[[Category:Continuous distributions]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Isserlis' theorem",
      "url": "https://en.wikipedia.org/wiki/Isserlis%27_theorem",
      "text": "{{about||the result in quantum field theory about products of creation and annihilation operators|Wick's theorem}}\nIn [[probability theory]], '''Isserlis' theorem''' or '''Wick's theorem''' is a formula that allows one to compute higher-order moments of the [[multivariate normal distribution]] in terms of its covariance matrix.  It is named after [[Leon Isserlis]].\n\nThis theorem is particularly important in [[particle physics]], where it is known as [[Wick's theorem]] after the work of {{harvtxt|Wick|1950}}.<ref>{{cite journal\n  | last = Wick\n  | first = G.C.\n  | year = 1950\n  | title = The evaluation of the collision matrix\n  | journal = [[Physical Review]]\n  | volume = 80\n  | issue = 2\n  | pages = 268–272\n  | doi = 10.1103/PhysRev.80.268\n  | bibcode = 1950PhRv...80..268W\n  }}</ref> Other applications include the analysis of portfolio returns,<ref>{{cite journal\n  | last1 = Repetowicz\n  | first1 = Przemysław\n  | last2 = Richmond\n  | first2 = Peter\n  | year = 2005\n  | title = Statistical inference of multivariate distribution parameters for non-Gaussian distributed time series\n  | journal = Acta Physica Polonica B\n  | volume = 36\n  | issue = 9\n  | pages = 2785–2796\n  | url = http://th-www.if.uj.edu.pl/acta/vol36/pdf/v36p2785.pdf\n  }}</ref> quantum field theory<ref>{{cite journal\n  | last1 = Perez-Martin\n  | first1 = S.\n  | last2 = Robledo\n  | first2 = L.M.\n  | year = 2007\n  | title = Generalized Wick's theorem for multiquasiparticle overlaps as a limit of Gaudin's theorem\n  | journal = Physical Review C\n  | volume = 76\n  | issue = 6\n | pages = 064314\n | doi = 10.1103/PhysRevC.76.064314\n  | arxiv = 0707.3365\n  | bibcode = 2007PhRvC..76f4314P\n  }}</ref> and generation of colored noise.<ref>{{cite journal\n  | last = Bartosch\n  | first = L.\n  | year = 2001\n  | title = Generation of colored noise\n  | journal = International Journal of Modern Physics C\n  | volume = 12\n  | issue = 6\n  | pages = 851–855\n  | doi = 10.1142/S0129183101002012\n  | bibcode = 2001IJMPC..12..851B\n  }}</ref>\n\n==Statement==\nIf <math>(X_1,\\dots X_{2n})</math> is a zero-mean [[multivariate normal]] random vector, then\n: <math>\\begin{align}\n    & \\operatorname{E} [\\,X_1 X_2\\cdots X_{2n}\\,] = \\sum\\prod \\operatorname{E}[\\,X_i X_j\\,] = \\sum\\prod \\operatorname{Cov}(\\,X_i, X_j\\,), \\\\\n    & \\operatorname{E}[\\,X_1 X_2\\cdots X_{2n-1}\\,] = 0,\n  \\end{align}</math>\nwhere the notation ∑&thinsp;∏ means summing over all distinct ways of partitioning ''X''<sub>1</sub>, …, ''X''<sub>2''n''</sub> into pairs ''X''<sub>''i''</sub>,''X''<sub>''j''</sub> and each summand is the product of the ''n'' pairs.<ref>{{cite journal\n  | last1 = Michalowicz\n  | first1 = J.V.\n  | last2 = Nichols\n  | first2 = J.M.\n  | last3 = Bucholtz\n  | first3 = F.\n  | last4 = Olson\n  | first4 = C.C.\n  | title = An Isserlis' theorem for mixed Gaussian variables: application to the auto-bispectral density\n  | journal = Journal of Statistical Physics\n  | year = 2009\n  | volume = 136\n  | issue = 1\n  | pages = 89–102\n  | doi = 10.1007/s10955-009-9768-3\n  | bibcode = 2009JSP...136...89M\n  }}</ref>  This yields <math>(2n)!/(2^{n}n!) = (2n-1)!!</math> terms in the sum (see [[double factorial]]). For example, for fourth order moments (four variables) there are three terms. For sixth-order moments there are 3&nbsp;×&nbsp;5 = 15 terms, and for eighth-order moments there are 3&nbsp;×&nbsp;5&nbsp;×&nbsp;7 = 105 terms (as you can check in the examples below). \n\nIn his original paper,<ref>{{cite journal\n  | last = Isserlis\n  | first = L.\n  | authorlink = Leon Isserlis\n  | year = 1918\n  | title = On a formula for the product-moment coefficient of any order of a normal frequency distribution in any number of variables\n  | journal = [[Biometrika]]\n  | volume = 12\n  | issue = 1–2\n | pages = 134–139\n  | jstor = 2331932\n  | doi=10.1093/biomet/12.1-2.134\n  }}</ref> [[Leon Isserlis]] proves this theorem by mathematical induction, generalizing the formula for the fourth-order moments,<ref>{{cite journal\n  | last = Isserlis\n  | first = L.\n  | authorlink = Leon Isserlis\n  | year = 1916\n  | title =  On Certain Probable Errors and Correlation Coefficients of Multiple Frequency Distributions with Skew Regression\n  | journal = [[Biometrika]]\n  | volume = 11\n  | issue = 3\n | pages = 185–190\n  | jstor = 2331846\n  | doi=10.1093/biomet/11.3.185\n  | url = https://zenodo.org/record/1431585\n }}</ref> which takes the appearance\n: <math>\n    \\operatorname{E}[\\,X_1 X_2 X_3 X_4\\,] =\n        \\operatorname{E}[X_1X_2]\\,\\operatorname{E}[X_3X_4] +\n        \\operatorname{E}[X_1X_3]\\,\\operatorname{E}[X_2X_4] +\n        \\operatorname{E}[X_1X_4]\\,\\operatorname{E}[X_2X_3].\n  </math>\nFor sixth-order moments, Isserlis' theorem is:\n:<math>\\begin{align}\n& \\operatorname{E}[X_1 X_2 X_3 X_4 X_5 X_6] \\\\[5pt]\n= {} & \\operatorname{E}[X_1 X_2 ]\\operatorname{E}[X_3 X_4 ]\\operatorname{E}[X_5 X_6 ] + \\operatorname{E}[X_1 X_2 ]\\operatorname{E}[X_3 X_5 ]\\operatorname{E}[X_4 X_6] + \\operatorname{E}[X_1 X_2 ]\\operatorname{E}[X_3 X_6 ]\\operatorname{E}[X_4 X_5] \\\\[5pt]\n& {} + \\operatorname{E}[X_1 X_3 ]\\operatorname{E}[X_2 X_4 ]\\operatorname{E}[X_5 X_6 ] + \\operatorname{E}[X_1 X_3 ]\\operatorname{E}[X_2 X_5 ]\\operatorname{E}[X_4 X_6 ] + \\operatorname{E}[X_1 X_3]\\operatorname{E}[X_2 X_6]\\operatorname{E}[X_4 X_5] \\\\[5pt]\n& {} + \\operatorname{E}[X_1 X_4]\\operatorname{E}[X_2 X_3]\\operatorname{E}[X_5 X_6]+\\operatorname{E}[X_1 X_4]\\operatorname{E}[X_2 X_5]\\operatorname{E}[X_3 X_6]+\\operatorname{E}[X_1 X_4]\\operatorname{E}[X_2 X_6]\\operatorname{E}[X_3 X_5] \\\\[5pt]\n& {} + \\operatorname{E}[X_1 X_5]\\operatorname{E}[X_2 X_3]\\operatorname{E}[X_4 X_6]+\\operatorname{E}[X_1 X_5]\\operatorname{E}[X_2 X_4]\\operatorname{E}[X_3 X_6]+\\operatorname{E}[X_1 X_5]\\operatorname{E}[X_2 X_6]\\operatorname{E}[X_3 X_4] \\\\[5pt]\n& {} + \\operatorname{E}[X_1 X_6]\\operatorname{E}[X_2 X_3]\\operatorname{E}[X_4 X_5 ] + \\operatorname{E}[X_1 X_6]\\operatorname{E}[X_2 X_4 ]\\operatorname{E}[X_3 X_5] + \\operatorname{E}[X_1 X_6] \\operatorname{E}[X_2 X_5]\\operatorname{E}[X_3 X_4].\n\\end{align}</math>\n\n==See also==\n* [[Wick's theorem]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n* {{cite book\n  | last = Koopmans\n  | first = Lambert G.\n  | title = The spectral analysis of time series\n  | publisher = [[Academic Press]]\n  | location = San Diego, CA\n  | year = 1974\n  }}\n\n{{DEFAULTSORT:Isserlis' Theorem}}\n[[Category:Moment (mathematics)]]\n[[Category:Normal distribution]]\n[[Category:Probability theorems]]"
    },
    {
      "title": "James–Stein estimator",
      "url": "https://en.wikipedia.org/wiki/James%E2%80%93Stein_estimator",
      "text": "{{Short description|Biased estimator for Gaussian random vectors, better than ordinary least-squared-error minimization}}{{technical|date=November 2017}}\nThe '''James–Stein estimator''' is a [[Bias of an estimator|biased]] [[estimator]] of the [[mean]] of Gaussian [[random vector]]s. It can be shown that the James–Stein estimator [[dominating decision rule|dominates]] the \"ordinary\" [[least squares]] approach, i.e., it has lower [[mean squared error]]. It is the best-known example of [[Stein's phenomenon]].\n\nAn earlier version of the estimator was developed by [[Charles Stein (statistician)|Charles Stein]] in 1956,<ref name=\"stein-56\">{{Citation\n  | last = Stein | first = C.\n  | author-link = Charles Stein (statistician)\n  | contribution = Inadmissibility of the usual estimator for the mean of a multivariate distribution\n  | title = Proc. Third Berkeley Symp. Math. Statist. Prob.\n  | year = 1956\n  | volume = 1\n  | pages = 197–206\n  | url = http://projecteuclid.org/euclid.bsmsp/1200501656\n  | mr = 0084922 | zbl = 0073.35602\n}}</ref> and is sometimes referred to as '''Stein's estimator'''.{{Citation needed|date=October 2010}} The result was improved by Willard James and Charles Stein in 1961.<ref name=\"james-stein-61\">{{Citation\n  | last = James  | first = W.\n  | last2 = Stein | first2 = C. | author2-link = Charles Stein (statistician)\n  | contribution = Estimation with quadratic loss\n  | title = Proc. Fourth Berkeley Symp. Math. Statist. Prob.\n  | year = 1961\n  | volume = 1\n  | pages = 361–379\n  | url = http://projecteuclid.org/euclid.bsmsp/1200512173\n  |mr = 0133191\n}}</ref>\n\n== Setting ==\nSuppose the vector <math>\\boldsymbol\\theta</math> is the unknown [[Expected value|mean]] of a [[Multivariate normal distribution|<math>m</math>-variate normally distributed]] (with known [[covariance matrix]] <math>\\sigma^2 I </math>) [[random variable]] <math>{\\mathbf Y}</math>:\n:<math>\n{\\mathbf Y} \\sim N({\\boldsymbol \\theta}, \\sigma^2 I).\\,\n</math>\n\nWe are interested in obtaining an estimate <math>\\widehat{\\boldsymbol \\theta} </math> of  <math>\\boldsymbol\\theta</math>, based on a single observation, <math>{\\mathbf y} </math>, of <math>{\\mathbf Y} </math>.\n\nThis is an everyday situation in which a set of parameters is measured, and the measurements are corrupted by independent Gaussian noise. Since the noise has zero mean, it is very reasonable to use the measurements themselves as an estimate of the parameters. This is the approach of the [[least squares]] estimator, which is <math>\\widehat{\\boldsymbol \\theta}_{LS} = {\\mathbf y}</math>.\n\nAs a result, there was considerable shock and disbelief when Stein demonstrated that, in terms of [[mean squared error]] <math>\\operatorname{E} \\left[ \\left\\| {\\boldsymbol \\theta}-\\widehat {\\boldsymbol \\theta} \\right\\|^2 \\right]</math>, this approach is suboptimal.<ref name=\"stein-56\"/> The result became known as [[Stein's phenomenon]].\n\n== The James–Stein estimator ==\n[[Image:MSE of ML vs JS.png|thumb|right|350px|MSE (R) of least squares estimator (ML) vs. James–Stein estimator (JS). The James–Stein estimator gives its best estimate when the norm of the actual parameter vector θ is near zero.]]\nIf <math>\\sigma^2</math> is known, the James–Stein estimator is given by\n\n:<math>\n\\widehat{\\boldsymbol \\theta}_{JS} = \n\\left( 1 - \\frac{(m-2) \\sigma^2}{\\|{\\mathbf y}\\|^2} \\right) {\\mathbf y}.\n</math>\n\nJames and Stein showed that the above estimator [[dominating decision rule|dominates]] <math>\\widehat{\\boldsymbol \\theta}_{LS}</math> for any <math>m \\ge 3</math>, meaning that the James–Stein estimator always achieves lower [[mean squared error]] (MSE) than the [[maximum likelihood]] estimator.<ref name=\"james-stein-61\"/><ref name=\"lehmann-casella-98\">{{Citation\n  | first = E. L. | last = Lehmann\n  | first2 = G.   | last2 = Casella\n  | year = 1998\n  | title = Theory of Point Estimation\n  | edition = 2nd\n  | location = New York\n  | publisher = Springer\n}}</ref> By definition, this makes the least squares estimator [[admissible decision rule|inadmissible]] when <math>m \\ge 3</math>.\n\nNotice that if <math>(m-2) \\sigma^2<\\|{\\mathbf y}\\|^2 </math> then this estimator simply  takes the natural estimator <math>\\mathbf y</math> and shrinks it towards the origin '''0'''.  In fact this is not the only direction of [[Shrinkage (statistics)|shrinkage]] that works.  Let '''''ν''''' be an arbitrary fixed vector of length <math>m</math>.  Then there exists an estimator of the James-Stein type that shrinks toward '''''ν''''', namely\n\n:<math>\n\\widehat{\\boldsymbol \\theta}_{JS} = \n\\left( 1 - \\frac{(m-3) \\sigma^2}{\\|{\\mathbf y} - {\\boldsymbol\\nu}\\|^2} \\right) ({\\mathbf y}-{\\boldsymbol\\nu}) + {\\boldsymbol\\nu}, m\\ge 4.\n</math>\n\nThe James–Stein estimator dominates the usual estimator for any '''''ν'''''.  A natural question to ask is whether the improvement over the usual estimator is independent of the choice of '''''ν'''''.  The answer is no. The improvement is small if <math>\\|{\\boldsymbol\\theta - \\boldsymbol\\nu}\\|</math> is large.  Thus to get a very great improvement some knowledge of the location of '''''θ''''' is necessary.  Of course this is the quantity we are trying to estimate so we don't have this knowledge a priori.  But we may have some guess as to what the mean vector is.  This can be considered a disadvantage of the estimator: the choice is not objective as it may depend on the beliefs of the researcher.\n\n== Interpretation ==\nSeeing the James–Stein estimator as an [[empirical Bayes method]] gives some intuition to this result: One assumes that '''''θ''''' itself is a random variable with [[Prior probability|prior distribution]] <math>\\sim N(0, A)</math>, where ''A'' is estimated from the data itself. Estimating ''A'' only gives an advantage compared to the [[Maximum likelihood|maximum-likelihood estimator]] when the dimension <math>m</math> is large enough; hence it does not work for <math>m\\leq 2</math>. The James–Stein estimator is a member of a class of Bayesian estimators that dominate the maximum-likelihood estimator.<ref>{{cite journal | last1 = Efron | first1 = B. | last2 = Morris | first2 = C. | year = 1973 | title = Stein's Estimation Rule and Its Competitors—An Empirical Bayes Approach | journal = Journal of the American Statistical Association | volume = 68 | issue = 341 | pages = 117–130 | publisher = American Statistical Association | doi = 10.2307/2284155}}</ref>\n\nA consequence of the above discussion is the following counterintuitive result: When three or more unrelated parameters are measured, their total MSE can be reduced by using a combined estimator such as the James–Stein estimator; whereas when each parameter is estimated separately, the least squares (LS) estimator is [[Admissible decision rule|admissible]].  A quirky example would be estimating the speed of light, tea consumption in Taiwan, and hog weight in Montana, all together.  The James–Stein estimator always improves upon the ''total'' MSE, i.e., the sum of the expected errors of each component. Therefore, the total MSE in measuring light speed, tea consumption, and hog weight would improve by using the James–Stein estimator. However, any particular component (such as the speed of light) would improve for some parameter values, and deteriorate for others. Thus, although the James–Stein estimator dominates the LS estimator when three or more parameters are estimated, any single component does not dominate the respective component of the LS estimator.\n\nThe conclusion from this hypothetical example is that measurements should be combined if one is interested in minimizing their total MSE. For example, in a [[telecommunication]] setting, it is reasonable to combine [[channel (communications)|channel]] tap measurements in a [[channel estimation]] scenario, as the goal is to minimize the total channel estimation error. Conversely, there could be objections to combining channel estimates of different users, since no user would want their channel estimate to deteriorate in order to improve the average network performance.{{Citation needed|date=February 2013}}\n\nThe James–Stein estimator has also found use in fundamental quantum theory, where the estimator has been used to improve the theoretical bounds of the entropic uncertainty principle (a recent development of the Heisenberg [[uncertainty principle]]) for more than three measurements.<ref name=\"stander-17\">{{Citation\n  | last = Stander | first = M.\n  | title = Using Stein's estimator to correct the bound on the entropic uncertainty principle for more than two measurements\n  | year = 2017\n  | arxiv = 1702.02440\n\n| bibcode = 2017arXiv170202440S}}</ref>\n\n== Improvements ==\nThe basic James–Stein estimator has the peculiar property that for small values of  <math>\\|{\\mathbf y} - {\\boldsymbol\\nu} \\|,</math> the multiplier on <math>{\\mathbf y} - {\\boldsymbol\\nu}</math> is actually negative.   This can be easily remedied by replacing this multiplier by zero when it is negative.  The resulting estimator is called the ''positive-part James–Stein estimator'' and is given by\n\n:<math>\n\\widehat{\\boldsymbol \\theta}_{JS+} = \n\\left( 1 - \\frac{(m-3) \\sigma^2}{\\|{\\mathbf y} - {\\boldsymbol\\nu}\\|^2} \\right)^+ ({\\mathbf y}-{\\boldsymbol\\nu}) + {\\boldsymbol\\nu}, m \\ge 4.\n</math>\n\nThis estimator has a smaller risk than the basic James–Stein estimator.  It follows that the basic James–Stein estimator is itself [[admissible decision rule|inadmissible]].<ref name=\"Anderson-84\">{{Citation\n  | first = T. W. | last = Anderson\n  | year = 1984\n  | title = An Introduction to Multivariate Statistical Analysis\n  | edition = 2nd\n  | location = New York\n  | publisher = John Wiley & Sons\n}}</ref>\n\nIt turns out, however, that the positive-part estimator is also inadmissible.<ref name=\"lehmann-casella-98\"/> This follows from a general result which requires admissible estimators to be smooth.\n\n== Extensions ==\nThe James–Stein estimator may seem at first sight to be a result of some peculiarity of the problem setting. In fact, the estimator exemplifies a very wide-ranging effect; namely, the fact that the \"ordinary\" or least squares estimator is often [[admissible decision rule|inadmissible]] for simultaneous estimation of several parameters.{{Citation needed|date=February 2012}} This effect has been called [[Stein's phenomenon]], and has been demonstrated for several different problem settings, some of which are briefly outlined below.\n* James and Stein demonstrated that the estimator presented above can still be used when the variance <math>\\sigma^2</math> is unknown, by replacing it with the standard estimator of the variance, <math>\\widehat{\\sigma}^2 = \\frac{1}{n}\\sum ( y_i-\\overline{y} )^2</math>. The dominance result still holds under the same condition, namely, <math>m > 2</math>.<ref name=\"james-stein-61\"/>\n* The results in this article are for the case when only a single observation vector '''y''' is available.  For the more general case when <math>n</math> vectors are available, the results are similar:{{Citation needed|date=February 2012}}\n:: <math>\n\\widehat{\\boldsymbol \\theta}_{JS} = \n\\left( 1 - \\frac{(m-2) \\frac{\\sigma^2}{n}}{\\|{\\overline{\\mathbf y}}\\|^2} \\right) {\\overline{\\mathbf y}},\n</math>\n:where <math>{\\overline{\\mathbf y}}</math> is the <math>m</math>-length average of the <math>n</math> observations.\n\n* The work of James and Stein has been extended to the case of a general measurement covariance matrix, i.e., where measurements may be statistically dependent and may have differing variances.<ref name=\"bock75\"/> A similar dominating estimator can be constructed, with a suitably generalized dominance condition. This can be used to construct a [[linear regression]] technique which outperforms the standard application of the LS estimator.<ref name=\"bock75\">{{Citation\n  | last = Bock  | first = M. E.\n  | year = 1975\n  | title = Minimax estimators of the mean of a multivariate normal distribution\n  | journal = [[Annals of Statistics]]\n  | volume = 3\n  | issue = 1\n  | pages = 209–218\n  | doi = 10.1214/aos/1176343009\n  | mr = 381064 | zbl = 0314.62005\n}}</ref>\n* Stein's result has been extended to a wide class of distributions and loss functions. However, this theory provides only an existence result, in that explicit dominating estimators were not actually exhibited.<ref name=\"brown66\">{{Citation\n  | last = Brown | first = L. D. |authorlink = Lawrence D. Brown\n  | year = 1966\n  | title = On the admissibility of invariant estimators of one or more location parameters\n  | journal = Annals of Mathematical Statistics\n  | volume = 37\n  | issue = 5\n  | pages = 1087–1136\n  | doi = 10.1214/aoms/1177699259\n  | mr = 216647 | zbl = 0156.39401\n}}</ref> It is quite difficult to obtain explicit estimators improving upon the usual estimator without specific restrictions on the underlying distributions.<ref name=\"lehmann-casella-98\"/>\n\n== See also ==\n* [[Admissible decision rule]]\n* [[Hodges' estimator]]\n* [[Shrinkage estimator]]\n\n== References ==\n{{Reflist}}\n\n== Further reading ==\n* {{cite book |last=Judge |first=George G. |last2=Bock |first2=M. E. |title=The Statistical Implications of Pre-Test and Stein-Rule Estimators in Econometrics |location=New York |publisher=North Holland |year=1978 |isbn=0-7204-0729-X |chapter= |pages=229–257 }}\n\n{{DEFAULTSORT:James-Stein Estimator}}\n[[Category:Estimator]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Kurtosis risk",
      "url": "https://en.wikipedia.org/wiki/Kurtosis_risk",
      "text": "{{Short description|term in decision theory}}\n{{Use American English|date = January 2019}}\n\nIn [[statistics]] and [[decision theory]], '''kurtosis risk''' is the [[risk (statistics)|risk]] that results when a [[statistical model]] assumes the [[normal distribution]], but is applied to [[observation]]s that have a tendency to occasionally be much farther (in terms of number of standard deviations) from the average than is expected for a normal distribution.\n\nKurtosis risk applies to any [[kurtosis]]-related quantitative model that assumes the normal distribution for certain of its [[independent variable]]s when the latter may in fact have kurtosis much greater than does the normal distribution. Kurtosis risk is commonly referred to as \"[[fat tail]]\" risk. The \"fat tail\" metaphor explicitly describes the situation of having more observations at either extreme than the tails of the normal distribution would suggest; therefore, the tails are \"fatter\".\n\nIgnoring kurtosis risk will cause any model to understate the risk of variables with high kurtosis. For instance, [[Long-Term Capital Management]], a [[hedge fund]] cofounded by [[Myron Scholes]], ignored kurtosis risk to its detriment. After four successful years, this hedge fund had to be bailed out by major investment banks in the late 1990s because it understated the kurtosis of many [[security (finance)|financial securities]] underlying the fund's own trading positions.<ref>{{cite news |title=Rashomon in Connecticut |last=Krugman |first=Paul |authorlink=Paul Krugman |date=2 October 1998 |url=http://www.slate.com/id/1908 |publisher=Slate Magazine|accessdate=2008-05-16}}</ref><ref>{{cite news |title=Bailout of Long-Term Capital: A Bad Precedent? |newspaper=[[The New York Times]] |date=December 26, 2008 |url=https://www.nytimes.com/2008/12/28/business/economy/28view.html }}</ref>\n\n[[Benoit Mandelbrot]], a French mathematician, extensively researched this issue. He felt that the extensive reliance on the normal distribution for much of the body of modern finance and [[investment theory]] is a serious flaw of any related models including the [[Black–Scholes]] option model developed by Myron Scholes and [[Fischer Black]], and the [[capital asset pricing model]] developed by [[William F. Sharpe]]. Mandelbrot explained his views and alternative finance theory in his book: ''The (Mis)Behavior of Markets: A Fractal View of Risk, Ruin, and Reward'' published on September 18, 1997.\n\n==See also==\n*[[Kurtosis]]\n*[[Skewness risk]]\n*[[Stochastic volatility]]\n*[[Holy grail distribution]]\n*[[Taleb distribution]]\n*''[[The Black Swan: The Impact of the Highly Improbable]]'' by [[Nassim Nicholas Taleb]]\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{cite book|last2=Hudson|first2=Richard L.|last1=Mandelbrot|first1=Benoit|authorlink1=Benoit Mandelbrot|title=The (Mis)Behavior of Markets: A Fractal View of Risk, Ruin, and Reward|year=2004|publisher=[[Basic Books]]|location=New York|isbn=0-465-04355-0}}\n*Premaratne, G., Bera, A. K. (2000). Modeling Asymmetry and Excess Kurtosis in Stock Return Data. Office of Research Working Paper Number 00-0123, University of Illinois\n\n[[Category:Normal distribution]]\n[[Category:Investment]]\n[[Category:Risk analysis]]\n[[Category:Mathematical finance]]"
    },
    {
      "title": "Log-normal distribution",
      "url": "https://en.wikipedia.org/wiki/Log-normal_distribution",
      "text": "{{Probability distribution\n  | name      = Log-normal\n  | type      = continuous\n  | pdf_image = [[Image:PDF-log normal distributions.svg|300px|Plot of the Lognormal PDF]]<br /><small>Some log-normal density functions with identical parameter <math>\\mu</math> but differing parameters <math>\\sigma</math></small>\n  | cdf_image = [[Image:CDF-log normal distributions.svg|300px|Plot of the Lognormal CDF]]<br /><small>Cumulative distribution function of the log-normal distribution (with <math>\\mu = 0</math> )</small>\n  | notation  = <math>\\operatorname{Lognormal}(\\mu,\\,\\sigma^2)</math>\n  | parameters= <math>\\mu \\in (-\\infty, +\\infty) </math>, <br /> <math>\\sigma > 0</math>\n  | support   = <math>x \\in (0, +\\infty)</math>\n  | pdf       = <math>\\frac 1 {x\\sigma\\sqrt{2\\pi}}\\ \\exp\\left(-\\frac{\\left(\\ln x-\\mu\\right)^2}{2\\sigma^2}\\right)</math>\n  | cdf       = <math>\\frac12 + \\frac12\\operatorname{erf}\\Big[\\frac{\\ln x-\\mu}{\\sqrt{2}\\sigma}\\Big]</math>\n  | quantile  = <math>\\exp(\\mu + \\sqrt{2\\sigma^2}\\operatorname{erf}^{-1}(2F-1))</math>\n  | mean      = <math>\\exp\\left(\\mu+\\frac{\\sigma^2}{2}\\right)</math>\n  | median    = <math>\\exp(\\mu)</math>\n  | mode      = <math>\\exp(\\mu-\\sigma^2)</math>\n  | variance  = <math>[\\exp(\\sigma^2)-1] \\exp(2\\mu+\\sigma^2)</math>\n  | skewness  = <math>(e^{\\sigma^2}\\!\\!+2) \\sqrt{e^{\\sigma^2}\\!\\!-1}</math>\n  | kurtosis  = <math>\\exp(4\\sigma^2) + 2\\exp(3\\sigma^2) + 3\\exp(2\\sigma^2) - 6</math>\n  | Value at risk = = <math>e^{\\mu+\\z_p\\sigma}</math>\n  | entropy   = <math>\\log_2(\\sigma e^{\\mu+\\tfrac12} \\sqrt{2\\pi} )</math>\n  | mgf       = defined only for numbers with a non-positive real part, see text\n  | char      = representation <math>\\sum_{n=0}^{\\infty}\\frac{(it)^n}{n!}e^{n\\mu+n^2\\sigma^2/2}</math> is asymptotically divergent but sufficient for numerical purposes\n  | fisher    = <math>\\begin{pmatrix}1/\\sigma^2&0\\\\0&1/2\\sigma^4\\end{pmatrix}</math>\n  }}\n\nIn [[probability theory]], a '''log-normal (or lognormal) distribution''' is a continuous [[probability distribution]] of a [[random variable]] whose [[logarithm]] is [[normal distribution|normally distributed]]. Thus, if the random variable {{mvar|X}} is log-normally distributed, then {{math|''Y'' {{=}} ln(''X'')}} has a normal distribution. Likewise, if {{mvar|Y}} has a normal distribution, then the [[exponential function]] of {{mvar|Y}}, {{math|''X'' {{=}} exp(''Y'')}}, has a log-normal distribution. A random variable which is log-normally distributed takes only positive real values. The distribution is occasionally referred to as the '''Galton distribution''' or '''Galton's distribution''', after [[Francis Galton]].<ref name=JKB/> The log-normal distribution also has been associated with other names, such as McAlister, [[Gibrat's law|Gibrat]] and [[Cobb–Douglas]].<ref name=JKB/>\n\nA log-normal process is the statistical realization of the multiplicative [[mathematical product|product]] of many [[statistical independence|independent]] [[random variable]]s, each of which is positive. This is justified by considering the [[central limit theorem]] in the log domain. The log-normal distribution is the [[maximum entropy probability distribution]] for a random variate {{mvar|X}} for which the mean and variance of {{math|ln(''X'')}} are specified.<ref>{{cite journal |last1=Park |first1=Sung Y. |last2=Bera |first2=Anil K. |year=2009 |title=Maximum entropy autoregressive conditional heteroskedasticity model |journal=Journal of Econometrics |volume=150 |issue=2 |pages=219–230 |doi=10.1016/j.jeconom.2008.12.014 |url=http://www.wise.xmu.edu.cn/Master/Download/..%5C..%5CUploadFiles%5Cpaper-masterdownload%5C2009519932327055475115776.pdf |accessdate=2011-06-02 |citeseerx=10.1.1.511.9750 }} Table 1, p. 221.</ref>\n\n==Notation==\nGiven a log-normally distributed random variable <math>X</math> and two parameters <math>\\mu</math> and <math>\\sigma</math> that are, respectively, the [[mean]] and [[standard deviation]] of the variable’s natural [[logarithm]], then the logarithm of <math>X</math> is normally distributed, and we can write <math>X</math> as\n\n:<math> X=e^{\\mu+\\sigma Z} </math>\n\nwith <math>Z</math> a [[standard normal variable]]. [[File:Lognormal Distribution.svg|thumb|upright=1.5|Relation between normal and lognormal distribution. If <math>Y=\\mu+\\sigma Z</math> is normally distributed, then <math>X\\sim e^{Y}</math> is lognormally distributed.]]\n\nThis relationship is true regardless of the base of the logarithmic or exponential function. If <math>\\log_a(Y)</math> is normally distributed, then so is <math>\\log_b(Y)</math>, for any two positive numbers <math>a,b\\neq 1</math>. Likewise, if <math>e^X</math> is log-normally distributed, then so is <math>a^X</math>, where <math>0 < a \\neq 1</math>.\n\nThe two parameters <math>\\mu</math> and <math>\\sigma</math> are not location and scale parameters for a lognormally distributed random variable&nbsp;''X'', but they are respectively location and scale parameters for the normally distributed logarithm'''&nbsp;ln'''(''X)''. The quantity ''e''<sup>''&mu;''</sup> is a scale parameter for the family of lognormal distributions.\n\nIn contrast, the mean and variance of the non-logarithmized sample values are respectively denoted <math>m</math>, and <math>v</math> in this article. The two sets of parameters can be related as (see also [[#Arithmetic moments|Arithmetic moments]] below)<ref>{{cite web|url=http://www.mathworks.com/help/stats/lognstat.html|title=Lognormal mean and variance - MATLAB lognstat|author=|date=|website=www.mathworks.com|accessdate=14 April 2018}}</ref>\n\n:<math> \\mu=\\ln\\left(\\frac{m}{\\sqrt{1+\\frac{v}{m^2}}}\\right), \n\\qquad \\sigma^2=\\ln\\left(1+\\frac{v}{m^2}\\right),</math>\n:<math>  m = \\exp\\left(\\mu+\\frac{\\sigma^2}{2}\\right), \n\\qquad v = \\left(\\exp(\\sigma^2)-1\\right) \\exp(2\\mu+\\sigma^2).</math>\n\n== Characterization ==\n\n=== Probability density function ===\n\nA positive random variable ''X'' is log-normally distributed if the logarithm of ''X'' is normally distributed,\n\n:<math> \\ln(X) \\sim \\mathcal N(\\mu,\\sigma^2).</math>\n\nLet <math>\\Phi</math> and <math>\\varphi</math> be respectively the cumulative probability distribution function and the probability density function of the ''N''(0,1) distribution.\n\nThen we have<ref name=JKB/>\n\n: <math>\n\\begin{align}\nf_X(x) & = \\frac {\\rm d}{{\\rm d}x} \\Pr(X \\le x) = \\frac {\\rm d}{{\\rm d}x} \\Pr(\\ln X \\le \\ln x) \\\\[6pt]\n& = \\frac {\\rm d}{{\\rm d}x} \\Phi\\left( \\frac{\\ln x -\\mu} \\sigma \\right) \\\\[6pt]\n& = \\varphi\\left( \\frac{\\ln x - \\mu} \\sigma \\right) \\frac {\\rm d}{{\\rm d}x} \\left( \\frac{\\ln x - \\mu} \\sigma \\right) \\\\[6pt]\n& = \\varphi\\left( \\frac{\\ln x - \\mu} \\sigma \\right) \\frac 1 {\\sigma x} \\\\[6pt]\n& = \\frac 1 x \\cdot \\frac 1 {\\sigma\\sqrt{2\\pi\\,}} \\exp\\left( -\\frac{(\\ln x-\\mu)^2}{2\\sigma^2} \\right).\n\\end{align}\n</math>\n\n=== Cumulative distribution function ===\n<!-- erf changed into erfc because then the formula is slightly shorter, and besides the expression with erf is already present in the floating box -->\nThe [[cumulative distribution function]] is\n\n: <math> F_X(x) = \\Phi\\left( \\frac{(\\ln x) - \\mu} \\sigma \\right) </math>\n\nwhere <math>\\Phi</math> is the cumulative distribution function of the standard normal distribution (i.e. ''N''(0,1)).\n\nThis may also be expressed as follows:\n\n:<math>\n\\frac12 \\left[ 1 + \\operatorname{erf} \\left(\\frac{\\ln x - \\mu}{\\sigma\\sqrt{2}}\\right) \\right] = \\frac12 \\operatorname{erfc} \\left(-\\frac{\\ln x - \\mu}{\\sigma\\sqrt{2}}\\right)\n</math>\n\nwhere erfc is the [[complementary error function]].\n\n=== Characteristic function and moment generating function ===\n\nAll moments of the log-normal distribution exist and\n\n:<math>\\operatorname{E}[X^n]= e^{n\\mu+n^2\\sigma^2/2}</math>\n\nThis can be derived by letting <math>z=\\frac{\\ln(x) - (\\mu+n\\sigma^2)}{\\sigma}</math> within the integral. However, the expected value <math>\\operatorname{E}[e^{t X}]</math> is not defined for any positive value of the argument <math>t</math> as the defining integral diverges.  In consequence the [[moment generating function]] is not defined.<ref name=\"Heyde\">{{citation|last=Heyde|first=CC.|title=On a property of the lognormal distribution|journal=Journal of the Royal Statistical Society, Series B|volume=25|issue=2|pages=392–393|year=1963|doi=10.1007/978-1-4419-5823-5_6|isbn=978-1-4419-5822-8}}</ref>  The last is related to the fact that the lognormal distribution is not uniquely determined by its moments.\n\nThe [[characteristic function (probability theory)|characteristic function]] <math>\\operatorname{E}[e^{i t X}]</math> is defined for real values of {{math|''t''}} but is not defined for any complex value of {{math|''t''}} that has a negative imaginary part, and therefore the characteristic function is not [[Analytic function|analytic]] at the origin.  In consequence, the characteristic function of the log-normal distribution cannot be represented as an infinite convergent series.<ref name=\"Holgate\">{{Cite journal|url = |title = The lognormal characteristic function, vol. 18, pp. 4539–4548, 1989|last = Holgate|first = P.|journal = Communications in Statistical – Theory and Methods|accessdate = |doi = 10.1080/03610928908830173|pages = 4539–4548|volume = 18|issue = 12|year = 1989}}</ref> In particular, its Taylor [[formal series]]  diverges:\n\n: <math>\\sum_{n=0}^\\infty \\frac{(it)^n}{n!}e^{n\\mu+n^2\\sigma^2/2}</math>\n\nHowever, a number of alternative [[divergent series]] representations have been obtained<ref name=Holgate /><ref name=\"Barakat\">{{cite journal|first=R. |last=Barakat |title=Sums of independent lognormally distributed random variables |journal=Journal of the Optical Society of America |volume=66 |issue=3 |pages=211–216 |doi=10.1364/JOSA.66.000211 |year=1976}}</ref><ref name=\"Barouch\">{{cite journal| last1=Barouch |first1=E. |last2=Kaufman |first2=GM. |first3=ML. |last3=Glasser |title=On sums of lognormal random variables |journal=Studies in Applied Mathematics |volume=75 |issue=1 |year=1986 |pages=37–55 |url=http://dspace.mit.edu/bitstream/handle/1721.1/48703/onsumsoflognorma00baro.pdf|doi=10.1002/sapm198675137 }}</ref><ref name=\"Leipnik\">{{cite journal|last=Leipnik |first=Roy B. |date= January 1991 |title=On Lognormal Random Variables: I – The Characteristic Function |journal=Journal of the Australian Mathematical Society Series B |volume=32 |issue=3 |pages=327–347 |doi=10.1017/S0334270000006901|url=https://www.cambridge.org/core/services/aop-cambridge-core/content/view/F1563B5AD8918EF2CD51092F82EB0B73/S0334270000006901a.pdf/div-class-title-on-lognormal-random-variables-i-the-characteristic-function-div.pdf }}</ref>\n\nA closed-form formula for the characteristic function <math>\\varphi(t)</math> with <math>t</math> in the domain of convergence is not known. A relatively simple approximating formula is available in closed form and given by<ref name=\"Asmussen\">S. Asmussen, J.L. Jensen, L. Rojas-Nandayapa (2016). \"On the Laplace transform of the Lognormal distribution\",\n[https://link.springer.com/article/10.1007/s11009-014-9430-7 Methodology and Computing in Applied Probability 18 (2), 441-458.]\n[http://data.imf.au.dk/publications/thiele/2013/math-thiele-2013-06.pdf Thiele report 6 (13).]</ref>\n\n:<math>\\varphi(t)\\approx\\frac{\\exp\\left(-\\frac{W^2(-it\\sigma^2e^\\mu) + 2W(-it\\sigma^2e^\\mu)}{2\\sigma^2} \\right)}{\\sqrt{1+W(-it\\sigma^2e^\\mu)}}</math>\n\nwhere <math>W</math> is the [[Lambert W function]].  This approximation is derived via an asymptotic method but it stays sharp all over the domain of convergence of <math>\\varphi</math>.\n\n== Properties ==\n\nLet <math>\\operatorname{GM}[X]</math> denote the [[geometric mean]], and <math>\\operatorname{GSD}[X]</math> the [[geometric standard deviation]] of the random variable&nbsp;<math>X</math>, and let <math>\\operatorname{E}[X]</math> and <math>\\operatorname{SD}[X]</math> be the arithmetic mean, or expected value, and the arithmetic standard deviation as usual.\n\n===Geometric moments===\n\nThe [[geometric mean]] of the log-normal distribution is <math>\\operatorname{GM}[X] = e^\\mu</math>, and the [[geometric standard deviation]] is <math>\\operatorname{GSD}[X] = e^{\\sigma}</math>.<ref name=\"ReferenceA\">{{cite journal |last1=Kirkwood |first1=Thomas BL |title=Geometric means and measures of dispersion |journal=Biometrics |date=Dec 1979 |volume=35 |issue=4 |pages=908–9|jstor=2530139 }}</ref><ref>{{cite journal |last1=Limpert |first1=E |last2=Stahel |first2=W |last3=Abbt |first3=M |title=Lognormal distributions across the sciences: keys and clues |journal=BioScience |year=2001 |volume=51 |issue=5 |pages=341–352 |doi=10.1641/0006-3568(2001)051[0341:LNDATS]2.0.CO;2}}</ref> By analogy with the arithmetic statistics, one can define a geometric variance, <math>\\operatorname{GVar}[X] = e^{\\sigma^2}</math>, and a [[Coefficient of variation#Log-normal data|geometric coefficient of variation]],<ref name=\"ReferenceA\"/> <math>\\operatorname{GCV}[X] = e^{\\sigma} - 1</math>.\n\nBecause the [[Data transformation (statistics)|log-transformed]] variable <math>Y = \\ln X</math> is symmetric and quantiles are preserved under monotonic transformations, the geometric mean of a log-normal distribution is equal to its median, <math>\\operatorname{Med}[X]</math>.<ref>{{cite book|first1=Leslie E. |last1=Daly |first2=Geoffrey Joseph |last2=Bourke |year=2000 |title=Interpretation and uses of medical statistics |edition=5th |publisher=Wiley-Blackwell |isbn=978-0-632-04763-5 |page=89 |doi=10.1002/9780470696750}}</ref>\n\nNote that the geometric mean is ''less'' than the arithmetic mean. This is due to the [[AM–GM inequality]], and corresponds to the logarithm being convex down. In fact,\n\n: <math>\\operatorname{E}[X] = e^{\\mu + \\frac12 \\sigma^2} = e^{\\mu} \\cdot \\sqrt{e^{\\sigma^2}} = \\operatorname{GM}[X] \\cdot \\sqrt{\\operatorname{GVar}[X]}.</math><ref name=\"Acoustic Stimuli Revisited 2016\">{{cite journal |last1=Heil P|first1=Friedrich B|title=Onset-Duration Matching of Acoustic Stimuli Revisited: Conventional Arithmetic vs. Proposed Geometric Measures of Accuracy and Precision|journal=Frontiers in Psychology|volume=7|pages=2013|doi=10.3389/fpsyg.2016.02013|pmid=28111557|pmc=5216879|year=2017}}</ref>\n\nIn finance the term <math>e^{-\\frac12\\sigma^2}</math> is sometimes interpreted as a [[convexity correction]]. From the point of view of [[stochastic calculus]], this is the same correction term as in [[Itō's lemma#Geometric Brownian motion|Itō's lemma for geometric Brownian motion]].\n\n===Arithmetic moments===\n\nFor any real or complex number {{math|''n''}}, the {{math|''n''}}-th [[moment (mathematics)|moment]] of a log-normally distributed variable {{math|''X''}} is given by<ref name=JKB/>\n: <math>\\operatorname{E}[X^n] = e^{n\\mu + \\frac12n^2\\sigma^2}.</math>\n\nSpecifically, the arithmetic mean, expected square, arithmetic variance, and arithmetic standard deviation of a log-normally distributed variable {{math|''X''}} are given by\n\n: <math>\\begin{align}\n  \\operatorname{E}[X] & = e^{\\mu + \\tfrac{1}{2}\\sigma^2}, \\\\[4pt]\n  \\operatorname{E}[X^2] & = e^{2\\mu + 2\\sigma^2}, \\\\[4pt]\n  \\operatorname{Var}[X] & = \\operatorname{E}[X^2] - \\operatorname[{E}X]^2 = (\\operatorname{E}[X])^2(e^{\\sigma^2} - 1) = e^{2\\mu + \\sigma^2} (e^{\\sigma^2} - 1), \\\\[4pt]\n  \\operatorname{SD}[X] & = \\sqrt{\\operatorname{Var}[X]} = \\operatorname{E}[X] \\sqrt{e^{\\sigma^2} - 1}\n  = e^{\\mu + \\tfrac{1}{2}\\sigma^2}\\sqrt{e^{\\sigma^2} - 1},\n  \\end{align}</math>\n\nrespectively.\n\nThe parameters {{math|''μ''}} and {{math|''σ''}} can be obtained if the arithmetic mean and the arithmetic variance are known:\n\n: <math>\\begin{align}\n\\mu &= \\ln \\left(\\frac{\\operatorname{E}[X]^2}{\\sqrt{\\operatorname{E}[X^2]}}\\right) = \\ln \\left( \\frac{\\operatorname{E}[X]^2}{\\sqrt{\\operatorname{Var}[X] + \\operatorname{E}[X]^2}} \\right), \\\\[4pt]\n  \\sigma^2 &=  \\ln \\left(\\frac{\\operatorname{E}[X^2]}{\\operatorname{E}[X]^2}\\right) =  \\ln \\left(1 + \\frac{\\operatorname{Var}[X]}{\\operatorname{E}[X]^2}\\right).\n  \\end{align}</math>\n\nA probability distribution is not uniquely determined by the moments {{math|1=E[''X''<sup>''n''</sup>] = e<sup>''nμ'' + {{sfrac|1|2}}''n''<sup>2</sup>''σ''<sup>2</sup></sup>}} for {{math|''n'' ≥ 1}}.  That is, there exist other distributions with the same set of moments.<ref name=JKB/> In fact, there is a whole family of distributions with the same moments as the log-normal distribution.{{Citation needed|date=March 2012}}\n\n=== Mode and median ===\n[[Image:Comparison mean median mode.svg|thumb|upright=1.25|Comparison of [[mean]], [[median]] and [[mode (statistics)|mode]] of two log-normal distributions with different [[skewness]].]]\nThe [[mode (statistics)|mode]] is the point of global maximum of the probability density function.  In particular, it solves the equation <math>(\\ln f)'=0</math>:\n\n: <math>\\operatorname{Mode}[X] = e^{\\mu - \\sigma^2}.</math>\n\nThe [[median]] is such a point where <math>F_X=0.5</math>:\n\n:<math>\\operatorname{Med}[X] = e^\\mu.</math>\n\n=== Arithmetic coefficient of variation ===\nThe arithmetic [[coefficient of variation]] <math>\\operatorname{CV}[X]</math> is the ratio <math>\\frac{\\operatorname{SD}[X]}{\\operatorname{E}[X]}</math> (on the natural scale). For a log-normal distribution it is equal to\n: <math>\\operatorname{CV}[X] = \\sqrt{e^{\\sigma^2} - 1}.</math>\nContrary to the arithmetic standard deviation, the arithmetic coefficient of variation is independent of the arithmetic mean.\n\n=== Partial expectation ===\n\nThe partial expectation of a random variable <math>X</math> with respect to a threshold <math>k</math> is defined as\n\n:<math> g(k) = \\int_k^\\infty x f_X(x)\\, dx. </math>\n\nAlternatively, and using the definition of [[conditional expectation]], it can be written as <math>g(k)=\\operatorname{E}[X\\mid X>k] P(X>k)</math>. For a log-normal random variable the partial expectation is given by:\n\n:<math>g(k) = \\int_k^\\infty x f_X(x)\\, dx  = e^{\\mu+\\tfrac{1}{2} \\sigma^2}\\, \\Phi\\!\\left(\\frac{\\mu+\\sigma^2-\\ln k} \\sigma \\right) </math>\n\nwhere <math>\\Phi</math> is the [[normal cumulative distribution function]]. The derivation of the formula is provided in the discussion of this Wikipedia entry. The partial expectation formula has applications in insurance and economics, it is used in solving the partial differential equation leading to the [[Black–Scholes formula]].\n\n=== Conditional expectation ===\n\nThe conditional expectation of a lognormal random variable <math>X</math> with respect to a threshold <math>k</math> is its partial expectation divided by the cumulative probability of being in that range:\n\n:<math>\\begin{align}\nE[X\\mid X<k] & =e^{\\mu +\\frac{\\sigma^2}{2}}\\cdot \\frac{\\Phi \\left[\\frac{\\ln(k)-\\mu -\\sigma^2}{\\sigma} \\right]}{\\Phi \\left[\\frac{\\ln(k)-\\mu}{\\sigma} \\right]} \\\\[8pt]\nE[X\\mid X\\geqslant k] &=e^{\\mu +\\frac{\\sigma^2}{2}}\\cdot \\frac{\\Phi \\left[\\frac{\\mu +\\sigma^2-\\ln(k)} \\sigma \\right]}{1-\\Phi \\left[\\frac{\\ln(k)-\\mu}{\\sigma}\\right]}\n\\end{align}</math>\n\n=== Other ===\n\nA set of data that arises from the log-normal distribution has a symmetric [[Lorenz curve]] (see also [[Lorenz asymmetry coefficient]]).<ref name=EcolgyArticle>{{cite journal\n  | doi = 10.1890/0012-9658(2000)081[1139:DIIPSO]2.0.CO;2\n  | last1 = Damgaard\n  | first1 = Christian\n  | first2 = Jacob |last2=Weiner\n  | title = Describing inequality in plant size or fecundity\n  | journal = Ecology\n  | year = 2000  | volume = 81 | issue = 4 | pages = 1139–1142\n}}</ref>\n\nThe harmonic <math>H</math>, geometric <math>G</math> and arithmetic <math>A</math> means of this distribution are related;<ref name=Rossman1990>{{cite journal|last=Rossman |first=Lewis A |date=July 1990 |title=Design stream ﬂows based on harmonic means |journal=Journal of Hydraulic Engineering |volume=116 |issue=7 |pages=946–950 |doi=10.1061/(ASCE)0733-9429(1990)116:7(946)}}</ref> such relation is given by\n\n: <math>H = \\frac{G^2} A.</math>\n\nLog-normal distributions are [[infinite divisibility (probability)|infinitely divisible]],<ref name=OlofThorin1978LNInfDivi/> but they are not [[stable distribution]]s, which can be easily drawn from.<ref name=Gao/>\n\n==Occurrence and applications==\nThe log-normal distribution is important in the description of natural phenomena.  This follows, because many natural growth processes are driven by the accumulation of many small percentage changes.  These become additive on a log scale.  If the effect of any one change is negligible, the [[central limit theorem]] says that the distribution of their sum is more nearly normal than that of the summands.  When back-transformed onto the original scale, it makes the distribution of sizes approximately log-normal (though if the standard deviation is sufficiently small, the normal distribution can be an adequate approximation).\n\nThis multiplicative version of the [[central limit theorem]]  is also known as  [[Gibrat's law]], after Robert Gibrat (1904–1980)  who formulated it for companies.<ref>{{cite journal|jstor=2729692| last=Sutton |first=John |date=Mar 1997 |title=Gibrat's Legacy |journal=Journal of Economic Literature |volume=32 |issue=1 |pages=40–59}}</ref> If the rate of accumulation of these small changes does not vary over time, growth becomes independent of size.  Even if that's not true, the size distributions at any age of things that grow over time tends to be log-normal.\n\nExamples include the following:\n\n* Human behaviors\n** The length of comments posted in Internet discussion forums follows a log-normal distribution.<ref>{{cite journal |last1=Pawel |first1=Sobkowicz|title=Lognormal distributions of user post lengths in Internet discussions - a consequence of the Weber-Fechner law? |journal=EPJ Data Science |year=2013|display-authors=etal}}</ref>\n** Users' dwell time on online articles (jokes, news etc.) follows a log-normal distribution.<ref>{{cite conference |last1=Yin|first1=Peifeng |last2=Luo|first2=Ping |last3=Lee|first3=Wang-Chien  |last4=Wang|first4=Min |title=Silence is also evidence: interpreting dwell time for recommendation from psychological perspective |conference=ACM International Conference on KDD |year=2013 |URL=http://mldm.ict.ac.cn/platform/pweb/academicDetail.htm?id=16}}</ref>\n** The length of [[chess]] games tends to follow a log normal distribution.<ref>{{cite web|url=http://chess.stackexchange.com/questions/2506/what-is-the-average-length-of-a-game-of-chess/4899#4899|title=What is the average length of a game of chess?|author=|date=|website=chess.stackexchange.com|accessdate=14 April 2018}}</ref>\n** Onset durations of acoustic comparison stimuli that are matched to a standard stimulus follow a log-normal distribution.<ref name=\"Acoustic Stimuli Revisited 2016\"/>\n* In [[biology]] and [[medicine]],  \n** Measures of size of living tissue (length, skin area, weight);<ref>{{cite book\n  | last = Huxley | first = Julian S.\n  | year = 1932\n  | title = Problems of relative growth\n  | publisher = London\n  | oclc = 476909537\n  | ref = harv\n  | isbn = 978-0-486-61114-3\n  }}</ref>\n** For highly communicable epidemics, such as SARS in 2003, if publication intervention is involved, the number of hospitalized cases is shown to satisfy the lognormal distribution with no free parameters if an entropy is assumed and the standard deviation is determined by the principle of maximum rate of entropy production.{{Citation needed|date=August 2018}}\n** The length of inert appendages (hair, claws, nails, teeth) of biological specimens, in the direction of growth;{{Citation needed|date=February 2011}}\n** The normalised RNA-Seq readcount for any genomic region can be well approximated by log-normal distribution.\n** Certain physiological measurements, such as blood pressure of adult humans (after separation on male/female subpopulations)<ref>{{cite journal|last=Makuch|first=Robert W. |author2=D.H. Freeman |author3=M.F. Johnson|title=Justification for the lognormal distribution as a model for blood pressure|journal=Journal of Chronic Diseases|year=1979|volume=32|issue=3|pages=245–250|doi=10.1016/0021-9681(79)90070-5|url=http://www.sciencedirect.com/science/article/pii/0021968179900705|accessdate=27 February 2012}}</ref>\n** In neuroscience, the distribution of firing rates across a population of neurons is often approximately lognormal. This has been first observed in the cortex and striatum <ref>{{Cite conference|last=Scheler|first=Gabriele|last2=Schumann|first2=Johann|title=Diversity and stability in neuronal output rates|conference=36th Society for Neuroscience Meeting, Atlanta|date=2006-10-08}}</ref> and later in hippocampus and entorhinal cortex,<ref>{{Cite journal|last=Mizuseki|first=Kenji|last2=Buzsáki|first2=György|date=2013-09-12|title=Preconfigured, skewed distribution of firing rates in the hippocampus and entorhinal cortex|journal=Cell Reports|volume=4|issue=5|pages=1010–1021|doi=10.1016/j.celrep.2013.07.039|issn=2211-1247|pmc=3804159|pmid=23994479}}</ref> and elsewhere in the brain.<ref>{{Cite journal|last=Buzsáki|first=György|last2=Mizuseki|first2=Kenji|date=2017-01-06|title=The log-dynamic brain: how skewed distributions affect network operations|journal=Nature Reviews. Neuroscience|volume=15|issue=4|pages=264–278|doi=10.1038/nrn3687|issn=1471-003X|pmc=4051294|pmid=24569488}}</ref><ref>{{Cite journal|last=Wohrer|first=Adrien|last2=Humphries|first2=Mark D.|last3=Machens|first3=Christian K.|date=2013-04-01|title=Population-wide distributions of neural activity during perceptual decision-making|journal=Progress in Neurobiology|volume=103|pages=156–193|doi=10.1016/j.pneurobio.2012.09.004|issn=1873-5118|pmid=23123501|pmc=5985929}}</ref> Also, intrinsic gain distributions and synaptic weight distributions appear to be lognormal<ref>{{Cite journal|last=Scheler|first=Gabriele|title=Logarithmic distributions prove that intrinsic learning is Hebbian|journal=F1000Research| doi=10.12688/f1000research.12130.2|date=2017-07-28|pmid=29071065|volume=6|pmc=5639933|page=1222}}</ref> as well.\n* In [[colloidal chemistry]] and [[polymer chemistry]]\n** [[Particle size distribution]]s\n** [[Molar mass distribution]]s\nConsequently, [[reference ranges]] for measurements in healthy individuals are more accurately estimated by assuming a log-normal distribution than by assuming a symmetric distribution about the mean.\n[[File:FitLogNormDistr.tif|thumb|Fitted cumulative log-normal distribution to annually maximum 1-day rainfalls, see [[distribution fitting]] ]]\n\n*In [[hydrology]], the log-normal distribution is used to analyze extreme values of such variables as monthly and annual maximum values of daily rainfall and river discharge volumes.<ref>{{cite book |last=Oosterbaan |first=R.J. |editor-last=Ritzema |editor-first=H.P. |chapter=6: Frequency and Regression Analysis |year=1994 |title=Drainage Principles and Applications, Publication 16 |publisher=International Institute for Land Reclamation and Improvement (ILRI) |location=Wageningen, The Netherlands |pages=175–224 |chapter-url=http://www.waterlog.info/pdf/freqtxt.pdf |isbn=978-90-70754-33-4}}</ref>\n\n::The image on the right, made with [[CumFreq]], illustrates an example of fitting the log-normal distribution to ranked annually maximum one-day rainfalls showing also the 90% [[confidence belt]] based on the [[binomial distribution]].<ref>[https://www.waterlog.info/cumfreq.htm CumFreq, free software for distribution fitting]</ref>\n\n::The rainfall data are represented by [[plotting position]]s as part of a [[cumulative frequency analysis]].\n\n* In social sciences and demographics\n** In [[economics]], there is evidence that the [[income]] of 97%–99% of the population is distributed log-normally.<ref>Clementi, Fabio; [[Mauro Gallegati|Gallegati, Mauro]] (2005) [http://ideas.repec.org/p/wpa/wuwpmi/0505006.html  \"Pareto's law of income distribution: Evidence for Germany, the United Kingdom, and the United States\"], EconWPA</ref>   (The distribution of higher-income individuals follows a [[Pareto distribution]].<ref>{{Cite journal|url = |title = Physics of Personal Income|last = Wataru|first = Souma|date = 2002-02-22|journal = |accessdate = |doi = |arxiv = cond-mat/0202388|bibcode = 2002cond.mat..2388S}}</ref>)\n** In [[finance]], in particular the [[Black–Scholes model]], changes in the ''logarithm'' of exchange rates, price indices, and stock market indices are assumed normal<ref>{{Cite journal | doi = 10.1086/260062| title = The Pricing of Options and Corporate Liabilities| journal = Journal of Political Economy| volume = 81| issue = 3| pages = 637| year = 1973| last1 = Black | first1 = F. | last2 = Scholes | first2 = M. }}</ref> (these variables behave like compound interest, not like simple interest, and so are multiplicative). However, some mathematicians such as [[Benoît Mandelbrot]] have argued <ref>{{cite book|last=Mandelbrot|first=Benoit|title=The (mis-)Behaviour of Markets |year=2004 |url=https://books.google.com/?id=9w15j-Ka0vgC |publisher=Basic Books |isbn=9780465043552}}</ref> that [[Lévy skew alpha-stable distribution|log-Lévy distributions]], which possesses [[heavy tails]] would be a more appropriate model, in particular for the analysis for [[stock market crash]]es. Indeed, stock price distributions typically exhibit a [[fat tail]].;<ref>Bunchen, P., ''Advanced Option Pricing'', University of Sydney coursebook, 2007</ref>   the fat tailed distribution of changes during stock market crashes invalidate the assumptions of the [[central limit theorem]].  \n** In [[scientometrics]], the number of citations to journal articles and patents follows a discrete log-normal distribution.<ref>{{cite journal |last1=Thelwall|first1=Mike|last2=Wilson|first2=Paul |title=Regression for citation data: An evaluation of different methods |journal=Journal of Infometrics |year=2014 |volume=8 |issue=4 |pages=963–971 |doi=10.1016/j.joi.2014.09.011|arxiv=1510.08877}}</ref> \n** [[Historical urban community sizes|City sizes]].\n* Technology\n** In [[Reliability (statistics)|reliability]] analysis, the lognormal distribution is often used to model times to repair a maintainable system.<ref>{{cite book  | last = O'Connor | first = Patrick\n  | last2 = Kleyner | first2 = Andre  | year = 2011\n  | title = Practical Reliability Engineering\n  | publisher = John Wiley & Sons\n  | isbn = 978-0-470-97982-2\n  | page = 35\n  }}</ref>\n** In [[wireless communication]], \"the local-mean power expressed in logarithmic values, such as dB or neper, has a normal (i.e., Gaussian) distribution.\"<ref>{{cite web |title=Shadowing |website=www.WirelessCommunication.NL |url=http://wireless.per.nl/reference/chaptr03/shadow/shadow.htm |dead-url=yes |archive-url=https://web.archive.org/web/20120113201345/http://wireless.per.nl/reference/chaptr03/shadow/shadow.htm |archive-date=January 13, 2012 }}</ref> Also, the random obstruction of radio signals due to large buildings and hills, called [[Fading|shadowing]], is often modeled as a lognormal distribution.\n** Particle size distributions produced by comminution with random impacts, such as in [[ball mill]]ing\n** The [[file size]] distribution of publicly available audio and video data files ([[MIME types]]) follows a log-normal distribution over five [[orders of magnitude]].<ref>\n{{cite journal |last1=Gros |first1=C |last2=Kaczor |first2=G.|last3=Markovic |first3=D |title=Neuropsychological constraints to human data production on a global scale |journal=The European Physical Journal B |year=2012 |volume=85 |issue=28 |pages=28 |doi=10.1140/epjb/e2011-20581-3|arxiv=1111.6849 |bibcode=2012EPJB...85...28G }}</ref>\n** In computer networks and [[Internet traffic]] analysis, lognormal is shown as a good statistical model to represent the amount of traffic per unit time. This has been shown by applying a robust statstical apporach on a large groups of real Interent traces. In this context, the log-normal distribution has shown a good performace in two main use cases: (1) predicting the proportion of time traffic will exceed a given level (for service level agreement or link capacity estimation) i.e. link dimensioning based on bandwidth provisioning and (2) predicting 95th percentile pricing.<ref>{{cite arxiv | last = Alamsar | first = Mohammed\n  | last2 = Parisis | first2 = George | last3 = Clegg | first3 = Richard | last4 = Zakhleniuk | first4 = Nickolay | year = 2019\n  | title = On the Distribution of Traffic Volumes in the Internet and its Implications\n  | eprint = 1902.03853\n  }}</ref>\n\n==  Extremal principle of entropy to fix the free parameter  <math>\\sigma</math>==\n\n* In applications,  <math>\\sigma</math> is a parameter to be determined.  For growing processes balanced by production and dissipation, the use of a extremal principle of Shannon entropy shows that    \n:<math>\\begin{align}\n\\sigma=1\\big/\\sqrt{6}\n\\end{align}</math> <ref name=bai>{{cite journal|last1=Wu|first1=Ziniu|last2=Li|first2=Juan|last3=Bai|first3=Chenyuan|title=Scaling Relations of Lognormal Type Growth Process with an Extremal Principle of Entropy|journal=Entropy|volume=19|issue=56|year=2017|pages=1–14|doi=10.3390/e19020056|bibcode=2017Entrp..19...56W}}</ref>\n\n* This value can then be used to give some scaling relation between the inflexion point and maximum point of the lognormal distribution.<ref name=bai/> It is shown that this relationship is determined by the base of natural logarithm, <math>e=2.718\\ldots</math>, and exhibits some geometrical similarity to the minimal surface energy principle.\n*These scaling relations are shown to be useful for predicting a number of growth processes  (epidemic spreading, droplet splashing, population growth, swirling rate of the bathtub vortex, distribution of language characters, velocity profile of turbulences, etc.).\n*For instance, the lognormal function with such  <math>\\sigma</math> fits well with the size of secondary produced droplet during droplet impact <ref name=wu/> and the spreading of one epidemic disease.<ref name=Wang>{{cite journal|last1=Wang|first1=WenBin|last2=Wu|first2=ZiNiu|last3=Wang|first3=ChunFeng|last4=Hu|first4=RuiFeng|title=Modelling the spreading rate of controlled communicable epidemics through an entropy-based thermodynamic model|journal=Science China Physics, Mechanics and Astronomy|volume=56|issue=11|year=2013|pages=2143–2150|issn=1674-7348|doi=10.1007/s11433-013-5321-0|arxiv=1304.5603|bibcode=2013SCPMA..56.2143W}}</ref>\n* The value <math>\\sigma=1\\big/\\sqrt{6}</math> is used to provide a probablistic solution for the Drake equation.<ref name=Bloetscher>{{cite journal|last1=Bloetscher|first1=Frederick|title=Using predictive Bayesian Monte Carlo- Markov Chain methods to provide a probablistic solution for the Drake equation|journal=Acta Astronautica\n|volume=155|issue= |year=2019|pages=118–130|doi=10.1016/j.actaastro.2018.11.033|bibcode=}}</ref>\n\n== Maximum likelihood estimation of parameters ==\n\nFor determining the [[maximum likelihood]] estimators of the log-normal distribution parameters ''μ'' and ''σ'', we can use the [[normal distribution#Estimation of parameters|same procedure]] as for the [[normal distribution]].  To avoid repetition, we observe that\n\n: <math>L (\\mu, \\sigma) = \\prod_{i=1}^n \\frac 1 {x_i} \\varphi_{\\mu,\\sigma} (\\ln x_i)</math>\n\nwhere <math>\\varphi_{}</math> is the density function of the normal distribution <math>\\mathcal N(\\mu,\\sigma^2)</math>.  Therefore, using the same indices to denote distributions, we can write the log-likelihood function thus:\n\n:<math>\\begin{align}\n\\ell (\\mu,\\sigma \\mid x_1, x_2, \\ldots, x_n) &= - \\sum _k \\ln x_k + \\ell_N (\\mu, \\sigma \\mid \\ln x_1, \\ln x_2, \\dots, \\ln x_n) \\\\\n&= \\text{constant} + \\ell_N (\\mu, \\sigma \\mid \\ln x_1, \\ln x_2, \\dots, \\ln x_n).\n\\end{align}</math>\n\nSince the first term is constant with regard to ''μ'' and ''σ'', both logarithmic likelihood functions, <math>\\ell_L</math> and <math>\\ell_N</math>, reach their maximum with the same <math>\\mu</math> and <math>\\sigma</math>.  Hence, using the formulas for the normal distribution maximum likelihood parameter estimators and the equality above, we deduce that for the log-normal distribution it holds that\n\n: <math>\\widehat \\mu = \\frac {\\sum_k \\ln x_k}{n}, \\qquad   \\widehat \\sigma^2 = \\frac {\\sum_k \\left( \\ln x_k - \\widehat \\mu \\right)^2} {n}.</math>\n\n== Multivariate log-normal ==\nIf <math>\\boldsymbol X \\sim \\mathcal{N}(\\boldsymbol\\mu,\\,\\boldsymbol\\Sigma)</math> is a [[multivariate normal distribution]] then <math>\\boldsymbol Y=\\exp(\\boldsymbol X)</math> has a multivariate log-normal distribution<ref>{{cite conference|last=Tarmast |first=Ghasem |year=2001 |url=http://isi.cbs.nl/iamamember/CD2/pdf/329.PDF |title=Multivariate Log–Normal Distribution |conference=ISI Proceedings: 53rd Session |location=Seoul}}</ref><ref>{{cite conference|last=Halliwell |first=Leigh |year=2015 |url=http://www.casact.org/pubs/forum/15spforum/Halliwell.pdf |title=The Lognormal Random Multivariate |conference=Casualty Actuarial Society E-Forum, Spring 2015 |location=Arlington, VA}}</ref> with mean\n\n:<math>\\operatorname{E}[\\boldsymbol Y]_i=e^{\\mu_i+\\frac{1}{2}\\Sigma_{ii}} ,</math>\n\nand [[covariance matrix]]\n\n:<math>\\operatorname{Var}[\\boldsymbol Y]_{ij}=e^{\\mu_i+\\mu_j + \\frac{1}{2}(\\Sigma_{ii}+\\Sigma_{jj}) }( e^{\\Sigma_{ij}} - 1) . </math>\n\n== Related distributions ==\n* If <math>X \\sim \\mathcal{N}(\\mu, \\sigma^2)</math> is a [[normal distribution]], then <math>\\exp(X) \\sim \\operatorname{Lognormal}(\\mu, \\sigma^2).</math>\n* If <math>X \\sim \\operatorname{Lognormal}(\\mu, \\sigma^2)</math> is distributed  log-normally, then <math>\\ln(X) \\sim \\mathcal{N}(\\mu, \\sigma^2)</math> is a normal random variable.\n* If <math>X_j \\sim \\operatorname{Lognormal} (\\mu_j, \\sigma_j^2)</math> are <math>n</math> [[statistical independence|independent]] log-normally distributed variables, and <math>Y = \\textstyle\\prod_{j=1}^n X_j</math>, then <math>Y</math> is also distributed log-normally:\n::<math>Y \\sim \\operatorname{Lognormal} \\Big(\\textstyle \\sum_{j=1}^n\\mu_j,\\ \\sum_{j=1}^n \\sigma_j^2 \\Big).</math>\n* Let <math>X_j \\sim \\operatorname{Lognormal}(\\mu_j,\\sigma_j^2)\\ </math> be independent log-normally distributed variables with possibly varying <math>\\sigma</math> and <math>\\mu</math> parameters, and <math>Y=\\textstyle\\sum_{j=1}^n X_j</math>.  The distribution of <math>Y</math> has no closed-form expression, but can be reasonably approximated by another log-normal distribution <math>Z</math> at the right tail.<ref name=\"Asmussen2\">{{cite journal|first1=S. |last1=Asmussen |first2=L. |last2=Rojas-Nandayapa |title=Asymptotics of Sums of Lognormal Random Variables with Gaussian Copula |journal=Statistics and Probability Letters |volume=78 |issue=16 |pages=2709–2714 |year=2008 |doi=10.1016/j.spl.2008.03.035}}</ref> Its probability density function at the neighborhood of 0 has been characterized<ref name=Gao/>  and it does not resemble any log-normal distribution. A commonly used approximation due to L.F. Fenton (but previously stated by R.I. Wilkinson and mathematical justified by Marlow<ref name=\"Marlow\">{{cite journal|first=NA. |last=Marlow |title=A normal limit theorem for power sums of independent normal random variables |journal=Bell System Technical Journal |volume=46 |issue=9 |pages=2081–2089 |date=Nov 1967 |doi=10.1002/j.1538-7305.1967.tb04244.x}}</ref>) is obtained by matching the mean and variance of another lognormal distribution:\n::<math>\\begin{align}\n  \\sigma^2_Z &= \\ln\\!\\left[ \\frac{\\sum e^{2\\mu_j+\\sigma_j^2}(e^{\\sigma_j^2}-1)}{(\\sum e^{\\mu_j+\\sigma_j^2/2})^2} + 1\\right], \\\\\n  \\mu_Z &= \\ln\\!\\left[ \\sum e^{\\mu_j+\\sigma_j^2/2} \\right] - \\frac{\\sigma^2_Z}{2}.\n  \\end{align}</math>\n:In the case that all <math>X_j</math> have the same variance parameter <math>\\sigma_j=\\sigma</math>, these formulas simplify to\n::<math>\\begin{align}\n  \\sigma^2_Z &= \\ln\\!\\left[ (e^{\\sigma^2}-1)\\frac{\\sum e^{2\\mu_j}}{(\\sum e^{\\mu_j})^2} + 1\\right], \\\\\n  \\mu_Z &= \\ln\\!\\left[ \\sum e^{\\mu_j} \\right] + \\frac{\\sigma^2}{2} -  \\frac{\\sigma^2_Z}{2}.\n  \\end{align}</math>\nFor a more accurate approximation one can use the [[Monte Carlo method]] to estimate the cumulative distribution function, the pdf and right tail. \n<ref name=\"BotLec2017\">{{cite conference \n|url=https://ieeexplore.ieee.org/document/8247924/\n|title=Accurate computation of the right tail of the sum of dependent log-normal variates \n|last1=Botev |first1=Z. I. \n|last2=L'Ecuyer |first2=P.  \n|date=2017 \n|publisher=IEEE\n|ISBN=978-1-5386-3428-8\n|book-title= 2017 Winter Simulation Conference (WSC) \n|pages=1880–1890 \n|location=  3th–6th Dec 2017 Las Vegas, NV, USA\n|doi= 10.1109/WSC.2017.8247924 \n|arxiv=1705.03196}}\n</ref>\n<ref name=\"AGL2016\">{{cite arXiv \n|last1=Asmussen\n|first1=A.\n|last2=Goffard\n|first2=P.-O. \n|last3=Laub\n|first3=P. J.\n|date=2016 \n|title=Orthonormal polynomial expansions and lognormal sum densities\n|eprint=1601.01763v1|class=math.PR\n}}\n</ref>\n* If <math>X \\sim \\operatorname{Lognormal}(\\mu, \\sigma^2)</math> then <math>X+c</math> is said to have a ''Three-parameter log-normal'' distribution with support <math>x\\in (c, +\\infty)</math>.<ref name=\"Sangal1970\">{{cite journal|first1=B. |last1=Sangal |first2=A. |last2=Biswas |title=The 3-Parameter Lognormal Distribution Applications in Hydrology |journal=Water Resources Research |volume=6 |issue=2 |pages=505–515 |year=1970 |doi=10.1029/WR006i002p00505}}</ref> <math>\\operatorname{E}[X+c] = \\operatorname{E}[X]+c</math>, <math>\\operatorname{Var}[X+c] = \\operatorname{Var}[X]</math>.\n* If <math>X \\sim \\operatorname{Lognormal}(\\mu, \\sigma^2)</math> then <math>a X \\sim \\operatorname{Lognormal}( \\mu + \\ln a,\\ \\sigma^2).</math>\n* If <math>X \\sim \\operatorname{Lognormal}(\\mu, \\sigma^2)</math> then <math>\\tfrac{1}{X} \\sim \\operatorname{Lognormal}(-\\mu,\\ \\sigma^2).</math>\n* If <math>X \\sim \\operatorname{Lognormal}(\\mu, \\sigma^2)</math> then <math>X^a \\sim \\operatorname{Lognormal}(a\\mu,\\ a^2 \\sigma^2)</math> for <math>a \\neq 0.</math>\n* Lognormal distribution is a special case of semi-bounded [[Johnson distribution]]\n* If <math>X\\mid Y \\sim \\operatorname{Rayleigh}(Y)\\,</math> with <math> Y \\sim \\operatorname{Lognormal}(\\mu, \\sigma^2)</math>, then <math> X \\sim \\operatorname{Suzuki}(\\mu, \\sigma)</math> ([[Suzuki distribution]])\n* A substitute for the log-normal whose integral can be expressed in terms of more elementary functions<ref>{{Cite journal | last1 = Swamee | first1 = P. K. | title = Near Lognormal Distribution | doi = 10.1061/(ASCE)1084-0699(2002)7:6(441) | journal = Journal of Hydrologic Engineering | volume = 7 | issue = 6 | pages = 441–444 | year = 2002 | pmid =  | pmc = }}</ref> can be obtained based on the [[logistic distribution]] to get an approximation for the [[Cumulative distribution function|CDF]]\n\n::<math> F(x;\\mu,\\sigma) = \\left[\\left(\\frac{e^\\mu}{x}\\right)^{\\pi/(\\sigma \\sqrt{3})} +1\\right]^{-1}.</math>\n\n: This is a [[log-logistic distribution]].\n\n== See also ==\n\n* [[Log-distance path loss model]]\n* [[Slow fading]]\n* [[Multiplicative calculus]]\n\n== Notes ==\n{{Reflist|30em|refs=\n\n<ref name=JKB>{{Citation | last1=Johnson | first1=Norman L. | last2=Kotz | first2=Samuel | last3=Balakrishnan | first3=N. | title=Continuous univariate distributions. Vol. 1 | publisher=[[John Wiley & Sons]] | location=New York | edition=2nd | series=Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics | isbn=978-0-471-58495-7 | mr = 1299979| year=1994 |chapter=14: Lognormal Distributions}}</ref>\n\n<ref name=Gao>{{cite journal | doi = 10.1155/2009/630857 | volume=2009 | title=Asymptotic Behavior of Tail Density for Sum of Correlated Lognormal Variables | year=2009 | journal=International Journal of Mathematics and Mathematical Sciences | pages=1–28 | last1 = Gao | first1 = Xin}}</ref>\n\n<ref name=wu>{{cite journal | doi = 10.1016/S0266-8920(03)00028-6 | volume=18 | issue=3 | title=Prediction of the size distribution of secondary ejected droplets by crown splashing of droplets impinging on a solid wall | year=2003 | journal=Probabilistic Engineering Mechanics | pages=241–249 | last1 = Wu | first1 = Zi-Niu}}</ref>\n\n<ref name=OlofThorin1978LNInfDivi>{{cite journal|last1=Thorin|first1=Olof|title=On the infinite divisibility of the lognormal distribution|journal=Scandinavian Actuarial Journal|volume=1977|issue=3|year=1977|pages=121–148|issn=0346-1238|doi=10.1080/03461238.1977.10405635}}</ref>\n\n}}\n\n== Further reading ==\n* {{Citation\n | editor-last = Crow\n | editor-first = Edwin L.\n | editor-last2 = Shimizu\n | editor-first2 = Kunio\n | title = Lognormal Distributions, Theory and Applications\n | place = New York\n | publisher = Marcel Dekker, Inc.\n | series = Statistics: Textbooks and Monographs\n | volume = 88\n | year = 1988\n | pages = xvi+387\n | isbn = 978-0-8247-7803-3\n | mr = 0939191\n | zbl = 0644.62014}}\n* Aitchison, J. and Brown, J.A.C. (1957)  ''The Lognormal Distribution'', Cambridge University Press.\n* {{cite journal |last1=Limpert |first1=E |last2=Stahel |first2=W |last3=Abbt |first3=M |title=Lognormal distributions across the sciences: keys and clues |journal=BioScience |year=2001 |volume=51 |issue=5 |pages=341–352 |doi=10.1641/0006-3568(2001)051[0341:LNDATS]2.0.CO;2}}\n* [[Eric W. Weisstein]] et al. [http://mathworld.wolfram.com/LogNormalDistribution.html Log Normal Distribution] at [[MathWorld]]. Electronic document, retrieved October 26, 2006.\n* {{Cite journal | last1 = Holgate | first1 = P. | title = The lognormal characteristic function | doi = 10.1080/03610928908830173 | journal = Communications in Statistics - Theory and Methods | volume = 18 | issue = 12 | pages = 4539–4548 | year = 1989 | pmid =  | pmc = }}\n* {{cite journal | last1 = Brooks | first1 = Robert | authorlink3 = Jimbo Wales | last2 = Corson | first2 = Jon | last3 = Donal | first3 = Wales | title = The Pricing of Index Options When the Underlying Assets All Follow a Lognormal Diffusion | ssrn = 5735 | journal = Advances in Futures and Options Research | volume = 7 | issue = | year = 1994 }}\n\n==External links==\n* [https://sites.google.com/site/nonnewtoniancalculus/ Non-Newtonian calculus website]\n\n{{commons category}}\n\n{{ProbDistributions|continuous-semi-infinite}}\n\n{{DEFAULTSORT:Log-Normal Distribution}}\n[[Category:Continuous distributions]]\n[[Category:Normal distribution]]\n[[Category:Exponential family distributions]]\n[[Category:Non-Newtonian calculus]]"
    },
    {
      "title": "Markov chain central limit theorem",
      "url": "https://en.wikipedia.org/wiki/Markov_chain_central_limit_theorem",
      "text": "{{expert}}In the mathematical theory of [[stochastic processes|random processes]], the '''Markov chain central limit theorem''' has a conclusion somewhat similar in form to that of the classic [[central limit theorem]] (CLT) of probability theory, but the quantity in the role taken by the variance in the classic CLT has a more complicated definition.\n\n== Statement ==\n\nSuppose that:\n\n* the sequence <math display=\"inline\"> X_1,X_2,X_3,\\ldots </math> of [[random element]]s of some set is a [[Markov chain]] that has a [[Markov_chain#Steady-state_analysis_and_limiting_distributions|stationary probability distribution]]; and\n* the initial distribution of the process, i.e. the distribution of <math display=\"inline\"> X_1</math>, is the stationary distribution, so that <math display=\"inline\"> X_1,X_2,X_3,\\ldots</math> are identically distributed. In the classic central limit theorem these random variables would be assumed to be [[independence (probability theory)|independent]], but here we have only the weaker assumption that the process has the [[Markov property]]; and\n* <math display=\"inline\"> g</math> is some (measurable) real-valued function for which <math display=\"inline\"> \\operatorname{var}(g(X_1)) <+\\infty.</math>\nNow let\n: <math>\n\\begin{align}\n\\mu & = \\operatorname E(g(X_1)), \\\\\n\\sigma^2 & = \\operatorname{var}(g(X_1)) + 2\\sum_{k=1}^\\infty \\operatorname{cov}( g(X_1), g(X_{1+k})), \\\\\n\\widehat\\mu_n & = \\frac 1 n \\sum_{k=1}^n g(X_k).\n\\end{align}\n</math>\nThen as <math display=\"inline\"> n \\to\\infty,</math> we have<ref>Geyer, Charles&nbsp;J. (2011).   Introduction  to  Markov Chain Monte Carlo.  In ''Handbook of MarkovChain Monte Carlo''.  Edited by S. P. Brooks, A. E. Gelman, G. L. Jones,and X. L. Meng.  Chapman & Hall/CRC, Boca Raton, FL, Section 1.8. http://www.mcmchandbook.net/HandbookChapter1.pdf</ref>\n: <math> \\hat\\mu_n \\approx \\operatorname{Normal}\\left( \\mu, \\frac{\\sigma^2} n \\right),\n</math>\nor more precisely,\n: <math>\n\\sqrt{n} (\\hat{\\mu}_n - \\mu) \\ \\xrightarrow{\\mathcal{D}} \\ \\text{Normal}(0, \\sigma^2),\n</math>\nwhere the decorated arrow indicates [[Convergence_of_random_variables#Convergence_in_distribution|convergence in distribution]].\n\n== Use ==\n\nThe Markov chain central limit theorem can be used to justify estimation of <math display=\"inline\"> \\mu = \\operatorname E(g(X_1)) </math> by [[Markov chain Monte Carlo]] MCMC methods, and provides bounds on the probable error of estimation.\n\n== References ==\n\n* Gordin, M. I. and Lifšic, B. A. (1978). \"Central limit theorem for stationary Markov processes.\" ''Soviet Mathematics, Doklady'', '''19''', 392–394. (English translation of Russian original).\n* Geyer, Charles J. (2011). \"Introduction to MCMC.\" In ''Handbook of Markov Chain Monte Carlo'', edited by S. P. Brooks, A. E. Gelman, G. L. Jones, and X. L. Meng. Chapman & Hall/CRC, Boca Raton, pp.&nbsp;3–48.\n{{reflist}}\n\n[[Category:Markov processes]]\n[[Category:Markov models]]\n[[Category:Stochastic processes]]\n[[Category:Stochastic models]]\n[[Category:Probability theorems]]\n[[Category:Central limit theorem| ]]\n[[Category:Asymptotic theory (statistics)]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Maxwell–Boltzmann distribution",
      "url": "https://en.wikipedia.org/wiki/Maxwell%E2%80%93Boltzmann_distribution",
      "text": "{{Short description|Specific probability distribution function, important in physics}}\n{{Hide in print|1={{Probability distribution\n| name       = Maxwell–Boltzmann\n| type       = density\n| pdf_image  = [[File:Maxwell-Boltzmann distribution pdf.svg|325px]]\n| cdf_image  = [[File:Maxwell-Boltzmann distribution cdf.svg|325px]]\n| parameters = <math>a>0</math>\n| support    = <math>x\\in (0;\\infty)</math>\n| pdf        = <math>\\sqrt{\\frac{2}{\\pi}} \\frac{x^2 e^{-x^2/\\left(2a^2\\right)}}{a^3}</math>\n| cdf        = <math>\\operatorname{erf}\\left(\\frac{x}{\\sqrt{2} a}\\right) -\\sqrt{\\frac{2}{\\pi}} \\frac{x e^{-x^2/\\left(2a^2\\right)}}{a} </math> where erf is the [[error function]]\n| mean       = <math>\\mu=2a \\sqrt{\\frac{2}{\\pi}}</math>\n| median     =\n| mode       = <math>\\sqrt{2} a</math>\n| variance   = <math>\\sigma^2=\\frac{a^2(3 \\pi - 8)}{\\pi}</math>\n| skewness   = <math>\\gamma_1=\\frac{2 \\sqrt{2} (16 -5 \\pi)}{(3 \\pi - 8)^{3/2}}</math>\n| kurtosis   = <math>\\gamma_2=4\\frac{\\left(-96+40\\pi-3\\pi^2\\right)}{(3 \\pi - 8)^2}</math>\n| entropy    = <math>\\ln\\left(a\\sqrt{2\\pi}\\right)+\\gamma-\\frac{1}{2}</math>\n| mgf        =\n| char       =\n}}}}\nIn [[physics]] (in particular in [[statistical mechanics]]), the '''Maxwell–Boltzmann distribution''' is a particular [[probability distribution]] named after [[James Clerk Maxwell]] and [[Ludwig Boltzmann]].\n\nIt was first defined and used for describing particle [[speed]]s in [[ideal gas|idealized gases]], where the particles move freely inside a stationary container without interacting with one another, except for very brief [[collision]]s in which they exchange energy and momentum with each other or with their thermal environment.  The term \"particle\" in this context refers to gaseous particles only ([[atoms]] or [[molecules]]), and the system of particles is assumed to have reached [[thermodynamic equilibrium]].<ref name=\"StatisticalPhysics\">''Statistical Physics'' (2nd Edition), F. Mandl, Manchester Physics, John Wiley & Sons, 2008, {{isbn|9780471915331}}</ref>  The energies of such particles follow what is known as  [[Maxwell-Boltzmann statistics]], and the statistical distribution of speeds is derived by equating particle energies with [[kinetic energy]].\n\nMathematically, the Maxwell–Boltzmann distribution is the [[chi distribution]] with three [[degrees of freedom]] (the components of the [[velocity]] vector in [[Euclidean space]]), with a [[scale parameter]] measuring speeds in units proportional to the square root of <math>T/m</math> (the ratio of temperature and particle mass).<ref>University Physics – With Modern Physics (12th Edition), H.D. Young, R.A. Freedman (Original edition), Addison-Wesley (Pearson International), 1st Edition: 1949, 12th Edition: 2008, {{isbn|978-0-321-50130-1}}</ref>\n\nThe Maxwell–Boltzmann distribution is a result of the [[kinetic theory of gases]], which provides a simplified explanation of many fundamental gaseous properties, including [[pressure]] and [[diffusion]].<ref>Encyclopaedia of Physics (2nd Edition), R.G. Lerner, G.L. Trigg, VHC publishers, 1991, {{isbn|3-527-26954-1}} (Verlagsgesellschaft), {{isbn|0-89573-752-3}} (VHC Inc.)</ref> The Maxwell–Boltzmann distribution applies fundamentally to particle velocities in three dimensions, but turns out to depend only on the speed (the [[Magnitude (mathematics)|magnitude]] of the velocity) of the particles.  A particle speed probability distribution indicates which speeds are more likely: a particle will have a speed selected randomly from the distribution, and is more likely to be within one range of speeds than another.  The kinetic theory of gases applies to the classical [[ideal gas]], which is an idealization of real gases. In real gases, there are various effects (e.g., [[van der Waals interaction]]s, [[vortex|vortical flow]], [[special relativity|relativistic]] speed limits, and [[quantum mechanics|quantum]] [[exchange interaction]]s) that can make their speed distribution different from the Maxwell–Boltzmann form. However, [[rarefied]] gases at ordinary temperatures behave very nearly like an ideal gas and the Maxwell speed distribution is an excellent approximation for such gases. Ideal [[plasma (physics)|plasmas]], which are ionized gases of sufficiently low density, frequently also have particle distributions that are partially or entirely Maxwellian.<ref>N.A. Krall and A.W. Trivelpiece, Principles of Plasma Physics, San Francisco Press, Inc., 1986, among many other texts on basic plasma physics</ref>\n\nThe distribution was first derived by Maxwell in 1860 on heuristic grounds.<ref name=Maxwell>See:\n*  Maxwell, J.C. (1860 A): ''Illustrations of the dynamical theory of gases. Part I. On the motions and collisions of perfectly elastic spheres.  The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science'', 4th Series, vol.19, pp.19-32. [https://www.biodiversitylibrary.org/item/53795#page/33/mode/1up] \n*  Maxwell, J.C. (1860 B): ''Illustrations of the dynamical theory of gases. Part II. On the process of diffusion of two or more kinds of moving particles among one another.  The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science'', 4th Ser., vol.20, pp.21-37. [https://www.biodiversitylibrary.org/item/20012#page/37/mode/1up]</ref> Boltzmann later, in the 1870s, carried out significant investigations into the physical origins of this distribution.\n\nThe distribution can be derived on the ground that it maximizes the entropy of the system. A list of derivations are:\n\n# [[Maximum entropy probability distribution#Distributions with measured constants|Maximum entropy probability distribution]] in the phase space, with the constraint of [[Conservation of energy|Conservation of average energy]] <math>\\langle H \\rangle = E</math>;\n# [[Canonical ensemble]].\n\n== Distribution function ==\nAssuming the system of interest contains a large number of particles, the fraction of the particles within an infinitesimal element of three-dimensional velocity space, <math>d^3v</math>, centered on a velocity vector of magnitude <math>v</math>, is <math>f(v) d^3v</math>, in which\n\n:<math> f(v) ~\\mathrm{d}^3v = \\left(\\frac{m}{2 \\pi kT}\\right)^{3/2}\\,  e^{- \\frac{m|v|^2}{2kT}} ~\\mathrm{d}^3v, </math>\n\nwhere <math>m</math> is the particle mass and <math>kT</math> is the product of [[Boltzmann's constant]] and [[thermodynamic temperature]].\n\n[[Image:MaxwellBoltzmann-en.svg|right|thumb|340px|The speed probability density functions of the speeds of a few [[noble gas]]es at a temperature of 298.15&nbsp;K (25&nbsp;°C). The ''y''-axis is in s/m so that the area under any section of the curve (which represents the probability of the speed being in that range) is dimensionless.]]\nOne can write the element of velocity space as d<math>^3v</math> = d<math>v_x</math>d<math>v_y</math>d<math>v_z</math>, for velocities in a standard Cartesian coordinate system, or as d<math>^3v</math> = <math>v^2</math>d<math>v</math>d<math>\\Omega</math> in a standard spherical coordinate system, where d<math>\\Omega</math> is an element of solid angle. Here <math>f(v)</math> is given as a probability distribution function, properly normalized so that <math>\\int f(v)</math>d<math>^3v</math> over all velocities equals one.  In plasma physics, the probability distribution is often multiplied by the particle density, so that the integral of the resulting distribution function equals the density.\n\nUndergraduate students are likely to work with the Maxwellian distribution function for particles moving in only one direction. If this direction is <math>x</math>, then one has \n:<math> f(v_x) ~\\mathrm{d}v_x = \\left(\\frac{m}{2 \\pi kT}\\right)^{1/2}\\,  e^{- \\frac{mv_x^2}{2kT}} ~\\mathrm{d}v_x, </math> \nwhich can be obtained by integrating the three-dimensional form given above over <math>v_y</math> and <math>v_z</math>.\n\nRecognizing the symmetry of <math>f(v)</math>, one can integrate over solid angle and write a probability distribution of speeds as the function<ref>[[Harald J. W. Mueller-Kirsten|H.J.W. Müller-Kirsten]] (2013), ''Basics of Statistical Physics'', 2nd ed., [[World Scientific]], {{isbn|978-981-4449-53-3}}, Chapter 2.</ref>\n\n:<math> f(v) ~\\mathrm{d}v= \\left(\\frac{m}{2 \\pi kT}\\right)^{3/2}\\, 4\\pi v^2 e^{- \\frac{mv^2}{2kT}} ~\\mathrm{d}v, </math>\n\nThis [[probability density function]] gives the probability, per unit speed, of finding the particle with a speed near <math>v</math>. This equation is simply the Maxwell–Boltzmann distribution (given in the infobox) with distribution parameter <math>a=\\sqrt{kT/m}</math>. \nThe Maxwell–Boltzmann distribution is equivalent to the [[chi distribution]] with three degrees of freedom and [[scale parameter]] <math>a=\\sqrt{kT/m}</math>.\n\nThe simplest [[ordinary differential equation]] satisfied by the distribution is:\n\n:<math>k T v f'(v)+f(v) (m v^2-2 kT)=0,</math>\n:<math>f(1)=\\sqrt{\\frac{2}{\\pi }} e^{-\\frac{m}{2 k T}} \\left(\\frac{m}{k T}\\right)^{3/2}</math>\n\nor in unitless presentation:\n\n:<math>a^2 x f'(x)+\\left(x^2-2 a^2\\right) f(x)=0, </math>\n:<math>f(1)=\\frac{\\sqrt{\\frac{2}{\\pi }} e^{-\\frac{1}{2 a^2}}}{a^3}.</math>\n<!--Note that a distribution (function) is not the same as the probability. The distribution (function) stands for an average number, as in all three kinds of statistics (Maxwell–Boltzmann, [[Bose–Einstein statistics|Bose–Einstein]], [[Fermi–Dirac statistics|Fermi–Dirac]]).--> \nWith the [[Darwin–Fowler method]] of mean values the Maxwell–Boltzmann distribution is obtained as an exact result.\n\n== Relaxation to the 2D Maxwell-Boltzmann distribution ==\n[[File:Simulation of gas for relaxation demonstration.gif|thumb|471x471px|Simulation of a 2D gas relaxing towards a Maxwell-Boltzmann speed distribution]]\nFor particles confined to move in a plane, the speed distribution is given by\n\n<math>P(s < |\\vec{v}| < s + ds) = \\frac{ms}{kT}\\exp\\bigg(-\\frac{ms^2}{2kT}\\bigg) ds  </math>\n\nThis distribution is used for describing systems in equilibrium. However, most systems do start out in their equilibrium state. The evolution of a system towards its equilibrium state is governed by the [[Boltzmann equation]]. The equation predicts that for short range interactions, the equilibrium velocity distribution will follow a Maxwell–Boltzmann distribution. To the right is a [[molecular dynamics]] (MD) simulation in which 900 [[Hard spheres|hard sphere]] particles are constrained to move in a rectangle. They interact via [[Elastic collision|perfectly elastic collisions]]. The system is initialized out of equilibrium, but we see that the velocity distribution (in blue) quickly converges to the 2D Maxwell–Boltzmann distribution (in orange).\n\n==Typical speeds==\nThe [[expectation value|mean]] speed <math> \\langle v \\rangle</math>, \nmost probable speed ([[Mode (statistics)|mode]]) {{mvar|v<sub>p</sub>}}, \nand root-mean-square speed  <math>\\sqrt{\\langle v^2 \\rangle}</math> \ncan be obtained from properties of the Maxwell distribution.\n\nThis works well for nearly [[ideal gas|ideal]], [[noble gas|monatomic]] gases like [[helium]], but also for [[Molecule|molecular gas]]es like diatomic [[oxygen]]. \nThis is because despite the larger [[heat capacity]] (larger internal energy at the same temperature) due to their larger number of [[Equipartition theorem|degrees of freedom]],  their [[Translation (physics)|translational]] [[kinetic energy]] (and thus their speed) is unchanged.<ref>\n{{cite book\n | title = College Physics, Volume 1\n | edition = 9th\n | author1=Raymond A. Serway |author2=Jerry S. Faughn |author3=Chris Vuille  |last-author-amp=yes | publisher = \n | year = 2011\n | isbn = 9780840068484\n | page = 352\n | url = https://books.google.com/books?id=HLxV-IKYO5IC&pg=PA352\n }}</ref>\n\n{{unordered list\n|1= The most probable speed, {{mvar|v<sub>p</sub>}}, is the speed most likely to be possessed by any molecule (of the same mass {{mvar|m}}) in the system and corresponds to the maximum value or [[mode (statistics)|mode]] of {{mvar|f(v)}}.  To find it, we calculate the [[derivative]] {{mvar|df/dv}}, set it to zero and solve for {{mvar|v}}:\n\n:<math>\\frac{df(v)}{dv} = -8\\pi \\left(\\frac{m}{2 \\pi kT}\\right)^{3/2}\\ v\\ e^{-\\frac{mv^2}{2kT}} \\left(\\frac{mv^2}{2kT}-1\\right) = 0</math>\n\nwith the solution:\n:<math>\\frac{mv_p^2}{2kT} = 1 </math>\n:<math>v_p = \\sqrt { \\frac{2kT}{m} } = \\sqrt { \\frac{2RT}{M} }</math>\n(the second solution <math>v=0</math> representing the \"least probable speed\").\n\n{{mvar|R}} is the [[gas constant]] and {{mvar|M}} is molar mass of the substance, and thus may be calculated as a product of particle mass, {{math|''m''}}, and [[Avogadro constant]], {{math|''N''<sub>''a''</sub>}}:\n:<math>M = m N_a</math>\n\nFor diatomic nitrogen (N<sub>2</sub>, the primary component of [[air]])<ref>The calculation is unaffected by the Nitrogen being diatomic. Despite the larger [[heat capacity]] (larger internal energy at the same temperature) of diatomic gases relative to monatomic gases, due to their larger number of [[Equipartition theorem|degrees of freedom]],  <math>{{3RT}\\over{M_m}}</math> is still the mean [[Translation (physics)|translational]] [[kinetic energy|kinetic]] energy. Nitrogen being diatomic only affects the value of the molar mass {{mvar|M}} {{=}} {{val|28|u=g/mol}}.\nSee e.g. K. Prakashan, ''Engineering Physics'' (2001), [https://books.google.com/books?id=6C0R1qpAk7EC&pg=SA2-PA278 2.278].</ref> \nat [[room temperature]] ({{val|300|u=K}}), this gives \n:<math>v_p \\approx  \\sqrt{\\frac{2\\cdot{}8.31\\ \\text{J}\\cdot{}\\text{mol}^{-1}\\text{K}^{-1}\\ 300\\ \\text{K}}{0.028\\ \\text{kg}\\cdot{}\\text{mol}^{-1}}} \\approx  422\\ \\text{m/s}.</math>\n|2= The mean speed is the [[expected value]] of the speed distribution, setting <math>b=\\frac{1}{2a^2}=\\frac{m}{2kT}</math>:\n\n:<math> \\langle v \\rangle = \\int_0^{\\infty} v \\, f(v) \\, dv</math>\n::<math> =  4 \\pi \\left (\\frac{b}{ \\pi  } \\right )^\\frac{3}{2} \\int_{0}^{\\infty} v^3\\ e^{-bv^{2}}dv\\,\\!</math>\n::<math> = 4 \\pi \\left (\\frac{b}{  \\pi } \\right )^\\frac{3}{2}  \\frac{1}{2b^2} = \\sqrt{\\frac{4}{\\pi b}} \\,\\!</math>\n::<math>= \\sqrt { \\frac{8kT}{\\pi m}}= \\sqrt { \\frac{8RT}{\\pi M}} = \\frac{2}{\\sqrt{\\pi}} v_p  </math>\n\n|3= The mean square speed <math>\\langle v^2 \\rangle</math> is the second-order [[Moment (mathematics)|raw moment]] of the speed distribution. The \"root mean square speed\" <math> v_\\mathrm{rms}</math>  is the square root of the mean square speed, corresponding to the speed of a particle with median [[kinetic energy]], setting <math>b=\\frac{1}{2a^2}=\\frac{m}{2kT}</math>:\n:<math> v_\\mathrm{rms} = \\sqrt{\\langle v^2 \\rangle} = \\left(\\int_0^{\\infty} v^2 \\, f(v) \\, dv  \\right)^{1/2}</math>\n<!-- The \\,\\! is to keep the formula rendered as PNG instead of HTML. Please don't remove it.-->\n::<math> = \\left( 4 \\pi \\left (\\frac{b}{  \\pi } \\right )^\\frac{3}{2} \\int_{0}^{\\infty}  v^4\\ e^{-bv^2}dv\\right)^{1/2}\\,\\!</math>\n::<math> = \\left(4 \\pi \\left (\\frac{b}{\\pi}\\right )^\\frac{3}{2} \\frac{3}{8}  \\sqrt{\\frac{\\pi}{b^5}} \\right)^{1/2} = \\left( \\frac{3}{2b} \\right)^{1/2}\\,\\!</math>\n:<math>= \\sqrt { \\frac{3kT}{m}}= \\sqrt { \\frac{3RT}{M} } = \\sqrt{ \\frac{3}{2} } v_p  </math>\n}}\n\nIn summary, the typical speeds are related as follows:\n:<math>v_p \\approx 88.6\\%\\ \\langle v \\rangle  < \\langle v \\rangle < 108.5\\%\\  \\langle v \\rangle \\approx v_\\mathrm{rms}. </math>\n\nThe root mean square speed is directly related to the [[speed of sound]] {{mvar|c}} in the gas, by\n:<math>c = \\sqrt{\\frac{\\gamma}{3}}\\ v_\\mathrm{rms}  = \\sqrt{\\frac{f+2}{3f}}\\ v_\\mathrm{rms} = \\sqrt{\\frac{f+2}{2f}}\\ v_p ,</math>\nwhere  <math>\\gamma = 1 + \\frac{2}{f}</math> is the [[adiabatic index]], \n{{mvar|f}} is the number of [[degrees of freedom]] of the individual gas molecule. \nFor the example above, diatomic nitrogen (approximating [[air]]) at {{val|300|u=K}}, <math>f = 5</math><ref>\nNitrogen at room temperature is considered a \"rigid\" diatomic gas, with two rotational degrees of freedom additional to the three translational ones, and the vibrational degree of freedom not accessible.</ref> and\n:<math>c = \\sqrt{\\frac{7}{15}}v_\\mathrm{rms} \\approx 68\\%\\ v_\\mathrm{rms} \\approx 84\\%\\ v_p  \\approx 353\\ \\mathrm{m/s}, </math>\nthe true value for air can be approximated by using the average molar weight of [[Atmospheric chemistry|air]] ({{val|29|u=g/mol}}), yielding \n{{val|347|u=m/s}} at {{val|300|u=K}} (corrections for variable [[humidity]] are of the order of 0.1% to 0.6%).\n\nThe average relative velocity\n:<math> v_{\\rm rel} \\equiv \\langle |\\vec{v}_1-\\vec{v}_2| \\rangle = \\int\\!d^3v_1d^3v_2|\\vec{v}_1-\\vec{v}_2|f(\\vec{v}_1)f(\\vec{v}_2) = \\frac{4}{\\sqrt{\\pi}}\\frac{kT}{m} = 2\\langle v \\rangle </math>\n\nwhere the three-dimensional velocity distribution is\n\n:<math> f(\\vec{v}) \\equiv \\frac{1}{(2\\pi kT/)^{3/2}}e^{-\\frac12 m\\vec{v}^2/kT}. </math>\n\nThe integral can easily be done by changing to coordinates <math> \\vec{u} = \\vec{v}_1-\\vec{v}_2 </math> and <math> \\vec{U} = \\frac{\\vec{v}_1+\\vec{v}_2}{2}.</math>\n\n==Derivation and related distributions==\n===Maxwell–Boltzmann statistics===\n{{main|Maxwell–Boltzmann statistics#Derivations|Boltzmann distribution}}\nThe original derivation in 1860 by [[James Clerk Maxwell]] was an argument based on molecular collisions of the [[Kinetic theory of gases]] as well as certain symmetries in the speed distribution function; Maxwell also gave an early argument that these molecular collisions entail a tendency towards equilibrium.<ref name=Maxwell/><ref>{{Cite journal | last1 = Gyenis | first1 = Balazs | doi = 10.1016/j.shpsb.2017.01.001 | title = Maxwell and the normal distribution: A colored story of probability, independence, and tendency towards equilibrium | journal = Studies in History and Philosophy of Modern Physics | volume = 57 | pages = 53–65 | year = 2017| arxiv = 1702.01411 | bibcode = 2017SHPMP..57...53G }}</ref> After Maxwell, [[Ludwig Boltzmann]] in 1872<ref>Boltzmann, L., \"Weitere studien über das Wärmegleichgewicht unter Gasmolekülen.\" ''Sitzungsberichte der Kaiserlichen Akademie der Wissenschaften in Wien, mathematisch-naturwissenschaftliche Classe'', '''66''', 1872, pp. 275–370.</ref> also derived the distribution on mechanical grounds and argued that gases should over time tend toward this distribution, due to collisions (see [[H-theorem]]). He later (1877)<ref>Boltzmann, L., \"Über die Beziehung zwischen dem zweiten Hauptsatz der mechanischen Wärmetheorie und der Wahrscheinlichkeitsrechnung respektive den Sätzen über das Wärmegleichgewicht.\" ''Sitzungsberichte der Kaiserlichen Akademie der Wissenschaften in Wien, Mathematisch-Naturwissenschaftliche Classe''. Abt. II, '''76''', 1877, pp. 373–435. Reprinted in ''Wissenschaftliche Abhandlungen'', Vol. II, pp. 164–223, Leipzig: Barth, 1909.</ref> derived the distribution again under the framework of [[statistical thermodynamics]]. The derivations in this section are along the lines of Boltzmann's 1877 derivation, starting with result known as [[Maxwell–Boltzmann statistics]] (from statistical thermodynamics). Maxwell–Boltzmann statistics gives the average number of particles found in a given single-particle [[Microstate (statistical mechanics)|microstate]]. Under certain assumptions, the logarithm of the fraction of particles in a given microstate is proportional   to the ratio of the energy of that state to the temperature of the system:\n:<math>-\\log \\left(\\frac{N_i}{N}\\right) \\propto \\frac{E_i}{T}.</math>\nThe assumptions of this equation are that the particles do not interact, and that they are classical; this means that each particle's state can be considered independently from the other particles' states. Additionally, the particles are assumed to be in thermal equilibrium.<ref name=\"StatisticalPhysics\" /><ref>McGraw Hill Encyclopaedia of Physics (2nd Edition), C.B. Parker, 1994, {{isbn|0-07-051400-3}}</ref>\n\nThis relation can be written as an equation by introducing a normalizing factor:\n{{NumBlk|:|<math>\n\\frac{N_i} N = \\frac{\\exp(-E_i/kT) } { \\sum_j \\exp(-E_j/kT) }</math>\n|{{EquationRef|1}}}}\n\nwhere:\n* {{mvar|N<sub>i</sub>}} is the expected number of particles in the single-particle microstate {{mvar|i}},\n* {{mvar|N}} is the total number of particles in the system,\n* {{mvar|E<sub>i</sub>}}  is the energy of microstate {{mvar|i}},\n* the sum over index {{mvar|j}} takes into account all microstates, \n*{{mvar|T}} is the equilibrium temperature of the system,\n*{{mvar|k}} is the [[Boltzmann constant]].\nThe denominator in Equation ({{EquationNote|1}}) is simply a normalizing factor so that the ratios <math>N_i:N</math>  add up to unity — in other words it is a kind of [[partition function (statistical mechanics)|partition function]] (for the single-particle system, not the usual partition function of the entire system).\n\nBecause velocity and speed are related to energy, Equation ({{EquationNote|1}}) can be used to derive relationships between temperature and the speeds of gas particles. All that is needed is to discover the density of microstates in energy, which is determined by dividing up momentum space into equal sized regions.\n\n===Distribution for the momentum vector===\n\nThe potential energy is taken to be zero, so that all energy is in the form of kinetic energy.\nThe relationship between [[Kinetic energy#Kinetic energy of rigid bodies|kinetic energy and momentum]] for massive non-[[special relativity|relativistic]] particles is\n\n{{NumBlk|:|<math>E=\\frac{p^2}{2m}</math>|{{EquationRef|2}}}}\n\nwhere ''p''<sup>2</sup> is the square of the momentum vector \n'''p'''&nbsp;=&nbsp;[''p''<sub>''x''</sub>,&nbsp;''p''<sub>''y''</sub>,&nbsp;''p''<sub>''z''</sub>]. We may therefore rewrite Equation ({{EquationNote|1}}) as:\n\n{{NumBlk|:|<math>\n\\frac{N_i}{N} = \n\\frac{1}{Z} \n\\exp \\left[\n-\\frac{p_{i, x}^2 + p_{i, y}^2 + p_{i, z}^2}{2mkT}\n\\right]</math>\n|{{EquationRef|3}}}}\n\nwhere ''Z'' is the [[partition function (statistical mechanics)|partition function]], corresponding to the denominator in Equation ({{EquationNote|1}}). Here ''m'' is the molecular mass of the gas, ''T'' is the thermodynamic temperature and ''k'' is the [[Boltzmann constant]].  This distribution of <math>N_i:N</math> is [[Proportionality (mathematics)|proportional]] to the [[probability density function]] ''f''<sub>'''p'''</sub> for finding a molecule with these values of momentum components, so:\n\n{{NumBlk|:|<math>\nf_\\mathbf{p} (p_x, p_y, p_z) \n\\propto\n\\exp \\left[\n-\\frac{p_x^2 + p_y^2 + p_z^2}{2mkT}\n\\right]</math>|{{EquationRef|4}}}}\n\nThe [[normalizing constant]]  can be determined by recognizing that the probability of a molecule having ''some'' momentum must be 1.  \nIntegrating  the exponential in ({{EquationNote|4}}) over all ''p''<sub>''x''</sub>, ''p''<sub>''y''</sub>, and ''p''<sub>''z''</sub> yields a factor of \n:<math>\\iiint_{-\\infty}^{+\\infty} \\exp \\left[ -\\frac{p_x^2 + p_y^2 + p_z^2}{2mkT}\\right] dp_x\\ dp_y\\ dp_z  = {(\\sqrt{\\pi} \\sqrt{2mkT})^3}</math>\n\nSo that the normalized distribution function is:\n{{Equation box 1 |indent=: |equation=\n<math>\nf_\\mathbf{p} (p_x, p_y, p_z) =\n\\left( 2 \\pi mkT \\right)^{-3/2}\n\\exp \\left[\n-\\frac{p_x^2 + p_y^2 + p_z^2}{2mkT}\n\\right]</math>\n|cellpadding |border |border colour = #50C878 |background colour = #ECFCF4|ref=6}}\n\nThe distribution is seen to be the product of three independent [[normal distribution|normally distributed]] variables <math>p_x</math>, <math>p_y</math>, and <math>p_z</math>, with variance <math>mkT</math>. Additionally, it can be seen that the magnitude of momentum will be distributed as a Maxwell–Boltzmann distribution, with <math>a=\\sqrt{mkT}</math>.\nThe Maxwell–Boltzmann distribution for the momentum (or equally for the velocities) can be obtained more fundamentally using the [[H-theorem]] at equilibrium within the [[Kinetic theory of gases]] framework.\n\n===Distribution for the energy===\n\nThe energy distribution is found imposing\n{{NumBlk|:|<math>\n  f_E(E) dE = f_p(\\textbf p) d^3 \\textbf p,\n</math>|{{EquationRef|7}}}}\nwhere <math>d^3 \\textbf p</math> is the infinitesimal phase-space volume of momenta corresponding to the energy interval <math>dE</math>.\nMaking use of the spherical symmetry of the energy-momentum dispersion relation <math>E=| \\textbf p|^2/2m</math>,\nthis can be expressed in terms of <math>dE</math> as\n{{NumBlk|:|<math>\n  d^3 \\textbf p = 4 \\pi | \\textbf p |^2 d |\\textbf p| = 4 \\pi m \\sqrt{2mE} dE.\n</math>|{{EquationRef|8}}}}\nUsing then ({{EquationNote|8}}) in ({{EquationNote|7}}), and expressing everything in terms of the energy <math>E</math>, we get\n:<math>\n  f_E(E) dE = \\frac{1}{(2\\pi m k T)^{3/2}} e^{-E/kT} 4 \\pi m \\sqrt{2mE} dE = 2 \\sqrt{\\frac{E}{\\pi}} \\left( \\frac{1}{kT} \\right)^{3/2} \\exp\\left(\\frac{-E}{kT} \\right) dE\n</math>\nand finally\n\n{{Equation box 1 |indent=: |equation=\n<math>f_E(E) = 2 \\sqrt{\\frac{E}{\\pi}} \\left( \\frac{1}{kT} \\right)^{3/2} \\exp\\left(\\frac{-E}{kT} \\right)</math>\n|cellpadding |border |border colour = #50C878 |background colour = #ECFCF4|ref=9}}\n\nSince the energy is proportional to the sum of the squares of the three normally distributed momentum components, this distribution is a [[gamma distribution]]; in particular, it is a [[chi-squared distribution]] with three degrees of freedom.\n\nBy the [[equipartition theorem]], this energy is evenly distributed among all three degrees of freedom, so that the energy per degree of freedom is distributed as a chi-squared distribution with one degree of freedom:<ref>{{cite book\n|title=Statistical thermodynamics: fundamentals and applications\n|first1=Normand M.\n|last1=Laurendeau\n|publisher=Cambridge University Press\n|year=2005\n|isbn=0-521-84635-8\n|page=434\n\n|url=https://books.google.com/books?id=QF6iMewh4KMC}}, [https://books.google.com/books?id=QF6iMewh4KMC&pg=PA434 Appendix N, page 434]\n</ref>\n:<math>\nf_\\epsilon(\\epsilon)\\,d\\epsilon= \\sqrt{\\frac{1 }{\\pi \\epsilon kT}}~\\exp\\left[\\frac{-\\epsilon}{kT}\\right]\\,d\\epsilon\n</math>\n\nwhere <math>\\epsilon</math> is the energy per degree of freedom. At equilibrium, this distribution will hold true for any number of degrees of freedom. For example, if the particles are rigid mass dipoles of fixed dipole moment, they will have three translational degrees of freedom and two additional rotational degrees of freedom. The energy in each degree of freedom will be described according to the above chi-squared distribution with one degree of freedom, and the total energy will be distributed according to a chi-squared distribution with five degrees of freedom. This has implications in the theory of the [[specific heat]] of a gas.\n\nThe Maxwell–Boltzmann distribution can also be obtained by considering the gas to be a type of [[gas in a box|quantum gas]] for which the approximation ''&epsilon; >> k T'' may be made.\n\n===Distribution for the velocity vector===\n\nRecognizing that the velocity probability density ''f''<sub>'''v'''</sub> is proportional to the momentum probability density function by\n\n:<math>\nf_\\mathbf{v} d^3v = f_\\mathbf{p} \\left(\\frac{dp}{dv}\\right)^3 d^3v\n</math>\n\nand using '''p''' = m'''v''' we get\n\n{{Equation box 1 |indent=: |equation=\n<math>\nf_\\mathbf{v} (v_x, v_y, v_z) =\n\\left(\\frac{m}{2 \\pi kT} \\right)^{3/2}\n\\exp \\left[-\n\\frac{m(v_x^2 + v_y^2 + v_z^2)}{2kT}\n\\right]\n</math>\n|cellpadding |border |border colour = #50C878 |background colour = #ECFCF4}}\n\nwhich is the Maxwell–Boltzmann velocity distribution. The probability of finding a particle with velocity in the infinitesimal element [''dv''<sub>''x''</sub>,&nbsp;''dv''<sub>''y''</sub>,&nbsp;''dv''<sub>''z''</sub>] about velocity '''v'''&nbsp;=&nbsp;[''v''<sub>''x''</sub>,&nbsp;''v''<sub>''y''</sub>,&nbsp;''v''<sub>''z''</sub>] is\n\n:<math>\nf_\\mathbf{v} \\left(v_x, v_y, v_z\\right)\\, dv_x\\, dv_y\\, dv_z.\n</math>\n\nLike the momentum, this distribution is seen to be the product of three independent [[normal distribution|normally distributed]] variables <math>v_x</math>, <math>v_y</math>, and <math>v_z</math>, but with variance <math>\\frac{kT}{m}</math>.  \nIt can also be seen that the Maxwell–Boltzmann velocity distribution for the vector velocity\n[''v''<sub>''x''</sub>,&nbsp;''v''<sub>''y''</sub>,&nbsp;''v''<sub>''z''</sub>] is the product of the distributions for each of the three directions:\n\n:<math>\nf_\\mathbf{v} \\left(v_x, v_y, v_z\\right) = f_v (v_x)f_v (v_y)f_v (v_z)\n</math>\n\nwhere the distribution for a single direction is\n\n:<math>\nf_v (v_i) =\n\\sqrt{\\frac{m}{2 \\pi kT}}\n\\exp \\left[\n\\frac{-mv_i^2}{2kT}\n\\right].\n</math>\n\nEach component of the velocity vector has a [[normal distribution]] with mean <math>\\mu_{v_x} = \\mu_{v_y} = \\mu_{v_z} = 0</math> and standard deviation <math>\\sigma_{v_x} = \\sigma_{v_y} = \\sigma_{v_z} = \\sqrt{\\frac{kT}{m}}</math>, so the vector has a 3-dimensional normal distribution, a particular kind of [[multivariate normal distribution]], with mean <math> \\mu_{\\mathbf{v}} = {\\mathbf{0}} </math> and standard deviation <math>\\sigma_{\\mathbf{v}} = \\sqrt{\\frac{3kT}{m}}</math>.\n\n===Distribution for the speed===\nThe Maxwell–Boltzmann distribution for the speed follows immediately from the distribution of the velocity vector, above.  Note that the speed is\n\n:<math>v = \\sqrt{v_x^2 + v_y^2 + v_z^2}</math>\n\nand the [[volume element]] in [[spherical coordinates]]\n\n:<math> dv_x\\, dv_y\\, dv_z = v^2 \\sin \\theta\\, dv\\, d\\theta\\, d\\phi = v^2   dv\\,  d\\Omega</math> \nwhere <math>\\phi</math> and <math>\\theta</math> are the [[Spherical coordinate system|spherical coordinate]] angles of the velocity vector. [[Spherical coordinate system#Integration and differentiation in spherical coordinates|Integration]] of the probability density function of the velocity  over the solid angles <math>d\\Omega</math> yields an  additional factor of <math>4\\pi</math>.\nThe speed distribution with substitution of the speed for the sum of the squares of the vector components:\n{{Equation box 1 |indent=: |equation=\n<math>\nf (v) =\n\\left(\\frac{2}{\\pi} \\right)^{1/2}\n\\left(\\frac{m}{ kT} \\right)^{3/2}\nv^2\n\\exp \\left[-\n\\frac{mv^2}{2kT}\n\\right].\n</math>\n}}\n\n== In n-dimensional space ==\nIn n-dimensional space, Maxwell-Boltzman distribution becomes:\n\n<math> f(v) ~\\mathrm{d}^nv = \\left(\\frac{m}{2 \\pi kT}\\right)^{n/2}\\,  e^{- \\frac{m|v|^2}{2kT}} ~\\mathrm{d}^nv </math>\n\nSpeed distribution becomes:\n\n<math> f(v) ~\\mathrm{d}v = const. \\times  e^{- \\frac{mv^2}{2kT}} \\times v^{n-1} ~\\mathrm{d}v </math>\n\nThe following integral result is useful:\n\n<math>\\begin{align}\n\\int_{0}^{+\\infty} v^a e^{-\\frac{mv^2}{2kT}} dv  \n&= \\left[\\frac{2kT}{m}\\right]^{(a+1)/2} \\int_{0}^{+\\infty} e^{-x}x^{\\frac{a}{2}}dx^{\\frac{1}{2}}\\\\\n&= \\left[\\frac{2kT}{m}\\right]^{(a+1)/2} \\int_{0}^{+\\infty} e^{-x}x^{\\frac{a}{2}}\\frac{x^{-\\frac{1}{2}}}{2}dx\\\\\n&= \\left[\\frac{2kT}{m}\\right]^{(a+1)/2} \\frac{\\Gamma (\\frac{a+1}{2})}{2}\n\\end{align}</math>\n\nwhere <math> \\Gamma(z)</math>is the [[Gamma function]]. This result can be used to calculate the [[Moment (mathematics)|moments]] of speed distribution function:\n\n<math> \\begin{align}\n\\langle v \\rangle \n&= \\frac{\\int_{0}^{+\\infty} v \\cdot v^{n-1} e^{-\\frac{mv^2}{2kT}} dv}\n{\\int_{0}^{+\\infty} v^{n-1} e^{-\\frac{mv^2}{2kT}} dv} \\\\\n&= \\left[\\frac{2kT}{m}\\right]^{1/2} \\frac{\\Gamma (\\frac{n+1}{2})}{\\Gamma (\\frac{n}{2})}\n\\end{align}</math>\n\nwhich is the [[expectation value|mean]] speed itself <math>v_{\\text{avg}} = \\langle v \\rangle = \\left[\\frac{2kT}{m}\\right]^{1/2} \\frac{\\Gamma (\\frac{n+1}{2})}{\\Gamma (\\frac{n}{2})}</math>.\n\n<math> \\begin{align}\n\\langle v^2 \\rangle \n&= \\frac{\\int_{0}^{+\\infty} v^2 \\cdot v^{n-1} e^{-\\frac{mv^2}{2kT}} dv}\n{\\int_{0}^{+\\infty} v^{n-1} e^{-\\frac{mv^2}{2kT}} dv} \\\\\n&= \\left[\\frac{2kT}{m}\\right] \\frac{\\Gamma (\\frac{n+2}{2})}{\\Gamma (\\frac{n}{2})} \\\\\n&= \\left[\\frac{2kT}{m}\\right] \\frac{n}{2} = \\frac{nkT}{m}\n\\end{align}</math>\n\nwhich gives root-mean-square speed  <math>v_{\\text{rms}} = \\sqrt{\\langle v^2 \\rangle} = \\left[\\frac{nkT}{m}\\right]^{1/2} </math>.\n\nThe derivative of speed distribution function:\n\n<math>\\frac{df(v)}{dv} = const. \\times \\ e^{-\\frac{mv^2}{2kT}} \\left(-\\frac{mv}{kT} v^{n-1}+(n-1)v^{n-2}\\right) = 0 </math>\n\nThis yields the most probable speed ([[Mode (statistics)|mode]]) <math>v_{\\text{p}} = \\left[\\frac{(n-1)kT}{m}\\right]^{1/2}</math>.\n\n==See also==\n* [[Quantum Boltzmann equation]]\n* [[Maxwell–Boltzmann statistics]]\n* [[Maxwell–Jüttner distribution]]\n* [[Boltzmann distribution]] \n* [[Boltzmann factor]]\n* [[Rayleigh distribution]]\n* [[Kinetic theory of gases]]\n\n==References==\n\n{{reflist}}\n\n==Further reading==\n\n* Physics for Scientists and Engineers – with Modern Physics (6th Edition), P. A. Tipler, G. Mosca, Freeman, 2008, {{isbn|0-7167-8964-7}}\n* Thermodynamics, From Concepts to Applications (2nd Edition), A. Shavit, C. Gutfinger, CRC Press (Taylor and Francis Group, USA), 2009, {{isbn|978-1-4200-7368-3}}\n* Chemical Thermodynamics, D.J.G. Ives, University Chemistry, Macdonald Technical and Scientific, 1971, {{isbn|0-356-03736-3}}\n* Elements of Statistical Thermodynamics (2nd Edition), L.K. Nash, Principles of Chemistry, Addison-Wesley, 1974, {{isbn|0-201-05229-6}}\n* Ward, CA & Fang, G 1999, 'Expression for predicting liquid evaporation flux: Statistical rate theory approach', Physical Review E, vol. 59, no. 1, pp.&nbsp;429–40.\n* Rahimi, P & Ward, CA 2005, 'Kinetics of Evaporation: Statistical Rate Theory Approach', International Journal of Thermodynamics, vol. 8, no. 9, pp.&nbsp;1–14.\n\n==External links==\n* [http://demonstrations.wolfram.com/TheMaxwellSpeedDistribution/ \"The Maxwell Speed Distribution\"] from The Wolfram Demonstrations Project at [[Mathworld]]\n\n{{ProbDistributions|continuous-semi-infinite}}\n\n{{DEFAULTSORT:Maxwell-Boltzmann Distribution}}\n[[Category:Continuous distributions]]\n[[Category:Gases]]\n[[Category:James Clerk Maxwell]]\n[[Category:Normal distribution]]\n[[Category:Particle distributions]]"
    },
    {
      "title": "Modified lognormal power-law distribution",
      "url": "https://en.wikipedia.org/wiki/Modified_lognormal_power-law_distribution",
      "text": "{{Multiple issues|\n{{Orphan|date=February 2017}}\n{{cleanup|reason=Formatting of mathematical formulas, particularly inappropriate mixing of wikicode and LaTeX.|date=March 2018}}\n}}\n\nThe '''Modified Lognormal Power-Law''' (MLP)  function is a three parameter function that can be used to model data that have characteristics of a [[log-normal distribution]] and a [[power law]] behavior. It has been used to model the functional form of the [[initial mass function|Initial Mass Function]] (IMF).  Unlike the other functional forms of the IMF, the MLP is a single function with no joining conditions.\n\n==Functional form of the MLP distribution==\n\nIf the random variable W is [[normal distribution|distributed normally]], i.e. W ~ N (μ,σ<sup>2</sup>), then the [[random variable]] M = e<sup>W</sup> will be distributed lognormally:\n\n:<math>\\begin{align}\nf_m(m;\\mu,\\sigma ^2) = \\frac{1}{m \\sqrt{2 \\pi}\\sigma} \\exp\\left(-\\frac{(\\ln(m-\\mu))^2}{2 \\sigma^2}\\right),\\ z > 0\n\n\\end{align}</math>\n\nThe parameters <math>\\begin{align}\\mu _0\\end{align}</math> and  <math>\\begin{align}\\sigma_0\\end{align}</math> follow while determining the initial value of the mass variable, <math>\\begin{align}M_0\\end{align}</math> lognormal distribution of <math>\\begin{align}m\\end{align}</math>. If the growth of this object with <math>\\begin{align}M_0 = m_0\\end{align}</math> is exponential with growth rate <math>\\begin{align}\\gamma \\end{align}</math>, then we can write <math>\\begin{align}\\frac{dm}{dt}= \\gamma m \\end{align}</math>. After time <math>\\begin{align}t\\end{align}</math>, the mean of the lognormal distribution would have changed to <math>\\begin{align}\\mu _0 + \\gamma t\\end{align}</math>. However, considering time as a random variable, we can write <math>\\begin{align}f(t) = \\delta \\exp(-\\delta t)\\end{align}</math>. The closed form of the probability density function of the MLP is as follows:\n\n:<math>\\begin{align}\nf(m)= \\frac{\\alpha}{2} \\exp\\left(\\alpha \\mu _0+ \\frac{\\alpha ^2 \\sigma _0 ^2}{2}\\right) m^{-(1+\\alpha)} \\text{erfc}\\left( \\frac{1}{\\sqrt{2}}\\left(\\alpha \\sigma _0 -\\frac{\\ln(m)- \\mu _0 }{\\sigma_0}\\right)\\right),\\ m \\in [0,\\infty)\n\\end{align}</math>\n\nwhere <math>\\begin{align} \\alpha = \\frac{\\delta}{\\gamma} \\end{align}</math>.\n\n==Mathematical Properties of the MLP distribution==\n\nFollowing are the few mathematical properties of the MLP distribution:\n\n===Cumulative Distribution===\n\nThe MLP [[cumulative distribution function]] (<math>F(m) = \\int_{-\\infty}^m f(t) \\,dt</math>) is given by:\n\n:<math>\\begin{align}\nF(m) = \\frac{1}{2} \\text{erfc}\\left(-\\frac{\\ln(m)-\\mu_0}{\\sqrt{2}\\sigma_0}\\right) - \\frac{1}{2} \\exp\\left(\\alpha \\mu _0 + \\frac{\\alpha ^2 \\sigma ^2 _0}{2}\\right) m^{-\\alpha} \\text{erfc}\\left(\\frac{\\alpha \\sigma _0}{\\sqrt{2}}\\left(\\alpha \\sigma _0 - \\frac{\\ln(m)- \\mu_0}{\\sqrt{2}\\sigma_0}\\right)\\right)\n\\end{align}</math>\n\nWe can see that as <math>m\\to 0,</math> that <math>\\textstyle F(m)\\to \\frac{1}{2} \\operatorname{erfc}\\left(-\\frac{\\ln(m - \\mu_0)}{\\sqrt{2}\\sigma_0}\\right),</math> which is the cumulative distribution function for a lognormal distribution with parameters ''μ''<sub>0</sub> and ''σ''<sub>0</sub>.\n\n===Mean, Variance, Raw Moments ===\n\nThe [[expectation value]] of <math>M</math><sup>k</sup> gives the <math>k</math><sup>th</sup> [[raw moment]] of <math>M</math>,\n\n:<math>\\begin{align}\n\\langle M^k\\rangle = \\int _0 ^{\\infty} m^k f(m) \\mathrm dm\n\\end{align}</math>\n\nThis exists if and only if α > <math>k</math>, in which case it becomes:\n\n:<math>\\begin{align}\n\\langle M^k\\rangle = \\frac{\\alpha}{\\alpha-k} \\exp\\left(\\frac{\\sigma_0 ^2 k^2}{2} + \\mu_0 k\\right),\\ \\alpha > k\n\\end{align}</math>\n\nwhich is the <math>k</math><sup>th</sup> raw moment of the lognormal distribution with the parameters μ<sub>0</sub> and σ<sub>0</sub> scaled by {{frac|α|α-<math>k</math>}} in the limit α→∞. This gives the mean and variance of the MLP distribution:\n\n:<math>\\begin{align}\n\\langle M \\rangle = \\frac{\\alpha}{\\alpha-1} \\exp\\left(\\frac{\\sigma ^2 _0}{2} + \\mu _0\\right),\\  \\alpha > 1\n\\end{align}</math>\n\n:<math>\\begin{align}\n\\langle M^2 \\rangle = \\frac{\\alpha}{\\alpha-2} \\exp\\left(2\\left(\\sigma ^2 _0 + \\mu _0\\right)\\right),\\  \\alpha > 2\n\\end{align}</math>\n\nVar(<math>M</math>) = ⟨<math>M</math><sup>2</sup>⟩-(⟨<math>M</math>⟩)<sup>2</sup> = α exp(σ<sub>0</sub><sup>2</sup> + 2μ<sub>0</sub>) ({{sfrac|exp(σ<sub>0</sub><sup>2</sup>)|α-2}} - {{sfrac|α|(α-2)<sup>2</sup>}}), α > 2\n\n=== Mode ===\n\nThe solution to the equation <math>f'(m)</math> = 0 (equating the slope to zero at the point of maxima) for <math>m</math> gives the mode of the MLP distribution.\n\n:<math>f'(m) = 0  \\Leftrightarrow K \\operatorname{erfc}(u) = \\exp(-u^2),</math>\n\nwhere <math>\\textstyle u = \\frac{1}{\\sqrt{2}} \\left( \\alpha\\sigma_0 - \\frac{\\ln m - \\mu_0}{\\sigma_0} \\right)</math> and <math>K = \\sigma_0(\\alpha+1)\\tfrac{\\sqrt{\\pi}}{2}.</math>\n\nNumerical methods are required to solve this transcendental equation. However, noting that if <math>K</math>≈1 then u = 0 gives us the mode <math>m</math><sup>*</sup>:\n\n:<math>m^* = \\exp (\\mu_0+ \\alpha \\sigma ^2 _0)</math>\n\n=== Random Variate ===\n\nThe lognormal random variate is:\n\n:<math>\\begin{align}\nL(\\mu,\\sigma) = \\exp(\\mu+\\sigma N(0,1))\n\\end{align}</math>\nwhere <math>N(0,1)</math> is standard normal random variate. The exponential random variate is :\n\n:<math>\\begin{align}\nE(\\delta) = - \\delta^{-1} \\ln(R(0,1))\n\\end{align}</math>\nwhere R(0,1) is the uniform random variate in the interval [0,1]. Using these two, we can derive the random variate for the MLP distribution to be:\n\n:<math>\\begin{align}\nM (\\mu_0,\\sigma_0,\\alpha) =  \\exp(\\mu_0 + \\sigma_0 N (0,1) - \\alpha^{-1} \\ln(R(0,1)))\n\\end{align}</math>\n\n== References == \n# {{cite journal|last1=Basu|first1=Shantanu|last2=Gil|first2=M|last3=Auddy|first3=Sayatan|title=The MLP distribution: a modified lognormal power-law model for the stellar initial mass function|journal=MNRAS|date=April 1, 2015|volume=449|issue=3|pages=2413–2420|doi=10.1093/mnras/stv445|url=http://mnras.oxfordjournals.org/content/449/3/2413.full|arxiv=1503.00023|bibcode=2015MNRAS.449.2413B}}\n\n[[Category:Normal distribution]]"
    },
    {
      "title": "Multivariate Behrens–Fisher problem",
      "url": "https://en.wikipedia.org/wiki/Multivariate_Behrens%E2%80%93Fisher_problem",
      "text": "In [[statistics]], the '''multivariate Behrens–Fisher problem''' is the problem of testing for the equality of means from two [[multivariate normal]] distributions when the covariance matrices are unknown and possibly not equal. Since this is a generalization of the univariate [[Behrens-Fisher problem]], it inherits all of the difficulties that arise in the univariate problem.\n\n==Notation and problem formulation==\nLet <math>X_{ij} \\sim \\mathcal{N}_p(\\mu_i,\\, \\Sigma_i) \\ \\ (j=1,\\dots,n_i; \\ \\ i=1,2)\\ </math> be independent random samples from two [[multivariate normal|<math>p</math>-variate normal distributions]] with unknown mean vectors <math>\\mu_i</math> and unknown [[covariance matrix|dispersion matrices]] <math>\\Sigma_i</math>. The index <math>i</math> refers to the first or second population, and the <math>j</math>th observation from the <math>i</math>th population is <math>X_{ij}</math>.\n\nThe multivariate Behrens–Fisher problem is to test the null hypothesis <math>H_0</math> that the means are equal versus the alternative <math>H_1</math> of non-equality:\n: <math> H_0 : \\mu_1 = \\mu_2 \\ \\ \\text{vs} \\ \\ H_1 : \\mu_1 \\neq \\mu_2.\n </math>\n\nDefine some statistics, which are used in the various attempts to solve the multivariate Behrens–Fisher problem, by\n: <math> \n\\begin{align}\n\\bar{X_i} &= \\frac{1}{n_i} \\sum_{j=1}^{n_i} X_{ij}, \\\\\nA_i &= \\sum_{j=1}^{n_i} (X_{ij} - \\bar{X_i})(X_{ij} - \\bar{X_i})', \\\\\n\nS_i &= \\frac{1}{n_i - 1} A_i, \\\\\n\n\\tilde{S_i} &= \\frac{1}{n_i}S_i, \\\\\n\n\\tilde{S} &= \\tilde{S_1} + \\tilde{S_2}, \\quad \\text{and} \\\\\n\nT^2 & = (\\bar{X_1} - \\bar{X_2})'\\tilde{S}^{-1}(\\bar{X_1} - \\bar{X_2}).\n\n\\end{align}\n</math>\n\nThe sample means <math>\\bar{X_i}</math> and sum-of-squares matrices <math>A_i</math> are [[sufficient statistic|sufficient]] for the multivariate normal parameters <math>\\mu_i, \\Sigma_i,\\ (i=1,2)</math>, so it suffices to perform inference be based on just these statistics. The distributions of <math>\\bar{X_i}</math> and <math>A_i</math> are independent and are, respectively, [[multivariate normal distribution|multivariate normal]] and [[wishart distribution|Wishart]]:<ref name=\"2003anderson\"/>\n: <math>\n\\begin{align}\n\\bar{X_i} &\\sim \\mathcal{N}_p \\left(\\mu_i, \\Sigma_i/n_i \\right), \\\\\n\nA_i &\\sim W_p(\\Sigma_i, n_i - 1).\n\\end{align}\n</math>\n\n==Background==\nIn the case where the dispersion matrices are equal, the distribution of the <math>T^2</math> statistic is known to be an [[F distribution]] under the null and a [[noncentral F-distribution]] under the alternative.<ref name=\"2003anderson\"/>\n\nThe main problem is that when the true values of the dispersion matrix are unknown, then under the null hypothesis the probability of rejecting <math>H_0</math> via a [[Hotelling's T-squared distribution#Hotelling.27s two-sample T-squared statistic|<math>T^2</math> test]] depends on the unknown dispersion matrices.<ref name=\"2003anderson\"/> In practice, this dependency harms inference when the dispersion matrices are far from each other or when the sample size is not large enough to estimate them accurately.<ref name=\"2003anderson\"/>\n\nNow, the mean vectors are independently and normally distributed,\n\n: <math> \\bar{X_i} \\sim \\mathcal{N}_p \\left(\\mu_i, \\Sigma_i/n_i \\right), </math>\n\nbut the sum <math> A_1 + A_2 </math> does not follow the Wishart distribution,<ref name=\"2003anderson\"/> which makes inference more difficult.\n\n==Proposed solutions==\nProposed solutions are based on a few main strategies:<ref name=\"1997christensen\"/><ref name=\"2007park\"/>\n* Compute statistics which mimick the <math>T^2</math> statistic and which have an approximate [[f distribution|<math>F</math> distribution]] with estimated degrees of freedom (df).\n* Use [[generalized p-values]] based on [[generalized p-values|generalized test variables]].\n* Use Roy's union-intersection principle <ref name=\"2007park\"/><ref name=\"1981olkin\"/><ref name=\"2004gamage\"/>\n\n===Approaches using the ''T''<sup>2</sup> with approximate degrees of freedom===\nBelow, <math>\\mathrm{tr}</math> indicates the [[Trace (linear algebra)|trace operator]].\n\n====Yao (1965)====\n(as cited by <ref name=\"2004krishnamoorthy\"/>)\n\n: <math>T^2 \\sim \\frac{\\nu p}{\\nu-p+1}F_{p,\\nu-p+1}, </math>\n\nwhere\n\n: <math>\n\\begin{align}\n\\nu &= \n\\left[ \\frac{1}{n_1}\n\\left(\n\\frac{\\bar{X}_d'\\tilde{S}^{-1}\\tilde{S}_1 \\tilde{S}^{-1}\\bar{X_d}} \n     {\\bar{X}_d'\\tilde{S}^{-1}\\bar{X}_d}\n\\right)^2 + \n\\frac{1}{n_2}\n\\left(\n\\frac{\\bar{X}_{d}'\\tilde{S}^{-1}\\tilde{S}_2 \\tilde{S}^{-1}X_d^{-1}}\n     {\\bar{X}_d'\\tilde{S}^{-1} \\bar{X}_d}\n\\right)^{2} \n\\right]^{-1}, \\\\\n\n\\bar{X}_d & = \\bar{X}_{1}-\\bar{X}_2. \n\\end{align}\n</math>\n\n====Johansen (1980)====\n(as cited by <ref name=\"2004krishnamoorthy\"/>)\n\n: <math> T^2 \\sim q F_{p,\\nu}, </math>\n\nwhere\n\n: <math> \n\\begin{align}\nq   &= p + 2D - \\frac{6D}{p(p-1)+2}, \\\\\n\n\\nu &= \\frac{p(p+2)}{3D}, \\\\\n\\end{align}\n</math>\n\nand\n\n: <math>\n\\begin{align}\nD = \\frac{1}{2}\\sum_{i=1}^2 \\frac{1}{n_i} \\Bigg\\{ \\ \n& \\mathrm{tr}\\left[{\\left( I - (\\tilde{S}_1^{-1} + \\tilde{S}_2^{-1})^{-1} \\tilde{S}_i^{-1}\\right)}^2\\right] \\\\\n\n& {}+ {\\left[\\mathrm{tr}\\left(I -(\\tilde{S}_1^{-1} + \\tilde{S}_2^{-1})^{-1} \\tilde{S}_i^{-1}\\right)\\right]}^2 \\ \\Bigg\\}. \\\\\n\\end{align}\n</math>\n\n====Nel and Van der Merwe's (1986)====\n(as cited by <ref name=\"2004krishnamoorthy\"/>)\n\n: <math>\nT^2 \\sim \\frac{\\nu p}{\\nu-p+1}F_{p,\\nu-p+1}, \n</math>\n\nwhere\n\n: <math>\n\\nu = \n\\frac{\n \\mathrm{tr}(\\tilde{S}^2) + [\\mathrm{tr}(\\tilde{S})]^2}\n{\n \\frac{1}{n_1} \\left\\{ \\mathrm{tr}(\\tilde{S_{1}}^2) + [\\mathrm{tr}(\\tilde{S_1})]^2\\right \\} + \n \\frac{1}{n_2} \\left\\{ \\mathrm{tr}(\\tilde{S_2}^2) + [\\mathrm{tr}(\\tilde{S_{2}})]^2 \\right \\}\n}.\n</math>\n\n====Comments on performance====\nKim (1992) proposed a solution that is based on a variant of <math>T^2</math>. Although its power is high, the fact that it is not invariant makes it less attractive. Simulation studies by Subramaniam and Subramaniam (1973) show that the size of Yao's test is closer to the nominal level than that of James's. Christensen and Rencher (1997) performed numerical studies comparing several of these testing procedures and concluded that Kim and Nel and Van der Merwe's tests had the highest power. However, these two procedures are not invariant.\n\n====Krishnamoorthy and Yu (2004)====\nKrishnamoorthy and Yu (2004) proposed a procedure which adjusts in Nel and Var der Merwe (1986)'s approximate df for the denominator of <math>T^2</math> under the null distribution to make it invariant. They show that the approximate degrees of freedom lies in the interval\n<math>\\left[\\min\\{n_1-1,n_2-1\\},n_1+n_2-2\\right]</math>\nto ensure that the degrees of freedom is not negative. They report numerical studies that indicate that their procedure is as powerful as Nel and Van der Merwe's test for smaller dimension, and more powerful for larger dimension. Overall, they claim that their procedure is the better than the invariant procedures of Yao (1965) and Johansen (1980). Therefore, Krishnamoorthy and Yu's (2004) procedure has the best known size and power as of 2004.\n\nThe test statistic <math>T^2</math> in Krishnmoorthy and Yu's procedure follows the distribution\n<math> T^2 \\sim \\nu pF_{p,\\nu-p+1}/(\\nu-p+1), </math> where\n\n: <math> \\nu = \n\\frac{p+p^2}\n{\n\\frac{1}{n_1-1}\\{\\mathrm{tr}[(\\tilde{S}_1 \\tilde{S}^{-1})^2]+[\\mathrm{tr}(\\tilde{S}_1 \\tilde{S}^{-1})]^2\\} + \n\\frac{1}{n_2-1} \\{\\mathrm{tr}[(\\tilde{S}_2 \\tilde{S}^{-1})^2]+[\\mathrm{tr}(\\tilde{S}_2 \\tilde{S}^{-1})]^{2}\\}\n}.\n</math>\n\n==References==\n\n{{Reflist|refs=\n\n<ref name=1997christensen>{{cite journal\n  | last = Christensen | first = W. F.\n  | authorlink =|author2=A.C. Rencher\n  | year = 1997\n  | title = A comparison of type I error rates and power levels for seven solutions to the multivariate Behrens–Fisher problem\n  | journal = Communications in Statist. Simulation and Computation\n  | volume = 26 | pages = 1251–1273\n  | doi=10.1080/03610919708813439\n  }}</ref>\n\n<ref name=2004gamage>{{cite journal\n  | last=Gamage\n  | first=J.\n  |author2=T. Mathew |author3=S. Weerahandi\n   | title=Generalized p-values and generalized confidence regions for the multivariate Behrens--Fisher problem and MANOVA\n  | journal=Journal of Multivariate Analysis\n  | year=2004\n  | volume=88\n  | pages=177–189\n  | doi=10.1016/s0047-259x(03)00065-4\n  }}</ref>\n\n<ref name=2007park>{{cite techreport\n  | last = Park | first = Junyong\n  | authorlink =|author2=Bimal Sinha\n  | title = Some aspects of multivariate Behrens–Fisher problem\n  | year = 2007\n  | url = http://www.math.umbc.edu/~kogan/technical_papers/2007/Park_Sinha.pdf\n  }}</ref>\n\n<ref name=1981olkin>{{cite journal\n  | last = Olkin | first = Ingram\n  | authorlink =|author2=Jack L. Tomsky\n  | title = A New Class of Multivariate Tests Based on the Union-Intersection Principle\n  | journal = Ann. Stat.\n  | volume = 9\n  | number = 4 \n  | year = 1981\n  | pages = 792–802.\n  | doi=10.1214/aos/1176345519\n  }}</ref>\n\n<ref name=2003anderson>{{cite book\n  | last = Anderson\n  | first = T. W.\n  | authorlink = T. W. Anderson\n  | title = An Introduction to Multivariate Statistical Analysis\n  | publisher = [[Wiley Interscience]]\n  | edition = 3rd\n  | location = Hoboken, N. J.\n  | year = 2003\n  | page = 259\n  | isbn = 0-471-36091-0 \n  }}</ref>\n\n<ref name=2004krishnamoorthy>{{cite journal\n | last=Krishnamoorthy\n | first=K.\n |author2=J. Yu\n | title=Modified Nel and Van der Merwe test for the multivariate Behrens-Fisher problem\n | journal=Statistics and Probability Letters\n | year=2004\n | volume=66\n | pages=161–169\n | doi=10.1016/j.spl.2003.10.012\n }}</ref> \n}}\n\n* Rodríguez-Cortés, F. J. and Nagar, D. K. (2007). Percentage points for testing equality of mean vectors. ''Journal of the Nigerian Mathematical Society'', 26:85–95.\n*Gupta, A. K., Nagar, D. K., Mateu, J. and Rodríguez-Cortés, F. J. (2013). Percentage points of a test statistic useful in manova with structured covariance matrices. ''Journal of Applied Statistical Science'', 20:29-41.\n\n{{DEFAULTSORT:Multivariate Behrens-Fisher problem}}\n[[Category:Multivariate continuous distributions]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Multivariate normal distribution",
      "url": "https://en.wikipedia.org/wiki/Multivariate_normal_distribution",
      "text": "{{Redirect|MVN|the airport with that [[International Air Transport Association airport code|IATA code]]|Mount Vernon Airport|the mvn build-automation software|Apache Maven}}\n\n{{Probability distribution\n  | name       = Multivariate normal\n  | type       = multivariate\n  | pdf_image  = [[Image:MultivariateNormal.png|300px]]<br/> <small>Many sample points from a multivariate normal distribution with <math>\\boldsymbol\\mu = \\left[\\begin{smallmatrix}0 \\\\ 0\\end{smallmatrix}\\right]</math> and <math>\\boldsymbol\\Sigma = \\left[\\begin{smallmatrix}1 & 3/5 \\\\ 3/5 & 2\\end{smallmatrix}\\right]</math>, shown along with the 3-sigma ellipse, the two marginal distributions, and the two 1-d histograms.</small>\n  | cdf_image  =\n  | notation   = <math>\\mathcal{N}(\\boldsymbol\\mu,\\,\\boldsymbol\\Sigma)</math>\n  | parameters = '''''μ''''' ∈ '''R'''<sup>''k''</sup> — [[location parameter|location]]<br/>'''Σ''' ∈ '''R'''<sup>''k''&nbsp;×&nbsp;''k''</sup> — [[covariance matrix|covariance]] ([[positive semi-definite matrix]])\n  | support    = '''''x''''' ∈ '''''μ''''' + span('''Σ''') ⊆ '''R'''<sup>''k''</sup>\n  | pdf        = <math>(2\\pi)^{-\\frac{k}{2}}\\det(\\boldsymbol\\Sigma)^{-\\frac{1}{2}} \\, e^{ -\\frac{1}{2}(\\mathbf{x} - \\boldsymbol\\mu)'\\boldsymbol\\Sigma^{-1}(\\mathbf{x} - \\boldsymbol\\mu) },</math><br/>exists only when '''Σ''' is [[positive-definite matrix|positive-definite]]\n  <!--- | cdf        = (no analytic expression) --->\n  | mean       = '''''μ'''''\n  | median     =\n  | mode       = '''''μ'''''\n  | variance   = '''Σ'''\n  | skewness   =\n  | kurtosis   =\n  | entropy    = <math>\\frac{1}{2} \\ln \\det\\left(2 \\pi \\mathrm{e} \\boldsymbol\\Sigma\\right)</math>\n  | mgf        = <math>\\exp\\!\\Big( \\boldsymbol\\mu'\\mathbf{t} + \\tfrac{1}{2} \\mathbf{t}'\\boldsymbol\\Sigma \\mathbf{t}\\Big)</math>\n  | char       = <math>\\exp\\!\\Big( i\\boldsymbol\\mu'\\mathbf{t} - \\tfrac{1}{2} \\mathbf{t}'\\boldsymbol\\Sigma \\mathbf{t}\\Big)</math>\n  | KLDiv      = see below\n  }}\nIn [[probability theory]] and [[statistics]], the '''multivariate normal distribution''', '''multivariate Gaussian distribution''', or '''joint normal distribution''' is a generalization of the one-dimensional ([[univariate]]) [[normal distribution]] to higher [[dimension]]s.  One definition is that a [[random vector]] is said to be ''k''-variate normally distributed if every [[linear combination]] of its ''k'' components has a univariate normal distribution. Its importance derives mainly from the [[Central limit theorem#Multidimensional CLT|multivariate central limit theorem]]. The multivariate normal distribution is often used to describe, at least approximately, any set of (possibly) [[Correlation (statistics)|correlated]] real-valued [[random variable]]s each of which clusters around a mean value.\n\n== Notation and parametrization ==\nThe multivariate normal distribution of a ''k''-dimensional random vector <math>\\mathbf{X} = (X_1,\\ldots,X_k)^T</math> can be written in the following notation:\n: <math>\n    \\mathbf{X}\\ \\sim\\ \\mathcal{N}(\\boldsymbol\\mu,\\, \\boldsymbol\\Sigma),\n  </math>\nor to make it explicitly known that ''X'' is ''k''-dimensional,\n: <math>\n    \\mathbf{X}\\ \\sim\\ \\mathcal{N}_k(\\boldsymbol\\mu,\\, \\boldsymbol\\Sigma),\n  </math>\nwith ''k''-dimensional [[mean vector]]\n:<math> \\boldsymbol\\mu = \\operatorname{E}[\\mathbf{X}] = [ \\operatorname{E}[X_1], \\operatorname{E}[X_2], \\ldots, \\operatorname{E}[X_k]]^{\\rm T}, </math>\nand <math>k \\times k</math> [[covariance matrix]]\n:<math> \\Sigma_{i,j} = :\\operatorname{E} [(X_i - \\mu_i)( X_j - \\mu_j)] = \\operatorname{Cov}[X_i, X_j] </math>\n\n:<math> \\boldsymbol\\Sigma = :\\operatorname{E} [(\\mathbf{X} - \\boldsymbol \\mu)( \\mathbf{X} - \\boldsymbol \\mu)^{\\rm T}] =  [ \\operatorname{Cov}[X_i, X_j]; 1 \\le i,j \\le k ].</math>\nThe [[Matrix inverse|inverse]] of the covariance matrix is called the [[Precision (statistics)|precision]] matrix, denoted by <math>\\boldsymbol{Q}=\\boldsymbol\\Sigma^{-1}</math>.\n\n== Definitions ==\n===Standard normal random vector===\nA real [[random vector]] <math>\\mathbf{X} = (X_1,\\ldots,X_k)^{\\mathrm T}</math> is called a '''standard normal random vector''' if all of its components <math>X_n</math> are independent and each is a zero-mean unit-variance normally distributed random variable, i.e. if <math>X_n \\sim\\ \\mathcal{N}(0,1)</math> for all <math>n</math>.<ref name=Lapidoth>{{cite book |first=Amos |last=Lapidoth |year=2009 |title=A Foundation in Digital Communication |publisher=Cambridge University Press |isbn=978-0-521-19395-5}}</ref>{{rp|p. 454}}\n\n===Centered normal random vector===\nA real random vector <math>\\mathbf{X} = (X_1,\\ldots,X_k)^{\\mathrm T}</math> is called a '''centered normal random vector''' if there exists a deterministic <math>k \\times \\ell</math> matrix <math>\\boldsymbol{A}</math> such that <math>\\boldsymbol{A} \\mathbf{Z}</math> has the same distribution as <math>\\mathbf{X}</math> where <math>\\mathbf{Z}</math> is a standard normal random vector with <math>\\ell</math> components.<ref name=Lapidoth/>{{rp|p. 454}}\n\n===Normal random vector===\nA real random vector <math>\\mathbf{X} = (X_1,\\ldots,X_k)^{\\mathrm T}</math> is called a '''normal random vector''' if there exists a random <math>\\ell</math>-vector <math>\\mathbf{Z}</math>, which is a standard normal random vector, a <math>k</math>-vector <math>\\mathbf{\\mu}</math>, and a <math>k \\times \\ell</math> matrix <math>\\boldsymbol{A}</math>, such that <math>\\mathbf{X}=\\boldsymbol{A} \\mathbf{Z} + \\mathbf{\\mu}</math>.<ref name=Gut>{{cite book |first=Allan |last=Gut |year=2009 |title=An Intermediate Course in Probability |publisher=Springer |isbn=978-1-441-90161-3}}</ref>{{rp|p. 454}}<ref name=Lapidoth/>{{rp|p. 455}}\n\nFormally:\n\n{{Equation box 1\n|indent =\n|title=\n|equation = \n<math>\n\\mathbf{X}\\ \\sim\\ \\mathcal{N}(\\mathbf{\\mu}, \\boldsymbol\\Sigma) \\quad \\iff \\quad \\text{there exist } \\mathbf{\\mu}\\in\\mathbb{R}^k,\\boldsymbol{A}\\in\\mathbb{R}^{k\\times \\ell} \\text{ such that } \\mathbf{X}=\\boldsymbol{A} \\mathbf{Z} + \\mathbf{\\mu} \\text{ for } Z_n \\sim\\ \\mathcal{N}(0, 1), \\text{i.i.d.}\n</math>\n|cellpadding= 6\n|border\n|border colour = #0073CF\n|background colour=#F5FFFA}}\n\nHere the [[covariance matrix]] is <math>\\boldsymbol\\Sigma = \\boldsymbol{A} \\boldsymbol{A}^{\\mathrm T}</math>.\n\nIn the [[Degeneracy (mathematics)|degenerate]] case where the covariance matrix is [[singular matrix|singular]], the corresponding distribution has no density; see the [[#Degenerate_case|section below]] for details.  This case arises frequently in [[statistics]]; for example, in the distribution of the vector of [[errors and residuals in statistics|residuals]] in the [[ordinary least squares]] regression.  Note also that the <math>X_i</math> are in general ''not'' independent; they can be seen as the result of applying the matrix <math>\\boldsymbol{A}</math> to a collection of independent Gaussian variables <math>\\mathbf{Z}</math>.\n\n=== Equivalent definitions ===\nThe following definitions are equivalent to the definition given above. A random vector <math>\\mathbf{X} = (X_1,\\ldots,X_k)^T</math> has a multivariate normal distribution if it satisfies one of the following equivalent conditions.\n*Every linear combination <math>Y=a_1 X_1 + \\cdots + a_k X_k</math> of its components is [[normal distribution|normally distributed]]. That is, for any constant vector <math>\\mathbf{a} \\in \\mathbb{R}^k</math>, the random variable <math>Y=\\mathbf{a}^{\\mathrm T}\\mathbf{X}</math> has a univariate normal distribution, where a univariate normal distribution with zero variance is a point mass on its mean.\n*There is a ''k''-vector <math>\\mathbf{\\mu}</math> and a symmetric, [[Positive-definite matrix#Negative-definite.2C semidefinite and indefinite matrices|positive semidefinite]] <math>k \\times k</math> matrix <math>\\boldsymbol\\Sigma</math>, such that the [[Characteristic function (probability theory)|characteristic function]] of <math>\\mathbf{X}</math> is\n:: <math>\n    \\varphi_\\mathbf{X}(\\mathbf{u}) = \\exp\\Big( i\\mathbf{u}^T\\boldsymbol\\mu - \\tfrac{1}{2} \\mathbf{u}^T\\boldsymbol\\Sigma \\mathbf{u} \\Big).\n  </math>\n\nThe spherical normal distribution can be characterised as the unique distribution where components are independent in any orthogonal coordinate system.<ref>{{cite journal |last1=Kac |first1=M. |title=On a characterization of the normal distribution |journal=American Journal of Mathematics |date=1939 |volume=61 |issue=3 |pages=726–728 |jstor=2371328 |doi=10.2307/2371328 }}</ref><ref>{{cite journal |last1=Sinz |first1=Fabian |last2=Gerwinn |first2=Sebastian |last3=Bethge |first3=Matthias |title=Characterization of the p-generalized normal distribution |journal=Journal of Multivariate Analysis |date=2009 |volume=100 |issue=5 |pages=817–820 |url=https://www.sciencedirect.com/science/article/pii/S0047259X08001681|doi=10.1016/j.jmva.2008.07.006 }}</ref>\n\n== Properties ==\n===Density function===\n[[File:Multivariate Gaussian.png|thumb|right|300px|Bivariate normal [[Joint probability distribution#Density function or mass function|joint density]]]]\n\n====Non-degenerate case====\nThe multivariate normal distribution is said to be \"non-degenerate\" when the symmetric [[covariance matrix]] <math>\\boldsymbol\\Sigma</math> is [[Positive-definite matrix|positive definite]]. In this case the distribution has [[probability density function|density]]<ref>[http://www.math.uiuc.edu/~r-ash/Stat/StatLec21-25.pdf UIUC, Lecture 21. ''The Multivariate Normal Distribution''], 21.5:\"Finding the Density\".</ref>\n\n{{Equation box 1\n|indent =\n|title=\n|equation = \n<math>\nf_{\\mathbf X}(x_1,\\ldots,x_k) = \\frac{\\exp\\left(-\\frac 1 2 ({\\mathbf x}-{\\boldsymbol\\mu})^\\mathrm{T}{\\boldsymbol\\Sigma}^{-1}({\\mathbf x}-{\\boldsymbol\\mu})\\right)}{\\sqrt{(2\\pi)^k|\\boldsymbol\\Sigma|}}\n</math>\n|cellpadding= 6\n|border\n|border colour = #0073CF\n|background colour=#F5FFFA}}\n\nwhere <math>{\\mathbf x}</math> is a real ''k''-dimensional column vector and <math>|\\boldsymbol\\Sigma|\\equiv \\det\\boldsymbol\\Sigma</math> is the [[determinant]] of <math>\\boldsymbol\\Sigma</math>. The equation above reduces to that of the univariate normal distribution if <math>\\boldsymbol\\Sigma</math> is a <math>1 \\times 1</math> matrix (i.e. a single real number).\n\nThe circularly symmetric version of the [[complex normal distribution]] has a slightly different form.\n\nEach iso-density locus&mdash;the locus of points in ''k''-dimensional space each of which gives the same particular value of the density&mdash;is an [[ellipse]] or its higher-dimensional generalization; hence the multivariate normal is a special case of the [[elliptical distribution]]s.\n\nThe descriptive statistic <math>\\sqrt{({\\mathbf x}-{\\boldsymbol\\mu})^\\mathrm{T}{\\boldsymbol\\Sigma}^{-1}({\\mathbf x}-{\\boldsymbol\\mu})}</math> is known as the [[Mahalanobis distance]], which represents the distance of the test point <math>{\\mathbf x}</math> from the mean <math>{\\boldsymbol\\mu}</math>. Note that in the case when <math>k = 1</math>, the distribution reduces to a univariate normal distribution and the Mahalanobis distance reduces to the absolute value of the [[standard score]]. See also [[#Interval|Interval]] below.\n\n====Bivariate case====\nIn the 2-dimensional nonsingular case ({{nowrap|1=''k'' = rank(Σ) = 2}}), the [[probability density function]] of a vector {{nowrap|[''X'' ''Y'']′}} is:\n\n: <math>\n    f(x,y) =\n      \\frac{1}{2 \\pi  \\sigma_X \\sigma_Y \\sqrt{1-\\rho^2}}\n      \\exp\\left(\n        -\\frac{1}{2(1-\\rho^2)}\\left[\n          \\frac{(x-\\mu_X)^2}{\\sigma_X^2} +\n          \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} -\n          \\frac{2\\rho(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X \\sigma_Y}\n        \\right]\n      \\right)\n</math>\n\nwhere ''ρ'' is the [[Pearson product-moment correlation coefficient|correlation]] between ''X'' and ''Y'' and\nwhere <math> \\sigma_X>0 </math> and <math> \\sigma_Y>0 </math>. In this case,\n: <math>\n    \\boldsymbol\\mu = \\begin{pmatrix} \\mu_X \\\\ \\mu_Y \\end{pmatrix}, \\quad\n    \\boldsymbol\\Sigma = \\begin{pmatrix} \\sigma_X^2 & \\rho \\sigma_X \\sigma_Y \\\\\n                             \\rho \\sigma_X \\sigma_Y  & \\sigma_Y^2 \\end{pmatrix}.\n  </math>\nIn the bivariate case, the first equivalent condition for multivariate normality can be made less restrictive: it is sufficient to verify that [[countably infinite|countably many]] distinct linear combinations of X and Y are normal in order to conclude that the vector {{nowrap|[X Y]′}} is bivariate normal.<ref name=HT/>\n\nThe bivariate iso-density loci plotted in the ''x,y''-plane are ellipses. As the absolute value of the correlation parameter ''ρ'' increases, these loci are squeezed toward the following line :\n\n: <math>\n    y(x) = \\sgn (\\rho)\\frac{\\sigma_Y}{\\sigma_X} (x - \\mu _X) + \\mu_Y.\n  </math>\n\nThis is because this expression, with sgn(''ρ'') (where sgn is the [[Sign function]]) replaced by ''ρ'', is the [[best linear unbiased prediction]] of ''Y'' given a value of ''X''.<ref name=wyattlms/>\n\n====Degenerate case====\nIf the covariance matrix <math>\\boldsymbol\\Sigma</math> is not full rank, then the multivariate normal distribution is degenerate and does not have a density. More precisely, it does not have a density with respect to ''k''-dimensional [[Lebesgue measure]] (which is the usual measure assumed in calculus-level probability courses). Only random vectors whose distributions are [[absolute continuity#Absolute continuity of measures|absolutely continuous]] with respect to a measure are said to have densities (with respect to that measure). To talk about densities but avoid dealing with measure-theoretic complications it can be simpler to restrict attention to a subset of <math>\\operatorname{rank}(\\boldsymbol\\Sigma)</math> of the coordinates of <math>\\mathbf{x}</math> such that the covariance matrix for this subset is positive definite; then the other coordinates may be thought of as an [[affine function]] of the selected coordinates.{{citation needed|date=July 2012}}\n\nTo talk about densities meaningfully in the singular case, then, we must select a different base measure. Using the [[disintegration theorem]] we can define a restriction of Lebesgue measure to the <math>\\operatorname{rank}(\\boldsymbol\\Sigma)</math>-dimensional affine subspace of <math>\\mathbb{R}^k</math> where the Gaussian distribution is supported, i.e. <math>\\{\\boldsymbol\\mu+\\boldsymbol{\\Sigma^{1/2}}\\mathbf{v} : \\mathbf{v} \\in \\mathbb{R}^k \\}</math>. With respect to this measure the distribution has density:\n\n:<math>f(\\mathbf{x})= \\left(\\det\\nolimits^*(2\\pi\\boldsymbol\\Sigma)\\right)^{-\\frac{1}{2}}\\, e^{ -\\frac{1}{2}(\\mathbf{x}-\\boldsymbol\\mu)'\\boldsymbol\\Sigma^+(\\mathbf{x}-\\boldsymbol\\mu) }</math>\n\nwhere <math>\\boldsymbol\\Sigma^+</math> is the [[generalized inverse]] and det* is the [[pseudo-determinant]].<ref name=rao/>\n\n===Higher moments===\n{{Main|Isserlis’ theorem}}\nThe ''k''th-order [[moment (mathematics)|moments]] of '''x''' are given by\n\n:<math>\n\\mu_{1,\\ldots,N}(\\mathbf{x})\\ \\stackrel{\\mathrm{def}}{=}\\  \\mu _{r_1,\\ldots,r_N}(\\mathbf{x})\\ \\stackrel{\\mathrm{def}}{=} \\operatorname E\\left[ \\prod_{j=1}^N X_j^{r_j} \\right]\n</math>\n\nwhere {{math|''r''<sub>1</sub> + ''r''<sub>2</sub> + ⋯ + ''r<sub>N</sub>'' {{=}} ''k''.}}\n\nThe ''k''th-order central moments are as follows\n\n{{ordered list|list_style_type=lower-alpha\n|If ''k'' is odd, {{math|''μ''<sub>1, …, ''N''</sub>('''x''' − '''''μ''''') {{=}} 0}}.\n|If ''k'' is even with {{math|''k'' {{=}} 2''λ''}}, then\n}}\n:<math>\n\\mu_{1,\\dots,2\\lambda}(\\mathbf{x}-\\boldsymbol\\mu )=\\sum \\left( \\sigma_{ij} \\sigma_{k\\ell} \\cdots \\sigma_{XZ}\\right)\n</math>\n\nwhere the sum is taken over all allocations of the set <math>\\left\\{ 1,\\ldots,2\\lambda \\right\\}</math> into ''λ'' (unordered) pairs. That is, for a ''k''th {{math| ({{=}} 2''λ'' {{=}} 6)}} central moment, one sums the products of {{nowrap|''λ'' {{=}} 3}} covariances (the expected value '''''μ''''' is taken to be 0 in the interests of parsimony):\n\n: <math>\\begin{align}\n& \\operatorname E[X_1 X_2 X_3 X_4 X_5 X_6] \\\\[8pt]\n= {} & \\operatorname E[X_1 X_2]\\operatorname E[X_3 X_4]\\operatorname E[X_5 X_6] + \\operatorname E[X_1 X_2]\\operatorname E[X_3 X_5]\\operatorname E[X_4 X_6] + \\operatorname E[X_1 X_2]\\operatorname E[X_3 X_6] \\operatorname E[X_4 X_5] \\\\[4pt]\n& {} + \\operatorname E[X_1 X_3]\\operatorname [X_2 X_4]\\operatorname E[X_5 X_6] + \\operatorname E[X_1 X_3]\\operatorname E[X_2 X_5]\\operatorname E[X_4 X_6] + \\operatorname E[X_1 X_3]\\operatorname E[X_2 X_6] \\operatorname E[X_4 X_5] \\\\[4pt]\n& {} +  \\operatorname E[X_1 X_4]\\operatorname E[X_2 X_3]\\operatorname E[X_5 X_6] + \\operatorname E[X_1 X_4]\\operatorname E[X_2 X_5]\\operatorname E[X_3 X_6]+ \\operatorname E[X_1 X_4]\\operatorname E[X_2 X_6] \\operatorname E[X_3 X_5] \\\\[4pt]\n& {} + \\operatorname E[X_1 X_5]\\operatorname E[X_2 X_3]\\operatorname E[X_4 X_6] + \\operatorname E[X_1 X_5]\\operatorname E[X_2 X_4]\\operatorname E[X_3 X_6] + \\operatorname E[X_1 X_5]\\operatorname E[X_2 X_6] \\operatorname E[X_3 X_4] \\\\[4pt]\n& {} + \\operatorname E[X_1 X_6]\\operatorname E[X_2 X_3]\\operatorname E[X_4 X_5] + \\operatorname E[X_1 X_6]\\operatorname E[X_2 X_4]\\operatorname E[X_3 X_5] + \\operatorname E[X_1 X_6] \\operatorname E[X_2 X_5]\\operatorname E[X_3 X_4].\n\\end{align}\n</math>\n\nThis yields <math>\\tfrac{(2\\lambda -1)!}{2^{\\lambda -1}(\\lambda -1)!}</math> terms in the sum (15 in the above case), each being the product of ''λ'' (in this case 3) covariances. For fourth order moments (four variables) there are three terms. For sixth-order moments there are {{math|3 × 5 {{=}} 15}} terms, and for eighth-order moments there are {{math|3 × 5 × 7 {{=}} 105}} terms.\n\nThe covariances are then determined by replacing the terms of the list <math>[ 1, \\ldots, 2\\lambda]</math> by the corresponding terms of  the list consisting of ''r''<sub>1</sub> ones, then ''r''<sub>2</sub> twos, etc.. To illustrate this, examine the following 4th-order central moment case:\n\n: <math>\n\\begin{align}\n\\operatorname E \\left [ X_i^4 \\right ] & = 3\\sigma_{ii}^2 \\\\[4pt]\n\\operatorname E \\left[ X_i^3 X_j \\right  ] & = 3\\sigma_{ii} \\sigma_{ij} \\\\[4pt]\n\\operatorname E \\left[ X_i^2 X_j^2 \\right  ] & = \\sigma_{ii}\\sigma_{jj}+2 \\sigma _{ij}^2 \\\\[4pt]\n\\operatorname E \\left[ X_i^2 X_j X_k \\right  ] & = \\sigma_{ii}\\sigma _{jk}+2\\sigma _{ij}\\sigma_{ik} \\\\[4pt]\n\\operatorname E \\left [ X_i X_j X_k X_n \\right  ] & = \\sigma_{ij}\\sigma _{kn} + \\sigma _{ik} \\sigma_{jn} + \\sigma_{in} \\sigma _{jk}.\n\\end{align}\n</math>\n\nwhere <math>\\sigma_{ij}</math> is the covariance of ''X<sub>i</sub>'' and ''X<sub>j</sub>''. With the above method one first finds the general case for a ''k''th moment with ''k'' different ''X'' variables, <math>E\\left[ X_i X_j X_k X_n\\right]</math>, and then one simplifies this accordingly. For example, for <math>\\operatorname E[ X_i^2 X_k X_n ]</math>, one lets {{math|''X<sub>i</sub>'' {{=}} ''X''<sub>''j''</sub>}} and one uses the fact that <math>\\sigma_{ii} = \\sigma_i^2</math>.\n\n===Likelihood function===\n\nIf the mean and variance matrix are known, a suitable log likelihood function for a single observation '''x'''  is\n\n:<math>\\ln L = -\\frac{1}{2} \\left[ \\ln (|\\boldsymbol\\Sigma|\\,) + (\\mathbf{x}-\\boldsymbol\\mu)^{\\rm T}\\boldsymbol\\Sigma^{-1}(\\mathbf{x}-\\boldsymbol\\mu) + k\\ln(2\\pi) \\right]</math>,\n\nwhere ''x'' is a vector of real numbers (to derive this, simply take the log of the PDF).   The circularly symmetric version of the complex case, where ''z'' is a vector of complex numbers, would be\n\n:<math>\\ln L  = -\\ln (|\\boldsymbol\\Sigma|\\,) -(\\mathbf{z}-\\boldsymbol\\mu)^\\dagger\\boldsymbol\\Sigma^{-1}(\\mathbf{z}-\\boldsymbol\\mu) -k\\ln(\\pi)</math>\n\ni.e. with the [[conjugate transpose]] (indicated by <math>\\dagger</math>) replacing the normal [[transpose]] (indicated by <math>{}^{\\rm T}</math>).  This is slightly different than in the real case, because the circularly symmetric version of the [[complex normal distribution]] has a slightly different form.\n\nA similar notation is used for [[multiple linear regression]].<ref>Tong, T. (2010) [http://amath.colorado.edu/courses/7400/2010Spr/lecture9.pdf Multiple Linear Regression : MLE and Its Distributional Results] {{webarchive|url=https://www.webcitation.org/6HPbX5thy?url=http://amath.colorado.edu/courses/7400/2010Spr/lecture9.pdf |date=2013-06-16 }}, Lecture Notes</ref>\n\n===Differential entropy===\n\nThe [[differential entropy]] of the multivariate normal distribution is<ref>{{cite journal\n | last1 = Gokhale | first1 = DV | authorlink1=\n | last2 = Ahmed | first2 = NA\n | last3 = Res |first3=BC\n | last4 = Piscataway |first4=NJ\n | date = May 1989\n | title = Entropy Expressions and Their Estimators for Multivariate Distributions\n | journal = IEEE Transactions on Information Theory\n | volume = 35  | issue = 3  | pages = 688–692\n | doi =10.1109/18.30996\n}}</ref>\n\n:<math>\n\\begin{align}\nh\\left(f\\right) & = -\\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty \\cdots\\int_{-\\infty}^\\infty f(\\mathbf{x}) \\ln f(\\mathbf{x})\\,d\\mathbf{x},\\\\\n& = \\frac12 \\ln\\left(\\left|\\left(2\\pi e\\right)\\boldsymbol\\Sigma \\right|\\right) = \\frac12 \\ln\\left(\\left(2\\pi e\\right)^k \\left|\\boldsymbol\\Sigma \\right|\\right) = \\frac{k}{2} \\ln\\left(2\\pi e\\right) + \\frac{1}{2} \\ln\\left(\\left|\\boldsymbol\\Sigma \\right|\\right) = \\frac{k}{2} + \\frac{k}{2} \\ln\\left(2\\pi \\right) + \\frac{1}{2} \\ln\\left(\\left|\\boldsymbol\\Sigma \\right|\\right)\\\\\n\\end{align}\n</math>\nwhere the bars denote the [[determinant|matrix determinant]] and {{math|''k''}} is the dimensionality of the vector space.\n\n===Kullback–Leibler divergence===\nThe [[Kullback–Leibler divergence]] from <math>\\mathcal{N}_0(\\boldsymbol\\mu_0, \\boldsymbol\\Sigma_0)</math> to <math>\\mathcal{N}_1(\\boldsymbol\\mu_1, \\boldsymbol\\Sigma_1)</math>, for non-singular matrices Σ<sub>0</sub> and Σ<sub>1</sub>, is:<ref>J. Duchi, Derivations for Linear Algebra and Optimization [http://stanford.edu/~jduchi/projects/general_notes.pdf]. pp. 13</ref>\n\n:<math>\nD_\\text{KL}(\\mathcal{N}_0 \\| \\mathcal{N}_1) = { 1 \\over 2 } \\left\\{ \\operatorname{tr} \\left( \\boldsymbol\\Sigma_1^{-1} \\boldsymbol\\Sigma_0 \\right) + \\left( \\boldsymbol\\mu_1 - \\boldsymbol\\mu_0\\right)^{\\rm T} \\boldsymbol\\Sigma_1^{-1} ( \\boldsymbol\\mu_1 - \\boldsymbol\\mu_0 ) - k +\\ln { |  \\boldsymbol \\Sigma_1 | \\over | \\boldsymbol\\Sigma_0 | } \\right\\},\n</math>\nwhere <math>k</math> is the dimension of the vector space.\n\nThe [[logarithm]] must be taken to base ''[[e (mathematical constant)|e]]'' since the two terms following the logarithm are themselves base-''e'' logarithms of expressions that are either factors of the density function or otherwise arise naturally.  The equation therefore gives a result measured in [[nat (unit)|nat]]s.  Dividing the entire expression above by log<sub>''e''</sub>&nbsp;2 yields the divergence in [[bit]]s.\n\nWhen <math>\\boldsymbol\\mu_1 = \\boldsymbol\\mu_0</math>,\n\n:<math>\nD_\\text{KL}(\\mathcal{CN}_0 \\| \\mathcal{CN}_1) = \\operatorname{tr} \\left( \\boldsymbol\\Sigma_1^{-1} \\boldsymbol\\Sigma_0 \\right) - k +\\ln { |  \\boldsymbol \\Sigma_1 | \\over | \\boldsymbol\\Sigma_0 | }.\n</math>\n\n===Mutual information===\nThe [[mutual information]] of a distribution is a special case of the [[Kullback–Leibler divergence]] in which <math>P</math> is the full multivariate distribution and <math>Q</math> is the product of the 1-dimensional marginal distributions. In the notation of the [[#Kullback–Leibler divergence|Kullback–Leibler divergence section]] of this article, <math>\\boldsymbol\\Sigma_1</math> is a [[diagonal matrix]] with the diagonal entries of <math>\\boldsymbol\\Sigma_0</math>, and <math>\\boldsymbol\\mu_1 = \\boldsymbol\\mu_0</math>. The resulting formula for mutual information is:\n\n:<math>\nI(\\boldsymbol{X}) = - { 1 \\over 2 } \\ln  |  \\boldsymbol \\rho_0 |,\n</math>\n\nwhere <math>\\boldsymbol \\rho_0</math> is the [[Covariance matrix#Correlation matrix|correlation matrix]] constructed from <math>\\boldsymbol \\Sigma_0</math>.\n\nIn the bivariate case the expression for the mutual information is:\n\n:<math>\nI(x;y) = - { 1 \\over 2 } \\ln (1 - \\rho^2).\n</math>\n\n=== Cumulative distribution function ===\nThe notion of [[cumulative distribution function]] (cdf) in dimension 1 can be extended in two ways to the multidimensional case, based on rectangular and ellipsoidal regions.\n\nThe first way is to define the cdf <math>F(\\mathbf{x})</math> of a random vector <math>\\mathbf{X}</math> as the probability that all components of <math>\\mathbf{X}</math> are less than or equal to the corresponding values in the vector <math>\\mathbf{x}</math>:<ref name=\"bo16\">{{cite journal|last1=Botev|first1=Z. I.|title=The normal law under linear restrictions: simulation and estimation via minimax tilting|journal=Journal of the Royal Statistical Society, Series B|volume=79|pages=125–148|date=2016|doi=10.1111/rssb.12162|arxiv=1603.04166|bibcode=2016arXiv160304166B}}</ref>\n\n:<math> F(\\mathbf{x}) = \\mathbb{P}(\\mathbf{X}\\leq \\mathbf{x}), \\quad \\text{where } \\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol\\mu,\\, \\boldsymbol\\Sigma).</math>\n\nThough there is no closed form for <math>F(\\mathbf{x})</math>, there are a number of algorithms that  [https://cran.r-project.org/web/packages/TruncatedNormal/ estimate it numerically].<ref name=\"bo16\"/><ref name=Genz>{{cite book|last=Genz|first=Alan|title=Computation of Multivariate Normal and t Probabilities|date=2009|publisher=Springer|isbn=978-3-642-01689-9|url=https://www.springer.com/statistics/computational+statistics/book/978-3-642-01688-2}}</ref>\n\nAnother way is to define the cdf <math>F(r)</math> as the probability that a sample lies inside the ellipsoid determined by its [[Mahalanobis distance]] <math>r</math> from the Gaussian, a direct generalization of the standard deviation\n.<ref name=Bensimhoun>[https://upload.wikimedia.org/wikipedia/commons/a/a2/Cumulative_function_n_dimensional_Gaussians_12.2013.pdf Bensimhoun Michael, ''N-Dimensional Cumulative Function, And Other Useful Facts About Gaussians and Normal Densities'' (2006)]</ref>\nIn order to compute the values of this function, closed analytic formulae exist,<ref name=\"Bensimhoun\"/> as follows.\n\n====Interval====\n{{further|Confidence region}}\n\nThe [[interval estimation|interval]] for the multivariate normal distribution yields a region consisting of those vectors '''x''' satisfying\n\n:<math>({\\mathbf x}-{\\boldsymbol\\mu})^T{\\boldsymbol\\Sigma}^{-1}({\\mathbf x}-{\\boldsymbol\\mu}) \\leq \\chi^2_k(p).</math>\n\nHere <math>{\\mathbf x}</math> is a <math>k</math>-dimensional vector, <math>{\\boldsymbol\\mu}</math> is the known <math>k</math>-dimensional mean vector, <math>\\boldsymbol\\Sigma</math> is the known [[covariance matrix]] and <math>\\chi^2_k(p)</math> is the  [[quantile function]] for probability <math>p</math> of the [[chi-squared distribution]] with <math>k</math> degrees of freedom.<ref name=Siotani/>\nWhen <math>k = 2,</math> the expression defines the interior of an ellipse and the chi-squared distribution simplifies to an [[exponential distribution]] with mean equal to two.\n\n=== Complementary cumulative distribution function (tail distribution) ===\nThe  [[cumulative distribution function#Derived functions|complementary cumulative distribution function]] (ccdf) or the '''tail distribution'''  \nis defined as <math>\\overline F(\\mathbf{x})=1-\\mathbb P(\\mathbf X\\leq \\mathbf x)</math>. \nWhen <math> \\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol\\mu,\\, \\boldsymbol\\Sigma)</math>, then\nthe ccdf can be written as a probability the  maximum of dependent Gaussian variables:<ref name=\"bmr15\">{{cite conference |url=https://ieeexplore.ieee.org/document/7408202/ \n|title=Tail distribution of the maximum of correlated Gaussian random variables |last1=Botev |first1=Z. I.  \n|last2=Mandjes |first2=M. |last3=Ridder |first3=A.  |date=2015 |publisher=IEEE|ISBN=978-1-4673-9743-8\n |book-title= 2015 Winter Simulation Conference (WSC) |pages=633–642 |location=  6th–9th Dec 2015 Huntington Beach, CA, USA \n|doi= 10.1109/WSC.2015.7408202 }}\n</ref>\n\n:<math> \\overline F(\\mathbf{x}) =\\mathbb P(\\cup_i\\{X_i\\geq x_i\\})= \\mathbb P(\\max_i Y_i\\geq 0), \n\\quad \\text{where } \\mathbf{Y} \\sim \\mathcal{N}(\\boldsymbol\\mu-\\mathbf{x},\\, \\boldsymbol\\Sigma).</math>\nWhile no simple closed formula exists for computing the ccdf, the maximum of dependent Gaussian variables can \nbe estimated accurately via the [[Monte Carlo method]].<ref name=\"bmr15\"/><ref name=\"abl08\">{{cite conference |url=https://ieeexplore.ieee.org/document/4736085/\n|title=Efficient simulation for tail probabilities of Gaussian random fields |last1=Adler |first1=R. J.  \n|last2=Blanchet |first2=J. |last3=Liu |first3=J.  |date=2008 |publisher=IEEE|ISBN=978-1-4244-2707-9\n |book-title= 2008 Winter Simulation Conference (WSC) |pages=328–336 |location=  7th–10th Dec 2008 Miami, FL, USA, USA \n|doi= 10.1109/WSC.2008.473608 }}\n</ref>\n\n==Joint normality==\n\n===Normally distributed and independent===\n\nIf <math>X</math> and <math>Y</math> are normally distributed and [[statistical independence|independent]], this implies they are \"jointly normally distributed\", i.e., the pair <math>(X,Y)</math> must have multivariate normal distribution.  However, a pair of jointly normally distributed variables need not be independent (would only be so if uncorrelated, <math> \\rho = 0</math> ).\n\n===Two normally distributed random variables need not be jointly bivariate normal===\n{{See also|normally distributed and uncorrelated does not imply independent}}\nThe fact that two random variables <math>X</math> and <math>Y</math> both have a normal distribution does not imply that the pair <math>(X,Y)</math> has a joint normal distribution.  A simple example is one in which X has a normal distribution with expected value 0 and variance 1, and <math>Y=X</math> if <math>|X| > c</math> and <math>Y=-X</math> if <math>|X| < c</math>, where <math>c > 0</math>.  There are similar counterexamples for more than two random variables. In general, they sum to a [[mixture model]].\n\n===Correlations and independence===\n\nIn general, random variables may be uncorrelated but statistically dependent.  But if a random vector has a multivariate normal distribution then any two or more of its components that are uncorrelated are [[statistical independence|independent]].  This implies that any two or more of its components that are [[pairwise independence|pairwise independent]] are independent. But, as pointed out just above, it is ''not'' true that two random variables that are (''separately'', marginally) normally distributed and uncorrelated are independent.\n\n==Conditional distributions==\n\nIf ''N''-dimensional '''x''' is partitioned as follows\n:<math>\n\\mathbf{x}\n=\n\\begin{bmatrix}\n \\mathbf{x}_1 \\\\\n \\mathbf{x}_2\n\\end{bmatrix}\n\\text{ with sizes }\\begin{bmatrix} q \\times 1 \\\\ (N-q) \\times 1 \\end{bmatrix}</math>\n\nand accordingly '''μ''' and '''Σ''' are partitioned as follows\n\n:<math>\n\\boldsymbol\\mu\n=\n\\begin{bmatrix}\n \\boldsymbol\\mu_1 \\\\\n \\boldsymbol\\mu_2\n\\end{bmatrix}\n\\text{ with sizes }\\begin{bmatrix} q \\times 1 \\\\ (N-q) \\times 1 \\end{bmatrix}</math>\n\n:<math>\n\\boldsymbol\\Sigma\n=\n\\begin{bmatrix}\n \\boldsymbol\\Sigma_{11} & \\boldsymbol\\Sigma_{12} \\\\\n \\boldsymbol\\Sigma_{21} & \\boldsymbol\\Sigma_{22}\n\\end{bmatrix}\n\\text{ with sizes }\\begin{bmatrix} q \\times q & q \\times (N-q) \\\\ (N-q) \\times q & (N-q) \\times (N-q) \\end{bmatrix}</math>\n\nthen the distribution of '''x'''<sub>1</sub> conditional on '''x'''<sub>2</sub> = '''a''' is multivariate normal {{nowrap|('''x'''<sub>1</sub>&nbsp;{{!}}&nbsp;'''x'''<sub>2</sub> {{=}} '''a''') ~ ''N''(<span style{{=}}\"text-decoration:overline;\">'''μ'''</span>, <span style{{=}}\"text-decoration:overline;\">'''Σ'''</span>)}} where\n\n:<math>\n\\bar{\\boldsymbol\\mu}\n=\n\\boldsymbol\\mu_1 + \\boldsymbol\\Sigma_{12} \\boldsymbol\\Sigma_{22}^{-1}\n\\left(\n \\mathbf{a} - \\boldsymbol\\mu_2\n\\right)\n</math>\n\nand covariance matrix\n\n:<math>\n\\overline{\\boldsymbol\\Sigma}\n=\n\\boldsymbol\\Sigma_{11} - \\boldsymbol\\Sigma_{12} \\boldsymbol\\Sigma_{22}^{-1} \\boldsymbol\\Sigma_{21}.\n</math><ref name=eaton>{{cite book|last=Eaton|first=Morris L.|title=Multivariate Statistics: a Vector Space Approach|year=1983|publisher=John Wiley and Sons|isbn=978-0-471-02776-8|pages=116–117}}</ref>\n\nThis matrix is the [[Schur complement]] of '''Σ'''<sub>22</sub> in '''Σ'''. This means that to calculate the conditional covariance matrix, one inverts the overall covariance matrix, drops the rows and columns corresponding to the variables being conditioned upon, and then inverts back to get the conditional covariance matrix. Here <math>\\boldsymbol\\Sigma_{22}^{-1}</math> is the [[generalized inverse]] of <math>\\boldsymbol\\Sigma_{22}</math>.\n\nNote that knowing that {{nowrap|'''x'''<sub>2</sub> {{=}} '''a'''}} alters the variance, though the new variance does not depend on the specific value of '''a'''; perhaps more surprisingly, the mean is shifted by <math>\\boldsymbol\\Sigma_{12} \\boldsymbol\\Sigma_{22}^{-1} \\left(\\mathbf{a} - \\boldsymbol\\mu_2 \\right)</math>; compare this with the situation of not knowing the value of '''a''', in which case '''x'''<sub>1</sub> would have distribution\n<math>\\mathcal{N}_q \\left(\\boldsymbol\\mu_1, \\boldsymbol\\Sigma_{11} \\right)</math>.\n\nAn interesting fact derived in order to prove this result, is that the random vectors <math>\\mathbf{x}_2</math> and <math>\\mathbf{y}_1=\\mathbf{x}_1-\\boldsymbol\\Sigma_{12}\\boldsymbol\\Sigma_{22}^{-1}\\mathbf{x}_2</math> are independent.\n\nThe matrix '''Σ'''<sub>12</sub>'''Σ'''<sub>22</sub><sup>−1</sup> is known as the matrix of [[regression analysis|regression]] coefficients.\n\n=== Bivariate case ===\nIn the bivariate case where '''x''' is partitioned into ''X''<sub>1</sub> and ''X''<sub>2</sub>, the conditional distribution of ''X''<sub>1</sub> given ''X''<sub>2</sub> is<ref>{{cite book|last=Jensen|first=J|title=Statistics for Petroleum Engineers and Geoscientists|year=2000|publisher=Elsevier|location=Amsterdam|pages=207}}</ref>\n\n: <math>X_1\\mid X_2=x_2 \\ \\sim\\ \\mathcal{N}\\left(\\mu_1+\\frac{\\sigma_1}{\\sigma_2}\\rho( x_2 - \\mu_2),\\, (1-\\rho^2)\\sigma_1^2\\right). </math>\n\nwhere <math>\\rho</math> is the [[Pearson product-moment correlation coefficient|correlation coefficient]] between ''X''<sub>1</sub> and ''X''<sub>2</sub>.\n\n=== Bivariate conditional expectation ===\n\n====In the general case====\n\n:<math>\n\\begin{pmatrix}\n X_1 \\\\\n X_2\n\\end{pmatrix}  \\sim \\mathcal{N} \\left( \\begin{pmatrix}\n \\mu_1 \\\\\n \\mu_2\n\\end{pmatrix} , \\begin{pmatrix}\n \\sigma^2_1 &  \\rho \\sigma_1 \\sigma_2 \\\\\n \\rho \\sigma_1 \\sigma_2 &  \\sigma^2_2\n\\end{pmatrix} \\right)\n</math>\n\nThe conditional expectation of X<sub>1</sub> given X<sub>2</sub> is:\n\n: <math>\\operatorname{E}(X_1 \\mid X_2=x_2) = \\mu_1 + \\rho \\frac{\\sigma_1}{\\sigma_2}(x_2 - \\mu_2)</math>\n\nProof: the result is obtained by taking the expectation of the conditional distribution <math>X_1\\mid X_2</math> above.\n\n====In the centered case with unit variances====\n\n:<math>\n\\begin{pmatrix}\n X_1 \\\\\n X_2\n\\end{pmatrix}  \\sim \\mathcal{N} \\left( \\begin{pmatrix}\n 0 \\\\\n 0\n\\end{pmatrix} , \\begin{pmatrix}\n 1 & \\rho \\\\\n \\rho & 1\n\\end{pmatrix} \\right)\n</math>\n\nThe conditional expectation of ''X''<sub>1</sub> given ''X''<sub>2</sub> is\n\n: <math>\\operatorname{E}(X_1 \\mid X_2=x_2)= \\rho x_2 </math>\n\nand the conditional variance is\n\n: <math> \\operatorname{var}(X_1 \\mid X_2 = x_2) = 1-\\rho^2; </math>\n\nthus the conditional variance does not depend on ''x''<sub>2</sub>.\n\nThe conditional expectation of ''X''<sub>1</sub> given that ''X''<sub>2</sub> is smaller/bigger than ''z'' is (Maddala 1983, p.&nbsp;367<ref name=Maddala83>{{cite book|last=Gangadharrao|first=Maddala|title=Limited Dependent and Qualitative Variables in Econometrics|year=1983|publisher=Cambridge University Press}}</ref>) :\n\n:<math>\n\\operatorname{E}(X_1 \\mid X_2 < z) = -\\rho { \\phi(z) \\over \\Phi(z) } ,\n</math>\n\n:<math>\n\\operatorname{E}(X_1 \\mid X_2 > z) = \\rho { \\phi(z) \\over (1- \\Phi(z)) } ,\n</math>\n\nwhere the final ratio here is called the [[inverse Mills ratio]].\n\nProof: the last two results are obtained using the result <math>\\operatorname{E}(X_1 \\mid X_2=x_2)= \\rho x_2 </math>, so that\n\n:<math>\n\\operatorname{E}(X_1 \\mid X_2 < z) = \\rho E(X_2 \\mid X_2 < z)</math> and then using the properties of the expectation of a [[truncated normal distribution]].\n\n==Marginal distributions==\nTo obtain the [[marginal distribution]] over a subset of multivariate normal random variables, one only needs to drop the irrelevant variables (the variables that one wants to marginalize out) from the mean vector and the covariance matrix.  The proof for this follows from the definitions of multivariate normal distributions and linear algebra.<ref>The formal proof for marginal distribution is shown here http://fourier.eng.hmc.edu/e161/lectures/gaussianprocess/node7.html</ref>\n\n''Example''\n\nLet {{nowrap|'''X''' {{=}} [''X''<sub>1</sub>, ''X''<sub>2</sub>, ''X''<sub>3</sub>]}} be multivariate normal random variables with mean vector {{nowrap|'''μ''' {{=}} [''μ''<sub>1</sub>, ''μ''<sub>2</sub>, ''μ''<sub>3</sub>]}} and covariance matrix '''Σ''' (standard parametrization for multivariate normal distributions). Then the joint distribution of {{nowrap|'''X′''' {{=}} [''X''<sub>1</sub>, ''X''<sub>3</sub>]}} is multivariate normal with mean vector {{nowrap|'''μ′''' {{=}} [''μ''<sub>1</sub>, ''μ''<sub>3</sub>]}} and covariance matrix\n<math> \\boldsymbol\\Sigma' =\n\\begin{bmatrix}\n\\boldsymbol\\Sigma_{11} & \\boldsymbol\\Sigma_{13} \\\\\n\\boldsymbol\\Sigma_{31} & \\boldsymbol\\Sigma_{33}\n\\end{bmatrix}\n</math>.\n\n==Affine transformation==\n\nIf {{nowrap|'''Y''' {{=}} '''c''' + '''BX'''}} is an [[affine transformation]] of <math>\\mathbf{X}\\ \\sim \\mathcal{N}(\\boldsymbol\\mu, \\boldsymbol\\Sigma),</math> where '''c''' is an <math>M \\times 1</math> vector of constants and '''B''' is a constant <math>M \\times N</math> matrix, then '''Y''' has a multivariate normal distribution with expected value {{nowrap|'''c''' + '''Bμ'''}} and variance '''BΣB'''<sup>T</sup> i.e., <math>\\mathbf{Y} \\sim \\mathcal{N} \\left(\\mathbf{c} + \\mathbf{B} \\boldsymbol\\mu, \\mathbf{B} \\boldsymbol\\Sigma \\mathbf{B}^{\\rm T}\\right)</math>. In particular, any subset of the ''X<sub>i</sub>'' has a marginal distribution that is also multivariate normal.\nTo see this, consider the following example: to extract the subset (''X''<sub>1</sub>, ''X''<sub>2</sub>, ''X''<sub>4</sub>)<sup>T</sup>, use\n\n:<math>\n\\mathbf{B}\n=\n\\begin{bmatrix}\n 1 & 0 & 0 & 0 & 0 & \\ldots & 0 \\\\\n 0 & 1 & 0 & 0 & 0 & \\ldots & 0 \\\\\n 0 & 0 & 0 & 1 & 0 & \\ldots & 0\n\\end{bmatrix}\n</math>\n\nwhich extracts the desired elements directly.\n\nAnother corollary is that the distribution of {{nowrap|'''Z''' {{=}} '''b''' · '''X'''}}, where '''b''' is a constant vector with the same number of elements as '''X''' and the dot indicates the [[dot product]], is univariate Gaussian with <math>Z\\sim\\mathcal{N}\\left(\\mathbf{b}\\cdot\\boldsymbol\\mu, \\mathbf{b}^{\\rm T}\\boldsymbol\\Sigma \\mathbf{b}\\right)</math>. This result follows by using\n\n:<math>\n\\mathbf{B}=\\begin{bmatrix}\nb_1    & b_2    & \\ldots & b_n\n\\end{bmatrix} = \\mathbf{b}^{\\rm T}.\n</math>\nObserve how the positive-definiteness of '''Σ''' implies that the variance of the dot product must be positive.\n\nAn affine transformation of '''X''' such as 2'''X''' is not the same as the [[Sum of normally distributed random variables|sum of two independent realisations]] of '''X'''.\n\n==Geometric interpretation==\n{{see also|Confidence region}}\n\nThe equidensity contours of a non-singular multivariate normal distribution are [[ellipsoid]]s (i.e. linear transformations of [[hypersphere]]s) centered at the mean.<ref>{{cite web|author=Nikolaus Hansen|title=The CMA Evolution Strategy: A Tutorial|url=http://www.lri.fr/~hansen/cmatutorial.pdf|access-date=2012-01-07|archive-url=https://web.archive.org/web/20100331114258/http://www.lri.fr/~hansen/cmatutorial.pdf|archive-date=2010-03-31|dead-url=yes|df=|bibcode=2016arXiv160400772H|year=2016|arxiv=1604.00772}}</ref> Hence the multivariate normal distribution is an example of the class of [[elliptical distribution]]s. The directions of the principal axes of the ellipsoids are given by the eigenvectors of the covariance matrix <math>\\boldsymbol\\Sigma</math>. The squared relative lengths of the principal axes are given by the corresponding eigenvalues.\n\nIf {{nowrap|'''Σ''' {{=}} '''UΛU'''<sup>T</sup> {{=}} '''UΛ'''<sup>1/2</sup>('''UΛ'''<sup>1/2</sup>)<sup>T</sup>}} is an [[eigendecomposition]] where the columns of '''U''' are unit eigenvectors and '''Λ''' is a [[diagonal matrix]] of the eigenvalues, then we have\n\n::<math>\\mathbf{X}\\ \\sim \\mathcal{N}(\\boldsymbol\\mu, \\boldsymbol\\Sigma) \\iff \\mathbf{X}\\ \\sim \\boldsymbol\\mu+\\mathbf{U}\\boldsymbol\\Lambda^{1/2}\\mathcal{N}(0, \\mathbf{I}) \\iff \\mathbf{X}\\ \\sim \\boldsymbol\\mu+\\mathbf{U}\\mathcal{N}(0, \\boldsymbol\\Lambda).</math>\n\nMoreover, '''U''' can be chosen to be a [[rotation matrix]], as inverting an axis does not have any effect on ''N''(0, '''Λ'''), but inverting a column changes the sign of '''U''''s determinant. The distribution ''N''('''μ''', '''Σ''') is in effect ''N''(0, '''I''') scaled by '''Λ'''<sup>1/2</sup>, rotated by '''U''' and translated by '''μ'''.\n\nConversely, any choice of '''μ''', full rank matrix '''U''', and positive diagonal entries Λ<sub>''i''</sub> yields a non-singular multivariate normal distribution. If any Λ<sub>''i''</sub> is zero and '''U''' is square, the resulting covariance matrix '''UΛU'''<sup>T</sup> is [[singular matrix|singular]]. Geometrically this means that every contour ellipsoid is infinitely thin and has zero volume in ''n''-dimensional space, as at least one of the principal axes has length of zero; this is the [[degenerate distribution|degenerate case]].\n\n\"The radius around the true mean in a bivariate normal random variable, re-written in [[polar coordinates]] (radius and angle), follows a [[Hoyt distribution]].\"<ref>{{cite web |title=The Hoyt Distribution (Documentation for R package 'shotGroups' version 0.6.2) |author=Daniel Wollschlaeger |url=http://finzi.psych.upenn.edu/usr/share/doc/library/shotGroups/html/hoyt.html }}{{dead link|date=December 2017 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>\n\n==Estimation of parameters==\n{{further|Estimation of covariance matrices}}\nThe derivation of the [[maximum likelihood|maximum-likelihood]] [[estimator]] of the covariance matrix of a multivariate normal distribution is straightforward.\n\nIn short, the  probability density function (pdf) of a multivariate normal is\n\n:<math>f(\\mathbf{x})= \\frac{1}{\\sqrt { (2\\pi)^k|\\boldsymbol \\Sigma| } }  \\exp\\left(-{1 \\over 2} (\\mathbf{x}-\\boldsymbol\\mu)^{\\rm T} \\boldsymbol\\Sigma^{-1} ({\\mathbf x}-\\boldsymbol\\mu)\\right)</math>\n\nand the ML estimator of the covariance matrix from a sample of ''n'' observations is\n\n:<math>\\widehat{\\boldsymbol\\Sigma} = {1 \\over n}\\sum_{i=1}^n ({\\mathbf x}_i-\\overline{\\mathbf x})({\\mathbf x}_i-\\overline{\\mathbf x})^T</math>\n\nwhich is simply the [[sample covariance matrix]].  This is a [[biased estimator]] whose expectation is\n\n:<math>E[\\widehat{\\boldsymbol\\Sigma}] = \\frac{n-1}{n} \\boldsymbol\\Sigma.</math>\n\nAn unbiased sample covariance is\n\n:<math>\\widehat{\\boldsymbol\\Sigma} = {1 \\over n-1}\\sum_{i=1}^n (\\mathbf{x}_i-\\overline{\\mathbf{x}})(\\mathbf{x}_i-\\overline{\\mathbf{x}})^{\\rm T}.</math>\n\nThe [[Fisher information matrix]] for estimating the parameters of a multivariate normal distribution has a closed form expression. This can be used, for example, to compute the [[Cramér–Rao bound]] for parameter estimation in this setting. See [[Fisher information#Multivariate normal distribution|Fisher information]] for more details.\n\n==Bayesian inference==\nIn [[Bayesian statistics]], the [[conjugate prior]] of the mean vector is another multivariate normal distribution, and the conjugate prior of the covariance matrix is an [[inverse-Wishart distribution]] <math>\\mathcal{W}^{-1}</math> .  Suppose then that ''n'' observations have been made\n:<math>\\mathbf{X} = \\{\\mathbf{x}_1,\\dots,\\mathbf{x}_n\\} \\sim \\mathcal{N}(\\boldsymbol\\mu,\\boldsymbol\\Sigma)</math>\nand that a conjugate prior has been assigned, where\n:<math>p(\\boldsymbol\\mu,\\boldsymbol\\Sigma)=p(\\boldsymbol\\mu\\mid\\boldsymbol\\Sigma)\\ p(\\boldsymbol\\Sigma),</math>\nwhere\n:<math>p(\\boldsymbol\\mu\\mid\\boldsymbol\\Sigma) \\sim\\mathcal{N}(\\boldsymbol\\mu_0,m^{-1}\\boldsymbol\\Sigma) ,</math>\nand\n:<math>p(\\boldsymbol\\Sigma) \\sim \\mathcal{W}^{-1}(\\boldsymbol\\Psi,n_0).</math>\n\nThen,{{citation needed|date=July 2012}}\n\n:<math>\n\\begin{array}{rcl}\np(\\boldsymbol\\mu\\mid\\boldsymbol\\Sigma,\\mathbf{X}) & \\sim & \\mathcal{N}\\left(\\frac{n\\bar{\\mathbf{x}} + m\\boldsymbol\\mu_0}{n+m},\\frac{1}{n+m}\\boldsymbol\\Sigma\\right),\\\\\np(\\boldsymbol\\Sigma\\mid\\mathbf{X}) & \\sim & \\mathcal{W}^{-1}\\left(\\boldsymbol\\Psi+n\\mathbf{S}+\\frac{nm}{n+m}(\\bar{\\mathbf{x}}-\\boldsymbol\\mu_0)(\\bar{\\mathbf{x}}-\\boldsymbol\\mu_0)', n+n_0\\right),\n\\end{array}\n</math>\nwhere\n:<math>\n\\begin{align}\n\\bar{\\mathbf{x}} & = n^{-1}\\sum_{i=1}^{n} \\mathbf{x}_i ,\\\\\n\\mathbf{S} & = n^{-1}\\sum_{i=1}^{n} (\\mathbf{x}_i - \\bar{\\mathbf{x}})(\\mathbf{x}_i - \\bar{\\mathbf{x}})' .\n\\end{align}\n</math>\n\n== Multivariate normality tests ==\n\nMultivariate normality tests check a given set of data for similarity to the multivariate [[normal distribution]].  The [[null hypothesis]] is that the [[data set]] is similar to the normal distribution, therefore a sufficiently small [[p-value|''p''-value]] indicates non-normal data. Multivariate normality tests include the Cox–Small test<ref>{{Cite journal | last1 = Cox | first1 = D. R. | last2 = Small | first2 = N. J. H. | doi = 10.1093/biomet/65.2.263 | title = Testing multivariate normality | journal = Biometrika | volume = 65 | issue = 2 | pages = 263 | year = 1978 | pmid =  | pmc = }}</ref>\nand Smith and Jain's adaptation<ref>{{Cite journal | last1 = Smith | first1 = S. P. | last2 = Jain | first2 = A. K. | doi = 10.1109/34.6789 | title = A test to determine the multivariate normality of a data set | journal = IEEE Transactions on Pattern Analysis and Machine Intelligence | volume = 10 | issue = 5 | pages = 757 | year = 1988 | pmid =  | pmc = }}</ref> of the Friedman–Rafsky test created by [[Larry rafsky|Larry Rafsky]] and [[Jerome H. Friedman|Jerome Friedman]].<ref>{{Cite journal | last1 = Friedman | first1 = J. H. | last2 = Rafsky | first2 = L. C. | doi = 10.1214/aos/1176344722 | title = Multivariate Generalizations of the Wald–Wolfowitz and Smirnov Two-Sample Tests | journal = The Annals of Statistics | volume = 7 | issue = 4 | pages = 697 | year = 1979 | pmid =  | pmc = }}</ref>\n\n'''Mardia's test'''<ref name=Mardia/> is based on multivariate extensions of [[skewness]] and [[kurtosis]] measures. For a sample {'''x'''<sub>1</sub>, ..., '''x'''<sub>''n''</sub>} of ''k''-dimensional vectors we compute\n: <math>\\begin{align}\n  & \\widehat{\\boldsymbol\\Sigma} = {1 \\over n} \\sum_{j=1}^n \\left(\\mathbf{x}_j - \\bar{\\mathbf{x}}\\right)\\left(\\mathbf{x}_j - \\bar{\\mathbf{x}}\\right)^T \\\\\n\n  & A = {1 \\over 6n} \\sum_{i=1}^n \\sum_{j=1}^n \\left[ (\\mathbf{x}_i - \\bar{\\mathbf{x}})^T\\;\\widehat{\\boldsymbol\\Sigma}^{-1} (\\mathbf{x}_j - \\bar{\\mathbf{x}}) \\right]^3 \\\\\n\n  & B = \\sqrt{\\frac{n}{8k(k+2)}}\\left\\{{1 \\over n} \\sum_{i=1}^n \\left[ (\\mathbf{x}_i - \\bar{\\mathbf{x}})^T\\;\\widehat{\\boldsymbol\\Sigma}^{-1} (\\mathbf{x}_i - \\bar{\\mathbf{x}}) \\right]^2 - k(k+2) \\right\\}\n  \\end{align}</math>\nUnder the null hypothesis of multivariate normality, the statistic ''A'' will have approximately a [[chi-squared distribution]] with {{nowrap|{{frac2|1|6}}⋅''k''(''k'' + 1)(''k'' + 2)}} degrees of freedom, and ''B'' will be approximately [[standard normal]] ''N''(0,1).\n\nMardia's kurtosis statistic is skewed and converges very slowly to the limiting normal distribution.  For medium size samples <math>(50 \\le n < 400)</math>, the parameters of the asymptotic distribution of the kurtosis statistic are modified<ref>Rencher (1995), pages 112–113.</ref> For small sample tests (<math>n<50</math>) empirical critical values are used. Tables of critical values for both statistics are given by Rencher<ref>Rencher (1995), pages 493–495.</ref> for ''k''&nbsp;=&nbsp;2,&nbsp;3,&nbsp;4.\n\nMardia's tests are affine invariant but not consistent.  For example, the multivariate skewness test is not consistent against\nsymmetric non-normal alternatives.<ref>{{Cite journal | last1 = Baringhaus | first1 = L. | last2 = Henze | first2 = N. | doi = 10.1016/0047-259X(91)90031-V | title = Limit distributions for measures of multivariate skewness and kurtosis based on projections | journal = Journal of Multivariate Analysis | volume = 38 | pages = 51–69 | year = 1991 | pmid =  | pmc = }}</ref>\n\nThe '''BHEP test'''<ref name=BH/> computes the norm of the difference between the empirical [[characteristic function (probability theory)|characteristic function]] and the theoretical characteristic function of the normal distribution. Calculation of the norm is performed in the [[Lp space|L<sup>2</sup>(''μ'')]] space of square-integrable functions with respect to the Gaussian weighting function <math>\\scriptstyle \\mu_\\beta(\\mathbf{t}) = (2\\pi\\beta^2)^{-k/2} e^{-|\\mathbf{t}|^2/(2\\beta^2)}</math>. The test statistic is\n: <math>\\begin{align}\n    T_\\beta &= \\int_{\\mathbb{R}^k} \\left| {1 \\over n} \\sum_{j=1}^n e^{i\\mathbf{t}^T\\widehat{\\boldsymbol\\Sigma}^{-1/2}(\\mathbf{x}_j - \\bar{\\mathbf{x})}} - e^{-|\\mathbf{t}|^2/2} \\right|^2 \\; \\boldsymbol\\mu_\\beta(\\mathbf{t}) \\, d\\mathbf{t} \\\\\n            &= {1 \\over n^2} \\sum_{i,j=1}^n e^{-{\\beta^2 \\over 2}(\\mathbf{x}_i-\\mathbf{x}_j)^T\\widehat{\\boldsymbol\\Sigma}^{-1}(\\mathbf{x}_i-\\mathbf{x}_j)} - \\frac{2}{n(1 + \\beta^2)^{k/2}}\\sum_{i=1}^n e^{ -\\frac{\\beta^2}{2(1+\\beta^2)} (\\mathbf{x}_i-\\bar{\\mathbf{x}})^T\\widehat{\\boldsymbol\\Sigma}^{-1}(\\mathbf{x}_i-\\bar{\\mathbf{x}})} + \\frac{1}{(1 + 2\\beta^2)^{k/2}}\n  \\end{align}</math>\nThe limiting distribution of this test statistic is a weighted sum of chi-squared random variables,<ref name=BH/> however in practice it is more convenient to compute the sample quantiles using the Monte-Carlo simulations.{{citation needed|date=July 2012}}\n\nA detailed survey of these and other test procedures is available.<ref name=Henze/>\n\n==Drawing values from the distribution==\n\nA widely used method for drawing (sampling) a random vector '''x''' from the ''N''-dimensional multivariate normal distribution with mean vector '''μ''' and [[covariance matrix]] '''Σ''' works as follows:<ref name=Gentle/>\n\n# Find any real matrix '''A''' such that {{nowrap|'''A'''&thinsp;'''A'''<sup>T</sup> {{=}} '''Σ'''}}. When '''Σ''' is positive-definite, the [[Cholesky decomposition]] is typically used, and the [[Cholesky decomposition#Avoiding taking square roots|extended form]] of this decomposition can always be used  (as the covariance matrix may be only positive semi-definite) in both cases a suitable matrix '''A''' is obtained. An alternative is to use the matrix '''A''' = '''UΛ'''<sup>½</sup> obtained from a [[Eigendecomposition of a matrix#Real symmetric matrices|spectral decomposition]] '''Σ''' = '''UΛU'''<sup>−1</sup> of '''Σ'''. The former approach is more computationally straightforward but the matrices '''A''' change for different orderings of the elements of the random vector, while the latter approach gives matrices that are related by simple re-orderings. In theory both approaches give equally good ways of determining a suitable matrix '''A''', but there are differences in computation time.\n# Let {{nowrap|'''z''' {{=}} (''z''<sub>1</sub>, …, ''z<sub>N</sub>'')<sup>T</sup>}} be a vector whose components are ''N'' [[statistical independence|independent]] [[normal distribution|standard normal]] variates (which can be generated, for example, by using the [[Box–Muller transform]]).\n# Let '''x''' be {{nowrap|'''μ''' + '''Az'''}}. This has the desired distribution due to the affine transformation property.\n\n== See also ==\n* [[Chi distribution]], the [[probability density function|pdf]] of the [[Norm (mathematics)#p-norm|2-norm]] (or [[Euclidean norm]]) of a multivariate normally distributed vector (centered at zero).\n* [[Complex normal distribution]], an application of bivariate normal distribution\n*  [[Gaussian copula|Copula]], for the definition of the Gaussian or normal copula model.\n* [[Multivariate t-distribution]], which is another widely used spherically symmetric multivariate distribution.\n* [[Multivariate stable distribution]] extension of the multivariate normal distribution, when the index (exponent in the characteristic function) is between zero and two.\n* [[Mahalanobis distance]]\n* [[Wishart distribution]]\n* [[Matrix normal distribution]]\n\n== References ==\n{{Reflist|refs=\n\n<ref name = Siotani>{{cite journal\n  | author = Siotani, Minoru\n  | title = Tolerance regions for a multivariate normal population\n  | journal = Annals of the Institute of Statistical Mathematics\n  | year = 1964\n  | volume = 16\n  | number = 1\n  | pages = 135–153\n  | doi = 10.1007/BF02868568\n  | url = http://www.ism.ac.jp/editsec/aism/pdf/016_1_0135.pdf\n  }}</ref>\n\n<ref name=Mardia>{{cite journal\n  | last = Mardia | first = K. V.\n  | year = 1970\n  | title = Measures of multivariate skewness and kurtosis with applications\n  | journal = Biometrika\n  | volume = 57 | issue = 3  | pages = 519–530\n  | doi = 10.1093/biomet/57.3.519\n  }}</ref>\n\n<!--\n<ref name=EP>{{cite journal\n  | last1 = Epps | first1 = Lawrence B.\n  | last2 = Pulley  | first2 = Lawrence B.\n  | year = 1983\n  | title = A test for normality based on the empirical characteristic function\n  | journal = Biometrika\n  | volume = 70 | issue = 3 | pages = 723–726\n  | doi = 10.1093/biomet/70.3.723\n  }}</ref>\n-->\n\n<ref name=BH>{{cite journal\n  | last1 = Baringhaus | first1 = L.\n  | last2 = Henze      | first2 = N.\n  | year = 1988\n  | title = A consistent test for multivariate normality based on the empirical characteristic function\n  | journal = Metrika\n  | volume = 35 | issue = 1 | pages = 339–348\n  | doi = 10.1007/BF02613322\n  }}</ref>\n\n<ref name=Henze>{{cite journal\n  | last = Henze | first = Norbert\n  | year = 2002\n  | title = Invariant tests for multivariate normality: a critical review\n  | journal = Statistical Papers\n  | volume = 43 | issue = 4   | pages = 467–506\n  | doi = 10.1007/s00362-002-0119-6\n  }}</ref>\n\n<ref name=HT>{{cite journal\n  | last1 = Hamedani | first1 = G. G.\n  | last2 = Tata     | first2 = M. N.\n  | year = 1975\n  | title = On the determination of the bivariate normal distribution from distributions of linear combinations of the variables\n  | journal = The American Mathematical Monthly\n  | volume = 82 | issue = 9   | pages = 913–915\n  | doi = 10.2307/2318494\n  | jstor = 2318494\n }}</ref>\n\n<ref name=wyattlms>{{cite web|last=Wyatt|first=John\n  | title=Linear least mean-squared error estimation\n  | url=http://web.mit.edu/6.041/www/LECTURE/lec22.pdf\n  | work=Lecture notes course on applied probability|accessdate=23 January 2012}}</ref>\n\n<ref name=rao>{{cite book\n  | author = Rao, C.R.\n  | title =  Linear Statistical Inference and Its Applications\n  | year = 1973\n  | publisher = Wiley\n  | location = New York\n  | pages = 527–528\n  }}</ref>\n\n<ref name=Gentle>{{cite book\n  | author = Gentle, J.E.\n  | title =  Computational Statistics\n  | year = 2009\n  | publisher = Springer\n  | location = New York\n  | pages = 315–316\n  | doi = 10.1007/978-0-387-98144-4\n  | series = Statistics and Computing\n | isbn = 978-0-387-98143-7\n | url = http://cds.cern.ch/record/1639470\n }}</ref>\n\n}}\n\n=== Literature ===\n\n{{refbegin}}\n* {{cite book\n  | author = Rencher, A.C.\n  | title =  Methods of Multivariate Analysis\n  | year = 1995\n  | publisher = Wiley\n  | location = New York\n  }}\n* {{cite book |first=Y. L. |last=Tong |title=The multivariate normal distribution |year=1990 |isbn=978-1-4613-9657-4 |series=Springer Series in Statistics |location=New York |publisher=Springer-Verlag |doi=10.1007/978-1-4613-9655-0 |ref=harv }}\n\n{{refend}}\n\n{{ProbDistributions|multivariate|state=collapsed}}\n{{statistics|analysis|state=collapsed}}\n\n{{DEFAULTSORT:Multivariate Normal Distribution}}\n[[Category:Continuous distributions]]\n[[Category:Multivariate continuous distributions]]\n[[Category:Normal distribution]]\n[[Category:Exponential family distributions]]\n[[Category:Stable distributions]]"
    },
    {
      "title": "Normal probability plot",
      "url": "https://en.wikipedia.org/wiki/Normal_probability_plot",
      "text": "{{Use dmy dates|date=August 2012}}\nThe '''normal probability plot''' is a [[graphical technique]] to identify substantive departures from [[normal distribution|normality]].  This includes identifying [[outlier]]s, [[skewness]], [[kurtosis]], a need for transformations, and [[Mixture distribution|mixtures]].  Normal probability plots are made of raw data, [[Errors and residuals in statistics|residuals from model fits]], and estimated parameters.  \n\n[[Image:normprob.png|thumb|350px|A normal probability plot]]\n\nIn a normal probability plot (also called a \"normal plot\"), the sorted data are plotted vs. values selected to make the resulting image look close to a straight line if the data are approximately normally distributed.  Deviations from a straight line suggest departures from normality. The plotting can be manually performed by using a special [[graph paper]], called ''normal probability paper''.  With modern computers normal plots are commonly made with software.  \n\nThe normal probability plot is a special case of the [[Q–Q plot|Q–Q]] [[probability plot]] for a normal distribution.  The theoretical [[quantile]]s are generally chosen to approximate either the mean or the median of the corresponding [[order statistic]]s.  \n\n==Definition==\nThe normal probability plot is formed by plotting the sorted data vs. an approximation to the means or medians of the corresponding [[order statistic]]s; see [[rankit]].  Some users plot the data on the vertical axis;<ref>e.g., Chambers et al. (1983, ch. 6.  Assessing distributional assumptions about data, p. 194)</ref> others plot the data on the horizontal axis.<ref>{{Citation | last = Box | first = George E. P. \n| last2 = Draper | first2 = Norman | author-link = George E. P. Box \n| year = 2007 \n| title = Response Surfaces, Mixtures, and Ridge Analysis \n| edition = 2nd \n| publisher = Wiley \n| isbn = 978-0-470-05357-7}}</ref><ref>\n{{Citation\n| last =  Titterington\n| first = D. M. \n| last2 = Smith\n| first2  = A. F. M. \n| last3 = Makov \n| first3 = U. E. \n| year = 1985 \n| title = Statistical Analysis of Finite Mixture Distributions \n| chapter = 4.  Learning about the parameters of a mixture \n| publisher = Wiley\n| isbn = 0-471-90763-4\n}}</ref>  \n\nDifferent sources use slightly different approximations for [[rankits]].  The formula used by the \"qqnorm\" function in the basic \"stats\" package in [[R (programming language)]] is as follows:  \n\n: <math> z_i = \\Phi^{-1}\\left( \\frac{i-a}{n+1-2a} \\right), </math>\n\nfor {{math|''i'' {{=}} 1, 2, ..., ''n''}}, \nwhere \n\n:{{math|''a'' {{=}} 3/8}} if {{math|''n''&nbsp;≤&nbsp;10}} and \n::0.5 for ''n''&nbsp;>&nbsp;10, \n\nand {{math|&Phi;{{sup|−1}}}} is the standard normal [[quantile]] function.  \n\nIf the data are consistent with a sample from a normal distribution, the points should lie close to a straight line. As a reference, a straight line can be fit to the points. The further the points vary from this line, the greater the indication of departure from normality. If the sample has mean 0, standard deviation 1 then a line through 0 with slope 1 could be used. \n\nWith more points, random deviations from a line will be less pronounced.  Normal plots are often used with as few as 7 points, e.g., with plotting the effects in a saturated model from a [[Fractional factorial design|2-level fractional factorial experiment]].  With fewer points, it becomes harder to distinguish between random variability and a substantive deviation from normality.\n\n==Other distributions==\n{{main|Probability plot}}\nProbability plots for distributions other than the normal are computed in exactly the same way. The normal quantile function {{math|&Phi;{{sup|−1}}}} is simply replaced by the quantile function of the desired distribution. In this way, a probability plot can easily be generated for any distribution for which one has the quantile function.\n\nWith a [[Location-scale family|location-scale family of distributions]], the [[location parameter|location]] and [[scale parameter]]s of the distribution can be estimated from the [[Y-intercept|intercept]] and the [[slope]] of the line.  For other distributions the parameters must first be estimated before a probability plot can be made.\n\n==Plot types==\nThis is a sample of size 50 from a normal distribution, plotted as both a histogram, and a normal probability plot.\n\n<gallery widths=\"200\" heights=\"180\">\nFile:normprob.png|Normal probability plot of a sample from a normal distribution – it looks fairly straight, at least  when the few large and small values are ignored.\nFile:normhist.png|Histogram of a sample from a normal distribution  – it looks fairly symmetric and unimodal\n</gallery>\n\nThis is a sample of size 50 from a right-skewed distribution, plotted as both a histogram, and a normal probability plot.\n\n<gallery widths=\"200\" heights=\"180\">\nFile:normexpprob.png|Normal probability plot of a sample from a right-skewed distribution – it has an inverted C shape.\nFile:normexphist.png|Histogram of a sample from a right-skewed distribution – it looks unimodal and skewed right.\n</gallery>\n\nThis is a sample of size 50 from a uniform distribution, plotted as both a histogram, and a normal probability plot.\n\n<gallery widths=\"200\" heights=\"180\">\nFile:normunifprob.png|Normal probability plot of a sample from a uniform distribution – it has an S shape.\nFile:normunifhist.png|Histogram of a sample from a uniform distribution  – it looks multimodal and supposedly roughly symmetric.\n</gallery>\n\n==See also==\n* [[P–P plot]]\n* [[Q–Q plot]]\n* [[Rankit]]\n\n==References==\n{{NIST-PD}}\n{{no footnotes|date=July 2011}}\n{{reflist}}\n\n== Further reading ==\n*{{cite book|last = Chambers|first= John|author2=William Cleveland |author3=Beat Kleiner |author4=Paul Tukey |year = 1983|title = Graphical Methods for Data Analysis|publisher = Wadsworth}}\n\n==External links==\n{{Commons category|Probability plots}}\n* [http://www.itl.nist.gov/div898/handbook/eda/section3/normprpl.htm Engineering Statistics Handbook: Normal Probability Plot]\n* [http://www.statit.com/support/quality_practice_tips/testingfornearnormality.shtml Statit Support: Testing for \"Near-Normality\": The Probability Plot]\n\n\n{{Distribution fitting}}\n\n\n[[Category:Statistical charts and diagrams]]\n[[Category:Normal distribution]]\n[[Category:Normality tests]]"
    },
    {
      "title": "Normal-gamma distribution",
      "url": "https://en.wikipedia.org/wiki/Normal-gamma_distribution",
      "text": "{{Probability distribution |\n  name       =normal-gamma|\n  type       =density|\n  pdf_image  =|\n  cdf_image  =|\n  parameters =<math>\\mu\\,</math> [[location parameter|location]] ([[real number|real]])<br /><math>\\lambda > 0\\,</math> (real)<br /><math>\\alpha > 0\\,</math> (real)<br /><math>\\beta > 0\\,</math> (real)|\n  support    =<math>x \\in (-\\infty, \\infty)\\,\\!, \\; \\tau \\in (0,\\infty)</math>|\n  pdf        =<math>f(x,\\tau\\mid\\mu,\\lambda,\\alpha,\\beta) = \\frac{\\beta^\\alpha \\sqrt{\\lambda}}{\\Gamma(\\alpha)\\sqrt{2\\pi}}  \\, \\tau^{\\alpha-\\frac{1}{2}}\\,e^{-\\beta\\tau}\\,e^{ -\\frac{ \\lambda \\tau (x- \\mu)^2}{2}}</math>|\n  cdf        =|\n  mean       =<ref name=BS434>Bernardo & Smith (1993, p. 434)</ref>   <math>\\operatorname{E}(X)=\\mu\\,\\! ,\\quad \\operatorname{E}(\\Tau)= \\alpha \\beta^{-1}</math>  |\n  median     = <!-- <math>\\mu\\,</math> --> |\n  mode       = <math>\\left(\\mu, \\frac{\\alpha - \\frac12}{\\beta}\\right)</math>|\n  variance   =<ref name=BS434/> <math>\\operatorname{var}(X)= \\Big(\\frac{\\beta}{\\lambda (\\alpha-1)}\\Big) ,\\quad \n\\operatorname{var}(\\Tau)=\\alpha \\beta^{-2} </math> |\n  skewness   =|\n  kurtosis   =|\n  entropy    =|\n  mgf        =|\n  char       =|\n}}\nIn [[probability theory]] and [[statistics]], the '''normal-gamma distribution''' (or '''Gaussian-gamma distribution''') is a bivariate four-parameter family of continuous [[probability distribution]]s. It is the [[conjugate prior]] of a [[normal distribution]] with unknown [[mean]] and [[Precision (statistics)|precision]].<ref>Bernardo & Smith (1993, pages 136, 268, 434)</ref>\n\n==Definition==\nFor a pair of [[random variables]], (''X'',''T''), suppose that the [[conditional distribution]] of ''X'' given ''T'' is given by\n\n:<math>  X\\mid T \\sim N(\\mu,1 /(\\lambda  T)) \\,\\! , </math>\n\nmeaning that the conditional distribution is a [[normal distribution]] with [[mean]] <math> \\mu</math> and [[precision (statistics)|precision]] <math> \\lambda T </math> — equivalently, with [[variance]] <math> 1 / (\\lambda T) . </math>\n\nSuppose also that the marginal distribution of ''T'' is given by\n\n:<math>T \\mid \\alpha, \\beta \\sim \\operatorname{Gamma}(\\alpha,\\beta),</math>\n\nwhere this means that  ''T'' has a [[gamma distribution]]. Here ''&lambda;'', ''&alpha;'' and ''&beta;'' are parameters of the joint distribution.\n\nThen  (''X'',''T'') has a normal-gamma distribution, and this is denoted by\n:<math> (X,T) \\sim \\operatorname{NormalGamma}(\\mu,\\lambda,\\alpha,\\beta).\n</math>\n\n==Properties==\n\n===Probability density function===\n\nThe joint [[probability density function]] of (''X'',''T'') is{{citation needed|date=April 2013}}\n: <math>f(x,\\tau\\mid\\mu,\\lambda,\\alpha,\\beta) = \\frac{\\beta^\\alpha \\sqrt{\\lambda}}{\\Gamma(\\alpha)\\sqrt{2\\pi}}  \\, \\tau^{\\alpha-\\frac{1}{2}}\\,e^{-\\beta\\tau}\\exp\\left( -\\frac{ \\lambda \\tau (x- \\mu)^2}{2}\\right)</math>\n\n===Marginal distributions===\n\nBy construction, the [[marginal distribution]] of <math>\\tau</math> is a [[gamma distribution]], and the [[conditional distribution]] of <math>x</math> given <math>\\tau</math> is a [[Gaussian distribution]].  The [[marginal distribution]] of <math>x</math> is a three-parameter non-standardized [[Student's t-distribution]] with parameters <math>(\\nu, \\mu, \\sigma^2)=(2\\alpha, \\mu, \\beta/(\\lambda\\alpha))</math>.{{citation needed|date=April 2013}}\n\n===Exponential family===\n\nThe normal-gamma distribution is a four-parameter [[exponential family]] with [[natural parameters]] <math>\\alpha-1/2, -\\beta-\\lambda\\mu^2/2, \\lambda\\mu, -\\lambda/2</math> and [[natural statistics]] <math>\\ln\\tau, \\tau, \\tau x, \\tau x^2</math>.{{citation needed|date=April 2013}}\n\n===Moments of the natural statistics===\nThe following moments can be easily computed using the [[exponential family#Moment generating function of the sufficient statistic|moment generating function of the sufficient statistic]]:{{citation needed|date=April 2013}}\n\n:<math>\\operatorname{E}(\\ln T)=\\psi\\left(\\alpha\\right) - \\ln\\beta,</math>\n\nwhere <math>\\psi\\left(\\alpha\\right)</math> is the [[digamma function]],\n\n:<math>\n\\begin{align}\n\\operatorname{E}(T) & =\\frac{\\alpha}{\\beta}, \\\\[5pt]\n\\operatorname{E}(TX) & =\\mu \\frac{\\alpha}{\\beta}, \\\\[5pt]\n\\operatorname{E}(TX^2) & =\\frac{1}{\\lambda} + \\mu^2 \\frac{\\alpha}{\\beta}.\n\\end{align}\n</math>\n\n===Scaling===\n\nIf <math> (X,T) \\sim \\mathrm{NormalGamma}(\\mu,\\lambda,\\alpha,\\beta), </math> then for any ''b'' > 0, (''bX'',''bT'') is distributed as{{citation needed|date=April 2013}} <math>{\\rm NormalGamma}(b\\mu, \\lambda, \\alpha, b^2\\beta).</math>{{dubious|date=April 2013}}\n\n== Posterior distribution of the parameters ==\nAssume that ''x'' is distributed according to a normal distribution with unknown mean <math>\\mu</math> and precision <math>\\tau</math>.\n\n:<math> x \\sim \\mathcal{N}(\\mu, \\tau^{-1}) </math>\nand that the prior distribution on <math>\\mu</math> and <math>\\tau</math>,  <math>(\\mu,\\tau)</math>, has a normal-gamma distribution\n\n:<math>\n(\\mu,\\tau)  \\sim \\text{NormalGamma}(\\mu_0,\\lambda_0,\\alpha_0,\\beta_0) ,\n</math>\n\nfor which the density {{pi}} satisfies\n:<math> \n\\pi(\\mu,\\tau) \\propto \\tau^{\\alpha_0-\\frac{1}{2}}\\,\\exp[-\\beta_0\\tau]\\,\\exp\\left[ -\\frac{\\lambda_0\\tau(\\mu-\\mu_0)^2} 2 \\right].\n</math>\n\nSuppose\n\n: <math>\nx_1,\\ldots,x_n \\mid \\mu,\\tau \\sim \\operatorname{{i.}{i.}{d.}} \\operatorname N\\left( \\mu,   \\tau^{-1} \\right),\n</math>\ni.e. the components of <math>\\mathbf X = (x_1,\\ldots,x_n)</math> are conditionally independent given <math>\\mu,\\tau</math> and the conditional distribution of each of them given <math> \\mu,\\tau</math> is normal with expected value <math>\\mu</math> and variance <math> 1 / \\tau. </math> The posterior distribution of <math>\\mu</math> and <math>\\tau</math> given this dataset <math> \\mathbb X</math> can be analytically determined by [[Bayes' theorem]].<ref>{{cite web |url=http://www.trinity.edu/cbrown/bayesweb/ |title=Archived copy |accessdate=2014-08-05 |deadurl=no |archiveurl=https://web.archive.org/web/20140807091855/http://www.trinity.edu/cbrown/bayesweb/ |archivedate=2014-08-07 |df= }}</ref> Explicitly,\n\n:<math>\\mathbf{P}(\\tau,\\mu \\mid \\mathbf{X}) \\propto \\mathbf{L}(\\mathbf{X} \\mid \\tau,\\mu) \\pi(\\tau,\\mu),</math>\n\nwhere <math>\\mathbf{L}</math> is the likelihood of the data given the parameters.\n\nSince the data are i.i.d, the likelihood of the entire dataset is equal to the product of the likelihoods of the individual data samples:\n\n:<math>\n\\mathbf{L}(\\mathbf{X} \\mid \\tau, \\mu) = \\prod_{i=1}^n \\mathbf{L}(x_i \\mid \\tau, \\mu).\n</math>\n\nThis expression can be simplified as follows:\n\n:<math>\n\\begin{align}\n\\mathbf{L}(\\mathbf{X} \\mid \\tau, \\mu) & \\propto \\prod_{i=1}^n \\tau^{1/2} \\exp\\left[\\frac{-\\tau}{2}(x_i-\\mu)^2\\right] \\\\[5pt]\n&  \\propto \\tau^{n/2} \\exp\\left[\\frac{-\\tau}{2}\\sum_{i=1}^n(x_i-\\mu)^2\\right] \\\\[5pt]\n&  \\propto \\tau^{n/2} \\exp\\left[\\frac{-\\tau}{2} \\sum_{i=1}^n(x_i-\\bar{x} +\\bar{x} -\\mu)^2 \\right] \\\\[5pt]\n&  \\propto \\tau^{n/2} \\exp\\left[\\frac{-\\tau} 2 \\sum_{i=1}^n \\left((x_i-\\bar{x})^2 + (\\bar{x} -\\mu)^2 \\right)\\right] \\\\[5pt]\n& \\propto \\tau^{n/2} \\exp\\left[\\frac{-\\tau}{2}\\left(n s + n(\\bar{x} -\\mu)^2\\right)\\right],\n\\end{align}\n</math>\n\nwhere <math>\\bar{x}= \\frac{1}{n}\\sum_{i=1}^n x_i</math>, the mean of the data samples, and <math>s= \\frac{1}{n} \\sum_{i=1}^n(x_i-\\bar{x})^2</math>, the sample variance.\n\nThe posterior distribution of the parameters is proportional to the prior times the likelihood.\n\n:<math>\n\\begin{align}\n\\mathbf{P}(\\tau, \\mu \\mid \\mathbf{X}) &\\propto \\mathbf{L}(\\mathbf{X} \\mid \\tau,\\mu) \\pi(\\tau,\\mu) \\\\\n&\\propto \\tau^{n/2} \\exp \\left[ \\frac{-\\tau}{2}\\left(n s + n(\\bar{x} -\\mu)^2\\right) \\right] \n \\tau^{\\alpha_0-\\frac{1}{2}}\\,\\exp[{-\\beta_0\\tau}]\\,\\exp\\left[-\\frac{\\lambda_0\\tau(\\mu-\\mu_0)^2}{2}\\right] \\\\ \n &\\propto \\tau^{\\frac{n}{2} + \\alpha_0 - \\frac{1}{2}}\\exp\\left[-\\tau \\left( \\frac{1}{2} n s + \\beta_0 \\right) \\right] \\exp\\left[- \\frac{\\tau}{2}\\left(\\lambda_0(\\mu-\\mu_0)^2 + n(\\bar{x} -\\mu)^2\\right)\\right] \n\\end{align}\n</math>\n\nThe final exponential term is simplified by completing the square.\n\n:<math>\n\\begin{align}\n\\lambda_0(\\mu-\\mu_0)^2 + n(\\bar{x} -\\mu)^2&=\\lambda_0 \\mu^2 - 2 \\lambda_0 \\mu \\mu_0 + \\lambda_0 \\mu_0^2 + n \\mu^2  - 2 n \\bar{x} \\mu + n \\bar{x}^2 \\\\\n&= (\\lambda_0 + n) \\mu^2 - 2(\\lambda_0 \\mu_0 + n \\bar{x}) \\mu + \\lambda_0 \\mu_0^2 +n \\bar{x}^2 \\\\\n&= (\\lambda_0 + n)( \\mu^2 - 2 \\frac{\\lambda_0 \\mu_0 + n \\bar{x}}{\\lambda_0 + n} \\mu ) + \\lambda_0 \\mu_0^2 +n \\bar{x}^2 \\\\\n&= (\\lambda_0 + n)\\left(\\mu - \\frac{\\lambda_0 \\mu_0 + n \\bar{x}}{\\lambda_0 + n} \\right) ^2 + \\lambda_0 \\mu_0^2 +n \\bar{x}^2 -  \\frac{\\left(\\lambda_0 \\mu_0 +n \\bar{x}\\right)^2} {\\lambda_0 + n} \\\\\n&= (\\lambda_0 + n)\\left(\\mu - \\frac{\\lambda_0 \\mu_0 + n \\bar{x}}{\\lambda_0 + n} \\right) ^2 + \\frac{\\lambda_0 n (\\bar{x} - \\mu_0 )^2}{\\lambda_0 +n}\n\\end{align}\n</math>\n\nOn inserting this back into the expression above,\n\n:<math>\n\\begin{align}\n\\mathbf{P}(\\tau, \\mu \\mid \\mathbf{X})  & \\propto \\tau^{\\frac{n}{2} + \\alpha_0 - \\frac{1}{2}} \\exp \\left[-\\tau \\left( \\frac{1}{2} n s  + \\beta_0 \\right) \\right] \\exp \\left[- \\frac{\\tau}{2} \\left( \\left(\\lambda_0 + n \\right) \\left(\\mu- \\frac{\\lambda_0 \\mu_0 + n \\bar{x}}{\\lambda_0 + n} \\right)^2 + \\frac{\\lambda_0 n (\\bar{x} - \\mu_0 )^2}{\\lambda_0 +n} \\right) \\right]\\\\\n& \\propto \\tau^{\\frac{n}{2} + \\alpha_0 - \\frac{1}{2}} \\exp \\left[-\\tau \\left( \\frac{1}{2} n s  + \\beta_0 + \\frac{\\lambda_0 n (\\bar{x} - \\mu_0 )^2}{2(\\lambda_0 +n)} \\right) \\right] \\exp \\left[- \\frac{\\tau}{2} \\left(\\lambda_0 + n \\right) \\left(\\mu- \\frac{\\lambda_0 \\mu_0 + n \\bar{x}}{\\lambda_0 + n} \\right)^2 \\right]\n\\end{align}\n</math>\n\nThis final expression is in exactly the same form as a Normal-Gamma distribution, i.e.,\n:<math>\n\\mathbf{P}(\\tau, \\mu \\mid \\mathbf{X}) = \\text{NormalGamma}\\left(\\frac{\\lambda_0 \\mu_0 + n \\bar{x}}{\\lambda_0 + n}, \\lambda_0 + n, \\alpha_0+\\frac{n}{2}, \\beta_0+ \\frac{1}{2}\\left(n s + \\frac{\\lambda_0 n (\\bar{x} - \\mu_0 )^2}{\\lambda_0 +n} \\right) \\right)\n</math>\n\n=== Interpretation of parameters ===\n\nThe interpretation of parameters in terms of pseudo-observations is as follows:\n*The new mean takes a weighted average of the old pseudo-mean and the observed mean, weighted by the number of associated (pseudo-)observations.\n*The precision was estimated from <math>2\\alpha</math> pseudo-observations (i.e. possibly a different number of pseudo-observations, to allow the variance of the mean and precision to be controlled separately) with sample mean <math>\\mu</math> and sample variance <math>\\frac{\\beta}{\\alpha}</math> (i.e. with sum of [[squared deviations]] <math>2\\beta</math>).\n*The posterior updates the number of pseudo-observations (<math>\\lambda_{0}</math>) simply by adding up the corresponding number of new observations (<math>n</math>).\n*The new sum of squared deviations is computed by adding the previous respective sums of squared deviations.  However, a third \"interaction term\" is needed because the two sets of squared deviations were computed with respect to different means, and hence the sum of the two underestimates the actual total squared deviation.\n\nAs a consequence, if one has a prior mean of <math>\\mu_0</math> from <math> n_\\mu </math> samples and a prior precision of <math> \\tau_0 </math> from <math>n_\\tau</math> samples, the prior distribution over <math> \\mu </math> and <math> \\tau </math> is\n:<math>\n\\mathbf{P}(\\tau,\\mu \\mid \\mathbf{X}) = \\operatorname{NormalGamma} \\left(\\mu_0, n_\\mu , \\frac{n_\\tau}{2}, \\frac{n_\\tau}{2 \\tau_0}\\right)\n</math>\n\nand after observing <math>n</math> samples with mean <math>\\mu</math> and variance <math>s</math>, the posterior probability is\n:<math>\n\\mathbf{P}(\\tau,\\mu \\mid \\mathbf{X}) = \\text{NormalGamma}\\left( \\frac{n_\\mu \\mu_0 + n \\mu}{n_\\mu +n}, n_\\mu +n ,\\frac{1}{2}(n_\\tau+n), \\frac{1}{2}\\left(\\frac{n_\\tau}{\\tau_0} + n s + \\frac{n_\\mu n (\\mu-\\mu_0)^2}{n_\\mu+n}\\right) \\right)\n</math>\n\nNote that in some programming languages, such as [[Matlab]], the gamma distribution is implemented with the inverse definition of <math>\\beta</math>, so the fourth argument of the Normal-Gamma distribution is <math> 2 \\tau_0 /n_\\tau</math>.\n\n== Generating normal-gamma random variates ==\nGeneration of random variates is straightforward:\n# Sample <math>\\tau</math> from a gamma distribution with parameters <math>\\alpha</math> and <math>\\beta</math>\n# Sample <math>x</math> from a normal distribution with mean <math>\\mu</math> and variance <math>1/(\\lambda \\tau)</math>\n\n== Related distributions ==\n* The [[normal-inverse-gamma distribution]] is essentially the same distribution parameterized by variance rather than precision\n* The [[normal-exponential-gamma distribution]]\n\n==Notes==\n{{reflist}}\n\n== References ==\n*  Bernardo, J.M.; Smith, A.F.M. (1993) ''Bayesian Theory'', Wiley. {{ISBN|0-471-49464-X}}\n*  Dearden et al. [http://www.aaai.org/Papers/AAAI/1998/AAAI98-108.pdf \"Bayesian Q-learning\"], ''Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI-98)'', July 26–30, 1998, Madison, Wisconsin, USA.\n\n{{ProbDistributions|multivariate}}\n\n{{DEFAULTSORT:Normal-gamma distribution}}\n[[Category:Multivariate continuous distributions]]\n[[Category:Conjugate prior distributions]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Normal-inverse-gamma distribution",
      "url": "https://en.wikipedia.org/wiki/Normal-inverse-gamma_distribution",
      "text": "{{Probability distribution |\n  name       =normal-inverse-gamma|\n  type       =density|\n  pdf_image  =[[File:Normal-inverse-gamma.svg|565px|Probability density function of normal-inverse-gamma distribution for ''α'' = 1.0, 2.0 and 4.0, plotted in shifted and scaled coordinates.]]|\n  cdf_image  =|\n  parameters =<math>\\mu\\,</math> [[location parameter|location]] ([[real number|real]])<br /><math>\\lambda > 0\\,</math> (real)<br /><math>\\alpha > 0\\,</math> (real)<br /><math>\\beta > 0\\,</math> (real)|\n  support    =<math>x \\in (-\\infty, \\infty)\\,\\!, \\; \\sigma^2 \\in (0,\\infty)</math>|\n  pdf        =<math>\n \\frac{ \\sqrt{ \\lambda } }{ \\sqrt{ 2 \\pi \\sigma^2 }}\n\\frac{ \\beta^\\alpha }{ \\Gamma( \\alpha ) } \n\\left( \\frac{1}{\\sigma^2 } \\right)^{\\alpha + 1}\n\\exp \\left( -\\frac { 2\\beta + \\lambda (x - \\mu)^2} {2\\sigma^2}\\right) </math>\n|\n  cdf        =|\n  mean       =<math>\\operatorname{E}[x] = \\mu</math><br />\n<math>\\operatorname{E}[\\sigma^2] = \\frac{\\beta}{\\alpha - 1}</math>, for <math>\\alpha >1</math><br />|\n  median     =|\n  mode       =<math> x = \\mu \\;  \\textrm{(univariate)}, x = \\boldsymbol{\\mu} \\;  \\textrm{(multivariate)} </math> <br />\n<math> \\sigma^2 = \\frac{\\beta}{\\alpha + 1 + 1/2} \\;  \\textrm{(univariate)}, \\sigma^2 = \\frac{\\beta}{\\alpha + 1 + k/2} \\;  \\textrm{(multivariate)} </math>|\n  variance   =<math>\\operatorname{Var}[x] = \\frac{\\beta}{(\\alpha -1)\\lambda}</math>, for <math>\\alpha > 1</math><br/>\n<math>\\operatorname{Var}[\\sigma^2] = \\frac{\\beta^2}{(\\alpha -1)^2(\\alpha -2)}</math>, for <math>\\alpha > 2</math><br />\n<math>\\operatorname{Cov}[x, \\sigma^2] = 0</math>, for <math>\\alpha > 1</math>|\n  skewness   =|\n  kurtosis   =|\n  entropy    =|\n  mgf        =|\n  char       =|\n}}\nIn [[probability theory]] and [[statistics]], the '''normal-inverse-gamma distribution''' (or '''Gaussian-inverse-gamma distribution''') is a four-parameter family of multivariate continuous [[probability distribution]]s. It is the [[conjugate prior]] of a [[normal distribution]] with unknown [[mean]] and [[variance]].\n\n==Definition==\nSuppose\n\n:<math>  x \\mid \\sigma^2, \\mu, \\lambda\\sim \\mathrm{N}(\\mu,\\sigma^2 / \\lambda) \\,\\! </math> \nhas a [[normal distribution]] with [[mean]] <math> \\mu</math> and [[variance]] <math> \\sigma^2 / \\lambda</math>, where\n\n:<math>\\sigma^2\\mid\\alpha, \\beta \\sim \\Gamma^{-1}(\\alpha,\\beta) \\!</math> \nhas an [[inverse gamma distribution]]. Then <math>(x,\\sigma^2) </math> \nhas a normal-inverse-gamma distribution, denoted as\n:<math> (x,\\sigma^2) \\sim \\text{N-}\\Gamma^{-1}(\\mu,\\lambda,\\alpha,\\beta) \\! .\n</math> \n\n(<math>\\text{NIG}</math> is also used instead of <math>\\text{N-}\\Gamma^{-1}.</math>)\n\nIn a multivariate form of the normal-inverse-gamma distribution, <math>  \\mathbf{x} \\mid \\sigma^2, \\boldsymbol{\\mu}, \\mathbf{V}^{-1}\\sim \\mathrm{N}(\\boldsymbol{\\mu},\\sigma^2 \\mathbf{V}^{-1}) \\,\\! </math> – that is, conditional on <math> \\sigma^2 </math>, <math>  \\mathbf{x} </math> is a <math> k \\times 1 </math> random vector that follows the [[multivariate normal distribution]] with mean <math> \\boldsymbol{\\mu} </math> and [[covariance matrix | covariance]] <math> \\sigma^2\\mathbf{V}^{-1}</math> – while, as in the univariate case, <math>\\sigma^2\\mid\\alpha, \\beta \\sim \\Gamma^{-1}(\\alpha,\\beta) \\!</math>.\n\n==Characterization==\n\n===Probability density function===\n\n: <math>f(x,\\sigma^2\\mid\\mu,\\lambda,\\alpha,\\beta) =  \\frac {\\sqrt{\\lambda}} {\\sigma\\sqrt{2\\pi} } \\, \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\, \\left( \\frac{1}{\\sigma^2} \\right)^{\\alpha + 1}   \\exp \\left( -\\frac { 2\\beta + \\lambda(x - \\mu)^2} {2\\sigma^2}  \\right) </math>\n\nFor the multivariate form where <math>  \\mathbf{x} </math> is a <math> k \\times 1 </math> random vector, \n\n: <math>f(\\mathbf{x},\\sigma^2\\mid\\mu,\\mathbf{V}^{-1},\\alpha,\\beta) =  |\\mathbf{V}|^{-1/2} {(2\\pi)^{-k/2} } \\, \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\, \\left( \\frac{1}{\\sigma^2} \\right)^{\\alpha + 1 + k/2}   \\exp \\left( -\\frac { 2\\beta + (\\mathbf{x} - \\boldsymbol{\\mu})' \\mathbf{V}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})} {2\\sigma^2}  \\right). </math>\n\nwhere <math>|\\mathbf{V}|</math> is the [[determinant]] of the <math> k \\times k </math> [[matrix (mathematics) | matrix]] <math>\\mathbf{V}</math>. Note how this last equation reduces to the first form if <math>k = 1</math> so that <math>\\mathbf{x}, \\mathbf{V}, \\boldsymbol{\\mu}</math> are [[scalar (mathematics)| scalars]].\n\n==== Alternative parameterization ====\nIt is also possible to let <math> \\gamma = 1 / \\lambda</math> in which case the pdf becomes\n\n: <math>f(x,\\sigma^2\\mid\\mu,\\gamma,\\alpha,\\beta) =  \\frac {1} {\\sigma\\sqrt{2\\pi\\gamma} } \\, \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\, \\left( \\frac{1}{\\sigma^2} \\right)^{\\alpha + 1}   \\exp \\left( -\\frac{2\\gamma\\beta + (x - \\mu)^2}{2\\gamma \\sigma^2} \\right)</math>\n\nIn the multivariate form, the corresponding change would be to regard the covariance matrix <math>\\mathbf{V}</math> instead of its [[invertible matrix | inverse]] <math>\\mathbf{V}^{-1}</math> as a parameter.\n\n===Cumulative distribution function===\n: <math>F(x,\\sigma^2\\mid\\mu,\\lambda,\\alpha,\\beta) =  \\frac{e^{-\\frac{\\beta}{\\sigma^2}} \\left(\\frac{\\beta }{\\sigma ^2}\\right)^\\alpha\n   \\left(\\operatorname{erf}\\left(\\frac{\\sqrt{\\lambda} (x-\\mu )}{\\sqrt{2} \\sigma }\\right)+1\\right)}{2\n   \\sigma^2 \\Gamma (\\alpha)} </math>\n\n==Properties==\n\n===Marginal distributions===\n\nGiven <math> (x,\\sigma^2) \\sim \\text{N-}\\Gamma^{-1}(\\mu,\\lambda,\\alpha,\\beta) \\! .\n</math> as above, <math>\\sigma^2</math> by itself follows an [[inverse gamma distribution]]:\n\n:<math>\\sigma^2 \\sim \\Gamma^{-1}(\\alpha,\\beta) \\!</math>\n\nwhile <math> \\sqrt{\\frac{\\alpha\\lambda}{\\beta}} (x - \\mu) </math> follows a [[Student's t-distribution | t distribution]] with <math> 2 \\alpha </math> degrees of freedom.\n\nIn the multivariate case, the marginal distribution of <math>\\mathbf{x}</math> is a [[multivariate Student distribution | multivariate t distribution]]:\n\n:<math>\\mathbf{x} \\sim t_{2\\alpha}(\\boldsymbol{\\mu}, \\frac{\\beta}{\\alpha} \\mathbf{V}^{-1}) \\!</math>\n\n===Summation===\n\n===Scaling===\n\n===Exponential family===\n\n===Information entropy===\n\n===Kullback–Leibler divergence===\n\n== Maximum likelihood estimation ==\n\n{{Empty section|date=July 2010}}\n\n== Posterior distribution of the parameters ==\nSee the articles on [[normal-gamma distribution]] and [[conjugate prior]].\n\n== Interpretation of the parameters ==\nSee the articles on [[normal-gamma distribution]] and [[conjugate prior]].\n\n== Generating normal-inverse-gamma random variates ==\nGeneration of random variates is straightforward:\n# Sample <math>\\sigma^2</math> from an inverse gamma distribution with parameters <math>\\alpha</math> and <math>\\beta</math>\n# Sample <math>x</math> from a normal distribution with mean <math>\\mu</math> and variance <math>\\sigma^2/\\lambda</math>\n\n== Related distributions ==\n* The [[normal-gamma distribution]] is the same distribution parameterized by [[precision (statistics)|precision]] rather than [[variance]]\n* A generalization of this distribution which allows for a multivariate mean and a completely unknown positive-definite covariance matrix <math>\\sigma^2 \\mathbf{V}</math> (whereas in the multivariate inverse-gamma distribution the covariance matrix is regarded as known up to the scale factor <math>\\sigma^2</math>) is the [[normal-inverse-Wishart distribution]]\n\n== References ==\n*  Denison, David G. T. ; Holmes, Christopher C.; Mallick, Bani K.; Smith, Adrian F. M. (2002) ''Bayesian Methods for Nonlinear Classification and Regression'', Wiley. {{ISBN|0471490369}}\n*  Koch, Karl-Rudolf (2007) ''Introduction to Bayesian Statistics'' (2nd Edition), Springer. {{ISBN|354072723X}}\n\n{{ProbDistributions|multivariate}}\n\n[[Category:Continuous distributions]]\n[[Category:Multivariate continuous distributions]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Normal-inverse-Wishart distribution",
      "url": "https://en.wikipedia.org/wiki/Normal-inverse-Wishart_distribution",
      "text": "{{Probability distribution |\n  name       =normal-inverse-Wishart|\n  type       =density|\n  pdf_image  =|\n  cdf_image  =|\n  notation =<math>(\\boldsymbol\\mu,\\boldsymbol\\Sigma) \\sim \\mathrm{NIW}(\\boldsymbol\\mu_0,\\lambda,\\boldsymbol\\Psi,\\nu)</math>|\n  parameters =<math>\\boldsymbol\\mu_0\\in\\mathbb{R}^D\\,</math> [[location parameter|location]] (vector of [[real number|real]])<br /><math>\\lambda > 0\\,</math> (real)<br /><math>\\boldsymbol\\Psi \\in\\mathbb{R}^{D\\times D}</math> inverse scale matrix ([[positive-definite matrix|pos. def.]])<br /><math>\\nu > D-1\\,</math> (real)|\n  support    =<math>\\boldsymbol\\mu\\in\\mathbb{R}^D ; \\boldsymbol\\Sigma \\in\\mathbb{R}^{D\\times D}</math> [[covariance matrix]] ([[positive-definite matrix|pos. def.]])|\n  pdf        =<math>f(\\boldsymbol\\mu,\\boldsymbol\\Sigma|\\boldsymbol\\mu_0,\\lambda,\\boldsymbol\\Psi,\\nu) = \\mathcal{N}(\\boldsymbol\\mu|\\boldsymbol\\mu_0,\\tfrac{1}{\\lambda}\\boldsymbol\\Sigma)\\ \\mathcal{W}^{-1}(\\boldsymbol\\Sigma|\\boldsymbol\\Psi,\\nu)</math>|\n  cdf        =|\n  mean       =|\n  median     =|\n  mode       =|\n  variance   =|\n  skewness   =|\n  kurtosis   =|\n  entropy    =|\n  mgf        =|\n  char       =|\n}}\nIn [[probability theory]] and [[statistics]], the '''normal-inverse-Wishart distribution''' (or '''Gaussian-inverse-Wishart distribution''') is a multivariate four-parameter family of continuous [[probability distribution]]s. It is the [[conjugate prior]] of a [[multivariate normal distribution]] with unknown [[mean]] and [[covariance matrix]] (the inverse of the [[precision matrix]]).<ref name=\"murphy\">Murphy, Kevin P. (2007). \"Conjugate Bayesian analysis of the Gaussian distribution.\" [http://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf]</ref>\n\n==Definition==\nSuppose\n\n:<math>  \\boldsymbol\\mu|\\boldsymbol\\mu_0,\\lambda,\\boldsymbol\\Sigma \\sim \\mathcal{N}\\left(\\boldsymbol\\mu\\Big|\\boldsymbol\\mu_0,\\frac{1}{\\lambda}\\boldsymbol\\Sigma\\right)</math>\nhas a [[multivariate normal distribution]] with [[mean]] <math>\\boldsymbol\\mu_0</math> and [[covariance matrix]] <math>\\tfrac{1}{\\lambda}\\boldsymbol\\Sigma</math>, where\n\n:<math>\\boldsymbol\\Sigma|\\boldsymbol\\Psi,\\nu \\sim \\mathcal{W}^{-1}(\\boldsymbol\\Sigma|\\boldsymbol\\Psi,\\nu)</math>\nhas an [[inverse Wishart distribution]]. Then <math>(\\boldsymbol\\mu,\\boldsymbol\\Sigma) </math>\nhas a normal-inverse-Wishart distribution, denoted as\n:<math> (\\boldsymbol\\mu,\\boldsymbol\\Sigma) \\sim \\mathrm{NIW}(\\boldsymbol\\mu_0,\\lambda,\\boldsymbol\\Psi,\\nu)  .\n</math>\n\n==Characterization==\n\n===Probability density function===\n\n: <math>f(\\boldsymbol\\mu,\\boldsymbol\\Sigma|\\boldsymbol\\mu_0,\\lambda,\\boldsymbol\\Psi,\\nu) = \\mathcal{N}\\left(\\boldsymbol\\mu\\Big|\\boldsymbol\\mu_0,\\frac{1}{\\lambda}\\boldsymbol\\Sigma\\right) \\mathcal{W}^{-1}(\\boldsymbol\\Sigma|\\boldsymbol\\Psi,\\nu)</math>\n\n==Properties==\n\n===Scaling===\n\n===Marginal distributions===\nBy construction, the [[marginal distribution]] over <math>\\boldsymbol\\Sigma</math> is an [[inverse Wishart distribution]], and the [[conditional distribution]] over <math>\\boldsymbol\\mu</math> given <math>\\boldsymbol\\Sigma</math> is a [[multivariate normal distribution]].  The [[marginal distribution]] over <math>\\boldsymbol\\mu</math> is a [[multivariate t-distribution]].\n\n== Posterior distribution of the parameters ==\nSuppose the sampling density is a multivariate normal distribution\n\n:<math>\\boldsymbol{y_i}|\\boldsymbol\\mu,\\boldsymbol\\Sigma \\sim \\mathcal{N}_p(\\boldsymbol\\mu,\\boldsymbol\\Sigma)</math>\n\nwhere <math>\\boldsymbol{y}</math> is an <math>n\\times p</math> matrix and <math>\\boldsymbol{y_i}</math> (of length <math>p</math>) is row <math>i</math> of the matrix .\n\nWith the mean and covariance matrix of the sampling distribution is unknown, we can place a Normal-Inverse-Wishart prior on the mean and covariance parameters jointly\n\n:<math>\n(\\boldsymbol\\mu,\\boldsymbol\\Sigma) \\sim \\mathrm{NIW}(\\boldsymbol\\mu_0,\\lambda,\\boldsymbol\\Psi,\\nu).\n</math>\n\nThe resulting posterior distribution for the mean and covariance matrix will also be a Normal-Inverse-Wishart \n\n:<math>\n(\\boldsymbol\\mu,\\boldsymbol\\Sigma|y) \\sim \\mathrm{NIW}(\\boldsymbol\\mu_n,\\lambda_n,\\boldsymbol\\Psi_n,\\nu_n),\n</math>\n\nwhere\n\n:<math>\n\\boldsymbol\\mu_n = \\frac{\\lambda\\boldsymbol\\mu_0 + n \\bar{\\boldsymbol y}}{\\lambda+n}\n</math>\n\n:<math>\n\\lambda_n = \\lambda + n\n</math>\n\n:<math>\n\\nu_n = \\nu + n\n</math>\n\n:<math>\n\\boldsymbol\\Psi_n = \\boldsymbol{\\Psi + S} +\\frac{\\lambda n}{\\lambda+n} \n(\\boldsymbol{\\bar{y}-\\mu_0})(\\boldsymbol{\\bar{y}-\\mu_0})^T\n~~~\\mathrm{ with, }~~\\boldsymbol{S}= \\sum_{i=1}^{n} (\\boldsymbol{y_i-\\bar{y}})(\\boldsymbol{y_i-\\bar{y}})^T\n</math>.\n\n\nTo sample from the joint posterior of <math>(\\boldsymbol\\mu,\\boldsymbol\\Sigma)</math>, one simply draws samples from <math>\\boldsymbol\\Sigma|\\boldsymbol y \\sim \\mathcal{W}^{-1}(\\boldsymbol\\Psi_n,\\nu_n)</math>, then draw <math>\\boldsymbol\\mu | \\boldsymbol{\\Sigma,y} \\sim \\mathcal{N}_p(\\boldsymbol\\mu_n,\\boldsymbol\\Sigma/\\lambda_n)</math>. To draw from the posterior predictive of a new observation, draw <math>\\boldsymbol\\tilde{y}|\\boldsymbol{\\mu,\\Sigma,y} \\sim \\mathcal{N}_p(\\boldsymbol\\mu,\\boldsymbol\\Sigma)</math> , given the already drawn values of <math>\\boldsymbol\\mu</math> and <math>\\boldsymbol\\Sigma</math>.<ref>Gelman, Andrew, et al. Bayesian data analysis. Vol. 2, p.73. Boca Raton, FL, USA: Chapman & Hall/CRC, 2014.</ref>\n\n== Generating normal-inverse-Wishart random variates ==\nGeneration of random variates is straightforward:\n# Sample <math>\\boldsymbol\\Sigma</math> from an [[inverse Wishart distribution]] with parameters <math>\\boldsymbol\\Psi</math> and <math>\\nu</math>\n# Sample <math>\\boldsymbol\\mu</math> from a [[multivariate normal distribution]] with mean <math>\\boldsymbol\\mu_0</math> and variance <math>\\boldsymbol \\tfrac{1}{\\lambda} \\boldsymbol\\Sigma</math>\n\n== Related distributions ==\n* The [[normal-Wishart distribution]] is essentially the same distribution parameterized by precision rather than variance.  If <math> (\\boldsymbol\\mu,\\boldsymbol\\Sigma) \\sim \\mathrm{NIW}(\\boldsymbol\\mu_0,\\lambda,\\boldsymbol\\Psi,\\nu)</math> then <math>(\\boldsymbol\\mu,\\boldsymbol\\Sigma^{-1}) \\sim \\mathrm{NW}(\\boldsymbol\\mu_0,\\lambda,\\boldsymbol\\Psi^{-1},\\nu)</math> .\n* The [[normal-inverse-gamma distribution]] is the one-dimensional equivalent.\n* The [[multivariate normal distribution]] and [[inverse Wishart distribution]] are the component distributions out of which this distribution is made.\n\n==Notes==\n{{reflist}}\n\n== References ==\n* Bishop, Christopher M. (2006). ''Pattern Recognition and Machine Learning.'' Springer Science+Business Media.\n* Murphy, Kevin P. (2007). \"Conjugate Bayesian analysis of the Gaussian distribution.\" [http://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf]\n\n{{ProbDistributions|multivariate}}\n\n[[Category:Multivariate continuous distributions]]\n[[Category:Conjugate prior distributions]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Normal-Wishart distribution",
      "url": "https://en.wikipedia.org/wiki/Normal-Wishart_distribution",
      "text": "{{Probability distribution |\n  name       =Normal-Wishart|\n  type       =density|\n  pdf_image  =|\n  cdf_image  =|\n  notation =<math> (\\boldsymbol\\mu,\\boldsymbol\\Lambda) \\sim \\mathrm{NW}(\\boldsymbol\\mu_0,\\lambda,\\mathbf{W},\\nu)</math>|\n  parameters =<math>\\boldsymbol\\mu_0\\in\\mathbb{R}^D\\,</math> [[location parameter|location]] (vector of [[real number|real]])<br /><math>\\lambda > 0\\,</math> (real)<br /><math>\\mathbf{W} \\in\\mathbb{R}^{D\\times D}</math> scale matrix ([[positive-definite matrix|pos. def.]])<br /><math>\\nu > D-1\\,</math> (real)|\n  support    =<math>\\boldsymbol\\mu\\in\\mathbb{R}^D ; \\boldsymbol\\Lambda \\in\\mathbb{R}^{D\\times D}</math> [[covariance matrix]] ([[positive-definite matrix|pos. def.]])|\n  pdf        =<math>f(\\boldsymbol\\mu,\\boldsymbol\\Lambda|\\boldsymbol\\mu_0,\\lambda,\\mathbf{W},\\nu) = \\mathcal{N}(\\boldsymbol\\mu|\\boldsymbol\\mu_0,(\\lambda\\boldsymbol\\Lambda)^{-1})\\ \\mathcal{W}(\\boldsymbol\\Lambda|\\mathbf{W},\\nu)</math>|\n  cdf        =|\n  mean       =|\n  median     =|\n  mode       =|\n  variance   =|\n  skewness   =|\n  kurtosis   =|\n  entropy    =|\n  mgf        =|\n  char       =|\n}}\nIn [[probability theory]] and [[statistics]], the '''normal-Wishart distribution''' (or '''Gaussian-Wishart distribution''') is a multivariate four-parameter family of continuous [[probability distribution]]s. It is the [[conjugate prior]] of a [[multivariate normal distribution]] with unknown [[mean]] and [[precision matrix]] (the inverse of the [[covariance matrix]]).<ref name=\"bishop\">Bishop, Christopher M. (2006). ''Pattern Recognition and Machine Learning.'' Springer Science+Business Media. Page 690.</ref>\n\n==Definition==\nSuppose\n\n:<math>  \\boldsymbol\\mu|\\boldsymbol\\mu_0,\\lambda,\\boldsymbol\\Lambda \\sim \\mathcal{N}(\\boldsymbol\\mu_0,(\\lambda\\boldsymbol\\Lambda)^{-1}) </math>\nhas a [[multivariate normal distribution]] with [[mean]] <math>\\boldsymbol\\mu_0</math> and [[covariance matrix]] <math>(\\lambda\\boldsymbol\\Lambda)^{-1}</math>, where\n\n:<math>\\boldsymbol\\Lambda|\\mathbf{W},\\nu \\sim \\mathcal{W}(\\boldsymbol\\Lambda|\\mathbf{W},\\nu)</math>\nhas a [[Wishart distribution]]. Then <math>(\\boldsymbol\\mu,\\boldsymbol\\Lambda) </math>\nhas a normal-Wishart distribution, denoted as\n:<math> (\\boldsymbol\\mu,\\boldsymbol\\Lambda) \\sim \\mathrm{NW}(\\boldsymbol\\mu_0,\\lambda,\\mathbf{W},\\nu) .\n</math>\n\n==Characterization==\n\n===Probability density function===\n\n: <math>f(\\boldsymbol\\mu,\\boldsymbol\\Lambda|\\boldsymbol\\mu_0,\\lambda,\\mathbf{W},\\nu) = \\mathcal{N}(\\boldsymbol\\mu|\\boldsymbol\\mu_0,(\\lambda\\boldsymbol\\Lambda)^{-1})\\ \\mathcal{W}(\\boldsymbol\\Lambda|\\mathbf{W},\\nu)</math>\n\n==Properties==\n\n===Scaling===\n\n===Marginal distributions===\nBy construction, the [[marginal distribution]] over <math>\\boldsymbol\\Lambda</math> is a [[Wishart distribution]], and the [[conditional distribution]] over <math>\\boldsymbol\\mu</math> given <math>\\boldsymbol\\Lambda</math> is a [[multivariate normal distribution]].  The [[marginal distribution]] over <math>\\boldsymbol\\mu</math> is a [[multivariate t-distribution]].\n\n== Posterior distribution of the parameters ==\nAfter making <math>n</math> observations <math>\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_n</math>, the posterior distribution of the parameters is\n:<math>(\\boldsymbol\\mu,\\boldsymbol\\Lambda) \\sim \\mathrm{NW}(\\boldsymbol\\mu_n,\\lambda_n,\\mathbf{W}_n,\\nu_n),</math>\nwhere\n:<math>\\lambda_n = \\lambda + n,</math>\n:<math>\\boldsymbol\\mu_n = \\frac{\\lambda \\boldsymbol\\mu_0 + n\\boldsymbol{\\bar{x}}}{\\lambda + n},</math>\n:<math>\\nu_n = \\nu + n,</math>\n:<math>\\mathbf{W}_n^{-1} = \\mathbf{W}^{-1} + \\sum_{i=1}^n (\\boldsymbol{x}_i - \\boldsymbol{\\bar{x}})(\\boldsymbol{x}_i - \\boldsymbol{\\bar{x}})^T + \\frac{n \\lambda}{n + \\lambda} (\\boldsymbol{\\bar{x}} - \\boldsymbol\\mu_0)(\\boldsymbol{\\bar{x}} - \\boldsymbol\\mu_0)^T.</math><ref>Cross Validated, https://stats.stackexchange.com/q/324925</ref>\n\n== Generating normal-Wishart random variates ==\nGeneration of random variates is straightforward:\n# Sample <math>\\boldsymbol\\Lambda</math> from a [[Wishart distribution]] with parameters <math>\\mathbf{W}</math> and <math>\\nu</math>\n# Sample <math>\\boldsymbol\\mu</math> from a [[multivariate normal distribution]] with mean <math>\\boldsymbol\\mu_0</math> and variance <math>(\\lambda\\boldsymbol\\Lambda)^{-1}</math>\n\n== Related distributions ==\n* The [[normal-inverse Wishart distribution]] is essentially the same distribution parameterized by variance rather than precision.\n* The [[normal-gamma distribution]] is the one-dimensional equivalent.\n* The [[multivariate normal distribution]] and [[Wishart distribution]] are the component distributions out of which this distribution is made.\n\n==Notes==\n{{reflist}}\n\n== References ==\n* Bishop, Christopher M. (2006). ''Pattern Recognition and Machine Learning.'' Springer Science+Business Media.\n\n{{ProbDistributions|multivariate}}\n\n{{DEFAULTSORT:Normal-Wishart Distribution}}\n[[Category:Multivariate continuous distributions]]\n[[Category:Conjugate prior distributions]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Normally distributed and uncorrelated does not imply independent",
      "url": "https://en.wikipedia.org/wiki/Normally_distributed_and_uncorrelated_does_not_imply_independent",
      "text": "In [[probability theory]], although simple examples illustrate that [[Pearson product-moment correlation coefficient|linear uncorrelatedness]] of two random variables does not in general imply their [[statistical independence|independence]], it is sometimes mistakenly thought that it does imply that when the two random variables are [[normal distribution|normally distributed]]. This article demonstrates that assumption of normal distributions does not have that consequence, although the [[multivariate normal distribution]], including the [[bivariate normal distribution]], does.\n\nTo say that the pair <math>(X,Y)</math> of random variables has a bivariate normal distribution means that every [[linear combination]] <math>aX+bY</math> of <math>X</math> and <math>Y</math> for constant (i.e. not random) coefficients <math>a</math> and <math>b</math> has a univariate normal distribution. In that case, if <math>X</math> and <math>Y</math> are uncorrelated then they are independent.<ref>{{cite book |title=Probability and Statistical Inference |year=2001 |last1=Hogg |first1=Robert |authorlink1=Robert V. Hogg |last2=Tanis |first2=Elliot | authorlink2=Elliot Tanis |edition=6th |chapter=Chapter 5.4 The Bivariate Normal Distribution |pages=258–259 |isbn=0130272949}}</ref> However, it is possible for two random variables <math>X</math> and <math>Y</math> to be so distributed jointly that each one alone is marginally normally distributed, and they are uncorrelated, but they are not independent; examples are given below.\n\n==Examples==\n=== A symmetric example ===\n\n[[File:uncorrelated sym.png|thumb|alt=Two normally distributed, uncorrelated but dependent variables.|Joint range of <math>X</math> and <math>Y</math>. Darker indicates higher value of the density function.]]\n\nSuppose <math>X</math> has a normal distribution with [[expected value]] 0 and variance 1. Let <math>W</math> have the [[Rademacher distribution]], so that <math>W=1</math> or <math>W=-1</math>, each with probability 1/2, and assume <math>W</math> is independent of <math>X</math>.  Let <math>Y=WX</math>. Then \n\n* <math>X</math> and <math>Y</math> are uncorrelated;\n* both have the same normal distribution; and\n* <math>X</math> and <math>Y</math> are not independent.<ref>[http://www.math.uiuc.edu/~r-ash/Stat/StatLec21-25.pdf UIUC, Lecture 21. ''The Multivariate Normal Distribution''], 21.6:\"Individually Gaussian Versus Jointly Gaussian\".</ref>\n\nTo see that <math>X</math> and <math>Y</math> are uncorrelated, one may consider the [[covariance]] <math>\\operatorname{cov}(X,Y)</math>: by definition, it is\n\n: <math> \\operatorname{cov}(X,Y) = \\operatorname E(XY) - \\operatorname E(X)\\operatorname E(Y).</math>\n\nThen by definition of the random variables <math>X</math>, <math>Y</math>, and <math>W</math>, and the independence of <math>W</math> from <math>X</math>, one has\n\n: <math> \\operatorname{cov}(X,Y) = \\operatorname E(XY) - 0 = \\operatorname E(X^2W)=\\operatorname E(X^2)\\operatorname E(W)=\\operatorname E(X^2) \\cdot 0=0.</math>\n\nTo see that <math>Y</math> has the same normal distribution as <math>X</math>, consider\n\n: <math>\\begin{align}\n\\Pr(Y \\le x) & {} = \\operatorname E(\\Pr(Y \\le x\\mid W)) \\\\\n& {} = \\Pr(X \\le x)\\Pr(W = 1) + \\Pr(-X\\le x)\\Pr(W = -1) \\\\\n& {} = \\Phi(x) \\cdot\\frac12 + \\Phi(x)\\cdot\\frac12\n\\end{align}</math>\n\n(since <math>X</math> and <math>-X</math> both have the same normal distribution), where <math>\\Phi(x)</math> is the [[cumulative distribution function]] of the normal distribution..\n\nTo see that <math>X</math> and <math>Y</math> are not independent, observe that <math>|Y|=|X|</math> or that <math>\\operatorname{Pr}(Y > 1 | X = 1/2) = \\operatorname{Pr}(X > 1 | X = 1/2) = 0</math>.\n\nFinally, the distribution of the simple linear combination <math>X+Y</math> concentrates positive probability at 0: <math>\\operatorname{Pr}(X+Y=0) = 1/2</math>.  Therefore, the random variable <math>X+Y</math> is not normally distributed, and so also <math>X</math> and <math>Y</math> are not jointly normally distributed (by the definition above).\n\n=== An asymmetric example ===\n\n[[File:Uncorrelated asym.png|thumb|The joint density of <math>X</math> and <math>Y</math>. Darker indicates a higher value of the density.]]\n\nSuppose <math>X</math> has a normal distribution with [[expected value]] 0 and variance 1.  Let\n\n: <math>Y=\\left\\{\\begin{matrix} X & \\text{if } \\left|X\\right| \\leq c \\\\\n-X & \\text{if } \\left|X\\right|>c \\end{matrix}\\right.</math>\n\nwhere <math>c</math> is a positive number to be specified below.  If <math>c</math> is very small, then the [[correlation]] <math>\\operatorname{corr}(X,Y)</math> is near <math>-1</math> if <math>c</math> is very large, then <math>\\operatorname{corr}(X,Y)</math> is near 1.  Since the correlation is a [[continuous function]] of <math>c</math>, the [[intermediate value theorem]] implies there is some particular value of <math>c</math> that makes the correlation&nbsp;0.  That value is approximately&nbsp;1.54.  In that case, <math>X</math> and <math>Y</math> are uncorrelated, but they are clearly not independent, since <math>X</math> completely determines <math>Y</math>.\n\nTo see that <math>Y</math> is normally distributed&mdash;indeed, that its distribution is the same as that of <math>X</math> &mdash;one may compute its [[cumulative distribution function]]:\n\n:<math>\\begin{align}\\Pr(Y \\leq x) &= \\Pr(\\{|X| \\leq c\\text{ and }X \\leq x\\}\\text{ or }\\{|X|>c\\text{ and }-X \\leq x\\})\\\\\n&= \\Pr(|X| \\leq c\\text{ and }X \\leq x) + \\Pr(|X|>c\\text{ and }-X \\leq x)\\\\\n&= \\Pr(|X| \\leq c\\text{ and }X \\leq x) + \\Pr(|X|>c\\text{ and }X \\leq x) \\\\\n&= \\Pr(X \\leq x), \\end{align}</math>\n\nwhere the next-to-last equality follows from the symmetry of the distribution of <math>X</math> and the symmetry of the condition that <math>|X| \\leq c</math>.\n\nIn this example, the difference <math>X-Y</math> is nowhere near being normally distributed, since it has a substantial probability (about&nbsp;0.88) of it being equal to&nbsp;0.  By contrast, the normal distribution, being a continuous distribution, has no discrete part&mdash;that is, it does not concentrate more than zero probability at any single point.  Consequently <math>X</math> and <math>Y</math> are not ''jointly'' normally distributed, even though they are separately normally distributed.<ref>Edward L. Melnick and Aaron Tenenbein, \"Misspecifications of the Normal Distribution\", ''[[The American Statistician]]'', volume 36, number 4 November 1982, pages 372&ndash;373</ref>\n\n== See also ==\n\n* [[Correlation and dependence]]\n\n== References ==\n{{reflist}}\n\n[[Category:Theory of probability distributions]]\n[[Category:Covariance and correlation]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Owen's T function",
      "url": "https://en.wikipedia.org/wiki/Owen%27s_T_function",
      "text": "In mathematics, '''Owen's T function''' ''T''(''h'',&nbsp;''a''), named after [[statistician]] Donald Bruce Owen, is defined by\n\n:<math>\n T(h,a)=\\frac{1}{2\\pi}\\int_{0}^{a} \\frac{e^{-\\frac{1}{2} h^2 (1+x^2)}}{1+x^2}  dx \\quad \\left(-\\infty < h, a < +\\infty\\right).\n</math>\n\nThe function was first introduced by Owen in 1956.<ref>Owen, D B (1956). \"Tables for computing bivariate normal probabilities\". ''Annals of Mathematical Statistics'',\n27, 1075&ndash;1090.</ref>\n\n==Applications==\nThe function ''T''(''h'',&nbsp;''a'') gives the probability of the event (''X'' > ''h'' and 0 < ''Y'' < ''aX'') where ''X'' and ''Y'' are [[statistically independent|independent]] [[standard normal distribution|standard normal]] [[random variable]]s.\n\nThis function can be used to calculate [[bivariate normal distribution]] probabilities<ref>Sowden, R R and Ashford, J R (1969). \"Computation of the bivariate normal integral\". ''Applied Statististics'', 18, 169&ndash;180.</ref><ref>Donelly, T G (1973). \"Algorithm 462. Bivariate normal distribution\". ''Commun. Ass. Comput.Mach.'', 16, 638.</ref> and, from there, in the calculation of [[multivariate normal distribution]] probabilities.<ref>Schervish, M H (1984). \"Multivariate normal probabilities with [[error bound]]\". ''Applied Statistics'', 33, 81&ndash;94.</ref>\nIt also frequently appears in [[List of integrals of Gaussian functions|various integrals involving Gaussian functions]].\n\nComputer algorithms for the accurate calculation of this function are available;<ref>Patefield, M. and Tandy, D.  (2000) \"[http://www.jstatsoft.org/v05/i05/paper Fast and accurate Calculation of Owen’s T-Function]\", ''Journal of Statistical Software'', 5 (5), 1&ndash;25. </ref> [[Gaussian quadrature|quadrature]] having been employed since the 1970s. <ref>[http://people.sc.fsu.edu/~jburkardt/m_src/asa076/tfn.m JC Young and Christoph Minder. Algorithm AS 76] </ref>\n\n==Properties==\n: <math> T(h,0) = 0 </math>\n: <math> T(0,a) = \\frac{1}{2\\pi} \\arctan(a) </math>\n: <math> T(-h,a) = T(h,a) </math>\n: <math> T(h,-a) = -T(h,a) </math>\n: <math> T(h,a) + T(ah,\\frac{1}{a}) = \\frac{1}{2} \\left(\\Phi(h) + \\Phi(ah)\\right) - \\Phi(h)\\Phi(ah) \\quad \\text{if} \\quad a \\geq 0 </math>\n: <math> T(h,a) + T(ah,\\frac{1}{a}) = \\frac{1}{2} \\left(\\Phi(h) + \\Phi(ah)\\right) - \\Phi(h)\\Phi(ah) - \\frac{1}{2} \\quad \\text{if} \\quad a < 0 </math>\n: <math> T(h, 1) = \\frac{1}{2} \\Phi(h) \\left(1 - \\Phi(h)\\right) </math>\n: <math> \\int T(0,x) \\, \\mathrm{d}x = x T(0,x) - \\frac{1}{4 \\pi} \\ln(1+x^2) + C </math>\nHere Φ(''x'') is the [[Normal distribution|standard normal cumulative distribution function]]\n: <math> \\Phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^x \\exp\\left(-t^2 / 2\\right) \\, \\mathrm{d}t </math>\nMore properties can be found in the literature.<ref>{{harvtxt|Owen|1980}}</ref>\n\n==References==\n{{reflist}}\n\n* {{cite journal\n  | last1 = Owen | first1 = D.\n  | year = 1980\n  | title = A table of normal integrals\n  | journal = Communications in Statistics: Simulation and Computation\n  | pages = 389–419\n  | volume = B9\n  | ref = harv\n  }}\n\n==Software==\n* [http://people.sc.fsu.edu/~burkardt/f_src/owens/owens.html Owen's T function] (user web site) - offers C++, FORTRAN77, FORTRAN90, and MATLAB libraries released under the LGPL license [[LGPL]]\n* Owen's T-function is implemented in [[Mathematica]] since version 8, as [http://reference.wolfram.com/mathematica/ref/OwenT.html OwenT].\n\n==External links==\n* [http://blog.wolfram.com/2010/10/07/why-you-should-care-about-the-obscure/ Why You Should Care about the Obscure] (Wolfram blog post)\n\n[[Category:Normal distribution]]\n[[Category:Computational statistics]]\n[[Category:Functions related to probability distributions]]\n\n\n{{statistics-stub}}"
    },
    {
      "title": "Power transform",
      "url": "https://en.wikipedia.org/wiki/Power_transform",
      "text": "In [[statistics]], a '''power transform''' is a family of functions that are applied to create a [[Monotonic function|monotonic transformation]] of data using [[power function]]s. This is a useful [[data transformation (statistics)|data transformation]] technique used to stabilize variance, make the data more [[normal distribution]]-like, improve the validity of measures of association such as the [[Pearson product-moment correlation coefficient|Pearson correlation]] between variables and for other data stabilization procedures.\n\n==Definition==\nThe power transformation is defined as a continuously varying function, with respect to the power parameter ''λ'', in a piece-wise function form that makes it continuous at the point of singularity (''λ''&nbsp;=&nbsp;0). For data vectors (''y''<sub>1</sub>,...,&nbsp;''y''<sub>''n''</sub>) in which each ''y''<sub>''i''</sub>&nbsp;>&nbsp;0, the power transform is\n\n: <math>y_i^{(\\lambda)} =\n\\begin{cases}\n\\dfrac{y_i^\\lambda-1}{\\lambda(\\operatorname{GM}(y))^{\\lambda -1}} , &\\text{if } \\lambda \\neq 0 \\\\[12pt]\n\\operatorname{GM}(y)\\ln{y_i} , &\\text{if } \\lambda = 0\n\\end{cases}\n</math>\n\nwhere\n\n: <math> \\operatorname{GM}(y) = (y_1\\cdots y_n)^{1/n} \\, </math>\n\nis the [[geometric mean]] of the observations ''y''<sub>1</sub>,&nbsp;...,&nbsp;''y''<sub>''n''</sub>.  The case for <math>\\lambda = 0</math> is the limit as <math>\\lambda</math> approaches 0.  To see this, note that <math>y_i^{\\lambda}</math> = <math>\\operatorname{exp}({\\lambda \\operatorname{log}(y_i)}) = 1 + \\lambda \\operatorname{log}(y_i) + O((\\lambda \\operatorname{log}(y_i))^2)</math>.  Then <math>\\dfrac{y_i^\\lambda-1}\\lambda</math> = <math>\\operatorname{log}(y_i) + O(\\lambda)</math>, and everything but <math>\\operatorname{log}(y_i)</math> becomes negligible for <math>\\lambda</math> sufficiently small.\n\nThe inclusion of the (''λ''&nbsp;−&nbsp;1)th power of the geometric mean in the denominator simplifies the [[Dimensional analysis|scientific interpretation of any equation involving]] <math>y_i^{(\\lambda)}</math>, because the units of measurement do not change as ''λ'' changes.\n\nBox and Cox (1964) introduced the geometric mean into this transformation by first including the [[Jacobian matrix and determinant|Jacobian]] of rescaled power transformation\n\n: <math> \\dfrac{y^\\lambda-1}{\\lambda} </math>.\n\nwith the likelihood.  This Jacobian is as follows:\n\n: <math> J(\\lambda; y_1, ..., y_n) = \\prod_{i=1}^n |d y_i^{(\\lambda)} / dy|\n= \\prod_{i=1}^n y_i^{\\lambda-1}\n= \\operatorname{GM}(y)^{n(\\lambda-1)}\n</math>\n\nThis allows the normal [[Maximum likelihood#Continuous distribution, continuous parameter space|log likelihood at its maximum]] to be written as follows:\n\n:<math>\n   \\log ( \\mathcal{L} (\\hat\\mu,\\hat\\sigma)) = (-n/2)(\\log(2\\pi\\hat\\sigma^2) +1) +\nn(\\lambda-1) \\log(\\operatorname{GM}(y))\n</math>\n::<math>\n       = (-n/2)(\\log(2\\pi\\hat\\sigma^2 / \\operatorname{GM}(y)^{2(\\lambda-1)}) + 1).\n</math>\n\nFrom here, absorbing <math>\\operatorname{GM}(y)^{2(\\lambda-1)}</math> into the expression for <math>\\hat\\sigma^2</math> produces an expression that establishes that minimizing the sum of squares of [[errors and residuals in statistics|residuals]] from <math>y_i^{(\\lambda)}</math>is equivalent to maximizing the sum of the normal [[Likelihood function|log likelihood]] of deviations from <math>(y^\\lambda-1)/\\lambda</math> and the log of the Jacobian of the transformation.\n\nThe value at ''Y'' = 1 for any ''λ'' is 0, and the [[derivative]] with respect to ''Y'' there is 1 for any ''λ''. Sometimes ''Y'' is a version of some other variable scaled to give ''Y'' = 1 at some sort of average value.\n\nThe transformation is a [[power (mathematics)|power]] transformation, but done in such a way as to make it [[continuous function|continuous]] with the parameter ''λ'' at ''λ'' = 0. It has proved popular in [[regression analysis]], including [[econometrics]].\n\nBox and Cox also proposed a more general form of the transformation that incorporates a shift parameter.\n\n:<math>\\tau(y_i;\\lambda, \\alpha) = \\begin{cases} \\dfrac{(y_i + \\alpha)^\\lambda - 1}{\\lambda (\\operatorname{GM}(y+\\alpha))^{\\lambda - 1}} & \\text{if } \\lambda\\neq 0, \\\\  \\\\\n\\operatorname{GM}(y+\\alpha)\\ln(y_i + \\alpha)& \\text{if } \\lambda=0,\\end{cases}</math>\nwhich holds if ''y''<sub>''i''</sub>&nbsp;+&nbsp;α > 0 for all&nbsp;''i''.  If τ(''Y'', λ, α) follows a [[truncated normal distribution]], then ''Y'' is said to follow a [[Box–Cox distribution]].\n\nBickel and Doksum eliminated the need to use a [[truncated distribution]] by extending the range of the transformation to all ''y'', as follows:\n\n:<math>\\tau(y_i;\\lambda, \\alpha) = \\begin{cases}\n\\dfrac{\\operatorname{sgn}(y_i + \\alpha)|y_i + \\alpha|^\\lambda - 1}{\\lambda (\\operatorname{GM}(y+\\alpha))^{\\lambda - 1}} & \\text{if } \\lambda\\neq 0, \\\\  \\\\\n\\operatorname{GM}(y+\\alpha)\\operatorname{sgn}(y+\\alpha)\\ln(y_i + \\alpha)& \\text{if } \\lambda=0,\\end{cases}</math>,\n\nwhere sgn(.) is the [[sign function]]. This change in definition has little practical import as long as <math>\\alpha</math> is less than <math>\\operatorname{min}(y_i)</math>, which it usually is.<ref name=\"Bickel and Doksum\">{{Cite journal | last = Bickel | first = Peter J. | last2 = Doksum | first2 = Kjell A.\n| author-link = Peter J. Bickel\n| date = June 1981 | title = An analysis of transformations revisited\n| journal = Journal of the American Statistical Association\n| volume = 76 | issue = 374\n| pages = 296–311 | doi=10.1080/01621459.1981.10477649}}</ref>\n\nBickel and Doksum also proved that the parameter estimates are [[Consistency (statistics)|consistent]] and [[Local asymptotic normality|asymptotically normal]] under appropriate regularity conditions, though the standard [[Cramér–Rao bound|Cramér–Rao lower bound]] can substantially underestimate the variance when parameter values are small relative to the noise variance.<ref name=\"Bickel and Doksum\" /> However, this problem of underestimating the variance may not be a substantive problem in many applications.<ref>{{Citation | last = Sakia | first = R. M. | year = 1992\n| title = The Box–Cox transformation technique: a review | journal = The Statistician\n| volume = 41 | issue = 2 | pages = 169–178 | doi=10.2307/2348250| jstor = 2348250 | citeseerx = 10.1.1.469.7176 }}</ref><ref>{{Citation\n| last = Li | first = Fengfei\n| date = April 11, 2005\n| title = Box–Cox Transformations: An Overview\n| type = slide presentation\n| publisher = University of Sao Paulo, Brazil\n| location = Sao Paulo, Brazil\n| url = http://www.ime.usp.br/~abe/lista/pdfm9cJKUmFZp.pdf\n| accessdate = 2014-11-02}}</ref>\n\n==Box–Cox transformation==\nThe one-parameter Box–Cox transformations are defined as\n\n:<math>\ny_i^{(\\lambda)} =\n\\begin{cases}\n \\dfrac{y_i^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0, \\\\\n \\ln y_i & \\text{if } \\lambda = 0,\n\\end{cases}\n</math>\n\nand the two-parameter Box–Cox transformations as\n\n:<math>\ny_i^{(\\boldsymbol{\\lambda})} =\n\\begin{cases}\n \\dfrac{(y_i + \\lambda_2)^{\\lambda_1} - 1}{\\lambda_1} & \\text{if } \\lambda_1 \\neq 0, \\\\\n \\ln (y_i + \\lambda_2) & \\text{if } \\lambda_1 = 0,\n\\end{cases}\n</math>\n\nas described in the original article.<ref name=boxcox>{{cite journal |last=Box |first=George E. P. |authorlink=George E. P. Box |last2=Cox |first2=D. R. |authorlink2=David Cox (statistician) |title=An analysis of transformations |journal=Journal of the Royal Statistical Society, Series B |volume=26 |issue=2 |pages=211–252 |year=1964 |mr=192611 |jstor=2984418 }}</ref><ref>{{cite book |first=J. |last=Johnston |authorlink=John Johnston (econometrician) |title=Econometric Methods |location=New York |publisher=McGraw-Hill |edition=Third |year=1984 |isbn=978-0-07-032685-9 |pages=61–74 }}</ref>  Moreover, the first transformations hold for <math>y_i > 0</math>, and the second for <math>y_i > -\\lambda_2</math>.<ref name=\"boxcox\" />\n\nThe parameter <math>\\lambda</math> is estimated using the [[profile likelihood]] function.\n\n==Confidence interval==\nConfidence interval for the Box–Cox transformation can be [[Confidence interval#Methods of derivation|asymptotically constructed]] using [[Likelihood-ratio test#Distribution: Wilks.27s theorem|Wilks's theorem]] on the [[profile likelihood]] function to find all the possible values of <math>\\lambda</math> that fulfill the following restriction:<ref>Abramovich, Felix, and Ya'acov Ritov. Statistical Theory: A Concise Introduction. CRC Press, 2013. Pages 121–122.</ref>\n\n:<math>\\ln \\big(L(\\lambda)\\big) \\ge \\ln \\big(L(\\hat\\lambda)\\big) - \\frac{1}{2} {\\chi^2}_{1,1 - \\alpha}.</math>\n\n==Use of the power transform==\n* Power transforms are ubiquitously used in various fields. For example, [[Multiresolution analysis|multi-resolution and wavelet analysis]]<ref>{{Cite journal|last=Gao|first=Peisheng|last2=Wu|first2=Weilin|date=2006|title=Power Quality Disturbances Classification Using Wavelet and Support Vector Machines|url=http://dx.doi.org/10.1109/ISDA.2006.217|journal=Proceedings of the Sixth International Conference on Intelligent Systems Design and Applications - Volume 01|series=ISDA '06|location=Washington, DC, USA|publisher=IEEE Computer Society|pages=201–206|doi=10.1109/ISDA.2006.217|isbn=9780769525280}}</ref>, statistical data analysis, medical research, modeling of physical processes<ref>{{Cite journal|last=Gluzman|first=S.|last2=Yukalov|first2=V. I.|date=2006-01-01|title=Self-similar power transforms in extrapolation problems|url=https://doi.org/10.1007/s10910-005-9003-7|journal=Journal of Mathematical Chemistry|language=en|volume=39|issue=1|pages=47–56|doi=10.1007/s10910-005-9003-7|issn=1572-8897|arxiv=cond-mat/0606104}}</ref>, [[Geochemical modeling|geochemical data analysis]]<ref>{{Cite journal|last=Howarth|first=R. J.|last2=Earle|first2=S. A. M.|date=1979-02-01|title=Application of a generalized power transformation to geochemical data|url=https://doi.org/10.1007/BF01043245|journal=Journal of the International Association for Mathematical Geology|language=en|volume=11|issue=1|pages=45–62|doi=10.1007/BF01043245|issn=1573-8868}}</ref>, [[epidemiology]]<ref>{{Cite journal | last1 = Peters | first1 = J. L. | last2 = Rushton | first2 = L. | last3 = Sutton | first3 = A. J. | last4 = Jones | first4 = D. R. | last5 = Abrams | first5 = K. R. | last6 = Mugglestone | first6 = M. A. | doi = 10.1111/j.1467-9876.2005.00476.x | title = Bayesian methods for the cross-design synthesis of epidemiological and toxicological evidence | journal = Journal of the Royal Statistical Society, Series C | volume = 54 | pages = 159–172 | year = 2005 | pmid =  | pmc = }}</ref> and many other clinical, environmental and social research areas.\n\n==Example==\nThe BUPA liver data set<ref>[ftp://ftp.ics.uci.edu/pub/machine-learning-databases/liver-disorders BUPA liver disorder dataset]</ref> contains data on liver enzymes [[Alanine transaminase|ALT]] and [[Gamma-glutamyl transpeptidase|γGT]]. Suppose we are interested in using log(γGT) to predict ALT. A plot of the data appears in panel (a) of the figure. There appears to be non-constant variance, and a Box–Cox transformation might help.\n\n[[image:BUPA BoxCox.JPG]]\n\nThe log-likelihood of the power parameter appears in panel (b). The horizontal reference line is at a distance of χ<sub>1</sub><sup>2</sup>/2 from the maximum and can be used to read off an approximate 95% confidence interval for λ. It appears as though a value close to zero would be good, so we take logs.\n\nPossibly, the transformation could be improved by adding a shift parameter to the log transformation. Panel (c) of the figure shows the log-likelihood. In this case, the maximum of the likelihood is close to zero suggesting that a shift parameter is not needed. The final panel shows the transformed data with a superimposed regression line.\n\nNote that although Box–Cox transformations can make big improvements in model fit, there are some issues that the transformation cannot help with. In the current example, the data are rather heavy-tailed so that the assumption of normality is not realistic and a [[robust regression]] approach leads to a more precise model.\n\n==Econometric application==\n\nEconomists often characterize production relationships by some variant of the Box–Cox transformation.\n\nConsider a common representation of production ''Q'' as dependent on services provided by a capital stock ''K'' and by labor hours ''N'':\n\n:<math>\\tau(Q)=\\alpha \\tau(K)+ (1-\\alpha)\\tau(N).\\,</math>\n\nSolving for ''Q'' by inverting the Box–Cox transformation we find\n\n:<math>Q=\\big(\\alpha K^\\lambda + (1-\\alpha) N^\\lambda\\big)^{1/\\lambda},\\,</math>\n\nwhich is known as the ''constant elasticity of substitution (CES)'' production function.\n\nThe CES production function is a [[homogeneous function]] of degree one.\n\nWhen ''λ'' = 1, this produces the linear production function:\n\n: <math>Q=\\alpha K + (1-\\alpha)N.\\,</math>\n\nWhen ''λ'' → 0 this produces the famous [[Cobb–Douglas]] production function:\n\n: <math>Q=K^\\alpha N^{1-\\alpha}.\\,</math>\n\n==Activities and demonstrations==\nThe [[SOCR]] resource pages contain a number of hands-on interactive activities<ref>[http://wiki.stat.ucla.edu/socr/index.php/SOCR_EduMaterials_Activities_PowerTransformFamily_Graphs Power Transform Family Graphs], SOCR webpages</ref> demonstrating the Box–Cox (power) transformation using Java applets and charts. These directly illustrate the effects of this transform on [[Q-Q plot]]s, X-Y [[scatterplot]]s, [[time-series]] plots and [[histogram]]s.\n\n==Yeo–Johnson transformation==\nThe Yeo–Johnson transformation\n<ref name=yeojohnson>{{cite journal |last=Yeo |first=In-Kwon |last2=Johnson |first2=Richard A. |title=A New Family of Power Transformations to Improve Normality or Symmetry |journal=[[Biometrika]] |volume=87 |issue=4 |pages=954–959 |year=2000 |jstor=2673623 |doi=10.1093/biomet/87.4.954 }}</ref>\nallows also for zero and negative values of <math>y</math>.\nThe allowed range of the parameter is <math>0 \\le \\lambda \\le 2</math>, where <math>\\lambda = 1</math> produces the identity transformation.\nThe transformation law reads:\n\n<math>\ny_i^{(\\lambda)} = \\begin{cases} ((y_i+1)^\\lambda-1)/\\lambda                      &  \\text{if }\\lambda \\neq 0, y \\geq 0 \\\\ \n                                \\log(y_i + 1)                                    &  \\text{if }\\lambda =    0, y \\geq 0 \\\\ \n                                -[(-y_i + 1)^{(2-\\lambda)} - 1] / (2 - \\lambda)  &  \\text{if }\\lambda \\neq 2, y <    0 \\\\ \n                                -\\log(-y_i + 1)                                  &  \\text{if }\\lambda =    2, y <    0 \n                  \\end{cases}\n</math>\n\n==Notes==\n{{reflist}}\n\n==References==\n* {{cite journal\n | last = Box | first = George E. P. | authorlink = George E. P. Box\n | last2= Cox |first2=D. R. |authorlink2=David Cox (statistician)\n | title = An analysis of transformations\n | journal = [[Journal of the Royal Statistical Society, Series B]]\n | volume = 26 |issue=2| pages = 211–252 | year = 1964\n | mr=192611 | jstor = 2984418\n}}\n*  {{cite journal |last=Carroll |first=RJ |last2=Ruppert |first2=D. |url=http://wiki.stat.ucla.edu/socr/uploads/b/b8/PowerTransformFamily_Biometrica609.pdf |title=On prediction and the power transformation family |journal=Biometrika |volume=68 |issue=3 |pages=609–615|doi=10.1093/biomet/68.3.609 |year=1981 }}\n* {{cite journal | last = DeGroot| first = M. H. |authorlink=Morris H. DeGroot | title = A Conversation with George Box | journal = Statistical Science | volume = 2 | pages = 239–258 | year = 1987| doi = 10.1214/ss/1177013223 | issue = 3}}\n* {{cite journal |last=Handelsman |first=DJ |title=Optimal Power Transformations for Analysis of Sperm Concentration and Other Semen Variables |journal=Journal of Andrology |volume=23 |issue=5 |year=2002}}\n* {{cite journal |last=Gluzman |first=S. |last2=Yukalov |first2=V. I. |title=Self-similar power transforms in extrapolation problems |journal=Journal of Mathematical Chemistry |volume=39 |issue=1 |year=2006 |doi=10.1007/s10910-005-9003-7 |pages=47–56 |arxiv=cond-mat/0606104 }}\n* {{cite journal |last=Howarth |first=R. J. |last2=Earle |first2=S. A. M. |title=Application of a generalized power transformation to geochemical data |journal=Journal of the International Association for Mathematical Geology |volume=11 |issue=1 |year=1979 |doi=10.1007/BF01043245 |pages=45–62 }}\n\n==External links==\n* {{SpringerEOM |title=Box–Cox transformation |id=B/b110790 |first=R. |last=Nishii}} ([http://www.encyclopediaofmath.org/index.php/Box%E2%80%93Cox_transformation fixed link])\n* Sanford Weisberg, [https://www.stat.umn.edu/arc/yjpower.pdf Yeo-Johnson Power Transformations]\n\n[[Category:Normal distribution]]\n[[Category:Statistical data transformation]]"
    },
    {
      "title": "Pregaussian class",
      "url": "https://en.wikipedia.org/wiki/Pregaussian_class",
      "text": "In [[probability theory]], a '''pregaussian class''' or '''pregaussian set''' of functions is a set of functions, [[square integrable]] with respect to some [[probability measure]], such that there exists a certain [[Gaussian process]], indexed by this set, satisfying the conditions below.\n\n==Definition==\nFor a [[probability space]] (''S'', &Sigma;, ''P''), denote by <math>L^2_P(S)</math> a set of square integrable with respect to ''P'' functions <math>f:S\\to R</math>, that is\n\n:<math> \\int f^2 \\, dP<\\infty</math>\n\nConsider a set <math>\\mathcal{F}\\subset L^2_P(S)</math>. There exists a [[Gaussian process]] <math>G_P</math>, indexed by <math>\\mathcal{F}</math>, with mean 0 and covariance\n\n:<math>\\operatorname{Cov} (G_P(f),G_P(g))= E G_P(f)G_P(g)=\\int fg\\, dP-\\int f\\,dP \\int g\\,dP\\text{ for }f,g\\in\\mathcal{F}</math>\nSuch a process exists because the given covariance is positive definite. This covariance defines a semi-inner product as well as a [[pseudometric space|pseudometric]] on <math>L^2_P(S)</math> given by\n:<math>\\varrho_P(f,g)=(E(G_P(f)-G_P(g))^2)^{1/2}</math>\n\n'''Definition''' A class <math>\\mathcal{F}\\subset L^2_P(S)</math> is called '''pregaussian''' if for each <math>\\omega\\in S,</math> the function <math>f\\mapsto G_P(f)(\\omega)</math> on <math>\\mathcal{F}</math> is bounded, <math>\\varrho_P</math>-uniformly continuous, and prelinear.\n\n==Brownian bridge==\nThe <math>G_P</math> process is a generalization of the [[brownian bridge]]. Consider <math>S=[0,1],</math> with ''P'' being the [[uniform distribution (continuous)|uniform measure]]. In this case, the <math>G_P</math> process indexed by the [[indicator function]]s <math>I_{[0,x]}</math>, for <math>x\\in [0,1],</math> is in fact the standard [[brownian bridge]] ''B''(''x''). This set of the indicator functions is pregaussian, moreover, it is the [[Donsker class]].\n\n== References ==\n*{{citation | author = R. M. Dudley | title = Uniform central limit theorems | publisher = Cambridge University Press | location = Cambridge, UK | year = 1999 | pages = 436 | isbn = 0-521-46102-2}}\n\n[[Category:Stochastic processes]]\n[[Category:Empirical process]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Probit",
      "url": "https://en.wikipedia.org/wiki/Probit",
      "text": "{{See also|Probit model}}\n{{technical|date=January 2013}}\n[[File:Probit plot.png|thumbnail|right|Plot of probit function]]\nIn [[probability theory]] and [[statistics]], the '''probit''' function is the [[quantile function]] associated with the standard [[normal distribution]], which is commonly denoted as N(0,1). Mathematically, it is the inverse of the [[cumulative distribution function]] of the standard normal distribution, which is denoted as <math>\\Phi(z)</math>, so the probit is denoted as <math>\\Phi^{-1}(p)</math>. It has applications in [[Q-Q plot|exploratory statistical graphics]] and specialized [[probit model|regression modeling of binary response variables]].\n\nLargely because of the [[central limit theorem]], the standard normal distribution plays a fundamental role in probability theory and statistics. If we consider the familiar fact that the standard normal distribution places 95% of probability between −1.96 and 1.96, and is symmetric around zero, it follows that\n\n:<math>\\Phi(-1.96) = 0.025 = 1-\\Phi(1.96).\\,\\!</math>\n\nThe probit function gives the 'inverse' computation, generating a value of an N(0,1) random variable, associated with specified cumulative probability. Continuing the example,\n\n:<math>\\operatorname{probit}(0.025) = -1.96 = -\\operatorname{probit}(0.975)</math>.\n\nIn general,\n\n:<math> \\Phi(\\operatorname{probit}(p))=p</math> \n:and \n:<math>\\operatorname{probit}(\\Phi(z))=z.</math>\n\n==Conceptual development==\nThe idea of the probit function was published by [[Chester Ittner Bliss]] (1899–1979) in a 1934 article in ''[[Science (journal)|Science]]'' on how to treat data such as the percentage of a pest killed by a [[pesticide]].<ref>{{cite journal | journal=Science | volume=79 | issue=2037 | pages=38–39 | year=1934 | author=Bliss CI. | title=The method of probits | pmid=17813446 | doi =10.1126/science.79.2037.38 |jstor=1659792 }}</ref> Bliss proposed transforming the percentage killed into a \"'''prob'''ability un'''it'''\" (or \"probit\") which was linearly related to the modern definition (he defined it arbitrarily as equal to 0 for 0.0001 and 1 for 0.9999). He included a table to aid other researchers to convert their kill percentages to his probit, which they could then plot against the logarithm of the dose and thereby, it was hoped, obtain a more or less straight line. Such a so-called [[probit model]] is still important in toxicology, as well as other fields. The approach is justified in particular if response variation can be rationalized as a [[lognormal]] distribution of tolerances among subjects on test, where the tolerance of a particular subject is the dose just sufficient for the response of interest.\n\nThe method introduced by Bliss was carried forward in ''Probit Analysis'', an important text on toxicological applications by [[D. J. Finney]].<ref>Finney, D.J. (1947), ''Probit Analysis''. (1st edition) Cambridge University Press, Cambridge, UK.</ref><ref>{{cite book| author=Finney, D.J. | year=1971 | title=Probit Analysis (3rd edition)| publisher= Cambridge University Press, Cambridge, UK| isbn=0-521-08041-X| oclc=174198382 }}</ref> Values tabled by Finney can be derived from probits as defined here by adding a value of 5. This distinction is summarized by Collett (p.&nbsp;55):<ref>{{cite book | author = Collett, D. | year=1991 | title=Modelling Binary Data | publisher=Chapman and Hall / CRC}}</ref> \"The original definition of a probit [with 5 added] was primarily to avoid having to work with negative probits; ... This definition is still used in some quarters, but in the major statistical software packages for what is referred to as '''probit analysis''', probits are defined without the addition of 5.\" It should be observed that probit methodology, including numerical optimization for fitting of probit functions, was introduced before widespread availability of electronic computing. When using tables, it was convenient to have probits uniformly positive. Common areas of application do not require positive probits.\n\n==Diagnosing deviation of a distribution from normality==\n{{main | Q-Q plot}}\nIn addition to providing a basis for important types of regression, the probit function is useful in statistical analysis for diagnosing deviation from normality, according to the method of Q-Q plotting. If a set of data is actually a [[Sample (statistics)|sample]] of a [[normal distribution]], a plot of the values against their probit scores will be approximately linear. Specific deviations from normality such as [[skewness|asymmetry]], [[kurtosis|heavy tails]], or [[bimodal distribution|bimodality]] can be diagnosed based on detection of specific deviations from linearity. While the Q-Q plot can be used for comparison to any distribution family (not only the normal), the normal Q-Q plot is a relatively standard exploratory data analysis procedure because the assumption of normality is often a starting point for analysis.\n\n==Computation==\nThe normal distribution CDF and its inverse are not available in [[closed-form expression|closed form]], and computation requires careful use of numerical procedures. However, the functions are widely available in software for statistics and probability modeling, and in spreadsheets. In [[Microsoft Excel]], for example, the probit function is available as norm.s.inv(p). In computing environments where numerical implementations of the [[error function|inverse error function]] are available, the probit function may be obtained as\n:<math>\n\\operatorname{probit}(p) = \\sqrt{2}\\,\\operatorname{erf}^{-1}(2p-1).\n</math>\nAn example is [[MATLAB]], where an 'erfinv' function is available. The language [[Mathematica]] implements 'InverseErf'. Other environments directly implement the probit function as is shown in the following session in the [[R programming language]].\n<source lang=\"rout\">\n> qnorm(0.025)\n[1] -1.959964\n> pnorm(-1.96)\n[1] 0.02499790\n</source>\n\nDetails for computing the inverse error function can be found at [https://stackedboxes.org/2017/05/01/acklams-normal-quantile-function/]. Wichura gives a fast algorithm for computing the probit function to 16 decimal places; this is used in R to generate random variates for the normal distribution.<ref>{{cite journal |author=Wichura, M.J. |year=1988 |title=Algorithm AS241: The Percentage Points of the Normal Distribution |journal=Applied Statistics |volume=37 |pages=477–484 |doi=10.2307/2347330 |jstor=2347330 |issue=3 |publisher=Blackwell Publishing}}</ref>\n\n===An ordinary differential equation for the probit function===\nAnother means of computation is based on forming a non-linear ordinary differential equation (ODE) for probit, as per the Steinbrecher and Shaw method.<ref>{{cite journal| author= Steinbrecher, G., Shaw, W.T. | year=2008 | title=Quantile mechanics| journal= European Journal of Applied Mathematics| volume= 19 | issue=2| pages=87–112| doi = 10.1017/S0956792508007341 }}</ref> Abbreviating the probit function as <math>w(p)</math>, the ODE is\n\n:<math>\\frac{d w}{d p} = \\frac{1}{f(w)} </math>\nwhere <math>f(w)</math> is the probability density function of {{mvar|w}}.\n\nIn the case of the Gaussian:\n:<math>\\frac{d w}{d p} = \\sqrt{2 \\pi }  \\ e^{\\frac{w^2}{2}} </math>\n\nDifferentiating again:\n\n:<math>\\frac{d^2 w}{d p^2} = w \\left(\\frac{d w}{d p}\\right)^2 </math>\n\nwith the centre (initial) conditions\n\n:<math>w\\left(1/2\\right) = 0,</math>\n\n:<math>w'\\left(1/2\\right) = \\sqrt{2\\pi}.</math>\n\nThis equation may be solved by several methods, including the classical power series approach. From this, solutions of arbitrarily high accuracy may be developed based on Steinbrecher's approach to the series for the inverse error function. The power series solution is given by\n\n:<math> w(p) = \\sqrt \\frac{\\pi}{2} \\sum_{k=0}^{\\infty} \\frac{d_k}{(2k+1)}(2p-1)^{(2k+1)} </math>\n\nwhere the coefficients <math>d_k </math> satisfy the non-linear recurrence\n\n:<math>d_{k+1} = \\frac{\\pi}{4} \\sum_{j=0}^k \\frac{d_j d_{k-j}}{(j+1)(2j+1)}</math>\n\nwith <math>d_0=1</math>. In this form the ratio <math>d_{k+1}/d_k \\rightarrow 1</math> as <math>k \\rightarrow \\infty</math>.\n<!--- are these numerically stable? --->\n\n== See also ==\n[[File:Logit-probit.svg|right|300px|thumb|Comparison of the [[logit function]] with a scaled probit (i.e. the inverse [[cumulative distribution function|CDF]] of the [[normal distribution]]), comparing <math>\\operatorname{logit}(x)</math> vs. <math>\\Phi^{-1}(x)/\\sqrt{\\frac{\\pi}{8}}</math>, which makes the slopes the same at the origin.]]\n\nClosely related to the probit function (and [[probit model]]) are the [[logit]] function and [[logit model]]. The inverse of the logistic function is given by\n\n:<math>\\operatorname{logit}(p)=\\log\\left( \\frac{p}{1-p} \\right).</math>\n\nAnalogously to the probit model, we may assume that such a quantity is related linearly to a set of predictors, resulting in the [[logit model]], the basis in particular of [[logistic regression]] model, the most prevalent form of [[regression analysis]] for categorical response data. In current statistical practice, probit and logit regression models are often handled as cases of the [[generalized linear model]].\n\n==See also==\n*[[Detection error tradeoff]] graphs (DET graphs, an alternative to the ROC)\n*[[Logistic regression]] (a.k.a. logit model)\n*[[Logit]]\n*[[Probit model]]\n*[[Multinomial probit]]\n*[[Q-Q plot]]\n*[[Continuous function]]\n*[[Monotonic function]]\n*[[Quantile function]]\n*[[Sigmoid function]]\n*[[Rankit]] analysis, also developed by Chester Bliss\n*[[Ridit scoring]]\n\n==References==\n{{reflist}}\n\n[[Category:Statistical analysis]]\n[[Category:Single-equation methods (econometrics)]]\n[[Category:Normal distribution]]\n\n[[ru:Пробит регрессия]]"
    },
    {
      "title": "Rankit",
      "url": "https://en.wikipedia.org/wiki/Rankit",
      "text": "In [[statistics]], '''rankits''' of a set of data are the expected values of the [[order statistic]]s of a sample from the standard [[normal distribution]] the same size as the data. They are primarily used in the [[normal probability plot]], a [[graphical technique]] for [[normality test]]ing.\n\n[[File:Normal probability plot.gif|thumb|380px|Sample [[normal probability plot]]; horizontal axis coordinates are rankits.]]\n\n==Example==\n\nThis is perhaps most readily understood by means of an example.  If an [[Independent identically-distributed random variables|i.i.d.]] sample of six items is taken from a [[normal distribution|normally distributed]] population with [[expected value]] 0 and [[variance]] 1 (the [[standard normal distribution]]) and then sorted into increasing order, the expected values of the resulting [[order statistic]]s are:\n\n:&minus;1.2672, &nbsp;&nbsp;&minus;0.6418, &nbsp;&nbsp;&minus;0.2016, &nbsp;&nbsp;0.2016, &nbsp;&nbsp;0.6418, &nbsp;&nbsp;1.2672.\n\nSuppose the numbers in a data set are\n\n: 65,  75,  16,  22,  43,  40.\n\nThen one may sort these and line them up with the corresponding rankits; in order they are\n: 16, 22, 40, 43, 65,  75,\nwhich yields the points:\n{| class=\"wikitable\" style=\"text-align:right\"\n! data point !! rankit\n|-\n| 16 || &minus;1.2672\n|-\n| 22 || &minus;0.6418\n|-\n| 40 || &minus;0.2016\n|-\n| 43 || 0.2016\n|-\n| 65 || 0.6418\n|-\n| 75 || 1.2672\n|}\n\nThese points are then plotted as the vertical and horizontal coordinates of a [[scatter plot]].\n\n=== Alternative method ===\nAlternatively, rather than ''sort'' the data points, one may ''rank'' them, and ''rearrange'' the rankits accordingly. This yields the same pairs of numbers, but in a different order.\n\nFor:\n: 65,  75,  16,  22,  43,  40,\nthe corresponding ranks are:\n: 5,  6,  1,  2,  4,  3,\n\ni.e., the number appearing first is the 5th-smallest, the number appearing second is 6th-smallest, the number appearing third is smallest, the number appearing fourth is 2nd-smallest, etc.  One rearranges the expected normal order statistics accordingly, getting the '''rankits''' of this data set:\n\n{| class=\"wikitable\" style=\"text-align:right\"\n! data point !! rank !! rankit\n|-\n| 65 || 5 || 0.6418\n|-\n| 75 || 6 || 1.2672\n|-\n| 16 || 1 || &minus;1.2672\n|-\n| 22 || 2 || &minus;0.6418\n|-\n| 43 || 4 || 0.2016\n|-\n| 40 || 3 || &minus;0.2016\n|}\n\n==Rankit plot==\n{{main|Normal probability plot}}\nA graph plotting the rankits on the horizontal axis and the data points on the vertical axis is called a '''rankit plot''' or a '''[[normal probability plot]]'''.  Such a plot is necessarily nondecreasing.  In large samples from a normally distributed population, such a plot will approximate a straight line.  Substantial deviations from straightness are considered evidence against normality of the distribution.  \n\nRankit plots are usually used to visually demonstrate whether data are from a specified [[probability distribution]].\n\nA rankit plot is a kind of [[Q-Q plot]] – it plots the order statistics (quantiles) of the sample against certain quantiles (the rankits) of the assumed normal distribution. Q-Q plots may use other quantiles for the normal distribution, however.\n\n==History==\nThe rankit plot and the word '''''rankit''''' was introduced by the biologist and statistician [[Chester Ittner Bliss]] (1899&ndash;1979).\n\n== See also ==\n* [[Probit]] analysis developed by C. I. Bliss in 1934.\n\n==External links==\n\n* [http://www.itl.nist.gov/div898/handbook/eda/section3/normprpl.htm Engineering Statistics Handbook]\n\n[[Category:Statistical charts and diagrams]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Rectified Gaussian distribution",
      "url": "https://en.wikipedia.org/wiki/Rectified_Gaussian_distribution",
      "text": "{{distinguish|truncated Gaussian distribution}}\n\nIn [[probability theory]], the '''rectified Gaussian distribution''' is a modification of the [[Gaussian distribution]] when its negative elements are reset to 0 (analogous to an electronic [[rectifier]]). It is essentially a mixture of a [[discrete distribution]] (constant 0) and a [[continuous distribution]] (a [[truncated Gaussian distribution]] with interval <math>(0,\\infty)</math>) as a result of [[censoring (statistics)|censoring]].\n\n== Density function ==\nThe [[probability density function]] of a rectified Gaussian distribution, for which [[random variable]]s ''X'' having this distribution, derived from the normal distribution <math>\\mathcal{N}(\\mu,\\sigma^2),</math> are displayed as  <math>X \\sim \\mathcal{N}^{\\textrm{R}}(\\mu,\\sigma^2) </math>,   is given by\n: <math>\n    f(x;\\mu,\\sigma^2) =\\Phi(-\\frac{\\mu}{\\sigma})\\delta(x)+ \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\; e^{ -\\frac{(x-\\mu)^2}{2\\sigma^2}}\\textrm{U}(x).\n</math>\n[[Image:Truncated Gaussian.jpg|500px|thumb|A comparison of Gaussian distribution, rectified Gaussian distribution, and truncated Gaussian distribution.]]\nHere, <math> \\Phi(x) </math> is the [[cumulative distribution function]] (cdf) of the [[standard normal distribution]]:\n: <math>\n    \\Phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-t^2/2} \\, dt\n            \\quad x\\in\\mathbb{R},\n  </math>\n<math> \\delta(x) </math> is the [[Dirac delta function]]\n: <math>\\delta(x) = \\begin{cases} +\\infty, & x = 0 \\\\ 0, & x \\ne 0 \\end{cases}</math>\nand, <math> \\textrm{U}(x) </math> is the [[unit step function]]:\n:<math>\\textrm{U}(x)=\\begin{cases} 0, & x \\leq 0, \\\\ 1, & x > 0. \\end{cases} </math>\n\n==Mean and variance==\n\nSince the unrectified normal distribution has [[mean]] <math>\\mu</math> and since in transforming it to the rectified distribution some probability mass has been shifted to a higher value (from negative values to 0), the mean of the rectified distribution is greater than <math>\\mu.</math>\n\nSince the rectified distribution is formed by moving some of the probability mass toward the rest of the probability mass, the rectification is a [[mean-preserving contraction]] combined with a mean-changing rigid shift of the distribution, and thus the [[variance]] is decreased; therefore the variance of the rectified distribution is less than <math>\\sigma^2.</math>\n\n== Generating values==\nTo generate values computationally, one can use\n:<math> s\\sim\\mathcal{N}(\\mu,\\sigma^2), \\quad x=\\textrm{max}(0,s), </math>\nand then\n:<math> x\\sim\\mathcal{N}^{\\textrm{R}}(\\mu,\\sigma^2).</math>\n\n== Application==\nA rectified Gaussian distribution is semi-conjugate to the Gaussian likelihood, and it has been recently applied to [[factor analysis]], or particularly, (non-negative) rectified factor analysis.\nHarva <ref>{{Cite journal | last1 = Harva | first1 = M. | last2 = Kaban | first2 = A. | doi = 10.1016/j.sigpro.2006.06.006 | title = Variational learning for rectified factor analysis☆ | journal = Signal Processing | volume = 87 | issue = 3 | pages = 509 | year = 2007 | pmid =  | pmc = }}</ref> proposed a [[Variational Bayesian methods|variational learning]] algorithm for the rectified factor model, where the factors follow a mixture of rectified Gaussian; and later Meng <ref>{{cite journal|last1=Meng|first1=Jia|last2=Zhang|first2=Jianqiu (Michelle)|last3=Chen|first3=Yidong|last4=Huang|first4=Yufei|title=Bayesian non-negative factor analysis for reconstructing transcription factor mediated regulatory networks|journal=Proteome Science|volume=9|issue=Suppl 1|year=2011|pages=S9|issn=1477-5956|doi=10.1186/1477-5956-9-S1-S9}}</ref> proposed an infinite rectified factor model coupled with its Gibbs sampling solution, where the factors follow a [[Dirichlet process]] mixture of rectified Gaussian distribution, and applied it in [[computational biology]] for reconstruction of [[gene regulatory network]]s.\n\n== References ==\n{{Reflist}}\n\n{{ProbDistributions|mixed}}\n\n[[Category:Probability distributions]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Skew normal distribution",
      "url": "https://en.wikipedia.org/wiki/Skew_normal_distribution",
      "text": "{{Probability distribution |\n  name       =Skew Normal|\n  type       =density|\n  pdf_image  =[[Image:Skew normal densities.svg|325px|Probability density plots of skew normal distributions]]|\n  cdf_image  =[[Image:Skew normal cdfs.svg|325px|Cumulative distribution function plots of skew normal distributions]]|\n  parameters =<math>\\xi \\,</math> [[location parameter|location]] ([[real number|real]])<br/><math>\\omega \\,</math> [[scale parameter|scale]] (positive, [[real number|real]])<br/><math>\\alpha \\,</math> [[shape parameter|shape]] ([[real number|real]])|\n  support    =<math>x \\in (-\\infty; +\\infty)\\!</math>|\n  pdf        = <math>\\frac{2}{\\omega \\sqrt{2 \\pi}} e^{-\\frac{(x-\\xi)^2}{2\\omega^2}} \\int_{-\\infty}^{\\alpha\\left(\\frac{x-\\xi}{\\omega}\\right)} \\frac{1}{\\sqrt{2 \\pi}}  e^{-\\frac{t^2}{2}}\\ dt</math>|\n  cdf        =<math>\\Phi\\left(\\frac{x-\\xi}{\\omega}\\right)-2T\\left(\\frac{x-\\xi}{\\omega},\\alpha\\right)</math><br/><math>T(h,a)</math> is [[Owen's T function]]|\n  mean       =<math>\\xi + \\omega\\delta\\sqrt{\\frac{2}{\\pi}}</math> where <math>\\delta = \\frac{\\alpha}{\\sqrt{1+\\alpha^2}}</math>|\n  median     =<!-- to do -->|\n  mode       =<math>\\xi + \\omega m_o(\\alpha) </math>|\n  variance   =<math>\\omega^2\\left(1 - \\frac{2\\delta^2}{\\pi}\\right)</math>|\n  skewness   =<math>\\gamma_1 = \\frac{4-\\pi}{2} \\frac{\\left(\\delta\\sqrt{2/\\pi}\\right)^3}{  \\left(1-2\\delta^2/\\pi\\right)^{3/2}}</math>|\n  kurtosis   =<math>2(\\pi - 3)\\frac{\\left(\\delta\\sqrt{2/\\pi}\\right)^4}{\\left(1-2\\delta^2/\\pi\\right)^2}</math>|\n  entropy    =<!-- to do -->|\n  mgf        =<math>M_X\\left(t\\right)=2\\exp\\left(\\xi t+\\frac{\\omega^2t^2}{2}\\right)\\Phi\\left(\\omega\\delta t\\right)</math>|\n  cf         =<math>M_X\\left(i\\delta\\omega t\\right)</math>|\n  char       =<math>e^{i t \\xi -t^2\\omega^2/2}\\left(1+i\\, \\textrm{Erfi}\\left(\\frac{\\delta\\omega t}{\\sqrt{2}}\\right)\\right)</math>|\n}}\n\nIn [[probability theory]] and [[statistics]], the '''skew normal distribution''' is a [[continuous probability distribution]] that generalises the [[normal distribution]] to allow for non-zero [[skewness]].\n\n==Definition==\n\nLet <math>\\phi(x)</math> denote the [[Normal distribution|standard normal]] [[probability density function]]\n:<math>\\phi(x)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}</math>\nwith the [[cumulative distribution function]] given by\n:<math>\\Phi(x) = \\int_{-\\infty}^{x} \\phi(t)\\ dt = \\frac{1}{2} \\left[ 1 + \\operatorname{erf} \\left(\\frac{x}{\\sqrt{2}}\\right)\\right]</math>,\n\nwhere \"erf\" is the [[error function]].  Then the probability density function (pdf) of the skew-normal distribution with parameter <math>\\alpha</math> is given by\n:<math>f(x) = 2\\phi(x)\\Phi(\\alpha x). \\,</math>\n\nThis distribution was first introduced by O'Hagan and  Leonard (1976). Approximations to this distribution that are easier to manipulate mathematically have been given by Ashour and Abdel-Hamid (2010) and by Mudholkar and Hutson (2000).\n\nA stochastic process that underpins the distribution was described by Andel, Netuka and Zvara (1984).<ref>[http://dml.cz/bitstream/handle/10338.dmlcz/124493/Kybernetika_20-1984-2_1.pdf Andel, J., Netuka, I. and Zvara, K. (1984) On threshold autoregressive processes. Kybernetika, 20, 89-106]</ref> Both the distribution and its stochastic process underpinnings were consequences of the symmetry argument developed in Chan and Tong (1986), which applies to multivariate cases beyond normality, e.g. skew multivariate t distribution and others. The distribution is a particular case of a general class of distributions with probability density functions of the form ''f(x)=2 &phi;(x) &Phi;(x)'' where ''&phi;()'' is any [[Probability density function|PDF]] symmetric about zero and ''&Phi;()'' is any [[Cumulative distribution function|CDF]] whose PDF is symmetric about zero.<ref Name=\"Azzalini1985\">{{cite journal |last=Azzalini |first=A. |authorlink= |year=1985 |title=A class of distributions which includes the normal ones|journal=Scandinavian Journal of Statistics |volume=12 |issue= |pages=171–178}}</ref>\n\nTo add [[location parameter|location]] and [[scale parameter|scale]] parameters to this, one makes the usual transform <math>x\\rightarrow\\frac{x-\\xi}{\\omega}</math>. One can verify that the normal distribution is recovered when <math>\\alpha = 0</math>, and that the absolute value of the [[skewness]] increases as the absolute value of <math>\\alpha</math> increases. The distribution is right skewed if <math>\\alpha>0</math> and is left skewed if <math>\\alpha<0</math>. The probability density function with location <math>\\xi</math>, scale <math>\\omega</math>, and parameter <math>\\alpha</math> becomes\n:<math>f(x) = \\frac{2}{\\omega}\\phi\\left(\\frac{x-\\xi}{\\omega}\\right)\\Phi\\left(\\alpha \\left(\\frac{x-\\xi}{\\omega}\\right)\\right). \\,</math>\nNote, however, that the skewness (<math> \\gamma_1 </math>) of the distribution is limited to the interval <math>(-1,1)</math>.\n\nAs has been shown <ref Name=\"Azzalini2014\">{{cite book|first1=Adelchi|last1=Azzalini|first2=Antonella|last2=Capitanio |year=2014 |title=The skew-normal and related families|isbn=978-1-107-02927-9 |pages=32–33}}</ref>, the mode (maximum) of the distribution is unique.   For general <math> \\alpha </math> there's no analytic expression for <math> m_o </math> , but a quite accurate (numerical) approximation is:\n\n:<math> m_o (\\alpha) \\approx \\mu_z - \\frac{\\gamma_1 \\sigma_z}{2} - \\frac{\\mathrm{sgn}(\\alpha)}{2} \\exp\\left(-\\frac{2 \\pi}{|\\alpha |}\\right) </math>\n\nwhere <math> \\mu_z = \\sqrt{\\frac{2}{\\pi}} \\delta </math> and <math> \\sigma_z = \\sqrt{1- \\mu_z^2} </math>\n\n==Estimation==\n\n[[Maximum likelihood]] estimates for <math>\\xi</math>, <math>\\omega</math>, and <math>\\alpha</math> can be computed numerically, but no closed-form expression for the estimates is available unless <math>\\alpha=0</math>.  If a closed-form expression is needed, the [[Method of moments (statistics)|method of moments]] can be applied to estimate <math>\\alpha</math> from the sample skew, by inverting the skewness equation.  This yields the estimate\n\n:<math>|\\delta| = \\sqrt{\\frac{\\pi}{2} \\frac{  |\\hat{\\gamma}_1|^{\\frac{2}{3}}  }{|\\hat{\\gamma}_1|^{\\frac{2}{3}}+((4-\\pi)/2)^\\frac{2}{3}}}</math>\n\nwhere <math>\\delta = \\frac{\\alpha}{\\sqrt{1+\\alpha^2}}</math>, and <math>\\hat{\\gamma}_1</math> is the sample skew.  The sign of <math>\\delta</math> is the same as the sign of <math>\\hat{\\gamma}_1</math>.  Consequently, <math>\\hat{\\alpha} = \\delta/\\sqrt{1-\\delta^2}</math>.\n\nThe  maximum (theoretical) skewness is obtained by setting <math>{\\delta = 1}</math> in the skewness equation, giving <math>\\gamma_1 \\approx 0.9952717</math>. However it is possible that the sample skewness is larger, and then <math>\\alpha</math> cannot be determined from these equations. When using the method of moments in an automatic fashion, for example to give starting values for maximum likelihood iteration, one should therefore let (for example) <math>|\\hat{\\gamma}_1| = \\min(0.99, |(1/n)\\sum{((x_i-\\bar{x})/s)^3}|)</math>.\n\nConcern has been expressed about the impact of skew normal methods on the reliability of inferences based upon them.<ref>[http://www.tandfonline.com/doi/pdf/10.1080/02664760050120542 Pewsey, Arthur. \"Problems of inference for Azzalini's skewnormal distribution.\" Journal of Applied Statistics 27.7 (2000): 859-870]</ref>\n\n==Related distributions==\n\nThe [[exponentially modified Gaussian distribution|exponentially modified normal distribution]] is another 3-parameter distribution that is a generalization of the normal distribution to skewed cases. The skew normal still has a normal-like tail in the direction of the skew, with a shorter tail in the other direction; that is, its density is asymptotically proportional to <math>e^{-kx^2}</math> for some positive <math>k</math>. Thus, in terms of the [[seven states of randomness]], it shows \"proper mild randomness\". In contrast, the exponentially modified normal has an exponential tail in the direction of the skew; its density is asymptotically proportional to <math>e^{-k|x|}</math>. In the same terms, it shows \"borderline mild randomness\".\n\nThus, the skew normal is useful for modeling skewed distributions which nevertheless have no more outliers than the normal, while the exponentially modified normal is useful for cases with an increased incidence of outliers in (just) one direction.\n\n==See also==\n\n* [[Generalized normal distribution]]\n* [[Log-normal distribution]]\n\n==References==\n{{Reflist}}\n\n* Andel, J., Netuka, I. and Zvara, K. (1984). On threshold autoregressive processes. Kybernetika, 20, 89-106 [http://dml.cz/bitstream/handle/10338.dmlcz/124493/Kybernetika_20-1984-2_1.pdf].\n* Ashour, S., and Abdel-Hamid, M. (2010). Approximate skew normal distribution. Journal of Advanced Research, 1, 341-350.\n* Chan, K-S. and Tong, H. (1986). A note on certain integral equations associated with non-linear time series analysis. Probability and Related Fields, 73, 153-158.\n* O'Hagan, A. and Leonard, T. (1976). Bayes estimation subject to uncertainty about parameter constraints. Biometrika, 63, 201-202.\n* Mudholkar, G. S. and Hutson, A. D. (2000) The epsilon-skew-normal distribution for analyzing near-normal data. Journal of Statistical Planning and Inference, 83, 291-309.\n\n==External links==\n* [http://biomet.oxfordjournals.org/content/83/4/715.full.pdf The multi-variate skew-normal distribution with an application to body mass, height and Body Mass Index]\n* [http://azzalini.stat.unipd.it/SN/Intro/intro.html A very brief introduction to the skew-normal distribution]\n* [http://azzalini.stat.unipd.it/SN/ The Skew-Normal Probability Distribution (and related distributions, such as the skew-t)]\n* [http://people.sc.fsu.edu/~burkardt/cpp_src/owens/owens.html OWENS: Owen's T Function]\n* [http://dahoiv.net/master/index.html Closed-skew Distributions - Simulation, Inversion and Parameter Estimation]\n\n{{ProbDistributions|continuous-infinite}}\n{{Statistics|hide}}\n\n{{DEFAULTSORT:Skew Normal Distribution}}\n[[Category:Continuous distributions]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Skewed generalized t distribution",
      "url": "https://en.wikipedia.org/wiki/Skewed_generalized_t_distribution",
      "text": "{{More footnotes|date=September 2015}}\n{{DISPLAYTITLE:Skewed generalized ''t'' distribution}}\n\nIn [[probability]] and [[statistics]], the skewed generalized “t” distribution is a family of continuous [[probability distribution]]s. The distribution was first introduced by Panayiotis Theodossiou<ref name=\"theodossiou\" /> in 1998. The distribution has since been used in different applications.<ref name=\"hansen1\" /><ref name=\"hansen2\" /><ref name=\"mcdonald1\" /><ref name=\"mcdonald2\" /><ref name=\"mcdonald3\" /><ref name=\"savva\" /> There are different parameterizations for the skewed generalized t distribution,<ref name=\"theodossiou\" /><ref name=\"mcdonald2\" /> which we account for in this article.\n\n==Definition==\n\n===Probability density function===\n\n<math>f_{SGT}(x; \\mu, \\sigma, \\lambda, p, q) = \\frac{p}{2 v \\sigma  q^{1/p} B(\\frac{1}{p},q) (\\frac{| x-\\mu + m |^p}{q (v \\sigma)^p (\\lambda sign(x-\\mu + m)+1)^p}+1)^{\\frac{1}{p}+q}}</math>\n\nwhere <math>B</math> is the [[beta function]], <math>\\mu</math> is the location parameter, <math>\\sigma > 0</math> is the scale parameter, <math>-1 < \\lambda < 1</math> is the skewness parameter, and <math>p > 0</math> and <math>q > 0</math> are the parameters that control the kurtosis. Note that <math>m</math> and <math>v</math> are not parameters, but functions of the other parameters that are used here to scale or shift the distribution appropriately to match the various parameterizations of this distribution.\n\nIn the original parameterization<ref name=\"theodossiou\" /> of the skewed generalized t distribution, \n:<math>m = \\frac{2 v \\sigma \\lambda q^{\\frac{1}{p}} B(\\frac{2}{p},q-\\frac{1}{p})}{B(\\frac{1}{p},q)} </math> \nand \n:<math>v = \\frac{q^{-\\frac{1}{p}}}{\\sqrt{ (3 \\lambda^2 + 1)  \\frac{ B ( \\frac{3}{p}, q - \\frac{2}{p} )}{B (\\frac{1}{p}, q )}  -4 \\lambda^2 \\frac{B ( \\frac{2}{p}, q - \\frac{1}{p} )^2}{ B (\\frac{1}{p}, q )^2}}}</math>.\nThese values for <math>m</math> and <math>v</math> yield a distribution with mean of <math>\\mu</math> if <math>pq > 1</math> and a variance of <math>\\sigma^2</math> if <math>pq > 2</math>. In order for <math>m</math> to take on this value however, it must be the case that <math>pq > 1</math>. Similarly, for <math>v</math> to equal the above value, <math>pq > 2</math>.\n\nThe parameterization that yields the simplest functional form of the probability density function sets <math>m = 0</math> and <math>v = 1</math>. This gives a mean of\n:<math>\\mu+ \\frac{2 v \\sigma \\lambda q^{\\frac{1}{p}} B(\\frac{2}{p},q-\\frac{1}{p})}{B(\\frac{1}{p},q)} </math>\nand a variance of\n:<math> \\sigma^2 q^{\\frac{2}{p}} ( (3 \\lambda^2 + 1) \\frac{ B ( \\frac{3}{p}, q - \\frac{2}{p} )}{B (\\frac{1}{p}, q )} -4 \\lambda^2 \\frac{B ( \\frac{2}{p}, q - \\frac{1}{p} )^2}{ B (\\frac{1}{p}, q )^2}) </math>\n\nThe <math>\\lambda</math> parameter controls the skewness of the distribution. To see this, let <math>M</math> denote the mode of the distribution, and note that \n:<math>\n\\int_{-\\infty}^{M}  f_{SGT}(x; \\mu, \\sigma, \\lambda, p, q) dx = \\frac{1-\\lambda}{2}\n</math>\n\nSince <math>-1 < \\lambda < 1</math>, the probability left of the mode, and therefore right of the mode as well, can equal any value in (0,1) depending on the value of <math>\\lambda</math>. Thus the skewed generalized t distribution can be highly skewed as well as symmetric. If <math>-1 < \\lambda < 0</math>, then the distribution is negatively skewed. If <math>0 < \\lambda < 1</math>, then the distribution is positively skewed. If <math>\\lambda = 0</math>, then the distribution is symmetric.\n\nFinally, <math>p</math> and <math>q</math> control the kurtosis of the distribution. As <math>p</math> and <math>q</math> get smaller, the kurtosis increases<ref name=\"theodossiou\" /> (i.e. becomes more leptokurtic). Large values of <math>p</math> and <math>q</math> yield a distribution that is more platykurtic.\n\n===Moments===\n\nLet <math>X</math> be a random variable distributed with the skewed generalized t distribution.The <math>h^{th}</math> moment (i.e. <math>E[(X- E(X))^h]</math>), for <math>pq > h</math>, is:\n<math>\n \\sum_{r=0}^{h} \\binom{h}{r} ((1+\\lambda)^{r+1}+(-1)^r (1-\\lambda)^{r+1} ) ( - \\lambda )^{h-r} \\frac{ (v \\sigma)^h q^{\\frac{h}{p}} B(\\frac{r+1}{p},q-\\frac{r}{p}) B(\\frac{2}{p},q-\\frac{1}{p} )^{h-r}}{ 2^{r-h+1} B(\\frac{1}{p},q)^{h-r+1} }\n</math>\n\nThe mean, for <math>pq > 1</math>, is:\n:<math>\n\\mu + \\frac{2 v \\sigma \\lambda q^{\\frac{1}{p}} B(\\frac{2}{p},q-\\frac{1}{p})}{B(\\frac{1}{p},q)} - m\n</math>\n\nThe variance (i.e. <math>E[(X- E(X))^2]</math>), for <math>pq > 2</math>, is:\n:<math> \n(v \\sigma)^2 q^{\\frac{2}{p}} ( (3 \\lambda^2 + 1) \\frac{ B ( \\frac{3}{p}, q - \\frac{2}{p} )}{B (\\frac{1}{p}, q )} -4 \\lambda^2 \\frac{B ( \\frac{2}{p}, q - \\frac{1}{p} )^2}{ B (\\frac{1}{p}, q )^2})\n</math>\n\nThe skewness (i.e. <math>E[(X- E(X))^3]</math>), for <math>pq > 3</math>, is:\n:<math>\n\\frac{2 q^{3/p} \\lambda  (v \\sigma) ^3}{B(\\frac{1}{p},q)^3} \\Bigg( 8 \\lambda ^2 B(\\frac{2}{p},q-\\frac{1}{p})^3-3 (1+3 \\lambda ^2) B(\\frac{1}{p},q)\n</math>\n:<math>\n\\times B(\\frac{2}{p},q-\\frac{1}{p}) B(\\frac{3}{p},q-\\frac{2}{p})+2\n(1+\\lambda ^2) B(\\frac{1}{p},q)^2 B(\\frac{4}{p},q-\\frac{3}{p}) \\Bigg)\n</math>\n\nThe kurtosis (i.e. <math>E[(X- E(X))^4]</math>), for <math>pq > 4</math>, is:\n:<math>\n\\frac{q^{4/p} (v \\sigma) ^4}{B(\\frac{1}{p},q)^4} \\Bigg( -48 \\lambda ^4 B(\\frac{2}{p},q-\\frac{1}{p})^4+24\n\\lambda ^2 (1+3 \\lambda ^2) B(\\frac{1}{p},q) B(\\frac{2}{p},q-\\frac{1}{p})^2 \n</math>\n:<math>\n\\times B(\\frac{3}{p},q-\\frac{2}{p})-32\n\\lambda ^2 (1+\\lambda ^2) B(\\frac{1}{p},q)^2 B(\\frac{2}{p},q-\\frac{1}{p}) B(\\frac{4}{p},q-\\frac{3}{p})\n</math>\n:<math>\n+(1+10\n\\lambda ^2+5 \\lambda ^4) B(\\frac{1}{p},q)^3 B(\\frac{5}{p},q-\\frac{4}{p})\\Bigg)\n</math>\n\n==Special Cases==\n\nSpecial and limiting cases of the skewed generalized t distribution include the skewed generalized error distribution, the generalized t distribution introduced by McDonald and Newey,<ref name=\"mcdonald3\" /> the skewed t proposed by Hansen,<ref name=\"hansen\" /> the skewed Laplace distribution, the generalized error distribution (also known as the [[generalized normal distribution]]), a skewed normal distribution, the [[student t distribution]], the skewed Cauchy distribution, the [[Laplace distribution]], the [[uniform distribution (continuous)|uniform distribution]], the [[normal distribution]], and the [[Cauchy distribution]]. The graphic below, adapted from Hansen, McDonald, and Newey,<ref name=\"hansen1\" /> shows which parameters should be set to obtain some of the different special values of the skewed generalized t distribution.\n\n[[File:SGTtree.png|thumb|The skewed generalized t distribution tree]]\n\n===Skewed generalized error distribution===\nThe Skewed Generalized Error Distribution has the pdf:\n:<math> \n\\lim_{q\\to\\infty} f_{SGT}(x; \\mu, \\sigma, \\lambda, p, q) \n</math>\n:<math>\n= f_{SGED}(x; \\mu, \\sigma, \\lambda, p) = \\frac{p e^{- ( \\frac{|x - \\mu + m|}{ v \\sigma (1 + \\lambda sign(x - \\mu + m))} )^p }}{2 v \\sigma \\Gamma ( 1/p )}\n</math>\nwhere \n:<math>\nm = \\frac{ 2^{ \\frac{2}{p} } v \\sigma \\lambda \\Gamma ( \\frac{1}{2} + \\frac{1}{p} ) }{ \\sqrt{\\pi}}\n</math>\ngives a mean of <math>\\mu</math>. Also\n:<math>\nv = \\sqrt{\\frac{\\pi  \\Gamma(\\frac{1}{p})}{ \\pi\n (1+3 \\lambda ^2) \\Gamma( \\frac{3}{p})-16^{\\frac{1}{p}} \\lambda ^2 \\Gamma(\\frac{1}{2}+\\frac{1}{p})^2 \\Gamma(\\frac{1}{p})}}\n</math>\ngives a variance of <math>\\sigma^2</math>.\n\n===Generalized t distribution===\nThe Generalized T Distribution has the pdf:  \n:<math> \nf_{SGT}(x; \\mu, \\sigma, \\lambda = 0, p, q) \n</math>\n:<math> \n= f_{GT}(x; \\mu, \\sigma, p, q) = \\frac{p}{2 v \\sigma  q^{1/p} B(\\frac{1}{p},q) (\\frac{\\left| x-\\mu \\right| ^p}{q (v \\sigma) ^p }+1)^{\\frac{1}{p}+q}}\n</math>\nwhere \n:<math>\nv = \\frac{1}{q^{1/p}} \\sqrt{\\frac{ B(\\frac{1}{p},q) }{ B(\\frac{3}{p},q-\\frac{2}{p}) }}\n</math>\ngives a variance of <math>\\sigma^2</math>.\n\n===Skewed t distribution===\nThe Skewed T Distribution has the pdf:\n:<math> \nf_{SGT}(x; \\mu, \\sigma, \\lambda, p = 2, q) \n</math>\n:<math> \n= f_{ST}(x; \\mu, \\sigma, \\lambda, q) = \\frac{\\Gamma ( \\frac{1}{2}+q )}{v \\sigma (\\pi q)^{1/2} \\Gamma (q) (\\frac{\\left| x-\\mu + m \\right| ^2}{q (v \\sigma) ^2 (\\lambda  ~{\\rm sign}(x-\\mu + m)+1)^2}+1)^{\\frac{1}{2}+q}}\n</math>\nwhere \n:<math>\nm = \\frac{2 v \\sigma \\lambda q^{1/2} \\Gamma (q-\\frac{1}{2})}{\\pi^{1/2} \\Gamma (q+\\frac{1}{2})}\n</math>\ngives a mean of <math>\\mu</math>. Also\n:<math>\nv = \\frac{1}{q^{1/2} \\sqrt{ (3 \\lambda^2 + 1) ( \\frac{1}{2q-2} ) -\\frac{4 \\lambda^2}{\\pi} \\left( \\frac{\\Gamma ( q - \\frac{1}{2} )}{ \\Gamma ( q )} \\right)^2 }}\n</math>\ngives a variance of <math>\\sigma^2</math>.\n\n===Skewed Laplace distribution===\nThe Skewed Laplace Distribution has the pdf:\n:<math> \n\\lim_{q\\to\\infty} f_{SGT}(x; \\mu, \\sigma, \\lambda, p = 1, q) \n</math>\n:<math>\n= f_{SLaplace}(x; \\mu, \\sigma, \\lambda) = \\frac{e^{ \\frac{- |x - \\mu + m|}{ v \\sigma (1 + \\lambda sign(x - \\mu + m))} }}{2 v \\sigma }\n</math>\nwhere \n:<math>\nm = 2 v \\sigma \\lambda\n</math>\ngives a mean of <math>\\mu</math>. Also\n:<math>\nv = [ 2 (1+\\lambda ^2) ]^{-\\frac{1}{2}}\n</math>\ngives a variance of <math>\\sigma^2</math>.\n\n===Generalized error distribution===\nThe Generalized Error Distribution (also known as the [[generalized normal distribution]]) has the pdf:\n:<math> \n\\lim_{q\\to\\infty} f_{SGT}(x; \\mu, \\sigma, \\lambda = 0, p, q) \n</math>\n:<math>\n= f_{GED}(x; \\mu, \\sigma, p) = \\frac{p e^{- ( \\frac{|x - \\mu|}{ v \\sigma} )^p }}{2 v \\sigma \\Gamma ( 1/p )}\n</math>\nwhere \n:<math>\nv = \\sqrt{ \\frac{\\Gamma ( \\frac{1}{p} )}{ \\Gamma ( \\frac{3}{p} )}}\n</math>\ngives a variance of <math>\\sigma^2</math>.\n\n===Skewed normal distribution===\nThe Skewed Normal Distribution has the pdf:\n:<math> \n\\lim_{q\\to\\infty} f_{SGT}(x; \\mu, \\sigma, \\lambda, p = 2, q) \n</math>\n:<math>\n= f_{SNormal}(x; \\mu, \\sigma, \\lambda) = \\frac{e^{- ( \\frac{|x - \\mu + m|}{ v \\sigma (1 + \\lambda sign(x - \\mu + m))} )^2 }}{v \\sigma \\sqrt{\\pi}}\n</math>\nwhere \n:<math>\nm = \\frac{ 2 v \\sigma \\lambda }{ \\sqrt{\\pi}}\n</math>\ngives a mean of <math>\\mu</math>. Also\n:<math>\nv = \\sqrt{\\frac{2 \\pi }{ (\\pi -8 \\lambda ^2+3 \\pi  \\lambda ^2)} }\n</math>\ngives a variance of <math>\\sigma^2</math>.\n\n===Student's t-distribution===\nThe [[Student's t-distribution]] has the pdf:\n:<math> \nf_{SGT}(x; \\mu = 0, \\sigma = 1, \\lambda = 0, p = 2, q = d/2) \n</math>\n:<math> \n= f_{T}(x; d) = \\frac{\\Gamma ( \\frac{d+1}{2} )}{ (\\pi d)^{1/2} \\Gamma (d/2) (\\frac{x^2}{d}+1)^{\\frac{d+1}{2}}}\n</math>\nNote that we substituted <math>v = \\sqrt{2}</math>.\n\n===Skewed Cauchy distribution===\nThe Skewed Cauchy Distribution has the pdf:\n:<math> \nf_{SGT}(x; \\mu, \\sigma, \\lambda, p = 2, q = 1/2) \n</math>\n:<math> \n= f_{SCauchy}(x; \\mu, \\sigma, \\lambda) = \\frac{1}{\\sigma \\pi (\\frac{\\left| x-\\mu \\right| ^2}{ \\sigma ^2 (\\lambda  sign(x-\\mu )+1)^2}+1)}\n</math>\nNote that we substituted <math>v = \\sqrt{2}</math> and <math>m = 0</math>.\n\nAlso note that the mean, variance, skewness, and kurtosis of the skewed Cauchy distribution are all undefined.\n\n===Laplace distribution===\nThe [[Laplace distribution]] has the pdf:\n:<math> \n\\lim_{q\\to\\infty} f_{SGT}(x; \\mu, \\sigma, \\lambda = 0, p = 1, q) \n</math>\n:<math>\n= f_{Laplace}(x; \\mu, \\sigma) = \\frac{e^{ \\frac{- |x - \\mu|}{ \\sigma } }}{2 \\sigma }\n</math>\nNote that we substituted <math>v = 1</math>.\n\n===Uniform Distribution===\nThe [[Uniform distribution (continuous)|Uniform distribution]] has the pdf:\n:<math> \n\\lim_{p\\to\\infty} f_{SGT}(x; \\mu, \\sigma, \\lambda, p, q) \n</math>\n:<math>\n = f(x)=\\begin{cases}\n  \\frac{1}{2 v \\sigma} & |x - \\mu| < v \\sigma \\\\\n  0 & \\mathrm{otherwise}\n  \\end{cases} \n</math>\nThus the standard uniform parameterization is obtained if <math>\\mu = \\frac{a+b}{2}</math>, <math>v = 1</math>, and <math>\\sigma = \\frac{b-a}{2}</math>.\n\n===Normal distribution===\nThe [[Normal distribution]] has the pdf:\n:<math> \n\\lim_{q\\to\\infty} f_{SGT}(x; \\mu, \\sigma, \\lambda = 0, p = 2, q) \n</math>\n:<math>\n= f_{Normal}(x; \\mu, \\sigma) = \\frac{e^{- ( \\frac{|x - \\mu|}{ v \\sigma } )^2 }}{v \\sigma \\sqrt{\\pi}}\n</math>\nwhere \n:<math>\nv = \\sqrt{2}\n</math>\ngives a variance of <math>\\sigma^2</math>.\n\n===Cauchy Distribution===\nThe [[Cauchy distribution]] has the pdf:\n:<math> \nf_{SGT}(x; \\mu, \\sigma, \\lambda = 0, p = 2, q = 1/2) \n</math>\n:<math> \n= f_{Cauchy}(x; \\mu, \\sigma) = \\frac{1}{ \\sigma \\pi (  ( \\frac{x- \\mu}{\\sigma} )^2+1)}\n</math>\nNote that we substituted <math>v = \\sqrt{2}</math>.\n\n==References==\n*{{Cite journal\n| last1 = Hansen | first1 = B.\n| year = 1994\n| title = Autoregressive Conditional Density Estimation\n| journal = [[International Economic Review]]\n| volume = 35\n| issue = 3\n| pages = 705–730\n | doi=10.2307/2527081\n| jstor = 2527081\n}}\n*{{Cite journal\n| last1 = Hansen | first1 = C. \n| last2 = McDonald | first2 = J. \n| last3 = Newey | first3 = W.\n| year =  2010\n| title = Instrumental Variables Estimation with Flexible Distributions\n| journal = [[Journal of Business and Economic Statistics]] \n| volume = 28\n| pages = 13–25\n | doi=10.1198/jbes.2009.06161\n}}\n*{{Cite journal\n| last1 = Hansen | first1 = C.\n| last2 = McDonald | first2 = J.\n| last3 = Theodossiou | first3 = P.\n| year = 2007\n| title = Some Flexible Parametric Models for Partially Adaptive Estimators of Econometric Models\n| journal = Economics: The Open-Access, Open-Assessment E-Journal\n| volume = 1\n| issue = 2007–7\n| pages = 1\n| doi = 10.5018/economics-ejournal.ja.2007-7\n}}\n*{{Cite journal\n| last1 = McDonald | first1 = J.\n| last2 = Michefelder | first2 = R.\n| last3 = Theodossiou | first3 = P.\n| year = 2009\n| title = Evaluation of Robust Regression Estimation Methods and Intercept Bias: A Capital Asset Pricing Model Application\n| journal = Multinational Finance Journal\n| volume = 15\n| issue = 3/4\n| pages = 293–321\n| doi = 10.17578/13-3/4-6\n}}\n*{{Cite journal\n| last1 = McDonald | first1 = J. \n| last2 = Michelfelder | first2 = R.\n| last3 = Theodossiou | first3 = P.\n| year = 2010\n| title = Robust Estimation with Flexible Parametric Distributions: Estimation of Utility Stock Betas\n| journal = Quantitative Finance\n| volume = 10 \n| issue = 4 \n| pages = 375–387\n| doi = 10.1080/14697680902814241 \n}}\n*{{Cite journal\n| last1 = McDonald | first1 = J.\n| last2 = Newey | first2 = W.\n| year = 1988\n| title = Partially Adaptive Estimation of Regression Models via the Generalized t Distribution\n| journal = [[Econometric Theory]] \n| volume = 4\n| issue = 3\n| pages = 428–457\n | doi=10.1017/s0266466600013384\n}}\n*{{Cite journal\n| last1 = Savva | first1 = C.\n| last2 = Theodossiou | first2 = P.\n| year = 2015\n| title = Skewness and the Relation between Risk and Return\n|journal = [[Management Science (journal)|Management Science]]\n}}\n*{{Cite journal\n| last1 = Theodossiou | first1 = P.\n| year = 1998\n| title = Financial Data and the Skewed Generalized T Distribution\n|journal = [[Management Science (journal)|Management Science]] \n| volume = 44\n| issue = 12–part–1\n| pages = 1650–1661\n | doi=10.1287/mnsc.44.12.1650\n}}\n\n==External links==\n*[https://cran.r-project.org/web/packages/sgt/vignettes/sgt.pdf outlines skewed generalized t distribution, its special cases, and a program to calculate its pdf, cdf, and critical values]\n\n==Notes==\n{{Reflist|30em|refs=\n\n*<ref name=\"hansen\">{{cite journal | last1 = Hansen | first1 = B | year = 1994 | title = Autoregressive Conditional Density Estimation | url = | journal = International Economic Review | volume = 35 | issue = 3| pages = 705–730 | doi=10.2307/2527081| jstor = 2527081 }}</ref>\n*<ref name=\"hansen1\">{{cite journal | last1 = Hansen | first1 = C. | last2 = McDonald | first2 = J. | last3 = Newey | first3 = W. | year = 2010 | title = Instrumental Variables Estimation with Flexible Distributions | url = | journal = Journal of Business and Economic Statistics | volume = 28 | issue = | pages = 13–25 | doi=10.1198/jbes.2009.06161}}</ref>\n*<ref name=\"hansen2\">Hansen, C., J. McDonald, and P. Theodossiou (2007) \"Some Flexible Parametric Models for Partially Adaptive Estimators of Econometric Models\" ''Economics: The Open-Access, Open-Assessment E-Journal''</ref>\n*<ref name=\"mcdonald1\">{{cite journal | last1 = McDonald | first1 = J. | last2 = Michelfelder | first2 = R. | last3 = Theodossiou | first3 = P. | year = 2009 | title = Evaluation of Robust Regression Estimation Methods and Intercept Bias: A Capital Asset Pricing Model Application | url = | journal = Multinational Finance Journal | volume = 15 | issue = 3/4| pages = 293–321 | doi = 10.17578/13-3/4-6 }}</ref>\n*<ref name=\"mcdonald2\">McDonald J., R. Michelfelder, and P. Theodossiou (2010) \"Robust Estimation with Flexible Parametric Distributions: Estimation of Utility Stock Betas\" ''Quantitative Finance'' 375-387.</ref>\n*<ref name=\"mcdonald3\">{{cite journal | last1 = McDonald | first1 = J. | last2 = Newey | first2 = W. | year = 1998 | title = Partially Adaptive Estimation of Regression Models via the Generalized t Distribution | url = | journal = Econometric Theory | volume = 4 | issue = 3| pages = 428–457 | doi = 10.1017/S0266466600013384 }}</ref>\n*<ref name=\"savva\">Savva C. and P. Theodossiou (2015) \"Skewness and the Relation between Risk and Return\" ''Management Science'', forthcoming.</ref>\n*<ref name=\"theodossiou\">{{cite journal | last1 = Theodossiou | first1 = P | year = 1998 | title = Financial Data and the Skewed Generalized T Distribution | url = | journal = Management Science | volume = 44 | issue = 12–part–1| pages = 1650–1661 | doi=10.1287/mnsc.44.12.1650}}</ref>\n}}\n\n{{ProbDistributions|continuous-infinite}}\n{{Statistics|state=collapsed}}\n\n{{DEFAULTSORT:Skewed generalized t distribution}}\n[[Category:Continuous distributions]]\n[[Category:Normal distribution]]\n[[Category:Probability distributions with non-finite variance]]\n[[Category:Location-scale family probability distributions]]"
    },
    {
      "title": "Slash distribution",
      "url": "https://en.wikipedia.org/wiki/Slash_distribution",
      "text": "{{Probability distribution|\n   name       =Slash|\n   type       =density|\n   pdf_image  =[[File:Slashpdf.svg|275px|center]] |\n   cdf_image  =[[File:Slashcdf.svg|275px|center]]|\n   parameters =none|\n   support    =<math>x\\in(-\\infty,\\infty)</math>|\n   pdf        =<math>\\begin{cases}\n\\frac{\\varphi(0) - \\varphi(x)}{x^2} &  x \\ne 0 \\\\\n\\frac{1}{2\\sqrt{2\\pi}} & x = 0 \\\\\n\\end{cases}</math> |\n   cdf        =<math>\\begin{cases}\n\\Phi(x) - \\left[ \\varphi(0) - \\varphi(x) \\right] / x &  x \\ne 0 \\\\\n1 / 2 & x = 0 \\\\\n\\end{cases}</math> |\n   mean       =Does not exist|\n   median     =0|\n   mode       =0|\n   variance   =Does not exist|\n   skewness   =Does not exist|\n   kurtosis   =Does not exist|\n   entropy    =|\n   mgf        =Does not exist |\n   char       = <math>\\sqrt{2\\pi}\\Big(\\varphi(t)+t\\Phi(t)-\\max\\{t,0\\}\\Big)</math> |\n }}\nIn [[probability theory]], the '''slash distribution''' is the [[probability distribution]] of a standard [[normal distribution|normal]] variate divided by an independent [[uniform distribution (continuous)#Standard uniform|standard uniform]] variate.<ref>{{cite book|last1=Davison|first1=Anthony Christopher|last2=Hinkley|first2=D. V.|authorlink2=David V. Hinkley|title=Bootstrap methods and their application |publisher=Cambridge University Press|url=http://www.cambridge.org/us/knowledge/isbn/item1154176/?site_locale=en_US |date=1997|isbn=978-0-521-57471-6|page=484|accessdate=24 September 2012}}</ref> In other words, if the [[random variable]] ''Z'' has a normal distribution with zero mean and unit [[variance]], the random variable ''U'' has a uniform distribution on [0,1] and ''Z'' and ''U'' are [[statistically independent]], then the random variable ''X'' =&nbsp;''Z''&nbsp;/&nbsp;''U'' has a slash distribution. The slash distribution is an example of a [[ratio distribution]]. The distribution was named by William H. Rogers and [[John Tukey]] in a paper published in 1972.<ref>{{Cite journal| last1 = Rogers | first1 = W. H.| last2 = Tukey | first2 = J. W.| authorlink2 = John Tukey| title = Understanding some long-tailed symmetrical distributions| journal = Statistica Neerlandica | volume = 26| issue = 3 | pages = 211–226 | year = 1972 | doi = 10.1111/j.1467-9574.1972.tb00191.x}}</ref>\n\nThe [[probability density function]] (pdf) is\n\n:<math> f(x) = \\frac{\\varphi(0) - \\varphi(x)}{x^2}.</math>\n\nwhere ''&phi;''(''x'') is the probability density function of the standard normal distribution.<ref name=nist />  The result is undefined at ''x''&nbsp;=&nbsp;0, but the [[removable discontinuity|discontinuity is removable]]:\n\n: <math> \\lim_{x\\to 0} f(x) = \\frac{\\varphi(0)}{2} = \\frac{1}{2\\sqrt{2\\pi}} </math>\n\nThe most common use of the slash distribution is in [[simulation]] studies. It is a useful distribution in this context because it has [[heavy tail|heavier tails]] than a normal distribution, but it is not as [[pathological (mathematics)|pathological]] as the [[Cauchy distribution]].<ref name=nist>{{cite web|url=http://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/slapdf.htm|title=SLAPDF|publisher=Statistical Engineering Division, National Institute of Science and Technology|accessdate=2009-07-02}}</ref>\n\n==References==\n<references />\n\n{{NIST-PD}}\n\n{{ProbDistributions|continuous-infinite}}\n\n[[Category:Continuous distributions]]\n[[Category:Normal distribution]]\n[[Category:Probability distributions with non-finite variance]]"
    },
    {
      "title": "Split normal distribution",
      "url": "https://en.wikipedia.org/wiki/Split_normal_distribution",
      "text": "In [[probability theory]] and [[statistics]], the '''split normal distribution''' also known as the '''two-piece normal distribution''' results from joining at the mode the corresponding halves of two [[normal distributions]] with the same [[Mode (statistics)|mode]] but different [[Variance | variances]].  It is claimed by Johnson et al.<ref name=\"Johnson1994\" /> that this distribution was introduced by Gibbons and Mylroie<ref name=Gibbons/> and by John.<ref name=\"John1982\" />  But these are two of several independent rediscoveries of the Zweiseitige Gauss'sche Gesetz introduced in the posthumously published ''Kollektivmasslehre'' (1897)<ref>Fechner, G.T. (ed. Lipps, G.F.) (1897). ''Kollectivmasslehre''. Engelmann, Leipzig.</ref> of [[Gustav Theodor Fechner]] (1801-1887), see Wallis (2014).<ref>Wallis, K.F. (2014). The two-piece normal, binormal, or double Gaussian distribution: its origin and rediscoveries. ''Statistical Science'', vol. 29, no. 1, pp.106-112. doi:10.1214/13-STS417.</ref> Surprisingly, another rediscovery has appeared more recently in a finance journal.<ref>de Roon, F. and Karehnke, P. (2016).  A simple skewed distribution with asset pricing applications. ''[[European Finance Association|Review of Finance]]'', 2016, 1-29.</ref>\n\n{{Probability distribution\n  | name       = Split-normal\n  | type       = density\n<!--  | pdf_image  = [[Image:Normal Distribution PDF.svg|350px|Probability density function for the normal distribution]]<br /><small>The red curve is the ''standard normal distribution''</small>\n  | cdf_image  = [[Image:Normal Distribution CDF.svg|350px|Cumulative distribution function for the normal distribution]]<br /> -->\n  | notation   = <math>\\mathcal{SN}(\\mu,\\,\\sigma_1,\\sigma_2)</math>\n  | parameters = <math>\\mu \\in \\Re </math> — [[Mode (statistics)|mode]] ([[location parameter|location]], [[real number|real]])<br/><math>\\sigma_1 > 0 </math> — left-hand-side [[standard deviation]] ([[scale parameter|scale]], [[real number|real]])<br/><math>\\sigma_2 > 0 </math> — right-hand-side [[standard deviation]] ([[scale parameter|scale]], [[real number|real]])\n  | support    = <math>x \\in \\Re </math>|\n  | pdf        = <math> A \\exp \\left(- \\frac {(x-\\mu)^2}{2 \\sigma_1^2}\\right) \\quad \\text{if } x< \\mu</math>\n<br/><math> A \\exp \\left(- \\frac {(x-\\mu)^2}{2 \\sigma_2^2}\\right) \\quad \\text{otherwise,}</math>\n<br/><math> \\text{where} \\quad A= \\sqrt{2/\\pi} (\\sigma_1+\\sigma_2)^{-1}</math>\n  | mode       = <math> \\mu </math>\n  | mean       = <math> \\mu+\\sqrt{2 / \\pi}(\\sigma_2-\\sigma_1)</math>\n  | variance   = <math>(1-2 / \\pi)(\\sigma_2-\\sigma_1)^2 + \\sigma_1 \\sigma_2</math>\n  | skewness   = <math> \\gamma_3 = \\sqrt{\\frac{2}{\\pi}}(\\sigma_2-\\sigma_1)\\left[\\left(\\frac{4}{\\pi}-1\\right)(\\sigma_2-\\sigma_1)^2 + \\sigma_1 \\sigma_2\\right]</math>\n  }}\n\n== Definition ==\nThe split normal distribution arises from merging two opposite halves of two [[probability density function]]s (PDFs) of [[normal distribution]]s in their common [[Mode (statistics)|mode]].\n\nThe PDF of the split normal distribution is given by<ref name=Johnson1994/>\n\n:<math>f(x;\\mu,\\sigma_1,\\sigma_2)= A \\exp \\left(- \\frac {(x-\\mu)^2}{2 \\sigma_1^2}\\right) \\quad \\text{if } x< \\mu\n</math>\n:<math> f(x;\\mu,\\sigma_1,\\sigma_2)=   A \\exp \\left(- \\frac {(x-\\mu)^2}{2 \\sigma_2^2}\\right) \\quad \\text{otherwise}\n</math>\nwhere\n:<math>\\quad A= \\sqrt{2/\\pi} (\\sigma_1+\\sigma_2)^{-1}.</math>\n\n=== Discussion ===\nThe split normal distribution results from merging two halves of normal distributions. In a general case the 'parent' normal distributions can have different variances which implies that the joined PDF would not be [[continuous function|continuous]]. To ensure that the resulting PDF [[Integral|integrates]] to 1, the [[normalizing constant]] '''A''' is used.\n\nIn a special case when <math>\\sigma_1^2=\\sigma_2^2=\\sigma_{*}^2</math> the split normal distribution reduces to [[normal distribution]] with variance <math>\\sigma_{*}^2</math>.\n\nWhen σ<sub>2</sub>≠σ<sub>1</sub> the constant '''A''' it is different from the constant of normal distribution. However, when <math>\\sigma_1^2=\\sigma_2^2=\\sigma_{*}^2</math> the constants are equal.\n\nThe sign of its third central moment is determined by the difference (σ<sub>2</sub>-σ<sub>1</sub>). If this difference is positive, the distribution is skewed to the right and if negative, then it is skewed to the left.\n\nOther properties of the split normal density were discussed by Johnson et al.<ref name=\"Johnson1994\" /> and Julio.<ref name=\"Julio2007\" />\n\n== Alternative formulations ==\nThe formulation discussed above originates from John.<ref name=\"John1982\" /> The literature offers two mathematically equivalent alternative parameterizations . Britton, Fisher and Whitley<ref name=\"BFW1998\" /> offer a parameterization if terms of mode, dispersion and normed skewness, denoted with <math>\\mathcal{SN}(\\mu,\\, \\sigma^2,\\gamma)</math>. The parameter μ is the mode and has equivalent to the mode in John’s formulation. The parameter σ <sup>2</sup>>0 informs about the dispersion (scale) and should not be confused with variance. The third parameter, γ ∈ (-1,1), is the normalized  skew.\n\nThe second alternative parameterization is used in the [[Bank of England|Bank of England’s]] communication and is written in terms of mode, dispersion and unnormed skewness and is denoted with <math>\\mathcal{SN}(\\mu,\\, \\sigma^2,\\xi)</math>. In this formulation  the parameter μ is the mode and is identical as in John’s <ref name=\"John1982\" /> and Britton, Fisher and Whitley’s <ref name=\"BFW1998\" /> formulation. The parameter σ <sup>2</sup> informs about the dispersion (scale) and is the same as in the Britton, Fisher and Whitley’s formulation. The parameter ξ equals the difference between the distribution’s mean and mode and can be viewed as unnormed measure of skewness.\n\nThe three parameterizations are mathematically equivalent, meaning that there is a strict relationship between the parameters and that it is possible to go from one parameterization to another. The following relationships hold:<ref name=\"BD2011\" />\n:<math>\\begin{align}\n\\sigma^2 &= \\sigma_1^2(1+\\gamma)= \\sigma_2^2(1-\\gamma) \\\\\n\\gamma   &= \\frac{\\sigma_2-\\sigma_1}{\\sigma_2+\\sigma_1} \\\\\n\\xi             &=\\sqrt{2 / \\pi}(\\sigma_2-\\sigma_1) \\\\\n\\gamma   &= \\operatorname{sgn}(\\xi) \\sqrt{1-\\left( \\frac{\\sqrt{1+2\\beta}-1}{\\beta} \\right)^2}, \\quad \\text{where} \\quad \\beta = \\frac{\\pi\\xi^2}{2\\sigma^2}.\n\\end{align}</math>\n\n== Multivariate Extensions ==\nThe multivariate generalization of the split normal distribution was proposed by Villani and Larsson.<ref name=\"VL2006\"/> They assume that each of the [[Principal component analysis|principal components]] has univariate split normal distribution with a different set of parameters μ, σ<sub>2</sub> and σ<sub>1</sub>.\n\n== Estimation of parameters ==\nJohn<ref name=\"John1982\" /> proposes to estimate the parameters using [[maximum likelihood]] method. He shows that the likelihood function can be expressed in an intensive form, in which the scale parameters σ<sub>1</sub>  and σ<sub>2</sub> are a function of the location parameter μ. The likelihood in its intensive form is:\n:<math> L(\\mu) = -\\left[\\sum_{x_i: x_i<\\mu}  (x_i-\\mu)^2 \\right]^{1/3} - \\left[\\sum_{x_i: x_i>\\mu}  (x_i-\\mu)^2 \\right]^{1/3}</math>\nand has to be maximized numerically with respect to a single parameter μ only.\n\nGiven the maximum likelihood estimator <math>\\hat{\\mu}</math> the other parameters take values:\n:<math> \\hat{\\sigma}_1^2 = \\frac{-L(\\mu)}{N} \\left[\\sum_{x_i: x_i<\\mu}  (x_i-\\mu)^2 \\right]^{2/3},</math>\n:<math> \\hat{\\sigma}_2^2 = \\frac{-L(\\mu)}{N} \\left[\\sum_{x_i: x_i>\\mu}  (x_i-\\mu)^2 \\right]^{2/3},</math>\nwhere '''N''' is the number of observations.\n\nVillani and Larsson<ref name=\"VL2006\" /> propose to use either  [[maximum likelihood]] method or [[Bayes estimator|bayesian estimation]] and provide some analytical results for either univariate and multivariate case.\n\n== Applications ==\nThe split normal distribution has been used mainly in econometrics and time series. A remarkable area of application is the construction of the [[Fan chart (time series)|fan chart]], a representation of the [[inflation]] forecast distribution reported by [[inflation targeting]] central banks around the globe.<ref name=\"Julio2007\"/><ref name=\"BoEInfRep\"/>\n\n== References ==\n{{Reflist|refs=\n<ref name=\"BoEInfRep\">Bank of England, ''[http://www.bankofengland.co.uk/publications/inflationreport/irfanch.htm Inflation Report] {{webarchive|url=https://web.archive.org/web/20100813232522/http://www.bankofengland.co.uk/publications/inflationreport/irfanch.htm |date=2010-08-13 }}''</ref>\n\n<ref name=Gibbons>{{cite journal\n|last1=Gibbons|first1=J.F.\n|last2=Mylroie|first2=S.\n|year=1973\n|title=Estimation of impurity profiles in ion-implanted amorphous targets using joined half-Gaussian distributions\n|journal= Applied Physics Letters\n|volume= 22 |issue=11\n|pages= 568&ndash;569|doi=10.1063/1.1654511}}\n</ref>\n\n<ref name=BD2011>{{Cite conference\n| last = Banerjee\n| first = N.\n|author2=A. Das\n | title = Fan Chart: Methodology and its Application to Inflation Forecasting in India\n| series = Reserve Bank of India Working Paper Series\n| year = 2011\n}}\n</ref>\n\n<ref name=BFW1998>{{Cite journal\n| volume = February 1998\n| pages =  30–37\n| last = Britton\n| first = E. |author2=P. Fisher |author3=Whitley, J.\n| title = The inflation report projections: understanding the fan chart\n| journal = Quarterly Bulletin\n| year = 1998\n}}\n</ref>\n\n<ref name=VL2006>{{Cite journal\n| issn = 0361-0926\n| volume = 35\n| issue = 6\n| pages = 1123–1140\n| last = Villani\n| first = Mattias\n|author2=Rolf Larsson\n | title = The Multivariate Split Normal Distribution and Asymmetric Principal Components Analysis\n| journal = Communications in Statistics - Theory and Methods\n| year = 2006\n| doi=10.1080/03610920600672252\n| citeseerx = 10.1.1.533.4095\n}}\n</ref>\n\n<ref name=\"John1982\">{{Cite journal\n| journal = Communications in Statistics - Theory and Methods\n| volume = 11\n| issue = 8\n| pages = 879–885\n| last = John |first=S.\n| title = The three-parameter two-piece normal family of distributions and its fitting\n| year = 1982\n| doi=10.1080/03610928208828279\n}}</ref>\n\n<ref name=\"Johnson1994\">{{Cite book\n| publisher = John Wiley & Sons\n| last =  Johnson, N.L., Kotz, S. and Balakrishnan, N.\n| title = Continuous Univariate Distributions, Volume 1\n|page=173 |isbn=978-0-471-58495-7\n| year = 1994\n}}</ref>\n\n<ref name=\"Julio2007\">{{Cite conference\n| publisher = Banco de la República\n| last = Juan Manuel Julio\n| title = The Fan Chart: The Technical Details Of The New Implementation\n| accessdate = 2010-09-11\n| year = 2007\n| url = http://ideas.repec.org/p/col/000094/004294.html\n | postscript =, [http://www.banrep.gov.co/docum/ftp/borra468.pdf direct link]\n}}</ref>\n\n}}\n{{ProbDistributions|continuous-infinite}}\n\n[[Category:Continuous distributions]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Standard normal deviate",
      "url": "https://en.wikipedia.org/wiki/Standard_normal_deviate",
      "text": "A '''standard normal deviate''' is a [[normal distribution|normally distributed]] [[deviate (statistics)|deviate]]. It is a [[realization (statistics)|realization]] of a '''standard normal random variable''', defined as a [[random variable]] with [[expected value]]&nbsp;0 and [[variance]]&nbsp;1.<ref>Dodge, Y. (2003) The Oxford Dictionary of Statistical Terms. OUP. {{isbn|0-19-920613-9}}</ref> Where collections of such random variables are used, there is often an associated (possibly unstated) assumption that members of such collections are [[statistically independent]].\n\nStandard normal variables play a major role in theoretical statistics in the description of many types of model, particularly in [[regression analysis]], the [[analysis of variance]] and [[time series analysis]].\n\nWhen the term \"deviate\" is used, rather than \"variable\", there is a connotation that the value concerned is treated as the no-longer-random outcome of a standard norm random variable. The terminology here is the same as that for [[random variable]] and [[random variate]]. Standard normal deviates arise in practical [[statistics]] in two ways.\n:*Given a model for a set of observed data, a set of manipulations of the data can result in a derived quantity which, assuming that the model is a true representation of reality, is a standard normal deviate (perhaps in an approximate sense). This enables a [[significance test]] to be made for the validity of the model.\n:*In the computer generation of a [[pseudorandom number sequence]], the aim may be to generate random numbers having a [[normal distribution]]: these can be obtained from standard normal deviates (themselves the output of a pseudorandom number sequence) by multiplying by the scale parameter and adding the location parameter. More generally, the generation of pseudorandom number sequence having other [[marginal distribution]]s may involve manipulating sequences of standard normal deviates: an example here is the [[chi-squared distribution]], random values of which can be obtained by adding the squares of standard normal deviates (although this would seldom be the fastest method of generating such values).\n\n==See also==\n*[[Standard normal table]]\n\n==References==\n<references/>\n\n{{statistics-stub}}\n\n[[Category:Normal distribution]]"
    },
    {
      "title": "Standard normal table",
      "url": "https://en.wikipedia.org/wiki/Standard_normal_table",
      "text": "A '''standard normal table''', also called the '''unit normal table''' or '''Z table'''<ref>{{cite web|title=Z Table. History of Z Table. Z Score|url=https://www.ztable.net/|accessdate=21 December 2018}}</ref>, is a [[mathematical table]] for the values of Φ, which are the values of the [[cumulative distribution function]] of the [[normal distribution]].  It is used to find the [[probability]] that a [[statistic]] is observed below, above, or between values on the [[standard normal distribution]], and by extension, any [[normal distribution]].  Since probability tables cannot be printed for every normal distribution, as there are an infinite variety of normal distributions, it is common practice to convert a normal to a standard normal and then use the standard normal table to find probabilities.<ref>{{cite book |title=Elementary Statistics: Picturing the World|first1=Ron \n|last1=Larson|first2=Elizabeth|last2=Farber|publisher=清华大学出版社|year=2004|isbn=7-302-09723-2|page=214}}</ref>\n\n==Normal and standard normal distribution==\n[[Normal distributions]] are symmetrical, bell-shaped distributions that are useful in describing real-world data. The ''standard'' normal distribution, represented by the letter Z, is the normal distribution having a [[mean]] of 0 and a [[standard deviation]] of 1.\n\n===Conversion===\n{{main|Standard normal deviate}}\nIf ''X'' is a random variable from a normal distribution with mean μ and standard deviation σ, its Z-score may be calculated from X by subtracting μ and dividing by the standard deviation:\n\n: <math>Z = \\frac{X - \\mu}{\\sigma } </math>\n\nFor the average of a sample of size n from some population in which the mean is μ and the standard deviation is σ, the standard error is σ/&radic;''n'':\n\n: <math>Z = \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} </math>\n\n==Reading a Z table==\n\n===Formatting / layout===\nZ tables are typically composed as follows:\n* The label for rows contains the integer part and the first decimal place of Z.\n* The label for columns contains the second decimal place of Z.\n* The values within the table are the probabilities corresponding to the table type. These probabilities are calculations of the area under the normal curve from the starting point (0 for ''cumulative from mean'', negative infinity for ''cumulative'' and positive infinity for ''complementary cumulative'') to Z.\n\nExample: To find '''0.69''', one would look down the rows to find '''0.6''' and then across the columns to '''0.09''' which would yield a probability of '''0.25490''' for a ''cumulative from mean'' table or '''0.75490''' from a ''cumulative'' table.\n\nBecause the normal distribution curve is symmetrical, probabilities for only positive values of Z are typically given. The user has to use a complementary operation on the absolute value of Z, as in the example below.\n\n===Types of tables===\nZ tables use at least three different conventions:\n\n;Cumulative from mean: gives a probability that a statistic is between 0 (mean) and Z.  Example: Prob(0&nbsp;≤&nbsp;Z&nbsp;≤&nbsp;0.69)&nbsp;=&nbsp;0.2549\n\n;Cumulative: gives a probability that a statistic is less than Z.  This equates to the area of the distribution below Z. Example: Prob(Z&nbsp;≤&nbsp;0.69)&nbsp;=&nbsp;0.7549.\n\n;Complementary cumulative: gives a probability that a statistic is greater than Z. This equates to the area of the distribution above Z.\n\n:Example: Find Prob(''Z''&nbsp;≥&nbsp;0.69).  Since this is the portion of the area above Z, the proportion that is greater than Z is found by subtracting Z from 1.  That is Prob(''Z''&nbsp;≥&nbsp;0.69) = 1 - Prob(Z ≤ 0.69) or Prob(''Z''&nbsp; ≥ &nbsp;0.69) = 1 - 0.7549 = 0.2451.\n\n==Table examples==\n\n===Cumulative from mean (0 to Z)===\n[[File:Z cumulative from mean.svg|thumb|right|The values correspond to the shaded area for given Z]]\nThis table gives a probability that a statistic is between 0 (the mean) and Z.\n\n: <math> f(z) = \\Phi(z) - \\frac12</math>\n\nNote that for z = 1, 2, 3, one obtains (after multiplying by 2 to account for the [-z,z] interval) the results f(z) = 0.6827, 0.9545, 0.9974, \ncharacteristic of the [[68–95–99.7 rule]].\n\n{| class=\"wikitable\"\n! ''z'' !!+0.00!!+0.01!!+0.02!!+0.03!!+0.04!!rowspan=\"50\" style=\"padding:0;display:none;\"| !!+0.05!!+0.06!!+0.07!!+0.08!!+0.09\n|-\n! 0.0\n|0.00000||0.00399||0.00798||0.01197||0.01595||0.01994||0.02392||0.02790||0.03188||0.03586\n|-\n! 0.1\n|0.03983||0.04380||0.04776||0.05172||0.05567||0.05962||0.06356||0.06749||0.07142||0.07535\n|-\n! 0.2\n|0.07926||0.08317||0.08706||0.09095||0.09483||0.09871||0.10257||0.10642||0.11026||0.11409\n|-\n! 0.3\n|0.11791||0.12172||0.12552||0.12930||0.13307||0.13683||0.14058||0.14431||0.14803||0.15173\n|-\n! 0.4\n|0.15542||0.15910||0.16276||0.16640||0.17003||0.17364||0.17724||0.18082||0.18439||0.18793\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n! 0.5\n|0.19146||0.19497||0.19847||0.20194||0.20540||0.20884||0.21226||0.21566||0.21904||0.22240\n|-\n! 0.6\n|0.22575||0.22907||0.23237||0.23565||0.23891||0.24215||0.24537||0.24857||0.25175||0.25490\n|-\n! 0.7\n|0.25804||0.26115||0.26424||0.26730||0.27035||0.27337||0.27637||0.27935||0.28230||0.28524\n|-\n! 0.8\n|0.28814||0.29103||0.29389||0.29673||0.29955||0.30234||0.30511||0.30785||0.31057||0.31327\n|-\n! 0.9\n|0.31594||0.31859||0.32121||0.32381||0.32639||0.32894||0.33147||0.33398||0.33646||0.33891\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n! 1.0\n|0.34134||0.34375||0.34614||0.34849||0.35083||0.35314||0.35543||0.35769||0.35993||0.36214\n|-\n! 1.1\n|0.36433||0.36650||0.36864||0.37076||0.37286||0.37493||0.37698||0.37900||0.38100||0.38298\n|-\n! 1.2\n|0.38493||0.38686||0.38877||0.39065||0.39251||0.39435||0.39617||0.39796||0.39973||0.40147\n|-\n! 1.3\n|0.40320||0.40490||0.40658||0.40824||0.40988||0.41149||0.41308||0.41466||0.41621||0.41774\n|-\n! 1.4\n|0.41924||0.42073||0.42220||0.42364||0.42507||0.42647||0.42785||0.42922||0.43056||0.43189\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n! 1.5\n|0.43319||0.43448||0.43574||0.43699||0.43822||0.43943||0.44062||0.44179||0.44295||0.44408\n|-\n! 1.6\n|0.44520||0.44630||0.44738||0.44845||0.44950||0.45053||0.45154||0.45254||0.45352||0.45449\n|-\n! 1.7\n|0.45543||0.45637||0.45728||0.45818||0.45907||0.45994||0.46080||0.46164||0.46246||0.46327\n|-\n! 1.8\n|0.46407||0.46485||0.46562||0.46638||0.46712||0.46784||0.46856||0.46926||0.46995||0.47062\n|-\n! 1.9\n|0.47128||0.47193||0.47257||0.47320||0.47381||0.47441||0.47500||0.47558||0.47615||0.47670\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n! 2.0\n|0.47725||0.47778||0.47831||0.47882||0.47932||0.47982||0.48030||0.48077||0.48124||0.48169\n|-\n! 2.1\n|0.48214||0.48257||0.48300||0.48341||0.48382||0.48422||0.48461||0.48500||0.48537||0.48574\n|-\n! 2.2\n|0.48610||0.48645||0.48679||0.48713||0.48745||0.48778||0.48809||0.48840||0.48870||0.48899\n|-\n! 2.3\n|0.48928||0.48956||0.48983||0.49010||0.49036||0.49061||0.49086||0.49111||0.49134||0.49158\n|-\n! 2.4\n|0.49180||0.49202||0.49224||0.49245||0.49266||0.49286||0.49305||0.49324||0.49343||0.49361\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n! 2.5\n|0.49379||0.49396||0.49413||0.49430||0.49446||0.49461||0.49477||0.49492||0.49506||0.49520\n|-\n! 2.6\n|0.49534||0.49547||0.49560||0.49573||0.49585||0.49598||0.49609||0.49621||0.49632||0.49643\n|-\n! 2.7\n|0.49653||0.49664||0.49674||0.49683||0.49693||0.49702||0.49711||0.49720||0.49728||0.49736\n|-\n! 2.8\n|0.49744||0.49752||0.49760||0.49767||0.49774||0.49781||0.49788||0.49795||0.49801||0.49807\n|-\n! 2.9\n|0.49813||0.49819||0.49825||0.49831||0.49836||0.49841||0.49846||0.49851||0.49856||0.49861\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n! 3.0\n|0.49865||0.49869||0.49874||0.49878||0.49882||0.49886||0.49889||0.49893||0.49896||0.49900\n|-\n!3.1\n|0.49903||0.49906||0.49910||0.49913||0.49916||0.49918||0.49921||0.49924||0.49926||0.49929\n|-\n!3.2\n|0.49931||0.49934||0.49936||0.49938||0.49940||0.49942||0.49944||0.49946||0.49948||0.49950\n|-\n!3.3\n|0.49952||0.49953||0.49955||0.49957||0.49958||0.49960||0.49961||0.49962||0.49964||0.49965\n|-\n!3.4\n|0.49966||0.49968||0.49969||0.49970||0.49971||0.49972||0.49973||0.49974||0.49975||0.49976\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n!3.5\n|0.49977||0.49978||0.49978||0.49979||0.49980||0.49981||0.49981||0.49982||0.49983||0.49983\n|-\n!3.6\n|0.49984||0.49985||0.49985||0.49986||0.49986||0.49987||0.49987||0.49988||0.49988||0.49989\n|-\n!3.7\n|0.49989||0.49990||0.49990||0.49990||0.49991||0.49991||0.49992||0.49992||0.49992||0.49992\n|-\n!3.8\n|0.49993||0.49993||0.49993||0.49994||0.49994||0.49994||0.49994||0.49995||0.49995||0.49995\n|-\n!3.9\n|0.49995||0.49995||0.49996||0.49996||0.49996||0.49996||0.49996||0.49996||0.49997||0.49997\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n!4.0\n|0.49997||0.49997||0.49997||0.49997||0.49997||0.49997||0.49998||0.49998||0.49998||0.49998\n|}\n<ref>{{cite web|title=Cumulative Distribution Function of the Standard Normal Distribution|publisher=NIST|url=http://www.itl.nist.gov/div898/handbook/eda/section3/eda3671.htm|accessdate=5 May 2012}}</ref>\n\n===Cumulative===\nThis table gives a probability that a statistic is less than Z (i.e. between negative infinity and Z).\n\nThe values are calculated using the [[normal cumulative distribution function|cumulative distribution function]] of a standard normal distribution with mean of zero and standard deviation of one, usually denoted with the capital Greek letter <math>\\Phi</math> ([[phi (letter)|phi]]), is the integral\n\n:<math>\\Phi(z) = \\frac 1 {\\sqrt{2\\pi}} \\int_{-\\infty}^z e^{-t^2/2} \\, dt</math>\n\n<math>\\Phi</math>(z) is related to the [[error function]], or erf(''z'').\n\n: <math> \\Phi(z) = \\frac12\\left[1 + \\operatorname{erf}\\left( \\frac z {\\sqrt 2} \\right) \\right]</math>\n\n{| class=\"wikitable\"\n! ''z'' !!+0.00!!+0.01!!+0.02!!+0.03!!+0.04!!rowspan=\"50\" style=\"padding:0;display:none;\"| !!+0.05!!+0.06!!+0.07!!+0.08!!+0.09\n|-\n! 0.0\n|0.50000||0.50399||0.50798||0.51197||0.51595||0.51994||0.52392||0.52790||0.53188||0.53586\n|-\n! 0.1\n|0.53983||0.54380||0.54776||0.55172||0.55567||0.55966||0.56360||0.56749||0.57142||0.57535\n|-\n! 0.2\n|0.57926||0.58317||0.58706||0.59095||0.59483||0.59871||0.60257||0.60642||0.61026||0.61409\n|-\n! 0.3\n|0.61791||0.62172||0.62552||0.62930||0.63307||0.63683||0.64058||0.64431||0.64803||0.65173\n|-\n! 0.4\n|0.65542||0.65910||0.66276||0.66640||0.67003||0.67364||0.67724||0.68082||0.68439||0.68793\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n! 0.5\n|0.69146||0.69497||0.69847||0.70194||0.70540||0.70884||0.71226||0.71566||0.71904||0.72240\n|-\n! 0.6\n|0.72575||0.72907||0.73237||0.73565||0.73891||0.74215||0.74537||0.74857||0.75175||0.75490\n|-\n! 0.7\n|0.75804||0.76115||0.76424||0.76730||0.77035||0.77337||0.77637||0.77935||0.78230||0.78524\n|-\n! 0.8\n|0.78814||0.79103||0.79389||0.79673||0.79955||0.80234||0.80511||0.80785||0.81057||0.81327\n|-\n! 0.9\n|0.81594||0.81859||0.82121||0.82381||0.82639||0.82894||0.83147||0.83398||0.83646||0.83891\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n! 1.0\n|0.84134||0.84375||0.84614||0.84849||0.85083||0.85314||0.85543||0.85769||0.85993||0.86214\n|-\n! 1.1\n|0.86433||0.86650||0.86864||0.87076||0.87286||0.87493||0.87698||0.87900||0.88100||0.88298\n|-\n! 1.2\n|0.88493||0.88686||0.88877||0.89065||0.89251||0.89435||0.89617||0.89796||0.89973||0.90147\n|-\n! 1.3\n|0.90320||0.90490||0.90658||0.90824||0.90988||0.91149||0.91308||0.91466||0.91621||0.91774\n|-\n! 1.4\n|0.91924||0.92073||0.92220||0.92364||0.92507||0.92647||0.92785||0.92922||0.93056||0.93189\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n! 1.5\n|0.93319||0.93448||0.93574||0.93699||0.93822||0.93943||0.94062||0.94179||0.94295||0.94408\n|-\n! 1.6\n|0.94520||0.94630||0.94738||0.94845||0.94950||0.95053||0.95154||0.95254||0.95352||0.95449\n|-\n! 1.7\n|0.95543||0.95637||0.95728||0.95818||0.95907||0.95994||0.96080||0.96164||0.96246||0.96327\n|-\n! 1.8\n|0.96407||0.96485||0.96562||0.96638||0.96712||0.96784||0.96856||0.96926||0.96995||0.97062\n|-\n! 1.9\n|0.97128||0.97193||0.97257||0.97320||0.97381||0.97441||0.97500||0.97558||0.97615||0.97670\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n! 2.0\n|0.97725||0.97778||0.97831||0.97882||0.97932||0.97982||0.98030||0.98077||0.98124||0.98169\n|-\n! 2.1\n|0.98214||0.98257||0.98300||0.98341||0.98382||0.98422||0.98461||0.98500||0.98537||0.98574\n|-\n! 2.2\n|0.98610||0.98645||0.98679||0.98713||0.98745||0.98778||0.98809||0.98840||0.98870||0.98899\n|-\n! 2.3\n|0.98928||0.98956||0.98983||0.99010||0.99036||0.99061||0.99086||0.99111||0.99134||0.99158\n|-\n! 2.4\n|0.99180||0.99202||0.99224||0.99245||0.99266||0.99286||0.99305||0.99324||0.99343||0.99361\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n! 2.5\n|0.99379||0.99396||0.99413||0.99430||0.99446||0.99461||0.99477||0.99492||0.99506||0.99520\n|-\n! 2.6\n|0.99534||0.99547||0.99560||0.99573||0.99585||0.99598||0.99609||0.99621||0.99632||0.99643\n|-\n! 2.7\n|0.99653||0.99664||0.99674||0.99683||0.99693||0.99702||0.99711||0.99720||0.99728||0.99736\n|-\n! 2.8\n|0.99744||0.99752||0.99760||0.99767||0.99774||0.99781||0.99788||0.99795||0.99801||0.99807\n|-\n! 2.9\n|0.99813||0.99819||0.99825||0.99831||0.99836||0.99841||0.99846||0.99851||0.99856||0.99861\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n! 3.0\n|0.99865||0.99869||0.99874||0.99878||0.99882||0.99886||0.99889||0.99893||0.99896||0.99900\n|-\n!3.1\n|0.99903||0.99906||0.99910||0.99913||0.99916||0.99918||0.99921||0.99924||0.99926||0.99929\n|-\n!3.2\n|0.99931||0.99934||0.99936||0.99938||0.99940||0.99942||0.99944||0.99946||0.99948||0.99950\n|-\n!3.3\n|0.99952||0.99953||0.99955||0.99957||0.99958||0.99960||0.99961||0.99962||0.99964||0.99965\n|-\n!3.4\n|0.99966||0.99968||0.99969||0.99970||0.99971||0.99972||0.99973||0.99974||0.99975||0.99976\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n!3.5\n|0.99977||0.99978||0.99978||0.99979||0.99980||0.99981||0.99981||0.99982||0.99983||0.99983\n|-\n!3.6\n|0.99984||0.99985||0.99985||0.99986||0.99986||0.99987||0.99987||0.99988||0.99988||0.99989\n|-\n!3.7\n|0.99989||0.99990||0.99990||0.99990||0.99991||0.99991||0.99992||0.99992||0.99992||0.99992\n|-\n!3.8\n|0.99993||0.99993||0.99993||0.99994||0.99994||0.99994||0.99994||0.99995||0.99995||0.99995\n|-\n!3.9\n|0.99995||0.99995||0.99996||0.99996||0.99996||0.99996||0.99996||0.99996||0.99997||0.99997\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n!4.0\n|0.99997||0.99997||0.99997||0.99997||0.99997||0.99997||0.99998||0.99998||0.99998||0.99998\n|}\n<ref>0.5 + each value in ''Cumulative from mean'' table</ref>\n\n===Complementary cumulative===\nThis table gives a probability that a statistic is greater than Z.\n\n:<math>f(z) = 1 - \\Phi(z)</math>\n\n{| class=\"wikitable\"\n! ''z'' !!+0.00!!+0.01!!+0.02!!+0.03!!+0.04!!rowspan=\"50\" style=\"padding:0;display:none;\"| !!+0.05!!+0.06!!+0.07!!+0.08!!+0.09\n|-\n! 0.0\n|0.50000||0.49601||0.49202||0.48803||0.48405||0.48006||0.47608||0.47210||0.46812||0.46414\n|-\n! 0.1\n|0.46017||0.45620||0.45224||0.44828||0.44433||0.44038||0.43640||0.43251||0.42858||0.42465\n|-\n! 0.2\n|0.42074||0.41683||0.41294||0.40905||0.40517||0.40129||0.39743||0.39358||0.38974||0.38591\n|-\n! 0.3\n|0.38209||0.37828||0.37448||0.37070||0.36693||0.36317||0.35942||0.35569||0.35197||0.34827\n|-\n! 0.4\n|0.34458||0.34090||0.33724||0.33360||0.32997||0.32636||0.32276||0.31918||0.31561||0.31207\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n! 0.5\n|0.30854||0.30503||0.30153||0.29806||0.29460||0.29116||0.28774||0.28434||0.28096||0.27760\n|-\n! 0.6\n|0.27425||0.27093||0.26763||0.26435||0.26109||0.25785||0.25463||0.25143||0.24825||0.24510\n|-\n! 0.7\n|0.24196||0.23885||0.23576||0.23270||0.22965||0.22663||0.22363||0.22065||0.21770||0.21476\n|-\n! 0.8\n|0.21186||0.20897||0.20611||0.20327||0.20045||0.19766||0.19489||0.19215||0.18943||0.18673\n|-\n! 0.9\n|0.18406||0.18141||0.17879||0.17619||0.17361||0.17106||0.16853||0.16602||0.16354||0.16109\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n! 1.0\n|0.15866||0.15625||0.15386||0.15151||0.14917||0.14686||0.14457||0.14231||0.14007||0.13786\n|-\n! 1.1\n|0.13567||0.13350||0.13136||0.12924||0.12714||0.12507||0.12302||0.12100||0.11900||0.11702\n|-\n! 1.2\n|0.11507||0.11314||0.11123||0.10935||0.10749||0.10565||0.10383||0.10204||0.10027||0.09853\n|-\n! 1.3\n|0.09680||0.09510||0.09342||0.09176||0.09012||0.08851||0.08692||0.08534||0.08379||0.08226\n|-\n! 1.4\n|0.08076||0.07927||0.07780||0.07636||0.07493||0.07353||0.07215||0.07078||0.06944||0.06811\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n! 1.5\n|0.06681||0.06552||0.06426||0.06301||0.06178||0.06057||0.05938||0.05821||0.05705||0.05592\n|-\n! 1.6\n|0.05480||0.05370||0.05262||0.05155||0.05050||0.04947||0.04846||0.04746||0.04648||0.04551\n|-\n! 1.7\n|0.04457||0.04363||0.04272||0.04182||0.04093||0.04006||0.03920||0.03836||0.03754||0.03673\n|-\n! 1.8\n|0.03593||0.03515||0.03438||0.03362||0.03288||0.03216||0.03144||0.03074||0.03005||0.02938\n|-\n! 1.9\n|0.02872||0.02807||0.02743||0.02680||0.02619||0.02559||0.02500||0.02442||0.02385||0.02330\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n! 2.0\n|0.02275||0.02222||0.02169||0.02118||0.02068||0.02018||0.01970||0.01923||0.01876||0.01831\n|-\n! 2.1\n|0.01786||0.01743||0.01700||0.01659||0.01618||0.01578||0.01539||0.01500||0.01463||0.01426\n|-\n! 2.2\n|0.01390||0.01355||0.01321||0.01287||0.01255||0.01222||0.01191||0.01160||0.01130||0.01101\n|-\n! 2.3\n|0.01072||0.01044||0.01017||0.00990||0.00964||0.00939||0.00914||0.00889||0.00866||0.00842\n|-\n! 2.4\n|0.00820||0.00798||0.00776||0.00755||0.00734||0.00714||0.00695||0.00676||0.00657||0.00639\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n! 2.5\n|0.00621||0.00604||0.00587||0.00570||0.00554||0.00539||0.00523||0.00508||0.00494||0.00480\n|-\n! 2.6\n|0.00466||0.00453||0.00440||0.00427||0.00415||0.00402||0.00391||0.00379||0.00368||0.00357\n|-\n! 2.7\n|0.00347||0.00336||0.00326||0.00317||0.00307||0.00298||0.00289||0.00280||0.00272||0.00264\n|-\n! 2.8\n|0.00256||0.00248||0.00240||0.00233||0.00226||0.00219||0.00212||0.00205||0.00199||0.00193\n|-\n! 2.9\n|0.00187||0.00181||0.00175||0.00169||0.00164||0.00159||0.00154||0.00149||0.00144||0.00139\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n! 3.0\n|0.00135||0.00131||0.00126||0.00122||0.00118||0.00114||0.00111||0.00107||0.00104||0.00100\n|-\n!3.1\n|0.00097||0.00094||0.00090||0.00087||0.00084||0.00082||0.00079||0.00076||0.00074||0.00071\n|-\n!3.2\n|0.00069||0.00066||0.00064||0.00062||0.00060||0.00058||0.00056||0.00054||0.00052||0.00050\n|-\n!3.3\n|0.00048||0.00047||0.00045||0.00043||0.00042||0.00040||0.00039||0.00038||0.00036||0.00035\n|-\n!3.4\n|0.00034||0.00032||0.00031||0.00030||0.00029||0.00028||0.00027||0.00026||0.00025||0.00024\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n!3.5\n|0.00023||0.00022||0.00022||0.00021||0.00020||0.00019||0.00019||0.00018||0.00017||0.00017\n|-\n!3.6\n|0.00016||0.00015||0.00015||0.00014||0.00014||0.00013||0.00013||0.00012||0.00012||0.00011\n|-\n!3.7\n|0.00011||0.00010||0.00010||0.00010||0.00009||0.00009||0.00008||0.00008||0.00008||0.00008\n|-\n!3.8\n|0.00007||0.00007||0.00007||0.00006||0.00006||0.00006||0.00006||0.00005||0.00005||0.00005\n|-\n!3.9\n|0.00005||0.00005||0.00004||0.00004||0.00004||0.00004||0.00004||0.00004||0.00003||0.00003\n|-\n| colspan=\"11\" style=\"padding:0;\"|\n|-\n!4.0\n|0.00003||0.00003||0.00003||0.00003||0.00003||0.00003||0.00002||0.00002||0.00002||0.00002\n|}\n<ref>0.5 - each value in ''Cumulative from mean (0 to Z)'' table</ref>\n\nThis table gives a probability that a statistic is greater than Z, for large integer Z values.\n{| class=\"wikitable\"\n! ''z'' !!+0!!+1!!+2!!+3!!+4!!rowspan=\"38\" style=\"padding:0;display:none;\"| !!+5!!+6!!+7!!+8!!+9\n|-\n! 0\n|5.00000 E -1||1.58655 E -1||2.27501 E -2||1.34990 E -3||3.16712 E -5||2.86652 E -7||9.86588 E -10||1.27981 E -12||6.22096 E -16||1.12859 E -19\n|-\n! 10\n|7.61985 E -24||1.91066 E -28||1.77648 E -33||6.11716 E -39||7.79354 E -45||3.67097 E -51||6.38875 E -58||4.10600 E -65||9.74095 E -73||8.52722 E -81\n|-\n! 20\n|2.75362 E -89||3.27928 E -98||1.43989 E -107||2.33064 E -117||1.39039 E -127||3.05670 E -138||2.47606 E -149||7.38948 E -161||8.12387 E -173||3.28979 E -185\n|-\n! 30\n|4.90671 E -198||2.69525 E -211||5.45208 E -225||4.06119 E -239||1.11390 E -253||1.12491 E -268||4.18262 E -284||5.72557 E -300||2.88543 E -316||5.35312 E -333\n|-\n! 40\n|3.65589 E -350||9.19086 E -368||8.50515 E -386||2.89707 E -404||3.63224 E -423||1.67618 E -442||2.84699 E -462||1.77976 E -482||4.09484 E -503||3.46743 E -524\n|-\n! 50\n|1.08060 E -545||1.23937 E -567||5.23127 E -590||8.12606 E -613||4.64529 E -636||9.77237 E -660||7.56547 E -684||2.15534 E -708||2.25962 E -733||8.71741 E -759\n|-\n! 60\n|1.23757 E -784||6.46517 E -811||1.24283 E -837||8.79146 E -865||2.28836 E -892||2.19180 E -920||7.72476 E -949||1.00178 E -977||4.78041 E -1007||8.39374 E -1037\n|-\n!70\n|5.42304 E -1067||1.28921 E -1097||1.12771 E -1128||3.62960 E -1160||4.29841 E -1192||1.87302 E -1224||3.00302 E -1257||1.77155 E -1290||3.84530 E -1324||3.07102 E -1358\n|}\n\n==Examples of use==\n\nA professor's exam scores are approximately distributed normally with mean 80 and standard deviation 5. Only a ''cumulative from mean'' table is available.\n\n* What is the probability that a student scores an 82 or less?\n\n:: <math>P(X \\le 82) = P\\left(Z \\le \\frac{82 - 80}{5}\\right) = P(Z \\le 0.40)</math> &nbsp;<math>= 0.15542 + 0.5 = 0.65542</math>\n\n* What is the probability that a student scores a 90 or more?\n\n:: <math>P(X \\ge 90) = P\\left(Z \\ge \\frac{90 - 80}{5}\\right) = P(Z \\ge 2.00)</math> &nbsp;<math>= 1 - P(Z \\le 2.00) = 1 - (0.47725 + 0.5) = 0.02275</math>\n\n* What is the probability that a student scores a 74 or less?\n\n:: <math>P(X \\le 74) = P\\left(Z \\le \\frac{74 - 80}{5}\\right) = P(Z \\le - 1.20)</math>\n\n:Since this table does not include negatives, the process involves the following additional step:\n\n:: <math>P(Z \\le -1.20) = P(Z \\ge 1.20)</math> &nbsp;<math>= 1 - (0.38493 + 0.5) = 0.11507</math>\n\n* What is the probability that a student scores between 74 and 82?\n\n:: <math>P(74 \\le X \\le 82) = P(X \\le 82) - P(X \\le 74) = 0.65542 - 0.11507</math> [as in above examples] <math> = 0.54035</math>\n\n* What is the probability that an average of three scores is 82 or less?\n\n:: <math>P(X \\le 82) = P\\left(Z \\le \\frac{82 - 80}{5/\\sqrt{3}}\\right)</math> &nbsp;<math>= P(Z \\le 0.69) = 0.2549 + 0.5 = 0.7549</math>\n\n==References==\n{{reflist}}\n\n[[Category:Normal distribution]]\n[[Category:Mathematical tables]]"
    },
    {
      "title": "Sum of normally distributed random variables",
      "url": "https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables",
      "text": "In [[probability theory]], calculation of the '''sum of normally distributed random variables''' is an instance of the arithmetic of [[random variable]]s, which can be quite complex based on the [[probability distribution]]s of the random variables involved and their relationships.\n\nThis is not to be confused with the '''sum of normal distributions''' which forms a [[mixture distribution]].\n\n==Independent random variables==\nIf ''X'' and ''Y'' are [[statistical independence|independent]] [[random variable]]s that are [[normal distribution|normally distributed]] (and therefore also jointly so), then their sum is also normally distributed. i.e., if\n\n:<math>X \\sim N(\\mu_X, \\sigma_X^2)</math>\n:<math>Y \\sim N(\\mu_Y, \\sigma_Y^2)</math>\n:<math>Z=X+Y,</math>\n\nthen\n\n:<math>Z \\sim N(\\mu_X + \\mu_Y, \\sigma_X^2 + \\sigma_Y^2).</math>\n\nThis means that the sum of two independent normally distributed random variables is normal, with its mean being the sum of the two means, and its variance being the sum of the two variances (i.e., the square of the standard deviation is the sum of the squares of the standard deviations).<ref>{{Citation\n| last1  = Lemons\n| first1 = Don S.\n| title     = An Introduction to Stochastic Processes in Physics\n| publisher = The Johns Hopkins University Press\n| year    = 2002\n| isbn    = 0-8018-6866-1\n| page=34 \n}}</ref>\n\nIn order for this result to hold, the assumption that ''X'' and ''Y'' are independent cannot be dropped, although it can be weakened to the assumption that ''X'' and ''Y'' are [[multivariate normal distribution|jointly]], rather than separately, normally distributed.<ref>Lemons (2002) pp. 35-36</ref> (See [[Normally distributed and uncorrelated does not imply independent#A symmetric example|here for an example]].)\n\nThe result about the mean holds in all cases, while the result for the variance requires uncorrelatedness, but not independence.\n\n===Proofs===\n====Proof using characteristic functions====\n\nThe [[characteristic function (probability theory)|characteristic function]]\n\n:<math>\\varphi_{X+Y}(t) = \\operatorname{E}\\left(e^{it(X+Y)}\\right)</math>\n\nof the sum of two independent random variables ''X'' and ''Y'' is just the product of the two separate characteristic functions:\n\n:<math>\\varphi_X (t) = \\operatorname{E}\\left(e^{itX}\\right), \\qquad \\varphi_Y(t) = \\operatorname{E}\\left(e^{itY}\\right)</math>\n\nof ''X'' and ''Y''.\n\nThe characteristic function of the normal distribution with expected value μ and variance σ<sup>2</sup> is\n\n:<math>\\varphi(t) = \\exp\\left(it\\mu - {\\sigma^2 t^2 \\over 2}\\right).</math>\n\nSo\n\n:<math>\n\\begin{align}\n\\varphi_{X+Y}(t)=\\varphi_X(t) \\varphi_Y(t) & =\\exp\\left(it\\mu_X - {\\sigma_X^2 t^2 \\over 2}\\right) \\exp\\left(it\\mu_Y - {\\sigma_Y^2 t^2 \\over 2}\\right) \\\\[6pt]\n& = \\exp \\left(  it (\\mu_X +\\mu_Y) - {(\\sigma_X^2 + \\sigma_Y^2) t^2 \\over 2}\\right).\n\\end{align}\n</math>\n\nThis is the characteristic function of the normal distribution with expected value <math>\\mu_X + \\mu_Y</math> and variance <math>\\sigma_X^2+\\sigma_Y^2</math>\n\nFinally, recall that no two distinct distributions can both have the same characteristic function, so the distribution of ''X''&nbsp;+&nbsp;''Y'' must be just this normal distribution.\n\n====Proof using convolutions====\n\nFor independent random variables ''X'' and ''Y'', the distribution ''f''<sub>''Z''</sub> of ''Z'' = ''X''&nbsp;+&nbsp;''Y'' equals the convolution of ''f''<sub>''X''</sub> and ''f''<sub>''Y''</sub>:\n\n:<math>f_Z(z) = \\int_{-\\infty}^\\infty f_Y(z-x) f_X(x) \\, dx</math>\n\nGiven that ''f''<sub>''X''</sub> and ''f''<sub>''Y''</sub> are normal densities,\n\n:<math>\n\\begin{align}\nf_X(x) = \\mathcal{N}(x; \\mu_X, \\sigma_X^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma_X} e^{-(x-\\mu_X)^2/(2\\sigma_X^2)} \\\\[5pt]\nf_Y(y) = \\mathcal{N}(y; \\mu_Y, \\sigma_Y^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma_Y} e^{-(y-\\mu_Y)^2/(2\\sigma_Y^2)}\n\\end{align}\n</math>\n\nSubstituting into the convolution:\n\n:<math>\n\\begin{align}\nf_Z(z)\n&= \\int_{-\\infty}^\\infty\n   \\frac{1}{\\sqrt{2\\pi}\\sigma_Y}\n   \\exp \\left[-{(z-x-\\mu_Y)^2 \\over 2\\sigma_Y^2}\\right]\n   \\frac{1}{\\sqrt{2\\pi}\\sigma_X}\n   \\exp \\left[-{(x-\\mu_X)^2 \\over 2\\sigma_X^2}\\right]\n   \\, dx \n\\\\[6pt]\n&= \\int_{-\\infty}^\\infty\n   \\frac{1}{\\sqrt{2\\pi}\\sqrt{2\\pi}\\sigma_X\\sigma_Y}\n   \\exp \\left[-\\frac{\\sigma_X^2(z-x-\\mu_Y)^2 + \\sigma_Y^2(x-\\mu_X)^2}{2\\sigma_X^2\\sigma_Y^2}\\right]\n   \\, dx \n\\\\[6pt]\n&= \\int_{-\\infty}^\\infty\n   \\frac{1}{\\sqrt{2\\pi}\\sqrt{2\\pi}\\sigma_X\\sigma_Y}\n   \\exp \\left[\n      -\\frac\n         {\\sigma_X^2(z^2 + x^2 + \\mu_Y^2 - 2xz - 2z\\mu_Y + 2x\\mu_Y) + \\sigma_Y^2(x^2 + \\mu_X^2 - 2x\\mu_X)}\n         {2\\sigma_Y^2\\sigma_X^2}\n   \\right]\n   \\, dx \n\\\\[6pt]\n&= \\int_{-\\infty}^\\infty\n   \\frac{1}{\\sqrt{2\\pi}\\sqrt{2\\pi}\\sigma_X\\sigma_Y}\n   \\exp \\left[\n      -\\frac\n         {\n            x^2(\\sigma_X^2 + \\sigma_Y^2) - \n            2x(\\sigma_X^2(z - \\mu_Y) + \\sigma_Y^2\\mu_X) +\n            \\sigma_X^2(z^2 + \\mu_Y^2 - 2z\\mu_Y) + \\sigma_Y^2\\mu_X^2\n         }\n         {2\\sigma_Y^2\\sigma_X^2}\n   \\right]\n   \\, dx\n\\\\[6pt]\n\\end{align}\n</math>\n\nDefining <math>\\sigma_Z = \\sqrt{\\sigma_X^2 + \\sigma_Y^2}</math>, and [[completing the square]]:\n\n:<math>\n\\begin{align}\nf_Z(z)\n&= \\int_{-\\infty}^\\infty\n   \\frac{1}{\\sqrt{2\\pi}\\sigma_Z}\n   \\frac{1}{\\sqrt{2\\pi}\\frac{\\sigma_X\\sigma_Y}{\\sigma_Z}}\n   \\exp \\left[\n      -\\frac\n         {\n            x^2 - \n            2x\\frac{\\sigma_X^2(z - \\mu_Y) + \\sigma_Y^2\\mu_X}{\\sigma_Z^2} +\n            \\frac{\\sigma_X^2(z^2 + \\mu_Y^2 - 2z\\mu_Y) + \\sigma_Y^2\\mu_X^2}{\\sigma_Z^2}\n         }\n         {2\\left(\\frac{\\sigma_X\\sigma_Y}{\\sigma_Z}\\right)^2}\n   \\right]\n   \\, dx \n\\\\[6pt]\n&= \\int_{-\\infty}^\\infty\n   \\frac{1}{\\sqrt{2\\pi}\\sigma_Z}\n   \\frac{1}{\\sqrt{2\\pi}\\frac{\\sigma_X\\sigma_Y}{\\sigma_Z}}\n   \\exp \\left[\n      -\\frac\n         {\n            \\left(x - \\frac{\\sigma_X^2(z - \\mu_Y) + \\sigma_Y^2\\mu_X}{\\sigma_Z^2}\\right)^2 -\n            \\left(\\frac{\\sigma_X^2(z - \\mu_Y) + \\sigma_Y^2\\mu_X}{\\sigma_Z^2}\\right)^2 +\n            \\frac{\\sigma_X^2(z - \\mu_Y)^2 + \\sigma_Y^2\\mu_X^2}{\\sigma_Z^2}\n         }\n         {2\\left(\\frac{\\sigma_X\\sigma_Y}{\\sigma_Z}\\right)^2}\n   \\right]\n   \\, dx \n\\\\[6pt]\n&= \\int_{-\\infty}^\\infty\n   \\frac{1}{\\sqrt{2\\pi}\\sigma_Z}\n   \\exp \\left[\n      -\\frac\n         {\n            \\sigma_Z^2\\left(\\sigma_X^2(z - \\mu_Y)^2 + \\sigma_Y^2\\mu_X^2\\right) -\n            \\left(\\sigma_X^2(z - \\mu_Y) + \\sigma_Y^2\\mu_X\\right)^2\n         }\n         {2\\sigma_Z^2\\left(\\sigma_X\\sigma_Y\\right)^2}\n   \\right]\n   \\frac{1}{\\sqrt{2\\pi}\\frac{\\sigma_X\\sigma_Y}{\\sigma_Z}}\n   \\exp \\left[\n      -\\frac\n         {\n            \\left(x - \\frac{\\sigma_X^2(z - \\mu_Y) + \\sigma_Y^2\\mu_X}{\\sigma_Z^2}\\right)^2\n         }\n         {2\\left(\\frac{\\sigma_X\\sigma_Y}{\\sigma_Z}\\right)^2}\n   \\right]\n   \\, dx \n\\\\[6pt]\n&= \\frac{1}{\\sqrt{2\\pi}\\sigma_Z}\n   \\exp \\left[ - { (z-(\\mu_X+\\mu_Y))^2 \\over 2\\sigma_Z^2 } \\right]\n   \\int_{-\\infty}^{\\infty}\n   \\frac{1}{\\sqrt{2\\pi}\\frac{\\sigma_X\\sigma_Y}{\\sigma_Z}}\n   \\exp \\left[ - \\frac{\\left(x-\\frac{\\sigma_X^2(z-\\mu_Y)+\\sigma_Y^2\\mu_X}{\\sigma_Z^2}\\right)^2}{2\\left(\\frac{\\sigma_X\\sigma_Y}{\\sigma_Z}\\right)^2} \\right]\n   \\, dx\n\\end{align}\n</math>\n\nThe expression in the integral is a normal density distribution on ''x'', and so the integral evaluates to 1. The desired result follows:\n\n:<math>f_Z(z) = \\frac{1}{\\sqrt{2\\pi}\\sigma_Z} \\exp \\left[ - { (z-(\\mu_X+\\mu_Y))^2 \\over 2\\sigma_Z^2 } \\right]</math>\n\n===== Using the [[convolution theorem]] =====\n\nIt can be shown that the [[Fourier transform]] of a Gaussian, <math>f_X(x) = \\mathcal{N}(x; \\mu_X, \\sigma_X^2)</math>, is<ref>{{cite web|last1=Derpanis|first1=Konstantinos G.|title=Fourier Transform of the Gaussian|url=http://www.cse.yorku.ca/~kosta/CompVis_Notes/fourier_transform_Gaussian.pdf|date=October 20, 2005}}</ref>\n\n:<math>\\mathcal{F}\\{f_X\\} = F_X(\\omega) = \\exp \\left[ -j \\omega \\mu_X \\right] \\exp \\left[-\\tfrac{\\sigma_X^2\\omega^2}{2} \\right]</math>\n\nBy the [[convolution theorem]]:\n\n:<math>\n\\begin{align}\nf_Z(z) &= (f_X * f_Y)(z) \\\\[5pt]\n    &= \\mathcal{F}^{-1}\\big\\{\\mathcal{F}\\{f_X\\}\\cdot\\mathcal{F}\\{f_Y\\}\\big\\} \\\\[5pt]\n    &= \\mathcal{F}^{-1}\\big\\{\n        \\exp \\left[ -j \\omega \\mu_X \\right] \\exp \\left[-\\tfrac{\\sigma_X^2\\omega^2}{2} \\right]\n        \\exp \\left[ -j \\omega \\mu_Y \\right] \\exp \\left[-\\tfrac{\\sigma_Y^2\\omega^2}{2} \\right]\n    \\big\\} \\\\[5pt]\n    &= \\mathcal{F}^{-1}\\big\\{\n        \\exp \\left[ -j \\omega (\\mu_X + \\mu_Y) \\right] \\exp \\left[-\\tfrac{(\\sigma_X^2\\ + \\sigma_Y^2)\\omega^2}{2} \\right]\n    \\big\\} \\\\[5pt]\n    &= \\mathcal{N}(z; \\mu_X + \\mu_Y, \\sigma_X^2 + \\sigma_Y^2)\n\\end{align}\n</math>\n\n====Geometric proof====\nFirst consider the normalized case when ''X'', ''Y'' ~ ''N''(0, 1), so that their [[probability density function|PDF]]s are\n:<math>f(x) = \\frac 1 {\\sqrt{2\\pi \\,}} e^{-x^2/2}</math>\nand\n:<math>g(y) = \\frac1 {\\sqrt{2\\pi\\,}} e^{-y^2/2}.</math>\nLet ''Z'' = ''X''&nbsp;+&nbsp;''Y''. Then the [[cumulative distribution function|CDF]] for ''Z''  will be\n:<math>z \\mapsto \\int_{x+y \\leq z} f(x)g(y) \\, dx \\, dy. </math>\nThis integral is over the half-plane which lies under the line ''x''+''y'' = ''z''.\n\nThe key observation is that the function\n\n:<math> f(x)g(y) = \\frac 1 {2\\pi}e^{-(x^2 + y^2)/2}\\,</math>\n\nis radially symmetric. So we rotate the coordinate plane about the origin, choosing new coordinates <math>x',y'</math> such that the line ''x''+''y'' = ''z'' is described by the equation <math> x' = c </math> where <math> c = c(z) </math> is determined geometrically. Because of the radial symmetry, we have <math> f(x)g(y) = f(x')g(y') </math>, and the CDF for ''Z'' is\n\n:<math>\\int_{x'\\leq c, y' \\in \\reals} f(x')g(y') \\, dx' \\, dy'.</math>\n\nThis is easy to integrate; we find that the CDF for ''Z'' is\n\n:<math>\\int_{-\\infty}^{c(z)} f(x') \\, dx' = \\Phi(c(z)).</math>\n\nTo determine the value <math>c(z)</math>, note that we rotated the plane so that the line ''x''+''y'' = ''z'' now runs vertically with ''x''-intercept equal to ''c''. So ''c'' is just the distance from the origin to the line ''x''+''y'' = ''z'' along the perpendicular bisector, which meets the line at its nearest point to the origin, in this case <math>(z/2,z/2)\\,</math>. So the distance is <math>c = \\sqrt{ (z/2)^2 + (z/2)^2 } = z/\\sqrt{2}\\,</math>, and the CDF for ''Z'' is <math> \\Phi(z/\\sqrt{2})</math>, i.e., <math>Z = X+Y \\sim N(0, 2).</math>\n\nNow, if ''a'', ''b'' are any real constants (not both zero!) then the probability that <math>aX+bY \\leq z</math> is found by the same integral as above, but with the bounding line <math>ax+by =z</math>. The same rotation method works, and in this more general case we find that the closest point on the line to the origin is located a (signed) distance\n: <math>\\frac{z}{\\sqrt{a^2 + b^2}}</math>\naway, so that\n:<math>aX + bY \\sim N(0, a^2 + b^2).</math>\nThe same argument in higher dimensions shows that if\n:<math>X_i \\sim N(0,\\sigma_i^2), \\qquad i=1, \\dots, n,</math>\nthen\n:<math>X_1+ \\cdots + X_n \\sim N(0, \\sigma_1^2 + \\cdots + \\sigma_n^2).</math>\n\nNow we are essentially done, because\n:<math>X \\sim N(\\mu,\\sigma^2) \\Leftrightarrow \\frac{1}{\\sigma} (X - \\mu) \\sim N(0,1).</math>\nSo in general, if\n:<math>X_i \\sim N(\\mu_i, \\sigma_i^2), \\qquad i=1, \\dots, n,</math>\nthen\n: <math>\\sum_{i=1}^n a_i X_i \\sim N\\left(\\sum_{i=1}^n a_i \\mu_i, \\sum_{i=1}^n (a_i \\sigma_i)^2 \\right).</math>\n\n==Correlated random variables==\nIn the event that the variables ''X'' and ''Y'' are jointly normally distributed random variables, then ''X''&nbsp;+&nbsp;''Y'' is still normally distributed (see [[Multivariate normal distribution]]) and the mean is the sum of the means. However, the variances are not additive due to the correlation. Indeed,\n\n:<math>\\sigma_{X+Y} = \\sqrt{\\sigma_X^2+\\sigma_Y^2+2\\rho\\sigma_X \\sigma_Y},</math>\n\nwhere ρ is the [[correlation]]. In particular, whenever ρ&nbsp;<&nbsp;0, then the variance is less than the sum of the variances of ''X'' and ''Y''.\n\n[[Variance#Sum of correlated variables|Extensions of this result]] can be made for more than two random variables, using the [[covariance matrix]].\n\n===Proof===\nIn this case (with ''X'' and ''Y'' having zero means), one needs to consider\n\n:<math>\\frac{1}{2 \\pi \\sigma_x \\sigma_y \\sqrt{1-\\rho^2}} \\iint_{x\\,y} \\exp \\left[ -\\frac{1}{2(1-\\rho^2)} \\left(\\frac{x^2}{\\sigma_x^2} + \\frac{y^2}{\\sigma_y^2} - \\frac{2 \\rho x y}{\\sigma_x\\sigma_y}\\right)\\right] \\delta(z - (x+y))\\, \\mathrm{d}x\\,\\mathrm{d}y. </math>\n\nAs above, one makes the substitution <math>y\\rightarrow z-x</math>\n\nThis integral is more complicated to simplify analytically, but can be done easily using a symbolic mathematics program. The probability distribution ''f''<sub>''Z''</sub>(''z'') is given in this case by\n\n:<math>f_Z(z)=\\frac{1}{\\sqrt{2 \\pi}\\sigma_+ }\\exp\\left(-\\frac{z^2}{2\\sigma_+^2}\\right)</math>\nwhere\n:<math>\\sigma_+ = \\sqrt{\\sigma_x^2+\\sigma_y^2+2\\rho\\sigma_x \\sigma_y}.</math>\n\nIf one considers instead ''Z'' = ''X''&nbsp;−&nbsp;''Y'', then one obtains\n:<math>f_Z(z)=\\frac{1}{\\sqrt{2\\pi(\\sigma_x^2+\\sigma_y^2-2\\rho\\sigma_x \\sigma_y)}}\\exp\\left(-\\frac{z^2}{2(\\sigma_x^2+\\sigma_y^2-2\\rho\\sigma_x \\sigma_y)}\\right)</math>\nwhich also can be rewritten with\n:<math>\\sigma_-=\\sqrt{\\sigma_x^2+\\sigma_y^2-2\\rho\\sigma_x \\sigma_y}.</math>\n\nThe standard deviations of each distribution are obvious by comparison with the standard normal distribution.\n\n==References==\n{{reflist}}\n\n==See also==\n* [[Propagation of uncertainty]]\n* [[Algebra of random variables]]\n* [[Stable distribution]]\n* [[Standard error (statistics)]]\n* [[Ratio distribution]]\n* [[Product distribution]]\n* [[Slash distribution]]\n* [[List of convolutions of probability distributions]]\n\n[[Category:Normal distribution]]"
    },
    {
      "title": "T-statistic",
      "url": "https://en.wikipedia.org/wiki/T-statistic",
      "text": "{{DISPLAYTITLE:''t''-statistic}}\n{{Unreferenced|date=February 2011}}\nIn [[statistics]], the '''''t''-statistic''' is the ratio of the departure of the estimated value of a parameter from its hypothesized value to its [[Standard error (statistics)|standard error]]. It is used in [[statistical hypothesis testing|hypothesis testing]] via [[Student's t-test]]. For example, it is used in estimating the [[population mean]] from a [[sampling distribution]] of [[sample mean]]s if the population [[standard deviation]] is unknown.\n\n==Definition and features==\nLet <math style=\"vertical-align:-.3em\">\\scriptstyle\\widehat\\beta</math> be an [[estimator]] of parameter ''β'' in some [[statistical model]]. Then a ''t''-statistic for this parameter is any quantity of the form\n: <math>\n    t_{\\widehat{\\beta}} = \\frac{\\widehat\\beta - \\beta_0}{\\operatorname{s.e.}(\\widehat\\beta)}   \n  </math>\nwhere ''β''<sub>0</sub> is a non-random, known constant which may or may not match the actual unknown parameter value ''β'', and <math style=\"vertical-align:-.3em\">\\operatorname{s.e.}(\\widehat\\beta)</math> is the [[standard error (statistics)|standard error]] of the estimator <math style=\"vertical-align:-.3em\">\\scriptstyle\\widehat\\beta</math> for ''β''. \n\nBy default, statistical packages report ''t''-statistic with {{nowrap|''β''<sub>0</sub> {{=}} 0}} (these ''t''-statistics are used to test the significance of corresponding regressor). However, when ''t''-statistic is needed to test the hypothesis of the form {{nowrap|''H''<sub>0</sub>: ''β'' {{=}} ''β''<sub>0</sub>}}, then a non-zero ''β''<sub>0</sub> may be used.\n\nIf <math style=\"vertical-align:-.3em\">\\scriptstyle\\widehat\\beta</math> is an [[ordinary least squares]] estimator in the classical [[linear regression model]] (that is, with [[normal distribution|normally distributed]] and [[homoscedasticity|homoscedastic]] error terms), and if the true value of the parameter ''β'' is equal to ''β''<sub>0</sub>, then the [[sampling distribution]] of the ''t''-statistic is the [[Student's t-distribution|Student's ''t''-distribution]] with {{nowrap|(''n − k'')}} degrees of freedom, where ''n'' is the number of observations, and ''k'' is the number of regressors (including the intercept){{Citation needed|date=November 2018}}.\n\nIn the majority of models, the estimator <math style=\"vertical-align:-.3em\">\\scriptstyle\\widehat\\beta</math> is [[consistent estimator|consistent]] for ''β'' and is distributed [[asymptotic normality|asymptotically normally]]. If the true value of the parameter ''β'' is equal to ''β''<sub>0</sub> and the quantity <math style=\"vertical-align:-.3em\">\\scriptstyle \\operatorname{s.e.}(\\widehat\\beta)</math> correctly estimates the asymptotic variance of this estimator, then the ''t''-statistic will asymptotically have the [[standard normal]] distribution.\n\nIn some models the distribution of the ''t''-statistic is different from the normal distribution, even asymptotically. For example, when a [[time series]] with a [[unit root]] is regressed in the [[augmented Dickey–Fuller test]], the test ''t''-statistic will asymptotically have one of the Dickey–Fuller distributions (depending on the test setting).\n\n==Use==\n{{Main|t-test}}\n\nMost frequently, ''t'' statistics are used in [[Student's t test|Student's ''t''-test]]s, a form of [[statistical hypothesis testing]], and in the computation of certain [[confidence interval]]s.\n\nThe key property of the ''t'' statistic is that it is a [[pivotal quantity]] – while defined in terms of the sample mean, its sampling distribution does not depend on the population parameters, and thus it can be used regardless of what these may be.\n\nOne can also divide a [[Errors and residuals in statistics|residual]] by the sample [[standard deviation]]:\n:<math> g(x,X) = \\frac{x - \\overline{X}}{s} </math>\nto compute an estimate for the number of standard deviations a given sample is from the mean, as a sample version of a [[z-score]], the z-score requiring the population parameters.\n\n===Prediction===\n{{details|Prediction interval#Unknown mean, unknown variance}}\nGiven a normal distribution <math>N(\\mu,\\sigma^2)</math> with unknown mean and variance, the ''t''-statistic of a future observation <math>X_{n+1},</math> after one has made ''n'' observations, is an [[ancillary statistic]] – a pivotal quantity (does not depend on the values of ''μ'' and ''σ''<sup>2</sup>) that is a statistic (computed from observations). This allows one to compute a frequentist [[prediction interval]] (a predictive [[confidence interval]]), via the following t-distribution:\n:<math>\\frac{X_{n+1}-\\overline{X}_n}{s_n\\sqrt{1+n^{-1}}} \\sim T^{n-1}</math>\nSolving for <math>X_{n+1}</math> yields the prediction distribution\n:<math>\\overline{X}_n + s_n\\sqrt{1+n^{-1}} \\cdot T^{n-1}</math>\nfrom which one may compute predictive confidence intervals – given a probability ''p,'' one may compute intervals such that 100''p''% of the time, the next observation <math>X_{n+1}</math> will fall in that interval.\n\n==History==\n{{details|Student's t-test}}\nThe term \"''t''-statistic\" is abbreviated from \"hypothesis test statistic\",{{Citation needed|date=February 2011}} while \"Student\" was the [[pen name]] of [[William Sealy Gosset]], who introduced the ''t''-statistic and ''t''-test in 1908, while working for the [[Guinness]] [[brewery]] in [[Dublin, Ireland]].\n\n==Related concepts==\n*[[z-score|''z''-score (standardization)]]: If the population parameters are known, then rather than computing the t-statistic, one can compute the z-score; analogously, rather than using a ''t''-test, one uses a [[z-test|''z''-test]]. This is rare outside of [[Standardized testing (statistics)|standardized testing]].\n*[[Studentized residual]]: In [[regression analysis]], the standard errors of the estimators at different data points vary (compare the middle versus endpoints of a [[simple linear regression]]), and thus one must divide the different residuals by different estimates for the error, yielding what are called [[studentized residual]]s.\n\n==See also==\n{{Portal|Statistics}}\n* [[F-test|''F''-test]]\n* [[Hotelling's t-squared statistic|''t''<sup>2</sup>-statistic]]\n\n==References==\n{{Reflist}}\n\n==External links==\n\n{{Use dmy dates|date=November 2010}}\n\n{{DEFAULTSORT:T-statistic}}\n[[Category:Statistical ratios]]\n[[Category:Parametric statistics]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Truncated normal distribution",
      "url": "https://en.wikipedia.org/wiki/Truncated_normal_distribution",
      "text": "{{Probability distribution\n  | name       =\n  | type       = density\n  | pdf_image  = tnormPDF.png\n  | pdf_caption = Probability density function for the truncated normal distribution for different sets of parameters. In all cases, ''a'' = −10 and ''b'' = 10. For the black: ''μ'' = −8, ''σ'' = 2; blue: ''μ'' = 0, ''σ'' = 2; red: ''μ'' = 9, ''σ'' = 10; orange: ''μ'' = 0, ''σ'' = 10.\n  | cdf_image  = tnormCDF.svg\n  | cdf_caption = Cumulative distribution function for the truncated normal distribution for different sets of parameters. In all cases, ''a'' = −10 and ''b'' = 10. For the black: ''μ'' = −8, ''σ'' = 2; blue: ''μ'' = 0, ''σ'' = 2; red: ''μ'' = 9, ''σ'' = 10; orange: ''μ'' = 0, ''σ'' = 10.\n  | notation   = <math>\\xi=\\frac{x-\\mu}{\\sigma},\\ \\alpha=\\frac{a-\\mu}{\\sigma},\\ \\beta=\\frac{b-\\mu}{\\sigma}</math><br><math>Z=\\Phi(\\beta)-\\Phi(\\alpha)</math>\n  | parameters = {{nowrap|''μ'' ∈ '''R'''}}<br>{{nowrap|''σ''<sup>2</sup>  ≥ 0 (but see definition)}}<br> {{nowrap|a ∈ '''R'''}} — minimum value of ''x'' <br> {{nowrap|b ∈ '''R'''}} — maximum value of ''x'' (''b'' > ''a'')\n  | support    = {{nowrap|''x'' ∈ [''a'',''b'']}}\n  | pdf        = <math>f(x;\\mu,\\sigma, a,b) = \\frac{\\phi(\\xi)}{\\sigma Z}\\,</math><ref name='ist-lecture-4'>{{cite web|title=Lecture 4: Selection|url=http://web.ist.utl.pt/~ist11038/compute/qc/,truncG/lecture4k.pdf|website=web.ist.utl.pt|publisher=[[Instituto Superior Técnico]]|accessdate=14 July 2015|page=1|date=November 11, 2002}}</ref>\n  | cdf        = <math>F(x;\\mu,\\sigma, a,b) = \\frac{\\Phi(\\xi) - \\Phi(\\alpha)}{Z}</math>\n  | mean       = <math>\\mu +  \\frac{\\phi(\\alpha)-\\phi(\\beta)}{Z}\\sigma</math>\n  | mode       = <math>\\left\\{\\begin{array}{ll}a, & \\mathrm{if}\\ \\mu<a \\\\ \\mu, & \\mathrm{if}\\ a\\le\\mu\\le b\\\\ b, & \\mathrm{if}\\ \\mu>b\\end{array}\\right.</math>\n  | variance   = <math>\\sigma^2\\left[1+\\frac{\\alpha\\phi(\\alpha)-\\beta\\phi(\\beta)}{Z}\n-\\left(\\frac{\\phi(\\alpha)-\\phi(\\beta)}{Z}\\right)^2\\right]</math>\n  | median = <math>\\mu + \\Phi^{-1}\\left(\\frac{\\Phi(\\alpha)+\\Phi(\\beta)}{2}\\right) \\sigma</math>\n  | skewness   = \n  | kurtosis   =\n  | entropy    = <math>\\ln(\\sqrt{2 \\pi e} \\sigma Z) + \\frac{\\alpha\\phi(\\alpha)-\\beta\\phi(\\beta)}{2Z}</math>\n  | mgf        = <math>e^{\\mu t + \\sigma^2 t^2 / 2}  \\left[ \\frac{ \\Phi(\\beta- \\sigma t) - \\Phi(\\alpha - \\sigma t)  }{\\Phi(\\beta) - \\Phi(\\alpha) }  \\right] </math>\n  | char       = \n}}\n\n{{distinguish|text=[[rectified Gaussian distribution]], where negative elements are reset to zero}}\n\nIn probability and statistics, the '''truncated normal distribution''' is the probability distribution derived from that of a [[normally distributed]] random variable by bounding the random variable from either below or above (or both). The truncated normal distribution has wide applications in statistics and [[econometrics]]. For example, it is used to model the probabilities of the binary outcomes in the [[probit model]] and to model censored data in the [[Tobit model]].\n\n==Definition==\n\nSuppose <math> X </math> has a normal distribution with mean <math>\\mu</math> and variance <math>\\sigma^2</math> and lies within the interval <math>(a,b), \\text{with} \\; -\\infty \\leq a < b \\leq \\infty </math>. Then <math>X</math> conditional on <math> a < X < b </math> has a truncated normal distribution.\n\nIts [[probability density function]], <math>f</math>, for <math> a \\leq x \\leq b </math>, is given by\n\n:<math>\nf(x;\\mu,\\sigma,a,b) = \\frac{\\phi(\\frac{x - \\mu}{\\sigma})}{\\sigma\\left(\\Phi(\\frac{b - \\mu}{\\sigma}) - \\Phi(\\frac{a - \\mu}{\\sigma})\\right) }</math>\n\nand by <math>f=0</math> otherwise.\n\nHere,\n:<math>\\phi(\\xi)=\\frac{1}{\\sqrt{2 \\pi}}\\exp\\left(-\\frac{1}{2}\\xi^2\\right)</math>\nis the probability density function of the [[standard normal distribution]] and <math>\\Phi(\\cdot)</math> is its [[cumulative distribution function]]\n:<math>\\Phi(x)=\\frac{1}{2} \\left( 1+\\operatorname{erf}(x/\\sqrt{2}) \\right).</math> \nBy definition, if <math>b=\\infty</math>, then <math>\\Phi\\left(\\tfrac{b - \\mu}{\\sigma}\\right) =1</math>, and similarly, if <math>a=-\\infty</math>, then <math>\\Phi\\left(\\tfrac{a - \\mu}{\\sigma}\\right) =0</math>.\n\n\nThe above formulae show that when <math>-\\infty<a<b<+\\infty</math> the scale parameter <math>\\sigma^2</math> of the truncated normal distribution is allowed to assume negative values. The parameter <math>\\sigma</math> is in this case imaginary, but the function <math>f</math> is nevertheless real, positive, and normalizable. The scale parameter <math>\\sigma^2</math> of the ''canonical'' normal distribution must be positive because the distribution would not be normalizable otherwise. The doubly truncated normal distribution, on the other hand, can in principle have a negative scale parameter (which is different from the variance, see summary formulae), because no such integrability problems arise on a bounded domain. In this case the distribution cannot be interpreted as a canonical normal conditional on <math> a < X < b </math>, of course, but can still be interpreted as a [[Maximum_entropy_probability_distribution|maximum-entropy distribution]] with first and second moments as constraints, and has an additional peculiar feature: it presents ''two'' local maxima instead of one, located at <math>x=a</math> and <math>x=b</math>.\n\n==Moments==\n\nIf the random variable has been truncated only from below, some probability mass has been shifted to higher values, giving a [[first-order stochastic dominance|first-order stochastically dominating]] distribution and hence increasing the mean to a value higher than the mean <math>\\mu</math> of the original normal distribution. Likewise, if the random variable has been truncated only from above, the truncated distribution has a mean less than <math>\\mu.</math>\n\nRegardless of whether the random variable is bounded above, below, or both, the truncation is a [[mean-preserving contraction]] combined with a mean-changing rigid shift, and hence the variance of the truncated distribution is less than the variance <math>\\sigma^2</math> of the original normal distribution.\n\nLet <math>\\alpha=(a-\\mu)/\\sigma</math> and <math>\\beta=(b-\\mu)/\\sigma: </math>\n\n;Two sided truncation:<ref>Johnson, N.L., Kotz, S., Balakrishnan, N. (1994) ''Continuous Univariate Distributions, Volume 1'', Wiley. {{isbn|0-471-58495-9}} (Section 10.1)</ref> \n:<math> \\operatorname{E}(X \\mid a<X<b) = \\mu +  \\sigma\\frac{\\phi(\\frac{a-\\mu}{\\sigma})-\\phi(\\frac{b-\\mu}{\\sigma})}{\\Phi(\\frac{b-\\mu}{\\sigma})-\\Phi(\\frac{a-\\mu}{\\sigma})}\\! = \\mu +  \\sigma\\frac{\\phi(\\alpha)-\\phi(\\beta)}{\\Phi(\\beta)-\\Phi(\\alpha)}\\!</math>\n:<math> \\operatorname{Var}(X \\mid a<X<b) = \\sigma^2\\left[1+\\frac{\\frac{a-\\mu}{\\sigma}\\phi(\\frac{a-\\mu}{\\sigma})-\\frac{b-\\mu}{\\sigma}\\phi(\\frac{b-\\mu}{\\sigma})}{\\Phi(\\frac{b-\\mu}{\\sigma})-\\Phi(\\frac{a-\\mu}{\\sigma})}\n-\\left(\\frac{\\phi(\\frac{a-\\mu}{\\sigma})-\\phi(\\frac{b-\\mu}{\\sigma})}{\\Phi(\\frac{b-\\mu}{\\sigma})-\\Phi(\\frac{a-\\mu}{\\sigma})}\\right)^2\\right]\\!\n = \\sigma^2\\left[1+\\frac{\\alpha\\phi(\\alpha)-\\beta\\phi(\\beta)}{\\Phi(\\beta)-\\Phi(\\alpha)}\n-\\left(\\frac{\\phi(\\alpha)-\\phi(\\beta)}{\\Phi(\\beta)-\\Phi(\\alpha)}\\right)^2\\right]\\!</math>\n:Care must be taken in the numerical evaluation of these formulas, which can result in [[Loss of significance|catastrophic cancellation]] when the interval <math>[a,b]</math> does not include <math>\\mu</math>. There are better ways to rewrite them that avoid this issue.<ref name=\":0\">{{Citation|last=Fernandez-de-Cossio-Diaz|first=Jorge|title=TruncatedNormal.jl: Compute mean and variance of the univariate truncated normal distribution (works far from the peak)|date=2017-12-06|url=https://github.com/cossio/TruncatedNormal.jl|accessdate=2017-12-06}}</ref>\n\n;One sided truncation (of lower tail):<ref>{{cite book |last=Greene |first= William H. |title= Econometric Analysis (5th ed.)|publisher= Prentice Hall |year= 2003 |isbn= 978-0-13-066189-0 }}</ref>\nIn this case <math>\\; \\phi(\\beta)=0, \\; \\Phi(\\beta)=1,</math> and\n:<math> \\operatorname{E}(X \\mid X>a) = \\mu +\\sigma \\phi(\\alpha)/Z ,\\!</math>\n:<math> \\operatorname{Var}(X \\mid X>a) = \\sigma^2[1+ \\alpha  \\phi(\\alpha)/Z- (\\phi(\\alpha)/Z)^2 ],</math>\n\nwhere <math> Z=1-\\Phi(\\alpha). </math>\n\n;One sided truncation (of upper tail):\n:<math> \\operatorname{E}(X \\mid X<b) = \\mu -\\sigma\\frac{\\phi(\\beta)}{\\Phi(\\beta)} \\!</math>\n:<math> \\operatorname{Var}(X \\mid X<b) = \\sigma^2\\left[1-\\beta \\frac{\\phi(\\beta)}{\\Phi(\\beta)}- \\left(\\frac{\\phi(\\beta)}{\\Phi(\\beta)} \\right)^2\\right].\\!</math>\n\nBarr and Sherrill (1999) give a simpler expression for the variance of one sided truncations. Their formula is in terms of the chi-square CDF, which is implemented in standard software libraries.  Bebu and Mathew (2009) provide formulas for (generalized) confidence intervals around the truncated moments.\n\n;A recursive formula\n\nAs for the non-truncated case, there is a recursive formula for the truncated moments.<ref>Document by Eric Orjebin, \"http://www.smp.uq.edu.au/people/YoniNazarathy/teaching_projects/studentWork/EricOrjebin_TruncatedNormalMoments.pdf\"</ref>\n\n'''Multivariate'''\n\nComputing the moments of a multivariate truncated normal is harder. There are codes available to compute the moments in the bivariate case, for example <ref name=\":0\" />.\n\n== Simulating ==\nA random variate x defined as\n<math> x = \\Phi^{-1}( \\Phi(\\alpha) + U\\cdot(\\Phi(\\beta)-\\Phi(\\alpha)))\\sigma + \\mu </math> \nwith <math>\\Phi</math> the cumulative distribution function and <math>\\Phi^{-1}</math> its inverse, <math>U</math> a uniform random number on <math>(0, 1)</math>, follows the distribution truncated to the range <math>(a, b)</math>. This is simply the [[inverse transform method]] for simulating random variables. Although one of the simplest, this method can either fail\nwhen sampling in the tail of the normal distribution,<ref>{{cite book|last1=Kroese|first1=D. P.|last2=Taimre|first2=T.|last3=Botev|first3=Z. I.|title=Handbook of Monte Carlo methods|year=2011 |publisher=John Wiley & Sons}}</ref> or be much too slow.<ref name=\"boLec17\">{{cite conference |title=Simulation from the Normal Distribution Truncated to an Interval in the Tail |last1=Botev |first1=Z. I.  |last2=L'Ecuyer |first2=P.  |date=2017 |publisher=ACM|ISBN=978-1-63190-141-6 |book-title=10th EAI International Conference on Performance Evaluation Methodologies and Tools |pages=23–29 |location=  25th–28th Oct 2016 Taormina, Italy |doi= 10.4108/eai.25-10-2016.2266879 }}\n</ref> Thus, in  practice, one has to find alternative methods of simulation.\n\nOne such truncated normal generator (implemented in [http://www.mathworks.com/matlabcentral/fileexchange/53180-truncated-normal-generator Matlab] and\nin [[R (programming language)]] as [https://cran.r-project.org/web/packages/TruncatedNormal trandn.R]  ) is based on an acceptance rejection idea due to Marsaglia.<ref>{{cite journal|last1=Marsaglia|first1=George|title=Generating a variable from the tail of the normal distribution|journal=Technometrics|date=1964|volume=6|issue=1|pages=101–102|doi=10.2307/1266749|jstor=1266749}}</ref> Despite the slightly suboptimal acceptance rate of Marsaglia (1964) in comparison with Robert (1995),  Marsaglia's method is typically  faster,<ref name=\"boLec17\"/> because it does not require the costly numerical evaluation of the exponential function.    \n    \nFor more on simulating a draw from the truncated normal distribution, see Robert (1995), Lynch (2007) Section 8.1.3 (pages 200–206), Devroye (1986).  The [https://cran.r-project.org/web/packages/msm/index.html MSM] package in R has a function, [https://web.archive.org/web/20120208134826/http://rss.acs.unt.edu/Rdoc/library/msm/html/tnorm.html rtnorm], that calculates draws from a truncated normal.  The [https://cran.r-project.org/web/packages/truncnorm/ truncnorm] package in R also has functions to draw from a truncated normal.\n\nChopin (2011) proposed ([https://arxiv.org/abs/1201.6140 arXiv]) an algorithm inspired from the Ziggurat algorithm of Marsaglia and Tsang (1984, 2000), which is usually considered as the fastest Gaussian sampler, and is also very close to Ahrens’s algorithm (1995). Implementations can be found in [http://www.crest.fr/ckfinder/userfiles/files/Pageperso/chopin/truncnorm_20120618.tgz C], [http://miv.u-strasbg.fr/mazet/rtnorm/rtnormCpp.zip C++], [http://miv.u-strasbg.fr/mazet/rtnorm/rtnormM.zip Matlab] and [http://www.christophlassner.de/blog/2013/08/12/Generation-of-Truncated-Gaussian-Samples/ Python].\n\nSampling from the ''multivariate'' truncated normal distribution \nis considerably more difficult.<ref name=\"bo16\">{{cite journal|last1=Botev|first1=Z. I.|title=The normal law under linear restrictions: simulation and estimation via minimax tilting|journal=Journal of the Royal Statistical Society, Series B|volume=79|pages=125–148|date=2016|doi=10.1111/rssb.12162|arxiv=1603.04166}}</ref> Exact or perfect simulation is only feasible in the case of \ntruncation of the normal distribution to a polytope region.<ref name=\"bo16\"/> In  more general cases, Damien and Walker (2001) introduce a general methodology for sampling truncated densities within a [[Gibbs sampling]] framework. Their algorithm introduces one latent variable and, within a Gibbs sampling framework, it is more computationally efficient than the algorithm of Robert (1995).\n\n== See also ==\n* [[Normal distribution]]\n* [[Truncated distribution]]\n* [[PERT distribution]]\n\n{{More footnotes|date=June 2010}}\n\n==References==\n<references/>\n\n* {{cite book |last=Greene |first= William H. |title= Econometric Analysis (5th ed.)|publisher= Prentice Hall |year= 2003 |isbn= 978-0-13-066189-0 }}\n* Norman L. Johnson and Samuel Kotz (1970). ''Continuous univariate distributions-1'', chapter 13. John Wiley & Sons.\n* {{cite book|last=Lynch|first=Scott|title=Introduction to Applied Bayesian Statistics and Estimation for Social Scientists|year=2007|publisher=Springer|location=New York|isbn=978-1-4419-2434-6|url=https://www.springer.com/social+sciences/book/978-0-387-71264-2}}\n* {{cite journal|last=Robert|first=Christian P.|title=Simulation of truncated normal variables|journal=Statistics and Computing|year=1995|volume=5|issue=2|pages=121–125|doi=10.1007/BF00143942|arxiv=0907.4010}}\n* {{cite journal|last1=Barr|first1=Donald R.|last2=Sherrill|first2=E.Todd|title=Mean and variance of truncated normal distributions|journal=The American Statistician|year=1999|volume=53|issue=4|pages=357–361|doi=10.1080/00031305.1999.10474490}}\n* {{cite journal|last1=Bebu|first1=Ionut|last2=Mathew|first2=Thomas|title=Confidence intervals for limited moments and truncated moments in normal and lognormal models|journal=Statistics and Probability Letters|year=2009|volume=79|issue=3|pages=375–380|doi=10.1016/j.spl.2008.09.006}}\n* {{cite journal|last1=Damien|first1=Paul|last2=Walker|first2=Stephen G.|title=Sampling truncated normal, beta, and gamma densities|journal=Journal of Computational and Graphical Statistics|year=2001|volume=10|issue=2|pages=206–215|doi=10.1198/10618600152627906}}\n* Nicolas Chopin, \"Fast simulation of truncated Gaussian distributions\". ''Statistics and Computing'' '''21'''(2): 275-288, 2011, [[Digital Object Identifier|doi:]][https://dx.doi.org/10.1007/s11222-009-9168-1 10.1007/s11222-009-9168-1]\n* {{cite web|last1=Burkardt|first1=John|title=The Truncated Normal Distribution|url=https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf|website=Department of Scientific Computing website|publisher=Florida State University|accessdate=15 February 2018}}\n\n{{ProbDistributions|continuous-semi-infinite}}\n\n[[Category:Continuous distributions]]\n[[Category:Normal distribution]]\n\n[[fr:Loi tronquée#Loi normale tronquée]]"
    },
    {
      "title": "Whittle likelihood",
      "url": "https://en.wikipedia.org/wiki/Whittle_likelihood",
      "text": "In [[statistics]], '''Whittle likelihood''' is an approximation to the [[likelihood function]] of a stationary Gaussian [[time series]]. It is named after the mathematician and statistician [[Peter Whittle (mathematician)|Peter Whittle]], who introduced it in his PhD thesis in 1951.<ref name=\"Whittle1951\">{{cite book|last=Whittle|first=P.|title=Hypothesis testing in times series analysis|publisher=Almqvist & Wiksells Boktryckeri AB|place=Uppsala|year=1951}}</ref>\nIt is commonly utilized in [[time series analysis]] and [[signal processing]] for parameter estimation and signal detection.\n\n== Context ==\nIn a [[Gaussian process|stationary Gaussian time series model]], the [[likelihood function]] is (as usual in Gaussian models) a function of the associated mean and covariance parameters. With a large number (<math>N</math>) of observations, the (<math>N \\times N</math>) covariance matrix may become very large, making computations very costly in practice. However, due to stationarity, the covariance matrix has a rather simple structure, and by using an approximation, computations may be simplified considerably (from <math>O(N^2)</math> to <math>O(N\\log(N))</math>).<ref>{{cite web|last=Hurvich|first=C.|title=Whittle's approximation to the likelihood function|publisher=[[NYU Stern]]|url=http://people.stern.nyu.edu/churvich/TimeSeries/Handouts/Whittle.pdf|year=2002}}</ref> The idea effectively boils down to assuming a [[heteroscedasticity|heteroscedastic]] zero-mean Gaussian model in [[Fourier domain]]; the model formulation is based on the time series' [[discrete Fourier transform]] and its [[power spectral density]].<ref name=\"CalderDavis\">{{Citation|last1=Calder|first1=M.|last2=Davis|first2=R. A.|editor1-last=Kotz|editor1-first=S.|editor2-last=Johnson|editor2-first=N. L.|title=Breakthroughs in Statistics|chapter=An introduction to Whittle (1953) \"The analysis of multiple stationary time series\"|pages=141–169|publisher=Springer-Verlag|place=New York|year=1997|doi=10.1007/978-1-4612-0667-5_7|series=Springer Series in Statistics|isbn=978-0-387-94989-5}} <br> See also: {{Citation|last1=Calder|first1=M.|last2=Davis|first2=R. A.|title=An introduction to Whittle (1953) \"The analysis of multiple stationary time series\"|work=Technical report 1996/41|publisher=Department of Statistics, [[Colorado State University]]|url=http://www.stat.colostate.edu/statresearch/stattechreports.html|year=1996}}</ref><ref name=\"hannan1994\">{{Citation|last=Hannan|first=E. J.|editor-last=Kelly|editor-first=F. P.|title=Probability, statistics and optimization; a tribute to Peter Whittle|chapter=The Whittle likelihood and frequency estimation|publisher=Wiley|place=Chichester|year=1994}}</ref><ref>{{Citation|last1=Pawitan|first1=Y.|editor1-last=Kotz|editor1-first=S.|editor2-last=Read|editor2-first=C. B.|editor3-last=Banks|editor3-first=D. L.|title=Encyclopedia of Statistical Sciences|chapter=Whittle likelihood|pages=708–710|volume=Update Volume 2|publisher=Wiley & Sons|place=New York|year=1998|doi=10.1002/0471667196.ess0753|isbn=978-0471667193}}</ref>\n\n== Definition ==\nLet <math>X_1,\\ldots,X_N</math> be a stationary Gaussian time series with (''one-sided'') power spectral density <math>S_1(f)</math>, where <math>N</math> is even and samples are taken at constant sampling intervals <math>\\Delta_t</math>.\nLet <math>\\tilde{X}_1,\\ldots,\\tilde{X}_{N/2+1}</math> be the (complex-valued) [[discrete Fourier transform]] (DFT) of the time series. Then for the Whittle likelihood one effectively assumes independent zero-mean [[Gaussian distribution]]s for all <math>\\tilde{X}_j</math> with variances for the real and imaginary parts given by\n\n:<math> \\operatorname{Var}\\left(\\operatorname{Re}(\\tilde{X}_j)\\right) = \\operatorname{Var} \\left( \\operatorname{Im} (\\tilde{X}_j)\\right) = S_1(f_j)</math>\n\nwhere <math>f_j=\\frac j {N \\, \\Delta_t}</math> is the <math>j</math>th Fourier frequency. This approximate model immediately leads to the (logarithmic) likelihood function\n\n:<math>\\log\\left(P(x_1,\\ldots,x_N)\\right) \\propto -\\sum_j \\left( \\log\\left( S_1(f_j) \\right) + \\frac{|\\tilde{x}_j|^2}{\\frac N {2\\,\\Delta_t} S_1(f_j)} \\right)</math>\n\nwhere <math>|\\cdot|</math> denotes the absolute value with <math>|\\tilde{x}_j|^2=\\left(\\operatorname{Re}(\\tilde{x}_j)\\right)^2 + \\left( \\operatorname{Im} (\\tilde{x}_j)\\right)^2</math>.<ref name=\"CalderDavis\" /><ref name=\"hannan1994\" /><ref name=\"RoeverEtAl2011a\">{{cite journal|last1=Röver|first1=C.|last2=Meyer|first2=R.|last3=Christensen|first3=N.|title=Modelling coloured residual noise in gravitational-wave signal processing|journal=Classical and Quantum Gravity|volume=28|issue=1|year=2011|pages=025010|doi=10.1088/0264-9381/28/1/015010|arxiv=0804.3853|bibcode=2011CQGra..28a5010R}}</ref>\n\n== Special case of a known noise spectrum ==\nIn case the noise spectrum is assumed a-priori ''known'', and noise properties are not to be inferred from the data, the likelihood function may be simplified further by ignoring constant terms, leading to the sum-of-squares expression\n\n:<math>\\log\\left(P(x_1,\\ldots,x_N)\\right) \\;\\propto\\; -\\sum_j\\frac{|\\tilde{x}_j|^2}{\\frac N {2 \\, \\Delta_t} S_1(f_j)}</math>\n\nThis expression also is the basis for the common [[matched filter]].\n\n== Accuracy of approximation ==\nThe Whittle likelihood in general is only an approximation, it is only exact if the spectrum is constant, i.e., in the trivial case of [[white noise]].\nThe [[efficiency (statistics)|efficiency]] of the Whittle approximation always depends on the particular circumstances.<ref>{{cite journal|last1=Choudhuri|first1=N.|last2=Ghosal|first2=S.|last3=Roy|first3=A.|title=Contiguity of the Whittle measure for a Gaussian time series|journal=Biometrika|volume=91|issue=4|year=2004|pages=211–218|doi=10.1093/biomet/91.1.211}}</ref>\n<ref>{{cite journal|last1=Countreras-Cristán|first1=A.|last2=Gutiérrez-Peña|first2=E.|last3=Walker|first3=S. G.|title=A Note on Whittle's Likelihood|journal=Communications in Statistics – Simulation and Computation|volume=35|issue=4|year=2006|pages=857–875|doi=10.1080/03610910600880203}}</ref>\n\nNote that due to [[linearity]] of the Fourier transform, Gaussianity in Fourier domain implies Gaussianity in time domain and vice versa. What makes the Whittle likelihood only approximately accurate is related to the [[sampling theorem]]—the effect of Fourier-transforming only a ''finite'' number of data points, which also manifests itself as [[spectral leakage]] in related problems (and which may be ameliorated using the same methods, namely, [[window function|windowing]]). In the present case, the implicit periodicity assumption implies correlation between the first and last samples (<math>x_1</math> and <math>x_N</math>), which are effectively treated as \"neighbouring\" samples (like <math>x_1</math> and <math>x_2</math>).\n\n== Applications ==\n\n===Parameter estimation===\nWhittle's likelihood is commonly used to estimate signal parameters for signals that are buried in non-white noise. The [[power spectral density|noise spectrum]] then may be assumed known,<ref name=\"finn1992\">{{cite journal|last=Finn|first=L. S.|title=Detection, measurement and gravitational radiation|journal=Physical Review D|volume=46|issue=12|year=1992|pages=5236–5249|doi=10.1103/PhysRevD.46.5236|arxiv=gr-qc/9209010|bibcode=1992PhRvD..46.5236F}}</ref>\nor it may be inferred along with the signal parameters.<ref name=\"hannan1994\" /><ref name=\"RoeverEtAl2011a\" />\n\n===Signal detection===\nSignal detection is commonly performed utilizing the [[matched filter]], which is based on the Whittle likelihood for the case of a ''known'' noise power spectral density.<ref>{{cite journal|last=Turin|first=G. L.|title=An introduction to matched filters|journal=IRE Transactions on Information Theory|year=1960|volume=6|issue=3|pages=311–329|doi=10.1109/TIT.1960.1057571}}</ref><ref>{{cite book|last1=Wainstein|first1=L. A.|last2=Zubakov|first2=V. D.|title=Extraction of signals from noise|publisher=Prentice-Hall|place=Englewood Cliffs, NJ|year=1962}}</ref>\nThe matched filter effectively does a [[maximum-likelihood]] fit of the signal to the noisy data and uses the resulting [[Likelihood-ratio test|likelihood ratio]] as the detection statistic.<ref name=\"Roever2011b\">{{cite journal|last=Röver|first=C.|title=Student-t-based filter for robust signal detection|journal=Physical Review D|volume=84|issue=12|year=2011|pages=122004|doi=10.1103/PhysRevD.84.122004|arxiv=1109.0442|bibcode=2011PhRvD..84l2004R}}</ref>\n\nThe matched filter may be generalized to an analogous procedure based on a [[Student-t distribution]] by also considering uncertainty (e.g. [[spectral density estimation|estimation]] uncertainty) in the noise spectrum. On the technical side, this entails repeated or iterative matched-filtering.<ref name=\"Roever2011b\" />\n\n===Spectrum estimation===\nThe Whittle likelihood is also applicable for estimation of the [[power spectral density|noise spectrum]], either alone or in conjunction with signal parameters.<ref>{{cite journal|last1=Choudhuri|first1=N.|last2=Ghosal|first2=S.|last3=Roy|first3=A.|title=Bayesian estimation of the spectral density of a time series|journal=Journal of the American Statistical Association|volume=99|issue=468|year=2004|pages=1050–1059|doi=10.1198/016214504000000557|url=http://www4.stat.ncsu.edu/~sghosal/papers/specden.pdf|citeseerx=10.1.1.212.2814}}</ref><ref>{{cite journal|last1=Edwards|first1=M. C.|last2=Meyer|first2=R.|last3=Christensen|first3=N.|title=Bayesian semiparametric power spectral density estimation in gravitational wave data analysis|journal=Physical Review D|volume=92|issue=6|year=2015|pages=064011|doi=10.1103/PhysRevD.92.064011|arxiv=1506.00185|bibcode=2015PhRvD..92f4011E}}</ref>\n\n== See also ==\n*[[colors of noise|Coloured noise]]\n*[[Discrete Fourier transform]]\n*[[Likelihood function]]\n*[[Matched filter]]\n*[[Power spectral density]]\n*[[Statistical signal processing]]\n*[[Weighted least squares]]\n\n== References ==\n{{Reflist}}\n\n{{Statistics|analysis}}\n\n<!--- Categories --->\n[[Category:Time series]]\n[[Category:Time series models]]\n[[Category:Frequency-domain analysis]]\n[[Category:Statistical inference]]\n[[Category:Statistical models]]\n[[Category:Statistical signal processing]]\n[[Category:Signal estimation]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Wrapped normal distribution",
      "url": "https://en.wikipedia.org/wiki/Wrapped_normal_distribution",
      "text": "{{Probability distribution|\n  name       =Wrapped Normal|\n  type       =density|\n  pdf_image =[[File:WrappedNormalPDF.png|325px|Plot of the von Mises PMF]]<br /><small>The support is chosen to be [-π,π] with μ=0</small>|\n  cdf_image  =[[File:WrappedNormalCDF.png|325px|Plot of the von Mises CMF]]<br /><small>The support is chosen to be [-π,π] with μ=0</small>|\n  parameters =<math>\\mu</math> real<br><math>\\sigma>0</math>|\n  support    =<math>\\theta \\in</math> any interval of length 2π|\n  pdf        =<math>\\frac{1}{2\\pi}\\vartheta\\left(\\frac{\\theta-\\mu}{2\\pi},\\frac{i\\sigma^2}{2\\pi}\\right)</math>|\n  cdf        =|\n  mean       =<math>\\mu</math> if support is on interval <math>\\mu\\pm \\pi</math>|\n  median     =<math>\\mu</math> if support is on interval <math>\\mu\\pm \\pi</math>|\n  mode       =<math>\\mu</math>|\n  variance   =<math>1-e^{-\\sigma^2/2}</math> (circular)|\n  skewness   =|\n  kurtosis   =|\n  entropy    =(see text)|\n  mgf        =|\n  cf         =<math>e^{-\\sigma^2n^2/2+in\\mu}</math>|\n}}\nIn [[probability theory]] and [[directional statistics]], a '''wrapped normal distribution''' is a [[wrapped distribution|wrapped probability distribution]] that results from the \"wrapping\" of the [[normal distribution]] around the [[unit circle]]. It finds application in the theory of [[Brownian motion]] and is a solution to the [[Theta function#A solution to heat equation|heat equation]] for [[periodic boundary conditions]]. It is closely approximated by the [[von Mises distribution]], which, due to its mathematical simplicity and tractability, is the most commonly used distribution in directional statistics.\n\n==Definition==\nThe [[probability density function]] of the wrapped normal distribution is<ref name=\"Mardia99\">{{cite book |title=Directional Statistics |last=Mardia |first=Kantilal |authorlink=Kantilal Mardia |author2=Jupp, Peter E.  |year=1999|publisher=Wiley |location= |isbn=978-0-471-95333-3 |url=https://www.amazon.com/Directional-Statistics-Kanti-V-Mardia/dp/0471953334/ref=sr_1_1?s=books&ie=UTF8&qid=1311003484&sr=1-1#reader_0471953334 |accessdate=2011-07-19}}</ref>\n\n:<math>\nf_{WN}(\\theta;\\mu,\\sigma)=\\frac{1}{\\sigma \\sqrt{2\\pi}} \\sum^{\\infty}_{k=-\\infty} \\exp \\left[\\frac{-(\\theta - \\mu + 2\\pi k)^2}{2 \\sigma^2} \\right],\n</math>\n\nwhere ''μ'' and ''σ'' are the mean and standard deviation of the unwrapped distribution, respectively. [[Wrapped distribution|Expressing]] the above density function in terms of the [[characteristic function (probability theory)|characteristic function]] of the normal distribution yields:<ref name=\"Mardia99\"/>\n\n:<math>\nf_{WN}(\\theta;\\mu,\\sigma)=\\frac{1}{2\\pi}\\sum_{n=-\\infty}^\\infty e^{-\\sigma^2n^2/2+in(\\theta-\\mu)} =\\frac{1}{2\\pi}\\vartheta\\left(\\frac{\\theta-\\mu}{2\\pi},\\frac{i\\sigma^2}{2\\pi}\\right) ,\n</math>\n\nwhere <math>\\vartheta(\\theta,\\tau)</math> is the [[Theta function|Jacobi theta function]], given by\n\n:<math>\n\\vartheta(\\theta,\\tau)=\\sum_{n=-\\infty}^\\infty (w^2)^n q^{n^2}\n \\text{ where } w \\equiv e^{i\\pi \\theta}</math> and <math>q \\equiv e^{i\\pi\\tau} .</math>\n\nThe wrapped normal distribution may also be expressed in terms of the [[Jacobi triple product]]:<ref name=\"W&W\">{{cite book |title=A Course of Modern Analysis |last=Whittaker |first=E. T. |authorlink= |author2=Watson, G. N.  |year=2009 |publisher=Book Jungle |location= |isbn=978-1-4385-2815-1 |page= |pages= |url= |accessdate=}}</ref>\n\n:<math>f_{WN}(\\theta;\\mu,\\sigma)=\\frac{1}{2\\pi}\\prod_{n=1}^\\infty (1-q^n)(1+q^{n-1/2}z)(1+q^{n-1/2}/z) .</math>\n\nwhere <math>z=e^{i(\\theta-\\mu)}\\,</math> and <math>q=e^{-\\sigma^2}.</math>\n\n== Moments ==\n\nIn terms of the circular variable <math>z=e^{i\\theta}</math> the circular moments of the wrapped normal distribution are the characteristic function of the normal distribution evaluated at integer arguments:\n\n:<math>\\langle z^n\\rangle=\\int_\\Gamma e^{in\\theta}\\,f_{WN}(\\theta;\\mu,\\sigma)\\,d\\theta = e^{i n \\mu-n^2\\sigma^2/2}.</math>\n\nwhere <math>\\Gamma\\,</math> is some interval of length <math>2\\pi</math>. The first moment is then the average value of ''z'', also known as the mean resultant, or mean resultant vector:\n\n:<math>\n\\langle z \\rangle=e^{i\\mu-\\sigma^2/2}\n</math>\n\nThe mean angle is\n\n:<math>\n\\theta_\\mu=\\mathrm{Arg}\\langle z \\rangle = \\mu\n</math>\n\nand the length of the mean resultant is\n\n:<math>\nR=|\\langle z \\rangle| = e^{-\\sigma^2/2}\n</math>\n\nThe circular standard deviation, which is a useful measure of dispersion for the wrapped normal distribution and its close relative, the [[von Mises distribution]] is given by:\n\n:<math>\ns=\\ln(R^{-2})^{1/2} = \\sigma\n</math>\n\n== Estimation of parameters ==\n\nA series of ''N'' measurements ''z''<sub>''n''</sub>&nbsp;=&nbsp;''e''<sup>&nbsp;''i&theta;''<sub>''n''</sub></sup> drawn from a wrapped normal distribution may be used to estimate certain parameters of the distribution. The average of the series {{overbar|''z''}} is defined as\n\n:<math>\\overline{z}=\\frac{1}{N}\\sum_{n=1}^N z_n</math>\n\nand its expectation value will be just the first moment:\n\n:<math>\\langle\\overline{z}\\rangle=e^{i\\mu-\\sigma^2/2}. \\,</math>\n\nIn other words, {{overbar|''z''}} is an unbiased estimator of the first moment. If we assume that the mean ''&mu;'' lies in the interval <nowiki>[</nowiki>&minus;''&pi;'',&nbsp;''&pi;''<nowiki>)</nowiki>, then Arg&nbsp;{{overbar|''z''}} will be a (biased) estimator of the mean&nbsp;''&mu;''.\n\nViewing the ''z''<sub>''n''</sub> as a set of vectors in the complex plane, the {{overbar|''R''}}<sup>2</sup> statistic is the square of the length of the averaged vector:\n\n:<math>\\overline{R}^2=\\overline{z}\\,\\overline{z^*}=\\left(\\frac{1}{N}\\sum_{n=1}^N \\cos\\theta_n\\right)^2+\\left(\\frac{1}{N}\\sum_{n=1}^N \\sin\\theta_n\\right)^2 \\, </math>\n\nand its expected value is:\n\n:<math>\\left\\langle \\overline{R}^2\\right\\rangle = \\frac{1}{N}+\\frac{N-1}{N}\\,e^{-\\sigma^2}\\,</math>\n\nIn other words, the statistic\n\n:<math>R_e^2=\\frac{N}{N-1}\\left(\\overline{R}^2-\\frac{1}{N}\\right)</math>\n\nwill be an unbiased estimator of ''e''<sup>&minus;''&sigma;''<sup>2</sup></sup>, and ln(1/''R''<sub>''e''</sub><sup>2</sup>) will be a (biased) estimator of&nbsp;''&sigma;''<sup>2</sup>\n\n== Entropy ==\n\nThe [[Entropy (information theory)|information entropy]] of the wrapped normal distribution is defined as:<ref name=\"Mardia99\"/>\n\n:<math>H = -\\int_\\Gamma f_{WN}(\\theta;\\mu,\\sigma)\\,\\ln(f_{WN}(\\theta;\\mu,\\sigma))\\,d\\theta</math>\n\nwhere <math>\\Gamma</math> is any interval of length <math>2\\pi</math>. Defining <math>z=e^{i(\\theta-\\mu)}</math> and <math>q=e^{-\\sigma^2}</math>, the [[Jacobi triple product]] representation for the wrapped normal is:\n\n:<math>f_{WN}(\\theta;\\mu,\\sigma) = \\frac{\\phi(q)}{2\\pi}\\prod_{m=1}^\\infty (1+q^{m-1/2}z)(1+q^{m-1/2}z^{-1})</math>\n\nwhere <math>\\phi(q)\\,</math> is the [[Euler function]]. The logarithm of the density of the wrapped normal distribution may be written:\n\n:<math>\\ln(f_{WN}(\\theta;\\mu,\\sigma))=  \\ln\\left(\\frac{\\phi(q)}{2\\pi}\\right)+\\sum_{m=1}^\\infty\\ln(1+q^{m-1/2}z)+\\sum_{m=1}^\\infty\\ln(1+q^{m-1/2}z^{-1})</math>\n\nUsing the series expansion for the logarithm:\n\n:<math>\\ln(1+x)=-\\sum_{k=1}^\\infty \\frac{(-1)^k}{k}\\,x^k</math>\n\nthe logarithmic sums may be written as:\n\n:<math>\\sum_{m=1}^\\infty\\ln(1+q^{m-1/2}z^{\\pm 1})=-\\sum_{m=1}^\\infty \\sum_{k=1}^\\infty \\frac{(-1)^k}{k}\\,q^{mk-k/2}z^{\\pm k} = -\\sum_{k=1}^\\infty \\frac{(-1)^k}{k}\\,\\frac{q^{k/2}}{1-q^k}\\,z^{\\pm k}</math>\n\nso that the logarithm of density of the wrapped normal distribution may be written as:\n\n:<math>\\ln(f_{WN}(\\theta;\\mu,\\sigma))=\\ln\\left(\\frac{\\phi(q)}{2\\pi}\\right)-\\sum_{k=1}^\\infty \\frac{(-1)^k}{k} \\frac{q^{k/2}}{1-q^k}\\,(z^k+z^{-k}) </math>\n\nwhich is essentially a [[Fourier series]] in <math>\\theta\\,</math>. Using the characteristic function representation for the wrapped normal distribution in the left side of the integral:\n\n:<math>f_{WN}(\\theta;\\mu,\\sigma) =\\frac{1}{2\\pi}\\sum_{n=-\\infty}^\\infty q^{n^2/2}\\,z^n</math>\n\nthe entropy may be written:\n\n:<math>H = -\\ln\\left(\\frac{\\phi(q)}{2\\pi}\\right)+\\frac{1}{2\\pi}\\int_\\Gamma \\left( \\sum_{n=-\\infty}^\\infty\\sum_{k=1}^\\infty \\frac{(-1)^k}{k} \\frac{q^{(n^2+k)/2}}{1-q^k}\\left(z^{n+k}+z^{n-k}\\right) \\right)\\,d\\theta</math>\n\nwhich may be integrated to yield:\n\n:<math>H = -\\ln\\left(\\frac{\\phi(q)}{2\\pi}\\right)+2\\sum_{k=1}^\\infty \\frac{(-1)^k}{k}\\, \\frac{q^{(k^2+k)/2}}{1-q^k}</math>\n\n== See also ==\n\n* [[Wrapped distribution]]\n* [[Dirac comb]]\n* [[Wrapped Cauchy distribution]]\n\n== References ==\n{{More footnotes|date=June 2014}}\n<references/>\n* {{cite book |title=Statistics of Earth Science Data |last=Borradaile |first=Graham |year=2003 |publisher=Springer |isbn=978-3-540-43603-4 |url=https://books.google.com/books?id=R3GpDglVOSEC&printsec=frontcover&source=gbs_navlinks_s#v=onepage&q=&f=false |accessdate=31 Dec 2009}}\n* {{cite book |title=Statistical Analysis of Circular Data |last=Fisher |first=N. I. |year=1996 |publisher=Cambridge University Press |location= |isbn=978-0-521-56890-6\n|url=https://books.google.com/books?id=IIpeevaNH88C&dq=%22circular+variance%22+fisher&source=gbs_navlinks_s |accessdate=2010-02-09}}\n* {{cite journal |last1=Breitenberger |first1=Ernst |year=1963 |title=Analogues of the normal distribution on the circle and the sphere |journal=Biometrika |volume=50 |pages=81 |url=http://biomet.oxfordjournals.org/cgi/pdf_extract/50/1-2/81 |doi=10.2307/2333749}}\n\n==External links==\n* [http://www.codeproject.com/Articles/190833/Circular-Values-Math-and-Statistics-with-Cplusplus Circular Values Math and Statistics with C++11], A C++11 infrastructure for circular values (angles, time-of-day, etc.) mathematics and statistics\n\n{{ProbDistributions|directional}}\n\n[[Category:Continuous distributions]]\n[[Category:Directional statistics]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Z-test",
      "url": "https://en.wikipedia.org/wiki/Z-test",
      "text": "{{DISPLAYTITLE:''Z''-test}}\n{{No footnotes|date=November 2009}}\nA '''''Z''-test''' is any [[statistics|statistical]] [[statistical hypothesis testing|test]] for which the [[probability distribution|distribution]] of the [[test statistic]] under the [[null hypothesis]] can be approximated by a [[normal distribution]]. Because of the [[central limit theorem]], many test statistics are approximately normally distributed for large samples. For each significance level, the ''Z''-test has a single critical value (for example, 1.96 for 5% two tailed) which makes it more convenient than the [[Student's t-test|Student's ''t''-test]] which has separate critical values for each sample size. Therefore, many statistical tests can be conveniently performed as approximate ''Z''-tests if the sample size is large or the population variance is known. If the population variance is unknown (and therefore has to be estimated from the sample itself) and the sample size is not large (n < 30), the Student's ''t''-test may be more appropriate. \n\nIf ''T'' is a statistic that is approximately normally distributed under the null hypothesis, the next step in performing a ''Z''-test is to estimate the [[expected value]] θ of ''T'' under the null hypothesis, and then obtain an estimate ''s'' of the [[standard deviation]] of ''T''. After that the [[standard score]] ''Z''&nbsp;=&nbsp;(''T''&nbsp;&minus;&nbsp;θ)&nbsp;/&nbsp;''s'' is calculated, from which [[one- and two-tailed tests|one-tailed and two-tailed]] [[p-values|''p''-values]] can be calculated as Φ(&minus;''Z'') (for upper-tailed tests), Φ(''Z'') (for lower-tailed tests) and 2Φ(&minus;|''Z''|) (for two-tailed tests) where Φ is the standard [[normal distribution|normal]] [[cumulative distribution function]].\n\n==Use in location testing==\n\nThe term \"''Z''-test\" is often used to refer specifically to the [[location test|one-sample location test]] comparing the mean of a set of measurements to a given constant when the sample variance is known. If the observed data ''X''<sub>1</sub>, ..., ''X''<sub>n</sub> are (i) independent, (ii) have a common mean μ, and (iii) have a common variance σ<sup>2</sup>, then the sample average <span style=\"text-decoration: overline\">''X''</span> has mean μ and variance σ<sup>2</sup>&nbsp;/&nbsp;''n''. \n\nThe null hypothesis is that the mean value of X is a given number μ<sub>0</sub>. \nWe can use <span style=\"text-decoration: overline\">''X''</span>&nbsp; \nas a test-statistic, rejecting the null hypothesis if <span style=\"text-decoration: overline\">''X''</span>&nbsp;− μ<sub>0</sub> is large.\n\nTo calculate the standardized statistic ''Z''&nbsp;=&nbsp;(<span style=\"text-decoration: overline\">''X''</span> &nbsp;−&nbsp; μ<sub>0</sub>)&nbsp;/&nbsp;''s'', we need to either know or have an approximate value for σ<sup>2</sup>, from which we can calculate ''s''<sup>2</sup>&nbsp;=&nbsp;σ<sup>2</sup>&nbsp;/&nbsp;''n''. In some applications, σ<sup>2</sup> is known, but this is uncommon. \n\nIf the sample size is moderate or large, we can substitute the [[Sample variance#Population variance and sample variance|sample variance]] for σ<sup>2</sup>, giving a ''plug-in'' test. The resulting test will not be an exact ''Z''-test since the uncertainty in the sample variance is not accounted for—however, it will be a good approximation unless the sample size is small. \n\nA [[t-test|''t''-test]] can be used to account for the uncertainty in the sample variance when the data are exactly [[normal distribution|normal]]. \n\nThere is no universal constant at which the sample size is generally considered large enough to justify use of the plug-in test. Typical rules of thumb: the sample size should be 50 observations or more. \n\nFor large sample sizes, the ''t''-test procedure gives almost identical ''p''-values as the ''Z''-test procedure.\n\nOther location tests that can be performed as ''Z''-tests are the two-sample location test and the [[paired difference test]].\n\n== Conditions ==\n\nFor the ''Z''-test to be applicable, certain conditions must be met.\n\n* [[Nuisance parameter]]s should be known, or estimated with high accuracy (an example of a nuisance parameter would be the [[standard deviation]] in a one-sample location test). ''Z''-tests focus on a single parameter, and treat all other unknown parameters as being fixed at their true values. In practice, due to [[Slutsky's theorem]], \"plugging in\" [[consistent estimator|consistent]] estimates of nuisance parameters can be justified. However if the sample size is not large enough for these estimates to be reasonably accurate, the ''Z''-test may not perform well.\n* The test statistic should follow a [[normal distribution]]. Generally, one appeals to the [[central limit theorem]] to justify assuming that a test statistic varies normally. There is a great deal of statistical research on the question of when a test statistic varies approximately normally. If the variation of the test statistic is strongly non-normal, a ''Z''-test should not be used.\n\nIf estimates of nuisance parameters are plugged in as discussed above, it is important to use estimates appropriate for the way the data were [[sampling (statistics)|sampled]]. In the special case of ''Z''-tests for the one or two sample location problem, the usual sample standard deviation is only appropriate if the data were collected as an independent sample.\n\nIn some situations, it is possible to devise a test that properly accounts for the variation in plug-in estimates of nuisance parameters. In the case of one and two sample location problems, a [[t-test|''t''-test]] does this.\n\n== Example ==\n\nSuppose that in a particular geographic region, the mean and standard deviation of scores on a reading test are 100 points, and 12 points, respectively. Our interest is in the scores of 55 students in a particular school who received a mean score of 96. We can ask whether this mean score is significantly lower than the regional mean—that is, are the students in this school comparable to a simple random sample of 55 students from the region as a whole, or are their scores surprisingly low?\n\nFirst calculate the [[standard error (statistics)|standard error]] of the mean:\n\n:<math>\\mathrm{SE} = \\frac{\\sigma}{\\sqrt n} = \\frac{12}{\\sqrt{55}} = \\frac{12}{7.42} = 1.62 \\,\\!</math>\n\nwhere <math>{\\sigma}</math> is the population standard deviation.\n\nNext calculate the [[standard score|''z''-score]], which is the distance from the sample mean to the population mean in units of the standard error:\n\n:<math>z = \\frac{M - \\mu}{\\mathrm{SE}} = \\frac{96 - 100}{1.62} = -2.47 \\,\\!</math>\n\nIn this example, we treat the population mean and variance as known, which would be appropriate if all students in the region were tested. When population parameters are unknown, a t test should be conducted instead.\n\nThe classroom mean score is 96, which is −2.47 standard error units from the population mean of 100. Looking up the ''z''-score in a table of the standard [[normal distribution]], we find that the probability of observing a standard normal value below −2.47 is approximately 0.5 − 0.4932 = 0.0068. This is the [[one-tailed|one-sided]] [[p-value|''p''-value]] for the null hypothesis that the 55 students are comparable to a simple random sample from the population of all test-takers. The two-sided ''p''-value is approximately 0.014 (twice the one-sided ''p''-value).\n\nAnother way of stating things is that with probability 1&nbsp;−&nbsp;0.014&nbsp;=&nbsp;0.986, a simple random sample of 55 students would have a mean test score within 4 units of the population mean. We could also say that with 98.6% confidence we reject the [[null hypothesis]] that the 55 test takers are comparable to a simple random sample from the population of test-takers.\n\nThe ''Z''-test tells us that the 55 students of interest have an unusually low mean test score compared to most simple random samples of similar size from the population of test-takers. A deficiency of this analysis is that it does not consider whether the [[effect size]] of 4 points is meaningful. If instead of a classroom, we considered a subregion containing 900 students whose mean score was 99, nearly the same ''z''-score and ''p''-value would be observed. This shows that if the sample size is large enough, very small differences from the null value can be highly statistically significant. See [[statistical hypothesis testing]] for further discussion of this issue.\n\n== ''Z''-tests other than location tests ==\n\nLocation tests are the most familiar ''Z''-tests. Another class of ''Z''-tests arises in [[maximum likelihood]] estimation of the [[parameter]]s in a [[parametric statistics|parametric]] [[statistical model]]. Maximum likelihood estimates are approximately normal under certain conditions, and their asymptotic variance can be calculated in terms of the Fisher information. The maximum likelihood estimate divided by its standard error can be used as a test statistic for the null hypothesis that the population value of the parameter equals zero. More generally, if <math>\\hat{\\theta}</math> is the maximum likelihood estimate of a parameter θ, and θ<sub>0</sub> is the value of θ under the null hypothesis,\n\n:<math>\n(\\hat{\\theta}-\\theta_0)/{\\rm SE}(\\hat{\\theta})\n</math>\n\ncan be used as a ''Z''-test statistic.\n\nWhen using a ''Z''-test for maximum likelihood estimates, it is important to be aware that the normal approximation may be poor if the sample size is not sufficiently large. Although there is no simple, universal rule stating how large the sample size must be to use a ''Z''-test, [[Monte Carlo method|simulation]] can give a good idea as to whether a ''Z''-test is appropriate in a given situation.\n\n''Z''-tests are employed whenever it can be argued that a test statistic follows a normal distribution under the null hypothesis of interest. Many [[non-parametric statistics|non-parametric]] test statistics, such as [[U statistic]]s, are approximately normal for large enough sample sizes, and hence are often performed as ''Z''-tests.\n\n== See also ==\n* [[Normal distribution]]\n* [[Standard normal table]]\n* [[Standard score]]\n* [[Student's t-test|Student's ''t''-test]]\n\n==References==\n* {{cite book |last=Sprinthall |first=R. C. |year=2011 |title=Basic Statistical Analysis |edition=9th |publisher=Pearson Education |location= |isbn=978-0-205-05217-2 }}\n\n{{Statistics}}\n{{Public health}}\n\n[[Category:Statistical tests]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Zero degrees of freedom",
      "url": "https://en.wikipedia.org/wiki/Zero_degrees_of_freedom",
      "text": "In [[statistics]], the '''non-central chi-squared distribution with zero degrees of freedom''' can be used in [[statistical hypothesis testing|testing]] the [[null hypothesis]] that a sample is from a [[uniform distribution (continuous)|uniform distribution]] on the interval&nbsp;(0,&nbsp;1). This distribution was introduced by Andrew&nbsp;F.&nbsp;Siegel in 1979.<ref>Siegel, A. F. (1979), \"The noncentral chi-squared distribution with zero degrees of freedom and testing for uniformity\", ''[[Biometrika]]'', 66, 381–386</ref>\n\nThe [[chi-squared distribution]] with ''n'' degrees of freedom is the [[probability distribution]] of the sum\n\n: <math> X_1^2+\\cdots+X_n^2 \\, </math>\n\nwhere\n\n: <math> X_1,\\ldots,X_n \\sim \\operatorname{i.i.d. N}(0,1). \\, </math>\n\nHowever, if\n\n: <math> X_k \\sim \\operatorname{N} (\\mu_k,1) </math>\n\nand <math>X_1,\\ldots,X_n</math> are independent, then the sum of squares above has a [[non-central chi-squared distribution]] with ''n'' degrees of freedom and \"noncentrality parameter\"\n\n: <math> \\mu_1^2 + \\cdots + \\mu_n^2. \\, </math>\n\nIt is trivial that a \"central\" chi-square distribution with zero degrees of freedom concentrates all probability at zero.\n\nAll of this leaves open the question of what happens with zero degrees of freedom when the noncentrality parameter is not zero.\n\nThe noncentral chi-squared distribution with zero degrees of freedom and with noncentrality parameter&nbsp;''&mu;'' is the distribution of\n\n: <math>\n\\begin{align}\n& \\sum_{k\\,=\\,1}^{2K} X_k^2 \\\\\n\\text{where } & K\\sim\\operatorname{Poisson}(\\mu/2) \\\\\n\\text{and } & X_1,X_2,X_3,\\ldots\\sim\\operatorname{i.i.d. N}(0,1).\n\\end{align}\n</math>\n\nThis concentrates probability&nbsp;''e''<sup>−''&mu;''/2</sup> at zero; thus it is a mixture of discrete and continuous distributions\n\n== References ==\n\n{{reflist}}\n\n\n[[Category:Continuous distributions]]\n[[Category:Normal distribution]]\n[[Category:Exponential family distributions]]\n[[Category:Probability distributions]]\n[[Category:Statistical hypothesis testing]]"
    },
    {
      "title": "Central limit theorem",
      "url": "https://en.wikipedia.org/wiki/Central_limit_theorem",
      "text": "In [[probability theory]], the '''central limit theorem''' ('''CLT''') establishes that, in some situations, when [[Statistical independence|independent random variables]] are added, their properly normalized sum tends toward a [[normal distribution]] (informally a \"''bell curve''\") even if the original variables themselves are not normally distributed.  The theorem is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions.\n\nFor example, suppose that a [[Sample (statistics)|sample]] is obtained containing a large number of [[Random variate|observations]], each observation being randomly generated in a way that does not depend on the values of the other observations, and that the [[arithmetic mean]] of the observed values is computed. If this procedure is performed many times, the central limit theorem says that the [[Probability distribution|distribution]] of the average will be closely approximated by a [[normal distribution]].  A simple example of this is that if one [[Coin flipping|flips a coin many times]] the probability of getting a given number of heads in a series of flips will approach a normal curve, with mean equal to half the total number of flips in each series. (In the limit of an infinite number of flips, it will equal a normal curve.)\n\nThe central limit theorem has a number of variants. In its common form, the random variables must be [[identically distributed]]. In variants, convergence of the mean to the normal distribution also occurs for non-identical distributions or for non-independent observations, given that they comply with certain conditions.\n\nThe earliest version of this theorem, that the [[normal distribution]] may be used as an approximation to the [[binomial distribution]], is now known as the [[de Moivre–Laplace theorem]].\n\nIn more general usage, a central limit theorem is any of a set of [[Weak convergence of measures|weak-convergence]] theorems in probability theory. They all express the fact that a sum of many [[Independent and identically distributed random variables|independent and identically distributed]] (i.i.d.) random variables, or alternatively, random variables with specific types of dependence, will tend to be distributed according to one of a small set of ''[[attractor]] distributions''. When the variance of the i.i.d. variables is finite, the attractor distribution is the normal distribution. In contrast, the sum of a number of [[Independent and identically distributed random variables|i.i.d. random variables]] with [[power law]] tail distributions decreasing as {{math|{{abs|''x''}}<sup>−''α'' − 1</sup>}} where {{math|0 < ''α'' < 2}} (and therefore having infinite variance) will tend to an alpha-[[stable distribution]] with stability parameter (or index of stability) of {{mvar|α}} as the number of variables grows.<ref>{{Cite book |first1=Johannes |last1=Voit |page=124 |year=2003|title=The Statistical Mechanics of Financial Markets|publisher=Springer-Verlag|isbn=3-540-00978-7}}</ref>\n\n==Independent sequences==\n[[File:Central limit thm.png|400px|thumb|right|A distribution being \"smoothed out\" by [[summation]], showing original [[Probability density function|density of distribution]] and three subsequent summations; see [[Illustration of the central limit theorem]] for further details.]]\n[[File:IllustrationCentralTheorem.png|400px|thumb|right|Whatever the form of the population distribution, the sampling distribution tends to a Gaussian, and its dispersion is given by the Central Limit Theorem.<ref>{{cite book |last=Rouaud |first=Mathieu |title=Probability, Statistics and Estimation|year=2013 |page=10 |url=http://www.incertitudes.fr/book.pdf }}</ref>]]\n\n===Classical CLT===\nLet {{math|{''X''<sub>1</sub>, …, ''X<sub>n</sub>''}}} be a [[random sample]] of size {{mvar|n}}—that is, a sequence of [[independent and identically distributed]] (i.i.d.) random variables drawn from a distribution of [[expected value]] given by {{mvar|µ}} and finite [[variance]] given by {{math|''σ''<sup>2</sup>}}. Suppose we are interested in the [[sample mean|sample average]]\n\n:<math>S_n := \\frac{X_1+\\cdots+X_n}{n}</math>\n\nof these random variables. By the [[law of large numbers]], the sample averages [[Convergence in probability|converge in probability]] and [[Almost sure convergence|almost surely]] to the expected value {{mvar|µ}} as {{math|''n'' → ∞}}. The classical central limit theorem describes the size and the distributional form of the stochastic fluctuations around the deterministic number {{mvar|µ}} during this convergence. More precisely, it states that as {{mvar|n}} gets larger, the distribution of the difference between the sample average {{mvar|S<sub>n</sub>}} and its limit {{mvar|µ}}, when multiplied by the factor {{math|{{sqrt|''n''}}}} (that is {{math|{{sqrt|''n''}}(''S<sub>n</sub>'' − ''µ'')}}), approximates the [[normal distribution]] with mean 0 and variance {{math|''σ''<sup>2</sup>}}. For large enough {{mvar|n}}, the distribution of {{mvar|S<sub>n</sub>}} is close to the normal distribution with mean {{mvar|µ}} and variance {{math|''σ''<sup>2</sup>}}/{{math|''n''}}. The usefulness of the theorem is that the distribution of {{math|{{sqrt|''n''}}(''S<sub>n</sub>'' − ''µ'')}} approaches normality regardless of the shape of the distribution of the individual {{mvar|X<sub>i</sub>}}. Formally, the theorem can be stated as follows:\n\n<blockquote>'''Lindeberg–Lévy CLT.''' Suppose {{math|{''X''<sub>1</sub>, ''X''<sub>2</sub>, …}}} is a sequence of [[independent and identically distributed|i.i.d.]] random variables with {{math|E[''X<sub>i</sub>''] {{=}} ''µ''}} and {{math|Var[''X<sub>i</sub>''] {{=}} ''σ''<sup>2</sup> < ∞}}. Then as {{mvar|n}} approaches infinity, the random variables {{math|{{sqrt|''n''}}(''S<sub>n</sub>'' − ''µ'')}} [[convergence in distribution|converge in distribution]] to a [[normal distribution|normal]] {{math|''N''(0,''σ''<sup>2</sup>)}}:<ref>Billingsley (1995, p.&nbsp;357)</ref>\n\n:<math>\\sqrt{n}\\left(S_n - \\mu\\right)\\ \\xrightarrow{d}\\ N\\left(0,\\sigma^2\\right).</math></blockquote>\n\nIn the case {{math|''σ'' > 0}}, convergence in distribution means that the [[cumulative distribution function]]s of {{math|{{sqrt|''n''}}(''S<sub>n</sub>'' − ''µ'')}} converge pointwise to the cdf of the {{math|''N''(0, ''σ''<sup>2</sup>)}} distribution: for every real number&nbsp;{{mvar|z}},\n\n:<math>\\lim_{n\\to\\infty} \\Pr\\left[\\sqrt{n}(S_n-\\mu) \\le z\\right] = \\lim_{n\\to\\infty} \\Pr\\left[\\frac{\\sqrt{n}(S_n-\\mu)}{\\sigma } \\le \\frac{z}{\\sigma}\\right]= \\Phi\\left(\\frac{z}{\\sigma}\\right) ,</math>\n\nwhere {{math|Φ(''z'')}} is the standard normal cdf evaluated at&nbsp;{{mvar|z}}. Note that the convergence is uniform in {{mvar|z}} in the sense that\n\n:<math>\\lim_{n\\to\\infty}\\sup_{z\\in\\R}\\left|\\Pr\\left[\\sqrt{n}(S_n-\\mu) \\le z\\right] - \\Phi\\left(\\frac{z}{\\sigma}\\right)\\right| = 0,</math>\n\nwhere sup denotes the least upper bound (or [[supremum]]) of the set.<ref>Bauer (2001, Theorem 30.13, p.199)</ref>\n\n===Lyapunov CLT===\nThe theorem is named after Russian mathematician [[Aleksandr Lyapunov]]. In this variant of the central limit theorem the random variables {{mvar|X<sub>i</sub>}} have to be independent, but not necessarily identically distributed. The theorem also requires that random variables {{math|{{abs|''X<sub>i</sub>''}}}} have [[moment (mathematics)|moment]]s of some order {{math|(2 + ''δ'')}}, and that the rate of growth of these moments is limited by the Lyapunov condition given below.\n\n<blockquote>'''Lyapunov CLT.'''<ref>Billingsley (1995, p.362)</ref> Suppose {{math|{''X''<sub>1</sub>, ''X''<sub>2</sub>, …}}} is a sequence of independent random variables, each with finite expected value {{mvar|μ<sub>i</sub>}} and variance {{math|''σ''{{su|b=''i''|p=2}}}}. Define\n\n:<math>s_n^2 = \\sum_{i=1}^n \\sigma_i^2</math>\n\nIf for some {{math|''δ'' > 0}}, ''Lyapunov’s condition''\n\n: <math>\\lim_{n\\to\\infty} \\frac{1}{s_{n}^{2+\\delta}} \\sum_{i=1}^{n} \\operatorname{E}\\left[|X_{i} - \\mu_{i}|^{2+\\delta}\\right] = 0</math>\n\nis satisfied, then a sum of {{math|{{sfrac|''X<sub>i</sub>'' − ''μ<sub>i</sub>''|''s<sub>n</sub>''}}}} converges in distribution to a standard normal random variable, as {{mvar|n}} goes to infinity:\n\n: <math>\\frac{1}{s_n} \\sum_{i=1}^{n} \\left(X_i - \\mu_i\\right) \\ \\xrightarrow{d}\\ N(0,1).</math></blockquote>\n\nIn practice it is usually easiest to check Lyapunov's condition for {{math|''δ'' {{=}} 1}}.\n\nIf a sequence of random variables satisfies Lyapunov's condition, then it also satisfies Lindeberg's condition. The converse implication, however, does not hold.\n\n===Lindeberg CLT===\n{{Main|Lindeberg's condition}}\n\nIn the same setting and with the same notation as above, the Lyapunov condition can be replaced with the following weaker one (from [[Jarl Waldemar Lindeberg|Lindeberg]] in 1920).\n\nSuppose that for every {{math|''ε'' > 0}}\n\n:<math>  \\lim_{n \\to \\infty} \\frac{1}{s_n^2}\\sum_{i = 1}^{n} \\operatorname{E}\\left[(X_i - \\mu_i)^2 \\cdot \\mathbf{1}_{\\{ | X_i - \\mu_i | > \\varepsilon s_n \\}}  \\right] = 0</math>\n\nwhere {{math|'''1'''<sub>{…}</sub>}} is the [[indicator function]]. Then the distribution of the standardized sums\n\n:<math>\\frac{1}{s_n}\\sum_{i = 1}^n \\left( X_i - \\mu_i \\right)</math>\n\nconverges towards the standard normal distribution {{math|''N''(0,1)}}.\n\n===Multidimensional CLT===\nProofs that use characteristic functions can be extended to cases where each individual {{math|X<sub>i</sub>}} is a [[random vector]] in {{math|'''ℝ'''<sup>''k''</sup>}}, with mean vector {{math|'''μ''' {{=}} E(''X<sub>i</sub>'')}} and [[covariance matrix]] {{math|'''Σ'''}} (among the components of the vector), and these random vectors are independent and identically distributed. Summation of these vectors is being done componentwise. The multidimensional central limit theorem  states that when scaled, sums converge to a [[multivariate normal distribution]].<ref>{{Cite book|last = Van der Vaart|first = A. W.|title = Asymptotic statistics|year = 1998| publisher = Cambridge University Press  | location = New York  | isbn = 978-0-521-49603-2|lccn = 98015176| ref = CITEREFvan_der_Vaart1998}}</ref>\n\nLet\n\n:<math>\\mathbf{X}_i=\\begin{bmatrix} X_{i(1)} \\\\ \\vdots \\\\ X_{i(k)} \\end{bmatrix}</math>\n\nbe the {{mvar|k}}-vector. The bold in {{math|'''X'''<sub>''i''</sub>}} means that it is a random vector, not a random (univariate) variable. Then the [[summation|sum]] of the random vectors will be\n\n:<math>\\begin{bmatrix} X_{1(1)} \\\\ \\vdots \\\\ X_{1(k)} \\end{bmatrix}+\\begin{bmatrix} X_{2(1)} \\\\ \\vdots \\\\ X_{2(k)} \\end{bmatrix}+\\cdots+\\begin{bmatrix} X_{n(1)} \\\\ \\vdots \\\\ X_{n(k)} \\end{bmatrix} = \\begin{bmatrix} \\sum_{i=1}^{n} \\left [ X_{i(1)} \\right ] \\\\ \\vdots \\\\ \\sum_{i=1}^{n} \\left [ X_{i(k)} \\right ] \\end{bmatrix} = \\sum_{i=1}^{n} \\mathbf{X}_i</math>\n\nand the average is\n\n:<math> \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{X}_i= \\frac{1}{n}\\begin{bmatrix} \\sum_{i=1}^{n} X_{i(1)} \\\\ \\vdots \\\\ \\sum_{i=1}^{n} X_{i(k)} \\end{bmatrix} = \\begin{bmatrix} \\bar X_{i(1)} \\\\ \\vdots \\\\ \\bar X_{i(k)} \\end{bmatrix}=\\mathbf{\\bar X_n}</math>\n\nand therefore\n\n:<math>\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\left [\\mathbf{X}_i - \\operatorname{E}\\left ( X_i\\right ) \\right ]=\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n} ( \\mathbf{X}_i - \\boldsymbol\\mu ) = \\sqrt{n}\\left(\\overline{\\mathbf{X}}_n - \\boldsymbol\\mu\\right). </math>\n\nThe multivariate central limit theorem states that\n\n:<math>\\sqrt{n}\\left(\\overline{\\mathbf{X}}_n - \\boldsymbol\\mu\\right)\\ \\stackrel{D}{\\rightarrow}\\ N_k(0,\\boldsymbol\\Sigma)</math>\n\nwhere the [[covariance matrix]] {{math|'''Σ'''}} is equal to\n\n:<math> \\boldsymbol\\Sigma=\\begin{bmatrix}\n{\\operatorname{Var} \\left (X_{1(1)} \\right)} & \\operatorname{Cov} \\left (X_{1(1)},X_{1(2)} \\right) & \\operatorname{Cov} \\left (X_{1(1)},X_{1(3)} \\right) & \\cdots & \\operatorname{Cov} \\left (X_{1(1)},X_{1(k)} \\right) \\\\\n\\operatorname{Cov} \\left (X_{1(2)},X_{1(1)} \\right) & \\operatorname{Var} \\left (X_{1(2)} \\right) & \\operatorname{Cov} \\left(X_{1(2)},X_{1(3)} \\right) & \\cdots & \\operatorname{Cov} \\left(X_{1(2)},X_{1(k)} \\right) \\\\\n\\operatorname{Cov}\\left (X_{1(3)},X_{1(1)} \\right) & \\operatorname{Cov} \\left (X_{1(3)},X_{1(2)} \\right) & \\operatorname{Var} \\left (X_{1(3)} \\right) & \\cdots & \\operatorname{Cov} \\left (X_{1(3)},X_{1(k)} \\right) \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\operatorname{Cov} \\left (X_{1(k)},X_{1(1)} \\right) & \\operatorname{Cov} \\left (X_{1(k)},X_{1(2)} \\right) & \\operatorname{Cov} \\left (X_{1(k)},X_{1(3)} \\right) & \\cdots & \\operatorname{Var} \\left (X_{1(k)} \\right) \\\\\n\\end{bmatrix}.</math>\n\nThe rate of convergence is given by the following [[Berry–Esseen theorem|Berry–Esseen]] type result:\n\n<blockquote>'''Theorem.'''<ref>Ryan O’Donnell (2014, Theorem 5.38) http://www.contrib.andrew.cmu.edu/~ryanod/?p=866</ref> Let <math>X_1,\\dots, X_n</math> be independent <math>R^d</math>-valued random vectors, each having mean zero. Write <math>S =\\sum^n_{i=1}X_i</math> and assume <math>\\Sigma = \\operatorname{Cov}[S]</math> is invertible. Let <math>Z \\sim N(0,\\Sigma)</math> be a <math>d</math>-dimensional Gaussian with the same mean and covariance matrix as <math>S</math>. Then for all convex sets <math>U \\subseteq R^d</math>,\n\n:<math>|\\Pr[S \\in U]-\\Pr[Z \\in U]| \\le C d^{1/4} \\gamma,</math>\n\nwhere <math>C</math> is a universal constant, <math>\\gamma = \\sum^n_{i=1} \\operatorname{E} [\\| \\Sigma^{-1/2}X_i\\|^3_2]</math>, and <math>\\|\\cdot\\|_2</math> denotes the Euclidean norm on <math>R^d</math>.\n</blockquote>\n\nIt is unknown whether the factor <math>d^{1/4}</math> is necessary.<ref>{{cite journal |first=V. |last=Bentkus |title=A Lyapunov-type Bound in <math>\\mathbb{R}^d</math> |journal=Theory Probab. Appl. |volume=49 |year=2005 |issue=2 |pages=311–323 |doi=10.1137/S0040585X97981123 }}</ref>\n\n===Generalized theorem===\n{{Main|Stable distribution#A generalized central limit theorem}}\n\nThe central limit theorem states that the sum of a number of independent and identically distributed random variables with finite variances will tend to a [[normal distribution]] as the number of variables grows. A generalization due to [[Boris Vladimirovich Gnedenko|Gnedenko]] and [[Andrey Nikolaevich Kolmogorov|Kolmogorov]] states that the sum of a number of random variables with a power-law tail ([[Pareto distribution|Paretian tail]]) distributions decreasing as {{math|{{abs|''x''}}<sup>−''α'' − 1</sup>}} where {{math|0 < ''α'' < 2}} (and therefore having infinite variance) will tend to a stable distribution {{math|''f''(''x'';''α'',0,''c'',0)}} as the number of summands grows.<ref name=Voit2003a>{{cite book|first=Johannes |last=Voit|year=2003 |title=The Statistical Mechanics of Financial Markets |series=Texts and Monographs in Physics |publisher=Springer-Verlag |isbn=3-540-00978-7 |chapter=Section 5.4.3 |chapterurl=https://books.google.com/books?id=6zUlh_TkWSwC }}</ref><ref>{{cite book |first=B. V. |last=Gnedenko |first2=A. N. |last2=Kolmogorov |title=Limit distributions for sums of independent random variables |location=Cambridge |publisher=Addison-Wesley |year=1954 |url=https://books.google.com/books/about/Limit_distributions_for_sums_of_independ.html?id=rYsZAQAAIAAuJ }}</ref> If {{math|''α'' > 2}} then the sum converges to a [[stable distribution]] with stability parameter equal to 2, i.e. a Gaussian distribution.<ref name=Uchaikin>{{cite book |first=Vladimir V. |last=Uchaikin |first2=V. M. |last2=Zolotarev |year=1999 |title=Chance and stability: stable distributions and their applications |location= |publisher=VSP |isbn=90-6764-301-7 |pages=61–62 }}</ref>\n\n==Dependent processes==\n\n===CLT under weak dependence===\nA useful generalization of a sequence of independent, identically distributed random variables is a [[Mixing (mathematics)|mixing]] random process in discrete time; \"mixing\" means, roughly, that random variables temporally far apart from one another are nearly independent. Several kinds of mixing are used in ergodic theory and probability theory. See especially [[Mixing (mathematics)#Mixing in stochastic processes|strong mixing]] (also called α-mixing) defined by {{math|''α''(''n'') → 0}} where {{math|''α''(''n'')}} is so-called [[Mixing (mathematics)#Mixing in stochastic processes|strong mixing coefficient]].\n\nA simplified formulation of the central limit theorem under strong mixing is:<ref>Billingsley (1995, Theorem 27.5)</ref>\n\n<blockquote>'''Theorem.''' Suppose that {{math|''X''<sub>1</sub>, ''X''<sub>2</sub>, …}} is stationary and {{mvar|α}}-mixing with {{math|''α''<sub>''n''</sub> {{=}} ''O''(''n''<sup>−5</sup>)}} and that {{math|E(''X<sub>n</sub>'') {{=}} 0}} and {{math|E(''X''{{su|b=''n''|p=12}}) < ∞}}. Denote {{math|''S<sub>n</sub>'' {{=}} ''X''<sub>1</sub> + … + ''X<sub>n</sub>''}}, then the limit\n\n:<math> \\sigma^2 = \\lim_n \\frac{\\operatorname{E}\\left(S_n^2\\right)}{n} </math>\n\nexists, and if {{math|''σ'' ≠ 0}} then {{math|{{sfrac|''S<sub>n</sub>''|''σ''{{sqrt|''n''}}}}}} converges in distribution to {{math|''N''(0,1)}}.</blockquote>\n\nIn fact,\n\n:<math>\\sigma^2  = \\operatorname{E}\\left(X_1^2\\right) + 2 \\sum_{k=1}^{\\infty} \\operatorname{E}\\left(X_1 X_{1+k}\\right),</math>\n\nwhere the series converges absolutely.\n\nThe assumption {{math|''σ'' ≠ 0}} cannot be omitted, since the asymptotic normality fails for {{math|''X<sub>n</sub>'' {{=}} ''Y<sub>n</sub>'' − ''Y''<sub>''n'' − 1</sub>}} where {{math|Y<sub>n</sub>}} are another [[stationary sequence]].\n\nThere is a stronger version of the theorem:<ref>Durrett (2004, Sect. 7.7(c), Theorem 7.8)</ref> the assumption {{math|E(''X''{{su|b=''n''|p=12}}) < ∞}} is replaced with {{math|E({{abs|''X<sub>n</sub>''}}<sup>2 + ''δ''</sup>) < ∞}}, and the assumption {{math|''α<sub>n</sub>'' {{=}} ''O''(''n''<sup>−5</sup>)}} is replaced with\n\n:<math>\\sum_n \\alpha_n^{\\frac\\delta{2(2+\\delta)}} < \\infty.</math>\n\nExistence of such {{math|''δ'' > 0}} ensures the conclusion. For encyclopedic treatment of limit theorems under mixing conditions see {{harv|Bradley|2007}}.\n\n===Martingale difference CLT===\n{{Main|Martingale central limit theorem}}\n<blockquote>'''Theorem'''. Let a [[Martingale (probability theory)|martingale]] {{mvar|M<sub>n</sub>}} satisfy\n* <math> \\frac1n \\sum_{k=1}^n \\operatorname{E} \\left(\\left(M_k-M_{k-1}\\right)^2 | M_1,\\dots,M_{k-1}\\right) \\to 1 </math>  in probability as {{math|''n'' → ∞}},\n* for every {{math|''ε'' > 0}}, <math> \\frac1n \\sum_{k=1}^n \\operatorname{E} \\left( \\left(M_k-M_{k-1}\\right)^2; |M_k-M_{k-1}| > \\varepsilon \\sqrt n \\right) \\to 0 </math>  as {{math|''n'' → ∞}},\nthen {{math|{{sfrac|''M<sub>n</sub>''|{{sqrt|''n''}}}}}} converges in distribution to {{math|''N''(0,1)}} as {{math|''n'' → ∞}}.<ref>Durrett (2004, Sect. 7.7, Theorem 7.4)</ref><ref>Billingsley (1995, Theorem 35.12)</ref></blockquote>\n\n''Caution:'' The [[restricted expectation]]{{clarify|reason=give source for this style of notation, else write using indicator functions|date=April 2012}} {{math|E(''X'' ; ''A'')}} should not be confused with the conditional expectation {{math|E(''X'' {{!}} ''A'') {{=}} {{sfrac|E(''X'' ; ''A'')|'''P'''(''A'')}}}}.\n\n==Remarks==\n\n===Proof of classical CLT===\nThe central limit theorem has a simple proof using [[characteristic function (probability theory)|characteristic functions]].<ref>{{cite web|url=https://jhupbooks.press.jhu.edu/content/introduction-stochastic-processes-physics|title=An Introduction to Stochastic Processes in Physics|website=jhupbooks.press.jhu.edu|access-date=2016-08-11}}</ref> It is similar to the proof of the (weak) [[Proof of the law of large numbers|law of large numbers]].\n\nAssume {{math|{''X''<sub>1</sub>, …, ''X<sub>n</sub>''}}} are independent and identically distributed random variables, each with mean {{mvar|µ}} and finite variance {{math|''σ''<sup>2</sup>}}. The sum {{math|''X''<sub>1</sub> + … + ''X<sub>n</sub>''}} has [[Linearity of expectation|mean]] {{mvar|nµ}} and [[Variance#Sum of uncorrelated variables (Bienaymé formula)|variance]] {{math|''nσ''<sup>2</sup>}}. Consider the random variable\n: <math>Z_n \\ =\\ \\frac{X_1+\\cdots+X_n - n \\mu}{\\sqrt{n \\sigma^2}} \\ =\\ \\sum_{i=1}^n \\frac{X_i - \\mu}{\\sqrt{n \\sigma^2}} \\ =\\ \\sum_{i=1}^n \\frac{1}{\\sqrt{n}} Y_i,</math>\nwhere in the last step we defined the new random variables {{math|''Y<sub>i</sub>'' {{=}} {{sfrac|''X<sub>i</sub>'' − ''μ''|''σ''}}}}, each with zero mean and unit variance ({{math|var(''Y'') {{=}} 1}}). The [[Characteristic function (probability theory)|characteristic function]] of {{mvar|Z<sub>n</sub>}} is given by\n\n:<math>\\varphi_{Z_n}\\!(t) \\ =\\ \\varphi_{\\sum_{i=1}^n {\\frac{1}{\\sqrt{n}}Y_i}}\\!(t) \\ =\\ \\varphi_{Y_1}\\!\\!\\left(\\frac{t}{\\sqrt{n}}\\right) \\varphi_{Y_2}\\!\\! \\left(\\frac{t}{\\sqrt{n}}\\right)\\cdots \\varphi_{Y_n}\\!\\! \\left(\\frac{t}{\\sqrt{n}}\\right) \\ =\\ \\left[\\varphi_{Y_1}\\!\\!\\left(\\frac{t}{\\sqrt{n}}\\right)\\right]^n,\n</math>\n\nwhere in the last step we used the fact that all of the {{mvar|Y<sub>i</sub>}} are identically distributed. The characteristic function of {{math|''Y''<sub>1</sub>}} is, by [[Taylor's theorem]],\n\n:<math>\\varphi_{Y_1}\\!\\!\\left(\\frac{t}{\\sqrt{n}}\\right) \\ =\\ 1 - \\frac{t^2}{2n} + o\\!\\!\\left(\\frac{t^2}{n}\\right), \\quad \\bigg(\\frac{t}{\\sqrt{n}}\\bigg) \\rightarrow 0</math>\n\nwhere {{math|''o''(''t''<sup>2</sup>)}} is \"[[Little-o notation|little {{mvar|o}} notation]]\" for some function of {{mvar|t}} that goes to zero more rapidly than {{math|''t''<sup>2</sup>}}. By the limit of the [[exponential function]] ({{math|''e''<sup>''x''</sup>{{=}} lim(1 + {{sfrac|''x''|''n''}})<sup>''n''</sup>}}), the characteristic function of {{mvar|Z<sub>n</sub>}} equals\n\n:<math>\\varphi_{Z_n}(t) = \\left(1 - \\frac{t^2}{2n} + o\\left(\\frac{t^2}{n}\\right) \\right)^n \\rightarrow e^{-\\frac12 t^2}, \\quad n \\rightarrow \\infty.</math>\n\nNote that all of the higher order terms vanish in the limit {{math|''n'' → ∞}}. The right hand side equals the characteristic function of a standard normal distribution {{math|''N''(0,1)}}, which implies through [[Lévy continuity theorem|Lévy's continuity theorem]] that the distribution of {{mvar|Z<sub>n</sub>}} will approach {{math|''N''(0,1)}} as {{math|''n'' → ∞}}. Therefore, the sum {{math|''X''<sub>1</sub> + … + ''X<sub>n</sub>''}} will approach that of the normal distribution {{math|''N''(''nµ'',''nσ''<sup>2</sup>)}}, and the [[sample mean|sample average]]\n\n:<math>S_n = \\frac{X_1+\\cdots+X_n}{n}</math>\n\nconverges to the normal distribution {{math|''N''(''µ'',{{sfrac|''σ''<sup>2</sup>|''n''}})}}, from which the central limit theorem follows.\n\n===Convergence to the limit===\nThe central limit theorem gives only an [[asymptotic distribution]]. As an approximation for a finite number of observations, it provides a reasonable approximation only when close to the peak of the normal distribution; it requires a very large number of observations to stretch into the tails.{{citation needed|reason=Not immediately obvious, I didn't find a source via google|date=July 2016}}\n\nThe convergence in the central limit theorem is [[uniform convergence|uniform]] because the limiting cumulative distribution function is continuous. If the third central [[Moment (mathematics)|moment]] {{math|E((''X''<sub>1</sub> − ''μ'')<sup>3</sup>)}} exists and is finite, then the speed of convergence is at least on the order of {{math|{{sfrac|1|{{sqrt|''n''}}}}}} (see [[Berry–Esseen theorem]]). [[Stein's method]]<ref name=\"stein1972\">{{Cite journal| last = Stein |first=C. |authorlink=Charles Stein (statistician)| title = A bound for the error in the normal approximation to the distribution of a sum of dependent random variables| journal = Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability| pages= 583–602| year = 1972| mr=402873 | zbl = 0278.60026| url=http://projecteuclid.org/euclid.bsmsp/1200514239\n}}</ref> can be used not only to prove the central limit theorem, but also to provide bounds on the rates of convergence for selected metrics.<ref>{{Cite book|  title = Normal approximation by Stein's method|  publisher = Springer|  year = 2011|last1=Chen |first1=L. H. Y. |last2=Goldstein |first2=L. |last3=Shao |first3=Q. M. |isbn = 978-3-642-15006-7}}</ref>\n\nThe convergence to the normal distribution is monotonic, in the sense that the [[information entropy|entropy]] of {{mvar|Z<sub>n</sub>}} increases [[monotonic function|monotonically]] to that of the normal distribution.<ref name=ABBN/>\n\nThe central limit theorem applies in particular to sums of independent and identically distributed [[discrete random variable]]s.  A sum of [[discrete random variable]]s is still a [[discrete random variable]], so that we are confronted with a sequence of [[discrete random variable]]s whose cumulative probability distribution function converges towards a cumulative probability distribution function corresponding to a continuous variable (namely that of the [[normal distribution]]).  This means that if we build a [[histogram]] of the realisations of the sum of {{mvar|n}} independent identical discrete variables, the curve that joins the centers of the upper faces of the rectangles forming the histogram converges toward a Gaussian curve as {{mvar|n}} approaches infinity, this relation is known as [[de Moivre–Laplace theorem]]. The [[binomial distribution]] article details such an application of the central limit theorem in the simple case of a discrete variable taking only two possible values.\n\n===Relation to the law of large numbers===\n[[Law of large numbers|The law of large numbers]] as well as the central limit theorem are partial solutions to a general problem: \"What is the limiting behaviour of {{math|S<sub>{{mvar|n}}</sub>}} as {{mvar|n}} approaches infinity?\" In mathematical analysis, [[asymptotic series]] are one of the most popular tools employed to approach such questions.\n\nSuppose we have an asymptotic expansion of {{math|''f''(''n'')}}:\n\n: <math>f(n)= a_1 \\varphi_{1}(n)+a_2 \\varphi_{2}(n)+O\\big(\\varphi_{3}(n)\\big) \\qquad  (n \\rightarrow \\infty).</math>\n\nDividing both parts by {{math|''φ''<sub>1</sub>(''n'')}} and taking the limit will produce {{math|''a''<sub>1</sub>}}, the coefficient of the highest-order term in the expansion, which represents the rate at which {{math|''f''(''n'')}} changes in its leading term.\n\n: <math>\\lim_{n\\to\\infty}\\frac{f(n)}{\\varphi_{1}(n)}=a_1.</math>\n\nInformally, one can say: \"{{math|''f''(''n'')}} grows approximately as {{math|''a''<sub>1</sub>''φ''<sub>1</sub>(''n'')}}\". Taking the difference between {{math|''f''(''n'')}} and its approximation and then dividing by the next term in the expansion, we arrive at a more refined statement about {{math|''f''(''n'')}}:\n\n: <math>\\lim_{n\\to\\infty}\\frac{f(n)-a_1 \\varphi_{1}(n)}{\\varphi_{2}(n)}=a_2 .</math>\n\nHere one can say that the difference between the function and its approximation grows approximately as {{math|''a''<sub>2</sub>''φ''<sub>2</sub>(''n'')}}.  The idea is that dividing the function by appropriate normalizing functions, and looking at the limiting behavior of the result, can tell us much about the limiting behavior of the original function itself.\n\nInformally, something along these lines happens when the sum, {{mvar|S<sub>n</sub>}}, of independent identically distributed random variables, {{math|''X''<sub>1</sub>, …, ''X<sub>n</sub>''}}, is studied in classical probability theory.{{Citation needed|date=April 2012}}  If each {{mvar|X<sub>i</sub>}} has finite mean {{mvar|μ}}, then by the law of large numbers, {{math|{{sfrac|''S<sub>n</sub>''|''n''}} → ''μ''}}.<ref>{{cite book|last=Rosenthal |first=Jeffrey Seth |date=2000 |title=A First Look at Rigorous Probability Theory |publisher=World Scientific |ISBN=981-02-4322-7 |at=Theorem 5.3.4, p. 47}}</ref>  If in addition each {{mvar|X<sub>i</sub>}} has finite variance {{math|''σ''<sup>2</sup>}}, then by the central limit theorem,\n\n: <math> \\frac{S_n-n\\mu}{\\sqrt{n}} \\rightarrow \\xi ,</math>\n\nwhere {{mvar|ξ}} is distributed as {{math|''N''(0,''σ''<sup>2</sup>)}}.  This provides values of the first two constants in the informal expansion\n\n: <math>S_n \\approx \\mu n+\\xi \\sqrt{n}. </math>\n\nIn the case where the {{mvar|X<sub>i</sub>}} do not have finite mean or variance, convergence of the shifted and rescaled sum can also occur with different centering and scaling factors:\n\n:<math>\\frac{S_n-a_n}{b_n} \\rightarrow \\Xi,</math>\n\nor informally\n\n: <math>S_n \\approx a_n+\\Xi b_n. </math>\n\nDistributions {{math|Ξ}} which can arise in this way are called ''[[stable distribution|stable]]''.<ref>{{cite book|last=Johnson |first=Oliver Thomas |date=2004 |title=Information Theory and the Central Limit Theorem |publisher=Imperial College Press |ISBN= 1-86094-473-6 |page= 88}}</ref>  Clearly, the normal distribution is stable, but there are also other stable distributions, such as the [[Cauchy distribution]], for which the mean or variance are not defined.  The scaling factor {{mvar|b<sub>n</sub>}} may be proportional to {{mvar|n<sup>c</sup>}}, for any {{math|''c'' ≥ {{sfrac|1|2}}}}; it may also be multiplied by a [[slowly varying function]] of {{mvar|n}}.<ref name=Uchaikin /><ref>{{cite book|last1=Borodin |first1=A. N. |last2=Ibragimov |first2=I. A. |last3=Sudakov |first3=V. N. |date=1995 |title=Limit Theorems for Functionals of Random Walks |publisher=AMS Bookstore |ISBN= 0-8218-0438-3 |at=Theorem 1.1, p. 8}}</ref>\n\nThe [[law of the iterated logarithm]] specifies what is happening \"in between\" the [[law of large numbers]] and the central limit theorem. Specifically it says that the normalizing function {{math|{{sqrt|''n'' log log ''n''}}}}, intermediate in size between {{mvar|n}} of the law of large numbers and {{math|{{sqrt|''n''}}}} of the central limit theorem, provides a non-trivial limiting behavior.\n\n===Alternative statements of the theorem===\n\n====Density functions====\nThe [[probability density function|density]] of the sum of two or more independent variables is the [[convolution]] of their densities (if these densities exist).  Thus the central limit theorem can be interpreted as a statement about the properties of density functions under convolution: the convolution of a number of density functions tends to the normal density as the number of density functions increases without bound. These theorems require stronger hypotheses than the forms of the central limit theorem given above. Theorems of this type are often called local limit theorems. See Petrov<ref>{{Cite book|last=Petrov|first=V. V.|title=Sums of Independent Random Variables|year=1976|publisher=Springer-Verlag|location=New York-Heidelberg|at=ch. 7}}</ref> for a particular local limit theorem for sums of [[independent and identically distributed random variables]].\n\n====Characteristic functions====\nSince the [[characteristic function (probability theory)|characteristic function]] of a convolution is the product of the characteristic functions of the densities involved, the central limit theorem has yet another restatement: the product of the characteristic functions of a number of density functions becomes close to the characteristic function of the normal density as the number of density functions increases without bound, under the conditions stated above. Specifically, an appropriate scaling factor needs to be applied to the argument of the characteristic function.\n\nAn equivalent statement can be made about [[Fourier transform]]s, since the characteristic function is essentially a Fourier transform.\n\n===Calculating the variance===\nLet {{mvar|S<sub>n</sub>}} be the sum of {{mvar|n}} random variables. Many central limit theorems provide conditions such that {{math|{{mvar|S<sub>n</sub>}}/{{sqrt|Var({{mvar|S<sub>n</sub>}})}}}} converges in distribution to {{math|''N''(0,1)}} (the normal distribution with mean 0, variance 1) as {{math|{{mvar|n}}→ ∞}}.  In some cases, it is possible to find a constant {{mvar|σ<sup>2</sup>}}  and function {{mvar|f(n)}} such that {{math|{{mvar|S<sub>n</sub>}}/(σ{{sqrt|{{mvar|n⋅f}}({{mvar|n}})}})}} converges in distribution to {{math|''N''(0,1)}} as {{math|{{mvar|n}}→ ∞}}.\n\n<blockquote>'''Lemma.'''<ref>{{cite journal|last1=Hew|first1=Patrick Chisan|title=Asymptotic distribution of rewards accumulated by alternating renewal processes|journal=Statistics and Probability Letters|date=2017|volume=129|pages=355–359|doi=10.1016/j.spl.2017.06.027}}</ref> Suppose <math>X_1, X_2, \\dots</math> is a sequence of real-valued and strictly stationary random variables with <math>\\mathbb{E}(X_i) = 0</math> for all <math>i</math>, <math>g : [0,1] \\rightarrow \\mathbb{R}</math>, and <math>S_n = \\sum_{i=1}^{n} g(\\tfrac{i}{n}) X_i</math>. Construct\n\n:<math>\\sigma^2 = \\mathbb{E}(X_1^2) + 2\\sum_{i=1}^{\\infty} \\mathbb{E}(X_1 X_{1+i})</math>\n\n# If <math>\\sum_{i=1}^{\\infty} \\mathbb{E}(X_1 X_{1+i})</math> is absolutely convergent, <math>\\left| \\int_0^1 g(x)g'(x) \\, dx\\right| < \\infty</math>, and <math>0 < \\int_0^1 (g(x))^2 dx < \\infty</math> then <math>\\mathrm{Var}(S_n)/(n \\gamma_n) \\rightarrow \\sigma^2</math> as <math>n \\rightarrow \\infty</math> where <math>\\gamma_n = \\frac{1}{n}\\sum_{i=1}^{n} (g(\\tfrac{i}{n}))^2</math>.\n# If in addition <math>\\sigma > 0</math> and <math>S_n/\\sqrt{\\mathrm{Var}(S_n)}</math> converges in distribution to <math>\\mathcal{N}(0,1)</math> as <math>n \\rightarrow \\infty</math> then <math>S_n/(\\sigma\\sqrt{n \\gamma_n})</math> also converges in distribution to <math>\\mathcal{N}(0,1)</math> as <math>n \\rightarrow \\infty</math>.\n</blockquote>\n\n==Extensions==\n\n===Products of positive random variables===\nThe [[logarithm]] of a product is simply the sum of the logarithms of the factors.  Therefore, when the logarithm of a product of random variables that take only positive values approaches a normal distribution, the product itself approaches a [[log-normal distribution]].  Many physical quantities (especially mass or length, which are a matter of scale and cannot be negative) are the products of different [[random]] factors, so they follow a log-normal distribution.  This multiplicative version of the central limit theorem is sometimes called [[Gibrat's law]].\n\nWhereas the central limit theorem for sums of random variables requires the condition of finite variance, the corresponding theorem for products requires the corresponding condition that the density function be square-integrable.<ref name=Rempala/>\n\n==Beyond the classical framework==\nAsymptotic normality, that is, [[Convergence in distribution|convergence]] to the normal distribution after appropriate shift and rescaling, is a phenomenon much more general than the classical framework treated above, namely, sums of independent random variables (or vectors). New frameworks are revealed from time to time; no single unifying framework is available for now.\n\n===Convex body===\n<blockquote>'''Theorem.'''  There exists a sequence {{math|''ε<sub>n</sub>'' ↓ 0}} for which the following holds. Let {{math|''n'' ≥ 1}}, and let random variables {{math|''X''<sub>1</sub>, …, ''X<sub>n</sub>''}} have a [[Logarithmically concave function|log-concave]] [[Joint density function|joint density]] {{mvar|f}} such that {{math|''f''(''x''<sub>1</sub>, …, ''x<sub>n</sub>'') {{=}} ''f''({{abs|''x''<sub>1</sub>}}, …, {{abs|''x<sub>n</sub>''}})}} for all {{math|''x''<sub>1</sub>, …, ''x<sub>n</sub>''}},  and {{math|E(''X''{{su|b=''k''|p=2}}) {{=}} 1}} for all {{math|''k'' {{=}} 1, …, ''n''}}. Then the distribution of\n\n:<math> \\frac{X_1+\\cdots+X_n}{\\sqrt n} </math>\n\nis {{mvar|ε<sub>n</sub>}}-close to {{math|''N''(0,1)}} in the [[Total variation distance of probability measures|total variation distance]].<ref>Klartag (2007, Theorem 1.2)</ref></blockquote>\n\nThese two {{mvar|ε<sub>n</sub>}}-close distributions have densities (in fact, log-concave densities), thus, the total variance distance between them is the integral of the absolute value of the difference between the densities. Convergence in total variation is stronger than weak convergence.\n\nAn important example of a log-concave density is a function constant inside a given convex body and vanishing outside; it corresponds to the uniform distribution on the convex body, which explains the term \"central limit theorem for convex bodies\".\n\nAnother example: {{math|''f''(''x''<sub>1</sub>, …, ''x<sub>n</sub>'') {{=}} const · exp( − ({{abs|''x''<sub>1</sub>}}<sup>''α''</sup> + … + {{abs|''x<sub>n</sub>''}}<sup>''α''</sup>)<sup>''β''</sup>)}} where {{math|''α'' > 1}} and {{math|''αβ'' > 1}}. If {{math|''β'' {{=}} 1}} then {{math|''f''(''x''<sub>1</sub>, …, ''x<sub>n</sub>'')}} factorizes into {{math|const · exp (−{{abs|''x''<sub>1</sub>}}<sup>''α''</sup>) … exp(−{{abs|''x<sub>n</sub>''}}<sup>''α''</sup>), }} which means {{math|''X''<sub>1</sub>, …, ''X<sub>n</sub>''}} are independent. In general, however, they are dependent.\n\nThe condition {{math|''f''(''x''<sub>1</sub>, …, ''x<sub>n</sub>'') {{=}} ''f''({{abs|''x''<sub>1</sub>}}, …, {{abs|''x<sub>n</sub>''}})}} ensures that {{math|''X''<sub>1</sub>, …, ''X<sub>n</sub>''}} are of zero mean and [[uncorrelated]];{{Citation needed|date=June 2012}} still, they need not be independent, nor even [[Pairwise independence|pairwise independent]].{{Citation needed|date=June 2012}} By the way, pairwise independence cannot replace independence in the classical central limit theorem.<ref>Durrett (2004, Section 2.4, Example 4.5)</ref>\n\nHere is a [[Berry–Esseen theorem|Berry–Esseen]] type result.\n\n<blockquote>'''Theorem.''' Let {{math|''X''<sub>1</sub>, …, ''X<sub>n</sub>''}} satisfy the assumptions of the previous theorem, then <ref>Klartag (2008, Theorem 1)</ref>\n\n: <math> \\left| \\mathbb{P} \\left( a \\le \\frac{ X_1+\\cdots+X_n }{ \\sqrt n } \\le b \\right) - \\frac1{\\sqrt{2\\pi}} \\int_a^b \\mathrm{e}^{-\\frac12 t^2} \\, dt \\right| \\le \\frac{C}{n} </math>\n\nfor all {{math|''a'' < ''b''}}; here {{mvar|C}} is a [[mathematical constant|universal (absolute) constant]]. Moreover, for every {{math|''c''<sub>1</sub>, …, ''c<sub>n</sub>'' ∈ '''ℝ'''}} such that {{math|''c''{{su|b=1|p=2}} + … + ''c''{{su|b=''n''|p=2}} {{=}} 1}},\n\n: <math> \\left| \\mathbb{P} \\left( a \\le c_1 X_1+\\cdots+c_n X_n \\le b \\right) - \\frac1{\\sqrt{2\\pi}} \\int_a^b \\mathrm{e}^{-\\frac12 t^2} \\, dt \\right| \\le C \\left( c_1^4+\\dots+c_n^4 \\right). </math></blockquote>\n\nThe distribution of {{math|{{sfrac|''X''<sub>1</sub> + … + ''X<sub>n</sub>''|{{sqrt|''n''}}}}}} need not be approximately normal (in fact, it can be uniform).<ref>Klartag (2007, Theorem 1.1)</ref> However, the distribution of {{math|''c''<sub>1</sub>''X''<sub>1</sub> + … + ''c<sub>n</sub>X<sub>n</sub>''}} is close to {{math|''N''(0,1)}} (in the total variation distance) for most vectors {{math|(''c''<sub>1</sub>, …, ''c<sub>n</sub>'')}} according to the uniform distribution on the sphere {{math|''c''{{su|b=1|p=2}} + … + ''c''{{su|b=''n''|p=2}} {{=}} 1}}.\n\n===Lacunary trigonometric series===\n<blockquote>'''Theorem''' ([[Raphaël Salem|Salem]]–[[Antoni Zygmund|Zygmund]]): Let {{mvar|U}} be a random variable distributed uniformly on {{math|(0,2π)}}, and {{math|''X<sub>k</sub>'' {{=}} ''r<sub>k</sub>'' cos(''n<sub>k</sub>U'' + ''a<sub>k</sub>'')}}, where\n* {{mvar|n<sub>k</sub>}} satisfy the lacunarity condition: there exists {{math|''q'' > 1}} such that {{math|''n''<sub>''k'' + 1</sub> ≥ ''qn''<sub>''k''</sub>}} for all {{mvar|k}},\n* {{mvar|r<sub>k</sub>}} are such that\n:: <math> r_1^2 + r_2^2 + \\cdots = \\infty \\quad\\text{ and }\\quad \\frac{ r_k^2 }{ r_1^2+\\cdots+r_k^2 } \\to 0, </math>\n* {{math|0 ≤ ''a''<sub>''k''</sub> < 2π}}.\nThen<ref name=Zygmund/><ref>Gaposhkin (1966, Theorem 2.1.13)</ref>\n\n: <math> \\frac{ X_1+\\cdots+X_k }{ \\sqrt{r_1^2+\\cdots+r_k^2} } </math>\n\nconverges in distribution to {{math|''N''(0, {{sfrac|1|2}})}}.</blockquote>\n\n===Gaussian polytopes===\n<blockquote>'''Theorem:'''  Let {{math|''A''<sub>1</sub>, …, ''A''<sub>''n''</sub>}} be independent random points on the plane {{math|'''ℝ'''<sup>2</sup>}} each having the two-dimensional standard normal distribution. Let {{mvar|K<sub>n</sub>}} be the [[convex hull]] of these points, and {{mvar|X<sub>n</sub>}} the area of {{mvar|K<sub>n</sub>}} Then<ref>Bárány & Vu (2007, Theorem 1.1)</ref>\n\n: <math> \\frac{ X_n - \\mathrm{E} (X_n) }{ \\sqrt{\\operatorname{Var} (X_n)} } </math>\n\nconverges in distribution to {{math|''N''(0,1)}} as {{mvar|n}} tends to infinity.</blockquote>\n\nThe same also holds in all dimensions greater than 2.\n\nThe [[convex polytope|polytope]] {{mvar|K<sub>n</sub>}} is called a Gaussian random polytope.\n\nA similar result holds for the number of vertices (of the Gaussian polytope), the number of edges, and in fact, faces of all dimensions.<ref>Bárány & Vu (2007, Theorem 1.2)</ref>\n\n===Linear functions of orthogonal matrices===\nA linear function of a matrix {{math|'''M'''}} is a linear combination of its elements (with given coefficients), {{math|'''M''' ↦ tr('''AM''')}} where {{math|'''A'''}} is the matrix of the coefficients; see [[Trace (linear algebra)#Inner product]].\n\nA random [[orthogonal matrix]] is said to be distributed uniformly, if its distribution is the normalized [[Haar measure]] on the [[orthogonal group]] {{math|O(''n'','''ℝ''')}}; see [[Rotation matrix#Uniform random rotation matrices]].\n\n<blockquote>'''Theorem.''' Let {{math|'''M'''}} be a random orthogonal {{math|''n'' × ''n''}} matrix distributed uniformly, and {{math|'''A'''}} a fixed {{math|''n'' × ''n''}} matrix such that {{math|tr('''AA'''*) {{=}} ''n''}}, and let {{math|''X'' {{=}} tr('''AM''')}}. Then<ref name=Meckes/> the distribution of {{mvar|X}} is close to {{math|''N''(0,1)}} in the total variation metric up to{{clarify|reason=what does up to mean here|date=June 2012}} {{math|{{sfrac|2{{sqrt|3}}|''n'' − 1}}}}.</blockquote>\n\n===Subsequences===\n<blockquote>'''Theorem.'''  Let random variables {{math|''X''<sub>1</sub>, ''X''<sub>2</sub>, … ∈ ''L''<sub>2</sub>(Ω)}} be such that {{math|''X<sub>n</sub>'' → 0}} [[Weak convergence (Hilbert space)|weakly]] in {{math|''L''<sub>2</sub>(Ω)}} and {{math|''X''{{su|b=''n''|2}} → 1}} weakly in {{math|''L''<sub>1</sub>(Ω)}}. Then there exist integers {{math|''n''<sub>1</sub> < ''n''<sub>2</sub> < …}} such that\n:<math> \\frac{ X_{n_1}+\\cdots+X_{n_k} }{ \\sqrt k }</math>\nconverges in distribution to {{math|''N''(0,1)}} as {{mvar|k}} tends to infinity.<ref>Gaposhkin (1966, Sect. 1.5)</ref></blockquote>\n\n===Random walk on a crystal lattice===\n\nThe central limit theorem may be established for the simple [[random walk]] on a crystal lattice (an infinite-fold abelian covering graph over a finite graph), and is used for design of crystal structures.\n<ref>{{cite book |last1=Kotani |first1=M. |last2=Sunada |first2=Toshikazu |author-link2=Toshikazu Sunada |date=2003 |title=Spectral geometry of crystal lattices |publisher=Contemporary Math |volume=338 |pp=271–305 |isbn=978-0-8218-4269-0}}</ref><ref>{{cite book |author-link=Toshikazu Sunada |last=Sunada |first=Toshikazu |date=2012 |title=Topological Crystallography – With a View Towards Discrete Geometric Analysis|series=Surveys and Tutorials in the Applied Mathematical Sciences |volume=6 |publisher=Springer |isbn=978-4-431-54177-6}}</ref>\n\n==Applications and examples==\n===Simple example===\n[[File:Empirical CLT - Figure - 040711.jpg|right|thumb|500px|This figure demonstrates the central limit theorem.  The sample means are generated using a random number generator, which draws numbers between 0 and 100 from a uniform probability distribution.  It illustrates that increasing sample sizes result in the 500 measured sample means being more closely distributed about the population mean (50 in this case).  It also compares the observed distributions with the distributions that would be expected for a normalized Gaussian distribution, and shows the [[Pearson's chi-squared test|chi-squared]] values that quantify the goodness of the fit (the fit is good if the reduced [[Pearson's chi-squared test|chi-squared]] value is less than or approximately equal to one).  The input into the normalized Gaussian function is the mean of sample means (~50) and the mean sample standard deviation divided by the square root of the sample size (~28.87/{{math|{{sqrt|''n''}}}}), which is called the standard deviation of the mean (since it refers to the spread of sample means).]]\n\nA simple example of the central limit theorem is rolling a large number of identical, unbiased dice. The distribution of the sum (or average) of the rolled numbers will be well approximated by a normal distribution. Since real-world quantities are often the balanced sum of many unobserved random events, the central limit theorem also provides a partial explanation for the prevalence of the normal probability distribution. It also justifies the approximation of large-sample [[statistic]]s to the normal distribution in controlled experiments.\n\n[[File:Dice sum central limit theorem.svg|left|thumb|250px|Comparison of probability density functions, {{math|**''p''(''k'')}} for the sum of {{mvar|n}} fair 6-sided dice to show their convergence to a normal distribution with increasing {{mvar|n}}, in accordance to the central limit theorem. In the bottom-right graph, smoothed profiles of the previous graphs are rescaled, superimposed and compared with a normal distribution (black curve).]]\n\n{{clear left}}\n\n[[File:Central Limit Theorem.png|Central Limit Theorem|center|thumb|640px|Another simulation using the binomial distribution. Random 0s and 1s were generated, and then their means calculated for sample sizes ranging from 1 to 512. Note that as the sample size increases the tails become thinner and the distribution becomes more concentrated around the mean.]]\n{{clear right}}\n\n===Real applications===\nPublished literature contains a number of useful and interesting examples and applications relating to the central limit theorem.<ref>Dinov, Christou & Sánchez (2008)</ref> One source<ref>{{cite web|url=http://wiki.stat.ucla.edu/socr/index.php/SOCR_EduMaterials_Activities_GCLT_Applications |title=SOCR EduMaterials Activities GCLT Applications - Socr |website=Wiki.stat.ucla.edu |date=2010-05-24 |accessdate=2017-01-23}}</ref> states the following examples:\n*The probability distribution for total distance covered in a [[random walk]] (biased or unbiased) will tend toward a [[normal distribution]].\n*Flipping a large number of coins will result in a normal distribution for the total number of heads (or equivalently total number of tails).\n\nFrom another viewpoint, the central limit theorem explains the common appearance of the \"bell curve\" in [[density estimation|density estimates]] applied to real world data. In cases like electronic noise, examination grades, and so on, we can often regard a single measured value as the weighted average of a large number of small effects. Using generalisations of the central limit theorem, we can then see that this would often (though not always) produce a final distribution that is approximately normal.\n\nIn general, the more a measurement is like the sum of independent variables with equal influence on the result, the more normality it exhibits. This justifies the common use of this distribution to stand in for the effects of unobserved variables in models like the [[linear model]].\n\n==Regression==\n[[Regression analysis]] and in particular [[ordinary least squares]] specifies that a [[dependent variable]] depends according to some function upon one or more [[independent variable]]s, with an additive [[Errors and residuals in statistics|error term]]. Various types of statistical inference on the regression assume that the error term is normally distributed. This assumption can be justified by assuming that the error term is actually the sum of a large number of independent error terms; even if the individual error terms are not normally distributed, by the central limit theorem their sum can be well approximated by a normal distribution.\n\n===Other illustrations===\n{{Main|Illustration of the central limit theorem}}\nGiven its importance to statistics, a number of papers and computer packages are available that demonstrate the convergence involved in the central limit theorem.<ref name=\"Marasinghe\">{{cite journal|last1=Marasinghe |first1=M. |last2=Meeker |first2=W. |last3=Cook |first3=D. |last4=Shin |first4=T. S. |date=Aug 1994 |title=Using graphics and simulation to teach statistical concepts |series=Paper presented at the Annual meeting of the American Statistician Association, Toronto, Canada}}</ref>\n\n==History==\nDutch mathematician [[Henk Tijms]] writes:<ref name=Tijms/>\n\n{{quote|The central limit theorem has an interesting history. The first version of this theorem was postulated by the French-born mathematician [[Abraham de Moivre]] who, in a remarkable article published in 1733, used the normal distribution to approximate the distribution of the number of heads resulting from many tosses of a fair coin. This finding was far ahead of its time, and was nearly forgotten until the famous French mathematician [[Pierre-Simon Laplace]] rescued it from obscurity in his monumental work ''Théorie analytique des probabilités'', which was published in 1812.  Laplace expanded De Moivre's finding by approximating the binomial distribution with the normal distribution. But as with De Moivre, Laplace's finding received little attention in his own time. It was not until the nineteenth century was at an end that the importance of the central limit theorem was discerned, when, in 1901, Russian mathematician [[Aleksandr Lyapunov]] defined it in general terms and proved precisely how it worked mathematically. Nowadays, the central limit theorem is considered to be the unofficial sovereign of probability theory.}}\n\nSir [[Francis Galton]] described the Central Limit Theorem in this way:<ref>{{cite book|last=Galton|first= F. |date=1889 |title=Natural Inheritance |url=http://galton.org/cgi-bin/searchImages/galton/search/books/natural-inheritance/pages/natural-inheritance_0073.htm |page= 66}}</ref>\n\n{{quote|I know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the \"Law of Frequency of Error\". The law would have been personified by the Greeks and deified, if they had known of it. It reigns with serenity and in complete self-effacement, amidst the wildest confusion. The huger the mob, and the greater the apparent anarchy, the more perfect is its sway. It is the supreme law of Unreason. Whenever a large sample of chaotic elements are taken in hand and marshalled in the order of their magnitude, an unsuspected and most beautiful form of regularity proves to have been latent all along.}}\n\nThe actual term \"central limit theorem\" (in German: \"zentraler Grenzwertsatz\") was first used by [[George Pólya]] in 1920 in the title of a paper.<ref name=Polya1920>{{Cite journal|last=Pólya|first=George|authorlink=George Pólya|year=1920|title=Über den zentralen Grenzwertsatz der Wahrscheinlichkeitsrechnung und das Momentenproblem|trans-title=On the central limit theorem of probability calculation and the problem of moments |journal=[[Mathematische Zeitschrift]]|volume=8|pages=171–181|language=German|url=http://www-gdz.sub.uni-goettingen.de/cgi-bin/digbib.cgi?PPN266833020_0008|doi=10.1007/BF01206525|issue=3–4}}</ref><ref name=LC1986/> Pólya referred to the theorem as \"central\" due to its importance in probability theory. According to Le Cam, the French school of probability interprets the word ''central'' in the sense that \"it describes the behaviour of the centre of the distribution as opposed to its tails\".<ref name=LC1986/> The abstract of the paper ''On the central limit theorem of calculus of probability and the problem of moments'' by Pólya<ref name=Polya1920/> in 1920 translates as follows.\n\n{{quote|text=The occurrence of the Gaussian probability density {{math|1 {{=}} ''e''<sup>−''x''<sup>2</sup></sup>}} in repeated experiments, in errors of measurements, which result in the combination of very many and very small elementary errors, in diffusion processes etc., can be explained, as is well-known, by the very same limit theorem, which plays a central role in the calculus of probability. The actual discoverer of this limit theorem is to be named Laplace; it is likely that its rigorous proof was first given by Tschebyscheff and its sharpest formulation can be found, as far as I am aware of, in an article by Liapounoff. ... }}\n\nA thorough account of the theorem's history, detailing Laplace's foundational work, as well as [[Augustin Louis Cauchy|Cauchy]]'s, [[Friedrich Bessel|Bessel]]'s and [[Siméon Denis Poisson|Poisson]]'s contributions, is provided by Hald.<ref name=Hald/> Two historical accounts, one covering the development from Laplace to Cauchy, the second the contributions by [[Richard von Mises|von Mises]], [[George Pólya|Pólya]], [[Jarl Waldemar Lindeberg|Lindeberg]], [[Paul Lévy (mathematician)|Lévy]], and [[Harald Cramér|Cramér]] during the 1920s, are given by Hans Fischer.<ref name=Fischer/> Le Cam describes a period  around 1935.<ref name=LC1986/> Bernstein<ref name=Bernstein/> presents a historical discussion focusing on the work of [[Pafnuty Chebyshev]] and his students [[Andrey Markov]] and [[Aleksandr Lyapunov]] that led to the first proofs of the CLT in a general setting.\n\nThrough the 1930s, progressively more general proofs of the Central Limit Theorem were presented. Many natural systems were found to exhibit [[Normal distribution|Gaussian distributions]]—a typical example being height distributions for humans. When statistical methods such as analysis of variance became established in the early 1900s, it became increasingly common to assume underlying Gaussian distributions.<ref>{{cite book|last=Wolfram|first=Stephen|title=A New Kind of Science|publisher=Wolfram Media, Inc.|year=2002|page=977|isbn=1-57955-008-8}}</ref>\n\nA curious footnote to the history of the Central Limit Theorem is that a proof of a result similar to the 1922 Lindeberg CLT was the subject of [[Alan Turing]]'s 1934 Fellowship Dissertation for [[King's College, Cambridge|King's College]] at the [[University of Cambridge]].  Only after submitting the work did Turing learn it had already been proved. Consequently, Turing's dissertation was not published.<ref>{{cite book|last=Hodges |first=Andrew | authorlink=Andrew Hodges | date=1983 |title=[[Alan Turing: The Enigma]] |location=London |publisher=Burnett Books |pages= 87–88 |isbn=0091521300}}</ref><ref name=Zabell/><ref name=Aldrich/>\n\n==See also==\n* [[Asymptotic equipartition property]]\n* [[Asymptotic distribution]]\n* [[Bates distribution]]\n* [[Benford's law]] – Result of extension of CLT to product of random variables.\n* [[Berry–Esseen theorem]]\n* [[Central limit theorem for directional statistics]] – Central limit theorem applied to the case of directional statistics\n* [[Delta method]] – to compute the limit distribution of a function of a random variable.\n* [[Erdős–Kac theorem]] – connects the number of prime factors of an integer with the normal probability distribution\n* [[Fisher–Tippett–Gnedenko theorem]] – limit theorem for extremum values (such as {{math|max{''X<sub>n</sub>''}}})\n* [[Irwin–Hall distribution]]\n* [[Markov chain central limit theorem]]\n* [[Normal distribution]]\n* [[Tweedie distribution|Tweedie convergence theorem]] – A theorem that can be considered to bridge between the central limit theorem and the [[Poisson convergence theorem]]<ref name=\"Jørgensen-1997\">{{cite book| last= Jørgensen|first= Bent | year = 1997| title = The Theory of Dispersion Models| publisher = Chapman & Hall | isbn = 978-0412997112}}</ref>\n* [[Normal distribution]]\n\n==Notes==\n{{reflist|30em|refs=\n\n<ref name=ABBN>{{Citation| first1= S.|last1= Artstein |author1-link=Shiri Artstein| first2= K. |last2= Ball |author2-link=Keith Martin Ball|first3= F. |last3= Barthe|author3-link=Franck Barthe|first4= A. |last4= Naor|author4-link=Assaf Naor |year=2004 |url=http://www.ams.org/jams/2004-17-04/S0894-0347-04-00459-X/home.html |title=Solution of Shannon's Problem on the Monotonicity of Entropy |journal=Journal of the American Mathematical Society  |volume=17 |pages= 975–982| doi= 10.1090/S0894-0347-04-00459-X| issue= 4 }}</ref>\n\n<ref name=Aldrich>{{cite journal|last=Aldrich |first=John |date=2009 |title=England and Continental Probability in the Inter-War Years |journal=Electronic Journ@l for History of Probability and Statistics |volume=5|issue=2 |url=http://www.jehps.net/decembre2009.html |at=Section 3}}</ref>\n\n<ref name=Bernstein>{{cite book|authorlink=Sergei Natanovich Bernstein|last=Bernstein|first=S. N. |date=1945 |contribution=On the work of P. L. Chebyshev in Probability Theory |title=Nauchnoe Nasledie P. L. Chebysheva. Vypusk Pervyi: Matematika |language=Russian |trans-title=The Scientific Legacy of P. L. Chebyshev. Part I: Mathematics |editor-first=S. N.|editor-last= Bernstein. |publisher=Academiya Nauk SSSR |location=Moscow & Leningrad |page=174}}</ref>\n\n<ref name=Fischer>{{cite book|last=Fischer|first=Hans|title=A History of the Central Limit Theorem: From Classical to Modern Probability Theory|series=Sources and Studies in the History of Mathematics and Physical Sciences|year=2011|publisher=Springer|isbn=978-0-387-87856-0|doi=10.1007/978-0-387-87857-7|location=New York|zbl=1226.60004|mr=2743162}} (Chapter 2: The Central Limit Theorem from Laplace to Cauchy: Changes in Stochastic Objectives and in Analytical Methods, Chapter 5.2: The Central Limit Theorem in the Twenties)</ref>\n\n<ref name=LC1986>{{cite journal| authorlink=Lucien Le Cam|last=Le Cam |first= Lucien|year=1986 |url=http://projecteuclid.org/euclid.ss/1177013818 |title=The central limit theorem around 1935  |journal=Statistical Science |volume=1|issue=1|pages=78–91|doi=10.2307/2245503}}</ref>\n\n<ref name=Hald>{{cite book|last=Hald |first=Andreas |url=http://www.gbv.de/dms/goettingen/229762905.pdf |title=A History of Mathematical Statistics from 1750 to 1930|isbn=978-0471179122|website=Gbv.de|at=chapter 17}}</ref>\n\n<ref name=Meckes>{{Cite journal|last=Meckes|first= Elizabeth|year=2008|title=Linear functions on the classical matrix groups|journal=Transactions of the American Mathematical Society|volume=360|pages=5355–5366|doi=10.1090/S0002-9947-08-04444-9|issue=10 |arxiv=math/0509441 }}</ref>\n\n<ref name=Rempala>{{cite journal | last1 = Rempala | first1 = G. | last2 = Wesolowski | first2 = J. | year = 2002 | title = Asymptotics of products of sums and ''U''-statistics | url = http://www.math.washington.edu/~ejpecp/EcpVol7/paper5.pdf | format = PDF | journal = Electronic Communications in Probability | volume = 7 | issue = | pages = 47–54 | doi=10.1214/ecp.v7-1046}}</ref>\n\n<ref name=Tijms>{{Cite book | first=Tijms |last= Henk |year=2004 |title= Understanding Probability: Chance Rules in Everyday Life|location= Cambridge |publisher= Cambridge University Press | isbn= 0-521-54036-4 | page=169}}</ref>\n\n<ref name=Zabell>{{cite book|last=Zabell |first=S. L. |date=2005 |title=Symmetry and Its Discontents: Essays on the History of Inductive Probability |publisher=Cambridge University Press |ISBN= 0-521-44470-5 |page=199}}</ref>\n\n<ref name=Zygmund>{{cite book|last=Zygmund|first=Antoni|authorlink=Antoni Zygmund|orig-year=1959|title=Trigonometric Series|publisher=Cambridge University Press|date=2003 |ISBN= 0-521-89053-5 |at=vol. II, sect. XVI.5, Theorem 5-5}}</ref>\n}}\n\n==References==\n*{{cite journal|last1=Bárány|first1=Imre|authorlink1=Imre Bárány|last2=Vu|first2=Van|year=2007|title=Central limit theorems for Gaussian polytopes|journal=Annals of Probability|publisher=Institute of Mathematical Statistics|volume=35|issue=4|pages=1593–1621|doi=10.1214/009117906000000791 |arxiv=math/0610192 }}\n*{{cite book|last=Bauer|first=Heinz|title=Measure and Integration Theory|publisher=de Gruyter|location=Berlin|year=2001|isbn=3110167190}}\n*{{cite book|last=Billingsley|first=Patrick|title=Probability and Measure|edition=3rd|publisher=John Wiley & Sons|year=1995|isbn=0-471-00710-2}}\n*{{cite book|last=Bradley|first=Richard|title=Introduction to Strong Mixing Conditions |edition=1st|year=2007|isbn=0-9740427-9-X|publisher=Kendrick Press|location=Heber City, UT}}\n*{{cite journal|last=Bradley|first=Richard|title=Basic Properties of Strong Mixing Conditions. A Survey and Some Open Questions\n|journal=Probability Surveys|year=2005|volume=2|pages=107–144 |arxiv=math/0511078v1 |doi=10.1214/154957805100000104 |url=https://arxiv.org/pdf/math/0511078.pdf}}\n*{{cite journal|last1=Dinov|first1=Ivo|last2=Christou|first2=Nicolas| last3=Sanchez|first3=Juana |year=2008|title=Central Limit Theorem: New SOCR Applet and Demonstration Activity|journal=Journal of Statistics Education|publisher=ASA|volume=16|issue=2 |url=http://www.amstat.org/publications/jse/v16n2/dinov.html|doi=10.1080/10691898.2008.11889560|pmc=3152447}}\n*{{Cite book|last=Durrett|first=Richard|authorlink=Rick Durrett|title=Probability: theory and examples|edition=3rd|year=2004|publisher= Cambridge University Press|isbn=0521765390}}\n*{{cite journal|last=Gaposhkin|first=V. F.|year=1966|title=Lacunary series and independent functions|journal=Russian Mathematical Surveys|volume=21|issue=6|pages=1–82| doi=10.1070/RM1966v021n06ABEH001196|bibcode=1966RuMaS..21....1G}}.\n*{{cite journal|last=Klartag |first=Bo'az |date=2007 |title=A central limit theorem for convex sets |journal=Inventiones Mathematicae |volume=168 |pages=91–131 |doi=10.1007/s00222-006-0028-8 |arxiv=math/0605014|bibcode=2007InMat.168...91K }}\n*{{cite journal|last=Klartag |first=Bo'az |date=2008 |title=A Berry–Esseen type inequality for convex bodies with an unconditional basis |journal=Probability Theory and Related Fields |doi=10.1007/s00440-008-0158-6 |arxiv=0705.0832 |volume=145 |pages=1–33}}\n\n==External links==\n{{commons category}}\n* Simplified, step-by-step explanation of the classical  [http://www.quantumfieldtheory.info/CentralLimitTheorem.pdf  Central Limit Theorem.] with histograms at every step.\n* Hands-on explanation of the  [https://www.khanacademy.org/math/probability/statistics-inferential/sampling_distribution/v/central-limit-theorem Central Limit Theorem in tutorial videos from Khan Academy], with many examples\n* [http://blog.vctr.me/posts/central-limit-theorem.html Central Limit Theorem Visualized in D3] interactive HTML5 simulation of flipping coins.\n*{{springer|title=Central limit theorem|id=p/c021180}}\n*[https://statistical-engineering.com/clt-summary/ Animated examples of the CLT]\n*[http://www.vias.org/simulations/simusoft_cenlimit.html Central Limit Theorem] interactive simulation to experiment with various parameters\n*[http://ccl.northwestern.edu/curriculum/ProbLab/CentralLimitTheorem.html CLT in NetLogo (Connected Probability &mdash; ProbLab)] interactive simulation with a variety of modifiable parameters\n*[http://wiki.stat.ucla.edu/socr/index.php/SOCR_EduMaterials_Activities_GeneralCentralLimitTheorem General Central Limit Theorem Activity] & corresponding [http://www.socr.ucla.edu/htmls/SOCR_Experiments.html SOCR CLT Applet] (Select the Sampling Distribution CLT Experiment from the drop-down list of [http://wiki.stat.ucla.edu/socr/index.php/About_pages_for_SOCR_Experiments SOCR Experiments])\n*[http://www.indiana.edu/~jkkteach/ExcelSampler/ Generate sampling distributions in Excel] Specify arbitrary population, [[sample size]], and sample statistic.\n* MIT OpenCourseWare Lecture 18.440 ''Probability and Random Variables'', Spring 2011, Scott Sheffield [https://web.archive.org/web/20120529212657/http://ocw.mit.edu/courses/mathematics/18-440-probability-and-random-variables-spring-2011/lecture-notes/MIT18_440S11_Lecture31.pdf Another proof.] Retrieved 2012-04-08.\n*[http://www.causeweb.org CAUSEweb.org] is a site with many resources for teaching statistics including the Central Limit Theorem\n* [http://demonstrations.wolfram.com/TheCentralLimitTheorem/ The Central Limit Theorem] by Chris Boucher, [[Wolfram Demonstrations Project]].\n* {{MathWorld |title=Central Limit Theorem |urlname=CentralLimitTheorem}}\n* [https://web.archive.org/web/20081102151742/http://animation.yihui.name/prob%3Acentral_limit_theorem Animations for the Central Limit Theorem] by Yihui Xie using the [[R (programming language)|R]] package [https://cran.r-project.org/package=animation animation]\n* Teaching demonstrations of the CLT: clt.examp function in {{cite book|author=Greg Snow|year=2012|title= TeachingDemos: Demonstrations for teaching and learning. R package version 2.8.|url= https://cran.r-project.org/package=TeachingDemos|ref=harv}}\n\n{{statistics}}\n\n{{DEFAULTSORT:Central Limit Theorem}}\n[[Category:Probability theorems]]\n[[Category:Statistical theorems]]\n[[Category:Articles containing proofs]]\n[[Category:Central limit theorem| ]]\n[[Category:Asymptotic theory (statistics)]]"
    },
    {
      "title": "Central limit theorem for directional statistics",
      "url": "https://en.wikipedia.org/wiki/Central_limit_theorem_for_directional_statistics",
      "text": "In [[probability theory]], the [[central limit theorem]] states conditions under which the average of a sufficiently large number of [[Statistical independence|independent]] [[random variables]], each with finite mean and variance, will be approximately [[normal distribution|normally distributed]].<ref>{{harvtxt|Rice|1995}}{{full|date=November 2012}}</ref>  \n\n[[Directional statistics]] is the subdiscipline of [[statistics]] that deals with directions ([[unit vector]]s in '''R'''<sup>''n''</sup>), [[Cartesian Coordinate System|axes]] (lines through the origin in '''R'''<sup>''n''</sup>) or [[rotation]]s in '''R'''<sup>''n''</sup>. The means and variances of directional quantities are all finite, so that the central limit theorem may be applied to the particular case of directional statistics.<ref name=\"SRJ\">{{cite book |title=Topics in circular statistics |last=Jammalamadaka |first=S. Rao |authorlink= |author2=SenGupta, A.|year=2001 |publisher=World Scientific |location=New Jersey |isbn=978-981-02-3778-3 |url=https://books.google.com/?id=sKqWMGqQXQkC&printsec=frontcover&dq=Jammalamadaka+Topics+in+circular#v=onepage&q&f=false |accessdate=2011-05-15}}</ref>\n\nThis article will deal only with unit vectors in 2-dimensional space ('''R'''<sup>''2''</sup>) but the method described can be extended to the general case.\n\n== The central limit theorem ==\n\nA sample of angles <math>\\theta_i</math> are measured, and since they are indefinite to within a factor of <math>2\\pi</math>, the complex definite quantity <math>z_i=e^{i\\theta_i}=\\cos(\\theta_i)+i\\sin(\\theta_i)</math> is used as the random variate. The probability distribution from which the sample is drawn may be characterized by its moments, which may be expressed in Cartesian and polar form:\n\n:<math>m_n=E(z^n)= C_n +i S_n = R_n e^{i \\theta_n}\\,</math>\n\nIt follows that:\n\n:<math>C_n=E(\\cos (n\\theta))\\,</math>\n:<math>S_n=E(\\sin (n\\theta))\\,</math>\n:<math>R_n=|E(z^n)|=\\sqrt{C_n^2+S_n^2}\\,</math>\n:<math>\\theta_n=\\arg(E(z^n))\\,</math>\n\nSample moments for N trials are:\n\n:<math>\\overline{m_n}=\\frac{1}{N}\\sum_{i=1}^N z_i^n =\\overline{C_n} +i \\overline{S_n} = \\overline{R_n} e^{i \\overline{\\theta_n}}</math>\n\nwhere \n\n:<math>\\overline{C_n}=\\frac{1}{N}\\sum_{i=1}^N\\cos(n\\theta_i)</math>\n:<math>\\overline{S_n}=\\frac{1}{N}\\sum_{i=1}^N\\sin(n\\theta_i)</math>\n:<math>\\overline{R_n}=\\frac{1}{N}\\sum_{i=1}^N |z_i^n|</math>\n:<math>\\overline{\\theta_n}=\\frac{1}{N}\\sum_{i=1}^N \\arg(z_i^n)</math>\n\nThe vector [<math>\\overline{ C_1 },\\overline{ S_1 }</math>] may be used as a representation of the sample mean <math>(\\overline{m_1})</math> and may be taken as a 2-dimensional random variate.<ref name=\"SRJ\"/> The bivariate [[central limit theorem]] states that the [[joint probability distribution]] for <math>\\overline{ C_1 }</math> and <math>\\overline{ S_1 }</math> in the limit of a large number of samples is given by:\n\n:<math>[\\overline{C_1},\\overline{S_1}] \\xrightarrow{d} \\mathcal{N}([C_1,S_1],\\Sigma/N)</math>\n\nwhere <math>\\mathcal{N}()</math> is the [[bivariate normal distribution]] and <math>\\Sigma</math> is the [[covariance matrix]] for the circular distribution:\n\n:<math>\n\\Sigma\n=\n\\begin{bmatrix}\n \\sigma_{CC} & \\sigma_{CS} \\\\\n \\sigma_{SC} & \\sigma_{SS}\n\\end{bmatrix}\n\\quad</math>\n\n:<math>\\sigma_{CC}=E(\\cos^2\\theta)-E(\\cos\\theta)^2\\,</math>\n:<math>\\sigma_{CS}=\\sigma_{SC}=E(\\cos\\theta\\sin\\theta)-E(\\cos\\theta)E(\\sin\\theta)\\,</math>\n:<math>\\sigma_{SS}=E(\\sin^2\\theta)-E(\\sin\\theta)^2\\,</math>\n\nNote that the bivariate normal distribution is defined over the entire plane, while the mean is confined to be in the unit ball (on or inside the unit circle). This means that the integral of the limiting (bivariate normal) distribution over the unit ball will not be equal to unity, but rather approach unity as ''N'' approaches infinity.\n\nIt is desired to state the limiting bivariate distribution in terms of the moments of the distribution.\n\n== Covariance matrix in terms of moments ==\n\nUsing multiple angle [[trigonometric identities]]<ref name=\"SRJ\"/>\n\n:<math>C_2= E(\\cos(2\\theta)) = E(\\cos^2\\theta-1)=E(1-\\sin^2\\theta)\\,</math>\n:<math>S_2= E(\\sin(2\\theta)) = E(2\\cos\\theta\\sin\\theta)\\,</math>\n\nIt follows that:\n\n:<math>\\sigma_{CC}=E(\\cos^2\\theta)-E(\\cos\\theta)^2 =\\frac{1}{2}\\left(1 + C_2 - 2C_1^2\\right)</math>\n:<math>\\sigma_{CS}=E(\\cos\\theta\\sin\\theta)-E(\\cos\\theta)E(\\sin\\theta)=\\frac{1}{2}\\left(S_2 - 2 C_1 S_1   \\right)</math>\n:<math>\\sigma_{SS}=E(\\sin^2\\theta)-E(\\sin\\theta)^2 =\\frac{1}{2}\\left(1   - C_2 - 2S_1^2\\right)</math>\n\nThe covariance matrix is now expressed in terms of the moments of the circular distribution.\n\nThe central limit theorem may also be expressed in terms of the polar components of the mean. If <math>P(\\overline{C_1},\\overline{S_1})d\\overline{C_1}d\\overline{S_1}</math> is the probability of finding the mean in area element <math>d\\overline{C_1}d\\overline{S_1}</math>, then that probability may also be written <math>P(\\overline{R_1}\\cos(\\overline{\\theta_1}),\\overline{R_1}\\sin(\\overline{\\theta_1}))\\overline{R_1}d\\overline{R_1}d\\overline{\\theta_1}</math>.\n\n==References==\n\n<references/>\n\n[[Category:Directional statistics]]\n[[Category:Central limit theorem| ]]\n[[Category:Asymptotic theory (statistics)]]"
    },
    {
      "title": "Berry–Esseen theorem",
      "url": "https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem",
      "text": "In [[probability theory]], the [[central limit theorem]] states that, under certain circumstances, the [[probability distribution]] of the scaled [[sample mean|mean of a random sample]] [[convergence in distribution|converges]] to a [[normal distribution]] as the sample size increases to infinity. Under stronger assumptions, the '''Berry–Esseen theorem''', or '''Berry–Esseen inequality''', gives a more quantitative result, because it also specifies the rate at which this convergence takes place by giving a bound on the maximal error of [[approximation theory|approximation]] between the normal distribution and the true distribution of the scaled sample mean. The approximation is measured by the [[Kolmogorov–Smirnov test#Kolmogorov–Smirnov statistic|Kolmogorov–Smirnov distance]]. In the case of [[Independence (probability theory)|independent samples]], the convergence rate is {{math|''n''<sup>−1/2</sup>}}, where {{math|''n''}} is the sample size, and the constant is estimated in terms of the [[Skewness|third]] absolute [[Moment (mathematics)|normalized moments]].\n\n==Statement of the theorem==\nStatements of the theorem vary, as it was independently discovered by two [[mathematician]]s, [[Andrew C. Berry]] (in 1941) and [[Carl-Gustav Esseen]] (1942), who then, along with other authors, refined it repeatedly over subsequent decades.\n\n===Identically distributed summands===\n\nOne version, sacrificing generality somewhat for the sake of clarity, is the following:\n\n:There exists a positive [[Constant (mathematics)|constant]] ''C'' such that if ''X''<sub>1</sub>, ''X''<sub>2</sub>, ..., are [[Independent and identically distributed random variables|i.i.d. random variables]] with [[Expected value|E]](''X''<sub>1</sub>) = 0, E(''X''<sub>1</sub><sup>2</sup>) = σ<sup>2</sup> > 0, and E(|''X''<sub>1</sub>|<sup>3</sup>) = ρ < ∞,<ref group=\"note\">Since the random variables are identically distributed, ''X''<sub>2</sub>, ''X''<sub>3</sub>, ... all have the same [[moment (mathematics)|moments]] as ''X''<sub>1</sub>.</ref> and if we define\n::<math>Y_n = {X_1 + X_2 + \\cdots + X_n \\over n}</math>\n:the [[sample mean]], with ''F''<sub>''n''</sub> the [[cumulative distribution function]] of\n::<math>{Y_n \\sqrt{n} \\over {\\sigma}},</math><!-- please DO NOT CHANGE this formula unless you have read and understood the relevant comments on the talk page -->\n:and Φ the cumulative distribution function of the [[standard normal distribution]], then for all ''x'' and ''n'',\n::<math>\\left|F_n(x) - \\Phi(x)\\right| \\le {C \\rho \\over \\sigma^3\\sqrt{n}}.\\ \\ \\ \\ (1)</math>\n\n[[Image:BerryEsseenTheoremCDFGraphExample.png|thumb|250px|Illustration of the difference in cumulative distribution functions alluded to in the theorem.]]\nThat is: given a sequence of [[independent and identically distributed random variables]], each having [[mean]] zero and positive [[variance]], if additionally the third absolute [[moment (mathematics)|moment]] is finite, then the [[cumulative distribution function]]s of the [[Standard score|standardized]] sample mean and the standard normal distribution differ (vertically, on a graph) by no more than the specified amount.  Note that the approximation error for all ''n'' (and hence the limiting rate of convergence for indefinite ''n'' sufficiently large) is bounded by  the [[Big O notation|order]] of ''n''<sup>−1/2</sup>.\n\nCalculated values of the constant ''C'' have decreased markedly over the years, from the original value of 7.59 by {{harvtxt|Esseen|1942}}, to 0.7882 by {{harvtxt|van Beek|1972}}, then 0.7655 by {{harvtxt|Shiganov|1986}}, then 0.7056 by {{harvtxt|Shevtsova|2007}}, then 0.7005 by {{harvtxt|Shevtsova|2008}}, then 0.5894 by {{harvtxt|Tyurin|2009}}, then 0.5129 by {{harvtxt|Korolev|Shevtsova|2009}}, then 0.4785 by {{harvtxt|Tyurin|2010}}. The detailed review can be found in the papers {{harvtxt|Korolev|Shevtsova|2009}}, {{harvtxt|Korolev|Shevtsova|2010}}. The best estimate {{Asof|2012|lc=yes}}, ''C''&nbsp;<&nbsp;0.4748, follows from the inequality\n:<math>\\sup_{x\\in\\mathbb R}\\left|F_n(x) - \\Phi(x)\\right| \\le {0.33554 (\\rho+0.415\\sigma^3)\\over \\sigma^3\\sqrt{n}},</math>\ndue to {{harvtxt|Shevtsova|2011}}, since σ<sup>3</sup>&nbsp;≤&nbsp;ρ and 0.33554&nbsp;·&nbsp;1.415&nbsp;<&nbsp;0.4748. However, if ρ&nbsp;≥&nbsp;1.286σ<sup>3</sup>, then the estimate \n:<math>\\sup_{x\\in\\mathbb R}\\left|F_n(x) - \\Phi(x)\\right| \\le {0.3328 (\\rho+0.429\\sigma^3)\\over \\sigma^3\\sqrt{n}},</math>\nwhich is also proved in {{harvtxt|Shevtsova|2011}}, gives an even tighter upper estimate.\n\n{{harvtxt|Esseen|1956}} proved that the constant also satisfies the lower bound\n: <math>\n    C\\geq\\frac{\\sqrt{10}+3}{6\\sqrt{2\\pi}} \\approx 0.40973 \\approx \\frac{1}{\\sqrt{2\\pi}} + 0.01079 .\n  </math>\n\n===Non-identically distributed summands===\n\n:Let ''X''<sub>1</sub>, ''X''<sub>2</sub>, ..., be independent random variables with [[expected value|E]](''X''<sub>''i''</sub>) = 0, E(''X''<sub>''i''</sub><sup>2</sup>) = σ<sub>''i''</sub><sup>2</sup> > 0, and E(|''X''<sub>''i''</sub>|<sup>3</sup>) = ρ<sub>''i''</sub> < ∞. Also, let\n::<math>S_n = {X_1 + X_2 + \\cdots + X_n \\over \\sqrt{\\sigma_1^2+\\sigma_2^2+\\cdots+\\sigma_n^2} }</math>\n:be the normalized ''n''-th partial sum. Denote ''F''<sub>''n''</sub> the [[cumulative distribution function|cdf]] of ''S''<sub>''n''</sub>, and Φ the cdf of the [[standard normal distribution]]. For the sake of convenience denote  \n::<math>\\vec{\\sigma}=(\\sigma_1,\\ldots,\\sigma_n),\\ \\vec{\\rho}=(\\rho_1,\\ldots,\\rho_n).</math>\n:In 1941, [[Andrew C. Berry]] proved that for all ''n'' there exists an absolute constant ''C''<sub>1</sub> such that\n::<math>\\sup_{x\\in\\mathbb R}\\left|F_n(x) - \\Phi(x)\\right| \\le C_1\\cdot\\psi_1,\\ \\ \\ \\ (2)</math>\n:where\n::<math>\\psi_1=\\psi_1\\big(\\vec{\\sigma},\\vec{\\rho}\\big)=\\Big({\\textstyle\\sum\\limits_{i=1}^n\\sigma_i^2}\\Big)^{-1/2}\\cdot\\max_{1\\le\ni\\le n}\\frac{\\rho_i}{\\sigma_i^2}.</math>\n\n:Independently, in 1942, [[Carl-Gustav Esseen]] proved that for all ''n'' there exists an absolute constant ''C''<sub>0</sub> such that\n::<math>\\sup_{x\\in\\mathbb R}\\left|F_n(x) - \\Phi(x)\\right| \\le C_0\\cdot\\psi_0, \\ \\ \\ \\ (3)</math>\n:where\n::<math>\\psi_0=\\psi_0\\big(\\vec{\\sigma},\\vec{\\rho}\\big)=\\Big({\\textstyle\\sum\\limits_{i=1}^n\\sigma_i^2}\\Big)^{-3/2}\\cdot\\sum\\limits_{i=1}^n\\rho_i.</math>\n\nIt is easy to make sure that ψ<sub>0</sub>≤ψ<sub>1</sub>. Due to this circumstance inequality (3) is conventionally called the Berry–Esseen inequality, and the quantity ψ<sub>0</sub> is called the Lyapunov fraction of the third order. Moreover, in the case where the summands ''X''<sub>1</sub>, ..., ''X''<sub>''n''</sub> have identical distributions \n::<math>\\psi_0=\\psi_1=\\frac{\\rho_1}{\\sigma_1^3\\sqrt{n}},</math>\nand thus the bounds stated by inequalities (1), (2) and (3) coincide apart from the constant.\n\nRegarding ''C''<sub>0</sub>, obviously, the lower bound established by {{harvtxt|Esseen|1956}} remains valid:\n: <math>\n    C_0\\geq\\frac{\\sqrt{10}+3}{6\\sqrt{2\\pi}} = 0.4097\\ldots.\n  </math>\n\nThe upper bounds for ''C''<sub>0</sub> were subsequently lowered from the original estimate 7.59 due to {{harvtxt|Esseen|1942}} to (considering recent results only) 0.9051 due to {{harvtxt|Zolotarev|1967}}, 0.7975 due to {{harvtxt|van Beek|1972}}, 0.7915 due to {{harvtxt|Shiganov|1986}}, 0.6379 and 0.5606 due to {{harvtxt|Tyurin|2009}} and {{harvtxt|Tyurin|2010}}. {{Asof|2011}} the best estimate is 0.5600 obtained by {{harvtxt|Shevtsova|2010}}.\n\n==See also==\n*[[Chernoff's inequality]]\n*[[Edgeworth series]]\n*[[List of inequalities]]\n*[[List of mathematical theorems]]\n*[[Concentration inequality]]\n\n==Notes==\n<references group=\"note\" />\n\n==References==\n{{refbegin}}\n* {{cite journal| first=Andrew C. | last=Berry | year=1941\n  | title=The Accuracy of the Gaussian Approximation to the Sum of Independent Variates\n  | journal=Transactions of the American Mathematical Society\n  | volume=49 | issue=1 | pages=122–136 | ref=harv| jstor=1990053| doi=10.1090/S0002-9947-1941-0003498-3\n  }}\n* Durrett, Richard (1991). ''Probability: Theory and Examples''. Pacific Grove, CA: Wadsworth & Brooks/Cole. {{ISBN|0-534-13206-5}}.\n* {{cite journal | ref=harv\n  | last = Esseen | first = Carl-Gustav\n  | title = On the Liapunoff limit of error in the theory of probability\n  | year = 1942\n  | journal = Arkiv för Matematik, Astronomi och Fysik | issn = 0365-4133\n  | volume = A28\n  | pages = 1–19\n  }}\n* {{cite journal | ref=harv\n  | last = Esseen | first = Carl-Gustav\n  | title = A moment inequality with an application to the central limit theorem\n  | year = 1956\n  | journal = Skand. Aktuarietidskr.\n  | volume = 39\n  | pages = 160–170\n  }}\n* Feller, William (1972). ''An Introduction to Probability Theory and Its Applications, Volume II'' (2nd ed.). New York: John Wiley & Sons. {{ISBN|0-471-25709-5}}.\n* {{cite journal | ref=harv\n  | last1 = Korolev | first1 = V. Yu. | last2 = Shevtsova | first2 = I. G.\n  | title = On the upper bound for the absolute constant in the Berry–Esseen inequality\n  | year = 2010\n  | journal = Theory of Probability and Its Applications \n  | volume = 54 | issue = 4\n  | pages = 638–658\n  | doi = 10.1137/S0040585X97984449\n  }}\n* {{cite journal | ref=harv\n  | last1 = Korolev | first1 = Victor | last2 = Shevtsova | first2 = Irina\n  | title = An improvement of the Berry–Esseen inequality with applications to Poisson and mixed Poisson random sums\n  | year = 2010\n  | journal = Scandinavian Actuarial Journal\n  | volume = 2012 | issue = 2 | doi= 10.1080/03461238.2010.485370 | pages=1–25\n  | arxiv= 0912.2795}}\n* Manoukian, Edward B. (1986). ''Modern Concepts and Theorems of Mathematical Statistics''. New York: Springer-Verlag. {{ISBN|0-387-96186-0}}.\n* Serfling, Robert J. (1980). ''Approximation Theorems of Mathematical Statistics''. New York: John Wiley & Sons. {{ISBN|0-471-02403-1}}.\n* {{cite journal | ref=harv\n  | last = Shevtsova | first = I. G.\n  | title = On the absolute constant in the Berry–Esseen inequality\n  | year = 2008\n  | journal = The Collection of Papers of Young Scientists of the Faculty of Computational Mathematics and Cybernetics \n  | issue = 5\n  | pages = 101–110\n  }}\n* {{cite journal | ref=harv\n  | last = [[Irina Shevtsova|Shevtsova]] | first = Irina\n  | title = Sharpening of the upper bound of the absolute constant in the Berry–Esseen inequality\n  | year = 2007\n  | journal = Theory of Probability and Its Applications \n  | volume = 51 | issue = 3 \n  | pages = 549–553\n  | doi= 10.1137/S0040585X97982591\n  }}\n* {{cite journal | ref=harv\n  | last = Shevtsova | first = Irina\n  | title = An Improvement of Convergence Rate Estimates in the Lyapunov Theorem\n  | year = 2010\n  | journal = Doklady Mathematics \n  | volume = 82 | issue = 3 \n  | pages = 862–864\n  | doi= 10.1134/S1064562410060062\n  }}\n* {{cite arXiv | ref=harv\n  | last = Shevtsova | first = Irina\n  | title = On the absolute constants in the Berry Esseen type inequalities for identically distributed summands\n  | year = 2011\n  | eprint = 1111.6554\n  | class = math.PR\n  }}\n* {{cite journal | ref=harv\n  | last = Shiganov | first = I.S.\n  | title = Refinement of the upper bound of a constant in the remainder term of the central limit theorem\n  | year = 1986\n  | journal = Journal of Soviet Mathematics \n  | volume = 35\n  | pages = 109–115\n  | doi=10.1007/BF01121471 | issue=3\n  }}\n* {{cite journal | ref=harv\n  | last = Tyurin | first = I.S.\n  | title = On the accuracy of the Gaussian approximation\n  | year = 2009\n  | journal = Doklady Mathematics \n  | volume = 80 | issue = 3\n  | pages = 840–843 | doi=10.1134/S1064562409060155\n  }}\n* {{cite journal | ref=harv\n  | last = Tyurin | first = I.S.\n  | title = An improvement of upper estimates of the constants in the Lyapunov theorem\n  | year = 2010\n  | journal = Russian Mathematical Surveys \n  | volume = 65 | issue = 3(393)\n  | pages = 201–202\n  }}\n* {{cite journal | ref=harv\n  | last = van Beek | first = P.\n  | title = An application of Fourier methods to the problem of sharpening the Berry–Esseen inequality\n  | year = 1972\n  | journal = Zeitschrift für Wahrscheinlichkeitstheorie und Verwandte Gebiete \n  | volume = 23\n  | pages = 187–196\n  | doi=10.1007/BF00536558 | issue=3\n  }}\n* {{cite journal | ref=harv\n  | last = Zolotarev | first = V. M.\n  | title = A sharpening of the inequality of Berry–Esseen\n  | year = 1967\n  | journal = Zeitschrift für Wahrscheinlichkeitstheorie und Verwandte Gebiete\n  | volume = 8\n  | pages = 332–342 | doi=10.1007/BF00531598 | issue=4\n  }}\n{{refend}}\n\n==External links==\n* Gut, Allan & Holst Lars. [https://web.archive.org/web/20040202103426/http://www.stat.unipd.it/bernoulli/02a/bn_3.html Carl-Gustav Esseen], retrieved Mar. 15, 2004.\n* {{springer|title=Berry–Esseen inequality|id=p/b015760}}\n\n{{DEFAULTSORT:Berry-Esseen theorem}}\n[[Category:Probabilistic inequalities]]\n[[Category:Statistical theorems]]\n[[Category:Central limit theorem]]"
    },
    {
      "title": "De Moivre–Laplace theorem",
      "url": "https://en.wikipedia.org/wiki/De_Moivre%E2%80%93Laplace_theorem",
      "text": "{{Short description|Convergence in distribution of binomial to normal distribution}}[[File:Quincunx (Galton Box) - Galton 1889 diagram.png|right|250px|thumb|Within a system whose bins are filled according to the [[binomial distribution]] (such as [[Francis Galton|Galton's]] \"[[bean machine]]\", shown here), given a sufficient number of trials (here the rows of pins, each of which causes a dropped \"bean\" to fall toward the left or right), a shape representing the probability distribution of ''k'' successes in ''n'' trials (see bottom of Fig.&nbsp;7) matches approximately the Gaussian distribution with mean ''np'' and variance ''np''(1−''p''), assuming the trials are independent and successes occur with probability ''p''.]]\n\n[[File:De moivre-laplace.gif|right|250px|thumb|Consider tossing a set of ''n'' coins a very large number of times and counting the number of \"heads\" that result each time. The possible number of heads on each toss, ''k'', runs from 0 to ''n'' along the horizontal axis, while the vertical axis represents the relative frequency of occurrence of the outcome ''k'' heads. The height of each dot is thus the probability of observing ''k'' heads when tossing ''n'' coins (a [[binomial distribution]] based on ''n'' trials). According to the de Moivre–Laplace theorem, as ''n'' grows large, the shape of the discrete distribution converges to the continuous Gaussian curve of the [[normal distribution]].]]\n\nIn [[probability theory]], the '''de Moivre–Laplace theorem''', which is a special case of the [[central limit theorem]], states that the [[normal distribution]] may be used as an approximation to the [[binomial distribution]] under certain conditions.  In particular, the theorem shows that the [[probability mass function]] of the random number of \"successes\" observed in a series of <math>n</math> [[statistical independence|independent]] [[Bernoulli trial]]s, each having probability <math>p</math> of success (a binomial distribution with <math>n</math> trials), [[Convergence in distribution|converges]] to the [[probability density function]] of the normal distribution with mean <math>np</math> and standard deviation<math>\\sqrt{np(1-p)}</math>, as <math>n</math> grows large, assuming <math>p</math> is not <math>0</math> or <math>1</math>.\n\nThe theorem appeared in the second edition of ''[[The Doctrine of Chances]]'' by [[Abraham de Moivre]], published in 1738.<!--it appears de Moivre first published it in an article in 1733 or 1734 which later made it into The Doctrine of Chances, but I couldn't find a reference for that.-->  Although de Moivre did not use the term \"Bernoulli trials\", he wrote about the [[probability distribution]] of the number of times \"heads\" appears when a coin is tossed 3600 times.<ref>{{cite book|chapter=De Moivre on the law of normal probability|last=Walker|first=Helen M|chapterurl=http://www.york.ac.uk/depts/maths/histstat/demoivre.pdf|editor-last=Smith|editor-first=David Eugene|title=A source book in mathematics|year=1985|publisher=Dover|isbn=0-486-64690-4|page=78|quote=But altho’ the taking an infinite number of Experiments be not practicable, yet the preceding Conclusions may very well be applied to finite numbers, provided they be great, for Instance, if 3600 Experiments be taken, make ''n'' {{=}} 3600, hence ½''n'' will be {{=}} 1800, and ½√''n'' 30, then the Probability of the Event’s neither appearing oftner than 1830 times, nor more rarely than 1770, will be 0.682688.}}</ref>\n\nThis is one derivation of the particular [[Gaussian function]] used in the normal distribution.\n\n== Theorem ==\nAs ''n'' grows large, for ''k'' in the [[neighborhood (mathematics)|neighborhood]] of ''np'' we can approximate<ref>{{cite book |last=Papoulis |first=Athanasios |authorlink=Athanasios Papoulis |last2=Pillai |first2=S. Unnikrishna |title=Probability, Random Variables, and Stochastic Processes |location=Boston |publisher=McGraw-Hill |edition=4th |year=2002 |isbn=0-07-122661-3 }}</ref><ref>{{cite book |last=Feller |first=W. |authorlink=William Feller |year=1968 |title=An Introduction to Probability Theory and Its Applications |volume=Volume 1 |publisher=Wiley |isbn=0-471-25708-7 |at=Section VII.3 }}</ref>\n: <math>{n \\choose k}\\, p^k q^{n-k} \\simeq \\frac{1}{\\sqrt{2 \\pi npq}}\\,e^{-\\frac{(k-np)^2}{2npq}}, \\qquad p+q=1,\\ p, q > 0</math>\nin the sense that the ratio of the left-hand side to the right-hand side converges to 1 as ''n'' → ∞.\n\n===Proof===\n\nThe theorem can be more rigorously stated as follows: <math>\\left(X\\!\\,-\\!\\, np\\right)\\!/\\!\\sqrt{npq}</math>, with <math>\\textstyle X</math> a binomially distributed random variable, approaches the standard normal as <math>n\\!\\to\\!\\infty</math>, with the ratio of the probability mass of <math>X</math> to the limiting normal density being 1. This can be shown for an arbitrary nonzero and finite point <math>c</math>. On the unscaled curve for <math>X</math>, this would be a point <math>k</math> given by \n\n:<math>k=np+c\\sqrt{npq}</math>\n\nFor example, with <math>c</math> at 3, <math>k</math> stays 3 standard deviations from the mean in the unscaled curve.\n\nThe normal distribution with mean <math>\\mu</math> and standard deviation <math>\\sigma</math> is defined by the differential equation (DE)\n\n:<math>f'\\!(x)\\!=\\!-\\!\\,\\frac{x-\\mu}{\\sigma^2}f(x)</math> with  initial condition set by the probability axiom <math>\\int_{-\\infty}^{\\infty}\\!f(x)\\,dx\\!=\\!1</math>. \nThe binomial distribution limit approaches the normal if the binomial satisfies this DE. As the binomial is discrete the equation starts as a [[difference equation]] whose limit morphs to a DE. Difference equations use the [https://calculus.subwiki.org/wiki/discrete%20derivative discrete derivative], <math>\\textstyle p(k\\!+\\!1)\\!-\\!p(k)</math>, the change for step size 1. As <math>\\textstyle n\\!\\to\\!\\infty</math>, the discrete derivative becomes the [[derivative|continuous derivative]]. Hence the proof need show only that, for the unscaled binomial distribution, \n\n:<math>\\frac{f'\\!(x)}{f\\!(x)}\\!\\cdot\\!-\\!\\,\\frac{\\sigma^2}{x-\\mu} \\!\\to\\! 1</math> as <math> n\\!\\to\\!\\infty</math>.\n\nThe required result can be shown directly:\n\n:<math>\n\\begin{align}\n\\frac{f'\\!(x)}{f\\!(x)}\\frac{npq}{np\\!\\,-\\!\\,k}\\!&=\\frac{p\\left(n, k + 1\\right) - p\\left(n, k\\right)}{p\\left(n, k\\right)}\\frac{\\sqrt{npq}}{-c} \\\\ &= \\frac{np - k -q}{kq+q}\\frac{\\sqrt{npq}}{-c} \\\\ &= \\frac{-c\\sqrt{npq} -q}{npq + cq\\sqrt{npq}+q}\\frac{\\sqrt{npq}}{-c} \\\\ & \\to 1\n\\end{align}\n</math>\n\nThe last holds because the term <math>-cnpq</math> dominates both the denominator and the numerator as <math>n\\!\\to\\!\\infty</math>.\n\nAs <math>\\textstyle k</math> takes just integral values, the constant <math>\\textstyle c</math> is subject to a rounding error. However, the maximum of this error, <math>\\textstyle {0.5}/\\!\\sqrt{npq}</math>, is a vanishing value.<ref>{{cite journal|last1=Thamattoor|first1=Ajoy|title=Normal limit of the binomial via the discrete derivative|journal=The College Mathematics Journal|date=2018|volume=49|issue=3|pages=216–217|doi=10.1080/07468342.2018.1440872}}</ref>\n\n===Alternate Proof===\n\nThe proof consists of transforming the left-hand side (in the statement of the theorem) to the right-hand side by three approximations.\n\nFirst, according to [[Stirling's formula]], the factorial of a large number ''n'' can be replaced with the approximation\n\n: <math>n! \\simeq  n^n e^{-n}\\sqrt{2 \\pi n}\\qquad \\text{as } n \\to \\infty.</math>\n\nThus\n\n: <math>\\begin{align}{n \\choose k} p^k q^{n-k} & = \\frac{n!}{k!(n-k)!} p^k q^{n-k} \\\\& \\simeq \\frac{n^n e^{-n}\\sqrt{2\\pi n} }{k^ke^{-k}\\sqrt{2\\pi k}  (n-k)^{n-k}e^{-(n-k)}\\sqrt{2\\pi (n-k)}} p^k q^{n-k}\\\\&=\\sqrt{\\frac{n}{2\\pi k\\left(n-k\\right)}}\\frac{n^n}{k^k\\left(n-k\\right)^{n-k}}p^kq^{n-k}\\\\&=\\sqrt{\\frac{n}{2\\pi k\\left(n-k\\right)}}\\left(\\frac{np}{k}\\right)^k\\left(\\frac{nq}{n-k}\\right)^{n-k}\\end{align}</math>\n\nNext, the approximation <math>\\tfrac{k}{n} \\to p</math> is used to match the root above to the desired root on the right-hand side.\n\n: <math>\\begin{align}{n \\choose k} p^k q^{n-k} & \\simeq \\sqrt{\\frac{1}{2\\pi n\\frac{k}{n}\\left(1-\\frac{k}{n}\\right)}}\\left(\\frac{np}{k}\\right)^{k} \\left(\\frac{nq}{n-k}\\right)^{n-k}\\\\&\\simeq\\frac{1}{\\sqrt {2\\pi npq}}\\left(\\frac{np}{k}\\right)^{k} \\left(\\frac{nq}{n-k}\\right)^{n-k} \\qquad p+q=1\\\\ \\end{align}</math>\n\nFinally, the expression is rewritten as an exponential and the Taylor Series approximation for ln(1+x) is used:\n\n:<math> \\ln\\left(1+x\\right)\\simeq x-\\frac{x^2}{2}+\\frac{x^3}{3}-\\cdots</math>\n\nThen\n\n: <math>\\begin{align}{n \\choose k} p^k q^{n-k} &\\simeq \\frac{1}{\\sqrt {2\\pi npq}}\\exp\\left\\{\\ln \\left( \\left(\\frac{np}{k}\\right)^{k} \\right)+\\ln \\left( \\left(\\frac{nq}{n-k}\\right)^{n-k}\\right)\\right\\}\\\\ &=\\frac{1}{\\sqrt {2\\pi npq}}\\exp\\left\\{-k\\ln\\left(\\frac{k}{np}\\right)+(k-n)\\ln \\left(\\frac{n-k}{nq}\\right)\\right\\}\\\\&=\\frac{1}{\\sqrt {2\\pi npq}}\\exp\\left\\{-k\\ln\\left(\\frac{np+x\\sqrt{npq}}{np}\\right)+(k-n)\\ln \\left(\\frac{n-np-x\\sqrt{npq}}{nq}\\right)\\right\\}\\\\&=\\frac{1}{\\sqrt {2\\pi npq}}\\exp\\left\\{-k\\ln\\left({1+x\\sqrt\\frac{q}{np}}\\right)+(k-n)\\ln \\left({1-x\\sqrt\\frac{p}{nq}}\\right)\\right\\}\\qquad p+q=1\\\\&=\\frac{1}{\\sqrt {2\\pi npq}}\\exp\\left\\{-k\\left({x\\sqrt\\frac{q}{np}}-\\frac{x^2q}{2np}+\\cdots\\right)+(k-n) \\left({-x\\sqrt\\frac{p}{nq}-\\frac{x^2p}{2nq}}-\\cdots\\right)\\right\\}\\\\&=\\frac{1}{\\sqrt {2\\pi npq}}\\exp\\left\\{\\left(-np-x\\sqrt{npq}\\right)\\left({x\\sqrt\\frac{q}{np}}-\\frac{x^2q}{2np}+\\cdots\\right)+\\left(np+x\\sqrt{npq}-n\\right) \\left(-x\\sqrt\\frac{p}{nq}-\\frac{x^2p}{2nq}-\\cdots\\right)\\right\\}\\\\&=\\frac{1}{\\sqrt {2\\pi npq}}\\exp\\left\\{\\left(-np-x\\sqrt{npq}\\right)\\left(x\\sqrt\\frac{q}{np}-\\frac{x^2q}{2np}+\\cdots\\right)-\\left(nq-x\\sqrt{npq}\\right) \\left(-x\\sqrt\\frac{p}{nq}-\\frac{x^2p}{2nq}-\\cdots\\right)\\right\\}\\\\&=\\frac{1}{\\sqrt {2\\pi npq}}\\exp\\left\\{\\left(-x\\sqrt{npq}+\\frac{1}{2}x^2q-x^2q+\\cdots\\right)+\\left(x\\sqrt{npq}+\\frac{1}{2}x^2p-x^2p-\\cdots\\right) \\right\\}\\\\&=\\frac{1}{\\sqrt {2\\pi npq}}\\exp\\left\\{-\\frac{1}{2}x^2q-\\frac{1}{2}x^2p-\\cdots\\right\\}\\\\&=\\frac{1}{\\sqrt {2\\pi npq}}\\exp\\left\\{-\\frac{1}{2}x^2(p+q)-\\cdots\\right\\}\\\\&\\simeq\\frac{1}{\\sqrt {2\\pi npq}}\\exp\\left\\{-\\frac{1}{2}x^2\\right\\}\\\\&=\\frac{1}{\\sqrt {2\\pi npq}}e^\\frac{-(k-np)^2}{2npq}\\\\ \\end{align}</math>\n\nEach \"<math>\\simeq</math>\" in the above argument is a statement that two quantities are asymptotically equivalent as ''n'' increases, in the same sense as in the original statement of the theorem—i.e., that the ratio of each pair of quantities approaches 1 as ''n'' → ∞.\n\n==Trivia==\n* ''[[The Wall (game show)|The Wall]]'' is an example of a television [[game show]] that uses the De Moivre–Laplace theorem.<ref>{{cite_web|title=What if God Were a Giant Game of Plinko?|last=Roeder|first=Oliver|website=[[FiveThirtyEight]]|url=https://fivethirtyeight.com/features/what-if-god-were-a-giant-game-of-plinko/|date=November 17, 2017|access-date=November 24, 2017}}</ref>\n\n== See also ==\n\n* [[Poisson distribution]] is an alternative approximation of the binomial distribution for large values of ''n''.\n\n== Notes ==\n{{reflist}}\n\n{{DEFAULTSORT:De Moivre-Laplace, Theorem Of}}\n[[Category:Central limit theorem]]"
    },
    {
      "title": "Illustration of the central limit theorem",
      "url": "https://en.wikipedia.org/wiki/Illustration_of_the_central_limit_theorem",
      "text": "This article gives two concrete '''illustrations''' of the '''[[central limit theorem]]'''. Both involve the sum of [[independent and identically-distributed random variables]] and show how the [[probability distribution]] of the sum approaches the [[normal distribution]] as the number of terms in the sum increases.\n\nThe first illustration involves a [[continuous probability distribution]], for which the random variables have a [[probability density function]].\n\nThe second illustration, for which most of the computation can be done by hand, involves a [[discrete probability distribution]], which is characterized by a [[probability mass function]].\n\nA free full-featured interactive simulation that allows the user to set up various distributions and adjust the sampling parameters is available through the [[#|External links]] section at the bottom of this page.\n\n==Illustration of the continuous case==\nThe [[probability density function#Sums of independent random variables|density of the sum of two independent real-valued random variables]] equals the [[convolution]] of the density functions of the original variables.\n\nThus, the density of the sum of ''m''+''n'' terms of a sequence of independent identically distributed variables equals the convolution of the densities of the sums of ''m'' terms and of ''n'' term. In particular, the density of the sum of ''n''+1 terms equals the convolution of the density of the sum of ''n'' terms with the original density (the \"sum\" of 1 term).\n\nA [[probability density function]] is shown in the first figure below. Then the densities of the sums of two, three, and four [[independent identically distributed variables]], each having the original density, are shown in the following figures.\nIf the original density is a [[piecewise]] [[polynomial]], as it is in the example, then so are the sum densities, of increasingly higher degree. Although the original density is far from normal, the density of the sum of just a few variables with that density is much smoother and has some of the qualitative features of the [[normal distribution|normal density]].\n\nThe convolutions were computed via the [[discrete Fourier transform]]. A list of values ''y'' = ''f''(''x''<sub>0</sub> + ''k'' Δ''x'') was constructed, where ''f'' is the original density function, and Δ''x'' is approximately equal to 0.002, and ''k'' is equal to 0 through 1000. The discrete Fourier transform ''Y'' of ''y'' was computed. Then the convolution of ''f'' with itself is proportional to the inverse discrete Fourier transform of the [[pointwise product]] of ''Y'' with itself.\n\n{{clear}}\n[[Image:Central limit thm 1.png|right|thumb|''A probability density function''.]]\n\n===Original probability density function===\n\nWe start with a probability density function. This function, although discontinuous, is far from the most pathological example that could be created. It is a piecewise polynomial, with pieces of degrees 0 and 1. The mean of this distribution is 0 and its standard deviation is 1.\n{{clear}}\n[[Image:Central limit thm 2.png|right|thumb|''Density of a sum of two variables''.]]\n\n===Probability density function of the sum of two terms===\n\nNext we compute the density of the sum of two independent variables, each having the above density. \nThe density of the sum is the [[convolution]] of the above density with itself.\n\nThe sum of two variables has mean 0.\nThe density shown in the figure at right has been rescaled by <math>\\sqrt{2}</math>, so that its standard deviation is 1.\n\nThis density is already smoother than the original.\nThere are obvious lumps, which correspond to the intervals on which the original density was defined.\n\n{{clear}}\n[[Image:Central limit thm 3.png|right|thumb|''Density of a sum of three variables''.]]\n\n===Probability density function of the sum of three terms===\n\nWe then compute the density of the sum of three independent variables, each having the above density. \nThe density of the sum is the convolution of the first density with the second.\n\nThe sum of three variables has mean 0.\nThe density shown in the figure at right has been rescaled by {{radic|3}}, so that its standard deviation is 1.\n\nThis density is even smoother than the preceding one.\nThe lumps can hardly be detected in this figure.\n\n{{clear}}\n[[Image:Central limit thm 4.png|right|thumb|''Density of a sum of four variables'']]\n\n===Probability density function of the sum of four terms===\n\nFinally, we compute the density of the sum of four independent variables, each having the above density. \nThe density of the sum is the convolution of the first density with the third (or the second density with itself).\n\nThe sum of four variables has mean 0.\nThe density shown in the figure at right has been rescaled by {{radic|4}}, so that its standard deviation is 1.\n\nThis density appears qualitatively very similar to a normal density.\nNo lumps can be distinguished by the eye.\n\n{{clear}}\n\n==Illustration of the discrete case==\nThis section illustrates the central limit theorem via an example for which the computation can be done quickly by hand on paper, unlike the more computing-intensive example of the previous section.\n\n===Original probability mass function===\nSuppose the probability distribution of a [[discrete random variable]] ''X'' puts equal weights on 1, 2, and 3:\n\n:<math>X=\\left\\{\\begin{matrix} 1 & \\mbox{with}\\ \\mbox{probability}\\ 1/3, \\\\\n2 & \\mbox{with}\\ \\mbox{probability}\\ 1/3, \\\\\n3 & \\mbox{with}\\ \\mbox{probability}\\ 1/3.\n\\end{matrix}\\right.</math>\n\nThe probability mass function of the random variable ''X'' may be depicted by the following [[bar graph]]:\n\n     o    o    o\n    -------------\n     1    2    3\n\nClearly this looks nothing like the bell-shaped curve of the normal distribution.  Contrast the above with the depictions below.\n\n===Probability mass function of the sum of two terms===\nNow consider the sum of two independent copies of ''X'':\n\n:<math>\\left\\{\\begin{matrix}\n1+1 & = & 2 \\\\\n1+2 & = & 3 \\\\\n1+3 & = & 4 \\\\\n2+1 & = & 3 \\\\\n2+2 & = & 4 \\\\\n2+3 & = & 5 \\\\\n3+1 & = & 4 \\\\\n3+2 & = & 5 \\\\\n3+3 & = & 6\n\\end{matrix}\\right\\}\n=\\left\\{\\begin{matrix}\n2 & \\mbox{with}\\ \\mbox{probability}\\ 1/9 \\\\\n3 & \\mbox{with}\\ \\mbox{probability}\\ 2/9 \\\\\n4 & \\mbox{with}\\ \\mbox{probability}\\ 3/9 \\\\\n5 & \\mbox{with}\\ \\mbox{probability}\\ 2/9 \\\\\n6 & \\mbox{with}\\ \\mbox{probability}\\ 1/9\n\\end{matrix}\\right\\}\n</math>\n\nThe probability mass function of this sum may be depicted thus:\n\n               o\n          o    o    o\n     o    o    o    o    o\n    ----------------------------\n     2    3    4    5    6\n\nThis still does not look very much like the bell-shaped curve, but, like the bell-shaped curve and unlike the probability mass function of ''X'' itself, it is higher in the middle than in the two tails.\n\n===Probability mass function of the sum of three terms===\nNow consider the sum of ''three'' independent copies of this random variable:\n\n:<math>\\left\\{\\begin{matrix}\n1+1+1 & = & 3 \\\\\n1+1+2 & = & 4 \\\\\n1+1+3 & = & 5 \\\\\n1+2+1 & = & 4 \\\\\n1+2+2 & = & 5 \\\\\n1+2+3 & = & 6 \\\\\n1+3+1 & = & 5 \\\\\n1+3+2 & = & 6 \\\\\n1+3+3 & = & 7 \\\\\n2+1+1 & = & 4 \\\\\n2+1+2 & = & 5 \\\\\n2+1+3 & = & 6 \\\\\n2+2+1 & = & 5 \\\\\n2+2+2 & = & 6 \\\\\n2+2+3 & = & 7 \\\\\n2+3+1 & = & 6 \\\\\n2+3+2 & = & 7 \\\\\n2+3+3 & = & 8 \\\\\n3+1+1 & = & 5 \\\\\n3+1+2 & = & 6 \\\\\n3+1+3 & = & 7 \\\\\n3+2+1 & = & 6 \\\\\n3+2+2 & = & 7 \\\\\n3+2+3 & = & 8 \\\\\n3+3+1 & = & 7 \\\\\n3+3+2 & = & 8 \\\\\n3+3+3 & = & 9 \n\\end{matrix}\\right\\}\n=\\left\\{\\begin{matrix}\n3 & \\mbox{with}\\ \\mbox{probability}\\ 1/27 \\\\\n4 & \\mbox{with}\\ \\mbox{probability}\\ 3/27 \\\\\n5 & \\mbox{with}\\ \\mbox{probability}\\ 6/27 \\\\\n6 & \\mbox{with}\\ \\mbox{probability}\\ 7/27 \\\\\n7 & \\mbox{with}\\ \\mbox{probability}\\ 6/27 \\\\\n8 & \\mbox{with}\\ \\mbox{probability}\\ 3/27 \\\\\n9 & \\mbox{with}\\ \\mbox{probability}\\ 1/27\n\\end{matrix}\\right\\}\n</math>\n\nThe probability mass function of this sum may be depicted thus:\n\n                    o\n               o    o    o\n               o    o    o\n               o    o    o\n          o    o    o    o    o\n          o    o    o    o    o\n     o    o    o    o    o    o    o\n    ---------------------------------\n     3    4    5    6    7    8    9\n\nNot only is this bigger at the center than it is at the tails, but as one moves toward the center from either tail, the slope first increases and then decreases, just as with the bell-shaped curve.\n\nThe degree of its resemblance to the bell-shaped curve can be quantified as follows.  Consider\n\n:Pr(''X''<sub>1</sub> + ''X''<sub>2</sub> + ''X''<sub>3</sub> &le; 7) = 1/27 + 3/27 + 6/27 + 7/27 + 6/27 = 23/27 = 0.85185... .\n\nHow close is this to what a [[normal distribution|normal]] approximation would give?  It can readily be seen that the expected value of ''Y'' = ''X''<sub>1</sub> + ''X''<sub>2</sub> + ''X''<sub>3</sub> is 6 and the standard deviation of ''Y'' is the [[square root of 2]].  Since ''Y'' ≤ 7 (weak inequality) if and only if ''Y'' < 8 (strict inequality), we use a [[continuity correction]] and seek\n\n:<math>\\mbox{Pr}(Y\\leq 7.5)\n=\\mbox{P}\\left({Y-6 \\over \\sqrt{2}}\\leq{7.5-6 \\over \\sqrt{2}}\\right)\n=\\mbox{Pr}(Z\\leq 1.0606602\\dots) = 0.85558\\dots</math>\n\nwhere ''Z'' has a standard normal distribution.  The difference between 0.85185... and 0.85558... seems remarkably small when it is considered that the number of independent random variables that were added was only three.\n\n===Probability mass function of the sum of 1,000 terms===\n[[Image:Central theorem 2.svg|thumb|350px]]\nThe following image shows the result of a simulation based on the example presented in this page. The extraction from the uniform distribution is repeated 1,000 times, and the results are summed.\n\nSince the simulation is based on the [[Monte Carlo method]], the process is repeated 10,000 times. The results shows that the distribution of the sum of 1,000 uniform extractions resembles the bell-shaped curve very well.\n\n==External links==\n*[http://mathworld.wolfram.com/UniformSumDistribution.html Uniform summation at Mathworld]\n*[http://www.statisticalengineering.com/central_limit_theorem.html Animated examples of the CLT]\n*[http://wiki.stat.ucla.edu/socr/index.php/SOCR_EduMaterials_Activities_GeneralCentralLimitTheorem General Dynamic SOCR CLT Activity]\n*[http://blog.vctr.me/posts/central-limit-theorem.html Interactive JavaScript application demonstrating the Central Limit Theorem]\n*[http://www.vias.org/simulations/simusoft_cenlimit.html Interactive Simulation of the Central Limit Theorem for Windows]\n* [http://wiki.stat.ucla.edu/socr/index.php/SOCR_EduMaterials_Activities_GeneralCentralLimitTheorem The SOCR CLT activity provides hands-on demonstration of the theory and applications of this limit theorem].\n\n[[Category:Central limit theorem]]"
    },
    {
      "title": "Lindeberg's condition",
      "url": "https://en.wikipedia.org/wiki/Lindeberg%27s_condition",
      "text": "In [[probability theory]], '''Lindeberg's condition''' is a [[Necessary and sufficient condition|sufficient condition]] (and under certain conditions also a necessary condition) for the [[central limit theorem]] (CLT) to hold for a sequence of independent [[random variables]].<ref>{{cite book |first=P. |last=Billingsley |title=Probability and Measure |publisher=Wiley |year=1986 |edition=2nd |page=369 |url=https://books.google.com/books?id=Q2IPAQAAMAAJ&pg=PA369 }}</ref><ref>{{cite book |first=R. B. |last=Ash |title=Probability and measure theory |year=2000\n|edition=2nd |page=307 }}</ref><ref>{{cite book |first=S. I. |last=Resnick | title=A probability Path |year=1999 |page=314 }}</ref> Unlike the classical CLT, which requires that the random variables in question have finite [[variance]] and be both [[Independent and identically-distributed random variables|independent and identically distributed]], [[Central limit theorem#Lindeberg CLT|Lindeberg's CLT]] only requires that they have finite variance, satisfy Lindeberg's condition, and be [[Statistical independence|independent]]. It is named after the Finnish mathematician [[Jarl Waldemar Lindeberg]].<ref>{{cite journal |first=J. W. |last=Lindeberg | title=Eine neue Herleitung des Exponentialgesetzes in der Wahrscheinlichkeitsrechnung | year=1922 | pages=211–225 | journal = [[Mathematische Zeitschrift]] |volume = 15 | issue = 1 | doi = 10.1007/BF01494395 | url=http://gdz.sub.uni-goettingen.de/en/dms/load/img/?PPN=PPN266833020_0015&DMDID=dmdlog21}}</ref>\n\n==Statement==\n\nLet <math>(\\Omega, \\mathcal{F}, \\mathbb{P})</math> be a [[probability space]], and <math>X_k : \\Omega \\to \\mathbb{R},\\,\\, k \\in \\mathbb{N}</math>, be ''independent'' random variables defined on that space. Assume the expected values <math>\\mathbb{E}\\,[X_k] = \\mu_k</math> and variances <math>\\mathrm{Var}\\,[X_k] = \\sigma_k^2</math> exist and are finite. Also let <math>s_n^2 := \\sum_{k=1}^n \\sigma_k^2 .</math>\n\nIf this sequence of independent random variables <math>X_k</math> satisfies '''Lindeberg's condition''':\n\n:<math>  \\lim_{n \\to \\infty} \\frac{1}{s_n^2}\\sum_{k = 1}^n \\mathbb{E} \\left[(X_k - \\mu_k)^2 \\cdot \\mathbf{1}_{\\{ | X_k - \\mu_k | > \\varepsilon s_n \\}} \\right] = 0</math>\n\nfor all <math>\\varepsilon > 0</math>, where '''1'''<sub>{…}</sub> is the [[indicator function]], then the [[central limit theorem]] holds, i.e. the random variables\n\n:<math>Z_n := \\frac{\\sum_{k = 1}^n (X_k - \\mu_k)}{s_n}</math>\n\n[[convergence in distribution|converge in distribution]] to a [[normal distribution|standard normal random variable]] as <math>n \\to \\infty.</math>\n\nLindeberg's condition is sufficient, but not in general necessary (i.e. the inverse implication does not hold in general).\nHowever, if the sequence of independent random variables in question satisfies\n\n:<math>\\max_{k=1,\\ldots,n} \\frac{\\sigma_k^2}{s_n^2} \\to 0, \\quad \\text{ as } n \\to \\infty,</math>\n\nthen Lindeberg's condition is both sufficient and necessary, i.e. it holds if and only if the result of central limit theorem holds.\n\n==Interpretation==\n\nBecause the Lindeberg condition implies <math>\\max_{k=1,\\ldots,n}\\frac{\\sigma^2_k}{s_n^2} \\to 0</math> as <math>n \\to \\infty</math>, it guarantees that the contribution of any individual random variable <math>X_k</math> (<math>1\\leq k\\leq n</math>) to the variance <math>s_n^2</math> is arbitrarily small, for sufficiently large values of <math>n</math>.\n\n==See also==\n*[[Lyapunov condition]]\n*[[Central limit theorem]]\n\n==References==\n{{Reflist}}\n\n[[Category:Statistical theorems]]\n[[Category:Central limit theorem]]"
    },
    {
      "title": "Lyapunov's central limit theorem",
      "url": "https://en.wikipedia.org/wiki/Lyapunov%27s_central_limit_theorem",
      "text": "#REDIRECT [[Central limit theorem#Lyapunov CLT]]\n\n{{Redirect category shell|1=\n{{R to section}}\n}}\n\n[[Category:Probability theorems]]\n[[Category:Statistical theorems]]\n[[Category:Central limit theorem]]"
    },
    {
      "title": "Martingale central limit theorem",
      "url": "https://en.wikipedia.org/wiki/Martingale_central_limit_theorem",
      "text": "In [[probability theory]], the [[central limit theorem]] says that, under certain conditions, the sum of many [[independent identically-distributed random variables]], when scaled appropriately, [[converges in distribution]] to a standard [[normal distribution]].  The '''martingale central limit theorem''' generalizes this result for random variables to [[Martingale (probability theory)|martingales]], which are [[stochastic process]]es where the change in the value of the process from time ''t'' to time ''t''&nbsp;+&nbsp;1 has [[expected value|expectation]] zero, even conditioned on previous outcomes.\n\n==Statement==\nHere is a simple version of the martingale central limit theorem: Let\n:<math>X_1, X_2, \\dots\\,</math> -- be a martingale with bounded increments, i.e., suppose\n\n:<math>\\operatorname{E}[X_{t+1} -  X_t \\vert X_1,\\dots, X_t]=0\\,,</math>\n\nand\n\n:<math>|X_{t+1} - X_t| \\le k</math>\n\n[[almost surely]] for some fixed bound ''k'' and all ''t''. Also assume that <math>|X_1|\\le k</math> almost surely.\n\nDefine\n\n:<math>\\sigma_t^2 = \\operatorname{E}[(X_{t+1}-X_t)^2|X_1, \\ldots, X_t],</math>\n\nand let\n\n:<math>\\tau_\\nu = \\min\\left\\{t : \\sum_{i=1}^{t} \\sigma_i^2 \\ge \\nu\\right\\}.</math>\n\nThen\n\n:<math>\\frac{X_{\\tau_\\nu}}{\\sqrt{\\nu}}</math>\n\nconverges in distribution to the normal distribution with mean 0 and variance 1 as <math>\\nu \\to +\\infty \\!</math>. More explicitly,\n\n:<math>\\lim_{\\nu \\to +\\infty} \\operatorname{P} \\left(\\frac{X_{\\tau_\\nu}}{\\sqrt{\\nu}} < x\\right) = \\Phi(x)\n= \\frac{1}{\\sqrt{2\\pi}}\n\\int_{-\\infty}^x\n\\exp\\left(-\\frac{u^2}{2}\\right)\n\\, du, \\quad x\\in\\mathbb{R}.\n</math>\n\n==The sum of variances must diverge to infinity==\n\nThe statement of the above result implicitly assumes the variances sum to infinity, so the following holds with probability 1:\n\n:<math> \\sum_{t=1}^{\\infty} \\sigma_t^2 = \\infty </math>\n\nThis ensures that with probability 1:\n\n:<math> \\tau_v < \\infty , \\forall v \\geq 0 </math>\n\nThis condition is violated, for example, by a martingale that is defined to be zero almost surely for all time.\n\n==Intuition on the result==\n\nThe result can be intuitively understood by writing the ratio as a summation: \n\n:<math> \\frac{X_{\\tau_v}}{\\sqrt{v}} = \\frac{X_1}{\\sqrt{v}} + \\frac{1}{\\sqrt{v}} \\sum_{i=1}^{\\tau_v-1} (X_{i+1}-X_i)  ,  \\forall \\tau_v \\geq 1 </math>\n\nThe first term on the right-hand-side asymptotically converges to zero, while the second term is qualitatively similar to the summation formula for the central limit theorem in the simpler case of i.i.d. random variables. While the terms in the above expression are not necessarily i.i.d., they are uncorrelated and have zero mean.  Indeed:\n\n:<math> E[(X_{i+1}-X_i)] = 0 , \\forall i \\in \\{1, 2, 3, ...\\}</math>\n\n:<math> E[(X_{i+1}-X_i)(X_{j+1}-X_j)] = 0 , \\forall i \\neq j, i, j \\in \\{1, 2, 3, ...\\} </math>\n\n==References==\nMany other variants on the martingale central limit theorem can be found in:\n* {{cite book | first = Peter | last = Hall |author2=C. C. Heyde | year = 1980 | title = Martingale Limit Theory and Its Application | publisher = Academic Press | location = New York | isbn = 0-12-319350-8}}\n* For the discussion of Theorem 5.4 there, and correct form of Corollary 5.3(ii), see {{cite journal| \nlast=Bradley| first=Richard|\n  journal=Journal of Theoretical Probability|\n  volume=1|\n  pages=115–119|\n  year=1988|\n  publisher=Springer | title=On some results of MI Gordin: a clarification of a misunderstanding|\ndoi=10.1007/BF01046930| \nissue=2}}\n\n[[Category:Martingale theory]]\n[[Category:Central limit theorem]]"
    },
    {
      "title": "Normality test",
      "url": "https://en.wikipedia.org/wiki/Normality_test",
      "text": "In [[statistics]], '''normality tests''' are used to determine if a [[data set]] is well-modeled by a [[normal distribution]] and to compute how likely it is for a [[random variable]] underlying the data set to be normally distributed.\n\nMore precisely, the tests are a form of [[model selection]], and can be interpreted several ways, depending on one's [[interpretations of probability]]:\n* In [[descriptive statistics]] terms, one measures a [[goodness of fit]] of a normal model to the data – if the fit is poor then the data are not well modeled in that respect by a normal distribution, without making a judgment on any underlying variable.\n* In [[frequentist statistics]] [[statistical hypothesis testing]], data are tested against the [[null hypothesis]] that it is normally distributed.\n* In [[Bayesian statistics]], one does not \"test normality\" per se, but rather computes the likelihood that the data come from a normal distribution with given parameters ''μ'',''σ'' (for all ''μ'',''σ''), and compares that with the likelihood that the data come from other distributions under consideration, most simply using a [[Bayes factor]] (giving the relative likelihood of seeing the data given different models), or more finely taking a [[prior distribution]] on possible models and parameters and computing a [[posterior distribution]] given the computed likelihoods.\n\n==Graphical methods==\nAn informal approach to testing normality is to compare a [[histogram]] of the sample data to a normal probability curve. The empirical distribution of the data (the histogram) should be bell-shaped and resemble the normal distribution. This might be difficult to see if the sample is small. In this case one might proceed by regressing the data against the [[quantile]]s of a normal distribution with the same mean and variance as the sample. Lack of fit to the regression line suggests a departure from normality (see Anderson Darling coefficient and minitab).\n\nA graphical tool for assessing normality is the [[normal probability plot]], a [[quantile-quantile plot]] (QQ plot) of the standardized data against the [[standard normal distribution]]. Here the [[Pearson product-moment correlation coefficient|correlation]] between the sample data and normal quantiles (a measure of the goodness of fit) measures how well the data are modeled by a normal distribution. For normal data the points plotted in the QQ plot should fall approximately on a straight line, indicating  high positive correlation. These plots are easy to interpret and also have the benefit that outliers are easily identified.\n\n==Back-of-the-envelope test==\n{{anchor|Back of the envelope test}}\nSimple [[back-of-the-envelope]] test takes the [[sample maximum and minimum]] and computes their [[z-score]], or more properly [[t-statistic]]\n(number of sample standard deviations that a sample is above or below the sample mean), and compares it to the [[68–95–99.7 rule]]:\nif one has a 3''σ'' event (properly, a 3''s'' event) and substantially fewer than 300 samples, or a 4''s'' event and substantially fewer than 15,000 samples, then a normal distribution will understate the maximum magnitude of deviations in the sample data.\n\nThis test is useful in cases where one faces [[kurtosis risk]] – where large deviations matter – and has the benefits that it is very easy to compute and to communicate: non-statisticians can easily grasp that \"6''σ'' events are very rare in normal distributions\".\n\n==Frequentist tests==\nTests of univariate normality include the following:\n* [[D'Agostino's K-squared test]], \n* [[Jarque–Bera test]], \n* [[Anderson–Darling test]], \n* [[Cramér–von Mises criterion]], \n* [[Lilliefors test]],\n* [[Kolmogorov–Smirnov test]], \n* [[Shapiro–Wilk test]], and \n* [[Pearson's chi-squared test]].\nA 2011 study concludes that Shapiro–Wilk has the best [[Statistical power|power]] for a given significance, followed closely by Anderson–Darling when comparing the Shapiro–Wilk, Kolmogorov–Smirnov, Lilliefors, and Anderson–Darling tests.<ref>{{cite journal |last=Razali |first=Nornadiah |last2=Wah |first2=Yap Bee |title=Power comparisons of Shapiro–Wilk, Kolmogorov–Smirnov, Lilliefors and Anderson–Darling tests |journal=Journal of Statistical Modeling and Analytics |year=2011 |volume=2 |issue=1 |pages=21–33 |url=http://instatmy.org.my/downloads/e-jurnal%202/3.pdf <!--|accessdate=5 June 2012--> |archiveurl=https://web.archive.org/web/20150630110326/http://instatmy.org.my/downloads/e-jurnal%202/3.pdf |archivedate=2015-06-30 }}</ref>\n\nSome published works recommend the Jarque–Bera test,<ref>{{cite book |last=Judge |first=George G. |last2=Griffiths |first2=W. E. |last3=Hill |first3=R. Carter |last4=Lütkepohl |first4=Helmut |authorlink4=Helmut Lütkepohl |last5=Lee |first5=T. |year=1988 |title=Introduction to the Theory and Practice of Econometrics |edition=Second |pages=890–892 |publisher=Wiley |isbn=978-0-471-08277-4 |url=https://books.google.com/books?id=Iyy7AAAAIAAJ&pg=PA890 }}</ref><ref>{{cite book |last=Gujarati |first=Damodar N. |year=2002 |title=Basic Econometrics |edition=Fourth |pages=147–148 |publisher=McGraw Hill |isbn=978-0-07-123017-9 }}</ref> but the test has weakness. In particular, the test has low power for distributions with short tails, especially for bimodal distributions.<ref>{{cite journal|last=Thadewald|first=Thorsten|author2=Büning, Herbert|title=Jarque–Bera Test and its Competitors for Testing Normality – A Power Comparison|journal=Journal of Applied Statistics|date=1 January 2007|volume=34|issue=1|pages=87–105 |doi=10.1080/02664760600994539 |citeseerx=10.1.1.507.1186}}</ref> Some authors have declined to include its results in their studies because of its poor overall performance.<ref>{{cite journal |last=Sürücü |first=Barış |title=A power comparison and simulation study of goodness-of-fit tests |journal=Computers & Mathematics with Applications |date=1 September 2008 |volume=56 |issue=6 |pages=1617–1625 |doi=10.1016/j.camwa.2008.03.010 }}</ref>\n\nHistorically, the third and fourth [[standardized moment]]s ([[skewness]] and [[kurtosis]]) were some of the earliest tests for normality. The [[Lin-Mudholkar test]] specifically targets asymmetric alternatives.<ref>{{cite journal|last=Lin|first=C. C.|author2=Mudholkar, G. S. |title=A simple test for normality against asymmetric alternatives|journal=Biometrika |year=1980 |volume=67 |issue=2 |pages=455–461 |url=http://biomet.oxfordjournals.org/content/67/2/455.short |accessdate=15 Nov 2015 |doi=10.1093/biomet/67.2.455}}</ref> The [[Jarque–Bera test]] is itself derived from [[skewness]] and [[kurtosis]] estimates. [[Mardia's test|Mardia's multivariate skewness and kurtosis tests]] generalize the moment tests to the multivariate case.<ref>Mardia, K. V. (1970). Measures of multivariate skewness and kurtosis with applications. ''Biometrika'' 57, 519–530.</ref> Other early [[test statistic]]s include the ratio of the [[mean absolute deviation]] to the standard deviation and of the range to the standard deviation.<ref>{{cite journal |last=Filliben |first=J. J. |date=February 1975 |title = The Probability Plot Correlation Coefficient Test for Normality |journal = Technometrics |pages = 111–117 |doi = 10.2307/1268008 |volume = 17 |issue=1 |jstor=1268008 }}</ref>\n\nMore recent tests of normality include the energy test<ref>Székely, G. J. and Rizzo, M. L. (2005) A new test for multivariate normality, Journal of Multivariate Analysis 93, 58–80.</ref> (Székely and Rizzo) and the tests based on the [[empirical characteristic function]] (ECF) (e.g. Epps and Pulley,<ref>Epps, T. W., and Pulley, L. B. (1983). A test for normality based on the empirical characteristic function. ''Biometrika'' 70, 723–726.</ref> Henze–Zirkler,<ref>Henze, N., and Zirkler, B. (1990). A class of invariant and consistent tests for multivariate normality. ''Communications in Statistics - Theory and Methods'' 19, 3595–3617.</ref> [[BHEP test]]<ref>Henze, N., and Wagner, T. (1997). A new approach to the BHEP tests for multivariate normality. ''Journal of Multivariate Analysis'' 62, 1–23.</ref>). The energy and the ECF tests are powerful tests that apply for testing univariate or [[Multivariate normal distribution|multivariate normality]] and are statistically consistent against general alternatives.\n\nThe normal distribution has the [[Differential entropy|highest entropy]] of any distribution for a given standard deviation. There are a number of normality tests based on this property, the first attributable to Vasicek.<ref>{{cite journal |first=Oldrich |last=Vasicek |title=A Test for Normality Based on Sample Entropy |journal=Journal of the Royal Statistical Society |series=Series B (Methodological) |volume=38 |issue=1 |year=1976 |pages=54–59 |jstor=2984828 }}</ref>\n\n==Bayesian tests==\n[[Kullback–Leibler divergence]]s between the whole posterior distributions of the slope and variance do not indicate non-normality.  However, the ratio of expectations of these posteriors and the expectation of the ratios give similar results to the Shapiro–Wilk statistic except for very small samples, when non-informative priors are used.<ref>Young K. D. S. (1993), \"Bayesian diagnostics for checking assumptions of normality\". ''Journal of Statistical Computation and Simulation'', 47 (3–4),167–180</ref>\n\nSpiegelhalter suggests using a [[Bayes factor]] to compare normality with a different class of distributional alternatives.<ref>Spiegelhalter, D.J. (1980). An omnibus test for normality for small samples. Biometrika, 67, 493–496. {{doi|10.1093/biomet/67.2.493}}</ref>  This approach has been extended by Farrell and Rogers-Stewart.<ref>Farrell, P.J., Rogers-Stewart, K. (2006) \"Comprehensive study of tests for normality and symmetry: extending the Spiegelhalter test\". ''Journal of Statistical Computation and Simulation'', 76(9), 803 – 816. {{doi|10.1080/10629360500109023}}</ref>\n\n==Applications==\nOne application of normality tests is to the [[errors and residuals in statistics|residuals]] from a [[linear regression]] model. If they are not normally distributed, the residuals should not be used in Z tests or in any other tests derived from the normal distribution, such as [[t test]]s, [[F test]]s and [[chi-squared test]]s. If the residuals are not normally distributed, then the dependent variable or at least one [[explanatory variable]] may have the wrong functional form, or important variables may be missing, etc. Correcting one or more of these systematic errors may produce residuals that are normally distributed.{{Citation needed|reason=short of inline reference|date=April 2014}}\n\n==See also==\n* [[Randomness test]]\n\n==Notes==\n{{Reflist|30em}}\n\n==References==\n*{{Cite book|\n  editor  =      D'Agostino, R.B. |editor2=Stephens, M.A.|\n  year =         1986|\n  title =        Goodness-of-Fit Techniques|\n  chapter =      Tests for the Normal Distribution|\n  author =       Ralph B. D'Agostino|\n  publisher =    Marcel Dekker|\n  location =     New York|\n  isbn =         978-0-8247-7487-5}}\n\n*{{Cite book|author=Henry C. Thode, Jr.|title= Testing for Normality|publisher = Marcel Dekker, Inc.|location = New York |\nyear = 2002 | pages = 479|isbn = 978-0-8247-9613-6}}\n\n\n{{Statistics |inference}}\n\n[[Category:Parametric statistics]]\n[[Category:Normality tests| ]]\n[[Category:Statistical tests]]"
    },
    {
      "title": "Anderson–Darling test",
      "url": "https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test",
      "text": "The '''Anderson–Darling test''' is a [[Hypothesis testing|statistical test]] of whether a given sample of data is drawn from a given [[probability distribution]]. In its basic form, the test assumes that there are no parameters to be estimated in the distribution being tested, in which case the test and its set of [[critical values]] is distribution-free. However, the test is most often used in contexts where a family of distributions is being tested, in which case the parameters of that family need to be estimated and account must be taken of this in adjusting either the test-statistic or its critical values.  When applied to testing whether a [[normal distribution]] adequately describes a set of data, it is one of the most powerful statistical tools for detecting most departures from [[normal distribution|normality]].<ref name=Stephens74>\n{{Cite journal | first = M. A. | last = Stephens |year = 1974 | title = EDF Statistics for Goodness of Fit and Some Comparisons\n | journal = Journal of the American Statistical Association\n | volume = 69 | issue = | pages = 730–737 | id = | url =\n | doi = 10.2307/2286009}}</ref><ref name=Stephens86>\n{{Cite book|editor1=D'Agostino, R. B. |editor2=Stephens, M. A. |\n  year =         1986| title =        Goodness-of-Fit Techniques|\n  chapter =      Tests Based on EDF Statistics|  author =       M. A. Stephens|\n  publisher =    Marcel Dekker|  location =     New York|  isbn =         0-8247-7487-6}}</ref>\n'''''K''-sample Anderson–Darling tests''' are available for testing whether several collections of observations can be modelled as coming from a single population, where the [[cumulative distribution function|distribution function]] does not have to be specified.\n\nIn addition to its use as a test of fit for distributions, it can be used in parameter estimation as the basis for a form of [[minimum distance estimation]] procedure.\n\nThe test is named after [[Theodore Wilbur Anderson]] (1918–2016) and [[Donald Allan Darling|Donald A. Darling]] (1915–2014), who invented it in 1952.<ref>{{cite journal | first = T. W. | last = Anderson | authorlink = Theodore W. Anderson\n | author2 = [[Donald Allan Darling|Darling, D. A.]]\n | year = 1952 | month =\n | title = Asymptotic theory of certain \"goodness-of-fit\" criteria based on stochastic processes\n | journal = Annals of Mathematical Statistics\n | volume = 23 | issue = | pages = 193–212\n | id = | url =\n | doi = 10.1214/aoms/1177729437\n }}\n</ref>\n\n==The single-sample test==\n\nThe Anderson–Darling and [[Cramér–von Mises criterion|Cramér–von Mises statistics]] belong to the class of\nquadratic [[Empirical distribution function|EDF]] statistics (tests based on the [[empirical distribution function]]).<ref name=Stephens86/> If the hypothesized distribution is <math>F</math>, and empirical (sample) cumulative distribution function is <math>F_n</math>, then the quadratic EDF statistics measure the distance between <math>F</math> and <math>F_n</math> by\n:<math>\n n \\int_{-\\infty}^\\infty (F_n(x) - F(x))^2\\,w(x)\\,dF(x),\n</math>\nwhere <math>w(x)</math> is a weighting function.  When the weighting function is <math>w(x)=1</math>, the statistic\nis the [[Cramér–von Mises criterion|Cramér–von Mises statistic]].  The Anderson–Darling (1954) test<ref name=AD54>\n{{Cite journal|author1=Anderson, T.W.  |author2=Darling, D.A.|title = A Test of Goodness-of-Fit| journal = Journal of the American Statistical Association| year = 1954| volume = 49|pages = 765–769| doi=10.2307/2281537}}</ref> is based on the distance\n:<math>\nA^2 = n \\int_{-\\infty}^\\infty  \\frac{(F_n(x) - F(x))^2}{F(x)\\; (1-F(x))} \\, dF(x),\n</math>\nwhich is obtained when the weight function is <math>w(x)=[F(x)\\; (1-F(x))]^{-1}</math>. Thus, compared with the [[Cramér–von Mises criterion|Cramér–von Mises distance]], the Anderson–Darling distance places more weight on observations in the tails of the distribution.\n\n===Basic test statistic===\n\nThe Anderson–Darling test assesses whether a [[Sample (statistics)|sample]] comes from a specified distribution.  It makes use of the fact that, when given a hypothesized underlying distribution and assuming the data does arise from this distribution, the [[cumulative distribution function]] (CDF) of the data can be assumed to follow a [[Uniform distribution (continuous)|uniform distribution]]. The data can be then tested for uniformity with a distance test (Shapiro 1980). The formula for the [[test statistic]] <math>A</math> to assess if data <math>\\{Y_1<\\cdots <Y_n\\}</math> (note that the data must be put in order) comes from a CDF <math>F</math> is\n\n: <math>A^2 = -n-S \\,,</math>\n\nwhere\n\n: <math>S=\\sum_{i=1}^n \\frac{2i-1}{n}\\left[\\ln( F(Y_i)) + \\ln\\left(1-F(Y_{n+1-i})\\right)\\right].</math>\n\nThe test statistic can then be compared against the critical values of the theoretical distribution. Note that in this case no parameters are estimated in relation to the cumulative distribution function <math>F</math>.\n\n==Tests for families of distributions==\n\nEssentially the same test statistic can be used in the test of fit of a family of distributions, but then it must be compared against the critical values appropriate to that family of theoretical distributions and dependent also on the method used for parameter estimation.\n\n===Test for normality===\n\nEmpirical testing has found<ref>{{cite journal|last=Razali |first=Nornadiah |author2=Wah, Yap Bee |title=Power comparisons of Shapiro–Wilk, Kolmogorov–Smirnov, Lilliefors and Anderson–Darling tests |journal=Journal of Statistical Modeling and Analytics |year=2011 |volume=2 |issue=1 |pages=21–33 |url=http://instatmy.org.my/downloads/e-jurnal%202/3.pdf |accessdate=5 June 2012 |deadurl=yes |archiveurl=https://web.archive.org/web/20150630110326/http://instatmy.org.my/downloads/e-jurnal%202/3.pdf |archivedate=30 June 2015 |df= }}</ref> that the Anderson–Darling test is not quite as good as [[Shapiro-Wilk]], but is better than other tests.  Stephens<ref name=Stephens74/> found <math>A^2</math> to be one of the best [[empirical distribution function]] statistics for detecting most departures from normality.\n\nThe computation differs based on what is known about the distribution:<ref name=RBD86>{{Cite book|editor1=D'Agostino, R.B. |editor2=Stephens, M.A. |\n  year =         1986|\n  title =        Goodness-of-Fit Techniques|\n  chapter =      Tests for the Normal Distribution|\n  author =       Ralph B. D'Agostino|\n  publisher =    Marcel Dekker|\n  location =     New York|\n  isbn =         0-8247-7487-6\n}}</ref>\n\n* Case 0: The mean <math>\\mu</math> and the variance <math>\\sigma^2</math> are both known.\n* Case 1: The variance <math>\\sigma^2</math> is known, but the mean <math>\\mu</math> is unknown.\n* Case 2: The mean <math>\\mu</math> is known, but the variance <math>\\sigma^2</math> is unknown.\n* Case 3: Both the mean <math>\\mu</math> and the variance <math>\\sigma^2</math> are unknown.\n\nThe ''n'' observations, <math>X_i</math>, for <math>i=1,\\ldots n</math>, of the variable <math>X</math> must be sorted such that <math>X_1\\leq X_2\\leq ... \\leq X_n</math> and the notation in the following assumes that ''X<sub>i</sub>'' represent the ordered observations. Let\n:<math>\n\\hat{\\mu} =\n\\begin{cases}\n\\mu, & \\text{if the mean is known.} \\\\\n\\bar{X}, = \\frac{1}{n} \\sum_{i = 1}^n X_i & \\text{otherwise.}\n\\end{cases}\n</math>\n\n:<math>\n\\hat{\\sigma}^2 =\n\\begin{cases}\n\\sigma^2, & \\text{if the variance is known.} \\\\\n\\frac{1}{n} \\sum_{i = 1}^n (X_i - \\mu)^2, & \\text{if the variance is not known, but the mean is.} \\\\\n\\frac{1}{n - 1} \\sum_{i = 1}^n (X_i - \\bar{X})^2, & \\text{otherwise.}\n\\end{cases}\n</math>\nThe values <math>X_i</math> are standardized to create new values <math>Y_i</math>, given by\n:<math>Y_i=\\frac{X_i-\\hat{\\mu}}{\\hat{\\sigma}}.</math>\nWith the standard normal CDF <math>\\Phi</math>, <math>A^2</math> is calculated using\n:<math>A^2 = -n -\\frac{1}{n} \\sum_{i=1}^n (2i-1)(\\ln \\Phi(Y_i)+ \\ln(1-\\Phi(Y_{n+1-i}))).</math>\nAn alternative expression in which only a single observation is dealt with at each step of the summation is:\n:<math>A^2 = -n -\\frac{1}{n} \\sum_{i=1}^n\\left[(2i-1)\\ln\\Phi(Y_i)+(2(n-i)+1)\\ln(1-\\Phi(Y_i))\\right].</math>\nA modified statistic can be calculated using\n:<math>\nA^{*2} = \n\\begin{cases}\nA^2\\left(1+\\frac{4}{n}-\\frac{25}{n^2}\\right), & \\text{if the variance and the mean are both unknown.} \\\\\nA^2, & \\text{otherwise.}\n\\end{cases}\n</math>\n\nIf <math>A^{2}</math> or <math>A^{*2}</math> exceeds a given critical value, then the hypothesis of normality is rejected with\nsome significance level. The critical values  are given in the table below for values of <math>A^{2}</math>.<ref name=Stephens74/>\n<ref name=Marsaglia04>\n{{Cite journal | first = G.| last = Marsaglia|year = 2004 | title = Evaluating the Anderson-Darling Distribution\n | journal = Journal of Statistical Software\n | volume = 9 | issue = 2 | pages = 730–737 | id = | url = }}</ref>\n\nNote 1: If <math>\\hat{\\sigma}</math> = 0 or any <math>\\Phi(Y_i)=</math>(0 or 1) then <math>A^2</math> cannot be calculated and is undefined.\n\nNote 2: The above adjustment formula is taken from Shorak & Wellner (1986, p239). Care is required in comparisons across different sources as often the specific adjustment formula is not stated.\n\nNote 3: Stephens<ref name=Stephens74/> notes that the test becomes better when the parameters are computed from the data, even if they are known.\n\nNote 4: Marsaglia & Marsaglia<ref name=Marsaglia04/> provide a more accurate result for Case 0 at 85% and 99%.\n\n{| class=\"wikitable\"\n|-\n! Case !! n                   !! 15%   !! 10%   !! 5%    !! 2.5%  !! 1%\n|-\n| 0    || <math>\\geq 5</math> || 1.621 || 1.933 || 2.492 || 3.070 || 3.878 \n|-\n| 1    ||                     ||       || 0.908 || 1.105 || 1.304 || 1.573\n|-\n| 2    || <math>\\geq 5</math> ||       || 1.760 || 2.323 || 2.904 || 3.690\n|-\n| 3    || 10                  || 0.514 || 0.578 || 0.683 || 0.779 || 0.926\n|-\n|      || 20                  || 0.528 || 0.591 || 0.704 || 0.815 || 0.969\n|-\n|      || 50                  || 0.546 || 0.616 || 0.735 || 0.861 || 1.021\n|-\n|      || 100                 || 0.559 || 0.631 || 0.754 || 0.884 || 1.047\n|-\n|      || <math>\\infty</math> || 0.576 || 0.656 || 0.787 || 0.918 || 1.092\n|}\n\nAlternatively, for case 3 above (both mean and variance unknown), D'Agostino (1986) <ref name=RBD86/> in Table 4.7 on p.&nbsp;123 and on pages 372–373 gives the adjusted statistic:\n\n:<math>A^{*2}=A^2\\left(1+\\frac{0.75}{n}+\\frac{2.25}{n^2}\\right) .</math>\n\nand normality is rejected if <math>A^{*2}</math> exceeds 0.631, 0.752, 0.873, 1.035, or 1.159 at 10%, 5%, 2.5%, 1%, and 0.5% significance levels, respectively; the procedure is valid for sample size at least n=8. The formulas for computing the [[p-value|''p''-values]] for other values of <math>A^{*2}</math>  are given in Table 4.9 on p.&nbsp;127 in the same book.\n\n===Tests for other distributions===\n\nAbove, it was assumed that the variable <math>X_i</math> was being tested for normal distribution. Any other family of distributions can be tested but the test for each family is implemented by using a different modification of the basic test statistic and this is referred to critical values specific to that family of distributions. The modifications of the statistic and tables of critical values are given by Stephens (1986)<ref name=Stephens86/> for the exponential, extreme-value, Weibull, gamma, logistic, Cauchy, and von Mises distributions. Tests for the (two-parameter) [[log-normal distribution]] can be implemented by transforming the data using a logarithm and using the above test for normality. Details for the required modifications to the test statistic and for the critical values for  the [[normal distribution]] and the [[exponential distribution]] have been published by Pearson & Hartley (1972, Table 54). Details for these distributions, with the addition of the [[Gumbel distribution]], are also given by Shorak & Wellner (1986, p239). Details for the [[logistic distribution]] are given by Stephens (1979). A test for the (two parameter) [[Weibull distribution]] can be obtained by making use of the fact that the logarithm of a Weibull variate has a [[Gumbel distribution]].\n\n==Non-parametric ''k''-sample tests==\nFritz Scholz and Michael A. Stephens (1987) discuss a test, based on the Anderson–Darling measure of agreement between distributions, for whether a number of random samples with possibly different sample sizes may have arisen from the same distribution, where this distribution is unspecified.<ref>{{cite journal |last=Scholz |first=F. W. |last2=Stephens |first2=M. A. |year=1987 |title=K-sample Anderson–Darling Tests |journal=[[Journal of the American Statistical Association]] |volume=82 |issue=399 |pages=918–924 |doi=10.1080/01621459.1987.10478517 }}</ref> The [[R (programming language)|R package]] kSamples implements this rank test for comparing k samples among several other such rank tests.<ref>{{cite web |title=kSamples: K-Sample Rank Tests and their Combinations |work=R Project |url=https://cran.r-project.org/web/packages/kSamples/index.html }}</ref>\n\n==See also==\n*[[Kolmogorov–Smirnov test]]\n*[[Kuiper's test]]\n*[[Shapiro–Wilk test]]\n*[[Jarque–Bera test]]\n*[[Goodness of fit]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n* Corder, G.W., Foreman, D.I. (2009).''Nonparametric Statistics for Non-Statisticians: A Step-by-Step Approach'' Wiley, {{ISBN|978-0-470-45461-9}}\n* Mehta, S. (2014) ''Statistics Topics'' {{ISBN|978-1499273533}}\n* Pearson E.S., Hartley, H.O. (Editors) (1972) ''Biometrika Tables for Statisticians'', Volume II.  CUP. {{ISBN|0-521-06937-8}}.\n* Shapiro, S.S. (1980) How to test normality and other distributional assumptions. In: The ASQC basic references in quality control: statistical techniques 3, pp.&nbsp;1–78.\n* Shorack, G.R., Wellner, J.A. (1986) ''Empirical Processes with Applications to Statistics'', Wiley. {{ISBN|0-471-86725-X}}.\n* Stephens, M.A. (1979) ''Test of fit for the logistic distribution based on the empirical distribution function'', Biometrika, 66(3), 591–5.\n\n==External links==\n*[http://www.itl.nist.gov/div898/handbook/eda/section3/eda35e.htm US NIST Handbook of Statistics]\n\n{{DEFAULTSORT:Anderson-Darling test}}\n[[Category:Statistical tests]]\n[[Category:Nonparametric statistics]]\n[[Category:Normality tests]]"
    },
    {
      "title": "Cramér–von Mises criterion",
      "url": "https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93von_Mises_criterion",
      "text": "In [[statistics]] the '''Cramér–von Mises criterion''' is a criterion used for judging the [[goodness of fit]] of a [[cumulative distribution function]] <math>F^*</math> compared to a given [[empirical distribution function]] <math>F_n</math>, or for comparing two empirical distributions.  It is also used as a part of other algorithms, such as [[minimum distance estimation]].  It is defined as\n\n:<math>\\omega^2 = \\int_{-\\infty}^{\\infty} [F_n(x)-F^*(x)]^2\\,\\mathrm{d}F^*(x)</math>\n\nIn one-sample applications <math>F^*</math> is the theoretical distribution and <math>F_n</math> is the [[empirical distribution function|empirically observed distribution]].  Alternatively the two distributions can both be empirically estimated ones; this is called the two-sample case.\n\nThe criterion is named after [[Harald Cramér]] and [[Richard Edler von Mises]] who first proposed it in 1928–1930.<ref>{{cite journal |first=H. |last=Cramér |title=On the Composition of Elementary Errors |journal=Scandinavian Actuarial Journal |volume=1928 |issue=1 |pages=13–74 |year=1928 |doi=10.1080/03461238.1928.10416862 }}</ref><ref>{{cite book |first=R. E. |last=von Mises |title=Wahrscheinlichkeit, Statistik und Wahrheit |location= |publisher=Julius Springer |year=1928 |url= }}</ref> The generalization to two samples is due to [[Theodore Wilbur Anderson|Anderson]].<ref name=anderson>{{cite journal\n|last= Anderson\n|first= T. W.\n|authorlink = Theodore Wilbur Anderson\n|year= 1962\n|title= On the Distribution of the Two-Sample Cramer–von Mises Criterion\n|journal= [[Annals of Mathematical Statistics]]\n|volume= 33\n|issue= 3\n|pages= 1148–1159\n|publisher= [[Institute of Mathematical Statistics]]\n|issn= 0003-4851\n|doi= 10.1214/aoms/1177704477\n|url= http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&id=pdf_1&handle=euclid.aoms/1177704477\n|format= PDF\n|accessdate= June 12, 2009\n}}</ref>\n\nThe Cramér–von Mises test is an alternative to the [[Kolmogorov–Smirnov test]].\n\n==Cramér–von Mises test (one sample)==\n\nLet <math>x_1,x_2,\\cdots,x_n</math> be the observed values, in increasing order.  Then the statistic is<ref name=anderson/>{{rp|1153}}<ref name=PH1>[[Egon Pearson|Pearson, E.S.]], Hartley, H.O. (1972) ''Biometrika Tables for Statisticians, Volume 2'', CUP. {{ISBN|0-521-06937-8}} (page 118 and Table 54)</ref>\n\n:<math>T = n\\omega^2 = \\frac{1}{12n} + \\sum_{i=1}^n \\left[ \\frac{2i-1}{2n}-F(x_i) \\right]^2. </math>\n\nIf this value is larger than the tabulated value, then the hypothesis that the data came from the distribution <math>F</math> can be rejected.\n\n===Watson test===\n\nA modified version of the Cramér–von Mises test is the Watson test<ref name=W1>Watson, G.S. (1961) \"Goodness-Of-Fit Tests on a Circle\", ''[[Biometrika]]'', 48 (1/2), 109-114 {{jstor|2333135}}</ref> which uses the statistic ''U''<sup>2</sup>, where<ref name=\"PH1\"/>\n\n:<math>U^2= T-n( \\bar{F}-\\tfrac{1}{2} )^2,</math>\n\nwhere\n:<math>\\bar{F}=\\frac{1}{n} \\sum_{i=1}^n F(x_i).</math>\n\n==Cramér–von Mises test (two samples)==\n\nLet <math>x_1,x_2,\\cdots,x_N</math> and <math>y_1,y_2,\\cdots,y_M</math> be the observed values in the first and second sample respectively, in increasing order.  Let <math>r_1,r_2,\\cdots,r_N</math> be the ranks of the x's in the combined sample, and let <math>s_1,s_2,\\cdots,s_M</math> be the ranks of the y's in the combined sample. Anderson<ref name=anderson/>{{rp|1149}} shows that\n\n:<math>T = \\frac{NM}{N+M} \\omega^2 = \\frac{U}{N M (N+M)}-\\frac{4 M N - 1}{6(M+N)} </math>\n\nwhere U is defined as\n\n:<math>U = N \\sum_{i=1}^N (r_i-i)^2 + M \\sum_{j=1}^M (s_j-j)^2 </math>\n\nIf the value of T is larger than the tabulated values,<ref name=anderson/>{{rp|1154–1159}} the hypothesis that the two samples come from the same distribution can be rejected.  (Some books{{Specify|date=December 2008}} give critical values for U, which is more convenient, as it avoids the need to compute T via the expression above.  The conclusion will be the same).\n\nThe above assumes there are no duplicates in the <math>x</math>, <math>y</math>, and <math>r</math> sequences.  So <math>x_i</math> is unique, and its rank is <math>i</math> in the sorted list <math>x_1,...x_N</math>.  If there are  duplicates, and <math>x_i</math> through <math>x_j</math> are a run of identical values in the sorted list, then one common approach is the ''midrank''<ref>Ruymgaart, F. H., (1980) \"A unified approach to the asymptotic distribution theory of certain midrank statistics\". In: ''Statistique non Parametrique Asymptotique'', 1±18, J. P. Raoult (Ed.), Lecture Notes on Mathematics, No. 821, Springer, Berlin.</ref> method: assign each duplicate a \"rank\" of <math>(i+j)/2</math>.  In the above equations, in the expressions <math>(r_i-i)^2</math> and <math>(s_j-j)^2</math>, duplicates can modify all four variables <math>r_i</math>,   <math>i</math>,   <math>s_j</math>,  and <math>j</math>.\n\n==References==\n{{Reflist}}\n*{{Cite book|editor1=D'Agostino, R.B. |editor2=Stephens, M.A. |\n  year =         1986|\n  title =        Goodness-of-Fit Techniques|\n  chapter =      Tests Based on EDF Statistics|\n  author =       M. A. Stephens|\n  publisher =    Marcel Dekker|\n  location =     New York|\n  isbn =         0-8247-7487-6}}\n\n==Further reading==\n\n* {{cite journal\n| last        = Xiao\n| first       = Y.\n|author2=A. Gordon |author3=A. Yakovlev\n |date=January 2007\n| title       = A C++ Program for the Cramér–von Mises Two-Sample Test\n| journal     = [[Journal of Statistical Software]]\n| volume      = 17\n| issue       = 8\n| publisher   = [[American Statistical Association]]\n| issn        = 1548-7660\n| oclc        = 42456366\n| url         = http://www.jstatsoft.org/v17/i08/paper\n| format      = PDF\n| accessdate  = June 12, 2009\n}}\n\n==External links==\n* [http://finzi.psych.upenn.edu/R/library/CvM2SL2Test/html/00Index.html C-vM Two Sample Test] (Documentation for performing the test using [[R (programming language)|R]])\n\n{{DEFAULTSORT:Cramer-von Mises criterion}}\n[[Category:Statistical tests]]\n[[Category:Statistical distance]]\n[[Category:Nonparametric statistics]]\n[[Category:Normality tests]]"
    },
    {
      "title": "D'Agostino's K-squared test",
      "url": "https://en.wikipedia.org/wiki/D%27Agostino%27s_K-squared_test",
      "text": "In [[statistics]], '''D’Agostino’s ''K''<sup>2</sup> test''', named for [[Ralph D'Agostino (statistician)|Ralph D'Agostino]], is a [[goodness-of-fit]] measure of departure from [[normal distribution|normality]], that is the test aims to establish whether or not the given sample comes from a normally distributed population. The test is based on transformations of the sample [[kurtosis]] and [[skewness]], and has power only against the alternatives that the distribution is skewed and/or kurtic.\n\n== Skewness and kurtosis ==\nIn the following, { ''x<sub>i</sub>'' } denotes a sample of ''n'' observations, ''g''<sub>1</sub> and ''g''<sub>2</sub> are the sample [[skewness]] and [[kurtosis]], ''m<sub>j</sub>''’s are the ''j''-th sample [[central moment]]s, and <math style=\"position:relative;top:-.3em\">\\bar{x}</math> is the sample [[mean]]. Frequently in the literature related to [[normality tests|normality testing]], the skewness and kurtosis are denoted as {{radic|''β''<sub>1</sub>}} and ''β''<sub>2</sub> respectively. Such notation can be inconvenient since, for example, {{radic|''β''<sub>1</sub>}} can be a negative quantity.\n\nThe sample skewness and kurtosis are defined as\n: <math>\\begin{align}\n    & g_1 = \\frac{ m_3 }{ m_2^{3/2} } = \\frac{\\frac{1}{n} \\sum_{i=1}^n \\left( x_i - \\bar{x} \\right)^3}{\\left( \\frac{1}{n} \\sum_{i=1}^n \\left( x_i - \\bar{x} \\right)^2 \\right)^{3/2}}\\ , \\\\\n    & g_2 = \\frac{ m_4 }{ m_2^{2} }-3 = \\frac{\\frac{1}{n} \\sum_{i=1}^n \\left( x_i - \\bar{x} \\right)^4}{\\left( \\frac{1}{n} \\sum_{i=1}^n \\left( x_i - \\bar{x} \\right)^2 \\right)^2} - 3\\ .\n  \\end{align}</math>\n\nThese quantities [[consistent estimator|consistently]] estimate the theoretical skewness and kurtosis of the distribution, respectively. Moreover, if the sample indeed comes from a normal population, then the exact finite sample distributions of the skewness and kurtosis can themselves be analysed in terms of their means ''μ''<sub>1</sub>, variances ''μ''<sub>2</sub>, skewnesses ''γ''<sub>1</sub>, and kurtoses ''γ''<sub>2</sub>. This has been done by {{harvtxt|Pearson|1931}}, who derived the following expressions:{{better source|reason=need more accessible source so that quoted expression can be checked|date=November 2010}}\n\n: <math>\\begin{align}\n    & \\mu_1(g_1) = 0, \\\\\n    & \\mu_2(g_1) = \\frac{ 6(n-2) }{ (n+1)(n+3) }, \\\\\n    & \\gamma_1(g_1) \\equiv \\frac{\\mu_3(g_1)}{\\mu_2(g_1)^{3/2}} = 0, \\\\\n    & \\gamma_2(g_1) \\equiv \\frac{\\mu_4(g_1)}{\\mu_2(g_1)^{2}}-3 = \\frac{ 36(n-7)(n^2+2n-5) }{ (n-2)(n+5)(n+7)(n+9) }.\n  \\end{align}</math>\nand\n: <math>\\begin{align}\n    & \\mu_1(g_2) = - \\frac{6}{n+1}, \\\\\n    & \\mu_2(g_2) = \\frac{ 24n(n-2)(n-3) }{ (n+1)^2(n+3)(n+5) }, \\\\\n    & \\gamma_1(g_2) \\equiv \\frac{\\mu_3(g_2)}{\\mu_2(g_2)^{3/2}} = \\frac{6(n^2-5n+2)}{(n+7)(n+9)} \\sqrt{\\frac{6(n+3)(n+5)}{n(n-2)(n-3)}}, \\\\\n    & \\gamma_2(g_2) \\equiv \\frac{\\mu_4(g_2)}{\\mu_2(g_2)^{2}}-3 = \\frac{ 36(15n^6-36n^5-628n^4+982n^3+5777n^2-6402n+900) }{ n(n-3)(n-2)(n+7)(n+9)(n+11)(n+13) }.\n  \\end{align}</math>\nFor example, a sample with size {{nowrap|''n'' {{=}} 1000}} drawn from a normally distributed population can be expected to have a skewness of {{nowrap|0, SD 0.08}} and a kurtosis of {{nowrap|0, SD 0.15}}, where SD indicates the standard deviation.{{citation needed|date=January 2012}}\n\n== Transformed sample skewness and kurtosis ==\nThe sample skewness ''g''<sub>1</sub> and kurtosis ''g''<sub>2</sub> are both asymptotically normal. However, the rate of their convergence to the distribution limit is frustratingly slow, especially for ''g''<sub>2</sub>. For example even with {{nowrap|1=''n'' = 5000}} observations the sample kurtosis ''g''<sub>2</sub> has both the skewness and the kurtosis of approximately 0.3, which is not negligible. In order to remedy this situation, it has been suggested to transform the quantities ''g''<sub>1</sub> and ''g''<sub>2</sub> in a way that makes their distribution as close to standard normal as possible.\n\nIn particular, {{harvtxt|D’Agostino|1970}} suggested the following transformation for sample skewness:\n: <math>\n    Z_1(g_1) = \\delta \\operatorname{asinh}\\left( \\frac{g_1}{\\alpha\\sqrt{\\mu_2}} \\right),\n  </math>\nwhere constants ''α'' and ''δ'' are computed as\n: <math>\\begin{align}\n    & W^2 = \\sqrt{2\\gamma_2 + 4} - 1, \\\\\n    & \\delta = 1 / \\sqrt{\\ln W}, \\\\\n    & \\alpha^2 = 2 / (W^2-1),\n  \\end{align}</math>\nand where ''μ''<sub>2</sub> = ''μ''<sub>2</sub>(''g''<sub>1</sub>) is the variance of ''g''<sub>1</sub>, and ''γ''<sub>2</sub> = ''γ''<sub>2</sub>(''g''<sub>1</sub>) is the kurtosis — the expressions given in the previous section.\n\nSimilarly, {{harvtxt|Anscombe|Glynn|1983}} suggested a transformation for ''g''<sub>2</sub>, which works reasonably well for sample sizes of 20 or greater:\n: <math>\n    Z_2(g_2) = \\sqrt{\\frac{9A}{2}} \\left\\{1 - \\frac{2}{9A} - \\left(\\frac{ 1-2/A }{ 1+\\frac{g_2-\\mu_1}{\\sqrt{\\mu_2}}\\sqrt{2/(A-4)} }\\right)^{\\!1/3}\\right\\},\n  </math>\nwhere\n: <math>\n    A = 6 + \\frac{8}{\\gamma_1} \\left( \\frac{2}{\\gamma_1} + \\sqrt{1+4/\\gamma_1^2}\\right),\n  </math>\nand ''μ''<sub>1</sub> = ''μ''<sub>1</sub>(''g''<sub>2</sub>), ''μ''<sub>2</sub> = ''μ''<sub>2</sub>(''g''<sub>2</sub>), ''γ''<sub>1</sub> = ''γ''<sub>1</sub>(''g''<sub>2</sub>) are the quantities computed by Pearson.\n\n== Omnibus ''K''<sup>2</sup> statistic ==\nStatistics ''Z''<sub>1</sub> and ''Z''<sub>2</sub> can be combined to produce an omnibus test, able to detect deviations from normality due to either skewness or kurtosis {{harv|D’Agostino|Belanger|D’Agostino|1990}}:\n: <math>\n    K^2 = Z_1(g_1)^2 + Z_2(g_2)^2\\,\n  </math>\n\nIf the [[null hypothesis]] of normality is true, then ''K''<sup>2</sup> is approximately [[chi-squared distribution|''χ''<sup>2</sup>-distributed]] with 2 degrees of freedom.\n\nNote that the statistics ''g''<sub>1</sub>, ''g''<sub>2</sub> are not independent, only uncorrelated. Therefore, their transforms ''Z''<sub>1</sub>, ''Z''<sub>2</sub> will be dependent also {{harv|Shenton|Bowman|1977}}, rendering the validity of ''χ''<sup>2</sup> approximation questionable. Simulations show that under the null hypothesis the ''K''<sup>2</sup> test statistic is characterized by\n<!-- each experiment was based on 1,000,000 simulations -->\n{|class=\"wikitable\" style=\"text-align:right\"\n|-\n!\n! expected value\n! standard deviation\n! 95% quantile\n|-\n|style=\"text-align:left\"| ''n'' = 20\n| 1.971\n| 2.339\n| 6.373\n|-\n|style=\"text-align:left\"| ''n'' = 50\n| 2.017\n| 2.308\n| 6.339\n|-\n|style=\"text-align:left\"| ''n'' = 100\n| 2.026\n| 2.267\n| 6.271\n|-\n|style=\"text-align:left\"| ''n'' = 250\n| 2.012\n| 2.174\n| 6.129\n|-\n|style=\"text-align:left\"| ''n'' = 500\n| 2.009\n| 2.113\n| 6.063\n|-\n|style=\"text-align:left\"| ''n'' = 1000\n| 2.000\n| 2.062\n| 6.038\n|-\n| ''χ''<sup>2</sup>(2) distribution\n| 2.000\n| 2.000\n| 5.991\n|}\n\n== See also ==\n* [[Shapiro–Wilk test]]\n* [[Jarque–Bera test]]\n\n== References ==\n{{Refbegin}}\n* {{cite journal\n  | title = Distribution of the kurtosis statistic ''b''<sub>2</sub> for normal statistics\n  | first1 = F.J.\n  | last1 = Anscombe\n  | first2 = William J.\n  | last2 = Glynn\n  | year = 1983\n  | journal = [[Biometrika]]\n  | volume = 70\n  | issue = 1\n  | pages = 227–234\n  | jstor = 2335960\n  | ref = CITEREFAnscombeGlynn1983\n  | doi=10.1093/biomet/70.1.227\n  }}\n* {{cite journal\n  | title = Transformation to normality of the null distribution of ''g''<sub>1</sub>\n  | first = Ralph B.\n  | last = D’Agostino\n  | journal = [[Biometrika]]\n  | volume = 57\n  | issue = 3\n  | year = 1970\n  | pages = 679–681\n  | jstor = 2334794\n  | ref = CITEREFD.E2.80.99Agostino1970\n  | doi=10.1093/biomet/57.3.679\n  }}\n* {{cite journal\n  | title = A suggestion for using powerful and informative tests of normality\n  | author1 = D’Agostino, Ralph B.\n  | author2 = Albert Belanger\n  | author3 = Ralph B. D’Agostino, Jr\n  | journal = [[The American Statistician]]\n  | volume = 44\n  | issue = 4\n  | year = 1990\n  | pages = 316–321\n  | jstor = 2684359\n  | doi = 10.2307/2684359\n  | url = http://www.cee.mtu.edu/~vgriffis/CE%205620%20materials/CE5620%20Reading/DAgostino%20et%20al%20-%20normaility%20tests.pdf\n  | ref = CITEREFD.E2.80.99AgostinoBelangerD.E2.80.99Agostino1990\n  | deadurl = yes\n  | archiveurl = https://web.archive.org/web/20120325140006/http://www.cee.mtu.edu/~vgriffis/CE%205620%20materials/CE5620%20Reading/DAgostino%20et%20al%20-%20normaility%20tests.pdf\n  | archivedate = 2012-03-25\n  | df = \n  }}\n* {{cite journal\n  | last = Pearson | first = Egon S. | authorlink = Egon Pearson\n  | title = Note on tests for normality\n  | year = 1931\n  | journal = [[Biometrika]] | volume = 22 | issue = 3/4\n  | pages = 423–424\n  | ref = harv\n  | jstor = 2332104 | doi=10.1093/biomet/22.3-4.423\n  }}\n* {{cite journal\n  | title = A bivariate model for the distribution of  √b<sub>1</sub> and b<sub>2</sub>\n  | last1 = Shenton\n  | first1 = L.R.\n  | last2 = Bowman\n  | first2 = K.O. | author2-link = K. O. Bowman\n  | year = 1977\n  | journal = Journal of the American Statistical Association\n  | volume = 72\n  | issue = 357\n  | pages = 206–211\n  | ref = CITEREFShentonBowman1977\n  | jstor = 2286939\n  | doi=10.1080/01621459.1977.10479940\n  }}\n{{Refend}}\n\n{{Statistics |inference}}\n\n{{DEFAULTSORT:D'agostino'S K-Squared Test}}\n[[Category:Parametric statistics]]\n[[Category:Normality tests]]"
    },
    {
      "title": "Jarque–Bera test",
      "url": "https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test",
      "text": "In [[statistics]], the '''Jarque–Bera test''' is a [[goodness-of-fit]] test of whether sample data have the [[skewness]] and [[kurtosis]] matching a [[normal distribution]]. The test is named after [[Carlos Jarque]] and [[Anil K. Bera]].\nThe test statistic is always nonnegative. If it is far from zero, it signals the data do not have a normal distribution.\n\nThe [[test statistic]] ''JB'' is defined as\n\n: <math>\n    \\mathit{JB} = \\frac{n-k+1}{6} \\left( S^2 + \\frac14 (C-3)^2 \\right)\n  </math>\n\nwhere ''n'' is the number of observations (or degrees of freedom in general); ''S'' is the sample [[skewness]], ''C'' is the sample [[kurtosis]], and k is the number of regressors (being 1 outside a regression context):\n\n: <math>\n     S = \\frac{ \\hat{\\mu}_3 }{ \\hat{\\sigma}^3 }\n        = \\frac{\\frac1n \\sum_{i=1}^n (x_i-\\bar{x})^3} {\\left(\\frac1n \\sum_{i=1}^n (x_i-\\bar{x})^2 \\right)^{3/2}} ,\n</math>\n: <math>\n C = \\frac{ \\hat{\\mu}_4 }{ \\hat{\\sigma}^4 }\n    = \\frac{\\frac1n \\sum_{i=1}^n (x_i-\\bar{x})^4} {\\left(\\frac1n \\sum_{i=1}^n (x_i-\\bar{x})^2 \\right)^{2}} ,\n</math>\nwhere <math>\\hat{\\mu}_3</math> and <math>\\hat{\\mu}_4</math> are the estimates of third and fourth [[central moment]]s, respectively, <math>\\bar{x}</math> is the sample [[mean]], and\n<math>\\hat{\\sigma}^2</math> is the estimate of the second central moment, the [[variance]].\n\nIf the data comes from a normal distribution, the ''JB'' statistic [[asymptotic analysis|asymptotically]] has a [[chi-squared distribution]] with two [[degrees of freedom (statistics)|degrees of freedom]], so the statistic can be used to [[statistical hypothesis testing|test]] the hypothesis that the data are from a [[normal distribution]]. The [[null hypothesis]] is a joint hypothesis of the skewness being zero and the [[excess kurtosis]] being zero. Samples from a normal distribution have an expected skewness of 0 and an expected excess kurtosis of 0 (which is the same as a kurtosis of 3). As the definition of ''JB'' shows, any deviation from this increases the JB statistic.\n\nFor small samples the chi-squared approximation is overly sensitive, often rejecting the null hypothesis when it is true. Furthermore, the distribution of [[P-value|''p''-values]] departs from a [[uniform distribution (continuous)|uniform distribution]] and becomes a [[right-skewed]] [[unimodal distribution]], especially for small ''p''-values. This leads to a large [[Type I error]] rate. The table below shows some ''p''-values approximated by a chi-squared distribution that differ from their true alpha levels for small samples.\n\n:{| class=\"wikitable\"\n|+Calculated ''p''-values equivalents to true alpha levels at given sample sizes\n! True α level !! 20 !! 30 !! 50 !! 70 !! 100\n|-\n! 0.1\n| 0.307 || 0.252 || 0.201 || 0.183 || 0.1560\n|-\n! 0.05\n| 0.1461 || 0.109 || 0.079 || 0.067 || 0.062\n|-\n! 0.025\n| 0.051 || 0.0303 || 0.020 || 0.016 || 0.0168\n|-\n! 0.01\n| 0.0064 || 0.0033 || 0.0015 || 0.0012 || 0.002\n|}\n(These values have been approximated using [[Monte Carlo simulation]] in [[Matlab]])\n\nIn [[MATLAB]]'s implementation, the chi-squared approximation for the JB statistic's distribution is only used for large sample sizes (>&nbsp;2000). For smaller samples, it uses a table derived from [[Monte Carlo simulations]] in order to interpolate ''p''-values.<ref name=\"MathWorks\">{{cite web|url=http://www.mathworks.com/access/helpdesk/help/toolbox/stats/jbtest.html|title=Analysis of the JB-Test in MATLAB|publisher=MathWorks|accessdate=May 24, 2009}}</ref>\n\n==History==\nConsidering normal sampling, and {{radic|''β''<sub>1</sub>}} and ''β''<sub>2</sub> contours, {{harvtxt|Bowman|Shenton|1975}} noticed that the statistic ''JB'' will be asymptotically ''χ''<sup>2</sup>(2)-distributed; however they also noted that “large sample sizes would doubtless be required for the ''χ''<sup>2</sup> approximation to hold”. Bowman and Shelton did not study the properties any further, preferring [[D’Agostino’s K-squared test]].\n\n==Jarque–Bera test in regression analysis==\nAccording to Robert Hall, David Lilien, et al. (1995) when using this test along with multiple regression analysis the right estimate is:\n\n: <math>\n    \\mathit{JB} = \\frac{n-k}{6} \\left( S^2 + \\frac14 (C-3)^2 \\right)\n  </math>\n\nwhere ''n'' is the number of observations and ''k'' is the number of regressors when examining residuals to an equation.\n\n==Implementations==\n* [http://www.alglib.net/statistics/hypothesistesting/jarqueberatest.php ALGLIB] includes an implementation of the Jarque–Bera test in C++, C#, Delphi, Visual Basic, etc.\n* [[gretl]] includes an implementation of the Jarque–Bera test\n* [[R (programming language)|R]] includes implementations of the Jarque–Bera test: ''jarque.bera.test'' in the package ''tseries'',<ref>{{cite web |title=tseries: Time Series Analysis and Computational Finance |work=R Project |date= |url=https://cran.r-project.org/web/packages/tseries/index.html }}</ref> for example, and ''jarque.test'' in the package ''moments''.<ref>{{cite web |title=moments: Moments, cumulants, skewness, kurtosis and related tests |work=R Project |date= |url=https://cran.r-project.org/web/packages/moments/index.html }}</ref>\n* [[Matlab|MATLAB]] includes an implementation of the Jarque–Bera test, the function \"jbtest\".\n* [[Python (programming language)|Python]] [[statsmodels]] includes an implementation of the Jarque–Bera test, \"statsmodels.stats.stattools.py\".\n* [[Wolfram Mathematica|Wolfram]] includes a built in function called, JarqueBeraALMTest<ref>{{cite web|url=http://reference.wolfram.com/language/ref/JarqueBeraALMTest.html|title=JarqueBeraALMTest—Wolfram Language Documentation|website=reference.wolfram.com|language=en|access-date=2017-10-26}}</ref> and is not limited to testing against a Gaussian distribution.\n\n==References==\n{{reflist}}\n\n==Further reading==\n* {{cite journal\n  | first1 = K.O. | last1 = Bowman | author1-link = K. O. Bowman\n  | first2 = L.R. | last2 = Shenton\n  | title = Omnibus contours for departures from normality based on √''b''<sub>1</sub> and ''b''<sub>2</sub>\n  | year = 1975\n  | journal = Biometrika\n  | volume = 62 | issue = 2\n  | pages = 243–250\n  | jstor = 2335355\n  | ref = CITEREFBowmanShenton1975\n  | doi=10.1093/biomet/62.2.243\n  }}\n* {{cite journal\n  | first1 = Carlos M. | last1 = Jarque | authorlink1 = Carlos Jarque\n  | first2 = Anil K. | last2 = Bera\n  | title = Efficient tests for normality, homoscedasticity and serial independence of regression residuals\n  | year = 1980\n  | journal = Economics Letters\n  | volume = 6 | issue = 3\n  | pages = 255–259\n  | doi = 10.1016/0165-1765(80)90024-5\n  | ref = CITEREFJarqueBera1980\n  }}\n* {{cite journal\n  | first1 = Carlos M. | last1 = Jarque | authorlink1 = Carlos Jarque\n  | first2 = Anil K. | last2 = Bera\n  | title = Efficient tests for normality, homoscedasticity and serial independence of regression residuals: Monte Carlo evidence\n  | year = 1981\n  | journal = Economics Letters\n  | volume = 7 | issue = 4\n  | pages = 313–318\n  | doi = 10.1016/0165-1765(81)90035-5\n  | ref = CITEREFJarqueBera1981\n  }}\n* {{cite journal\n  | first1 = Carlos M. | last1 = Jarque | authorlink1 = Carlos Jarque\n  | first2 = Anil K. | last2 = Bera\n  | title = A test for normality of observations and regression residuals\n  | year = 1987\n  | journal = International Statistical Review\n  | volume = 55 | issue = 2\n  | pages = 163–172\n  | jstor = 1403192\n  | ref = CITEREFJarqueBera1987\n  }}\n* {{cite book\n  | last = Judge\n  | title = Introduction and the theory and practice of econometrics\n  | year = 1988\n  | edition = 3rd\n  | pages = 890–892\n  |display-authors=etal}}\n* {{cite book\n  | first1 = Robert E. | last1 = Hall\n  | first2 = David M. | last2 = Lilien\n\n  | title = EViews User Guide\n  | year = 1995\n  | page = 141\n  |display-authors=etal\n}}\n\n{{Statistics}}\n\n{{DEFAULTSORT:Jarque-Bera test}}\n[[Category:Normality tests]]"
    },
    {
      "title": "Kolmogorov–Smirnov test",
      "url": "https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test",
      "text": "[[File:KS Example.png|thumb|300px|Illustration of the Kolmogorov–Smirnov statistic. Red line is [[Cumulative distribution function|CDF]], blue line is an [[Empirical distribution function|ECDF]], and the black arrow is the K–S statistic.]]\n\nIn [[statistics]], the '''Kolmogorov–Smirnov test''' ('''K–S test''' or '''KS test''') is a [[nonparametric statistics|nonparametric test]] of the equality of continuous (or discontinuous, see [[#Discrete and mixed null distribution|Section 2.2]]), one-dimensional [[probability distribution]]s that can be used to compare a [[random sample|sample]] with a reference probability distribution (one-sample K–S test), or to compare two samples (two-sample K–S test).  It is named after  [[Andrey Kolmogorov]] and [[Nikolai Smirnov (mathematician)|Nikolai Smirnov]].\n\nThe Kolmogorov–Smirnov statistic quantifies a [[metric (mathematics)|distance]] between the [[empirical distribution function]] of the sample and the [[cumulative distribution function]] of the reference distribution, or between the empirical distribution functions of two samples. The [[null distribution]] of this statistic is calculated under the [[null hypothesis]] that the sample is drawn from the reference distribution (in the one-sample case) or that the samples are drawn from the same distribution (in the two-sample case). In the one-sample case, the distribution considered under the null hypothesis may be continuous (see [[#Kolmogorov distribution|Section 2]]), purely discrete or mixed (see [[#Discrete and mixed null distribution|Section 2.2]]). In the two-sample case (see [[#Two-sample Kolmogorov–Smirnov test|Section 3]]), the distribution considered under the null hypothesis is a continuous distribution but is otherwise unrestricted.\n\nThe two-sample K–S test is one of the most useful and general nonparametric methods for comparing two samples, as it is sensitive to differences in both location and shape of the empirical cumulative distribution functions of the two samples.\n\nThe Kolmogorov–Smirnov test can be modified to serve as a [[goodness of fit]] test. In the special case of testing for [[Normal distribution|normality]] of the distribution, samples are standardized and compared with a standard normal distribution. This is equivalent to setting the mean and variance of the reference distribution equal to the sample estimates, and it is known that using these to define the specific reference distribution changes the null distribution of the test statistic: see [[#Test with estimated parameters|below]]. Various studies have found that, even in this corrected form, the test is less powerful for testing normality than the [[Shapiro–Wilk test]] or [[Anderson–Darling test]].<ref>{{cite journal\n | first = M. A. | last = Stephens | year = 1974 | title = EDF Statistics for Goodness of Fit and Some Comparisons\n | journal = Journal of the American Statistical Association\n | volume = 69 | issue = 347| pages = 730–737 | jstor =2286009\n | doi = 10.2307/2286009\n }}</ref> However, these other tests have their own disadvantages. For instance the Shapiro–Wilk test is known not to work well in samples with many identical values.\n\n==Kolmogorov–Smirnov statistic==\nThe [[empirical distribution function]] ''F''<sub>''n''</sub> for ''n'' [[Independent and identically distributed random variables|iid]] ordered observations ''X<sub>i</sub>'' is defined as\n\n:<math>F_n(x)={1 \\over n}\\sum_{i=1}^n I_{[-\\infty,x]}(X_i)</math>\n\nwhere <math>I_{[-\\infty,x]}(X_i)</math> is the [[indicator function]], equal to 1 if <math>X_i \\le x</math> and equal to 0 otherwise.\n\nThe Kolmogorov–Smirnov [[statistic]] for a given [[cumulative distribution function]] ''F''(''x'') is\n\n:<math>D_n= \\sup_x |F_n(x)-F(x)|</math>\n\nwhere sup<sub>''x''</sub> is the [[supremum]] of the set of distances. By the [[Glivenko–Cantelli theorem]], if the sample comes from distribution ''F''(''x''), then ''D''<sub>''n''</sub> converges to 0 [[almost surely]] in the limit when <math>n</math> goes to infinity. Kolmogorov strengthened this result, by effectively providing the rate of this convergence (see below). [[Donsker's theorem]] provides yet a stronger result.\n\nIn practice, the statistic requires a relatively large number of data points (in comparison to other goodness of fit criteria such as the [[Anderson–Darling test]] statistic) to properly reject the null hypothesis.\n\n==Kolmogorov distribution==\nThe Kolmogorov distribution is the distribution of the [[random variable]]\n\n:<math>K=\\sup_{t\\in[0,1]}|B(t)|</math>\n\nwhere ''B''(''t'') is the [[Brownian bridge]]. The [[cumulative distribution function]] of ''K'' is given by<ref>{{Cite journal |vauthors=Marsaglia G, Tsang WW, Wang J |year=2003 |title=Evaluating Kolmogorov's Distribution |journal=Journal of Statistical Software |volume=8 |issue=18 |pages=1–4 |url=http://www.jstatsoft.org/v08/i18/paper|doi=10.18637/jss.v008.i18 }}</ref>\n\n:<math>\\operatorname{Pr}(K\\leq x)=1-2\\sum_{k=1}^\\infty (-1)^{k-1} e^{-2k^2 x^2}=\\frac{\\sqrt{2\\pi}}{x}\\sum_{k=1}^\\infty e^{-(2k-1)^2\\pi^2/(8x^2)},</math>\n\nwhich can also be expressed by the [[Jacobi theta function]] <math>\\vartheta_{01}(z=0;\\tau=2ix^2/\\pi)</math>. Both the form of the Kolmogorov–Smirnov test statistic and its asymptotic distribution under the null hypothesis were published by [[Andrey Kolmogorov]],<ref name=AK>{{Cite journal |author=Kolmogorov A |year=1933 |title=Sulla determinazione empirica di una legge di distribuzione |journal=G. Ist. Ital. Attuari |volume=4 |pages=83–91}}</ref> while a table of the distribution was published by [[Nikolai Smirnov (mathematician)|Nikolai Smirnov]].<ref>{{Cite journal |author=Smirnov N |year=1948 |title=Table for estimating the goodness of fit of empirical distributions |journal=[[Annals of Mathematical Statistics]] |volume=19 |issue=2 |pages=279–281 |doi=10.1214/aoms/1177730256}}</ref> Recurrence relations for the distribution of the test statistic in finite samples are available.<ref name=AK/>\n\nUnder null hypothesis that the sample comes from the hypothesized distribution ''F''(''x''),\n\n:<math>\\sqrt{n}D_n\\xrightarrow{n\\to\\infty}\\sup_t |B(F(t))|</math>\n\n[[convergence of random variables|in distribution]], where ''B''(''t'') is the [[Brownian bridge]].\n\nIf ''F'' is continuous then under the null hypothesis <math>\\sqrt{n}D_n</math> converges to the Kolmogorov distribution, which does not depend on ''F''. This result may also be known as the Kolmogorov theorem. The accuracy of this limit as an approximation to the exact cdf of <math>K</math> when <math>n</math> is finite is not very impressive: even when <math>n=1000</math>, the corresponding maximum error is about <math>0.9\\%</math>; this error increases to <math>2.6\\%</math> when <math>n=100</math> and to a totally unacceptable <math>7\\%</math> when <math>n=10</math>.  However, a very simple expedient of replacing <math>x</math> by \n\n:<math>x+\\frac{1}{6\\sqrt{n}}+ \\frac{x-1}{4n}</math>\n\nin the argument of the Jacobi theta function reduces these errors to \n<math>0.003\\%</math>, <math>0.027\\%</math>, and <math>0.27\\%</math> respectively; such accuracy would be usually considered more than adequate for all practical applications.<ref>{{Cite journal |vauthors=Vrbik, Jan |year=2018 |title=Small-Sample Corrections to  Kolmogorov–Smirnov Test Statistic |journal=Pioneer Journal of Theoretical and Applied Statistics |volume=15 |issue=1–2 |pages=15–23}}</ref>\n\nThe ''goodness-of-fit'' test or the Kolmogorov–Smirnov test can be constructed by using the critical values of the Kolmogorov distribution. This test is asymptotically valid when <math>n \\to\\infty</math>. It rejects the null hypothesis at level <math>\\alpha</math> if\n\n:<math>\\sqrt{n}D_n>K_\\alpha,\\,</math>\n\nwhere ''K''<sub>''α''</sub> is found from\n\n:<math>\\operatorname{Pr}(K\\leq K_\\alpha)=1-\\alpha.\\,</math>\n\nThe asymptotic [[statistical power|power]] of this test is 1.\n\nFast and accurate algorithms to compute the cdf <math>\\operatorname{Pr}(D_n \\leq x)</math> or its complement for arbitrary <math>n</math> and <math>x</math>, are available from:\n\n* <ref name=SL2011>{{Cite journal |vauthors=Simard R, L'Ecuyer P |year=2011 |title=Computing the Two-Sided Kolmogorov–Smirnov Distribution |journal=Journal of Statistical Software |volume=39 |issue=11 |pages=1–18 |url=http://www.jstatsoft.org/v39/i11/paper|doi=10.18637/jss.v039.i11 }}</ref> and <ref>{{Cite journal |vauthors=Moscovich A, Nadler B |year=2017 |title=Fast calculation of boundary crossing probabilities for Poisson processes |journal=Statistics and Probability Letters |volume=123 |issue= |pages=177–182 |url=https://doi.org/10.1016/j.spl.2016.11.027 }}</ref>  for continuous null distributions with code in C and Java to be found in <ref name=SL2011/>.\n\n* <ref name=DKT2019>{{Cite journal |vauthors=Dimitrova DS, Kaishev VK, Tan S |year=2019 |title=Computing the Kolmogorov–Smirnov Distribution when the Underlying cdf is Purely Discrete, Mixed or Continuous |journal=Journal of Statistical Software |volume=forthcoming |url=http://openaccess.city.ac.uk/18541/ }}</ref> for purely discrete, mixed or continuous null distribution implemented in the KSgeneral package <ref name=KSgeneral>{{Cite web|url=https://cran.r-project.org/web/packages/KSgeneral/index.html|title=KSgeneral: Computing P-Values of the K-S Test for (Dis)Continuous Null Distribution|last1=Dimitrova|first1=Dimitrina | last2=Kaishev| first2=Vladimir | last3=Tan|first3=Senren|date=|website=cran.r-project.org/web/packages/KSgeneral/index.html|publisher=|access-date=}}</ref> of the [[R (programming language)|R project for statistical computing]], which for a given sample also computes the KS test statistic and its p-value. Alternative C++ implementation is available from <ref name=DKT2019/>.\n\n===Test with estimated parameters===\n\nIf either the form or the parameters of ''F''(''x'') are determined from the data ''X''<sub>''i''</sub> the critical values determined in this way are invalid. In such cases, [[Monte Carlo method|Monte Carlo]] or other methods may be required, but tables have been prepared for some cases. Details for the required modifications to the test statistic and for the critical values for  the [[normal distribution]] and the [[exponential distribution]] have been published,<ref name=\"Pearson & Hartley\">{{cite book |title= Biometrika Tables for Statisticians |editors=Pearson, E. S. and Hartley, H. O. |year=1972 |volume=2 |publisher=Cambridge University Press |isbn=978-0-521-06937-3 |pages=117–123, Tables 54, 55}}</ref> and later publications also include the [[Gumbel distribution]].<ref name=\"Shorak & Wellner\">{{cite book |title=Empirical Processes with Applications to Statistics |first1=Galen R. |last1=Shorack |first2=Jon A. |last2=Wellner |year=1986 |isbn=978-0471867258 |publisher=Wiley |page=239}}</ref> The [[Lilliefors test]] represents a special case of this for the normal distribution. The logarithm transformation may help to overcome cases where the Kolmogorov test data does not seem to fit the assumption that it came from the normal distribution.\n\nUsing estimated parameters, the questions arises which estimation method should be used. Usually this would be the maximum likelihood method, but e.g. for the normal distribution MLE has a large bias error on sigma! Using a moment fit or KS minimization instead has a large impact on the critical values, and also some impact on test power. If we need to decide for Student-T data with df&nbsp;=&nbsp;2 via KS test whether the data could be normal or not, then a ML estimate based on H<sub>0</sub> (data is normal, so using the standard deviation for scale) would give much larger KS distance, than a fit with minimum KS. In this case we should reject H<sub>0</sub>, which is often the case with MLE, because the sample standard deviation might be very large for T-2 data, but with KS minimization we may get still a too low KS to reject&nbsp;H<sub>0</sub>. In the Student-T case, a modified KS test with KS estimate instead of MLE, makes the KS test indeed slightly worse. However, in other cases, such a modified KS test leads to slightly better test power.\n\n===Discrete and mixed null distribution===\n\nUnder the assumption that <math>F(x)</math> is non-decreasing and right-continuous, with countable (possibly infinite) number of jumps, the KS test statistic can be expressed as:\n\n:<math>D_n= \\sup_x |F_n(x)-F(x)| = \\sup_{0 \\leq t \\leq 1} |F_n(F^{-1}(t)) - F(F^{-1}(t))|. </math>\n\nFrom the right-continuity of <math>F(x)</math>, it follows that <math>F(F^{-1}(t)) \\geq t</math> and <math>F^{-1}(F(x)) \\leq x </math> and hence, the distribution of <math>D_{n}</math> depends on the null distribution <math>F(x)</math>, i.e., is no longer distribution-free as in the continuous case. Therefore, a fast and accurate method has been developed to compute the exact and asymptotic distribution of <math>D_{n}</math> when <math>F(x)</math> is purely discrete or mixed <ref name=DKT2019/>, implemented in C++ and in the KSgeneral package <ref name=KSgeneral/> of the [[R (programming language)|R project for statistical computing]]. The functions <code>disc_ks_test()</code>, <code>mixed_ks_test()</code> and <code>cont_ks_test()</code> compute also the KS test statistic and p-values for purely discrete, mixed or continuous null distributions and arbitrary sample sizes. The KS test and its p-values for discrete null distributions and small sample sizes are also computed in <ref name=arnold-emerson>{{Cite journal |first1=Taylor B. |last1=Arnold |first2=John W. |last2=Emerson |year=2011 |title=Nonparametric Goodness-of-Fit Tests for Discrete Null Distributions |journal=The R Journal |volume=3 |issue=2 |pages=34\\[Dash]39 |url=http://journal.r-project.org/archive/2011-2/RJournal_2011-2_Arnold+Emerson.pdf}}</ref> as part of the dgof package of the [[R (programming language)|R project for statistical computing]]. Major statistical packages among which [[SAS (software)|SAS]] <code>PROC NPAR1WAY</code> <ref>{{cite web|url=https://support.sas.com/documentation/cdl/en/statug/68162/HTML/default/viewer.htm#statug_npar1way_toc.htm|title=SAS/STAT(R) 14.1 User's Guide|author=|date=|website=support.sas.com|accessdate=14 April 2018}}</ref>, [[Stata]] <code>ksmirnov</code> <ref>{{cite web|url=https://www.stata.com/manuals13/rksmirnov.pdf|title=ksmirnov — Kolmogorov–Smirnov equality-of-distributions test|author=|date=|website=stata.com|accessdate=14 April 2018}}</ref> implement the KS test under the assumption that <math>F(x)</math> is continuous, which is more conservative if the null distribution is actually not continuous (see <ref name=Noether63>{{Cite journal |vauthors=Noether GE |year=1963|title=Note on the Kolmogorov Statistic in the Discrete Case |journal=Metrika |volume=7 |issue=1 |pages=115-116|url= }}</ref> \n<ref name=Slakter65>{{Cite journal |vauthors=Slakter MJ |year=1965|title=A Comparison of the Pearson Chi-Square and Kolmogorov Goodness-of-Fit Tests with Respect to Validity |journal=Journal of the American Statistical Association |volume=60 |issue=311 |pages=854-858 |url= }}</ref> \n<ref name=Walsh63>{{Cite journal |vauthors=Walsh JE  |year=1963 |title=Bounded Probability Properties of Kolmogorov–Smirnov and Similar Statistics for Discrete Data |journal=Annals of the Institute of Statistical Mathematics |volume=15 |issue=1 |pages=153-158|url= }}</ref>).\n\n==Two-sample Kolmogorov–Smirnov test==\n[[File:KS2 Example.png|thumb|300px|Illustration of the two-sample Kolmogorov–Smirnov statistic. Red and blue lines each correspond to an empirical distribution function, and the black arrow is the two-sample KS statistic.]]\n\nThe Kolmogorov–Smirnov test may also be used to test whether two underlying one-dimensional probability distributions differ. In this case, the Kolmogorov–Smirnov statistic is\n\n:<math>D_{n,m}=\\sup_x |F_{1,n}(x)-F_{2,m}(x)|,</math>\n\nwhere <math>F_{1,n}</math> and <math>F_{2,m}</math> are the [[empirical distribution function]]s of the first and the second sample respectively, and <math>\\sup</math> is the [[Infimum and supremum|supremum function]].\n\nFor large samples, the null hypothesis is rejected at level <math>\\alpha</math> if\n\n:<math>D_{n,m}>c(\\alpha)\\sqrt{\\frac{n + m}{n m}}.</math>\n\nWhere <math>n</math> and <math>m</math> are the sizes of first and second sample respectively. The value of <math>c({\\alpha})</math> is given in the table below for the most common levels of <math>\\alpha</math>\n\n{| class=\"wikitable\"\n|-\n| <math>\\alpha</math> || 0.10 || 0.05 || 0.025 || 0.01 || 0.005 || 0.001\n|-\n| <math>c({\\alpha})</math> || 1.073 || 1.224 || 1.358 || 1.517 || 1.628 || 1.858\n|}\n\nand in general<ref>Eq. (15) in Section 3.3.1 of Knuth, D.E., The Art of Computer Programming, Volume 2 (Seminumerical Algorithms), 3rd Edition, Addison Wesley, Reading Mass, 1998.</ref> by\n\n:<math>c\\left(\\alpha\\right)=\\sqrt{-\\frac{1}{2}\\ln\\alpha}.</math>\n\nNote that the two-sample test checks whether the two data samples come from the same distribution. This does not specify what that common distribution is (e.g. whether it's normal or not normal). Again, tables of critical values have been published. A shortcoming of the Kolmogorov–Smirnov test is that it is not very powerful because it is devised to be sensitive against all possible types of differences between two distribution functions. <ref>{{cite journal |last1=Marozzi |first1=Marco |title=Some Notes on the Location-Scale Cucconi Test |journal=Journal of Nonparametric Statistics |date=2009 |volume=21 |issue=5 |page=629–647 |doi=10.1080/10485250902952435 }}</ref> and <ref>{{cite journal |last1=Marozzi |first1=Marco |title=Nonparametric Simultaneous Tests for Location and Scale Testing: a Comparison of Several Methods |journal=Communications in Statistics – Simulation and Computation |date=2013 |volume=42 |issue=6 |page=1298–1317 |doi=10.1080/03610918.2012.665546 }}</ref> showed evidence that the [[Cucconi test]], originally proposed for simultaneously comparing location and scale, is much more powerful than the Kolmogorov–Smirnov test when comparing two distribution functions.\n\n==Setting confidence limits for the shape of a distribution function==\n{{main article|Dvoretzky–Kiefer–Wolfowitz inequality}}\n\nWhile the Kolmogorov–Smirnov test is usually used to test whether a given ''F''(''x'') is the underlying probability distribution of ''F''<sub>''n''</sub>(''x''), the procedure may be inverted to give confidence limits on ''F''(''x'') itself. If one chooses a critical value of the test statistic ''D''<sub>''α''</sub> such that P(''D''<sub>''n''</sub>&nbsp;>&nbsp;''D''<sub>''α''</sub>) = ''α'', then a band of width ±''D''<sub>''α''</sub> around ''F''<sub>''n''</sub>(''x'') will entirely contain ''F''(''x'') with probability 1&nbsp;−&nbsp;''α''.\n\n==The Kolmogorov–Smirnov statistic in more than one dimension==\n\nA distribution-free multivariate Kolmogorov–Smirnov goodness of fit test has been proposed by Justel, Peña and Zamar (1997).<ref>{{cite journal |last=Justel |first=A. |last2=Peña |first2=D. |last3=Zamar |first3=R. |year=1997 |title=A multivariate Kolmogorov–Smirnov test of goodness of fit |journal=Statistics & Probability Letters |volume=35 |issue=3 |pages=251–259 |doi=10.1016/S0167-7152(97)00020-5 |citeseerx=10.1.1.498.7631 }}</ref>  The test uses a statistic which is built using Rosenblatt's transformation, and an algorithm is developed to compute it in the bivariate case.  An approximate test that can be easily computed in any dimension is also presented.\n\nThe Kolmogorov–Smirnov test statistic needs to be modified if a similar test is to be applied to [[multivariate statistics|multivariate data]]. This is not straightforward because the maximum difference between two joint [[cumulative distribution function]]s is not generally the same as the maximum difference of any of the complementary distribution functions. Thus the maximum difference will differ depending on which of <math>\\Pr(x < X \\land y < Y)</math> or <math>\\Pr(X < x \\land Y > y)</math> or any of the other two possible arrangements is used. One might require that the result of the test used should not depend on which choice is made.\n\nOne approach to generalizing the Kolmogorov–Smirnov statistic to higher dimensions which meets the above concern is to compare the cdfs of the two samples with all possible orderings, and take the largest of the set of resulting K–S statistics.  In ''d'' dimensions, there are 2<sup>''d''</sup>−1 such orderings.  One such variation is due to Peacock<ref name=\"Peacock\">{{cite journal  |author = Peacock J.A. |title = Two-dimensional goodness-of-fit testing in astronomy |journal = [[Monthly Notices of the Royal Astronomical Society]] |volume = 202 |issue = 3 |pages = 615–627  |year = 1983 |bibcode = 1983MNRAS.202..615P |doi=10.1093/mnras/202.3.615}}</ref> and another to Fasano and Franceschini<ref name=\"Fasano\">{{cite journal |authors= Fasano, G., Franceschini, A. |year=1987 |title= A multidimensional version of the Kolmogorov–Smirnov test |journal= Monthly Notices of the Royal Astronomical Society |issn=0035-8711 |volume= 225 |pages= 155–170 |bibcode=1987MNRAS.225..155F |doi=10.1093/mnras/225.1.155}}</ref> (see Lopes et al. for a comparison and computational details).<ref name=\"Lopes\">{{cite conference |authors= Lopes, R.H.C., Reid, I., Hobson, P.R.  |title= The two-dimensional Kolmogorov–Smirnov test |conference= XI International Workshop on Advanced Computing and Analysis Techniques in Physics Research |date= April 23–27, 2007 |location= Amsterdam, the Netherlands |url= http://dspace.brunel.ac.uk/bitstream/2438/1166/1/acat2007.pdf }}</ref> Critical values for the test statistic can be obtained by simulations, but depend on the dependence structure in the joint distribution.\n\nIn one dimension, the Kolmogorov–Smirnov statistic is identical to the so-called star discrepancy D, so another native KS extension to higher dimensions would be simply to use D also for higher dimensions. Unfortunately, the star discrepancy is hard to calculate in high dimensions.\n\n==Implementations==\nKolmogorov - Smirnov Test (one or two sampled test verifies the equility of distributions) is well documented in many packages of statistical softwares and programms. The list of packages in some programms:\n\n* [[MATLAB]] has [https://de.mathworks.com/help/stats/kstest.html kstest] in its Statistics Toolbox.\n* [[R (programming language)|R]]'s statistics base-package implements the test [https://stat.ethz.ch/R-manual/R-patched/library/stats/html/ks.test.html as ks.test {stats}] in its \"stats\" package.\n* [[SAS (software)|SAS]] implements the test in its PROC NPAR1WAY procedure.\n* [[Python (programming language)]] has an implementation of this test provided by [[SciPy]]<ref>{{cite web |url= https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.kstest.html |title=scipy.stats.kstest |work=SciPy SciPy v0.14.0 Reference Guide |publisher=The Scipy community |access-date= 18 June 2019}}</ref> by Statistical functions (scipy.stats)\n* [[SYSTAT (statistics)|SYSTAT]] (SPSS Inc., Chicago, IL)\n* [[Java (programming language)]] has an implementation of this test provided by [[Apache Commons]]<ref>{{cite web |url=https://commons.apache.org/proper/commons-math/javadocs/api-3.5/org/apache/commons/math3/stat/inference/KolmogorovSmirnovTest.html |title=KolmogorovSmirnovTes|access-date= 18 June 2019}}</ref>\n* [[StatsDirect]] (StatsDirect Ltd, Manchester, UK) implements [https://www.statsdirect.com/help/nonparametric_methods/smirnov.htm all common variants].\n* [[Stata]] (Stata Corporation, College Station, TX) implements the test in ksmirnov (Kolmogorov–Smirnov equality-of-distributions test) command. <ref> {{ cite web |url=https://www.stata.com/manuals13/rksmirnov.pdf|title=ksmirnov — Kolmogorov –Smirnov equality-of-distributions test |access-date= 18 June 2019}} </ref>\n* [[PSPP]] implements the test in its [https://www.gnu.org/software/pspp/manual/html_node/KOLMOGOROV_002dSMIRNOV.html KOLMOGOROV-SMIRNOV (or using K-S shortcut] function.\n* [[Microsoft Excel|Excel]] runs the test  as KSCRIT and KSPROB <ref>{{cite web |url=http://www.real-statistics.com/tests-normality-and-symmetry/statistical-tests-normality-symmetry/kolmogorov-smirnov-test/|title=Kolmogorov-Smirnov Test for Normality Hypothesis Testing | access-date= 18 June 2019}}</ref>\n\n==See also==\n*[[Kuiper's test]]\n*[[Shapiro–Wilk test]]\n*[[Anderson–Darling test]]\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n* {{cite book |last=Daniel |first=Wayne W. |chapter=Kolmogorov–Smirnov one-sample test |title=Applied Nonparametric Statistics |location=Boston |publisher=PWS-Kent |edition=2nd |year=1990 |isbn=978-0-534-91976-4 |pages=319–330 |chapterurl=https://books.google.com/books?id=0hPvAAAAMAAJ&pg=PA319 }}\n* {{cite book\n  | last = Eadie\n  | first = W.T. |author2=D. Drijard |author3=F.E. James |author4=M. Roos |author5=B. Sadoulet\n  | title = Statistical Methods in Experimental Physics\n  | publisher = North-Holland\n  | year = 1971\n  | location = Amsterdam\n  | pages = 269–271\n  | isbn = 978-0-444-10117-4 }}\n* {{cite book\n  | last1 = Stuart\n  | first1 = Alan\n  | first2 = Keith\n  | last2 = Ord\n  | first3=Steven [F.]\n  | last3=Arnold\n  | title=Classical Inference and the Linear Model\n  | edition=Sixth\n  | series = Kendall's Advanced Theory of Statistics\n  | volume = 2A\n  | year = 1999\n  | publisher = Arnold\n  | location = London\n  | isbn=978-0-340-66230-4\n  | mr=1687411\n  | pages = 25.37–25.43 }}\n*{{cite book |last=Corder |first=G. W. |last2=Foreman |first2=D. I. |year=2014 |title=Nonparametric Statistics: A Step-by-Step Approach |location= |publisher=Wiley |isbn=978-1118840313 }}\n*{{cite journal |last=Stephens |first=M. A. |year=1979 |title=Test of fit for the logistic distribution based on the empirical distribution function |journal=Biometrika |volume=66 |issue=3 |pages=591–595 |doi=10.1093/biomet/66.3.591 }}\n\n==External links==\n*{{springer|title=Kolmogorov–Smirnov test|id=p/k055740}}\n*[https://web.archive.org/web/20050710021649/http://www.physics.csbsju.edu/stats/KS-test.html Short introduction] \n*[http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm KS test explanation]\n*[http://www.ciphersbyritter.com/JAVASCRP/NORMCHIK.HTM JavaScript implementation of one- and two-sided tests]\n*[http://jumk.de/statistic-calculator/ Online calculator with the KS test]\n* Open-source C++ code to compute the [http://root.cern.ch/root/html/TMath.html#TMath:KolmogorovProb Kolmogorov distribution] and perform the [http://root.cern.ch/root/html/TMath.html#TMath:KolmogorovTest KS test]\n*Paper on [http://www.jstatsoft.org/v08/i18/paper Evaluating Kolmogorov’s Distribution]; contains C implementation. This is the method used in [[Matlab]].\n*Paper on [http://www.jstatsoft.org/v39/i11/paper Computing the Two-Sided Kolmogorov–Smirnov Distribution]; computing the cdf of the KS statistic in C or Java.\n*Paper [http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0085777 powerlaw: A Python Package for Analysis of Heavy-Tailed Distributions]; Jeff Alstott, Ed Bullmore, Dietmar Plenz. Among others, it also performs the Kolmogorov–Smirnov test. Source code and installers of powerlaw package are available at [https://pypi.python.org/pypi/powerlaw PyPi].\n{{Statistics}}\n\n{{DEFAULTSORT:Kolmogorov-Smirnov Test}}\n[[Category:Statistical tests]]\n[[Category:Statistical distance]]\n[[Category:Nonparametric statistics]]\n[[Category:Normality tests]]"
    },
    {
      "title": "Lilliefors test",
      "url": "https://en.wikipedia.org/wiki/Lilliefors_test",
      "text": "{{No footnotes|date=October 2010}}\nIn [[statistics]], the '''Lilliefors test''' is a [[normality test]] based on the [[Kolmogorov–Smirnov test]].  It is used to test the [[null hypothesis]] that data come from a [[normal distribution|normally distributed]] population, when the null hypothesis does not specify ''which'' normal distribution; i.e., it does not specify the [[expected value]] and [[variance]] of the distribution. It is named after [[Hubert Lilliefors]], professor of statistics at [[George Washington University]].\n\n==The test==\nThe test proceeds as follows:\n\n# First estimate the population mean and population variance based on the data.\n# Then find the maximum discrepancy between the [[empirical distribution function]] and the [[cumulative distribution function]] (CDF) of the normal distribution with the estimated mean and estimated variance.  Just as in the Kolmogorov&ndash;Smirnov test, this will be the test statistic.\n# Finally, assess whether the maximum discrepancy is large enough to be [[statistical significance|statistically significant]], thus requiring rejection of the null hypothesis.  This is where this test becomes more complicated than the Kolmogorov&ndash;Smirnov test.  Since the hypothesised CDF has been moved closer to the data by estimation based on those data, the maximum discrepancy has been made smaller than it would have been if the null hypothesis had singled out just one normal distribution.  Thus the \"null distribution\" of the test statistic, i.e. its [[probability distribution]] assuming the null hypothesis is true, is [[stochastic order|stochastically smaller]] than the Kolmogorov&ndash;Smirnov distribution.  This is the '''Lilliefors distribution'''.  To date, tables for this distribution have been computed only by [[Monte Carlo method]]s.<!-- I think the foregoing sentence was true as of May 2003; if anyone knows any relevant more recent work, of if it's otherwise wrong, please edit this accordingly. -- Mike Hardy -->\n\n==See also==\n* [[Normality test]]\n* [[Jarque&ndash;Bera test]]\n\n==References==\n{{Reflist}}\n\n==Sources==\n* Lilliefors, H. (June 1967), \"On the Kolmogorov&ndash;Smirnov test for normality with mean and variance unknown\", ''[[Journal of the American Statistical Association]]'', Vol. 62. pp.&nbsp;399&ndash;402.\n* Lilliefors, H. (1969), \"On the Kolmogorov&ndash;Smirnov test for the exponential distribution with mean unknown\", ''[[Journal of the American Statistical Association]]'', Vol. 64 . pp.&nbsp;387&ndash;389.\n* Dallal, G.E. (1986), \"An analytic approximation to the distribution of Lilliefors's test statistic for normality\", ''[[The American Statistician]]'', Vol. 40. p.&nbsp;40&ndash;294-296.\n* Conover, W.J. (1999), \"Practical nonparametric statistics\", 3rd ed. Wiley : New York.\n\n==External links==\n*[http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm US NIST Handbook of Statistics]\n\n{{Statistics}}\n\n{{DEFAULTSORT:Lilliefors Test}}\n[[Category:Normality tests]]"
    },
    {
      "title": "Pearson's chi-squared test",
      "url": "https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test",
      "text": "{{about|the particular test|the more general category of tests|Chi-squared test}}\n\n{{More footnotes|date=August 2017}}\n\n'''Pearson's chi-squared test''' (''[[Chi (letter)|χ]]''{{sup|2}}) is a statistical test applied to sets of [[categorical data]] to evaluate how likely it is that any observed difference between the sets arose by chance. It is the most widely used of many [[chi-squared test]]s (e.g., [[Yates's correction for continuity|Yates]], [[Likelihood-ratio test|likelihood ratio]], [[Portmanteau test|portmanteau test in time series]], etc.) – [[Statistics|statistical]] procedures whose results are evaluated by reference to the [[chi-squared distribution]]. Its properties were first investigated by [[Karl Pearson]] in 1900.<ref>{{Cite journal | last = Pearson | first = Karl | authorlink = Karl Pearson | title = On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling | doi = 10.1080/14786440009463897 | journal = Philosophical Magazine |series=Series 5 | volume = 50 | issue = 302 | pages = 157–175 | year = 1900 | url = http://www.economics.soton.ac.uk/staff/aldrich/1900.pdf | pmid =  | pmc = }}</ref> In contexts where it is important to improve a distinction between the [[test statistic]] and its distribution, names similar to ''Pearson χ-squared'' test or statistic are used.\n\nIt tests a [[null hypothesis]] stating that the [[frequency distribution]] of certain [[event (probability theory)|event]]s observed in a [[Sample (statistics)|sample]] is consistent with a particular theoretical distribution. The events considered must be mutually exclusive and have total probability 1. A common case for this is where the events each cover an outcome of a [[Level of measurement|categorical variable]]. \nA simple example is the hypothesis that an ordinary six-sided {{dice}} is \"fair\" (i. e., all six outcomes are equally likely to occur.)\n\n==Definition==\nPearson's chi-squared test is used to assess three types of comparison: [[goodness of fit]], [[homogeneity (statistics)|homogeneity]], and [[Independence (probability theory)|independence]].\n* A '''test of goodness of fit''' establishes whether an observed [[frequency distribution]] differs from a theoretical distribution.\n* A '''test of homogeneity''' compares the distribution of counts for two or more groups using the same categorical variable (e.g. choice of activity—college, military, employment, travel—of graduates of a high school reported a year after graduation, sorted by graduation year, to see if number of graduates choosing a given activity has changed from class to class, or from decade to decade).<ref name=\"Bock\">David E. Bock, Paul F. Velleman, Richard D. De Veaux (2007). \"Stats, Modeling the World,\" pp.&nbsp;606-627, Pearson Addison Wesley, Boston, {{ISBN|0-13-187621-X}}</ref>\n* A '''test of independence''' assesses whether observations consisting of measures on two variables, expressed in a [[contingency table]], are independent of each other (e.g. polling responses from people of different nationalities to see if one's nationality is related to the response).\n\nFor all three tests, the computational procedure includes the following steps:\n# Calculate the chi-squared test [[statistic]], ''χ''², which resembles a [[Normalization (statistics)|normalized]] sum of squared deviations between observed and theoretical [[Frequency (statistics)|frequencies]] (see below).\n# Determine the [[degrees of freedom (statistics)|degrees of freedom]], '''df''', of that statistic.\n## For a '''test of goodness-of-fit''', {{nobreak|1= df = Cats &minus; Parms}}, where ''Cats'' is the number of observation categories recognized by the model, and ''Parms'' is the number of parameters in the model adjusted to make the model best fit the observations: The number of categories reduced by the number of fitted parameters in the distribution.\n## For '''test of homogeneity''', {{nobreak|1= df = (Rows &minus; 1)×(Cols &minus; 1)}}, where ''Rows'' corresponds to the number of categories (i.e. rows in the associated contingency table), and ''Cols'' corresponds the number of independent groups (i.e. columns in the associated contingency table).<ref name=\"Bock\" />\n## For '''test of independence''', {{nobreak|1= df = (Rows &minus; 1)×(Cols &minus; 1)}}, where in this case, ''Rows'' corresponds to number of categories in one variable, and ''Cols'' corresponds to number of categories in the second variable.<ref name=\"Bock\" />\n# Select a desired level of confidence (significance level, [[p-value]] or the corresponding [[alpha level]]) for the result of the test.\n# Compare ''χ''² to the critical value from the [[chi-squared distribution]] with ''df'' degrees of freedom and the selected confidence level (one-sided since the test is only one direction, i.e. is the test value greater than the critical value?), which in many cases gives a good approximation of the distribution of ''χ''².\n# Sustain or reject the null hypothesis that the observed frequency distribution is the same as the theoretical distribution based on whether the test statistic exceeds the critical value of ''χ''². If the test statistic exceeds the critical value of ''χ''², the null hypothesis (<math>H_0</math> = there is ''no'' difference between the distributions) can be rejected, and the alternative hypothesis (<math>H_1</math> = there ''is'' a difference between the distributions) can be accepted, both with the selected level of confidence. If the test statistic falls below the threshold ''χ''² value, then no clear conclusion can be reached, and the null hypothesis is sustained, but not necessarily accepted.\n\n==Test for fit of a distribution==\n\n===Discrete uniform distribution===\n\nIn this case <math>N</math> observations are divided among <math>n</math> cells. A simple application is to test the hypothesis that, in the general population, values would occur in each cell with equal frequency. The \"theoretical frequency\" for any cell (under the null hypothesis of a [[discrete uniform distribution]]) is thus calculated as\n:<math>E_i=\\frac{N}{n}\\, ,</math>\nand the reduction in the degrees of freedom is <math>p=1</math>, notionally because the observed frequencies <math>O_i</math> are constrained to sum to <math>N</math>.\n\nOne specific example of its application would be its application for log-rank test.\n\n===Other distributions===\n\nWhen testing whether observations are random variables whose distribution belongs to a given family of distributions, the \"theoretical frequencies\" are calculated using a distribution from that family fitted in some standard way. The reduction in the degrees of freedom is calculated as <math>p=s+1</math>, where <math>s</math> is the number of [[Covariate|co-variates]] used in fitting the distribution. For instance, when checking a three-co-variate Weibull distribution, <math>p=4</math>, and when checking a normal distribution (where the parameters are mean and standard deviation), <math>p=3</math>, and when checking a Poisson distribution (where the parameter is the expected value), <math>p=2</math>. Thus, there will be <math>n-p</math> degrees of freedom, where <math>n</math> is the number of categories.\n\nThe degrees of freedom are not based on the number of observations as with a [[Student's t]] or [[F-distribution]]. For example, if testing for a fair, six-sided {{dice}}, there would be five degrees of freedom because there are six categories/parameters (each number). The number of times the die is rolled does not influence the number of degrees of freedom.\n\n===Calculating the test-statistic===\n[[File:Chi-square distributionCDF-English.png|thumb|right|300px|[[Chi-squared distribution]], showing ''X''<sup>2</sup> on the x-axis and P-value on the y-axis.]]\n{| class=\"infobox wikitable collapsible collapsed\" style=\"text-align:center;font-size:75%;line-height:0.9;\"\n! colspan=\"6\" style=\"font-weight:normal;font-size:125%;\"|Upper-tail critical values of chi-square distribution <ref>{{cite web|title=1.3.6.7.4. Critical Values of the Chi-Square Distribution|url=http://www.itl.nist.gov/div898/handbook/eda/section3/eda3674.htm|accessdate=14 October 2014}}</ref>\n|-\n! rowspan=\"2\"|Degrees<br /> of<br />freedom\n! colspan=\"5\"|Probability less than the critical value\n|-\n!    0.90 ||    0.95 ||   0.975 ||    0.99 ||   0.999\n|-\n!   1\n|   2.706 ||   3.841 ||   5.024 ||   6.635 ||  10.828\n|-\n!   2\n|   4.605 ||   5.991 ||   7.378 ||   9.210 ||  13.816\n|-\n!   3\n|   6.251 ||   7.815 ||   9.348 ||  11.345 ||  16.266\n|-\n!   4\n|   7.779 ||   9.488 ||  11.143 ||  13.277 ||  18.467\n|-\n!   5\n|   9.236 ||  11.070 ||  12.833 ||  15.086 ||  20.515\n|-\n!   6\n|  10.645 ||  12.592 ||  14.449 ||  16.812 ||  22.458\n|-\n!   7\n|  12.017 ||  14.067 ||  16.013 ||  18.475 ||  24.322\n|-\n!   8\n|  13.362 ||  15.507 ||  17.535 ||  20.090 ||  26.125\n|-\n!   9\n|  14.684 ||  16.919 ||  19.023 ||  21.666 ||  27.877\n|-\n!  10\n|  15.987 ||  18.307 ||  20.483 ||  23.209 ||  29.588\n|-\n!  11\n|  17.275 ||  19.675 ||  21.920 ||  24.725 ||  31.264\n|-\n!  12\n|  18.549 ||  21.026 ||  23.337 ||  26.217 ||  32.910\n|-\n!  13\n|  19.812 ||  22.362 ||  24.736 ||  27.688 ||  34.528\n|-\n!  14\n|  21.064 ||  23.685 ||  26.119 ||  29.141 ||  36.123\n|-\n!  15\n|  22.307 ||  24.996 ||  27.488 ||  30.578 ||  37.697\n|-\n!  16\n|  23.542 ||  26.296 ||  28.845 ||  32.000 ||  39.252\n|-\n!  17\n|  24.769 ||  27.587 ||  30.191 ||  33.409 ||  40.790\n|-\n!  18\n|  25.989 ||  28.869 ||  31.526 ||  34.805 ||  42.312\n|-\n!  19\n|  27.204 ||  30.144 ||  32.852 ||  36.191 ||  43.820\n|-\n!  20\n|  28.412 ||  31.410 ||  34.170 ||  37.566 ||  45.315\n|-\n!  21\n|  29.615 ||  32.671 ||  35.479 ||  38.932 ||  46.797\n|-\n!  22\n|  30.813 ||  33.924 ||  36.781 ||  40.289 ||  48.268\n|-\n!  23\n|  32.007 ||  35.172 ||  38.076 ||  41.638 ||  49.728\n|-\n!  24\n|  33.196 ||  36.415 ||  39.364 ||  42.980 ||  51.179\n|-\n!  25\n|  34.382 ||  37.652 ||  40.646 ||  44.314 ||  52.620\n|-\n!  26\n|  35.563 ||  38.885 ||  41.923 ||  45.642 ||  54.052\n|-\n!  27\n|  36.741 ||  40.113 ||  43.195 ||  46.963 ||  55.476\n|-\n!  28\n|  37.916 ||  41.337 ||  44.461 ||  48.278 ||  56.892\n|-\n!  29\n|  39.087 ||  42.557 ||  45.722 ||  49.588 ||  58.301\n|-\n!  30\n|  40.256 ||  43.773 ||  46.979 ||  50.892 ||  59.703\n|-\n!  31\n|  41.422 ||  44.985 ||  48.232 ||  52.191 ||  61.098\n|-\n!  32\n|  42.585 ||  46.194 ||  49.480 ||  53.486 ||  62.487\n|-\n!  33\n|  43.745 ||  47.400 ||  50.725 ||  54.776 ||  63.870\n|-\n!  34\n|  44.903 ||  48.602 ||  51.966 ||  56.061 ||  65.247\n|-\n!  35\n|  46.059 ||  49.802 ||  53.203 ||  57.342 ||  66.619\n|-\n!  36\n|  47.212 ||  50.998 ||  54.437 ||  58.619 ||  67.985\n|-\n!  37\n|  48.363 ||  52.192 ||  55.668 ||  59.893 ||  69.347\n|-\n!  38\n|  49.513 ||  53.384 ||  56.896 ||  61.162 ||  70.703\n|-\n!  39\n|  50.660 ||  54.572 ||  58.120 ||  62.428 ||  72.055\n|-\n!  40\n|  51.805 ||  55.758 ||  59.342 ||  63.691 ||  73.402\n|-\n!  41\n|  52.949 ||  56.942 ||  60.561 ||  64.950 ||  74.745\n|-\n!  42\n|  54.090 ||  58.124 ||  61.777 ||  66.206 ||  76.084\n|-\n!  43\n|  55.230 ||  59.304 ||  62.990 ||  67.459 ||  77.419\n|-\n!  44\n|  56.369 ||  60.481 ||  64.201 ||  68.710 ||  78.750\n|-\n!  45\n|  57.505 ||  61.656 ||  65.410 ||  69.957 ||  80.077\n|-\n!  46\n|  58.641 ||  62.830 ||  66.617 ||  71.201 ||  81.400\n|-\n!  47\n|  59.774 ||  64.001 ||  67.821 ||  72.443 ||  82.720\n|-\n!  48\n|  60.907 ||  65.171 ||  69.023 ||  73.683 ||  84.037\n|-\n!  49\n|  62.038 ||  66.339 ||  70.222 ||  74.919 ||  85.351\n|-\n!  50\n|  63.167 ||  67.505 ||  71.420 ||  76.154 ||  86.661\n|-\n!  51\n|  64.295 ||  68.669 ||  72.616 ||  77.386 ||  87.968\n|-\n!  52\n|  65.422 ||  69.832 ||  73.810 ||  78.616 ||  89.272\n|-\n!  53\n|  66.548 ||  70.993 ||  75.002 ||  79.843 ||  90.573\n|-\n!  54\n|  67.673 ||  72.153 ||  76.192 ||  81.069 ||  91.872\n|-\n!  55\n|  68.796 ||  73.311 ||  77.380 ||  82.292 ||  93.168\n|-\n!  56\n|  69.919 ||  74.468 ||  78.567 ||  83.513 ||  94.461\n|-\n!  57\n|  71.040 ||  75.624 ||  79.752 ||  84.733 ||  95.751\n|-\n!  58\n|  72.160 ||  76.778 ||  80.936 ||  85.950 ||  97.039\n|-\n!  59\n|  73.279 ||  77.931 ||  82.117 ||  87.166 ||  98.324\n|-\n!  60\n|  74.397 ||  79.082 ||  83.298 ||  88.379 ||  99.607\n|-\n!  61\n|  75.514 ||  80.232 ||  84.476 ||  89.591 || 100.888\n|-\n!  62\n|  76.630 ||  81.381 ||  85.654 ||  90.802 || 102.166\n|-\n!  63\n|  77.745 ||  82.529 ||  86.830 ||  92.010 || 103.442\n|-\n!  64\n|  78.860 ||  83.675 ||  88.004 ||  93.217 || 104.716\n|-\n!  65\n|  79.973 ||  84.821 ||  89.177 ||  94.422 || 105.988\n|-\n!  66\n|  81.085 ||  85.965 ||  90.349 ||  95.626 || 107.258\n|-\n!  67\n|  82.197 ||  87.108 ||  91.519 ||  96.828 || 108.526\n|-\n!  68\n|  83.308 ||  88.250 ||  92.689 ||  98.028 || 109.791\n|-\n!  69\n|  84.418 ||  89.391 ||  93.856 ||  99.228 || 111.055\n|-\n!  70\n|  85.527 ||  90.531 ||  95.023 || 100.425 || 112.317\n|-\n!  71\n|  86.635 ||  91.670 ||  96.189 || 101.621 || 113.577\n|-\n!  72\n|  87.743 ||  92.808 ||  97.353 || 102.816 || 114.835\n|-\n!  73\n|  88.850 ||  93.945 ||  98.516 || 104.010 || 116.092\n|-\n!  74\n|  89.956 ||  95.081 ||  99.678 || 105.202 || 117.346\n|-\n!  75\n|  91.061 ||  96.217 || 100.839 || 106.393 || 118.599\n|-\n!  76\n|  92.166 ||  97.351 || 101.999 || 107.583 || 119.850\n|-\n!  77\n|  93.270 ||  98.484 || 103.158 || 108.771 || 121.100\n|-\n!  78\n|  94.374 ||  99.617 || 104.316 || 109.958 || 122.348\n|-\n!  79\n|  95.476 || 100.749 || 105.473 || 111.144 || 123.594\n|-\n!  80\n|  96.578 || 101.879 || 106.629 || 112.329 || 124.839\n|-\n!  81\n|  97.680 || 103.010 || 107.783 || 113.512 || 126.083\n|-\n!  82\n|  98.780 || 104.139 || 108.937 || 114.695 || 127.324\n|-\n!  83\n|  99.880 || 105.267 || 110.090 || 115.876 || 128.565\n|-\n!  84\n| 100.980 || 106.395 || 111.242 || 117.057 || 129.804\n|-\n!  85\n| 102.079 || 107.522 || 112.393 || 118.236 || 131.041\n|-\n!  86\n| 103.177 || 108.648 || 113.544 || 119.414 || 132.277\n|-\n!  87\n| 104.275 || 109.773 || 114.693 || 120.591 || 133.512\n|-\n!  88\n| 105.372 || 110.898 || 115.841 || 121.767 || 134.746\n|-\n!  89\n| 106.469 || 112.022 || 116.989 || 122.942 || 135.978\n|-\n!  90\n| 107.565 || 113.145 || 118.136 || 124.116 || 137.208\n|-\n!  91\n| 108.661 || 114.268 || 119.282 || 125.289 || 138.438\n|-\n!  92\n| 109.756 || 115.390 || 120.427 || 126.462 || 139.666\n|-\n!  93\n| 110.850 || 116.511 || 121.571 || 127.633 || 140.893\n|-\n!  94\n| 111.944 || 117.632 || 122.715 || 128.803 || 142.119\n|-\n!  95\n| 113.038 || 118.752 || 123.858 || 129.973 || 143.344\n|-\n!  96\n| 114.131 || 119.871 || 125.000 || 131.141 || 144.567\n|-\n!  97\n| 115.223 || 120.990 || 126.141 || 132.309 || 145.789\n|-\n!  98\n| 116.315 || 122.108 || 127.282 || 133.476 || 147.010\n|-\n!  99\n| 117.407 || 123.225 || 128.422 || 134.642 || 148.230\n|-\n! 100\n| 118.498 || 124.342 || 129.561 || 135.807 || 149.449\n|}\nThe value of the test-statistic is\n\n:<math> \\chi^2 = \\sum_{i=1}^{n} \\frac{(O_i - E_i)^2}{E_i} =  N \\sum_{i=1}^n \\frac{\\left(O_i/N - p_i\\right)^2}{p_i} </math>\n\nwhere\n\n:<math> \\chi^2</math> = Pearson's cumulative test statistic, which asymptotically approaches a [[chi-squared distribution|<math>\\chi^2</math> distribution]].\n:<math>O_i</math> = the number of observations of type ''i''.\n:<math>N</math> = total number of observations\n:<math>E_i = N p_i</math> = the expected (theoretical) count of type ''i'', asserted by the null hypothesis that the fraction of type ''i'' in the population is <math> p_i</math>\n:<math>n</math>  = the number of cells in the table.\n\nThe chi-squared statistic can then be used to calculate a [[p-value]] by [[Chi-squared distribution#Table of χ2 values vs p-values|comparing the value of the statistic]] to a [[chi-squared distribution]]. The number of [[degrees of freedom (statistics)|degrees of freedom]] is equal to the number of cells <math>n</math>, minus the reduction in degrees of freedom, <math>p</math>.\n\nThe result about the numbers of degrees of freedom is valid when the original data are multinomial and hence the estimated parameters are efficient for minimizing the chi-squared statistic. More generally however, when maximum likelihood estimation does not coincide with minimum chi-squared estimation, the distribution will lie somewhere between a chi-squared distribution with <math>n-1-p</math> and <math>n-1</math> degrees of freedom (See for instance Chernoff and Lehmann, 1954).\n\n===Bayesian method===\n\n{{details|Categorical distribution#Bayesian inference using conjugate prior}}\nIn [[Bayesian statistics]], one would instead use a [[Dirichlet distribution]] as [[conjugate prior]]. If one took a uniform prior, then the [[maximum likelihood estimate]] for the population probability is the observed probability, and one may compute a [[credible region]] around this or another estimate.\n\n==Testing for statistical independence==\nIn this case, an \"observation\" consists of the values of two outcomes and the null hypothesis is that the occurrence of these outcomes is [[statistically independent]]. Each observation is allocated to one cell of a two-dimensional array of cells (called a [[contingency table]]) according to the values of the two outcomes. If there are ''r'' rows and ''c'' columns in the table, the \"theoretical frequency\" for a cell, given the hypothesis of independence, is\n\n:<math>E_{i,j}= N  p_{i\\cdot} p_{\\cdot j} ,</math>\n\nwhere <math>N</math> is the total sample size (the sum of all cells in the table), and\n\n:<math> p_{i\\cdot} = \\frac{O_{i\\cdot}}{N} = \\sum_{j=1}^c \\frac{O_{i,j}}{N},</math>\n\nis the fraction of observations of type ''i'' ignoring the column attribute (fraction of row totals), and\n:<math> p_{\\cdot j} = \\frac{O_{\\cdot j}}{N}  = \\sum_{i = 1}^r \\frac{O_{i,j}}{N} </math>\n\nis the fraction of observations of type ''j'' ignoring the row attribute (fraction of column totals).  The term \"[[frequency distribution|frequencies]]\" refers to absolute numbers rather than  already normalised values.\n\nThe value of the test-statistic is\n\n:<math> \\chi^2 = \\sum_{i=1}^{r} \\sum_{j=1}^{c} {(O_{i,j} - E_{i,j})^2 \\over E_{i,j}}</math>\n:<math> \\  \\  \\  \\ = N \\sum_{i,j} p_{i\\cdot}p_{\\cdot j} \\left(\\frac{(O_{i,j}/N) - p_{i\\cdot}p_{\\cdot j}}{p_{i\\cdot}p_{\\cdot j}}\\right)^2 </math>\n\nNote that <math> \\chi^2 </math> is 0 if and only if <math> O_{i,j} = E_{i,j} \\forall i,j </math>, i.e. only if the expected and true number of observations are equal in all cells.\n\nFitting the model of \"independence\" reduces the number of degrees of freedom by ''p''&nbsp;=&nbsp;''r''&nbsp;+&nbsp;''c''&nbsp;−&nbsp;1.  The number of [[degrees of freedom (statistics)|degrees of freedom]] is equal to the number of cells ''rc'', minus the reduction in degrees of freedom, ''p'', which reduces to&nbsp;(''r''&nbsp;−&nbsp;1)(''c''&nbsp;−&nbsp;1).\n\nFor the test of independence, also known as the test of homogeneity, a chi-squared probability of less than or equal to 0.05 (or the chi-squared statistic being at or larger than the 0.05 critical point) is commonly interpreted by applied workers as justification for rejecting the null hypothesis that the row variable is independent of the column variable.<ref>{{cite web|title=Critical Values of the Chi-Squared Distribution |url=http://www.itl.nist.gov/div898/handbook/eda/section3/eda3674.htm |work=NIST/SEMATECH e-Handbook of Statistical Methods |publisher=National Institute of Standards and Technology}}</ref>\nThe [[alternative hypothesis]] corresponds to the variables having an association or relationship where the structure of this relationship is not specified.\n\n==Assumptions==\nThe chi-squared test, when used with the standard approximation that a chi-squared distribution is applicable, has the following assumptions:{{Citation needed|date=July 2009}}\n\n; [[Simple random sample]]: The sample data is a random sampling from a fixed distribution or population where every collection of members of the population of the given sample size has an equal probability of selection.  Variants of the test have been developed for complex samples, such as where the data is weighted. Other forms can be used such as [[purposive sampling]].<ref>See {{cite book |title=Discovering Statistics Using SPSS |first=Andy |last=Field }} for assumptions on Chi Square.</ref>\n; Sample size (whole table): A sample with a sufficiently large size is assumed. If a chi squared test is conducted on a sample with a smaller size, then the chi squared test will yield an inaccurate inference. The researcher, by using chi squared test on small samples, might end up committing a [[Type II error]].\n; Expected cell count: Adequate expected cell counts. Some require 5 or more, and others require 10 or more. A common rule is 5 or more in all cells of a 2-by-2 table, and 5 or more in 80% of cells in larger tables, but no cells with zero expected count. When this assumption is not met, [[Yates's correction for continuity|Yates's correction]] is applied.\n; Independence: The observations are always assumed to be independent of each other. This means chi-squared cannot be used to test correlated data (like matched pairs or panel data). In those cases, [[McNemar's test]] may be more appropriate.\n\nA test that relies on different assumptions is [[Fisher's exact test]]; if its assumption of fixed marginal distributions is met it is substantially more accurate in obtaining a significance level, especially with few observations. In the vast majority of applications this assumption will not be met, and Fisher's exact test will be over conservative and not have correct coverage.<ref>{{cite web|title=A Bayesian Formulation for Exploratory Data Analysis and Goodness-of-Fit Testing|url=http://www.stat.columbia.edu/~gelman/research/published/isr.pdf | page=375 |publisher=International Statistical Review}}</ref>\n\n==Derivation==\n{{hidden begin|border=1px #aaa solid|title={{center|Derivation using Central Limit Theorem}}}}\n\nThe null distribution of the Pearson statistic with ''j'' rows and ''k'' columns is approximated by the [[chi-squared distribution]] with\n(''k''&nbsp;−&nbsp;1)(''j''&nbsp;−&nbsp;1) degrees of freedom.<ref name=\"ocw\">Statistics for Applications. ''MIT OpenCourseWare''. [http://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2003/lecture-notes/lec23.pdf Lecture 23]. Pearson's Theorem.  Retrieved 21 March 2007.</ref>\n\nThis approximation arises as the true distribution, under the null hypothesis, if the expected value is given by a [[multinomial distribution]]. For large sample sizes, the [[central limit theorem]] says this distribution tends toward a certain [[multivariate normal distribution]].\n\n===Two cells===\nIn the special case where there are only two cells in the table, the expected values follow a [[binomial distribution]],\n\n:<math> E \\ \\sim \\ \\mbox{Bin}(n,p), \\, </math>\n\nwhere\n\n:''p'' = probability, under the null hypothesis,\n:''n'' = number of observations in the sample.\n\nIn the above example the hypothesised probability of a male observation is 0.5, with 100 samples. Thus we expect to observe 50 males.\n\nIf ''n'' is sufficiently large, the above binomial distribution may be approximated by a Gaussian (normal) distribution and thus the Pearson test statistic approximates a chi-squared distribution,\n\n:<math> \\text{Bin}(n,p) \\approx \\text{N}(np, np(1-p)). \\, </math>\n\nLet ''O''<sub>1</sub> be the number of observations from the sample that are in the first cell. The Pearson test statistic can be expressed as\n\n:<math> \\frac{(O_1-np)^2}{np} + \\frac{(n-O_1-n(1-p))^2}{n(1-p)}, </math>\n\nwhich can in turn be expressed as\n\n:<math> \\left(\\frac{O_1-np}{\\sqrt{np(1-p)}}\\right)^2. </math>\n\nBy the normal approximation to a binomial this is the squared of one standard normal variate, and hence is distributed as chi-squared with 1 degree of freedom.  Note that the denominator is one standard deviation of the Gaussian approximation, so can be written\n\n:<math> \\frac{(O_1-\\mu)^2}{\\sigma^2}. </math>\n\nSo as consistent with the meaning of the chi-squared distribution, we are measuring how probable the observed number of standard deviations away from the mean is under the Gaussian approximation (which is a good approximation for large ''n'').\n\nThe chi-squared distribution is then integrated on the right of the statistic value to obtain the [[P-value]], which is equal to the probability of getting a statistic equal or bigger than the observed one, assuming the null hypothesis.\n\n===Two-by-two contingency tables===\nWhen the test is applied to a [[contingency table]] containing two rows and two columns, the test is equivalent to a [[Z-test]] of proportions.{{Citation needed|date=September 2018|reason=Claim needs a citation or more precise context -- we were unable to reproduce this using standard library functions in Python or R}}\n\n===Many cells===\nSimilar arguments as above lead to the desired result. {{Citation needed|date=January 2011}} <!-- (TODO: details) --> Each cell (except the final one, whose value is completely determined by the others) is treated as an independent binomial variable, and their contributions are summed and each contributes one degree of freedom.\n\nLet us now prove that the distribution indeed approaches asymptotically the <math>\\chi^2</math> distribution as the number of observations approaches infinity.\n\nLet <math>n</math> be the number of observations, <math>m</math> the number of cells and <math>p_i</math> the probability of an observation to fall in the i-th cell, for <math>1\\le i\\le m</math>. We denote by <math>\\{k_i\\}</math> the configuration where for each i there are <math>k_i</math> observations in the i-th cell. Note that\n\n:<math>\\sum_{i=1}^m k_i = n \\qquad \\text{and} \\qquad \\sum_{i=1}^m p_i = 1.</math>\n\nLet <math>\\chi^2_P(\\{k_i\\},\\{p_i\\})</math> be Pearson's cumulative test statistic for such a configuration, and let <math>\\chi^2_P(\\{p_i\\})</math> be the distribution of this statistic. We will show that the latter probability approaches the <math>\\chi^2</math> distribution with <math>m-1</math> degrees of freedom, as <math>n \\to \\infty.</math>\n\nFor any arbitrary value T:\n\n:<math> P(\\chi^2_P(\\{p_i\\}) > T) = \\sum_{\\{k_i\\}|\\chi^2_P(\\{k_i\\},\\{p_i\\}) > T} \\frac{n!}{k_1! \\cdots k_m!} \\prod_{i=1}^m {p_i}^{k_i} </math>\n\nWe will use a procedure similar to the approximation in [[de Moivre–Laplace theorem]].  Contributions from small <math>k_i</math> are of subleading order in <math>n</math> and thus for large <math>n</math> we may use  [[Stirling's formula]] for both <math>n!</math> and <math>k_i!</math> to get the following:\n\n:<math>P(\\chi^2_P(\\{p_i\\}) > T) \\sim \\sum_{\\{k_i\\}|\\chi^2_P(\\{k_i\\},\\{p_i\\}) > T} \\prod_{i=1}^m \\left (\\frac{np_i}{k_i}\\right)^{k_i} \\sqrt{\\frac{2\\pi n}{\\prod_{i=1}^m 2\\pi k_i}}</math>\n\nBy substituting for\n\n:<math>x_i = \\frac{k_i-np_i}{\\sqrt{n}}, \\qquad i = 1, \\cdots, m-1, </math>\n\nwe may approximate for large <math>n</math> the sum over the <math>k_i</math> by an integral over the <math>x_i</math>. Noting that:\n\n:<math>k_m = np_m-\\sqrt{n} \\sum_{i=1}^{m-1}x_i,</math>\n\nwe arrive at\n\n:<math> \\begin{align}\nP(\\chi^2_P (\\{p_i\\}) > T) &\\sim \\sqrt{\\frac{2\\pi n}{\\prod_{i=1}^m 2\\pi k_i}} \\int_{\\chi^2_P(\\{\\sqrt{n} x_i+n p_i\\},\\{p_i\\}) > T} \\left \\{ \\prod_{i=1}^{m-1} {\\sqrt{n} dx_i}\\right\\} \\left \\{\\prod_{i=1}^{m-1} \\left (1+\\frac{x_i}{\\sqrt{n} p_i}\\right)^{-(n p_i + \\sqrt{n} x_i) } \\left(1-\\frac{\\sum_{i=1}^{m-1}{x_i}}{\\sqrt{n} p_m}\\right)^{-\\left(n p_m-\\sqrt{n} \\sum_{i=1}^{m-1}x_i\\right)} \\right\\} \\\\\n&=  \\sqrt{\\frac{2\\pi n}{\\prod_{i=1}^m \\left (2\\pi n p_i + 2\\pi \\sqrt{n} x_i\\right)}} \\int_{\\chi^2_P(\\{\\sqrt{n} x_i+n p_i\\},\\{p_i\\}) > T} \\left \\{\\prod_{i=1}^{m-1} {\\sqrt{n} dx_i}\\right \\}\\times \\\\\n&\\qquad \\qquad \\times \\left \\{ \\prod_{i=1}^{m-1} \\exp\\left[-\\left(n p_i + \\sqrt{n} x_i \\right) \\ln \\left(1+\\frac{x_i}{\\sqrt{n} p_i}\\right)\\right] \\exp \\left[ -\\left(n p_m-\\sqrt{n} \\sum_{i=1}^{m-1}x_i\\right) \\ln \\left(1-\\frac{\\sum_{i=1}^{m-1}{x_i}}{\\sqrt{n}p_m}\\right) \\right] \\right \\}\n\\end{align}</math>\n\nBy [[Taylor expansion|expanding]] the logarithm and taking the leading terms in <math>n</math>, we get\n\n:<math> P(\\chi^2_P(\\{p_i\\}) > T) \\sim  \\frac{1}{\\sqrt{(2\\pi)^{m-1} \\prod_{i=1}^{m} p_i}}  \\int_{\\chi^2_P(\\{\\sqrt{n} x_i+n p_i\\},\\{p_i\\}) > T} \\left \\{ \\prod_{i=1}^{m-1} dx_i\\right \\} \\prod_{i=1}^{m-1} \\exp\\left [-\\frac{1}{2}\\sum_{i=1}^{m-1}\\frac{x_i^2}{p_i} -\\frac{1}{2p_m}\\left (\\sum_{i=1}^{m-1}{x_i} \\right )^2 \\right]</math>\n\nNow, it should be noted that Pearson's chi, <math>\\chi^2_P(\\{k_i\\},\\{p_i\\}) = \\chi^2_P(\\{\\sqrt{n} x_i+n p_i\\},\\{p_i\\})</math>, is precisely the argument of the exponent (except for the -1/2; note that the final term in the exponent's argument is equal to <math>(k_m-n p_m)^2/(n p_m)</math>).\n\nThis argument can be written as:\n\n:<math>-\\frac{1}{2}\\sum_{i,j=1}^{m-1}x_i A_{ij} x_j, \\qquad i,j = 1, \\cdots, m-1, \\quad A_{ij} = \\tfrac{\\delta_{ij}}{p_i} + \\tfrac{1}{p_m}.</math>\n\n<math>A</math> is a regular symmetric <math>(m-1) \\times (m-1)</math> matrix, and hence [[diagonalizable]]. It is therefore possible to make a linear change of variables in <math>\\{x_i\\}</math> so as to get <math>m-1</math> new variables <math>\\{y_i\\}</math> so that:\n\n:<math>\\sum_{i,j=1}^{m-1}x_i A_{ij} x_j = \\sum_{i=1}^{m-1}y_i^2.</math>\n\nThis linear change of variables merely multiplies the integral by a constant [[Jacobian matrix and determinant|Jacobian]], so we get:\n\n:<math>P(\\chi^2_P(\\{p_i\\}) > T) \\sim C \\int_{\\sum_{i=1}^{m-1} y_i^2 > T} \\left\\{\\prod_{i=1}^{m-1} dy_i \\right\\} \\prod_{i=1}^{m-1} \\exp\\left[-\\frac{1}{2}\\left(\\sum_{i=1}^{m-1} y_i^2 \\right)\\right]</math>\n\nWhere C is a constant.\n\nThis is the probability that squared sum of <math>m-1</math> independent normally distributed variables of zero mean and unit variance will be greater than T, namely that <math>\\chi^2</math> with <math>m-1</math> degrees of freedom is larger than T.\n\nWe have thus shown that at the limit where <math>n \\to \\infty,</math> the distribution of Pearson's chi approaches the chi distribution with <math>m-1</math> degrees of freedom.\n{{hidden end}}\n\n==Examples==\n\n===Fairness of dice===\nA 6-sided dice is thrown 60 times. The number of times it lands with 1, 2, 3, 4, 5 and 6 face up is 5, 8, 9, 8, 10 and 20, respectively. Is the die biased, according to the Pearson's chi-squared test at a significance level of 95% and/or 99%?\n\n''n'' = 6 as there are 6 possible outcomes, 1 to 6. The null hypothesis is that the die is unbiased, hence each number is expected to occur the same number of times, in this case, {{sfrac|60|''n''}} = 10. The outcomes can be tabulated as follows:\n{| class=\"wikitable\" style=\"text-align:center;\"\n|-\n! style=\"padding:0 1em;\"|''i''\n! style=\"padding:0 1em;\"|''O<sub>i</sub>''\n! style=\"padding:0 1em;\"|''E<sub>i</sub>''\n! ''O<sub>i</sub>''&thinsp;&minus;''E<sub>i</sub>''\n! (''O<sub>i</sub>''&thinsp;&minus;''E<sub>i</sub>''&thinsp;)<sup>2</sup>\n! {{sfrac|(''O<sub>i</sub>''&thinsp;&minus;''E<sub>i</sub>''&thinsp;)<sup>2</sup>|''E<sub>i</sub>''}}\n|-\n| 1 ||  5 || 10 || &minus;5 ||  25 ||  2.5\n|-\n| 2 ||  8 || 10 || &minus;2 ||   4 ||  0.4\n|-\n| 3 ||  9 || 10 || &minus;1 ||   1 ||  0.1\n|-\n| 4 ||  8 || 10 || &minus;2 ||   4 ||  0.4\n|-\n| 5 || 10 || 10 ||        0 ||   0 ||  0\n|-\n| 6 || 20 || 10 ||       10 || 100 || 10\n|-\n| colspan=\"5\" style=\"text-align:right;\"|Sum || 13.4\n|}\n\nThe number of degrees of freedom is ''n'' &minus; 1 = 5. The ''Upper-tail critical values of chi-square distribution'' table gives a critical value of 11.070 at 95% significance level:\n{| class=\"wikitable\" style=\"text-align:center;font-size:90%;line-height:0.9;\"\n|-\n! rowspan=\"2\"|Degrees<br /> of<br />freedom\n! colspan=\"5\"|Probability less than the critical value\n|-\n!    0.90 ||    ''0.95'' ||   0.975 ||    ''0.99'' ||   0.999\n|-\n!   ''5''\n|   9.236 ||  ''11.070'' ||  12.833 ||  ''15.086'' ||  20.515\n|}\n\nAs the chi-squared statistic of 13.4 exceeds this critical value, we reject the null hypothesis and conclude that the die is biased at 95% significance level.\n\nAt 99% significance level, the critical value is 15.086. As the chi-squared statistic does not exceed it, we fail to reject the null hypothesis and thus conclude that there is insufficient evidence to show that the die is biased at 99% significance level.\n\n===Goodness of fit===\n{{main|Goodness of fit}}\nIn this context, the [[frequency distribution|frequencies]] of both theoretical and empirical distributions are unnormalised counts, and for a chi-squared test the total sample sizes <math>N</math> of both these distributions (sums of all cells of the corresponding [[contingency tables]]) have to be the same.\n\nFor example, to test the hypothesis that a random sample of 100 people has been drawn from a population in which men and women are equal in frequency, the observed number of men and women would be compared to the theoretical frequencies of 50 men and 50 women. If there were 44 men in the sample and 56 women, then\n\n:<math> \\chi^2 = {(44 - 50)^2 \\over 50} + {(56 - 50)^2 \\over 50} = 1.44.</math>\n\nIf the null hypothesis is true (i.e., men and women are chosen with equal probability), the test statistic will be drawn from a chi-squared distribution with one [[degrees of freedom (statistics)|degree of freedom]] (because if the male frequency is known, then the female frequency is determined).\n\nConsultation of the [[chi-squared distribution]] for 1 degree of freedom shows that the [[probability]] of observing this difference (or a more extreme difference than this) if men and women are equally numerous in the population is approximately 0.23. This probability is higher than conventional criteria for [[statistical significance]] (0.01 or 0.05), so normally we would not reject the null hypothesis that the number of men in the population is the same as the number of women (i.e., we would consider our sample within the range of what we would expect for a 50/50 male/female ratio.)\n\n==Problems==\nThe approximation to the chi-squared distribution breaks down if expected frequencies are too low. It will normally be acceptable so long as no more than 20% of the events have expected frequencies below 5. Where there is only 1 degree of freedom, the approximation is not reliable if expected frequencies are below 10. In this case, a better approximation can be obtained by reducing the absolute value of each difference between observed and expected frequencies by 0.5 before squaring; this is called [[Yates's correction for continuity]].\n\nIn cases where the expected value, E, is found to be small (indicating a small underlying population probability, and/or a small number of observations), the normal approximation of the multinomial distribution can fail, and in such cases it is found to be more appropriate to use the [[G-test]], a [[likelihood-ratio test|likelihood ratio]]-based test statistic.  When the total sample size is small, it is necessary to use an appropriate exact test, typically either the [[binomial test]] or (for contingency tables) [[Fisher's exact test]]. This test uses the conditional distribution of the test statistic given the marginal totals; however, it does not assume that the data were generated from an experiment in which the marginal totals are fixed{{dubious|date=April 2019}} and is valid whether or not that is the case.{{dubious|date=April 2019}}{{citation needed|date=April 2019}}\n\nIt can be shown that the <math>\\chi^2</math> test is a low order approximation of the <math>\\Psi</math> test.<ref>{{cite book |author-link=Edwin Thompson Jaynes |last=Jaynes |first=E.T. |year=2003 |title=Probability Theory: The Logic of Science |publisher=C. University Press |ISBN=978-0-521-59271-0 |page=298 |url=http://www-biba.inrialpes.fr/Jaynes/prob.html}} (''Link is to a fragmentary edition of March&nbsp;1996''.)</ref> The above reasons for the above issues become apparent when the higher order terms are investigated.\n\n==See also==\n*[[G-test]], test to which chi-squared test is an approximation\n*[[Degrees of freedom (statistics)]]\n*[[Fisher's exact test]]\n*[[Median test]]\n*[[Lexis ratio]], earlier statistic, replaced by chi-squared\n*[[Nomogram#Chi-squared test computation nomogram|Chi-squared nomogram]]\n*[[Deviance (statistics)]], another measure of the quality of fit\n*[[Mann–Whitney U test]]\n*[[Cramér's V]] – a measure of correlation for the chi-squared test\n*[[Minimum chi-square estimation]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n{{refbegin}}\n*{{Cite journal | last1 = Chernoff | first1 = H. | authorlink1 = Herman Chernoff | last2 = Lehmann | first2 = E. L. | doi = 10.1214/aoms/1177728726 | title = The Use of Maximum Likelihood Estimates in <math>\\chi^2</math> Tests for Goodness of Fit | journal = The Annals of Mathematical Statistics | volume = 25 | issue = 3 | pages = 579–586 | year = 1954 | pmid =  | pmc = }}\n*{{Cite journal | last = Plackett | first = R. L. | authorlink = Robin Plackett | doi = 10.2307/1402731| title = Karl Pearson and the Chi-Squared Test | journal = International Statistical Review | volume = 51 | issue = 1 | pages = 59–72 | year = 1983 | jstor = 1402731 | publisher = [[International Statistical Institute]] (ISI) }}\n*{{cite book |title=A guide to chi-squared testing |last1=Greenwood|authorlink= Cindy Greenwood |first1=P.E. |last2=Nikulin |first2=M.S. |year=1996 |publisher=Wiley |location=New York |isbn=0-471-55779-X |ref=harv }}\n{{refend}}\n\n{{Statistics}}\n{{Use dmy dates|date=September 2010}}\n\n[[Category:Statistical tests for contingency tables]]\n[[Category:Normality tests]]\n[[Category:Statistical approximations]]"
    },
    {
      "title": "Shapiro–Francia test",
      "url": "https://en.wikipedia.org/wiki/Shapiro%E2%80%93Francia_test",
      "text": "{{more citations needed|date=April 2017}}\n\nThe '''Shapiro–Francia test''' is a [[Normality test|statistical test for the normality]] of a population, based on sample data. It was introduced by [[Samuel Sanford Shapiro|S. S. Shapiro]] and R. S. Francia in 1972 as a simplification of the [[Shapiro–Wilk test]].<ref name=\"Shapiro–Francia\">S. S. Shapiro and R. S. Francia, \"An approximate analysis of variance test for normality\", Journal of the\nAmerican Statistical Association 67 (1972) 215–216.</ref>\n\n== Theory ==\nLet <math>x_{(i)}</math> be the <math>i</math>th ordered value from our size-<math>n</math> sample. For example, if the sample consists of the values <math>\\left\\{ 5.6, -1.2, 7.8, 3.4 \\right\\}</math>, <math>x_{(2)} = 3.4</math>, because that is the second-lowest value. Let <math>m_{i:n}</math> be the [[mean]] of the <math>i</math>th [[order statistic]] when making <math>n</math> independent draws from a [[normal distribution]]. For example, <math>m_{2:4} \\approx -0.297</math>, meaning that the second-lowest value in a sample of four draws from a normal distribution is typically about 0.297 standard deviations below the mean.<ref name=\"Arnold-Balakrishnan-Nagaraja\">B. C. Arnold, N. Balakrishnan, H. N. Nagaraja, A First Course in Order Statistics, Classics in Applied Mathematics 54, SIAM, 1992</ref>  Form the [[Pearson correlation coefficient]] between the <math>x</math> and the <math>m</math>:\n\n:<math>W' = \\frac{\\operatorname{cov}(x, m)}{\\sigma_x \\sigma_m} =\n\\frac{\\sum_{i=1}^n (x_{(i)} - \\bar{x}) (m_i - \\bar{m})}{\\sqrt{\\left( \\sum_{i=1}^n (x_{(i)} - \\bar{x})^2 \\right) \\left( \\sum_{i=1}^n (m_i - \\bar{m})^2 \\right)}}</math>\n\nUnder the [[null hypothesis]] that the data is drawn from a [[normal distribution]], this correlation will be strong, so <math>W'</math> values will cluster just under 1, with the peak becoming narrower and closer to 1 as <math>n</math> increases. If the data deviate strongly from a normal distribution, <math>W'</math> will be smaller.<ref name=\"Shapiro–Francia\" />\n\nThis test is a formalization of the older practice of forming a [[q-q plot]] to compare two distributions, with the <math>x</math> playing the role of the quantile points of the sample distribution and the <math>m</math> playing the role of the corresponding quantile points of a [[normal distribution]].\n\nCompared to the [[Shapiro–Wilk test]] statistic <math>W</math>, the Shapiro–Francia test statistic <math>W'</math> is easier to compute, because it does not require that we form and invert the matrix of covariances between order statistics.\n\n== Practice ==\n\nThere is no known [[closed-form expression|closed-form analytic expression]] for the values of <math>m_{i:n}</math> required by the test. There, are however, several approximations that are adequate for most practical purposes.<ref name=\"Arnold-Balakrishnan-Nagaraja\" />\n\nThe exact form of the null distribution of <math>W'</math> is known only for <math>n=3</math>.<ref name=\"Shapiro–Francia\" /> [[Monte Carlo method|Monte-Carlo]] simulations have shown that the transformed statistic <math>\\ln(1-W')</math> is nearly normally distributed, with values of the mean and standard deviation that vary slowly with <math>n</math> in an easily parameterized form.<ref name=\"Royston\">Royston, \"A toolkit for testing for non-normality in complete and censored samples\", Statistician 42 (1993) 37–43</ref>\n\n== Power ==\n\nComparison studies have concluded that order statistic correlation tests such as Shapiro–Francia and [[Shapiro–Wilk test|Shapiro–Wilk]] are among the most [[Statistical power|powerful]] of the established [[Normality test|statistical tests for normality]].<ref name=\"Razali-Wah\">N. M. Razali and Y. B. Wah, \"Power Comparisons of Shapiro–Wilk, Kolmogorov–Smirnov, Lilliefors, and Anderson–Darling Tests\", Journal of Statistical Modeling and Analytics 2 (2011) 21</ref> One might assume that the covariance-adjusted weighting of different order statistics used by the [[Shapiro–Wilk test]] should make it slightly better, but in practice the Shapiro–Wilk and Shapiro–Francia variants are about equally good. In fact, the Shapiro–Francia variant actually exhibits more power to distinguish some alternative hypothesis.<ref name=\"Ahmad-Kahn\">F. Ahmad and  R. A. Khan, \"A power comparison of various normality tests\", Pakistan Journal of Statistics and Operation Research 11 (2015)</ref>\n\n==References==\n<references/>\n\n{{Statistics}}\n\n{{DEFAULTSORT:Shapiro-Francia test}}\n[[Category:Statistical tests]]\n[[Category:Normality tests]]"
    },
    {
      "title": "Shapiro–Wilk test",
      "url": "https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test",
      "text": "The '''Shapiro–Wilk test''' is a [[Normality test|test of normality]] in frequentist [[statistics]]. It was published in 1965 by [[Samuel Sanford Shapiro]] and [[Martin Wilk]].<ref name=\"Shapiro–Wilk\" />\n\n==Theory==\nThe Shapiro–Wilk test tests the [[null hypothesis]] that a [[statistical sample|sample]] ''x''<sub>1</sub>, ..., ''x''<sub>''n''</sub> came from a [[normal distribution|normally distributed]] population. The [[test statistic]] is\n\n:<math>W = {\\left(\\sum_{i=1}^n a_i x_{(i)}\\right)^2 \\over \\sum_{i=1}^n (x_i-\\overline{x})^2},</math>\n\nwhere\n\n* <math>x_{(i)}</math> (with parentheses enclosing the subscript index ''i''; not to be confused with <math>x_i</math>) is the ''i''th [[order statistic]], i.e., the ''i''th-smallest number in the sample;\n* <math>\\overline{x} = \\left( x_1 + \\cdots + x_n \\right) / n</math> is the sample mean.\n\nThe coefficients <math>a_i</math> are given by:<ref name=\"Shapiro–Wilk\">{{cite journal\n |last=Shapiro |first=S. S.\n |last2=Wilk |first2=M. B. |authorlink2=Martin Wilk\n |year=1965\n |title=An analysis of variance test for normality (complete samples)\n |journal=[[Biometrika]]\n |volume=52 |issue=3–4 |pages=591–611\n |doi=10.1093/biomet/52.3-4.591 |jstor=2333709 | mr = 205384\n}} p.&nbsp;593</ref>\n:<math>(a_1,\\dots,a_n) = {m^{\\mathsf{T}} V^{-1} \\over C},</math>\nwhere ''C'' is a [[vector norm]]:<ref>[https://math.mit.edu/~rmd/465/shapiro.pdf]</ref>\n:<math>C = \\| V^{-1} m \\| = (m^{\\mathsf{T}} V^{-1}V^{-1}m)^{1/2}</math>\nand the vector ''m'',\n:<math>m = (m_1,\\dots,m_n)^{\\mathsf{T}}\\,</math>\nis made of the [[expected value]]s of the [[order statistic]]s of [[independent and identically distributed random variables]] sampled from the standard normal distribution; finally, <math>V</math> is the [[covariance matrix]] of those normal order statistics.<ref>[https://apps.dtic.mil/dtic/tr/fulltext/u2/a053857.pdf]</ref>\n\n==Interpretation==\nThe [[Statistical hypothesis testing|null-hypothesis]] of this test is that the population is normally distributed. Thus, on the one hand, if the [[p-value|''p'' value]] is less than the chosen [[alpha level]], then the null hypothesis is rejected and there is evidence that the data tested are not normally distributed. On the other hand, if the ''p'' value is greater than the chosen alpha level, then the null hypothesis that the data came from a normally distributed population can not be rejected (e.g., for an alpha level of .05, a data set with a ''p'' value of less than .05 rejects the null hypothesis that the data are from a normally distributed population).<ref>{{cite web |url= http://www.jmp.com/support/notes/35/406.html | title=How do I interpret the Shapiro–Wilk test for normality?\n|first= |last= |work=JMP |year=2004 |accessdate=March 24, 2012}}</ref> Like most [[statistical_significance|statistical significance tests]], if the sample size is sufficiently large this test may detect even trivial departures from the null hypothesis (i.e., although there may be some [[Statistical_significance#Effect_size|statistically significant effect]], it may be too small to be of any practical significance); thus, additional investigation of the ''effect size'' is typically advisable, e.g., a [[Q–Q plot]] in this case.<ref>{{cite book|last=Field|first=Andy|title=Discovering statistics using SPSS |year=2009|publisher=SAGE Publications|location=Los Angeles [i.e. Thousand Oaks, Calif.]|isbn=978-1-84787-906-6|page=143|edition=3rd}}</ref>\n\n==Power analysis==\n[[Monte Carlo method|Monte Carlo simulation]] has found that Shapiro–Wilk has the best [[Statistical power|power]] for a given [[Statistical significance|significance]], followed closely by [[Anderson–Darling test|Anderson–Darling]] when comparing the Shapiro–Wilk, [[Kolmogorov–Smirnov test|Kolmogorov–Smirnov]], [[Lilliefors test|Lilliefors]] and Anderson–Darling tests.<ref>{{cite journal|last=Razali|first=Nornadiah|author2=Wah, Yap Bee |title=Power comparisons of Shapiro–Wilk, Kolmogorov–Smirnov, Lilliefors and Anderson–Darling tests|journal=Journal of Statistical Modeling and Analytics|year=2011|volume=2|issue=1|pages=21–33|url=https://www.researchgate.net/publication/267205556|accessdate=30 March 2017}}</ref>\n\n==Approximation==\nRoyston proposed an alternative method of calculating the coefficients vector by providing an algorithm for calculating values, which extended the sample size to 2,000.<ref>{{cite journal|last=Royston|first=Patrick|title=Approximating the Shapiro–Wilk ''W''-test for non-normality|journal=Statistics and Computing|date=September 1992|volume=2|issue=3|pages=117–119|doi=10.1007/BF01891203}}</ref> This technique is used in several software packages including Stata,<ref>{{cite journal|last=Royston|first=Patrick|title=Shapiro–Wilk and Shapiro–Francia Tests|journal=Stata Technical Bulletin, StataCorp LP|volume=1|issue=3}}</ref><ref>[https://www.stata.com/manuals13/rswilk.pdf Shapiro–Wilk and Shapiro–Francia tests for normality]</ref> SPSS and SAS.<ref>{{cite journal|last=Park|first=Hun Myoung|title=Univariate Analysis and Normality Test Using SAS, Stata, and SPSS|journal=[working paper]|date=2002–2008|url=http://rt.uits.iu.edu/visualization/analytics/docs/normality-docs/normality.pdf|accessdate=26 February 2014}}</ref> Rahman and Govidarajulu extended the sample size further up to 5,000.<ref>{{cite journal|last=Rahman und Govidarajulu|title=A modification of the test of Shapiro and Wilk for normality|journal=Journal of Applied Statistics|date=1997|volume=24|issue=2|pages=219–236|doi=10.1080/02664769723828}}</ref>\n\n==See also==\n* [[Anderson–Darling test]]\n* [[Cramér–von Mises criterion]]\n* [[D'Agostino's K-squared test]]\n* [[Kolmogorov–Smirnov test]]\n* [[Lilliefors test]]\n* [[Normal probability plot]]\n* [[Shapiro-Francia test]]\n\n==References==\n<references/>\n\n==External links==\n* [http://www.real-statistics.com/tests-normality-and-symmetry/statistical-tests-normality-symmetry/shapiro-wilk-test/ Worked example using Excel]\n* [http://lib.stat.cmu.edu/apstat/R94 Algorithm AS R94 (Shapiro Wilk) FORTRAN code]\n* [http://cran.us.r-project.org/doc/manuals/R-intro.html#Examining-the-distribution-of-a-set-of-data Exploratory analysis using the Shapiro–Wilk normality test in R]\n* [http://www.real-statistics.com/tests-normality-and-symmetry/statistical-tests-normality-symmetry/shapiro-wilk-expanded-test/ Real Statistics Using Excel: the Shapiro-Wilk Expanded Test] \n\n{{Statistics}}\n\n{{DEFAULTSORT:Shapiro-Wilk Test}}\n[[Category:Statistical tests]]\n[[Category:Normality tests]]"
    },
    {
      "title": "Generating function",
      "url": "https://en.wikipedia.org/wiki/Generating_function",
      "text": "{{About|generating functions in mathematics|generating functions in classical mechanics|Generating function (physics)|signalling molecule|Epidermal growth factor|generators in computer programming|Generator (computer programming)|the moment generating function in statistics|Moment generating function}}\n{{Multiple issues|\n{{Technical|date=March 2018}}\n{{Very long|rps=76|date=March 2018}}\n}}\n\nIn [[mathematics]], a '''generating function''' is a way of encoding an [[infinite sequence]] of numbers (''a''<sub>''n''</sub>) by treating them as the [[coefficient]]s of a [[formal power series|power series]]. This formal power series is the generating function. Unlike an ordinary series, this [[formal series]] is allowed to [[Divergent series|diverge]], meaning that the generating function is not always a true function and the \"variable\" is actually an [[Indeterminate (variable)|indeterminate]]. Generating functions were first introduced by [[Abraham de Moivre]] in 1730, in order to solve the general linear recurrence problem.<ref>[[Donald Knuth|Donald E. Knuth]], ''[[The Art of Computer Programming]], Volume 1 Fundamental Algorithms (Third Edition)'' Addison-Wesley. {{ISBN|0-201-89683-4}}. Section 1.2.9: \"Generating Functions\".</ref> One can generalize to formal series in more than one indeterminate, to encode information about arrays of numbers indexed by several natural numbers.\n\nThere are various types of generating functions, including '''ordinary generating functions''', '''exponential generating functions''', '''Lambert series''', '''Bell series''', and '''Dirichlet series'''; definitions and examples are given below. Every sequence in principle has a generating function of each type (except that Lambert and Dirichlet series require indices to start at 1 rather than 0), but the ease with which they can be handled may differ considerably. The particular generating function, if any, that is most useful in a given context will depend upon the nature of the sequence and the details of the problem being addressed.\n\nGenerating functions are often expressed in [[Closed-form expression|closed form]] (rather than as a series), by some expression involving operations defined for formal series. These expressions in terms of the indeterminate&nbsp;''x'' may involve arithmetic operations, differentiation with respect to&nbsp;''x'' and composition with (i.e., substitution into) other generating functions; since these operations are also defined for functions, the result looks like a function of&nbsp;''x''. Indeed, the closed form expression can often be interpreted as a function that can be evaluated at (sufficiently small) concrete values of ''x'', and which has the formal series as its [[series expansion]]; this explains the designation \"generating functions\". However such interpretation is not required to be possible, because formal series are not required to give a [[convergent series]] when a nonzero numeric value is substituted for&nbsp;''x''. Also, not all expressions that are meaningful as functions of&nbsp;''x'' are meaningful as expressions designating formal series; for example, negative and fractional powers of&nbsp;''x'' are examples of functions that do not have a corresponding formal power series.\n\nGenerating functions are not functions in the formal sense of a mapping from a [[Domain of a function|domain]] to a [[codomain]].  Generating functions are sometimes called '''generating series''',<ref>This alternative term can already be found in E.N. Gilbert (1956), \"Enumeration of Labeled graphs\", ''[[Canadian Journal of Mathematics]]'' 3, [https://books.google.com/books?id=x34z99fCRbsC&lpg=PA405&ots=eOp9p9mIoD&dq=%22generating%20series%22&lr=lang_en&pg=PA407#v=onepage&q=%22generating%20series%22&f=false p.&nbsp;405–411], but its use is rare before the year 2000; since then it appears to be increasing.</ref> in that a series of terms can be said to be the generator of its sequence of term coefficients.\n\n==Definitions==\n\n: ''A generating function is a device somewhat similar to a bag. Instead of carrying many little objects detachedly, which could be embarrassing, we put them all in a bag, and then we have only one object to carry, the bag.''\n:—[[George Pólya]], ''[[Mathematics and plausible reasoning]]'' (1954)\n\n:''A generating function is a clothesline on which we hang up a sequence of numbers for display.''\n:—[[Herbert Wilf]], ''[http://www.math.upenn.edu/~wilf/DownldGF.html Generatingfunctionology]'' (1994)\n\n===Ordinary generating function (OGF)===\n\nThe ''ordinary generating function'' of a sequence ''a''<sub>''n''</sub> is\n\n:<math>G(a_n;x)=\\sum_{n=0}^\\infty a_nx^n.</math>\n\nWhen the term ''generating function'' is used without qualification, it is usually taken to mean an ordinary generating function.\n\nIf ''a''<sub>''n''</sub> is the [[probability mass function]] of a [[discrete random variable]], then its ordinary generating function is called a [[probability-generating function]].\n\nThe ordinary generating function can be generalized to arrays with multiple indices. For example, the ordinary generating function of a two-dimensional array ''a''<sub>''m, n''</sub> (where ''n'' and ''m'' are natural numbers) is\n\n:<math>G(a_{m,n};x,y)=\\sum_{m,n=0}^\\infty a_{m,n}x^my^n.</math>\n\n===Exponential generating function (EGF)===\n\nThe ''exponential generating function'' of a sequence ''a''<sub>''n''</sub> is\n\n:<math>\\operatorname{EG}(a_n;x)=\\sum _{n=0}^\\infty a_n \\frac{x^n}{n!}.</math>\n\nExponential generating functions are generally more convenient than ordinary generating functions for [[combinatorial enumeration]] problems that involve labelled objects.<ref>Flajolet & Sedgewick (2009) p.95</ref>\n\n===Poisson generating function===\nThe ''Poisson generating function'' of a sequence ''a''<sub>''n''</sub> is\n\n:<math>\\operatorname{PG}(a_n;x)=\\sum _{n=0}^\\infty a_n e^{-x} \\frac{x^n}{n!} = e^{-x}\\, \\operatorname{EG}(a_n;x).</math>\n\n===Lambert series===\nThe ''[[Lambert series]]'' of a sequence ''a''<sub>''n''</sub> is\n\n:<math>\\operatorname{LG}(a_n;x)=\\sum _{n=1}^\\infty a_n \\frac{x^n}{1-x^n}.</math>\n\nThe Lambert series coefficients in the power series expansions <math>b_n := [x^n] \\operatorname{LG}(a_n;x)</math> for integers <math>n \\geq 1</math> are related by the [[divisor sum]] <math>b_n = \\sum_{d|n} a_d</math>. The main article provides several more classical, or at least well-known examples related to special [[arithmetic functions]] in [[number theory]]. \nNote that in a Lambert series the index ''n'' starts at 1, not at 0, as the first term would otherwise be undefined.\n\n===Bell series===\n\nThe [[Bell series]] of a sequence ''a''<sub>''n''</sub> is an expression in terms of both an indeterminate ''x'' and a prime ''p'' and is given by<ref>{{Apostol IANT}} pp.42–43</ref>\n\n:<math>\\operatorname{BG}_p(a_n;x)=\\sum_{n=0}^\\infty a_{p^n}x^n.</math>\n\n===Dirichlet series generating functions (DGFs)===\n\n[[Formal Dirichlet series]] are often classified as generating functions, although they are not strictly formal power series. The ''Dirichlet series generating function'' of a sequence ''a''<sub>''n''</sub> is<ref name=W56>Wilf (1994) p.56</ref>\n\n:<math>\\operatorname{DG}(a_n;s)=\\sum _{n=1}^\\infty \\frac{a_n}{n^s}.</math>\n\nThe Dirichlet series generating function is especially useful when ''a''<sub>''n''</sub> is a [[multiplicative function]], in which case it has an [[Euler product]] expression<ref name=W59>Wilf (1994) p.59</ref> in terms of the function's Bell series\n\n:<math>\\operatorname{DG}(a_n;s)=\\prod_{p} \\operatorname{BG}_p(a_n;p^{-s})\\,.</math>\n\nIf ''a''<sub>''n''</sub> is a [[Dirichlet character]] then its Dirichlet series generating function is called a [[Dirichlet L-series]].\nWe also have a relation between the pair of coefficients in the [[Lambert series]] expansions above and their DGFs. Namely, we can prove that <math>[x^n] \\operatorname{LG}(a_n; x) = b_n</math> if and only if <math>\\operatorname{DG}(a_n;s) \\zeta(s) = \\operatorname{DG}(b_n;s)</math> where <math>\\zeta(s)</math> is the [[Riemann zeta function]].<ref>{{cite book|last1=Hardy and Wright|title=An Introduction to the Theory of Numbers|date=2008|publisher=Oxford University Press|location=New York|page=339|edition=Sixth}}</ref>\n\n===Polynomial sequence generating functions===\n\nThe idea of generating functions can be extended to sequences of other objects. Thus, for example, polynomial sequences of [[binomial type]] are generated by\n\n:<math>e^{xf(t)}=\\sum_{n=0}^\\infty \\frac{p_n(x)}{n!} t^n</math>\n\nwhere ''p''<sub>''n''</sub>(''x'') is a sequence of polynomials and ''f''(''t'') is a function of a certain form. [[Sheffer sequence]]s are generated in a similar way.  See the main article [[generalized Appell polynomials]] for more information.\n\n== Ordinary generating functions ==\n\n=== Examples of generating functions for simple sequences ===\n\nPolynomials are a special case of ordinary generating functions, corresponding to finite sequences, or equivalently sequences that vanish after a certain point. These are important in that many finite sequences can usefully be interpreted as generating functions, such as the [[Poincaré polynomial]] and others.\n\nA key generating function is that of the constant sequence 1,&nbsp;1,&nbsp;1,&nbsp;1,&nbsp;1,&nbsp;1,&nbsp;1,&nbsp;1,&nbsp;1,&nbsp;..., whose ordinary generating function is the [[geometric series]]\n\n:<math>\\sum_{n=0}^\\infty x^n = \\frac{1}{1-x}.</math>\n\nThe left-hand side is the [[Maclaurin series]] expansion of the right-hand side.  Alternatively, the equality can be justified by multiplying the power series on the left by 1&nbsp;&minus;&nbsp;''x'', and checking that the result is the constant power series 1 (in other words, that all coefficients except the one of ''x''<sup>0</sup> are equal to 0). Moreover, there can be no other power series with this property.  The left-hand side therefore designates the [[multiplicative inverse]] of 1&nbsp;&minus;&nbsp;''x'' in the ring of power series.\n\nExpressions for the ordinary generating function of other sequences are easily derived from this one. For instance, the substitution ''x''&nbsp;→&nbsp;''ax'' gives the generating function for the [[Geometric progression|geometric sequence]] 1, ''a'', ''a''<sup>2</sup>, ''a''<sup>3</sup>, ... for any constant ''a'':\n\n:<math>\\sum_{n=0}^\\infty(ax)^n= \\frac{1}{1-ax}.</math>\n\n(The equality also follows directly from the fact that the left-hand side is the Maclaurin series expansion of the right-hand side.)  In particular,\n\n:<math>\\sum_{n=0}^\\infty(-1)^nx^n= \\frac{1}{1+x}.</math>\n\nOne can also introduce regular \"gaps\" in the sequence by replacing ''x'' by some power of ''x'', so for instance for the sequence 1,&nbsp;0,&nbsp;1,&nbsp;0,&nbsp;1,&nbsp;0,&nbsp;1,&nbsp;0,&nbsp;.... one gets the generating function\n\n:<math>\\sum_{n=0}^\\infty x^{2n}=\\frac{1}{1-x^2}.</math>\n\nBy squaring the initial generating function, or by finding the derivative of both sides with respect to ''x'' and making a change of running variable ''n''&nbsp;→&nbsp;''n''&nbsp;+&nbsp;1, one sees that the coefficients form the sequence 1,&nbsp;2,&nbsp;3,&nbsp;4,&nbsp;5,&nbsp;..., so one has\n\n:<math>\\sum_{n=0}^\\infty(n+1)x^n= \\frac{1}{(1-x)^2},</math>\n\nand the third power has as coefficients the [[triangular number]]s 1,&nbsp;3,&nbsp;6,&nbsp;10,&nbsp;15,&nbsp;21,&nbsp;... whose term ''n'' is the [[binomial coefficient]] <math>\\tbinom{n+2}2</math>, so that\n\n:<math>\\sum_{n=0}^\\infty\\binom{n+2}2 x^n= \\frac{1}{(1-x)^3}.</math>\n\nMore generally, for any non-negative integer ''k'' and non-zero real value ''a'', it is true that\n\n:<math>\\sum_{n=0}^\\infty a^n\\binom{n+k}k x^n= \\frac{1}{(1-ax)^{k+1}}\\,.</math>\n\nNote that, since\n\n:<math>2\\binom{n+2}2 - 3\\binom{n+1}1 + \\binom{n}0 = 2\\frac{(n+1)(n+2)}2 -3(n+1) + 1 = n^2,</math>\n\none can find the ordinary generating function for the sequence 0,&nbsp;1,&nbsp;4,&nbsp;9,&nbsp;16,&nbsp;... of [[square number]]s by linear combination of binomial-coefficient generating sequences:\n\n:<math>\nG(n^2;x) = \\sum_{n=0}^\\infty n^2x^n = \\frac{2}{(1-x)^3} - \\frac{3}{(1-x)^2} + \\frac{1}{1-x} = \\frac{x(x+1)}{(1-x)^3}.\n</math>\n\nWe may also expand alternately to generate this same sequence of squares as a sum of derivatives of the [[geometric series]] in the following form:\n\n:<math>\n\\begin{align}\nG(n^2;x) & =\\sum_{n=0}^\\infty n^2x^n = \\sum_{n=0}^\\infty n(n-1) x^n + \\sum_{n=0}^\\infty n x^n \\\\ \n         & = x^2 D^2\\left[\\frac{1}{1-x}\\right] + x D\\left[\\frac{1}{1-x}\\right] \\\\ \n         & = \\frac{2 x^2}{(1-x)^3} + \\frac{x}{(1-x)^2} =\\frac{x(x+1)}{(1-x)^3}.\n\\end{align} \n</math>\n\nBy induction, we can similarly show for positive integers <math>m \\geq 1</math> that\n<ref>{{cite journal|first1= Michael Z. |last1=Spivey|title=Combinatorial Sums and Finite Differences|year=2007 |journal = Discrete Math. |doi=10.1016/j.disc.2007.03.052| volume=307|number=24|pages=3130–3146|mr=2370116}}</ref><ref>{{cite arxiv|first1=R. J. |last1=Mathar|year=2012|eprint=1207.5845|title=Yet another table of integrals}} v4  eq. (0.4)</ref>\n\n:<math>n^m = \\sum_{j=0}^m \\left\\{\\begin{matrix} m \\\\ j \\end{matrix} \\right\\} \\frac{n!}{(n-j)!}, </math>\n\nwhere <math>\\left\\{\\begin{matrix} n \\\\ k \\end{matrix} \\right\\}</math> denote the [[Stirling numbers of the second kind]] and where the generating function <math>\\sum_{n \\geq 0} n! / (n-j)! \\, z^n = j! \\cdot z^j / (1-z)^{j+1}</math>, so that we can form the analogous generating functions over the integral <math>m</math>-th powers generalizing the result in the square case above. In particular, since we can write <math>\\frac{z^k}{(1-z)^{k+1}} = \\sum_{i=0}^k \\binom{k}{i} \\frac{(-1)^{k-i}}{(1-z)^{i+1}}</math>, we can apply a well-known finite sum identity involving the [[Stirling numbers]] to obtain that<ref>See Table 265 in Section 6.1 of ''Concrete Mathematics'' for finite sum identities involving the Stirling number triangles.</ref>\n\n:<math>\\sum_{n \\geq 0} n^m z^n = \\sum_{j=0}^{m} \\left\\{\\begin{matrix} m+1 \\\\ j+1 \\end{matrix} \\right\\} \\frac{(-1)^{m-j} j!}{(1-z)^{j+1}}. </math>\n\n=== Rational functions ===\n{{Main|Linear recursive sequence}}\nThe ordinary generating function of a sequence can be expressed as a [[rational function]] (the ratio of two finite-degree polynomials) if and only if the sequence is a [[linear recursive sequence]] with constant coefficients; this generalizes the examples above. Conversely, every sequence generated by a fraction of polynomials satisfies a linear recurrence with constant coefficients; these coefficients are identical to the coefficients of the fraction denominator polynomial (so they can be directly read off). This observation shows it is easy to solve for generating functions of sequences defined by a linear [[finite difference equation]] with constant coefficients, and then hence, for explicit closed-form formulas for the coefficients of these generating functions. The prototypical example here is to derive [[Binet's formula]] for the [[Fibonacci numbers]] via generating function techniques.\n\nWe also notice that the class of rational generating functions precisely corresponds to the generating functions that enumerate ''quasi-polynomial'' sequences of the form <ref name=\"GFLECT\">See section 2.4 in Lando's book ''Lectures on Generating Functions'' (2002).</ref>\n\n:<math>f_n = p_1(n) \\rho_1^n + \\cdots + p_{\\ell}(n) \\rho_{\\ell}^n, </math>\n\nwhere the reciprocal roots, <math>\\rho_i \\in \\mathbb{C}</math>, are fixed scalars and where <math>p_i(n)</math> is a polynomial in <math>n</math> for all <math>1 \\leq i \\leq \\ell</math>.\n\nIn general, [[Generating function transformation#Hadamard products and diagonal generating functions|Hadamard products]] of rational functions produce rational generating functions. Similarly, if <math>F(s, t) := \\sum_{m,n \\geq 0} f(m, n) w^m z^n</math> is a bivariate rational generating function, then its corresponding ''diagonal generating function'', <math>\\operatorname{diag}(F) := \\sum_{n \\geq 0} f(n, n) z^n</math>, is ''algebraic''. For example, if we let <ref>Example from Section 6.3 of R. P. Stanley's ''Enumerative Combinatorics'' (Volume 2).</ref>\n\n:<math>F(s, t) := \\sum_{i,j \\geq 0} \\binom{i+j}{i} s^i t^j = \\frac{1}{1-s-t}, </math>\n\nthen this generating function's diagonal coefficient generating function is given by the well-known OGF formula\n\n:<math>\\operatorname{diag}(F) = \\sum_{n \\geq 0} \\binom{2n}{n} z^n = \\frac{1}{\\sqrt{1-4z}}. </math>\n\nThis result is computed in many ways, including [[Cauchy's integral formula]] or [[contour integration]], taking complex [[residue (complex analysis)|residue]]s, or by direct manipulations of [[formal power series]] in two variables.\n\n=== Operations on generating functions ===\n\n==== Multiplication yields convolution ====\n{{Main|Cauchy product}}\nMultiplication of ordinary generating functions yields a discrete [[convolution]] (the [[Cauchy product]]) of the sequences.  For example, the sequence of cumulative sums (compare to the slightly more general [[Euler–Maclaurin formula]])\n\n:<math>(a_0, a_0 + a_1, a_0 + a_1 + a_2, \\ldots)</math>\n\nof a sequence with ordinary generating function ''G''(''a<sub>n</sub>'';&nbsp;''x'') has the generating function\n\n:<math>G(a_n; x) \\cdot \\frac{1}{1-x}</math>\n\nbecause 1/(1&nbsp;−&nbsp;''x'') is the ordinary generating function for the sequence (1, 1, ...). See also the [[Generating function#Convolution .28Cauchy products.29|section on convolutions]] in the applications section of this article below for further examples of problem solving with convolutions of generating functions and interpretations.\n\n==== Shifting sequence indices ====\n\nFor integers <math>m \\geq 1</math>, we have the following two analogous identities for the modified generating functions enumerating the shifted sequence variants of <math>\\langle g_{n-m} \\rangle</math> and <math>\\langle g_{n+m} \\rangle</math>, respectively:\n\n:<math>\n\\begin{align} \nz^m G(z) & = \\sum_{n \\geq m} g_{n-m} z^n \\\\ \n\\frac{G(z) - g_0 - g_1 z - \\cdots - g_{m-1} z^{m-1}}{z^m} & = \\sum_{n \\geq 0} g_{n+m} z^n. \n\\end{align} \n</math>\n\n==== Differentiation and integration of generating functions ====\n\nWe have the following respective power series expansions for the first derivative of a generating function and its integral:\n\n:<math>\n\\begin{align} \nG^\\prime(z) & = \\sum_{n \\geq 0} (n+1) g_{n+1} z^n \\\\ \nz \\cdot G^{\\prime}(z) & = \\sum_{n \\geq 0} n g_{n} z^n \\\\ \n\\int_0^z G(t) \\, dt & = \\sum_{n \\geq 1} \\frac{g_{n-1}}{n} z^n. \n\\end{align} \n</math>\n\nThe differentiation–multiplication operation of the second identity can be repeated <math>k</math> times to multiply the sequence by <math>n^k</math>, but that requires alternating between differentiation and multiplication. If instead doing <math>k</math> differentiations in sequence, the effect is to multiply by the <math>k</math><sup>th</sup> [[falling factorial]]:\n\n:<math> z^k G^{(k)}(z) = \\sum_{n \\geq 0} n^{\\underline{k}} g_n z^n = \\sum_{n \\geq 0} n (n-1) \\dotsb (n-k+1) g_n z^n \\text{ for all } k \\in \\mathbb{N}. </math>\n\nUsing the [[Stirling numbers of the second kind]], that can be turned into another formula for multiplying by <math>n^k</math> as follows (see the main article on [[Generating function transformation#Derivative transformations|generating function transformations]]):\n\n:<math> \\sum_{j=0}^k \\left\\{\\begin{matrix} k \\\\ j \\end{matrix}\\right\\} z^j F^{(j)}(z) = \\sum_{n \\geq 0} n^k f_n z^n \\text{ for all } k \\in \\mathbb{N}. </math>\n\nA negative-order reversal of this sequence powers formula corresponding to the operation of repeated integration is defined by the [[Generating function transformation#Derivative transformations|zeta series transformation]] and its generalizations defined as a derivative-based [[generating function transformation|transformation of generating functions]], or alternately termwise by an performing an [[Generating function transformation#Polylogarithm series transformations|integral transformation]] on the sequence generating function. Related operations of performing [[fractional calculus|fractional integration]] on a sequence generating function are discussed [[Generating function transformation#Fractional integrals and derivatives|here]].\n\n==== Enumerating arithmetic progressions of sequences ====\nIn this section we give formulas for generating functions enumerating the sequence <math>\\{f_{an+b}\\}</math> given an ordinary generating function <math>F(z)</math> where <math>a, b \\in \\mathbb{N}</math>, <math>a \\geq 2</math>, and <math> 0 \\leq b < a</math> (see the [[generating function transformation|main article on transformations]]). For <math> a=2 </math>, this is simply the familiar decomposition of a function into [[even and odd functions|even and odd parts]] (i.e., even and odd powers):\n\n:<math> \\sum_{n \\geq 0} f_{2n} z^{2n} = \\frac{1}{2}\\left(F(z) + F(-z)\\right)</math> \n:<math> \\sum_{n \\geq 0} f_{2n+1} z^{2n+1} = \\frac{1}{2}\\left(F(z) - F(-z)\\right).</math>\n\nMore generally, suppose that <math>a \\geq 3</math> and that <math>\\omega_a = \\exp\\left( 2\\pi\\imath / a \\right)</math> denotes the <math>a</math><sup>th</sup> [[root of unity|primitive root of unity]]. Then, as an application of the [[discrete Fourier transform]], we have the formula<ref name=\"TAOCPV1\">See Section 1.2.9 in Knuth's ''The Art of Computer Programming'' (Vol. 1).</ref>\n\n:<math>\\sum_{n \\geq 0} f_{an+b} z^{an+b} = \\frac{1}{a} \\sum_{m=0}^{a-1} \\omega_a^{-mb} F\\left(\\omega_a^{m} z\\right).</math>\n\nFor integers <math>m \\geq 1</math>, another useful formula providing somewhat ''reversed'' floored arithmetic progressions — effectively repeating each coefficient <math>m</math> times — are generated by the identity<ref>Solution to exercise 7.36 on page 569 in Graham, Knuth and Patshnik.</ref>\n\n:<math>\\sum_{n \\geq 0} f_{\\lfloor \\frac{n}{m} \\rfloor} z^n = \\frac{1-z^m}{1-z} F(z^m) = \\left(1+z+\\cdots+z^{m-2} + z^{m-1}\\right) F(z^m).</math>\n\n===P-recursive sequences and holonomic generating functions===\n\n====Definitions====\n\nA formal power series (or function) <math>F(z)</math> is said to be '''holonomic''' if it satisfies a linear differential equation of the form <ref>{{cite book|last1=Flajolet and Sedgewick|title=Analytic Combinatorics|date=2010|publisher=Cambridge University Press|location=New York|isbn=978-0-521-89806-5}} (Section B.4)</ref>\n\n:<math>c_0(z) F^{(r)}(z) + c_1(z) F^{(r-1)}(z) + \\cdots + c_r(z) F(z) = 0, </math>\n\nwhere the coefficients <math>c_i(z)</math> are in the field of rational functions, <math>\\mathbb{C}(z)</math>. Equivalently, <math>F(z)</math> is holonomic if the vector space over <math>\\mathbb{C}(z)</math> spanned by the set of all of its derivatives is finite dimensional.\n\nSince we can clear denominators if need be in the previous equation, we may assume that the functions, <math>c_i(z)</math> are polynomials in <math>z</math>. Thus we can see an equivalent condition that a generating function is holonomic if its coefficients satisfy a '''P-recurrence''' of the form\n\n:<math>\\widehat{c}_s(n) f_{n+s} + \\widehat{c}_{s-1}(n) f_{n+s-1} + \\cdots + \\widehat{c}_0(n) f_n = 0,</math>\n\nfor all large enough <math>n \\geq n_0</math> and where the <math>\\widehat{c}_i(n)</math> are fixed finite-degree polynomials in <math>n</math>. In other words, the properties that a sequence be ''P-recursive'' and have a holonomic generating function are equivalent. Holonomic functions are closed under the [[Generating function transformation#Hadamard products and diagonal generating functions|Hadamard product]] operation <math>\\odot</math> on generating functions.\n\n====Examples====\n\nThe functions <math>e^z</math>, <math>\\log(z)</math>, <math>\\cos(z)</math>, <math>\\arcsin(z)</math>, <math>\\sqrt{1+z}</math>, the [[dilogarithm]] function <math>\\operatorname{Li}_2(z)</math>, the [[generalized hypergeometric function]]s <math>_pF_q(...;...;z)</math> and the functions defined by the power series <math>\\sum_{n \\geq 0} z^n / (n!)^2</math> and the non-convergent <math>\\sum_{n \\geq 0} n! \\cdot z^n</math> are all holonomic. Examples of P-recursive sequences with holonomic generating functions include <math>f_n := \\frac{1}{n+1} \\binom{2n}{n}</math> and <math>f_n := 2^n / (n^2+1)</math>, where sequences such as <math>\\sqrt{n}</math> and <math>\\log(n)</math> are ''not'' P-recursive due to the nature of singularities in their corresponding generating functions. Similarly, functions with infinitely-many singularities such as <math>\\tan(z)</math>, <math>\\sec(z)</math>, and <math>\\Gamma(z)</math> are ''not'' holonomic functions.\n\n====Software for working with P-recursive sequences and holonomic generating functions====\n\nTools for processing and working with P-recursive sequences in ''Mathematica'' include the software packages provided for non-commercial use on the [https://www.risc.jku.at/research/combinat/software/ RISC Combinatorics Group algorithmic combinatorics software] site. Despite being mostly closed-source, particularly powerful tools in this software suite are provided by the '''Guess''' package for guessing ''P-recurrences'' for arbitrary input sequences (useful for [[experimental mathematics]] and exploration) and the '''Sigma''' package which is able to find P-recurrences for many sums and solve for closed-form solutions to P-recurrences involving generalized [[harmonic number]]s.<ref>{{cite journal|last1=Schneider|first1=C.|title=Symbolic Summation Assists Combinatorics|journal=Sem.Lothar.Combin.|date=2007|volume=56|pages=1–36}}</ref> Other packages listed on this particular RISC site are targeted at working with holonomic ''generating functions'' specifically. \n(''Depending on how in depth this article gets on the topic, there are many, many other examples of useful software tools that can be listed here or on this page in another section.'')\n\n=== Relation to discrete-time Fourier transform ===\n{{Main|Discrete-time Fourier transform}}\nWhen the series [[Absolute convergence|converges absolutely]],\n\n:<math>G \\left ( a_n; e^{-i \\omega} \\right) = \\sum_{n=0}^\\infty a_n e^{-i \\omega n}</math>\n\nis the discrete-time Fourier transform of the sequence ''a''<sub>0</sub>,&nbsp;''a''<sub>1</sub>,&nbsp;....\n\n=== Asymptotic growth of a sequence ===\nIn calculus, often the growth rate of the coefficients of a power series can be used to deduce a [[radius of convergence]] for the power series.  The reverse can also hold; often the radius of convergence for a generating function can be used to deduce the [[Asymptotic analysis|asymptotic growth]] of the underlying sequence.\n\nFor instance, if an ordinary generating function ''G''(''a''<sub>''n''</sub>;&nbsp;''x'') that has a finite radius of convergence of ''r'' can be written as\n\n:<math>G(a_n; x) = \\frac{A(x) + B(x) \\left (1- \\frac{x}{r} \\right )^{-\\beta}}{x^{\\alpha}}</math>\n\nwhere each of ''A''(''x'') and ''B''(''x'') is a function that is [[analytic function|analytic]] to a radius of convergence greater than ''r'' (or is [[Entire function|entire]]), and where ''B''(''r'')&nbsp;≠&nbsp;0 then\n\n:<math>a_n \\sim \\frac{B(r)}{r^{\\alpha} \\Gamma(\\beta)} \\, n^{\\beta-1}(1/r)^{n} \\sim \\frac{B(r)}{r^{\\alpha}} \\binom{n+\\beta-1}{n}(1/r)^{n} = \\frac{B(r)}{r^{\\alpha}} \\left(\\!\\!\\binom{\\beta}{n}\\!\\!\\right)(1/r)^{n}\\,,</math>\nusing the [[Gamma function]], a [[binomial coefficient]], or a [[multiset coefficient]].\n\nOften this approach can be iterated to generate several terms in an asymptotic series for ''a''<sub>''n''</sub>.  In particular,\n:<math>G\\left(a_n - \\frac{B(r)}{r^{\\alpha}} \\binom{n+\\beta-1}{n}(1/r)^{n}; x \\right) = G(a_n; x) - \\frac{B(r)}{r^{\\alpha}} \\left(1 - \\frac{x}{r}\\right)^{-\\beta}\\,.</math>\nThe asymptotic growth of the coefficients of this generating function can then be sought via the finding of ''A'', ''B'', α, β, and ''r'' to describe the generating function, as above.\n\nSimilar asymptotic analysis is possible for exponential generating functions.  With an exponential generating function, it is ''a''<sub>''n''</sub>/''n''! that grows according to these asymptotic formulae.\n\n==== Asymptotic growth of the sequence of squares ====\nAs derived above, the ordinary generating function for the sequence of squares is\n\n:<math>\\frac{x(x+1)}{(1-x)^3}.</math>\n\nWith ''r''&nbsp;=&nbsp;1, α&nbsp;=&nbsp;0, β&nbsp;=&nbsp;3, ''A''(''x'')&nbsp;=&nbsp;0, and ''B''(''x'')&nbsp;=&nbsp;''x''(''x''+1), we can verify that the squares grow as expected, like the squares:\n\n:<math>a_n \\sim \\frac{B(r)}{r^{\\alpha} \\Gamma(\\beta)} \\, n^{\\beta-1} \\left (\\frac{1}{r} \\right )^{n} = \\frac{1(1+1)}{1^0\\,\\Gamma(3)}\\,n^{3-1} (1/1)^n = n^2.</math>\n\n==== Asymptotic growth of the Catalan numbers ====\n{{Main|Catalan number}}\n\nThe ordinary generating function for the Catalan numbers is\n\n:<math>\\frac{1-\\sqrt{1-4x}}{2x}.</math>\n\nWith ''r''&nbsp;=&nbsp;1/4, α&nbsp;=&nbsp;1, β&nbsp;=&nbsp;−1/2, ''A''(''x'')&nbsp;=&nbsp;1/2, and ''B''(''x'')&nbsp;=&nbsp;−1/2, we can conclude that, for the Catalan numbers,\n\n:<math>a_n \\sim \\frac{B(r)}{r^{\\alpha} \\Gamma(\\beta)} \\, n^{\\beta-1} \\left(\\frac{1}{r} \\right )^{n} = \\frac{-\\frac{1}{2}}{(\\frac{1}{4})^1 \\Gamma(-\\frac{1}{2})} \\, n^{-\\frac{1}{2}-1} \\left(\\frac{1}{\\frac{1}{4}}\\right)^n = \\frac{1}{\\sqrt{\\pi}}n^{-\\frac{3}{2}} \\, 4^n.</math>\n\n=== Bivariate and multivariate generating functions ===\nOne can define generating functions in several variables for arrays with several indices. These are called '''multivariate generating functions''' or, sometimes, '''super generating functions'''. For two variables, these are often called '''bivariate generating functions'''.\n\nFor instance, since <math>(1+x)^n</math> is the ordinary generating function for [[binomial coefficients]] for a fixed ''n'', one may ask for a bivariate generating function that generates the binomial coefficients <math>\\binom{n}{k}</math> for all ''k'' and ''n''. To do this, consider <math>(1+x)^n</math> as itself a series, in ''n'', and find the generating function in ''y'' that has these as coefficients. Since the generating function for <math>a^n</math> is\n\n:<math>\\frac{1}{1-ay},</math>\n\nthe generating function for the binomial coefficients is:\n\n:<math>\\sum_{n,k} \\binom{n}{k} x^k y^n = \\frac{1}{1-(1+x)y}=\\frac{1}{1-y-xy}.</math>\n\n=== Representation by continued fractions (Jacobi-type J-fractions) ===\n\n==== Definitions ====\n\nExpansions of (formal) ''Jacobi-type'' and ''Stieltjes-type'' [[generalized continued fraction|continued fractions]] (''J-fractions'' and ''S-fractions'', respectively) whose <math>h^{th}</math> rational convergents represent [[Order of accuracy|<math>2h</math>-order accurate]] power series are another way to express the typically divergent ordinary generating functions for many special one and two-variate sequences. The particular form of the [[Jacobi-type continued fraction]]s (J-fractions) are expanded as in the following equation and have the next corresponding power series expansions with respect to <math>z</math> for some specific, application-dependent component sequences, <math>\\{\\text{ab}_i\\}</math> and <math>\\{c_i\\}</math>, where <math>z \\neq 0</math> denotes the formal variable in the second power series expansion given below:<ref>See P. Flajolet's article ''[http://algo.inria.fr/flajolet/Publications/Flajolet80b.pdf Combinatorial aspects of continued fractions]'' (1980) and also refer to H. S. Wall's ''Analytic theory of continued fractions'' (1948) for more complete information on the properties of J-fractions.</ref>\n\n:<math>\n\\begin{align} \nJ^{[\\infty]}(z) & = \\cfrac{1}{1-c_1 z-\\cfrac{\\text{ab}_2 z^2}{1-c_2 z-\\cfrac{\\text{ab}_3 z^2}{\\ddots}}} \\\\ \n                & = 1 + c_1 z + \\left(\\text{ab}_2+c_1^2\\right) z^2 + \\left(2 \\text{ab}_2 c_1+c_1^3 + \\text{ab}_2 c_2\\right) z^3 + \\cdots. \n\\end{align} \n</math>\n\nThe coefficients of <math>z^n</math>, denoted in shorthand by <math>j_n := [z^n] J^{[\\infty]}(z)</math>, in the previous equations correspond to matrix solutions of the equations\n\n:<math>\\begin{bmatrix}k_{0,1} & k_{1,1} & 0 & 0 & \\cdots \\\\ k_{0,2} & k_{1,2} & k_{2,2} & 0 & \\cdots \\\\ k_{0,3} & k_{1,3} & k_{2,3} & k_{3,3} & \\cdots \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\end{bmatrix} = \n       \\begin{bmatrix}k_{0,0} & 0 & 0 & 0 & \\cdots \\\\ k_{0,1} & k_{1,1} & 0 & 0 & \\cdots \\\\ k_{0,2} & k_{1,2} & k_{2,2} & 0 & \\cdots \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\end{bmatrix} \\cdot \n       \\begin{bmatrix}c_1 & 1 & 0 & 0 & \\cdots \\\\ \\text{ab}_2 & c_2 & 1 & 0 & \\cdots \\\\ 0 & \\text{ab}_3 & c_3 & 1 & \\cdots \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\end{bmatrix}, \n</math>\n\nwhere <math>j_0 \\equiv k_{0,0} = 1</math>, <math>j_n = k_{0,n}</math> for <math>n \\geq 1</math>, <math>k_{r,s} = 0</math> if <math>r > s</math>, and where for all integers <math>p, q \\geq 0</math>, we have an ''addition formula'' relation given by\n\n:<math>j_{p+q} = k_{0,p} \\cdot k_{0,q} + \\sum_{i=1}^{\\min(p, q)} \\text{ab}_2 \\cdots \\text{ab}_{i+1} \\times k_{i,p} \\cdot k_{i,q}. </math>\n\n==== Properties of the {{math|''h''}}<sup>th</sup> convergent functions ====\n\nFor <math>h \\geq 0</math> (though in practice when <math>h \\geq 2</math>), we can define the rational <math>h^\\text{th}</math> convergents to the infinite J-fraction, <math>J^{[\\infty]}(z)</math>, expanded by\n\n:<math>\\operatorname{Conv}_h(z) := \\frac{P_h(z)}{Q_h(z)} = j_0 + j_1 z + \\cdots + j_{2h-1} z^{2h-1} + \\sum_{n \\geq 2h} \\widetilde{j}_{h,n} z^n,</math>\n\ncomponent-wise through the sequences, <math>P_h(z)</math> and <math>Q_h(z)</math>, defined recursively by\n\n:<math>\n\\begin{align} \nP_h(z) & = (1-c_h z) P_{h-1}(z) - \\text{ab}_h z^2 P_{h-2}(z) + \\delta_{h,1} \\\\ \nQ_h(z) & = (1-c_h z) Q_{h-1}(z) - \\text{ab}_h z^2 Q_{h-2}(z) + (1-c_1 z) \\delta_{h,1} + \\delta_{0,1}. \n\\end{align} \n</math>\n\nMoreover, the rationality of the convergent function, <math>\\text{Conv}_h(z)</math> for all <math>h \\geq 2</math> implies additional finite difference equations and congruence properties satisfied by the sequence of <math>j_n</math>, ''and'' for <math>M_h := \\text{ab}_2 \\cdots \\text{ab}_{h+1}</math> if <math>h |\\mid M_h</math> then we have the congruence\n\n:<math>j_n \\equiv [z^n] \\operatorname{Conv}_h(z) \\pmod h, </math>\n\nfor non-symbolic, determinate choices of the parameter sequences, <math>\\{\\text{ab}_i\\}</math> and <math>\\{c_i\\}</math>, when <math>h \\geq 2</math>, i.e., when these sequences do not implicitly depend on an auxiliary parameter such as <math>q</math>, <math>x</math>, or <math>R</math> as in the examples contained in the table below.\n\n==== Examples ====\n\nThe next table provides examples of closed-form formulas for the component sequences found computationally (and subsequently proved correct in the cited references <ref>See the following articles: \n* ''[https://arxiv.org/abs/1612.02778 Continued Fractions for Square Series Generating Functions]'' (2016) \n* ''[https://cs.uwaterloo.ca/journals/JIS/VOL20/Schmidt/schmidt14.html Jacobi Type Continued Fractions for the Ordinary Generating Functions of Generalized Factorial Functions]'' (2017)\n* ''[https://arxiv.org/abs/1702.01374 Jacobi-Type Continued Fractions and Congruences for Binomial Coefficients Modulo Integers <math>h \\geq 2</math>]'' (2017)\n</ref>) \nin several special cases of the prescribed sequences, <math>j_n</math>, generated by the general expansions of the J-fractions defined in the first subsection. Here we define <math>0 < |a|, |b|, |q| < 1</math> and the parameters <math>R</math>, <math>\\alpha \\in \\mathbb{Z}^{+}</math> and <math>x</math> to be indeterminates with respect to these expansions, where the prescribed sequences enumerated by the expansions of these J-fractions are defined in terms of the [[q-Pochhammer symbol]], [[Pochhammer symbol]], and the [[binomial coefficients]].\n\n{| class=\"wikitable\"\n|-\n! <math>j_n</math> !! <math>c_1</math> !! <math>c_i (i \\geq 2)</math> !! <math>\\text{ab}_i (i \\geq 2)</math> \n|-\n| <math>q^{n^2}</math> || <math>q</math> || <math>q^{2h-3}\\left(q^{2h}+q^{2h-2}-1\\right)</math> || <math>q^{6h-10}\\left(q^{2h-2}-1\\right)</math>\n|-\n| <math>(a; q)_n</math> || <math>1-a</math> || <math>q^{h-1} - a q^{h-2} \\left(q^{h} + q^{h-1} - 1\\right)</math> || <math>a q^{2h-4} (a q^{h-2}-1)(q^{h-1}-1)</math>\n|-\n| <math>\\left(z q^{-n}; q\\right)_n</math> || <math>\\frac{q-z}{q}</math> || <math>\\frac{q^h - z - qz + q^h z}{q^{2h-1}}</math> || <math>\\frac{(q^{h-1}-1) (q^{h-1}-z) \\cdot z}{q^{4h-5}}</math>\n|-\n| <math>\\frac{(a; q)_n}{(b; q)_n}</math> || <math>\\frac{1-a}{1-b}</math> || <math>\\frac{q^{i-2}\\left(q+ab q^{2i-3}+a(1-q^{i-1}-q^i)+b(q^{i}-q-1)\\right)}{(1-bq^{2i-4})(1-bq^{2i-2})}</math> || <math>\\frac{q^{2i-4}(1-bq^{i-3})(1-aq^{i-2})(a-bq^{i-2})(1-q^{i-1})}{(1-bq^{2i-5})(1-bq^{2i-4})^2(1-bq^{2i-3})}</math>\n|-\n| <math>\\alpha^n \\cdot \\left(\\frac{R}{\\alpha}\\right)_n</math> || <math>R</math> || <math>R+2\\alpha (i-1)</math> || <math>(i-1)\\alpha(R+(i-2)\\alpha)</math>\n|-\n| <math>(-1)^n \\binom{x}{n}</math> || <math>-x</math> || <math>-\\frac{(x+2(i-1)^2)}{(2i-1)(2i-3)}</math> || \n  <math>\\begin{cases}-\\frac{(x-i+2)(x+i-1)}{4 \\cdot (2i-3)^2} & i \\geq 3; \\\\ -\\frac{1}{2}x(x+1) & i = 2. \\end{cases}</math>\n|-\n| <math>(-1)^n \\binom{x+n}{n}</math> || <math>-(x+1)</math> || <math>\\frac{(x-2i(i-2)-1)}{(2i-1)(2i-3)}</math> || \n  <math>\\begin{cases}-\\frac{(x-i+2)(x+i-1)}{4 \\cdot (2i-3)^2} & i \\geq 3; \\\\ -\\frac{1}{2}x(x+1) & i = 2. \\end{cases}</math>\n|}\n\nNote that the radii of convergence of these series corresponding to the definition of the Jacobi-type J-fractions given above are in general different from that of the corresponding power series expansions defining the ordinary generating functions of these sequences.\n\n==Examples==\n{{Main|Examples of generating functions}}\nGenerating functions for the sequence of [[square number]]s ''a''<sub>''n''</sub> = ''n''<sup>2</sup> are:\n\n===Ordinary generating function===\n:<math>G(n^2;x)=\\sum_{n=0}^\\infty n^2x^n = \\frac{x(x+1)}{(1-x)^3}</math>\n\n===Exponential generating function===\n:<math>\\operatorname{EG}(n^2;x)=\\sum _{n=0}^\\infty \\frac{n^2x^n}{n!}=x(x+1)e^x</math>\n\n===Lambert series===\n\nAs an example of a Lambert series identity not given in the [[Lambert series|main article]], we can show that for <math>|x|, |xq| < 1</math> we have that <ref>{{cite web|title=Lambert series identity|url=http://mathoverflow.net/questions/140418/lambert-series-identity|website=Math Overflow|date=2017}}</ref>\n\n:<math>\\sum_{n \\geq 1} \\frac{q^n x^n}{1-x^n} = \\sum_{n \\geq 1} \\frac{q^n x^{n^2}}{1-q x^n} + \\sum_{n \\geq 1} \\frac{q^n x^{n(n+1)}}{1-x^n}, </math>\n\nwhere we have the special case identity for the generating function of the [[divisor function]], <math>d(n) \\equiv \\sigma_0(n)</math>, given by\n\n:<math>\\sum_{n \\geq 1} \\frac{x^n}{1-x^n} = \\sum_{n \\geq 1} \\frac{x^{n^2} (1+x^n)}{1-x^n}. </math>\n\n===Bell series===\n:<math>\\operatorname{BG}_p(n^2;x)=\\sum_{n=0}^\\infty (p^{n})^2x^n=\\frac{1}{1-p^2x}</math>\n\n===Dirichlet series generating function===\n:<math>\\operatorname{DG}(n^2;s)=\\sum_{n=1}^\\infty \\frac{n^2}{n^s}=\\zeta(s-2),</math>\n\nusing the [[Riemann zeta function]].\n\nThe sequence ''a<sub>k</sub>'' generated by a [[Dirichlet series]] generating function (DGF) corresponding to:\n\n:<math>\\operatorname{DG}(a_k;s)=\\zeta(s)^m</math>\n\nwhere <math>\\zeta(s)</math> is the [[Riemann zeta function]], has the ordinary generating function:\n\n:<math>\\sum_{k=1}^{k=n} a_k x^k = x + {m \\choose 1} \\sum_{2 \\leq a \\leq n} x^{a} + {m \\choose 2}\\underset{ab \\leq n}{\\sum_{a \\geq 2} \\sum_{b \\geq 2}} x^{ab} + {m \\choose 3}\\underset{abc \\leq n}{\\sum_{a \\geq 2} \\sum_{c \\geq 2} \\sum_{b \\geq 2}} x^{abc} + {m \\choose 4}\\underset{abcd \\leq n}{\\sum_{a \\geq 2} \\sum_{b \\geq 2} \\sum_{c \\geq 2} \\sum_{d \\geq 2}} x^{abcd} + \\cdots</math>\n\n===Multivariate generating functions===\nMultivariate generating functions arise in practice when calculating the number of [[contingency tables]] of non-negative integers with specified row and column totals.   Suppose the table has ''r'' rows and ''c'' columns; the row sums are <math>t_1,\\ldots t_r</math> and the column sums are <math>s_1,\\ldots s_c</math>.  Then, according to [[I. J. Good]],<ref name=\"Good 1986\">{{cite journal| doi=10.1214/aos/1176343649| last=Good| first=I. J.|  title=On applications of symmetric Dirichlet distributions and their mixtures to contingency tables| journal=[[Annals of Statistics]]| year=1986| volume=4| issue=6|pages=1159–1189}}</ref> the number of such tables is the coefficient of\n\n:<math>x_1^{t_1}\\ldots x_r^{t_r}y_1^{s_1}\\ldots y_c^{s_c}</math>\n\nin\n\n:<math>\\prod_{i=1}^{r}\\prod_{j=1}^c\\frac{1}{1-x_iy_j}.</math>\n\nIn the bivariate case, non-polynomial double sum examples of so-termed \"''double''\" or \"''super''\" generating functions of the form <math>G(w, z) := \\sum_{m,n \\geq 0} g_{m,n} w^m z^n</math> include the following two-variable generating functions for the [[binomial coefficients]], the [[Stirling numbers]], and the [[Eulerian numbers]]:<ref>See the usage of these terms in Section 7.4 of ''Concrete Mathematics'' on special sequence generating functions.</ref>\n\n:<math>\n\\begin{align} \ne^{z+wz} & = \\sum_{m,n \\geq 0} \\binom{n}{m} w^m \\frac{z^n}{n!} \\\\ \ne^{w(e^z-1)} & = \\sum_{m,n \\geq 0} \\left\\{\\begin{matrix} n \\\\ m \\end{matrix} \\right\\} w^m \\frac{z^n}{n!} \\\\ \n\\frac{1}{(1-z)^w} & = \\sum_{m,n \\geq 0} \\left[\\begin{matrix} n \\\\ m \\end{matrix} \\right] w^m \\frac{z^n}{n!} \\\\ \n\\frac{1-w}{e^{(w-1)z}-w} & = \\sum_{m,n \\geq 0} \\left\\langle\\begin{matrix} n \\\\ m \\end{matrix} \\right\\rangle w^m \\frac{z^n}{n!} \\\\ \n\\frac{e^w-e^z}{w e^z-z e^w} &= \\sum_{m,n \\geq 0} \\left\\langle\\begin{matrix} m+n+1 \\\\ m \\end{matrix} \\right\\rangle \\frac{w^m z^n}{(m+n+1)!}. \n\\end{align} \n</math>\n\n==Applications==\n\n===Various techniques: Evaluating sums and tackling other problems with generating functions===\n\n====Example 1: A formula for sums of harmonic numbers====\n\nGenerating functions give us several methods to manipulate sums and to establish identities between sums.\n\nThe simplest case occurs when <math>s_n = \\sum_{k=0}^{n}{a_k}</math>. We then know that <math>S(z) = \\frac{A(z)}{1-z}</math> for the corresponding ordinary generating functions.\n\nFor example, we can manipulate <math>s_n=\\sum_{k=1}^{n} H_{k}</math>, where <math>H_k = 1 + \\frac{1}{2} + \\cdots + \\frac{1}{k}</math> are the [[harmonic number]]s.  Let <math>H(z) = \\sum_{n \\geq 1}{H_n z^n}</math> be the ordinary generating function of the harmonic numbers.  Then\n:<math>H(z) = \\dfrac{\\sum_{n\\geq1}{\\frac{1}{n}z^n}}{1-z}\\,,</math>\nand thus\n:<math>S(z) = \\sum_{n\\geq1}{s_n z^n} = \\dfrac{\\sum_{n\\geq1}{\\frac{1}{n}z^n}}{(1-z)^2}\\,.</math>\n\nUsing <math>\\frac{1}{(1-z)^2} = \\sum_{n\\geq0}{(n+1)z^n}</math>, [[Generating function#Convolution .28Cauchy products.29|convolution]] with the numerator yields\n:<math>s_n = \\sum_{k = 1}^{n}{\\frac{1}{k}(n+1-k)} = (n+1)H_n - n\\,,</math>\nwhich can also be written as\n:<math>\\sum_{k = 1}^{n}{H_k} = (n+1)(H_{n+1} - 1)\\,.</math>\n\n====Example 2: Modified binomial coefficient sums and the binomial transform====\n\nAs another example of using generating functions to relate sequences and manipulate sums, for an arbitrary sequence <math>\\langle f_n \\rangle</math> we define the two sequences of sums\n\n:<math>s_n := \\sum_{m=0}^n \\binom{n}{m} f_m 3^{n-m} </math> \n:<math>\\widetilde{s}_n := \\sum_{m=0}^n \\binom{n}{m} (m+1)(m+2)(m+3) f_m 3^{n-m}, </math>\n\nfor all <math>n \\geq 0</math>, and seek to express the second sums in terms of the first. We suggest an approach by generating functions.\n\nFirst, we use the [[binomial transform]] to write the generating function for the first sum as\n\n:<math>S(z) = \\frac{1}{(1-3z)} F\\left(\\frac{z}{1-3z}\\right). </math>\n\nSince the generating function for the sequence <math>\\langle (n+1)(n+2)(n+3) f_n \\rangle</math> is given by <math>6 F(z) + 18z F^{\\prime}(z) + 9z^2 F^{\\prime\\prime}(z) + z^3 F^{(3)}(z)</math>, we may write the generating function for the second sum defined above in the form\n\n:<math>\\widetilde{S}(z) = \\frac{6}{(1-3z)} F\\left(\\frac{z}{1-3z}\\right)+\\frac{18z}{(1-3z)^2} F^{\\prime}\\left(\\frac{z}{1-3z}\\right)+\\frac{9z^2}{(1-3z)^3} F^{\\prime\\prime}\\left(\\frac{z}{1-3z}\\right)+\\frac{z^3}{(1-3z)^4} F^{(3)}\\left(\\frac{z}{1-3z}\\right). </math>\n\nIn particular, we may write this modified sum generating function in the form of\n\n:<math>a(z) \\cdot S(z) + b(z) \\cdot z S^{\\prime}(z) + c(z) \\cdot z^2 S^{\\prime\\prime}(z) + d(z) \\cdot z^3 S^{(3)}(z), </math>\n\nfor <math>a(z) = 6(1-3z)^3</math>, <math>b(z) = 18 (1-3z)^3</math>, <math>c(z) = 9 (1-3z)^3</math>, and <math>d(z) = (1-3z)^3</math> where <math>(1-3z)^3 = 1-9z+27z^2-27z^3</math>.\n\nFinally, it follows that we may express the second sums through the first sums in the following form:\n\n:<math>\n\\begin{align} \n\\widetilde{s}_n & = [z^n]\\left(6(1-3z)^3 \\sum_{n \\geq 0} s_n z^n + 18 (1-3z)^3 \\sum_{n \\geq 0} n s_n z^n + 9 (1-3z)^3 \\sum_{n \\geq 0} n(n-1) s_n z^n + (1-3z)^3 \\sum_{n \\geq 0} n(n-1)(n-2) s_n z^n\\right) \\\\ \n     & = (n+1)(n+2)(n+3) s_n - 9 n(n+1)(n+2) s_{n-1} + 27 (n-1)n(n+1) s_{n-2} - (n-2)(n-1)n s_{n-3}. \n\\end{align}\n</math>\n\n====Example 3: Generating functions for mutually recursive sequences====\n\nIn this example, we re-formulate a generating function example given in Section 7.3 of ''Concrete Mathematics'' (see also Section 7.1 of the same reference for pretty pictures of generating function series). In particular, suppose that we seek the total number of ways (denoted <math>U_n</math>) to tile a <math>3 \\times n</math> rectangle with unmarked <math>2 \\times 1</math> domino pieces. Let the auxiliary sequence, <math>V_n</math>, be defined as the number of ways to cover a <math>3 \\times n</math> rectangle-minus-corner section of the full rectangle. We seek to use these definitions to give a [[Closed-form expression|closed form]] formula for <math>U_n</math> without breaking down this definition further to handle the cases of vertical versus horizontal dominoes. Notice that the ordinary generating functions for our two sequences correspond to the series\n\n:<math>U(z) = 1 + 3z^2 + 11 z^4 + 41 z^6 + \\cdots </math> \n:<math>V(z) = z + 4z^3 + 15 z^5 + 56 z^7 + \\cdots. </math>\n\nIf we consider the possible configurations that can be given starting from the left edge of the <math>3 \\times n</math> rectangle, we are able to express the following mutually dependent, or ''mutually recursive'', recurrence relations for our two sequences when <math>n \\geq 2</math> defined as above where <math>U_0 = 1</math>, <math>U_1 = 0</math>, <math>V_0 = 0</math>, and <math>V_1 = 1</math>:\n\n:<math>\n\\begin{align}\nU_n & = 2 V_{n-1} + U_{n-2} \\\\ \nV_n & = U_{n-1} + V_{n-2}. \n\\end{align} \n</math>\n\nSince we have that for all integers <math>m \\geq 0</math>, the index-shifted generating functions satisfy <math>z^m G(z) = \\sum_{n \\geq m} g_{n-m} z^n</math> (incidentally, we also have a corresponding formula when <math>m < 0</math> given by <math>\\sum_{n \\geq 0} g_{n+m} z^n = \\frac{G(z) - g_0 -g_1 z - \\cdots - g_{m-1} z^{m-1}}{z^m}</math>), we can use the initial conditions specified above and the previous two recurrence relations to see that we have the next two equations relating the generating functions for these sequences given by\n\n:<math>\n\\begin{align}\nU(z) & = 2z V(z) + z^2 U(z) + 1 \\\\ \nV(z) & = z U(z) + z^2 V(z) \\\\ \n     & = \\frac{z}{1-z^2} U(z), \n\\end{align} \n</math>\n\nwhich then implies by solving the system of equations (and this is the particular trick to our method here) that\n\n:<math>U(z) = \\frac{1-z^2}{1-4z^2+z^4} = \\frac{1}{3-\\sqrt{3}} \\cdot \\frac{1}{1-(2+\\sqrt{3}) z^2} + \\frac{1}{3+\\sqrt{3}} \\cdot \\frac{1}{1-(2-\\sqrt{3}) z^2}. </math>\n\nThus by performing algebraic simplifications to the sequence resulting from the second partial fractions expansions of the generating function in the previous equation, we find that <math>U_{2n+1} \\equiv 0</math> and that\n\n:<math>U_{2n} = \\left\\lceil \\frac{(2+\\sqrt{3})^n}{3-\\sqrt{3}} \\right\\rceil, </math>\n\nfor all integers <math>n \\geq 0</math>. We also note that the same shifted generating function technique applied to the second-order [[recurrence relation|recurrence]] for the [[Fibonacci numbers]] is the prototypical example of using generating functions to solve recurrence relations in one variable already covered, or at least hinted at, in the subsection on [[rational functions]] given above.\n\n===Convolution (Cauchy products)===\n\nA discrete ''convolution'' of the terms in two formal power series turns a product of generating functions into a generating function enumerating a convolved sum of the original sequence terms (see [[Cauchy product]]).\n\n:1.Consider ''A''(''z'') and ''B''(''z'') are ordinary generating functions. \n::<math>C(z) = A(z)B(z) \\Leftrightarrow [z^n]C(z) = \\sum_{k=0}^{n}{a_k b_{n-k}}</math>\n\n:2.Consider ''A''(''z'') and ''B''(''z'') are exponential generating functions.\n::<math>C(z) = A(z)B(z) \\Leftrightarrow [z^n/n!]C(z) = \\sum_{k=0}^n \\binom{n}{k} a_k b_{n-k}</math>\n\n:3.Consider the triply convolved sequence resulting from the product of three ordinary generating functions \n::<math>C(z) = F(z) G(z) H(z) \\Leftrightarrow [z^n]C(z) = \\sum_{j+k+\\ell=n} f_j g_k h_\\ell</math>\n\n:4.Consider the <math>m</math>-fold convolution of a sequence with itself for some positive integer <math> m \\geq 1</math> (see the example below for an application) \n::<math>C(z) = G(z)^m \\Leftrightarrow [z^n]C(z) = \\sum_{k_1+k_2+\\cdots+k_m=n} g_{k_1} g_{k_2} \\cdots g_{k_m}</math>\n\nMultiplication of generating functions, or convolution of their underlying sequences, can correspond to a notion of independent events in certain counting and probability scenarios. For example, if we adopt the notational convention that the [[probability generating function]], or ''pgf'', of a random variable <math>Z</math> is denoted by <math>G_Z(z)</math>, then we can show that for any two random variables <ref>Section 8.3 in ''Concrete Mathematics''.</ref>\n\n:<math>G_{X+Y}(z) = G_X(z) G_Y(z), </math>\n\nif <math>X</math> and <math>Y</math> are independent. Similarly, the number of ways to pay <math>n \\geq 0</math> cents in coin denominations of values in the set <math>\\{1, 5, 10, 25, 50\\}</math> (i.e., in pennies, nickels, dimes, quarters, and half dollars, respectively) is generated by the product\n\n:<math>C(z) = \\frac{1}{1-z} \\frac{1}{1-z^5} \\frac{1}{1-z^{10}} \\frac{1}{1-z^{25}} \\frac{1}{1-z^{50}}, </math>\n\nand moreover, if we allow the <math>n</math> cents to be paid in coins of any positive integer denomination, we arrive at the generating for the number of such combinations of change being generated by the [[partition function (mathematics)|partition function]] generating function expanded by the infinite [[q-Pochhammer symbol]] product of <math>\\prod_{n \\geq 1} (1-z^n)^{-1}</math>.\n\n====Example: The generating function for the Catalan numbers====\n\nAn example where convolutions of generating functions are useful allows us to solve for a specific closed-form function representing the ordinary generating function for the [[Catalan numbers]], <math>C_n</math>. In particular, this sequence has the combinatorial interpretation as being the number of ways to insert parentheses into the product <math>x_0 \\cdot x_1 \\cdots x_n</math> so that the order of multiplication is completely specified. For example, <math>C_2 = 2</math> which corresponds to the two expressions <math>x_0 \\cdot (x_1 \\cdot x_2)</math> and <math>(x_0 \\cdot x_1) \\cdot x_2</math>. It follows that the sequence satisfies a recurrence relation given by\n\n:<math>C_n = \\sum_{k=0}^{n-1} C_k C_{n-1-k} + \\delta_{n,0} = C_0 C_{n-1} + C_1 C_{n-2} + \\cdots + C_{n-1} C_0 + \\delta_{n,0},\\ n \\geq 0, </math>\n\nand so has a corresponding convolved generating function, <math>C(z)</math>, satisfying\n\n:<math>C(z) = z \\cdot C(z)^2 + 1.</math>\n\nSince <math>C(0) = 1 \\neq \\infty</math>, we then arrive at a formula for this generating function given by\n\n:<math>\n\\begin{align} \nC(z) & = \\frac{1-\\sqrt{1-4z}}{2z} \\\\ \n     & = \\sum_{n \\geq 0} \\frac{1}{n+1}\\binom{2n}{n} z^n. \n\\end{align} \n</math>\n\nNote that the first equation implicitly defining <math>C(z)</math> above implies that\n\n:<math>C(z) = \\frac{1}{1-z \\cdot C(z)}, </math>\n\nwhich then leads to another \"simple\" (as in of form) continued fraction expansion of this generating function.\n\n====Example: Spanning trees of fans and convolutions of convolutions====\n\nA ''fan of order <math>n</math>'' is defined to be a graph on the vertices <math>\\{0, 1, \\ldots, n\\}</math> with <math>2n-1</math> edges connected according to the following rules: Vertex <math>0</math> is connected by a single edge to each of the other <math>n</math> vertices, and vertex <math>k</math> is connected by a single edge to the next vertex <math>k+1</math> for all <math>1 \\leq k < n</math>.<ref>See Example 6 in Section 7.3 of ''Concrete Mathematics'' for another method and the complete setup of this problem using generating functions. This more \"''convoluted''\" approach is given in Section 7.5 of the same reference.</ref> There is one fan of order one, three fans of order two, eight fans of order three, and so on. A [[spanning tree]] is a subgraph of a graph which contains all of the original vertices and which contains enough edges to make this subgraph connected, but not so many edges that there is a cycle in the subgraph. We ask how many spanning trees <math>f_n</math> of  a fan of order <math>n</math> are possible for each <math>n \\geq 1</math>.\n\nAs an observation, we may approach the question by counting the number of ways to join adjacent sets of vertices. For example, when <math>n = 4</math>, we have that <math>f_4 = 4 + 3 \\cdot 1 + 2 \\cdot 2 + 1 \\cdot 3 + 2 \\cdot 1 \\cdot 1 + 1 \\cdot 2 \\cdot 1 + 1 \\cdot 1 \\cdot 2 + 1\\cdot1\\cdot1\\cdot1\\cdot1 = 21</math>, which is a sum over the <math>m</math>-fold convolutions of the sequence <math>g_n = n = [z^n] z / (1-z)^2</math> for <math>m := 1,2,3,4</math>. More generally, we may write a formula for this sequence as\n\n:<math>f_n = \\sum_{m > 0} \\sum_{\\scriptstyle k_1+k_2+\\cdots+k_m=n\\atop\\scriptstyle k_1, k_2, \\ldots,k_m > 0} g_{k_1} g_{k_2} \\cdots g_{k_m}, </math>\n\nfrom which we see that the ordinary generating function for this sequence is given by the next sum of convolutions as\n\n:<math>\n\\begin{align}\nF(z) & = G(z) + G(z)^2 + G(z)^3 + \\cdots \\\\[6pt]\n     & = \\frac{G(z)}{1-G(z)} \\\\[6pt]\n     & = \\frac{z}{(1-z)^2-z} = \\frac{z}{1-3z+z^2},\n\\end{align}\n</math>\n\nfrom which we are able to extract an exact formula for the sequence by taking the [[partial fraction expansion]] of the last generating function.\n\n===Implicit generating functions and the Lagrange inversion formula===\n\n{{expand section|This section needs to be added to the list of techniques with generating functions|date=April 2017}}\n\n===Introducing a free parameter (snake oil method)===\nSometimes the sum <math>s_n</math> is complicated, and it is not always easy to evaluate. The \"Free Parameter\" method is another method (called \"snake oil\" by H. Wilf) to evaluate these sums.\n\nBoth methods discussed so far have <math>n</math> as limit in the summation. When n does not appear explicitly in the summation, we may consider <math>n</math> as a “free” parameter and treat <math>s_n</math> as a coefficient of <math>F(z) = \\sum{s_n z^n}</math>, change the order of the summations on <math>n</math> and <math>k</math>, and try to compute the inner sum.\n\nFor example, if we want to compute\n\n:<math>s_n = \\sum_{k\\geq0}{\\binom{n+k}{m+2k}\\binom{2k}{k}\\frac{(-1)^k}{k+1}} \\quad (m,n \\in \\mathbb{N}_0)</math>\n\nwe can treat <math>n</math> as a \"free\" parameter, and set\n\n:<math>F(z) = \\sum_{n\\geq0}{\\left[ \\sum_{k\\geq0}{\\binom{n+k}{m+2k}\\binom{2k}{k}\\frac{(-1)^k}{k+1}}\\right] }z^n</math>\n\nInterchanging summation (“snake oil”) gives\n\n:<math>F(z) = \\sum_{k\\geq0}{\\binom{2k}{k}\\frac{(-1)^k}{k+1} z^{-k}}\\sum_{n\\geq0}{\\binom{n+k}{m+2k} z^{n+k}}</math>\n\nNow the inner sum is <math>\\frac{z^{m+2k}}{(1-z)^{m+2k+1}}</math>. Thus\n\n:<math>\n\\begin{align}\nF(z) \n&= \\frac{z^m}{(1-z)^{m+1}}\\sum_{k\\geq0}{\\frac{1}{k+1}\\binom{2k}{k}(\\frac{-z}{(1-z)^2})^k} \\\\\n&= \\frac{z^m}{(1-z)^{m+1}}\\sum_{k\\geq0}{C_k(\\frac{-z}{(1-z)^2})^k} \\quad \\text{(where } C_k = k^\\text{th} \\text{ Catalan number)} \\\\\n&= \\frac{z^m}{(1-z)^{m+1}}\\frac{1-\\sqrt{1+\\frac{4z}{(1-z)^2}}}{\\frac{-2z}{(1-z)^2}} \\\\\n&= \\frac{-z^{m-1}}{2(1-z)^{m-1}}(1-\\frac{1+z}{1-z}) \\\\\n&= \\frac{z^m}{(1-z)^m} = z\\frac{z^{m-1}}{(1-z)^m}.\n\\end{align}\n</math>\n\nThen we obtain\n\n:<math>s_n = \\binom{n-1}{m-1}\\quad \\text{ for } \\quad m\\geq1 \\quad,\\quad s_n = [n = 0]\\quad \\text{ for } \\quad m = 0.</math>\n\n===Generating functions prove congruences===\n\nWe say that two generating functions (power series) are congruent modulo <math>m</math>, written <math>A(z) \\equiv B(z) \\pmod{m}</math> if their coefficients are congruent modulo <math>m</math> for all <math>n \\geq 0</math>, i.e., <math>a_n \\equiv b_n \\pmod{m}</math> for all relevant cases of the integers <math>n</math> (note that we need not assume that <math>m</math> is an integer here—it may very well be polynomial-valued in some indeterminate <math>x</math>, for example). If the \"''simpler''\" right-hand-side generating function, <math>B(z)</math>, is a rational function of <math>z</math>, then the form of this sequences suggests that the sequence is [[periodic function|eventually periodic]] modulo fixed particular cases of integer-valued <math>m \\geq 2</math>. For example, we can prove that the [[Euler numbers]], <math>\\langle E_n \\rangle = \\langle 1, 1, 5, 61, 1385, \\ldots \\rangle \\longmapsto \\langle 1,1,2,1,2,1,2,\\ldots \\rangle \\pmod{3}</math>, satisfy the following congruence modulo <math>3</math>:<ref>See Section 5 of Lando's ''Lectures on Generating Functions''.</ref>\n\n:<math>\\sum_{n \\geq 0} E_n z^n = \\frac{1-z^2}{1+z^2} \\pmod{3}. </math>\n\nOne of the most useful, if not downright powerful, methods of obtaining congruences for sequences enumerated by special generating functions modulo any integers (i.e., ''not only prime powers'' <math>p^k</math>) is given in the section on continued fraction representations of (even non-convergent) ordinary generating functions by J-fractions above. We cite one particular result related to generating series expanded through a representation by continued fraction from Lando's ''Lectures on Generating Functions'' as follows:\n\n::'''Theorem: (Congruences for Series Generated by Expansions of Continued Fractions)''' Suppose that the generating function <math>A(z)</math> is represented by an infinite [[continued fraction]] of the form\n\n::<math>A(z) = \\frac{1}{1-c_1 z} \\frac{p_1 z^2}{1-c_2z} \\frac{p_2 z^2}{1-c_3z} \\cdots , </math>\n\n::and that <math>A_p(z)</math> denotes the <math>p^{th}</math> convergent to this continued fraction expansion defined such that <math>a_n = [z^n] A_p(z)</math> for all <math>0 \\leq n < 2p</math>. Then 1) the function <math>A_p(z)</math> is rational for all <math>p \\geq 2</math> where we assume that one of divisibility criteria of <math>p|p_1, p_1p_2, p_1p_2p_3\\cdots</math> is met, i.e., <math>p | p_1 p_2 \\cdots p_k</math> for some <math>k \\geq 1</math>; and 2) If the integer <math>p</math> divides the product <math>p_1 p_2 \\cdots p_k</math>, then we have that <math>A(z) \\equiv A_k(z) \\pmod{p}</math>.\n\nGenerating functions also have other uses in proving congruences for their coefficients. We cite the next two specific examples deriving special case congruences for the [[Stirling numbers of the first kind]] and for the [[partition function (mathematics)]] <math>p(n)</math> which show the versatility of generating functions in tackling problems involving [[integer sequences]].\n\n====The Stirling numbers modulo small integers====\n\nThe [[Stirling numbers of the first kind#Congruences|main article]] on the Stirling numbers generated by the finite products\n\n:<math>S_n(x) := \\sum_{k=0}^n \\left[\\begin{matrix} n \\\\ k \\end{matrix} \\right] x^k = x(x+1)(x+2) \\cdots (x+n-1),\\ n \\geq 1, </math>\n\nprovides an overview of the congruences for these numbers derived strictly from properties of their generating function as in Section 4.6 of Wilf's stock reference ''Generatingfunctionology''. \nWe repeat the basic argument and notice that when reduces modulo <math>2</math>, these finite product generating functions each satisfy\n\n:<math>S_n(x) = [x(x+1)] \\cdot [x(x+1)] \\cdots = x^{\\lceil n/2 \\rceil} (x+1)^{\\lfloor n/2 \\rfloor}, </math>\n\nwhich implies that the parity of these [[Stirling numbers]] matches that of the binomial coefficient\n\n:<math>\\left[\\begin{matrix} n \\\\ k \\end{matrix} \\right] \\equiv \\binom{\\lfloor n/2 \\rfloor}{k - \\lceil n/2 \\rceil} \\pmod{2}, </math>\n\nand consequently shows that <math>\\left[\\begin{matrix} n \\\\ k \\end{matrix} \\right]</math> is even whenever <math>k < \\left\\lceil \\frac{n}{2} \\right\\rceil</math>.\n\nSimilarly, we can reduce the right-hand-side products defining the Stirling number generating functions modulo <math>3</math> to obtain slightly more complicated expressions providing that\n\n:<math> \n\\begin{align} \n\\left[\\begin{matrix} n \\\\ m \\end{matrix} \\right] & \\equiv \n     [x^m] \\left( \n     x^{\\lceil n/3 \\rceil} (x+1)^{\\lceil (n-1)/3 \\rceil} \n     (x+2)^{\\lfloor n/3 \\rfloor} \n     \\right) && \\pmod{3} \\\\ \n     & \\equiv \n     \\sum_{k=0}^{m} \\binom{\\lceil (n-1)/3 \\rceil}{k} \n     \\binom{\\lfloor n/3 \\rfloor}{m-k - \\lceil n/3 \\rceil} \\times \n     2^{\\lceil n/3 \\rceil + \\lfloor n/3 \\rfloor -(m-k)} && \\pmod{3}. \n\\end{align} \n</math>\n\n====Congruences for the partition function====\n\nIn this example, we pull in some of the machinery of infinite products whose power series expansions generate the expansions of many special functions and enumerate partition functions. In particular, we recall that ''the'' [[partition function (number theory)|partition function]] <math>p(n)</math> is generated by the reciprocal infinite [[q-Pochhammer symbol]] product (or z-Pochhammer product as the case may be) given by\n\n: <math>\n\\begin{align}\n\\sum_{n \\geq 1} p(n) z^n & = \\frac{1}{(1-z)(1-z^2)(1-z^3) \\cdots} \\\\[4pt]\n& = 1 + z + 2z^2 + 3 z^3 + 5z^4 + 7z^5 + 11z^6 + \\cdots.\n\\end{align}\n</math>\n\nThis partition function satisfies many known [[Ramanujan's congruences|congruence properties]], which notably include the following results though there are still many open questions about the forms of related integer congruences for the function:<ref>See Section 19.12 of Hardy and Wright's classic book ''An introduction to the theory of numbers''.</ref>\n\n:<math>\n\\begin{align} \np(5m+4) & \\equiv 0 \\pmod{5} \\\\ \np(7m+5) & \\equiv 0 \\pmod{7} \\\\ \np(11m+4) & \\equiv 0 \\pmod{11} \\\\ \np(25m+24) & \\equiv 0 \\pmod{5^2}. \n\\end{align} \n</math>\n\nWe show how to use generating functions and manipulations of congruences for formal power series to give a highly elementary proof of the first of these congruences listed above.\n\nFirst, we observe that the binomial coefficient generating function, <math>1 / (1-z)^5</math>, satisfies that each of its coefficients are divisible by <math>5</math> with the exception of those which correspond to the powers of <math>1, z^5, z^{10}, \\ldots</math>, all of which otherwise have a remainder of <math>1</math> modulo <math>5</math>. Thus we may write\n\n:<math>\\frac{1}{(1-z)^5} \\equiv \\frac{1}{1-z^5} \\pmod{5} \\qquad \\iff \\qquad \\frac{1-z^5}{(1-z)^{5}} \\equiv 1 \\pmod{5}, </math>\n\nwhich in particular shows us that\n\n:<math>\\frac{(1-z^5)(1-z^{10})(1-z^{15}) \\cdots }{\\left\\{(1-z)(1-z^2)(1-z^3) \\cdots \\right\\}^{5}} \\equiv 1 \\pmod{5}. </math>\n\nHence, we easily see that <math>5</math> divides each coefficient of <math>z^{5m+1}</math> in the infinite product expansions of\n\n:<math>z \\cdot \\frac{(1-z^5)(1-z^{10}) \\cdots }{(1-z)(1-z^2) \\cdots } = z \\cdot \\left\\{(1-z)(1-z^2) \\cdots \\right\\}^{4} \\times \\frac{(1-z^5)(1-z^{10}) \\cdots }{\\left\\{(1-z)(1-z^2) \\cdots \\right\\}^{5}}. </math>\n\nFinally, since we may write the generating function for the partition function as\n\n:<math>\n\\begin{align} \n& \\frac{z}{(1-z)(1-z^2) \\cdots} \\\\[5pt]\n= {} & z \\cdot \\frac{(1-z^5)(1-z^{10}) \\cdots }{(1-z)(1-z^2) \\cdots } \\times (1+z^5+z^{10}+\\cdots)(1+z^{10}+z^{20}+\\cdots) \\cdots \\\\[5pt] \n= {} & z + \\sum_{n \\geq 2} p(n-1) z^n, \n\\end{align} \n</math>\n\nwe may equate the coefficients of <math>z^{5m+5}</math> in the previous equations to prove our desired congruence result, namely that, <math>p(5m+4) \\equiv 0 \\pmod{5}</math> for all <math>m \\geq 0</math>.\n\n===Transformations of generating functions===\n\nThere are a number of transformations of generating functions that provide other applications (see the [[generating function transformation|main article]]). A transformation of a sequence's ''ordinary generating function'' (OGF) provides a method of converting the generating function for one sequence into a generating function enumerating another. These transformations typically involve integral formulas involving a sequence OGF (see [[Generating function transformation#Integral Transformations|integral transformations]]) or weighted sums over the higher-order derivatives of these functions (see [[Generating function transformation#Derivative Transformations|derivative transformations]]).\n\nGenerating function transformations can come into play when we seek to express a generating function for the sums\n\n:<math>s_n := \\sum_{m=0}^{n} \\binom{n}{m} C_{n,m} a_m, </math>\n\nin the form of <math>S(z) = g(z) A(f(z))</math> involving the original sequence generating function. For example, if the sums <math>s_n := \\sum_{k \\geq 0} \\binom{n+k}{m+2k} a_k</math>, then the generating function for the modified sum expressions is given by <math>S(z) = \\frac{z^m}{(1-z)^{m+1}} A\\left(\\frac{z}{(1-z)^2}\\right)</math> <ref>Solution to exercise 5.71 on page 535 in ''Concrete Mathematics'' by Graham, Knuth and Patashnik.</ref> (see also the [[binomial transform]] and the [[Stirling transform]]).\n\nThere are also integral formulas for converting between a sequence's OGF, <math>F(z)</math>, and its exponential generating function, or EGF, <math>\\widehat{F}(z)</math>, and vice versa given by\n\n:<math>F(z) = \\int_0^\\infty \\widehat{F}(tz) e^{-t} \\, dt</math>\n:<math>\\widehat{F}(z) = \\frac{1}{2\\pi} \\int_{-\\pi}^\\pi F\\left(z e^{-\\imath\\vartheta}\\right) e^{e^{\\imath\\vartheta}} \\, d\\vartheta, </math>\n\nprovided that these integrals converge for appropriate values of <math>z</math>.\n\n===Other applications===\nGenerating functions are used to:\n\n* Find a [[closed formula]] for a sequence given in a recurrence relation. For example, consider [[Fibonacci number#Power series|Fibonacci numbers]].\n* Find [[recurrence relation]]s for sequences—the form of a generating function may suggest a recurrence formula.\n* Find relationships between sequences—if the generating functions of two sequences have a similar form, then the sequences themselves may be related.\n* Explore the asymptotic behaviour of sequences.\n* Prove identities involving sequences.\n* Solve [[enumeration]] problems in [[combinatorics]] and encoding their solutions. [[Rook polynomial]]s are an example of an application in combinatorics.\n* Evaluate infinite sums.\n\n==Other generating functions==\n\n===Examples===\n\nExamples of [[polynomial sequence]]s generated by more complex generating functions include:\n\n* [[Appell polynomials]]\n* [[Chebyshev polynomials]]\n* [[Difference polynomials]]\n* [[Generalized Appell polynomials]]\n* [[Q-difference polynomial]]s\n\nOther sequences generated by more complex generating functions:\n\n* Double exponential generating functions. For example: [https://oeis.org/search?q=1%2C1%2C2%2C2%2C3%2C5%2C5%2C7%2C10%2C15%2C15&sort=&language=&go=Search Aitken's Array: Triangle of Numbers]\n* Hadamard products of generating functions / diagonal generating functions and their corresponding [[generating function transformation#Hadamard products and diagonal generating functions|integral transformations]]\n\n===Convolution polynomials===\n\nKnuth's article titled \"''Convolution Polynomials''\"<ref>{{cite journal|last1=Knuth|first1=D. E.|title=Convolution Polynomials|journal=Mathematica J.|date=1992|volume=2|pages=67–78|url=https://arxiv.org/pdf/math/9207221.pdf}}</ref> defines a generalized class of ''convolution polynomial'' sequences by their special generating functions of the form\n\n:<math>F(z)^x = \\exp\\left(x \\log F(z)\\right) = \\sum_{n \\geq 0} f_n(x) z^n,</math>\n\nfor some analytic function <math>F</math> with a power series expansion such that <math>F(0) = 1</math>. \nWe say that a family of polynomials, <math>f_0, f_1, f_2, \\ldots</math>, forms a ''convolution family'' if <math>\\deg\\{f_n\\} \\leq n</math> and if the following convolution condition holds for all <math>x, y</math> and for all <math>n \\geq 0</math>:\n\n:<math>f_n(x+y) = f_n(x) f_0(y) + f_{n-1}(x) f_1(y) + \\cdots + f_1(x) f_{n-1}(y) + f_0(x) f_n(y). </math>\n\nWe see that for non-identically zero convolution families, this definition is equivalent to requiring that the sequence have an ordinary generating function of the first form given above.\n\nA sequence of convolution polynomials defined in the notation above has the following properties:\n\n* The sequence <math>n! \\cdot f_n(x)</math> is of [[binomial type]]\n* Special values of the sequence include <math>f_n(1) = [z^n] F(z)</math> and <math>f_n(0) = \\delta_{n,0}</math>, and \n* For arbitrary (fixed) <math>x, y, t \\in \\mathbb{C}</math>, these polynomials satisfy convolution formulas of the form\n\n:<math>\n\\begin{align} \nf_n(x+y) & = \\sum_{k=0}^n f_k(x) f_{n-k}(y) \\\\ \nf_n(2x) & = \\sum_{k=0}^n f_k(x) f_{n-k}(x) \\\\ \nxn f_n(x+y) & = (x+y) \\sum_{k=0}^n k f_k(x) f_{n-k}(y) \\\\ \n\\frac{(x+y) f_n(x+y+tn)}{x+y+tn} & = \\sum_{k=0}^n \\frac{x f_k(x+tk)}{x+tk} \\frac{y f_{n-k}(y+t(n-k))}{y+t(n-k)}. \n\\end{align} \n</math>\n\nFor a fixed non-zero parameter <math>t \\in \\mathbb{C}</math>, we have modified generating functions for these convolution polynomial sequences given by\n\n: <math>\\frac{z F_n(x+tn)}{(x+tn)} = [z^n] \\mathcal{F}_t(z)^x, </math>\n\nwhere <math>\\mathcal{F}_t(z)</math> is implicitly defined by a [[functional equation]] of the form <math>\\mathcal{F}_t(z) = F\\left(x \\mathcal{F}_t(z)^t\\right)</math>. \nMoreover, we can use matrix methods (as in the reference) to prove that given two convolution polynomial sequences, <math>\\langle f_n(x) \\rangle</math> and <math>\\langle g_n(x) \\rangle</math>, with respective corresponding generating functions, <math>F(z)^x</math> and <math>G(z)^x</math>, then for arbitrary <math>t</math> we have the identity\n\n:<math>[z^n] \\left(G(z) F\\left(z G(z)^t\\right)\\right)^x = \\sum_{k=0}^n F_k(x) G_{n-k}(x+tk). </math>\n\nExamples of convolution polynomial sequences include the ''binomial power series'', <math>\\mathcal{B}_t(z) = 1 + z \\mathcal{B}_t(z)^t</math>, so-termed ''tree polynomials'', the [[Bell numbers]], <math>B(n)</math>, the [[Laguerre polynomials]], and the [[Stirling polynomial|Stirling convolution polynomials]].\n\n===Tables of special generating functions===\n\nAn initial listing of special mathematical series is found [[List of mathematical series|here]]. A number of useful and special sequence generating functions are found in Section 5.4 and 7.4 of ''Concrete Mathematics'' and in Section 2.5 of Wilf's ''Generatingfunctionology''. Other special generating functions of note include the entries in the next table, which is by no means complete.<ref>See also the ''1031 Generating Functions'' found in the article referenced [http://www.lacim.uqam.ca/%7Eplouffe/articles/FonctionsGeneratrices.pdf here].</ref>\n\n{{expand section|Lists of special and special sequence generating functions. The next table is a start|date=April 2017}}\n\n{| class=\"wikitable\"\n|-\n! Formal power series !! Generating-function formula !! Notes\n|-\n| <math>\\sum_{n \\geq 0} \\binom{m+n}{n} \\left(H_{n+m}-H_m\\right) z^n</math> || <math>\\frac{1}{(1-z)^{m+1}} \\ln \\frac{1}{1-z}</math> || <math>H_n</math> is a first-order [[harmonic number]]\n|-\n| <math>\\sum_{n \\geq 0} B_n \\frac{z^n}{n!}</math> || <math>\\frac{z}{e^z-1}</math> || <math>B_n</math> is a [[Bernoulli number]]\n|-\n| <math>\\sum_{n \\geq 0} F_{mn} z^n</math> || <math>\\frac{F_m z}{1-(F_{m-1}+F_{m+1})z+(-1)^m z^2}</math> || <math>F_n</math> is a [[Fibonacci number]] and <math>m \\in \\mathbb{Z}^{+}</math>\n|-\n| <math>\\sum_{n \\geq 0} \\left\\{\\begin{matrix} n \\\\ m \\end{matrix} \\right\\} z^n</math> || <math>(z^{-1})^{\\overline{-m}} = \\frac{z^m}{(1-z)(1-2z)\\cdots(1-mz)}</math> || <math>x^{\\overline{n}}</math> denotes the [[rising factorial]], or [[Pochhammer symbol]] and some integer <math>m \\geq 0</math>\n|-\n| <math>\\sum_{n \\geq 0} \\left[\\begin{matrix} n \\\\ m \\end{matrix} \\right] z^n</math> || <math>z^{\\overline{m}} = z(z+1) \\cdots (z+m-1)</math>\n|- \n| <math>\\sum_{n \\geq 1} \\frac{(-1)^{n-1}4^n (4^n-2) B_{2n} z^{2n}}{(2n) \\cdot (2n)!}</math> || <math>\\ln \\frac{\\tan(z)}{z}</math>\n|-\n| <math>\\sum_{n \\geq 0} \\frac{(1/2)^{\\overline{n}} z^{2n}}{(2n+1) \\cdot n!}</math> || <math>z^{-1} \\arcsin(z)</math>\n|- \n| <math>\\sum_{n \\geq 0} H_n^{(s)} z^n</math> || <math>\\frac{\\operatorname{Li}_s(z)}{1-z}</math> || <math>\\operatorname{Li}_s(z)</math> is the [[polylogarithm]] function and <math>H_n^{(s)}</math> is a generalized [[harmonic number]] for <math>\\Re(s) > 1</math> \n|-\n| <math>\\sum_{n \\geq 0} n^m z^n</math> || <math>\\sum_{0 \\leq j \\leq m} \\left\\{\\begin{matrix} m \\\\ j \\end{matrix} \\right\\} \\frac{j! \\cdot z^j}{(1-z)^{j+1}}</math> || <math>\\left\\{\\begin{matrix} n \\\\ m \\end{matrix} \\right\\}</math> is a [[Stirling number of the second kind]] and where the individual terms in the expansion satisfy <math>\\frac{z^i}{(1-z)^{i+1}} = \\sum_{k=0}^{i} \\binom{i}{k} \\frac{(-1)^{k-i}}{(1-z)^{k+1}}</math>\n|-\n| <math>\\sum_{k < n} \\binom{n-k}{k} \\frac{n}{n-k} z^k</math> || <math>\\left(\\frac{1+\\sqrt{1+4z}}{2}\\right)^n + \\left(\\frac{1-\\sqrt{1+4z}}{2}\\right)^n</math> || \n|-\n| <math>\\sum_{n_1, \\ldots, n_m \\geq 0} \\min(n_1, \\ldots, n_m) z_1^{n_1} \\cdots z_m^{n_m}</math> || <math>\\frac{z_1 \\cdots z_m}{(1-z_1) \\cdots (1-z_m) (1-z_1 \\cdots z_m)}</math> || The two-variable case is given by <math>M(w, z) := \\sum_{m,n \\geq 0} \\min(m, n) w^m z^n = \\frac{wz}{(1-w)(1-z)(1-wz)}</math> \n|- \n| <math>\\sum_{n \\geq 0} \\binom{s}{n} z^n</math> || <math>(1+z)^s</math> || <math>s \\in \\mathbb{C}</math> \n|- \n| <math>\\sum_{n \\geq 0} \\binom{n}{k} z^n</math> || <math>\\frac{z^k}{(1-z)^{k+1}}</math> || <math>k \\in \\mathbb{N}</math> \n|}\n\n== History ==\n[[George Pólya]] writes in ''[[Mathematics and plausible reasoning]]'':\n: ''The name \"generating function\" is due to [[Laplace]]. Yet, without giving it a name, [[Euler]] used the device of generating functions long before Laplace [..]. He applied this mathematical tool to several problems in Combinatory Analysis and the [[Number theory|Theory of Numbers]].''\n\n==See also==\n* [[Moment-generating function]]\n* [[Probability-generating function]]\n* [[Generating function transformation]]\n* [[Stanley's reciprocity theorem]]\n* Applications to [[Partition (number theory)]]\n* [[Combinatorial principles]]\n* [[Cyclic sieving]]\n* [[Z-transform]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n\n* {{cite journal |title=On the foundations of combinatorial theory. VI. The idea of generating function |last1=Doubilet |first1=Peter | author1-link= |last2=Rota | first2=Gian-Carlo | author2-link=Gian-Carlo Rota | last3=Stanley | first3=Richard | author3-link=Richard P. Stanley | journal=Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability |volume=2 |pages=267–318 |year=1972 | zbl=0267.05002 | url=http://projecteuclid.org/euclid.bsmsp/1200514223 |postscript=. }}  Reprinted in {{cite book | last=Rota | first=Gian-Carlo | authorlink=Gian-Carlo Rota | others=With the collaboration of P. Doubilet, C. Greene, D. Kahaner, [[Andrew Odlyzko|A. Odlyzko]] and [[Richard P. Stanley|R. Stanley]] | title=Finite Operator Calculus | chapter=3. The idea of generating function | pages=83–134 | publisher=Academic Press | year=1975 | isbn=0-12-596650-4 | zbl=0328.05007 }}\n* {{cite book | last1 = Flajolet | first1 = Philippe | authorlink1 = Philippe Flajolet | last2 = Sedgewick | first2 = Robert | authorlink2 = Robert Sedgewick (computer scientist) | title = Analytic Combinatorics | url = http://algo.inria.fr/flajolet/Publications/book.pdf | year = 2009 | publisher = Cambridge University Press | location = | isbn = 978-0-521-89806-5 | zbl=1165.05001 }}\n* {{cite book | last1 = Goulden | first1 = Ian P. | last2 = Jackson | first2 = David M. | authorlink2 = David M. Jackson | title = Combinatorial Enumeration  | year = 2004 | publisher = [[Dover Publications]]  | isbn = 978-0486435978 }}\n* {{cite book |title=[[Concrete Mathematics|Concrete Mathematics. A foundation for computer science]] |edition=second |year=1994 |publisher=Addison-Wesley |isbn=0-201-55802-5 |chapter=Chapter 7: Generating Functions |pages=320–380| zbl=0836.00001 |author1 = Ronald L. Graham|author2 = Donald E. Knuth|author3=Oren Patashnik |authorlink1=Ronald Graham |authorlink2=Donald Knuth |authorlink3=Oren Patashnik }}\n* {{cite book | last=Wilf | first=Herbert S. | authorlink=Herbert Wilf | title=Generatingfunctionology | edition=2nd | location=Boston, MA | publisher=Academic Press | year=1994 | isbn=0-12-751956-4 | zbl=0831.05001 | url=http://www.math.upenn.edu/%7Ewilf/DownldGF.html }}\n*Martin Aigner. A Course in Enumeration\n\n==External links==\n* [http://garsia.math.yorku.ca/~zabrocki/MMM1/MMM1Intro2OGFs.pdf \"Introduction To Ordinary Generating Functions\"] by Mike Zabrocki, York University, Mathematics and Statistics\n* {{springer|title=Generating function|id=p/g043900}}\n* [http://www.cut-the-knot.org/ctk/GeneratingFunctions.shtml Generating Functions, Power Indices and Coin Change] at [[cut-the-knot]]\n* [http://demonstrations.wolfram.com/GeneratingFunctions/ \"Generating Functions\"] by [[Ed Pegg, Jr.]], [[Wolfram Demonstrations Project]], 2007.\n\n{{DEFAULTSORT:Generating Function}}\n[[Category:Generating functions| ]]"
    },
    {
      "title": "Betti number",
      "url": "https://en.wikipedia.org/wiki/Betti_number",
      "text": "In [[algebraic topology]], the '''Betti numbers''' are used to distinguish [[topological space]]s based on the connectivity of ''n''-dimensional [[simplicial complex]]es. For the most reasonable finite-dimensional spaces (such as compact manifolds, finite simplicial complexes or CW complexes), the sequence of Betti numbers is 0 from some point onward (Betti numbers vanish above the dimension of a space), and they are all finite.\n\nThe ''n''<sup>th</sup> Betti number represents the rank of the ''n''<sup>th</sup> [[homology group]], denoted ''H''<sub>''n''</sub>, which tells us the maximum amount of cuts that must be made before separating a surface into two pieces or 0-cycles, 1-cycles, etc.<ref>{{cite web|last=Barile, and Weisstein|first=Margherita and Eric|title=Betti number|url=http://mathworld.wolfram.com/BettiNumber.html|publisher=From MathWorld--A Wolfram Web Resource.}}</ref> These numbers are used today in fields such as [[simplicial homology]], [[computer science]], [[digital images]], etc.\n\nThe term \"Betti numbers\" was coined by [[Henri Poincaré]] after [[Enrico Betti]]. The modern formulation is due to [[Emmy Noether#Contributions to topology|Emmy Noether]].\n\n==Definitions==\nInformally, the ''k''th Betti number refers to the number of ''k''-dimensional holes on a topological surface. The first few Betti numbers have the following definitions for 0-dimensional, 1-dimensional, and 2-dimensional [[simplicial complex]]es:\n[[File:Torus.png|thumb|alt=A torus.|A torus has Betti numbers b<sub>0</sub>&nbsp;=&nbsp;1, b<sub>1</sub>&nbsp;=&nbsp;2, and b<sub>2</sub>&nbsp;=&nbsp;1]]\n* b<sub>0</sub> is the number of connected components\n* b<sub>1</sub> is the number of one-dimensional or \"circular\" holes \n* b<sub>2</sub> is the number of two-dimensional \"voids\" or \"cavities\"\n\nThus, for example, a torus has one connected surface component so b<sub>0</sub> = 1, a circular (two-dimensional) hole at each end of the central space so b<sub>1</sub> = 2 and a single cavity enclosed within the surface so b<sub>2</sub> = 1.\n\nClosely related to the Betti numbers of a topological surface is the '''Poincaré polynomial''' of that surface.  The Poincaré polynomial of a surface is defined to be the [[generating function]] of its Betti numbers.  For example, the Betti numbers of the torus are 1, 2, and 1; thus its Poincaré polynomial is <math>1+2x+x^2</math>. The same definition applies to any topological space  which has a finitely generated homology.\n\nThe two-dimensional Betti numbers are easier to understand because we see the world in 0, 1, 2, and 3-dimensions; however, the succeeding Betti numbers are of higher-dimension than apparent physical space.\n\nFor a non-negative [[integer]]&nbsp;''k'', the ''k''th Betti number ''b''<sub>''k''</sub>(''X'') of the space ''X'' is defined  as the [[rank of an abelian group|rank]] (number of linearly independent generators) of the [[abelian group]] ''H''<sub>''k''</sub>(''X''), the ''k''th [[homology group]] of&nbsp;''X''.  The ''k''th homology group is <math> H_{k} = \\ker \\delta_{k} / \\mathrm{Im} \\delta_{k+1} </math>,  the <math> \\delta_{k}s </math> are the boundary maps of the [[simplicial complex]] and the rank of H<sub>k</sub> is the ''k''th Betti number. Equivalently, one can define it as the [[vector space dimension]] of ''H''<sub>''k''</sub>(''X'';&nbsp;'''Q''') since the homology group in this case is a vector space over&nbsp;'''Q'''. The [[universal coefficient theorem]], in a very simple torsion-free case, shows that these definitions are the same.\n\nMore generally, given a [[Field (mathematics)|field]] ''F'' one can define ''b''<sub>''k''</sub>(''X'',&nbsp;''F''), the ''k''th Betti number with coefficients in ''F'', as the vector space dimension of ''H''<sub>''k''</sub>(''X'',&nbsp;''F'').\n\nGiven a topological space  which has finitely generated homology, the Poincaré polynomial is defined as the generating function of its Betti numbers, viz the polynomial where the coefficient of <math>x^n</math> is <math>b_n</math>.\n\n==Example 1: Betti numbers of a simplicial complex ''K''==\n[[File:Simplicialexample.png|160x320px|alt=Example|right]]\nThis is a simple example of how to compute the Betti numbers for a [[simplicial complex]].\n\nWe have a [[simplicial complex]] with 0-simplices: a, b, c, and d,\n1-simplices: E, F, G, H and I, and the only 2-simplex is J, which is the shaded region in the figure.\nIt is clear that there is one connected component in this figure (b<sub>0</sub>);\none hole, which is the unshaded region (b<sub>1</sub>); and no \"voids\" or \"cavities\" (''b''<sub>2</sub>).\n\nThis means that the rank of <math>H_0</math> is 1, the rank of <math>H_{1}</math> is 1 and the rank of <math>H_2</math> is 0.\n\nThe Betti number sequence for this figure is 1,1,0,0,...; the Poincaré polynomial is\n<math>1 +x \\,</math>\n\n==Example 2: the first Betti number in graph theory==\nIn [[topological graph theory]] the first Betti number of a graph ''G'' with ''n'' vertices, ''m'' edges and ''k'' [[Connected component (graph theory)|connected component]]s equals\n\n:<math>m - n + k.\\ </math>\n\nThis may be proved straightforwardly by [[mathematical induction]] on the number of edges. A new edge either increments the number of 1-cycles or decrements the number of connected components.\n\nThe first Betti number is also called the [[cyclomatic number]]—a term introduced by [[Gustav Kirchhoff]] before Betti's paper.<ref name=\"Kotiuga2010\">{{cite book|author=Peter Robert Kotiuga|title=A Celebration of the Mathematical Legacy of Raoul Bott|url=https://books.google.com/books?id=mqLXi0FRIZwC&pg=PA20|year=2010|publisher=American Mathematical Soc.|isbn=978-0-8218-8381-5|page=20}}</ref> See [[cyclomatic complexity]] for an application to [[software engineering]].\n\nThe \"zero-th\" Betti number of a graph is simply the number of connected components ''k''.<ref name=\"Hage1996\">{{cite book|author=Per Hage|title=Island Networks: Communication, Kinship, and Classification Structures in Oceania|url=https://books.google.com/books?id=ZBdLknuP0BYC&pg=PA49|year=1996|publisher=Cambridge University Press|isbn=978-0-521-55232-5|page=49}}</ref>\n\n==Properties==\nThe (rational) Betti numbers ''b''<sub>''k''</sub>(''X'') do not take into account any [[torsion subgroup|torsion]] in the homology groups, but they are very useful basic topological invariants. In the most intuitive terms, they allow one to count the number of ''holes'' of different dimensions.\n\nFor a finite CW-complex ''K'' we have\n\n:<math>\\chi(K)=\\sum_{i=0}^\\infty(-1)^ib_i(K,F), \\,</math>\n\nwhere <math>\\chi(K)</math> denotes [[Euler characteristic]] of ''K'' and any field&nbsp;''F''.\n\nFor any two spaces ''X'' and ''Y'' we have\n\n:<math>P_{X\\times Y}=P_X P_Y ,</math>\n\nwhere <math>P_X</math> denotes the '''Poincaré polynomial''' of ''X'', (more generally, the [[Hilbert–Poincaré series]], for infinite-dimensional spaces), i.e., the\n[[generating function]] of the Betti numbers of ''X'':\n:<math>P_X(z)=b_0(X)+b_1(X)z+b_2(X)z^2+\\cdots , \\,\\!</math>\nsee  [[Künneth theorem]].\n\nIf ''X'' is ''n''-dimensional manifold, there is symmetry interchanging <math>k</math> and <math>n-k</math>, for any <math>k</math>:\n\n:<math>b_k(X)=b_{n-k}(X) ,</math>\n\nunder conditions (a ''closed'' and ''oriented'' manifold); see [[Poincaré duality]].\n\nThe dependence on the field ''F'' is only through its [[characteristic (field)|characteristic]]. If the homology groups are [[torsion (algebra)|torsion-free]], the Betti numbers are independent of ''F''. The connection of ''p''-torsion and the Betti number for [[characteristic p|characteristic&nbsp;''p'']], for ''p'' a prime number, is given in detail by the [[universal coefficient theorem]] (based on [[Tor functor]]s, but in a simple case).\n\n==Examples==\n#The Betti number sequence for a circle is 1, 1, 0, 0, 0, ...;\n#:the Poincaré polynomial is\n#::<math>1+x \\,</math>.\n#The Betti number sequence for a three-[[torus]] is 1, 3, 3, 1, 0, 0, 0, ... .\n#:the Poincaré polynomial is\n#::<math>(1+x)^3=1+3x+3x^2+x^3 \\,</math>.\n#Similarly, for an ''n''-[[torus]],\n#:the Poincaré polynomial is\n#::<math>(1+x)^n \\,</math> (by the [[Künneth theorem]]), so the Betti numbers are the [[binomial coefficient]]s.\n\nIt is possible for spaces that are infinite-dimensional in an essential way to have an infinite sequence of non-zero Betti numbers. An example is the infinite-dimensional [[complex projective space]], with sequence 1, 0, 1, 0, 1, ... that is periodic, with [[period length]] 2.\nIn this case the Poincaré function is not a polynomial but rather an infinite series \n:<math>1+x^2+x^4+\\dotsb</math>,\n\nwhich, being a geometric series, can be expressed as the rational function\n: <math>\\frac{1}{1-x^2}.</math>\n\nMore generally, any sequence that is periodic can be expressed as a sum of geometric series, generalizing the above (e.g., <math>a,b,c,a,b,c,\\dots,</math> has generating function \n:<math>(a+bx+cx^2)/(1-x^3) \\,</math>\n\nand more generally [[linear recursive sequence]]s are exactly the sequences generated by [[rational functions]]; thus the Poincaré series is expressible as a rational function if and only if the sequence of Betti numbers is a linear recursive sequence.\n\nThe Poincaré polynomials of the compact simple [[Lie groups]] are:\n:<math>P_{SU(n+1)}(x) = (1+x^3)(1+x^5)\\cdots(1+x^{2n+1})</math>\n\n:<math>P_{SO(2n+1)}(x) = (1+x^3)(1+x^7)\\cdots(1+x^{4n-1})</math>\n\n:<math>P_{Sp(n)}(x) = (1+x^3)(1+x^7)\\cdots(1+x^{4n-1})</math>\n\n:<math>P_{SO(2n)}(x) = (1+x^{2n-1})(1+x^3)(1+x^7)\\cdots(1+x^{4n-5})</math>\n\n:<math>P_{G_2}(x) = (1+x^3)(1+x^{11})</math>\n\n:<math>P_{F_4}(x) = (1+x^3)(1+x^{11})(1+x^{15})(1+x^{23})</math>\n\n:<math>P_{E_6}(x) = (1+x^3)(1+x^{9})(1+x^{11})(1+x^{15})(1+x^{17})(1+x^{23})</math>\n\n:<math>P_{E_7}(x) = (1+x^3)(1+x^{11})(1+x^{15})(1+x^{19})(1+x^{23})(1+x^{27})(1+x^{35})</math>\n\n:<math>P_{E_{8}}(x) = (1+x^3)(1+x^{15})(1+x^{23})(1+x^{27})(1+x^{35})(1+x^{39})(1+x^{47})(1+x^{59})</math>\n\n==Relationship with dimensions of spaces of differential forms==\nIn geometric situations when <math>X</math> is a [[closed manifold]], the importance of the Betti numbers may arise from a different direction, namely that they predict the dimensions of vector spaces of [[closed differential form]]s ''[[Modular arithmetic|modulo]]'' [[exact differential form]]s. The connection with the definition given above is via three basic results, [[de Rham's theorem]] and [[Poincaré duality]] (when those apply), and the [[universal coefficient theorem]] of [[homology theory]].\n\nThere is an alternate reading, namely that the Betti numbers give the dimensions of spaces of [[harmonic form]]s. This requires also the use of some of the results of [[Hodge theory]], about the [[Hodge Laplacian]].\n\nIn this setting, [[Morse theory]] gives a set of inequalities for alternating sums of Betti numbers in terms of a corresponding alternating sum of the number of [[critical point (mathematics)|critical points]] <math>N_i</math> of a [[Morse function]] of a given [[Morse theory|index]]:\n\n:<math> b_i(X) - b_{i-1} (X) +  \\cdots \\le N _i - N_{i-1} + \\cdots. </math>\n\n[[Edward Witten]] gave an explanation of these inequalities by using the Morse function to modify the [[exterior derivative]] in the [[de Rham complex]].<ref>{{citation|last=Witten|first= Edward|authorlink=Edward Witten| year=1982|title=Supersymmetry and Morse theory|journal= [[Journal of Differential Geometry]] |volume=17 |issue=4|pages= 661—692|doi=10.4310/jdg/1214437492}}{{open access}}</ref>\n\n==See also==\n* [[Topological data analysis]]\n* [[Torsion coefficient (topology)|Torsion coefficient]]\n\n== References ==\n<references />\n\n*{{Citation |first=Frank Wilson |last=Warner |title=Foundations of differentiable manifolds and Lie groups |location=New York |publisher=Springer |year=1983 |isbn=0-387-90894-3 }}.\n*{{Citation |first=John |last=Roe |title=Elliptic Operators, Topology, and Asymptotic Methods |edition=Second |series=Research Notes in Mathematics Series |volume=395 |location=Boca Raton, FL |publisher=Chapman and Hall |year=1998 |isbn=0-582-32502-1 }}.\n\n{{DEFAULTSORT:Betti Number}}\n[[Category:Algebraic topology]]\n[[Category:Graph invariants]]\n[[Category:Topological graph theory]]\n[[Category:Generating functions]]"
    },
    {
      "title": "Cumulant-generating function",
      "url": "https://en.wikipedia.org/wiki/Cumulant-generating_function",
      "text": "#REDIRECT[[cumulant]]\n\n[[Category:Generating functions]]"
    },
    {
      "title": "Cyclic sieving",
      "url": "https://en.wikipedia.org/wiki/Cyclic_sieving",
      "text": "In [[combinatorics|combinatorial]] mathematics, '''cyclic sieving''' is a phenomenon by which evaluating a [[generating function]] for a finite set at [[root of unity|roots of unity]] counts symmetry classes of objects acted on by a [[cyclic group]].<ref>{{citation|first1=Victor|last1=Reiner|first2=Dennis|last2=Stanton|first3=Dennis|last3=White|title=What is... Cyclic Sieving?|journal=[[Notices of the American Mathematical Society]]|volume=61|issue=2|date=February 2014|pages=169–171|url=http://www.ams.org/notices/201402/rnoti-p169.pdf|doi=10.1090/noti1084}}</ref>\n\n== Definition ==\n\nLet ''C'' be a [[cyclic group]] generated by an element ''c'' of order&nbsp;''n''.  Suppose ''C'' [[Group action (mathematics)|acts on]] a set&nbsp;''X''.  Let ''X''(''q'') be a polynomial with integer coefficients.  Then the triple (''X'',&nbsp;''X''(''q''),&nbsp;''C'') is said to '''exhibit the cyclic sieving phenomenon (CSP)''' if for all integers&nbsp;''d'', the value ''X''(''e''<sup>2{{pi}}''id''/''n''</sup>) is the number of elements fixed by&nbsp;''c''<sup>''d''</sup>.  In particular ''X''(1) is the cardinality of the set&nbsp;''X'', and for that reason ''X''(''q'') is regarded as a generating function for&nbsp;''X''.\n\n==Examples==\nThe [[Q-binomial coefficient|''q''-binomial coefficient]]\n\n:<math>\\left[{n \\atop k}\\right]_q</math>\n\nis the polynomial in ''q'' defined by\n\n:<math>\\left[{n \\atop k}\\right]_q = \\frac{\\prod_{i = 1}^n (1 + q + q^2 + \\cdots + q^{i - 1})}{\\left(\\prod_{i = 1}^k (1 + q + q^2 + \\cdots + q^{i - 1})\\right) \\cdot \\left(\\prod_{i = 1}^{n - k} (1 + q + q^2 + \\cdots + q^{i - 1})\\right)}.</math>\n\nIt is easily seen that its value at ''q''&nbsp;=&nbsp;1 is the usual [[binomial coefficient]] <math>\\binom{n}{k}</math>, so it is a generating function for the subsets of {1,&nbsp;2,&nbsp;...,&nbsp;''n''} of size&nbsp;''k''.  These subsets carry a natural action of the cyclic group ''C'' of order ''n'' which acts by adding 1 to each element of the set, modulo&nbsp;''n''.  For example, when ''n''&nbsp;=&nbsp;4 and ''k''&nbsp;=&nbsp;2, the group orbits are\n\n: <math>\\{1, 3\\} \\to \\{2, 4\\} \\to \\{1, 3\\}</math> (of size 2)\n\nand\n\n: <math>\\{1, 2\\} \\to \\{2, 3\\} \\to \\{3, 4\\} \\to \\{1, 4\\} \\to \\{1, 2\\}</math> (of size 4).\n\nOne can show<ref>V. Reiner, D. Stanton and D. White, The cyclic sieving phenomenon, Journal of Combinatorial Theory, Series A, Volume 108 Issue 1, October 2004, Pages 17–50</ref> that evaluating the ''q''-binomial coefficient when ''q'' is an ''n''th root of unity gives the number of subsets fixed by the corresponding group element.\n\nIn the example ''n'' = 4 and ''k'' = 2, the ''q''-binomial coefficient is\n\n: <math>\\left[{4 \\atop 2}\\right]_q = 1 + q + 2q^2 + q^3 + q^4;</math>\n\nevaluating this polynomial at ''q'' = 1 gives 6 (as all six subsets are fixed by the identity element of the group); evaluating it at ''q''&nbsp;=&nbsp;−1 gives&nbsp;2 (the subsets {1, 3} and {2, 4} are fixed by two applications of the group generator); and evaluating it at ''q''&nbsp;=&nbsp;±''i'' gives 0 (no subsets are fixed by one or three applications of the group generator).\n\n===List of cyclic sieving phenomena===\n\nIn the Reiner–Stanton–White paper, the following example is given:\n\nLet α be a composition of ''n'', and let ''W''(''α'') be the set of all words of length ''n'' with α<sub>i</sub>\nletters equal to ''i''. A ''descent'' of a word ''w'' is any index ''j'' such that <math>w_j>w_{j+1}</math>.\nDefine the major index  <math>\\operatorname{maj}(w)</math> on words as the sum of all descents.\n\n----\n\nThe triple <math>(X_n,C_{n-1},\\frac{1}{[n+1]_q}\\left[{2n \\atop n}\\right]_q)</math> exhibit a cyclic sieving phenomenon, where <math>X_n</math> is the set of non-crossing (1,2)-configurations of [''n''&nbsp;−&nbsp;1].\n<ref>{{cite journal|last1=Thiel|first1=Marko|title=A new cyclic sieving phenomenon for Catalan objects|journal=Discrete Mathematics|date=March 2017|volume=340|issue=3|pages=426–429|doi=10.1016/j.disc.2016.09.006|arxiv=1601.03999}}</ref>\n\n----\n\nLet ''λ'' be a rectangular partition of size ''n'', and let ''X'' be the set of standard Young tableaux \nof shape ''λ''. Let ''C'' = ''Z''/''nZ'' act on ''X'' via promotion. Then  <math>(X,C,\\frac{[n]!_q}{\\prod_{(i,j)\\in \\lambda} [h_{ij}]_q})</math> exhibit the cyclic sieving phenomenon. Note that the polynomial is a ''q''-analogue of the [[hook length formula]].\n\nFurthermore, let ''λ'' be a rectangular partition of size ''n'', and let ''X'' be the set of semi-standard Young tableaux of shape ''λ''. Let ''C'' = ''Z''/''kZ'' act on ''X'' via ''k''-promotion.\nThen  <math>(X,C, q^{-\\kappa(\\lambda)}s_\\lambda(1,q,q^2,\\dotsc,q^{k-1} ))</math>\nexhibit the cyclic sieving phenomenon. Here, <math>\\kappa(\\lambda)=\\sum_i (i-1)\\lambda_i</math>\nand ''s''<sub>λ</sub> is the [[Schur polynomial]].\n<ref>{{cite journal|last1=Rhoades|first1=Brendon|title=Cyclic sieving, promotion, and representation theory|journal=Journal of Combinatorial Theory, Series A|date=January 2010|volume=117|issue=1|pages=38–76|doi=10.1016/j.jcta.2009.03.017}}</ref>\n\n----\n\nAn increasing tableau is a semi-standard Young tableau, where both rows and columns are strictly increasing,\nand the set of entries is of the form <math>1,2,\\dotsc,\\ell</math> for some <math>\\ell</math>.\nLet <math>Inc_k(2\\times n)</math> denote the set of increasing tableau with two rows of length ''n'',\nand maximal entry <math>2n-k</math>. Then\n<math>(\\operatorname{Inc}_k(2\\times n),C_{2n-k},  q^{n+\\binom{k}{2}} \\frac{\\left[{n-1 \\atop k}\\right]_q \\left[{2n-k \\atop n-k-1}\\right]_q}{ [n-k]_q })</math>\nexhibit the cyclic sieving phenomenon, where <math>C_{2n-k}</math> act via ''K''-promotion.<ref>{{cite journal|last1=Pechenik|first1=Oliver|title=Cyclic sieving of increasing tableaux and small Schröder paths|journal=Journal of Combinatorial Theory, Series A|date=July 2014|volume=125|pages=357–378|doi=10.1016/j.jcta.2014.04.002|arxiv=1209.1355}}</ref>\n\n----\n\nLet <math>S_{\\lambda,j}</math> be the set of permutations of cycle type ''λ'' and exactly ''j'' excedances.\nLet <math>a_{\\lambda,j}(q) = \\sum_{\\sigma \\in S_{\\lambda,j} }q^{\\operatorname{maj}(\\sigma)-\\operatorname{exc}(\\sigma)}</math>,\nand let <math>C_n</math> act on <math>S_{\\lambda,j}</math> by conjugation.\n\nThen <math>(S_{\\lambda,j}, C_n, a_{\\lambda,j}(q))</math> exhibit the cyclic sieving phenomenon.\n<ref>{{cite journal|last1=Sagan|first1=Bruce|last2=Shareshian|first2=John|last3=Wachs|first3=Michelle L.|title=Eulerian quasisymmetric functions and cyclic sieving|journal=Advances in Applied Mathematics|date=January 2011|volume=46|issue=1-4|pages=536–562|doi=10.1016/j.aam.2010.01.013}}</ref>\n\n== Notes and references ==\n{{reflist}}\n* Sagan, Bruce. ''The cyclic sieving phenomenon: a survey.'' Surveys in combinatorics 2011, 183–233, London Math. Soc. Lecture Note Ser., 392, Cambridge Univ. Press, Cambridge, 2011.\n\n[[Category:Combinatorics]]\n[[Category:Generating functions]]"
    },
    {
      "title": "Examples of generating functions",
      "url": "https://en.wikipedia.org/wiki/Examples_of_generating_functions",
      "text": "The following '''examples of [[generating function]]s''' are in the spirit of [[George Pólya]], who advocated learning mathematics by doing and re-capitulating as many examples and proofs as possible.{{citation needed|date=October 2015}} The purpose of this article is to present common ways of creating generating functions.\n\n== Worked example A: basics ==\nNew [[generating function]]s can be created by extending simpler generating functions. For [[Worked-example effect|example]], starting with\n\n:<math>G(1;x)=\\sum_{n=0}^{\\infty} x^n = \\frac{1}{1-x}</math>\n\nand replacing <math>x</math> with <math>ax</math>, we obtain\n\n:<math>G(1;ax)=\\frac{1}{1-ax} = 1+(ax)+(ax)^2+\\cdots+(ax)^n+\\cdots =\\sum_{n=0}^{\\infty} a^n x^n = G(a^n;x).</math>\n\n=== Bivariate generating functions ===\nOne can define generating functions in several variables, for series with several indices. These are often called '''super generating functions,''' and for 2 variables are often called '''bivariate generating functions.'''\n\nFor instance, since <math>(1+x)^n</math> is the generating function for [[binomial coefficients]] for a fixed ''n,'' one may ask for a bivariate generating function that generates the binomial coefficients <math>\\binom{n}{k}</math> for all ''k'' and ''n.''\nTo do this, consider <math>(1+x)^n</math> as ''itself'' a series (in ''n''), and find the generating function in ''y'' that has these as coefficients. Since the generating function for <math>a^n</math> is just <math>1/(1-ay)</math>, the generating function for the binomial coefficients is:\n:<math>\\frac{1}{1-(1+x)y}=1+(1+x)y+(1+x)^2y^2+\\dots,</math>\nand the coefficient on <math>x^ky^n</math> is the <math>\\binom{n}{k}</math> binomial coefficient.\n\n== Worked example B: Fibonacci numbers ==\n\nConsider the problem of finding a [[Closed-form expression|closed formula]] for the [[Fibonacci number]]s ''F''<sub>''n''</sub> defined by ''F''<sub>0</sub> = 0, ''F''<sub>1</sub> = 1, and ''F''<sub>''n''</sub> = ''F''<sub>''n''&minus;1</sub> + ''F''<sub>''n''&minus;2</sub> for ''n'' ≥ 2. We form the ordinary generating function\n\n:<math>\nf = \\sum_{n \\ge 0} F_n x^n\n</math>\n\nfor this sequence. The generating function for the sequence (''F''<sub>''n''&minus;1</sub>) is ''xf'' and that of (''F''<sub>''n''&minus;2</sub>) is ''x''<sup>2</sup>''f''. From the recurrence relation, we therefore see that the power series ''xf'' + ''x''<sup>2</sup>''f'' agrees with ''f'' except for the first two coefficients:\n\n:<math>\n\\begin{array}{rcrcrcrcrcrcr}\nf    & = & F_0x^0 & + & F_1x^1 & + & F_2x^2 & + & \\cdots & + & F_ix^i & + &\\cdots\\\\\nxf   & = &        &  & F_0x^1  & + & F_1x^2 & + & \\cdots & + &F_{i-1}x^i & + &\\cdots\\\\\nx^2f & = &        &  &         &   & F_0x^2 & + & \\cdots & + &F_{i-2}x^i & +&\\cdots\\\\\n(x+x^2)f & = &    &  & F_0x^1  & + & (F_0+F_1)x^2 & + & \\cdots & + & (F_{i-1}+F_{i-2})x^i & +&\\cdots\\\\\n     & = &        &  &         &   & F_2x^2       & + & \\cdots & + & F_ix^i & +& \\cdots\\\\\n\\end{array}\n</math>\n\nTaking these into account, we find that\n\n:<math>\nf = xf + x^2 f + x .\n</math>\n\n(This is the crucial step; recurrence relations can almost always be translated into equations for the generating functions.) Solving this equation for ''f'', we get\n\n:<math>\nf = \\frac{x} {1 - x - x^2} .\n</math>\n\nThe denominator can be factored using the [[golden ratio]] φ<sub>1</sub> = (1 + {{radic|5}})/2 and φ<sub>2</sub> = (1 &minus; {{radic|5}})/2, and the technique of [[partial fraction decomposition]] yields\n\n:<math>\nf = \\frac{1}{\\sqrt{5}} \\left (\\frac{1}{1-\\varphi_1 x} - \\frac{1} {1- \\varphi_2 x} \\right ) .\n</math>\n\nThese two formal power series are known explicitly because they are [[geometric series]]; comparing coefficients, we find the explicit formula\n\n:<math>\nF_n = \\frac{1} {\\sqrt{5}} (\\varphi_1^n - \\varphi_2^n).\n</math>\n\n==Worked example C: Number of ways to make change==\nThe number of {{em|unordered}} ways ''a''<sub>''n''</sub> to make change for ''n'' cents using coins with values 1, 5, 10, and 25 is given by the generating function\n:<math>G(a_n, x) = \\sum_{n=0}^\\infty a_n x^n = \\frac{1}{(1-x)(1-x^5)(1-x^{10})(1-x^{25})}\\,.</math>\nFor example there are two unordered ways to make change for 6 cents; one way is six 1-cent coins, the other way is one 1-cent coin and one 5-cent coin.  See {{oeis|A001299}}.\n\nOn the other hand, the number of {{em|ordered}} ways ''b''<sub>''n''</sub> to make change for ''n'' cents using coins with values 1, 5, 10, and 25 is given by the generating function\n:<math>G(b_n, x) = \\sum_{n=0}^\\infty b_n x^n = \\frac{1}{1-x-x^5-x^{10}-x^{25}}\\,.</math>\nFor example there are three ordered ways to make change for 6 cents; one way is six 1-cent coins, a second way is one 1-cent coin and one 5-cent coin, and a third way is one 5-cent coin and one 1-cent coin.  Compare to {{oeis|A114044}}, which differs from this example by also including coins with values 50 and 100.\n\n==External links==\n* [http://www.cut-the-knot.org/ctk/GeneratingFunctions.shtml Generating Functions, Power Indices and Coin Change] at [[cut-the-knot]]\n* [http://www.math.upenn.edu/~wilf/gfologyLinked2.pdf Generatingfunctionology (PDF)]\n\n{{DEFAULTSORT:Examples Of Generating Functions}}\n[[Category:Generating functions]]\n[[Category:Mathematical examples|Generating functions]]"
    },
    {
      "title": "Factorial moment generating function",
      "url": "https://en.wikipedia.org/wiki/Factorial_moment_generating_function",
      "text": "{{Refimprove|date=December 2009}}\nIn [[probability theory]] and [[statistics]], the '''factorial moment generating function''' of the [[probability distribution]] of a [[real number|real-valued]] [[random variable]] ''X'' is defined as \n:<math>M_X(t)=\\operatorname{E}\\bigl[t^{X}\\bigr]</math>\nfor all [[complex number]]s ''t'' for which this [[expected value]] exists. This is the case at least for all ''t'' on the [[unit circle]] <math>|t|=1</math>, see [[characteristic function (probability theory)|characteristic function]]. If&nbsp;''X'' is a discrete random variable taking values only in the set {0,1, ...} of non-negative [[integer]]s, then <math>M_X</math> is also called [[probability-generating function]] of ''X'' and <math>M_X(t)</math> is well-defined at least for all ''t'' on the [[closed set|closed]] [[unit disk]] <math>|t|\\le1</math>.\n\nThe factorial moment generating function generates the [[factorial moment]]s of the [[probability distribution]].\nProvided <math>M_X</math> exists in a [[neighbourhood (mathematics)|neighbourhood]] of ''t''&nbsp;=&nbsp;1, the ''n''th factorial moment is given by <ref>http://homepages.nyu.edu/~bpn207/Teaching/2005/Stat/Generating_Functions.pdf</ref>\n:<math>\\operatorname{E}[(X)_n]=M_X^{(n)}(1)=\\left.\\frac{\\mathrm{d}^n}{\\mathrm{d}t^n}\\right|_{t=1} M_X(t),</math>\nwhere the [[Pochhammer symbol]] (''x'')<sub>''n''</sub> is the [[falling factorial]]\n:<math>(x)_n = x(x-1)(x-2)\\cdots(x-n+1).\\,</math>\n(Many mathematicians, especially in the field of [[special function]]s, use the same notation to represent the [[rising factorial]].)\n\n==Example==\nSuppose ''X'' has a [[Poisson distribution]] with [[expected value]] λ, then its factorial moment generating function is\n:<math>M_X(t)\n=\\sum_{k=0}^\\infty t^k\\underbrace{\\operatorname{P}(X=k)}_{=\\,\\lambda^ke^{-\\lambda}/k!}\n=e^{-\\lambda}\\sum_{k=0}^\\infty \\frac{(t\\lambda)^k}{k!} = e^{\\lambda(t-1)},\\qquad t\\in\\mathbb{C},\n</math>\n(use the [[Exponential_function#Formal_definition|definition of the exponential function]]) and thus we have\n:<math>\\operatorname{E}[(X)_n]=\\lambda^n.</math>\n\n==See also==\n* [[Moment (mathematics)]]\n* [[Moment-generating function]]\n* [[Cumulant-generating function]]\n\n{{Reflist}}\n\n{{DEFAULTSORT:Factorial Moment Generating Function}}\n[[Category:Factorial and binomial topics]]\n[[Category:Moment (mathematics)]]\n[[Category:Generating functions]]"
    },
    {
      "title": "Generating function transformation",
      "url": "https://en.wikipedia.org/wiki/Generating_function_transformation",
      "text": "{{About|transformations of generating functions in mathematics|generating functions (main article)|generating function|generating functions in classical mechanics|Generating function (physics)|generating function transformations in classical mechanics|canonical transformation}}\n\nIn mathematics, a transformation of a [[sequence|sequence's]] [[generating function]] provides a method of converting the generating function for one sequence into a generating function enumerating another. These transformations typically involve integral formulas applied to a sequence generating function (see [[Generating function transformation#Integral transformations|integral transformations]]) or weighted sums over the higher-order derivatives of these functions (see [[Generating function transformation#Derivative transformations|derivative transformations]]).\n\nGiven a sequence, <math>\\{f_n\\}_{n=0}^{\\infty}</math>, the [[Generating function|ordinary generating function]] (OGF) of the sequence, denoted <math>F(z)</math>, and the [[Generating function|exponential generating function]] (EGF) of the sequence, denoted <math>\\widehat{F}(z)</math>, are defined by the [[formal power series]]\n\n:<math>F(z) = \\sum_{n=0}^\\infty f_n z^n = f_0 + f_1 z + f_2 z^2 + \\cdots</math>\n:<math>\\widehat{F}(z) = \\sum_{n=0}^\\infty \\frac{f_n}{n!} z^n = \\frac{f_0}{0!} + \\frac{f_1}{1!} z + \\frac{f_2}{2!} z^2 + \\cdots.</math>\n\nIn this article, we use the convention that the ordinary (exponential) generating function for a sequence <math>\\{f_n\\}</math> is denoted by the uppercase function <math>F(z)</math> / <math>\\widehat{F}(z)</math> for some fixed or formal <math>z</math> when the context of this notation is clear. Additionally, we use the bracket notation for coefficient extraction from the ''Concrete Mathematics'' reference which is given by <math>[z^n]F(z) := f_n</math>.\nThe [[Generating function|main article]] gives examples of generating functions for many sequences. Other examples of generating function variants include [[Dirichlet generating function]]s (DGFs), [[Lambert series]], and [[Newton series]]. In this article we focus on transformations of generating functions in mathematics and keep a running list of useful transformations and transformation formulas.\n\n==Extracting arithmetic progressions of a sequence==\n\nThe focus of this section is to give formulas for generating functions enumerating the sequence <math>\\{f_{an+b}\\}</math> given an ordinary generating function <math>F(z)</math> where <math>a, b \\in \\mathbb{N}</math>, <math>a \\geq 2</math>, and <math> 0 \\leq b < a</math>. In the first two cases where <math>(a, b) := (2, 0), (2, 1)</math>, we can expand these arithmetic progression generating functions directly in terms of <math>F(z)</math>:\n\n:<math> \\sum_{n \\geq 0} f_{2n} z^{2n} = \\frac{1}{2}\\left(F(z) + F(-z)\\right)</math> \n:<math> \\sum_{n \\geq 0} f_{2n+1} z^{2n+1} = \\frac{1}{2}\\left(F(z) - F(-z)\\right).</math>\n\nMore generally, suppose that <math>a \\geq 3</math> and that <math>\\omega_a \\equiv \\exp\\left(\\frac{2\\pi\\imath}{a}\\right)</math> denotes the <math>a^{th}</math> [[root of unity|primitive root of unity]]. Then we have the formula<ref name=\"TAOCPV1\">See Section 1.2.9 in Knuth's ''The Art of Computer Programming'' (Vol. 1).</ref>\n\n:<math>\\sum_{n \\geq 0} f_{an+b} z^{an+b} = \\frac{1}{a} \\times \\sum_{m=0}^{a-1} \\omega_a^{-mb} F\\left(\\omega_a^{m}z\\right).</math>\n\nFor integers <math>m \\geq 1</math>, another useful formula providing somewhat ''reversed'' floored arithmetic progressions are generated by the identity<ref name=\"GKP1\">Solution to exercise 7.36 on page 569 in Graham, Knuth and Patshnik.</ref>\n\n:<math>\\sum_{n \\geq 0} f_{\\lfloor \\frac{n}{m} \\rfloor} z^n = \\frac{1-z^m}{1-z} F(z^m) = \\left(1+z+\\cdots+z^{m-2} + z^{m-1}\\right) F(z^m).</math>\n\n==Powers of an OGF and composition with functions==\n\nThe [[Bell polynomials|exponential Bell polynomials]], <math>B_{n,k}(x_1, \\ldots, x_n) := n! \\cdot [t^n u^k] \\Phi(t, u)</math>, are defined by the exponential generating function<ref name=\"ADVCOMB1\">See section 3.3 in Comtet.</ref>\n\n:<math>\\Phi(t, u) = \\exp\\left(u \\times \\sum_{m \\geq 1} x_m \\frac{t^m}{m!}\\right) = \n        1 + \\sum_{n \\geq 1} \\left\\{\\sum_{k=1}^{n} B_{n,k}(x_1, x_2, \\ldots) u^k\\right\\} \\frac{t^n}{n!}. </math>\n\nThe next formulas for powers, logarithms, and compositions of formal power series are expanded by these polynomials with variables in the coefficients of the original generating functions.<ref name=\"ADVCOMB2\">See sections 3.3–3.4 in Comtet.</ref><ref name=\"NISTHB1\">See section 1.9(vi) in the ''NIST Handbook.</ref> The formula for the exponential of a generating function is given implicitly through the [[Bell polynomials]] by the EGF for these polynomials defined in the previous formula for some sequence of <math>\\{x_i\\}</math>.\n\n===Reciprocals of an OGF (special case of the powers formula)===\n\nThe power series for the reciprocal of a generating function, <math>F(z)</math>, is expanded by\n\n:<math>\\frac{1}{F(z)} = \\frac{1}{f_0} - \\frac{f_1}{f_0^2} z + \\frac{\\left(f_1^2-f_0 f_2\\right)}{f_0^3} z^2 - \n       \\frac{f_1^3-2f_0f_1f_2+f_0^2f_3}{f_0^4} + \\cdots.</math>\n\nIf we let <math>b_n := [z^n] 1 / F(z)</math> denote the coefficients in the expansion of the reciprocal generating function, then we have the following recurrence relation:\n\n:<math>b_n = - \\frac{1}{f_0}\\left(f_1 b_{n-1} + f_2 b_{n-2} + \\cdots + f_n b_0\\right), n \\geq 1.</math>\n\n===Powers of an OGF===\n\nLet <math>m \\in \\mathbb{C}</math> be fixed, suppose that <math>f_0 = 1</math>, and denote <math>b_n^{(m)} := [z^n] F(z)^m</math>. Then we have a series expansion for <math>F(z)^m</math> given by\n\n:<math>F(z)^m = 1 + m f_1 z + m\\left((m-1)f_1^2+2f_2\\right) \\frac{z^2}{2} + \n       \\left(m(m-1)(m-2)f_1^3 +6m(m-1) f_2+6mf_3\\right) \\frac{z^3}{6} + \\cdots,</math>\n\nand the coefficients <math>b_n^{(m)}</math> satisfy a recurrence relation of the form\n\n:<math> n \\cdot b_n^{(m)} = (m-n+1) f_1 b_{n-1}^{(m)} + (2m-n+2) f_2 b_{n-2}^{(m)} + \\cdots + ((n-1)m-1) f_{n-1} b_1^{(m)} + n m f_n, n \\geq 1.</math>\n\nAnother formula for the coefficients, <math>b_n^{(m)}</math>, is expanded by the [[Bell polynomials]] as\n\n:<math>F(z)^m = f_0^m + \\sum_{n \\geq 1} \\left(\\sum_{1 \\leq k \\leq n} (m)_k f_0^{m-k} B_{n,k}(f_1 \\cdot 1!, f_2 \\cdot 2!, \\ldots)\\right) \\frac{z^n}{n!},</math>\n\nwhere <math>(r)_n</math> denotes the [[Pochhammer symbol]].\n\n===Logarithms of an OGF===\n\nIf we let <math>f_0 = 1</math> and define <math>q_n := [z^n] \\log F(z)</math>, then we have a power series expansion for the composite generating function given by\n\n:<math>\\log F(z) = f_1 + \\left(2f_2-f_1^2\\right) \\frac{z}{2} + \\left(3f_3-3f_1f_2+f_1^3\\right) \\frac{z^2}{3} + \\cdots, </math>\n\nwhere the coefficients, <math>q_n</math>, in the previous expansion satisfy the recurrence relation given by\n\n:<math>n \\cdot q_n = n f_n - (n-1)f_1 q_{n-1} - (n-2)f_2 q_{n-2} - \\cdots - f_{n-1} q_1, </math>\n\nand a corresponding formula expanded by the Bell polynomials in the form of the power series coefficients of the following generating function:\n\n:<math>\\log F(z) = \\sum_{n \\geq 1} \\left(\\sum_{1 \\leq k \\leq n} (-1)^{k-1} (k-1)! B_{n,k}(f_1 \\cdot 1!, f_2 \\cdot 2!, \\ldots)\\right) \\frac{z^n}{n!}. </math>\n\n===Faà di Bruno's formula===\n\nLet <math>\\widehat{F}(z)</math> denote the EGF of the sequence, <math>\\{f_n\\}_{n \\geq 0}</math>, and suppose that <math>\\widehat{G}(z)</math> is the EGF of the sequence, <math>\\{g_n\\}_{n \\geq 0}</math>. The sequence, <math>\\{h_n\\}_{n \\geq 0}</math>, generated by the exponential generating function for the composition, <math>\\widehat{H}(z) := \\widehat{F}(\\widehat{G}(z))</math>, is given in terms of the exponential Bell polynomials as follows:\n\n:<math>h_n = \\sum_{1 \\leq k \\leq n} f_k \\cdot B_{n,k}(g_1, g_2, \\cdots, g_{n-k+1}) + f_0 \\cdot \\delta_{n,0}. </math>\n\nWe compare the statement of this result to the other known statement of [[Faà di Bruno's formula]] which provides an analogous expansion of the <math>n^{th}</math> derivatives of a composite function in terms of the derivatives of the two functions of <math>z</math> defined as above.\n\n==Integral transformations==\n\n===OGF <math>\\longleftrightarrow</math> EGF conversion formulas===\n\nWe have the following integral formulas for <math>a, b \\in \\mathbb{Z}^{+}</math> which can be applied termwise with respect to <math>z</math> when <math>z</math> is taken to be any formal power series variable:<ref name=\"GKP2\">See page 566 of Graham, Knuth and Patashnik for the statement of the last conversion formula.</ref>\n\n:<math>\\sum_{n \\geq 0} f_n z^n = \\int_0^{\\infty} \\widehat{F}(tz) e^{-t} dt</math>\n:<math>\\sum_{n \\geq 0} \\Gamma(an+b) \\cdot f_n z^n = \\int_0^{\\infty} t^{b-1} e^{-t} F(t^a z) dt.</math>\n:<math>\\sum_{n \\geq 0} \\frac{f_n}{n!} z^n = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} F\\left(z e^{-\\imath\\vartheta}\\right) e^{e^{\\imath\\vartheta}} d\\vartheta. </math>\n\nNotice that the first and last of these integral formulas are used to convert between the EGF to the OGF of a sequence, and from the OGF to the EGF of a sequence whenever these integrals are convergent.\n\nThe first integral formula corresponds to the [[Laplace transform]] (or sometimes the formal ''Laplace–Borel'' transformation) of generating functions, denoted by <math>\\mathcal{L}[F](z)</math>, defined in.<ref name=\"ACOMB1\">See Appendix B.13 of Flajolet and Sedgewick.</ref> Other integral representations for the [[gamma function]] in the second of the previous formulas can of course also be used to construct similar integral transformations. One particular formula results in the case of the double factorial function example given immediately below in this section. The last integral formula is compared to [[Hankel contour|Hankel's loop integral]] for the [[reciprocal gamma function]] applied termwise to the power series for <math>F(z)</math>.\n\n====Example: A double factorial integral for the EGF of the Stirling numbers of the second kind====\n\nThe [[factorial|single factorial function]], <math>(2n)!</math>, is expressed as a product of two [[double factorial]] functions of the form\n\n:<math>(2n)! = (2n)!! \\times (2n-1)!! = \\frac{4^n \\cdot n!}{\\sqrt{\\pi}} \\times \\Gamma\\left(n+\\frac{1}{2}\\right), </math>\n\nwhere an integral for the double factorial function, or rational [[gamma function]], is given by\n\n:<math>\\frac{1}{2} \\cdot (2n-1)!! = \\frac{2^n}{\\sqrt{4\\pi}} \\Gamma\\left(n+\\frac{1}{2}\\right) = \n       \\frac{1}{\\sqrt{2\\pi}} \\times \\int_0^{\\infty} e^{-t^2 / 2} t^{2n} \\, dt, </math>\n\nfor natural numbers <math>n \\geq 0</math>. This integral representation of <math>(2n-1)!!</math> then implies that for fixed non-zero <math>q \\in \\mathbb{C}</math> and any integral powers <math>k \\geq 0</math>, we have the formula\n\n:<math>\\frac{\\log(q)^k}{k!} = \\frac{1}{(2k)!} \\times \\left[\\int_0^{\\infty} \\frac{2 e^{-t^2/2}}{\\sqrt{2\\pi}} \n       \\left(\\sqrt{2\\log(q)} \\cdot t\\right)^{2k} \\, dt\\right]. </math>\n\nThus for any prescribed integer <math>j \\geq 0</math>, we can use the previous integral representation together with the formula for extracting arithmetic progressions from a sequence OGF given above, to formulate the next integral representation for the so-termed ''modified'' [[Stirling number of the second kind|Stirling number]] EGF as\n\n:<math>\\sum_{n \\geq 0} \\left\\{\\begin{matrix} 2n \\\\ j \\end{matrix} \\right\\} \\frac{\\log(q)^n}{n!} = \n       \\int_0^{\\infty} \\frac{e^{-t^2/2}}{\\sqrt{2\\pi} \\cdot j!}\\left[\\sum_{b = \\pm 1} \n       \\left(e^{b\\sqrt{2 \\log(q)} \\cdot t}-1\\right)^j\\right] dt, </math>\n\nwhich is convergent provided suitable conditions on the parameter <math>0 < |q| < 1</math>.<ref name=\"SQSERIESMDS1\">Refer to the proof of Theorem 2.3 in ''Math.NT/1609.02803''.</ref>\n\n====Example: An EGF formula for the higher-order derivatives of the geometric series====\n\nFor fixed non-zero <math>c, z \\in \\mathbb{C}</math> defined such that <math>|cz| < 1</math>, let the [[geometric series]] over the non-negative integral powers of <math>(cz)^n</math> be denoted by <math>G(z) := 1 / (1-cz)</math>. The corresponding higher-order <math>j^{th}</math> derivatives of the geometric series with respect to <math>z</math> are denoted by the sequence of functions\n\n:<math>G_j(z) := \\frac{(cz)^j}{1-cz} \\times \\left(\\frac{d}{dz}\\right)^{(j)}\\left[G(z)\\right], </math>\n\nfor non-negative integers <math>j \\geq 0</math>. These <math>j^{th}</math> derivatives of the ordinary geometric series can be shown, for example by induction, to satisfy an explicit closed-form formula given by\n\n:<math>G_j(z) = \\frac{(cz)^j \\cdot j!}{(1-cz)^{j+2}}, </math>\n\nfor any <math>j \\geq 0</math> whenever <math>|cz| < 1</math>. As an example of the third OGF <math>\\longmapsto</math> EGF conversion formula cited above, we can compute the following corresponding ''exponential'' forms of the generating functions <math>G_j(z)</math>:\n\n:<math>\\widehat{G}_j(z) = \\frac{1}{2\\pi} \\int_{-\\pi}^{+\\pi} G_j\\left(z e^{-\\imath t}\\right) e^{e^{\\imath t}} dt = \n       \\frac{(cz)^j e^{cz}}{(j+1)}\\left(j+1+z\\right). </math>\n\n===Fractional integrals and derivatives===\n\nFractional integrals and fractional derivatives (see the [[fractional calculus|main article]]) form another generalized class of integration and differentiation operations that can be applied to the OGF of a sequence to form the corresponding OGF of a transformed sequence. For <math>\\Re(\\alpha) > 0</math> we define the ''fractional integral operator'' (of order <math>\\alpha</math>) by the integral transformation<ref name=\"NISTHB2\">See section 1.15(vi)–(vii) in the ''NIST Handbook''.</ref>\n\n:<math>I^{\\alpha} F(z) = \\frac{1}{\\Gamma(\\alpha)} \\int_0^{z} (z-t)^{\\alpha-1} F(t) dt, </math>\n\nwhich corresponds to the (formal) power series given by\n\n:<math>I^{\\alpha} F(z) = \\sum_{n \\geq 0} \\frac{n!}{\\Gamma(n+\\alpha+1)} f_n z^{n+\\alpha}. </math>\n\nFor fixed <math>\\alpha, \\beta \\in \\mathbb{C}</math> defined such that <math>\\Re(\\alpha), \\Re(\\beta) > 0</math>, we have that the operators <math>I^{\\alpha} I^{\\beta} = I^{\\alpha+\\beta}</math>. \nMoreover, for fixed <math>\\alpha \\in \\mathbb{C}</math> and integers <math>n</math> satisfying <math>0 < \\Re(\\alpha) < n</math> we can define the notion of the ''fractional derivative'' satisfying the properties that\n\n:<math>D^{\\alpha} F(z) = \\frac{d^{(n)}}{dz^{(n)}} I^{n-\\alpha}F(z), </math>\n\nand\n\n:<math>D^{k} I^{\\alpha} = D^{n} I^{\\alpha+n-k}</math> for <math>k = 1, 2, \\ldots, n, </math>\n\nwhere we have the semigroup property that <math>D^{\\alpha} D^{\\beta} = D^{\\alpha+\\beta}</math> only when none of <math>\\alpha, \\beta, \\alpha+\\beta</math> is integer-valued.\n\n===Polylogarithm series transformations===\n\nFor fixed <math>s \\in \\mathbb{Z}^{+}</math>, we have that (compare to the special case of the integral formula for the ''Nielsen generalized polylogarithm function'' defined in<ref>{{cite web|last1=Weisstein|first1=Eric W.|title=Nielsen Generalized Polylogarithm|url=http://mathworld.wolfram.com/NielsenGeneralizedPolylogarithm.html|website=MathWorld}}</ref>) <ref>See equation (4) in section 2 of Borwein, Borwein and Girgensohn's article ''Explicit evaluation of Euler sums'' (1994).</ref>\n\n:<math>\\sum_{n \\geq 0} \\frac{f_n}{(n+1)^s} z^n = \\frac{(-1)^{s-1}}{(s-1)!} \\int_0^1 \\log^{s-1}(t) F(tz) dt.</math>\n\nNotice that if we set <math>g_n \\equiv f_{n+1}</math>, the integral with respect to the generating function, <math>G(z)</math>, in the last equation when <math>z \\equiv 1</math> corresponds to the [[Dirichlet generating function]], or DGF, <math>\\widetilde{F}(s)</math>, of the sequence of <math>\\{f_n\\}</math> provided that the integral converges. This class of [[polylogarithm|polylogarithm-related]] integral transformations is related to the derivative-based zeta series transformations defined in the next sections.\n\n===Square series generating function transformations===\n\nFor fixed non-zero <math>q, c, z \\in \\mathbb{C}</math> such that <math>|q| < 1</math> and <math>|cz| < 1</math>, we have the following integral representations for the so-termed ''square series'' generating function associated with the sequence <math>\\{f_n\\}</math>, which can be integrated termwise with respect to <math>z</math>:<ref name=\"SQSERIESMDS2\">See the article ''Math.NT/1609.02803''.</ref>\n\n:<math>\\sum_{n \\geq 0} q^{n^2} f_n \\cdot (cz)^n = \\frac{1}{\\sqrt{2\\pi}} \\int_0^{\\infty} e^{-t^2/2}\\left[F\\left(e^{t\\sqrt{2\\log(q)}} \\cdot cz\\right) + F\\left(e^{-t \\sqrt{2\\log(q)}} \\cdot cz\\right)\\right] dt. </math>\n\nThis result, which is proved in the reference, follows from a variant of the double factorial function transformation integral for the Stirling numbers of the second kind given as an example above. In particular, since\n\n:<math>q^{n^2} = \\exp(n^2 \\cdot \\log(q)) = 1 + n^2 \\log(q) + n^4 \\frac{\\log(q)^2}{2!} + n^6 \\frac{\\log(q)^3}{3!} + \\cdots,</math>\n\nwe can use a variant of the positive-order derivative-based OGF transformations defined in the next sections involving the [[Stirling numbers of the second kind]] to obtain an integral formula for the generating function of the sequence, <math>\\left\\{S(2n, j) / n!\\right\\}</math>, and then perform a sum over the <math>j^{th}</math> derivatives of the formal OGF, <math>F(z)</math> to obtain the result in the previous equation where the arithmetic progression generating function at hand is denoted by\n\n:<math>\\sum_{n \\geq 0} \\left\\{\\begin{matrix} 2n \\\\ j \\end{matrix} \\right\\} \\frac{z^{2n}}{(2n)!} = \\frac{1}{2 j!}\\left((e^z-1)^j + (e^{-z}-1)^j\\right), </math>\n\nfor each fixed <math>j \\in \\mathbb{N}</math>.\n\n==Hadamard products and diagonal generating functions==\n\nWe have an integral representation for the Hadamard product of two generating functions, <math>F(z)</math> and <math>G(z)</math>, stated in the following form:\n\n:<math>(F \\odot G)(z) := \\sum_{n \\geq 0} f_n g_n z^n = \n       \\frac{1}{2\\pi} \\int_0^{2\\pi} F\\left(\\sqrt{z} e^{\\imath t}\\right) G\\left(\\sqrt{z} e^{-\\imath t}\\right) dt.</math>\n\nMore information about Hadamard products as ''diagonal generating functions'' of multivariate sequences and/or generating functions and the classes of generating functions these diagonal OGFs belong to is found in Stanley's book.<ref name=\"ECV2\">See section 6.3 in Stanley's book.</ref> The reference also provides nested coefficient extraction formulas of the form\n\n:<math>\\operatorname{diag}\\left(F_1 \\cdots F_k\\right) := \\sum_{n \\geq 0} f_{1,n} \\cdots f_{k,n} z^n = \n       [x_{k-1}^0 \\cdots x_2^0 x_1^0] F_k\\left(\\frac{z}{x_{k-1}}\\right) \n        F_{k-1}\\left(\\frac{x_{k-1}}{x_{k-2}}\\right) \\cdots F_2\\left(\\frac{x_2}{x_1}\\right) F_1(x_1), </math>\n\nwhich are particularly useful in the cases where the component sequence generating functions, <math>F_i(z)</math>, can be expanded in a [[Laurent series]], or fractional series, in <math>z</math>, such as in the special case where all of the component generating functions are rational, which leads to an ''algebraic'' form of the corresponding diagonal generating function.\n\n===Example: Hadamard products of rational generating functions===\n\nIn general, the Hadamard product of two ''[[Constant-recursive sequence|rational generating functions]]'' is itself rational.<ref name=\"GFLECT\">See section 2.4 in Lando's book.</ref> This is seen by noticing that the coefficients of a [[Generating function#Rational functions|rational generating function]] form ''quasi-polynomial'' terms of the form\n\n:<math>f_n = p_1(n) \\rho_1^n + \\cdots + p_{\\ell}(n) \\rho_{\\ell}^n, </math>\n\nwhere the reciprocal roots, <math>\\rho_i \\in \\mathbb{C}</math>, are fixed scalars and where <math>p_i(n)</math> is a polynomial in <math>n</math> for all <math>1 \\leq i \\leq \\ell</math>. \nFor example, the Hadamard product of the two generating functions\n\n:<math>F(z) := \\frac{1}{1+a_1 z+a_2 z^2}</math>\n\nand\n\n:<math>G(z) := \\frac{1}{1+b_1 z+b_2 z^2}</math>\n\nis given by the rational generating function formula<ref>{{cite article|first1=E. A. |last1=Potekhina| title=Application of Hadamard product to some combinatorial and probabilistic problems|journal=Discr. Math. Appl.|year=2017|volume=27|number=3|pages=177-186|doi=10.1515/dma-2017-0020}}</ref>\n\n:<math>(F \\odot G)(z) = \\frac{1-a_2b_2 z^2}{1-a_1b_1 z + \\left(a_2b_1^2+a_1^2b_2-a_2b_2\\right) z^2 -a_1a_2b_1b_2 z^3 + a_2^2b_2^2 z^4}. </math>\n\n===Example: Factorial (approximate Laplace) transformations===\n\nOrdinary generating functions for generalized factorial functions formed as special cases of the ''generalized rising factorial product functions'', or [[Pochhammer k-symbol]], defined by\n\n:<math>p_n(\\alpha, R) := R(R+\\alpha) \\cdots (R+(n-1)\\alpha) = \\alpha^n \\cdot \\left(\\frac{R}{\\alpha}\\right)_n, </math>\n\nwhere <math>R</math> is fixed, <math>\\alpha \\neq 0</math>, and <math>(x)_n</math> denotes the [[Pochhammer symbol]] are generated (at least formally) by the [[Generating function#Representation by continued fractions .28Jacobi-type J-fractions.29|Jacobi-type J-fractions]] (or special forms of [[continued fraction]]s) established in the reference.<ref name=\"MULTIFACT-CFRACS\">{{cite journal|first1=M. D. |last1=Schmidt|title=Jacobi type continued fractions for ordinary generating functions of generalized factorial functions|year=2017|journal = J. Int. Seq. | volume=20|pages=17.3.4|url=https://cs.uwaterloo.ca/journals/JIS/VOL20/Schmidt/schmidt14.html|arxiv=1610.09691}}</ref> If we let <math>\\operatorname{Conv}_h(\\alpha, R; z) := \\operatorname{FP}_h(\\alpha, R; z) / \\operatorname{FQ}_h(\\alpha, R; z)</math> denote the <math>h^\\text{th}</math> convergent to these infinite continued fractions where the component convergent functions are defined for all integers <math>h \\geq 2</math> by\n\n:<math>\\operatorname{FP}_h(\\alpha, R; z) = \\sum_{n=0}^{h-1}\\left[\\sum_{k=0}^{n} \\binom{h}{k} \\left(1-h-\\frac{R}{\\alpha}\\right)_k \\left(\\frac{R}{\\alpha}\\right)_{n-k}\\right] (\\alpha z)^n, </math>\n\nand\n\n:<math>\n   \\begin{align} \n     \\operatorname{FQ}_h(\\alpha, R; z) & = (-\\alpha z)^h \\cdot h! \\times L_h^{\\left(R / \\alpha-1\\right)}\\left((\\alpha z)^{-1}\\right) \\\\ \n     & = \\sum_{k=0}^{h} \\binom{h}{k} \\left[\\prod_{j=0}^{k-1} (R+(j-1-j)\\alpha)\\right] (-z)^k, \n   \\end{align}\n</math>\n\nwhere <math>L_n^{(\\beta)}(x)</math> denotes an [[Laguerre polynomials|associated Laguerre polynomial]], then we have that the <math>h^{th}</math> convergent function, <math>\\operatorname{Conv}_h(\\alpha, R; z)</math>, exactly enumerates the product sequences, <math>p_n(\\alpha, R)</math>, for all <math>0 \\leq n < 2h</math>. For each <math>h \\geq 2</math>, the <math>h^{th}</math> convergent function is expanded as a finite sum involving only paired reciprocals of the Laguerre polynomials in the form of\n\n:<math>\\operatorname{Conv}_h(\\alpha, R; z) = \\sum_{i=0}^{h-1} \\binom{\\frac{R}{\\alpha}+i-1}{i} \\times \n       \\frac{(-\\alpha z)^{-1}}{(i+1) \\cdot L_i^{\\left(R/\\alpha-1\\right)}\\left((\\alpha z)^{-1}\\right) \n       L_{i+1}^{\\left(R/\\alpha-1\\right)}\\left((\\alpha z)^{-1}\\right)} </math>\n\nMoreover, since the [[factorial|single factorial function]] is given by both <math>n! = p_n(1, 1)</math> and <math>n! = p_n(-1, n)</math>, we can generate the single factorial function terms using the approximate ''rational'' convergent generating functions up to order <math>2h</math>. This observation suggests an approach to approximating the exact (formal) Laplace–Borel transform usually given in terms of the integral representation from the previous section by a Hadamard product, or diagonal-coefficient, generating function. In particular, given any OGF <math>G(z)</math> we can form the approximate Laplace transform, which is <math>2h</math>-order accurate, by the diagonal coefficient extraction formula stated above given by\n\n:<math>\n     \\begin{align} \n     \\widetilde{\\mathcal{L}}_h[G](z) & := [x^0] \\operatorname{Conv}_h\\left(1, 1; \\frac{z}{x}\\right) G(x) \\\\ \n                                     &\\ = \\frac{1}{2\\pi} \\int_0^{2\\pi} \n       \\operatorname{Conv}_h\\left(1, 1; \\sqrt{z} e^{\\imath t}\\right) G\\left(-\\sqrt{z} e^{\\imath t}\\right) dt. \n     \\end{align}\n</math>\n\nExamples of sequences enumerated through these diagonal coefficient generating functions arising from the sequence factorial function multiplier provided by the rational convergent functions include\n\n:<math>\n   \\begin{align} \n     n!^2 & = [z^n][x^0] \\operatorname{Conv}_h\\left(-1, n; \\frac{z}{x}\\right) \\operatorname{Conv}_h\\left(-1, n; x\\right), h \\geq n \\\\ \n     \\binom{2n}{n} & = [x_1^0 x_2^0 z^n] \\operatorname{Conv}_h\\left(-2, 2n; \\frac{z}{x_2}\\right) \n                       \\operatorname{Conv}_h\\left(-2, 2n-1; \\frac{x_2}{x_1}\\right) \n                       I_0(2\\sqrt{x_1}) \\\\ \n     \\binom{3n}{n} \\binom{2n}{n} & = [x_1^0 x_2^0 z^n] \\operatorname{Conv}_h\\left(-3, 3n-1; \\frac{3z}{x_2}\\right) \n                                     \\operatorname{Conv}_h\\left(-3, 3n-2; \\frac{x_2}{x_1}\\right) \n                                     I_0(2\\sqrt{x_1}) \\\\ \n     !n & = n! \\times \\sum_{i=0}^{n} \\frac{(-1)^i}{i!} = \n            [z^n x^0] \\left(\\frac{e^{-x}}{(1-x)} \\operatorname{Conv}_n\\left(-1, n; \\frac{z}{x}\\right)\\right) \\\\ \n     \\operatorname{af}(n) & = \\sum_{k=1}^{n} (-1)^{n-k} k! = [z^n]\\left(\\frac{\\operatorname{Conv}_n(1, 1; z)-1}{1+z}\\right) \\\\ \n     (t-1)^n P_n\\left(\\frac{t+1}{t-1}\\right) & = \\sum_{k=0}^{n} \\binom{n}{k}^2 t^k \\\\ \n          & = \n          [x_1^0 x_2^0] [z^n] \\left(\\operatorname{Conv}_n\\left(1, 1; \\frac{z}{x_1}\\right) \n          \\operatorname{Conv}_n\\left(1, 1; \\frac{x_1}{x_2}\\right) \n          I_0(2\\sqrt{t \\cdot x_2}) I_0(2\\sqrt{x_2})\\right), n \\geq 1 \\\\ \n     (2n-1)!! & = \\sum_{k=1}^{n} \\frac{(n-1)!}{(k-1)!} k \\cdot (2k-3)!! \\\\ \n              & = [x_1^0 x_2^0 x_3^{n-1}]\\left(\\operatorname{Conv}_n\\left(1, 1; \\frac{x_3}{x_2}\\right) \n                  \\operatorname{Conv}_n\\left(2, 1; \\frac{x_2}{x_1}\\right) \n                  \\frac{(x_1+1) e^{x_1}}{(1-x_2)}\\right), \n\\end{align} \n</math>\n\nwhere <math>I_0(z)</math> denotes a [[Bessel function|modified Bessel function]], <math>!n</math> denotes the [[derangements|subfactorial function]], <math>\\operatorname{af}(n)</math> denotes the [[alternating factorial]] function, and <math>P_n(x)</math> is a [[Legendre polynomials|Legendre polynomial]]. Other examples of sequences enumerated through applications of these rational Hadamard product generating functions given in the article include the [[Barnes G-function]], combinatorial sums involving the [[double factorial]] function, [[Faulhaber's formula|sums of powers]] sequences, and sequences of binomials.\n\n==Derivative transformations==\n\n===Positive and negative-order zeta series transformations===\n\nFor fixed <math>k \\in \\mathbb{Z}^{+}</math>, we have that if the sequence OGF <math>F(z)</math> has <math>j^{th}</math> derivatives of all required orders for <math>1 \\leq j \\leq k</math>, that the ''positive-order zeta series transformation'' is given by<ref name=\"SQSERIESMDS3\">See the inductive proof given in section 2 of ''Math.NT/1609.02803''.</ref>\n\n:<math>\\sum_{n \\geq 0} n^k f_n z^n = \\sum_{j=0}^{k} \\left\\{\\begin{matrix} k \\\\ j \\end{matrix} \\right\\} z^j F^{(j)}(z), </math>\n\nwhere <math>\\scriptstyle{\\left\\{\\begin{matrix} n \\\\ k \\end{matrix} \\right\\}}</math> denotes a [[Stirling number of the second kind]]. \nIn particular, we have the following special case identity when <math>f_n \\equiv 1 \\forall n</math> when <math>\\scriptstyle{\\left\\langle\\begin{matrix} n \\\\ m \\end{matrix} \\right\\rangle}</math> denotes the triangle of [[Eulerian number|first-order Eulerian numbers]]:<ref name=\"GKP3\">See the table in section 7.4 of Graham, Knuth and Patashnik.</ref>\n\n:<math>\\sum_{n \\geq 0} n^k z^n = \\sum_{j=0}^{k} \\left\\{\\begin{matrix} k \\\\ j \\end{matrix} \\right\\} \\frac{z^j \\cdot j!}{(1-z)^{j+1}} = \n       \\frac{1}{(1-z)^{k+1}} \\times \\sum_{0 \\leq m < k} \\left\\langle\\begin{matrix} k \\\\ m \\end{matrix} \\right\\rangle z^{m+1}. </math>\n\nWe can also expand the ''negative-order zeta series transformations'' by a similar procedure to the above expansions given in terms of the <math>j^{th}</math>-order derivatives of some <math>F(z) \\in C^{\\infty}</math> and an infinite, non-triangular set of generalized Stirling numbers ''in reverse'', or generalized Stirling numbers of the second kind defined within this context.\n\nIn particular, for integers <math>k, j \\geq 0</math>, define these generalized classes of Stirling numbers of the second kind by the formula\n\n:<math>\\left\\{\\begin{matrix} k+2 \\\\ j \\end{matrix} \\right\\}_{\\ast} := \\frac{1}{j!} \\times \\sum_{m=1}^{j} \\binom{j}{m} \\frac{(-1)^{j-m}}{m^{k}}. </math>\n\nThen for <math>k \\in \\mathbb{Z}^{+}</math> and some prescribed OGF, <math>F(z) \\in C^{\\infty}</math>, i.e., so that the higher-order <math>j^{th}</math> derivatives of <math>F(z)</math> exist for all <math>j \\geq 0</math>, we have that\n\n:<math>\\sum_{n \\geq 1} \\frac{f_n}{n^k} z^n = \\sum_{j \\geq 1} \\left\\{\\begin{matrix} k+2 \\\\ j \\end{matrix} \\right\\}_{\\ast} z^j F^{(j)}(z). </math>\n\nA table of the first few zeta series transformation coefficients, <math>\\scriptstyle{\\left\\{\\begin{matrix} k \\\\ j \\end{matrix} \\right\\}_{\\ast}}</math>, appears below. These weighted-harmonic-number expansions are almost identical to the known formulas for the [[Stirling numbers of the first kind]] up to the leading sign on the weighted [[harmonic number]] terms in the expansions.\n\n{| class=\"wikitable\"\n|-\n! k !! <math>\\left\\{\\begin{matrix} k \\\\ j \\end{matrix} \\right\\}_{\\ast} \\times (-1)^{j-1} j!</math>\n|-\n| 2 || <math>1</math> \n|-\n| 3 || <math>H_j</math>\n|-\n| 4 || <math>\\frac{1}{2}\\left(H_j^2+H_j^{(2)}\\right)</math>\n|-\n| 5 || <math>\\frac{1}{6}\\left(H_j^3+3H_j H_j^{(2)} + 2 H_j^{(3)}\\right)</math>\n|-\n| 6 || <math>\\frac{1}{24}\\left(H_j^4+6H_j^2 H_j^{(2)} + 3\\left(H_j^{(2)}\\right)^2+ 8H_j H_j^{(3)}+ 6H_j^{(4)}\\right)</math>\n|}\n\n====Examples of the negative-order zeta series transformations====\n\nThe next series related to the [[polylogarithm|polylogarithm functions]] (the ''[[dilogarithm]]'' and ''[[trilogarithm]]'' functions, respectively), the [[Dirichlet eta function|alternating zeta function]] and the [[Riemann zeta function]] are formulated from the previous negative-order series results found in the references. In particular, when <math>s := 2</math> (or equivalently, when <math>k := 4</math> in the table above), we have the following special case series for the [[dilogarithm]] and corresponding constant value of the alternating zeta function:\n\n:<math> \n\\begin{align} \n\\text{Li}_2(z) & = \\sum_{j \\geq 1} \\frac{(-1)^{j-1}}{2} \\left(H_j^2+H_j^{(2)}\\right) \\frac{z^j}{(1-z)^{j+1}} \\\\ \n\\zeta^{\\ast}(2) & = \\frac{\\pi^2}{12} = \\sum_{j \\geq 1} \\frac{\\left(H_j^2+H_j^{(2)}\\right)}{4 \\cdot 2^j}. \n\\end{align}\n</math>\n\nWhen <math>s := 3</math> (or when <math>k := 5</math> in the notation used in the previous subsection), we similarly obtain special case series for these functions given by\n\n:<math> \n\\begin{align} \n\\text{Li}_3(z) & = \\sum_{j \\geq 1} \\frac{(-1)^{j-1}}{6} \\left(H_j^3+3H_j H_j^{(2)} + 2 H_j^{(3)}\\right) \\frac{z^j}{(1-z)^{j+1}} \\\\ \n\\zeta^{\\ast}(3) & = \\frac{3}{4} \\zeta(3) = \\sum_{j \\geq 1} \\frac{\\left(H_j^3+3H_j H_j^{(2)} + 2 H_j^{(3)}\\right)}{12 \\cdot 2^j} \\\\ \n                & = \\frac{1}{6} \\log(2)^3 + \\sum_{j \\geq 0} \\frac{H_j H_j^{(2)}}{2^{j+1}}. \n\\end{align}\n</math>\n\nIt is known that the [[harmonic number|first-order harmonic numbers]] have a closed-form exponential generating function expanded in terms of the [[natural logarithm]], the [[incomplete gamma function]], and the [[exponential integral]] given by\n\n:<math>\\sum_{n \\geq 0} \\frac{H_n}{n!} z^n = e^z \\left( \\mbox{E}_1(z) + \\gamma + \\log z\\right) = e^z \\left(\\Gamma (0,z) + \\gamma + \\log z\\right).</math>\n\nAdditional series representations for the [[harmonic number|r-order harmonic number]] exponential generating functions for integers <math>r \\geq 2</math> are formed as special cases of these negative-order derivative-based series transformation results. For example, the ''second-order harmonic numbers'' have a corresponding exponential generating function expanded by the series\n\n:<math>\\sum_{n \\geq 0} \\frac{H_n^{(2)}}{n!} z^n =\\sum_{j \\geq 1} \\frac{H_j^2+H_j^{(2)}}{2 \\cdot (j+1)!} z^j e^z \\left(j+1+z\\right).</math>\n\n===Generalized negative-order zeta series transformations===\n\nA further generalization of the negative-order series transformations defined above is related to more [[Hurwitz zeta function|''Hurwitz-zeta-like'']], or [[Lerch zeta function|''Lerch-transcendent-like'']], generating functions. Specifically, if we define the even more general parametrized Stirling numbers of the second kind by\n\n:<math>\\left\\{\\begin{matrix} k+2 \\\\ j \\end{matrix} \\right\\}_{(\\alpha, \\beta)^{\\ast}} := \\frac{1}{j!} \\times \\sum_{0 \\leq m \\leq j} \\binom{j}{m} \\frac{(-1)^{j-m}}{(\\alpha m+\\beta)^{k}}</math>,\n\nfor non-zero <math>\\alpha, \\beta \\in \\mathbb{C}</math> such that <math>-\\frac{\\beta}{\\alpha} \\notin \\mathbb{Z}^{+}</math>, and some fixed <math>k \\geq 1</math>, we have that\n\n:<math>\\sum_{n \\geq 1} \\frac{f_n}{(\\alpha n+\\beta)^{k}} z^n = \\sum_{j \\geq 1} \\left\\{\\begin{matrix} k+2 \\\\ j \\end{matrix} \\right\\}_{(\\alpha, \\beta)^{\\ast}} z^j F^{(j)}(z). </math>\n\nMoreover, for any integers <math>u, u_0 \\geq 0</math>, we have the partial series approximations to the full infinite series in the previous equation given by\n\n:<math>\\sum_{n=1}^{u} \\frac{f_n}{(\\alpha n+\\beta)^{k}} z^n = [w^u]\\left(\\sum_{j=1}^{u+u_0} \\left\\{\\begin{matrix} k+2 \\\\ j \\end{matrix} \\right\\}_{(\\alpha, \\beta)^{\\ast}} \\frac{(wz)^j F^{(j)}(wz)}{1-w}\\right). </math>\n\n====Examples of the generalized negative-order zeta series transformations====\n\nSeries for special constants and [[list of zeta functions|zeta-related functions]] resulting from these generalized derivative-based series transformations typically involve the ''generalized r-order harmonic numbers'' defined by \n<math>H_n^{(r)}(\\alpha, \\beta) := \\sum_{1 \\leq k \\leq n} (\\alpha k + \\beta)^{-r}</math> for integers <math>r \\geq 1</math>. A pair of particular series expansions for the following constants when <math>n \\in \\mathbb{Z}^{+}</math> is fixed follow from special cases of [[Bailey–Borwein–Plouffe formula|BBP-type identities]] as\n\n:<math>\n\\begin{align}\n\\frac{4 \\sqrt{3} \\pi}{9} & = \\sum_{j \\geq 0} \\frac{8}{9^{j+1}}\\left(2 \\binom{j+\\frac{1}{3}}{\\frac{1}{3}}^{-1} + \\frac{1}{2} \\binom{j+\\frac{2}{3}}{\\frac{2}{3}}^{-1}\\right) \\\\ \n\\log\\left(\\frac{n^2-n+1}{n^2}\\right) & = \\sum_{j \\geq 0} \\frac{1}{(n^2+1)^{j+1}}\\left(\\frac{2}{3 \\cdot (j+1)} - n^2 \\binom{j+\\frac{1}{3}}{\\frac{1}{3}}^{-1} + \\frac{n}{2} \\binom{j+\\frac{2}{3}}{\\frac{2}{3}}^{-1}\\right). \n\\end{align} \n</math>\n\nSeveral other series for the ''zeta-function-related'' cases of the [[Legendre chi function]], the [[polygamma function]], and the [[Riemann zeta function]] include\n\n:<math>\n\\begin{align} \n\\chi_1(z) & = \\sum_{j \\geq 0} \\binom{j+\\frac{1}{2}}{\\frac{1}{2}}^{-1} \\frac{z \\cdot (-z^2)^j}{(1-z^2)^{j+1}} \\\\ \n\\chi_2(z) & = \\sum_{j \\geq 0} \\binom{j+\\frac{1}{2}}{\\frac{1}{2}}^{-1} \\left(1 + H_j^{(1)}(2, 1)\\right) \\frac{z \\cdot (-z^2)^j}{(1-z^2)^{j+1}} \\\\ \n\\sum_{k \\geq 0} \\frac{(-1)^k}{(z+k)^2} & = \\sum_{j \\geq 0} \\binom{j+z}{z}^{-1} \\left(\\frac{1}{z^2} + \\frac{1}{z} H_j^{(1)}(2, z)\\right) \\frac{1}{2^{j+1}} \\\\ \n\\frac{13}{18} \\zeta(3) & = \\sum_{i=1,2} \\sum_{j \\geq 0} \\binom{j+\\frac{i}{3}}{\\frac{i}{3}}^{-1} \\left(\\frac{1}{i^3} + \\frac{1}{i^2} H_j^{(1)}(3, i) + \\frac{1}{2i}\\left(H_j^{(1)}(3, i)^2+H_j^{(2)}(3, i)\\right)\\right) \\frac{(-1)^{i+1}}{2^{j+1}}. \n\\end{align}\n</math>\n\nAdditionally, we can give another new explicit series representation of the [[inverse tangent|inverse tangent function]] through its relation to the [[Fibonacci numbers]] <ref name=\"MWINVTANGENTSERIES\">See equation (30) on the [http://mathworld.wolfram.com/InverseTangent.html MathWorld page] for the inverse tangent function.</ref> expanded as in the references by\n\n:<math>\\tan^{-1}(x) = \\frac{\\sqrt{5}}{2\\imath} \\times \\sum_{b = \\pm 1} \\sum_{j \\geq 0} \\frac{b}{\\sqrt{5}} \\binom{j+\\frac{1}{2}}{j}^{-1}\\left[ \n       \\frac{\\left(b\\imath\\varphi t / \\sqrt{5}\\right)^j}{\\left(1-\\frac{b\\imath\\varphi t}{\\sqrt{5}}\\right)^{j+1}} - \n       \\frac{\\left(b\\imath\\Phi t / \\sqrt{5}\\right)^j}{\\left(1+\\frac{b\\imath\\Phi t}{\\sqrt{5}}\\right)^{j+1}}\\right], \n</math>\n\nfor <math>t \\equiv 2x / \\left(1+\\sqrt{1+\\frac{4}{5} x^2}\\right)</math> and where the [[golden ratio]] (and its reciprocal) are respectively defined by <math>\\varphi,\\Phi := \\frac{1}{2}\\left(1 \\pm \\sqrt{5}\\right)</math>.\n\n==Inversion relations and generating function identities==\n\n===Inversion relations===\n\nAn ''inversion relation'' is a pair of equations of the form\n\n:<math>g_n = \\sum_{k=0}^{n} A_{n,k} \\cdot f_k \\quad\\longleftrightarrow\\quad \n       f_n = \\sum_{k=0}^{n} B_{n,k} \\cdot g_k, </math>\n\nwhich is equivalent to the ''orthogonality relation''\n\n:<math>\\sum_{k=j}^{n} A_{n,k} \\cdot B_{k,j} = \\delta_{n,j}. </math>\n\nGiven two sequences, <math>\\{f_n\\}</math> and <math>\\{g_n\\}</math>, related by an inverse relation of the previous form, we sometimes seek to relate the OGFs and EGFs of the pair of sequences by functional equations implied by the inversion relation. This goal in some respects mirrors the more number theoretic ([[Lambert series]]) generating function relation guaranteed by the [[Möbius inversion formula]], which provides that whenever\n\n:<math>a_n = \\sum_{d | n} b_d \\quad\\longleftrightarrow\\quad \n       b_n = \\sum_{d | n} \\mu\\left(\\frac{n}{d}\\right) a_d, </math>\n\nthe generating functions for the sequences, <math>\\{a_n\\}</math> and <math>\\{b_n\\}</math>, are related by the ''Möbius transform'' given by\n\n:<math>\\sum_{n \\geq 1} a_n z^n = \\sum_{n \\geq 1} \\frac{b_n z^n}{1-z^n}. </math>\n\nSimilarly, the ''Euler transform'' of generating functions for two sequences, <math>\\{a_n\\}</math> and <math>\\{b_n\\}</math>, satisfying the relation<ref>{{cite web|last1=Weisstein|first1=E.|title=Euler Transform|url=http://mathworld.wolfram.com/EulerTransform.html|website=MathWorld}}</ref>\n\n:<math>1 + \\sum_{n \\geq 1} b_n z^n = \\prod_{i \\geq 1} \\frac{1}{(1-z^i)^{a_i}}, </math>\n\nis given in the form of\n\n:<math>1 + B(z) = \\exp\\left(\\sum_{k \\geq 1} \\frac{A(z^k)}{k}\\right), </math>\n\nwhere the corresponding inversion formulas between the two sequences is given in the reference.\n\nThe remainder of the results and examples given in this section sketch some of the more well-known generating function transformations provided by sequences related by inversion formulas (the [[binomial transform]] and the [[Stirling transform]]), and provides several tables of known inversion relations of various types cited in Riordan's ''Combinatorial Identities'' book. In many cases, we omit the corresponding functional equations implied by the inversion relationships between two sequences (''this part of the article needs more work'').\n\n{{expand section|Need to add functional equations between generating functions related by the inversion pairs in the next subsections. For example, by exercise 5.71 of ''Concrete Mathematics'', if <math>s_n = \\sum_{k \\geq 0} \\binom{n+k}{m+2k} a_k</math>, then <math>S(z) = \\frac{z^m}{(1-z)^{m+1}} A\\left(\\frac{z}{(1-z)^2}\\right)</math>|date=March 2017}}\n\n===The binomial transform===\n\nThe first inversion relation provided below implicit to the [[binomial transform]] is perhaps the simplest of all inversion relations we will consider in this section. For any two sequences, <math>\\{f_n\\}</math> and <math>\\{g_n\\}</math>, related by the inversion formulas\n\n:<math>g_n = \\sum_{k=0}^{n} \\binom{n}{k} (-1)^k f_k \\quad\\longleftrightarrow\\quad \n       f_n = \\sum_{k=0}^{n} \\binom{n}{k} (-1)^k g_k, </math>\n\nwe have functional equations between the OGFs and EGFs of these sequences provided by the [[binomial transform]] in the forms of\n\n:<math>G(z) = \\frac{1}{1-z} F\\left(\\frac{z}{1-z}\\right) </math>\n\nand\n\n:<math>\\widehat{G}(z) = e^z \\widehat{F}(-z). </math>\n\n===The Stirling transform===\n\nFor any pair of sequences, <math>\\{f_n\\}</math> and <math>\\{g_n\\}</math>, related by the [[Stirling number]] inversion formula\n\n:<math>g_n = \\sum_{k=1}^{n} \\left\\{\\begin{matrix} n \\\\ k \\end{matrix} \\right\\} f_k \\quad\\longleftrightarrow\\quad \n       f_n = \\sum_{k=1}^{n} \\left[\\begin{matrix} n \\\\ k \\end{matrix} \\right] (-1)^{n-k} g_k, </math>\n\nthese inversion relations between the two sequences translate into functional equations between the sequence EGFs given by the [[Stirling transform]] as\n\n:<math>\\widehat{G}(z) = \\widehat{F}\\left(e^z-1\\right)</math>\n\nand\n\n:<math>\\widehat{F}(z) = \\widehat{G}\\left(\\log(1+z)\\right). </math>\n\n===Tables of inversion pairs from Riordan's book===\n\nThese tables appear in chapters 2 and 3 in Riordan's book providing an introduction to inverse relations with many examples, though which does not stress functional equations between the generating functions of sequences related by these inversion relations. The interested reader is encouraged to pick up a copy of the original book for more details.\n\n====Several forms of the simplest inverse relations====\n\n{| class=\"wikitable\"\n|-\n! Relation !! Formula !! Inverse Formula !! Generating Functions (OGF) !! Generating Functions (EGF) !! Notes / References\n|-\n| 1 || <math>a_n = \\sum_{k=0}^n \\binom{n}{k} b_k</math> || <math>b_n = \\sum_{k=0}^n \\binom{n}{k} (-1)^{n-k} a_k</math> || <math>B(z) = \n\\frac{1}{1-z} A\\left(-\\frac{z}{1-z}\\right)</math> || <math>\\widehat{B}(z) = e^z \\widehat{A}(-z)</math> || See the [[Binomial transform]]\n|-\n| 2 || <math>a_n = \\sum_{k=0}^n \\binom{p-k}{p-n} b_k</math> || <math>b_n = \\sum_{k=0}^n \\binom{p-k}{p-n} (-1)^{n-k} a_k</math> || <center><math>\\ast</math></center> || <center><math>\\ast</math></center> ||\n|-\n| 3 || <math>a_n = \\sum_{k=0}^n \\binom{n+p}{k+p} b_k</math> || <math>b_n = \\sum_{k=0}^n \\binom{n+p}{k+p} (-1)^{n-k} a_k</math> || <math>B(z) = \n\\frac{1}{(1+z)^{p+1}} A\\left(\\frac{z}{1+z}\\right)</math> || <center><math>\\ast</math></center> ||\n|-\n| 4 || <math>a_n = \\sum_{k=0}^n \\binom{k+p}{n+p} b_k</math> || <math>b_n = \\sum_{k=0}^n \\binom{k+p}{n+p} (-1)^{n-k} a_k</math> || <center><math>\\ast</math></center> || <center><math>\\ast</math></center> ||\n|-\n| 5 || <math>a_n = \\sum_{k=1}^n \\frac{n!}{k!} \\binom{n-1}{k-1} b_k</math> || <math>b_n = \\sum_{k=1}^n \\frac{n!}{k!} \\binom{n-1}{k-1} (-1)^{n-k} a_k</math> || <center><math>\\ast</math></center> || <math>\\widehat{B}(z) = \\widehat{A}\\left(\\frac{z}{1+z}\\right)</math> ||\n|-\n| 6 || <math>a_n = \\sum_{k=0}^n \\binom{n}{k}^2 k! b_{n-k}</math> || <math>b_n = \\sum_{k=0}^n \\binom{n}{k}^2 (-1)^k k! a_{n-k}</math> || <center><math>\\ast</math></center>  || <math>\\widehat{B}(z) = \\frac{1}{1+z} \\widehat{A}\\left(\\frac{z}{1+z}\\right)</math> ||\n|-\n| 7 || <math>\\frac{n! a_n}{(n+p)!} = \\sum_{k=0}^n \\binom{n}{k} \\frac{k! b_{k}}{(k+p)!}</math> || <math>\\frac{n! b_n}{(n+p)!} = \\sum_{k=0}^n \\binom{n}{k} \\frac{(-1)^{n-k} k! a_{k}}{(k+p)!}</math> || <math>B(z) = \n\\frac{1}{(1+z)^{p+1}} A\\left(\\frac{z}{1+z}\\right)</math> || <center><math>\\ast</math></center> || \n|-\n|8 || <math>s_n = \\sum_{k \\geq 0} \\binom{n+k}{m+2k} a_k</math> || <center><math>\\ast</math></center> || <math>S(z) = \\frac{z^m}{(1-z)^{m+1}} A\\left(\\frac{z}{(1-z)^2}\\right)</math> || <center><math>\\ast</math></center> || See.<ref>Solution to exercise 5.71 in ''Concrete Mathematics''.</ref>\n|-\n|9 || <math>a_n = \\sum_{k=0}^n \\binom{n}{k} a^k (-c)^{n-k} b_{k}</math> || <center><math>\\ast</math></center> || <math>A(z) = \\frac{1}{1+cx} B\\left(\\frac{ax}{1+cx}\\right)</math> || <center><math>\\ast</math></center> || Generalization of the [[binomial transform]] for <math>a,b,c \\in \\mathbb{C}</math> such that <math>|ax / (1+cx)| < \\sigma_B</math>. \n|-\n|10 || <math>w_n = \\sum_{i=0}^n \\binom{n}{i} k^n a_i,\\ k \\neq 0</math> || <center><math>\\ast</math></center> || <center><math>\\ast</math></center> || <math>\\widehat{W}(A, k; z) = e^{kz} \\widehat{A}(kz)</math> || The '''<math>k</math>-binomial transform''' (see <ref name=\"SPIVEY7\">{{cite journal|last1=Spivey|first1=M. Z.|title=The k-binomial transforms and the Hankel transform|journal=Journal of Integer Sequences|date=2006|volume=9|issue=Article 06.1.1|url=https://cs.uwaterloo.ca/journals/JIS/VOL9/Spivey/spivey7.html}}</ref>) \n|- \n|11 || <math>f_n = \\sum_{i=0}^n \\binom{n}{i} k^{n-i} a_i,\\ k \\neq 0</math> || <center><math>\\ast</math></center> || <center><math>\\ast</math></center> || <math>\\widehat{F}(A, k; z) = e^{kz} \\widehat{A}(z)</math> || The '''falling <math>k</math>-binomial transform''' (refer to Spivey's article in <ref name=\"SPIVEY7\" />)\n|-\n|12 || <math>r_n = \\sum_{i=0}^n \\binom{n}{i} k^{i} a_i,\\ k \\neq 0</math> || <center><math>\\ast</math></center> || <center><math>\\ast</math></center> || <math>\\widehat{R}(A, k; z) = e^{z} \\widehat{A}(kz)</math> || The '''rising <math>k</math>-binomial transform''' (refer to Spivey's article in <ref name=\"SPIVEY7\" />)\n|}\n\n====Gould classes of inverse relations====\n\nThe terms, <math>A_{n,k}</math> and <math>B_{n,k}</math>, in the inversion formulas of the form\n\n:<math>a_n = \\sum_k A_{n,k} \\cdot b_k \\quad\\longleftrightarrow\\quad b_n = \\sum_k B_{n,k} \\cdot (-1)^{n-k} a_k, </math>\n\nforming several special cases of ''Gould classes of inverse relations'' are given in the next table.\n\n{| class=\"wikitable\"\n|-\n! Class !! <math>A_{n,k}</math> !! <math>B_{n,k}</math>\n|-\n| 1 || <math>\\binom{p+qk-k}{n-k}</math> || <math>\\binom{p+qn-k}{n-k}-q\\binom{p+qn-k-1}{n-k-1}</math>\n|-\n| 2 || <math>\\binom{p+qk-k}{n-k}+q\\binom{p+qk-k}{n-1-k}</math> || <math>\\binom{p+qn-k}{n-k}</math>\n|-\n| 3 || <math>\\binom{p+qn-n}{k-n}</math> || <math>\\binom{p+qk-n}{k-n}-q\\binom{p+qk-n-1}{k-n-1}</math>\n|-\n| 4 || <math>\\binom{p+qn-n}{k-n}+q\\binom{p+qn-n}{k-1-n}</math> || <math>\\binom{p+qk-n}{k-n}</math>\n|}\n\nFor classes 1 and 2, the range on the sum satisfies <math>k \\in [0, n]</math>, and for classes 3 and 4 the bounds on the summation are given by <math>k=n,n+1,\\ldots</math>. These terms are also somewhat simplified from their original forms in the table by the identities\n\n:<math>\\binom{p+qn-k}{n-k}-q \\times \\binom{p+qn-k-1}{n-k-1} = \\frac{p+qk-k}{p+qn-k} \\binom{p+qn-k}{n-k}</math> \n:<math>\\binom{p+qk-k}{n-k}+q \\times \\binom{p+qk-k}{n-1-k} = \\frac{p+qn-n+1}{p+qk-n+1} \\binom{p+qk-k}{n-k}. </math>\n\n====The simpler Chebyshev inverse relations====\n\nThe so-termed ''simpler'' cases of the Chebyshev classes of inverse relations in the subsection below are given in the next table.\n\n{| class=\"wikitable\"\n|-\n! Relation !! Formula for <math>a_n</math> !! Inverse Formula for <math>b_n</math> \n|-\n| 1 || <math>a_n = \\sum_{k} \\binom{n}{k} b_{n-2k}</math> || <math>b_n = \\sum_k \\left[\\binom{n-k}{k}+\\binom{n-k-1}{k-1}\\right] (-1)^k a_{n-2k}</math> \n|-\n| 2 || <math>a_n = \\sum_{k} \\left[\\binom{n}{k}-\\binom{n}{k-1}\\right] b_{n-2k}</math> || <math>b_n = \\sum_k \\binom{n-k}{k} (-1)^k a_{n-2k}</math> \n|-\n| 3 || <math>a_n = \\sum_k \\binom{n+2k}{k} b_{n+2k}</math> || <math>b_n = \\sum_k \\left[\\binom{n+k}{k}+\\binom{n+k-1}{k-1}\\right] (-1)^k a_{n+2k}</math> \n|-\n| 4 || <math>a_n = \\sum_k \\left[\\binom{n+2k}{k}-\\binom{n+2k}{k-1}\\right] b_{n+2k}</math> || <math>b_n = \\sum_k \\binom{n+2k}{k} (-1)^k a_{n+2k}</math> \n|-\n| 5 ||<math>a_n = \\sum_k \\binom{n-k}{k} b_{n-k}</math> || <math>b_n = \\sum_k \\left[\\binom{n+k-1}{k}-\\binom{n+k-1}{k-1}\\right] (-1)^k a_{n-k}</math> \n|-\n| 6 || <math>a_n = \\sum_k \\left[\\binom{n+1-k}{k}+\\binom{n-k}{k-1}\\right] b_{n-k}</math> || <math>b_n = \\sum_k \\binom{n+k}{k} (-1)^k a_{n-k}</math> \n|-\n| 7 || <math>a_n = \\sum_{k=0}^n \\binom{n}{k} b_{n+ck}</math> || <math>b_n = \\sum_k \\binom{n+ck+k}{k} \\frac{n (-1)^k}{n+ck+k} a_{n+ck}</math> \n|}\n\nThe formulas in the table are simplified somewhat by the following identities:\n\n:<math> \n   \\begin{align} \n     \\binom{n-k}{k} + \\binom{n-k-1}{k-1} & = \\frac{n}{n-k} \\binom{n-k}{k} \\\\ \n     \\binom{n}{k} - \\binom{n}{k-1} & = \\frac{n+1-k}{n+1-2k} \\binom{n}{k} \\\\ \n     \\binom{n+2k}{k}-\\binom{n+2k}{k-1} & = \\frac{n+1}{n+1+k} \\binom{n+2k}{k} \\\\ \n     \\binom{n+k-1}{k} - \\binom{n+k-1}{k-1} & = \\frac{n-k}{n+k} \\binom{n+k}{k}. \n   \\end{align} \n</math>\n\nAdditionally the inversion relations given in the table also hold when <math>n \\longmapsto n+p</math> in any given relation.\n\n====Chebyshev classes of inverse relations====\n\nThe terms, <math>A_{n,k}</math> and <math>B_{n,k}</math>, in the inversion formulas of the form\n\n:<math>a_n = \\sum_k A_{n,k} \\cdot b_{n+ck} \\quad\\longleftrightarrow\\quad b_n = \\sum_k B_{n,k} \\cdot (-1)^{k} a_{n+ck}, </math>\n\nfor non-zero integers <math>c</math>\nforming several special cases of ''Chebyshev classes of inverse relations'' are given in the next table.\n\n{| class=\"wikitable\"\n|-\n! Class !! <math>A_{n,k}</math> !! <math>B_{n,k}</math> \n|-\n| 1 || <math>\\binom{n}{k}</math> || <math>\\binom{n+ck+k}{k}-(c+1)\\binom{n+ck+k-1}{k-1}</math> \n|-\n| 2 || <math>\\binom{n}{k}+(c+1)\\binom{n}{k-1}</math> || <math>\\binom{n+ck+k}{k}</math> \n|-\n| 3 || <math>\\binom{n+ck}{k}</math> || <math>\\binom{n-1+k}{k}+c\\binom{n-1+k}{k-1}</math> \n|-\n| 4 || <math>\\binom{n+ck}{k}-(c-1)\\binom{n+ck}{k-1}</math> || <math>\\binom{n+k}{k}</math> \n|}\n\nAdditionally, these inversion relations also hold when <math>n \\longmapsto n+p</math> for some <math>p=0,1,2,\\ldots,</math> or when the sign factor of <math>(-1)^k</math> is shifted from the terms <math>B_{n,k}</math> to the terms <math>A_{n,k}</math>. The formulas given in the previous table are simplified somewhat by the identities\n\n:<math>\n   \\begin{align} \n     \\binom{n+ck+k}{k}-(c+1)\\binom{n+ck+k-1}{k-1} & = \\frac{n}{n+ck+k} \\binom{n+ck+k}{k} \\\\ \n     \\binom{n}{k} + (c+1) \\binom{n}{k-1} & = \\frac{n+1+ck}{n+1-k} \\binom{n}{k} \\\\ \n     \\binom{n-1+k}{k} + c \\binom{n-1+k}{k-1} & = \\frac{n+ck}{n} \\binom{n-1+k}{k} \\\\ \n     \\binom{n+ck}{k} - (c-1) \\binom{n+ck}{k-1} & = \\frac{n+1}{n+1+ck-k} \\binom{n+ck}{k}. \n   \\end{align} \n</math>\n\n====The simpler Legendre inverse relations====\n\n{| class=\"wikitable\"\n|-\n! Relation !! Formula for <math>a_n</math> !! Inverse Formula for <math>b_n</math>\n|-\n| 1 || <math>a_n = \\sum_k \\binom{n+p+k}{n-k} b_k</math> || <math>b_n = \\sum_k \\left[\\binom{2n+p}{n-k}-\\binom{2n+p}{n-k-1}\\right] (-1)^{n-k} a_k</math> \n|-\n| 2 || <math>a_n = \\sum_k \\binom{2n+p}{n-k} b_k</math> || <math>b_n = \\sum_k \\left[\\binom{n+p+k}{n-k}-\\binom{n+p+k-1}{n-k-1}\\right] (-1)^{n-k} a_k</math> \n|-\n| 3 || <math>a_n =\\sum_{k\\geq n} \\binom{n+p+k}{k-n} b_k</math> || <math>b_n = \\sum_{k \\geq n} \\left[\\binom{2k+p}{k-n}-\\binom{2k+p}{k-n-1}\\right] (-1)^{n-k} a_k</math> \n|-\n| 4 || <math>a_n =\\sum_{k\\geq n} \\binom{2k+p}{k-n} b_k</math> || <math>b_n = \\sum_{k \\geq n} \\left[\\binom{n+p+k}{k-n}-\\binom{n+p+k-1}{k-n-1}\\right] (-1)^{n-k} a_k</math> \n|-\n| 5 || <math>a_n = \\sum_k \\binom{2n+p}{k} b_{n-2k}</math> || <math>b_n = \\sum_k \\left[\\binom{2n+p-3k}{k}+3\\binom{2n+p-3k-1}{k-1}\\right] (-1)^k a_{n-2k}</math> \n|-\n| 6 || <math>a_n = \\sum_k \\left[\\binom{2n+p}{k} - 3 \\binom{2n+p}{k-1}\\right] b_{n-2k}</math> || <math>b_n = \\sum_k \\binom{2n+p-3k}{k} (-1)^k a_{n-2k}</math> \n|-\n| 7 || <math>a_n = \\sum_{k=0}^{[n/2]} \\binom{3n}{k} b_{n-2k}</math> || <math>b_n =  \\sum_{k=0}^{[n/2]} \\left[\\binom{3n-5k}{k}+5\\binom{3n-5k-1}{k-1}\\right] (-1)^k a_{n-2k}</math> \n|-\n| 8 || <math>a_n = \\sum_{k=0}^{[n/3]} \\binom{2n}{k} b_{n-3k}</math> || <math>b_n =  \\sum_{k=0}^{[n/3]} \\left[\\binom{2n-5k}{k}+5\\binom{2n-5k-1}{k-1}\\right] (-1)^k a_{n-3k}</math> \n|}\n\n====Legendre–Chebyshev classes of inverse relations====\n\nThe ''Legendre–Chebyshev classes of inverse relations'' correspond to inversion relations of the form\n\n:<math>a_n = \\sum_k A_{n,k} \\cdot b_k \\quad\\longleftrightarrow\\quad b_n = \\sum_k B_{n,k} \\cdot (-1)^{n-k} a_k, </math>\n\nwhere the terms, <math>A_{n,k}</math> and <math>B_{n,k}</math>, implicitly depend on some fixed non-zero <math>c \\in \\mathbb{Z}</math>. In general, given a class of Chebyshev inverse pairs of the form\n\n:<math>a_n = \\sum_k A_{n,k} \\cdot b_{n-ck} \\quad\\longleftrightarrow\\quad b_n = \\sum_k B_{n,k} \\cdot (-1)^{k} a_{n-ck}, </math>\n\nif <math>c</math> a prime, the substitution of <math>n \\longmapsto cn+p</math>, <math>a_{cn+p} \\longmapsto A_n</math>, and <math>b_{cn+p} \\longmapsto B_n</math> (possibly replacing <math>k \\longmapsto n-k</math>) leads to a ''Legendre–Chebyshev'' pair of the form<ref>See section 2.5 of Riordan</ref>\n\n:<math>A_n = \\sum_k A_{cn+p,k} B_{n-k} \\quad\\longleftrightarrow\\quad B_n = \\sum_k B_{cn+p,k} (-1)^k A_{n-k}. </math>\n\nSimilarly, if the positive integer <math>c := d e</math> is composite, we can derive inversion pairs of the form\n\n:<math>A_n = \\sum_k A_{dn+p,k} B_{n-ek} \\quad\\longleftrightarrow\\quad B_n = \\sum_k B_{dn+p,k} (-1)^k A_{n-ek}. </math>\n\nThe next table summarizes several generalized classes of Legendre–Chebyshev inverse relations for some non-zero integer <math>c</math>.\n\n{| class=\"wikitable\"\n|-\n! Class !! <math>A_{n,k}</math> !! <math>B_{n,k}</math> \n|-\n| 1 || <math>\\binom{cn+p}{n-k}</math> || <math>\\binom{n+p-1+ck-k}{n-k}+c\\binom{n+p-1+ck-k}{n-k-1}</math> \n|-\n| 2 || <math>\\binom{cn+p}{k-n}</math> || <math>\\binom{ck+k+p-n-1}{k-n}-c\\binom{ck+k+p-n-1}{k-n-1}</math> \n|-\n| 3 || <math>\\binom{ck+p}{n-p}</math> || <math>\\binom{cn+n+p-k-1}{n-k}-c\\binom{cn+n+p-k-1}{n-k-1}</math> \n|-\n| 4 || <math>\\binom{ck+p}{k-n}</math> || <math>\\binom{cn-n+p+k-1}{k-n}+c\\binom{cn-n+p+k-1}{k-n-1}</math> \n|-\n| 5 || <math>\\binom{cn+p}{n-k}-(c-1)\\binom{cn+p}{n-k-1}</math> || <math>\\binom{n+p+ck-k}{n-k}</math> \n|-\n| 6 || <math>\\binom{cn+p}{k-n}+(c+1)\\binom{cn+p}{k-n-1}</math> || <math>\\binom{ck+k+p-n}{k-n}</math> \n|-\n| 7 || <math>\\binom{ck+p}{n-k}+(c+1)\\binom{ck+p}{n-k-1}</math> || <math>\\binom{cn+n+p-k}{n-k}</math> \n|-\n| 8 || <math>\\binom{ck+p}{k-n}-(c-1)\\binom{ck+p}{k-n-1}</math> || <math>\\binom{cn-n+p+k}{k-n}</math> \n|}\n\n====Abel inverse relations====\n\n''Abel inverse relations'' correspond to ''Abel inverse pairs'' of the form\n\n:<math>a_n = \\sum_{k=0}^n \\binom{n}{k} A_{nk} b_k \\quad\\longleftrightarrow\\quad b_n = \\sum_{k=0}^n \\binom{n}{k} B_{nk}(-1)^{n-k} a_k, </math>\n\nwhere the terms, <math>A_{nk}</math> and <math>B_{nk}</math>, may implicitly vary with some indeterminate summation parameter <math>x</math>. \nThese relations also still hold if the binomial coefficient substitution of <math>\\binom{n}{k} \\longmapsto \\binom{n+p}{k+p}</math> is performed for some non-negative integer <math>p</math>. \nThe next table summarizes several notable forms of these Abel inverse relations.\n\n{| class=\"wikitable\"\n|-\n! Number !! <math>A_{nk}</math> !! <math>B_{nk}</math> !! Generating Function Identity\n|-\n| 1 || <math>x (x+n-k)^{n-k-1}</math> || <math>x (x-n+k)^{n-k-1}</math> || <center><math>\\ast</math></center> \n|-\n| 2 || <math>(x+n-k)^{n-k}</math> || <math>(x^2-n+k) (x-n+k)^{n-k-2}</math> || <center><math>\\ast</math></center>\n|-\n| 3 || <math>(x+k)^{n-k}</math> || <math>(x+k) (x+n)^{n-k-1}</math> || <center><math>\\ast</math></center>\n|-\n| 3a || <math>(x+n) (x+k)^{n-k-1}</math> || <math>(x+n)^{n-k}</math> || <center><math>\\ast</math></center>\n|-\n| 4 || <math>(x+2n) (x+n+k)^{n-k-1}</math> || <math>(x+2n) (x+n+k)^{n-k-1}</math> || <center><math>\\ast</math></center>\n|-\n| 4a || <math>(x+2k) (x+n+k)^{n-k-1}</math> || <math>(x+2k) (x+n+k)^{n-k-1}</math> || <center><math>\\ast</math></center>\n|-\n| 5 || <math>(n+k)^{n-k}</math> || <math>\\left[n+k(4n-1)\\right] (n+k)^{n-k-2}</math> || <center><math>\\ast</math></center>\n|}\n\n====Inverse relations derived from ordinary generating functions====\n\nIf we let the ''convolved Fibonacci numbers'', <math>f_k^{(\\pm p)}</math>, be defined by\n\n:<math>\n\\begin{align} \n     f_n^{(p)} & = \\sum_{j \\geq 0} \\binom{p+n-j-1}{n-j} \\binom{n-j}{j} \\\\ \n     f_n^{(-p)} & = \\sum_{j \\geq 0} \\binom{p}{n+j} \\binom{n-j}{j} (-1)^{n-j}, \n\\end{align} \n</math>\n\nwe have the next table of inverse relations which are obtained from properties of ordinary sequence generating functions proved as in section 3.3 of Riordan's book.\n\n{| class=\"wikitable\"\n|-\n! Relation !! Formula for <math>a_n</math> !! Inverse Formula for <math>b_n</math> \n|-\n| 1 || <math>a_n = \\sum_{k=0}^n \\binom{p+k}{k} b_{n-k}</math> || <math>b_n = \\sum_{k=0}^n \\binom{p+1}{k} (-1)^k a_{n-k}</math> \n|-\n| 2 || <math>a_n = \\sum_{k \\geq 0} \\binom{p+k}{k} b_{n-qk}</math> || <math>b_n = \\sum_k \\binom{p+1}{k} (-1)^k a_{n-qk}</math> \n|-\n| 3 || <math>a_n = \\sum_{k=0}^{n} f_k^{(p)} b_{n-k}</math> || <math>b_n = \\sum_{k=0}^n f_k^{(-p)} a_{n-k}</math> \n|-\n| 4 || <math>a_n = \\sum_{k=0}^n \\binom{2k}{k} b_{n-k} </math> || <math>\\sum_{k=0}^n \\binom{2k}{k} \\frac{a_{n-k}}{(1-2k)}</math> \n|-\n| 5 || <math>a_n = \\sum_{k=0}^n \\binom{2k}{k} \\frac{b_{n-k}}{(k+1)}</math> || <math>b_n = a_n - \\sum_{k=1}^n \\binom{2k}{k} \\frac{a_{n-k}}{k}</math> \n|-\n| 6 || <math>a_n = \\sum_{k=0}^n \\binom{2p+2k}{p+k} \\binom{p+k}{k} \\binom{2p}{p}^{-1} b_{n-k}</math> || <math>b_n = \\sum_{k=0}^n \\binom{2p+1}{2k} \\binom{p+k}{k} \\binom{p+k}{2k}^{-1} (-1)^k a_{n-k}</math> \n|-\n| 7 || <math>a_n = \\sum_k \\binom{4k}{2k} b_{n-2k}</math> || <math>b_n = \\sum_k \\binom{4k}{2k} \\frac{(8k+1) a_{n-2k}}{(2k+1)(k+1)}</math> \n|-\n| 8 || <math>a_n = \\sum_k \\binom{4k+2}{2k+1} b_{n-2k}</math> || <math>b_n = \\frac{a_n}{2} - \\sum_{k \\geq 1} \\binom{4k-2}{2k-1} \\frac{(8k-3) a_{n-2k}}{2k (4k-3)}</math> \n|-\n| 9 || <math>a_n = \\binom{4k}{2k} \\frac{b_{n-2k}}{(1-4k)}</math> || <math>b_n = \\sum_k \\binom{4k}{2k} \\frac{a_{n-2k}}{(2k+1)}</math> \n|}\n\nNote that relations 3, 4, 5, and 6 in the table may be transformed according to the substitutions <math>a_{n-k} \\longmapsto a_{n-qk}</math> and <math>b_{n-k} \\longmapsto b_{n-qk}</math> for some fixed non-zero integer <math>q \\geq 1</math>.\n\n====Inverse relations derived from exponential generating functions====\n\nLet <math>B_n</math> and <math>E_n</math> denote the [[Bernoulli number]]s and [[Euler number]]s, respectively, and suppose that the sequences, <math>\\{d_{2n}\\}</math>, <math>\\{e_{2n}\\}</math>, and <math>\\{f_{2n}\\}</math> are defined by the following exponential generating functions:<ref>See section 3.4 in Riordan.</ref>\n\n:<math>\n\\begin{align} \n     \\sum_{n \\geq 0} \\frac{d_{2n} z^{2n}}{(2n)!} & = \\frac{2z}{e^z-e^{-z}} \\\\ \n     \\sum_{n \\geq 0} \\frac{e_{2n} z^{2n}}{(2n)!} & = \\frac{z^2}{e^z+e^{-z}-2} \\\\ \n     \\sum_{n \\geq 0} \\frac{f_{2n} z^{2n}}{(2n)!} & = \\frac{z^3}{3(e^z-e^{-z}-2z)}. \n\\end{align} \n</math>\n\nThe next table summarizes several notable cases of inversion relations obtained from exponential generating functions in section 3.4 of Riordan's book.<ref>Compare to the inversion formulas given in section 24.5(iii) of the ''NIST Handbook''.</ref>\n\n{| class=\"wikitable\"\n|-\n! Relation !! Formula for <math>a_n</math> !! Inverse Formula for <math>b_n</math> \n|-\n| 1 || <math>a_n = \\sum_{k=0}^n \\binom{n}{k} \\frac{b_k}{(k+1)}</math> || <math>b_n = \\sum_{k=0}^n B_k a_{n-k}</math> \n|-\n| 2 || <math>a_n = \\sum_k \\binom{n+k}{k} \\frac{b_{n+k}}{(k+1)}</math> || <math>b_n = \\sum_k \\binom{n+k}{k} B_k a_{n+k}</math> \n|-\n| 3 || <math>a_n = \\sum_k \\binom{n}{2k} b_{n-2k}</math> || <math>b_n = \\sum_k \\binom{n}{2k} E_{2k} a_{n-2k}</math> \n|-\n| 4 || <math>a_n = \\sum_k \\binom{n+2k}{2k} b_{n+2k}</math> || <math>b_n = \\sum_k \\binom{n+2k}{2k} E_{2k} a_{n+2k}</math> \n|-\n| 5 || <math>a_n = \\sum_k \\binom{n}{2k} \\frac{b_{n-2k}}{(2k+1)}</math> || <math>b_n = \\sum_k \\binom{n}{2k} d_{2k} a_{n-2k}</math> \n|-\n| 6 || <math>a_n = \\sum_k \\binom{n+1}{2k+1} b_{n-2k}</math> || <math>(n+1) \\cdot b_n = \\sum_k \\binom{n+1}{2k} d_{2k} a_{n-2k}</math> \n|-\n| 7 || <math>a_n = \\sum_k \\binom{n}{2k} \\binom{2k+2}{2}^{-1} b_{n-2k}</math> || <math>b_n = \\sum_k \\binom{n}{2k} e_{2k} a_{n-2k}</math> \n|-\n| 8 || <math>a_n = \\sum_k \\binom{n+2}{2k+2} b_{n-2k}</math> || <math>\\binom{n+2}{2} \\cdot b_n = \\sum_k \\binom{n+2}{2k} e_{2k} a_{n-2k}</math> \n|-\n| 9 || <math>a_n = \\sum_k \\binom{n}{2k} \\binom{2k+3}{3}^{-1} b_{n-2k}</math> || <math>b_n = \\sum_k \\binom{n}{2k} f_{2k} a_{n-2k}</math> \n|-\n| 10 || <math>a_n = \\sum_k \\binom{n+3}{2k+3} b_{n-2k}</math> || <math>\\binom{n+3}{3} \\cdot b_n = \\sum_k \\binom{n+3}{2k} f_{2k} a_{n-2k}</math> \n|}\n\n====Multinomial inverses====\n\nThe inverse relations used in formulating the [[binomial transform]] cited in the previous subsection are generalized to corresponding two-index inverse relations for sequences of two indices, and to multinomial inversion formulas for sequences of <math>j \\geq 3</math> indices involving the binomial coefficients in Riordan.<ref>See section 3.5 in Riordan's book.</ref> \nIn particular, we have the form of a two-index inverse relation given by\n\n:<math>a_{mn} = \\sum_{j=0}^m \\sum_{k=0}^n \\binom{m}{j} \\binom{n}{k} (-1)^{j+k} b_{jk} \\quad\\longleftrightarrow\\quad \n       b_{mn} = \\sum_{j=0}^m \\sum_{k=0}^n \\binom{m}{j} \\binom{n}{k} (-1)^{j+k} a_{jk}, </math>\n\nand the more general form of a multinomial pair of inversion formulas given by\n\n:<math>a_{n_1 n_2\\cdots n_j} = \\sum_{k_1, \\ldots, k_j} \\binom{n_1}{k_1} \\cdots \\binom{n_j}{k_j} (-1)^{k_1+\\cdots+k_j} \n       b_{k_1 k_2 \\cdots k_j} \\quad\\longleftrightarrow\\quad \n       b_{n_1 n_2\\cdots n_j} = \\sum_{k_1, \\ldots, k_j} \\binom{n_1}{k_1} \\cdots \\binom{n_j}{k_j} (-1)^{k_1+\\cdots+k_j} \n       a_{k_1 k_2 \\cdots k_j}. </math>\n\n==Notes==\n{{Reflist|30em}}\n\n==References==\n* {{cite book|last1=Comtet|first1=L.|title=Advanced Combinatorics|date=1974|publisher=D. Reidel Publishing Company|isbn=9027703809|url=http://plouffe.fr/simon/math/AdvancedComb.pdf}}\n* {{cite book|last1=Flajolet and Sedgewick|title=Analytic Combinatorics|date=2010|publisher=Cambridge University Press|isbn=978-0-521-89806-5}}\n* {{cite book|last1=Graham, Knuth and Patashnik|title=Concrete Mathematics: A Foundation for Computer Science|date=1994|publisher=Addison-Wesley|isbn=0201558025|edition=2nd}}\n* {{cite book|last1=Knuth|first1=D. E.|title=The Art of Computer Programming: Fundamental Algorithms|date=1997|publisher=Addison-Wesley|isbn=0-201-89683-4|volume=1}}\n* {{cite book|last1=Lando|first1=S. K.|title=Lectures on Generating Functions|date=2002|publisher=American Mathematical Society|isbn=0-8218-3481-9}}\n* {{cite book|last1=Oliver, Lozier, Boisvert and Clark|title=NIST Handbook of Mathematical Functions|date=2010|publisher=Cambridge University Press|isbn=978-0-521-14063-8|url=http://dlmf.nist.gov/}}\n* {{cite book|last1=Riordan|first1=J.|title=Combinatorial Identities|date=1968|publisher=Wiley and Sons}}\n* {{cite book|last1=Roman|first1=S.|title=The Umbral Calculus|date=1984|publisher=Dover|isbn=0-486-44139-3}}\n* {{cite arxiv|last1=Schmidt|first1=M. D.|title=Zeta Series Generating Function Transformations Related to Generalized Stirling Numbers and Partial Sums of the Hurwitz Zeta Function|date=3 Nov 2016|arxiv=1611.00957}}\n* {{cite arxiv|last1=Schmidt|first1=M. D.|title=Zeta Series Generating Function Transformations Related to Polylogarithm Functions and the ''k''-Order Harmonic Numbers|date=30 Oct 2016|arxiv=1610.09666}}\n* {{cite journal|last1=Schmidt|first1=M. D.|title=Jacobi-Type Continued Fractions for the Ordinary Generating Functions of Generalized Factorial Functions|journal=Journal of Integer Sequences|date=2017|volume=20|url=https://cs.uwaterloo.ca/journals/JIS/VOL20/Schmidt/schmidt14.html}}\n* {{cite arxiv|last1=Schmidt|first1=M. D.|title=Square Series Generating Function Transformations|date=9 Sep 2016|arxiv=1609.02803}}\n* {{cite book|last1=Stanley|first1=R. P.|title=Enumerative Combinatorics|date=1999|publisher=Cambridge University Press|isbn=978-0-521-78987-5|volume=2}}\n\n[[Category:Generating functions]]"
    },
    {
      "title": "Matsushima's formula",
      "url": "https://en.wikipedia.org/wiki/Matsushima%27s_formula",
      "text": "In [[mathematics]], '''Matsushima’s formula''', introduced by {{harvs|txt|last=Matsushima|year=1967|authorlink= Yozô Matsushima}}, is a formula for the [[Betti number]]s of a quotient of a [[symmetric space]] ''G''/''H'' by a [[discrete group]], in terms of [[unitary representation|unitary]] [[group representation|representations]] of the [[group (mathematics)|group]] ''G''.\n<ref>{{Citation | last1=Matsushima | first1=Yozô | title=A formula for the Betti numbers of compact locally symmetric Riemannian manifolds |mr=0222908 | year=1967 | journal=Journal of Differential Geometry | issn=0022-040X | volume=1 | pages=99–109}}</ref> The '''Matsushima–Murakami formula''' is a generalization giving dimensions of spaces of [[automorphic form]]s, introduced by {{harvtxt|Matsushima|Murakami|1968}}.<ref>{{Citation | last1=Matsushima | first1=Yozô | last2=Murakami | first2=Shingo | title=On certain cohomology groups attached to Hermitian symmetric spaces. II |mr=0266238 | year=1968 | journal=Osaka Journal of Mathematics | issn=0030-6126 | volume=5 | pages=223–241}}</ref>\n\n==References==\n<references/>\n\n[[Category:Differential geometry]]\n[[Category:Algebraic topology]]\n[[Category:Topological graph theory]]\n[[Category:Generating functions]]\n\n{{topology-stub}}"
    },
    {
      "title": "Moment-generating function",
      "url": "https://en.wikipedia.org/wiki/Moment-generating_function",
      "text": "In [[probability theory]] and [[statistics]], the '''moment-[[generating function]]''' of a real-valued [[random variable]] is an alternative specification of its [[probability distribution]]. Thus, it provides the basis of an alternative route to analytical results compared with working directly with [[probability density function]]s or [[cumulative distribution function]]s. There are particularly simple results for the moment-generating functions of distributions defined by the weighted sums of random variables. However, not all random variables have moment-generating functions.\n\nAs its name implies, the moment generating function can be used to compute a distribution’s moments: the ''n''th moment about 0 is the ''n''th derivative of the moment-generating function, evaluated at 0.\n\nIn addition to real-valued distributions (univariate distributions), moment-generating functions can be defined for vector- or matrix-valued random variables, and can even be extended to more general cases.\n\nThe moment-generating function of a real-valued distribution does not always exist, unlike the [[Characteristic function (probability theory)|characteristic function]]. There are relations between the behavior of the moment-generating function of a distribution and properties of the distribution, such as the existence of moments.\n\n==Definition==\nThe moment-generating function of a [[random variable]] ''X'' is\n\n:<math> M_X(t) := \\operatorname E \\left[e^{tX}\\right], \\quad t \\in \\mathbb{R}, </math>\n\nwherever this [[expected value|expectation]] exists. In other words, the moment-generating function is the [[expected value|expectation]] of the random variable <math> e^{tX}</math>. More generally, when <math>\\mathbf X = ( X_1, \\ldots, X_n)^{\\mathrm{T}}</math>, an <math>n</math>-dimensional [[random vector]], and <math>\\mathbf t</math> is a fixed vector, one uses <math>\\mathbf t \\cdot \\mathbf X = \\mathbf t^\\mathrm T\\mathbf X</math> instead of&nbsp;<math>tX</math>:\n\n:<math> M_{\\mathbf X}(\\mathbf t) := \\operatorname E \\left(e^{\\mathbf t^\\mathrm T\\mathbf X}\\right).</math>\n\n<math> M_X(0) </math> always exists and is equal to&nbsp;1. However, a key problem with moment-generating functions is that moments and the moment-generating function may not exist, as the integrals need not converge absolutely. By contrast, the [[Characteristic function (probability theory)|characteristic function]] or Fourier transform always exists (because it is the integral of a bounded function on a space of finite [[measure (mathematics)|measure]]), and for some purposes may be used instead.\n\nThe moment-generating function is so named because it can be used to find the moments of the distribution.<ref>Bulmer, M. G., Principles of Statistics, Dover, 1979, pp. 75–79.</ref>  The series expansion of <math>e^{tX}</math> is\n\n: <math>\ne^{t\\,X} = 1 + t\\,X + \\frac{t^2\\,X^2}{2!} + \\frac{t^3\\,X^3}{3!} + \\cdots +\\frac{t^n\\,X^n}{n!} + \\cdots.\n</math>\n\nHence\n\n: <math>\n\\begin{align}\nM_X(t) = \\operatorname E (e^{t\\,X}) &= 1 + t \\operatorname E (X) + \\frac{t^2 \\operatorname E (X^2)}{2!} + \\frac{t^3\\operatorname E (X^3)}{3!}+\\cdots + \\frac{t^n\\operatorname E (X^n)}{n!}+\\cdots \\\\\n& = 1 + tm_1 + \\frac{t^2m_2}{2!} + \\frac{t^3m_3}{3!}+\\cdots + \\frac{t^nm_n}{n!} + \\cdots,\n\\end{align}\n</math>\n\nwhere <math>m_n</math> is the <math>n</math>th [[moment (mathematics)|moment]]. Differentiating <math>M_X(t)</math> <math>i</math> times with respect to <math>t</math> and setting <math>t = 0</math>, we obtain the <math>i</math>th moment about the origin, <math>m_i</math>;\nsee [[Moment-generating function#Calculations of moments|Calculations of moments]] below.\n\nIf <math>X</math> is a continuous random variable, the following relation between its moment-generating function <math>M_X(t)</math> and the [[two-sided Laplace transform]] of its probability density function <math>f_X(x)</math> holds:\n\n: <math>\nM_X(t) = \\mathcal{B}\\{f_X\\}(-t),\n</math>\n\nsince the PDF's two-sided Laplace transform is given as\n\n: <math>\n\\mathcal{B}\\{f_X\\}(s) = \\int_{-\\infty}^\\infty e^{-sx} f_X(x)\\, dx,\n</math>\n\nand the moment-generating function's definition expands (by the [[law of the unconscious statistician]]) to\n: <math>\nM_X(t) = \\operatorname E \\left[e^{tX}\\right] = \\int_{-\\infty}^\\infty e^{tx} f_X(x)\\, dx.\n</math>\n\nThis is consistent with the characteristic function of <math>X</math> being a [[Wick rotation]] of <math>M_X(t)</math> when the moment generating function exists, as the characteristic function of a continuous random variable <math>X</math> is the [[Fourier transform]] of its probability density function <math>f_X(x)</math>, and in general when a function <math>f(x)</math> is of [[exponential order]], the Fourier transform of <math>f</math> is a Wick rotation of its two-sided Laplace transform in the region of convergence. See [[Fourier transform#Laplace_transform|the relation of the Fourier and Laplace transforms]] for further information.\n\n==Examples==\nHere are some examples of the moment-generating function and the characteristic function for comparison. It can be seen that the characteristic function is a [[Wick rotation]] of the moment-generating function <math>M_X(t)</math> when the latter exists.\n:{|class=\"wikitable\"\n|-\n! Distribution\n! Moment-generating function <math>M_X(t)</math>\n! Characteristic function <math>\\varphi (t)</math>\n|-\n|[[Degenerate distribution|Degenerate]] <math>\\delta_a</math>\n|<math>e^{ta}</math>\n|<math>e^{ita}</math>\n|-\n| [[Bernoulli distribution|Bernoulli]] <math>P(X = 1) = p</math> \n| <math>1 - p + pe^t</math>\n| <math>1 - p + pe^{it}</math>\n|-\n| [[Geometric distribution|Geometric]]  <math>(1 - p)^{k-1}\\,p</math>\n| <math>\\frac{p e^t}{1 - (1 - p) e^t}</math> <br/> <math>\\forall t < -\\ln(1 - p)</math>\n| <math>\\frac{p e^{it}}{1 - (1 - p)\\,e^{it}}</math>\n|-\n| [[Binomial distribution|Binomial]] <math>B(n, p)</math>\n| <math>\\left(1 - p + pe^t\\right)^n</math>\n| <math>\\left(1 - p + pe^{it}\\right)^n</math>\n|-\n|[[Negative binomial distribution|Negative Binomial]] <math>NB(r, p)</math>\n|<math>\\frac{(1 - p)^r}{\\left(1 - pe^t\\right)^r}</math>\n|<math>\\frac{(1 - p)^r}{\\left(1 - pe^{it}\\right)^r}</math>\n|-\n| [[Poisson distribution|Poisson]] <math>Pois(\\lambda)</math>\n| <math>e^{\\lambda(e^t - 1)}</math> \n| <math>e^{\\lambda(e^{it} - 1)}</math> \n|- \n| [[Uniform distribution (continuous)|Uniform (continuous)]] <math>U(a, b)</math>\n| <math>\\frac{e^{tb} - e^{ta}}{t(b - a)}</math>\n| <math>\\frac{e^{itb} - e^{ita}}{it(b - a)}</math>\n|- \n| [[Discrete uniform distribution|Uniform (discrete)]] <math>DU(a, b)</math>\n| <math>\\frac{e^{at} - e^{(b + 1)t}}{(b - a + 1)(1 - e^{t})}</math>\n| <math>\\frac{e^{ait} - e^{(b + 1)it}}{(b - a + 1)(1 - e^{it})}</math>\n|-\n|[[Laplace distribution|Laplace]] <math>L(\\mu, b)</math>\n|<math>\\frac{e^{t\\mu}}{1 - b^2t^2}, ~ |t| < 1/b</math>\n|<math>\\frac{e^{it\\mu}}{1 + b^2t^2}</math>\n|-\n| [[Normal distribution|Normal]] <math>N(\\mu, \\sigma^2)</math>\n| <math>e^{t\\mu + \\frac{1}{2}\\sigma^2t^2}</math>\n| <math>e^{it\\mu - \\frac{1}{2}\\sigma^2t^2}</math>\n|-\n| [[Chi-squared distribution|Chi-squared]] <math>\\Chi^2_k</math>\n| <math>(1 - 2t)^{-\\frac{k}{2}}</math>\n| <math>(1 - 2it)^{-\\frac{k}{2}}</math>\n|-\n|[[Noncentral chi-squared distribution|Noncentral chi-squared]] <math>\\Chi^2_k(\\lambda)</math>\n| <math>e^{\\lambda t/(1-2t)}(1 - 2t)^{-\\frac{k}{2}}</math>\n| <math>e^{i\\lambda t/(1-2it)}(1 - 2it)^{-\\frac{k}{2}}</math>\n|-\n| [[Gamma distribution|Gamma]] <math>\\Gamma(k, \\theta)</math>\n|<math>(1 - t\\theta)^{-k}, ~ \\forall t < \\tfrac{1}{\\theta}</math>\n| <math>(1 - it\\theta)^{-k}</math>\n|-\n| [[Exponential distribution|Exponential]] <math>Exp(\\lambda)</math>\n| <math>\\left(1 - t\\lambda^{-1}\\right)^{-1}, ~ t < \\lambda</math>\n| <math>\\left(1 - it\\lambda^{-1}\\right)^{-1}</math>\n|-\n| [[Multivariate normal distribution|Multivariate normal]] <math>N(\\mathbf{\\mu}, \\mathbf{\\Sigma})</math>\n|<math>e^{\\mathbf{t}^\\mathrm{T} \\left(\\boldsymbol{\\mu} + \\frac{1}{2} \\mathbf{\\Sigma t}\\right)}</math>\n|<math>e^{\\mathbf{t}^\\mathrm{T} \\left(i \\boldsymbol{\\mu} - \\frac{1}{2} \\boldsymbol{\\Sigma} \\mathbf{t}\\right)}</math>\n|-\n| [[Cauchy distribution|Cauchy]] <math>Cauchy(\\mu, \\theta)</math>\n|[[Indeterminate form|Does not exist]]\n| <math>e^{it\\mu - \\theta|t|}</math>\n|-\n|[[Multivariate Cauchy distribution|Multivariate Cauchy]] \n<math>MultiCauchy(\\mu, \\Sigma)</math><ref>Kotz et al. p. 37 using 1 as the number of degree of freedom to recover the Cauchy distribution</ref>\n|Does not exist\n|<math>\\!\\, e^{i\\mathbf{t}^{\\mathrm{T}}\\boldsymbol\\mu - \\sqrt{\\mathbf{t}^{\\mathrm{T}}\\boldsymbol{\\Sigma} \\mathbf{t}}}</math>\n|-\n|}\n\n==Calculation==\nThe moment-generating function is the expectation of a function of the random variable, it can be written as:\n\n* For a discrete [[probability mass function]], <math>M_X(t)=\\sum_{i=1}^\\infty e^{tx_i}\\, p_i</math>\n* For a continuous [[probability density function]], <math> M_X(t)  = \\int_{-\\infty}^\\infty e^{tx} f(x)\\,dx </math>\n* In the general case: <math>M_X(t) = \\int_{-\\infty}^\\infty e^{tx}\\,dF(x)</math>, using the [[Riemann&ndash;Stieltjes integral]], and  where <math>F</math> is the [[cumulative distribution function]].\n\nNote that for the case where <math>X</math> has a continuous [[probability density function]] <math>f(x)</math>,  <math>M_X(-t)</math> is the [[two-sided Laplace transform]] of <math>f(x)</math>.\n\n: <math>\n\\begin{align}\nM_X(t) & = \\int_{-\\infty}^\\infty e^{tx} f(x)\\,dx \\\\\n& = \\int_{-\\infty}^\\infty \\left( 1+ tx + \\frac{t^2x^2}{2!} + \\cdots + \\frac{t^nx^n}{n!} + \\cdots\\right) f(x)\\,dx \\\\\n& = 1 + tm_1 + \\frac{t^2m_2}{2!} +\\cdots + \\frac{t^nm_n}{n!} +\\cdots,\n\\end{align}\n</math>\n\nwhere <math>m_n</math> is the <math>n</math>th [[moment (mathematics)|moment]].\n\n===Linear combination of independent random variables===\n\nIf <math>S_n = \\sum_{i=1}^{n} a_i X_i</math>, where the ''X''<sub>''i''</sub> are independent random variables and the ''a''<sub>''i''</sub> are constants, then the probability density function for ''S''<sub>''n''</sub> is the [[convolution]] of the probability density functions of each of the ''X''<sub>''i''</sub>, and the moment-generating function for ''S''<sub>''n''</sub> is given by\n\n: <math>\nM_{S_n}(t)=M_{X_1}(a_1t)M_{X_2}(a_2t)\\cdots M_{X_n}(a_nt) \\, .\n</math>\n<!----------\nBelow was lifted from [[generating function]] ... there should be an \nanalog for the moment-generating functionbuted with common probability-generating function ''G''<sub>X</sub>, then\n\n::<math>G_{S_N}(z) = G_N(G_X(z)).</math>\n-------->\n\n===Vector-valued random variables===\nFor [[random vector|vector-valued random variables]] <math>\\mathbf X</math> with [[real number|real]] components, the moment-generating function is given by\n\n:<math> M_X(\\mathbf t) = E\\left(e^{\\langle \\mathbf t, \\mathbf X \\rangle}\\right) </math>\n\nwhere <math>\\mathbf t</math> is a vector and <math>\\langle \\cdot, \\cdot \\rangle</math> is the [[dot product]].\n\n==Important properties==\n\nMoment generating functions are positive and [[Logarithmically convex function|log-convex]], with ''M''(0) = 1.\n\nAn important property of the moment-generating function is that if two distributions have the same moment-generating function, then they are identical at [[almost all]] points.<ref name=\"Gentle\">{{cite book| author = Grimmett, Geoffrey| title = Probability - An Introduction| publisher = [[Oxford University Press]]| isbn = 978-0-19-853264-4|year=1986|page=101}}</ref> That is, if for all values of&nbsp;''t'',\n\n:<math>M_X(t) = M_Y(t),\\, </math>\n\nthen\n\n:<math>F_X(x) = F_Y(x) \\, </math>\n\nfor all values of ''x'' (or equivalently ''X'' and ''Y'' have the same distribution). This statement is not equivalent to the statement \"if two distributions have the same moments, then they are identical at all points.\" This is because in some cases, the moments exist and yet the moment-generating function does not, because the limit\n\n:<math>\\lim_{n \\rightarrow \\infty} \\sum_{i=0}^n \\frac{t^im_i}{i!}</math>\n\nmay not exist. The [[lognormal distribution]] is an example of when this occurs.\n<!--\nIf the moment generating function is defined on such an interval, then it uniquely determines a probability distribution. -->\n\n===Calculations of moments===\nThe moment-generating function is so called because if it exists on an open interval around ''t''&nbsp;=&nbsp;0, then it is the [[exponential generating function]] of the [[moment (mathematics)|moments]] of the [[probability distribution]]:\n\n:<math>m_n = E \\left( X^n \\right) = M_X^{(n)}(0) = \\left. \\frac{d^n M_X}{dt^n}\\right|_{t=0}.</math>\n\nThat is, with ''n'' being a nonnegative integer, the ''n''th moment about 0 is the ''n''th derivative of the moment generating function, evaluated at ''t'' = 0.\n\n==Other properties==\n\n[[Jensen's inequality]] provides a simple lower bound on the moment-generating function:\n:<math> M_X(t) \\geq e^{\\mu t}, </math>\nwhere <math>\\mu</math> is the mean of ''X''.\n\nUpper bounding the moment-generating function can be used in conjunction with [[Markov's inequality]] to bound the upper tail of a real random variable ''X''. This statement is also called the [[Chernoff bound]]. Since <math>x\\mapsto e^{xt}</math> is monotonically increasing for <math>t>0</math>, we have\n: <math> P(X\\ge a) = P(e^{tX}\\ge e^{ta}) \\le e^{-at}E[e^{tX}] = e^{-at}M_X(t)</math>\nfor any <math>t>0</math> and any ''a'', provided <math>M_X(t)</math> exists.  For example, when ''X'' is a standard normal distribution and <math>a>0</math>, we can choose <math>t=a</math> and recall that <math>M_X(t)=e^{t^2/2}</math>.  This gives <math>P(X\\ge a)\\le e^{-a^2/2}</math>, which is within a factor of 1+''a'' of the exact value.\n\nVarious lemmas, such as [[Hoeffding's lemma]] or [[Bennett's inequality]] provide bounds on the moment-generating function in the case of a zero-mean, bounded random variable.\n\nWhen all moments are non-negative, the moment generating function gives a simple, useful bound on the moments:\n:<math>M_X(t) \\ge \\frac{t^k}{k!} E[X^k].</math>\n\nThis can be extended to non-integer powers <math>k</math> by applying the mentioned Chernoff bound and the [[Law of the unconscious statistician]]:\n\n:<math>\\begin{align}\nE[X^k]\n&= \\int_0^\\infty k a^{k-1} \\Pr[X \\ge a] \\,d a\n\\\\&\\le \\int_0^\\infty k a^{k-1} e^{-a t} M_X(t) \\,d a\n\\\\&= \\frac{\\Gamma(k+1)}{t^{k}} M_X(t)\n.\\end{align}</math>\n\n==Relation to other functions==\nRelated to the moment-generating function are a number of other [[integral transform|transforms]] that are common in probability theory:\n\n;[[Characteristic function (probability theory)|Characteristic function]]: The [[characteristic function (probability theory)|characteristic function]] <math>\\varphi_X(t)</math> is related to the moment-generating function via <math>\\varphi_X(t) = M_{iX}(t) = M_X(it):</math> the characteristic function is the moment-generating function of ''iX'' or the moment generating function of ''X'' evaluated on the imaginary axis.  This function can also be viewed as the [[Fourier transform]] of the [[probability density function]], which can therefore be deduced from it by inverse Fourier transform.\n;[[Cumulant-generating function]]: The [[cumulant-generating function]] is defined as the logarithm of the moment-generating function; some instead define the cumulant-generating function as the logarithm of the [[Characteristic function (probability theory)|characteristic function]], while others call this latter the ''second'' cumulant-generating function.\n;[[Probability-generating function]]: The [[probability-generating function]] is defined as <math>G(z) = E\\left[z^X\\right].\\,</math> This immediately implies that <math>G(e^t) = E\\left[e^{tX}\\right] = M_X(t).\\,</math>\n\n==See also==\n* [[Entropic value at risk]]\n* [[Factorial moment generating function]]\n* [[Rate function]]\n* [[Hamburger moment problem]]\n\n{{More footnotes|date=February 2010}}\n\n== References ==\n=== Citations ===\n{{Reflist}}\n\n=== Sources ===\n{{refbegin}}\n* {{cite book |last1=Casella |first1=George |last2=Berger |first2=Roger |title=Statistical Inference |edition=2nd |ISBN = 978-0-534-24312-8 |pages=59–68 }}\n{{refend}}\n\n{{-}}\n{{Theory of probability distributions}}\n\n{{DEFAULTSORT:Moment-Generating Function}}\n[[Category:Moment (mathematics)]]\n[[Category:Generating functions]]"
    },
    {
      "title": "Probability-generating function",
      "url": "https://en.wikipedia.org/wiki/Probability-generating_function",
      "text": "In [[probability theory]], the '''probability generating function''' of a [[discrete random variable]] is a [[power series]] representation (the [[generating function]]) of the [[probability mass function]] of the [[random variable]].  Probability generating functions are often employed for their succinct description of the sequence of probabilities Pr(''X'' = ''i'') in the [[probability mass function]] for a [[random variable]] ''X'', and to make available the well-developed theory of power series with non-negative coefficients.\n\n==Definition==\n\n=== Univariate case ===\nIf ''X'' is a [[discrete random variable]] taking values in the non-negative [[integer]]s {0,1, ...}, then the ''probability generating function'' of ''X'' is defined as\n<ref>http://www.am.qub.ac.uk/users/g.gribakin/sor/Chap3.pdf</ref>\n\n:<math>G(z) = \\operatorname{E} (z^X) = \\sum_{x=0}^{\\infty}p(x)z^x,</math>\nwhere ''p'' is the [[probability mass function]] of ''X''.  Note that the subscripted notations ''G''<sub>''X''</sub> and ''p<sub>X</sub>'' are often used to emphasize that these pertain to a particular random variable ''X'', and to its [[Probability distribution|distribution]]. The power series [[absolute convergence|converges absolutely]] at least for all [[complex number]]s ''z'' with |''z''|&nbsp;≤&nbsp;1; in many examples the radius of convergence is larger.\n\n=== Multivariate case ===\nIf {{nowrap|''X'' {{=}} (''X''<sub>1</sub>,...,''X<sub>d</sub>''&thinsp;)}} is a discrete random variable taking values in the ''d''-dimensional non-negative [[integer lattice]] {0,1, ...}<sup>''d''</sup>, then the ''probability generating function'' of ''X'' is defined as\n:<math>G(z) = G(z_1,\\ldots,z_d)=\\operatorname{E}\\bigl (z_1^{X_1}\\cdots z_d^{X_d}\\bigr) = \\sum_{x_1,\\ldots,x_d=0}^{\\infty}p(x_1,\\ldots,x_d)z_1^{x_1}\\cdots z_d^{x_d},</math>\nwhere ''p'' is the probability mass function of ''X''. The power series converges absolutely at least for all complex vectors {{nowrap| ''z'' {{=}} (''z''<sub>1</sub>,...,''z<sub>d</sub>''&thinsp;) ∈ ℂ<sup>''d''</sup>}} with {{nowrap|max<nowiki>{|</nowiki>''z''<sub>1</sub><nowiki>|</nowiki>,...,<nowiki>|</nowiki>''z<sub>d</sub>''&thinsp;<nowiki>|}</nowiki> ≤ 1}}.\n\n==Properties==\n\n===Power series===\n\nProbability generating functions obey all the rules of power series with non-negative coefficients.  In particular, ''G''(1<sup>&minus;</sup>) = 1, where ''G''(1<sup>&minus;</sup>) = lim<sub>z→1</sub>''G''(''z'') [[One-sided limit|from below]], since the probabilities must sum to one. So the [[radius of convergence]] of any probability generating function must be at least 1, by [[Abel's theorem]] for power series with non-negative coefficients.\n\n===Probabilities and expectations===\n\nThe following properties allow the derivation of various basic quantities related to ''X'':\n# The probability mass function of ''X'' is recovered by taking [[derivative]]s of ''G,''<br/><!--\n-->&nbsp;&nbsp;&nbsp;<math>  p(k) = \\operatorname{Pr}(X = k) = \\frac{G^{(k)}(0)}{k!}.</math>\n# It follows from Property 1 that if random variables ''X'' and ''Y'' have probability generating functions that are equal, ''G''<sub>''X''</sub> = ''G''<sub>''Y''</sub>, then ''p''<sub>''X''</sub> = ''p''<sub>''Y''</sub>.  That is, if ''X'' and ''Y'' have identical probability generating functions, then they have identical distributions.\n# The normalization of the probability density function can be expressed in terms of the generating function by<br/><!--\n-->&nbsp;&nbsp;&nbsp;<math>\\operatorname{E}(1)=G(1^-)=\\sum_{i=0}^\\infty f(i)=1.</math><br/><!--\n-->The [[expected value|expectation]] of ''X'' is given by<br/><!--\n-->&nbsp;&nbsp;&nbsp;<math> \\operatorname{E}(X) = G'(1^-).</math><br/><!--\n-->More generally, the ''k''<sup>th</sup> [[factorial moment]], <math>\\operatorname{E}(X(X -  1) \\cdots (X - k + 1))</math> of ''X'' is given by<br/><!--\n-->&nbsp;&nbsp;&nbsp;<math>\\operatorname{E}\\left(\\frac{X!}{(X-k)!}\\right) = G^{(k)}(1^-), \\quad k \\geq 0.</math><br/><!--\n-->So the [[variance]] of ''X'' is given by<br/><!--\n-->&nbsp;&nbsp;&nbsp;<math>\\operatorname{Var}(X)=G''(1^-) + G'(1^-) - \\left [G'(1^-)\\right ]^2.</math>\n# <math>G_X(e^t) = M_X(t)</math> where ''X'' is a random variable, <math>G_X(t)</math> is the probability generating function (of ''X'') and <math>M_X(t)</math> is the [[moment-generating function]] (of ''X'') .\n\n===Functions of independent random variables===\n\nProbability generating functions are particularly useful for dealing with functions of [[statistical independence|independent]] random variables. For example:\n\n* If ''X''<sub>1</sub>, ''X''<sub>2</sub>, ..., ''X''<sub>''N''</sub> is a sequence of independent (and not necessarily identically distributed) random variables, and\n\n::<math>S_N = \\sum_{i=1}^N a_i X_i,</math>\n\n:where the ''a''<sub>''i''</sub> are constants, then the probability generating function is given by\n\n::<math>G_{S_N}(z) = \\operatorname{E}(z^{S_N}) = \\operatorname{E} \\left( z^{\\sum_{i=1}^N a_i X_i,} \\right) = G_{X_1}( z^{a_1})G_{X_2}(z^{a_2})\\cdots G_{X_N}(z^{a_N}).</math>\n\n:For example, if\n\n::<math>S_N = \\sum_{i=1}^N X_i,</math>\n\n:then the probability generating function, ''G''<sub>''S''<sub>''N''</sub></sub>(''z''), is given by\n\n::<math>G_{S_N}(z) = G_{X_1}(z)G_{X_2}(z)\\cdots G_{X_N}(z).</math>\n\n:It also follows that the probability generating function of the difference of two independent random variables ''S'' = ''X''<sub>1</sub> &minus; ''X''<sub>2</sub> is\n\n::<math>G_S(z) = G_{X_1}(z) G_{X_2}(1/z).</math>\n\n*Suppose that ''N'' is also an independent, discrete random variable taking values on the non-negative integers, with probability generating function ''G''<sub>''N''</sub>.  If the ''X''<sub>1</sub>, ''X''<sub>2</sub>, ..., ''X''<sub>''N''</sub> are independent ''and'' identically distributed with common probability generating function ''G''<sub>''X''</sub>, then\n\n::<math>G_{S_N}(z) = G_N(G_X(z)).</math>\n\n:This can be seen, using the [[law of total expectation]], as follows:\n\n::<math>\n\\begin{align}\nG_{S_N}(z) & = \\operatorname{E}(z^{S_N}) = \\operatorname{E}(z^{\\sum_{i=1}^N X_i}) \\\\[4pt]\n& = \\operatorname{E}\\big(\\operatorname{E}(z^{\\sum_{i=1}^N X_i} \\mid N) \\big) = \\operatorname{E}\\big( (G_X(z))^N\\big) =G_N(G_X(z)).\n\\end{align}\n</math>\n\n:This last fact is useful in the study of [[Galton&ndash;Watson process]]es and [[compound Poisson process]]es.\n\n*Suppose again  that ''N'' is also an independent, discrete random variable taking values on the non-negative integers, with probability generating function ''G''<sub>''N''</sub> and probability density <math>f_i = \\Pr\\{N = i\\}</math>.  If the ''X''<sub>1</sub>, ''X''<sub>2</sub>, ..., ''X''<sub>''N''</sub> are independent, but ''not'' identically distributed random variables, where <math>G_{X_i}</math> denotes the probability generating function of <math>X_i</math>, then\n\n::<math>G_{S_N}(z) = \\sum_{i \\ge 1} f_i \\prod_{k=1}^i G_{X_i}(z).</math>\n\n:For identically distributed ''X<sub>i</sub>'' this simplifies to the identity stated before. The general case is sometimes useful to obtain a decomposition of ''S<sub>N</sub>'' by means of generating functions.\n\n==Examples==\n\n* The probability generating function of a [[degenerate distribution|constant random variable]], i.e. one with Pr(''X'' = ''c'') = 1, is\n\n::<math>G(z) = z^c. </math>\n\n* The probability generating function of a [[binomial distribution|binomial random variable]], the number of successes in ''n'' trials, with probability ''p'' of success in each trial, is\n\n::<math>G(z) = \\left[(1-p) + pz\\right]^n. </math>\n\n:Note that this is the ''n''-fold product of the probability generating function of a [[Bernoulli distribution|Bernoulli random variable]] with parameter ''p''.\n:So the probability generating function of a [[fair coin]], is\n\n::<math>G(z) = 1/2 + z/2. </math>\n\n* The probability generating function of a [[negative binomial distribution|negative binomial random variable]] on {0,1,2 ...}, the number of failures until the ''r''th success with probability of success in each trial ''p'', is\n\n::<math>G(z) = \\left(\\frac{p}{1 - (1-p)z}\\right)^r.</math>\n\n:(Convergence for <math>|z| < \\frac{1}{1-p}</math>).\n\n:Note that this is the ''r''-fold product of the probability generating function of a [[geometric distribution|geometric random variable]] with parameter 1&nbsp;−&nbsp;''p'' on {0,1,2,...}.\n\n* The probability generating function of a [[Poisson distribution|Poisson random variable]] with rate parameter ''λ'' is\n\n::<math>G(z) = e^{\\lambda(z - 1)}.</math>\n<!--\nTO BE COMPLETED:\n\n==Joint probability generating functions==\n\nThe concept of the probability generating function for single random variables can be extended to the joint probability generating function of two or more random variables.\n\nSuppose that ''X'' and ''Y'' are both discrete random variables (not necessarily independent or identically distributed), again taking values on some subset of the non-negative integers. -->\n\n==Related concepts==\n\nThe probability generating function is an example of a [[generating function]] of a sequence: see also [[formal power series]]. It is equivalent to, and sometimes called, the [[z-transform]] of the probability mass function.\n\nOther generating functions of random variables include the [[moment-generating function]], the [[Characteristic function (probability theory)|characteristic function]] and the [[cumulant generating function]].\n\n{{refimprove|date=April 2012}}\n\n==Notes==\n{{reflist|refs=\n\n}}\n\n==References==\n\n*Johnson, N.L.; Kotz, S.; Kemp, A.W. (1993) ''Univariate Discrete distributions'' (2nd edition). Wiley. {{isbn|0-471-54897-9}} (Section 1.B9)\n\n{{Theory of probability distributions}}\n\n{{DEFAULTSORT:Probability Generating Function}}\n[[Category:Functions related to probability distributions]]\n[[Category:Generating functions]]"
    },
    {
      "title": "Rook polynomial",
      "url": "https://en.wikipedia.org/wiki/Rook_polynomial",
      "text": "In [[combinatorics|combinatorial]] [[mathematics]], a '''rook polynomial''' is a [[generating polynomial]] of the number of ways to place non-attacking [[rook (chess)|rooks]] on a '''board''' that looks like a [[checkerboard]]; that is, no two rooks may be in the same row or column.  The board is any subset of the squares of a rectangular board with ''m'' rows and ''n'' columns; we think of it as the squares in which one is allowed to put a rook.  The board is the ordinary [[chessboard]] if all squares are allowed and ''m'' = ''n'' = 8 and a chessboard of any size if all squares are allowed and ''m'' = ''n''.  The [[coefficient]] of ''x''<sup>&nbsp;''k''</sup> in the rook polynomial ''R''<sub>''B''</sub>(''x'') is the number of ways ''k'' rooks, none of which attacks another, can be arranged in the squares of ''B''.  The rooks are arranged in such a way that there is no pair of rooks  in the same row or column.  In this sense, an arrangement is the positioning of rooks on a static, immovable board; the arrangement will not be different if the board is rotated or reflected while keeping the squares stationary. The polynomial also remains the same if rows are interchanged or columns are interchanged.\n\nThe term \"rook polynomial\" was coined by [[John Riordan (mathematician)|John Riordan]].<ref>[[John Riordan (mathematician)|John Riordan]],  [https://books.google.com/books?id=zWgIPlds29UC ''Introduction to Combinatorial Analysis''], Princeton University Press, 1980 (originally published by John Wiley and Sons, New York; Chapman and Hall, London, 1958) {{isbn|978-0-691-02365-6}} (reprinted again in 2002, by Dover Publications). See chapters 7 & 8.</ref>\nDespite the name's derivation from [[chess]], the impetus for studying rook polynomials is their connection with counting [[permutation]]s (or [[partial permutation]]s) with restricted positions.  A board ''B'' that is a subset of the ''n'' &times; ''n'' chessboard corresponds to permutations of ''n'' objects, which we may take to be the numbers 1, 2, ..., ''n'', such that the number ''a''<sub>''j''</sub> in the ''j''-th position in the permutation must be the column number of an allowed square in row ''j'' of ''B''.  Famous examples include the number of ways to place ''n'' non-attacking rooks on:\n*an entire ''n''&nbsp;&times;&nbsp;''n'' chessboard, which is an elementary combinatorial problem;\n*the same board with its diagonal squares forbidden; this is the [[derangement]] or \"hat-check\" problem;\n*the same board without the squares on its diagonal and immediately above its diagonal (and without the bottom left square), which is essential in the solution of the [[problème des ménages]].\n\nInterest in rook placements arises in pure and applied combinatorics, [[group theory]], [[number theory]], and [[statistical physics]].  The particular value of rook polynomials comes from the utility of the generating function approach, and also from the fact that the [[zero of a function|zeroes]] of the rook polynomial of a board provide valuable information about its coefficients, i.e., the number of non-attacking placements of ''k'' rooks.\n\n== Definition ==\n\nThe '''rook polynomial''' ''R''<sub>''B''</sub>(''x'') of a board ''B'' is the [[generating function]] for the numbers of arrangements of non-attacking rooks:\n\n: <math>R_B(x)= \\sum_{k=0}^\\infty r_k(B) x^k</math>\n\nwhere ''r''<sub>''k''</sub> is the number of ways to place ''k'' non-attacking rooks on the board.  Despite the notation, this is a finite sum, since the board is finite so there is a maximum number of non-attacking rooks it can hold; indeed, there cannot be more rooks than the smaller of the number of rows and columns in the board.\n\n=== Complete boards ===\nThe first few rook polynomials on square ''n''&nbsp;×&nbsp;''n'' boards are (with ''R''<sub>''n''</sub> = ''R''<sub>''B''</sub>):\n\n: <math>\\begin{align}\n  R_1(x) & = x + 1 \\\\\n  R_2(x) & = 2 x^2 + 4 x + 1 \\\\\n  R_3(x) & = 6 x^3 + 18 x^2 + 9 x + 1 \\\\\n  R_4(x) & = 24 x^4 + 96 x^3 + 72 x^2 + 16 x + 1.\n\\end{align}</math>\n\nIn words, this means that on a 1&nbsp;×&nbsp;1 board, 1 rook can be arranged in 1 way, and zero rooks can also be arranged in 1 way (empty board); on a complete 2&nbsp;×&nbsp;2 board, 2 rooks can be arranged in 2 ways (on the diagonals), 1 rook can be arranged in 4 ways, and zero rooks can be arranged in 1 way; and so forth for larger boards.\n\nFor complete ''m''&nbsp;×&nbsp;''n'' rectangular boards ''B''<sub>''m'',''n''</sub> we write ''R<sub>m,n</sub>''&nbsp;:=&nbsp;''R''<sub>B<sub>''m'',''n''</sub></sub>&nbsp;.  The smaller of ''m'' and ''n'' can be taken as an upper limit for ''k'', since obviously ''r''<sub>''k''</sub>&nbsp;=&nbsp;0 if ''k'' &gt; min(''m'', ''n'').  This is also shown in the formula for ''R<sub>m,n</sub>''(''x'').\n\nThe rook polynomial of a rectangular chessboard is closely related to the generalized [[Laguerre polynomial]] ''L''<sub>''n''</sub><sup>''α''</sup>(''x'') by the identity\n\n: <math>R_{m,n}(x)= n! x^n L_n^{(m-n)}(-x^{-1}).</math>\n\n== Matching polynomials ==\n\nA rook polynomial is a special case of one kind of [[matching polynomial]], which is the generating function of the number of ''k''-edge [[Matching (graph theory)|matching]]s in a graph.\n\nThe rook polynomial ''R''<sub>''m'',''n''</sub>(''x'') corresponds to the [[complete bipartite graph]]&nbsp;''K''<sub>''m'',''n''</sub>&nbsp;.  The rook polynomial of a general board ''B'' ⊆ ''B''<sub>''m'',''n''</sub> corresponds to the bipartite graph with left vertices ''v''<sub>1</sub>, ''v''<sub>2</sub>, ..., ''v''<sub>''m''</sub> and right vertices ''w''<sub>1</sub>, ''w''<sub>2</sub>, ..., ''w''<sub>''n''</sub> and an edge ''v''<sub>''i''</sub>''w''<sub>''j''</sub> whenever the square (''i'',&nbsp;''j'') is allowed, i.e., belongs to ''B''.  Thus, the theory of rook polynomials is, in a sense, contained in that of matching polynomials.\n\nWe deduce an important fact about the coefficients ''r''<sub>''k''</sub>, which we recall given the number of non-attacking placements of ''k'' rooks in ''B'': these numbers are [[unimodal]], i.e., they increase to a maximum and then decrease.  This follows (by a standard argument) from the theorem of Heilmann and Lieb<ref>Ole J. Heilmann and Elliott H. Lieb, Theory of monomer-dimer systems.  ''Communications in Mathematical Physics'', Vol. 25 (1972), pp. 190–232.</ref> about the zeroes of a matching polynomial (a different one from that which corresponds to a rook polynomial, but equivalent to it under a change of variables), which implies that all the zeroes of a rook polynomial are negative real numbers.\n\n== Connection to matrix permanents ==\n\nFor incomplete square ''n''&nbsp;×&nbsp;''n'' boards, (i.e. rooks are not allowed to be played on some arbitrary subset of the board's squares) computing the number of ways to place ''n'' rooks on the board is equivalent to computing the [[Permanent (mathematics)|permanent]] of a 0–1 matrix.\n\n== Complete rectangular boards ==\n\n=== Rooks problems ===\n{{Chess diagram\n| tright\n|\n\n|  |  |  |  |  |  |  |rd\n|  |  |  |  |  |  |rd|xo\n|  |  |  |  |  |rd|xo|xo\n|  |  |  |  |rd|xo|xo|xo\n|  |  |  |rd|xo|xo|xo|xo\n|  |  |rd|xo|xo|xo|xo|xo\n|  |rd|xo|xo|xo|xo|xo|xo\n|rd|xo|xo|xo|xo|xo|xo|xo\n| '''Fig. 1.''' The maximal number of non-attacking rooks on an 8 × 8 chessboard is 8. Rook + dots mark the number of squares on a rank, available to each rook, after placing the rooks on the lower ranks.\n}}\n\nA precursor to the rook polynomial is the classic \"Eight rooks problem\" by H. E. Dudeney<ref>Dudeney, Henry E. Amusements In Mathematics. 1917. Nelson. (republished by Plain Label Books: {{isbn|1-60303-152-9}}, also as a collection of newspaper clippings, Dover Publications, 1958; Kessinger Publishing, 2006). The book can be freely downloaded from [[Project Gutenberg]] site [http://www.gutenberg.org/etext/16713]</ref> in which he shows that the maximum number of non-attacking rooks on a chessboard is eight by placing them on one of the main diagonals (Fig. 1). The question asked is: \"In how many ways can eight rooks be placed on an 8 × 8 chessboard so that neither of them attacks the other?\" The answer is: \"Obviously there must be a rook in every row and every column. Starting with the bottom row, it is clear that the first rook can be put on any one of eight different squares (Fig. 1). Wherever it is placed, there is the option of seven squares for the second rook in the second row. Then there are six squares from which to select the third row, five in the fourth, and so on. Therefore the number of different ways must be 8 × 7 × 6 × 5 × 4 × 3 × 2 × 1 = 40,320\" (that is, 8<nowiki>!</nowiki>, where \"!\" is [[factorial]]).<ref>Dudeney, Problem 295</ref>\n\nThe same result can be obtained in a slightly different way. Let us endow each rook with a positional number, corresponding to the number of its rank, and assign it a name that corresponds to the name of its file. Thus, rook a1 has position 1 and name \"a\", rook b2 has position 2 and name \"b\", etc. Then let us order the rooks into an ordered list ([[sequence]]) by their positions. The diagram on Fig. 1 will then transform in the sequence (a,b,c,d,e,f,g,h). Placing any rook on another file would involve moving the rook that hitherto occupied the second file to the file, vacated by the first rook. For instance, if rook a1 is moved to \"b\" file, rook b2 must be moved to \"a\" file, and now they will become rook b1 and rook a2. The new sequence will become (b,a,c,d,e,f,g,h). In combinatorics, this operation is termed [[permutation]], and the sequences, obtained as a result of the permutation, are permutations of the given sequence. The total number of permutations, containing 8 elements from a sequence of 8 elements is 8! ([[factorial]] of 8).\n\nTo assess the effect of the imposed limitation \"rooks must not attack each other\", consider the problem without such limitation. In how many ways can eight rooks be placed on an 8 × 8 chessboard? This will be the total number of [[combination]]s of 8 rooks on 64 squares:\n\n:<math> {64 \\choose 8} = \\frac{64!}{8!(64-8)!} = 4,426,165,368.</math>\n\nThus, the limitation \"rooks must not attack each other\" reduces the total number of allowable positions from combinations to permutations which is a factor of about 109,776.\n\nA number of problems from different spheres of human activity can be reduced to the rook problem by giving them a \"rook formulation\". As an example: A company must employ ''n'' workers on ''n'' different jobs and each job must be carried out only by one worker. In how many ways can this appointment be done?\n\nLet us put the workers on the ranks of the ''n''&nbsp;&times;&nbsp;''n'' chessboard, and the jobs − on the files. If worker ''i'' is appointed to job ''j'', a rook is placed on the square where rank ''i'' crosses file ''j''. Since each job is carried out only by one worker and each worker is appointed to only one job, all files and ranks will contain only one rook as a result of the arrangement of ''n'' rooks on the board, that is, the rooks do not attack each other.\n\n=== The rook polynomial as a generalization of the rooks problem ===\n\nThe classical rooks problem immediately gives the value of ''r''<sub>8</sub>, the coefficient in front of the highest order term of the rook polynomial. Indeed, its result is that 8 non-attacking rooks can be arranged on an 8 × 8 chessboard in ''r''<sub>8</sub> = 8! = 40320 ways.\n\nLet us generalize this problem by considering an ''m'' × ''n'' board, that is, a board with ''m'' ranks (rows) and ''n'' files (columns). The problem becomes: In how many ways can one arrange ''k'' rooks on an ''m'' × ''n'' board in such a way that they do not attack each other?\n\nIt is clear that for the problem to be solvable, ''k'' must be less or equal to the smaller of the numbers ''m'' and ''n''; otherwise one cannot avoid placing a pair of rooks on a rank or on a file. Let this condition be fulfilled. Then the arrangement of rooks can be carried out in two steps. First, choose the set of ''k'' ranks on which to place the rooks. Since the number of ranks is ''m'', of which ''k'' must be chosen, this choice can be done in <math>\\binom{m}{k}</math> ways. Similarly, the set of ''k'' files on which to place the rooks can be chosen in <math>\\binom{n}{k}</math> ways. Because the choice of files does not depend on the choice of ranks, according to the products rule there are <math>\\binom{m}{k}\\binom{n}{k}</math> ways to choose the square on which to place the rook.\n\nHowever, the task is not yet finished because ''k'' ranks and ''k'' files intersect in ''k''<sup>2</sup> squares. By deleting unused ranks and files and compacting the remaining ranks and files together, one obtains a new board of ''k'' ranks and ''k'' files. It was already shown that on such board ''k'' rooks can be arranged in ''k''! ways (so that they do not attack each other). Therefore, the total number of possible non-attacking rooks arrangements is:<ref>Vilenkin, Naum Ya. Combinatorics (Kombinatorika). 1969. Nauka Publishers, Moscow (In Russian).</ref>\n\n:<math>r_k = \\binom{m}{k}\\binom{n}{k} k! = \\frac{n! m!}{k! (n-k)! (m-k)!}.</math>\n\nFor instance, 3 rooks can be placed on a conventional chessboard (8 × 8) in <math>\\textstyle{\\frac{8! 8!}{3!5!5!}} = 18,816</math> ways. For ''k'' = ''m'' = ''n'', the above formula gives ''r<sub>k</sub>'' = ''n''! that corresponds to the result obtained for the classical rooks problem.\n\nThe rook polynomial with explicit coefficients is now:\n\n:<math>R_{m,n}(x) = \\sum_{k=0}^{\\min(m,n)} \\binom{m}{k} \\binom{n}{k} k! x^k = \\sum_{k=0}^{\\min(m,n)}\\frac{n! m!}{k! (n-k)! (m-k)!} x^k.</math>\n\nIf the limitation \"rooks must not attack each other\" is removed, one must choose any ''k'' squares from ''m'' × ''n'' squares. This can be done in:\n\n:<math>\\binom{mn}{k} = \\frac{(mn)!}{k! (mn-k)!}</math>  ways.\n\nIf the ''k'' rooks differ in some way from each other, e.g., they are labelled or numbered, all the results obtained so far must be multiplied by ''k''!, the number of permutations of ''k'' rooks.\n\n=== Symmetric arrangements ===\nAs a further complication to the rooks problem, let us require that rooks not only be non-attacking but also symmetrically arranged on the board. Depending on the type of symmetry, this is equivalent to rotating or reflecting the board. \nSymmetric arrangements lead to many problems, depending on the symmetry condition.<ref>Vilenkin, Naum Ya. Popular Combinatorics (Populyarnaya kombinatorika). 1975. Nauka Publishers, Moscow (In Russian).</ref><ref>Gik, Evgeny Ya. Mathematics on the Chessboard (Matematika na shakhmatnoy doske). 1976. Nauka Publishers, Moscow (In Russian).</ref><ref>Gik, Evgeny Ya. Chess and Mathematics (Shakhmaty i matematika). 1983. Nauka Publishers, Moscow (In Russian). {{isbn|3-87144-987-3}} ([http://gso.gbv.de GVK-Gemeinsamer Verbundkatalog])</ref><ref>Kokhas', Konstantin P. Rook Numbers and Polynomials (Ladeynye chisla i mnogochleny). MCNMO, Moscow, 2003 (in Russian). {{isbn|5-94057-114-X}} {{url|https://www.mccme.ru/free-books/mmmf-lectures/book.26.pdf}} ([http://gso.gbv.de GVK-Gemeinsamer Verbundkatalog])</ref>\n{{Chess diagram\n| tright\n|\n\n|  |rd|  |  |  |  |  |  \n|  |  |  |  |  |  |  |rd\n|  |  |  |  |rd|  |  |  \n|  |  |rd|xo|xo|  |  |  \n|  |  |  |xo|xo|rd|  |  \n|  |  |  |rd|  |  |  |  \n|rd|  |  |  |  |  |  |  \n|  |  |  |  |  |  |rd|  \n| '''Fig. 2.''' A symmetric arrangement of non-attacking rooks about the centre of an 8 × 8 chessboard. Dots mark the 4 central squares that surround the centre of symmetry.\n}}\nThe simplest of those arrangements is when rooks are symmetric about the centre of the board. Let us designate with ''G<sub>n</sub>'' the number of arrangements in which ''n'' rooks are placed on a board with ''n'' ranks and ''n'' files. Now let us make the board to contain 2''n'' ranks and 2''n'' files. A rook on the first file can be placed on any of the 2''n'' squares of that file. According to the symmetry condition, placement of this rook defines the placement of the rook that stands on the last file − it must be arranged symmetrically to the first rook about the board centre. Let us remove the first and the last files and the ranks that are occupied by rooks (since the number of ranks is even, the removed rooks cannot stand on the same rank). This will give a board of 2''n''&nbsp;−&nbsp;2 files and 2''n''&nbsp;−&nbsp;2 ranks. It is clear that to each symmetric arrangement of rooks on the new board corresponds a symmetric arrangement of rooks on the original board. Therefore, ''G''<sub>2''n''</sub> = 2''nG''<sub>2''n''&nbsp;−&nbsp;2</sub> (the factor 2''n'' in this expression comes from the possibility for the first rook to occupy any of the 2''n'' squares on the first file). By iterating the above formula one reaches to the case of a 2&nbsp;&times;&nbsp;2 board, on which there are 2 symmetric arrangements (on the diagonals). As a result of this iteration, the final expression is ''G''<sub>2''n''</sub> = 2<sup>''n''</sup>''n''! For the usual chessboard (8&nbsp;&times;&nbsp;8), ''G''<sub>8</sub> = 2<sup>4</sup>&nbsp;&times;&nbsp;4! = 16&nbsp;&times;&nbsp;24 = 384 centrally symmetric arrangements of 8 rooks. One such arrangement is shown in Fig. 2.\n\nFor odd-sized boards (containing 2''n''&nbsp;+&nbsp;1 ranks and 2''n''&nbsp;+&nbsp;1 files) there is always a square that does not have its symmetric double − this is the central square of the board. There must always be a rook placed on this square.  Removing the central file and rank, one obtains a symmetric arrangement of 2''n'' rooks on a 2''n''&nbsp;&times;&nbsp;2''n'' board. Therefore, for such board, once again ''G''<sub>2''n''&nbsp;+&nbsp;1</sub> = ''G''<sub>2''n''</sub> = 2<sup>''n''</sup>''n''!\n\nA little more complicated problem is to find the number of non-attacking arrangements that do not change upon 90° rotation of the board. Let the board have 4''n'' files and 4''n'' ranks, and the number of rooks is also 4''n''. In this case, the rook that is on the first file can occupy any square on this file, except the corner squares (a rook cannot be on a corner square because after a 90° rotation there would 2 rooks that attack each other). There are another 3 rooks that correspond to that rook and they stand, respectively, on the last rank, the last file, and the first rank (they are obtained from the first rook by 90°, 180°, and 270° rotations). Removing the files and ranks of those rooks, one obtains the rook arrangements for a (4''n''&nbsp;−&nbsp;4)&nbsp;&times;&nbsp;(4''n''&nbsp;−&nbsp;4) board with the required symmetry. Thus, the following [[recurrence relation]] is obtained: ''R''<sub>4''n''</sub> = (4''n''&nbsp;−&nbsp;2)''R''<sub>4''n''&nbsp;−&nbsp;4</sub>, where ''R<sub>n</sub>'' is the number of arrangements for a ''n''&nbsp;&times;&nbsp;''n'' board. Iterating, it follows that ''R''<sub>4''n''</sub> = 2''n''(2''n''&nbsp;−&nbsp;1)(2''n''&nbsp;−&nbsp;3)...1. The number of arrangements for a (4''n''&nbsp;+&nbsp;1)&nbsp;&times;&nbsp;(4''n''&nbsp;+&nbsp;1) board is the same as that for a 4''n''&nbsp;&times;&nbsp;4''n'' board; this is because on a (4''n''&nbsp;+&nbsp;1)&nbsp;&times;&nbsp;(4''n''&nbsp;+&nbsp;1) board, one rook must necessarily stand in the centre and thus the central rank and file can be removed. Therefore ''R''<sub>4''n''&nbsp;+&nbsp;1</sub> = ''R''<sub>4''n''</sub>. For the traditional chessboard (''n'' = 2), ''R''<sub>8</sub> = 4&nbsp;&times;&nbsp;3&nbsp;&times;&nbsp;1 = 12 possible arrangements with rotational symmetry.\n\nFor (4''n''&nbsp;+&nbsp;2)&nbsp;&times;&nbsp;(4''n''&nbsp;+&nbsp;2) and (4''n''&nbsp;+&nbsp;3)&nbsp;&times;&nbsp;(4''n''&nbsp;+&nbsp;3) boards, the number of solutions is zero. Two cases are possible for each rook: either it stands in the centre or it doesn't stand in the centre. In the second case, this rook is included in the rook quartet that exchanges squares on turning the board at 90°. Therefore, the total number of rooks must be either 4''n'' (when there is no central square on the board) or 4''n''&nbsp;+&nbsp;1. This proves that ''R''<sub>4''n''&nbsp;+&nbsp;2</sub> = ''R''<sub>4''n''&nbsp;+&nbsp;3</sub> = 0.\n\nThe number of arrangements of ''n'' non-attacking rooks symmetric to one of the diagonals (for determinacy, the diagonal corresponding to a1–h8 on the chessboard) on a ''n''&nbsp;&times;&nbsp;''n'' board is given by the [[Telephone number (mathematics)|telephone numbers]] defined by the recurrence ''Q''<sub>''n''</sub> = ''Q''<sub>''n''&nbsp;−&nbsp;1</sub> + (''n''&nbsp;−&nbsp;1)''Q''<sub>''n''&nbsp;−&nbsp;2</sub>. This recurrence is derived in the following way. Note that the rook on the first file either stands on the bottom corner square or it stands on another square. In the first case, removal of the first file and the first rank leads to the symmetric arrangement ''n''&nbsp;−&nbsp;1 rooks on a (''n''&nbsp;−&nbsp;1)&nbsp;&times;&nbsp;(''n''&nbsp;−&nbsp;1) board. The number of such arrangements is ''Q''<sub>''n''&nbsp;−&nbsp;1</sub>. In the second case, for the original rook there is another rook, symmetric to the first one about the chosen diagonal. Removing the files and ranks of those rooks leads to a symmetric arrangement ''n''&nbsp;−&nbsp;2 rooks on a (''n''&nbsp;−&nbsp;2)&nbsp;&times;&nbsp;(''n''&nbsp;−&nbsp;2) board. Since the number of such arrangements is ''Q''<sub>''n''&nbsp;−&nbsp;2</sub> and the rook can be put on the ''n''&nbsp;−&nbsp;1 square of the first file, there are (''n''&nbsp;−&nbsp;1)''Q''<sub>''n''&nbsp;−&nbsp;2</sub> ways for doing this, which immediately gives the above recurrence. The number of diagonal-symmetric arrangements is then given by the expression:\n\n:<math>Q_n = 1 + \\binom{n}{2} + \\frac{1}{1 \\times 2}\\binom{n}{2}\\binom{n-2}{2} + \\frac{1}{1 \\times 2 \\times 3}\\binom{n}{2}\\binom{n-2}{2}\\binom{n-4}{2} + \\cdots.</math>\n\nThis expression is derived by partitioning all rook arrangements in classes; in class ''s'' are those arrangements in which ''s'' pairs of rooks do not stand on the diagonal. In exactly the same way, it can be shown that the number of ''n''-rook arrangements on a ''n''&nbsp;×&nbsp;''n'' board, such that they do not attack each other and are symmetric to both diagonals is given by the recurrence equations ''B''<sub>2''n''</sub> = 2''B''<sub>2''n''&nbsp;−&nbsp;2</sub> + (2''n''&nbsp;−&nbsp;2)''B''<sub>2''n''&nbsp;−&nbsp;4</sub> and ''B''<sub>2''n''&nbsp;+&nbsp;1</sub> = ''B''<sub>2''n''</sub>.\n\n=== Arrangements counted by symmetry classes ===\n\nA different type of generalization is that in which rook arrangements that are obtained from each other by symmetries of the board are counted as one.  For instance, if rotating the board by 90 degrees is allowed as a symmetry, then any arrangement obtained by a rotation of 90, 180, or 270 degrees is considered to be \"the same\" as the original pattern, even though these arrangements are counted separately in the original problem where the board is fixed.  For such problems, Dudeney<ref>Dudeney, Answer to Problem 295</ref> observes: \"How many ways there are if mere reversals and reflections are not counted as different has not yet been determined; it is a difficult problem.\" The problem reduces to that of counting symmetric arrangements via [[Burnside's lemma]].\n\n== References ==\n<references />\n\n{{DEFAULTSORT:Rook Polynomial}}\n[[Category:Generating functions]]\n[[Category:Enumerative combinatorics]]\n[[Category:Polynomials]]\n[[Category:Mathematical chess problems]]\n[[Category:Factorial and binomial topics]]\n[[Category:Permutations]]\n[[Category:Orthogonal polynomials]]"
    },
    {
      "title": "Weisner's method",
      "url": "https://en.wikipedia.org/wiki/Weisner%27s_method",
      "text": "In [[mathematics]], '''Weisner's method''' is a method for finding [[generating function]]s for [[special function]]s using [[representation theory]] of [[Lie group]]s and [[Lie algebra]]s, introduced by {{harvtxt|Weisner|1955}}. It includes [[Truesdell's method]] as a special case, and is essentially the same as [[Rainville's method]].\n\n==References==\n\n*{{Citation | last1=McBride | first1=Elna Browning | title=Obtaining generating functions | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Springer Tracts in Natural Philosophy | isbn=978-0-387-05255-7 | mr=0279355 | year=1971 | volume=21}}\n*{{Citation | last1=Weisner | first1=Louis | title=Group-theoretic origin of certain generating functions | url=http://projecteuclid.org/euclid.pjm/1172000968 | mr=0086905 | year=1955 | journal=[[Pacific Journal of Mathematics]] | issn=0030-8730 | volume=5 | pages=1033–1039 | doi=10.2140/pjm.1955.5.1033}}\n\n[[Category:Generating functions]]\n\n{{math-stub}}"
    },
    {
      "title": "Cauchy distribution",
      "url": "https://en.wikipedia.org/wiki/Cauchy_distribution",
      "text": "{{distinguish-redirect|Lorentz distribution|Lorenz curve|Lorenz equation}}\n{{Probability distribution\n | name       =Cauchy\n | type       =density\n | box_width  =300px\n | pdf_image  =[[File:cauchy pdf.svg|300px|Probability density function for the Cauchy distribution]]<br><small>The purple curve is the standard Cauchy distribution</small>\n | cdf_image  =[[File:cauchy cdf.svg|300px|Cumulative distribution function for the Cauchy distribution]]\n | parameters =<math>x_0\\!</math> [[location parameter|location]] ([[real number|real]])<br><math>\\gamma > 0</math> [[scale parameter|scale]] (real)\n | support    =<math>\\displaystyle x \\in (-\\infty, +\\infty)\\!</math>\n | pdf        =<math>\\frac{1}{\\pi\\gamma\\,\\left[1 + \\left(\\frac{x-x_0}{\\gamma}\\right)^2\\right]}\\!</math>\n | cdf        =<math>\\frac{1}{\\pi} \\arctan\\left(\\frac{x-x_0}{\\gamma}\\right)+\\frac{1}{2}\\!</math>\n | quantile   = <math>x_0+\\gamma\\,\\tan[\\pi(F-\\tfrac{1}{2})]</math>\n | qdf        =<math>\\gamma\\,\\pi\\,\\sec\\^2(\\pi\\,(p-\\tfrac{1}{2}))\\!</math>\n | mean       =[[indeterminate form|undefined]]\n | median     =<math>x_0\\!</math>\n | mode       =<math>x_0\\!</math>\n | variance   =[[indeterminate form|undefined]]\n | skewness   =[[indeterminate form|undefined]]\n | kurtosis   =[[indeterminate form|undefined]]\n | entropy    =<math>\\log(4\\pi\\gamma)\\!</math>\n | mgf        =does not exist\n | char       =<math>\\displaystyle \\exp(x_0\\,i\\,t-\\gamma\\,|t|)\\!</math>\n}}\nThe '''Cauchy distribution''', named after [[Augustin Cauchy]], is a [[continuous probability distribution]].  It is also known, especially among [[physicist]]s, as the '''Lorentz distribution''' (after [[Hendrik Lorentz]]), '''Cauchy–Lorentz distribution''', '''Lorentz(ian) function''', or '''Breit–Wigner distribution'''.  The Cauchy distribution <math>f(x; x_0,\\gamma)</math> is the distribution of the {{mvar|x}}-intercept of a ray issuing from <math>(x_0,\\gamma)</math> with a uniformly distributed angle.  It is also the distribution of the [[Ratio distribution|ratio of two]] independent [[Normal Distribution | normally distributed]] random variables if the denominator distribution has mean zero.\n\nThe Cauchy distribution is often used in statistics as the canonical example of a \"[[pathological (mathematics)|pathological]]\" distribution since both its [[expected value]] and its [[variance]] are undefined. (But see the section ''[[#Explanation of undefined moments|Explanation of undefined moments]]'' below.) The Cauchy distribution does not have finite [[moment (mathematics)|moment]]s of order greater than or equal to one; only fractional absolute moments exist.<ref name=jkb1>{{cite book|author1=N. L. Johnson |author2=S. Kotz |author3=N. Balakrishnan |title=Continuous Univariate Distributions, Volume 1|publisher=Wiley|location=New York|year=1994|ref=harv}}, Chapter 16.</ref> The Cauchy distribution has no [[moment generating function]].\n\nIn [[mathematics]], it is closely related to the [[Poisson kernel]], which is the [[fundamental solution]] for the [[Laplace equation]] in the [[upper half-plane]].  In [[spectroscopy]], it is the description of the shape of [[spectral line]]s which are subject to [[homogeneous broadening]] in which all atoms interact in the same way with the frequency range contained in the line shape. Many mechanisms cause homogeneous broadening, most notably [[Line broadening#Pressure broadening|collision broadening]].<ref>{{cite book |author=E. Hecht |year=1987 |title=Optics |page=603 |edition=2nd |publisher=[[Addison-Wesley]] |isbn=}}</ref>\n\nIt is one of the few distributions that is [[stable distribution|stable]] and has a probability density function that can be expressed analytically, the others being the [[normal distribution]] and the [[Lévy distribution]].\n\n==History==\nFunctions with the form of the density function of the Cauchy distribution were studied by mathematicians in the 17th century, but in a different context and under the title of the [[witch of Agnesi]]. Despite its name, the first explicit analysis of the properties of the Cauchy distribution was published by the French mathematician [[Siméon Denis Poisson|Poisson]] in 1824, with Cauchy only becoming associated with it during an academic controversy in 1853.<ref>Cauchy and the Witch of Agnesi in ''Statistics on the Table'', S M Stigler Harvard 1999 Chapter 18</ref> As such, the name of the distribution is a case of [[Stigler's law of eponymy|Stigler's Law of Eponymy]]. Poisson noted that if the mean of observations following such a distribution were taken, the mean error did not converge to any finite number. As such, [[Pierre-Simon Laplace|Laplace's]] use of the [[Central Limit Theorem]] with such a distribution was inappropriate, as it assumed a finite mean and variance. Despite this, Poisson did not regard the issue as important, in contrast to [[Irénée-Jules Bienaymé|Bienaymé]], who was to engage Cauchy in a long dispute over the matter.\n\n==Characterisation==\n\n===Probability density function===\nThe Cauchy distribution has the [[probability density function]] (PDF)<ref name=jkb1/><ref name=feller>{{cite book|last=Feller|first=William|title=An Introduction to Probability Theory and Its Applications, Volume II|edition=2|publisher=John Wiley & Sons Inc.|location=New York|year=1971|pages=704|isbn=978-0-471-25709-7}}</ref>\n:<math>f(x; x_0,\\gamma) = \\frac{1}{\\pi\\gamma \\left[1 + \\left(\\frac{x - x_0}{\\gamma}\\right)^2\\right]} = { 1 \\over \\pi \\gamma } \\left[ { \\gamma^2 \\over (x - x_0)^2 + \\gamma^2  } \\right], </math>\n\nwhere <math>x_0</math> is the [[location parameter]], specifying the location of the peak of the distribution, and <math>\\gamma</math> is the [[scale parameter]] which specifies the half-width at half-maximum (HWHM), alternatively <math>2\\gamma</math> is [[full width at half maximum]] (FWHM). <math>\\gamma</math> is also equal to half the [[interquartile range]] and is sometimes called the [[probable error]]. [[Augustin-Louis Cauchy]] exploited such a density function in 1827 with an [[infinitesimal]] scale parameter, defining what would now be called a [[Dirac delta function]].\n\nThe maximum value or amplitude of the Cauchy PDF is <math>\\frac{1}{\\pi \\gamma}</math>, located at <math>x=x_0</math>.\n\nIt is sometimes convenient to express the PDF  in terms of the complex parameter <math>\\psi= x_0 + i\\gamma</math>\n\n:<math>\nf(x;\\psi)=\\frac{1}{\\pi}\\,\\textrm{Im}\\left(\\frac{1}{x-\\psi}\\right)=\\frac{1}{\\pi}\\,\\textrm{Re}\\left(\\frac{-i}{x-\\psi}\\right)\n</math>\n\nThe special case when <math>x_0 = 0</math> and <math>\\gamma = 1</math> is called the '''standard Cauchy distribution''' with the probability density function<ref name=mathmethods>{{cite book|last1=Riley|first1=Ken F.|last2=Hobson|first2=Michael P.|last3=Bence|first3=Stephen J.|title=Mathematical Methods for Physics and Engineering|edition=3|publisher=Cambridge University Press|location=Cambridge, UK|year=2006|pages=1333|isbn=978-0-511-16842-0}}</ref><ref name=primer>{{cite book|last1=Balakrishnan|first1=N.|last2=Nevrozov|first2=V. B.|title=A Primer on Statistical Distributions|edition=1|publisher=John Wiley & Sons Inc.|location=Hoboken, New Jersey|year=2003|pages=305|isbn=0-471-42798-5}}</ref>\n:<math> f(x; 0,1) = \\frac{1}{\\pi (1 + x^2)}. \\!</math>\n\nIn physics, a three-parameter Lorentzian function is often used:\n:<math>f(x; x_0,\\gamma,I) = \\frac{I}{\\left[1 + \\left(\\frac{x-x_0}{\\gamma}\\right)^2\\right]} = I \\left[ { \\gamma^2 \\over (x - x_0)^2 + \\gamma^2  } \\right], </math>\nwhere <math>I</math> is the height of the peak. The three-parameter Lorentzian function indicated is not, in general, a probability density function, since it does not integrate to 1, except in the special case where  <math>I = \\frac{1}{\\pi\\gamma}.\\!</math>\n\n===Cumulative distribution function===\nThe [[cumulative distribution function]] of the Cauchy distribution is:\n:<math>F(x; x_0,\\gamma)=\\frac{1}{\\pi} \\arctan\\left(\\frac{x-x_0}{\\gamma}\\right)+\\frac{1}{2}</math>\n\nand the [[quantile function]] (inverse [[cumulative distribution function|cdf]]) of the Cauchy distribution is\n:<math>Q(p; x_0,\\gamma) = x_0 + \\gamma\\,\\tan\\left[\\pi\\left(p-\\tfrac{1}{2}\\right)\\right].</math>\nIt follows that the first and third quartiles are <math>(x_0 - \\gamma, x_0 + \\gamma)</math>, and hence the [[interquartile range]] is <math>2\\gamma</math>.\n\nFor the standard distribution, the cumulative distribution function simplifies to [[Inverse trigonometric functions|arctangent function]] <math>\\arctan(x)</math>:\n:<math>F(x; 0,1)=\\frac{1}{\\pi} \\arctan\\left(x\\right)+\\frac{1}{2}</math>\n\n=== Entropy ===\n\nThe entropy of the Cauchy distribution is given by:\n\n: <math>\n\\begin{align}\nH(\\gamma) & =-\\int_{-\\infty}^\\infty f(x;x_0,\\gamma) \\log(f(x;x_0,\\gamma)) \\, dx \\\\[6pt]\n& =\\log(4\\pi\\gamma)\n\\end{align}\n</math>\n\nThe derivative of the [[quantile function]], the quantile density function, for the Cauchy distribution is:\n\n:<math>Q'(p; \\gamma) = \\gamma\\,\\pi\\,{\\sec}^2\\left[\\pi\\left(p-\\tfrac 1 2 \\right)\\right].\\!</math>\n\nThe [[differential entropy]] of a distribution can be defined in terms of its quantile density,<ref>{{cite journal |last1=Vasicek  |first1=Oldrich |year=1976 |title=A Test for Normality Based on Sample Entropy |journal=Journal of the Royal Statistical Society, Series B |volume=38 |issue=1 |pages=54–59 }}</ref> specifically:\n\n:<math>H(\\gamma) = \\int_0^1 \\log\\,(Q'(p; \\gamma))\\,\\mathrm dp = \\log(4\\pi\\gamma)</math>\n\nThe Cauchy distribution is the [[maximum entropy probability distribution]] for a random variate <math>X</math> for which \n\n:<math>\\operatorname{E}[\\log(1+(X-x_0)^2/\\gamma^2)]=\\log 4</math>\n\nor, alternatively, for a random variate <math>X</math> for which \n\n:<math>\\operatorname{E}[\\log(1+(X-x_0)^2)]=2\\log(1+\\gamma).</math>\n\nIn its standard form, it is the [[maximum entropy probability distribution]] for a random variate <math>X</math> for which<ref>{{cite journal |last1=Park |first1=Sung Y. |last2=Bera |first2=Anil K. |year=2009 |title=Maximum entropy autoregressive conditional heteroskedasticity model |journal=Journal of Econometrics |volume= |issue= |pages=219–230 |publisher=Elsevier |doi= |url=http://www.econ.yorku.ca/cesg/papers/berapark.pdf |accessdate=2011-06-02 |deadurl=yes |archiveurl=https://web.archive.org/web/20110930062639/http://www.econ.yorku.ca/cesg/papers/berapark.pdf |archivedate=2011-09-30 |df= }}</ref> \n\n:<math>\\operatorname{E}\\!\\left[\\ln(1+X^2) \\right]=\\ln 4.</math>\n\n==Properties==\nThe Cauchy distribution is an example of a distribution which has no [[mean]], [[variance]] or higher [[moment (mathematics)|moments]] defined. Its [[mode (statistics)|mode]] and [[median]] are well defined and are both equal to <math>x_0</math>.\n\nWhen <math>U</math> and <math>V</math> are two independent [[normal distribution|normally distributed]] [[random variable]]s with [[expected value]] 0 and [[variance]] 1, then the ratio <math>U/V</math> has the standard Cauchy distribution.\n\nIf <math>\\Sigma</math> is a <math>p\\times p</math> positive-semidefinite covariance matrix with strictly positive diagonal entries, then for [[Independent and identically distributed random variables|independent and identically distributed]] <math>X,Y\\sim N(0,\\Sigma)</math> and any random <math>p</math>-vector <math>w</math> independent of <math>X</math> and <math>Y</math> such that <math>w_1+\\cdots+w_p=1</math> and <math>w_i\\geq 0, i=1,\\ldots,p,</math> (defining a  [[categorical distribution]]) it holds that\n\n:<math>\\sum_{j=1}^p w_j\\frac{X_j}{Y_j}\\sim\\mathrm{Cauchy}(0,1).</math><ref name=\":0\">{{Cite journal|authors=Pillai N. and Meng, X.L.|date=|year=2016|title=An unexpected encounter with Cauchy and Lévy|url=https://arxiv.org/pdf/1505.01957.pdf|journal=[[The Annals of Statistics]]|volume=44|issue=5|pages=2089-2097|doi=|jstor=|via=}}</ref>\n\nIf <math>X_1, \\ldots, X_n</math> are [[independent and identically distributed]] random variables, each with a standard Cauchy distribution, then the [[Arithmetic mean|sample mean]] <math>(X_1 + \\cdots + X_n)/n</math> has the same standard Cauchy distribution. To see that this is true, compute the [[Characteristic function (probability theory)|characteristic function]] of the sample mean:\n:<math>\\varphi_{\\overline{X}}(t) = \\mathrm{E}\\left[e^{i\\overline{X}t}\\right]</math>\n\nwhere <math>\\overline{X}</math> is the sample mean. This example serves to show that the hypothesis of finite variance in the [[central limit theorem]] cannot be dropped. It is also an example of a more generalized version of the central limit theorem that is characteristic of all [[stable distribution]]s, of which the Cauchy distribution is a special case.\n\nThe Cauchy distribution is an [[infinitely divisible probability distribution]]. It is also a strictly [[stability (probability)|stable]] distribution.<ref>{{cite book |authors=Campbell B. Read, N. Balakrishnan, Brani Vidakovic and Samuel Kotz |year=2006 |title=[[Encyclopedia of Statistical Sciences]] |page=778 |edition=2nd |publisher=[[John Wiley & Sons]] |isbn=978-0-471-15044-2}}</ref>\n\nThe standard Cauchy distribution coincides with the [[Student's t-distribution|Student's ''t''-distribution]] with one degree of freedom.\n\nLike all stable distributions, the [[location-scale family]] to which the Cauchy distribution belongs is closed under [[linear transformations]] with [[real number|real]] coefficients. In addition, the Cauchy distribution is closed under [[Möbius transformation|linear fractional transformations]] with real coefficients.<ref>{{cite journal|author=F. B. Knight|title=A characterization of the Cauchy type|journal=Proceedings of the American Mathematical Society|volume = 55|year = 1976|pages= 130–135|ref=harv|doi=10.2307/2041858|jstor=2041858}}</ref> In this connection, see also [[McCullagh's parametrization of the Cauchy distributions]].\n\n===Characteristic function===\nLet <math>X</math> denote a Cauchy distributed random variable. The [[Characteristic function (probability theory)|characteristic function]] of the Cauchy distribution is given by\n\n:<math>\\varphi_X(t; x_0,\\gamma) = \\operatorname{E}\\left[e^{iXt} \\right ] =\\int_{-\\infty}^\\infty f(x;x_0,\\gamma)e^{ixt}\\,dx =  e^{ix_0t - \\gamma |t|}.</math>\n\nwhich is just the [[Fourier transform]] of the probability density. The original probability density may be expressed in terms of the characteristic function, essentially by using the inverse Fourier transform:\n\n:<math>f(x; x_0,\\gamma) = \\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\varphi_X(t;x_0,\\gamma)e^{-ixt} \\, dt \\!</math>\n\nThe ''n''th moment of a distribution is the ''n''th derivative of the characteristic function evaluated at <math>t=0</math>. Observe that the characteristic function is not [[Differentiable function|differentiable]] at the origin: this corresponds to the fact that the Cauchy distribution does not have well-defined moments higher than the zeroth moment.\n\n==Explanation of undefined moments==\n\n===Mean===\nIf a [[probability distribution]] has a [[probability density function|density function]] <math>f(x)</math>, then the mean, if it exists, is given by\n:<math>\\int_{-\\infty}^\\infty x f(x)\\,dx. \\qquad\\qquad (1)\\!</math>\n\nWe may evaluate this two-sided [[improper integral]] by computing the sum of two one-sided improper integrals. That is,\n:<math>\\int_a^\\infty x f(x)\\,dx+\\int_{-\\infty}^a x f(x)\\,dx.\\qquad\\qquad (2) \\!</math>\nfor an arbitrary real number <math>a</math>.\n\nFor the integral to exist (even as an infinite value), at least one of the terms in this sum should be finite, or both should be infinite and have the same sign. But in the case of the Cauchy distribution, both the terms in this sum (2) are infinite and have opposite sign. Hence (1) is undefined, and thus so is the mean.<ref name=\"uah\">{{cite web | url=http://www.math.uah.edu/stat/special/Cauchy.html | title=Cauchy Distribution | publisher=[[University of Alabama]] at Huntsville | work=Virtual Laboratories | accessdate=September 19, 2018}}</ref>\n\nNote that the  [[Cauchy principal value]] of the mean of the Cauchy distribution is\n:<math>\\lim_{a\\to\\infty}\\int_{-a}^a x f(x)\\,dx, \\!</math>\n\nwhich is zero. On the other hand, the related integral\n:<math>\\lim_{a\\to\\infty}\\int_{-2a}^a x f(x)\\,dx, \\!</math>\n\nis ''not'' zero, as can be seen easily by computing the integral. This again shows that the mean (1) can not exist.\n\nVarious results in probability theory about [[expected value]]s, such as the [[strong law of large numbers]], fail to hold for the Cauchy distribution.<ref name=\"uah\"/>\n\n===Higher moments===\nThe Cauchy distribution does not have finite moments of any order.  Some of the higher [[raw moment]]s do exist and have a value of infinity, for example the raw second moment:\n\n: <math>\n\\begin{align}\n\\operatorname{E}[X^2] & \\propto \\int_{-\\infty}^\\infty \\frac{x^2}{1+x^2}\\,dx = \\int_{-\\infty}^\\infty 1 - \\frac{1}{1+x^2}\\,dx \\\\[8pt]\n& = \\int_{-\\infty}^\\infty dx - \\int_{-\\infty}^\\infty \\frac{1}{1+x^2}\\,dx = \\int_{-\\infty}^\\infty dx-\\pi = \\infty.\n\\end{align}\n</math>\n\nBy re-arranging the formula, one can see that the second moment is essentially the infinite integral of a constant (here 1).  Higher even-powered raw moments will also evaluate to infinity.  Odd-powered raw moments, however, are undefined, which is distinctly different from existing with the value of infinity. The odd-powered raw moments are undefined because their values are essentially equivalent to <math>\\infty - \\infty</math> since the two halves of the integral both diverge and have opposite signs.  The first raw moment is the mean, which, being odd, does not exist. (See also the discussion above about this.) This in turn means that all of the [[central moment]]s and [[standardized moment]]s are undefined, since they are all based on the mean.  The variance—which is the second central moment—is likewise non-existent (despite the fact that the raw second moment exists with the value infinity).\n\nThe results for higher moments follow from [[Hölder's inequality]], which implies that higher moments (or halves of moments) diverge if lower ones do.\n\n===Moments of truncated distributions===\nConsider the [[truncated distribution]] defined by restricting the standard Cauchy distribution to the interval {{math|[−10<sup>100</sup>, 10<sup>100</sup>]}}. Such a truncated distribution has all moments (and the central limit theorem applies for [[i.i.d.]] observations from it); yet for almost all practical purposes it behaves like a Cauchy distribution.<ref>{{citation| last= Hampel | first= Frank | title= Is statistics too difficult? | journal= Canadian Journal of Statistics | year= 1998 | volume= 26 | pages= 497–513 | doi= 10.2307/3315772 }}.</ref>\n\n==Estimation of parameters ==\nBecause the parameters of the Cauchy distribution do not correspond to a mean and variance, attempting to estimate the parameters of the Cauchy distribution by using a sample mean and a sample variance will not succeed.<ref>[http://www.statistics4u.info/fundstat_eng/ee_distri_cauchy.html Illustration of instability of sample means]</ref> For example, if an i.i.d. sample of size ''n'' is taken from a Cauchy distribution, one may calculate the sample mean as:\n\n:<math>\\bar{x}=\\frac 1 n \\sum_{i=1}^n x_i</math>\n\nAlthough the sample values <math>x_i</math> will be concentrated about the central value <math>x_0</math>, the sample mean will become increasingly variable as more observations are taken, because of the increased probability of encountering sample points with a large absolute value. In fact, the distribution of the sample mean will be equal to the distribution of the observations themselves; i.e., the sample mean of a large sample is no better (or worse) an estimator of <math>x_0</math> than any single observation from the sample. Similarly, calculating the sample variance will result in values that grow larger as more observations are taken.\n\nTherefore, more robust means of estimating the central value <math>x_0</math> and the scaling parameter <math>\\gamma</math> are needed. One simple method is to take the median value of the sample as an estimator of <math>x_0</math> and half the sample [[interquartile range]] as an estimator of <math>\\gamma</math>. Other, more precise and robust methods have been developed <ref>{{cite journal |last1=Cane |first1=Gwenda J. |year=1974 |title=Linear Estimation of Parameters of the Cauchy Distribution Based on Sample Quantiles |journal=Journal of the American Statistical Association |volume=69 |issue=345 |pages= 243–245 |jstor=2285535 |doi=10.1080/01621459.1974.10480163}}</ref><ref>{{cite journal |last=Zhang |first=Jin |year=2010 |title=A Highly Efficient L-estimator for the Location Parameter of the Cauchy Distribution |journal=Computational Statistics |volume=25 |issue=1 |pages=97–105 |url=http://www.springerlink.com/content/3p1430175v4806jq |doi=10.1007/s00180-009-0163-y}}</ref>  For example, the [[truncated mean]] of the middle 24% of the sample [[order statistics]] produces an estimate for <math>x_0</math> that is more efficient than using either the sample median or the full sample mean.<ref name=rothenberg>{{cite journal|last1=Rothenberg |first1=Thomas J. |last2=Fisher|first2=Franklin, M.|last3=Tilanus|first3=C.B.|year=1964|volume=59|issue=306|journal=Journal of the American Statistical Association|title=A note on estimation from a Cauchy sample|pages=460-463|doi=10.1080/01621459.1964.10482170}}</ref><ref name=bloch>{{cite journal|last1=Bloch|first1=Daniel|year=1966|volume=61 |issue=316 |journal=Journal of the American Statistical Association|title=A note on the estimation of the location parameters of the Cauchy distribution|pages=852&ndash;855|jstor=2282794|doi=10.1080/01621459.1966.10480912}}</ref> However, because of the [[fat tails]] of the Cauchy distribution, the efficiency of the estimator decreases if more than 24% of the sample is used.<ref name=rothenberg/><ref name=bloch/>\n\n[[Maximum likelihood]] can also be used to estimate the parameters <math>x_0</math> and <math>\\gamma</math>. However, this tends to be complicated by the fact that this requires finding the roots of a high degree polynomial, and there can be multiple roots that represent local maxima.<ref name=ferguson>{{cite journal|last1=Ferguson|first1=Thomas S. |year=1978 |journal=Journal of the American Statistical Association |volume=73|issue=361|title=Maximum Likelihood Estimates of the Parameters of the Cauchy Distribution for Samples of Size 3 and 4|page=211|jstor=2286549 |doi=10.1080/01621459.1978.10480031}}</ref> Also, while the maximum likelihood estimator is asymptotically efficient, it is relatively inefficient for small samples.<ref>{{cite journal|title=The Pitman estimator of the Cauchy location parameter|last1=Cohen Freue|first1=Gabriella V.|journal=Journal of Statistical Planning and Inference|volume=137|issue=6|year=2007|page=1901|url=http://faculty.ksu.edu.sa/69424/USEPAP/Coushy%20dist.pdf|doi=10.1016/j.jspi.2006.05.002|deadurl=yes|archiveurl=https://web.archive.org/web/20110816002255/http://faculty.ksu.edu.sa/69424/USEPAP/Coushy%20dist.pdf|archivedate=2011-08-16|df=}}</ref><ref>{{cite book|title=Introduction to Robust Estimation & Hypothesis Testing |last1=Wilcox |first1=Rand |year=2012 |publisher=Elsevier}}</ref>  The log-likelihood function for the Cauchy distribution for sample size <math>n</math> is:\n\n:<math>\\hat\\ell(\\!x_0,\\gamma\\mid x_1,\\dotsc,x_n) = - n \\log (\\gamma \\pi) - \\sum_{i=1}^n \\log \\left(1 + \\left(\\frac{x_i - x_0}{\\gamma}\\right)^2\\right)</math>\n\nMaximizing the log likelihood function with respect to <math>x_0</math> and <math>\\gamma</math> produces the following system of equations:\n\n:<math> \\sum_{i=1}^n \\frac{x_i - x_0}{\\gamma^2 + [x_i - \\!x_0]^2} = 0</math>\n:<math> \\sum_{i=1}^n \\frac{\\gamma^2}{\\gamma^2 + [x_i - x_0]^2} - \\frac{n}{2} = 0</math>\n\nNote that\n\n:<math> \\sum_{i=1}^n \\frac{\\gamma^2}{\\gamma^2 + [x_i - x_0]^2} </math>\n\nis a monotone function in <math>\\gamma</math> and that the solution <math>\\gamma</math> must satisfy\n\n:<math> \\min |x_i-x_0|\\le \\gamma\\le \\max |x_i-x_0|. </math>\n\nSolving just for <math>x_0</math> requires solving a polynomial of degree <math>2n-1</math>,<ref name=ferguson/> and solving just for <math>\\gamma</math> requires solving a polynomial of degree <math>n</math> (first for <math>\\gamma^2</math>, then <math>x_0</math>). Therefore, whether solving for one parameter or for both parameters simultaneously, a [[numerical analysis|numerical]] solution on a computer is typically required. The benefit of maximum likelihood estimation is asymptotic efficiency; estimating <math>x_0</math> using the sample median is only about 81% as asymptotically efficient as estimating <math>x_0</math> by maximum likelihood.<ref name=bloch/><ref>{{cite journal|last1=Barnett|first1=V. D.|year=1966|journal=Journal of the American Statistical Association |volume=61|issue=316|title=Order Statistics Estimators of the Location of the Cauchy Distribution|page=1205|jstor=2283210|doi=10.1080/01621459.1966.10482205}}</ref> The truncated sample mean using the middle 24% order statistics is about 88% as asymptotically efficient an estimator of <math>x_0</math> as the maximum likelihood estimate.<ref name=bloch/> When [[Newton's method]] is used to find the solution for the maximum likelihood estimate, the middle 24% order statistics can be used as an initial solution for <math>x_0</math>.\n\n==Multivariate Cauchy distribution==\nA [[random vector]] <math>X=(X_1, \\ldots, X_k)'</math> is said to have the multivariate Cauchy distribution if every linear combination of its components <math>Y=a_1X_1+ \\cdots + a_kX_k</math> has a Cauchy distribution. That is, for any constant vector <math>a\\in \\mathbb R^k</math>, the random variable <math>Y=a'X</math> should have a univariate Cauchy distribution.<ref name=ferg2>{{cite journal|last1=Ferguson|first1=Thomas S.|title=A Representation of the Symmetric Bivariate Cauchy Distribution|journal=The Annals of Mathematical Statistics |volume= |issue= |year=1962 |page=1256 |jstor=2237984|doi=10.1214/aoms/1177704357|url=http://projecteuclid.org/download/pdf_1/euclid.aoms/1177704357|accessdate=2017-01-07 }}</ref>  The characteristic function of a multivariate Cauchy distribution is given by:\n\n:<math>\\varphi_X(t) =  e^{ix_0(t)-\\gamma(t)}, \\!</math>\n\nwhere <math>x_0(t)</math> and <math>\\gamma(t)</math> are real functions with <math>x_0(t)</math> a [[homogeneous function]] of degree one and <math>\\gamma(t)</math> a positive homogeneous function of degree one.<ref name=ferg2/>  More formally:<ref name=ferg2/>\n\n:<math>x_0(at) = ax_0(t),</math>\n:<math>\\gamma (at) = |a|\\gamma (t),</math>\n\nfor all <math>t</math>.\n\nAn example of a bivariate Cauchy distribution can be given by:<ref name=bivar>{{cite journal|title=Non-linear Integral Equations to Approximate Bivariate Densities with Given Marginals and Dependence Function|last1=Molenberghs|first1=Geert|last2=Lesaffre|first2=Emmanuel|journal=Statistica Sinica|volume=7|year=1997|pages=713&ndash;738|url=http://www3.stat.sinica.edu.tw/statistica/oldpdf/A7n310.pdf|deadurl=yes|archiveurl=https://web.archive.org/web/20090914055538/http://www3.stat.sinica.edu.tw/statistica/oldpdf/A7n310.pdf|archivedate=2009-09-14|df=}}</ref>\n:<math>f(x, y; x_0,y_0,\\gamma)= { 1 \\over 2 \\pi } \\left[ { \\gamma \\over ((x - x_0)^2 + (y - y_0)^2 +\\gamma^2)^{1.5}  } \\right] .</math>\nNote that in this example, even though there is no analogue to a covariance matrix, <math>x</math> and <math>y</math> are not [[Independence (probability theory)|statistically independent]].<ref name=bivar/>\n\nWe also can write this formula for complex variable. Then the probability density function of complex cauchy is :\n\n<math>f(z; z_0,\\gamma)= { 1 \\over 2 \\pi } \\left[ { \\gamma \\over (|z-z_0|^2 +\\gamma^2)^{1.5}  } \\right] .</math>\n\nAnalogous to the univariate density, the multidimensional Cauchy density also relates to the [[multivariate Student distribution]]. They are equivalent when the degrees of freedom parameter is equal to one. The density of a <math>k</math> dimension Student distribution with one degree of freedom becomes:\n\n:<math>f({\\mathbf x}; {\\mathbf\\mu},{\\mathbf\\Sigma}, k)= \\frac{\\Gamma\\left(\\frac{1+k}{2}\\right)}{\\Gamma(\\frac{1}{2})\\pi^{\\frac{k}{2}}\\left|{\\mathbf\\Sigma}\\right|^{\\frac{1}{2}}\\left[1+({\\mathbf x}-{\\mathbf\\mu})^T{\\mathbf\\Sigma}^{-1}({\\mathbf x}-{\\mathbf\\mu})\\right]^{\\frac{1+k}{2}}} .</math>\n\nProperties and details for this density can be obtained by taking it as a particular case of the multivariate Student density.\n\n==Transformation properties==\n*If <math>X \\sim \\operatorname{Cauchy}(x_0,\\gamma)\\,</math> then <math> kX+\\ell \\sim \\textrm{Cauchy}(x_0 k+\\ell,\\gamma |k|)</math><ref>{{Citation\n| last1  = Lemons\n| first1 = Don S.\n| title     = An Introduction to Stochastic Processes in Physics\n| publisher = The Johns Hopkins University Press\n| year    = 2002\n| isbn    = 0-8018-6866-1\n| page=35 \n}}</ref>\n*If <math>X \\sim \\operatorname{Cauchy}(x_0,\\gamma_0)\\,</math> and <math>Y \\sim \\operatorname{Cauchy}(x_1,\\gamma_1)\\,</math> are independent, then <math> X+Y \\sim \\operatorname{Cauchy}(x_0+x_1,\\gamma_0+\\gamma_1)\\,</math> and <math> X-Y \\sim \\operatorname{Cauchy}(x_0-x_1,\\gamma_0+\\gamma_1)\\,</math>\n*If <math>X \\sim \\operatorname{Cauchy}(0,\\gamma)\\,</math> then <math> \\tfrac{1}{X} \\sim \\operatorname{Cauchy}(0,\\tfrac{1}{\\gamma})\\,</math>\n*[[McCullagh's parametrization of the Cauchy distributions]]:<ref name=\"McCullagh1992\">[[Peter McCullagh|McCullagh, P.]], [http://biomet.oxfordjournals.org/cgi/content/abstract/79/2/247 \"Conditional inference and Cauchy models\"], ''[[Biometrika]]'', volume 79 (1992), pages 247&ndash;259. [http://www.stat.uchicago.edu/~pmcc/pubs/paper18.pdf PDF] from McCullagh's homepage.</ref> Expressing a Cauchy distribution in terms of one complex parameter <math>\\psi=x_0+i\\gamma</math>, define <math>X \\sim \\operatorname{Cauchy}(\\psi)</math> to mean <math>X \\sim \\operatorname{Cauchy}(x_0,|\\gamma|)</math>. If <math>X \\sim \\operatorname{Cauchy}(\\psi)</math> then:\n:: <math>\\frac{aX+b}{cX+d} \\sim \\operatorname{Cauchy}\\left(\\frac{a\\psi+b}{c\\psi+d}\\right)</math> \nwhere <math>a</math>, <math>b</math>, <math>c</math> and <math>d</math> are real numbers.\n* Using the same convention as above, if <math>X \\sim \\operatorname{Cauchy}(\\psi)</math> then:<ref name=\"McCullagh1992\"/>\n:: <math>\\frac{X-i}{X+i} \\sim \\operatorname{CCauchy}\\left(\\frac{\\psi-i}{\\psi+i}\\right)</math>\n:where <math>\\operatorname{CCauchy}</math> is the [[circular Cauchy distribution]].\n\n== Lévy measure ==\nThe Cauchy distribution is the [[stable distribution]] of index 1. The [[Lévy process#L.C3.A9vy.E2.80.93Khintchine representation|Lévy–Khintchine representation]] of such a stable distribution of parameter <math> \\gamma </math> is given, for <math> X \\sim \\operatorname{Stable}(\\gamma, 0, 0)\\,</math> by:\n\n: <math>\\operatorname{E}\\left( e^{ixX} \\right) = \\exp\\left( \\int_{ \\mathbb{R} } (e^{ixy} - 1) \\Pi_\\gamma(dy) \\right)</math>\n\nwhere\n\n:<math>\\Pi_\\gamma(dy) = \\left( c_{1, \\gamma}  \\frac{1}{y^{1 + \\gamma}} 1_{ \\left\\{y > 0\\right\\} } + c_{2,\\gamma}  \\frac{1}{|y|^{1 + \\gamma}} 1_{\\left\\{ y < 0 \\right\\}} \\right) \\, dy \n</math>\n\nand <math> c_{1, \\gamma}, c_{2, \\gamma} </math> can be expressed explicitly.<ref>{{cite book |authors=Kyprianou, Andreas |year=2009 |title=Lévy processes and continuous-state branching processes:part I |page=11 |url=http://www.maths.bath.ac.uk/~ak257/LCSB/part1.pdf}}</ref> In the case <math> \\gamma = 1 </math> of the Cauchy distribution, one has <math> c_{1, \\gamma} = c_{2, \\gamma}  </math>.\n\nThis last representation is a consequence of the formula\n\n: <math>|x| = \\int_{\\mathbb{R}} (1 - e^{ixy}) \\, \\frac{dy}{y^2} </math>\n\n==Related distributions==\n*<math>\\operatorname{Cauchy}(0,1) \\sim \\textrm{t}(\\mathrm{df}=1)\\,</math> [[Student's t distribution|Student's ''t'' distribution]]\n*<math>\\operatorname{Cauchy}(\\mu,\\sigma) \\sim \\textrm{t}_{(\\mathrm{df}=1)}(\\mu,\\sigma)\\,</math> [[Student's t distribution#Non-standardized|non-standardized Student's ''t'' distribution]]\n*If <math>X, Y \\sim \\textrm{N}(0,1)\\, X, Y</math> independent, then <math> \\tfrac X Y\\sim \\textrm{Cauchy}(0,1)\\,</math>\n*If <math>X \\sim \\textrm{U}(0,1)\\,</math> then <math> \\tan \\left( \\pi \\left(X-\\tfrac{1}{2}\\right) \\right) \\sim \\textrm{Cauchy}(0,1)\\,</math>\n*If <math>X \\sim \\operatorname{Log-Cauchy}(0, 1)</math> then <math>\\ln(X) \\sim \\textrm{Cauchy}(0, 1)</math>\n*The Cauchy distribution is a limiting case of a [[Pearson distribution]] of type 4{{Citation needed|date=March 2011}}\n*The Cauchy distribution is a special case of a [[Pearson distribution]] of type 7.<ref name=jkb1/>\n*The Cauchy distribution is a [[stable distribution]]: if <math>X \\sim \\textrm{Stable}(1, 0, \\gamma, \\mu)</math>, then <math>X \\sim \\operatorname{Cauchy}(\\mu, \\gamma)</math>.\n*The Cauchy distribution is a singular limit of a [[hyperbolic distribution]]{{Citation needed|date=April 2011}}\n*The [[wrapped Cauchy distribution]], taking values on a circle, is derived from the Cauchy distribution by wrapping it around the circle.\n\n==Relativistic Breit–Wigner distribution==\n{{Main article|Relativistic Breit–Wigner distribution}}\nIn [[nuclear physics|nuclear]] and [[particle physics]], the energy profile of a [[resonance]] is described by the [[relativistic Breit–Wigner distribution]], while the Cauchy distribution is the (non-relativistic) Breit–Wigner distribution.{{Citation needed|date=March 2011}}\n\n==Occurrence and applications==\n\n*Applications of the Cauchy distribution or its transformation can be found in fields working with exponential growth.  A 1958 paper by White <ref>White, J.S. (1958) The Limiting Distribution of the Serial Correlation Coefficient in the Explosive Case. The Annals of Mathematical Statistics, 29, 1188-1197.\nhttps://doi.org/10.1214/aoms/1177706450</ref> derived the test statistic for estimators of <math>\\hat{\\beta}</math> for the equation <math>x_{t+1}=\\beta{x}_t+\\varepsilon_{t+1},\\beta>1</math> and where the maximum likelihood estimator is found using ordinary least squares showed the sampling distribution of the statistic is the Cauchy distribution.\n[[File:Cauchy distribution.png|thumb|240px|Fitted cumulative Cauchy distribution to maximum one-day rainfalls using [[CumFreq]], see also [[distribution fitting]] <ref>CumFreq, free software for cumulative frequency analysis and probability distribution fitting [https://www.waterlog.info/cumfreq.htm]</ref>]]\n\n*Outside of finance{{Clarify|date=January 2019}}, the Cauchy distribution is often the distribution of observations for objects that are spinning.  The classic reference for this is called the Gull's lighthouse problem<ref>Gull, S.F. (1988) Bayesian Inductive Inference and Maximum Entropy. Kluwer Academic Publishers, Berlin. https://doi.org/10.1007/978-94-009-3049-0_4</ref> and as in the above section as the Breit–Wigner distribution in particle physics.\n\n*In [[hydrology]] the Cauchy distribution is applied to extreme events such as annual maximum one-day rainfalls and river discharges. The blue picture illustrates an example of fitting the Cauchy distribution to ranked monthly maximum one-day rainfalls showing also the 90% [[confidence belt]] based on the [[binomial distribution]]. The rainfall data are represented by [[plotting position]]s as part of the [[cumulative frequency analysis]].\n*The expression for imaginary part of complex [[Permittivity|electrical permittivity]] according to Lorentz model is a Cauchy distribution.\n\n==See also==\n* [[Lévy flight]] and [[Lévy process]]\n* [[Cauchy process]]\n* [[Stable process]]\n* [[Slash distribution]]\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n* {{springer|title=Cauchy distribution|id=p/c020850}}\n* [http://jeff560.tripod.com/c.html Earliest Uses: The entry on Cauchy distribution has some historical information.]\n* {{MathWorld | urlname=CauchyDistribution | title=Cauchy Distribution}}\n* [https://www.gnu.org/software/gsl/manual/gsl-ref.html#SEC294 GNU Scientific Library &ndash; Reference Manual]\n* [http://www.jstatsoft.org/v16/i04/paper Ratios of Normal Variables by George Marsaglia]\n{{ProbDistributions|continuous-infinite}}\n\n{{DEFAULTSORT:Cauchy Distribution}}\n[[Category:Continuous distributions]]\n[[Category:Probability distributions with non-finite variance]]\n[[Category:Power laws]]\n[[Category:Stable distributions]]\n[[Category:Location-scale family probability distributions]]"
    },
    {
      "title": "Gamma correction",
      "url": "https://en.wikipedia.org/wiki/Gamma_correction",
      "text": "[[File:GammaCorrection demo.jpg|thumb|The effect of gamma correction on an image: The original image was taken to varying powers, showing that powers larger than 1 make the shadows darker, while powers smaller than 1 make dark regions lighter.]]\n\n'''Gamma correction''', or often simply '''gamma''', is a nonlinear operation used to encode and decode [[Relative luminance|luminance]] or [[CIE 1931 color space#Tristimulus values|tristimulus values]] in [[video]] or [[still image]] systems.<ref name=poynton>{{cite book | title = Digital Video and HDTV: Algorithms and Interfaces | author = Charles A. Poynton | publisher = Morgan Kaufmann | year = 2003 | isbn = 1-55860-792-7 | pages = 260, 630 | url = https://books.google.com/books?id=ra1lcAwgvq4C&pg=RA1-PA630&dq=gamma-encoding}}</ref> Gamma correction is, in the simplest cases, defined by the following [[Power law|power-law]] expression:\n: <math>V_{\\text{out}} = A {V_{\\text{in}}^\\gamma}</math>\nwhere the non-negative real input value <math>V_{\\text{in}}</math> is raised to the power <math>\\gamma</math> and multiplied by the constant ''A,'' to get the output value <math>V_{\\text{out}}</math>. In the common case of {{nowrap|1=''A'' = 1}}, inputs and outputs are typically in the range 0–1.\n\nA gamma value <math>\\gamma < 1</math> is sometimes called an ''encoding gamma'', and the process of encoding with this compressive power-law nonlinearity is called '''gamma compression'''; conversely a gamma value <math>\\gamma > 1</math> is called a ''decoding gamma'' and the application of the expansive power-law nonlinearity is called '''gamma expansion'''.\n\n==Explanation==\nGamma encoding of images is used to optimize the usage of bits when encoding an image, or bandwidth used to transport an image, by taking advantage of the non-linear manner in which humans perceive light and color.<ref name=poynton/> The human perception of brightness, under common illumination conditions (not pitch black nor blindingly bright), follows an approximate [[power function]] (note: no relation to the [[gamma function]]), with greater sensitivity to relative differences between darker tones than between lighter ones, consistent with the [[Stevens power law]] for brightness perception. If images are not gamma-encoded, they allocate too many bits or too much bandwidth to highlights that humans cannot differentiate, and too few bits or too little bandwidth to shadow values that humans are sensitive to and would require more bits/bandwidth to maintain the same visual quality.<ref>{{cite web |url=http://www.w3.org/TR/PNG-GammaAppendix.html |title=PNG Specification 13. Appendix: Gamma Tutorial |publisher=W3C |date=1996-10-01 |accessdate=2018-12-03 |quote=What is gamma correction? }}</ref><ref name=poynton/><ref name=FQAGamma>[http://www.poynton.com/notes/color/GammaFQA.html Charles Poynton (2010). Frequently Questioned Answers about Gamma.]</ref> Gamma encoding of [[floating-point]] images is not required (and may be counterproductive), because the floating-point format already provides a piecewise linear approximation of a logarithmic curve.<ref>{{cite book|author1=Erik Reinhard |author2=Wolfgang Heidrich |author3=Paul Debevec |author4=Sumanta Pattanaik |author5=Greg Ward |author6=Karol Myszkowski | title = High Dynamic Range Imaging: Acquisition, Display, and Image-Based Lighting | year = 2010 | publisher = Morgan Kaufmann | isbn = 9780080957111 | page = 82 | url = https://books.google.com/books?id=w1i_1kejoYcC&pg=PA82}}</ref>\n\nAlthough gamma encoding was developed originally to compensate for the input–output characteristic of [[cathode ray tube]] (CRT) displays, that is not its main purpose or advantage in modern systems. In CRT displays, the light intensity varies nonlinearly with the electron-gun voltage. Altering the input signal by gamma compression can cancel this nonlinearity, such that the output picture has the intended luminance. However, the gamma characteristics of the display device do not play a factor in the gamma encoding of images and video—they need gamma encoding to maximize the visual quality of the signal, regardless of the gamma characteristics of the display device.<ref name=poynton/><ref name=FQAGamma/> The similarity of CRT physics to the inverse of gamma encoding needed for video transmission was a combination of coincidence and engineering, which simplified the electronics in early television sets.<ref>{{cite web |last=McKesson|first=Jason L. |title=Chapter 12. Dynamic Range – Linearity and Gamma |url=http://www.arcsynthesis.org/gltut/Illumination/Tut12%20Monitors%20and%20Gamma.html |archive-url=https://web.archive.org/web/20130718042406/http://www.arcsynthesis.org/gltut/Illumination/Tut12%20Monitors%20and%20Gamma.html |archive-date=18 July 2013 |work=Learning Modern 3D Graphics Programming |accessdate=11 July 2013}}</ref>\n\n==Generalized gamma==\nThe concept of gamma can be applied to any nonlinear relationship. For the [[power-law]] relationship <math>V_\\text{out} = V_\\text{in}^\\gamma </math>, the curve on a log–log plot is a straight line, with slope everywhere equal to gamma (slope is represented here by the [[derivative]] operator):\n: <math>\\gamma = \\frac{\\mathrm{d} \\log(V_{\\text{out}})}{\\mathrm{d} \\log(V_{\\text{in}})}</math>\n\nThat is, gamma can be visualized as the slope of the input–output curve when plotted on logarithmic axes. For a power-law curve, this slope is constant, but the idea can be extended to any type of curve, in which case gamma (strictly speaking, \"point gamma\"<ref>R.W.G. Hunt, ''The Reproduction of Colour'', 6th Ed, p48</ref>) is defined as the slope of the curve in any particular region.\n\n==Film photography==\n{{anchor|Photography}}\n{{main|Sensitometry}}\nWhen a [[photographic film]] is exposed to light, the result of the exposure can be represented on a graph showing log of [[Exposure (photography)|exposure]] on the horizontal axis, and density, or log of transmittance, on the vertical axis. For a given film formulation and processing method, this curve is its [[Sensitometry|characteristic or Hurter–Driffield curve]].<ref>Kodak, \"Basic sensitometry and characteristics of film\" [http://motion.kodak.com/motion/uploadedFiles/US_plugins_acrobat_en_motion_newsletters_filmEss_06_Characteristics_of_Film.pdf]: \"A characteristic curve is like a film’s fingerprint.\"</ref><ref name=kodak>{{cite web |url=http://www.kodak.com/global/en/professional/support/techPubs/f4017/f4017.jhtml |title=Kodak Professional Tri-X 320 and 400 Films |date=May 2007 |publisher=[[Eastman Kodak Company]] |format=PDF}}</ref> Since both axes use logarithmic units, the slope of the linear section of the curve is called the gamma of the film. Negative film typically has a gamma less than 1;<ref name=kodak/><ref name=\"portra160\">{{cite web |title=KODAK PROFESSIONAL PORTRA  160 Film |url=http://imaging.kodakalaris.com/sites/prod/files/files/products/e4051_portra_160.pdf |website=imaging.kodakalaris.com |publisher=kodak |accessdate=29 January 2019}}</ref> positive film (slide film, reversal film) typically has a gamma with absolute value greater than 1.<ref name=\"kodachromeProfessionalFilm\">{{cite web |title=KODACHROME 25, 64, and 200  Professional Film |url=http://wwwuk.kodak.com/global/en/professional/support/techPubs/e55/e55.pdf |website=wwwuk.kodak.com |publisher=Kodak |accessdate=29 January 2019}}</ref>\n\nPhotographic film has a much greater ability to record fine differences in shade than can be reproduced on [[photographic paper]]. Similarly, most video screens are not capable of displaying the range of brightnesses (dynamic range) that can be captured by typical electronic cameras.<ref>\n{{cite book\n | title = An introduction to video and audio measurement\n | edition = 3rd\n | author = Peter Hodges\n | publisher = Elsevier\n | year = 2004\n | isbn = 978-0-240-80621-1\n | page = 174\n | url = https://books.google.com/books?id=Fmd0lbD32R0C&pg=PA174\n }}</ref>\nFor this reason, considerable artistic effort is invested in choosing the reduced form in which the original image should be presented. The gamma correction, or contrast selection, is part of the photographic repertoire used to adjust the reproduced image.\n\nAnalogously, digital cameras record light using electronic sensors that usually respond linearly. In the process of rendering linear raw data to conventional [[RGB color model|RGB]] data (e.g. for storage into [[JPEG]] image format), color space transformations and rendering transformations will be performed. In particular, almost all standard [[RGB color space|RGB color spaces]] and file formats use a non-linear encoding (a [[gamma compression]]) of the intended intensities of the [[primary color]]s of the photographic reproduction; in addition, the intended reproduction is almost always nonlinearly related to the measured scene intensities, via a [[tone reproduction]] nonlinearity.\n\n==Windows, Mac, sRGB and TV/video standard gammas==\n{{further|sRGB#Specification of the transformation}}\n[[File:SRGB gamma.svg|thumb|227px|Plot of the [[sRGB]] standard gamma-expansion nonlinearity in red, and its local gamma value (slope in log–log space) in blue. The local gamma rises from 1 to about 2.2.]]\n\nIn most computer display systems, images are encoded with a gamma of about 0.45 and decoded with the reciprocal gamma of 2.2. A notable exception, until the release of Mac OS X 10.6 (Snow Leopard) in September 2009, were [[Macintosh]] computers, which encoded with a gamma of 0.55 and decoded with a gamma of 1.8. In any case, [[binary file|binary]] data in still image files (such as [[JPEG]]) are explicitly encoded (that is, they carry gamma-encoded values, not linear intensities), as are motion picture files (such as [[MPEG]]). The system can optionally further manage both cases, through [[color management]], if a better match to the output device gamma is required.\n\nThe [[sRGB color space]] standard used with most cameras, PCs, and printers does not use a simple power-law nonlinearity as above, but has a decoding gamma value near 2.2 over much of its range, as shown in the plot to the right. Below a compressed value of 0.04045 or a linear intensity of 0.00313, the curve is linear (encoded value proportional to intensity), so {{nowrap|1=''γ'' = 1}}. The dashed black curve behind the red curve is a standard {{nowrap|1=''γ'' = 2.2}} power-law curve, for comparison.\n\nOutput to CRT-based television receivers and monitors does not usually require further gamma correction, since the standard video signals that are transmitted or stored in image files incorporate gamma compression that provides a pleasant image after the gamma expansion of the CRT (it is not the exact inverse). For television signals, the actual gamma values are defined by the video standards ([[NTSC]], [[PAL]] or [[SECAM]]), and are always fixed and well-known values.\n\nGamma correction in computers is used, for example, to display a gamma = 1.8 Apple picture correctly on a gamma = 2.2 PC monitor by changing the image gamma. Another usage is equalizing of the individual color-channel gammas to correct for monitor discrepancies.\n\n==Gamma meta information==\nSome picture formats include gamma [[metadata]] to allow the display system an automatic gamma correction. The PNG format has the gAMA metadata, the Exif format has the Gamma tag. The [http://www.w3.org/Graphics/PNG/all_seven.html W3 PNG gamma test images] show that the Mozilla Firefox browser supports PNG gAMA and the Microsoft Explorer and Edge browsers do not. The gamma metadata is part of the [[color management]] meta information. See [[Color management#Application level|color management application level]] for web browser color management support.\n\n==Power law for video display==\nA ''gamma characteristic'' is a [[power law|power-law]] relationship that approximates the relationship between the encoded [[Luminance (video)|luma]] in a [[television]] system and the actual desired image luminance.\n\nWith this nonlinear relationship, equal steps in encoded luminance correspond roughly to subjectively equal steps in brightness. Ebner and Fairchild<ref name=\"EbnerCIC61998\">Fritz Ebner and Mark D Fairchild, \"Development and testing of a color space (IPT) with improved hue uniformity,\" ''Proceedings of IS&T/SID's Sixth Color Imaging Conference,'' p 8-13 (1998).</ref> used an exponent of 0.43 to convert linear intensity into lightness (luma) for neutrals; the reciprocal, approximately 2.33 (quite close to the 2.2 figure cited for a typical display subsystem), was found to provide approximately optimal perceptual encoding of grays. \n\nThe following illustration shows the difference between a scale with linearly-increasing encoded luminance signal (linear gamma-compressed luma input) and a scale with linearly-increasing intensity scale (linear luminance output).\n\n{| style=\"background: #000000; color: gray; padding: 1em; margin: 0 2em 0 2em;\"\n| style=\"color: #A0A0A0;\" | Linear encoding\n| style=\"color: #A0A0A0;\" | ''V''<sub>S</sub> = &nbsp;\n| style=\"background: #000000; color: #0D4F8B;\" | 0.0\n| style=\"background: #1A1A1A; color: #0D4F8B;\" | 0.1\n| style=\"background: #333333; color: #0D4F8B;\" | 0.2\n| style=\"background: #4D4D4D; color: #0D4F8B;\" | 0.3\n| style=\"background: #666666; color: #0D4F8B;\" | 0.4\n| style=\"background: #808080; color: #0D4F8B;\" | 0.5\n| style=\"background: #999999; color: #0D4F8B;\" | 0.6\n| style=\"background: #B3B3B3; color: #0D4F8B;\" | 0.7\n| style=\"background: #CCCCCC; color: #0D4F8B;\" | 0.8\n| style=\"background: #E6E6E6; color: #0D4F8B;\" | 0.9\n| style=\"background: #FFFFFF; color: #0D4F8B;\" | 1.0\n|-\n| style=\"color: #A0A0A0;\" | Linear intensity\n| style=\"color: #A0A0A0;\" | &nbsp;''I'' =&nbsp;\n| style=\"background: #000000; color: #0D4F8B;\" | 0.0\n| style=\"background: #5A5A5A; color: #0D4F8B;\" | 0.1\n| style=\"background: #7B7B7B; color: #0D4F8B;\" | 0.2\n| style=\"background: #949494; color: #0D4F8B;\" | 0.3\n| style=\"background: #A9A9A9; color: #0D4F8B;\" | 0.4\n| style=\"background: #BBBBBB; color: #0D4F8B;\" | 0.5\n| style=\"background: #CBCBCB; color: #0D4F8B;\" | 0.6\n| style=\"background: #D9D9D9; color: #0D4F8B;\" | 0.7\n| style=\"background: #E7E7E7; color: #0D4F8B;\" | 0.8\n| style=\"background: #F4F4F4; color: #0D4F8B;\" | 0.9\n| style=\"background: #FFFFFF; color: #0D4F8B;\" | 1.0\n|}\n\nOn most displays (those with gamma of about 2.2), one can observe that the linear-intensity scale has a large jump in perceived brightness between the intensity values 0.0 and 0.1, while the steps at the higher end of the scale are hardly perceptible. The gamma-encoded scale, which has a nonlinearly-increasing intensity, will show much more even steps in perceived brightness.\n\nA [[cathode ray tube]] (CRT), for example, converts a video signal to light in a nonlinear way, because the electron gun's intensity (brightness) as a function of applied video voltage is nonlinear. The light intensity ''I'' is related to the source [[voltage]] ''V''<sub>s</sub> according to\n: <math>I \\propto V_{\\rm s}^{\\gamma}</math>\nwhere ''γ'' is the [[Greek alphabet|Greek]] letter [[gamma]]. For a CRT, the gamma that relates brightness to voltage is usually in the range 2.35 to 2.55; video [[look-up table]]s in computers usually adjust the system gamma to the range 1.8 to 2.2,<ref name=poynton/> which is in the region that makes a uniform encoding difference give approximately uniform perceptual brightness difference, as illustrated in the diagram at the top of this section.\n\nFor simplicity, consider the example of a monochrome CRT. In this case, when a video signal of 0.5 (representing a mid-gray) is fed to the display, the intensity or brightness is about 0.22 (resulting in a mid-gray, about 22% the intensity of white). Pure black (0.0) and pure white (1.0) are the only shades that are unaffected by gamma.\n\nTo compensate for this effect, the inverse transfer function (gamma correction) is sometimes applied to the video signal so that the end-to-end response is linear. In other words, the transmitted signal is deliberately distorted so that, after it has been distorted again by the display device, the viewer sees the correct brightness. The inverse of the function above is:\n: <math>V_{\\rm c} \\propto V_{\\rm s}^{1/\\gamma}</math>\nwhere ''V''<sub>c</sub> is the corrected voltage and ''V''<sub>s</sub> is the source voltage, for example from an [[image sensor]] that converts photocharge linearly to a voltage. In our CRT example 1/''γ'' is 1/2.2 or 0.45.\n\nA color CRT receives three video signals (red, green, and blue) and in general each color has its own value of gamma, denoted ''γ''<sub>''R''</sub>, ''γ''<sub>''G''</sub> or ''γ''<sub>''B''</sub>. However, in simple display systems, a single value of ''γ'' is used for all three colors.\n\nOther display devices have different values of gamma: for example, a [[Game Boy Advance]] display has a gamma between 3 and 4 depending on lighting conditions. In LCDs such as those on laptop computers, the relation between the signal voltage ''V''<sub>s</sub> and the intensity ''I'' is very nonlinear and cannot be described with gamma value. However, such displays apply a correction onto the signal voltage in order to approximately get a standard {{nowrap|1=''γ'' = 2.5}} behavior. In [[NTSC]] [[television]] recording, {{nowrap|1=''γ'' = 2.2}}.\n\nThe power-law function, or its inverse, has a slope of infinity at zero. This leads to problems in converting from and to a gamma colorspace. For this reason most formally defined colorspaces such as [[sRGB]] will define a straight-line segment near zero and add raising {{nowrap|''x'' + ''K''}} (where ''K'' is a constant) to a power so the curve has continuous slope. This straight line does not represent what the CRT does, but does make the rest of the curve more closely match the effect of ambient light on the CRT. In such expressions the exponent is ''not'' the gamma; for instance, the sRGB function uses a power of 2.4 in it, but more closely resembles a power-law function with an exponent of 2.2, without a linear portion.\n\n==Methods to perform display gamma correction in computing==\nUp to four elements can be manipulated in order to achieve gamma encoding to correct the image to be shown on a typical 2.2- or 1.8-gamma computer display:\n* The pixel's intensity values in a given image file; that is, the binary pixel values are stored in the file in such way that they represent the light intensity via gamma-compressed values instead of a linear encoding. This is done systematically with digital video files (as those in a [[DVD]] movie), in order to minimize the gamma-decoding step while playing, and maximize image quality for the given storage. Similarly, pixel values in standard image file formats are usually gamma-compensated, either for sRGB gamma (or equivalent, an approximation of typical of legacy monitor gammas), or according to some gamma specified by metadata such as an [[ICC profile]]. If the encoding gamma does not match the reproduction system's gamma, further correction may be done, either on display or to create a modified image file with a different profile.\n* The rendering software writes gamma-encoded pixel binary values directly to the video memory (when [[highcolor]]/[[True Color|truecolor]] modes are used) or in the [[CLUT]] [[hardware register]]s (when [[indexed color]] modes are used) of the [[display adapter]]. They drive [[Digital-to-Analog Converter]]s (DAC) which output the proportional voltages to the display. For example, when using [[List of monochrome and RGB palettes#24-bit RGB|24-bit RGB]] color (8 bits per channel), writing a value of 128 (rounded midpoint of the 0–255 [[byte]] range) in video memory it outputs the proportional {{nowrap|≈ 0.5}} voltage to the display, which it is shown darker due to the monitor behavior. Alternatively, to achieve {{nowrap|≈ 50%}} intensity, a gamma-encoded [[look-up table]] can be applied to write a value near to 187 instead of 128 by the rendering software.\n* Modern display adapters have dedicated calibrating CLUTs, which can be loaded once with the appropriate gamma-correction [[look-up table]] in order to modify the encoded signals digitally before the DACs that output voltages to the monitor.<ref>[http://msdn2.microsoft.com/en-us/library/ms536529.aspx ''SetDeviceGammaRamp'', the Win32 API to download arbitrary gamma ramps to display hardware]</ref> Setting up these tables to be correct is called ''hardware calibration''.<ref name=cm>[http://ftp2.bmtmicro.com/dlc/Color%20Management.pdf Jonathan Sachs (2003). Color Management. Digital Light & Color.] {{webarchive|url=https://web.archive.org/web/20080704074540/http://ftp2.bmtmicro.com/dlc/Color%20Management.pdf |date=2008-07-04 }}</ref>\n* Some modern monitors allow the user to manipulate their gamma behavior (as if it were merely another brightness/contrast-like setting), encoding the input signals by themselves before they are displayed on screen. This is also a ''calibration by hardware'' technique but it is performed on the analog electric signals instead of remapping the digital values, as in the previous cases.\n\nIn a correctly calibrated system, each component will have a specified gamma for its input and/or output encodings.<ref name=cm/> Stages may change the gamma to correct for different requirements, and finally the output device will do gamma decoding or correction as needed, to get to a linear intensity domain. All the encoding and correction methods can be arbitrarily superimposed, without mutual knowledge of this fact among the different elements; if done incorrectly, these conversions can lead to highly distorted results, but if done correctly as dictated by standards and conventions will lead to a properly functioning system.\n\nIn a typical system, for example from camera through [[JPEG]] file to display, the role of gamma correction will involve several cooperating parts. The camera encodes its rendered image into the JPEG file using one of the standard gamma values such as 2.2, for storage and transmission. The display computer may use a [[color management]] engine to convert to a different color space (such as older Macintosh's {{nowrap|1=''γ'' = 1.8}} color space) before putting pixel values into its video memory. The monitor may do its own gamma correction to match the CRT gamma to that used by the video system. Coordinating the components via standard interfaces with default standard gamma values makes it possible to get such system properly configured.\n\n==Simple monitor tests==\n[[File:Gamma correction test picture.png|left|497px|frame|Gamma correction test image. Only valid at browser zoom=100%]]\n\nThis procedure is useful for making a monitor display images approximately correctly, on systems in which profiles are not used (for example, the Firefox browser prior to version 3.0 and many others) or in systems that assume untagged source images are in the sRGB colorspace.\n\nIn the test pattern, the intensity of each solid color bar is intended to be the average of the intensities in the surrounding striped dither; therefore, ideally, the solid areas and the dithers should appear equally bright in a system properly adjusted to the indicated gamma.\n\nNormally a graphics card has contrast and brightness control and a [[Transflective liquid-crystal display|transmissive LCD]] monitor has contrast, brightness, and [[backlight]] control.  Graphics card and monitor contrast and brightness have an influence on effective gamma, and should not be changed after gamma correction is completed.\n\nThe top two bars of the test image help to set correct contrast and brightness values. There are eight three digits numbers in each bar. A good monitor with proper calibration shows the six numbers on the right in both bars, a cheap monitor shows only four numbers.\n\nGiven a desired display-system gamma, if the observer sees the same brightness in the checkered part and in the homogeneous part of every colored area, then the gamma correction is approximately correct.<ref>{{cite web |url=http://www.normankoren.com/makingfineprints1A.html#gammachart |title=Monitor calibration and gamma |last=Koren |first=Norman |accessdate=2018-12-10 |quote=The chart below enables you to set the black level (brightness) and estimate display gamma over a range of 1 to 3 with precison better than 0.1. }}</ref><ref>{{cite web |url=http://www.lagom.nl/lcd-test/gamma_calibration.php |title=Gamma calibration |last=Nienhuys |first=Han-Kwang |date=2008 |accessdate=2018-11-30 |quote=The reason for using 48% rather than 50% as a luminance is that many LCD screens have saturation issues in the last 5 percent of their brightness range that would distort the gamma measurement. }}</ref><ref>{{cite web |url=http://www.photoscientia.co.uk/Gamma.htm |title=The Monitor calibration and Gamma assessment page |last=Andrews |first=Peter |accessdate=2018-11-30 |quote=the problem is caused by the risetime of most monitor hardware not being sufficiently fast to turn from full black to full white in the space of a single pixel, or even two, in some cases. }}</ref> In many cases the gamma correction values for the primary colors are slightly different.\n\nSetting the [[color temperature]] or [[white point]] is the next step in monitor adjustment.\n\nBefore gamma correction the desired gamma and [[color temperature]] should be set using the monitor controls. Using the controls for gamma, contrast and brightness, the gamma correction on a [[Liquid-crystal display|LCD]] can only be done for one specific vertical viewing angle, which implies one specific horizontal line on the monitor, at one specific brightness and contrast level. An [[ICC profile]] allows to adjust the monitor for several brightness levels. The quality (and price) of the monitor determines how much deviation of this operating point still gives a satisfactory gamma correction. [[Twisted nematic field effect|Twisted nematic]] (TN) displays with 6-bit [[color depth]] per primary color have lowest quality. [[IPS panel|In-plane switching]] (IPS) displays with typically 8-bit color depth are better. Good monitors have 10-bit color depth, have hardware [[color management]] and allow hardware calibration with a [[tristimulus colorimeter]]. Often a 6bit plus [[Frame rate control|FRC]] panel is sold as 8bit and a 8bit plus FRC panel is sold as 10bit. FRC is no true replacement for more bits. The 24-bit and 32-bit color depth formats have 8 bits per primary color.\n\nWith Microsoft Windows 7 and above the user can set the gamma correction through the display color calibration tool dccw.exe or other programs.<ref>{{cite web |url=https://support.microsoft.com/en-us/help/14217/windows-8-get-best-display-on-your-monitor |title=Get the best display on your monitor - Calibrate your display |publisher=Microsoft |accessdate=2018-12-10 |quote=If you have a display calibration device and software, it's a good idea to use them instead of Display Color Calibration because they'll give you better calibration results. }}</ref><ref>{{cite web |url=https://www.quickgamma.de/indexen.html |title=Quickgamma |last=Werle |first=Eberhard |accessdate=2018-12-10 |quote=QuickGamma is a small utility program to calibrate a monitor on the fly without having to buy expensive hardware tools. }}</ref><ref>{{cite web |url=http://www.hex2bit.com/products/product_mcw.html |title=Monitor Calibration Wizard |last=Walters |first=Mike |accessdate=2018-12-10 |quote=Easy wizard for creating color profiles for you monitor. }}</ref> These programs create an ICC profile file and load it as default. This makes [[color management]] easy.<ref>{{cite web |url=https://support.microsoft.com/en-gb/help/4462979/windows-about-color-management |title=About Color Management |publisher=Microsoft |accessdate=2018-12-10 |quote=Usually Windows handles this on its own }}</ref> Increase the gamma slider in the dccw program until the last colored area, often the green color, has the same brightness in checkered and homogeneous area. Use the color balance or individual colors gamma correction sliders in the gamma correction programs to adjust the two other colors. Some old graphics card drivers do not load the [[Palette (computing)|color Look Up Table]] correctly after waking up from standby or hibernate mode and show wrong gamma. In this case update the graphics card driver.\n\nOn some operating systems running the [[X Window System]], one can set the gamma correction factor (applied to the existing gamma value) by issuing the command <code>xgamma -gamma 0.9</code> for setting gamma correction factor to 0.9, and <code>xgamma</code> for querying current value of that factor (the default is 1.0). In [[macOS]] systems, the gamma and other related screen calibrations are made through the System Preferences.\n\n==Terminology==\nThe term [[luminous exitance|intensity]] refers strictly to the amount of light that is emitted per unit of time and per unit of surface, in units of [[lux]]. Note, however, that in many fields of science this quantity is called [[luminous exitance]], as opposed to [[luminous intensity]], which is a different quantity. These distinctions, however, are largely irrelevant to gamma compression, which is applicable to any sort of normalized linear intensity-like scale.\n\n\"Luminance\" can mean several things even within the context of video and imaging:\n* ''[[luminance]]'' is the photometric brightness of an object, taking into account the wavelength-dependent sensitivity of the human eye (in units of [[candela|cd]]/m²);\n* ''[[relative luminance]]'' is the luminance relative to a white level, used in a color-space encoding;\n* ''[[Luma (video)|luma]]'' is the encoded video brightness signal, i.e., similar to the signal voltage ''V''<sub>''S''</sub>.\n\nOne contrasts relative luminance in the sense of color (no gamma compression) with luma in the sense of video (with gamma compression), and denote relative luminance by ''Y'' and luma by ''Y''′, the prime symbol (′) denoting gamma compression.<ref>Engineering Guideline EG 28, \"Annotated Glossary of Essential Terms for Electronic Production,\" SMPTE, 1993.</ref>\nNote that luma is not directly calculated from luminance, it is the (somewhat arbitrary) weighted sum of gamma compressed RGB components.<ref name=poynton/>\n\nLikewise, ''[[brightness]]'' is sometimes applied to various measures, including light levels, though it more properly applies to a subjective visual attribute.\n\nGamma correction is a type of [[power law]] function whose exponent is the [[Greek letter]] [[gamma]] (''γ''). It should not be confused with the mathematical [[Gamma function]]. The lower case gamma, ''γ'', is a [[parameter]] of the former; the upper case letter, Γ, is the name of (and symbol used for) the latter (as in Γ(''x'')). To use the word \"function\" in conjunction with gamma correction, one may avoid confusion by saying \"generalized power law function\".\n\nWithout context, a value labeled gamma might be either the encoding or the decoding value. Caution must be taken to correctly interpret the value as that to be applied-to-compensate or to be compensated-by-applying its inverse. In common parlance, in many occasions the decoding value (as 2.2) is employed as if it were the encoding value, instead of its inverse (1/2.2 in this case), which is the ''real'' value that must be applied to encode gamma.\n\n==See also==\n{{div col}}\n* [[Brightness]]\n* [[Callier Effect]]\n* [[Colour balance]]\n* [[Colour cast]]\n* [[Color management]]\n* [[Color grading]]\n* [[Color temperature]]\n* [[Contrast (vision)]]\n* {{section link|Image editing|Gamma correction}}\n* [[Luminance]]\n* [[Luminance (video)]]\n* [[Luminance (relative)]]\n* [[Optical transfer function]] (OTF)\n* [[Post-production]]\n* [[Telecine]]\n* [[Tone mapping]]\n* [[Video calibration software]]\n* [[White point]]\n{{div col end}}\n\n==References==\n{{reflist}}\n\n==External links==\n{{commons category|Gamma correction}}\n\n===General information===\n* [http://www.w3.org/TR/PNG-GammaAppendix.html PNG Specification; Version 1.0; 13. Appendix: Gamma Tutorial]\n* [http://www.poynton.com/PDFs/Rehabilitation_of_gamma.pdf Rehabilitation of Gamma] by [[Charles Poynton]]\n* [http://www.poynton.com/notes/colour_and_gamma/GammaFAQ.html Frequently Asked Questions about Gamma]\n* [https://web.archive.org/web/20070701001437/http://www.cgsd.com/papers/gamma.html CGSD – Gamma Correction Home Page] by Computer Graphics Systems Development Corporation\n* Stanford University CS 178 [http://graphics.stanford.edu/courses/cs178/applets/gamma.html interactive Flash demo] about gamma correction.\n* [http://www.w3.org/Graphics/Color/sRGB.html A Standard Default Color Space for the Internet – sRGB], defines and explains ''viewing gamma'', ''camera gamma'', ''CRT gamma'', ''LUT gamma'' and ''display gamma''\n* {{cite techreport |title = Gamma Correction |type = Technical Memo 9 |author = [[Alvy Ray Smith]] |institution = [[Microsoft]] |date = 1 September 1995 |url = https://www.cs.princeton.edu/courses/archive/fall00/cs426/papers/smith95d.pdf |format = PDF }}\n\n===Monitor gamma tools===\n* [http://www.lagom.nl/lcd-test/index.php The Lagom LCD monitor test pages]\n* [http://www.photoscientia.co.uk/Gamma.htm The Gamma adjustment page]\n* [http://www.normankoren.com/makingfineprints1A.html#Monitor_test_pattern Monitor test pattern] for correct gamma correction (by Norman Koren)\n* [http://www.quickgamma.de/indexen.html QuickGamma]\n\n[[Category:Display technology]]\n[[Category:Science of photography]]\n[[Category:Power laws]]\n[[Category:Photometry]]"
    },
    {
      "title": "Generalized Pareto distribution",
      "url": "https://en.wikipedia.org/wiki/Generalized_Pareto_distribution",
      "text": "{{About|a particular family of continuous distributions referred to as the generalized Pareto distribution|the hierarchy of generalized Pareto distributions|Pareto distribution}}\n{{Refimprove|date=March 2012}}\n{{Probability distribution\n | name       =Generalized Pareto distribution\n | type       =density\n | pdf_image  = [[File:Gpdpdf.svg|320px|Gpdpdf]]\n | pdf_caption = GPD distribution functions for <math>\\mu=0</math> and different values of <math>\\sigma</math> and <math>\\xi</math>\n | cdf_image  =[[File:Gpdcdf.svg|320px|Gpdcdf]]\n | parameters =\n<math>\\mu \\in (-\\infty,\\infty) \\,</math> [[location parameter|location]] ([[real numbers|real]])<br />\n<math>\\sigma \\in (0,\\infty)    \\,</math> [[scale parameter|scale]] (real)<br />\n<math>\\xi\\in (-\\infty,\\infty)  \\,</math> [[shape parameter|shape]] (real)\n | support    =<math>x \\geqslant \\mu\\,\\;(\\xi \\geqslant 0)</math><br />\n<math>\\mu \\leqslant x \\leqslant \\mu-\\sigma/\\xi\\,\\;(\\xi < 0)</math>\n | pdf        =<math>\\frac{1}{\\sigma}(1 + \\xi z )^{-(1/\\xi +1)} </math><br />\nwhere <math>z=\\frac{x-\\mu}{\\sigma}</math>\n | cdf        =<math>1-(1+\\xi z)^{-1/\\xi} \\,</math>\n | mean       =<math>\\mu + \\frac{\\sigma}{1-\\xi}\\, \\; (\\xi < 1) </math>\n | median     =<math>\\mu + \\frac{\\sigma( 2^{\\xi} -1)}{\\xi} </math>\n | entropy    =<math>\\log(\\sigma) + \\xi + 1</math>\n | mode       =<math></math>\n | skewness   =<math>\\frac{2(1+\\xi)\\sqrt{1-2\\xi}}{(1-3\\xi)}\\,\\;(\\xi<1/3)</math>\n | kurtosis   =<math>\\frac{3(1-2\\xi)(2\\xi^2+\\xi+3)}{(1-3\\xi)(1-4\\xi)}-3\\,\\;(\\xi<1/4)</math>\n | mgf        =<math>e^{\\theta\\mu}\\,\\sum_{j=0}^\\infty \\left[\\frac{(\\theta\\sigma)^j}{\\prod_{k=0}^j(1-k\\xi)}\\right], \\;(k\\xi<1)</math>|\n | cf         =<math>e^{it\\mu}\\,\\sum_{j=0}^\\infty \\left[\\frac{(it\\sigma)^j}{\\prod_{k=0}^j(1-k\\xi)}\\right], \\;(k\\xi<1)</math>\n | variance   =<math>\\frac{\\sigma^2}{(1-\\xi)^2(1-2\\xi)}\\, \\; (\\xi < 1/2)</math>\n  }}\n\nIn [[statistics]], the '''generalized Pareto distribution''' (GPD) is a family of continuous [[probability distribution]]s. It is often used to model the tails of another distribution. It is specified by three parameters: location <math>\\mu</math>, scale <math>\\sigma</math>, and shape <math>\\xi</math>.<ref>{{Cite book |title=An Introduction to Statistical Modeling of Extreme Values |last=Coles |first=Stuart |publisher=Springer |page=75 |url=https://books.google.com/books?id=2nugUEaKqFEC |isbn=9781852334598 |date=2001-12-12}}</ref><ref>{{Cite journal | last1 = Dargahi-Noubary | first1 = G. R. | title = On tail estimation: An improved method | doi = 10.1007/BF00894450 | journal = Mathematical Geology | volume = 21 | issue = 8 | pages = 829–842 | year = 1989 | pmid =  | pmc = }}</ref> Sometimes it is specified by only scale and shape<ref>{{Cite journal | last1 = Hosking | first1 = J. R. M. | last2 = Wallis | first2 = J. R. | title = Parameter and Quantile Estimation for the Generalized Pareto Distribution | journal = Technometrics | volume = 29 | issue = 3 | pages = 339–349 | doi = 10.2307/1269343 | year = 1987 | pmid =  | pmc = | jstor = 1269343 }}</ref> and sometimes only by its shape parameter. Some references give the shape parameter as <math> \\kappa =  - \\xi \\,</math>.<ref>{{Cite book |title=Statistical Extremes and Applications |editor-last=de Oliveira |editor-first=J. Tiago |publisher=Kluwer |last=Davison |first=A. C. |chapter=Modelling Excesses over High Thresholds, with an Application |page=462 |chapter-url=https://books.google.com/books?id=6M03_6rm8-oC&pg=PA462 |isbn=9789027718044 |date=1984-09-30}}</ref>\n\n==Definition==\nThe standard cumulative distribution function (cdf) of the GPD is defined by<ref>{{Cite book |last1=Embrechts |first1=Paul |last2=Klüppelberg |first2=Claudia |last3=Mikosch |first3=Thomas |title=Modelling extremal events for insurance and finance |page=162 |url=https://books.google.com/books?id=BXOI2pICfJUC |isbn=9783540609315 |date=1997-01-01}}</ref>\n\n: <math>F_{\\xi}(z) = \\begin{cases}\n1 - \\left(1+ \\xi z\\right)^{-1/\\xi} & \\text{for }\\xi \\neq 0, \\\\\n1 - e^{-z} & \\text{for }\\xi = 0.\n\\end{cases}\n</math>\n\nwhere the support is <math> z \\geq 0 </math> for <math> \\xi \\geq 0</math> and <math> 0 \\leq z \\leq - 1 /\\xi </math> for <math> \\xi < 0</math>.\n\n: <math>f_{\\xi}(z) = \\begin{cases}\n(\\xi  z+1)^{-\\frac{\\xi +1}{\\xi }} & \\text{for }\\xi \\neq 0, \\\\\ne^{-z} & \\text{for }\\xi = 0.\n\\end{cases}\n</math>\n\n==Characterization==\nThe related location-scale family of distributions is obtained by replacing the argument ''z'' by <math>\\frac{x-\\mu}{\\sigma}</math> and adjusting the support accordingly: The [[cumulative distribution function]] is\n\n: <math>F_{(\\xi,\\mu,\\sigma)}(x) = \\begin{cases}\n1 - \\left(1+ \\frac{\\xi(x-\\mu)}{\\sigma}\\right)^{-1/\\xi} & \\text{for }\\xi \\neq 0, \\\\\n1 - \\exp \\left(-\\frac{x-\\mu}{\\sigma}\\right) & \\text{for }\\xi = 0.\n\\end{cases}\n</math>\n\nfor <math> x \\geqslant \\mu </math> when <math> \\xi \\geqslant 0 \\,</math>, and <math> \\mu \\leqslant x \\leqslant \\mu - \\sigma /\\xi </math>  when <math> \\xi < 0</math>, where <math>\\mu\\in\\mathbb R</math>, <math>\\sigma>0</math>, and <math>\\xi\\in\\mathbb R</math>.\n\nThe [[probability density function]] (pdf) is\n\n: <math>f_{(\\xi,\\mu,\\sigma)}(x) = \\frac{1}{\\sigma}\\left(1 + \\frac{\\xi (x-\\mu)}{\\sigma}\\right)^{\\left(-\\frac{1}{\\xi} - 1\\right)}</math>,\n\nagain, for <math> x \\geqslant \\mu </math> when <math> \\xi \\geqslant 0</math>, and <math> \\mu \\leqslant x \\leqslant \\mu - \\sigma /\\xi </math>  when <math> \\xi < 0</math>.\n\nThe pdf is a solution of the following [[differential equation]]:\n\n:<math>\\left\\{\\begin{array}{l}\nf'(x) (-\\mu \\xi +\\sigma+\\xi x)+(\\xi+1) f(x)=0, \\\\\nf(0)=\\frac{\\left(1-\\frac{\\mu \\xi}{\\sigma}\\right)^{-\\frac{1}{\\xi }-1}}{\\sigma}\n\\end{array}\\right\\}\n</math>\n\n==Special cases==\n*If the shape <math>\\xi</math> and location <math>\\mu</math> are both zero, the GPD is equivalent to the [[exponential distribution]].\n*With shape <math>\\xi > 0</math> and location <math>\\mu = \\sigma/\\xi</math>, the GPD is equivalent to the [[Pareto distribution]] with scale <math>x_m=\\sigma/\\xi</math> and shape <math>\\alpha=1/\\xi</math>.\n*If <math> X </math> <math>\\sim</math>  <math>GPD</math> <math>(</math><math>\\mu  = 0</math>, <math>\\sigma</math>, <math>\\xi</math> <math>)</math>, then <math> Y = \\log (X)</math> <math>\\sim</math> <math>exGPD</math> <math>(</math><math>\\mu  = 0 </math>, <math>\\sigma</math>, <math>\\xi</math> <math>)</math>, where exGPD stands for the [https://www.tandfonline.com/doi/abs/10.1080/03610926.2018.1441418 exponentiated generalized Pareto distribution]. Unlike GPD, exGPD has the finite moments of all orders and possesses separate interpretations for the scale parameter and the shape parameter, which leads to stable and efficient parameter estimation than using GPD.\n*GPD is similar to the [[Burr distribution]].\n\n== Generating generalized Pareto random variables ==\nIf ''U'' is [[uniform distribution (continuous)|uniformly distributed]] on\n(0,&nbsp;1<nowiki>]</nowiki>, then\n\n:<math> X = \\mu + \\frac{\\sigma (U^{-\\xi}-1)}{\\xi} \\sim \\mbox{GPD}(\\mu, \\sigma, \\xi \\neq 0)</math>\nand\n:<math> X = \\mu - \\sigma \\ln(U) \\sim \\mbox{GPD}(\\mu,\\sigma,\\xi =0).</math>\n\nBoth formulas are obtained by inversion of the cdf.\n\nIn Matlab Statistics Toolbox, you can easily use \"gprnd\" command to generate generalized Pareto random numbers.\n\n=== GPD as an Exponential-Gamma Mixture ===\n\nA GPD random variable can also be expressed as an exponential random variable, with a Gamma distributed rate parameter. \n\n:<math>X|\\Lambda \\sim Exp(\\Lambda)   </math>\nand\n:<math>\\Lambda \\sim Gamma(\\alpha, \\beta)   </math>\nthen\n:<math>X \\sim GPD(\\xi = 1/\\alpha, \\ \\sigma = \\beta/\\alpha)   </math>\n\nNotice however, that since the parameters for the Gamma distribution must be greater than zero, we obtain the additional restrictions that:<math>\\xi</math> must be positive.\n\n==See also==\n*[[Burr distribution]]\n*[[Pareto distribution]]\n*[[Generalized extreme value distribution]]\n*[https://www.tandfonline.com/doi/abs/10.1080/03610926.2018.1441418 Exponentiated generalized Pareto distribution]\n*[[Pickands–Balkema–de Haan theorem]]\n\n==References==\n{{Reflist|refs}}\n\n==Further reading==\n* {{Cite journal |last=Pickands |first=James |title=Statistical inference using extreme order statistics |journal=Annals of Statistics |volume=3 |year=1975 |pages=119–131 |doi=10.1214/aos/1176343003 }}\n* {{Cite journal |last1=Balkema |first1=A. |title=Residual life time at great age |journal=Annals of Probability |volume=2 |year=1974 |pages=792–804 |doi=10.1214/aop/1176996548 |first2=Laurens |last2=De Haan |issue=5 |authorlink2=Laurens de Haan }}\n* {{Cite journal |last1=Lee|first1=Seyoon |title = Exponentiated generalized Pareto distribution:Properties and applications towards extreme value theory |journal=Communications in Statistics - Theory and Methods|volume=0 |year=2018|pages=1–25 |doi=10.1080/03610926.2018.1441418 |first2=J.H.K. |last2=Kim |arxiv=1708.01686 }}\n* {{cite book|title=Continuous Univariate Distributions Volume 1, second edition|author1=N. L. Johnson |author2=S. Kotz |author3=N. Balakrishnan |publisher=Wiley|location=New York|year=1994|isbn=978-0-471-58495-7|ref=harv}} Chapter 20, Section 12: Generalized Pareto Distributions.\n* {{cite book|editor= Duangkamon Chotikapanich|year=2011|title=Modeling Distributions and Lorenz Curves|publisher=Springer|location=New York|author=Barry C. Arnold|chapter=Chapter 7: Pareto and Generalized Pareto Distributions|chapter-url=https://books.google.com/books?id=fUJZZLj1kbwC&lpg=PR1&pg=PA119#v=onepage&q&f=false|ref=harv|isbn= 9780387727967}}\n* {{cite book|author1=Arnold, B. C.  |author2=Laguna, L.|year=1977|title= On generalized Pareto distributions with applications to income data|location= Ames, Iowa| publisher=Iowa State University, Department of Economics|ref=harv}}\n\n==External links==\n*[http://www.mathworks.com/help/stats/generalized-pareto-distribution.html Mathworks: Generalized Pareto distribution]\n\n{{ProbDistributions|continuous-variable}}\n\n[[Category:Continuous distributions]]\n[[Category:Power laws]]\n[[Category:Probability distributions with non-finite variance]]"
    },
    {
      "title": "Hack's law",
      "url": "https://en.wikipedia.org/wiki/Hack%27s_law",
      "text": "'''Hack's law''' is an [[empirical relationship]] between the length of [[streams]] and the area of their [[Drainage basin|basins]]. If ''L'' is the length of the longest stream in a basin, and ''A'' is the area of the basin, then Hack's law may be written as\n\n:<math>L = C A^h\\ </math>\n\nfor some constant ''C'' where the [[exponent]] ''h'' is slightly less than 0.6 in most basins. ''h'' varies slightly from region to region and slightly decreases for larger basins (>8,000&nbsp;mi², or 20,720&nbsp;km²). In addition to the catchment-scales, Hack's law was observed on unchanneled small-scale surfaces when the morphology measured at high resolutions (Cheraghi et al., 2018).\n\nThe law is named after American geomorphologist [[John Tilton Hack]]. \n\n== References ==\n* [[John Tilton Hack|Hack, J.]], 1957, \"Studies of longitudinal stream profiles in Virginia and Maryland\", ''U.S. Geological Survey Professional Paper'', 294-B.\n* Rigon, R., et al., 1996, \"On Hack's law\" ''Water Resources Research'', '''32''', 11, pp.&nbsp;3367–3374.\n* Willemin, J.H., 2000, \"Hack’s law: Sinuosity, convexity, elongation\". ''Water Resources Research'', '''36''', 11, pp.&nbsp;3365–3374.\n*Cheraghi, M., Rinaldo, A., Sander, G. C., Perona, P., & Barry, D. A. (2018). Catchment drainage network scaling laws found experimentally in over-land flow morphologies. Geophysical Research Letters, 45, 9614–9622. https://doi.org/10.1029/2018GL078351\n{{river morphology}}\n\n[[Category:Hydrology]]\n[[Category:Rivers]]\n[[Category:Geomorphology]]\n[[Category:Water streams]]\n[[Category:Power laws]]"
    },
    {
      "title": "Holtsmark distribution",
      "url": "https://en.wikipedia.org/wiki/Holtsmark_distribution",
      "text": "{{Probability distribution\n  | name       = Holtsmark\n  | type       = continuous\n  | pdf_image  = [[File:Levy distributionPDF.png|325px|Symmetric stable distributions]]<br /><small>Symmetric ''α''-stable distributions with unit scale factor; ''α''=1.5 (blue line) represents the Holtsmark distribution</small>\n  | cdf_image  = [[File:Levy distributionCDF.png|325px|CDF's for symmetric ''α''-stable distributions; ''α''=3/2 represents the Holtsmark distribution]]\n  | parameters = ''c'' ∈ (0, ∞) — [[scale parameter]] <br>\n''μ'' ∈ (−∞, ∞) — [[location parameter]]\n  | support    = ''x'' ∈ '''R'''\n  | pdf        = expressible in terms of [[hypergeometric function]]s; see text\n  | cdf        = \n  | mean       = ''μ'' \n  | median     = ''μ''\n  | mode       = ''μ'' \n  | variance   =  infinite\n  | skewness   =  undefined\n  | kurtosis   =  undefined\n  | entropy    = \n  | mgf        = undefined\n  | char       = <math>\\exp\\left[~it\\mu\\!-\\!|c t|^{3/2}~\\right]</math>\n  }}\n\nThe (one-dimensional) '''Holtsmark distribution''' is a [[continuous probability distribution]].  The Holtsmark distribution is a special case of a [[stable distribution]] with the index of stability or shape parameter <math>\\alpha</math> equal to 3/2 and skewness parameter <math>\\beta</math> of zero.  Since <math>\\beta</math> equals zero, the distribution is symmetric, and thus an example of a  symmetric alpha-stable distribution.  The Holtsmark distribution is one of the few examples of a stable distribution for which a closed form expression of the [[probability density function]] is known.  However, its probability density function is not expressible in terms of [[elementary functions]]; rather, the probability density function is expressed in terms of [[hypergeometric functions]].\n\nThe Holtsmark distribution has applications in plasma physics and astrophysics.<ref name=lee/>  In 1919, Norwegian physicist [[Johan Peter Holtsmark|J. Holtsmark]] proposed the distribution as a model for the fluctuating fields in plasma due to [[chaotic]] motion of charged particles.<ref>{{Cite journal\n| doi = 10.1002/andp.19193630702\n| volume = 363\n| issue = 7\n| pages = 577–630\n| last = Holtsmark\n| first = J.\n| title = Uber die Verbreiterung von Spektrallinien\n| journal = Annalen der Physik\n| year = 1919\n|bibcode = 1919AnP...363..577H }}</ref> It is also applicable to other types of Coulomb forces, in particular to modeling of gravitating bodies, and thus is important in astrophysics.<ref>{{Cite journal\n| doi = 10.1086/144420\n| issn = 0004-637X\n| volume = 95\n| pages = 489\n| last = Chandrasekhar\n| first = S.\n|author2=J. von Neumann\n| title = The Statistics of the Gravitational Field Arising from a Random Distribution of Stars. I. The Speed of Fluctuations\n| journal = The Astrophysical Journal\n| year = 1942\n| bibcode=1942ApJ....95..489C\n}}</ref><ref>{{Cite journal\n| doi = 10.1103/RevModPhys.15.1\n| volume = 15\n| issue = 1\n| pages = 1–89\n| last = Chandrasekhar\n| first = S.\n| title = Stochastic Problems in Physics and Astronomy\n| journal = Reviews of Modern Physics\n| date = 1943-01-01\n| bibcode=1943RvMP...15....1C\n}}</ref>\n\n==Characteristic function==\n\nThe [[characteristic function (probability theory)|characteristic function]] of a symmetric stable distribution is:\n\n: <math>\n\\varphi(t;\\mu,c) = \n\\exp\\left[~it\\mu\\!-\\!|c t|^\\alpha~\\right],\n</math>\n\nwhere <math>\\alpha</math> is the shape parameter, or index of stability, <math>\\mu</math> is the [[location parameter]], and ''c'' is the [[scale parameter]].\n\nSince the Holtsmark distribution has <math>\\alpha=3/2,</math> its characteristic function is:<ref name=zolotarev>{{cite book|title=One-Dimensional Stable Distributions|author=Zolotarev, V. M.|pages=1, 41|year=1986|location=Providence, RI|publisher=[[American Mathematical Society]]|isbn=978-0-8218-4519-6|url=https://books.google.com/books?id=ydwt9SotnN0C&pg=PR7&dq=Vladimir+Zolotarev+One-dimensional+stable+laws#v=onepage&q=holtsmark&f=false}}</ref>\n\n: <math>\n\\varphi(t;\\mu,c) = \n\\exp\\left[~it\\mu\\!-\\!|c t|^{3/2}~\\right] .\n</math>\n\nSince the Holtsmark distribution is a stable distribution with {{nowrap|''α'' > 1}}, <math>\\mu</math> represents the [[mean]] of the distribution.<ref name=nolan>{{cite book|chapter=Basic Properties of Univariate Stable Distributions|title=Stable Distributions: Models for Heavy Tailed Data|author=Nolan, J. P.|pages=3, 15–16|chapter-url=http://lpmt-theory.wdfiles.com/local--files/michael-blog/stablePDF.pdf|year=2008|accessdate=2011-02-06}}</ref><ref>{{cite book|title=Handbook of Heavy Tailed Distributions in Finance|editor=Rachev, S. T.|chapter=Modeling Financial Data|pages=111–112|author=Nolan, J. P.|year=2003|location=Amsterdam|publisher=[[Elsevier]]|isbn=978-0-444-50896-6}}</ref>  Since {{nowrap|''β'' {{=}} 0}}, <math>\\mu</math> also represents the [[median]] and [[Mode (statistics)|mode]] of the distribution. And since {{nowrap|''α'' < 2}}, the [[variance]] of the Holtsmark distribution is infinite.<ref name=nolan/>  All higher [[Moment (mathematics)|moments]] of the distribution are also infinite.<ref name=nolan/>  Like other stable distributions (other than the normal distribution), since the variance is infinite the dispersion in the distribution is reflected by the [[scale parameter]], c.  An alternate approach to describing the dispersion of the distribution is through fractional moments.<ref name=nolan/>\n\n==Probability density function==\n\nIn general, the [[probability density function]], ''f''(''x''), of a continuous probability distribution can be derived from its characteristic function by:\n\n:<math>\nf(x)=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\varphi(t)e^{-ixt}\\,dt .\n</math>\n\nMost stable distributions do not have a known closed form expression for their probability density functions.  Only the [[normal distribution|normal]], [[Cauchy distribution|Cauchy]] and [[Lévy distribution]]s have known closed form expressions in terms of [[elementary functions]].<ref name=lee/>  The Holtsmark distribution is one of two symmetric stable distributions to have a known closed form expression in terms of [[generalized hypergeometric function|hypergeometric functions]].<ref name=lee/>  When <math>\\mu</math> is equal to 0 and the scale parameter is equal to 1, the Holtsmark distribution has the probability density function:\n\n:<math>\n\\begin{align}\nf(x; 0, 1)\n&= { 1 \\over \\pi }\\, \\Gamma\\left({5 \\over 3}\\right) {_2F_3}\\!\\left({5 \\over 12}, {11 \\over 12}; {1 \\over 3}, {1 \\over 2}, {5 \\over 6}; -{4x^6 \\over 729}\\right) \\\\\n& {} \\quad{} - { x^2 \\over 3\\pi }\\, {_3F_4}\\!\\left({3 \\over 4}, {1}, {5 \\over 4}; {2 \\over 3}, {5 \\over 6}, {7 \\over 6}, {4 \\over 3}; -{4x^6 \\over 729}\\right) \\\\\n& {} \\quad{} + { 7x^4 \\over 81\\pi }\\, \\Gamma\\left({4 \\over 3}\\right) {_2F_3}\\!\\left({13 \\over 12}, {19 \\over 12}; {7 \\over 6}, {3 \\over 2}, {5 \\over 3}; -{4x^6 \\over 729}\\right),\n\\end{align}\n</math>\n\nwhere <math>{\\Gamma(x)}</math> is the [[gamma function]] and <math> \\;_mF_n()</math> is a [[generalized hypergeometric function|hypergeometric function]].<ref name=lee>{{cite book|title=Continuous and Discrete Properties of Stochastic Processes|author=Lee, W. H.|pages=37–39|type=PhD thesis|year=2010|location=[[University of Nottingham]]|url=http://etheses.nottingham.ac.uk/11194/1/Thesis_Wai_Ha_Lee.pdf}}</ref>\n\n==References==\n{{reflist}}\n\n*{{Cite journal | last1 = Hummer | first1 = D. G. | doi = 10.1016/0022-4073(86)90011-7 | title = Rational approximations for the holtsmark distribution, its cumulative and derivative | journal = Journal of Quantitative Spectroscopy and Radiative Transfer | volume = 36 | pages = 1–5| year = 1986 | pmid =  | pmc = | url = https://zenodo.org/record/1253952/files/article.pdf}}\n\n{{ProbDistributions|continuous-infinite}}\n\n[[Category:Continuous distributions]]\n[[Category:Probability distributions with non-finite variance]]\n[[Category:Power laws]]\n[[Category:Stable distributions]]\n[[Category:Location-scale family probability distributions]]"
    },
    {
      "title": "Kleiber's law",
      "url": "https://en.wikipedia.org/wiki/Kleiber%27s_law",
      "text": "'''Kleiber's law''', named after [[Max Kleiber]] for his biology work in the early 1930s, is the observation that, for the vast majority of animals, an animal's [[basal metabolic rate|metabolic rate]] scales to the ¾ power of the animal's mass. Symbolically: if ''q''<sub>0</sub> is the animal's metabolic rate, and ''M'' the animal's mass, then Kleiber's law states that ''q''<sub>0</sub>&nbsp;~&nbsp;''M''<sup>¾</sup>. Thus, over the same timespan, a cat having a mass 100 times that of a mouse will consume only about 32 times the energy the mouse uses. \n\nThe exact value of the exponent in Kleiber's law is unclear, in part because there is currently no completely satisfactory theoretical explanation for the law.  \n\n[[image:Kleiber1947.jpg|right|thumb|400px|Kleiber's plot comparing body size to metabolic rate for a variety of species.<ref>{{cite journal | vauthors = Kleiber M | title = Body size and metabolic rate | journal = Physiological Reviews | volume = 27 | issue = 4 | pages = 511–41 | date = October 1947 | pmid = 20267758 | doi = 10.1152/physrev.1947.27.4.511 }}</ref> ]]\n\n==Proposed explanations for the law==\nKleiber's law, as many other biological [[allometric law]]s, is a consequence of the [[physics]] and/or [[geometry]] of animal [[circulatory system]]s.<ref>{{cite book|title=Scaling: Why is animal size so important?|last=Schmidt-Nielsen|first=Knut|publisher=Cambridge University Press|year=1984|isbn=978-0521266574|location=NY, NY}}</ref>  Max Kleiber first discovered the law when analyzing a large number of independent studies on respiration within individual species.<ref>{{cite journal| first = Max | last = Kleiber | name-list-format = vanc |year=1932|title=Body size and metabolism|journal=Hilgardia|volume=6| issue = 11 |pages=315–351|doi=10.3733/hilg.v06n11p315}}</ref>  Kleiber expected to find an exponent of {{Frac|2|3}} (for reasons explained below), and was confounded by the exponent of {{Frac|3|4}} he discovered.<ref name=\"balles\">{{cite journal | vauthors = Ballesteros FJ, Martinez VJ, Luque B, Lacasa L, Valor E, Moya A | title = On the thermodynamic origin of metabolic scaling | journal = Scientific Reports | volume = 8 | issue = 1 | pages = 1448 | date = January 2018 | pmid = 29362491 | pmc = 5780499 | doi = 10.1038/s41598-018-19853-6 | ref = harvid | bibcode = 2018NatSR...8.1448B }}</ref>  \n\n=== Heuristic explanation ===\nOne explanation for Kleiber's law lies in the difference between structural and growth mass.  Structural mass involves maintenance costs, reserve mass does not.  Hence, small adults of one species respire more per unit of weight than large adults of another species because a larger fraction of their body mass consists of structure rather than reserve.{{cite needed|date=January 2019}}  Within each species, young (i.e., small) organisms respire more per unit of weight than old (large) ones of the same species because of the overhead costs of growth.<ref name=\"History\" />    \n\n=== Exponent {{Frac|2|3}} ===\nExplanations for {{Frac|2|3}}-scaling tend to assume that metabolic rates scale to avoid [[heat exhaustion]].  Because bodies lose heat passively via their surface, but produce heat metabolically throughout their mass, the metabolic rate must scale in such a way as to counteract the [[square–cube law]].  The precise exponent to do so is {{Frac|2|3}}.<ref name=\"Empirics\">{{cite journal | vauthors = Dodds PS, Rothman DH, Weitz JS | title = Re-examination of the \"3/4-law\" of metabolism | journal = Journal of Theoretical Biology | volume = 209 | issue = 1 | pages = 9–27 | date = March 2001 | pmid = 11237567 | doi = 10.1006/jtbi.2000.2238 | arxiv = physics/0007096 }}</ref>  \n\nSuch an argument does not address the fact that different organisms exhibit different shapes (and hence have different [[Meeh factor|surface-to-volume ratios]], even when scaled to the same size).  Reasonable estimates for organisms' surface area do appear to scale linearly with the metabolic rate.<ref name=\"History\">{{cite journal|last=Hulbert|first=A. J.|date=28 April 2014|title=A {{As written|Sceptics}} View: \"Kleiber's Law\" or the \"3/4 Rule\" is neither a Law nor a Rule but Rather an Empirical Approximation|url=https://www.mdpi.com/2079-8954/2/2/186|journal=Systems | volume=2|issue=2|pages=186–202|doi=10.3390/systems2020186|ref=harvid}}</ref>    \n\n=== Exponent {{Frac|3|4}} ===\nA model due to [[Geoffrey West|West]], [[Brian Enquist|Enquist]], and Brown (hereafter WEB) suggests that {{Frac|3|4}}-scaling arises because of efficiency in nutrient distribution and transport throughout an organism.  In most organisms, metabolism is supported by a circulatory system featuring branching tubules (i.e., plant vascular systems, insect tracheae, or the human cardiovascular system).  WEB claim that (1) metabolism should scale proportionally to nutrient flow (or, equivalently, total fluid flow) in this circulatory system and (2) in order to minimize the energy dissipated in transport, the volume of fluid used to transport nutrients (i.e., blood volume) is a fixed fraction of body mass.<ref name=\"WBEModel\">{{cite journal | vauthors = West GB, Brown JH, Enquist BJ | title = A general model for the origin of allometric scaling laws in biology | journal = Science | volume = 276 | issue = 5309 | pages = 122–6 | date = April 1997 | pmid = 9082983 | doi = 10.1126/science.276.5309.122 }}</ref>  \n\nThey then proceed by analyzing the consequences of these two claims at the level of the smallest circulatory tubules (capillaries, alveoli, etc.).  Experimentally, the volume contained in those smallest tubules is constant across a wide range of masses.  Because fluid flow through a tubule is determined by the volume thereof, the total fluid flow is proportional to the total number of smallest tubules.  Thus, if {{Mvar|B}} denotes the basal metabolic rate, {{Mvar|Q}} the total fluid flow, and {{Mvar|N}} the number of minimal tubules,<blockquote><math>B\\propto Q\\propto N</math>.</blockquote>Circulatory systems do not grow by simply scaling proportionally larger; they become [[Fractal|more deeply nested]].  The depth of nesting depends on the [[Fractal dimension|self-similarity exponents]] of the tubule dimensions, and the effects of that depth depend on how many \"child\" tubules each branching produces.  Connecting these values to macroscopic quantities depends (very loosely) on a precise model of tubules.  WEB show that, if the tubules are well-approximated by rigid cylinders, then, in order to prevent the fluid from [[Incompressible flow|\"getting clogged\"]] in small cylinders, the total fluid volume {{Mvar|V}} satisfies<blockquote><math>N^4\\propto V^3</math>.<ref name=\"Reformulated\" /></blockquote>Because blood volume is a fixed fraction of body mass, <blockquote><math>B\\propto M^{\\frac{3}{4}}</math>.<ref name=\"WBEModel\" /></blockquote>\n\n=== Non-power-law scaling ===\nCloser analysis suggests that Kleiber's law does not hold over a wide variety of scales.  Metabolic rates for smaller animals (birds under {{Cvt|10|kg||disp=sqbr}}, or insects) typically fit to {{frac|2|3}} much better than {{frac|3|4}}; for larger animals, the reverse holds.<ref name=\"Empirics\" />  As a result, log-log plots of metabolic rate versus body mass appear to \"curve\" upward, and fit better to quadratic models.<ref>{{cite journal | vauthors = Kolokotrones T, Deeds EJ, Fontana W | title = Curvature in metabolic scaling | journal = Nature | volume = 464 | issue = 7289 | pages = 753–6 | date = April 2010 | pmid = 20360740 | doi = 10.1038/nature08920 | bibcode = 2010Natur.464..753K }}  <br />But note that a quadratic curve has undesirable theoretical implications; see {{cite journal | vauthors = MacKay NJ | title = Mass scale and curvature in metabolic scaling. Comment on: T. Kolokotrones et al., curvature in metabolic scaling, Nature 464 (2010) 753-756 | journal = Journal of Theoretical Biology | volume = 280 | issue = 1 | pages = 194–6 | date = July 2011 | pmid = 21335012 | doi = 10.1016/j.jtbi.2011.02.011 }}</ref>  In all cases, local fits exhibit exponents in the {{Math|[{{frac|2|3}},{{frac|3|4}}]}} range.<ref name=\"ArbNetworks\" />  \n\n==== Modified circulatory models ====\nAdjustments to the WBE model that retain assumptions of network shape predict ''larger'' scaling exponents, worsening the discrepancy with observed data.<ref>{{cite journal | vauthors = Savage VM, Deeds EJ, Fontana W | title = Sizing up allometric scaling theory | journal = PLoS Computational Biology | volume = 4 | issue = 9 | pages = e1000171 | date = September 2008 | pmid = 18787686 | pmc = 2518954 | doi = 10.1371/journal.pcbi.1000171 }}</ref>  But one can retain a similar theory by relaxing WBE's assumption of a nutrient transport network that is both [[fractal]] and circulatory.<ref name=\"ArbNetworks\" />  (WBE argued that fractal circulatory networks would necessarily evolve to minimize energy used for transport, but other researchers argue that their derivation contains subtle errors.<ref name=\"Empirics\" /><ref>{{cite journal| vauthors = Apol ME, Etienne RS, Olff H |date=2008|title=Revisiting the evolutionary origin of allometric metabolic scaling in biology |journal=Functional Ecology | volume=22|issue=6|pages=1070–1080|doi=10.1111/j.1365-2435.2008.01458.x }}</ref>)  Different networks are less efficient, in that they exhibit a lower scaling exponent, but a metabolic rate determined by nutrient transport will always exhibit scaling between {{frac|2|3}} and {{frac|3|4}}.<ref name=\"ArbNetworks\">{{cite journal | vauthors = Banavar JR, Moses ME, Brown JH, Damuth J, Rinaldo A, Sibly RM, Maritan A | title = A general basis for quarter-power scaling in animals | journal = Proceedings of the National Academy of Sciences of the United States of America | volume = 107 | issue = 36 | pages = 15816–20 | date = September 2010 | pmid = 20724663 | pmc = 2936637 | doi = 10.1073/pnas.1009974107 | bibcode = 2010PNAS..10715816B }}</ref>  If larger metabolic rates are evolutionarily favored, then low-mass organisms will prefer to arrange their networks to scale as {{frac|2|3}}, but large-mass organisms will prefer to arrange their networks as {{frac|3|4}}, which produces the observed curvature.<ref name = \"Savage_2004\">{{cite journal| vauthors = Savage VM, Gillooly JF, Woodruff WH, West GB, Allen AP, Enquist BJ, Brown JH |title=The predominance of quarter-power scaling in biology |journal=Functional Ecology | date = April 2004 |volume=18|issue=2|pages=257–282|doi=10.1111/j.0269-8463.2004.00856.x |quote=The original paper by West ''et al''. (1997), which derives a  model  for  the  mammalian  arterial  system, predicts that  smaller  mammals  should  show  consistent deviations in the direction of  higher metabolic rates than expected from {{math|''M''{{sup|{{frac|3|4}}}}}} scaling. Thus, metabolic scaling relationships are predicted to show a slight curvilinearity  at  the  smallest  size  range.}}</ref>  \n\n==== Modified thermodynamic models ====\nAn alternative model notes that metabolic rate does not solely serve to generate heat.  Metabolic rate contributing solely to useful work should scale with power 1 (linearly), whereas metabolic rate contributing to heat generation should be limited by surface area and scale with power {{Frac|2|3}}.  Basal metabolic rate is then the [[convex combination]] of these two effects: if the proportion of useful work is {{Mvar|f}}, then the basal metabolic rate should scale as<blockquote><math>B=f\\cdot kM+(1-f)\\cdot k'M^{\\frac{2}{3}}</math></blockquote>where {{Mvar|k}} and {{Math|''k''{{prime}}}} are constants of proportionality.  {{Math|''k''{{prime}}}} in particular describes the [[Meeh factor|surface area ratio]] of organisms and is approximately {{Math|0.1&nbsp;{{frac|kJ|hr}}&middot;g{{sup|-{{frac|2|3}}}}}}<ref name=\"balles\" />; typical values for {{Mvar|f}} are 15-20%.<ref>{{cite book|url=https://books.google.com/books/about/Thermodynamic_Bases_of_Biological_Proces.html|title=Thermodynamic Bases of Biological Processes: Physiological Reactions and Adaptations|last=Zotin|first=A. I.|date=1990|publisher=Walter de Gruyter|isbn=9783110114010|language=en}}</ref>  The theoretical maximum value of {{Mvar|f}} is 21%, because the efficiency of [[Glucose oxidation reaction|Glucose oxidation]] is only 42%, and half of the [[Adenosine triphosphate|ATP]] so produced is wasted.<ref name=\"balles\" />    \n\n==Experimental support==\nAnalyses of variance for a variety of physical variables suggest that although most variation in basal metabolic rate is determined by mass, additional variables with significant effects include body temperature and taxonomic order.<ref>{{cite journal | vauthors = Clarke A, Rothery P, Isaac NJ | title = Scaling of basal metabolic rate with body mass and temperature in mammals | journal = The Journal of Animal Ecology | volume = 79 | issue = 3 | pages = 610–9 | date = May 2010 | pmid = 20180875 | doi = 10.1111/j.1365-2656.2010.01672.x }}</ref><ref>{{cite journal | vauthors = Hayssen V, Lacy RC | title = Basal metabolic rates in mammals: taxonomic differences in the allometry of BMR and body mass | journal = Comparative Biochemistry and Physiology. A, Comparative Physiology | volume = 81 | issue = 4 | pages = 741–54 | pmid = 2863065 | doi = 10.1016/0300-9629(85)90904-1 | year = 1985 }}</ref>\n\nA 1932 work by Brody calculated that the scaling was approximately 0.73.<ref name=\"History\" /><ref>{{cite book|title=Bioenergetics and Growth|last=Brody|first=S.|publisher=Reinhold|year=1945|location=NY, NY}}</ref>\n\nA 2004 analysis of field metabolic rates for mammals conclude that they appear to scale with exponent 0.749.<ref name = \"Savage_2004\" /> \n\n== Criticism of the law ==\nKozlowski and Konarzewski (hereafter \"K&K\") have argued that attempts to explain Kleiber's law via any sort of limiting factor is flawed, because metabolic rates vary by factors of 4-5 between rest and activity.  Hence any limits that affect the scaling of ''basal'' metabolic rate would in fact make elevated metabolism — and hence all animal activity — impossible.<ref>{{cite journal|vauthors=Kozlowski J, Konarzewski M|year=2004|title=Is West, Brown and Enquist's model of allometric scaling mathematically correct and biologically relevant?|journal=Functional Ecology|volume=18|issue=2|pages=283–9|doi=10.1111/j.0269-8463.2004.00830.x}}</ref>  WEB conversely argue that animals may well optimize for minimal transport energy dissipation during rest, without abandoning the ability for less efficient function at other times.<ref>{{cite journal| vauthors = Brown JH, West GB, Enquist BJ |date=2005|title=Yes, West, Brown and Enquist's model of allometric scaling is both mathematically correct and biologically relevant |journal=Functional Ecology | volume=19|issue=4|pages=735–738|doi=10.1111/j.1365-2435.2005.01022.x }}</ref>\n\nOther researchers have also noted that K&K's criticism of the law tends to focus on precise structural details of the WEB circulatory networks, but that the latter are not essential to the model.<ref name=\"Reformulated\">{{cite journal| vauthors = Etienne RS, Apol ME, Olff HA |date=2006 |title=Demystifying the West, Brown & Enquist model of the allometry of metabolism |journal=Functional Ecology | volume=20 |issue=2 |pages=394–399 |doi=10.1111/j.1365-2435.2006.01136.x }}</ref>  \n\nKleiber's law only appears when studying animals as a whole; scaling exponents within taxonomic subgroupings differ substantially.<ref>{{cite journal | vauthors = White CR, Blackburn TM, Seymour RS | title = Phylogenetically informed analysis of the allometry of Mammalian Basal metabolic rate supports neither geometric nor quarter-power scaling | journal = Evolution; International Journal of Organic Evolution | volume = 63 | issue = 10 | pages = 2658–67 | date = October 2009 | pmid = 19519636 | doi = 10.1111/j.1558-5646.2009.00747.x }}</ref><ref>{{cite journal | vauthors = Sieg AE, O'Connor MP, McNair JN, Grant BW, Agosta SJ, Dunham AE | title = Mammalian metabolic allometry: do intraspecific variation, phylogeny, and regression models matter? | journal = The American Naturalist | volume = 174 | issue = 5 | pages = 720–33 | date = November 2009 | pmid = 19799501 | doi = 10.1086/606023 }}</ref>  \n\n== Generalizations ==\nKleiber's law only applies to interspecific comparisons; it (usually) does not apply to intraspecific ones.<ref>{{cite journal|last=Heusner|first=A. A.|date=1982-04-01|title=Energy metabolism and body size I. Is the 0.75 mass exponent of Kleiber's equation a statistical artifact?|url=http://www.sciencedirect.com/science/article/pii/0034568782900469|journal=Respiration Physiology|volume=48|issue=1|pages=1–12|doi=10.1016/0034-5687(82)90046-9|pmid=7111915|issn=0034-5687}}</ref>  \n\n=== In other kingdoms ===\nA 1999 analysis concluded that biomass production in a given plant scaled with the {{Frac|3|4}} power of the plant's mass during the plant's growth,<ref>{{cite journal| vauthors = Enquist BJ, West GB, Charnov EL, Brown JH |date=28 October 1999|title=Allometric scaling of production and life-history variation in vascular plants|url=https://www.nature.com/articles/44819|journal=Nature | volume=401|issue=6756|pages=907–911|doi=10.1038/44819|issn=1476-4687}}  <br />\n[https://www.nature.com/articles/35047140 Corrigendum] published 7 December 2000.</ref> but a 2001 paper that included various types of unicellular photosynthetic organisms found scaling exponents intermediate between 0.75 and 1.00.<ref>{{cite journal | vauthors = Niklas KJ | title = A phyletic perspective on the allometry of plant biomass-partitioning patterns and functionally equivalent organ-categories | journal = The New Phytologist | volume = 171 | issue = 1 | pages = 27–40 | date = 2006 | pmid = 16771980 | doi = 10.1111/j.1469-8137.2006.01760.x }}</ref>  \n\nA 2006 paper in ''Nature'' argued that the exponent of mass is close to 1 for plant seedlings, but that variation between species, phyla, and growth conditions overwhelm any \"Kleiber's law\"-like effects.<ref>{{cite journal | vauthors = Reich PB, Tjoelker MG, Machado JL, Oleksyn J | title = Universal scaling of respiratory metabolism, size and nitrogen in plants | journal = Nature | volume = 439 | issue = 7075 | pages = 457–61 | date = January 2006 | pmid = 16437113 | doi = 10.1038/nature04282 | bibcode = 2006Natur.439..457R }}  <br />\nFor a contrary view, see {{cite journal | vauthors = Enquist BJ, Allen AP, Brown JH, Gillooly JF, Kerkhoff AJ, Niklas KJ, Price CA, West GB | title = Biological scaling: does the exception prove the rule? | journal = Nature | volume = 445 | issue = 7127 | pages = E9–10; discussion E10–1 | date = February 2007 | pmid = 17268426 | doi = 10.1038/nature05548 | url = https://www.nature.com/articles/nature05548.pdf | first8 = Brian J. | first5 = James F. | first6 = James H. | first7 = Andrew P. }} and associated responses.</ref>\n\n=== Intra-organismal results ===\nBecause cell protoplasm appears to have constant density across a range of organism masses, a consequence of Kleiber's law is that, in larger species, less energy is available to each cell volume.  Cells appear to cope with this difficulty via choosing one of the following two strategies: a slower cellular metabolic rate, or smaller cells.  The latter strategy is exhibited by neurons and adipocytes; the former by every other type of cell.<ref>{{cite journal | vauthors = Savage VM, Allen AP, Brown JH, Gillooly JF, Herman AB, Woodruff WH, West GB | title = Scaling of number, size, and metabolic rate of cells with body size in mammals | journal = Proceedings of the National Academy of Sciences of the United States of America | volume = 104 | issue = 11 | pages = 4718–23 | date = March 2007 | pmid = 17360590 | pmc = 1838666 | doi = 10.1073/pnas.0611235104 | url = http://www.pnas.org/cgi/pmidlookup?view=long&pmid=17360590 | bibcode = 2007PNAS..104.4718S }}</ref>  As a result, different organs exhibit different allometric scalings (see table).<ref name=\"History\" />\n\n:{| class=\"wikitable sortable\"\n|+\nAllometric scalings for BMR-vs.-mass in human tissue\n!Organ\n!Scaling Exponent\n|-\n|Brain\n|0.7\n|-\n|Kidney\n|0.85\n|-\n|Liver\n|0.87\n|-\n|Heart\n|0.98\n|-\n|Muscle\n|1.0\n|-\n|Skeleton\n|1.1\n|}\n\n== See also ==\n* [[Allometric law]]\n* [[Evolutionary physiology]]\n* [[Metabolic theory of ecology]]\n* [[Scaling law]]\n\n== References ==\n{{Reflist}}\n\n== Further reading ==\n{{refbegin}}\n* {{cite journal | vauthors = Rau AR | title = Biological scaling and physics | journal = Journal of Biosciences | volume = 27 | issue = 5 | pages = 475–8 | date = September 2002 | pmid = 12381870 | doi = 10.1007/BF02705043 }}\n* {{cite journal | vauthors = Wang Z, O'Connor TP, Heshka S, Heymsfield SB | title = The reconstruction of Kleiber's law at the organ-tissue level | journal = The Journal of Nutrition | volume = 131 | issue = 11 | pages = 2967–70 | date = November 2001 | pmid = 11694627 | doi = 10.1093/jn/131.11.2967 }}\n* {{cite book|last=Whitfield|first=J.|title=In the Beat of a Heart|year=2006|publisher=Joseph Henry Press|location=Washington, D.C.}}\n* {{cite journal | vauthors = Glazier DS | title = A unifying explanation for diverse metabolic scaling in animals and plants | journal = Biological Reviews of the Cambridge Philosophical Society | volume = 85 | issue = 1 | pages = 111–38 | date = February 2010 | pmid = 19895606 | doi = 10.1111/j.1469-185X.2009.00095.x }}\n* {{cite journal|last=Glazier|first=Douglas S. | name-list-format = vanc |date=1 October 2014 |title=Metabolic Scaling in Complex Living Systems |journal=Systems | volume=2 |issue=4 |pages=451–540 |doi= 10.3390/systems2040451 }}\n*{{cite news |url= https://web.archive.org/web/20081203225711/http://courses.missouristate.edu/mcb095f/bio121/lab/Respiration/of_mice_and_elephants.htm|title=Of mice and Elephants |last=Johnson |first=George | name-list-format = vanc |date=12 January 1999 |archive-url=https://web.archive.org/web/20081203225711/http://courses.missouristate.edu/mcb095f/bio121/lab/Respiration/of_mice_and_elephants.htm|archive-date=3 December 2008}}\n* {{cite web |last=Woolley |first=Thomas| name-list-format = vanc |title=3/4 and Kleiber's Law |url=http://www.numberphile.com/videos/kleibers.html |work=Numberphile |publisher=[[Brady Haran]]}}\n{{refend}}\n\n{{modelling ecosystems|expanded=other}}\n\n[[Category:Power laws]]\n[[Category:Ecological theories]]"
    },
    {
      "title": "Landau distribution",
      "url": "https://en.wikipedia.org/wiki/Landau_distribution",
      "text": "{{Probability distribution\n  | name       = Landau distribution\n  | type       = density\n  | pdf_image  = [[File:Landau Distribution PDF.svg|350px]]\n  | support    = <math>\\mathbb{R}</math>\n  | parameters = <math>c \\in(0,\\infty)</math> — [[scale parameter]] <br>\n                 <math>\\mu\\in(-\\infty,\\infty)</math> — [[location parameter]]\n  | char       = <math>\\exp\\left(it\\mu -\\frac{2ict}{\\pi}\\log|t| - c|t|\\right)</math>\n  | mean       = Undefined\n  | variance   = Undefined\n  | mgf        = Undefined\n  | pdf        = <math> \\frac{1}{\\pi c}\\int_0^\\infty e^{-t}\\cos\\left(t\\left(\\frac{x-\\mu}{c}\\right) + \\frac{2t}{\\pi}\\log\\left(\\frac{t}{c}\\right)\\right)\\, dt</math>\n  }}\n\nIn [[probability theory]], the '''Landau distribution'''<ref>{{ cite journal | last = Landau | first = L. | title = On the energy loss of fast particles by ionization | journal = J. Phys. (USSR) | volume = 8 | page = 201 | date = 1944 }}</ref> is a [[probability distribution]] named after [[Lev Landau]].\nBecause of the distribution's \"fat\" tail, the [[Moment (mathematics)|moments]] of the distribution, like mean or variance, are undefined. The distribution is a particular case of [[stable distribution]].\n\n==Definition==\nThe [[probability density function]], as written originally by Landau, is defined by the [[complex number|complex]] [[integral]]:\n\n:<math>p(x) = \\frac{1}{2 \\pi i} \\int_{a-i\\infty}^{a+i\\infty} e^{s \\log(s) + x s}\\, ds , </math>\n\nwhere ''a'' is an arbitrary positive [[real number]], meaning that the integration path can be any parallel to the imaginary axis, intersecting the real positive semi-axis, and <math>\\log</math> refers to the [[natural logarithm]].\n\nThe following real integral is equivalent to the above:\n\n:<math>p(x) = \\frac{1}{\\pi} \\int_0^\\infty e^{-t \\log(t) - x t} \\sin(\\pi t)\\, dt.</math>\n\nThe full family of Landau distributions is obtained by extending the original distribution to a [[location-scale family]] of [[stable distributions]] with parameters <math>\\alpha=1</math> and <math>\\beta=1</math><ref>{{ cite book | last = Gentle | first = James E. | title = Random Number Generation and Monte Carlo Methods | edition = 2nd | publisher = Springer | location = New York, NY | date = 2003 | series=Statistics and Computing | isbn =978-0-387-00178-4 | doi = 10.1007/b97336 |page=196}} </ref>, with [[characteristic function (probability theory)|characteristic function]]<ref>{{cite book|last1=Zolotarev|first1=V.M.|title=One-dimensional stable distributions|date=1986|publisher=American Mathematical Society|location=Providence, R.I.|isbn=0-8218-4519-5}}</ref>:\n\n:<math>\\varphi(t;\\mu,c)=\\exp\\left(it\\mu -\\tfrac{2ict}{\\pi}\\log|t|-c|t|\\right)</math>\n\nwhere <math>c\\in(0,\\infty)</math> and <math>\\mu\\in(-\\infty,\\infty)</math>, which yields a density function:\n\n:<math>p(x;\\mu,c) = \\frac{1}{\\pi c}\\int_{0}^{\\infty} e^{-t}\\cos\\left(t\\left(\\frac{x-\\mu}{c}\\right)+\\frac{2t}{\\pi}\\log\\left(\\frac{t}{c}\\right)\\right)\\, dt , </math>\n\nLet us note that the original form of <math>p(x)</math> is obtained for <math>\\mu=0</math> and <math>c=\\frac{\\pi}{2}</math>, while the following is an approximation<ref>{{ cite book | last = Behrens | first = S. E. | last2 = Melissinos | first2 = A.C. | title = Univ. of Rochester Preprint UR-776 (1981) }}</ref> of <math>p(x;\\mu,c)</math> for <math>\\mu=0</math> and <math>c=1</math>:\n\n:<math>p(x) \\approx \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x + e^{-x}}{2}\\right).</math>\n\n==Related distributions==\n* If <math>X \\sim \\textrm{Landau}(\\mu,c)\\, </math> then <math> X + m \\sim \\textrm{Landau}(\\mu + m ,c) \\,</math>.\n* The Landau distribution is a [[stable distribution]] with stability parameter <math>\\alpha</math> and skewness parameter <math>\\beta</math> both equal to 1.\n\n== References ==\n{{Reflist}}\n\n{{ProbDistributions|continuous-infinite}}\n\n{{DEFAULTSORT:Landau Distribution}}\n[[Category:Continuous distributions]]\n[[Category:Probability distributions with non-finite variance]]\n[[Category:Power laws]]\n[[Category:Stable distributions]]"
    },
    {
      "title": "Lévy distribution",
      "url": "https://en.wikipedia.org/wiki/L%C3%A9vy_distribution",
      "text": "{{for|the more general family of Lévy alpha-stable distributions, of which this distribution is a special case|stable distribution}}\n{{Probability distribution|\n  name       =Lévy (unshifted)|\n  type       =density|\n  pdf_image  =[[Image:Levy0 distributionPDF.svg|325px|Levy distribution PDF]]<br /><small></small>|\n  cdf_image  =[[Image:Levy0 distributionCDF.svg|325px|Levy distribution CDF]]<br /><small></small>|\n  parameters =<math>\\mu</math> location; <math>c > 0\\,</math> [[scale parameter|scale]]|\n  support    =<math>x \\in [\\mu, \\infty)</math>|\n  pdf        =<math>\\sqrt{\\frac{c}{2\\pi}}~~\\frac{e^{-\\frac{c}{2(x-\\mu)}}}{(x-\\mu)^{3/2}}</math>|\n  cdf        =<math>\\textrm{erfc}\\left(\\sqrt{\\frac{c}{2(x-\\mu)}}\\right)</math>|\n  mean       =<math>\\infty</math>|\n  median     =<math>c/2(\\textrm{erfc}^{-1}(1/2))^2\\,</math>, for <math>\\mu=0</math>|\n  mode       =<math>\\frac{c}{3}</math>, for <math>\\mu=0</math>|\n  variance   =<math>\\infty</math>|\n  skewness   =undefined|\n  kurtosis   =undefined|\n  entropy    =<math>\\frac{1+3\\gamma+\\ln(16\\pi c^2)}{2}</math>\nwhere <math>\\gamma</math> is [[Euler's constant]]|\n  mgf        =undefined|\n  char       =<math>e^{i\\mu t-\\sqrt{-2ict}}</math>|\n}}\nIn [[probability theory]] and [[statistics]], the '''Lévy distribution''', named after [[Paul Lévy (mathematician)|Paul Lévy]], is a [[continuous probability distribution]] for a non-negative [[random variable]]. In [[spectroscopy]], this distribution, with frequency as the dependent variable, is known as a  '''van der Waals profile'''.<ref group=\"note\">\"van der Waals profile\" appears with lowercase \"van\" in almost all sources, such as: ''Statistical mechanics of the liquid surface'' by Clive Anthony Croxton, 1980, A Wiley-Interscience publication, {{isbn|0-471-27663-4}}, {{isbn|978-0-471-27663-0}}, [https://books.google.com/books?id=Wve2AAAAIAAJ&q=%22Van+der+Waals+profile%22&dq=%22Van+der+Waals+profile%22&hl=en]; and in ''Journal of technical physics'', Volume 36, by Instytut Podstawowych Problemów Techniki (Polska Akademia Nauk), publisher: Państwowe Wydawn. Naukowe., 1995, [https://books.google.com/books?id=2XpVAAAAMAAJ&q=%22Van+der+Waals+profile%22&dq=%22Van+der+Waals+profile%22&hl=en]<!-- and many more --></ref>  It is a special case of the [[inverse-gamma distribution]]. It is a stable distribution.\n\n\n==Definition==\n\nThe [[probability density function]] of the Lévy distribution over the domain <math>x\\ge \\mu</math> is\n\n:<math>f(x;\\mu,c)=\\sqrt{\\frac{c}{2\\pi}}~~\\frac{e^{ -\\frac{c}{2(x-\\mu)}}} {(x-\\mu)^{3/2}}</math>\n\nwhere <math>\\mu</math> is the [[location parameter]] and <math>c</math> is the [[scale parameter]]. The cumulative distribution function is\n\n:<math>F(x;\\mu,c)=\\textrm{erfc}\\left(\\sqrt{\\frac{c}{2(x-\\mu)}}\\right)</math>\n\nwhere <math>\\textrm{erfc}(z)</math> is the complementary [[error function]]. The shift parameter <math>\\mu</math> has the effect of shifting the curve to the right by an amount <math>\\mu</math>, and changing the support to the interval [<math>\\mu</math>, <math>\\infty</math>). Like all [[stable distribution]]s, the Levy distribution has a standard form f(x;0,1) which has the following property:\n\n:<math>f(x;\\mu,c)dx = f(y;0,1)dy\\,</math>\n\nwhere ''y'' is defined as\n\n:<math>y = \\frac{x-\\mu}{c}\\,</math>\n\nThe [[characteristic function (probability theory)|characteristic function]] of the Lévy distribution is given by\n\n:<math>\\varphi(t;\\mu,c)=e^{i\\mu t-\\sqrt{-2ict}}.</math>\n\nNote that the characteristic function can also be written in the same form used for the stable distribution with <math>\\alpha=1/2</math> and <math>\\beta=1</math>:\n\n:<math>\\varphi(t;\\mu,c)=e^{i\\mu t-|ct|^{1/2}~(1-i~\\textrm{sign}(t))}.</math>\n\nAssuming <math>\\mu=0</math>, the ''n''th [[moment (mathematics)|moment]] of the unshifted Lévy distribution is formally defined by:\n\n:<math>m_n\\ \\stackrel{\\mathrm{def}}{=}\\ \\sqrt{\\frac{c}{2\\pi}}\\int_0^\\infty \\frac{e^{-c/2x}\\,x^n}{x^{3/2}}\\,dx</math>\n\nwhich diverges for all ''n''&nbsp;>&nbsp;0 so that the moments of the Lévy distribution do not exist. The [[moment generating function]] is then formally defined by:\n\n:<math>M(t;c)\\ \\stackrel{\\mathrm{def}}{=}\\  \\sqrt{\\frac{c}{2\\pi}}\\int_0^\\infty \\frac{e^{-c/2x+tx}}{x^{3/2}}\\,dx</math>\n\nwhich diverges for <math>t>0</math> and is therefore not defined in an interval around zero, so that the moment generating function is not defined ''per se''. Like all [[stable distribution]]s except the [[normal distribution]], the wing of the probability density function exhibits heavy tail behavior falling off according to a power law:\n\n:<math>f(x;\\mu,c) \\sim \\sqrt{\\frac{c}{2\\pi}}\\frac{1}{x^{3/2}}</math> &nbsp; as &nbsp; <math>x\\to\\infty.</math>\n\n(This shows that Lévy is not just [[Heavy-tailed distribution|Heavy-tailed]] but also [[Fat-tailed distribution|Fat-tailed]].)  \n\nThis is illustrated in the diagram below, in which the probability density functions for various values of ''c'' and <math>\\mu=0</math> are plotted on a log-log scale.\n\n[[Image:Levy0 LdistributionPDF.svg|325px|thumb|left|Probability density function for the Lévy distribution on a log-log scale.]]\n\n{{clear}}\n\n\nThe standard Lévy distribution satisfies the condition of being [[Stable_distribution|Stable]]\n: <math> (X_1 + X_2 + \\dotsb + X_n) \\sim n^{1/\\alpha}X </math>, \n\nwhere <math>X_1, X_2, \\ldots, X_n, X </math> are independent standard Lévy-variables with <math>\\alpha=1/2</math>.\n\n==Related distributions==\n* If <math>X \\sim \\textrm{Levy}(\\mu,c)\\, </math> then <math> k X + b \\sim \\textrm{Levy}(k \\mu + b ,k c) \\,</math>\n* If <math>X\\,\\sim\\,\\textrm{Levy}(0,c)</math> then <math>X\\,\\sim\\,\\textrm{Inv-Gamma}(\\tfrac{1}{2},\\tfrac{c}{2})</math> ([[inverse gamma distribution]])\n* Lévy distribution is a special case of type 5 [[Pearson distribution]]\n* If <math>Y\\,\\sim\\,\\textrm{Normal}(\\mu,\\sigma)</math> ([[Normal distribution]]) then <math>{(Y-\\mu)}^{-2} \\sim\\,\\textrm{Levy}(0,1/\\sigma^2)</math>\n* If <math>X \\sim \\textrm{Normal}(\\mu,\\tfrac{1}{\\sqrt{\\sigma}})\\, </math> then <math>{(X-\\mu)}^{-2} \\sim \\textrm{Levy}(0,\\sigma)\\,</math>\n* If <math>X\\,\\sim\\,\\textrm{Levy}(\\mu,c)</math> then <math>X\\,\\sim\\,\\textrm{Stable}(1/2,1,c,\\mu) \\,</math> ([[Stable distribution]])\n* If <math>X\\,\\sim\\,\\textrm{Levy}(0,c)</math> then <math>X\\,\\sim\\,\\textrm{Scale-inv-}\\chi^2(1,c)</math> ([[Scaled-inverse-chi-squared distribution]])\n* If <math>X\\,\\sim\\,\\textrm{Levy}(\\mu,c)</math> then <math>{(X-\\mu)}^{-\\tfrac{1}{2}} \\sim\\,\\textrm{FoldedNormal}(0,1/\\sqrt{c})</math> ([[Folded normal distribution]])\n\n==Random sample generation==\nRandom samples from the Lévy distribution can be generated using [[inverse transform sampling]]. Given a random variate ''U'' drawn from the [[uniform distribution (continuous)|uniform distribution]] on the unit interval (0,&nbsp;1], the variate ''X'' given by<ref>How to derive the function for a random sample from a Lévy Distribution: http://www.math.uah.edu/stat/special/Levy.html</ref>\n\n:<math>X=F^{-1}(U)=\\frac{c}{(\\Phi^{-1}(1-U/2))^2}+\\mu</math>\n\nis Lévy-distributed with location <math>\\mu</math> and scale <math>c</math>. Here <math>\\Phi(x)</math> is the cumulative distribution function of the standard [[normal distribution]].\n\n==Applications==\n\n* The frequency of [[geomagnetic reversal]]s appears to follow a Lévy distribution<!-- for ref see that article -->\n*The [[hitting time|time of hitting]] a single point, at distance <math>\\alpha</math> from the starting point, by the [[Wiener process|Brownian motion]] has the Lévy distribution with <math>c=\\alpha^2</math>. (For a Brownian motion with drift, this time may follow an [[inverse Gaussian distribution]], which has the Lévy distribution as a limit.)\n* The length of the path followed by a photon in a turbid medium follows the Lévy distribution.<ref>{{cite journal |last=Rogers |first=Geoffrey L. |title=Multiple path analysis of reflectance from turbid media |journal=Journal of the Optical Society of America A |volume=25 |issue=11 |pages=2879–2883 |year=2008 |doi=10.1364/josaa.25.002879}}</ref>\n* A [[Cauchy process]] can be defined as a [[Brownian motion]] [[subordinator (mathematics)|subordinated]] to a process associated with a Lévy distribution.<ref name=applebaum>{{cite web|title=Lectures on Lévy processes and Stochastic calculus, Braunschweig; Lecture 2: Lévy processes|url=http://www.applebaum.staff.shef.ac.uk/Brauns2notes.pdf|author=Applebaum, D.|pages=37–53|publisher=University of Sheffield}}</ref>\n\n== Footnotes ==\n{{Reflist|group=\"note\"}}\n\n== Notes ==\n{{Reflist}}\n\n== References ==\n* {{cite web | title=Information on stable distributions| work= | url=http://academic2.american.edu/~jpnolan/stable/stable.html | accessdate=July 13, 2005}} - John P. Nolan's introduction to stable distributions, some papers on stable laws, and a free program to compute stable densities, cumulative distribution functions, quantiles, estimate parameters, etc. See especially [http://academic2.american.edu/~jpnolan/stable/chap1.pdf An introduction to stable distributions, Chapter 1]\n\n== External links ==\n* {{MathWorld|title=Lévy Distribution|urlname=LevyDistribution}}\n\n{{ProbDistributions|continuous-semi-infinite}}\n\n{{DEFAULTSORT:Levy distribution}}\n[[Category:Continuous distributions]]\n[[Category:Probability distributions with non-finite variance]]\n[[Category:Power laws]]\n[[Category:Stable distributions]]\n[[Category:Paul Lévy (mathematician)]]"
    },
    {
      "title": "Pareto distribution",
      "url": "https://en.wikipedia.org/wiki/Pareto_distribution",
      "text": "{{Probability distribution\n | name       =Pareto Type I\n | type       =density\n | pdf_image  =[[File:Probability density function of Pareto distribution.svg|325px|Pareto Type I probability density functions for various ''α'']]<br />Pareto Type I probability density functions for various <math>\\alpha</math> with <math>x_\\mathrm{m} = 1.</math> As <math>\\alpha \\rightarrow \\infty,</math> the distribution approaches <math>\\delta(x - x_\\mathrm{m}),</math> where <math>\\delta</math> is the [[Dirac delta function]].\n | cdf_image  =[[File:Cumulative distribution function of Pareto distribution.svg|325px|Pareto Type I cumulative distribution functions for various ''α'']]<br />Pareto Type I cumulative distribution functions for various <math>\\alpha</math> with <math>x_\\mathrm{m} = 1.</math>\n | parameters =<math>x_\\mathrm{m} > 0</math> [[scale parameter|scale]] ([[real number|real]])<br /><math>\\alpha > 0</math> [[shape parameter|shape]] (real)\n | support    =<math>x \\in [x_\\mathrm{m}, \\infty)</math>\n | pdf        =<math>\\frac{\\alpha x_\\mathrm{m}^\\alpha}{x^{\\alpha+1}}</math>\n | cdf        =<math>1-\\left(\\frac{x_\\mathrm{m}}{x}\\right)^\\alpha</math>\n | mean       =<math>\\begin{cases}\n     \\infty & \\text{for }\\alpha\\le 1 \\\\\n     \\dfrac{\\alpha x_\\mathrm{m}}{\\alpha-1} & \\text{for }\\alpha>1\n   \\end{cases}</math>\n | median     =<math>x_\\mathrm{m} \\sqrt[\\alpha]{2}</math>\n | mode       =<math>x_\\mathrm{m}</math>\n | variance   =<math>\\begin{cases}\n     \\infty & \\text{for }\\alpha\\le 2 \\\\\n     \\dfrac{x_\\mathrm{m}^2\\alpha}{(\\alpha-1)^2(\\alpha-2)} & \\text{for }\\alpha>2\n   \\end{cases}</math>\n | skewness   =<math>\\frac{2(1+\\alpha)}{\\alpha-3}\\sqrt{\\frac{\\alpha-2}{\\alpha}}\\text{ for }\\alpha>3</math>\n | kurtosis   =<math>\\frac{6(\\alpha^3+\\alpha^2-6\\alpha-2)}{\\alpha(\\alpha-3)(\\alpha-4)}\\text{ for }\\alpha>4</math>\n | entropy    =<math>\\log\\left(\\left(\\frac{x_\\mathrm{m}}{\\alpha}\\right)\\,e^{1+\\tfrac{1}{\\alpha}}\\right) </math>\n | mgf        =<math>\\alpha(-x_\\mathrm{m}t)^\\alpha\\Gamma(-\\alpha,-x_\\mathrm{m}t)\\text{ for }t<0</math>\n | char       =<math>\\alpha(-ix_\\mathrm{m}t)^\\alpha\\Gamma(-\\alpha,-ix_\\mathrm{m}t)</math>\n | fisher     =<math>\\mathcal{I}(x_\\mathrm{m},\\alpha) = \\begin{bmatrix}\n                   \\dfrac{\\alpha}{x_\\mathrm{m}^2} & -\\dfrac{1}{x_\\mathrm{m}} \\\\\n                   -\\dfrac{1}{x_\\mathrm{m}} & \\dfrac{1}{\\alpha^2}\n               \\end{bmatrix}</math>\n}}\n\nThe '''Pareto distribution''', named after the Italian [[civil engineer]], [[economist]], and sociologist [[Vilfredo Pareto]], is a [[power-law]] [[probability distribution]] that is used in description of [[social sciences|social]], [[scientific]], [[geophysical]], [[actuarial science|actuarial]], and many other types of observable phenomena. Originally applied to describing the [[distribution of wealth]] in a society, fitting the trend that a large portion of wealth is held by a small fraction of the population, the Pareto distribution has colloquially become known and referred to as the [[Pareto principle]], or \"80-20 rule\", and is sometimes called the \"[[Matthew principle]]\".<!-- can we find a better reference than https://youtu.be/5WX9UEYZsR8 at 2'10\". -->  This rule states that, for example, 80% of the wealth of a society is held by 20% of its population. However, one should not conflate the Pareto distribution for the Pareto Principle as the former only produces this result for a particular power value, <math>\\alpha</math> (''α''&nbsp;=&nbsp;log<sub>4</sub>5&nbsp;≈&nbsp;1.16). While <math>\\alpha</math> is variable, empirical observation has found the 80-20 distribution to fit a wide range of cases, including natural phenomena and human activities.{{cn|date=March 2019}}\n\n==Definition==\nIf ''X'' is a [[random variable]] with a Pareto (Type I) distribution,<ref name=arnold>{{cite book |author=Barry C. Arnold |year=1983 |title=Pareto Distributions |publisher=International Co-operative Publishing House |isbn= 978-0-89974-012-6|ref=harv}}</ref> then the probability that ''X'' is greater than some number ''x'', i.e. the [[survival function]] (also called tail function), is given by\n\n:<math>\\overline{F}(x) = \\Pr(X>x) = \\begin{cases}\n\\left(\\frac{x_\\mathrm{m}}{x}\\right)^\\alpha & x\\ge x_\\mathrm{m}, \\\\\n1 & x < x_\\mathrm{m},\n\\end{cases}\n</math>\n\nwhere ''x''<sub>m</sub> is the (necessarily positive) minimum possible value of ''X'', and ''α'' is a positive parameter. The Pareto Type I distribution is characterized by a scale parameter ''x''<sub>m</sub> and a shape parameter ''α'', which is known as the ''tail index''. When this distribution is used to model the distribution of wealth, then the parameter ''α'' is called the [[Pareto index]].\n\n==Properties==\n\n===Cumulative distribution function===\nFrom the definition, the [[cumulative distribution function]] of a Pareto random variable with parameters ''α'' and ''x''<sub>m</sub> is\n\n:<math>F_X(x) = \\begin{cases}\n1-\\left(\\frac{x_\\mathrm{m}}{x}\\right)^\\alpha & x \\ge x_\\mathrm{m}, \\\\\n0 & x < x_\\mathrm{m}.\n\\end{cases}</math>\n\n===Probability density function===\nIt follows (by [[Derivative|differentiation]]) that the [[probability density function]] is\n\n:<math>f_X(x)= \\begin{cases} \\frac{\\alpha x_\\mathrm{m}^\\alpha}{x^{\\alpha+1}} & x \\ge x_\\mathrm{m}, \\\\ 0 & x < x_\\mathrm{m}. \\end{cases} </math>\n\nWhen plotted on linear axes, the distribution assumes the familiar J-shaped curve which approaches each of the orthogonal axes [[asymptotically]]. All segments of the curve are self-similar (subject to appropriate scaling factors). When plotted in a [[log-log plot]], the distribution is represented by a straight line.\n\n===Moments and characteristic function===\n* The [[expected value]] of a [[random variable]] following a Pareto distribution is\n:\n:: <math>\\operatorname{E}(X)= \\begin{cases} \\infty & \\alpha\\le 1, \\\\\n\\frac{\\alpha x_\\mathrm{m}}{\\alpha-1} & \\alpha>1.\n\\end{cases}</math>\n* The [[variance]] of a [[random variable]] following a Pareto distribution is\n\n:: <math>\\operatorname{Var}(X)= \\begin{cases}\n\\infty & \\alpha\\in(1,2], \\\\\n\\left(\\frac{x_\\mathrm{m}}{\\alpha-1}\\right)^2 \\frac{\\alpha}{\\alpha-2} & \\alpha>2.\n\\end{cases}</math>\n\n: (If ''α'' ≤ 1, the variance does not exist.)\n* The raw [[moment (mathematics)|moments]] are\n\n:: <math>\\mu_n'= \\begin{cases} \\infty & \\alpha\\le n, \\\\ \\frac{\\alpha x_\\mathrm{m}^n}{\\alpha-n} & \\alpha>n. \\end{cases}</math>\n* The [[Moment-generating function|moment generating function]] is only defined for non-positive values ''t''&nbsp;≤&nbsp;0 as\n\n::<math>M\\left(t;\\alpha,x_\\mathrm{m}\\right) = \\operatorname{E} \\left [e^{tX} \\right ] = \\alpha(-x_\\mathrm{m} t)^\\alpha\\Gamma(-\\alpha,-x_\\mathrm{m} t)</math>\n::<math>M\\left(0,\\alpha,x_\\mathrm{m}\\right)=1.</math>\n* The [[Characteristic function (probability theory)|characteristic function]] is given by\n\n:: <math>\\varphi(t;\\alpha,x_\\mathrm{m})=\\alpha(-ix_\\mathrm{m} t)^\\alpha\\Gamma(-\\alpha,-ix_\\mathrm{m} t),</math>\n\n: where Γ(''a'',&nbsp;''x'') is the [[incomplete gamma function]].\n\n===Conditional distributions===\nThe [[conditional probability distribution]] of a Pareto-distributed random variable, given the event that it is greater than or equal to a particular number&nbsp;<math>x_1</math> exceeding <math>x_\\text{m}</math>, is a Pareto distribution with the same Pareto index&nbsp;<math>\\alpha</math> but with minimum&nbsp;<math>x_1</math> instead of <math>x_\\text{m}</math>.\n\n===A characterization theorem===\nSuppose <math>X_1, X_2, X_3, \\dotsc</math> are [[independent identically distributed]] [[random variable]]s whose probability distribution is supported on the interval <math>[x_\\text{m},\\infty)</math> for some <math>x_\\text{m}>0</math>. Suppose that for all <math>n</math>, the two random variables <math>\\min\\{X_1,\\dotsc,X_n\\}</math> and <math>(X_1+\\dotsb+X_n)/\\min\\{X_1,\\dotsc,X_n\\}</math> are independent. Then the common distribution is a Pareto distribution.{{Citation needed|date=February 2012}}\n\n===Geometric mean===\nThe [[geometric mean]] (''G'') is<ref name=Johnson1994>Johnson NL, Kotz S, Balakrishnan N (1994) Continuous univariate distributions Vol 1. Wiley Series in Probability and Statistics.</ref>\n\n: <math> G = x_\\text{m} \\exp \\left( \\frac{1}{\\alpha} \\right).</math>\n\n===Harmonic mean===\nThe [[harmonic mean]] (''H'') is<ref name=\"Johnson1994\"/>\n\n: <math> H = x_\\text{m} \\left( 1 + \\frac{ 1 }{ \\alpha } \\right).</math>\n\n==Generalized Pareto distributions==\n{{See also|Generalized Pareto distribution}}\n\nThere is a hierarchy <ref name=arnold/><ref name=jkb94>Johnson, Kotz, and Balakrishnan (1994), (20.4).</ref> of Pareto distributions known as Pareto Type I, II, III, IV, and Feller–Pareto distributions.<ref name=arnold/><ref name=jkb94/><ref name=kk03>{{cite book |author1=Christian Kleiber  |author2=Samuel Kotz  |lastauthoramp=yes |year=2003 |title=Statistical Size Distributions in Economics and Actuarial Sciences |publisher=[[John Wiley & Sons|Wiley]] |isbn=978-0-471-15064-0|ref=harv| url=https://books.google.com/books?id=7wLGjyB128IC&printsec=frontcover}}</ref> Pareto Type IV contains Pareto Type I–III as special cases. The Feller–Pareto<ref name=jkb94/><ref name=feller>{{cite book|last=Feller |first= W.| year=1971| title=An Introduction to Probability Theory and its Applications| volume=II| edition=2nd | location= New York|publisher=Wiley|page=50}} \"The densities (4.3) are sometimes called after the economist ''Pareto''. It was thought (rather naïvely from a modern statistical standpoint) that income distributions should have a tail with a density ~ ''Ax''<sup>−''α''</sup> as ''x''&nbsp;→&nbsp;∞.\"</ref> distribution generalizes Pareto Type IV.\n<!--- In this context using x_m for the lower bound for the scale parameter is not meaningful, usual notation is \\sigma --->\n\n===Pareto types I–IV===\nThe Pareto distribution hierarchy is summarized in the next table comparing the [[survival function]]s (complementary CDF).\n\nWhen ''μ'' = 0, the Pareto distribution Type II is also known as the [[Lomax distribution]].<ref>{{cite journal | last1 = Lomax | first1 = K. S. | year = 1954 | title = Business failures. Another example of the analysis of failure data | url = | journal = Journal of the American Statistical Association | volume = 49 | issue = 268| pages = 847–52 | doi=10.1080/01621459.1954.10501239}}</ref>\n\nIn this section, the symbol ''x''<sub>m</sub>, used before to indicate the minimum value of ''x'', is replaced by&nbsp;''σ''.\n\n{|class=\"wikitable\" border=\"1\"\n|+Pareto distributions\n! !! <math> \\overline{F}(x)=1-F(x)</math> !! Support !! Parameters\n|-\n| Type I\n|| <math>\\left[\\frac x \\sigma \\right]^{-\\alpha}</math>\n|| <math>x \\ge \\sigma</math>\n|| <math>\\sigma > 0, \\alpha</math>\n|-\n| Type II\n|| <math>\\left[1 + \\frac{x-\\mu} \\sigma \\right]^{-\\alpha}</math>\n|| <math>x \\ge \\mu</math>\n|| <math>\\mu \\in \\mathbb R, \\sigma > 0, \\alpha</math>\n|-\n| Lomax\n|| <math>\\left[1 + \\frac x \\sigma \\right]^{-\\alpha}</math>\n|| <math>x \\ge 0</math>\n|| <math>\\sigma > 0, \\alpha</math>\n|-\n| Type III\n|| <math>\\left[1 + \\left(\\frac{x-\\mu} \\sigma \\right)^{1/\\gamma}\\right]^{-1} </math>\n|| <math>x \\ge \\mu</math>\n|| <math> \\mu \\in \\mathbb R, \\sigma, \\gamma > 0</math>\n|-\n| Type IV\n|| <math>\\left[1 + \\left(\\frac{x-\\mu} \\sigma \\right)^{1/\\gamma}\\right]^{-\\alpha}</math>\n|| <math>x \\ge \\mu</math>\n|| <math>\\mu \\in \\mathbb R, \\sigma, \\gamma > 0, \\alpha</math>\n|-\n|-\n|}\n\nThe shape parameter ''α'' is the [[tail index]], ''μ'' is location, ''σ'' is scale, ''γ'' is an inequality parameter. Some special cases of Pareto Type (IV) are\n\n::<math> P(IV)(\\sigma, \\sigma, 1, \\alpha) = P(I)(\\sigma, \\alpha),</math>\n::<math> P(IV)(\\mu, \\sigma, 1, \\alpha) = P(II)(\\mu, \\sigma, \\alpha),</math>\n::<math> P(IV)(\\mu, \\sigma, \\gamma, 1) = P(III)(\\mu, \\sigma, \\gamma).</math>\n\nThe finiteness of the mean, and the existence and the finiteness of the variance depend on the tail index ''α'' (inequality index ''γ''). In particular, fractional ''δ''-moments are finite for some ''δ'' > 0, as shown in the table below, where ''δ'' is not necessarily an integer.\n\n{|class=\"wikitable\" border=\"1\"\n|+Moments of Pareto I–IV distributions (case ''μ'' = 0)\n! !! <math>\\operatorname{E}[X]</math> !! Condition !! <math>\\operatorname{E}[X^\\delta]</math> !! Condition\n|-\n| Type I\n|| <math>\\frac{\\sigma \\alpha}{\\alpha-1}</math>\n|| <math>\\alpha > 1</math>\n|| <math>\\frac{\\sigma^\\delta \\alpha}{\\alpha-\\delta}</math>\n|| <math> \\delta < \\alpha</math>\n|-\n| Type II\n|| <math> \\frac{ \\sigma }{\\alpha-1}</math>\n|| <math>\\alpha > 1</math>\n|| <math> \\frac{ \\sigma^\\delta \\Gamma(\\alpha-\\delta)\\Gamma(1+\\delta)}{\\Gamma(\\alpha)}</math>\n|| <math>-1 < \\delta < \\alpha</math>\n|-\n| Type III\n|| <math>\\sigma\\Gamma(1-\\gamma)\\Gamma(1 + \\gamma)</math>\n|| <math> -1<\\gamma<1</math>\n|| <math>\\sigma^\\delta\\Gamma(1-\\gamma \\delta)\\Gamma(1+\\gamma \\delta)</math>\n|| <math>-\\gamma^{-1}<\\delta<\\gamma^{-1}</math>\n|-\n| Type IV\n|| <math>\\frac{\\sigma\\Gamma(\\alpha-\\gamma)\\Gamma(1+\\gamma)}{\\Gamma(\\alpha)}</math>\n|| <math> -1<\\gamma<\\alpha</math>\n|| <math>\\frac{\\sigma^\\delta\\Gamma(\\alpha-\\gamma \\delta)\\Gamma(1+\\gamma \\delta)}{\\Gamma(\\alpha)}</math>\n|| <math>-\\gamma^{-1}<\\delta<\\alpha/\\gamma </math>\n|-\n|-\n|}\n\n===Feller–Pareto distribution===\nFeller<ref name=jkb94/><ref name=feller/> defines a Pareto variable by transformation ''U''&nbsp;=&nbsp;''Y''<sup>−1</sup>&nbsp;−&nbsp;1 of a [[beta distribution|beta random variable]] ''Y'', whose probability density function is\n\n:<math> f(y) = \\frac{y^{\\gamma_1-1} (1-y)^{\\gamma_2-1}}{B(\\gamma_1, \\gamma_2)}, \\qquad 0<y<1; \\gamma_1,\\gamma_2>0,</math>\n\nwhere ''B''(&nbsp;) is the [[beta function]]. If\n\n:<math> W = \\mu + \\sigma(Y^{-1}-1)^\\gamma, \\qquad \\sigma>0, \\gamma>0,</math>\n\nthen ''W'' has a Feller–Pareto distribution FP(''μ'', ''σ'', ''γ'', ''γ''<sub>1</sub>, ''γ''<sub>2</sub>).<ref name=arnold/>\n\nIf <math>U_1 \\sim \\Gamma(\\delta_1, 1)</math> and <math>U_2 \\sim \\Gamma(\\delta_2, 1)</math> are independent [[Gamma distribution|Gamma variables]], another construction of a Feller–Pareto (FP) variable is<ref>{{cite book |last=Chotikapanich |first=Duangkamon |title=Modeling Income Distributions and Lorenz Curves |chapter=Chapter 7: Pareto and Generalized Pareto Distributions |pages=121–22 |chapter-url=https://books.google.com/books?id=fUJZZLj1kbwC}}</ref>\n\n:<math>W = \\mu + \\sigma \\left(\\frac{U_1}{U_2}\\right)^\\gamma</math>\n\nand we write ''W'' ~ FP(''μ'', ''σ'', ''γ'', ''δ''<sub>1</sub>, ''δ''<sub>2</sub>). Special cases of the Feller–Pareto distribution are\n\n:<math>FP(\\sigma, \\sigma, 1, 1, \\alpha) = P(I)(\\sigma, \\alpha)</math>\n:<math>FP(\\mu, \\sigma, 1, 1, \\alpha) = P(II)(\\mu, \\sigma, \\alpha)</math>\n:<math>FP(\\mu, \\sigma, \\gamma, 1, 1) = P(III)(\\mu, \\sigma, \\gamma)</math>\n:<math>FP(\\mu, \\sigma, \\gamma, 1, \\alpha) = P(IV)(\\mu, \\sigma, \\gamma, \\alpha).</math>\n\n==Applications==\n[[Vilfredo Pareto]] originally used this distribution to describe the [[Distribution of wealth|allocation of wealth]] among individuals since it seemed to show rather well the way that a larger portion of the wealth of any society is owned by a smaller percentage of the people in that society. He also used it to describe distribution of income.<ref>Pareto, Vilfredo, ''Cours d'Économie Politique: Nouvelle édition par G.-H. Bousquet et G. Busino'', Librairie Droz, Geneva, 1964, pp. 299–345.</ref> This idea is sometimes expressed more simply as the [[Pareto principle]] or the \"80-20 rule\" which says that 20% of the population controls 80% of the wealth.<ref>For a two-quantile population, where approximately 18% of the population owns 82% of the wealth, the [[Theil index]] takes the value 1.</ref> However, the 80-20 rule corresponds to a particular value of ''α'', and in fact, Pareto's data on British income taxes in his ''Cours d'économie politique'' indicates that about 30% of the population had about 70% of the income.{{fact|date=May 2019}} The [[probability density function]] (PDF) graph at the beginning of this article shows that the \"probability\" or fraction of the population that owns a small amount of wealth per person is rather high, and then decreases steadily as wealth increases. (The Pareto distribution is not realistic for wealth for the lower end, however. In fact, [[net worth]] may even be negative.) This distribution is not limited to describing wealth or income, but to many situations in which an equilibrium is found in the distribution of the \"small\" to the \"large\". The following examples are sometimes seen as approximately Pareto-distributed:\n<!-- THESE TWO SEEM TO BELONG UNDER [[Zipf's law]] RATHER THAN THE PARETO DISTRIBUTION\n* Frequencies of words in longer texts (a few words are used often, lots of words are used infrequently)\n* Frequencies of [[Given name#Popularity distribution of given names|given names]] -->\n* The sizes of human settlements (few cities, many hamlets/villages)<ref name=\"Reed\">{{cite journal |citeseerx=10.1.1.70.4555 |first=William J. |last=Reed |title=The Double Pareto-Lognormal Distribution – A New Parametric Model for Size Distributions |journal=Communications in Statistics – Theory and Methods |volume=33 |issue=8 |pages=1733–53 |year=2004 |doi=10.1081/sta-120037438|display-authors=etal}}</ref>\n* File size distribution of Internet traffic which uses the TCP protocol (many smaller files, few larger ones)<ref name =\"Reed\" />\n* [[Hard disk drive]] error rates<ref>{{cite journal |title=Understanding latent sector error and how to protect against them |url=http://www.usenix.org/event/fast10/tech/full_papers/schroeder.pdf |first1=Bianca |last1=Schroeder |first2=Sotirios |last2=Damouras |first3=Phillipa |last3=Gill |journal=8th Usenix Conference on File and Storage Technologies (FAST 2010)| date=2010-02-24 |accessdate=2010-09-10 |quote=We experimented with 5 different distributions (Geometric,Weibull, Rayleigh, Pareto, and Lognormal), that are commonly used in the context of system reliability, and evaluated their fit through the total squared differences between the actual and hypothesized frequencies (χ<sup>2</sup> statistic). We found consistently across all models that the geometric distribution is a poor fit, while the Pareto distribution provides the best fit.}}</ref>\n* Clusters of [[Bose–Einstein condensate]] near [[absolute zero]]<ref name=\"Simon\">{{cite journal|first2=Herbert A.|last2=Simon|author=Yuji Ijiri |title=Some Distributions Associated with Bose–Einstein Statistics|journal=Proc. Natl. Acad. Sci. USA|date=May 1975|volume=72|issue=5|pages=1654–57|pmc=432601|pmid=16578724|doi=10.1073/pnas.72.5.1654|bibcode=1975PNAS...72.1654I}}</ref>\n[[File:FitParetoDistr.tif|thumb|250px|Fitted cumulative Pareto (Lomax) distribution to maximum one-day rainfalls using [[CumFreq]], see also [[distribution fitting]] ]]\n* The values of [[oil reserves]] in oil fields (a few [[Giant oil and gas fields|large fields]], many [[Stripper well|small fields]])<ref name =\"Reed\" />\n* The length distribution in jobs assigned supercomputers (a few large ones, many small ones)<ref>{{Cite journal|last=Harchol-Balter|first=Mor|author1-link=Mor Harchol-Balter|last2=Downey|first2=Allen|date=August 1997|title=Exploiting Process Lifetime Distributions for Dynamic Load Balancing|url=https://users.soe.ucsc.edu/~scott/courses/Fall11/221/Papers/Sync/harcholbalter-tocs97.pdf|journal=ACM Transactions on Computer Systems|volume=15|issue=3|pages=253–258|doi=10.1145/263326.263344}}</ref>\n* The standardized price returns on individual stocks <ref name=\"Reed\" />\n* Sizes of sand particles <ref name =\"Reed\" />\n* The size of meteorites\n* Severity of large [[casualty (person)|casualty]] losses for certain lines of business such as general liability, commercial auto, and workers compensation.<ref>Kleiber and Kotz (2003): p. 94.</ref><ref>{{cite journal |last1=Seal |first1=H. |year=1980 |title=Survival probabilities based on Pareto claim distributions |journal=ASTIN Bulletin |volume=11 |pages=61–71|doi=10.1017/S0515036100006620 }}</ref>\n* Amount of time a user on steam will spend playing different games. (Some games get played a lot, but most get played  almost never.) [https://docs.google.com/spreadsheets/d/1BDv2W4IsgxiAhhUznTbMSfRtcLia320Zq1HxzwhKao0/edit#gid=0]\n* In [[hydrology]] the Pareto distribution is applied to extreme events such as annually maximum one-day rainfalls and river discharges.<ref>CumFreq, software for cumulative frequency analysis and probability distribution fitting [https://www.waterlog.info/cumfreq.htm]</ref> The blue picture illustrates an example of fitting the Pareto distribution to ranked annually maximum one-day rainfalls showing also the 90% [[confidence belt]] based on the [[binomial distribution]]. The rainfall data are represented by [[plotting position]]s as part of the [[cumulative frequency analysis]].\n\n==Relation to other distributions==\n\n===Relation to the exponential distribution===\nThe Pareto distribution is related to the [[exponential distribution]] as follows. If ''X'' is Pareto-distributed with minimum ''x''<sub>m</sub> and index&nbsp;''α'', then\n\n: <math> Y = \\log\\left(\\frac{X}{x_\\mathrm{m}}\\right) </math>\n\nis [[exponential distribution|exponentially distributed]] with rate parameter&nbsp;''α''. Equivalently, if ''Y'' is exponentially distributed with rate&nbsp;''α'', then\n\n: <math> x_\\mathrm{m} e^Y</math>\n\nis Pareto-distributed with minimum ''x''<sub>m</sub> and index&nbsp;''α''.\n\nThis can be shown using the standard change-of-variable techniques:\n\n: <math>\n\\begin{align}\n\\Pr(Y<y) & = \\Pr\\left(\\log\\left(\\frac{X}{x_\\mathrm{m}}\\right)<y\\right) \\\\\n& = \\Pr(X<x_\\mathrm{m} e^y) = 1-\\left(\\frac{x_\\mathrm{m}}{x_\\mathrm{m}e^y}\\right)^\\alpha=1-e^{-\\alpha y}.\n\\end{align}\n</math>\n\nThe last expression is the cumulative distribution function of an exponential distribution with rate&nbsp;''α''.\n\n===Relation to the log-normal distribution===\nThe Pareto distribution and [[log-normal distribution]] are alternative distributions for describing the same types of quantities. One of the connections between the two is that they are both the distributions of the exponential of random variables distributed according to other common distributions, respectively the [[exponential distribution]] and [[normal distribution]].{{citation needed|date=December 2010}}\n\n===Relation to the generalized Pareto distribution===\nThe Pareto distribution is a special case of the [[generalized Pareto distribution]], which is a family of distributions of similar form, but containing an extra parameter in such a way that the support of the distribution is either bounded below (at a variable point), or bounded both above and below (where both are variable), with the [[Lomax distribution]] as a special case. This family also contains both the unshifted and shifted [[exponential distribution]]s.\n\nThe Pareto distribution with scale <math>x_m</math> and shape <math>\\alpha</math> is equivalent to the generalized Pareto distribution with location <math>\\mu=x_m</math>, scale <math>\\sigma=x_m/\\alpha</math> and shape <math>\\xi=1/\\alpha</math>. Vice versa one can get the Pareto distribution from the GPD by <math>x_m = \\sigma/\\xi</math> and <math>\\alpha=1/\\xi</math>.\n\n===Relation to Zipf's law===\nThe Pareto distribution is continuous probability distribution. [[Zipf's law]], also sometimes called the [[zeta distribution]], is a discrete distribution, separating the values into a simple ranking. Both are a simple power law with a negative exponent, scaled so that their cumulative distributions equal 1.  Zipf's can be derived from the Pareto distribution if the <math>x</math> values (incomes) are binned into <math>N</math> ranks so that the number of people in each bin follows a 1/rank pattern. The distribution is normalized by defining <math>x_m</math> so that <math>\\alpha x_\\mathrm{m}^\\alpha = \\frac{1}{H(N,\\alpha-1)}</math> where <math>H(N,\\alpha-1)</math> is the [[Harmonic_number#Generalized_harmonic_numbers|generalized harmonic number]]. This makes Zipf's probability density function derivable from Pareto's.\n\n: <math>f(x) = \\frac{\\alpha x_\\mathrm{m}^\\alpha}{x^{\\alpha+1}} = \\frac{1}{x^s H(N,s)}</math>\n\nwhere  <math>s = \\alpha-1</math> and <math>x</math> is an integer representing rank from 1 to N where N is the highest income bracket.  So a randomly selected person (or word, website link, or city) from a population (or language, internet, or country) has <math>f(x)</math> probability of ranking <math>x</math>.\n\n===Relation to the \"Pareto principle\"===\nThe \"[[80-20 law]]\", according to which 20% of all people receive 80% of all income, and 20% of the most affluent 20% receive 80% of that 80%, and so on, holds precisely when the Pareto index is&nbsp;''α''&nbsp;=&nbsp;log<sub>4</sub>(5)&nbsp;=&nbsp;log(5)/log(4), approximately 1.161. This result can be derived from the [[Lorenz curve]] formula given below. Moreover, the following have been shown<ref>{{cite journal |last1=Hardy |first1=Michael |year=2010 |title=Pareto's Law |journal=[[Mathematical Intelligencer]] |volume=32 |issue=3 |pages=38–43 |doi=10.1007/s00283-010-9159-2}}</ref> to be mathematically equivalent:\n* Income is distributed according to a Pareto distribution with index ''α''&nbsp;>&nbsp;1.\n* There is some number 0&nbsp;≤&nbsp;''p''&nbsp;≤&nbsp;1/2 such that 100''p'' % of all people receive 100(1&nbsp;−&nbsp;''p'')% of all income, and similarly for every real (not necessarily integer) ''n''&nbsp;>&nbsp;0, 100''p<sup>n</sup>'' % of all people receive 100(1&nbsp;−&nbsp;''p'')<sup>''n''</sup> percentage of all income. ''α'' and ''p'' are related by\n:<math>1-\\frac{1}{\\alpha}=\\frac{\\ln(1-p^n)}{\\ln(1-(1-p)^n)}</math>\n\nThis does not apply only to income, but also to wealth, or to anything else that can be modeled by this distribution.\n\nThis excludes Pareto distributions in which&nbsp;0&nbsp;<&nbsp;''α''&nbsp;≤&nbsp;1, which, as noted above, have infinite expected value, and so cannot reasonably model income distribution.\n\n===Relation to Price's law===\n[[Derek_J._de_Solla_Price#Scientific_contributions|Price's square root law]] is sometimes offered as a property of or as similar to the Pareto distribution. However, the law only holds in the case that <math>\\alpha=1</math>. Note that in this case, the total and expected amount of wealth are not defined, and the rule only applies asymptotically to random samples. The extended Pareto Principle mentioned above is a far more general rule.\n\n==Lorenz curve and Gini coefficient==\n[[File:ParetoLorenzSVG.svg|thumb|325px|Lorenz curves for a number of Pareto distributions. The case ''α''&nbsp;=&nbsp;∞ corresponds to perfectly equal distribution (''G''&nbsp;=&nbsp;0) and the ''α''&nbsp;=&nbsp;1 line corresponds to complete inequality (''G''&nbsp;=&nbsp;1)]]\n\nThe [[Lorenz curve]] is often used to characterize income and wealth distributions. For any distribution, the Lorenz curve ''L''(''F'') is written in terms of the PDF ''f'' or the CDF ''F'' as\n\n:<math>L(F)=\\frac{\\int_{x_\\mathrm{m}}^{x(F)}xf(x)\\,dx}{\\int_{x_\\mathrm{m}}^\\infty xf(x)\\,dx} =\\frac{\\int_0^F x(F')\\,dF'}{\\int_0^1 x(F')\\,dF'}</math>\n\nwhere ''x''(''F'') is the inverse of the CDF. For the Pareto distribution,\n\n:<math>x(F)=\\frac{x_\\mathrm{m}}{(1-F)^{\\frac{1}{\\alpha}}}</math>\n\nand the Lorenz curve is calculated to be\n\n:<math>L(F) = 1-(1-F)^{1-\\frac{1}{\\alpha}},</math>\n\nFor <math>0<\\alpha\\le 1</math> the denominator is infinite, yielding ''L''=0. Examples of the Lorenz curve for a number of Pareto distributions are shown in the graph on the right.\n\nAccording to [[Oxfam]] (2016) the richest 62 people have as much wealth as the poorest half of the world's population.<ref>{{cite web|title=62 people own the same as half the world, reveals Oxfam Davos report|url=https://www.oxfam.org/en/pressroom/pressreleases/2016-01-18/62-people-own-same-half-world-reveals-oxfam-davos-report|publisher=Oxfam|date=Jan 2016}}</ref> We can estimate the Pareto index that would apply to this situation. Letting ε equal <math>62/(7\\times 10^9)</math> we have:\n:<math>L(1/2)=1-L(1-\\varepsilon)</math>\nor\n:<math>1-(1/2)^{1-\\frac{1}{\\alpha}}=\\varepsilon^{1-\\frac{1}{\\alpha}}</math>\n<!--:<math>\\ln(1-(1/2)^{1-\\frac{1}{\\alpha}})=(1-\\frac{1}{\\alpha})\\ln\\varepsilon</math>\n:<math>\\ln(1-(1/2)^{1-\\frac{1}{\\alpha}})=(\\ln\\varepsilon/\\ln 2)(1-\\frac{1}{\\alpha})\\ln 2</math>\n:<math>\\ln(1-(1/2)^{1-\\frac{1}{\\alpha}})=-(\\ln\\varepsilon/\\ln 2)\\ln((1/2)^{1-\\frac{1}{\\alpha}})</math>\n:<math>\\ln(1-(1/2)^{1-\\frac{1}{\\alpha}})\\approx(\\ln\\varepsilon/\\ln 2)(1-(1/2)^{1-\\frac{1}{\\alpha}})</math>\n:<math>-\\ln(1-(1/2)^{1-\\frac{1}{\\alpha}})\\exp(-\\ln(1-(1/2)^{1-\\frac{1}{\\alpha}}))\\approx -\\ln\\varepsilon/\\ln 2</math>\n:<math>-\\ln(1-(1/2)^{1-\\frac{1}{\\alpha}})\\approx W(-\\ln\\varepsilon/\\ln 2)</math>\nwhere ''W'' is the [[Lambert W function]]. So\n:<math>(1/2)^{1-\\frac{1}{\\alpha}}\\approx 1-\\exp(-W(-\\ln\\varepsilon/\\ln 2))</math>\n:<math>{1-\\frac{1}{\\alpha}}\\approx -\\ln(1-\\exp(-W(-\\ln\\varepsilon/\\ln 2)))/\\ln 2</math>\n:<math>\\alpha\\approx 1/(1+\\ln(1-\\exp(-W(-\\ln\\varepsilon/\\ln 2)))/\\ln 2)</math>\n-->The solution is that ''α'' equals about 1.15, and about 9% of the wealth is owned by each of the two groups. But actually the poorest 69% of the world adult population owns only about 3% of the wealth.<ref>{{cite web|title=Global Wealth Report 2013|url=https://publications.credit-suisse.com/tasks/render/file/?fileID=BCDB1364-A105-0560-1332EC9100FF5C83|publisher=Credit Suisse|page=22|date=Oct 2013}}</ref>\n\nThe [[Gini coefficient]] is a measure of the deviation of the Lorenz curve from the equidistribution line which is a line connecting [0,&nbsp;0] and [1,&nbsp;1], which is shown in black (''α''&nbsp;=&nbsp;∞) in the Lorenz plot on the right. Specifically, the Gini coefficient is twice the area between the Lorenz curve and the equidistribution line. The Gini coefficient for the Pareto distribution is then calculated (for <math>\\alpha\\ge 1</math>) to be\n\n:<math>G = 1-2 \\left (\\int_0^1L(F) \\, dF \\right ) = \\frac{1}{2\\alpha-1}</math>\n\n(see Aaberge 2005).\n\n==Parameter estimation==\nThe [[likelihood function]] for the Pareto distribution parameters ''α'' and ''x''<sub>m</sub>, given an independent [[sample (statistics)|sample]] ''x'' =&nbsp;(''x''<sub>1</sub>,&nbsp;''x''<sub>2</sub>,&nbsp;...,&nbsp;''x<sub>n</sub>''), is\n\n: <math>L(\\alpha, x_\\mathrm{m}) = \\prod_{i=1}^n \\alpha \\frac {x_\\mathrm{m}^\\alpha} {x_i^{\\alpha+1}} = \\alpha^n x_\\mathrm{m}^{n\\alpha} \\prod_{i=1}^n \\frac {1}{x_i^{\\alpha+1}}.</math>\n\nTherefore, the logarithmic likelihood function is\n\n: <math>\\ell(\\alpha, x_\\mathrm{m}) = n \\ln \\alpha + n\\alpha \\ln x_\\mathrm{m} - (\\alpha + 1) \\sum_{i=1} ^n \\ln x_i.</math>\n\nIt can be seen that <math>\\ell(\\alpha, x_\\mathrm{m})</math> is monotonically increasing with ''x''<sub>m</sub>, that is, the greater the value of ''x''<sub>m</sub>, the greater the value of the likelihood function. Hence, since ''x'' ≥ ''x''<sub>m</sub>, we conclude that\n\n: <math>\\widehat x_\\mathrm{m} = \\min_i {x_i}.</math>\n\nTo find the [[estimator]] for ''α'', we compute the corresponding partial derivative and determine where it is zero:\n\n: <math>\\frac{\\partial \\ell}{\\partial \\alpha} = \\frac{n}{\\alpha} + n \\ln x_\\mathrm{m} - \\sum _{i=1}^n \\ln x_i = 0.</math>\n\nThus the [[maximum likelihood]] estimator for ''α'' is:\n\n: <math>\\widehat \\alpha = \\frac{n}{\\sum _i  \\ln (x_i/\\widehat x_\\mathrm{m}) }.</math>\n\nThe expected statistical error is:<ref>{{cite journal |author=M. E. J. Newman |year=2005 |title=Power laws, Pareto distributions and Zipf's law |journal=[[Contemporary Physics]] |volume=46 |issue=5 |pages=323–51| arxiv=cond-mat/0412004 |doi=10.1080/00107510500052444 |bibcode=2005ConPh..46..323N}}</ref>\n\n: <math>\\sigma = \\frac {\\widehat \\alpha} {\\sqrt n}. </math>\n\nMalik (1970)<ref>{{cite journal |author=H. J. Malik |year=1970 |title=Estimation of the Parameters of the Pareto Distribution |journal=Metrika |volume=15|pages=126–132 |doi=10.1007/BF02613565 }}</ref> gives the exact joint distribution of <math>(\\hat{x}_\\mathrm{m},\\hat\\alpha)</math>. In particular, <math>\\hat{x}_\\mathrm{m}</math> and <math>\\hat\\alpha</math> are [[Independence (probability theory)|independent]] and <math>\\hat{x}_\\mathrm{m}</math> is Pareto with scale parameter ''x''<sub>m</sub> and shape parameter ''nα'', whereas <math>\\hat\\alpha</math> has an [[inverse-gamma distribution]] with shape and scale parameters ''n''&nbsp;−&nbsp;1 and ''nα'', respectively.\n\n==Graphical representation==\nThe characteristic curved '[[long tail]]' distribution when plotted on a linear scale, masks the underlying simplicity of the function when plotted on a [[log-log graph]], which then takes the form of a straight line with negative gradient: It follows from the formula for the probability density function that for ''x'' ≥ ''x''<sub>m</sub>,\n\n:<math>\\log f_X(x)= \\log \\left(\\alpha\\frac{x_\\mathrm{m}^\\alpha}{x^{\\alpha+1}}\\right) = \\log (\\alpha x_\\mathrm{m}^\\alpha) - (\\alpha+1) \\log x.</math>\n\nSince ''α'' is positive, the gradient −(''α''&nbsp;+&nbsp;1) is negative.\n\n==Random sample generation==\nRandom samples can be generated using [[inverse transform sampling]]. Given a random variate ''U'' drawn from the [[uniform distribution (continuous)|uniform distribution]] on the unit interval (0,&nbsp;1], the variate ''T'' given by\n\n:<math>T=\\frac{x_\\mathrm{m}}{U^{1/\\alpha}}</math>\n\nis Pareto-distributed.<ref>{{cite book |last=Tanizaki |first=Hisashi |title=Computational Methods in Statistics and Econometrics |year=2004 |page=133 |publisher=CRC Press |url=https://books.google.com/books?id=pOGAUcn13fMC&printsec=frontcover|isbn=9780824750886 }}</ref> If ''U'' is uniformly distributed on [0,&nbsp;1), it can be exchanged with (1&nbsp;−&nbsp;''U'').\n\n==Variants==\n\n===Bounded Pareto distribution===\n{{See also|Truncated distribution}}\n{{Probability distribution\n | name       =Bounded Pareto\n | type       =density\n | pdf_image  =\n | cdf_image  =\n | parameters =\n<math>L > 0</math> [[location parameter|location]] ([[real numbers|real]])<br />\n<math>H > L</math> [[location parameter|location]] ([[real numbers|real]])<br />\n<math>\\alpha > 0</math> [[shape parameter|shape]] (real)\n | support    =<math>L \\leqslant x \\leqslant H</math>\n | pdf        =<math>\\frac{\\alpha L^\\alpha x^{-\\alpha - 1}}{1-\\left(\\frac{L}{H}\\right)^\\alpha}</math>\n | cdf        =<math>\\frac{1-L^\\alpha x^{-\\alpha}}{1-\\left(\\frac{L}{H}\\right)^\\alpha}</math>\n | mean       =<math>\\frac{L^\\alpha}{1 - \\left(\\frac{L}{H}\\right)^\\alpha} \\cdot \\left(\\frac{\\alpha}{\\alpha-1}\\right) \\cdot \\left(\\frac{1}{L^{\\alpha-1}} - \\frac{1}{H^{\\alpha-1}}\\right), \\alpha\\neq 1 </math>\n | median     =<math> L \\left(1- \\frac{1}{2}\\left(1-\\left(\\frac{L}{H}\\right)^\\alpha\\right)\\right)^{-\\frac{1}{\\alpha}}</math>\n | mode       =\n | variance   =<math>\\frac{L^\\alpha}{1 - \\left(\\frac{L}{H}\\right)^\\alpha} \\cdot \\left(\\frac{\\alpha}{\\alpha-2}\\right) \\cdot \\left(\\frac{1}{L^{\\alpha-2}} - \\frac{1}{H^{\\alpha-2}}\\right), \\alpha\\neq 2</math> (this is the second raw moment, not the variance)\n | skewness   = <math>\\frac{L^{\\alpha}}{1-\\left(\\frac{L}{H}\\right)^{\\alpha}} \\cdot \\frac{\\alpha * (L^{k-\\alpha}-H^{k-\\alpha})}{(\\alpha-k)}, \\alpha \\neq j </math>\n(this is the kth raw moment, not the skewness)\n | kurtosis   =\n | entropy    =\n | mgf        =\n | char       =\n}}\n\nThe bounded (or truncated) Pareto distribution has three parameters: ''α'', ''L'' and ''H''. As in the standard Pareto distribution ''α'' determines the shape. ''L'' denotes the minimal value, and ''H'' denotes the maximal value.\n\nThe [[probability density function]] is\n\n: <math>\\frac{\\alpha L^\\alpha x^{-\\alpha - 1}}{1-\\left(\\frac{L}{H}\\right)^\\alpha}</math>,\n\nwhere ''L''&nbsp;≤&nbsp;''x''&nbsp;≤&nbsp;''H'', and ''α''&nbsp;>&nbsp;0.\n\n====Generating bounded Pareto random variables====\nIf ''U'' is [[uniform distribution (continuous)|uniformly distributed]] on (0,&nbsp;1), then applying inverse-transform method <ref>http://www.cs.bgu.ac.il/~mps042/invtransnote.htm</ref>\n\n:<math>U = \\frac{1 - L^\\alpha x^{-\\alpha}}{1 - (\\frac{L}{H})^\\alpha}</math>\n:<math>x = \\left(-\\frac{U H^\\alpha - U L^\\alpha - H^\\alpha}{H^\\alpha L^\\alpha}\\right)^{-\\frac{1}{\\alpha}}</math>\n\nis a bounded Pareto-distributed.{{Citation needed|date=February 2011}}\n{{Clear}}\n\n===Symmetric Pareto distribution===\nThe symmetric Pareto distribution can be defined by the [[probability density function]]:<ref>{{cite web|title=Do Financial Returns Have Finite or Infinite Variance? A Paradox and an Explanation|author1=Grabchak, M.  |author2=Samorodnitsky, D. |lastauthoramp=yes | pages=7–8 |url=http://people.orie.cornell.edu/~gennady/techreports/RetTailParadoxExplFinal.pdf}}</ref>\n\n:<math>f(x;\\alpha,x_\\mathrm{m}) = \\begin{cases}\n\\tfrac{1}{2}\\alpha x_\\mathrm{m}^\\alpha |x|^{-\\alpha-1} & |x|>x_\\mathrm{m} \\\\\n0 & \\text{otherwise}.\n\\end{cases}</math>\n\nIt has a similar shape to a Pareto distribution for ''x'' > ''x''<sub>m</sub> and is [[reflection symmetry|mirror symmetric]] about the vertical axis.\n\n==Multivariate Pareto distribution==\nThe univariate Pareto distribution has been extended to a [[multivariate distribution|multivariate]] Pareto distribution.<ref>{{cite journal\n|last1=Rootzén|first1=Holger |last2=Tajvidi|first2=Nader \n|title=Multivariate generalized Pareto distributions |journal=Bernoulli|volume=12|year=2006|number=5 |pages=917–30 \n|doi=10.3150/bj/1161614952 \n|ref=harv |citeseerx=10.1.1.145.2991}}</ref>\n\n==See also==\n* [[Bradford's law]]\n* [[Matthew effect]]\n* [[Pareto analysis]]\n* [[Pareto efficiency]]\n* [[Pareto interpolation]]\n* [[Power law#Power-law probability distributions|Power law probability distributions]]\n* [[Sturgeon's law]]\n* [[Traffic generation model]]\n* [[Zipf's law]]\n* [[Heavy-tailed distribution]]\n\n==References==\n{{reflist|30em}}\n\n==Notes==\n* {{cite journal |author=M. O. Lorenz |year=1905 |title=Methods of measuring the concentration of wealth |journal=[[Publications of the American Statistical Association]] |volume=9 |issue=70 |pages=209–19 |doi=10.2307/2276207 |bibcode=1905PAmSA...9..209L|jstor=2276207}}\n\n* {{cite journal\n| title=Ecrits sur la courbe de la répartition de la richesse\n| first=Vilfredo\n| last=Pareto\n| editor=Librairie Droz\n| year=1965\n| pages=48\n| series=Œuvres complètes : T. III\n| isbn=9782600040211}}\n\n* {{cite journal | last = Pareto | first = Vilfredo | year = 1895 | title = La legge della domanda | journal = Giornale Degli Economisti | volume = 10 | issue = | pages = 59–68 }}\n\n* {{cite journal\n| first=Vilfredo\n| last=Pareto\n| year=1896\n| title=Cours d'économie politique\n| doi=10.1177/000271629700900314}}\n\n==External links== \n* {{springer|title=Pareto distribution|id=p/p071580}}\n\n* {{MathWorld |title=Pareto distribution |id=ParetoDistribution}}\n\n* {{citation\n| url=http://www3.unisi.it/eventi/GiniLorenz05/25%20may%20paper/PAPER_Aaberge.pdf\n| title=Gini's Nuclear Family\n| first=Rolf\n| last=Aabergé\n| conference=International Conference to Honor Two Eminent Social Scientists\n| date=May 2005}}\n\n* {{cite conference\n| url=https://www.cs.bu.edu/~crovella/paper-archive/self-sim/journal-version.pdf\n| title=Self-Similarity in World Wide Web Traffic: Evidence and Possible Causes\n| first1=Mark E.\n| last1=Crovella\n| first2=Azer\n| last2=Bestavros\n| conference=IEEE/ACM Transactions on Networking\n| volume=5\n| number=6\n| pages=835-846\n| date=December 1997}}\n\n* [http://www.csee.usf.edu/~kchriste/tools/syntraf1.c syntraf1.c] is a [[C program]] to generate synthetic packet traffic with bounded Pareto burst size and exponential interburst time.\n\n{{ProbDistributions|continuous-semi-infinite}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Pareto Distribution}}\n[[Category:Actuarial science]]\n[[Category:Continuous distributions]]\n[[Category:Power laws]]\n[[Category:Probability distributions with non-finite variance]]\n[[Category:Exponential family distributions]]\n[[Category:Vilfredo Pareto]]"
    },
    {
      "title": "Simon model",
      "url": "https://en.wikipedia.org/wiki/Simon_model",
      "text": "In applied probability theory, the '''Simon model''' is a class of [[stochastic model]]s that results in a [[power-law]] distribution function.  It was proposed by [[Herbert A. Simon]]<ref name=simon>Simon, H. A., 1955, Biometrika 42, 425.</ref> to account for the wide range of empirical [[Frequency distribution|distributions]] following a power-law. It models the dynamics of a system of elements with associated counters (e.g., words and their frequencies in texts, or nodes in a network and their connectivity <math>k</math>). In this model the dynamics of the system is based on constant growth via addition of new elements (new instances of words) as well as incrementing the counters (new occurrences of a word) at a rate proportional to their current values.\n\n== Description ==\nTo model this type of network growth as described above, Bornholdt and Ebel<ref name=BE>Bornholdt, S. and H. Ebel, Phys. Rev. E 64 (2001) 035104(R).</ref> considered a network with <math>n</math> nodes, and each node with connectivities <math>k_i</math>, <math>i = 1, \\ldots, n</math>. These nodes\nform classes <math>[k]</math> of <math>f(k)</math> nodes with identical connectivity <math>k</math>.\nRepeat the following steps:\n\n(i) With probability <math>\\alpha</math> add a new node and attach a link to it from an arbitrarily chosen node.\n\n(ii) With probability <math>1-\\alpha</math> add one link from an arbitrary node to a node <math>j</math> of class <math>[k]</math> chosen with a probability proportional to <math>k f(k)</math>.\n\nFor this stochastic process, Simon found a stationary solution exhibiting [[power-law]] scaling, <math>P(k) \\propto k^{- \\gamma}</math>, with exponent <math>\\gamma = 1 + \\frac{1}{1- \\alpha}.</math>\n\n== Properties ==\n(i) [[Barabási-Albert (BA) model]] can be mapped to the subclass <math>\\alpha= 1/2</math> of Simon's model, when using the simpler probability for a node being\nconnected to another node <math>i</math> with connectivity <math>k_i</math> <math>P(\\mathrm{new\\ link\\ to\\ } i) \\propto k_i </math> (same as the preferential attachment at [[BA model]]). In other words, the Simon model describes a general class of stochastic processes that can result in a [[scale-free network]], appropriate to capture [[Zipf's law|Pareto and Zipf's laws]].\n\n(ii) The only free parameter of the model <math>\\alpha</math> reflects the relative\ngrowth of number of nodes versus the number of links. In general <math>\\alpha</math> has small values; therefore, the scaling exponents can be predicted to be <math>\\gamma\\approx 2</math>. For instance, Bornholdt and Ebel<ref name=\"BE\"/> studied the linking dynamics of World Wide Web, and predicted the scaling exponent as <math>\\gamma \\approx 2.1</math>, which was consistent with observation.\n\n(iii) The interest in the scale-free model comes from its ability to describe the topology of complex networks. The Simon model does not have an underlying network structure, as it was designed to describe events whose frequency follows a [[power-law]]. Thus network measures going beyond the [[degree distribution]] such\nas the [[average path length]], [https://web.archive.org/web/20070128124135/http://austria.phys.nd.edu/netwiki/index.php/Graph_Spectra spectral properties], and [[clustering coefficient]], cannot be obtained from this mapping.\n\nThe Simon model is related to [[generalized scale-free model]]s with growth and preferential attachment properties. For more reference, see.<ref name=BA>Barabási, A.-L., and R. Albert, Statistical mechanics of complex networks, Reviews of Modern Physics, Vol 74, page 47-97, 2002.</ref><ref name=AM>Amaral, L. A. N., A. Scala, M. Barthelemy, and H. E. Stanley, 2000, Proc. Natl. Acad. Sci. U.S.A. '''97''', 11149.</ref>\n\n== References ==\n<references/>\n\n[[Category:Power laws]]"
    },
    {
      "title": "Stable distribution",
      "url": "https://en.wikipedia.org/wiki/Stable_distribution",
      "text": "{{Distinguish|Stationary distribution}}\n\n{{Probability distribution\n  | name       = Stable\n  | type       = continuous\n  | pdf_image  = [[Image:Levy distributionPDF.png|325px|Symmetric stable distributions]]<br /><small>Symmetric ''α''-stable distributions with unit scale factor</small><br />[[Image:Levyskew distributionPDF.png|325px|Skewed centered stable distributions]]<br /><small>Skewed centered stable distributions with unit scale factor</small>\n  | cdf_image  = [[Image:Levy distributionCDF.png|325px|CDF's for symmetric ''α''-stable distributions]]<br /><small>CDFs for symmetric ''α''-stable distributions <br />[[Image:Levyskew distributionCDF.png|325px|CDF's for skewed centered Lévy distributions]]<br /><small>CDFs for skewed centered stable distributions</small>\n  | parameters = α ∈ (0, 2] — stability parameter <br>\nβ ∈ [−1, 1] — skewness parameter (note that [[skewness]] is undefined)<br>\n''c'' ∈ (0, ∞) — [[scale parameter]] <br>\nμ ∈ (−∞, ∞) — [[location parameter]]\n  | support    = ''x'' ∈ '''R''', or ''x'' ∈ [μ, +∞) if α < 1 and {{nowrap|β {{=}} 1}}, or ''x'' ∈ (-∞, μ] if {{nowrap|α < 1}} and {{nowrap|β {{=}} −1}}\n  | pdf        = not analytically expressible, except for some parameter values\n  | cdf        = not analytically expressible, except for certain parameter values\n  | mean       = μ when {{nowrap|α > 1}}, otherwise undefined\n  | median     = μ when {{nowrap|β {{=}} 0}}, otherwise not analytically expressible\n  | mode       = μ when {{nowrap|β {{=}} 0}}, otherwise not analytically expressible\n  | variance   = 2''c''<sup>2</sup> when {{nowrap|α {{=}} 2}}, otherwise infinite\n  | skewness   = 0 when {{nowrap|α {{=}} 2}}, otherwise undefined\n  | kurtosis   = 0 when {{nowrap|α {{=}} 2}}, otherwise undefined\n  | entropy    = not analytically expressible, except for certain parameter values\n  | mgf        = <math>\\exp\\!\\left(t\\mu + c^2t^2\\right)</math> when <math>\\alpha=2</math>, otherwise undefined\n  | char       = <math>\\exp\\!\\Big[\\; it\\mu - |c\\,t|^\\alpha\\,(1-i \\beta\\sgn(t)\\Phi) \\;\\Big],</math><br>\nwhere <math>\\Phi = \\begin{cases} \\tan\\tfrac{\\pi\\alpha}{2} & \\text{if }\\alpha \\ne 1 \\\\ -\\tfrac{2}{\\pi}\\log|c\\,t| & \\text{if }\\alpha = 1 \\end{cases}</math>}}\n\nIn [[probability theory]], a [[probability distribution|distribution]] is said to be '''stable''' if a [[linear combination]] of two [[Independence (probability theory)|independent]] [[random variable]]s with this distribution has the same distribution, [[up to]] [[location parameter|location]] and [[scale parameter|scale]] parameters. A random variable is said to be '''stable''' if its distribution is stable. The stable distribution family is also sometimes referred to as the '''Lévy alpha-stable distribution''', after [[Paul Lévy (mathematician)|Paul Lévy]], the first mathematician to have studied it.<ref name=\"BM 1960\">B. Mandelbrot, The Pareto–Lévy Law and the Distribution of Income, International Economic Review 1960 https://www.jstor.org/stable/2525289</ref><ref>Paul Lévy, Calcul des probabilités 1925</ref>\n\nOf the four parameters defining the family, most attention has been focused on the stability parameter, α (see panel). Stable distributions have 0 < α ≤ 2, with the upper bound corresponding to the [[normal distribution]], and α = 1 to the [[Cauchy distribution]]. The distributions have undefined [[variance]] for α < 2, and undefined [[mean]] for α ≤ 1. The importance of stable probability distributions is that they are \"[[attractor]]s\" for properly normed sums of independent and identically distributed ([[iid]]) random variables. The normal distribution defines a family of stable distributions. By the classical [[central limit theorem]] the properly normed sum of a set of random variables, each with finite variance, will tend toward a normal distribution as the number of variables increases. Without the finite variance assumption, the limit may be a stable distribution that is not normal.  [[Benoit Mandelbrot|Mandelbrot]] referred to such distributions as \"stable Paretian distributions\",<ref>B.Mandelbrot, Stable Paretian Random Functions and the Multiplicative Variation of Income, Econometrica 1961 https://www.jstor.org/stable/pdfplus/1911802.pdf</ref><ref>B. Mandelbrot, The variation of certain Speculative Prices, The Journal of Business 1963 [http://web.williams.edu/Mathematics/sjmiller/public_html/341Fa09/econ/Mandelbroit_VariationCertainSpeculativePrices.pdf]</ref><ref>Eugene F. Fama, Mandelbrot and the Stable Paretian Hypothesis, The Journal of Business 1963</ref> after [[Vilfredo Pareto]]. In particular, he referred to those maximally skewed in the positive direction with 1&nbsp;<&nbsp;α&nbsp;<&nbsp;2 as \"Pareto–Lévy distributions\",<ref name=\"BM 1960\"/> which he regarded as better descriptions of stock and commodity prices than normal distributions.<ref name=\"BM 1963\">Mandelbrot, B., New methods in statistical economics [[The Journal of Political Economy]], 71 #5, 421–440 (1963).</ref>\n\n==Definition==\nA non-[[degenerate distribution]] is a stable distribution if it satisfies the following property:\n\n:Let ''X''<sub>1</sub> and ''X''<sub>2</sub> be independent copies of a [[random variable]] ''X''. Then ''X'' is said to be '''stable''' if for any constants ''a'' > 0 and ''b'' > 0 the random variable ''aX''<sub>1</sub> + ''bX''<sub>2</sub> has the same distribution as ''cX'' + ''d'' for some constants ''c'' > 0 and ''d''. The distribution is said to be ''strictly stable'' if this holds with ''d'' = 0.<ref name=\":0\">{{Cite web|url = http://academic2.american.edu/~jpnolan/stable/chap1.pdf|title = Stable Distributions – Models for Heavy Tailed Data|date = |accessdate = 2009-02-21|website = |last = Nolan|first = John P.}}</ref>\n\nSince the [[normal distribution]], the [[Cauchy distribution]], and the [[Lévy distribution]] all have the above property, it follows that they are special cases of stable distributions.\n\nSuch distributions form a four-parameter family of continuous [[probability distribution]]s parametrized by location and scale parameters μ and ''c'', respectively, and two shape parameters β and α, roughly corresponding to measures of asymmetry and concentration, respectively (see the figures).\n\nAlthough the probability density function for a general stable distribution cannot be written analytically, the general characteristic function can be. Any probability distribution is given by the [[Fourier transform]] of its probability density function, or simply its [[Characteristic function (probability theory)|characteristic function]] φ(''t'') by:<ref>{{Cite web|url=https://www.randomservices.org/random/special/Stable.html|title=Stable Distributions|last=Siegrist|first=Kyle|website=www.randomservices.org|language=en|access-date=2018-10-18}}</ref>\n\n:<math> f(x)=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\varphi(t)e^{-ixt}\\,dt </math>\n\nA random variable ''X'' is called stable if its characteristic function can be written as<ref name=\":0\" /><ref name=\":1\">{{Cite book|title = The Statistical Mechanics of Financial Markets – Springer|doi = 10.1007/b137351|last = Voit|first = Johannes|publisher = Springer|year = 2005|series = Texts and Monographs in Physics|isbn = 978-3-540-26285-5}}</ref>\n\n: <math> \\varphi(t;\\alpha,\\beta,c,\\mu) =  \\exp\\left (it\\mu - |c t|^\\alpha \\left (1- i \\beta\\sgn(t)\\Phi \\right ) \\right ) </math>\n\nwhere sgn(''t'') is just the [[sign function|sign]] of ''t'' and\n\n:<math>\\Phi= \\begin{cases} \\tan \\left (\\frac{\\pi \\alpha}{2} \\right) & \\alpha \\neq 1 \\\\ -\\frac{2}{\\pi}\\log|t| & \\alpha = 1 \\end{cases}</math>\n\nμ ∈ '''R'''  is a shift parameter, β ∈ [−1, 1], called the ''skewness parameter'', is a measure of asymmetry. Notice that in this context the usual [[skewness]] is not well defined, as for α < 2 the distribution does not admit 2nd or higher [[moment (mathematics)|moments]], and the usual skewness definition is the 3rd [[central moment]].\n\nThe reason this gives a stable distribution is that the characteristic function for the sum of two random variables equals the product of the two corresponding characteristic functions. Adding two random variables from a stable distribution gives something with the same values of α and β, but possibly different values of μ and ''c''.\n\nNot every function is the characteristic function of a legitimate probability distribution (that is, one whose cumulative distribution function is real and goes from 0 to 1 without decreasing), but the characteristic functions given above will be legitimate so long as the parameters are in their ranges. The value of the characteristic function at some value ''t'' is the complex conjugate of its value at −''t'' as it should be so that the probability distribution function will be real.\n\nIn the simplest case β = 0, the characteristic function is just a [[stretched exponential function]]; the  distribution is symmetric about μ and is referred to as a (Lévy) '''symmetric alpha-stable distribution''', often abbreviated ''SαS''.\n\nWhen α < 1 and β = 1, the distribution is supported by [μ, ∞).\n\nThe parameter ''c'' > 0 is a scale factor which is a measure of the width of the distribution while α is the exponent or index of the distribution and specifies the asymptotic behavior of the distribution.\n\n=== Parametrizations ===\nThe above definition is only one of the parametrizations in use for stable distributions; it is the most common but is not continuous in the parameters at {{math|''α'' {{=}} 1}}.\n\nA continuous parametrization  is<ref name=\":0\" />\n: <math> \\varphi(t;\\alpha,\\beta,\\gamma,\\delta)=\\exp\\left (it\\delta-|\\gamma t|^\\alpha \\left (1-i\\beta \\sgn(t)\\Phi \\right ) \\right ) </math>\n\nwhere:\n\n:<math>\\Phi= \\begin{cases} \\left (|\\gamma t|^{1-\\alpha}-1 \\right )\\tan \\left (\\tfrac{\\pi \\alpha}{2} \\right )  & \\alpha \\neq 1 \\\\  -\\frac{2}{\\pi}\\log|\\gamma t| & \\alpha = 1 \\end{cases} </math>\n\nThe ranges of α and β are the same as before, γ (like ''c'') should be positive, and δ (like μ) should be real.\n\nIn either parametrization one can make a linear transformation of the random variable to get a random variable whose density is <math>f(y;\\alpha,\\beta,1,0)</math>. In the first parametrization, this is done by defining the new variable:\n\n:<math> y= \\begin{cases} \\frac{x-\\mu}\\gamma & \\alpha \\neq 1 \\\\  \\frac{x-\\mu}\\gamma-\\beta\\frac 2\\pi\\ln\\gamma  & \\alpha = 1 \\end{cases} </math>\n\nFor the second parametrization, we simply use\n\n:<math> y=\\frac{x-\\delta}\\gamma.</math>\n\nno matter what α is. In the first parametrization, if the mean exists (that is, {{math|''α'' > 1}}) then it is equal to μ, whereas in the second parametrization when the mean exists it is equal to <math>\\delta-\\beta\\gamma\\tan \\left (\\tfrac{\\pi\\alpha}{2} \\right).</math>\n\n===The distribution===\nA stable distribution is therefore specified by the above four parameters. It can be shown that any non-degenerate stable distribution has a smooth (infinitely differentiable) density function.<ref name=\":0\" /> If <math>f(x;\\alpha, \\beta,c,\\mu)</math> denotes the density of ''X'' and ''Y'' is the sum of independent copies of ''X'':\n\n:<math>Y = \\sum_{i=1}^N k_i (X_i-\\mu)\\,</math>\n\nthen ''Y'' has the density <math>s^{-1}f(y/s;\\alpha,\\beta,c,0)</math> with\n\n:<math>s=\\left(\\sum_{i=1}^N |k_i|^\\alpha\\right)^{\\frac{1}{\\alpha}}.</math>\n\nThe asymptotic behavior is described, for α< 2, by:<ref name=\":0\" />\n\n:<math>f(x)\\sim\\frac{1}{|x|^{1+\\alpha}}  \\left (c^\\alpha (1+\\sgn(x)\\beta) \\sin \\left (\\frac{\\pi \\alpha}{2} \\right )\\frac{\\Gamma(\\alpha+1) }{\\pi} \\right ) </math>\n\nwhere Γ is the [[Gamma function]] (except that when ''α'' &ge; 1 and ''β'' = ±1, the tail does not vanish to the left or right, resp., of ''μ'', although the above expression is 0). This \"[[heavy tail]]\" behavior causes the variance of stable distributions to be infinite for all α < 2. This property is illustrated in the log-log plots below.\n\nWhen ''α'' = 2, the distribution is Gaussian (see below), with tails asymptotic to exp(−''x''<sup>2</sup>/4''c''<sup>2</sup>)/(2c√π).\n\n== One-sided stable distribution and stable count distribution ==\nWhen α < 1 and β = 1, the distribution is supported by [μ, ∞). This family is called \"one-sided stable distribution\".<ref>{{Cite journal|last=Penson|first=K. A.|last2=Górska|first2=K.|date=2010-11-17|title=Exact and Explicit Probability Densities for One-Sided L\\'evy Stable Distributions|journal=Physical Review Letters|volume=105|issue=21|pages=210604|doi=10.1103/PhysRevLett.105.210604|pmid=21231282|arxiv=1007.0193|bibcode=2010PhRvL.105u0604P}}</ref> Its standard distribution is denoted as\n:<math>L_\\alpha(x)=f(x;\\alpha,1,\\cos(\\frac{\\pi\\alpha}{2})^{1/\\alpha},0)</math>, where  <math>\\alpha<1</math>.\n\nConsider the Lévy sum <math>Y = \\sum_{i=1}^N X_i</math> where <math>X_i\\sim L_\\alpha(x)</math>, then ''Y'' has the density <math>\\frac{1}{\\nu}L_\\alpha(\\frac{x}{\\nu})</math>  where <math>\\nu=N^{1/\\alpha}</math>. Set <math>x=1</math>, we arrive at the \"stable count distribution\".<ref name=\":4\" /> Its standard distribution is defined as\n\n:<math>\\mathfrak{N}_\\alpha(\\nu)=\\frac{\\alpha}{\\Gamma(\\frac{1}{\\alpha})} \\frac{1}{\\nu} L_\\alpha(\\frac{1}{\\nu})\n</math>, where  <math>\\nu>0</math> and <math>\\alpha<1</math>.\n\nThe stable count distribution is the conjugate prior of the one-sided stable distribution. Its location-scale family is defined as\n\n:<math>\\mathfrak{N}_\\alpha(\\nu;\\nu_0,\\theta)= \\frac{\\alpha}{\\Gamma(\\frac{1}{\\alpha})} \n\\frac{1}{\\nu-\\nu_0} L_\\alpha(\\frac{\\theta}{\\nu-\\nu_0})\n</math>, where  <math>\\nu>\\nu_0</math>, <math>\\theta>0</math>, and <math>\\alpha<1</math>.\n\nIt is also a one-sided distribution supported by <math>[\\nu_0,\\infty)\n</math>. The location parameter <math>\\nu_0\n</math> is the cut-off location, while <math>\\theta\n</math>  defines the scale of the one-sided distribution.\n\nWhen <math>\\alpha=\\frac{1}{2}</math>, <math>L_{\\frac{1}{2}}(x)</math> is the [[Lévy distribution]] which is an inverse gamma distribution. Thus <math>\\mathfrak{N}_{\\frac{1}{2}}(\\nu;\\nu_0,\\theta)\n</math> is a shifted [[gamma distribution]] of shape 3/2 and scale <math>4\\theta\n</math>,\n\n:<math>\\mathfrak{N}_{\\frac{1}{2}}(\\nu;\\nu_0,\\theta)= \n\\frac{1}{4\\sqrt{\\pi}\\theta^{3/2}} \n(\\nu-\\nu_0)^{1/2} e^{-\\frac{\\nu-\\nu_0}{4\\theta}}\n</math>, where  <math>\\nu>\\nu_0</math>, <math>\\theta>0</math>.\n\nIts mean is <math>\\nu_0+6\\theta\n</math> and its standard deviation is <math>\\sqrt{24}\\theta\n</math>. It is hypothesized that [[VIX]] is distributed like <math>\\mathfrak{N}_{\\frac{1}{2}}(\\nu;\\nu_0,\\theta)\n</math> with <math>\\nu_0=10.4\n</math> and <math>\\theta=1.6\n</math> (See Section 7 of <ref name=\":4\" />). Thus the stable count distribution is the first-order marginal distribution of a volatility process. In this context, <math>\\nu_0\n</math> is called the \"floor volatility\".\n\nAnother approach to derive the stable count distribution is to use the Laplace transform of the one-sided stable distribution, (Section 2.4 of <ref name=\":4\" />)\n\n:<math>\\int_{0}^{\\infty} e^{-z x} L_\\alpha(x) dx = e^{-z^\\alpha}</math>, where  <math>\\alpha<1</math>.\n\nLet <math>x=1/\\nu</math>, and one can decompose the integral on the left hand side as a [[product distribution]] of a standard [[Laplace distribution]] and a standard stable count distribution,\n\n:<math>\\int_{0}^{\\infty} \\left ( \\frac{1}{2\\nu} e^{-\\frac{|z|}{\\nu} }\\right )\n\\left (\\frac{\\alpha}{\\Gamma(\\frac{1}{\\alpha})}  \\frac{1}{\\nu} L_\\alpha(\\frac{1}{\\nu}) \\right ) d \\nu \n= \\frac{1}{2} \\frac{\\alpha}{\\Gamma(\\frac{1}{\\alpha})} e^{-|z|^\\alpha}</math>, where  <math>\\alpha<1</math>.\n\nThis is called the \"lambda decomposition\" (See Section 4 of <ref name=\":4\" />) since the right hand side was named as \"symmetric lambda distribution\" in Lihn's former works. However, it has several more popular names such as \"[[exponential power distribution]]\", or the \"generalized error/normal distribution\", often referred to when α > 1'''.'''\n\nThe n-th moment of <math>\\mathfrak{N}_\\alpha(\\nu)\n</math> is the <math>-(n+1)\n</math>-th moment of <math>L_\\alpha(x)</math>, All positive moments are finite. This in a way solves the thorny issue of diverging moments in the stable distribution.\n\n== Properties ==\n* All stable distributions are [[infinitely divisible distribution|infinitely divisible]].\n* With the exception of the [[normal distribution]] (α = 2), stable distributions are [[leptokurtotic]] and [[heavy-tailed distribution]]s.\n* Closure under convolution\n\nStable distributions are closed under convolution for a fixed value of α. Since convolution is equivalent to multiplication of the Fourier-transformed function, it follows that the product of two stable characteristic functions with the same α will yield another such characteristic function. The product of two stable characteristic functions is given by:\n\n:<math>\\exp\\left (it\\mu_1+it\\mu_2 - |c_1 t|^\\alpha - |c_2 t|^\\alpha +i\\beta_1|c_1 t|^\\alpha\\sgn(t)\\Phi +i\\beta_2|c_2 t|^\\alpha\\sgn(t)\\Phi \\right )</math>\n\nSince Φ is not a function of the μ, ''c'' or β variables it follows that these parameters for the convolved function are given by:\n\n:<math>\\begin{align}\n\\mu &=\\mu_1+\\mu_2 \\\\\n|c|  &= \\left (|c_1|^\\alpha+|c_2|^\\alpha \\right )^{\\frac{1}{\\alpha}} \\\\[6pt]\n\\beta &= \\frac{\\beta_1 |c_1|^\\alpha+\\beta_2|c_2|^\\alpha}{|c_1|^\\alpha+|c_2|^\\alpha}\n\\end{align}</math>\n\nIn each case, it can be shown that the resulting parameters lie within the required intervals for a stable distribution.\n\n== A generalized central limit theorem ==\nAnother important property of stable distributions is the role that they play in a generalized [[central limit theorem]]. The central limit theorem states that the sum of a number of independent and identically distributed (i.i.d.) random variables with finite non-zero variances will tend to a [[normal distribution]] as the number of variables grows.\n\nA generalization due to [[Boris Vladimirovich Gnedenko|Gnedenko]] and [[Andrey Nikolaevich Kolmogorov|Kolmogorov]] states that the sum of a number of random variables with symmetric distributions having power-law tails ([[Pareto distribution|Paretian tail]]s), decreasing as <math>|x|^{-\\alpha-1}</math> where <math>0 < \\alpha \\leqslant 2</math> (and therefore having infinite variance), will tend to a stable distribution <math>f(x;\\alpha,0,c,0)</math> as the number of summands grows.<ref>B.V. Gnedenko, A.N. Kolmogorov. Limit distributions for sums of independent random variables, Cambridge, Addison-Wesley 1954 https://books.google.com/books/about/Limit_distributions_for_sums_of_independ.html?id=rYsZAQAAIAAJ&redir_esc=y</ref> If <math>\\alpha > 2</math> then the sum converges to a stable distribution with stability parameter equal to 2, i.e. a Gaussian distribution.<ref>Vladimir V. Uchaikin, Vladimir M. Zolotarev, Chance and Stability: Stable Distributions and their Applications, De Gruyter 1999 https://books.google.com/books/about/Chance_and_Stability.html?id=Y0xiwAmkb_oC&redir_esc=y</ref>\n\nThere are other possibilities as well. For example, if the characteristic function of the random variable is asymptotic to <math>1+a|t|^\\alpha\\ln|t|</math> for small ''t'' (positive or negative), then we may ask how ''t'' varies with ''n'' when the value of the characteristic function for the sum of ''n'' such random variables equals a given value ''u'':\n\n:<math>\\varphi_\\text{sum}=\\varphi^n=u</math>\n\nAssuming for the moment that ''t'' → 0, we take the limit of the above as {{math|''n'' → ∞}}:\n\n:<math> \\ln u =\\lim_{n\\to\\infty} n\\ln\\varphi =\\lim_{n\\to\\infty} na|t|^\\alpha\\ln|t|.</math>\n\nTherefore:\n\n:<math>\n\\begin{align}\n\\ln(\\ln u) & = \\ln \\left ( \\lim_{n\\to\\infty} na|t|^\\alpha\\ln|t| \\right ) \\\\[5pt]\n& = \\lim_{n\\to\\infty}\\ln \\left (  na|t|^\\alpha\\ln|t|  \\right ) =  \\lim_{n\\to\\infty} \\left \\{ \\ln(na)+\\alpha\\ln|t|+\\ln (\\ln |t|) \\right \\}\n\\end{align}\n</math>\n\nThis shows that <math>\\ln|t|</math> is asymptotic to <math>\\tfrac{-1}{\\alpha}\\ln n,</math> so using the previous equation we have\n\n:<math>|t|\\sim  \\left ( \\frac{-\\alpha \\ln u}{na\\ln n} \\right )^{1/\\alpha}.</math>\n\nThis implies that the sum divided by\n\n:<math>\\left ( \\frac{na \\ln n}{\\alpha} \\right ) ^{\\frac{1}{\\alpha}}</math>\n\nhas a characteristic function whose value at some ''t′'' goes to ''u'' (as ''n'' increases) when <math>t'=(-\\ln u)^{\\frac{1}{\\alpha}}.</math> In other words, the characteristic function converges pointwise to <math>\\exp(-(t')^\\alpha)</math> and therefore by [[Lévy's continuity theorem]] the sum divided by\n\n:<math>\\left ( \\frac{na \\ln n}{\\alpha} \\right ) ^{\\frac{1}{\\alpha}}</math>\n\n[[converges in distribution]] to the symmetric alpha-stable distribution with stability parameter <math>\\alpha</math> and scale parameter 1.\n\nThis can be applied to a random variable whose tails decrease as <math>|x|^{-3}</math>. This random variable has a mean but the variance is infinite. Let us take the following distribution:\n\n:<math>f(x)=\\begin{cases}\n\\frac 13 & |x|\\leqslant 1 \\\\\n\\frac 13x^{-3} & |x|>1\n\\end{cases}</math>\n\nWe can write this as\n\n:<math>f(x)=\\int_1^\\infty\\frac 2{w^4} h \\left (\\frac{x}{w} \\right ) dw</math>\n\nwhere\n\n:<math>h \\left (\\frac{x}{w} \\right )\n=\\begin{cases}\n\\frac 12 & \\left|\\frac x w\\right|<1, \\\\\n0 & \\left|\\frac x w\\right|>1.\n\\end{cases}</math>\n\nWe want to find the leading terms of the asymptotic expansion of the characteristic function. The characteristic function of the probability distribution <math>\\tfrac {1}{w} h\\left(\\tfrac xw\\right)</math> is <math>\\tfrac{\\sin(tw)}{tw},</math> so the characteristic function for ''f''(''x'') is\n\n:<math>\\varphi(t)=\\int_1^\\infty\\frac{2\\sin(tw)}{tw^4}dw</math>\n\nand we can calculate:\n\n:<math>\\begin{align}\n\\varphi(t)-1 &=\\int_1^\\infty\\frac 2 {w^3} \\left[\\frac{\\sin(tw)}{tw} -1 \\right] \\,dw \\\\\n&= \\int_1^{\\frac{1}{|t|}} \\frac 2 {w^3} \\left[\\frac{\\sin(tw)}{tw} -1 \\right] \\,dw + \\int_{\\frac{1}{|t|}}^{\\infty} \\frac 2 {w^3} \\left[\\frac{\\sin(tw)}{tw} -1 \\right] \\,dw \\\\\n&= \\int_1^{\\frac{1}{|t|}} \\frac 2 {w^3} \\left[\\frac{\\sin(tw)}{tw} -1 + \\left \\{ -\\frac{t^2w^2}{3!} + \\frac{t^2w^2}{3!} \\right \\} \\right] \\,dw +  \\int_{\\frac{1}{|t|}}^{\\infty} \\frac 2 {w^3} \\left[\\frac{\\sin(tw)}{tw} -1 \\right] \\,dw \\\\\n&= \\int_1^{\\frac{1}{|t|}} -\\frac{t^2dw}{3w} + \\int_1^{\\frac{1}{|t|}} \\frac 2 {w^3} \\left[\\frac{\\sin(tw)}{tw} -1 + \\frac{t^2w^2}{3!} \\right] dw +\\int_{\\frac{1}{|t|}}^{\\infty} \\frac 2 {w^3} \\left[\\frac{\\sin(tw)}{tw} -1 \\right] dw \\\\\n&= \\int_1^{\\frac{1}{|t|}} -\\frac{t^2dw}{3w} + \\left \\{ \\int_0^{\\frac{1}{|t|}} \\frac 2 {w^3} \\left[\\frac{\\sin(tw)}{tw} -1 + \\frac{t^2w^2}{3!} \\right] dw - \\int_0^1 \\frac 2 {w^3} \\left[\\frac{\\sin(tw)}{tw} -1 + \\frac{t^2w^2}{3!} \\right] dw \\right \\} +\\int_{\\frac{1}{|t|}}^{\\infty} \\frac 2 {w^3} \\left[\\frac{\\sin(tw)}{tw} -1 \\right] dw \\\\\n&= \\int_1^{\\frac{1}{|t|}} -\\frac{t^2dw}{3w} +t^2\\int_0^1\\frac 2{y^3}\\left[\\frac{\\sin(y)}{y}-1+\\frac{y^2}6\\right]dy -\\int_0^1\\frac 2{w^3}\\left[\\frac{\\sin(tw)}{tw}-1+\\frac{t^2w^2}6\\right]dw +t^2\\int_1^\\infty\\frac 2{y^3}\\left[\\frac{\\sin(y)}y-1\\right]dy \\\\\n&= -\\frac{t^2}{3} \\int_1^{\\frac{1}{|t|}} \\frac{dw}{w} + t^2 C_1 - \\int_0^1\\frac 2{w^3}\\left[\\frac{\\sin(tw)}{tw}-1+\\frac{t^2w^2}6\\right]dw + t^2 C_2 \\\\\n&= \\frac{t^2}3\\ln|t| + t^2 C_3 - \\int_0^1\\frac 2{w^3}\\left[\\frac{\\sin(tw)}{tw}-1+\\frac{t^2w^2}6\\right]dw \\\\\n&= \\frac{t^2}3\\ln|t| + t^2 C_3 - \\int_0^1\\frac 2{w^3}\\left[ \\frac{t^4w^4}{5!} + \\cdots \\right ] dw \\\\\n&= \\frac{t^2}3\\ln|t| + t^2 C_3 - \\mathcal{O} \\left ( t^4 \\right )\n\\end{align}</math>\n\nwhere <math>C_1, C_2</math> and <math>C_3</math> are constants. Therefore,\n\n:<math>\\varphi(t)\\sim 1+ \\frac{t^2}{3}\\ln|t|</math>\n\nand according to what was said above (and the fact that the variance of ''f''(''x'';2,0,1,0) is 2), the sum of ''n'' instances of this random variable, divided by <math>\\sqrt{n(\\ln n)/12},</math> will converge in distribution to a Gaussian distribution with variance 1. But the variance at any particular ''n'' will still be infinite. Note that the width of the limiting distribution grows faster than in the case where the random variable has a finite variance (in which case the width grows as the square root of ''n''). The '''average''', obtained by dividing the sum by ''n'', tends toward a Gaussian whose width approaches zero as ''n'' increases, in accordance with the [[Law of large numbers]].\n\n==Special cases==\n[[Image:Levy LdistributionPDF.png|325px|left|thumb|Log-log plot of symmetric centered stable distribution PDF's showing the power law behavior for large ''x''. The power law behavior is evidenced by the straight-line appearance of the PDF for large ''x'', with the slope equal to −(α+1). (The only exception is for α = 2, in black, which is a normal distribution.)]]\n[[Image:Levyskew LdistributionPDF.png|325px|left|thumb|Log-log plot of skewed centered stable distribution PDF's showing the power law behavior for large ''x''. Again the slope of the linear portions is equal to −(α+1)]]\n\nThere is no general analytic solution for the form of ''p''(''x''). There are, however three special cases which can be expressed in terms of [[elementary functions]] as can be seen by inspection of the [[Characteristic function (probability theory)|characteristic function]]:<ref name=\":0\" /><ref name=\":1\" /><ref>{{Cite book|title = Stable Non-Gaussian Random Processes: Stochastic Models with Infinite Variance|last = Samorodnitsky|first = G.|publisher = CRC Press|year = 1994|isbn = 9780412051715|location = |pages = |url = https://www.crcpress.com/Stable-Non-Gaussian-Random-Processes-Stochastic-Models-with-Infinite-Variance/Samoradnitsky-Taqqu/9780412051715|last2 = Taqqu|first2 = M.S.}}</ref>\n\n* For α = 2 the distribution reduces to a [[Gaussian distribution]] with variance σ<sup>2</sup> = 2''c''<sup>2</sup> and mean μ; the skewness parameter β has no effect.\n* For α = 1 and β = 0 the distribution reduces to a [[Cauchy distribution]] with scale parameter ''c'' and shift parameter μ.\n* For α = 1/2 and β = 1 the distribution reduces to a [[Lévy distribution]] with scale parameter ''c'' and shift parameter μ.\n\nNote that the above three distributions are also connected, in the following way: A standard Cauchy random variable can be viewed as a [[Compound probability distribution|mixture]] of Gaussian random variables (all with mean zero), with the variance being drawn from a standard Lévy distribution. And in fact this is a special case of a more general theorem (See p.&nbsp;59 of <ref name=\":2\">{{Cite book|url=http://eprints.nottingham.ac.uk/11194/|title=Continuous and discrete properties of stochastic processes|last= Lee|first=Wai Ha|publisher=PhD thesis, University of Nottingham|year=2010|isbn=|location=|pages=}}</ref>) which allows any symmetric alpha-stable distribution to be viewed in this way (with the alpha parameter of the mixture distribution equal to twice the alpha parameter of the mixing distribution—and the beta parameter of the mixing distribution always equal to one).\n\nA general closed form expression for stable PDF's with rational values of α is available in terms of [[Meijer G-function]]s.<ref>{{Cite journal|title = On Representation of Densities of Stable Laws by Special Functions|journal = Theory of Probability and Its Applications|date = 1995|issn = 0040-585X|pages = 354–362|volume = 39|issue = 2|doi = 10.1137/1139025|first = V.|last = Zolotarev}}</ref> Fox H-Functions can also be used to express the stable probability density functions. For simple rational numbers, the closed form expression is often in terms of less complicated [[special functions]]. Several closed form expressions having rather simple expressions in terms of special functions are available. In the table below, PDF's expressible by elementary functions are indicated by an ''E'' and those that are expressible by special functions are indicated by an ''s''.<ref name=\":2\" />\n\n{| class=\"wikitable\" style=\"text-align: center;\"\n|-\n| ||\n! colspan=\"7\" | α\n|-\n| || || {{1/3}} || {{1/2}} || {{2/3}} || 1   || {{frac|4|3}} || {{frac|3|2}} || 2 \n|-\n! rowspan=\"2\" | β \n|  0 ||  s  ||  s  ||  s  ||   '''[[Cauchy distribution|E]]''' ||  s  ||  '''[[Holtsmark distribution|s]]'''  || rowspan=\"2\" | '''[[Normal distribution|E]]'''\n|-\n|  1 ||  s  ||  '''[[Lévy distribution|E]]'''  ||  s  ||  '''[[Landau distribution|s]]'''  ||     ||  s\n|-\n|}\n\nSome of the special cases are known by particular names:\n\n* For α = 1 and β = 1, the distribution is a [[Landau distribution]] which has a specific usage in physics under this name.\n* For α = 3/2 and β = 0 the distribution reduces to a [[Holtsmark distribution]] with scale parameter ''c'' and shift parameter μ.\n\nAlso, in the limit as ''c'' approaches zero or as α approaches zero the distribution will approach a [[Dirac delta function]] {{math|''δ''(''x''&nbsp;−&nbsp;''μ'')}}.\n\n==Series representation==\nThe stable distribution can be restated as the real part of a simpler integral:<ref name=\":3\">{{Cite journal|title = Theory of the pressure broadening and shift of spectral lines|journal = Advances in Physics|date = 1981|issn = 0001-8732|pages = 367–474|volume = 30|issue = 3|doi = 10.1080/00018738100101467|first = G.|last = Peach}}</ref>\n\n:<math>f(x;\\alpha,\\beta,c,\\mu)=\\frac{1}{\\pi}\\Re\\left[ \\int_0^\\infty e^{it(x-\\mu)}e^{-(ct)^\\alpha(1-i\\beta\\Phi)}\\,dt\\right].</math>\n\nExpressing the second exponential as a [[Taylor series]], we have:\n\n:<math>f(x;\\alpha,\\beta,c,\\mu)=\\frac{1}{\\pi}\\Re\\left[ \\int_0^\\infty e^{it(x-\\mu)}\\sum_{n=0}^\\infty\\frac{(-qt^\\alpha)^n}{n!}\\,dt\\right]</math>\n\nwhere <math>q=c^\\alpha(1-i\\beta\\Phi)</math>. Reversing the order of integration and summation, and carrying out the integration yields:\n\n:<math>f(x;\\alpha,\\beta,c,\\mu)=\\frac{1}{\\pi}\\Re\\left[ \\sum_{n=1}^\\infty\\frac{(-q)^n}{n!}\\left(\\frac{i}{x-\\mu}\\right)^{\\alpha n+1}\\Gamma(\\alpha n+1)\\right]</math>\n\nwhich will be valid for ''x'' ≠ μ and will converge for appropriate values of the parameters. (Note that the ''n'' = 0 term which yields a [[Dirac delta function|delta function]] in ''x''−μ has therefore been dropped.) Expressing the first exponential as a series will yield another series in positive powers of ''x''−μ which is generally less useful.\n\n== Simulation of stable variables ==\nSimulating sequences of stable random variables is not straightforward, since there are no analytic expressions for the inverse <math>F^{-1}(x)</math> nor the CDF <math>F(x)</math> itself.<ref>{{Cite journal|title = Numerical calculation of stable densities and distribution functions|journal = Communications in Statistics. Stochastic Models|date = 1997|issn = 0882-0287|pages = 759–774|volume = 13|issue = 4|doi = 10.1080/15326349708807450|first = John P.|last = Nolan}}</ref><ref name=\":4\">{{Cite journal|last=Lihn|first=Stephen|date=2017|title=A Theory of Asset Return and Volatility Under Stable Law and Stable Lambda Distribution|url=https://ssrn.com/abstract=3046732|journal=SSRN|volume=|issue=|pages=|doi=|issn=|via=}}</ref> All standard approaches like the rejection or the inversion methods would require tedious computations. A much more elegant and efficient solution was proposed by Chambers, Mallows and Stuck (CMS),<ref>{{Cite journal|title = A Method for Simulating Stable Random Variables|journal = Journal of the American Statistical Association|date = 1976|issn = 0162-1459|pages = 340–344|volume = 71|issue = 354|doi = 10.1080/01621459.1976.10480344|first = J. M.|last = Chambers|first2 = C. L.|last2 = Mallows|first3 = B. W.|last3 = Stuck}}</ref> who noticed that a certain integral formula<ref>{{Cite book|title = One-Dimensional Stable Distributions|last = Zolotarev|first = V. M.|publisher = American Mathematical Society|year = 1986|isbn = 978-0-8218-4519-6|location = |pages = }}</ref> yielded the following algorithm:<ref>{{Cite book|title = Heavy-Tailed Distributions in VaR Calculations|publisher = Springer Berlin Heidelberg|date = 2012|isbn = 978-3-642-21550-6|pages = 1025–1059|series = Springer Handbooks of Computational Statistics|doi = 10.1007/978-3-642-21551-3_34|first = Adam|last = Misiorek|first2 = Rafał|last2 = Weron|editor-first = James E.|editor-last = Gentle|editor-first2 = Wolfgang Karl|editor-last2 = Härdle|editor-first3 = Yuichi|editor-last3 = Mori}}</ref>\n* generate a random variable <math>U</math> uniformly distributed on <math>\\left (-\\tfrac{\\pi}{2},\\tfrac{\\pi}{2} \\right )</math> and an independent exponential random variable <math>W</math> with mean 1;\n* for <math>\\alpha\\ne 1</math> compute:\n::<math>X = \\left (1+\\zeta^2 \\right )^\\frac{1}{2\\alpha} \\frac{\\sin ( \\alpha(U+\\xi)) }{ (\\cos(U))^{\\frac{1}{\\alpha}}} \\left (\\frac{\\cos (U - \\alpha(U+\\xi)) }{W} \\right )^\\frac{1-\\alpha}{\\alpha},</math>\n\n* for <math>\\alpha=1</math> compute:\n::<math>X = \\frac{1}{\\xi}\\left\\{\\left(\\frac{\\pi}{2}+\\beta U \\right)\\tan U- \\beta\\log\\left(\\frac{\\frac{\\pi}{2} W\\cos U}{\\frac{\\pi}{2}+\\beta U}\\right)\\right\\},</math>\n:where\n::<math>\\zeta = -\\beta\\tan\\frac{\\pi\\alpha}{2}, \\qquad \\xi =\\begin{cases}\t \\frac{1}{\\alpha} \\arctan(-\\zeta) & \\alpha \\ne 1 \\\\\t \\frac{\\pi}{2} & \\alpha=1\t\\end{cases}</math>\n\nThis algorithm yields a random variable <math>X\\sim S_\\alpha(\\beta,1,0)</math>. For a detailed proof see.<ref>{{Cite journal|title = On the Chambers-Mallows-Stuck method for simulating skewed stable random variables|url = http://www.sciencedirect.com/science/article/pii/0167715295001131|journal = Statistics & Probability Letters|date = 1996|pages = 165–171|volume = 28|issue = 2|doi = 10.1016/0167-7152(95)00113-1|first = Rafał|last = Weron|citeseerx = 10.1.1.46.3280}}</ref>\n\nGiven the formulas for simulation of a standard stable random variable, we can easily simulate a stable random variable for all admissible values of the parameters <math>\\alpha</math>, <math>c</math>, <math>\\beta</math> and <math>\\mu</math> using the following property. If <math>X\\sim S_\\alpha(\\beta,1,0)</math> then\n\n:<math>Y=\\begin{cases} c X+\\mu & \\alpha \\ne 1 \\\\ c X+\\frac{2}{\\pi}\\beta c\\log c + \\mu & \\alpha=1\\end{cases}</math>\n\nis <math>S_\\alpha(\\beta,c,\\mu)</math>. For <math>\\alpha=2</math> (and <math>\\beta=0</math>) the CMS method reduces to the well known [[Box–Muller transform|Box-Muller transform]] for generating [[Normal distribution|Gaussian]] random variables.<ref>{{Cite book|title = Simulation and Chaotic Behavior of Alpha-stable Stochastic Processes|last = Janicki|first = Aleksander|publisher = CRC Press|year = 1994|isbn = 9780824788827|location = |pages = |url = https://www.crcpress.com/Simulation-and-Chaotic-Behavior-of-Alpha-stable-Stochastic-Processes/Janicki-Weron/9780824788827|last2 = Weron|first2 = Aleksander}}</ref> Many other approaches have been proposed in the literature, including application of Bergström and LePage series expansions, see <ref>{{Cite journal|title = Fast, accurate algorithm for numerical simulation of L\\'evy stable stochastic processes|journal = Physical Review E|date = 1994|pages = 4677–4683|volume = 49|issue = 5|doi = 10.1103/PhysRevE.49.4677|first = Rosario Nunzio|last = Mantegna|bibcode = 1994PhRvE..49.4677M}}</ref> and,<ref>{{Cite journal|title = Computer investigation of the Rate of Convergence of Lepage Type Series to α-Stable Random Variables|journal = Statistics|date = 1992|issn = 0233-1888|pages = 365–373|volume = 23|issue = 4|doi = 10.1080/02331889208802383|first = Aleksander|last = Janicki|first2 = Piotr|last2 = Kokoszka}}</ref> respectively. However, the CMS method is regarded as the fastest and the most accurate.\n\n== Applications ==\nStable distributions owe their importance in both theory and practice to the generalization of the [[central limit theorem]] to random variables without second (and possibly first) order [[moment (mathematics)|moments]] and the accompanying [[self-similarity]] of the stable family. It was the seeming departure from normality along with the demand for a self-similar model for financial data (i.e. the shape of the distribution for yearly asset price changes should resemble that of the constituent daily or monthly price changes) that led [[Benoît Mandelbrot]] to propose that cotton prices follow an alpha-stable distribution with α equal to 1.7.<ref name=\"BM 1963\"/> [[Lévy distribution]]s are frequently found in analysis of [[critical behavior]] and financial data.<ref name=\":1\" /><ref>{{Cite book|title = Stable Paretian Models in Finance|last = Rachev|first = Svetlozar T.|publisher = Wiley|year = 2000|isbn = 978-0-471-95314-2|location = |pages = |url = http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0471953148.html|last2 = Mittnik|first2 = Stefan}}</ref>\n\nThey are also found in [[spectroscopy]] as a general expression for a quasistatically [[pressure broadening|pressure broadened spectral line]].<ref name=\":3\" />\n\nThe Lévy distribution of solar flare waiting time events (time between flare events) was demonstrated for [[CGRO]] BATSE hard x-ray solar flares in December 2001. Analysis of the Lévy statistical signature revealed that two different memory signatures were evident; one related to the solar cycle and the second whose origin appears to be associated with a localized or combination of localized solar active region effects.<ref>[http://www.library.unt.edu/theses/open/20013/leddon_deborah/dissertation.pdf Leddon, D., A statistical Study of Hard X-Ray Solar Flares]</ref>\n\n== Other analytic cases ==\n\nA number of cases of analytically expressible stable distributions are known. Let the stable distribution be expressed by <math>f(x;\\alpha,\\beta,c,\\mu)</math> then we know:\n\n* The [[Cauchy Distribution]] is given by <math>f(x;1,0,1,0).</math>\n* The [[Lévy distribution]] is given by <math>f(x;\\tfrac{1}{2},1,1,0).</math>\n* The [[Normal distribution]] is given by <math>f(x;2,0,1,0).</math>\n* Let <math>S_{\\mu,\\nu}(z)</math> be a [[Lommel function]], then:<ref name=\"Garoni2002\">{{cite journal |last=Garoni |first=T. M. |last2=Frankel |first2=N. E. |date=2002 |title=Lévy flights: Exact results and asymptotics beyond all orders |url= |journal=Journal of Mathematical Physics |volume=43 |issue=5 |pages=2670–2689  |doi= 10.1063/1.1467095|bibcode=2002JMP....43.2670G}}</ref>\n::<math> f \\left (x;\\tfrac{1}{3},0,1,0\\right ) = \\Re\\left ( \\frac{2e^{- \\frac{i \\pi}{4}}}{3 \\sqrt{3} \\pi}  \\frac{1}{\\sqrt{x^3}} S_{0,\\frac{1}{3}} \\left (\\frac{2e^{\\frac{i \\pi}{4}}}{3 \\sqrt{3}}  \\frac{1}{\\sqrt{x}} \\right) \\right )</math>\n\n* Let <math>S(x)</math> and <math>C(x)</math> denote the [[Fresnel Integrals]] then:<ref name=\"Hopcraft1999\">{{cite journal |last=Hopcraft |first=K. I. |last2=Jakeman |first2=E.|last3=Tanner|first3=R. M. J. |date=1999 |title=Lévy random walks with fluctuating step number and multiscale behavior |url= |journal=Physical Review E |volume=60 |issue=5 |pages=5327–5343 |doi= 10.1103/physreve.60.5327|bibcode=1999PhRvE..60.5327H}}</ref>\n::<math>f\\left (x;\\tfrac{1}{2},0,1,0\\right ) = \\frac{1}{{\\sqrt{2\\pi|x|^3}}}\\left (\\sin\\left(\\tfrac{1}{4|x|}\\right)\\left [\\frac{1}{2}-S\\left (\\tfrac{1}{\\sqrt{2\\pi|x|}}\\right )\\right ]+\\cos\\left(\\tfrac{1}{4|x|} \\right) \\left [\\frac{1}{2}-C\\left (\\tfrac{1}{\\sqrt{2\\pi|x|}}\\right )\\right ]\\right )</math>\n\n* Let <math>K_v(x)</math> be the [[modified Bessel function]] of the second kind then:<ref name=\"Hopcraft1999\"/>\n::<math>f\\left (x;\\tfrac{1}{3},1,1,0\\right ) = \\frac{1}{\\pi} \\frac{2\\sqrt{2}}{3^{\\frac{7}{4}}} \\frac{1}{\\sqrt{x^3}}K_{\\frac{1}{3}}\\left (\\frac{4\\sqrt{2}}{3^{\\frac{9}{4}}} \\frac{1}{\\sqrt{x}} \\right )</math>\n\n* If the <math>{}_mF_n</math> denote the [[hypergeometric function]]s then:<ref name=\"Garoni2002\"/>\n::<math>\\begin{align}\n f\\left (x;\\tfrac{4}{3},0,1,0\\right ) &= \\frac{3^{\\frac{5}{4}}}{4 \\sqrt{2 \\pi}} \\frac{\\Gamma \\left (\\tfrac{7}{12} \\right ) \\Gamma \\left (\\tfrac{11}{12} \\right )}{\\Gamma\\left (\\tfrac{6}{12} \\right ) \\Gamma \\left (\\tfrac{8}{12} \\right )} {}_2F_2 \\left ( \\tfrac{7}{12}, \\tfrac{11}{12}; \\tfrac{6}{12}, \\tfrac{8}{12}; \\tfrac{3^3 x^4}{4^4} \\right ) - \\frac{3^{\\frac{11}{4}}x^3}{4^3 \\sqrt{2 \\pi}} \\frac{\\Gamma \\left (\\tfrac{13}{12} \\right ) \\Gamma \\left (\\tfrac{17}{12} \\right )}{\\Gamma \\left (\\tfrac{18}{12} \\right ) \\Gamma \\left (\\tfrac{15}{12} \\right )} {}_2F_2 \\left ( \\tfrac{13}{12}, \\tfrac{17}{12}; \\tfrac{18}{12}, \\tfrac{15}{12}; \\tfrac{3^3 x^4}{4^4} \\right ) \\\\[6pt]\nf\\left (x;\\tfrac{3}{2},0,1,0\\right ) &= \\frac{\\Gamma \\left(\\tfrac{5}{3} \\right)}{\\pi} {}_2F_3 \\left ( \\tfrac{5}{12}, \\tfrac{11}{12}; \\tfrac{1}{3}, \\tfrac{1}{2}, \\tfrac{5}{6}; - \\tfrac{2^2 x^6}{3^6} \\right )\n- \\frac{x^2}{3 \\pi} {}_3F_4 \\left ( \\tfrac{3}{4}, 1, \\tfrac{5}{4}; \\tfrac{2}{3}, \\tfrac{5}{6}, \\tfrac{7}{6}, \\tfrac{4}{3}; - \\tfrac{2^2 x^6}{3^6} \\right ) + \\frac{7 x^4\\Gamma  \\left(\\tfrac{4}{3} \\right)}{3^4 \\pi ^ 2} {}_2F_3 \\left ( \\tfrac{13}{12}, \\tfrac{19}{12}; \\tfrac{7}{6}, \\tfrac{3}{2}, \\tfrac{5}{3}; -\\tfrac{2^2 x^6}{3^6} \\right)\n\\end{align}</math> \n:with the latter being the [[Holtsmark distribution]].\n\n* Let <math>W_{k,\\mu}(z)</math> be a [[Whittaker function]], then:<ref name=\"Zolotarev1999\">{{cite journal |last=Uchaikin |first=V. V.  |last2=Zolotarev |first2=V. M. |date=1999 |title=Chance And Stability – Stable Distributions And Their Applications |url= |journal=VSP |volume= |issue= |pages= |doi= }}</ref><ref>{{cite journal |last=Zlotarev |first=V. M. |date=1961 |title=Expression of the density of a stable distribution with exponent alpha greater than one by means of a frequency with exponent 1/alpha |url= |journal=Selected Translations in Mathematical Statistics and Probability (Translated from the Russian Article: Dokl. Akad. Nauk SSSR. 98, 735–738 (1954)) |volume=1 |issue= |pages=163–167 |doi= }}</ref><ref>{{cite journal |last=Zaliapin |first=I. V. |last2=Kagan |first2=Y. Y. |last3=Schoenberg |first3=F. P. |date=2005 |title=Approximating the Distribution of Pareto Sums |url= http://www.escholarship.org/uc/item/8940b4k8|journal=Pure and Applied Geophysics |volume=162 |issue=6 |pages=1187–1228 |doi= 10.1007/s00024-004-2666-3|bibcode=2005PApGe.162.1187Z }}</ref>\n\n::<math>\\begin{align}\nf\\left (x;\\tfrac{2}{3},0,1,0\\right ) &= \\frac{\\sqrt{3}}{6\\sqrt{\\pi}|x|} \\exp\\left (\\tfrac{2}{27}x^{-2}\\right ) W_{-\\frac{1}{2},\\frac{1}{6}}\\left (\\tfrac{4}{27}x^{-2}\\right ) \\\\[8pt]\nf\\left (x;\\tfrac{2}{3},1,1,0\\right ) &= \\frac{\\sqrt{3}}{\\sqrt{\\pi}|x|} \\exp\\left (-\\tfrac{16}{27}x^{-2}\\right ) W_{\\frac{1}{2},\\frac{1}{6}} \\left (\\tfrac{32}{27}x^{-2}\\right ) \\\\[8pt]\nf\\left (x;\\tfrac{3}{2},1,1,0\\right ) &= \\begin{cases} \\frac{\\sqrt{3}}{\\sqrt{\\pi}|x|} \\exp\\left (\\frac{1}{27}x^3\\right ) W_{\\frac{1}{2},\\frac{1}{6}}\\left (- \\frac{2}{27}x^3\\right ) & x<0\\\\ {} \\\\ \\frac{\\sqrt{3}}{6\\sqrt{\\pi}|x|} \\exp\\left (\\frac{1}{27}x^3\\right ) W_{-\\frac{1}{2},\\frac{1}{6}}\\left (\\frac{2}{27}x^3\\right ) & x \\geq 0 \\end{cases}\n\\end{align}</math>\n\n==See also==\n*[[Lévy flight]]\n*[[Lévy process]]\n*[[Fractional quantum mechanics]]\n* Other \"power law\" distributions\n** [[Pareto distribution]]\n** [[Zeta distribution]]\n** [[Zipf's law|Zipf distribution]]\n** [[Zipf–Mandelbrot law|Zipf–Mandelbrot distribution]]\n*[[Stable and tempered stable distributions with volatility clustering – financial applications]]\n*[[Multivariate stable distribution]]\n*[[Discrete-stable distribution]]\n\n==Notes==\n* The STABLE program for Windows is available from John Nolan's stable webpage: http://academic2.american.edu/~jpnolan/stable/stable.html. It calculates the density (pdf), cumulative distribution function (cdf) and quantiles for a general stable distribution, and performs maximum likelihood estimation of stable parameters and some exploratory data analysis techniques for assessing the fit of a data set.\n* [http://www.lpi.tel.uva.es/stable libstable] is a [[C (programming language)|C]] implementation for the Stable distribution pdf, cdf, random number, quantile and fitting functions (along with a benchmark replication package and an R package).\n* [[R (programming language)|R]] Package [https://cran.r-project.org/web/packages/stabledist/stabledist.pdf 'stabledist' ] by Diethelm Wuertz, Martin Maechler and Rmetrics core team members.  Computes stable density, probability, quantiles, and random numbers.  Updated Sept. 12, 2016.\n\n==References==\n{{Reflist|30em}}\n\n{{ProbDistributions|continuous-infinite}}\n\n[[Category:Continuous distributions]]\n[[Category:Probability distributions with non-finite variance]]\n[[Category:Power laws]]\n[[Category:Stable distributions| ]]\n[[Category:Stability (probability)]]"
    },
    {
      "title": "Stefan–Boltzmann law",
      "url": "https://en.wikipedia.org/wiki/Stefan%E2%80%93Boltzmann_law",
      "text": "{{See also |Black body|Black-body radiation|Planck's law|Thermal radiation}}\n[[File:Stefan Boltzmann 001.svg|thumb|Graph of a function of total emitted energy of a black body <math>j^{\\star}</math> proportional to its thermodynamic temperature <math>T\\,</math>. In blue is a total energy according to the [[Wien approximation]], <math> j^{\\star}_{W} = j^{\\star} / \\zeta(4) \\approx 0.924 \\, \\sigma T^{4} \\!\\, </math>]]\n\nThe '''Stefan–Boltzmann law''' describes the power radiated from a [[black body]] in terms of its [[thermodynamic temperature|temperature]].  Specifically, the Stefan–Boltzmann law states that the total [[energy]] radiated per unit [[area|surface area]] of a [[black body]] across [[Black-body radiation#Spectrum|all wavelengths]] per unit [[time]] <math> j^{\\star}</math> (also known as the black-body ''[[radiant emittance]]'') is directly [[Proportionality (mathematics)|proportional]] to the fourth power of the black body's [[thermodynamic temperature]] ''T'':\n\n:<math> j^{\\star} = \\sigma T^{4}.</math>\n\nThe [[constant of proportionality]] ''σ'', called the [[Stefan–Boltzmann constant]], is derived from other known [[physical constant]]s.  The value of the constant is\n\n:<math>\n\\sigma=\\frac{2\\pi^5 k^4}{15c^2h^3}= 5.670373 \\times 10^{-8}\\, \\mathrm{W\\, m^{-2}K^{-4}},\n</math>\n\nwhere ''k'' is the [[Boltzmann constant]], ''h'' is [[Planck's constant]], and ''c'' is [[speed of light|the speed of light in a vacuum]]. The [[radiance]] (watts per square metre per [[steradian]]) is given by\n\n:<math> L = \\frac{j^{\\star}}\\pi = \\frac\\sigma\\pi T^{4}.</math>\n\nA body that does not absorb all incident radiation (sometimes known as a grey body) emits less total energy than a black body and is characterized by an [[emissivity]], <math>\\varepsilon < 1</math>:\n\n:<math> j^{\\star} = \\varepsilon\\sigma T^{4}.</math>\n\nThe radiant emittance <math> j^{\\star}</math> has [[dimensional analysis#Definition|dimensions]] of [[energy flux]] (energy per time per area), and the [[SI units]] of measure are [[joule]]s per second per square metre, or equivalently, [[watt]]s per square metre. The SI unit for absolute temperature ''T'' is the [[kelvin]].  ''<math>\\varepsilon</math>'' is the [[emissivity]] of the grey body; if it is a perfect blackbody, <math>\\varepsilon=1</math>. In the still more general (and realistic) case, the emissivity depends on the wavelength, <math>\\varepsilon=\\varepsilon(\\lambda)</math>.\n\nTo find the total [[Power (physics)|power]] radiated from an object, multiply by its surface area, <math>A</math>:\n\n:<math> P= A j^{\\star} = A \\varepsilon\\sigma T^{4}.</math>\n\nWavelength- and subwavelength-scale particles,<ref name=\"Bohren\">\n{{cite book\n |last1=Bohren |first1=Craig F.\n |last2=Huffman |first2=Donald R.\n |year=1998\n |title=Absorption and scattering of light by small particles\n |url=https://books.google.com/books?id=ib3EMXXIRXUC\n |publisher=Wiley\n |isbn=978-0-471-29340-8\n |pages=123–126\n}}</ref> [[metamaterial]]s,<ref>\n{{Cite book\n |last1=Narimanov |first1=Evgenii E.\n |last2=Smolyaninov |first2=Igor I.\n |year=2012\n |chapter=Beyond Stefan–Boltzmann Law: Thermal Hyper-Conductivity\n |title=Conference on Lasers and Electro-Optics 2012\n |series=OSA Technical Digest\n |publisher=Optical Society of America\n |volume=<!-- --> |issue=<!-- --> |pages=QM2E.1\n |doi=10.1364/QELS.2012.QM2E.1\n |isbn=978-1-55752-943-5\n|citeseerx=10.1.1.764.846\n }}</ref> and other nanostructures are not subject to ray-optical limits and may be designed to exceed the Stefan–Boltzmann law.\n\n== History ==\nIn 1864, [[John Tyndall]] presented measurements of the infrared emission by a platinum filament and the corresponding color of the filament.<ref>\n*  {{cite journal|last1=Tyndall|first1=John|title=On luminous [i.e., visible] and obscure [i.e., infrared] radiation|journal=Philosophical Magazine|date=1864|volume=28|pages=329–341|url=https://babel.hathitrust.org/cgi/pt?id=umn.31951000614117o;view=1up;seq=357|series=4th series}} ; see p. 333.\nIn his physics textbook of 1875, [[Adolf Wüllner|Adolph Wüllner]] quoted Tyndall's results and then added estimates of the temperature that corresponded to the platinum filament's color:\n*  {{cite book|last1=Wüllner|first1=Adolph|title=Lehrbuch der Experimentalphysik|trans-title=Textbook of experimental physics|volume=vol. 3|date=1875|publisher=B.G. Teubner|location=Leipzig, Germany|page=215|url=https://babel.hathitrust.org/cgi/pt?id=uc1.b4062759;view=1up;seq=231|language=German}}\nFrom (Wüllner, 1875), p. 215:  ''\"Wie aus gleich zu besprechenden Versuchen von Draper hervorgeht, … also fast um das 12fache zu.\"'' (As follows from the experiments of Draper, which will be discussed shortly, a temperature of about 525°[C] corresponds to the weak red glow; a [temperature] of about 1200°[C], to the full white glow.  Thus, while the temperature climbed only somewhat more than double, the intensity of the radiation increased from 10.4 to 122 ; thus, almost 12-fold.)<br>\nSee also:\n*  {{cite journal|last1=Wisniak|first1=Jaime|title=Heat radiation law – from Newton to Stefan|journal=Indian Journal of Chemical Technology|date=November 2002|volume=9|pages=545–555}} ; see pp. 551–552.  Available at:  [http://nopr.niscair.res.in/bitstream/123456789/18926/1/IJCT%209%286%29%20545-555.pdf National Institute of Science Communication and Information Resources (New Dehli, India)]</ref> \nThe proportionality to the fourth power of the absolute temperature was deduced by [[Josef Stefan]] (1835–1893) in 1879 on the basis of Tyndall's experimental measurements, in the article ''Über die Beziehung zwischen der Wärmestrahlung und der Temperatur'' (''On the relationship between thermal radiation and temperature'') in the ''Bulletins from the sessions'' of the Vienna Academy of Sciences.<ref>{{cite journal|last1=Stefan|first1=J.|title=Über die Beziehung zwischen der Wärmestrahlung und der Temperatur|journal=Sitzungsberichte der Kaiserlichen Akademie der Wissenschaften:  Mathematisch-Naturwissenschaftliche Classe (Proceedings of the Imperial Philosophical Academy [of Vienna]:  Mathematical and Scientific Class)|date=1879|volume=79|pages=391–428|url=https://babel.hathitrust.org/cgi/pt?id=hvd.32044093294874;view=1up;seq=419|trans-title=On the relation between heat radiation and temperature|language=German}}</ref><ref>Stefan stated (Stefan, 1879), p. 421:  ''\"Zuerst will ich hier die Bemerkung anführen, … die Wärmestrahlung der vierten Potenz der absoluten Temperatur proportional anzunehmen.\"'' (First of all, I want to point out here the observation which Wüllner, in his textbook, added to the report of Tyndall's experiments on the radiation of a platinum wire that was brought to glowing by an electric current, because this observation first caused me to suppose that thermal radiation is proportional to the fourth power of the absolute temperature.)</ref>\nA derivation of the law  from theoretical considerations was presented by [[Ludwig Boltzmann]] (1844–1906) in 1884, drawing upon the work of [[Adolfo Bartoli]].<ref>{{cite journal|last1=Boltzmann|first1=Ludwig|title=Ableitung des Stefan'schen Gesetzes, betreffend die Abhängigkeit der Wärmestrahlung von der Temperatur aus der electromagnetischen Lichttheorie|journal=Annalen der Physik und Chemie|date=1884|volume=258|issue=6|pages=291–294|doi=10.1002/andp.18842580616|url=https://babel.hathitrust.org/cgi/pt?id=uc1.a0002763670;view=1up;seq=305|trans-title=Derivation of Stefan's law, concerning the dependency of heat radiation on temperature, from the electromagnetic theory of light|language=German|bibcode=1884AnP...258..291B}}</ref> \nBartoli in 1876 had derived the existence of [[radiation pressure]] from the principles of [[thermodynamics]]. Following Bartoli, Boltzmann considered an ideal [[heat engine]] using electromagnetic radiation instead of an ideal gas as  working matter.\n\nThe law was almost immediately experimentally verified. Heinrich Weber in 1888 pointed out deviations at higher temperatures, but perfect accuracy within measurement uncertainties was confirmed up to temperatures of 1535 K by 1897.<ref>Massimiliano Badino, ''The Bumpy Road: Max Planck from Radiation Theory to the Quantum (1896–1906)'' (2015), [https://books.google.ch/books?id=JcvWCQAAQBAJ&pg=PA31 p. 31].</ref>\nThe law, including the theoretical prediction of the [[Stefan–Boltzmann constant]] as a function of the [[speed of light]], the [[Boltzmann constant]] and [[Planck's constant]], is a [[#Derivation_from_Planck's_law|direct consequence]] of [[Planck's law]] as formulated in 1900.\n\n== Examples ==\n=== Temperature of the Sun ===\nWith his law Stefan also determined the temperature of the [[Sun]]'s surface.<ref>(Stefan, 1879), pp. 426–427.</ref> He inferred from the data of [[Jacques-Louis Soret]] (1827&ndash;1890)<ref>Soret, J.L. (1872) \"Comparaison des intensités calorifiques du rayonnement solaire et du rayonnement d'un corps chauffé à la lampe oxyhydrique\" [Comparison of the heat intensities of solar radiation and of radiation from a body heated with an oxy-hydrogen torch], ''Archives des sciences physiques et naturelles'' (Geneva, Switzerland), 2nd series, [https://babel.hathitrust.org/cgi/pt?id=wu.89048214449;view=1up;seq=684 '''44''':  220–229] ; [https://babel.hathitrust.org/cgi/pt?id=wu.89048214449;view=1up;seq=1094 '''45''': 252–256.]</ref> that the energy flux density from the Sun is 29 times greater than the energy flux density of a certain warmed metal [[Lamella (materials)|lamella]] (a thin plate). A round lamella was placed at such a distance from the measuring device that it would be seen at the same angle as the Sun. Soret estimated the temperature of the lamella to be approximately 1900 [[Celsius|°C]] to 2000&nbsp;°C. Stefan surmised that ⅓ of the energy flux from the Sun is absorbed by the [[Earth's atmosphere]], so he took for the correct Sun's energy flux a value 3/2 times greater than Soret's value, namely 29 &times; 3/2 = 43.5.\n\nPrecise measurements of atmospheric [[Absorption (electromagnetic radiation)|absorption]] were not made until 1888 and 1904. The temperature Stefan obtained was a median value of previous ones, 1950&nbsp;°C and the absolute thermodynamic one 2200&nbsp;K. As 2.57<sup>4</sup> = 43.5, it follows from the law that the temperature of the Sun is 2.57 times greater than the temperature of the lamella, so Stefan got a value of 5430&nbsp;°C or 5700&nbsp;K (the modern value is 5778&nbsp;K<ref>{{Cite web | url=https://nssdc.gsfc.nasa.gov/planetary/factsheet/sunfact.html | title=Sun Fact Sheet}}</ref>). This was the first sensible value for the temperature of the Sun. Before this, values ranging from as low as 1800&nbsp;°C to as high as 13,000,000&nbsp;°C<ref>{{cite journal|last1=Waterston|first1=John James|title=An account of observations on solar radiation|journal=Philosophical Magazine|date=1862|volume=23|pages=497–511|url=https://books.google.com/books?id=v1YEAAAAYAAJ&pg=PA497#v=onepage&q&f=false|series=4th series}} On p. 505, the Scottish physicist [[John James Waterston]] estimated that the temperature of the sun's surface could be 12,880,000°.</ref> were claimed. The lower value of 1800&nbsp;°C was determined by [[Claude Pouillet]] (1790–1868) in 1838 using the [[Dulong–Petit law]].<ref>See:\n*  {{cite journal|last1=Pouillet|title=Mémoire sur la chaleur solaire, sur les pouvoirs rayonnants et absorbants de l'air atmosphérique, et sur la température de l'espace|journal=Comptes Rendus|date=1838|volume=7|issue=2|pages=24–65|url=https://www.biodiversitylibrary.org/item/81350#page/34/mode/1up|trans-title=Memoir on solar heat, on the radiating and absorbing powers of the atmospheric air, and on the temperature of space|language=French}}  On p. 36, Pouillet estimates the sun's temperature:  ''\" … cette température pourrait être de 1761° … \"'' ( … this temperature [i.e., of the Sun] could be 1761° … )\n*  English translation:  Pouillet (1838) [https://books.google.com/books?id=Qcc-AAAAYAAJ&pg=PA44#v=onepage&q&f=false \"Memoir on the solar heat, on the radiating and absorbing powers of atmospheric air, and on the temperature of space\"] in:  Taylor, Richard, ed. (1846) ''Scientific Memoirs, Selected from the Transactions of Foreign Academies of Science and Learned Societies, and from Foreign Journals.'' vol. 4. London, England:  Richard and John E. Taylor. pp. 44–90 ; see pp. 55–56.</ref> Pouillet also took just half the value of the Sun's correct energy flux.\n\n=== Temperature of stars ===\n\nThe temperature of [[star]]s other than the Sun can be approximated using a similar means by treating the emitted energy as a [[black body]] radiation.<ref name=\"luminosity\">{{cite web |url=http://outreach.atnf.csiro.au/education/senior/astrophysics/photometry_luminosity.html |title=Luminosity of Stars |publisher=Australian Telescope Outreach and Education |accessdate=2006-08-13 }}</ref> So:\n\n: <math>L = 4 \\pi R^2 \\sigma {T_e}^4 </math>\n\nwhere ''L'' is the [[luminosity]], ''σ'' is the [[Stefan–Boltzmann constant]], ''R'' is the stellar radius and ''T'' is the [[effective temperature]]. This same formula can be used to compute the approximate radius of a main sequence star relative to the sun:\n\n: <math>\\frac{R}{R_\\odot} \\approx \\left ( \\frac{T_\\odot}{T} \\right )^{2} \\cdot \\sqrt{\\frac{L}{L_\\odot}}</math>\n\nwhere <math>R_\\odot</math> is the [[solar radius]], <math>L_\\odot</math> is the [[solar luminosity]], and so forth.\n\nWith the Stefan–Boltzmann law, [[astronomer]]s can easily infer the radii of stars. The law is also met in the [[Black hole thermodynamics|thermodynamics]] of [[black hole]]s in so-called [[Hawking radiation]].\n\n=== Effective temperature of the Earth ===\n\nSimilarly we can calculate the [[effective temperature]] of the Earth ''T''<sub>⊕</sub> by equating the energy received from the Sun and the energy radiated by the Earth, under the black-body approximation (Earth's own production of energy being small enough to be negligible).  The luminosity of the Sun, ''L''<sub>⊙</sub>, is given by:\n:<math>\nL_\\odot = 4\\pi R_\\odot^2 \\sigma T_\\odot^4\n</math>\n\nAt Earth, this energy is passing through a sphere with a radius of ''a''<sub>0</sub>, the distance between the Earth and the Sun, and the [[irradiance]] (received power per unit area) is given by\n\n:<math>\nE_\\oplus = \\frac{L_\\odot}{4\\pi a_0^2}\n</math>\n\nThe Earth has a radius of ''R''<sub>⊕</sub>, and therefore has a cross-section of <math>\\pi R_\\oplus^2</math>.  The [[radiant flux]] (i.e. solar power) absorbed by the Earth is thus given by:\n\n:<math>\n\\Phi_\\text{abs} = \\pi R_\\oplus^2 \\times E_\\oplus\n:</math>\n\nBecause the Stefan–Boltzmann law uses a fourth power, it has a stabilizing effect on the exchange and the flux emitted by Earth tends to be equal to the flux absorbed, close to the steady state where:\n\n:<math>\n\\begin{align}\n4\\pi R_\\oplus^2 \\sigma T_\\oplus^4 &= \\pi R_\\oplus^2 \\times E_\\oplus \\\\\n &= \\pi R_\\oplus^2 \\times \\frac{4\\pi R_\\odot^2\\sigma T_\\odot^4}{4\\pi a_0^2} \\\\\n\\end{align}\n</math>\n\n''T''<sub>⊕</sub> can then be found:\n\n:<math>\n\\begin{align}\nT_\\oplus^4 &= \\frac{R_\\odot^2 T_\\odot^4}{4 a_0^2} \\\\\nT_\\oplus &= T_\\odot \\times \\sqrt\\frac{R_\\odot}{2 a_0} \\\\\n& = 5780 \\; {\\rm K} \\times \\sqrt{696 \\times 10^{6} \\; {\\rm m} \\over 2 \\times 149.598 \\times 10^{9} \\; {\\rm m} } \\\\\n& \\approx 279 \\; {\\rm K}\n\\end{align}\n</math>\n\nwhere ''T''<sub>⊙</sub> is the temperature of the Sun, ''R''<sub>⊙</sub> the radius of the Sun, and ''a''<sub>0</sub> is the distance between the Earth and the Sun. This gives an effective temperature of 6&nbsp;°C on the surface of the Earth, assuming that it perfectly absorbs all emission falling on it and has no atmosphere.\n\nThe Earth has an [[albedo]] of 0.3, meaning that 30% of the solar radiation that hits the planet gets scattered back into space without absorption.  The effect of albedo on temperature can be approximated by assuming that the energy absorbed is multiplied by 0.7, but that the planet still radiates as a black body (the latter by definition of [[effective temperature]], which is what we are calculating).  This approximation reduces the temperature by a factor of 0.7<sup>1/4</sup>, giving 255&nbsp;K (&minus;18&nbsp;°C).<ref name= \"IPCC4_ch01\">[http://www.ipcc.ch/pdf/assessment-report/ar4/wg1/ar4-wg1-chapter1.pdf Intergovernmental Panel on Climate Change Fourth Assessment Report. Chapter 1: Historical overview of climate change science] page 97</ref><ref>[http://eesc.columbia.edu/courses/ees/climate/lectures/radiation/ Solar Radiation and the Earth's Energy Balance<!-- Bot generated title -->]</ref>\n\nThe above temperature is Earth's as seen from space, not ground temperature but an average over all emitting bodies of Earth from surface to high altitude. Because of the [[greenhouse effect]], the Earth's actual average surface temperature is about 288&nbsp;K (15&nbsp;°C), which is higher than the 255&nbsp;K effective temperature, and even higher than the 279&nbsp;K temperature that a black body would have.\n\nIn the above discussion, we have assumed that the whole surface of the earth is at one temperature. Another interesting question is to ask what the temperature of a blackbody surface on the earth would be assuming that it reaches equilibrium with the sunlight falling on it. This of course depends on the angle of the sun on the surface and on how much air the sunlight has gone through. When the sun is at the zenith and the surface is horizontal, the irradiance can be as high as 1120&nbsp;W/m<sup>2</sup>.<ref name=\"Solar constant at ground level\">{{cite web|title=Introduction to Solar Radiation|url= http://www.newport.com/Introduction-to-Solar-Radiation/411919/1033/content.aspx|publisher= Newport Corporation|deadurl= no|archiveurl= https://web.archive.org/web/20131029234117/http://www.newport.com/Introduction-to-Solar-Radiation/411919/1033/content.aspx|archivedate=Oct 29, 2013}}</ref> The Stefan–Boltzmann law then gives a temperature of\n\n:<math>T=\\left(\\frac{1120\\text{ W/m}^2}\\sigma\\right)^{1/4}\\approx 375\\text{ K}</math>\n\nor 102&nbsp;°C. (Above the atmosphere, the result is even higher: 394 K.) We can think of the earth's surface as \"trying\" to reach equilibrium temperature during the day, but being cooled by the atmosphere, and \"trying\" to reach equilibrium with starlight and possibly moonlight at night, but being warmed by the atmosphere.\n\n== Origination ==\n\n=== Thermodynamic derivation of the energy density ===\n\nThe fact that the [[energy density]] of the box containing radiation is proportional to <math>T^{4}</math> can be derived using thermodynamics.<ref>{{Cite web|url=http://www.pha.jhu.edu/~kknizhni/StatMech/Derivation_of_Stefan_Boltzmann_Law.pdf|title=Derivation of the Stefan–Boltzmann Law|last=Knizhnik|first=Kalman|date=|website=Johns Hopkins University – Department of Physics & Astronomy|archive-url=https://web.archive.org/web/20160304133636/http://www.pha.jhu.edu/~kknizhni/StatMech/Derivation_of_Stefan_Boltzmann_Law.pdf|archive-date=2016-03-04|dead-url=yes|access-date=2018-09-03}}</ref><ref>(Wisniak, 2002), p. 554.</ref> This derivation uses the relation between the [[radiation pressure]] ''p'' and the [[internal energy]] density <math>u</math>, a relation that [[Radiation pressure#Compression in a uniform radiation field|can be shown]] using the form of the [[electromagnetic stress–energy tensor]]. This relation is:\n\n: <math> p = \\frac{u}{3}.</math>\n\nNow, from the [[fundamental thermodynamic relation]]\n\n: <math> dU = T \\, dS - p \\, dV, </math>\n\nwe obtain the following expression, after dividing by <math> dV </math> and fixing <math> T </math> :\n\n: <math> \\left(\\frac{\\partial U}{\\partial V}\\right)_T = T \\left(\\frac{\\partial S}{\\partial V}\\right)_T - p = T \\left(\\frac{\\partial p}{\\partial T}\\right)_V - p. </math>\n\nThe last equality comes from the following [[Maxwell relations|Maxwell relation]]:\n\n: <math> \\left(\\frac{\\partial S}{\\partial V}\\right)_T = \\left(\\frac{\\partial p}{\\partial T}\\right)_V. </math>\n\nFrom the definition of energy density it follows that\n\n: <math> U = u V </math>\n\nwhere the energy density of radiation only depends on the temperature, therefore\n\n: <math> \\left(\\frac{\\partial U}{\\partial V}\\right)_T = u \\left(\\frac{\\partial V}{\\partial V}\\right)_T = u. </math>\n\nNow, the equality\n\n: <math> \\left(\\frac{\\partial U}{\\partial V}\\right)_T = T \\left(\\frac{\\partial p}{\\partial T}\\right)_V - p, </math>\n\nafter substitution of <math> \\left(\\frac{\\partial U}{\\partial V}\\right)_{T}</math> and <math> p </math> for the corresponding expressions, can be written as\n\n: <math> u = \\frac{T}{3} \\left(\\frac{\\partial u}{\\partial T}\\right)_V - \\frac{u}{3}. </math>\n\nSince the partial derivative <math> \\left(\\frac{\\partial u}{\\partial T}\\right)_V </math> can be expressed as a relationship between only <math> u </math> and <math> T </math> (if one isolates it on one side of the equality), the partial derivative can be replaced by the ordinary derivative. After separating the differentials the equality becomes\n\n: <math> \\frac{du}{4u} = \\frac{dT}{T}, </math>\n\nwhich leads immediately to <math> u = A T^4 </math>, with <math> A </math> as some constant of integration.\n\n=== Derivation from Planck's law ===\n[[File:Stephan-boltz law.jpg|thumb|Deriving the Stefan–Boltzmann Law using the [[Planck's law]].]]\nThe law can be derived by considering a small flat [[black body]] surface radiating out into a half-sphere. This derivation uses [[spherical coordinates]], with ''θ'' as the zenith angle and ''φ'' as the azimuthal angle; and the small flat blackbody surface lies on the xy-plane, where ''θ'' = <sup>{{pi}}</sup>/<sub>2</sub>.\n\nThe intensity of the light emitted from the blackbody surface is given by [[Planck's law]] :\n\n::<math>I(\\nu,T) =\\frac{2 h\\nu^3}{c^2}\\frac{1}{ e^{h\\nu/(kT)}-1}.</math>\n:where\n:*<math>I(\\nu,T)\\,</math> is the amount of [[Power (physics)|power]] per unit [[surface area]] per unit [[solid angle]] per unit [[frequency]] emitted at a frequency <math>\\nu \\,</math> by a black body at temperature ''T''.\n:*<math>h \\,</math> is [[Planck's constant]]\n:*<math>c \\,</math> is the [[speed of light]], and\n:*<math>k \\,</math> is [[Boltzmann's constant]].\n\nThe quantity <math>I(\\nu,T) ~A ~d\\nu ~d\\Omega</math> is the [[Power (physics)|power]] radiated by a surface of area A through a [[solid angle]] ''dΩ'' in the frequency range between ''ν'' and ''ν''&nbsp;+&nbsp;''dν''.\n\nThe Stefan–Boltzmann law gives the power emitted per unit area of the emitting body,\n::<math>\\frac{P}{A} = \\int_0^\\infty I(\\nu,T) \\, d\\nu \\int \\cos \\theta \\, d\\Omega \\,</math>\n\n<!-- Linked from [[Radiance#Description]]: -->\n{{anchor|Integration of intensity derivation}}\nNote that the cosine appears because black bodies are ''Lambertian'' (i.e. they obey [[Lambert's cosine law]]), meaning that the intensity observed along the sphere will be the actual intensity times the cosine of the zenith angle.\nTo derive the Stefan–Boltzmann law, we must integrate ''dΩ'' = sin(''θ'') ''dθ dφ'' over the half-sphere and integrate ''ν'' from 0 to ∞.\n\n:: <math>\n\\begin{align}\n\\frac{P}{A} & = \\int_0^\\infty I(\\nu,T) \\, d\\nu \\int_0^{2\\pi} \\, d\\varphi \\int_0^{\\pi/2} \\cos \\theta \\sin \\theta \\, d\\theta  \\\\\n& = \\pi \\int_0^\\infty I(\\nu,T) \\, d\\nu\n\\end{align}\n</math>\n\nThen we plug in for ''I'':\n\n:: <math>\\frac{P}{A} = \\frac{2 \\pi h}{c^2} \\int_0^\\infty \\frac{\\nu^3}{ e^{\\frac{h\\nu}{kT}}-1} \\, d\\nu </math>\n\nTo evaluate this integral, do a substitution,\n\n::<math>\n\\begin{align}\nu & = \\frac{h \\nu}{k T} \\\\[6pt]\ndu & = \\frac{h}{k T} \\, d\\nu\n\\end{align}\n</math>\n\nwhich gives:\n\n: <math>\\frac{P}{A} = \\frac{2 \\pi h }{c^2} \\left(\\frac{k T}{h} \\right)^4 \\int_0^\\infty \\frac{u^3}{ e^u - 1} \\, du.</math>\n\nThe integral on the right is standard and goes by many names: it is a particular case of a [[Bose–Einstein integral]], or the [[Riemann zeta function]], <math> \\zeta(4) </math>, or the [[polylogarithm]]. The value of the integral is <math> \\frac{\\pi^4}{15} </math>, giving the result that, for a perfect blackbody surface:\n\n: <math>j^\\star =  \\sigma T^4 ~, ~~ \\sigma = \\frac{2 \\pi^5 k^4 }{15 c^2 h^3} = \\frac{\\pi^2 k^4}{60 \\hbar^3 c^2}. </math>\n\nFinally, this proof started out only considering a small flat surface. However, any [[Differentiable manifold|differentiable]] surface can be approximated by a collection of small flat surfaces. So long as the geometry of the surface does not cause the blackbody to reabsorb its own radiation, the total energy radiated is just the sum of the energies radiated by each surface; and the total surface area is just the sum of the areas of each surface—so this law holds for all [[convex set|convex]] blackbodies, too, so long as the surface has the same temperature throughout.  The law extends to radiation from non-convex bodies by using the fact that the [[convex hull]] of a black body radiates as though it were itself a black body.\n\n===Energy density===\nThe total energy density ''U'' can be similarly calculated, except the integration is over the whole sphere and there is no cosine, and the energy flux should be divided by the velocity ''c'':\n::<math>U = \\frac{1}{c} \\int_0^\\infty I(\\nu,T) \\, d\\nu \\int cos \\theta \\, d\\Omega \\,</math>\nThus  <math>\\int_0^{\\pi/2} \\cos \\theta \\sin \\theta \\, d\\theta </math> is replaced by  <math>\\int_0^{\\pi} \\sin \\theta \\, d\\theta </math>, giving an extra factor of 4.\n\nThus, in total:\n::<math>U = \\frac{4}{c} \\, \\sigma \\, T^4 </math>\n\n== See also ==\n*[[Wien's displacement law]]\n*[[Rayleigh&ndash;Jeans law]]\n*[[Radiance]]\n*[[Climate model#Zero-dimensional models|Zero-dimensional models]]\n*[[Black body]]\n*[[Sakuma–Hattori equation]]\n*[[Radó von Kövesligethy]]\n\n== Notes ==\n{{reflist}}\n\n== References ==\n* {{Citation |last=Stefan |first=J. |title=Über die Beziehung zwischen der Wärmestrahlung und der Temperatur |journal=Sitzungsberichte der Mathematisch-naturwissenschaftlichen Classe der Kaiserlichen Akademie der Wissenschaften |language=de |trans-title=On the relationship between heat radiation and temperature |url=http://www.ing-buero-ebel.de/strahlung/Original/Stefan1879.pdf |volume=79 |year=1879 |pages=391–428}}\n* {{Citation |last=Boltzmann |first=L. |title=Ableitung des Stefan'schen Gesetzes, betreffend die Abhängigkeit der Wärmestrahlung von der Temperatur aus der electromagnetischen Lichttheorie |language=de |trans-title=Derivation of Stefan's little law concerning the dependence of thermal radiation on the temperature of the electro-magnetic theory of light |journal=Annalen der Physik und Chemie |volume=258 |issue=6 |year=1884 |pages=291–294 |doi=10.1002/andp.18842580616 |bibcode = 1884AnP...258..291B }}\n\n{{blackbody radiation laws}}\n\n{{DEFAULTSORT:Stefan-Boltzmann law}}\n[[Category:Laws of thermodynamics]]\n[[Category:Power laws]]\n[[Category:Heat transfer]]"
    },
    {
      "title": "Stevens's power law",
      "url": "https://en.wikipedia.org/wiki/Stevens%27s_power_law",
      "text": "{{inline|date=February 2019}}\n{| class=\"wikitable sortable\" style=\"float: right;margin-left:1em\"\n! Continuum || Exponent (<math>a</math>) || Stimulus condition\n|-\n|Loudness || 0.67 || Sound pressure of 3000&nbsp;Hz tone\n|-\n|Vibration || 0.95 || Amplitude of 60&nbsp;Hz on finger\n|-\n|Vibration || 0.6 || Amplitude of 250&nbsp;Hz on finger\n|-\n|Brightness || 0.33 || 5° target in dark\n|-\n|Brightness || 0.5 || [[Point source]]\n|-\n|Brightness || 0.5 || Brief flash\n|-\n|Brightness || 1 || Point source briefly flashed\n|-\n|Lightness || 1.2 || Reflectance of gray papers\n|-\n|Visual length || 1 || Projected line\n|-\n|Visual area || 0.7 || Projected square\n|-\n|Redness (saturation) || 1.7 || Red–gray mixture\n|-\n|Taste || 1.3 || [[Sucrose]]\n|-\n|Taste || 1.4 || [[Sodium chloride|Salt]]\n|-\n|Taste || 0.8 || [[Saccharin]]\n|-\n|Smell || 0.6 || [[Heptane]]\n|-\n|Cold || 1 || Metal contact on arm\n|-\n|Warmth || 1.6 || Metal contact on arm\n|-\n|Warmth || 1.3 || Irradiation of skin, small area\n|-\n|Warmth || 0.7 || Irradiation of skin, large area\n|-\n|Discomfort, cold || 1.7 || Whole-body irradiation\n|-\n|Discomfort, warm || 0.7 || Whole-body irradiation\n|-\n|Thermal pain || 1 || Radiant heat on skin\n|-\n|Tactual roughness || 1.5 || Rubbing emery cloths\n|-\n|Tactual hardness || 0.8 || Squeezing rubber\n|-\n|Finger span || 1.3 || Thickness of blocks\n|-\n|Pressure on palm || 1.1 || Static force on skin\n|-\n|Muscle force || 1.7 || Static contractions\n|-\n|Heaviness || 1.45 || Lifted weights\n|-\n|Viscosity || 0.42 || Stirring silicone fluids\n|-\n|Electric shock || 3.5 || Current through fingers\n|-\n|[[Vocal effort]] || 1.1 || Vocal sound pressure\n|-\n|Angular acceleration || 1.4 || 5 s rotation\n|-\n|Duration || 1.1 || White-noise stimuli\n|}\n\n'''Stevens's power law''' is an empirical relationship in [[psychophysics]] between an increased intensity or strength in a physical stimulus and the perceived [[magnitude (mathematics)|magnitude]] increase in the sensation created by the stimulus. It is often considered to supersede the [[Weber–Fechner law]], based on a logarithmic relationship between stimulus and sensation, because the power law describes a wider range of sensory comparisons.\n\nThe theory is named after psychophysicist [[Stanley Smith Stevens]] (1906–1973).  Although the idea of a [[power law]] had been suggested by 19th-century researchers, Stevens is credited with reviving the law and publishing a body of psychophysical data to support it in 1957.\n\nThe general form of the law is\n:<math>\\psi(I) = k I ^a,</math>\nwhere ''I'' is the intensity or strength of the stimulus in physical units (energy, weight, pressure, mixture proportions, etc.), ψ(''I'') is the magnitude of the sensation evoked by the stimulus, ''a'' is an exponent that depends on the type of stimulation or sensory modality, and ''k'' is a [[proportionality (mathematics)|proportionality]] constant that depends on the units used.\n\nA distinction has been made between local [[psychophysics]], where stimuli can only be discriminated with a probability around 50%, and global psychophysics, where the stimuli can be discriminated correctly with near certainty ([[R. Duncan Luce|Luce]] & Krumhansl, 1988). The Weber–Fechner law and methods described by [[L. L. Thurstone]] are generally applied in local psychophysics, whereas Stevens's methods are usually applied in global psychophysics.\n\nThe table to the right lists the exponents reported by Stevens.\n\n==Methods==\nThe principal methods used by Stevens to measure the perceived intensity of a stimulus were ''magnitude estimation'' and ''magnitude production''. In magnitude estimation with a standard, the experimenter presents a stimulus called a ''standard'' and assigns it a number called the ''modulus''. For subsequent stimuli, subjects report numerically their perceived intensity relative to the standard so as to preserve the ratio between the sensations and the numerical estimates (e.g., a sound perceived twice as loud as the standard should be given a number twice the modulus). In magnitude estimation without a standard (usually just ''magnitude estimation''), subjects are free to choose their own standard, assigning any number to the first stimulus and all subsequent ones with the only requirement being that the ratio between sensations and numbers is preserved. In magnitude production a number and a reference stimulus is given and subjects produce a stimulus that is perceived as that number times the reference. Also used is ''cross-modality matching'', which generally involves subjects altering the magnitude of one physical quantity, such as the brightness of a light, so that its perceived intensity is equal to the perceived intensity of another type of quantity, such as warmth or pressure.\n\n==Criticisms==\nStevens generally collected magnitude estimation data from multiple observers, averaged the data across subjects, and then fitted a power function to the data.  Because the fit was generally reasonable, he concluded the power law was correct.\n\nA principal criticism has been that Stevens's approach provides neither a direct test of the power law itself nor the underlying assumptions of the magnitude estimation/production method: it simply fits curves to data points. In addition, the power law can be deduced mathematically from the Weber-Fechner logarithmic function ([[Mackay, 1963]]<ref>MacKay, D. M. Psychophysics of perceived intensity: A theoretical basis for Fechner's and Stevens' laws. Science, 1963, 139, 1213-1216.</ref>), and the relation makes predictions consistent with data ([http://hdl.handle.net/10161/6003 Staddon], 1978<ref>Staddon, J. E. R.)].  Theory of behavioral power functions. Psychological Review, 85, 305-320. </ref>). As with all psychometric studies, Stevens's approach ignores individual differences in the stimulus-sensation relationship, and there are generally large individual differences in this relationship that averaging the data will obscure {{Harv|Green|Luce|1974}}.\n\nStevens's main assertion was that using magnitude estimations/productions respondents were able to make judgements on a [[Scale (ratio)|ratio scale]] (i.e., if ''x'' and ''y'' are values on a given ratio scale, then there exists a constant ''k'' such that ''x'' = ''ky''). In the context of [[axiomatic system|axiomatic]] psychophysics, {{harv|Narens|1996}} formulated a testable property capturing the implicit underlying assumption this assertion entailed. Specifically, for two proportions ''p'' and ''q'', and three stimuli, ''x'', ''y'', ''z'', if ''y'' is judged ''p'' times ''x'', ''z'' is judged ''q'' times ''y'', then ''t'' = ''pq'' times ''x'' should be equal to ''z''. This amounts to assuming that respondents interpret numbers in a veridical way. This property was unambiguously rejected ({{harvnb|Ellermeier|Faulhammer|2000}}, {{harvnb|Zimmer|2005}}). Without assuming veridical interpretation of numbers, {{Harv|Narens|1996}} formulated another property that, if sustained, meant that respondents could make ratio scaled judgments, namely, if ''y'' is judged ''p'' times ''x'', ''z'' is judged ''q'' times ''y'', and if ''y''{{'}} is judged ''q'' times ''x'', ''z''{{'}} is judged ''p'' times ''y''{{'}}, then ''z'' should equal ''z''{{'}}. This property has been sustained in a variety of situations ({{Harvnb|Ellermeier|Faulhammer|2000}}, {{harvnb|Zimmer|2005}}).\n\nCritics of the power law also point out that the validity of the law is contingent on the measurement of perceived stimulus intensity that is employed in the relevant experiments. {{harv|Luce|2002}}, under the condition that respondents' numerical distortion function and the psychophysical functions could be separated, formulated a behavioral condition equivalent to the psychophysical function being a power function. This condition was confirmed for just over half the respondents, and the power form was found to be a reasonable approximation for the rest {{Harv|Steingrimsson|Luce|2006}}.\n\nIt has also been questioned, particularly in terms of [[signal detection theory]], whether any given stimulus is actually associated with a particular and ''absolute'' perceived intensity; i.e. one that is independent of contextual factors and conditions. Consistent with this, Luce (1990, p.&nbsp;73) observed that \"by introducing contexts such as background noise in loudness judgements, the shape of the magnitude estimation functions certainly deviates sharply from a power function\". Indeed, nearly all sensory judgments can be changed by the context in which a stimulus is perceived.\n\n==See also==\n* [[Perception]]\n* [[Sone]]\n\n==References==\n{{reflist}}\n* {{citation |doi=10.3758/BF03212151 |last=Ellermeier |first=W. |last2=Faulhammer |first2=G. |year=2000 |title=Empirical evaluation of axioms fundamental to Stevens's ratio-scaling approach: I. Loudness production |journal=Perception & Psychophysics |volume=62 |issue=8 |pages=1505–1511}}\n* {{citation |doi=10.3758/BF03213947 |last=Green |first=D.M. |last2=Luce |first2=R.D. |year=1974 |title=Variability of magnitude estimates: a timing theory analysis |journal=Perception & Psychophysics |volume=15 |issue=2 |pages=291–300}}\n* {{citation |last=Luce |first=R.D. |year=1990 |title=Psychophysical laws: cross-modal matching |journal=Psychological Review |volume=97 |issue=1 |pages=66–77 |doi=10.1037/0033-295X.97.1.66}}\n* {{citation |last=Luce |first=R.D. |year=2002 |title=A psychophysical theory of intensity proportions, joint presentations, and matches |journal=Psychological Review |volume=109 |issue=3 |pages=520–532 |doi= 10.1037/0033-295X.109.3.520 |pmid=12088243|citeseerx=10.1.1.320.6454 }}\n* {{citation |last=Narens |first=L. |year=1996 |title=A theory of ratio magnitude estimation |journal=Journal of Mathematical Psychology |volume=40 |issue=2 |pages=109–129 |doi=10.1006/jmps.1996.0011}}\n* Luce, R. D. & Krumhansl, C. (1988) Measurement, scaling, and psychophysics. In R. C. Atkinson, R. J. Herrnstein, G. Lindzey, & R. D. Luce (Eds.) ''Stevens' Handbook of Experimental Psychology''. New York: Wiley. Pp.&nbsp;1–74.\n* [[Neil Smelser|Smelser, N.J.]], & Baltes, P.B. (2001). ''International encyclopedia of the social & behavioral sciences''. [http://web.mit.edu/epl/StevensBiography.pdf pp. 15105–15106]. Amsterdam; New York: Elsevier. {{ISBN|0-08-043076-7}}.\n* {{citation |last=Steingrimsson |first=R. |last2=Luce |first2=R.D. |year=2006 |title=Empirical evaluation of a model of global psychophysical judgments: III. A form for the psychophysical function and intensity filtering |journal=Journal of Mathematical Psychology |volume=50 |issue=1 |pages=15–29 |doi=10.1016/j.jmp.2005.11.005}}\n* {{cite journal | last1 = Stevens | first1 = S.S. | year = 1957 | title = On the psychophysical law | url = | journal = Psychological Review | volume = 64 | issue = 3| pages = 153–181 | pmid = 13441853 | doi=10.1037/h0046162}}\n* Stevens, S.S. (1975), Geraldine Stevens, editor. ''[https://books.google.com/books?id=r5JOHlXX8bgC&pg=PA15 Psychophysics: introduction to its perceptual, neural, and social prospects]'', Transaction Publishers, {{ISBN|978-0-88738-643-5}}.\n* {{cite journal | last1 = Zimmer | first1 = K. | year = 2005 | title = Examining the validity of numerical ratios in loudness fractionation | url = | journal = Perception & Psychophysics | volume = 67 | issue = 4| pages = 569–579 | doi=10.3758/bf03193515}}\n\n[[Category:Perception]]\n[[Category:Behavioral concepts]]\n[[Category:Power laws]]\n[[Category:Psychophysics]]\n[[Category:Mathematical psychology]]\n[[Category:Psychoacoustics]]\n\n[[it:Soglia percettiva]]"
    },
    {
      "title": "Zipf–Mandelbrot law",
      "url": "https://en.wikipedia.org/wiki/Zipf%E2%80%93Mandelbrot_law",
      "text": "<!-- EDITORS! Please see [[Wikipedia:WikiProject Probability#Standards]] for a discussion\nof standards used for probability distribution articles such as this one. -->\n{{Probability distribution|\n  name       =Zipf–Mandelbrot|\n  type       =mass|\n  pdf_image  =|\n  cdf_image  =|\n  parameters =<math>N \\in \\{1,2,3\\ldots\\}</math> ([[integer]])<br /><math>q \\in [0;\\infty)</math> ([[Real number|real]])<br /><math>s>0\\,</math> ([[real number|real]])|\n  support    =<math>k \\in \\{1,2,\\ldots,N\\}</math>|\n  pdf        =<math>\\frac{1/(k+q)^s}{H_{N,q,s}}</math>|\n  cdf        =<math>\\frac{H_{k,q,s}}{H_{N,q,s}}</math>|\n  mean       =<math>\\frac{H_{N,q,s-1}}{H_{N,q,s}}-q</math>|\n  median     =|\n  mode       =<math>1\\,</math>|\n  variance   =|\n  skewness   =|\n  kurtosis   =|\n  entropy    =<math>\\frac{s}{H_{N,q,s}}\\sum_{k=1}^N\\frac{\\ln(k + q)}{(k + q)^s} +\\ln(H_{N,q,s})</math>|\n  mgf        =|\n  char       =|\n}}\nIn [[probability theory]] and [[statistics]], the '''Zipf–Mandelbrot law''' is a [[discrete probability distribution]]. Also known as the [[Vilfredo Pareto|Pareto]]-Zipf law, it is a [[power-law]] distribution on [[Ranking|ranked data]], named after the [[linguistics|linguist]] [[George Kingsley Zipf]] who suggested a simpler distribution called [[Zipf's law]], and the mathematician [[Benoit Mandelbrot]], who subsequently generalized it.\n\nThe [[probability mass function]] is given by:\n\n:<math>f(k;N,q,s)=\\frac{1/(k+q)^s}{H_{N,q,s}}</math>\n\nwhere <math>H_{N,q,s}</math> is given by:\n\n:<math>H_{N,q,s}=\\sum_{i=1}^N \\frac{1}{(i+q)^s}</math>\n\nwhich may be thought of as a generalization of a [[harmonic number]]. In the formula, <math>k</math> is the rank of the data, and <math>q</math> and <math>s</math> are parameters of the distribution. In the limit as <math>N</math> approaches infinity, this becomes the [[Hurwitz zeta function]] <math>\\zeta(s,q)</math>. For finite <math>N</math> and <math>q=0</math> the Zipf–Mandelbrot law becomes [[Zipf's law]]. For infinite <math>N</math> and <math>q=0</math> it becomes a [[Zeta distribution]].\n\n==Applications==\n\nThe distribution of words ranked by their [[frequency]] in a random\n[[text corpus]] is approximated by a [[power-law]] distribution, known\nas [[Zipf's law]].\n\nIf one plots the [[frequency]] rank of words contained in a moderately sized corpus of text data versus the number of occurrences or actual frequencies, one obtains a [[power-law]] distribution, with [[exponent]] close to one (but see Powers, 1998 and Gelbukh & Sidorov, 2001). Zipf's law implicitly assumes a fixed vocabulary size, but the [[Harmonic number|Harmonic series]] with ''s''=1 does not converge, while the Zipf-Mandelbrot generalization with ''s''&gt;1 does. Furthermore, there is evidence that the closed class of functional words that define a language obeys a Zipf-Mandelbrot distribution with different parameters from the open classes of contentive words that vary by topic, field and register.<ref>{{cite paper\n| last = Powers | first = David M W\n| title = Applications and explanations of Zipf's law\n| year = 1998\n| conference = Joint conference on new methods in language processing and computational natural language learning\n| pages = 151–160\n| publisher = Association for Computational Linguistics\n}}</ref>\n\nIn ecological field studies, the [[relative abundance distribution]] (i.e. the graph of the number of species observed as a function of their abundance) is often found to conform to a Zipf–Mandelbrot law.<ref>{{cite journal | last = Mouillot | first = D |author2=Lepretre, A\n   | title = Introduction of relative abundance distribution (RAD) indices, estimated from the rank-frequency diagrams (RFD), to assess changes in community diversity\n  | journal = Environmental Monitoring and Assessment | volume = 63 | issue = 2 | pages = 279–295\n  | publisher = Springer | year = 2000 | url = http://cat.inist.fr/?aModele=afficheN&cpsidt=1411186\n  | accessdate = 24 Dec 2008 | doi = 10.1023/A:1006297211561}}</ref>\n\nWithin music, many metrics of measuring \"pleasing\" music conform to Zipf–Mandelbrot distributions.<ref>{{cite journal | last = Manaris | first = B |author2=Vaughan, D |author3=Wagner, CS |author4=Romero, J |author5=Davis, RB  | title = Evolutionary Music and the Zipf-Mandelbrot Law: Developing Fitness Functions for Pleasant Music | journal = Proceedings of 1st European Workshop on Evolutionary Music and Art (EvoMUSART2003) | volume = 611 | url = http://shaunwagner.com/writings_computer_evomus.html}}</ref>\n\n==Notes==\n{{reflist}}\n\n== References==\n* {{Cite book\n | last = Mandelbrot | first = Benoît | authorlink = Benoît Mandelbrot\n | chapter = Information Theory and Psycholinguistics\n | title = Scientific psychology\n | editor= B.B. Wolman and E. Nagel\n | year = 1965\n | publisher = Basic Books\n}} Reprinted as\n** {{Cite book\n | last = Mandelbrot | first = Benoît | authorlink = Benoît Mandelbrot\n | chapter = Information Theory and Psycholinguistics\n | title = Language\n | editor= R.C. Oldfield and J.C. Marchall\n | year = 1968\n | origyear = 1965\n | publisher = Penguin Books\n}}\n* {{cite paper\n| last = Powers | first = David M W\n| title = Applications and explanations of Zipf's law\n| year = 1998\n| conference = Joint conference on new methods in language processing and computational natural language learning\n| pages = 151–160\n| publisher = Association for Computational Linguistics\n}}\n* {{cite book\n  | last = Zipf\n  | first = George Kingsley\n  | authorlink = George Kingsley Zipf\n  | title = Selected Studies of the Principle of Relative Frequency in Language\n  | publisher = Harvard University Press\n  | year = 1932\n  | location = Cambridge, MA}}\n\n==External links==\n* [https://arxiv.org/abs/physics/9901035 Z. K. Silagadze: Citations and the Zipf-Mandelbrot's law]\n* [https://xlinux.nist.gov/dads/HTML/zipfslaw.html NIST: Zipf's law]\n* [https://web.archive.org/web/20060428014625/http://www.nslij-genetics.org/wli/zipf/index.html W. Li's References on Zipf's law]\n* [http://www.gelbukh.com/CV/Publications/2001/CICLing-2001-Zipf.htm Gelbukh & Sidorov, 2001: Zipf and Heaps Laws’ Coefficients Depend on Language]\n* [https://github.com/gkohri/discreteRNG C++ Library for generating random Zipf-Mandelbrot deviates.]\n\n{{ProbDistributions|discrete-finite}}\n\n{{DEFAULTSORT:Zipf-Mandelbrot Law}}\n[[Category:Discrete distributions]]\n[[Category:Power laws]]\n[[Category:Computational linguistics]]\n[[Category:Quantitative linguistics]]\n[[Category:Corpus linguistics]]"
    },
    {
      "title": "Zipf's law",
      "url": "https://en.wikipedia.org/wiki/Zipf%27s_law",
      "text": "<!-- EDITORS! Please see [[Wikipedia:WikiProject Probability123#Standards]] for a discussion of standards used for probability distribution articles such as this one.\n-->{{Probability distribution|\n  name       =Zipf's law|\n  type       =mass|\n  pdf_image  =[[Image:Zipf distribution PMF.png|325px|Plot of the Zipf PMF for ''N'' = 10]]<br /><small>Zipf PMF for ''N'' = 10 on a log–log scale. The horizontal axis is the index ''k''&nbsp;. (Note that the function is only defined at integer values of ''k''. The connecting lines do not indicate continuity.)</small>|\n  cdf_image  =[[Image:Zipf distribution CMF.png|325px|Plot of the Zipf CDF for N=10]]<br /><small>Zipf CDF for ''N'' = 10. The horizontal axis is the index ''k''&nbsp;. (Note that the function is only defined at integer values of ''k''. The connecting lines do not indicate continuity.)</small>|\n  parameters =<math>s \\geq 0\\,</math> ([[real number|real]])<br /><math>N \\in \\{1,2,3\\ldots\\}</math> ([[integer]])|\n  support    =<math>k \\in \\{1,2,\\ldots,N\\}</math>|\n  pdf        =<math>\\frac{1/k^s}{H_{N,s}}</math> where ''H<sub>N,s</sub>'' is the ''N''th generalized [[harmonic number]]|\n  cdf        =<math>\\frac{H_{k,s}}{H_{N,s}}</math>|\n  mean       =<math>\\frac{H_{N,s-1}}{H_{N,s}}</math>|\n  median     =|\n  mode       =<math>1\\,</math>|\n  variance   =<math>\\frac{H_{N,s-2}}{H_{N,s}}-\\frac{H^2_{N,s-1}}{H^2_{N,s}}</math>|\n  skewness   =|\n  kurtosis   =|\n  entropy    =<math>\\frac{s}{H_{N,s}}\\sum\\limits_{k=1}^N\\frac{\\ln(k)}{k^s}\n+\\ln(H_{N,s})</math>|\n  mgf        =<math>\\frac{1}{H_{N,s}}\\sum\\limits_{n=1}^N \\frac{e^{nt}}{n^s}</math>|\n  char       =<math>\\frac{1}{H_{N,s}}\\sum\\limits_{n=1}^N \\frac{e^{int}}{n^s}</math>|\n}}\n'''Zipf's law''' ({{IPAc-en|z|ɪ|f}}) is an [[empirical law]] formulated using [[mathematical statistics]] that refers to the fact that many types of data studied in the [[physical science|physical]] and [[social science|social]] sciences can be approximated with a Zipfian distribution, one of a family of related discrete [[power law]] [[probability distribution]]s.  ''Zipf distribution'' is related to the [[zeta distribution]], but is not identical.\n\nFor example, Zipf's law states that given some [[Text corpus|corpus]] of [[natural language]] utterances, the frequency of any word is [[inversely proportional]] to its rank in the [[frequency table]]. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.: the [[rank-frequency distribution]] is an inverse relation. For example, in the [[Brown Corpus]] of American English text, the word ''[[English articles#Definite article|the]]'' is the most frequently occurring word, and by itself accounts for nearly 7% of all word occurrences (69,971 out of slightly over 1 million). True to Zipf's Law, the second-place word ''of'' accounts for slightly over 3.5% of words (36,411 occurrences), followed by ''and'' (28,852). Only 135 vocabulary items are needed to account for half the [[Brown Corpus]].<ref>{{citation|contribution=An introduction to textual econometrics|first1=Stephen|last1=Fagan|first2=Ramazan|last2=Gençay|pages=133–153|title=Handbook of Empirical Economics and Finance|editor1-first=Aman|editor1-last=Ullah|editor2-first=David E. A.|editor2-last=Giles|publisher=CRC Press|year=2010|isbn=9781420070361}}. [https://books.google.com/books?hl=en&lr=&id=QAUv9R6bJzwC&oi=fnd&pg=PA139 P.&nbsp;139]: \"For example, in the Brown Corpus, consisting of over one million words, half of the word volume consists of repeated uses of only 135 words.\"</ref>\n\nThe law is named after the American [[linguistics|linguist]] [[George Kingsley Zipf]] (1902–1950), who popularized it and sought to explain it (Zipf 1935, 1949), though he did not claim to have originated it.<ref name=Powers1998>{{cite journal| last = Powers | first = David M W| url=http://aclweb.org/anthology/W98-1218 |title=Applications and explanations of Zipf's law| year = 1998| conference = Joint conference on new methods in language processing and computational natural language learning| pages = 151–160| publisher = Association for Computational Linguistics }}</ref> The French stenographer [[Jean-Baptiste Estoup]] (1868–1950) appears to have noticed the regularity before Zipf.<ref>Christopher D. Manning, Hinrich Schütze ''Foundations of Statistical Natural Language Processing'', MIT Press (1999), {{isbn|978-0-262-13360-9}}, p. 24</ref>{{not in body|reason=This inline citation may well be adequate to verify the claim, but something like this should not be in the lead if it is not also in the body.|date=May 2016}} It was also noted in 1913 by German physicist [[Felix Auerbach]]<ref name=\"Auerbach1913\">Auerbach F. (1913) Das Gesetz der Bevölkerungskonzentration. Petermann’s Geographische Mitteilungen 59, 74–76</ref> (1856–1933).\n\n==Other datasets==\nThe same relationship occurs in many other rankings unrelated to language, such as the population ranks of cities in various countries, corporation sizes, income rankings, ranks of number of people watching the same TV channel,<ref>M. Eriksson, S.M. Hasibur Rahman, F. Fraille, M. Sjöström, [http://apachepersonal.miun.se/~mageri/myresearch/bmsb2013-Eriksson.pdf Efficient Interactive Multicast over DVB-T2 - Utilizing Dynamic SFNs and PARPS] {{webarchive|url=https://web.archive.org/web/20140502183246/http://apachepersonal.miun.se/~mageri/myresearch/bmsb2013-Eriksson.pdf |date=2014-05-02}}, 2013 IEEE International Conference on Computer and Information Technology (BMSB'13), London, UK, June 2013. Suggests a heterogeneous Zipf-law TV channel-selection model</ref> and so on. The appearance of the distribution in rankings of cities by population was first noticed by Felix Auerbach in 1913.<ref name=\"Auerbach1913\"/> Empirically, a data set can be tested to see whether Zipf's law applies by checking the goodness of fit of an empirical distribution to the hypothesized power law distribution with a [[Kolmogorov–Smirnov test]], and then comparing the (log) likelihood ratio of the power law distribution to alternative distributions like an exponential distribution or lognormal distribution.<ref name=\"Clausetetal2009\">Clauset, A., Shalizi, C. R., & Newman, M. E. J. (2009). Power-Law Distributions in Empirical Data. SIAM Review, 51(4), 661–703. doi:10.1137/070710111</ref> When Zipf's law is checked for cities, a better fit has been found with exponent ''s'' = 1.07; i.e. the <math>n^{th}</math> largest settlement is <math>\\frac{1}{n^{1.07}}</math> the size of the largest settlement.\n\n==Theoretical review==\nZipf's law is most easily observed by [[graph of a function|plotting]] the data on a [[log-log]] graph, with the axes being [[logarithm|log]] (rank order) and log (frequency). For example, the word ''\"the\"'' (as described above) would appear at ''x'' = log(1), ''y'' = log(69971). It is also possible to plot reciprocal rank against frequency or reciprocal frequency or interword interval against rank.<ref name=Powers1998/> The data conform to Zipf's law to the extent that the plot is [[linear equation|linear]].\n\nFormally, let:\n* ''N'' be the number of elements;\n* ''k'' be their rank;\n* ''s'' be the value of the exponent characterizing the distribution.\nZipf's law then predicts that out of a population of ''N'' elements, the normalized frequency of elements of rank ''k'', ''f''(''k'';''s'',''N''), is:\n\n:<math>f(k;s,N)=\\frac{1/k^s}{\\sum\\limits_{n=1}^N (1/n^s)}</math>\n\nZipf's law holds if the number of elements with a given frequency is a random variable with power law distribution <math>p(f) = \\alpha f^{-1-1/s}.</math><ref name=Adamic2000>Adamic, Lada A. (2000) [http://www.hpl.hp.com/research/idl/papers/ranking/ranking.html \"Zipf, Power-laws, and Pareto - a ranking tutorial\", originally published at http://www.parc.xerox.com/istl/groups/iea/papers/ranking/ranking.html] {{webarchive|url=https://web.archive.org/web/20071026062626/http://www.hpl.hp.com/research/idl/papers/ranking/ranking.html |date=2007-10-26 }}</ref>\n\nIt has been claimed that this representation of Zipf's law is more suitable for statistical testing, and in this way it has been analyzed in more than 30,000 English texts. The goodness-of-fit tests yield that only about 15% of the texts are statistically compatible with this form of Zipf's law. Slight variations in the definition of Zipf's law can increase this percentage up to close to 50%.<ref>{{Cite journal|last = Moreno-Sánchez|first = I|last2 = Font-Clos|first2 = F|last3 = Corral|first3 = A|date = 2016|title = Large-Scale Analysis of Zipf's Law in English Texts|url =|journal = PLoS ONE|volume = 11|pages = e0147073|doi = 10.1371/journal.pone.0147073|pmid =|access-date =|arxiv = 1509.04486}}</ref>\n\nIn the example of the frequency of words in the English language, ''N'' is the number of words in the English language and, if we use the classic version of Zipf's law, the exponent ''s'' is 1. ''f''(''k'';&nbsp;''s'',''N'') will then be the fraction of the time the ''k''th most common word occurs.\n\nThe law may also be written:\n\n:<math>f(k;s,N)=\\frac{1}{k^s H_{N,s}}</math>\n\nwhere ''H<sub>N,s</sub>'' is the ''N''th generalized [[harmonic number]].\n\nThe simplest case of Zipf's law is a \"<sup>1</sup>⁄<sub>''f''</sub> function.\" Given a set of Zipfian distributed frequencies, sorted from most common to least common, the second most common frequency will occur ½ as often as the first.  The third most common frequency will occur ⅓ as often as the first. The fourth most common frequency will occur ¼ as often as the first. The ''n''<sup>th</sup> most common frequency will occur <sup>1</sup>⁄<sub>''n''</sub> as often as the first. However, this cannot hold exactly, because items must occur an integer number of times; there cannot be 2.5 occurrences of a word. Nevertheless, over fairly wide ranges, and to a fairly good approximation, many natural phenomena obey Zipf's law.\n\nIn human languages, word frequencies have a very heavy-tailed distribution, and can therefore be modeled reasonably well by a Zipf distribution with an ''s'' close to 1.\n\nAs long as the exponent ''s'' exceeds 1, it is possible for such a law to hold with infinitely many words, since if ''s''&nbsp;>&nbsp;1 then\n:<math>\\zeta (s) = \\sum_{n=1}^\\infty \\frac{1}{n^s}<\\infty. \\!</math>\nwhere ''ζ'' is [[Riemann zeta function|Riemann's zeta function]].\n\n==Statistical explanation==\n\n[[File:Zipf 30wiki en labels.png|thumbnail|A plot of the rank versus frequency for the first 10 million words in 30 Wikipedias (dumps from October 2015) in a [[log-log]] scale.]]\n\nAlthough Zipf’s Law holds for all languages, even non-natural ones like [[Esperanto]],<ref>{{cite conference |author1=Bill Manaris |author2=Luca Pellicoro |author3=George Pothering |author4=Harland Hodges |title=INVESTIGATING ESPERANTO’S STATISTICAL PROPORTIONS RELATIVE TO OTHER LANGUAGES USING NEURAL NETWORKS AND ZIPF’S LAW |url=http://www.cs.cofc.edu/~manaris/uploads/Main/IASTED2006.pdf |journal=[[Artificial Intelligence and Applications]] |date=13 February 2006 |location=Innsbruck, Austria |pages=102–108 |deadurl=no |archiveurl=https://web.archive.org/web/20160305040450/http://www.cs.cofc.edu/~manaris/uploads/Main/IASTED2006.pdf |archivedate=5 March 2016 |df= }}</ref> the reason is still not well understood.<ref>[[Léon Brillouin]], ''La science et la théorie de l'information'', 1959, réédité en 1988, traduction anglaise rééditée en 2004</ref> However, it may be partially explained by the statistical analysis of randomly generated texts. Wentian Li has shown that in a document in which each character has been chosen randomly from a uniform distribution of all letters (plus a space character), the \"words\" with different lengths follow the macro-trend of the Zipf's law (the more probable words are the shortest with equal probability).<ref>{{cite journal |author=Wentian Li |title=Random Texts Exhibit Zipf's-Law-Like Word Frequency Distribution |journal=[[IEEE Transactions on Information Theory]] |volume=38 |issue=6 |year=1992 |pages=1842–1845 |doi=10.1109/18.165464 |df= |citeseerx=10.1.1.164.8422 }}</ref>  [[Vitold Belevitch]] in a paper, ''On the Statistical Laws of Linguistic Distribution'' offered a mathematical derivation.  He took a large class of well-behaved [[statistical distribution]]s (not only the [[normal distribution]]) and expressed them in terms of rank. He then expanded each expression into a [[Taylor series]]. In every case Belevitch obtained the remarkable result that a first-order truncation of the series resulted in Zipf's law. Further, a second-order truncation of the Taylor series resulted in [[Zipf–Mandelbrot law|Mandelbrot's law]].<ref>[[Peter G. Neumann|Neumann, Peter G.]] [http://www.csl.sri.com/users/neumann/#12a \"Statistical metalinguistics and Zipf/Pareto/Mandelbrot\"], ''SRI International Computer Science Laboratory'', accessed and [https://www.webcitation.org/5z2UByabR?url=http://www.csl.sri.com/users/neumann/ archived] 29 May 2011.</ref><ref>{{cite journal\n| author = Belevitch V\n| title = On the statistical laws of linguistic distributions\n| journal = Annales de la Société Scientifique de Bruxelles\n| volume = 73\n| series = I\n| date = 18 December 1959\n| pages = 310–326\n}}</ref>\n\nThe [[principle of least effort]] is another possible explanation:\nZipf himself proposed that neither speakers nor hearers using a given language want to work any harder than necessary to reach understanding, and the process that results in approximately equal distribution of effort leads to the observed Zipf distribution.<ref>{{cite book\n| author = Zipf GK\n| title = Human Behavior and the Principle of Least Effort\n| location = Cambridge, Massachusetts\n| publisher = Addison-Wesley\n| year =1949\n| page = 1\n}}</ref><ref>{{cite journal\n |author1       = Ramon Ferrer i Cancho\n |author2       = Ricard V. Sole\n |lastauthoramp = yes\n |year          = 2003\n |title         = Least effort and the origins of scaling in human language\n |url           = http://www.pnas.org/content/100/3/788.abstract?sid=cc7fae18-87c9-4b67-863a-4195bb47c1d1\n |journal       = [[Proceedings of the National Academy of Sciences of the United States of America]]\n |volume        = 100\n |pages         = 788–791\n |issue         = 3\n |doi           = 10.1073/pnas.0335980100\n |pmid          = 12540826\n |pmc           = 298679\n |deadurl       = no\n |archiveurl    = https://web.archive.org/web/20111201040141/http://www.pnas.org/content/100/3/788.abstract?sid=cc7fae18-87c9-4b67-863a-4195bb47c1d1\n |archivedate   = 2011-12-01\n |df            = \n}}</ref>\n\nSimilarly, [[preferential attachment]] (intuitively, \"the rich get richer\" or \"success breeds success\") that results in the [[Yule–Simon distribution]] has been shown to fit word frequency versus rank in language<ref>{{cite arXiv |title=Scaling laws in human speech, decreasing emergence of new words and a generalized model |df= |eprint = 1412.4846|last1 = Lin|first1 = Ruokuang|last2 = Ma|first2 = Qianli D. Y.|last3 = Bian|first3 = Chunhua|class = cs.CL|year = 2014}}</ref> and population versus city rank<ref>{{cite journal |title=Test of two hypotheses explaining the size of populations in a system of cities |journal = Journal of Applied Statistics|volume = 42|issue = 12|pages = 2686–2693|df= |arxiv = 1506.08535|doi = 10.1080/02664763.2015.1047744|year = 2015|last1 = Vitanov|first1 = Nikolay K.|last2 = Ausloos|first2 = Marcel|last3 = Bian|first3 = Chunhua}}</ref> better than Zipf's law. It was originally derived to explain population versus rank in species by Yule, and applied to cities by Simon.\n\n==Related laws==\n\n[[Image:Wikipedia-n-zipf.png|thumb|A plot of word frequency in Wikipedia (November 27, 2006). The plot is in [[log-log]] coordinates. ''x''&nbsp; is rank of a word in the frequency table; ''y''&nbsp; is the total number of the word’s occurrences. Most popular words are \"the\", \"of\" and \"and\", as expected. Zipf's law corresponds to the middle linear portion of the curve, roughly following the green (1/''x'')&nbsp; line, while the early part is closer to the magenta (1/''x''<sup>0.5</sup>) line while the later part is closer to the cyan (1/(''k''&nbsp;+&nbsp;''x'')<sup>2.0</sup>) line. These lines correspond to three distinct parameterizations of the Zipf–Mandelbrot distribution, overall a [[broken power law]] with three segments: a head, middle, and tail.]]\n\n''Zipf's law'' in fact refers more generally to frequency distributions of \"rank data,\" in which the relative frequency of the ''n''th-ranked item is given by the [[Zeta distribution]], 1/(''n''<sup>''s''</sup>''ζ''(''s'')), where the parameter ''s''&nbsp;>&nbsp;1 indexes the members of this family of [[probability distribution]]s.  Indeed, ''Zipf's law'' is sometimes synonymous with \"zeta distribution,\" since probability distributions are sometimes called \"laws\". This distribution is sometimes called the '''Zipfian''' distribution.\n\nA generalization of Zipf's law is the [[Zipf–Mandelbrot law]], proposed by [[Benoît Mandelbrot]], whose frequencies are:\n\n:<math>f(k;N,q,s)=\\frac{[\\text{constant}]}{(k+q)^s}.\\,</math>\n\nThe \"constant\" is the reciprocal of the [[Hurwitz zeta function]] evaluated at ''s''. In practice, as easily observable in distribution plots for large corpora, the observed distribution can be modelled more accurately as a sum of separate distributions for different subsets or subtypes of words that follow different parameterizations of the Zipf–Mandelbrot distribution, in particular the closed class of functional words exhibit ''s'' lower than 1, while open-ended vocabulary growth with document size and corpus size require ''s'' greater than 1 for convergence of the [[harmonic series (mathematics)|Generalized Harmonic Series]].<ref name=Powers1998/>\n\nZipfian distributions can be obtained from [[Pareto distribution]]s by an exchange of variables.<ref name=\"Adamic2000\"/>\n\nThe Zipf distribution is sometimes called the '''discrete Pareto distribution'''<ref>{{cite book|title=Univariate Discrete Distributions|edition=second|year=1992|author1=N. L. Johnson |author2=S. Kotz |author3=A. W. Kemp  |last-author-amp=yes |publisher=John Wiley & Sons, Inc.|location=New York|isbn=978-0-471-54897-3|ref=harv}}, p. 466.</ref> because it is analogous to the continuous [[Pareto distribution]] in the same way that the [[Uniform distribution (discrete)|discrete uniform distribution]] is analogous to the [[Uniform distribution (continuous)|continuous uniform distribution]].\n\nThe tail frequencies of the [[Yule–Simon distribution]] are approximately\n\n:<math>f(k;\\rho) \\approx \\frac{[\\text{constant}]}{k^{\\rho+1}}</math>\n\nfor any choice of ''ρ'' > 0.\n\nIn the [[parabolic fractal distribution]], the logarithm of the frequency is a quadratic polynomial of the logarithm of the rank. This can markedly improve the fit over a simple power-law relationship.<ref name=\"Galien\">{{cite web |url=http://home.zonnet.nl/galien8/factor/factor.html |title=Factorial randomness: the Laws of Benford and Zipf with respect to the first digit distribution of the factor sequence from the natural numbers |author=Johan Gerard van der Galien |date=2003-11-08 |accessdate=8 July 2016 |archiveurl=https://web.archive.org/web/20070305150334/http://home.zonnet.nl/galien8/factor/factor.html |archivedate=2007-03-05}}</ref> Like fractal dimension, it is possible to calculate Zipf dimension, which is a useful parameter in the analysis of texts.<ref>Ali Eftekhari (2006) Fractal geometry of texts. ''Journal of Quantitative Linguistic'' 13(2-3): 177–193.</ref>\n\nIt has been argued that [[Benford's law]] is a special bounded case of Zipf's law,<ref name=\"Galien\"/> with the connection between these two laws being explained by  their both originating from scale invariant functional relations from statistical physics and critical phenomena.<ref>L. Pietronero, E. Tosatti, V. Tosatti, A. Vespignani (2001) Explaining the uneven distribution of numbers in nature: The laws of Benford and Zipf. ''Physica A'' 293: 297–304.</ref> The ratios of probabilities in Benford's law are not constant. The leading digits of data satisfying Zipf's law with s = 1 satisfy Benford's law.\n{| class=\"wikitable\" style=\"text-align: center;\"\n|-\n!<math>n</math>\n!Benford's law: <math>P(n) = </math><br/><math>\\log_{10}(n+1)-\\log_{10}(n)</math>\n!<math>\\frac{\\log(P(n)/P(n-1))}{\\log(n/(n-1))}</math>\n|-\n| 1\n| 0.30103000\n|  \n|-\n| 2\n| 0.17609126\n| −0.7735840\n|-\n| 3\n| 0.12493874\n| −0.8463832\n|-\n| 4\n| 0.09691001\n| −0.8830605\n|-\n| 5\n| 0.07918125\n| −0.9054412\n|-\n| 6\n| 0.06694679\n| −0.9205788\n|-\n| 7\n| 0.05799195\n| −0.9315169\n|-\n| 8\n| 0.05115252\n| −0.9397966\n|-\n| 9\n| 0.04575749\n| −0.9462848\n|}\n\n== Applications ==\nIn [[information theory]], a symbol (event, signal) of probability <math>p</math> contains <math>\\log_2(1/p)</math> [[bit]]s of information. Hence, Zipf law for natural numbers: <math>\\Pr(x) \\approx 1/x</math> is equivalent with number <math>x</math> containing <math>\\log_2(x)</math> bits of information. To add information from a symbol of probability <math>p</math> into information already stored in a natural number <math>x</math>, we should go to <math>x'</math> such that <math>\\log_2(x') \\approx \\log_2(x) + \\log_2(1/p)</math>, or equivalently <math>x' \\approx x/p</math>. For instance, in standard binary system we would have <math>x' = 2x + s</math>, what is optimal for <math>\\Pr(s=0) = \\Pr(s=1) = 1/2</math> probability distribution. Using <math>x' \\approx x/p</math> rule for a general probability distribution is the base of [[Asymmetric Numeral Systems]] family of  [[entropy coding]] methods used in [[data compression]], which state distribution is also governed by Zipf law.\n\nZipf's law also has been used for extraction of parallel fragments of texts out of comparable corpora.<ref>{{cite conference |url=https://comparable.limsi.fr/bucc2016/pdf/BUCC04.pdf |title=Parallel Document Identification using Zipf's Law |last1=Mohammadi |first1=Mehdi |author-link1= |date=2016 |publisher= |book-title=Proceedings of the Ninth Workshop on Building and Using Comparable Corpora |pages=21–25 |location=Portorož, Slovenia |conference=LREC 2016 |id= |deadurl=no |archiveurl=https://web.archive.org/web/20180323154706/https://comparable.limsi.fr/bucc2016/pdf/BUCC04.pdf |archivedate=2018-03-23 |df= }}</ref>\n\n==See also==\n{{div col|colwidth=20em}}\n* [[Bradford's law]]\n* [[Benford's law]]\n* [[Demographic gravitation]]\n* [[Frequency list]]\n* [[Gibrat's law]]\n* [[Heaps' law]]\n* [[Hapax legomenon]]\n* [[Lorenz curve]]\n* [[Lotka's law]]\n* [[Pareto distribution]]\n* [[Pareto principle]], a.k.a. the \"80–20 rule\"\n* [[Principle of least effort]]\n* [[Price's law]]\n* [[Rank-size distribution]]\n* [[King effect]]\n* [[Stigler's law of eponymy]]\n* [[1% rule (Internet culture)]]\n{{Div col end}}\n\n==References==\n{{Reflist}}\n\n==Further reading==\nPrimary:\n* [[George K. Zipf]] (1949) ''Human Behavior and the Principle of Least Effort''. Addison-Wesley. \"Online text [https://archive.org/details/in.ernet.dli.2015.90211]\" \n* George K. Zipf (1935) ''The Psychobiology of Language''. Houghton-Mifflin.\nSecondary:\n* Alexander Gelbukh and Grigori Sidorov (2001) [http://www.gelbukh.com/CV/Publications/2001/CICLing-2001-Zipf.htm \"Zipf and Heaps Laws’ Coefficients Depend on Language\"]. Proc. [[CICLing]]-2001, ''Conference on Intelligent Text Processing and Computational Linguistics'', February 18–24, 2001, Mexico City. Lecture Notes in Computer Science N 2004, {{ISSN|0302-9743}}, {{isbn|3-540-41687-0}}, Springer-Verlag: 332–335.\n* Damián H. Zanette (2006) \"[http://xxx.arxiv.org/abs/cs.CL/0406015 Zipf's law and the creation of musical context,]\" ''Musicae Scientiae 10'': 3–18.\n* Frans J. Van Droogenbroeck (2016) \"[https://www.academia.edu/24147736/ Handling the Zipf distribution in computerized authorship attribution]\"\n* Kali R. (2003) \"The city as a giant component: a random graph approach to Zipf's law,\" ''Applied Economics Letters 10'': 717–720(4)\n*{{cite journal |last= Gabaix|first= Xavier|authorlink= Xavier Gabaix|date=August 1999|title= Zipf's Law for Cities: An Explanation |journal= Quarterly Journal of Economics|volume= 114|issue= 3|pages= 739–67|issn= 0033-5533|url= http://pages.stern.nyu.edu/~xgabaix/papers/zipf.pdf|doi= 10.1162/003355399556133|citeseerx= 10.1.1.180.4097}}\n* Axtell, Robert L; [http://www.sciencemag.org/content/293/5536/1818.short Zipf distribution of US firm sizes], Science, 293, 5536, 1818, 2001, American Association for the Advancement of Science\n* Ramu Chenna, Toby Gibson; [http://www.worldcomp-proceedings.com/proc/p2011/BIC4329.pdf Evaluation of the Suitability of a Zipfian Gap Model for Pairwise Sequence Alignment],\nInternational Conference on Bioinformatics Computational Biology: 2011.\n* Shyklo A. (2017); [https://ssrn.com/abstract=2918642 Simple Explanation of Zipf's Mystery via New Rank-Share Distribution, Derived from Combinatorics of the Ranking Process], Available at SSRN: https://ssrn.com/abstract=2918642.\n\n==External links==\n{{commons category}}\n*{{Cite news | last = Strogatz | first = Steven  | authorlink = Steven Strogatz | title = Guest Column: Math and the City  | date = 2009-05-29 | url = http://judson.blogs.nytimes.com/2009/05/19/math-and-the-city/ | accessdate = 2009-05-29 | postscript = <!--None--> | work=The New York Times}}—An article on Zipf's law applied to city populations\n*[https://www.theatlantic.com/issues/2002/04/rauch.htm Seeing Around Corners (Artificial societies turn up Zipf's law)]\n*[https://web.archive.org/web/20021018011011/http://planetmath.org/encyclopedia/ZipfsLaw.html PlanetMath article on Zipf's law]\n*[http://www.hubbertpeak.com/laherrere/fractal.htm Distributions de type \"fractal parabolique\" dans la Nature (French, with English summary)]\n*[https://www.newscientist.com/article.ns?id=mg18524904.300 An analysis of income distribution]\n*[http://www.lexique.org/listes/liste_mots.txt Zipf List of French words]\n*[http://1.1o1.in/en/webtools/semantic-depth Zipf list for English, French, Spanish, Italian, Swedish, Icelandic, Latin, Portuguese and Finnish from Gutenberg Project and online calculator to rank words in texts]\n*[https://arxiv.org/abs/physics/9901035 Citations and the Zipf–Mandelbrot's law]\n*[http://www.geoffkirby.co.uk/ZIPFSLAW.pdf Zipf's Law examples and modelling (1985)]\n*[http://www.nature.com/nature/journal/v474/n7350/full/474164a.html Complex systems: Unzipping Zipf's law (2011)]\n*[http://terrytao.wordpress.com/2009/07/03/benfords-law-zipfs-law-and-the-pareto-distribution/ Benford’s law, Zipf’s law, and the Pareto distribution] by Terence Tao.\n\n{{ProbDistributions|discrete-finite}}\n\n{{Authority control}}\n\n[[Category:Discrete distributions]]\n[[Category:Computational linguistics]]\n[[Category:Power laws]]\n[[Category:Statistical laws]]\n[[Category:Empirical laws]]\n[[Category:Tails of probability distributions]]\n[[Category:Quantitative linguistics]]\n[[Category:Bibliometrics]]\n[[Category:Corpus linguistics]]\n[[Category:1949 introductions]]"
    },
    {
      "title": "Cosecans hyperbolicus",
      "url": "https://en.wikipedia.org/wiki/Cosecans_hyperbolicus",
      "text": "#REDIRECT [[Hyperbolic function#Csch]]\n\n{{Redirect category shell|1=\n{{R from other language|und|en}}\n{{R to anchor}}\n}}\n\n[[Category:Hyperbolic functions]]"
    },
    {
      "title": "Cosech",
      "url": "https://en.wikipedia.org/wiki/Cosech",
      "text": "#REDIRECT [[Hyperbolic function#Csch]]\n\n{{Redirect category shell|1=\n{{R to related topic}}\n{{R from mathematical symbol or equation}}\n}}\n\n[[Category:Hyperbolic functions]]"
    },
    {
      "title": "Cosh (mathematical function)",
      "url": "https://en.wikipedia.org/wiki/Cosh_%28mathematical_function%29",
      "text": "#REDIRECT [[Hyperbolic function#Cosh]]\n\n{{Redirect category shell|1=\n{{R to related topic}}\n{{R from mathematical symbol or equation}}\n}}\n\n[[Category:Hyperbolic functions]]"
    },
    {
      "title": "Cosinus hyperbolicus",
      "url": "https://en.wikipedia.org/wiki/Cosinus_hyperbolicus",
      "text": "#REDIRECT [[Hyperbolic function#Cosh]]\n\n{{Redirect category shell|1=\n{{R from other language|und|en}}\n{{R to anchor}}\n}}\n\n[[Category:Hyperbolic functions]]"
    },
    {
      "title": "Cotangens hyperbolicus",
      "url": "https://en.wikipedia.org/wiki/Cotangens_hyperbolicus",
      "text": "#REDIRECT [[Hyperbolic function#Coth]]\n\n{{Redirect category shell|1=\n{{R from other language|und|en}}\n{{R to anchor}}\n}}\n\n[[Category:Hyperbolic functions]]"
    },
    {
      "title": "Coth",
      "url": "https://en.wikipedia.org/wiki/Coth",
      "text": "#REDIRECT [[Hyperbolic function#Coth]]\n\n{{Redirect category shell|1=\n{{R to section}}\n{{R to related topic}}\n{{R from mathematical symbol or equation}}\n}}\n\n[[Category:Hyperbolic functions]]"
    },
    {
      "title": "Csch",
      "url": "https://en.wikipedia.org/wiki/Csch",
      "text": "#REDIRECT [[Hyperbolic function#Csch]]\n\n{{Redirect category shell|1=\n{{R to section}}\n{{R to related topic}}\n{{R from mathematical symbol or equation}}\n}}\n\n[[Category:Hyperbolic functions]]"
    },
    {
      "title": "Hyberbolic cosecant",
      "url": "https://en.wikipedia.org/wiki/Hyberbolic_cosecant",
      "text": "#REDIRECT [[Hyperbolic function#Csch]] {{R to related topic}}\n\n[[Category:Hyperbolic functions]]"
    },
    {
      "title": "Hyberbolic cosine",
      "url": "https://en.wikipedia.org/wiki/Hyberbolic_cosine",
      "text": "#REDIRECT [[Hyperbolic function#Cosh]] {{R to related topic}}\n\n[[Category:Hyperbolic functions]]"
    },
    {
      "title": "Hyberbolic cotangent",
      "url": "https://en.wikipedia.org/wiki/Hyberbolic_cotangent",
      "text": "#REDIRECT [[Hyperbolic function#Coth]] {{R to related topic}}\n\n[[Category:Hyperbolic functions]]"
    },
    {
      "title": "Hyberbolic secant",
      "url": "https://en.wikipedia.org/wiki/Hyberbolic_secant",
      "text": "#REDIRECT [[Hyperbolic function#Sech]] {{R to related topic}}\n\n[[Category:Hyperbolic functions]]"
    },
    {
      "title": "Hyberbolic sine",
      "url": "https://en.wikipedia.org/wiki/Hyberbolic_sine",
      "text": "#REDIRECT [[Hyperbolic function#Sinh]] {{R to related topic}}\n\n[[Category:Hyperbolic functions]]"
    },
    {
      "title": "Hyberbolic tangent",
      "url": "https://en.wikipedia.org/wiki/Hyberbolic_tangent",
      "text": "#REDIRECT [[Hyperbolic function#Tanh]] {{R to related topic}}\n\n[[Category:Hyperbolic functions]]"
    },
    {
      "title": "Secans hyperbolicus",
      "url": "https://en.wikipedia.org/wiki/Secans_hyperbolicus",
      "text": "#REDIRECT [[Hyperbolic function#Sech]] {{R from other language}}\n\n[[Category:Hyperbolic functions]]"
    },
    {
      "title": "Sech (function)",
      "url": "https://en.wikipedia.org/wiki/Sech_%28function%29",
      "text": "#Redirect [[Hyperbolic function#Sech]]\n\n{{Redirect category shell|\n{{R to related topic}}\n{{R from mathematical symbol or equation}}\n{{R hatnote}}\n{{R to anchor|printworthy}}\n{{R printworthy}}\n}}\n\n[[Category:Hyperbolic functions]]"
    },
    {
      "title": "Sinh (mathematical function)",
      "url": "https://en.wikipedia.org/wiki/Sinh_%28mathematical_function%29",
      "text": "#REDIRECT [[Hyperbolic function#Sinh]]\n\n{{Redirect category shell|1=\n{{R to related topic}}\n{{R from mathematical symbol or equation}}\n}}\n\n[[Category:Hyperbolic functions]]"
    },
    {
      "title": "Sinus hyperbolicus",
      "url": "https://en.wikipedia.org/wiki/Sinus_hyperbolicus",
      "text": "#REDIRECT[[Hyperbolic function#Sinh]] {{R from other language}}\n\n[[Category:Hyperbolic functions]]"
    },
    {
      "title": "Tangens hyperbolicus",
      "url": "https://en.wikipedia.org/wiki/Tangens_hyperbolicus",
      "text": "#REDIRECT [[Hyperbolic function#Tanh]] {{R from other language}}\n\n[[Category:Hyperbolic functions]]"
    }
  ]
}