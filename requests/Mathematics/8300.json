{
  "pages": [
    {
      "title": "Gomory–Hu tree",
      "url": "https://en.wikipedia.org/wiki/Gomory%E2%80%93Hu_tree",
      "text": "In [[combinatorial optimization]], the '''Gomory–Hu tree'''<ref>{{cite journal | last1 = Gomory | first1 = R. E. | authorlink = Ralph Edward Gomory | last2 = Hu | first2 = T. C. | title = Multi-terminal network flows | url = | journal = Journal of the Society for Industrial and Applied Mathematics | volume = 9 | issue = | year = 1961 }}</ref> of an undirected graph with capacities is a weighted [[Tree (graph theory)|tree]] that represents the minimum ''s''-''t'' cuts for all ''s''-''t'' pairs in the graph. The Gomory–Hu tree can be constructed in |&nbsp;''V''&nbsp;|&nbsp;−&nbsp;1 [[Maximum flow problem|maximum flow]] computations.\n\n==Definition==\nLet ''G'' = ((''V''<sub>G</sub>, ''E''<sub>G</sub>), ''c'') be an undirected graph with ''c''(''u'',''v'') being the capacity of the edge (''u'',''v'') respectively.\n\n: Denote the minimum capacity of an ''s''-''t'' cut by λ<sub>st</sub> for each ''s'', ''t'' ∈ ''V''<sub>G</sub>.\n: Let ''T'' = (''V''<sub>T</sub>,''E''<sub>T</sub>) be a tree with ''V''<sub>T</sub> = ''V''<sub>G</sub>, denote the set of edges in an ''s''-''t'' path by ''P''<sub>st</sub> for each ''s'',''t'' ∈ ''V''<sub>T</sub>.\nThen ''T'' is said to be a '''Gomory–Hu tree''' of ''G'' if\n: λ<sub>st</sub> = min<sub>e∈P<sub>st</sub></sub> ''c''(''S''<sub>e</sub>, ''T''<sub>e</sub>) for all ''s'', ''t'' ∈ ''V''<sub>G</sub>,\nwhere\n# ''S''<sub>e</sub> and ''T''<sub>e</sub> are the two connected components of ''T''∖{''e''} in the sense that (''S''<sub>e</sub>, ''T''<sub>e</sub>) form a ''s''-''t'' cut in ''G'', and\n# ''c''(''S''<sub>e</sub>, ''T''<sub>e</sub>) is the capacity of the cut in ''G''.\n\n==Algorithm==\n'''Gomory–Hu Algorithm'''\n:''Input'': A weighted undirected graph G = ((''V''<sub>G</sub>, ''E''<sub>G</sub>), ''c'')\n: ''Output'': A Gomory–Hu Tree ''T'' = (''V''<sub>T</sub>, ''E''<sub>T</sub>).\n:1. Set ''V''<sub>T</sub> = {''V''<sub>G</sub>} and ''E''<sub>T</sub> = ∅.\n:2. Choose some ''X''∈''V''<sub>T</sub> with | ''X'' | ≥ 2 if such ''X'' exists. Otherwise, go to step 6.\n:3. For each connected component ''C'' = (''V''<sub>C</sub>, ''E''<sub>C</sub>) in ''T''∖''X''. Let ''S''<sub>C</sub> = ∪<sub>v<sub>T</sub>∈V<sub>C</sub></sub> ''v''<sub>T</sub>. Let ''S'' = { ''S''<sub>C</sub> | ''C'' is a connected component in ''T''∖''X''}.\n:&nbsp;&nbsp;&nbsp; Contract the components to form ''G''<nowiki>'</nowiki> = ((''V''<sub>G<nowiki>'</nowiki></sub>, ''E''<sub>G<nowiki>'</nowiki></sub>), ''c''<nowiki>'</nowiki>), where\n:: ''V''<sub>G<nowiki>'</nowiki></sub> = ''X'' ∪ ''S''.\n:: ''E''<sub>G<nowiki>'</nowiki></sub> = ''E''<sub>G</sub>|<sub>X×X</sub> ∪ {(''u'', ''S''<sub>C</sub>) ∈ ''X''×''S'' | (''u'',''v'')∈''E''<sub>G</sub> for some ''v''∈''S''<sub>C</sub>}  ∪ {(''S''<sub>C1</sub>, ''S''<sub>C2</sub>) ∈ S×''S'' | (''u'',''v'')∈''E''<sub>G</sub> for some u∈''S''<sub>C1</sub> and ''v''∈''S''<sub>C2</sub>}.\n:: ''c''<nowiki>'</nowiki> : ''V''<sub>G<nowiki>'</nowiki></sub>×''V''<sub>G<nowiki>'</nowiki></sub>→''R''<sup>+</sup> is the capacity function defined as,\n::# if (''u'',''S''<sub>C</sub>)∈''E''<sub>G</sub>|<sub>X×S</sub>, ''c''<nowiki>'</nowiki>(''u'',''S''<sub>C</sub>) = Σ<sub>v∈S<sub>C</sub>:(u,v)∈E<sub>G</sub></sub>''c''(''u'',''v''),\n::# if (''S''<sub>C1</sub>,''S''<sub>C2</sub>)∈''E''<sub>G</sub>|<sub>S×S</sub>, ''c''<nowiki>'</nowiki>(''S''<sub>C1</sub>,''S''<sub>C2</sub>) = Σ<sub>(u,v)∈E<sub>G</sub>:u∈S<sub>C1</sub>∧v∈S<sub>C2</sub></sub> ''c''(''u'',''v''),\n::# ''c''<nowiki>'</nowiki>(''u'',''v'') = ''c''(''u'',''v'') otherwise.\n:4. Choose two vertices ''s'', ''t'' ∈ ''X'' and find a minimum ''s''-''t'' cut (''A''<nowiki>'</nowiki>,''B''<nowiki>'</nowiki>) in ''G''<nowiki>'</nowiki>.\n:&nbsp;&nbsp;&nbsp; Set ''A'' = (∪<sub>S<sub>C</sub>∈''A''<nowiki>'</nowiki>∩''S''</sub> ''S''<sub>C</sub>) ∪ (''A''<nowiki>'</nowiki> ∩ ''X'') and ''B'' = (∪<sub>S<sub>C</sub>∈''B''<nowiki>'</nowiki>∩''S''</sub> ''S''<sub>C</sub>) ∪ (''B''<nowiki>'</nowiki> ∩ ''X'').\n: 5. Set ''V''<sub>T</sub> = (''V''<sub>T</sub>∖''X'') ∪ {''A'' ∩ ''X'', ''B'' ∩ ''X''}.\n:&nbsp;&nbsp;&nbsp; For each ''e'' = (''X'', ''Y'') ∈ ''E''<sub>T</sub> do\n:: If ''Y''⊂''A'', set ''e''<nowiki>'</nowiki> = (''A'' ∩ ''X'', ''Y''), else set ''e''<nowiki>'</nowiki> = (''B'' ∩ ''X'', ''Y'').\n:: Set ''E''<sub>T</sub> = (''E''<sub>T</sub>∖{''e''}) ∪ {''e''<nowiki>'</nowiki>} and ''w''(''e''<nowiki>'</nowiki>) = ''w''(''e'').\n:&nbsp;&nbsp;&nbsp; Set ''E''<sub>T</sub> = ''E''<sub>T</sub> ∪ {(''A''∩''X'', ''B''∩''X'')}.\n:&nbsp;&nbsp;&nbsp; Set ''w''((''A''∩''X'', ''B''∩''X'')) = ''c''<nowiki>'</nowiki>(''A''<nowiki>'</nowiki>, ''B''<nowiki>'</nowiki>).\n:&nbsp;&nbsp;&nbsp; Go to step 2.\n: 6. Replace each {''v''} ∈ ''V''<sub>T</sub> by ''v'' and each ({''u''},{''v''}) ∈ ''E''<sub>T</sub> by (''u'',''v''). Output ''T''.\n\n==Analysis==\nUsing the [[submodular]] property of the capacity function ''c'', one has\n: ''c''(''X'') + ''c''(''Y'') ≥ ''c''(''X'' ∩ ''Y'') + ''c''(''X'' ∪ ''Y'').\nThen it can be shown that the minimum ''s''-''t'' cut in ''G''<nowiki>'</nowiki> is also a minimum ''s''-''t'' cut in ''G'' for any ''s'', ''t'' ∈ ''X''.\n\nTo show that for all (''P'', ''Q'') ∈ ''E''<sub>T</sub>, ''w''(''P'',''Q'') = λ<sub>''pq''</sub> for some ''p'' ∈ ''P'', ''q'' ∈ ''Q'' throughout the algorithm, one makes use of the following Lemma,\n: For any ''i'', ''j'', ''k'' in ''V''<sub>G</sub>, λ<sub>ik</sub> ≥ min(λ<sub>ij</sub>, λ<sub>jk</sub>).\n\nThe Lemma can be used again repeatedly to show that the output ''T'' satisfies the properties of a Gomory–Hu Tree.\n\n==Example==\nThe following is a simulation of the Gomory–Hu's algorithm, where\n# ''green'' circles are vertices of ''T''.\n# ''red'' and ''blue'' circles are the vertices in ''G''<nowiki>'</nowiki>.\n# ''grey'' vertices are the chosen ''s'' and ''t''.\n# ''red'' and ''blue'' coloring represents the ''s''-''t'' cut.\n# ''dashed'' edges are the ''s''-''t'' cut-set.\n# ''A'' is the set of vertices circled in ''red'' and ''B'' is the set of vertices circled in ''blue''.\n\n{| class=\"wikitable\" style=\"text-align:center; width:800px;\" border=\"1\"\n|-\n! width=\"15px\" |\n\n! ''G''<nowiki>'</nowiki>\n! ''T''\n|-\n|\n| [[Image:Gomory–Hu G.svg|300px]]\n| [[Image:Gomory–Hu T.svg|300px]]\n|-\n|\n| align=\"left\" colspan=\"2\" |\n:1. Set ''V''<sub>T</sub> = {''V''<sub>G</sub>} = { {0, 1, 2, 3, 4, 5} } and ''E''<sub>T</sub> = ∅.\n:2. Since ''V''<sub>T</sub> has only one vertex, choose ''X'' = ''V''<sub>G</sub> = {0, 1, 2, 3, 4, 5}. Note that | ''X'' | = 6 ≥ 2.\n|-\n! 1.\n| [[Image:Gomory–Hu Gp1.svg|300px]]\n| [[Image:Gomory–Hu T1.svg|300px]]\n|-\n|\n| align=\"left\" colspan=\"2\" |\n: 3. Since ''T''∖''X'' = ∅, there is no contraction and therefore ''G''<nowiki>'</nowiki> = ''G''.\n\n: 4. Choose ''s'' = 1 and ''t'' = 5. The minimum ''s''-''t'' cut (''A''<nowiki>'</nowiki>, ''B''<nowiki>'</nowiki>) is ({0, 1, 2, 4}, {3, 5}) with ''c''<nowiki>'</nowiki>(''A''<nowiki>'</nowiki>, ''B''<nowiki>'</nowiki>) = 6.\n:&nbsp;&nbsp;&nbsp; Set ''A'' = {0, 1, 2, 4} and ''B'' = {3, 5}.\n: 5. Set ''V''<sub>T</sub> = (''V''<sub>T</sub>∖''X'') ∪ {''A'' ∩ ''X'', ''B'' ∩ ''X''} = { {0, 1, 2, 4}, {3, 5} }.\n:&nbsp;&nbsp;&nbsp; Set ''E''<sub>T</sub> = { ({0, 1, 2, 4}, {3, 5}) }.\n:&nbsp;&nbsp;&nbsp; Set ''w''( ({0, 1, 2, 4}, {3, 5}) ) = ''c''<nowiki>'</nowiki>(''A''<nowiki>'</nowiki>, ''B''<nowiki>'</nowiki>) = 6.\n:&nbsp;&nbsp;&nbsp; Go to step 2.\n: 2. Choose ''X'' = {3, 5}. Note that | ''X'' | = 2 ≥ 2.\n|-\n! 2.\n| [[Image:Gomory–Hu Gp2.svg|300px]]\n| [[Image:Gomory–Hu T2.svg|300px]]\n|-\n|\n| align=\"left\" colspan=\"2\" |\n: 3. {0, 1, 2, 4} is the connected component in ''T''∖''X'' and thus ''S'' = { {0, 1, 2, 4} }.\n:&nbsp;&nbsp;&nbsp; Contract {0, 1, 2, 4} to form ''G''<nowiki>'</nowiki>, where\n:: ''c''<nowiki>'</nowiki>( (3, {0, 1, 2 ,4}) ) = ''c''( (3, 1) ) + ''c''( (3, 4) ) = 4.\n:: ''c''<nowiki>'</nowiki>( (5, {0, 1, 2, 4}) ) = ''c''( (5, 4) ) = 2.\n:: ''c''<nowiki>'</nowiki>( (3, 5)) = ''c''( (3, 5) ) = 6.\n: 4. Choose ''s'' = 3, ''t'' = 5. The minimum ''s''-''t'' cut (''A''<nowiki>'</nowiki>, ''B''<nowiki>'</nowiki>) in ''G''<nowiki>'</nowiki> is ( <nowiki>{{0, 1, 2, 4}, 3}, {5} )</nowiki> with ''c''<nowiki>'</nowiki>(''A''<nowiki>'</nowiki>, ''B''<nowiki>'</nowiki>) = 8.\n:&nbsp;&nbsp;&nbsp; Set ''A'' = {0, 1, 2, 3, 4} and ''B'' = {5}.\n: 5. Set ''V''<sub>T</sub> = (''V''<sub>T</sub>∖''X'') ∪ {''A'' ∩ ''X'', ''B'' ∩ ''X''} = <nowiki>{ {0, 1, 2, 4}, {3}, {5} }</nowiki>.\n:&nbsp;&nbsp;&nbsp; Since (''X'', {0, 1, 2, 4}) ∈ ''E''<sub>T</sub> and {0, 1, 2, 4} ⊂ ''A'', replace it with (''A'' ∩ ''X'', ''Y'') = ({3}, {0, 1, 2 ,4}).\n:&nbsp;&nbsp;&nbsp; Set ''E''<sub>T</sub> = { ({3}, {0, 1, 2 ,4}), ({3}, {5}) } with\n:: ''w''(({3}, {0, 1, 2 ,4})) = ''w''((''X'', {0, 1, 2, 4})) = 6.\n:: ''w''(({3}, {5})) = ''c''<nowiki>'</nowiki>(''A''<nowiki>'</nowiki>, ''B''<nowiki>'</nowiki>) = 8.\n:&nbsp;&nbsp;&nbsp; Go to step 2.\n: 2. Choose ''X'' = {0, 1, 2, 4}. Note that | ''X'' | = 4 ≥ 2.\n|-\n! 3.\n| [[Image:Gomory–Hu Gp3.svg|300px]]\n| [[Image:Gomory–Hu T3.svg|300px]]\n|-\n|\n| align=\"left\" colspan=\"2\" |\n: 3. { {3}, {5} } is the connected component in ''T''∖''X'' and thus ''S'' = { {3, 5} }.\n:&nbsp;&nbsp;&nbsp; Contract {3, 5} to form ''G''<nowiki>'</nowiki>, where\n:: ''c''<nowiki>'</nowiki>( (1, {3, 5}) ) = ''c''( (1, 3) ) = 3.\n:: ''c''<nowiki>'</nowiki>( (4, {3, 5}) ) = ''c''( (4, 3) ) + ''c''( (4, 5) ) = 3.\n:: ''c''<nowiki>'</nowiki>(''u'',''v'') = ''c''(''u'',''v'') for all ''u'',''v'' ∈ ''X''.\n: 4. Choose ''s'' = 1, ''t'' = 2. The minimum ''s''-''t'' cut (''A''<nowiki>'</nowiki>, ''B''<nowiki>'</nowiki>) in ''G''<nowiki>'</nowiki> is ( {1, {3, 5}, 4}, {0, 2} ) with ''c''<nowiki>'</nowiki>(''A''<nowiki>'</nowiki>, ''B''<nowiki>'</nowiki>) = 6.\n:&nbsp;&nbsp;&nbsp; Set ''A'' = {1, 3, 4, 5} and ''B'' = {0, 2}.\n: 5. Set ''V''<sub>T</sub> = (''V''<sub>T</sub>∖''X'') ∪ {''A'' ∩ ''X'', ''B'' ∩ ''X''} = { {3}, {5}, {1, 4}, {0, 2} }.\n:&nbsp;&nbsp;&nbsp; Since (''X'', {3}) ∈ ''E''<sub>T</sub> and {3} ⊂ ''A'', replace it with (''A'' ∩ ''X'', ''Y'') = ({1, 4}, {3}).\n:&nbsp;&nbsp;&nbsp; Set ''E''<sub>T</sub> = { ({1, 4}, {3}), ({3}, {5}), ({0, 2}, {1, 4}) } with\n:: ''w''(({1, 4}, {3})) = ''w''((''X'', {3})) = 6.\n:: ''w''(({0, 2}, {1, 4})) = ''c''<nowiki>'</nowiki>(''A''<nowiki>'</nowiki>, ''B''<nowiki>'</nowiki>) = 6.\n:&nbsp;&nbsp;&nbsp; Go to step 2.\n: 2. Choose ''X'' = {1, 4}. Note that | ''X'' | = 2 ≥ 2.\n|-\n! 4.\n| [[Image:Gomory–Hu Gp4.svg|300px]]\n| [[Image:Gomory–Hu T4.svg|300px]]\n|-\n|\n| align=\"left\" colspan=\"2\" |\n: 3. { {3}, {5} }, { {0, 2} } are the connected components in ''T''∖''X'' and thus ''S'' = { {0, 2}, {3, 5} }\n:&nbsp;&nbsp;&nbsp; Contract {0, 2} and {3, 5} to form ''G''<nowiki>'</nowiki>, where\n:: ''c''<nowiki>'</nowiki>( (1, {3, 5}) ) = ''c''( (1, 3) ) = 3.\n:: ''c''<nowiki>'</nowiki>( (4, {3, 5}) ) = ''c''( (4, 3) ) + ''c''( (4, 5) ) = 3.\n:: ''c''<nowiki>'</nowiki>( (1, {0, 2}) ) = ''c''( (1, 0) ) + ''c''( (1, 2) ) = 2.\n:: ''c''<nowiki>'</nowiki>( (4, {0, 2}) ) = ''c''( (4, 2) ) = 4.\n:: ''c''<nowiki>'</nowiki>(''u'',''v'') = ''c''(''u'',''v'') for all ''u'',''v'' ∈ ''X''.\n: 4. Choose ''s'' = 1, ''t'' = 4. The minimum ''s''-''t'' cut (''A''<nowiki>'</nowiki>, ''B''<nowiki>'</nowiki>) in ''G''<nowiki>'</nowiki> is ( <nowiki>{1, {3, 5}}, {{0, 2}, 4}</nowiki> ) with ''c''<nowiki>'</nowiki>(''A''<nowiki>'</nowiki>, ''B''<nowiki>'</nowiki>) = 7.\n:&nbsp;&nbsp;&nbsp; Set ''A'' = {1, 3, 5} and ''B'' = {0, 2, 4}.\n: 5. Set ''V''<sub>T</sub> = (''V''<sub>T</sub>∖''X'') ∪ {''A'' ∩ ''X'', ''B'' ∩ ''X''} = { {3}, {5}, {0, 2}, {1}, {4} }.\n:&nbsp;&nbsp;&nbsp; Since (''X'', {3}) ∈ ''E''<sub>T</sub> and {3} ⊂ ''A'', replace it with (''A'' ∩ ''X'', ''Y'') = ({1}, {3}).\n:&nbsp;&nbsp;&nbsp; Since (''X'', {0, 2}) ∈ ''E''<sub>T</sub> and {0, 2} ⊂ ''B'', replace it with (''B'' ∩ ''X'', ''Y'') = ({4}, {0, 2}).\n:&nbsp;&nbsp;&nbsp; Set ''E''<sub>T</sub> = { ({1}, {3}), ({3}, {5}), ({4}, {0, 2}), ({1}, {4}) } with\n:: ''w''(({1}, {3})) = ''w''((''X'', {3})) = 6.\n:: ''w''(({4}, {0, 2})) = ''w''((''X'', {0, 2})) = 6.\n:: ''w''(({1}, {4})) = ''c''<nowiki>'</nowiki>(''A''<nowiki>'</nowiki>, ''B''<nowiki>'</nowiki>) = 7.\n:&nbsp;&nbsp;&nbsp; Go to step 2.\n: 2. Choose ''X'' = {0, 2}. Note that | ''X'' | = 2 ≥ 2.\n|-\n! 5.\n| [[Image:Gomory–Hu Gp5.svg|300px]]\n| [[Image:Gomory–Hu T5.svg|300px]]\n|-\n|\n| align=\"left\" colspan=\"2\" |\n: 3. { {1}, {3}, {4}, {5} } is the connected component in ''T''∖''X'' and thus ''S'' = { {1, 3, 4, 5} }.\n:&nbsp;&nbsp;&nbsp; Contract {1, 3, 4, 5} to form ''G''<nowiki>'</nowiki>, where\n:: ''c''<nowiki>'</nowiki>( (0, {1, 3, 4, 5}) ) = ''c''( (0, 1) ) = 1.\n:: ''c''<nowiki>'</nowiki>( (2, {1, 3, 4, 5}) ) = ''c''( (2, 1) ) + ''c''( (2, 4) ) = 5.\n:: ''c''<nowiki>'</nowiki>( (0, 2) ) = ''c''( (0, 2) ) = 7.\n: 4. Choose ''s'' = 0, ''t'' = 2. The minimum ''s''-''t'' cut (''A''<nowiki>'</nowiki>, ''B''<nowiki>'</nowiki>) in ''G''<nowiki>'</nowiki> is ( <nowiki>{0}, {2, {1, 3, 4, 5}}</nowiki> ) with ''c''<nowiki>'</nowiki>(''A''<nowiki>'</nowiki>, ''B''<nowiki>'</nowiki>) = 8.\n:&nbsp;&nbsp;&nbsp; Set ''A'' = {0} and ''B'' = {1, 2, 3 ,4 ,5}.\n: 5. Set ''V''<sub>T</sub> = (''V''<sub>T</sub>∖''X'') ∪ {''A'' ∩ ''X'', ''B'' ∩ ''X''} = { {3}, {5}, {1}, {4}, {0}, {2} }.\n:&nbsp;&nbsp;&nbsp; Since (''X'', {4}) ∈ ''E''<sub>T</sub> and {4} ⊂ ''B'', replace it with (''B'' ∩ ''X'', ''Y'') = ({2}, {4}).\n:&nbsp;&nbsp;&nbsp; Set ''E''<sub>T</sub> = { ({1}, {3}), ({3}, {5}), ({2}, {4}), ({1}, {4}), ({0}, {2}) } with\n:: ''w''(({2}, {4})) = ''w''((''X'', {4})) = 6.\n:: ''w''(({0}, {2})) = ''c''<nowiki>'</nowiki>(''A''<nowiki>'</nowiki>, ''B''<nowiki>'</nowiki>) = 8.\n:&nbsp;&nbsp;&nbsp; Go to step 2.\n: 2. There does not exist ''X''∈''V''<sub>T</sub> with | ''X'' | ≥ 2. Hence, go to step 6.\n|-\n! 6.\n| align=\"center\" colspan=\"2\" |\n[[Image:Gomory–Hu output.svg|300px]]\n|-\n|\n| align=\"left\" colspan=\"2\" |\n: 6. Replace ''V''<sub>T</sub> = { {3}, {5}, {1}, {4}, {0}, {2} } by {3, 5, 1, 4, 0, 2}.\n:&nbsp;&nbsp;&nbsp; Replace ''E''<sub>T</sub> = { ({1}, {3}), ({3}, {5}), ({2}, {4}), ({1}, {4}), ({0}, {2}) } by { (1, 3), (3, 5), (2, 4), (1, 4), (0, 2) }.\n:&nbsp;&nbsp;&nbsp; Output ''T''. Note that exactly |&nbsp;''V''&nbsp;|&nbsp;−&nbsp;1 =&nbsp;6&nbsp;−&nbsp;1&nbsp;=&nbsp;5 times min-cut computation is performed.\n|-\n|}\n\n== Implementations: Sequential and Parallel ==\nGusfield's algorithm can be used to find a Gomory–Hu tree without any vertex contraction in the same running time-complexity, which simplifies the implementation of constructing a Gomory–Hu Tree.\n\n[[Andrew V. Goldberg]] and K. Tsioutsiouliklis implemented the Gomory-Hu algorithm and Gusfield algorithm. Experimental results comparing these algorithms are reported in<ref>{{cite journal|last=Goldberg|first=A. V.|author2=Tsioutsiouliklis, K.|title=Cut Tree Algorithms: An Experimental Study|journal=Journal of Algorithms|year=2001|volume=38|issue=1|pages=51–83|doi=10.1006/jagm.2000.1136 }}</ref> Source code is available [https://web.archive.org/web/20130225053024/http://www.cs.princeton.edu/~kt/cut-tree/ here].\n\nCohen et al.<ref>{{cite journal|last=Cohen|first=J. |author2=L. A. Rodrigues |author3=F. Silva |author4=R. Carmo |author5=A. Guedes |author6=E. P. Duarte Jr. |title=Parallel Implementations of Gusfield's Cut Tree Algorithm|journal=Lecture Notes in Computer Science (LNCS)|year=2011|series=7016|issue=11th International Conference Algorithms and Architectures for Parallel Processing (ICA3PP)|url=http://www.springerlink.com/content/g5k587003313j82u/|publisher=Springer|issn=0302-9743}}</ref> reports results on two parallel implementations of Gusfield's algorithm using OpenMP and MPI, respectively. Source code of these implementations is available here: [https://archive.is/20130101000727/http://www.inf.ufpr.br/jaime/parallel-cuttree.html Parallel Cut Tree Algorithms Page].\n\n==History==\nThe Gomory–Hu tree was introduced by [[Ralph E. Gomory|R. E. Gomory]] and T. C. Hu in 1961.\n\n==Related concepts==\nIn [[planar graph]]s, the Gomory–Hu tree is dual to the minimum weight [[cycle basis]], in the sense that the cuts of the Gomory–Hu tree are dual to a collection of cycles in the [[dual graph]] that form a minimum-weight cycle basis.<ref>{{cite journal\n | last1 = Hartvigsen | first1 = D.\n | last2 = Mardon | first2 = R.\n | doi = 10.1137/S0895480190177042\n | issue = 3\n | journal = SIAM J. Discrete Math.\n | pages = 403–418\n | title = The all-pairs min cut problem and the minimum cycle basis problem on planar graphs\n | volume = 7\n | year = 1994}}.</ref>\n\n==See also==\n* [[Cut (graph theory)]]\n* [[Max-flow min-cut theorem]]\n* [[Maximum flow problem]]\n\n==References==\n{{reflist}}\n*{{cite journal\n | doi = 10.1137/0219009\n | author = Dan Gusfield\n | title = Very Simple Methods for All Pairs Network Flow Analysis\n | journal = SIAM J. Comput.\n | volume = 19\n | issue = 1\n | pages = 143–155\n | year = 1990\n}}\n* {{cite book | author = B. H. Korte, Jens Vygen | title = Combinatorial Optimization: Theory and Algorithms (Algorithms and Combinatorics, 21) | chapter = 8.6 Gomory–Hu Trees | year = 2008 | publisher = Springer Berlin Heidelberg | isbn = 978-3-540-71844-4 | pages = 180–186}}\n\n{{DEFAULTSORT:Gomory-Hu tree}}\n[[Category:Combinatorial optimization]]\n[[Category:Network flow problem]]\n[[Category:Graph algorithms]]"
    },
    {
      "title": "Graph bandwidth",
      "url": "https://en.wikipedia.org/wiki/Graph_bandwidth",
      "text": "In [[graph theory]], the '''graph bandwidth problem''' is to label the ''n'' vertices ''v<sub>i</sub>'' of a graph ''G'' with distinct integers ''f''(''v<sub>i</sub>'') so that the quantity <math>\\max\\{\\,| f(v_i) - f(v_j)| : v_iv_j \\in E \\,\\}</math> is minimized (''E'' is the edge set of ''G'').<ref>{{harv|Chinn|Chvátalová|Dewdney|Gibbs|1982}}</ref>\nThe problem may be visualized as placing the vertices of a graph at distinct integer points along the ''x''-axis so that the length of the longest edge is minimized. Such placement is called '''linear graph arrangement''', '''linear graph layout''' or '''linear graph placement'''.<ref name=feige/>\n\nThe '''weighted graph bandwidth problem''' is a generalization wherein the edges are assigned weights ''w<sub>ij</sub>'' and the [[Loss function|cost function]] to be minimized is <math>\\max\\{\\, w_{ij} |f(v_i) - f(v_j)| : v_iv_j \\in E \\,\\}</math>.\n\nIn terms of matrices, the (unweighted) graph bandwidth is the [[bandwidth (matrix theory)|bandwidth]] of the [[symmetric matrix]] which is the [[adjacency matrix]] of the graph.\nThe bandwidth may also be defined as one less than the [[maximum clique]] size in a [[proper interval graph|proper interval]] supergraph of the given graph, chosen to minimize its clique size {{harv|Kaplan|Shamir|1996}}.\n\n==Bandwidth formulas for some graphs==\nFor several families of graphs, the bandwidth <math>\\varphi(G)</math> is given by an explicit formula.\n\nThe bandwidth of a [[path graph]] ''P''<sub>''n''</sub> on ''n'' vertices is&nbsp;1, and for a complete graph ''K''<sub>''m''</sub> we have <math>\\varphi(K_n)=n-1</math>. For the [[complete bipartite graph]] ''K''<sub>''m'',''n''</sub>,\n\n:<math>\\varphi(K_{m,n}) = \\lfloor (m-1)/2\\rfloor+n</math>, assuming <math>m \\ge n\\ge 1,</math>\n\nwhich was proved by Chvátal.<ref>A remark on a problem of Harary. V. Chvátal, ''Czechoslovak Mathematical Journal'' '''20'''(1):109&ndash;111, 1970. [http://dml.cz/dmlcz/100949 http://dml.cz/dmlcz/100949]</ref> As a special case of this formula, the [[star graph]] <math>S_k = K_{k,1}</math> on ''k''&nbsp;+&nbsp;1 vertices has bandwidth <math>\\varphi(S_{k}) = \\lfloor (k-1)/2\\rfloor+1</math>.\n\nFor the [[hypercube graph]] <math>Q_n</math> on <math>2^n</math> vertices the bandwidth was determined by {{harvtxt|Harper|1966}} to be\n:<math>\\varphi(Q_n)=\\sum_{m=0}^{n-1} \\binom{m}{\\lfloor m/2\\rfloor}.</math>\n\nChvatálová showed<ref>Optimal Labelling of a product of two paths. J. Chvatálová, ''Discrete Mathematics'' '''11''', 249&ndash;253, 1975.</ref> that the bandwidth of the ''m''&nbsp;&times;&nbsp;''n'' [[lattice graph|square grid graph]] <math>P_m \\times P_n</math>, that is, the [[Cartesian product of graphs|Cartesian product]] of two path graphs on <math>m</math> and <math>n</math> vertices, is equal to&nbsp;min{''m'',''n''}.\n\n==Bounds==\n\nThe bandwidth of a graph can be bounded in terms of various other graph parameters. For instance, letting χ(''G'') denote the [[chromatic number]] of ''G'', \n\n: <math> \\varphi(G) \\ge \\chi(G) - 1; </math>\n\nletting diam(''G'') denote the [[diameter (graph theory)|diameter]] of ''G'', the following inequalities hold:<ref>{{harvnb|Chinn|Chvátalová|Dewdney|Gibbs|1982}}</ref>\n\n:<math>\\lceil (n-1)/\\operatorname{diam}(G) \\rceil \\le \\varphi(G) \\le n - \\operatorname{diam}(G),</math>\n\nwhere <math>n</math> is the number of vertices in <math>G</math>.\n\nIf a graph ''G'' has bandwidth ''k'', then its [[pathwidth]] is at most ''k'' {{harv|Kaplan|Shamir|1996}}, and its [[tree-depth]] is at most ''k''&nbsp;log(''n''/''k'') {{harv|Gruber|2012}}. In contrast, as noted in the previous section, the star graph ''S''<sub>''k''</sub>, a structurally very simple example of a [[tree (graph theory)|tree]], has comparatively large bandwidth. Observe that the [[pathwidth]] of ''S''<sub>''k''</sub> is&nbsp;1, and its tree-depth is&nbsp;2.\n\nSome graph families of bounded degree have sublinear bandwidth: {{harvtxt|Chung|1988}} proved that if ''T'' is a tree of maximum degree at most ∆, then \n\n:<math>\\varphi(T) \\le \\frac{5n}{\\log_\\Delta n}. </math>\n\nMore generally, for [[planar graph]]s of bounded maximum degree at most ''∆'', a similar bound holds (cf. {{harvnb|Böttcher|Pruessmann|Taraz|Würfl|2010}}):\n\n:<math>\\varphi(G) \\le \\frac{20n}{\\log_\\Delta n}.</math>\n\n==Computing the bandwidth==\nBoth the unweighted and weighted versions are special cases of the [[quadratic bottleneck assignment problem]].\nThe bandwidth problem is [[NP-hard]], even for some special cases.<ref>Garey–Johnson: GT40</ref> Regarding the existence of efficient\n[[approximation algorithm]]s, it is known that the bandwidth is [[hardness of approximation|NP-hard to approximate]] within any constant, and this even holds when the input graphs are restricted to [[caterpillar tree]]s with maximum hair length 2 {{harv|Dubey|Feige|Unger|2010}}.\nFor the case of dense graphs, a 3-approximation algorithm was designed by {{harvtxt|Karpinski|Wirtgen|Zelikovsky|1997}}.\nOn the other hand, a number of polynomially-solvable special cases are known.<ref name=feige>\"Coping with the NP-Hardness of the Graph Bandwidth Problem\", Uriel Feige, ''[[Lecture Notes in Computer Science]]'', Volume 1851, 2000, pp. 129-145, {{doi|10.1007/3-540-44985-X_2}}</ref> A [[heuristic]] algorithm for obtaining linear graph layouts of low bandwidth is the [[Cuthill–McKee algorithm]]. Fast multilevel algorithm for graph bandwidth computation was proposed in.<ref name=\"multilevellinord\">\n{{cite journal\n | author = Ilya Safro and Dorit Ron and Achi Brandt\n | title = Multilevel Algorithms for Linear Ordering Problems\n | journal = ACM Journal of Experimental Algorithmics\n | year = 2008\n | pages = 1.4–1.20\n | volume = 13\n | doi=10.1145/1412228.1412232\n }}</ref>\n\n==Applications==\nThe interest in this problem comes from some application areas.\n\nOne area is [[sparse matrix]]/[[band matrix]] handling, and general algorithms from this area, such as [[Cuthill–McKee algorithm]], may be applied to find approximate solutions for the graph bandwidth problem.\n\nAnother application domain is in [[electronic design automation]]. In [[standard cell]] design methodology, typically standard cells have the same height, and their [[placement (EDA)|placement]] is arranged in a number of rows. In this context, graph bandwidth problem models the problem of placement of a set of standard cells in a single row with the goal of minimizing the maximal [[propagation delay]] (which is assumed to be proportional to wire length).\n\n==See also==\n*[[Pathwidth]], a different NP-complete optimization problem involving linear layouts of graphs.\n\n==References==\n{{reflist}}\n*{{Cite journal | last1 = Böttcher | first1 = J. | last2 = Pruessmann | first2 = K. P. | last3 = Taraz | first3 = A. | last4 = Würfl | first4 = A. | title = Bandwidth, expansion, treewidth, separators and universality for bounded-degree graphs | doi = 10.1016/j.ejc.2009.10.010 | journal = European Journal of Combinatorics | volume = 31 | pages = 1217–1227 | year = 2010 | pmid =  | pmc = |ref=harv | arxiv = 0910.3014 }}\n*{{Cite journal | last1 = Chinn | first1 = P. Z. |author1-link=Phyllis Chinn| last2 = Chvátalová | first2 = J. | last3 = Dewdney | first3 = A. K. |author3-link=Alexander Dewdney| last4 = Gibbs | first4 = N. E. | title = The bandwidth problem for graphs and matrices—a survey | journal = Journal of Graph Theory | volume = 6 | pages = 223–254| year = 1982 | doi = 10.1002/jgt.3190060302 |ref=harv}}\n*{{citation\n | last = Chung\n | first = Fan R. K.\n | author-link =Fan Chung\n | last2 =\n | first2 =\n | author2-link =\n | year = 1988\n | publication-date =\n | contribution = Labelings of Graphs\n | contribution-url =\n | editor-last = Beineke\n | editor-first = Lowell W.\n | editor-link = \n | editor2-last = Wilson\n | editor2-first = Robin J.\n | editor2-link =\n | title = Selected Topics in Graph Theory\n | edition =\n | series =\n | place =\n | publication-place =\n | publisher = Academic Press\n | volume =\n | pages = 151–168\n | id =\n | isbn = 978-0-12-086203-0 \n | doi =\n | oclc =\n | url = http://www.math.ucsd.edu/~fan/mypaps/fanpap/86log.PDF\n | ref=harv\n}}\n*{{Cite journal | last1 = Dubey | first1 = C. | last2 = Feige | first2 = U. | last3 = Unger | first3 = W. | title = Hardness results for approximating the bandwidth | journal = Journal of Computer and System Sciences | volume = 77 | pages = 62–90| year = 2010 | doi = 10.1016/j.jcss.2010.06.006 | ref=harv}}\n* {{cite book\n | last = Garey\n | first = M.R.\n | authorlink = Michael Garey\n |author2=Johnson, D.S. |authorlink2=David S. Johnson \n | title = [[Computers and Intractability: A Guide to the Theory of NP-Completeness]]\n | year = 1979\n | publisher = W.H. Freeman\n | location = New York\n | isbn =  0-7167-1045-5\n | ref=harv\n}}\n*{{Citation\n | title = On Balanced Separators, Treewidth, and Cycle Rank\n | year = 2012\n | last = Gruber | first = Hermann\n | journal = Journal of Combinatorics\n | pages = 669–682\n | volume = 3\n | issue = 4\n | doi=10.4310/joc.2012.v3.n4.a5\n | ref=harv\n| arxiv = 1012.1344\n }}\n*{{Cite journal | last1 = Harper | first1 = L. | title = Optimal numberings and isoperimetric problems on graphs | journal = Journal of Combinatorial Theory | volume = 1 | pages = 385–393 | year = 1966 | doi = 10.1016/S0021-9800(66)80059-5 |ref=harv}}\n*{{Citation\n | title = Pathwidth, bandwidth, and completion problems to proper interval graphs with small cliques\n | year = 1996\n | journal = [[SIAM Journal on Computing]]\n | pages = 540–561\n | volume = 25\n | issue = 3\n | last1 = Kaplan     | first1 =  Haim\n | last2 =  Shamir     | first2 =  Ron\n | ref=harv\n | doi=10.1137/s0097539793258143}}\n*{{Cite journal\n | last1 = Karpinski | first1 = Marek \n | last2 = Wirtgen | first2 = Jürgen \n | last3 = Zelikovsky | first3 = Aleksandr \n | title = An Approximation Algorithm for the Bandwidth Problem on Dense Graphs\n | journal = Electronic Colloquium on Computational Complexity\n | volume = 4 | number = 17\n | year = 1997\n | url = http://eccc.hpi-web.de/report/1997/017/\n | ref=harv\n}}\n\n==External links==\n*[http://www.csc.kth.se/~viggo/wwwcompendium/node53.html Minimum bandwidth problem], in: Pierluigi Crescenzi and Viggo Kann (eds.), ''A compendium of NP optimization problems.'' Accessed May 26, 2010.\n\n[[Category:Graph algorithms]]\n[[Category:Combinatorial optimization]]\n[[Category:NP-hard problems]]\n[[Category:Graph invariants]]"
    },
    {
      "title": "Graph cut optimization",
      "url": "https://en.wikipedia.org/wiki/Graph_cut_optimization",
      "text": "{{short description|Combinatorial optimization method for a family of functions of discrete variables}}\n'''Graph cut optimization''' is a [[combinatorial optimization]] method applicable to a family of [[function (mathematics)|function]]s of [[Continuous or discrete variable|discrete variable]]s, named after the concept of [[cut (graph theory)|cut]] in the theory of [[flow network]]s. Thanks to the [[max-flow min-cut theorem]], determining the [[minimum cut]] over a [[graph (discrete mathematics)|graph]] representing a flow network is equivalent to computing the [[maximum flow]] over the network. Given a [[pseudo-Boolean function]], if it is possible to construct a flow network such that the variables of the function are represented by nodes in the network, and for each possible cut the value of the flow equals the value of the function when each variable assumes a binary value depending on the belonging of its representing node to the source or sink component after the cut, then it is possible to find the [[global optimum]] of such function in [[polynomial time]] by computing a minimum cut of the graph.\n\nNot all pseudo-Boolean functions can be represented by a flow network, and in the general case the global optimization problem is [[NP-hard]]. There exist sufficient conditions to characterise families of functions that can be optimised through graph cuts, such as [[Pseudo-Boolean function#Submodularity|submodular quadratic functions]]. Graph cut optimization can be extended to functions of discrete variables with a finite number of values, that can be approached with iterative algorithms with strong optimality properties, computing one graph cut at each iteration.\n\nGraph cut optimization is an important tool for inference over [[graphical models]] such as [[Markov random field]]s or [[conditional random field]]s, and it has applications in [[computer vision]] problems such as [[image segmentation]],<ref name=\"peng\" /><ref name=\"grabcut\" /> [[image denoising|denoising]],<ref name=\"lombaert\" /> [[image registration|registration]]<ref name=\"so\" /><ref name=\"tang\" /> and [[stereo cameras|stereo matching]].<ref name=\"kim\" /><ref name=\"hong\" />\n\n== Representability ==\n\nA [[pseudo-Boolean function]] <math>f: \\{0, 1\\}^n \\to \\mathbb{R}</math> is said to be ''representable'' if there exists a graph <math>G = (V, E)</math> with non-negative weights and with source and sink nodes <math>s</math> and <math>t</math> respectively, and there exists a set of nodes <math>V_0 = \\{v_1, \\dots, v_n\\} \\subset V - \\{s, t\\}</math> such that, for each tuple of values <math>(x_1, \\dots, x_n) \\in \\{0, 1\\}^n</math> assigned to the variables, <math>f(x_1, \\dots, x_n)</math> equals (up to a constant) the value of the flow determined by a minimum [[cut (graph theory)|cut]] <math>C = (S, T)</math> of the graph <math>G</math> such that <math>v_i \\in S</math> if <math>x_i = 0</math> and <math>v_i \\in T</math> if <math>x_i = 1</math>.<ref name=\"what\" />\n\nIt is possible to classify pseudo-Boolean functions according to their order, determined by the maximum number of variables contributing to each single term. All first order functions, where each term depends upon at most one variable, are always representable. Quadratic functions\n\n:<math> f(\\mathbf{x}) = w_0 + \\sum_i w_i(x_i) + \\sum_{i < j} w_{ij}(x_i, x_j) . </math>\n\nare representable if and only if they are submodular, i.e. for each quadratic term <math>w_{ij}</math> the following condition is satisfied\n\n:<math> w_{ij}(0, 0) + w_{ij}(1, 1) \\le w_{ij}(0, 1) + w_{ij}(1, 0) . </math>\n\nCubic functions\n\n:<math> f(\\mathbf{x}) = w_0 + \\sum_i w_i(x_i) + \\sum_{i < j} w_{ij}(x_i, x_j) + \\sum_{i < j < k} w_{ijk}(x_i, x_j, x_k) </math>\n\nare representable if and only if they are ''regular'', i.e. all possible binary projections to two variables, obtained by fixing the value of the remaining variable, are submodular. For higher-order functions, regularity is a necessary condition for representability.<ref name=\"what\" />\n\n== Graph construction ==\n\nGraph construction for a representable function is simplified by the fact that the sum of two representable functions <math>f'</math> and <math>f''</math> is representable, and its graph <math>G = (V' \\cup V'', E' \\cup E'')</math> is the union of the graphs <math>G' = (V', E')</math> and <math>G'' = (V'', E'')</math> representing the two functions. Such theorem allows to build separate graphs representing each term and combine them to obtain a graph representing the entire function.<ref name=\"what\" />\n\nThe graph representing a quadratic function of <math>n</math> variables contains <math>n + 2</math> vertices, two of them representing the source and sink and the others representing the variables. When representing higher-order functions, the graph contains auxiliary nodes that allow to model higher-order interactions.\n\n=== Unary terms ===\n\nA unary term <math>w_i</math> depends only on one variable <math>x_i</math> and can be represented by a graph with one non-terminal node <math>v_i</math> and one edge <math>s \\rightarrow v_i</math> with weight <math>w_i(1) - w_i(0)</math> if <math>w_i(1) \\ge w_i(0)</math>, or <math>v_i \\rightarrow t</math> with weight <math>w_i(0) - w_i(1)</math> if <math>w_i(1) < w_i(0)</math>.<ref name=\"what\" />\n\n=== Binary terms ===\n\n[[File:Graph cut binary.svg|thumb|Example of a graph representing a quadratic term <math>w_{ij}(x_i, x_j)</math> in case <math>w_{ij}(1, 0) - w_{ij}(0, 0) > 0</math> and <math>w_{ij}(1, 1) - w_{ij}(1, 0) < 0</math>.]]\n\nA quadratic (or binary) term <math>w_{ij}</math> can be represented by a graph containing two non-terminal nodes <math>v_i</math> and <math>v_j</math>. The term can be rewritten as\n\n:<math>w_{ij}(x_i, x_j) = w_{ij}(0, 0) + k_i x_i + k_j x_j + k_{ij} \\left( (1 - x_i) x_j + x_i (1 - x_j) \\right)</math>\n\nwith\n\n:<math>\n\\begin{align}\n    k_i    &= \\frac{1}{2} (w_{ij}(1, 0) - w_{ij}(0, 0)) \\\\\n    k_j    &= \\frac{1}{2} (w_{ij}(1, 1) - w_{ij}(1, 0)) \\\\\n    k_{ij} &= \\frac{1}{2} (w_{ij}(0, 1) + w_{ij}(1, 0) - w_{ij}(0, 0) - w_{ij}(1, 1)) .\n\\end{align}\n</math>\n\nIn this expression, the first term is constant and it is not represented by any edge, the two following terms depend on one variable and are represented by one edge, as shown in the previous section for unary terms, while the third term is represented by an edge <math>v_i \\rightarrow v_j</math> with weight <math>w_{ij}(0, 1) + w_{ij}(1, 0) - w_{ij}(0, 0) - w_{ij}(1, 1)</math> (submodularity guarantees that the weight is non-negative).<ref name=\"what\" />\n\n=== Ternary terms ===\n\nA cubic (or ternary) term <math>w_{ijk}</math> can be represented by a graph with four non-terminal nodes, three of them (<math>v_i</math>,  <math>v_j</math> and <math>v_k</math>) associated to the three variables plus one fourth auxiliary node <math>v_{ijk}</math>.<ref name=\"fn auxiliary node\" group=\"note\" /> A generic ternary term can be rewritten as the sum of a constant, three unary terms, three binary terms, and a ternary term in simplified form. There may be two different cases, according to the sign of <math>p = w_{ijk}(0, 0, 0) + w_{ijk}(0, 1, 1) + w_{ijk}(1, 0, 1) + w_{ijk}(1, 1, 0)</math>. If <math>p > 0</math> then\n\n:<math>\n    w_{ijk}(x_i, x_j, x_k) =\n          w_{ijk}(0, 0, 0)\n        + p_1 (x_i - 1) + p_2 (x_j - 1) + p_3 (x_k - 1)\n        + p_{23}(x_j - 1) x_k + p_{31} x_i (x_k - 1) + p_{12} (x_i - 1) x_j\n        - p x_i x_j x_k\n</math>\n\n[[File:Graph cut ternary.svg|thumb|upright=2|Example of a graph representing the ternary term <math>p x_i x_j x_k</math> when <math>p > 0</math> (left) and when <math>p < 0</math> (right).]]\nwith\n\n:<math>\n\\begin{align}\n    p_1    &= w_{ijk}(1, 0, 1) - w_{ijk}(0, 0, 1) \\\\\n    p_2    &= w_{ijk}(1, 1, 0) - w_{ijk}(1, 0, 1) \\\\\n    p_3    &= w_{ijk}(0, 1, 1) - w_{ijk}(0, 1, 0) \\\\\n    p_{23} &= w_{ijk}(0, 0, 1) + w_{ijk}(0, 1, 0) - w_{ijk}(0, 0, 0) - w_{ijk}(0, 1, 1) \\\\\n    p_{31} &= w_{ijk}(0, 0, 1) + w_{ijk}(1, 0, 0) - w_{ijk}(0, 0, 0) - w_{ijk}(1, 0, 1) \\\\\n    p_{12} &= w_{ijk}(0, 1, 0) + w_{ijk}(1, 0, 0) - w_{ijk}(0, 0, 0) - w_{ijk}(1, 1, 0) .\n\\end{align}\n</math>\n\nIf <math>p < 0</math> the construction is similarly, but the variables will have opposite value. If the function is regular, then all its projections of two variables will be submodular, implying that <math>p_{23}</math>, <math>p_{31}</math> and <math>p_{12}</math> are positive and then all terms in the new representation are submodular.\n\nIn this decomposition, the constant, unary and binary terms can be represented as shown in the previous sections. If <math>p > 0</math> the ternary term can be represented with a graph with four edges <math>v_i \\rightarrow v_{ijk}</math>, <math>v_j \\rightarrow v_{ijk}</math>, <math>v_k \\rightarrow v_{ijk}</math>, <math>v_{ijk} \\rightarrow t</math>, all with weight <math>p</math>, while if <math>p < 0</math> the term can be represented by four edges <math>v_{ijk} \\rightarrow v_i</math>, <math>v_{ijk} \\rightarrow v_j</math>, <math>v_{ijk} \\rightarrow v_k</math>, <math>s \\rightarrow v_{ijk}</math> with weight <math>-p</math>.<ref name=\"what\" />\n\n== Minimum cut ==\n\nAfter building a graph representing a pseudo-Boolean function, it is possible to compute a minimum cut using one among the various algorithms developed for flow networks, such as [[Ford–Fulkerson algorithm|Ford–Fulkerson]], [[Edmonds–Karp algorithm|Edmonds–Karp]], and [[Boykov–Kolmogorov algorithm]]. The result is a partition of the graph in two connected components <math>S</math> and <math>T</math> such that <math>s \\in S</math> and <math>t \\in T</math>, and the function attains its global minimum when <math>x_i = 0</math> for each <math>i</math> such that the corresponding node <math>v_i \\in\nS</math>, and <math>x_i = 1</math> for each <math>i</math> such that the corresponding node <math>v_i \\in T</math>.\n\nMax-flow algorithms such as Boykov&ndash;Kolmogorov's are very efficient in practice for sequential computation, but they are difficult to parallelise, making them not suitable for [[distributed computing]] applications and preventing them from exploiting the potential of modern [[Central Processing Unit|CPU]]s. Parallel max-flow algorithms were developed, such as [[Push–relabel maximum flow algorithm|push-relabel]]<ref name=\"goldberg\" /> and [[jump-flood algorithm|jump-flood]],<ref name=\"peng\" /> that can also take advantage of hardware acceleration in [[GPGPU]] implementations.<ref name=\"cudacuts\" /><ref name=\"peng\" /><ref name=\"stich\" />\n\n== Functions of discrete variables with more than two values ==\n\nThe previous construction allows global optimization of pseudo-Boolean functions only, but it can be extended to quadratic functions of discrete variables with a finite number of values, in the form\n\n:<math>f(\\mathbf{x}) = \\sum_{i \\in V} D(x_i) + \\sum_{(i, j) \\in E} S(x_i, x_j)</math>\n\nwhere <math>E \\subseteq V \\times V</math> and <math>x_i \\in \\Lambda = \\{1, \\dots, k\\}</math>. The function <math>D(x_i)</math> represents the unary contribution of each variable (often referred as ''data term''), while the function <math>S(x_i, x_j)</math> represents binary interactions between variables (''smoothness term''). In the general case, optimization of such functions is a [[NP-hard]] problem, and [[stochastic optimization]] methods such as [[simulated annealing]] are sensitive to [[local minima]] and in practice they can generate arbitrarily sub-optimal results.<ref name=\"annealing\" group=\"note\" /> With graph cuts it is possible to construct move-making algorithms that allow to reach in polynomial time a local minima with strong optimality properties for a wide family of quadratic functions of practical interest (when the binary interaction <math>S(x_i, x_j)</math> is a [[metric (mathematics)|metric]] or a [[semimetric]]), such that the value of the function in the solution lies within a constant and known factor from the global optimum.<ref name=\"fast\" />\n\nGiven a function <math>f: \\Lambda^n \\to \\mathbb{R}</math> with <math>\\Lambda = \\{1, \\dots, k\\}</math>, and a certain assignment of values <math>\\mathbf{x} = (x_1, \\dots, x_n) \\in \\Lambda^n</math> to the variables, it is possible to associate each assignment <math>\\mathbf{x}</math> to a partition <math>P = \\{P_l | l \\in \\Lambda \\}</math> of the set of variables, such that, <math>P_l = \\{ x_i | x_i = l \\in \\Lambda \\}</math>. Give two distinct assignments <math>P</math> and <math>P'</math> and a value <math>\\alpha \\in \\Lambda</math>, a move that transforms <math>P</math> into <math>P'</math> is said to be an <math>\\alpha</math>-expansion if <math>P_\\alpha \\subset P'_\\alpha</math> and <math>P'_l \\subset P_l \\; \\forall l \\in \\Lambda - \\{ \\alpha \\}</math>. Given a couple of values <math>\\alpha</math> and <math>\\beta</math>, a move is said to be an <math>\\alpha\\beta</math>-swap if <math>P_l = P'_l \\; \\forall l \\in \\Lambda - \\{ \\alpha, \\beta \\}</math>. Intuitively, an <math>\\alpha</math>-expansion move from <math>\\mathbf{x}</math> assigns the value of <math>\\alpha</math> to some variables that have a different value in <math>\\mathbf{x}</math>, while an <math>\\alpha\\beta</math>-swap move assigns <math>\\alpha</math> to some variables that have value <math>\\beta</math> in <math>\\mathbf{x}</math> and vice versa.\n\nFor each iteration, the <math>\\alpha</math>-expansion algorithm computes, for each possible value <math>\\alpha</math>, the minimum of the function among all assignments <math>\\Alpha(\\mathbf{x})</math> that can be reached with a single <math>\\alpha</math>-expansion move from the current temporary solution <math>\\mathbf{x}</math>, and takes it as the new temporary solution.\n\n <math>\\mathbf{x} := \\text{arbitrary value in } \\Lambda^n</math>\n <math>\\text{exit} := 0</math>\n '''while''' <math>\\text{exit} \\ne 1</math>:\n     <math>\\text{exit} = 1</math>\n     '''foreach''' <math>\\alpha \\in \\Lambda</math>:\n         <math>\\mathbf{\\hat{x}} := \\arg \\min_{\\mathbf{y} \\in \\Alpha(\\mathbf{x})} f(\\mathbf{y})</math>\n         '''if''' <math>f(\\mathbf{\\hat{x}}) < \\mathbf{x}</math>:\n             <math>\\mathbf{x} = \\mathbf{\\hat{x}}</math>\n             <math>\\text{exit} := 0</math>\n\nThe <math>\\alpha\\beta</math>-swap algorithm is similar, but it searches for the minimum among all assignments <math>\\Alpha\\Beta(\\mathbf{x})</math> reachable with a single <math>\\alpha\\beta</math>-swap move from <math>\\mathbf{x}</math>.\n\n <math>\\mathbf{x} := \\text{arbitrary value in } \\Lambda^n</math>\n <math>\\text{exit} := 0</math>\n '''while''' <math>\\text{exit} \\ne 1</math>:\n     <math>\\text{exit} = 1</math>\n     '''foreach''' <math>(\\alpha, \\beta) \\in \\Lambda^2</math>:\n         <math>\\mathbf{\\hat{x}} := \\arg \\min_{\\mathbf{y} \\in \\Alpha\\Beta(\\mathbf{x})} f(\\mathbf{y})</math>\n         '''if''' <math>f(\\mathbf{\\hat{x}}) < \\mathbf{x}</math>:\n             <math>\\mathbf{x} = \\mathbf{\\hat{x}}</math>\n             <math>\\text{exit} := 0</math>\n\nIn both cases, the optimization problem in the innermost loop can be solved exactly and efficiently with a graph cut. Both algorithms terminate certainly in a finite number of iterations of the outer loop, and in practice such number is small, with most of the improvement happening at the first iteration. The algorithms can generate different solutions depending on the initial guess, but in practice they are robust with respect to initialisation, and starting with a point where all variables are assigned to the same random value is usually sufficient to produce good quality results.<ref name=\"fast\" />\n\nThe solution generated by such algorithms is not necessarily a global optimum, but it has strong guarantees of optimality. If <math>S(x_i, x_j)</math> is a [[metric (mathematics)|metric]] and <math>\\mathbf{x}</math> is a solution generated by the <math>\\alpha</math>-expansion algorithm, or if <math>S(x_i, x_j)</math> is a [[semimetric]] and <math>\\mathbf{x}</math> is a solution generated by the <math>\\alpha\\beta</math>-swap algorithm, then <math>f(\\mathbf{x})</math> lies within a known and constant factor from the global minimum <math>f(\\mathbf{x}^*)</math>:<ref name=\"fast\" />\n\n:<math>f(\\mathbf{x}) \\le 2 \\frac{ \\max_{\\alpha \\ne \\beta \\in \\Lambda} S(\\alpha, \\beta) }{ \\min_{\\alpha \\ne \\beta \\in \\Lambda} S(\\alpha, \\beta) } f(\\mathbf{x}^*) . </math>\n\n== Non-submodular functions ==\n{{see also|Quadratic pseudo-Boolean optimization}}\n\nGenerally speaking, the problem of optimizing a non-submodular pseudo-Boolean function is [[NP-hard]] and cannot be solved in polynomial time with a simple graph cut. The simplest approach is to approximate the function with a similar but submodular one, for instance truncating all non-submodular terms or replacing them with similar submodular expressions. Such approach is generally sub-optimal, and it produces acceptable results only if the number of non-submodular terms is relatively small.<ref name=\"review\" />\n\nIn case of quadratic non-submodular functions, it is possible to compute in polynomial time a partial solution using algorithms such as [[QPBO]].<ref name=\"review\" /> Higher-order functions can be reduced in polynomial time to a quadratic form that can be optimised with QPBO.<ref name=\"elc\" />\n\n== Higher-order functions ==\n\nQuadratic functions are extensively studied and were characterised in detail, but more general results were derived also for higher-order functions. While quadratic functions can indeed model many problems of practical interest, they are limited by the fact they can represent only binary interactions between variables. The possibility to capture higher-order interactions allows to better capture the nature of the problem and it can provide higher quality results that could be difficult to achieve with quadratic models. For instance in [[computer vision]] applications, where each variable represents a [[pixel]] or [[voxel]] of the image, higher-order interactions can be used to model texture information, that would be difficult to capture using only quadratic functions.<ref name=\"p3\" />\n\nSufficient conditions analogous to submodularity were developed to characterise higher-order pseudo-Boolean functions that can be optimised in polynomial time,<ref name=\"freedman\" /> and there exists algorithms analogous to <math>\\alpha</math>-expansion and <math>\\alpha\\beta</math>-swap for some families of higher-order functions.<ref name=\"p3\" /> The problem is NP-hard in the general case, and approximate methods were developed for fast optimization of functions that do not satisfy such conditions.<ref name=\"freedman\" /><ref name=\"kohli\" />\n\n== References ==\n<references>\n<ref name=\"cudacuts\">Vineet and Narayanan (2008).</ref>\n<ref name=\"elc\">Ishikawa (2014).</ref>\n<ref name=\"fast\">Boykov et al. (2001).</ref>\n<ref name=\"freedman\">Freedman & Drineas (2005).</ref>\n<ref name=\"goldberg\">Goldberg & Tarjan (1988).</ref>\n<ref name=\"grabcut\">Rother et al. (2012).</ref>\n<ref name=\"hong\">Hong and Chen (2004).</ref>\n<ref name=\"kim\">Kim et al. (2003).</ref>\n<ref name=\"kohli\">Kohli et al. (2008).</ref>\n<ref name=\"lombaert\">Lombaert and Cheriet (2012).</ref>\n<ref name=\"p3\">Kohli et al. (2009).</ref>\n<ref name=\"peng\">Peng et al. (2015).</ref>\n<ref name=\"review\">Kolmogorov and Rother (2007).</ref>\n<ref name=\"so\">So et al. (2011).</ref>\n<ref name=\"stich\">Stich (2009).</ref>\n<ref name=\"tang\">Tang and Chung (2007).</ref>\n<ref name=\"what\">Kolmogorov and Zabin (2004).</ref>\n</references>\n\n* {{cite journal|title=Fast approximate energy minimization via graph cuts|year=2001|journal=IEEE Transactions on Pattern Analysis and Machine Intelligence|volume=23|issue=11|last1=Boykov|first1=Yuri|last2=Veksler|first2=Olga|last3=Zabih|first3=Ramin|pages=1222–1239}}\n* {{cite conference|title=Energy minimization via graph cuts: Settling what is possible|conference=IEEE Computer Society Conference on Computer Vision and Pattern Recognition|year=2005|volume=2|last1=Freedman|first1=Daniel|last2=Drineas|first2=Petros|pages=939–946}}\n* {{cite journal|title=A new approach to the maximum–flow problem|year=1988|journal=Journal of the ACM|volume=35|pages=921–940|issue=4|last1=Goldberg|first1=Andrew V|last2=Tarjan|first2=Robert E}}\n* {{cite conference|first=Hiroshi|last=Ishikawa|title=Higher–Order Clique Reduction Without Auxiliary Variables|conference=IEEE Conference on Computer Vision and Pattern Recognition|year=2014|publisher=IEEE|pages=1362–1369}}\n* {{cite conference|title=Segment-based stereo matching using graph cuts|conference=Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition|year=2004|volume=1|last1=Hong|first1=Li|last2=Chen|first2=George|pages=74–81}}\n* {{cite journal|first1=Pushmeet|last1=Kohli|first2=M. Pawan|last2=Kumar|first3=Philip H.S.|last3=Torr|title=P<sup>3</sup> & Beyond: Move Making Algorithms for Solving Higher Order Functions|journal=IEEE Transactions on Pattern Analysis and Machine Intelligence|volume=31|number=9|year=2009|pages=1645–1656}}\n* {{cite conference|title=Visual correspondence using energy minimization and mutual information|conference=Proceedings of the Ninth IEEE International Conference on Computer Vision|year=2003|last1=Kim|first1=Junhwan|last2=Kolmogorov|first2=Vladimir|last3=Zabih|first3=Ramin|pages=1033–1040}}\n* {{cite techreport|title=Graph cuts for minimizing robust higher order potentials|year=2008|institution=Oxford Brookes University|last1=Kohli|first1=Pushmeet|last2=Ladicky|first2=Lubor|last3=Torr|first3=PHS|pages=1–9}}\n* {{cite journal|first1=Vladimir|last1=Kolmogorov|first2=Carsten|last2=Rother|title=Minimizing Nonsubmodular Functions: A Review|journal=IEEE Transactions on Pattern Analysis and Machine Intelligence|volume=29|number=7|year=2007|pages=1274–1279}}\n* {{cite journal|first1=Vladimir|last1=Kolmogorov|first2=Ramin|last2=Zabin|title=What energy functions can be minimized via graph cuts?|journal=IEEE Transactions on Pattern Analysis and Machine Intelligence|volume=26|number=2|year=2004|pages=1645–1656}}\n* {{cite conference|title=Simultaneous image de-noising and registration using graph cuts: Application to corrupted medical images|conference=11th International Conference on Information Science, Signal Processing and their Applications|year=2012|last1=Lombaert|first1=Herve|last2=Cheriet|first2=Farida|pages=264–268}}\n* {{cite journal|title=JF-Cut: a parallel graph cut approach for large-scale image and video|year=2015|journal=IEEE Transactions on Image Processing|volume=24|pages=655–666|issue=2|last1=Peng|first1=Yi|last2=Chen|first2=Li|last3=Ou-Yang|first3=Fang-Xin|last4=Chen|first4=Wei|last5=Yong|first5=Jun-Hai|bibcode=2015ITIP...24..655P|doi=10.1109/TIP.2014.2378060}}\n* {{cite conference|title=Grabcut: Interactive foreground extraction using iterated graph cuts|conference=ACM transactions on graphics|year=2004|volume=23|last1=Rother|first1=Carsten|last2=Kolmogorov|first2=Vladimir|last3=Blake|first3=Andrew|pages=309–314}}\n* {{cite journal|title=Non-rigid image registration of brain magnetic resonance images using graph-cuts|year=2011|journal=Pattern Recognition|volume=44|issue=10–11|last1=So|first1=Ronald WK|last2=Tang|first2=Tommy WH|last3=Chung|first3=Albert CS|pages=2450–2467}}\n* {{cite conference|title=Graph Cuts with CUDA|first1=Timo|last1=Stich|conference=GPU Technology Conference|year=2009|url=https://www.nvidia.com/content/gtc/documents/1060_gtc09.pdf}}\n* {{cite conference|title=Non-rigid image registration using graph-cuts|conference=International Conference on Medical Image Computing and Computer-Assisted Intervention|year=2007|last1=Tang|first1=Tommy WH|last2=Chung|first2=Albert CS|pages=916–924}}\n* {{cite conference|title=CUDA cuts: Fast graph cuts on the GPU|conference=IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops|year=2008|last1=Vineet|first1=Vibhav|last2=Narayanan|first2=PJ|pages=1–8}}\n\n== Notes ==\n<references group=\"note\">\n<ref name=\"annealing\">Algorithms such as [[simulated annealing]] have strong theoretical convergence properties for some scheduling of the temperature to infinity. Such scheduling cannot be realised in practice.</ref>\n<ref name=\"fn auxiliary node\">Adding one node is necessary, graphs without auxiliary nodes can only represent binary interactions between variables.</ref>\n</references>\n\n== External links ==\n*[http://pub.ist.ac.at/~vnk/software.html Implementation (C++) of several graph cut algorithms] by Vladimir Kolmogorov.\n*[https://github.com/nsubtil/gco-v3.0 GCO], graph cut optimization library by Olga Veksler and Andrew Delong.\n\n{{DEFAULTSORT:Graph cut optimization}}\n[[Category:Combinatorial optimization]]\n[[Category:Computer vision]]\n[[Category:Computational problems in graph theory]]"
    },
    {
      "title": "Greedoid",
      "url": "https://en.wikipedia.org/wiki/Greedoid",
      "text": "In [[combinatorics]], a '''greedoid''' is a type of [[set system]]. It arises from the notion of the [[matroid]], which was originally introduced by [[Hassler Whitney|Whitney]] in 1935 to study [[planar graph]]s and was later used by [[Jack Edmonds|Edmonds]] to characterize a class of optimization problems that can be solved by [[greedy algorithm]]s. Around 1980, [[Bernhard Korte|Korte]] and [[László Lovász|Lovász]] introduced the greedoid to further generalize this characterization of greedy algorithms; hence the name greedoid. Besides [[mathematical optimization]], greedoids have also been connected to [[graph theory]], language theory, [[poset]] theory, and other [[areas of mathematics]].\n\n==Definitions==\nA '''set system''' (''F'', E) is a collection ''F'' of [[subset]]s of a ground set E (i.e. ''F'' is a subset of the [[power set]] of E). When considering a greedoid, a member of ''F'' is called a '''feasible set'''. When considering a [[matroid]], a feasible set is also known as an ''independent set''.\n\nAn '''accessible set system''' (''F'', E) is a set system in which every nonempty feasible set X contains an element x such that X\\{x} is feasible. This implies that any nonempty, [[finite set|finite]], accessible set system necessarily contains the [[empty set]] ∅.<ref>Note that the accessibility property is strictly weaker than the ''hereditary property'' of a [[matroid]], which requires that ''every'' subset of an independent set be independent.</ref>\n\nA '''greedoid''' (''F'', E) is an accessible set system that satisfies the ''exchange property'':\n\n* for all X,Y ∈ ''F'' with |X| > |Y|, there is some x ∈ X\\Y such that Y∪{x} ∈ ''F''\n\n(Note:  Some people reserve the term ''exchange property'' for a condition on the bases of a greedoid, and prefer to call the above condition the “Augmentation Property”.)\n\nA '''basis''' of a greedoid is a maximal feasible set, meaning it is a feasible set but not contained in any other one.  A basis of a subset X of E is a maximal feasible set contained in X.\n\nThe '''rank''' of a greedoid is the size of a basis.  \nBy the exchange property, all bases have the same size.\nThus, the rank function is [[well defined]].  The rank of a subset X of E is the size of a basis of X. Just as with matroids, greedoids have a [[cryptomorphism]] in terms of rank functions.<ref>{{Citation|last1=Björner|first1=Anders|last2=Ziegler|first2=Günter M.|authorlink2=Günter M. Ziegler|authorlink1=Anders Björner|chapter=8. Introduction to greedoids|series=Encyclopedia of Mathematics and its Applications|volume=40|editor-last=White|editor-first=Neil|publisher=Cambridge University Press|location=Cambridge|year=1992|isbn=0-521-38165-7|pages=284–357|doi=10.1017/CBO9780511662041.009|ref=harv|mr=1165537| zbl=0772.05026 |title=Matroid Applications}}\n</ref>\nA function <math>r:2^E \\to \\mathbb{Z}</math> is the rank function of a greedoid on the ground set E if and only if <math>r</math> is subcardinal, monotonic, and locally semimodular, that is, for any <math>X,Y \\subseteq E</math> and any <math>e,f \\in E</math> we have\n* '''subcardinality''': <math>r(X)\\le|X|</math>;\n* '''monotonic''': <math>r(X)\\le r(Y)</math> whenever <math>X \\subseteq Y \\subseteq E</math>; and\n*'''locally semimodularity''': <math>r(X) = r(X\\cup\\{e,f\\})</math> whenever <math>r(X) = r(X \\cup \\{e\\}) = r(X \\cup \\{f\\})</math>.\n\n==Classes==\nMost classes of greedoids have many equivalent definitions in terms of set system, language, poset, [[simplicial complex]], and so on. The following description takes the traditional route of listing only a couple of the more well-known characterizations.\n\nAn '''interval greedoid''' (''F'', E) is a greedoid that satisfies the ''Interval Property'':\n\n* if A, B, C ∈ ''F'' with A ⊆ B ⊆ C, then, for all x ∈ E\\C,  (A∪{x} ∈ ''F'' and C∪{x} ∈ ''F'') implies B∪{x} ∈ ''F''\n\nEquivalently, an interval greedoid is a greedoid such that the union of any two feasible sets is feasible if it is contained in another feasible set.\n\nAn '''[[antimatroid]]''' (''F'', E) is a greedoid that satisfies the ''Interval Property without Upper Bounds'':\n\n* if A, B ∈ ''F'' with A ⊆ B, then, for all x ∈ E\\B, A∪{x} ∈ ''F'' implies B∪{x} ∈ ''F''\n\nEquivalently, an antimatroid is (i) a greedoid with a unique basis; or (ii) an accessible set system closed under union.  It is easy to see that an antimatroid is also an interval greedoid.\n\nA '''[[matroid]]''' (''F'', E) is a non-empty greedoid that satisfies the ''Interval Property without Lower Bounds'':\n\n* if B, C ∈ ''F'' with B ⊆ C, then, for all x ∈ E\\C, C∪{x} ∈ ''F'' implies B∪{x} ∈ ''F''\n\nIt is easy to see that a matroid is also an interval greedoid.\n\n==Examples==\n*Consider an undirected [[Graph (discrete mathematics)|graph]] G.  Let the ground set be the edges of G and the feasible sets be the edge set of each ''forest'' (i.e. subgraph containing no cycle) of G. This set system is called the '''cycle matroid'''. A set system is said to be a [[graphic matroid]] if it is the cycle matroid of some graph. (Originally cycle matroid was defined on '''circuits''', or minimal ''dependent sets''. Hence the name cycle.)\n*Consider a finite, undirected graph G [[rooted graph|rooted]] at the vertex r. Let the ground set be the vertices of G and the feasible sets be the vertex subsets containing r that induce connected subgraphs of G. This is called the '''vertex search greedoid''' and is a kind of antimatroid.\n*Consider a finite, [[directed graph]] D rooted at r.  Let the ground set be the (directed) edges of D and the feasible sets be the edge sets of each directed subtree rooted at r with all edges pointing away from r.  This is called the '''line search greedoid''', or '''directed branching greedoid'''.  It is an interval greedoid, but neither an antimatroid nor a matroid.\n*Consider an m-by-n [[Matrix (mathematics)|matrix]] M.  Let the ground set E be the indices of the columns from 1 to n and the feasible sets be ''F'' = {X ⊆ E: submatrix M<sub>{1,...,|X<nowiki>|}</nowiki>,X</sub> is an [[invertible matrix]]}. This is called the '''Gaussian elimination greedoid''' because this structure underlies the [[Gaussian elimination]] algorithm.  It is a greedoid, but not an interval greedoid.\n\n== Greedy algorithm ==\n\nIn general, a [[greedy algorithm]] is just an iterative process in which a ''locally best choice'', usually an input of minimum weight, is chosen each round until all available choices have been exhausted.\nIn order to describe a greedoid-based condition in which a greedy algorithm is optimal, we need some more common terminologies in greedoid theory.\n[[Without loss of generality]], we consider a greedoid G = (''F'', E) with E finite.\n\nA subset X of E is '''rank feasible''' if the largest intersection of X with any feasible set has size equal to the rank of X.\nIn a matroid, every subset of E is rank feasible.\nBut the equality does not hold for greedoids in general.\n\nA function w: E → ℝ is '''''R''-compatible''' if {x ∈ E: w(x) ≥ c} is rank feasible for all [[real number]]s c.\n\nAn objective function f: 2<sup>S</sup> → ℝ is '''linear''' over a set S if, for all X ⊆ S, we have f(X) = Σ<sub>x ∈ X</sub> w(x) for some [[weight function]] w: S → ℜ.\n\n'''Proposition.'''  A greedy algorithm is optimal for every '''''R'''''-compatible linear objective function over a greedoid.\n\nThe intuition behind this proposition is that, during the iterative process, each optimal exchange of minimum weight is made possible by the exchange property, and optimal results are obtainable from the feasible sets in the underlying greedoid. This result guarantees the optimality of many well-known algorithms. For example, a [[minimum spanning tree]] of a [[weighted graph]] may be obtained using [[Kruskal's algorithm]], which is a greedy algorithm for the cycle matroid. [[Prim's algorithm]] can be explained by taking the vertex search greedoid instead.\n\n== See also ==\n* [[Matroid]]\n* [[Polymatroid]]\n\n== References ==\n{{Reflist}}\n* {{Citation|last1=Björner|first1=Anders|last2=Ziegler|first2=Günter M.|authorlink2=Günter M. Ziegler|authorlink1=Anders Björner|chapter=8. Introduction to greedoids|series=Encyclopedia of Mathematics and its Applications|volume=40|editor-last=White|editor-first=Neil|publisher=Cambridge University Press|location=Cambridge|year=1992|isbn=0-521-38165-7|pages=284–357|doi=10.1017/CBO9780511662041.009|ref=harv|mr=1165537| zbl=0772.05026 |title=Matroid Applications}}\n*{{citation | last=Edmonds | first=Jack | authorlink=Jack Edmonds | year=1971 | title=Matroids and the greedy algorithm | journal=Mathematical Programming | volume=1 | pages=127–136| doi=10.1007/BF01584082 | zbl=0253.90027 }}.\n*{{citation | last1=Helman | first1=Paul | last2=Moret | first2=Bernard M. E. | last3=Shapiro | first3=Henry D. | year=1993 | title=An exact characterization of greedy structures | journal=[[SIAM Journal on Discrete Mathematics]] | volume=6 | issue=2 | pages=274–283 | doi=10.1137/0406021 | zbl=0798.68061 | citeseerx=10.1.1.37.1825 }}.\n*{{citation | last1=Korte | first1=Bernhard | last2=Lovász | first2=László  | authorlink1=Bernhard Korte | authorlink2=László Lovász | contribution=Mathematical structures underlying greedy algorithms | editor-last=Gecseg | editor-first=Ferenc | title=Fundamentals of Computation Theory: Proceedings of the 1981 International FCT-Conference, Szeged, Hungaria, August 24–28, 1981 | series=Lecture Notes in Computer Science | volume=117 | pages=205–209 | location=Berlin | publisher=[[Springer-Verlag]] | year=1981 |  doi=10.1007/3-540-10854-8_22 | zbl=0473.68019 }}.\n*{{citation | last1=Korte | first1=Bernhard | last2=Lovász | first2=László | authorlink2=László Lovász | last3=Schrader | first3=Rainer | year=1991 | title=Greedoids | location=New York, Berlin | publisher=[[Springer-Verlag]] | zbl=0733.05023 | series=Algorithms and Combinatorics | volume=4 | isbn=3-540-18190-3 }}.\n*{{citation | last=Oxley | first=James G. | authorlink = James Oxley | title=Matroid theory | series=Oxford Science Publications | location=Oxford | publisher=[[Oxford University Press]] | year=1992 | isbn=0-19-853563-5 | zbl=0784.05002 }}.\n*{{citation | last=Whitney | first=Hassler | authorlink=Hassler Whitney | year=1935 | title=On the abstract properties of linear independence | journal=[[American Journal of Mathematics]] | volume=57 | issue=3 | pages=509–533 | doi=10.2307/2371182 | jstor=2371182 | zbl=0012.00404 }}.\n\n== External links ==\n* [https://www.mi.fu-berlin.de/math/groups/discgeom/ziegler/Preprintfiles/006PREPRINT.pdf?1397057423 Introduction to Greedoids]\n* [https://parasol.tamu.edu/~welch/teaching/411.f08/greedy.pdf Theory of Greedy Algorithms]\n* [http://www.kurims.kyoto-u.ac.jp/~fujishig/Book1a.html Submodular Functions and Optimization]\n* [http://people.csail.mit.edu/nickh/PhDThesis.pdf Matchings, Matroids and Submodular Functions]\n\n[[Category:Order theory]]\n[[Category:Combinatorial optimization]]\n[[Category:Set families]]"
    },
    {
      "title": "Greedy randomized adaptive search procedure",
      "url": "https://en.wikipedia.org/wiki/Greedy_randomized_adaptive_search_procedure",
      "text": "The '''greedy randomized adaptive search procedure''' (also known as '''GRASP''') is a [[metaheuristic]] algorithm commonly applied to [[combinatorial optimization]] problems.  GRASP typically consists of iterations made up from successive constructions of a ''[[Greedy algorithm|greedy]] [[Randomized algorithm|randomized]]'' solution and subsequent iterative improvements of it through a [[Local search (optimization)|local search]].  The greedy randomized solutions are generated by adding elements to the problem's solution set from a list of elements ranked by a ''greedy function'' according to the quality of the solution they will achieve.  To obtain variability in the candidate set of greedy solutions, well-ranked candidate elements are often placed in a ''restricted candidate list'' (also known as '''RCL'''), and chosen at random when building up the solution. This kind of greedy randomized construction method is also known as a '''semi-greedy heuristic''', first described in Hart and Shogan (1987).\n\nGRASP was first introduced in Feo and Resende (1989). Survey papers on GRASP include Feo and Resende (1995), Pitsoulis and Resende (2002), and Resende and Ribeiro (2003).  An annotated bibliography of GRASP can be found in Festa, G. C  Resende (2002).\n\n==References==\n{{refbegin}}\n* J. P. Hart and A. W. Shogan (1987) Semi-greedy heuristics: An empirical study. ''Operations Research Letters'', 6:107&ndash;114, 1987.\n* T. A. Feo and M. G. C. Resende (1989) A probabilistic heuristic for a computationally difficult set covering problem. ''Operations Research Letters'', 8:67&ndash;71, 1989.\n* T. A. Feo and M. G. C. Resende (1995) Greedy randomized adaptive search procedures. ''Journal of Global Optimization'', 6:109&ndash;133, 1995.\n* L. Pitsoulis and M. G. C. Resende (2002) [http://www.research.att.com/~mgcr/doc/grasp-hao.pdf Greedy randomized adaptive search procedures]. In P. M. Pardalos and M. G. C. Resende, editors, ''Handbook of Applied Optimization'', pp.&nbsp;168&ndash;181, Oxford University Press.{{Dl|date=January 2018}}\n* M. G. C. Resende and C. C. Ribeiro (2003) [http://www.optimization-online.org/DB_FILE/2001/09/371.pdf Greedy randomized adaptive search procedures]. In F. Glover and G. Kochenberger, editors, ''Handbook of Metaheuristics'', pp.&nbsp;219&ndash;249, Kluwer Academic Publishers, 2003.\n* P. Festa and M. G. C. Resende (2002) [http://www.research.att.com/~mgcr/doc/gabib.pdf GRASP: An annotated bibliography]. In C. C. Ribeiro and P. Hansen, editors, ''Essays and Surveys on Metaheuristics'', pp.&nbsp;325&ndash;367, Kluwer Academic Publishers, 2002.{{Dl|date=January 2018}}\n\n{{refend}}\n\n== See also ==\n<!-- Please don't add a whole list of optimization algorithms, the categories serve that purpose. -->\n<!-- 27/3/11 Updated with more meaningful category organisation - I have tried to keep the list as brief as possible, relying on sub-pages for further breakdown of categories. Please try to maintain this spirit on edits. -->\n\n* [[Metaheuristic]]\n* [[Local search (optimization)]]\n* [[Constructive cooperative coevolution]]\n* [[Cooperative coevolution]]\n* [[Simulated annealing]]\n* [[Tabu search]]\n\n{{DEFAULTSORT:Greedy Randomized Adaptive Search Procedure}}\n[[Category:Combinatorial optimization]]\n\n\n{{combin-stub}}"
    },
    {
      "title": "Hungarian algorithm",
      "url": "https://en.wikipedia.org/wiki/Hungarian_algorithm",
      "text": "The '''Hungarian method''' is a [[combinatorial optimization]] [[algorithm]] that solves the [[assignment problem]] in [[polynomial time]] and which anticipated later [[Duality (optimization)|primal-dual methods]]. It was developed and published in 1955 by [[Harold Kuhn]], who gave the name \"Hungarian method\" because the algorithm was largely based on the earlier works of two [[Hungary|Hungarian]] mathematicians: [[Dénes Kőnig]] and [[Jenő Egerváry]].<ref name=\"kuhn1955\">Harold W. Kuhn, \"The Hungarian Method for the assignment problem\", ''[[Naval Research Logistics Quarterly]]'', '''2''': 83–97, 1955.  Kuhn's original publication.</ref><ref name=\"kuhn1956\">Harold W. Kuhn, \"Variants of the Hungarian method for assignment problems\", ''Naval Research Logistics Quarterly'', '''3''': 253–258, 1956.</ref>\n\n[[James Munkres]] reviewed the algorithm in 1957 and observed that it is [[Time complexity#Strongly and weakly polynomial time|(strongly) polynomial]].<ref name=\"munkres\">J. Munkres, \"Algorithms for the Assignment and Transportation Problems\", ''[[Journal of the Society for Industrial and Applied Mathematics]]'', '''5'''(1):32–38, 1957 March.</ref> Since then the algorithm has been known also as the '''Kuhn–Munkres algorithm''' or '''Munkres assignment algorithm'''. The [[Computational complexity theory#Time and space complexity|time complexity]] of the original algorithm was <math>O(n^4)</math>, however [[Jack Edmonds|Edmonds]] and [[Richard Karp|Karp]], and independently Tomizawa noticed that it can be modified to achieve an <math>O(n^3)</math> running time. One of the most popular <math>O(n^3)</math> variants is the Jonker-Volgenant algorithm.<ref name=\"JVAlg\">{{cite journal |last1=Jonker |first1=R. |last2=Volgenant |first2=A. |title=A shortest augmenting path algorithm for dense and sparse linear assignment problems |journal=Computing |date=December 1987 |volume=38 |issue=4 |pages=325–340 |doi=10.1007/BF02278710}}</ref> [[L. R. Ford, Jr.|Ford]] and [[D. R. Fulkerson|Fulkerson]] extended the method to general transportation problems.{{Citation needed|date=November 2017}} In 2006, it was discovered that [[Carl Gustav Jacobi]] had solved the assignment problem in the 19th century, and the solution had been published posthumously in 1890 in Latin.<ref>http://www.lix.polytechnique.fr/~ollivier/JACOBI/jacobiEngl.htm</ref>.\n\n==The problem==\n{{Main|Assignment problem}}\n\n=== Example ===\nIn this simple example there are three workers: Armond, Francine, and Herbert. One of them has to clean the bathroom, another sweep the floors and the third washes the windows, but they each demand different pay for the various tasks. The problem is to find the lowest-cost way to assign the jobs. The problem can be represented in a [[Matrix (mathematics)|matrix]] of the costs of the workers doing the jobs. For example:\n{| class=\"wikitable\" border=\"1\"\n|-\n!\n! Clean bathroom\n! Sweep floors\n! Wash windows\n|-\n! Armond\n| $2\n| $3\n| $3\n|-\n! Francine\n| $3\n| $2\n| $3\n|-\n! Herbert\n| $3\n| $3\n| $2\n|}\n\nThe Hungarian method, when applied to the above table, would give the minimum cost: this is $6, achieved by having Armond clean the bathroom, Francine sweep the floors, and Herbert wash the windows.\n\n=== Matrix formulation ===\nThere are two ways to formulate the problem: as a ''matrix'' or as a ''bipartite graph''.\n\nIn the matrix formulation, we are given a nonnegative ''n''×''n'' [[Matrix (mathematics)|matrix]], where the element in the ''i''-th row and ''j''-th column represents the cost of assigning the ''j''-th job to the ''i''-th worker. We have to find an assignment of the jobs to the workers, such that each job is assigned to one worker and each worker is assigned one job, such that the total cost of assignment is minimum.\n\nIf the goal is to find the assignment that yields the ''maximum'' cost, the problem can be altered to fit the setting by replacing each cost with the maximum cost subtracted by the cost.\n\n=== Bigraph formulation ===\nThe algorithm is easier to describe if we formulate the problem using a bipartite graph. We have a [[complete bipartite graph]] <math>G=(S, T; E)</math> with <math>n</math> worker vertices (<math>S</math>) and <math>n</math> job vertices (<math>T</math>), and each edge has a nonnegative cost <math>c(i,j)</math>. We want to find a [[perfect matching]] with a minimum total cost.\n\n==The algorithm in terms of bipartite graphs==\nLet us call a function <math>y: (S\\cup T) \\to \\mathbb{R}</math> a '''potential''' if <math>y(i)+y(j) \\leq c(i, j)</math> for each <math>i \\in S, j \\in T</math>. The ''value'' of potential <math>y</math> is the sum of the potential over all vertices: <math>\\sum_{v\\in S\\cup T} y(v)</math>.\n\nIt is easy to see that the cost of each perfect matching is at least the value of each potential: the total cost of the matching is the sum of costs of all edges; the cost of each edge is at least the sum of potentials of its endpoints; since the matching is perfect, each vertex is an endpoint of exactly one edge; hence the total cost is at least the total potential.\n\nThe Hungarian method finds a perfect matching and a potential such that the matching cost equals the potential value. This proves that both of them are optimal. In fact, the Hungarian method finds a perfect matching of '''tight edges''': an edge <math>ij</math> is called tight for a potential <math>y</math> if <math>y(i)+y(j) = c(i, j)</math>. Let us denote the [[Glossary of graph theory#Subgraphs|subgraph]] of tight edges by <math>G_y</math>. The cost of a perfect matching in <math>G_y</math> (if there is one) equals the value of <math>y</math>.\n\nDuring the algorithm we maintain a potential  <math>y</math> and an [[Glossary of graph theory#orientation|orientation]] of <math>G_y</math> (denoted by <math>\\overrightarrow{G_y}</math>) which has the property that the edges oriented from ''T'' to ''S'' form a matching ''M''. Initially, ''y'' is 0 everywhere, and all edges are oriented from ''S'' to ''T'' (so ''M'' is empty). In each step, either we modify ''y'' so that its value increases, or modify the orientation to obtain a matching with more edges. We maintain the invariant that all the edges of ''M'' are tight.  We are done if ''M'' is a perfect matching.\n\nIn a general step, let <math>R_S \\subseteq S</math> and <math>R_T \\subseteq T</math> be the vertices not covered by ''M'' (so <math>R_S</math>consists of the vertices in ''S'' with no incoming edge and <math>R_T</math> consists of the vertices in ''T'' with no outgoing edge). Let <math>Z</math> be the set of vertices reachable in <math>\\overrightarrow{G_y}</math> from <math>R_S</math> by a directed path only following edges that are tight. This can be computed by [[breadth-first search]].\n\nIf <math>R_T \\cap Z</math> is nonempty, then reverse the orientation of a directed path in <math>\\overrightarrow{G_y}</math> from <math>R_S</math> to <math>R_T</math>. Thus the size of the corresponding matching increases by 1.\n\nIf <math>R_T \\cap Z</math> is empty, then let\n\n:<math>\\Delta := \\min \\{c(i,j)-y(i)-y(j): i \\in Z \\cap S, j \\in T \\setminus Z\\}.</math>\n\n<math>\\Delta</math> is positive because there are no tight edges between <math>Z \\cap S</math> and <math>T \\setminus Z</math>. Increase ''y'' by <math>\\Delta</math> on the vertices of <math>Z \\cap S</math> and decrease ''y'' by <math>\\Delta</math> on the vertices of <math>Z \\cap T</math>. The resulting ''y'' is still a potential, and although the graph <math>G_y</math> changes, it still contains ''M'' (see the next subsections). We orient the new edges from ''S'' to ''T''. By the definition of <math>\\Delta</math> the set ''Z'' of vertices reachable from <math>R_S</math> increases (note that the number of tight edges does not necessarily increase).\n\nWe repeat these steps until ''M'' is a perfect matching, in which case it gives a minimum cost assignment. The running time of this version of the method is <math>O(n^4)</math>: ''M'' is augmented ''n'' times, and in a phase where ''M'' is unchanged, there are at most ''n'' potential changes (since ''Z'' increases every time). The time sufficient for a potential change is <math>O(n^2)</math>.\n\n===Proof that adjusting the potential ''y'' leaves ''M'' unchanged===\nTo show that every edge in ''M'' remains after adjusting ''y'', it suffices to show that for an arbitrary edge in ''M'', either both of its endpoints, or neither of them, are in ''Z''. To this end let ''vu'' be an edge in ''M'' from ''T'' to ''S''. It is easy to see that if ''v'' is in ''Z'' then ''u'' must be too, since every edge in ''M'' is tight. Now suppose, toward contradiction, that <math>u \\in Z</math> but <math>v \\notin Z</math>. ''u'' itself cannot be in <math>R_S</math> because it is the endpoint of a matched edge, so there must be some directed path of tight edges from a vertex in <math>R_S</math> to ''u''. This path must avoid ''v'', since that is by assumption not in ''Z'', so the vertex immediately preceding ''u'' in this path is some other vertex <math>v' \\in T</math>. <math>v'u</math> is a tight edge from ''T'' to ''S'' and is thus in ''M''. But then ''M'' contains two edges that share the vertex ''u'', contradicting the fact that ''M'' is a matching. Thus every edge in ''M'' has either both endpoints or neither endpoint in ''Z''.\n\n===Proof that ''y'' remains a potential===\nTo show that ''y'' remains a potential after being adjusted, it suffices to show that no edge has its total potential increased beyond its cost. This is already established for edges in ''M'' by the preceding paragraph, so consider an arbitrary edge ''uv'' from ''S'' to ''T''. If <math>y(u)</math> is increased by <math>\\Delta</math>, then either <math>v \\in Z \\cap T</math>, in which case <math>y(v)</math> is decreased by <math>\\Delta</math>, leaving the total potential of the edge unchanged, or <math>v \\in T \\setminus Z</math>, in which case the definition of <math>\\Delta</math> guarantees that <math>y(u)+y(v)+\\Delta \\leq c(u,v)</math>. Thus ''y'' remains a potential.\n\n==Matrix interpretation==\nGiven <math>n</math> workers and tasks, and an ''n''×''n''  matrix containing the cost of assigning each worker to a task, find the cost minimizing assignment.\n\nFirst the problem is written in the form of a matrix as given below\n\n:{{aligned table|cols=4|class=wikitable\n|a1 | a2 | a3 | a4\n|b1 | b2 | b3 | b4\n|c1 | c2 | c3 | c4\n|d1 | d2 | d3 | d4}}\n\nwhere a, b, c and d are the workers who have to perform tasks 1, 2, 3 and 4. a1, a2, a3, a4 denote the penalties incurred when worker \"a\" does task 1, 2, 3, 4 respectively. The same holds true for the other symbols as well. The matrix is square, so each worker can perform only one task.\n\n'''Step 1'''\n\nThen we perform row operations on the matrix. To do this, the lowest of all ''a<sub>i</sub>'' (i belonging to 1-4) is taken and is subtracted from each element in that row. This will lead to at least one zero in that row (We get multiple zeros when there are two equal elements which also happen to be the lowest in that row). This procedure is repeated for all rows. We now have a matrix with at least one zero per row. Now we try to assign tasks to agents such that each agent is doing only one task and the penalty incurred in each case is zero. This is illustrated below.\n\n:{|class=\"wikitable\" style=\"text-align:center\"\n|-\n| 0 ||a2'||a3' ||a4'\n|-\n|b1'||b2'||b3'|| 0\n|-\n|c1'|| 0 ||c3'||c4'\n|-\n|d1'||d2'|| 0 ||d4'\n|}\n\nThe zeros that are indicated as 0 are the assigned tasks.\n\n'''Step 2'''\n\nSometimes it may turn out that the matrix at this stage cannot be used for assigning, as is the case for the matrix below.\n\n:{|class=\"wikitable\" style=\"text-align:center\"\n|-\n|0  ||a2'||a3'||a4'\n|-\n|b1'||b2'||b3'||0\n|-\n|0  ||c2'||c3'||c4'\n|-\n|d1'||0  ||d3'||d4'\n|}\n\nIn the above case, no assignment can be made. Note that task 1 is done efficiently by both agent a and c. Both can't be assigned the same task. Also note that no one does task 3 efficiently.\nTo overcome this, we repeat the above procedure for all columns (i.e. the minimum element in each column is subtracted from all the elements in that column) and then check if an assignment is possible.\n\nIn most situations this will give the result, but if it is still not possible then we need to keep going.\n\n'''Step 3'''\n\nAll zeros in the matrix must be covered by marking as few rows and/or columns as possible. The following procedure is one way to accomplish this:\n\nFirst, assign as many tasks as possible.\n\n* Row 1 has one zero, so it is assigned.  The 0 in row 3 is crossed out because it is in the same column.\n* Row 2 has one zero, so it is assigned.\n* Row 3's only zero has been crossed out, so nothing is assigned.\n* Row 4 has two uncrossed zeros.  Either one can be assigned, and the other zero is crossed out.\n\nAlternatively, the 0 in row 3 may be assigned, causing the 0 in row 1 to be crossed instead.\n\n:{|class=\"wikitable\" style=\"text-align:center\"\n|-\n| 0'||a2'||a3'||a4'\n|-\n|b1'||b2'||b3'|| 0'\n|-\n| 0 ||c2'||c3'||c4'\n|-\n|d1'|| 0'|| 0 ||d4'\n|}\n\nNow to the drawing part.\n* Mark all rows having no assignments (row 3).\n* Mark all columns having zeros in newly marked row(s) (column 1).\n* Mark all rows having assignments in newly marked columns (row 1).\n* Repeat for all non-assigned rows.\n\n<!-- this might need to make the borders go away, if the current appearance is thought to be ugly -->\n:{|class=\"wikitable\" style=\"text-align:center\"\n|- style=\"background: white\"\n|&times;  || || || ||\n|-\n| 0'||a2'||a3'||a4'\n|style=\"background: white\"|&times;\n|-\n|b1'||b2'||b3'||0'\n|style=\"background: white\"|\n|-\n| 0 ||c2'||c3'||c4'\n|style=\"background: white\"|&times;\n|-\n|d1'||0' ||0||d4'\n|style=\"background: white\"|\n|}\n\nNow draw lines through all marked columns and '''unmarked''' rows.\n\n:{|class=\"wikitable\" style=\"text-align:center\"\n|- style=\"background: white\"\n|&times;  || || || ||\n|-\n|style=\"background:lightgrey\"| 0'||a2'||a3'||a4'\n|style=\"background: white\"|&times;\n|- style=\"background:lightgrey\"\n|b1'||b2'||b3'||0'\n|-\n|style=\"background:lightgrey\"| 0 ||c2'||c3'||c4'\n|style=\"background: white\"|&times;\n|- style=\"background:lightgrey\"\n|d1'||0' ||0||d4'\n|}\n\nThe aforementioned detailed description is just one way to draw the minimum number of lines to cover all the 0s. Other methods work as well.\n\n'''Step 4'''\n\nFrom the elements that are left, find the lowest value. Subtract this from every unmarked element and add it to every element covered by two lines.\n\nRepeat steps 3–4 until an assignment is possible; this is when the minimum number of lines used to cover all the 0s is equal to max(number of people, number of assignments), assuming dummy variables (usually the max cost) are used to fill in when the number of people is greater than the number of assignments.\n\nBasically you find the second minimum cost among the remaining choices.  The procedure is repeated until you are able to distinguish among the workers in terms of least cost.\n\n==Bibliography==\n* R.E. Burkard, M. Dell'Amico, S. Martello: ''Assignment Problems'' (Revised reprint). SIAM, Philadelphia (PA.) 2012. {{ISBN|978-1-61197-222-1}}\n* M. Fischetti, \"Lezioni di Ricerca Operativa\", Edizioni Libreria Progetto Padova, Italia, 1995.\n* [[Ravindra K. Ahuja|R. Ahuja]], [[Thomas L. Magnanti|T. Magnanti]], [[James B. Orlin|J. Orlin]], \"Network Flows\", Prentice Hall, 1993.\n* S. Martello, \"Jeno Egerváry: from the origins of the Hungarian algorithm to satellite communication\". Central European Journal of Operational Research 18, 47–58, 2010\n\n==References==\n{{Reflist}}\n\n==External links==\n* Bruff, Derek, [http://www.math.harvard.edu/archive/20_spring_05/handouts/assignment_overheads.pdf The Assignment Problem and the Hungarian Method] (matrix formalism).\n* Mordecai J. Golin, [http://www.cse.ust.hk/~golin/COMP572/Notes/Matching.pdf Bipartite Matching and the Hungarian Method] (bigraph formalism), Course Notes, [[Hong Kong University of Science and Technology]].\n*[https://brilliant.org/wiki/hungarian-matching Hungarian maximum matching algorithm] (both formalisms), in Brilliant website.\n* [[R. A. Pilgrim]], ''[http://csclab.murraystate.edu/bob.pilgrim/445/munkres.html Munkres' Assignment Algorithm. Modified for Rectangular Matrices]'', Course notes, [[Murray State University]].\n* [[Mike Dawes]], ''[https://web.archive.org/web/20060812030313/http://www.math.uwo.ca/~mdawes/courses/344/kuhn-munkres.pdf The Optimal Assignment Problem]'', Course notes, [[University of Western Ontario]].\n* [http://www.cs.elte.hu/egres/tr/egres-04-14.pdf On Kuhn's Hungarian Method – A tribute from Hungary], [[András Frank]], Egervary Research Group, Pazmany P. setany 1/C, H1117, Budapest, Hungary.\n* Lecture: [https://www.youtube.com/watch?v=BUGIhEecipE Fundamentals of Operations Research - Assignment Problem - Hungarian Algorithm], Prof. G. Srinivasan, Department of Management Studies, IIT Madras.\n* Extension: [http://www.roboticsproceedings.org/rss06/p16.html Assignment sensitivity analysis (with O(n^4) time complexity)], Liu, Shell.\n* [http://www.hungarianalgorithm.com/solve.php Solve any Assignment Problem online], provides a step by step explanation of the Hungarian Algorithm.\n\n:{|class=\"mw-collapsible mw-collapsed\"\n|+ '''Implementations'''<br/>{{nowrap|Note that not all of these satisfy the <math>O(n^3)</math> time constraint.}}\n|-\n| [https://github.com/maandree/hungarian-algorithm-n3/blob/master/hungarian.c C implementation with <math>O(n^3)</math> time complexity] || \n|-\n| [https://github.com/KevinStern/software-and-algorithms/blob/master/src/main/java/blogspot/software_and_algorithms/stern_library/optimization/HungarianAlgorithm.java Java implementation of <math>O(n^3)</math> time variant] ||\n|-\n| [http://shawntoneil.com/index.php/items/code/java@Hungarian@Hungarian.java Java implementation of <math>O(n^3)</math> time variant by Shawn T. O'Neil] ||\n|-\n| [https://github.com/USNavalResearchLaboratory/TrackerComponentLibrary/blob/master/Assignment%20Algorithms/2D%20Assignment/assign2D.m Matlab implementation with <math>O(n^3)</math> time complexity] (public domain) || \n||\n|-\n| [http://software.clapper.org/munkres/ Python implementation]  || \n|-\n| [https://github.com/evansenter/gene/blob/f515fd73cb9d6a22b4d4b146d70b6c2ec6a5125b/objects/extensions/hungarian.rb Ruby implementation with unit tests] || \n|-\n| [https://github.com/antifriz/hungarian-algorithm-n3 C# implementation with <math>O(n^3)</math> time complexity] || \n|-\n| [http://www.fantascienza.net/leonardo/so/hungarian.d D implementation with unit tests (port of the Java <math>O(n^3)</math> version)] || \n|-\n| [http://www.ifors.ms.unimelb.edu.au/tutorial/hungarian/welcome_frame.html Online interactive implementation] <!--Please note that this implements a variant of the algorithm as described above. --> || \n|-\n| [http://web.axelero.hu/szilardandras/gaps.html Graphical implementation with options] ([[Java applet]]) || \n|-\n| [http://www.netlib.org/utk/lsi/pcwLSI/text/node220.html Serial and parallel implementations.] || \n|-\n| [http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=6543 Matlab and C] ||\n|-\n| [https://metacpan.org/module/Algorithm::Munkres Perl implementation] || \n|-\n| [http://robotics.usc.edu/~lantao/code.html C++ (STL) implementation (multi-functional bipartite graph version)] || \n|-\n| [https://github.com/saebyn/munkres-cpp C++ implementation] || \n|-\n| [http://dlib.net/optimization.html#max_cost_assignment C++ implementation of the <math>O(n^3)</math> algorithm] (BSD style open source licensed)  || \n|-\n| [http://timefinder.svn.sourceforge.net/viewvc/timefinder/trunk/timefinder-algo/src/main/java/de/timefinder/algo/roomassignment/ Java implementation with JUnit tests (Apache 2.0)] || \n|-\n| [http://www.mathworks.com/matlabcentral/fileexchange/20652-hungarian-algorithm-for-linear-assignment-problems--v2-3- MATLAB implementation] || \n|-\n| [https://launchpad.net/lib-bipartite-match C implementation] || \n|-\n| [https://github.com/Gerjo/esoteric/blob/master/Hungarian.js JavaScript implementation with unit tests (port of the Java <math>O(n^3)</math> version)]  || \n|-\n| [https://cran.r-project.org/web/packages/clue/clue.pdf  Clue R package proposes an implementation, solve_LSAP]  || \n|-\n| [https://github.com/addaleax/munkres-js Node.js implementation on GitHub] ||\n|-\n| [https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.optimize.linear_sum_assignment.html Python implementation in scipy package] ||\n|-\n|}\n\n{{Use dmy dates|date=September 2010}}\n\n{{DEFAULTSORT:Hungarian Algorithm}}\n[[Category:Matching]]\n[[Category:Combinatorial optimization]]"
    },
    {
      "title": "Integer programming",
      "url": "https://en.wikipedia.org/wiki/Integer_programming",
      "text": "An '''integer programming''' problem is a [[mathematical optimization]] or [[Constraint satisfaction problem|feasibility]] program in which some or all of the variables are restricted to be [[integer|integers]]. In many settings the term refers to '''integer [[linear programming]]''' (ILP), in which the objective function and the constraints (other than the integer constraints) are [[Linear function (calculus)|linear]].\n\nInteger programming is [[NP-complete]]. In particular, the special case of 0-1 integer linear programming, in which unknowns are binary, and only the restrictions must be satisfied, is one of [[Karp's 21 NP-complete problems]].\n\nIf some decision variables are not discrete the problem is known as a '''mixed-integer programming''' problem.<ref>{{cite web |url=http://macc.mcmaster.ca/maccfiles/chachuatnotes/07-MILP-I_handout.pdf |title=Mixed-Integer Linear Programming (MILP): Model Formulation |accessdate=16 April 2018}}</ref>\n\n==Canonical and standard form for ILPs==\n\nAn integer linear program in canonical form is expressed as:<ref name=\"optBook\">{{cite book|last1=Papadimitriou|first1=C. H.|author1-link=Christos Papadimitriou|last2=Steiglitz|first2= K.|author2-link=Kenneth Steiglitz|title=Combinatorial optimization: algorithms and complexity|year=1998|publisher=Dover|location=Mineola, NY|isbn=0486402584}}</ref>\n\n:<math> \\begin{align}\n& \\text{maximize}   && \\mathbf{c}^\\mathrm{T} \\mathbf{x}\\\\\n& \\text{subject to} && A \\mathbf{x} \\le \\mathbf{b}, \\\\\n&  && \\mathbf{x} \\ge \\mathbf{0}, \\\\\n& \\text{and} && \\mathbf{x} \\in \\mathbb{Z}^n,\n\\end{align} </math>\n\nand an ILP in standard form is expressed as\n\n:<math> \\begin{align}\n& \\text{maximize}   && \\mathbf{c}^\\mathrm{T} \\mathbf{x}\\\\\n& \\text{subject to} && A \\mathbf{x} + \\mathbf{s} = \\mathbf{b}, \\\\\n&  && \\mathbf{s} \\ge \\mathbf{0}, \\\\\n&  && \\mathbf{x} \\ge \\mathbf{0}, \\\\\n& \\text{and} && \\mathbf{x} \\in \\mathbb{Z}^n,\n\\end{align} </math>\nwhere <math>\\mathbf{c}, \\mathbf{b}</math> are vectors and <math>A</math> is a matrix, where all entries are integers.  As with linear programs, ILPs not in standard form can be [[simplex algorithm#Standard form|converted to standard form]] by eliminating inequalities, introducing slack variables (<math>\\mathbf{s}</math>) and replacing variables that are not sign-constrained with the difference of two sign-constrained variables\n\n==Example==\n[[File:IP polytope with LP relaxation.svg|350px|thumb|IP polytope with LP relaxation]]\nThe plot on the right shows the following problem.\n:<math>\n     \\begin{align}\n      \\max  & \\text{ } y \\\\\n            -x +y & \\leq 1  \\\\\n            3x +2y & \\leq 12 \\\\\n            2x +3y & \\leq 12 \\\\\n            x,y & \\ge 0 \\\\\n            x,y & \\in \\mathbb{Z}\n     \\end{align}\n</math>\n\nThe feasible integer points are shown in red, and the red dashed lines indicate their convex hull, which is the smallest polyhedron that contains all of these points.  The blue lines together with the coordinate axes define the polyhedron of the LP relaxation, which is given by the inequalities without the integrality constraint. The goal of the optimization is to move the black dotted line as far upward while still touching the polyhedron. The optimal solutions of the integer problem are the points <math>(1,2)</math> and <math>(2,2)</math> which both have an objective value of 2. The unique optimum of the relaxation is <math>(1.8,2.8)</math> with objective value of 2.8.  Note that if the solution of the relaxation is rounded to the nearest integers, it is not feasible for the ILP.\n\n==Proof of NP-hardness==\n\nThe following is a reduction from minimum [[vertex cover]] to integer programming that will serve as the proof of NP-hardness.\n\nLet <math>G = (V,E)</math> be an undirected graph. Define a linear program as follows:\n\n:<math> \\begin{align}\n      \\min \\sum_{v \\in V} y_v \\\\\n      y_v + y_u & \\ge 1 && \\forall uv \\in E\\\\\n      y_v & \\ge 0 && \\forall v \\in V\\\\\n      y_v & \\in \\mathbb{Z} && \\forall v \\in V\n\\end{align}</math>\n\nGiven that the constraints limit <math>y_v</math> to either 0 or 1, any feasible solution to the integer program is a subset of vertices. The first constraint implies that at least one end point of every edge is included in this subset. Therefore, the solution describes a vertex cover. Additionally given some vertex cover C, <math>y_v</math> can be set to 1 for any <math>v\\in C</math> and to 0 for any <math>v\\not\\in C</math> thus giving us a feasible solution to the integer program. Thus we can conclude that if we minimize the sum of <math>y_v</math> we have also found the minimum vertex cover.<ref>{{cite web|last1=Erickson|first1=J.|title=Integer Programming Reduction|url=https://courses.engr.illinois.edu/cs498dl1/sp2015/solutions/hw10sol.pdf|year=2015|archiveurl=https://web.archive.org/web/20150518072946/https://courses.engr.illinois.edu/cs498dl1/sp2015/solutions/hw10sol.pdf|archivedate=18 May 2015|deadurl=yes}}</ref>\n\n==Variants==\n\n'''Mixed integer linear programming''' ('''MILP''') involves problems in which only some of the variables, <math>x_i</math>, are constrained to be integers, while other variables are allowed to be non-integers.\n\n'''Zero-one linear programming''' involves problems in which the variables are restricted to be either 0 or 1.  Note that any bounded integer variable can be expressed as a combination of binary variables.<ref>{{cite book|last=Williams|first=H.P.|title=Logic and integer programming|series=International Series in Operations Research & Management Science|year=2009|volume=130|isbn= 978-0-387-92280-5}}</ref>  For example, given an integer variable, <math>0\\le x\\le U</math>, the variable can be expressed using <math>\\lfloor \\log_2U\\rfloor+1</math> binary variables:\n:<math>\nx = x_1+2x_2+4x_3+\\cdots+2^{\\lfloor \\log_2U\\rfloor}x_{\\lfloor \\log_2U\\rfloor+1}.\n</math>\n\n==Applications==\n\nThere are two main reasons for using integer variables when modeling problems as a linear program:\n#The integer variables represent quantities that can only be integer. For example, it is not possible to build 3.7 cars.\n#The integer variables represent decisions (e.g. whether to include an edge in a [[Graph (discrete mathematics)|graph]]) and so should only take on the value 0 or 1.\nThese considerations occur frequently in practice and so integer linear programming can be used in many applications areas, some of which are briefly described below.\n\n===Production planning===\nMixed integer programming has many applications in industrial production, including job-shop modelling. One important example happens in agricultural [[production planning]] involves determining production yield   for several crops  that can share resources (e.g. Land, labor, capital, seeds, fertilizer, etc.). A possible objective is to maximize the total production, without exceeding the available resources. In some cases, this can be expressed in terms of a linear program, but variables must be constrained to be integer.\n\n===Scheduling===\n\nThese problems involve service and vehicle scheduling in transportation networks.  For example, a problem may involve assigning buses or subways to individual routes so that a timetable can be met, and also to equip them with drivers. Here binary decision variables indicate whether a bus or subway is assigned to a route and whether a driver is assigned to a particular train or subway.\nThe zero-one programming technique has been successfully applied to solve a project selection problem in which projects are mutually exclusive and/or technologically interdependent. It is used in a special case of integer programming, in which all the decision variables are integers. It can assume the values either as zero or one.\n\n===Telecommunications networks===\n\nThe goal of these problems is to design a network of lines to install so that a predefined set of communication requirements are met and the total cost of the network is minimal.<ref>{{cite web|last1=Borndörfer|first1=R.|last2=Grötschel|first2=M.|author2-link= Martin Grötschel |title=Designing telecommunication networks by integer programming|url=http://www.zib.de/groetschel/teaching/SS2012/120503Vorlesung-DesigningTelcomNetworks-reduced.pdf|year=2012}}</ref>  This requires optimizing both the topology of the network along with the setting the capacities of the various lines.  In many cases, the capacities are constrained to be integer quantities.  Usually there are, depending on the technology used, additional restrictions that can be modeled as linear inequalities with integer or binary variables.\n\n===Cellular networks===\nThe task of frequency planning in [[GSM]] mobile networks involves distributing available frequencies across the antennas so that users can be served and interference is minimized between the antennas.<ref>{{cite web|last=Sharma|first=Deepak|title=Frequency Planning|url=http://www.slideshare.net/deepakecrbs/gsm-frequency-planning|year= 2010}}</ref>  This problem can be formulated as an integer linear program in which binary variables indicate whether a frequency is assigned to an antenna.\n\n==Algorithms==\n\nThe naive way to solve an ILP is to simply remove the constraint that '''x''' is integer, solve the corresponding LP (called the [[Linear programming relaxation|LP relaxation]] of the ILP), and then round the entries of the solution to the LP relaxation.  But, not only may this solution not be optimal, it may not even be feasible; that is, it may violate some constraint.\n\n===Using total unimodularity===\n\nWhile in general the solution to LP relaxation will not be guaranteed to be integral, if the ILP has the form <math>\\max\\mathbf{c}^\\mathrm{T} \\mathbf{x}</math> such that <math>A\\mathbf{x} = \\mathbf{b}</math> where <math>A, \\mathbf{b},</math> and <math>\\mathbf{c}</math> have all integer entries and <math>A</math> is [[unimodular matrix#Total unimodularity|totally unimodular]], then every basic feasible solution is integral.  Consequently, the solution returned by the [[simplex algorithm]] is guaranteed to be integral.  To show that every basic feasible solution is integral, let <math>\\mathbf{x}</math> be an arbitrary basic feasible solution . Since <math>\\mathbf{x}</math> is feasible,\nwe know that <math>A\\mathbf{x}=\\mathbf{b}</math>. Let <math>\\mathbf{x}_0=[x_{n_1},x_{n_2},\\cdots,x_{n_j}]</math> be the elements corresponding to the basis columns for the basic solution <math>\\mathbf{x}</math>. By definition of a basis, there is some square submatrix <math>B</math> of\n<math>A</math> with linearly independent columns such that <math>B\\mathbf{x}_0=\\mathbf{b}</math>.\n\nSince the columns of <math>B</math> are linearly independent and <math>B</math> is square, <math>B</math> is nonsingular,\nand therefore by assumption, <math>B</math> is [[unimodular matrix|unimodular]] and so <math>\\det(B)=\\pm1</math>. Also, since <math>B</math> is nonsingular, it is invertible and therefore <math>\\mathbf{x}_0=B^{-1}\\mathbf{b}</math>. By definition, <math>B^{-1}=\\frac{B^{adj}}{\\det(B)}=\\pm B^{adj}</math>. Here <math>B^{adj}</math> denotes the [[Adjugate matrix|adjugate]] of <math>B</math> and is integral because <math>B</math> is integral.  Therefore,\n:<math>\n\\begin{align}\n&\\Rightarrow B^{-1}=\\pm B^{adj} \\text{ is integral.} \\\\\n&\\Rightarrow x_0=B^{-1}b \\text{ is integral.} \\\\\n&\\Rightarrow \\text{Every basic feasible solution is integral.}\n\\end{align}\n</math>\nThus, if the matrix <math>A</math> of an ILP is totally unimodular, rather than use an ILP algorithm, the simplex method can be used to solve the LP relaxation and the solution will be integer.\n\n===Exact algorithms===\nWhen the matrix <math>A</math> is not totally unimodular, there are a variety of algorithms that can be used to solve integer linear programs exactly.  One class of algorithms are [[Cutting-plane method|cutting plane methods]] which work by solving the LP relaxation and then adding linear constraints that drive the solution towards being\ninteger without excluding any integer feasible points.\n\nAnother class of algorithms are variants of the [[branch and bound]] method.  For example, the [[branch and cut]] method that combines both branch and bound and cutting plane methods.  Branch and bound algorithms have a number of advantages over algorithms that only use cutting planes.  One advantage is that the algorithms can be terminated early and as long as at least one integral solution has been found, a feasible, although not necessarily optimal, solution can be returned.  Further, the solutions of the LP relaxations can be used to provide a worst-case estimate of how far from optimality the returned solution is.  Finally, branch and bound methods can be used to return multiple optimal solutions.\n\nLenstra in 1983 showed <ref>[[Hendrik Lenstra|H.W. Lenstra]], \"Integer programming with a fixed number of variables\", Mathematics of operations research, Vol 8, No 8, November 1983</ref> that, when the number of variables is fixed, the feasibility integer programming problem can be solved in polynomial time.\n\n===Heuristic methods===\nSince integer linear programming is [[NP-hard]], many problem instances are intractable and so heuristic methods must be used instead.  For example, [[tabu search]] can be used to search for solutions to ILPs.<ref>{{cite journal|last=Glover|first=F.|authorlink=Fred W. Glover|title=Tabu search-Part II|journal=ORSA Journal on computing|year=1989|volume=1|issue=3|pages=4–32|doi= 10.1287/ijoc.2.1.4 }}</ref>  To use tabu search to solve ILPs, moves can be defined as incrementing or decrementing an integer constrained variable of a feasible solution while keeping all other integer-constrained variables constant.  The unrestricted variables are then solved for.  Short term memory can consist of previously tried solutions while medium-term memory can consist of values for the integer constrained variables that have resulted in high objective values (assuming the ILP is a maximization problem).  Finally, long term memory can guide the search towards integer values that have not previously been tried.\n\nOther heuristic methods that can be applied to ILPs include\n*[[Hill climbing]]\n*[[Simulated annealing]]\n*Reactive search optimization\n*[[Ant_colony_optimization_algorithms|Ant colony optimization]]\n*[[Hopfield network|Hopfield neural networks]]\n\nThere are also a variety of other problem-specific heuristics, such as the [[Travelling salesman problem#Iterative improvement|k-opt heuristic]] for the traveling salesman problem.  Note that a disadvantage of heuristic methods is that if they fail to find a solution, it cannot be determined whether it is because there is no feasible solution or whether the algorithm simply was unable to find one.  Further, it is usually impossible to quantify how close to optimal a solution returned by these methods is.\n\n==See also==\n* [[Constrained least squares]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n* {{cite book|author1=[[George Nemhauser|George L. Nemhauser]]|author2=Laurence A. Wolsey|title=Integer and combinatorial optimization|year=1988|publisher=Wiley|isbn=978-0-471-82819-8}}\n* {{cite book|author=Alexander Schrijver|authorlink=Alexander Schrijver|title=Theory of linear and integer programming|year=1998|publisher=John Wiley and Sons|isbn=978-0-471-98232-6}}\n* {{cite book|author=Laurence A. Wolsey|title=Integer programming|year=1998|publisher=Wiley|isbn=978-0-471-28366-9}}\n* {{cite book|author1=Dimitris Bertsimas|author2=Robert Weismantel|title=Optimization over integers|year=2005|publisher=Dynamic Ideas|isbn=978-0-9759146-2-5}}\n* {{cite book|author=John K. Karlof|title=Integer programming: theory and practice|year=2006|publisher=CRC Press|isbn=978-0-8493-1914-3}}\n* {{cite book|author=H. Paul Williams|title=Logic and Integer Programming|year=2009|publisher=Springer|isbn=978-0-387-92279-9}}\n* {{cite book|editor1=Michael Jünger |editor2=Thomas M. Liebling |editor3=Denis Naddef |editor4=[[George Nemhauser]] |editor5=[[William R. Pulleyblank]] |editor6=Gerhard Reinelt |editor7=Giovanni Rinaldi |editor8=Laurence A. Wolsey |title=50 Years of Integer Programming 1958-2008: From the Early Years to the State-of-the-Art|year=2009|publisher=Springer|isbn=978-3-540-68274-5}}\n* {{cite book|author1=Der-San Chen|author2=Robert G. Batson|author3=Yu Dang|title=Applied Integer Programming: Modeling and Solution|year=2010|publisher=John Wiley and Sons|isbn=978-0-470-37306-4}}\n* {{cite book|author1=Gerard Sierksma|author2=Yori Zwols|title=Linear and Integer Optimization: Theory and Practice|year=2015|publisher=CRC Press|isbn=978-1-498-71016-9}}\n\n==External links==\n* [http://mat.gsia.cmu.edu/orclass/integer/integer.html A Tutorial on Integer Programming]\n* Conference [http://www.mathopt.org/?nav=ipco Integer Programming and Combinatorial Optimization, IPCO]\n* [http://www.iasi.cnr.it/aussois The Aussois Combinatorial Optimization Workshop]\n{{Optimization algorithms|combinatorial|state=expanded}}\n\n[[Category:Combinatorial optimization]]"
    },
    {
      "title": "Kernighan–Lin algorithm",
      "url": "https://en.wikipedia.org/wiki/Kernighan%E2%80%93Lin_algorithm",
      "text": ": ''This article is about the heuristic algorithm for the graph partitioning problem. For a heuristic for the traveling salesperson problem, see [[Lin–Kernighan heuristic]].''\nThe '''Kernighan–Lin algorithm''' is a [[Heuristic (computer science)|heuristic algorithm]] for [[graph partition|finding partitions of graphs]].\nThe algorithm has important applications in the layout of digital circuits and components in [[VLSI]].<ref name=\"kl\"/><ref name=\"ravikumar\"/>\n\n==Description==\nThe input to the algorithm is an [[undirected graph]] {{math|1=''G'' = (''V'',''E'')}} with vertex set {{mvar|V}}, edge set {{mvar|E}}, and (optionally) numerical weights on the edges in {{mvar|E}}. The goal of the algorithm is to partition {{mvar|V}} into two disjoint subsets {{mvar|A}} and {{mvar|B}} of equal (or nearly equal) size, in a way that minimizes the sum {{mvar|T}} of the weights of the subset of edges that cross from {{mvar|A}} to {{mvar|B}}. If the graph is unweighted, then instead the goal is to minimize the number of crossing edges; this is equivalent to assigning weight one to each edge. The algorithm maintains and improves a partition, in each pass using a [[greedy algorithm]] to pair up vertices of {{mvar|A}} with vertices of {{mvar|B}}, so that moving the paired vertices from one side of the partition to the other will improve the partition. After matching the vertices, it then performs a subset of the pairs chosen to have the best overall effect on the solution quality {{mvar|T}}.\nGiven a graph with {{mvar|n}} vertices, each pass of the algorithm runs in time {{math|''O''(''n''<sup>2</sup> log ''n'')}}. \n\nIn more detail, for each <math>a \\in A</math>, let <math>I_{a}</math> be the ''internal cost'' of ''a'', that is, the sum of the costs of edges between ''a'' and other nodes in ''A'', and let <math>E_{a}</math> be the ''external cost'' of ''a'', that is, the sum of the costs of edges between ''a'' and nodes in ''B''. Similarly, define <math>I_{b}</math>, <math>E_{b}</math> for each <math>b \\in B</math>. Furthermore, let \n:<math>D_{s} = E_{s} - I_{s}</math>\nbe the difference between the external and internal costs of ''s''. If ''a'' and ''b'' are interchanged, then the reduction in cost is\n:<math>T_{old} - T_{new} = D_{a} + D_{b} - 2c_{a,b}</math>\nwhere <math>c_{a,b}</math> is the cost of the possible edge between ''a'' and ''b''.\n\nThe algorithm attempts to find an optimal series of interchange operations between elements of <math>A</math> and <math>B</math> which maximizes <math>T_{old} - T_{new}</math> and then executes the operations, producing a partition of the graph to ''A'' and ''B''.<ref name=\"kl\">{{cite journal|first1=B. W.|last1=Kernighan|authorlink1=Brian Kernighan|first2=Shen|last2=Lin| year = 1970 | title = An efficient heuristic procedure for partitioning graphs | journal = Bell System Technical Journal|volume=49|pages=291–307| doi=10.1002/j.1538-7305.1970.tb01770.x}}</ref>\n\n==Pseudocode==\nSee <ref name=\"ravikumar\">{{cite book |last=Ravikumar |first=C. P |title=Parallel methods for VLSI layout design|publisher=Greenwood Publishing Group|year=1995|pages=73|isbn=978-0-89391-828-6|url=https://books.google.com/books?id=VPXAxkTKxXIC}}</ref>\n\n<code>\n  1  '''function''' Kernighan-Lin(''G(V,E)''):\n  2      determine a balanced initial partition of the nodes into sets A and B\n  3      \n  4      '''do'''\n  5         compute D values for all a in A and b in B\n  6         let gv, av, and bv be empty lists\n  7         '''for (n := 1 to |V|/2)'''\n  8            find a from A and b from B, such that g = D[a] + D[b] - 2*c(a, b) is maximal\n  9            remove a and b from further consideration in this pass\n  10           add g to gv, a to av, and b to bv\n  11           update D values for the elements of A = A \\ a and B = B \\ b\n  12        '''end for'''\n  13        find k which maximizes g_max, the sum of gv[1],...,gv[k]\n  14        '''if (g_max > 0)''' '''then'''\n  15           Exchange av[1],av[2],...,av[k] with bv[1],bv[2],...,bv[k]\n  16     '''until (g_max <= 0)'''\n  17  '''return G(V,E)'''\n</code>\n\n==See also==\n* [[Fiduccia-Mattheyses algorithm]]\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Kernighan-Lin algorithm}}\n[[Category:Combinatorial optimization]]\n[[Category:Combinatorial algorithms]]\n[[Category:Heuristic algorithms]]"
    },
    {
      "title": "Linear bottleneck assignment problem",
      "url": "https://en.wikipedia.org/wiki/Linear_bottleneck_assignment_problem",
      "text": "In [[combinatorial optimization]], a field within mathematics, the '''linear bottleneck assignment problem''' ('''LBAP''') is similar to the [[linear assignment problem]].<ref>[http://www.assignmentproblems.com/ Assignment Problems], by [[Rainer Burkard]], Mauro Dell'Amico, Silvano Martello, 2009, Chapter 6.2 \"[https://books.google.com/books?id=nHIzbApLOr0C&pg=PA198&dq=%22bottleneck+assignment+problem%22&hl=en&ei=A7f0S9DjMYH6sQOYt8CIBQ&sa=X&oi=book_result&ct=result&resnum=2&ved=0CDIQ6AEwAQ#v=onepage&q=%22bottleneck%20assignment%20problem%22&f=false Linear Bottleneck Assignment Problem]\" (p. 172)</ref>\n\nIn plain words the problem is stated as follows:\n:There are a number of ''agents'' and a number of ''tasks''. Any agent can be assigned to perform any task, incurring some ''cost'' that may vary depending on the agent-task assignment. It is required to perform all tasks by assigning exactly one agent to each task in such a way that the ''maximum cost'' among the individual assignments is minimized.\n\nThe term \"[[wikt:bottleneck|bottleneck]]\" is explained by a common type of application of the problem, where the cost is the duration of the task performed by an agent. In this setting the \"maximum cost\" is \"maximum duration\", which is the bottleneck for the schedule of the overall job, to be minimized.\n\n==Formal definition==\nThe formal definition of the bottleneck assignment problem is\n\n:Given two sets, ''A'' and ''T'', together with a [[weight function]] ''C'' : ''A'' &times; ''T'' &rarr; '''[[real number|R]]'''. Find a [[bijection]] ''f'' : ''A'' &rarr; ''T'' such that the [[Loss function|cost function]]:\n\n::<math>\\max_{a\\in A}C(a,f(a))</math>\n:is minimized.\n\nUsually the weight function is viewed as a square real-valued [[matrix (mathematics)|matrix]] ''C'', so that the cost function is written down as:\n\n::<math>\\max_{a\\in A}C_{a,f(a)}</math>\n\n==Mathematical programming formulation==\n:<math>\t\\min \\, \\max_{i,j}\t  c_{ij}x_{ij}</math>\t\nsubject to:\n:<math>\t\n\\sum^n_{ j = 1}\t x_{ij} = 1      (i = 1,2,\\dots, n), \n</math>\n:<math>\t\n\\sum^n_{ i = 1}\t x_{ij} = 1      (j = 1,2,\\dots, n), \n</math>\n:<math>\t\nx_{ij}\t\\in \t\\{0,1\\}     (i, j = 1,2,\\dots,n)\n</math>\n\n==Asymptotics==\nLet <math> c^*_n </math> denote the optimal objective function value for the problem with ''n'' agents and ''n'' tasks.  If the costs <math> c_{ij} </math> are sampled from the uniform distribution on (0,1), then <ref>Michael Z. Spivey, \"Asymptotic Moments of the Bottleneck Assignment Problem,\" ''Mathematics of Operations Research'', 36 (2): 205-226, 2011.</ref>\n:<math> E[c^*_n] = \\frac{\\log n + \\log 2 + \\gamma}{n} + O\\left(\\frac{(\\log n)^2}{n^{7/5}}\\right)</math>\nand\n:<math> Var[c_n^*] = \\frac{\\zeta(2) - 2(\\log 2)^2}{n^2} + O\\left( \\frac{(\\log n)^2}{n^{7/3}}\\right).</math>\n\n==References==\n{{reflist}}\n\n[[Category:Combinatorial optimization]]"
    },
    {
      "title": "List of knapsack problems",
      "url": "https://en.wikipedia.org/wiki/List_of_knapsack_problems",
      "text": "The '''[[knapsack problem]]''' is one of the most studied problems in [[combinatorial optimization]], with many real-life applications. For this reason, many special cases and generalizations have been examined.\n\nCommon to all versions are a set of ''n'' items, with each item <math>1 \\leq j \\leq n</math> having an associated profit ''p<sub>j</sub>'' ,weight ''w<sub>j</sub>''. The binary decision variable ''x<sub>j</sub>'' is used to select the item. The objective is to pick some of the items, with maximal total profit, while obeying that the maximum total weight of the chosen items must not exceed ''W''. Generally, these coefficients are scaled to become integers, and they are almost always assumed to be positive.\n\nThe '''knapsack problem''' in its most basic form:\n{|\n|colspan=\"2\"|maximize <math>\\sum_{j=1}^n p_j x_j</math>\n|\n|-\n|subject to\n|<math>\\sum_{j=1}^n w_j x_j \\leq W,</math>\n|\n|-\n|\n|<math> x_j \\in \\{0,1\\}</math>\n|<math>\\forall j \\in \\{1,\\ldots, n\\}</math>\n|}\n\n__TOC__\n\n==Direct generalizations==\nOne common variant is that each item can be chosen multiple times. The '''bounded knapsack problem''' specifies, for each item ''j'', an upper bound ''u<sub>j</sub>'' (which may be a positive integer, or infinity) on the number of times item ''j'' can be selected:\n\n{|\n|colspan=\"2\"|maximize <math>\\sum_{j=1}^n p_j x_j</math>\n|\n|-\n| subject to\n|<math>\\sum_{j=1}^n w_j x_j \\leq W,</math>\n|\n|-\n|\n|<math> u_j \\geq x_j \\geq 0, x_j</math> integral for all ''j''\n|}\n\nThe '''unbounded knapsack problem''' (sometimes called the '''integer knapsack problem''') does not put any upper bounds on the number of times an item may be selected:\n\n{|\n|colspan=\"2\"|maximize <math>\\sum_{j=1}^n p_j x_j</math>\n|\n|-\n| subject to\n|<math>\\sum_{j=1}^n w_j x_j \\leq W,</math>\n|\n|-\n|\n|<math> x_j \\geq 0, x_j</math> integral for all ''j''\n|}\n\nThe unbounded variant was shown to be [[NP-complete]] in 1975 by Lueker.<ref>{{Cite paper\n  | first = G.S.\n  | last = Lueker\n  | title = Two NP-complete problems in nonnegative integer programming\n  | series = Report No. 178, Computer Science Laboratory, Princeton\n  | year = 1975}}</ref> Both the bounded and unbounded variants admit an [[Polynomial-time approximation scheme|FPTAS]] (essentially the same as the one used in the 0-1 knapsack problem).\n\nIf the items are subdivided into ''k'' classes denoted <math>N_i</math>, and exactly one item must be taken from each class, we get the '''multiple-choice knapsack problem''':\n\n{|\n|colspan=\"2\"|maximize <math>\\sum_{i=1}^k\\sum_{j\\in N_i} p_{ij} x_{ij}</math>\n|\n|-\n|subject to\n|<math>\\sum_{i=1}^k\\sum_{j\\in N_i} w_{ij} x_{ij} \\leq W,</math>\n|\n|-\n|\n|<math>\\sum_{j \\in N_i}x_{ij} = 1,</math>\n|for all <math>1 \\leq i \\leq k</math>\n|-\n|\n|<math> x_{ij} \\in \\{0,1\\}</math>\n|for all <math>1 \\leq i \\leq k</math> and all <math>j \\in N_i</math>\n|}\n\nIf for each item the profit and weight are equal, we get the '''[[subset sum problem]]''' (often the corresponding [[decision problem]] is given instead):\n\n{|\n|colspan=\"2\"|maximize <math>\\sum_{j=1}^n p_j x_j</math>\n|\n|-\n|subject to\n|<math>\\sum_{j=1}^n p_j x_j \\leq W,</math>\n|\n|-\n|\n|<math> x_j \\in \\{0,1\\}</math>\n|\n|}\n\nIf we have ''n'' items and ''m'' knapsacks with capacities <math>W_i</math>, we get the '''multiple knapsack problem''':\n\n{|\n|colspan=\"2\"|maximize <math>\\sum_{i=1}^m\\sum_{j=1}^n p_j x_{ij}</math>\n|\n|-\n|subject to\n|<math>\\sum_{j=1}^n w_j x_{ij} \\leq W_i,</math>\n|for all <math>1 \\leq i \\leq m</math>\n|-\n|\n|<math>\\sum_{i=1}^m x_{ij} \\leq 1,</math>\n|for all <math>1 \\leq j \\leq n</math>\n|-\n|\n|<math> x_{ij} \\in \\{0,1\\}</math>\n|for all <math>1 \\leq j \\leq n</math> and all <math>1 \\leq i \\leq m</math>\n|}\n\nAs a special case of the multiple knapsack problem, when the profits are equal to weights and all bins have the same capacity, we can have '''multiple subset sum problem'''.\n\n'''[[Quadratic knapsack problem]]''':\n\n{|\n|colspan=\"2\"|maximize <math>\\sum_{j=1}^n p_j x_j+\\sum_{i=1}^{n-1}\\sum_{j=i+1}^n p_{ij} x_i x_j</math>\n|\n|-\n|subject to\n|<math>\\sum_{j=1}^n w_j x_j \\leq W,</math>\n|\n|-\n|\n|<math> x_j \\in \\{0,1\\}</math>\n|for all <math>1 \\leq j \\leq n</math>\n|\n|}\n\n'''Set-Union Knapsack Problem''':\n\nSUKP is defined by Kellerer et al<ref name = KP-KPP>{{cite book\n | author= Kellerer, Hans and Pferschy, Ulrich and Pisinger, David\n | year = 2004\n | title = Knapsack Problems\n | publisher = [[Springer Verlag]]\n | isbn = 978-3-540-40286-2\n }}</ref> (on page 423) as follows:\n<blockquote>\nGiven a set of <math>n</math> items <math>N = \\{1, \\ldots, n\\}</math> and a set of <math>m</math> so-called elements <math>P = \\{1, \\ldots, m\\}</math>, each item <math>j</math> corresponds to a subset <math>P_j</math> of the element set <math>P</math>. The items <math>j</math> have non-negative profits <math>p_j</math>, <math>j = 1, \\ldots, n</math>, and the elements <math>i</math> have non-negative weights <math>w_i</math>, <math>i = 1, \\ldots, m</math>. The total weight of a set of items is given by the total weight of the elements of the union of the corresponding element sets. The objective is to find a subset of the items with total weight not exceeding the knapsack capacity and maximal profit.\n</blockquote>\n\n==Multiple constraints==\nIf there is more than one constraint (for example, both a volume limit and a weight limit, where the volume and weight of each item are not related), we get the '''multiply-constrained knapsack problem''', '''multidimensional knapsack problem''', or ''m''-'''dimensional knapsack problem'''. (Note, \"dimension\" here does not refer to the shape of any items.) This has 0-1, bounded, and unbounded variants; the unbounded one is shown below.\n\n{|\n|colspan=\"2\"|maximize <math>\\sum_{j=1}^n p_j x_j</math>\n|\n|-\n|subject to\n|<math>\\sum_{j=1}^n w_{ij} x_j \\leq W_i,</math>\n|for all <math>1 \\leq i \\leq m </math>\n|-\n|\n|<math>x_j \\geq 0</math>, <math>x_j</math> integer\n|for all <math> 1\\leq j \\leq n</math>\n|}\n\nThe 0-1 variant (for any fixed <math>m \\ge 2</math>) was shown to be [[NP-complete]] around 1980 and more strongly, has no FPTAS unless P=NP.<ref>{{cite news|authors = Gens, G. V. and Levner, E. V.|year=1979|title=Complexity and Approximation Algorithms for Combinatorial Problems: A Survey|publisher = Central Economic and Mathematical Institute, Academy of Sciences of the USSR, Moscow}}</ref><ref>{{cite journal|author8 = Korte, B. and Schrader, R. | year = 1980 | title = On the Existence of Fast Approximation Schemes| journal = Nonlinear Programming | volume = 4 | pages = 415–437}}</ref>\n\nThe bounded and unbounded variants (for any fixed <math>m \\ge 2</math>) also exhibit the same hardness.<ref>{{cite journal|doi = 10.1287/moor.9.2.244|last1 = Magazine|title = A Note on Approximation Schemes for Multidimensional Knapsack Problems|first1 = M. J.|last2 = Chern|first2 = M.-S.|\n     author8 = Magazine, Michael J. and Chern, Maw-Sheng |\n     journal = Mathematics of Operations Research |\n     volume = 9|\n     issue = 2|\n     pages = 244–247|year = 1984}}</ref>\n\nFor any fixed <math>m \\ge 2</math>, these problems do admit a [[pseudo-polynomial time]] algorithm (similar to the one for basic knapsack) and a [[Polynomial-time approximation scheme|PTAS]].<ref name = KP-KPP />\n\n==Knapsack-like problems==\nIf all the profits are 1, we will try to maximize the number of items which would not exceed the knapsack capacity:\n\n{|\n|colspan=\"2\"|maximize <math>\\sum_{j=1}^n x_j</math>\n|\n|-\n|subject to\n|<math>\\sum_{j=1}^n w_j x_j \\leq W,</math>\n|\n|-\n|\n|<math> x_j \\in \\{0,1\\},</math>\n|<math> \\forall j \\in \\{1,\\ldots,n\\}</math>\n|}\n\nIf we have a number of containers (of the same size), and we wish to pack all ''n'' items in as few containers as possible, we get the '''[[bin packing problem]]''', which is modelled by having indicator variables <math>y_i=1 \\Leftrightarrow</math> container ''i'' is being used:\n\n{|\n|colspan=\"2\"|minimize <math>\\sum_{i=1}^n y_i</math>\n|\n|-\n|subject to\n|<math>\\sum_{j=1}^n w_j x_{ij} \\leq Wy_i,</math>\n|<math>\\forall i \\in \\{1,\\ldots,n\\}</math>\n|-\n|\n|<math>\\sum_{i=1}^n x_{ij} = 1</math>\n|<math>\\forall j \\in \\{1,\\ldots,n\\}</math>\n|-\n|\n|<math> y_i \\in \\{0,1\\}</math>\n|<math>\\forall i \\in \\{1,\\ldots,n\\}</math>\n|-\n|\n|<math> x_{ij} \\in \\{0,1\\}</math>\n|<math>\\forall i \\in \\{1,\\ldots,n\\} \\wedge \\forall j \\in \\{1,\\ldots,n\\}</math>\n|}\n\nThe '''[[cutting stock problem]]''' is identical to the [[bin packing problem]], but since practical instances usually have far fewer types of items, another formulation is often used. Item ''j'' is needed ''B<sub>j</sub>'' times, each \"pattern\" of items which fit into a single knapsack have a variable, ''x<sub>i</sub>'' (there are ''m'' patterns), and pattern ''i'' uses item ''j'' ''b<sub>ij</sub>'' times:\n\n{|\n|colspan=\"2\"|minimize <math>\\sum_{i=1}^m x_i</math>\n|\n|-\n|subject to\n|<math>\\sum_{i=1}^m b_{ij} x_i \\leq B_j,</math>\n|for all <math>1 \\leq j \\leq n</math>\n|-\n|\n|<math>x_i \\in \\{0,1,\\ldots ,n\\}</math>\n|for all <math>1\\leq i \\leq m</math>\n|}\n\nIf, to the multiple choice knapsack problem, we add the constraint that each subset is of size ''n'' and remove the restriction on total weight, we get the '''[[assignment problem]]''', which is also the problem of finding a maximal '''[[bipartite matching]]''':\n\n{|\n|colspan=\"2\"|maximize <math>\\sum_{i=1}^k\\sum_{j=1}^n p_{ij} x_{ij}</math>\n|\n|-\n|subject to\n|<math>\\sum_{i=1}^n x_{ij} = 1,</math>\n|for all <math>1 \\leq j \\leq n</math>\n|-\n|\n|<math>\\sum_{j=1}^n x_{ij} = 1,</math>\n|for all <math>1 \\leq i \\leq n</math>\n|-\n|\n|<math> x_{ij} \\in \\{0,1\\}</math>\n|for all <math>1 \\leq i \\leq k</math> and all <math>j \\in N_i</math>\n|}\n\nIn the '''Maximum Density Knapsack''' variant there is an initial weight <math>w_0</math>, \nand we maximize the density of selected of items which do not violate the capacity constraint:\n<ref>{{Cite journal |doi = 10.1016/j.ipl.2008.03.017|title = The Generalized Maximum Coverage Problem|journal = Information Processing Letters|volume = 108|pages = 15–22|year = 2008|last1 = Cohen|first1 = Reuven|last2 = Katzir|first2 = Liran|citeseerx = 10.1.1.156.2073}}</ref>\n\n{|\n|colspan=\"2\"|maximize <math>\\frac{\\sum_{j=1}^n x_j p_j}{w_0 +\\sum_{j=1}^n x_j w_j}</math>\n|\n|-\n|subject to\n|<math>\\sum_{j=1}^n w_j x_j \\leq W,</math>\n|\n|-\n|\n|<math> x_j \\in \\{0,1\\},</math>\n|<math> \\forall j \\in \\{1,\\ldots,n\\}</math>\n|}\n\nAlthough less common than those above, several other knapsack-like problems exist, including:\n\n* Nested knapsack problem\n* Collapsing knapsack problem\n* Nonlinear knapsack problem\n* Inverse-parametric knapsack problem\n\nThe last three of these are discussed in Kellerer et al's reference work, ''Knapsack Problems''.<ref name=KP-KPP />\n\n==References==\n<references/>\n* [http://www.diku.dk/~pisinger/95-1.pdf \"Algorithms for Knapsack Problems\"], D. Pisinger. Ph.D. thesis, DIKU, University of Copenhagen, Report 95/1 (1995).\n\n[[Category:Combinatorial optimization]]"
    },
    {
      "title": "Matching (graph theory)",
      "url": "https://en.wikipedia.org/wiki/Matching_%28graph_theory%29",
      "text": "{{for|comparisons of two graphs|Graph matching}}\n\nIn the mathematical discipline of [[graph theory]], a '''matching''' or '''independent edge set''' in a [[Graph (discrete mathematics)|graph]] is a set of [[Edge (graph theory)|edges]] without common [[vertex (graph theory)|vertices]]. Finding a matching in a [[bipartite graph]] can be treated as a [[Flow network|network flow]] problem.\n\n{{Covering-Packing_Problem_Pairs}}\n\n== Definition ==\n\nGiven a [[Graph (discrete mathematics)|graph]] ''G'' = (''V'',''E''), a '''matching''' ''M'' in ''G'' is a set of pairwise [[non-adjacent]] edges, none of which are [[loop (graph theory)|loop]]s; that is, no two edges share a common vertex.\n\nA vertex is '''matched''' (or '''saturated''') if it is an endpoint of one of the edges in the matching.  Otherwise the vertex is '''unmatched'''.\n\nA '''maximal matching''' is a matching ''M'' of a graph ''G'' with the property that if any edge not in ''M'' is added to ''M'', it is no longer a matching, that is, ''M'' is maximal if it is not a subset of any other matching in graph ''G''.  In other words, a matching ''M'' of a graph ''G'' is maximal if every edge in ''G'' has a non-empty intersection with at least one edge in ''M''. The following figure shows examples of maximal matchings (red) in three graphs.\n\n:[[File:Maximal-matching.svg]]\n\nA '''maximum matching''' (also known as maximum-cardinality matching<ref>Alan Gibbons, Algorithmic Graph Theory, Cambridge University Press, 1985, Chapter 5.</ref>) is a matching that contains the largest possible number of edges.  There may be many maximum matchings.  The '''matching number''' <math>\\nu(G)</math> of a graph <math>G</math> is the size of a maximum matching. Note that every maximum matching is maximal, but not every maximal matching is a maximum matching. The following figure shows examples of maximum matchings in the same three graphs.\n\n:[[File:Maximum-matching-labels.svg]]\n\nA '''perfect matching''' (a.k.a. [[1-factor]]) is a matching which matches all vertices of the graph. That is, every vertex of the graph is [[incidence (geometry)|incident]] to exactly one edge of the matching. Every perfect matching is maximum and hence maximal. In some literature, the term '''complete matching''' is used. In the above figure, only part (b) shows a perfect matching. A perfect matching is also a minimum-size [[edge cover]]. Thus, {{math|<var>&nu;(G) &le; &rho;(G) </var>}}, that is, the size of a maximum matching is no larger than the size of a minimum edge cover. A perfect matching can only occur when the graph has an even number of vertices.\n\nA '''near-perfect matching''' is one in which exactly one vertex is unmatched.  This can only occur when the graph has an [[odd number]] of vertices, and such a matching must be maximum. In the above figure, part (c) shows a near-perfect matching. If, for every vertex in a graph, there is a near-perfect matching that omits only that vertex, the graph is also called [[factor-critical graph|factor-critical]].\n\nGiven a matching ''M'',\n* an '''alternating path''' is a path that begins with an unmatched vertex and<ref>http://diestel-graph-theory.com/basic.html</ref> whose edges belong alternately to the matching and not to the matching.\n* an '''augmenting path''' is an alternating path that starts from and ends on free (unmatched) vertices.\n\nOne can prove that a matching is maximum if and only if it does not have any augmenting path. (This result is sometimes called [[Berge's lemma]].)\n\nAn [[induced matching]] is a matching that is an [[induced subgraph]].<ref>{{citation\n | last = Cameron | first = Kathie\n | department = Special issue for First Montreal Conference on Combinatorics and Computer Science, 1987\n | doi = 10.1016/0166-218X(92)90275-F\n | issue = 1-3\n | journal = [[Discrete Applied Mathematics]]\n | mr = 1011265\n | pages = 97–102\n | title = Induced matchings\n | volume = 24\n | year = 1989}}</ref>\n\n== Properties ==\n\nIn any graph without isolated vertices, the sum of the matching number and the [[edge covering number]] equals the number of vertices.<ref>{{citation|last=Gallai|first=Tibor|title=Über extreme Punkt- und Kantenmengen|journal=Ann. Univ. Sci. Budapest. Eötvös Sect. Math. |volume=2|pages=133–138|year=1959}}.</ref> If there is a perfect matching, then both the matching number and the edge cover number are {{math|{{!}}''V'' {{!}} / 2}}.\n\nIf {{math|''A''}} and {{math|''B''}} are two maximal matchings, then {{math|{{!}}''A''{{!}}&nbsp;≤&nbsp;2{{!}}''B''{{!}}}} and {{math|{{!}}''B''{{!}}&nbsp;≤&nbsp;2{{!}}''A''{{!}}}}. To see this, observe that each edge in {{math|''B''&nbsp;\\&nbsp;''A''}} can be adjacent to at most two edges in {{math|''A''&nbsp;\\&nbsp;''B''}} because {{math|''A''}} is a matching; moreover each edge in {{math|''A''&nbsp;\\&nbsp;''B''}} is adjacent to an edge in {{math|''B''&nbsp;\\&nbsp;''A''}} by maximality of {{math|''B''}}, hence\n\n:<math>|A \\setminus B| \\le 2|B \\setminus A |.</math>\n\nFurther we deduce that\n\n:<math>|A| = |A \\cap B| + |A \\setminus B| \\le 2|B \\cap A| + 2|B \\setminus A| = 2|B|.</math>\n\nIn particular, this shows that any maximal matching is a 2-approximation of a maximum matching and also a 2-approximation of a minimum maximal matching. This inequality is tight: for example, if {{math|''G''}} is a path with 3 edges and 4 vertices, the size of a minimum maximal matching is 1 and the size of a maximum matching is 2.\n\n== Matching polynomials ==\n{{main|Matching polynomial}}\nA [[generating function]] of the number of ''k''-edge matchings in a graph is called a matching polynomial.  Let ''G'' be a graph and ''m<sub>k</sub>'' be the number of ''k''-edge matchings.  One matching polynomial of ''G'' is\n:<math>\\sum_{k\\geq0} m_k x^k.</math>\nAnother definition gives the matching polynomial as\n:<math>\\sum_{k\\geq0} (-1)^k m_k x^{n-2k},</math>\nwhere ''n'' is the number of vertices in the graph.  Each type has its uses; for more information see the article on matching polynomials.\n\n== Algorithms and computational complexity ==\n{{anchor|Bipartite matching}}\n\n=== Maximum matching ===\nA fundamental problem in [[combinatorial optimization]] is finding a ''maximum matching''. This problem has various algorithms for different classes of graphs: \n\n* In an ''unweighted bipartite graph'', the optimization problem is to find a [[maximum cardinality matching]]<ref name=\"Wes01\">{{citation|last=West|first=Douglas Brent|title=Introduction to Graph Theory|year=1999|at=Chapter 3|edition=2nd|publisher=Prentice Hall|isbn=0-13-014400-2}}</ref>. This problem is often called '''maximum bipartite matching'''<ref name=\"Wes01\" /> or '''maximum cardinality bipartite matching'''.  Micali and Vazirani's matching algorithm, the fastest general algorithm known so far, runs in time {{math|<var>O</var>({{radical|<var>V</var>}}<var>E</var>)}} time.<ref>{{citation\n | last1 = Micali | first1 = S. | author1-link = Silvio Micali\n | last2 = Vazirani | first2 = V. V. | author2-link = Vijay Vazirani\n | contribution = An <math>\\scriptstyle O(\\sqrt{|V|}\\cdot|E|)</math> algorithm for finding maximum matching in general graphs\n | doi = 10.1109/SFCS.1980.12\n | pages = 17–27\n | title = [[Symposium on Foundations of Computer Science|Proc. 21st IEEE Symp. Foundations of Computer Science]]\n | year = 1980}}.</ref> A randomised algorithm by Mucha and Sankowski,<ref name=\"Mucha\">{{citation|last1=Mucha|first1=M.|title=[[Symposium on Foundations of Computer Science|Proc. 45th IEEE Symp. Foundations of Computer Science]]|pages=248–255|year=2004|contribution=Maximum Matchings via Gaussian Elimination|contribution-url=http://www.mimuw.edu.pl/~mucha/pub/mucha_sankowski_focs04.pdf|last2=Sankowski|first2=P.}}</ref> based on the fast [[matrix multiplication]] algorithm, gives <math>O(V^{2.376})</math> complexity. For the special case of [[planar graph]]s the problem can be solved in time <math>O(n\\log^3 n)</math>.<ref>{{citation|last1=Borradaile|first1=Glencora|title=Multiple-source multiple-sink maximum flow in directed planar graphs in near-linear time|journal=[[SIAM Journal on Computing]]|volume=46|issue=4|pages=1280–1303|year=2017|arxiv=1105.2228|doi=10.1137/15M1042929|mr=3681377|last2=Klein|first2=Philip N.|last3=Mozes|first3=Shay|last4=Nussbaum|first4=Yahav|last5=Wulff–Nilsen|first5=Christian}}</ref>\n* In a [[weighted graph|''weighted'']] ''bipartite graph,'' the optimization problem is to find a maximum-weight matching; a dual problem is to find a minimum-weight matching. This problem is often called '''maximum weighted bipartite matching''', or the '''[[assignment problem]]'''. The [[Hungarian algorithm]] solves the assignment problem and it was one of the beginnings of combinatorial optimization algorithms. It uses a modified [[shortest path]] search in the augmenting path algorithm. If the [[Bellman–Ford algorithm]] is used for this step, the running time of the Hungarian algorithm becomes <math>O(V^2 E)</math>, or the edge cost can be shifted with a potential to achieve <math>O(V^2 \\log{V} + V E)</math> running time with the [[Dijkstra algorithm]] and [[Fibonacci heap]].<ref name=\"Fredman87\">{{citation|last1=Fredman|first1=Michael L.|title=Fibonacci heaps and their uses in improved network optimization algorithms|journal=[[Journal of the ACM]]|volume=34|issue=3|pages=596–615|year=1987|doi=10.1145/28869.28874|last2=Tarjan|first2=Robert Endre}}</ref>\n* In a ''non-bipartite weighted graph'', the problem of '''[[maximum weight matching]]''' can be solved in time  <math>O(V^{2}E)</math>using simply [[Edmonds's matching algorithm|Edmonds' blossom algorithm]].\n\n=== Maximal matchings ===\n\nA maximal matching can be found with a simple greedy algorithm. A maximum matching is also a maximal matching, and hence it is possible to find a ''largest'' maximal matching in polynomial time. However, no polynomial-time algorithm is known for finding a '''minimum maximal matching''', that is, a maximal matching that contains the ''smallest'' possible number of edges.\n\nNote that a maximal matching with ''k'' edges is an [[edge dominating set]] with ''k'' edges. Conversely, if we are given a minimum edge dominating set with ''k'' edges, we can construct a maximal matching with ''k'' edges in polynomial time. Therefore, the problem of finding a minimum maximal matching is essentially equal to the problem of finding a minimum edge dominating set.<ref>{{citation\n | first1=Mihalis | last1=Yannakakis\n | first2=Fanica | last2=Gavril\n | title=Edge dominating sets in graphs\n | journal=SIAM Journal on Applied Mathematics\n | year=1980\n | volume=38\n | pages=364–372\n | doi=10.1137/0138030\n | issue=3\n}}.</ref> Both of these two optimization problems are known to be [[NP-hard]]; the decision versions of these problems are classical examples of [[NP-complete]] problems.<ref>{{citation\n | last1=Garey | first1=Michael R. | authorlink1=Michael R. Garey\n | last2=Johnson | first2=David S. | authorlink2=David S. Johnson\n | year = 1979\n | title = [[Computers and Intractability: A Guide to the Theory of NP-Completeness]]\n | publisher = W.H. Freeman\n | isbn=0-7167-1045-5\n}}. Edge dominating set (decision version) is discussed under the dominating set problem, which is the problem GT2 in Appendix&nbsp;A1.1. Minimum maximal matching (decision version) is the problem GT10 in Appendix&nbsp;A1.1.</ref> Both problems can be [[approximation algorithm|approximated]] within factor 2 in polynomial time: simply find an arbitrary maximal matching ''M''.<ref>{{citation\n | last1=Ausiello | first1=Giorgio\n | last2=Crescenzi | first2=Pierluigi\n | last3=Gambosi | first3=Giorgio\n | last4=Kann | first4=Viggo\n | last5=Marchetti-Spaccamela | first5=Alberto\n | last6=Protasi | first6=Marco\n | title=Complexity and Approximation: Combinatorial Optimization Problems and Their Approximability Properties\n | publisher=Springer\n | year=2003\n}}. Minimum edge dominating set (optimisation version) is the problem GT3 in Appendix B (page 370). Minimum maximal matching (optimisation version) is the problem GT10 in Appendix B (page 374). See also [http://www.nada.kth.se/~viggo/wwwcompendium/node13.html Minimum Edge Dominating Set] and [http://www.nada.kth.se/~viggo/wwwcompendium/node21.html Minimum Maximal Matching] in the [http://www.nada.kth.se/~viggo/wwwcompendium/ web compendium].</ref>\n\n=== Counting problems ===\n{{main|Hosoya index}}\nThe number of matchings in a graph is known as the [[Hosoya index]] of the graph. It is [[Sharp-P-complete|#P-complete]] to compute this quantity, even for bipartite graphs.<ref>[[Leslie Valiant]], ''The Complexity of Enumeration and Reliability Problems'',  SIAM J. Comput., 8(3), 410–421</ref> It is also #P-complete to count perfect matchings, even in [[bipartite graph]]s, because computing the [[Permanent (mathematics)|permanent]] of an arbitrary 0–1 matrix (another #P-complete problem) is the same as computing the number of perfect matchings in the bipartite graph having the given matrix as its [[biadjacency matrix]]. However, there exists a fully polynomial time randomized approximation scheme for counting the number of bipartite matchings.<ref>{{cite journal\n| last1        = Bezáková\n| first1       = Ivona\n| last2        = Štefankovič\n| first2       = Daniel\n| last3        = Vazirani\n| first3       = Vijay V.\n| authorlink3  = Vijay Vazirani\n| last4        = Vigoda\n| first4       = Eric\n| year         = 2008\n| title        = Accelerating Simulated Annealing for the Permanent and Combinatorial Counting Problems\n| journal      = [[SIAM Journal on Computing]]\n| volume       = 37\n| issue        = 5\n| pages        = 1429–1454\n| doi          = 10.1137/050644033\n| citeseerx= 10.1.1.80.687\n}}</ref> A remarkable theorem of [[Pieter Kasteleyn|Kasteleyn]] states that the number of perfect matchings in a [[planar graph]] can be computed exactly in polynomial time via the [[FKT algorithm]].\n\nThe number of perfect matchings in a [[complete graph]] ''K''<sub>''n''</sub> (with ''n'' even) is given by the [[double factorial]] (''n''&nbsp;&minus;&nbsp;1)!!.<ref>{{citation|title=A combinatorial survey of identities for the double factorial|first=David|last=Callan|arxiv=0906.1317|year=2009|bibcode=2009arXiv0906.1317C}}.</ref> The numbers of matchings in complete graphs, without constraining the matchings to be perfect, are given by the [[Telephone number (mathematics)|telephone number]]s.<ref>{{citation\n | last1 = Tichy | first1 = Robert F.\n | last2 = Wagner | first2 = Stephan\n | doi = 10.1089/cmb.2005.12.1004\n | issue = 7\n | journal = [[Journal of Computational Biology]]\n | pages = 1004–1013\n | title = Extremal problems for topological indices in combinatorial chemistry\n | url = http://www.math.tugraz.at/fosp/pdfs/tugraz_main_0052.pdf\n | volume = 12\n | year = 2005}}.</ref>\n\n=== Finding all maximally-matchable edges ===\nOne of the basic problems in matching theory is to find in a given graph all edges that may be extended to a maximum matching\nin the graph. (Such edges are called '''maximally-matchable''' edges, or '''allowed''' edges.) The best deterministic algorithm for solving this problem in general graphs runs in time <math>O(VE)</math>\n.<ref>{{citation\n | first1=Marcelo H.| last1=de Carvalho | first2=Joseph| last2=Cheriyan | contribution=An <math>O(VE)</math> algorithm for ear decompositions of matching-covered graphs\n | title=Proc. ACM/SIAM Symposium on Discrete Algorithms (SODA)\n | year=2005\n | pages=415–423 }}.</ref>\nThere exists a randomized algorithm that solves this problem in time <math>\\tilde{O}(V^{2.376}) </math>\n.<ref>{{citation\n | first1=Michael O. | last1=Rabin\n | first2=Vijay V. | last2=Vazirani  | title=Maximum matchings in general graphs through randomization | journal=Journal of Algorithms\n | year=1989 | volume=10\n | pages=557–567 | doi=10.1016/0196-6774(89)90005-9}}.</ref>\nIn the case of bipartite graphs, it is possible to find a single maximum matching and then use it in order to find all\nmaximally-matchable edges in linear time;<ref>{{citation\n | first1=Tamir | last1=Tassa | title=Finding all maximally-matchable edges in a bipartite graph| year=2012 | journal=[[Theoretical Computer Science (journal)|Theoretical Computer Science]] | volume = 423 | pages = 50–58 | doi = 10.1016/j.tcs.2011.12.071}}.</ref>\nthe resulting overall runtime is <math>O(V^{1/2}E)</math> for general bipartite\ngraphs and <math>O((V/\\log V)^{1/2}E)</math> for dense bipartite graphs with <math>E=\\Theta(V^2)</math>. In cases where\none of the maximum matchings is known upfront,<ref>{{citation\n | first1=Aris| last1=Gionis | first2=Arnon| last2=Mazza | first3=Tamir |last3=Tassa | contribution=''k''-Anonymization revisited\n | title=[[International Conference on Data Engineering (ICDE)]]\n | year=2008\n | pages=744–753}}.</ref> the overall runtime of the algorithm is <math>O(V+E)</math>.\n\n== Online bipartite matching ==\nThe problem of developing an [[online algorithm]] for matching was first considered by Karp et al.<ref>{{cite journal |title=An Optimal Algorithm for On-line Bipartite Matching |url=https://people.eecs.berkeley.edu/~vazirani/pubs/online.pdf}}</ref> In the online setting, nodes on one side of the bipartite graph arrive one at a time and must either be immediately matched to the other side of the graph or discarded. This is a natural generalization of the [[secretary problem]] and has applications to online ad auctions.<ref>{{cite book |title=Online Matching and Ad Allocation |url=http://www.cs.cmu.edu/~arielpro/15896s15/docs/paper13b.pdf |accessdate=285-299}}</ref> The best online algorithm, for the unweighted maximization case with a random arrival model, attains a [[Competitive analysis (online algorithm)|competitive ratio]] of <math>1 - \\frac{1}{e}</math>.<ref>{{cite book |title=Online Matching and Ad Allocation |url=http://www.cs.cmu.edu/~arielpro/15896s15/docs/paper13b.pdf |accessdate=285-299}}</ref>\n\n== Characterizations and notes ==\n\n[[Kőnig's theorem (graph theory)|Kőnig's theorem]] states that, in bipartite graphs, the maximum matching is equal in size to the minimum [[vertex cover]]. Via this result, the minimum vertex cover, [[maximum independent set]], and [[maximum vertex biclique]] problems may be solved in [[polynomial time]] for bipartite graphs.\n\n[[Hall's marriage theorem]] provides a characterization of bipartite graphs which have a perfect matching and the [[Tutte theorem]] provides a characterization for arbitrary graphs.\n\nA perfect matching is a spanning [[Regular graph|1-regular]] subgraph, a.k.a. a [[1-factor]]. In general, a spanning ''k''-regular subgraph is a [[Factor (graph theory)|''k''-factor]].\n\n== Applications ==\n\n=== Matching in general graphs ===\n* A '''Kekulé structure''' of an [[Aromaticity|aromatic compound]] consists of a perfect matching of its [[skeletal formula|carbon skeleton]], showing the locations of [[double bond]]s in the [[chemical structure]]. These structures are named after [[Friedrich August Kekulé von Stradonitz]], who showed that [[benzene]] (in graph theoretical terms, a 6-vertex cycle) can be given such a structure.<ref>See, e.g., {{citation|title=On some solved and unsolved problems of [[chemical graph theory]]|last1=Trinajstić|first1=Nenad|authorlink=Nenad Trinajstić|last2=Klein|first2=Douglas J.|last3=Randić|first3=Milan |authorlink3=Milan Randić|journal=International Journal of Quantum Chemistry|year=1986|volume=30|issue=S20|pages=699–742|doi=10.1002/qua.560300762}}.</ref>\n* The [[Hosoya index]] is the number of non-empty matchings plus one; it is used in [[computational chemistry]] and [[mathematical chemistry]] investigations for organic compounds.\n\n=== Matching in bipartite graphs ===\n* [http://community.topcoder.com/stat?c=problem_statement&pm=2852&rd=5075 Graduation problem] is about choosing minimum set of classes from given requirements for graduation.\n* [[Transportation theory (mathematics)|Hitchcock transport problem]] involves bipartite matching as sub-problem.\n* [[Subgraph isomorphism problem|Subtree isomorphism]] problem involves bipartite matching as sub-problem.\n\n== See also ==\n* [[Dulmage–Mendelsohn decomposition]], a partition of the vertices of a bipartite graph into subsets such that each edge belongs to a perfect matching if and only if its endpoints belong to the same subset\n* [[Edge coloring]], a partition of the edges of a graph into matchings\n* [[Matching preclusion]], the minimum number of edges to delete to prevent a perfect matching from existing\n* [[Rainbow matching]], a matching in an edge-colored bipartite graph with no repeated colors\n* [[Skew-symmetric graph]], a type of graph that can be used to model alternating path searches for matchings\n* [[Stable matching]], a matching in which no two elements prefer each other to their matched partners\n* [[Vertex independent set]], a set of vertices (rather than edges) no two of which are adjacent to each other\n* [[Stable marriage problem]] (also known as stable matching problem)\n\n== References ==\n{{Reflist|2}}\n\n== Further reading ==\n#{{Citation\n  | author1 = László Lovász\n  | authorlink1 = László Lovász\n  | author2 = M. D. Plummer | author2-link = Michael D. Plummer\n  | title = Matching Theory\n  | publisher = North-Holland\n  | year = 1986\n  | isbn = 0-444-87916-1\n}}\n#{{Citation\n | author = [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]] and [[Clifford Stein]]\n | title = [[Introduction to Algorithms]]\n | publisher = MIT Press and McGraw–Hill\n | year = 2001\n | isbn = 0-262-53196-8\n | edition = second\n | at = Chapter 26, pp. 643&ndash;700\n}}\n#{{cite techreport\n | author = [[András Frank]]\n | url = http://www.cs.elte.hu/egres/tr/egres-04-14.pdf\n | title = On Kuhn's Hungarian Method – A tribute from Hungary\n | institution = Egerváry Research Group\n | year = 2004\n}}\n#{{Citation\n | author = [[Michael L. Fredman]] and [[Robert E. Tarjan]]\n | title = Fibonacci heaps and their uses in improved network optimization algorithms\n | journal = [[Journal of the ACM]]\n | volume = 34\n | year = 1987\n | pages = 595&ndash;615\n | doi = 10.1145/28869.28874\n | issue = 3\n | postscript = .\n}}\n#{{Citation\n |author1=S. J. Cyvin   |author2=Ivan Gutman\n  |lastauthoramp=yes | title = Kekule Structures in Benzenoid Hydrocarbons\n | publisher = Springer-Verlag\n | year = 1988\n}}\n#{{Citation\n | author = [[Marek Karpinski]] and Wojciech Rytter\n | title = Fast Parallel Algorithms for Graph Matching Problems\n | publisher = Oxford University Press\n | year = 1998\n | isbn = 978-0-19-850162-6\n}}\n\n== External links ==\n* [http://lemon.cs.elte.hu/ A graph library with Hopcroft–Karp and Push–Relabel-based maximum cardinality matching implementation ]\n\n{{Authority control}}\n\n[[Category:Matching| ]]\n[[Category:Combinatorial optimization]]\n[[Category:Polynomial-time problems]]\n[[Category:Computational problems in graph theory]]"
    },
    {
      "title": "Matroid intersection",
      "url": "https://en.wikipedia.org/wiki/Matroid_intersection",
      "text": "In [[combinatorial optimization]], the '''matroid intersection''' problem is to find a largest common independent set in two [[matroid]]s over the same ground set. If the elements of the matroid are assigned real weights, the weighted matroid intersection problem is to find a common independent set with the maximum possible weight. These problems generalize many problems in combinatorial optimization including finding [[maximum matching]]s and [[maximum weight matching]]s in [[bipartite graph]]s and finding [[Arborescence (graph theory)|arborescence]]s in [[directed graph]]s.\n\nThe '''matroid intersection theorem''', due to [[Jack Edmonds]], says that there is always a simple upper bound certificate, consisting of a partitioning of the ground set amongst the two matroids, whose value (sum of respective [[Matroid rank|ranks]]) equals the size of a maximum common independent set. Based on this theorem, the matroid intersection problem for two matroids can be solved in polynomial time using [[matroid partitioning]] algorithms.\n\n==Example==\nLet ''G''&nbsp;=&nbsp;(''U'',''V'',''E'') be a [[bipartite graph]]. One may define a [[partition matroid]] ''M<sub>U</sub>'' on the ground set ''E'', in which a set of edges is independent if no two of the edges have the same endpoint in ''U''. Similarly one may define a matroid ''M<sub>V</sub>'' in which a set of edges is independent if no two of the edges have the same endpoint in ''V''. Any set of edges that is independent in both ''M<sub>U</sub>'' and ''M<sub>V</sub>'' has the property that no two of its edges share an endpoint; that is, it is a [[Matching (graph theory)|matching]]. Thus, the largest common independent set of ''M<sub>U</sub>'' and ''M<sub>V</sub>'' is a [[maximum matching]] in ''G''.\n\n==Extension==\n\nThe matroid intersection problem becomes [[NP-hard]] when three matroids are involved, instead of only two.\n\nOne proof of this hardness result uses a [[Reduction (complexity)|reduction]] from the [[Hamiltonian path]] problem in [[directed graph]]s. Given a directed graph ''G'' with ''n'' vertices, and specified nodes ''s'' and ''t'', the Hamiltonian path problem is the problem of determining whether there exists a simple path of length ''n''&nbsp;&minus;&nbsp;1 that starts at ''s'' and ends at ''t''. It may be assumed without loss of generality that ''s'' has no incoming edges and ''t'' has no outgoing edges. Then, a Hamiltonian path exists if and only if there is a set of ''n''&nbsp;&minus;&nbsp;1 elements in the intersection of three matroids on the edge set of the graph: two partition matroids ensuring that the in-degree and out-degree of the selected edge set are both at most one, and the [[graphic matroid]] of the [[undirected graph]] formed by forgetting the edge orientations in ''G'', ensuring that the selected edge set has no cycles {{harv|Welsh|2010}}.\n\nAnother computational problem on matroids, the [[matroid parity problem]], was formulated by {{harvtxt|Lawler|1976}} as a common generalization of matroid intersection and non-bipartite graph matching.\nHowever, although it can be solved in polynomial time for [[linear matroid]]s, it is NP-hard for other matroids, and requires exponential time in the [[matroid oracle]] model {{harv|Jensen|Korte|1982}}.\n\n==References==\n*{{citation|first1=Carl|last1=Brezovec|first2=Gérard|last2=Cornuéjols|author2-link= Gérard Cornuéjols |first3=Fred|last3=Glover|title=Two algorithms for weighted matroid intersection|journal=Mathematical Programming|volume=36|issue=1|year=1986|pages=39–53|doi=10.1007/BF02591988}}.\n*{{citation|doi=10.1090/S0002-9947-1971-0286689-5|first1=Martin|last1=Aigner|first2=Thomas|last2=Dowling|title=Matching theory for combinatorial geometries|journal=Transactions of the American Mathematical Society|volume=158|year=1971|issue=1|pages=231–245}}.\n*{{citation\n | last = Edmonds\n | first = Jack\n | contribution = Submodular functions, matroids, and certain polyhedra\n | editor = R. Guy |editor2=H. Hanam |editor3=N. Sauer |editor4=J. Schonheim\n | title = Combinatorial structures and their applications (Proc. 1969 Calgary Conference)\n | pages = 69&ndash;87\n | publisher = Gordon and Breach, New York\n | year = 1970\n}}. Reprinted in M. Jünger et al. (Eds.): Combinatorial Optimization (Edmonds Festschrift), LNCS 2570, pp. 1126, Springer-Verlag, 2003.\n*{{citation|last=Frank|first=András|year=1981|title=A weighted matroid intersection algorithm|journal=Journal of Algorithms|volume=2|issue=4|pages=328–336|doi=10.1016/0196-6774(81)90032-8}}.\n*{{citation|first1=Greg N.|last1=Frederickson|first2=Mandayam A.|last2=Srinivas|title=Algorithms and data structures for an expanded family of matroid intersection problems|journal=SIAM Journal on Computing|volume=18|issue=1|year=1989|pages=112–138|doi=10.1137/0218008|url=http://www.dtic.mil/get-tr-doc/pdf?AD=ADA191482}}.\n*{{citation|last1=Gabow|first1=Harold N.|last2=Tarjan|first2=Robert E.|author2-link=Robert Tarjan|title=Efficient algorithms for a family of matroid intersection problems|journal=Journal of Algorithms|doi=10.1016/0196-6774(84)90042-7|volume=5|issue=1|year=1984|pages=80–131}}.\n*{{citation\n | last1 = Jensen | first1 = Per M.\n | last2 = Korte | first2 = Bernhard | author2-link = Bernhard Korte\n | doi = 10.1137/0211014\n | issue = 1\n | journal = [[SIAM Journal on Computing]]\n | mr = 646772\n | pages = 184–190\n | title = Complexity of matroid property algorithms\n | volume = 11\n | year = 1982}}.\n*{{citation|last=Lawler|first=Eugene L.|authorlink=Eugene Lawler|title=Matroid intersection algorithms|journal=Mathematical Programming|volume=9|issue=1|year=1975|doi=10.1007/BF01681329|pages=31–56}}.\n*{{citation\n | last = Lawler | first = Eugene L. | authorlink = Eugene Lawler\n | contribution = Chapter 9: The Matroid Parity Problem\n | contribution-url = https://books.google.com/books?id=MTuoAAAAQBAJ&pg=PA356\n | location = New York\n | mr = 0439106\n | pages = 356–367\n | publisher = Holt, Rinehart and Winston\n | title = Combinatorial Optimization: Networks and Matroids\n | year = 1976}}.\n*{{citation\n | last = Welsh | first = D. J. A. | authorlink = Dominic Welsh\n | isbn = 9780486474397\n | page = 131\n | publisher = Courier Dover Publications\n | title = Matroid Theory\n | year = 2010 | origyear = 1976}}.\n\n[[Category:Combinatorial optimization]]\n[[Category:Matroid theory|Intersection]]"
    },
    {
      "title": "Matroid parity problem",
      "url": "https://en.wikipedia.org/wiki/Matroid_parity_problem",
      "text": "[[File:Graphic matroid parity.svg|thumb|upright=1.35|An instance of the matroid parity problem on a [[graphic matroid]]: given a graph with colored edges, having exactly two edges per color, find as large a forest as possible that again has exactly two edges per color.]]\nIn [[combinatorial optimization]], the '''matroid parity problem''' is a problem of finding the largest independent set of paired elements in a [[matroid]].{{r|cll}} The problem was formulated by {{harvtxt|Lawler|1976}} as a common generalization of [[graph matching]] and [[matroid intersection]].{{r|cll|el}} It is also known as [[polymatroid]] matching, or the '''matchoid problem'''.{{r|ll}}\n\nMatroid parity can be solved in [[polynomial time]] for [[linear matroid]]s. However, it is [[NP-hard]] for certain compactly-represented matroids, and requires more than a polynomial number of steps in the [[matroid oracle]] model.{{r|cll|jk}}\n\nApplications of matroid parity algorithms include finding [[planarization|large planar subgraphs]]{{r|cffk}} and finding [[graph embedding]]s of [[Xuong tree|maximum genus]].{{r|fgm}} These algorithms can also be used to find [[connected dominating set]]s and [[feedback vertex set]]s in graphs of maximum degree three.{{r|ukg}}\n\n==Formulation==\nA [[matroid]] can be defined from a [[finite set]] of elements and from a notion of what it means for subsets of elements to be independent, subject to the following constraints:\n*Every subset of an independent set should be independent.\n*If <math>S</math> and <math>T</math> are independent sets, with <math>|T|>|S|</math>, then there exists an element <math>t\\in T</math> such that <math>S\\cup\\{t\\}</math> is independent.{{r|cll}}\n\nExamples of matroids include the [[linear matroid]]s (in which the elements are vectors in a [[vector space]], with [[linear independence]]), the [[graphic matroid]]s (in which the elements are edges in an [[undirected graph]], independent when they contain no [[Cycle (graph theory)|cycle]]), and the [[partition matroid]]s (in which the elements belong to a family of [[disjoint sets]], and are independent when they contain at most one element in each set). Graphic matroids and partition matroids are special cases of linear matroids.{{r|cll}}\n\nIn the matroid parity problem, the input consists of a matroid together with a pairing on its elements, so that each element belongs to one pair. The goal is to find a subset of the pairs, as large as possible, so that the union of the pairs in the chosen subset is independent.{{r|cll|el}} Another seemingly more general variation, in which the allowable pairs form a graph rather than having only one pair per element, is equivalent: an element appearing in more than one pair could be replaced by multiple copies of the element, one per pair.{{r|lsv}}\n\n==Algorithms==\nThe matroid parity problem for linear matroids can be solved by a [[randomized algorithm]] in time <math>O(nr^{\\omega-1})</math>, where <math>n</math> is the number of elements of the matroid, <math>r</math> is its [[Matroid rank|rank]] (the size of the largest independent set), and <math>\\omega</math> is the exponent in the time bounds for [[fast matrix multiplication]].{{r|cll}}\nIn particular, using a matrix multiplication algorithm of Le Gall,{{r|lg}} it can be solved in time <math>O(nr^{1.3729})</math>.\nWithout using fast matrix multiplication, the linear matroid parity problem can be solved in time <math>O(nr^2)</math>.{{r|cll}}\n\nThese algorithms are based on a [[linear algebra]] formulation of the problem by {{harvtxt|Geelen|Iwata|2005}}. Suppose that an input to the problem consists of <math>m</math> pairs of <math>r</math>-dimensional vectors (arranged as [[column vector]]s in a [[Matrix (mathematics)|matrix]] <math>M</math> of size <math>r\\times 2m</math>). Then the number of pairs in the optimal solution is\n\n:<math>\\frac{1}{2}\\operatorname{rank}\\begin{pmatrix}0&M\\\\M^T&T\\end{pmatrix} -m,</math>\n\nwhere <math>T</math> is a [[block diagonal matrix]] whose blocks are <math>2\\times 2</math> submatrices of the form\n\n:<math>\\begin{pmatrix}0&t_i\\\\-t_i&0\\end{pmatrix}</math>\n\nfor a sequence of variables <math>t_1,\\dots t_m</math>.{{r|gi}} The [[Schwartz–Zippel lemma]] can be used to test whether this matrix has full rank or not (that is, whether the solution has size <math>r/2</math> or not), by assigning random values to the variables <math>t_i</math> and testing whether the resulting matrix has [[determinant]] zero. By applying a [[greedy algorithm]] that removes pairs one at a time by setting their indeterminates to zero as long as the matrix remains of full rank (maintaining the inverse matrix using the [[Sherman–Morrison formula]] to check the rank after each removal), one can find a solution whenever this test shows that it exists. Additional methods extend this algorithm to the case that the optimal solution to the matroid parity problem has fewer than <math>r/2</math> pairs.{{r|cll}}\n\nFor graphic matroids, more efficient algorithms are known, with running time <math>O(mn\\log^6 n)</math> on graphs with <math>m</math> vertices and <math>n</math> edges.{{r|gs}}\nFor [[simple graph]]s, <math>m</math> is <math>O(n^2)</math>, but for [[multigraph]]s, it may be larger, so it is also of interest to have algorithms with smaller or no dependence on <math>m</math> and worse dependence on <math>n</math>. In these cases, it is also possible to solve the graphic matroid parity problem in randomized expected time <math>O(n^4)</math>, or in time <math>O(n^3)</math> when each pair of edges forms a path.{{r|cll}}\n\nAlthough the matroid parity problem is [[NP-hard]] for arbitrary matroids, it can still be approximated efficiently. Simple [[Local search (optimization)|local search algorithm]]s provide a [[polynomial-time approximation scheme]] for this problem, and find solutions whose size, as a fraction of the optimal solution size, is arbitrarily close to one. The algorithm starts with the [[empty set]] as its solution, and repeatedly attempts to increase the solution size by one by removing at most a constant number <math>C</math> of pairs from the solution and replacing them by a different set with one more pair. When no more such moves are possible, the resulting solution is returned as the approximation to the optimal solution. To achieve an approximation ratio of <math>1-\\epsilon</math>, it suffices to choose <math>C</math> to be approximately <math>5^{\\lceil 1/2\\epsilon\\rceil}</math>.{{r|lsv}}\n\n==Applications==\nMany other optimization problems can be formulated as linear matroid parity problems, and solved in polynomial time using this formulation.\n\n{{glossary}}\n\n{{term|Graph matching}}\n{{defn|A [[maximum matching]] in a graph is a subset of edges, no two sharing an endpoint, that is as large as possible. It can be formulated as a matroid parity problem in a partition matroid that has an element for each vertex-edge incidence in the graph. In this matroid, two elements are paired if they are the two incidences for the same edge as each other. A subset of elements is independent if it has at most one incidence for each vertex of the graph. The pairs of elements in a solution to the matroid parity problem for this matroid are the incidences between edges in a maximum matching and their endpoints.{{r|el}}}}\n\n{{term|Matroid intersection}}\n{{defn|An instance of the matroid intersection problem consists of two matroids on the same set of elements; the goal is to find a subset of the elements that is as large as possible and is independent in both matroids. To formulate matroid intersection as a matroid parity problem, construct a new matroid whose elements are the disjoint union of two copies of the given elements, one for each input matroid. In the new matroid, a subset is independent if its restriction to each of the two copies is independent in each of the two matroids, respectively. Pair the elements of the new matroid so that each element is paired with its copy. The pairs of elements in a solution to the matroid parity problem for this matroid are the two copies of each element in a solution to the matroid intersection problem.{{r|el}}}}\n\n{{term|Large planar subgraphs}}\n{{defn|<p>In an arbitrary graph, the problem of finding the largest set of triangles in a given graph, with no cycles other than the chosen triangles, can be formulated as a matroid parity problem on a graphic matroid whose elements are edges of the graph, with one pair of edges per triangle (duplicating edges if necessary when they belong to more than one triangle). The pairs of elements in a solution to the matroid parity problem for this matroid are the two edges in each triangle of an optimal set of triangles.\nThe same problem can also be described as one of finding the largest [[Hypergraph|Berge-acyclic]] sub-hypergraph of a 3-uniform [[hypergraph]]. In the hypergraph version of the problem, the hyper-edges are the triangles of the given graph.{{r|ll}}</p>\n\n<p>A [[cactus graph]] is a graph in which each two cycles have at most one vertex in common. As a special case, the graphs in which each cycle is a triangle are necessarily cactus graphs. The largest triangular cactus in the given graph\ncan then be found by adding additional edges from a [[spanning tree]], without creating any new cycles, so that the resulting subgraph has the same [[connected component (graph theory)|connected components]] as the original graph. Cactus graphs are automatically [[planar graph]]s, and the problem of finding triangular cactus graphs forms the basis for the best known [[approximation algorithm]] to the problem of finding the largest planar subgraph of a given graph, an important step in [[planarization]]. The largest triangular cactus always has at least 4/9 the number of edges of the largest planar subgraph, improving the 1/3 approximation ratio obtained by using an arbitrary spanning tree.{{r|cffk}}</p>}}\n\n{{term|Combinatorial rigidity}}\n{{defn|A framework of rigid bars in the [[Euclidean plane]], connected at their endpoints at flexible joints, can be fixed into a single position in the plane by pinning some of its joints to points of the plane. The minimum number of joints that need to be pinned to fix the framework is called its pinning number. It can be computed from a solution to an associated matroid parity problem.{{r|ll}}}}\n\n{{term|Maximum-genus embeddings}}\n{{defn|[[File:Xuong tree.svg|thumb|A [[Xuong tree]]]]\n<p>A [[graph embedding|cellular embedding]] of a given graph onto a surface of the maximum possible [[genus (mathematics)|genus]] can be obtained from a [[Xuong tree]] of the graph. This is a spanning tree with the property that, in the subgraph of edges not in the tree, the number of [[Connected component (graph theory)|connected components]] with an odd number of edges is as small as possible.</p>\n\n<p>To formulate the problem of finding a Xuong tree as a matroid parity problem, first subdivide each edge <math>e</math> of the given graph into a path, with the length of the path equal to the number of other edges incident to <math>e</math>. Then, pair the edges of the subdivided graph, so that each pair of edges in the original graph is represented by a single pair of edges in the subdivided graph, and each edge in the subdivided graph is paired exactly once. Solve a matroid parity problem with the paired edges of the subdivided graph, using its cographic matroid (a linear matroid in which a subset of edges is independent if its removal does not separate the graph). Any spanning tree of the original graph that avoids the edges used in the matroid parity solution is necessarily a Xuong tree.{{r|fgm}}</p>}}\n\n{{term|Connected domination}}\n{{defn|A [[connected dominating set]] in a graph is a subset of vertices whose [[induced subgraph]] is connected, adjacent to all other vertices.\nIt is NP-hard to find the smallest connected dominating set in arbitrary graphs,\nbut can be found in polynomial time for graphs of maximum degree three.\nIn a [[cubic graph]], one can replace each vertex by a two-edge path connected to the ends of its three endpoints, and formulate a matroid parity problem on the pairs of edges generated in this way, using the cographic matroid of the expanded graph. The vertices whose paths are not used in the solution form a minimum connected dominating set. In a graph of maximum degree three, some simple additional transformations reduce the problem to one on a cubic graph.{{r|ukg}}}}\n\n{{term|Feedback vertex set}}\n{{defn|A [[feedback vertex set]] in a graph is a subset of vertices that touches all cycles. In cubic graphs, there is a linear equation relating the number of vertices, [[cyclomatic number]], number of connected components, size of a minimum connected dominating set, and size of a minimum feedback vertex set.{{r|ds}} It follows that the same matroid parity problem used to find connected dominating sets can also be used to find feedback vertex sets in graphs of maximum degree three.{{r|ukg}}}}\n\n{{glossary end}}\n\n==Hardness==\nThe [[clique problem]], of finding a <math>k</math>-vertex [[Clique (graph theory)|complete subgraph]] in a given <math>n</math>-vertex graph <math>G</math>, can be transformed into an instance of matroid parity as follows.\nConstruct a [[paving matroid]] on <math>2n</math> elements, paired up so that there is one pair of elements per pair of vertices. Define a subset <math>S</math> of these elements to be independent if it satisfies any one of the following three conditions:\n*<math>S</math> has fewer than <math>2k</math> elements.\n*<math>S</math> has exactly <math>2k</math> elements, but is not the union of <math>k</math> pairs of elements.\n*<math>S</math> is the union of <math>k</math> pairs of elements that form a clique in <math>G</math>.\nThen there is a solution to the matroid parity problem for this matroid, of size <math>2k</math>, if and only if <math>G</math> has a clique of size <math>k</math>. Since finding cliques of a given size is NP-complete, it follows that determining whether this type of matrix parity problem has a solution of size <math>2k</math> is also NP-complete.{{r|js}}\n\nThis problem transformation does not depend on the structure of the clique problem in any deep way, and would work for any other problem of finding size-<math>k</math> subsets of a larger set that satisfy a computable test. By applying it to a randomly-permuted graph that contains exactly one clique of size <math>k</math>, one can show that any deterministic or randomized algorithm for matroid parity that accesses its matroid only by independence tests needs to make an exponential number of tests.{{r|jk}}\n\n==References==\n{{reflist|refs=\n\n<ref name=cffk>{{citation\n | last1 = Călinescu | first1 = Gruia\n | last2 = Fernandes | first2 = Cristina G.\n | last3 = Finkler | first3 = Ulrich\n | last4 = Karloff | first4 = Howard\n | doi = 10.1006/jagm.1997.0920\n | issue = 2\n | journal = Journal of Algorithms\n | mr = 1622397\n | pages = 269–302\n | title = A better approximation algorithm for finding planar subgraphs\n | volume = 27\n | year = 1998| citeseerx = 10.1.1.47.4731\n }}.</ref>\n \n<ref name=cll>{{citation\n | last1 = Cheung | first1 = Ho Yee\n | last2 = Lau | first2 = Lap Chi\n | last3 = Leung | first3 = Kai Man\n | doi = 10.1145/2601066\n | issue = 3\n | journal = [[ACM Transactions on Algorithms]]\n | mr = 3233690\n | pages = 10:1–10:26\n | title = Algebraic algorithms for linear matroid parity problems\n | url = https://cs.uwaterloo.ca/~lapchi/papers/parity.pdf\n | volume = 10\n | year = 2014| citeseerx = 10.1.1.194.604\n }}</ref>\n\n<ref name=ds>{{citation\n | last = Speckenmeyer | first = E.\n | contribution = Bounds on feedback vertex sets of undirected cubic graphs\n | mr = 875903\n | pages = 719–729\n | publisher = North-Holland | location = Amsterdam\n | series = Colloquia Mathematica Societatis János Bolyai\n | title = Algebra, Combinatorics and Logic in Computer Science, Vol. I, II (Győr, 1983)\n | volume = 42\n | year = 1986}}</ref>\n\n<ref name=el>{{citation\n | last = Lawler | first = Eugene L. | authorlink = Eugene Lawler\n | contribution = Chapter 9: The Matroid Parity Problem\n | contribution-url = https://books.google.com/books?id=MTuoAAAAQBAJ&pg=PA356\n | location = New York\n | mr = 0439106\n | pages = 356–367\n | publisher = Holt, Rinehart and Winston\n | title = Combinatorial Optimization: Networks and Matroids\n | year = 1976}}</ref>\n\n<ref name=fgm>{{citation\n | last1 = Furst | first1 = Merrick L.\n | last2 = Gross | first2 = Jonathan L.\n | last3 = McGeoch | first3 = Lyle A.\n | doi = 10.1145/44483.44485\n | issue = 3\n | journal = [[Journal of the ACM]]\n | mr = 963159\n | pages = 523–534\n | title = Finding a maximum-genus graph imbedding\n | volume = 35\n | year = 1988}}</ref>\n\n<ref name=gi>{{citation\n | last1 = Geelen | first1 = James | author1-link = Jim Geelen\n | last2 = Iwata | first2 = Satoru\n | doi = 10.1007/s00493-005-0013-7\n | issue = 2\n | journal = [[Combinatorica]]\n | mr = 2127610\n | pages = 187–215\n | title = Matroid matching via mixed skew-symmetric matrices\n | volume = 25\n | year = 2005| citeseerx = 10.1.1.702.5431 }}</ref>\n\n<ref name=gs>{{citation\n | last1 = Gabow | first1 = Harold N.\n | last2 = Stallmann | first2 = Matthias\n | editor-last = Brauer | editor-first = Wilfried\n | contribution = Efficient algorithms for graphic matroid intersection and parity (extended abstract)\n | doi = 10.1007/BFb0015746\n | location = Berlin\n | mr = 819256\n | pages = 210–220\n | publisher = Springer\n | series = Lecture Notes in Computer Science\n | title = 12th International Colloquium on Automata, Languages, and Programming (ICALP), Nafplion, Greece, July 15–19, 1985\n | volume = 194\n | year = 1985| isbn = 978-3-540-15650-5\n }}</ref>\n\n<ref name=jk>{{citation\n | last1 = Jensen | first1 = Per M.\n | last2 = Korte | first2 = Bernhard | author2-link = Bernhard Korte\n | doi = 10.1137/0211014\n | issue = 1\n | journal = [[SIAM Journal on Computing]]\n | mr = 646772\n | pages = 184–190\n | title = Complexity of matroid property algorithms\n | volume = 11\n | year = 1982}}</ref>\n\n<ref name=js>{{citation\n | last = Soto | first = José A.\n | arxiv = 1102.3491\n | doi = 10.1016/j.dam.2012.10.019\n | issue = part 2\n | journal = [[Discrete Applied Mathematics]]\n | mr = 3159127\n | pages = 406–412\n | title = A simple PTAS for weighted matroid matching on strongly base orderable matroids\n | volume = 164\n | year = 2014}}</ref>\n\n<ref name=lg>{{citation\n | last = Le Gall | first = François\n | contribution = Powers of tensors and fast matrix multiplication\n | doi = 10.1145/2608628.2608664\n | location = New York\n | mr = 3239939\n | pages = 296–303\n | publisher = ACM\n | title = Proceedings of the 39th International Symposium on Symbolic and Algebraic Computation (ISSAC 2014)\n | year = 2014| isbn = 9781450325011\n }}</ref>\n\n<ref name=ll>{{citation\n | last = Lovász | first = L. | authorlink = László Lovász\n | doi = 10.1016/0095-8956(80)90066-0\n | issue = 2\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 572475\n | pages = 208–236\n | series = Series B\n | title = Matroid matching and some applications\n | volume = 28\n | year = 1980}}</ref>\n\n<ref name=lsv>{{citation\n | last1 = Lee | first1 = Jon\n | last2 = Sviridenko | first2 = Maxim\n | last3 = Vondrák | first3 = Jan\n | doi = 10.1137/11083232X\n | issue = 1\n | journal = [[SIAM Journal on Computing]]\n | mr = 3033132\n | pages = 357–379\n | title = Matroid matching: the power of local search\n | volume = 42\n | year = 2013| citeseerx = 10.1.1.600.4878\n }}</ref>\n\n<ref name=ukg>{{citation\n | last1 = Ueno | first1 = Shuichi\n | last2 = Kajitani | first2 = Yoji\n | last3 = Gotoh | first3 = Shin'ya\n | department = Proceedings of the First Japan Conference on Graph Theory and Applications (Hakone, 1986)\n | doi = 10.1016/0012-365X(88)90226-9\n | issue = 1–3\n | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]\n | mr = 975556\n | pages = 355–360\n | title = On the nonseparating independent set problem and feedback set problem for graphs with no vertex degree exceeding three\n | volume = 72\n | year = 1988}}</ref>\n\n}}\n\n[[Category:Combinatorial optimization]]\n[[Category:Matroid theory|Intersection]]"
    },
    {
      "title": "Max-flow min-cut theorem",
      "url": "https://en.wikipedia.org/wiki/Max-flow_min-cut_theorem",
      "text": "In [[computer science]] and [[Optimization (mathematics)|optimization theory]], the '''max-flow min-cut theorem''' states that in a [[flow network]], the maximum amount of flow passing from the [[Glossary of graph theory#Direction|''source'']] to the [[Glossary of graph theory#Direction|''sink'']] is equal to the total weight of the edges in the [[minimum cut]], i.e. the smallest total weight of the edges which if removed would disconnect the source from the sink.\n\nThe '''max-flow min-cut theorem''' is a special case of the [[dual problem|duality theorem]] for [[linear program]]s and can be used to derive [[Menger's theorem]] and the [[Kőnig's theorem (graph theory)|Kőnig–Egerváry theorem]].<ref>{{cite journal|last1=Dantzig|first1=G.B.|last2=Fulkerson|first2=D.R.|title=On the max-flow min-cut theorem of networks|journal=RAND corporation|date=9 September 1964|page=13|url=http://www.dtic.mil/dtic/tr/fulltext/u2/605014.pdf|accessdate=10 January 2018}}</ref> \n\n==Definitions and statement==\n\nThe theorem relates two quantities: the maximum flow through a network, and the minimum weight of a cut of the network. To state the theorem, each of these quantities must first be defined. \nLet {{math|''N'' {{=}} (''V'', ''E'')}} be a [[directed graph]], where ''V'' denotes the set of vertices and ''E'' is the set of edges. Let {{math|''s'' ∈ ''V''}} and {{math|''t'' ∈ ''V''}} be the source and the sink of {{mvar|N}}, respectively.  The '''capacity''' of an edge is a mapping {{math|''c'' : ''E'' → '''R'''<sup>+</sup>}}, denoted by {{mvar|c<sub>uv</sub>}} or {{math|''c''(''u'', ''v'')}}. It represents the maximum amount of flow that can pass through an edge.\n\n=== Flows ===\n\nA '''flow''' is a mapping {{math|''f'' : ''E'' → '''R'''<sup>+</sup>}}, denoted by {{mvar|f<sub>uv</sub>}} or {{math|&thinsp;''f''&thinsp;(''u'', ''v'')}}, subject to the following two constraints:\n# Capacity Constraint: \n#:For every edge ''(u,v'') in ''E'', <math>f_{uv} \\le c_{uv}</math>\n# Conservation of Flows:\n#: For each vertex ''v'' apart from ''s'' and ''t'' (i.e. the source and sink, respectively), the equality <math>\\sum\\nolimits_{\\{ u : (u,v)\\in E\\}} f_{uv} = \\sum\\nolimits_{\\{w : (v,w)\\in E\\}} f_{vw}</math> holds. \nA flow can be visualized as a physical flow of a fluid through the network, following the direction of each edge.   The capacity constraint then says that the volume flowing through each edge per unit time is less than or equal to the maximum capacity of the edge, and the conservation constraint says that the amount that flows into each vertex equals the amount flowing out of each vertex, apart from the source and sink vertices. \n\nThe '''value''' of a flow is defined by \n:<math>|f| = \\sum\\nolimits_{\\{v : (s,v)\\in E\\}} f_{sv}=\\sum\\nolimits_{\\{v : (v,t)\\in E\\}} f_{vt},</math> \n\nwhere as above {{mvar|s}} is the source node and {{mvar|t}} is the sink node. In the fluid analogy, it represents the amount of fluid entering the network at the source node. Because of the conservation axiom for flows, this is the same as the amount of flow leaving the network at the sink node.  \n\nThe maximum flow problem asks for the largest flow on a given network.  \n:'''[[maximum flow problem|Maximum Flow Problem.]]''' Maximize {{math|{{!}}&thinsp;''f''&thinsp;{{!}}}}, that is, to route as much flow as possible from {{mvar|s}} to {{mvar|t}}.\n\n=== Cuts ===\n\nThe other half of the max-flow min-cut theorem refers to a different aspect of a network: the collection of cuts.  An '''s-t cut''' {{math|''C'' {{=}} (''S'', ''T'')}} is a partition of {{mvar|V}} such that {{math|''s'' ∈ ''S''}} and {{math|''t'' ∈ ''T''}}.  That is, ''s''-''t'' cut is a division of the vertices of the network into two parts, with the source in one part and the sink in the other. The '''cut-set''' <math>X_C</math> of a cut {{mvar|C}} is the set of edges that connect the source part of the cut to the sink part: \n:<math>X_C := \\{ (u, v) \\in E \\ : \\ u \\in S, v \\in T \\} = (S\\times T) \\cap E.</math>\nThus, if all the edges in the cut-set of {{mvar|C}} are removed, then no positive flow is possible, because there is no path in the resulting graph from the source to the sink. \n\nThe '''capacity''' of an ''s-t cut'' is the total weight of its edges,  \n:<math>c (S,T) = \\sum\\nolimits_{(u,v) \\in X_C} c_{uv} = \\sum\\nolimits_{(i,j) \\in E } c_{ij}d_{ij},</math>\nwhere <math>d_{ij} = 1</math> if <math>i \\in S</math> and <math>j \\in T</math>, <math>\n0 </math> otherwise.\n\nThere are typically many cuts in a graph, but cuts with smaller weights are often more difficult to find. \n:'''Minimum s-t Cut Problem.''' Minimize {{math|''c''(''S'', ''T'')}}, that is, determine {{mvar|S}} and {{mvar|T}} such that the capacity of the ''S-T cut'' is minimal.\n\n=== Main theorem ===\nThe main theorem links the maximum flow through a network with the minimum cut of the network.\n: '''Max-flow min-cut theorem.''' The maximum value of an s-t flow is equal to the minimum capacity over all s-t cuts.\n\n==Linear program formulation==\nThe max-flow problem and min-cut problem can be formulated as two primal-dual linear programs.<ref>{{Cite web|url=http://theory.stanford.edu/~trevisan/cs261/lecture15.pdf|title=Lecture 15 from CS261: Optimization|last=Trevisan|first=Luca|date=|website=|archive-url=|archive-date=|dead-url=|access-date=}}</ref>\n\n{| class=\"wikitable\" rules=\"\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\"\n!\n! style=\"border: 1px solid darkgrey;\"|\nMax-flow (Primal)\n! style=\"border-top: 1px solid darkgrey; border-bottom: 1px solid darkgrey; border-right: 1px solid darkgrey;\"|\nMin-cut (Dual)\n|-\n|'''variables'''\n| style=\"border-right: 1px solid darkgrey; border-left: 1px solid darkgrey; padding: 1em;\" valign=\"top\" align=\"left\" |\n<math>f_{uv}</math>  <math>\\forall (u,v)\\in E</math> ''[a variable per edge]''\n| style=\"border-right: 1px solid darkgrey; padding: 1em;\" valign=\"top\" align=\"left\" |\n<math>d_{uv}</math>   <math>\\forall (u,v)\\in E</math>    ''[a variable per edge]''\n\n<math>z_{v}</math>        <math>\\forall v\\in V\\setminus \\{s,t\\}</math> ''[a variable per non-terminal node]''\n|-\n|'''objective'''\n| style=\"border-right: 1px solid darkgrey; border-left: 1px solid darkgrey; padding: 1em;\" valign=\"top\" align=\"left\" |\nmaximize <math>\\sum\\nolimits_{v: (s,v)\\in E} f_{sv}</math>\n\n''[max total flow from source]''\n| style=\"border-right: 1px solid darkgrey; padding: 1em;\" valign=\"top\" align=\"left\" |\nminimize <math>\\sum\\nolimits_{(u,v) \\in E } c_{uv}d_{uv}</math>\n\n''[min total capacity of edges in cut]''\n|-\n|'''constraints'''\n| style=\"border-left: 1px solid darkgrey; border-bottom: 1px solid darkgrey; border-right: 1px solid darkgrey; padding: 1em;\" valign=\"top\" |\nsubject to \n\n:<math>\n\\begin{array}{rclr} \nf_{uv} & \\leq & c_{uv} & \\forall (u, v) \\in E \\\\\n\\sum_{u} f_{uv}  - \\sum_{w} f_{vw} & = & 0 & v \\in V\\setminus \\{s,t\\} \n\\end{array}\n </math>\n\n''[a constraint per edge and a constraint per non-terminal node]''\n| style=\"border-bottom: 1px solid darkgrey; border-right: 1px solid darkgrey; padding: 1em\" valign=\"top\" |\nsubject to\n\n:<math>\\begin{array}{rclr}\nd_{uv} - z_u + z_v & \\geq & 0 &  \\forall (u, v) \\in E, u\\neq s, v\\neq t  \\\\\nd_{sv} + z_v & \\geq & 1 &  \\forall (s, v) \\in E \\\\\nd_{ut} - z_u & \\geq & 0 &  \\forall (u, t) \\in E \\\\\n\\end{array}</math>\n\n''[a constraint per edge]''\n|-\n|'''sign constraints'''\n|<math>\n\\begin{array}{rclr} \nf_{uv} & \\geq & 0 &  \\forall (u, v) \\in E\\\\\n\\end{array}\n </math>\n|<math>\\begin{array}{rclr}\nd_{uv} & \\geq & 0 &  \\forall (u, v) \\in E \\\\\nz_v  & \\in & \\mathbb{R} & \\forall v \\in V\\setminus \\{s,t\\} \n\\end{array}</math>\n|}\n\nThe max-flow LP is straightforward. The dual LP is obtained using the algorithm described in [[dual linear program]]. The resulting LP requires some explanation. The interpretation of the variables in the min-cut LP is:\n\n<math>d_{uv} = \\begin{cases} 1, & \\text{if }u \\in S \\text{ and } v \\in T \\text{ (the edge } uv \\text{ is in the cut) }\n\\\\ 0, & \\text{otherwise} \\end{cases}</math> \n\n<math>z_{u} = \\begin{cases} 1, & \\text{if } u \\in S \\\\ 0, & \\text{otherwise} \\end{cases}</math>\n\nThe minimization objective sums the capacity over all the edges that are contained in the cut.\n\nThe constraints guarantee that the variables indeed represent a legal cut:<ref>{{Cite web|url=http://u.cs.biu.ac.il/~rosenfa5/Alg2/LP.ppt|title=LP min-cut max-flow presentation|last=Keller|first=Orgad|date=|website=|archive-url=|archive-date=|dead-url=|access-date=}}</ref>\n\n* The constraints <math>d_{uv} - z_u + z_v \\geq 0</math>(equivalent to <math>d_{uv} \\geq z_u - z_v </math>) guarantee that, For non-terminal nodes ''u,v'', if ''u'' is in ''S'' and ''v'' is in ''T'', then the edge (''u'',''v'') is counted in the cut (<math>d_{uv} \\geq 1 </math>).\n* The constraints <math>d_{sv} + z_v  \\geq  1</math>(equivalent to <math>d_{sv} \\geq 1 - z_v </math>) guarantee that, if ''v'' is in ''T'', then the edge ''(s,v)'' is counted in the cut (since ''s'' is by definition in ''S'').\n* The constraints <math>d_{ut} - z_u  \\geq  0</math>(equivalent to <math>d_{ut} \\geq z_u </math>) guarantee that, if ''u'' is in ''S'', then the edge ''(u,t)'' is counted in the cut (since ''t'' is by definition in ''T'').\n\nNote that, since this is a minimization problem, we do not have to guarantee that an edge is ''not'' in the cut - we only have to guarantee that each edge that should be in the cut, is summed in the objective function.\n\nThe equality in the '''max-flow min-cut theorem''' follows from the [[Dual linear program|strong duality theorem in linear programming]], which states that if the primal program has an optimal solution, ''x''*, then the dual program also has an optimal solution, ''y''*, such that the optimal values formed by the two solutions are equal.\n\n==Example==\n[[File:max-flow min-cut example.svg|frame|right|A network with the value of flow equal to the capacity of an s-t cut]]\nThe figure on the right is a network having a value of flow of 7. The vertex in white and the vertices in grey form the subsets {{mvar|S}} and {{mvar|T}} of an s-t cut, whose cut-set contains the dashed edges. Since the capacity of the s-t cut is 7, which equals to the value of flow, the max-flow min-cut theorem tells us that the value of flow and the capacity of the s-t cut are both optimal in this network.\n\n==Application==\n\n===Generalized max-flow min-cut theorem===\nIn addition to edge capacity, consider there is capacity at each vertex, that is, a mapping {{math|''c'' : ''V'' → '''R'''<sup>+</sup>}}, denoted by {{math|''c''(''v'')}}, such that the flow {{math|&thinsp;''f''&thinsp;}} has to satisfy not only the capacity constraint and the conservation of flows, but also the vertex capacity constraint\n\n:<math>\\forall v \\in V \\setminus \\{s,t\\} : \\qquad \\sum\\nolimits_{\\{u\\in V\\mid (u,v)\\in E\\}} f_{uv} \\le c(v).</math>\n\nIn other words, the amount of ''flow'' passing through a vertex cannot exceed its capacity. Define an ''s-t cut'' to be the set of vertices and edges such that for any path from ''s'' to ''t'', the path contains a member of the cut. In this case, the ''capacity of the cut'' is the sum the capacity of each edge and vertex in it.\n\nIn this new definition, the '''generalized max-flow min-cut theorem''' states that the maximum value of an s-t flow is equal to the minimum capacity of an s-t cut in the new sense.\n\n===Menger's theorem===\n{{See also| Menger's Theorem}}\nIn the undirected edge-disjoint paths problem, we are given an undirected graph {{math|''G'' {{=}} (''V'', ''E'')}} and two vertices {{mvar|s}} and {{mvar|t}}, and we have to find the maximum number of edge-disjoint s-t paths in {{mvar|G}}.\n\nThe '''Menger's theorem''' states that the maximum number of edge-disjoint s-t paths in an undirected graph is equal to the minimum number of edges in an s-t cut-set.\n\n===Project selection problem===\n{{See also|Maximum flow problem}}\n[[File:Max-flow min-cut project-selection.svg|thumb|right|A network formulation of the project selection problem with the optimal solution]]\nIn the project selection problem, there are {{mvar|n}} projects and {{mvar|m}} machines. Each project {{mvar|p<sub>i</sub>}} yields revenue {{math|''r''(''p<sub>i</sub>'')}} and each machine {{mvar|q<sub>j</sub>}} costs {{math|''c''(''q<sub>j</sub>'')}} to purchase. Each project requires a number of machines and each machine can be shared by several projects. The problem is to determine which projects and machines should be selected and purchased respectively, so that the profit is maximized.\n\nLet {{mvar|P}} be the set of projects ''not'' selected and {{mvar|Q}} be the set of machines purchased, then the problem can be formulated as,\n\n: <math>\\max \\{g\\} = \\sum_{i} r(p_i) - \\sum_{p_i \\in P} r(p_i) - \\sum_{q_j \\in Q} c(q_j).</math>\n\nSince the first term does not depend on the choice of {{mvar|P}} and {{mvar|Q}}, this maximization problem can be formulated as a minimization problem instead, that is,\n\n: <math>\\min \\{g'\\} = \\sum_{p_i \\in P} r(p_i) + \\sum_{q_j \\in Q} c(q_j).</math>\n\nThe above minimization problem can then be formulated as a minimum-cut problem by constructing a network, where the source is connected to the projects with capacity {{math|''r''(''p<sub>i</sub>'')}}, and the sink is connected by the machines with capacity {{math|''c''(''q<sub>j</sub>'')}}. An edge {{math|(''p<sub>i</sub>'', ''q<sub>j</sub>'')}} with ''infinite'' capacity is added if project {{mvar|p<sub>i</sub>}} requires machine {{mvar|q<sub>j</sub>}}. The s-t cut-set represents the projects and machines in {{mvar|P}} and {{mvar|Q}} respectively. By the max-flow min-cut theorem, one can solve the problem as a [[maximum flow problem]].\n\nThe figure on the right gives a network formulation of the following project selection problem:\n\n{| class=\"wikitable\" style=\"text-align:center; width:500px;\" border=\"1\"\n|-\n! width=\"20px\" |\n! width=\"100px\" |\nProject  {{math|''r''(''p<sub>i</sub>'')}}\n! width=\"100px\" |\nMachine  {{math|''c''(''q<sub>j</sub>'')}}\n!\n|-\n! 1\n| 100 || 200\n| align=\"left\" style=\"padding-left: 1em;\" |\nProject 1 requires machines 1 and 2.\n|-\n! 2\n| 200 || 100\n| align=\"left\" style=\"padding-left: 1em;\" |\nProject 2 requires machine 2.\n|-\n! 3\n| 150 || 50\n| align=\"left\" style=\"padding-left: 1em;\" |\nProject 3 requires machine 3.\n|}\n\nThe minimum capacity of a s-t cut is 250 and the sum of the revenue of each project is 450; therefore the maximum profit ''g'' is 450 − 250 = 200, by selecting projects {{math|''p''<sub>2</sub>}} and {{math|''p''<sub>3</sub>}}.\n\nThe idea here is to 'flow' the project profits through the 'pipes' of the machine. If we cannot fill the pipe, the machine's return is less than its cost, and the min cut algorithm will find it cheaper to cut the project's profit edge instead of the machine's cost edge.\n\n===Image segmentation problem===\n{{See also|Maximum flow problem}}\n[[File:Image segmentation.jpg|thumb|Each black node denotes a pixel.]]\nIn the image segmentation problem, there are {{mvar|n}} pixels. Each pixel {{mvar|i}} can be assigned a foreground value {{mvar|&thinsp;f<sub>i</sub>}} or a background value {{mvar|b<sub>i</sub>}}. There is a penalty of {{mvar|p<sub>ij</sub>}} if pixels {{mvar|i, j}} are adjacent and have different assignments. The problem is to assign pixels to foreground or background such that the sum of their values minus the penalties is maximum.\n\nLet {{mvar|P}} be the set of pixels assigned to foreground and {{mvar|Q}} be the set of points assigned to background, then the problem can be formulated as,\n\n: <math>\\max \\{g\\} = \\sum_{i \\in P} f_i + \\sum_{i \\in Q} b_i - \\sum_{i \\in P,j \\in Q \\lor j \\in P,i \\in Q } p_{ij}.</math>\n\nThis maximization problem can be formulated as a minimization problem instead, that is,\n\n: <math>\\min \\{g'\\} = \\sum_{i \\in P,j \\in Q \\lor j \\in P,i \\in Q } p_{ij}.</math>\n\nThe above minimization problem can be formulated as a minimum-cut problem by constructing a network where the source (orange node) is connected to all the pixels with capacity {{mvar|&thinsp;f<sub>i</sub>}}, and the sink (purple node) is connected by all the pixels with capacity {{mvar|b<sub>i</sub>}}. Two edges ({{mvar|i, j}}) and ({{mvar|j, i}}) with {{mvar|p<sub>ij</sub>}} capacity are added between two adjacent pixels. The s-t cut-set then represents the pixels assigned to the foreground in {{mvar|P}} and pixels assigned to background in {{mvar|Q}}.\n\n==History==\nAn account of the discovery of the theorem was given by Ford and Fulkerson in 1962:<ref>[[L. R. Ford Jr.]] & [[D. R. Fulkerson]] (1962) ''Flows in Networks'', page 1, [[Princeton University Press]] {{mr|id=0159700}}</ref>\n\n\"Determining a maximal steady state flow from one point to another in a network subject to capacity limitations on arcs ... was posed to the authors in the spring of 1955 by T.E. Harris, who, in conjunction with General F. S. Ross (Ret.) had formulated a simplified model of railway traffic flow, and pinpointed this particular problem as the central one suggested by the model. It was not long after this until the main result, Theorem 5.1, which we call the max-flow min-cut theorem, was conjectured and established.<ref>L. R. Ford Jr. and D. R. Fulkerson (1956) \"Maximal flow through a network\", [[Canadian Journal of Mathematics]] 8: 399-404}}</ref> A number of proofs have since appeared.\"<ref>P. Elias, A. Feinstein, and C. E. Shannon (1956) \"A note on the maximum flow through a network\", IRE. Transactions on Information Theory, 2(4):  117–119</ref><ref>[[George Dantzig]] and D. R. Fulkerson (1956) \"On the Max-Flow MinCut Theorem of Networks\", in ''Linear Inequalities'', Ann. Math. Studies, no. 38, Princeton, New Jersey</ref><ref>L. R. Ford & D. R. Fulkerson (1957) \"A simple algorithm for finding the maximum network flows and an application to the Hitchcock problem\", ''Canadian Journal of Mathematics'' 9: 210–18</ref>\n\n==Proof==\nLet {{math|''G'' {{=}} (''V'', ''E'')}} be a network (directed graph) with {{mvar|s}} and {{mvar|t}} being the source and the sink of {{mvar|G}} respectively.\n\nConsider the flow {{math|&thinsp;''f''&thinsp;}} computed for {{mvar|G}} by [[Ford–Fulkerson algorithm]]. In the residual graph {{math|(''G<sub>f</sub>''&thinsp;)}} obtained for {{mvar|G}} (after the final flow assignment by [[Ford–Fulkerson algorithm]]), define two subsets of vertices as follows:\n# {{mvar|A}}: the set of vertices reachable from {{mvar|s}} in {{mvar|G<sub>f</sub>}}\n# {{mvar|A<sup>c</sup>}}: the set of remaining vertices i.e. {{mvar|V − A}}\n\n'''Claim.''' {{math|value(&thinsp;''f''&thinsp;) {{=}} ''c''(''A'', ''A<sup>c</sup>'')}}, where the '''capacity''' of an ''s-t cut'' is defined by \n:<math>c(S,T) = \\sum\\nolimits_{(u,v) \\in S \\times T} c_{uv}</math>.\n\nNow, we know, <math>value(f) = f_{out}(A) - f_{in}(A)</math> for any subset of vertices, {{mvar|A}}. Therefore, for {{math|value(&thinsp;''f''&thinsp;) {{=}} ''c''(''A'', ''A<sup>c</sup>'')}} we need:\n* All ''outgoing edges'' from the cut must be '''fully saturated'''.\n* All ''incoming edges'' to the cut must have '''zero flow'''.\n\nTo prove the above claim we consider two cases:\n\n*In {{mvar|G}}, there exists an ''outgoing edge'' <math>(x,y), x \\in A, y \\in A^c</math> such that it is not saturated, i.e., {{math|&thinsp;''f''&thinsp;(''x'', ''y'') < ''c<sub>xy</sub>''}}. This implies, that there exists a '''forward edge''' from {{mvar|x}} to {{mvar|y}} in {{mvar|G<sub>f</sub>}}, therefore there exists a path from {{mvar|s}} to {{mvar|y}} in {{mvar|G<sub>f</sub>}}, which is a contradiction. Hence, any outgoing edge {{math|(''x'', ''y'')}} is fully saturated.\n*In {{mvar|G}}, there exists an ''incoming edge'' <math>(y,x), x \\in A, y \\in A^c</math> such that it carries some non-zero flow, i.e., {{math|&thinsp;''f''&thinsp;(''y'', ''x'') > 0}}. This implies, that there exists a '''backward edge''' from {{mvar|x}} to {{mvar|y}} in {{mvar|G<sub>f</sub>}}, therefore there exists a path from {{mvar|s}} to {{mvar|y}} in {{mvar|G<sub>f</sub>}}, which is again a contradiction. Hence, any incoming edge {{math|(''y'', ''x'')}} must have zero flow.\n\nBoth of the above statements prove that the capacity of cut obtained in the above described manner is equal to the flow obtained in the network. Also, the flow was obtained by [[Ford-Fulkerson algorithm]], so it is the [[max-flow]] of the network as well.\n\n:Also, since ''any flow in the network is always less than or equal to capacity of every cut possible in a network'', the above described cut is also the [[min-cut]] which obtains the [[max-flow]].\n\n==See also==\n* [[GNRS conjecture]]\n* [[Linear programming]]\n* [[Maximum flow]]\n* [[Minimum cut]]\n* [[Flow network]]\n* [[Edmonds–Karp algorithm]]\n* [[Ford–Fulkerson algorithm]]\n* [[Approximate max-flow min-cut theorem]]\n\n==References==\n{{reflist}}\n* {{cite book|author=Eugene Lawler | authorlink = Eugene Lawler|title = Combinatorial Optimization: Networks and Matroids | chapter = 4.5. Combinatorial Implications of Max-Flow Min-Cut Theorem, 4.6. Linear Programming Interpretation of Max-Flow Min-Cut Theorem | year = 2001 | publisher = Dover | isbn = 0-486-41453-1 | pages = 117–120}}\n* {{cite book|author=[[Christos H. Papadimitriou]], [[Kenneth Steiglitz]]|title=Combinatorial Optimization: Algorithms and Complexity | chapter=6.1 The Max-Flow, Min-Cut Theorem | year = 1998| publisher = Dover | isbn = 0-486-40258-4 |pages= 120–128}}\n* {{cite book|author=[[Vijay Vazirani|Vijay V. Vazirani]]|title=Approximation Algorithms|chapter=12. Introduction to LP-Duality | year = 2004 | publisher = Springer | isbn = 3-540-65367-8 | pages = 93–100}}\n\n[[Category:Combinatorial optimization]]\n[[Category:Theorems in graph theory]]\n[[Category:Network flow problem]]"
    },
    {
      "title": "Maximum cut",
      "url": "https://en.wikipedia.org/wiki/Maximum_cut",
      "text": "[[File:Max-cut.svg|thumb|right|An example of a maximum cut]]\n\nFor a [[Graph (discrete mathematics)|graph]], a '''maximum cut'''  is a [[cut (graph theory)|cut]] whose size is at least the size of any other cut. The problem of finding a maximum cut in a graph is known as the '''Max-Cut Problem.'''\n\nThe problem can be stated simply as follows.  One wants a subset ''S'' of the vertex set such that the number of edges between ''S'' and the complementary subset is as large as possible.\n\nThere is a more general version of the problem called '''weighted Max-Cut'''.  In this version each edge has a real number, its '''weight''', and the objective is to maximize not the number of edges but the total weight of the edges between ''S'' and its complement.  The weighted Max-Cut problem is often, but not always, restricted to non-negative weights, because negative weights can change the nature of the problem.\n\n==Computational complexity==\n\nThe following [[decision problem]] related to maximum cuts has been studied widely in [[theoretical computer science]]:\n\n:Given a graph ''G'' and an integer ''k'', determine whether there is a cut of size at least ''k'' in ''G''.\n\nThis problem is known to be [[NP-complete]]. It is easy to see that the problem is in [[NP (complexity)|NP]]: a ''yes'' answer is easy to prove by presenting a large enough cut. The NP-completeness of the problem can be shown, for example, by a reduction from [[maximum 2-satisfiability]] (a restriction of the [[maximum satisfiability problem]]).<ref>{{harvtxt|Garey|Johnson|1979}}.</ref> The weighted version of the decision problem was one of [[Karp's 21 NP-complete problems]];<ref>{{harvtxt|Karp|1972}}.</ref> Karp showed the NP-completeness by a reduction from the [[partition problem]].\n\nThe canonical [[optimization problem|optimization variant]] of the above decision problem is usually known as the ''Maximum-Cut Problem'' or ''Max-Cut'' and is defined as:\n\n:Given a graph ''G'', find a maximum cut.\n\nThe opposite problem, that of finding a [[minimum cut]] is known to be efficiently solvable via the [[Ford–Fulkerson algorithm]].\n\n== Algorithms ==\n\n===Polynomial-time algorithms===\n\nAs the Max-Cut Problem is NP-hard, no polynomial-time algorithms for Max-Cut in general graphs are known.\n\n=== Planar graphs ===\nHowever, in [[planar graph]]s, the Maximum-Cut Problem is dual to the [[route inspection problem]] (the problem of finding a shortest tour that visits each edge of a graph at least once), in the sense that the edges that do not belong to a maximum cut-set of a graph ''G'' are the duals of the edges that are doubled in an optimal inspection tour of the [[dual graph]] of ''G''. The optimal inspection tour forms a self-intersecting curve that separates the plane into two subsets, the subset of points for which the [[winding number]] of the curve is even and the subset for which the winding number is odd; these two subsets form a cut that includes all of the edges whose duals appear an odd number of times in the tour. The route inspection problem may be solved in polynomial time, and this duality allows the maximum cut problem to also be solved in polynomial time for planar graphs.<ref>{{harvtxt|Hadlock|1975}}.</ref> The Maximum-Bisection problem is known however to be NP-hard.<ref>{{harvtxt|Jansen|Karpinski|Lingas|Seidel|2005}}.</ref>\n\n===Approximation algorithms===\nThe Max-Cut Problem is [[Constant-factor approximation algorithm|APX-hard]],<ref>{{harvtxt|Papadimitriou|Yannakakis|1991}} prove [[MaxSNP]]-completeness.</ref> meaning that there is no polynomial-time approximation scheme (PTAS), arbitrarily close to the optimal solution, for it, unless P = NP. Thus, every known polynomial-time approximation algorithm achieves an [[approximation ratio]] strictly less than one.\n\nThere is a simple [[Randomized algorithm|randomized]] 0.5-[[approximation algorithm]]: for each vertex flip a coin to decide to which half of the partition to assign it.<ref>{{harvtxt|Mitzenmacher|Upfal|2005}}, Sect. 6.2.</ref><ref>{{harvtxt|Motwani|Raghavan|1995}}, Sect. 5.1.</ref> In expectation, half of the edges are cut edges. This algorithm can be [[derandomization|derandomized]] with the [[method of conditional probabilities]]; therefore there is a simple deterministic polynomial-time 0.5-approximation algorithm as well.<ref>{{harvtxt|Mitzenmacher|Upfal|2005}}, Sect. 6.3.</ref><ref>{{harvtxt|Khuller|Raghavachari|Young|2007}}.</ref> One such algorithm starts with an arbitrary partition of the vertices of the given graph <math>G = (V, E)</math> and repeatedly moves one vertex at a time from one side of the partition to the other, improving the solution at each step, until no more improvements of this type can be made. The number of iterations is at most <math>|E|</math> because the algorithm improves the cut by at least one edge at each step. When the algorithm terminates, at least half of the edges incident to every vertex belong to the cut, for otherwise moving the vertex would improve the cut. Therefore, the cut includes at least <math>|E|/2</math> edges.\n\nThe polynomial-time approximation algorithm for Max-Cut with the best known approximation ratio is a method by Goemans and Williamson using [[semidefinite programming]] and [[randomized rounding]] that achieves an approximation ratio <math>\\alpha \\approx 0.878</math>, where \n\n<math>\\alpha = \\tfrac{2}{\\pi} \\min_{0 \\le \\theta \\le \\pi} \\tfrac{\\theta}{1 - \\cos \\theta}</math>.<ref>{{harvtxt|Gaur|Krishnamurti|2007}}.</ref><ref>{{harvtxt|Ausiello|Crescenzi|Gambosi|Kann|2003}}\n</ref> \n\nIf the [[unique games conjecture]] is true, this is the best possible approximation ratio for maximum cut.<ref>{{harvtxt|Khot|Kindler|Mossel|O'Donnell|2007}}.</ref> Without such unproven assumptions, it has been proven to be NP-hard to approximate the max-cut value with an approximation ratio better than <math>\\tfrac{16}{17} \\approx 0.941</math>.<ref>{{harvtxt|Håstad|2001}}</ref><ref>{{harvtxt|Trevisan|Sorkin|Sudan|Williamson|2000}}</ref>\n\nIn <ref>{{harvtxt|Dunning|Gupta|Silberholz|2018}}</ref> there is an extended analysis of 10 heuristics for this problem, including open-source implementation.\n\n== Applications ==\n\n=== Theoretical physics ===\nIn [[statistical physics]] and [[Order and disorder|disordered systems]], the Max Cut problem is equivalent to minimizing the [[Hamiltonian mechanics|Hamiltonian]] of a [[spin glass]] model, most simply the [[Ising model]].<ref name=\":0\">{{Cite journal|last=Barahona|first=Francisco|last2=Grötschel|first2=Martin|last3=Jünger|first3=Michael|last4=Reinelt|first4=Gerhard|date=1988|title=An Application of Combinatorial Optimization to Statistical Physics and Circuit Layout Design|journal=Operations Research|volume=36|issue=3|pages=493–513|issn=0030-364X|jstor=170992|doi=10.1287/opre.36.3.493}}</ref> For the Ising model on a graph G and only nearest-neighbor interactions, the Hamiltonian is\n\n<math>H[s] = -\\sum_{ij\\in E(G)} J_{ij}s_is_j</math>\n\nHere each vertex i of the graph is a spin site that can take a spin value <math>s_i = \\pm 1 </math>. A spin configuration partitions <math>V(G)</math> into two sets, those with spin up <math>V^+</math> and those with spin down <math>V^-</math>. We denote with <math>\\delta(V^+)</math> the set of edges that connect the two sets. We can then rewrite the Hamiltonian as\n\n<math>\\begin{align}\nH[s] &= -\\sum_{ij\\in E(V^+)} J_{ij} - \\sum_{ij\\in E(V^-)} J_{ij} + \\sum_{ij\\in \\delta(V^+)} J_{ij} \\\\\n&= -\\sum_{ij \\in E(G)} J_{ij} + 2 \\sum_{ij\\in \\delta(V^+)} J_{ij} \\\\\n&= C + 2 \\sum_{ij\\in \\delta(V^+)} J_{ij}.\n\\end{align}</math>\n\nMinimizing this energy is equivalent to the min-cut problem or by setting the graph weights as <math>w_{ij} = -J_{ij}</math> , the max-cut problem.<ref name=\":0\" />\n\n=== Circuit design ===\nThe max cut problem has applications in [[Very Large Scale Integration|VLSI design]].<ref name=\":0\" />\n\n==See also==\n*[[Minimum cut]]\n*[[Minimum k-cut]]\n\n==Notes==\n{{reflist|2}}\n\n==References==\n*{{citation\n | last1=Ausiello | first1=Giorgio\n | last2=Crescenzi | first2=Pierluigi\n | last3=Gambosi | first3=Giorgio\n | last4=Kann | first4=Viggo\n | last5=Marchetti-Spaccamela | first5=Alberto\n | last6=Protasi | first6=Marco\n | title=Complexity and Approximation: Combinatorial Optimization Problems and Their Approximability Properties\n | publisher=Springer\n | year=2003\n}}.\n::Maximum cut (optimisation version) is the problem ND14 in Appendix B (page 399).\n*{{citation\n | last1=Garey | first1=Michael R. | authorlink1=Michael R. Garey\n | last2=Johnson | first2=David S. | authorlink2=David S. Johnson\n | year = 1979\n | title = Computers and Intractability: A Guide to the Theory of NP-Completeness\n | publisher = W.H. Freeman\n | isbn=978-0-7167-1045-5\n| title-link=Computers and Intractability: A Guide to the Theory of NP-Completeness }}.\n::Maximum cut (decision version) is the problem ND16 in Appendix A2.2.\n::Maximum bipartite subgraph (decision version) is the problem GT25 in Appendix A1.2.\n*{{citation\n | last1=Gaur | first1=Daya Ram\n | last2=Krishnamurti | first2=Ramesh\n | chapter=LP rounding and extensions\n | title=Handbook of Approximation Algorithms and Metaheuristics\n | editor-last=Gonzalez | editor-first=Teofilo F. | editor-link = Teofilo F. Gonzalez\n | publisher=Chapman &amp; Hall/CRC\n | year=2007\n}}.\n*{{citation\n | last1=Goemans | first1=Michel X. | author1-link = Michel Goemans\n | last2=Williamson | first2=David P. | author2-link = David P. Williamson\n | title=Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming\n | journal=[[Journal of the ACM]]\n | volume=42\n | issue=6\n | year=1995\n | pages=1115–1145  \n | doi=10.1145/227683.227684\n}}.\n*{{citation\n | last=Hadlock | first=F. \n | title=Finding a Maximum Cut of a Planar Graph in Polynomial Time\n | journal=[[SIAM J. Comput.]]\n | volume=4\n | issue=3\n | year=1975\n | pages=221–225\n | doi=10.1137/0204019\n}}.\n*{{citation\n | last=Håstad | first=Johan | authorlink=Johan Håstad\n | title=Some optimal inapproximability results\n | journal=[[Journal of the ACM]]\n | year=2001\n | volume=48\n | pages=798–859\n | issue=4\n | doi=10.1145/502090.502098\n}}.\n*{{citation\n | last1=Jansen | first1=Klaus \n | last2=Karpinski | first2=Marek | authorlink2=Marek Karpinski\n | last3=Lingas | first3=Andrzej\n | last4=Seidel | first4=Eike\n | title=Polynomial Time Approximation Schemes for MAX-BISECTION on Planar and Geometric Graphs\n | journal=[[SIAM Journal on Computing]]\n | year=2005\n | volume=35\n | issue=1\n | pages=110–119 \n | doi=10.1137/s009753970139567x \n| citeseerx=10.1.1.62.5082}}.\n*{{citation\n | last=Karp | first=Richard M. | authorlink=Richard Karp\n | chapter=[[Reducibility among combinatorial problems]]\n | editor1-first=R. E. | editor1-last=Miller\n | editor2-first=J. W. | editor2-last=Thacher\n | title=Complexity of Computer Computation\n | publisher=Plenum Press\n | pages=85–103\n | year=1972\n}}.\n*{{citation\n | last1=Khot | first1=Subhash |authorlink1=Subhash Khot\n | last2=Kindler | first2=Guy\n | last3=Mossel | first3=Elchanan\n | last4=O'Donnell | first4=Ryan\n | title=Optimal inapproximability results for MAX-CUT and other 2-variable CSPs?\n | journal=[[SIAM Journal on Computing]]\n | volume=37\n | issue=1\n | year=2007\n | pages=319–357\n | doi=10.1137/S0097539705447372\n}}.\n*{{citation\n | last1=Khuller | first1=Samir\n | last2=Raghavachari | first2=Balaji\n | last3=Young | first3=Neal E.\n | chapter=Greedy methods\n | title=Handbook of Approximation Algorithms and Metaheuristics\n | editor-last=Gonzalez | editor-first=Teofilo F.\n | publisher=Chapman &amp; Hall/CRC\n | year=2007\n}}.\n*{{citation\n | last1=Mitzenmacher | first1=Michael | authorlink1=Michael Mitzenmacher\n | last2=Upfal | first2=Eli | authorlink2=Eli Upfal\n | title=Probability and Computing: Randomized Algorithms and Probabilistic Analysis\n | publisher=Cambridge\n | year=2005\n}}.\n*{{citation\n | last1=Motwani | first1=Rajeev | authorlink1=Rajeev Motwani\n | last2=Raghavan | first2=Prabhakar\n | title=Randomized Algorithms\n | publisher=Cambridge\n | year=1995\n}}.\n*{{citation\n | last=Newman | first=Alantha\n | chapter=Max cut\n | title=Encyclopedia of Algorithms\n | publisher=Springer\n | editor-last=Kao | editor-first=Ming-Yang\n | year=2008\n | doi=10.1007/978-0-387-30162-4_219\n | pages=489–492\n | isbn=978-0-387-30770-1\n}}.\n*{{citation\n | last1=Papadimitriou | first1=Christos H. | authorlink1=Christos Papadimitriou\n | last2=Yannakakis | first2=Mihalis | authorlink2=Mihalis Yannakakis\n | title=Optimization, approximation, and complexity classes\n | journal=Journal of Computer and System Sciences\n | year=1991\n | volume=43\n | issue=3\n | pages=425–440\n | doi=10.1016/0022-0000(91)90023-X\n}}.\n*{{citation\n | last1=Trevisan | first1=Luca | authorlink=Luca Trevisan\n | last2=Sorkin | first2=Gregory\n | last3=Sudan | first3=Madhu\n | last4=Williamson | first4=David\n | title=Gadgets, Approximation, and Linear Programming\n | journal=Proceedings of the 37th IEEE Symposium on Foundations of Computer Science\n | year=2000\n | pages=617–626\n}}.\n*{{citation\n | last1=Dunning | first1=Iain | authorlink=Iain Dunning\n | last2=Gupta | first2=Swati\n | last3=Silberholz | first3=John\n | title=What works best when? A systematic evaluatoin of heuristics for Max-Cut and QUBO\n | journal=INFORMS Journal on Computing\n | volume=30 | issue=3 | year=2018\n | pages=608–624\n| doi=10.1287/ijoc.2017.0798 }}.\n\n==External links==\n* Pierluigi Crescenzi, Viggo Kann, Magnús Halldórsson, Marek Karpinski, [[Gerhard J. Woeginger|Gerhard Woeginger]] (2000), [http://www.nada.kth.se/~viggo/wwwcompendium/node85.html \"Maximum Cut\"], in [http://www.nada.kth.se/~viggo/wwwcompendium/ \"A compendium of NP optimization problems\"].\n* Andrea Casini, Nicola Rebagliati (2012), [http://code.google.com/p/maxcutpy/ \"A Python library for solving Max Cut\"]\n\n[[Category:Graph theory objects]]\n[[Category:Combinatorial optimization]]\n[[Category:NP-complete problems]]\n[[Category:Computational problems in graph theory]]"
    },
    {
      "title": "Maximum satisfiability problem",
      "url": "https://en.wikipedia.org/wiki/Maximum_satisfiability_problem",
      "text": "In [[computational complexity theory]], the '''maximum satisfiability problem''' ('''MAX-SAT''') is the problem of determining the maximum number of clauses, of a given [[Propositional formula|Boolean]] formula in [[conjunctive normal form]], that can be made true by an assignment of truth values to the variables of the formula. It is a generalization of the [[Boolean satisfiability problem]], which asks whether there exists a truth assignment that makes all clauses true.\n\n==Example==\nThe conjunctive normal form formula\n:<math> (x_0\\lor x_1)\\land(x_0\\lor\\lnot x_1)\\land(\\lnot x_0\\lor x_1)\\land(\\lnot x_0\\lor\\lnot x_1)</math>\nis not satisfiable: no matter which truth values are assigned to its two variables, at least one of its four clauses will be false.\nHowever, it is possible to assign truth values in such a way as to make three out of four clauses true; indeed, every truth assignment will do this.\nTherefore, if this formula is given as an instance of the MAX-SAT problem, the solution to the problem is the number three.\n\n==Hardness==\nThe MAX-SAT problem is [[NP-hard]], since its solution easily leads to the solution of the [[boolean satisfiability problem]], which is [[NP-complete]].\n\nIt is also difficult to find an [[approximation algorithm|approximate]] solution of the problem, that satisfies a number of clauses within a guaranteed [[approximation ratio]] of the optimal solution. More precisely, the problem is [[APX]]-complete, and thus does not admit a [[polynomial-time approximation scheme]] unless P = NP.<ref>Mark Krentel. The Complexity of Optimization Problems. Proc. of STOC '86. 1986.</ref><ref>Christos Papadimitriou. Computational Complexity. Addison-Wesley, 1994.</ref><ref>Cohen, Cooper, Jeavons. A complete characterization of complexity for boolean constraint optimization problems. CP 2004.</ref>\n\n== Weighted MAX-SAT ==\nMore generally, one can define a weighted version of MAX-SAT as follows: given a conjunctive normal form formula with non-negative weights assigned to each clause, find truth values for its variables that maximize the combined weight of the satisfied clauses. The MAX-SAT problem is an instance of weighted MAX-SAT where all weights are 1.{{sfn|Vazirani|2001|p=131}}<ref>{{Cite journal|last=Borchers|first=Brian|last2=Furman|first2=Judith|date=1998-12-01|title=A Two-Phase Exact Algorithm for MAX-SAT and Weighted MAX-SAT Problems|journal=Journal of Combinatorial Optimization|language=en|volume=2|issue=4|pages=299–306|doi=10.1023/A:1009725216438|issn=1382-6905}}</ref><ref>{{Cite book|url=https://books.google.com/?id=_GOVQRL50kcC&pg=PA393&dq=weighted+max+sat#v=onepage&q=weighted%20max%20sat&f=false|title=Satisfiability Problem: Theory and Applications : DIMACS Workshop, March 11-13, 1996|last=Du|first=Dingzhu|last2=Gu|first2=Jun|last3=Pardalos|first3=Panos M.|date=1997-01-01|publisher=American Mathematical Soc.|isbn=9780821870808|language=en|pp=393}}</ref>\n\n=== Approximation algorithms ===\n<!-- Given a conjunctive normal form formula {{var|F}} with variables {{var|x}}<sub>1</sub>, {{var|x}}<sub>2</sub>, ..., {{var|x}}<sub>n</sub>, -->\n\n==== 1/2-approximation ====\n\nRandomly assigning each variable to be true with probability 1/2 gives an [[Expected value|expected]] [[Approximation algorithm#Performance guarantees|2-approximation]]. More precisely, if each clause has at least {{var|k}} variables, then this yields a (1 − 2<sup>−{{var|k}}</sup>)-approximation.{{sfn|Vazirani|2001|loc=Lemma 16.2}} This algorithm can be [[Randomized algorithm#Derandomization|derandomized]] using the [[method of conditional probabilities]].{{sfn|Vazirani|2001|loc=Section 16.2}}\n\n==== (1-1/{{var|e}})-approximation ====\n\nMAX-SAT can also be expressed using an [[integer linear program]] (ILP). Fix a conjunctive normal form formula {{var|F}} with variables {{var|x}}<sub>1</sub>, {{var|x}}<sub>2</sub>, ..., {{var|x}}<sub>n</sub>, and let {{var|C}} denote the clauses of {{var|F}}. For each clause {{var|c}} in {{var|C}}, let {{var|S}}<sup>+</sup><sub>{{var|c}}</sub> and {{var|S}}<sup>−</sup><sub>{{var|c}}</sub> denote the sets of variables which are not negated in {{var|c}}, and those that are negated in {{var|c}}, respectively. The variables {{var|y}}<sub>{{var|x}}</sub> of the ILP will correspond to the variables of the formula {{var|F}}, whereas the variables {{var|z}}<sub>{{var|c}}</sub> will correspond to the clauses. The ILP is as follows: \n{|\n| maximize\n| <math>\\sum_{c \\in C} w_c\\cdot z_c</math>\n|\n| (maximize the weight of the satisfied clauses)\n|-\n| subject to\n| <math>z_c\\leq\\sum_{x\\in S_c^+} y_x+\\sum_{x\\in S_c^-} (1-y_x)</math>\n| for all <math>c\\in C</math>\n| (clause is true [[If and only if|iff]] it has a true, non-negated variable or a false, negated one)\n|-\n|\n| <math>z_c \\in \\{0,1\\}</math>\n| for all <math>c\\in C</math>.\n| (every clause is either satisfied or not)\n|-\n|\n| <math>y_x \\in \\{0,1\\}</math>\n| for all <math>x\\in F</math>.\n| (every variable is either true or false)\n|}\n\nThe above program can be [[Linear programming relaxation|relaxed]] to the following linear program {{var|L}}:\n\n{|\n| maximize\n| <math>\\sum_{c \\in C} w_c\\cdot z_c</math>\n|\n| (maximize the weight of the satisfied clauses)\n|-\n| subject to\n| <math>z_c\\leq\\sum_{x\\in S_c^+} y_x+\\sum_{x\\in S_c^-} (1-y_x)</math>\n| for all <math>c\\in C</math>\n| (clause is true [[If and only if|iff]] it has a true, non-negated variable or a false, negated one)\n|-\n|\n| <math>0\\leq z_c \\leq 1</math>\n| for all <math>c\\in C</math>.\n|-\n|\n| <math>0\\leq y_x\\leq 1</math>\n| for all <math>x\\in F</math>.\n|}\n\nThe following algorithm using that relaxation is an expected (1-1/[[E (mathematical constant)|e]])-approximation:{{sfn|Vazirani|p=136}}\n# Solve the linear program {{var|L}} and obtain a solution {{var|O}}\n# Set variable {{var|x}} to be true with probability {{var|y}}<sub>{{var|x}}</sub> where {{var|y}}<sub>{{var|x}}</sub> is the value given in {{var|O}}.\n\nThis algorithm can also be derandomized using the method of conditional probabilities.\n\n==== 3/4-approximation ====\n\nThe 1/2-approximation algorithm does better when clauses are large whereas the (1-1/{{var|e}})-approximation  does better when clauses are small. They can be combined as follows:\n# Run the (derandomized) 1/2-approximation algorithm to get a truth assignment {{var|X}}.\n# Run the (derandomized) (1-1/e)-approximation to get a truth assignment {{var|Y}}.\n# Output whichever of {{var|X}} or {{var|Y}} maximizes the weight of the satisfied clauses.\n\nThis is a deterministic factor (3/4)-approximation.{{sfn|Vazirani|2001|loc=Theorem 16.9}}\n\n==== Example ====\n\nOn the formula\n:<math display=\"block\">F=\\underbrace{(x\\lor y)}_{\\text{weight }1}\\land \\underbrace{(x\\lor\\lnot y)}_{\\text{weight }1}\\land\\underbrace{(\\lnot x\\lor z)}_{\\text{weight }2+\\epsilon}</math>\n\nwhere <math>\\epsilon >0</math>, the (1-1/{{var|e}})-approximation will set each variable to True with probability 1/2, and so will behave identically to the 1/2-approximation. Assuming that the assignment of {{var|x}} is chosen first during derandomization, the derandomized algorithms will pick a solution with total weight <math>3+\\epsilon</math>, whereas the optimal solution has weight <math>4+\\epsilon</math>.{{sfn|Vazirani|2001|loc=Example 16.11}}\n\n==Solvers==\nMany exact solvers for MAX-SAT have been developed during recent years, and many of them were presented in the well-known conference on the boolean satisfiability problem and related problems, the SAT Conference. In 2006 the SAT Conference hosted the first '''MAX-SAT evaluation''' comparing performance of practical solvers for MAX-SAT, as it has done in the past for the [[0-1 integer programming|pseudo-boolean satisfiability]] problem and the [[quantified boolean formula]] problem.\nBecause of its NP-hardness, large-size MAX-SAT instances cannot in general be solved exactly, and one must often resort to [[approximation algorithm]]s\nand [[Metaheuristic|heuristics]] <ref>\nR. Battiti and M. Protasi.\nApproximate Algorithms and Heuristics for MAX-SAT\nHandbook of Combinatorial Optimization, Vol 1, 1998, 77-148, Kluwer Academic Publishers.</ref>\n\nThere are several solvers submitted to the last Max-SAT Evaluations:\n* [[Branch and Bound]] based: Clone, MaxSatz (based on [[Satz (SAT solver)|Satz]]), IncMaxSatz, IUT_MaxSatz, WBO, GIDSHSat.\n* Satisfiability based: SAT4J, QMaxSat.\n* Unsatisfiability based: msuncore, WPM1, PM2.\n\n==Special cases==\nMAX-SAT is one of the optimization extensions of the [[boolean satisfiability problem]], which is the problem of determining whether the variables of a given [[Propositional formula|Boolean]] formula can be assigned in such a way as to make the formula evaluate to TRUE. If the clauses are restricted to have at most 2 literals, as in [[2-satisfiability]], we get the [[MAX-2SAT]] problem. If they are restricted to at most 3 literals per clause, as in [[3-satisfiability]], we get the [[MAX-3SAT]] problem.\n\n==Related problems==\nThere are many problems related to the satisfiability of conjunctive normal form Boolean formulas.\n\n* [[Decision problem]]s:\n** [[2-satisfiability|2SAT]]\n** [[Boolean satisfiability problem|3SAT]]\n* Optimization problems, where the goal is to maximize the number of clauses satisfied:\n** MAX-SAT, and the corresponded weighted version [[#Weighted MAX-SAT|Weighted MAX-SAT]]\n** MAX-{{var|k}}SAT, where each clause has exactly {{var|k}} variables:\n*** [[2-satisfiability#Maximum-2-satisfiability|MAX-2SAT]]\n*** [[MAX-3SAT]]\n*** [[MAXEkSAT]]\n** The partial maximum satisfiability problem (PMAX-SAT) asks for the maximum number of clauses which can be satisfied by any assignment of a given subset of clauses. The rest of the clauses must be satisfied.\n** The soft satisfiability problem (soft-SAT), given a set of SAT problems, asks for the maximum number of sets which can be satisfied by any assignment.<ref>Josep Argelich and Felip Manyà. [http://www.springerlink.com/content/870v1535q0h51717/ Exact Max-SAT solvers for over-constrained problems]. In Journal of Heuristics 12(4) pp. 375-392. Springer, 2006.</ref>\n** The minimum satisfiability problem.\n* The MAX-SAT problem can be extended to the case where the variables of the [[constraint satisfaction problem]] belong the set of reals. The problem amounts to finding the smallest ''q'' such that the ''q''-[[relaxed intersection]] of the constraints is not empty.<ref>{{cite journal|last1=Jaulin|first1=L.|last2=Walter|first2=E.| title=Guaranteed robust nonlinear minimax estimation| journal=IEEE Transactions on Automatic Control|year=2002|volume=47| url=http://www.ensta-bretagne.fr/jaulin/paper_qminimax.pdf}}</ref>\n\n== See also ==\n* [[Boolean satisfiability problem|Boolean Satisfiability Problem]]\n* [[Constraint satisfaction]]\n* [[Satisfiability modulo theories]]\n\n== External links ==\n* http://www.satisfiability.org/\n* https://web.archive.org/web/20060324162911/http://www.iiia.csic.es/~maxsat06/\n* http://www.maxsat.udl.cat\n* [http://www.nlsde.buaa.edu.cn/~kexu/benchmarks/max-sat-benchmarks.htm Weighted Max-2-SAT Benchmarks with Hidden Optimum Solutions]\n* [http://www.cs.tau.ac.il/~azar/Methods-Class6.pdf Lecture Notes on MAX-SAT Approximation]\n\n== References ==\n<references />\n* {{Citation| last = Vazirani | first = Vijay V.\n | authorlink = Vijay Vazirani\n | title = Approximation Algorithms\n | year = 2001\n | publisher = Springer-Verlag\n | isbn = 978-3-540-65367-7\n | url = http://www.cc.gatech.edu/fac/Vijay.Vazirani/book.pdf\n}}\n\n[[Category:Logic in computer science]]\n[[Category:Combinatorial optimization]]\n[[Category:Satisfiability problems]]"
    },
    {
      "title": "Maximum weight matching",
      "url": "https://en.wikipedia.org/wiki/Maximum_weight_matching",
      "text": "In [[Computer science|computer science]], the '''maximum weight matching''' problem is the problem of finding, in a [[Weighted graph|weighted graph]], a [[Matching (graph theory)|matching]] in which the sum of weights is maximized. \n\nA special case of it is the [[Assignment problem|assignment problem]], in which the input is restricted to be a [[Bipartite graph|bipartite graph]].\n\n== Algorithms ==\nThere is a <math>O(V^{2}E)</math> time algorithm to find a maximum matching or a maximum weight matching in a graph that is not bipartite; it is due to [[Jack Edmonds]], is called the ''paths, trees, and flowers'' method or simply [[Edmonds's matching algorithm|Edmonds' algorithm]], and uses [[Bidirected graph|bidirected edges]]. A generalization of the same technique can also be used to find [[Maximum independent set|maximum independent sets]] in [[Claw-free graph|claw-free graphs]]. \n\nEdmonds' maximum matching algorithm has subsequently been improved to run in time <math>O(\\sqrt{V}\\cdot E)</math>  time using Micali and Vazirani's matching algorithm.<ref>{{citation|last1=Micali|first1=S.|title=[[Symposium on Foundations of Computer Science|Proc. 21st IEEE Symp. Foundations of Computer Science]]|pages=17–27|year=1980|contribution=An <math>\\scriptstyle O(\\sqrt{|V|}\\cdot|E|)</math> algorithm for finding maximum matching in general graphs|doi=10.1109/SFCS.1980.12|last2=Vazirani|first2=V. V.|author1-link=Silvio Micali|author2-link=Vijay Vazirani}}.</ref>\n\nAnother (randomized) algorithm by Mucha and Sankowski,<ref name=\"Mucha\">{{citation|last1=Mucha|first1=M.|title=[[Symposium on Foundations of Computer Science|Proc. 45th IEEE Symp. Foundations of Computer Science]]|pages=248–255|year=2004|contribution=Maximum Matchings via Gaussian Elimination|contribution-url=http://www.mimuw.edu.pl/~mucha/pub/mucha_sankowski_focs04.pdf|last2=Sankowski|first2=P.}}</ref> based on the fast [[matrix multiplication]] algorithm, gives <math>O(V^{2.376})</math> complexity.\n\n== References ==\n{{Reflist}}\n\n[[Category:Combinatorial optimization]]\n[[Category:Computational problems in graph theory]]"
    },
    {
      "title": "Multiprocessor scheduling",
      "url": "https://en.wikipedia.org/wiki/Multiprocessor_scheduling",
      "text": "In [[computer science]], '''multiprocessor scheduling''' is an [[NP-hard]] optimization problem.  The problem statement is: \"Given a set ''J'' of jobs where job ''j<sub>i</sub>'' has length ''l<sub>i</sub>'' and a number of processors ''m'', what is the minimum possible time required to schedule all jobs in ''J'' on ''m'' processors such that none overlap?\"<ref>{{cite book\n|first1=Michael R.\n|last1=Garey\n|first2=David S.\n|last2=Johnson\n|title=Computers and Intractability: A Guide to the Theory of NP-Completeness\n|publisher=W. H. Freeman and Company\n|page=238\n|isbn=978-0716710448|year=1979\n}}</ref>\nThe applications of this problem are numerous, but are, as suggested by the name of the problem, most strongly associated with the [[scheduling (computing)|scheduling]] of computational tasks in a [[multiprocessor]] environment.\n\nMultiprocessor schedulers have to schedule tasks which may or may not be dependent upon one another.\nFor example, take the case of reading user credentials from console, then use it to authenticate, then if authentication is successful display some data on the console.\nClearly one task is dependent upon another. This is a clear case of where some kind of [[Order theory|ordering]] exists between the tasks.\nIn fact it is clear that it can be modelled with [[partial ordering]]. Then, by definition, the set of tasks constitute a [[Lattice (order)|lattice structure]].\n\nThe general multiprocessor scheduling problem is a generalization of the optimization version of the [[partition problem|number partitioning problem]], which considers the case of partitioning a set of numbers (jobs) into two equal sets (processors).<ref>{{citation | year=2006 | chapter=The Easiest Hard Problem: Number Partitioning | last=Mertens|first=Stephan | title = Computational complexity and statistical physics | editor1=Allon Percus | editor2=Gabriel Istrate | editor3=[[Cris Moore|Cristopher Moore]] | publisher=Oxford University Press US | isbn=978-0-19-517737-4 | page=125 | chapter-url=https://books.google.com/books?id=4YD6AxV95zEC&pg=PA125 |arxiv=cond-mat/0310317| bibcode=2003cond.mat.10317M }}</ref>\n\n==Algorithms==\nA simple, often-used algorithm is the '''LPT algorithm''' (Longest Processing Time) which sorts the jobs by their processing time, longest first, and then assigns them to the machine with the earliest end time so far. This algorithm achieves an upper bound of 4/3&nbsp;-&nbsp;1/(3m)&nbsp;'''OPT'''.<ref>{{cite journal|last=Graham|first=R. L.|title=Bounds on Multiprocessing Timing Anomalies|journal=SIAM Journal on Applied Mathematics|year=1969|volume=17|issue=2|pages=416–429|doi=10.1137/0117039|citeseerx=10.1.1.90.8131}}</ref>\n\n== Static versus Dynamic ==\nMultiprocessor scheduling algorithms are static or dynamic. A scheduling algorithm is static if the scheduling decisions as to what computational tasks will be allocated to what processors are made before running the program. An algorithm is dynamic if it is taken at run time. For static scheduling algorithms, a typical approach is to rank the tasks according to their precedence relationships and use a list scheduling technique to schedule them onto the processors.<ref>{{Cite journal|last=Kwok|first=Yu-Kwong|last2=Ahmad|first2=Ishfaq|date=1999-12-01|title=Static scheduling algorithms for allocating directed task graphs to multiprocessors|journal=ACM Computing Surveys|volume=31|issue=4|pages=406–471|doi=10.1145/344588.344618|issn=0360-0300|citeseerx=10.1.1.322.2295}}</ref>\n\n==See also==\n*[[Job shop scheduling]], a similar problem for scheduling jobs on machines. Some variants of multiprocessor scheduling and job shop scheduling are equivalent problems.<!-- merge these two articles? -->\n\n==References==\n<references/>\n*A compendium of NP optimization problems. Editors: Pierluigi Crescenzi, and Viggo Kann [http://www.ensta.fr/~diam/ro/online/viggo_wwwcompendium/node180.html]\n\n[[Category:Scheduling (computing)]]\n[[Category:Combinatorial optimization]]\n\n{{comp-sci-stub}}"
    },
    {
      "title": "Network flow problem",
      "url": "https://en.wikipedia.org/wiki/Network_flow_problem",
      "text": "{{unreferenced|date=April 2018}}\nIn [[combinatorial optimization]], '''network flow problems''' are a class of computational problems in which the input is a [[flow network]] (a graph with numerical capacities on its edges), and the goal is to construct a [[Flow network#Flows|flow]], numerical values on each edge that respect the capacity constraints and that have incoming flow equal to outgoing flow at all vertices except for certain designated terminals.\n\nSpecific types of network flow problems include:\n*The [[maximum flow problem]], in which the goal is to maximize the total amount of flow out of the source terminals and into the sink terminals\n*The [[minimum-cost flow problem]], in which the edges have costs as well as capacities and the goal is to achieve a given amount of flow (or a maximum flow) that has the minimum possible cost\n*The [[multi-commodity flow problem]], in which one must construct multiple flows for different commodities whose total flow amounts together respect the capacities\n*[[Nowhere-zero flow]], a type of flow studied in combinatorics in which the flow amounts are restricted to a finite set of nonzero values\n\nThe [[max-flow min-cut theorem]] equates the value of a maximum flow to the value of a [[minimum cut]], a partition of the vertices of the flow network that minimizes the total capacity of edges crossing from one side of the partition to the other. [[Approximate max-flow min-cut theorem]]s provide an extension of this result to multi-commodity flow problems. The [[Gomory–Hu tree]] of an undirected flow network provides a concise representation of all minimum cuts between different pairs of terminal vertices.\n\n[[Algorithm]]s for constructing flows include\n*[[Dinic's algorithm]], a strongly polynomial algorithm for maximum flow\n*The [[Edmonds–Karp algorithm]], a faster strongly polynomial algorithm for maximum flow\n*The [[Ford–Fulkerson algorithm]], a greedy algorithm for maximum flow that is not in general strongly polynomial\n*The [[network simplex algorithm]], a method based on linear programming but specialized for network flow\n*The [[out-of-kilter algorithm]] for minimum-cost flow\n*The [[push–relabel maximum flow algorithm]], one of the most efficient known techniques for maximum flow\n\n{{sia}}\n\n[[Category:Network flow problem| ]]\n[[Category:Graph algorithms]]\n[[Category:Combinatorial optimization]]\n[[Category:Directed graphs]]"
    },
    {
      "title": "Nurse scheduling problem",
      "url": "https://en.wikipedia.org/wiki/Nurse_scheduling_problem",
      "text": "The '''nurse scheduling problem''' ('''NSP'''), also called the '''nurse rostering problem''' ('''NRP'''), is the [[operations research]] problem of finding an optimal way to assign nurses to shifts, typically with a set of [[Hard constraint|hard constraints]] which all valid solutions must follow, and a set of soft constraints which define the relative quality of valid solutions.<ref name=IoannisTassopoulos>\n{{cite journal\n | last1 = Solos\n | first1 = Ioannis\n | last2 = Tassopoulos\n | first2 = Ioannis\n | last3 = Beligiannis\n | first3 = Grigorios\n | title = A Generic Two-Phase Stochastic Variable Neighborhood Approach for Effectively Solving the Nurse Rostering Problem\n | journal = [[Algorithms (journal)|Algorithms]]\n | volume = 6\n | issue = 6\n | pages = 278–308\n | publisher = [[Multidisciplinary Digital Publishing Institute]]\n | date = 21 May 2013\n | url = http://mdpi.com/1999-4893/6/2/278/pdf\n | issn = 1999-4893\n | doi = 10.3390/a6020278\n | accessdate = 14 February 2014}}</ref> Solutions to the nurse scheduling problem can be applied to constrained scheduling problems in other fields.<ref name=YoungWoongKo>{{cite journal\n | last1 = Ko  | first1 = Young-Woong\n | last2 = Kim | first2 = Donghoi\n | last3 = Jeong | first3 = Minyeong\n | last4 = Jeon | first4 = Wooram\n | last5 = Uhmn | first5 = Saangyong\n | last6 = Kim | first6 = Jin\n | title = An Improvement Technique for Simulated Annealing and Its Application to Nurse Scheduling Problem\n | journal = International Journal of Software Engineering and its Applications\n | volume = 7\n | issue = 4\n | pages = 269–278\n | publisher = Science & Engineering Research Support Society\n | date = July 2013\n | url = http://www.sersc.org/journals/IJSEIA/vol7_no4_2013/23.pdf\n | accessdate = 20 March 2014}}</ref><ref name=UweAickelin >\n{{cite journal\n | last1 = Aickelin | first1 = Uwe\n | last2 = Dowsland | first2 = Kathryn A.\n | title = An Indirect Genetic Algorithm for a Nurse Scheduling Problem \n | journal = Computers & Operations Research\n | volume = 31\n | issue = 5\n | pages = 761–778\n | publisher = [[arXiv]]\n | date = 2004\n | doi=10.1016/s0305-0548(03)00034-0| arxiv = 0803.2969}}</ref><ref name=GarethBeddoe>\n{{cite journal\n | last1 = Beddoe | first1 = Gareth\n | last2 = Petrovic | first2 = Sanja\n | title = A novel approach to finding feasible solutions to personnel rostering problems\n | pages = 1–13\n | publisher = Proceedings of the 14th Annual Conference of the Production and Operation Management Society\n | location = Savannah, Georgia\n | date = 2003\n | url = http://pomsmeetings.org/ConfProceedings/001/Papers/PSC-08.1.pdf\n | accessdate = 20 March 2014}}</ref>\n\nThe nurse scheduling problem has been studied since before 1969,<ref name=RubenLagatie>{{cite journal\n | last1 = Lagatie\n | first1 = Ruben\n | last2 = Haspeslagh\n | first2 = Stefaan\n | last3 = De Causmaecker\n | first3 = Patrick\n | title = Negotiation Protocols for Distributed Nurse Rostering\n | publisher = [[Eindhoven University of Technology]] Department of Computer Science\n | year = 2009\n | url = http://wwwis.win.tue.nl/bnaic2009/papers/junk/bnaic2009_submission_41.pdf\n | accessdate = 14 February 2014}}</ref> and is known to have [[NP-hard]] complexity.<ref name=IoannisTassopoulos />\n\n==General description==\nThe nurse scheduling problem involves the assignment of shifts and holidays to [[nurse]]s. Each nurse has their own wishes and restrictions, as does the hospital. The problem is described as finding a schedule that both respects the constraints of the nurses and fulfills the objectives of the hospital. Conventionally, a nurse can work 3 shifts because nursing is [[shift work]]:\n* day shift\n* night shift\n* late night shift\n\nIn this problem we must search for a solution satisfying as many wishes as possible while not compromising the needs of the hospital.\n\n== Constraints ==\nThere are two types of constraints:\n* hard constraints: if this constraint fails then the entire schedule is invalid.\n* soft constraints: it is desirable that these constraints are met but not meeting them does not make the schedule invalid.\n\nSome examples of constraints are:\n* A nurse does not work the day shift, night shift and late night shift on the same day (for obvious reasons).\n* A nurse may go on a holiday and will not work shifts during this time.\n* A nurse does not do a late night shift followed by a day shift the next day.\n\nHard constraints typically include a specification of shifts (e.g. morning, afternoon, and night), that each nurse should work no more than one shift per day, and that all patients should have nursing coverage.<ref name=IoannisTassopoulos /> Differences in qualifications between nurses also create hard constraints.<ref>\n{{cite journal\n | last1 = Aickelin\n | first1 = Uwe\n | last2 = White\n | first2 = Paul\n | title = Building Better Nurse Scheduling Algorithms\n | journal = [[Annals of Operations Research]]\n | volume = 128\n | issue = 1–4\n | pages = 159–177\n | publisher = [[arXiv]]\n | date = 2004\n | doi=10.1023/b:anor.0000019103.31340.a6| arxiv = 0803.2967\n }}\n</ref> Soft constraints may include minimum and maximum numbers of shifts assigned to a given nurse in a given week, of hours worked per week, of days worked consecutively, of days off consecutively, and so on.<ref name=IoannisTassopoulos /> The shift preferences of individual nurses may be treated as a soft constraint,<ref name=MelissaGoodman>{{cite journal\n | last1 = Goodman | first1 = Melissa D.\n | last2 = Dowsland | first2 = Kathryn A.\n | last3 = Thompson | first3 = Jonathan M.\n | title = A GRASP-KNAPSACK HYBRID FOR A NURSE-SCHEDULING PROBLEM\n | pages = 1–31\n | publisher = [[Cardiff University]] School of Mathematics\n | location = Cardiff\n | date = 2009\n | url = http://www.caerdydd.ac.uk/maths/resources/grasp_nurse.pdf\n | accessdate = 4 October 2015}}\n</ref> or as a hard constraint.<ref name=GrahamWinstanley>{{cite journal\n | last = Winstanley\n | first = Graham \n | title = A hybrid approach to staff scheduling: The Staff Work Allocation Tool (SWAT) \n | pages = 1–12\n | publisher = [[University of Brighton]] School of Computing, Engineering and Mathematics\n | location = Brighton\n | url = http://www.cem.brighton.ac.uk/research/cig/papers/SWAT.pdf\n | accessdate = 20 March 2014}}</ref>\n\n== Solutions ==\nSolutions to the problem use a variety of techniques, including both mathematically exact solutions<ref name=MelissaGoodman/> and a variety of heuristic solutions using [[decomposition (computer science)|decomposition]],<ref name=RubenLagatie /> [[parallel computing]],<ref name=RubenLagatie /><ref name=BaumeltZdenek /> [[stochastic optimization]],<ref name=IoannisTassopoulos /> [[genetic algorithm]]s,<ref name=MelissaGoodman/> [[ant colony optimization algorithms|colony optimization]],<ref name=MelissaGoodman/> [[simulated annealing]],<ref name=MelissaGoodman/>,quantum annealing <ref>{{Cite journal|last=Humble|first=Travis S.|last2=Nakamura|first2=Yuma|last3=Ikeda|first3=Kazuki|date=2019-04-27|title=Application of Quantum Annealing to Nurse Scheduling Problem|url=https://arxiv.org/abs/1904.12139v1|language=en}}</ref> [[Tabu search]],<ref name=MelissaGoodman/> and [[coordinate descent]].<ref name=BaumeltZdenek>{{cite journal\n | last1 = Bäumelt | first1 = Zdeněk\n | last2 = Dvořák | first2 = Jan\n | last3 = Šůcha | first3 = Přemysl\n | last4 = Hanzálek | first4 = Zdeněk\n | title = A Novel Approach for Nurse Rerostering based on a Parallel Algorithm\n | journal = [[European Journal of Operational Research]]\n | volume = 251\n | issue = 2\n | pages = 624–639\n | publisher = Elsevier\n | date = 2016\n | url = http://www.sciencedirect.com/science/article/pii/S0377221715010711\n | accessdate = 2 December 2015\n | doi=10.1016/j.ejor.2015.11.022}}\n</ref><ref name=LizzyAugustine>{{cite journal\n | last1 = Augustine\n | first1 = Lizzy\n | last2 = Faer\n | first2 = Morgan\n | last3 = Kavountzis\n | first3 = Andreas\n | last4 = Patel\n | first4 = Reema\n | title = A Brief Study of the Nurse Scheduling Problem (NSP)\n | pages = 1–11\n | publisher = [[Carnegie Mellon School of Computer Science]]\n | location = Pittsburgh\n | date = 15 December 2009\n | url = http://www.math.cmu.edu/~af1p/Teaching/OR2/Projects/P23/ORProject_Final_Copy.pdf\n | accessdate = 20 March 2014}}</ref> \n\nBurke ''et al''. (2004)<ref name=EdmundBurke>{{cite journal\n| last1 = Burke\n| first1 = Edmund\n| last2 = De Causmaecker\n| first2 = Patrick\n| last3 = Berghe\n| first3 = Greet Vanden\n| last4 = Van Landeghem\n| first4 = Hendrik\n| title = The state of the art of nurse rostering\n| journal = Journal of Scheduling\n| volume = 7\n| issue = 6\n| pages = 441–499\n| url = https://lirias.kuleuven.be/bitstream/123456789/123829/1/JOS_\n| accessdate = 10 January 2016\n| doi = 10.1023/B:JOSH.0000046076.75950.0b\n| year = 2004\n}}</ref> summarised the state of art of academic research to the nurse rostering problem, including brief introductions of various then published solutions.\n\n== See also ==\n{{Portal|Nursing|Computer science}}\n* [[Assignment problem]]\n* [[Constraint programming]]\n* [[Employee scheduling software]]\n\n== References ==\n{{reflist|2}}\n\n==External links==\n* {{webarchive |url=https://web.archive.org/web/20150101000000/http://www.lania.mx/~ccoello/EMOO/jan00.ps.gz |date=January 1, 2015 |title=A study on how to solve the NSP using CGA }}\n*[https://www.mjc2.com/staff-planning-complexity.htm Why is Scheduling People Hard?] \n\n[[Category:NP-hard problems]]\n[[Category:Combinatorial optimization]]\n[[Category:Nursing informatics]]\n[[Category:Constraint programming]]\n[[Category:Time management]]\n[[Category:Scheduling (computing)]]"
    },
    {
      "title": "Parametric search",
      "url": "https://en.wikipedia.org/wiki/Parametric_search",
      "text": "In the design and analysis of [[algorithm]]s for [[combinatorial optimization]], '''parametric search''' is a technique invented by {{harvs|first=Nimrod|last=Megiddo|year=1983|authorlink=Nimrod Megiddo|txt}} for transforming a [[decision problem|decision algorithm]] (does this optimization problem have a solution with quality better than some given threshold?) into an [[Mathematical optimization|optimization algorithm]] (find the best solution). It is frequently used for solving optimization problems in [[computational geometry]].\n\n==Technique==\nThe basic idea of parametric search is to simulate a ''test algorithm'' that takes as input a numerical parameter <math>X</math>, as if it were being run with the  (unknown) optimal solution value <math>X^*</math> as its input.  This test algorithm is assumed to behave [[continuous function|discontinuously]] when <math>X=X^*</math>, and to operate on its parameter <math>X</math> only by simple comparisons of <math>X</math> with other computed values, or by testing the sign of low-degree [[polynomial function]]s of <math>X</math>. To simulate the algorithm, each of these comparisons or tests needs to be simulated, even though the <math>X</math> of the simulated algorithm is unknown.\nTo simulate each comparison, the parametric search applies a second algorithm, a ''decision algorithm'', that takes as input another numerical parameter <math>Y</math>, and that determines whether <math>Y</math> is above, below, or equal to the optimal solution value <math>X^*</math>.\n\nSince the decision algorithm itself necessarily behaves discontinuously at <math>X^*</math>, the same algorithm can also be used as the test algorithm. However, many applications use other test algorithms (often, [[comparison sort]]ing algorithms). Advanced versions of the parametric search technique use a [[parallel algorithm]]  as the test algorithm, and group the comparisons that must be simulated into batches, in order to significantly reduce the number of instantiations of the decision algorithm.\n\n===Sequential test algorithm===\nIn the most basic form of the parametric search technique, both the test algorithm and the decision algorithms are sequential (non-parallel) algorithms, possibly the same algorithm as each other. The technique simulates the test algorithm step by step, as it would run when given the (unknown) optimal solution value as its parameter <math>X</math>. Whenever the simulation reaches a step in which the test algorithm compares its parameter <math>X</math> to some other number <math>Y</math>, it cannot perform this step by doing a numerical comparison, as it does not know what <math>X</math> is. Instead, it calls the decision algorithm with parameter <math>Y</math>, and uses the result of the decision algorithm to determine the output of the comparison. In this way, the time for the simulation ends up equalling the product of the times for the test and decision algorithms. Because the test algorithm is assumed to behave discontinuously at the optimal solution value, it cannot be simulated accurately unless one of the parameter values <math>Y</math> passed to the decision algorithm is actually equal to the optimal solution value. When this happens, the decision algorithm can detect the equality and save the optimal solution value for later output.\nIf the test algorithm needs to know the sign of a polynomial in <math>X</math>, this can again be simulated by passing the [[root of a polynomial|roots of the polynomial]] to the decision algorithm in order to determine whether the unknown solution value is one of these roots, or, if not, which two roots it lies between.\n\nAn example considered both by {{harvtxt|Megiddo|1983}} and {{harvtxt|van Oostrum|Veltkamp|2002}} concerns a system of an odd number of particles, all moving rightward at different constant speeds along the same line. The median of the particles, also, will have a rightward motion, but one that is piecewise linear rather than having constant speed, because different particles will be the median at different times. Initially the particles, and their median, are to the left of the [[origin (mathematics)|origin]] of the line, and eventually they and their median will all be to the right of the origin. The problem is to compute the time <math>t_0</math> at which the median lies exactly on the origin.\nA [[linear time]] decision algorithm is easy to define: for any time <math>t</math>, one can compute the position of each particle at time <math>t</math> and count the number of particles on each side of the origin. If there are more particles on the left than on the right, then <math>t<t_0</math>, and if there are more particles on the right than on the left, then <math>t>t_0</math>. Each step of this decision algorithm compares the input parameter <math>t</math> to the time that one of the particles crosses the origin.\n\nUsing this decision algorithm as both the test algorithm and the decision algorithm of a parametric search leads to an algorithm for finding the optimal time <math>t_0</math> in quadratic total time. To simulate the decision algorithm for parameter <math>t_0</math>, the simulation must determine, for each particle, whether its crossing time is before or after <math>t_0</math>, and therefore whether it is to the left or right of the origin at time <math>t_0</math>. Answering this question for a single particle – comparing the crossing time for the particle with the unknown optimal crossing time <math>t_0</math> – can be performed by running the same decision algorithm with the crossing time for the particle as its parameter.\nThus, the simulation ends up running the decision algorithm on each of the particle crossing times, one of which must be the optimal crossing time.\nRunning the decision algorithm once takes linear time, so running it separately on each crossing time takes quadratic time.\n\n===Parallel test algorithm===\nAs {{harvtxt|Megiddo|1983}} already observed, the parametric search technique can be substantially sped up by replacing the simulated test algorithm by an efficient [[parallel algorithm]], for instance in the [[parallel random-access machine]] (PRAM) model of parallel computation, where a collection of processors operate in synchrony on a [[shared memory]], all performing the same sequence of operations on different memory addresses. If the test algorithm is a PRAM algorithm uses <math>P</math> processors and takes time <math>T</math> (that is, <math>T</math> steps in which each processor performs a single operation), then each of its steps may be simulated by using the decision algorithm to determine the results of at most <math>P</math> numerical comparisons. By finding a median or near-median value in the set of comparisons that need to be evaluated, and passing this single value to the decision algorithm, it is possible to eliminate half or nearly half of the comparisons with only a single call of the decision algorithm. By repeatedly halving the set of comparisons required by the simulation in this way, until none are left, it is possible to simulate the results of <math>P</math> numerical comparisons using only <math>O(\\log P)</math> calls to the decision algorithm. Thus, the total time for parametric search in this case becomes <math>O(PT)</math> (for the simulation itself) plus the time for <math>O(T\\log P)</math> calls to the decision algorithm (for <math>T</math> batches of comparisons, taking <math>O(\\log P)</math> calls per batch). Often, for a problem that can be solved in this way, the time-processor product of the PRAM algorithm is comparable to the time for a sequential decision algorithm, and the parallel time is [[Time complexity#Polylogarithmic time|polylogarithmic]], leading to a total time for the parametric search that is slower than the decision algorithm by only a polylogarithmic factor.\n\nIn the case of the example problem of finding the crossing time of the median of <math>n</math>moving particles, the sequential test algorithm can be replaced by a parallel [[sorting]] algorithm that sorts the positions of the particles at the time given by the algorithm's parameter, and then uses the sorted order to determine the median particle and find the sign of its position.\nThe best choice for this algorithm (according to its theoretical analysis, if not in practice) is the [[sorting network]] of {{harvs|author1-link=Miklós Ajtai|last1=Ajtai|author2-link=János Komlós (mathematician)|last2=Komlós|author3-link=Endre Szemerédi|last3=Szemerédi|year=1983|txt}}. This can be interpreted as a PRAM algorithm in which the number <math>P</math> of processors is proportional to the input length <math>n</math>, and the number of parallel steps is logarithmic. Thus, simulating this algorithm sequentially takes time <math>O(n\\log n)</math> for the simulation itself, together with <math>O(\\log n)</math> batches of comparisons, each of which can be handled by <math>O(\\log n)</math> calls to the linear-time decision algorithm. Putting these time bounds together gives <math>O(n\\log^2 n)</math> total time for the parametric search. This is not the optimal time for this problem – the same problem can be solved more quickly by observing that the crossing time of the median equals the median of the crossing times of the particles – but the same technique can be applied to other more complicated optimization problems, and in many cases provides the fastest known strongly polynomial algorithm for these problems.\n\nBecause of the large constant factors arising in the analysis of the AKS sorting network, parametric search using this network as the test algorithm is not practical. Instead, {{harvtxt|van Oostrum|Veltkamp|2002}} suggest using a parallel form of [[quicksort]] (an algorithm that repeatedly partitions the input into two subsets and then recursively sorts each subset). In this algorithm, the partition step is massively parallel (each input element should be compared to a chosen pivot element) and the two recursive calls can be performed in parallel with each other. The resulting parametric algorithm is slower in the [[worst-case analysis|worst case]] than an algorithm based on the AKS sorting network. However, van Oostrum and Veltkamp argue that if the results of past decision algorithms are saved (so that only the comparisons unresolved by these results will lead to additional calls to the decision algorithm) and the comparison values tested by the simulated test algorithm are sufficiently evenly distributed, then as the algorithm progresses the number of unresolved comparisons generated in each time step will be small. Based on this heuristic analysis, and on experimental results with an implementation of the algorithm, they argue that a quicksort-based parametric search algorithm will be more practical than its worst-case analysis would suggest.\n\n===Desynchronized sorting===\n{{harvtxt|Cole|1987}} further optimized the parametric search technique for cases (such as the example) in which the test algorithm is a comparison sorting algorithm. For the AKS sorting network and some other sorting algorithms that can be used in its place, Cole observes that it is not necessary to keep the simulated processors synchronized with each other: instead, one can allow some of them to progress farther through the sorting algorithm while others wait for the results of their comparisons to be determined. Based on this principle, Cole modifies the simulation of the sorting algorithm, so that it maintains a collection of unresolved simulated comparisons that may not all come from the same parallel time step of the test algorithm. As in the synchronized parallel version of parametric search, it is possible to resolve half of these comparisons by finding the median comparison value and calling the decision algorithm on this value. Then, instead of repeating this halving procedure until the collection of unresolved comparisons becomes empty, Cole allows the processors that were waiting on the resolved comparisons to advance through the simulation until they reach another comparison that must be resolved.\nUsing this method, Cole shows that a parametric search algorithm in which the test algorithm is sorting may be completed using only a logarithmic number of calls to the decision algorithm, instead of the <math>O(\\log^2 n)</math> calls made by Megiddo's original version of parametric search. Instead of using the AKS sorting network, it is also possible to combine this technique with a parallel [[merge sort]] algorithm of {{harvtxt|Cole|1988}}, resulting in time bounds with smaller constant factors that, however, are still not small enough to be practical. A similar speedup can be obtained for any problem that can be computed on a distributed computing network of bounded [[degree (graph theory)|degree]] (as the AKS sorting network is), either by Cole's technique or by a related technique of simulating multiple computation paths {{harv|Fernández-Baca|2001}}.\n\n{{harvtxt|Goodrich|Pszona|2013}} combine Cole's theoretical improvement with the practical speedups of {{harvtxt|van Oostrum|Veltkamp|2002}}. Instead of using a parallel quicksort, as van Oostrum and Veltkamp did, they use boxsort, a variant of quicksort developed by {{harvtxt|Reischuk|1985}} in which the partitioning step partitions the input randomly into <math>O(\\sqrt n)</math> smaller subproblems instead of only two subproblems. As in Cole's technique, they use a desynchronized parametric search, in which each separate thread of execution of the simulated parallel sorting algorithm is allowed to progress until it needs to determine the result of another comparison, and in which the number of unresolved comparisons is halved by finding the median comparison value and calling the decision algorithm with that value. As they show, the resulting randomized parametric search algorithm makes only a logarithmic number of calls to the decision algorithm with high probability, matching Cole's theoretical analysis. An extended version of their paper includes experimental results from an implementation of the algorithm, which show that the total running time of this method on several natural geometric optimization problems is similar to that of the best synchronized parametric search implementation (the quicksort-based method of van Oostrum and Veltkamp): a little faster on some problems and a little slower on some others. However, the number of calls that it makes to the decision algorithm is significantly smaller, so this method would obtain greater benefits in applications of parametric searching where the decision algorithm is slower.\n\nOn the example problem of finding the median crossing time of a point, both Cole's algorithm and the algorithm of Goodrich and Pszona obtain running time <math>O(n\\log n)</math>. In the case of Goodrich and Pszona, the algorithm is randomized, and obtains this time bound with high probability.\n\n==Comparison with binary search==\nThe [[bisection method]] (binary search) can also be used to transform decision into optimization. In this method, one maintains an [[interval (mathematics)|interval]] of numbers, known to contain the optimal solution value, and repeatedly shrinks the interval by calling the decision algorithm on its median value and keeping only the half-interval above or below the median, depending on the outcome of the call. Although this method can only find a numerical approximation to the optimal solution value, it does so in a number of calls to the decision algorithm proportional to the number of bits of precision of accuracy for this approximation, resulting in [[Time complexity#Strongly and weakly polynomial time|weakly polynomial]] algorithms.\n\nInstead, when applicable, parametric search finds strongly polynomial algorithms, whose running time is a function only of the input size and is independent of numerical precision. However, parametric search leads to an increase in time complexity (compared to the decision algorithm) that may be larger than logarithmic. Because they are strongly rather than weakly polynomial, algorithms based on parametric search are more satisfactory from a theoretical point of view. In practice, binary search is fast and often much simpler to implement, so [[algorithm engineering]] efforts are needed to make parametric search practical. Nevertheless, {{harvtxt|van Oostrum|Veltkamp|2002}} write that \"while a simple binary-search approach is often advocated as a practical replacement for parametric search, it is outperformed by our [parametric search] algorithm\" in the experimental comparisons that they performed.\n\n==Applications==\n[[File:Thiel-Sen estimator.svg|thumb|The [[Theil–Sen estimator]] compared to [[simple linear regression]]]]\nParametric search has been applied in the development of efficient algorithms for optimization problems, particularly in [[computational geometry]] {{harv|Agarwal|Sharir|Toledo|1994}}.\nThey include the following:\n*A [[Centerpoint (geometry)|centerpoint]] of a point set in a [[Euclidean space]] is a point such that any [[Half-space (geometry)|half-space]] containing the centerpoint also contains a constant fraction of the input points. For <math>d</math>-dimensional spaces, the optimal fraction that can be achieved is always at least <math>1/(d+1)</math>. Parametric-search based algorithms for constructing two-dimensional centerpoints were later improved to [[linear time]] using other algorithmic techniques. However, this improvement does not extend to higher dimensions. In three dimensions, parametric search can be used to find centerpoints in time <math>O(n^2\\log^4 n)</math> {{harv|Cole|1987}}.\n*Parametric search can be used as the basis for an <math>O(n\\log n)</math> time algorithm for the [[Theil–Sen estimator]], a method in [[robust statistics]] for fitting a line to a set of points that is much less sensitive to [[outlier]]s than [[simple linear regression]] {{harv|Cole|Salowe|Steiger|Szemerédi|1989}}.\n*In [[computer graphics]], the [[ray shooting]] problem (determining the first object in a scene that is intersected by a given line of sight or light beam) can be solved by combining parametric search with a data structure for a simpler problem, testing whether any of a given set of obstacles occludes a given ray {{harv|Agarwal|Matoušek|1993}}.\n*The [[biggest stick problem]] involves finding the longest line segment that lies entirely within a given [[polygon]]. It can be solved in time <math>O(n^{8/5+\\epsilon})</math>, for <math>n</math>-sided polygons and any <math>\\epsilon>0</math>, using an algorithm based on parametric search {{harv|Agarwal|Sharir|Toledo|1994}}.\n*The [[Annulus (mathematics)|annulus]] that contains a given point set and has the smallest possible width (difference between inner and outer radii) can be used as a measure of the [[Roundness (object)|roundness]] of the point set. Again, this problem can be solved in time <math>O(n^{8/5+\\epsilon})</math>, for <math>n</math>-sided polygons and any <math>\\epsilon>0</math>, using an algorithm based on parametric search {{harv|Agarwal|Sharir|Toledo|1994}}.\n*The [[Hausdorff distance]] between [[Translation (geometry)|translates]] of two polygons, with the translation chosen to minimize the distance, can be found using parametric search in time <math>O((mn)^2\\log^3 mn)</math>, where <math>m</math> and <math>n</math> are the numbers of sides of the polygons {{harv|Agarwal|Sharir|Toledo|1994}}.\n*The [[Fréchet distance]] between two [[polygonal chain]]s can be computed using parametric search in time <math>O(mn\\log mn)</math>, where <math>m</math> and <math>n</math> are the numbers of segments of the curves {{harv|Alt|Godau|1995}}.\n*For <math>n</math> points in the Euclidean plane, moving at constant velocities, the time at which the points obtain the smallest [[diameter]] (and the diameter at that time) can be found using a variation of Cole's technique in time <math>O(n\\log^2 n)</math>. Parametric search can also be used for other similar problems of finding the time at which some measure of a set of moving points obtains its minimum value, for measures including the size of the [[Smallest-circle problem|minimum enclosing ball]] or the weight of the [[minimum spanning tree|maximum spanning tree]] {{harv|Fernández-Baca|2001}}.\n\n==References==\n*{{citation\n | last1 = Agarwal | first1 = Pankaj K. | author1-link = Pankaj K. Agarwal\n | last2 = Matoušek | first2 = Jiří | author2-link = Jiří Matoušek (mathematician)\n | doi = 10.1137/0222051\n | issue = 4\n | journal = SIAM Journal on Computing\n | mr = 1227762\n | pages = 794–806\n | title = Ray shooting and parametric search\n | volume = 22\n | year = 1993}}\n*{{citation\n | last1 = Agarwal | first1 = Pankaj K. | author1-link = Pankaj K. Agarwal\n | last2 = Sharir | first2 = Micha | author2-link = Micha Sharir\n | last3 = Toledo | first3 = Sivan\n | doi = 10.1006/jagm.1994.1038\n | issue = 3\n | journal = Journal of Algorithms\n | mr = 1300253\n | pages = 292–318\n | title = Applications of parametric searching in geometric optimization\n | volume = 17\n | year = 1994}}.\n*{{citation\n | last1 = Ajtai | first1 = M. | author1-link = Miklós Ajtai\n | last2 = Komlós | first2 = J. | author2-link = János Komlós (mathematician)\n | last3 = Szemerédi | first3 = E. | author3-link = Endre Szemerédi\n | contribution = An {{math|''O''(''n'' log ''n'')}} sorting network\n | doi = 10.1145/800061.808726\n | isbn = 0-89791-099-0\n | pages = 1–9\n | title = [[Symposium on Theory of Computing|Proceedings of the 15th ACM Symposium on Theory of Computing (STOC '83)]]\n | year = 1983}}.\n*{{citation\n | last1 = Alt | first1 = Helmut\n | last2 = Godau | first2 = Michael\n | doi = 10.1142/S0218195995000064\n | issue = 1-2\n | journal = International Journal of Computational Geometry & Applications\n | mr = 1331177\n | pages = 75–91\n | title = Computing the Fréchet distance between two polygonal curves\n | url = http://www.staff.science.uu.nl/~kreve101/asci/ag-cfdbt-95.pdf\n | volume = 5\n | year = 1995}}.\n*{{citation\n | last = Cole | first = Richard\n | doi = 10.1145/7531.7537\n | issue = 1\n | journal = [[Journal of the ACM]]\n | mr = 882669\n | pages = 200–208\n | title = Slowing down sorting networks to obtain faster sorting algorithms\n | volume = 34\n | year = 1987}}.\n*{{citation\n | last = Cole | first = Richard\n | doi = 10.1137/0217049\n | issue = 4\n | journal = [[SIAM Journal on Computing]]\n | mr = 953293\n | pages = 770–785\n | title = Parallel merge sort\n | volume = 17\n | year = 1988}}.\n*{{citation\n | last1 = Cole | first1 = Richard\n | last2 = Salowe | first2 = Jeffrey S.\n | last3 = Steiger | first3 = W. L.\n | last4 = Szemerédi | first4 = Endre | author4-link = Endre Szemerédi\n | doi = 10.1137/0218055\n | issue = 4\n | journal = [[SIAM Journal on Computing]]\n | mr = 1004799\n | pages = 792–810\n | title = An optimal-time algorithm for slope selection\n | volume = 18\n | year = 1989}}.\n*{{citation\n | last = Fernández-Baca | first = D.\n | doi = 10.1007/s00453-001-0001-2\n | issue = 1\n | journal = Algorithmica\n | mr = 1816864\n | pages = 1–11\n | title = On nonlinear parametric search\n | volume = 30\n | year = 2001}}.\n*{{citation\n | last1 = Goodrich | first1 = Michael T. | author1-link = Michael T. Goodrich\n | last2 = Pszona | first2 = Paweł\n | arxiv = 1306.3000\n | contribution = Cole's parametric search technique made practical\n | contribution-url = http://cccg.ca/proceedings/2013/papers/paper_19.pdf\n | title = Proc. 25th Canadian Conference on Computational Geometry (CCCG 2013)\n | year = 2013| bibcode = 2013arXiv1306.3000G}}.\n*{{citation\n | last = Megiddo | first = Nimrod | authorlink = Nimrod Megiddo\n | doi = 10.1145/2157.322410\n | issue = 4\n | journal = [[Journal of the ACM]]\n | mr = 819134\n | pages = 852–865\n | title = Applying parallel computation algorithms in the design of serial algorithms\n | volume = 30\n | year = 1983}}.\n*{{citation\n | last = Reischuk | first = Rüdiger\n | doi = 10.1137/0214030\n | issue = 2\n | journal = [[SIAM Journal on Computing]]\n | mr = 784745\n | pages = 396–409\n | title = Probabilistic parallel algorithms for sorting and selection\n | volume = 14\n | year = 1985}}.\n*{{citation\n | last1 = van Oostrum | first1 = René\n | last2 = Veltkamp | first2 = Remco C.\n | contribution = Parametric search made practical\n | doi = 10.1145/513400.513401\n | isbn = 1-58113-504-1\n | location = New York, NY, USA\n | pages = 1–9\n | publisher = ACM\n | title = [[Symposium on Computational Geometry|Proceedings of the Eighteenth Annual Symposium on Computational Geometry (SoCG '02)]]\n | year = 2002}}.\n\n[[Category:Combinatorial optimization]]"
    },
    {
      "title": "Quadratic assignment problem",
      "url": "https://en.wikipedia.org/wiki/Quadratic_assignment_problem",
      "text": "{{short description|combinatorial optimization problem}}\nThe '''quadratic assignment problem''' ('''QAP''') is one of the fundamental [[combinatorial optimization]] problems in the branch of [[Optimization (mathematics)|optimization]] or [[operations research]] in [[mathematics]], from the category of the [[facilities location]] problems.\n\nThe problem models the following real-life problem:\n\n:There are a set of ''n'' facilities and a set of ''n'' locations. For each pair of locations, a ''distance'' is specified and for each pair of facilities a ''weight'' or ''flow'' is specified (e.g., the amount of supplies transported between the two facilities). The problem is to assign all facilities to different locations with the goal of minimizing the sum of the distances multiplied by the corresponding flows.\n\nIntuitively, the cost function encourages factories with high flows between each other to be placed close together.\n\nThe problem statement resembles that of the [[assignment problem]], except that the [[Loss function|cost function]] is expressed in terms of quadratic inequalities, hence the name.\n\n==Formal mathematical definition==\n\nThe formal definition of the quadratic assignment problem is as follows:\n\n:Given two sets, ''P'' (\"facilities\") and ''L'' (\"locations\"), of equal size, together with a [[weight function]] ''w'' : ''P'' &times; ''P'' → '''[[real number|R]]''' and a distance function ''d'' : ''L'' &times; ''L'' → '''[[real number|R]]'''. Find the [[bijection]] ''f'' : ''P'' → ''L'' (\"assignment\") such that the cost function:\n\n::<math>\\sum_{a,b\\in P}w(a,b)\\cdot d(f(a), f(b))</math>\n:is minimized.\n\nUsually  weight and distance functions are viewed as square real-valued [[matrix (mathematics)|matrices]], so that the cost function is written down as:\n\n:<math>\\sum_{a,b\\in P}w_{a,b}d_{f(a),f(b)}</math>\n\nIn matrix notation:\n\n:<math>\\min_{X\\in\\Pi_n} \\operatorname{trace}(WXDX^T)</math>\n\nwhere <math>\\Pi_n</math> is the set of <math>n \\times n</math> permutation matrices, <math>W</math> is the weight matrix and <math>D</math> is the distance matrix.\n\n== Computational complexity ==\n\nThe problem is [[NP-hard]], so there is no known [[algorithm]] for solving this problem in polynomial time, and even small instances may require long computation time. It was also proven that the problem does not have an approximation algorithm running in polynomial time for any (constant) factor,  unless P = NP.<ref>{{Cite journal|url = http://dl.acm.org/citation.cfm?id=321958.321975&coll=DL&dl=GUIDE&CFID=533242734&CFTOKEN=20099075\n|title = P-Complete Approximation Problems\n|last1 = Sahni|first1 = Sartaj\n|last2 = Gonzalez |first2 = Teofilo \n|date = July 1976\n|volume=23 |issue=3\n|journal = Journal of the ACM|accessdate = \n|doi = 10.1145/321958.321975|pmid = }}</ref> The [[travelling salesman problem]] may be seen as a special case of QAP if one assumes that the flows connect all facilities only along a single ring, all flows have the same non-zero (constant) value. Many other problems of standard [[combinatorial optimization]] problems may be written in this form.\n\n== Applications ==\n\nIn addition to the original plant location formulation, QAP is a mathematical model for the problem of placement of interconnected [[electronic component]]s onto a [[printed circuit board]] or on a [[integrated circuit|microchip]], which is part of the [[place and route]] stage of [[computer aided design]] in the electronics industry.\n\n==See also==\n*[[Quadratic bottleneck assignment problem]]\n\n== References ==\n;Notes\n{{Reflist}}\n;Sources\n* {{cite book|author = [[Michael R. Garey]] and [[David S. Johnson]] | year = 1979 | title = [[Computers and Intractability: A Guide to the Theory of NP-Completeness]] | publisher = W.H. Freeman | isbn = 0-7167-1045-5}} A2.5: ND43, pg.218.\n\n==External links==\n* http://www.seas.upenn.edu/qaplib/ QAPLIB - A Quadratic Assignment Problem Library\n* http://issuu.com/spconguy/docs/ant-algorithm-applied-to-the-quadratic-assignment- - A QAP sample application\n* [http://www.adaptivebox.net/CILib/code/qapcodes_link.html Links to Quadratic Assignment Problem Solvers]\n\n[[Category:NP-hard problems]]\n[[Category:Combinatorial optimization]]"
    },
    {
      "title": "Quadratic bottleneck assignment problem",
      "url": "https://en.wikipedia.org/wiki/Quadratic_bottleneck_assignment_problem",
      "text": "In mathematics, the '''quadratic bottleneck assignment problem''' ('''QBAP''') is one of fundamental [[combinatorial optimization]] problems in the branch of [[Optimization (mathematics)|optimization]] or [[operations research]], from the category of the [[facilities location]] problems.<ref>[http://www.assignmentproblems.com/ Assignment Problems], by [[Rainer Burkard]], Mauro Dell'Amico, Silvano Martello, 2009</ref>\n\nIt is related to the [[quadratic assignment problem]] in the same way as the [[linear bottleneck assignment problem]] is related to the [[linear assignment problem]], the \"sum\" is replaced with \"max\" in the [[objective function]].\n\nThe problem models the following real-life problem:\n\n:There are a set of ''n'' facilities and a set of ''n'' locations. For each pair of locations, a ''distance'' is specified and for each pair of facilities a ''weight'' or ''flow'' is specified (e.g., the amount of supplies transported between the two facilities). The problem is to assign all facilities to different locations with the goal of minimizing the maximum of the distances multiplied by the corresponding flows.\n\n==Computational complexity==\nThe problem is [[NP-hard]], as it can be used to formulate the [[Hamiltonian cycle]] problem by using flows in the pattern of a cycle and distances that are short for graph edges and long for non-edges.<ref>{{citation\n | last1 = Burkard | first1 = R. E.\n | last2 = Fincke | first2 = U.\n | doi = 10.1007/BF01583791\n | issue = 2\n | journal = Mathematical Programming\n | mr = 657082\n | pages = 227–232\n | title = On random quadratic bottleneck assignment problems\n | volume = 23\n | year = 1982}}.</ref>\n\n==Special cases==\n*[[Bottleneck traveling salesman problem]]\n*[[Graph bandwidth problem]]\n\n==References==\n{{reflist}}\n\n[[Category:NP-hard problems]]\n[[Category:Combinatorial optimization]]"
    },
    {
      "title": "Quadratic pseudo-Boolean optimization",
      "url": "https://en.wikipedia.org/wiki/Quadratic_pseudo-Boolean_optimization",
      "text": "{{short description|Combinatorial optimization method for pseudo-Boolean functions}}\n'''Quadratic pseudo-Boolean optimisation''' ('''QPBO''') is a [[combinatorial optimization]] method for quadratic [[pseudo-Boolean function]]s in the form\n\n:<math> f(\\mathbf{x}) = w_0 + \\sum_{p \\in V} w_p(x_p) + \\sum_{(p, q) \\in E} w_{pq}(x_p, x_q) </math>\n\nin the binary variables <math>x_p \\in \\{0, 1\\} \\; \\forall p \\in V = \\{1, \\dots, n\\}</math>, with <math>E \\subseteq V \\times V</math>. If <math>f</math> is submodular then QPBO produces a global optimum equivalently to [[graph cut optimization]], while if <math>f</math> contains non-submodular terms then the algorithm produces a partial solution with specific optimality properties, in both cases in [[polynomial time]].<ref name=\"review\" />\n\nQPBO is a useful tool for inference on [[Markov random field]]s and [[conditional random field]]s, and has applications in [[computer vision]] problems such as [[image segmentation]] and [[stereo cameras|stereo matching]].<ref name=\"rother\" />\n\n== Optimization of non-submodular functions ==\n\nIf the coefficients <math>w_{pq}</math> of the quadratic terms satisfy the submodularity condition\n\n:<math> w_{pq}(0, 0) + w_{pq}(1, 1) \\le w_{pq}(0, 1) + w_{pq}(1, 0) </math>\n\nthen the function can be efficiently optimised with [[graph cut optimization]]. It is indeed possible to represent it with a non-negative weighted [[graph (discrete mathematics)|graph]], and the global minimum can be found in polynomial time by computing a [[minimum cut]] of the graph, which can be computed with algorithms such as [[Ford–Fulkerson algorithm|Ford–Fulkerson]], [[Edmonds–Karp algorithm|Edmonds–Karp]], and [[Boykov–Kolmogorov algorithm|Boykov–Kolmogorov]]'s.\n\nIf the function is not submodular, then the problem is [[NP-hard]] in the general case and it is not always possible to solve it exactly in polynomial time. It is possible to replace the target function with a similar but submodular approximation, e.g. by removing all non-submodular terms or replacing them with submodular approximations, but such approach is generally sub-optimal and it produces satisfying results only if the number of non-submodular terms is relatively small. <ref name=\"review\" />\n\nQPBO builds an extended graph, introducing a set of auxiliary variables ideally equivalent to the negation of the variables in the problem. If the nodes in the graph associated to a variable (representing the variable itself and its negation) are separated by the [[minimum cut]] of the graph in two different connected components, then the optimal value for such variable is well defined, otherwise it is not possible to infer it. Such method produces results generally superior to submodular approximations of the target function.<ref name=\"review\" />\n\n== Properties ==\nQPBO produces a solution where each variable assumes one of three possible values, ''true'', ''false'', and ''undefined'', noted in the following as 1, 0, and <math>\\emptyset</math> respectively. The solution satisfies the two following properties:\n\n* ''Partial optimality'': if <math>f</math> is submodular, then QPBO produces a global minimum exactly, equivalently to [[graph cut optimization|graph cut]], and all variables have a non-undefined value. If submodularity is not satisfied, the result will be a partial solution <math>\\mathbf{x}</math> where a subset <math>\\hat{V} \\subseteq V</math> of the variables have a non-undefined value. Such partial solution is always part of a global solution, i.e. there exist a global minimum point <math>\\mathbf{x^*}</math> for <math>f</math> such that <math>x_i = x_i^*</math> for each <math>i \\in \\hat{V}</math>.\n\n* ''persistence'': given a solution <math>\\mathbf{x}</math> generated by QPBO and an arbitrary assignment of values <math>\\mathbf{y}</math> to the variables, if a new solution <math>\\hat{\\mathbf{y}}</math> is constructed by replacing <math>y_i</math> with <math>x_i</math> for each <math>i \\in \\hat{V}</math>, then <math>f(\\hat{\\mathbf{y}}) \\le f(\\mathbf{y})</math>.<ref name=\"review\" />\n\n== Algorithm ==\n[[File:Qpbo.svg|thumb|upright=2|Graph representing a function of two variables <math>x_p</math> and <math>x_q</math>.]]\n\nThe algorithm can be divided in three steps: graph construction, max-flow computation, and assignment of values to the variables.\n\nWhen constructing the graph, the set of vertices <math>V</math> contains the source and sink nodes <math>s</math> and <math>t</math>, and a couple of nodes <math>p</math> and <math>p'</math> for each variable. After re-parametrising the function to normal form,<ref name=\"normal form\" group=\"note\" /> a pair of edges is added to the graph for each term <math>w</math>:\n* for each term <math>w_p(0)</math> the edges <math>p \\rightarrow t</math> and <math>s \\rightarrow p'</math>, with weight <math>\\frac{1}{2} w_p(0)</math>;\n* for each term <math>w_p(1)</math> the edges <math>s \\rightarrow p</math> and <math>p' \\rightarrow t</math>, with weight <math>\\frac{1}{2} w_p(1)</math>;\n* for each term <math>w_{pq}(0, 1)</math> the edges <math>p \\rightarrow q</math> and <math>q' \\rightarrow p'</math>, with weight <math>\\frac{1}{2} w_{pq}(0, 1)</math>;\n* for each term <math>w_{pq}(1, 0)</math> the edges <math>q \\rightarrow p</math> and <math>p' \\rightarrow q'</math>, with weight <math>\\frac{1}{2} w_{pq}(1, 0)</math>;\n* for each term <math>w_{pq}(0, 0)</math> the edges <math>p \\rightarrow q'</math> and <math>q \\rightarrow p'</math>, with weight <math>\\frac{1}{2} w_{pq}(0, 0)</math>;\n* for each term <math>w_{pq}(1, 1)</math> the edges <math>q' \\rightarrow p</math> and <math>p' \\rightarrow q</math>, with weight <math>\\frac{1}{2} w_{pq}(1, 1)</math>.\n\nThe [[minimum cut]] of the graph can be computed with a [[max-flow min-cut theorem|max-flow algorithm]]. In the general case, the minimum cut is not unique, and each minimum cut correspond to a different partial solution, however it is possible to build a minimum cut such that the number of undefined variables is minimal.\n\nOnce the minimum cut is known, each variable receives a value depending upon the position of its corresponding nodes <math>p</math> and <math>p'</math>: if <math>p</math> belongs to the connected component containing the source and <math>p'</math> belongs to the connected component containing the sink then the variable will have value of 0. Vice versa, if <math>p</math> belongs to the connected component containing the sink and <math>p'</math> to the one containing the source, then the variable will have value of 1. If both nodes <math>p</math> and <math>p'</math> belong to the same connected component, then the value of the variable will be undefined.<ref name=\"rother\" />\n\nThe way undefined variables can be handled is dependant upon the context of the problem. In the general case, given a [[partition of a set|partition]] of the graph in two sub-graphs and two solutions, each one optimal for one of the sub-graphs, then it is possible to combine the two solutions into one solution optimal for the whole graph in polynomial time.<ref name=\"billionnet\" /> However, computing an optimal solution for the subset of undefined variables is still a [[NP-hard]] problem. In the context of iterative algorithms such as <math>\\alpha</math>-expansion, a reasonable approach is to leave the value of undefined variables unchanged, since the persistence property guarantees that the target function will have non-increasing value.<ref name=\"review\" /> Different exact and approximate strategies to minimise the number of undefined variables exist.<ref name=\"rother\" />\n\n== Higher order terms ==\n\nThe problem of optimizing higher-order pseudo-boolean functions is generally difficult. It is always possible to reduce a higher-order function to a quadratic function which is equivalent with respect to the optimisation, problem known as \"higher-order [[clique (graph theory)|clique]] reduction\" (HOCR), and the result of such reduction can be optimized with QPBO. Generic methods for reduction of arbitrary functions rely on specific substitution rules and in the general case they require the introduction of auxiliary variables.<ref name=\"fix\" /> In practice most terms can be reduced without introducing additional variables, resulting in a simpler optimization problem, and the remaining terms can be reduced exactly, with addition of auxiliary variables, or approximately, without addition of any new variable.<ref name=\"elc\" />\n\n== References ==\n<references>\n<ref name=\"review\">Kolmogorov and Rother (2007).</ref>\n<ref name=\"fix\">Fix et al. (2011).</ref>\n<ref name=\"elc\">Ishikawa (2014).</ref>\n<ref name=\"billionnet\">Billionnet and Jaumard (1989).</ref>\n<ref name=\"rother\">Rother et al. (2007).</ref>\n</references>\n* {{cite journal|first1=Alain|last1=Billionnet|first2=Brigitte|last2=Jaumard|title=A decomposition method for minimizing quadratic pseudo-boolean functions|journal=Operations Research Letters|volume=8|number=3|publisher=Elsevier|pages=161–163|year=1989}}\n* {{cite conference|first1=Alexander|last1=Fix|first2=Aritanan|last2=Gruber|first3=Endre|last3=Boros|first4=Ramin|last4=Zabih|title=A graph cut algorithm for higher-order Markov random fields|conference=IEEE International Conference on Computer Vision|year=2011|pages=1020–1027}}\n* {{cite conference|first1=Hiroshi|last1=Ishikawa|title=Higher-Order Clique Reduction Without Auxiliary Variables|conference=IEEE Conference on Computer Vision and Pattern Recognition|year=2014|publisher=IEEE|pages=1362–1269}}\n* {{cite journal|first1=Vladimir|last1=Kolmogorov|first2=Carsten|last2=Rother|title=Minimizing Nonsubmodular Functions: A Review|journal=IEEE Transactions on Pattern Analysis and Machine Intelligence|volume=29|number=7|year=2007|pages=1274–1279|publisher=IEEE}}\n* {{cite conference|first1=Carsten|last1=Rother|first2=Vladimir|last2=Kolmogorov|first3=Victor|last3=Lempitsky|first4=Martin|last4=Szummer|title=Optimizing binary MRFs via extended roof duality|conference=IEEE Conference on Computer Vision and Pattern Recognition|pages=1–8|year=2007}}\n\n== Notes ==\n<references group=\"note\">\n<ref name=\"normal form\">The representation of a pseudo-Boolean function with coefficients <math>\\mathbf{w} = (w_0, w_1, \\dots, w_{nn})</math> is not unique, and if two coefficient vectors <math>\\mathbf{w}</math> and <math>\\mathbf{w}'</math> represent the same function then <math>\\mathbf{w}'</math> is said to be a reparametrisation of <math>\\mathbf{w}</math> and vice versa. In some constructions it is useful to ensure that the function has a specific form, called ''noraml form'', which is always definded for any function, and it is not unique. A function <math>f</math> is in normal form if the two following conditions hold (Kolmogorov and Rother (2007)):\n# <math>\\min \\{ w_p^0, w_p^1 \\} = 0</math> for each <math>p \\in V</math>;\n# <math>\\min \\{ w_{pq}^{0j}, w_{pq}^{1j} \\} = 0</math> for each <math>(p, q) \\in E</math> and for each <math>j \\in \\{0, 1\\}</math>.\n\nGiven an arbitrary function <math>f</math>, it is always possible to find a reparametrisation to normal form with the following algorithm in two steps (Kolmogorov and Rother (2007)):\n\n# as long as there exist indices <math>(p, q) \\in E</math> and <math>j \\in \\{0, 1\\}</math> such that the second condition of normality is not satisfied, substitute:\n#* <math>w_{pq}^{0j}</math> with <math>w_{pq}^{0j} - a</math>\n#* <math>w_{pq}^{1j}</math> with <math>w_{pq}^{1j} - a</math>\n#* <math>w_q^j</math> with <math>w_q^j + a</math>\n#: where <math>a = \\min \\{ w_{pq}^{0j}, w_{pq}^{1j} \\}</math>;\n# for <math>p = 1, \\dots, n</math>, substitute:\n#* <math>w_0</math> with <math>w_0 + a</math>\n#* <math>w_p^0</math> with <math>w_p^0 - a</math>\n#* <math>w_p^1</math> with <math>w_p^1 - a</math>\n#: where <math>a = \\min \\{ w_p^0, w_p^1 \\}</math>.\n</ref>\n</references>\n\n== External links ==\n* [http://pub.ist.ac.at/~vnk/software.html#qpbo Implementation of QPBO (C++)], available under the [[GNU General Public License]], by Vladimir Kolmogorov.\n* [http://www.f.waseda.jp/hfs/software.html Implementation of HOCR (C++)], available under the [[MIT license]], by Hiroshi Ishikawa.\n\n[[Category:Combinatorial optimization]]\n[[Category:Computational problems in graph theory]]"
    },
    {
      "title": "Smallest-circle problem",
      "url": "https://en.wikipedia.org/wiki/Smallest-circle_problem",
      "text": "[[Image:Smallest circle problem.svg|thumb|right|300px|Some instances of the smallest bounding circle.]]\n\nThe '''smallest-circle problem''' or '''minimum covering circle problem''' is a [[mathematical problem]] of computing the smallest [[circle]] that contains all of a given [[set (mathematics)|set]] of [[point (geometry)|points]] in the [[Euclidean plane]]. The corresponding problem in [[n-dimensional space|''n''-dimensional space]], the [[smallest bounding sphere]] problem, is to compute the smallest [[n-sphere|''n''-sphere]] that contains all of a given set of points.<ref name=\"eh-mcsp-72\">{{citation|first1=J.|last1=Elzinga|first2=D. W.|last2=Hearn|title=The minimum covering sphere problem|journal=[[Management Science (journal)|Management Science]]|volume=19|pages=96–104|year=1972|doi=10.1287/mnsc.19.1.96}}</ref> The smallest-circle problem was initially proposed by the English mathematician [[James Joseph Sylvester]] in 1857.<ref>{{citation|first=J. J.|last=Sylvester|authorlink=James Joseph Sylvester|title=A question in the geometry of situation|journal=[[Quarterly Journal of Mathematics]]|volume=1|page=79|year=1857}}.</ref>\n\nThe smallest-circle problem in [[the plane]] is an example of a [[facility location]] problem (the [[1-center problem]]) in which the location of a new facility must be chosen to provide service to a number of customers, minimizing the farthest distance that any customer must travel to reach the new facility.<ref>{{citation|last1=Francis|first1=R. L.|first2=L. F.|last2=McGinnis|first3=J. A.|last3=White|title=Facility Layout and Location: An Analytical Approach|edition=2nd|publisher=Prentice–Hall, Inc.|location=Englewood Cliffs, N.J.|year=1992}}.</ref> Both the smallest circle problem in the plane, and the smallest bounding sphere problem in any higher-dimensional space of bounded dimension are solvable in [[Worst-case complexity|worst-case]] [[linear time]].\n\n==Characterization==\nMost of the geometric approaches for the problem look for points that lie on the boundary of the minimum circle and are based on the following simple facts:\n* The minimum covering circle is unique.\n* The minimum covering circle of a set ''S'' can be determined by at most three points in ''S'' which lie on the boundary of the circle. If it is determined by only two points, then the [[line segment]] joining those two points must be a [[diameter]] of the minimum circle. If it is determined by three points, then the triangle consisting of those three points is not [[obtuse triangle|obtuse]].\n\n{{Collapse top|title=Proof that the minimum covering disk is unique}}\nLet {{var|P}} be any set of points in the plane, and suppose that there are two smallest enclosing disks of {{var|P}}, of radius with centers at <math>\\vec{z}_1</math> and <math>\\vec{z}_2</math>. Let <math>r</math> be their shared radius, and let <math>2a</math> be the distance between their centers. Since {{var|P}} is a subset of both disks it is a subset of their intersection. However, their intersection is contained within the disk with center <math>\\frac 12 (\\vec{z}_1+\\vec{z}_2)</math> and radius <math>\\sqrt{r^2-a^2}</math>, as shown in the following image:\n\n:[[Image:The smallest enclosing disk of points in the plane is unique.svg|300px]]\n\nSince {{var|r}} is minimal, we must have <math>\\sqrt{r^2-a^2}=r</math>, meaning <math>a=0</math>, so the disks are identical.{{sfn|Welzl|1991|p=2}}\n{{Collapse bottom}}\n\n==Linear-time solutions==\nAs [[Nimrod Megiddo]] showed,<ref>{{citation\n | last = Megiddo | first = Nimrod | authorlink = Nimrod Megiddo\n | doi = 10.1137/0212052\n | issue = 4\n | journal = [[SIAM Journal on Computing]]\n | mr = 721011\n | pages = 759–776\n | title = Linear-time algorithms for linear programming in {{math|'''R'''<sup>3</sup>}} and related problems\n | volume = 12\n | year = 1983}}.</ref> the minimum enclosing circle can be found in linear time, and the same linear time bound also applies to the [[smallest enclosing sphere]] in Euclidean spaces of any constant dimension. His article also gives a brief overview of earlier O(n^3) and O(n log n) algorithms.<ref name=\"msw\" />\n\n[[Emo Welzl]]<ref>{{citation\n | last = Welzl | first = Emo | authorlink = Emo Welzl\n | editor-last = Maurer | editor-first = H.\n | doi = 10.1007/BFb0038202\n | contribution = Smallest enclosing disks (balls and ellipsoids)\n | volume = 555\n | pages = 359–370\n | publisher = Springer-Verlag\n | series = Lecture Notes in Computer Science\n | title = New Results and New Trends in Computer Science\n | year = 1991| isbn = 978-3-540-54869-0 | citeseerx = 10.1.1.46.1450 }}.</ref> proposed a simple [[randomized algorithm]] for the\nminimum covering circle problem that runs in [[expected time]] <math>O(N)</math>, based on a [[linear programming]] algorithm of [[Raimund Seidel]].\n\nSubsequently, the smallest-circle problem was included in a general class of [[LP-type problem]]s that can be solved by algorithms like Welzl's based on linear programming. As a consequence of membership in this class, it was shown that the dependence on the dimension of the constant factor in the <math>O(N)</math> time bound, which was factorial for Seidel's method, could be reduced to [[subexponential time|subexponential]].<ref name=msw>{{citation\n | last1 = Matoušek | first1 = Jiří | author1-link = Jiří Matoušek (mathematician)\n | last2 = Sharir | first2 = Micha | author2-link = Micha Sharir\n | last3 = Welzl | first3 = Emo\n | doi = 10.1007/BF01940877\n | journal = [[Algorithmica]]\n | pages = 498–516\n | title = A subexponential bound for linear programming\n | url = http://www.inf.ethz.ch/personal/emo/PublFiles/SubexLinProg_ALG16_96.pdf\n | volume = 16\n | issue = 4–5 | year = 1996| citeseerx = 10.1.1.46.5644 }}.</ref>\n\n=== Megiddo's algorithm ===\n[[File:Megiddo's minimum enclosing circle algorithm prune stage2.png|thumb|Run of Megiddo's algorithm phase, discarding from point set A,B,...,U needless points E, T.]]\nMegiddo's algorithm<ref>{{citation\n | last = Megiddo | first = Nimrod | authorlink = Nimrod Megiddo\n | doi = 10.1137/0212052\n | issue = 4\n | journal = [[SIAM Journal on Computing]]\n | mr = 721011\n | pages = 759–776\n | title = Linear-time algorithms for linear programming in {{math|'''R'''<sup>3</sup>}} and related problems\n | volume = 12\n | year = 1983}}.</ref> is based on the technique called prune and search reducing size of the problem by removal of n/16 of unnecessary points.\nThat leads to the recurrence t(n)&le; t(15n/16)+cn giving t(n)=16cn.\n\nThe algorithm is rather complicated and it is reflected in big multiplicative constant. \nThe reduction needs to solve twice the similar problem where center of the sought-after enclosing circle is constrained to lie on a given line.\nThe solution of the subproblem is either solution of unconstrained problem or it is used to determine the half-plane where the unconstrained solution center is located.\n\nThe n/16 points to be discarded are found the following way:\nPoints {{var|P<sub>i</sub>}} are arranged to pairs what defines n/2 lines {{var|p<sub>j</sub>}} as their bisectors.\nMedian {{var|p<sub>m</sub>}} of bisectors in order by their directions (oriented to the same half-plane determined by bisector {{var|p<sub>1</sub>}}) is found and pairs from bisectors are made, such that in each pair one bisector has direction at most {{var|p<sub>m</sub>}} and the other at least {{var|p<sub>m</sub>}}\n(direction {{var|p<sub>1</sub>}} could be considered as -<math>\\infty</math> or +<math>\\infty</math> according our needs.) Let {{var|Q<sub>k</sub>}} be intersection of bisectors of {{var|k}}-th pair.\n\nLine {{var|q}} in the {{var|p<sub>1</sub>}} direction is placed to go through an intersection {{var|Q<sub>x</sub>}} such that there is n/8 intersections in each half-plane defined by the line (median position).\nConstrained version of the enclosing problem is run on line {{var|q}} what determines half-plane where the center is located.\nLine {{var|q'}} in the {{var|p<sub>m</sub>}} direction is placed to go through an intersection {{var|Q<sub>x'</sub>}} such that there is n/16 intersections in each half of the half-plane not containing the solution.\nConstrained version of the enclosing problem is run on line {{var|q'}} what together with {{var|q}} determines quadrant, where the center is located. \nWe consider the points {{var|Q<sub>k</sub>}} in the quadrant not contained in a half-plane containing solution. \nOne of the bisectors of pair defining {{var|Q<sub>k</sub>}} has the direction ensuring which of points {{var|P<sub>i</sub>}} defining the bisector is closer to each point in the quadrant containing the center of the enclosing circle. This point could be discarded.\n\nThe constrained version of the algorithm is also solved by the prune and search technique, but reducing problem size by removal of n/4 points \nleading to recurrence t(n)&le; t(3n/4)+cn giving t(n)=4cn.\n\nThe n/4 points to be discarded are found the following way:\nPoints {{var|P<sub>i</sub>}} are arranged to pairs.\nFor each pair intersection {{var|Q<sub>j</sub>}} of its bisector with the constraining line {{var|q}} is found.\n(If intersection does not exist we could remove one point from the pair immediately).\nMedian {{var|M}} of points {{var|Q<sub>j</sub>}} on the line {{var|q}} is found and in O(n) time is determined which halfline of {{var|q}} starting in {{var|M}} \ncontains the solution of the constrained problem.\nWe consider points {{var|Q<sub>j</sub>}} from the other half.\nWe know which of points {{var|P<sub>i</sub>}} defining {{var|Q<sub>j</sub>}} is closer to the each point of the halfline containing center of the enclosing circle\nof the constrained problem solution. This point could be discarded.\n\nThe half-plane where the unconstrained solution lies could be determined by \nthe points {{var|P<sub>i</sub>}} on the boundary of the constrained circle solution. (First and last point on the circle in each half-plane suffice. If the center belongs to their convex hull, it is unconstrained solution, otherwise direction to the nearest edge determines half-plane of the unconstrained solution.)\n\n=== Welzl's algorithm ===\n\nWelzl described the algorithm in a [[recursive algorithm|recursive]] form.\nFor size of input set at most 3, trivial algorithm is created. It outputs empty circle, circle with radius 0, circle on diameter made by arc of input points for input sizes 0, 1 and 2 respectively.\nFor input size 3 if the points are vertices of sharp triangle, the circumscribed circle is returned. Otherwise the circle on diameter on the triangle longest edge is returned.\nFor bigger sizes the algorithm uses solution of 1 smaller size where the ignored point {{var|p}} is chosen [[Randomly selected|randomly]] and [[Uniform distribution (continuous)|uniformly]].\nThe returned circle {{var|D}} is checked and if it encloses {{var|p}}, it is returned as result.\nOtherwise we know point {{var|p}} is on border of the result. We rerun the algorithm again, but we call it with the information {{var|p}} is on the result circle boundary.\nPoints which are known to be on the boundary are accumulated in the set {{var|R}} and they are excluded from the subset {{var|P}} from which random choices are made in the following calls.\n\nThe condition to use the trivial algorithm is |{{var|P}}|=0 or |{{var|R}}|=3, and the trivial algoritm is called for {{var|R}}.\nEquivalent condition would be |{{var|R}}|=3 or |{{var|P}}|+|{{var|R}}|&le;3, and call of the trivial algoritm for {{var|R}} in the case |{{var|R}}|=3\nand for [[Union (set theory)|union]] of {{var|P}} and {{var|R}} otherwise. \nThis would prevent some recursive calls on bottom of the recursion.\n\n '''algorithm''' welzl:<ref>{{citation\n | last = Welzl | first = Emo | authorlink = Emo Welzl\n | editor-last = Maurer | editor-first = H.\n | doi = 10.1007/BFb0038202\n | contribution = Smallest enclosing disks (balls and ellipsoids)\n | volume = 555\n | pages = 359–370\n | publisher = Springer-Verlag\n | series = Lecture Notes in Computer Science\n | title = New Results and New Trends in Computer Science\n | year = 1991| isbn = 978-3-540-54869-0 | citeseerx = 10.1.1.46.1450 }}.</ref>\n     '''input:''' Finite sets {{var|P}} and {{var|R}} of points in the plane |{{var|R}}|&le; 3\n     '''output:''' Minimal disk enclosing {{var|P}} with {{var|R}} on the boundary\n     '''if''' {{var|P}} is empty '''or''' |{{var|R}}| = 3:\n             return trivial({{var|R}})\n     '''choose''' {{var|p}} in {{var|P}} ([[Randomly selected|randomly]] and [[Uniform distribution (continuous)|uniformly]])\n     D := welzl({{var|P}} - { {{var|p}} }, {{var|R}})\n     '''if''' {{var|p}} is in {{var|D}}:\n         return {{var|D}}\n     return welzl({{var|P}} - { {{var|p}} }, {{var|R}} ∪ { {{var|p}} })\n\nWelzl indicated implementation without need of recursion. {{cn|date=March 2019|reason=Not in the paper cited}}\nEmpty {{var|R}} and random permutation of {{var|P}} is used at start.\nSets on bottom of recursion correspond to prefixes in the permutation.\nTrivial algorithm is called for {{var|R}} extended by prefix of {{var|P}} to size of 3, \nand following points are checked they are enclosed by its result {{var|D}}. \nWhenever check fails on point {{var|p}}, {{var|p}} is added to {{var|R}} and removed from {{var|P}},\nand the prefix of the permutation up to {{var|p}} is reshufled.\nTo make this implementation equivalent to the recursive one, we should maintain stack of prefix lengths and if \nlonger prefix than the top would be inserted, last point from {{var|R}} should be removed and inserted to {{var|P}}, the top prefix on the stack should be removed at the same time. Therefore the prefix length on stack would be nondecreasing from its top down.\nWhen |{{var|R}}|=3, the points upto prefix length on top position of the stack are not checked. \n\nTypical implementation uses one list(array) of points of {{var|R}} ∪ {{var|P}} with {{var|R}} in its start, remembering the size of {{var|R}}, therefore {{var|sizeR}} could be used as stack pointer and its decrements while top prefix length was smaller than the current prefix length to be inserted does all required actions.\n\nWelzl also proposes functionally different version of the algorithm using random permutation of points on input, \nwhere the points are not reshufled when the point {{var|p}} is moved to front of the list. \nThe algorithm is again presented in the recursive form.\n\n=== Matoušek, Sharir, Welzl's algorithm ===\n\n[[File:Original Welzl's Algorithm Counter Example2.png|thumb|Example of Welzl's algorithm run which returns wrong result]]\n\nMatoušek, Sharir, Welzl <ref name=msw/> implicitly warned that the assumption point moved to {{var|R}} remains on enclosing circle's boundary for all subsets of \n{{var|R}} ∪ {{var|P}} does not hold, and the required additional condition is the radius of the enclosing circle does not decrease.{{cn|date=March 2019}}\n\nThey proposed tiny change to the algorithm forcing the radius of the enclosing circle does not decrease. {{cn|date=March 2019}}\nIf {{var|p}} is not in enclosing circle {{var|D}} of {{var|R}} ∪ {{var|P}} - { {{var|p}} }, \nthe radius {{var|r}} of enclosing circle of {{var|D}} must be smaller than radius {{var|r'}} of enclosing circle {{var|D'}} of {{var|R}} ∪ {{var|P}}.\nBut radius of an enclosing circle of any subset of {{var|R}} ∪ {{var|P}} - { {{var|p}} } is at most {{var|r}},\nso whenever {{var|r*}}&gt;{{var|r}} is radius of an enclosing circle of a subset of {{var|R}} ∪ {{var|P}}, {{var|p}} must be in its boundary.\n\nMSW algorithm choses 3 random points of {{var|P}} as initial base {{var|R}} and they find initial \n{{var|D}} by running the trivial algorithm on them.\nThe presented recursive version of the algorithm is very simillar to Welzl's.\nPoint {{var|p}} of {{var|P}} is chosen in random order and algorithm for the set {{var|P}} - { {{var|p}} } is called.\nIf {{var|p}} is not enclosed by returned circle {{var|D}}, {{var|p}} replaces one of 3 points in the base \n({{var|R}} ∪ {{var|P}} does not change, and |{{var|R}}| remains 3), \nsuch that the enclosing circle of the new base encloses the point removed from the base.\nThe result recursion is restarted with changed {{var|P}}, {{var|R}}.\n\n '''algorithm''' msw:{{cn|date=March 2019|reason=not in the pdf file linked}}\n     '''input:''' Finite sets {{var|P}} and {{var|R}} of points in the plane |{{var|R}}|=3\n     '''output:''' Minimal disk enclosing {{var|P}} ∪ {{var|R}}\n     '''if''' {{var|P}} is empty\n             return trivial({{var|R}})\n     '''choose''' {{var|p}} in {{var|P}} ([[Randomly selected|randomly]] and [[Uniform distribution (continuous)|uniformly]])\n     D := msw({{var|P}} - { {{var|p}} }, {{var|R}})\n     '''if''' {{var|p}} is in {{var|D}}:\n         return {{var|D}}\n     {{var|q}} = nonbase({{var|R}} ∪ { {{var|p}} }) (Welzl's algorithm for 4 points could be used to find what would not be in {{var|R}})\n     return msw({{var|P}} - { {{var|p}} } ∪ { {{var|q}} }, {{var|R}} ∪ { {{var|p}} } - { {{var|q}} })\n\nUse of base extended by one point is mentioned in the paper.\nIf we put {{var|p}} to the extended base replacing extension, \nwe could use Welzl's recurrence to rearrange the extended base to base and the extra point. \nYou can look at it as allowing temporary decrease of radius {{var|r}} which would be restored when the extended base is finished.\nThe move to front version of Welzl's algorithm works according to this schema except the randomization of choices.\nThis is why it returns correct results, on the contrary to the Welzl's original algorithm.{{Citation needed|date=May 2019}}\n\nMove to front version of welzl without recursion and one list for {{var|R}} ∪ {{var|P}} in view of msw algorithm  \ncould be simplified by not maintaining the size of {{var|R}} at all.\n\nExperiments indicate the move to front version have performance slightly better (say 2% less checks {{var|p}} belongs to {{var|D}})\nthan the rerandomized one (of msw).\n\n==Other algorithms==\nPrior to Megiddo's result showing that  the smallest-circle problem may be solved in linear time,  several algorithms of higher complexity appeared in the literature. A naive algorithm solves the problem in time O(''n''<sup>4</sup>) by testing the circles determined by all pairs and triples of points.\n* An algorithm of Chrystal and Peirce applies a [[Local search (optimization)|local optimization]] strategy that maintains two points on the boundary of an enclosing circle and repeatedly shrinks the circle, replacing the pair of boundary points, until an optimal circle is found. Chakraborty and Chaudhuri<ref>{{citation|first1=R. K.|last1=Chakraborty|first2=P. K.|last2=Chaudhuri|title=Note on geometrical solutions for some minimax location problems|journal=[[Transportation Science]]|volume=15|issue=2|pages=164–166|year=1981|doi=10.1287/trsc.15.2.164}}.</ref> propose a linear-time method for selecting a suitable initial circle and a pair of boundary points on that circle. Each step of the algorithm includes as one of the two boundary points a new vertex of the [[convex hull]], so if the hull has ''h'' vertices this method can be implemented to run in time O(''nh'').\n* Elzinga and Hearn<ref>{{citation|first1=J.|last1=Elzinga|first2=D. W.|last2=Hearn|title=Geometrical solutions for some minimax location problems|journal=[[Transportation Science]]|volume=6|issue=4|pages=379–394|year=1972|doi=10.1287/trsc.6.4.379}}.</ref> described an algorithm which maintains a covering circle for a subset of the points. At each step, a point not covered by the current sphere is used to find a larger sphere that covers a new subset of points, including the point found. Although its worst case running time is O(''h''<sup>3</sup>''n''), the authors report that it ran in linear time in their experiments.  The complexity of the method has been analyzed by Drezner and Shelah.<ref>{{citation|first1=Z.|last1=Drezner|first2=S.|last2=Shelah|title=On the complexity of the Elzinga–Hearn algorithm for the 1-center problem|journal=[[Mathematics of Operations Research]]|volume=12|issue=2|pages=255–261|year=1987|jstor=3689688|doi=10.1287/moor.12.2.255}}.</ref> Both Fortran and C codes are available from {{harvtxt|Hearn|Vijay|Nickel|1995}}.<ref>{{citation|first1= D. W.|last1=Hearn|first2=J.|last2=Vijay|first3=S.|last3=Nickel|title=Codes of geometrical algorithms for the (weighted) minimum circle problem|journal=European Journal of Operational Research|volume=80|pages=236–237|year=1995|doi=10.1016/0377-2217(95)90075-6}}.</ref>\n* The smallest sphere problem can be formulated as a [[quadratic program]]<ref name=\"eh-mcsp-72\"/> defined by a system of linear constraints with a convex quadratic objective function. Therefore, any feasible direction algorithm can give the solution of the problem.<ref>{{citation|first=S. K.|last=Jacobsen|title=An algorithm for the minimax Weber problem|journal=European Journal of Operational Research|volume=6|issue=2|pages=144–148|year=1981|doi=10.1016/0377-2217(81)90200-9}}.</ref> Hearn and Vijay<ref name=\"hv-eawmcp-82\">{{citation|first1=D. W.|last1=Hearn|first2=J.|last2=Vijay|title=Efficient algorithms for the (weighted) minimum circle problem|journal=[[Operations Research (journal)|Operations Research]]|volume=30|issue=4|pages=777–795|year=1982|doi=10.1287/opre.30.4.777}}.</ref> proved that the feasible direction approach chosen by Jacobsen is equivalent to the Chrystal–Peirce algorithm.\n* The dual to this quadratic program may also be formulated explicitly;<ref>{{citation|first1=J.|last1=Elzinga|first2=D. W.|last2=Hearn|first3=W. D.|last3=Randolph|title=Minimax multifacility location with Euclidean distances|journal=[[Transportation Science]]|volume=10|issue=4|pages=321–336|year=1976|doi=10.1287/trsc.10.4.321}}.</ref> an algorithm of Lawson<ref>{{citation|first=C. L.|last=Lawson|title=The smallest covering cone or sphere|journal=[[SIAM Review]]|volume=7|issue=3|doi=10.1137/1007084|pages=415–417|year=1965}}.</ref> can be described in this way as a primal dual algorithm.<ref name=\"hv-eawmcp-82\"/>\n* Shamos and Hoey<ref>{{citation|first1=M. I.|last1=Shamos|author1-link=Michael Ian Shamos|first2=D.|last2=Hoey|contribution=Closest point problems|title=Proceedings of 16th Annual IEEE Symposium on Foundations of Computer Science|pages=151–162|year=1975|doi=10.1109/SFCS.1975.8|title-link=Symposium on Foundations of Computer Science}}.</ref> proposed an O(''n''&nbsp;log&nbsp;''n'') time algorithm for the problem based on the observation that the center of the smallest enclosing circle must be a vertex of the farthest-point [[Voronoi diagram]] of the input point set.\n\n==Weighted variants of the problem==\n\nThe weighted version of the minimum covering circle problem takes as input a set of points in a Euclidean space, each with weights; the goal is to find a single point that minimizes the maximum weighted distance to any point. The original minimum covering circle problem can be recovered by setting all weights to the same number. As with the unweighted problem, the weighted problem may be solved in linear time in any space of bounded dimension, using approaches closely related to bounded dimension linear programming algorithms, although slower algorithms are again frequent in the literature.<ref name=\"hv-eawmcp-82\"/><ref>{{citation|first=N.|last=Megiddo|authorlink=Nimrod Megiddo|title=The weighted Euclidean 1-center problem|journal=[[Mathematics of Operations Research]]|volume=8|issue=4|pages=498–504|year=1983|doi=10.1287/moor.8.4.498}}.</ref><ref>{{citation|first1=N.|last1=Megiddo|author1-link=Nimrod Megiddo|first2=E.|last2=Zemel|title=An ''O''(''n''&nbsp;log&nbsp;''n'') randomizing algorithm for the weighted Euclidean 1-center problem|journal=Journal of Algorithms|volume=7|issue=3|pages=358–368|year=1986|doi=10.1016/0196-6774(86)90027-1}}.</ref>\n\n== See also ==\n* [[Bounding sphere]]\n* [[1-center problem]]\n* [[Circumscribed circle]]\n* [[Closest string]]\n\n==References==\n{{reflist|colwidth=30em}}\n\n==External links==\n* [http://www.inf.ethz.ch/personal/gaertner/miniball.html Bernd Gärtner's smallest enclosing ball code]\n* [http://www.cgal.org/Manual/latest/doc_html/cgal_manual/Bounding_volumes/Chapter_main.html CGAL] the ''Min_sphere_of_spheres'' package of the ''Computational Geometry Algorithms Library'' (CGAL)\n* [https://github.com/hbf/miniball Miniball] an open-source implementation of an algorithm for the smallest enclosing ball problem for low and moderately high dimensions\n\n[[Category:Computational geometry]]\n[[Category:Combinatorial optimization]]\n[[Category:Circles]]"
    },
    {
      "title": "Steiner travelling salesman problem",
      "url": "https://en.wikipedia.org/wiki/Steiner_travelling_salesman_problem",
      "text": "<!-- Don't mess with this line! --><!-- Write your article below this line -->\n\nThe '''Steiner traveling salesman problem (Steiner TSP, or STSP)''' is an [[generalization|extension]] of the [[traveling salesman problem]], one of the fundamental [[combinatorial optimization]] problems. Given a list of cities, some of which are required, and the lengths of the roads between them, the goal is to find the shortest possible walk that visits each required city and then returns to the origin city. As we are looking for a walk, vertices can be visited more than once, and edges may be traversed more than once.\n\n==References==\n{{reflist}}\n<!-- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. -->\n* M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory of NP-Completeness. W. H. Freeman and Company, 1979.\n*Huili Zhang, Weitian Tong, Yinfeng Xu, and Guohui Lin. The steiner traveling salesman problem with online edge blockages. ''European Journal of Operational Research'', 243(1):30–40, 2015.\n*Gerard Cornuejols, Jean Fonlupt, and Denis Naddef. The traveling salesman problem on a graph and some related integer polyhedra. ''Mathematical Programming'', 33(1):1–27, 1985.\n*S. Borne, A.R. Mahjoub, and R. Taktak. A branch-and-cut algorithm for the multiple steiner TSP with order constraints. ''Electronic Notes in Discrete Mathematics'', 41:487–494, 2013.\n*Huili Zhang, Weitian Tong, Yinfeng Xu, and Guohui Lin. The steiner traveling salesman problem with online advanced edge blockages. ''Computers & Operations Research'', 70:26–38, 2016.\n*Adam N. Letchford, Saeideh D. Nasiri, and Dirk Oliver Theis. Compact formulations of the steiner traveling salesman problem and related problems. ''European Journal of Operational Research'', 228(1):83–92, 2013.\n*Adam N. Letchford and Saeideh D. Nasiri. The steiner travelling salesman problem with correlated costs. ''European Journal of Operational Research'', 245(1):62–69, 2015.\n*Juan-Jos´e Salazar-Gonz´alez. The steiner cycle polytope. ''European Journal of Operational Research'', 147(3):671–679, 2003.\n\n[[Category:Combinatorial optimization]]"
    },
    {
      "title": "Subadditive set function",
      "url": "https://en.wikipedia.org/wiki/Subadditive_set_function",
      "text": "In mathematics, a '''subadditive set function''' is a [[set function]] whose value, informally, has the property that the value of function on the union of two sets is at most the sum of values of the function on each of the sets. This is thematically related to the [[subadditivity]] property of real-valued functions.\n\n== Definition ==\nLet <math>\\Omega</math> be a [[set (mathematics)|set]] and <math>f \\colon 2^{\\Omega} \\rightarrow \\mathbb{R}</math> be a [[set function]], where <math>2^\\Omega</math> denotes the [[Power set#Representing subsets as functions|power set]] of <math>\\Omega</math>. The function ''f'' is ''subadditive'' if for each subset <math>S</math> and <math>T</math> of <math>\\Omega</math>, we have <math>f(S) + f(T) \\geq f(S \\cup T)</math>.<ref name=\"UF\" /><ref name=\"DNS\" />\n\n== Examples of subadditive functions ==\n\nEvery non-negative [[submodular set function]] is subadditive (the family of non-negative submodular functions is strictly contained in the family of subadditive functions).\n\nThe function that counts the number of sets required to [[set cover|cover]] a given set is subadditive. Let <math>T_1, \\dotsc, T_m \\subseteq \\Omega</math> such that <math>\\cup_{i=1}^m T_i=\\Omega</math>. Define <math>f</math> as the minimum number of subsets required to cover a given set. Formally, <math>f(S)</math> is the minimum number <math>t</math> such that there are sets <math>T_{i_1}, \\dotsc, T_{i_t}</math> satisfying <math>S\\subseteq \\cup_{j=1}^t T_{i_j}</math>. Then <math>f</math> is subadditive.\n\nThe [[maximum]] of [[additive map|additive set function]]s is subadditive (dually, the [[minimum]] of additive functions is [[superadditive]]). Formally, for each <math>i \\in \\{1, \\dotsc, m\\}</math>, let <math>a_i \\colon \\Omega \\to \\mathbb{R}_+</math> be additive set functions. Then <math>f(S)=\\max_{i}\\left(\\sum_{x\\in S}a_i(x)\\right)</math> is a subadditive set function.\n\nFractionally subadditive set functions are a generalization of submodular functions and a special case of subadditive functions. A subadditive function <math>f</math> is furthermore fractionally subadditive if it satisfies the following definition.<ref name=\"UF\" /> For every <math>S \\subseteq \\Omega</math>, every <math>X_1, \\dotsc, X_n \\subseteq \\Omega</math>, and every <math>\\alpha_1, \\dotsc, \\alpha_n \\in [0, 1]</math>, if <math>1_S \\leq \\sum_{i=1}^n \\alpha_i 1_{X_i}</math>, then <math>f(S) \\leq \\sum_{i=1}^n \\alpha_i f(X_i)</math>. The set of fractionally subadditive functions equals the set of functions that can be expressed as the maximum of additive functions, as in the example in the previous paragraph.<ref name=\"UF\" />\n\n== See also ==\n* [[Submodular set function]]\n* [[Utility functions on indivisible goods]]\n\n== Citations ==\n{{reflist|\nrefs=\n<ref name=\"UF\">{{cite article | first=Uriel | last=Feige | authorlink=Uriel Feige | title=On Maximizing Welfare when Utility Functions are Subadditive | journal=SIAM Journal on Computing | volume=39 | issue=1 | year=2009 | pages=122–142 | doi=10.1137/070680977}}</ref>\n<ref name=\"DNS\">{{cite article | first1=Shahar | last1=Dobzinski | first2=Noam | last2=Nisan | first3=Michael | last3=Schapira | authorlink2=Noam Nisan | title=Approximation Algorithms for Combinatorial Auctions with Complement-Free Bidders | journal=Mathematics of Operations Research | volume=35 | issue=1 | year=2010 | pages=1–13 | doi=10.1145/1060590.1060681}}</ref>\n}}\n\n<!--- Categories --->\n[[:Category:Combinatorial optimization| ]]\n[[:Category:Approximation algorithms| ]]\n\n[[Category:Combinatorial optimization]]"
    },
    {
      "title": "Superadditive set function",
      "url": "https://en.wikipedia.org/wiki/Superadditive_set_function",
      "text": "In mathematics, a '''superadditive set function''' is a [[set function]] whose value, informally, has the property that the value of function on the [[union (set theory)|union]] of two [[disjoint sets]] is at least the sum of values of the function on each of the sets. This is thematically related to the [[superadditivity]] property of real-valued functions. It is contrasted to [[subadditive set function]].\n\n== Definition ==\nLet <math>\\Omega</math> be a [[set (mathematics)|set]] and <math>f \\colon 2^{\\Omega} \\rightarrow \\mathbb{R}</math> be a [[set function]], where <math>2^\\Omega</math> denotes the [[Power set#Representing subsets as functions|power set]] of <math>\\Omega</math>. The function ''f'' is ''superadditive'' if for any pair of disjoint subsets <math>S,T</math> of <math>\\Omega</math>, we have <math>f(S) + f(T) \\leq f(S \\cup T)</math>.<ref name=Megiddo88>{{cite web | url=http://theory.stanford.edu/~megiddo/pdf/Finding_supperadditiveX.pdf | title=ON  FINDING  ADDITIVE,  SUPERADDITIVE  AND  SUBADDITIVE  SET-FUNCTIONS SUBJECT TO  LINEAR  INEQUALITIES | date=1988 | accessdate=21 December 2015 | author=Nimrod  Megiddo}}</ref>\n\n== See also ==\n* [[Utility functions on indivisible goods]]\n\n== Citations ==\n{{reflist}}\n\n<!--- Categories --->\n[[:Category:Combinatorial optimization| ]]\n[[:Category:Approximation algorithms| ]]\n\n[[Category:Combinatorial optimization]]"
    },
    {
      "title": "Utility functions on indivisible goods",
      "url": "https://en.wikipedia.org/wiki/Utility_functions_on_indivisible_goods",
      "text": "Some branches of [[economics]] and [[game theory]] deal with '''indivisible goods''', discrete items that can be traded only as a whole. For example, in combinatorial auctions there is a finite set of items, and every agent can buy a subset of the items, but an item cannot be divided among two or more agents.\n\nIt is usually assumed that every agent assigns subjective [[utility]] to every subset of the items. This can be represented in one of two ways:\n* An [[ordinal utility]] preference relation, usually marked by <math>\\succ</math>. The fact that an agent prefers a set <math>A</math> to a set <math>B</math> is written <math>A \\succ B</math>. If the agent only weakly prefers <math>A</math> (i.e. either prefers <math>A</math> or is indifferent between <math>A</math> and <math>B</math>) then this is written <math>A \\succeq B</math>.\n* A [[cardinal utility]] function, usually denoted by <math>u</math>. The utility an agent gets from a set <math>A</math> is written <math>u(A)</math>. Cardinal utility functions are often normalized such that <math>u(\\emptyset)=0</math>, where <math>\\emptyset</math> is the empty set.\n\nA cardinal utility function implies a preference relation: <math>u(A)>u(B)</math> implies <math>A \\succ B</math> and <math>u(A)\\geq u(B)</math> implies <math>A \\succeq B</math>. Utility functions can have several properties.<ref name=gs99>{{Cite journal | doi = 10.1006/jeth.1999.2531| title = Walrasian Equilibrium with Gross Substitutes| journal = Journal of Economic Theory| volume = 87| pages = 95| year = 1999| last1 = Gul | first1 = F. | last2 = Stacchetti | first2 = E. }}</ref>\n\n== Monotonicity ==\n[[Monotonicity]] means that an agent always (weakly) prefers to have extra items. Formally:\n* For a preference relation: <math>A\\supseteq B</math> implies <math>A \\succeq B</math>.\n* For a utility function: <math>A\\supseteq B</math> implies <math>u(A) \\geq  u(B)</math> (i.e. ''u'' is a [[monotone function]]).\n\nMonotonicity is equivalent to the ''free disposal'' assumption: if an agent may always discard unwanted items, then extra items can never decrease the utility.\n\n== Additivity ==\n{{Main|Additive utility}}\n{| class=\"wikitable\" style=\"float:right\"\n|+Additive utility\n|-\n! <math>A</math> !! <math>u(A)</math>\n|-\n| <math>\\emptyset</math> || 0\n|-\n| apple || 5\n|-\n| hat || 7\n|-\n| apple and hat || 12\n|}\nAdditivity (also called ''linearity'' or ''modularity'') means that \"the whole is equal to the sum of its parts.\" That is, the utility of a set of items is the sum of the utilities of each item separately. This property is relevant only for cardinal utility functions. It says that for every set <math>A</math> of items,\n:<math>u(A)=\\sum_{x\\in A}u({x})</math>\nassuming that <math>u(\\emptyset)=0</math>. In other words, <math>u</math> is an [[additive map|additive function]]. An equivalent definition is: for any sets of items <math>A</math> and <math>B</math>,\n:<math>u(A)+u(B) = u(A\\cup B)+u(A\\cap B).</math>\n\nAn additive utility function is characteristic of [[independent goods]]. For example, an apple and a hat are considered independent: the utility a person receives from having an apple is the same whether or not he has a hat, and vice versa. A typical utility function for this case is given at the right.\n\n== Submodularity and supermodularity  ==\n\n{| class=\"wikitable\" style=\"float:right\"\n|+Submodular utility\n|-\n! <math>A</math> !! <math>u(A)</math>\n|-\n| <math>\\emptyset</math> || 0\n|-\n| apple || 5\n|-\n| bread || 7\n|-\n| apple and bread || 9\n|}\n'''Submodularity''' means that \"the whole is not more than the sum of its parts (and may be less).\" Formally, for all sets <math>A</math> and <math>B</math>,\n:<math>u(A)+u(B)\\ge u(A\\cup B)+u(A\\cap B)</math>\nIn other words, <math>u</math> is a [[submodular set function]].\n\nAn equivalent property is [[Marginal utility#Diminishing marginal utility|diminishing marginal utility]], which means that for any sets <math>A</math> and <math>B</math> with <math>A \\subseteq B</math>, and every <math>x \\notin B</math>:<ref>{{cite book | last = Moulin | first = Hervé | title = Axioms of cooperative decision making | publisher = Cambridge University Press | location = Cambridge England New York | year = 1991 | isbn = 9780521424585 }}</ref>\n:<math>u(A\\cup \\{x\\})-u(A)\\geq u(B\\cup \\{x\\})-u(B)</math>.\n\nA submodular utility function is characteristic of [[substitute goods]]. For example, an apple and a bread loaf can be considered substitutes: the utility a person receives from eating an apple is smaller if he has already ate bread (and vice versa), since he is less hungry in that case. A typical utility function for this case is given at the right.\n\n{| class=\"wikitable\" style=\"float:right\"\n|+Supermodular utility\n|-\n! <math>A</math> !! <math>u(A)</math>\n|-\n| <math>\\emptyset</math> || 0\n|-\n| apple || 5\n|-\n| knife || 7\n|-\n| apple and knife || 15\n|}\n'''Supermodularity''' is the opposite of submodularity: it means that \"the whole is not less than the sum of its parts (and may be more)\". Formally, for all sets <math>A</math> and <math>B</math>,\n:<math>u(A)+u(B) \\leq u(A\\cup B)+u(A\\cap B)</math>\nIn other words, <math>u</math> is a [[Supermodular_function#Supermodular_functions_of_subsets|supermodular set function]].\n\nAn equivalent property is ''increasing marginal utility'', which means that for all sets <math>A</math> and <math>B</math> with <math>A \\subseteq B</math>, and every <math>x \\notin B</math>: \n:<math>u(B\\cup \\{x\\})-u(B)\\geq u(A\\cup \\{x\\})-u(A)</math>.\n\nA supermoduler utility function is characteristic of [[complementary goods]]. For example, an apple and a knife can be considered complementary: the utility a person receives from an apple is larger if he already has a knife (and vice versa), since it is easier to eat an apple after cutting it with a knife. A possible utility function for this case is given at the right.\n\nA utility function is [[#Additivity|additive]] if and only if it is both submodular and supermodular.\n\n== Subadditivity and superadditivity ==\n{| class=\"wikitable\" style=\"float:right; width:300px\"\n|+Subadditive but not submodular\n|-\n! <math>A</math> !! <math>u(A)</math>\n|-\n| <math>\\emptyset</math> || 0\n|-\n| X or Y or Z || 2\n|-\n| X,Y or Y,Z or Z,X || 3\n|-\n| X,Y,Z || 5\n|}\n'''Subadditivity''' means that for every pair of disjoint sets <math>A,B</math>\n::<math>u(A\\cup B)\\leq u(A)+u(B)</math>\nIn other words, <math>u</math> is a [[subadditive set function]].\n\nAssuming <math>u(\\emptyset)</math> is non-negative, every submodular function is subadditive. \nHowever, there are non-negative subadditive functions that are not submodular. \nFor example, assume that there are 3 identical items, <math>X, Y</math>, and Z, and the utility depends only on their quantity. The table on the right describes a utility function that is subadditive but not submodular, since\n::<math> u(\\{X,Y\\})+u(\\{Y,Z\\}) < u(\\{X,Y\\}\\cup\\{Y,Z\\})+u(\\{X,Y\\}\\cap\\{Y,Z\\}). </math>\n\n\n\n{| class=\"wikitable\" style=\"float:right; width:300px\"\n|+Superadditive but not supermodular\n|-\n! <math>A</math> !! <math>u(A)</math>\n|-\n| <math>\\emptyset</math> || 0\n|-\n| X or Y or Z || 1\n|-\n| X,Y or Y,Z or Z,X || 3\n|-\n| X,Y,Z || 4\n|}\n'''Superadditivity''' means that for every pair of disjoint sets <math>A,B</math>\n::<math>u(A\\cup B)\\geq u(A)+u(B)</math>\nIn other words, <math>u</math> is a [[superadditive set function]].\n\nAssuming <math>u(\\emptyset)</math> is non-positive, every supermodular function is superadditive.\nHowever, there are non-negative superadditive functions that are not supermodular. \nFor example, assume that there are 3 identical items, <math>X, Y</math>, and Z, and the utility depends only on their quantity. The table on the right describes a utility function that is non-negative and superadditive but not supermodular, since\n::<math> u(\\{X,Y\\})+u(\\{Y,Z\\}) < u(\\{X,Y\\}\\cup\\{Y,Z\\})+u(\\{X,Y\\}\\cap\\{Y,Z\\}). </math>\n\nA utility function with <math>u(\\emptyset) = 0</math> is said to be [[#Additivity|additive]] if and only if it is both superadditive and subadditive.\n\nWith the typical assumption that <math>u(\\emptyset) = 0</math>, every submodular function is subadditive and every supermodular function is superadditive.\nWithout any assumption on the utility from the empty set, these relations do not hold.\n\nIn particular, if a submodular function is not subadditive, then <math>u(\\emptyset)</math> must be negative.\nFor example, suppose there are two items, <math>X, Y</math>, with <math>u(\\emptyset) = -1</math>, <math>u(\\{X\\}) = u(\\{Y\\}) = 1</math> and <math>u(\\{X, Y\\}) = 3</math>.\nThis utility function is submodular and supermodular and non-negative except on the empty set, but is not subadditive, since\n::<math>u(\\{X,Y\\}) > u(\\{X\\}) + u(\\{Y\\}).</math>\nAlso, if a supermodular function is not superadditive, then <math>u(\\emptyset)</math> must be positive.\nSuppose instead that <math>u(\\emptyset) = u(\\{X\\}) = u(\\{Y\\}) = u(\\{X, Y\\}) = 1</math>.\nThis utility function is non-negative, supermodular, and submodular, but is not superadditive, since\n::<math>u(\\{X,Y\\}) < u(\\{X\\}) + u(\\{Y\\}).</math>\n\n== Unit demand ==\n{{Main|Unit demand}}\n{| class=\"wikitable\" style=\"float:right\"\n|+Unit demand utility\n|-\n! <math>A</math> !! <math>u(A)</math>\n|-\n| <math>\\emptyset</math> || 0\n|-\n| apple || 5\n|-\n| pear || 7\n|-\n| apple and pear || 7\n|}\nUnit demand (UD) means that the agent only wants a single good. If the agent gets two or more goods, he uses the one of them that gives him the highest utility, and discards the rest. Formally:\n* For a preference relation: for every set <math>B</math> there is a subset <math>A\\subseteq B</math> with cardinality <math>|A|=1</math>, such that <math>A \\succeq B</math>.\n* For a utility function:  For every set <math>A</math>:<ref name=kb57>{{Cite journal | doi = 10.2307/1907742| jstor = 1907742| title = Assignment Problems and the Location of Economic Activities| journal = Econometrica| volume = 25| pages = 53| year = 1957| last1 = Koopmans | first1 = T. C. | last2 = Beckmann | first2 = M. }}</ref>\n:<math>u(A)=\\max_{x\\in A}u({x})</math>\n\nA unit-demand function is an extreme case of a submodular function. It is characteristic of goods that are pure substitutes. For example, if there are an apple and a pear, and an agent wants to eat a single fruit, then his utility function is unit-demand, as exemplified in the table at the right.\n\n== Gross substitutes ==\n[[File:Utilities.png|thumb|An illustration of the containment relations between common classes of utility functions.]]\n{{Main|Gross substitutes (indivisible items)}}\nGross substitutes (GS) means that the agents regards the items as [[substitute goods]] or [[independent goods]] but not [[complementary goods]]. There are many formal definitions to this property, all of which are equivalent. \n* Every UD valuation is GS, but the opposite is not true. \n* Every GS valuation is submodular, but the opposite is not true. \nSee [[Gross substitutes (indivisible items)]] for more details.\n\nHence the following relations hold between the classes:\n:<math>UD \\subsetneq GS \\subsetneq Submodular \\subsetneq Subadditive</math>\nSee diagram on the right.\n\n== Aggregates of utility functions ==\nA utility function describes the happiness of an individual. Often, we need a function that describes the happiness of an entire society. Such a function is called a [[social welfare function]], and it is usually an [[aggregate function]] of two or more utility functions. If the individual utility functions are [[#Additivity|additive]], then the following is true for the aggregate functions:\n\n{| class=\"wikitable\"\n|-\n! rowspan=2 | Aggregate<br/>function !!  rowspan=2 | Property !! colspan=4| Example<br/>{{small|values of functions<br/>on {a}, {b} and {a,b}}}\n|-\n! f        !! g        !! h        !! aggregate(f,g,h)\n|-\n| [[Summation|Sum]] || Additive  || 1,3; 4  || 3,1; 4    || || 4,4; 8\n|-\n| [[Average]] || Additive  || 1,3; 4  || 3,1; 4  || || 2,2; 4\n|-\n| [[Minimum]] || Super-additive  || 1,3; 4  || 3,1; 4  || || 1,1; 4\n|-\n| [[Maximum]] || Sub-additive  || 1,3; 4  || 3,1; 4  || ||  3,3; 4\n|-\n|rowspan=\"2\"| [[Median]]  ||rowspan=\"2\"| neither       || 1,3; 4  || 3,1; 4  || 1,1; 2 || 1,1; 4\n|-\n|                                                        1,3; 4  || 3,1; 4  || 3,3; 6 || 3,3; 4\n|}\n\n==See also==\n\n*[[Utility functions on divisible goods]]\n\n== References ==\n{{reflist}}\n\n[[Category:Utility function types]]\n[[Category:Combinatorial optimization]]"
    },
    {
      "title": "Vehicle routing problem",
      "url": "https://en.wikipedia.org/wiki/Vehicle_routing_problem",
      "text": "{{pp-protected|reason=Excessive spam|expiry=14:39, 8 July 2019|small=yes}}\n[[File:Figure illustrating the vehicle routing problem.png|thumb|A figure illustrating the vehicle routing problem]]\n\nThe '''vehicle routing problem''' ('''VRP''') is a [[combinatorial optimization]] and [[integer programming]] problem which asks \"What is the optimal set of routes for a fleet of vehicles to traverse in order to deliver to a given set of customers?\". It generalises the well-known [[travelling salesman problem]] (TSP). It first appeared in a paper by [[George Dantzig]] and John Ramser in 1959,<ref name=DantzigRamser1959>{{cite journal|last=Dantzig|first=George Bernard|author2=Ramser, John Hubert |title=The Truck Dispatching Problem|journal=Management Science|date=October 1959|volume=6|issue=1|pages=80–91|url=http://andresjaquep.files.wordpress.com/2008/10/2627477-clasico-dantzig.pdf|doi=10.1287/mnsc.6.1.80}}</ref> in which first algorithmic approach was written and was applied to petrol deliveries. Often, the context is that of delivering goods located at a central depot to customers who have placed orders for such goods. The objective of the VRP is to minimize the total route cost. In 1964, Clarke and Wright improved on Dantzig and Ramser's approach using an effective greedy approach called the savings algorithm.\n\nDetermining the optimal solution to VRP is [[NP-hard]],<ref name=toth>{{cite book |editor=Toth, P. |editor2=Vigo, D. |title=The Vehicle Routing Problem |volume=9 |series=Monographs on Discrete Mathematics and Applications |year=2002 |publisher=Society for Industrial and Applied Mathematics |location=Philadelphia |isbn=0-89871-579-2}}</ref> so the size of problems that can be solved, optimally, using [[mathematical programming]] or [[combinatorial optimization]] may be limited. Therefore, commercial solvers tend to use heuristics due to the size and frequency of real world VRPs they need to solve. (For a non-technical explanation of why the VRP is so challenging please see the External Links below.)\n\nThe VRP has many obvious applications in industry. In fact, the use of computer optimization programs can give savings of 5% to a company<ref name=\"Springer Verlag\">{{cite book|editor=Geir Hasle |editor2=Knut-Andreas Lie |editor3=Ewald Quak |title=Geometric Modelling, Numerical Simulation, and Optimization:: Applied Mathematics at SINTEF |date=2007 |publisher=Springer Verlag |location=Berlin |isbn=978-3-540-68783-2}}</ref> as transportation is usually a significant component of the cost of a product (10%)<ref>{{cite book|last1=Comtois|first1=Claude|last2=Slack|first2=Brian|last3=Rodrigue|first3=Jean-Paul|title=The geography of transport systems |date=2013 |publisher=Routledge, Taylor & Francis Group|location=London|isbn=978-0-415-82254-1 |edition=3rd}}</ref> - indeed, the transportation sector makes up 10% of the [[European Union|EU's]] [[Gross domestic product|GDP]]. Consequently, any savings created by the VRP, even less than 5%, are significant.<ref name=\"Springer Verlag\"/>\n\n== Setting up the problem ==\nThe VRP concerns the service of a delivery company. How things are delivered from one or more ''depots'' which has a given set of home ''vehicles'' and operated by a set of ''drivers'' who can move on a given ''road network'' to a set of ''customers''. It asks for a determination of a set of ''routes'', ''S'', (one route for each vehicle that must start and finish at its own depot) such that all customers' requirements and operational constraints are satisfied and the ''global transportation cost'' is minimized. This cost may be monetary, distance or otherwise.<ref name=toth />\n\nThe road network can be described using a [[Graph (discrete mathematics)|graph]] where the [[directed edge|arcs]] are roads and vertices are junctions between them. The arcs may be directed or undirected due to the possible presence of one way streets or different costs in each direction. Each arc has an associated cost which is generally its length or travel time which may be dependent on vehicle type.<ref name=toth />\n\nTo know the global cost of each route, the travel cost and the travel time between each customer and the depot must be known. To do this our original graph is transformed into one where the vertices are the customers and depot, and the arcs are the roads between them. The cost on each arc is the lowest cost between the two points on the original road network. This is easy to do as [[shortest path problems]] are relatively easy to solve. This transforms the sparse original graph into a [[complete graph]]. For each pair of vertices ''i'' and ''j'', there exists an arc ''(i,j)'' of the complete graph whose cost is written as <math>C_{ij}</math> and is defined to be the cost of shortest path from ''i'' to ''j''. The travel time <math>t_{ij}</math> is the sum of the travel times of the arcs on the shortest path from ''i'' to ''j'' on the original road graph.\n\nSometimes it is impossible to satisfy all of a customer's demands and in such cases solvers may reduce some customers' demands or leave some customers unserved. To deal with these situations a priority variable for each customer can be introduced or associated penalties for the partial or lack of service for each customer given <ref name=toth/>\n\nThe objective function of a VRP can be very different depending on the particular application of the result but a few of the more common objectives are:<ref name=toth/>\n*Minimize the global transportation cost based on the global distance travelled as well as the fixed costs associated with the used vehicles and drivers\n*Minimize the number of vehicles needed to serve all customers\n*Least variation in travel time and vehicle load\n*Minimize penalties for low quality service\n\n==VRP variants==\n[[File:Map of vrp subproblems.jpg|thumb|right|upright=2|A map showing the relationship between common VRP subproblems.]]\n\nSeveral variations and specializations of the vehicle routing problem exist:\n*Vehicle Routing Problem with Pickup and Delivery (VRPPD): A number of goods need to be moved from certain pickup locations to other delivery locations.  The goal is to find optimal routes for a fleet of vehicles to visit the pickup and drop-off locations.\n*Vehicle Routing Problem with [[LIFO (computing)|LIFO]]: Similar to the VRPPD, except an additional restriction is placed on the loading of the vehicles: at any delivery location, the item being delivered must be the item most recently picked up. This scheme reduces the loading and unloading times at delivery locations because there is no need to temporarily unload items other than the ones that should be dropped off.\n*Vehicle Routing Problem with Time Windows (VRPTW): The delivery locations have time windows within which the deliveries (or visits) must be made.\n* Capacitated Vehicle Routing Problem: CVRP or CVRPTW. The vehicles have limited carrying capacity of the goods that must be delivered.\n* Vehicle Routing Problem with Multiple Trips (VRPMT): The vehicles can do more than one route.\n* Open Vehicle Routing Problem (OVRP): Vehicles are not required to return to the depot.\n\nSeveral software vendors have built software products to solve the various VRP problems. Numerous articles are available for more detail on their research and results.\n\nAlthough VRP is related to the [[Job Shop Scheduling]] Problem, the two problems are typically solved using different techniques.<ref name=Beck2003>{{cite conference|author=Beck, J.C. |author2=Prosser, P. |author3=Selensky, E. |year=2003 |title=Vehicle routing and job shop scheduling: What’s the difference? |conference=|booktitle=Proceedings of the 13th International Conference on Artificial Intelligence Planning and Scheduling |publisher= |url=http://www.dcs.gla.ac.uk/pras/pubs/Icaps03.pdf |conferenceurl=}}</ref>\n\n==Exact solution methods==\nThere are three main different approaches to modelling the VRP\n#'''Vehicle flow formulations'''—this uses integer variables associated with each arc that count the number of times that the edge is traversed by a vehicle. It is generally used for basic VRPs. This is good for cases where the solution cost can be expressed as the sum of any costs associated with the arcs. However it can't be used to handle many practical applications.<ref name=toth />\n#'''Commodity flow formulations'''—additional integer variables are associated with the arcs or edges which represent the flow of commodities along the paths travelled by the vehicles. This has only recently been used to find an exact solution.<ref name=toth />\n#'''Set partitioning problem'''—These have an exponential number of binary variables which are each associated with a different feasible circuit. The VRP is then instead formulated as a set partitioning problem which asks what is the collection of circuits with minimum cost that satisfy the VRP constraints. This allows for very general route costs.<ref name=toth />\n\n===Vehicle flow formulations===\nThe formulation of the TSP by Dantzig, Fulkerson and Johnson  was extended to create the two index vehicle flow formulations for the VRP\n\n:<math>\\text{min} \\sum_{i\\in V}\\sum_{j \\in V}c_{ij}x_{ij}</math>\n\nsubject to\n\n{{NumBlk|:|<math> \\sum_{i \\in V}x_{ij}=1 \\quad \\forall j \\in V\\backslash \\left \\{ 0 \\right \\}</math>|{{EquationRef|1}}}}\n{{NumBlk|:|<math> \\sum_{j \\in V}x_{ij}=1 \\quad \\forall i \\in V\\backslash \\left \\{ 0 \\right \\}</math>|{{EquationRef|2}}}}\n{{NumBlk|:|<math> \\sum_{i \\in V}x_{i0}=K</math>|{{EquationRef|3}}}}\n{{NumBlk|:|<math> \\sum_{j \\in V}x_{0j}=K</math>|{{EquationRef|4}}}}\n{{NumBlk|:|<math> \\sum_{i\\notin S}\\sum_{j\\in S} x_{ij}\\ge r(S), ~~\\forall S \\subseteq V\\setminus \\{0\\}, S\\neq \\emptyset</math>|{{EquationRef|5}}}}\n{{NumBlk|:|<math> x_{ij}\\in \\{0,1\\} \\quad \\forall i,j \\in V</math>|{{EquationRef|6}}}}\n\nConstraints {{EquationNote|1}} and {{EquationNote|2}} state that exactly one arc enters and exactly one leaves each vertex associated with a customer, respectively. Constraints {{EquationNote|3}} and {{EquationNote|4}}  say that the number of vehicles leaving the depot is the same as the number entering. Constraints {{EquationNote|5}}  are the capacity cut constraints, which impose that the routes must be connected and that the demand on each route must not exceed the vehicle capacity. Finally, constraints {{EquationNote|6}} are the integrality constraints.<ref name=toth />\n\nOne arbitrary constraint among the <math>2|V|</math> constraints is actually implied by the remaining <math>2|V|-1</math> ones so it can be removed. Each cut defined by a customer set ''S'' is crossed by a number of arcs not smaller than {{tmath|r(s)}}(minimum number of vehicles needed to serve set ''S'').<ref name=toth />\n\nAn alternative formulation may be obtained by transforming the capacity cut constraints into generalised subtour elimination constraints (GSECs).\n:<math>\n\\sum_{i\\in S}\\sum_{j\\in S}x_{ij} \\leq |S|-r(s)\n</math>\nwhich imposes that at least {{tmath|r(s)}}arcs leave each customer set ''S''.<ref name=toth />\n\nGCECs and CCCs have an exponential number of constraints so it is practically impossible to solve the linear relaxation. A possible way to solve this is to consider a limited subset of these constraints and add the rest if needed.\n\nA different method again is to use a family of constraints which have a polynomial cardinality which are known as the MTZ constraints, they were first proposed for the TSP <ref name=mtz>{{cite journal|last1=Miller|first1=C. E.|last2=Tucker|first2=E. W.|last3=Zemlin|first3=R. A.|title=Integer Programming Formulations and Travelling Salesman Problems|journal=J. ACM|date=1960|volume=7|pages=326–329|doi=10.1145/321043.321046}}</ref> and subsequently extended by Christofides, Mingozzi and Toth.<ref name=christof>{{cite book|last1=Christofides|first1=N.|last2=Mingozzi|first2=A.|last3=Toth|first3=P.|title=The Vehicle Routing Problem|date=1979|publisher=Wiley|location=Chichester, UK|pages=315–338}}</ref>\n:<math>\nu_i-u_j\\geq d_j-C(1-x_{ij}) ~~~~~~\\forall i,j \\in V\\backslash\\{0\\}, i\\neq j~~~~\\text{s.t. } d_i +d_j \\leq C\n</math>\n\n:<math>\n0 \\leq u_i \\leq C-d_i ~~~~~~\\forall i \\in V\\backslash \\{0\\}\n</math>\n\nwhere <math> u_i,~i \\in V \\backslash \\{0\\}</math> is an additional continuous variable which represents the load left in the vehicle '''after''' visiting customer ''i'' and ''d_i'' is the demand of customer ''i''. These impose both the connectivity and the capacity requirements. When <math>x_{ij}=0</math> constraint then ''i'' 'is not binding' since <math>u_i\\leq C</math> and <math>u_j\\geq d_j</math> whereas <math>x_{ij} = 1</math> they impose that <math>u_j \\geq u_i +d_j</math>.\n\nThese have been used extensively to model the basic VRP (CVRP) and the VRPB. However, their power is limited to these simple problems. They can only be used when the cost of the solution can be expressed as the sum of the costs of the arc costs. We cannot also know which vehicle traverses each arc. Hence we cannot use this for more complex models where the cost and or feasibility is dependent on the order of the customers or the vehicles used.<ref name=toth />\n\n===Manual versus automatic optimum routing===\nThere are many methods how to solve vehicle routing problem manually. For example, optimum routing is a big efficiency issue for forklifts in large warehouses. Some of the manual methods to decide upon the most efficient route are: Largest gap, S-shape, Aisle-by-aisle, Combined and Combined +. While Combined + method is the most complex, thus the hardest to be used by lift truck operators, it is the most efficient routing method. Still the percentage difference between the manual optimum routing method and the real optimum route was on average 13%.<ref>{{cite web|url=http://locatible.com/blog/logistics/why-is-manual-warehouse-optimum-routing-so-unefficient/ |title=Why Is Manual Warehouse Optimum Routing So Inefficient? |website=Locatible.com |date=2016-09-26 |accessdate=2016-09-26}}</ref><ref>{{cite web|last=Roodbergen |first=Kees Jan |url=http://roodbergen.com/publications/IJPR2001.pdf |title=Routing methods for warehouses with multiple cross aisles |website=roodbergen.com |date=2001 |accessdate=2016-09-26}}</ref>\n\n==Metaheuristics==\n\nDue to the difficulty of solving to optimality large-scale instances of vehicle routing problems, a significant research effort has been dedicated to [[Metaheuristic|metaheuristics]] such as [[Genetic algorithms]], [[Tabu search]], and [[Simulated annealing]]. Some of the most recent and efficient metaheuristics for vehicle routing problems reach solutions within 0.5% or 1% of the optimum for problem instances counting hundreds or thousands of delivery points\n<ref>{{cite journal|vauthors=Vidal T, Crainic TG, Gendreau M, Prins C||year=2014|title=A unified solution framework for multi-attribute vehicle routing problems\n|journal=European Journal of Operational Research|volume=234|issue=3|pages=658-673|doi=10.1016/j.ejor.2013.09.045}}</ref>.\nThese methods are also more robust in the sense that they can be more easily adapted to deal with a variety of side constraints. As such, the application of metaheuristic techniques is often privileged for large-scale applications with complicating constraints and decision sets.\n\n==See also==\n* [[Chinese postman problem]]\n* [[Travelling salesman problem]]\n* [[Vehicle rescheduling problem]]\n* [[Arc routing]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n*{{cite journal|author=Oliveira, H.C.B.de|author2=Vasconcelos, G.C. |year=2008|title=A hybrid search method for the vehicle routing problem with time windows|journal=Annals of Operations Research |url=http://www.springerlink.com/content/f863257kn036x1rp/ |accessdate=2009-01-29|doi=10.1007/s10479-008-0487-y|volume=180|pages=125–144}}\n*{{cite conference |author=Frazzoli, E. |author2=Bullo, F. |year=2004 |title=Decentralized algorithms for vehicle routing in a stochastic time-varying environment |conference=43rd IEEE Conference on Decision and Control, 14-17 Dec. 2004, Nassau, Bahamas |booktitle=2004 43rd IEEE Conference on Decision and Control (CDC) |volume=4 |publisher=IEEE |url=http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1429220 |doi=10.1109/CDC.2004.1429220 |issn=0191-2216 |isbn=0-7803-8682-5}}\n*{{cite journal|author=Psaraftis, H.N.|year=1988|title=Dynamic vehicle routing problems|journal=Vehicle Routing: Methods and Studies|volume=16|pages=223–248}}\n*{{cite journal|author=Bertsimas, D.J.|author2=Van Ryzin, G.|year=1991|title=A Stochastic and Dynamic Vehicle Routing Problem in the Euclidean Plane|journal=Operations Research|volume=39|issue=4|pages=601–615 |doi=10.1287/opre.39.4.601 |jstor=171167}}\n*{{cite journal|vauthors=Vidal T, Crainic TG, Gendreau M, Prins C||year=2013|title=Heuristics for multi-attribute vehicle routing problems: A survey and synthesis |journal=European Journal of Operational Research |volume=231 |issue=1 |pages=1–21 |doi=10.1016/j.ejor.2013.02.053}}\n[[Category:NP-complete problems]]\n[[Category:Combinatorial optimization]]"
    },
    {
      "title": "Weapon target assignment problem",
      "url": "https://en.wikipedia.org/wiki/Weapon_target_assignment_problem",
      "text": "The '''weapon target assignment problem''' ('''WTA''') is a class of [[combinatorial optimization]] problems present in the fields of [[Optimization (mathematics)|optimization]] and [[operations research]].  It consists of finding an optimal assignment of a set of [[weapon]]s of various types to a set of targets in order to maximize the total expected damage done to the opponent.\n\nThe basic problem is as follows:\n\n:There are a number of weapons and a number of targets. The weapons are of type <math> i = 1, \\ldots, m </math>. There are <math> W_{i} </math> available weapons of type <math>i</math>. Similarly, there are <math> j = 1, \\ldots, n </math> targets, each with a value of <math> V_{j} </math>. Any of the weapons can be assigned to any target. Each weapon type has a certain probability of destroying each target, given by <math> p_{ij} </math>.\n\nNotice that as opposed to the classic [[assignment problem]] or the [[generalized assignment problem]], more than one agent (i.e., weapon) can be assigned to each ''task'' (i.e., target) and not all targets are required to have weapons assigned. Thus, we see that the WTA allows one to formulate optimal assignment problems wherein tasks require cooperation among agents.  Additionally, it provides the ability to model probabilistic completion of tasks in addition to costs.\n\nBoth static and dynamic versions of WTA can be considered.  In the static case, the weapons are assigned to targets once. The dynamic case involves many rounds of assignment where the state of the system after each exchange of fire (round) is considered in the next round. While the majority of work has been done on the static WTA problem, recently the dynamic WTA problem has received more attention.\n\nIn spite of the name, there are nonmilitary applications of the WTA. The main one is to search for a lost object or person by heterogeneous assets such as dogs, aircraft, walkers, etc. The problem is to assign the assets to a partition of the space in which the object is located to minimize the probability of not finding the object. The \"value\" of each element of the partition is the probability that the object is located there.\n\n==Formal mathematical definition==\n\nThe '''weapon target assignment problem''' is often formulated as the following nonlinear [[integer programming]] problem:\n\n:<math>\\min \\sum_{j = 1}^n \\left ( V_{j}\\prod_{i = 1}^m q_{ij}^{x_{ij}} \\right )</math>\n\nsubject to the constraints\n\n:<math>\\sum_{j = 1}^n x_{ij}\\leq W_i \\text{ for }i = 1, \\ldots, m, \\, </math>\n:<math>x_{ij}\\ge 0\\text{ and integer for }i = 1, \\ldots, m \\text{ and }j = 1, \\ldots, n.</math>\n\nWhere the variable <math>x_{ij}</math> represents the assignment of as many weapons of type <math>i</math> to target <math>j</math> and <math>q_{ij}</math> is the probability of survival (<math> 1 - p_{ij} </math>). The first constraint requires that the number of weapons of each type assigned does not exceed the number available.  The second constraint is the integral constraint.\n\nNotice that minimizing the expected survival value is the same as maximizing the expected damage.\n\n== Algorithms and generalizations ==\n\nAn exact solution can be found using [[branch and bound]] techniques which utilize [[relaxation (approximation)]]. Many [[heuristic algorithm]]s have been proposed which provide near-optimal solutions in [[polynomial time]].<ref>Ahuja, R. et al. Exact and Heuristic Algorithms for the Weapon-Target Assignment Problem. Operations Research 55(6), pp. 1136–1146, 2007</ref>\n\n==Example==\nA commander has 5 tanks, 2 aircraft, and 1 sea vessel and is told to engage 3 targets with values 5, 10, and 20.  Each weapon type has the following success probabilities against each target:\n::{| class=\"wikitable\"\n|-\n! Weapon Type !! <math> V_{1} = 5 </math> !! <math> V_{2} = 10 </math> !! <math> V_{3} = 20 </math>\n|-\n| Tank || 0.3 || 0.2 || 0.5\n|-\n| Aircraft || 0.1 || 0.6 || 0.5\n|-\n| Sea Vessel || 0.4 || 0.5 || 0.4\n|}\nOne feasible solution is to assign the sea vessel and one aircraft to the highest valued target (3). This results in an expected survival value of <math> 20(0.6)(0.5)= 6 </math>.  One could then assign the remaining aircraft and 2 tanks to target #2, resulting in expected survival value of <math> 10 (0.4)(0.8)^2 = 2.56 </math>.  Finally, the remaining 3 tanks are assigned to target #1 which has an expected survival value of <math> 5 (0.7)^3 = 1.715 </math>.  Thus, we have a total expected survival value of <math> 6 + 2.56 + 1.715 = 10.275 </math>. Note that a better solution can be achieved by assigning 3 tanks to target #1, 2 tanks and sea vessel to target #2 and 2 aircraft to target #3, giving an expected survival value of <math> 5(0.7)^3 +10(0.5)(0.8)^2 + 20(0.5)^2 = 9.915 </math>.\n\n==See also==\n*[[Auction algorithm]]\n*[[Closure problem]]\n*[[Generalized assignment problem]]\n*[[Linear bottleneck assignment problem]]\n*[[Quadratic assignment problem]]\n*[[Stable marriage problem]]\n\n== References ==\n{{Reflist}}\n\n== Further reading ==\n* {{cite book\n | authorlink = Ravindra K. Ahuja\n | first = Ravindra | last = Ahuja\n |author2=T. L. Magnanti |author3=J. B. Orlin\n  | year = 1993\n | title = Network Flows\n | publisher = Prentice Hall\n | isbn = 0-13-617549-X\n }}\n\n[[Category:Combinatorial optimization]]\n[[Category:Matching]]"
    },
    {
      "title": "Weight function",
      "url": "https://en.wikipedia.org/wiki/Weight_function",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Construct related to weighted sums and averages}}\nA '''weight function''' is a mathematical device used when performing a sum, integral, or average to give some elements more \"weight\" or influence on the result than other elements in the same set. The result of this application of a weight function is a '''weighted sum''' or [[weighted average]]. Weight functions occur frequently in [[statistics]] and [[mathematical analysis|analysis]], and are closely related to the concept of a [[measure (mathematics)|measure]].  Weight functions can be employed in both discrete and continuous settings. They can be used to construct systems of calculus called \"weighted calculus\"<ref>Jane Grossman, Michael Grossman, Robert Katz. [https://books.google.com/books?as_brr=0&q=%22The+First+Systems+of+Weighted+Differential+and+Integral+Calculus%E2%80%8E%22&btnG=Search+Books, ''The First Systems of Weighted Differential and Integral Calculus''], {{isbn|0-9771170-1-4}}, 1980.</ref> and \"meta-calculus\".<ref>Jane Grossman.[https://books.google.com/books?q=%22Non-Newtonian+Calculus%22&btnG=Search+Books&as_brr=0, ''Meta-Calculus: Differential and Integral''], {{isbn|0-9771170-2-2}}, 1981.</ref>\n\n== Discrete weights ==\n=== General definition ===\nIn the discrete setting, a weight function <math>\\scriptstyle w\\colon A \\to {\\mathbb R}^+</math> is a positive function defined on a [[discrete mathematics|discrete]] [[Set (mathematics)|set]] '''<math>A</math>''', which is typically [[finite set|finite]] or [[countable]].  The weight function <math>w(a) := 1</math> corresponds to the ''unweighted'' situation in which all elements have equal weight.  One can then apply this weight to various concepts.\n\nIf the function <math>\\scriptstyle f\\colon A \\to {\\mathbb R}</math> is a [[real number|real]]-valued [[mathematical function|function]], then the ''unweighted [[summation|sum]] of <math>f</math> on '''<math>A</math>''''' is defined as\n\n:<math>\\sum_{a \\in A} f(a);</math>\n\nbut given a ''weight function'' <math>\\scriptstyle w\\colon A \\to {\\mathbb R}^+</math>, the '''weighted sum''' or [[conical combination]] is defined as\n\n:<math>\\sum_{a \\in A} f(a) w(a).</math>\n\nOne common application of weighted sums arises in [[numerical integration]].\n\nIf ''B'' is a [[finite set|finite]] subset of ''A'', one can replace the unweighted [[cardinality]] ''|B|'' of ''B'' by the ''weighted cardinality'' \n\n:<math>\\sum_{a \\in B} w(a).</math>\n\nIf ''A'' is a [[finite set|finite]] non-empty set, one can replace the unweighted [[mean]] or [[average]] \n\n:<math>\\frac{1}{|A|} \\sum_{a \\in A} f(a)</math>\n\nby the [[weighted mean]] or [[weighted average]] \n\n:<math> \\frac{\\sum_{a \\in A} f(a) w(a)}{\\sum_{a \\in A} w(a)}.</math>\n\nIn this case only the ''relative'' weights are relevant.\n\n=== Statistics ===\nWeighted means are commonly used in [[statistics]] to compensate for the presence of [[Bias_(statistics)|bias]].  For a quantity  <math>f</math> measured multiple independent times <math>f_i</math> with [[variance]] <math>\\scriptstyle\\sigma^2_i</math>, the best estimate of the signal is obtained  by averaging all the measurements with weight <math>\\scriptstyle w_i=\\frac 1 {\\sigma_i^2}</math>, and\nthe resulting variance is smaller than each of the independent measurements <math>\\scriptstyle\\sigma^2=1/\\sum w_i</math>. The [[maximum likelihood]] method weights the difference between fit and data using the same weights <math>w_i</math>.\n\nThe [[expected value]] of a random variable is the weighted average of the possible values it might take on, with the weights being the respective [[probability|probabilities]]. More generally, the expected value of a function of a random variable is the probability-weighted average of the values the function takes on for each possible value of the random variable.\n\nIn [[linear regression|regressions]] in which the [[dependent variable]] is assumed to be affected by both current and lagged (past) values of the [[independent variable]], a [[distributed lag]] function is estimated, this function being a weighted average of the current and various lagged independent variable values. Similarly, a [[moving average model]] specifies an evolving variable as a weighted average of current and various lagged values of a random variable.\n\n=== Mechanics ===\nThe terminology ''weight function'' arises from [[mechanics]]: if one has a collection of ''<math>n</math>'' objects on a [[lever]], with weights <math>\\scriptstyle w_1, \\ldots, w_n</math> (where [[weight]] is now interpreted in the physical sense) and locations :<math>\\scriptstyle\\boldsymbol{x}_1,\\dotsc,\\boldsymbol{x}_n</math>, then the lever will be in balance if the [[Lever|fulcrum]] of the lever is at the [[center of mass]] \n\n:<math>\\frac{\\sum_{i=1}^n w_i \\boldsymbol{x}_i}{\\sum_{i=1}^n w_i},</math>\n\nwhich is also the weighted average of the positions <math>\\scriptstyle\\boldsymbol{x}_i</math>.\n\n== Continuous weights ==\nIn the continuous setting, a weight is a positive [[measure (mathematics)|measure]] such as <math>w(x) \\, dx</math> on some [[domain (mathematics)|domain]] <math>\\Omega</math>,which is typically a [[subset]] of a [[Euclidean space]] <math>\\scriptstyle{\\mathbb R}^n</math>, for instance <math>\\Omega</math> could be an [[Interval (mathematics)|interval]] <math>[a,b]</math>.  Here <math>dx</math> is [[Lebesgue measure]] and <math>\\scriptstyle w\\colon \\Omega \\to \\R^+</math> is a non-negative [[measurable]] [[mathematical function|function]].  In this context, the weight function <math>w(x)</math> is sometimes referred to as a [[density]].\n\n=== General definition ===\nIf <math>f\\colon \\Omega \\to {\\mathbb R}</math> is a [[real number|real]]-valued [[mathematical function|function]], then the ''unweighted'' [[integral]]\n\n:<math>\\int_\\Omega f(x)\\ dx</math>\n\ncan be generalized to the ''weighted integral'' \n\n:<math>\\int_\\Omega f(x) w(x)\\, dx</math>\n\nNote that one may need to require <math>f</math> to be [[absolutely integrable function|absolutely integrable]] with respect to the weight <math>w(x) \\, dx</math> in order for this integral to be finite.\n\n=== Weighted volume ===\nIf ''E'' is a subset of <math>\\Omega</math>, then the [[volume]] vol(''E'') of ''E'' can be generalized to the ''weighted volume'' \n:<math> \\int_E w(x)\\ dx,</math>\n\n=== Weighted average ===\nIf <math>\\Omega</math> has finite non-zero weighted volume, then we can replace the unweighted [[average]] \n\n:<math>\\frac{1}{\\mathrm{vol}(\\Omega)} \\int_\\Omega f(x)\\ dx</math>\n\nby the '''weighted average''' \n\n:<math> \\frac{\\int_\\Omega f(x)\\ w(x) dx}{\\int_\\Omega w(x)\\ dx}</math>\n\n=== Bilinear form ===\nIf <math> f\\colon \\Omega \\to {\\mathbb R}</math> and <math> g\\colon \\Omega \\to {\\mathbb R}</math> are two functions, one can generalize the unweighted [[bilinear form]] \n\n:<math>\\langle f, g \\rangle := \\int_\\Omega f(x) g(x)\\ dx</math>\n\nto a weighted bilinear form \n\n:<math>\\langle f, g \\rangle := \\int_\\Omega f(x) g(x)\\ w(x)\\ dx.</math>\n\nSee the entry on [[orthogonal polynomials]] for examples of weighted [[orthogonal functions]].\n\n== See also ==\n* [[Center of mass]]\n* [[Numerical integration]]\n* [[Orthogonality]]\n* [[Weighted mean]]\n* [[Kernel (statistics)]]\n* [[Measure (mathematics)]]\n* [[Riemann–Stieltjes integral]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Weight Function}}\n[[Category:Mathematical analysis]]\n[[Category:Measure theory]]\n[[Category:Combinatorial optimization]]\n[[Category:Functional analysis]]\n[[Category:Types of functions]]"
    },
    {
      "title": "Matroid",
      "url": "https://en.wikipedia.org/wiki/Matroid",
      "text": "In [[combinatorics]], a branch of [[mathematics]], a '''matroid''' {{IPAc-en|ˈ|m|eɪ|t|r|ɔɪ|d}} is a structure that abstracts and generalizes the notion of [[linear independence]] in [[vector space]]s. There are many equivalent ways to define a matroid, the most significant being in terms of independent sets, bases, circuits, closed sets or flats, closure operators, and rank functions.\n\nMatroid theory borrows extensively from the terminology of [[linear algebra]] and [[graph theory]], largely because it is the abstraction of various notions of central importance in these fields. Matroids have found applications in geometry, [[topology]], [[combinatorial optimization]], [[network theory]] and [[coding theory]].<ref name=Neel2009>{{cite journal|last1=Neel|first1=David L.|last2=Neudauer|first2=Nancy Ann|title=Matroids you have known|journal=Mathematics Magazine|date=2009|volume=82|issue=1|pages=26–41|url=http://www.maa.org/sites/default/files/pdf/shortcourse/2011/matroidsknown.pdf|accessdate=4 October 2014|doi=10.4169/193009809x469020}}</ref><ref name=Kashyap2009>{{cite web|last1=Kashyap|first1=Navin|last2=Soljanin|first2=Emina|last3=Vontobel|first3=Pascal|title=Applications of Matroid Theory and Combinatorial Optimization to Information and Coding Theory|url=https://www.birs.ca/workshops/2009/09w5103/report09w5103.pdf|website=www.birs.ca|accessdate=4 October 2014}}</ref>\n\n==Definition==<!-- [[Hereditary property (matroid)]] redirects to this section title-->\nThere are many equivalent ([[Cryptomorphism|cryptomorphic]]) ways to define a (finite) matroid.<ref name=\"oxley\">A standard source for basic definitions and results about matroids is Oxley (1992).  An older standard source is Welsh (1976).  See Brylawski's appendix in White (1986), pp.&nbsp;298–302, for a list of equivalent axiom systems.</ref>\n\n===Independent sets===\nIn terms of independence, a finite matroid <math>M</math> is a pair <math>(E,\\mathcal{I})</math>, where <math>E</math> is a [[finite set]] (called the '''ground set''') and <math>\\mathcal{I}</math> is a [[family of sets|family]] of [[subset]]s of <math>E</math> (called the '''independent sets''') with the following properties:<ref name=\"w7-9\">{{harvtxt|Welsh|1976}}, Section 1.2, \"Axiom Systems for a Matroid\", pp. 7–9.</ref>\n# The [[empty set]] is independent, i.e., <math>\\emptyset\\in\\mathcal{I}</math>. Alternatively, at least one subset of <math>E</math> is independent, i.e., <math>\\mathcal{I}\\neq\\emptyset</math>.\n# Every subset of an independent set is independent, i.e., for each <math>A'\\subset A\\subset E</math>, if <math>A\\in\\mathcal{I}</math> then <math>A'\\in\\mathcal{I}</math>.  This is sometimes called the '''hereditary property'''.\n# If <math>A</math> and <math>B</math> are two independent sets (i.e., each set is independent) and <math>A</math> has more elements than <math>B</math>, then there exists <math>x\\in A \\backslash B</math> such that <math>B \\cup \\{x\\}</math> is in <math>\\mathcal{I}</math>. This is sometimes called the '''augmentation property''' or the '''independent set exchange property'''.\n\nThe first two properties define a combinatorial structure known as an [[independence system]] (or [[abstract simplicial complex]]).\n\n===Bases and circuits===\nA subset of the ground set <math>E</math> that is not independent is called '''dependent'''.  A maximal independent set—that is, an independent set which becomes dependent on adding any element of <math>E</math>—is called a '''basis''' for the matroid. A '''circuit''' in a matroid <math>M</math> is a minimal dependent subset of <math>E</math>—that is, a dependent set whose proper subsets are all independent.  The terminology arises because the circuits of [[graphic matroid]]s are cycles in the corresponding graphs.<ref name=\"w7-9\"/>\n\nThe dependent sets, the bases, or the circuits of a matroid characterize the matroid completely: a set is independent if and only if it is not dependent, if and only if it is a subset of a basis, and if and only if it does not contain a circuit. The collections of dependent sets, of bases, and of circuits each have simple properties that may be taken as axioms for a matroid.  For instance, one may define a matroid <math>M</math> to be a pair <math>(E,\\mathcal{B})</math>, where <math>E</math> is a finite set as before and <math>\\mathcal{B}</math> is a collection of subsets of <math>E</math>, called \"bases\", with the following properties:<ref name=\"w7-9\"/>\n# <math>\\mathcal{B}</math> is nonempty.\n# If <math>A</math> and <math>B</math> are distinct members of <math>\\mathcal{B}</math> and <math>a\\in A\\setminus B</math>, then there exists an element <math>b\\in B\\setminus A</math> such that <math>A \\setminus \\{ a \\} \\cup \\{b\\} \\in \\mathcal{B}</math>.  This property is called the '''basis exchange property'''.\nIt follows from the basis exchange property that no member of <math>\\mathcal{B}</math> can be a proper subset of another.\n\n===Rank functions===\nIt is a basic result of matroid theory, directly analogous to a similar theorem of [[basis (linear algebra)|bases in linear algebra]], that any two bases of a matroid <math>M</math> have the same number of elements.  This number is called the '''[[matroid rank|rank]]''' of&nbsp;<math>M</math>. If <math>M</math> is a matroid on <math>E</math>, and <math>A</math> is a subset of <math>E</math>, then a matroid on <math>A</math> can be defined by considering a subset of <math>A</math> to be independent if and only if it is independent in <math>M</math>. This allows us to talk about '''submatroids''' and about the rank of any subset of <math>E</math>. The rank of a subset <math>A</math> is given by the '''[[matroid rank|rank function]]''' <math>r(A)</math> of the matroid, which has the following properties:<ref name=\"w7-9\"/>\n*The value of the rank function is always a non-negative [[integer]].\n*For any subset <math>A</math> of <math>E</math>, <math>r(A) \\le |A|</math>.\n*For any two subsets <math>A</math> and <math>B</math> of <math>E</math>, <math>r(A\\cup B)+r(A\\cap B)\\le r(A)+r(B)</math>. That is, the rank is a [[submodular function]].\n*For any set <math>A</math> and element <math>x</math>, <math>r(A)\\le r(A\\cup\\{x\\})\\le r(A)+1</math>. From the first of these two inequalities it follows more generally that, if <math>A\\subset B\\subset E</math>, then <math>r(A)\\leq r(B)\\leq r(E)</math>. That is, the rank is a [[monotonic function]].\nThese properties can be used as one of the alternative definitions of a finite matroid: if <math>(E,r)</math> satisfies these properties, then the independent sets of a matroid over <math>E</math> can be defined as those subsets <math>A</math> of <math>E</math> with <math>r(A)=|A|</math>.\n\nThe difference <math>|A|-r(A)</math> is called the '''nullity''' or '''corank''' of the subset <math>A</math>. It is the minimum number of elements that must be removed from <math>A</math> to obtain an independent set. The nullity of <math>E</math> in <math>M</math> is called the nullity or corank of <math>M</math>.\n\n===Closure operators===\nLet <math>M</math> be a matroid on a finite set <math>E</math>, with rank function <math>r</math> as above.  The '''closure'''  <math>\\operatorname{cl}(A)</math>  of a subset <math>A</math> of <math>E</math> is the set\n:<math>\\operatorname{cl}(A) = \\Bigl\\{x\\in E\\mid r(A)=r\\bigl(A\\cup\\{x\\}\\bigr)\\Bigr\\}</math>.\nThis defines a [[closure operator]] <math>\\operatorname{cl}: \\mathcal{P}(E)\\to \\mathcal{P}(E)</math> where <math>\\mathcal{P}</math> denotes the [[power set]], with the following properties:\n* For all subsets <math>X</math> of <math>E</math>, <math>X\\subseteq \\operatorname{cl}(X)</math>.\n* For all subsets <math>X</math> of <math>E</math>, <math>\\operatorname{cl}(X)= \\operatorname{cl}(\\operatorname{cl}(X))</math>.\n* For all subsets <math>X</math> and <math>Y</math> of <math>E</math> with <math>X\\subseteq Y</math>, <math>\\operatorname{cl}(X)\\subseteq \\operatorname{cl}(Y)</math>.\n* For all elements <math>a</math>, and <math>b</math> of <math>E</math>  and all subsets <math>Y</math> of <math>E</math>, if <math>a\\in\\operatorname{cl}(Y\\cup \\{b\\}) \\setminus \\operatorname{cl}(Y)</math> then <math>b\\in\\operatorname{cl}(Y\\cup \\{a\\}) \\setminus \\operatorname{cl}(Y)</math>.\nThe first three of these properties are the defining properties of a closure operator. The fourth is sometimes called the '''Mac Lane–Steinitz exchange property'''. These properties may be taken as another definition of matroid: every function <math>\\operatorname{cl}: \\mathcal{P}(E)\\to \\mathcal{P}(E)</math> that obeys these properties determines a matroid.<ref name=\"w7-9\"/>\n\n===Flats===\nA set whose closure equals itself is said to be '''closed''', or a '''flat''' or '''subspace''' of the matroid.<ref>{{harvtxt|Welsh|1976}}, Section 1.8, \"Closed sets = Flats = Subspaces\", pp. 21–22.</ref>  A set is closed if it is [[maximal element|maximal]] for its rank, meaning that the addition of any other element to the set would increase the rank. The closed sets of a matroid are characterized by a covering partition property:\n* The whole point set <math>E</math> is closed.\n* If <math>S</math> and <math>T</math> are flats, then <math>S\\cap T</math> is a flat.\n* If <math>S</math> is a flat, then the flats <math>T</math> that [[Covering relation|cover]] <math>S</math> (meaning that <math>T</math> properly contains <math>S</math> but there is no flat <math>U</math> between <math>S</math> and <math>T</math>), partition the elements of&nbsp;<math>E\\setminus S</math>.\n\nThe class <math>\\mathcal{L}(M)</math> of all flats, [[partially ordered set|partially ordered]] by set inclusion, forms a [[matroid lattice]].\nConversely, every matroid lattice <math>L</math> forms a matroid over its set <math>E</math> of [[Atom (order theory)|atoms]] under the following closure operator:  for a set <math>S</math> of atoms with join <math>\\bigvee S</math>,\n:<math>\\operatorname{cl}(S) = \\{ x\\in E\\mid x\\le\\bigvee S \\}</math>.\nThe flats of this matroid correspond one-for-one with the elements of the lattice; the flat corresponding to lattice element <math>y</math> is the set\n:<math>\\{ x\\in E\\mid x\\le y\\}</math>.\nThus, the lattice of flats of this matroid is naturally isomorphic to&nbsp;<math>L</math>.\n\n===Hyperplanes===\nIn a matroid of rank <math>r</math>, a flat of rank <math>r-1</math> is called a '''hyperplane'''.  (Hyperplanes are also called '''coatoms''' or '''copoints'''.) These are the maximal proper flats; that is, the only superset of a hyperplane that is also a flat is the set <math>E</math> of all the elements of the matroid.  An equivalent definition is that a coatom is a subset of ''E'' that does not span ''M'', but such that adding any other element to it does make a spanning set.<ref name=\"w38-39\">{{harvtxt|Welsh|1976}}, Section 2.2, \"The Hyperplanes of a Matroid\", pp. 38–39.</ref>\n\nThe family <math>\\mathcal{H}</math> of hyperplanes of a matroid has the following properties, which may be taken as yet another axiomatization of matroids:<ref name=\"w38-39\"/>\n*There do not exist distinct sets <math>X</math> and <math>Y</math> in <math>\\mathcal{H}</math> with <math>X\\subset Y</math>. That is, the hyperplanes form a [[Sperner family]].\n*For every <math>x\\in E</math> and distinct <math>Y,Z\\in\\mathcal{H}</math> with <math>x\\notin Y\\cup Z</math>, there exists <math>X\\in\\mathcal{H}</math> with <math>(Y\\cap Z)\\cup\\{x\\}\\subseteq X</math>.\n\n===Graphoids===\n\nMinty (1966) defined a '''graphoid''' as a triple <math>(L, C, D)</math> in which <math>C</math> and <math>D</math> are classes of nonempty subsets of <math>L</math> such that \n*no element of <math>C</math> (called a \"circuit\") contains another, \n*no element of <math>D</math> (called a \"cocircuit\") contains another, \n*no set in <math>C</math> and set in <math>D</math> intersect in exactly one element, and \n*whenever <math>L</math> is represented as the disjoint union of subsets <math>R, G, B</math> with <math>G=\\{g\\}</math> (a singleton set), then either an <math>X \\in C</math> exists such that <math>g \\in X \\subseteq R \\cup G</math> or a <math>Y \\in D</math> exists such that <math>g \\in Y \\subseteq B \\cup G.</math>\n\nHe proved that there is a matroid for which <math>C</math> is the class of circuits and <math>D</math> is the class of cocircuits.  Conversely, if <math>C</math> and <math>D</math> are the circuit and cocircuit classes of a matroid <math>M</math> with ground set <math>E</math>, then <math>(E, C, D)</math> is a graphoid.  Thus, graphoids give a self-dual cryptomorphic axiomatization of matroids.\n\n== Examples ==\n\n===Uniform matroids===\n\nLet <math>E</math> be a finite set and <math>k</math> a [[natural number]].  One may define a matroid on <math>E</math> by taking every <math>k</math>-element subset of <math>E</math> to be a basis.  This is known as the [[uniform matroid]] of rank <math>k</math>.  A uniform matroid with rank <math>k</math> and with <math>n</math> elements is denoted <math>U_{k,n}</math>. All uniform matroids of rank at least 2 are simple. The uniform matroid of rank 2 on <math>n</math> points is called the <math>n</math>-'''point line'''. A matroid is uniform if and only if it has no circuits of size less than one plus the rank of the matroid. The direct sums of uniform matroids are called [[partition matroid]]s.\n\nIn the uniform matroid <math>U_{0,n}</math>, every element is a loop (an element that does not belong to any independent set), and in the uniform matroid <math>U_{n,n}</math>, every element is a coloop (an element that belongs to all bases). The direct sum of matroids of these two types is a partition matroid in which every element is a loop or a coloop; it is called a '''discrete matroid'''. An equivalent definition of a discrete matroid is a matroid in which every proper, non-empty subset of the ground set <math>E</math> is a separator.\n\n===Matroids from linear algebra===\n[[File:fano plane.svg|thumb|The Fano matroid, derived from the [[Fano plane]]. It is [[GF(2)]]-linear but not real-linear.]]\n[[File:Vamos matroid.svg|thumb|The [[Vámos matroid]], not linear over any field]]\nMatroid theory developed mainly out of a deep examination of the properties of independence and dimension in vector spaces.  There are two ways to present the matroids defined in this way:\n* If <math>E</math> is any finite subset of a [[vector space]] <math>V</math>, then we can define a matroid <math>M</math> on <math>E</math> by taking the independent sets of <math>M</math> to be the [[linear independence|linearly independent]] subsets of <math>E</math>. The validity of the independent set axioms for this matroid follows from the [[Steinitz exchange lemma]].  If <math>M</math> is a matroid that can be defined in this way, we say the set <math>E</math> '''[[matroid representation|represents]]''' <math>M</math>.  Matroids of this kind are called '''vector matroids'''. An important example of a matroid defined in this way is the Fano matroid, a rank-three matroid derived from the [[Fano plane]], a [[finite geometry]] with seven points (the seven elements of the matroid) and seven lines (the nontrivial flats of the matroid). It is a linear matroid whose elements may be described as the seven nonzero points in a three-dimensional vector space over the [[finite field]] [[GF(2)]]. However, it is not possible to provide a similar representation for the Fano matroid using the [[real number]]s in place of GF(2).\n* A [[matrix (mathematics)|matrix]] <math>A</math> with entries in a [[field (mathematics)|field]] gives rise to a matroid <math>M</math> on its set of columns.  The dependent sets of columns in the matroid are those that are linearly dependent as vectors.  This matroid is called the '''column matroid''' of <math>A</math>, and <math>A</math> is said to '''represent''' <math>M</math>.  For instance, the Fano matroid can be represented in this way as a 3&nbsp;&times;&nbsp;7 [[Logical matrix|(0,1)-matrix]]. Column matroids are just vector matroids under another name, but there are often reasons to favor the matrix representation.  (There is one technical difference: a column matroid can have distinct elements that are the same vector, but a vector matroid as defined above cannot.  Usually this difference is insignificant and can be ignored, but by letting <math>E</math> be a [[multiset]] of vectors one brings the two definitions into complete agreement.)\n\nA matroid that is equivalent to a vector matroid, although it may be presented differently, is called '''representable''' or '''linear'''.  If <math>M</math> is equivalent to a vector matroid over a [[field (mathematics)|field]] <math>F</math>, then we say <math>M</math> is '''representable over''' <math>F</math> &nbsp;; in particular, <math>M</math> is '''real-representable''' if it is representable over the real numbers.  For instance, although a graphic matroid (see below) is presented in terms of a graph, it is also representable by vectors over any field.  A basic problem in matroid theory is to characterize the matroids that may be represented over a given field <math>F</math>; [[Rota's conjecture]] describes a possible characterization for every [[finite field]].  The main results so far are characterizations of [[binary matroid]]s (those representable over GF(2)) due to Tutte (1950s), of ternary matroids (representable over the 3-element field) due to Reid and Bixby, and separately to Seymour (1970s), and of quaternary matroids (representable over the 4-element field) due to Geelen, Gerards, and Kapoor (2000).  This is very much an open area.\n\nA [[regular matroid]] is a matroid that is representable over all possible fields. The [[Vámos matroid]] is the simplest example of a matroid that is not representable over any field.\n\n===Matroids from graph theory===\nA second original source for the theory of matroids is [[graph theory]].\n\nEvery finite graph (or [[multigraph]]) <math>G</math> gives rise to a matroid <math>M(G)</math> as follows: take as <math>E</math> the set of all edges in <math>G</math> and consider a set of edges independent if and only if it is a [[tree (graph theory)|forest]]; that is, if it does not contain a [[simple cycle]]. Then <math>M(G)</math> is called a '''cycle matroid'''. Matroids derived in this way are  '''[[graphic matroid]]s'''.  Not every matroid is graphic, but all matroids on three elements are graphic.<ref name=Ox13/>  Every graphic matroid is regular.\n\nOther matroids on graphs were discovered subsequently:\n*The [[bicircular matroid]] of a graph is defined by calling a set of edges independent if every connected subset contains at most one cycle.\n*In any directed or undirected graph <math>G</math> let <math>E</math> and <math>F</math> be two distinguished sets of vertices.  In the set <math>E</math>, define a subset <math>U</math> to be independent if there are |<math>U</math>| vertex-disjoint paths from <math>F</math> onto <math>U</math>.  This defines a matroid on <math>E</math> called a '''[[gammoid]]''':<ref name=Ox115/> a '''strict gammoid''' is one for which the set <math>E</math> is the whole vertex set of <math>G</math>.<ref name=Ox100>{{harvnb|Oxley|1992|p=100}}</ref>\n*In a [[bipartite graph]] <math>G = (U,V,E)</math>, one may form a matroid in which the elements are vertices on one side <math>U</math> of the bipartition, and the independent subsets are sets of endpoints of [[Matching (graph theory)|matchings]] of the graph. This is called a '''transversal matroid''',<ref name=Ox4648>{{harvnb|Oxley|1992|pp=46–48}}</ref><ref name=Wh877297>{{White|1987|pp=72–97}}</ref> and it is a special case of a gammoid.<ref name=Ox115>{{harvnb|Oxley|1992|pp=115}}</ref>  The transversal matroids are the [[dual matroid]]s to the strict gammoids.<ref name=Ox100/>\n*Graphic matroids have been generalized to matroids from [[signed graph]]s, [[gain graph]]s, and [[biased graph]]s.  A graph <math>G</math> with a distinguished linear class <math>B</math> of cycles, known as a \"biased graph\" <math>(G, B)</math>, has two matroids, known as the '''frame matroid''' and the '''lift matroid''' of the biased graph.  If every cycle belongs to the distinguished class, these matroids coincide with the cycle matroid of <math>G</math>.  If no cycle is distinguished, the frame matroid is the bicircular matroid of <math>G</math>. A signed graph, whose edges are labeled by signs, and a gain graph, which is a graph whose edges are labeled orientably from a group, each give rise to a biased graph and therefore have frame and lift matroids.\n*The [[Laman graph]]s form the bases of the two-dimensional [[rigidity matroid]], a matroid defined in the theory of [[structural rigidity]].\n*Let <math>G</math> be a connected graph and <math>E</math> be its edge set. Let <math>I</math> be the collection of subsets <math>F</math> of <math>E</math> such that <math>G - F</math> is still connected. Then <math>M^*(G)</math>, whose element set is <math>E</math> and with <math>I</math> as its class of independent sets, is a matroid called the '''bond matroid''' of <math>G</math>.  The rank function <math>r(F)</math> is the [[cyclomatic number]] of the subgraph induced on the edge subset <math>F</math>, which equals the number of edges outside a maximal forest of that subgraph, and also the number of independent cycles in it.\n\n===Matroids from field extensions===\nA third original source of matroid theory is [[field theory (mathematics)|field theory]].\n\nAn [[extension field|extension]] of a field gives rise to a matroid.  Suppose <math>F</math> and <math>K</math> are fields with <math>K</math> containing <math>F</math>.  Let <math>E</math> be any finite subset of <math>K</math>.  Define a subset <math>S</math> of <math>E</math> to be [[algebraic independence|algebraically independent]] if the extension field <math>F(S)</math> has [[transcendence degree]] equal to <math>|S|</math>.<ref name=Ox215>{{harvnb|Oxley|1992|p=215}}</ref>\n\nA matroid that is equivalent to a matroid of this kind is called an [[algebraic matroid]].<ref name=Ox216>{{harvnb|Oxley|1992|p=216}}</ref>  The problem of characterizing algebraic matroids is extremely difficult; little is known about it. The [[Vámos matroid]] provides an example of a matroid that is not algebraic.\n\n== Basic constructions ==\nThere are some standard ways to make new matroids out of old ones.\n\n===Duality===\nIf ''M'' is a finite matroid, we can define the '''orthogonal''' or '''[[dual matroid]]''' ''M''* by taking the same underlying set and calling a set a ''basis'' in ''M''* if and only if its complement is a basis in ''M''.  It is not difficult to verify that ''M''* is a matroid and that the dual of ''M''* is ''M''.<ref name=Whi8632>{{harvnb|White|1986|p=32}}</ref>\n\nThe dual can be described equally well in terms of other ways to define a matroid.  For instance:\n\n* A set is independent in ''M''* if and only if its complement spans ''M''.\n* A set is a circuit of ''M''* if and only if its complement is a coatom in ''M''.\n* The rank function of the dual is <math>r^*(S) = |S|- r(M) + r\\left(E\\setminus S\\right)</math>.\n\nAccording to a matroid version of [[Kuratowski's theorem]], the dual of a graphic matroid ''M'' is a graphic matroid if and only if ''M'' is the matroid of a [[planar graph]]. In this case, the dual of ''M'' is the matroid of the [[dual graph]] of ''G''.<ref name=Whi86105>{{harvnb|White|1986|p=105}}</ref>  The dual of a vector matroid representable over a particular field ''F'' is also representable over ''F''. The dual of a transversal matroid is a strict gammoid and vice versa.\n\n'''Example'''\n\nThe cycle matroid of a graph is the dual matroid of its bond matroid.\n\n===Minors===\n{{Main|Matroid minor}}\nIf ''M'' is a matroid with element set ''E'', and ''S'' is a subset of ''E'', the '''restriction''' of ''M'' to ''S'', written ''M''&nbsp;|''S'', is the matroid on the set ''S'' whose independent sets are the independent sets of ''M'' that are contained in ''S''.  Its circuits are the circuits of ''M'' that are contained in ''S'' and its rank function is that of ''M'' restricted to subsets of ''S''. In linear algebra, this corresponds to restricting to the subspace generated by the vectors in ''S''.  Equivalently if ''T'' = ''M''−''S'' this may be termed the '''deletion''' of ''T'', written ''M''\\''T'' or ''M''−''T''.   The submatroids of ''M'' are precisely the results of a sequence of deletions: the order is irrelevant.<ref name=Whi86131>{{harvnb|White|1986|p=131}}</ref><ref name=Whi86224>{{harvnb|White|1986|p=224}}</ref>\n\nThe dual operation of restriction is contraction.<ref name=Whi866139>{{harvnb|White|1986|p=139}}</ref> If ''T'' is a subset of ''E'', the '''contraction''' of ''M'' by ''T'', written ''M''/''T'', is the matroid on the underlying set ''E &minus; T'' whose rank function is <math>r'(A) = r(A \\cup T) - r(T).</math><ref name=Whi86140>{{harvnb|White|1986|p=140}}</ref>  In linear algebra, this corresponds to looking at the quotient space by the linear space generated by the vectors in ''T'', together with the images of the vectors in ''E - T''.\n\nA matroid ''N'' that is obtained from ''M'' by a sequence of restriction and contraction operations is called a [[matroid minor|minor]] of ''M''.<ref name=Whi86224/><ref name=Whi86150>{{harvnb|White|1986|p=150}}</ref>  We say ''M'' '''contains''' ''N'' '''as a minor'''. Many important families of matroids may be characterized by the [[minimal element|minor-minimal]] matroids that do not belong to the family; these are called '''forbidden''' or '''excluded minors'''.<ref name=Whi861467>{{harvnb|White|1986|pp=146–147}}</ref>\n\n===Sums and unions===\nLet ''M'' be a matroid with an underlying set of elements ''E'', and let ''N'' be another matroid on an underlying set ''F''.\nThe '''direct sum''' of matroids ''M'' and ''N'' is the matroid whose underlying set is the [[disjoint union]] of ''E'' and ''F'', and whose independent sets are the disjoint unions of an independent set of ''M'' with an independent set of ''N''.\n\nThe '''union''' of ''M'' and ''N'' is the matroid whose underlying set is the union (not the disjoint union) of ''E'' and ''F'', and whose independent sets are those subsets which are the union of an independent set in ''M'' and one in ''N''.  Usually the term \"union\" is applied when ''E'' = ''F'', but that assumption is not essential.  If ''E'' and ''F'' are disjoint, the union is the direct sum.\n\n== Additional terminology == <!--Linked to by [[Simple matroid]] (redirect)-->\nLet ''M'' be a matroid with an underlying set of elements ''E''.\n* ''E'' may be called the '''ground set''' of ''M''.  Its elements may be called the '''points''' of ''M''.\n* A subset of ''E'' '''spans''' ''M'' if its closure is ''E''.  A set is said to '''span''' a closed set ''K'' if its closure is ''K''.\n* The [[matroid girth|girth]] of a matroid is the size of its smallest circuit or dependent set.\n* An element that forms a single-element circuit of ''M'' is called a '''loop'''. Equivalently, an element is a loop if it belongs to no basis.<ref name=Ox13/><ref name=Wh86130>{{harvnb|White|1986|p=130}}</ref>\n* An element that belongs to no circuit is called a '''coloop''' or '''isthmus'''. Equivalently, an element is a coloop if it belongs to every basis.  Loop and coloops are mutually dual.<ref name=Wh86130/>\n* If a two-element set {''f, g''} is a circuit of ''M'', then ''f'' and ''g'' are '''parallel''' in ''M''.<ref name=Ox13/>\n* A matroid is called '''simple''' if it has no circuits consisting of 1 or 2 elements.  That is, it has no loops and no parallel elements.  The term '''combinatorial geometry''' is also used.<ref name=Ox13>{{harvnb|Oxley|1992|p=13}}</ref>  A simple matroid obtained from another matroid ''M'' by deleting all loops and deleting one element from each 2-element circuit until no 2-element circuits remain is called a '''simplification''' of ''M''.<ref name=Ox52>{{harvnb|Oxley|1992|p=52}}</ref>  A matroid is '''co-simple''' if its dual matroid is simple.<ref name=Ox347>{{harvnb|Oxley|1992|p=347}}</ref>\n* A union of circuits is sometimes called a '''cycle''' of ''M''.  A cycle is therefore the complement of a flat of the dual matroid.  (This usage conflicts with the common meaning of \"cycle\" in graph theory.)\n* A '''separator''' of ''M'' is a subset ''S'' of ''E'' such that <math>r(S) + r(E-S) = r(M)</math>.  A '''proper''' or '''non-trivial separator''' is a separator that is neither ''E'' nor the empty set.<ref name=Ox128>{{harvnb|Oxley|1992|p=128}}</ref>  An '''irreducible separator''' is a separator that contains no other non-empty separator.  The irreducible separators partition the ground set ''E''.\n* A matroid which cannot be written as the direct sum of two nonempty matroids, or equivalently which has no proper separators, is called '''connected''' or '''irreducible'''.  A matroid is connected if and only if its dual is connected.<ref name=Wh86110>{{harvnb|White|1986|p=110}}</ref>\n* A maximal irreducible submatroid of ''M'' is called a '''component''' of ''M''.  A component is the restriction of ''M'' to an irreducible separator, and contrariwise, the restriction of ''M'' to an irreducible separator is a component.  A separator is a union of components.<ref name=Ox128/>\n* A matroid ''M'' is called a '''frame matroid''' if it, or a matroid that contains it, has a basis such that all the points of ''M'' are contained in the lines that join pairs of basis elements.<ref>{{cite journal | zbl=0797.05027 | last=Zaslavsky | first=Thomas | title=Frame matroids and biased graphs | journal=Eur. J. Comb. | volume=15 | number=3 | pages=303–307 | year=1994 | issn=0195-6698 | url=http://www.sciencedirect.com/science/article/pii/S0195669884710341 | doi=10.1006/eujc.1994.1034}}</ref>\n* A matroid is called a [[paving matroid]] if all of its circuits have size at least equal to its rank.<ref name=Ox26>{{harvnb|Oxley|1992|p=26}}</ref>\n* The [[matroid polytope]] <math>P_M</math> is the [[convex hull]] of the [[indicator vector]]s of the bases of <math>M</math>.\n\n==Algorithms==\n\n===Greedy algorithm===\nA [[weighted matroid]] is a matroid together with a function from its elements to the nonnegative [[real number]]s. The weight of a subset of elements is defined to be the sum of the weights of the elements in the subset. The [[greedy algorithm]] can be used to find a maximum-weight basis of the matroid, by starting from the empty set and repeatedly adding one element at a time, at each step choosing a maximum-weight element among the elements whose addition would preserve the independence of the augmented set.<ref name=Ox63>{{harvnb|Oxley|1992|p=63}}</ref> This algorithm does not need to know anything about the details of the matroid's definition, as long as it has access to the matroid through an [[matroid oracle|independence oracle]], a subroutine for testing whether a set is independent.\n\nThis optimization algorithm may be used to characterize matroids: if a family ''F'' of sets, closed under taking subsets, has the property that, no matter how the sets are weighted, the greedy algorithm finds a maximum-weight set in the family, then ''F'' must be the family of independent sets of a matroid.<ref name=Ox64>{{harvnb|Oxley|1992|p=64}}</ref>\n\nThe notion of matroid has been generalized to allow for other types of sets on which a greedy algorithm gives optimal solutions; see [[greedoid]] and [[matroid embedding]] for more information.\n\n===Matroid partitioning===\nThe [[matroid partitioning]] problem is to partition the elements of a matroid into as few independent sets as possible, and the matroid packing problem is to find as many disjoint spanning sets as possible. Both can be solved in polynomial time, and can be generalized to the problem of computing the rank or finding an independent set in a matroid sum.\n\n===Matroid intersection===\nThe [[matroid intersection|intersection]] of two or more matroids is the family of sets that are simultaneously independent in each of the matroids. The problem of finding the largest set, or the maximum weighted set, in the intersection of two matroids can be found in [[polynomial time]], and provides a solution to many other important combinatorial optimization problems. For instance, [[maximum matching]] in [[bipartite graph]]s can be expressed as a problem of intersecting two [[partition matroid]]s. However, finding the largest set in an intersection of three or more matroids is [[NP-complete]].\n\n===Matroid software===\n\nTwo standalone systems for calculations with matroids are Kingan's [http://userhome.brooklyn.cuny.edu/skingan/software.html Oid] and Hlineny's [http://www.fi.muni.cz/~hlineny/MACEK/ Macek]. Both of them are open sourced packages. \"Oid\" is an interactive, extensible software system for experimenting with matroids. \"Macek\" is a specialized software system with tools and routines for reasonably efficient combinatorial computations with representable matroids.\n\n[[SageMath|SAGE]], the open source mathematics software system, contains a matroid package.\n\n==Polynomial invariants==\n\nThere are two especially significant polynomials associated to a finite matroid ''M'' on the ground set ''E''.  Each is a '''matroid invariant''', which means that isomorphic matroids have the same polynomial.\n\n===Characteristic polynomial===\n\nThe '''characteristic polynomial''' of ''M'' (which is sometimes called the '''chromatic polynomial''',<ref name=Wh87127/> although it does not count colorings), is defined to be\n:<math>p_M(\\lambda) := \\sum_{S \\subseteq E} (-1)^{|S|}\\lambda^{r(M)-r(S)},</math>\nor equivalently (as long as the empty set is closed in ''M'') as\n:<math>p_M(\\lambda) := \\sum_{A} \\mu(\\emptyset,A) \\lambda^{r(M)-r(A)} \\ ,</math>\nwhere μ denotes the [[Möbius function (combinatorics)|Möbius function]] of the [[geometric lattice]] of the matroid and the sum is taken over all the flats A of the matroid.<ref name=Wh87120>{{harvnb|White|1987|p=120}}</ref>\n\nWhen ''M'' is the cycle matroid ''M''(''G'') of a graph ''G'', the characteristic polynomial is a slight transformation of the [[chromatic polynomial]], which is given by χ<sub>''G''</sub>&nbsp;(λ) = λ<sup>c</sup>''p''<sub>''M''(''G'')</sub>&nbsp;(λ), where ''c'' is the number of connected components of ''G''.\n\nWhen ''M'' is the bond matroid ''M''*(''G'') of a graph ''G'', the characteristic polynomial equals the [[Tutte polynomial#Flow polynomial|flow polynomial]] of ''G''.\n\nWhen ''M'' is the matroid ''M''(''A'') of an [[Arrangement of hyperplanes|arrangement]] ''A'' of linear hyperplanes in '''R'''<sup>''n''</sup> (or ''F''<sup>''n''</sup> where ''F'' is any field), the characteristic polynomial of the arrangement is given by ''p''<sub>''A''</sub>&nbsp;(λ) = λ<sup>''n''&minus;''r''(''M'')</sup>''p''<sub>''M''(''A'')</sub>&nbsp;(λ).\n\n====Beta invariant====\nThe '''beta invariant''' of a matroid, introduced by Crapo (1967), may be expressed in terms of the characteristic polynomial ''p'' as an evaluation of the derivative<ref name=Wh87123>{{harvnb|White|1987|p=123}}</ref>\n:<math> \\beta(M) = (-1)^{r(M)-1} p_M'(1) \\  </math>\nor directly as<ref name=Wh87124>{{harvnb|White|1987|p=124}}</ref>\n:<math> \\beta(M) = (-1)^{r(M)} \\sum_{X \\subseteq E} (-1)^{|X|} r(X) \\ . </math>\nThe beta invariant is non-negative, and is zero if and only if ''M'' is disconnected, or empty, or a loop.  Otherwise it depends only on the lattice of flats of ''M''.  If ''M'' has no loops and coloops then β(''M'') = β(''M''<sup>&lowast;</sup>).<ref name=Wh87124/>\n\n===Tutte polynomial===\n\nThe '''[[Tutte polynomial]]''' of a matroid, ''T''<sub>''M''</sub>&nbsp;(''x'',''y''), generalizes the characteristic polynomial to two variables.  This gives it more combinatorial interpretations, and also gives it the duality property\n:<math>T_{M^*}(x,y) = T_M(y,x),</math>\nwhich implies a number of dualities between properties of ''M'' and properties of ''M''&nbsp;*.  One definition of the Tutte polynomial is\n:<math>T_M(x,y) = \\sum_{S \\subseteq E} (x-1)^{r(M)-r(S)}(y-1)^{|S|-r(S)}.</math>\nThis expresses the Tutte polynomial as an evaluation of the '''corank-nullity''' or '''rank generating polynomial''',<ref name=Wh87126>{{harvnb|White|1987|p=126}}</ref>\n:<math>R_M(u,v) = \\sum_{S\\subseteq E} u^{r(M)-r(S)}v^{|S|-r(S)}.</math>\nFrom this definition it is easy to see that the characteristic polynomial is, up to a simple factor, an evaluation of ''T''<sub>''M''</sub>, specifically, \n:<math>p_M(\\lambda) = (-1)^{r(M)} T_M(1-\\lambda,0). </math>\n\nAnother definition is in terms of internal and external activities and a sum over bases, reflecting the fact that ''T''(1,1) is the number of bases.<ref name=Wh92188>{{harvnb|White|1992|p=188}}</ref>  This, which sums over fewer subsets but has more complicated terms, was Tutte's original definition.\n\nThere is a further definition in terms of recursion by deletion and contraction.<ref name=Wh86260>{{harvnb|White|1986|p=260}}</ref>  The deletion-contraction identity is\n:<math>F(M) = F(M-e)+F(M/e)</math> when <math>e</math> is neither a loop nor a coloop.\nAn invariant of matroids (i.e., a function that takes the same value on isomorphic matroids) satisfying this recursion and the multiplicative condition\n:<math>F(M\\oplus M') = F(M) F(M')</math>\nis said to be a '''Tutte-Grothendieck invariant'''.<ref name=Wh87126/> The Tutte polynomial is the most general such invariant; that is, the Tutte polynomial is a Tutte-Grothendieck invariant and every such invariant is an evaluation of the Tutte polynomial.<ref name=Wh87127>{{harvnb|White|1987|p=127}}</ref>\n\nThe [[Tutte polynomial]] ''T''<sub>''G''</sub>&nbsp; of a graph is the Tutte polynomial ''T''<sub>''M''(''G'')</sub> of its cycle matroid.\n\n== Infinite matroids ==\n<!-- [[Infinite matroid]] redirects here. -->\nThe theory of infinite matroids is much more complicated than that of finite matroids and forms a subject of its own.  For a long time, one of the difficulties has been that there were many reasonable and useful definitions, none of which appeared to capture all the important aspects of finite matroid theory.  For instance, it seemed to be hard to have bases, circuits, and duality together in one notion of infinite matroids.\n\nThe simplest definition of an infinite matroid is to require ''finite rank''; that is, the rank of ''E'' is finite.  This theory is similar to that of finite matroids except for the failure of duality due to the fact that the dual of an infinite matroid of finite rank does not have finite rank.  Finite-rank matroids include any subsets of finite-dimensional vector spaces and of [[Field (mathematics)|field extensions]] of finite [[transcendence degree]].\n\nThe next simplest infinite generalization is finitary matroids.  A matroid is '''finitary''' if it has the property that\n:<math>x \\in cl(Y) \\Leftrightarrow (\\exists Y' \\subseteq Y) Y' \\text{ is finite and } x \\in cl(Y').</math>\nEquivalently, every dependent set contains a finite dependent set.\nExamples are linear dependence of arbitrary subsets of infinite-dimensional [[vector space]]s (but not infinite dependencies as in [[Hilbert space|Hilbert]] and [[Banach space]]s), and algebraic dependence in arbitrary subsets of field extensions of possibly infinite transcendence degree.  Again, the class of finitary matroid is not self-dual, because the dual of a finitary matroid is not finitary.\nFinitary infinite matroids are studied in [[model theory]], a branch of [[mathematical logic]] with strong ties to [[algebra]].\n\nIn the late 1960s matroid theorists asked for a more general notion that shares the different aspects of finite matroids and generalizes their duality. Many notions of infinite matroids were defined in response to this challenge, but the question remained open. One of the approaches examined by D.A. Higgs became known as ''B-matroids'' and was studied by Higgs, Oxley and others in the 1960s and 1970s. According to a recent result by {{harvs|last1=Bruhn|last2=Diestel|last3=Kriesell|last4=Pendavingh|year=2013|txt}}, it solves the problem: Arriving at the same notion independently, they provided five equivalent systems of axioms – in terms of independence, bases, circuits, closure and rank. The duality of B-matroids generalizes dualities that can be observed in infinite graphs.\n\nThe independence axioms are as follows:\n# The empty set is independent.\n# Every subset of an independent set is independent.\n# For every [[maximal element|nonmaximal]] (under set inclusion) independent set ''I'' and maximal independent set ''J'', there is <math>x\\in J \\setminus I</math> such that <math>I\\cup\\{x\\}</math> is independent.\n# For every subset ''X'' of the base space, every independent subset ''I'' of ''X'' can be extended to a maximal independent subset of ''X''.\n\nWith these axioms, every matroid has a dual.\n\n==History==\n\nMatroid theory was introduced by {{harvs|last=Whitney|first=Hassler|authorlink=Hassler Whitney|year=1935|txt}}. It was also independently discovered by [[Takeo Nakasawa]], whose work was forgotten for many years {{harv|Nishimura|Kuroda|2009}}.\n\nIn his seminal paper, Whitney provided two axioms for independence, and defined any structure adhering to these axioms to be \"matroids\".\n(Although it was perhaps implied, he did not include an axiom requiring at least one subset to be independent.)\nHis key observation was that these axioms provide an abstraction of \"independence\" that is common to both graphs and matrices.\nBecause of this, many of the terms used in matroid theory resemble the terms for their analogous concepts in [[linear algebra]] or [[graph theory]].\n\nAlmost immediately after Whitney first wrote about matroids, an important article was written by {{harvs|last=Mac Lane|first=Saunders|authorlink=Saunders Mac Lane|year=1936|txt}} on the relation of matroids to [[projective geometry]]. A year later, {{harvs|last=van der Waerden|first=B. L.|authorlink=Bartel Leendert van der Waerden|year=1937|txt}} noted similarities between algebraic and linear dependence in his classic textbook on Modern Algebra.\n\nIn the 1940s [[Richard Rado]] developed further theory under the name \"independence systems\" with an eye towards [[Transversal (combinatorics)|transversal theory]], where his name for the subject is still sometimes used.\n\nIn the 1950s [[W. T. Tutte]] became the foremost figure in matroid theory, a position he retained for many years.  His contributions were plentiful, including the characterization of [[binary matroid|binary]], [[regular matroid|regular]], and [[graphic matroid|graphic]] matroids by [[Matroid minor|excluded minors]]; the regular-matroid representability theorem; the theory of chain groups and their matroids; and the tools he used to prove many of his results, the \"Path theorem\" and \"[[Tutte homotopy theorem|Homotopy theorem]]\" (see, e.g., {{harvnb|Tutte|1965}}), which are so complex that later theorists have gone to great trouble to eliminate the necessity of using them in proofs.  (A fine example is [[A. M. H. Gerards]]' short proof ([[#CITEREFGerards1989|1989]]) of Tutte's characterization of regular matroids.)\n\n{{harvs|first=Henry|last=Crapo|year=1969|txt}} and {{harvs|first=Thomas|last=Brylawski|year=1972|txt}} generalized to matroids Tutte's \"dichromate\", a graphic polynomial now known as the [[Tutte polynomial]] (named by Crapo).  Their work has recently (especially in the 2000s) been followed by a flood of papers&mdash;though not as many as on the Tutte polynomial of a graph.\n\nIn 1976 [[Dominic Welsh]] published the first comprehensive book on matroid theory.\n\n[[Paul Seymour (mathematician)|Paul Seymour]]'s decomposition theorem for regular matroids ([[#CITEREFSeymour1980|1980]]) was the most significant and influential work of the late 1970s and the 1980s.\nAnother fundamental contribution, by {{harvtxt|Kahn|Kung|1982}}, showed why projective geometries and Dowling geometries play such an important role in matroid theory.\n\nBy this time there were many other important contributors, but one should not omit to mention [[Geoff Whittle]]'s extension to ternary matroids of Tutte's characterization of binary matroids that are representable over the rationals {{harv|Whittle|1995}}, perhaps the biggest single contribution of the 1990s.  In the current period (since around 2000) the Matroid Minors Project of [[Jim Geelen]], Gerards, Whittle, and others, which attempts to duplicate for matroids that are representable over a finite field the success of the Robertson&ndash;Seymour Graph Minors Project (see [[Robertson–Seymour theorem]]), has produced substantial advances in the structure theory of matroids.  Many others have also contributed to that part of matroid theory, which (in the first and second decades of the 21st century) is flourishing.\n\n==Researchers==\n\nMathematicians who pioneered the study of matroids include [[Takeo Nakasawa]],{{sfnp|Nishimura|Kuroda|2009}} [[Saunders Mac Lane]], [[Richard Rado]], [[W. T. Tutte]], [[Bartel Leendert van der Waerden|B. L. van der Waerden]], and [[Hassler Whitney]].\nOther major contributors include [[Jack Edmonds]], [[Jim Geelen]], [[Eugene Lawler]], [[László Lovász]], [[Gian-Carlo Rota]], [[Paul Seymour (mathematician)|P. D. Seymour]], and [[Dominic Welsh]].\n\n==See also==\n* [[Antimatroid]]\n* [[Coxeter matroid]]\n* [[Oriented matroid]]\n* [[Pregeometry (model theory)]]\n* [[Polymatroid]]\n* [[Greedoid]]\n\n==Notes==\n{{Reflist|30em}}\n\n== References ==\n*{{citation\n | last1 = Bruhn | first1 = Henning\n | last2 = Diestel | first2 = Reinhard\n | last3 = Kriesell | first3 = Matthias\n | last4 = Pendavingh | first4 = Rudi\n | last5 = Wollan | first5 = Paul\n | arxiv = 1003.3919\n | doi = 10.1016/j.aim.2013.01.011\n | journal = Advances in Mathematics\n | mr = 3045140\n | pages = 18–46\n | title = Axioms for infinite matroids\n | volume = 239\n | year = 2013}}.\n*{{citation|last1=Bryant|first1=Victor|last2=Perfect|first2=Hazel|author2-link=Hazel Perfect|year=1980|title=Independence Theory in Combinatorics|publisher=Chapman and Hall|location=London and New York|isbn=978-0-412-22430-0}}.\n*{{citation|last=Brylawski|first=Thomas H.|year=1972|title=A decomposition for combinatorial geometries|journal=Transactions of the American Mathematical Society|volume=171|pages=235&ndash;282|doi=10.2307/1996381|jstor=1996381}}.\n*{{citation|last=Crapo|first=Henry H.|year=1969|title=The Tutte polynomial|journal=[[Aequationes Mathematicae]]|volume=3|issue=3|pages=211&ndash;229|doi=10.1007/BF01817442}}.\n*{{citation|last1=Crapo|first=Henry H.|last2=Rota|first2=Gian-Carlo|author2-link=Gian-Carlo Rota|year=1970|title=On the Foundations of Combinatorial Theory: Combinatorial Geometries|publisher=M.I.T. Press|location=Cambridge, Mass.|isbn=978-0-262-53016-3|mr=0290980}}.\n*{{citation|last1=Geelen|first1=Jim|last2=Gerards|first2=A. M. H.|last3=Whittle|first3=Geoff|year=2007|contribution=Towards a matroid-minor structure theory|editor=Grimmett, Geoffrey|title=Combinatorics, Complexity, and Chance: A Tribute to Dominic Welsh|series=Oxford Lecture Series in Mathematics and its Applications|volume=34|pages=72&ndash;82|publisher=Oxford University Press|location=Oxford|display-editors=etal}}.\n*{{citation|last=Gerards|first=A. M. H.|year=1989|title=A short proof of Tutte's characterization of totally unimodular matrices|journal=[[Linear Algebra and its Applications]]|volume=114/115|pages=207&ndash;212|doi=10.1016/0024-3795(89)90461-8}}.\n*{{citation|last1=Kahn|first1=Jeff|last2=Kung|first2=Joseph P. S.|year=1982|title=Varieties of combinatorial geometries|journal=Transactions of the American Mathematical Society|volume=271|pages=485&ndash;499|doi=10.2307/1998894|issue=2|jstor=1998894}}.\n*{{citation|last1=Kingan|first1=Robert|last2=Kingan|first2=Sandra | year=2005|contribution=A software system for matroids|title=Graphs and Discovery|series=DIMACS Series in Discrete Mathematics and Theoretical Computer Science|pages=287&ndash;296}}.\n*{{citation|editor-last=Kung|editor-first=Joseph P. S.|title=A Source Book in Matroid Theory|publisher=Birkhäuser|isbn=978-0-8176-3173-4|location=Boston|year=1986|mr=0890330|doi=10.1007/978-1-4684-9199-9}}.\n*{{citation|last=Mac Lane|first=Saunders|authorlink=Saunders Mac Lane|year=1936|title=Some interpretations of abstract linear dependence in terms of projective geometry|journal=American Journal of Mathematics|volume=58|pages=236–240|doi=10.2307/2371070|issue=1|jstor=2371070}}.\n*{{citation|last=Minty|first=George J.|title=On the axiomatic foundations of the theories of directed linear graphs, electrical networks and network-programming|journal=Journal of Mathematics and Mechanics|volume=15|year=1966|pages=485–520|mr=0188102}}.\n*{{citation|mr=2516551|zbl=1163.01001|title=A lost mathematician, Takeo Nakasawa. The forgotten father of matroid theory|editor-first=Hirokazu |editor-last=Nishimura |editor2-first=Susumu |editor2-last=Kuroda|publisher= Birkhäuser Verlag|place= Basel|year= 2009|isbn= 978-3-7643-8572-9|url=http://www.springerlink.com/content/978-3-7643-8572-9 |doi=10.1007/978-3-7643-8573-6}}.\n*{{citation|last=Oxley|first=James | authorlink = James Oxley|year=1992|title=Matroid Theory|publisher=Oxford University Press|location=Oxford|isbn=978-0-19-853563-8|mr=1207587|zbl=0784.05002}}.\n*{{citation|last=Recski|first=András|year=1989|title=Matroid Theory and its Applications in Electric Network Theory and in Statics|volume=6|publisher=Springer-Verlag and Akademiai Kiado|location=Berlin and Budapest|isbn=978-3-540-15285-9|mr=1027839|doi=10.1007/978-3-662-22143-3|series=Algorithms and Combinatorics}}.\n*{{eom|id=M/m062870|first=A.A.|last= Sapozhenko}}\n*{{citation|last=Seymour|first=Paul D.|authorlink=Paul Seymour (mathematician)|year=1980|title=Decomposition of regular matroids|journal=Journal of Combinatorial Theory, Series B|volume=28|issue=3|pages=305&ndash;359|doi=10.1016/0095-8956(80)90075-1|zbl=0443.05027}}.\n*{{citation|last=Truemper|first=Klaus|title=Matroid Decomposition|publisher=Academic Press|location=Boston|year=1992|isbn=978-0-12-701225-4|url=http://www.emis.de/monographs/md/index.html|mr=1170126}}.\n*{{citation|last=Tutte|first=W. T.|authorlink=W. T. Tutte|year=1959|title=Matroids and graphs|journal=Transactions of the American Mathematical Society|volume=90|pages=527–552|doi=10.2307/1993185|issue=3|mr=0101527|jstor=1993185}}.\n*{{citation|last=Tutte|first=W. T.|authorlink=W. T. Tutte|year=1965|title=Lectures on matroids|journal=Journal of Research of the National Bureau of Standards Section B|volume=69|pages=1&ndash;47}}.\n*{{citation | zbl=0231.05027 | last=Tutte | first=W.T. | authorlink=W. T. Tutte | title=Introduction to the theory of matroids | series=Modern Analytic and Computational Methods in Science and Mathematics | volume=37 | location=New York | publisher=American Elsevier Publishing Company | year=1971 }}.\n*{{citation|last=Vámos|first=Peter|year=1978|title=The missing axiom of matroid theory is lost forever|journal=Journal of the London Mathematical Society|volume=18|pages=403–408|doi=10.1112/jlms/s2-18.3.403|issue=3}}.\n*{{citation|last=van der Waerden|first=B. L.|authorlink=Bartel Leendert van der Waerden|year=1937|title=Moderne Algebra}}.\n*{{citation|last=Welsh|first=D. J. A.|year=1976|title=Matroid Theory|publisher=Academic Press|isbn=978-0-12-744050-7|zbl=0343.05002|series=L.M.S. Monographs | volume=8}}.\n*{{citation|editor-last=White|editor-first=Neil|year=1986|title=Theory of Matroids|series=Encyclopedia of Mathematics and its Applications|volume=26|publisher=Cambridge University Press|location=Cambridge|isbn=978-0-521-30937-0 | zbl=0579.00001 }}.\n*{{citation | editor-last=White | editor-first=Neil | title=Combinatorial geometries | series=Encyclopedia of Mathematics and its Applications | volume=29 | location=Cambridge | publisher=[[Cambridge University Press]] | year=1987 | isbn=978-0-521-33339-9 | zbl=0626.00007 }}\n*{{citation|editor-last=White|editor-first=Neil|year=1992|title=Matroid Applications|series=Encyclopedia of Mathematics and its Applications|volume=40|publisher=Cambridge University Press|location=Cambridge|isbn=978-0-521-38165-9 | zbl=0742.00052 }}.\n*{{citation|last=Whitney|first=Hassler|authorlink=Hassler Whitney|year=1935|title=On the abstract properties of linear dependence|journal=American Journal of Mathematics|volume=57|pages=509–533|doi=10.2307/2371182|issue=3|mr=1507091|jstor=2371182}}. Reprinted in {{harvtxt|Kung|1986}}, pp.&nbsp;55–79.\n*{{citation|last=Whittle|first=Geoff|year=1995|title=A characterization of the matroids representable over ''GF''(3) and the rationals|journal=Journal of Combinatorial Theory, Series B|volume=65|issue=2|pages=222&ndash;261|url=http://eprints.kfupm.edu.sa/39296/1/39296.pdf|doi=10.1006/jctb.1995.1052}}{{dead link|date=March 2018 |bot=InternetArchiveBot |fix-attempted=yes }}.\n\n== External links ==\n\n* {{springer|title=Matroid|id=p/m062870}}\n* Kingan, Sandra : [http://userhome.brooklyn.cuny.edu/skingan/matroids/ Matroid theory]. A large bibliography of matroid papers, matroid software, and links.\n* Locke, S. C. : [http://euler.math.fau.edu/locke/Greedy.htm Greedy Algorithms].\n* Pagano, Steven R. : [http://www.math.binghamton.edu/zaslav/Pagano/Matridx.htm Matroids and Signed Graphs].\n* Mark Hubenthal: [https://web.archive.org/web/20100812232232/http://www.math.washington.edu/~hubenjm/matroid2.pdf A Brief Look At Matroids] ([[pdf]]) (contain proofs for statements of this article)\n* James Oxley : [https://www.math.lsu.edu/~oxley/survey4.pdf What is a matroid?] (pdf)\n* Neil White : [https://books.google.com/books?id=uD2H-RAcBpwC&lpg=PA285&ots=JL6z3p--j8&dq=greedoid%20theory&pg=PP1#v=onepage&q=greedoid%20theory&f=false Matroid Applications]\n\n[[Category:Matroid theory| ]]\n[[Category:Closure operators]]\n[[Category:Set families]]"
    },
    {
      "title": "Algebraic matroid",
      "url": "https://en.wikipedia.org/wiki/Algebraic_matroid",
      "text": "In mathematics, an '''algebraic matroid''' is a [[matroid]], a combinatorial structure, which expresses an abstraction of the relation of [[algebraic independence]].\n\n==Definition==\nGiven a [[field extension]] ''L''/''K'', [[Zorn's lemma]] can be used to show that there always exists a maximal algebraically independent subset of ''L'' over ''K''. Further, all the maximal algebraically independent subsets have the same [[cardinality]], known as the [[transcendence degree]] of the extension.\n\nFor every finite set ''S'' of elements of ''L'', the algebraically independent subsets of ''S'' satisfy the axioms that define the independent sets of a [[matroid]]. In this matroid, the rank of a set of elements is its transcendence degree, and the flat generated by a set ''T'' of elements is the intersection of ''L'' with the field ''K''[''T''].<ref name=Ox216>Oxley (1992) p.216</ref>  A matroid that can be generated in this way is called ''algebraic'' or ''algebraically representable''.<ref name=Ox218>Oxley (1992) p.218</ref>  No good characterization of algebraic matroids is known,<ref name=Ox215>Oxley (1992) p.215</ref> but certain matroids are known to be non-algebraic; the smallest is the [[Vámos matroid]].<ref>{{cite journal | last1 = Ingleton | first1 = A. W. | last2 = Main | first2 = R. A. | doi = 10.1112/blms/7.2.144 | journal = [[Bulletin of the London Mathematical Society]] | mr = 0369110 | zbl=0315.05018  | pages = 144–146 | title = Non-algebraic matroids exist | volume = 7 | year = 1975}}.</ref><ref name=Ox221/>\n\n==Relation to linear matroids==\nMany finite matroids may be [[Matroid representation|represented]] by a [[matrix (mathematics)|matrix]] over a field ''K'', in which the matroid elements correspond to matrix columns, and a set of elements is independent if the corresponding set of columns is [[linear independence|linearly independent]]. Every matroid with a linear representation of this type over a field ''F'' may also be represented as an algebraic matroid over ''F'',<ref name=Ox220>Oxley (1992) p.220</ref><ref name=Wh8724>White (1987) p.24</ref> by choosing an [[Indeterminate (variable)|indeterminate]] for each row of the matrix, and by using the matrix coefficients within each column to assign each matroid element a linear combination of these transcendentals.  For fields of characteristic zero (such as the real numbers) linear and algebraic matroids coincide, but for other fields there may exist algebraic matroids that are not linear;<ref>{{cite book | last = Ingleton | first = A. W. | chapter = Representation of matroids | location = London | mr = 0278974 | zbl=0222.05025 | pages = 149–167 | publisher = Academic Press | title = Combinatorial Mathematics and its Applications (Proc. Conf., Oxford, 1969) | year = 1971}}</ref><ref>{{citation|title=Applied Discrete Structures|first=K. D.|last=Joshi|publisher=New Age International|year=1997|isbn=9788122408263|page=909}}.</ref> indeed the non-Pappus matroid is algebraic over any finite field, but not linear and not algebraic over any field of characteristic zero.<ref name=Wh8724/>  However, if a matroid is algebraic over a field ''F'' of characteristic zero then it is linear over ''F''(''T'') for some finite set of transcendentals ''T'' over ''F''<ref name=Ox221>Oxley (1992) p.221</ref> and over the [[algebraic closure]] of ''F''.<ref name=Wh8724/>\n\n==Closure properties==\nIf a matroid is algebraic over a [[simple extension]] ''F''(''t'') then it is algebraic over ''F''.  It follows that the class of algebraic matroids is closed under [[Matroid minor|contraction]],<ref name=Ox222>Oxley (1992) p.222</ref> and that a matroid algebraic over ''F'' is algebraic over the [[prime field]] of ''F''.<ref name=Ox224>Oxley (1992) p.224</ref>\n\nThe class of algebraic matroids is closed under truncation and matroid union.<ref name=Wh8725/>  It is not known whether the [[matroid dual|dual]] of an algebraic matroid is always algebraic<ref name=Ox223>Oxley (1992) p.223</ref> and there is no excluded minor characterisation of the class.<ref name=Wh8725/>\n\n==Characteristic set==\nThe '''(algebraic) characteristic set''' ''K''(''M'') of a matroid ''M'' is the set of possible [[Characteristic (algebra)|characteristics]] of fields over which ''M'' is algebraically representable.<ref name=Wh8724/>\n\n* If 0 is in ''K''(''M'') then all sufficiently large primes are in ''K''(''M'').<ref name=Wh8724/>\n* Every prime occurs as the unique characteristic for some matroid.<ref name=Wh8724/><ref name=Lind85c>{{cite journal | last=Lindström | first=Bernt | title=On the algebraic characteristic set for a class of matroids | journal=[[Proceedings of the American Mathematical Society]] | volume=95 | pages=147–151 | year=1985 | zbl=0572.05019 | jstor=2045591 | doi=10.2307/2045591}}</ref>\n* If ''M'' is algebraic over ''F'' then any contraction of ''M'' is algebraic over ''F'' and hence so is any minor of ''M''.<ref name=Wh8725>White (1987) p.25</ref>\n\n==Notes==\n{{reflist|30em}}\n\n==References==\n*{{cite book | last=Oxley | first=James G. | title=Matroid theory | series=Oxford Science Publications | location=Oxford | publisher=[[Oxford University Press]] | year=1992 | isbn=0-19-853563-5 | zbl=0784.05002 }}\n*{{cite book | last = Welsh | first = D. J. A. | authorlink = Dominic Welsh | isbn = 9780486474397 | publisher = Courier Dover Publications | title = Matroid Theory | year = 2010 | origyear=1976 }}\n*{{citation | editor-last=White | editor-first=Neil | title=Combinatorial geometries | series=Encyclopedia of Mathematics and its Applications | volume=29 | location=Cambridge | publisher=[[Cambridge University Press]] | year=1987 | isbn=0-521-33339-3 | zbl=0626.00007 }}\n\n[[Category:Matroid theory]]"
    },
    {
      "title": "Antimatroid",
      "url": "https://en.wikipedia.org/wiki/Antimatroid",
      "text": "[[Image:Antimatroid.svg|thumb|360px|Three views of an antimatroid: an inclusion ordering on its family of feasible sets, a formal language, and the corresponding path poset.]]\nIn [[mathematics]], an '''antimatroid''' is a [[formal system]] that describes processes in which a [[set (mathematics)|set]] is built up by including elements one at a time, and in which an element, once available for inclusion, remains available until it is included. Antimatroids are commonly [[Cryptomorphism|axiomatized in two equivalent ways]], either as a [[set system]] modeling the possible states of such a process, or as a [[formal language]] modeling the different sequences in which elements may be included.\n[[Robert P. Dilworth|Dilworth]] (1940) was the first to study antimatroids, using yet another axiomatization based on [[lattice (order)|lattice theory]], and they have been frequently rediscovered in other contexts;<ref>Two early references are {{harvtxt|Edelman|1980}} and {{harvtxt|Jamison|1980}}; Jamison was the first to use the term \"antimatroid\". {{harvtxt|Monjardet|1985}} surveys the history of rediscovery of antimatroids.</ref> see Korte et al. (1991) for a comprehensive survey of antimatroid theory with many additional references.\n\nThe axioms defining antimatroids as set systems are very similar to those of [[matroid]]s, but whereas matroids are defined by an ''[[Matroid#Independent sets, bases, and circuits|exchange axiom]]'' (e.g., the ''basis exchange'', or ''independent set exchange'' axioms), antimatroids are defined instead by an ''[[#Convex geometries|anti-exchange axiom]]'', from which their name derives.\nAntimatroids can be viewed as a special case of [[greedoid]]s and of [[semimodular lattice]]s, and as a generalization of [[partial order]]s and of [[distributive lattice]]s. \nAntimatroids are equivalent, by [[complement (set theory)|complementation]], to '''[[#Convex geometries|convex geometries]]''', a combinatorial abstraction of [[convex set]]s in [[geometry]].\n\nAntimatroids have been applied to model precedence constraints in [[Job shop scheduling|scheduling problems]], potential event sequences in simulations, task planning in [[artificial intelligence]], and the states of knowledge of human learners.\n\n== Definitions ==\nAn antimatroid can be defined as a finite family ''F'' of sets, called ''feasible sets'', with the following two properties:\n* The [[Union (set theory)|union]] of any two feasible sets is also feasible. That is, ''F'' is [[Closure (mathematics)|closed]] under unions.\n* If ''S'' is a nonempty feasible set, then there exists some ''x'' in ''S'' such that ''S'' \\ {''x''} (the set formed by removing ''x'' from ''S'') is also feasible. That is, ''F'' is an [[accessible set system]].\n\nAntimatroids also have an equivalent definition as a [[formal language]], that is, as a set of [[String (computer science)|strings]] defined from a finite alphabet of [[symbol]]s. A language ''L'' defining an antimatroid must satisfy the following properties:\n* Every symbol of the alphabet occurs in at least one word of ''L''.\n* Each word of ''L'' contains at most one copy of any symbol.\n* Every [[Prefix (computer science)|prefix]] of a string in ''L'' is also in ''L''.\n* If ''s'' and ''t'' are strings in ''L'', and ''s'' contains at least one symbol that is not in ''t'', then there is a symbol ''x'' in ''s'' such that ''tx'' is another string in ''L''.\n\nIf ''L'' is an antimatroid defined as a formal language, then the sets of symbols in strings of ''L'' form an accessible union-closed set system. In the other direction, if ''F'' is an accessible union-closed set system, and ''L'' is the language of strings ''s'' with the property that the set of symbols in each prefix of ''s'' is feasible, then ''L'' defines an antimatroid. Thus, these two definitions lead to mathematically equivalent classes of objects.<ref>Korte et al., Theorem 1.4.</ref>\n\n==Examples==\n[[Image:Convex shelling.svg|thumb|300px|A shelling sequence of a planar point set. The line segments show edges of the [[convex hull]]s after some of the points have been removed.]]\n\n*A ''chain antimatroid'' has as its formal language the prefixes of a single word, and as its feasible sets the sets of symbols in these prefixes. For instance the chain antimatroid defined by the word \"abcd\" has as its formal language the strings {ε, \"a\", \"ab\", \"abc\", \"abcd\"} and as its feasible sets the sets Ø, {a}, {a,b}, {a,b,c}, and {a,b,c,d}.<ref name=\"Gordon (1997)\"/>\n*A ''poset antimatroid'' has as its feasible sets the [[lower set]]s of a finite [[partially ordered set]]. By [[Birkhoff's representation theorem]] for distributive lattices, the feasible sets in a poset antimatroid (ordered by set inclusion) form a distributive lattice, and any distributive lattice can be formed in this way. Thus, antimatroids can be seen as generalizations of distributive lattices. A chain antimatroid is the special case of a poset antimatroid for a [[total order]].<ref name=\"Gordon (1997)\"/>\n*A ''shelling sequence'' of a finite set ''U'' of points in the [[Euclidean plane]] or a higher-dimensional [[Euclidean space]] is an ordering on the points such that, for each point ''p'', there is a [[Line (geometry)|line]] (in the Euclidean plane, or a [[hyperplane]] in a Euclidean space) that separates ''p'' from all later points in the sequence. Equivalently, ''p'' must be a vertex of the [[convex hull]] of it and all later points. The partial shelling sequences of a point set form an antimatroid, called a ''shelling antimatroid''. The feasible sets of the shelling antimatroid are the [[Intersection (set theory)|intersection]]s of ''U'' with the [[complement (set theory)|complement]] of a convex set.<ref name=\"Gordon (1997)\"/> Every antimatroid is isomorphic to a shelling antimatroid of points in a sufficiently high-dimensional space.{{sfnp|Kashiwabara|Nakamura|Okamoto|2005}}\n*A ''[[perfect elimination ordering]]'' of a [[chordal graph]] is an ordering of its vertices such that, for each vertex ''v'', the neighbors of ''v'' that occur later than ''v'' in the ordering form a [[clique (graph theory)|clique]]. The prefixes of perfect elimination orderings of a chordal graph form an antimatroid.<ref name=\"Gordon (1997)\">Gordon (1997) describes several results related to antimatroids of this type, but these antimatroids were mentioned earlier e.g. by Korte et al. Chandran et al. (2003) use the connection to antimatroids as part of an algorithm for efficiently listing all perfect elimination orderings of a given chordal graph.</ref> Antimatroids also describe some other kinds of vertex removal orderings in graphs, such as the dismantling orders of [[cop-win graph]]s.\n\n==Paths and basic words==\nIn the set theoretic axiomatization of an antimatroid there are certain special sets called ''paths'' that determine the whole antimatroid, in the sense that the sets of the antimatroid are exactly the unions of paths. If ''S'' is any feasible set of the antimatroid, an element ''x'' that can be removed from ''S'' to form another feasible set is called an ''endpoint'' of ''S'', and a feasible set that has only one endpoint is called a ''path'' of the antimatroid. The family of paths can be partially ordered by set inclusion, forming the ''path poset'' of the antimatroid.\n\nFor every feasible set ''S'' in the antimatroid, and every element ''x'' of ''S'', one may find a path subset of ''S'' for which ''x'' is an endpoint: to do so, remove one at a time elements other than ''x'' until no such removal leaves a feasible subset. Therefore, each feasible set in an antimatroid is the union of its path subsets. If ''S'' is not a path, each subset in this union is a [[proper subset]] of ''S''. But, if ''S'' is itself a path with endpoint ''x'', each proper subset of ''S'' that belongs to the antimatroid excludes ''x''. Therefore, the paths of an antimatroid are exactly the sets that do not equal the unions of their proper subsets in the antimatroid. Equivalently, a given family of sets ''P'' forms the set of paths of an antimatroid if and only if, for each ''S'' in ''P'', the union of subsets of ''S'' in ''P'' has one fewer element than ''S'' itself. If so, ''F'' itself is the family of unions of subsets of ''P''.\n\nIn the formal language formalization of an antimatroid we may also identify a subset of words that determine the whole language, the ''basic words''.\nThe longest strings in ''L'' are called ''basic words''; each basic word forms a permutation of the whole alphabet. For instance, the basic words of a poset antimatroid are the [[linear extension]]s of the given partial order. If ''B'' is the set of basic words, ''L'' can be defined from ''B'' as the set of prefixes of words in ''B''. It is often convenient to define antimatroids from basic words in this way, but it is not straightforward to write an axiomatic definition of antimatroids in terms of their basic words.\n\n==Convex geometries==\n{{See also|Convex set|Convex geometry|Closure operator}}\nIf ''F'' is the set system defining an antimatroid, with ''U'' equal to the union of the sets in ''F'', then the family of sets\n:<math>G = \\{U\\setminus S\\mid S\\in F\\}</math>\n[[Complement (set theory)|complementary]] to the sets in ''F'' is sometimes called a '''convex geometry''' and the sets in ''G'' are called '''convex sets'''. For instance, in a shelling antimatroid, the convex sets are intersections of ''U'' with convex subsets of the Euclidean space into which ''U'' is embedded.\n\nComplementarily to the properties of set systems that define antimatroids, the set system defining a convex geometry should be closed under intersections, and for any set ''S'' in ''G'' that is not equal to ''U'' there must be an element ''x'' not in ''S'' that can be added to ''S'' to form another set in ''G''.\n\nA convex geometry can also be defined in terms of a [[closure operator]] τ that maps any subset of ''U'' to its minimal closed superset. To be a closure operator, τ should have the following properties:\n* τ(∅) = ∅: the closure of the [[empty set]] is empty.\n* Any set ''S'' is a subset of τ(''S'').\n* If ''S'' is a subset of ''T'', then τ(''S'') must be a subset of τ(''T'').\n* For any set ''S'', τ(''S'') = τ(τ(''S'')).\nThe family of closed sets resulting from a closure operation of this type is necessarily closed under intersections. The closure operators that define convex geometries also satisfy an additional '''anti-exchange axiom''':\n*If neither ''y'' nor ''z'' belong to τ(''S''), but ''z'' belongs to τ(''S'' ∪ {''y''}), then ''y'' does not belong to τ(''S'' ∪ {''z''}).\nA closure operation satisfying this axiom is called an '''anti-exchange closure'''. If ''S'' is a closed set in an anti-exchange closure, then the anti-exchange axiom determines a partial order on the elements not belonging to ''S'', where ''x'' ≤ ''y'' in the partial order when ''x'' belongs to τ(''S'' ∪ {''y''}). If ''x'' is a minimal element of this partial order, then ''S'' ∪ {''x''} is closed. That is, the family of closed sets of an anti-exchange closure has the property that for any set other than the universal set there is an element ''x'' that can be added to it to produce another closed set. This property is complementary to the accessibility property of antimatroids, and the fact that intersections of closed sets are closed is complementary to the property that unions of feasible sets in an antimatroid are feasible. Therefore, the complements of the closed sets of any anti-exchange closure form an antimatroid.<ref>Korte et al., Theorem 1.1.</ref>\n\nThe [[undirected graph]]s in which the convex sets (subsets of vertices that contain all [[shortest path]]s between vertices in the subset) form a convex geometry are exactly the [[Ptolemaic graph]]s.{{sfnp|Farber|Jamison|1986}}\n\n==Join-distributive lattices==\nAny two sets in an antimatroid have a unique [[least upper bound]] (their union) and a unique [[greatest lower bound]] (the union of the sets in the antimatroid that are contained in both of them). Therefore, the sets of an antimatroid, [[partial order|partially ordered]] by set inclusion, form a [[Lattice (order)|lattice]]. Various important features of an antimatroid can be interpreted in lattice-theoretic terms; for instance the paths of an antimatroid are the [[Lattice (order)#Important lattice-theoretic notions|join-irreducible]] elements of the corresponding lattice, and the basic words of the antimatroid correspond to [[maximal chain]]s in the lattice. The lattices that arise from antimatroids in this way generalize the finite [[distributive lattice]]s, and can be characterized in several different ways.\n\n*The description originally considered by {{harvtxt|Dilworth|1940}} concerns [[Lattice (order)#Important lattice-theoretic notions|meet-irreducible]] elements of the lattice. For each element ''x'' of an antimatroid, there exists a unique maximal feasible set ''S<sub>x</sub>'' that does not contain ''x'' (''S<sub>x</sub>'' is the union of all feasible sets not containing ''x''). ''S<sub>x</sub>'' is meet-irreducible, meaning that it is not the meet of any two larger lattice elements: any larger feasible set, and any intersection of larger feasible sets, contains ''x'' and so does not equal ''S<sub>x</sub>''. Any element of any lattice can be decomposed as a meet of meet-irreducible sets, often in multiple ways, but in the lattice corresponding to an antimatroid each element ''T'' has a unique minimal family of meet-irreducible sets ''S<sub>x</sub>'' whose meet is ''T''; this family consists of the sets ''S<sub>x</sub>'' such that ''T''&nbsp;∪&nbsp;{''x''} belongs to the antimatroid. That is, the lattice has ''unique meet-irreducible decompositions''.\n*A second characterization concerns the ''intervals'' in the lattice, the sublattices defined by a pair of lattice elements ''x''&nbsp;≤&nbsp;''y'' and consisting of all lattice elements ''z'' with ''x''&nbsp;≤&nbsp;''z''&nbsp;≤&nbsp;''y''. An interval is [[Atom (order theory)|atomistic]] if every element in it is the join of atoms (the minimal elements above the bottom element ''x''), and it is [[Boolean algebra (structure)|Boolean]] if it is isomorphic to the lattice of [[power set|all subsets]] of a finite set. For an antimatroid, every interval that is atomistic is also boolean.\n*Thirdly, the lattices arising from antimatroids are [[semimodular lattice]]s, lattices that satisfy the [[Semimodular lattice|''upper semimodular law'']] that for any two elements ''x'' and ''y'', if ''y'' covers ''x''&nbsp;∧&nbsp;''y'' then ''x''&nbsp;∨&nbsp;''y'' covers ''x''. Translating this condition into the sets of an antimatroid, if a set ''Y'' has only one element not belonging to ''X'' then that one element may be added to ''X'' to form another set in the antimatroid. Additionally, the lattice of an antimatroid has the ''meet-semidistributive property'': for all lattice elements ''x'', ''y'', and ''z'', if ''x''&nbsp;∧&nbsp;''y'' and ''x''&nbsp;∧&nbsp;''z'' are both equal then they also equal ''x''&nbsp;∧&nbsp;(''y''&nbsp;∨&nbsp;''z''). A semimodular and meet-semidistributive lattice is called a ''join-distributive lattice''.\n\nThese three characterizations are equivalent: any lattice with unique meet-irreducible decompositions has boolean atomistic intervals and is join-distributive, any lattice with boolean atomistic intervals has unique meet-irreducible decompositions and is join-distributive, and any join-distributive lattice has unique meet-irreducible decompositions and boolean atomistic intervals.<ref>{{harvtxt|Adaricheva|Gorbunov|Tumanov|2003}}, Theorems 1.7 and 1.9; {{harvtxt|Armstrong|2007}}, Theorem 2.7.</ref> Thus, we may refer to a lattice with any of these three properties as join-distributive. Any antimatroid gives rise to a finite join-distributive lattice, and any finite join-distributive lattice comes from an antimatroid in this way.<ref>{{harvtxt|Edelman|1980}}, Theorem 3.3; {{harvtxt|Armstrong|2007}}, Theorem 2.8.</ref> Another equivalent characterization of finite join-distributive lattices is that they are [[graded poset|graded]] (any two maximal chains have the same length), and the length of a maximal chain equals the number of meet-irreducible elements of the lattice.<ref>{{harvtxt|Monjardet|1985}} credits a dual form of this characterization to several papers from the 1960s by S. P. Avann.</ref> The antimatroid representing a finite join-distributive lattice can be recovered from the lattice: the elements of the antimatroid can be taken to be the meet-irreducible elements of the lattice, and the feasible set corresponding to any element ''x'' of the lattice consists of the set of meet-irreducible elements ''y'' such that ''y'' is not greater than or equal to ''x'' in the lattice.\n\nThis representation of any finite join-distributive lattice as an accessible family of sets closed under unions (that is, as an antimatroid) may be viewed as an analogue of [[Birkhoff's representation theorem]] under which any finite distributive lattice has a representation as a family of sets closed under unions and intersections.\n\n==Supersolvable antimatroids==\nMotivated by a problem of defining partial orders on the elements of a [[Coxeter group]], {{harvtxt|Armstrong|2007}} studied antimatroids which are also supersolvable lattices. A supersolvable antimatroid is defined by a [[Total order|totally ordered]] collection of elements, and a [[family of sets]] of these elements. The family must include the empty set. Additionally, it must have the property that if two sets ''A'' and ''B'' belong to the family, the [[set-theoretic difference]] ''B''&nbsp;\\&nbsp;''A'' is nonempty, and ''x'' is the smallest element of ''B''&nbsp;\\&nbsp;''A'', then ''A''&nbsp;∪&nbsp;{''x''} also belongs to the family. As Armstrong observes, any family of sets of this type forms an antimatroid. Armstrong also provides a lattice-theoretic characterization of the antimatroids that this construction can form.\n\n==Join operation and convex dimension==\nIf ''A'' and ''B'' are two antimatroids, both described as a family of sets, and if the maximal sets in ''A'' and ''B'' are equal, we can form another antimatroid, the ''join'' of ''A'' and ''B'', as follows:\n\n:<math>A\\vee B = \\{ S\\cup T \\mid S\\in A \\wedge T\\in B \\}.</math>\n\nThis is a different operation than the join considered in the lattice-theoretic characterizations of antimatroids: it combines two antimatroids to form another antimatroid, rather than combining two sets in an antimatroid to form another set.\nThe family of all antimatroids that have a given maximal set forms a [[semilattice]] with this join operation.\n\nJoins are closely related to a closure operation that maps formal languages to antimatroids, where the closure of a language ''L'' is the intersection of all antimatroids containing ''L'' as a sublanguage. This closure has as its feasible sets the unions of prefixes of strings in ''L''. In terms of this closure operation, the join is the closure of the union of the languages of ''A'' and ''B''.\n\nEvery antimatroid can be represented as a join of a family of chain antimatroids, or equivalently as the closure of a set of basic words; the ''convex dimension'' of an antimatroid ''A'' is the minimum number of chain antimatroids (or equivalently the minimum number of basic words) in such a representation. If ''F'' is a family of chain antimatroids whose basic words all belong to ''A'', then ''F'' generates ''A'' if and only if the feasible sets of ''F'' include all paths of ''A''. The paths of ''A'' belonging to a single chain antimatroid must form a [[chain (order theory)|chain]] in the path poset of ''A'', so the convex dimension of an antimatroid equals the minimum number of chains needed to cover the path poset, which by [[Dilworth's theorem]] equals the width of the path poset.<ref>{{harvtxt|Edelman|Saks|1988}}; Korte et al., Theorem 6.9.</ref>\n\nIf one has a representation of an antimatroid as the closure of a set of ''d'' basic words, then this representation can be used to map the feasible sets of the antimatroid into ''d''-dimensional Euclidean space: assign one coordinate per basic word ''w'', and make the coordinate value of a feasible set ''S'' be the length of the longest prefix of ''w'' that is a subset of ''S''. With this embedding, ''S'' is a subset of ''T'' if and only if the coordinates for ''S'' are all less than or equal to the corresponding coordinates of ''T''. Therefore, the [[order dimension]] of the inclusion ordering of the feasible sets is at most equal to the convex dimension of the antimatroid.<ref>Korte et al., Corollary 6.10.</ref> However, in general these two dimensions may be very different: there exist antimatroids with order dimension three but with arbitrarily large convex dimension.\n\n==Enumeration==\nThe number of possible antimatroids on a set of elements grows rapidly with the number of elements in the set. For sets of one, two, three, etc. elements, the number of distinct antimatroids is\n:1, 3, 22, 485, 59386, 133059751, ... {{OEIS|id=A119770}}.\n\n==Applications==\nBoth the precedence and release time constraints in the standard [[notation for theoretic scheduling problems]] may be modeled by antimatroids. {{harvtxt|Boyd|Faigle|1990}} use antimatroids to generalize a [[greedy algorithm]] of [[Eugene Lawler]] for optimally solving single-processor scheduling problems with precedence constraints in which the goal is to minimize the maximum penalty incurred by the late scheduling of a task.\n\n{{harvtxt|Glasserman|Yao|1994}} use antimatroids to model the ordering of events in [[discrete event simulation]] systems.\n\n{{harvtxt|Parmar|2003}} uses antimatroids to model progress towards a goal in [[artificial intelligence]] [[Automated planning and scheduling|planning]] problems.\n\nIn [[Optimality Theory]], grammars are logically equivalent to antimatroids ({{harvtxt|Merchant|Riggle|2016}}).\n\nIn [[mathematical psychology]], antimatroids have been used to describe [[knowledge space|feasible states of knowledge]] of a human learner. Each element of the antimatroid represents a concept that is to be understood by the learner, or a class of problems that he or she might be able to solve correctly, and the sets of elements that form the antimatroid represent possible sets of concepts that could be understood by a single person. The axioms defining an antimatroid may be phrased informally as stating that learning one concept can never prevent the learner from learning another concept, and that any feasible state of knowledge can be reached by learning a single concept at a time. The task of a knowledge assessment system is to infer the set of concepts known by a given learner by analyzing his or her responses to a small and well-chosen set of problems. In this context antimatroids have also been called \"learning spaces\" and \"well-graded knowledge spaces\".<ref>{{harvtxt|Doignon|Falmagne|1999}}.</ref>\n\n==Notes==\n{{reflist|2}}\n\n==References==\n{{refbegin|2}}\n*{{citation\n | last1 = Adaricheva | first1 = K. V.\n | last2 = Gorbunov | first2 = V. A.\n | last3 = Tumanov | first3 = V. I.\n | doi = 10.1016/S0001-8708(02)00011-7\n | issue = 1\n | journal = Advances in Mathematics\n | pages = 1–49\n | title = Join-semidistributive lattices and convex geometries\n | volume = 173\n | year = 2003}}.\n*{{citation\n | last = Armstrong | first = Drew\n | title = The sorting order on a Coxeter group\n | year = 2007\n | arxiv = 0712.1047| bibcode = 2007arXiv0712.1047A}}.\n*{{citation\n | last1 = Birkhoff | first1 = Garrett | author1-link = Garrett Birkhoff\n | last2 = Bennett | first2 = M. K.\n | doi = 10.1007/BF00333128\n | issue = 3\n | journal = [[Order (journal)|Order]]\n | pages = 223–242\n | title = The convexity lattice of a poset\n | volume = 2\n | year = 1985| doi-broken-date = 2019-06-06 }}.\n* {{Citation|last1=Björner|first1=Anders|last2=Ziegler|first2=Günter M.|authorlink2=Günter M. Ziegler|authorlink1=Anders Björner|chapter=Introduction to greedoids|series=Encyclopedia of Mathematics and its Applications|volume=40|editor-last=White|editor-first=Neil|publisher=Cambridge University Press|location=Cambridge|year=1992|isbn=0-521-38165-7|pages=284–357|doi=10.1017/CBO9780511662041.009|ref=harv|mr=1165537|title=Matroid Applications}}\n*{{citation\n | last1 = Boyd | first1 = E. Andrew\n | last2 = Faigle | first2 = Ulrich\n | doi = 10.1016/0166-218X(90)90002-T\n | issue = 3\n | journal = Discrete Applied Mathematics\n | pages = 197–205\n | title = An algorithmic characterization of antimatroids\n | volume = 28\n | year = 1990}}.\n*{{citation\n |last1=Chandran \n |first1=L. S. \n |last2=Ibarra \n |first2=L. \n |last3=Ruskey \n |first3=F. \n |last4=Sawada \n |first4=J. \n |doi=10.1016/S0304-3975(03)00221-4 \n |journal=Theoretical Computer Science \n |pages=303–317 \n |title=Generating and characterizing the perfect elimination orderings of a chordal graph \n |url=http://skeeter.socs.uoguelph.ca/~sawada/papers/chordal.pdf\n |volume=307 \n |year=2003 \n |issue=2 \n}}\n*{{citation\n | last = Dilworth | first = Robert P. | author-link = Robert P. Dilworth\n | doi = 10.2307/1968857\n | journal = [[Annals of Mathematics]]\n | pages = 771–777\n | issue = 4\n | title = Lattices with unique irreducible decompositions\n | volume = 41\n | year = 1940\n | jstor = 1968857}}.\n*{{citation\n | last1 = Doignon\n | first1 = Jean-Paul\n | authorlink1 = Jean-Paul Doignon\n | last2 = Falmagne\n | first2 = Jean-Claude\n | authorlink2 = Jean-Claude Falmagne\n | title = Knowledge Spaces\n | year = 1999\n | publisher = Springer-Verlag\n | isbn = 3-540-64501-2}}.\n*{{citation\n | last = Edelman | first = Paul H.\n | doi = 10.1007/BF02482912\n | issue = 1\n | journal = Algebra Universalis\n | pages = 290–299\n | title = Meet-distributive lattices and the anti-exchange closure\n | volume = 10\n | year = 1980}}.\n*{{citation\n | last1 = Edelman | first1 = Paul H.\n | last2 = Saks | first2 = Michael E.\n | doi = 10.1007/BF00143895\n | issue = 1\n | journal = [[Order (journal)|Order]]\n | pages = 23–32\n | title = Combinatorial representation and convex dimension of convex geometries\n | volume = 5\n | year = 1988}}.\n*{{citation\n | last1 = Farber | first1 = Martin\n | last2 = Jamison | first2 = Robert E.\n | doi = 10.1137/0607049\n | issue = 3\n | journal =  SIAM Journal on Algebraic and Discrete Methods\n | mr = 844046\n | pages = 433–444\n | title = Convexity in graphs and hypergraphs\n | volume = 7\n | year = 1986| hdl = 10338.dmlcz/127659\n }}.\n*{{citation\n | last1 = Glasserman | first1 = Paul\n | last2 = Yao | first2 = David D.\n | isbn = 978-0-471-58041-6\n | publisher = Wiley Interscience\n | series = Wiley Series in Probability and Statistics\n | title = Monotone Structure in Discrete Event Systems\n | year = 1994}}.\n*{{citation\n | last = Gordon | first = Gary\n | issue = 1\n | journal = [[Electronic Journal of Combinatorics]]\n | page = Research Paper 13\n | title = A β invariant for greedoids and antimatroids\n | url = http://www.combinatorics.org/Volume_4/Abstracts/v4i1r13.html\n | volume = 4\n | year = 1997\n | mr = 1445628}}.\n*{{citation\n | last = Jamison | first = Robert\n | contribution = Copoints in antimatroids\n | series = Congressus Numerantium\n | pages = 535–544\n | title = Proceedings of the Eleventh Southeastern Conference on Combinatorics, Graph Theory and Computing (Florida Atlantic Univ., Boca Raton, Fla., 1980), Vol. II\n | volume = 29\n | year = 1980\n | mr = 608454}}.\n*{{citation\n | last1 = Kashiwabara | first1 = Kenji\n | last2 = Nakamura | first2 = Masataka\n | last3 = Okamoto | first3 = Yoshio\n | doi = 10.1016/j.comgeo.2004.05.001\n | issue = 2\n | journal = Computational Geometry\n | mr = 2107032\n | pages = 129–144\n | title = The affine representation theorem for abstract convex geometries\n | volume = 30\n | year = 2005}}.\n*{{citation\n | last1 = Korte | first1 = Bernhard| author1-link = Bernhard Korte\n | last2 = Lovász | first2 = László | author2-link = László Lovász\n | last3 = Schrader | first3 = Rainer\n | isbn = 3-540-18190-3\n | pages = 19–43\n | publisher = Springer-Verlag\n | title = Greedoids\n | year = 1991}}.\n\n*{{citation\n | last1 = Merchant | first1 = Nazarre\n | last2 = Riggle | first2 = Jason\n | pages = 241–269\n | volume = 34\n | title = OT grammars, beyond partial orders: ERC sets and antimatroids\n | url = http://roa.rutgers.edu/article/view/1226\n | doi = 10.1007/s11049-015-9297-5\n | journal = Nat Lang Linguist Theory\n | year = 2016}}.\n\n*{{citation\n | last = Monjardet | first = Bernard\n | doi = 10.1007/BF00582748\n | issue = 4\n | journal = [[Order (journal)|Order]]\n | pages = 415–417\n | title = A use for frequently rediscovering a concept\n | volume = 1\n | year = 1985}}.\n*{{citation\n | last = Parmar | first = Aarati\n | contribution = Some Mathematical Structures Underlying Efficient Planning\n | title = AAAI Spring Symposium on Logical Formalization of Commonsense Reasoning\n | url = http://www-formal.stanford.edu/aarati/papers/SS603AParmar.pdf\n | year = 2003}}.\n{{refend}}\n\n[[Category:Algebraic combinatorics]]\n[[Category:Lattice theory]]\n[[Category:Convex geometry]]\n[[Category:Formal languages]]\n[[Category:Set families]]\n[[Category:Matroid theory]]\n[[Category:Discrete mathematics]]"
    },
    {
      "title": "Basis (linear algebra)",
      "url": "https://en.wikipedia.org/wiki/Basis_%28linear_algebra%29",
      "text": "{{redirect|Basis vector|basis vector in the context of crystals|Crystal structure|a more general concept in physics|Frame of reference}}\n{{redirects here|Basis (mathematics)||Basis (disambiguation)#Mathematics{{!}}Basis}}\n[[File:3d two bases same vector.svg|130px|thumb|The same vector can be represented in two different bases (purple and red arrows).]]\nIn [[mathematics]], a set {{mvar|B}} of elements (vectors) in a [[vector space]] {{math|''V''}} is called a '''basis''', if every element of {{math|''V''}} may be written in a unique way as a (finite) [[linear combination]] of elements of {{mvar|B}}. The coefficients of this linear combination are referred to as '''components''' or '''coordinates''' on {{mvar|B}} of the vector. The elements of a basis are called '''{{visible anchor|basis vectors}}'''.\n\nEquivalently {{mvar|B}} is a basis if its elements are linearly independent and every element of {{mvar|V}} is a linear combination of elements of {{mvar|B}}.<ref>{{cite book |last=Halmos |first=Paul Richard |authorlink=Paul Halmos |year=1987 |title=Finite-Dimensional Vector Spaces |edition=4th |publisher=Springer |location=New York |url=https://books.google.com/books?id=mdWeEhA17scC&pg=PA10 |page=10 |isbn=978-0-387-90093-3 }}</ref>  In more general terms, a basis is a linearly independent [[spanning set]].\n\nA vector space can have several bases; however all the bases have the same number of elements, called the [[dimension (vector space)|dimension]] of the vector space.\n\n== Definition ==\nA '''basis''' {{math|''B''}} of a [[vector space]] {{math|''V''}} over a [[field (mathematics)|field]] {{math|''F''}} (such as the [[real numbers]] {{math|'''R'''}} or the [[complex number]]s {{math|'''C'''}}) is a [[Linear independence|linearly independent]] subset of {{math|''V''}} that [[linear span|span]]s {{mvar|''V''}}.\nThis means that a subset {{mvar|B}} of {{math|''V''}} is a basis if it satisfies the two following conditions:\n* the ''linear independence'' property:\n:: for every finite subset {{math|{''b''<sub>1</sub>, ..., ''b''<sub>''n''</sub>}{{void}}}} of {{mvar|B}} and every {{math|''a''<sub>1</sub>, ..., ''a''<sub>''n''</sub>}} in  {{math|''F''}}, if {{math|1=''a''<sub>1</sub>''b''<sub>1</sub> + ⋅⋅⋅ + ''a''<sub>''n''</sub>''b''<sub>''n''</sub> = 0}}, then necessarily {{math|1=''a''<sub>1</sub> = ⋅⋅⋅ = ''a''<sub>''n''</sub> = 0}};\n* the ''spanning'' property:\n:: for every (vector) {{math|''v''}} in {{math|''V''}}, it is possible to choose {{math|''v''<sub>1</sub>, ..., ''v''<sub>''n''</sub>}} in {{math|''F''}} and {{math|''b''<sub>1</sub>, ..., ''b''<sub>''n''</sub>}} in {{mvar|B}} such that {{math|1=''v'' = ''v''<sub>1</sub>''b''<sub>1</sub> + ⋅⋅⋅ + ''v''<sub>''n''</sub>''b''<sub>''n''</sub>}}.\n\nThe [[scalar (mathematics)|scalar]]s {{math|''v''<sub>''i''</sub>}} are called the coordinates of the vector {{math|''v''}} with respect to the basis {{math|''B''}}, and by the first property they are uniquely determined.\n\nA vector space that has a [[finite set|finite]] basis is called [[Dimension (vector space)|finite-dimensional]]. In this case, the subset {{math|{''b''<sub>1</sub>, ..., ''b''<sub>''n''</sub>}{{void}}}} that is considered (twice) in the above definition may be chosen as {{mvar|B}} itself.\n\nIt is often convenient or even necessary to have an [[sequence|ordering]] on the basis vectors, e.g. for discussing [[orientation (vector space)|orientation]], or when one considers the scalar coefficients of a vector with respect to a basis, without referring explicitly to the basis elements. In this case, the ordering is necessary for associating each coefficient to the corresponding basis element. This ordering can be done by numbering the basis elements. For example, when dealing with [[matrix (mathematics)|(''m'', ''n'')-matrices]], the {{math|(''i'', ''j'')th}} element (in the {{mvar|i}}th row and {{mvar|j}}th column) can be referred to the {{math|(''m''⋅(''j'' - 1) + ''i'')}}th element of a basis consisting of the (''m'', ''n'')-unit-matrices (varying column-indices before row-indices). For emphasizing that an order has been chosen, one speaks of an '''ordered basis''', which is therefore not simply an unstructured [[Set (mathematics)|set]], but e.g. a [[sequence]], or an [[indexed family]], or similar; see ''[[#Ordered bases and coordinates|Ordered bases and coordinates]]'' below.\n\n== Examples ==\n[[File:Basis graph (no label).svg|thumb|400px|This picture illustrates the [[standard basis]] in '''''R'''<sup>2</sup>''. The blue and orange vectors are the elements of the basis; the green vector can be given in terms of the basis vectors, and so is [[linearly dependent]] upon them.]]\n*The set [[exponentiation over sets|{{math|'''R'''<sup>2</sup>}}]] of the [[ordered pair]]s of [[real number]]s is a vector space for the component-wise addition\n::<math>(a, b) + (c, d) = (a + c, b+d),</math>\n:and scalar multiplication \n::<math>\\lambda (a,b) = (\\lambda a, \\lambda b),</math>\n:where <math>\\lambda</math> is any real number. A simple basis of this vector space, called the [[standard basis]] consists of the two vectors {{math|1=''e''<sub>1</sub> = (1,0)}} and {{math|1=''e''<sub>2</sub> = (0,1)}}, since, any vector {{math|1=''v'' = (''a'', ''b'')}} of {{math|'''R'''<sup>2</sup>}} may be uniquely written as\n::<math>v= ae_1+be_2.</math>\n:Any other pair of linearly independent vectors of {{math|'''R'''<sup>2</sup>}}, such as {{math|(1, 1)}} and {{math|(−1, 2)}}, forms also a basis of '''R'''<sup>2</sup>.\n*More generally, if {{mvar|F}} is a [[field (mathematics)|field]], the set <math>F^n</math> of [[tuple|{{mvar|n}}-tuples]] of elements of {{mvar|F}} is a vector space for similarly defined addition and scalar multiplication. Let\n::<math>e_i = (0, \\ldots, 0,1,0,\\ldots, 0)</math>\n: be the {{mvar|n}}-tuple with all components equal to 0, except the {{mvar|i}}th, which is 1. Then <math>e_1, \\ldots, e_n</math> is a basis of <math>F^n,</math> which is called the ''standard basis'' of <math>F^n.</math>\n*If {{mvar|F}} is a field, the [[polynomial ring]] {{math|''F''[''X'']}} of the [[polynomial]]s in one [[indeterminate (variable)|indeterminate]] has a basis {{mvar|B}}, called the [[monomial basis]], consisting of all [[monomial]]s:\n::<math>B=\\{1, X, X^2, \\ldots\\}.</math>\n:Any set of polynomials such that there is exactly one polynomial of each degree is also a basis. Such a set of polynomials is called a [[polynomial sequence]]. Example (among many) of such polynomial sequences are [[Bernstein polynomial|Bernstein basis polynomial]]s, and [[Chebyshev polynomials]].\n\n== Properties ==\n\nMany properties of finite bases result from the [[Steinitz exchange lemma]], which states that, given a finite [[spanning set]] {{mvar|S}} and a [[linearly independent]] subset {{mvar|L}} of {{mvar|n}} elements of {{mvar|S}}, one may replace {{mvar|n}} well chosen elements of {{mvar|S}} by the elements of {{mvar|L}} for getting a spanning set containing {{mvar|L}}, having its other elements in {{mvar|S}}, and having the same number of elements as {{mvar|S}}.\n\nMost properties resulting from the Steinitz exchange lemma remain true when there is no finite spanning set, but their proof in the infinite case requires generally the [[axiom of choice]] or a weaker form of it, such as the [[ultrafilter lemma]].\n\nIf {{mvar|V}} is a vector space over a field {{mvar|F}}, then:\n* If {{mvar|L}} is a linearly independent subset of a spanning set {{math|''S'' ⊆ ''V''}}, then there is a basis {{mvar|B}} such that \n::<math>L\\subseteq B\\subseteq S.</math>\n* {{mvar|V}} has a basis (this is the preceding property with {{mvar|L}} being the [[empty set]], and {{math|1=''S'' = ''V''}}).\n* All bases of {{mvar|V}} have the same [[cardinality]], which is called the [[Dimension (vector space)|dimension]] of {{mvar|V}}. This is the [[dimension theorem for vector spaces|dimension theorem]].\n* A generating set {{mvar|S}} is a basis of {{mvar|V}} if and only if it is minimal, that is, no [[subset|proper subset]] of {{mvar|S}} is also a generating set of {{mvar|V}}.\n* A linearly independent set {{mvar|L}} is a basis if and only if it is maximal, that is, it is not a proper subset of any linearly independent set.\n\nIf {{mvar|V}} is a vector space of dimension {{mvar|n}}, then:\n* A subset of {{mvar|V}} with {{mvar|n}} elements is a basis if and only if it is linearly independent.\n* A subset of {{mvar|V}} with {{mvar|n}} elements is a basis if and only if it is spanning set of {{mvar|V}}.\n\n== Coordinates {{anchor|Ordered bases and coordinates}} ==\n\nLet {{mvar|V}} be a vector space of finite dimension {{mvar|n}} over a field {{mvar|F}}, and \n:<math>B=\\{b_1, \\ldots, b_n\\}</math>\nbe a basis of {{mvar|V}}. By definition of a basis, for every {{mvar|v}} in {{mvar|V}} may be written, in a unique way,\n:<math>v=\\lambda_1 b_1 + \\cdots + \\lambda_n b_n,</math>\nwhere the coefficients <math>\\lambda_1, \\ldots, \\lambda_n</math> are scalars (that is, elements of {{mvar|F}}), which are called the ''coordinates'' of {{mvar|v}} over {{mvar|B}}. However, if one talks of the ''set'' of the coefficients, one looses the correspondence between coefficients and basis elements, and several vectors may have the same ''set'' of coefficients. For example, <math>3b_1 +2b_2</math> and <math>2b_1 +3b_2</math> have the same set of coefficients {{math|{2, 3}{{void}}}}, and are different. It is therefore often convenient to work with an '''ordered basis'''; this is typically done by [[index set|indexing]] the basis elements by the first natural numbers. Then, the coordinates of a vector form a [[sequence (mathematics)|sequence]] similarly indexed, and a vector is completely characterized by the sequence of coordinates. An ordered basis is also called a '''frame''', a word commonly used, in various contexts, for referring to a sequence of data allowing defining coordinates.\n\nLet, as usual, <math>F^n</math> be the set of the [[tuple|{{mvar|n}}-tuples]] of elements of {{mvar|F}}. This set is an {{mvar|F}}-vector space, with addition and scalar multiplication defined component-wise. The map \n:<math>\\varphi: (\\lambda_1, \\ldots, \\lambda_n) \\mapsto \\lambda_1 b_1 + \\cdots + \\lambda_n b_n</math>\nis a [[linear isomorphism]] from the vector space <math>F^n</math> onto {{mvar|V}}. In other words, <math>F^n</math> is the [[coordinate space]] of {{mvar|V}}, and the {{mvar|n}}-tuple <math>\\varphi^{-1}(v)</math> is the [[coordinate vector]] of {{mvar|v}}.\n\nThe [[inverse image]] by <math>\\varphi</math> of <math>b_i</math> is the {{mvar|n}}-tuple <math>e_i</math> all of whose components are 0, except the {{mvar|i}}th that is 1. The <math>e_i</math> form an ordered basis of <math>F^n,</math> which is called its [[standard basis]] or [[canonical basis]]. The ordered basis {{mvar|B}} is the image by <math>\\varphi</math> of the canonical basis of <math>F^n.</math> \n\nIt follows from what precedes that every ordered basis is the image by a linear isomorphism of the canonical basis of <math>F^n,</math> and that every linear isomorphism from <math>F^n</math> onto {{mvar|V}} may be defined as the isomorphism that maps the canonical basis of <math>F^n</math> onto a given ordered basis of {{mvar|V}}. In other words it is equivalent to define an ordered basis of {{mvar|V}}, or a linear isomorphism from <math>F^n</math> onto {{mvar|V}}.\n\n== Change of basis ==\n{{main|Change of basis}}\nLet {{math|''V''}} be a vector space of dimension {{mvar|n}} over a field {{math|''F''}}. Given two (ordered) bases <math>B_\\mathrm {old}=(v_1, \\ldots, v_n)</math> and <math>B_\\mathrm {new}=(w_1, \\ldots, w_n)</math> of {{math|''V''}}, it is often useful to express the coordinates of a vector {{mvar|x}} with respect to <math>B_\\mathrm {old}</math> in terms of the coordinates with respect to <math>B_\\mathrm {new}.</math> This can be done by the ''change-of-basis formula'', that is described below. The subscripts \"old\" and \"new\" have been chosen because it is customary to refer to <math>B_\\mathrm {old}</math> and <math>B_\\mathrm {new}</math> as the ''old basis'' and the ''new basis'', respectively. It is useful to describe the old coordinates in terms of the new ones, because, in general, one has [[expression (mathematics)|expressions]] involving the old coordinates, and if one wants to obtain equivalent expressions in terms of the new coordinates; this is obtained by replacing  the old coordinates by their expressions in terms of the new coordinates.\n\nTypically, the new basis vectors are given by their coordinates over the old basis, that is, \n:<math>w_j=\\sum_{i=1}^n a_{i,j}v_i.</math>\nIf <math>(x_1, \\ldots, x_n)</math> and <math>(y_1, \\ldots, y_n)</math> are the coordinates of a vector {{mvar|x}} over the old and the new basis respectively, the change-of-basis formula is \n:<math>x_i = \\sum_{j=1}^n a_{i,j}y_j,</math>\nfor {{math|1=''i'' = 1, ..., ''n''}}.\n\nThis formula may be concisely written in [[matrix (mathematics)|matrix]] notation. Let {{mvar|A}} be the matrix of the <math>a_{i,j},</math> and\n:<math>X= \\begin{pmatrix}x_1\\\\\\vdots\\\\x_n\\end{pmatrix}\\quad</math> and <math>\\quad Y= \\begin{pmatrix}y_1\\\\\\vdots\\\\y_n\\end{pmatrix}</math>\nbe the [[column vector]]s of the coordinates of {{mvar|v}} in the old and the new basis respectively, then the formula for changing coordinates is\n:<math>X=AY.</math>\n\nThe formula can be proven by considering the decomposition of the vector {{mvar|x}} on the two bases: one has \n:<math>x=\\sum_{i=1}^n x_i v_i,</math>\nand\n:<math>\\begin{align}\nx&=\\sum_{j=1}^n y_j w_j \\\\\n &=\\sum_{j=1}^n y_j\\sum_{i=1}^n a_{i,j}v_i\\\\\n &=\\sum_{i=1}^n \\left(\\sum_{j=1}^n a_{i,j}y_j\\right)v_i.\n\\end{align}</math>\n\nThe change-of-basis formula results then from the uniqueness of the decomposition of a vector over a basis, here <math>B_\\mathrm {old};</math> that is\n:<math>x_i = \\sum_{j=1}^n a_{i,j}y_j,</math>\nfor {{math|1=''i'' = 1, ..., ''n''}}.\n\n== Related notions ==\n===Free module===\n{{main|Free module|Free abelian group}} \nIf one replaces the field occurring in the definition of a vector space by a [[ring (mathematics)|ring]], one gets the definition of a [[module (mathematics)|module]]. For modules, [[linear independence]] and [[spanning set]]s are defined exactly as for vector spaces, although \"[[generating set of a module|generating set]]\" is more commonly used than that of \"spanning set\".\n\nLike for vector spaces, a ''basis'' of a module is a linearly independent subset that is also a generating set. A major difference with the theory of vector spaces is that not every module has a basis. A module that has a basis is called a ''free module''. Free modules play a fundamental role in module theory, as they may be used for describing the structure of non-free modules through [[free resolution]]s.\n\nA module over the integers is exactly the same thing as an [[abelian group]]. Thus a free module over the integers is also a free abelian group. Free abelian groups have specific properties that are not shared by modules over other rings. Specifically, every subgroup of a free abelian group is a group, and, if {{mvar|G}} is a subgroup of a finitely generated free abelian group {{mvar|H}} (that is an abelian group that has a finite basis), there is a basis <math>e_1, \\ldots, e_n</math> of {{mvar|H}} and an integer {{math|0 ≤ ''k'' ≤ ''n''}} such that <math>a_1e_1, \\ldots, a_ke_k</math> is a basis of {{mvar|G}}, for some nonzero integers <math>a_1, \\ldots, a_k.</math> For details, see {{slink|Free abelian group|Subgroups}}.\n\n=== Analysis ===\nIn the context of infinite-dimensional vector spaces over the real or complex numbers, the term '''{{visible anchor|Hamel basis}}''' (named after [[Georg Hamel]]) or '''algebraic basis''' can be used to refer to a basis as defined in this article. This is to make a distinction with other notions of \"basis\" that exist when infinite-dimensional vector spaces are endowed with extra structure. The most important alternatives are [[orthogonal basis|orthogonal bases]] on [[Hilbert space]]s, [[Schauder basis|Schauder bases]], and [[Markushevich basis|Markushevich bases]] on [[normed linear space]]s. In the case of the real numbers '''R''' viewed as a vector space over the field '''Q''' of rational numbers, Hamel bases are uncountable, and have specifically the [[cardinality]] of the continuum, which is the [[cardinal number]] <math>2^{\\aleph_0},</math> where <math>\\aleph_0</math> is the smallest infinite cardinal, the cardinal of the integers.\n\nThe common feature of the other notions is that they permit the taking of infinite linear combinations of the basis vectors in order to generate the space. This, of course, requires that infinite sums are meaningfully defined on these spaces, as is the case for [[topological vector space]]s – a large class of vector spaces including e.g. [[Hilbert space]]s, [[Banach space]]s, or [[Fréchet space]]s.\n\nThe preference of other types of bases for infinite-dimensional spaces is justified by the fact that the Hamel basis becomes \"too big\" in Banach spaces: If ''X'' is an infinite-dimensional normed vector space which is [[complete space|complete]] (i.e. ''X'' is  a [[Banach space]]), then any Hamel basis of ''X'' is necessarily [[uncountable]]. This is a consequence of the [[Baire category theorem]]. The completeness as well as infinite dimension are crucial assumptions in the previous claim. Indeed, finite-dimensional spaces have by definition finite bases and there are infinite-dimensional (''non-complete'') normed spaces which have countable Hamel bases. Consider  <math>c_{00}</math>, the space of the [[sequence]]s <math>x=(x_n)</math> of real numbers which have only finitely many non-zero elements, with the norm <math>\\|x\\|=\\sup_n |x_n|.</math> Its [[standard basis]], consisting of the sequences having only one non-zero element, which is equal to 1, is a countable Hamel basis.\n\n==== Example ====\nIn the study of [[Fourier series]], one learns that the functions {1} ∪ { sin(''nx''), cos(''nx'') : ''n'' = 1, 2, 3, ... } are an \"orthogonal basis\" of the (real or complex) vector space of all (real or complex valued) functions on the interval [0, 2π] that are square-integrable on this interval, i.e., functions ''f'' satisfying\n\n:<math>\\int_0^{2\\pi} \\left|f(x)\\right|^2\\,dx<\\infty.</math>\n\nThe functions {1} ∪ { sin(''nx''), cos(''nx'') : ''n'' = 1, 2, 3, ... } are linearly independent, and every function ''f'' that is square-integrable on [0, 2π] is an \"infinite linear combination\" of them, in the sense that\n\n:<math>\\lim_{n\\rightarrow\\infty}\\int_0^{2\\pi}\\biggl|a_0+\\sum_{k=1}^n \\bigl(a_k\\cos(kx)+b_k\\sin(kx)\\bigr)-f(x)\\biggr|^2\\,dx=0</math>\n\nfor suitable (real or complex) coefficients ''a''<sub>''k''</sub>, ''b''<sub>''k''</sub>.  But many<ref>Note that one cannot say \"most\" because the cardinalities of the two sets (functions that can and cannot be represented with a finite number of basis functions) are the same.</ref> square-integrable functions cannot be represented as ''finite'' linear combinations of these basis functions, which therefore ''do not'' comprise a Hamel basis. Every Hamel basis of this space is much bigger than this merely countably infinite set of functions. Hamel bases of spaces of this kind are typically not useful, whereas [[orthonormal bases]] of these spaces are essential in [[Fourier analysis]].\n\n===Geometry===\nThe geometric notions of an [[affine space]], [[projective space]], [[convex set]], and [[Cone (linear algebra)|cone]] have related notions of {{anchor|affine basis}} ''basis''.<ref>{{cite book |title=Notes on Geometry |first=Elmer G. |last=Rees |location=Berlin |publisher=Springer |year=2005 |url=https://books.google.com/books?id=JkzPRaihGIYC&pg=PA7 |page=7 |isbn=978-3-540-12053-7 }}</ref> An '''affine basis''' for an ''n''-dimensional affine space is <math>n+1</math> points in [[general linear position]]. A  '''{{visible anchor|projective basis}}''' is <math>n+2</math> points in general position, in a projective space of dimension ''n''. A '''{{visible anchor|convex basis}}''' of a [[polytope]] is the set of the vertices of its [[convex hull]]. A '''{{visible anchor|cone basis}}'''<ref>{{cite journal |title=Some remarks about additive functions on cones |first=Marek |last=Kuczma |journal=[[Aequationes Mathematicae]] |year=1970 |volume=4 |issue=3 |pages=303–306 |doi=10.1007/BF01844160 }}</ref> consists of one point by edge of a polygonal cone. See also a [[Hilbert basis (linear programming)]].\n\n===Random basis===\nFor a [[probability distribution]] in '''R'''<sup>''n''</sup> with a [[probability density function]], such as the equidistribution  in a ''n''-dimensional ball with respect to Lebesgue measure, it can be shown that ''n'' randomly and independently chosen vectors will form a basis [[with probability one]], which is due to the fact that ''n'' linearly dependent vectors '''x'''<sub>1</sub>, ..., '''x'''<sub>''n''</sub> in '''R'''<sup>''n''</sup> should satisfy the equation {{nowrap|1=det['''x'''<sub>1</sub>, ..., '''x'''<sub>''n''</sub>] = 0}} (zero determinant of the matrix with columns '''x'''<sub>''i''</sub>), and the set of zeros of a non-trivial polynomial has zero measure. This observation has led to techniques for approximating random bases.<ref>{{cite journal |first=B. |last=Igelnik |first2=Y.-H. |last2=Pao |title=Stochastic choice of basis functions in adaptive function approximation and the functional-link net |journal=IEEE Trans. Neural Netw. |volume=6 |issue=6 |year=1995 |pages=1320–1329 |doi=10.1109/72.471375 |pmid=18263425 }}</ref><ref name = \"GorbanTyukin2016\">{{cite journal |first=A. N. |last=Gorban |first2=I. Yu. |last2=Tyukin |first3=D. V. |last3=Prokhorov |first4=K. I. |last4=Sofeikov |title=Approximation with Random Bases: Pro et Contra |journal=Information Sciences |volume=364–365 |year=2016 |pages=129–145 |doi=10.1016/j.ins.2015.09.021 |arxiv=1506.04631 }}</ref>\n\n[[File:Random almost orthogonal sets.png|thumb|270px|Empirical distribution of lengths N of pairwise almost orthogonal chains of vectors that are independently randomly sampled from the ''n''-dimensional cube {{nowrap|[−1, 1]<sup>''n''</sup>}} as a function of dimension, ''n''. Boxplots show the second and third quartiles of this data for each ''n'', red bars correspond to the medians, and blue stars indicate means. Red curve shows theoretical bound given by Eq. (1) and green curve shows a refined estimate.<ref name = \"GorbanTyukin2016\"/>]] \n\nIt is difficult to check numerically the linear dependence or exact orthogonality. Therefore, the notion of ε-orthogonality is used. For [[Inner product space|spaces with inner product]],  ''x'' is  ε-orthogonal to ''y'' if <math>|\\langle x,y \\rangle|/(\\|x\\|\\|y\\|)<\\epsilon</math> (that is, cosine  of the angle between ''x'' and ''y'' is less than ε).\n\nIn high dimensions, two independent random vectors are with high probability almost orthogonal, and the number of independent random vectors, which all are with given high probability pairwise almost orthogonal, grows exponentially with dimension. More precisely, consider equidistribution in ''n''-dimensional ball. Choose ''N'' independent random vectors from a ball (they are [[Independent and identically distributed random variables|independent and identically distributed]]). Let ''θ'' be a small positive number. Then for\n\n{{NumBlk|:|<math>N\\leq e^{\\frac{\\epsilon^2n}{4}}[-\\ln(1-\\theta)]^{\\frac{1}{2}}</math>|Eq. 1}}\n\n''N'' random vectors are all pairwise ε-orthogonal with probability {{nowrap|1 − ''θ''}}.<ref name = \"GorbanTyukin2016\"/> This ''N'' growth exponentially with dimension ''n'' and <math>N\\gg n</math> for sufficiently big ''n''. This property of random bases is a manifestation of the so-called ''measure concentration phenomenon''.<ref>{{cite journal |first=S. |last=Artstein|authorlink=Shiri Artstein |title=Proportional concentration phenomena of the sphere |journal=[[Israel Journal of Mathematics|Israel J. Math.]] |volume=132 |year=2002 |issue=1 |pages=337–358 |doi=10.1007/BF02784520 |url=http://www.tau.ac.il/~shiri/israelj/ISRAJ.pdf |citeseerx=10.1.1.417.2375}}</ref>\n\nThe figure (right) illustrates distribution of lengths N of pairwise almost orthogonal chains of vectors that are independently randomly sampled from the ''n''-dimensional cube {{nowrap|[−1, 1]<sup>''n''</sup>}} as a function of dimension, ''n''. A point is first randomly selected in the cube. The second point is randomly chosen in the same cube. If the angle between the vectors was within {{nowrap|π/2 ± 0.037π/2}} then the vector was retained. At the next step a new vector is generated in the same hypercube, and its angles with the previously generated vectors are evaluated. If these angles are within {{nowrap|π/2 ± 0.037π/2}} then the vector is retained. The process is repeated until the chain of almost orthogonality breaks, and the number of such pairwise almost orthogonal vectors (length of the chain) is recorded. For each ''n'', 20 pairwise almost orthogonal chains where constructed numerically for each dimension. Distribution of the length of these chains is presented.\n\n==Proof that every vector space has a basis==\n\nLet '''V''' be any vector space over some field '''F'''.\nLet '''X''' be the set of all linearly independent subsets of '''V'''.\n\nThe set '''X''' is nonempty since the empty set is an independent subset of '''V''',\nand it is [[Partial order|partially ordered]] by inclusion, which is denoted, as usual, by {{math|⊆}}.\n\nLet '''Y''' be a subset of '''X''' that is totally ordered by {{math|⊆}},\nand let L<sub>'''Y'''</sub> be the union of all the elements of '''Y''' (which are themselves certain subsets of '''V''').\n\nSince ('''Y''', ⊆) is totally ordered, every finite subset of L<sub>'''Y'''</sub> is a subset of an element of '''Y''',\nwhich is a linearly independent subset of '''V''',\nand hence every finite subset of L<sub>'''Y'''</sub> is linearly independent.\nThus L<sub>'''Y'''</sub> is linearly independent, so L<sub>'''Y'''</sub> is an element of '''X'''.\nTherefore, L<sub>'''Y'''</sub> is an upper bound for '''Y''' in ('''X''', ⊆):\nit is an element of '''X''', that contains every element '''Y'''.\n\nAs '''X''' is nonempty, and every totally ordered subset of ('''X''', ⊆) has an upper bound in '''X''', [[Zorn's lemma]] asserts that '''X''' has a maximal element. In other words, there exists some element L<sub>'''max'''</sub> of '''X''' satisfying the condition that whenever L<sub>'''max'''</sub> ⊆ L for some element L of '''X''', then L = L<sub>'''max'''</sub>.\n\nIt remains to prove that L<sub>'''max'''</sub> is a basis of '''V'''.  Since L<sub>'''max'''</sub> belongs to '''X''', we already know that L<sub>'''max'''</sub> is a linearly independent subset of '''V'''.\n\nIf L<sub>'''max'''</sub> would not span '''V''', there would exist some vector '''w''' of '''V''' that cannot be expressed as a linear combination of elements of L<sub>'''max'''</sub> (with coefficients in the field '''F'''). In particular, '''w''' cannot be an element of L<sub>'''max'''</sub>.\nLet L<sub>'''w'''</sub>  =  L<sub>'''max'''</sub> ∪ {'''w'''}. This set is an element of '''X''', that is, it is a linearly independent subset of '''V''' (because '''w''' is not in the span of L<sub>'''max'''</sub>, and L<sub>'''max'''</sub> is independent). As L<sub>'''max'''</sub> ⊆ L<sub>'''w'''</sub>, and L<sub>'''max'''</sub> ≠ L<sub>'''w'''</sub> (because L<sub>'''w'''</sub> contains the vector '''w''' that is not contained in L<sub>'''max'''</sub>), this contradicts the maximality of L<sub>'''max'''</sub>. Thus this shows that L<sub>'''max'''</sub> spans '''V'''.\n\nHence L<sub>'''max'''</sub> is linearly independent and spans '''V'''. It is thus a basis of '''V''', and this proves that every vector space has a basis.\n\nThis proof relies on Zorn's lemma, which is equivalent to the [[axiom of choice]]. Conversely, it may be proved that if every vector space has a basis, then the axiom of choice is true; thus the two assertions are equivalent.\n\n==See also==\n* [[Change of basis]]\n* [[Frame of a vector space]]\n* [[Spherical basis]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n\n===General references===\n* {{Citation | last1=Blass | first1=Andreas | title=Axiomatic set theory  | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Contemporary Mathematics volume 31 | mr=763890 | year=1984 | chapter=Existence of bases implies the axiom of choice | pages=31–33|isbn=978-0-8218-5026-8}}\n* {{Citation | last1=Brown | first1=William A. | title=Matrices and vector spaces | publisher=M. Dekker | location=New York | isbn=978-0-8247-8419-5 | year=1991}}\n* {{Citation | last1=Lang | first1=Serge | author1-link=Serge Lang | title=Linear algebra | publisher=[[Springer-Verlag]] | location=Berlin, New York | isbn=978-0-387-96412-6 | year=1987}}\n\n===Historical references===\n* {{Citation | last1=Banach | first1=Stefan | author1-link=Stefan Banach | title=Sur les opérations dans les ensembles abstraits et leur application aux équations intégrales (On operations in abstract sets and their application to integral equations) | url=http://matwbn.icm.edu.pl/ksiazki/fm/fm3/fm3120.pdf | year=1922 | journal=[[Fundamenta Mathematicae]] | issn=0016-2736 | volume=3| pages=133–181 |language=fr| doi=10.4064/fm-3-1-133-181 }}\n* {{Citation | last1=Bolzano | first1=Bernard | author1-link=Bernard Bolzano | title=Betrachtungen über einige Gegenstände der Elementargeometrie (Considerations of some aspects of elementary geometry) | url=http://dml.cz/handle/10338.dmlcz/400338 | year=1804|language=de}}\n* {{Citation | last1=Bourbaki | first1=Nicolas | author1-link=Nicolas Bourbaki | title=Éléments d'histoire des mathématiques (Elements of history of mathematics) | publisher=Hermann | location=Paris | year=1969|language=fr}}\n* {{Citation | last1=Dorier | first1=Jean-Luc | title=A general outline of the genesis of vector space theory | mr=1347828 | year=1995 | journal=[[Historia Mathematica]] | volume=22 | issue=3 | pages=227–261 | doi=10.1006/hmat.1995.1024}}\n* {{Citation | last1=Fourier | first1=Jean Baptiste Joseph | author1-link=Joseph Fourier | title=Théorie analytique de la chaleur | url=https://books.google.com/books?id=TDQJAAAAIAAJ | publisher=Chez Firmin Didot, père et fils | year=1822|language=fr}}\n* {{Citation | last1=Grassmann | first1=Hermann | author1-link=Hermann Grassmann | title=Die Lineale Ausdehnungslehre - Ein neuer Zweig der Mathematik | url=https://books.google.com/books?id=bKgAAAAAMAAJ&pg=PA1| year=1844|language=de}}, reprint: {{Citation | others=Kannenberg, L.C. | title=Extension Theory | publisher=[[American Mathematical Society]] | location=Providence, R.I. | isbn=978-0-8218-2031-5 | year=2000 | author=Hermann Grassmann. Translated by Lloyd C. Kannenberg.}}\n* {{Citation | last1=Hamilton | first1=William Rowan | author1-link=William Rowan Hamilton | title=Lectures on Quaternions | url=http://historical.library.cornell.edu/cgi-bin/cul.math/docviewer?did=05230001&seq=9 | publisher=Royal Irish Academy | year=1853}}\n* {{Citation |last1=Möbius |first1=August Ferdinand |author1-link=August Ferdinand Möbius |title=Der Barycentrische Calcul : ein neues Hülfsmittel zur analytischen Behandlung der Geometrie (Barycentric calculus: a new utility for an analytic treatment of geometry) |url=http://mathdoc.emath.fr/cgi-bin/oeitem?id=OE_MOBIUS__1_1_0 |year=1827 |language=de |deadurl=yes |archiveurl=https://web.archive.org/web/20090412013616/http://mathdoc.emath.fr/cgi-bin/oeitem?id=OE_MOBIUS__1_1_0 |archivedate=2009-04-12 |df= }}\n* {{Citation | last1=Moore | first1=Gregory H. | title=The axiomatization of linear algebra: 1875–1940 | year=1995 | journal=[[Historia Mathematica]] | volume=22 | issue=3 | pages=262–303 | doi=10.1006/hmat.1995.1025}}\n* {{Citation | last1=Peano | first1=Giuseppe | author1-link=Giuseppe Peano | title=Calcolo Geometrico secondo l'Ausdehnungslehre di H. Grassmann preceduto dalle Operazioni della Logica Deduttiva | year=1888 | location=Turin|language=it}}\n\n==External links==\n* Instructional videos from Khan Academy\n**[https://web.archive.org/web/20120426050335/http://khanexercises.appspot.com/video?v=zntNi3-ybfQ Introduction to bases of subspaces]\n**[https://web.archive.org/web/20120426050418/http://khanexercises.appspot.com/video?v=Zn2K8UIT8r4 Proof that any subspace basis has same number of elements]\n* {{Cite web |title=Linear combinations, span, and basis vectors |work=Essence of linear algebra |date=August 6, 2016 |via=[[YouTube]] |url=https://www.youtube.com/watch?v=k7RM-ot2NWY&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=3 }}\n* {{springer|title=Basis|id=p/b015350}}\n\n{{linear algebra}}\n{{tensors}}\n\n{{DEFAULTSORT:Basis (Linear Algebra)}}\n[[Category:Linear algebra]]\n[[Category:Articles containing proofs]]\n[[Category:Matroid theory]]\n[[Category:Axiom of choice]]"
    },
    {
      "title": "Biased graph",
      "url": "https://en.wikipedia.org/wiki/Biased_graph",
      "text": "In [[mathematics]], a '''biased graph''' is a [[graph theory|graph]] with a list of distinguished circles (edge sets of [[simple cycle]]s), such that if two circles in the list are contained in a [[glossary of graph theory|theta graph]], then the third circle of the theta graph is also in the list.  A biased graph is a generalization of the combinatorial essentials of a [[gain graph]] and in particular of a [[signed graph]].\n\nFormally, a '''biased graph''' Ω is a pair (''G'', '''''B''''') where '''''B''''' is a '''linear class''' of circles; this by definition is a class of circles that satisfies the theta-graph property mentioned above.\n\nA [[Glossary of graph theory#Subgraphs|subgraph]] or edge set whose circles are all in '''''B''''' (and which contains no [[Graph (discrete mathematics)|half-edges]]) is called '''balanced'''.  For instance, a circle belonging to '''''B''''' is ''balanced'' and one that does not belong to '''''B''''' is ''unbalanced''.\n\nBiased graphs are interesting mostly because of their [[matroid]]s, but also because of their connection with multiary [[quasigroups]].  See below.\n\n==Technical notes==\n\nA biased graph may have [[Graph (discrete mathematics)|half-edges]] (one endpoint) and [[Graph (discrete mathematics)|loose edges]] (no endpoints).  The edges with two endpoints are of two kinds: a link has two distinct endpoints, while a loop has two coinciding endpoints.\n\nLinear classes of circles are a special case of linear subclasses of circuits in a [[matroid]].\n\n==Examples==\n\n* If every circle belongs to '''''B''''', and there are no half-edges, Ω is balanced.  A balanced biased graph is (for most purposes) essentially the same as an ordinary graph.\n* If '''''B''''' is empty, Ω is called '''contrabalanced'''.  Contrabalanced biased graphs are related to [[bicircular matroid]]s.\n* If '''''B''''' consists of the circles of even length, Ω is called '''antibalanced''' and is the biased graph obtained from an all-negative [[signed graph]].\n* The linear class '''''B''''' is '''additive''', that is, closed under repeated [[symmetric difference]] (when the result is a circle), [[if and only if]] '''''B''''' is the class of positive circles of a signed graph.\n* Ω may have underlying graph that is a cycle of length ''n'' ≥ 3 with all edges doubled.  Call this a '''biased 2''C<sub>n</sub>''''' .  Such biased graphs in which no [[digon]] (circle of length 2) is balanced lead to spikes and swirls (see Matroids, below).\n* Some kinds of biased graph are obtained from [[gain graph]]s or are generalizations of special kinds of gain graph.  The latter include '''biased expansion graphs''', which generalize [[gain graph|group expansion graph]]s.\n\n==Minors==\n\nA [[Minor (graph theory)|minor]] of a biased graph Ω = (''G'', '''''B''''') is the result of any sequence of taking subgraphs and contracting edge sets.  For biased graphs, as for graphs, it suffices to take a subgraph (which may be the whole graph) and then contract an edge set (which may be the empty set).\n\nA '''subgraph''' of Ω consists of a subgraph ''H'' of the underlying graph ''G'', with balanced circle class consisting of those balanced circles that are in ''H''.  The '''deletion''' of an edge set ''S'', written Ω &minus; ''S'', is the subgraph with all vertices and all edges except those of ''S''.\n\n'''Contraction''' of Ω is relatively complicated.  To contract one edge ''e'', the procedure depends on the kind of edge ''e'' is.  If ''e'' is a link, contract it in ''G''.  A circle ''C'' in the contraction ''G''/''e'' is balanced if either ''C'' or <math>C \\cup e</math> is a balanced circle of ''G''.   If ''e'' is a balanced loop or a loose edge, it is simply deleted.  If it is an unbalanced loop or a half-edge, it and its vertex ''v'' are deleted; each other edge with ''v'' as an endpoint loses that endpoint, so a link with ''v'' as one endpoint becomes a half-edge at its other endpoint, while a loop or half-edge at ''v'' becomes a loose edge.\n\nIn the contraction Ω/''S'' by an arbitrary edge set ''S'', the edge set is ''E'' &minus; ''S''.  (We let ''G'' = (''V'', ''E'').)  The vertex set is the class of vertex sets of balanced components of the subgraph (''V'', ''S'') of Ω.  That is, if (''V'', ''S'') has balanced components with vertex sets ''V''<sub>1</sub>, ..., ''V''<sub>''k''</sub>, then Ω/''S'' has ''k'' vertices ''V''<sub>1</sub>, ..., ''V''<sub>''k''</sub> .  An edge ''e'' of Ω, not in ''S'', becomes an edge of Ω/''S'' and each endpoint ''v''<sub>''i''</sub> of ''e'' in Ω that belongs to some ''V<sub>i</sub>'' becomes the endpoint ''V<sub>i</sub>'' of ''e'' in Ω/''S'' ; thus, an endpoint of ''e'' that is not in a balanced component of (''V'', ''S'') disappears.  An edge with all endpoints in unbalanced components of (''V'', ''S'') becomes a loose edge in the contraction.  An edge with only one endpoint in a balanced component of (''V'', ''S'') becomes a half-edge.  An edge with two endpoints that belong to different balanced components becomes a link, and an edge with two endpoints that belong to the same balanced component becomes a loop.\n\n==Matroids==\n\nThere are two kinds of [[matroid]] associated with a biased graph, both of which generalize the cycle matroid of a graph (Zaslavsky, 1991).\n\n===The frame matroid===\n\nThe '''frame matroid''' (sometimes called '''bias matroid''') of a biased graph, ''M''(Ω), (Zaslavsky, 1989) has for its ground set the edge set ''E''.  An edge set is independent if each component contains either no circles or just one circle, which is unbalanced.  (In matroid theory a half-edge acts like an unbalanced loop and a loose edge acts like a balanced loop.)  ''M''(Ω) is a [[Matroid#Additional terminology|frame matroid]] in the abstract sense, meaning that it is a submatroid of a matroid in which, for at least one basis, the set of lines generated by pairs of basis elements covers the whole matroid. Conversely, every abstract frame matroid is the frame matroid of some biased graph.\n\nThe circuits of the matroid are called '''frame circuits''' or '''bias circuits'''.  There are four kinds.  One is a balanced circle.  Two other kinds are a pair of unbalanced circles together with a connecting simple path, such that the two circles are either disjoint (then the connecting path has one end in common with each circle and is otherwise disjoint from both) or share just a single common vertex (in this case the connecting path is that single vertex).  The fourth kind of circuit is a theta graph in which every circle is unbalanced.\n\nThe rank of an edge set ''S'' is ''n'' &minus; ''b'', where ''n'' is the number of vertices of ''G'' and ''b'' is the number of balanced components of ''S'', counting isolated vertices as balanced components.\n\nMinors of the frame matroid agree with minors of the biased graph; that is, ''M''(Ω&minus;''S'') = ''M''(Ω)&minus;''S'' and ''M''(Ω/''S'') = ''M''(Ω)/''S''.\n\nFrame matroids generalize the [[Dowling geometry|Dowling geometries]] associated with a group (Dowling, 1973).  The frame matroid of a biased 2''C''<sub>''n''</sub> (see Examples, above) which has no balanced digons is called a '''swirl'''.  It is important in matroid structure theory.\n\n===The lift matroid===\n\nThe '''extended lift matroid''' ''L''<sub>0</sub>(Ω) has for its ground set the set ''E''<sub>0</sub>, which is the union of ''E'' with an '''extra point''' ''e''<sub>0</sub>.  The '''lift matroid''' ''L''(Ω) is the extended lift matroid restricted to ''E''.  The extra point acts exactly like an unbalanced loop or a half-edge, so we describe only the lift matroid.\n\nAn edge set is independent if it contains either no circles or just one circle, which is unbalanced.\n\nA circuit is a balanced circle, a pair of unbalanced circles that are either disjoint or have just a common vertex, or a theta graph whose circles are all unbalanced.\n\nThe rank of an edge set ''S'' is ''n'' &minus; ''c'' + ε, where ''c'' is the number of components of ''S'', counting isolated vertices, and ε is 0 if ''S'' is balanced and 1 if it is not.\n\nMinors of the lift and extended lift matroids agree in part with minors of the biased graph.  Deletions agree: ''L''(Ω&minus;''S'') = ''L''(Ω)&minus;''S''.  Contractions agree only for balanced edge sets: ''M''(Ω/''S'') = ''M''(Ω)/''S'' if ''S'' is balanced, but not if it is unbalanced.  If ''S'' is unbalanced, ''M''(Ω/''S'') = ''M''(''G'')/''S'' = ''M''(''G''/''S'') where ''M'' of a graph denotes the ordinary [[Matroid#Matroids from graph theory|graphic matroid]].\n\nThe lift matroid of a 2''C''<sub>''n''</sub> (see Examples, above) which has no balanced digons is called a '''spike'''.  Spikes are quite important in matroid structure theory.\n\n==Multiary quasigroups==\n\nJust as a group expansion of a complete graph ''K''<sub>''n''</sub> encodes the group (see [[Dowling geometry]]), its combinatorial analog expanding a simple cycle of length ''n'' + 1 encodes an ''n''-ary (multiary) [[Quasigroup#Polyadic or multiary quasigroups|quasigroup]].  It is possible to prove theorems about multiary quasigroups by means of biased graphs (Zaslavsky, t.a.)\n\n==References==\n\n*T. A. Dowling (1973),  A class of geometric lattices based on finite groups.  ''Journal of Combinatorial Theory, Series B'', Vol. 14, pp.&nbsp;61&ndash;86.\n*Thomas Zaslavsky (1989),  Biased graphs.  I.  Bias, balance, and gains.  ''Journal of Combinatorial Theory, Series B'', Vol. 47, pp.&nbsp;32&ndash;52.\n*Thomas Zaslavsky (1991),  Biased graphs.  II.  The three matroids.  ''Journal of Combinatorial Theory, Series B'', Vol. 51, pp.&nbsp;46&ndash;72.\n*Thomas Zaslavsky (1999).  A mathematical bibliography of signed and gain graphs and allied areas.  1999 edition: [https://web.archive.org/web/20100912123133/http://www.combinatorics.org/Surveys/index.html ''Electronic Journal of Combinatorics'', Dynamic Surveys in Combinatorics, #DS8, archived].  Current edition:  [http://www.combinatorics.org/issue/view/Surveys  ''Electronic Journal of Combinatorics'', Dynamic Surveys in Combinatorics, #DS8].\n*Thomas Zaslavsky (2012),  Associativity in multiary quasigroups: the way of biased expansions.  ''[[Aequationes Mathematicae]]'', Vol. 83, pp.&nbsp;1–66.\n\n[[Category:Graph families]]\n[[Category:Matroid theory]]"
    },
    {
      "title": "Bicircular matroid",
      "url": "https://en.wikipedia.org/wiki/Bicircular_matroid",
      "text": "In the [[mathematics|mathematical]] subject of [[matroid]] theory, the '''bicircular matroid''' of a [[Graph (discrete mathematics)|graph]] ''G'' is the matroid ''B''(''G'') whose points are the edges of ''G'' and whose independent sets are the edge sets of [[pseudoforest]]s of ''G'', that is, the edge sets in which each [[connected component (graph theory)|connected component]] contains at most one [[cycle (graph theory)|cycle]].\n\nThe bicircular matroid was introduced by {{harvtxt|Simões-Pereira|1972}} and explored further by {{harvtxt|Matthews|1977}} and others.  It is a special case of the [[frame matroid]] of a [[biased graph]].\n\n==Circuits==\nThe circuits, or minimal dependent sets, of this matroid are the '''bicircular graphs''' (or '''bicycles''', but that term has other meanings in graph theory); these are connected graphs whose [[circuit rank]] is exactly two.\n\nThere are three distinct types of bicircular graph:\n*The [[Glossary of graph theory#Walks|theta graph]] consists of three paths joining the same two vertices but not intersecting each other.\n*The figure eight graph (or tight handcuff) consists of two cycles having just one common vertex.\n*The loose handcuff (or barbell) consists of two disjoint cycles and a minimal connecting path.\nAll these definitions apply to [[multigraph]]s, i.e., they permit multiple edges (edges sharing the same endpoints) and loops (edges whose two endpoints are the same vertex).\n\n==Flats==\nThe [[Matroid#Closed sets (flats)|closed sets]] (flats) of the bicircular matroid of a graph {{mvar|G}} can be described as the [[Graph theory|forest]]s {{mvar|F}} of {{mvar|G}} such that in the [[induced subgraph]] of {{math|''V''(''G'') &minus; ''V''(''F'')}}, every connected component has a cycle.  Since the flats of a matroid form a [[geometric lattice]] when [[partial ordering|partially ordered]] by set inclusion, these forests of {{mvar|G}} also form a geometric lattice.  In the partial ordering for this lattice, that {{math|''F''<sub>1</sub> &le; ''F''<sub>2</sub>}} if \n* each component tree of {{math|''F''<sub>1</sub>}} is either contained in or vertex-disjoint from every tree of {{math|''F''<sub>2</sub>}}, and \n* each vertex of {{math|''F''<sub>2</sub>}} is a vertex of {{math|''F''<sub>1</sub>}}.  \nFor the most interesting example, let {{math|''G''{{hsp}}<sup>o</sup>}} be {{mvar|G}} with a loop added to every vertex.  Then the flats of {{math|''B''(''G''{{hsp}}<sup>o</sup>)}} are all the forests of {{mvar|G}}, spanning or nonspanning. Thus, all forests of a graph {{mvar|G}} form a geometric lattice, the '''forest lattice''' of ''G'' {{harv|Zaslavsky|1982}}.\n\n==As transversal matroids==\nBicircular matroids can be characterized as the [[transversal matroid]]s that arise from a [[family of sets]] in which each set element belongs to at most two sets. That is, the independent sets of the matroid are the subsets of elements that can be used to form a system of distinct representatives for some or all of the sets.\nIn this description, the elements correspond to the edges of a graph, and there is one set per vertex, the set of edges having that vertex as an endpoint.\n\n==Minors==\nUnlike transversal matroids in general, bicircular matroids form a [[Matroid minor|minor-closed class]]; that is, any submatroid or contraction of a bicircular matroid is also a bicircular matroid, as can be seen from their description in terms of [[biased graph]]s {{harv|Zaslavsky|1991}}. \nHere is a description of deletion and contraction of an edge in terms of the underlying graph:  To delete an edge from the matroid, remove it from the graph.  The rule for contraction depends on what kind of edge it is.  To contract a link (a non-loop) in the matroid, contract it in the graph in the usual way.  To contract a loop ''e'' at vertex ''v'', delete ''e'' and ''v'' but not the other edges incident with v; rather, each edge incident with ''v'' and another vertex ''w'' becomes a loop at ''w''.  Any other graph loops at ''v'' become matroid loops&mdash;to describe this correctly in terms of the graph one needs half-edges and loose edges; see [[Biased graph#Minors|biased graph minors]].\n\n==Characteristic polynomial==\n\nThe [[Matroid#Characteristic polynomial|characteristic polynomial]] of the bicircular matroid ''B''(''G''&nbsp;<sup>o</sup>) expresses in a simple way the numbers of spanning [[Tree (graph theory)|forest]]s (forests that contain all vertices of ''G'') of each size in ''G''.  The formula is \n:<math>p_{B(G)}(\\lambda) := \\sum_{k=0}^n (-1)^k f_k (\\lambda-1)^{n-k},</math>\nwhere ''f''<sub>''k''</sub> equals the number of ''k''-edge spanning forests in ''G''.  See {{harvtxt|Zaslavsky|1982}}.\n\n==Vector representation==\n\nBicircular matroids, like all other transversal matroids, can be [[Matroid representation|represented]] by vectors over any infinite [[Field (mathematics)|field]].  However, unlike [[graphic matroid]]s, they are not [[regular matroid|regular]]: they cannot be represented by vectors over an arbitrary [[finite field]].  The question of the fields over which a bicircular matroid has a vector representation leads to the largely unsolved problem of finding the fields over which a graph has multiplicative [[Gain graph|gains]].  See {{harvtxt|Zaslavsky|2007}}.\n\n==References==\n*{{citation\n | last = Matthews | first = Laurence R.\n | doi = 10.1093/qmath/28.2.213\n | issue = 110\n | journal = [[Quarterly Journal of Mathematics]]\n | mr = 0505702\n | pages = 213–227\n | series = Second Series\n | title = Bicircular matroids\n | volume = 28\n | year = 1977}}.\n*{{citation\n | last = Simões-Pereira | first = J. M. S.\n | doi = 10.1007/BF01111390\n | journal = [[Mathematische Zeitschrift]]\n | mr = 0317973\n | pages = 315–322\n | title = On subgraphs as matroid cells\n | volume = 127\n | year = 1972}}.\n*{{citation\n | last = Zaslavsky | first = Thomas | authorlink = Thomas Zaslavsky\n | doi = 10.1093/qmath/33.4.493\n | issue = 132\n | journal = [[Quarterly Journal of Mathematics]]\n | mr = 679818\n | pages = 493–511\n | series = Second Series\n | title = Bicircular geometry and the lattice of forests of a graph\n | volume = 33\n | year = 1982}}.\n*{{citation\n | last = Zaslavsky | first = Thomas | authorlink = Thomas Zaslavsky\n | doi = 10.1016/0095-8956(91)90005-5\n | issue = 1\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 1088626\n | pages = 46–72\n | series = Series B\n | title = Biased graphs. II. The three matroids\n | volume = 51\n | year = 1991}}.\n*{{citation\n | last = Zaslavsky | first = Thomas | authorlink = Thomas Zaslavsky\n | doi = 10.1016/j.jctb.2007.03.001\n | issue = 6\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 2354716\n | pages = 1019–1040\n | series = Series B\n | title = Biased graphs. VII. Contrabalance and antivoltages\n | volume = 97\n | year = 2007}}.\n\n[[Category:Graph theory]]\n[[Category:Matroid theory]]"
    },
    {
      "title": "Binary matroid",
      "url": "https://en.wikipedia.org/wiki/Binary_matroid",
      "text": "In [[Matroid|matroid theory]], a '''binary matroid''' is a matroid that can be [[Matroid representation|represented]] over the [[finite field]] [[GF(2)]].<ref name=\"w76\">{{citation\n | last = Welsh | first = D. J. A. | authorlink = Dominic Welsh\n | contribution = 10. Binary Matroids\n | isbn = 9780486474397\n | pages = 161–182\n | publisher = Courier Dover Publications\n | title = Matroid Theory\n | year = 2010 | origyear=1976}}.</ref> That is, up to isomorphism, they are the matroids whose elements are the columns of a [[Logical matrix|(0,1)-matrix]] and whose sets of elements are independent if and only if the corresponding columns are [[linearly independent]] in GF(2).\n\n==Alternative characterizations==\nA matroid <math>M</math> is binary if and only if\n*It is the matroid defined from a [[symmetric matrix|symmetric]] (0,1)-matrix.<ref>{{citation\n | last = Jaeger | first = F.\n | contribution = Symmetric representations of binary matroids\n | location = Amsterdam\n | mr = 841317\n | pages = 371–376\n | publisher = North-Holland\n | series = North-Holland Math. Stud.\n | title = Combinatorial mathematics (Marseille-Luminy, 1981)\n | volume = 75\n | year = 1983}}.</ref>\n*For every set <math>\\mathcal{S}</math> of circuits of the matroid, the [[symmetric difference]] of the circuits in <math>\\mathcal{S}</math> can be represented as a [[disjoint union]] of circuits.<ref>{{citation|last=Whitney|first=Hassler|authorlink=Hassler Whitney|year=1935|title=On the abstract properties of linear dependence|journal=American Journal of Mathematics|volume=57|pages=509–533|doi=10.2307/2371182|issue=3|publisher=The Johns Hopkins University Press|mr=1507091|jstor=2371182}}. Reprinted in {{harvtxt|Kung|1986}}, pp.&nbsp;55–79.</ref><ref name=\"w-thm3\">{{harvtxt|Welsh|2010}}, Theorem 10.1.3, p. 162.</ref>\n*For every pair of circuits of the matroid, their symmetric difference contains another circuit.<ref name=\"w-thm3\"/>\n*For every pair <math>C,D</math> where <math>C</math> is a circuit of <math>M</math> and <math>D</math> is a circuit of the [[dual matroid]] of <math>M</math>, <math>|C\\cap D|</math> is an even number.<ref name=\"w-thm3\"/><ref name=\"vs\">{{citation\n | last1 = Harary | first1 = Frank | author1-link = Frank Harary\n | last2 = Welsh | first2 = Dominic | author2-link = Dominic Welsh\n | contribution = Matroids versus graphs\n | doi = 10.1007/BFb0060114\n | location = Berlin\n | mr = 0263666\n | pages = 155–170\n | publisher = Springer\n | series = Lecture Notes in Mathematics\n | title = The Many Facets of Graph Theory (Proc. Conf., Western Mich. Univ., Kalamazoo, Mich., 1968)\n | volume = 110\n | year = 1969}}.</ref>\n*For every pair <math>B,C</math> where <math>B</math> is a basis of <math>M</math> and <math>C</math> is a circuit of <math>M</math>, <math>C</math> is the symmetric difference of the fundamental circuits induced in <math>B</math> by the elements of <math>C\\setminus B</math>.<ref name=\"w-thm3\"/><ref name=\"vs\"/>\n*No [[matroid minor]] of <math>M</math> is the [[uniform matroid]] <math>U{}^2_4</math>, the four-point line.<ref>{{citation\n | last = Tutte | first = W. T. | authorlink = W. T. Tutte\n | journal = [[Transactions of the American Mathematical Society]]\n | mr = 0101526\n | pages = 144–174\n | title = A homotopy theorem for matroids. I, II\n | volume = 88\n | year = 1958\n | doi=10.2307/1993244}}.</ref><ref name=\"tutte\">{{citation\n | last = Tutte | first = W. T.\n | journal = Journal of Research of the National Bureau of Standards\n | mr = 0179781\n | pages = 1–47\n | title = Lectures on matroids\n | url = http://cdm16009.contentdm.oclc.org/cdm/ref/collection/p13011coll6/id/66650\n | volume = 69B\n | year = 1965\n | doi=10.6028/jres.069b.001}}.</ref><ref name=\"w-10-2\">{{harvtxt|Welsh|2010}}, Section 10.2, \"An excluded minor criterion for a matroid to be binary\", pp. 167–169.</ref>\n*In the [[geometric lattice]] associated to the matroid, every interval of height two has at most five elements.<ref name=\"w-10-2\"/>\n\n==Related matroids==\nEvery [[regular matroid]], and every [[graphic matroid]], is binary.<ref name=\"vs\"/> A binary matroid is regular if and only if it does not contain the [[Fano plane]] (a seven-element non-regular binary matroid) or its dual as a [[matroid minor|minor]].<ref>{{harvtxt|Welsh|2010}}, Theorem 10.4.1, p. 175.</ref> A binary matroid is graphic if and only if its minors do not include the dual of the graphic matroid of <math>K_5</math> nor of <math>K_{3,3}</math>.<ref>{{harvtxt|Welsh|2010}}, Theorem 10.5.1, p. 176.</ref> If every circuit of a binary matroid has odd cardinality, then its circuits must all be disjoint from each other; in this case, it may be represented as the graphic matroid of a [[cactus graph]].<ref name=\"vs\"/>\n\n==Additional properties==\nIf <math>M</math> is a binary matroid, then so is its dual, and so is every [[matroid minor|minor]] of <math>M</math>.<ref name=\"vs\"/> Additionally, the direct sum of binary matroids is binary.\n\n{{harvtxt|Harary|Welsh|1969}} define a [[bipartite matroid]] to be a matroid in which every circuit has even cardinality, and an [[Eulerian matroid]] to be a matroid in which the elements can be partitioned into disjoint circuits. Within the class of graphic matroids, these two properties describe the matroids of [[bipartite graph]]s and [[Eulerian graph]]s (not-necessarily-connected graphs in which all vertices have even degree), respectively. For [[planar graphs]] (and therefore also for the graphic matroids of planar graphs) these two properties are dual: a planar graph or its matroid is bipartite if and only if its dual is Eulerian. The same is true for binary matroids. However, there exist non-binary matroids for which this duality breaks down.<ref name=\"vs\"/><ref>{{citation\n | last = Welsh | first = D. J. A. | authorlink = Dominic Welsh\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 0237368\n | pages = 375–377\n | title = Euler and bipartite matroids\n | volume = 6\n | year = 1969\n | doi=10.1016/s0021-9800(69)80033-5}}/</ref>\n\nAny algorithm that tests whether a given matroid is binary, given access to the matroid via an [[matroid oracle|independence oracle]], must perform an exponential number of oracle queries, and therefore cannot take polynomial time.<ref>{{citation\n | last = Seymour | first = P. D. | authorlink = Paul Seymour (mathematician)\n | doi = 10.1007/BF02579179\n | issue = 1\n | journal = [[Combinatorica]]\n | mr = 602418\n | pages = 75–78\n | title = Recognizing graphic matroids\n | volume = 1\n | year = 1981}}.</ref>\n\n==References==\n{{reflist|colwidth=30em}}\n\n[[Category:Matroid theory]]"
    },
    {
      "title": "Bipartite matroid",
      "url": "https://en.wikipedia.org/wiki/Bipartite_matroid",
      "text": "In mathematics, a '''bipartite matroid''' is a [[matroid]] all of whose circuits have [[even number|even]] size.\n\n==Example==\nA [[uniform matroid]] <math>U{}^r_n</math> is bipartite if and only if <math>r</math> is an odd number, because the circuits in such a matroid have size <math>r+1</math>.\n\n==Relation to bipartite graphs==\nEulerian matroids were defined by {{harvtxt|Welsh|1969}} as a generalization of the [[bipartite graph]]s, graphs in which every cycle has even size. A [[graphic matroid]] is bipartite if and only if it comes from a bipartite graph.<ref name=\"w69\">{{citation\n | last = Welsh | first = D. J. A. | authorlink = Dominic Welsh\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 0237368\n | pages = 375–377\n | title = Euler and bipartite matroids\n | volume = 6\n | year = 1969\n | doi=10.1016/s0021-9800(69)80033-5}}.</ref>\n\n==Duality with Eulerian matroids==\nAn [[Eulerian graph]] is one in which all vertices have even degree; Eulerian graphs may be disconnected. For [[planar graph]]s, the properties of being bipartite and Eulerian are dual: a planar graph is bipartite if and only if its [[dual graph]] is Eulerian. As Welsh showed, this duality extends to [[binary matroid]]s: a binary matroid is bipartite if and only if its [[dual matroid]] is an [[Eulerian matroid]], a matroid that can be partitioned into disjoint circuits.\n\nFor matroids that are not binary, the duality between Eulerian and bipartite matroids may break down. For instance, the uniform matroid <math>U{}^4_6</math> is non-bipartite but its dual <math>U{}^2_6</math> is Eulerian, as it can be partitioned into two 3-cycles. The self-dual uniform matroid <math>U{}^3_6</math> is bipartite but not Eulerian.\n\n==Computational complexity==\nIt is possible to test in [[polynomial time]] whether a given binary matroid is bipartite.<ref>{{citation\n | last1 = Lovász | first1 = László | author1-link = László Lovász\n | last2 = Seress | first2 = Ákos\n | doi = 10.1006/eujc.1993.1027\n | issue = 3\n | journal = European Journal of Combinatorics\n | mr = 1215334\n | pages = 241–250\n | title = The cocycle lattice of binary matroids\n | volume = 14\n | year = 1993}}.</ref> However, any algorithm that tests whether a given matroid is Eulerian, given access to the matroid via an [[matroid oracle|independence oracle]], must perform an exponential number of oracle queries, and therefore cannot take polynomial time.<ref>{{citation\n | last1 = Jensen | first1 = Per M.\n | last2 = Korte | first2 = Bernhard\n | doi = 10.1137/0211014\n | issue = 1\n | journal = [[SIAM Journal on Computing]]\n | mr = 646772\n | pages = 184–190\n | title = Complexity of matroid property algorithms\n | volume = 11\n | year = 1982}}.</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Matroid theory]]"
    },
    {
      "title": "Branch-decomposition",
      "url": "https://en.wikipedia.org/wiki/Branch-decomposition",
      "text": "[[File:Branch-decomposition.svg|thumb|upright=1.35|Branch decomposition of a [[grid graph]], showing an e-separation. The separation, the decomposition, and the graph all have width three.]]\nIn [[graph theory]], a '''branch-decomposition''' of an [[undirected graph]] ''G'' is a [[hierarchical clustering]] of the edges of ''G'', represented by an [[unrooted binary tree]] ''T'' with the edges of ''G'' as its leaves. Removing any edge from ''T'' partitions the edges of ''G'' into two subgraphs, and the width of the decomposition is the maximum number of shared vertices of any pair of subgraphs formed in this way. \nThe '''branchwidth''' of ''G'' is the minimum width of any branch-decomposition of ''G''.\n\nBranchwidth is closely related to [[tree decomposition|tree-width]]: for all graphs, both of these numbers are within a constant factor of each other, and both quantities may be characterized by [[forbidden graph characterization|forbidden minors]]. And as with treewidth, many graph optimization problems may be solved efficiently for graphs of small branchwidth. However, unlike treewidth, the branchwidth of [[planar graphs]] may be computed exactly, in [[polynomial time]]. Branch-decompositions and branchwidth may also be generalized from graphs to [[matroid]]s.\n\n==Definitions==\nAn [[unrooted binary tree]] is a connected undirected graph with no cycles in which each non-leaf node has exactly three neighbors. A branch-decomposition may be represented by an unrooted binary tree ''T'', together with a bijection between the leaves of ''T'' and the edges of the given graph ''G''&nbsp;=&nbsp;(''V'',''E'').\nIf ''e'' is any edge of the tree ''T'', then removing ''e'' from ''T'' partitions it into two subtrees ''T''<sub>1</sub> and ''T''<sub>2</sub>. This partition of ''T'' into subtrees induces a partition of the edges associated with the leaves of ''T'' into two subgraphs ''G''<sub>1</sub> and ''G''<sub>2</sub> of ''G''. This partition of ''G'' into two subgraphs is called an '''e-separation'''.\n\nThe width of an e-separation is the number of vertices of ''G'' that are incident both to an edge of ''E''<sub>1</sub> and to an edge of ''E''<sub>2</sub>; that is, it is the number of vertices that are shared by the two subgraphs ''G''<sub>1</sub> and ''G''<sub>2</sub>. The width of the branch-decomposition is the maximum width of any of its e-separations. The branchwidth of ''G'' is the minimum width of a branch-decomposition of ''G''.\n\n==Relation to treewidth==\nBranch-decompositions of graphs are closely related to [[tree decomposition]]s, and branch-width is closely related to [[tree decomposition|tree-width]]: the two quantities are always within a constant factor of each other. In particular, in the paper in which they introduced branch-width, [[Neil Robertson (mathematician)|Neil Robertson]] and [[Paul Seymour (mathematician)|Paul Seymour]]<ref>{{harvnb|Robertson|Seymour|1991}}, Theorem 5.1, p. 168.</ref> showed that for a graph ''G''\nwith tree-width ''k'' and branchwidth {{nowrap|''b'' > 1,}} \n:<math>b -1 \\le k \\le \\left\\lfloor\\frac{3}{2}b\\right\\rfloor -1.</math>\n\n==Carving width==\nCarving width is a concept defined similarly to branch width, except with edges replaced by vertices and vice versa. A carving decomposition is an unrooted binary tree with each leaf representing a vertex in the original graph, and the width of a cut is the number (or total weight in a weighted graph) of edges that are incident to a vertex in both subtrees.\n\nBranch width algorithms typically work by reducing to an equivalent carving width problem. In particular, the carving width of the [[medial graph]] of a planar graph is exactly twice the branch width of the original graph.<ref name=\"st94\">{{harvtxt|Seymour|Thomas|1994}}.</ref>\n\n==Algorithms and complexity==\nIt is [[NP-complete]] to determine whether a graph ''G'' has a branch-decomposition of width at most ''k'', when ''G'' and ''k'' are both considered as inputs to the problem.<ref name=\"st94\"/> However, the graphs with branchwidth at most ''k'' form a [[Minor (graph theory)|minor-closed family of graphs]],<ref>{{harvtxt|Robertson|Seymour|1991}}, Theorem 4.1, p. 164.</ref> from which it follows that computing the branchwidth is [[Parameterized complexity|fixed-parameter tractable]]: there is an algorithm for computing optimal branch-decompositions whose running time, on graphs of branchwidth ''k'' for any fixed constant ''k'', is linear in the size of the input graph.<ref>{{harvtxt|Bodlaender|Thilikos|1997}}. {{harvtxt|Fomin|Mazoit|Todinca|2009}} describe an algorithm with improved dependence on ''k'', (2{{radic|3}})<sup>''k''</sup>, at the expense of an increase in the dependence on the number of vertices from linear to quadratic.</ref>\n\nFor [[planar graph]]s, the branchwidth can be computed exactly in polynomial time. This in contrast to treewidth for which the complexity on planar graphs is a well known open problem.<ref>{{citation|title=\tEncyclopedia of Algorithms|editor-first=Ming-Yang|editor-last=Kao|publisher=Springer|year=2008|isbn=9780387307701|contribution=Treewidth of graphs|page=969|url=https://books.google.com/books?id=i3S9_GnHZwYC&pg=PA969|quote=Another long-standing open problem is whether there is a polynomial-time algorithm to compute the treewidth of planar graphs.}}</ref> The original algorithm for planar branchwidth, by [[Paul Seymour (mathematician)|Paul Seymour]] and [[Robin Thomas (mathematician)|Robin Thomas]], took time O(''n''<sup>2</sup>) on graphs with ''n'' vertices, and their algorithm for constructing a branch decomposition of this width took time O(''n''<sup>4</sup>).<ref name=\"st94\"/> This was later sped up to O(''n''<sup>3</sup>).{{sfnp|Gu|Tamaki|2008}}\n\nAs with treewidth, branchwidth can be used as the basis of [[dynamic programming]] algorithms for many NP-hard optimization problems, using an amount of time that is exponential in the width of the input graph or matroid.<ref>{{harvtxt|Hicks|2000}}; {{harvtxt|Hliněný|2003}}.</ref> For instance, {{harvtxt|Cook|Seymour|2003}} apply branchwidth-based dynamic programming to a problem of merging multiple partial solutions to the [[travelling salesman problem]] into a single global solution, by forming a sparse graph from the union of the partial solutions, using a [[spectral clustering]] heuristic to find a good branch-decomposition of this graph, and applying dynamic programming to the decomposition. {{harvtxt|Fomin|Thilikos|2006}} argue that branchwidth works better than treewidth in the development of fixed-parameter-tractable algorithms on planar graphs, for multiple reasons: branchwidth may be more tightly bounded by a function of the parameter of interest than the bounds on treewidth, it can be computed exactly in polynomial time rather than merely approximated, and the algorithm for computing it has no large hidden constants.\n\n==Generalization to matroids==\nIt is also possible to define a notion of branch-decomposition for [[matroid]]s that generalizes branch-decompositions of graphs.<ref>{{harvnb|Robertson|Seymour|1991}}. Section 12, \"Tangles and Matroids\", pp. 188–190.</ref> A branch-decomposition of a matroid is a hierarchical clustering of the matroid elements, represented as an unrooted binary tree with the elements of the matroid at its leaves. An e-separation may be defined in the same way as for graphs, and results in a partition of the set ''M'' of matroid elements into two subsets ''A'' and ''B''. If ρ denotes the [[Matroid rank|rank function]] of the matroid, then the width of an e-separation is defined as {{nowrap|ρ(''A'') + ρ(''B'') &minus; ρ(''M'') + 1}}, and the width of the decomposition and the branchwidth of the matroid are defined analogously. The branchwidth of a graph and the branchwidth of the corresponding [[graphic matroid]] may differ: for instance, the three-edge [[path graph]] and the three-edge [[star (graph theory)|star]] have different branchwidths, 2 and 1 respectively, but they both induce the same graphic matroid with branchwidth 1.<ref name=\"mt07\"/> However, for graphs that are not trees, the branchwidth of the graph is equal to the branchwidth of its associated graphic matroid.<ref>{{harvtxt|Mazoit|Thomassé|2007}}; {{harvtxt|Hicks|McMurray|2007}}.</ref> The branchwidth of a matroid is equal to the branchwidth of its [[dual matroid]], and in particular this implies that the branchwidth of any planar graph that is not a tree is equal to that of its dual.<ref name=\"mt07\">{{harvtxt|Mazoit|Thomassé|2007}}.</ref>\n\nBranchwidth is an important component of attempts to extend the theory of [[graph minor]]s to [[matroid minor]]s: although [[treewidth]] can also be generalized to matroids,<ref>{{harvtxt|Hliněný|Whittle|2006}}.</ref> and plays a bigger role than branchwidth in the theory of graph minors, branchwidth has more convenient properties in the matroid setting.<ref>{{harvtxt|Geelen|Gerards|Whittle|2006}}.</ref> Robertson and Seymour conjectured that the matroids representable over any particular [[finite field]] are [[well-quasi-ordering|well-quasi-ordered]], analogously to the [[Robertson–Seymour theorem]] for graphs, but so far this has been proven only for the matroids of bounded branchwidth.<ref>{{harvtxt|Geelen|Gerards|Whittle|2002}}; {{harvtxt|Geelen|Gerards|Whittle|2006}}.</ref> Additionally, if a minor-closed family of matroids representable over a finite field does not include the graphic matroids of all planar graphs, then there is a constant bound on the branchwidth of the matroids in the family, generalizing similar results for minor-closed graph families.<ref>{{harvtxt|Geelen|Gerards|Whittle|2006}}; {{harvtxt|Geelen|Gerards|Whittle|2007}}.</ref>\n\nFor any fixed constant ''k'', the matroids with branchwidth at most ''k'' can be recognized in [[polynomial time]] by an algorithm that has access to the matroid via an [[matroid oracle|independence oracle]].<ref>{{harvtxt|Oum|Seymour|2007}}.</ref>\n\n==Forbidden minors==\n[[File:Branchwidth 3-forbidden minors.svg|thumb|The four [[forbidden minor]]s for graphs of branchwidth three.]]\nBy the [[Robertson–Seymour theorem]], the graphs of branchwidth ''k'' can be characterized by a finite set of [[forbidden minor]]s. The graphs of branchwidth 0 are the [[matching (graph theory)|matchings]]; the minimal forbidden minors are a two-edge [[path graph]] and a triangle graph  (or the two-edge cycle, if multigraphs rather than simple graphs are considered).<ref name=\"rs91-4.2\"/> The graphs of branchwidth 1 are the graphs in which each [[connected component (graph theory)|connected component]] is a [[star (graph theory)|star]]; the minimal forbidden minors for branchwidth 1 are the triangle graph (or the two-edge cycle, if multigraphs rather than simple graphs are considered) and the three-edge path graph.<ref name=\"rs91-4.2\"/> The graphs of branchwidth 2 are the graphs in which each [[biconnected component]] is a [[series-parallel graph]]; the only minimal forbidden minor is the [[complete graph]] ''K''<sub>4</sub> on four vertices.<ref name=\"rs91-4.2\">{{harvtxt|Robertson|Seymour|1991}}, Theorem 4.2, p. 165.</ref> A graph has branchwidth three if and only if it has treewidth three and does not have the [[hypercube graph|cube graph]] as a minor; therefore, the four minimal forbidden minors are three of the four forbidden minors for treewidth three (the graph of the [[octahedron]], the complete graph ''K''<sub>5</sub>, and the [[Wagner graph]]) together with the cube graph.<ref>{{harvtxt|Bodlaender|Thilikos|1999}}. The fourth forbidden minor for treewidth three, the pentagonal prism, has the cube graph as a minor, so it is not minimal for branchwidth three.</ref>\n\nForbidden minors have also been studied for matroid branchwidth, despite the lack of a full analogue to the Robertson–Seymour theorem in this case. A matroid has branchwidth one if and only if every element is either a loop or a coloop, so the unique minimal forbidden minor is the [[uniform matroid]] U(2,3), the graphic matroid of the triangle graph. A matroid has branchwidth two if and only if it is the graphic matroid of a graph of branchwidth two, so its minimal forbidden minors are the graphic matroid of ''K''<sub>4</sub> and the non-graphic matroid U(2,4). The matroids of branchwidth three are not well-quasi-ordered without the additional assumption of representability over a finite field, but nevertheless the matroids with any finite bound on their branchwidth have finitely many minimal forbidden minors, all of which have a number of elements that is at most exponential in the branchwidth.<ref>{{harvtxt|Hall|Oxley|Semple|Whittle|2002}}; {{harvtxt|Geelen|Gerards|Robertson|Whittle|2003}}.</ref>\n\n==Notes==\n{{Commons category|Tree decomposition}}\n{{reflist|2}}\n\n==References==\n{{refbegin|2}}\n*{{citation\n | last1 = Bodlaender | first1 = Hans L. | author1-link = Hans L. Bodlaender\n | last2 = Thilikos | first2 = Dimitrios M.\n | contribution = Constructive linear time algorithms for branchwidth\n | doi = 10.1007/3-540-63165-8_217\n | pages = 627–637\n | publisher = Springer-Verlag\n | series = Lecture Notes in Computer Science\n | title = Proc. 24th International Colloquium on Automata, Languages and Programming (ICALP '97)\n | volume = 1256\n | year = 1997| hdl = 2117/96447}}.\n*{{citation\n | last1 = Bodlaender | first1 = Hans L. | author1-link = Hans L. Bodlaender\n | last2 = Thilikos | first2 = Dimitrios M.\n | doi = 10.1006/jagm.1999.1011\n | issue = 2\n | journal = Journal of Algorithms\n | pages = 167–194\n | title = Graphs with branchwidth at most three\n | volume = 32\n | year = 1999}}.\n*{{citation\n | doi = 10.1287/ijoc.15.3.233.16078\n | last1 = Cook | first1 = William\n | last2 = Seymour | first2 = Paul D. | author2-link = Paul Seymour (mathematician)\n | issue = 3\n | journal = INFORMS Journal on Computing\n | pages = 233–248\n | title = Tour merging via branch-decomposition\n | url = http://www.cs.utk.edu/~langston/projects/papers/tmerge.pdf\n | volume = 15\n | year = 2003}}.\n*{{citation\n | last1 = Fomin | first1 = Fedor V.\n | last2 = Thilikos | first2 = Dimitrios M.\n | doi = 10.1137/S0097539702419649\n | journal = SIAM Journal on Computing\n | page = 281\n | title = Dominating sets in planar graphs: branch-width and exponential speed-up\n | volume = 36\n | year = 2006\n | issue = 2}}.\n*{{citation\n | last1 = Fomin | first1 = Fedor V.\n | last2 = Mazoit | first2 = Frédéric\n | last3 = Todinca | first3 = Ioan\n | doi = 10.1016/j.dam.2008.08.009\n | issue = 12\n | journal = Discrete Applied Mathematics\n | pages = 2726–2736\n | title = Computing branchwidth via efficient triangulations and blocks\n | url = http://hal.archives-ouvertes.fr/hal-00390623/\n | volume = 157\n | year = 2009}}.\n*{{citation\n | last1 = Geelen | first1 = Jim | author1-link = Jim Geelen\n | last2 = Gerards | first2 = Bert\n | last3 = Robertson | first3 = Neil | author3-link = Neil Robertson (mathematician)\n | last4 = Whittle | first4 = Geoff\n | doi = 10.1016/S0095-8956(02)00046-1\n | issue = 2\n | journal = Journal of Combinatorial Theory, Series B\n | pages = 261–265\n | title = On the excluded minors for the matroids of branch-width ''k''\n | volume = 88\n | year = 2003}}.\n*{{citation\n | last1 = Geelen | first1 = Jim | author1-link = Jim Geelen\n | last2 = Gerards | first2 = Bert\n | last3 = Whittle | first3 = Geoff\n | doi = 10.1006/jctb.2001.2082\n | issue = 2\n | journal = Journal of Combinatorial Theory, Series B\n | pages = 270–290\n | title = Branch-width and well-quasi-ordering in matroids and graphs\n | volume = 84\n | year = 2002}}.\n*{{citation\n | last1 = Geelen | first1 = Jim | author1-link = Jim Geelen\n | last2 = Gerards | first2 = Bert\n | last3 = Whittle | first3 = Geoff\n | contribution = Towards a structure theory for matrices and matroids\n | pages = 827–842\n | title = Proc. [[International Congress of Mathematicians]]\n | contribution-url = http://www.icm2006.org/proceedings/Vol_III/contents/ICM_Vol_3_41.pdf\n | volume = III\n | year = 2006}}.\n*{{citation\n |last1=Geelen \n |first1=Jim \n |author1-link=Jim Geelen \n |last2=Gerards \n |first2=Bert \n |last3=Whittle \n |first3=Geoff \n |doi=10.1016/j.jctb.2007.02.005 \n |issue=6 \n |journal=Journal of Combinatorial Theory, Series B \n |pages=971–998 \n |title=Excluding a planar graph from GF(''q'')-representable matroids \n |url=http://www.math.uwaterloo.ca/~jfgeelen/publications/grid.pdf \n |volume=97 \n |year=2007 \n |deadurl=yes \n |archiveurl=https://web.archive.org/web/20100924110915/http://www.math.uwaterloo.ca/~jfgeelen/publications/grid.pdf \n |archivedate=2010-09-24 \n |df= \n}}.\n*{{citation\n | last1 = Gu | first1 = Qian-Ping\n | last2 = Tamaki | first2 = Hisao\n | date = July 2008\n | doi = 10.1145/1367064.1367070\n | issue = 3\n | journal = ACM Transactions on Algorithms\n | pages = 30:1–30:13\n | title = Optimal branch-decomposition of planar graphs in O(''n''<sup>3</sup>) time\n | volume = 4}}.\n*{{citation\n | last1 = Hall | first1 = Rhiannon\n | last2 = Oxley | first2 = James | author2-link = James Oxley\n | last3 = Semple | first3 = Charles\n | last4 = Whittle | first4 = Geoff\n | doi = 10.1006/jctb.2002.2120\n | issue = 1\n | journal = Journal of Combinatorial Theory, Series B\n | pages = 148–171\n | title = On matroids of branch-width three\n | volume = 86\n | year = 2002}}.\n*{{citation\n | last = Hicks | first = Illya V.\n | publisher = Rice University\n | series = Ph.D. thesis\n | title = Branch Decompositions and their Applications\n | url = http://www.caam.rice.edu/caam/trs/2000/TR00-17.ps\n | year = 2000}}.\n*{{citation\n | last1 = Hicks | first1 = Illya V.\n | last2 = McMurray | first2 = Nolan B., Jr.\n | doi = 10.1016/j.jctb.2006.12.007\n | issue = 5\n | journal = Journal of Combinatorial Theory, Series B\n | pages = 681–692\n | title = The branchwidth of graphs and their cycle matroids\n | volume = 97\n | year = 2007}}.\n*{{citation\n | last = Hliněný | first = Petr\n | contribution = On matroid properties definable in the MSO logic\n | doi = 10.1007/978-3-540-45138-9\\_41\n | pages = 470–479\n | publisher = Springer-Verlag\n | series = Lecture Notes in Computer Science\n | title = Proc. 28th International Symposium on Mathematical Foundations of Computer Science (MFCS '03)\n | volume = 2747\n | year = 2003}}\n*{{citation\n | last1 = Hliněný | first1 = Petr\n | last2 = Whittle | first2 = Geoff\n | doi = 10.1016/j.ejc.2006.06.005\n | issue = 7\n | journal = European Journal of Combinatorics\n | pages = 1117–1128\n | title = Matroid tree-width\n | url = http://www.fi.muni.cz/~hlineny/Research/papers/matr-tw-final.pdf\n | volume = 27\n | year = 2006}}. \n**Addendum and corrigendum: {{citation \n | last1 = Hliněný | first1 = Petr\n | last2 = Whittle | first2 = Geoff\n | title = Addendum to matroid tree-width \n | journal = European Journal of Combinatorics\n | volume=30\n | issue=4\n | pages=1036–1044 \n | year = 2009 \n | doi=10.1016/j.ejc.2008.09.028}}.\n*{{citation\n | last1 = Mazoit | first1 = Frédéric\n | last2 = Thomassé | first2 = Stéphan\n | contribution = Branchwidth of graphic matroids\n | editor1-last = Hilton | editor1-first = Anthony\n | editor2-last = Talbot | editor2-first = John\n | page = 275\n | publisher = Cambridge University Press\n | series = London Mathematical Society Lecture Note Series\n | title = Surveys in Combinatorics 2007\n | url = http://hal.archives-ouvertes.fr/docs/00/04/09/28/PDF/Branchwidth.pdf\n | volume = 346\n | year = 2007}}.\n*{{citation\n | last1 = Oum | first1 = Sang-il\n | last2 = Seymour | first2 = Paul | author2-link = Paul Seymour (mathematician)\n | doi = 10.1016/j.jctb.2006.06.006\n | issue = 3\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 2305892\n | pages = 385–393\n | series = Series B\n | title = Testing branch-width\n | volume = 97\n | year = 2007}}.\n*{{citation\n | last1 = Robertson | first1 = Neil | author1-link = Neil Robertson (mathematician)\n | last2 = Seymour | first2 = Paul D. | author2-link = Paul Seymour (mathematician)\n | doi = 10.1016/0095-8956(91)90061-N\n | issue = 2\n | journal = Journal of Combinatorial Theory\n | pages = 153–190\n | title = Graph minors. X. Obstructions to tree-decomposition\n | volume = 52\n | year = 1991}}.\n*{{citation\n | last1 = Seymour | first1 = Paul D. | author1-link = Paul Seymour (mathematician)\n | last2 = Thomas | first2 = Robin\n | doi = 10.1007/BF01215352\n | issue = 2\n | journal = Combinatorica\n | pages = 217–241\n | title = Call routing and the ratcatcher\n | volume = 14\n | year = 1994}}.\n{{refend}}\n\n[[Category:Trees (graph theory)]]\n[[Category:Graph minor theory]]\n[[Category:Graph invariants]]\n[[Category:Matroid theory]]"
    },
    {
      "title": "Circuit rank",
      "url": "https://en.wikipedia.org/wiki/Circuit_rank",
      "text": "[[File:6n-graf.svg|thumb|upright=1.3|This graph has circuit rank {{math|1=''r'' = 2}} because it can be made into a tree by removing two edges, for instance the edges 1–2 and 2–3, but removing any one edge leaves a cycle in the graph.]]\nIn [[graph theory]], a branch of [[mathematics]], the '''circuit rank''', '''cyclomatic number''', '''cycle rank''', or '''nullity''' of an [[undirected graph]] is the minimum number of  edges that must be removed from the graph to break all its [[cycle (graph theory)|cycles]], making it into a [[tree (graph theory)|tree]] or forest. It is equivalent to the minimal number of independent cycles in the graph (minimal [[cycle basis]]). Unlike the corresponding [[feedback arc set]] problem for [[directed graph]]s, the circuit rank {{mvar|r}} is easily computed using the formula\n:<math>r = m - n + c</math>,\nwhere {{mvar|m}} is the number of edges in the given graph, {{mvar|n}} is the number of [[vertex (graph theory)|vertices]], and {{mvar|c}} is the number of [[Connected component (graph theory)|connected components]].\n<ref name=\"berge\">{{citation|title=The Theory of Graphs|first=Claude|last=Berge|authorlink=Claude Berge|publisher=Courier Dover Publications|year=2001|isbn=9780486419756|pages=27–30|contribution=Cyclomatic number|url=https://books.google.com/books?id=h5BjnaoKyOwC&pg=PA27}}.</ref> It is also possible to construct a minimum-size set of edges that breaks all cycles efficiently, either using a [[greedy algorithm]] or by complementing a [[spanning forest]].\n\nThe circuit rank can be explained in terms of [[algebraic graph theory]] as the dimension of the [[cycle space]] of a graph, in terms of [[matroid theory]] as the corank of a [[graphic matroid]], and in terms of [[topology]] as one of the [[Betti number]]s of a topological space derived from the graph. It counts the ears in an [[ear decomposition]] of the graph, forms the basis of [[parameterized complexity]] on almost-trees, and has been applied in [[software metric]]s as part of the definition of [[cyclomatic complexity]] of a piece of code. Under the name of cyclomatic number, the concept was introduced by [[Gustav Kirchhoff]].<ref name=\"Kotiuga2010\">{{cite book|author=Peter Robert Kotiuga|title=A Celebration of the Mathematical Legacy of Raoul Bott|url=https://books.google.com/books?id=mqLXi0FRIZwC&pg=PA20|year=2010|publisher=American Mathematical Soc.|isbn=978-0-8218-8381-5|page=20}}</ref><ref name=\"Hage1996\">{{cite book|author=Per Hage|title=Island Networks: Communication, Kinship, and Classification Structures in Oceania|url=https://books.google.com/books?id=ZBdLknuP0BYC&pg=PA48|year=1996|publisher=Cambridge University Press|isbn=978-0-521-55232-5|page=48}}</ref>\n\n==Matroid rank and construction of a minimum feedback edge set==\nThe circuit rank of a graph {{mvar|G}} may be described using [[matroid theory]] as the [[matroid rank|corank]] of the [[graphic matroid]] of {{mvar|G}}.<ref>{{citation|title=Graphs and Hypergraphs|volume=6|series=North-Holland Mathematical Library|first=Claude|last=Berge|authorlink=Claude Berge|publisher=Elsevier|year=1976|isbn=9780720424539|page=477|url=https://books.google.com/books?id=Wy2mhanRnk4C&pg=PA477}}.</ref> Using the greedy property of matroids, this means that one can find a minimum set of edges that breaks all cycles using a [[greedy algorithm]] that at each step chooses an edge that belongs to at least one cycle of the remaining graph.\n\nAlternatively, a minimum set of edges that breaks all cycles can be found by constructing a [[spanning forest]] of {{mvar|G}} and choosing the [[Complement (set theory)|complementary]] set of edges that do not belong to the spanning forest.\n\n==The number of independent cycles==\nIn [[algebraic graph theory]], the circuit rank is also the dimension of the [[cycle space]] of <math>G</math>. Intuitively, this can be explained as meaning that the circuit rank counts the number of independent cycles in the graph, where a collection of cycles is independent if it is not possible to form one of the cycles as the [[symmetric difference]] of some subset of the others.<ref name=\"berge\"/>\n\nThis count of independent cycles can also be explained using [[homology theory]], a branch of [[topology]]. Any graph {{mvar|G}} may be viewed as an example of a 1-dimensional [[simplicial complex]], a type of [[topological space]] formed by representing each graph edge by a [[line segment]] and gluing these line segments together at their endpoints.\nThe cyclomatic number is the [[Rank of an abelian group|rank]] of the first (integer) [[homology group]] of this complex,<ref>{{citation|title=Trees|first=Jean-Pierre|last=Serre|authorlink=Jean-Pierre Serre|page=23|publisher=Springer|series=Springer Monographs in Mathematics|year=2003|url=https://books.google.com/books?id=MOAqeoYlBMQC&pg=PA23}}.</ref>\n:<math>r = \\operatorname{rank}\\left[H_1(G,\\Z)\\right].</math>\nBecause of this topological connection, the cyclomatic number of a graph {{mvar|G}} is also called the '''first [[Betti number]]''' of {{mvar|G}}.<ref name=\"BerkolaikoKuchment2013\">{{cite book|author1=Gregory Berkolaiko|author2=Peter Kuchment|title=Introduction to Quantum Graphs|url=https://books.google.com/books?id=QAs8tiBsvEoC&pg=PA4|year=2013|publisher=American Mathematical Soc.|isbn=978-0-8218-9211-4|pages=4}}</ref> More generally, the first Betti number of any topological space, defined in the same way, counts the number of independent cycles in the space.\n\n==Applications==\n===Meshedness coefficient===\nA variant of the circuit rank for [[planar graph]]s, normalized by dividing by the maximum possible circuit rank of any planar graph with the same vertex set, is called the [[meshedness coefficient]]. For a connected planar graph with {{mvar|m}} edges and {{mvar|n}} vertices, the meshedness coefficient can be computed by the formula<ref>{{citation\n | last1 = Buhl | first1 = J.\n | last2 = Gautrais | first2 = J.\n | last3 = Sole | first3 = R.V.\n | last4 = Kuntz | first4 = P.\n | last5 = Valverde | first5 = S.\n | last6 = Deneubourg | first6 = J.L.\n | last7 = Theraulaz | first7 = G.\n | doi = 10.1140/epjb/e2004-00364-9\n | issue = 1\n | journal = The European Physical Journal B\n | pages = 123–129\n | publisher = Springer-Verlag\n | title = Efficiency and robustness in ant networks of galleries\n | volume = 42\n | year = 2004}}.</ref>\n:<math>\\frac{m-n+1}{2n-5}.</math>\nHere, the numerator <math>m-n+1</math> of the formula is the circuit rank of the given graph, and the denominator <math>2n-5</math> is the largest possible circuit rank of an {{mvar|n}}-vertex planar graph. The meshedness coefficient ranges between 0 for trees and 1 for [[maximal planar graph]]s.\n\n===Ear decomposition===\nThe circuit rank controls the number of ears in an [[ear decomposition]] of a graph, a partition of the edges of the graph into paths and cycles that is useful in many graph algorithms.\nIn particular, a graph is [[k-vertex-connected graph|2-vertex-connected]] if and only if it has an open ear decomposition. This is a sequence of subgraphs, where the first subgraph is a simple cycle, the remaining subgraphs are all simple paths, each path starts and ends on vertices that belong to previous subgraphs,\nand each internal vertex of a path appears for the first time in that path. In any biconnected graph with circuit rank <math>r</math>, every open ear decomposition has exactly <math>r</math> ears.<ref>{{citation\n | last = Whitney | first = H. | authorlink = Hassler Whitney\n | journal = [[Transactions of the American Mathematical Society]]\n | pages = 339–362\n | title = Non-separable and planar graphs\n | volume = 34\n | year = 1932\n | doi=10.2307/1989545| pmc = 1076008}}. See in particular Theorems 18 (relating ear decomposition to circuit rank) and 19 (on the existence of ear decompositions).</ref>\n\n===Almost-trees===\nA graph with cyclomatic number <math>r</math> is also called a '''''r''-almost-tree''', because only ''r'' edges need to be removed from the graph to make it into a tree or forest.  A 1-almost-tree is a  '''near-tree''': a connected near-tree is a [[pseudoforest|pseudotree]], a cycle with a (possibly trivial) tree rooted at each vertex.<ref name=CMC349>{{citation | last=Brualdi | first=Richard A. | title=Combinatorial Matrix Classes | series=Encyclopedia of Mathematics and Its Applications | volume=108 | location=Cambridge | publisher=[[Cambridge University Press]] | year=2006 | isbn=0-521-86565-4 | zbl=1106.05001 | page=349}}</ref>\n\nSeveral authors have studied the [[parameterized complexity]] of graph algorithms on ''r''-near-trees, parameterized by <math>r</math>.<ref>{{citation\n | last1 = Coppersmith | first1 = Don | author1-link = Don Coppersmith\n | last2 = Vishkin | first2 = Uzi | author2-link = Uzi Vishkin\n | doi = 10.1016/0166-218X(85)90057-5 | zbl=0573.68017\n | issue = 1\n | journal = Discrete Applied Mathematics\n | pages = 27–45\n | title = Solving NP-hard problems in 'almost trees': Vertex cover\n | volume = 10\n | year = 1985}}.</ref><ref>{{citation\n | last1 = Fiala | first1 = Jiří\n | last2 = Kloks | first2 = Ton\n | last3 = Kratochvíl | first3 = Jan\n | doi = 10.1016/S0166-218X(00)00387-5 | zbl=0982.05085\n | issue = 1\n | journal = Discrete Applied Mathematics\n | pages = 59–72\n | title = Fixed-parameter complexity of λ-labelings\n | volume = 113\n | year = 2001}}.</ref>\n\n===Generalizations to directed graphs===\nThe [[cycle rank]] is an invariant of [[directed graph]]s that measures the level of nesting of cycles in the graph. It has a more complicated definition than circuit rank  (closely related to the definition of [[tree-depth]] for undirected graphs) and is more difficult to compute. Another problem for directed graphs related to the circuit rank is the minimum [[feedback arc set]], the smallest set of edges whose removal breaks all directed cycles. Both cycle rank and the minimum feedback arc set are [[NP-hard]] to compute.\n\nIt is also possible to compute a simpler invariant of directed graphs by ignoring the directions of the edges and computing the circuit rank of the underlying undirected graph. This principle forms the basis of the definition of [[cyclomatic complexity]], a software metric for estimating how complicated a piece of computer code is.\n\n===Computational chemistry===\nIn the fields of [[chemistry]] and [[cheminformatics]], the circuit rank of a [[molecular graph]] (the number of [[ring (chemistry)|rings]] in the [[smallest set of smallest rings]]) is sometimes referred to as the '''Frèrejacque number'''.<ref>{{cite journal|last1=May|first1=John W.|last2=Steinbeck|first2=Christoph|title=Efficient ring perception for the Chemistry Development Kit|journal=[[Journal of Cheminformatics]]|volume=6|issue=3|year=2014|doi=10.1186/1758-2946-6-3|pmid=24479757|pmc=3922685}}</ref><ref>{{cite journal|last1=Downs|first1=G.M.|last2=Gillet|first2=V.J.|last3=Holliday|first3=J.D.|last4=Lynch|first4=M.F.|year=1989|title=A review of ''Ring Perception Algorithms for Chemical Graphs''|journal=[[J. Chem. Inf. Comput. Sci.]]|volume=29|issue=3|pages=172–187|doi=10.1021/ci00063a007}}</ref><ref>{{cite journal|first1=Marcel|last1=Frèrejacque|title=No. 108-Condensation d'une molecule organique|trans-title=Condenstation of an organic molecule|journal=[[Bull. Soc. Chim. Fr.]]|volume=5|pages=1008–1011|year=1939}}</ref>\n\n==Related concepts==\nOther numbers defined in terms of edge deletion from undirected graphs include the [[edge connectivity]], the minimum number of edges to delete in order to disconnect the graph, and [[matching preclusion]], the minimum number of edges to delete in order to prevent the existence of a [[perfect matching]].\n\n==References==\n{{reflist}}\n\n[[Category:Graph invariants]]\n[[Category:Matroid theory]]\n[[Category:Spanning tree]]"
    },
    {
      "title": "Colored matroid",
      "url": "https://en.wikipedia.org/wiki/Colored_matroid",
      "text": "In [[mathematics]], a '''colored matroid''' is a [[matroid]] whose elements are labeled from a set of colors, which can be any set that suits the purpose, for instance the set of the first ''n'' positive integers, or the sign set {+, &minus;}.\n\nThe interest in colored matroids is through their invariants, especially the colored [[Tutte polynomial]],<ref>{{citation\n | last = Zaslavsky | first = Thomas\n | doi = 10.2307/2153985\n | issue = 1\n | journal = Transactions of the American Mathematical Society\n | mr = 1080738\n | pages = 317–347\n | title = Strong Tutte functions of matroids and graphs\n | volume = 334\n | year = 1992| jstor = 2153985\n }}.</ref> which generalizes the Tutte polynomial of a [[signed graph]] of {{harvtxt|Kauffman|1989}}.<ref>{{citation\n | last = Kauffman | first = Louis H.\n | doi = 10.1016/0166-218X(89)90049-8\n | issue = 1–2\n | journal = Discrete Applied Mathematics\n | mr = 1031266\n | pages = 105–127\n | title = A Tutte polynomial for signed graphs\n | volume = 25\n | year = 1989}}.</ref>\n\nThere has also been study of optimization problems on matroids where the objective function of the optimization depends on the set of colors chosen as part of a matroid basis.<ref>{{citation\n | last1 = Maffioli | first1 = Francesco\n | last2 = Rizzi | first2 = Romeo\n | last3 = Benati | first3 = Stefano\n | doi = 10.1016/j.dam.2007.04.015\n | issue = 15\n | journal = Discrete Applied Mathematics\n | mr = 2351979\n | pages = 1958–1970\n | title = Least and most colored bases\n | volume = 155\n | year = 2007}}.</ref>\n\n==See also==\n*[[Bipartite matroid]]\n*[[Rota's basis conjecture]]\n\n==References==\n{{reflist}}\n\n[[Category:Matroid theory]]\n\n\n{{Combin-stub}}"
    },
    {
      "title": "Corank",
      "url": "https://en.wikipedia.org/wiki/Corank",
      "text": "In [[mathematics]], '''corank''' is complementary to the concept of the rank of a mathematical object, and may refer to the dimension of the left [[Nullspace|nullspace of a matrix]], the dimension of the [[cokernel]] of a linear transformation of a vector space, or the number of elements of a [[Matroid rank|matroid minus its rank]]. \n\n== Left nullspace of a matrix ==\nThe corank of an <math>m\\times n</math> [[Matrix (mathematics)|matrix]] is <math>n-r</math> where <math>r</math> is the [[rank (linear algebra)|rank]] of the matrix. Equivalently, by the [[fundamental theorem of linear algebra]], it is the dimension of the left nullspace of the matrix.\n\n== Cokernel of a linear transformation ==\nGeneralizing matrices to [[linear transformation]]s of [[vector space]]s, the corank of a linear transformation is the dimension of the [[cokernel]] of the transformation, which is the quotient of the [[codomain]] by the image of the transformation.\n\n== Matroid ==\nFor a [[matroid]] with <math>n</math> elements and [[matroid rank]] <math>r</math>, the corank or nullity of the matroid is <math>n-r</math>. In the case of [[linear matroid]]s this coincides with the matrix corank. In the case of [[graphic matroid]]s the corank is also known as the [[circuit rank]] or cyclomatic number.\n\n==See also==\n*[[Sylvester's law of inertia]]\n\n{{sia|mathematics}}\n\n[[Category:Linear algebra]]\n[[Category:Matroid theory]]"
    },
    {
      "title": "Coxeter matroid",
      "url": "https://en.wikipedia.org/wiki/Coxeter_matroid",
      "text": "In mathematics, '''Coxeter matroids''' are generalization of [[matroid]]s depending on a choice of a [[Coxeter group]] ''W'' and a [[parabolic subgroup]]&nbsp;''P''. Ordinary matroids correspond to the case when ''P'' is a maximal parabolic subgroup of a  symmetric group ''W''. They were introduced by {{harvs|txt|last1=Gelfand|last2=Serganova|year1=1987|year2=1987b}}, who named them after [[H. S. M. Coxeter]]. \n\n{{harvtxt|Borovik|Gelfand|White|2003}} give a detailed account of Coxeter matroids.\n\n==Definition==\n\nSuppose that ''W'' is a Coxeter group, generated by a set ''S'' of involutions, and ''P'' is a parabolic subgroup (the subgroup generated by some subset of ''S''). A '''Coxeter matroid''' is a subset ''M'' of ''W''/''P'' that for every ''w'' in ''W'', ''M'' contains a unique minimal element with respect to the ''w''-[[Bruhat order]].\n\n==Relation to matroids==\n\nSuppose that the Coxeter group ''W'' is the [[symmetric group]] ''S''<sub>''n''</sub> and ''P'' is the parabolic subgroup ''S''<sub>''k''</sub>×''S''<sub>''n''–''k''</sub>. Then ''W''/''P'' can be identified with the ''k''-element subsets of the ''n''-element set {1,2,...,''n''} and the elements ''w'' of ''W'' correspond to the linear orderings of this set. A Coxeter matroid consists of ''k'' elements sets such that for each ''w'' there is a unique minimal element in the corresponding Bruhat ordering of ''k''-element subsets. This is exactly the definition of a matroid of rank ''k'' on an ''n''-element set in terms of bases: a matroid can be defined as some ''k''-element subsets called bases of an ''n''-element set such that for each linear ordering of the set there is a unique minimal base in the [[Gale ordering]] of ''k''-element subsets.\n\n==References==\n\n*{{Citation | last1=Borovik | first1=Alexandre V. | last2=Gelfand | first2=I. M. | last3=White | first3=Neil | title=Coxeter matroids | publisher=Birkhäuser Boston | location=Boston, MA | series=Progress in Mathematics | isbn=978-0-8176-3764-4 | doi=10.1007/978-1-4612-2066-4 |mr=1989953 | year=2003 | volume=216}}\n*{{Citation | last1=Gelfand | first1=I. M. | last2=Serganova | first2=V. V. | author2-link = Vera Serganova | title=On the general definition of a matroid and a greedoid | language=Russian |mr=871945 | year=1987 | journal=Doklady Akademii Nauk SSSR | issn=0002-3264 | volume=292 | issue=1 | pages=15–20}}\n*{{Citation | last1=Gelfand | first1=I. M. | last2=Serganova | first2=V. V. | author2-link = Vera Serganova | title=Combinatorial geometries and the strata of a torus on homogeneous compact manifolds | doi= 10.1070/RM1987v042n02ABEH001308 |mr=0898623 | year=1987b | journal=Akademiya Nauk SSSR I Moskovskoe Matematicheskoe Obshchestvo. Uspekhi Matematicheskikh Nauk | issn=0042-1316 | volume=42 | issue=2 | pages=107–134}} – English translation in Russian Mathematical Surveys 42 (1987), no. 2, 133–168\n\n[[Category:Matroid theory]]\n[[Category:Coxeter groups]]"
    },
    {
      "title": "Cryptomorphism",
      "url": "https://en.wikipedia.org/wiki/Cryptomorphism",
      "text": "In [[mathematics]], two objects, especially systems of axioms or semantics for them, are called '''cryptomorphic''' if they are equivalent but not obviously equivalent.  This word is a play on the many [[morphism]]s in mathematics, but \"cryptomorphism\" is only very distantly related to \"[[isomorphism]]\", \"[[homomorphism]]\", or \"morphisms\". The equivalence may possibly be in some informal sense, or may be formalized in terms of a [[bijection]] or [[equivalence of categories]] between the mathematical objects defined by the two cryptomorphic axiom systems.\n\n==Etymology==\n\nThe word was coined by [[Garrett Birkhoff]] before 1967, for use in the third edition of his book ''Lattice Theory''.  Birkhoff did not give it a formal definition, though others working in the field have made some attempts since.\n\n==Use in matroid theory==\n\nIts informal sense was popularized (and greatly expanded in scope) by [[Gian-Carlo Rota]] in the context of [[matroid]] theory: there are dozens of equivalent axiomatic approaches to matroids, but two different systems of axioms often look very different.  \n\nIn his 1997 book ''Indiscrete Thoughts'', Rota describes the situation as follows:\n\n{{cquote|Like many another great idea, matroid theory was invented by one of the great American pioneers, [[Hassler Whitney]].  His paper, which is still today the best entry to the subject, flagrantly reveals the unique peculiarity of this field, namely, the exceptional variety of cryptomorphic definitions for a matroid, embarrassingly unrelated to each other and exhibiting wholly different mathematical pedigrees.  It is as if one were to condense all trends of present day mathematics onto a single finite structure, a feat that anyone would ''a priori'' deem impossible, were it not for the fact that matroids do exist.}}\n\nThough there are many cryptomorphic concepts in mathematics outside of matroid theory and [[universal algebra]], the word has not caught on among mathematicians generally.  It is, however, in fairly wide use among researchers in matroid theory.\n\n==See also==\n*[[Combinatorial class]], an equivalence among [[combinatorial enumeration]] problems hinting at the existence of a cryptomorphism\n\n==References==\n\n* Birkhoff, G.:  ''Lattice Theory'',  3rd edition. American Mathematical Society Colloquium Publications, Vol. XXV. 1967.\n* Crapo, H. and Rota, G-C.: ''On the foundations of combinatorial theory: Combinatorial geometries.''  M.I.T. Press, Cambridge, Mass. 1970.\n* Elkins, James: Chapter ''Cryptomorphs'' in ''Why Are Our Pictures Puzzles?: On the Modern Origins of Pictorial Complexity'', 1999\n* Rota, G-C.:  ''Indiscrete Thoughts'', Birkhäuser Boston, Inc., Boston, MA. 1997.\n* White, N., editor:  ''Theory of Matroids'', Encyclopedia of Mathematics and its Applications, 26.  Cambridge University Press, Cambridge. 1986.\n\n[[Category:Mathematical terminology]]\n[[Category:Matroid theory]]"
    },
    {
      "title": "Delta-matroid",
      "url": "https://en.wikipedia.org/wiki/Delta-matroid",
      "text": "In mathematics, a '''delta-matroid''' or '''Δ-matroid''' is a [[family of sets]] obeying an exchange axiom generalizing an axiom of [[matroid]]s. A non-empty family of sets is a delta-matroid if, for every two sets <math>E</math> and <math>F</math> in the family, and for every element <math>e</math> in their [[symmetric difference]] <math>E\\triangle F</math>, there exists an <math>f\\in E\\triangle F</math> such that <math>E\\triangle\\{e,f\\}</math> is in the family. For the basis sets of a matroid, the corresponding exchange axiom requires in addition that <math>e\\in E</math> and <math>f\\in F</math>, ensuring that <math>E</math> and <math>F</math> have the same cardinality. For a delta-matroid, either of the two elements may belong to either of the two sets, and it is also allowed for the two elements to be equal.{{r|chun}}\nAn alternative an equivalent definition is that a family of sets forms a delta-matroid when the [[convex hull]] of its [[indicator vector]]s (the analogue of a [[matroid polytope]]) has the property that every edge length is either one or the [[square root of two]].\n\nDelta-matroids were defined by André Bouchet in 1987.{{r|bouchet}}\nAlgorithms for [[matroid intersection]] and the [[matroid parity problem]] can be extended to some cases of delta-matroids.{{r|bj|gim}}\n\nDelta-matroids have also been used to study [[constraint satisfaction problem]]s.{{r|ff}} As a special case, an ''even delta-matroid'' is a delta-matroid in which all sets have an even number of elements. If a constraint satisfaction problem has a [[Boolean variable]] on each edge of a planar graph, and if the variables of the edges incident to each vertex of the graph are constrained to belong to an even delta-matroid (possibly a different even delta-matroid for each vertex), then the problem can be solved in [[polynomial time]]. This result plays a key role in a characterization of the planar Boolean constraint satisfaction problems that can be solved in polynomial time.{{r|kkr}}\n\n==References==\n{{reflist|refs=\n\n<ref name=bj>{{citation\n | last1 = Bouchet | first1 = André\n | last2 = Jackson | first2 = Bill\n | journal = Electronic Journal of Combinatorics\n | mr = 1741336\n | page = R14:1–R14:22\n | title = Parity systems and the delta-matroid intersection problem\n | url = https://www.combinatorics.org/Volume_7/Abstracts/v7i1r14.html\n | volume = 7\n | year = 2000}}</ref>\n\n<ref name=bouchet>{{citation\n | last = Bouchet | first = André\n | doi = 10.1007/BF02604639\n | issue = 2\n | journal = Mathematical Programming\n | mr = 904585\n | pages = 147–159\n | title = Greedy algorithm and symmetric matroids\n | volume = 38\n | year = 1987}}</ref>\n\n<ref name=chun>{{citation|url=http://matroidunion.org/?p=1882|title=Delta-matroids: Origins|date=July 13, 2016|first=Carolyn|last=Chun|work=The Matroid Union}}</ref>\n\n<ref name=ff>{{citation\n | last1 = Feder | first1 = Tomás\n | last2 = Ford | first2 = Daniel\n | doi = 10.1137/S0895480104445009\n | issue = 2\n | journal = SIAM Journal on Discrete Mathematics\n | mr = 2257268\n | pages = 372–394\n | title = Classification of bipartite Boolean constraint satisfaction through delta-matroid intersection\n | volume = 20\n | year = 2006}}</ref>\n\n<ref name=gim>{{citation\n | last1 = Geelen | first1 = James F.\n | last2 = Iwata | first2 = Satoru\n | last3 = Murota | first3 = Kazuo\n | doi = 10.1016/S0095-8956(03)00039-X\n | issue = 2\n | journal = Journal of Combinatorial Theory\n | mr = 1983366\n | pages = 377–398\n | series = Series B\n | title = The linear delta-matroid parity problem\n | volume = 88\n | year = 2003}}</ref>\n\n<ref name=kkr>{{citation\n | last1 = Kazda | first1 = Alexandr\n | last2 = Kolmogorov | first2 = Vladimir\n | last3 = Rolínek | first3 = Michal\n | arxiv = 1602.03124\n | date = December 2018\n | doi = 10.1145/3230649\n | issue = 2\n | journal = ACM Transactions on Algorithms\n | pages = 22:1–22:33\n | title = Even delta-matroids and the complexity of planar Boolean CSPs\n | volume = 15}}</ref>\n\n}}\n\n[[Category:Matroid theory]]"
    },
    {
      "title": "Dual matroid",
      "url": "https://en.wikipedia.org/wiki/Dual_matroid",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Abstract algebraic and combinatorial construction}}\nIn [[matroid theory]], the '''dual''' of a matroid <math>M</math> is another matroid <math>M^\\ast</math> that has the same elements as <math>M</math>, and in which a set is independent if and only if <math>M</math> has a basis set disjoint from it.<ref name=\"s03\">{{citation\n | last = Schrijver | first = Alexander | author-link = Alexander Schrijver\n | isbn = 3-540-44389-4\n | location = Berlin\n | mr = 1956925\n | pages = 652\n | publisher = Springer-Verlag\n | series = Algorithms and Combinatorics\n | title = Combinatorial Optimization: Polyhedra and Efficiency. Vol. B: Matroids, Trees, Stable Sets\n | url = https://books.google.com/books?id=mqGeSQ6dJycC&pg=RA1-PA652\n | volume = 24\n | year = 2003}}.</ref><ref>{{citation\n | last = Welsh | first = D. J. A. | authorlink = Dominic Welsh\n | isbn = 9780486474397\n | page = 34\n | publisher = Courier Dover Publications\n | title = Matroid Theory\n | url = https://books.google.com/books?id=QL2iYMBLpFwC&pg=PA222\n | year = 2010}}.</ref><ref name=\"oxley\">{{citation\n | last = Oxley | first = James G. | authorlink = James Oxley\n | isbn = 9780199202508\n | pages = 69–70\n | publisher = Oxford University Press\n | series = Oxford Graduate Texts in Mathematics\n | title = Matroid Theory\n | url = https://books.google.com/books?id=puKta1Hdz-8C&pg=PA69\n | volume = 3\n | year = 2006}}.</ref>\n\nMatroid duals go back to the original paper by [[Hassler Whitney]] defining matroids.<ref name=\"w35\">{{citation|last=Whitney|first=Hassler|authorlink=Hassler Whitney|year=1935|title=On the abstract properties of linear dependence|journal=American Journal of Mathematics|volume=57|pages=509–533|doi=10.2307/2371182|issue=3|publisher=The Johns Hopkins University Press|mr=1507091|jstor=2371182}}. Reprinted in {{harvtxt|Kung|1986}}, pp.&nbsp;55–79. See in particular section 11, \"Dual matroids\", pp. 521–524.</ref> They generalize to matroids the notions of [[dual graph|plane graph duality]].\n\n==Basic properties==\nDuality is an [[Involution (mathematics)|involution]]: for all <math>M</math>, <math>(M^\\ast)^\\ast=M</math>.<ref name=\"s03\"/><ref name=\"oxley\"/><ref name=\"w35\"/>\n\nAn alternative definition of the dual matroid is that its basis sets are the [[complement (set theory)|complements]] of the basis sets of <math>M</math>. The basis exchange axiom, used to define matroids from their bases, is self-complementary, so the dual of a matroid is necessarily a matroid.<ref name=\"oxley\"/>\n\nThe [[Matroid_theory#Flats|flats]] of <math>M</math> are complementary to the cyclic sets (unions of circuits) of <math>M^\\ast</math>, and vice versa.<ref name=\"oxley\"/>\n\nIf <math>r</math> is the [[matroid rank|rank function]] of a matroid <math>M</math> on ground set <math>E</math>, then the rank function of the dual matroid is <math>r^\\ast(S)=r(E \\setminus S)+|S|-r(E)</math>.<ref name=\"s03\"/><ref name=\"oxley\"/><ref name=\"w35\"/>\n\n==Minors==\nA [[matroid minor]] is formed from a larger matroid <math>M</math> by two operations: the restriction <math>M\\setminus x</math> deletes element <math>x</math> from <math>M</math> without changing the independence or rank of the remaining sets, and the contraction <math>M/x</math> deletes <math>x</math> from <math>M</math> after subtracting one from the rank of every set it belongs to. These two operations are dual: <math>M\\setminus X=(M^\\ast/x)^\\ast</math> and <math>M/X=(M^\\ast\\setminus x)^\\ast</math>. Thus, a minor of a dual is the same thing as a dual of a minor.<ref>{{harvtxt|Schrijver|2003}}, p. 653.</ref>\n\n==Self-dual matroids==\nAn individual matroid is self-dual (generalizing e.g. the [[Dual polyhedron#Self-dual polyhedra|self-dual polyhedra]] for graphic matroids) if it is isomorphic to its own dual. The isomorphism may, but is not required to, leave the elements of the matroid fixed. Any algorithm that tests whether a given matroid is self-dual, given access to the matroid via an [[matroid oracle|independence oracle]], must perform an exponential number of oracle queries, and therefore cannot take polynomial time.<ref>{{citation\n | last1 = Jensen | first1 = Per M.\n | last2 = Korte | first2 = Bernhard\n | doi = 10.1137/0211014\n | issue = 1\n | journal = [[SIAM Journal on Computing]]\n | mr = 646772\n | pages = 184–190\n | title = Complexity of matroid property algorithms\n | volume = 11\n | year = 1982}}.</ref>\n\n==Matroid families==\nMany important matroid families are self-dual, meaning that a matroid belongs to the family if and only if its dual does. Many other matroid families come in dual pairs. Examples of this phenomenon include:\n*The [[binary matroid]]s (matroids representable over [[GF(2)]]), the matroids representable over any other field, and the [[regular matroid]]s, are all self-dual families.<ref>{{harvtxt|Whitney|1935}}, Section 13, \"Orthogonal hyperplanes and dual matroids\".</ref>\n*The [[gammoid]]s form a self-dual family. The strict gammoids are dual to the [[transversal matroid]]s.<ref>{{harvtxt|Schrijver|2003}}, pp. 659–661; {{harvtxt|Welsh|2010}}, pp. 222–223.</ref>\n*The [[uniform matroid]]s and [[partition matroid]]s are self-dual. The dual to a uniform matroid <math>U{}^r_n</math> is the uniform matroid <math>U{}^{n-r}_n</math>.<ref>{{harvtxt|Oxley|2006}}, pp. 77 & 111.</ref>\n*The dual of a [[graphic matroid]] is itself graphic if and only if the underlying graph is planar; the matroid of the dual of a planar graph is the same as the dual of the matroid of the graph. Thus, the family of graphic matroids of planar graphs is self-dual.<ref>{{citation\n | last = Tutte | first = W. T.\n | journal = Journal of Research of the National Bureau of Standards\n | mr = 0179781\n | pages = 1–47\n | title = Lectures on matroids\n | url = http://cdm16009.contentdm.oclc.org/cdm/ref/collection/p13011coll6/id/66650\n | volume = 69B\n | year = 1965\n | doi=10.6028/jres.069b.001}}.</ref>\n*Among the graphic matroids, and more generally among the binary matroids, the [[bipartite matroid]]s (matroids in which every circuit is even) are dual to the [[Eulerian matroid]]s (matroids that can be partitioned into disjoint circuits).<ref>{{citation\n | last = Welsh | first = D. J. A. | authorlink = Dominic Welsh\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 0237368\n | pages = 375–377\n | title = Euler and bipartite matroids\n | volume = 6\n | issue = 4 | year = 1969\n | doi=10.1016/s0021-9800(69)80033-5}}.</ref><ref>{{citation\n | last1 = Harary | first1 = Frank | author1-link = Frank Harary\n | last2 = Welsh | first2 = Dominic | author2-link = Dominic Welsh\n | contribution = Matroids versus graphs\n | doi = 10.1007/BFb0060114\n | location = Berlin\n | mr = 0263666\n | pages = 155–170\n | publisher = Springer\n | series = Lecture Notes in Mathematics\n | title = The Many Facets of Graph Theory (Proc. Conf., Western Mich. Univ., Kalamazoo, Mich., 1968)\n | volume = 110\n | year = 1969| isbn = 978-3-540-04629-5 }}.</ref>\nIt is  an open problem whether the family of [[algebraic matroid]]s is self-dual.\n\n==References==\n{{reflist}}\n\n[[Category:Matroid theory]]\n[[Category:Duality theories|Matroid]]"
    },
    {
      "title": "Ear decomposition",
      "url": "https://en.wikipedia.org/wiki/Ear_decomposition",
      "text": "[[File:Ear decomposition.png|thumb|An example of an ear decomposition of a graph containing 3 ears.]]\nIn [[graph theory]], an '''ear''' of an [[undirected graph]] ''G'' is a [[path (graph theory)|path]] ''P'' where the two endpoints of the path may coincide, but where otherwise no repetition of edges or vertices is allowed, so every internal vertex of ''P'' has [[degree (graph theory)|degree]] two in ''P''. An '''ear decomposition''' of an undirected graph ''G'' is a [[Partition of a set|partition]] of its set of edges into a sequence of ears, such that the one or two endpoints of each ear belong to earlier ears in the sequence and such that the internal vertices of each ear do not belong to any earlier ear. Additionally, in most cases the first ear in the sequence must be a cycle. An '''open ear decomposition''' or a '''proper ear decomposition''' is an ear decomposition in which the two endpoints of each ear after the first are distinct from each other.\n\nEar decompositions may be used to characterize several important graph classes, and as part of efficient [[graph algorithm]]s. They may also be generalized from graphs to [[matroid]]s.\n\n==Characterizing graph classes==\nSeveral important classes of graphs may be characterized as the graphs having certain types of ear decompositions.\n\n===Graph connectivity===\nA graph is [[k-vertex-connected graph|''k''-vertex-connected]] if the removal of any (''k''&nbsp;&minus;&nbsp;1) vertices leaves a connected subgraph, and [[k-edge-connected graph|''k''-edge-connected]] if the removal of any (''k''&nbsp;&minus;&nbsp;1) edges leaves a connected subgraph.\n\nThe following result is due to {{harvs|first=Hassler|last=Whitney|authorlink=Hassler Whitney|year=1932|txt}}:\n:A graph <math> G=(V,E)  </math> with <math> |E|\\geq 2 </math> is 2-vertex-connected if and only if it has an open ear decomposition.\n\nThe following result is due to {{harvs|first=Herbert|last=Robbins|authorlink=Herbert Robbins|year=1939|txt}}:\n:A graph is 2-edge-connected if and only if it has an ear decomposition.\n\nIn both cases the number of ears is necessarily equal to the [[circuit rank]] of the given graph. Robbins introduced the ear decomposition of 2-edge-connected graphs as a tool for proving the [[Robbins theorem]], that these are exactly the graphs that may be given a [[strongly connected]] orientation. Because of the pioneering work of Whitney and Robbins on ear decompositions, an ear decomposition is sometimes also called a '''Whitney–Robbins synthesis''' {{harv|Gross|Yellen|2006}}.\n\nA '''non-separating ear decomposition''' is an open ear decomposition such that, for each vertex ''v'' with only one exception, ''v'' has a neighbor whose first appearance in the decomposition is in a later ear than the first appearance of ''v''. This type of ear decomposition may be used to generalize Whitney's result:\n:A graph <math> G=(V,E)  </math> with <math> |V|\\geq 2 </math> is 3-vertex-connected if and only if ''G'' has a non-separating ear decomposition.\nIf such a decomposition exists, it can be chosen with respect to a particular edge ''uv'' of ''G'' in such a way that ''u'' is in the first ear, ''v'' is the new vertex in the last ear with more than one edge, and ''uv'' is a single-edge ear.\nThis result was first stated explicitly by {{harvtxt|Cheriyan|Maheshwari|1988}}, but as {{harvtxt|Schmidt|2013b}} describes, it is equivalent to a result in the 1971 Ph.D. thesis of Lee Mondshein. Structures closely related to non-separating ear decompositions of maximal planar graphs, called canonical orderings, are also a standard tool in [[graph drawing]].\n\n===Strong connectivity of directed graphs===\nThe above definitions can also be applied to [[directed graph]]s. An '''ear''' would then be  a directed path where all internal vertices have [[indegree]] and [[outdegree]] equal to 1. A directed graph is [[strongly connected]] if it contains a directed path from every vertex to every other vertex. Then we have the following theorem:\n:A directed graph is strongly connected if and only if it has an ear decomposition.\nSimilarly, a directed graph is [[biconnected graph|biconnected]] if, for every two vertices, there exists a simple cycle in the graph containing both of them. Then\n:A directed graph is biconnected if and only if it has an open ear decomposition.\n\n===Factor-critical graphs===\nAn ear decomposition is ''odd'' if each of its ears uses an odd number of edges. A [[factor-critical graph]] is a graph with an odd number of vertices, such that for each vertex ''v'', if ''v'' is removed from the graph then the remaining vertices have a [[perfect matching]]. {{harvs|first=László|last=Lovász|authorlink=László Lovász|year=1972|txt}} found that:\n:A graph ''G'' is factor-critical if and only if ''G'' has an odd ear decomposition.\nMore generally, a result of {{harvtxt|Frank|1993}} makes it possible to find in any graph ''G'' the ear decomposition with the fewest even ears.\n\n===Series-parallel graphs===\nA ''tree'' ear decomposition is a proper ear decomposition in which the first ear is a single edge and for each subsequent ear <math> P_i </math>, there is a single ear <math> P_j </math>, <math> i>j </math>, such that both endpoints of <math> P_i </math> lie on <math> P_j </math> {{harv|Khuller|1989}}. A ''nested'' ear decomposition is a tree ear decomposition such that, within each ear <math> P_j </math>, the set of pairs of endpoints of other ears <math> P_i </math> that lie within <math> P_j </math> form a set of nested intervals. A [[series-parallel graph]] is a graph with two designated terminals ''s'' and ''t'' that can be formed recursively by combining smaller series-parallel graphs in one of two ways: series composition (identifying one terminal from one smaller graph with one terminal from the other smaller graph, and keeping the other two terminals as the terminals of the combined graph) and parallel composition (identifying both pairs of terminals from the two smaller graphs).\n\nThe following result is due to {{harvs|first=David|last=Eppstein|authorlink=David Eppstein|year=1992|txt}}:\n:A 2-vertex-connected graph is series-parallel if and only if it has a nested ear decomposition.\nMoreover, any open ear decomposition of a 2-vertex-connected series-parallel graph must be nested. The result may be extended to series-parallel graphs that are not 2-vertex-connected by using open ear decompositions that start with a path between the two terminals.\n\n==Matroids==\nThe concept of an ear decomposition can be extended from graphs to [[matroid]]s. An ear decomposition of a matroid is defined to be a sequence of circuits of the matroid, with two properties:\n*each circuit in the sequence has a nonempty intersection with the previous circuits, and\n*each circuit in the sequence remains a circuit even if all previous circuits in the sequence are contracted.\nWhen applied to the [[graphic matroid]] of a graph ''G'', this definition of an ear decomposition coincides with the definition of a proper ear decomposition of ''G'': improper decompositions are excluded by the requirement that each circuit include at least one edge that also belongs to previous circuits. Using this definition, a matroid may be defined as factor-critical when it has an ear decomposition in which each circuit in the sequence has an odd number of new elements {{harv|Szegedy|Szegedy|2006}}.\n\n==Algorithms==\n\nOn classical computers, ear decompositions of 2-edge-connected graphs and open ear decompositions of 2-vertex-connected graphs may be found by [[greedy algorithms]] that find each ear one at a time. A simple greedy approach that computes at the same time ear decompositions, open ear decompositions, st-numberings and -orientations in linear time (if exist) is given in {{harvtxt|Schmidt|2013a}}. The approach is based on computing a special ear decomposition named [[chain decomposition]] by one path-generating rule. {{harvtxt|Schmidt|2013b}} shows that non-separating ear decompositions may also be constructed in linear time.\n\n{{harvtxt|Lovász|1985}}, {{harvtxt|Maon|Schieber|Vishkin|1986}}, and {{harvtxt|Miller|Ramachandran|1986}} provided efficient [[parallel algorithm]]s for constructing ear decompositions of various types. For instance, to find an ear decomposition of a 2-edge-connected graph, the algorithm of {{harvtxt|Maon|Schieber|Vishkin|1986}} proceeds according to the following steps:\n# Find a [[spanning tree]] of the given graph and choose a root for the tree.\n# Determine, for each edge ''uv'' that is not part of the tree, the distance between the root and the [[lowest common ancestor]] of ''u'' and ''v''.\n# For each edge ''uv'' that is part of the tree, find the corresponding \"master edge\", a non-tree edge ''wx'' such that the cycle formed by adding ''wx'' to the tree passes through ''uv'' and such that, among such edges, ''w'' and ''x'' have a lowest common ancestor that is as close to the root as possible (with ties broken by edge identifiers).\n# Form an ear for each non-tree edge, consisting of it and the tree edges for which it is the master, and order the ears by their master edges' distance from the root (with the same tie-breaking rule).\nThese algorithms may be used as subroutines for other problems including testing connectivity, recognizing series-parallel graphs, and constructing ''st''-numberings of graphs (an important subroutine in [[planarity testing]]).\n\nAn ear decomposition of a given matroid, with the additional constraint that every ear contains the same fixed element of the matroid, may be found in [[polynomial time]] given access to an [[matroid oracle|independence oracle]] for the matroid {{harv|Coullard|Hellerstein|1996}}.\n\n==References==\n{{refbegin|colwidth=30em}}\n*{{citation\n | last1 = Cheriyan | first1 = J.\n | last2 = Maheshwari | first2 = S. N.\n | doi = 10.1016/0196-6774(88)90015-6\n | issue = 4\n | journal = Journal of Algorithms\n | mr = 970192\n | pages = 507–537\n | title = Finding nonseparating induced cycles and independent spanning trees in 3-connected graphs\n | volume = 9\n | year = 1988}}.\n*{{citation\n | last1 = Coullard | first1 = Collette R.\n | last2 = Hellerstein | first2 = Lisa\n | doi = 10.1007/BF01844845\n | issue = 2\n | journal = [[Combinatorica]]\n | mr = 1401892\n | pages = 189–208\n | title = Independence and port oracles for matroids, with an application to computational learning theory\n | volume = 16\n | year = 1996}}.\n*{{citation\n | last = Eppstein| first = D.  | authorlink = David Eppstein\n | journal = Information and Computation\n | pages = 41–55\n | title = Parallel recognition of series-parallel graphs\n | volume = 98\n | issue = 1\n | doi = 10.1016/0890-5401(92)90041-D\n | mr = 1161075\n | year = 1992}}.\n*{{citation\n | last = Frank | first = András | author-link = András Frank\n | doi = 10.1007/BF01202790\n | issue = 1\n | journal = [[Combinatorica]]\n | mr = 1221177\n | pages = 65–81\n | title = Conservative weightings and ear-decompositions of graphs\n | volume = 13\n | year = 1993}}.\n*{{citation\n | last1 = Gross | first1 = Jonathan L.\n | last2 = Yellen | first2 = Jay\n | contribution = Characterization of strongly orientable graphs\n | edition = 2nd\n | isbn = 978-1-58488-505-4\n | mr = 2181153\n | pages = 498–499\n | publisher = Chapman &Hall/CRC, Boca Raton, FL\n | series = Discrete Mathematics and its Applications (Boca Raton)\n | title = Graph theory and its applications\n | url = https://books.google.com/books?id=unEloQ_sYmkC&pg=PA498\n | year = 2006}}.\n*{{citation\n | last = Khuller | first = Samir\n | title = Ear decompositions\n | journal = [[SIGACT News]]\n | url = http://portalparts.acm.org/70000/65780/bm/backmatter.pdf\n | page = 128\n | year = 1989\n | volume = 20\n | issue = 1}}.\n*{{citation\n | last = Lovász | first = László | authorlink = László Lovász\n | journal = Studia Sci. Math. Hung. \n | mr = 0335371\n | pages = 279–280\n | title = A note on factor-critical graphs\n | volume = 7\n | year = 1972}}.\n*{{citation\n | last = Lovász | first = László\n | contribution = Computing ears and branchings in parallel\n | doi = 10.1109/SFCS.1985.16\n | pages = 464–467\n | title = [[Symposium on Foundations of Computer Science|26th Annual Symposium on Foundations of Computer Science]]\n | year = 1985}}.\n*{{citation\n | last1 = Maon | first1 = Y.\n | last2 = Schieber | first2 = B. \n | last3 = Vishkin | first3 = U. | author3-link = Uzi Vishkin\n | journal = [[Theoretical Computer Science (journal)|Theoretical Computer Science]]\n | title = Parallel ear decomposition search (EDS) and ST-numbering in graphs\n | volume = 47\n | issue = 3\n | mr = 0882357\n | doi = 10.1016/0304-3975(86)90153-2\n | year = 1986}}.\n*{{citation\n | last1 = Miller | first1 = G. | author1-link = Gary Miller (computer scientist)\n | last2 = Ramachandran | first2 = V.\n | title = Efficient parallel ear decomposition with applications\n | publisher = Unpublished manuscript\n | year = 1986}}.\n*{{citation\n | last = Robbins | first = H. E. | authorlink = Herbert Robbins\n | journal = [[American Mathematical Monthly]]\n | pages = 281–283\n | title = A theorem on graphs, with an application to a problem of traffic control\n | volume = 46\n | year = 1939\n | doi=10.2307/2303897}}.\n*{{citation\n | last1 = Schmidt| first1 = Jens M.\n | journal = Information Processing Letters\n | title = A Simple Test on 2-Vertex- and 2-Edge-Connectivity\n | year = 2013a\n | pages = 241–244\n | volume = 113\n | number = 7\n | doi = 10.1016/j.ipl.2013.01.016| arxiv = 1209.0700\n }}.\n*{{citation\n | last = Schmidt | first = Jens M.\n | arxiv = 1311.0750\n | title = The Mondshein sequence\n | year = 2013b| bibcode = 2013arXiv1311.0750S}}.\n*{{citation\n | last1 = Schrijver | first1 = Alexander | authorlink = Alexander Schrijver\n | title = Combinatorial Optimization. Polyhedra and efficiency. Vol A\n | year = 2003\n | publisher = Springer-Verlag\n | isbn = 978-3-540-44389-6}}.\n*{{citation\n | last1 = Szegedy | first1 = Balázs | author1-link = Balázs Szegedy\n | last2 = Szegedy | first2 = Christian\n | doi = 10.1007/s00493-006-0020-3\n | issue = 3\n | journal = [[Combinatorica]]\n | mr = 2246153\n | pages = 353–377\n | title = Symplectic spaces and ear-decomposition of matroids\n | volume = 26\n | year = 2006}}.\n*{{citation\n | last = Whitney | first = H. | authorlink = Hassler Whitney\n | doi = 10.1090/S0002-9947-1932-1501641-2\n | journal = [[Transactions of the American Mathematical Society]]\n | jstor = 1989545\n | pages = 339–362\n | title = Non-separable and planar graphs\n | volume = 34\n | year = 1932| pmc = 1076008}}.\n{{refend}}\n\n[[Category:Graph theory objects]]\n[[Category:Matroid theory]]"
    },
    {
      "title": "Matroid embedding",
      "url": "https://en.wikipedia.org/wiki/Matroid_embedding",
      "text": "In [[combinatorics]], a '''matroid embedding''' is a [[set system]] ('''''F''''', ''E''), where '''''F''''' is a collection of ''feasible sets'', that satisfies the following properties:\n# (''Accessibility Property'') Every non-empty feasible set ''X'' contains an element ''x'' such that ''X''\\{''x''} is feasible;\n# (''Extensibility Property'') For every feasible subset ''X'' of a ''basis'' (''i.e.'', maximal feasible set) ''B'', some element in ''B'' but not in ''X'' belongs to the '''extension''' ext(''X'') of ''X'', where ext(''X'') is the set of all elements ''e'' not in ''X'' such that ''X''∪{''e''} is feasible;\n# (''Closure-Congruence Property'') For every [[superset]] ''A'' of a feasible set ''X'' disjoint from ext(''X''), ''A''∪{''e''} is contained in some feasible set for either all or no ''e'' in ext(''X'');\n# The collection of all subsets of feasible sets forms a [[matroid]].\n\nMatroid embedding was introduced by {{harvtxt|Helman|Moret|Shapiro|1993}} to characterize problems that can be optimized by a [[greedy algorithm]].\n\n== References ==\n*{{citation\n | last1 = Helman | first1 = Paul\n | last2 = Moret | first2 = Bernard M. E. | author2-link = Bernard Moret\n | last3 = Shapiro | first3 = Henry D.\n | doi = 10.1137/0406021\n | issue = 2\n | journal = [[SIAM Journal on Discrete Mathematics]]\n | mr = 1215233\n | pages = 274–283\n | title = An exact characterization of greedy structures\n | volume = 6\n | year = 1993| citeseerx = 10.1.1.37.1825}}\n\n[[Category:Matroid theory|Embedding]]"
    },
    {
      "title": "Eulerian matroid",
      "url": "https://en.wikipedia.org/wiki/Eulerian_matroid",
      "text": "In [[matroid theory]], an '''Eulerian matroid''' is a matroid whose elements can be partitioned into a collection of disjoint circuits.\n\n==Examples==\nIn a [[uniform matroid]] <math>U{}^r_n</math>, the circuits are the sets of exactly <math>r+1</math> elements. Therefore, a uniform matroid is Eulerian if and only if <math>r+1</math> is a divisor of <math>n</math>. For instance, the <math>n</math>-point lines <math>U{}^2_n</math> are Eulerian if and only if <math>n</math> is divisible by three.\n\nThe [[Fano plane]] has two kinds of circuits: sets of three collinear points, and sets of four points that do not contain any line. The three-point circuits are the [[complement (set theory)|complements]] of the four-point circuits, so it is possible to partition the seven points of the plane into two circuits, one of each kind. Thus, the Fano plane is also Eulerian.\n\n==Relation to Eulerian graphs==\nEulerian matroids were defined by {{harvtxt|Welsh|1969}} as a generalization of the [[Eulerian graph]]s, graphs in which every vertex has even degree. By [[Veblen's theorem]] the edges of every such graph may be partitioned into simple cycles, from which it follows that the [[graphic matroid]]s of Eulerian graphs are examples of Eulerian matroids.<ref name=\"w69\">{{citation\n | last = Welsh | first = D. J. A. | authorlink = Dominic Welsh\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 0237368\n | pages = 375–377\n | title = Euler and bipartite matroids\n | volume = 6\n | year = 1969\n | doi=10.1016/s0021-9800(69)80033-5}}.</ref>\n\nThe definition of an Eulerian graph above allows graphs that are disconnected, so not every such graph has an Euler tour. {{harvtxt|Wilde|1975}} observes that the graphs that have Euler tours can be characterized in an alternative way that generalizes to matroids: a graph <math>G</math> has an Euler tour if and only if it can be formed from some other graph <math>H</math>, and a cycle <math>C</math> in <math>H</math>, by [[graph minor|contracting]] the edges of <math>H</math> that do not belong to <math>C</math>. In the contracted graph, <math>C</math> generally stops being a simple cycle and becomes instead an Euler tour. Analogously, Wilde considers the matroids that can be formed from a larger matroid by [[matroid minor|contracting]] the elements that do not belong to some particular circuit. He shows that this property is trivial for general matroids (it implies only that each element belongs to at least one circuit) but can be used to characterize the Eulerian matroids among the [[binary matroid]]s, matroids [[Matroid representation|representable]] over [[GF(2)]]:\na binary matroid is Eulerian if and only if it is the contraction of another binary matroid onto a circuit.<ref>{{citation\n | last = Wilde | first = P. J.\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 0384577\n | pages = 260–264\n | series = Series B\n | title = The Euler circuit theorem for binary matroids\n | volume = 18\n | year = 1975\n | doi=10.1016/0095-8956(75)90051-9}}.</ref>\n\n==Duality with bipartite matroids==\nFor [[planar graph]]s, the properties of being Eulerian and [[bipartite graph|bipartite]] are dual: a planar graph is Eulerian if and only if its [[dual graph]] is bipartite. As Welsh showed, this duality extends to binary matroids: a binary matroid is Eulerian if and only if its [[dual matroid]] is a [[bipartite matroid]], a matroid in which every circuit has even cardinality.<ref name=\"w69\"/><ref>{{citation\n | last1 = Harary | first1 = Frank | author1-link = Frank Harary\n | last2 = Welsh | first2 = Dominic | author2-link = Dominic Welsh\n | contribution = Matroids versus graphs\n | doi = 10.1007/BFb0060114\n | location = Berlin\n | mr = 0263666\n | pages = 155–170\n | publisher = Springer\n | series = Lecture Notes in Mathematics\n | title = The Many Facets of Graph Theory (Proc. Conf., Western Mich. Univ., Kalamazoo, Mich., 1968)\n | volume = 110\n | year = 1969}}.</ref>\n\nFor matroids that are not binary, the duality between Eulerian and bipartite matroids may break down. For instance, the uniform matroid <math>U{}^2_6</math> is Eulerian but its dual <math>U{}^4_6</math> is not bipartite, as its circuits have size five. The self-dual uniform matroid <math>U{}^3_6</math> is bipartite but not Eulerian.\n\n==Alternative characterizations==\nBecause of the correspondence between Eulerian and bipartite matroids among the binary matroids, the binary matroids that are Eulerian may be characterized in alternative ways. The characterization of {{harvtxt|Wilde|1975}} is one example; two more are that a binary matroid is Eulerian if and only if every element belongs to an odd number of circuits, if and only if the whole matroid has an odd number of partitions into circuits.<ref>{{citation\n | last = Shikare | first = M. M.\n | issue = 2\n | journal = Indian Journal of Pure and Applied Mathematics\n | mr = 1820861\n | pages = 215–219\n | title = New characterizations of Eulerian and bipartite binary matroids\n | url = http://www.dli.gov.in/rawdataupload/upload/insa/INSA_1/20005b01_215.pdf\n | volume = 32\n | year = 2001}}.</ref> {{harvtxt|Lovász|Seress|1993}} collect several additional characterizations of Eulerian binary matroids, from which they derive a [[polynomial time]] algorithm for testing whether a binary matroid is Eulerian.<ref>{{citation\n | last1 = Lovász | first1 = László | author1-link = László Lovász\n | last2 = Seress | first2 = Ákos\n | doi = 10.1006/eujc.1993.1027\n | issue = 3\n | journal = European Journal of Combinatorics\n | mr = 1215334\n | pages = 241–250\n | title = The cocycle lattice of binary matroids\n | volume = 14\n | year = 1993}}.</ref>\n\n==Computational complexity==\nAny algorithm that tests whether a given matroid is Eulerian, given access to the matroid via an [[matroid oracle|independence oracle]], must perform an exponential number of oracle queries, and therefore cannot take polynomial time.<ref>{{citation\n | last1 = Jensen | first1 = Per M.\n | last2 = Korte | first2 = Bernhard\n | doi = 10.1137/0211014\n | issue = 1\n | journal = [[SIAM Journal on Computing]]\n | mr = 646772\n | pages = 184–190\n | title = Complexity of matroid property algorithms\n | volume = 11\n | year = 1982}}.</ref>\n\n==Eulerian extension==\nIf <math>M</math> is a binary matroid that is not Eulerian, then it has a unique '''Eulerian extension''', a binary matroid <math>\\bar M</math> whose elements are the elements of <math>M</math> together with one additional element <math>e</math>, such that the restriction of <math>\\bar M</math> to the elements of <math>M</math> is isomorphic to <math>M</math>. The dual of <math>\\bar M</math> is a bipartite matroid formed from the dual of <math>M</math> by adding <math>e</math> to every odd circuit.<ref>{{citation\n | last = Sebő | first = András\n | contribution = The cographic multiflow problem: an epilogue\n | location = Providence, RI\n | mr = 1105128\n | pages = 203–234\n | publisher = Amer. Math. Soc.\n | series = DIMACS Ser. Discrete Math. Theoret. Comput. Sci.\n | title = Polyhedral combinatorics (Morristown, NJ, 1989)\n | volume = 1\n | year = 1990}}.</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Matroid theory]]"
    },
    {
      "title": "Fano plane",
      "url": "https://en.wikipedia.org/wiki/Fano_plane",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Finite projective plane of order 2}}\n[[File:Fano plane.svg|thumb|The Fano plane]]\n\nIn [[finite geometry]], the '''Fano plane''' (after [[Gino Fano]]) is the [[Projective plane#Finite projective planes|finite projective plane]] of order 2. It is the finite projective plane with the smallest possible number of points and lines: 7 points and 7 lines, with 3 points on every line and 3 lines through every point. The standard notation for this plane, as a member of a family of [[projective space]]s, is {{math|PG(2, 2)}} where {{math|PG}} stands for \"[[projective geometry]]\", the first parameter is the geometric dimension and the second parameter is the order.\n\nThe Fano plane is an example of a finite [[incidence structure]], so many of its properties can be established using [[Combinatorics|combinatorical techniques]] and other tools used in the study of [[incidence geometry|incidence geometries]]. Since it is a projective space, algebraic techniques can also be effective tools in its study.\n\n==Homogeneous coordinates==\nThe Fano plane can be constructed via [[linear algebra]] as the [[projective plane]] over the [[finite field]] with two elements. One can similarly construct projective planes over any other finite field, with the Fano plane being the smallest.\n\nUsing the standard construction of projective spaces via [[homogeneous coordinates]], the seven points of the Fano plane may be labeled with the seven non-zero ordered triples of binary digits 001, 010, 011, 100, 101, 110, and 111. This can be done in such a way that for every two points ''p'' and ''q'', the third point on line ''pq'' has the label formed by adding the labels of ''p'' and ''q'' modulo 2. In other words, the points of the Fano plane correspond to the non-zero points of the finite [[vector space]] of dimension 3 over the finite field of order 2.\n\nDue to this construction, the Fano plane is considered to be a [[Desarguesian plane]], even though the plane is too small to contain a non-degenerate [[Desargues configuration]] (which requires 10 points and 10 lines).\n\nThe lines of the Fano plane may also be given homogeneous coordinates, again using non-zero triples of binary digits. With this system of coordinates, a point is incident to a line if the coordinate for the point and the coordinate for the line have an even number of positions at which they both have nonzero bits: for instance, the point 101 belongs to the line 111, because they have nonzero bits at two common positions. In terms of the underlying linear algebra, a point belongs to a line if the [[inner product]] of the vectors representing the point and line is zero.\n\nThe lines can be classified into three types.\n*On three of the lines the binary triples for the points have the 0 in a constant position: the line 100 (containing the points 001, 010, and 011) has 0 in the first position, and the lines 010 and 001 are formed in the same way.\n*On three of the lines, two of the positions in the binary triples of each point have the same value: in the line 110 (containing the points 001, 110, and 111) the first and second positions are always equal, and the lines 101 and 011 are formed in the same way.\n*In the remaining line 111 (containing the points 011, 101, and 110), each binary triple has exactly two nonzero bits.\n\n==Group-theoretic construction==\nAlternatively, the 7 points of the plane correspond to the 7 non-identity elements of the [[group (mathematics)|group]] (''Z''<sub>2</sub>)<sup>3</sup> = ''Z''<sub>2</sub> &times; ''Z''<sub>2</sub> &times; ''Z''<sub>2</sub>. The lines of the plane correspond to the subgroups of order 4, isomorphic to ''Z''<sub>2</sub> &times; ''Z''<sub>2</sub>.  The [[automorphism]] group [[GL(3,2)]] of the group (''Z''<sub>2</sub>)<sup>3</sup> is that of the Fano plane, and has order 168.\n\n==Levi graph==\n[[File:Heawood graph 2COL.svg|thumb|Bipartite Heawood graph. Points are represented by vertices of one color and lines by vertices of the other color.]]\nAs with any incidence structure, the [[Levi graph]] of the Fano plane is a [[bipartite graph]], the vertices of one part representing the points and the other representing the lines, with two vertices joined if the corresponding point and line are [[Incidence (geometry)|incident]]. This particular graph is a connected [[cubic graph]] (regular of degree 3), has [[Girth (graph theory)|girth 6]] and each part contains 7 vertices. It is the [[Heawood graph]], the unique [[Cage (graph theory)|6-cage]].<ref>{{harvnb|Pisanski|Servatius|2013|loc=p. 171}}</ref>\n\n==Collineations==\n[[File:Fanoperm364.svg|180px|thumb|left|A collineation of the Fano plane corresponding to the 3-bit [[Gray code]] permutation]]\nA  ''[[collineation]]'', ''[[automorphism]]'', or ''[[symmetry]]'' of the Fano plane is a permutation of the 7 points that preserves collinearity: that is, it carries [[incidence (geometry)|collinear]] points (on the same line) to collinear points. By the [[Fundamental theorem of projective geometry]], the full [[collineation|collineation group]] (or [[automorphism group]], or [[symmetry group]]) is the [[projective linear group]] PGL(3,2),<ref>Actually it is PΓL(3,2), but since the finite field of order 2 has no non-identity automorphisms, this becomes PGL(3,2).</ref> also denoted <math>\\mathrm{PGL}_3(\\mathbb{F}_2)</math>. Since the field has only one nonzero element, this group is isomorphic to the [[projective special linear group]] PSL(3,2) and the [[general linear group]] GL(3,2). It is also isomorphic to PSL(2,7).<ref>{{harvnb|Hirschfeld|1979|loc=p. 131}}</ref> \n\nThis is a [[PSL(3,2)|well-known group]] of order 168 = 2<sup>3</sup>·3·7, the second-largest non-abelian simple group after [[icosahedral symmetry|A<sub>5</sub>]] of order 60.\n\nAs a [[permutation group]] [[Group action (mathematics)|acting]] on the 7 points of the plane, the collineation group is [[doubly transitive]] meaning that any [[ordered pair]] of points can be mapped by at least one collineation to any other ordered pair of points.<ref>{{citation|first=Robert D.|last=Carmichael|title=Introduction to the theory of groups of finite order|year=1956|origyear=1937|publisher=Dover|isbn=0-486-60300-8|page=363}}</ref> (See below.)\n\nCollineations may also be viewed as the color-preserving automorphisms of the Heawood graph (see figure).\n\n[[File:Fano plane Hasse diagram.svg|thumb|Duality in the Fano plane: Each point corresponds to a line and vice versa.]]\n===Dualities===\n\n{{main|Duality (projective geometry)}}\nA [[bijection]] between the point set and the line set that preserves incidence is called a ''duality'' and a duality of order two is called a ''polarity''.<ref>{{harvnb|Polster|1998|loc=p. 11}}</ref>\n\nDualities can be viewed in the context of the Heawood graph as color reversing automorphisms. An example of a polarity is given by reflection through a vertical line that bisects the Heawood graph representation given on the right.<ref>{{harvnb|Polster|1998|loc=p. 15}}</ref> The existence of this polarity shows that the Fano plane is ''self-dual''. This is also an immediate consequence of the symmetry between points and lines in the definition of the incidence relation in terms of homogeneous coordinates, as detailed in an earlier section.\n{{clear}}\n\n===Cycle structure===\n[[File:Fano plane nimbers.svg|thumb|180px|A [[nimber]] numbering of the Fano plane]]\nThe collineation group, thought of as a [[permutation group]] of the 7 points as numbered in the figure, is generated by:<ref>{{harvnb|Pisanski|Servatius|2013|loc=p. 173}} given with a different labeling</ref>\n\n:(1432657), (162)(374), (14)(27), (17)(24), (17)(24)(36).\n\nIt comprises 6 [[conjugacy classes]]. \nThe following [[Cycles and fixed points|cycle structures]] each define a single conjugacy class:\n\n*[[File:Fanoperm124.svg|40px]] The identity permutation\n*[[File:Fanoperm421.svg|40px]] 21 permutations with two [[Transposition (mathematics)|2-cycles]]\n*[[File:Fanoperm621.svg|40px]] 42 permutations with a 4-cycle and a 2-cycle\n*[[File:Fanoperm521.svg|40px]] 56 permutations with two 3-cycles\nThe 48 permutations with a complete 7-cycle form two distinct conjugacy classes with 24 elements:\n*[[File:Fanoperm713.svg|40px]] ''A'' maps to ''B'', ''B'' to ''C'', ''C'' to ''D''. Then ''D'' is on the same line as ''A'' and ''B''.\n*[[File:Fanoperm265.svg|40px]] ''A'' maps to ''B'', ''B'' to ''C'', ''C'' to ''D''. Then ''D'' is on the same line as ''A'' and ''C''.\n\nSee [[v:3-bit Walsh permutation#Fano plane collineations|Fano plane collineations]] for a complete list.\n\nHence, by the [[Pólya enumeration theorem]], the number of inequivalent colorings of the Fano plane with ''n'' colors is:\n:<math>{1 \\over 168}\\left(n^7 + 21 n^5 + 98 n^3 + 48 n\\right).</math>\n\n==Complete quadrangles and Fano subplanes==\n{{main|Complete quadrangle}}\nIn any projective plane a set of four points, no three of which are collinear, and the six lines joining pairs of these points is a [[Projective configuration|configuration]] known as a [[complete quadrangle]]. The lines are called ''sides'' and pairs of sides that do not meet at one of the four points are called ''opposite sides''. The points at which opposite sides meet are called ''diagonal points'' and there are three of them.<ref>{{citation|first=Frederick W.|last=Stevenson|title=Projective Planes|year=1972|publisher= W.H. Freeman and Co.|isbn=0-7167-0443-9|page= 21}}</ref>\n\nIf this configuration lies in a projective plane and the three diagonal points are collinear, then the seven points and seven lines of the expanded configuration form a subplane of the projective plane that is isomorphic to the Fano plane and is called a ''Fano subplane''. \n\nA famous result, due to [[Andrew M. Gleason]] states that if every complete quadrangle in a finite projective plane extends to a Fano subplane (that is, has collinear diagonal points) then the plane is Desarguesian.<ref>{{citation|first=Andrew M.|last=Gleason|author-link=Andrew M. Gleason|title=Finite Fano planes|journal=American Journal of Mathematics|year=1956|volume=78|pages=797-807}}</ref> Gleason called any projective plane satisfying this condition a ''Fano plane'' thus creating some confusion with modern terminology. To compound the confusion, ''Fano's axiom'' states that the diagonal points of a complete quadrangle are ''never'' collinear, a condition that holds in the Euclidean and real projective planes. Thus, what Gleason called Fano planes do not satisfy Fano's axiom.<ref>{{harvnb|Dembowski|1968|loc=p. 168}}</ref>\n \n==Configurations==\n\nThe Fano plane contains the following numbers of configurations of points and lines of different types. For each type of configuration, the number of copies of configuration multiplied by the number of symmetries of the plane that keep the configuration unchanged is equal to 168, the size of the entire collineation group, provided each copy can be mapped to any other copy (see [[Orbit-Stabiliser theorem]]). Since the Fano plane is self-dual, these configurations come in dual pairs and it can be shown that the number of collineations fixing a configuration equals the number of collineations that fix its dual configuration.\n* There are 7 points with 24 symmetries fixing any point and dually, there are 7 lines with 24 symmetries fixing any line. The number of symmetries follows from the 2-transitivity of the collineation group, which implies the group acts transitively on the points.\n*There are 42 [[ordered pair]]s of points, and each may be mapped by a symmetry onto any other ordered pair. For any ordered pair there are 4 symmetries fixing it. Correspondingly, there are 21 [[unordered pair]]s of points, each of which may be mapped by a symmetry onto any other unordered pair. For any unordered pair there are 8 symmetries fixing it.\n*There are 21 [[flag (geometry)|flags]] consisting of a line and a point on that line. Each flag corresponds to the unordered pair of the other two points on the same line. For each flag, 8 different symmetries keep it fixed.\n* There are 7 ways of selecting a [[Quadrilateral|quadrangle]] of four (unordered) points no three of which are collinear.  These four points form the complement of a line, which is the ''diagonal line'' of the quadrangle and a collineation fixes the quadrangle if and only if it fixes the diagonal line. Thus, there are 24 symmetries that fix any such quadrangle. The dual configuration is a quadrilateral consisting of four lines no three of which meet at a point and their six points of intersection, it is the complement of a point in the Fano plane.\n* There are <math>\\tbinom{7}{3} = 35</math> triples of points, seven of which are collinear triples, leaving 28 non-collinear triples or ''[[triangles]]''. The configuration consisting of the three points of a triangle and the three lines joining pairs of these points is represented by a 6-cycle in the Heawood graph. A color-preserving automorphism of the Heawood graph that fixes each vertex of a 6-cycle must be the identity automorphism.<ref>{{harvnb|Pisanski|Servatius|2013|loc=p. 171}}</ref> This means that there are 168 labeled triangles fixed only by the identity collineation and only six collineations that stabilize an unlabeled triangle, one for each permutation of the points. These 28 triangles may be viewed as corresponding to the 28 [[bitangents of a quartic]].<ref> {{harvnb|Manivel|2006}}</ref> There are 84 ways of specifying a triangle together with one distinguished point on that triangle and two symmetries fixing this configuration. The dual of the triangle configuration is also a triangle.\n*There are 28 ways of selecting a point and a line that are not incident to each other (an ''anti-flag''), and six ways of permuting the Fano plane while keeping an anti-flag fixed. For every non-incident point-line pair (''p'',''l''), the three points that are unequal to ''p'' and that do not belong to ''l'' form a triangle, and for every triangle there is a unique way of grouping the remaining four points into an anti-flag.\n*There are 28 ways of specifying a [[hexagon]] in which no three consecutive vertices lie on a line, and six symmetries fixing any such hexagon.\n*There are 84 ways of specifying a [[pentagon]] in which no three consecutive vertices lie on a line, and two symmetries fixing any pentagon.\n\nThe Fano plane is an example of an {{math|(''n''<sub>3</sub>)}}-configuration, that is, a set of {{mvar|n}} points and {{mvar|n}} lines with three points on each line and three lines through each point. The Fano plane, a (7<sub>3</sub>)-configuration, is unique and is the smallest such configuration.<ref>{{harvnb|Pisanski|Servatius|2013|loc=p. 165}}</ref> According to a theorem by [[Ernst Steinitz|Steinitz]]<ref>{{citation|first=Ernst|last= Steinitz|author-link=Ernst Steinitz|title=Über die construction der configurationen {{math|''n''<sub>3</sub>}}|year=1894|type=Ph. D. thesis|publisher=Kgl. Universität, Breslau}}</ref>configurations of this type can be realized in the Euclidean plane having at most one curved line (all other lines lying on Euclidean lines).<ref>{{harvnb|Pisanski|Servatius|2013|loc=p. 221}}</ref>\n\n==Block design theory==\n\nThe Fano plane is a small [[Block design#Symmetric BIBDs|symmetric block design]], specifically a 2-(7,3,1)-design.  The points of the design are the points of the plane, and the blocks of the design are the lines of the plane.<ref name=vanLint196>{{harvnb|van Lint|Wilson|1992|loc= pp. 196−197}}</ref>  As such it is a valuable example in (block) design theory.\n\nWith the points labelled 0, 1, 2, ..., 6 the lines (as point sets) are the translates of the (7, 3, 1) planar [[difference set]] given by {0, 1, 3} in the group <math>\\mathbb{Z} / 7\\mathbb{Z}.</math><ref name=vanLint196 /> With the lines labeled ''ℓ''<sub>0</sub>, ...,''ℓ''<sub>6</sub> the [[incidence matrix]] (table) is given by:\n\n:{| align=left class=wikitable\n|-\n! ||0||1||2||3||4||5||6\n|-\n!''ℓ''<sub>0</sub>\n|1||1||0||1||0||0||0\n|-\n!''ℓ''<sub>1</sub>\n|0||1||1||0||1||0||0 \n|-\n!''ℓ''<sub>2</sub>\n|0||0||1||1||0||1||0 \n|-\n!''ℓ''<sub>3</sub>\n|0||0||0||1||1||0||1\n|-\n!''ℓ''<sub>4</sub>\n|1||0||0||0||1||1||0\n|-\n!''ℓ''<sub>5</sub>\n|0||1||0||0||0||1||1\n|-\n!''ℓ''<sub>6</sub>\n|1||0||1||0||0||0||1\n|}\n{{clr}}\n===Steiner system===\n{{main|Steiner system}}\n\nThe Fano plane, as a block design, is a [[Steiner system#Steiner triple systems|Steiner triple system]].<ref>{{harvnb|Polster|1998|loc=p. 23}}</ref>  As such, it can be given the structure of a [[quasigroup]].  This quasigroup coincides with the multiplicative structure defined by the unit [[octonion]]s ''e''<sub>1</sub>, ''e''<sub>2</sub>, ..., ''e''<sub>7</sub> (omitting 1) if the signs of the octonion products are ignored {{harv|Baez|2002}}.\n\n==Matroid theory==\n{{main|Matroid theory}}\n\nThe Fano plane is one of the important examples in the structure theory of [[matroid]]s.  Excluding the Fano plane as a [[matroid minor]] is necessary to characterize several important classes of matroids, such as [[regular matroid|regular]], [[graphic matroid|graphic]], and cographic ones.\n\nIf you break one line apart into three 2-point lines you obtain the \"non-Fano configuration\", which can be embedded in the real plane. It is another important example in matroid theory, as it must be excluded for many theorems to hold.\n\n==PG(3,2)==\n{{main|PG(3,2)}}\n\nThe Fano plane can be extended in a third dimension to form a three-dimensional projective space, denoted by '''PG(3,2)'''.\nIt has 15 points, 35 lines, and 15 planes and is the smallest three-dimensional [[projective space]].<ref>{{citation|first=Bruce E.|last=Meserve|title=Fundamental Concepts of Geometry|year=1983|origyear=1955|publisher=Dover|isbn=0-486-63415-9|page=29}}</ref>  It also has the following properties:<ref>{{harvnb|Polster|1998|loc=p. 69}}</ref>\n\n* Each point is contained in 7 lines and 7 planes\n* Each line is contained in 3 planes and contains 3 points \n* Each plane contains 7 points and 7 lines\n* Each plane is [[Isomorphism|isomorphic]] to the Fano plane\n* Every pair of distinct planes intersect in a line\n* A line and a plane not containing the line intersect in exactly one point\n\n==See also==\n{{commons category}}\n\n*[[Projective configuration]]\n*[[Transylvania lottery]]\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{citation |last=Baez |first=John |authorlink=John Baez |title=The Octonions |url=http://www.ams.org/bull/2002-39-02/S0273-0979-01-00934-X/home.html |journal=Bull. Amer. Math. Soc. |volume=39 |year=2002 |pages=145–205 |doi=10.1090/S0273-0979-01-00934-X |issue=2|arxiv=math/0105155 }} ([http://math.ucr.edu/home/baez/octonions/ Online HTML version])\n*{{Citation | last1=Dembowski | first1=Peter | title=Finite geometries | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=[[Ergebnisse der Mathematik und ihrer Grenzgebiete]], Band 44 | mr=0233275  | year=1968 | isbn=3-540-61786-8}}\n* {{Citation\n|title=Projective Geometries Over Finite Fields\n|first1=J. W. P.\n|last1=Hirschfeld\n|publisher=[[Oxford University Press]]\n|year=1979\n|isbn=978-0-19-850295-1}}\n*{{citation |last=Manivel |first=L. |doi=10.1016/j.jalgebra.2006.04.029 |issue=1 |journal=Journal of Algebra |issn=0021-8693 |pages=457–486 |title=Configurations of lines and models of Lie algebras |volume=304 |year=2006|arxiv=math/0507118 }}\n* {{citation|first1=Tomaž|last1=Pisanski|first2=Brigitte|last2=Servatius|author1-link=Tomaž Pisanski|author2-link=Brigitte Servatius|title=Configurations from a Graphical Viewpoint|year=2013|publisher=Birkhäuser|isbn=978-0-8176-8363-4}}\n* {{citation|first=Burkard|last=Polster|author-link=Burkard Polster|year=1998|title=A Geometrical Picture Book|publisher=Springer|isbn=978-0-387-98437-7}}\n<!-- Chapter 1: \"Introduction via the Fano Plane\", also pp 21, 23, 27, 29, 71, 73, 77, 112, 115, 116, 132, 174 -->\n* {{citation |last1=van Lint |first1=J. H. |last2=Wilson |first2=R. M. |year=1992 |title=A Course in Combinatorics |publisher=Cambridge University Press|isbn=978-0-521-42260-4}}\n\n==External links==\n*{{MathWorld|title=Fano Plane|urlname=FanoPlane}}\n\n[[Category:Projective geometry]]\n[[Category:Finite geometry]]\n[[Category:Incidence geometry]]\n[[Category:Configurations]]\n[[Category:Matroid theory]]"
    },
    {
      "title": "Gain graph",
      "url": "https://en.wikipedia.org/wiki/Gain_graph",
      "text": "A '''gain graph''' is a [[Graph (discrete mathematics)|graph]] whose edges are labelled \"invertibly\", or \"orientably\", by elements of a [[Group (mathematics)|group]] ''G''.  This means that, if an edge ''e'' in one direction has label ''g'' (a group element), then in the other direction it has label ''g''<sup>&nbsp;&minus;1</sup>.  The label function φ therefore has the property that it is defined differently, but not independently, on the two different orientations, or directions, of an edge ''e''.  The group ''G'' is called the '''gain group''', φ is the '''gain function''', and the value φ(''e'') is the '''gain''' of ''e'' (in some indicated direction).  A gain graph is a generalization of a [[signed graph]], where the gain group ''G'' has only two elements.  See Zaslavsky (1989, 1991).\n\nA gain should not be confused with a '''weight''' on an edge, whose value is independent of the orientation of the edge.\n\n==Applications==\nSome reasons to be interested in gain graphs are their connections to [[flow network|network flow]] theory in [[combinatorial optimization]], to [[geometry]], and to [[physics]].\n\n* The mathematics of a [[network with gains]], or [[generalized network]], is connected with the [[biased graph|frame matroid]] of the gain graph.\n* Suppose we have some [[hyperplane]]s in ''R&nbsp;<sup>n</sup>'' given by equations of the form ''x<sub>j</sub>'' = ''g&nbsp;x<sub>i</sub>'' .  The geometry of the hyperplanes can be treated by using the following gain graph:  The vertex set is {1,2,...,''n''}.  There is an edge ''ij'' with gain ''g'' (in the direction from ''i'' to ''j'') for each hyperplane with equation ''x<sub>j</sub> = g x<sub>i</sub>'' .  These hyperplanes are treated through the frame matroid of the gain graph (Zaslavsky 2003).\n* Or, suppose we have hyperplanes given by equations of the form ''x<sub>j</sub>'' = ''x<sub>i</sub>'' + ''g''.  The geometry of these hyperplanes can be treated by using the gain graph with the same vertex set and an edge ''ij'' with gain ''g'' (in the direction from ''i'' to ''j'') for each hyperplane with equation ''x<sub>j</sub>'' = ''x<sub>i</sub>'' + ''g''.  These hyperplanes are studied via the [[biased graph|lift matroid]] of the gain graph (Zaslavsky 2003).\n* Suppose the gain group has an [[Group action (mathematics)|action]] on a set ''Q''.  Assigning an element ''s<sub>i</sub>'' of ''Q'' to each vertex gives a '''state''' of the gain graph.  An edge is '''satisfied''' if, for each edge ''ij'' with gain ''g'' (in the direction from ''i'' to ''j''), the equation ''s<sub>j</sub>'' = ''s<sub>i</sub>&nbsp;g'' is satisfied; otherwise it is '''frustrated'''.  A state is ''satisfied'' if every edge is satisfied.  In physics this corresponds to a ground state (a state of lowest energy), if such a state exists.  An important problem in physics, especially in the theory of [[spin glass]]es, is to determine a state with the fewest frustrated edges.\n\n==Related concepts==\nGain graphs used in [[topological graph theory]] as a means to construct [[graph embedding]]s in surfaces are known as \"[[voltage graph]]s\" (Gross 1974; Gross and Tucker 1977).  The term \"gain graph\" is more usual in other contexts, e.g., [[biased graph]] theory and [[matroid theory]].  The term '''group-labelled graph''' has also been used, but it is ambiguous since \"group labels\" may be&mdash;and have been&mdash;treated as weights.\n\nSince much of the theory of gain graphs is a special case of that of biased graphs (and much of the theory of biased graphs is a generalization of that of gain graphs), the reader should refer to the article on [[biased graph]]s for more information and examples.\n\n==References==\n\n*Jonathan L. Gross (1974),  Voltage graphs.  ''Discrete Mathematics'', Vol. 9, pp.&nbsp;239&ndash;246.\n*J.L. Gross and T.W. Tucker (1977),  Generating all graph coverings by permutation voltage assignments.  ''Discrete Mathematics'', Vol. 18, pp.&nbsp;273&ndash;283.\n*Thomas Zaslavsky (1989),  Biased graphs.  I.  Bias, balance, and gains.  ''Journal of Combinatorial Theory, Series B'', Vol. 47, 32&ndash;52.\n*Thomas Zaslavsky (1991),  Biased graphs.  II.  The three matroids.  ''Journal of Combinatorial Theory, Series B'', Vol. 51, 46&ndash;72.\n*Thomas Zaslavsky (1999).  A mathematical bibliography of signed and gain graphs and allied areas.  [https://web.archive.org/web/20100912123133/http://www.combinatorics.org/Surveys/index.html ''Electronic Journal of Combinatorics'', Dynamic Surveys in Combinatorics, #DS8].\n*Thomas Zaslavsky (2003),  Biased graphs IV:  Geometrical realizations. ''Journal of Combinatorial Theory, Series B'', Vol. 89, no. 2, pp.&nbsp;231&ndash;297.\n\n[[Category:Matroid theory]]\n[[Category:Extensions and generalizations of graphs]]"
    },
    {
      "title": "Gammoid",
      "url": "https://en.wikipedia.org/wiki/Gammoid",
      "text": "In [[matroid theory]], a field within mathematics, a '''gammoid''' is a certain kind of matroid, describing sets of [[Vertices (graph theory)|vertices]] that can be reached by vertex-disjoint [[Path (graph theory)|paths]] in a [[directed graph]].\n\nThe concept of a gammoid was introduced and shown to be a matroid by {{harvs|first=Hazel|last=Perfect|authorlink=Hazel Perfect|year=1968|txt}}, based on considerations related to [[Menger's theorem]] characterizing the obstacles to the existence of systems of disjoint paths.<ref>{{citation\n | last = Perfect | first = Hazel\n | doi = 10.1016/0022-247X(68)90163-7\n | journal = Journal of Mathematical Analysis and Applications\n | mr = 0224494\n | pages = 96–111\n | title = Applications of Menger's graph theorem\n | volume = 22\n | year = 1968}}.</ref> Gammoids were given their name by {{harvtxt|Pym|1969}}<ref name=\"pym69\">\n{{citation\n | last = Pym | first = J. S.\n | doi = 10.1112/jlms/s1-44.1.542\n | issue = 1\n | journal = Journal of the London Mathematical Society\n | pages = 542–550\n | title = The Linking of Sets in Graphs\n | volume = s1-44\n | year = 1969}}.</ref> and studied in more detail by {{harvtxt|Mason|1972}}.<ref name=\"mason72\">{{citation\n | last = Mason | first = J. H.\n | doi = 10.1112/plms/s3-25.1.55\n | issue = 1\n | journal = Proceedings of the London Mathematical Society\n | mr = 0311496\n | pages = 55–74\n | series = Third Series\n | title = On a class of matroids arising from paths in graphs\n | volume = 25\n | year = 1972}}.</ref>\n\n==Definition==\nLet <math>G</math> be a directed graph, <math>S</math> be a set of starting vertices, and <math>T</math> be a set of destination vertices (not necessarily disjoint from <math>S</math>). The gammoid <math>\\Gamma</math> derived from this data has <math>T</math> as its set of elements. A subset <math>I</math> of <math>T</math> is independent in <math>\\Gamma</math> if there exists a set of vertex-disjoint paths whose starting points all belong to <math>S</math> and whose ending points are exactly <math>I</math>.<ref name=\"schrijver\">{{citation\n | last = Schrijver | first = Alexander | author-link = Alexander Schrijver\n | isbn = 3-540-44389-4\n | location = Berlin\n | mr = 1956925\n | pages = 659–661\n | publisher = Springer-Verlag\n | series = Algorithms and Combinatorics\n | title = Combinatorial Optimization: Polyhedra and Efficiency. Vol. B: Matroids, Trees, Stable Sets\n | url = https://books.google.com/books?id=mqGeSQ6dJycC&pg=PA659\n | volume = 24\n | year = 2003}}.</ref>\n\nA '''strict gammoid''' is a gammoid in which the set <math>T</math> of destination vertices consists of every vertex in <math>G</math>. Thus, a gammoid is a restriction of a strict gammoid, to a subset of its elements.<ref name=\"schrijver\"/><ref name=Ox100>{{harvnb|Oxley|2006|p=100}}</ref>\n\n==Example==\nConsider the [[uniform matroid]] <math>U{}^r_n</math> on a set of <math>n</math> elements, in which every set of <math>r</math> or fewer elements is independent. One way to represent this matroid as a gammoid would be to form a [[complete bipartite graph]] <math>K_{r,n}</math> with a set <math>S</math> of <math>r</math> vertices on one side of the bipartition, with a set <math>T</math> of <math>n</math> vertices on the other side of the bipartition, and with every edge directed from <math>S</math> to <math>T</math>. In this graph, a subset of <math>T</math> is the set of endpoints of a set of disjoint paths if and only if it has <math>r</math> or fewer vertices, for otherwise there aren't enough vertices in <math>S</math> to start the paths. The special structure of this graph shows that the uniform matroid is a [[transversal matroid]] as well as being a gammoid.<ref>{{citation\n | last = Oxley | first = James G. | authorlink = James Oxley\n | isbn = 9780199202508\n | pages = 48–49\n | publisher = Oxford University Press\n | series = Oxford Graduate Texts in Mathematics\n | title = Matroid Theory\n | volume = 3\n | year = 2006}}.</ref>\n\nAlternatively, the same uniform matroid <math>U{}^r_n</math> may be represented as a gammoid on a smaller graph, with only <math>n</math> vertices, by choosing a subset <math>S</math> of <math>r</math> vertices and connecting each of the chosen vertices to every other vertex in the graph. Again, a subset of the vertices of the graph can be endpoints of disjoint paths if and only if it has <math>r</math> or fewer vertices, because otherwise there are not enough vertices that can be starts of paths. In this graph, every vertex corresponds to an element of the matroid, showing that the uniform matroid is a strict gammoid.<ref>{{harvtxt|Oxley|2006}}, p. 100.</ref>\n\n==Menger's theorem and gammoid rank==\nThe rank of a set <math>X\\subset T</math> in a gammoid defined from a graph <math>G</math> and vertex subsets <math>S</math> and <math>T</math> is, by definition, the maximum number of vertex-disjoint paths from <math>S</math> to <math>X</math>. By [[Menger's theorem]], it also equals the minimum cardinality of a set <math>Y</math> that intersects every path from <math>S</math> to <math>X</math>.<ref name=\"schrijver\"/>\n\n==Relation to transversal matroids==\nA [[transversal matroid]] is defined from a [[family of sets]]: its elements are the elements of the sets, and a set <math>X</math> of these elements is independent whenever there exists a one-to-one matching of the elements of <math>X</math> to disjoint sets containing them, called a [[system of distinct representatives]]. Equivalently, a transversal matroid may be represented by a special kind of gammoid, defined from a directed [[bipartite graph]] <math>(S,T,E)</math> that has a vertex in <math>S</math> for each set, a vertex in <math>T</math> for each element, and an edge from each set to each element contained in it.\n\nLess trivially, the strict gammoids are exactly the [[dual matroid]]s of the transversal matroids. To see that every strict gammoid is dual to a transversal matroid, let <math>\\gamma</math> be a strict gammoid defined from a directed graph <math>G</math> and starting vertex set <math>S</math>, and consider the transversal matroid for the family of sets <math>N_v</math> for each vertex <math>v\\in V(G)\\setminus S</math>, where vertex <math>u</math> belongs to <math>N_v</math> if it equals <math>v</math> or it has an edge to <math>v</math>. Any basis of the strict gammoid, consisting of the endpoints of some set of <math>|S|</math> disjoint paths from <math>S</math>, is the complement of a basis of the transversal matroid, matching each <math>N_v</math> to the vertex <math>u</math> such that <math>uv</math> is a path edge (or <math>v</math> itself, if <math>v</math> does not participate in one of the paths). Conversely every basis of the transversal matroid, consisting of a representative <math>u_v</math> for each <math>N_v</math>, gives rise to a complementary basis of the strict gammoid, consisting of the endpoints of the paths formed by the set of edges <math>u_vv</math>.<ref name=\"schrijver\"/><ref name=\"pi\">{{citation\n | last1 = Ingleton | first1 = A. W.\n | last2 = Piff | first2 = M. J.\n | doi = 10.1016/0095-8956(73)90031-2\n | journal = Journal of Combinatorial Theory\n | mr = 0329936\n | pages = 51–68\n | series = Series B\n | title = Gammoids and transversal matroids\n | volume = 15\n | year = 1973}}.</ref>\n\nTo see, conversely, that every transversal matroid is dual to a strict gammoid, find a subfamily of the sets defining the matroid such that the subfamily has a system of distinct representatives and defines the same matroid. Form a graph that has the union of the sets as its vertices and that has an edge to the representative element of each set from the other members of the same set. Then the sets <math>N_v</math> formed as above for each representative element <math>v</math> are exactly the sets defining the original transversal matroid, so the strict gammoid formed by this graph and by the set of representative elements is dual to the given transversal matroid.<ref name=\"schrijver\"/><ref name=\"pi\"/>\n\nEvery gammoid is a [[matroid minor|contraction]] of a transversal matroid. The gammoids are the smallest class of matroids that includes the transversal matroids and is closed under duality and taking [[matroid minor|minor]]s.<ref name=\"schrijver\"/><ref name=Ox115>{{harvnb|Oxley|2006|p=115}}</ref><ref>{{citation\n | last = Welsh | first = D. J. A. | authorlink = Dominic Welsh\n | isbn = 9780486474397\n | pages = 222–223\n | publisher = Courier Dover Publications\n | title = Matroid Theory\n | url = https://books.google.com/books?id=QL2iYMBLpFwC&pg=PA222\n | year = 2010}}.</ref>\n\n==Representability==\nIt is not true that every gammoid is [[regular matroid|regular]], i.e., [[Matroid representation|representable]] over every field. In particular, the uniform matroid <math>U{}^2_4</math> is not a binary matroid, and more generally the <math>n</math>-point line <math>U{}^2_n</math> can only be represented over fields with <math>n-1</math> or more elements. However, every gammoid may be represented over almost every [[finite field]].<ref name=\"mason72\"/><ref name=\"schrijver\"/> More specifically, a gammoid with element set <math>S</math> may be represented over every [[field (mathematics)|field]] that has at least <math>2^{|S|}</math> elements.<ref name=\"schrijver\"/><ref>{{citation\n | last = Atkin | first = A. O. L.\n | doi = 10.1016/0095-8956(72)90053-6\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 0316281\n | pages = 179–182\n | series = Series B\n | title = Remark on a paper of Piff and Welsh\n | volume = 13\n | year = 1972}}.</ref><ref>{{citation\n | last = Lindström | first = Bernt\n | doi = 10.1112/blms/5.1.85\n | journal = The Bulletin of the London Mathematical Society\n | mr = 0335313\n | pages = 85–90\n | title = On the vector representations of induced matroids\n | volume = 5\n | year = 1973}}.</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Matroid theory]]\n[[Category:Graph connectivity]]"
    },
    {
      "title": "Geometric lattice",
      "url": "https://en.wikipedia.org/wiki/Geometric_lattice",
      "text": "In the mathematics of [[matroid]]s and [[lattice (order)|lattices]], a '''geometric lattice''' is a [[finite set|finite]] [[Atom (order theory)|atomistic]] [[semimodular lattice]], and a '''matroid lattice''' is an atomistic semimodular lattice without the assumptions of finiteness. Geometric lattices and matroid lattices, respectively, form the lattices of flats of finite and infinite matroids, and every geometric or matroid lattice comes from a matroid in this way.\n\n==Definition==\nA '''[[lattice (order)|lattice]]''' is a [[partially ordered set|poset]] in which any two elements <math>x</math> and <math>y</math> have both a [[supremum]], denoted by <math>x\\vee y</math>, and an [[infimum]], denoted by <math>x\\wedge y</math>.\n: The following definitions apply to posets in general, not just lattices, except where otherwise stated.\n* For a [[minimal element]] <math>x</math>, there is no element <math>y</math> such that <math>y < x</math>.\n* An element <math>x</math> '''[[covering relation|covers]]''' another element <math>y</math> (written as <math>x :> y</math> or <math> y <: x</math>) if <math>x > y</math> and there is no element <math>z</math> distinct from both <math>x</math> and <math>y</math> so that <math>x > z > y</math>.\n* A cover of a minimal element is called an '''[[Atom (order theory)|atom]]'''.\n* A lattice is '''[[atomistic (order theory)|atomistic]]''' if every element is the supremum of some set of atoms.\n* A poset is '''[[Graded poset|graded]]''' when it can be given a rank function <math>r(x)</math> mapping its elements to integers, such that <math>r(x)>r(y)</math> whenever <math>x>y</math>, and in particular <math>r(x)=r(y)+1</math> whenever <math>x :> y</math>.\n: When a graded poset has a bottom element, one may assume, without loss of generality, that its rank is zero.  In this case, the atoms are the elements with rank one.\n* A graded lattice is '''[[semimodular lattice|semimodular]]''' if, for every <math>x</math> and <math>y</math>, its rank function obeys the identity<ref>{{harvtxt|Birkhoff|1995}}, Theorem 15, p.&nbsp;40. More precisely, Birkhoff's definition reads \"We shall call P (upper) semimodular when it satisfies: If ''a''≠''b'' both cover ''c'', then there exists a ''d''∈''P'' which covers both ''a'' and ''b''\" (p.39). Theorem 15 states: \"A graded lattice of finite length is semimodular if and only if ''r''(''x'')+''r''(''y'')≥''r''(''x''∧''y'')+''r''(''x''∨''y'')\".</ref>\n:: <math>r(x)+r(y)\\ge r(x\\wedge y)+r(x\\vee y). \\, </math>\n* A '''matroid lattice''' is a lattice that is both atomistic and semimodular.<ref>{{citation\n | last1 = Maeda | first1 = F.\n | last2 = Maeda | first2 = S.\n | location = New York\n | mr = 0282889\n | publisher = Springer-Verlag\n | series = Die Grundlehren der mathematischen Wissenschaften, Band 173\n | title = Theory of Symmetric Lattices\n | year = 1970}}.</ref><ref>{{citation\n | last = Welsh | first = D. J. A.\n | isbn = 9780486474397\n | page = 388\n | publisher = Courier Dover Publications\n | title = Matroid Theory\n | year = 2010}}.</ref>  A '''geometric lattice''' is a ''finite'' matroid lattice.<ref name=\"w10-51\">{{harvtxt|Welsh|2010}}, p.&nbsp;51.</ref>\n: Some authors consider only finite matroid lattices, and use the terms \"geometric lattice\" and \"matroid lattice\" interchangeably for both.<ref>{{citation|title=Lattice Theory|volume=25|series=Colloquium Publications|publisher=American Mathematical Society|first=Garrett|last=Birkhoff|authorlink=Garrett Birkhoff|edition=3rd|year=1995|isbn=9780821810255|page=80|url=https://books.google.com/books?id=0Y8d-MdtVwkC&pg=PA80}}.</ref>\n\n==Cryptomorphism==\nThe geometric lattices are [[cryptomorphism|cryptomorphic]] to (finite, simple) matroids, and the matroid lattices are cryptomorphic to simple matroids without the assumption of finiteness.\n\nLike geometric lattices, matroids are endowed with [[matroid rank|rank functions]], but these functions map sets of elements to numbers rather than taking individual elements as arguments. The rank function of a matroid must be monotonic (adding an element to a set can never decrease its rank) and they must be [[submodular function]]s, meaning that they obey an inequality similar to the one for semimodular lattices:\n\n:<math>r(X)+r(Y)\\ge r(X\\cap Y)+r(X\\cup Y). \\,</math>\n\nThe [[maximal element|maximal]] sets of a given rank are called flats. The intersection of two flats is again a flat, defining a greatest lower bound operation on pairs of flats; one can also define a least upper bound of a pair of flats to be the (unique) maximal superset of their union that has the same rank as their union. In this way, the flats of a matroid form a matroid lattice, or (if the matroid is finite) a geometric lattice.<ref name=\"w10-51\"/>\n\nConversely, if <math>L</math> is a matroid lattice, one may define a rank function on sets of its atoms, by defining the rank of a set of atoms to be the lattice rank of the greatest lower bound of the set. This rank function is necessarily monotonic and submodular, so it defines a matroid. This matroid is necessarily simple, meaning that every two-element set has rank two.<ref name=\"w10-51\"/>\n\nThese two constructions, of a simple matroid from a lattice and of a lattice from a matroid, are inverse to each other: starting from a geometric lattice or a simple matroid, and performing both constructions one after the other, gives a lattice or matroid that is isomorphic to the original one.<ref name=\"w10-51\"/>\n\n==Duality==\nThere are two different natural notions of duality for a geometric lattice <math>L</math>: the dual matroid, which has as its basis sets the [[complement (set theory)|complements]] of the bases of the matroid corresponding to <math>L</math>, and the [[order dual|dual lattice]], the lattice that has the same elements as <math>L</math> in the reverse order. They are not the same, and indeed the dual lattice is generally not itself a geometric lattice: the property of being atomistic is not preserved by order-reversal. {{harvtxt|Cheung|1974}} defines the [[adjoint]] of a geometric lattice <math>L</math> (or of the matroid defined from it) to be a minimal geometric lattice into which the dual lattice of <math>L</math> is [[order embedding|order-embedded]]. Some matroids do not have adjoints; an example is the [[Vámos matroid]].<ref>{{citation\n | last = Cheung | first = Alan L. C.\n | doi = 10.4153/CMB-1974-066-5\n | issue = 3\n | journal = [[Canadian Mathematical Bulletin]]\n | mr = 0373976\n | pages = 363–365; correction, ibid. 17 (1974), no. 4, 623\n | title = Adjoints of a geometry\n | volume = 17\n | year = 1974}}.</ref>\n\n==Additional properties==\nEvery interval of a geometric lattice (the subset of the lattice between given lower and upper bound elements) is itself geometric; taking an interval of a geometric lattice corresponds to forming a [[matroid minor|minor]] of the associated matroid. Geometric lattices are [[complemented lattice|complemented]], and because of the interval property they are also relatively complemented.<ref>{{harvtxt|Welsh|2010}}, pp. 55, 65–67.</ref>\n\nEvery finite lattice is a sublattice of a geometric lattice.<ref>{{harvtxt|Welsh|2010}}, p. 58; Welsh credits this result to [[Robert P. Dilworth]], who proved it in 1941–1942, but does not give a specific citation for its original proof.</ref>\n\n==References==\n{{reflist}}\n\n==External links==\n* {{planetmath reference|id=7972|title=Geometric lattice}}\n* {{OEIS el|A281574|Number of unlabeled geometric lattices with ''n'' elements}}\n\n[[Category:Lattice theory]]\n[[Category:Matroid theory]]"
    },
    {
      "title": "Matroid girth",
      "url": "https://en.wikipedia.org/wiki/Matroid_girth",
      "text": "In [[matroid theory]], a mathematical discipline, the '''girth''' of a matroid is the size of its smallest circuit or dependent set. The '''cogirth''' of a matroid is the girth of its [[dual matroid]]. Matroid girth generalizes the notion of the shortest cycle in a graph, the edge connectivity of a graph, Hall sets in bipartite graphs, even sets in families of sets, and general position of point sets. It is hard to compute, but [[parameterized complexity|fixed-parameter tractable]] for linear matroids when parameterized both by the [[matroid rank]] and the field size of a linear representation.\n\n==Examples==\nThe \"girth\" terminology generalizes the use of [[Girth (graph theory)|girth in graph theory]], meaning the length of the shortest cycle in a graph: the girth of a [[graphic matroid]] is the same as the girth of its underlying graph.<ref name=\"ccd\">{{citation\n | last1 = Cho | first1 = Jung Jin\n | last2 = Chen | first2 = Yong\n | last3 = Ding | first3 = Yu\n | doi = 10.1016/j.dam.2007.06.015\n | issue = 18\n | journal = Discrete Applied Mathematics\n | mr = 2365057\n | pages = 2456–2470\n | title = On the (co)girth of a connected matroid\n | volume = 155\n | year = 2007}}.</ref>\n\nThe girth of other classes of matroids also corresponds to important combinatorial problems. For instance, the girth of a co-graphic matroid (or the cogirth of a graphic matroid) equals the [[k-edge-connected graph|edge connectivity]] of the underlying graph, the number of edges in a [[minimum cut]] of the graph.<ref name=\"ccd\"/> The girth of a [[transversal matroid]] gives the cardinality of a minimum Hall set in a bipartite graph: this is a set of vertices on one side of the bipartition that does not form the set of endpoints of a [[matching (graph theory)|matching]] in the graph.<ref name=\"prs\">{{citation\n | last1 = Panolan | first1 = Fahad\n | last2 = Ramanujan | first2 = M. S.\n | last3 = Saurabh | first3 = Saket\n | editor1-last = Dehne | editor1-first = Frank\n | editor2-last = Sack | editor2-first = Jörg-Rüdiger\n | editor3-last = Stege | editor3-first = Ulrike\n | contribution = On the parameterized complexity of girth and connectivity problems on linear matroids\n | contribution-url = http://people.scs.carleton.ca/~wads/WADS2015-papers/paper_85.pdf\n | doi = 10.1007/978-3-319-21840-3_47\n | pages = 566–577\n | publisher = Springer\n | series = Lecture Notes in Computer Science\n | title = Algorithms and Data Structures: 14th International Symposium, WADS 2015, Victoria, BC, Canada, August 5-7, 2015, Proceedings\n | volume = 9214\n | year = 2015}}.</ref>\n\nAny set of points in [[Euclidean space]] gives rise to a real [[linear matroid]] by interpreting the [[Cartesian coordinate]]s of the points as the [[Vector (mathematics and physics)|vectors]] of a [[matroid representation]].\nThe girth of the resulting matroid equals one plus the dimension of the space when the underlying set of point is in [[general position]], and is smaller otherwise.\nGirths of real linear matroids also arise in [[compressed sensing]], where the same concept is referred to as the [[Spark (mathematics)|spark]] of a matrix.<ref>{{Citation\n  | last = Donoho  | first = David L.  | author-link = David Donoho\n  | last2 = Elad  | first2 = Michael\n  | title = Optimally sparse representation in general (nonorthogonal) dictionaries via ℓ<sup>1</sup> minimization\n  | journal = [[Proceedings of the National Academy of Sciences of the United States of America]]\n  | volume = 100\n  | issue = 5\n  | pages = 2197–2202\n  | year = 2003\n  | doi = 10.1073/pnas.0437847100\n  | pmid = 16576749\n  | pmc = 153464 }}.</ref>\n\nThe girth of a [[binary matroid]] gives the cardinality of a minimum even set, a subcollection of a family of sets that includes an even number of copies of each set element.<ref name=\"prs\"/>\n\n==Computational complexity==\nDetermining the girth of a [[binary matroid]] is [[NP-hard]].<ref>{{harvtxt|Cho|Chen|Ding|2007}} observe that this is a corollary of a result of [[Alexander Vardy]] in coding theory: {{citation\n | last = Vardy | first = Alexander\n | doi = 10.1109/18.641542\n | issue = 6\n | journal = IEEE Transactions on Information Theory\n | mr = 1481035\n | pages = 1757–1766\n | title = The intractability of computing the minimum distance of a code\n | volume = 43\n | year = 1997}}.</ref>\n \nAdditionally, determining the girth of a linear matroid given by a matrix representing the matroid is [[parameterized complexity|W[1]-hard]] when parameterized by the girth or by the rank of the matroid, but fixed-parameter tractable when parameterized by a combination of the rank and the size of the underlying [[field (mathematics)|field]].<ref name=\"prs\"/>\n\nFor an arbitrary matroid, given by a [[matroid oracle|independence oracle]], it is impossible to find the girth using a subexponential number of matroid queries.<ref>{{citation\n | last1 = Jensen | first1 = Per M.\n | last2 = Korte | first2 = Bernhard\n | doi = 10.1137/0211014\n | issue = 1\n | journal = [[SIAM Journal on Computing]]\n | mr = 646772\n | pages = 184–190\n | title = Complexity of matroid property algorithms\n | volume = 11\n | year = 1982}}.</ref> Similarly, for a real linear matroid of rank {{mvar|r}}, with {{mvar|n}} elements, described by an oracle that gives the [[oriented matroid|orientation]] of any {{mvar|r}}-tuple of elements, it requires <math>\\Omega(n^{r-1})</math> oracle queries to determine the girth.<ref>{{citation\n | last1 = Erickson | first1 = J.\n | last2 = Seidel | first2 = R. | author2-link = Raimund Seidel\n | doi = 10.1007/BF02574027\n | issue = 1\n | journal = [[Discrete and Computational Geometry]]\n | mr = 1300508\n | pages = 41–57\n | title = Better lower bounds on detecting affine and spherical degeneracies\n | volume = 13\n | year = 1995}}.</ref>\n\nComputations using a girth oracle (an oracle that reports the smallest dependent subset of a given set of elements) have also been considered.<ref>{{citation\n | last1 = Hausmann | first1 = D.\n | last2 = Korte | first2 = B.\n | contribution = Algorithmic versus axiomatic definitions of matroids\n | doi = 10.1007/BFb0120924\n | volume = 14\n | mr = 600125\n | pages = 98–111\n | series = Mathematical Programming Studies\n | title = Mathematical programming at Oberwolfach (Proc. Conf., Math. Forschungsinstitut, Oberwolfach, 1979)\n | year = 1981}}.</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Matroid theory|Girth]]"
    },
    {
      "title": "Graphic matroid",
      "url": "https://en.wikipedia.org/wiki/Graphic_matroid",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Matroid whose independent sets are forests in an undirected graph}}\nIn the mathematical theory of [[Matroid theory|matroid]]s, a '''graphic matroid''' (also called a '''cycle matroid''' or '''polygon matroid''') is a [[matroid]] whose independent sets are the [[tree (graph theory)|forests]] in a given finite [[undirected graph]]. The [[dual matroid]]s of graphic matroids are called '''co-graphic matroids''' or '''bond matroids'''.<ref>{{harvtxt|Tutte|1965}} uses a reversed terminology, in which he called bond matroids \"graphic\" and cycle matroids \"co-graphic\", but this has not been followed by later authors.</ref> A matroid that is both graphic and co-graphic is called a '''planar matroid'''; these are exactly the graphic matroids formed from [[planar graph]]s.\n\n==Definition==\nA [[matroid]] may be defined as a family of finite sets (called the \"independent sets\" of the matroid) that is closed under subsets and that satisfies the \"exchange property\": if sets <math>A</math> and <math>B</math> are both independent, and <math>A</math> is larger than <math>B</math>, then there is an element <math>x\\in A\\setminus B</math> such that <math>B\\cup\\{x\\}</math> remains independent.  If <math>G</math> is an undirected graph, and <math>F</math> is the family of sets of edges that form forests in <math>G</math>, then <math>F</math> is clearly closed under subsets (removing edges from a forest leaves another forest). It also satisfies the exchange property: if <math>A</math> and <math>B</math> are both forests, and <math>A</math> has more edges than <math>B</math>, then it has fewer connected components, so by the [[pigeonhole principle]] there is a component <math>C</math> of <math>A</math> that contains vertices from two or more components of <math>B</math>. Along any path in <math>C</math> from a vertex in one component of <math>B</math> to a vertex of another component, there must be an edge with endpoints in two components, and this edge may be added to <math>B</math> to produce a forest with more edges. Thus, <math>F</math> forms the independent sets of a matroid, called the graphic matroid of <math>G</math> or <math>M(G)</math>. More generally, a matroid is called graphic whenever it is [[isomorphic]] to the graphic matroid of a graph, regardless of whether its elements are themselves edges in a graph.<ref name=\"tutte65\"/>\n\nThe bases of a graphic matroid <math>M(G)</math> are the [[spanning tree|spanning forests]] of <math>G</math>, and the circuits of <math>M(G)</math> are the [[cycle (graph theory)|simple cycles]] of <math>G</math>. The [[Matroid rank|rank]] in <math>M(G)</math> of a set <math>X</math> of edges of a graph <math>G</math> is <math>r(X)=n-c</math> where <math>n</math> is the number of vertices in the [[Glossary of graph theory#Subgraphs|subgraph]] formed by the edges in <math>X</math> and <math>c</math> is the number of connected components of the same subgraph.<ref name=\"tutte65\"/> The corank of the graphic matroid is known as the [[circuit rank]] or cyclomatic number.\n\n==The lattice of flats==\nThe [[matroid|closure]] <math>\\operatorname{cl}(S)</math> of a set <math>S</math> of edges in <math>M(G)</math> is a [[matroid|flat]] consisting of the edges that are not independent of <math>S</math> (that is, the edges whose endpoints are connected to each other by a path in <math>S</math>). This flat may be identified with the partition of the vertices of <math>G</math> into the [[Connected component (graph theory)|connected components]] of the subgraph formed by <math>S</math>: Every set of edges having the same closure as <math>S</math> gives rise to the same partition of the vertices, and <math>\\operatorname{cl}(S)</math> may be recovered from the partition of the vertices, as it consists of the edges whose endpoints both belong to the same set in the partition. In the [[geometric lattice|lattice of flats]] of this matroid, there is an order relation <math>x\\le y</math> whenever the partition corresponding to flat&nbsp;<math>x</math> is a refinement of the partition corresponding to flat&nbsp;<math>y</math>.\n\nIn this aspect of graphic matroids, the graphic matroid for a [[complete graph]] <math>K_n</math> is particularly important, because it allows every possible partition of the vertex set to be formed as the set of connected components of some subgraph. Thus, the lattice of flats of the graphic matroid of <math>K_n</math> is naturally isomorphic to the [[partition of a set|lattice of partitions of an <math>n</math>-element set]]. Since the lattices of flats of matroids are exactly the [[geometric lattice]]s, this implies that the lattice of partitions is also geometric.<ref>{{citation|title=Lattice Theory|volume=25|series=Colloquium Publications|publisher=American Mathematical Society|first=Garrett|last=Birkhoff|authorlink=Garrett Birkhoff|edition=3rd|year=1995|isbn=9780821810255|page=95|url=https://books.google.com/books?id=0Y8d-MdtVwkC&pg=PA95}}.</ref> <!-- test -->\n\n==Representation==\nThe graphic matroid of a graph <math>G</math> can be defined as the column matroid of any [[incidence matrix|oriented incidence matrix]] of <math>G</math>. Such a matrix has one row for each vertex, and one column for each edge. The column for edge <math>e</math> has <math>+1</math> in the row for one endpoint, <math>-1</math> in the row for the other endpoint, and <math>0</math> elsewhere; the choice of which endpoint to give which sign is arbitrary. The column matroid of this matrix has as its independent sets the linearly independent subsets of columns.\n\nIf a set of edges contains a cycle, then the corresponding columns (multiplied by <math>-1</math> if necessary to reorient the edges consistently around the cycle) sum to zero, and is not independent. Conversely, if a set of edges forms a forest, then by repeatedly removing leaves from this forest it can be shown by induction that the corresponding set of columns is independent. Therefore, the column matrix is isomorphic to <math>M(G)</math>.\n\nThis method of representing graphic matroids works regardless of the [[field (mathematics)|field]] over which the incidence is defined. Therefore, graphic matroids form a subset of the [[regular matroid]]s, matroids that have [[Matroid representation|representations]] over all possible fields.<ref name=\"tutte65\"/>\n\nThe lattice of flats of a graphic matroid can also be realized as the lattice of a [[Arrangement of hyperplanes|hyperplane arrangement]], in fact as a subset of the [[Braid group|braid arrangement]], whose hyperplanes are the diagonals <math>H_{ij}=\\{(x_1,\\ldots,x_n) \\in \\mathbb{R}^n \\mid x_i = x_j\\}</math>. Namely, if the vertices of <math>G</math> are <math>v_1,\\ldots,v_n,</math> we include the hyperplane <math>H_{ij}</math> whenever <math>e = v_iv_j</math> is an edge of <math>G</math>.\n\n==Matroid connectivity==\nA matroid is said to be connected if it is not the direct sum of two smaller matroids; that is, it is connected if and only if there do not exist two disjoint subsets of elements such that the rank function of the matroid equals the sum of the ranks in these separate subsets. Graphic matroids are connected if and only if the underlying graph is both [[connected graph|connected]] and [[k-vertex-connected graph|2-vertex-connected]].<ref name=\"tutte65\"/>\n\n==Minors and duality==\n[[File:Nonisomorphic planar duals.svg|thumb|300px|Two different graphs (red) that are duals of the same planar graph (pale blue). Despite being non-isomorphic as graphs, they have isomorphic graphic matroids.]]\nA matroid is graphic if and only if its [[Matroid minor|minors]] do not include any of five forbidden minors: the [[uniform matroid]] <math>U{}^2_4</math>, the [[Fano plane]] or its dual, or the duals of <math>M(K_5)</math> and <math>M(K_{3,3})</math> defined from the [[complete graph]] <math>K_5</math> and the [[complete bipartite graph]] <math>K_{3,3}</math>.<ref name=\"tutte65\">{{citation\n | last = Tutte | first = W. T.\n | journal = Journal of Research of the National Bureau of Standards\n | mr = 0179781\n | pages = 1–47\n | title = Lectures on matroids\n | url = http://cdm16009.contentdm.oclc.org/cdm/ref/collection/p13011coll6/id/66650\n | volume = 69B\n | year = 1965\n | doi=10.6028/jres.069b.001}}. See in particular section 2.5, \"Bond-matroid of a graph\", pp. 5–6, section 5.6, \"Graphic and co-graphic matroids\", pp. 19–20, and section 9, \"Graphic matroids\", pp. 38–47.</ref><ref>{{citation\n | last = Seymour | first = P. D. | authorlink = Paul Seymour (mathematician)\n | doi = 10.1016/S0167-5060(08)70855-0\n | journal = Annals of Discrete Mathematics\n | mr = 597159\n | pages = 83–90\n | title = On Tutte's characterization of graphic matroids\n | volume = 8\n | year = 1980}}.</ref><ref>{{citation\n | last = Gerards | first = A. M. H.\n | doi = 10.1002/jgt.3190200311\n | issue = 3\n | journal = Journal of Graph Theory\n | mr = 1355434\n | pages = 351–359\n | title = On Tutte's characterization of graphic matroids—a graphic proof\n | volume = 20\n | year = 1995}}.</ref> The first three of these are the forbidden minors for the regular matroids,<ref>{{citation\n | last = Tutte | first = W. T. | authorlink = W. T. Tutte\n | journal = Transactions of the American Mathematical Society\n | mr = 0101526\n | pages = 144–174\n | title = A homotopy theorem for matroids. I, II\n | volume = 88\n | year = 1958\n | doi=10.2307/1993244}}.</ref> and the duals of <math>M(K_5)</math> and <math>M(K_{3,3})</math> are regular but not graphic.\n\nIf a matroid is graphic, its dual (a \"co-graphic matroid\") cannot contain the duals of these five forbidden minors. Thus, the dual must also be regular, and cannot contain as minors the two graphic matroids <math>M(K_5)</math> and <math>M(K_{3,3})</math>.<ref name=\"tutte65\"/>\n\nBecause of this characterization and [[Wagner's theorem]] characterizing the [[planar graph]]s as the graphs with no <math>K_5</math> or <math>K_{3,3}</math> [[graph minor]], it follows that a graphic matroid <math>M(G)</math> is co-graphic if and only if <math>G</math> is planar; this is [[Whitney's planarity criterion]]. If <math>G</math> is planar, the dual of <math>M(G)</math> is the graphic matroid of the [[dual graph]] of <math>G</math>. While <math>G</math> may have multiple dual graphs, their graphic matroids are all isomorphic.<ref name=\"tutte65\"/>\n\n==Algorithms==\nA minimum weight basis of a graphic matroid is a [[minimum spanning tree]] (or minimum spanning forest, if the underlying graph is disconnected). Algorithms for computing minimum spanning trees have been intensively studied; it is known how to solve the problem in linear randomized expected time in a comparison model of computation,<ref>{{citation\n | last1 = Karger | first1 = David R. | author1-link = David Karger\n | last2 = Klein | first2 = Philip N.\n | last3 = Tarjan | first3 = Robert E. | author3-link = Robert Tarjan\n | doi = 10.1145/201019.201022\n | mr = 1409738\n | issue = 2\n | journal = [[Journal of the Association for Computing Machinery]]\n | pages = 321–328\n | title = A randomized linear-time algorithm to find minimum spanning trees\n | volume = 42\n | year = 1995}}</ref> or in linear time in a model of computation in which the edge weights are small integers and bitwise operations are allowed on their binary representations.<ref>{{citation\n | last1 = Fredman | first1 = M. L. | author1-link = Michael Fredman\n | last2 = Willard | first2 = D. E. | author2-link = Dan Willard\n | doi = 10.1016/S0022-0000(05)80064-9\n | mr = 1279413\n | issue = 3\n | journal = [[Journal of Computer and System Sciences]]\n | pages = 533–551\n | title = Trans-dichotomous algorithms for minimum spanning trees and shortest paths\n | volume = 48\n | year = 1994}}.</ref> The fastest known time bound that has been proven for a deterministic algorithm is slightly superlinear.<ref>{{citation\n | last = Chazelle | first = Bernard | authorlink = Bernard Chazelle\n | doi = 10.1145/355541.355562\n | mr = 1866456\n | issue = 6\n | journal = [[Journal of the Association for Computing Machinery]]\n | pages = 1028–1047\n | title = A minimum spanning tree algorithm with inverse-Ackermann type complexity\n | volume = 47\n | year = 2000}}.</ref>\n\nSeveral authors have investigated algorithms for testing whether a given matroid is graphic.<ref>{{citation\n | last = Tutte | first = W. T. | authorlink = W. T. Tutte\n | journal = Proceedings of the American Mathematical Society\n | mr = 0117173\n | pages = 905–917\n | title = An algorithm for determining whether a given binary matroid is graphic.\n | volume = 11\n | year = 1960\n | doi=10.2307/2034435}}.</ref><ref>{{citation\n | last1 = Bixby | first1 = Robert E.\n | last2 = Cunningham | first2 = William H.\n | doi = 10.1287/moor.5.3.321\n | issue = 3\n | journal = Mathematics of Operations Research\n | mr = 594849\n | pages = 321–357\n | title = Converting linear programs to network problems\n | volume = 5\n | year = 1980}}.</ref><ref>{{citation\n | last = Seymour | first = P. D. | authorlink = Paul Seymour (mathematician)\n | doi = 10.1007/BF02579179\n | issue = 1\n | journal = Combinatorica\n | mr = 602418\n | pages = 75–78\n | title = Recognizing graphic matroids\n | volume = 1\n | year = 1981}}.</ref> For instance, an algorithm of {{harvtxt|Tutte|1960}} solves this problem when the input is known to be a [[binary matroid]]. {{harvtxt|Seymour|1981}} solves this problem for arbitrary matroids given access to the matroid only through an [[matroid oracle|independence oracle]], a subroutine that determines whether or not a given set is independent.\n\n==Related classes of matroids==\nSome classes of matroid have been defined from well-known families of graphs, by phrasing a characterization of these graphs in terms that make sense more generally for matroids. These include the [[bipartite matroid]]s, in which every circuit is even, and the [[Eulerian matroid]]s, which can be partitioned into disjoint circuits. A graphic matroid is bipartite if and only if it comes from a [[bipartite graph]] and a graphic matroid is Eulerian if and only if it comes from an [[Eulerian graph]]. Within the graphic matroids (and more generally within the [[binary matroid]]s) these two classes are dual: a graphic matroid is bipartite if and only if its [[dual matroid]] is Eulerian, and a graphic matroid is Eulerian if and only if its dual matroid is bipartite.<ref name=\"w69\">{{citation\n | last = Welsh | first = D. J. A. | authorlink = Dominic Welsh\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 0237368\n | pages = 375–377\n | title = Euler and bipartite matroids\n | volume = 6\n | year = 1969\n | doi=10.1016/s0021-9800(69)80033-5}}.</ref>\n\nGraphic matroids are one-dimensional [[rigidity matroid]]s, matroids describing the degrees of freedom of structures of rigid beams that can rotate freely at the vertices where they meet. In one dimension, such a structure has a number of degrees of freedom equal to its number of connected components (the number of vertices minus the matroid rank) and in higher dimensions the number of degrees of freedom of a ''d''-dimensional structure with ''n'' vertices is ''dn'' minus the matroid rank. In two-dimensional rigidity matroids, the [[Laman graph]]s play the role that spanning trees play in graphic matroids, but the structure of rigidity matroids in dimensions greater than two is not well understood.<ref name=\"whiteley\">{{citation\n | last = Whiteley | first = Walter | authorlink = Walter Whiteley\n | contribution = Some matroids from discrete applied geometry\n | doi = 10.1090/conm/197/02540\n | location = Providence, RI\n | mr = 1411692\n | pages = 171–311\n | publisher = American Mathematical Society\n | series = Contemporary Mathematics\n | title = Matroid theory (Seattle, WA, 1995)\n | volume = 197\n | year = 1996}}.</ref>\n\n==References==\n{{reflist|colwidth=30em}}\n\n[[Category:Matroid theory]]\n[[Category:Planar graphs]]\n[[Category:Graph connectivity]]\n[[Category:Spanning tree]]"
    },
    {
      "title": "Hereditary property",
      "url": "https://en.wikipedia.org/wiki/Hereditary_property",
      "text": "In [[mathematics]], a '''hereditary property''' is a property of an object, that inherits to all its  ''subobjects'', where the term subobject depends on the context. These properties are particularly considered in [[topology]] and [[graph theory]], but also in [[set theory]].\n\n==In topology==\nIn [[topology]], a [[topological property]] is said to be ''hereditary'' if whenever a [[topological space]] has that property, then so does every [[Subspace topology|subspace]] of it.  If the latter is true only for [[Closed set|closed subspaces]], then the property is called ''weakly hereditary'' or\n''closed-hereditary''.\n\nFor example, [[second countability]] and [[metrisability]] are hereditary properties. [[sequential space|Sequentiality]] and [[Compact space|Hausdorff compactness]] are weakly hereditary, but not hereditary.<ref>*Goreham, Anthony, \"[http://www.citebase.org/cgi-bin/citations?id=oai:arXiv.org:math/0412558 Sequential Convergence in Topological Spaces]</ref> [[Connected space|Connectivity]] is not weakly hereditary.\n\nIf ''P'' is a property of a topological space ''X'' and every subspace also has property ''P'', then ''X'' is said to be \"hereditarily ''P''\".\n\n==In graph theory==\nIn [[graph theory]], a ''hereditary property'' is a [[graph property|property]] of a [[graph (discrete mathematics)|graph]] which also holds for (is \"inherited\" by) its [[induced subgraph]]s.<ref name=\"AS\">{{Cite journal | last = Alon|first = Noga|author-link = Noga Alon|last2 = Shapira| first2 = Asaf|title = Every monotone graph property is testable|journal = SIAM Journal on Computing|volume = 38|issue = 2|year = 2008|pages = 505–522|doi = 10.1137/050633445|url = http://www.math.tau.ac.il/~nogaa/PDFS/monotone1.pdf|citeseerx = 10.1.1.108.2303}}</ref> Alternately, a hereditary property is preserved by the removal of vertices.  A graph class <math>\\mathcal{G}</math> is said hereditary if it is closed under induced subgraphs.  Examples of hereditary graph classes are independent graphs (graphs with no edges), which is a special case (with ''c'' = 1) of being ''c''-colorable for some number ''c'', being [[Tree (graph theory)|forest]]s, [[Planar graph|planar]], [[Clique (graph theory)|complete]], [[Complete multipartite graph|complete multipartite]] etc.\n\nIn some cases, the term \"hereditary\" has been defined with reference to [[graph minor]]s, but this is more properly called a '''minor-hereditary property'''. The [[Robertson–Seymour theorem]] implies that a minor-hereditary property may be characterized in terms of a finite set of [[forbidden minor]]s.\n\nThe term \"hereditary\" has been also used for graph properties that are closed with respect to taking subgraphs.<ref name=\"Survey\">{{citation\n | last1 = Borowiecki | first1 = Mieczysław\n | last2 = Broere | first2 = Izak\n | last3 = Frick | first3 = Marietjie\n | last4 = Mihók | first4 = Peter\n | last5 = Semanišin | first5 = Gabriel\n | doi = 10.7151/dmgt.1037\n | issue = 1\n | journal = Discussiones Mathematicae Graph Theory\n | mr = 1633268\n | pages = 5–50\n | title = A survey of hereditary properties of graphs\n | volume = 17\n | year = 1997}}</ref> In such a case, properties that are closed with respect to taking induced subgraphs, are called  '''induced-hereditary'''. This approach is used by the members of the scientific society [[Hereditarnia]] Club. The language of hereditary properties and induced-hereditary properties provides a powerful tool for study of structural properties of various types of generalized [[Graph coloring|colourings]].  The most important result from this area is the '''Unique Factorisation Theorem'''.<ref name=\"UFT\">{{cite journal|doi=10.1002/jgt.20062 | volume=49 | issue=1 | title=Factorizations and characterizations of induced-hereditary and compositive properties | year=2005 | journal=Journal of Graph Theory | pages=11–27 | last1 = Farrugia | first1 = Alastair}}</ref> \n \n===Monotone property=== <!--Monotone_property redirects here-->\nThere is no consensus for the meaning of \"'''monotone property'''\" in graph theory. Examples of definitions are:\n* Preserved by the removal of edges.<ref>{{cite journal | last1 = King | first1 = R | year = 1990 | title = A lower bound for the recognition of digraph properties | url = | journal = Combinatorica | volume = 10 | issue = | pages = 53–59 | doi=10.1007/bf02122695}}</ref>\n* Preserved by the removal of edges and vertices (i.e., the property should hold for all subgraphs).<ref name=\"AS\" />\n* Preserved by the addition of edges and vertices (i.e., the property should hold for all supergraphs).<ref>http://www.cs.ucsc.edu/~optas/papers/k-col-threshold.pdf</ref>\n* Preserved by the addition of edges.<ref>Spinrad, J. (2003), ''Efficient Graph Representations'', AMS Bookstore, {{isbn|0-8218-2815-0}}, p9.</ref> (This meaning is used in the statement of the [[Aanderaa–Karp–Rosenberg conjecture]].)\n\nThe complementary property of a property that is preserved by the removal of edges is preserved under the addition of edges. Hence some authors avoid this ambiguity by saying a property A is monotone if A or A<sup>C</sup> (the complement of A) is monotone.<ref>{{cite journal|author1=Ashish Goel|title=Monotone properties of random geometric graphs have sharp thresholds|year=2003|author2=Sanatan Rai|author3=Bhaskar Krishnamachari|doi=10.1214/105051605000000575|journal=Annals of Applied Probability|volume=15|issue=4|pages=2535–2552|arxiv=math.PR/0310232}}</ref> Some authors choose to resolve this by using the term  ''increasing'' monotone for properties preserved under the addition of some object, and ''decreasing'' monotone for those preserved under the removal of the same object.\n\n==In problem solving==\nIn [[planning]] and [[problem solving]], or more formally [[one-person game]]s, the search space is seen as a [[directed graph]] with ''states'' as nodes, and ''transitions'' as edges.  States can have properties, and such a property P is hereditary if ''for each state S that has P, each state that can be reached from S also has P''.\n\nThe subset of all states that have P plus the subset of all states that have ~P form a partition of the set of states called a [[hereditary partition]].  This notion can trivially be extended to more discriminating partitions by instead of properties, considering ''aspects'' of states and their domains.  If states have an aspect A, with d<sub>i</sub> &sub; D a partition of the domain D of A, then the subsets of states for which A&isin;d<sub>i</sub> form a hereditary partition of the total set of states.\n\nIf the current state and the goal state are in different elements of a hereditary partition, there is no path from the current state to the goal state — the problem has no solution.\n\nCan a checkers board be covered with domino tiles, each of which covers exactly two adjacent fields?  Yes.  What if we remove the top left and the bottom right field?  Then no covering is possible any more, because the difference between number of uncovered white fields and the number of uncovered black fields is 2, and adding a domino tile (which covers one white and one black field) keeps that number at 2.  For a total covering the number is 0, so a total covering cannot be reached from the start position.\n\nThis notion was first introduced by [[Laurent Siklóssy]] and Roach<ref>{{cite article |url=https://dl.acm.org/citation.cfm?id=1624822 |title=Proving the Impossible is impossible is possible}}</ref>.\n\n==In model theory==\nIn [[model theory]] and [[universal algebra]], a class ''K'' of [[structure (mathematical logic)|structures]] of a given [[signature (logic)|signature]] is said to have the ''hereditary property'' if every [[substructure (mathematics)|substructure]] of a structure in ''K'' is again in ''K''. A variant of this definition is used in connection with [[Fraïssé's theorem]]: A class ''K'' of finitely generated structures has the ''hereditary property'' if every finitely generated substructure is again in ''K''. See [[age (model theory)|age]].\n\n==In matroid theory==\nIn a [[matroid]], every subset of an independent set is again independent. This is also sometimes called the ''hereditary property.''\n\n==In set theory==\n[[Recursive definition]]s using the adjective \"hereditary\" are often encountered in [[set theory]].\n\nA [[Set (mathematics)|set]] is said to be [[Hereditary set|hereditary (or ''pure'')]] if all of its elements are hereditary sets. It is [[vacuously true]] that the empty set is a hereditary set, and thus the set <math>\\{\\varnothing\\}</math> containing only the [[empty set]] <math>\\varnothing</math> is a hereditary set, and [[recursion|recursively]] so is <math>\\{\\varnothing, \\{\\varnothing \\}\\}</math>, for example. In formulations of set theory that are intended to be interpreted in the [[von Neumann universe]] or to express the content of [[Zermelo–Fraenkel set theory]], all sets are hereditary, because the only sort of object that is even a candidate to be an element of a set is another set. Thus the notion of hereditary set is interesting only in a context in which there may be [[urelement]]s.\n\nA couple of notions are defined analogously:\n* A [[hereditarily finite set]] is defined as a [[finite set]] consisting of zero or more hereditarily finite sets. Equivalently, a set is hereditarily finite if and only if its [[transitive set|transitive closure]] is finite.\n* A [[hereditarily countable set]] is a [[countable set]] of hereditarily countable sets.  Assuming the [[axiom of countable choice]], then a set is hereditarily countable if and only if its transitive closure is countable.\n\nBased on the above, it follows that in ZFC a more general notion can be defined for any predicate <math>\\Phi(x)</math>. A set ''x'' is said to have ''hereditarily'' the property <math>\\Phi(x)</math> if ''x'' itself and all members of its transitive closure satisfy <math>\\Phi(y)</math>, i.e. <math>x\\cup \\mathop{\\rm tc}(x)\\subseteq \\{y\\mid\\Phi(y)\\}</math>. Equivalently, ''x'' hereditarily satisfies <math>\\Phi(x)</math> [[iff]] it is a member of a transitive subset of <math>\\{y\\mid\\Phi(y)\\}</math>.<ref>Azriel Levy (2002), ''Basic set theory'', p. 82</ref><ref>Thomas Forster (2003), ''Logic, induction and sets'', p. 197</ref> A property (of a set) is thus said to be hereditary if it is inherited by every subset. For example, being well-ordered is a hereditary property, and so it being finite.<ref>Judith Roitman (1990), ''Introduction to modern set theory'', p. 10</ref>\n\nIf we instantiate in the above schema <math>\\Phi(x)</math> with \"''x'' has cardinality less than κ\", we obtain the more general notion of a set being ''hereditarily of cardinality less than κ'', usually denoted by <math>H_\\kappa </math><ref>Levy (2002), p. 137</ref> or <math>H(\\kappa) </math>. We regain the two simple notions we introduced above as <math>H(\\omega)</math> being the set of hereditarily finite sets and <math>H(\\omega_1)</math> being the set of hereditarily countable sets.<ref>Kenneth Kunen (1983), ''Set theory'', p. 131</ref> (<math>\\omega_1</math> is the [[first uncountable ordinal]].)\n\n==References==\n<references/>\n\n{{set index article|mathematics}}\n\n[[Category:Graph theory]]\n[[Category:Set theory]]\n[[Category:Model theory]]\n[[Category:Matroid theory]]\n\n[[ru:Наследственное свойство]]"
    },
    {
      "title": "K-set (geometry)",
      "url": "https://en.wikipedia.org/wiki/K-set_%28geometry%29",
      "text": "[[Image:k-sets.svg|thumb|300px|A set of six points (red), its six 2-sets (the sets of points contained in the blue ovals), and lines separating each k-set from the remaining points (dashed black).]]\nIn [[discrete geometry]], a ''k''-set of a finite point set ''S'' in the [[Euclidean plane]] is a [[subset]] of ''k'' elements of ''S'' that can be strictly separated from the remaining points by a [[line (geometry)|line]]. More generally, in [[Euclidean space]] of higher dimensions, a ''k''-set of a finite point set is a subset of ''k'' elements that can be separated from the remaining points by a [[hyperplane]]. In particular, when ''k'' = ''n''/2 (where ''n'' is the size of ''S''), the line or hyperplane that separates a ''k''-set from the rest of ''S'' is a '''halving line''' or '''halving plane'''.\n\n''K''-sets are related by [[projective duality]] to ''k''-levels in [[arrangement of lines|line arrangements]]; the ''k''-level in an arrangement of ''n'' lines in the plane is the curve consisting of the points that lie on one of the lines and have exactly ''k'' lines below them. Discrete and computational geometers have also studied levels in arrangements of more general kinds of curves and surfaces.<ref>Agarwal et al. (1997); Chan (2003; 2005a,b).</ref>\n\n== Combinatorial bounds ==\n{{unsolved|mathematics|What is the largest possible number of halving lines for a set of <math>n</math> points in the plane?}}\nIt is of importance in the analysis of geometric algorithms to bound the number of ''k''-sets of a planar point set,<ref>Chazelle and Preparata (1986); Cole et al. (1987); Edelsbrunner and Welzl (1986).</ref> or equivalently the number of ''k''-levels of a planar line arrangement, a problem first studied by [[László Lovász|Lovász]] (1971) and [[Paul Erdős|Erdős]] et al. (1973). The best known upper bound for this problem is ''O''(''nk''<sup>1/3</sup>), as was shown by [[Tamal Dey]] (1998) using the [[crossing number inequality]] of Ajtai, [[Václav Chvátal|Chvátal]], Newborn, and [[Endre Szemerédi|Szemerédi]]. However, the best known lower bound is far from Dey's upper bound: it is &Omega;(''n'' exp(''c'' (log''k'')<sup>1/2</sup>)) for some constant ''c'', as shown by Toth (2001).\n\nIn three dimensions, the best upper bound known is ''O''(''nk''<sup>3/2</sup>), and the best lower bound known is &Omega;(''nk'' exp(''c'' (log''k'')<sup>1/2</sup>)).<ref>Sharir et al. (2001).</ref>\nFor points in three dimensions that are in [[convex position]], that is, are the vertices of some convex polytope, the number of k-sets is \n&Theta;(''(n-k)k''), which follows from arguments used for bounding the complexity of k-th order Voronoi diagrams.<ref>Lee (1982); Clarkson and Shor (1989).</ref>\n\nFor the case when ''k'' = ''n''/2 (halving lines), the maximum number of combinatorially distinct lines through two points of ''S'' that bisect the remaining points when ''k'' = 1, 2, ... is\n:1,3,6,9,13,18,22... {{OEIS|id=A076523}}.\n\nBounds have also been proven on the number of ≤''k''-sets, where a ≤''k''-set is a ''j''-set for some ''j'' ≤ ''k''. In two dimensions, the maximum number of ≤''k''-sets is exactly ''nk'',<ref>Alon and Győri (1986).</ref> while in ''d'' dimensions the bound is <math>O(n^{\\lfloor d/2\\rfloor}k^{\\lceil d/2\\rceil})</math>.<ref>Clarkson and Shor (1989).</ref>\n\n== Construction algorithms ==\nEdelsbrunner and Welzl (1986) first studied the problem of constructing all ''k''-sets of an input point set, or dually of constructing the ''k''-level of an arrangement. The ''k''-level version of their algorithm can be viewed as a [[plane sweep]] algorithm that constructs the level in left-to-right order. Viewed in terms of ''k''-sets of point sets, their algorithm maintains a [[dynamic convex hull]] for the points on each side of a separating line, repeatedly finds a [[bitangent]] of these two hulls, and moves each of the two points of tangency to the opposite hull. Chan (1999) surveys subsequent results on this problem, and shows that it can be solved in time proportional to Dey's ''O''(''nk''<sup>1/3</sup>) bound on the complexity of the ''k''-level.\n\nAgarwal and [[Jiří Matoušek (mathematician)|Matoušek]] describe algorithms for efficiently constructing an approximate level; that is, a curve that passes between the (''k'' - ''d'')-level and the (''k'' + ''d'')-level for some small approximation parameter ''d''. They show that such an approximation can be found, consisting of a number of line segments that depends only on ''n''/''d'' and not on ''n'' or ''k''.<ref>Agarwal (1990); Matoušek (1990,1991).</ref>\n\n== Matroid generalizations ==\nThe planar ''k''-level problem can be generalized to one of parametric optimization in a [[matroid]]: one is given a matroid in which each element is weighted by a linear function of a parameter &lambda;, and must find the minimum weight basis of the matroid for each possible value of &lambda;. If one graphs the weight functions as lines in a plane, the ''k''-level of the arrangement of these lines graphs as a function of &lambda; the weight of the largest element in an optimal basis in a [[uniform matroid]], and Dey showed that his  ''O''(''nk''<sup>1/3</sup>) bound on the complexity of the ''k''-level could be generalized to count the number of distinct optimal bases of any matroid with ''n'' elements and rank ''k''.\n\nFor instance, the same ''O''(''nk''<sup>1/3</sup>) upper bound holds for counting the number of different [[minimum spanning tree]]s formed in a graph with ''n'' edges and ''k'' vertices, when the edges have weights that vary linearly with a parameter &lambda;. This parametric minimum spanning tree problem has been studied by various authors and can be used to solve other bicriterion spanning tree optimization problems.<ref>Gusfield (1980); Ishii et al. (1981); Katoh and Ibaraki (1983); Hassin and Tamir (1989); Fernández-Baca et al. (1996); Chan (2005c).</ref>\n\nHowever, the best known lower bound for the parametric minimum spanning tree problem is &Omega;(''n'' &alpha;(''k'')), where &alpha; is the [[inverse Ackermann function]], an even weaker bound than that for the ''k''-set problem. For more general matroids, Dey's ''O''(''nk''<sup>1/3</sup>) upper bound has a matching lower bound.<ref>Eppstein (1998).</ref>\n\n==Notes==\n{{reflist}}\n\n==References==\n{{refbegin|colwidth=30em}}\n*{{cite journal\n | last = Agarwal | first = P. K. | authorlink = Pankaj K. Agarwal\n | title = Partitioning arrangements of lines I: An efficient deterministic algorithm\n | journal = [[Discrete and Computational Geometry]]\n | volume = 5\n | issue = 1\n | year = 1990\n | pages = 449–483\n | doi = 10.1007/BF02187805}}\n*{{cite conference\n | last1 = Agarwal | first1 = P. K. | author1-link = Pankaj K. Agarwal\n | last2 = Aronov | first2 = B. | author2-link = Boris Aronov\n | last3 = Sharir | first3 = M. | author3-link = Micha Sharir\n | title = On levels in arrangements of lines, segments, planes, and triangles\n | booktitle = Proc. 13th Annual Symposium on Computational Geometry\n | year = 1997\n | pages = 30–38}}\n*{{cite journal\n | last1 = Alon | first1 = N. | author1-link = Noga Alon\n | last2 = Győri | first2 = E.\n | title = The number of small semi-spaces of a finite set of points in the plane\n | journal = Journal of Combinatorial Theory, Series A\n | volume = 41\n | pages = 154–157\n | year = 1986\n | doi = 10.1016/0097-3165(86)90122-6}}\n*{{cite journal\n |last        = Chan\n |first       = T. M.\n |authorlink  = Timothy M. Chan\n |title       = Remarks on k-level algorithms in the plane\n |year        = 1999\n |url         = http://www.cs.uwaterloo.ca/~tmchan/lev2d_7_7_99.ps.gz\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20101104182509/http://www.cs.uwaterloo.ca/~tmchan/lev2d_7_7_99.ps.gz\n |archivedate = 2010-11-04\n |df          = \n}}\n*{{cite journal\n | last = Chan | first = T. M. | authorlink = Timothy M. Chan\n | title = On levels in arrangements of curves\n | journal = [[Discrete and Computational Geometry]]\n | volume = 29\n | pages = 375–393\n | year = 2003\n | doi = 10.1007/s00454-002-2840-2\n | issue = 3}}\n*{{cite journal\n | last = Chan | first = T. M. | authorlink = Timothy M. Chan\n | title = On levels in arrangements of curves, II: a simple inequality and its consequence\n | journal = [[Discrete and Computational Geometry]]\n | volume = 34\n | pages = 11–24\n | year = 2005a\n | doi = 10.1007/s00454-005-1165-3}}\n*{{cite conference\n | last = Chan | first = T. M. | authorlink = Timothy M. Chan\n | title = On levels in arrangements of surfaces in three dimensions\n | booktitle = Proceedings of the 16th Annual ACM-SIAM Symposium on Discrete Algorithms\n | pages = 232–240\n | year = 2005b\n | url = http://www.cs.uwaterloo.ca/~tmchan/surf_soda.ps}}\n*{{cite conference\n | last = Chan | first = T. M. | authorlink = Timothy M. Chan\n | title = Finding the shortest bottleneck edge in a parametric minimum spanning tree\n | booktitle = Proceedings of the 16th Annual ACM-SIAM Symposium on Discrete Algorithms\n | pages = 232–240\n | year = 2005c\n | url = http://www.cs.uwaterloo.ca/~tmchan/bottle_soda.ps}}\n*{{cite journal\n | last1 = Chazelle | first1 = B. | author1-link = Bernard Chazelle\n | last2 = Preparata | first2 = F. P. | author2-link = Franco Preparata\n | title = Halfspace range search: an algorithmic application of ''k''-sets\n | journal = [[Discrete and Computational Geometry]]\n | volume = 1\n | issue = 1\n | year = 1986\n | pages = 83–93\n |mr=0824110\n | doi = 10.1007/BF02187685}}\n*{{cite journal\n | last1 = Clarkson | first1 = K. L. | author1-link = Kenneth L. Clarkson\n | last2 = Shor | first2 = P. | author2-link = Peter Shor\n | title = Applications of random sampling, II\n | journal = [[Discrete and Computational Geometry]]\n | volume = 4\n | pages = 387–421\n | year = 1989\n | doi = 10.1007/BF02187740}}\n*{{cite journal\n | last1 = Cole | first1 = R.\n | last2 = Sharir | first2 = M. | author2-link = Micha Sharir\n | last3 = Yap | first3 = C. K.\n | title = On ''k''-hulls and related problems\n | journal = [[SIAM Journal on Computing]]\n | volume = 16\n | issue = 1\n | year = 1987\n | pages = 61–77\n |mr=0873250\n | doi = 10.1137/0216005}}\n*{{cite journal\n | last = Dey | first = T. K. | authorlink = Tamal Dey\n | title = Improved bounds for planar ''k''-sets and related problems\n | journal = [[Discrete and Computational Geometry]]\n | volume = 19\n | issue = 3\n | year = 1998\n | pages = 373–382\n | doi = 10.1007/PL00009354\n |mr=1608878}}\n*{{cite journal\n | last1 = Edelsbrunner | first1 = H. | author1-link = Herbert Edelsbrunner\n | last2 = Welzl | first2 = E. | author2-link = Emo Welzl\n | title = Constructing belts in two-dimensional arrangements with applications\n | journal = [[SIAM Journal on Computing]]\n | volume = 15\n | issue = 1\n | year = 1986\n | pages = 271–284\n | doi = 10.1137/0215019}}\n*{{cite journal\n | title = Geometric lower bounds for parametric matroid optimization\n | last = Eppstein | first = D. | authorlink = David Eppstein\n | journal = [[Discrete and Computational Geometry]]\n | volume = 20\n | pages = 463–476\n | year = 1998\n | url = http://www.ics.uci.edu/~eppstein/pubs/Epp-DCG-98.pdf\n | doi = 10.1007/PL00009396\n | issue = 4}}\n*{{cite conference\n |author1=[[Paul Erdős|Erdős, P.]] |author2=[[László Lovász|Lovász, L.]] |author3=Simmons, A. |author4=[[Ernst G. Straus|Straus, E. G.]]\n | title = Dissection graphs of planar point sets\n | booktitle = A Survey of Combinatorial Theory (Proc. Internat. Sympos., Colorado State Univ., Fort Collins, Colo., 1971)\n | publisher = North-Holland\n | location = Amsterdam\n | date = 1973\n | pages = 139–149\n |mr=0363986}}\n*{{cite journal\n | title = Using sparsification for parametric minimum spanning tree problems\n |author1=Fernández-Baca, D. |author2=Slutzki, G. |author3=[[David Eppstein|Eppstein, D.]]\n | journal = [[Nordic Journal of Computing]]\n | volume = 3\n | issue = 4\n | pages = 352–366\n | year = 1996}}\n*{{cite journal\n | author = Gusfield, D.\n | title = Sensitivity analysis for combinatorial optimization\n | version = Tech. Rep. UCB/ERL M80/22\n | publisher = University of California, Berkeley\n | date = 1980}}\n*{{cite journal\n |author1=Hassin, R. |author2=Tamir, A. | title = Maximizing classes of two-parametric objectives over matroids\n | journal = Math. Oper. Res.\n | volume = 14\n | pages = 362–375\n | year = 1989\n | doi = 10.1287/moor.14.2.362\n | issue = 2}}\n*{{cite journal\n |author1=Ishii, H. |author2=Shiode, S. |author3=Nishida, T. | title = Stochastic spanning tree problem\n | journal = [[Discrete Applied Mathematics]]\n | volume = 3\n | pages = 263–273\n | year = 1981\n | doi = 10.1016/0166-218X(81)90004-4\n | issue = 4}}\n*{{cite journal\n |author1=Katoh, N. |author2=Ibaraki, T. | title = On the total number of pivots required for certain parametric combinatorial optimization problems\n | version = Working Paper 71\n | publisher = Inst. Econ. Res., Kobe Univ. of Commerce\n | date = 1983}}\n*{{cite journal\n | last = Lee | first = Der-Tsai | authorlink = Der-Tsai Lee\n | title = On k-Nearest Neighbor Voronoi Diagrams in the Plane\n | journal = IEEE Transactions on Computers\n | volume = 31\n | year = 1982\n | pages = 478–487\n | doi = 10.1109/TC.1982.1676031\n | issue = 6}}\n*{{cite journal\n | author = [[László Lovász|Lovász, L.]]\n | title = On the number of halving lines\n | journal = Annales Universitatis Scientiarum Budapestinensis de Rolando Eőtvős Nominatae Sectio Mathematica\n | volume = 14\n | year = 1971\n | pages = 107–108}}\n*{{cite journal\n | last = Matoušek | first = J. | authorlink = Jiří Matoušek (mathematician)\n | title = Construction of &epsilon;-nets\n | journal = [[Discrete and Computational Geometry]]\n | volume = 5\n | issue = 5\n | year = 1990\n | pages = 427–448\n |mr=1064574\n | doi = 10.1007/BF02187804}}\n*{{cite journal\n | last = Matoušek | first = J. | authorlink = Jiří Matoušek (mathematician)\n | title = Approximate levels in line arrangements\n | journal = [[SIAM Journal on Computing]]\n | volume = 20\n | issue = 2\n | pages = 222–227\n | year = 1991\n | doi = 10.1137/0220013}}\n*{{cite journal\n |author1=[[Micha Sharir|Sharir, M.]] |author2=Smorodinsky, S. |author3=Tardos, G.\n | title = An improved bound for ''k''-sets in three dimensions\n | journal = [[Discrete and Computational Geometry]]\n | volume = 26\n | year = 2001\n | pages = 195–204\n | doi = 10.1007/s00454-001-0005-3}}\n*{{cite journal\n | author = Tóth, G.\n | title = Point sets with many ''k''-sets\n | journal = [[Discrete and Computational Geometry]]\n | volume = 26\n | issue = 2\n | pages = 187–194\n | year = 2001\n | doi = 10.1007/s004540010022}}\n{{refend}}\n\n==External links==\n*[http://compgeom.cs.uiuc.edu/~jeffe/open/ksets.html Halving lines and k-sets], Jeff Erickson\n*[http://maven.smith.edu/~orourke/TOPP/P7.html The Open Problems Project, Problem 7: ''k''-sets]\n\n[[Category:Discrete geometry]]\n[[Category:Matroid theory]]"
    },
    {
      "title": "MacLane matroid",
      "url": "https://en.wikipedia.org/wiki/MacLane_matroid",
      "text": "#REDIRECT [[Möbius–Kantor configuration]]\n[[Category:Matroid theory]]"
    },
    {
      "title": "Matroid minor",
      "url": "https://en.wikipedia.org/wiki/Matroid_minor",
      "text": "In the mathematical theory of [[matroid]]s, a '''minor''' of a matroid ''M'' is another matroid ''N'' that is obtained from ''M'' by a sequence of restriction and contraction operations. Matroid minors are closely related to [[graph minor]]s, and the restriction and contraction operations by which they are formed correspond to edge deletion and edge contraction operations in graphs. The theory of matroid minors leads to structural decompositions of matroids, and characterizations of matroid families by forbidden minors, analogous to the corresponding theory in graphs.\n\n==Definitions==\nIf ''M'' is a matroid on the set ''E'' and ''S'' is a subset of ''E'', then the restriction of ''M'' to ''S'', written ''M''&nbsp;|''S'', is the matroid on the set ''S'' whose independent sets are the independent sets of ''M'' that are contained in ''S''.  Its circuits are the circuits of ''M'' that are contained in ''S'' and its [[matroid rank|rank function]] is that of ''M'' restricted to subsets of ''S''.\n\nIf ''T'' is an independent subset of ''E'', the contraction of ''M'' by ''T'', written ''M''/''T'', is the matroid on the underlying set ''E &minus; T'' whose independent sets are the sets whose union with ''T'' is independent in ''M''. This definition may be extended to arbitrary ''T'' by choosing a basis for ''T'' and defining a set to be independent in the contraction if its union with this basis remains independent in ''M''. The rank function of the contraction is <math>r'(A) = r(A \\cup T) - r(T).</math>\n\nA matroid ''N'' is a minor of a matroid ''M'' if it can be constructed from ''M'' by restriction and contraction operations.\n\nIn terms of the [[geometric lattice]] formed by the flats of a matroid, taking a minor of a matroid corresponds to taking an interval of the lattice, the part of the lattice lying between a given lower bound and upper bound element.{{sfnp|Welsh|2010}}\n\n==Forbidden matroid characterizations==\nMany important families of matroids are closed under the operation of taking minors: if a matroid ''M'' belongs to the family, then every minor of ''M'' also belongs to the family. In this case, the family may be characterized by its set of \"forbidden matroids\", the minor-minimal matroids that do not belong to the family. A matroid belongs to the family if and only if it does not have a forbidden matroid as a minor. Often, but not always, the set of forbidden matroids is finite, paralleling the [[Robertson–Seymour theorem]] which states that the set of forbidden minors of a minor-closed graph family is always finite.\n\nAn example of this phenomenon is given by the [[regular matroid]]s, matroids that are representable over all fields. Equivalently a matroid is regular if it can be represented by a [[totally unimodular matrix]] (a matrix whose square submatrices all have determinants equal to 0, 1, or &minus;1). {{harvtxt|Tutte|1958}} proved that a matroid is regular if and only if it does not have one of three forbidden minors: the [[uniform matroid]] <math>U{}^2_4</math> (the four-point line), the [[Fano plane]], or the [[dual matroid]] of the Fano plane. For this he used his difficult [[Tutte homotopy theorem|homotopy theorem]].  Simpler proofs have since been found.\n\nThe [[graphic matroid]]s, matroids whose independent sets are the forest subgraphs of a graph, have five forbidden minors: the three for the regular matroids, and the two duals of the graphic matroids for the graphs ''K''<sub>5</sub> and ''K''<sub>3,3</sub> that by [[Wagner's theorem]] are forbidden minors for the [[planar graph]]s.\n\nThe [[binary matroid]]s, matroids representable over the two-element [[finite field]], include both graphic and regular matroids. Tutte again showed that these matroids have a forbidden minor characterization: they are the matroids that do not have the four-point line as a minor. [[Rota's conjecture|Rota conjectured]] that, for any finite field, the matroids representable over that field have finitely many forbidden minors.<ref>{{harvtxt|Rota|1971}}.</ref> A full proof of this conjecture has been announced by Geelen, Gerards, and Whittle;<ref>{{citation|title=Solving Rota's conjecture|journal=Notices of the American Mathematical Society|url=http://www.ams.org/notices/201407/rnoti-p736.pdf|date=Aug 17, 2014|pages=736–743}}</ref> {{as of|2015|lc=y}} it has not appeared.  However, the matroids that can be represented over the [[real number]]s have infinitely many forbidden minors.<ref>{{harvtxt|Vámos|1978}}.</ref>\n\n==Branchwidth==\n[[Branch-decomposition]]s of matroids may be defined analogously to their definition for graphs.\nA branch-decomposition of a matroid is a [[hierarchical clustering]] of the matroid elements, represented as an unrooted binary tree with the elements of the matroid at its leaves. Removing any edge of this tree partitions the matroids into two disjoint subsets; such a partition is called an e-separation. If ''r'' denotes the rank function of the matroid, then the width of an e-separation is defined as {{nowrap|''r''(''A'') + ''r''(''B'') &minus; ''r''(''M'') + 1}}. The width of a decomposition is the maximum width of any of its e-separations, and the branchwidth of a matroid is the minimum width of any of its branch-decompositions.\n\nThe branchwidth of a graph and the branchwidth of the corresponding [[graphic matroid]] may differ: for instance, the three-edge [[path graph]] and the three-edge [[star (graph theory)|star]] have different branchwidths, 2 and 1 respectively, but they both induce the same graphic matroid with branchwidth 1.<ref name=\"mt07\"/> However, for graphs that are not trees, the branchwidth of the graph is equal to the branchwidth of its associated graphic matroid.<ref>{{harvtxt|Mazoit|Thomassé|2007}}; {{harvtxt|Hicks|McMurray|2007}}.</ref> The branchwidth of a matroid always equals the branchwidth of its dual.<ref name=\"mt07\">{{harvtxt|Mazoit|Thomassé|2007}}.</ref>\n\nBranchwidth is an important component of attempts to extend the theory of graph minors to matroids: although [[treewidth]] can also be generalized to matroids,<ref>{{harvtxt|Hliněný|Whittle|2006}}.</ref> and plays a bigger role than branchwidth in the theory of graph minors, branchwidth has more convenient properties in the matroid setting.<ref name=\"ggw06\">{{harvtxt|Geelen|Gerards|Whittle|2006}}.</ref>\nIf a minor-closed family of matroids representable over a finite field does not include the graphic matroids of all planar graphs, then there is a constant bound on the branchwidth of the matroids in the family, generalizing similar results for minor-closed graph families.<ref>{{harvtxt|Geelen|Gerards|Whittle|2006}}; {{harvtxt|Geelen|Gerards|Whittle|2007}}.</ref>\n\n==Well-quasi-ordering==\nThe [[Robertson–Seymour theorem]] implies that every matroid property of ''graphic'' matroids characterized by a list of forbidden minors can be characterized by a finite list. Another way of saying the same thing is that the [[partial order]] on graphic matroids formed by the minor operation is a [[well-quasi-ordering]]. However, the example of the real-representable matroids, which have infinitely many forbidden minors, shows that the minor ordering is not a well-quasi-ordering on all matroids.\n\nRobertson and Seymour conjectured that the matroids representable over any particular [[finite field]] are well-quasi-ordered. So far this has been proven only for the matroids of bounded branchwidth.<ref>{{harvtxt|Geelen|Gerards|Whittle|2002}}; {{harvtxt|Geelen|Gerards|Whittle|2006}}.</ref>\n\n==Matroid decompositions==\nThe [[graph structure theorem]] is an important tool in the theory of graph minors, according to which the graphs in any minor-closed family can be built up from simpler graphs by [[clique-sum]] operations. Some analogous results are also known in matroid theory. In particular, [[Seymour's decomposition theorem]] states that all regular matroids can be built up in a simple way as the clique-sum of graphic matroids, their duals, and one special 10-element matroid.<ref>{{harvtxt|Seymour|1980}}.</ref>  As a consequence, [[linear program]]s defined by totally unimodular matrices may be solved combinatorially by combining the solutions to a set of [[minimum spanning tree]] problems corresponding to the graphic and co-graphic parts of this decomposition.\n\n==Algorithms and complexity==\nOne of the important components of graph minor theory is the existence of an algorithm for testing whether a graph ''H'' is a minor of another graph ''G'', taking an amount of time that is polynomial in ''G'' for any fixed choice of ''H'' (and more strongly [[parameterized complexity#FPT|fixed-parameter tractable]] if the size of ''H'' is allowed to vary). By combining this result with the Robertson–Seymour theorem, it is possible to recognize the members of any minor-closed graph family in [[polynomial time]]. Correspondingly, in matroid theory, it would be desirable to develop efficient algorithms for recognizing whether a given fixed matroid is a minor of an input matroid. Unfortunately, such a strong result is not possible: in the [[matroid oracle]] model, the only minors that can be recognized in polynomial time are the [[uniform matroid]]s with rank or corank one.{{sfnp|Seymour|Walton|1981}} However, if the problem is restricted to the matroids that are representable over some fixed finite field (and represented as a matrix over that field) then, as in the graph case, it is conjectured to be possible to recognize the matroids that contain any fixed minor in polynomial time.<ref name=\"ggw06\"/>\n\n==Notes==\n{{reflist|colwidth=30em}}\n\n==References==\n{{refbegin|colwidth=30em}}\n*{{citation\n | last1 = Geelen | first1 = J. F. | author1-link = Jim Geelen\n | last2 = Gerards | first2 = A. M. H.\n | last3 = Kapoor | first3 = A.\n | doi = 10.1006/jctb.2000.1963\n | issue = 2\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 1769191\n | pages = 247–299\n | series = Series B\n | title = The excluded minors for GF(4)-representable matroids\n | volume = 79\n | year = 2000}}.\n*{{citation\n | last1 = Geelen | first1 = Jim | author1-link = Jim Geelen\n | last2 = Gerards | first2 = Bert\n | last3 = Robertson | first3 = Neil | author3-link = Neil Robertson (mathematician)\n | last4 = Whittle | first4 = Geoff\n | doi = 10.1016/S0095-8956(02)00046-1\n | issue = 2\n | journal = [[Journal of Combinatorial Theory]] | series = Series B\n | pages = 261–265\n | title = On the excluded minors for the matroids of branch-width ''k''\n | volume = 88\n | year = 2003}}.\n*{{citation\n | last1 = Geelen | first1 = Jim | author1-link = Jim Geelen\n | last2 = Gerards | first2 = Bert\n | last3 = Whittle | first3 = Geoff\n | doi = 10.1006/jctb.2001.2082\n | issue = 2\n | journal = [[Journal of Combinatorial Theory]] | series = Series B\n | pages = 270–290\n | title = Branch-width and well-quasi-ordering in matroids and graphs\n | volume = 84\n | year = 2002}}.\n*{{citation\n | last1 = Geelen | first1 = Jim | author1-link = Jim Geelen\n | last2 = Gerards | first2 = Bert\n | last3 = Whittle | first3 = Geoff\n | contribution = Towards a structure theory for matrices and matroids\n | pages = 827–842\n | title = Proc. [[International Congress of Mathematicians]]\n | contribution-url = http://www.icm2006.org/proceedings/Vol_III/contents/ICM_Vol_3_41.pdf\n | volume = III\n | year = 2006}}.\n*{{citation\n |last1=Geelen \n |first1=Jim \n |author1-link=Jim Geelen \n |last2=Gerards \n |first2=Bert \n |last3=Whittle \n |first3=Geoff \n |doi=10.1016/j.jctb.2007.02.005 \n |issue=6 \n |journal=[[Journal of Combinatorial Theory]] \n |series=Series B \n |pages=971–998 \n |title=Excluding a planar graph from GF(''q'')-representable matroids \n |url=http://www.math.uwaterloo.ca/~jfgeelen/publications/grid.pdf \n |volume=97 \n |year=2007 \n |deadurl=yes \n |archiveurl=https://web.archive.org/web/20100924110915/http://www.math.uwaterloo.ca/~jfgeelen/publications/grid.pdf \n |archivedate=2010-09-24 \n |df= \n}}.\n*{{citation\n | last1 = Hicks | first1 = Illya V.\n | last2 = McMurray | first2 = Nolan B., Jr.\n | doi = 10.1016/j.jctb.2006.12.007\n | issue = 5\n | journal = [[Journal of Combinatorial Theory]] | series = Series B\n | pages = 681–692\n | title = The branchwidth of graphs and their cycle matroids\n | volume = 97\n | year = 2007}}.\n*{{citation\n | last = Hliněný | first = Petr\n | contribution = On matroid properties definable in the MSO logic\n | doi = 10.1007/978-3-540-45138-9\\_41\n | pages = 470–479\n | publisher = Springer-Verlag\n | series = Lecture Notes in Computer Science\n | title = Proc. 28th International Symposium on Mathematical Foundations of Computer Science (MFCS '03)\n | volume = 2747\n | year = 2003}}\n*{{citation\n | last1 = Hliněný | first1 = Petr\n | last2 = Whittle | first2 = Geoff\n | doi = 10.1016/j.ejc.2006.06.005\n | issue = 7\n | journal = European Journal of Combinatorics\n | pages = 1117–1128\n | title = Matroid tree-width\n | url = http://www.fi.muni.cz/~hlineny/Research/papers/matr-tw-final.pdf\n | volume = 27\n | year = 2006}}. Addendum and corrigendum: {{citation \n | last1 = Hliněný | first1 = Petr\n | last2 = Whittle | first2 = Geoff\n | title = Addendum to matroid tree-width \n | journal = European Journal of Combinatorics\n | volume=30\n | issue=4\n | pages=1036–1044 \n | year = 2009 \n | doi=10.1016/j.ejc.2008.09.028}}.\n*{{citation\n | last1 = Mazoit | first1 = Frédéric\n | last2 = Thomassé | first2 = Stéphan\n | contribution = Branchwidth of graphic matroids\n | editor1-last = Hilton | editor1-first = Anthony\n | editor2-last = Talbot | editor2-first = John\n | page = 275\n | publisher = Cambridge University Press\n | series = London Mathematical Society Lecture Note Series\n | title = Surveys in Combinatorics 2007\n | url = http://hal.archives-ouvertes.fr/docs/00/04/09/28/PDF/Branchwidth.pdf\n | volume = 346\n | year = 2007}}.\n*{{citation\n | last = Rota | first = Gian-Carlo | authorlink = Gian-Carlo Rota\n | contribution = Combinatorial theory, old and new\n | location = Paris\n | mr = 0505646\n | pages = 229–233\n | publisher = Gauthier-Villars\n | title = Actes du Congrès International des Mathématiciens (Nice, 1970), Tome 3\n | year = 1971}}.\n*{{citation\n | last1 = Seymour | first1 = P. D. | authorlink1 = Paul Seymour (mathematician)\n | title = Decomposition of regular matroids\n | journal = [[Journal of Combinatorial Theory]] | series = Series B\n | volume = 28 | issue = 3 | year = 1980 | pages = 305–359\n | mr = 0579077 \n | doi = 10.1016/0095-8956(80)90075-1}}.\n*{{citation\n | last1 = Seymour | first1 = P. D. | author1-link = Paul Seymour (mathematician)\n | last2 = Walton | first2 = P. N.\n | doi = 10.1112/jlms/s2-23.2.193\n | issue = 2\n | journal = Journal of the London Mathematical Society\n | mr = 609098\n | pages = 193–203\n | series = Second Series\n | title = Detecting matroid minors\n | volume = 23\n | year = 1981}}.\n*{{citation\n | last = Tutte | first = W. T. | authorlink = W. T. Tutte\n | journal = Transactions of the American Mathematical Society\n | mr = 0101526\n | pages = 144–174\n | title = A homotopy theorem for matroids. I, II\n | volume = 88\n | year = 1958\n | doi=10.2307/1993244}}.\n*{{citation\n | last = Vámos | first = P.\n | doi = 10.1112/jlms/s2-18.3.403\n | issue = 3\n | journal = Journal of the London Mathematical Society\n | mr = 518224\n | pages = 403–408\n | series = Second Series\n | title = The missing axiom of matroid theory is lost forever\n | volume = 18\n | year = 1978}}.\n*{{citation\n | last = Welsh | first = D. J. A. | authorlink = Dominic Welsh\n | contribution = 4.4 Minors and their representation in the lattice\n | isbn = 9780486474397\n | pages = 65–67\n | publisher = Courier Dover Publications\n | title = Matroid Theory\n | year = 2010 | origyear=1976}}.\n{{refend}}\n\n[[Category:Graph minor theory]]\n[[Category:Matroid theory|Minor]]"
    },
    {
      "title": "Matroid oracle",
      "url": "https://en.wikipedia.org/wiki/Matroid_oracle",
      "text": "In mathematics and computer science, a '''matroid oracle''' is a [[subroutine]] through which an [[algorithm]] may access a [[matroid]], an abstract combinatorial structure that can be used to describe the [[Linear independence|linear dependencies]] between vectors in a [[vector space]] or the [[spanning tree]]s of a [[undirected graph|graph]], among other applications.\n\nThe most commonly used oracle of this type is an independence oracle, a subroutine for testing whether a set of matroid elements is independent. Several other types of oracle have also been used; some of them have been shown to be weaker than independence oracles, some stronger, and some equivalent in computational power.<ref name=\"Robinson 1980\">{{harvtxt|Robinson|Welsh|1980}}; {{harvtxt|Hausmann|Korte|1981}}; {{harvtxt|Coullard|Hellerstein|1996}}.</ref>\n\nMany [[algorithm]]s that perform computations on matroids have been designed to take an oracle as input, allowing them to run efficiently without change on many different kinds of matroids, and without additional assumptions about what kind of matroid they are using. For instance, given an independence oracle for any matroid, it is possible to find the minimum weight basis of the matroid by applying a [[greedy algorithm]] that adds elements to the basis in sorted order by weight, using the independence oracle to test whether each element can be added.<ref name=\"e71\">{{harvtxt|Edmonds|1971}}.</ref>\n\nIn [[computational complexity theory]], the [[Oracle machine|oracle model]] has led to unconditional [[lower bound]]s proving that certain matroid problems cannot be solved in polynomial time, without invoking unproved assumptions such as the assumption that [[P versus NP problem|P ≠ NP]]. Problems that have been shown to be hard in this way include testing whether a matroid is [[binary matroid|binary]] or [[uniform matroid|uniform]], or testing whether it contains certain fixed [[matroid minor|minors]].<ref name=\"jk82\">{{harvtxt|Jensen|Korte|1982}}.</ref>\n\n==Why oracles?==\nAlthough some authors have experimented with computer representations of matroids that explicitly list all independent sets or all basis sets of the matroid,<ref>{{harvtxt|Mayhew|2008}}.</ref> these representations are not ''succinct'': a matroid with <math>\\scriptstyle n</math> elements may expand into a representation that takes space exponential in <math>\\scriptstyle n</math>. Indeed, the number of distinct matroids on <math>\\scriptstyle n</math> elements grows [[double exponential function|doubly exponentially]] as\n:<math>2^{2^n n^{-3/2+o(1)}}</math><ref>{{harvtxt|Piff|Welsh|1971}}; {{harvtxt|Piff|1973}}; {{harvtxt|Knuth|1974}}; {{harvtxt|Bansal|Pendavingh|van der Pol|2012}}.</ref>\nfrom which it follows that any explicit representation capable of handling all possible matroids would necessarily use exponential space.<ref name=\"rw80\">{{harvtxt|Robinson|Welsh|1980}}.</ref>\n\nInstead, different types of matroids may be represented more efficiently from the other structures from which they are defined: [[uniform matroid]]s from their two numeric parameters, [[graphic matroid]]s, [[bicircular matroid]]s, and [[gammoid]]s from graphs, [[linear matroid]]s from [[matrix (mathematics)|matrices]], etc. However, an algorithm for performing computations on arbitrary matroids needs a uniform method of accessing its argument, rather than having to be redesigned for each of these matroid classes. The oracle model provides a convenient way of codifying and classifying the kinds of access that an algorithm might need.\n\n==History==\nStarting with {{harvtxt|Rado|1942}}, \"independence functions\" or \"<math>\\scriptstyle I</math>-functions\" have been studied as one of many equivalent ways of axiomatizing matroids. An independence function maps a set of matroid elements to the number <math>\\scriptstyle 1</math> if the set is independent or <math>\\scriptstyle 0</math> if it is dependent; that is, it is the [[indicator function]] of the family of independent sets, essentially the same thing as an independence oracle.<ref>For additional research on matroids based on the independence function axiomatization, see e.g. {{harvtxt|Rado|1957}}, {{harvtxt|Lazarson|1958}}, and {{harvtxt|Ingleton|1959}}.</ref>\n\nMatroid oracles have also been part of the earliest algorithmic work on matroids. Thus, {{harvtxt|Edmonds|1965}}, in studying matroid partition problems, assumed that the access to the given matroid was through a subroutine that takes as input an independent set <math>\\scriptstyle I</math> and an element <math>\\scriptstyle x</math>, and either returns a circuit in <math>\\scriptstyle I\\cup\\{x\\}</math> (necessarily unique and containing <math>\\scriptstyle x</math>, if it exists) or determines that no such circuit exists. {{harvtxt|Edmonds|1971}} used a subroutine that tests whether a given set is independent (that is, in more modern terminology, an independence oracle), and observed that the information it provides is sufficient to find the minimum weight basis in polynomial time.\n\nBeginning from the work of \n{{harvtxt|Korte|Hausmann|1978}} and\n{{harvtxt|Hausmann||Korte|1978}}, researchers began studying oracles from the point of view of proving lower bounds on algorithms for matroids and related structures. These two papers by Hausmann and Korte both concerned the problem of finding a maximum cardinality independent set, which is easy for matroids but (as they showed) harder to approximate or compute exactly for more general [[independence system]]s represented by an independence oracle. This work kicked off a flurry of papers in the late 1970s and early 1980s showing similar hardness results for problems on matroids<ref>{{harvtxt|Lovász|1981}}; {{harvtxt|Seymour|1981}}; {{harvtxt|Seymour|Walton|1981}}; {{harvtxt|Jensen|Korte|1982}}; {{harvtxt|Truemper|1982}}.</ref> and comparing the power of different kinds of matroid oracles.<ref name=\"rwhk\"/>\n\nSince that time, the independence oracle has become standard for most research on matroid algorithms.<ref>E.g. see {{harvtxt|Cunningham|1986}}, {{harvtxt|Kelmans|Polesskiĭ|1994}} {{harvtxt|Fujishige|Zhang|1995}}, {{harvtxt|Chávez Lomelí|Welsh|1996}}, {{harvtxt|Khachiyan|Boros|Elbassioni|Gurvich|2005}}, and {{harvtxt|Oum|Seymour|2007}}.</ref> There has also been continued research on lower bounds,<ref>{{harvtxt|Azar|Broder|Frieze|1994}}.</ref> and comparisons of different types of oracle.<ref>{{harvtxt|Karp|Upfal|Wigderson|1988}}; {{harvtxt|Coullard|Hellerstein|1996}}.</ref>\n\n==Types of oracles==\nThe following types of matroid oracles have been considered.\n*An '''independence oracle''' takes as its input a set of matroid elements, and returns as output a [[Boolean value]], true if the given set is independent and false otherwise.<ref>{{harvtxt|Edmonds|1971}}; {{harvtxt|Robinson|Welsh|1980}}; {{harvtxt|Hausmann|Korte|1981}}.</ref> It may be implemented easily based on the underlying structure from which the matroid was defined for [[graphic matroid]]s, [[transversal matroid]]s, [[gammoid]]s, and linear matroids, and for matroids formed from these by standard operations such as direct sums.<ref name=\"jk82\"/>\n*The oracle from {{harvtxt|Edmonds|1965}} takes as input an independent set and an additional element, and either determines that their union is independent or finds a circuit in the union and returns it.\n*A '''rank oracle''' takes as its input a set of matroid elements, and returns as its output a numerical value, the [[Matroid rank|rank]] of the given set.<ref name=\"rwhk\">{{harvtxt|Robinson|Welsh|1980}}; {{harvtxt|Hausmann|Korte|1981}}.</ref>\n*A '''basis oracle''' takes as its input a set of matroid elements, and returns as output a Boolean value, true if the given set is a basis and false otherwise.<ref name=\"rwhk\"/>\n*A '''circuit oracle''' takes as its input a set of matroid elements, and returns as output a Boolean value, true if the given set is a circuit and false otherwise.<ref name=\"rwhk\"/>\n*Three types of '''closure oracle''' have been considered: one that tests if a given element belongs to the closure of a given set, a second one that returns the closure of the set, and a third one that tests whether a given set is closed.<ref name=\"rwhk\"/>\n*A '''spanning oracle''' takes as its input a set of matroid elements, and returns as output a Boolean value, true if the given set is spanning (i.e. contains a basis and has the same rank as the whole matroid) and false otherwise.<ref name=\"hk01\">{{harvtxt|Hausmann|Korte|1981}}.</ref>\n*A '''girth oracle''' takes as its input a set of matroid elements, and returns as its output a numerical value, the size of the smallest circuit within that set (or <math>\\scriptstyle +\\infty</math> if the given set is independent).<ref name=\"hk01\"/>\n*A '''port oracle''' for a fixed element <math>\\scriptstyle x</math> of the matroid takes as its input a set of matroid elements, and returns as output a Boolean value, true if the given set contains a circuit that includes <math>\\scriptstyle x</math> and false otherwise.<ref name=\"ch96\">{{harvtxt|Coullard|Hellerstein|1996}}.</ref>\n\n==Relative power of different oracles==\nAlthough there are many known types of oracles, the choice of which to use can be simplified, because many of them are equivalent in computational power. An oracle <math>\\scriptstyle X</math> is said to be ''polynomially reducible'' to another oracle <math>\\scriptstyle Y</math> if any call to <math>\\scriptstyle X</math> may be simulated by an algorithm that accesses the matroid using only oracle <math>\\scriptstyle Y</math> and takes [[polynomial time]] as measured in terms of the number of elements of the matroid; in complexity-theoretic terms, this is a [[Turing reduction]]. Two oracles are said to be ''polynomially equivalent'' if they are polynomially reducible to each other. If <math>\\scriptstyle X</math> and <math>\\scriptstyle Y</math> are polynomially equivalent, then every result that proves the existence or nonexistence of a polynomial time algorithm for a matroid problem using oracle <math>\\scriptstyle X</math> also proves the same thing for oracle <math>\\scriptstyle Y</math>.\n\nFor instance, the independence oracle is polynomially equivalent to the circuit-finding oracle of {{harvtxt|Edmonds|1965}}. If a circuit-finding oracle is available, a set may be tested for independence using at most <math>\\scriptstyle n</math> calls to the oracle by starting from an [[empty set]], adding elements of the given set one element at a time, and using the circuit-finding oracle to test whether each addition preserves the independence of the set that has been constructed so far. In the other direction, if an independence oracle is available, the circuit in a set <math>\\scriptstyle I\\cup\\{x\\}</math> may be found using at most <math>\\scriptstyle n</math> calls to the oracle by testing, for each element <math>\\scriptstyle y\\in I</math>, whether <math>\\scriptstyle I\\setminus\\{y\\}\\cup\\{x\\}</math> is independent and returning the elements for which the answer is no. The independence oracle is also polynomially equivalent to the rank oracle, the spanning oracle, the first two types of closure oracle, and the port oracle.<ref name=\"Robinson 1980\"/>\n\nThe basis oracle, the circuit oracle, and the oracle that tests whether a given set is closed are all weaker than the independence oracle: they can be simulated in polynomial time by an algorithm that accesses the matroid using an independence oracle, but not vice versa. Additionally, none of these three oracles can simulate each other within polynomial time. The girth oracle is stronger than the independence oracle, in the same sense.<ref name=\"rwhk\"/>\n\nAs well as polynomial time Turing reductions, other types of reducibility have been considered as well. In particular, {{harvtxt|Karp|Upfal|Wigderson|1988}} showed that,\nin [[parallel algorithm]]s, the rank and independence oracles are significantly different in computational power. The rank oracle allows the construction of a minimum weight basis by <math>\\scriptstyle n</math> simultaneous queries, of the prefixes of the sorted order of the matroid elements: an element belongs to the optimal basis if and only if the rank of its prefix differs from the rank of the previous prefix. In contrast, finding a minimum basis with an independence oracle is much slower: it can be solved deterministically in <math>\\scriptstyle O(\\sqrt n)</math> time steps, and there is a lower bound of <math>\\scriptstyle \\Omega((n/\\log n)^{1/3})</math> even for randomized parallel algorithms.\n\n==Algorithms==\nMany problems on matroids are known to be solvable in [[polynomial time]], by algorithms that access the matroid only through an independence oracle or another oracle of equivalent power, without need of any additional assumptions about what kind of matroid has been given to them. These polynomially-solvable problems include:\n*Finding a minimum or maximum weight basis of a [[weighted matroid]], using a [[greedy algorithm]].<ref name=\"e71\"/>\n*Partitioning the elements of a matroid into a minimum number of independent sets, and finding the largest set that is simultaneously independent in two given matroids. The latter problem is called [[matroid intersection]], and the solutions to both problems are closely related to each other.<ref>{{harvtxt|Edmonds|1965}}; {{harvtxt|Cunningham|1986}}.</ref>\n*Testing whether a matroid is <math>\\scriptstyle k</math>-connected (in the sense of {{harvnb|Tutte|1966}}) for <math>\\scriptstyle k\\le 3</math>.<ref>{{harvtxt|Bixby|Cunningham|1979}}. A paper claiming a similar result for any fixed constant <math>\\scriptstyle k</math> was announced by Cunningham and Edmonds at roughly the same time, but appears not to have been published. {{harvtxt|Truemper|1998}}, pp. 186–187, writes \"Locating <math>\\scriptstyle k</math>-sums for general <math>\\scriptstyle k\\ge 4</math> is much more difficult ... We do not know how this can be efficiently accomplished for binary matroids, let alone for general matroids.\"</ref>\n*Testing whether a given matroid is [[graphic matroid|graphic]]<ref>{{harvtxt|Seymour|1981}}.</ref> or [[regular matroid|regular]].<ref>{{harvtxt|Truemper|1982}}.</ref>\n*Finding an [[ear decomposition]] of a given matroid, a sequence of circuits whose union is the matroid and in which each circuit remains a circuit after all previous circuits in the sequence are contracted. Such a decomposition may also be found with the additional property that a chosen matroid element belongs to every circuit.<ref name=\"ch96\"/>\n*Finding a [[branch-decomposition]] of a given matroid, whenever its branch-width is no more than a fixed constant.<ref>{{harvtxt|Oum|Seymour|2007}}.</ref>\n*Listing all of the bases, flats, or circuits of a matroid, in polynomial time per output set.<ref>{{harvtxt|Khachiyan|Boros|Elbassioni|Gurvich|2005}}.</ref>\n*Approximating the number of bases by a [[Polynomial-time approximation scheme|fully polynomial-time randomized approximation scheme]], for a matroid with <math>\\scriptstyle n</math> elements and rank <math>\\scriptstyle r</math>, with the additional assumption that the number of bases is within a polynomial factor of the number of <math>\\scriptstyle r</math>-element sets.<ref>{{harvtxt|Chávez Lomelí|Welsh|1996}}. In contrast, it is not possible for deterministic algorithms to approximate the number of bases of a matroid accurately in polynomial time {{harv|Azar|Broder|Frieze|1994}}.</ref>\n\n==Impossibility proofs==\nFor many matroid problems, it is possible to show that an independence oracle does not provide enough power to allow the problem to be solved in polynomial time. The main idea of these proofs is to find two matroids <math>\\scriptstyle M</math> and <math>\\scriptstyle M'</math> on which the answer to the problem differs and which are difficult for an algorithm to tell apart. In particular, if <math>\\scriptstyle M</math> has a high degree of symmetry, and differs from <math>\\scriptstyle M'</math> only in the answers to a small number of queries, then it may take a very large number of queries for an algorithm to be sure of distinguishing an input of type <math>\\scriptstyle M</math> from an input formed by using one of the symmetries of <math>\\scriptstyle M</math> to permute <math>\\scriptstyle M'</math>.<ref name=\"jk82\"/>\n\nA simple example of this approach can be used to show that it is difficult to test whether a matroid is [[uniform matroid|uniform]]. For simplicity of exposition, let <math>\\scriptstyle n</math> be even, let <math>\\scriptstyle M</math> be the uniform matroid <math>\\scriptstyle U{}^{n/2}_n</math>, and let <math>\\scriptstyle M'</math> be a matroid formed from <math>\\scriptstyle M</math> by making a single one of the <math>\\scriptstyle n/2</math>-element basis sets of <math>\\scriptstyle M</math> dependent instead of independent. In order for an algorithm to correctly test whether its input is uniform, it must be able to distinguish <math>\\scriptstyle M</math> from every possible permutation of <math>\\scriptstyle M'</math>. But in order for a deterministic algorithm to do so, it must test every one of the <math>\\scriptstyle n/2</math>-element subsets of the elements: if it missed one set, it could be fooled by an oracle that chose that same set as the one to make dependent. Therefore, testing for whether a matroid is uniform may require\n:<math>\\binom{n}{n/2}=\\Omega\\left(\\frac{2^n}{\\sqrt n}\\right)</math>\nindependence queries, much higher than polynomial. Even a randomized algorithm must make nearly as many queries in order to be confident of distinguishing these two matroids.<ref name=\"rwjk\">{{harvtxt|Robinson|Welsh|1980}}; {{harvtxt|Jensen|Korte|1982}}.</ref>\n\n{{harvtxt|Jensen|Korte|1982}} formalize this approach by proving that, whenever there exist two matroids <math>\\scriptstyle M</math> and <math>\\scriptstyle M'</math> on the same set of elements but with differing problem answers, an algorithm that correctly solves the given problem on those elements must use at least\n:<math>\\frac{|\\operatorname{aut}(M)|}{\\sum_i|\\operatorname{fix}(M,Q_i)|}</math>\nqueries, where <math>\\scriptstyle \\operatorname{aut}(M)</math> denotes the [[automorphism group]] of <math>\\scriptstyle M</math>, <math>\\scriptstyle Q_i</math> denotes the family of sets whose independence differs from <math>\\scriptstyle M</math> to <math>\\scriptstyle M'</math>, and <math>\\scriptstyle \\operatorname{fix}(M,Q_i)</math> denotes the subgroup of automorphisms that maps <math>\\scriptstyle Q_i</math> to itself. For instance, the automorphism group of the uniform matroid is just the [[symmetric group]], with size <math>\\scriptstyle n!</math>, and in the problem of testing uniform matroids there was only one set <math>\\scriptstyle Q_i</math> with <math>\\scriptstyle |\\operatorname{fix}(M,Q_i)|=(n/2)!^2</math>, smaller by an exponential factor than <math>\\scriptstyle n!</math>.<ref>As well as being in {{harvtxt|Jensen|Korte|1982}}, this formalization is surveyed in {{harvtxt|Korte|Schrader|1981}}. In most of the applications of this technique in {{harvtxt|Jensen|Korte|1982}}, <math>\\scriptstyle M</math> is uniform, but {{harvtxt|Seymour|1981}} applies the same idea to a non-uniform but highly symmetric matroid.</ref>\n\nProblems that have been proven to be impossible for a matroid oracle algorithm to compute in polynomial time include:\n*Testing whether a given matroid is uniform.<ref name=\"rwjk\"/>\n*Testing whether a given matroid contains a fixed matroid <math>\\scriptstyle H</math> as a minor, except in the special cases that <math>\\scriptstyle H</math> is uniform with rank or corank at most one. More generally, if <math>\\scriptstyle\\mathcal{H}</math> is a fixed finite set of matroids, and there is no uniform matroid in <math>\\scriptstyle\\mathcal{H}</math>, then it is not possible to test in polynomial time whether a given matroid contains one or more of the matroids in <math>\\scriptstyle\\mathcal{H}</math> as a minor.<ref>{{harvtxt|Seymour|Walton|1981}}. Results of {{harvtxt|Seymour|1981}} and {{harvtxt|Jensen|Korte|1982}} give special cases of this for the problems of finding a <math>\\scriptstyle U{}^2_4</math> minor and a [[Vamos matroid]] minor, respectively. Testing whether a matroid is graphic or regular may be expressed in terms of a finite set of forbidden minors, and may be solved in polynomial time, but the forbidden minors for these problems include the uniform matroid <math>\\scriptstyle U{}^2_4</math>, so they do not contradict this impossibility result.</ref>\n*Testing whether a given matroid is [[binary matroid|binary]], is representable over any particular fixed [[field (mathematics)|field]], or whether there exists a field over which it is representable.<ref>{{harvtxt|Seymour|1981}} showed this for binary matroids, {{harvtxt|Seymour|Walton|1981}} for finite fields, {{harvtxt|Truemper|1982}} for arbitrary fields, and {{harvtxt|Jensen|Korte|1982}} for the existence of a field over which the matroid is representable.</ref>\n*Solving the matroid matching problem, in which the input is a graph and a matroid on its vertices, and the goal is to find a [[Matching (graph theory)|matching]] in the graph that is as large as possible, subject to the constraint that the matched vertices form an independent set.<ref>{{harvtxt|Lovász|1981}}; {{harvtxt|Jensen|Korte|1982}}. However, the special case of this problem for [[bipartite graph]]s can be solved in polynomial time as a [[matroid intersection]] problem.</ref>\n*Testing whether a given matroid is [[dual matroid|self-dual]], [[transversal matroid|transversal]], [[bipartite matroid|bipartite]], [[Eulerian matroid|Eulerian]], or [[oriented matroid|orientable]].<ref name=\"jk82\"/>\n*Computing the girth (size of the smallest circuit), size of the largest circuit, number of circuits, number of bases, number of flats, number of maximum-rank flats, size of the largest flat, [[Tutte polynomial]], or connectivity of a given matroid.<ref name=\"jk82\"/>\n\nAmong the set of all properties of <math>\\scriptstyle n</math>-element matroids, the fraction of the properties that do not require exponential time to test goes to zero, in the limit, as <math>\\scriptstyle n</math> goes to infinity.<ref name=\"rw80\"/>\n\n==See also==\n*[[Black box group]], an oracle-like model for [[group theory]]\n*[[Implicit graph]], an oracle-like model for graph algorithms\n\n==Notes==\n{{reflist|colwidth=30em}}\n\n==References==\n{{refbegin|colwidth=30em}}\n*{{citation\n | last1 = Azar | first1 = Y.\n | last2 = Broder | first2 = A. Z. | author2-link = Andrei Broder\n | last3 = Frieze | first3 = A. M. | author3-link = Alan M. Frieze\n | doi = 10.1016/0020-0190(94)90037-X\n | issue = 1\n | journal = [[Information Processing Letters]]\n | mr = 1279491\n | pages = 9–11\n | title = On the problem of approximating the number of bases of a matroid\n | volume = 50\n | year = 1994}}.\n*{{citation\n | last1 = Bansal | first1 = N.\n | last2 = Pendavingh | first2 = R.\n | last3 = van der Pol | first3 = J.\n | arxiv = 1206.6270\n | title = On the number of matroids\n | year = 2012| bibcode = 2012arXiv1206.6270B}}.\n*{{citation\n | last1 = Bixby | first1 = Robert E.\n | last2 = Cunningham | first2 = William H.\n | contribution = Matroids, graphs, and 3-connectivity\n | location = New York\n | mr = 538038\n | pages = 91–103\n | publisher = Academic Press\n | title = Graph theory and related topics (Proc. Conf., Univ. Waterloo, Waterloo, Ont., 1977)\n | year = 1979}}.\n*{{citation\n | last1 = Chávez Lomelí | first1 = Laura\n | last2 = Welsh | first2 = Dominic | author2-link = Dominic Welsh\n | contribution = Randomised approximation of the number of bases\n | doi = 10.1090/conm/197/02534\n | location = Providence, RI\n | mr = 1411698\n | pages = 371–376\n | publisher = American Mathematical Society\n | series = Contemporary Mathematics\n | title = Matroid Theory (Seattle, WA, 1995)\n | volume = 197\n | year = 1996}}.\n*{{citation\n | last1 = Coullard | first1 = Collette R.\n | last2 = Hellerstein | first2 = Lisa\n | doi = 10.1007/BF01844845\n | issue = 2\n | journal = [[Combinatorica]]\n | mr = 1401892\n | pages = 189–208\n | title = Independence and port oracles for matroids, with an application to computational learning theory\n | volume = 16\n | year = 1996}}.\n*{{citation\n | last = Cunningham | first = William H.\n | doi = 10.1137/0215066\n | issue = 4\n | journal = [[SIAM Journal on Computing]]\n | mr = 861361\n | pages = 948–957\n | title = Improved bounds for matroid partition and intersection algorithms\n | volume = 15\n | year = 1986}}.\n*{{citation\n | last = Edmonds | first = Jack | author-link = Jack Edmonds\n | journal = Journal of Research of the National Bureau of Standards\n | mr = 0190025\n | pages = 67–72\n | title = Minimum partition of a matroid into independent subsets\n | url = http://cdm16009.contentdm.oclc.org/cdm/ref/collection/p13011coll6/id/66398\n | volume = 69B\n | year = 1965\n | doi=10.6028/jres.069b.004}}.\n*{{citation\n | last = Edmonds | first = Jack | author-link = Jack Edmonds\n | doi = 10.1007/BF01584082\n | journal = [[Mathematical Programming (journal)|Mathematical Programming]]\n | mr = 0297357\n | pages = 127–136\n | title = Matroids and the greedy algorithm\n | volume = 1\n | year = 1971}}.\n*{{citation\n | last1 = Fujishige | first1 = Satoru\n | last2 = Zhang | first2 = Xiaodong\n | issue = 1\n | journal = Journal of the Operations Research Society of Japan\n | mr = 1337446\n | pages = 124–136\n | title = An efficient cost scaling algorithm for the independent assignment problem\n | volume = 38\n | year = 1995}}.\n*{{citation\n | last1 = Hausmann | first1 = Dirk\n | last2 = Korte | first2 = Bernhard\n | doi = 10.1016/0012-365X(78)90097-3\n | issue = 3\n | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]\n | mr = 523316\n | pages = 261–276\n | title = Lower bounds on the worst-case complexity of some oracle algorithms\n | volume = 24\n | year = 1978}}.\n*{{citation\n | last1 = Hausmann | first1 = D.\n | last2 = Korte | first2 = B.\n | contribution = Algorithmic versus axiomatic definitions of matroids\n | doi = 10.1007/BFb0120924\n | volume = 14\n | mr = 600125\n | pages = 98–111\n | series = Mathematical Programming Studies\n | title = Mathematical programming at Oberwolfach (Proc. Conf., Math. Forschungsinstitut, Oberwolfach, 1979)\n | year = 1981}}.\n*{{citation\n | last = Ingleton | first = A. W.\n | doi = 10.1112/jlms/s1-34.1.49\n | journal = Journal of the London Mathematical Society\n | mr = 0101848\n | pages = 49–56\n | series = Second Series\n | title = A note on independence functions and rank\n | volume = 34\n | year = 1959}}.\n*{{citation\n | last1 = Jensen | first1 = Per M.\n | last2 = Korte | first2 = Bernhard\n | doi = 10.1137/0211014\n | issue = 1\n | journal = [[SIAM Journal on Computing]]\n | mr = 646772\n | pages = 184–190\n | title = Complexity of matroid property algorithms\n | volume = 11\n | year = 1982}}.\n*{{citation\n | last1 = Karp | first1 = Richard M. | author1-link = Richard M. Karp\n | last2 = Upfal | first2 = Eli | author2-link = Eli Upfal\n | last3 = Wigderson | first3 = Avi | author3-link = Avi Wigderson\n | doi = 10.1016/0022-0000(88)90027-X\n | issue = 2\n | journal = [[Journal of Computer and System Sciences]]\n | mr = 950432\n | pages = 225–253\n | title = The complexity of parallel search\n | volume = 36\n | year = 1988}}.\n*{{citation\n | last1 = Kelmans | first1 = A. K.\n | last2 = Polesskiĭ | first2 = V. P.\n | contribution = Extremal sets and covering and packing problems in matroids\n | location = Providence, RI\n | mr = 1269136\n | pages = 149–174\n | publisher = Amer. Math. Soc.\n | series = Amer. Math. Soc. Transl. Ser. 2\n | title = Selected topics in discrete mathematics (Moscow, 1972–1990)\n | volume = 158\n | year = 1994}}.\n*{{citation\n | last1 = Khachiyan | first1 = L. | author1-link = Leonid Khachiyan\n | last2 = Boros | first2 = E.\n | last3 = Elbassioni | first3 = K.\n | last4 = Gurvich | first4 = V.\n | last5 = Makino | first5 = K.\n | doi = 10.1137/S0895480103428338\n | issue = 4\n | journal = [[SIAM Journal on Discrete Mathematics]]\n | mr = 2206374\n | pages = 966–984\n | title = On the complexity of some enumeration problems for matroids\n | volume = 19\n | year = 2005| citeseerx = 10.1.1.124.4286}}.\n*{{citation\n | last = Knuth | first = Donald E. | author-link = Donald Knuth\n | doi = 10.1016/0097-3165(74)90063-6\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 0335312\n | pages = 398–400\n | series = Series A\n | title = The asymptotic number of geometries\n | volume = 16\n | year = 1974}}.\n*{{citation\n | last1 = Korte | first1 = Bernhard\n | last2 = Hausmann | first2 = Dirk\n | contribution = An analysis of the greedy heuristic for independence systems\n | doi = 10.1016/S0167-5060(08)70322-4\n | mr = 500689\n | pages = 65–74\n | series = Annals of Discrete Mathematics\n | title = Algorithmic Aspects of Combinatorics (Conf., Vancouver Island, B.C., 1976)\n | volume = 2\n | year = 1978}}.\n*{{citation\n | last1 = Korte | first1 = B.\n | last2 = Schrader | first2 = R.\n | editor1-last = Gruska | editor1-first = Jozef\n | editor2-last = Chytil | editor2-first = Michal\n | contribution = A survey on oracle techniques\n | location = Berlin\n | mr = 652740\n | pages = 61–77\n | publisher = Springer\n | series = Lecture Notes in Computer Science\n | title = Mathematical Foundations of Computer Science 1981, Proceedings, 10th Symposium Štrbské Pleso, Czechoslovakia August 31 – September 4, 1981\n | doi = 10.1007/3-540-10856-4_74\n | volume = 118\n | year = 1981}}.\n*{{citation\n | last = Lazarson | first = T.\n | doi = 10.1112/jlms/s1-33.1.21\n | journal = Journal of the London Mathematical Society\n | mr = 0098701\n | pages = 21–25\n | series = Second Series\n | title = The representation problem for independence functions\n | volume = 33\n | year = 1958}}.\n*{{citation\n | last = Lovász | first = L. | authorlink = László Lovász\n | contribution = The matroid matching problem\n | location = Amsterdam\n | mr = 642059\n | pages = 495–517\n | publisher = North-Holland\n | series = Colloq. Math. Soc. János Bolyai\n | title = Algebraic methods in graph theory, Vol. I, II (Szeged, 1978)\n | volume = 25\n | year = 1981}}.\n*{{citation\n | last = Mayhew | first = Dillon\n | arxiv = math/0702567\n | doi = 10.1137/050640576\n | issue = 2\n | journal = [[SIAM Journal on Discrete Mathematics]]\n | mr = 2399359\n | pages = 455–466\n | title = Matroid complexity and nonsuccinct descriptions\n | volume = 22\n | year = 2008}}.\n*{{citation\n | last1 = Oum | first1 = Sang-il\n | last2 = Seymour | first2 = Paul | author2-link = Paul Seymour (mathematician)\n | doi = 10.1016/j.jctb.2006.06.006\n | issue = 3\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 2305892\n | pages = 385–393\n | series = Series B\n | title = Testing branch-width\n | volume = 97\n | year = 2007}}.\n*{{citation\n | last = Piff | first = M. J.\n | journal = [[Journal of Combinatorial Theory]]\n | doi = 10.1016/0095-8956(73)90006-3\n | mr = 0316282\n | pages = 241–245\n | series = Series B\n | title = An upper bound for the number of matroids\n | volume = 14\n | year = 1973}}.\n*{{citation\n | last1 = Piff | first1 = M. J.\n | last2 = Welsh | first2 = D. J. A. | author2-link = Dominic Welsh\n | doi = 10.1112/blms/3.1.55\n | journal = The Bulletin of the London Mathematical Society\n | mr = 0282867\n | pages = 55–56\n | title = The number of combinatorial geometries\n | volume = 3\n | year = 1971}}.\n*{{citation\n | last = Rado | first = R. | authorlink = Richard Rado\n | doi = 10.1093/qmath/os-13.1.83\n | journal = The Quarterly Journal of Mathematics\n | mr = 0008250\n | pages = 83–89\n | series = Second Series\n | title = A theorem on independence relations\n | volume = 13\n | year = 1942| bibcode = 1942QJMat..13...83R}}.\n*{{citation\n | last = Rado | first = R. | authorlink = Richard Rado\n | doi = 10.1112/plms/s3-7.1.300\n | journal = Proceedings of the London Mathematical Society\n | mr = 0088459\n | pages = 300–320\n | series = Third Series\n | title = Note on independence functions\n | volume = 7\n | year = 1957}}.\n*{{citation\n | last1 = Robinson | first1 = G. C.\n | last2 = Welsh | first2 = D. J. A. | author2-link = Dominic Welsh\n | doi = 10.1017/S0305004100056498\n | issue = 1\n | journal = Mathematical Proceedings of the Cambridge Philosophical Society\n | mr = 549295\n | pages = 29–45\n | title = The computational complexity of matroid properties\n | volume = 87\n | year = 1980| bibcode = 1980MPCPS..87...29R}}.\n*{{citation\n | last = Seymour | first = P. D. | authorlink = Paul Seymour (mathematician)\n | doi = 10.1007/BF02579179\n | issue = 1\n | journal = [[Combinatorica]]\n | mr = 602418\n | pages = 75–78\n | title = Recognizing graphic matroids\n | volume = 1\n | year = 1981}}.\n*{{citation\n | last1 = Seymour | first1 = P. D. | author1-link = Paul Seymour (mathematician)\n | last2 = Walton | first2 = P. N.\n | doi = 10.1112/jlms/s2-23.2.193\n | issue = 2\n | journal = Journal of the London Mathematical Society\n | mr = 609098\n | pages = 193–203\n | series = Second Series\n | title = Detecting matroid minors\n | volume = 23\n | year = 1981}}.\n*{{citation\n | last = Truemper | first = K.\n | issue = 3\n | journal = European Journal of Combinatorics\n | mr = 679212\n | pages = 275–291\n | title = On the efficiency of representability tests for matroids\n | volume = 3\n | year = 1982\n | doi=10.1016/s0195-6698(82)80039-5}}.\n*{{citation\n | last = Truemper | first = K.\n | edition = revised\n | title = Matroid Decomposition\n | url = http://www.utdallas.edu/~klaus/Mbook/matroiddecompositionbook.pdf\n | year = 1998}}.\n*{{citation\n | last = Tutte | first = W. T. | author-link = W. T. Tutte\n | doi = 10.4153/CJM-1966-129-2\n | journal = [[Canadian Journal of Mathematics]]\n | mr = 0205880\n | pages = 1301–1324\n | title = Connectivity in matroids\n | volume = 18\n | year = 1966}}.\n{{refend}}\n\n[[Category:Matroid theory|Oracle]]"
    },
    {
      "title": "Partition matroid",
      "url": "https://en.wikipedia.org/wiki/Partition_matroid",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Direct sum of uniform matroids}}\nIn mathematics, a '''partition matroid''' or '''partitional matroid''' is a [[matroid]] formed from a [[direct sum]] of [[uniform matroid]]s.<ref>{{citation\n | last = Recski | first = A.\n | contribution = On partitional matroids with applications\n | location = Amsterdam\n | mr = 0389630\n | pages = 1169–1179\n | publisher = North-Holland\n | series = Colloq. Math. Soc. János Bolyai\n | title = Infinite and finite sets (Colloq., Keszthely, 1973; dedicated to P. Erdős on his 60th birthday), Vol. III\n | volume = 10\n | year = 1975}}.</ref>\n\n==Definition==\nLet <math>B_i</math> be a collection of [[disjoint sets]], and let <math>d_i</math> be integers with <math>0\\le d_i\\le |B_i|</math>. Define a subset <math>I\\subset \\bigcup_i B_i</math> to be \"independent\" when, for every index <math>i</math>, <math>|I\\cap B_i|\\le d_i</math>. Then the sets that are independent sets in this way form the independent sets of a [[matroid]], called a partition matroid. The sets <math>B_i</math> are called the blocks of the partition matroid. A basis of the matroid is a set whose intersection with every block <math>B_i</math> has size exactly <math>d_i</math>, and a circuit of the matroid is a subset of a single block <math>B_i</math> with size exactly <math>d_i+1</math>. The [[Matroid rank|rank]] of the matroid is <math>\\sum d_i</math>.<ref>{{citation\n | last = Lawler | first = Eugene L. | authorlink = Eugene Lawler\n | location = Rinehart and Winston, New York\n | mr = 0439106\n | page = 272\n | publisher = Holt\n | title = Combinatorial Optimization: Networks and Matroids\n | year = 1976}}.</ref>\n\nEvery [[uniform matroid]] <math>U{}^r_n</math> is a partition matroid, with a single block <math>B_1</math> of <math>n</math> elements and with <math>d_1=r</math>. Every partition matroid is the direct sum of a collection of uniform matroids, one for each of its blocks.\n\nIn some publications, the notion of a partition matroid is defined more restrictively, with every <math>d_i=1</math>. The partitions that obey this more restrictive definition are the [[transversal matroid]]s of the family of disjoint sets given by their blocks.<ref>E.g., see {{harvtxt|Kashiwabara|Okamoto|Uno|2007}}. {{harvtxt|Lawler|1976}} uses the broader definition but notes that the <math>d_i=1</math> restriction is useful in many applications.</ref>\n\n==Properties==\nAs with the uniform matroids they are formed from, the [[dual matroid]] of a partition matroid is also a partition matroid, and every [[matroid minor|minor]] of a partition matroid is also a partition matroid. Direct sums of partition matroids are partition matroids as well.\n\n==Matching==\nA [[maximum matching]] in a graph is a set of edges that is as large as possible subject to the condition that no two edges share an endpoint. In a [[bipartite graph]] with bipartition <math>(U,V)</math>, the sets of edges satisfying the condition that no two edges share an endpoint in <math>U</math> are the independent sets of a partition matroid with one block per vertex in <math>U</math> and with each of the numbers <math>d_i</math> equal to one. The sets of edges satisfying the condition that no two edges share an endpoint in <math>V</math> are the independent sets of a second partition matroid. Therefore, the bipartite maximum matching problem can be represented as a [[matroid intersection]] of these two matroids.<ref>{{citation\n | last1 = Papadimitriou | first1 = Christos H. | author1-link = Christos Papadimitriou\n | last2 = Steiglitz | first2 = Kenneth | author2-link = Kenneth Steiglitz\n | isbn = 0-13-152462-3\n | location = Englewood Cliffs, N.J.\n | mr = 663728\n | pages = 289–290\n | publisher = Prentice-Hall Inc.\n | title = Combinatorial Optimization: Algorithms and Complexity\n | year = 1982}}.</ref>\n\nMore generally the matchings of a graph may be represented as an intersection of two matroids if and only if every odd cycle in the graph is a triangle containing two or more degree-two vertices.<ref>{{citation\n | last1 = Fekete | first1 = Sándor P.\n | last2 = Firla | first2 = Robert T.\n | last3 = Spille | first3 = Bianca\n | arxiv = math/0212235\n | doi = 10.1007/s001860300301\n | issue = 2\n | journal = Mathematical Methods of Operations Research\n | mr = 2015015\n | pages = 319–329\n | title = Characterizing matchings as the intersection of matroids\n | volume = 58\n | year = 2003}}.</ref>\n\n==Clique complexes==\nA [[clique complex]] is a family of sets of vertices of a graph <math>G</math> that induce complete subgraphs of <math>G</math>. A clique complex forms a matroid if and only if <math>G</math> is a [[complete multipartite graph]], and in this case the resulting matroid is a partition matroid. The clique complexes are exactly the set systems that can be formed as [[Matroid intersection|intersections]] of families of partition matroids for which every {{nowrap|<math>d_i=1</math>.<ref>{{citation\n | last1 = Kashiwabara | first1 = Kenji\n | last2 = Okamoto | first2 = Yoshio\n | last3 = Uno | first3 = Takeaki\n | doi = 10.1016/j.dam.2007.05.004\n | issue = 15\n | journal = Discrete Applied Mathematics\n | mr = 2351976\n | pages = 1910–1929\n | title = Matroid representation of clique complexes\n | volume = 155\n | year = 2007}}. For the same results in a complementary form using independent sets in place of cliques, see {{citation\n | last1 = Tyshkevich | first1 = R. I. | author1-link = Regina Tyshkevich\n | last2 = Urbanovich | first2 = O. P.\n | last3 = Zverovich | first3 = I. È.\n | contribution = Matroidal decomposition of a graph\n | location = Warsaw\n | mr = 1097648\n | pages = 195–205\n | publisher = PWN\n | series = Banach Center Publ.\n | title = Combinatorics and graph theory (Warsaw, 1987)\n | volume = 25\n | year = 1989}}.</ref>}}\n\n==Enumeration==\nThe number of distinct partition matroids that can be defined over a set of <math>n</math> labeled elements, for <math>n=0,1,2,\\dots</math>, is\n:1, 2, 5, 16, 62, 276, 1377, 7596, 45789, 298626, 2090910, ... {{OEIS|A005387}}.\nThe [[exponential generating function]] of this sequence is <math>f(x)=\\exp(e^x(x-1)+2x+1)</math>.<ref>{{citation\n | last = Recski | first = A.\n | journal = Studia Scientiarum Mathematicarum Hungarica\n | mr = 0379248\n | pages = 247–249 (1975)\n | title = Enumerating partitional matroids\n | volume = 9\n | year = 1974}}.</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Matroid theory]]\n[[Category:Matching]]"
    },
    {
      "title": "Matroid partitioning",
      "url": "https://en.wikipedia.org/wiki/Matroid_partitioning",
      "text": "The '''matroid partitioning''' problem is a problem arising in the mathematical study of [[matroid]]s and in the design and analysis of [[algorithm]]s, in which the goal is to partition the elements of a matroid into as few independent sets as possible. An example is the problem of computing the [[arboricity]] of an [[undirected graph]], the minimum number of [[tree (graph theory)|forests]] needed to cover all of its edges. Matroid partitioning may be solved in [[polynomial time]], given an [[matroid oracle|independence oracle]] for the matroid. It may be generalized to show that a [[matroid sum]] is itself a matroid, to provide an algorithm for computing [[Matroid rank|ranks]] and independent sets in matroid sums, and to compute the largest common independent set in the [[matroid intersection|intersection]] of two given matroids.<ref name=\"fgt\">{{citation\n | last1 = Scheinerman | first1 = Edward R. | author1-link = Ed Scheinerman\n | last2 = Ullman | first2 = Daniel H.\n | contribution = 5. Fractional arboricity and matroid methods\n | isbn = 0-471-17864-0\n | location = New York\n | mr = 1481157\n | pages = 99–126\n | publisher = John Wiley & Sons Inc.\n | series = Wiley-Interscience Series in Discrete Mathematics and Optimization\n | title = Fractional graph theory\n | year = 1997}}.</ref> \n\n==Example==\n[[File:K44 arboricity.svg|thumb|A partition of the edges of the [[complete bipartite graph]] ''K''<sub>4,4</sub> into three forests, showing that it has arboricity at most three]]\nThe [[arboricity]] of an [[undirected graph]] is the minimum number of [[tree (graph theory)|forests]] into which its edges can be partitioned, or equivalently (by adding overlapping edges to each forest as necessary) the minimum number of [[spanning tree|spanning forests]] whose union is the whole graph. A formula proved by [[Crispin Nash-Williams]] characterizes the arboricity exactly: it is the maximum, over all subgraphs <math>H</math> of the given graph <math>G</math>, of the quantity <math>\\left\\lceil\\frac{|E(H)|}{|V(H)|-1}\\right\\rceil</math>.<ref>{{citation\n | last = Nash-Williams | first = C. St. J. A. | author-link = Crispin St. J. A. Nash-Williams\n | doi = 10.1112/jlms/s1-39.1.12\n | issue = 1\n | journal = [[Journal of the London Mathematical Society]]\n | mr = 0161333\n | page = 12\n | title = Decomposition of finite graphs into forests\n | volume = 39\n | year = 1964}}.</ref>\n\nThe forests of a graph form the independent sets of the associated [[graphic matroid]], and the quantity <math>|V(H)|-1</math> appearing in Nash-Williams' formula is the rank of the graphic matroid of <math>H</math>, the maximum size of one of its independent sets. Thus, the problem of determining the arboricity of a graph is exactly the matroid partitioning problem for the graphic matroid. The fact that the <math>|E(H)|</math> elements of this matroid cannot be partitioned into fewer than <math>\\frac{|E(H)|}{|V(H)|-1}</math> independent subsets is then just an application of the [[pigeonhole principle]] saying that, if <math>x</math> items are partitioned into sets of size at most <math>y</math>, then at least <math>x/y</math> sets are needed. The harder direction of Nash-Williams' formula, which can be generalized to all matroids, is the proof that a partition of this size always exists.<ref name=\"fgt\"/>\n\n==Formula for partition size==\nTo generalize Nash-Williams' formula, one may replace <math>G</math> by a matroid <math>M</math>, and the subgraph <math>H</math> of <math>G</math> with a [[matroid minor|restriction]] <math>M|S</math> of <math>M</math> to a subset <math>S</math> of its elements. The number of edges of the subgraph <math>H</math> becomes, in this generalization, the cardinality <math>|S|</math> of the selected subset, and the formula <math>|V(H)|-1</math> for the maximum size of a forest in <math>H</math> becomes the rank <math>r(S)</math>. Thus, the minimum number of independent sets in a partition of the given matroid <math>M</math> should be given by the formula\n:<math>k(M)=\\max_S \\left\\lceil\\frac{|S|}{r(S)}\\right\\rceil,</math>\nwhich is valid for all matroids and was given an algorithmic proof by {{harvtxt|Edmonds|1965}}.<ref name=\"fgt\"/><ref name=\"e65\">{{citation\n | last = Edmonds | first = Jack | author-link = Jack Edmonds\n | journal = Journal of Research of the National Bureau of Standards\n | mr = 0190025\n | pages = 67–72\n | title = Minimum partition of a matroid into independent subsets\n | url = http://cdm16009.contentdm.oclc.org/cdm/ref/collection/p13011coll6/id/66398\n | volume = 69B\n | year = 1965\n | doi=10.6028/jres.069b.004}}.</ref>\n\n==Algorithms==\nThe first algorithm for matroid partitioning was given by {{harvtxt|Edmonds|1965}}.<ref name=\"e65\"/> It is an incremental augmenting-path algorithm that considers the elements of the matroid one by one, in an arbitrary order, maintaining at each step of the algorithm an optimal partition for the elements that have been considered so far. At each step, when considering an element <math>x</math> that has not yet been placed into a partition, the algorithm constructs a [[directed graph]] that has as its nodes the elements that have already been partitioned, the new element <math>x</math>, and a special element <math>\\bot_i</math> for each of the <math>k</math> independent sets in the current partition. It then forms a directed graph <math>G_x</math> on this node set, with a directed arc <math>\\bot_i\\rightarrow y</math> for each matroid element <math>y</math> that can be added to partition set <math>i</math> without causing it to become dependent, and with a directed arc <math>z\\rightarrow y</math> for each pair of matroid elements <math>(y,z)</math> such that removing <math>z</math> from its partition and replacing it with <math>y</math> forms another independent set.<ref name=\"fgt\"/><ref name=\"e65\"/>\n\nIf this graph contains a [[directed path]] from an element <math>\\bot_i</math> to the newly considered element <math>x</math>, then the shortest such path (or more generally any path that does not have any shortcutting edges) describes a sequence of changes that can be made simultaneously to the partition sets in order to form a new partition, with the same number of sets, that also includes <math>x</math>. In this case, the algorithm performs these changes and continues. If, on the other hand, no such path exists, then let <math>S</math> consist of the matroid elements from which <math>x</math> is [[reachability|reachable]] in <math>D</math>. Each set in the current partition must be a maximal independent set in <math>M|S</math>, for if some element <math>y</math> of <math>S</math> could be added to partition set <math>i</math> in the restriction, then either there would exist an arc <math>\\bot_i\\rightarrow y</math> (if partition set <math>i</math> is non-maximal in the full matroid <math>M</math>) or an arc <math>z\\rightarrow y</math> where <math>z\\notin S</math> (if the partition set is non-maximal in <math>S</math> but maximal in the full matroid). In either case the existence of this arc contradicts the assumed construction of the set <math>S</math>, and the contradiction proves that each partition set is maximal. Thus, by the easy direction of the matroid partitioning formula, the number of sets needed to partition <math>S</math> is at least\n:<math>\\left\\lceil\\frac{|S|}{r(S)}\\right\\rceil=\\left\\lceil\\frac{kr(S)+1}{r(S)}\\right\\rceil=k+1</math>,\nso in this case the algorithm may find an optimal partition by placing <math>x</math> into its own new independent set and leaving the other independent sets unchanged.<ref name=\"fgt\"/><ref name=\"e65\"/>\n\nThe overall algorithm, then, considers  each element <math>x</math> of the given matroid in turn, constructs the graph <math>G_x</math>, tests which nodes can reach <math>x</math>, and uses this information to update the current partition so that it includes <math>x</math>. At each step, the partition of the elements considered so far is optimal, so when the algorithm terminates it will have found an optimal partition for the whole matroid.\nProving that this algorithm is correct requires showing that a shorcut-free path in the auxiliary graph always describes a sequence of operations that, when performed simultaneously, correctly preserves the independence of the sets in the partition; a proof of this fact was given by Edmonds.\nBecause the algorithm only increases the number of sets in the partition when the matroid partitioning formula shows that a larger number is needed, the correctness of this algorithm also shows the correctness of the formula.<ref name=\"fgt\"/><ref name=\"e65\"/>\n\nAlthough this algorithm depends only on the existence of an [[matroid oracle|independence oracle]] for its correctness, faster algorithms can be found in many cases by taking advantage of the more specialized structure of specific types of matroids (such as [[graphic matroid]]s) from which a particular partitioning problem has been defined.<ref name=\"gw92\">{{citation\n | last1 = Gabow | first1 = Harold N.\n | last2 = Westermann | first2 = Herbert H.\n | doi = 10.1007/BF01758774\n | issue = 5-6\n | journal = Algorithmica\n | mr = 1154585\n | pages = 465–497\n | title = Forests, frames, and games: algorithms for matroid sums and applications\n | volume = 7\n | year = 1992}}.</ref> \n\n==Related problems==\nA [[matroid sum]] <math>\\sum_i M_i</math> (where each <math>M_i</math> is a matroid) is itself a matroid, having as its elements the union of the elements of the summands. A set is independent in the sum if it can be partitioned into sets that are independent within each summand. The matroid partitioning algorithm generalizes to the problem of testing whether a set is independent in a matroid sum, and its correctness can be used to prove that a matroid sum is necessarily a matroid.<ref name=\"e65\"/><ref name=\"gw92\"/>\n\nThe [[matroid intersection]] problem of finding the largest set that is independent in two matroids <math>M_1</math> and <math>M_2</math> may be solved by turning it into an equivalent matroid sum problem: if <math>B</math> is a basis of the sum <math>M_1+M_2^*</math>, where <math>M_2^*</math> is the dual of <math>M_2</math>, then <math>B</math> must have full rank in <math>M_2^*</math> and removing a maximal independent set of <math>M_2^*</math> from <math>B</math> leaves a maximum intersection.<ref>{{citation\n | last = Edmonds | first = Jack\n | contribution = Submodular functions, matroids, and certain polyhedra\n | location = New York\n | mr = 0270945\n | pages = 69–87\n | publisher = Gordon and Breach\n | title = Combinatorial Structures and their Applications (Proc. Calgary Internat. Conf., Calgary, Alta., 1969)\n | year = 1970}}.</ref>\n\nMatroid partitioning is a form of [[set cover]] problem, and the corresponding [[set packing]] problem (find a maximum number of disjoint spanning sets within a given matroid) is also of interest. It can be solved by algorithms similar to those for matroid partitioning.<ref name=\"gw92\"/> The fractional set packing and set covering problems associated with a matroid (that is, assign a weight to each independent set in such a way that for every element the total weight of the sets containing it is at most one or at least one, maximizing or minimizing the total weight of all the sets, respectively) can also be solved in polynomial time using matroid partitioning methods.<ref name=\"fgt\"/>\n\nAs well as its use in calculating the arboricity of a graph, matroid partitioning can be used with other matroids to find a subgraph of a given graph whose average [[degree (graph theory)|degree]] is maximum, and to find the edge toughness of a graph (a variant of [[graph toughness]] involving the deletion of edges in place of vertices).<ref name=\"fgt\"/>\n\n==References==\n{{reflist}}\n\n[[Category:Matroid theory|Partitioning]]"
    },
    {
      "title": "Paving matroid",
      "url": "https://en.wikipedia.org/wiki/Paving_matroid",
      "text": "[[File:Vamos matroid.svg|thumb|The [[Vámos matroid]], a paving matroid of rank four; the shaded parallelograms depict its five circuits of size four]]\nIn the mathematical theory of [[matroid]]s, a '''paving matroid''' is a matroid in which every circuit has size at least as large as the matroid's rank. In a matroid of rank <math>r</math> every circuit has size at most <math>r+1</math>, so it is equivalent to define paving matroids as the matroids in which the size of every circuit belongs to the set <math>\\{r,r+1\\}</math>.<ref name=\"w\">{{harvtxt|Welsh|1976}}.</ref> It has been conjectured that [[almost all]] matroids are paving matroids.\n\n==Examples==\nEvery simple matroid of rank three is a paving matroid; for instance this is true of the [[Fano plane|Fano matroid]].  The [[Vámos matroid]] provides another example, of rank four.\n\n[[Uniform matroid]]s of rank <math>r</math> have the property that every circuit is of length exactly <math>r+1</math> and hence are all paving matroids;<ref name=Ox26>{{harvnb|Oxley|1992|p=26}}</ref> the converse does not hold, for example, the [[cycle matroid]] of the [[complete graph]] <math>K_4</math> is paving but not uniform.<ref name=Ox27>{{harvnb|Oxley|1992|p=27}}</ref>\n\nA [[Steiner system]] <math>S(t,k,v)</math> is a pair <math>(S,\\mathcal{D})</math> where <math>S</math> is a [[finite set]] of size <math>v</math> and <math>\\mathcal{D}</math> is a family of <math>k</math>-element subsets of <math>S</math> with the property that every <math>t</math>-element subset of <math>S</math> is also a subset of exactly one set in <math>\\mathcal{D}</math>.  The elements of <math>\\mathcal{D}</math> form a <math>t</math>-partition of <math>S</math> and hence are the hyperplanes of a paving matroid on <math>S</math>.<ref name=Ox367>{{harvnb|Oxley|1992|p=367}}</ref>\n\n==''d''-Partitions==\nIf a paving matroid has rank <math>d+1</math>, then its hyperplanes form a [[family of sets|set system]] known as a <math>d</math>-partition. A family of two or more sets <math>\\mathcal{F}</math> forms a <math>d</math>-partition if every set in <math>\\mathcal{F}</math> has size at least <math>d</math> and every <math>d</math>-element subset of <math>\\bigcup\\mathcal{F}</math> is a subset of exactly one set in <math>\\mathcal{F}</math>. Conversely, if <math>\\mathcal{F}</math> is a <math>d</math>-partition, then it can be used to define a paving matroid on <math>E = \\bigcup\\mathcal{F}</math> for which <math>\\mathcal{F}</math> is the set of hyperplanes. In this matroid, a subset <math>I</math> of <math>E</math> is independent whenever either <math>|I|\\le d</math> or <math>|I|=d+1</math> and <math>I</math> is not a subset of any set in <math>\\mathcal{F}</math>.<ref name=\"w\"/>\n\n==Combinatorial enumeration==\n[[Combinatorial enumeration]] of the simple matroids on up to nine elements has shown that a large fraction of them are also paving matroids.<ref name=\"w\"/> On this basis, it has been conjectured that [[almost all]] matroids are paving matroids.{{sfnp|Mayhew|Newman|Welsh|Whittle|2011}} More precisely, according to this conjecture, the limit, as ''n'' goes to infinity, of the ratio between the number of paving matroids and the number of all matroids should equal one. If so, the same statement can be made for the '''sparse paving matroids''', matroids that are both paving and dual to a paving matroid. Although this remains open, a similar statement on the asymptotic ratio of the logarithms of the numbers of matroids and sparse paving matroids has been proven.{{sfnp|Pendavingh|van der Pol|2015}}\n\n==History==\nPaving matroids were initially studied by {{harvtxt|Hartmanis|1959}}, in their equivalent formulation in terms of <math>d</math>-partitions; Hartmanis called them generalized partition lattices. In their 1970 book ''Combinatorial Geometries'', Henry Crapo and [[Gian-Carlo Rota]] observed that these structures were matroidal; the name \"paving matroid\" was introduced by {{harvtxt|Welsh|1976}} following a suggestion of Rota.<ref name=Ox75>{{harvnb|Oxley|1992|p=75}}</ref>\n\nThe simpler structure of paving matroids, compared to arbitrary matroids, has allowed some facts about them to be proven that remain elusive in the more general case. An example is [[Rota's basis conjecture]], the statement that a set of ''n'' disjoint bases in a rank-''n'' matroid can be arranged into an ''n''&nbsp;&times;&nbsp;''n'' matrix so that the rows of the matrix are the given bases and the columns are also bases. It has been proven true for paving matroids, but remains open for most other matroids.{{sfnp|Geelen|Humphries|2006}}\n\n==Notes==\n{{reflist|colwidth=30em}}\n\n==References==\n*{{citation\n | last1 = Geelen | first1 = Jim | author1-link = Jim Geelen\n | last2 = Humphries | first2 = Peter J.\n | doi = 10.1137/060655596\n | issue = 4\n | journal = SIAM Journal on Discrete Mathematics\n | mr = 2272246\n | pages = 1042–1045 (electronic)\n | title = Rota's basis conjecture for paving matroids\n | url = http://www.math.uwaterloo.ca/~jfgeelen/publications/paving.pdf\n | volume = 20\n | year = 2006}}.\n*{{citation\n | last = Hartmanis | first = Juris | author-link = Juris Hartmanis\n | doi = 10.4153/CJM-1959-013-8\n | journal = Canadian Journal of Mathematics\n | mr = 0099931 | zbl=0089.37002 \n | pages = 97–106\n | title = Lattice theory of generalized partitions\n | volume = 11\n | year = 1959}}.\n*{{citation\n | last1 = Mayhew | first1 = Dillon\n | last2 = Newman | first2 = Mike\n | last3 = Welsh | first3 = Dominic\n | last4 = Whittle | first4 = Geoff\n | doi = 10.1016/j.ejc.2011.01.016\n | issue = 6\n | journal = European Journal of Combinatorics\n | mr = 2821559\n | pages = 882–890\n | title = On the asymptotic proportion of connected matroids\n | volume = 32\n | year = 2011}}.\n*{{citation | zbl=0784.05002 | last=Oxley | first=James G. | authorlink = James Oxley | title=Matroid theory | series=Oxford Science Publications | location=Oxford | publisher=[[Oxford University Press]] | year=1992 | isbn=0-19-853563-5 }}\n*{{citation\n | last1 = Pendavingh | first1 = Rudi\n | last2 = van der Pol | first2 = Jorn\n | issue = 2\n | journal = The Electronic Journal of Combinatorics\n | at = #2.51\n | title = On the number of matroids compared to the number of sparse paving matroids\n | volume = 22\n | year = 2015}}.\n*{{citation\n | last = Welsh | first = D. J. A. | authorlink = Dominic Welsh\n | contribution = 2.3. Paving Matroids\n | isbn = 9780486474397\n | pages = 40–41, 44\n | publisher = Courier Dover Publications\n | title = Matroid Theory\n | year = 1976}}. Reprinted 2010.\n\n[[Category:Matroid theory]]"
    },
    {
      "title": "Polymatroid",
      "url": "https://en.wikipedia.org/wiki/Polymatroid",
      "text": "{{Refimprove|date=February 2011}}\n\nIn mathematics, a '''polymatroid''' is a [[polytope]] associated with a [[submodular function]]. The notion was introduced by [[Jack Edmonds]] in 1970.<ref name=\"edmonds\" /> It is also described as the [[multiset]] analogue of the [[matroid]].\n\n==Definition==\nLet <math>E</math> be a finite [[Set (mathematics)|set]] and <math>f: 2^E\\rightarrow \\mathbb{R}_+</math> a non-decreasing [[Submodular set function|submodular function]], that is, for every <math> A\\subset B \\subset E</math> we have <math> f(A)\\leq f(B) </math>, and <math> f(A)+f(B) \\geq f(A\\cup B) + f(A\\cap B) </math>. We define the '''polymatroid''' associated to <math> f</math> to be the following [[polytope]]:\n\n<math>P_f:= \\left\\{\\textbf{x}\\in \\mathbb{R}_+^E~|~\\sum_{e\\in U}\\textbf{x}(e)\\leq f(U), \\forall U\\subseteq E\\right\\}</math>.\n\nWhen we allow the entries of <math> \\textbf{x} </math> to be negative we denote this polytope by <math>EP_f</math>, and call it the extended polymatroid associated to <math>f</math>.<ref name=\"schrijver\"/>\n\n=== An equivalent definition ===\nLet <math>E</math> be a finite [[Set (mathematics)|set]] and <math> \\textbf{u}, \\textbf{v} \\in \\mathbb{R}^E</math>. We call the ''modulus'' of <math>\\textbf{u}</math> to be the sum of all of its entries, and denote <math> \\textbf{u} \\leq \\textbf{v}</math> whenever <math> v_i-u_i\\geq 0</math> for every <math>i \\in E</math> (notice that this gives an [[Order (mathematics)|order]] to <math>\\mathbb{R}_+^E</math>). A '''polymatroid''' on the ground set <math>E</math> is a nonempty [[Compact (mathematics)|compact]] subset <math>P</math> in <math> \\mathbb{R}^E_+</math>, the set of independent vectors, such that:\n# We have that if <math> \\textbf{v} \\in P</math>, then <math> \\textbf{u} \\in P</math> for every <math> \\textbf{u}\\leq \\textbf{v}</math>:\n# If <math> \\textbf{u},\\textbf{v} \\in P</math> with <math> |\\textbf{v}|> |\\textbf{u}|</math>, then there is a vector <math>w\\in P</math> such that <math> \\textbf{u}<\\textbf{w}\\leq (\\max\\{u_1,v_1\\},\\dots,\\max\\{u_{|E|},v_{|E|}\\}) </math>.\n\nThis definition is equivalent to the one described before<ref name=\"Herzog\"/>, where <math> f</math> is the function defined by <math> f(A) = \\max\\left\\{\\sum_{i\\in A} \\textbf{v}(i)~|~ \\textbf{v} \\in P\\right\\}</math> for every <math> A\\subset E</math>.\n\n==Relation to matroids==\nTo every [[matroid]] <math> M</math> on the ground set <math>E</math> we can associate the set <math>  P_M= \\{\\textbf{w}_F~|~ F\\in M\\}</math>, where for every <math> i\\in E</math> we have that\n\n<math> \\textbf{w}_F(i)=\\begin{cases} 1, & i\\in F;\\\\ 0, & i\\not \\in F.\\end{cases}</math>\n\nBy taking the convex hull of <math> P_M</math> we get a polymatroid, in the sense of the second definition, associated to the rank function of <math>M</math>.\n\n==Relation to generalized permutahedra==\nBecause [[Permutohedron|generalized permutahedra]] can be constructed from submodular functions, and  every generalized permutahedron has an associated submodular function, we have that there should be a correspondence between generalized permutahedra and polymatroids. In fact every polymatroid is a generalized permutahedron that has been translated to have a vertex in the origin. This result suggests that the combinatorial information of polymatroids is shared with generalized permutahedra.\n\n==Properties==\n<math>P_f</math> is nonempty if and only if <math>f\\geq 0</math> and that <math>EP_f</math> is nonempty if and only if <math>f(\\emptyset)\\geq 0</math>.\n\nGiven any extended polymatroid <math>EP</math> there is a unique submodular function <math>f</math> such that <math>f(\\emptyset)=0</math> and <math>EP_f=EP</math>.\n\n==Contrapolymatroids==\nFor a [[supermodular]] ''f'' one analogously may define the '''contrapolymatroid'''\n:<math>\\left\\{w \\in\\mathbb{R}_+^E: \\forall S \\subseteq E, \\sum_{e\\in S}w(e)\\ge f(S)\\right\\}</math>\nThis analogously generalizes the dominant of the ''[[spanning set#Matroids|spanning set]] [[polytope]]'' of matroids.\n\n==Discrete polymatroids ==\nWhen we only focus on the [[lattice points]] of our polymatroids we get what is called, '''discrete polymatroids'''. Formally speaking, the definition of a [[Discrete mathematics|discrete]] polymatroid goes exactly as the one for polymatroids except for where the vectors will live in, instead of <math> \\mathbb{R}^E_+</math> they will live in <math> \\mathbb{Z}^E_+</math>. This combinatorial object is of great interest because of their relationship to [[Monomial ideal|monomial ideals]].\n\n==References==\n;Footnotes\n{{reflist|30em|refs=\n<ref name=\"edmonds\">Edmonds, Jack. ''Submodular functions, matroids, and certain polyhedra''. 1970. Combinatorial Structures and their Applications (Proc. Calgary Internat. Conf., Calgary, Alta., 1969) pp. 69–87 Gordon and Breach, New York. {{MathSciNet|id=0270945 }}</ref>\n<ref name=\"schrijver\">{{Citation|last=Schrijver|first=Alexander|authorlink=Alexander Schrijver|year=2003|title=Combinatorial Optimization|location=|publisher=[[Springer Publishing|Springer]]|isbn=3-540-44389-4|at=§44, p. 767}}</ref>\n<ref name=\"Herzog\">J.Herzog, T.Hibi. ''Monomial Ideals''. 2011. Graduate Texts in Mathematics 260,  pp. 237–263 Springer-Verlag, London.</ref>\n\n}}\n\n\n;Additional reading\n*{{Citation|last=Lee|first=Jon|authorlink=Jon Lee (mathematician)|year= 2004 |title=A First Course in Combinatorial Optimization |location=|publisher=[[Cambridge University Press]]|isbn= 0-521-01012-8}}\n*{{Citation|last=Fujishige|first=Saruto|year=2005|title=Submodular Functions and Optimization|location=|publisher=[[Elsevier]]|isbn=0-444-52086-4}}\n*{{Citation|last=Narayanan|first=H.|year= 1997 |title=Submodular Functions and Electrical Networks|location=|publisher=|isbn= 0-444-82523-1}}\n\n\n[[Category:Matroid theory]]"
    },
    {
      "title": "Matroid polytope",
      "url": "https://en.wikipedia.org/wiki/Matroid_polytope",
      "text": "In mathematics, a '''matroid polytope''', also called a '''matroid basis polytope''' (or '''basis matroid polytope''') to distinguish it from other polytopes derived from a matroid, is a [[polytope]] constructed via the bases of a [[matroid]]. Given a matroid <math>M</math>, the matroid polytope <math>P_M</math> is the [[convex hull]] of the [[indicator vector]]s of the bases of <math>M</math>.\n\n==Definition==\nLet <math>M</math> be a [[matroid]] on <math>n</math> elements. Given a basis <math>B \\subseteq \\{1,\\dots, n\\}</math> of <math>M</math>, the  '''indicator vector''' of <math>B</math> is\n:<math>\\mathbf e_B := \\sum_{i \\in B} \\mathbf e_i,</math>\nwhere <math>\\mathbf e_i</math> is the standard <math>i</math>th unit vector in <math>\\mathbb{R}^n</math>. The '''matroid polytope''' <math>P_M</math> is the [[convex hull]] of the set\n:<math>\\{\\mathbf e_B \\mid B \\text{ is a basis of } M\\} \\subseteq \\mathbb{R}^n.</math>\n\n==Examples==\n[[Image:Square pyramid.png|thumb|Square pyramid]]\n[[Image:Octahedron.svg|thumb|upright = 0.75|Octahedron]]\n* Let <math>M</math> be the rank 2 matroid on 4 elements with bases\n:: <math>\\mathcal{B}(M) =  \\{\\{1,2\\},\\{1,3\\},\\{1,4\\},\\{2,3\\},\\{2,4\\}\\}.</math>\n:That is, all 2-element subsets of  <math>\\{1,2,3,4\\} </math> except <math>\\{3,4\\} </math>.  The corresponding indicator vectors of <math>\\mathcal{B}(M) </math> are\n:: <math>\\{\\{1,1,0,0\\}, \\{1,0,1,0\\}, \\{1,0,0,1\\}, \\{0,1,1,0\\}, \\{0,1,0,1\\}\\}. </math>\n:The matroid polytope of <math> M </math> is\n: <math> P_M = \\text{conv}\\{\\{1,1,0,0\\}, \\{1,0,1,0\\}, \\{1,0,0,1\\}, \\{0,1,1,0\\}, \\{0,1,0,1\\}\\}. </math>\n:These points form four [[equilateral triangle]]s at point <math>\\{1,1,0,0\\}</math>, therefore its convex hull is the [[square pyramid]] by definition.\n\n* Let <math>N</math> be the rank 2 matroid on 4 elements with bases that are ''all'' 2-element subsets of <math>\\{1,2,3,4\\} </math>.  The corresponding matroid polytope <math>P_N </math> is the [[octahedron]]. Observe that the polytope <math>P_M</math> from the previous example is contained in <math>P_N </math>.\n* If <math>M</math> is the [[uniform matroid]] of rank <math>r </math> on <math> n </math> elements, then the matroid polytope <math>P_M</math> is the [[hypersimplex]] <math>\\Delta_n^r</math>.<ref>{{citation\n | last = Grötschel | first = Martin | authorlink = Martin Grötschel\n | contribution = Cardinality homogeneous set systems, cycles in matroids, and associated polytopes\n | mr = 2077557\n | pages = 99–120\n | publisher = SIAM, Philadelphia, PA\n | series = MPS/SIAM Ser. Optim.\n | title = The Sharpest Cut: The Impact of Manfred Padberg and His Work\n | year = 2004}}. See in particular the remarks following Prop.&nbsp;8.20 on [https://books.google.com/books?id=iXJxerJLyTEC&oi=fnd&pg=PA114 p.&nbsp;114].</ref>\n\n== Properties ==\n* A matroid polytope is contained in the [[hypersimplex]] <math>\\Delta_n^r</math>, where <math>r</math> is the rank of the associated matroid and <math>n</math> is the size of the ground set of the associated matroid.<ref name=\"goresky\">{{cite journal|last1=Gelfand |first1=I.M.|last2=Goresky |first2=R.M. |last3=MacPherson |first3=R.D. |last4=Serganova |first4=V.V.|author4-link= Vera Serganova |year=1987|title=Combinatorial geometries, convex polyhedra, and Schubert cells|journal=Advances in Mathematics|volume=63|issue=3|pages=301&ndash;316|doi=10.1016/0001-8708(87)90059-4 }}</ref>  More precisely, the vertices of <math>P_M</math> are a subset of the vertices of <math>\\Delta_n^r</math>.\n* Every edge of a matroid polytope <math>P_M</math> is a parallel translate of <math>e_i-e_j</math> for some <math>i,j\\in E</math>, the ground set of the associated matroid. In other words, the edges of <math>P_M</math> correspond exactly to the pairs of bases <math>B, B'</math> that satisfy the [[matroid#Definition|basis exchange property]]: <math>B' = B\\setminus{i\\cup j}</math> for some <math>i,j\\in E.</math><ref name=\"goresky\"/> Because of this property, every edge length is the [[square root of two]]. More generally, the families of sets for which the convex hull of indicator vectors has edge lengths one or the square root of two are exactly the [[delta-matroid]]s.\n* Matroid polytopes are members of the family of [[permutohedron|generalized permutohedra]].<ref name=\"volume\">{{cite journal |first1=Federico |last1=Ardila |first2=Carolina|last2=Benedetti|first3=Jeffrey|last3=Doker |title=Matroid polytopes and their volumes |journal=Discrete and Computational Geometry |volume=43 |issue=4 |pages=841–854 |year=2010 |arxiv=0810.3947 |doi=10.1007/s00454-009-9232-9}}</ref>\n* Let  <math> r:2^E \\rightarrow \\mathbb{Z} </math> be the rank function of a matroid <math> M </math>.  The matroid polytope <math> P_M </math> can be written uniquely as a signed [[Minkowski addition|Minkowski sum]] of [[simplex|simplices]]:<ref name=\"volume\"/>\n:: <math> P_M = \\sum_{A\\subseteq E} \\tilde{\\beta}(M/A) \\Delta_{E-A} </math>\n:where <math> E </math> is the ground set of the matroid <math> M </math> and <math> \\beta(M) </math> is the signed beta invariant of <math> M </math>:\n:: <math> \\tilde{\\beta}(M) = (-1)^{r(M)+1}\\beta(M), </math>\n:: <math> \\beta(M) = (-1)^{r(M)} \\sum_{X\\subseteq E} (-1)^{|X|}r(X). </math>\n::<math>P_M:= \\left\\{\\textbf{x}\\in \\mathbb{R}_+^E~|~\\sum_{e\\in U}\\textbf{x}(e)\\leq r(U), \\forall U\\subseteq E\\right\\}</math>\n\n== Related polytopes ==\n\n=== Independence matroid polytope ===\nThe '''matroid independence polytope''' or '''independence matroid polytope''' is the convex hull of the set\n:<math>\\{\\, \\mathbf e_I \\mid I \\text{ is an independent set of } M \\,\\} \\subseteq \\mathbb R^n.</math>\nThe (basis) matroid polytope is a face of the independence matroid polytope. Given the [[Matroid#Rank functions|rank]] <math> \\psi </math> of a matroid <math> M </math>, the independence matroid polytope is equal to the [[polymatroid]] determined by <math> \\psi </math>.\n\n=== Flag matroid polytope ===\nThe flag matroid polytope is another polytope constructed from the bases of matroids. A '''flag''' <math>\\mathcal{F}</math> is a strictly increasing sequence\n\n:<math> F^1 \\subset F^2\\subset \\cdots \\subset F^m \\, </math>\n\nof finite sets.<ref name=\"coxeter\">{{cite journal|last1=Borovik |first1=Alexandre V.|last2= Gelfand |first2=I.M. |last3=White |first3=Neil |title=Coxeter Matroids|journal=Progress in Mathematics|volume=216 |year=2013 |doi=10.1007/978-1-4612-2066-4 }}</ref> Let <math> k_i </math> be the cardinality of the set <math> F_i </math>. Two matroids <math> M </math> and <math> N </math> are said to be '''concordant''' if their rank functions satisfy\n\n: <math> r_M(Y) - r_M(X) \\leq r_N(Y) - r_N(X) \\text{ for all } X\\subset Y \\subseteq E. \\, </math>\n\nGiven pairwise concordant matroids <math> M_1,\\dots,M_m </math> on the ground set <math> E </math> with ranks <math> r_1<\\cdots <r_m </math>, consider the collection of flags <math> (B_1,\\dots, B_m) </math> where <math> B_i </math> is a basis of the matroid <math> M_i </math> and  <math> B_1 \\subset \\cdots\\subset B_m </math>.  Such a collection of flags is a '''flag matroid''' <math> \\mathcal{F} </math>.  The matroids <math> M_1,\\dots,M_m </math> are called the '''constituents''' of <math> \\mathcal{F} </math>.\nFor each flag <math> B=(B_1,\\dots,B_m) </math> in a flag matroid <math> \\mathcal{F} </math>, let <math> v_B</math> be the sum of the indicator vectors of each basis in <math> B </math>\n\n: <math> v_B = v_{B_1}+\\cdots+v_{B_m}. \\, </math>\n\nGiven a flag matroid <math> \\mathcal{F} </math>, the '''flag matroid polytope''' <math> P_\\mathcal{F} </math> is the convex hull of the set\n:<math> \\{v_B \\mid B\\text{ is a flag in }\\mathcal{F}\\}. </math>\nA flag matroid polytope can be written as a Minkowski sum of the (basis) matroid polytopes of the constituent matroids:<ref name=\"coxeter\"/>\n\n: <math> P_\\mathcal{F} = P_{M_1} + \\cdots + P_{M_k}. \\, </math>\n\n== References ==\n<references />\n\n[[Category:Matroid theory|Polytope]]\n[[Category:Polytopes]]"
    },
    {
      "title": "Pregeometry (model theory)",
      "url": "https://en.wikipedia.org/wiki/Pregeometry_%28model_theory%29",
      "text": "'''Pregeometry''', and in full '''combinatorial pregeometry''', are essentially synonyms for \"[[matroid]]\". They were introduced by G.-C. Rota with the intention of providing a less \"ineffably cacophonous\" alternative term.  Also, the term '''combinatorial geometry''', sometimes abbreviated to '''geometry''', was intended to replace \"simple matroid\".  These terms are now infrequently used in the study of matroids.\n\nIn the branch of [[mathematical logic]] called [[model theory]], infinite finitary matroids, there called \"pregeometries\" (and \"geometries\" if they are simple matroids), are used in the discussion of independence phenomena.\n\nIt turns out that many fundamental concepts of [[linear algebra]] – closure, independence, subspace, basis, dimension – are preserved in the framework of abstract geometries.\n\nThe study of how pregeometries, geometries, and abstract [[closure operator]]s influence the structure of first-order models is called [[geometric stability theory]].\n\n== Definitions ==\n\n=== Pregeometries and geometries ===\nA '''combinatorial pregeometry''' (also known as a '''finitary matroid'''), is a second-order structure: <math>(X,\\text{cl})</math>, where <math>\\text{cl}:\\mathcal{P}(X)\\to\\mathcal{P}(X)</math> (called the '''closure map''') satisfies the following axioms. For all <math>a, b\\in X</math> and <math>A, B,C\\subseteq X</math>:\n\n# <math>\\text{cl}:(\\mathcal{P}(X),\\subseteq)\\to(\\mathcal{P}(X),\\subseteq)</math> is a [[homomorphism]] in the category of [[partial order]]s ('''monotone increasing'''), and '''dominates''' <math>\\text{id}</math> (''I.e.'' <math>A\\subseteq B</math> implies <math>A\\subseteq\\text{cl}(A)\\subseteq\\text{cl}(B)</math>.) and is '''[[idempotent]]'''.\n# '''Finite character''': For each <math>a\\in\\text{cl}(A)</math> there is some finite <math>F\\subseteq A</math> with <math>a\\in\\text{cl}(F)</math>.\n# '''Exchange principle''': If <math>b\\in\\text{cl}(C\\cup\\{a\\})\\smallsetminus\\text{cl}(C)</math>, then <math>a\\in\\text{cl}(C\\cup\\{b\\}) </math> (and hence by monotonicity and idempotence in fact <math>a\\in\\text{cl}(C\\cup\\{b\\})\\smallsetminus\\text{cl}(C)</math>).\n\nA '''geometry''' is a pregeometry in which the closure of singletons are singletons and the closure of the empty set is the empty set.\n\n=== Independence, bases and dimension ===\n\nGiven sets <math>A,B\\subset S</math>, <math>A</math> is '''independent over''' <math>B</math> if <math>a\\notin \\text{cl}((A\\setminus\\{a\\})\\cup B)</math> for any <math>a\\in A</math>.\n\nA set <math>A_0 \\subset A</math> is a '''basis for''' <math>A</math> '''over''' <math>B</math> if it is independent over <math>B</math> and <math>A\\subset \\text{cl}(A_0\\cup B)</math>.\n\nSince a pregeometry satisfies the Steinitz exchange property all bases are of the same cardinality, hence the definition of the '''dimension''' of <math>A</math> over <math>B</math> as <math>\\text{dim}_B A = |A_0|</math> has no ambiguity.\n\nThe sets <math>A,B</math> are independent over <math>C</math> if <math>\\text{dim}_{B\\cup C} = \\dim_C A'</math>{{Inconsistent}} whenever <math>A'</math> is a finite subset of <math>A</math>. Note that this relation is symmetric.\n\nIn minimal sets over stable theories the independence relation coincides with the notion of forking independence.\n\n=== Geometry automorphism ===\n\nA '''geometry automorphism''' of a geometry <math>S</math> is a bijection <math>\\sigma:2^S\\to 2^S</math> such that <math>\\sigma\\text{cl}(X)=\\text{cl}(\\sigma X)</math> for any <math>X\\subset S</math>.\n\nA pregeometry <math>S</math> is said to be '''homogeneous''' if for any closed <math>X\\subset S</math> and any two elements <math>a,b\\in S\\setminus X</math> there is an automorphism of <math>S</math> which maps <math>a</math> to <math>b</math> and fixes <math>X</math> pointwise.\n\n=== The associated geometry and localizations ===\n\nGiven a pregeometry <math>(S,\\text{cl})</math> its '''associated geometry''' (sometimes referred in the literature as the '''canonical geometry''') is the geometry <math>(S',\\text{cl}')</math> where\n\n#<math>S'=\\{\\text{cl}(a)\\mid a\\in S\\setminus \\text{cl} (\\emptyset)\\}</math>, and\n#For any <math>X\\subset S</math>, <math>\\text{cl}'(\\{\\text{cl}(a)\\mid a\\in X\\}) = \\{\\text{cl}(b)\\mid b\\in\\text{cl}(X)\\}</math>\n\nIts easy to see that the associated geometry of a homogeneous pregeometry is homogeneous.\n\nGiven <math>A\\subset S</math> the '''localization''' of <math>S</math> is the geometry <math>(S,\\text{cl}_A)</math> where <math>\\text{cl}_A(X)=\\text{cl}(X\\cup A)</math>.\n\n=== Types of pregeometries ===\n\nLet <math>(S,\\text{cl})</math> be a pregeometry, then it is said to be:\n\n* '''trivial''' (or '''degenerate''') if <math>\\text{cl}(X)=\\bigcup\\{\\text{cl}(a)\\mid a\\in X\\}</math>.\n* '''modular''' if any two closed finite dimensional sets <math>X,Y\\subset S</math> satisfy the equation <math>\n\\text{dim}(X\\cup Y) = \\text{dim}(X) + \\text{dim}(Y) - \\text{dim}(X\\cap Y)\n</math> (or equivalently that <math>X</math> is independent of <math>Y</math> over <math>X\\cap Y</math>).\n* '''locally modular''' if it has a localization at a singleton which is modular.\n* (locally) '''projective''' if it is non-trivial and (locally) modular.\n* '''locally finite''' if closures of finite sets are finite.\n\nTriviality, modularity and local modularity pass to the associated geometry and are preserved under localization.\n\nIf <math>S</math> is a locally modular homogeneous pregeometry and <math>a\\in S\\setminus\\text{cl}\\emptyset</math> then the localization of <math>S</math> in <math>b</math> is modular.\n\nThe geometry <math>S</math> is modular if and only if whenever <math>a,b\\in S</math>, <math>A\\subset S</math>, <math>\\text{dim}\\{a,b\\}=2</math> and <math>\\text{dim}_A\\{a,b\\} \\le 1</math> then <math>(\\text{cl}\\{a,b\\}\\cap\\text{cl}(A))\\setminus\\text{cl}\\emptyset\\ne\\emptyset</math>.\n\n== Examples ==\n\n=== The trivial example ===\n\nIf <math>S</math> is any set we may define <math>\\text{cl}(A)=A</math>. This pregeometry is a trivial, homogeneous, locally finite geometry.\n\n=== Vector spaces and projective spaces ===\n\nLet <math>F</math> be a field (a division ring actually suffices) and let <math>V</math> be a <math>\\kappa</math>-dimensional vector space over <math>F</math>. Then <math>V</math> is a pregeometry where closures of sets are defined to be their span.\n\nThis pregeometry is homogeneous and modular. Vector spaces are considered to be the prototypical example of modularity.\n\n<math>V</math> is locally finite if and only if <math>F</math> is finite.\n\n<math>V</math> is not a geometry, as the closure of any nontrivial vector is a subspace of size at least <math>2</math>.\n\nThe associated geometry of a <math>\\kappa</math>-dimensional vector space over <math>F</math> is the <math>(\\kappa-1)</math>-dimensional [[projective space]] over <math>F</math>. It is easy to see that this pregeometry is a projective geometry.\n\n=== Affine spaces ===\n\nLet <math>V</math> be a <math>\\kappa</math>-dimensional [[affine space]] over a field <math>F</math>. Given a set define its closure to be its [[affine hull]] (i.e. the smallest affine subspace containing it).\n\nThis forms a homogeneous <math>(\\kappa+1)</math>-dimensional geometry.\n\nAn affine space is not modular (for example, if <math>X</math> and <math>Y</math> be parallel lines then the formula in the definition of modularity fails). However, it is easy to check that all localizations are modular.\n\n=== Algebraically closed fields ===\n\nLet <math>k</math> be an algebraically closed field with <math>\\text{tr.deg}(k)\\ge\\omega</math>, and define the closure of a set to be its algebraic closure.\n\nWhile vector spaces are modular and affine spaces are \"almost\" modular (i.e. everywhere locally modular), algebraically closed fields are examples of the other extremity, not being even locally modular (i.e. none of the localizations is modular).\n\n==References==\n\nH.H. Crapo and G.-C. Rota (1970),  ''On the Foundations of Combinatorial Theory: Combinatorial Geometries''.  M.I.T. Press, Cambridge, Mass.\n\nPillay, Anand (1996),  ''Geometric Stability Theory''. Oxford Logic Guides.  Oxford University Press.\n\n[[Category:Matroid theory]]\n[[Category:Geometry|*]]\n[[Category:Model theory]]"
    },
    {
      "title": "Pseudoforest",
      "url": "https://en.wikipedia.org/wiki/Pseudoforest",
      "text": "[[File:Pseudoforest.svg|thumb|upright=1.2|A 1-forest (a maximal pseudoforest), formed by three 1-trees]]\nIn [[graph theory]], a '''pseudoforest''' is an [[undirected graph]]<ref name=multigraph>The kind of undirected graph considered here is often called a [[multigraph]] or pseudograph, to distinguish it from a [[simple graph]].</ref> in which every [[Connected component (graph theory)|connected component]] has at most one [[Cycle (graph theory)|cycle]]. That is, it is a system of [[Vertex (graph theory)|vertices]] and [[Edge (graph theory)|edges]] connecting pairs of vertices, such that no two  cycles of consecutive edges share any vertex with each other, nor can any two cycles be connected to each other by a path of consecutive edges.  A '''pseudotree''' is a connected pseudoforest.\n\nThe names are justified by analogy to the more commonly studied [[Tree (graph theory)|trees]] and [[Forest (graph theory)|forests]].  (A tree is a connected graph with no cycles; a forest is a disjoint union of trees.)  Gabow and Tarjan<ref name=\"gt\">{{harvtxt|Gabow|Tarjan|1988}}.</ref> attribute the study of pseudoforests to Dantzig's 1963 book on [[linear programming]], in which pseudoforests arise in the solution of certain [[Flow network|network flow]] problems.<ref name=\"dantzig\">{{harvtxt|Dantzig|1963}}.</ref> Pseudoforests also form graph-theoretic models of functions and occur in several [[algorithm]]ic problems. Pseudoforests are [[sparse graph]]s – their number of edges is linearly bounded in terms of their number of vertices (in fact, they have at most as many edges as they have vertices) – and their [[matroid]] structure allows several other families of sparse graphs to be decomposed as unions of forests and pseudoforests. The name \"pseudoforest\" comes from {{harvtxt|Picard|Queyranne|1982}}.\n\n==Definitions and structure==\nWe define an undirected graph to be a set of [[vertex (graph theory)|vertices]] and [[edge (graph theory)|edges]] such that each edge has two vertices (which may coincide) as endpoints.  That is, we allow multiple edges (edges with the same pair of endpoints) and loops (edges whose two endpoints are the same vertex).<ref name=multigraph/>  A [[Glossary of graph theory#Subgraphs|subgraph]] of a graph is the graph formed by any subsets of its vertices and edges such that each edge in the edge subset has both endpoints in the vertex subset.\nA [[connected component (graph theory)|connected component]] of an undirected graph is the subgraph consisting of the vertices and edges that can be reached by following edges from a single given starting vertex. A graph is connected if every vertex or edge is reachable from every other vertex or edge. A [[cycle (graph theory)|cycle]] in an undirected graph is a connected subgraph in which each vertex is incident to exactly two edges, or is a loop.<ref>See the linked articles and the references therein for these definitions.</ref>\n\n[[File:The21.GIF|thumb|The 21 unicyclic graphs with at most six vertices]]\nA pseudoforest is an undirected graph in which each connected component contains at most one cycle.<ref>This is the definition used, e.g., by {{harvtxt|Gabow|Westermann|1992}}.</ref>  Equivalently, it is an undirected graph in which each connected component has no more edges than vertices.<ref>This is the definition in {{harvtxt|Gabow|Tarjan|1988}}.</ref>  The components that have no cycles are just [[tree (graph theory)|trees]], while the components that have a single cycle within them are called '''1-trees''' or '''unicyclic graphs'''.  That is, a 1-tree is a connected graph containing exactly one cycle.  A pseudoforest with a single connected component (usually called a '''pseudotree''', although some authors define a pseudotree to be a 1-tree) is either a tree or a 1-tree; in general a pseudoforest may have multiple connected components as long as all of them are trees or 1-trees.\n\nIf one removes from a 1-tree one of the edges in its cycle, the result is a tree. Reversing this process, if one augments a tree by connecting any two of its vertices by a new edge, the result is a 1-tree; the path in the tree connecting the two endpoints of the added edge, together with the added edge itself, form the 1-tree's unique cycle. If one augments a 1-tree by adding an edge that connects one of its vertices to a newly added vertex, the result is again a 1-tree, with one more vertex; an alternative method for constructing 1-trees is to start with a single cycle and then repeat this augmentation operation any number of times. The edges of any 1-tree can be partitioned in a unique way into two subgraphs, one of which is a cycle and the other of which is a forest, such that each tree of the forest contains exactly one vertex of the cycle.<ref>See, e.g., the proof of Lemma 4 in {{harvtxt|Àlvarez|Blesa|Serna|2002}}.</ref>\n\nCertain more specific types of pseudoforests have also been studied.\n:A '''1-forest''', sometimes called a '''maximal pseudoforest''', is a pseudoforest to which no more edges can be added without causing some component of the graph to contain multiple cycles. If a pseudoforest contains a tree as one of its components, it cannot be a 1-forest, for one can add either an edge connecting two vertices within that tree, forming a single cycle, or an edge connecting that tree to some other component. Thus, the 1-forests are exactly the pseudoforests in which every component is a 1-tree.\n\n:The '''spanning pseudoforests''' of an undirected graph ''G'' are the pseudoforest [[Glossary of graph theory#Subgraphs|subgraphs]] of ''G'' that have all the vertices of ''G''.  Such a pseudoforest need not have any edges, since for example the subgraph that has all the vertices of ''G'' and no edges is a pseudoforest (whose components are trees consisting of a single vertex).\n\n:The '''maximal pseudoforests of''' ''G'' are the pseudoforest subgraphs of ''G'' that are not contained within any larger  pseudoforest of ''G''.  A maximal pseudoforest of ''G'' is always a spanning pseudoforest, but not conversely.  If ''G'' has no connected components that are trees, then its maximal pseudoforests are 1-forests, but if ''G'' does have a tree component, its maximal pseudoforests are not 1-forests.  Stated precisely, in any graph ''G'' its maximal pseudoforests consist of every tree component of ''G'', together with one or more disjoint 1-trees covering the remaining vertices of ''G''.\n\n==Directed pseudoforests==\n\nVersions of these definitions are also used for [[directed graph]]s. Like an undirected graph, a directed graph consists of vertices and edges, but each edge is directed from one of its endpoints to the other endpoint. A '''directed pseudoforest''' is a directed graph in which each vertex has at most one outgoing edge; that is, it has [[outdegree]] at most one. A '''directed 1-forest''' &ndash; most commonly called a '''functional graph''' (see [[#Graphs of functions|below]]), sometimes '''maximal directed pseudoforest''' &ndash; is a directed graph in which each vertex has outdegree exactly one.<ref>{{harvtxt|Kruskal|Rudolph|Snir|1990}} instead use the opposite definition, in which each vertex has indegree one; the resulting graphs, which they call ''unicycular'', are the [[transpose graph|transposes]] of the graphs considered here.</ref> If ''D'' is a directed pseudoforest, the undirected graph formed by removing the direction from each edge of ''D'' is an undirected pseudoforest.\n\n==Number of edges==\nEvery pseudoforest on a set of ''n'' vertices has at most ''n'' edges, and every maximal pseudoforest on a set of ''n'' vertices has exactly ''n'' edges. Conversely, if a graph ''G'' has the property that, for every subset ''S'' of its vertices, the number of edges in the [[induced subgraph]] of ''S'' is at most the number of vertices in ''S'', then ''G'' is a pseudoforest.  1-trees can be defined as connected graphs with equally many vertices and edges.<ref name=\"gt\"/>\n\nMoving from individual graphs to graph families, if a family of graphs has the property that every subgraph of a graph in the family is also in the family, and every graph in the family has at most as many edges as vertices, then the family contains only pseudoforests. For instance, every subgraph of a [[thrackle]] (a graph [[graph drawing|drawn]] so that every pair of edges has one point of intersection) is also a thrackle, so [[Conway's thrackle conjecture|Conway's conjecture]] that every thrackle has at most as many edges as vertices can be restated as saying that every thrackle is a pseudoforest. A more precise characterization is that, if the conjecture is true, then the thrackles are exactly the pseudoforests with no four-vertex cycle and at most one odd cycle.<ref>{{harvtxt|Woodall|1969}}; {{harvtxt|Lovász|Pach|Szegedy|1997}}.</ref>\n\nStreinu and Theran<ref name=\"st\">{{harvtxt|Streinu|Theran|2009}}.</ref> generalize the [[sparse graph|sparsity]] conditions defining pseudoforests: they define a graph as being (''k'',''l'')-sparse if every nonempty subgraph with ''n'' vertices has at most ''kn''&nbsp;&minus;&nbsp;''l'' edges, and (''k'',''l'')-tight if it is (''k'',''l'')-sparse and has exactly ''kn''&nbsp;&minus;&nbsp;''l'' edges. Thus, the pseudoforests are the (1,0)-sparse graphs, and the maximal pseudoforests are the (1,0)-tight graphs. Several other important families of graphs may be defined from other values of ''k'' and ''l'',\nand when ''l''&nbsp;≤&nbsp;''k'' the (''k'',''l'')-sparse graphs may be characterized as the graphs formed as the edge-disjoint union of ''l'' forests and ''k''&nbsp;&minus;&nbsp;''l'' pseudoforests.<ref>{{harvtxt|Whiteley|1988}}.</ref>\n\nAlmost every sufficiently sparse [[random graph]] is pseudoforest.<ref name=\"rg\">{{harvtxt|Bollobás|1985}}. See especially Corollary 24, p.120, for a bound on the number of vertices belonging to unicyclic components in a random graph, and Corollary 19, p.113, for a bound on the number of distinct labeled unicyclic graphs.</ref> That is, if ''c'' is a constant with 0 &lt; ''c'' &lt; 1/2, and P<sub>''c''</sub>(''n'') is the probability that choosing uniformly at random among the ''n''-vertex graphs with ''cn'' edges results in a pseudoforest, then P<sub>''c''</sub>(''n'') tends to one in the limit for large ''n''. However, for ''c'' &gt; 1/2, almost every random graph with ''cn'' edges has a large component that is not unicyclic.\n\n==Enumeration==\n\nA graph is ''simple'' if it has no self-loops and no multiple edges with the same endpoints.  The number of simple 1-trees with ''n'' labelled vertices is<ref>{{harvtxt|Riddell|1951}}; see {{OEIS2C|A057500}} in the [[On-Line Encyclopedia of Integer Sequences]].</ref>\n:<math>n \\sum_{k=1}^n \\frac{(-1)^{k-1}}{k} \\sum_{n_1+\\cdots+n_k=n} \\frac{n!}{n_1! \\cdots n_k!} \\binom{\\binom{n_1}{2}+\\cdots +\\binom{n_k}{2}}{n}.</math>\nThe values for ''n'' up to 300 can be found in sequence {{OEIS2C|A057500}} of the [[On-Line Encyclopedia of Integer Sequences]].\n\nThe number of maximal directed pseudoforests on ''n'' vertices, allowing self-loops, is ''n<sup>n</sup>'', because for each vertex there are ''n'' possible endpoints for the outgoing edge. [[André Joyal]] used this fact to provide a [[bijective proof]] of [[Cayley's formula]], that the number of undirected trees on ''n'' nodes is ''n''<sup>''n''&nbsp;&minus;&nbsp;2</sup>, by finding a bijection between maximal directed pseudoforests and undirected trees with two distinguished nodes.<ref>{{harvtxt|Aigner|Ziegler|1998}}.</ref> If self-loops are not allowed, the number of maximal directed pseudoforests is instead (''n''&nbsp;&minus;&nbsp;1)<sup>''n''</sup>.\n\n==Graphs of functions==\n{{redirects here|Functional graph|other uses|Graph of a function}}\n[[Image:Functional graph.svg|thumb|upright=1.5|A function from the set {0,1,2,3,4,5,6,7,8} to itself, and the corresponding functional graph]]\nDirected pseudoforests and [[endofunction]]s are in some sense mathematically equivalent. Any function ƒ from a set ''X'' to itself (that is, an [[endomorphism]] of ''X'') can be interpreted as defining a directed pseudoforest which has an edge from ''x'' to ''y'' whenever ƒ(''x'') = ''y''. The resulting directed pseudoforest is maximal, and may include [[Loop (graph theory)|self-loops]] whenever some value ''x'' has ƒ(''x'') = ''x''. Alternatively, omitting the self-loops produces a non-maximal pseudoforest. In the other direction, any maximal directed pseudoforest determines a function ƒ such that ƒ(''x'') is the target of the edge that goes out from ''x'', and any non-maximal directed pseudoforest can be made maximal by adding self-loops and then converted into a function in the same way. For this reason, maximal directed pseudoforests are sometimes called '''functional graphs'''.<ref name=\"gt\"/> Viewing a function as a functional graph provides a convenient language for describing properties that are not as easily described from the function-theoretic point of view; this technique is especially applicable to problems involving [[iterated function]]s, which correspond to [[path (graph theory)|paths]] in functional graphs.\n\n[[Cycle detection]], the problem of following a path in a functional graph to find a cycle in it, has applications in [[cryptography]] and [[computational number theory]], as part of [[Pollard's rho algorithm]] for [[integer factorization]] and as a method for finding collisions in [[cryptographic hash function]]s. In these applications, ƒ is expected to behave randomly; [[Philippe Flajolet|Flajolet]] and [[Andrew Odlyzko|Odlyzko]]<ref>{{harvtxt|Flajolet|Odlyzko|1990}}.</ref> study the graph-theoretic properties of the functional graphs arising from randomly chosen mappings. In particular, a form of the [[birthday paradox]] implies that, in a random functional graph with ''n'' vertices, the path starting from a randomly selected vertex will typically loop back on itself to form a cycle within O({{radic|''n''}}) steps. [[Sergei Konyagin|Konyagin]] et al. have made analytical and computational progress on graph statistics.<ref>{{harvtxt|Konyagin|Luca|Mans|Mathieson|2010}}.</ref>\n\nMartin, [[Andrew Odlyzko|Odlyzko]], and [[Stephen Wolfram|Wolfram]]<ref>{{harvtxt|Martin|Odlyzko|Wolfram|1984}}.</ref> investigate pseudoforests that model the dynamics of [[cellular automaton|cellular automata]]. These functional graphs, which they call ''state transition diagrams'', have one vertex for each possible configuration that the ensemble of cells of the automaton can be in, and an edge connecting each configuration to the configuration that follows it according to the automaton's rule. One can infer properties of the automaton from the structure of these diagrams, such as the number of components, length of limiting cycles, depth of the trees connecting non-limiting states to these cycles, or symmetries of the diagram. For instance, any vertex with no incoming edge corresponds to a [[Garden of Eden pattern]] and a vertex with a self-loop corresponds to a [[Still life (cellular automaton)|still life pattern]].\n\nAnother early application of functional graphs is in the ''trains'' used to study [[Steiner system|Steiner triple system]]s.<ref>{{harvtxt|White|1913}}; {{harvtxt|Colbourn|Colbourn|Rosenbaum|1982}}; {{harvtxt|Stinson|1983}}.</ref> The train of a triple system is a functional graph having a vertex for each possible triple of symbols; each triple ''pqr'' is mapped by ƒ to ''stu'', where ''pqs'', ''prt'', and ''qru'' are the triples that belong to the triple system and contain the pairs ''pq'', ''pr'', and ''qr'' respectively. Trains have been shown to be a powerful invariant of triple systems although somewhat cumbersome to compute.\n\n==Bicircular matroid==\nA [[matroid]] is a mathematical structure in which certain sets of elements are defined to be [[independence system|independent]], in such a way that the independent sets satisfy properties modeled after the properties of [[linear independence]] in a [[vector space]]. One of the standard examples of a matroid is the [[graphic matroid]] in which the independent sets are the sets of edges in forests of a graph; the matroid structure of forests is important in algorithms for computing the [[minimum spanning tree]] of the graph. Analogously, we may define matroids from pseudoforests.\n\nFor any graph ''G'' = (''V'',''E''), we may define a matroid on the edges of ''G'', in which a set of edges is independent if and only if it forms a pseudoforest; this matroid is known as the '''[[bicircular matroid]]''' (or '''bicycle matroid''') of ''G''.<ref>{{harvtxt|Simoes-Pereira|1972}}.</ref><ref>{{harvtxt|Matthews|1977}}.</ref> The smallest dependent sets for this matroid are the minimal connected subgraphs of ''G'' that have more than one cycle, and these subgraphs are sometimes called bicycles. There are three possible types of bicycle: a [[Glossary of graph theory#Walks|theta graph]] has two vertices that are connected by three internally disjoint paths, a figure 8 graph consists of two cycles sharing a single vertex, and a handcuff graph is formed by two disjoint cycles connected by a path.<ref>[http://www.math.binghamton.edu/zaslav/Bsg/glossary.html Glossary of Signed and Gain Graphs and Allied Areas]</ref>\nA graph is a pseudoforest if and only if it does not contain a bicycle as a subgraph.\n<ref name=\"st\"/>\n\n==Forbidden minors==\n[[Image:Butterfly and diamond graphs.svg|thumb|The [[butterfly graph]] (left) and [[diamond graph]] (right), forbidden [[graph minor|minors]] for pseudoforests]]\nForming a [[Minor (graph theory)|minor]] of a pseudoforest by contracting some of its edges and deleting others produces another pseudoforest. Therefore, the family of pseudoforests is [[Closure (mathematics)|closed]] under minors, and the [[Robertson–Seymour theorem]] implies that pseudoforests can be characterized in terms of a finite set of [[forbidden minor]]s, analogously to [[Wagner's theorem]] characterizing the [[planar graph]]s as the graphs having neither the [[complete graph]] K<sub>5</sub> nor the [[complete bipartite graph]] K<sub>3,3</sub> as minors.\nAs discussed above, any non-pseudoforest graph contains as a subgraph a handcuff, figure 8, or theta graph; any handcuff or figure 8 graph may be contracted to form a ''[[butterfly graph]]'' (five-vertex figure 8), and any theta graph may be contracted to form a ''[[diamond graph]]'' (four-vertex theta graph),<ref>For this terminology, see the [http://www.graphclasses.org/smallgraphs.html list of small graphs] from the [http://www.graphclasses.org/ Information System on Graph Class Inclusions]. However, ''butterfly graph'' may also refer to a different family of graphs related to [[hypercube graph|hypercubes]], and the five-vertex figure 8 is sometimes instead called a ''bowtie graph''.</ref> so any non-pseudoforest contains either a butterfly or a diamond as a minor, and these are the only minor-minimal non-pseudoforest graphs. Thus, a graph is a pseudoforest if and only if it does not have the butterfly or the diamond as a minor. If one forbids only the diamond but not the butterfly, the resulting larger graph family consists of the [[cactus graph]]s and disjoint unions of multiple cactus graphs.<ref>{{harvtxt|El-Mallah|Colbourn|1988}}.</ref>\n\nMore simply, if [[multigraph]]s with [[self-loop]]s are considered, there is only one forbidden minor, a vertex with two loops.\n\n==Algorithms==\nAn early algorithmic use of pseudoforests involves the ''network simplex'' algorithm and its application to generalized flow problems modeling the conversion between [[commodity|commodities]] of different types.<ref name=\"dantzig\"/><ref name=\"amo\">{{harvtxt|Ahuja|Magnanti|Orlin|1993}}.</ref> In these problems, one is given as input a [[flow network]] in which the vertices model each commodity and the edges model allowable conversions between one commodity and another. Each edge is marked with a ''capacity'' (how much of a commodity can be converted per unit time), a ''flow multiplier'' (the conversion rate between commodities), and a ''cost'' (how much loss or, if negative, profit is incurred per unit of conversion). The task is to determine how much of each commodity to convert via each edge of the flow network, in order to minimize cost or maximize profit, while obeying the capacity constraints and not allowing commodities of any type to accumulate unused. This type of problem can be formulated as a [[linear program]], and solved using the [[simplex algorithm]]. The intermediate solutions arising from this algorithm, as well as the eventual optimal solution, have a special structure: each edge in the input network is either unused or used to its full capacity, except for a subset of the edges, forming a spanning pseudoforest of the input network, for which the flow amounts may lie between zero and the full capacity. In this application, unicyclic graphs are also sometimes called ''augmented trees'' and maximal pseudoforests are also sometimes called ''augmented forests''.<ref name=\"amo\"/>\n\nThe ''minimum spanning pseudoforest problem'' involves finding a spanning pseudoforest of minimum weight in a larger edge-weighted graph ''G''.\nDue to the matroid structure of pseudoforests, minimum-weight maximal pseudoforests may be found by [[greedy algorithm]]s similar to those for the [[minimum spanning tree]] problem. However, Gabow and Tarjan found a more efficient linear-time approach in this case.<ref name=\"gt\"/>\n\nThe '''pseudoarboricity''' of a graph ''G'' is defined by analogy to the [[arboricity]] as the minimum number of pseudoforests into which its edges can be partitioned; equivalently, it is the minimum ''k'' such that ''G'' is (''k'',0)-sparse, or the minimum ''k'' such that the edges of ''G'' can be oriented to form a directed graph with outdegree at most ''k''. Due to the matroid structure of pseudoforests, the pseudoarboricity may be computed in polynomial time.<ref>{{harvtxt|Gabow|Westermann|1992}}. See also the faster approximation schemes of {{harvtxt|Kowalik|2006}}.</ref>\n\nA [[random graph|random]] [[bipartite graph]] with ''n'' vertices on each side of its bipartition, and with ''cn'' edges chosen independently at random from each of the ''n''<sup>2</sup> possible pairs of vertices, is a pseudoforest with high probability whenever ''c'' is a constant strictly less than one. This fact plays a key role in the analysis of [[cuckoo hashing]], a data structure for looking up key-value pairs by looking in one of two hash tables at locations determined from the key: one can form a graph, the \"cuckoo graph\", whose vertices correspond to hash table locations and whose edges link the two locations at which one of the keys might be found, and the cuckoo hashing algorithm succeeds in finding locations for all of its keys if and only if the cuckoo graph is a pseudoforest.<ref>{{harvtxt|Kutzelnigg|2006}}.</ref>\n\nPseudoforests also play a key role in [[parallel algorithm]]s for [[graph coloring]] and related problems.<ref>{{harvtxt|Goldberg|Plotkin|Shannon|1988}}; {{harvtxt|Kruskal|Rudolph|Snir|1990}}.</ref>\n\n==Notes==\n{{reflist|2}}\n\n==References==\n{{refbegin|2}}\n*{{citation | first1=Ravindra K. | last1=Ahuja | author1-link = Ravindra K. Ahuja | first2 = Thomas L. | last2 = Magnanti | author2-link = Thomas L. Magnanti | first3 = James B. | last3 = Orlin | author3-link = James B. Orlin | title= Network Flows: Theory, Algorithms and Applications | publisher=Prentice Hall | year=1993 | isbn=0-13-617549-X}}.\n*{{citation\n | last1 = Aigner | first1 = Martin | author1-link = Martin Aigner\n | last2 = Ziegler | first2 = Günter M. | author2-link = Günter M. Ziegler\n | pages = 141–146\n | publisher = [[Springer-Verlag]]\n | title = [[Proofs from THE BOOK]]\n | year = 1998}}.\n*{{citation|last1=Àlvarez|first1=Carme|last2=Blesa|first2=Maria|last3=Serna|first3=Maria|contribution=Universal stability of undirected graphs in the adversarial queueing model|title=Proc. 14th ACM [[Symposium on Parallel Algorithms and Architectures]]|year=2002|pages=183–197|doi=10.1145/564870.564903}}.\n*{{citation|first=Béla|last=Bollobás|authorlink=Béla Bollobás|title=Random Graphs|publisher=Academic Press|year=1985}}.\n*{{citation|last1=Colbourn|first1=Marlene J.|last2=Colbourn|first2=Charles J.|author2-link = Charles Colbourn|last3=Rosenbaum|first3=Wilf L.|title=Trains: an invariant for Steiner triple systems|journal=[[Ars Combinatoria (journal)|Ars Combinatoria]]|volume=13|year=1982|pages=149–162|mr=0666934}}.\n*{{citation|first=G. B.|last=Dantzig|authorlink=George Dantzig|title=Linear Programming and Extensions|publisher=Princeton University Press|year=1963}}.\n*{{citation|last1=El-Mallah|first1=Ehab|last2=Colbourn|first2=Charles J.|author2-link=Charles Colbourn|title=The complexity of some edge deletion problems|journal=IEEE Transactions on Circuits and Systems|volume=35|issue=3|year=1988|pages=354–362|doi=10.1109/31.1748}}.\n*{{citation|first1=P.|last1=Flajolet|authorlink1=Philippe Flajolet|first2=A.|last2=Odlyzko|authorlink2=Andrew Odlyzko|contribution=Random mapping statistics|publisher=Springer-Verlag|series=Lecture Notes in Computer Science|title=Advances in Cryptology – EUROCRYPT '89: [[Workshop on the Theory and Application of Cryptographic Techniques]]|volume=434|pages=329–354|year=1990}}.\n*{{citation|first1=H. N.|last1=Gabow|first2=R. E.|last2=Tarjan|authorlink2=Robert Tarjan|title=A linear-time algorithm for finding a minimum spanning pseudoforest|journal=Information Processing Letters|volume=27|year=1988|issue=5|pages=259–263|doi=10.1016/0020-0190(88)90089-0}}.\n*{{citation|first1=H. N.|last1=Gabow|first2=H. H.|last2=Westermann|title=Forests, frames, and games: Algorithms for matroid sums and applications|journal=Algorithmica|volume=7|issue=1|year=1992|pages=465–497|doi=10.1007/BF01758774}}.\n*{{citation|first1=A. V.|last1=Goldberg|author1-link=Andrew V. Goldberg|first2=S. A.|last2=Plotkin|first3=G. E.|last3=Shannon|title=Parallel symmetry-breaking in sparse graphs|journal=[[SIAM Journal on Discrete Mathematics]]|volume=1|issue=4|year=1988|pages=434–446|doi=10.1137/0401044}}.\n*{{citation\n|first1=Sergei\n|last1=Konyagin\n|first2 = Florian\n|last2 = Luca\n|first3 = Bernard\n|last3 = Mans\n|first4 = Luke\n|last4 = Mathieson\n|first5 = Igor E.\n|last5 = Shparlinski\n|title = Functional Graphs of Polynomials over Finite Fields\n|year = 2010\n}}\n*{{citation|first=Ł.|last=Kowalik|contribution=Approximation Scheme for Lowest Outdegree Orientation and Graph Density Measures|publisher=Springer-Verlag|editor1-last=Asano|editor1-link=Tetsuo Asano|series=Lecture Notes in Computer Science|editor1-first=Tetsuo|volume=4288|title=Proceedings of the International Symposium on Algorithms and Computation|year=2006|pages=557–566|doi=10.1007/11940128}}.\n*{{citation|first1=Clyde P.|last1=Kruskal|authorlink1=Clyde Kruskal|first2=Larry|last2=Rudolph|first3=Marc|last3=Snir|title=Efficient parallel algorithms for graph problems|journal=Algorithmica|year=1990|volume=5|issue=1|pages=43–64|doi=10.1007/BF01840376}}.\n*{{citation\n | last1 = Picard | first1 = Jean-Claude\n | last2 = Queyranne | first2 = Maurice\n | doi = 10.1002/net.3230120206\n | issue = 2\n | journal = Networks\n | mr = 670021\n | pages = 141–159\n | title = A network flow solution to some nonlinear 0–1 programming problems, with applications to graph theory\n | volume = 12\n | year = 1982}}.\n*{{citation|first=Reinhard|last=Kutzelnigg|contribution-url=https://dmtcs.episciences.org/3486|contribution=Bipartite random graphs and cuckoo hashing|title=Fourth Colloquium on Mathematics and Computer Science|series=Discrete Mathematics and Theoretical Computer Science|year=2006|volume=AG|pages=403–406}}.\n*{{citation|first1=L.|last1=Lovász|authorlink1=László Lovász|first2=J.|last2=Pach|first3=M.|last3=Szegedy|authorlink3=Mario Szegedy|title=On Conway's thrackle conjecture|journal=[[Discrete and Computational Geometry]]|volume=18|issue=4|year=1997|pages=369–376|doi=10.1007/PL00009322}}.\n*{{citation|first1=O.|last1=Martin|first2=A. M.|last2=Odlyzko|authorlink2=Andrew Odlyzko|first3=S.|last3=Wolfram|authorlink3=Stephen Wolfram|title=Algebraic properties of cellular automata|journal=Communications in Mathematical Physics|volume=93|issue=2|year=1984|pages=219–258|doi=10.1007/BF01223745|url=http://www.stephenwolfram.com/publications/articles/mathematics/84-properties/|bibcode = 1984CMaPh..93..219M }}.\n*{{citation|first=L. R.|last=Matthews|title=Bicircular matroids|journal=The Quarterly Journal of Mathematics. Oxford. Second Series|volume=28|year=1977|issue=110|pages=213–227|mr=0505702|doi=10.1093/qmath/28.2.213}}.\n*{{citation|first=R. J.|last=Riddell|title=Contributions to the Theory of Condensation|series=Ph.D. thesis|publisher=University of Michigan|place=Ann Arbor|year=1951|bibcode=1951PhDT........20R}}.\n*{{citation|first=J. M. S.|last=Simoes-Pereira|title=On subgraphs as matroid cells|journal=[[Mathematische Zeitschrift]]|volume=127|year=1972|issue=4|pages=315–322|doi=10.1007/BF01111390}}.\n*{{citation|last=Stinson|first=D. R.|title=A comparison of two invariants for Steiner triple systems: fragments and trains|journal=Ars Combinatoria|volume=16|year=1983|pages=69–76|doi=|mr=0734047}}.\n*{{citation\n| doi = 10.1007/s00373-008-0834-4 \n| title = Sparsity-certifying Graph Decompositions \n| year = 2009 \n| last1 = Streinu | first1 = I. | author1-link = Ileana Streinu\n| last2 = Theran | first2 = L. \n| journal = [[Graphs and Combinatorics]]\n| volume = 25\n| issue = 2 \n| pages = 219\n| arxiv = 0704.0002}}.\n*{{citation|last=White|first=H. S.|title=Triple-systems as transformations, and their paths among triads|journal=[[Transactions of the American Mathematical Society]]|year=1913|volume=14|issue=1|pages=6–13|doi=10.2307/1988765|jstor=1988765|publisher=American Mathematical Society}}.\n*{{citation|first=W.|last=Whiteley|authorlink=Walter Whiteley|title=The union of matroids and the rigidity of frameworks|journal=[[SIAM Journal on Discrete Mathematics]]|volume=1|issue=2|pages=237–255|year=1988|doi=10.1137/0401025}}.\n*{{citation|first=D. R.|last=Woodall|contribution=Thrackles and deadlock|title=Combinatorial Mathematics and Its Applications|editor-first=D. J. A.|editor-last=Welsh|publisher=Academic Press|year=1969|pages=335–348}}.\n{{refend}}\n\n==External links==\n*{{mathworld | urlname = UnicyclicGraph | title = Unicyclic Graph}}\n\n{{good article}}\n\n[[Category:Matroid theory]]\n[[Category:Graph families]]"
    },
    {
      "title": "Matroid rank",
      "url": "https://en.wikipedia.org/wiki/Matroid_rank",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Maximum size of an independent set of the matroid}}\nIn the mathematical theory of [[matroid]]s, the '''rank''' of a matroid is the maximum size of an independent set in the matroid. The rank of a subset ''S'' of elements of the matroid is, similarly, the maximum size of an independent subset of ''S'', and the '''rank function''' of the matroid maps sets of elements to their ranks.\n\nThe rank function is one of the fundamental concepts of matroid theory via which matroids may be axiomatized. The rank functions of matroids form an important subclass of the [[submodular set function]]s, and the rank functions of the matroids defined from certain other types of mathematical object such as [[undirected graph]]s, [[matrix (mathematics)|matrices]], and [[field extension]]s are important within the study of those objects.\n\n==Properties and axiomatization==\nThe rank function of a matroid obeys the following properties.\n*The value of the rank function is always a non-negative [[integer]].\n*For any two subsets <math>A</math> and <math>B</math> of <math>E</math>, <math>r(A\\cup B)+r(A\\cap B)\\le r(A)+r(B)</math>. That is, the rank is a [[submodular function]].\n*For any set <math>A</math> and element <math>x</math>, <math>r(A)\\le r(A\\cup\\{x\\})\\le r(A)+1</math>. From the first of these two inequalities it follows more generally that, if <math>A\\subset B\\subset E</math>, then <math>r(A)\\leq r(B)\\leq r(E)</math>. That is, the rank is a [[monotonic function]].\nThese properties may be used as axioms to characterize the rank function of matroids: every integer-valued submodular function on the subsets of a finite set that obeys the inequalities <math>r(A)\\le r(A\\cup\\{x\\})\\le r(A)+1</math> for all <math>A</math> and <math>x</math> is the rank function of a matroid.<ref>{{citation|title=Combinatorial Optimization|first1=M. M.|last1=Shikare|first2=B. N.|last2=Waphare|publisher=Alpha Science Int'l Ltd.|year=2004|isbn=9788173195600|page=155|url=https://books.google.com/books?id=6N-wogwA7TcC&pg=PA155}}.</ref><ref>{{citation\n | last = Welsh | first = D. J. A. | authorlink = Dominic Welsh\n | isbn = 9780486474397\n | page = 8\n | publisher = Courier Dover Publications\n | title = Matroid Theory\n | year = 2010}}.</ref>\n\n==Other matroid properties from rank==\nThe rank function may be used to determine the other important properties of a matroid:\n*A set is independent if and only if its rank equals its cardinality, and dependent if and only if it has greater cardinality than rank.<ref name=\"o25\">{{harvtxt|Oxley|2006}}, p. 25.</ref>\n*A nonempty set is a circuit if its cardinality equals one plus its rank and every subset formed by removing one element from the set has equal rank.<ref name=\"o25\"/>\n*A set is a basis if its rank equals both its cardinality and the rank of the matroid.<ref name=\"o25\"/>\n*A set is closed if it is [[maximal element|maximal]] for its rank, in the sense that there does not exist another element that can be added to it while maintaining the same rank.\n*The difference <math>|A|-r(A)</math> is called the '''nullity''' of the subset <math>A</math>. It is the minimum number of elements that must be removed from <math>A</math> to obtain an independent set.<ref>{{harvtxt|Oxley|2006}}, p. 34.</ref>\n*The '''corank''' of a subset <math>A</math> can refer to at least two different quantities: some authors use it to refer to the rank of <math>A</math> in the dual matroid, <math>r^*(A) = |A| + r(E \\setminus A) - r(E)</math>, while other authors use corank to refer to the difference <math>r(E)- r(A)</math>.\n\n==Ranks of special matroids==\nIn [[graph theory]], the [[circuit rank]] (or cyclomatic number) of a graph is the corank of the associated [[graphic matroid]]; it measures the minimum number of edges that must be removed from the graph to make the remaining edges form a forest.<ref>{{citation|title=The Theory of Graphs|first=Claude|last=Berge|authorlink=Claude Berge|publisher=Courier Dover Publications|year=2001|isbn=9780486419756|pages=27–30|contribution=Cyclomatic number|url=https://books.google.com/books?id=h5BjnaoKyOwC&pg=PA27}}.</ref> Several authors have studied the [[parameterized complexity]] of graph algorithms parameterized by this number.<ref>{{citation\n | last1 = Coppersmith | first1 = Don | author1-link = Don Coppersmith\n | last2 = Vishkin | first2 = Uzi | author2-link = Uzi Vishkin\n | doi = 10.1016/0166-218X(85)90057-5 | zbl=0573.68017\n | issue = 1\n | journal = Discrete Applied Mathematics\n | pages = 27–45\n | title = Solving NP-hard problems in 'almost trees': Vertex cover\n | volume = 10\n | year = 1985}}.</ref><ref>{{citation\n | last1 = Fiala | first1 = Jiří\n | last2 = Kloks | first2 = Ton\n | last3 = Kratochvíl | first3 = Jan\n | doi = 10.1016/S0166-218X(00)00387-5 | zbl=0982.05085\n | issue = 1\n | journal = Discrete Applied Mathematics\n | pages = 59–72\n | title = Fixed-parameter complexity of λ-labelings\n | volume = 113\n | year = 2001}}.</ref>\n\nIn [[linear algebra]], the rank of a [[linear matroid]] defined by [[linear independence]] from the columns of a [[matrix (mathematics)|matrix]] is the [[Rank (linear algebra)|rank]] of the matrix,<ref>{{citation\n | last = Oxley | first = James G. | authorlink = James Oxley\n | isbn = 9780199202508\n | pages = 81\n | publisher = Oxford University Press\n | series = Oxford Graduate Texts in Mathematics\n | title = Matroid Theory\n | volume = 3\n | year = 2006}}.</ref> and it is also the [[Dimension (vector space)|dimension]] of the [[vector space]] spanned by the columns.\n\nIn [[abstract algebra]], the rank of a matroid defined from sets of elements in a [[field extension]] ''L''/''K'' by [[algebraic independence]] is known as the [[transcendence degree]].<ref>{{citation\n | last = Lindström | first = B.\n | contribution = Matroids, algebraic and non-algebraic\n | location = Cambridge\n | mr = 1052666\n | pages = 166–174\n | publisher = Cambridge Univ. Press\n | series = London Math. Soc. Lecture Note Ser.\n | title = Algebraic, extremal and metric combinatorics, 1986 (Montreal, PQ, 1986)\n | volume = 131\n | year = 1988}}.</ref>\n\n==See also==\n*[[Matroid oracle#Types of oracles|Rank oracle]]\n\n==References==\n{{reflist|2}}\n\n[[Category:Dimension]]\n[[Category:Matroid theory|Rank]]"
    },
    {
      "title": "Regular matroid",
      "url": "https://en.wikipedia.org/wiki/Regular_matroid",
      "text": "In mathematics, a '''regular matroid''' is a [[matroid]] that can be [[Matroid representation|represented]] over all [[field (mathematics)|fields]].\n\n==Definition==\nA [[matroid]] is defined to be a family of subsets of a finite set, satisfying certain axioms. The sets in the family are called \"independent sets\". One of the ways of constructing a matroid is to select a finite set of vectors in a [[vector space]], and to define a subset of the vectors to be independent in the matroid when it is [[linearly independent]] in the vector space. Every family of sets constructed in this way is a matroid, but not every matroid can be constructed in this way, and the vector spaces over different [[Field (mathematics)|fields]] lead to different sets of matroids that can be constructed from them.\n\nA matroid <math>M</math> is regular when, for every field <math>F</math>, <math>M</math> can be represented by a system of vectors over <math>F</math>.<ref name=\"sfo\">{{citation\n | last = Fujishige | first = Satoru\n | isbn = 9780444520869\n | page = 24\n | publisher = Elsevier\n | series = Annals of Discrete Mathematics\n | title = Submodular Functions and Optimization\n | year = 2005}}.</ref><ref>{{citation\n | last = Oxley | first = James G. | authorlink = James Oxley\n | isbn = 9780199202508\n | page = 209\n | publisher = Oxford University Press\n | series = Oxford Graduate Texts in Mathematics\n | title = Matroid Theory\n | volume = 3\n | year = 2006}}.</ref>\n\n==Properties==\nIf a matroid is regular, so is its [[dual matroid]],<ref name=\"sfo\"/> and so is every one of its [[matroid minor|minors]].<ref>{{harvtxt|Oxley|2006}}, p. 112.</ref> Every direct sum of regular matroids remains regular.<ref>{{harvtxt|Oxley|2006}}, p. 131.</ref>\n\nEvery [[graphic matroid]] (and every co-graphic matroid) is regular.<ref>{{citation\n | last = Tutte | first = W. T.\n | journal = Journal of Research of the National Bureau of Standards\n | mr = 0179781\n | pages = 1–47\n | title = Lectures on matroids\n | url = http://cdm16009.contentdm.oclc.org/cdm/ref/collection/p13011coll6/id/66650\n | volume = 69B\n | year = 1965\n | doi=10.6028/jres.069b.001}}.</ref> Conversely, every regular matroid may be constructed by combining graphic matroids, co-graphic matroids, and a certain ten-element matroid that is neither graphic nor co-graphic, using an operation for combining matroids that generalizes the [[clique-sum]] operation on graphs.<ref>{{citation\n | last = Seymour | first = P. D. | authorlink = Paul Seymour (mathematician)\n | doi = 10.1016/0095-8956(80)90075-1\n | issue = 3\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 579077\n | pages = 305–359\n | series = Series B\n | title = Decomposition of regular matroids\n | volume = 28\n | year = 1980}}.</ref>\n\nThe number of bases in a regular matroid may be computed as the [[determinant]] of an associated matrix, generalizing [[Kirchhoff's theorem|Kirchhoff's matrix-tree theorem]] for [[graphic matroid]]s.<ref>{{citation\n | last = Maurer | first = Stephen B.\n | issue = 1\n | journal = SIAM Journal on Applied Mathematics\n | mr = 0392635\n | pages = 143–148\n | title = Matrix generalizations of some theorems on trees, cycles and cocycles in graphs\n | volume = 30\n | year = 1976\n | doi=10.1137/0130017}}.</ref>\n\n==Characterizations==\nThe [[uniform matroid]] <math>U{}^2_4</math> (the four-point line) is not regular: it cannot be realized over the two-element [[finite field]] [[GF(2)]], so it is not a [[binary matroid]], although it can be realized over all other fields. The matroid of the [[Fano plane]] (a rank-three matroid in which seven of the triples of points are dependent) and its dual are also not regular: they can be realized over GF(2), and over all fields of characteristic two, but not over any other fields than those. As {{harvtxt|Tutte|1958}} showed, these three examples are fundamental to the theory of regular matroids: every non-regular matroid has at least one of these three as a [[matroid minor|minor]]. Thus, the regular matroids are exactly the matroids that do not have one of the three forbidden minors <math>U{}^2_4</math>, the Fano plane, or its dual.<ref name=\"t58\">{{citation\n | last = Tutte | first = W. T. | authorlink = W. T. Tutte\n | journal = Transactions of the American Mathematical Society\n | mr = 0101526\n | pages = 144–174\n | title = A homotopy theorem for matroids. I, II\n | volume = 88\n | year = 1958\n | doi=10.2307/1993244}}.</ref>\n\nIf a matroid is regular, it must clearly be realizable over the two fields GF(2) and GF(3). The converse is true: every matroid that is realizable over both of these two fields is regular. The result follows from a forbidden minor characterization of the matroids realizable over these fields, part of a family of results codified by [[Rota's conjecture]].<ref>{{citation\n | last = Seymour | first = P. D. | authorlink = Paul Seymour (mathematician)\n | doi = 10.1016/0095-8956(79)90055-8\n | issue = 2\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 532586\n | pages = 159–173\n | series = Series B\n | title = Matroid representation over GF(3)\n | volume = 26\n | year = 1979}}.</ref>\n\nThe regular matroids are the matroids that can be defined from a [[unimodular matrix|totally unimodular matrix]], a matrix in which every square submatrix has determinant 0, 1, or &minus;1. The vectors realizing the matroid may be taken as the rows of the matrix. For this reason, regular matroids are sometimes also called '''unimodular matroids'''.<ref>{{harvtxt|Oxley|2006}}, p. 20.</ref> The equivalence of regular matroids and unimodular matrices, and their characterization by forbidden minors, are deep results of [[W. T. Tutte]], originally proved by him using the [[Tutte homotopy theorem]].<ref name=\"t58\"/> {{harvtxt|Gerards|1989}} later published an alternative and simpler proof of the characterization of unimodular matrices by forbidden minors.<ref>{{citation|last=Gerards|first=A. M. H.|year=1989|title=A short proof of Tutte's characterization of totally unimodular matrices|journal=Linear Algebra and its Applications|volume=114/115|pages=207&ndash;212|doi=10.1016/0024-3795(89)90461-8}}.</ref>\n\n==Algorithms==\nThere is a [[polynomial time]] algorithm for testing whether a matroid is regular, given access to the matroid through an [[matroid oracle|independence oracle]].<ref>{{citation\n | last = Truemper | first = K.\n | issue = 3\n | journal = European Journal of Combinatorics\n | mr = 679212\n | pages = 275–291\n | title = On the efficiency of representability tests for matroids\n | volume = 3\n | year = 1982\n | doi=10.1016/s0195-6698(82)80039-5}}.</ref>\n\n==References==\n{{reflist|colwidth=30em}}\n\n[[Category:Matroid theory]]"
    },
    {
      "title": "Matroid representation",
      "url": "https://en.wikipedia.org/wiki/Matroid_representation",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Abstract combinatorial representation of linearly independent vectors}}\nIn the mathematical theory of [[matroid]]s, a '''matroid representation''' is a family of [[vector space|vectors]] whose [[linear independence]] relation is the same as that of a given matroid. Matroid representations are analogous to [[group representation]]s; both types of representation provide abstract algebraic structures (matroids and groups respectively) with concrete descriptions in terms of [[linear algebra]].\n\nA '''linear matroid''' is a matroid that has a representation, and an ''F''-'''linear matroid''' (for a [[field (mathematics)|field]] ''F'') is a matroid that has a representation using a [[vector space]] over ''F''. '''Matroid representation theory''' studies the existence of representations and the properties of linear matroids.\n\n==Definitions==\nA (finite) [[matroid]] <math>(E,\\mathcal{I})</math> is defined by a [[finite set]] <math>E</math> (the elements of the matroid) and a [[family of sets|family]] <math>\\mathcal{I}</math> of the subsets of <math>E</math>, called the independent sets of the matroid. It is required to satisfy the properties that every subset of an independent set is itself independent, and that if one independent set <math>A</math> is larger than a second independent set <math>B</math> then there exists an element <math>x\\in A\\setminus B</math> that can be added to <math>B</math> to form a larger independent set. One of the key motivating examples in the formulation of matroids was the notion of [[linear independence]] of vectors in a [[vector space]]: if <math>E</math> is a finite set or multiset of vectors, and <math>\\mathcal{I}</math> is the family of linearly independent subsets of <math>E</math>, then <math>(E,\\mathcal{I})</math> is a matroid.<ref>{{citation\n | last = Oxley | first = James G. | authorlink = James Oxley\n | isbn = 9780199202508\n | page = 8\n | publisher = Oxford University Press\n | series = Oxford Graduate Texts in Mathematics\n | title = Matroid Theory\n | volume = 3\n | year = 2006}}. For the rank function, see p. 26.</ref><ref>{{citation\n | last = Welsh | first = D. J. A. | authorlink = Dominic Welsh\n | isbn = 9780486474397\n | page = 10\n | publisher = Courier Dover Publications\n | title = Matroid Theory\n | year = 2010}}.</ref>\n\nMore generally, if <math>(E,\\mathcal{I})</math> is any matroid, then a representation of <math>(E,\\mathcal{I})</math> may be defined as a function <math>f</math> that maps <math>E</math> to a vector space <math>V</math>, with the property that a subset <math>A</math> of <math>E</math> is independent if and only if <math>f(A)</math> is linearly independent. A matroid with a representation is called a linear matroid, and if <math>V</math> is a vector space over field ''F'' then the matroid is called an ''F''-linear matroid. Thus, the linear matroids are exactly the matroids that are [[isomorphism|isomorphic]] to the matroids defined from sets or multisets of vectors. The function <math>f</math> will be [[bijection|one-to-one]] if and only if the underlying matroid is simple (having no two-element dependent sets).  Matroid representations may also be described more concretely using [[matrix (mathematics)|matrices]] over a field ''F'', with one column per matroid element and with a set of elements being independent in the matroid if and only if the corresponding set of matrix columns is linearly independent.  The [[matroid rank|rank function]] of a linear matroid is given by the [[rank (linear algebra)|matrix rank]] of submatrices of this matrix, or equivalently by the [[dimension (vector space)|dimension]] of the [[linear span]] of subsets of vectors.<ref>{{harvtxt|Oxley|2006}}, p. 12.</ref>\n\n==Characterization of linear matroids==\n[[File:Vamos matroid.svg|thumb|The [[Vámos matroid]], not linear over any field]]\n[[File:Perles configuration.svg|thumb|left|The [[Perles configuration]], linear over the reals but not the rationals]]\nNot every matroid is linear; the eight-element [[Vámos matroid]] is one of the smallest matroids that is unrepresentable over all fields.<ref>{{harvtxt|Oxley|2006}}, pp. 170–172, 196.</ref> If a matroid is linear, it may be representable over some but not all fields.\nFor instance, the nine-element rank-three matroid defined by the [[Perles configuration]] is representable over the [[real number]]s but not over the [[rational number]]s.\n\n[[Binary matroid]]s are the matroids that can be represented over the [[finite field]] [[GF(2)]]; they are exactly the matroids that do not have the [[uniform matroid]] <math>U{}^2_4</math> as a [[matroid minor|minor]].<ref name=\"tutte58\">{{citation\n | last = Tutte | first = W. T. | authorlink = W. T. Tutte\n | journal = [[Transactions of the American Mathematical Society]]\n | mr = 0101526\n | pages = 144–174\n | title = A homotopy theorem for matroids. I, II\n | volume = 88\n | year = 1958\n | doi=10.2307/1993244}}.</ref> The unimodular or [[regular matroid]]s are the matroids that can be represented over all fields;<ref name=Wh872>White (1987) p.2</ref> they can be characterized as the matroids that have none of <math>U{}^2_4</math>, the [[Fano plane]] (a binary matroid with seven elements), or the [[dual matroid]] of the Fano plane as minors.<ref name=\"tutte58\"/><ref name=Wh8712>White (1987) p.12</ref>  Alternatively, a matroid is regular if and only if it can be represented by a [[totally unimodular matrix]].<ref name=\"tutte65\"/>\n\n[[Rota's conjecture]] states that, for every finite field ''F'', the ''F''-linear matroids can be characterized by a finite set of forbidden minors, similar to the characterizations described above for the binary and regular matroids.<ref>{{citation\n | last = Rota | first = Gian-Carlo | authorlink = Gian-Carlo Rota\n | contribution = Combinatorial theory, old and new\n | location = Paris\n | mr = 0505646\n | pages = 229–233\n | publisher = Gauthier-Villars\n | title = Actes du Congrès International des Mathématiciens (Nice, 1970), Tome 3\n | year = 1971}}.</ref> As of 2012, it has been proven only for fields of four or fewer elements.<ref name=\"tutte58\"/><ref>{{citation\n | last = Bixby | first = Robert E.\n | doi = 10.1016/0095-8956(79)90056-X\n | issue = 2\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 532587\n | pages = 174–204\n | series = Series B\n | title = On Reid's characterization of the ternary matroids\n | volume = 26\n | year = 1979}}.</ref><ref name=\"s79\">{{citation\n | last = Seymour | first = P. D. | authorlink = Paul Seymour (mathematician)\n | doi = 10.1016/0095-8956(79)90055-8\n | issue = 2\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 532586\n | pages = 159–173\n | series = Series B\n | title = Matroid representation over GF(3)\n | volume = 26\n | year = 1979}}.</ref><ref>{{citation\n |last1=Geelen \n |first1=J. F. \n |author1-link=Jim Geelen \n |last2=Gerards \n |first2=A. M. H. \n |last3=Kapoor \n |first3=A. \n |doi=10.1006/jctb.2000.1963 \n |issue=2 \n |journal=[[Journal of Combinatorial Theory]] \n |mr=1769191 \n |pages=247–299 \n |series=Series B \n |title=The excluded minors for GF(4)-representable matroids \n |url=http://www.math.uwaterloo.ca/~jfgeelen/publications/gf4.pdf \n |volume=79 \n |year=2000 \n |deadurl=yes \n |archiveurl=https://web.archive.org/web/20100924110912/http://www.math.uwaterloo.ca/~jfgeelen/publications/gf4.pdf \n |archivedate=2010-09-24 \n |df= \n}}.</ref> For infinite fields (such as the field of the [[real number]]s) no such characterization is possible.<ref>{{citation\n | last = Vámos | first = P.\n | doi = 10.1112/jlms/s2-18.3.403\n | issue = 3\n | journal = [[Journal of the London Mathematical Society]]\n | mr = 518224\n | pages = 403–408\n | series = Second Series\n | title = The missing axiom of matroid theory is lost forever\n | volume = 18\n | year = 1978}}.</ref>\n\n==Field of definition==\nFor every [[algebraic number field]] and every [[finite field]] ''F'' there is a matroid ''M'' for which ''F'' is the minimal subfield of its algebraic closure over which ''M'' can be represented: ''M'' can be taken to be of rank 3.<ref name=Wh8718>{{citation | editor-last=White | editor-first=Neil | title=Combinatorial geometries | series=Encyclopedia of Mathematics and its Applications | volume=29 | location=Cambridge | publisher=[[Cambridge University Press]] | year=1987 | isbn=0-521-33339-3 | zbl=0626.00007 | page=18 }}</ref>\n\n==Characteristic set==\nThe '''characteristic set''' of a linear matroid is defined as the set of [[Characteristic (algebra)|characteristics]] of the fields over which it is linear.<ref name=Ing71>{{citation | zbl=0222.05025 | last=Ingleton | first=A.W. | chapter=Representation of matroids | pages=149–167 | title=Combinatorial mathematics and its applications.  Proceedings, Oxford, 1969 | editor-last=Welsh | editor-first=D.J.A. | year=1971 | publisher=Academic Press | isbn=0-12-743350-3 }}</ref>  For every [[prime number]] ''p'' there exist infinitely many matroids whose characteristic set is the singleton set {''p''},<ref>{{citation\n | last1 = Oxley | first1 = James\n | last2 = Semple | first2 = Charles\n | last3 = Vertigan | first3 = Dirk\n | last4 = Whittle | first4 = Geoff\n | doi = 10.1016/S0012-365X(00)00466-0\n | issue = 1-3\n | journal = Discrete Mathematics\n | mr = 1874763 \n | pages = 175–185\n | title = Infinite antichains of matroids with characteristic set {''p''}\n | volume = 242\n | year = 2002}}.</ref> and for every [[finite set]] of prime numbers there exists a matroid whose characteristic set is the given finite set.<ref>{{citation\n | last = Kahn | first = Jeff\n | doi = 10.1112/jlms/s2-26.2.207\n | issue = 2\n | journal = Journal of the London Mathematical Society\n | mr = 675165 | zbl=0468.05020\n | pages = 207–217\n | series = Second Series\n | title = Characteristic sets of matroids\n | volume = 26\n | year = 1982}}.</ref>\n\nIf the characteristic set of a matroid is infinite, it contains zero; and if it contains zero then it contains all but finitely many primes.<ref name=Ox225>{{harvtxt|Oxley|2006}}, p. 225.</ref>  Hence the only possible characteristic sets are finite sets not containing zero, and cofinite sets containing zero.<ref name=Ox226>{{harvtxt|Oxley|2006}}, p. 226.</ref>  Indeed, all such sets do occur.<ref name=Ox228>{{harvtxt|Oxley|2006}}, p. 228.</ref>\n\n==Related classes of matroids==\nA [[uniform matroid]] <math>U{}^r_n</math> has <math>n</math> elements, and its independent sets consist of all subsets of up to <math>r</math> of the elements. Uniform matroids may be represented by sets of vectors in [[general position]] in an <math>r</math>-dimensional vector space. The field of representation must be large enough for there to exist <math>n</math> vectors in general position in this vector space, so uniform matroids are ''F''-linear for all but finitely many fields ''F''.<ref>{{harvtxt|Oxley|2006}}, p. 100.</ref> The same is true for the [[partition matroid]]s, the direct sums of the uniform matroids, as the direct sum of any two ''F''-linear matroids is itself ''F''-linear.\n\nA [[graphic matroid]] is the matroid defined from the edges of an [[undirected graph]] by defining a set of edges to be independent if and only if it does not contain a [[cycle (graph theory)|cycle]]. Every graphic matroid is regular, and thus is ''F''-linear for every field ''F''.<ref name=\"tutte65\">{{citation\n | last = Tutte | first = W. T.\n | journal = Journal of Research of the National Bureau of Standards\n | mr = 0179781\n | pages = 1–47\n | title = Lectures on matroids\n | url = http://cdm16009.contentdm.oclc.org/cdm/ref/collection/p13011coll6/id/66650\n | volume = 69B\n | year = 1965\n | doi=10.6028/jres.069b.001}}.</ref>\n\nThe [[rigidity matroid]]s describe the [[Degrees of freedom (mechanics)|degrees of freedom]] of mechanical linkages formed by rigid bars connected at their ends by flexible hinges. A linkage of this type may be described as a graph, with an edge for each bar and a vertex for each hinge, and for one-dimensional linkages the rigidity matroids are exactly the graphic matroids. Higher-dimensional rigidity matroids may be defined using matrices of [[real number]]s with a structure similar to that of the [[incidence matrix]] of the underlying graph, and hence are <math>\\mathbb{R}</math>-linear.<ref>{{citation\n | last = Graver | first = Jack E.\n | doi = 10.1137/0404032\n | issue = 3\n | journal = SIAM Journal on Discrete Mathematics\n | mr = 1105942\n | pages = 355–368\n | title = Rigidity matroids\n | volume = 4\n | year = 1991}}.</ref><ref>{{citation\n | last = Whiteley | first = Walter | authorlink = Walter Whiteley\n | contribution = Some matroids from discrete applied geometry\n | doi = 10.1090/conm/197/02540\n | location = Providence, RI\n | mr = 1411692\n | pages = 171–311\n | publisher = American Mathematical Society\n | series = Contemporary Mathematics\n | title = Matroid theory (Seattle, WA, 1995)\n | volume = 197\n | year = 1996}}.</ref>\n\nLike uniform matroids and partition matroids, the [[gammoid]]s, matroids representing [[reachability]] in [[directed graph]]s, are linear over every sufficiently large field. More specifically, a gammoid with <math>n</math> elements may be represented over every field that has at least <math>2^n</math> elements.<ref>{{citation\n | last = Lindström | first = Bernt\n | doi = 10.1112/blms/5.1.85\n | journal = The Bulletin of the London Mathematical Society\n | mr = 0335313\n | pages = 85–90\n | title = On the vector representations of induced matroids\n | volume = 5\n | year = 1973}}.</ref>\n\nThe [[algebraic matroid]]s are matroids defined from sets of elements of a [[field extension]] using the notion of [[algebraic independence]]. Every linear matroid is algebraic, and for fields of characteristic zero (such as the real numbers) linear and algebraic matroids coincide, but for other fields there may exist algebraic matroids that are not linear.<ref>{{citation\n | last = Ingleton | first = A. W.\n | contribution = Representation of matroids\n | location = London\n | mr = 0278974\n | pages = 149–167\n | publisher = Academic Press\n | title = Combinatorial Mathematics and its Applications (Proc. Conf., Oxford, 1969)\n | year = 1971}}.</ref>\n\n==References==\n{{reflist|colwidth=30em}}\n\n[[Category:Matroid theory|Representation]]"
    },
    {
      "title": "Rigidity matroid",
      "url": "https://en.wikipedia.org/wiki/Rigidity_matroid",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Matroid describing degrees of freedom of an undirected graph in Euclidean space}}\nIn the mathematics of [[structural rigidity]], a '''rigidity matroid''' is a [[matroid]] that describes the number of [[Degrees of freedom (mechanics)|degrees of freedom]] of an [[undirected graph]] with rigid edges of fixed lengths, embedded into [[Euclidean space]]. In a rigidity matroid for a graph with ''n'' vertices in ''d''-dimensional space, a set of edges that defines a subgraph with ''k'' degrees of freedom has [[matroid rank]] ''dn''&nbsp;&minus;&nbsp;''k''. A set of edges is independent if and only if, for every edge in the set, removing the edge would increase the number of degrees of freedom of the remaining subgraph.<ref name=\"graver\">{{citation\n | last = Graver | first = Jack E.\n | doi = 10.1137/0404032\n | issue = 3\n | journal = SIAM Journal on Discrete Mathematics\n | mr = 1105942\n | pages = 355–368\n | title = Rigidity matroids\n | volume = 4\n | year = 1991}}.</ref><ref name=\"w92\">{{citation\n | last = Whiteley | first = Walter | authorlink = Walter Whiteley\n | contribution = Matroids and rigid structures\n | doi = 10.1017/CBO9780511662041.002\n | location = Cambridge\n | mr = 1165538\n | pages = 1–53\n | publisher = Cambridge Univ. Press\n | series = Encyclopedia of Mathematics and its Applications\n | title = Matroid Applications\n | volume = 40\n | year = 1992}}.</ref><ref name=\"whiteley\">{{citation\n | last = Whiteley | first = Walter | authorlink = Walter Whiteley\n | contribution = Some matroids from discrete applied geometry\n | doi = 10.1090/conm/197/02540\n | location = Providence, RI\n | mr = 1411692\n | pages = 171–311\n | publisher = American Mathematical Society\n | series = Contemporary Mathematics\n | title = Matroid theory (Seattle, WA, 1995)\n | volume = 197\n | year = 1996}}.</ref>\n\n==Definition==\nA ''framework'' is an [[undirected graph]], embedded into ''d''-dimensional Euclidean space by providing a ''d''-tuple of [[Cartesian coordinate]]s for each vertex of the graph. From a framework with ''n'' vertices and ''m'' edges, one can define a matrix with ''m'' rows and ''nd'' columns, an expanded version of the [[incidence matrix]] of the graph called the ''rigidity matrix''. In this matrix, the entry in row ''e'' and column (''v'',''i'') is zero if ''v'' is not an endpoint of edge ''e''. If, on the other hand, edge ''e'' has vertices ''u'' and ''v'' as endpoints, then the value of the entry is the difference between the ''i''th coordinates of ''v'' and ''u''.<ref name=\"graver\"/><ref name=\"whiteley\"/>\n\nThe rigidity matroid of the given framework is a [[linear matroid]] that has as its elements the edges of the graph. A set of edges is independent, in the matroid, if it corresponds to a set of rows of the rigidity matrix that is [[linear independence|linearly independent]]. A framework is called ''generic'' if the coordinates of its vertices are [[algebraic independence|algebraically independent]] real numbers. Any two generic frameworks on the same graph ''G'' determine the same rigidity matroid, regardless of their specific coordinates. This is the (''d''-dimensional) rigidity matroid of ''G''.<ref name=\"graver\"/><ref name=\"whiteley\"/>\n\n==Statics==\nA ''load'' on a framework is a system of forces on the vertices (represented as vectors). A ''stress'' is a special case of a load, in which equal and opposite forces are applied to the two endpoints of each edge (which may be imagined as a spring) and the forces formed in this way are added at each vertex. Every stress is an ''equilibrium load'', a load that does not impose any translational force on the whole system (the sum of its force vectors is zero) nor any rotational force. A linear dependence among the rows of the rigidity matrix may be represented as a ''self-stress'', an assignment of equal and opposite forces to the endpoints of each edge that is not identically zero but that adds to zero at every vertex. Thus, a set of edges forms an independent set in the rigidity matroid if and only if it has no self-stress.<ref name=\"whiteley\"/>\n\nThe vector space of all possible loads, on a system of ''n'' vertices, has dimension ''dn'', among which the equilibrium loads form a subspace of dimension\n<math>dn-\\binom{d+1}{2}</math>. An independent set in the rigidity matroid has a system of equilibrium loads whose dimension equals the cardinality of the set, so the maximum rank that any set in the matroid can have is <math>dn-\\binom{d+1}{2}</math>. If a set has this rank, it follows that its set of stresses is the same as the space of equilibrium loads. Alternatively and equivalently, in this case every equilibrium load on the framework may be ''resolved'' by a stress that generates an equal and opposite set of forces, and the framework is said to be statically rigid.<ref name=\"whiteley\"/>\n\n==Kinematics==\nIf the vertices of a framework are in a motion, then that motion may be described over small scales of distance by its [[gradient]], a vector for each vertex specifying its speed and direction. The gradient describes a linearized approximation to the actual motion of the points, in which each point moves at constant velocity in a straight line. The gradient may be described as a row vector that has one real number coordinate for each pair <math>(v,i)</math> where <math>v</math> is a vertex of the framework and <math>i</math> is the index of one of the Cartesian coordinates of <math>d</math>-dimensional space; that is, the dimension of the gradient is the same as the width of the rigidity matrix.<ref name=\"graver\"/><ref name=\"whiteley\"/>\n\nIf the edges of the framework are assumed to be rigid bars that can neither expand nor contract (but can freely rotate) then any motion respecting this rigidity must preserve the lengths of the edges: the derivative of length, as a function of the time over which the motion occurs, must remain zero. This condition may be expressed in linear algebra as a constraint that the gradient vector of the motion of the vertices must have zero [[inner product]] with the row of the rigidity matrix that represents the given edge. Thus, the family of gradients of (infinitesimally) rigid motions is given by the [[nullspace]] of the rigidity matrix.<ref name=\"graver\"/><ref name=\"whiteley\"/> For frameworks that are not in generic position, it is possible that some infinitesimally rigid motions (vectors in the nullspace of the rigidity matrix) are not the gradients of any continuous motion, but this cannot happen for generic frameworks.<ref name=\"w92\"/>\n\nA rigid motion of the framework is a motion such that, at each point in time, the framework is [[congruence (geometry)|congruent]] to its original configuration. Rigid motions include translations and rotations of Euclidean space; the gradients of rigid motions form a linear space having the translations and rotations as bases, of dimension <math>\\binom{d+1}{2}</math>, which must always be a subspace of the nullspace of the rigidity matrix.\nBecause the nullspace always has at least this dimension, the rigidity matroid can have rank at most <math>dn-\\binom{d+1}{2}</math>, and when it does have this rank the only motions that preserve the lengths of the edges of the framework are the rigid motions. In this case the framework is said to be first-order (or infinitesimally) rigid.<ref name=\"graver\"/><ref name=\"whiteley\"/> More generally, an edge <math>e</math> belongs to the matroid closure operation of a set <math>S</math> if and only if there does not exist a continuous motion of the framework that changes the length of <math>e</math> but leaves the lengths of the edges in <math>S</math> unchanged.<ref name=\"graver\"/>\n\nAlthough defined in different terms (column vectors versus row vectors, or forces versus motions) static rigidity and first-order rigidity reduce to the same properties of the underlying matrix and therefore coincide with each other. In two dimensions, the generic rigidity matroid also describes the number of degrees of freedom of a different kind of motion, in which each edge is constrained to stay parallel to its original position rather than being constrained to maintain the same length; however, the equivalence between rigidity and parallel motion breaks down in higher dimensions.<ref name=\"whiteley\"/>\n\n==Unique realization==\n[[File:Diamond graph.svg|thumb|The [[diamond graph]], generically rigid but not uniquely realizable]]\nA framework has a ''unique realization'' in ''d''-dimensional space if every placement of the same graph with the same edge lengths is congruent to it. Such a framework must necessarily be rigid, because otherwise there exists a continuous motion bringing it to a non-congruent placement with the same edge lengths, but unique realizability is stronger than rigidity. For instance, the [[diamond graph]] (two triangles sharing an edge) is rigid in two dimensions, but it is not uniquely realizable because it has two different realizations, one in which the triangles are on opposite sides of the shared edge and one in which they are both on the same side. Uniquely realizable graphs are important in applications that involve reconstruction of shapes from distances, such as [[triangulation]] in land surveying,<ref name=\"molecule\">{{citation\n | last = Hendrickson | first = Bruce\n | doi = 10.1137/0805040\n | issue = 4\n | journal = SIAM Journal on Optimization\n | mr = 1358807\n | pages = 835–857\n | title = The molecule problem: exploiting structure in global optimization\n | volume = 5\n | year = 1995| citeseerx = 10.1.1.55.2335\n }}.</ref> the determination of the positions of the nodes in a [[wireless sensor network]],<ref>{{citation\n | last1 = Eren | first1 = T.\n | last2 = Goldenberg | first2 = O.K.\n | last3 = Whiteley | first3 = W.\n | last4 = Yang | first4 = Y.R.\n | last5 = Morse | first5 = A.S.\n | last6 = Anderson | first6 = B.D.O.\n | last7 = Belhumeur | first7 = P.N.\n | contribution = Rigidity, computation, and randomization in network localization\n | doi = 10.1109/INFCOM.2004.1354686\n | pages = 2673–2684\n | title = Proc. Twenty-third Annual Joint Conference of the IEEE Computer and Communications Societies (INFOCOM 2004)\n | volume = 4\n | year = 2004}}.</ref> and the reconstruction of conformations of molecules via [[nuclear magnetic resonance spectroscopy]].<ref name=\"molecule\"/>\n\nBruce Hendrickson defined a graph to be ''redundantly rigid'' if it remains rigid after removing any one of its edges. In matroidal terms, this means that the rigidity matroid has the full rank <math>dn-\\binom{d+1}{2}</math> and that the matroid does not have any coloops. Hendrickson\nproved that every uniquely realizable framework (with generic edge lengths) is either a [[complete graph]] or a <math>(d+1)</math>-[[k-vertex-connected graph|vertex-connected]], redundantly rigid graph, and he conjectured that this is an exact characterization of the uniquely realizable frameworks.<ref>{{citation\n | last = Hendrickson | first = Bruce\n | doi = 10.1137/0221008\n | issue = 1\n | journal = SIAM Journal on Computing\n | mr = 1148818\n | pages = 65–84\n | title = Conditions for unique graph realizations\n | volume = 21\n | year = 1992}}.</ref> The conjecture is true for one and two dimensions; in the one-dimensional case, for instance, a graph is uniquely realizable if and only if it is connected and [[Bridge (graph theory)|bridgeless]].<ref>{{citation\n | last1 = Jackson | first1 = Bill\n | last2 = Jordán | first2 = Tibor\n | doi = 10.1016/j.jctb.2004.11.002\n | issue = 1\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 2130278\n | pages = 1–29\n | series = Series B\n | title = Connected rigidity matroids and unique realizations of graphs\n | volume = 94\n | year = 2005}}.</ref> However, Henrickson's conjecture is false for three or more dimensions.<ref>{{citation\n | last = Connelly | first = Robert | authorlink = Robert Connelly\n | contribution = On generic global rigidity\n | location = Providence, RI\n | mr = 1116345\n | pages = 147–155\n | publisher = American Mathematical Society\n | series = DIMACS Series on Discrete Mathematics and Theoretical Computer Science\n | title = Applied Geometry and Discrete Mathematics\n | volume = 4\n | year = 1991}}.</ref> For frameworks that are not generic, it is NP-hard to determine whether a given framework is uniquely realizable.<ref>{{citation\n | last = Saxe | first = J. B. | authorlink = James B. Saxe\n | location = Pittsburgh, PA\n | publisher = Computer Science Department, Carnegie-Mellon University\n | series = Technical Report\n | title = Embeddability of weighted graphs in ''k''-space is strongly NP-hard\n | year = 1979}}. As cited by {{harvtxt|Jackson|Jordán|2005}}.</ref>\n\n==Relation to sparsity==\n{{harvtxt|Streinu|Theran|2009}} define a graph as being <math>(k,l)</math>-sparse if every nonempty subgraph with <math>n</math> vertices has at most <math>kn-l</math> edges, and <math>(k,l)</math>-tight if it is <math>(k,l)</math>-sparse and has exactly <math>kn-l</math> edges.<ref>{{citation\n | last1 = Streinu | first1 = I. | author1-link = Ileana Streinu\n | last2 = Theran | first2 = L.\n | arxiv = math/0703921\n | doi = 10.1016/j.ejc.2008.12.018\n | issue = 8\n | journal = [[European Journal of Combinatorics]]\n | pages = 1944–1964\n | title = Sparse hypergraphs and pebble game algorithms\n | volume = 30\n | year = 2009}}.</ref> From the consideration of loads and stresses it can be seen that a set of edges that is independent in the rigidity matroid forms a <math>(d,\\binom{d+1}{2})</math>-sparse graph, for if not there would exist a subgraph whose number of edges would exceed the dimension of its space of equilibrium loads, from which it follows that it would have a self-stress.\nBy similar reasoning, a set of edges that is both independent and rigid forms a <math>(d,\\binom{d+1}{2})</math>-tight graph. For instance, in one dimension, the independent sets form the edge sets of forests, (1,1)-sparse graphs, and the independent rigid sets form the edge sets of trees, (1,1)-tight graphs. In this case the rigidity matroid of a framework is the same as the [[graphic matroid]] of the corresponding graph.<ref name=\"w92\"/>\n\nIn two dimensions, {{harvtxt|Laman|1970}} showed that the same characterization is true: the independent sets form the edge sets of (2,3)-sparse graphs and the independent rigid sets form the edge sets of (2,3)-tight graphs.<ref>{{citation\n | last = Laman | first = G. | authorlink = Gerard Laman\n | doi = 10.1007/BF01534980\n | issue = 4\n | journal = J. Engineering Mathematics\n | mr = 0269535\n | pages = 331–340\n | title = On graphs and the rigidity of plane skeletal structures\n | volume = 4\n | year = 1970| bibcode = 1970JEnMa...4..331L}}.</ref> Based on this work the (2,3)-tight graphs (the graphs of minimally rigid generic frameworks in two dimensions) have come to be known as [[Laman graph]]s. The family of Laman graphs on a fixed set of <math>n</math> vertices forms the set of bases of the rigidity matroid of a [[complete graph]], and more generally for every graph <math>G</math> that forms a rigid framework in two dimensions, the spanning Laman subgraphs of <math>G</math> are the bases of the rigidity matroid of <math>G</math>.\n\nHowever, in higher dimensions not every <math>(d,\\binom{d+1}{2})</math>-tight graph is minimally rigid, and characterizing the minimally rigid graphs (the bases of the rigidity matroid of the complete graph) is an important open problem.<ref>{{citation\n | last1 = Jackson | first1 = Bill\n | last2 = Jordán | first2 = Tibor\n | doi = 10.1142/S0218195906002117\n | issue = 5-6\n | journal = International Journal of Computational Geometry & Applications\n | mr = 2269396\n | pages = 415–429\n | title = On the rank function of the 3-dimensional rigidity matroid\n | url = http://web.cs.elte.hu/egres/tr/egres-05-09.pdf\n | volume = 16\n | year = 2006}}.</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Mathematics of rigidity]]\n[[Category:Matroid theory]]"
    },
    {
      "title": "Rota's basis conjecture",
      "url": "https://en.wikipedia.org/wiki/Rota%27s_basis_conjecture",
      "text": "In [[linear algebra]] and [[matroid|matroid theory]], '''Rota's basis conjecture''' is an unproven [[conjecture]] concerning rearrangements of [[Basis (linear algebra)|bases]], named after [[Gian-Carlo Rota]]. It states that, if ''X'' is either a [[vector space]] of dimension ''n'' or more generally a matroid of rank ''n'', with ''n'' disjoint bases ''B<sub>i</sub>'', then it is possible to arrange the elements of these bases into an ''n''&nbsp;&times;&nbsp;''n'' [[matrix (mathematics)|matrix]] in such a way that the rows of the matrix are exactly the given bases and the columns of the matrix are also bases. That is, it should be possible to find a second set of ''n'' disjoint bases ''C<sub>i</sub>'', each of which consists of one element from each of the bases ''B<sub>i</sub>''.\n\n==Examples==\n[[File:Rota's basis conjecture 2.svg|thumb|upright=1.1|The nine vertices of three colored triangles (red, blue, and yellow) regrouped into three rainbow triangles (black edges)]]\nRota's basis conjecture has a simple formulation for points in the [[Euclidean plane]]: it states that, given three triangles with distinct vertices, with each triangle colored with one of three colors, it must be possible to regroup the nine triangle vertices into three \"rainbow\" triangles having one vertex of each color. The triangles are all required to be non-degenerate, meaning that they do not have all three vertices on a line.\n\nTo see this as an instance of the basis conjecture, one may use either [[linear independence]] of the vectors (''x<sub>i</sub>'',''y<sub>i</sub>'',1) in a three-dimensional [[real number|real]] vector space (where  (''x<sub>i</sub>'',''y<sub>i</sub>'') are the [[Cartesian coordinates]] of the triangle vertices) or equivalently one may use a matroid of rank three in which a set ''S'' of points is independent if either |''S''|&nbsp;≤&nbsp;2 or ''S'' forms the three vertices of a non-degenerate triangle. For this linear algebra and this matroid, the bases are exactly the non-degenerate triangles. Given the three input triangles and the three rainbow triangles, it is possible to arrange the nine vertices into a 3&nbsp;&times;&nbsp;3 matrix in which each row contains the vertices of one of the single-color triangles and each column contains the vertices of one of the rainbow triangles.\n\nAnalogously, for points in three-dimensional Euclidean space, the conjecture states that the sixteen vertices of four non-degenerate tetrahedra of four different colors may be regrouped into four rainbow tetrahedra.\n\n==Partial results==\nThe statement of Rota's basis conjecture was first published by {{harvtxt|Huang|Rota|1994}}, crediting it (without citation) to Rota in 1989.<ref name=\"hr94\">{{citation\n | last1 = Huang | first1 = Rosa\n | last2 = Rota | first2 = Gian-Carlo | author2-link = Gian-Carlo Rota\n | doi = 10.1016/0012-365X(94)90114-7\n | issue = 1–3\n | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]\n | mr = 1271866\n | pages = 225–236\n | title = On the relations of various conjectures on Latin squares and straightening coefficients\n | volume = 128\n | year = 1994}}. See in particular Conjecture 4, p.&nbsp;226.</ref> The basis conjecture has been proven for [[paving matroid]]s (for all&nbsp;''n'')<ref>{{citation\n | last1 = Geelen | first1 = Jim | author1-link = Jim Geelen\n | last2 = Humphries | first2 = Peter J.\n | doi = 10.1137/060655596\n | issue = 4\n | journal = [[SIAM Journal on Discrete Mathematics]]\n | mr = 2272246\n | pages = 1042–1045\n | title = Rota's basis conjecture for paving matroids\n | url = http://www.math.uwaterloo.ca/~jfgeelen/publications/paving.pdf\n | volume = 20\n | year = 2006| citeseerx = 10.1.1.63.6806 }}.</ref> and for the case ''n''&nbsp;≤&nbsp;3 (for all types of matroid).<ref>{{citation\n | last = Chan | first = Wendy\n | doi = 10.1016/0012-365X(94)00071-3\n | issue = 1–3\n | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]\n | mr = 1360125\n | pages = 299–302\n | title = An exchange property of matroid\n | volume = 146\n | year = 1995}}.</ref> For arbitrary matroids, it is possible to arrange the basis elements into a matrix the first Ω({{radic|''n''}}) columns of which are bases.<ref>{{citation\n | last1 = Geelen | first1 = Jim | author1-link = Jim Geelen\n | last2 = Webb | first2 = Kerri\n | doi = 10.1137/060666494\n | issue = 3\n | journal = [[SIAM Journal on Discrete Mathematics]]\n | mr = 2354007\n | pages = 802–804\n | title = On Rota's basis conjecture\n | url = http://www.math.uwaterloo.ca/~jfgeelen/Publications/transversal.pdf\n | volume = 21\n | year = 2007}}.</ref> The basis conjecture for linear algebras over fields of characteristic zero and for even values of ''n'' would follow from another conjecture on [[Latin square]]s by Alon and Tarsi.<ref name=\"hr94\"/><ref>{{citation\n | last = Onn | first = Shmuel\n | doi = 10.2307/2974985\n | issue = 2\n | journal = [[The American Mathematical Monthly]]\n | mr = 1437419\n | pages = 156–159\n | title = A colorful determinantal identity, a conjecture of Rota, and Latin squares\n | volume = 104\n | year = 1997| jstor = 2974985\n }}.</ref> Based on this implication, the conjecture is known to be true for linear algebras over the [[real number]]s for infinitely many values of&nbsp;''n''.<ref>{{citation\n | last = Glynn | first = David G.\n | doi = 10.1137/090773751\n | issue = 2\n | journal = [[SIAM Journal on Discrete Mathematics]]\n | mr = 2646093\n | pages = 394–399\n | title = The conjectures of Alon–Tarsi and Rota in dimension prime minus one\n | volume = 24\n | year = 2010}}.</ref>\n\n==Related problems==\nIn connection with [[Tverberg's theorem]], {{harvtxt|Bárány|Larman|1992}} conjectured that, for every set of ''r''(''d''&nbsp;+&nbsp;1) points in ''d''-dimensional [[Euclidean space]], colored with ''d''&nbsp;+&nbsp;1 colors in such a way that there are ''r'' points of each color, there is a way to partition the points into rainbow simplices (sets of ''d''&nbsp;+&nbsp;1 points with one point of each color) in such a way that the convex hulls of these sets have a nonempty intersection.<ref>{{citation\n | last1 = Bárány | first1 = I. | author1-link = Imre Bárány\n | last2 = Larman | first2 = D. G.\n | doi = 10.1112/jlms/s2-45.2.314\n | issue = 2\n | journal = Journal of the London Mathematical Society\n | mr = 1171558\n | pages = 314–320\n | series = Second Series\n | title = A colored version of Tverberg's theorem\n | volume = 45\n | year = 1992| citeseerx = 10.1.1.108.9781 }}.</ref> For instance, the two-dimensional case (proven by Bárány and Larman) with ''r''&nbsp;=&nbsp;3 states that, for every set of nine points in the plane, colored with three colors and three points of each color, it is possible to partition the points into three intersecting rainbow triangles, a statement similar to Rota's basis conjecture which states that it is possible to partition the points into three non-degenerate rainbow triangles. The conjecture of Bárány and Larman allows a collinear triple of points to be considered as a rainbow triangle, whereas Rota's basis conjecture disallows this; on the other hand, Rota's basis conjecture does not require the triangles to have a common intersection. Substantial progress on the conjecture of Bárány and Larman was made by {{harvtxt|Blagojević|Matschke|Ziegler|2009}}.<ref>{{citation\n | last1 = Blagojević | first1 = Pavle V. M.\n | last2 = Matschke | first2 = Benjamin\n | last3 = Ziegler | first3 = Günter M. | author3-link = Günter M. Ziegler\n | arxiv = 0910.4987\n | title = Optimal bounds for the colored Tverberg problem\n | year = 2009| bibcode = 2009arXiv0910.4987B}}.</ref>\n\n==See also==\n*[[Rota's conjecture]], a different conjecture by Rota about linear algebra and matroids\n\n==References==\n{{reflist}}\n\n==External links==\n*[http://www.openproblemgarden.org/op/rotas_basis_conjecture Rota's basis conjecture], Open Problem Garden.\n\n[[Category:Linear algebra]]\n[[Category:Matroid theory]]\n[[Category:Conjectures]]"
    },
    {
      "title": "Rota's conjecture",
      "url": "https://en.wikipedia.org/wiki/Rota%27s_conjecture",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Conjecture in combinatorics}}\n'''Rota's excluded minors conjecture''' is one of a number of conjectures made by mathematician [[Gian-Carlo Rota]]. It is considered to be an important problem by some members of the structural combinatorics community. Rota [[conjecture]]d in 1971 that, for every [[finite field]], the family of [[matroid]]s that can be [[Matroid representation|represented]] over that field has only finitely many [[matroid minor|excluded minors]].<ref>{{citation\n | last = Rota | first = Gian-Carlo | authorlink = Gian-Carlo Rota\n | contribution = Combinatorial theory, old and new\n | location = Paris\n | mr = 0505646\n | pages = 229–233\n | publisher = Gauthier-Villars\n | title = Actes du Congrès International des Mathématiciens (Nice, 1970), Tome 3\n | year = 1971}}.</ref>\nA proof of the conjecture has been announced by Geelen, Gerards, and Whittle.<ref>{{citation|title=Solving Rota's conjecture|journal=Notices of the American Mathematical Society|url=http://www.ams.org/notices/201407/rnoti-p736.pdf|date=Aug 17, 2014|pages=736–743}}</ref>\n\n==Statement of the conjecture==\nIf <math>S</math> is a set of points in a [[vector space]] defined over a field <math>F</math>, then the linearly independent subsets of <math>S</math> form the independent sets of a [[matroid]] <math>M</math>; <math>S</math> is said to be a [[Matroid representation|representation]] of any matroid isomorphic to <math>M</math>. Not every matroid has a representation over every field, for instance, the [[Fano plane]] is representable only over fields of characteristic two. Other matroids are representable over no fields at all. The matroids that are representable over a particular field form a proper subclass of all matroids.\n\nA [[matroid minor|minor]] of a matroid is another matroid formed by a sequence of two operations: deletion and contraction. In the case of points from a vector space, deleting a point is simply the removal of that point from <math>S</math>; contraction is a dual operation in which a point is removed and the remaining points are projected a hyperplane that does not contain the removed points. It follows from this if a matroid is representable over a field, then so are all its minors. A matroid that is not representable over <math>F</math>, and is minor-[[minimal element|minimal]] with that property, is called an \"excluded minor\"; a matroid <math>M</math> is representable over <math>F</math> if and only if it does not contain one of the forbidden minors.\n\nFor representability over the [[real number]]s, there are infinitely many forbidden minors.<ref>{{citation\n | last = Vámos | first = P.\n | doi = 10.1112/jlms/s2-18.3.403\n | issue = 3\n | journal = [[Journal of the London Mathematical Society]]\n | mr = 518224\n | pages = 403–408\n | series = Second Series\n | title = The missing axiom of matroid theory is lost forever\n | volume = 18\n | year = 1978}}.</ref> Rota's conjecture is that, for every finite field <math>F</math>, there is only a finite number of forbidden minors.\n\n==Partial results==\n[[W. T. Tutte]] proved that the [[binary matroid]]s (matroids representable over the field of two elements) have a single forbidden minor, the [[uniform matroid]] <math>U{}^2_4</math> (geometrically, a line with four points on it).<ref>{{citation\n | last = Tutte | first = W. T. | authorlink = W. T. Tutte\n | journal = [[Transactions of the American Mathematical Society]]\n | mr = 0101526\n | pages = 144–174\n | title = A homotopy theorem for matroids. I, II\n | volume = 88\n | year = 1958\n | doi=10.2307/1993244}}.</ref><ref>{{citation\n | last = Tutte | first = W. T. | authorlink = W. T. Tutte\n | journal = Journal of Research of the National Bureau of Standards\n | mr = 0179781\n | pages = 1–47\n | title = Lectures on matroids\n | url = http://cdm16009.contentdm.oclc.org/cdm/ref/collection/p13011coll6/id/66650\n | volume = 69B\n | year = 1965\n | doi=10.6028/jres.069b.001}}. See in particular section 5.3, \"Characterization of binary matroids\", p.17.</ref>\n\nA matroid is representable over the ternary field GF(3) if and only if it does not have one or more of the following four matroids as minors: a five-point line <math>U{}^2_5</math>, its [[dual matroid]] <math>U{}^3_5</math> (five points in general position in three dimensions), the Fano plane, or the dual of the Fano plane. Thus, Rota's conjecture is true in this case as well.<ref>{{citation\n | last = Bixby | first = Robert E.\n | doi = 10.1016/0095-8956(79)90056-X\n | issue = 2\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 532587\n | pages = 174–204\n | series = Series B\n | title = On Reid's characterization of the ternary matroids\n | volume = 26\n | year = 1979}}. Bixby attributes this characterization of ternary matroids to Ralph Reid.</ref><ref name=\"s79\">{{citation\n | last = Seymour | first = P. D. | authorlink = Paul Seymour (mathematician)\n | doi = 10.1016/0095-8956(79)90055-8\n | issue = 2\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 532586\n | pages = 159–173\n | series = Series B\n | title = Matroid representation over GF(3)\n | volume = 26\n | year = 1979}}.</ref> As a consequence of this result and of the forbidden minor characterization by {{harvtxt|Tutte|1958}} of the [[regular matroid]]s (matroids that can be represented over all fields) it follows that a matroid is regular if and only if it is both binary and ternary.<ref name=\"s79\"/>\n\nThere are seven forbidden minors for the matroids representable over GF(4).<ref>{{citation\n |last1        = Geelen\n |first1       = J. F.\n |author1-link = Jim Geelen\n |last2        = Gerards\n |first2       = A. M. H.\n |last3        = Kapoor\n |first3       = A.\n |doi          = 10.1006/jctb.2000.1963\n |issue        = 2\n |journal      = [[Journal of Combinatorial Theory]]\n |mr           = 1769191\n |pages        = 247–299\n |series       = Series B\n |title        = The excluded minors for GF(4)-representable matroids\n |url          = http://www.math.uwaterloo.ca/~jfgeelen/publications/gf4.pdf\n |volume       = 79\n |year         = 2000\n |deadurl      = yes\n |archiveurl   = https://web.archive.org/web/20100924110912/http://www.math.uwaterloo.ca/~jfgeelen/publications/gf4.pdf\n |archivedate  = 2010-09-24\n |df           = \n}}.</ref> They are:\n*The six-point line <math>U{}^2_6</math>.\n*The dual <math>U{}^4_6</math> to the six-point line, six points in general position in four dimensions.\n*A self-dual six-point rank-three matroid with a single three-point line.\n*The non-Fano matroid formed by the seven points at the vertices, edge midpoints, and centroid of an [[equilateral triangle]] in the [[Euclidean plane]]. This configuration is one of two known sets of planar points with fewer than <math>n/2</math> [[Sylvester–Gallai theorem|two-point lines]].<ref>{{citation | last1 = Kelly|first1= L. M.|author1-link=Leroy Milton Kelly|last2= Moser|first2= W. O. J. | title = On the number of ordinary lines determined by ''n'' points | journal = Can. J. Math. | volume = 10 | year = 1958 | pages = 210–219 | url = http://www.cms.math.ca/cjm/v10/p210 | doi = 10.4153/CJM-1958-024-6}}.</ref>\n*The dual of the non-Fano matroid.\n*The eight-point matroid of a [[square antiprism]]. \n*The matroid obtained by relaxing the unique pair of disjoint circuit-hyperplanes of the square antiprism.\nThis result won the 2003 [[Fulkerson Prize]] for its authors [[Jim Geelen]], A. M. H. Gerards, and A. Kapoor.<ref>[http://www.mathopt.org/?nav=fulkerson_2003 2003 Fulkerson Prize citation], retrieved 2012-08-18.</ref>\n\nFor GF(5), several forbidden minors on up to 12 elements are known,<ref>{{citation\n |last1   = Betten\n |first1  = A.\n |last2   = Kingan\n |first2  = R. J.\n |last3   = Kingan\n |first3  = S. R.\n |issue   = 2\n |journal = MATCH Communications in Mathematical and in Computer Chemistry\n |mr      = 2357372\n |pages   = 511–521\n |title   = A note on GF(5)-representable matroids\n |url     = http://userhome.brooklyn.cuny.edu/skingan/papers/GF5RepresentableMatroids.pdf\n |volume  = 58\n |year    = 2007\n}}{{dead link|date=April 2018 |bot=InternetArchiveBot |fix-attempted=yes }}.</ref> but it is not known whether the list is complete.\n\n==Reported proof==\nGeoff Whittle announced during a 2013 visit to the UK that he, [[Jim Geelen]], and Bert Gerards have solved Rota's Conjecture.  The collaboration involved intense visits where the researchers sat in a room together, all day every day, in front of a whiteboard.<ref>[https://uwaterloo.ca/combinatorics-and-optimization/news/geelen-gerards-and-whittle-announce-proof-rotas-conjecture# Geelen, Gerards and Whittle announce a proof of Rota's conjecture] University of Waterloo, August 28, 2013</ref> It will take them years to write up their research in its entirety and publish it.<ref>[http://phys.org/news/2013-08-rota-conjecture-year-old-math-problem.html#jCp Rota's Conjecture: Researcher solves 40-year-old math problem] PhysOrg, August 15, 2013.</ref><ref>[http://www.cwi.nl/news/2013/cwi-researcher-proves-famous-rota%E2%80%99s-conjecture# CWI researcher proves famous Rota’s Conjecture] {{Webarchive|url=https://web.archive.org/web/20131026072059/http://www.cwi.nl/news/2013/cwi-researcher-proves-famous-rota%E2%80%99s-conjecture |date=2013-10-26 }} CWI, August 22, 2013.</ref> An outline of the proof has appeared in the Notices of the AMS.<ref>{{citation|title=Solving Rota's conjecture|journal=Notices of the American Mathematical Society|url=http://www.ams.org/notices/201407/rnoti-p736.pdf|date=Aug 17, 2014|pages=736–743}}</ref>\n\n==See also==\n*[[Rota's basis conjecture]], a different conjecture by Rota about linear algebra and matroids\n\n==References==\n{{reflist|colwidth=30em}}\n\n[[Category:Matroid theory]]"
    },
    {
      "title": "Signed graph",
      "url": "https://en.wikipedia.org/wiki/Signed_graph",
      "text": "[[File:Pox.jpg|thumb|There are eight ways that signs can be assigned to the sides of a triangle. An odd number of negative signs makes an unbalanced triangle, according to [[Fritz Heider]]'s theory.]]\nIn the area of [[graph theory]] in [[mathematics]], a '''signed graph''' is a graph in which each edge has a positive or negative sign.\n\nA signed graph is '''balanced''' if the product of edge signs around every [[cycle (graph theory)|cycle]] is positive.  Three fundamental questions about a signed graph are:  Is it balanced?  What is the largest size of a balanced edge set in it?  What is the smallest number of vertices that must be deleted to make it balanced?  The first question is easy to solve quickly; the second and third are computationally intractable (technically, they are [[NP-hard]]){{Citation needed|date=January 2018}}.\n\nThe name \"signed graph\" and the notion of balance appeared first in a mathematical paper of [[Frank Harary]] in 1953.<ref name=harnb>{{citation|last=Harary |first=Frank |authorlink=Frank Harary |journal=[[Michigan Mathematical Journal]] |mr=0067468 |pages=143–146 |title=On the notion of balance of a signed graph |url=http://projecteuclid.org/getRecord?id=euclid.mmj/1028989917 |archive-url=https://archive.is/20130415153307/http://projecteuclid.org/getRecord?id=euclid.mmj/1028989917 |dead-url=yes |archive-date=2013-04-15 |volume=2 |year=1955}}</ref> [[Dénes Kőnig]] had already studied equivalent notions in 1936 under a different terminology but without recognizing the relevance of the sign group.<ref name=koenig>{{citation | last = Kőnig | first = Dénes | authorlink = Dénes Kőnig | editor = Akademische Verlagsgesellschaft | title = Theorie der endlichen und unendlichen Graphen | year = 1936 }}</ref>\nAt the Center for Group Dynamics at the [[University of Michigan]], [[Dorwin Cartwright]] and Harary generalized [[Fritz Heider]]'s psychological theory of [[Balance theory|balance]] in triangles of sentiments to a psychological theory of balance in signed graphs.<ref name=carhar>Cartwright, D. and Harary, F. (1956)[http://snap.stanford.edu/class/cs224w-readings/cartwright56balance.pdf Structural balance: a generalization of Heider's theory],  [[Psychological Review]] 63: 277-293; link from [[Stanford University]]</ref><ref>[[Steven Strogatz]] (2010) [http://opinionator.blogs.nytimes.com/2010/02/14/the-enemy-of-my-enemy/?ref=opinion&_r=0 The enemy of my enemy], The [[New York Times]], February 14, 2010</ref>\n\nSigned graphs have been rediscovered many times because they come up naturally in many unrelated areas.<ref>{{citation\n | last = Zaslavsky | first = Thomas\n | journal = Electronic Journal of Combinatorics\n | mr = 1744869\n | at = Dynamic Surveys 8, 124 pp.\n | title = A mathematical bibliography of signed and gain graphs and allied areas\n | url = http://www.combinatorics.org/ojs/index.php/eljc/article/view/DS8\n | volume = 5\n | year = 1998}}.</ref>  For instance, they enable one to describe and analyze the geometry of subsets of the classical [[root system]]s.  They appear in [[topological graph theory]] and [[group theory]].  They are a natural context for questions about odd and even [[cycle (graph theory)|cycles]] in graphs.  They appear in computing the [[ground state]] energy in the non-ferromagnetic [[Ising model]]; for this one needs to find a largest balanced edge set in Σ.  They have been applied to data classification in [[correlation clustering]].\n\n==Examples==\n* The '''complete signed graph''' on ''n'' vertices with loops, denoted by ±''K''<sub>''n''</sub><sup>o</sup>, has every possible positive and negative edge including negative loops, but no positive loops.  Its edges correspond to the roots of the [[root system]] ''C''<sub>''n''</sub>; the column of an edge in the incidence matrix (see below) is the vector representing the root.\n* The '''complete signed graph''' with half-edges, ±''K''<sub>''n''</sub>', is ±''K''<sub>''n''</sub> with a half-edge at every vertex.  Its edges correspond to the roots of the root system ''B''<sub>''n''</sub>, half-edges corresponding to the unit basis vectors.\n* The '''complete signed link graph''', ±''K''<sub>''n''</sub>, is the same but without loops.  Its edges correspond to the roots of the root system ''D''<sub>''n''</sub>.\n* An '''all-positive''' signed graph has only positive edges.  If the underlying graph is ''G'', the all-positive signing is written +''G''.\n* An '''all-negative''' signed graph has only negative edges.  It is balanced if and only if it is [[Bipartite graph|bipartite]] because a circle is positive if and only if it has even length.  An all-negative graph with underlying graph ''G'' is written &minus;''G''.\n* A '''signed complete graph''' has as underlying graph ''G'' the ordinary complete graph ''K''<sub>''n''</sub>.  It may have any signs.  Signed complete graphs are equivalent to [[two-graph]]s, which are of value in [[finite group]] theory.  A '''two-graph''' can be defined as the class of vertex sets of negative triangles (having an odd number of negative edges) in a signed complete graph.\n\n==Adjacency matrix==\nThe [[adjacency matrix]] of a signed graph Σ on ''n'' vertices is an ''n'' &times; ''n'' [[matrix (mathematics)|matrix]] ''A''(Σ).  It has a row and column for each vertex.  The entry ''a''<sub>''vw''</sub> in row ''v'' and column ''w'' is the number of positive ''vw'' edges minus the number of negative ''vw'' edges.  On the diagonal, ''a''<sub>''vv''</sub> = 0 if there are no loops or half-edges; the correct definition when such edges exist depends on the circumstances.\n\n==Orientation==\nA signed graph is '''oriented''' when each end of each edge is given a direction, so that in a positive edge the ends are both directed from one endpoint to the other, and in a negative edge either both ends are directed outward, to their own vertices, or both are directed inward, away from their vertices.  Thus, an oriented signed graph is the same as a [[bidirected graph]].  (It is very different from a [[#Signed digraph|signed digraph]].)\n\n==Incidence matrix==\nThe (more correctly, \"an\") incidence matrix of a signed graph with ''n'' vertices and ''m'' edges is an ''n'' &times; ''m'' matrix, with a row for each vertex and a column for each edge.  It is obtained by orienting the signed graph in any way.  Then its entry η<sub>''ij''</sub> is +1 if edge ''j'' is oriented into vertex ''i'', &minus;1 if edge ''j'' is oriented out of vertex ''i'', and 0 if vertex ''i'' and edge ''j'' are not [[Glossary of graph theory#Adjacency and degree|incident]].  This rule applies to a link, whose column will have two nonzero entries with absolute value 1, a half-edge, whose column has a single nonzero entry +1 or &minus;1, and a loose edge, whose column has only zeroes.  The column of a loop, however, is all zero if the loop is positive, and if the loop is negative it has entry ±2 in the row corresponding to its incident vertex.\n\nAny two incidence matrices are related by negating some subset of the columns.  Thus, for most purposes it makes no difference which orientation we use to define the incidence matrix, and we may speak of '''the''' incidence matrix of Σ without worrying about exactly which one it is.\n\nNegating a row of the incidence matrix corresponds to switching the corresponding vertex.\n\n==Switching==\n'''Switching''' a vertex in Σ means negating the signs of all the edges incident to that vertex.  Switching a set of vertices means negating all the edges that have one end in that set and one end in the complementary set.  Switching a series of vertices, once each, is the same as switching the whole set at once.\n\nSwitching of signed graphs ('''signed switching''') is generalized from Seidel (1976), where it was applied to graphs (''[[two-graph|graph switching]]''), in a way that is equivalent to switching of signed complete graphs.\n\n'''Switching equivalence''' means that two graphs are related by switching, and an equivalence class of signed graphs under switching is called a '''switching class'''.  Sometimes these terms are applied to equivalence of signed graphs under the combination of switching and [[isomorphism]], especially when the graphs are unlabeled; but to distinguish the two concepts the combined equivalence may be called '''switching isomorphism''' and an equivalence class under switching isomorphism may be called a '''switching isomorphism class'''.\n\nSwitching a set of vertices affects the adjacency matrix by negating the rows and columns of the switched vertices.  It affects the incidence matrix by negating the rows of the switched vertices.\n\n==Fundamental theorem==\nThe sign of a [[path (graph theory)|path]] is the product of the signs of its edges. Thus a path is positive only if there are an even number of negative edges in it (where zero is even). In the mathematical [[balance theory]] of [[Frank Harary]], a signed graph is '''balanced''' when every [[cycle (graph theory)|cycle]] is positive. He proves that a signed graph is balanced when (1) for every pair of nodes, all paths between them have the same sign, or (2) the graph partitions into a pair of subgraphs, each consisting of positive edges, but connected by negative edges.<ref>Dorwin Cartwright & [[Frank Harary]] (1979) \"Balance and clusterability: An overview\", pages 25 to 50 in ''Perspectives in Social Network Research'', editors: Paul W. Holland & Samuel Leinhardt, [[Academic Press]] {{isbn|0-12-352550-0}}</ref>  The theorem was published by Harary in 1953.<ref name=harnb/> It generalizes the theorem that an ordinary (unsigned) graph is [[bipartite graph|bipartite]] if and only if every cycle has even length.\n\nA simple proof uses the method of switching.  To prove Harary's theorem, one shows by induction that Σ can be switched to be all positive if and only if it is balanced.\n\nA weaker theorem, but with a simpler proof, is that if every 3-cycle in a signed [[complete graph]] is positive, then the graph is balanced. For the proof, pick an arbitrary node ''n'' and place it and all those nodes that are linked to ''n'' by a positive edge in one group, called ''A'', and all those linked to ''n'' by a negative edge in the other, called ''B''. Since this is a complete graph, every two nodes in ''A'' must be friends and every two nodes in ''B'' must be friends, otherwise there would be a 3-cycle which was unbalanced.  (Since this is a complete graph, any one negative edge would cause an unbalanced 3-cycle.)  Likewise, all negative edges must go between the two groups.<ref>[http://www.scienceoftheweb.org/15-396/lectures/lecture03.pdf Luis Von Ahn Science of the Web Lecture 3 p. 28]</ref>\n\n==Frustration==\nGive each vertex a value of +1 or &minus;1; we call this a '''state''' of Σ.  An edge is called '''satisfied''' if it is positive and both endpoints have the same value, or it is negative and the endpoints have opposite values. An edge that is not satisfied is called '''frustrated'''. The smallest number of frustrated edges over all states is called the '''frustration index''' (or '''line index of balance''') of Σ.  Finding the frustration index is hard, in fact, it is [[NP-hard]]. Aref et al. suggest binary programming models that are capable of computing the frustration index of graphs with up to 10<sup>5</sup> edges in a reasonable time.<ref>{{cite arxiv|last=Aref|first=Samin|last2=Mason|first2=Andrew J.|last3=Wilson|first3=Mark C.|date=2016-11-28|title=An exact method for computing the frustration index in signed networks using binary programming|eprint=1611.09030|class=cs.SI}}</ref><ref>{{Citation|last=Aref|first=Samin|title=Computing the Line Index of Balance Using Integer Programming Optimisation|date=2018|url=https://doi.org/10.1007/978-3-319-94830-0_3|work=Optimization Problems in Graph Theory: In Honor of Gregory Z. Gutin's 60th Birthday|pages=65–84|editor-last=Goldengorin|editor-first=Boris|series=Springer Optimization and Its Applications|publisher=Springer International Publishing|language=en|doi=10.1007/978-3-319-94830-0_3|isbn=9783319948300|access-date=2019-06-21|last2=Mason|first2=Andrew J.|last3=Wilson|first3=Mark C.}}</ref><ref>{{Cite journal|last=Aref|first=Samin|last2=Wilson|first2=Mark C|date=2019-04-01|editor-last=Estrada|editor-first=Ernesto|title=Balance and frustration in signed networks|url=https://academic.oup.com/comnet/article/7/2/163/5074195|journal=Journal of Complex Networks|language=en|volume=7|issue=2|pages=163–189|doi=10.1093/comnet/cny015|issn=2051-1329}}</ref> One can see the NP-hard complexity by observing that the frustration index of an all-negative signed graph is equivalent to the [[Maxcut|maximum cut]] problem in graph theory, which is NP-hard.  The reason for the equivalence is that the frustration index equals the smallest number of edges whose negation (or, equivalently, deletion; a theorem of Harary) makes Σ balanced.  (This can be proved easily by switching.)\n\nThe frustration index is important in a model of [[spin glass]]es, the [[Ising model#Mixed|mixed Ising model]].  In this model, the signed graph is fixed.  A state consists of giving a \"spin\", either \"up\" or \"down\", to each vertex.  We think of spin up as +1 and spin down as &minus;1.  Thus, each state has a number of frustrated edges.  The energy of a state is larger when it has more frustrated edges, so a [[ground state]] is a state with the fewest frustrated energy.  Thus, to find the ground state energy of Σ one has to find the frustration index.\n\n==Matroid theory==\nThere are two [[matroid]]s associated with a signed graph, called the ''signed-graphic matroid'' (also called the ''frame matroid'' or sometimes ''bias matroid'') and the ''lift matroid'', both of which generalize the cycle matroid of a graph.  They are special cases of the same matroids of a [[biased graph]].\n\nThe '''frame matroid''' (or '''signed-graphic matroid''') ''M''(''G'') has for its ground set the edge set ''E''.<ref>{{citation | last = Zaslavsky | first = Thomas|authorlink=Thomas Zaslavsky| doi = 10.1016/0166-218X(82)90033-6 | issue = 1 | journal = [[Discrete Applied Mathematics]] | mr = 676405 | pages = 47–74 | title = Signed graphs | volume = 4 | year = 1982}}. Erratum.  ''Discrete Applied Mathematics'', '''5''' (1983), 248</ref>  An edge set is independent if each component contains either no circles or just one circle, which is negative.  (In matroid theory a half-edge acts exactly like a negative loop.)  A circuit of the matroid is either a positive circle, or a pair of negative circles together with a connecting simple path, such that the two circles are either disjoint (then the connecting path has one end in common with each circle and is otherwise disjoint from both) or share just a single common vertex (in this case the connecting path is that single vertex).  The rank of an edge set ''S'' is ''n'' &minus; ''b'', where ''n'' is the number of vertices of ''G'' and ''b'' is the number of balanced components of ''S'', counting isolated vertices as balanced components.  \nThis matroid is the [[matroid theory|column matroid]] of the incidence matrix of the signed graph.\nThat is why it describes the linear dependencies of the roots of a classical root system.\n\nThe '''extended lift matroid''' ''L''<sub>0</sub>(''G'') has for its ground set the set ''E''<sub>0</sub> the union of edge set ''E'' with an '''extra point''', which we denote ''e''<sub>0</sub>.  The '''lift matroid''' ''L''(''G'') is the extended lift matroid restricted to ''E''.  The extra point acts exactly like a negative loop, so we describe only the lift matroid.  An edge set is independent if it contains either no circles or just one circle, which is negative.  (This is the same rule that is applied separately to each component in the signed-graphic matroid.)  A matroid circuit is either a positive circle or a pair of negative circles that are either disjoint or have just a common vertex.  The rank of an edge set ''S'' is ''n'' &minus; ''c'' + ε, where ''c'' is the number of components of ''S'', counting isolated vertices, and ε is 0 if ''S'' is balanced and 1 if it is not.\n\n==Other kinds of \"signed graph\"==\nSometimes the signs are taken to be +1 and &minus;1.  This is only a difference of notation, if the signs are still multiplied around a circle and the sign of the product is the important thing.  However, there are two other ways of treating the edge labels that do not fit into signed graph theory.\n\nThe term ''signed graph'' is applied occasionally to graphs in which each edge has a weight, ''w''(''e'') = +1 or &minus;1.  These are not the same kind of signed graph; they are [[Graph (discrete mathematics)|weighted graphs]] with a restricted weight set.  The difference is that weights are added, not multiplied.  The problems and methods are completely different.\n\nThe name is also applied to graphs in which the signs function as colors on the edges.  The significance of the color is that it determines various weights applied to the edge, and not that its sign is intrinsically significant.  This is the case in [[knot theory]], where the only significance of the signs is that they can be interchanged by the two-element group, but there is no intrinsic difference between positive and negative.  The matroid of a sign-colored graph is the cycle matroid of the underlying graph; it is not the frame or lift matroid of the signed graph.   The sign labels, instead of changing the matroid, become signs on the elements of the matroid.\n\nIn this article we discuss only signed graph theory in the strict sense.  For sign-colored graphs see [[colored matroid]]s.\n\n===Signed digraph===\nA '''signed digraph''' is a [[directed graph]] with signed arcs.  Signed digraphs are far more complicated than signed graphs, because only the signs of directed cycles are significant.  For instance, there are several definitions of balance, each of which is hard to characterize, in strong contrast with the situation for signed undirected graphs.\n\nSigned digraphs should not be confused with [[#Orientation|oriented signed graphs]].  The latter are bidirected graphs, not directed graphs (except in the trivial case of all positive signs).\n\n==Coloring==\nAs with unsigned [[Graph theory|graphs]], there is a notion of '''signed graph coloring'''. Where a [[Graph coloring|coloring]] of a graph is a mapping from the vertex set to the natural numbers, a coloring of a signed graph is a mapping from the vertex set to the integers.\nThe constraints on [[graph coloring|proper colorings]] come from the edges of the signed graph. The integers assigned to two vertices must be distinct if they are connected by a positive edge. The labels on adjacent vertices must not be additive inverses if the vertices are connected by a negative edge. There can be no proper coloring of a signed graph with a positive loop.\n\nWhen restricting the vertex labels to the set of integers with magnitude at most a natural number ''k'', the set of proper colorings of a signed graph is finite. The relation between the number of such proper colorings and ''k'' is a polynomial in ''k''. This is analogous to the [[chromatic polynomial]] of unsigned graphs.\n\n==Applications==\n\n===Social psychology===\nIn [[social psychology]], signed graphs have been used to model social situations, with positive edges representing friendships and negative edges enmities between nodes, which represent people.<ref name=carhar/> Then, for example, a positive 3-cycle is either three mutual friends, or two friends with a common enemy; while a negative 3-cycle is either three mutual enemies, or two enemies who share a mutual friend. According to [[balance theory]], positive cycles are balanced and supposed to be stable social situations, whereas negative cycles are unbalanced and supposed to be unstable.  According to the theory, in the case of three mutual enemies, this is because sharing a common enemy is likely to cause [[The enemy of my enemy is my friend|two of the enemies to become friends]].  In the case of two enemies sharing a friend, the shared friend is likely to choose one over the other and turn one of his or her friendships into an enemy.\n\nAntal, Krapivsky and Reder consider [[social dynamics]] as the change in sign on an edge of a signed graph.<ref>T. Antal, P.L. Krapivsky & S. Redner (2006) [https://arxiv.org/pdf/physics/0605183v1.pdf Social Balance on Networks: The Dynamics of Friendship and Enmity]</ref> The social relations with previous friends of a divorcing couple are used to illustrate the evolution of a signed graph in society. Another illustration describes the changing international alliances between European powers in the decades before the [[First World War]]. They consider local triad dynamics and constrained triad dynamics, where in the latter case a relationship change is made only when the total number of unbalanced triads is reduced. The simulation presumed a complete graph with random relations having a random unbalanced triad selected for transformation. The evolution of the signed graph with ''N'' nodes under this process is studied and simulated to describe the stationary density of friendly links.\n\nBalance theory has been severely challenged, especially in its application to large systems, on the theoretical ground that friendly relations tie a society together, while a society divided into two camps of enemies would be highly unstable.<ref>B. Anderson, in ''Perspectives on Social Network Research'', ed. P.W. Holland and S. Leinhardt. New York: Academic Press, 1979.</ref>\nExperimental studies have also provided only weak confirmation of the predictions of structural balance theory.<ref>Julian O. Morrissette and John C. Jahnke (1967) \"No relations and relations of strength zero in the theory of structural balance\", ''Human Relations'', vol. 20: 189-195.</ref>\n\n===Spin glasses===\nIn physics, signed graphs are a natural context for the general, nonferromagnetic [[Ising model]], which is applied to the study of [[spin glass]]es.\n\n===Complex systems===\n[[File:Simple 3-level trophic system.png|thumb|right|A three-variable signed digraph representing a simple [[Trophic level|trophic system]]]]\nUsing an analytic method initially developed in population biology and ecology, but now used a similar notion in many scientific disciplines, analysis of the properties of signed digraphs has found application in applied formal causal reasoning about the behavior of complex causal systems.<ref>Puccia, Charles J. and [[Richard Levins| Levins, Richard]] (1986). ''[http://www.hup.harvard.edu/catalog.php?isbn=9780674435070 Qualitative Modeling of Complex Systems: An Introduction to Loop Analysis and Time Averaging]''. Harvard University Press, Cambridge, MA.</ref><ref>Dambacher, Jeffrey M., Li, Hiram W., and Rossignol, Philippe A. (2002). [https://www.jstor.org/stable/pdf/3071950 Relevance of community structure in assessing indeterminacy of ecological predictions]. ''Ecology'', 83(5):1372–1385.</ref> Such analyses answer questions about feedback at given levels of the system, and about the direction of variable response given a perturbation to a system at one or more points, variable correlations given such perturbations, the distribution of variance across the system, and the sensitivity or insensitivity of particular variables to system perturbations.\n\n===Data clustering===\n[[Correlation clustering]] looks for natural clustering of data by similarity.  The data points are represented as the vertices of a graph, with a positive edge joining similar items and a negative edge joining dissimilar items.\n\n==Generalizations==\nA signed graph is a special kind of [[gain graph]], where the gain group has order 2.  The pair (''G'', '''''B'''''(''&Sigma;'')) determined by a signed graph Σ is a special kind of [[biased graph]].\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{citation\n | last1 = Cartwright | first1 = D.\n | last2 = Harary | first2 = F. | author2-link = Frank Harary\n | journal = Psychological Review\n | pages = 277–293\n | title = Structural balance: a generalization of Heider's theory\n | volume = 63\n | issue = 5\n | year = 1956 | doi=10.1037/h0046049}}.\n*{{citation\n | last = Seidel | first = J. J.\n | contribution = A survey of two-graphs\n | location = Rome\n | mr = 0550136\n | pages = 481–511\n | series = Atti dei Convegni Lincei\n | volume = 17\n | publisher = [[Accademia Nazionale dei Lincei]]\n | title = Colloquio Internazionale sulle Teorie Combinatorie (Rome, 1973), Tomo I\n | year = 1976}}.\n*{{citation\n | last = Zaslavsky | first = Thomas\n | journal = Electronic Journal of Combinatorics\n | mr = 1744869\n | at = Dynamic Surveys 8, 124 pp.\n | title = A mathematical bibliography of signed and gain graphs and allied areas\n | url = http://www.combinatorics.org/ojs/index.php/eljc/article/view/DS8\n | volume = 5\n | year = 1998}}\n\n[[Category:Matroid theory]]\n[[Category:Extensions and generalizations of graphs]]\n[[Category:Oriented matroids]]"
    },
    {
      "title": "Steinitz exchange lemma",
      "url": "https://en.wikipedia.org/wiki/Steinitz_exchange_lemma",
      "text": "The '''Steinitz exchange lemma''' is a basic theorem in [[linear algebra]] used, for example, to show that any two [[Basis (linear algebra)|bases]] for a finite-[[Dimension (vector space)|dimensional]] [[vector space]] have the same number of elements. The result is named after the German mathematician [[Ernst Steinitz]]. The result is often called the '''Steinitz–Mac&nbsp;Lane exchange lemma''', also recognizing the generalization<ref>\n{{citation|last=Mac&nbsp;Lane|first=Saunders|authorlink=Saunders Mac Lane|year=1936|title=Some interpretations of abstract linear dependence in terms of projective geometry|journal=American Journal of Mathematics|volume=58|pages=236–240|doi=10.2307/2371070| jstor=2371070 | issue=1|publisher=The Johns Hopkins University Press}}.</ref>\nby [[Saunders Mac Lane]]\nof Steinitz's lemma to [[matroid]]s.<ref>\n{{citation|editor-last=Kung|editor-first=Joseph P. S.|title=A Source Book in Matroid Theory|publisher=Birkhäuser|mr=0890330|isbn=0-8176-3173-9|location=Boston|year=1986|doi=10.1007/978-1-4684-9199-9}}.\n</ref>\n\n== Statement ==\nIf <math>\\{v_1, \\dots, v_m\\}</math> is a set of <math>m</math> [[Linear independence|linearly independent]] vectors in a vector space <math>V</math>, and <math>\\{w_1, \\dots, w_n\\}</math> [[Linear span|span]] <math>V</math>, then:\n\n1. <math>m \\leq n</math>;\n\n2. Possibly after reordering the <math>w_i</math>, the set <math>\\{v_1, \\dots, v_m, w_{m+1}, \\dots, w_n\\}</math> spans <math>V</math>.\n\n==Proof==\n\nSuppose that we have the indicated sets of vectors. We wish to show that for each <math>k \\in \\{0, \\dots, m\\}</math>, we have that <math>k \\le n</math>, and that the set <math>\\{v_1, \\dotsc, v_k, w_{k + 1}, \\dotsc, w_n\\}</math> spans <math>V</math> (where the <math>w_j</math> have possibly been reordered, and the reordering depends on <math>k</math>). We proceed by [[mathematical induction]] on <math>k</math>.\n\nFor the base case, suppose <math>k</math> is zero.\nIn this case, the claim holds because there are no vectors <math>v_i</math>, and the set <math>\\{w_1, \\dotsc, w_n\\}</math> spans <math>V</math> by hypothesis.\n\nFor the inductive step, assume the proposition is true for some <math>k < m</math>. Since <math>v_{k+1}\\in V</math>, and <math>\\{ v_1,\\ldots, v_k,w_{k+1},\\ldots,w_n\\}</math> spans <math>V</math> (by the induction hypothesis), there exist coefficients <math>\\mu_1,\\ldots,\\mu_n</math> such that\n:<math>v_{k+1}=\\sum_{j=1}^k \\mu_j v_j+\\sum_{j=k+1}^n \\mu_j w_j</math>.\nAt least one of <math>\\{\\mu_{k+1},\\ldots,\\mu_n\\}</math> must be non-zero, since otherwise this equality would contradict the linear independence of <math>\\{ v_1,\\ldots,v_m \\}</math>; note that this additionally implies that <math>k+1 \\le n</math>. By reordering the <math>\\mu_{k+1}w_{k+1},\\ldots,\\mu_{n}w_n</math>, we may assume that <math>\\mu_{k+1}</math> is not zero. Therefore, we have\n: <math>w_{k+1}= \\frac{1}{\\mu_{k+1}}\\left(v_{k+1} - \\sum_{j=1}^k \\mu_j v_j - \\sum_{j=k+2}^n \\mu_j w_j\\right)</math>.\nIn other words, <math>w_{k+1}</math> is in the span of <math>\\{ v_1,\\ldots, v_{k+1},w_{k+2},\\ldots,w_n\\}</math>. The latter span therefore contains each of the vectors <math> v_1, \\ldots, v_k, w_{k+1}, w_{k+2}, \\ldots, w_n </math>, and hence must contain the span of these latter vectors as a subset. But since the latter span is <math>V</math> (by the induction hypothesis), this simply means that the span of <math>\\{ v_1,\\ldots, v_{k+1},w_{k+2},\\ldots,w_n\\}</math> contains <math>V</math> as a subset (thus is <math>V</math>). We have therefore shown that our claim is true of <math>k+1</math>, completing the inductive step.\n\nTo show <math> m \\leq n </math>, we prove by contradiction, and assume <math>m > n</math>. We continue the inductive step until we have shown that <math>\\{v_1, \\dotsc, v_n\\}</math> spans <math>V</math>, which means\n:<math>v_{n+1}=\\sum_{j=1}^n\\lambda_jv_j</math>\nfor some <math>\\lambda_1, \\ldots , \\lambda_n</math> not all 0, which contradicts that <math>\\{v_1, \\dotsc, v_m\\}</math> are linearly independent. So <math> m \\leq n </math>.\n\n==Applications==\nThe Steinitz exchange lemma is a basic result in [[computational mathematics]], especially in [[numerical linear algebra|linear algebra]] and in [[Matroid#Greedy_algorithm|combinatorial algorithms]].<ref>Page v in Stiefel:\n{{cite book|last=Stiefel|first=Eduard L.|authorlink=Eduard Stiefel|title=An introduction to numerical mathematics|edition=Translated by Werner C. Rheinboldt & Cornelie J. Rheinboldt from the second German|publisher=Academic Press|location=New York|year=1963|pages=x+286|mr=181077}}\n</ref>\n\n== References ==\n\n<references/>\n\n* Julio R. Bastida, ''Field extensions and Galois Theory'', [[Addison–Wesley|Addison–Wesley Publishing Company]] (1984).\n\n== External links ==\n* [[Mizar system]] proof: http://mizar.org/version/current/html/vectsp_9.html#T19\n\n[[Category:Linear algebra]]\n[[Category:Lemmas]]\n[[Category:Matroid theory]]"
    },
    {
      "title": "Supersolvable arrangement",
      "url": "https://en.wikipedia.org/wiki/Supersolvable_arrangement",
      "text": "In mathematics, a '''supersolvable arrangement''' is a  [[Arrangement of hyperplanes|hyperplane arrangement]] which has a maximal [[Flag (linear algebra)|flag]] with only [[Modular lattice|modular]] elements.\nA complex hyperplane arrangement is supersolvable if and only if its complement is fiber-type. \n\nExamples include arrangements associated with [[Coxeter group]]s of type A and B.\n\nIt is known that all [[Orlik–Solomon algebra]]s of supersolvable  arrangements are [[Koszul algebra]]s; whether the converse is true is an open problem.<ref>Sergey Yuzvinsky, Orlik–Solomon algebras in algebra and topology, Russian Math. Surveys 56 (2001), no. 2, 293–364. {{MR|1859708}}</ref>\n\n==References==\n\n<references/>\n\n[[Category:Discrete geometry]]\n[[Category:Matroid theory]]"
    },
    {
      "title": "Sylvester matroid",
      "url": "https://en.wikipedia.org/wiki/Sylvester_matroid",
      "text": "In [[matroid theory]], a '''Sylvester matroid''' is a matroid in which every pair of elements belongs to a three-element circuit (a ''triangle'') of the matroid.<ref name=\"m69\">{{citation\n | last = Murty | first = U. S. R. | authorlink = U. S. R. Murty\n | contribution = Sylvester matroids\n | location = New York\n | mr = 0255432\n | pages = 283–286\n | publisher = Academic Press\n | title = Recent Progress in Combinatorics (Proc. Third Waterloo Conf. on Combinatorics, 1968)\n | year = 1969}}.</ref><ref>{{citation\n | last = Welsh | first = D. J. A.\n | isbn = 9780486474397\n | page = 297\n | publisher = Courier Dover Publications\n | title = Matroid Theory\n | year = 2010}}.</ref>\n\n==Example==\nThe <math>n</math>-point line (i.e., the rank 2 [[uniform matroid]] on <math>n</math> elements, <math>U{}^2_n</math>) is a Sylvester matroid because every pair of elements is a basis and every triple is a circuit.\n\nA Sylvester matroid of rank three may be formed from any [[Steiner triple system]], by defining the lines of the matroid to be the triples of the system. Sylvester matroids of rank three may also be formed from [[Sylvester–Gallai configuration]]s, configurations of points and lines (in non-Euclidean spaces) with no two-point line. For example, the [[Fano plane]] and the [[Hesse configuration]] give rise to Sylvester matroids with seven and nine elements respectively, and may be interpreted either as Steiner triple systems or as Sylvester–Gallai configurations.\n\n==Properties==\nA Sylvester matroid with [[matroid rank|rank]] <math>r</math> must have at least <math>2^r-1</math> elements; this bound is tight only for the [[projective space]]s over [[GF(2)]], of which the Fano plane is an example.<ref>{{citation\n | last = Murty | first = U. S. R.\n | doi = 10.1007/BF01817744\n | journal = [[Aequationes Mathematicae]]\n | mr = 0265186\n | pages = 44–50\n | title = Matroids with Sylvester property\n | volume = 4\n | year = 1970}}.</ref>\n\nIn a Sylvester matroid, every independent set can be augmented by one more element to form a circuit of the matroid.<ref name=\"m69\"/><ref>{{citation\n | last1 = Bryant | first1 = V. W.\n | last2 = Dawson | first2 = J. E.\n | last3 = Perfect | first3 = Hazel | author3-link = Hazel Perfect\n | issue = 3\n | journal = Compositio Mathematica\n | mr = 511749\n | pages = 339–351\n | title = Hereditary circuit spaces\n | url = http://www.numdam.org/item?id=CM_1978__37_3_339_0\n | volume = 37\n | year = 1978}}.</ref>\n\nSylvester matroids cannot be [[Matroid representation|represented]] over the [[real number]]s (this is the [[Sylvester–Gallai theorem]]), nor can they be [[oriented matroid|oriented]].<ref>{{citation\n | last = Ziegler | first = Günter M. | author-link = Günter M. Ziegler\n | doi = 10.1007/BF00181199\n | issue = 3\n | journal = Geometriae Dedicata\n | mr = 1112674\n | pages = 365–371\n | title = Some minimal non-orientable matroids of rank three\n | volume = 38\n | year = 1991}}.</ref>\n\n==History==\nSylvester matroids were studied and named by {{harvtxt|Murty|1969}} after [[James Joseph Sylvester]], because they violate the [[Sylvester–Gallai theorem]] (for points and lines in the [[Euclidean plane]], or in higher-dimensional [[Euclidean space]]s) that for every [[finite set]] of points there is a line containing only two of the points.\n\n==References==\n{{reflist}}\n\n[[Category:Matroid theory]]"
    },
    {
      "title": "Sylvester–Gallai theorem",
      "url": "https://en.wikipedia.org/wiki/Sylvester%E2%80%93Gallai_theorem",
      "text": "{{redirect|Sylvester's problem|the matrix equation|Sylvester equation}}\n{{redirect|Sylvester theorem|other theorems named after Sylvester|Sylvester's theorem (disambiguation)}}\n[[File:Ordinary lines.svg|thumb|upright=0.75|Three of the ordinary lines in a 4&nbsp;&times;&nbsp;4 grid of points]]\nThe '''Sylvester–Gallai theorem''' in [[geometry]] states that, given a [[finite set|finite]] number of points in the [[Euclidean plane]], either\n* all the points [[Collinearity|lie on a single line]]; or\n* there is a line which contains exactly two of the points.\nIt is named after [[James Joseph Sylvester]], who posed it as a problem in 1893, and [[Tibor Gallai]], who published one of the first proofs of this theorem in 1944.\n\nA line that contains exactly two of a set of points is known as an ''ordinary line''. According to a strengthening of the theorem, every finite point set (not all on a line) has at least a linear number of ordinary lines. There is an [[algorithm]] that finds an ordinary line in a set of ''n'' points in time proportional to [[n log n|''n'' log ''n'']] in the [[worst case analysis|worst case]].<ref name=mukh97>{{harvtxt|Mukhopadhyay|Agrawal|Hosabettu|1997}}; {{harvtxt|Mukhopadhyay|Greene|2007}}.</ref>\n\n==History==\nThe Sylvester–Gallai theorem was posed as a problem by {{harvs|txt|authorlink=James Joseph Sylvester|first=J. J.|last=Sylvester|year=1893}}.  {{harvs|txt|authorlink=Leroy Milton Kelly|last=Kelly|year=1986}} suggests that Sylvester may have been motivated by a related phenomenon in [[algebraic geometry]], in which the [[inflection point]]s of a [[cubic curve]] in the [[complex projective plane]] form a [[configuration (geometry)|configuration]] of nine points and twelve lines (the [[Hesse configuration]]) in which each line determined by two of the points contains a third point. The Sylvester–Gallai theorem implies that it is impossible for all nine of these points to have real coordinates.{{sfnp|Elkies|Pretorius|Swanepoel|2006}}\n\n{{harvtxt|Woodall|1893}} claimed to have a short proof of the Sylvester–Gallai theorem, but it was already noted to be incomplete at the time of publication. {{harvs|txt|authorlink=Eberhard Melchior|first=Eberhard|last=Melchior|year=1941}} proved the theorem (and actually a slightly stronger result) in an equivalent formulation, its [[projective duality|projective dual]].  Unaware of Melchior's proof,{{sfnp|Borwein|Moser|1990}} {{harvs|txt|authorlink=Paul Erdős|first=Paul|last=Erdős|year=1943}} again stated the conjecture, which was proved first by [[Tibor Gallai]], and soon afterwards by other authors.<ref name=stein44>{{harvtxt|Steinberg|Buck|Grünwald|Steenrod|1944}}; {{harvtxt|Erdős|1982}}.</ref>\n\n== Projective and dual versions ==\nThe question of the existence of an ordinary line can also be posed for points in the real [[projective plane]] RP<sup>2</sup> instead of the [[Euclidean plane]]. The Euclidean plane can be viewed as a subset of the projective plane, but the additional points and lines of the projective plane do not change the problem, as any finite set of projective points can be transformed into a Euclidean point set without changing its set of ordinary lines. Therefore, any pattern of intersecting points and lines that exists in one of these two types of plane also exists in the other. However, the projective viewpoint allows certain configurations to be described more easily.\nBy [[projective duality]], the existence of an ordinary line for a set of non-collinear points in RP<sup>2</sup> is equivalent to the existence of an ''ordinary point'' in a nontrivial [[arrangement of lines|arrangement]] of finitely many lines. An arrangement is said to be trivial when all its lines pass through a common point, and nontrivial otherwise; an ordinary point is a point that belongs to exactly two lines.{{sfnp|Borwein|Moser|1990}}\n\n==Proofs==\nFor a description of Gallai's original proof of the theorem, see e.g. {{harvtxt|Borwein|Moser|1990}}.\n\n===Kelly's proof===\n[[File:Sylvester gallai kelly proof.svg|thumb|Notation for Kelly's proof]]\nThis proof is due to [[Leroy Milton Kelly]].\n\nSuppose that a finite set ''S'' of points is not all collinear. Define a connecting line to be a line that contains at least two points in the collection. By finiteness, there must exist a point ''P'' and a connecting line ''ℓ'' that are a positive distance apart but are closer than all other point-line pairs. We'll prove that ''ℓ'' is ordinary, [[Proof by contradiction|by contradiction]].\n\nAssume that ''ℓ'' is not ordinary. Then it goes through at least three points of ''S''. At least two of these are on the same side of ''P'<nowiki/>'', the perpendicular projection of ''P'' on ''ℓ''. Call them ''B'' and ''C'', with ''B'' being closest to ''P'<nowiki/>'' (and possibly coinciding with it'')''. Draw the connecting line ''m'' passing through ''P'' and ''C,'' and the perpendicular from ''B'' to ''B'<nowiki/>'' on ''m'' . Then ''BB'<nowiki/>'' is shorter than ''PP'''. This follows from the facts that ''PP'C'' and ''BB'C'' are [[similar triangle]]s, contained inside one another.\n\nHowever, this contradicts the original definition of ''P'' and ''ℓ'' as the point-line pair with the smallest positive distance. So the assumption that ''ℓ'' is not ordinary cannot be true, QED.\n\n===Melchior's proof===\n<!-- [[Melchior's inequality]] redirects here; do not change the section heading without also changing the redirect -->\nIn 1941 (thus, prior to Erdős publishing the question and Gallai's subsequent proof) Melchior showed that any nontrivial finite arrangement of lines in the projective plane has at least three ordinary points.  By duality, this results also says that any finite nontrivial set of points on the plane has at least three ordinary lines.{{sfnp|Melchior|1941}}\n\nMelchior observed that, for any graph [[graph embedding|embedded]] in the real projective plane, the formula ''V''&nbsp;&minus;&nbsp;''E''&nbsp;+&nbsp;''F'' must equal 1, the [[Euler characteristic]] of the projective plane. Here ''V'', ''E'', and ''F'', are the number of vertices, edges, and faces of the graph, respectively. Any  nontrivial line arrangement on the projective plane defines a graph in which each face is bounded by at least three edges, and each edge bounds two faces; so, [[double counting (proof technique)|double counting]] gives the additional inequality ''F''&nbsp;≤&nbsp;2''E''/3. Using this inequality to eliminate ''F'' from the Euler characteristic leads to the inequality ''E''&nbsp;≤&nbsp;3''V''&nbsp;&minus;&nbsp;3. But if every vertex in the arrangement were the crossing point of three or more lines, then the total number of edges would be at least 3''V'', contradicting this inequality. Therefore, some vertices must be the crossing point of only two lines, and as Melchior's more careful analysis shows, at least three ordinary vertices are needed in order to satisfy the inequality ''E''&nbsp;≤&nbsp;3''V''&nbsp;&minus;&nbsp;3.{{sfnp|Melchior|1941}}\n\n====Melchior's inequality====\nBy a similar argument, Melchior was able to prove a more general result.  For every ''k''&nbsp;≥&nbsp;2, let ''t''<sub>''k''</sub> be the number of points to which ''k'' lines are incident. Then{{sfnp|Melchior|1941}}\n:<math>\\displaystyle \\sum_{k\\geq2} (k-3) t_k \\leq -3.</math>\nor equivalently,\n\n:<math>\\displaystyle t_2 \\geqslant 3 + \\sum_{k\\geq4} (k-3) t_k.</math>\n\n===Coxeter's proof===\n{{harvs|txt|authorlink=Harold Scott MacDonald Coxeter|last=Coxeter|first=H. S. M.|year=1969}} gave another proof of the Sylvester–Gallai theorem within [[ordered geometry]], an axiomatization of geometry that includes not only Euclidean geometry but several other related geometries. See  {{harvs|txt|last=Pambuccian|year=2009}} for minimal axiom systems inside which the Sylvester–Gallai theorem can be proved.\n\n==The number of ordinary lines==\n[[Image:Few-ordinary-lines.svg|thumb|360px|The two known examples of point sets with fewer than n/2 ordinary lines.]]\nWhile the Sylvester–Gallai theorem states that an arrangement of points, not all collinear, must determine an ordinary line, it does not say how many must be determined.\n\nLet {{nowrap|''t''<sub>2</sub>(''n'')}} be the minimum number of ordinary lines determined over every set of ''n'' non-collinear points. Melchior's proof showed that {{nowrap|''t''<sub>2</sub>(''n'') ≥ 3.}} {{harvs|txt|author2-link=Paul Erdős|last2=Erdős|last1=de Bruijn|author1-link=Nicolaas Govert de Bruijn|year=1948}} raised the question of whether {{nowrap|''t''<sub>2</sub>(''n'')}} approaches infinity with ''n''. {{harvs|txt|authorlink=Theodore Motzkin|last=Motzkin|first=Theodore|year=1951}} confirmed that it does by proving that <math>t_2\\ge\\sqrt n</math>.  {{harvs|txt|authorlink=Gabriel Andrew Dirac|first=Gabriel|last=Dirac|year=1951}} conjectured that <math>t_2\\ge\\lfloor n/2\\rfloor</math>, for all values of ''n'', a conjecture that still stands {{as of|2013|lc=on}}.  This is often referred to as the ''Dirac-Motzkin conjecture'', see for example {{harvtxt|Brass|Moser|Pach|2005|p=304}}. {{harvtxt|Kelly|Moser|1958}} proved that ''t''<sub>2</sub>(''n'') ≥ 3''n''/7.\n\n[[Image:Boroczky-config-even.svg|thumb|200px|Example of Böröczky's (even) configuration with 10 points determining 5 ordinary lines.]]\nDirac's conjectured lower bound is asymptotically the best possible, since there is a proven matching upper bound {{nowrap|''t''<sub>2</sub>(''n'') ≤ ''n''/2}} for even ''n'' greater than four. The construction, due to [[Károly Böröczky]], that achieves this bound consists of the vertices of a regular ''m''-gon in the real [[projective plane]] and another ''m'' points (thus, {{nowrap|1=''n'' = 2''m''}}) on the line at infinity corresponding to each of the directions determined by pairs of vertices; although there are {{nowrap|''m''(''m'' &minus; 1)/2}} pairs, they determine only ''m'' distinct directions. This arrangement has only ''m'' ordinary lines, namely those that connect a vertex ''v'' with the point at infinity corresponding to the line determined by ''v'''s two neighboring vertices. Note that, as with any finite configuration in the real projective plane, this construction can be perturbed so that all points are finite, without changing the number of ordinary lines.<ref name=\"CM68\"/>\n\nFor odd ''n'', only two examples are known that match Dirac's lower bound conjecture, that is, with {{nowrap|1=''t''<sub>2</sub>(''n'') = (''n'' &minus; 1)/2.}} One example, by {{harvtxt|Kelly|Moser|1958}}, consists of the vertices, edge midpoints, and centroid of an equilateral triangle; these seven points determine only three ordinary lines. The [[projective configuration|configuration]] in which these three ordinary lines are replaced by a single line cannot be realized in the Euclidean plane, but forms a finite [[projective geometry|projective space]] known as the [[Fano plane]]. Because of this connection, the Kelly–Moser example has also been called the non-Fano configuration.<ref name=\"ggk00\"/> The other counterexample, due to McKee,<ref name=\"CM68\">{{harvtxt|Crowe|McKee|1968}}.</ref> consists of two regular pentagons joined edge-to-edge together with the midpoint of the shared edge and four points on the line at infinity in the [[projective plane]]; these 13 points have among them 6 ordinary lines. Modifications of Böröczky's construction lead to sets of odd numbers of points with <math>3\\lfloor n/4\\rfloor</math> ordinary lines.<ref name=\"ps09\">{{harvtxt|Pach|Sharir|2009}}</ref>\n\n{{harvtxt|Csima|Sawyer|1993}} proved that <math>t_2(n)\\ge\\lceil 6n/13\\rceil</math> except when ''n'' is seven. Asymptotically, this formula is already 12/13 ~ 92.3% of the proven ''n''/2 upper bound. The ''n''&nbsp;=&nbsp;7 case is an exception because otherwise the Kelly–Moser construction would be a counterexample; their construction shows that ''t''<sub>2</sub>(7)&nbsp;≤&nbsp;3. However, were the Csima–Sawyer bound valid for ''n''&nbsp;=&nbsp;7, it would claim that ''t''<sub>2</sub>(7)&nbsp;≥&nbsp;4.\n\nA closely related result is [[Beck's theorem (geometry)|Beck's theorem]], stating a tradeoff between the number of lines with few points and the number of points on a single line.\n\n[[Ben Green (mathematician)|Ben Green]] and [[Terence Tao]] showed that for all sufficiently large point sets, ''n'' &gt; ''n''<sub>0</sub>, the number of ordinary lines is indeed at least ''n''/2. Furthermore, when ''n'' is [[Parity (mathematics)|odd]], the number of ordinary lines is at least 3''n''/4&nbsp;&minus;&nbsp;''C'', for some constant ''C''. Thus, the constructions of Böröczky for even and odd (discussed above) are best possible.{{sfnp|Green|Tao|2013}}\n\n==The number of connecting lines==\nAs [[Paul Erdős]] observed, the Sylvester–Gallai theorem immediately implies that any set of ''n'' points that are not collinear determines at least ''n'' different lines. As a base case, the result is clearly true for ''n''&nbsp;=&nbsp;3. For any larger value of ''n'', the result can be reduced from ''n'' points to ''n''&nbsp;&minus;&nbsp;1 points, by deleting an ordinary line and one of the two points on it. Thus, it follows by mathematical induction. The example of a near-pencil (a set of ''n''&nbsp;&minus;&nbsp;1 collinear points together with one additional point that is not on the same line as the other points) shows that this bound is tight.<ref name=\"ps09\"/>\n\n==Generalizations==\n[[File:Hesse configuration.svg|thumb|The [[Hesse configuration]], in which the line through every pair of points contains a third point. The Sylvester–Gallai theorem shows that it cannot be realized by straight lines in the Euclidean plane, but it has a realization in the [[complex projective plane]].]]\nThe Sylvester–Gallai theorem does not directly apply to sets of infinitely many points or to geometries over finite fields. The set of all points in the plane is an infinite set with no ordinary lines, for instance, and the set of all points in a finite geometry also has no ordinary lines.\n\nFor geometries defined using [[complex number]] or [[quaternion]] coordinates, however, the situation is more complicated. For instance, in the [[complex projective plane]] there exists a [[Configuration (geometry)|configuration]] of nine points, [[Hesse's configuration]] (the inflection points of a cubic curve), in which every line is non-ordinary, violating the Sylvester–Gallai theorem. Such a configuration is known as a [[Sylvester–Gallai configuration]], and it cannot be realized by points and lines of the Euclidean plane. Another way of stating the Sylvester–Gallai theorem is that whenever the points of a Sylvester–Gallai configuration are embedded into a Euclidean space, preserving colinearities, the points must all lie on a single line, and the example of the Hesse configuration shows that this is false for the [[complex projective plane]]. However, {{harvtxt|Kelly|1986}} proved a complex-number analogue of the Sylvester–Gallai theorem: whenever the points of a Sylvester–Gallai configuration are embedded into a complex projective space, the points must all lie in a two-dimensional subspace. Similarly, {{harvtxt|Elkies|Pretorius|Swanepoel|2006}} showed that whenever they are embedded into a space defined over the quaternions, they must lie in a three-dimensional subspace.\n\nEvery set of points in the plane, and the lines connecting them, may be abstracted as the elements and flats of a rank-3 [[oriented matroid]]. In this context, the result of {{harvtxt|Kelly|Moser|1958}} lower-bounding the number of ordinary lines can be generalized to oriented matroids: every rank-3 oriented matroid with ''n'' elements has at least 3''n''/7 two-point lines, or equivalently every rank-3 [[matroid]] with fewer two-point lines must be non-orientable.<ref>{{harvtxt|Björner|Las Vergnas|Sturmfels|White|1993}}.</ref> A matroid without any two-point lines is called a [[Sylvester matroid]]. Relatedly, the Kelly–Moser configuration with seven points and only three ordinary lines forms one of the [[matroid minor|forbidden minors]] for [[Rota's conjecture|GF(4)-representable matroids]].<ref name=\"ggk00\">{{harvtxt|Geelen|Gerards|Kapoor|2000}}.</ref>\n\n==See also==\n*[[List of topics named after James Joseph Sylvester]]\n*The ''[[de Bruijn–Erdős theorem (incidence geometry)|de Bruijn–Erdős theorem]]'', a consequence of this theorem, states that a set of ''n'' noncollinear points determines at least ''n'' lines.\n*[[Orchard-planting problem]]\n\n==Notes==\n{{reflist|30em}}\n\n==References==\n{{refbegin|30em}}\n*{{citation\n | last1 = Björner | first1 = Anders | author1-link = Anders Björner\n | last2 = Las Vergnas | first2 = Michel | author2-link = Michel Las Vergnas\n | last3 = Sturmfels | first3 = Bernd | author3-link = Bernd Sturmfels\n | last4 = White | first4 = Neil\n | last5 = Ziegler | first5 = Günter M. | author5-link = Günter M. Ziegler\n | isbn = 0-521-41836-4\n | location = Cambridge\n | mr = 1226888\n | page = 273\n | publisher = Cambridge University Press\n | series = Encyclopedia of Mathematics and its Applications\n | title = Oriented matroids\n | volume = 46\n | year = 1993}}.\n*{{citation | last1 = Borwein | first1 = P. | author1-link = Peter Borwein | last2 = Moser | first2 = W. O. J.| title = A survey of Sylvester's problem and its generalizations| journal = [[Aequationes Mathematicae]] | volume = 40 | issue = 1 | year = 1990 | doi = 10.1007/BF02112289 | pages = 111–135}}.\n* {{Citation |last1=Brass|first1=Peter|last2=Moser|first2=William|last3=Pach|first3=János|authorlink3=János Pach|title=Research problems in discrete geometry |publisher=Springer |location=Berlin |year=2005 |isbn=0-387-23815-8|ref=harv}}.\n*{{citation | last1 = de Bruijn | first1 = N. G. | author1-link = Nicolaas Govert de Bruijn | last2 = Erdős | first2 = P.| author2-link = Paul Erdős| title = A combinatioral &#91;sic&#93; problem | journal = Indagationes Mathematicae | year = 1948 | volume = 10 | pages = 421–423 | url = http://www.renyi.hu/~p_erdos/1948-01.pdf }}.\n*{{citation | last=Coxeter | first=H. S. M. | authorlink = Harold Scott MacDonald Coxeter | pages=181–182 | title=Introduction to Geometry | location=New York | publisher=John Wiley & Sons | year=1969 | isbn=0-471-50458-0 }}.\n*{{citation  | doi = 10.2307/2687957  | last1 = Crowe | first1 = D. W. | last2 = McKee | first2 = T. A.| title = Sylvester's problem on collinear points | journal = [[Mathematics Magazine]] | volume = 41| issue = 1| year = 1968| pages = 30–34| jstor = 2687957}}.\n*{{citation| last1 = Csima | first1 = J. | last2 = Sawyer | first2 = E.| title = There exist 6''n''/13 ordinary points| journal = Discrete & Computational Geometry| year = 1993| volume = 9| pages = 187–202| doi = 10.1007/BF02189318}}.\n*{{citation| last = Dirac | first = G. | authorlink = Gabriel Andrew Dirac | title = Collinearity properties of sets of points | journal = Quart. J. Math. | year = 1951 | volume = 2 | pages = 221–227  | doi = 10.1093/qmath/2.1.221 | bibcode = 1951QJMat...2..221D }}.\n*{{citation\n | last1 = Elkies | first1 = Noam | author1-link = Noam Elkies\n | last2 = Pretorius | first2 = Lou M.\n | last3 = Swanepoel | first3 = Konrad J.\n | arxiv = math/0403023\n | doi = 10.1007/s00454-005-1226-7\n | issue = 3\n | journal = [[Discrete and Computational Geometry]]\n | mr = 2202107\n | pages = 361–373\n | title = Sylvester–Gallai theorems for complex numbers and quaternions\n | volume = 35\n | year = 2006}}.\n*{{citation | last = Erdős | first = P. | authorlink = Paul Erdős | title = Problem 4065 | journal = [[American Mathematical Monthly]] | volume = 50 | issue = 1 | year = 1943 | jstor = 2304011 | pages = 65–66 | doi = 10.2307/2304011 | department = Problems for solution: 4065–4069}}.\n*{{citation|first=P.|last=Erdős|authorlink=Paul Erdős|url=http://www.math-inst.hu/~p_erdos/1982-22.pdf|title=Personal reminiscences and remarks on the mathematical work of Tibor Gallai|journal=Combinatorica|volume=2|year=1982|pages=207–212|doi=10.1007/BF02579228|issue=3}}.\n*{{citation\n |last1        = Geelen\n |first1       = J. F.\n |author1-link = Jim Geelen\n |last2        = Gerards\n |first2       = A. M. H.\n |last3        = Kapoor\n |first3       = A.\n |doi          = 10.1006/jctb.2000.1963\n |issue        = 2\n |journal      = [[Journal of Combinatorial Theory]]\n |mr           = 1769191\n |pages        = 247–299\n |series       = Series B\n |title        = The excluded minors for GF(4)-representable matroids\n |url          = http://www.math.uwaterloo.ca/~jfgeelen/Publications/gf4.pdf\n |volume       = 79\n |year         = 2000\n |deadurl      = yes\n |archiveurl   = https://web.archive.org/web/20100924110912/http://www.math.uwaterloo.ca/~jfgeelen/publications/gf4.pdf\n |archivedate  = 2010-09-24\n |df           = \n}}.\n*{{citation |last1=Green|first1=Ben|author1-link=Ben Green (mathematician)|last2=Tao|first2=Terence|author2-link=Terence Tao|title=On sets defining few ordinary lines|journal=Discrete & Computational Geometry|year=2013|volume=50|issue=2|pages=409–468|arxiv=1208.4714|doi=10.1007/s00454-013-9518-9|mr=3090525}}\n*{{citation| first = L. M. | last = Kelly|authorlink = Leroy Milton Kelly| title = A resolution of the Sylvester–Gallai problem of J. P. Serre| journal = [[Discrete and Computational Geometry]]| volume = 1| issue = 2| pages = 101–104| doi = 10.1007/BF02187687| year = 1986}}.\n*{{citation | last1 = Kelly|first1= L. M.|author1-link=Leroy Milton Kelly|last2= Moser|first2= W. O. J. | title = On the number of ordinary lines determined by ''n'' points | journal = Can. J. Math. | volume = 10 | year = 1958 | pages = 210–219 | doi = 10.4153/CJM-1958-024-6}}.\n*{{ citation | last = Mandelkern | first = Mark | title = A constructive version of the Sylvester–Gallai theorem | journal = Acta Mathematica Hungarica | year = 2016 | volume = 150 | pages = 121-130 | doi = 10.1007/s10474-016-0624-z  }}.\n*{{citation|first=E.|last=Melchior|authorlink=Eberhard Melchior|year=1941|title=Über Vielseite der Projektive Ebene|journal=[[Deutsche Mathematik]]|volume=5|pages=461–475}}.\n*{{citation | last = Motzkin | first = Th. |authorlink=Theodore Motzkin| title = The lines and planes connecting the points of a finite set | journal = Transactions of the American Mathematical Society | volume = 70 | issue = 3 | year = 1951 | pages = 451–464 | jstor = 1990609 | doi = 10.2307/1990609 }}.\n*{{citation | last1 = Mukhopadhyay | first1 = A. | last2 = Agrawal | first2 = A. | last3 = Hosabettu | first3 = R. M.| title = On the ordinary line problem in computational geometry| journal = Nordic Journal of Computing| volume = 4 | issue = 4| year = 1997| pages = 330–341}}.\n*{{citation | last1 = Mukhopadhyay | first1 = A. | last2 = Greene | first2 = E. | contribution = The ordinary line problem revisited | title = Proc. 19th Canadian Conference on Computational Geometry | pages = 61–64 | url = http://cccg.ca/proceedings/2007/03a4.pdf | year = 2007}}.\n*{{citation\n | last1 = Pach | first1 = János | author1-link = János Pach\n | last2 = Sharir | first2 = Micha | author2-link = Micha Sharir\n | contribution = Chapter 1. Sylvester–Gallai Problem: The Beginnings of Combinatorial Geometry\n | pages = 1–12\n | publisher = [[American Mathematical Society]]\n | series = Mathematical Surveys and Monographs\n | title = Combinatorial Geometry and Its Algorithmic Applications: The Alcalá Lectures\n | volume = 152\n | year = 2009}}.\n*{{citation | last = Pambuccian | first = V. | title = A reverse analysis of the Sylvester–Gallai theorem |journal = Notre Dame Journal of Formal Logic | volume = 50 | issue = 3 | year = 2009 | pages = 245–260 | url = http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.ndjfl/1257862037 | doi= 10.1215/00294527-2009-010 }}.\n*{{citation | last1 = Steinberg | first1 = R. | last2 = Buck | first2 = R. C. | last3 = Grünwald | first3 = T. | author3-link = Tibor Gallai | last4 = Steenrod | first4 = N. E. | title = Three point collinearity  (solution to problem 4065) | journal = [[American Mathematical Monthly]] | volume = 51 | issue = 3 | year = 1944 | pages = 169–171  | jstor = 2303021 | doi = 10.2307/2303021}}.\n*{{citation| last = Sylvester | first = J. J. | authorlink = J. J. Sylvester| title = Mathematical question 11851 | journal = The Educational Times | volume = 46 | issue=383| year = 1893 | pages = 156}}.\n*{{citation|last=Woodall|first=H. J.|authorlink=H. J. Woodall|title=Mathematical question 11851 answered|journal=The Educational Times|volume=46|issue=385|page=231|year=1893}}. \n*{{citation|last=Woodall|first=H. J.|authorlink=H. J. Woodall|title=Mathematical question 11851 answered|journal=Mathematical Questions and Solutions from the Educational Times|volume=59|page=98|year=1893|url = https://books.google.com/books?id=qtUGAAAAYAAJ&pg=PA98#v=onepage&q=%2211851%22%20woodall&f=false }}. \n{{refend}}\n\n== External links ==\n*{{cite web\n |author      = Malkevitch, Joseph\n |year        = 2003\n |title       = A discrete geometrical gem\n |url         = http://e-math.ams.org/featurecolumn/archive/sylvester1.html\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20061010192721/http://e-math.ams.org/featurecolumn/archive/sylvester1.html\n |archivedate = 2006-10-10\n |df          = \n}}\n*{{mathworld | title = Ordinary Line | urlname = OrdinaryLine}}\n*[https://www.youtube.com/watch?v=6mG9HG4lfgI&t=12m16s Proof presentation] by [[Terence Tao]] at the 2013 Minerva Lectures\n\n{{DEFAULTSORT:Sylvester-Gallai theorem}}\n[[Category:Euclidean plane geometry]]\n[[Category:Theorems in discrete geometry]]\n[[Category:Matroid theory]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Transcendence degree",
      "url": "https://en.wikipedia.org/wiki/Transcendence_degree",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Cardinality of a maximally algebraically independent set of a field extension}}\n{{Plain English|date=December 2018}}\n\nIn [[abstract algebra]], the '''transcendence degree''' of a [[field extension]] ''L'' /''K'' is a certain rather coarse measure of the \"size\" of the extension. Specifically, it is defined as the largest [[cardinality]] of an [[Algebraic independence|algebraically independent]] [[subset]] of ''L'' over ''K''.\n\nA subset ''S'' of ''L'' is a '''transcendence basis''' of ''L'' /''K'' if it is algebraically independent over ''K'' and if furthermore ''L'' is an [[algebraic extension]] of the field ''K''(''S'') (the field obtained by adjoining the elements of ''S'' to ''K''). One can show that every field extension has a transcendence basis, and that all transcendence bases have the same cardinality; this cardinality is equal to the transcendence degree of the extension and is denoted trdeg<sub>''K''</sub>&nbsp;''L'' or trdeg(''L'' /''K'').\n\nIf no field ''K'' is specified, the transcendence degree of a field ''L'' is its degree relative to the [[prime field]] of the same [[characteristic (algebra)|characteristic]], i.e., '''Q''' if ''L'' is of characteristic 0 and '''F'''<sub>''p''</sub> if ''L'' is of characteristic ''p''.\n\nThe field extension ''L'' /''K'' is '''purely transcendental''' if there is a subset ''S'' of ''L'' that is algebraically independent over ''K'' and such that ''L'' = ''K''(''S'').\n \n== Examples ==\n\n*An extension is algebraic if and only if its transcendence degree is 0; the [[empty set]] serves as a transcendence basis here.\n*The field of rational functions in ''n'' variables ''K''(''x''<sub>1</sub>,...,''x''<sub>''n''</sub>) is a purely transcendental extension with transcendence degree ''n'' over ''K''; we can for example take {''x''<sub>1</sub>,...,''x''<sub>''n''</sub>} as a transcendence base.\n*More generally, the transcendence degree of the [[function field of an algebraic variety|function field]] ''L'' of an ''n''-dimensional [[algebraic variety]] over a ground field ''K'' is ''n''.\n*'''Q'''([[square root of two|&radic;2]], [[E (mathematical constant)|''e'']]) has transcendence degree 1 over '''Q''' because √2 is [[algebraic number|algebraic]] while ''e'' is [[transcendental number|transcendental]]. \n*The transcendence degree of '''C''' or '''R''' over '''Q''' is the [[continuum hypothesis|cardinality of the continuum]]. (This follows since any element has only countably many algebraic elements over it in '''Q''', since '''Q''' is itself countable.)\n*The transcendence degree of '''Q'''(''e'',  [[pi|π]]) over '''Q''' is either 1 or 2; the precise answer is unknown because it is not known whether ''e'' and π are algebraically independent.\n\n== Analogy with vector space dimensions ==\n\nThere is an analogy with the theory of [[vector space]] [[Hamel dimension|dimension]]s.  The analogy matches algebraically independent sets with [[linear independence|linearly independent sets]]; sets ''S'' such that ''L'' is algebraic over ''K''(''S'') with [[linear span|spanning sets]]; transcendence bases with [[Basis (linear algebra)|bases]]; and transcendence degree with dimension.  The fact that transcendence bases always exist (like the fact that bases always exist in linear algebra) requires the [[axiom of choice]].  The proof that any two bases have the same cardinality depends, in each setting, on an [[exchange lemma]].<ref>J.S. Milne, ''[http://www.jmilne.org/math/CourseNotes/FT.pdf Fields and Galois Theory]'', pp.100-101.</ref>\n\nThis analogy can be made more formal, by observing that linear independence in vector spaces and algebraic independence in field extensions both form examples of [[matroid]]s, called linear matroids and algebraic matroids respectively. Thus, the transcendence degree is the [[Matroid rank|rank function]] of an algebraic matroid. Every linear matroid is isomorphic to an algebraic matroid, but not vice versa.<ref>{{citation|title=Applied Discrete Structures|first=K. D.|last=Joshi|publisher=New Age International|year=1997|isbn=9788122408263|page=909|url=https://books.google.com/books?id=lxIgGGJXacoC&pg=PA909&lpg=PA909}}.</ref>\n\n== Facts ==\n\nIf ''M''/''L'' is a field extension and ''L'' /''K'' is another field extension, then the transcendence degree of ''M''/''K'' is equal to the sum of the transcendence degrees of ''M''/''L'' and ''L''/''K''. This is proven by showing that a transcendence basis of ''M''/''K'' can be obtained by taking the [[union (set theory)|union]] of a transcendence basis of ''M''/''L'' and one of ''L'' /''K''.\n\n== Applications ==\n\nTranscendence bases are a useful tool to prove various existence statements about field homomorphisms. Here is an example: Given an [[algebraically closed]] field ''L'', a [[Field extension|subfield]] ''K'' and a field [[automorphism]] ''f'' of ''K'', there exists a field automorphism of ''L'' which extends ''f'' (i.e. whose restriction to ''K'' is ''f''). For the proof, one starts with a transcendence basis ''S'' of ''L''/''K''. The elements of ''K''(''S'') are just quotients of polynomials in elements of ''S'' with coefficients in ''K''; therefore the automorphism ''f'' can be extended to one of ''K''(''S'') by sending every element of ''S'' to itself. The field ''L'' is the [[algebraic closure]] of ''K''(''S'') and algebraic closures are unique up to isomorphism; this means that the automorphism can be further extended from ''K''(''S'') to ''L''.\n\nAs another application, we show that there are (many) proper subfields of the [[complex number|complex number field]] '''C''' which are (as fields) isomorphic to '''C'''. For the proof, take a transcendence basis ''S'' of '''C'''/'''Q'''. ''S'' is an infinite (even uncountable) set, so there exist (many) maps ''f'': ''S'' → ''S'' which are [[injective]] but not [[surjective]]. Any such map can be extended to a field homomorphism '''Q'''(''S'') → '''Q'''(''S'') which is not surjective. Such a field homomorphism can in turn be extended to the algebraic closure '''C''', and the resulting field homomorphisms '''C''' → '''C''' are not surjective.\n\nThe transcendence degree can give an intuitive understanding of the size of a field. For instance, a theorem due to [[Carl Ludwig Siegel|Siegel]] states that if ''X'' is a compact, connected, complex manifold of dimension ''n'' and ''K''(''X'') denotes the field of (globally defined) [[meromorphic function]]s on it, then trdeg<sub>'''C'''</sub>(''K''(''X''))&nbsp;≤&nbsp;''n''.\n\n==References==\n<references/>\n\n[[Category:Field theory]]\n[[Category:Algebraic varieties]]\n[[Category:Matroid theory]]\n[[Category:Transcendental numbers]]"
    },
    {
      "title": "Tutte homotopy theorem",
      "url": "https://en.wikipedia.org/wiki/Tutte_homotopy_theorem",
      "text": "In mathematics, the '''Tutte homotopy theorem''', introduced by {{harvs|txt|last=Tutte|authorlink=William Thomas Tutte|year=1958}}, generalises the concept of \"path\" from [[Graph (discrete mathematics)|graphs]] to [[matroid]]s, and states roughly that closed paths can be written as compositions of elementary closed paths, so that in some sense they are homotopic to the trivial closed path.\n\n==Statement==\n\nA [[matroid]] on a set ''Q'' is specified by a class of non-empty subsets ''M'' of ''Q'', called '''circuits''', such that no element of ''M'' contains another, and if ''X'' and ''Y'' are in ''M'', ''a'' is in ''X'' and ''Y'', ''b'' is in ''X'' but not in ''Y'', then there is some ''Z'' in ''M'' containing ''b'' but not ''a'' and contained in ''X''∪''Y''.\n\nThe subsets of ''Q'' that are unions of circuits are called '''flats'''.  The elements of ''M'' are called 0-flats, the minimal non-empty flats that are not 0-flats are called 1-flats, the minimal nonempty flats that are not 0-flats or 1-flats are called 2-flats, and so on.\n\nA '''path''' is a finite sequence of 0-flats such that any two consecutive elements of the path lie in some 1-flat.\n\nAn '''elementary path''' is one of the form (''X'',''Y'',''X''), or (''X'',''Y'',''Z'',''X'') with ''X'',''Y'',''Z''  all lying in some 2-flat.\n\nTwo paths ''P'' and ''Q'' such that the last 0-flat of ''P'' is the same as the first 0-flat of ''Q'' can be composed in the obvious way to give a path ''PQ''.\n\nTwo paths are called '''homotopic''' if one can be obtained from the other by the operations of adding or removing elementary paths inside a path, in other words changing a path ''PR'' to ''PQR'' or vice versa, where ''Q'' is elementary.\n\nA weak form of Tutte's homotopy theorem states that any closed path is homotopic to the trivial path. A stronger form states a similar result for paths not meeting certain \"convex\" subsets.\n\n==References==\n\n*{{Citation | last1=Tutte | first1=William Thomas  | author1-link=W. T. Tutte | title=A homotopy theorem for matroids. I | jstor= 1993243 |mr=0101526 | year=1958 | journal=[[Transactions of the American Mathematical Society]] | issn=0002-9947 | volume=88 | pages=144–160 | doi=10.2307/1993243}}\n*{{Citation | last1=Tutte | first1=William Thomas | title=A homotopy theorem for matroids. II | jstor= 1993244 |mr=0101526 | year=1958 | journal=[[Transactions of the American Mathematical Society]] | issn=0002-9947 | volume=88 | pages=161–174 | doi=10.2307/1993244}}\n*{{citation | zbl=0231.05027 | last=Tutte | first=W.T. | authorlink=W. T. Tutte | title=Introduction to the theory of matroids | series=Modern Analytic and Computational Methods in Science and Mathematics | volume=37 | location=New York | publisher=American Elsevier Publishing Company | year=1971 | pages=72–77 }} \n*{{Citation | last1=White | first1=Neil | editor1-last=White | editor1-first=Neil | title=Combinatorial geometries | url=https://books.google.com/books?id=xKPZlhqMqnQC | publisher=[[Cambridge University Press]] | series=Encyclopedia of Mathematics and its Applications | isbn=978-0-521-33339-9 |mr=921064 | year=1987 | volume=29 | chapter=Unimodular matroids | chapterurl=https://books.google.com/books?id=xKPZlhqMqnQC&pg=PA40 | pages=40–52 | doi=10.1017/CBO9781107325715}}\n\n[[Category:Matroid theory]]"
    },
    {
      "title": "Tutte polynomial",
      "url": "https://en.wikipedia.org/wiki/Tutte_polynomial",
      "text": "{{about|the Tutte polynomial of a graph|the Tutte polynomial of a matroid|Matroid}}\n\n[[Image:Tutte polynomial and chromatic polynomial of the Bull graph.jpg|thumb|300px|right|The polynomial <math>x^4+x^3+x^2y</math> is the Tutte polynomial of the [[bull graph]]. The red line shows the intersection with the plane <math>y=0</math>, equivalent to the chromatic polynomial.]]\n\nThe '''Tutte polynomial''', also called the '''dichromate''' or the '''Tutte–Whitney polynomial''', is a [[graph polynomial]]. It is a [[polynomial]] in two variables which plays an important role in [[graph theory]]. It is defined for every [[undirected graph]] <math>G</math> and contains information about how the graph is connected. It is denoted by <math>T_G</math>.\n\nThe importance of this polynomial stems from the information it contains about <math>G</math>. Though originally studied in [[algebraic graph theory]] as a generalization of counting problems related to [[graph coloring]] and [[nowhere-zero flow]], it contains several famous other specializations from other sciences such as the [[Jones polynomial]] from [[knot theory]] and the [[Partition function (statistical mechanics)|partition functions]] of the [[Potts model]] from [[statistical physics]]. It is also the source of several central [[computational problem]]s in [[theoretical computer science]].\n\nThe Tutte polynomial has several equivalent definitions. It is equivalent to Whitney’s '''rank polynomial''', Tutte’s own '''dichromatic polynomial''' and Fortuin–Kasteleyn’s '''[[random cluster model]]''' under simple transformations. It is essentially a [[generating function]] for the number of edge sets of a given size and connected components, with immediate generalizations to [[matroid]]s. It is also the most general [[graph invariant]] that can be defined by a [[Deletion–contraction formula|deletion–contraction recurrence]]. Several textbooks about graph theory and matroid theory devote entire chapters to it.<ref>{{harvnb|Bollobás|1998|loc=chapter 10}}.</ref><ref>{{harvnb|Biggs|1993|loc=chapter 13}}.</ref><ref>{{harvnb|Godsil|Royle|2004|loc=chap. 15}}.</ref>\n\n==Definitions==\n<blockquote>'''Definition.''' For an undirected graph <math>G=(V,E)</math> one may define the '''Tutte polynomial''' as\n\n:<math>T_G(x,y)=\\sum\\nolimits_{A\\subseteq E} (x-1)^{k(A)-k(E)}(y-1)^{k(A)+|A|-|V|},</math>\n\nwhere <math>k(A)</math> denotes the number of [[connected component (graph theory)|connected component]]s of the graph <math>(V,A)</math>. In this definition it is clear that <math>T_G</math> is well-defined and a polynomial in <math>x</math> and <math>y</math>.</blockquote>\n\nThe same definition can be given using slightly different notation by letting <math>r(A)=|V|-k(A)</math> denote the [[rank (graph theory)|rank]] of the graph <math>(V,A)</math>. Then the '''Whitney rank generating function''' is defined as\n\n:<math>R_G(u,v)=\\sum\\nolimits_{A\\subseteq E} u^{r(E)-r(A)} v^{|A|-r(A)}.</math>\n\nThe two functions are equivalent under a simple change of variables:\n\n:<math>T_G(x,y)=R_G(x-1,y-1).</math>\n\nTutte’s '''dichromatic polynomial''' <math>Q_G</math> is the result of another simple transformation:\n\n:<math>T_G(x,y)=(x-1)^{-k(G)} Q_G(x-1,y-1).</math>\n\nTutte’s original definition of <math>T_G</math> is equivalent but less easily stated. For connected <math>G</math> we set\n\n:<math>T_G(x,y)=\\sum\\nolimits_{i,j} t_{ij} x^iy^j,</math>\n\nwhere <math>t_{ij}</math> denotes the number of [[Spanning tree (mathematics)|spanning tree]]s of ''internal activity'' <math>i</math>and ''external activity'' <math>j</math>.\n\nA third definition uses a '''[[Deletion–contraction formula|deletion–contraction recurrence]]'''. The [[edge contraction]] <math>G/uv</math> of graph <math>G</math> is the graph obtained by merging the vertices <math>u</math> and <math>v</math> and removing the edge <math>uv</math>. We write <math>G - uv</math> for the graph where the edge <math>uv</math> is merely removed. Then the Tutte polynomial is defined by the recurrence relation\n\n:<math>T_G= T_{G-e}+T_{G/e},</math>\n\nif <math>e</math> is neither a [[Loop (graph theory)|loop]] nor a [[Bridge (graph theory)|bridge]], with base case\n\n:<math>T_G(x,y)= x^i y^j, </math>\n\nif <math>G</math> contains <math>i</math> bridges and <math>j</math> loops and no other edges. Especially, <math>T_G=1</math> if <math>G</math> contains no edges.\n\nThe '''[[random cluster model]]''' from statistical mechanics due to {{harvtxt|Fortuin|Kasteleyn|1972}} provides yet another equivalent definition.<ref>{{harvnb|Sokal|2005}}.</ref> The polynomial\n\n:<math>Z_G(q,w)=\\sum\\nolimits_{F\\subseteq E}q^{k(F)}w^{|F|}</math>\n\nis equivalent to <math>T_G</math> under the transformation<ref>{{harvnb|Sokal|2005|loc=eq. (2.26)}}.</ref>\n\n:<math>T_G(x, y)=(x-1)^{-k(E)}(y-1)^{-|V|} \\cdot Z_G\\Big((x-1)(y-1),\\; y-1\\Big).</math>\n\n===Properties===\nThe Tutte polynomial factors into connected components. If <math>G</math> is the union of disjoint graphs <math>H</math> and <math>H'</math> then\n: <math>T_G= T_H \\cdot T_{H'}</math>\n\nIf <math>G</math> is planar and <math>G^*</math> denotes its [[dual graph]] then\n\n: <math>T_G(x,y)= T_{G^*} (y,x)</math>\n\nEspecially, the chromatic polynomial of a planar graph is the flow polynomial of its dual. Tutte refers to such functions as '''V-functions'''.<ref name=\"Tutte-2004\" />\n\n===Examples===\nIsomorphic graphs have the same Tutte polynomial, but the converse is not true. For example, the Tutte polynomial of every tree on <math>m</math> edges is <math>x^m</math>.\n\nTutte polynomials are often given in tabular form by listing the coefficients <math>t_{ij}</math> of <math>x^iy^j</math> in row <math>i</math> and column <math>j</math>. For example, the Tutte polynomial of the [[Petersen graph]],\n\n:<math>\\begin{align}\n36 x &+ 120 x^2 + 180 x^3 + 170x^4+114x^5 + 56x^6 +21 x^7 + 6x^8 + x^9 \\\\\n&+ 36y +84 y^2 + 75 y^3 +35 y^4 + 9y^5+y^6 \\\\\n&+ 168xy + 240x^2y +170x^3y +70 x^4y + 12x^5 y \\\\\n&+ 171xy^2+105 x^2y^2 + 30x^3y^2 \\\\\n&+ 65xy^3 +15x^2y^3 \\\\\n&+10xy^4,\n\\end{align}</math>\n\nis given by the following table.\n\n{|class=\"wikitable\"\n |   0   ||36||   84||   75||   35||    9||    1\n|- \n| 36  ||168 || 171  || 65  || 10\n|-\n |120 || 240 || 105 ||  15\n|- \n|180 || 170  || 30\n|-\n|170  || 70\n |-\n |114  || 12\n|- \n| 56\n|-\n | 21\n |-\n|   6\n|-\n|    1\n|}\n\nOther example, the Tutte polynomial of the octahedron graph is given by\n:<math>\\begin{align}\n&12\\,{y}^{2}{x}^{2}+11\\,x+11\\,y+40\\,{y}^{3}+32\\,{\ny}^{2}+46\\,yx+24\\,x{y}^{3}+52\\,x{y}^{2} \\\\\n&+25\\,{x}^{2}+29\\,{y}^{4}+15\\,{y\n}^{5}+5\\,{y}^{6}+6\\,{y}^{4}x \\\\\n&+39\\,y{x}^{2}+20\\,{x}^{3}+{y}^{7}+8\\,y{x}^\n{3}+7\\,{x}^{4}+{x}^{5}\n\\end{align}</math>\n\n==History==\n[[W. T. Tutte]]’s interest in the [[deletion–contraction formula]] started in his undergraduate days at [[Trinity College, Cambridge]], originally motivated by [[perfect rectangle]]s and [[Spanning tree (mathematics)|spanning tree]]s. He often applied the formula in his research and “wondered if there were other interesting [[graph invariant|functions of graphs, invariant under isomorphism]], with similar recursion formulae.”<ref name=\"Tutte-2004\">{{harvnb|Tutte|2004}}.</ref> [[R. M. Foster]] had already observed that the [[chromatic polynomial]] is one such function, and Tutte began to discover more. His original terminology for graph invariants that satisfy the deletion–contraction recursion was ''W-function'', and ''V-function'' if multiplicative over components. Tutte writes, “Playing with my ''W-functions'' I obtained a two-variable polynomial from which either the chromatic polynomial or the flow-polynomial could be obtained by setting one of the variables equal to zero, and adjusting signs.”<ref name=\"Tutte-2004\"/> Tutte called this function the ''dichromate'', as he saw it as a generalization of the chromatic polynomial to two variables, but it is usually referred to as the Tutte polynomial.  In Tutte’s words, “This may be unfair to [[Hassler Whitney]] who knew and used analogous coefficients without bothering to affix them to two variables.” (There is “notable confusion”<ref>Welsh.</ref> about the terms ''dichromate'' and ''dichromatic polynomial'', introduced by Tutte in different paper, and which differ only slightly.) The generalisation of the Tutte polynomial to matroids was first published by Crapo, though it appears already in Tutte’s thesis.<ref name=\"Farr 2007\">{{harvnb|Farr|2007}}.</ref>\n\nIndependently of the work in [[algebraic graph theory]], Potts began studying the [[partition function (statistical mechanics)|partition function]] of certain models in [[statistical mechanics]] in 1952. The work by Fortuin and Kasteleyn<ref>{{harvnb|Fortuin|Kasteleyn|1972}}.</ref> on the [[random cluster model]], a generalisation of the [[Potts model]], provided a unifying expression that showed the relation to the Tutte polynomial.<ref name=\"Farr 2007\"/>\n\n== Specialisations==\nAt various points and lines of the <math>(x,y)</math>-plane, the Tutte polynomial evaluates to quantities that have been studied in their own right in diverse fields of mathematics and physics. Part of the appeal of the Tutte polynomial comes from the unifying framework it provides for analysing these quantities.\n\n===Chromatic polynomial===\n{{Main|Chromatic polynomial}}\n\n[[Image:Chromatic in the Tutte plane.jpg|thumb|right|The chromatic polynomial drawn in the Tutte plane]]\n\nAt <math>y=0</math>, the Tutte polynomial specialises to the chromatic polynomial,\n\n:<math>\\chi_G(\\lambda) = (-1)^{|V|-k(G)} \\lambda^{k(G)} T_G(1-\\lambda,0),</math>\nwhere <math>k(G)</math> denotes the number of connected components of ''G''.\n\nFor integer λ the value of chromatic polynomial <math>\\chi_G(\\lambda)</math> equals the number of [[vertex colouring]]s of ''G'' using a set of λ colours. It is clear that <math>\\chi_G(\\lambda)</math> does not depend on the set of colours. What is less clear is that it is the evaluation at λ of a polynomial with integer coefficients.  To see this, we observe:\n# If ''G'' has ''n'' vertices and no edges, then <math>\\chi_G(\\lambda) = \\lambda^n</math>.\n# If ''G'' contains a loop (a single edge connecting a vertex to itself), then <math>\\chi_G(\\lambda) = 0</math>.\n# If ''e'' is an edge which is not a loop, then\n::<math>\\chi_G(\\lambda) = \\chi_{G\\setminus e}(\\lambda) - \\chi_{G/e}(\\lambda).</math>\n\nThe three conditions above enable us to calculate <math>\\chi_G(\\lambda)</math>, by applying a sequence of edge deletions and contractions; but they give no guarantee that a different sequence of deletions and contractions will lead to the same value. The guarantee comes from the fact that <math>\\chi_G(\\lambda)</math> counts something, independently of the recurrence. In particular,\n\n:<math>T_G(2,0) = (-1)^{|V|} \\chi_G(-1)</math>\n\ngives the number of acyclic orientations.\n\n===Jones polynomial===\n{{Main|Jones polynomial}}\n\n[[Image:Jones in the Tutte plane.jpg|thumb|right|The Jones polynomial drawn in the Tutte plane]]\n\nAlong the hyperbola <math>xy=1</math>, the Tutte polynomial of a planar graph specialises to the [[Jones polynomial]] of an associated [[alternating knot]].\n\n===Individual points===\n\n====(2,1)====\n<math>T_G(2,1)</math> counts the number of [[tree (graph theory)|forest]]s, i.e., the number of acyclic edge subsets.\n\n====(1,1)====\n<math>T_G(1,1)</math> counts the number of spanning forests (edge subsets without cycles and the same number of connected components as ''G'').  If the graph is connected, <math>T_G(1,1)</math> counts the number of spanning trees.\n\n====(1,2)====\n<math>T_G(1,2)</math> counts the number of spanning subgraphs (edge subsets with the same number of connected components as ''G'').\n\n====(2,0)====\n<math>T_G(2,0)</math> counts the number of [[acyclic orientation]]s of ''G''.<ref name=\"Welsh-1999\">{{harvnb|Welsh|1999}}.</ref>\n\n====(0,2)====\n<math>T_G(0,2)</math> counts the number of [[Robbins theorem|strongly connected orientations]] of ''G''.<ref>{{harvnb|Las Vergnas|1980}}.</ref>\n\n====(2,2)====\n<math>T_G(2,2)</math> is the number <math>2^{|E|}</math> where <math>|E|</math> is the number of edges of graph ''G''.\n\n====(0,−2)====\nIf ''G'' is a 4-regular graph, then \n:<math>(-1)^{|V|+k(G)}T_G(0,-2)</math> \ncounts the number of [[Eulerian orientation]]s of ''G''. Here <math>k(G)</math> is the number of connected components of ''G''.<ref name=\"Welsh-1999\"/>\n\n====(3,3)====\nIf ''G'' is the ''m''&nbsp;×&nbsp;''n'' [[grid graph]], then <math>2 T_G(3,3)</math> counts the number of ways to tile a rectangle of width 4''m'' and height 4''n'' with [[tetromino|T-tetrominoes]].<ref>{{harvnb|Korn|Pak|2004}}.</ref><ref>See {{harvnb|Korn|Pak|2003}} for combinatorial interpretations of many other points.</ref>\n\nIf ''G'' is a [[planar graph]], then <math>2 T_G(3,3)</math> equals the sum over weighted Eulerian orientations in a [[medial graph]] of ''G'', where the weight of an orientation is 2 to the number of saddle vertices of the orientation (that is, the number of vertices with incident edges cyclicly ordered \"in, out, in out\").<ref>{{harvnb|Las Vergnas|1988}}.</ref>\n\n===Potts and Ising models===\n{{Main|Ising model|Potts model}}\n\n[[Image:Potts and Ising in the Tutte plane.jpg|thumb|right|The partition functions for the Ising model and the 3- and 4-state Potts models drawn in the Tutte plane.]]\n\nDefine the hyperbola in the ''xy''−plane:\n\n:<math> H_2: \\quad (x-1)(y-1)=2,</math>\n\nthe Tutte polynomial specialises to the partition function, <math>Z(\\cdot),</math> of the [[Ising model]] studied in [[statistical physics]]. Specifically, along the hyperbola <math>H_2</math> the two are related by the equation:<ref>{{harvnb|Welsh|1993|p=62}}.</ref>\n\n:<math>Z(G) = 2\\left(e^{-\\alpha}\\right)^{|E| - r(E)} \\left(4 \\sinh \\alpha \\right )^{r(E)}  T_G \\left (\\coth \\alpha, e^{2 \\alpha} \\right).</math>\n\nIn particular,\n\n:<math>(\\coth \\alpha - 1) \\left(e^{2 \\alpha} - 1 \\right ) = 2</math>\n\nfor all complex α.\n\nMore generally, if for any positive integer ''q'', we define the hyperbola:\n\n:<math>H_q: \\quad (x-1)(y-1)=q,</math>\n\nthen the Tutte polynomial specialises to the partition function of the ''q''-state [[Potts model]]. Various physical quantities analysed in the framework of the Potts model translate to specific parts of the <math>H_q</math>.\n\n{|class=\"wikitable\" style=\"margin: 1em auto 1em auto\"\n|+ Correspondences between the Potts model and the Tutte plane <ref>{{harvnb|Welsh|Merino|2000}}.</ref>\n! Potts model || Tutte polynomial\n|-\n| [[Ferromagnetism|Ferromagnetic]]\n|| Positive branch of <math>H_q</math>\n|-\n| [[Antiferromagnetism|Antiferromagnetic]]\n||  Negative branch of <math>H_q</math> with <math>y>0</math>\n|-\n| High temperature\n|| Asymptote of <math>H_q</math> to <math>y=1</math>\n|-\n| Low temperature ferromagnetic\n|| Positive branch of <math>H_q</math> asymptotic to <math>x=1</math>\n|-\n| Zero temperature antiferromagnetic\n|| [[Graph coloring|Graph ''q''-colouring]], i.e., <math>x=1-q, y=0</math>\n|}\n\n===Flow polynomial===\n{{Main|Nowhere-zero flow}}\n\n[[Image:Flow in the Tutte plane.jpg|thumb|right|The flow polynomial drawn in the Tutte plane]]\n\nAt <math>x=0</math>, the Tutte polynomial specialises to the flow polynomial studied in combinatorics. For a connected and undirected graph ''G'' and integer ''k'', a nowhere-zero ''k''-flow is an assignment of “flow” values <math>1,2,\\dots,k-1</math> to the edges of an arbitrary orientation of ''G'' such that the total flow entering and leaving each vertex is congruent modulo ''k''. The flow polynomial <math>C_G(k)</math> denotes the number of nowhere-zero ''k''-flows. This value is intimately connected with the chromatic polynomial, in fact, if ''G'' is a [[planar graph]], the chromatic polynomial of ''G'' is equivalent to the flow polynomial of its [[dual graph]] <math>G^*</math> in the sense that\n\n<blockquote>'''Theorem (Tutte).'''\n\n:<math>C_G(k)=k^{-1} \\chi_{G^*}(k).</math>\n\nThe connection to the Tutte polynomial is given by:\n\n: <math>C_G(k)= (-1)^{|E|-|V|+k(G)} T_G(0,1-k).</math></blockquote>\n\n===Reliability polynomial===\n{{Main|Connectivity (graph theory)}}\n\n[[Image:Reliability in the Tutte plane.jpg|thumb|right|The reliability polynomial drawn in the Tutte plane]]\n\nAt <math>x=1</math>, the Tutte polynomial specialises to the all-terminal reliability polynomial studied in network theory. For a connected graph ''G'' remove every edge with probability ''p''; this models a network subject to random edge failures. Then the reliability polynomial is a function <math>R_G(p)</math>, a polynomial in ''p'', that gives the probability that every pair of vertices in ''G'' remains connected after the edges fail. The connection to the Tutte polynomial is given by\n\n:<math>R_G(p) = (1-p)^{|V|-k(G)} p^{|E|-|V|+k(G)} T_G \\left (1, \\tfrac{1}{p} \\right).</math>\n\n===Dichromatic polynomial===\nTutte also defined a closer 2-variable generalization of the chromatic polynomial, the '''dichromatic polynomial''' of a graph.  This is\n\n:<math>Q_G(u,v) = \\sum\\nolimits_{A \\subseteq E} u^{k(A)} v^{|A|-|V|+k(A)},</math>\n\nwhere <math>k(A)</math> is the number of [[connected component (graph theory)|connected components]] of the spanning subgraph (''V'',''A'').  This is related to the '''corank-nullity polynomial''' by\n\n:<math>Q_G(u,v) = u^{k(G)} \\, R_G(u,v).</math>\n\nThe dichromatic polynomial does not generalize to matroids because ''k''(''A'') is not a matroid property: different graphs with the same matroid can have different numbers of connected components.\n\n==Related polynomials==\n\n===Martin polynomial===\n{{main|Martin polynomial}}\nThe Martin polynomial <math>m_{\\vec{G}}(x)</math> of an oriented 4-regular graph <math>\\vec{G}</math> was defined by Pierre Martin in 1977.<ref>{{harvnb|Martin|1977}}.</ref> He showed that if ''G'' is a plane graph and <math>\\vec{G}_m</math> is its [[Medial graph#Directed medial graph|directed medial graph]], then\n\n:<math>T_G(x,x) = m_{\\vec{G}_m}(x).</math>\n\n==Algorithms==\n\n===Deletion–contraction===\n{{Main|Deletion–contraction formula}}\n[[Image:deletion-contraction.svg|thumb|right|300px|The deletion–contraction algorithm applied to the [[diamond graph]]. Red edges are deleted in the left child and contracted in the right child. The resulting polynomial is the sum of the monomials at the leaves, <math>x^3+2x^2 +y^2+2xy+x+y</math>. Based on {{harvtxt|Welsh|Merino|2000}}.]]\n\nThe deletion–contraction recurrence for the Tutte polynomial,\n: <math>T_G(x,y)= T_{G \\setminus e}(x,y) + T_{G/e}(x,y), \\qquad e \\text{ not a loop nor a bridge.} </math>\nimmediately yields a recursive algorithm for computing it: choose any such edge ''e'' and repeatedly apply the formula until all edges are either loops or bridges; the resulting base cases at the bottom of the evaluation are easy to compute.\n\nWithin a polynomial factor, the running time ''t'' of this algorithm can be expressed in terms of the number of vertices ''n'' and the number of edges ''m'' of the graph,\n:<math>t(n+m)= t(n+m-1) + t(n+m-2),</math>\na recurrence relation that scales as the [[Fibonacci numbers]] with solution<ref>{{harvnb|Wilf|1986|p=46}}.</ref>\n\n:<math> t(n+m)= \\left (\\frac{1+\\sqrt{5}}{2} \\right )^{n+m} = O \\left (1.6180^{n+m} \\right ).</math>\n\nThe analysis can be improved to within a polynomial factor of the number <math>\\tau(G)</math> of [[spanning tree (mathematics)|spanning trees]] of the input graph.<ref name=\"Sekine 1995\">{{harvnb|Sekine|Imai|Tani|1995}}.</ref> For [[Sparse graph|sparse graphs]] with <math>m=O(n)</math> this running time is <math>O(\\exp(n))</math>. For [[Regular graph|regular graphs]] of degree ''k'', the number of spanning trees can be bounded by\n\n:<math>\\tau(G) = O \\left (\\nu_k^n n^{-1} \\log n \\right ),</math>\n\nwhere\n\n:<math>\\nu_k = \\frac{(k-1)^{k-1}}{(k^2-2k)^{\\frac{k}{2}-1}}.</math>\n\nso the deletion–contraction algorithm runs within a polynomial factor of this bound. For example:<ref>{{harvnb|Chung|Yau|1999}}, following {{harvnb|Björklund|Husfeldt|Kaski|Koivisto|2008}}.</ref>\n\n:<math>\\nu_5 \\approx 4.4066.</math>\n\nIn practice, [[graph isomorphism]] testing is used to avoid some recursive calls. This approach works well for graphs that are quite sparse and exhibit many symmetries; the performance of the algorithm depends on the heuristic used to pick the edge ''e''.<ref name=\"Sekine 1995\"/><ref>{{harvnb|Haggard|Pearce|Royle|2010}}.</ref><ref>{{harvnb|Pearce|Haggard|Royle|2010}}.</ref>\n\n===Gaussian elimination===\nIn some restricted instances, the Tutte polynomial can be computed in polynomial time, ultimately because [[Gaussian elimination]] efficiently computes the matrix operations [[determinant]] and [[Pfaffian]]. These algorithms are themselves important results from [[algebraic graph theory]] and [[statistical mechanics]].\n\n<math>T_G(1,1)</math> equals the number <math>\\tau(G)</math> of [[Spanning tree (mathematics)|spanning tree]]s of a connected graph. This is\ncomputable in polynomial time as the [[determinant]] of a maximal principal submatrix of the [[Laplacian matrix]] of ''G'', an early result in algebraic graph theory known as [[Kirchhoff’s Matrix–Tree theorem]]. Likewise, the dimension of the bicycle space at <math>T_G(-1,-1)</math> can be computed in polynomial time by Gaussian elimination.\n\nFor planar graphs, the partition function of the Ising model, i.e., the Tutte polynomial at the hyperbola <math>H_2</math>, can be expressed as a Pfaffian and computed efficiently via the [[FKT algorithm]]. This idea was developed by [[Michael Fisher|Fisher]], [[Pieter Kasteleyn|Kasteleyn]], and [[Harold Neville Vazeille Temperley|Temperley]] to compute the number of [[domino tiling|dimer]] covers of a planar [[Lattice model (physics)|lattice model]].\n\n===Markov chain Monte Carlo===\nUsing a [[Markov chain Monte Carlo]] method, the Tutte polynomial can be arbitrarily well approximated along the positive branch of <math>H_2</math>, equivalently, the partition function of the ferromagnetic Ising model. This exploits the close connection between the Ising model and the problem of counting [[Matching (graph theory)|matchings]] in a graph. The idea behind this celebrated result of Jerrum and Sinclair<ref>{{harvnb|Jerrum|Sinclair|1993}}.</ref> is to set up a [[Markov chain]] whose states are the matchings of the input graph. The transitions are defined by choosing edges at random and modifying the matching accordingly. The resulting Markov chain is rapidly mixing and leads to “sufficiently random” matchings, which can be used to recover the partition function using random sampling. The resulting algorithm is a [[fully polynomial-time randomized approximation scheme]] (fpras).\n\n==Computational complexity==\nSeveral computational problems are associated with the Tutte polynomial. The most straightforward one is\n;'''Input: A graph <math>G</math>'''\n;'''Output: The coefficients of <math>T_G</math>'''\n\nIn particular, the output allows evaluating <math>T_G(-2,0)</math> which is equivalent to counting the number of 3-colourings of ''G''. This latter question is [[Sharp-P-complete|#P-complete]], even when restricted to the family of [[planar graph]]s, so the problem of computing the coefficients of the Tutte polynomial for a given graph is [[Sharp-P-hard|#P-hard]] even for planar graphs.\n\nMuch more attention has been given to the family of problems called Tutte<math>(x,y)</math> defined for every complex pair <math>(x,y)</math>:\n;<nowiki>Input: A graph </nowiki><math>G</math>\n;<nowiki>Output: The value of </nowiki><math>T_G(x,y)</math>\n\nThe hardness of these problems varies with the coordinates <math>(x,y)</math>.\n\n===Exact computation===\n[[Image:Tractable points of the Tutte polynomial in the real plane.svg|thumb|300px|right|\n  The Tutte plane.\n  Every point <math>(x,y)</math> in the real plane corresponds to a computational problem <math>T_G(x,y)</math>.\n  At any red point, the problem is polynomial-time computable;\n  at any blue point, the problem is #P-hard in general, but polynomial-time computable for planar graphs; and\n  at any point in the white regions, the problem is #P-hard even for bipartite planar graphs.\n]]\nIf both ''x'' and ''y'' are non-negative integers, the problem <math>T_G(x,y)</math> belongs to [[Sharp-P|#P]]. For general integer pairs, the Tutte polynomial contains negative terms, which places the problem in the complexity class [[GapP]], the closure of [[Sharp-P|#P]] under subtraction. To accommodate rational coordinates <math>(x,y)</math>, one can define a rational analogue of [[Sharp-P|#P]].<ref name=\"Goldberg-Jerrum-2008\">{{harvnb|Goldberg|Jerrum|2008}}.</ref>\n\nThe computational complexity of exactly computing <math>T_G(x,y)</math> falls into one of two classes for any <math>x, y \\in \\mathbb{C}</math>. The problem is #P-hard unless <math>(x,y)</math> lies on the hyperbola <math>H_1</math> or is one of the points\n\n:<math>\\left \\{ (1,1), (-1,-1), (0,-1), (-1,0), (i,-i), (-i,i), \\left(j,j^2 \\right), \\left(j^2,j \\right) \\right \\}, \\qquad j = e^{\\frac{2 \\pi i}{3}}.</math>\n\nin which cases it is computable in polynomial time.<ref>{{harvnb|Jaeger|Vertigan|Welsh|1990}}.</ref> If the problem is restricted to the class of planar graphs, the points on the hyperbola <math>H_2</math> become polynomial-time computable as well. All other points remain #P-hard, even for bipartite planar graphs.<ref>{{harvnb|Vertigan|Welsh|1992}}.</ref> In his paper on the dichotomy for planar graphs, Vertigan claims (in his conclusion) that the same result holds when further restricted to graphs with vertex degree at most three, save for the point <math>T_G(0,-2)</math>, which counts nowhere-zero '''Z'''<sub>3</sub>-flows and is computable in polynomial time.<ref>{{harvnb|Vertigan|2005}}.</ref>\n\nThese results contain several notable special cases. For example, the problem of computing the partition function of the Ising model is #P-hard in general, even though celebrated algorithms of Onsager and Fisher solve it for planar lattices. Also, the Jones polynomial is #P-hard to compute. Finally, computing the number of four-colorings of a planar graph is #P-complete, even though the decision problem is trivial by the [[four color theorem]]. In contrast, it is easy to see that counting the number of three-colorings for planar graphs is #P-complete because the decision problem is known to be NP-complete via a [[parsimonious reduction]].\n\n===Approximation===\nThe question which points admit a good approximation algorithm has been very well studied. Apart from the points that can be computed exactly in polynomial time, the only approximation algorithm known for <math>T_G(x,y)</math> is Jerrum and Sinclair’s FPRAS, which works for points on the “Ising” hyperbola <math>H_2</math> for ''y'' > 0. If the input graphs are restricted to dense instances, with degree <math>\\Omega(n)</math>, there is an FPRAS if ''x'' ≥ 1, ''y'' ≥ 1.<ref>For the case ''x'' ≥ 1 and ''y'' = 1, see {{harvnb|Annan|1994}}. For the case ''x'' ≥ 1 and ''y'' > 1, see {{harvnb|Alon|Frieze|Welsh|1995}}.</ref>\n\nEven though the situation is not as well understood as for exact computation, large areas of the plane are known to be hard to approximate.<ref name=\"Goldberg-Jerrum-2008\" />\n\n==See also==\n\n* [[Bollobás–Riordan polynomial]]\n* A [[Tutte–Grothendieck invariant]] is any evaluation of the Tutte polynomial\n\n==Notes==\n{{Reflist|colwidth=25em}}\n\n== References ==\n{{refbegin|colwidth=25em}}\n\n*{{Citation\n | last1 = Alon\n | first1 = N.\n | last2 = Frieze\n | first2 = A.\n | last3 = Welsh\n | first3 = D. J. A.\n | author-link3 = Dominic Welsh\n | title = Polynomial time randomized approximation schemes for Tutte-Gröthendieck invariants: The dense case\n | journal = Random Structures and Algorithms\n | volume = 6\n | issue = 4\n | pages = 459–478\n | year = 1995\n | doi = 10.1002/rsa.3240060409\n}}.\n*{{Citation\n | last1 = Annan\n | first1 = J. D.\n | title = A Randomised Approximation Algorithm for Counting the Number of Forests in Dense Graphs\n | journal = [[Combinatorics, Probability and Computing]]\n | volume = 3\n | issue = 3\n | pages = 273–283\n | year = 1994\n | doi = 10.1017/S0963548300001188\n}}.\n*{{Citation\n | last = Biggs\n | first = Norman\n | title = Algebraic Graph Theory\n | edition = 2nd\n | publisher = [[Cambridge University Press]]\n | year = 1993\n | isbn = 0-521-45897-8\n}}.\n*{{Citation\n | first1 = Andreas\n | last1 = Björklund\n | first2 = Thore\n | last2 = Husfeldt\n | first3 = Petteri\n | last3 = Kaski\n | first4 = Mikko\n | last4 = Koivisto\n | contribution = Computing the Tutte polynomial in vertex-exponential time\n | title = Proc. of the 47th annual IEEE Symposium on Foundations of Computer Science (FOCS 2008)\n | pages = 677–686\n | year = 2008\n | doi = 10.1109/FOCS.2008.40\n | isbn = 978-0-7695-3436-7\n| arxiv = 0711.2585\n }}.\n*{{Citation\n | last1 = Bollobás\n | first1 = Béla\n | author1-link = Béla Bollobás\n | title = Modern Graph Theory\n | publisher = [[Springer-Verlag|Springer]]\n | year = 1998\n | isbn = 978-0-387-98491-9\n}}.\n*{{Citation\n | last1 = Chung\n | first1 = Fan\n | author1-link = Fan Chung\n | last2 = Yau\n | first2 = S.-T.\n | author2-link = Shing-Tung Yau\n | title = Coverings, heat kernels and spanning trees\n | journal = [[Electronic Journal of Combinatorics]]\n | volume = 6\n | page = R12\n | year = 1999\n | mr = 1667452\n | url = http://www.combinatorics.org/Volume_6/Abstracts/v6i1r12.html\n}}.\n*{{Citation\n | last1 = Crapo\n | first1 = Henry H.\n | title = The Tutte polynomial\n | journal = [[Aequationes Mathematicae]]\n | volume = 3\n | issue = 3\n | pages = 211–229\n | year = 1969\n | doi = 10.1007/bf01817442\n}}.\n*{{Citation\n | last = Farr\n | first = Graham E.\n | editor1-last = Grimmett\n | editor1-first = Geoffrey\n | editor1-link = Geoffrey Grimmett\n | editor2-last = McDiarmid\n | editor2-first = Colin\n | contribution = Tutte-Whitney polynomials: some history and generalizations\n | title = Combinatorics, complexity, and chance. A tribute to Dominic Welsh\n | series = Oxford Lecture Series in Mathematics and its Applications\n | volume = 34\n | pages = 28–52\n | year = 2007\n | publisher = [[Oxford University Press]]\n | isbn = 0-19-857127-5\n | zbl = 1124.05020\n}}.\n*{{Citation\n | last1 = Fortuin\n | first1 = Cees M.\n | last2 = Kasteleyn\n | first2 = Pieter W.\n | title = On the random-cluster model: I. Introduction and relation to other models\n | journal = [[Physica (journal)|Physica]]\n | volume = 57\n | issue = 4\n | pages = 536–564\n | year = 1972\n | publisher = [[Elsevier]]\n | issn = 0031-8914\n | doi = 10.1016/0031-8914(72)90045-6\n| bibcode = 1972Phy....57..536F\n }}.\n*{{Citation\n | last1 = Godsil\n | first1 = Chris\n | author-link1 = Chris Godsil\n | last2 = Royle\n | first2 = Gordon\n | author-link2 = Gordon Royle\n | title = Algebraic Graph Theory\n | publisher = [[Springer-Verlag|Springer]]\n | year = 2004\n | isbn = 978-0-387-95220-8\n}}.\n*{{Citation\n | last1 = Goldberg\n | first1 = Leslie Ann\n | author1-link = Leslie Ann Goldberg\n | last2 = Jerrum\n | first2 =  Mark\n | author2-link = Mark Jerrum\n | title = Inapproximability of the Tutte polynomial\n | journal = [[Information and Computation]]\n | volume = 206\n | issue = 7\n | pages = 908–929\n | year = 2008\n | doi = 10.1016/j.ic.2008.04.003   \n| arxiv = cs/0605140\n }}.\n*{{Citation\n | last1 = Haggard\n | first1 = Gary\n | last2 = Pearce\n | first2 = David J.\n | last3 = Royle\n | first3 = Gordon\n | title = Computing Tutte polynomials\n | journal = [[ACM Transactions on Mathematical Software]]\n | volume = 37\n | issue = 3\n | page = Art. 24, 17\n | year = 2010\n | doi = 10.1145/1824801.1824802\n | mr = 2738228\n}}.\n*{{Citation\n | last1 = Jaeger\n | first1 = F.\n | last2 = Vertigan\n | first2 =  D. L.\n | last3 = Welsh\n | first3 = D. J. A.\n | authorlink3 = Dominic Welsh\n | title = On the computational complexity of the Jones and Tutte polynomials\n | journal = [[Mathematical Proceedings of the Cambridge Philosophical Society]]\n | volume = 108\n | pages = 35–53\n | year = 1990\n | doi = 10.1017/S0305004100068936\n| bibcode = 1990MPCPS.108...35J\n }}.\n*{{Citation\n | last1 = Jerrum\n | first1 = Mark\n | author-link = Mark Jerrum\n | last2 = Sinclair\n | first2 = Alistair\n | author2-link = Alistair Sinclair\n | title = Polynomial-time approximation algorithms for the Ising model\n | journal = [[SIAM Journal on Computing]]\n | volume = 22\n | issue= 5\n | pages = 1087–1116\n | year = 1993\n | doi= 10.1137/0222066\n}}.\n*{{Citation\n | last1 = Korn\n | first1 = Michael\n | last2 = Pak\n | first2 = Igor\n | author2-link = Igor Pak\n | title = Combinatorial evaluations of the Tutte polynomial\n | type = preprint\n | year = 2003\n | url = http://www.math.ucla.edu/~pak/papers/tutte7color.pdf\n}}.\n*{{Citation\n | last1 = Korn\n | first1 = Michael\n | last2 = Pak\n | first2 = Igor\n | author2-link = Igor Pak\n | title = Tilings of rectangles with T-tetrominoes\n | journal = [[Theoretical Computer Science (journal)|Theoretical Computer Science]]\n | volume = 319\n | issue = 1–3\n | pages = 3–27\n | year = 2004\n | doi = 10.1016/j.tcs.2004.02.023\n}}.\n*{{Citation\n | last = Las Vergnas\n | first = Michel\n | authorlink = Michel Las Vergnas\n | title = Convexity in oriented matroids\n | journal = [[Journal of Combinatorial Theory]]\n | series = Series B\n | volume = 29\n | issue = 2\n | pages = 231–243\n | year = 1980\n | issn = 0095-8956\n | doi = 10.1016/0095-8956(80)90082-9\n | url = http://www.sciencedirect.com/science/article/pii/0095895680900829\n | mr = 586435\n}}.\n*{{Citation\n | last1 = Las Vergnas\n | first1 = Michel\n | author1-link = Michel Las Vergnas\n | title = On the evaluation at (3, 3) of the Tutte polynomial of a graph\n | journal = [[Journal of Combinatorial Theory]]\n | series = Series B\n | volume = 45\n | issue = 3\n | pages = 367–372\n | year = 1988\n | issn = 0095-8956\n | doi = 10.1016/0095-8956(88)90079-2\n | url = http://www.sciencedirect.com/science/article/pii/0095895688900792\n}}.\n*{{Citation\n | last = Martin\n | first = Pierre\n | title = Enumérations Eulériennes dans les multigraphes et invariants de Tutte-Grothendieck\n | trans-title = Eulerian Enumerations in multigraphs and Tutte-Grothendieck invariants\n | type = Ph.D. thesis\n | publisher = [[Joseph Fourier University]]\n | year = 1977\n | language = French\n | url = http://tel.archives-ouvertes.fr/tel-00287330/en\n}}.\n*{{Citation\n | last1 = Pearce\n | first1 = David J.\n | last2 = Haggard\n | first2 = Gary\n | last3 = Royle\n | first3 = Gordon\n | title = Edge-selection heuristics for computing Tutte polynomials\n | journal = Chicago Journal of Theoretical Computer Science\n | page = Article 6, 14\n | year = 2010\n | url = http://cjtcs.cs.uchicago.edu/articles/2010/6/cats9-3-1.pdf\n | mr = 2659710\n}}.\n*{{Citation\n | last1 = Sekine\n | first1 = Kyoko\n | last2 = Imai\n | first2 = Hiroshi\n | last3 = Tani\n | first3 = Seiichiro\n | contribution = Computing the Tutte polynomial of a graph of moderate size\n | title = Algorithms and computations (Cairns, 1995)\n | volume = 1004\n | pages = 224–233\n | series = [[Lecture Notes in Computer Science]]\n | publisher = [[Springer-Verlag|Springer]]\n | year = 1995\n | doi = 10.1007/BFb0015427\n | mr = 1400247\n}}.\n*{{Citation\n | last = Sokal\n | first = Alan D.\n | editor-last = Webb\n | editor-first = Bridget S.\n | contribution = The multivariate Tutte polynomial (alias Potts model) for graphs and matroids\n | title = Surveys in Combinatorics\n | volume = 327\n | pages = 173–226\n | year = 2005\n | series = London Mathematical Society Lecture Note Series\n | publisher = [[Cambridge University Press]]\n | doi = 10.1017/CBO9780511734885.009\n | arxiv = math/0503607\n}}.\n*{{Citation\n | last = Tutte\n | first = W. T.\n | author-link = William Thomas Tutte\n | title = Graph Theory\n | publisher = [[Cambridge University Press]]\n | year = 2001\n | isbn = 978-0521794893\n}}.\n*{{Citation\n | last = Tutte\n | first = W. T.\n | author-link = William Thomas Tutte\n | title = Graph-polynomials\n | journal = [[Advances in Applied Mathematics]]\n | volume = 32\n | issue = 1–2\n | year = 2004\n | pages = 5–9\n | doi = 10.1016/S0196-8858(03)00041-1\n}}.\n*{{Citation\n | last1 = Vertigan\n | first1 = D. L.\n | last2 = Welsh\n | first2 = D. J. A.\n | author-link2 = Dominic Welsh\n | title = The Computational Complexity of the Tutte Plane: the Bipartite Case\n | journal = [[Combinatorics, Probability and Computing]]\n | volume = 1\n | issue = 2\n | pages = 181–187\n | year = 1992\n | doi = 10.1017/S0963548300000195\n | url = https://dx.doi.org/10.1017/S0963548300000195\n}}.\n*{{Citation\n | last = Vertigan\n | first = Dirk\n | title = The Computational Complexity of Tutte Invariants for Planar Graphs\n | journal = [[SIAM Journal on Computing]]\n | volume = 35\n | issue = 3\n | pages = 690–712\n | year = 2005\n | doi = 10.1137/S0097539704446797\n | url = https://dx.doi.org/10.1137/S0097539704446797\n}}.\n*{{Citation\n | last = Welsh\n | first = D. J. A.\n | author-link = Dominic Welsh\n | title = Matroid Theory\n | publisher = [[Academic Press]]\n | year = 1976\n | isbn = 012744050X\n}}.\n*{{Citation\n | last = Welsh\n | first = Dominic\n | author-link = Dominic Welsh\n | title = Complexity: Knots, Colourings and Counting\n | series = London Mathematical Society Lecture Note Series\n | year = 1993\n | publisher = [[Cambridge University Press]]\n | isbn = 978-0521457408\n}}.\n*{{Citation\n | last = Welsh\n | first = Dominic\n | author-link = Dominic Welsh\n | title = The Tutte polynomial\n | journal = Random Structures & Algorithms\n | volume = 15\n | issue = 3–4\n | pages = 210–228\n | year = 1999\n | publisher = [[John Wiley & Sons|Wiley]]\n | issn = 1042-9832\n | doi = 10.1002/(SICI)1098-2418(199910/12)15:3/4<210::AID-RSA2>3.0.CO;2-R\n | url = https://dx.doi.org/10.1002/(SICI)1098-2418(199910/12)15:3/4%3C210::AID-RSA2%3E3.0.CO;2-R\n}}.\n*{{Citation\n | last1 = Welsh\n | first1 = D. J. A.\n | authorlink1 = Dominic Welsh\n | last2 = Merino\n | first2 = C.\n | title = The Potts model and the Tutte polynomial\n | journal = [[Journal of Mathematical Physics]]\n | volume = 41\n | issue = 3\n | year = 2000 \n | doi = 10.1063/1.533181\n| bibcode = 2000JMP....41.1127W\n }}.\n*{{Citation\n | last = Wilf\n | first = Herbert S.\n | authorlink = Herbert Wilf\n | title = Algorithms and complexity\n | publisher = [[Prentice Hall]]\n | year = 1986\n | isbn = 0-13-021973-8\n | url = https://www.math.upenn.edu/~wilf/AlgoComp.pdf\n | mr = 897317\n}}.\n\n{{refend}}\n\n== External links ==\n* {{springer|title=Tutte polynomial|id=p/t120210}}\n* {{MathWorld | urlname=TuttePolynomial| title=Tutte polynomial}}\n* [[PlanetMath]] [https://planetmath.org/ChromaticPolynomial Chromatic polynomial]\n* Steven R. Pagano: [https://web.archive.org/web/20060202005537/http://www.ms.uky.edu/~pagano/Matridx.htm Matroids and Signed Graphs]\n* Sandra Kingan: [https://web.archive.org/web/20060211053237/http://members.aol.com/matroids/ Matroid theory]. Lots of links.\n* Code for computing Tutte, Chromatic and Flow Polynomials by Gary Haggard, David J. Pearce and Gordon Royle: [http://www.ecs.vuw.ac.nz/~djp/tutte/]\n\n[[Category:Computational problems]]\n[[Category:Duality theories]]\n[[Category:Matroid theory]]\n[[Category:Polynomials]]\n[[Category:Graph invariants]]"
    },
    {
      "title": "Uniform matroid",
      "url": "https://en.wikipedia.org/wiki/Uniform_matroid",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Matroid in which every permutation is a symmetry}}\nIn mathematics, a '''uniform matroid''' is a [[matroid]] in which every [[permutation]] of the elements is a [[Symmetry in mathematics|symmetry]].\n\n==Definition==\nThe uniform matroid <math>U{}^r_n</math> is defined over a set of <math>n</math> elements. A subset of the elements is independent if and only if it contains at most <math>r</math> elements. A subset is a basis if it has exactly <math>r</math> elements, and it is a circuit if it has exactly <math>r+1</math> elements. The [[matroid rank|rank]] of a subset <math>S</math> is <math>\\min(|S|,r)</math> and the rank of the matroid is <math>r</math>.<ref>{{citation\n | last = Oxley | first = James G. | authorlink = James Oxley\n | contribution = Example 1.2.7\n | isbn = 9780199202508\n | page = 19\n | publisher = Oxford University Press\n | series = Oxford Graduate Texts in Mathematics\n | title = Matroid Theory\n | volume = 3\n | year = 2006}}. For the rank function, see p. 26.</ref><ref>{{citation\n | last = Welsh | first = D. J. A. | authorlink = Dominic Welsh\n | isbn = 9780486474397\n | page = 10\n | publisher = Courier Dover Publications\n | title = Matroid Theory\n | year = 2010}}.</ref>\n\nA matroid of rank <math>r</math> is uniform if and only if all of its circuits have exactly <math>r+1</math> elements.<ref>{{harvtxt|Oxley|2006}}, p. 27.</ref>\n\nThe matroid <math>U{}^2_n</math> is called the '''<math>n</math>-point line'''.\n\n==Duality and minors==\nThe [[dual matroid]] of the uniform matroid <math>U{}^r_n</math> is another uniform matroid <math>U{}^{n-r}_n</math>. A uniform matroid is self-dual if and only if <math>r=n/2</math>.<ref>{{harvtxt|Oxley|2006}}, pp. 77 & 111.</ref>\n\nEvery [[matroid minor|minor]] of a uniform matroid is uniform. Restricting a uniform matroid <math>U{}^r_n</math> by one element (as long as <math>r < n</math>) produces the matroid\n<math>U{}^r_{n-1}</math> and contracting it by one element (as long as <math>r > 0</math>) produces the matroid <math>U{}^{r-1}_{n-1}</math>.<ref>{{harvtxt|Oxley|2006}}, pp. 106–107 & 111.</ref>\n\n==Realization==\nThe uniform matroid <math>U{}^r_n</math> may be [[Matroid representation|represented]] as the matroid of affinely independent subsets of <math>n</math> points in [[general position]] in <math>r</math>-dimensional [[Euclidean space]], or as the matroid of linearly independent subsets of <math>n</math> vectors in general position in an <math>(r+1)</math>-dimensional real [[vector space]].\n\nEvery uniform matroid may also be realized in [[projective space]]s and vector spaces over all sufficiently large [[finite field]]s.<ref name=\"Ox100\"/> However, the field must be large enough to include enough independent vectors. For instance, the <math>n</math>-point line <math>U{}^2_n</math> can be realized only over finite fields of <math>n-1</math> or more elements (because otherwise the projective line over that field would have fewer than <math>n</math> points): <math>U{}^2_4</math> is not a [[binary matroid]], <math>U{}^2_5</math> is not a ternary matroid, etc. For this reason, uniform matroids play an important role in [[Rota's conjecture]] concerning the [[matroid minor|forbidden minor]] characterization of the matroids that can be realized over finite fields.<ref>{{harvtxt|Oxley|2006}}, pp. 202–206.</ref>\n\n==Algorithms==\nThe problem of finding the minimum-weight basis of a [[weighted matroid|weighted]] uniform matroid is well-studied in computer science as the [[Selection algorithm|selection problem]]. It may be solved in [[linear time]].<ref>{{citation\n | last1 = Cormen | first1 = Thomas H. | author1-link = Thomas H. Cormen\n | last2 = Leiserson | first2 = Charles E. | author2-link = Charles E. Leiserson\n | last3 = Rivest | first3 = Ronald L. | author3-link = Ron Rivest\n | last4 = Stein | first4 = Clifford | author4-link = Clifford Stein\n | contribution = Chapter 9: Medians and Order Statistics\n | edition = 2nd\n | isbn = 0-262-03293-7\n | pages = 183–196\n | publisher = MIT Press and McGraw-Hill\n | title = [[Introduction to Algorithms]]\n | year = 2001}}.</ref>\n\nAny algorithm that tests whether a given matroid is uniform, given access to the matroid via an [[matroid oracle|independence oracle]], must perform an exponential number of oracle queries, and therefore cannot take polynomial time.<ref>{{citation\n | last1 = Jensen | first1 = Per M.\n | last2 = Korte | first2 = Bernhard\n | doi = 10.1137/0211014\n | issue = 1\n | journal = [[SIAM Journal on Computing]]\n | mr = 646772\n | pages = 184–190\n | title = Complexity of matroid property algorithms\n | volume = 11\n | year = 1982}}.</ref>\n\n==Related matroids==\nUnless <math>r\\in\\{0,n\\}</math>, a uniform matroid <math>U{}^r_n</math> is connected: it is not the direct sum of two smaller matroids.<ref>{{harvtxt|Oxley|2006}}, p. 126.</ref>\nThe direct sum of a family of uniform matroids (not necessarily all with the same parameters) is called a [[partition matroid]].\n\nEvery uniform matroid is a [[paving matroid]],<ref name=Ox26>{{harvtxt|Oxley|2006|p=26}}.</ref> a [[transversal matroid]]<ref>{{harvtxt|Oxley|2006}}, pp. 48–49.</ref> and a [[gammoid|strict gammoid]].<ref name=\"Ox100\">{{harvtxt|Oxley|2006}}, p. 100.</ref>\n\nNot every uniform matroid is [[graphic matroid|graphic]], and the uniform matroids provide the smallest example of a non-graphic matroid, <math>U{}^2_4</math>. The uniform matroid <math>U{}^1_n</math> is the graphic matroid of an <math>n</math>-edge [[dipole graph]], and the dual uniform matroid <math>U{}^{n-1}_n</math> is the graphic matroid of its [[dual graph]], the <math>n</math>-edge [[cycle graph]]. <math>U{}^0_n</math> is the graphic matroid of a graph with <math>n</math> self-loops, and <math>U{}^n_n</math> is the graphic matroid of an <math>n</math>-edge [[tree (graph theory)|forest]]. Other than these examples, every uniform matroid <math>U{}^r_n</math> with <math>1 < r < n-1</math> contains <math>U{}^2_4</math> as a minor and therefore is not graphic.<ref>{{harvtxt|Welsh|2010}}, p. 30.</ref>\n\nThe <math>n</math>-point line provides an example of a [[Sylvester matroid]], a matroid in which every line contains three or more points.<ref>{{harvtxt|Welsh|2010}}, p. 297.</ref>\n\n==See also==\n*[[K-set (geometry)]]\n\n==References==\n{{reflist|colwidth=30em}}\n\n[[Category:Matroid theory]]"
    },
    {
      "title": "Vámos matroid",
      "url": "https://en.wikipedia.org/wiki/V%C3%A1mos_matroid",
      "text": "[[File:Vamos matroid.svg|thumb|The Vámos matroid]]\nIn [[mathematics]], the '''Vámos matroid''' or '''Vámos cube''' is a [[matroid]] over a set of eight elements that cannot be [[Matroid representation|represented as a matrix]] over any [[field (mathematics)|field]]. It is named after English mathematician [[Peter Vámos]], who first described it in an unpublished manuscript in 1968.<ref>{{citation\n | last = Vámos | first = P.\n | title = On the representation of independence structures\n | year = 1968}}.</ref>\n\n==Definition==\nThe Vámos matroid has eight elements, which may be thought of as the eight vertices of a [[cube]] or [[cuboid]]. The matroid has rank 4: all sets of three or fewer elements are independent, and 65 of the 70 possible sets of four elements are also independent. The five exceptions are four-element circuits in the matroid. Four of these five circuits are formed by faces of the cuboid (omitting two opposite faces). The fifth circuit connects two opposite edges of the cuboid, each of which is shared by two of the chosen four faces.\n\nAnother way of describing the same structure is that it has two elements for each vertex of the [[diamond graph]], and a four-element circuit for each edge of the diamond graph.\n\n==Properties==\n*The Vámos matroid is a [[paving matroid]], meaning that all of its circuits have size at least equal to its [[matroid rank|rank]].<ref name=\"oxley\">{{citation\n | last = Oxley | first = James G. | authorlink = James Oxley\n | contribution = Example 2.1.22\n | isbn = 9780199202508\n | page = 76\n | publisher = Oxford University Press\n | series = Oxford Graduate Texts in Mathematics\n | title = Matroid Theory\n | url = https://books.google.com/books?id=puKta1Hdz-8C&pg=PA76\n | volume = 3\n | year = 2006}}.</ref>\n*The Vámos matroid is isomorphic to its [[dual matroid]], but it is not identically self-dual (the isomorphism requires a nontrivial permutation of the matroid elements).<ref name=\"oxley\"/>\n*The Vámos matroid cannot be represented over any field. That is, it is not possible to find a [[vector space]], and a system of eight vectors within that space, such that the matroid of [[linear independence]] of these vectors is isomorphic to the Vámos matroid.<ref>{{harvtxt|Oxley|2006}}, pp. 170–172.</ref> Indeed, it is one of the smallest non-representable matroids,<ref>{{harvtxt|Oxley|2006}}, Prop. 6.4.10, p. 196. A proof of representability for every matroid with fewer elements or the same number but smaller rank was given by {{citation\n | last = Fournier | first = Jean-Claude\n | journal = [[Comptes rendus de l'Académie des sciences]] | series = Sér. A-B\n | mr = 0263665\n | pages = A810–A813\n | title = Sur la représentation sur un corps des matroïdes à sept et huit éléments\n | volume = 270\n | year = 1970}}.</ref> and served as a counterexample to a conjecture of [[Aubrey William Ingleton|Ingleton]] that the matroids on eight or fewer elements were all representable.<ref>{{citation\n | last = Ingleton | first = A. W.\n | doi = 10.1112/jlms/s1-34.1.49\n | journal = [[Journal of the London Mathematical Society]]\n | mr = 0101848\n | pages = 49–56\n | series = Second Series\n | title = A note on independence functions and rank\n | volume = 34\n | year = 1959}}.</ref>\n*The Vámos matroid is a [[matroid minor|forbidden minor]] for the matroids representable over a field <math>F</math>, whenever <math>F</math> has five or more elements.<ref name=\"ox511\">{{harvtxt|Oxley|2006}}, p. 511.</ref> However, it is not possible to test in [[polynomial time]] whether it is a minor of a given matroid <math>M</math>, given access to <math>M</math> through an [[matroid oracle|independence oracle]].<ref>{{citation\n | last1 = Seymour | first1 = P. D. | author1-link = Paul Seymour (mathematician)\n | last2 = Walton | first2 = P. N.\n | doi = 10.1112/jlms/s2-23.2.193\n | issue = 2\n | journal = [[Journal of the London Mathematical Society]]\n | mr = 609098\n | pages = 193–203\n | series = Second Series\n | title = Detecting matroid minors\n | volume = 23\n | year = 1981}}. {{citation\n | last1 = Jensen | first1 = Per M.\n | last2 = Korte | first2 = Bernhard\n | doi = 10.1137/0211014\n | issue = 1\n | journal = [[SIAM Journal on Computing]]\n | mr = 646772\n | pages = 184–190\n | title = Complexity of matroid property algorithms\n | volume = 11\n | year = 1982}}.</ref>\n*The Vámos matroid is not algebraic. That is, there does not exist a [[field extension]] <math>L/K</math>, and a set of eight elements of <math>L</math>, such that the [[transcendence degree]] of sets of these eight elements equals the rank function of the matroid.<ref>{{citation\n | last1 = Ingleton | first1 = A. W.\n | last2 = Main | first2 = R. A.\n | doi = 10.1112/blms/7.2.144\n | journal = [[Bulletin of the London Mathematical Society]]\n | mr = 0369110\n | pages = 144–146\n | title = Non-algebraic matroids exist\n | volume = 7\n | year = 1975}}.</ref>\n*The Vámos matroid is not a secret-sharing matroid.<ref>{{citation\n | last = Seymour | first = P. D. | authorlink = Paul Seymour (mathematician)\n | doi = 10.1016/0095-8956(92)90007-K\n | issue = 1\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 1182458\n | pages = 69–73\n | series = Series B\n | title = On secret-sharing matroids\n | volume = 56\n | year = 1992}}.</ref> Secret-sharing matroids describe \"ideal\" [[secret sharing]] schemes in which any coalition of users who can gain any information about a secret key can learn the whole key (these coalitions are the dependent sets of the matroid), and in which the shared information contains no more information than is needed to represent the key.<ref>{{citation\n | last1 = Brickell | first1 = Ernest F.\n | last2 = Davenport | first2 = Daniel M.\n | doi = 10.1007/BF00196772\n | issue = 2\n | journal = [[Journal of Cryptology]]\n | pages = 123–134\n | title = On the classification of ideal secret sharing schemes\n | volume = 4\n | year = 1991}}.</ref> These matroids also have applications in [[coding theory]].<ref>{{citation\n | last1 = Simonis | first1 = Juriaan\n | last2 = Ashikhmin | first2 = Alexei\n | doi = 10.1023/A:1008244215660\n | issue = 2\n | journal = Designs, Codes and Cryptography\n | mr = 1614357\n | pages = 179–197\n | title = Almost affine codes\n | volume = 14\n | year = 1998}}.</ref>\n*The Vámos matroid has no adjoint. This means that the [[Order dual|dual lattice]] of the [[geometric lattice]] of the Vámos matroid cannot be [[order embedding|order-embedded]] into another geometric lattice of the same rank.<ref>{{citation\n | last = Cheung | first = Alan L. C.\n | doi = 10.4153/CMB-1974-066-5\n | issue = 3\n | journal = [[Canadian Mathematical Bulletin]]\n | mr = 0373976\n | pages = 363–365; correction, ibid. 17 (1974), no. 4, 623\n | title = Adjoints of a geometry\n | volume = 17\n | year = 1974}}.</ref>\n*The Vámos matroid can be [[oriented matroid|oriented]].<ref>{{citation\n | last1 = Bland | first1 = Robert G. | author1-link = Robert G. Bland\n | last2 = Las Vergnas | first2 = Michel | author2-link = Michel Las Vergnas\n | doi = 10.1016/0095-8956(78)90080-1\n | issue = 1\n | journal = [[Journal of Combinatorial Theory]]\n | mr = 0485461\n | pages = 94–123\n | series = Series B\n | title = Orientability of matroids\n | volume = 24\n | year = 1978}}.</ref> In oriented matroids, a form of the [[Hahn–Banach theorem]] follows from a certain intersection property of the flats of the matroid; the Vámos matroid provides an example of a matroid in which the intersection property is not true, but the Hahn–Banach theorem nevertheless holds.<ref>{{citation\n | last1 = Bachem | first1 = Achim\n | last2 = Wanka | first2 = Alfred\n | doi = 10.1016/0012-365X(88)90006-4\n | issue = 3\n | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]\n | mr = 955127\n | pages = 303–310\n | title = Separation theorems for oriented matroids\n | volume = 70\n | year = 1988}}.</ref>\n*The [[Tutte polynomial]] of the Vámos matroid is<ref>{{citation\n | last1 = Merino | first1 = Criel\n | last2 = Ramírez-Ibáñez | first2 = Marcelino\n | last3 = Sanchez | first3 = Guadalupe Rodríguez\n | arxiv = 1203.0090\n | title = The Tutte polynomial of some matroids\n | year = 2012| bibcode = 2012arXiv1203.0090M}}.</ref>\n*:<math>x^4+4x^3+10x^2+15x+5xy+15y+10y^2+4y^3+y^4.</math>\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Vamos matroid}}\n[[Category:Matroid theory]]"
    },
    {
      "title": "Weighted matroid",
      "url": "https://en.wikipedia.org/wiki/Weighted_matroid",
      "text": "In [[combinatorics]], a branch of [[mathematics]], a '''weighted matroid''' is a [[matroid]] endowed with function with respect to which one can perform a [[greedy algorithm]].\n\nA ''weight function'' <math> w : E \\rightarrow \\mathbb{R}^+ </math> for a matroid <math> M = (E, I) </math> assigns a strictly positive weight to each element of <math> E </math>. We extend the function to subsets of <math> E </math> by summation; <math> w(A) </math> is the sum of <math> w(x) </math> over <math>x</math> in <math>A</math>. A matroid with an associated weight function is called a weighted matroid.\n\n==Spanning forest algorithms==\nAs a simple example, say we wish to find the [[spanning tree (mathematics)|maximum spanning forest]] of a graph. That is, given a graph and a weight for each edge, find a forest containing every vertex and maximizing the total weight of the edges in the tree. This problem arises in some clustering applications. If we look at the definition of the forest matroid above, we see that the maximum spanning forest is simply the independent set with largest total weight &mdash; such a set must span the graph, for otherwise we can add edges without creating cycles. But how do we find it?\n\n===Finding a basis===\nThere is a simple algorithm for finding a basis:\n\n* Initially let <math>A</math> be the empty set.\n* For each <math>x</math> in <math>E</math>\n** if <math>A \\cup \\{ x\\}</math> is independent, then set <math>A</math> to <math>A \\cup \\{ x\\}</math>.\n\nThe result is clearly an independent set. It is a maximal independent set because if <math>B \\cup \\{ x\\}</math> is not independent for some subset <math> B </math> of <math>A</math>, then <math>A \\cup \\{ x\\}</math> is not independent either (the contrapositive follows from the [[hereditary property (matroid)|hereditary property]]). Thus if we pass up an element, we'll never have an opportunity to use it later. We will generalize this algorithm to solve a harder problem.\n\n===Extension to optimal===\nAn independent set of largest total weight is called an ''optimal'' set. Optimal sets are always bases, because if an edge can be added, it should be; this only increases the total weight. As it turns out, there is a trivial [[greedy algorithm]] for computing an optimal set of a weighted matroid. It works as follows:\n\n* Initially let <math>A</math> be the empty set.\n* For each <math>x</math> in <math>E</math>, taken in (monotonically) decreasing order by weight\n** if <math> A \\cup \\{ x \\} </math> is independent, then set <math>A</math> to <math> A \\cup \\{ x \\} </math>.\n\nThis algorithm finds a basis, since it is a special case of the above algorithm. It always chooses the element of largest weight that it can while preserving independence (thus the term \"greedy\"). This always produces an optimal set: suppose that it produces <math>A=\\{e_1,e_2,\\ldots,e_r\\}</math> and that <math>B=\\{f_1,f_2,\\ldots,f_r\\}</math>. Now for any <math>k</math> with <math>1\\le k\\le r</math>, consider open sets <math>O_1=\\{e_1,\\ldots,e_{k-1}\\}</math> and <math>O_2=\\{f_1,\\ldots,f_k\\}</math>. Since <math>O_1</math> is smaller than <math>O_2</math>, there is some element of <math>O_2</math> which can be put into <math>O_1</math> with the result still being independent. However <math>e_k</math> is an element of maximal weight that can be added to <math>O_1</math> to maintain independence. Thus <math>e_k</math> is of no smaller weight than some element of <math>O_2</math>, and hence <math>e_k</math> is of at least a large a weight as <math>f_k</math>. As this is true for all <math>k</math>, <math>A</math> is weightier than <math>B</math>.\n\n===Complexity analysis===\nThe easiest way to traverse the members of <math>E</math> in the desired order is to sort them. This requires <math>O(|E|\\log|E|)</math> time using a comparison [[sorting algorithm]]. We also need to test for each <math>x</math> whether <math> A \\cup \\{ x \\} </math> is independent; assuming independence tests require <math> O(f(|E|)) </math> time, the total time for the algorithm is <math> O(|E|\\log|E| + |E|f(|E|)) </math>.\n\nIf we want to find a minimum spanning tree instead, we simply \"invert\" the weight function by subtracting it from a large constant. More specifically, let <math> w_{\\text{min}}(x) = W - w(x) </math>, where <math>W</math> exceeds the total weight over all graph edges. Many more optimization problems about all sorts of matroids and weight functions can be solved in this trivial way, although in many cases more efficient algorithms can be found that exploit more specialized properties.\n\n===Matroid requirement===\nNote also that if we take a set <math>I</math> of \"independent\" sets which is a down-set but not a matroid, then the greedy algorithm will not always work. For then there are independent sets <math>I_1</math> and <math>I_2</math> with <math>|I_1|<|I_2|</math>, but such that for no <math>e\\in I_2\\setminus I_1</math> is <math>I_1\\cup e</math> independent.\n\nPick an <math>\\epsilon>0</math> and <math>\\tau>0</math> such that <math>(1+2\\epsilon)|I_1|+\\tau|E|<|I_2|</math>. Weight the elements of <math>I_1\\cup I_2</math> in the range <math>2</math> to <math>2+2\\epsilon</math>, the elements of <math>I_1\\setminus I_2</math> in the range <math>1+\\epsilon</math> to <math>1+2\\epsilon</math>, the elements of <math>I_2\\setminus I_1</math> in the range <math>1</math> to <math>1+\\epsilon</math>, and the rest in the range <math>0</math> to <math>\\tau</math>. The greedy algorithm will select the elements of <math>I_1</math>, and then cannot pick any elements of <math>I_2\\setminus I_1</math>. Therefore, the independent set it constructs will be of weight at most <math>(1+2\\epsilon)|I_1|+\\tau|E|+|I_1\\cup I_2|</math>, which is smaller than the weight of <math>I_2</math>.\n\n==History==\n\nIt was not until 1971 that [[Jack Edmonds]] connected weighted matroids to greedy algorithms in his paper       \"Matroids and the greedy algorithm\". Korte and Lovász would generalize these ideas to objects called ''[[greedoid]]s'', which allow even larger classes of problems to be solved by greedy algorithms.\n\n==References==\n* Jack Edmonds. Matroids and the Greedy Algorithm. Mathematical Programming, volume 1, p.&nbsp;125&ndash;136. 1971.\n\n[[Category:Matroid theory]]"
    },
    {
      "title": "Whitney's planarity criterion",
      "url": "https://en.wikipedia.org/wiki/Whitney%27s_planarity_criterion",
      "text": "[[File:Duals graphs.svg|thumb|300px|A planar graph and its dual. Every cycle in the blue graph is a minimal cut in the red graph, and vice versa, so the two graphs are algebraic duals and have dual graphic matroids.]]\nIn mathematics, '''Whitney's planarity criterion''' is a [[matroid]]-theoretic characterization of [[planar graph]]s, named after [[Hassler Whitney]].<ref>{{citation\n | last = Whitney | first = Hassler | author-link = Hassler Whitney\n | journal = Transactions of the American Mathematical Society\n | pages = 339–362\n | title = Non-separable and planar graphs\n | volume = 34\n | issue = 2\n |doi=10.1090/S0002-9947-1932-1501641-2 \n | year = 1932| pmc = 1076008}}.</ref> It states that a graph ''G'' is planar if and only if its [[graphic matroid]] is also cographic (that is, it is the [[dual matroid]] of another graphic matroid).\n\nIn purely graph-theoretic terms, this criterion can be stated as follows: There must be another (dual) graph ''G''<nowiki/>'=(''V''<nowiki/>',''E''<nowiki/>') and a bijective correspondence between the edges ''E''<nowiki/>' and the edges ''E'' of the original graph ''G'', such that a subset ''T'' of ''E'' forms a spanning tree of ''G'' if and only if the edges corresponding to the complementary subset ''E''-''T'' form a spanning tree of ''G''<nowiki/>'.\n\n==Algebraic duals==\nAn equivalent form of Whitney's criterion is that a graph ''G'' is planar if and only if it has a [[dual graph]] whose graphic matroid is dual to the graphic matroid of ''G''. \nA graph whose graphic matroid is dual to the graphic matroid of ''G'' is known as an algebraic dual of ''G''. This, Whitney's planarity criterion can be expressed succinctly as: a graph is planar if and only if it has an algebraic dual.\n\n==Topological duals==\nIf a graph is [[graph embedding|embedded]] into a topological surface such as the plane, in such a way that every face of the embedding is a topological disk, then the dual graph of the embedding is defined as the graph (or in some cases [[multigraph]]) ''H'' that has a vertex for every face of the embedding, and an edge for every adjacency between a pair of faces.\nAccording to Whitney's criterion, the following conditions are equivalent:\n*The surface on which the embedding exists is topologically equivalent to the plane, sphere, or punctured plane\n*''H'' is an algebraic dual of ''G''\n*Every simple cycle in ''G'' corresponds to a minimal cut in ''H'', and vice versa\n*Every simple cycle in ''H'' corresponds to a minimal cut in ''G'', and vice versa\n*Every [[spanning tree]] in ''G'' corresponds to the [[complement (set theory)|complement]] of a spanning tree in ''H'', and vice versa.<ref>{{citation\n | last = Tutte | first = W. T. | authorlink = W. T. Tutte\n | journal = Journal of Research of the National Bureau of Standards\n | mr = 0179781\n | pages = 1–47\n | title = Lectures on matroids\n | url = http://cdm16009.contentdm.oclc.org/cdm/ref/collection/p13011coll6/id/66650\n | volume = 69B\n | year = 1965\n | doi=10.6028/jres.069b.001}}. See in particular section 2.5, \"Bon-matroid of a graph\", pp. 5–6, section 5.6, \"Graphic and co-graphic matroids\", pp. 19–20, and section 9, \"Graphic matroids\", pp. 38–47.</ref>\n\nIt is possible to define dual graphs of graphs embedded on nonplanar surfaces such as the torus, but these duals do not generally have the correspondence between cuts, cycles, and spanning trees required by Whitney's criterion.\n\n==References==\n{{reflist}}\n\n[[Category:Matroid theory]]\n[[Category:Planar graphs]]"
    },
    {
      "title": "Oriented matroid",
      "url": "https://en.wikipedia.org/wiki/Oriented_matroid",
      "text": "[[File:max-flow min-cut example.svg|frame|right|Oriented-matroid theory allows a combinatorial approach to the [[max-flow min-cut theorem]]. A network with the value of flow equal to the capacity of an s-t cut]]\nAn '''oriented matroid''' is a [[mathematics|mathematical]] [[mathematical structure|structure]] that abstracts the properties of [[directed graph]]s, [[Vector space|vector]] arrangements over ordered fields, and [[Arrangement of hyperplanes|hyperplane arrangements]] over [[ordered field]]s.<ref>[[Rockafellar]] 1969. Björner et alia, Chapters 1-3. Bokowski, Chapter 1. Ziegler, Chapter 7.</ref> In comparison, an ordinary (i.e., non-oriented) [[matroid]] abstracts the [[linear independence|dependence]] properties that are common both to [[Graph (discrete mathematics)|graphs]], which are not necessarily ''directed'', and to arrangements of vectors over [[field (mathematics)|fields]], which are not necessarily ''ordered''.<ref>Björner et alia, Chapters 1-3. Bokowski, Chapters 1-4.</ref>\n<ref>Because matroids and oriented matroids are abstractions of other mathematical abstractions, nearly all the relevant books are written for mathematical scientists rather than for the general public. For learning about oriented matroids, a good preparation is to study the textbook on [[linear optimization]] by Nering and Tucker, which is infused with oriented-matroid ideas, and then to proceed to Ziegler's lectures on polytopes.</ref>\n\nAll oriented matroids have an underlying [[matroid]]. Thus, results on ordinary matroids can be applied to oriented matroids. However, the [[conversion (logic)|converse]] is false; some matroids cannot become an oriented matroid by ''orienting'' an underlying structure (e.g., circuits or independent sets).<ref>Björner et alia, Chapter 7.9.</ref>\nThe distinction between matroids and oriented matroids is discussed further below.\n\nMatroids are often useful in areas such as [[Dimension theory (algebra)|dimension theory]] and [[algorithms]].\nBecause of an oriented matroid's inclusion of additional details about the ''oriented'' nature of a structure,\nits usefulness extends further into several areas including [[geometry]] and [[optimization (mathematics)|optimization]].\n\n== Background ==\n\nIn order to abstract the concept of [[Orientation (graph theory)|orientation]] on the edges of a graph to sets, one needs the ability to assign \"direction\" to the elements of a set. The way this achieved is with the following definition of ''signed sets''.\n\n* A ''signed set'', <math>X</math>, combines a set of objects <math>\\underline{X}</math> with a partition of that set into two subsets: <math>X^+</math> and <math>X^-</math>.\n: The members of <math>X^+</math> are called the ''positive elements''; members of <math>X^-</math> are the ''negative elements''.\n\n* The set <math>\\underline{X} = X^+ \\cup X^-</math> is called the ''support'' of <math>X</math>.\n* The ''empty signed set'', <math> \\empty </math>, is defined as the empty set <math> \\underline{\\empty} </math> combined with a \"partition\" of it into two empty sets:  <math>\\emptyset^+</math> and <math>\\emptyset^-</math>.\n* The signed set <math>Y</math> is the ''opposite'' of <math>X</math>, i.e., <math>Y = -X</math>, if and only if <math>Y^+ = X^-</math> and <math>Y^- = X^+</math>\n\nGiven an element of the support <math>x</math>, we will write <math>x</math> for a positive element and <math>-x</math> for a negative element. In this way, a signed set is just adding negative signs to distinguished elements. This will make sense as a \"direction\" only when we consider orientations of larger structures. Then the sign of each element will encode its direction relative to this orientation.\n\n\n\n== Axiomatizations ==\n{{See also|Matroid#Bases and circuits|Axiomatic system|Cryptomorphism}}\n\nLike ordinary matroids, several equivalent [[Axiomatic system|systems of axioms]] exist.\n(Such structures that possess multiple equivalent axiomatizations are called [[cryptomorphism|cryptomorphic]].)\n\n=== Circuit axioms ===\nLet <math>E</math> be any set.  We refer to <math>E</math> as the ''ground set''.\nLet <math>\\mathcal{C}</math> be a collection of ''signed sets'', each of which is ''supported'' by a subset of <math>E</math>.\nIf the following axioms hold for <math>\\mathcal{C}</math>, then equivalently <math>\\mathcal{C}</math> is the set of ''signed circuits''\nfor an ''oriented matroid'' on <math>E</math>.\n\n* (C0) <math>\\empty \\notin \\mathcal{C}</math>\n* (C1) (symmetric) <math>\\text{ For all } X \\in \\mathcal{C},~ -\\!X \\in \\mathcal{C}.</math>\n* (C2) (incomparable) <math>\\text{ For all } X,Y \\in \\mathcal{C},~~ \\text{ if } X\\subseteq Y\\text{ then } (X=Y \\text{ or } X = -Y).</math>\n* (C3)  (weak elimination) <math>\\text{ For all } X,Y \\in \\mathcal{C}, X \\neq -Y \\text{ with an } e \\in X^+ \\cap Y^- \\text{ there is a } Z \\in \\mathcal{C} \\text{ such that }</math>\n** <math> Z^+ \\subseteq (X^+ \\cup Y^+)\\setminus\\{e\\} \\text{ and }</math>\n** <math> Z^- \\subseteq (X^- \\cup Y^-)\\setminus\\{e\\}.</math>\n\n=== Vector Axioms ===\nThe ''composition'' of signed sets <math>X</math>and <math>Y</math>is the signed set <math>X\\circ Y</math>defined by  <math>\\underline{X\\circ Y}= {\\underline X} \\cup {\\underline Y}</math>, <math>(X\\circ Y)^+  = X^+ \\cup \\left(Y^+ \\setminus X^-\\right)</math>, and <math>(X\\circ Y)^-  = X^- \\cup \\left(Y^- \\setminus X^+\\right)</math>.  The ''vectors'' of an oriented matroid are the compositions of circuits. The vectors <math>\\mathcal V</math> of an oriented matroid satisfy the following axioms:\n\n* <math>\\emptyset \\in \\mathcal V</math>  \n* <math>\\mathcal V = -\\mathcal V</math>\n* for all <math>X, Y\\in \\mathcal V</math>, <math>X\\circ Y \\in \\mathcal V</math>\n* for all <math>X, Y\\in \\mathcal V</math>, <math>e\\in X^+ \\cap Y^- </math>and <math>f\\in (\\underline X \\setminus \\underline Y)\\cup (\\underline Y \\setminus \\underline X) \\cup (X^+\\cap Y^+) \\cup (X^- \\cap Y^-)</math> , there is a <math>Z\\in \\mathcal V</math>such that \n** <math>Z^+ \\subset X^+ \\cup Y^+ \\setminus e</math>\n** <math>Z^- \\subset X^- \\cup Y^- \\setminus e</math>\n** <math>f\\in \\underline Z</math>\n\n=== Chirotope axioms ===\nLet <math>E</math> be as above.  For each non-negative integer  <math>r</math>, a ''chirotope of rank <math>r</math>'' is a function <math>\\chi\\colon E^r\\to \\{-1,0,1\\}</math> that satisfies the following axioms:\n\n* (B0) ''(non-trivial)'': <math>\\chi</math> is not identically zero\n* (B1) ''(alternating)'': For any [[permutation]] <math>\\sigma</math> and <math>x_1,\\dots,x_r\\in E</math>, <math>\\chi\\left(x_{\\sigma(1)},\\dots,x_{\\sigma(r)}\\right)=\\operatorname{sgn}(\\sigma)\\chi\\left(x_1,\\dots,x_r\\right)</math>, where <math>\\operatorname{sgn}(\\sigma)</math> is the [[Parity of a permutation|sign]] of the permutation.\n* (B2) ''(exchange)'': For any <math>x_1,\\dots,x_r,y_1,\\dots,y_r\\in E</math> such that <math>\\chi(y_i,x_2,\\dots,x_r)\\chi(y_1,\\dots,y_{i-1},x_1,y_{i+1},\\dots,y_r)\\ge 0</math> for each <math>i</math>, then we also have <math> \\chi\\left(x_1,\\dots,x_r\\right)\\chi\\left(y_1,\\dots,y_r\\right)\\ge0</math>.\n\nThe term ''chirotope'' is derived from the mathematical notion of [[Chirality (mathematics)|chirality]], which is a concept abstracted from [[Chirality (chemistry)|chemistry]], where it is used to distinguish molecules that have the same structure except for a reflection.\n\n=== Equivalence ===\nEvery chirotope of rank <math>r</math> gives rise to a set of bases of a matroid on <math>E</math> consisting of those <math>r</math>-element subsets that <math>\\chi</math> assigns a nonzero value.<ref>Björner et alia, Chapter 3.5</ref> The chirotope can then sign the circuits of that matroid. If <math> C</math> is a circuit of the described matroid, then <math>C\\subset \\{x_1,\\dots,x_r,x_{r+1}\\}</math> where <math>\\{x_1,\\dots,x_r\\}</math> is a basis. Then <math>C</math> can be signed with positive elements\n:<math>C^+=\\{x_i: (-1)^i\\chi(x_1,\\dots,x_{i-1},x_{i+1},\\dots,x_{r+1})=1\\}</math>\nand negative elements the complement. Thus a chirotope gives rise to the ''oriented bases'' of an oriented matroid. In this sense, (B0) is the nonempty axiom for bases and (B2) is the basis exchange property.\n\n== Examples ==\nOriented matroids are often introduced (e.g., Bachem and Kern) as an abstraction for directed graphs or systems of linear inequalities. Below are the explicit constructions.\n\n=== Directed graphs ===\n{{Main|Directed graph}}\n{{See also|Flow network}}\nGiven a [[Directed graph|digraph]], we define a signed circuit from the standard [[cycle (graph theory)|circuit]] of the graph by the following method. The support of the signed circuit <math>\\textstyle \\underline{X}</math> is the standard set of edges in a minimal cycle. We go along the cycle in the clockwise or anticlockwise direction assigning those edges whose orientation agrees with the direction to the positive elements <math>\\textstyle X^+</math> and those edges whose orientation disagrees with the direction to the negative elements <math>\\textstyle X^-</math>. If <math>\\textstyle \\mathcal{C} </math> is the set of all such <math>\\textstyle X</math>, then <math>\\textstyle \\mathcal{C} </math> is the set of signed circuits of an oriented matroid on the set of edges of the directed graph.\n\n[[File:Directed graph.svg|right|100px|thumb|A directed graph]]\nIf we consider the directed graph on the right, then we can see that there are only two circuits, namely <math>\\textstyle \\{(1,2),(1,3),(3,2)\\}</math> and <math>\\textstyle \\{(3,4),(4,3)\\}</math>. Then there are only four possible signed circuits corresponding to clockwise and anticlockwise orientations, namely <math>\\textstyle\\{ (1,2),-(1,3),-(3,2)\\}</math>,  <math>\\textstyle\\{ -(1,2),(1,3),(3,2)\\}</math>, <math>\\textstyle\\{(3,4),(4,3)\\}</math>, and <math>\\textstyle\\{-(3,4),-(4,3)\\}</math>. These four sets form the set of signed circuits of an oriented matroid on the set <math>\\textstyle \\{ (1,2),(1,3),(3,2),(3,4),(4,3)\\}</math>.\n\n=== Linear algebra ===\n{{See also|Matroid#Matroids from linear algebra}}\nIf <math>\\textstyle E</math> is any finite subset of <math>\\textstyle\\mathbb{R}^n</math>, then the set of minimal linearly dependent sets forms the circuit set of a matroid on <math>\\textstyle E</math>. To extend this construction to oriented matroids, for each circuit <math>\\textstyle \\{v_1,\\dots,v_m\\}</math> there is a minimal linear dependence\n:<math>\\sum_{i=1}^m \\lambda_i v_i =0</math>\nwith <math>\\textstyle \\lambda_i\\in\\mathbb{R}</math>. Then the signed circuit <math>\\textstyle X=\\{X^+,X^-\\}</math> has positive elements <math>\\textstyle X^+=\\{v_i:\\lambda_i>0\\}</math> and negative elements <math>\\textstyle X^-=\\{v_i:\\lambda_i<0\\}</math>. The set of all such <math>\\textstyle X</math> forms the set of signed circuits of an oriented matroid on <math>\\textstyle E</math>. Oriented matroids that can be realized this way are called [[Matroid representation|representable]].\n\nGiven the same set of vectors <math>E</math>, we can define the same oriented matroid with a chirotope <math>\\chi:E^r\\rightarrow\\{-1,0,1\\}</math>. For any <math>x_1,\\dots,x_r\\in E</math> let\n:<math>\\chi(x_1,\\dots,x_r)=\\operatorname{sgn}(\\det(x_1,\\dots,x_r))</math>\nwhere the right hand side of the equation is the sign of the [[determinant]]. Then <math> \\chi</math> is the chirotope of the same oriented matroid on the set <math>E</math>.\n\n=== Hyperplane arrangements ===\n{{Main|Arrangement of hyperplanes}}A real hyperplane arrangement <math>\\mathcal A = \\{H_1, \\ldots, H_n\\}</math> is a finite set of hyperplanes in <math>\\R^d\n</math>, each containing the origin. By picking one side of each hyperplane to be the positive side, we obtain an arrangement of half-spaces. A half-space arrangement breaks down the ambient space into a finite collection of cells, each defined by which side of each hyperplane it lands on. That is, assign each point <math>x\\in \\R^d</math>to the signed set <math>X = (X^+, X^-)</math>with  <math>i \\in X^+</math> if <math>x</math> is on the positive side of <math>H_i</math>and <math>i\\in X^-</math>if <math>x</math> is on the negative side of <math>H_i</math>. This collection of signed sets defines the set of covectors of the oriented matroid, which are the vectors of the dual oriented matroid.<ref>{{Cite book|title=Oriented Matroids|last=M.|first=Björner, Anders. Las Vergnas, Michel. Sturmfels, Bernd. White, Neil. Ziegler, Gunter|date=2000|publisher=Cambridge University Press|isbn=9780511586507|oclc=776950824}}</ref>\n\n=== Convex polytope ===\n{{Main|Convex polytope}}\nZiegler introduces oriented matroids via convex polytopes.\n\n== Results ==\n\n=== Orientability ===\nA standard matroid is called ''orientable'' if its circuits are the supports of signed circuits of some oriented matroid. It is known that all real representable matroids are orientable. It is also known that the class of orientable matroids is closed under taking [[Matroid minor|minors]], however the list of [[Matroid#Forbiden minor characterizations|forbidden minors]] for orientable matroids is known to be infinite.<ref>Björner et alia, Chapter 7.9</ref> In this sense, oriented matroids is a much stricter formalization than regular matroids.\n\n=== Duality ===\n{{See also|Matroid#Duality}}\nMuch like matroids have unique [[Dual matroid|dual]], oriented matroids have unique ''orthogonal'' dual. What this means is the underlying matroids are dual and that the cocircuits are signed so that they are ''orthogonal'' to every circuit. Two signed sets are said to be ''orthogonal'' if the intersection of their supports is empty or if the restriction of their positive elements to the intersection and negative elements to the intersection form two nonidentical and non-opposite signed sets. The existence and uniqueness of the dual oriented matroid depends on the fact that every signed circuit is orthogonal to every signed cocircuit.<ref>Björner et alia, Chapter 3.4</ref>  To see why orthogonality is necessary for uniqueness one needs only to look to the digraph example above. We know that for planar graphs, that the dual of the circuit matroid is the circuit matroid of the graph's [[Dual graph|planar dual]]. Thus there are as many different oriented matroids that are dual as there are ways to orient a graph and its dual.\n\nTo see the explicit construction of this unique orthogonal dual oriented matroid, consider an oriented matroid's chirotope <math>\\chi:E^r\\rightarrow\\{-1,0,1\\}</math>. If we consider a list of elements of <math> x_1,\\dots,x_k \\in E</math> as a cyclic permutation then we define <math>\\operatorname{sgn}(x_1,\\dots,x_k)</math> to be the sign of the associated permutation. If <math>\\chi^*:E^{|E|-r}\\rightarrow\\{-1,0,1\\}</math> is defined as\n:<math>\\chi^*(x_1,\\dots,x_r)\\mapsto \\chi(x_{r+1},\\dots,x_{|E|})\\operatorname{sgn}(x_1,\\dots,x_r,x_{r+1},\\dots,x_{|E|}),</math>\nthen <math>\\chi^*</math> is the chirotope of the unique orthogonal dual oriented matroid.<ref>Björner et alia, Chapter 3.6</ref>\n\n=== Topological representation ===\n[[File:Pappos pseudo.svg|thumb|200px| This is an example of a pseudoline arrangement that is [[Pappus's hexagon theorem|distinct]] from any line arrangement.]]\nNot all oriented matroids are representable—that is, not all have realizations as point configurations, or, equivalently, hyperplane arrangements. However, in some sense, all oriented matroids come close to having realizations are hyperplane arrangements. In particular, the  [[Jon Folkman|Folkman-]]Lawrence topological representation theorem states that any oriented matroid has a realization as an [[Pseudoline#Other types of arrangement|arrangement of pseudospheres]]. A <math>d</math>-dimensional ''pseudosphere'' is an embedding of <math>e:S^d\\hookrightarrow S^{d+1}</math> such that there exists a homeomorphism <math>h:S^{d+1}\\rightarrow S^{d+1}</math> so that <math>h \\circ e </math> embeds <math>S^d</math> as an equator of <math>S^{d+1}</math>. In this sense a pseudosphere is just a [[Tame manifold|tame]] sphere (as opposed to [[Alexander horned sphere|wild spheres]]). A ''pseudosphere arrangement in <math>S^d</math>'' is a collection of pseudospheres that intersect along pseudospheres. Finally, the Folkman Lawrence topological representation theorem states that every oriented matroid of rank <math>d+1</math> can be obtained from a pseudosphere arrangement in <math>S^d</math>.<ref>Björner et alia, Chapter 5.2</ref>\n\n=== Geometry ===\n[[File:Shapley–Folkman lemma.svg|thumb|300px|alt=Minkowski addition of four line-segments. The left-hand pane displays four sets, which are displayed in a two-by-two array. Each of the sets contains exactly two points, which are displayed in red. In each set, the two points are joined by a pink line segment, which is the convex hull of the original set. Each set has exactly one point that is indicated with a plus symbol. In the top row of the two-by-two array, the plus symbol lies in the interior of the line segment; in the bottom row, the plus symbol coincides with one of the red points. This completes the description of the left-hand pane of the diagram. The right-hand pane displays the Minkowski sum of the sets, which is the union of the sums having exactly one point from each summand set; for the displayed sets, the sixteen sums are distinct points, which are displayed in red: The right-hand red sum points are the sums of the left-hand red summand points. The convex hull of the sixteen red points is shaded in pink. In the pink interior of the right-hand sumset lies exactly one plus-symbol, which is the (unique) sum of the plus-symbols from the right-hand side. The right-hand plus symbol is indeed the sum of the four plus-symbols from the left-hand sets, precisely two points from the original non-convex summand sets and two points from the convex hulls of the remaining summand sets.|\n|A zonotope, which is a Minkowski sum of line segments, is a fundamental model for oriented matroids. The sixteen dark red points (on the right) form the Minkowski sum of the four non-convex sets (on the left), each of which consists of a pair of red points. Their convex hulls (shaded pink) contain plus signs (+): The right plus sign is the sum of the left plus signs.]]\n{{Main|Combinatorial geometry}}\n{{See also|Convex polytope|Zonotope}}\nThe theory of oriented matroids has influenced the development of [[combinatorial geometry]], especially the theory of  [[convex polytope]]s, [[zonotope]]s, and of configurations of vectors ([[arrangement of hyperplanes|arrangements of hyperplanes]]).<ref>Bachem and Kern, Chapters 1–2 and 4–9. Björner et alia, Chapters 1–8. Ziegler, Chapter 7–8. Bokowski, Chapters 7–10.</ref> Many results—[[Carathéodory's theorem (convex hull)|Carathéodory's theorem]], [[Helly's theorem]], [[Radon's theorem]], the [[Hahn–Banach theorem]], the [[Krein–Milman theorem]], the [[Farkas lemma|lemma of Farkas]]—can be formulated using appropriate oriented matroids.<ref>Bachem and Wanka, Chapters 1–2, 5, 7–9. Björner et alia, Chapter 8.</ref>\n\n=== Optimization ===\n[[File:Simplex description.png|thumb|240px|In convex geometry, the simplex algorithm for linear programming is interpreted as tracing a path along the vertices of a convex polyhedron. Oriented matroid theory studies the combinatorial invariants that are revealed in the sign patterns of the matrices that appear as pivoting algorithms exchange bases.]]\n{{See also|Linear programming|Quadratic programming|Criss-cross algorithm}}\n\nThe development of an axiom system for oriented matroids was initiated by [[R.&nbsp;Tyrrell Rockafellar]] to describe the sign patterns of the matrices arising through the pivoting operations of Dantzig's simplex algorithm; Rockafellar was inspired by [[Albert W. Tucker]]'s studies of such sign patterns in \"Tucker tableaux\".\n<ref>{{cite book\n|first=R.&nbsp;T.\n|last=Rockafellar\n| authorlink=R. Tyrrell Rockafellar\n| chapter=The elementary vectors of a subspace of <math>R^N</math> (1967)\n|pages=104–127\n|editor=[[R. C. Bose]] and T.&nbsp;A. Dowling\n|year=1969\n|title=Combinatorial Mathematics and its Applications\n|series=The University of North Carolina Monograph Series in Probability and Statistics\n|location=Chapel Hill, North Carolina\n|publisher=University of North Carolina Press.\n|issue=4\n|mr=278972\n|chapter-url=http://www.math.washington.edu/~rtr/papers/rtr-ElemVectors.pdf|ref=harv|id=[http://www.math.washington.edu/~rtr/papers/rtr-ElemVectors.pdf PDF reprint]}}</ref>\n\nThe theory of oriented matroids has led to breakthroughs in [[combinatorial optimization]]. In [[linear programming]], it was the language in which [[Robert G. Bland]] formulated his [[Bland's rule|pivoting rule]], by which the [[simplex algorithm]] avoids cycles. Similarly, it was used by Terlaky and Zhang to prove that their <!-- versions of the --> [[criss-cross algorithm]]s have finite termination for [[linear programming]] problems.  Similar results were made in convex [[quadratic programming]] by Todd and Terlaky.<ref>Björner et alia, Chapters 8–9.  Fukuda and Terlaky. Compare Ziegler.</ref> It has been applied to [[linear-fractional programming]],<ref name=\"LF99Hyperbolic\">{{harvtxt|Illés|Szirmai|Terlaky|1999}}</ref> [[quadratic programming|quadratic-programming]] problems, and [[linear complementarity problem]]s.<ref name=\"FukudaTerlaky\">{{harvtxt|Fukuda|Terlaky|1997}}</ref><ref name=\"FTNamiki\">{{harvtxt|Fukuda|Terlaky|1997|p=385}}</ref><ref name=\"FukudaNamiki\">{{harvtxt|Fukuda|Namiki|1994|p=367}}</ref>\n\nOutside of [[combinatorial optimization]], OM theory also appears in [[convex minimization]] in Rockafellar's theory of \"monotropic programming\" and related notions of \"fortified descent\".<ref>[[Rockafellar]] 1984 and 1998.</ref> Similarly, [[matroid]] theory has influenced the development of combinatorial algorithms, particularly the [[greedy algorithm]].<ref>Lawler. [[Rockafellar]] 1984 and 1998.</ref> More generally, a [[greedoid]] is useful for studying the finite termination of algorithms.\n\n== References ==\n{{reflist|30em}}\n\n<!-- {{More footnotes|date=January 2010}} -->\n\n== Further reading ==\n\n=== Books ===\n* A. Bachem and W. Kern. ''Linear Programming Duality: An Introduction to Oriented Matroids''. Universitext. Springer-Verlag, 1992.\n* {{cite book | last=Björner | first=Anders | last2=Las Vergnas | first2=Michel  | author2-link = Michel Las Vergnas | last3=Sturmfels | first3=Bernd | authorlink3=Bernd Sturmfels | last4=White | first4=Neil | last5=Ziegler | first5=Günter | authorlink5=Günter M. Ziegler | title=Oriented Matroids | publisher=[[Cambridge University Press]] | year=1999 | isbn=978-0-521-77750-6 | zbl=0944.52006 | series=Encyclopedia of Mathematics and Its Applications | volume=46 | edition=2nd }}\n* {{cite book | last=Bokowski | first=Jürgen | title=Computational oriented matroids. Equivalence classes of matrices within a natural framework | publisher=[[Cambridge University Press]] | year=2006 | isbn=978-0-521-84930-2 | zbl=1120.52011}}\n* {{cite book | first=Eugene | last=Lawler | authorlink=Eugene Lawler | title = Combinatorial Optimization: Networks and Matroids | year = 2001 | publisher = Dover | isbn = 978-0-486-41453-9 | zbl=1058.90057 }}\n* Evar D. Nering and [[Albert W. Tucker]], 1993, ''Linear Programs and Related Problems'', Academic Press. (elementary<!-- but profound -->)\n* [[R. T. Rockafellar]]. ''Network Flows and Monotropic Optimization'', Wiley-Interscience, 1984 (610 pages); republished by Athena Scientific of [[Dimitri Bertsekas]], 1998.\n* [[Günter M. Ziegler|Ziegler, Günter M.]], ''Lectures on Polytopes'', Springer-Verlag, New York, 1994.\n* Richter-Gebert, J. and [[Günter M. Ziegler|G. Ziegler]], Oriented Matroids, In ''Handbook of Discrete and Computational Geometry'', [[Jacob E. Goodman|J. Goodman]] and J.O'Rourke, (eds.), CRC Press, Boca Raton, 1997, pp.&nbsp;111–132.\n\n=== Articles ===\n* A. Bachem, A. Wanka, Separation theorems for oriented matroids, ''Discrete Math.'' 70 (1988) 303—310.\n*[[Robert G. Bland]], New finite pivoting rules for the simplex method, ''Math. Oper. Res.'' 2 (1977) 103–107.\n* {{cite journal|last1=Jon|first1=Folkman|authorlink1=Jon Folkman\n|last2=Jim|first2=Lawrence|authorlink2=Jim Lawrence (mathematician)|title=Oriented Matroids|journal=J. Combin. Theory Ser. B|volume=25|issue=2|date=October 1978|pages=199–236|doi=10.1016/0095-8956(78)90039-4}}\n* {{cite news|first1=Komei|last1=Fukuda|authorlink1=Komei Fukuda|first2=Tamás|last2=Terlaky|title=Criss-cross methods: A fresh view on pivot algorithms |journal=Mathematical Programming, Series B|volume=79|number=1—3|pages=369–395|editors=Thomas&nbsp;M. Liebling and Dominique de&nbsp;Werra|publisher=North-Holland Publishing&nbsp;Co. |location=Amsterdam|year=1997|doi=10.1007/BF02614325|mr=1464775|ref=harv}}\n*  {{cite journal|last1=Fukuda|first1=Komei|authorlink1=Komei Fukuda|last2=Namiki|first2=Makoto|title=On extremal behaviors of Murty's least index method|journal=Mathematical Programming|date=March 1994|pages=365–370|volume=64|doi=10.1007/BF01582581|mr=1286455|issue=1|ref=harv}}\n* {{cite journal|title=The finite criss-cross method for hyperbolic programming|journal=European Journal of Operational Research|volume=114|\npages=198–214|year=1999|issn=0377-2217|doi=10.1016/S0377-2217(98)00049-6|url=http://www.sciencedirect.com/science/article/B6VCT-3W3DFHB-M/2/4b0e2fcfc2a71e8c14c61640b32e805a|first1=Tibor|last1=Illés|first2=Ákos|last2=Szirmai|first3=Tamás|last3=Terlaky|ref=harv|id=[http://www.cas.mcmaster.ca/~terlaky/files/dut-twi-96-103.ps.gz PDF preprint]|issue=1|citeseerx=10.1.1.36.7090}}\n* [[R. T. Rockafellar]]. The elementary vectors of a subspace of <math>R^n</math>, in ''Combinatorial Mathematics and its Applications'', R. C. Bose and T. A. Dowling (eds.), Univ. of North Carolina Press, 1969, 104-127.\n* {{cite journal|last=Roos|first=C.|title=An exponential example for Terlaky's pivoting rule for the criss-cross simplex method|journal=Mathematical Programming|volume=46|year=1990|series=Series&nbsp;A|doi=10.1007/BF01585729|mr=1045573|ref=harv|issue=1|pages=79–84}}<!-- Google scholar reported no free versions -->\n* {{cite journal|last=Terlaky|first=T.|title=A convergent criss-cross method|journal=Optimization: A Journal of Mathematical Programming and Operations Research|volume=16|year=1985|pages=683–690|issn=0233-1934|doi=10.1080/02331938508843067|ref=harv|mr=798939|issue=5}}\n* {{cite journal|last=Terlaky|first=Tamás|authorlink=Tamás Terlaky|title=A finite crisscross method for oriented matroids|volume=42|year=1987|pages=319–327|journal=Journal of Combinatorial Theory|series=Series&nbsp;B|issn=0095-8956|doi=10.1016/0095-8956(87)90049-9|mr=888684|ref=harv|issue=3}}\n* {{cite journal|last1=Terlaky|first1=Tamás|last2=Zhang|first2=Shu&nbsp;Zhong|title=Pivot rules for linear programming: A Survey on recent theoretical developments|journal=Annals of Operations Research|volume=46–47|year=1993|number=1|pages=203–233|doi=10.1007/BF02096264|mr=1260019|citeseerx = 10.1.1.36.7658 |issn=0254-5330|ref=harv}}\n* Michael J. Todd, Linear and quadratic programming in oriented matroids, ''J. Combin. Theory Ser. B'' 39 (1985) 105—133.\n* {{cite journal|last=Wang|first=Zhe&nbsp;Min|title=A finite conformal-elimination free algorithm over oriented&nbsp;matroid programming|journal=Chinese Annals of Mathematics (Shuxue Niankan&nbsp;B&nbsp;Ji)|series=Series&nbsp;B|volume=8|year=1987|pages=120–125|issn=0252-9599|mr=886756|ref=harv|issue=1}}\n\n=== On the web ===\n*{{cite journal|url=http://www.combinatorics.org/Surveys/ds4.pdf|title=Oriented Matroids Today|year=1998|last=Ziegler|first=Günter|authorlink= Günter Ziegler|journal=The Electronic Journal of Combinatorics}}\n<!-- *{{cite book|last=Finschi|first=Lukas|title=A Graph Theoretical Approach for Reconstruction and Generation of Oriented Matroids|publisher=Swiss Federal Institute of Technology|location=Zurich|date=2001|pages=199|url=http://www.math.ethz.ch/research/groups/ifor/publications/2001_diss_finschi.pdf}} -->\n*{{cite web|url=http://www.ams.org/featurecolumn/archive/oriented1.html|title=Oriented Matroids: The Power of Unification|last=Malkevitch|first=Joseph|work=Feature Column|publisher=American Mathematical Society|accessdate=2009-09-14}}\n\n== External links ==\n* [https://web.archive.org/web/20110728105602/http://www.ifor.math.ethz.ch/~fukuda/ Komei Fukuda (ETH Zentrum, Zurich)] with [https://web.archive.org/web/20110728105643/http://www.ifor.math.ethz.ch/~fukuda/publ/publ.html publications] including [ftp://ftp.ifor.math.ethz.ch/pub/fukuda/reports/fukuda1982thesis.pdf ''Oriented matroid programming'' (1982 Ph.D. thesis)]\n* [http://coral.ie.lehigh.edu/~terlaky/ Tamás Terlaky  (Lehigh University)] with [http://coral.ie.lehigh.edu/~terlaky/publications publications]\n\n[[Category:Oriented matroids| ]]"
    },
    {
      "title": "Arrangement of hyperplanes",
      "url": "https://en.wikipedia.org/wiki/Arrangement_of_hyperplanes",
      "text": "In [[geometry]] and [[combinatorics]], an '''arrangement of hyperplanes''' is an [[arrangement (space partition)|arrangement]] of a finite set ''A'' of [[hyperplane]]s in a [[linear space|linear]], [[affine geometry|affine]], or [[projective geometry|projective]] space ''S''.  \nQuestions about a hyperplane arrangement ''A'' generally concern geometrical, topological, or other properties of the '''complement''', ''M''(''A''), which is the set that remains when the hyperplanes are removed from the whole space.  One may ask how these properties are related to the arrangement and its intersection semilattice.\nThe '''intersection [[semilattice]]''' of ''A'', written ''L''(''A''), is the set of all [[Euclidean subspace|subspaces]] that are obtained by intersecting some of the hyperplanes; among these subspaces are ''S'' itself, all the individual hyperplanes, all intersections of pairs of hyperplanes, etc. (excluding, in the affine case, the empty set).  These subspaces are called the '''flats''' of ''A''.  The intersection semilattice ''L''(''A'') is partially ordered by ''reverse inclusion''.  \n\nIf the whole space ''S'' is 2-dimensional, the hyperplanes are [[line (mathematics)|line]]s; such an arrangement is often called an '''[[arrangement of lines]]'''.  Historically, real arrangements of lines were the first arrangements investigated.  If ''S'' is 3-dimensional one has an '''arrangement of planes'''.\n[[File:Arrangement hyperplans.png|thumbnail|A hyperplane arrangement in space]]\n== General theory ==\n=== The intersection semilattice and the matroid ===\n\nThe intersection semilattice ''L''(''A'') is a meet semilattice and more specifically is a [[geometric semilattice]]. If the arrangement is linear or projective, or if the intersection of all hyperplanes is nonempty, the intersection lattice is a [[geometric lattice]].\n(This is why the semilattice must be ordered by reverse inclusion&mdash;rather than by inclusion, which might seem more natural but would not yield a geometric (semi)lattice.)\n\nWhen ''L''(''A'') is a lattice, the [[matroid]] of ''A'', written ''M''(''A''), has ''A'' for its ground set and has rank function ''r''(''S'') := codim(''I''), where ''S'' is any subset of ''A'' and ''I'' is the intersection of the hyperplanes in ''S''.  In general, when ''L''(''A'') is a semilattice, there is an analogous matroid-like structure called a [[semimatroid]], which is a generalization of a matroid (and has the same relationship to the intersection semilattice as does the matroid to the lattice in the lattice case), but is not a matroid if ''L''(''A'') is not a lattice.\n\n=== Polynomials ===\n\nFor a subset ''B'' of ''A'', let us define ''f''(''B'') := the intersection of the hyperplanes in ''B''; this is ''S'' if ''B'' is empty.  \nThe '''characteristic polynomial of''' ''A'', written ''p<sub>A</sub>''(''y''), can be defined by \n\n:<math>p_A(y) := \\sum_B (-1)^{|B|}y^{\\dim f(B)},</math>\n\nsummed over all subsets ''B'' of ''A'' except, in the affine case, subsets whose intersection is empty.  (The dimension of the empty set is defined to be &minus;1.)  This polynomial helps to solve some basic questions; see below.\nAnother polynomial associated with ''A'' is the '''Whitney-number polynomial''' ''w<sub>A</sub>''(''x'', ''y''), defined by\n\n:<math>w_A(x,y) := \\sum_B x^{n-\\dim f(B)} \\sum_C (-1)^{|C-B|}y^{\\dim f(C)},</math>\n\nsummed over ''B'' ⊆ ''C'' ⊆ ''A'' such that ''f''(''B'') is nonempty.\n\nBeing a geometric lattice or semilattice, ''L''(''A'') has a characteristic polynomial, ''p''<sub>''L''(''A'')</sub>(''y''), which has an extensive theory (see [[Matroid#Characteristic_polynomial|matroid]]).  Thus it is good to know that ''p''<sub>''A''</sub>(''y'') = ''y''<sup>''i''</sup> ''p''<sub>''L''(''A'')</sub>(''y''), where ''i'' is the smallest dimension of any flat, except that in the projective case it equals ''y''<sup>''i'' + 1</sup>''p''<sub>''L''(''A'')</sub>(''y'').  \nThe Whitney-number polynomial of ''A'' is similarly related to that of ''L''(''A'').  \n(The empty set is excluded from the semilattice in the affine case specifically so that these relationships will be valid.)\n\n=== The Orlik–Solomon algebra ===\n\nThe intersection semilattice determines another combinatorial invariant of the arrangement, the [[Orlik–Solomon algebra]]. To define it, fix a commutative subring ''K'' of the base field and form the exterior algebra ''E'' of the vector space\n:<math>\\bigoplus_{H \\in A} K e_H </math>\ngenerated by the hyperplanes.\nA [[chain complex]] structure is defined on ''E'' with the usual boundary operator <math>\\partial</math>.\nThe Orlik–Solomon algebra is then the quotient of ''E'' by the [[Ideal (ring theory)|ideal]] generated by elements of the form <math>e_{H_1} \\wedge \\cdots \\wedge e_{H_p}</math> for which <math>H_1, \\dots, H_p</math> have empty intersection, and by boundaries of elements of the same form for which <math>H_1 \\cap \\cdots \\cap H_p</math> has [[codimension]] less than ''p''.\n\n== Real arrangements ==\n\nIn [[real number|real]] [[affine space]], the complement is disconnected: it is made up of separate pieces called '''cells''' or '''regions''' or '''chambers''', each of which is either a bounded region that is a [[Convex polygon|convex]] [[polytope]], or an unbounded region that is a convex [[polyhedron#General|polyhedral]] region which goes off to infinity.  \nEach flat of ''A'' is also divided into pieces by the hyperplanes that do not contain the flat; these pieces are called the '''faces''' of ''A''.  \nThe regions are faces because the whole space is a flat.  \nThe faces of codimension 1 may be called the '''facets''' of ''A''.  \nThe '''face semilattice''' of an arrangement is the set of all faces, ordered by ''inclusion''.  Adding an extra top element to the face semilattice gives the '''face lattice'''.\n\nIn two dimensions (i.e., in the real affine [[plane (mathematics)|plane]]) each region is a convex [[polygon]] (if it is bounded) or a convex polygonal region which goes off to infinity.  \n* As an example, if the arrangement consists of three parallel lines, the intersection semilattice consists of the plane and the three lines, but not the empty set.  There are four regions, none of them bounded.  \n* If we add a line crossing the three parallels, then the intersection semilattice consists of the plane, the four lines, and the three points of intersection.  There are eight regions, still none of them bounded.  \n* If we add one more line, parallel to the last, then there are 12 regions, of which two are bounded [[parallelogram]]s.\n\nTypical problems about an arrangement in ''n''-dimensional real space are to say how many regions there are, or how many faces of dimension 4, or how many bounded regions.  These questions can be answered just from the intersection semilattice.  For instance, two basic theorems, from Zaslavsky (1975), are that the number of regions of an affine arrangement equals (&minus;1)<sup>''n''</sup>''p''<sub>''A''</sub>(&minus;1) and the number of bounded regions equals (&minus;1)<sup>''n''</sup>p<sub>''A''</sub>(1).  Similarly, the number of ''k''-dimensional faces or bounded faces can be read off as the coefficient of ''x''<sup>''n''&minus;''k''</sup> in (&minus;1)<sup>''n''</sup> w<sub>''A''</sub> (&minus;''x'', &minus;1) or (&minus;1)<sup>''n''</sup>''w''<sub>''A''</sub>(&minus;''x'', 1).\n\n{{harvtxt|Meiser|1993}} designed a fast algorithm to determine the face of an arrangement of hyperplanes containing an input point.\n\nAnother question about an arrangement in real space is to decide how many regions are [[simplex|simplices]] (the ''n''-dimensional generalization of [[triangle]]s and [[tetrahedron|tetrahedra]]).  This cannot be answered based solely on the intersection semilattice. The [[McMullen problem]] asks for the smallest arrangement of a given dimension in general position in [[real projective space]] for which there does not exist a cell touched by all hyperplanes.\n\nA real linear arrangement has, besides its face semilattice, a '''[[poset]] of regions''', a different one for each region.  This poset is formed by choosing an arbitrary base region, ''B''<sub>0</sub>, and associating with each region ''R'' the set ''S''(''R'') consisting of the hyperplanes that separate ''R'' from ''B''. The regions are partially ordered so that ''R''<sub>1</sub> ≥ ''R''<sub>2</sub> if ''S''(''R''<sub>1</sub>, ''R'') contains ''S''(''R''<sub>2</sub>, ''R'').  In the special case when the hyperplanes arise from a [[root system]], the resulting poset is the corresponding [[Weyl group]] with the weak Bruhat order. In general, the poset of regions is [[ranked poset|ranked]] by the number of separating hyperplanes and its [[Incidence algebra|Möbius function]] has been computed {{harv|Edelman|1984}}.\n\nVadim Schechtman and [[Alexander Varchenko]] introduced a matrix indexed by the regions. The matrix element for the region <math>R_i</math> and <math>R_j</math> is given by the product of indeterminate variables <math>a_H</math> for every hyperplane H that separates these two regions. If these variables are specialized to be all value q, then this is called the q-matrix (over the Euclidean domain <math>\\mathbb{Q}[q]</math>) for the arrangement and much information is contained in its [[Smith normal form]].\n\n==Complex arrangements==\n\nIn [[complex number|complex]] affine space (which is hard to visualize because even the complex affine plane has four real dimensions), the complement is connected (all one piece) with holes where the hyperplanes were removed.\n\nA typical problem about an arrangement in complex space is to describe the holes.\n\nThe basic theorem about complex arrangements is that the [[cohomology]] of the complement ''M''(''A'') is completely determined by the intersection semilattice.  To be precise, the cohomology ring of ''M''(''A'') (with integer coefficients) is [[isomorphic]] to the Orlik–Solomon algebra on '''Z'''.\n\nThe isomorphism can be described explicitly and gives a presentation of the cohomology in terms of generators and relations, where generators are represented (in the [[de Rham cohomology]]) as logarithmic [[differential form]]s\n\n:<math>\\frac{1}{2\\pi i}\\frac{d\\alpha}{\\alpha}.</math>\n\nwith <math>\\alpha</math> any linear form defining the generic hyperplane of the arrangement.\n\n==Technicalities==\n\nSometimes it is convenient to allow the '''degenerate hyperplane''', which is the whole space ''S'', to belong to an arrangement.  If ''A'' contains the degenerate hyperplane, then it has no regions because the complement is empty.  However, it still has flats, an intersection semilattice, and faces.  The preceding discussion assumes the degenerate hyperplane is not in the arrangement.\n\nSometimes one wants to allow repeated hyperplanes in the arrangement.  We did not consider this possibility in the preceding discussion, but it makes no material difference.\n\n==See also==\n*[[Supersolvable arrangement]]\n\n==References==\n*{{Springer|id=A/a110700|title=Arrangement of hyperplanes}}\n*{{citation\n | last = Edelman | first = Paul H.\n | doi = 10.2307/1999150\n | issue = 2\n | journal = [[Transactions of the American Mathematical Society]]\n | pages = 617–631\n | title = A partial order on the regions of <math>\\mathbb{R}^n</math> dissected by hyperplanes\n | volume = 283\n | year = 1984\n | mr = 0737888\n | jstor = 1999150}}.\n*{{citation\n | last = Meiser | first = Stefan\n | doi = 10.1006/inco.1993.1057\n | issue = 2\n | journal = Information and Computation\n | pages = 286–303\n | title = Point location in arrangements of hyperplanes\n | volume = 106\n | year = 1993\n | mr = 1241314}}.\n*{{citation\n | last1 = Orlik | first1 = Peter\n | last2 = Terao | first2 = Hiroaki\n | author1-link=Peter Orlik\n | author2-link=Hiroaki Terao\n | doi = 10.1007/978-3-662-02772-1\n | location = Berlin\n | publisher = Springer-Verlag\n | series = Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences]\n | title = Arrangements of Hyperplanes\n | volume = 300\n | year = 1992\n | mr = 1217488}}.\n*{{cite book|last = Stanley|first=Richard\n | author-link = Richard P. Stanley \n |title=Enumerative Combinatorics|volume=1|chapter=3.11 Hyperplane Arrangements|edition=2nd|ISBN=1107602629|year=2011|publisher=Cambridge University Press}}\n*{{citation\n | last = Zaslavsky | first = Thomas\n | author-link = Thomas Zaslavsky\n | doi = 10.1090/memo/0154\n | issue = No. 154\n | location = Providence, R.I.\n | publisher = [[American Mathematical Society]]\n | journal = Memoirs of the American Mathematical Society\n | title = Facing up to arrangements: face-count formulas for partitions of space by hyperplanes\n | year = 1975\n | mr = 0357135}}.\n\n[[Category:Discrete geometry]]\n[[Category:Combinatorics]]\n[[Category:Oriented matroids]]"
    },
    {
      "title": "CC system",
      "url": "https://en.wikipedia.org/wiki/CC_system",
      "text": "In [[computational geometry]], a '''CC system''' or '''counterclockwise system''' is a [[ternary relation]] {{math|''pqr''}} introduced by [[Donald Knuth]] to model the clockwise ordering of triples of points in [[general position]] in the [[Euclidean plane]].{{sfnp|Knuth|1992}}\n\n==Axioms==\nA CC system is required to satisfy the following axioms, for all distinct points ''p'', ''q'', ''r'', ''s'', and ''t'':{{sfnp|Knuth|1992|p=4}}\n\n# Cyclic symmetry: If {{math|''pqr''}} then {{math|''qrp''}}.\n# Antisymmetry: If {{math|''pqr''}} then not {{math|''prq''}}.\n# Nondegeneracy: Either {{math|''pqr''}} or {{math|''prq''}}.\n# Interiority: If {{math|''tqr''}} and {{math|''ptr''}} and {{math|''pqt''}}, then {{math|''pqr''}}.\n# Transitivity: If {{math|''tsp''}} and {{math|''tsq''}} and {{math|''tsr''}}, and {{math|''tpq''}} and {{math|''tqr''}}, then {{math|''tpr''}}.\n\nTriples of points that are not distinct are not considered as part of the relation.\n\n==Construction from planar point sets==\nA CC system may be defined from any set of points in the [[Euclidean plane]], with no three of the points collinear, by including in the relation a triple {{math|''pqr''}} of distinct points whenever the triple lists these three points in counterclockwise order around the triangle that they form. Using the [[Cartesian coordinate]]s of the points, the triple ''pqr'' is included in the relation exactly when{{sfnp|Knuth|1992|p=3}}\n:<math>\\det\\left( \\begin{array}{ccc}\nx_p & y_p & 1 \\\\\nx_q & y_q & 1 \\\\\nx_r & y_r & 1 \\end{array} \\right) > 0.</math>\nThe condition that the points are in general position is equivalent to the requirement that this matrix [[determinant]] is never zero for distinct points ''p'', ''q'', and ''r''.\n\nHowever, not every CC system comes from a Euclidean point set in this way.{{sfnp|Knuth|1992|pp=25–26}}\n\n==Equivalent notions==\nCC systems can also be defined from [[Arrangement of lines|pseudoline arrangement]]s, or from [[sorting network]]s in which the compare-exchange operations only compare adjacent pairs of elements (as in for instance [[bubble sort]]), and every CC system can be defined in this way.{{sfnp|Knuth|1992|pp=29–35}} This relation is not one-to-one, but the numbers of nonisomorphic CC systems on ''n'' points, of pseudoline arrangements with ''n'' lines, and of sorting networks on ''n'' values, are within polynomial factors of each other.{{sfnp|Knuth|1992|p=35}}\n\nThere exists a two-to-one correspondence between CC systems and uniform acyclic [[oriented matroid]]s of [[matroid rank|rank]] 3.{{sfnp|Knuth|1992|p=40}} These matroids in turn have a 1-1 correspondence to topological equivalence classes of pseudoline arrangements with one marked cell.{{sfnp|Knuth|1992|p=35}}\n\n==Algorithmic applications==\nThe information given by a CC system is sufficient to define a notion of a [[convex hull]] within a CC system. The convex hull is the set of ordered pairs ''pq'' of distinct points with the property that, for every third distinct point ''r'', ''pqr'' belongs to the system. It forms a cycle, with the property that every three points of the cycle, in the same cyclic order, belong to the system.{{sfnp|Knuth|1992|pp=45–46}} By adding points one at a time to a CC system, and maintaining the convex hull of the points added so far in its cyclic order using a [[binary search tree]], it is possible to construct the convex hull in time ''O''(''n''&nbsp;log&nbsp;''n''), matching the known time bounds for convex hull algorithms for Euclidean points.{{sfnp|Knuth|1992|p=47}}\n\nIt is also possible to find a single convex hull vertex, as well as the combinatorial equivalent of a bisecting line through a system of points, from a CC system in [[linear time]]. The construction of an extreme vertex allows the [[Graham scan]] algorithm for convex hulls to be generalized from point sets to CC systems, with a number of queries to the CC system that matches (to within lower-order terms) the number of comparisons needed in [[comparison sort]]ing.{{sfnp|Aichholzer|Miltzow|Pilz|2013}}\n\n==Combinatorial enumeration==\nThe number of non-isomorphic CC systems on ''n'' points is{{sfnp|Knuth|1992|p=35}}{{sfnp|Beygelzimer|Radziszowski|2002}}\n:1, 1, 1, 2, 3, 20, 242, 6405, 316835, 28627261 ... {{OEIS|A006246}}\n\nThese numbers grow exponentially in ''n''<sup>2</sup>;{{sfnp|Knuth|1992|p=37}} in contrast, the number of realizable CC systems grows exponentially only in &Theta;(''n''&nbsp;log&nbsp;''n'').{{sfnp|Knuth|1992|p=40}}\n\nMore precisely, the number ''C<sub>n</sub>'' of non-isomorphic CC systems on ''n'' points is at most{{sfnp|Knuth|1992|p=39}}\n:<math>3^{\\binom{n}{2}}.</math>\nKnuth conjectures more strongly that these numbers obey the recursive inequality\n:<math>C_n\\le n2^{n-2} C_{n-1}.</math>\n\n==Notes==\n{{reflist|30em}}\n\n==References==\n*{{citation\n | last1 = Aichholzer | first1 = Oswin\n | last2 = Miltzow | first2 = Tillmann\n | last3 = Pilz | first3 = Alexander\n | doi = 10.1016/j.comgeo.2013.05.001\n | issue = 8\n | journal = Computational Geometry\n | mr = 3061458\n | pages = 970–978\n | title = Extreme point and halving edge search in abstract order types\n | volume = 46\n | year = 2013| pmc = 3688538}}.\n*{{citation\n | last1 = Beygelzimer | first1 = Alina\n | last2 = Radziszowski | first2 = Stanisław\n | doi = 10.1016/S0012-365X(02)00430-2\n | issue = 2-3\n | journal = Discrete Mathematics\n | mr = 1935728\n | pages = 267–283\n | title = On halving line arrangements\n | volume = 257\n | year = 2002}}.\n*{{Citation |last=Knuth |first=Donald E. |authorlink=Donald Knuth |year=1992 |title=Axioms and hulls |series=Lecture Notes in Computer Science |volume=606 |location=Heidelberg |publisher=Springer-Verlag |pages=ix+109 |isbn=3-540-55611-7 |url=http://www-cs-faculty.stanford.edu/~uno/aah.html |accessdate=5 May 2011|doi=10.1007/3-540-55611-7|mr=1226891}}.\n\n[[Category:Computational geometry]]\n[[Category:Oriented matroids]]\n[[Category:Euclidean plane geometry]]"
    },
    {
      "title": "Approximate max-flow min-cut theorem",
      "url": "https://en.wikipedia.org/wiki/Approximate_max-flow_min-cut_theorem",
      "text": "{{technical|date=January 2016}}\n\nApproximate [[max-flow min-cut theorem]]s are mathematical propositions in [[Flow network|network flow]] theory. They deal with the relationship between maximum flow rate (\"max-flow\") and [[minimum cut]] (\"min-cut\") in a [[multi-commodity flow problem]]. The theorems have enabled the development of [[approximation algorithm]]s for use in [[graph partition]] and related problems.\n\n==Multicommodity flow problem==\nA \"commodity\" in a network flow problem is a pair of source and sink [[node (graph theory)|node]]s. In a multi-commodity flow problem, there are {{math|k≥1}} commodities, each with its own source <math>s_{i}</math>, sink <math>t_{i}</math>, and demand <math>D_{i}</math>. The objective is to simultaneously route <math>D_{\\text{i}}</math> units of commodity {{mvar|i}} from <math>s_{i}</math> to <math>t_{i}</math> for each {{mvar|i}}, such that the total amount of all commodities passing through any edge is no greater than its capacity. (In the case of undirected edges, the sum of the flows in both directions cannot exceed the capacity of the edge).<ref name=\"Leighton99\">{{cite journal |author-last1=Leighton |author-first1=Tom |author-last2=Rao |author-first2=Satish |title=Multicommodity Max-Flow Min-Cut Theorems and Their Use in Designing Approximation Algorithms |journal=[[Journal of the ACM]] |date=November 1999 |volume=46 |issue=6 |pages=787–832 |doi=10.1145/331524.331526|citeseerx=10.1.1.640.2995 }}</ref>\nSpecially, a 1-commodity (or single commodity) flow problem is also known as a [[maximum flow problem]]. According to the [[Ford–Fulkerson algorithm]], the max-flow and min-cut are always equal in a 1-commodity flow problem.\n\n===Max-flow and min-cut===\nIn a multicommodity flow problem, ''max-flow'' is the maximum value of {{mvar|f}}, where {{mvar|f}} is the common fraction of each commodity that is routed, such that <math>fD_{\\text{i}}</math> units of commodity {{mvar|i}} can be simultaneously routed for each {{mvar|i}} without violating any capacity constraints.\n''min-cut'' is the minimum of all cuts of the ratio <math>\\varphi</math> of the capacity of the cut to the demand of the cut.\nMax-flow is always upper bounded by the min-cut for a multicommodity flow problem.\n\n===Uniform multicommodity flow problem===\nIn a uniform multicommodity flow problem, there is a commodity for every pair of nodes and the demand for every commodity is the same. (Without loss of generality, the demand for every commodity is set to one.) The underlying network and capacities are arbitrary.<ref name=\"Leighton99\"/>\n\n===Product multicommodity flow problem===\nIn a product multicommodity flow problem, there is a nonnegative weight for each node <math>v \\in V</math> in graph <math>G=(V,E)</math>. The demand for the commodity between nodes {{mvar|u}} and {{mvar|v}} is the product of the weights of node {{mvar|u}} and node {{mvar|v}}. The uniform multicommodity flow problem is a special case of the product multicommodity flow problem for which the weight is set to 1 for all nodes <math>u \\in V</math>.<ref name=\"Leighton99\"/>\n\n===Duality of linear programming===\n{{see also|Linear programming}}\nIn general, the dual of a multicommodity flow problem for a graph {{mvar|G}} is the problem of apportioning a fixed amount of weight (where weights can be considered as distances) to the edges of {{mvar|G}} such that to maximize the cumulative distance between the source and sink pairs.<ref name=\"Leighton99\"/>\n\n===History===\nThe research on the relationship between the max-flow and min-cut of multicommodity flow problem has obtained great interest since Ford and Fulkerson's result for 1-commodity flow problems. Hu<ref name=\"hu63\">{{cite journal |author-last1=Hu |author-first1=T. C. |title=Multicommodity network flows |journal=[[Operations Research (journal)|Operations Research]] |date=1963 |volume=11 |issue=3 |pages=344–360|doi=10.1287/opre.11.3.344 }}</ref>\nshowed that the max-flow and min-cut are always equal for two commodities. Okamura and Seymour<ref name=\"Okamura and Seymour 81\">{{cite journal |author-last1=Okamura |author-first1=H. |author-last2=Seymour |author-first2=P. D. |title=Multicommodity flows in planar graphs |journal=[[Journal of Combinatorial Theory, Series B]] |date=1981 |volume=31 |issue= |pages=75–81|doi=10.1016/S0095-8956(81)80012-3 }}</ref> illustrated a 4-commodity flow problem with max-flow equals to 3/4 and min-cut equals 1. Shahrokhi and Matula<ref name=\"Shahrokhi and Matula 90\">{{cite journal |author-last1=Shahrokri |author-first1=F. |author-last2=Matula |author-first2=David W. |title=The maximum concurrent flow problem |journal=[[Journal of the ACM]] |date=1990 |volume=37 |issue=2 |pages=318–334 |doi=10.1145/77600.77620}}</ref> also proved that the max-flow and min-cut are equal provided the dual of the flow problem satisfies a certain cut condition in a uniform multicommodity flow problem. Many other researchers also showed concrete research results in similar problems<ref name=\"Klein97\">{{cite journal |author-last1=Klein |author-first1=P. |author-last2=Plotkin |author-first2=S. |author-last3=Rao |author-first3=S. |author-last4=Tardos |author-first4=E. |title=Bounds on the max-flow min-cut ratio for directed multicommodity flows |journal=[[J. Algorithms]] |date=1997 |volume=22 |pages=241–269}}</ref><ref name=\"Garg96\">{{cite journal |author-last1=Garg |author-first1=N. |author-last2=Vazarani |author-first2=V. |author-last3=Yannakakis |author-first3=M. |title=Approximate max-flow min-(multi)cut theorems and their applications |journal=[[SIAM Journal on Computing]] |date=1996 |volume=25 |issue=2 |pages=235–251 |doi=10.1137/s0097539793243016}}</ref><ref name=\"Plotkin and Tardos 93\">{{cite journal |author-last1=Plitkin |author-first1=S. |author-last2=Tardos |author-first2=E. |title=Improved bounds on the max-flow min-cut ratio for multicommodity flows |journal=Proceedings of the 25th Annual ACM Symposium on Theory of Computing |date=1993 |pages=691–697}}</ref>\n\nFor a general network flow problem, the max-flow is within a factor of {{mvar|k}} of the min-cut since each commodity can be optimized separately using <math>1/k</math> of the capacity of each edge. This is not a good result especially in case of large numbers of commodities.<ref name=\"Leighton99\"/>\n\n==Approximate max-flow min-cut theorems==\n\n===Theorems of uniform multicommodity flow problems===\nThere are two theorems first introduced by Tom Leighton and Satish Rao in 1988<ref name=\"Leighton88\">{{cite journal |author-last1=Leighton |author-first1=Tom |author-last2=Rao |author-first2=Satish |title=An approximate max-flow min-cut theorem for uniform multicommodity flow problems with applications to approximation algorithms |journal=Proceedings of the 29th IEEE Symposium on Foundations of Computer Science |date=1988 |pages=422–431}}</ref>\nand then extended in 1999.<ref name=\"Leighton99\"/> Theorem 2 gives a tighter bound compared to Theorem 1.\n\n'''Theorem 1.''' ''For any {{mvar|n}}, there is an {{mvar|n}}-node uniform multicommodity flow problem with max-flow {{mvar|f}} and min-cut <math>\\varphi</math> for which <math>f\\le O\\left(\\frac{\\varphi}{\\log n}\\right)</math>.''<ref name=\"Leighton99\"/>\n\n'''Theorem 2.''' ''For any uniform multicommodity flow problem, <math>\\Omega\\left(\\frac{\\varphi}{\\log n}\\right)\\le f\\le\\varphi</math>, where {{mvar|f}} is the max-flow and <math>\\varphi</math> is the min-cut of the uniform multicommodity flow problem.''<ref name=\"Leighton99\"/>\n\nTo prove Theorem 2, both the max-flow and the min-cut should be discussed. \nFor the max-flow, the techniques from duality theory of linear programming have to be employed. According to the duality theory of linear programming, an optimal distance function results in a total weight that is equal to the max-flow of the uniform multicommodity flow problem. \nFor the min-cut, a 3-stage process has to be followed:<ref name=\"Leighton99\"/><ref name=\"Garg96\"/>\n\nStage 1: Consider the dual of uniform commodity flow problem and use the optimal solution to define a graph with distance labels on the edges.\n\nStage 2: Starting from a source or a sink, grow a region in the graph until find a cut of small enough capacity separating the root from its mate.\n\nStage 3: Remove the region and repeat the process of stage 2 until all nodes get processed.\n\n===Generalized to product multicommodity flow problem===\n\n'''Theorem 3.''' ''For any product multicommodity flow problem with {{mvar|k}} commodities, <math>\\Omega\\left(\\frac{\\varphi}{\\log k}\\right)\\le f\\le \\varphi</math>, where {{mvar|f}} is the max-flow and <math>\\varphi</math> is the min-cut of the product multicommodity flow problem.''<ref name=\"Leighton99\"/>\n\nThe proof methodology is similar to that for Theorem 2; the major difference is to take node weights into consideration.\n\n===Extended to directed multicommodity flow problem===\n\nIn a directed multicommodity flow problem, each edge has a direction, and the flow is restricted to move in the specified direction. In a directed uniform multicommodity flow problem, the demand is set to 1 for every directed edge.\n\n'''Theorem 4.''' ''For any directed uniform multicommodity flow problem with {{mvar|n}} nodes, <math>\\Omega\\left(\\frac{\\varphi}{\\log n}\\right)\\le f\\le \\varphi</math>, where {{mvar|f}} is the max-flow and <math>\\varphi</math> is the min-cut of the uniform multicommodity flow problem.''<ref name=\"Leighton99\"/>\n\nThe major difference in the proof methodology compared to Theorem 2 is that, now the edge directions need to be considered when defining distance labels in stage 1 and for growing the regions in stage 2, more details can be found in.<ref name=\"Leighton99\"/>\n\nSimilarly, for product multicommodity flow problem, we have the following extended theorem:\n\n'''Theorem 5.''' ''For any directed product multicommodity flow problem with {{mvar|k}} commodities, <math>\\Omega\\left(\\frac{\\varphi}{\\log k}\\right)\\le f\\le\\varphi</math>, where {{mvar|f}} is the max-flow and <math>\\varphi</math> is the directed min-cut of the product multicommodity flow problem.''<ref name=\"Leighton99\"/>\n\n==Applications to approximation algorithms==\nThe above theorems are very useful to design [[approximation algorithm]]s for [[NP-hard]] problems, such as the [[graph partition]] problem and its variations. Here below we briefly introduce a few examples, and the in-depth elaborations can be found in Leighton and Rao (1999).<ref name=\"Leighton99\"/>\n\n===Sparsest cuts===\nA sparsest cut of a graph <math>G=(V,E)</math> is a partition for which the ratio of the number of edges connecting the two partitioned components over the product of the numbers of nodes of both components. This is a NP-hard problem, and it can be approximated to within <math>O(\\log n)</math> factor using Theorem 2. Also, a sparsest cut problem with weighted edges, weighted nodes or directed edges can be approximated within an <math>O(\\log p)</math> factor where {{mvar|p}} is the number of nodes with nonzero weight according to Theorem 3, 4 and 5.\n\n===Balanced cuts and separators===\nIn some applications, we want to find a small cut in a graph <math>G=(V,E)</math> that partitions the graph into nearly equal-size pieces. We usually call a cut ''b-balanced'' or a {{math|(''b'',1&nbsp;&minus;&nbsp;''b'')}}-''separator'' (for {{math|''b''&nbsp;≤&nbsp;1/2}}) if <math>b\\pi(V)\\le\\pi(U)\\le(1-b)\\pi(V)</math> where <math>\\pi(U)</math> is the sum of the node weights in {{mvar|U}}. This is also an [[NP-hard]] problem. An approximation algorithm has been designed for this problem,<ref name=\"Leighton99\"/> and the core idea is that {{mvar|G}} has a {{mvar|b}}-balanced cut of size {{mvar|S}}, then we find a {{math|''b''&prime;}}-balanced cut of size <math>O\\left(S\\log \\frac n b -b'\\right)</math> for any {{mvar|b'}} where {{math|''b''&prime;&nbsp;<&nbsp;''b''}} and {{math|''b''&prime;&nbsp;≤&nbsp;1/3}}. Then we repeat the process then finally obtain the result that total weight of the edges in the cut is at most <math>O\\left(\\frac{S\\log n}{b-b'}\\right)</math>.\n\n===VLSI layout problems===\nIt is helpful to find a layout of minimum size when designing a VLSI circuit. Such a problem can often be modeled as a graph embedding problem. The objective is to find an embedding for which the layout area is minimized. Finding the minimum layout area is also NP-hard. An approximation algorithm has been introduced<ref name=\"Leighton99\"/> and the result is <math>O(\\log^6 n)</math> times optimal.\n\n===Forwarding index problem===\nGiven an {{mvar|n}}-node graph {{mvar|G}} and an embedding of <math>K_n</math> in {{mvar|G}}, Chung et al.<ref name=\"CHUNG87\">{{cite journal |author-last1=Chung |author-first1=F. K. |author-last2=Coffman |author-first2=E. G. |author-last3=Reiman |author-first3=M. I. |author-last4=Simon |author-first4=B. E. |title=The forwarding index of communication networks |journal=[[IEEE Transactions on Information Theory]] |date=1987 |volume=33 |issue=2 |pages=224–232 |doi=10.1109/tit.1987.1057290}}</ref>\ndefined the ''forwarding index'' of the embedding to be the maximum number of paths (each corresponding to an edge of <math>K_n</math>) that pass through any node of {{mvar|G}}. The objective is to find an embedding that minimizes the forwarding index. Using embedding approaches<ref name=\"Leighton99\"/> it is possible to bound the node and edge-forwarding indices to within an <math>O(\\log n)</math>-factor for every graph {{mvar|G}}.\n\n===Planar edge deletion===\nTragoudas<ref name=\"TRAGOUDAS90\">{{cite book |author-last1=Tragoudas |author-first1=S. |title=VLSI partitioning approximation algorithms based on multicommodity flows and other techniques |type=Ph.D. dissertation |publisher=Department of Computer Sciences, University of Texas |date=1990}}</ref>\nuses the approximation algorithm for balanced separators to find a set of \n<math>O\\left((R\\log n + \\sqrt{nR})\\log\\frac{n}{R}\\right)</math>\nedges whose removal from a bounded-degree graph {{mvar|G}} results in a planar graph, where {{mvar|R}} is the minimum number of edges that need to be removed from {{mvar|G}} before it becomes planar. It remains an open question if there is a [[Polylogarithmic function|polylog]] {{mvar|n}} times optimal approximation algorithm for {{mvar|R}}.<ref name=\"Leighton99\"/>\n\n==References==\n{{reflist}}\n\n[[Category:Network flow problem]]\n[[Category:Mathematical theorems]]"
    },
    {
      "title": "Argus – Audit Record Generation and Utilization System",
      "url": "https://en.wikipedia.org/wiki/Argus_%E2%80%93_Audit_Record_Generation_and_Utilization_System",
      "text": "'''Argus – the Audit Record Generation and Utilization System''' is the first implementation of network flow monitoring, and is an ongoing open source network flow monitor project.  Started by Carter Bullard in 1984 at Georgia Tech, and developed for cyber security at Carnegie Mellon University in the early 1990s, Argus has been an important contributor to Internet [[cyber security]] technology over its 30 years.<ref>http://www.qosient.com/argus/publications.shtml</ref>   [http://resources.sei.cmu.edu/asset_files/Presentation/2014_017_001_90132.pdf].\n\n[[File:Network Flow Monitor Timeline.png|thumb|left|Network Flow Monitoring Timeline]]\n\nThe Argus Project is focused on developing all aspects of large scale network [[situational awareness]] and network [[audit trail]] establishment in support of Network Operations ([[NetOps]]), Performance and Security Management.  Motivated by the telco [[Call detail record]] (CDR), Argus attempts to generate network [[metadata]] that can be used to perform a large number of [[network management]] tasks.   Argus is used by many universities, corporations and government entities including US [[Defense Information Systems Agency|DISA]], DoD, [[United States Department of Homeland Security|DHS]], [[FFRDC]]s, [[GLORIAD]] and is a Top 100 Internet Security Tool.<ref>http://sectools.org</ref>   Argus is designed to be a [[Real-time computing|real-time]] situational awareness system, and its data can be used to track, alarm and alert on wire-line network conditions.  The data can also be used to establish a comprehensive audit of all network traffic, as described in the [[Rainbow Series|Red Book]], US DoD NCSC-TG-005,<ref>http://csrc.nist.gov/publications/secpubs/rainbow/tg005.txt</ref> supplementing traditional [[Intrusion detection system]] (IDS) based [[network security]].<ref>R. Bejtlich, The Tao of Network Security Monitoring: Beyond Intrusion Detection , New York:Addison-Wesley, 2004.</ref>  The audit trail is traditionally used as historical [[network traffic measurement]] data for [[network forensics]]<ref>{{cite journal | last1 = Pilli | first1 = Emmanuel S. | last2 = Joshi | first2 = R. C. | last3 = Niyogi | first3 = Rajdeep | year = 2010 | title = Network forensic frameworks: Survey and research challenges | url = | journal = Digit. Investig | volume = 7 | issue = 1–2| pages = 14–27 | doi = 10.1016/j.diin.2010.02.00 }}</ref> and [[Network Behavior Anomaly Detection]] (NBAD).<ref>G. Nychis, V. Sekar, D Andersen, H Kim, H Zhang, An empirical evaluation of entropy-based traffic anomaly detection, Proceedings of the 8th ACM SIGCOMM conference on Internet measurement, pp 151–156, October 20–22, 2008, Vouliagmeni, Greece</ref>  Argus has been used extensively in [[cybersecurity]], [[end-to-end principle|end-to-end]] performance analysis, and more recently, [[software-defined networking]] (SDN) research.<ref>J. Naous, D. Ericson, A. Covington, G Appenzeller, N. McKeown, Implementing an OpenFlow switch on the NetFPGA platform, Symposium On Architecture For Networking And Communications Systems, pp. 1–9, 2008, San Jose, CA</ref>  Argus has also been a topic in [[network management]] standards development. [[RMON]] (1995) <ref>ftp://ietf.org/ietf/rmonmib/rmonmib-minutes-94dec.txt</ref> and [[IPFIX]] (2001).<ref>http://www.ietf.org/proceedings/51/slides/ipfx-2/sld001.htm</ref>\n\nArgus is composed of an advanced comprehensive network flow data generator, the Argus monitor, which processes packets (either capture files or live packet data) and generates detailed network [[Traffic flow (computer networking)|traffic flow]] status reports of all the flows in the packet stream.  Argus monitors all [[network packet|network traffic]], [[data plane]], [[control plane]] and management plane, not just [[Internet Protocol]] (IP) traffic.  Argus captures much of the packet dynamics and semantics of each flow, with a great deal of data reduction, so you can store, process, inspect and analyze large amounts of network data efficiently.  Argus provides [[reachability]], [[availability]], [[connectivity (graph theory)|connectivity]], duration, rate, load, good-put, [[network congestion|loss]], [[jitter]], [[retransmission (data networks)]], and delay metrics for all network flows, and captures most attributes that are available from the packet contents, such as Layer 2 addresses, tunnel identifiers ([[MPLS]], GRE, [[IPsec]], etc...), protocol ids, SAP's, hop-count, options, L4 transport identification ([[Real-time Transport Protocol|RTP]] detection), host flow control indications, etc...  Argus has implemented a number of packet dynamics metrics specifically designed for cyber security.  Argus detects human typing behavior in any flow, but of particular interest is key-stroke detection in encrypted SSH [[tunneling protocol|tunnels]].<ref>Saptarshi Guha, Paul Kidwell, Asgrith Barthur, William S Cleveland, John Gerth, and Carter Bullard. 2011. SSH Keystroke Packet Detection, ICS-2011—Monterey, California, Jan 9–11.</ref>  and Argus generates the Producer Consumer Ratio (PCR) which indicates whether a network entity is a data producer and/or consumer,<ref>http://www.qosient.com/argus/presentations/Argus.FloCon.2014.PCR.Presentation.pdf</ref> an important property when evaluating the potential for a node to be involved in an [[Advanced persistent threat]] (APT) mediated exfiltration.\n\nArgus is an Open Source ([[GNU General Public License|GPL]]) project, owned and managed by QoSient, LLC, and has been ported to most operating systems and many hardware accelerated platforms, such as Bivio, Pluribus, Arista, and Tilera. The software should be portable to many other environments with little or no modifications. Performance is such that auditing an entire enterprise's Internet activity can be accomplished using modest computing resources.\n\n== Supported platforms ==\n* [[Linux]]: Unix operating system running the [[Linux kernel]]\n* [[Solaris (operating system)|Solaris]]: Unix operating system developed by [[Sun Microsystems]]\n* [[BSD]]: Unix operating system family ([[FreeBSD]], [[NetBSD]], [[OpenBSD]])\n* [[OS X]]: Unix operating system developed by [[Apple Inc.]]\n* [[IRIX]]: Unix operating system developed by [[Silicon Graphics]]\n* [[AIX]], Unix operating system developed by [[IBM]]\n* [[Windows]], (under [[Cygwin]]) operating system developed by [[Microsoft]]\n* [[OpenWrt]]: Unix operation system running the Linux kernel on embedded devices\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://www.qosient.com/argus/ Argus website]\n\n{{DEFAULTSORT:Argus - Audit Record Generation and Utilization System}}\n[[Category:Network flow problem]]\n[[Category:Network analyzers]]\n[[Category:Network performance]]\n[[Category:Performance management]]\n[[Category:Network management]]\n[[Category:Packets (information technology)]]\n[[Category:Internet Protocol based network software]]"
    },
    {
      "title": "Braess's paradox",
      "url": "https://en.wikipedia.org/wiki/Braess%27s_paradox",
      "text": "{{short description|proposed explanation for how trying to improve traffic flow actually has the reverse effect}}\n{{Use dmy dates|date=September 2016}}\n{{Use British English|date=September 2016}}\n\n'''Braess's paradox''' is the observation that adding one or more roads to a road network can end up impeding overall [[road traffic|traffic]] flow through it. The paradox was postulated in 1968 by German mathematician [[Dietrich Braess]], who noticed that adding a road to a particular [[traffic congestion|congested]] road traffic network would increase overall journey time.\n\nThe paradox may have analogies in [[electrical power grid]]s and biological systems. It has been suggested that in theory, the improvement of a malfunctioning network could be accomplished by removing certain parts of it. The paradox has been used to explain instances of improved [[traffic flow]] when existing major roads are closed.\n\n== Discovery and definition ==\nDietrich Braess, a mathematician at [[Ruhr University]], [[Germany]], noticed the flow in a road network could be impeded by adding a new road, when he was working on [[traffic modelling]]. His idea was that if each driver is making the [[optimization|optimal]] self-interested decision as to which route is quickest, a shortcut could be chosen too often for drivers to have the shortest travel times possible. More formally, the idea behind Braess's discovery is that the [[Nash equilibrium]] may not equate with the best overall flow through a network.<ref name=\"ReferenceA\">New Scientist, [https://www.newscientist.com/article/mg22129520-600-42nd-st-paradox-cull-the-best-to-make-things-better/  42nd St Paradox: Cull the best to make things better], 16 January 2014 by Justin Mullins</ref>\n\nThe paradox is stated as follows:<blockquote>\"For each point of a road network, let there be given the number of cars starting from it and the destination of the cars. Under these conditions, one wishes to estimate the distribution of traffic flow. Whether one street is preferable to another depends not only on the quality of the road, but also on the [[flow density|density of the flow]]. If every driver takes the path that looks most favourable to them, the resultant running times need not be minimal. Furthermore, it is indicated by an example that an extension of the road network may cause a redistribution of the traffic that results in longer individual running times.\"</blockquote>\n\nAdding extra capacity to a [[network (mathematics)|network]] when the moving entities selfishly choose their route can in some cases reduce overall performance. That is because the [[Nash equilibrium]] of such a system is not necessarily optimal. The network change induces a new game structure which leads to a (multiplayer) [[prisoner's dilemma]]. In a Nash equilibrium, drivers have no incentive to change their routes. While the system is not in a Nash equilibrium, individual drivers are able to improve their respective travel times by changing the routes they take. In the case of Braess's paradox, drivers will continue to switch until they reach Nash equilibrium despite the reduction in overall performance.\n\nIf the latency functions are linear, adding an edge can never make total travel time at equilibrium worse by a factor of more than 4/3.<ref name=\"RoughgardenTardos\">\n{{cite web|url=http://theory.stanford.edu/~tim/papers/routing.pdf|title=How Bad is Selfish Routing?|last2=Tardos|first2=Éva|publisher=Journal of the ACM|last1=Roughgarden|first1=Tim|accessdate=2016-07-18|archiveurl=https://web.archive.org/web/20160409061229/http://theory.stanford.edu/~tim/papers/routing.pdf|archivedate=2016-04-09|deadurl=no}}</ref>\n\n== Possible instances of the paradox in action ==\n\n=== Prevalence ===\nIn 1983, Steinberg and Zangwill provided, under reasonable assumptions, the necessary and sufficient conditions for Braess's paradox to occur in a general transportation network when a new route is added. (Note that their result applies to the addition of ''any'' new route, not just to the case of adding a single link.) As a corollary, they obtain that Braess's paradox is about as likely to occur as not occur; their result applies to random rather than planned networks and additions.<ref>{{Cite journal | doi = 10.1287/trsc.17.3.301| title = The Prevalence of Braess' Paradox| journal = Transportation Science| volume = 17| issue = 3| pages = 301| year = 1983| last1 = Steinberg | first1 = R. | last2 = Zangwill | first2 = W. I. }}</ref>\n\n=== Traffic ===\n\nBraess's paradox has a counterpart in case of a reduction of the road network (which may cause a reduction of individual commuting time).<ref name=Razemon/>\n\nIn [[Seoul]], [[South Korea]], a speeding up of traffic around the city was seen when a motorway was removed as part of the [[Cheonggyecheon]] restoration project.<ref>{{cite book | last1 = Easley | first1 = D. | last2 = Kleinberg | first2 = J. | title = Networks | page = 71 | publisher = Cornell Store Press | date = 2008 }}</ref> In [[Stuttgart]], [[Germany]], after investments into the road network in 1969, the traffic situation did not improve until a section of newly built road was closed for traffic again.<ref name=\"Knödel1969\">{{cite book|last=Knödel|first=W.|title=Graphentheoretische Methoden Und Ihre Anwendungen|url=https://books.google.com/books?id=bJ22pwAACAAJ|date=31 January 1969|publisher=[[Springer-Verlag]]|isbn=978-3-540-04668-4|pages=57–59}}</ref> In 1990 the temporary closing of 42nd Street in [[New York City]] for [[Earth Day]] reduced the amount of congestion in the area.<ref>{{cite news | last = Kolata | first = Gina |authorlink=Gina Kolata | date=1990-12-25 | publisher=New York Times | url=https://www.nytimes.com/1990/12/25/health/what-if-they-closed-42d-street-and-nobody-noticed.html |title=What if They Closed 42d Street and Nobody Noticed? |accessdate=2008-11-16}}</ref> In 2008 Youn, Gastner and Jeong demonstrated specific routes in Boston, New York City and London where that might actually occur and pointed out roads that could be closed to reduce predicted travel times.<ref name=\"YounGastner2008\">{{cite journal | last1 = Youn | first1 = Hyejin | last2 = Gastner | first2 = Michael | last3 = Jeong | first3 = Hawoong | title = Price of Anarchy in Transportation Networks: Efficiency and Optimality Control| journal = [[Physical Review Letters]] | volume = 101 | issue = 12 | year = 2008 | issn = 0031-9007 | pmid = 18851419 | doi = 10.1103/PhysRevLett.101.128701 | arxiv = 0712.1598 | bibcode = 2008PhRvL.101l8701Y | pages=128701 }}</ref>  In 2009, New York experimented with closures of [[Broadway (Manhattan)|Broadway]] at [[Times Square]] and [[Herald Square]], which resulted in improved traffic flow and permanent pedestrian plazas.<ref>{{cite episode |title=Braess's Paradox |first=Andrew |last=Boyd |series=Engines of Our Ingenuity |number=2814 |url=http://www.uh.edu/engines/epi2814.htm}}</ref>\n\nIn 2012, Paul Lecroart, of the institute of planning and development of the [[Île-de-France]], wrote that \"Despite initial fears, the removal of main roads does not cause deterioration of traffic conditions beyond the starting adjustments. The traffic transfer are limited and below expectations\".<ref name=Razemon/> He also notes that some motorized travels are not transferred on public transport and simply disappear (\"evaporate\").<ref name=Razemon/>\n\nThe same phenomenon was also observed when road closing was not part of an urban project but the consequence of an accident. In 2012 in [[Rouen]], a bridge was burned by an accident; during the two following years, other bridges were more used, but the total number of cars crossing bridges was reduced.<ref name=Razemon>{{fr}} Olivier Razemon, \"Le paradoxde de l'« évaporation » du trafic automobile\", ''[[Le monde]]'', Thursday 25 August 2016, page 5. Published on-line as [http://www.lemonde.fr/smart-cities/article/2016/08/24/et-si-le-trafic-s-evaporait_4987353_4811534.html \"Et si le trafic s’évaporait ?\"] on 24 August 2016 and updated on 25 August 2016 (page visited on 19 September 2016).</ref> Similarly, in 2015 in [[Warsaw]], a bridge was closed; authorities observed an increased use of other roads and public transport, but half of the vehicles usually crossing the bridge \"disappeared\" (52,000 out of 105,000 daily).<ref name=Razemon/>\n\n=== Electricity ===\nIn 2012, scientists at the [[Max Planck Institute for Dynamics and Self-Organization]] demonstrated, through [[computational modeling]], the potential for the phenomenon to occur in [[Electrical grid|power transmission networks]] where [[Electricity generation|power generation]] is decentralized.<ref name=rdmag_mpi>{{Citation\n |author=Staff (Max Planck Institute) |date=September 14, 2012 |title=Study: Solar and wind energy may stabilize the power grid |magazine=[[R&D Magazine]] |at=rdmag.com |url=http://www.rdmag.com/News/2012/09/Energy-Engineering-Study-Solar-and-wind-energy-may-stabilize-the-power-grid/ |accessdate=September 14, 2012\n }}</ref>\n\nIn 2012, an international team of researchers from Institut Néel (CNRS, France), INP (France), IEMN (CNRS, France) and UCL (Belgium) published in Physical Review Letters<ref name=\"PalaBaltazar2012\">{{cite journal | last1 = Pala | first1 = M. G. | last2 = Baltazar | first2 = S. | last3 = Liu | first3 = P. | last4 = Sellier | first4 = H. | last5 = Hackens | first5 = B. | last6 = Martins | first6 = F. | last7 = Bayot | first7 = V. | last8 = Wallart | first8 = X. | last9 = Desplanque | first9 = L. | last10 = Huant | first10 = S. | title = Transport Inefficiency in Branched-Out Mesoscopic Networks: An Analog of the Braess Paradox | journal = [[Physical Review Letters]] | volume = 108 | issue = 7 | pages = 076802 | year = 2012 | issn = 0031-9007 | doi = 10.1103/PhysRevLett.108.076802 | arxiv = 1112.1170 | origyear = 6 Dec 2011 (v1) | bibcode=2012PhRvL.108g6802P | pmid=22401236}}</ref> a paper showing that Braess's paradox may occur in [[Mesoscopic physics|mesoscopic]] electron systems. In particular, they showed that adding a path for electrons in a nanoscopic network paradoxically reduced its conductance. That was shown both by simulations as well as experiments at low temperature using as [[scanning gate microscopy]].\n\n=== Biology ===\n[[Adilson E. Motter]] and collaborators demonstrated that Braess's paradox outcomes may often occur in biological and ecological systems.<ref>{{Cite journal | doi=10.1002/bies.200900128| pmid=20127700| pmc=2841822|title = Improved network performance via antagonism: From synthetic rescues to multi-drug combinations| journal=BioEssays| volume=32| issue=3| pages=236–245|year = 2010|last1 = Motter|first1 = Adilson E.}}</ref> Motter suggests removing part of a perturbed network could rescue it. For resource management of endangered species [[food webs]], in which extinction of many species might follow sequentially, selective removal of a doomed species from the network could in principle bring about the positive outcome of preventing a series of further extinctions.<ref>Sahasrabudhe S., Motter A. E., [http://www.nature.com/articles/ncomms1163 Rescuing ecosystems from extinction cascades through compensatory perturbations], Nature Communications 2, 170 (2011)</ref>\n\n=== Team sports strategy ===\nIt has been suggested that in basketball, a team can be seen as a network of possibilities for a route to scoring a basket, with a different efficiency for each pathway, and a star player could reduce the overall efficiency of the team, analogous to a shortcut that is overused increasing the overall times for a journey through a road network. A proposed solution for maximum efficiency in scoring is for a star player to shoot about the same number of shots as teammates. However, this approach is not supported by hard statistical evidence, as noted in the original paper.<ref>{{Cite journal |bibcode = 2009arXiv0908.1801S |title = The price of anarchy in basketball |journal = Journal of Quantitative Analysis in Sports|volume = 6 |issue = 1 |last1 = Skinner|first1 = Brian|last2 = Gastner|first2 = Michael T|last3 = Jeong|first3 = Hawoong|year = 2009|arxiv = 0908.1801|doi = 10.2202/1559-0410.1217|citeseerx = 10.1.1.215.1658}}</ref>\n\nIn soccer [[Helenio Herrera]] is well known for his famous quote \"with 10 [players] our team plays better than with 11\".\n\n== Mathematical approach ==\n\n=== Example ===\n[[Image:Braess paradox road example.svg|right|500px]]\n\nConsider a road network as shown in the adjacent diagram on which 4000 drivers wish to travel from point Start to End. The travel time in minutes on the Start-A road is the number of travelers (T) divided by 100, and on Start-B is a constant 45 minutes (likewise with the roads across from them). If the dashed road does not exist (so the traffic network has 4 roads in total), the time needed to drive Start-A-End route with <math>a</math> drivers would be <math>\\tfrac{a}{100} + 45</math>. The time needed to drive the Start-B-End route with <math>b</math> drivers would be <math>\\tfrac{b}{100} + 45</math>. If either route took less time, it would not be a Nash equilibrium: a rational driver would switch from the longer route to the shorter route. As there are 4000 drivers, the fact that <math>a + b = 4000</math> can be used to derive the fact that <math>a = b = 2000</math> when the system is at equilibrium. Therefore, each route takes <math>\\tfrac{2000}{100} + 45 = 65</math> minutes.\n\nNow suppose the dashed line A-B is a road with an extremely short travel time of approximately 0 minutes. Suppose that the road is opened and one driver tries Start-A-B-End. To his surprise he finds that his time is <math>\\tfrac{2000}{100} + \\tfrac{2001}{100} = 40.01</math> minutes, a saving of 25 minutes. Soon, more of the 4000 drivers are trying this new route. The time taken rises from 40.01 and keeps climbing. When the number of drivers trying the new route reaches 2500, with 1500 still in the Start-B-End route, their time will be <math>\\tfrac{2500}{100} + \\tfrac{4000}{100} = 65</math> minutes, which is no improvement over the original route. Meanwhile, those 1500 drivers have been slowed to <math> 45 + \\tfrac{4000}{100} = 85</math> minutes, a 20-minute increase. They are obliged to switch to the new route via A too, so it now takes <math>\\tfrac{4000}{100} + \\tfrac{4000}{100} = 80</math> minutes. Nobody has any incentive to travel A-End or Start-B because any driver trying them will take 85 minutes. Thus, the opening of the cross route triggers an irreversible change to it by everyone, costing everyone 80 minutes instead of the original 65. If every driver were to agree not to use the A-B path, or if that route were closed, every driver would benefit by a 15-minute reduction in travel time.\n\n=== Existence of an equilibrium ===\nIf one assumes the travel time for each person driving on an edge to be equal, an equilibrium will always exist.\n\nLet <math>L_e(x)</math> be the formula for the travel time of each person traveling along edge <math>e</math> when <math>x</math> people take that edge. Suppose there is a traffic graph with <math>x_e</math> people driving along edge <math>e</math>. Let the energy of e, <math>E(e)</math>, be\n\n: <math>\\sum_{i=1}^{x_e} L_e(i) = L_e(1) + L_e(2) + \\cdots + L_e(x_e)</math>\n\n(If <math>x_e = 0</math> let <math>E(e) = 0</math>). Let the total energy of the traffic graph be the sum of the energies of every edge in the graph.\n\nTake a choice of routes that minimizes the total energy. Such a choice must exist because there are finitely many choices of routes. That will be an equilibrium.\n\nAssume, for contradiction, this is not the case. Then, there is at least one driver who can switch the route and improve the travel time. Suppose the original route is <math>e_0, e_1, \\ldots, e_n</math> while the new route is <math>e'_0, e'_1, \\ldots, e'_m</math>. Let <math>E</math> be total energy of the traffic graph, and consider what happens when the route <math>e_0, e_1, ... e_n</math> is removed. The energy of each edge <math>e_i</math> will be reduced by <math>L_{e_i}(x_{e_i})</math> and so the <math>E</math> will be reduced by <math>\\sum_{i=0}^n L_{e_i}(x_{e_i})</math>. That is simply the total travel time needed to take the original route. If the new route is then added, <math>e'_0, e'_1, \\ldots, e'_m</math>, the total energy <math>E</math> will be increased by the total travel time needed to take the new route. Because the new route is shorter than the original route, <math>E</math> must decrease relative to the original configuration, contradicting the assumption that the original set of routes minimized the total energy.\n\nTherefore, the choice of routes minimizing total energy is an equilibrium.\n\n=== Finding an equilibrium ===\nThe above proof outlines a procedure known as [[best response]] dynamics, which finds an equilibrium for a linear traffic graph and terminates in a finite number of steps. The algorithm is termed \"best response\" because at each step of the algorithm, if the graph is not at equilibrium then some driver has a best response to the strategies of all other drivers and switches to that response.\n\nPseudocode for Best Response Dynamics:\n  Let ''P'' be some traffic pattern.\n  '''while''' ''P'' is not at equilibrium:\n    compute the potential energy ''e'' of ''P''\n    '''for each''' driver ''d'' in ''P'':\n      '''for each''' alternate path ''p'' available to ''d'':\n         compute the potential energy ''n'' of the pattern when ''d'' takes path ''p''\n         '''if''' ''n'' < ''e'':\n           modify ''P'' so that ''d'' takes path ''p''\n  '''continue''' the topmost '''while'''\n\nAt each step, if some particular driver could do better by taking an alternate path (a \"best response\"), doing so strictly decreases the energy of the graph. If no driver has a best response, the graph is at equilibrium. Since the energy of the graph strictly decreases with each step, the best response dynamics algorithm must eventually halt.\n\n=== How far from optimal is traffic at equilibrium? ===\nIf the travel time functions are linear, that is <math>L_e(x) = a_e x + b_e</math> for some <math>a_e, b_e \\geq 0</math>, then at worst, traffic in the energy-minimizing equilibrium is twice as bad as socially optimal.<ref name=\"EasleyKleinberg\"><!-- please do not point the URL to http://www.cs.cornell.edu/home/kleinber/networks-book/networks-book-ch08.pdf, as it does not contain the reference cited by Easley and Kleinberg -->\n{{cite web|url=http://www.cs.cornell.edu/home/kleinber/networks-book/networks-book.pdf|title=Networks, Crowds, and Markets: Reasoning about a Highly Connected World (8.3 Advanced Material: The Social Cost of Traffic at Equilibrium)|last2=Kleinberg|first2=Jon|publisher=Jon Kleinberg|last1=Easley|first1=David|website=Jon Kleinberg's Homepage|accessdate=2015-05-30|archiveurl=https://web.archive.org/web/20150316015111/http://www.cs.cornell.edu/home/kleinber/networks-book/networks-book.pdf|archivedate=2015-03-16|deadurl=no}} – This is the preprint of {{ISBN|9780521195331}}</ref>\n\nProof:\nLet ''Z'' be some traffic configuration, with associated energy ''E''(''Z'') and total travel time ''T''(''Z''). For each edge, the energy is the sum of an [[arithmetic progression]], and using the formula for the sum of an arithmetic progression, one can show that ''E''(''Z'') ≤ ''T''(''Z'') ≤ 2''E''(''Z''). If <math>Z_o</math> is the socially-optimal traffic flow and <math>Z_e</math> is the energy-minimizing traffic flow, the inequality implies that <math>T(Z_e) \\leq 2E(Z_e) \\leq 2E(Z_o) \\leq 2T(Z_o)</math>.\n\nThus, the total travel time for the energy-minimizing equilibrium is at most twice as bad as for the optimal flow.\n\n=== Dynamics analysis of Braess's paradox ===\nIn 2013, Dal Forno and Merlone<ref name=\"Dal FornoMerlone2013\">{{cite journal | last1 = Dal Forno | first1 = Arianna | last2 = Merlone | first2 = Ugo | title = Border-collision bifurcations in a model of Braess paradox | journal = [[Mathematics and Computers in Simulation]] | volume = 87 | year = 2013 | pages = 1–18 | issn = 0378-4754 | doi = 10.1016/j.matcom.2012.12.001 }}</ref> interpret Braess's paradox as a dynamical ternary choice problem. The analysis shows how the new path changes the problem. Before the new path is available, the dynamics is the same as in binary choices with externalities, but the new path transforms it into a ternary choice problem. The addition of an extra resource enriches the complexity of the dynamics. In fact, there can even be coexistence of cycles, and the implication of the paradox on the dynamics can be seen from both a geometrical and an analytical perspective.\n\n== See also ==\n* [[Bélády's anomaly]]\n* [[Routing#Path selection|Route choice]]\n* [[Downs–Thomson paradox]]\n* [[Jevons paradox]]\n* [[Marchetti's constant]]\n* [[Induced demand]]\n* [[Lewis–Mogridge position]]\n* [[Hotelling's law]]\n* [[Paradox of enrichment]]: Increasing the food available to an ecosystem may introduce dynamic instability, and even lead to extinction.\n* [[Traffic wave]]\n* [[Apportionment paradox]]\n* [[John Glen Wardrop]]\n* [[Hydra effect]]\n* [[Rat running]]\n* [[Bufferbloat]]\n\n== References ==\n{{reflist|30em}}\n\n== Further reading ==\n* D. Braess, Über ein Paradoxon aus der Verkehrsplanung. ''Unternehmensforschung'' 12, 258–268 (1969) [http://homepage.ruhr-uni-bochum.de/Dietrich.Braess/paradox.pdf] [http://homepage.rub.de/Dietrich.Braess/Paradox-BNW.pdf]\n*Katharina Belaga-Werbitzky: „Das Paradoxon von Braess in erweiterten Wheatstone-Netzen mit M/M/1-Bedienern“ {{ISBN|3-89959-123-2}}\n* Translation of the Braess 1968 article from German to English appears as the article \"On a paradox of traffic planning,\" by D. Braess, A. Nagurney, and T. Wakolbinger in the journal Transportation Science, volume 39, 2005, pp.&nbsp;446–450. [http://supernet.som.umass.edu/cfoto/braess-visit/braessvisit.html More information]\n* {{Cite journal | doi = 10.1080/02698599308573460| title = How Braess' paradox solves Newcomb's problem| journal = International Studies in the Philosophy of Science| volume = 7| issue = 2| pages = 141–160| year = 1993| last1 = Irvine | first1 = A. D. }}\n* {{Cite journal | doi = 10.1287/trsc.17.3.301| title = The Prevalence of Braess' Paradox| journal = Transportation Science| volume = 17| issue = 3| pages = 301| year = 1983| last1 = Steinberg | first1 = R. | last2 = Zangwill | first2 = W. I. }}\n* A. Rapoport, T. Kugler, S. Dugar, and E. J. Gisches, Choice of routes in congested traffic networks: Experimental tests of the Braess Paradox. ''Games and Economic Behavior'' 65 (2009) [http://www.parisschoolofeconomics.eu/IMG/pdf/Choices_of_routes.pdf]\n* T. Roughgarden. \"The Price of Anarchy.\" MIT Press, Cambridge, MA, 2005.\n\n== External links ==\n{{commons category|Braess's paradox}}\n* [https://msdn.microsoft.com/magazine/ee310108 Software Testing Paradoxes (Dec. 2005)] [http://download.microsoft.com/download/3/a/7/3a7fa450-1f33-41f7-9e6d-3aa95b5a6aea/MSDNMagazineDecember2005en-us.chm Direct Link]\n* [http://homepage.ruhr-uni-bochum.de/Dietrich.Braess/#paradox Dietrich Braess' homepage]\n* [http://www.davros.org/science/roadparadox.html The Road Network Paradox]\n* [https://www.youtube.com/watch?v=sTQAu9TW4jM Short video illustrating the Braess Paradox with Lego minifigures]\n\n{{Economic paradoxes}}\n{{Portal bar|Mathematics|Roads|Transport|Economy}}\n\n[[Category:Mathematics paradoxes]]\n[[Category:Game theory]]\n[[Category:Network flow problem]]"
    },
    {
      "title": "Circulation problem",
      "url": "https://en.wikipedia.org/wiki/Circulation_problem",
      "text": "The '''circulation problem''' and its variants are a generalisation of [[flow network|network flow]] problems, with the added constraint of a lower bound on edge flows, and with '''flow conservation''' also being required for the source and sink (i.e. there are no special nodes). In variants of the problem, there are multiple commodities flowing through the network, and a cost on the flow.\n\n== Definition ==\nGiven flow network <math>G(V,E)</math> with:\n\n:<math>l(v,w)</math>, lower bound on flow from node <math>v</math> to node <math>w</math>,\n:<math>u(v,w)</math>, upper bound on flow from node <math>v</math> to node <math>w</math>,\n:<math>c(v,w)</math>, cost of a unit of flow on <math>(v,w)</math>\n\nand the constraints:\n\n:<math>l(v,w) \\leq f(v,w) \\leq u(v,w)</math>,\n:<math>\\sum_{w \\in V} f(u,w) = 0</math> (flow cannot appear or disappear in nodes).\n\nFinding a flow assignment satisfying the constraints gives a solution to the given circulation problem. \n\nIn the minimum cost variant of the problem, minimize\n\n: <math>\\sum_{(v,w) \\in E} c(v,w) \\cdot f(v,w).</math>\n\n=== Multi-commodity circulation ===\nIn a multi-commodity circulation problem, you also need to keep track of the flow of the individual commodities:\n\n:{|\n| <math>\\,f_i(v,w)</math> || The flow of commodity <math>i</math> from <math>v</math> to <math>w</math>.\n|-\n| <math>\\,f(v,w) = \\sum_i f_i(v,w)</math> || The total flow.\n|}\n\nThere is also a lower bound on each flow of commodity.\n\n:{|\n| <math>\\,l_i(v,w) \\leq f_i(v,w)</math>\n|}\n\nThe conservation constraint must be upheld individually for the commodities: \n\n:<math>\\ \\sum_{w \\in V} f_i(u,w) = 0.</math>\n\n== Solution ==\nFor the circulation problem, many polynomial algorithms have been developed (e.g., [[Edmonds-Karp algorithm|Edmonds and Karp algorithm]], 1972; Tarjan 1987-1988). Tardos found the first strongly polynomial algorithm.<ref name=\"Ta85\">{{cite journal | author = Éva Tardos | title = A strongly polynomial minimum cost circulation algorithm | journal = Combinatorica | volume = 5 | pages = 247–255 | doi = 10.1007/BF02579369}}</ref>\n <!-- which articles? -->\n\nFor the case of multiple commodities, the problem is [[NP-complete]] for integer flows.<ref name=\"EIS76\">{{cite journal | author = S. Even and A. Itai and A. Shamir | title = On the complexity of time table and multi-commodity flow problems | publisher = SIAM | year = 1976 | journal = SIAM Journal on Computing | volume = 5 | pages = 691–703 | url = http://link.aip.org/link/?SMJ/5/691/1 | doi = 10.1137/0205048 | issue = 4 | deadurl = yes | archiveurl = https://archive.is/20130112133748/http://link.aip.org/link/?SMJ/5/691/1 | archivedate = 2013-01-12 | df =  }}</ref> For fractional flows, it is solvable in [[polynomial time]], as one can formulate the problem as a [[linear programming|linear program]].\n\n== Related problems ==\n\nBelow are given some problems, and how to solve them with the general circulation setup given above.\n\n* Minimum cost multi-commodity circulation problem - Using all constraints given above.\n* Minimum cost circulation problem - Use a single commodity\n* Multi-commodity circulation - Solve without optimising cost.\n* Simple circulation - Just use one commodity, and no cost.\n* [[Multi-commodity flow problem|Multi-commodity flow]] - If <math>K_i(s_i,t_i,d_i)</math> denotes a demand of <math>d_i</math> for commodity <math>i</math> from <math>s_i</math> to <math>t_i</math>, create an edge <math>(t_i,s_i)</math> with <math>l_i(t_i,s_i) = u(t_i,s_i) = d_i</math> for all commodities <math>i</math>. Let <math>l_i(u,v)=0</math> for all other edges.\n* [[Minimum cost multi-commodity flow problem]] - As above, but minimize the cost.\n* [[Minimum cost flow problem]] - As above, with 1 commodity.\n* [[Maximum flow problem]] - Set all costs to 0, and add an edge from the sink <math>t</math> to the source <math>s</math> with <math>l(t,s)=0</math>, <math>u(t,s)=</math>∞ and <math>c(t,s)=-1</math>.\n* [[Minimum cost maximum flow problem]] - First find the maximum flow amount <math>m</math>. Then solve with <math>l(t,s)=u(t,s)=m</math> and <math>c(t,s)=0</math>.\n* [[Shortest path problem|Single-source shortest path]] - Let <math>l(u,v)=0</math> and <math>c(u,v)=1</math> for all edges in the graph, and add an edge <math>(t,s)</math> with <math>l(t,s)=c(t,s)=1</math> and <math>a(t,s)=0</math>.\n* [[Shortest path problem|All-pairs shortest path]] - Let all capacities be unlimited, and find a flow of 1 for <math>v(v-1)/2</math> commodities, one for each pair of nodes.\n\n== References ==\n<references/>\n\n[[Category:Network flow problem]]\n[[Category:Mathematical problems]]"
    },
    {
      "title": "Dinic's algorithm",
      "url": "https://en.wikipedia.org/wiki/Dinic%27s_algorithm",
      "text": "'''Dinic's algorithm''' or '''Dinitz's algorithm''' is a [[strongly polynomial]] algorithm for computing the [[maximum flow]] in a [[flow network]], conceived in 1970 by Israeli (formerly Soviet) computer scientist Yefim (Chaim) A. Dinitz.<ref>{{cite journal | author = [[Yefim Dinitz]] | title = Algorithm for solution of a problem of maximum flow in a network with power estimation | journal = Doklady Akademii Nauk SSSR | volume = 11 | year = 1970 | pages = 1277&ndash;1280 | url=http://www.cs.bgu.ac.il/~dinitz/D70.pdf}}</ref> The algorithm runs in <math>O(V^2 E)</math> time and is similar to the [[Edmonds–Karp algorithm]], which runs in <math>O(VE^2)</math> time, in that it uses shortest augmenting paths. The introduction of the concepts of the ''level graph'' and ''blocking flow'' enable Dinic's algorithm to achieve its performance.\n\n==History==\nYefim Dinitz invented this algorithm in response to a pre-class exercise in [[Georgy Adelson-Velsky|Adelson-Velsky]]'s algorithms class. At the time he was not aware of the basic facts regarding the [[Ford–Fulkerson algorithm]].<ref>{{cite web | date = 2009-11-27 |author1=Ilan Kadar |author2=Sivan Albagli | title = Dinitz's algorithm for finding a maximum flow in a network | url=http://www.powershow.com/view/c6619-OThkZ/Dinitzs_algorithm_for_finding_a_maximum_flow_in_a_network_powerpoint_ppt_presentation}}</ref>\n\nDinitz mentions inventing his algorithm in January 1969, which was published in 1970 in the journal ''Doklady Akademii Nauk SSSR''. In 1974, Shimon Even and (his then Ph.D. student) Alon Itai at the [[Technion – Israel Institute of Technology|Technion]] in Haifa were very curious and intrigued by Dinitz's algorithm as well as [[Alexander V. Karzanov]]'s related idea of blocking flow. However it was hard for them to decipher these two papers, each being limited to four pages to meet the restrictions of journal ''Doklady Akademii Nauk SSSR''. Even did not give up, and after three days of effort managed to understand both papers except for the layered network maintenance issue. Over the next couple of years, Even gave lectures on \"Dinic's algorithm\", mispronouncing the name of the author while popularizing it. Even and Itai also contributed to this algorithm by combining [[Breadth-first search|BFS]] and [[Depth-first search|DFS]], which is the current version of the algorithm.<ref>{{cite journal | author=Yefim Dinitz | title=Dinitz's Algorithm: The Original Version and Even's Version | url=http://www.cs.bgu.ac.il/~dinitz/Papers/Dinitz_alg.pdf}}</ref>\n\nFor about 10 years of time after the Ford–Fulkerson algorithm was invented, it was unknown if it could be made to terminate in polynomial time in the general case of irrational edge capacities. This caused a lack of any known polynomial-time algorithm to solve the max flow problem in generic cases. Dinitz's algorithm and the [[Edmonds–Karp algorithm]] (published in 1972) both independently showed that in the Ford–Fulkerson algorithm, if each augmenting path is the shortest one, then the length of the augmenting paths is non-decreasing and the algorithm always terminates.\n\n==Definition==\nLet <math>G = ((V,E),c,f,s,t)</math> be a network with <math>c(u,v)</math> and <math>f(u,v)</math> the capacity and the flow of the edge <math>(u,v)</math> respectively.\n\n:The '''residual capacity''' is a mapping <math>c_f\\colon V\\times V \\to R^+</math> defined as,\n:# if <math>(u,v)\\in E</math>,\n:#: <math>c_f(u,v) = c(u,v) - f(u,v) </math>\n:#: <math>c_f(v,u) = f(u,v)</math>\n:# <math>c_f(u,v) = 0</math> otherwise.\n\n:The '''residual graph''' is a unweighted graph <math>G_f = ((V, E_f), c_f|_{E_f}, s, t)</math>, where\n:: <math>E_f = \\{(u,v)\\in V \\times V : c_f(u,v) > 0\\}</math>.\n\n:An '''augmenting path''' is an <math>s-t</math> path in the residual graph <math>G_f</math>.\n\n:Define <math>\\operatorname{dist}(v)</math> to be the length of the shortest path from <math>s</math> to <math>v</math> in <math>G_f</math>. Then the '''level graph''' of <math>G_f</math> is the graph <math>G_L = ((V, E_L), c_f|_{E_L}, s,t)</math>, where\n:: <math>E_L = \\{(u,v)\\in E_f : \\operatorname{dist}(v) = \\operatorname{dist}(u) + 1\\}</math>.\n\n:A '''blocking flow''' is an <math>s-t</math> flow <math>f</math> such that the graph <math>G' = ((V,E_L'), s, t)</math> with <math>E_L' = \\{(u,v) : f(u,v) < c_f|_{E_L}(u,v)\\}</math> contains no <math>s-t</math> path.<ref>This means that the subgraph resulting from removing all saturated edges (i.e. all edges <math>(u,v)</math> such that <math>f(u,v)=c_f|_{E_L}(u,v)</math>) does not contain any path from <math>s</math> to <math>t</math>. In other terms, the ''blocking flow'' is such that every possible path from <math>s</math> to <math>t</math> contains a saturated edge.</ref>{{sfn|Tarjan|1983|p=102}}\n\n==Algorithm==\n'''Dinic's Algorithm'''\n: ''Input'': A network <math>G = ((V, E), c, s, t)</math>.\n: ''Output'': An <math>s-t</math> flow <math>f</math> of maximum value.\n# Set <math>f(e) = 0</math> for each <math>e\\in E</math>.\n# Construct <math>G_L</math> from <math>G_f</math> of <math>G</math>. If <math>\\operatorname{dist}(t) = \\infty</math>, stop and output <math>f</math>.\n# Find a blocking flow <math>f\\;'</math> in <math>G_L</math>.\n# Add augment flow <math>\\ f</math> by <math>f\\;'</math> and go back to step 2.\n\n==Analysis==\nIt can be shown that the number of layers in each blocking flow increases by at least 1 each time and thus there are at most <math>|V|-1</math> blocking flows in the algorithm. For each of them:\n\n* the level graph <math>G_L</math> can be constructed by [[breadth-first search]] in <math>O(E)</math> time\n* a blocking flow in the level graph <math>G_L</math> can be found in <math>O(VE)</math> time\n\nwith total running time <math>O(E + VE) = O(VE)</math> for each layer. As a consequence, the running time of Dinic's algorithm is <math>O(V^2 E)</math>.\n\nUsing a data structure called [[dynamic trees]], the running time of finding a blocking flow in each phase can be reduced to <math>O(E \\log V)</math> and therefore the running time of Dinic's algorithm can be improved to <math>O(VE \\log V)</math>.\n\n=== Special cases ===\nIn networks with unit capacities, a much stronger time bound holds. Each blocking flow can be found in <math>O(E)</math> time, and it can be shown that the number of phases does not exceed <math>O(\\sqrt{E})</math> and <math>O(V^{2/3})</math>. Thus the algorithm runs in <math>O(\\min\\{V^{2/3}, E^{1/2}\\}E)</math> time.<ref name=\"Even1975\">{{cite journal|last=Even|first=Shimon|last2=Tarjan|first2=R. Endre|year=1975|title=Network Flow and Testing Graph Connectivity|journal=SIAM Journal on Computing|volume=4|issue=4|pages=507–518|issn=0097-5397|doi=10.1137/0204043}}</ref>\n\nIn networks that arise from the [[bipartite matching]] problem, the number of phases is bounded by <math>O(\\sqrt{V})</math>, therefore leading to the <math>O(\\sqrt{V} E)</math> time bound. The resulting algorithm is also known as [[Hopcroft–Karp algorithm]]. More generally, this bound holds for any ''unit network'' — a network in which each vertex, except for source and sink, either has a single entering edge of capacity one, or a single outgoing edge of capacity one, and all other capacities are arbitrary integers.{{sfn|Tarjan|1983|p=102}}\n\n==Example==\nThe following is a simulation of Dinic's algorithm. In the level graph <math>G_L</math>, the vertices with labels in red are the values <math>\\operatorname{dist}(v)</math>. The paths in blue form a blocking flow.\n\n{| class=\"wikitable\" style=\"text-align:center; width:915px;\" border=\"1\"\n|-\n! width=\"15px\" |\n\n! <math>G</math>\n! <math>G_f</math>\n! <math>G_L</math>\n|-\n! 1.\n| [[File:Dinic algorithm G1.svg|300px]]\n| [[File:Dinic algorithm Gf1.svg|300px]]\n| [[File:Dinic algorithm GL1.svg|300px]]\n|-\n| \n| align=\"left\" colspan=\"3\" |\nThe blocking flow consists of\n# <math>\\{s, 1, 3, t\\}</math> with 4 units of flow,\n# <math>\\{s, 1, 4, t\\}</math> with 6 units of flow, and\n# <math>\\{s, 2, 4, t\\}</math> with 4 units of flow.\nTherefore, the blocking flow is of 14 units and the value of flow <math>|f|</math> is 14. Note that each augmenting path in the blocking flow has ''3'' edges.\n|-\n! 2.\n| [[File:Dinic algorithm G2.svg|300px]]\n| [[File:Dinic algorithm Gf2.svg|300px]]\n| [[File:Dinic algorithm GL2.svg|300px]]\n|-\n| \n| align=\"left\" colspan=\"3\" |\nThe blocking flow consists of\n# <math>\\{s, 2, 4, 3, t\\}</math> with 5 units of flow.\nTherefore, the blocking flow is of 5 units and the value of flow <math>|f|</math> is 14 + 5 = 19. Note that each augmenting path has 4 edges.\n|-\n! 3.\n| [[File:Dinic algorithm G3.svg|300px]]\n| [[File:Dinic algorithm Gf3.svg|300px]]\n| [[File:Dinic algorithm GL3.svg|300px]]\n|-\n| \n| align=\"left\" colspan=\"3\" |\nSince <math>t</math> cannot be reached in <math>G_f</math>, the algorithm terminates and returns a flow with maximum value of 19. Note that in each blocking flow, the number of edges in the augmenting path increases by at least 1.\n|-\n|}\n\n==See also==\n* [[Ford–Fulkerson algorithm]]\n* [[Maximum flow problem]]\n\n== Notes ==\n{{reflist}}\n\n==References==\n* {{cite book | author = Yefim Dinitz | editor =  [[Oded Goldreich]] |editor2=Arnold  L. Rosenberg |editor3=Alan L. Selman | title = Theoretical Computer Science: Essays in Memory of [[Shimon Even]] | chapter = Dinitz' Algorithm: The Original Version and Even's Version | year = 2006 | publisher = Springer | isbn = 978-3-540-32880-3 | pages = 218–240 | chapter-url = http://www.cs.bgu.ac.il/~dinitz/Papers/Dinitz_alg.pdf}}\n* {{cite book | ref=harv | last=Tarjan | first=R. E. |year =1983 | title=Data structures and network algorithms }}\n* {{cite book |author1=B. H. Korte |author2=Jens Vygen | title = Combinatorial Optimization: Theory and Algorithms (Algorithms and Combinatorics, 21) | chapter = 8.4 Blocking Flows and Fujishige's Algorithm | year = 2008 | publisher = Springer Berlin Heidelberg | isbn = 978-3-540-71844-4 | pages = 174–176}}\n\n[[Category:Network flow problem]]\n[[Category:Graph algorithms]]"
    },
    {
      "title": "Edmonds–Karp algorithm",
      "url": "https://en.wikipedia.org/wiki/Edmonds%E2%80%93Karp_algorithm",
      "text": "{{Use American English|date = April 2019}}\n{{Short description|algorithm to compute the maximum flow in a flow network (equivalently; the minimum cut)}}\nIn [[computer science]], the '''Edmonds–Karp algorithm''' is an implementation of the [[Ford–Fulkerson algorithm|Ford–Fulkerson method]] for computing the [[maximum flow problem|maximum flow]] in a [[flow network]] in [[big O notation|<math>O</math>]]<math>(|V||E|^2)</math> time. The algorithm was first published by Yefim Dinitz in 1970<ref>{{cite journal |first=E. A. |last=Dinic |title=Algorithm for solution of a problem of maximum flow in a network with power estimation |journal=Soviet Mathematics - Doklady |volume=11 |issue= |pages=1277–1280 |publisher=Doklady |year=1970 |url= |doi= |id= |accessdate= }}</ref><ref>{{cite journal | author=Yefim Dinitz | title=Dinitz's Algorithm: The Original Version and Even's Version | url=http://www.cs.bgu.ac.il/~dinitz/Papers/Dinitz_alg.pdf}}</ref> and independently published by [[Jack Edmonds]] and [[Richard Karp]] in 1972.<ref>{{cite journal |last1=Edmonds |first1=Jack |author1-link=Jack Edmonds |last2=Karp |first2=Richard M. |author2-link=Richard Karp |title=Theoretical improvements in algorithmic efficiency for network flow problems |journal=Journal of the ACM |volume=19 |issue=2 |pages=248–264  |year=1972 |url= |doi=10.1145/321694.321699 |id= |accessdate= }}</ref> [[Dinic's algorithm]] includes additional techniques that reduce the running time to <math>O(|V|^2|E|)</math>.\n\n==Algorithm==\n\nThe algorithm is identical to the [[Ford–Fulkerson algorithm]], except that the search order when finding the [[Flow network#Augmenting paths|augmenting path]] is defined. The path found must be a shortest path that has available capacity. This can be found by a [[breadth-first search]], where we apply a weight of 1 to each edge. The running time of <math>O(|V||E|^2)</math>is found by showing that each augmenting path can be found in <math>O(|E|)</math> time, that every time at least one of the <math>E</math> edges becomes saturated (an edge which has the maximum possible flow), that the distance from the saturated edge to the source along the augmenting path must be longer than last time it was saturated, and that the length is at most <math>|V|</math>. Another property of this algorithm is that the length of the shortest augmenting path increases monotonically. There is an accessible proof in ''[[Introduction to Algorithms]]''.<ref>{{cite book |author=[[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]] and [[Clifford Stein]] |title=Introduction to Algorithms |publisher=MIT Press | year = 2009 |isbn=978-0-262-03384-8 |edition=third |chapter=26.2 |pages=727–730 |title-link=Introduction to Algorithms }}</ref>\n\n==Pseudocode==\n{{Wikibooks|Algorithm implementation|Graphs/Maximum flow/Edmonds-Karp|Edmonds-Karp}}\n\n '''algorithm''' EdmondsKarp\n     '''input''':\n         graph   ''(graph[v] should be the list of edges coming out of vertex v in the''\n                 '' original graph '''and''' their corresponding constructed reverse edges''\n                 '' which are used for push-back flow.''\n                 '' Each edge should have a capacity, flow, source and sink as parameters,''\n                 '' as well as a pointer to the reverse edge.)''\n         s       ''(Source vertex)''\n         t       ''(Sink vertex)''\n     '''output''':\n         flow    ''(Value of maximum flow)''\n     \n     flow := 0   ''(Initialize flow to zero)''\n     '''repeat'''\n         ''(Run a bfs to find the shortest s-t path.''\n         '' We use 'pred' to store the edge taken to get to each vertex,''\n         '' so we can recover the path afterwards)''\n         q := '''queue'''()\n         q.push(s)\n         pred := '''array'''(graph.length)\n         '''while''' '''not''' empty(q)\n             cur := q.pull()\n             '''for''' Edge e '''in''' graph[cur]\n                  '''if''' pred[e.t] = '''null''' '''and''' e.t ≠ s '''and''' e.cap > e.flow\n                     pred[e.t] := e\n                     q.push(e.t)\n     \n         '''if''' '''not''' (pred[t] = null)         \n             ''(We found an augmenting path.''\n             '' See how much flow we can send)'' \n             df := '''∞'''\n             '''for''' (e := pred[t]; e ≠ null; e := pred[e.s])\n                 df := '''min'''(df, e.cap - e.flow)\n             ''(And update edges by that amount)''\n             '''for''' (e := pred[t]; e ≠ null; e := pred[e.s])\n                 e.flow  := e.flow + df\n                 e.rev.flow := e.rev.flow - df\n             flow := flow + df\n     \n     '''until''' pred[t] = null  ''(i.e., until no augmenting path was found)''\n     '''return''' flow\n\n==Example==\nGiven a network of seven nodes, source A, sink G, and capacities as shown below:\n\n[[Image:Edmonds-Karp flow example 0.svg|300px]]\n\nIn the pairs <math>f/c</math> written on the edges, <math>f</math> is the current flow, and <math>c</math> is the capacity. The residual capacity from <math>u</math> to <math>v</math> is <math>c_f(u,v)=c(u,v)-f(u,v)</math>, the total capacity, minus the flow that is already used. If the net flow from <math>u</math> to <math>v</math> is negative, it ''contributes'' to the residual capacity.\n\n{| class=\"wikitable\"\n|-\n! Capacity\n! Path\n! Resulting network\n|-\n| <math>\\begin{align}\n  & \\min(c_f(A,D),c_f(D,E),c_f(E,G)) \\\\\n= & \\min(3-0,2-0,1-0) = \\\\\n= & \\min(3,2,1) = 1\n\\end{align}</math>\n|align=\"center\"| <math>A,D,E,G</math>\n| [[Image:Edmonds-Karp flow example 1.svg|300px]]</td>\n|-\n| <math>\\begin{align}\n  & \\min(c_f(A,D),c_f(D,F),c_f(F,G)) \\\\\n= & \\min(3-1,6-0,9-0) \\\\\n= & \\min(2,6,9) = 2\n\\end{align}</math>\n|align=\"center\"| <math>A,D,F,G</math>\n| [[Image:Edmonds-Karp flow example 2.svg|300px]]</td>\n|-\n| <math>\\begin{align}\n  & \\min(c_f(A,B),c_f(B,C),c_f(C,D),c_f(D,F),c_f(F,G)) \\\\\n= & \\min(3-0,4-0,1-0,6-2,9-2) \\\\\n= & \\min(3,4,1,4,7) = 1\n\\end{align}</math>\n|align=\"center\"| <math>A,B,C,D,F,G</math>\n| [[Image:Edmonds-Karp flow example 3.svg|300px]]</td>\n|-\n| <math>\\begin{align}\n  & \\min(c_f(A,B),c_f(B,C),c_f(C,E),c_f(E,D),c_f(D,F),c_f(F,G)) \\\\\n= & \\min(3-1,4-1,2-0,0-(-1),6-3,9-3) \\\\\n= & \\min(2,3,2,1,3,6) = 1\n\\end{align}</math>\n|align=\"center\"| <math>A,B,C,E,D,F,G</math>\n| [[Image:Edmonds-Karp flow example 4.svg|300px]]</td>\n|}\n\nNotice how the length of the [[augmenting path]] found by the algorithm (in red) never decreases. The paths found are the shortest possible. The flow found is equal to the capacity across the [[max flow min cut theorem|minimum cut]] in the graph separating the source and the sink. There is only one minimal cut in this graph, partitioning the nodes into the sets <math>\\{A,B,C,E\\}</math> and <math>\\{D,F,G\\}</math>, with the capacity\n:<math>c(A,D)+c(C,D)+c(E,G)=3+1+1=5.\\ </math>\n\n==Notes==\n{{reflist|30em}}\n\n==References==\n# Algorithms and Complexity (see pages 63–69).  https://web.archive.org/web/20061005083406/http://www.cis.upenn.edu/~wilf/AlgComp3.html\n\n{{DEFAULTSORT:Edmonds-Karp Algorithm}}\n[[Category:Network flow problem]]\n[[Category:Graph algorithms]]"
    },
    {
      "title": "Flow network",
      "url": "https://en.wikipedia.org/wiki/Flow_network",
      "text": "In [[graph theory]], a '''flow network''' (also known as a '''transportation network''') is a [[directed graph]] where each edge has a '''capacity''' and each edge receives a flow. The amount of flow on an edge cannot exceed the capacity of the edge. Often in [[operations research]], a directed graph is called a '''network''', the vertices are called '''nodes''' and the edges are called '''arcs'''.  A flow must satisfy the restriction that the amount of flow into a node equals the amount of flow out of it, unless it is a '''source''', which has only outgoing flow, or '''sink''', which has only incoming flow.  A network can be used to model traffic in a computer network, circulation with demands, fluids in pipes, currents in an electrical circuit, or anything similar in which something travels through a network of nodes.\n\n==Definition==\n\nA '''network''' is a graph {{math|''G'' {{=}} (''V'', ''E'')}}, where {{math|''V''}} is a set of vertices and {{math|''E''}} is a set of {{math|''V''}}’s edges – a subset of {{math|''V'' × ''V''}} – together with a non-negative [[function (mathematics)|function]] {{math|''c'': ''V'' × ''V'' → ℝ<sub>∞</sub>}}, called the '''capacity''' function. [[Without loss of generality]], we may assume that if {{math|(''u'', ''v'') ∈ ''E''}} then {{math|(''v'', ''u'')}} is also a member of {{mvar|E}}, since if {{math|(''v'', ''u'') ∉ ''E''}} then we may add {{math|(''v'', ''u'')}} to ''E'' and then set {{math|''c''(''v'', ''u'') {{=}} 0}}.\n\nIf two nodes in {{mvar|G}} are distinguished, a source {{mvar|s}} and a sink {{mvar|t}}, then {{math|(''G'', ''c'', ''s'', ''t'')}} is called a '''flow network'''.<ref>A.V. Goldberg, É. Tardos and R.E. Tarjan, Network flow algorithms, Tech. Report STAN-CS-89-1252, Stanford University CS Dept., 1989</ref>\n\n==Flows==\n\nThere are various notions of a flow function that can be defined in a flow graph. Flow functions model the net flow of units between pairs of nodes, and are useful when asking questions such as ''what is the maximum number of units that can be transferred from the source node s to the sink node t?'' The simplest example of a flow function is  known as a pseudo-flow.\n\n:A '''pseudo-flow''' is a function {{math|''f'' : ''V'' × ''V'' → ℝ}} that satisfies the following two constraints for all nodes {{mvar|u}} and {{mvar|v}}:\n:*''Skew symmetry'': Only encode the net flow of units between a pair of nodes {{mvar|u}} and {{mvar|v}} (see [[Flow network#Intuition|intuition]] below), that is: {{math|''f'' (''u'', ''v'') {{=}} −''f'' (''v'', ''u'')}}.\n:*''Capacity constraint'': An arc's flow cannot exceed its capacity, that is: {{math|''f'' (''u'', ''v'') ≤ ''c''(''u'', ''v'')}}.\n<br />\nGiven a pseudo-flow {{mvar|f}} in a flow network, it is often useful to consider the net flow entering a given node {{mvar|v}}, that is, the sum of the flows entering {{mvar|v}}. The '''excess''' function {{math|''x''<sub>''f''</sub> : ''V'' → ℝ}} is defined by {{math|''x''<sub>''f''</sub> (''u'') {{=}} ∑<sub>''v'' ∈ ''V''</sub> ''f'' (''v'', ''u'')}}. A node {{mvar|u}} is said to be '''active''' if {{math|''x''<sub>''f''</sub> (''u'') > 0}}, '''deficient''' if {{math|''x''<sub>''f''</sub> (''u'') < 0}} or '''conserving''' if {{math|''x''<sub>''f''</sub> (''u'') {{=}} 0}}.\n\nThese final definitions lead to two strengthenings of the definition of a pseudo-flow:\n\n:A '''pre-flow''' is a pseudo-flow that, for all {{math|''v'' ∈ ''V'' \\{''s''}}}, satisfies the additional constraint:\n:*''Non-deficient flows'': The net flow ''entering'' the node {{mvar|v}} is non-negative, except for the source, which \"produces\" flow. That is: {{math|''x''<sub>''f''</sub> (''v'') ≥ 0}} for all {{math|''v'' ∈ ''V'' \\{''s''}}}.\n\n:A '''feasible flow''', or just a '''flow''', is a pseudo-flow that, for all {{math|''v'' ∈ ''V'' \\{''s'', ''t''}}}, satisfies the additional constraint:\n:*''Flow conservation'': The net flow ''entering'' the node {{mvar|v}} is 0, except for the source, which \"produces\" flow, and the sink, which \"consumes\" flow. That is: {{math|''x''<sub>''f''</sub> (''v'') {{=}} 0}} for all {{math|''v'' ∈ ''V'' \\{''s'', ''t''}}}.\n<br />\nThe '''value''' of a feasible flow {{mvar|f}}, denoted {{math|{{!}} ''f'' {{!}}}}, is the net flow into the sink {{mvar|t}} of the flow network. That is, {{math|{{!}} ''f'' {{!}} {{=}} ''x''<sub>''f''</sub> (''t'')}}.\n\n==Intuition==\n\nIn the context of flow analysis, there is only an interest in considering how units are transferred between nodes in a holistic sense. Put another way, it is not necessary to distinguish multiple arcs between a pair of nodes:\n*Given any two nodes {{mvar|u}} and {{mvar|v}}, if there are two arcs from {{mvar|u}} to {{mvar|v}} with capacities {{math|5}} and {{math|3}} respectively, this is equivalent to considering only a single arc between {{mvar|u}} and {{mvar|v}} with capacity {{math|8}} — it is only useful to know that {{math|8}} units can be transferred from {{mvar|u}} to {{mvar|v}}, not how they can be transferred.\n*Again, given two nodes {{mvar|u}} and {{mvar|v}}, if there is a flow of {{math|5}} units from {{mvar|u}} to {{mvar|v}}, and another flow of {{math|3}} units from {{mvar|v}} to {{mvar|u}}, this is equivalent to a net flow of {{math|2}} units from {{mvar|u}} to {{mvar|v}}, or a net flow of {{math|−2}} units from {{mvar|v}} to {{mvar|u}} (so sign indicates direction) — it is only useful to know that a net flow of {{math|2}} units will flow between {{mvar|u}} and {{mvar|v}}, and the direction that they will flow, not how that net flow is achieved.\n\nFor this reason, the ''capacity function'' {{math|''c'': ''V'' × ''V'' → ℝ<sub>∞</sub>}}, which does not allow for multiple arcs starting and ending at the same nodes, is sufficient for flow analysis. Similarly, it is enough to impose the ''skew symmetry'' constraint on flow functions to ensure that flow between two vertices is encoded by a single number (to indicate magnitude), and a sign (to indicate direction) — by knowing the flow between {{mvar|u}} and {{mvar|v}} you implicitly, via skew symmetry, know the flow between {{mvar|v}} and {{mvar|u}}. These simplifications of the model aren't always immediately intuitive, but they are convenient when it comes time to analyze flows.\n\nThe ''capacity constraint'' simply ensures that a flow on any one arc within the network cannot exceed the capacity of that arc.\n\n==Concepts useful to flow problems==\n\n===Residuals===\nThe '''residual capacity''' of an arc with respect to a pseudo-flow {{mvar|f}}, denoted {{math|''c''<sub>''f''</sub>}}, is the difference between the arc's capacity and its flow. That is, {{math|''c''<sub>''f''</sub> (''e'') {{=}} ''c''(''e'') - ''f''(''e'')}}. From this we can construct a '''residual network''', denoted {{math|''G''<sub>''f''</sub> (''V'', ''E''<sub>''f''</sub>)}}, which models the amount of ''available'' capacity on the set of arcs in {{math|''G'' {{=}} (''V'', ''E'')}}. More formally, given a flow network {{mvar|''G''}}, the residual network {{math|''G''<sub>''f''</sub>}}&nbsp; has the node set {{mvar|V}}, arc set {{math|''E''<sub>''f''</sub> {{=}} {{(}}''e'' ∈ ''V'' × ''V'' : ''c<sub>f</sub>'' (''e'') > 0{{)}}}} and capacity function {{math|''c''<sub>''f''</sub>}}.\n\nThis concept is used in [[Ford–Fulkerson algorithm]] which computes the [[maximum flow]] in a flow network.\n\nNote that there can be a path from {{mvar|u}} to {{mvar|v}} in the residual network, even though there is no path from {{mvar|u}} to {{mvar|v}} in the original network. Since flows in opposite directions cancel out, ''decreasing'' the flow from {{mvar|v}} to {{mvar|u}} is the same as ''increasing'' the flow from {{mvar|u}} to {{mvar|v}}.\n\n===Augmenting paths===\nAn '''augmenting path''' is a path {{math|(''u''<sub>1</sub>, ''u''<sub>2</sub>, ..., ''u''<sub>''k''</sub>)}} in the residual network, where {{math|''u''<sub>1</sub> {{=}} ''s''}}, {{math|''u''<sub>''k''</sub> {{=}} ''t''}}, and {{math|''c''<sub>''f''</sub> (''u''<sub>''i''</sub>, ''u''<sub>''i'' + 1</sub>) > 0}}. A network is at [[maximum flow]] if and only if there is no augmenting path in the residual network {{math|''G''<sub>''f''</sub>}}.\n\n===Multiple sources and/or sinks===\nSometimes, when modeling a network with more than one source, a '''supersource''' is introduced to the graph.<ref>{{DADS|Supersource|supersource}}</ref> This consists of a vertex connected to each of the sources with edges of infinite capacity, so as to act as a global source. A similar construct for sinks is called a '''supersink'''.<ref>{{DADS|Supersink|supersink}}</ref>\n\n==Example==\n\n[[File:Network Flow SVG.svg|left|thumb|332px|A flow network showing flow and capacity]]\nTo the left you see a flow network with source labeled {{mvar|s}}, sink {{mvar|t}}, and four additional nodes. The flow and capacity is denoted <math>f/c</math>. Notice how the network upholds skew symmetry, capacity constraints and flow conservation. The total amount of flow from {{mvar|s}} to {{mvar|t}} is 5, which can be easily seen from the fact that the total outgoing flow from {{mvar|s}} is 5, which is also the incoming flow to {{mvar|t}}. We know that no flow appears or disappears in any of the other nodes.\n\n[[File:Network flow residual SVG.svg|left|thumb|332px|Residual network for the above flow network, showing residual capacities.]]\nBelow you see the residual network for the given flow. Notice how there is positive residual capacity on some edges where the original capacity is zero, for example for the edge <math>(d,c)</math>. This flow is not a [[max flow|maximum flow]]. There is available capacity along the paths <math>(s,a,c,t)</math>, <math>(s,a,b,d,t)</math> and <math>(s,a,b,d,c,t)</math>, which are then the augmenting paths. The residual capacity of the first path is\n<math>\\min(c(s,a)-f(s,a), c(a,c)-f(a,c), c(c,t)-f(c,t))</math> <math>= \\min(5-3, 3-2, 2-1) = \\min(2, 1, 1) = 1</math>.{{Citation needed|reason=citation for the formula used|date=December 2015}} Notice that as long as there exists some path with a positive residual capacity, the flow will not be maximum. The residual capacity for some path is the minimum residual capacity of all edges in that path.\n\n==Applications==\n{{see also|Pipe network analysis}}\n\nPicture a series of water pipes, fitting into a network. Each pipe is of a certain diameter, so it can only maintain a flow of a certain amount of water. Anywhere that pipes meet, the total amount of water coming into that junction must be equal to the amount going out, otherwise we would quickly run out of water, or we would have a buildup of water. We have a water inlet, which is the source, and an outlet, the sink. A flow would then be one possible way for water to get from source to sink so that the total amount of water coming out of the outlet is consistent. Intuitively, the total flow of a network is the rate at which water comes out of the outlet.\n\nFlows can pertain to people or material over transportation networks, or to electricity over [[electrical distribution]] systems.  For any such physical network, the flow coming into any intermediate node needs to equal the flow going out of that node.  This conservation constraint is equivalent to [[Kirchhoff's current law]].\n\nFlow networks also find applications in [[ecology]]: flow networks arise naturally when considering the flow of nutrients and energy between different organisms in a [[food web]].  The mathematical problems associated with such networks are quite different from those that arise in networks of fluid or traffic flow.  The field of ecosystem network analysis, developed by [[Robert Ulanowicz]] and others, involves using concepts from [[information theory]] and [[thermodynamics]] to study the evolution of these networks over time.\n\n==Classifying flow problems==\n\nThe simplest and most common problem using flow networks is to find what is called the [[maximum flow problem|maximum flow]], which provides the largest possible total flow from the source to the sink in a given graph. There are many other problems which can be solved using max flow algorithms, if they are appropriately modeled as flow networks, such as [[bipartite matching]], the [[assignment problem]] and the [[transportation problem]]. Maximum flow problems can be solved efficiently with the [[relabel-to-front algorithm]]. The [[max-flow min-cut theorem]] states that finding a maximal network flow is equivalent to finding a  [[Cut (graph theory)|cut]] of minimum capacity that separates the source and the sink, where a cut is the division of vertices such that the source is in one division and the sink is in another.\n\n{| class=\"wikitable\" style=\"height: 200px;\" align=\"right\"\n|+ Well-known algorithms for the Maximum Flow Problem\n|-\n! Inventor(s) || Year || Time<br/>complexity<br/>(with {{math|''n''}} nodes<br/>and {{math|''m''}} arcs)\n|-\n| [[Edmonds–Karp algorithm]] || 1972 || {{math|''O''(''m''<sup>2</sup>''n'')}}\n|-\n| |MPM (Malhotra, Pramodh-Kumar and Maheshwari)<br/>algorithm<ref>{{Cite journal | last1 = Malhotra | first1 =V.M.| last2 = Kumar | first2 = M.Pramodh| last3 = Maheshwari | first3 = S.N.| doi = 10.1016/0020-0190(78)90016-9| title = An <math>O(|V|^3)</math> algorithm for finding maximum flows in networks| journal = Information Processing Letters| volume = 7| issue = 6| pages = 277–278 | year = 1978}}</ref>\n  ||  1978 || {{math|''O''(''n''<sup>3</sup>)}}\n|-\n| [[James B. Orlin]]<ref>{{Cite journal|last=Orlin|first=J. B.|date=2013|title=Max flows in O(nm) time, or better.|url=http://jorlin.scripts.mit.edu/docs/publications/O(nm)MaxFlow.pdf|journal=Proceedings of the 2013 Symposium on the Theory of Computing|volume=|pages=765-774|via=|df=}}  Archived at </ref> || 2013 || {{math|''O''(''mn'')}}\n|}\n\nIn a [[multi-commodity flow problem]], you have multiple sources and sinks, and various \"commodities\" which are to flow from a given source to a given sink. This could be for example various goods that are produced at various factories, and are to be delivered to various given customers through the ''same'' transportation network.\n\nIn a [[minimum cost flow problem]], each edge <math>u,v</math> has a given cost <math>k(u,v)</math>, and the cost of sending the flow <math>f(u,v)</math> across the edge is <math>f(u,v) \\cdot k(u,v)</math>. The objective is to send a given amount of flow from the source to the sink, at the lowest possible price.\n\nIn a [[circulation problem]], you have a lower bound <math>l(u,v)</math> on the edges, in addition to the upper bound <math>c(u,v)</math>. Each edge also has a cost. Often, flow conservation holds for ''all'' nodes in a circulation problem, and there is a connection from the sink back to the source. In this way, you can dictate the total flow with <math>l(t,s)</math> and <math>c(t,s)</math>. The flow ''circulates'' through the network, hence the name of the problem.\n\nIn a '''network with gains''' or '''generalized network''' each edge has a '''[[gain graph|gain]]''', a real number (not zero) such that, if the edge has gain ''g'', and an amount ''x'' flows into the edge at its tail, then an amount ''gx'' flows out at the head.\n\nIn a '''source localization problem''', an algorithm tries to identify the most likely source node of information diffusion through a partially observed network. This can be done in linear time for trees and cubic time for arbitrary networks and has applications ranging from tracking mobile phone users to identifying the originating village of disease outbreaks.<ref>http://www.pedropinto.org.s3.amazonaws.com/publications/locating_source_diffusion_networks.pdf</ref>\n\n==See also==\n* [[Braess' paradox]]\n* [[Centrality]]\n* [[Ford–Fulkerson algorithm]]\n* [[Dinic's algorithm]]\n* [[Flow (computer networking)]]\n* [[Flow graph (disambiguation)]]\n* [[Max-flow min-cut theorem]]\n* [[Oriented matroid]]\n* [[Shortest path problem]]\n* [[Nowhere-zero flow]]\n\n==References==\n{{Reflist}}\n\n== Further reading ==\n* {{cite book |author1=George T. Heineman |author2=Gary Pollice |author3=Stanley Selkow | title= Algorithms in a Nutshell | publisher=[[Oreilly Media]] | year=2008 | chapter=Chapter 8:Network Flow Algorithms | pages = 226–250 | isbn=978-0-596-51624-6 }}\n* {{cite book | author=[[Ravindra K. Ahuja]], [[Thomas L. Magnanti]], and [[James B. Orlin]] | title= Network Flows: Theory, Algorithms and Applications | publisher=Prentice Hall | year=1993 | isbn=0-13-617549-X }}\n* {{cite book | author=[[Béla Bollobás|Bollobás, Béla]] | title= Graph Theory: An Introductory Course | location=Heidelberg | publisher=Springer-Verlag | year=1979 | isbn=3-540-90399-2}}\n* {{cite book | author=[[Gary Theodore Chartrand|Chartrand, Gary]] & [[Ortrud Oellermann|Oellermann, Ortrud R.]] | title=Applied and Algorithmic Graph Theory | location=New York | publisher=McGraw-Hill | year=1993 | isbn=0-07-557101-3}}\n* {{cite book | author=Even, Shimon | title=Graph Algorithms | location=Rockville, Maryland | publisher=Computer Science Press | year=1979 | isbn=0-914894-21-8}}\n* {{cite book | author=Gibbons, Alan | title=Algorithmic Graph Theory | location=Cambridge | publisher=Cambridge University Press | year=1985 | isbn=0-521-28881-9 }}\n* {{cite book | author = [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]] | title = [[Introduction to Algorithms]] | origyear = 1990 | edition = 2nd | year = 2001 | publisher = MIT Press and McGraw-Hill | pages = 696–697 | chapter = 26 | isbn = 0-262-03293-7}}\n\n==External links==\n* [http://www-b2.is.tokushima-u.ac.jp/~ikeda/suuri/maxflow/Maxflow.shtml Maximum Flow Problem]\n* [http://www.dis.uniroma1.it/~challenge9/download.shtml Real graph instances]\n* [http://lemon.cs.elte.hu/ Lemon C++ library with several maximum flow and minimum cost circulation algorithms]\n* [http://quickgraph.codeplex.com/ QuickGraph], graph data structures and algorithms for .Net\n\n[[Category:Network flow problem]]"
    },
    {
      "title": "Ford–Fulkerson algorithm",
      "url": "https://en.wikipedia.org/wiki/Ford%E2%80%93Fulkerson_algorithm",
      "text": "{{Use American English|date = April 2019}}\n\n{{Short description|algorithm to compute the maximum flow in a flow network (equivalently; the minimum cut)}}\n\nThe '''Ford&ndash;Fulkerson method''' or '''Ford–Fulkerson algorithm''' ('''FFA''') is a [[greedy algorithm]] that computes the [[maximum flow problem|maximum flow]] in a [[flow network]]. It is sometimes called a \"method\" instead of an \"algorithm\" as the approach to finding augmenting paths in a residual graph is not fully specified<ref>{{Cite book|title = Electronic Design Automation: Synthesis, Verification, and Test|last = Laung-Terng Wang, Yao-Wen Chang, Kwang-Ting (Tim) Cheng|publisher = Morgan Kaufmann|year = 2009|isbn = 0080922007|location = |pages = 204}}</ref> or it is specified in several implementations with different running times.<ref>{{Cite book|title = Introduction to Algorithms|author1=Thomas H. Cormen |author2=Charles E. Leiserson |author3=Ronald L. Rivest |author4=Clifford Stein |publisher = MIT Press|year = 2009|isbn = 0262258102|location = |pages = 714}}</ref> It was published in 1956 by [[L. R. Ford, Jr.]] and [[D. R. Fulkerson]].<ref>{{Cite journal | last1 = Ford | first1 = L. R. | authorlink1 = L. R. Ford, Jr.| last2 = Fulkerson | first2 = D. R. | authorlink2 = D. R. Fulkerson| doi = 10.4153/CJM-1956-045-5 | title = Maximal flow through a network | journal = [[Canadian Journal of Mathematics]]| volume = 8 | pages = 399–404 | year = 1956 | pmid =  | pmc = | url = http://www.cs.yale.edu/homes/lans/readings/routing/ford-max_flow-1956.pdf }}</ref> The name \"Ford&ndash;Fulkerson\" is often also used for the [[Edmonds–Karp algorithm]], which is a fully defined implementation of the Ford&ndash;Fulkerson method.\n\nThe idea behind the algorithm is as follows: as long as there is a path from the source (start node) to the sink (end node), with available capacity on all edges in the path, we send flow along one of the paths. Then we find another path, and so on. A path with available capacity is called an [[augmenting path]].\n\n==Algorithm==\nLet <math>G(V,E)</math> be a graph, and for each edge from {{mvar|u}} to {{mvar|v}}, let <math>c(u,v)</math> be the capacity and <math>f(u,v)</math> be the flow. We want to find the maximum flow from the source {{mvar|s}} to the sink {{mvar|t}}. After every step in the algorithm the following is maintained:\n\n:{| class=\"wikitable\"\n! {{rh}} | Capacity constraints\n| <math>\\forall (u, v) \\in E: \\ f(u,v) \\le c(u,v)</math> || The flow along an edge can not exceed its capacity.\n|-\n! {{rh}} | Skew symmetry\n| <math>\\forall (u, v) \\in E: \\ f(u,v) = - f(v,u)</math> || The net flow from {{mvar|u}} to {{mvar|v}} must be the opposite of the net flow from {{mvar|v}} to {{mvar|u}} (see example).\n|-\n! {{rh}} | Flow conservation \n| <math style=\"vertical-align:-125%;\">\\forall u \\in V: u \\neq s \\text{ and } u \\neq t \\Rightarrow \\sum_{w \\in V} f(u,w) = 0</math> || The net flow to a node is zero, except for the source, which \"produces\" flow, and the sink, which \"consumes\" flow.\n|-\n! {{rh}} | Value(f)\n| <math>\\sum_{(s,u) \\in E} f(s, u) = \\sum_{(v,t) \\in E} f(v, t)</math> || The flow leaving from {{mvar|s}} must be equal to the flow arriving at {{mvar|t}}.\n|-\n|}\n\nThis means that the flow through the network is a ''legal flow'' after each round in the algorithm. We define the '''residual network''' <math>G_f(V,E_f)</math> to be the network with capacity <math>c_f(u,v) = c(u,v) - f(u,v)</math> and no flow. Notice that it can happen that a flow from {{mvar|v}} to {{mvar|u}} is allowed in the residual\nnetwork, though disallowed in the original network: if <math>f(u,v)>0</math> and <math>c(v,u)=0</math> then <math>c_f(v,u)=c(v,u)-f(v,u)=f(u,v)>0</math>.\n\n{{Algorithm-begin|name=Ford&ndash;Fulkerson}}\n:'''Inputs'''  Given a Network <math>G = (V,E)</math> with flow capacity {{mvar|c}}, a source node {{mvar|s}}, and a sink node {{mvar|t}}\n:'''Output''' Compute a flow {{mvar|f}} from {{mvar|s}} to {{mvar|t}} of maximum value\n:# <math>f(u,v) \\leftarrow 0</math> for all edges <math>(u,v)</math>\n:# While there is a path {{mvar|p}} from {{mvar|s}} to {{mvar|t}} in <math>G_f</math>, such that <math>c_f(u,v) > 0</math> for all edges <math>(u,v) \\in p</math>:\n:## Find <math>c_f(p) = \\min\\{c_f(u,v) : (u,v) \\in p\\}</math>\n:## For each edge <math>(u,v) \\in p</math>\n:### <math>f(u,v) \\leftarrow f(u,v) + c_f(p)</math> (''Send flow along the path'')\n:### <math>f(v,u) \\leftarrow f(v,u) - c_f(p)</math> (''The flow might be \"returned\" later'')\n{{Algorithm-end}}\n\nThe path in step 2 can be found with for example a [[breadth-first search]] (BFS) or a [[depth-first search]] in <math>G_f(V,E_f)</math>. If you use the former, the algorithm is called [[Edmonds–Karp algorithm|Edmonds–Karp]].\n\nWhen no more paths in step 2 can be found, {{mvar|s}} will not be able to reach {{mvar|t}} in the residual\nnetwork. If {{mvar|S}} is the set of nodes reachable by {{mvar|s}} in the residual network, then the total\ncapacity in the original network of edges from {{mvar|S}} to the remainder of {{mvar|V}} is on the one hand\nequal to the total flow we found from {{mvar|s}} to {{mvar|t}},\nand on the other hand serves as an upper bound for all such flows.\nThis proves that the flow we found is maximal. See also [[Max-flow min-cut theorem|Max-flow Min-cut theorem]].\n\nIf the graph <math>G(V,E)</math> has multiple sources and sinks, we act as follows:\nSuppose that <math>T=\\{t|t \\text{ is a sink}\\}</math> and <math>S=\\{s|s \\text{ is a source}\\}</math>. Add a new source <math> s^*</math> with an edge <math>(s^*,s)</math> from <math>s^* </math> to every node <math> s\\in S </math>, with capacity <math>c(s^*,s)=d_s\\;(d_s=\\sum_{(s,u)\\in E}c(s,u))</math>. And add a new sink <math> t^*</math> with an edge <math>(t, t^*)</math> from every node <math> t\\in T </math> to <math>t^* </math>, with capacity <math>c(t, t^*)=d_t\\;(d_t=\\sum_{(v,t)\\in E}c(v,t))</math>. Then apply the Ford&ndash;Fulkerson algorithm.\n\nAlso, if a node {{mvar|u}} has capacity constraint <math>d_u</math>, we replace this node with two nodes <math>u_{in},u_{out}</math>, and an edge <math> (u_{in},u_{out}) </math>, with capacity <math>c(u_{in},u_{out})=d_u</math>. Then apply the Ford&ndash;Fulkerson algorithm.\n\n==Complexity==\nBy adding the flow augmenting path to the flow already established in the graph, the maximum flow will be reached when no more flow augmenting paths can be found in the graph.  However, there is no certainty that this situation will ever be reached, so the best that can be guaranteed is that the answer will be correct if the algorithm terminates.  In the case that the algorithm runs forever, the flow might not even converge towards the maximum flow.  However, this situation only occurs with irrational flow values.  When the capacities are integers, the runtime of Ford–Fulkerson is bounded by <math>O(E f)</math> (see [[big O notation]]), where <math>E</math> is the number of edges in the graph and <math>f</math> is the maximum flow in the graph.  This is because each augmenting path can be found in <math>O(E)</math> time and increases the flow by an integer amount of at least <math>1</math>, with the upper bound <math>f</math>.\n\nA variation of the Ford&ndash;Fulkerson algorithm with guaranteed termination and a runtime independent of the maximum flow value is the [[Edmonds–Karp algorithm]], which runs in <math>O(VE^2)</math> time.\n\n==Integral example==\n\nThe following example shows the first steps of Ford–Fulkerson in a flow network with 4 nodes, source <math>A</math> and sink <math>D</math>. This example shows the worst-case behaviour of the algorithm. In each step, only a flow of <math>1</math> is sent across the network. If breadth-first-search were used instead, only two steps would be needed.\n\n{| cellpadding=\"10\"\n|- style=\"text-align:center\"\n! Path \n! Capacity\n! Resulting flow network\n|-\n| colspan=\"2\" style=\"text-align:center\" | Initial flow network\n| [[Image:Ford-Fulkerson example 0.svg|300px]]\n|-\n| <math>A,B,C,D</math>\n| <math>\n\\begin{align}\n  & \\min(c_f(A,B), c_f(B,C), c_f(C,D)) \\\\\n= & \\min(c(A,B)-f(A,B) ,c(B,C)-f(B,C), c(C,D)-f(C,D)) \\\\\n= & \\min(1000-0, 1-0, 1000-0) = 1\n\\end{align}\n</math>\n| [[Image:Ford-Fulkerson example 1.svg|300px]]\n|-\n| <math>A,C,B,D</math>\n| <math>\n\\begin{align}\n  & \\min(c_f(A,C), c_f(C,B), c_f(B,D)) \\\\\n= & \\min(c(A,C)-f(A,C), c(C,B)-f(C,B), c(B,D)-f(B,D)) \\\\\n= & \\min(1000-0, 0-(-1), 1000-0) = 1\n\\end{align}\n</math>\n| [[Image:Ford-Fulkerson example 2.svg|300px]]\n|-\n| colspan=\"3\" style=\"text-align:center\" | After 1998 more steps ...\n|-\n| colspan=\"2\" style=\"text-align:center\" | Final flow network\n| [[Image:Ford-Fulkerson example final.svg|300px]]\n|}\n\nNotice how flow is \"pushed back\" from <math>C</math> to <math>B</math> when finding the path <math>A,C,B,D</math>.\n\n==Non-terminating example==\n\n[[File:Ford-Fulkerson forever.svg|right]]\n\nConsider the flow network shown on the right, with source <math>s</math>, sink <math>t</math>, capacities of edges <math>e_1</math>, <math>e_2</math> and <math>e_3</math> respectively <math>1</math>, <math>r=(\\sqrt{5}-1)/2</math> and <math>1</math> and the capacity of all other edges some integer <math>M \\ge 2</math>. The constant <math>r</math> was chosen so, that <math>r^2 = 1 - r</math>. We use augmenting paths according to the following table, where <math>p_1 = \\{ s, v_4, v_3, v_2, v_1, t \\}</math>, <math>p_2 = \\{ s, v_2, v_3, v_4, t \\}</math> and <math>p_3 = \\{ s, v_1, v_2, v_3, t \\}</math>.\n\n{| class=\"wikitable\" style=\"text-align: center\"\n! rowspan=2 | Step !! rowspan=2 | Augmenting path !! rowspan=2 | Sent flow !! colspan=3 | Residual capacities\n|-\n! <math>e_1</math> !! <math>e_2</math> !! <math>e_3</math>\n|-\n| 0 || || || <math>r^0=1</math> || <math>r</math> || <math>1</math>\n|-\n| 1 || <math>\\{ s, v_2, v_3, t \\}</math> || <math>1</math> || <math>r^0</math> || <math>r^1</math> || <math>0</math>\n|-\n| 2 || <math>p_1</math> || <math>r^1</math> || <math>r^2</math> || <math>0</math> || <math>r^1</math>\n|-\n| 3 || <math>p_2</math> || <math>r^1</math> || <math>r^2</math> || <math>r^1</math> || <math>0</math>\n|-\n| 4 || <math>p_1</math> || <math>r^2</math> || <math>0</math> || <math>r^3</math> || <math>r^2</math>\n|-\n| 5 || <math>p_3</math> || <math>r^2</math> || <math>r^2</math> || <math>r^3</math> || <math>0</math>\n|}\n\nNote that after step 1 as well as after step 5, the residual capacities of edges <math>e_1</math>, <math>e_2</math> and <math>e_3</math> are in the form <math>r^n</math>, <math>r^{n+1}</math> and <math>0</math>, respectively, for some <math>n \\in \\mathbb{N}</math>. This means that we can use augmenting paths <math>p_1</math>, <math>p_2</math>, <math>p_1</math> and <math>p_3</math> infinitely many times and residual capacities of these edges will always be in the same form. Total flow in the network after step 5 is <math>1 + 2(r^1 + r^2)</math>. If we continue to use augmenting paths as above, the total flow converges to <math>\\textstyle 1 + 2\\sum_{i=1}^\\infty r^i = 3 + 2r</math>, while the maximum flow is <math>2M + 1</math>. In this case, the algorithm never terminates and the flow doesn't even converge to the maximum flow.<ref>{{cite journal| title = The smallest networks on which the Ford–Fulkerson maximum flow procedure may fail to terminate | first = Uri | last = Zwick|authorlink=Uri Zwick | journal = [[Theoretical Computer Science (journal)|Theoretical Computer Science]] | volume = 148 | issue = 1 | date = 21 August 1995 | pages = 165–170 | doi = 10.1016/0304-3975(95)00022-O}}</ref>\n\n{{clear}}\n\n==Python implementation of [[Edmonds–Karp]] algorithm==\n  \n<syntaxhighlight lang=\"python\">\n\nimport collections\n \n# This class represents a directed graph using adjacency matrix representation\nclass Graph:\n  \n    def __init__(self,graph):\n        self.graph = graph  # residual graph\n        self.ROW = len(graph)\n  \n    def BFS(self, s, t, parent):\n        '''Returns true if there is a path from source 's' to sink 't' in\n        residual graph. Also fills parent[] to store the path '''\n\n        # Mark all the vertices as not visited\n        visited = [False] * (self.ROW)\n         \n        # Create a queue for BFS\n        queue = collections.deque()\n         \n        # Mark the source node as visited and enqueue it\n        queue.append(s)\n        visited[s] = True\n         \n        # Standard BFS Loop\n        while queue:\n            u = queue.popleft()\n         \n            # Get all adjacent vertices's of the dequeued vertex u\n            # If an adjacent has not been visited, then mark it\n            # visited and enqueue it\n            for ind, val in enumerate(self.graph[u]):\n                if (visited[ind] == False) and (val > 0):\n                    queue.append(ind)\n                    visited[ind] = True\n                    parent[ind] = u\n \n        # If we reached sink in BFS starting from source, then return\n        # true, else false\n        return visited[t]\n             \n    # Returns the maximum flow from s to t in the given graph\n    def EdmondsKarp(self, source, sink):\n \n        # This array is filled by BFS and to store path\n        parent = [-1] * (self.ROW)\n \n        max_flow = 0 # There is no flow initially\n \n        # Augment the flow while there is path from source to sink\n        while self.BFS(source, sink, parent):\n \n            # Find minimum residual capacity of the edges along the\n            # path filled by BFS. Or we can say find the maximum flow\n            # through the path found.\n            path_flow = float(\"Inf\")\n            s = sink\n            while s != source:\n                path_flow = min(path_flow, self.graph[parent[s]][s])\n                s = parent[s]\n \n            # Add path flow to overall flow\n            max_flow += path_flow\n \n            # update residual capacities of the edges and reverse edges\n            # along the path\n            v = sink\n            while v !=  source:\n                u = parent[v]\n                self.graph[u][v] -= path_flow\n                self.graph[v][u] += path_flow\n                v = parent[v]\n \n        return max_flow\n</syntaxhighlight>\n\n== Notes ==\n{{reflist}}\n\n== References ==\n* {{cite book\n | first1 = Thomas H.\n | last1 = Cormen\n | authorlink1 = Thomas H. Cormen\n | first2 = Charles E.\n | last2 = Leiserson\n | authorlink2 = Charles E. Leiserson\n | first3 = Ronald L.\n | last3 = Rivest\n | authorlink3 = Ronald L. Rivest\n | first4 = Clifford\n | last4 = Stein\n | authorlink4 = Clifford Stein\n | title = [[Introduction to Algorithms]]\n | edition = Second\n | publisher = MIT Press and McGraw&ndash;Hill\n | year = 2001 \n | isbn = 0-262-03293-7\n | chapter = Section 26.2: The Ford&ndash;Fulkerson method\n | pages = 651&ndash;664\n}}\n* {{cite book |author1=George T. Heineman |author2=Gary Pollice |author3=Stanley Selkow | title= Algorithms in a Nutshell | publisher=[[Oreilly Media]] | year=2008 | chapter=Chapter 8:Network Flow Algorithms | pages = 226–250 | isbn=978-0-596-51624-6 }}\n* {{cite book |author1=Jon Kleinberg |author2=Éva Tardos | title= Algorithm Design | publisher=\nPearson Education | year=2006 | chapter=Chapter 7:Extensions to the Maximum-Flow Problem | pages = 378–384 | isbn=0-321-29535-8  }}\n* {{cite book |author=Samuel Gutekunst |title= ENGRI 1101 |publisher= Cornell University | year=2009 }}\n\n== See also ==\n\n* [[Approximate max-flow min-cut theorem]]\n* [[Turn restriction routing]]\n\n==External links==\n* [http://community.topcoder.com/tc?module=Static&d1=tutorials&d2=maxFlow A tutorial explaining the Ford–Fulkerson method to solve the max-flow problem]\n* [http://www.cs.pitt.edu/~kirk/cs1501/animations/Network.html Another Java animation]\n* [http://rrusin.blogspot.com/2011/03/implementing-graph-editor-in-javafx.html Java Web Start application]\n\n{{commonscat-inline}}\n\n{{DEFAULTSORT:Ford-Fulkerson Algorithm}}\n[[Category:Network flow problem]]\n[[Category:Articles with example pseudocode]]\n[[Category:Graph algorithms]]"
    },
    {
      "title": "Maximum flow problem",
      "url": "https://en.wikipedia.org/wiki/Maximum_flow_problem",
      "text": "[[File:Max flow.svg|thumb|upright=1.5|A network with an example of maximum flow. The source is ''s'', and the sink ''t''. The numbers denote flow and capacity.]]\nIn [[Optimization (mathematics)|optimization theory]], '''maximum flow problems''' involve finding a feasible flow through a [[flow network]] that is maximum.\n\nThe maximum flow problem can be seen as a special case of more complex network flow problems, such as the [[circulation problem]]. The maximum value of an s-t flow (i.e., flow from [[Glossary of graph theory#Direction|source]] s to [[Glossary of graph theory#Direction|sink]] t) is equal to the minimum capacity of an [[Cut (graph theory)|s-t cut]] (i.e., cut severing s from t) in the network, as stated in the [[max-flow min-cut theorem]].\n\n==History==\nThe maximum flow problem was first formulated in 1954 by [[Ted Harris (mathematician)|T. E. Harris]] and F. S. Ross as a simplified model of Soviet railway traffic flow.<ref>{{Cite journal | last1 = Schrijver | first1 = A. | title = On the history of the transportation and maximum flow problems | doi = 10.1007/s101070100259 | journal = Mathematical Programming | volume = 91 | issue = 3 | pages = 437–445 | year = 2002 | pmid =  | pmc = | citeseerx = 10.1.1.23.5134 }}</ref><ref>{{Cite book | doi = 10.1007/0-387-25837-X_5 | first1 =  Saul I. | last1 = Gass| first2 = Arjang A. | last2 = Assad | chapter = Mathematical, algorithmic and professional developments of operations research from 1951 to 1956 | title = An Annotated Timeline of Operations Research | series = International Series in Operations Research & Management Science | volume = 75 | pages = 79–110 | year = 2005 | isbn = 978-1-4020-8116-3 | pmid =  | pmc = }}</ref><ref>{{cite journal | first1 = T. E. | last1 = Harris | authorlink1 = Ted Harris (mathematician) | first2 = F. S. | last2 = Ross | year = 1955 | title = Fundamentals of a Method for Evaluating Rail Net Capacities | journal = Research Memorandum| url = http://www.dtic.mil/dtic/tr/fulltext/u2/093458.pdf}}</ref>\nIn 1955, [[Lester R. Ford, Jr.]] and [[D. R. Fulkerson|Delbert R. Fulkerson]] created the first known algorithm, the [[Ford–Fulkerson algorithm]].<ref>{{Cite journal | last1 = Ford | first1 = L. R. | authorlink1 = L. R. Ford, Jr.| last2 = Fulkerson | first2 = D. R. | authorlink2 = D. R. Fulkerson| doi = 10.4153/CJM-1956-045-5 | title = Maximal flow through a network | journal = [[Canadian Journal of Mathematics]]| volume = 8 | pages = 399–404 | year = 1956 | pmid =  | pmc = }}</ref><ref>Ford, L.R., Jr.; Fulkerson, D.R., ''Flows in Networks'', Princeton University Press (1962).</ref>\n\nOver the years, various improved solutions to the maximum flow problem were discovered, notably the shortest augmenting path algorithm of Edmonds and Karp and independently Dinitz; the blocking flow algorithm of Dinitz; the [[Push–relabel maximum flow algorithm|push-relabel algorithm]] of [[Andrew V. Goldberg|Goldberg]] and [[Robert Tarjan|Tarjan]]; and the binary blocking flow algorithm of Goldberg and Rao. The algorithms of Sherman<ref>{{Cite book | last = Sherman | first = Jonah | chapter = Nearly Maximum Flows in Nearly Linear Time | doi = 10.1109/FOCS.2013.36 | title = Proceedings of the 54th Annual IEEE Symposium on Foundations of Computer Science | pages = 263–269 | year = 2013 | arxiv = 1304.2077 | pmid =  | pmc = | isbn = 978-0-7695-5135-7 }}</ref> and Kelner, Lee, Orecchia and Sidford<ref>{{Cite book | last1 = Kelner | first1 = J. A. | last2 = Lee | first2 = Y. T. | last3 = Orecchia | first3 = L. | last4 = Sidford | first4 = A. | chapter = An Almost-Linear-Time Algorithm for Approximate Max Flow in Undirected Graphs, and its Multicommodity Generalizations | doi = 10.1137/1.9781611973402.16 | title = Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms | pages = 217 | year = 2014 | isbn = 978-1-61197-338-9 | chapter-url = http://math.mit.edu/~kelner/Publications/Docs/klos_maxflow_main.pdf | arxiv = 1304.2338 | pmid =  | pmc =  | deadurl = yes | archiveurl = https://web.archive.org/web/20160303170302/http://math.mit.edu/~kelner/Publications/Docs/klos_maxflow_main.pdf | archivedate = 2016-03-03 | df =  }}</ref><ref>{{cite web | url = http://web.mit.edu/newsoffice/2013/new-algorithm-can-dramatically-streamline-solutions-to-the-max-flow-problem-0107.html | title = New algorithm can dramatically streamline solutions to the 'max flow' problem | first = Helen | last = Knight | date = 7 January 2014 | accessdate = 8 January 2014 | publisher = MIT News}}</ref>, respectively, find an approximately optimal maximum flow but only work in undirected graphs.\n\nIn 2013 [[James B. Orlin]] published a paper describing an <math>O(|V| |E|)</math>algorithm for all values of <math>|V|</math>and <math>|E|</math>.<ref name=\"orlin\" />\n\n==Definition==\n[[File:MFP1.jpg|thumb|upright=0.8|A flow network, with source s and sink t. The numbers next to the edge are the capacities.]]\nLet <math>N = (V, E)</math> be a network with <math>s, t \\in V</math> being the source and the sink of <math>N</math> respectively.\n\n: The '''capacity''' of an edge is a mapping <math>c : E \\to \\mathbb{R}^+</math>, denoted by <math>c_{uv}</math> or <math>c(u, v)</math>. It represents the maximum amount of flow that can pass through an edge.\n\n: A '''flow''' is a mapping <math>f : E \\to \\mathbb{R}^+</math>, denoted by <math>f_{uv}</math> or <math>f(u, v)</math>, subject to the following two constraints:\n:# <math>f_{uv} \\leq c_{uv}</math>, for each <math>(u, v) \\in E</math> (capacity constraint: the flow of an edge cannot exceed its capacity);\n:# <math>\\sum_{u:(u, v) \\in E} f_{uv} = \\sum_{u:(v, u) \\in E} f_{vu}</math>, for each <math>v \\in V \\setminus \\{s, t\\}</math> (conservation of flows: the sum of the flows entering a node must equal the sum of the flows exiting a node, except for the source and the sink nodes).\n\n: The '''value of flow''' is defined by <math>|f| = \\sum_{v:(s,v) \\in E} f_{sv}</math>, where <math>s</math> is the source of <math>N</math>. It represents the amount of flow passing from the source to the sink.\n\nThe '''maximum flow problem''' is to maximize <math>|f|</math>, that is, to route as much flow as possible from <math>s</math> to <math>t</math>.\n\n==Solutions==\nThe following table lists algorithms for solving the maximum flow problem.\n\n{| class=\"wikitable\"\n! Method\n! Complexity\n! Description\n|-\n| [[Linear programming]]\n|\n| Constraints given by the definition of a [[flow network|legal flow]]. See the [[Max-flow min-cut theorem#Linear program formulation|linear program]] here.\n|-\n| [[Ford–Fulkerson algorithm]]\n| {{math|''O''(''E'' max<nowiki>|</nowiki> ''f'' <nowiki>|</nowiki>)}}\n| As long as there is an open path through the residual graph, send the minimum of the residual capacities on the path.\nThe algorithm is only guaranteed to terminate if all weights are [[rational numbers|rational]]. Otherwise it is possible that the algorithm will not converge to the maximum value. However, if the algorithm terminates, it is guaranteed to find the maximum value.\n|-\n| [[Edmonds–Karp algorithm]]\n|  {{math|''O''(''VE''<sup>2</sup>)}}\n| A specialization of Ford–Fulkerson, finding augmenting paths with [[breadth-first search]].\n|-\n| [[Dinic's algorithm|Dinic's blocking flow algorithm]]\n|  {{math|''O''(''V''<sup>2</sup>''E'')}}\n| In each phase the algorithms builds a layered graph with [[breadth-first search]] on the [[residual graph]]. The maximum flow in a layered graph can be calculated in {{math|''O''(''VE'')}} time, and the maximum number of the phases is {{math|''n''-1}}. In networks with unit capacities, Dinic's algorithm terminates in <math>O(\\min\\{V^{2/3}, E^{1/2}\\}E)</math> time.\n|-\n|MPM (Malhotra, Pramodh-Kumar and Maheshwari) algorithm<ref>{{Cite journal | last1 = Malhotra | first1 =V.M.| last2 = Kumar | first2 = M.Pramodh| last3 = Maheshwari | first3 = S.N.| doi = 10.1016/0020-0190(78)90016-9| title = An <math>O(|V|^3)</math> algorithm for finding maximum flows in networks| journal = Information Processing Letters| volume = 7| issue = 6| pages = 277–278 | year = 1978}}</ref>\n| {{math|''O''(''V''<sup>3</sup>)}}\n| Only works on acyclic networks. Refer to the [https://dx.doi.org/10.1016/0020-0190(78)90016-9 Original Paper].\n|-\n| [[Dinic's algorithm]]\n| {{math|''O''(''VE'' log(''V''))}}\n| The [[dynamic trees]] data structure speeds up the maximum flow computation in the layered graph to {{math|''O''(V ''E'' log(''V''))}}.\n|-\n| [[Relabel-to-front algorithm|General push-relabel maximum flow algorithm]]\n| {{math|''O''(''V''<sup>2</sup>''E'')}}\n| The push relabel algorithm maintains a preflow, i.e. a flow function with the possibility of excess in the vertices. The algorithm runs while there is a vertex with positive excess, i.e. an active vertex in the graph. The push operation increases the flow on a residual edge, and a height function on the vertices controls which residual edges can be pushed. The height function is changed with a relabel operation. The proper definitions of these operations guarantee that the resulting flow function is a maximum flow.\n|-\n| [[Relabel-to-front algorithm|Push-relabel algorithm with ''FIFO'' vertex selection rule]]\n| {{math|''O''(''V''<sup>3</sup>)}}\n| Push-relabel algorithm variant which always selects the most recently active vertex, and performs push operations until the excess is positive or there are admissible residual edges from this vertex.\n|-\n| [[Relabel-to-front algorithm|Push-relabel algorithm with dynamic trees]]\n| <math>O\\left(VE \\log \\frac {V^2} E \\right)</math>\n| The algorithm builds limited size trees on the residual graph regarding to height function. These trees provide multilevel push operations.\n|-\n|KRT (King, Rao, Tarjan)'s algorithm<ref>{{Cite journal | last1 = King | first1 = V.| last2 = Rao | first2 = S.| last3 = Tarjan | first3 = R.| doi = 10.1006/jagm.1994.1044| title = A Faster Deterministic Maximum Flow Algorithm| journal = Journal of Algorithms| volume = 17| issue = 3| pages = 447–474| year = 1994}}</ref>\n| <math>O(EV \\log_{\\frac E {V \\log V}}V)</math>\n|\n|-\n|Binary blocking flow algorithm<ref>{{Cite journal | last1 = Goldberg | first1 = A. V. | authorlink1 = Andrew V. Goldberg| last2 = Rao | first2 = S. | doi = 10.1145/290179.290181 | title = Beyond the flow decomposition barrier | journal = [[Journal of the ACM]]| volume = 45 | issue = 5 | pages = 783 | year = 1998 | pmid =  | pmc = }}</ref>\n| <math>O\\left(E \\cdot \\min(V^{\\frac 2 3},\\sqrt{E}) \\cdot \\log \\frac {V^2} E \\log{U}\\right)</math>\n|The value ''U'' corresponds to the maximum capacity of the network.\n|-\n|James B Orlin's + KRT (King, Rao, Tarjan)'s algorithm<ref name=\"orlin\">{{Cite book | last1 = Orlin | first1 = James B.| doi = 10.1145/2488608.2488705| title = Max flows in O(nm) time, or better| journal = STOC '13 Proceedings of the Forty-fifth Annual ACM Symposium on Theory of Computing| pages = 765–774| year = 2013| isbn = 9781450320290| citeseerx = 10.1.1.259.5759}}</ref>\n| <math>O(VE)</math>\n|[http://jorlin.scripts.mit.edu/Max_flows_in_O(nm)_time.html Orlin's algorithm] solves max-flow in {{math|''O''(''VE'')}} time for <math>E \\leq O(V^{{16 \\over 15} - \\epsilon})</math> while KRT solves it in {{math|''O''(''VE'')}} for <math>E > V^{1+\\epsilon}</math>.\n|}\n\nFor a more extensive list, see.<ref>{{Cite journal | last1 = Goldberg | first1 = A. V. | authorlink1 = Andrew V. Goldberg| last2 = Tarjan | first2 = R. E. | doi = 10.1145/48014.61051 | title = A new approach to the maximum-flow problem | journal = [[Journal of the ACM]]| volume = 35 | issue = 4 | pages = 921 | year = 1988 | pmid =  | pmc = }}</ref>\n\n==Integral flow theorem==\nThe integral flow theorem states that\n: '''If each edge in a flow network has integral capacity, then there exists an integral maximal flow.'''\n\n==Application==\n\n===Multi-source multi-sink maximum flow problem===\n[[File:Multi-source multi-sink flow problem.svg|thumb|right|Fig. 4.1.1. Transformation of a multi-source multi-sink maximum flow problem into a single-source single-sink maximum flow problem]]\nGiven a network <math>N = (V, E)</math> with a set of sources <math>S = \\{s_1, \\ldots, s_n\\}</math> and a set of sinks <math>T = \\{t_1, \\ldots, t_m\\}</math> instead of only one source and one sink, we are to find the maximum flow across <math>N</math>. We can transform the multi-source multi-sink problem into a maximum flow problem by adding a ''consolidated source'' connecting to each vertex in <math>S</math> and a ''consolidated sink ''connected by each vertex in <math>T</math> (also known as ''supersource'' and ''supersink'') with infinite capacity on each edge (See Fig. 4.1.1.).\n\n===Minimum path cover in directed acyclic graph===\nGiven a [[directed acyclic graph]] <math>G = (V, E)</math>, we are to find the minimum number of [[Path (graph theory)|vertex-disjoint paths]] to cover each vertex in <math>V</math>. We can construct a bipartite graph <math>G' = (V_{out} \\cup V_{in}, E')</math> from <math>G</math>, where\n# <math>V_{out} = \\{v \\in V : v \\text{ has positive out-degree}\\}</math>.\n# <math>V_{in} = \\{v \\in V : v \\text{ has positive in-degree}\\}</math>.\n# <math>E' = \\{(u,v) \\in V_{out} \\times V_{in} : (u,v) \\in E \\}</math>.\nThen it can be shown, via [[Kőnig's theorem (graph theory)|Kőnig's theorem]], that <math>G'</math> has a matching of size <math>m</math> if and only if there exists <math>n-m</math> vertex-disjoint paths that cover each vertex in <math>G</math>, where <math>n</math> is the number of vertices in <math>G</math>. Therefore, the problem can be solved by finding the maximum cardinality matching in <math>G'</math> instead.\n\n===Maximum cardinality bipartite matching===\n[[File:Maximum bipartite matching to max flow.svg|thumb|right|Fig. 4.3.1. Transformation of a maximum bipartite matching problem into a maximum flow problem]]\nGiven a [[bipartite graph]] <math>G = (X \\cup Y, E)</math>, we are to find a [[maximum cardinality matching]] in <math>G</math>, that is a matching that contains the largest possible number of edges. This problem can be transformed into a maximum flow problem by constructing a network <math>N = (X \\cup Y \\cup \\{s,t\\}, E')</math>, where\n# <math>E'</math> contains the edges in <math>G</math> directed from <math>X</math> to <math>Y</math>.\n# <math>(s,x) \\in E'</math> for each <math>x \\in X</math> and <math>(y,t) \\in E'</math> for each <math>y \\in Y</math>.\n# <math>c(e) = 1</math> for each <math>e \\in E'</math> (See Fig. 4.3.1).\n\nThen the value of the maximum flow in <math>N</math> is equal to the size of the maximum matching in <math>G</math>.\n\n===Maximum flow with vertex capacities===\n[[File:Node splitting.svg|thumb|right|Fig. 4.4.1. Transformation of a maximum flow problem with vertex capacities constraint into the original maximum flow problem by node splitting]]\nGiven a network <math>N = (V, E)</math>, in which there is capacity at each node in addition to edge capacity, that is, a mapping <math>c: V\\mapsto \\mathbb{R}^{+}</math>, denoted by <math>c(v)</math>, such that the flow <math>f</math> has to satisfy not only the capacity constraint and the conservation of flows, but also the vertex capacity constraint\n<center><math> \\sum_{i\\in V} f_{iv} \\le c(v) \\qquad \\forall v \\in V \\backslash \\{s,t\\}</math>.</center>\nIn other words, the amount of flow passing through a vertex cannot exceed its capacity. To find the maximum flow across <math>N</math>, we can transform the problem into the maximum flow problem in the original sense by expanding <math>N</math>. First, each <math>v\\in V</math> is replaced by <math>v_{\\text{in}}</math> and <math>v_{\\text{out}}</math>, where <math>v_{\\text{in}}</math> is connected by edges going into <math>v</math> and <math>v_{\\text{out}}</math> is connected to edges coming out from <math>v</math>, then assign capacity <math>c(v)</math> to the edge connecting <math>v_{\\text{in}}</math> and <math>v_{\\text{out}}</math> (see Fig. 4.4.1). In this expanded network, the vertex capacity constraint is removed and therefore the problem can be treated as the original maximum flow problem.\n\n===Maximum number of paths from s to t===\nGiven a directed graph <math>G = (V, E)</math> and two vertices <math>s</math> and <math>t</math>, we are to find the maximum number of paths from <math>s</math> to <math>t</math>. This problem has several variants:\n\n1. The paths must be edge-disjoint. This problem can be transformed to a maximum flow problem by constructing a network <math>N = (V, E)</math> from <math>G</math>, with <math>s</math> and <math>t</math> being the source and the sink of <math>N</math> respectively, and assigning each edge a capacity of <math>1</math>. In this network, the maximum flow is <math>k</math> iff there are <math>k</math> edge-disjoint paths.\n\n2. The paths must be independent, i.e., vertex-disjoint (except for <math>s</math> and <math>t</math>). We can construct a network <math>N = (V, E)</math> from <math>G</math> with vertex capacities, where the capacities of all vertices and all edges are <math>1</math>. Then the value of the maximum flow is equal to the maximum number of independent paths from <math>s</math> to <math>t</math>.\n\n3. In addition to the paths being edge-disjoint and/or vertex disjoint, the paths also have a length constraint: we count only paths whose length is exactly <math>k</math>, or at most <math>k</math>. Most variants of this problem are NP-complete, except for small values of <math>k</math>.<ref>{{Cite journal|last=Itai|first=A.|last2=Perl|first2=Y.|last3=Shiloach|first3=Y.|date=1982|title=The complexity of finding maximum disjoint paths with length constraints|journal=Networks|language=en|volume=12|issue=3|pages=277–286|doi=10.1002/net.3230120306|issn=1097-0037}}</ref>\n\n=== Closure problem ===\n{{Main|Closure problem}}\nA '''closure''' of a directed graph is a set of vertices with no outgoing edges. That is, the graph should have no edges that start within the closure and end outside the closure. The '''closure problem''' is the task of finding the maximum-weight or minimum-weight closure in a vertex-weighted directed graph. It may be solved in polynomial time using a reduction to the maximum flow problem.\n\n==Real world applications==\n\n===Baseball elimination===\n[[File:Baseball Elimination Problem.png|thumb|Construction of network flow for baseball elimination problem]]\nIn the [[baseball]] elimination problem there are ''n'' teams competing in a league. At a specific stage of the league season, ''w''<sub>i</sub> is the number of wins and ''r''<sub>i</sub> is the number of games left to play for team ''i'' and ''r''<sub>ij</sub> is the number of games left against team ''j''. A team is eliminated if it has no chance to finish the season in the first place. The task of the baseball elimination problem is to determine which teams are eliminated at each point during the season. Schwartz<ref>{{Cite journal | last1 = Schwartz | first1 = B. L. | title = Possible Winners in Partially Completed Tournaments | doi = 10.1137/1008062 | journal = [[SIAM Review]]| jstor = 2028206| volume = 8 | issue = 3 | pages = 302–308 | year = 1966 | pmid =  | pmc = }}</ref> proposed a method which reduces this problem to maximum network flow. In this method a network is created to determine whether team ''k'' is eliminated.\n\nLet ''G'' = (''V'', ''E'') be a network with ''s'',''t'' ∈ ''V'' being the source and the sink respectively. One adds a game node {''i'',''j''} with ''i'' < ''j'' to ''V'', and connects each of them from ''s'' by an edge with capacity ''r''<sub>ij</sub> – which represents the number of plays between these two teams. We also add a team node for each team and connect each game node {''i'',''j''} with two team nodes ''i'' and ''j'' to ensure one of them wins. One does not need to restrict the flow value on these edges. Finally, edges are made from team node ''i'' to the sink node ''t'' and the capacity of ''w''<sub>k</sub>+''r''<sub>k</sub>–''w''<sub>i</sub> is set to prevent team ''i'' from winning more than ''w''<sub>k</sub>+''r''<sub>k</sub>.\nLet ''S'' be the set of all teams participating in the league and let <math>\\scriptstyle r(S - \\{k\\}) = \\sum_{i,j \\in \\{S-\\{k\\}\\}, i < j} r_{ij}</math>. In this method it is claimed team ''k'' is not eliminated if and only if a flow value of size ''r''(''S'' − {''k''}) exists in network ''G''. In the mentioned article it is proved that this flow value is the maximum flow value from ''s'' to ''t''.\n\n===Airline scheduling===\nIn the airline industry a major problem is the scheduling of the flight crews. The airline scheduling problem can be considered as an application of extended maximum network flow. The input of this problem is a set of flights ''F'' which contains the information about where and when each flight departs and arrives. In one version of airline scheduling the goal is to produce a feasible schedule with at most ''k'' crews.\n\nIn order to solve this problem one uses a variation of the [[circulation problem]] called bounded circulation which is the generalization of [[flow network|network flow]] problems, with the added constraint of a lower bound on edge flows.\n\nLet ''G'' = (''V'', ''E'') be a network with ''s'',''t'' ∈ ''V'' as the source and the sink nodes. For the source and destination of every flight ''i'', one adds two nodes to ''V'', node ''s''<sub>''i''</sub> as the source and node ''d''<sub>''i''</sub> as the destination node of flight ''i''. One also adds the following edges to ''E'':\n# An edge with capacity [0, 1] between ''s'' and each ''s''<sub>''i''</sub>.\n# An edge with capacity [0, 1] between each ''d''<sub>''i''</sub> and ''t''.\n# An edge with capacity [1, 1] between each pair of ''s''<sub>''i''</sub> and ''d''<sub>''i''</sub>.\n# An edge with capacity [0, 1] between each ''d''<sub>''i''</sub> and ''s''<sub>''j''</sub>, if source ''s''<sub>''j''</sub> is reachable with a reasonable amount of time and cost from the destination of flight ''i''.\n# An edge with capacity [0, [[∞]]] between ''s'' and ''t''.\n\nIn the mentioned method, it is claimed and proved that finding a flow value of ''k'' in ''G'' between ''s'' and ''t'' is equal to finding a feasible schedule for flight set ''F'' with at most ''k'' crews.<ref name=\"ITA\">{{cite book | author = [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]] | title=Introduction to Algorithms, Second Edition | chapter = 26. Maximum Flow | year = 2001 | publisher = MIT Press and McGraw-Hill | isbn = 978-0-262-03293-3 | pages=643–668| title-link=Introduction to Algorithms }}</ref>\n\nAnother version of airline scheduling is finding the minimum needed crews to perform all the flights. In order to find an answer to this problem, a bipartite graph {{math|1=''G''' = (''A'' ∪ ''B'', ''E'')}} is created where each flight has a copy in set ''A'' and set ''B''. If the same plane can perform flight ''j'' after flight ''i'', ''i''∈''A'' is connected to ''j''∈''B''. A matching in {{mvar|G'}} induces a schedule for ''F'' and obviously maximum bipartite matching in this graph produces an airline schedule with minimum number of crews.<ref name=\"ITA\"/> As it is mentioned in the Application part of this article, the maximum cardinality bipartite matching is an application of maximum flow problem.\n\n===Circulation–demand problem===\nThere are some factories that produce goods and some villages where the goods have to be delivered. They are connected by a networks of roads with each road having a capacity {{mvar|c}} for maximum goods that can flow through it. The problem is to find if there is a circulation that satisfies the demand.\nThis problem can be transformed into a maximum-flow problem.\n# Add a source node {{mvar|s}} and add edges from it to every factory node {{mvar|f<sub>i</sub>}} with capacity {{mvar|p<sub>i</sub>}} where {{mvar|p<sub>i</sub>}} is the production rate of factory {{mvar|f<sub>i</sub>}}.\n# Add a sink node {{mvar|t}} and add edges from all villages {{mvar|v<sub>i</sub>}} to {{mvar|t}} with capacity {{mvar|d<sub>i</sub>}} where {{mvar|d<sub>i</sub>}} is the demand rate of village {{mvar|v<sub>i</sub>}}.\nLet ''G'' = (''V'', ''E'') be this new network. There exists a circulation that satisfies the demand if and only if :\n: {{math|Maximum flow value(''G'')}} <math> = \\sum_{i \\in v} d_i </math>.\nIf there exists a circulation, looking at the max-flow solution would give the answer as to how much goods have to be sent on a particular road for satisfying the demands.\n\nThe problem can be extended by adding a lower bound on the flow on some edges.<ref>{{Cite web|url=https://www.cs.cmu.edu/~ckingsf/bioinfo-lectures/flowext.pdf|title=Max-flow extensions: circulations with demands|last=Carl Kingsford|date=|website=|archive-url=|archive-date=|dead-url=|access-date=}}</ref>\n\n== Extensions ==\n1. In the '''[[minimum-cost flow problem]]''', each edge (''u'',v) also has a '''cost-coefficient''' ''a<sub>uv</sub>'' in addition to its capacity. If the flow through the edge is ''f<sub>uv</sub>'', then the total cost is ''a<sub>uv</sub>f<sub>uv</sub>.''  It is required to find a flow of a given size ''d'', with a smallest cost. In most variants, the cost-coefficients may be either positive or negative. There are various polynomial-time algorithms for this problem.\n\n2. The maximum-flow problem can be augmented by '''disjunctive constraints''': a ''negative disjunctive constraint'' says that a certain pair of edges cannot simultaneously have a nonzero flow; a ''positive disjunctive constraints'' says that, in a certain pair of edges, at least one must have a nonzero flow. With negative constraints, the problem becomes [[strongly NP-hard]] even for simple networks. With positive constraints, the problem is polynomial if fractional flows are allowed, but may be [[strongly NP-hard]] when the flows must be integral.<ref>{{Cite journal|last=Schauer|first=Joachim|last2=Pferschy|first2=Ulrich|date=2013-07-01|title=The maximum flow problem with disjunctive constraints|journal=Journal of Combinatorial Optimization|language=en|volume=26|issue=1|pages=109–119|doi=10.1007/s10878-011-9438-7|issn=1382-6905|citeseerx=10.1.1.414.4496}}</ref> \n\n== References ==\n{{reflist}}\n\n==Further reading==\n* {{cite journal\n | author = [[Joseph Cheriyan]] and [[Kurt Mehlhorn]]\n | title = An analysis of the highest-level selection rule in the preflow-push max-flow algorithm\n | journal = Information Processing Letters\n | volume = 69\n | year = 1999\n | pages = 239&ndash;242\n | doi = 10.1016/S0020-0190(99)00019-8\n | issue = 5\n| citeseerx = 10.1.1.42.8563\n }}\n* {{cite journal\n | author = [[Daniel D. Sleator]] and [[Robert E. Tarjan]]\n | title = A data structure for dynamic trees\n | journal = Journal of Computer and System Sciences\n | volume = 26\n | year = 1983\n | issn = 0022-0000\n | pages = 362&ndash;391\n | doi = 10.1016/0022-0000(83)90006-5\n | url = http://www.cs.cmu.edu/~sleator/papers/dynamic-trees.pdf\n | issue = 3\n}}\n* {{cite book | author =  Eugene Lawler | authorlink = Eugene Lawler | title = Combinatorial Optimization: Networks and Matroids | chapter = 4. Network Flows | year = 2001 | publisher = Dover | isbn = 978-0-486-41453-9 | pages = 109–177}}\n\n{{DEFAULTSORT:Maximum Flow Problem}}\n[[Category:Network flow problem]]\n[[Category:Computational problems in graph theory]]"
    },
    {
      "title": "Minimum cut",
      "url": "https://en.wikipedia.org/wiki/Minimum_cut",
      "text": "[[File:Min cut example.svg|thumb|A graph and two of its cuts. The dotted line in red represents a cut with three crossing edges. The dashed line in green represents one of the minimum cuts of this graph, crossing only two edges.<ref>{{Cite web|url = http://www.cs.dartmouth.edu/~ac/Teach/CS105-Winter05/Notes/loomis-scribe.ps|title = 4 Min-Cut Algorithms|date = |accessdate = |website = |publisher = |last = |first = }}</ref>]]\nIn [[graph theory]], a minimum cut of a [[Graph (discrete mathematics)|graph]] is a [[Cut (graph theory)|cut]] (a [[Partition of a set|partition]] of the vertices of a graph into two disjoint subsets that are joined by at least one edge) that is minimal in some sense.\n\nVariations of the minimum cut problem consider weighted graphs, directed graphs, terminals, and partitioning the vertices into more than two sets.\n\n__TOC__\n\n==Without Terminals==\nThe minimum cut problem in [[undirected graph|undirected]], weighted graphs can be solved in polynomial time by the [[Stoer-Wagner algorithm]]. In the special case when the graph is unweighted, [[Karger's algorithm]] provides an efficient randomized method for finding the cut. In this case, the minimum cut equals the [[k-edge-connected graph|edge connectivity]] of the graph.\n\nA generalization of the minimum cut problem without terminals is the [[Minimum k-cut|minimum {{mvar|k}}-cut]], in which the goal is to partition the graph into at least {{mvar|k}} connected components by removing as few edges as possible. For a fixed value of {{mvar|k}}, this problem can be solved in polynomial time, though the algorithm is not practical for large {{mvar|k}}. <ref>{{cite journal |title=A Polynomial Algorithm for the k-cut Problem for Fixed k |url=https://pubsonline.informs.org/doi/pdf/10.1287/moor.19.1.24}}</ref>\n\n==With Terminals==\nWhen two terminals are given, they are typically referred to as a source and sink. In a directed, weighted [[flow network]], the minimum cut separates the source and sink vertices and minimizes the total weight on the edges that are directed from the source side of the cut to the sink side of the cut. As shown in the [[max-flow min-cut theorem]], the weight of this cut equals the maximum amount of flow that can be sent from the source to the sink in the given network.\n\nIn a weighted, undirected network, it is possible to calculate the cut that separates a particular pair of vertices from each other and has minimum possible weight. A system of cuts that solves this problem for every possible vertex pair can be collected into a structure known as the [[Gomory–Hu tree]] of the graph.\n\nA generalization of the minimum cut problem with terminals is the {{mvar|k}}-terminal cut, or multiterminal cut. This problem is [[NP-hardness|NP-hard]], even for <math>k=3</math>.<ref>{{cite journal |title=The Complexity of Multiterminal Cuts |url=https://pdfs.semanticscholar.org/17ff/d84480267785c6a9987211a8a86a58cea1a9.pdf}}</ref>\n\n==Applications==\n[[Graph partition]] problems are a family of combinatorial optimization problems in which a graph is to be partitioned into two or more parts with additional constraints such as balancing the sizes of the two sides of the cut.\n\n==Number of minimum cuts==\nA graph with <math>n</math> vertices can at the most have <math> \\binom{n}{2} = \\frac{n (n-1)}{2} </math> distinct minimum cuts.\nThis bound is tight in the sense that a [[Cycle (graph theory)#Definitions|(simple) cycle]] on <math>n</math> vertices has exactly <math>\\frac{n (n-1)}{2}</math> minimum cuts.\n\n==See also==\n*[[Maximum cut]], a cut that is as large as possible instead of being as small as possible.\n*[[Vertex separator]], an analogous concept to minimum cuts for vertices instead of edges.\n\n==References==\n{{Reflist}}\n\n{{set index article}}\n\n[[Category:Graph theory objects]]\n[[Category:Network flow problem]]"
    }
  ]
}