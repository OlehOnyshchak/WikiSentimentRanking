{
  "pages": [
    {
      "title": "Heilbronn triangle problem",
      "url": "https://en.wikipedia.org/wiki/Heilbronn_triangle_problem",
      "text": "[[File:Heilbronn square n=6.svg|thumb|300px|Solution to the Heilbronn triangle problem for six points in the unit square. These points form triangles of four different shapes, with minimum area 1/8, as large as possible for six points in the square. This solution is an [[affine transformation]] of a [[regular hexagon]] but larger numbers of points have solutions that include interior points of the square.]]\nIn [[discrete geometry]] and [[discrepancy theory]], the '''Heilbronn triangle problem''' is a problem of placing points within a region in the plane, in order to avoid [[triangle]]s of small [[area]]. It is named after [[Hans Heilbronn]], who [[conjecture]]d prior to 1950 that this smallest triangle area is necessarily at most [[Proportionality (mathematics)#Inverse proportionality|inversely proportional]] to the [[Square (algebra)|square]] of the number of points. Heilbronn's conjecture was proven false, but the [[Asymptotic analysis|asymptotic growth]] rate of the minimum triangle area remains unknown.\n\n==Definition==\nThe problem may be defined in terms of any [[compact space|compact]] set ''D'' in the plane with nonzero area such as the  [[unit square]] or the [[unit disk]]. If ''S'' is a set of ''n'' points of ''D'', then every three points of ''S'' determine a triangle (possibly a degenerate one, with zero area). Let &Delta;(''S'') denote the minimum of the areas of these triangles, and let &Delta;(''n'') (for an integer ''n''&nbsp;≥&nbsp;3) denote the [[supremum]] of the values of &Delta;(''S'').\n\nThe question posed by Heilbronn was to give an expression, or matching asymptotic [[upper and lower bounds]], for &Delta;(''n''). That is, the goal is to find a [[function (mathematics)|function]] ''f'', described by a [[closed-form expression]], and constants ''c''<sub>1</sub> and ''c''<sub>2</sub>, such that for all ''n'',\n:<math>c_1 f(n) \\le \\Delta(n) \\le c_2 f(n)</math>.\nIn terms of [[big O notation]], the left inequality may be written as &Delta;(''n'')&nbsp;=&nbsp;&Omega;(''f''(''n'')), the right inequality may be written as &Delta;(''n'')&nbsp;=&nbsp;''O''(''f''(''n'')), and both of them together may be written as &Delta;(''n'')&nbsp;=&nbsp;&Theta;(''f''(''n'')). The shape and area of ''D'' may affect the exact values of &Delta;(''n''), but only by a constant factor, so they are unimportant for its asymptotic growth rate.\n\n==Heilbronn's conjecture and lower bound constructions==\nHeilbronn conjectured that\n:<math>\\Delta(n)=O\\left(\\frac{1}{n^2}\\right).</math>\nAs [[Paul Erdős]] showed, no smaller bound is possible: when ''n'' is a [[prime number]], the set of ''n'' points (''i'',&nbsp;''i''<sup>2</sup>&nbsp;mod&nbsp;''n'')  on an ''n''&nbsp;&times;&nbsp;''n''  [[integer lattice|integer grid]] have [[No-three-in-line problem|no three collinear points]], and therefore by [[Pick's formula]] each of the triangles they form has area at least 1/2. When this set of grid points is scaled to a unit square, they form a set of points whose smallest triangle area is at least proportional to 1/''n''<sup>2</sup>, matching Heilbronn's conjectured upper bound.<ref name=\"r51\">{{citation\n  | last = Roth\n  | first = K. F. | authorlink = Klaus Roth\n  | title = On a problem of Heilbronn\n  | journal = [[Journal of the London Mathematical Society]]\n  | volume = 26\n  | year = 1951\n  | issue = 3\n  | pages = 198&ndash;204\n  | doi = 10.1112/jlms/s1-26.3.198}}.</ref>\nIf ''n'' is not prime, then a similar construction using the next prime number larger than ''n'' achieves the same asymptotic lower bound.\n\n{{harvtxt|Komlós|Pintz|Szemerédi|1982}} eventually disproved Heilbronn's conjecture, by finding sets of points whose smallest triangle area grows asymptotically as\n:<math>\\Delta(n)=\\Omega\\left(\\frac{\\log n}{n^2}\\right).</math><ref>{{citation|last1=Komlós|first1=J.|author1-link=János Komlós (mathematician)|author2-link=János Pintz|last2=Pintz|first2=J.|last3=Szemerédi|first3=E.|author3-link=Endre Szemerédi|title=A lower bound for Heilbronn's problem|journal=[[Journal of the London Mathematical Society]]|volume=25|issue=1|pages=13–24|year=1982|mr=0645860|doi=10.1112/jlms/s2-25.1.13}}.</ref>\n\n==Upper bounds==\nTrivially, either by [[Point set triangulation|triangulating]] the [[convex hull]] of the given point set ''S'' or by choosing consecutive triples of points in the sorted order of their ''x''-coordinates, it is possible to show that every point set contains a small triangle, whose area is at most inversely proportional to&nbsp;''n''. {{harvtxt|Roth|1951}} was the first to prove a nontrivial upper bound on &Delta;(''n''), of the form<ref name=\"r51\"/>\n:<math>\\Delta(n)=O\\left(\\frac{1}{n\\sqrt{\\log\\log n}}\\right).</math>\nThe best bound known to date is of the form\n:<math>\\Delta(n)\\leq\\frac{\\exp{\\left(c\\sqrt{\\log n}\\right)}}{n^{8/7}},</math>\nfor some constant ''c'', proven by {{harvtxt|Komlós|Pintz|Szemerédi|1981}}.<ref>{{citation|last1=Komlós|first1=J.|author1-link=János Komlós (mathematician)|author2-link=János Pintz|last2=Pintz|first2=J.|last3=Szemerédi|first3=E.|author3-link=Endre Szemerédi|title=On Heilbronn's triangle problem|journal=[[Journal of the London Mathematical Society]]|volume=24|issue=3|pages=385–396|year=1981|mr=0635870|doi=10.1112/jlms/s2-24.3.385}}.</ref>\n\n==Specific shapes and numbers==\n{{harvtxt|Goldberg|1972}} has investigated the optimal arrangements of ''n'' points in a square, for ''n'' up to 16.<ref>{{citation\n | last = Goldberg | first = Michael\n | journal = [[Mathematics Magazine]]\n | jstor = 2687869\n | mr = 0296816\n | pages = 135–144\n | title = Maximizing the smallest triangle made by ''n'' points in a square\n | volume = 45\n | year = 1972\n | doi=10.2307/2687869}}.</ref> Goldberg's constructions for up to six points lie on the boundary of the square, and are placed to form an [[affine transformation]] of the vertices of a [[regular polygon]]. For larger values of ''n'', {{harvtxt|Comellas|Yebra|2002}} improved Goldberg's bounds, and for these values the solutions include points interior to the square.<ref>{{citation\n | last1 = Comellas | first1 = Francesc\n | last2 = Yebra | first2 = J. Luis A.\n | issue = 1\n | journal = [[Electronic Journal of Combinatorics]]\n | mr = 1887087\n | page = R6\n | title = New lower bounds for Heilbronn numbers\n | url = http://www.combinatorics.org/Volume_9/Abstracts/v9i1r6.html\n | volume = 9\n | year = 2002}}.</ref> These constructions have been proven optimal for up to seven points.<ref>{{citation\n | last1 = Zeng | first1 = Zhenbing\n | last2 = Chen | first2 = Liangyu\n | contribution = On the Heilbronn optimal configuration of seven points in the square\n | doi = 10.1007/978-3-642-21046-4_11\n | location = Heidelberg\n | mr = 2805061\n | pages = 196–224\n | publisher = Springer\n | series = Lecture Notes in Comput. Sci.\n | title = Automated deduction in geometry\n | volume = 6301\n | year = 2011}}.</ref>\n\n==Variations==\nThere have been many variations of this problem \nincluding the case of a uniformly random set of points, for which an argument based on [[Kolmogorov complexity]] shows that the [[expected value]] of the minimum area is inversely proportional to the cube of the number of points.<ref>{{citation\n | last1 = Jiang | first1 = Tao\n | last2 = Li | first2 = Ming\n | last3 = Vitányi | first3 = Paul | author3-link = Paul Vitanyi\n | doi = 10.1002/rsa.10024\n | issue = 2\n | journal = Random Structures & Algorithms\n | mr = 1884433\n | pages = 206–219\n | title = The average-case area of Heilbronn-type triangles\n | volume = 20\n | year = 2002| arxiv = math/9902043}}.</ref> Variations involving the volume of higher-dimensional [[simplex|simplices]] have also been studied.<ref>{{citation\n | last = Lefmann | first = Hanno\n | doi = 10.1007/s00454-007-9041-y\n | issue = 3\n | journal = [[Discrete and Computational Geometry]]\n | mr = 2443292\n | pages = 401–413\n | title = Distributions of points in ''d'' dimensions and large ''k''-point simplices\n | volume = 40\n | year = 2008}}.</ref>\n\n==See also==\n*[[Danzer set]], a set of points that avoids empty triangles of large area\n\n==References==\n{{reflist}}\n\n==External links==\n*{{mathworld|id=HeilbronnTriangleProblem|title=Heilbronn Triangle Problem}}\n*[http://www2.stetson.edu/~efriedma/packing.html Erich's Packing Center], by Erich Friedman, including the best known solutions to the Heilbronn problem for small values of ''n'' for squares, circles, equilateral triangles, and convex regions of variable shape but fixed area\n\n[[Category:Discrete geometry]]\n[[Category:Mathematical problems]]\n[[Category:Triangle geometry]]\n[[Category:Area]]"
    },
    {
      "title": "Heron's formula",
      "url": "https://en.wikipedia.org/wiki/Heron%27s_formula",
      "text": "{{about|calculating the area of a triangle|calculating a square root|Heron's method}}\n\n[[File:Triangle with notations 2 without points.svg|thumb|198px|A triangle with sides ''a'', ''b'', and ''c''.]]\n\nIn [[geometry]], '''Heron's formula''' (sometimes called Hero's formula), named after [[Hero of Alexandria]],<ref>{{cite web|title=Fórmula de Herón para calcular el área de cualquier triángulo|url=http://recursostic.educacion.es/descartes/web/materiales_didacticos/formula_heron/formula_de_Heron.htm|language=Spanish|accessdate=30 June 2012}}</ref> gives the [[area]] of a [[triangle]] when the length of all three sides are known. Unlike other triangle area formulae, there is no need to calculate angles or other distances in the triangle first.\n\n==Formulation==\nHeron's formula states that the [[area]] of a [[triangle]] whose sides have lengths {{math|''a''}}, {{math|''b''}}, and {{math|''c''}} is\n\n:<math>A = \\sqrt{s(s-a)(s-b)(s-c)},</math>\n\nwhere {{math|''s''}} is the [[semi-perimeter]] of the triangle; that is,\n\n:<math>s=\\frac{a+b+c}{2}.</math><ref>{{cite journal|author=Kendig, Keith|title=Is a 2000-Year-Old Formula Still Keeping Some Secrets?|journal=Amer. Math. Monthly|volume=107|year=2000|pages=402–415|url=http://www.maa.org/programs/maa-awards/writing-awards/is-a-2000-year-old-formula-still-keeping-some-secrets|doi=10.2307/2695295}}</ref>\n\nHeron's formula can also be written as\n\n:<math>A=\\frac{1}{4}\\sqrt{(a+b+c)(-a+b+c)(a-b+c)(a+b-c)}</math>\n:<math>A=\\frac{1}{4}\\sqrt{2(a^2 b^2+a^2c^2+b^2c^2)-(a^4+b^4+c^4)}</math>\n:<math>A=\\frac{1}{4}\\sqrt{(a^2+b^2+c^2)^2-2(a^4+b^4+c^4)}</math>\n:<math>A=\\frac{1}{4}\\sqrt{4(a^2b^2+a^2c^2+b^2c^2)-(a^2+b^2+c^2)^2}.</math>\n\n==Example==\n\nLet {{math|△''ABC''}} be the triangle with sides {{math|''a'' {{=}} 4}}, {{math|''b'' {{=}} 13}} and {{math|''c'' {{=}} 15}}. \nThe semiperimeter is {{math|''s'' {{=}} {{sfrac|1|2}}(''a'' + ''b'' + ''c'') {{=}} {{sfrac|1|2}}(4 + 13 + 15) {{=}} 16}}, and the area is\n\n:<math>\n\\begin{align}\nA &= \\sqrt{s\\left(s-a\\right)\\left(s-b\\right)\\left(s-c\\right)} = \\sqrt{16 \\cdot (16-4) \\cdot (16-13) \\cdot (16-15)}\\\\\n&= \\sqrt{16 \\cdot 12 \\cdot 3 \\cdot 1} = \\sqrt{576} = 24.\n\\end{align}\n</math>\n\nIn this example, the side lengths and area are all [[integer]]s, making it a [[Heronian triangle]]. However, Heron's formula works equally well in cases where one or all of these numbers is not an integer.\n\n== History ==\n\nThe formula is credited to [[Hero of Alexandria|Heron (or Hero) of Alexandria]], and a proof can be found in his book, ''Metrica'', written {{circa}} CE 60. It has been suggested that [[Archimedes]] knew the formula over two centuries earlier,<ref>{{cite book\n| author=Heath, Thomas L.\n| title=A History of Greek Mathematics (Vol II)\n| publisher=Oxford University Press\n| year=1921\n| pages=321–323}}</ref> and since ''Metrica'' is a collection of the mathematical knowledge available in the ancient world, it is possible that the formula predates the reference given in that work.<ref>{{MathWorld |urlname=HeronsFormula |title=Heron's Formula}}</ref>\n\nA formula equivalent to Heron's, namely\n\n:<math>A=\\frac1{2}\\sqrt{a^2c^2-\\left(\\frac{a^2+c^2-b^2}{2}\\right)^2}</math>\n\nwas discovered by the Chinese independently{{citation needed|date=October 2017}} of the Greeks. It was published in ''[[Mathematical Treatise in Nine Sections]]'' ([[Qin Jiushao]], 1247).<ref>{{Cite book|title=數學九章 (四庫全書本)|last=秦|first=九韶|publisher=|year=1773|isbn=|location=|pages=|chapter=卷五第二题, 三斜求积|chapter-url=https://zh.wikisource.org/zh-hant/%E6%95%B8%E5%AD%B8%E4%B9%9D%E7%AB%A0_(%E5%9B%9B%E5%BA%AB%E5%85%A8%E6%9B%B8%E6%9C%AC)/%E5%85%A8%E8%A6%BD#%E4%B8%89%E6%96%9C%E6%B1%82%E7%A9%8D}}</ref>\n\n== Proofs ==\nHeron's original proof made use of [[cyclic quadrilateral]]s, while other arguments appeal to [[trigonometry]] as below, or to the [[incenter]] and one [[excircle]] of the triangle [http://www.math.dartmouth.edu/~doyle/docs/heron/heron.txt].\n\n===Trigonometric proof using the law of cosines===\nA modern proof, which uses [[algebra]] and is quite unlike the one provided by Heron (in his book Metrica), follows.<ref>{{cite book\n| author=Niven, Ivan\n| title=Maxima and Minima Without Calculus\n| publisher=The Mathematical Association of America\n| year=1981\n| pages=7–8}}</ref>\nLet {{math|''a''}}, {{math|''b''}}, {{math|''c''}} be the sides of the triangle and {{math|''α''}}, {{math|''β''}}, {{math|''γ''}} the [[angle]]s opposite those sides.\nApplying the [[law of cosines]] we get\n\n:<math>\\cos \\gamma = \\frac{a^2+b^2-c^2}{2ab}</math>\n\nFrom this proof we get the algebraic statement that\n\n:<math>\\sin \\gamma = \\sqrt{1-\\cos^2 \\gamma} = \\frac{\\sqrt{4a^2 b^2 -(a^2 +b^2 -c^2)^2 }}{2ab}.</math>\n\nThe [[altitude (triangle)|altitude]] of the triangle on base {{math|''a''}} has length {{math|''b'' sin ''γ''}}, and it follows\n\n: <math>\n\\begin{align}\nA & = \\frac{1}{2} (\\mbox{base}) (\\mbox{altitude}) \\\\\n& = \\frac{1}{2} ab\\sin \\gamma \\\\\n& = \\frac{1}{4}\\sqrt{4a^2 b^2 -(a^2 +b^2 -c^2)^2} \\\\\n& = \\frac{1}{4}\\sqrt{(2a b -(a^2 +b^2 -c^2))(2a b +(a^2 +b^2 -c^2))} \\\\\n& = \\frac{1}{4}\\sqrt{(c^2 -(a -b)^2)((a +b)^2 -c^2)} \\\\\n& = \\sqrt{\\frac{(c -(a -b))(c +(a -b))((a +b) -c)((a +b) +c)}{16}} \\\\\n& = \\sqrt{\\frac{(b + c - a)}{2}\\frac{(a + c - b)}{2}\\frac{(a + b - c)}{2}\\frac{(a + b + c)}{2}} \\\\\n& = \\sqrt{\\frac{(a + b + c)}{2}\\frac{(b + c - a)}{2}\\frac{(a + c - b)}{2}\\frac{(a + b - c)}{2}} \\\\\n& = \\sqrt{s(s-a)(s-b)(s-c)}.\n\\end{align}\n</math>\n\nThe [[difference of two squares]] factorization was used in two different steps.\n\n===Algebraic proof using the Pythagorean theorem===\n[[Image:Triangle with notations 3.svg|thumb|270px|Triangle with altitude {{math|''h''}} cutting base {{math|''c''}} into {{math|''d'' + (''c'' − ''d'')}}.]]\nThe following proof is very similar to one given by Raifaizen.<ref>{{Cite journal\n  | last = Raifaizen\n  | first = Claude H.\n  | title = A Simpler Proof of Heron's Formula\n  | journal = Mathematics Magazine\n  | volume = 44\n  | number = 1\n  | pages = 27–28\n  | year = 1971\n}}</ref>\nBy the [[Pythagorean theorem]] we have {{math|''b''{{sup|2}} {{=}} ''h''{{sup|2}} + ''d''{{sup|2}}}} and {{math|''a''{{sup|2}} {{=}} ''h''{{sup|2}} + (''c'' − ''d''){{sup|2}}}} according to the figure at the right. Subtracting these yields {{math|''a''{{sup|2}} − ''b''{{sup|2}} {{=}} ''c''{{sup|2}} − 2''cd''}}. This equation allows us to express {{math|''d''}} in terms of the sides of the triangle:\n:<math>d=\\frac{-a^2+b^2+c^2}{2c}</math>\nFor the height of the triangle we have that {{math|''h''{{sup|2}} {{=}} ''b''{{sup|2}} − ''d''{{sup|2}}}}. By replacing {{math|''d''}} with the formula given above and applying the [[difference of squares]] identity we get\n:<math>\n\\begin{align}\nh^2 & = b^2-\\left(\\frac{-a^2+b^2+c^2}{2c}\\right)^2\\\\\n& = \\frac{(2bc-a^2+b^2+c^2)(2bc+a^2-b^2-c^2)}{4c^2}\\\\\n& = \\frac{((b+c)^2-a^2)(a^2-(b-c)^2)}{4c^2}\\\\\n& = \\frac{(b+c-a)(b+c+a)(a+b-c)(a-b+c)}{4c^2}\\\\\n& = \\frac{2(s-a)\\cdot 2s\\cdot 2(s-c)\\cdot 2(s-b)}{4c^2}\\\\\n& = \\frac{4s(s-a)(s-b)(s-c)}{c^2}\n\\end{align}\n</math>\n\nWe now apply this result to the formula that calculates the area of a triangle from its height:\n:<math>\n\\begin{align}\nA & = \\frac{ch}{2}\\\\\n& = \\sqrt{\\frac{c^2}{4}\\cdot \\frac{4s(s-a)(s-b)(s-c)}{c^2}}\\\\\n& = \\sqrt{s(s-a)(s-b)(s-c)}\n\\end{align}\n</math>\n\n===Trigonometric proof using the law of cotangents===\n[[File:Herontriangle2greek.svg|thumb|270px|right|Geometrical significance of {{math|''s'' − ''a''}}, {{math|''s'' − ''b''}}, and {{math|''s'' − ''c''}}.  See the [[Law of cotangents]] for the reasoning behind this.]]\nFrom the first part of the [[Law of cotangents]] proof,<ref>The second part of the Law of cotangents proof depends on Heron's formula itself, but this article depends only on the first part.</ref> we have that the triangle's area is both\n:<math>\n\\begin{align}\nA &= r\\big((s-a) + (s-b) + (s-c)\\big) = r^2\\left(\\frac{s-a}{r} + \\frac{s-b}{r} + \\frac{s-c}{r}\\right) \\\\\n&= r^2\\left(\\cot{\\frac{\\alpha}{2}} + \\cot{\\frac{\\beta}{2}} + \\cot{\\frac{\\gamma}{2}}\\right) \\\\\n\\end{align}\n</math>\nand {{math|''A'' {{=}} ''rs''}}, but, since the sum of the half-angles is {{sfrac|{{pi}}|2}}, the [[Proofs of trigonometric identities#Miscellaneous -- the triple cotangent identity|triple cotangent identity]] applies, so the first of these is\n:<math>\n\\begin{align}\nA &= r^2\\left(\\cot{\\frac{\\alpha}{2}} \\cot{\\frac{\\beta}{2}} \\cot{\\frac{\\gamma}{2}}\\right) = r^2\\left( \\frac{s-a}{r}\\cdot \\frac{s-b}{r}\\cdot \\frac{s-c}{r}\\right) \\\\\n&= \\frac{(s-a)(s-b)(s-c)}{r} \\\\\n\\end{align}\n</math>\n\nCombining the two, we get\n:<math>A^2 = s(s-a)(s-b)(s-c)</math>\nfrom which the result follows.\n\n== Numerical stability ==\nHeron's formula as given above is [[Numerical stability|numerically unstable]] for triangles with a very small angle when using floating point arithmetic. A stable alternative<ref>{{cite book |author-first=Pat H. |author-last=Sterbenz |title=Floating-Point Computation |date=1974-05-01 |edition=1st |series=Prentice-Hall Series in Automatic Computation |publisher=[[Prentice Hall]] |location=Englewood Cliffs, New Jersey, USA |isbn=0-13-322495-3<!-- 978-0-13-322495-5 -->}}</ref><ref>{{cite web |url=http://www.cs.berkeley.edu/~wkahan/Triangle.pdf |title=Miscalculating Area and Angles of a Needle-like Triangle |author=William M. Kahan |date=24 March 2000}}</ref> involves arranging the lengths of the sides so that {{math|''a'' ≥ ''b'' ≥ ''c''}} and computing\n:<math>A = \\frac{1}{4}\\sqrt{(a+(b+c)) (c-(a-b)) (c+(a-b)) (a+(b-c))}.</math>\nThe brackets in the above formula are required in order to prevent numerical instability in the evaluation.\n\n==Other area formulae resembling Heron's formula==\n\nThree other area formulae have the same structure as Heron's formula but are expressed in terms of different variables. First, denoting the medians from sides {{math|''a''}}, {{math|''b''}}, and {{math|''c''}} respectively as {{math|''m<sub>a</sub>''}}, {{math|''m<sub>b</sub>''}}, and {{math|''m<sub>c</sub>''}} and their semi-sum {{math|{{sfrac|1|2}}(''m<sub>a</sub>'' + ''m<sub>b</sub>'' + ''m<sub>c</sub>'')}} as {{math|''σ''}}, we have<ref>Benyi, Arpad, \"A Heron-type formula for the triangle,\" ''Mathematical Gazette\" 87, July 2003, 324–326.</ref>\n:<math>A = \\frac{4}{3} \\sqrt{\\sigma (\\sigma - m_a)(\\sigma - m_b)(\\sigma - m_c)}.</math>\n\nNext, denoting the altitudes from sides {{math|''a''}}, {{math|''b''}}, and {{math|''c''}} respectively as {{math|''h<sub>a</sub>''}}, {{math|''h<sub>b</sub>''}}, and {{math|''h<sub>c</sub>''}}, and denoting the semi-sum of the reciprocals of the altitudes as {{math|''H'' {{=}} {{sfrac|1|2}}(''h''{{su|b=''a''|p=−1}} + ''h''{{su|b=''b''|p=−1}} + ''h''{{su|b=''c''|p=−1}})}} we have<ref>Mitchell, Douglas W., \"A Heron-type formula for the reciprocal area of a triangle,\" ''Mathematical Gazette'' 89, November 2005, 494.</ref>\n:<math>A^{-1} = 4 \\sqrt{H(H-h_a^{-1})(H-h_b^{-1})(H-h_c^{-1})}.</math>\n\nFinally, denoting the semi-sum of the angles' sines as {{math|''S'' {{=}} {{sfrac|1|2}}(sin ''α'' + sin ''β'' + sin ''γ'')}}, we have<ref>Mitchell, Douglas W., \"A Heron-type area formula in terms of sines,\" ''Mathematical Gazette'' 93, March 2009, 108–109.</ref>\n:<math>A = D^{2} \\sqrt{S(S-\\sin \\alpha)(S-\\sin \\beta)(S-\\sin \\gamma)}</math>\n\nwhere {{math|''D''}} is the diameter of the circumcircle: {{math|''D'' {{=}} {{sfrac|''a''|sin ''α''}} {{=}} {{sfrac|''b''|sin ''β''}} {{=}} {{sfrac|''c''|sin ''γ''}}}}.\n\n== Generalizations ==\nHeron's formula is a special case of [[Brahmagupta's formula]] for the area of a [[cyclic quadrilateral]]. Heron's formula and Brahmagupta's formula are both special cases of [[Bretschneider's formula]] for the area of a [[quadrilateral]]. Heron's formula can be obtained from Brahmagupta's formula or Bretschneider's formula by setting one of the sides of the quadrilateral to zero.\n\nHeron's formula is also a special case of the [[trapezoid#Area|formula]] for the area of a trapezoid or trapezium based only on its sides. Heron's formula is obtained by setting the smaller parallel side to zero.\n\nExpressing Heron's formula with a [[Cayley–Menger determinant]] in terms of the squares of the [[distance]]s between the three given vertices,\n:<math> A =  \\frac{1}{4} \\sqrt{- \\begin{vmatrix} \n  0 & a^2 & b^2 & 1 \\\\\na^2 & 0   & c^2 & 1 \\\\\nb^2 & c^2 & 0   & 1 \\\\\n  1 &   1 &   1 & 0\n\\end{vmatrix} } </math>\nillustrates its similarity to [[Tartaglia's formula]] for the [[volume]] of a [[Simplex|three-simplex]].\n\nAnother generalization of Heron's formula to pentagons and hexagons inscribed in a circle was discovered by [[David P. Robbins]].<ref>D. P. Robbins, \"Areas of Polygons Inscribed in a Circle\", Discr. Comput. Geom. 12, 223-236, 1994.</ref>\n\n=== Heron-type formula for the volume of a tetrahedron ===\nIf {{math|''U''}}, {{math|''V''}}, {{math|''W''}}, {{math|''u''}}, {{math|''v''}}, {{math|''w''}} are lengths of edges of the tetrahedron (first three form a triangle; {{math|''u''}} opposite to {{math|''U''}} and so on), then<ref>W. Kahan, \"What has the Volume of a Tetrahedron to do with Computer Programming Languages?\", [http://www.cs.berkeley.edu/~wkahan/VtetLang.pdf], pp.&nbsp;16–17.</ref>\n:<math>\n\\text{volume} = \\frac{\\sqrt {\\,( - a + b + c + d)\\,(a - b + c + d)\\,(a + b - c + d)\\,(a + b + c - d)}}{192\\,u\\,v\\,w}</math>\nwhere\n: <math>\n    \\begin{align} a & = \\sqrt {xYZ} \\\\ b & = \\sqrt {yZX} \\\\ c & = \\sqrt {zXY} \\\\ d & = \\sqrt {xyz} \\\\ X & = (w - U + v)\\,(U + v + w) \\\\ x & = (U - v + w)\\,(v - w + U) \\\\ Y & = (u - V + w)\\,(V + w + u) \\\\ y & = (V - w + u)\\,(w - u + V) \\\\ Z & = (v - W + u)\\,(W + u + v) \\\\ z & = (W - u + v)\\,(u - v + W). \\end{align} \n</math>\n\n==See also==\n*[[Shoelace formula]]\n\n== References ==\n<references/>\n\n== External links ==\n*[http://www.cut-the-knot.org/pythagoras/herons.shtml A Proof of the Pythagorean Theorem From Heron's Formula] at [[cut-the-knot]]\n*[http://www.mathopenref.com/heronsformula.html Interactive applet and area calculator using Heron's Formula]\n*[http://www.math.dartmouth.edu/~doyle/docs/heron/heron.txt J.H. Conway discussion on Heron's Formula]\n*{{MathPages|id=home/kmath196/kmath196|title=Heron's Formula and Brahmagupta's Generalization}}\n*[http://jwilson.coe.uga.edu/EMT668/EMAT6680.2000/Umberger/MATH7200/HeronFormulaProject/GeometricProof/geoproof.html A Geometric Proof of Heron's Formula]\n*[http://www.maa.org/sites/default/files/0746834212944.di020798.02p0691h.pdf An alternative proof of Heron's Formula without words]\n*[http://www.maa.org/sites/default/files/Pratt-CMJ0902994.pdf Factoring Heron]\n\n{{DEFAULTSORT:Heron's Formula}}\n[[Category:Triangle geometry]]\n[[Category:Articles containing proofs]]\n[[Category:Area]]\n[[Category:Theorems in plane geometry]]\n<!-- dummy edit -->"
    },
    {
      "title": "History of measurement",
      "url": "https://en.wikipedia.org/wiki/History_of_measurement",
      "text": "{{multiple issues| \n{{more footnotes|date=October 2017}}\n{{speculation|date=December 2017}}\n{{accuracy|date=December 2017}}\n{{cleanup rewrite|date=April 2015}}\n}}\n{{Use mdy dates|date=June 2013}}\n[[File:Coudée-turin detail.jpg|thumb|250px|Detail of a [[cubit]] rod in the [[Museo Egizio]] of [[Turin]]]]\nThe earliest recorded systems of weights and measures originate in the 3rd or 4th millennium BC. Even the very earliest civilizations needed measurement for purposes of agriculture, construction, and trade. Early standard units might only have applied to a single community or small region, with every area developing its own standards for lengths, areas, volumes and masses. Often such systems were closely tied to one field of use, so that volume measures used, for example, for dry grains were unrelated to those for liquids, with neither bearing any particular relationship to units of length used for measuring cloth or land. With development of manufacturing technologies, and the growing importance of trade between communities and ultimately across the Earth, standardized weights and measures became critical. Starting in the 18th century, modernized, simplified and uniform systems of weights and measures were developed, with the fundamental units defined by ever more precise methods in the science of [[metrology]]. The discovery and application of [[electricity]] was one factor motivating the development of standardized internationally applicable units.\n\n==Sources of information==\n\n[[Weights and measures]] have taken a great variety of forms over the course of history, from simple informal expectations in barter transactions to elaborate state and supranational systems that integrate measures of many different kinds.  Weights and measures from the oldest societies can often be inferred at least in part from [[archaeology|archaeological specimens]], often preserved in museums.{{Citation needed|date=February 2007}} The comparison of the dimensions of buildings with the descriptions of contemporary writers is another source of information. An interesting example of this is the comparison of the dimensions of the Greek [[Parthenon]] with the description given by [[Plutarch]] from which a fairly accurate idea of the size of the [[Ancient Greek units of measurement#Length|Attic foot]] is obtained. Because of the comparative volume of artifacts and documentation, we know much more about the state-sanctioned measures of large, advanced societies than we do about those of smaller societies or about the informal measures that often coexisted with official ones throughout history. In some cases, we have only plausible theories and we must sometimes select the interpretation to be given to the evidence.\n\nBy studying the evidence given by all available sources, and by correlating the relevant facts, we obtain some idea of the origin and development of the units. We find that they have changed more or less gradually with the passing of time in a complex manner because of a great variety of modifying influences.{{Citation needed|date=May 2013}} It is possible to group official measurement systems for large societies into historical systems that are relatively stable over time, including: the Babylonian system, the Egyptian system, the Phileterian system of the [[Ptolemaic dynasty|Ptolemaic]] age, the Olympic system of Greece, the Roman system, the [[English unit|British system]], and the [[metric system]].\n\n==Earliest known systems==\nThe earliest known uniform systems of weights and measures seem all to have been created at some time in the [[4th millennium BC|4th]] and [[3rd millennium BC|3rd millennia BC]] among the ancient peoples of [[Egypt]], [[Mesopotamia]] and the [[Indus Valley]], and perhaps also [[Elam]] (in [[Iran]]) as well.\n\nEarly [[Babylonia]]n and [[History of Egypt|Egyptian]] records and the [[Hebrew Bible]] indicate that length was first measured with the forearm, hand, or finger and that time was measured by the periods of the sun, moon, and other heavenly bodies. When it was necessary to compare the capacities of containers such as [[gourd]]s or [[clay]] or metal vessels, they were filled with plant seeds which were then counted to measure the [[volume]]s. When means for weighing were invented, seeds and stones served as standards. For instance, the [[Carat (unit)|carat]], still used as a unit for gems, was derived from the [[carob]] seed.\n\n==History of units==\n{{main|Historical metrology}}\n\n=== Units of length ===\nThe Egyptian [[cubit]], the [[Indus Valley]] units of length referred to above and the Mesopotamian cubit were used in the 3rd millennium BC and are the earliest known units used by ancient peoples to measure length. The units of length used in ancient India included the dhanus, or dhanush (bow), the krosa (cry, or cow-call) and the [[yojana]] (stage).\n\nThe common cubit was the length of the forearm from the elbow to the tip of the middle finger. It was divided into the span of the hand or the length between the tip of little finger to the tip of the thumb (one-half cubit), the palm or width of the hand (one sixth), and the digit or width of the middle finger (one twenty-fourth).  The Royal Cubit, which was a standard cubit enhanced by an extra palm—thus 7 palms or 28 digits long—was used in constructing buildings and monuments and in surveying in ancient Egypt.  The [[inch]], [[foot (length)|foot]], and [[yard]] evolved from these units through a complicated transformation not yet fully understood. Some believe they evolved from cubic measures; others believe they were simple proportions or multiples of the cubit. In whichever case, the Greeks and Romans inherited the foot from the Egyptians. The Roman foot (~296&nbsp;mm) was divided into both 12 ''unciae'' (inches) (~24.7&nbsp;mm) and 16 digits (~18.5&nbsp;mm). The Romans also introduced the ''mille passus'' (1000 paces) or double steps, the pace being equal to five Roman feet (~1480&nbsp;mm). The Roman [[mile]] of 5000 feet (1480 m) was introduced into England during the occupation. [[Elizabeth I of England|Queen Elizabeth I]] (reigned from 1558 to 1603) changed, by statute, the mile to 5280 feet (~1609 m) or 8 furlongs, a [[furlong]] being 40 [[rod (unit)]]s (~201 m) of 5.5 yards (~5.03 m) each.\n\nThe introduction of the yard (0.9144 m) as a unit of length came later, but its origin is not definitely known. Some believe the origin was the double cubit, others believe that it originated from cubic measure. Whatever its origin, the early yard was divided by the binary method into 2, 4, 8, and 16 parts called the half-yard, span, finger, and nail. The association of the yard with the \"gird\" or circumference of a person's waist or with the distance from the tip of the nose to the end of the thumb of [[Henry I of England|King Henry I]] (reigned 1100–1135) are probably standardizing actions, since several yards were in use in Britain.\nThere were also Rods, Poles and Perches for measurements of length. The following table lists the equivalents.\n\n{| class=\"wikitable sortable\" style=\"text-align:center;\"\n!width=\"44\"| components\n!width=\"66\"| unit\n|-\n|12 lines\n|1&nbsp;inch\n|-\n|12&nbsp;inches\n|1 foot\n|-\n|3 feet\n|1 yard\n|-\n|1760 yards\n|1 mile\n|-\n|36&nbsp;inches\n|1 yard\n|-\n|440 yards\n|quarter-mile\n|-\n|880 yards\n|half-mile\n|-\n|100 links\n|1 chain\n|-\n|10 chains\n|1 furlong\n|-\n|8 furlongs\n|1 mile\n|-\n|4&nbsp;inches\n|1 hand\n|-\n|22 yards\n|1 chain\n|-\n|5.5 yards\n|1 rod, pole or perch\n|-\n|4 poles\n|1 chain\n|-\n|40 poles\n|1 furlong\n|}\n\n=== Units of mass ===\nThe [[Grain (mass)|grain]] was the earliest [[Mass#Units of mass|unit of mass]] and is the smallest unit in the [[Apothecarie' system of mass|apothecary]], [[avoirdupois]], Tower, and [[Troy weight|troy]] systems. The early unit was a grain of wheat or barleycorn used to weigh the precious metals silver and gold. Larger units preserved in stone standards were developed that were used as both units of mass and of monetary currency. The [[Pound (mass)|pound]] was derived from the mina used by ancient civilizations.  A smaller unit was the shekel, and a larger unit was the [[talent (measurement)|talent]]. The magnitude of these units varied from place to place. The Babylonians and Sumerians had a system in which there were 60 shekels in a mina and 60 minas in a talent. The Roman talent consisted of 100 libra (pound) which were smaller in magnitude than the mina. The troy pound (~373.2 g) used in England and the United States for monetary purposes, like the Roman pound, was divided into 12 ounces, but the Roman uncia (ounce) was smaller. The carat is a unit for measuring gemstones that had its origin in the carob seed, which later was standardized at 1/144 ounce and then 0.2&nbsp;gram. \n\nGoods of commerce were originally traded by number or volume.  When weighing of goods began, units of mass based on a volume of grain or water were developed. The diverse magnitudes of units having the same name, which still appear today in our dry and liquid measures, could have arisen from the various commodities traded.  The larger avoirdupois pound for goods of commerce might have been based on volume of water which has a higher [[bulk density]] than grain.\n\nThe stone, quarter, hundredweight, and ton were larger units of mass used in Britain.  Today only the stone continues in customary use for measuring personal body weight.  The present stone is 14 pounds (~6.35&nbsp;kg), but an earlier unit appears to have been 16 pounds (~7.25&nbsp;kg).  The other units were multiples of 2, 8, and 160 times the stone, or 28, 112, and 2240 pounds (~12.7&nbsp;kg, 50.8&nbsp;kg, 1016&nbsp;kg), respectively.  The hundredweight was approximately equal to two talents.  The ton of 2240 pounds is called the \"long ton\".  The \"short ton\" is equal to 2000 pounds (~907&nbsp;kg).  A tonne (t) is equal to 1000&nbsp;kg.\n\n=== Units of time and angle ===\nThe division of the circle into 360 degrees and the day into hours, minutes, and seconds can be traced to the Babylonians who had [[sexagesimal]] system of numbers.  The 360 degrees may have been related to a [[360-day calendar|year of 360 days]].  Many other [[systems of measurement]] divided the day differently -- [[Hour#Counting hours|counting hours]], [[decimal time]], etc. Other [[calendar]]s divided the year differently.\n\n== Forerunners of the metric system ==\nDecimal numbers are an essential part of the metric system, with only one base unit and multiples created on the decimal base, the figures remain the same. This simplifies calculations. Although the [[India|Indians]] used decimal numbers for mathematical computations, it was [[Simon Stevin]] who in 1585 first advocated the use of decimal numbers for everyday purposes in his booklet ''De Thiende'' (old Dutch for 'the tenth').  He also declared that it would only be a matter of time before decimal numbers were used for currencies and measurements. [[Simon Stevin#Decimal fractions|His notation]] for decimal fractions was clumsy, but this was overcome with the introduction of the decimal point, generally attributed to [[Bartholomaeus Pitiscus]] who used this notation in his trigonometrical tables (1595).<ref name=\"Stevin_MacTutor2\">{{MacTutor|id=Stevin|date=January 2004}}</ref>\n\nIn his [[An Essay towards a Real Character and a Philosophical Language|''Essay towards a Real Character and a Philosophical Language'']], published in 1668, [[John Wilkins]] proposed a system of measurement that was very similar in concept to today's metric system.  He proposed retaining the second as the basic unit of time and proposed that the length of a pendulum which crossed the zero position once a second (i.e. had a period of two seconds) should be the base unit of length.  This length, for which he proposed the name \"standard\", would have been 994&nbsp;mm.  His base unit of mass, which he proposed calling a \"hundred\", would have been the mass of a cubic standard of distilled rainwater.  The names that he proposed for decimal multiples and subunits of his base units of measure were the names of units of measure that were in use at the time.<ref>Reproduction (33&nbsp;MB):{{cite book|chapter-url=http://www.metricationmatters.com/docs/WilkinsTranslationLong.pdf|title=An Essay towards a Real Character and a Philosophical Language|author=John Wilkins|publisher=The Royal Society|year=1668|pages=190–194|chapter=VII|accessdate=6 March 2011|author-link=John Wilkins}}</ref><ref>{{cite web|url=http://www.metricationmatters.com/docs/WilkinsTranslationShort.pdf|title=An ESSAY Towards a REAL CHARACTER, And a PHILOSOPHICAL LANGUAGE|publisher=Metricationmatters.com|accessdate=24 November 2014}}</ref>\n\nIn 1670, [[Gabriel Mouton]] published a proposal that was in essence similar to Wilkins' proposal, except that his base unit of length would have been 1/1000 of a [[minute of arc]] (about 1.852&nbsp;m) of geographical latitude.  He proposed calling this unit the virga.  Rather than using different names for each unit of length, he proposed a series of names that had prefixes, rather like the prefixes found in SI.<ref name=\"Mouton2\">{{MacTutor|id=Mouton|date=January 2004}}</ref>\n\nIn 1790, [[Thomas Jefferson]] submitted a [[Plan for Establishing Uniformity in the Coinage, Weights, and Measures of the United States|report]] to the [[United States Congress]] in which he proposed the adoption of a decimal system of coinage and of weights and measures.  He proposed calling his base unit of length a \"foot\" which he suggested should be either {{frac|3|10}} or {{frac|1|3}} of the length of a pendulum that had a period of one second – that is {{frac|3|10}} or {{frac|1|3}} of the \"standard\" proposed by Wilkins over a century previously.  This would have equated to 11.755 English inches (29.8&nbsp;cm) or 13.06 English inches (33.1&nbsp;cm). Like Wilkins, the names that he proposed for multiples and subunits of his base units of measure were the names of units of measure that were in use at the time.<ref>{{cite web|url=http://avalon.law.yale.edu/18th_century/jeffplan.asp|title=Plan for Establishing Uniformity in the Coinage, Weights, and Measures of the United States Communicated to the House of Representatives, July 13, 1790|last=Jefferson|first=Thomas|date=4 July 1790|location=New York}}</ref> The great interest in [[geodesy]] during this era, and the measurement system ideas that developed, influenced how the continental US was [[Surveying|surveyed]] and parceled. The story of how Jefferson's full vision for the new measurement system came close to displacing the [[Chain (unit)|Gunter chain]] and the traditional [[acre]], but ended up not doing so, is explored in [[Andro Linklater]]'s ''Measuring America''.<ref name=\"Linklater_20022\">{{cite book|title=Measuring America: How an Untamed Wilderness Shaped the United States and Fulfilled the Promise of Democracy|last=Linklater|first=Andro|publisher=Walker & Co|year=2002|isbn=978-0-8027-1396-4}}</ref>\n\n== Metric conversion ==\n{{Main article|Metrication|History of the metric system}}\nThe [[metric system]] was first described in 1668 and officially adopted by France in 1799.  Over nineteenth and twentieth centuries, it became the dominant system worldwide, although several countries, including the United States and China, continue to use their customary units. Among the numerous customary systems, many have been adapted to become an integer multiple of a related metric unit: The [[Scandinavian mile]] is now defined as 10&nbsp;km, the [[Chinese jin]] is now defined as 0.5&nbsp;kg, and the [[Dutch units of measurement|Dutch ons]] is now defined as 100&nbsp;g. The [[US customary units|American system]] is unusual in that its units have not been adapted in such a manner.\n\n== References ==\n*{{NIST-PD|article=Specifications, Tolerances, and Other Technical Requirements for Weighing (Handbook 44 -2018)|url=https://www.nist.gov/pml/weights-and-measures/nist-handbook-44-2018-current-edition}}\n{{Reflist}}\n\n== Further reading ==\n*, ''Measures and Weights in the Islamic World. An English Translation of Professor Walther Hinz's Handbook “Islamische Maße und Gewichte“'',  with a foreword by Professor Bosworth, F.B.A. Kuala Lumpur, ISTAC, 2002, {{ISBN|983-9379-27-5}}. This work is an annotated translation of a work in German by the late German orientalist Walther Hinz, published in the ''Handbuch der Orientalistik'', erste Abteilung, Ergänzungsband I, Heft 1, [[Leiden]], The Netherlands: E. J. Brill, 1970.\n* ''Scales and Weights: A Historical Outline'', Bruno Kisch. (New Haven: Yale University Press, 1965). [https://web.archive.org/web/20080504184533/http://www.med.yale.edu/library/historical/streeter.htm Based in part on the Edward C. Streeter collection at Yale Medical Historical Library]\n\n{{systems of measurement}}\n\n{{DEFAULTSORT:History of Measurement}}\n[[Category:Customary units of measurement in the United States]]\n[[Category:Human-based units of measurement]]\n[[Category:Imperial units]]\n[[Category:Measurement]]\n[[Category:Obsolete units of measurement|*]]\n[[Category:Systems of units]]\n[[Category:Units of area]]\n[[Category:Units of length]]\n[[Category:Units of mass]]\n[[Category:Units of measurement]]\n[[Category:Units of volume]]\n[[Category:Area]]"
    },
    {
      "title": "One-seventh area triangle",
      "url": "https://en.wikipedia.org/wiki/One-seventh_area_triangle",
      "text": "[[Image:One-seventh area triangle.svg|thumb|The area of the pink triangle is one-seventh of the area of the large triangle ABC.]]\nIn [[plane geometry]], a triangle ''ABC'' contains a [[triangle]] of one-seventh [[area]] of ''ABC'' formed as follows: the sides of this triangle lie on [[cevian]]s ''p, q, r'' where\n:''p'' connects ''A'' to a point on ''BC'' that is one-third the distance from ''B'' to ''C'',\n:''q'' connects ''B'' to a point on ''CA'' that is one-third the distance from ''C'' to ''A'',\n:''r'' connects ''C'' to a point on ''AB'' that is one-third the distance from ''A'' to ''B''.\n\nThe proof of the existence of the '''one-seventh area triangle''' follows from the construction of six parallel lines:\n:  two parallel to ''p'', one through ''C'', the other through ''q.r''\n:  two parallel to ''q'', one through ''A'', the other through ''r.p''\n:  two parallel to ''r'', one through ''B'', the other through ''p.q''.\n\nThe suggestion of [[Hugo Steinhaus]] is that the (central) triangle with sides ''p,q,r'' be reflected in its sides and vertices.<ref>Hugo Steinhaus (1960) ''Mathematical Snapshots''</ref> These six extra triangles partially cover ''ABC'', and leave six overhanging extra triangles lying outside ''ABC''. Focusing on the parallelism of the full construction (offered by [[Martin Gardner]] through [[James Randi]]’s on-line magazine), the pair-wise congruences of overhanging and missing pieces of ''ABC'' is evident. As seen in the graphical solution, six plus the original equals the whole triangle ''ABC''.<ref>[[James Randi]] (2001) [https://web.archive.org/web/20060427055758/http://www.randi.org/jr/02-09-2001.html That Dratted Triangle], proof by [[Martin Gardner]]</ref>\n[[File:TriangleOneSeventhAreaGraphicalSoln.png|alt=Graphical solution to the one-seventh area triangle problem.|thumb|Congruence of edge lengths allows rotation of the selected triangles to form three equal-area parallelograms, which bisect into six triangles of equal size to the original interior triangle.]]\nAn early exhibit of this geometrical construction and area computation was given by Robert Potts in 1859 in his Euclidean geometry textbook.<ref>Robert Potts (1859) ''Euclid's Elements of Geometry'', Fifth school edition, problems 59 and 100, [https://archive.org/stream/euclidselements00unkngoog#page/n90/mode/1up pages 78 & 80] via [[Internet Archive]]</ref>\n\nAccording to Cook and Wood (2004), this triangle puzzled [[Richard Feynman]] in a dinner conversation; they go on to give four different proofs<ref>R.J. Cook & G.V. Wood (2004) \"Feynman's Triangle\", ''[[Mathematical Gazette]]'' 88:299–302</ref> De Villiers (2005) provides a generalization and an analogous result for a [[parallelogram]].<ref>Michael de Villiers (2005) [http://mysite.mweb.co.za/residents/profmd/feynman.pdf \"Feynman's Triangle: Some Feedback and More\"] ''Mathematical Gazette'' 89:107</ref>\n\nA more general result is known as [[Routh's theorem]].\n\n==References==\n{{Reflist}}\n*[[H. S. M. Coxeter]] (1969) ''Introduction to Geometry'', page 211, [[John Wiley & Sons]].\n== External links ==\n* [http://frink.machighway.com/~dynamicm/feynman.html Feynman's Triangle] at [http://dynamicmathematicslearning.com/JavaGSPLinks.htm Dynamic Geometry Sketches], an interactive dynamic geometry sketch with some generalizations as well.\n\n[[Category:Triangle geometry]]\n[[Category:Articles containing proofs]]\n[[Category:Area]]\n[[Category:Affine geometry]]"
    },
    {
      "title": "Pappus's area theorem",
      "url": "https://en.wikipedia.org/wiki/Pappus%27s_area_theorem",
      "text": "[[File:Pappus area theorem proof2.svg|thumb|dark grey area = light grey area]]\n'''Pappus's area theorem''' describes the relationship between the areas of three [[parallelogram]]s attached to three sides of an arbitrary [[triangle]]. The theorem, which can also be thought of as a generalization of the [[Pythagorean theorem]], is named after the Greek mathematician [[Pappus of Alexandria]] (4th century AD), who discovered it.\n\n== Theorem ==\nGiven an arbitrary triangle with two arbitrary parallelograms attached to two of its sides the theorem tells how to construct a parallelogram over the third side, such that the area of the third parallelogram equals the sum of the areas of the other two parallelograms.\n\nLet ''ABC'' be the arbitrary triangle and ''ABDE'' and ''ACFG'' the two arbitrary parallelograms attached to the triangle sides AB and AC. The extended parallelogram sides DE and FG intersect at H. The line segment AH now \"becomes\" the side of the third parallelogram BCLM attached to the triangle side BC, i.e., one constructs line segments BL and CM over BC, such that BL and CM are a parallel and equal in length to AH. The following identity then holds for the areas (denoted by A) of the parallelograms:\n\n:<math>\\text{A}_{ABDE}+\\text{A}_{ACFG}=\\text{A}_{BCLM}</math>\n\nThe theorem generalizes the Pythagorean theorem twofold. Firstly it works for arbitrary triangles rather than only for right angled ones and secondly it uses parallelograms rather than squares. For squares on two sides of an arbitrary triangle it yields a parallelogram of equal area over the third side and if the two sides are the legs of a right angle the parallelogram over the third side will be square as well. For a right-angled triangle, two parallelograms attached to the legs of the right angle yield a rectangle of equal area on the third side and again if the two parallelograms are squares then the rectangle on the third side will be a square as well.\n\n== Proof ==\nDue to having the same base length and height the parallelograms ''ABDE'' and ''ABUH'' have the same area, the same argument applying to the parallelograms ''ACFG'' and ''ACVH'', ''ABUH'' and ''BLQR'', ''ACVH'' and ''RCMQ''. This already yields the desired result, as we have:\n:<math>\n\\begin{align}\n  \\text{A}_{ABDE}+\\text{A}_{ACFG} &=\\text{A}_{ABUH}+\\text{A}_{ACVH}\\\\\n &=\\text{A}_{BLRQ}+\\text{A}_{RCMQ}\\\\\n &=\\text{A}_{BCLM}\n\\end{align}\n</math>\n\n== References ==\n*Howard Eves: ''Pappus's Extension of the Pythagorean Theorem''.The Mathematics Teacher, Vol. 51, No. 7 (November 1958), pp.&nbsp;544–546 ([https://www.jstor.org/stable/27955752 JSTOR])\n*Howard Eves: ''Great Moments in Mathematics (before 1650)''. Mathematical Association of America, 1983, {{ISBN|9780883853108}}, p.&nbsp;37 ({{Google books|9_w5jDPTvCQC|excerpt|page=37}})\n*[[Eli Maor]]: ''The Pythagorean Theorem: A 4,000-year History''. Princeton University Press, 2007, {{ISBN|9780691125268}}, pp.&nbsp;58–59 ({{Google books|Z5VoBGy3AoAC|excerpt|page=58}})\n*Claudi Alsina, Roger B. Nelsen: ''Charming Proofs: A Journey Into Elegant Mathematics''. MAA, 2010, {{ISBN|9780883853481}}, pp.&nbsp;77–78 ({{Google books|mIT5-BN_L0oC|excerpt|page=77}})\n\n== External links ==\n*[http://math.ucr.edu/~res/math153/history05f.pdf ''The Pappus Area Theorem'']\n*[http://users.math.uoc.gr/~pamfilos/eGallery/problems/Pappus.html '' Pappus theorem'']\n\n{{Commons category|Pappus&#39;s area theorem}}\n\n[[Category:Area]]\n[[Category:Articles containing proofs]]\n[[Category:Equations]]\n[[Category:Euclidean plane geometry]]\n[[Category:Theorems in plane geometry]]\n[[Category:Triangle geometry]]"
    },
    {
      "title": "Pappus's centroid theorem",
      "url": "https://en.wikipedia.org/wiki/Pappus%27s_centroid_theorem",
      "text": "[[File:Pappus centroid theorem areas.gif|thumb|right|400px|The theorem applied to an open cylinder, cone and a sphere to obtain their surface areas. The centroids are at a distance ''a'' (in red) from the axis of rotation.]]\n\nIn mathematics, '''Pappus's centroid theorem''' (also known as  the '''Guldinus theorem''', '''Pappus–Guldinus theorem''' or '''Pappus's theorem''') is either of two related [[theorem]]s dealing with the [[surface area]]s and [[volume]]s of [[surface of revolution|surface]]s and [[solid of revolution|solid]]s of revolution.\n\nThe theorems are attributed to [[Pappus of Alexandria]]{{efn|See:<ref>{{cite book|author=Pappus of Alexandria|author-link=Pappus of Alexandria|editor-last=Jones|editor-first=Alexander|year=1986|title=Book 7 of the ''Collection''|volume=8|location=New York|publisher=Springer-Verlag|isbn=978-1-4612-4908-5|doi=10.1007/978-1-4612-4908-5|orig-year=c. 320|series=Sources in the History of Mathematics and Physical Sciences}}</ref>\n{{Quote|\nThey who look at these things are hardly exalted, as were the ancients and all who wrote the finer things. When I see everyone occupied with the rudiments of mathematics and of the material for inquiries that nature sets before us, I am ashamed; I for one have proved things that are much more valuable and offer much application. In order not to end my discourse declaiming this with empty hands, I will give this for the benefit of the readers:<br>\n\nThe ratio of solids of complete revolution is compounded of (that) of the revolved figures and (that) of the straight lines similarly drawn to the axes from the centers of gravity in them; that of (solids of) incomplete (revolution) from (that) of the revolved figures and (that) of the arcs that the centers of gravity in them describe, where the (ratio) of these arcs is, of course, (compounded) of (that) of the (lines) drawn and (that) of the angles of revolution that their extremities contain, if these (lines) are also at (right angles) to the axes. These propositions, which are practically a single one, contain many theorems of all kinds, for curves and surfaces and solids, all at once and by one proof, things not yet and things already demonstrated, such as those in the twelfth book of the ''First Elements''.\n |author=Pappus\n |source=''Collection'', Book VII, ¶41‒42}}\n}} and [[Paul Guldin]].{{efn|\"Quantitas rotanda in viam rotationis ducta, producit Potestatem Rotundam uno gradu altiorem, Potestate sive Quantitate rotata.\"<ref>{{cite book|author-last=Guldin|author-first=Paul|author-link=Paul Guldin|title=De centro gravitatis trium specierum quanitatis continuae|volume=2|pages=147|year=1640|publisher=Gelbhaar, Cosmerovius|location=Vienna|url=https://books.google.com/books?id=CNaI61CYc94C&lpg=PA147|access-date=2016-08-04}}</ref>\nThat is: \"A quantity in rotation, multiplied by its circular trajectory, creates a circular power of higher degree, power, or quantity in rotation.\" <ref name=\"RdG2015\">{{cite book|author-last=Radelet-de Grave|author-first=Patricia|editor-last=Jullien|editor-first=Vincent|title=Seventeenth-Century Indivisibles Revisited|chapter=Kepler, Cavalieri, Guldin. Polemics with the departed|pages=68|publisher=Birkhäuser|series=Science Networks. Historical Studies|volume=49|isbn=978-3-3190-0131-9|chapter-url=https://books.google.com/books?id=8Vt1CQAAQBAJ&lpg=PA68|date=2015-05-19|doi=10.1007/978-3-319-00131-9|issn=1421-6329|location=Basel|access-date=2016-08-04}}</ref> }}\n\n==The first theorem==\nThe first theorem states that the [[surface area]] ''A'' of a [[surface of revolution]] generated by rotating a [[plane curve]] ''C'' about an [[axis of rotation|axis]] external to ''C'' and on the same plane is equal to the product of the [[arc length]] ''s'' of ''C'' and the distance ''d'' traveled by the [[Centroid|geometric centroid]] of ''C'':\n\n: <math>A = sd.</math>\n\nFor example, the surface area of the [[torus]] with minor [[radius]] ''r'' and major radius ''R'' is\n\n: <math>A = (2\\pi r)(2\\pi R) = 4\\pi^2 R r.</math>\n\n==The second theorem==\nThe second theorem states that the [[volume]] ''V'' of a [[solid of revolution]] generated by rotating a [[plane figure]] ''F'' about an external axis is equal to the product of the area ''A'' of ''F'' and the distance ''d'' traveled by the geometric centroid of ''F''. (Note that the centroid of ''F'' is usually different from the centroid of its boundary curve ''C''.) That is:\n\n: <math>V = Ad.</math>\n\nFor example, the volume of the [[torus]] with minor radius ''r'' and major radius ''R'' is\n\n: <math>V = (\\pi r^2)(2\\pi R) = 2\\pi^2 R r^2.</math>\n\nThis special case was derived by [[Johannes Kepler]] using infinitesimals.{{efn|Theorem XVIII of Kepler's ''Nova Stereometria Doliorum Vinariorum'' (1615):<ref>{{cite book|author-last=Kepler|author-first=Johannes|chapter=Nova Stereometria Doliorum Vinariorum|editor-last=Frisch|editor-first=Christian|title=Joannis Kepleri astronomi opera omnia|volume=4|page=582|location=Frankfurt|year=1870|orig-year=1615|access-date=2016-08-04|publisher=Heyder and Zimmer|chapter-url=https://archive.org/details/joanniskeplerias04kepl}}</ref> \"Omnis annulus sectionis circularis vel ellipticae est aequalis cylindro, cujus altitudo aequat longitudinem circumferentiae, quam centrum figurae circumductae descripsit, basis vero eadem est cum sectione annuli.\" Translation:<ref name=\"RdG2015\" /> \"Any ring whose cross-section is circular or elliptic is equal to a cylinder whose height equals the length of the circumference covered by the center of the figure during its circular movement, and whose base is equal to the section of the ring.\"}}\n\n===Proof===\nLet <math>A</math> be the area of <math>F</math>, <math>W</math> the solid of revolution of <math>F</math>, and <math>V</math> the volume of <math>W</math>. Suppose <math>F</math> starts in the <math>xz</math>-plane and rotates around the <math>z</math>-axis. The distance of the centroid of <math>F</math> from the <math>z</math>-axis is its <math>x</math>-coordinate\n\n:<math>R = \\frac{\\iint_F x\\,dA}{A},</math>\n\nand the theorem states that\n\n:<math>V = Ad = A \\cdot 2\\pi R = 2\\pi\\iint_F x\\,dA.</math>\n\nTo show this, let <math>F</math> be in the ''xz''-plane, [[Parametric equation|parametrized]] by <math>\\mathbf{\\Phi}(u,v) = (x(u,v),0,z(u,v))</math> for  <math>(u,v)\\in F^*</math>, a parameter region. Since <math>\\mathbf{\\Phi}</math> is essentially a mapping from <math>\\mathbb{R}^2</math> to <math>\\mathbb{R}^2</math>, the area of <math>F</math> is given by the [[Integration by substitution#Substitution for multiple variables|change of variables]] formula:\n\n:<math>A = \\iint_F dA = \\iint_{F^*} \\left|\\frac{\\partial(x,z)}{\\partial(u,v)}\\right|\\,du\\,dv = \\iint_{F^*} \\left|\\frac{\\partial x}{\\partial u}\\frac{\\partial z}{\\partial v} - \\frac{\\partial x}{\\partial v}\\frac{\\partial z}{\\partial u}\\right|\\,du\\,dv,</math>\n\nwhere <math>\\tfrac{\\partial(x,z)}{\\partial(u,v)}</math> is the [[determinant]] of the [[Jacobian matrix and determinant|Jacobian matrix]] of the change of variables. \n\nThe solid <math>W</math> has the [[torus|toroidal]] parametrization <math>\\mathbf{\\Phi}(u,v,\\theta) = (x(u,v)\\cos\\theta,x(u,v)\\sin\\theta,z(u,v))</math> for <math>(u,v,\\theta)</math> in the parameter region <math>W^*=F^*\\times [0,2\\pi]</math>; and its volume is\n\n:<math>V = \\iiint_W dV = \\iiint_{W^*} \\left|\\frac{\\partial(x,y,z)}{\\partial(u,v,\\theta)}\\right|\\,du\\,dv\\,d\\theta.</math>\n\nExpanding,\n\n: <math>\n\\begin{align}\n\\left|\\frac{\\partial(x,y,z)}{\\partial(u,v,\\theta)}\\right| & = \n\\left|\\det\\begin{bmatrix}\n\\frac{\\partial x}{\\partial u}\\cos\\theta & \\frac{\\partial x}{\\partial v}\\cos\\theta & -x\\sin\\theta \\\\[6pt]\n\\frac{\\partial x}{\\partial u}\\sin\\theta & \\frac{\\partial x}{\\partial v}\\sin\\theta &  x\\cos\\theta \\\\[6pt]\n\\frac{\\partial z}{\\partial u}           & \\frac{\\partial z}{\\partial v}           &  0\n\\end{bmatrix}\\right| \\\\[5pt]\n& = \\left|-\\frac{\\partial z}{\\partial v}\\frac{\\partial x}{\\partial u}\\,x + \\frac{\\partial z}{\\partial u}\\frac{\\partial x}{\\partial v}\\,x\\right|\n=\\ \\left|-x\\,\\frac{\\partial(x,z)}{\\partial(u,v)}\\right| = x\\left|\\frac{\\partial(x,z)}{\\partial(u,v)}\\right|.\n\\end{align}\n</math>\n\nThe last equality holds because the axis of rotation must be external to <math>F</math>, meaning <math>x \\geq 0</math>. Now,\n\n:<math>\n\\begin{align}\nV &= \\iiint_{W^*} \\left|\\frac{\\partial(x,y,z)}{\\partial(u,v,\\theta)}\\right|\\,du\\,dv\\,d\\theta = \\int_0^{2\\pi}\\!\\!\\!\\!\\iint_{F^*} x(u,v)\\left|\\frac{\\partial(x,z)}{\\partial(u,v)}\\right|\\,du\\,dv\\,d\\theta \\\\[6pt]\n& = 2\\pi\\iint_{F^*} x(u,v)\\left|\\frac{\\partial(x,z)}{\\partial(u,v)}\\right|\\,du\\,dv = 2\\pi\\iint_F x\\,dA\n\\end{align}\n</math>\n\nby change of variables.\n\n==Generalizations==\n\nThe theorems can be generalized for arbitrary curves and shapes, under appropriate conditions. \n\nGoodman & Goodman<ref name=generalizations>{{cite journal|last1=Goodman|first1=A. W.|last2=Goodman|first2=G.|title=Generalizations of the Theorems of Pappus|journal=The American Mathematical Monthly|volume=76|issue=4|pages=355–366|publisher=The American Mathematical Monthly|ref=generalizations|jstor=2316426|year=1969|doi=10.1080/00029890.1969.12000217}}</ref> generalize the second theorem as follows. If the figure ''F'' moves through space so that it remains [[perpendicular]] to the curve ''L'' traced by the centroid of ''F'', then it sweeps out a solid of volume ''V'' = ''Ad'', where ''A'' is the area of ''F'' and ''d'' is the length of ''L''. (This assumes the solid does not intersect itself.) In particular, ''F'' may rotate about its centroid during the motion. \n\nHowever, the corresponding generalization of the first theorem is only true if the curve ''L'' traced by the centroid lies in a plane perpendicular to the plane of ''C''.\n\n== In n-dimensions ==\nIn general, one can generate an <math>n</math> dimensional solid by rotating an <math>n-p</math> dimensional solid <math>F</math> around a <math>p</math> dimensional sphere. This is called an <math>n</math>-solid of revolution of species <math>p</math>. Let the <math>p</math>-th centroid of <math>F</math> be defined by \n\n<math>R = \\frac{\\iint_F x^p\\,dA}{A},</math>\n\nThen Pappus' theorems generalize to:<ref>{{Cite book|url=https://cds.cern.ch/record/254647|title=An introduction to the geometry of n dimensions|last=McLaren-Young-Sommerville|first=Duncan|date=1958|publisher=Dover|year=|isbn=|location=New York, NY|pages=|chapter=8.17 Extensions of Pappus' Theorem}}</ref><blockquote>Volume(<math>n</math>-solid of revolution of species <math>p</math>)\n\n= \n\nVolume(the generating <math>(n-p)</math>-solid) <math>\\times</math>\n\nSurface area(the <math>p</math>-sphere traced by the <math>p</math>-th centroid of the generating solid)</blockquote>and<blockquote>Surface area(<math>n</math>-solid of revolution of species <math>p</math>)\n\n= \n\nSurface area(the generating <math>(n-p)</math>-solid) <math>\\times</math>\n\nSurface area(the <math>p</math>-sphere traced by the <math>p</math>-th centroid of the generating solid)</blockquote>The original Pappus' theorems are the case with <math>n=3, p = 1</math>.\n\n== Footnotes ==\n{{notelist}}\n\n== References ==\n{{reflist}}\n\n==External links==\n{{commons category|Pappus-Guldinus theorem}}\n*{{MathWorld|title=Pappus's Centroid Theorem|urlname=PappussCentroidTheorem}}\n\n[[Category:Theorems in calculus]]\n[[Category:Geometric centers]]\n[[Category:Theorems in geometry]]\n[[Category:Area]]\n[[Category:Volume]]"
    },
    {
      "title": "Pick's theorem",
      "url": "https://en.wikipedia.org/wiki/Pick%27s_theorem",
      "text": "{{for|the theorem in complex analysis|Schwarz lemma#Schwarz–Pick theorem}}\n[[File:Pick-theorem.png|right|thumb|{{color|red|{{math|''i'' {{=}} 7}}}}, {{color|green|{{math|''b'' {{=}} 8}}}}, {{math|''A'' {{=}} {{color|red|''i''}} + {{sfrac|{{color|green|''b''}}|2}} − 1 {{=}} 10}}]]\n[[Image:coprime-lattice.svg|thumb|right|300px| The triangle with vertices at the lower left, lower right, and upper right points has {{math|''i'' {{=}} 12}} and {{math|''b'' {{=}} 14}}, giving by Pick's theorem {{math|''A'' {{=}} ''i'' + {{sfrac|''b''|2}} − 1 {{=}} 18}}; this is confirmed by the triangle area formula {{nowrap|{{sfrac|1|2}} × base × height}} = {{nowrap|{{sfrac|1|2}} × 9 × 4}} = 18.]]\n\nGiven a [[simple polygon]] constructed on a grid of equal-distanced points (i.e., points with [[integer]] coordinates) such that all the polygon's [[vertex (geometry)|vertices]] are grid points, '''Pick's theorem''' provides a simple [[formula]] for calculating the [[area]] {{mvar|A}} of this polygon in terms of the number {{mvar|i}} of ''lattice points in the interior'' located in the polygon and the number {{mvar|b}} of ''lattice points on the boundary'' placed on the polygon's perimeter:<ref>{{cite journal |last=Trainin |first=J. |title=An elementary proof of Pick's theorem |journal=[[Mathematical Gazette]] |volume=91 |issue=522 |date=November 2007 |pages=536–540}}</ref>\n\n:<math>A = i + \\frac{b}{2} - 1.</math>\n\nIn the example shown, we have {{math|''i'' {{=}} 7}} interior points  and {{math|''b'' {{=}} 8}} boundary points, so the area is {{mvar|A}}&nbsp;=&nbsp;7&nbsp;+&nbsp;{{sfrac|8|2}}&nbsp;−&nbsp;1 =&nbsp;7&nbsp;+&nbsp;4&nbsp;−&nbsp;1 =&nbsp;10 square units.\n\nNote that the theorem as stated above is only valid for ''simple'' polygons, i.e., ones that consist of a single, non-self-intersecting boundary (and thus does not contain holes). For a general polygon,  Pick's formula generalizes to<ref name=\":0\">{{Cite web|url=https://documents.kenyon.edu/math/GarbettJSenEx2011.pdf|title=Lattice Point Geometry: Pick’s Theorem and Minkowski’s Theorem, Senior Exercise in Mathematics|last=Garbett|first=Jennifer|date=November 18, 2010|website=|archive-url=https://web.archive.org/web/20170829044759/http://documents.kenyon.edu/math/GarbettJSenEx2011.pdf|archive-date=29 Aug 2017|dead-url=|access-date=}}</ref>\n\n<math>A = v - \\frac 1 2 e_b + h - 1</math>\n\nwhere <math>v</math> is the number of vertices both in and on the boundary of the polygon, <math>e_b</math> is the number of lattice edges on the boundary of the polygon, and <math>h</math> is the number of holes in the polygon.\n\nAs an example, consider the \"polygon\" made by connecting the points <math>(0, 0), (2, 0)</math>. It has 3 vertices, 0 holes, and 0 area. To make the formula work, it must have 4 edges. Thus, one just have to count each edge twice, \"once on each side\".\n\nThe result was first described by [[Georg Alexander Pick]] in 1899.<ref>{{cite journal |last=Pick |first=Georg |title=Geometrisches zur Zahlenlehre |journal=Sitzungsberichte des deutschen naturwissenschaftlich-medicinischen Vereines für Böhmen \"Lotos\" in Prag |series=(Neue Folge) |year=1899 |volume=19 |pages=311–319 |url=https://www.biodiversitylibrary.org/item/50207#page/327 |jfm=33.0216.01 }} [http://citebank.org/node/47270 CiteBank:47270]</ref> The [[Reeve tetrahedron]] shows that there is no analogue of Pick's theorem in three dimensions that expresses the volume of a polytope by counting its interior and boundary points. However, there is a generalization in higher dimensions via [[Ehrhart polynomial]]s.\n\n==Proof==\nConsider a polygon {{mvar|P}} and a triangle {{mvar|T}}, with one edge in common with {{mvar|P}}. Assume Pick's theorem is true for both {{mvar|P}} and {{mvar|T}} separately; we want to show that it is also true for the polygon {{mvar|PT}} obtained by adding {{mvar|T}} to {{mvar|P}}. Since {{mvar|P}} and {{mvar|T}} share an edge, all the boundary points along the edge in common are merged to interior points, except for the two endpoints of the edge, which are merged to boundary points. So, calling the number of boundary points in common {{mvar|c}}, we have<ref>{{cite book|last1=Beck|first1=Matthias|last2=Robins|first2=Sinai|date=2007|title=Computing the Continuous Discretely, Integer-point enumeration in polyhedra|series=[[Undergraduate Texts in Mathematics]]|location=New York|publisher=Springer-Verlag|ISBN=978-0-387-29139-0|MR=2271992|at=ch. 2}}</ref>\n\n:<math>i_{PT} = i_P + i_T + (c - 2)</math>\n\nand\n\n:<math>b_{PT} = b_P + b_T - 2(c - 2) - 2.</math>\n\nFrom the above follows\n\n:<math>i_P + i_T = i_{PT} - (c - 2)</math>\n\nand\n\n:<math>b_P + b_T = b_{PT} + 2(c - 2) + 2.</math>\n\nSince we are assuming the theorem for {{mvar|P}} and for {{mvar|T}} separately,\n\n: <math>\\begin{align}\nA_{PT} &= A_P + A_T\\\\\n&= \\left(i_P + \\frac{b_P}{2} - 1\\right) + \\left(i_T + \\frac{b_T}{2} - 1\\right)\\\\\n&= i_P + i_T + \\frac{b_P + b_T}{2} - 2\\\\\n&= i_{PT} - (c - 2) + \\frac{b_{PT} + 2(c - 2) + 2}{2} - 2\\\\\n&= i_{PT} + \\frac{b_{PT}}{2} - 1.\n\\end{align}</math>\n\nTherefore, if the theorem is true for polygons constructed from {{mvar|n}} triangles, the theorem is also true for polygons constructed from {{math|''n'' + 1}} triangles. For general [[polytope]]s, it is well known that they can always be [[triangulation (geometry)|triangulated]]. That this is true in dimension 2 is an easy fact. To finish the proof by [[mathematical induction]], it remains to show that the theorem is true for triangles.  The verification for this case can be done in these short steps:\n\n* observe that the formula holds for any unit square (with vertices having integer coordinates);\n* deduce from this that the formula is correct for any [[rectangle]] with sides [[parallel (geometry)|parallel]] to the axes;\n* deduce it, now, for right-angled triangles obtained by cutting such rectangles along a [[diagonal]];\n* now any triangle can be turned into a rectangle by attaching such right triangles; since the formula is correct for the right triangles and for the rectangle, it also follows for the original triangle.\n\nThe last step uses the fact that if the theorem is true for the polygon {{mvar|PT}} and for the triangle {{mvar|T}}, then it's also true for {{mvar|P}}; this can be seen by a calculation very much similar to the one shown above.\n\n== Inequality for convex sets ==\nLet <math>C</math> be a bounded, convex region in  <math>\\mathbb R^2</math>, not necessarily closed. Then \n\n<math>|L(C)| \\le Area(C) + \\frac 1 2 Perimeter(C) + 1</math>.<ref name=\":0\" />\n\nwhere <math>L(C)</math> is the set of lattice points in <math>C</math>, and <math>|L(C)|</math>  is the number of them. Equality holds iff <math>C</math> is a closed lattice polygon.\n\nThe proof is by taking the convex hull <math>\\bar{C}</math> of <math>L(C)</math>, which should be thought of as a lattice approximation of <math>C</math>, then apply Pick's theorem to it.\n\n<math>\\begin{align}\n|L(C)| = |L(\\bar C)| &= Area(\\bar C) + \\frac 1 2 B(\\bar C) + 1 \\\\\n&\\le Area(\\bar C) + \\frac 1 2 Perimeter(\\bar C) + 1 \n\\\\\n&\\le Area(C) + \\frac 1 2 Perimeter(C) + 1\n\\end{align} </math>\n\nwhere <math>B(\\bar C)</math> is the number of boundary points of <math>\\bar{C}</math>, which is equal to the number of edges of it, and since each edge is at least length 1, <math>B(\\bar C)\\le Perimeter(\\bar C)</math>. And the step <math>Perimeter(\\bar C) \\le Perimeter(C)</math> uses the property that, between two nested, convex, closed curves, the inner one is shorter, which is an application of [[Crofton formula#Applications|Crofton formula]].\n\nNote that this still works in the degenerate case when <math>L(C)</math> is on the same line. One just have to count each edge twice, \"once on each side\".\n\n==See also==\n*[[Integer points in convex polyhedra]]\n*[[Steinhaus longimeter]]\n*[[Farey sequence]]\n\n==References==\n{{reflist}}\n\n==External links==\n{{commons category}}\n* [http://www.cut-the-knot.org/ctk/Pick.shtml Pick's Theorem (Java)] at [[cut-the-knot]]\n* [http://www.mcs.drexel.edu/~crorres/Archimedes/Stomachion/Pick.html Pick's Theorem]\n* [http://www.geometer.org/mathcircles/pick.pdf Pick's Theorem proof] by Tom Davis\n* [http://demonstrations.wolfram.com/PicksTheorem/ Pick's Theorem] by [[Ed Pegg, Jr.]], the [[Wolfram Demonstrations Project]].\n* {{MathWorld |title=Pick's Theorem  |urlname=PicksTheorem}}\n\n[[Category:Digital geometry]]\n[[Category:Lattice points]]\n[[Category:Euclidean plane geometry]]\n[[Category:Theorems in geometry]]\n[[Category:Area]]\n[[Category:Polygons]]\n[[Category:Articles containing proofs]]\n[[Category:Analytic geometry]]"
    },
    {
      "title": "Pizza theorem",
      "url": "https://en.wikipedia.org/wiki/Pizza_theorem",
      "text": "[[File:Pizza theorem 1.svg|thumb|right|250px|8 sectors: yellow area = purple area]]\n[[File:Pizza proof without words.svg|thumb|right|250px|[[Proof without words]] for 8 sectors by {{harvtxt|Carter|Wagon|1994a}}.]]\nIn elementary [[geometry]], the '''pizza theorem''' states the equality of two areas that arise when one partitions a [[Disk (mathematics)|disk]] in a certain way.\n\nLet ''p'' be an interior point of the disk, and let ''n'' be a multiple of 4 and greater than or equal to 8. Form ''n'' sectors of the disk with equal angles by choosing an arbitrary line through ''p'', rotating the line {{nowrap|{{sfrac|''n''|2}}&nbsp;−&nbsp;1}} times by an angle of {{sfrac|2{{pi}}|''n''}} [[radian]]s, and slicing the disk on each of the resulting {{sfrac|''n''|2}} lines. Number the sectors consecutively in a clockwise or anti-clockwise fashion. Then the pizza theorem states that:\n\n:'' The sum of the areas of the odd-numbered sectors equals the sum of the areas of the even-numbered sectors'' {{harv|Upton|1968}}.\n\nThe pizza theorem is so called because it mimics a traditional [[pizza]] slicing technique. It shows that, if two people share a pizza sliced in this way by taking alternating slices, then they each get an equal amount of pizza.\n\n==History==\nThe pizza theorem was originally proposed as a challenge problem by {{harvtxt|Upton|1968}}. The published solution to this problem, by Michael Goldberg, involved direct manipulation of the algebraic expressions for the areas of the sectors.\n{{harvtxt|Carter|Wagon|1994a}} provide an alternative proof by [[Dissection puzzle|dissection]]. They show how to partition the sectors into smaller pieces so that each piece in an odd-numbered sector has a [[congruence (geometry)|congruent]] piece in an even-numbered sector, and vice versa. {{harvtxt|Frederickson|2012}} gave a family of dissection proofs for all cases (in which the number of sectors {{nowrap|is 8, 12, 16, ...}}).\n\n==Generalizations==\n[[File:Pizza theorem2.jpg|thumb|right|250px|12 sectors: green area = orange area]]\nThe requirement that the number of sectors be a multiple of four is necessary: as [[Don Coppersmith]] showed, dividing a disk into four sectors, or a number of sectors that is not divisible by four, does not in general produce equal areas. {{harvtxt|Mabry|Deiermann|2009}} answered a problem of {{harvtxt|Carter|Wagon|1994b}} by providing a more precise version of the theorem that determines which of the two sets of sectors has greater area in the cases that the areas are unequal. Specifically, if the number of sectors is 2 (mod 8) and no slice passes through the center of the disk, then the subset of slices containing the center has smaller area than the other subset, while if the number of sectors is 6 (mod 8) and no slice passes through the center, then the subset of slices containing the center has larger area. An odd number of sectors is not possible with straight-line cuts, and a slice through the center causes the two subsets to be equal regardless of the number of sectors.\n\n{{harvtxt|Mabry|Deiermann|2009}} also observe that, when the pizza is divided evenly, then so is its crust (the crust may be interpreted as either the perimeter of the disk or the area between the boundary of the disk and a smaller circle having the same center, with the cut-point lying in the latter's interior), and since the disks bounded by both circles are partitioned evenly so is their difference. However, when the pizza is divided unevenly, the diner who gets the most pizza area actually gets the least crust.\n\nAs {{harvtxt|Hirschhorn|Hirschhorn|Hirschhorn|Hirschhorn|1999}} note, an equal division of the pizza also leads to an equal division of its toppings, as long as each topping is distributed in a disk (not necessarily concentric with the whole pizza) that contains the central point ''p'' of the division into sectors.\n\n==Related results==\n{{harvtxt|Hirschhorn|Hirschhorn|Hirschhorn|Hirschhorn|1999}} show that a pizza sliced in the same way as the pizza theorem, into a number ''n'' of sectors with equal angles where ''n'' is divisible by four, can also be shared equally among ''n''/4 people. For instance, a pizza divided into 12 sectors can be shared equally by three people as well as by two; however, to accommodate all five of the Hirschhorns, a pizza would need to be divided into 20 sectors.\n\n{{harvtxt|Cibulka|Kynčl|Mészáros|Stolař|2010}} and {{harvtxt|Knauer|Micek|Ueckerdt|2011}} study the [[game theory]] of choosing free slices of pizza in order to guarantee a large share, a problem posed by Dan Brown and [[Peter Winkler]]. In the version of the problem they study, a pizza is sliced radially (without the guarantee of equal-angled sectors) and two diners alternately choose pieces of pizza that are adjacent to an already-eaten sector. If the two diners both try to maximize the amount of pizza they eat, the diner who takes the first slice can guarantee a 4/9 share of the total pizza, and there exists a slicing of the pizza such that he cannot take more. The [[fair division]] or cake cutting problem considers similar games in which different players have different criteria for how they measure the size of their share; for instance, one diner may prefer to get the most pepperoni while another diner may prefer to get the most cheese.\n\n==See also==\nOther mathematical results related to pizza-slicing involve the [[lazy caterer's sequence]], a sequence of integers that counts the maximum number of pieces of pizza that one can obtain by a given number of straight slices, and the [[ham sandwich theorem]], a result about slicing three-dimensional objects whose two-dimensional version implies that any pizza, no matter how misshapen, can have its area and its crust length simultaneously bisected by a single carefully chosen straight-line cut, and whose three-dimensional version implies that a plane cut exists that equally shares base, tomato and cheese.\n\n==References==\n*{{citation|first1=Larry|last1=Carter|first2=Stan|last2=Wagon|author2-link=Stan Wagon|title=Proof without Words: Fair Allocation of a Pizza|journal=Mathematics Magazine|volume=67|issue=4|pages=267|year=1994a|jstor=2690845|doi=10.1080/0025570X.1994.11996228}}.\n*{{citation|last1=Carter|first1=Larry|last2=Wagon|first2=Stan|author2-link=Stan Wagon|title=Problem 1457|journal=Mathematics Magazine|volume=67|issue=4|pages=303–310|jstor=2690855|year=1994b}}.\n*{{citation|last1=Cibulka|first1=Josef|last2=Kynčl|first2=Jan|last3=Mészáros|first3=Viola|last4=Stolař|first4=Rudolf|last5=Valtr|first5=Pavel|contribution=Solution of Peter Winkler's pizza problem|year=2010|arxiv=0812.4322|title=Fete of Combinatorics and Computer Science|publisher=János Bolyai Mathematical Society and Springer-Verlag|series=Bolyai Society Mathematical Studies|volume=20|pages=63–93|doi=10.1007/978-3-642-13580-4_4|isbn=978-3-642-13579-8}}.\n*{{citation|last1=Hirschhorn|first1=J.|last2=Hirschhorn|first2=M. D.|last3=Hirschhorn|first3=J. K.|last4=Hirschhorn|first4=A. D.|last5=Hirschhorn|first5=P. M. Hirschhorn|title=The pizza theorem|journal=Austral. Math. Soc. Gaz.|volume=26|year=1999|pages=120–121|url=http://www.maths.unsw.edu.au/~mikeh/webpapers/paper57.pdf}}.\n*{{citation|last1=Frederickson|first1=Greg|title=The Proof Is in the Pizza|year=2012|journal=[[Mathematics Magazine]]|volume=85|issue=1|pages=26–33|doi=10.4169/math.mag.85.1.26|jstor=10.4169/math.mag.85.1.26}}.\n*{{citation|last1=Knauer|first1=Kolja|last2=Micek|first2=Piotr|last3=Ueckerdt|first3=Torsten|title=How to eat 4/9 of a pizza|year=2011|arxiv=0812.2870|journal=[[Discrete Mathematics (journal)|Discrete Mathematics]]|volume=311|issue=16|pages=1635–1645|doi=10.1016/j.disc.2011.03.015}}.\n*{{citation|first1=Rick|last1=Mabry|first2=Paul|last2=Deiermann|jstor=40391118|title=Of Cheese and Crust: A Proof of the Pizza Conjecture and Other Tasty Results|journal=[[American Mathematical Monthly]]|volume=116|issue=5|pages=423–438|year=2009|doi=10.4169/193009709x470317}}.\n*{{citation|last=Ornes|first=Stephen|title=The perfect way to slice a pizza|journal=[[New Scientist]]|date=December 11, 2009|url=https://www.newscientist.com/article/mg20427381.500-the-perfect-way-to-slice-a-pizza.html}}.\n*{{citation|last=Upton|first=L. J.|title=Problem 660|journal=Mathematics Magazine|year=1968|volume=41|issue=1|pages=42|jstor=2687962|postscript=. Solution by Michael Goldberg}}.\n\n==External links==\n*{{MathWorld|urlname=PizzaTheorem|title=Pizza Theorem}}\n*{{citation|url=http://www.math.uni-bielefeld.de/~sillke/PUZZLES/pizza-theorem|title=Pizza Theorem|last=Sillke|first=Torsten|accessdate=2009-11-24}}\n\n[[Category:Area]]\n[[Category:Circles]]\n[[Category:Pizza]]\n[[Category:Theorems in plane geometry]]\n[[Category:Proof without words]]"
    },
    {
      "title": "Routh's theorem",
      "url": "https://en.wikipedia.org/wiki/Routh%27s_theorem",
      "text": "{{short description|A geometrical theorem of triangles}}\n[[File:Routh theorem2.svg|thumb|upright=1.0|Routh's theorem]]\n\nIn [[geometry]], '''Routh's theorem''' determines the ratio of areas between a given triangle and a triangle formed by the pairwise intersections of three [[cevian]]s.  The theorem states that if in [[triangle]] <math>ABC</math> points <math>D</math>, <math>E</math>, and <math>F</math> lie on segments <math>BC</math>, <math>CA</math>, and <math>AB</math>, then writing <math>\\tfrac{CD}{BD} </math><math>= x</math>, <math>\\tfrac{AE}{CE} </math><math>= y</math>, and <math>\\tfrac{BF}{AF} </math><math>= z</math>, the signed [[area]] of the triangle formed by the cevians <math>AD</math>, <math>BE</math>, and <math>CF</math> is the area of triangle <math>ABC</math> times\n\n: <math>\\frac{(xyz - 1)^2}{(xy + y + 1)(yz + z + 1)(zx + x + 1)}.</math>\n\nThis theorem was given by [[Edward John Routh]] on page 82 of his ''Treatise on Analytical Statics with Numerous Examples'' in 1896. The particular case <math> x = y = z = 2</math> has become popularized as the [[one-seventh area triangle]]. The <math> x = y = z = 1</math> case implies that the three [[median (geometry)|median]]s are concurrent (through the [[centroid]]).\n\n==Proof==\n[[File:Routh theorem 1.svg|thumb|upright=1.0|Routh's theorem]]\n\nSuppose that the area of triangle<math>ABC</math> is 1. For triangle<math>ABD</math> and line<math>FRC</math> using [[Menelaus's theorem]], We could obtain:\n:<math>\\frac{AF}{FB} \\times \\frac{BC}{CD} \\times \\frac{DR}{RA} = 1</math>\nThen<math>\\frac{DR}{RA} = \\frac{BF}{FA} \\times \\frac{DC}{CB} = \\frac{zx}{x+1}</math>\nSo the area of triangle<math>ARC</math> is:\n:<math>S_{ARC} = \\frac{AR}{AD} S_{ADC} = \\frac{AR}{AD} \\times \\frac{DC}{BC} S_{ABC} = \\frac{x}{zx+x+1}</math>\nSimilarly, we could know: <math>S_{BPA} = \\frac{y}{xy+y+1}</math> and <math>S_{CQB} = \\frac{z}{yz+z+1}</math>\nThus the area of triangle<math>PQR</math> is:\n:<math>\\displaystyle S_{PQR} = S_{ABC} - S_{ARC} - S_{BPA} - S_{CQB} </math>\n:<math>= 1 - \\frac{x}{zx+x+1} - \\frac{y}{xy+y+1} - \\frac{z}{yz+z+1} </math>\n: <math>=\\frac{(xyz - 1)^2}{(xz + x + 1)(yx + y + 1)(zy + z + 1)}.</math>\n\n== Citations ==\nThe citation commonly given for Routh's Theorem is Routh's ''Treatise on Analytical Statics with Numerous Examples'', Volume 1, Chap. IV, in the [https://archive.org/details/atreatiseonanal00routgoog second edition] of 1896\n[https://archive.org/stream/atreatiseonanal00routgoog#page/n98/mode/1up p. 82], possibly because that edition has been easier to hand. However, Routh gave the theorem already in the [https://archive.org/details/cu31924001080237 first edition] of 1891, Volume 1, Chap. IV, [https://archive.org/stream/cu31924001080237#page/n102/mode/1up p. 89]. Although there is a change in pagination between the editions, the wording of the relevant footnote remained the same.\n\nRouth concludes his extended footnote with a ''caveat'':\n\n: ''The author has not met with these expressions for the areas of two triangles that often occur. He has therefore placed them here in order that the argument in the text may be more easily understood.''\n\nPresumably Routh felt those circumstances had not changed in the five years between editions. On the other hand, the title of Routh's book had been used earlier by [[Isaac Todhunter]]; both had been coached by [[William Hopkins]].\n\nAlthough Routh published the theorem in his book,  that is not the first published statement.  It is stated and proved as rider (vii) on page 33 of Solutions of the Cambridge Senate-house Problems and Riders for the Year 1878, i.e., the mathematical tripos of that year, and the link is https://archive.org/details/solutionscambri00glaigoog.  It is stated that the author of the problems with roman numerals is [[Glaisher]].\nRouth was a famous [[Mathematical Tripos]] coach when his book came out and was surely familiar with the content of the 1878 tripos examination.  Thus his statement ''The author has not met with these expressions for the areas of two triangles that often occur. '' is puzzling.\n\nProblems in this spirit have a long history in [[recreational mathematics]] and mathematical [[pedagogy|paedagogy]], perhaps one of the oldest instances of being the determination of the proportions of the fourteen regions of the [[Stomachion]] board. With Routh's [[Cambridge]] in mind, the ''[[one-seventh_area_triangle|one-seventh-area triangle]]'', associated in some accounts with [[Richard Feynman]], shows up, for example, as Question 100, [https://archive.org/stream/euclidselements00unkngoog#page/n91/mode/1up p. 80], in ''Euclid's Elements of Geometry ([https://archive.org/details/euclidselements01pottgoog Fifth School Edition])'', by [[Robert Potts]] (1805--1885,) of Trinity College, published in 1859; compare also his Questions 98, 99, on the same page. Potts stood twenty-sixth [[Wrangler (University of Cambridge)|Wrangler]] in 1832 and then, like Hopkins and Routh, coached at Cambridge. Pott's expository writings in geometry were recognized by a [http://babel.hathitrust.org/cgi/pt?id=mdp.39015065371117#page/ii/mode/1up medal] at the International Exhibition of 1862, as well as by an Hon. LL.D. from the [[College of William & Mary|College of William and Mary]], [[Williamsburg,_Virginia|Williamsburg]], [[Virginia]].\n\n== References ==\n* [[Murray S. Klamkin]] and A. Liu (1981) \"Three more proofs of Routh's theorem\", ''[[Crux Mathematicorum]]'' 7:199&ndash;203.\n* [[H. S. M. Coxeter]] (1969) ''Introduction to Geometry'', statement p. 211, proof pp. 219&ndash;20, 2nd edition, Wiley, New York.\n* J. S. Kline and D. Velleman (1995) \"Yet another proof of Routh's theorem\" (1995) ''[[Crux Mathematicorum]]'' 21:37&ndash;40\n* [[Ivan Niven]] (1976) \"A New Proof of Routh's Theorem\", [[Mathematics Magazine]] 49(1): 25–7, {{doi|10.2307/2689876}}\n* Jay Warendorff, [http://demonstrations.wolfram.com/RouthsTheorem/ Routh's Theorem], [[The Wolfram Demonstrations Project]]. \n* {{MathWorld |title=Routh's Theorem |urlname=RouthsTheorem}}\n* [http://www.mathpages.com/home/kmath652/kmath652.htm Routh's Theorem by Cross Products] at MathPages\n* Ayoub, Ayoub B. (2011/2012) \"Routh's theorem revisited\", ''Mathematical Spectrum'' 44 (1): 24-27.\n\n[[Category:Triangle geometry]]\n[[Category:Area]]\n[[Category:Theorems in plane geometry]]\n[[Category:Affine geometry]]"
    },
    {
      "title": "Shoelace formula",
      "url": "https://en.wikipedia.org/wiki/Shoelace_formula",
      "text": "[[File:Shoelace3.png|thumb]]\n\nThe '''shoelace formula''' or '''shoelace algorithm''' (also known as '''[[Carl Friedrich Gauss|Gauss]]'s area formula''' and the '''[[surveying|surveyor]]'s formula'''<ref name=\"Braden\">{{cite journal|author=Bart Braden|title=The Surveyor’s Area Formula|journal=The College Mathematics Journal|volume=17|issue=4|year=1986|pages=326–337|url=http://www.maa.org/sites/default/files/pdf/pubs/Calc_Articles/ma063.pdf|doi=10.2307/2686282}}</ref>) is a mathematical [[algorithm]] to determine the [[area]] of a [[simple polygon]] whose vertices are described by their [[Cartesian coordinates]] in the plane.<ref name=\"mathreference\" /> The user [[Cross product|cross-multiplies]] corresponding coordinates to find the area encompassing the polygon, and subtracts it from the surrounding polygon to find the area of the polygon within. It is called the shoelace formula because of the constant cross-multiplying for the coordinates making up the polygon, like tying shoelaces.<ref name=\"mathreference\">{{cite web|last=Dahlke|first=Karl|title=Shoelace Formula|url=http://www.mathreference.com/la-det,shoe.html|accessdate=9 June 2008}}</ref> It is also sometimes called the '''shoelace method'''.  It has applications in surveying and forestry,<ref name=\"Pretzsch\">Hans Pretzsch, ''[https://books.google.com/books?id=ZLNZMCSuUAQC&pg=PA232 Forest Dynamics, Growth and Yield: From Measurement to Model]'', Springer, 2009, {{isbn|3-540-88306-1}}, p. 232.</ref> among other areas.\n\nThe formula was described by Meister (1724-1788) in 1769<ref>{{citation\n | last = Meister | first = A. L. F.\n | journal = Nov. Com. Gött.\n | language = Latin\n | page = 144\n | title = Generalia de genesi figurarum planarum et inde pendentibus earum affectionibus\n | url = https://books.google.com/books?id=fOE_AAAAcAAJ\n | volume = 1\n | year = 1769}}.</ref> and by [[Carl Friedrich Gauss|Gauss]] in 1795.{{Full citation needed|date=March 2018}} It can be verified by dividing the polygon into triangles, and can be considered to be a special case of [[Green's theorem]].\n\nThe area formula is derived by taking each edge ''AB'', and calculating the area of triangle ''ABO'' with a vertex at the origin ''O'', by taking the cross-product (which gives the area of a parallelogram) and dividing by 2. As one wraps around the polygon, these triangles with positive and negative area will overlap, and the areas between the origin and the polygon will be cancelled out and sum to 0, while only the area inside the reference triangle remains. This is why the formula is called the Surveyor's Formula, since the \"surveyor\" is at the origin; if going counterclockwise, positive area is added when going from left to right and negative area is added when going from right to left, from the perspective of the origin.{{citation needed|date=March 2018}}\n\nThe area formula can also be applied to self-overlapping polygons since the meaning of area is still clear even though self-overlapping polygons are not generally simple.<ref>{{citation\n | author = P.W. Shor\n | author2 = C.J. Wan Wyk\n | journal = Comput. Geom. Theory Appl.\n | pages = 31-50.\n | title = Detecting and decomposing self-overlapping curves\n | volume = 2(1)\n | year = 1992}}</ref> Furthermore, a self-overlapping polygon can have multiple \"interpretations\" but the Shoelace formula can be used to show that the polygon's area is the same regardless of the interpretation.<ref>{{cite conference\n | author = Ralph P. Boland |\n | author2 = Jorge Urrutia\n | conference = 12th Canadian Conference on Computational Geometry.\n | pages = 159-162\n | title = Polygon Area Problems\n | year = 2000}}</ref>\n==Statement==\nThe formula can be represented by the expression\n\n: <math> \\begin{align} \\mathbf{A} & = {1 \\over 2} \\Big | \\sum_{i=1}^{n-1} x_iy_{i+1} + x_ny_1 - \\sum_{i=1}^{n-1} x_{i+1}y_i - x_1y_n \\Big | \\\\\n & = {1 \\over 2}|x_1y_2 + x_2y_3 + \\cdots + x_{n-1}y_n + x_ny_1 - x_2y_1 - x_3y_2 - \\cdots - x_ny_{n-1} - x_1y_n| \\\\ \\end{align} </math>\n\nwhere\n\n* ''A'' is the area of the polygon,\n* ''n'' is the number of sides of the polygon, and\n* (''x''<sub>''i''</sub>,&nbsp;''y''<sub>''i''</sub>), ''i''&nbsp;=&nbsp;1,&nbsp;2,...,&nbsp;''n'' are the vertices (or \"corners\") of the polygon.\n\nAlternatively<ref name=\"Pretzsch\" /><ref name=\"AoPSWiki\">[http://www.artofproblemsolving.com/wiki/index.php?title=Shoelace_Theorem Shoelace Theorem], ''Art of Problem Solving Wiki''.</ref><ref name=\"Wolfram MathWorld\">{{Cite web|url=http://mathworld.wolfram.com/PolygonArea.html|title=Polygon Area|accessdate=24 July 2012|last=Weisstein|first=Eric W|authorlink=Eric W. Weisstein|work=Wolfram MathWorld}}</ref>\n\n: <math>\\mathbf{A} = {1 \\over 2} \\Big | \\sum_{i=1}^{n} x_i(y_{i+1}-y_{i-1}) \\Big | = {1 \\over 2} \\Big | \\sum_{i=1}^{n} y_i(x_{i+1}-x_{i-1}) \\Big | = {1 \\over 2} \\Big | \\sum_{i=1}^{n} x_iy_{i+1}-x_{i+1}y_i \\Big | = {1 \\over 2} \\Big | \\sum_{i=1}^{n} (x_{i+1}+x_i)(y_{i+1}-y_{i}) \\Big | = {1 \\over 2} \\Big | \\sum_{i=1}^{n} \\det\\begin{pmatrix} x_i & x_{i+1} \\\\ y_i & y_{i+1} \\end{pmatrix} \\Big | </math>\n\nwhere\n''x''<sub>''n''+1</sub> = ''x''<sub>1</sub> and ''x''<sub>0</sub> = ''x''<sub>''n''</sub>,\nas well as\n''y''<sub>''n''+1</sub> = ''y''<sub>1</sub> and ''y''<sub>0</sub> = ''y''<sub>''n''</sub>.\n\nIf the points are labeled sequentially in the counterclockwise direction, then the sum of the above [[determinant]]s is positive and the absolute value signs can be omitted;<ref name=\"Braden\" /> if they are labeled in the clockwise direction, the sum of the determinants will be negative. This is because the formula can be viewed as a special case of [[Green's Theorem]].\n\nA particularly concise statement of the formula can be given in terms of the [[exterior algebra]]. If <math> v_1, \\dots, v_n </math> are the \nconsecutive vertices of the polygon (regarded as vectors in \nthe Cartesian plane) then \n\n: <math> A = \\frac{1}{2} \\cdot \\left|  \\sum_{i=1}^{n-1} v_i \\wedge v_{i+1}  \\right|.  </math>\n\n==Proofs==\n\n===Proof for a triangle===\n[[File:Triangle area from coordinates JCB.jpg|thumb|right|400px|Given the coordinates of a triangle, find its area <math>\\mathbf{A}</math>.]]\n\nReferring to the figure, let <math>\\mathbf{A}</math> be the area of the triangle whose vertices are given by the coordinates <math>(x_1,y_1), (x_2,y_2),</math> and <math>(x_3,y_3).</math> Draw the minimum area rectangle around the triangle so its sides are parallel to the <math>x</math> or <math>y</math> axes. At least one vertex of the triangle will be on a corner of the rectangle. In the figure, the areas of the three surrounding triangles are <math>\\mathbf{C}, \\mathbf{D},</math> and <math>\\mathbf{E}.</math> Obviously <math>\\mathbf{A}</math> is equal to the area of the rectangle (call it <math>\\mathbf{R}</math>) minus the areas of the other three triangles. The equation describing this relationship is\n\n::<math>\\mathbf{A}=\\mathbf{R}-\\mathbf{C}-\\mathbf{D}-\\mathbf{E}</math>\n\nBy inspection of the figure it can be seen that the areas are given by\n\n::<math>\\mathbf{R}=(x_3-x_2)(y_1-y_3)=(x_3y_1+x_2y_3)-(x_3y_3+x_2y_1)</math>\n\n::<math>-\\mathbf{C}=-\\frac{1}{2}(x_3-x_2)(y_2-y_3)=\\frac{1}{2}(-x_3y_2-x_2y_3)+\\frac{1}{2}(x_3y_3+x_2y_2)</math>\n\n::<math>-\\mathbf{D}=-\\frac{1}{2}(x_3-x_1)(y_1-y_3)=\\frac{1}{2}(-x_3y_1-x_1y_3)+\\frac{1}{2}(x_3y_3+x_1y_1)</math>\n\n::<math>-\\mathbf{E}=-\\frac{1}{2}(x_1-x_2)(y_1-y_2)=\\frac{1}{2}(-x_1y_1-x_2y_2)+\\frac{1}{2}(x_1y_2+x_2y_1)</math>\n\nCollecting terms and rearranging yields\n\n::<math>\\mathbf{A}=\\frac{1}{2}((x_2y_3-x_3y_2)-(x_1y_3-x_3y_1)+(x_1y_2-x_2y_1))</math>\n\nwhich can be written as a determinant\n\n::<math>\\mathbf{A}=\\frac{1}{2}\\begin{vmatrix}1&1&1\\\\x_1&x_2&x_3\\\\y_1&y_2&y_3\\end{vmatrix}</math>\n\nIf the coordinates are written in a clockwise order, the value of the determinant will be <math>-\\mathbf{A}.</math>\n\nRearranging another way\n\n::<math>\\mathbf{A}=\\frac{1}{2}|x_1y_2+x_2y_3+x_3y_1-x_2y_1-x_3y_2-x_1y_3|</math>\n\nwhich is the form of the shoelace formula. This formula can be extended to find the area of any polygon since a simple polygon can be divided into triangles.\n\n[[File:Quadrilateral area JCB.jpg|thumb|right|400px|Given the coordinates of a quadrilateral, find its area <math>\\mathbf{A}_\\text{quad.}</math>.]]\n\n===Proof for a quadrilateral and general polygon===\n\nFinding the area of a quadrilateral demonstrates how the shoelace formula is generalized to any polygon by dividing the polygon into triangles. Consider the figure of a quadrilateral whose coordinates are labeled in counterclockwise order. The quadrilateral is divided into two triangles with areas <math>\\mathbf{A}</math> and <math>\\mathbf{B}.</math> Using the triangle formula on each triangle we get\n\n::<math>\\mathbf{A}=\\frac{1}{2}(x_1y_2+x_2y_3+x_3y_1-x_2y_1-x_3y_2-x_1y_3)</math>\n\n::<math>\\mathbf{B}=\\frac{1}{2}(x_1y_3+x_3y_4+x_4y_1-x_3y_1-x_4y_3-x_1y_4)</math>\n\nSince both triangles were traced in a counterclockwise direction, both areas are positive and we get the area of the quadrilateral by adding the two areas. The last positive term and the last negative term of <math>\\mathbf{A}</math> cancel with the first positive term and the first negative term of <math>\\mathbf{B}</math> giving\n\n::<math>\\mathbf{A}_\\text{quad.}=\\frac{1}{2}(x_1y_2+x_2y_3+x_3y_4+x_4y_1-x_2y_1-x_3y_2-x_4y_3-x_1y_4)</math>\n\n== Examples ==\n\nThe user must know the points of the polygon in a Cartesian plane. For example, take a [[triangle]] with coordinates {(2,&nbsp;1),&nbsp;(4,&nbsp;5),&nbsp;(7,&nbsp;8)}. Take the first ''x''-coordinate and multiply it by the second ''y''-value, then take the second ''x''-coordinate and multiply it by the third ''y''-value, and repeat as many times until it is done for all wanted points. This can be defined by this formula:<ref>{{cite book|title=Geometry for Enjoyment and Challenge|year=1991|publisher=McDougal Littell|isbn=0-86609-965-4|author=Richard Rhoad|edition=new|author2=George Milauskas |author3=Robert Whipple |pages=717–718}}</ref>\n\n: <math> \\mathbf{A}_\\text{tri.} = {1 \\over 2}|x_1y_2 + x_2y_3 + x_3y_1 - x_2y_1 - x_3y_2 - x_1y_3| </math>\n\nfor ''x''<sub>''i''</sub> and ''y''<sub>''i''</sub> representing each respective coordinate. This formula is just the expansion of those given above for the case n = 3. Using it, one can find that the area of the triangle equals one half of the [[absolute value]] of 10&nbsp;+&nbsp;32&nbsp;+&nbsp;7&nbsp;−&nbsp;4&nbsp;−&nbsp;35&nbsp;−&nbsp;16, which equals 3. The number of  variables depends on the number of sides of the [[polygon]]. For example, a [[pentagon]] will be defined up to ''x''<sub>5</sub> and ''y''<sub>5</sub>:\n\n: <math> \\mathbf{A}_\\text{pent.} = {1 \\over 2}|x_1y_2 + x_2y_3 + x_3y_4 + x_4y_5 + x_5y_1 - x_2y_1 - x_3y_2 - x_4y_3 - x_5y_4 - x_1y_5|</math>\n\nA [[quadrilateral]] will be defined up to ''x''<sub>4</sub> and ''y''<sub>4</sub>:\n\n: <math> \\mathbf{A}_\\text{quad.} = {1 \\over 2}|x_1y_2 + x_2y_3 +x_3y_4 + x_4y_1 - x_2y_1 - x_3y_2 - x_4y_3 - x_1y_4| </math>\n\n==More complex example==\nConsider the polygon defined by the points (3,4), (5,11), (12,8), (9,5), and (5,6), and illustrated in the following diagram:\n\n[[File:polygon area formula (English).svg|Figure|Figure of this example]]\n\nThe area of this polygon is:\n\n: <math>\n\\begin{align}\n\\mathbf{A} & = {1 \\over 2}|3 \\times 11 + 5 \\times 8 + 12 \\times 5 + 9 \\times 6 + 5 \\times 4 \\\\\n& {} \\qquad {} - 4 \\times 5 - 11 \\times 12 - 8 \\times 9 - 5 \\times 5 - 6 \\times 3| \\\\[10pt]\n& = {60 \\over 2} = 30\n\\end{align}\n</math>\n\n==Etymology==\n\n[[File:Shoelace3.png|thumb]]\n\nThe reason this formula is called the shoelace formula is because of a common method used to evaluate it. This method uses [[matrix (mathematics)|matrices]]. As an example, choose the triangle with vertices (2,4), (3,−8), and (1,2). Then construct the following matrix by “walking around” the triangle and ending with the initial point.<ref>IMSA JHMC Guide, Page. 10 \"Shoelace\" by Cindy Xi</ref>\n\n:: <math> \\begin{bmatrix} 2 & 4 \\\\ 3 & -8 \\\\ 1 & 2 \\\\ 2 & 4 \\end{bmatrix}</math>\n\nFirst, draw diagonal down and to the right slashes (as shown below),\n:&nbsp;&nbsp;[[File:ShoelaceMatrix2.GIF]]\nand multiply the two numbers connected by each slash, then add all the products: (2&nbsp;×&nbsp;−8)&nbsp;+&nbsp;(3&nbsp;×&nbsp;2)&nbsp;+&nbsp;(1&nbsp;×&nbsp;4) = −6. Do the same thing with slashes diagonal down and to the left (shown below with downwards slashes):\n:&nbsp;&nbsp;[[File:ShoelaceMatrix3.GIF]]\n(4&nbsp;×&nbsp;3)&nbsp;+&nbsp;(−8&nbsp;×&nbsp;1)&nbsp;+&nbsp;(2&nbsp;×&nbsp;2) = 8. Then take the difference of these two numbers: |(−6&nbsp;)−(&nbsp;8)| = 14. Halving this gives the area of the triangle: 7. Organizing the numbers like this makes the formula easier to recall and evaluate. With all the slashes drawn, the matrix loosely resembles a shoe with the laces done up, giving rise to the algorithm's name.\n\n==See also==\n*[[Planimeter]]\n*[[Pick's theorem]]\n\n== External links ==\n\n* [https://www.youtube.com/watch?v=0KjG8Pg6LGk Mathologer video about Gauss' shoelace formula]\n\n==References==\n{{Reflist}}\n\n[[Category:Area]]\n[[Category:Geometric algorithms]]\n[[Category:Surveying]]"
    },
    {
      "title": "Square trisection",
      "url": "https://en.wikipedia.org/wiki/Square_trisection",
      "text": "In [[geometry]], a '''square trisection''' consists of cutting a square into pieces that can be rearranged to form three identical squares.\n\n[[File:Trisection Blanvillain.png|thumb|Square trisection using 6 pieces of same area (2010).]]\n\n== Problem History ==\n\nThe [[Dissection problem|dissection]] of a square in three [[Congruence (geometry)|congruent]] [[Partition of a set|partitions]] is a geometrical problem that dates back to the [[Islamic Golden Age]]. Craftsman who mastered the art of [[zellige]] needed innovative techniques to achieve their fabulous mosaics with complex geometric figures. The first solution to this problem was proposed in the 10th century AD by the Persian mathematician [[Abul Wafa|Abu'l-Wafa']] (940-998) in his treatise ''\"On the geometric constructions necessary for the artisan\"''.<ref>Alpay Özdural (1995). ''Omar Khayyam, Mathematicians, and “conversazioni” with Artisans.'' Journal of the Society of Architectural [https://www.jstor.org/pss/991025 Vol. 54, No. 1, Mar., 1995]</ref> [[Abul Wafa|Abu'l-Wafa']] also used his dissection to demonstrate [[Pythagorean theorem|Pythagoras' theorem]].<ref>Reza Sarhangi, Slavik Jablan (2006). ''Elementary Constructions of Persian Mosaics.'' Towson University and The Mathematical Institute. [http://pages.towson.edu/gsarhang/Math%20Horizons%202.pdf online] {{webarchive|url=https://web.archive.org/web/20110728021642/http://pages.towson.edu/gsarhang/Math%20Horizons%202.pdf |date=2011-07-28 }}</ref> This geometrical proof of Pythagoras' theorem would be rediscovered in the years 1835 - 1840 <ref>See appendix of L. J. Rogers (1897). ''Biography of Henry Perigal: On certain Regular Polygons in Modular Network''. Proceedings London Mathematical Society. [http://plms.oxfordjournals.org/cgi/reprint/s1-29/1/706 Volume s1-29, Appendix pp. 732-735.]</ref> by [[Henry Perigal]] and published in 1875.<ref>[[s:author:Henry Perigal|Henry Perigal]] (1875). ''On Geometric Dissections and Transformations'', Messenger of Mathematics, [http://sunsite.ubc.ca/DigitalMathArchive/Euclid/perigal/perigal.html No 19, 1875].</ref>\n\n== Search of optimality ==\n\nThe beauty of a dissection depends on several parameters. However, it is usual to search for solutions with the minimum number of parts. Far from being minimal, the square trisection proposed by [[Abul Wafa|Abu'l-Wafa']] uses 9 pieces. In the 14th century [[Abu Bakr al-Khalil]] gave two solutions, one of which uses 8 pieces.<ref>Alpay Özdural (2000). ''Mathematics and Arts: Connections between Theory and Practice in the Medieval Islamic World'', Historia Mathematica, [http://www.sciencedirect.com/science/article/pii/S0315086099922747 Volume 27, Issue 2, May 2000, Pages 171-201].</ref> In the late 17th century [[Jacques Ozanam]] came back to this issue <ref>'''(fr)''' Jean-Etienne Montucla (1778), completed and re-edited by Jacques Ozanam (1640-1717) ''Récréations mathématiques'', [http://cnum.cnam.fr/CGI/fpage.cgi?8PY9.1/319/100/506/441/460 Tome 1 (1694), p. 297 Pl.15].</ref> and in the 19th century, solutions using 8 and 7 pieces were found, including one given by the mathematician [[Édouard Lucas]].<ref>'''(fr)''' Edouard Lucas (1883). ''Récréations Mathématiques'', Volume 2. Paris, Gauthier-Villars. Second of four volumes. Second edition (1893) reprinted by Blanchard in 1960. See pp. 151 and 152 in Volume 2 of this edition. [http://visualiseur.bnf.fr/Visualiseur?Destination=Gallica&O=NUMM-3944 online (pp. 145-147).]</ref> In 1891 [[Henry Perigal]] published the first known solution with only 6 pieces <ref>[[s:author:Henry Perigal|Henry Perigal]] (1891). ''Geometric Dissections and Transpositions'', Association for the Improvement of Geometrical Teaching. [[s:Geometric Dissections and Transpositions|wikisource]]</ref> (see illustration below). Nowadays, new dissections are still found <ref>Christian Blanvillain, [[:en:János Pach|János Pach]] (2010). ''Square Trisection''. Bulletin d'Informatique Approfondie et Applications [http://sites.univ-provence.fr/biaa/-upload/biaano86.htm N°86 - Juin 2010] {{webarchive|url=https://web.archive.org/web/20110724153252/http://sites.univ-provence.fr/biaa/-upload/biaano86.htm |date=2011-07-24 }} also at [[EPFL]]: [http://infoscience.epfl.ch/record/161493?ln=fr oai:infoscience.epfl.ch:161493].</ref> (see illustration above) and the conjecture that 6 is the minimal number of necessary pieces remains unproved.\n\n[[File:ThreeSqrs2One.svg|Henry Perigal (1891)]]\n\n== See also ==\n* [[Pythagorean_theorem#Proofs by dissection and rearrangement|Proofs by dissection and rearrangement of Pythagorean theorem]]\n* [[Dissection puzzle]]\n* [[Tangram]]\n\n== Bibliography ==\n*{{cite book\n  | last = Frederickson\n  | first = Greg N.\n  | authorlink =\n  | coauthors =\n  | title = Dissections: Plane and Fancy\n  | publisher = [[Cambridge University Press]]\n  | year = 1997\n  | location =\n  | url = http://www.cs.purdue.edu/homes/gnf/book.html\n  | doi =\n  | isbn = 0-521-57197-9 }}\n*{{cite book\n  | last = Frederickson\n  | first = Greg N.\n  | authorlink =\n  | coauthors =\n  | title = Hinged Dissections: Swinging and Twisting\n  | publisher = [[Cambridge University Press]]\n  | year = 2002\n  | location =\n  | url = http://www.cs.purdue.edu/homes/gnf/book2.html\n  | doi =\n  | isbn = 0-521-81192-9 }}\n*{{cite book\n  | last = Frederickson\n  | first = Greg N.\n  | authorlink =\n  | coauthors =\n  | title = Piano-hinged Dissections: Time to Fold!\n  | publisher = [[:en:A K Peters]]\n  | year = 2006\n  | location =\n  | url = http://www.cs.purdue.edu/homes/gnf/book3.html\n  | doi =\n  | isbn = 1-56881-299-X }}\n\n== References ==\n<references />\n\n==External links==\n* [http://www.cs.purdue.edu/homes/gnf/ Greg N. Frederickson web site]\n\n{{DEFAULTSORT:Square Trisection}}\n[[Category:Euclidean plane geometry]]\n[[Category:Mathematical problems]]\n[[Category:History of geometry]]\n[[Category:Area]]\n[[Category:Geometric dissection]]"
    },
    {
      "title": "Square–cube law",
      "url": "https://en.wikipedia.org/wiki/Square%E2%80%93cube_law",
      "text": "[[File:Galileo Galilei, Discorsi e Dimostrazioni Matematiche Intorno a Due Nuove Scienze, 1638 (1400x1400).png|right|thumb|The square–cube law was first mentioned in ''Two New Sciences'' (1638).]]\n\nThe '''square–cube law''' (or '''cube–square law''') is a mathematical principle, applied in a variety of scientific fields, which describes the relationship between the volume and the surface area as a shape's size increases or decreases. It was first described in 1638 by [[Galileo Galilei]] in his ''[[Two New Sciences]]'' as the \"...ratio of two volumes is greater than the ratio of their surfaces\".<ref>{{cite web|url=  https://books.google.com/books?id=wRm4BAAAQBAJ&pg=PA176&lpg=PA176&dq=square+cube+law+galileo&source=bl&ots=gTvbwNAElG&sig=A5PVzej4PexQ4gzIt9mytn2Nw6k&hl=en&sa=X&ei=0Y2iVb_nEorGogTYiImICg&ved=0CEMQ6AEwBjgK#v=onepage&q=square%20cube%20law%20galileo&f=false  |title=How Mechanics Shaped the Modern World|work=book|author=David H. Allen}}</ref>\n\nThis principle states that, as a shape grows in size, its volume grows faster than its surface area. When applied to the real world this principle has many implications which are important in fields ranging from [[mechanical engineering]] to [[biomechanics]]. It helps explain phenomena including why large mammals like [[elephants]] have a harder time cooling themselves than small ones like mice, and why building taller and taller [[skyscrapers]] is increasingly difficult.\n\n==Description==\n[[File:Comparison of surface area vs volume of shapes.svg|thumb|Graphs of surface area, ''A'' against volume, ''V'' of the Platonic solids and a sphere, showing that the surface-area-to-volume ratio decreases with increasing volume.]]\n\nThe square–cube law can be stated as follows:\n\n{{quote|When an object undergoes a proportional increase in size, its new surface area is proportional to the square of the multiplier and its new volume is proportional to the cube of the multiplier.}}\n\nRepresented mathematically:<ref>{{cite web|url=  http://www.world-builders.org/lessons/less/les9/area.html  |title=World Builders: The Sizes of Living Things|work=|author=}}</ref>\n\n:<math>A_2=A_1\\left(\\frac{\\ell_2}{\\ell_1}\\right)^2</math>\n\nwhere <math>A_1</math> is the original surface area and <math>A_2</math> is the new surface area.\n\n:<math>V_2=V_1\\left(\\frac{\\ell_2}{\\ell_1}\\right)^3</math>\n\nwhere <math>V_1</math> is the original volume, <math>V_2</math> is the new volume, <math>\\ell_1</math> is the original length and <math>\\ell_2</math> is the new length.\n\nFor example, a cube with a side length of 1 meter has a surface area of 6&nbsp;m<sup>2</sup> and a volume of 1&nbsp;m<sup>3</sup>.  If the dimensions of the cube were multiplied by 2, its surface area would be multiplied by the ''square'' of 2 and become 24&nbsp;m<sup>2</sup>. Its volume would be multiplied by the ''cube'' of 2 and become 8&nbsp;m<sup>3</sup>. \n\nThe original cube (1m sides) has a surface area to volume ratio of 6:1. The larger (2m sides) cube has a surface area to volume ratio of (24/8) 3:1. As the dimensions increase, the volume will continue to grow faster than the surface area. Thus the square–cube law.  This principle applies to all solids.<ref>{{cite web|url=http://fathom.lib.uchicago.edu/2/21701757/|title=The Biology of B-Movie Monsters|work=|author=Michael C. LaBarbera}}</ref>\n\n==Applications==\n\n===Engineering===\nWhen a physical object maintains the same density and is scaled up, its volume and mass are increased by the cube of the multiplier while its surface area increases only by the square of said multiplier.  This would mean that when the larger version of the object is accelerated at the same rate as the original, more pressure would be exerted on the surface of the larger object.\n\nConsider a simple example of a body of mass, M, having an acceleration, a, and surface area, A, of the surface upon which the accelerating force is acting.  The force due to acceleration, <math>F= M a</math> and the thrust pressure,  <math>T = \\frac{F}{A} = M\\frac{a}{A}</math>.\n\nNow, consider the object be exaggerated by a multiplier factor = x so that it has a new mass, <math>M' = x^3 M</math>, and the surface upon which the force is acting has a new surface area, <math>A' = x^2 A</math>.\n\nThe new force due to acceleration <math>F' = x^3 Ma</math> and the resulting thrust pressure,\n\n:<math>\\begin{align}\nT' &= \\frac{F'}{A'}\\\\\n   &= \\frac{x^3}{x^2} \\times M\\frac{a}{A}\\\\\n   &= x \\times M \\frac{a}{A}\\\\\n   &= x \\times T\\\\\n\\end{align}</math>\n                                  \nThus, just scaling up the size of an object, keeping the same material of construction (density), and same acceleration, would increase the thrust by the same scaling factor. This would indicate that the object would have less ability to resist stress and  would be more prone to collapse while accelerating.\n\nThis is why large vehicles perform poorly in crash tests and why there are limits to how high buildings can be built.  Similarly, the larger an object is, the less other objects would resist its motion, causing its deceleration.\n\n====Engineering examples====\n{{refimprove section|small=y|date=September 2018}}\n*[[Watt steam engine|Steam engine]]: [[James Watt]], working as an instrument maker for the [[University of Glasgow]], was given a scale model [[Newcomen steam engine]] to put in working order.  Watt recognized the problem as being related to the square–cube law, in that the surface to volume ratio of the model's cylinder was greater than that of the much larger commercial engines, leading to excessive heat loss.<ref>\n{{cite book\n|title=The Most Powerful Idea in the World: A Story of Steam, Industry and Invention\n |last1=\tRosen\n |first1= William\n|authorlink= \n|coauthors= \n|year= 2012 |publisher = University Of Chicago Press\n|location= \n|isbn= 978-0226726342 |pages=98\n| postscript = <!--None-->}}\n</ref>  Experiments with this model led to Watt's famous improvements to the steam engine.\n\n* [[Airbus A380]]: the lift and control surfaces (wings, rudders and elevators) are relatively big compared to the fuselage of the airplane. For example, taking a [[Boeing 737]] and merely magnifying its dimensions to the size of an A380 would result in wings that are too small for the aircraft weight, because of the square–cube rule.\n* [[Expander cycle]] [[rocket engines]] suffer from the square–cube law. Their size, and therefore thrust, is limited by [[heat transfer]] [[efficiency]] due to the surface area of the nozzle increasing slower than the volume of fuel flowing through the nozzle.\n* A [[clipper]] needs relatively more sail surface than a [[sloop]] to reach the same speed, meaning there is a higher sail-surface-to-sail-surface ratio between these craft than there is a weight-to-weight ratio.\n* [[Aerostats]] generally benefit from the square–cube law. As the radius (<math>r</math>) of a balloon is increased, the cost in surface area increases quadratically (<math>r ^ 2</math>), but the lift generated from volume increases cubically (<math>r^3</math>).\n* [[Structural Engineering]]: Materials that work at small scales may not work at larger scales. For example, the compressive stress at the bottom of small free-standing column scales at the same rate as the size of the column. Therefore, there exists a size for a given material and density at which a column will collapse on itself.\n\n===Biomechanics===\nIf an animal were isometrically scaled up by a considerable amount, its relative muscular strength would be severely reduced, since the cross section of its muscles would increase by the ''square'' of the scaling factor while its mass would increase by the ''cube'' of the scaling factor.  As a result of this, cardiovascular  and respiratory functions would be severely burdened.\n\nIn the case of flying animals, the wing loading would be increased if they were isometrically scaled up, and they would therefore have to fly faster to gain the same amount of [[Lift (force)|lift]].  Air resistance per unit mass is also higher for smaller animals, which is why a small animal like an [[ant]] cannot be seriously injured from impact with the ground after being dropped from any height.\n\nAs stated by [[J. B. S. Haldane]], large animals do not look like small animals: an elephant cannot be mistaken for a mouse scaled up in size.  This is due to [[allometry|allometric scaling]]: the bones of an elephant are necessarily proportionately much larger than the bones of a mouse, because they must carry proportionately higher weight. Haldane illustrates this in his seminal 1928 essay ''On Being the Right Size'' in referring to [[allegory|allegorical ]] giants: \"...consider a man 60 feet high...Giant Pope and Giant Pagan in the illustrated ''Pilgrim's Progress:'' ...These monsters...weighed 1000 times as much as [[Pilgrim's Progress|Christian]]. Every square inch of a giant bone had to support 10 times the weight borne by a square inch of human bone.  As the average human thigh-bone breaks under about 10 times the human weight, Pope and Pagan would have broken their thighs every time they took a step.\"<ref>{{cite web|last1=Haldane|first1=J. B. S.|title=On Being the Right Size|url=http://irl.cs.ucla.edu/papers/right-size.html|website=Internet Research Lab|publisher=UCLA|accessdate=1 April 2017}}</ref>  Consequently, most animals show [[allometry|allometric scaling]] with increased size, both among species and within a species. The giant creatures seen in monster movies (e.g., [[Godzilla]], [[King Kong]], and [[Them!]]) are also unrealistic, given that their sheer size would force them to collapse.\n\nHowever, the buoyancy of water negates to some extent the effects of gravity. Therefore, sea creatures can grow to very large sizes without the same musculoskeletal structures that would be required of similarly sized land creatures, and it is no coincidence that the largest animals to ever exist on earth are [[Largest organisms#Animals|aquatic animals]].\n\nThe metabolic rate of animals scales with a mathematical principle named [[Kleiber's law|quarter-power scaling]]<ref>{{cite web| url=https://www.nytimes.com/library/national/science/011299sci-scaling.html| title=Of Mice and Elephants: A Matter of Scale| author=George Johnson| publisher=[[The New York Times]]| date=January 12, 1999| accessdate=2015-06-11}}</ref> according to the [[metabolic theory of ecology]].\n\n=== Mass and heat transfer ===\nMass transfer such as diffusion to smaller objects such as living cells is faster than diffusion to larger objects such as entire animals. Thus, in chemical processes that take place on a surface - rather than in the bulk - finer-divided material is more active. For example, the activity of a heterogeneous [[catalyst]] is higher when it is divided into finer particles.\n\nHeat production from a chemical process scales with the cube of the linear dimension (height, width) of the vessel, but the vessel surface area scales with only the square of the linear dimension. Consequently, larger vessels are much more difficult to cool. Also, large-scale piping for transferring hot fluids is difficult to simulate in small scale, because heat is transferred faster out from smaller pipes. Failure to take this into account in process design may lead to catastrophic [[thermal runaway]].\n\n== See also ==\n{{wikiversity|Proportions}}\n* [[Biomechanics]]\n* [[Allometric law]]\n* \"[[On Being the Right Size]]\", an essay by [[J. B. S. Haldane]] that considers the changes in shape of animals that would be required by a large change in size\n* [[Surface-area-to-volume ratio]]\n* [[Kleiber's law]]\n\n==References==\n\n{{Reflist|30em}}\n\n{{DEFAULTSORT:Square-Cube Law}}\n[[Category:Area]]\n[[Category:Volume]]\n[[Category:Mathematical principles]]"
    },
    {
      "title": "Surface area",
      "url": "https://en.wikipedia.org/wiki/Surface_area",
      "text": "{{pp-pc1|small=yes}}\n[[Image:Sphere wireframe 10deg 6r.svg|right|thumb|A [[sphere]] of radius <math>r</math> has surface area <math>4\\pi r^2</math>]]\nThe '''surface area''' of a [[Solid geometry|solid]] object is a measure of the total [[area]] that the [[Surface (mathematics)|surface]] of the object occupies.<ref>{{MathWorld|title=Surface Area|urlname=SurfaceArea}}</ref> The mathematical definition of surface area in the presence of curved surfaces is considerably more involved than the definition of [[arc length]] of one-dimensional curves, or of the surface area for [[polyhedra]] (i.e., objects with flat polygonal [[Face (geometry)|faces]]), for which the surface area is the sum of the areas of its faces. Smooth surfaces, such as a [[sphere]], are assigned surface area using their representation as [[parametric surface]]s. This definition of surface area is based on methods of [[infinitesimal]] [[calculus]] and involves [[partial derivative]]s and [[double integration]].\n\nA general definition of surface area was sought by [[Henri Lebesgue]] and [[Hermann Minkowski]] at the turn of the twentieth century. Their work led to the development of [[geometric measure theory]], which studies various notions of surface area for irregular objects of any dimension. An important example is the [[Minkowski content]] of a surface.\n\n==Definition==\n\nWhile the areas of many simple surfaces have been known since antiquity, a rigorous mathematical ''definition'' of area requires a great deal of care.\nThis should provide a function\n\n: <math> S \\mapsto A(S) </math>\n\nwhich assigns a positive [[real number]] to a certain class of [[Surface (topology)|surface]]s that satisfies several natural requirements. The most fundamental property of the surface area is its '''additivity''': ''the area of the whole is the sum of the areas of the parts''. More rigorously, if a surface ''S'' is a union of finitely many pieces ''S''<sub>1</sub>, …, ''S''<sub>''r''</sub> which do not overlap except at their boundaries, then \n: <math> A(S) = A(S_1) + \\cdots + A(S_r). </math>\n\nSurface areas of flat polygonal shapes must agree with their geometrically defined [[area]]. Since surface area is a geometric notion, areas of [[congruence (geometry)|congruent]] surfaces must be the same and the area must depend only on the shape of the surface, but not on its position and orientation in space. This means that surface area is invariant under the [[Euclidean group|group of Euclidean motions]]. These properties uniquely characterize surface area for a wide class of geometric surfaces called ''piecewise smooth''. Such surfaces consist of finitely many pieces that can be represented in the [[parametric surface|parametric form]]\n\n: <math> S_D: \\vec{r}=\\vec{r}(u,v), \\quad (u,v)\\in D </math>\n\nwith a [[continuously differentiable]] function <math>\\vec{r}.</math> The area of an individual piece is defined by the formula\n\n: <math> A(S_D) = \\iint_D\\left |\\vec{r}_u\\times\\vec{r}_v\\right | \\, du \\, dv. </math>\n\nThus the area of ''S''<sub>''D''</sub> is obtained by integrating the length of the normal vector <math>\\vec{r}_u\\times\\vec{r}_v</math> to the surface over the appropriate region ''D'' in the parametric ''uv'' plane. The area of the whole surface is then obtained by adding together the areas of the pieces, using additivity of surface area. The main formula can be specialized to different classes of surfaces, giving, in particular, formulas for areas of graphs ''z'' = ''f''(''x'',''y'') and [[surface of revolution|surfaces of revolution]].\n\n[[File:Schwarz-lantern.gif|thumb|[[Schwarz lantern]] with <math>M</math> axial slices and <math>N</math> radial vertices. The limit of the area as <math>M</math> and <math>N</math> tend to infinity doesn't converge. In particular it doesn't converge to the area of the cylinder.]]One of the subtleties of surface area, as compared to [[arc length]] of curves, is that surface area cannot be defined simply as the limit of areas of polyhedral shapes approximating a given smooth surface. It was demonstrated by [[Hermann Schwarz]] that already for the cylinder, different choices of approximating flat surfaces can lead to different limiting values of the area; this example is known as the [[Schwarz lantern]].<ref name=sch1>{{cite web|url=http://fredrickey.info/hm/CalcNotes/schwarz-paradox.pdf|title=Schwarz's Paradox|publisher=|date=|accessdate=2017-03-21|deadurl=no|archiveurl=https://web.archive.org/web/20160304073957/http://fredrickey.info/hm/CalcNotes/schwarz-paradox.pdf|archivedate=2016-03-04|df=}}</ref><ref name=sch2>{{cite web |url=http://mathdl.maa.org/images/upload_library/22/Polya/00494925.di020678.02p0385w.pdf |title=Archived copy |accessdate=2012-07-24 |deadurl=yes |archiveurl=https://web.archive.org/web/20111215152255/http://mathdl.maa.org/images/upload_library/22/Polya/00494925.di020678.02p0385w.pdf |archivedate=2011-12-15 |df= }}</ref>\n\n\nVarious approaches to a general definition of surface area were developed in the late nineteenth and the early twentieth century by [[Henri Lebesgue]] and [[Hermann Minkowski]]. While for piecewise smooth surfaces there is a unique natural notion of surface area, if a surface is very irregular, or rough, then it may not be possible to assign an area to it at all. A typical example is given by a surface with spikes spread throughout in a dense fashion. Many surfaces of this type occur in the study of [[fractal]]s. Extensions of the notion of area which partially fulfill its function and may be defined even for very badly irregular surfaces are studied in [[geometric measure theory]]. A specific example of such an extension is the [[Minkowski content]] of the surface.\n\n== Common formulas ==\n\n{| class=\"wikitable\"\n|+ Surface areas of common solids\n|-\n!Shape\n!Equation\n!Variables\n|-\n|[[Cube]]\n|<math> 6s^2 \\, </math>\n|''s'' = side length\n|-\n|[[Cuboid]]\n|<math> 2(\\ell w + \\ell h + wh) \\, </math>\n|''ℓ'' = length, ''w'' = width, ''h'' = height\n|-\n|[[Triangular prism]]\n|<math> bh + l(a + b + c) </math>\n|''b'' = base length of triangle, ''h'' = height of triangle, ''l'' = distance between triangular bases, ''a'', ''b'', ''c'' = sides of triangle\n|-\n|All [[Prism (geometry)|prisms]]\n|<math> 2B + Ph \\, </math>\n|''B'' = the area of one base, ''P'' = the perimeter of one base, ''h'' = height\n|-\n|[[Sphere]]\n|<math> 4\\pi r^2 = \\pi d^2\\, </math>\n|''r'' = radius of sphere, ''d'' = diameter\n|-\n|[[Spherical lune]]\n|<math> 2r^2\\theta \\, </math>\n|''r'' = radius of sphere, ''θ'' = [[dihedral angle]]\n|-\n|[[Torus]]\n|<math> (2\\pi r)(2\\pi R) = 4\\pi^2 Rr</math>\n|''r'' = minor radius (radius of the tube), ''R'' = major radius (distance from center of tube to center of torus)\n|-\n|Closed [[Cylinder (geometry)|cylinder]]\n|<math> 2\\pi r^2 + 2\\pi rh = 2\\pi r(r+h) \\, </math>\n|''r'' = radius of the circular base, ''h'' = height of the cylinder\n|-\n|Lateral surface area of a [[cone (geometry)|cone]]\n|<math> \\pi r \\left(\\sqrt{r^2+h^2}\\right) = \\pi rs \\, </math>\n|<math> s = \\sqrt{r^2+h^2} </math><br>\n''s'' = slant height of the cone,<br>\n''r'' = radius of the circular base,<br>\n''h'' = height of the cone\n|-\n|Full surface area of a cone\n|<math> \\pi r \\left(r + \\sqrt{r^2+h^2}\\right) = \\pi r(r + s) \\, </math>\n| ''s'' = slant height of the cone,<br>\n''r'' = radius of the circular base,<br>\n''h'' = height of the cone\n|-\n|[[Pyramid (geometry)|Pyramid]]\n|<math>B + \\frac{PL}{2}</math>\n|''B'' = area of base, ''P'' = perimeter of base, ''L'' = slant height\n|-\n|[[Square pyramid]]\n|<math> b^2 + 2bs = b^2 + 2b\\sqrt{\\left(\\frac{b}{2}\\right)^2+h^2} </math>\n|''b'' = base length, ''s'' = slant height, ''h'' = vertical height\n|-\n|Rectangular pyramid\n|<math> lw + l\\sqrt{\\left(\\frac{w}{2}\\right)^2+h^2} + w\\sqrt{\\left(\\frac{l}{2}\\right)^2+h^2} </math>\n|''ℓ'' = length, ''w'' = width, ''h'' = height\n|-\n|[[Tetrahedron]]\n|<math> \\sqrt{3}a^2 </math>\n|''a'' = side length\n|}\n\n===Ratio of surface areas of a sphere and cylinder of the same radius and height===\n\n[[Image:Inscribed cone sphere cylinder.svg|thumb|300px|A cone, sphere and cylinder of radius ''r'' and height ''h''.]]\nThe below given formulas can be used to show that the surface area of a [[sphere]] and [[cylinder (geometry)|cylinder]] of the same radius and height are in the ratio '''2&nbsp;:&nbsp;3''', as follows.\n\nLet the radius be ''r'' and the height be ''h'' (which is 2''r'' for the sphere).\n\n<math>\\begin{array}{rlll}\n\\text{Sphere surface area}   & = 4 \\pi r^2      &                    & = (2 \\pi r^2) \\times 2 \\\\\n\\text{Cylinder surface area} & = 2 \\pi r (h + r) & = 2 \\pi r (2r + r) & = (2 \\pi r^2) \\times 3\n\\end{array}</math>\n\nThe discovery of this ratio is credited to [[Archimedes]].<ref>{{cite web|first = Chris|last = Rorres|url = http://www.math.nyu.edu/~crorres/Archimedes/Tomb/Cicero.html|title = Tomb of Archimedes: Sources|publisher = Courant Institute of Mathematical Sciences|accessdate = 2007-01-02|deadurl = no|archiveurl = https://web.archive.org/web/20061209201723/http://www.math.nyu.edu/~crorres/Archimedes/Tomb/Cicero.html|archivedate = 2006-12-09|df = }}</ref>\n{{clear}}\n\n== In chemistry ==\n[[File:Surface area.svg|thumb|Surface area of particles of different sizes.]]\n{{see also|Accessible surface area}}\nSurface area is important in [[chemical kinetics]]. Increasing the surface area of a substance generally increases the [[reaction rate|rate]] of a [[chemical reaction]].  For example, [[iron]] in a fine powder will [[combustion|combust]], while in solid blocks it is stable enough to use in structures. For different applications a minimal or maximal surface area may be desired.\n{{clear}}\n\n== In biology ==\n{{see also|Surface-area-to-volume ratio}}\n[[Image:Mitochondrion 186.jpg|right|thumb|The inner membrane of the [[mitochondrion]] has a large surface area due to infoldings, allowing higher rates of [[cellular respiration]] (electron [[micrograph]]).]]\nThe surface area of an organism is important in several considerations, such as regulation of body temperature and [[digestion]]. Animals use their [[teeth]] to grind food down into smaller particles, increasing the surface area available for digestion. The epithelial tissue lining the digestive tract contains [[microvilli]], greatly increasing the area available for absorption. [[Elephant]]s have large [[ear]]s, allowing them to regulate their own body temperature. In other instances, animals will need to minimize surface area; for example, people will fold their arms over their chest when cold to minimize heat loss.\n\nThe [[surface area to volume ratio]] (SA:V) of a [[cell (biology)|cell]] imposes upper limits on size, as the volume increases much faster than does the surface area, thus limiting the rate at which substances diffuse from the interior across the [[cell membrane]] to interstitial spaces or to other cells. Indeed, representing a cell as an idealized [[sphere]] of radius ''r'', the volume and surface area are, respectively, ''V'' = 4/3 π ''r''<sup>3</sup>; ''SA'' = 4 π ''r''<sup>2</sup>.  The resulting surface area to volume ratio is therefore 3/''r''. Thus, if a cell has a radius of 1 μm, the SA:V ratio is 3; whereas if the radius of the cell is instead 10 μm, then the SA:V ratio becomes 0.3. With a cell radius of 100, SA:V ratio is 0.03. Thus, the surface area falls off steeply with increasing volume.\n\n== See also ==\n* [[Perimeter length]]\n* [[BET theory]], technique for the measurement of the specific surface area of materials\n* [[Spherical area]]\n* [[Surface integral]]\n\n== References ==\n\n<references />\n\n* {{eom|title=Area|id=A/a013180|author=Yu.D. Burago, V.A. Zalgaller, L.D. Kudryavtsev}}\n\n==External links==\n*[http://blog.thinkwell.com/2010/07/6th-grade-math-surface-area.html Surface Area Video] at Thinkwell\n\n[[Category:Area]]"
    },
    {
      "title": "Surface integral",
      "url": "https://en.wikipedia.org/wiki/Surface_integral",
      "text": "{{Calculus |Multivariable}}\n{{Refimprove|date=January 2017}}\n\nIn [[mathematics]], a '''surface integral'''  is a generalization of [[multiple integral]]s to integration over [[surface (differential geometry)|surface]]s. It can be thought of as the [[double integral]] analogue of the [[line integral]].  Given a surface, one may integrate over its [[scalar field]]s (that is, [[function (mathematics)|functions]] which return [[Scalar (mathematics)|scalars]] as values), and [[vector field]]s (that is, functions which return [[Vector (geometric)|vectors]] as values).\n\nSurface integrals have applications in [[physics]], particularly with the theories of [[classical electromagnetism]].\n[[Image:Surface integral illustration.svg|right|thumb|The definition of surface integral relies on splitting the surface into small surface elements.]]\n[[Image:Surface integral1.svg|right|thumb|An illustration of a single surface element. These elements are made infinitesimally small, by the limiting process, so as to approximate the surface.]]\n\n== Surface integrals of scalar fields ==\nTo find an explicit formula for the surface integral, we need to [[Coordinate system|parameterize]] the surface of interest, ''S'', by considering a system of [[curvilinear coordinates]] on ''S'', like the [[Geographic coordinate system|latitude and longitude]] on a [[sphere]]. Let such a parameterization be '''x'''(''s'', ''t''), where (''s'', ''t'') varies in some region ''T'' in the [[Cartesian coordinate system#Cartesian coordinates in two dimensions|plane]]. Then, the surface integral is given by\n\n:<math>\n\\iint\\limits_{S} f \\,\\mathrm dS\n= \\iint\\limits_{T} f(\\mathbf{x}(s, t)) \\left\\|{\\partial \\mathbf{x} \\over \\partial s}\\times {\\partial \\mathbf{x} \\over \\partial t}\\right\\| \\mathrm ds\\, \\mathrm dt\n</math>\nwhere the expression between bars on the right-hand side is the [[Magnitude (mathematics)|magnitude]] of the [[cross product]] of the [[partial derivative]]s of '''x'''(''s'', ''t''), and is known as the surface [[volume element#Area_element_of_a_surface|element]].  The surface integral can also be expressed in the equivalent form\n\n:<math>\n\\iint\\limits_{S} f \\,\\mathrm d\\Sigma\n= \\iint\\limits_{T} f(\\mathbf{x}(s, t)) \\sqrt{g} \\, \\mathrm ds\\, \\mathrm dt\n</math>\nwhere ''g'' is the determinant of the [[first fundamental form]] of the surface mapping '''x'''(''s'', ''t'').<ref>{{Cite book|title = Advanced Calculus of Several Variables|last = Edwards|first = C. H.|publisher = Dover|year = 1994|isbn = 0-486-68336-2|location = Mineola, NY|pages = 335}}</ref><ref>{{Cite book|title = Encyclopedia of Mathematics|last = Hazewinkel|first = Michiel|publisher = Springer|year = 2001|isbn = 978-1-55608-010-4|location = |pages = Surface Integral|url = https://www.encyclopediaofmath.org/index.php/Surface_integral}}</ref>\n\nFor example, if we want to find the [[surface area]] of the graph of some scalar function, say <math>z=f\\,(x,y)</math>, we have\n:<math>\nA = \\iint\\limits_S \\,\\mathrm d\\Sigma\n= \\iint\\limits_T \\left\\|{\\partial \\mathbf{r} \\over \\partial x}\\times {\\partial \\mathbf{r} \\over \\partial y}\\right\\| \\mathrm dx\\, \\mathrm dy\n</math>\nwhere <math>\\mathbf{r}=(x, y, z)=(x, y, f(x,y))</math>.  So that <math>{\\partial \\mathbf{r} \\over \\partial x}=(1, 0, f_x(x,y))</math>, and <math>{\\partial \\mathbf{r} \\over \\partial y}=(0, 1, f_y(x,y))</math>.  So,\n:<math>\\begin{align}\nA\n&{} = \\iint\\limits_T \\left\\|\\left(1, 0, {\\partial f \\over \\partial x}\\right)\\times \\left(0, 1, {\\partial f \\over \\partial y}\\right)\\right\\| \\mathrm dx\\, \\mathrm dy \\\\\n&{} = \\iint\\limits_T \\left\\|\\left(-{\\partial f \\over \\partial x}, -{\\partial f \\over \\partial y}, 1\\right)\\right\\| \\mathrm dx\\, \\mathrm dy \\\\\n&{} = \\iint\\limits_T \\sqrt{\\left({\\partial f \\over \\partial x}\\right)^2+\\left({\\partial f \\over \\partial y}\\right)^2+1}\\, \\,  \\mathrm dx\\, \\mathrm dy\n\\end{align}</math>\nwhich is the standard formula for the area of a surface described this way.  One can recognize the vector in the second line above as the [[surface normal|normal vector]] to the surface.\n\nNote that because of the presence of the cross product, the above formulas only work for surfaces embedded in three-dimensional space.\n\nThis can be seen as integrating a [[Riemannian volume form]] on the parameterized surface, where the [[metric tensor]] is given by the [[first fundamental form]] of the surface.\n\n== Surface integrals of vector fields ==<!-- This section is linked from [[Flux]] -->\n[[Image:Surface vectors.png|right|thumb|300px|A vector field on a surface]]\nConsider a vector field '''v''' on ''S'', that is, for each '''x''' in ''S'', '''v'''('''x''') is a vector.\n\nThe surface integral can be defined component-wise according to the definition of the surface integral of a scalar field; the result is a vector. This applies for example in the expression of the electric field at some fixed point due to an electrically charged surface, or the gravity at some fixed point due to a sheet of material.\n\nAlternatively, if we integrate the [[normal component]] of the vector field, the result is a scalar. Imagine that we have a fluid flowing through ''S'', such that '''v'''('''x''') determines the velocity of the fluid at '''x'''. The [[flux]] is defined as the quantity of fluid flowing through ''S'' per unit time.\n\nThis illustration implies that if the vector field is [[tangent]] to ''S'' at each point, then the flux is zero because the fluid just flows in [[Parallel (geometry)|parallel]] to ''S'', and neither in nor out. This also implies that if '''v''' does not just flow along ''S'', that is, if '''v''' has both a tangential and a normal component, then only the normal component contributes to the flux. Based on this reasoning, to find the flux, we need to take the [[dot product]] of  '''v''' with the unit [[surface normal]] '''n''' to ''S'' at each point, which will give us a scalar field, and integrate the obtained field as above. We find the formula\n\n:<math>\\begin{align}\n\\iint\\limits_S {\\mathbf v}\\cdot\\mathrm d{\\mathbf {\\Sigma}} &= \\iint\\limits_S \\left({\\mathbf v}\\cdot {\\mathbf n}\\right)\\,\\mathrm d\\Sigma\\\\\n&{}= \\iint\\limits_T \\left({\\mathbf v}(\\mathbf{x}(s, t)) \\cdot {\\left({\\partial \\mathbf{x} \\over \\partial s}\\times {\\partial \\mathbf{x} \\over \\partial t}\\right) \\over \\left\\|\\left({\\partial \\mathbf{x} \\over \\partial s}\\times {\\partial \\mathbf{x} \\over \\partial t}\\right)\\right\\|}\\right) \\left\\|\\left({\\partial \\mathbf{x} \\over \\partial s}\\times {\\partial \\mathbf{x} \\over \\partial t}\\right)\\right\\| \\mathrm ds\\, \\mathrm dt\\\\\n&{}=\\iint\\limits_T {\\mathbf v}(\\mathbf{x}(s, t))\\cdot \\left({\\partial \\mathbf{x} \\over \\partial s}\\times {\\partial \\mathbf{x} \\over \\partial t}\\right) \\mathrm ds\\, \\mathrm dt.\n\\end{align}</math>\n\nThe cross product on the right-hand side of this expression is a (not necessarily unital) surface normal determined by the parametrization.\n\nThis formula ''defines'' the integral on the left (note the dot and the vector notation for the surface element).\n\nWe may also interpret this as a special case of integrating 2-forms, where we identify the vector field with a 1-form, and then integrate its [[Hodge dual]] over the surface.\nThis is equivalent to integrating <math>\\langle \\mathbf{v}, \\mathbf{n} \\rangle \\;\\mathrm d\\Sigma </math> over the immersed surface, where <math>\\mathrm d\\Sigma</math> is the induced volume form on the surface, obtained\nby [[interior multiplication]] of the Riemannian metric of the ambient space with the outward normal of the surface.\n\n== Surface integrals of differential 2-forms ==\nLet\n:<math> f=f_{z}\\, \\mathrm dx \\wedge \\mathrm dy + f_{x}\\, \\mathrm dy \\wedge \\mathrm dz + f_{y}\\, \\mathrm dz  \\wedge \\mathrm dx </math>\nbe a [[differential form|differential 2-form]] defined on the surface ''S'', and let\n\n:<math>\\mathbf{x} (s,t)=( x(s,t), y(s,t), z(s,t))\\!</math>\n\nbe an [[orientability|orientation preserving]] parametrization of ''S'' with <math>(s,t)</math> in ''D''. Changing coordinates from <math>(x, y)</math>\nto <math>(s, t)</math>, the differential forms transform as\n\n:<math>\\mathrm dx=\\frac{\\partial x}{\\partial s}\\mathrm ds+\\frac{\\partial x}{\\partial t}\\mathrm dt</math>\n\n:<math>\\mathrm dy=\\frac{\\partial y}{\\partial s}\\mathrm ds+\\frac{\\partial y}{\\partial t}\\mathrm dt</math>\n\nSo <math> \\mathrm dx \\wedge \\mathrm dy </math> transforms to <math> \\frac{\\partial(x,y)}{\\partial(s,t)}  \\mathrm ds \\wedge \\mathrm dt </math>, where <math> \\frac{\\partial(x,y)}{\\partial(s,t)} </math> denotes the determinant of the Jacobian of the transition function from <math>(s, t)</math> to <math>(x,y)</math>. The transformation of the other forms are similar.\n\nThen, the surface integral of ''f'' on ''S'' is given by\n\n:<math>\\iint\\limits_D \\left[ f_{z} ( \\mathbf{x} (s,t)) \\frac{\\partial(x,y)}{\\partial(s,t)} + f_{x} ( \\mathbf{x} (s,t))\\frac{\\partial(y,z)}{\\partial(s,t)} + f_{y} ( \\mathbf{x} (s,t))\\frac{\\partial(z,x)}{\\partial(s,t)} \\right]\\, \\mathrm ds\\, \\mathrm dt</math>\n\nwhere\n:<math>{\\partial \\mathbf{x} \\over \\partial s}\\times {\\partial \\mathbf{x} \\over \\partial t}=\\left(\\frac{\\partial(y,z)}{\\partial(s,t)}, \\frac{\\partial(z,x)}{\\partial(s,t)}, \\frac{\\partial(x,y)}{\\partial(s,t)}\\right)</math>\nis the surface element normal to ''S''.\n\nLet us note that the surface integral of this 2-form is the same as the surface integral of the vector field which has as components <math>f_x</math>, <math>f_y</math> and <math>f_z</math>.\n\n== Theorems involving surface integrals ==\nVarious useful results for surface integrals can be derived using [[differential geometry]] and [[vector calculus]], such as the [[divergence theorem]], and its generalization, [[Stokes' theorem]].\n\n== Advanced issues ==\nLet us notice that we defined the surface integral by using a parametrization of the surface ''S''. We know that a given surface might have several parametrizations. For example, if we move the locations of the North Pole and the South Pole on a sphere, the latitude and longitude change for all the points on the sphere. A natural question is then whether the definition of the surface integral depends on the chosen parametrization. For integrals of scalar fields, the answer to this question is simple, the value of the surface integral will be the same no matter what parametrization one uses.\n\nFor integrals of vector fields, things are more complicated, because the surface normal is involved. It can be proven that given two parametrizations of the same surface, whose surface normals point in the same direction, one obtains the same value for the surface integral with both parametrizations. If, however, the normals for these parametrizations point in opposite directions, the value of the surface integral obtained using one parametrization is the negative of the one obtained via the other parametrization. It follows that given a surface, we do not need to stick to any unique parametrization; but, when integrating vector fields, we do need to decide in advance which direction the normal will point to and then choose any parametrization consistent with that direction.\n\nAnother issue is that sometimes surfaces do not have parametrizations which cover the whole surface. The obvious solution is then to split that surface into several pieces, calculate the surface integral on each piece, and then add them all up. This is indeed how things work, but when integrating vector fields, one needs to again be careful how to choose the normal-pointing vector for each piece of the surface, so that when the pieces are put back together, the results are consistent. For the cylinder, this means that if we decide that for the side region the normal will point out of the body, then for the top and bottom circular parts the normal must point out of the body too.\n\nLastly, there are surfaces which do not admit a surface normal at each point with consistent results (for example, the [[Möbius strip]]).  If such a surface is split into pieces, on each piece a parametrization and corresponding surface normal is chosen, and the pieces are put back together, we will find that the normal vectors coming from different pieces cannot be reconciled. This means that at some junction between two pieces we will have normal vectors pointing in opposite directions. Such a surface is called [[Orientability|non-orientable]], and on this kind of surface, one cannot talk about integrating vector fields.\n\n== See also ==\n* [[Divergence theorem]]\n* [[Stokes' theorem]]\n* [[Line integral]]\n* [[Volume element]]\n* [[Volume integral]]\n* [[Cartesian coordinate system]]\n* [[Spherical coordinate system#Integration and differentiation in spherical coordinates|Volume and surface area elements in spherical coordinate systems]]\n* [[Cylindrical coordinate system#Line and volume elements|Volume and surface area elements in cylindrical coordinate systems]]\n* [[Holstein–Herring method]]\n\n==References==\n{{Reflist}}\n\n== External links ==\n* [http://mathworld.wolfram.com/SurfaceIntegral.html Surface Integral — from MathWorld]\n* [http://www.math.gatech.edu/%7Ecain/notes/cal15.pdf Surface Integral — Theory and exercises]\n\n[[Category:Multivariable calculus]]\n[[Category:Area]]\n[[Category:Surfaces]]"
    },
    {
      "title": "Vector area",
      "url": "https://en.wikipedia.org/wiki/Vector_area",
      "text": "In 3-dimensional geometry, for a finite planar surface of scalar area {{mvar|S}} and [[unit normal]] {{math|'''n̂'''}}, the vector area {{math|'''S'''}} is defined as the unit normal scaled by the area:\n\n:<math>\\mathbf{S} = \\mathbf{\\hat n}S</math>\n\nFor an [[orientable]] surface {{mvar|S}} composed of a set {{mvar|S<sub>i</sub>}} of flat [[Facet_(geometry)|facet]] areas, the vector area of the surface is given by\n\n:<math>\\mathbf{S} = \\sum_i \\mathbf{\\hat n}_i S_i</math>\n\nwhere {{math|'''n̂'''<sub>''i''</sub>}} is the unit normal vector to the area {{mvar|S<sub>i</sub>}}.\n\nFor bounded, oriented curved surfaces that are sufficiently [[well-behaved]], we can still define vector area. First, we split the surface into infinitesimal elements, each of which is effectively flat.  For each infinitesimal element of area, we have an area vector, also infinitesimal.\n\n:<math>d\\mathbf{S} = \\mathbf{\\hat n}dS</math>\n\nwhere {{math|'''n̂'''}} is the local unit vector perpendicular to {{mvar|dS}}.  Integrating gives the vector area for the surface.\n\n:<math>\\mathbf{S} = \\int d\\mathbf{S}</math>\n\nFor a curved or faceted surface, the vector area is smaller in magnitude than the area.  As an extreme example, a closed surface can possess arbitrarily large area, but its vector area is necessarily zero.<ref>{{cite book|first=Murray R.|last=Spiegel|title=Theory and problems of vector analysis|series=Schaum's Outline Series|publisher=McGraw Hill|date=1959|page=25}}</ref>  Surfaces that share a boundary may have very different areas, but they must have the same vector area—the vector area is entirely determined by the boundary.  These are consequences of [[Stokes' theorem]].\n\nThe concept of an area vector simplifies the equation for determining the [[flux]] through the surface. Consider a planar surface in a uniform [[Field (physics)|field]]. The flux can be written as the [[dot product]] of the field and area vector. This is much simpler than multiplying the field strength by the surface area and the cosine of the angle between the field and the surface normal.\n\n== Projection of area onto planes ==\nThe projected area onto (for example) the {{mvar|xy}}-plane is equivalent to the {{mvar|z}}-component of the vector area, and is given as\n\n:<math>\\mathbf{S}_z = \\left| \\mathbf{S} \\right| \\cos \\theta</math>\n\nwhere {{mvar|θ}} is the angle between the plane normal and the {{mvar|z}}-axis.\n\n== See also ==\n* [[Cross product]]\n* [[Bivector]]\n* [[Surface normal]]\n* [[Surface integral]]\n\n== Notes ==\n<references />\n\n[[Category:Area]]\n[[Category:Vectors (mathematics and physics)]]\n[[Category:Analytic geometry]]"
    },
    {
      "title": "Absolute convergence",
      "url": "https://en.wikipedia.org/wiki/Absolute_convergence",
      "text": "{{more footnotes|date=February 2013}}\nIn [[mathematics]], an [[series (mathematics)|infinite series]] of numbers is said to '''converge absolutely''' (or to be '''absolutely convergent''') if the sum of the [[absolute value]]s of the summands is finite<!-- don't link to [[finite set]], please -->.  More precisely, a real or complex series <math>\\textstyle\\sum_{n=0}^\\infty a_n</math> is said to '''converge absolutely''' if <math>\\textstyle\\sum_{n=0}^\\infty \\left|a_n\\right| = L</math> for some real number <math>\\textstyle L</math>. Similarly, an [[improper integral]] of a [[function (mathematics)|function]], <math>\\textstyle\\int_0^\\infty f(x)\\,dx</math>, is said to converge absolutely if the integral of the absolute value of the integrand is finite—that is, if <math>\\textstyle\\int_0^\\infty \\left|f(x)\\right|dx = L.</math>\n\nAbsolute convergence is important for the study of infinite series because its definition is strong enough to have properties of finite sums that not all convergent series possess, yet is broad enough to occur commonly. (A convergent series that is not absolutely convergent is called [[Conditional convergence|conditionally convergent]].)  Absolutely convergent series behave \"nicely\".  For instance, rearrangements do not change the value of the sum.  This is not true for conditionally convergent series: The [[alternating harmonic series]] <math display=\"inline\">1-\\frac{1}{2}+\\frac{1}{3}-\\frac{1}{4}+\\frac{1}{5}-\\frac{1}{6}+\\cdots</math> converges to <math>\\ln 2</math>, while its rearrangement <math display=\"inline\">1+\\frac{1}{3}-\\frac{1}{2}+\\frac{1}{5}+\\frac{1}{7}-\\frac{1}{4}+\\cdots</math> (in which the repeating pattern of signs is two positive terms followed by one negative term) converges to <math display=\"inline\">\\frac{3}{2}\\ln 2</math>.\n\n==Background==\nOne may study the convergence of series <math>\\sum_{n=0}^{\\infty} a_n </math> whose terms ''a<sub>n</sub>'' are elements of an arbitrary [[topological abelian group|abelian topological group]].  The notion of absolute convergence requires more structure, namely a [[Norm (mathematics)|norm]], which is a positive real-valued function <math>\\|\\cdot\\|: G \\to \\mathbb{R_+}</math> on an abelian group ''G'' (written [[Abelian group#Notation|additively]], with identity element 0) such that:\n# The norm of the identity element of ''G'' is zero: <math>\\|0\\| = 0.</math>\n# For every ''x'' in ''G'', <math>\\|x\\| = 0</math> implies <math>x = 0.</math>\n# For every ''x'' in ''G'', <math>\\|-x\\| = \\|x\\|.</math>\n# For every ''x'', ''y'' in ''G'', <math>\\|x+y\\| \\leq \\|x\\| + \\|y\\|.</math>\n\nIn this case, the function <math>d(x,y) = \\|x-y\\|</math> induces on ''G'' the structure of a [[metric space]] (a type of [[topology]]). We can therefore consider ''G''-valued series and define such a series to be absolutely convergent if <math>\\sum_{n=0}^{\\infty} \\|a_n\\| < \\infty.</math>\n\nIn particular, these statements apply using the norm |''x''| ([[absolute value]]) in the space of real numbers or complex numbers.\n\n==Relation to convergence==\nIf ''G'' is [[Complete (topology)|complete]] with respect to the metric ''d'', then every absolutely convergent series is convergent.  The proof is the same as for complex-valued series: use the completeness to derive the Cauchy criterion for convergence&mdash;a series is convergent if and only if its tails can be made arbitrarily small in norm&mdash;and apply the triangle inequality.<!-- too HOWTOish? -->\n\nIn particular, for series with values in any [[Banach space]], absolute convergence implies convergence. The converse is also true: if absolute convergence implies convergence in a normed space, then the space is a Banach space.<!--Banach spaces are specific here.-->\n\nIf a series is convergent but not absolutely convergent, it is called [[conditionally convergent]]. An example of a conditionally convergent series is the [[alternating harmonic series]]. Many standard tests for divergence and convergence, most notably including the [[ratio test]] and the [[root test]], demonstrate absolute convergence. This is because a [[power series]] is absolutely convergent on the interior of its disk<!--What's a 'disk'? Need a more standard synonym (or explanation)--> of convergence.\n\n===Proof that any absolutely convergent series of complex numbers is convergent===\nSuppose that <math display=\"inline\">\\sum |a_k|, a_k\\in\\mathbb{C}</math> is convergent. Then equivalently, <math display=\"inline\">\\sum [\\mathrm{Re}(a_k)^2+\\mathrm{Im}(a_k)^2]^{1/2}</math> is convergent, which implies that <math display=\"inline\">\\sum |\\mathrm{Re}(a_k)|</math> and <math display=\"inline\">\\sum|\\mathrm{Im}(a_k)|</math> converge by termwise comparison of non-negative terms.  It suffices to show that the convergence of these series implies the convergence of <math display=\"inline\">\\sum \\mathrm{Re}(a_k)</math> and <math display=\"inline\">\\sum \\mathrm{Im}(a_k)</math>, for then, the convergence of <math display=\"inline\">\\sum a_k=\\sum \\mathrm{Re}(a_k)+i\\sum\\mathrm{Im}(a_k)</math> would follow, by the definition of the convergence of complex-valued series. \n\nThe preceding discussion shows that we need only prove that convergence of <math display=\"inline\">\\sum |a_k|, a_k\\in\\mathbb{R}</math> implies the convergence of <math display=\"inline\">\\sum a_k</math>.\n\nLet <math display=\"inline\">\\sum |a_k|, a_k\\in\\mathbb{R}</math> be convergent.  Since  <math>0 \\leq a_k + |a_k| \\leq 2|a_k|</math>, we have\n:<math>0 \\leq \\sum_{k = 1}^n (a_k + |a_k|) \\leq \\sum_{k = 1}^n 2|a_k|</math>.\nSince <math display=\"inline\">\\sum 2|a_k|</math> is convergent, <math display=\"inline\">s_n=\\sum_{k = 1}^n (a_k + |a_k|)</math> is a bounded monotonic sequence of partial sums, and <math display=\"inline\">\\sum (a_k + |a_k|)</math> must also converge.  Noting that <math display=\"inline\">\\sum a_k = \\sum(a_k+|a_k|) - \\sum |a_k|</math> is the difference of convergent series, we conclude that it too is a convergent series, as desired.\n\n==== Alternative proof using the Cauchy criterion and triangle inequality ====\nBy applying the Cauchy criterion for the convergence of a complex series, we can also prove this fact as a simple implication of the [[triangle inequality]].<ref>{{Cite book|url=https://archive.org/details/1979RudinW|title=Principles of Mathematical Analysis|last=Rudin|first=Walter|publisher=McGraw-Hill|year=1976|isbn=0-07-054235-X|location=New York|pages=71–72|quote=|via=}}</ref>  By the [[Cauchy's convergence test|Cauchy criterion]], <math display=\"inline\">\\sum |a_i|</math> converges if and only if for any <math>\\epsilon>0</math>, there exists <math>N</math> such that <math display=\"inline\">\\big|\\sum_{i=m}^n |a_i|\\big|=\\sum_{i=m}^n |a_i|<\\epsilon</math> for any <math>m,n\\geq N</math>. But the triangle inequality implies that <math display=\"inline\">\\big|\\sum_{i=m}^n a_i\\big|\\leq\\sum_{i=m}^n |a_i|</math>, so that <math display=\"inline\">\\big|\\sum_{i=m}^n a_i\\big|<\\epsilon</math> for any <math>m,n\\geq N</math>, which is exactly the Cauchy criterion for <math display=\"inline\">\\sum a_i</math>.\n\n===Proof that any absolutely convergent series in a Banach space is convergent===\nThe above result can be easily generalized to every Banach space {{nowrap|(''X'', ǁ&thinsp;&sdot;&thinsp;ǁ)}}.  Let {{nowrap| &sum;''x''<sub>''n''</sub> }} be an absolutely convergent series in&nbsp;''X''.  As <math>\\scriptstyle\\sum_{k=1}^n\\|x_k\\|</math> is a Cauchy sequence of real numbers, for any {{nowrap|&epsilon; &gt; 0}} and large enough natural numbers {{nowrap|''m'' &gt; ''n''}} it holds:\n\n:<math>\\left|\\sum_{k=1}^m\\|x_k\\|-\\sum_{k=1}^n\\|x_k\\|\\right| = \\sum_{k=n+1}^m\\|x_k\\|< \\varepsilon.</math>\n\nBy the triangle inequality for the norm {{nowrap| ǁ&thinsp;&sdot;&thinsp;ǁ}}, one immediately gets:\n:<math>\\left\\|\\sum_{k=1}^m x_k-\\sum_{k=1}^nx_k\\right\\| = \\left\\|\\sum_{k=n+1}^m x_k\\right\\| \\le \\sum_{k=n+1}^m\\|x_k\\|<\\varepsilon,</math>\nwhich means that <math>\\scriptstyle\\sum_{k=1}^nx_k</math> is a Cauchy sequence in&nbsp;''X'', hence the series is convergent in&nbsp;''X''.<ref>{{citation\n | last = Megginson | first = Robert E. | authorlink = Robert Megginson\n | title = An introduction to Banach space theory\n | series = Graduate Texts in Mathematics\n | volume = 183 \n | publisher = Springer-Verlag\n | location = New York \n | year = 1998\n | isbn = 0-387-98431-3\n | page = 20\n}} (Theorem 1.3.9)</ref>\n\n==Rearrangements and unconditional convergence==\nIn the general context of a ''G''-valued series, a distinction is made between absolute and unconditional convergence, and the assertion that a real or complex series which is not absolutely convergent is necessarily conditionally convergent (meaning not unconditionally convergent) is then a theorem, not a definition.  This is discussed in more detail below.\n\nGiven a series <math>\\sum_{n=0}^{\\infty} a_n</math> with values in a normed abelian group ''G'' and a [[permutation]] σ of the natural numbers, one builds a new series <math>\\sum_{n=0}^\\infty a_{\\sigma(n)}</math>, said to be a rearrangement of the original series.  A series is said to be [[unconditionally convergent]] if all rearrangements of the series are convergent to the same value.\n\nWhen ''G'' is complete, absolute convergence implies unconditional convergence:\n\n:'''Theorem.''' Let \n::<math>\\sum_{i=1}^\\infty a_i=A \\in G, \\quad \\sum_{i=1}^\\infty \\|a_i\\|<\\infty</math> \n:and let {{math|''σ'' : '''N''' → '''N'''}} be a permutation. Then: \n::<math>\\sum_{i=1}^\\infty a_{\\sigma(i)}=A.</math>\n\nThe issue of the converse is interesting.  For real series it follows from the [[Riemann rearrangement theorem]] that unconditional convergence implies absolute convergence.  Since a series with values in a finite-dimensional normed space is absolutely convergent if each of its one-dimensional projections is absolutely convergent, it follows that absolute and unconditional convergence coincide for '''R'''<sup>''n''</sup>-valued series.\n\nBut there are unconditionally and non-absolutely convergent series with values in [[Banach space]] ℓ<sup>∞</sup>, for example:\n \n:<math>a_n = \\tfrac{1}{n} e_n,</math>\n\nwhere <math>\\{e_n\\}_{n=1}^{\\infty}</math> is an orthonormal basis.  A theorem of [[Aryeh Dvoretzky|A.&nbsp;Dvoretzky]] and [[Claude Ambrose Rogers|C.&nbsp;A.&nbsp;Rogers]] asserts that every infinite-dimensional Banach space admits an unconditionally  convergent series that is not absolutely convergent.<ref>Dvoretzky, A.; Rogers, C. A. (1950), \"Absolute and unconditional convergence in normed linear spaces\", Proc. Natl. Acad. Sci. U.S.A. '''36''':192&ndash;197.</ref>\n\n===Proof of the theorem===\nFor any ε > 0, we can choose some <math>\\kappa_\\varepsilon,\\lambda_\\varepsilon \\in \\mathbf{N}</math>, such that:\n\n:<math>\\begin{align}\n\\forall N>\\kappa_\\varepsilon &\\quad \\sum_{n=N}^\\infty \\|a_n\\| < \\tfrac{\\varepsilon}{2} \\\\ \n\\forall N>\\lambda_\\varepsilon &\\quad \\left\\|\\sum_{n=1}^N a_n-A\\right\\| < \\tfrac{\\varepsilon}{2}\n\\end{align}</math>\n\nLet\n\n:<math>\\begin{align}\nN_\\varepsilon &=\\max \\left \\{ \\kappa_\\varepsilon, \\lambda_\\varepsilon \\right \\} \\\\\nM_{\\sigma,\\varepsilon} &= \\max \\left\\{  \\sigma^{-1}\\left( \\left \\{ 1,\\dots,N_\\varepsilon \\right \\}\\right) \\right\\}\n\\end{align}</math>\n\nFinally for any integer <math> N > M_{\\sigma,\\varepsilon}</math> let\n\n:<math>\\begin{align}\nI_{\\sigma,\\varepsilon} &= \\left\\{ 1,\\ldots,N \\right\\}\\setminus \\sigma^{-1}\\left( \\left \\{ 1,\\dots,N_\\varepsilon \\right \\}\\right) \\\\\nS_{\\sigma,\\varepsilon} &= \\min \\left \\{ \\sigma(k) \\ : \\ k \\in I_{\\sigma,\\varepsilon} \\right \\} \\\\\nL_{\\sigma,\\varepsilon} &= \\max \\left \\{ \\sigma(k) \\ : \\ k \\in I_{\\sigma,\\varepsilon} \\right \\}\n\\end{align}</math>\n\nThen\n\n:<math>\\begin{align}\n\\left\\|\\sum_{i=1}^N a_{\\sigma(i)}-A \\right\\| &= \\left\\| \\sum_{i \\in \\sigma^{-1}\\left(\\{ 1,\\dots,N_\\varepsilon \\}\\right)} a_{\\sigma(i)} - A  +  \n\\sum_{i\\in I_{\\sigma,\\varepsilon}} a_{\\sigma(i)} \\right\\| \\\\\n&\\leq \\left\\|\\sum_{j=1}^{N_\\varepsilon} a_j - A \\right\\| + \\left\\|\\sum_{i\\in I_{\\sigma,\\varepsilon}} a_{\\sigma(i)} \\right\\| \\\\\n&\\leq \\left\\|\\sum_{j=1}^{N_\\varepsilon} a_j - A \\right\\| + \\sum_{i \\in I_{\\sigma,\\varepsilon}} \\left \\| a_{\\sigma(i)} \\right \\| \\\\\n&\\leq \\left\\|\\sum_{j=1}^{N_\\varepsilon} a_j - A \\right\\| + \\sum_{j = S_{\\sigma,\\varepsilon}}^{L_{\\sigma,\\varepsilon}} \\left \\| a_j \\right \\| \\\\\n&\\leq \\left\\|\\sum_{j=1}^{N_\\varepsilon} a_j - A \\right\\| + \\sum_{j = N_\\varepsilon + 1}^{\\infty} \\left \\| a_j \\right \\| && S_{\\sigma,\\varepsilon} \\geq N_{\\varepsilon}+1\\\\\n&< \\varepsilon\n\\end{align}</math>\n\nThis shows that\n\n:<math> \\forall\\varepsilon > 0, \\exists M_{\\sigma,\\varepsilon}, \\forall N > M_{\\sigma,\\varepsilon} \\quad \\left\\|\\sum_{i=1}^N a_{\\sigma(i)}-A \\right\\|< \\varepsilon, </math>\n\nthat is:\n\n:<math>\\sum_{i=1}^\\infty a_{\\sigma(i)}=A</math>\n\n[[Q.E.D.]]\n\n==Products of series==\nThe [[Cauchy product]] of two series converges to the product of the sums if at least one of the series converges absolutely.  That is, suppose that\n\n:<math>\\sum_{n=0}^\\infty a_n = A</math> and <math>\\sum_{n=0}^\\infty b_n = B</math>.\n\nThe Cauchy product is defined as the sum of terms ''c<sub>n</sub>'' where:\n\n:<math>c_n = \\sum_{k=0}^n a_k b_{n-k}.</math>\n\nThen, if ''either'' the ''a<sub>n</sub>'' or ''b<sub>n</sub>'' sum converges absolutely, then\n\n:<math>\\sum_{n=0}^\\infty c_n = AB.</math>\n\n==Absolute convergence of integrals==\nThe integral <math>\\int_A f(x)\\,dx</math> of a real or complex-valued function is said to '''converge absolutely''' if <math>\\int_A \\left|f(x)\\right|\\,dx < \\infty.</math>  One also says that <math>f</math> is '''absolutely integrable'''.  The issue of absolute integrability is intricate and depends on whether the Riemann, Lebesgue, or Kurzweil-Henstock (gauge) integral is considered; for the Riemann integral, it also depends on whether we only consider integrability in its proper sense (<math>f</math> and <math>A</math> both bounded), or permit the more general case of improper integrals. \n\nAs a standard property of the Riemann integral, when <math>A=[a,b]</math> is a bounded interval, every continuous function is bounded and (Riemann) integrable, and since <math>f</math> continuous implies <math>|f|</math> continuous, every continuous function is absolutely integrable.  In fact, since <math>g\\circ f</math> is Riemann integrable on <math>[a,b]</math> if <math>f</math> is (properly) integrable and <math>g</math> is continuous, it follows that <math>|f|=|\\cdot|\\circ f</math> is properly Riemann integrable if <math>f</math> is.  However, this implication does not hold in the case of improper integrals.  For instance, the function <math>f:[1,\\infty)\\to\\mathbb{R},\\ \\ x\\mapsto x^{-1}\\sin x</math> is improperly Riemann integrable on its unbounded domain, but it is not absolutely integrable: <blockquote><math>\\int_1^{\\infty} \\frac{\\sin x}{x} dx=\\frac{1}{2}\\big[\\pi - 2\\mathrm{Si}(1)\\big]\\approx 0.62</math>, but <math>\\int_1^{\\infty} \\Big|\\frac{\\sin x}{x}\\Big|dx=\\infty</math>.   </blockquote>Indeed, more generally, given any series <math>\\sum_{n=0}^\\infty a_n </math> one can consider the associated step function <math>f_a: [0,\\infty) \\rightarrow \\mathbb{R}</math> defined by <math>f_a([n,n+1)) = a_n</math>.  Then <math>\\int_0^\\infty f_a \\, dx</math> converges absolutely, converges conditionally or diverges according to the corresponding behavior of <math>\\sum_{n=0}^\\infty a_n. </math>\n\nThe situation is different for the Lebesgue integral, which does not handle bounded and unbounded domains of integration separately (''see below'').  The fact that the integral of <math>|f|</math> is unbounded in the examples above implies that <math>f</math> is also not integrable in the Lebesgue sense.  In fact, in the Lebesgue theory of integration, given that <math>f</math> is measurable, <math>f </math> is (Lebesgue) integrable if and only if <math>|f|</math> is (Lebesgue) integrable.  However, the hypothesis that <math>f</math> is measurable is crucial; it is not generally true that absolutely integrable functions on <math>[a,b]</math> are integrable (simply because they may fail to be measurable): let <math>S \\subset [a,b]</math> be a nonmeasurable subset and consider <math>f = \\chi_S - 1/2,</math> where <math>\\chi_S</math> is the characteristic function of <math>S</math>.  Then <math>f</math> is not Lebesgue measurable and thus not integrable, but <math>|f|\\equiv 1/2</math> is a constant function and clearly integrable.  \n\nOn the other hand, a function <math>f</math> may be Kurzweil-Henstock integrable (or \"gauge integrable\") while <math>|f|</math> is not.  This includes the case of improperly Riemann integrable functions.\n\nIn a general sense, on any measure space <math>A</math>, the Lebesgue integral of a real-valued function is defined in terms of its positive and negative parts, so the facts:\n\n# ''f'' integrable implies |''f''| integrable\n# ''f'' measurable, |''f''| integrable implies ''f'' integrable\n\nare essentially built into the definition of the Lebesgue integral.  In particular, applying the theory to the counting measure on a set ''S'', one recovers the notion of unordered summation of series developed by Moore–Smith using (what are now called) nets.  When ''S'' = '''N''' is the set of natural numbers, Lebesgue integrability, unordered summability and absolute convergence all coincide.\n\nFinally, all of the above holds for integrals with values in a Banach space.  The definition of a Banach-valued Riemann integral is an evident modification of the usual one.  For the Lebesgue integral one needs to circumvent the decomposition into positive and negative parts with Daniell's more functional analytic approach, obtaining the [[Bochner integral]].\n\n==See also==\n*[[Convergence of Fourier series]]\n*[[Conditional convergence]]\n*[[Modes of convergence (annotated index)]]\n*[[Cauchy principal value]]\n*[[Fubini's theorem]]\n*[[1/2 − 1/4 + 1/8 − 1/16 + · · ·]]\n*[[1/2 + 1/4 + 1/8 + 1/16 + · · ·]]\n\n==Notes==\n<references/>\n\n==References==\n* Walter Rudin, ''Principles of Mathematical Analysis'' (McGraw-Hill: New York, 1964).\n\n[[Category:Mathematical series]]\n[[Category:Integral calculus]]\n[[Category:Summability theory]]\n[[Category:Convergence (mathematics)]]"
    },
    {
      "title": "Antiderivative",
      "url": "https://en.wikipedia.org/wiki/Antiderivative",
      "text": "{{for|lists of antiderivatives of primitive functions|lists of integrals}}\n{{Calculus |Integral}}\n[[File:Slope Field.png|thumb|The [[slope field]] of <math>F(x) = \\frac{x^3}{3}-\\frac{x^2}{2}-x+c</math>, showing three of the infinitely many solutions that can be produced by varying the [[Constant of integration|arbitrary constant]] {{math|''c''}}.]]\n\nIn [[calculus]], an '''antiderivative''', '''primitive function''', '''primitive integral''' or '''indefinite integral'''{{#tag:ref|Antiderivatives are also called '''general integrals''', and sometimes '''integrals'''. The latter term is generic, and refers not only to indefinite integrals (antiderivatives), but also to [[definite integral]]s. When the word ''integral'' is used without additional specification, the reader is supposed to deduce from the context whether it refers to a definite or indefinite integral. Some authors define the indefinite integral of a function as the set of its infinitely many possible antiderivatives. Others define it as an arbitrarily selected element of that set. This article adopts the latter approach.|group=Note}} of a [[function (mathematics)|function]] {{math|''f''}} is a differentiable function {{math|''F''}} whose [[derivative]] is equal to the original function {{math|''f''}}. This can be stated symbolically as <math>F' = f</math>.<ref>{{cite book | last=Stewart | first=James | authorlink=James Stewart (mathematician) | title=Calculus: Early Transcendentals |publisher=[[Brooks/Cole]] | edition=6th | year=2008 | isbn=0-495-01166-5}}</ref><ref>{{cite book | last1=Larson | first1=Ron | authorlink=Ron Larson (mathematician)| last2=Edwards | first2=Bruce H. | title=Calculus | publisher=[[Brooks/Cole]] | edition=9th | year=2009 | isbn=0-547-16702-4}}</ref> The process of solving for antiderivatives is called '''antidifferentiation''' (or '''indefinite integration''') and its opposite operation is called differentiation, which is the process of finding a derivative.\n\nAntiderivatives are related to [[integral|definite integral]]s through the [[fundamental theorem of calculus]]: the definite integral of a function over an [[interval (mathematics)|interval]] is equal to the difference between the values of an antiderivative evaluated at the endpoints of the interval.\n\nThe discrete equivalent of the notion of antiderivative is [[antidifference]].\n\n==Example==\nThe function <math>F(x) = \\frac{x^3}{3}</math> is an antiderivative of <math>f(x) = x^2</math>, as the derivative of <math>\\frac{x^3}{3}</math> is <math>x^2</math>. As the derivative of a [[Constant function|constant]] is [[0 (number)|zero]], <math>x^2</math> will have an [[Infinite set|infinite]] number of antiderivatives, such as <math>\\frac{x^3}{3}</math>, <math>\\frac{x^3}{3}+1</math>, <math>\\frac{x^3}{3}-2</math>, etc. Thus, all the antiderivatives of <math>x^2</math> can be obtained by changing the value of {{math|''c''}} in <math>F(x) = \\frac{x^3}{3}+c</math>, where {{math|''c''}} is an arbitrary constant known as the [[constant of integration]]. Essentially, the [[graph of a function|graphs]] of antiderivatives of a given function are [[vertical translation]]s of each other; each graph's vertical location depending upon the [[Value (mathematics)|value]] {{math|''c''}}.\n\nIn physics, the integration of acceleration yields velocity plus a constant. The constant is the initial velocity term that would be lost upon taking the derivative of velocity because the derivative of a constant term is zero. This same pattern applies to further integrations and derivatives of motion (position, velocity, acceleration, and so on).\n\n==Uses and properties==\nAntiderivatives can be used to [[integral#Calculating integrals|compute definite integrals]], using the [[fundamental theorem of calculus]]: if {{math|''F''}} is an antiderivative of the [[Riemann integral|integrable]] function {{math|''f''}} over the interval <math>[a,b]</math>, then:\n\n:<math>\\int_a^b f(x)\\,dx = F(b) - F(a).</math>\n\nBecause of this, each of the infinitely many antiderivatives of a given function {{math|''f''}} is sometimes called the \"general integral\" or \"indefinite integral\" of ''f'' and is written using the integral symbol with no bounds:\n:<math>\\int f(x)\\, dx.</math>\n\nIf {{math|''F''}} is an antiderivative of {{math|''f''}}, and the function {{math|''f''}} is defined on some interval, then every other antiderivative {{math|''G''}} of {{math|''f''}} differs from {{math|''F''}} by a constant: there exists a number {{math|''c''}} such that <math>G(x) = F(x)+c</math> for all {{math|''x''}}. {{math|''c''}} is called the [[constant of integration]]. If the domain of {{math|''F''}} is a [[disjoint union]] of two or more (open) intervals, then a different constant of integration may be chosen for each of the intervals. For instance\n\n:<math>F(x)=\\begin{cases}-\\frac{1}{x}+c_1\\quad x<0\\\\-\\frac{1}{x}+c_2\\quad x>0\\end{cases}</math>\n\nis the most general antiderivative of <math>f(x)=1/x^2</math> on its natural domain <math>(-\\infty,0)\\cup(0,\\infty).</math>\n\nEvery [[continuous function]] {{math|''f''}} has an antiderivative, and one antiderivative {{math|''F''}} is given by the definite integral of {{math|''f''}} with variable upper boundary:\n:<math>F(x)=\\int_0^x f(t)\\,dt.</math>\nVarying the lower boundary produces other antiderivatives (but not necessarily all possible antiderivatives). This is another formulation of the [[fundamental theorem of calculus]].\n\nThere are many functions whose antiderivatives, even though they exist, cannot be expressed in terms of [[elementary function]]s (like [[polynomial]]s, [[exponential function]]s, [[logarithm]]s, [[trigonometric functions]], [[inverse trigonometric functions]] and their combinations). Examples of these are\n:<math>\\int e^{-x^2}\\,dx,\\qquad \\int \\sin x^2\\,dx, \\qquad\\int \\frac{\\sin x}{x}\\,dx,\\qquad \\int\\frac{1}{\\ln x}\\,dx,\\qquad \\int x^{x}\\,dx.</math>\n''From left to right, the first four are the [[error function]], the [[Fresnel function]], the [[trigonometric integral]], and the [[logarithmic integral function]].''\n\nSee also [[Differential Galois theory]] for a more detailed discussion.\n\n==Techniques of integration==\nFinding antiderivatives of elementary functions is often considerably harder than finding their derivatives. For some elementary functions, it is impossible to find an antiderivative in terms of other elementary functions. See the articles on [[Elementary function (differential algebra)|elementary functions]] and [[nonelementary integral]] for further information.\n\nThere are various methods available:\n\n* the [[linearity of integration]] allows us to break complicated integrals into simpler ones\n* [[integration by substitution]], often combined with [[trigonometric identity|trigonometric identities]] or the [[natural logarithm]]\n** the [[inverse chain rule method]], a special case of integration by substitution\n* [[integration by parts]] to integrate products of functions\n* [[Inverse function integration]], a formula that expresses the antiderivative of the inverse <math>f^{-1}</math> of an invertible and continuous function <math>f</math> in terms of the antiderivative of <math>f</math> and of <math>f^{-1}</math>.\n* the method of [[partial fractions in integration]] allows us to integrate all [[rational function]]s (fractions of two polynomials)\n* the [[Risch algorithm]]\n* when integrating multiple times, certain additional techniques can be used, see for instance [[double integral]]s and [[Polar coordinate system|polar coordinates]], the [[Jacobian matrix and determinant|Jacobian]] and the [[Stokes' theorem]]\n* if a function has no elementary antiderivative (for instance, <math>\\exp (-x^2)</math>), its definite integral can be approximated using [[numerical integration]]\n* it is often convenient to algebraically manipulate the integrand such that other integration techniques, such as integration by substitution, may be used.\n* to calculate the ({{math|''n''}} times) repeated antiderivative of a function {{math|''f''}}, [[Cauchy]]'s formula is useful (cf. [[Cauchy formula for repeated integration]]):\n::<math>\\int_{x_0}^x \\int_{x_0}^{x_1} \\dots \\int_{x_0}^{x_{n-1}} f(x_n) \\,dx_n \\dots \\, dx_2\\, dx_1= \\int_{x_0}^x f(t) \\frac{(x-t)^{n-1}}{(n-1)!}\\,dt.</math>\n\n[[Computer algebra system]]s can be used to automate some or all of the work involved in the symbolic techniques above, which is particularly useful when the algebraic manipulations involved are very complex or lengthy. Integrals which have already been derived can be looked up in a [[table of integrals]].\n\n==Of non-continuous functions==\nNon-continuous functions can have antiderivatives. While there are still open questions in this area, it is known that:\n* Some highly [[pathological (mathematics)|pathological functions]] with large sets of discontinuities may nevertheless have antiderivatives.\n* In some cases, the antiderivatives of such pathological functions may be found by [[Riemann integral|Riemann integration]], while in other cases these functions are not Riemann integrable.\n\nAssuming that the domains of the functions are open intervals:\n* A necessary, but not sufficient, condition for a function {{math|''f''}} to have an antiderivative is that {{math|''f''}} have the [[intermediate value theorem|intermediate value property]]. That is, if {{math|[''a'', ''b'']}} is a subinterval of the domain of {{math|''f''}} and {{math|''y''}} is any real number between {{math|''f''(''a'')}} and {{math|''f''(''b'')}}, then there exists a {{mvar|c}} between {{mvar|a}} and {{mvar|b}} such that {{math|1=''f''(''c'') = ''y''}}. This is a consequence of [[Darboux's theorem (analysis)|Darboux's theorem]].\n* The set of discontinuities of {{math|''f''}} must be a [[meagre set]]. This set must also be an [[F-sigma]] set (since the set of discontinuities of any function must be of this type). Moreover, for any meagre F-sigma set, one can construct some function {{math|''f''}} having an antiderivative, which has the given set as its set of discontinuities.\n* If {{math|''f''}} has an antiderivative, is [[bounded function|bounded]] on closed finite subintervals of the domain and has a set of discontinuities of [[Lebesgue measure]] 0, then an antiderivative may be found by integration in the sense of Lebesgue. In fact, using more powerful integrals like the [[Henstock–Kurzweil integral]], every function for which an antiderivative exists is integrable, and its general integral coincides with its antiderivative.\n* If {{math|''f''}} has an antiderivative {{math|''F''}} on a closed interval <math>[a,b]</math>, then for any choice of partition <math>a=x_0<x_1<x_2<\\dots<x_n=b</math>, if one chooses sample points <math>x_i^*\\in[x_{i-1},x_i]</math> as specified by the [[mean value theorem]], then the corresponding Riemann sum [[telescoping series|telescopes]] to the value <math>F(b)-F(a)</math>.\n\n:: <math>\n\\begin{align}\n\\sum_{i=1}^n f(x_i^*)(x_i-x_{i-1}) & = \\sum_{i=1}^n [F(x_i)-F(x_{i-1})] \\\\\n& = F(x_n)-F(x_0) = F(b)-F(a)\n\\end{align}\n</math>\n\n:However if {{math|''f''}} is unbounded, or if {{math|''f''}} is bounded but the set of discontinuities of {{math|''f''}} has positive Lebesgue measure, a different choice of sample points <math>x_i^*</math> may give a significantly different value for the Riemann sum, no matter how fine the partition. See Example 4 below.\n\n===Some examples===\n{{ordered list\n|1= The function\n:<math>f(x)=2x\\sin\\left(\\frac{1}{x}\\right)-\\cos\\left(\\frac{1}{x}\\right)</math>\n\nwith <math>f\\left(0\\right)=0</math> is not continuous at <math>x=0</math> but has the antiderivative\n\n:<math>F\\left(x\\right)=x^2\\sin\\left(\\frac{1}{x}\\right)</math>\n\nwith <math>F\\left(0\\right)=0</math>. Since {{math|''f''}} is bounded on closed finite intervals and is only discontinuous at 0, the antiderivative {{math|''F''}} may be obtained by integration: <math>F(x)=\\int_0^x f(t)\\,dt</math>.\n|2= The function\n\n:<math>f(x)=2x\\sin\\left(\\frac{1}{x^2}\\right)-\\frac{2}{x}\\cos\\left(\\frac{1}{x^2}\\right)</math>\n\nwith <math>f\\left(0\\right)=0</math> is not continuous at <math>x=0</math> but has the antiderivative\n\n:<math>F(x)=x^2\\sin\\left(\\frac{1}{x^2}\\right)</math>\n\nwith <math>F\\left(0\\right)=0</math>. Unlike Example 1, {{math|''f''(''x'')}} is unbounded in any interval containing 0, so the Riemann integral is undefined.\n\n|3= If {{math|''f''(''x'')}} is the function in Example 1 and {{math|''F''}} is its antiderivative, and <math>\\{x_n\\}_{n\\ge1}</math> is a [[dense set|dense]] [[countable]] [[subset]] of the open interval <math>\\left(-1,1\\right)</math>, then the function\n\n:<math>g(x)=\\sum_{n=1}^\\infty \\frac{f(x-x_n)}{2^n}</math>\n\nhas an antiderivative\n\n:<math>G(x)=\\sum_{n=1}^\\infty \\frac{F(x-x_n)}{2^n}.</math>\n\nThe set of discontinuities of {{math|''g''}} is precisely the set <math>\\{x_n\\}_{n\\ge1}</math>. Since {{math|''g''}} is bounded on closed finite intervals and the set of discontinuities has measure 0, the antiderivative {{math|''G''}} may be found by integration.\n\n|4= Let <math>\\{x_n\\}_{n\\ge1}</math> be a [[dense set|dense]] [[countable]] subset of the open interval <math>\\left(-1,1\\right)</math>. Consider the everywhere continuous strictly increasing function\n\n:<math>F(x)=\\sum_{n=1}^\\infty\\frac{1}{2^n}(x-x_n)^{1/3}.</math>\n\nIt can be shown that\n\n:<math>F'(x)=\\sum_{n=1}^\\infty\\frac{1}{3\\cdot2^n}(x-x_n)^{-2/3}</math>\n[[Image:Antideriv1.png|125px|right|thumb|Figure 1.]]\n[[Image:Antideriv2.png|thumb|right|125px|Figure 2.]]\n\nfor all values {{math|''x''}} where the series converges, and that the graph of {{math|''F''(''x'')}} has vertical tangent lines at all other values of {{math|''x''}}. In particular the graph has vertical tangent lines at all points in the set <math>\\{x_n\\}_{n\\ge1}</math>.\n\nMoreover <math>F\\left(x\\right)\\ge0</math> for all {{math|''x''}} where the derivative is defined. It follows that the inverse function <math>G=F^{-1}</math> is differentiable everywhere and that\n\n:<math>g\\left(x\\right)=G'\\left(x\\right)=0</math>\n\nfor all {{math|''x''}} in the set <math>\\{F(x_n)\\}_{n\\ge1}</math> which is dense in the interval <math>\\left[F\\left(-1\\right),F\\left(1\\right)\\right]</math>. Thus {{math|''g''}} has an antiderivative {{math|''G''}}. On the other hand, it can not be true that\n\n:<math>\\int_{F(-1)}^{F(1)}g(x)\\,dx=GF(1)-GF(-1)=2,</math>\n\nsince for any partition of <math>\\left[F\\left(-1\\right),F\\left(1\\right)\\right]</math>, one can choose sample points for the Riemann sum from the set <math>\\{F(x_n)\\}_{n\\ge1}</math>, giving a value of 0 for the sum. It follows that {{math|''g''}} has a set of discontinuities of positive Lebesgue measure. Figure 1 on the right shows an approximation to the graph of {{math|''g''(''x'')}} where <math>\\{x_n=\\cos(n)\\}_{n\\ge1}</math> and the series is truncated to 8 terms. Figure 2 shows the graph of an approximation to the antiderivative {{math|''G''(''x'')}}, also truncated to 8 terms. On the other hand if the Riemann integral is replaced by the [[Lebesgue integral]], then [[Fatou's lemma]] or the [[dominated convergence theorem]] shows that {{math|''g''}} does satisfy the fundamental theorem of calculus in that context.\n\n|5= In Examples 3 and 4, the sets of discontinuities of the functions {{math|''g''}} are dense only in a finite open interval <math>\\left(a,b\\right)</math>. However, these examples can be easily modified so as to have sets of discontinuities which are dense on the entire real line <math>(-\\infty,\\infty)</math>. Let\n:<math>\\lambda(x) = \\frac{a+b}{2} + \\frac{b-a}{\\pi}\\tan^{-1} x.</math>\nThen <math>g\\left(\\lambda(x)\\right)\\lambda'(x)</math> has a dense set of discontinuities on <math>(-\\infty,\\infty)</math> and has antiderivative <math>G\\cdot\\lambda.</math>\n\n|6= Using a similar method as in Example 5, one can modify {{math|''g''}} in Example 4 so as to vanish at all [[rational numbers]]. If one uses a naive version of the [[Riemann integral]] defined as the limit of left-hand or right-hand Riemann sums over regular partitions, one will obtain that the integral of such a function {{math|''g''}} over an interval <math>\\left[a,b\\right]</math> is 0 whenever {{math|''a''}} and {{math|''b''}} are both rational, instead of <math>G\\left(b\\right)-G\\left(a\\right)</math>. Thus the fundamental theorem of calculus will fail spectacularly.\n\n|7= A function which has an antiderivative may still fail to be Riemann integrable.  The derivative of [[Volterra's function]] is an example.\n\n}}\n\n==See also==\n* [[Antiderivative (complex analysis)]]\n* [[Lists of integrals]]\n* [[Symbolic integration]]\n* [[Area]]\n\n==Notes==\n{{reflist|group=Note}}\n\n==References==\n<div class=\"references\">\n<references />\n</div>\n\n==Further reading==\n* ''Introduction to Classical Real Analysis'', by Karl R. Stromberg; Wadsworth, 1981 (see [https://groups.google.com/group/sci.math/browse_frm/thread/8d900a2d79429d43/0ba4ff0d46efe076?lnk=st&q=&rnum=19&hl=en#0ba4ff0d46efe076 also])\n* [https://groups.google.com/group/sci.math/msg/814be41b1ea8c024 ''Historical Essay On Continuity Of Derivatives''] by Dave L. Renfro\n\n==External links==\n* [http://integrals.wolfram.com Wolfram Integrator] — Free online symbolic integration with [[Mathematica]]\n* [http://um.mendelu.cz/maw-html/index.php?lang=en&form=integral Mathematical Assistant on Web] — symbolic computations online. Allows users to integrate in small steps (with hints for next step (integration by parts, substitution, partial fractions, application of formulas and others), powered by [[Maxima (software)|Maxima]]\n* [http://wims.unice.fr/wims/wims.cgi?module=tool/analysis/function.en Function Calculator] from WIMS\n* [http://hyperphysics.phy-astr.gsu.edu/hbase/integ.html Integral] at [[HyperPhysics]]\n* [https://www.khanacademy.org/video/antiderivatives-and-indefinite-integrals Antiderivatives and indefinite integrals] at the [[Khan Academy]]\n* [http://www.symbolab.com/solver/integral-calculator Integral calculator] at [[Symbolab]]\n* [http://www-math.mit.edu/~djk/calculus_beginners/chapter16/section01.html The Antiderivative] at [[MIT]]\n* [http://www.sparknotes.com/math/calcab/introductiontointegrals/section1.rhtml Introduction to Integrals] at [[SparkNotes]]\n* [https://www.math.hmc.edu/calculus/tutorials/antiderivatives/ Antiderivatives] at Harvy Mudd College\n\n[[Category:Integral calculus]]\n[[Category:Linear operators in calculus]]"
    },
    {
      "title": "Arc length",
      "url": "https://en.wikipedia.org/wiki/Arc_length",
      "text": "{{Short description|Distance along a curve}}\n[[File:Arc length.gif|right|frame|When rectified, the curve gives a straight line segment with the same length as the curve's arc length.]]\n[[File:Logarithmic spiral arc length.gif|thumb|right|Arc length ''s'' of a [[logarithmic spiral]] as a function of its parameter ''θ''.]]\n\n'''Arc length''' is the distance between two points along a section of a [[curve]].\n\nDetermining the length of an irregular arc segment is also called {{em|rectification}} of a curve. The advent of [[infinitesimal calculus]] led to a general formula that provides [[closed-form expression|closed-form solutions]] in some cases.\n\n== General approach ==\n[[File:Arclength.svg|400px|right|thumb|Approximation by multiple linear segments]]\n\nA [[curve]] in the [[Euclidean space|plane]] can be approximated by connecting a [[wiktionary:Finite|finite]] number of [[point (geometry)|points]] on the curve using [[line segment]]s to create a [[polygonal chain|polygonal path]]. Since it is straightforward to calculate the [[length]] of each linear segment (using the [[Pythagorean theorem]] in Euclidean space, for example), the total length of the approximation can be found by [[summation|summing]] the lengths of each linear segment; {{anchor|Chordal distance}}that approximation is known as the ''(cumulative) [[chord (geometry)|chordal]] distance''.<ref>{{cite book|page=51|last1=Ahlberg|last2=Nilson|date=1967|title=The Theory of Splines and Their Applications|publisher=Academic Press|url=https://books.google.com/books?id=S7d1pjJHsRgC&lpg=PA51|isbn=9780080955452}}</ref>\n\nIf the curve is not already a polygonal path, using a progressively larger number of segments of smaller lengths will result in better approximations. The lengths of the successive approximations will not decrease and may keep increasing indefinitely, but for smooth curves they will tend to a finite limit as the lengths of the segments get [[arbitrarily large|arbitrarily small]].\n\nFor some curves there is a smallest number <math>L</math> that is an upper bound on the length of any polygonal approximation. These curves are called {{em|rectifiable}} and the number <math>L</math> is defined as the {{em|arc length}}.\n\n==Definition for a smooth curve==\n{{See also|Curve#Length_of_a_curve|l1=Length of a curve}}\n\nLet <math>f\\colon[a,b]\\to\\R^n</math> be a [[Continuous function|continuously]] [[Differentiable function|differentiable]] function.  The length of the curve defined by <math>f</math> can be defined as the [[Limit (mathematics)|limit]] of the sum of line segment lengths for a regular partition of <math>[a,b]</math> as the number of segments approaches infinity.  This means\n\n:<math>L(f)=\\lim_{N\\to\\infty}\\sum_{i=1}^N \\bigg|f(t_i)-f(t_{i-1})\\bigg|</math>\n\nwhere <math>t_i=a+i(b-a)/N=a+i\\Delta t</math> for <math>i=0,1,\\dotsc,N.</math> This definition is equivalent to the standard definition of arc length as an integral:\n\n:<math>\\lim_{N\\to\\infty}\\sum_{i=1}^N \\bigg|f(t_i)-f(t_{i-1})\\bigg|=\\lim_{N\\to\\infty}\\sum_{i=1}^N \\left|\\frac{f(t_i)-f(t_{i-1})}{\\Delta t}\\right|\\Delta t=\\int_a^b \\Big|f'(t)\\Big|\\ dt.</math>\n\nThe last equality above is true because the definition of the derivative as a limit implies that there is a positive real function <math>\\delta(\\epsilon)</math>  of positive real <math>\\epsilon</math> such that <math>\\Delta t<\\delta(\\epsilon)</math> implies <math>\\left|\\bigg|\\frac{f(t_i)-f(t_{i-1})}{\\Delta t}\\bigg|-\\Big|f'(t_i)\\Big|\\right|<\\epsilon.</math> This means\n\n:<math>\n\\sum_{i=1}^N \\left|\\frac{f(t_i)-f(t_{i-1})}{\\Delta t}\\right|\\Delta t-\\sum_{i=1}^N \\Big|f'(t_i)\\Big|\\Delta t\n</math>\n\nhas absolute value less than <math>\\epsilon (b-a)</math> for <math>N>(b-a)/\\delta(\\epsilon).</math>  This means that in the limit <math>N\\rightarrow\\infty,</math> the left term above equals the right term, which is just the [[Riemann integral]] of <math>|f'(t)|</math> on  <math>[a,b].</math>  This definition of arc length shows that the length of a curve <math>f:[a,b]\\rightarrow\\mathbb{R}^n</math> continuously differentiable on <math>[a,b]</math> is always finite.  In other words, the curve is always rectifiable.\n\nThe definition of arc length of a smooth curve as the integral of the norm of the derivative is equivalent to the definition\n\n:<math>L(f)=\\sup\\sum_{i=1}^N \\bigg|f(t_i)-f(t_{i-1})\\bigg|</math>\n\nwhere the [[Infimum and supremum|supremum]] is taken over all possible partitions <math>a=t_0<t_1<\\dots <t_{N-1}<t_N=b</math> of <math>[a,b].</math><ref>{{Cite book|title = Principles of Mathematical Analysis|last = Rudin|first = Walter|publisher = McGraw-Hill, Inc.|year = 1976|isbn = 978-0-07-054235-8|location = |pages = 137}}</ref> This definition is also valid if <math>f</math> is merely continuous, not differentiable.\n\nA curve can be parameterized in infinitely many ways. Let <math>\\varphi:[a,b]\\to [c,d]</math> be any continuously differentiable [[bijection]]. Then <math>g=f\\circ\\varphi^{-1}:[c,d]\\to\\R^n</math> is another continuously differentiable parameterization of the curve originally defined by <math>f.</math> The arc length of the curve is the same regardless of the parameterization used to define the curve:\n\n:<math>\\begin{align}\nL(f) &= \\int_a^b \\Big|f'(t)\\Big|\\ dt=\\int_a^b \\Big|g'(\\varphi(t))\\varphi'(t)\\Big|\\ dt \\\\\n&= \\int_a^b \\Big|g'(\\varphi(t))\\Big|\\varphi'(t)\\ dt \\quad \\textrm{since} \\ \\varphi \\ \\textrm{must} \\ \\textrm{be} \\ \\textrm{non-decreasing} \\\\\n&= \\int_c^d \\Big|g'(u)\\Big|\\ du \\quad \\textrm{using} \\ \\textrm{integration} \\ \\textrm{by} \\ \\textrm{substitution}\\\\\n&= L(g).\n\\end{align}</math>\n\n==Finding arc lengths by integrating==\n{{See also|Differential geometry of curves#Length and natural parametrization|l1=Differential geometry of curves}}\n\n[[File:Quarter circle.png|thumb|400x400px|Quarter circle]]\nIf a [[plane curve|planar curve]] in <math>\\mathbb{R}^2</math> is defined by the equation <math> y=f(x), </math> where <math>f</math> is continuously differentiable, then it is simply a special case of a parametric equation where <math>x = t </math> and <math> y = f(t).</math>  The arc length is then given by:\n\n:<math>s=\\int_a^b \\sqrt{1+\\left(\\frac{dy}{dx}\\right)^2}dx.</math>\n\nCurves with [[Solution in closed form|closed-form solutions]] for arc length include the [[catenary]], [[circle]], [[cycloid]], [[logarithmic spiral]], [[parabola]], [[semicubical parabola]] and [[line (mathematics)|straight line]]. The lack of a closed form solution for the arc length of an [[Ellipse#Circumference|elliptic]] and [[Hyperbola|hyperbolic]] arc led to the development of the [[elliptic integral]]s.\n\n===Numerical integration===\nIn most cases, including even simple curves, there are no closed-form solutions for arc length and [[numerical integration]] is necessary.  Numerical integration of the arc length integral is usually very efficient.  For example, consider the problem of finding the length of a quarter of the unit circle by numerically integrating the arc length integral.  The upper half of the unit circle can be parameterized as <math>y=\\sqrt{1-x^2}.</math>  The interval <math>x\\in [-\\sqrt{2}/2, \\sqrt{2}/2] </math> corresponds to a quarter of the circle.  Since <math>dy/dx=-x/\\sqrt{1-x^2}</math> and <math>1+(dy/dx)^2 = 1/(1-x^2),</math> the length of a quarter of the unit circle is\n\n:<math>\\int_{-\\sqrt{2}/2}^{\\sqrt{2}/2}\\frac{1}{\\sqrt{1-x^2}} \\, dx.</math>\n\nThe 15-point [[Gauss–Kronrod quadrature formula|Gauss–Kronrod]] rule estimate for this integral of {{val|1.570796326808177}} differs from the true length of\n\n:<math>\\Big[\\arcsin x\\Big]^{\\sqrt{2}/2}_{-\\sqrt{2}/2}=\\frac \\pi 2</math>\n\nby {{val|1.3e-11}} and the 16-point [[Gaussian quadrature]] rule estimate of {{val|1.570796326794727}} differs from the true length by only {{val|1.7e-13}}.  This means it is possible to evaluate this integral to almost [[Machine epsilon|machine precision]] with only 16 integrand evaluations.\n\n=== Curve on a surface ===\nLet <math>\\mathbf{x}(u,v)</math> be a surface mapping and let <math>\\mathbf{C}(t) = (u(t), v(t))</math> be a curve on this surface.  The integrand of the arc length integral is <math>|(\\mathbf{x}\\circ\\mathbf{C})'(t)|.</math>  Evaluating the derivative requires the [[chain rule]] for vector fields:\n\n: <math>D(\\mathbf{x} \\circ \\mathbf{C}) = (\\mathbf{x}_u \\ \\mathbf{x}_v)\\binom{u'}{v'} = \\mathbf{x}_u u' + \\mathbf{x}_v v'.</math>\n\nThe squared norm of this vector is <math>(\\mathbf{x}_u u' + \\mathbf{x}_v v') \\cdot (\\mathbf{x}_u u' + \\mathbf{x}_v v') = g_{11}(u')^2 + 2g_{12}u'v' + g_{22}(v')^2 </math> (where <math>g_{ij} </math> is the [[first fundamental form]] coefficient), so the integrand of the arc length integral can be written as <math>\\sqrt{g_{ab} (u^a)' (u^b)'} </math> (where <math>u^1 = u </math> and <math>u^2 = v </math>).\n\n=== Other coordinate systems ===\nLet <math>\\mathbf{C}(t) = (r(t), \\theta(t))</math> be a curve expressed in polar coordinates.  The mapping that transforms from polar coordinates to rectangular coordinates is\n\n:<math>\\mathbf{x}(r,\\theta) = (r\\cos\\theta, r\\sin\\theta ).</math>\n\nThe integrand of the arc length integral is  <math>|(\\mathbf{x}\\circ\\mathbf{C})'(t)|.</math>  The chain rule for vector fields shows that <math>D(\\mathbf{x}\\circ \\mathbf{C}) = \\mathbf{x}_r r' + \\mathbf{x}_{\\theta} \\theta'.</math>  So the squared integrand of the arc length integral is\n\n:<math>(\\mathbf{x_r}\\cdot\\mathbf{x}_r)(r')^2 + 2(\\mathbf{x}_r\\cdot\\mathbf{x}_{\\theta})r'\\theta' + (\\mathbf{x}_{\\theta}\\cdot\\mathbf{x}_{\\theta})(\\theta')^2 = (r')^2 + r^2(\\theta')^2.</math>\n\nSo for a curve expressed in polar coordinates, the arc length is\n\n:<math>\\int_{t_1}^{t_2} \\sqrt{\\left(\\frac{dr}{dt}\\right)^2 + r^2\\left(\\frac{d\\theta}{dt}\\right)^2 }dt = \\int_{\\theta(t_1)}^{\\theta(t_2)} \\sqrt{\\left(\\frac{dr}{d\\theta}\\right)^2 + r^2}d\\theta.</math>\n\nNow let <math>\\mathbf{C}(t) = (r(t), \\theta(t), \\phi(t))</math> be a curve expressed in spherical coordinates where <math>\\theta</math> is the polar angle measured from the positive <math>z</math>-axis and <math>\\phi</math> is the azimuthal angle.  The mapping that transforms from spherical coordinates to rectangular coordinates is\n\n:<math>\\mathbf{x}(r,\\theta,\\phi) = (r\\sin\\theta\\cos\\phi, r\\sin\\theta\\sin\\phi, r\\cos\\theta).</math>\n\nUsing the chain rule again shows that <math>D(\\mathbf{x}\\circ\\mathbf{C}) = \\mathbf{x}_r r' + \\mathbf{x}_{\\theta}\\theta' + \\mathbf{x}_{\\phi}\\phi'.</math>  All dot products <math>\\mathbf{x}_i \\cdot \\mathbf{x}_j</math> where <math>i</math> and <math>j</math> differ are zero, so the squared norm of this vector is\n\n:<math>(\\mathbf{x}_r\\cdot \\mathbf{x}_r )(r'^2) + (\\mathbf{x}_{\\theta} \\cdot \\mathbf{x}_{\\theta})(\\theta')^2 + (\\mathbf{x}_{\\phi}\\cdot \\mathbf{x}_{\\phi})(\\phi')^2 = (r')^2 + r^2(\\theta')^2 + r^2 \\sin^2\\theta (\\phi')^2.</math>\n\nSo for a curve expressed in spherical coordinates, the arc length is\n\n:<math>\\int_{t_1}^{t_2} \\sqrt{\\left(\\frac{dr}{dt}\\right)^2 + r^2\\left(\\frac{d\\theta}{dt}\\right)^2 + r^2\\sin^2\\theta \\left(\\frac{d\\phi}{dt}\\right)^2}dt. </math>\n\nA very similar calculation shows that the arc length of a curve expressed in cylindrical coordinates is\n\n:<math>\\int_{t_1}^{t_2} \\sqrt{\\left(\\frac{dr}{dt}\\right)^2 + r^2\\left(\\frac{d\\theta}{dt}\\right)^2 + \\left(\\frac{dz}{dt}\\right)^2}dt. </math>\n\n== Simple cases ==\n\n=== Arcs of circles ===\n<!--\nArc length is not the same as [[arc measure]]. [What is that?] -->\n\nArc lengths are denoted by ''s'', since the Latin word for length (or size) is ''spatium''.\n\nIn the following lines, <math>r</math> represents the [[radius]] of a [[circle]], <math>d</math> is its [[diameter]], <math>C</math> is its [[circumference]], <math>s</math> is the length of an arc of the circle, and <math>\\theta</math> is the angle which the arc subtends at the [[Centre (geometry)|centre]] of the circle. The distances <math>r, d, C,</math> and <math>s</math> are expressed in the same units.\n\n* <math>C=2\\pi r,</math> which is the same as <math>C=\\pi d.</math>  This equation is a definition of [[Pi|<math>\\pi.</math>]]\n* If the arc is a [[semicircle]], then <math>s=\\pi r.</math>\n* {{anchor|Circular}}For an arbitrary circular arc:\n** If <math>\\theta</math> is in [[radian]]s then <math>s =r\\theta.</math> This is a definition of the radian.\n** If <math>\\theta</math> is in [[Degree (angle)|degrees]], then <math>s=\\frac{\\pi r \\theta}{180\\ \\text{deg}},</math> which is the same as <math>s=\\frac{C \\theta}{360\\ \\text{deg}}.</math>\n** If <math>\\theta</math> is in [[Grad (angle)|grads]] (100 grads, or grades, or gradians are one [[right-angle]]), then <math>s=\\frac{\\pi r \\theta}{200\\ \\text{grad}},</math> which is the same as <math>s=\\frac{C \\theta}{400\\ \\text{grad}}.</math>\n** If <math>\\theta</math> is in [[Turn (geometry)|turns]] (one turn is a complete rotation, or 360°, or 400 grads, or <math>2\\pi</math> radians), then <math>s=C \\theta/\\text{turn}</math>.\n\n====Arcs of great circles on the Earth====\n{{main|Great-circle distance}}\n{{further|Geodesics on an ellipsoid}}\n\nTwo units of length, the [[nautical mile]] and the [[metre]] (or kilometre), were originally defined so the lengths of arcs of [[great circle]]s on the Earth's surface would be simply numerically related to the angles they subtend at its centre. The simple equation <math>s=\\theta</math> applies in the following circumstances:\n\n:* if <math>s</math> is in nautical miles, and <math>\\theta</math> is in [[arcminute]]s ({{frac|60}} degree), or\n:* if <math>s</math> is in kilometres, and <math>\\theta</math> is in centigrades ({{frac|100}}[[Gradian|grad]]).\n\nThe lengths of the distance units were chosen to make the circumference of the Earth equal {{val|40000}} kilometres, or {{val|21600}} nautical miles. Those are the numbers of the corresponding angle units in one complete turn.\n\nThose definitions of the metre and the nautical mile have been superseded by more precise ones, but the original definitions are still accurate enough for conceptual purposes and some calculations. For example, they imply that one kilometre is exactly 0.54 nautical miles. Using official modern definitions, one nautical mile is exactly 1.852 kilometres,<ref>{{cite web |url=http://www.physics.nist.gov/Pubs/SP811/appenB8.html |title=Special Publication 811 |first=Curt |last=Suplee |date=2 July 2009 |website=nist.gov}}</ref> which implies that 1 kilometre is about {{val|0.53995680}} nautical miles.<ref>[[CRC Handbook of Chemistry and Physics]], p. F-254</ref> This modern ratio differs from the one calculated from the original definitions by less than one part in 10,000.\n\n=== Length of an arc of a parabola ===\n{{for|a calculation of the length of a parabolic arc|Parabola#Arc length}}\n\n==Historical methods==\n\n===Antiquity===\nFor much of the [[history of mathematics]], even the greatest thinkers considered it impossible to compute the length of an irregular arc. Although [[Archimedes]] had pioneered a way of finding the area beneath a curve with his \"[[method of exhaustion]]\", few believed it was even possible for curves to have definite lengths, as do straight lines. The first ground was broken in this field, as it often has been in [[calculus]], by [[approximation]]. People began to inscribe [[polygon]]s within the curves and compute the length of the sides for a somewhat accurate measurement of the length. By using more segments, and by decreasing the length of each segment, they were able to obtain a more and more accurate approximation. In particular, by inscribing a polygon of many sides in a circle, they were able to find approximate values of [[pi (mathematical constant)|π]].<ref>{{Cite journal|last=Richeson|first=David|date=May 2015|title=Circular Reasoning: Who First Proved That C Divided by d Is a Constant?|journal=The College Mathematics Journal|volume=46|issue=3|pages=162–171|doi=10.4169/college.math.j.46.3.162|issn=0746-8342}}</ref><ref>{{Cite journal|last=Coolidge|first=J. L.|authorlink=Julian Coolidge|date=February 1953|title=The Lengths of Curves|journal=The American Mathematical Monthly|volume=60|issue=2|pages=89–93|doi=10.2307/2308256|jstor=2308256}}</ref>\n\n===17th century===\nIn the 17th century, the method of exhaustion led to the rectification by geometrical methods of several [[transcendental curve]]s: the [[logarithmic spiral]] by [[Evangelista Torricelli]] in 1645 (some sources say [[John Wallis]] in the 1650s), the [[cycloid]] by [[Christopher Wren]] in 1658, and the [[catenary]] by [[Gottfried Leibniz]] in 1691.\n\nIn 1659, Wallis credited [[William Neile]]'s discovery of the first rectification of a nontrivial [[algebraic curve]], the [[semicubical parabola]].<ref>{{cite book|first=John|last=Wallis|title=Tractatus Duo. Prior, De Cycloide et de Corporibus inde Genitis…|location=Oxford|publisher=University Press|date=1659|url=http://gallica.bnf.fr/ark:/12148/bpt6k5759200j/f110.image|pages=91–96}}</ref> The accompanying figures appear on page 145. On page 91, William Neile is mentioned as ''Gulielmus Nelius''.\n\n===Integral form===\nBefore the full formal development of calculus, the basis for the modern integral form for arc length was independently discovered by [[Hendrik van Heuraet]] and [[Pierre de Fermat]].\n\nIn 1659 van Heuraet published a construction showing that the problem of determining arc length could be transformed into the problem of determining the area under a curve (i.e., an integral).  As an example of his method, he determined the arc length of a semicubical parabola, which required finding the area under a [[parabola]].<ref>{{cite book|first=Hendrik|last=van Heuraet|url=https://books.google.com/books/ucm?id=lGFxGEEK52oC&pg=PA517#v=onepage&q&f=false|contribution=Epistola de transmutatione curvarum linearum in rectas [Letter on the transformation of curved lines into right ones]|title=Renati Des-Cartes Geometria|edition=2nd|location=Amsterdam|publisher=Louis & Daniel Elzevir|date=1659|pages=517–520}}</ref>  In 1660, Fermat published a more general theory containing the same result in his ''De linearum curvarum cum lineis rectis comparatione dissertatio geometrica'' (Geometric dissertation on curved lines in comparison with straight lines).<ref>{{cite book|author=M.P.E.A.S. (pseudonym of Fermat)|url=https://books.google.com/books?id=BBqoHZej2ZsC&pg=PA1#v=onepage&q&f=false|title=De Linearum Curvarum cum Lineis Rectis Comparatione Dissertatio Geometrica|location=Toulouse|publisher=Arnaud Colomer|date=1660}}</ref>\n\n[[File:Arc length, Fermat.svg|thumb|300px|Fermat's method of determining arc length]]\n\nBuilding on his previous work with tangents, Fermat used the curve\n\n:<math> y = x^{3/2} \\,</math>\n\nwhose [[tangent]] at ''x'' = ''a'' had a [[slope]] of\n\n:<math> \\textstyle {3 \\over 2} a^{1/2} </math>\n\nso the tangent line would have the equation\n\n:<math> y = \\textstyle {3 \\over 2} {a^{1/2}}(x - a) + f(a). </math>\n\nNext, he increased ''a'' by a small amount to ''a'' + ''ε'', making segment ''AC'' a relatively good approximation for the length of the curve from ''A'' to ''D''. To find the length of the segment ''AC'', he used the [[Pythagorean theorem]]:\n\n: <math>\\begin{align}\nAC^2 &{}= AB^2 + BC^2 \\\\\n&{} = \\textstyle \\varepsilon^2 + {9 \\over 4} a \\varepsilon^2 \\\\\n&{}= \\textstyle \\varepsilon^2 \\left (1 + {9 \\over 4} a \\right )\n\\end{align}</math>\n\nwhich, when solved, yields\n\n:<math>AC = \\textstyle \\varepsilon \\sqrt { 1 + {9 \\over 4} a\\ }.</math>\n\nIn order to approximate the length, Fermat would sum up a sequence of short segments.\n\n==Curves with infinite length{{anchor|Infinite}}==\n{{see also|Coastline paradox}}\n[[File:Koch curve.svg|thumb|The Koch curve.]] [[File:xsinoneoverx.svg|thumb|The graph of ''x''sin(1/''x'').]]\nAs mentioned above, some curves are non-rectifiable. That is, there is no upper bound on the lengths of polygonal approximations; the length can be made [[Mathematical jargon#arbitrarily large|arbitrarily large]]. Informally, such curves are said to have infinite length. There are continuous curves on which every arc (other than a single-point arc) has infinite length. An example of such a curve is the [[Koch snowflake|Koch curve]]. Another example of a curve with infinite length is the graph of the function defined by ''f''(''x'') =&nbsp;''x''&nbsp;sin(1/''x'') for any open set with 0 as one of its delimiters and ''f''(0) = 0. Sometimes the [[Hausdorff dimension]] and [[Hausdorff measure]] are used to quantify the size of such curves.\n\n==Generalization to (pseudo-)Riemannian manifolds==\nLet <math>M</math> be a [[pseudo-Riemannian manifold|(pseudo-)Riemannian manifold]], <math>\\gamma:[0,1]\\rightarrow M</math> a curve in <math>M</math> and <math>g</math> the (pseudo-) [[metric tensor]].\n\nThe length of <math>\\gamma</math> is defined to be\n\n:<math>\\ell(\\gamma)=\\int_{0}^{1} \\sqrt{ \\pm g(\\gamma'(t),\\gamma '(t)) } \\, dt, </math>\n\nwhere <math>\\gamma'(t)\\in T_{\\gamma(t)}M</math> is the tangent vector of <math>\\gamma</math> at <math>t.</math>  The sign in the square root is chosen once for a given curve, to ensure that the square root is a real number. The positive sign is chosen for spacelike curves; in a pseudo-Riemannian manifold, the negative sign may be chosen for timelike curves. Thus the length of a curve is a non-negative real number. Usually no curves are considered which are partly spacelike and partly timelike.\n\nIn [[theory of relativity]], arc length of timelike curves ([[world line]]s) is the [[proper time]] elapsed along the world line, and arc length of a spacelike curve the [[proper distance]] along the curve.\n\n==See also==\n* [[Arc (geometry)]]\n* [[Circumference]]\n* [[Crofton formula]]\n* [[Elliptic integral]]\n* [[Geodesic]]s\n* [[Intrinsic equation]]\n* [[Numerical integration|Integral approximations]]\n* [[Line integral]]\n* [[Meridian arc]]\n* [[Multivariable calculus]]\n* [[Sinuosity]]\n\n==References==\n{{reflist}}\n\n==Sources==\n* {{cite book|last=Farouki|first=Rida T.|date=1999|contribution=Curves from motion, motion from curves|editor1-first=P.-J.|editor1-last=Laurent|editor2-first=P.|editor2-last=Sablonniere|editor3-first=L. L.|editor3-last=Schumaker|title=Curve and Surface Design: Saint-Malo 1999|pages=63–90|publisher=Vanderbilt Univ. Press|isbn=978-0-8265-1356-4}}\n\n==External links==\n* {{springer|title=Rectifiable curve|id=p/r080130}}\n<!-- * [http://math.kennesaw.edu/~jdoto/13.pdf Math Before Calculus] not working -->\n* [https://web.archive.org/web/20071106083431/http://www3.villanova.edu/maple/misc/history_of_curvature/k.htm The History of Curvature]\n* {{MathWorld|title=Arc Length|urlname=ArcLength}}\n* [http://demonstrations.wolfram.com/ArcLength/ Arc Length] by [[Ed Pegg, Jr.]], [[The Wolfram Demonstrations Project]], 2007.\n* [http://www.pinkmonkey.com/studyguides/subjects/calc/chap8/c0808501.asp{{dead link|date=July 2017 |bot=InternetArchiveBot |fix-attempted=yes }} Calculus Study Guide – Arc Length (Rectification)]\n* [http://www-groups.dcs.st-and.ac.uk/~history/Curves/Curves.html Famous Curves Index] ''The MacTutor History of Mathematics archive''\n* [http://demonstrations.wolfram.com/ArcLengthApproximation/ Arc Length Approximation] by Chad Pierson, Josh Fritz, and Angela Sharp, [[The Wolfram Demonstrations Project]].\n* [https://web.archive.org/web/20110720095511/http://numericalmethods.eng.usf.edu/experiments/Length_of_curve_experiment.pdf Length of a Curve Experiment] Illustrates numerical solution of finding length of a curve.\n\n{{DEFAULTSORT:Arc Length}}\n[[Category:Integral calculus]]\n[[Category:Curves]]\n[[Category:Length]]"
    },
    {
      "title": "Berezin integral",
      "url": "https://en.wikipedia.org/wiki/Berezin_integral",
      "text": "{{mergefrom|Grassmann integral|discuss=Talk:Grassmann integral#Merger proposal|date=May 2019}}\n\nIn mathematical physics, a '''Berezin integral''', named after [[Felix Berezin]], (or '''Grassmann integral''', after [[Hermann Grassmann]]) is a way to define [[integral|integration]] of elements of the [[exterior algebra]] (Hermann Grassmann 1844). It is called integral because it is used in physics as a sum over histories for [[fermion]]s, an extension of the [[Functional integration|path integral]].\n\n==Integration on an exterior algebra==\n\nLet <math>\\Lambda^n</math> be the exterior algebra of polynomials in anticommuting elements <math>\\theta_{1},\\dots,\\theta_{n}</math> over the field of complex numbers. (The ordering of the generators <math>\\theta_1,\\dots,\\theta_n</math> is fixed and defines the orientation of the exterior algebra.) The '''Berezin integral''' on <math>\\Lambda^{n}</math> is the linear functional <math>\\int_{\\Lambda^{n} }\\cdot\\textrm{d}\\theta</math> with the following properties:\n\n:<math>\\int_{\\Lambda^n}\\theta_{n}\\cdots\\theta_{1}\\,\\mathrm{d}\\theta=1,</math>\n:<math>\\int_{\\Lambda^n}\\frac{\\partial f}{\\partial\\theta_{i}}\\,\\mathrm{d}\\theta=0,\\ i=1,\\dots,n</math>\n\nfor any <math>f\\in\\Lambda^n,</math> where <math>\\partial/\\partial\\theta_{i}</math> means the left or the right partial derivative. These properties define the integral uniquely. The formula\n\n:<math>\\int_{\\Lambda^n}f\\left( \\theta\\right) \\, \\mathrm{d}\\theta=\\int_{\\Lambda^1}\\left(  \\cdots \\int_{\\Lambda^1}\\left(  \\int_{\\Lambda^1}f\\left(\\theta\\right) \\, \\mathrm{d}\\theta_{1}\\right) \\, \\mathrm{d}\\theta_2 \\cdots \\right)\\mathrm{d}\\theta_n</math>\n\nexpresses the Fubini law. On the right-hand side, the interior integral of a monomial <math>f=g\\left(  \\theta^{\\prime}\\right)  \\theta_{1}</math> is set to be <math>g\\left(  \\theta^{\\prime}\\right)  ,\\ </math> where <math>\\theta^{\\prime}=\\left(\\theta_{2},...,\\theta_{n}\\right)</math>; the integral of <math>f=g\\left( \\theta^{\\prime}\\right)  </math> vanishes. The integral with respect to <math>\\theta_{2}</math> is calculated in the similar way and so on.\n\n==Change of Grassmann variables==\n\nLet <math>\\theta_{i}=\\theta_{i}\\left(  \\xi_{1},...,\\xi_{n}\\right)  ,\\ i=1,...,n,</math> be odd polynomials in some antisymmetric variables <math>\\xi_{1},...,\\xi_{n}</math>. The Jacobian is the matrix\n\n:<math>D=\\left\\{  \\frac{\\partial\\theta_{i}}{\\partial\\xi_{j}},\\ i,j=1,...,n\\right\\},</math>\n\nwhere the left and the right derivatives coincide and are even polynomials. The formula for the coordinate change reads\n\n:<math>\\int f\\left(  \\theta\\right)  \\mathrm{d}\\theta=\\int f\\left(  \\theta\\left( \\xi\\right)  \\right)  \\left(  \\det D\\right)  ^{-1}\\mathrm{d}\\xi.</math>\n\n==Berezin integral==\n\nConsider now the algebra <math>\\Lambda^{m\\mid n}</math> of functions of real commuting variables <math>x=x_{1},...,x_{m}</math> and of anticommuting variables <math>\\theta_{1},...,\\theta_{n}</math> (which is called the free superalgebra of dimension <math>\\left(  m\\mid n\\right)  </math>). This means that an element <math>f=f\\left(x,\\theta\\right)  \\in\\Lambda^{m\\mid n}</math> is a function of the argument <math>x</math> that varies in an open set <math>X\\subset\\mathbb{R}^{m}</math> with values in the algebra <math>\\Lambda^{n}.</math> Suppose that this function is continuous<math>\\ </math>and vanishes in the complement of a compact set <math>K\\subset\\mathbb{R}^{m}.</math> The Berezin integral is the number\n\n:<math>\\int_{\\Lambda^{m\\mid n}}f\\left(  x,\\theta\\right)  \\mathrm{d}\\theta \\mathrm{d}x=\\int_{\\mathbb{R}^{m}}\\mathrm{d}x\\int_{\\Lambda^{n}}f\\left( x,\\theta\\right)  \\mathrm{d}\\theta.</math>\n\n==Change of even and odd variables==\n\nLet a coordinate transformation be given by <math>x_{i}=x_{i}\\left(  y,\\xi\\right) ,\\ i=1,...,m;\\ \\theta_{j}=\\theta_{j}\\left(  y,\\xi\\right)  ,j=1,...,n</math>, where <math>x_{i}</math> are even and <math>\\theta_{j}</math> are odd polynomials of <math>\\xi</math> depending on even variables <math>y.</math> The Jacobian matrix of this transformation has the block form:\n\n:<math>\\mathrm{J}=\\frac{\\partial\\left(  x,\\theta\\right)  }{\\partial\\left(y,\\xi\\right)  }=\\left(\\begin{array}[c]{cc}\nA & B\\\\ C & D\\end{array}\\right)  ,</math>\n\nwhere each even derivative <math>\\partial/\\partial y_{j}</math> commutes with all elements of the algebra <math>\\Lambda^{m\\mid n}</math>; the odd derivatives commute with even elements and anticommute with odd elements. The entries of the diagonal blocks <math>A=\\partial x/\\partial y</math> and <math>D=\\partial\\theta/\\partial\\xi</math> are even and the entries of the offdiagonal blocks <math>B=\\partial x/\\partial \\xi,\\ C=\\partial\\theta/\\partial y</math> are odd functions, where <math>\\partial /\\partial\\xi_{j}</math> mean right derivatives. The '''Berezinian''' (or the '''superdeterminant''') of the matrix <math>\\mathrm{J}</math> is the even function\n\n:<math>\\mathrm{Ber~J}=\\det\\left(  A-BD^{-1}C\\right)  \\det D^{-1}</math>\n\ndefined when the function <math>\\det D</math> is invertible in <math>\\Lambda^{m\\mid n}.</math> Suppose that the real functions <math>x_{i}=x_{i}\\left(  y,0\\right)  </math> define a smooth invertible map <math>F:Y\\rightarrow X</math> of open sets <math>X,\\ Y</math> in <math>\\mathbb{R}^{m}</math> and the linear part of the map <math>\\xi\\mapsto\\theta=\\theta\\left(y,\\xi\\right)  </math> is invertible for each <math>y\\in Y.</math> The general transformation law for the Berezin integral reads\n\n:<math>\\int_{\\Lambda^{m\\mid n}}f\\left(  x,\\theta\\right)  \\mathrm{d}\\theta\\mathrm{d}x=\\int_{\\Lambda^{m\\mid n}}f\\left(  x\\left(  y,\\xi\\right)\n,\\theta\\left(  y,\\xi\\right)  \\right)  \\varepsilon\\mathrm{Ber~J~d}\\xi\\mathrm{d}y</math>\n\n:<math>=\\int_{\\Lambda^{m\\mid n}}f\\left(  x\\left(  y,\\xi\\right)  ,\\theta\\left(y,\\xi\\right)  \\right)  \\varepsilon\\frac{\\det\\left(  A-BD^{-1}C\\right)  }{\\det D}\\mathrm{d}\\xi\\mathrm{d}y,</math>\n\nwhere <math>\\varepsilon=\\mathrm{sgn~\\det}\\partial x\\left(  y,0\\right)  /\\partial y</math> is the sign of the orientation of the map <math>F.</math> The superposition <math>f\\left( x\\left(  y,\\xi\\right)  ,\\theta\\left(  y,\\xi\\right)  \\right)  </math> is defined in the obvious way, if the functions <math>x_{i}\\left(  y,\\xi\\right)  </math> do not depend on <math>\\xi.</math> In the general case, we write <math>x_{i}\\left(  y,\\xi\\right) =x_{i}\\left(  y,0\\right)  +\\delta_{i},</math> where <math>\\delta_{i},\\ i=1,...,m</math> are even nilpotent elements of <math>\\Lambda^{m\\mid n}</math> and set\n\n:<math>f\\left(  x\\left(  y,\\xi\\right)  ,\\theta\\right)  =f\\left(  x\\left(y,0\\right)  ,\\theta\\right)  +\\sum_i\\frac{\\partial f}{\\partial x_{i}}\\left(x\\left(  y,0\\right)  ,\\theta\\right)  \\delta_{i}+\\frac{1}{2}\\sum_{i,j}\\frac{\\partial^{2}f}{\\partial x_{i}\\partial x_{j}}\\left(  x\\left(  y,0\\right),\\theta\\right)  \\delta_{i}\\delta_{j}+...,</math>\n\nwhere the Taylor series is finite.\n\n==History==\n\nThe mathematical theory of the integral with commuting and anticommuting variables was invented and developed by [[Felix Berezin]]. Some important earlier insights were made by [[David John Candlin]]. Other authors contributed to these developments, including the physicists Khalatnikov<ref>{{cite journal|first=I.M.|last=Khalatnikov|year=1955|title=Predstavlenie funkzij Grina v kvantovoj elektrodinamike v forme kontinualjnyh integralov|language=RU|journal=Journal of Experimental and Theoretical Physics|volume=28|issue=3|page=633|url=http://jetp.ac.ru/cgi-bin/dn/e_001_03_0568.pdf|trans-title=The Representation of Green's Function in Quantum Electrodynamics in the Form of Continual Integrals}}</ref> (although his paper contains mistakes), Matthews and Salam<ref>{{cite journal | last=Matthews | first=P. T. | last2=Salam | first2=A. | title=Propagators of quantized field | journal=Il Nuovo Cimento | publisher=Springer Science and Business Media LLC | volume=2 | issue=1 | year=1955 | issn=0029-6341 | doi=10.1007/bf02856011 | pages=120–134}}</ref>, and Martin.<ref>{{cite journal | title=The Feynman principle for a Fermi system | journal=Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences | publisher=The Royal Society | volume=251 | issue=1267 | date=23 June 1959 | issn=2053-9169 | doi=10.1098/rspa.1959.0127 | pages=543–549}}</ref>\n\n==See also==\n\n* [[Grassmann integral]]\n* [[Supermanifold]]\n* [[Berezinian]]\n\n==References==\n<references />\n\n[[Category:Multilinear algebra]]\n[[Category:Differential forms]]\n[[Category:Integral calculus]]\n[[Category:Mathematical physics]]\n[[Category:Quantum field theory]]\n[[Category:Quantum electrodynamics]]"
    },
    {
      "title": "Bioche's rules",
      "url": "https://en.wikipedia.org/wiki/Bioche%27s_rules",
      "text": "'''Bioche's rules''', formulated by the French mathematician [[Charles Bioche]] (1859–1949), are rules to aid in the computation of certain [[indefinite integral]]s in which the [[integrand]] contains [[sine]]s and [[cosine]]s.\n\nIn the following, <math>f(t)</math> is a [[Rational function|rational expression]] in <math>\\sin t</math> and <math>\\cos t</math>. In order to calculate <math>\\int f(t)\\,dt</math>, consider the integrand <math>\\omega(t)=f(t)\\,dt</math>. We consider the behavior of this entire integrand, including the <math display=\"inline> dt</math>, under translation and reflections of the ''t'' axis. The translations and reflections are ones that correspond to the symmetries and periodicities of the basic trigonometric functions.\n\nBioche's rules state that:\n\n# If <math>\\omega(-t)=\\omega(t)</math>, a good change of variables is <math>u=\\cos t</math>.\n# If <math>\\omega(\\pi-t)=\\omega(t)</math>, a good change of variables is <math>u=\\sin t</math>.\n# If <math>\\omega(\\pi+t)=\\omega(t)</math>, a good change of variables is <math>u=\\tan t</math>.\n# If two of the preceding relations both hold, a good change of variables is <math>u=\\cos 2t</math>.\n# In all other cases, use <math>u=\\tan(t/2)</math>.\n\nBecause rules 1 and 2 involve flipping the ''t'' axis, they flip the sign of ''dt'', and therefore the behavior of ''ω'' under these transformations differs from that of ''ƒ'' by a sign. Although the rules could be stated in terms of ''ƒ'', stating them in terms of ''ω'' has a mnemonic advantage, which is that we choose the change of variables ''u''(''t'') that has the same symmetry as&nbsp;''ω''.\n\n=Examples=\n==Example 1==\nAs a trivial example, consider\n::<math>\\int \\sin t \\,dt.</math>\nThen <math>f(t)=\\sin t</math> is an odd function, but under a reflection of the ''t'' axis about the origin, ω stays the same. That is, ω acts like an even function. This is the same as the symmetry of the cosine, which is an even function, so the mnemomic tells us to use the substitution <math>u=\\cos t</math> (rule 1). Under this substitution, the integral becomes <math>-\\int du</math>. The integrand involving transcendental functions has been reduced to one involving a rational function (a constant). The result is <math>-u+c=-\\cos t+c</math>, which is of course elementary and could have been done without Bioche's rules.\n\n==Example 2==\nThe integrand in\n::<math>\\int \\frac{dt}{\\sin t}</math>\nhas the same symmetries as the one in example 1, so we use the same substitution <math>u=\\cos t</math>. This transforms the integral into\n::<math>\\int - \\frac{du}{1 - u^2},</math>\nwhich can be integrated using partial fractions, since <math>1/(1-u^2)=(1/2)[1/(1+u)+1/(1-u)]</math>. The result is that\n::<math>\\int \\frac{dt}{\\sin t}=-\\frac{1}{2}\\ln\\frac{1+\\cos t}{1-\\cos t}+c.</math>\n\n==Example 3==\nConsider\n::<math>\\int \\frac{dt}{1+\\beta\\cos t},</math>\nwhere <math>\\beta^2<1</math>. Although the function ''f'' is even, the integrand as a whole ω is odd, so it does not fall under rule 1. It also lacks the symmetries described in rules 2 and 3, so we fall back to the last-resort substitution <math>u=\\tan(t/2)</math>. This leads to the result\n::<math>\\int \\frac{dt}{1+\\beta\\cos t} = t-2(1-\\beta^2)^{-1/2}\\arctan\\left[\\frac{(1-\\beta)\\sin t}{\\sqrt{1-\\beta^2}(1+\\cos t)}\\right] + c.</math>\n\n==References==\n* Zwillinger, ''Handbook of Integration'', p. 108\n* Stewart, ''How to Integrate It: A practical guide to finding elementary integrals'', pp. 190−197.\n\n{{math-stub}}\n\n[[Category:Integral calculus]]\n[[Category:Theorems in calculus]]"
    },
    {
      "title": "Cauchy formula for repeated integration",
      "url": "https://en.wikipedia.org/wiki/Cauchy_formula_for_repeated_integration",
      "text": "The '''Cauchy formula for repeated integration''', named after [[Augustin Louis Cauchy]], allows one to compress ''n'' [[antidifferentiation]]s of a function into a single integral (cf. [[Antiderivative#Techniques of integration|Cauchy's formula]]).\n\n==Scalar case==\nLet ''&fnof;'' be a continuous function on the real line.  Then the ''n''th [[repeated integral]] of ''&fnof;'' based at ''a'',\n\n:<math>f^{(-n)}(x) = \\int_a^x \\int_a^{\\sigma_1} \\cdots \\int_a^{\\sigma_{n-1}} f(\\sigma_{n}) \\, \\mathrm{d}\\sigma_{n} \\cdots \\, \\mathrm{d}\\sigma_2 \\, \\mathrm{d}\\sigma_1</math>,\n\nis given by single integration\n\n:<math>f^{(-n)}(x) = \\frac{1}{(n-1)!} \\int_a^x\\left(x-t\\right)^{n-1} f(t)\\,\\mathrm{d}t</math>.\n\nA proof is given by [[mathematical induction|induction]].  Since ''&fnof;'' is continuous, the base case follows from the [[Fundamental Theorem of Calculus|Fundamental theorem of calculus]]:\n\n:<math>\\frac{\\mathrm{d}}{\\mathrm{d}x} f^{(-1)}(x) = \\frac{\\mathrm{d}}{\\mathrm{d}x}\\int_a^x f(t)\\,\\mathrm{d}t = f(x)</math>;\n\nwhere\n\n:<math>f^{(-1)}(a) = \\int_a^a f(t)\\,\\mathrm{d}t = 0</math>.\n\nNow, suppose this is true for ''n'', and let us prove it for ''n+1''. Firstly, using the [[Leibniz Integral Rule|Leibniz integral rule]], note that\n:<math>\\frac{\\mathrm{d}}{\\mathrm{d} x} \\left[ \\frac{1}{n!} \\int_a^x \\left(x-t\\right)^n f(t)\\,\\mathrm{d}t \\right] = \\frac{1}{(n-1)!} \\int_a^x\\left(x-t\\right)^{n-1} f(t)\\,\\mathrm{d}t </math>.\nThen, applying the induction hypothesis,\n\n:<math>\n\\begin{align}\nf^{-(n+1)}(x) &= \\int_a^x \\int_a^{\\sigma_1} \\cdots \\int_a^{\\sigma_{n}} f(\\sigma_{n+1}) \\, \\mathrm{d}\\sigma_{n+1} \\cdots \\, \\mathrm{d}\\sigma_2 \\, \\mathrm{d}\\sigma_1 \\\\\n&= \\int_a^x \\frac{1}{(n-1)!} \\int_a^{\\sigma_1}\\left(\\sigma_1-t\\right)^{n-1} f(t)\\,\\mathrm{d}t\\,\\mathrm{d}\\sigma_1 \\\\\n&= \\int_a^x \\frac{\\mathrm{d}}{\\mathrm{d}\\sigma_1} \\left[\\frac{1}{n!} \\int_a^{\\sigma_1} \\left(\\sigma_1-t\\right)^n f(t)\\,\\mathrm{d}t\\right] \\,\\mathrm{d}\\sigma_1 \\\\\n&= \\frac{1}{n!}\\int_a^x \\frac{\\mathrm{d}}{\\mathrm{d}\\sigma_1} \\left[\\int_a^{\\sigma_1} \\left(\\sigma_1-t\\right)^n \\mathrm{d}\\sigma_1\\right] f(t)\\,\\mathrm{d}t \\\\ \n&= \\frac{1}{n!} \\int_a^x \\left(x-t\\right)^n f(t)\\,\\mathrm{d}t\n\\end{align}\n</math>\n\nThis completes the proof.\n\n==Applications==\n\nIn [[fractional calculus]], this formula can be used to construct a notion of [[differintegral]], allowing one to differentiate or integrate a fractional number of times.  Integrating a fractional number of times with this formula is straightforward; one can use fractional ''n'' by interpreting (''n''-1)! as Γ(''n'') (see [[Gamma function]]). Differentiating a fractional number of times can be accomplished by fractional integration, then differentiating the result.\n\n==References==\n\n*Gerald B. Folland, ''Advanced Calculus'', p.&nbsp;193, Prentice Hall (2002).  {{ISBN|0-13-065265-2}}\n\n==External links==\n*{{cite web|author=Alan Beardon|url=http://nrich.maths.org/public/viewer.php?obj_id=1369|title=Fractional calculus II|publisher=University of Cambridge|year=2000}}\n\n[[Category:Integral calculus]]\n[[Category:Theorems in analysis]]"
    },
    {
      "title": "Conditional convergence",
      "url": "https://en.wikipedia.org/wiki/Conditional_convergence",
      "text": "In [[mathematics]], a [[series (mathematics)|series]] or [[integral]] is said to be '''conditionally convergent''' if it converges, but it does not [[Absolute convergence|converge absolutely]].\n\n==Definition==\nMore precisely, a series <math display=\"inline\">\\sum_{n=0}^\\infty a_n</math> is said to '''converge conditionally''' if \n<math display=\"inline\">\\lim_{m\\rightarrow\\infty}\\,\\sum_{n=0}^m a_n</math> exists and is a finite number (not &infin; or &minus;&infin;), but <math display=\"inline\">\\sum_{n=0}^\\infty \\left|a_n\\right| = \\infty.</math>\n\nA classic example is the [[alternating series]] given by <math display=\"block\">1 - {1 \\over 2} + {1 \\over 3} - {1 \\over 4} + {1 \\over 5} - \\cdots =\\sum\\limits_{n=1}^\\infty {(-1)^{n+1}  \\over n},</math> which converges to  <math>\\ln (2)</math>, but is not absolutely convergent (see [[Harmonic series (mathematics)|Harmonic series]]).\n\n[[Bernhard Riemann]] proved that a conditionally convergent series may be rearranged to converge to any value at all, including &infin; or &minus;&infin;; see ''[[Riemann series theorem]]''. The [[Lévy–Steinitz theorem]] identifies the set of values to which a series of terms in '''R'''<sup>''n''</sup> can converge.\n\nA typical conditionally convergent integral is that on the non-negative real axis of <math display=\"inline\">\\sin (x^2)</math> (see [[Fresnel integral]]).\n\n==See also==\n*[[Absolute convergence]]\n*[[Unconditional convergence]]\n\n==References==\n* Walter Rudin, ''Principles of Mathematical Analysis'' (McGraw-Hill: New York, 1964).\n[[Category:Mathematical series]]\n[[Category:Integral calculus]]\n[[Category:Convergence (mathematics)]]\n[[Category:Summability theory]]"
    },
    {
      "title": "Constant factor rule in integration",
      "url": "https://en.wikipedia.org/wiki/Constant_factor_rule_in_integration",
      "text": "{{Unreferenced|date=August 2010}}\n\nThe '''constant factor rule in integration''' is a dual of the [[constant factor rule in differentiation]], and is a consequence of the [[linearity of integration]].  It states that a constant factor within an [[integrand]] can be separated from the integrand and instead multiplied by the integral.  For example, where k is a constant:\n\n<math>\\int k \\frac{dy}{dx} dx = k \\int \\frac{dy}{dx} dx. \\quad </math>\n\n== Proof ==\nStart by noticing that, from the definition of [[integral|integration]] as the [[Inverse function|inverse]] process of [[derivative|differentiation]]:\n\n:<math>y = \\int \\frac{dy}{dx} dx.</math>\n\nNow [[product (mathematics)|multiply]] both sides by a [[Coefficient|constant]] ''k''. Since ''k'' is a constant it is [[not dependent on]] ''x'':\n\n:<math>ky = k \\int \\frac{dy}{dx} dx. \\quad \\mbox{(1)}</math>\n\nTake the [[constant factor rule in differentiation]]:\n\n:<math>\\frac{d\\left(ky\\right)}{dx} = k \\frac{dy}{dx}.</math>\n\n[[integral|Integrate]] with respect to ''x'':\n\n:<math>ky = \\int k \\frac{dy}{dx} dx. \\quad \\mbox{(2)}</math>\n\nNow from (1) and (2) we have:\n\n:<math>ky = k \\int \\frac{dy}{dx} dx</math>\n:<math>ky = \\int k \\frac{dy}{dx} dx.</math>\n\nTherefore:\n\n:<math>\\int k \\frac{dy}{dx} dx = k \\int \\frac{dy}{dx} dx. \\quad \\mbox{(3)}</math>\n\nNow make a new differentiable [[function (mathematics)|function]]:\n\n:<math>u = \\frac{dy}{dx}.</math>\n\n[[Substitution property of equality|Substitute]] in (3):\n\n:<math>\\int ku dx = k \\int u dx.</math>\n\nNow we can re-substitute ''y'' for something different from what it was originally:\n\n:<math>y = u. \\,</math>\n\nSo:\n\n:<math>\\int ky dx = k \\int y dx.</math>\n\nThis is the constant factor rule in integration.\n\nA [[special case]] of this, with ''k''=-1, yields:\n\n:<math>\\int -y dx = -\\int y dx.</math>\n\n{{DEFAULTSORT:Constant Factor Rule In Integration}}\n[[Category:Integral calculus]]"
    },
    {
      "title": "Constant of integration",
      "url": "https://en.wikipedia.org/wiki/Constant_of_integration",
      "text": "{{More footnotes|date=February 2019}}\n\nIn [[calculus]], the [[indefinite integral]] of a given function (i.e., the [[Set (mathematics)|set]] of all [[antiderivative]]s of the function) on a [[connected set|connected domain]] is only defined [[up to]] an additive constant, the '''constant of integration'''.<ref>{{cite book | last=Stewart | first=James | authorlink=James Stewart (mathematician) | title=Calculus: Early Transcendentals |publisher=[[Brooks/Cole]] | edition=6th | year=2008 | isbn=0-495-01166-5}}</ref><ref>{{cite book | last1=Larson | first1=Ron | authorlink=Ron Larson (mathematician)| last2=Edwards | first2=Bruce H. | title=Calculus | publisher=[[Brooks/Cole]] | edition=9th | year=2009 | isbn=0-547-16702-4}}</ref> This constant expresses an ambiguity inherent in the construction of antiderivatives.  If a function <math>f(x)</math> is defined on an [[interval (mathematics)|interval]] and <math>F(x)</math> is an antiderivative of <math>f(x)</math>, then the set of ''all'' antiderivatives of <math>f(x)</math> is given by the functions <math>F(x) + C</math>, where ''C'' is an arbitrary constant (meaning that ''any'' value for ''C'' makes <math>F(x) + C</math> a valid antiderivative). The constant of integration is sometimes omitted in [[lists of integrals]] for simplicity.\n\n==Origin==\n\nThe [[derivative]] of any constant function is zero.  Once one has found one antiderivative <math>F(x)</math> for a function <math>f(x)</math>, adding or subtracting any constant ''C'' will give us another antiderivative, because <math>(F(x) + C)' = F\\,'(x) + C\\,' = F\\,'(x)</math>. The constant is a way of expressing that every function with at least one antiderivative has an infinite number of them.\n\nLet <math>F:\\mathbb{R}\\rightarrow\\mathbb{R}</math> and <math>G:\\mathbb{R}\\rightarrow\\mathbb{R}</math> be two everywhere differentiable functions.  Suppose that <math>F\\,'(x) = G\\,'(x)</math> for every real number ''x''.  Then there exists a real number ''C'' such that <math>F(x) - G(x) = C</math> for every real number ''x''.\n\nTo prove this, notice that <math>[F(x) - G(x)]' = 0</math>.  So ''F'' can be replaced by ''F''-''G'' and ''G'' by the constant function 0, making the goal to prove that an everywhere differentiable function whose derivative is always zero must be constant:\n\nChoose a real number ''a'', and let <math>C = F(a)</math>.  For any ''x'', the [[fundamental theorem of calculus]], together with the assumption that the derivative of ''F'' vanishes, implies that\n\n: <math>\\begin{align}\n& 0= \\int_a^x F'(t)\\ dt\\\\\n& 0= F(x)-F(a)         \\\\\n& 0= F(x)-C            \\\\  \n& F(x)=C               \\\\\n\\end{align}</math>\n\ntherefore ''F'' is a constant function.\n\nTwo facts are crucial in this proof.  First, the real line is [[Connected space|connected]].  If the real line were not connected, we would not always be able to integrate from our fixed ''a'' to any given ''x''.  For example, if we were to ask for functions defined on the union of intervals [0,1] and [2,3], and if ''a'' were 0, then it would not be possible to integrate from 0 to 3, because the function is not defined between 1 and 2.  Here there will be ''two'' constants, one for each [[Connected space|connected component]] of the [[Function domain|domain]].  In general, by replacing constants with [[Locally constant function|locally constant functions]], we can extend this theorem to disconnected domains. For example, there are two constants of integration for <math>\\textstyle\\int dx/x</math> and infinitely many for <math>\\textstyle\\int \\tan x\\,dx,</math> so for example the general form for the integral of 1/''x'' is:<ref>\"[http://golem.ph.utexas.edu/category/2012/03/reader_survey_logx_c.html Reader Survey: log|''x''| + ''C'']\", Tom Leinster, ''The ''n''-category Café'', March 19, 2012</ref><ref>{{cite book|title=The calculus lifesaver : all the tools you need to excel at calculus|last1=Banner|first1=Adrian|date=2007|publisher=Princeton University Press|isbn=978-0-691-13088-0|location=Princeton [u.a.]|page=380}}</ref>\n\n: <math>\\int {1 \\over x}\\,dx = \\begin{cases}\\ln \\left|x \\right| + C^- & x < 0\\\\\n\\ln \\left|x \\right| + C^+ & x > 0\n\\end{cases}</math>\n\nSecond, ''F'' and ''G'' were assumed to be everywhere differentiable.  If ''F'' and ''G'' are not differentiable at even one point, the theorem fails.  As an example, let <math>F(x)</math> be the [[Heaviside step function]], which is zero for negative values of ''x'' and one for non-negative values of ''x'', and let <math>G(x)=0</math>.  Then the derivative of ''F'' is zero where it is defined, and the derivative of ''G'' is always zero.  Yet it's clear that ''F'' and ''G'' do not differ by a constant. Even if it is assumed that ''F'' and ''G'' are everywhere continuous and [[almost everywhere]] differentiable the theorem still fails.  As an example, take ''F'' to be the [[Cantor function]] and again let ''G'' = 0.\n\nFor example, suppose one wants to find antiderivatives of <math>\\cos(x)</math>.  One such antiderivative is <math>\\sin(x)</math>.  Another one is <math>\\sin(x)+1</math>.  A third is <math>\\sin(x)-\\pi</math>.  Each of these has derivative <math>\\cos(x)</math>, so they are all antiderivatives of <math>\\cos(x)</math>.\n\nIt turns out that adding and subtracting constants is the only flexibility we have in finding different antiderivatives of the same function.  That is, all antiderivatives are the same up to a constant.  To express this fact for cos(''x''), we write:\n:<math>\\int \\cos(x)\\,dx = \\sin(x) + C.</math>\nReplacing ''C'' by a number will produce an antiderivative.  By writing ''C'' instead of a number, however, a compact description of all the possible antiderivatives of cos(''x'') is obtained.  ''C'' is called the '''constant of integration'''.  It is easily determined that all of these functions are indeed antiderivatives of <math>\\cos(x)</math>:\n:<math>\\begin{align}\n\\frac{d}{dx}[\\sin(x) + C] &= \\frac{d}{dx}[\\sin(x)] + \\frac{d}{dx}[C] \\\\\n                          &= \\cos(x) + 0 \\\\\n                          &= \\cos(x)\n\\end{align}</math>\n\n==Necessity==\n\nAt first glance it may seem that the constant is unnecessary, since it can be set to zero.  Furthermore, when evaluating [[definite integral]]s using the [[fundamental theorem of calculus]], the constant will always cancel with itself.\n\nHowever, trying to set the constant equal to zero does not always make sense.  For example, <math>2\\sin(x)\\cos(x)</math> can be integrated in at least three different ways:\n\n:<math>\\begin{align}\n\\int 2\\sin(x)\\cos(x)\\,dx &=&  \\sin^2(x) + C &=& -\\cos^2(x) + 1 + C &=& -\\frac12\\cos(2x) + C\\\\\n\\int 2\\sin(x)\\cos(x)\\,dx &=& -\\cos^2(x) + C &=&  \\sin^2(x) - 1 + C &=& -\\frac12\\cos(2x) + C\\\\\n\\int 2\\sin(x)\\cos(x)\\,dx &=& -\\frac12\\cos(2x) + C &=& \\sin^2(x) + C &=& -\\cos^2(x) + C\n\\end{align}</math>\n\nSo setting ''C'' to zero can still leave a constant.  This means that, for a given function, there is no \"simplest antiderivative\".\n\nAnother problem with setting ''C'' equal to zero is that sometimes we want to find an antiderivative that has a given value at a given point (as in an [[initial value problem]]).  For example, to obtain the antiderivative of <math>\\cos(x)</math> that has the value 100 at ''x'' = &pi;, then only one value of ''C'' will work (in this case ''C'' = 100).\n\nThis restriction can be rephrased in the language of [[differential equations]].  Finding an indefinite integral of a function <math>f(x)</math> is the same as solving the differential equation <math>\\frac{dy}{dx} = f(x)</math>.  Any differential equation will have many solutions, and each constant represents the unique solution of a well-posed [[initial value problem]].  Imposing the condition that our antiderivative takes the value 100 at ''x'' = &pi; is an initial condition.  Each initial condition corresponds to one and only one value of ''C'', so without ''C'' it would be impossible to solve the problem.\n\nThere is another justification, coming from [[abstract algebra]]. The space of all (suitable) real-valued functions on the [[real number]]s is a [[vector space]], and the [[differential operator]] <math>\\frac{d}{dx}</math> is a [[linear operator]].  The operator<math>\\frac{d}{dx}</math> maps a function to zero if and only if that function is constant.  Consequently, the [[kernel (algebra)|kernel]] of <math>\\frac{d}{dx}</math> is the space of all constant functions.  The process of indefinite integration amounts to finding a preimage of a given function.  There is no canonical preimage for a given function, but the set of all such preimages forms a [[coset]].  Choosing a constant is the same as choosing an element of the coset.  In this context, solving an [[initial value problem]] is interpreted as lying in the [[hyperplane]] given by the [[initial condition]]s.\n\n==References==\n{{reflist}}\n\n[[Category:Integral calculus]]"
    },
    {
      "title": "Disc integration",
      "url": "https://en.wikipedia.org/wiki/Disc_integration",
      "text": "[[File:Disc integration.svg|thumb|right|170px]]\n{{Calculus |Integral}}\n'''Disc integration''', also known in [[integral calculus]] as the '''disc method''', is a means of calculating the [[volume]] of a [[solid of revolution]] of a solid-state material when [[Integral|integrating]] along an axis \"parallel\" to the [[axis of revolution]]. This method models the resulting three-dimensional shape as a stack of an infinite number of discs of varying radius and infinitesimal thickness. It is also possible to use the same principles with rings instead of discs (the \"'''washer method'''\") to obtain hollow solids of revolutions. This is in contrast to [[shell integration]] which integrates along an axis ''perpendicular'' to the axis of revolution.\n\n==Definition==\n===Function of {{mvar|x}}===\nIf the function to be revolved is a function of {{mvar|x}}, the following integral represents the volume of the solid of revolution:\n\n:<math>\\pi\\int_a^b R(x)^2\\,dx</math>\n\nwhere {{math|''R''(''x'')}} is the distance between the function and the axis of rotation. This works only if the [[axis of rotation]] is horizontal (example: {{math|''y'' {{=}} 3}} or some other constant).\n\n===Function of {{mvar|y}}===\nIf the function to be revolved is a function of {{mvar|y}}, the following integral will obtain the volume of the solid of revolution:\n\n:<math>\\pi\\int_c^d R(y)^2\\,dy</math>\n\nwhere {{math|''R''(''y'')}} is the distance between the function and the axis of rotation. This works only if the [[axis of rotation]] is vertical (example: {{math|''x'' {{=}} 4}} or some other constant).\n\n===Washer method===\nTo obtain a hollow solid of revolution (the “washer method”), the procedure would be to take the volume of the inner solid of revolution and subtract it from the volume of the outer solid of revolution. This can be calculated in a single integral similar to the following: \n\n:<math>\\pi\\int_a^b\\left(R_\\mathrm{O}(x)^2 - R_\\mathrm{I}(x)^2\\right)\\,dx</math>\n\nwhere {{math|''R''<sub>O</sub>(''x'')}} is the function that is farthest from the axis of rotation and {{math|''R''<sub>I</sub>(''x'')}} is the function that is closest to the axis of rotation. For example, the next figure shows the rotation along the {{mvar|x}}-axis of the red \"leaf\" enclosed between the square-root and quadratic curves: \n[[File:Solid of revolution.gif|thumb|Rotation about x-axis]]\nThe volume of this solid is:\n:<math>\\pi\\int_0^1\\left(\\left(\\sqrt{x}\\right)^2 - \\left(x^2\\right)^2 \\right)\\,dx\\,.</math>\n\nOne should take caution not to evaluate the square of the difference of the two functions, but to evaluate the difference of the squares of the two functions.\n\n:<math>R_\\mathrm{O}(x)^2 - R_\\mathrm{I}(x)^2 \\not\\equiv \\left(R_\\mathrm{O}(x) - R_\\mathrm{I}(x)\\right)^2</math>\n\n(This formula only works for revolutions about the {{mvar|x}}-axis.)\n\nTo rotate about any horizontal axis, simply subtract from that axis each formula. If {{mvar|h}} is the value of a horizontal axis, then the volume equals\n\n:<math>\\pi\\int_a^b\\left(\\left(h-R_\\mathrm{O}(x)\\right)^2 - \\left(h-R_\\mathrm{I}(x)\\right)^2\\right)\\,dx\\,.</math>\n\nFor example, to rotate the region between {{math|''y'' {{=}} −2''x'' + ''x''<sup>2</sup>}} and {{math|''y'' {{=}} ''x''}} along the axis {{math|''y'' {{=}} 4}}, one would integrate as follows:\n\n:<math>\\pi\\int_0^3\\left(\\left(4-\\left(-2x+x^2\\right)\\right)^2 - (4-x)^2\\right)\\,dx\\,.</math>\n\nThe bounds of integration are the zeros of the first equation minus the second. Note that when integrating along an axis other than the {{mvar|x}}, the graph of the function that is farthest from the axis of rotation may not be that obvious. In the previous example, even though the graph of {{math|''y'' {{=}} ''x''}} is, with respect to the x-axis, further up than the graph of {{math|''y'' {{=}} −2''x'' + ''x''<sup>2</sup>}}, with respect to the axis of rotation the function {{math|''y'' {{=}} ''x''}} is the inner function: its graph is closer to {{math|''y'' {{=}} 4}} or the equation of the axis of rotation in the example.\n\nThe same idea can be applied to both the {{mvar|y}}-axis and any other vertical axis. One simply must solve each equation for {{mvar|x}} before one inserts them into the integration formula.\n\n==See also==\n*[[Solid of revolution]]\n*[[Shell integration]]\n\n==References==\n{{Refbegin|normalfont=yes}}\n*{{cite web|title=Volumes of Solids of Revolution|url=https://www.cliffsnotes.com/study-guides/calculus/calculus/applications-of-the-definite-integral/volumes-of-solids-of-revolution|work=CliffsNotes.com|accessdate=July 8, 2014}}\n*{{MathWorld|title=Method of Disks|urlname=MethodofDisks|accessdate=July 12, 2013}}\n*[[Frank J. Ayres|Frank Ayres]], [[Elliott Mendelson]]. ''[[Schaum's Outlines]]: Calculus''. McGraw-Hill Professional 2008, {{ISBN|978-0-07-150861-2}}. pp.&nbsp;244–248 ({{Google books|Ag26M8TII6oC|online copy|page=244}}. Retrieved July 12, 2013.)\n{{Refend}}\n\n[[Category:Integral calculus]]\n[[Category:Volume]]"
    },
    {
      "title": "Essential supremum and essential infimum",
      "url": "https://en.wikipedia.org/wiki/Essential_supremum_and_essential_infimum",
      "text": "{{bots|deny=ClueBot NG}}\nIn [[mathematics]], the concepts of '''essential supremum''' and '''essential infimum''' are related to the notions of [[supremum]] and [[infimum]], but  adapted to [[measure theory]] and [[functional analysis]], where one often deals with statements that are not valid for ''all'' elements in a [[Set (mathematics)|set]], but rather ''[[almost everywhere]]'', i.e., except on a [[null set|set of measure zero]].\n\n==Definition==\n\nLet ''f''&nbsp;:&nbsp;''X''&nbsp;&rarr;&nbsp;'''R''' be a [[real number|real]] valued  [[function (mathematics)|function]] defined on a set ''X''. A real number ''a'' is called an ''[[upper bound]]'' for ''f'' if ''f''(''x'')&nbsp;&le;&nbsp;''a'' for all ''x'' in ''X'', i.e., if the set\n\n:<math>f^{-1}(a, \\infty) = \\{x\\in X: f(x)>a\\} </math>\n\nis [[empty set|empty]]. Let\n\n:<math> U_f = \\{a \\in \\mathbb{R}: f^{-1}(a, \\infty)  = \\varnothing\\}  \\,</math>\n\nbe the set of upper bounds of ''f''. Then the supremum of ''f'' is defined by \n \n:<math>  \\sup f=\\inf {U_f} \\, </math>\n\nif the set of upper bounds <math> U_f</math> is nonempty, and <math>\\sup f = + \\infty</math> otherwise. \n\nAlternatively, if for some <math> a \\in \\mathbb{R}</math> we have <math> f(x) \\le a </math> for ''all'' <math>x \\in X</math> then <math>\\sup f \\le a</math>. \n\nNow assume in addition that <math>(X,\\Sigma;\\mu)</math> is a [[measure (mathematics)|measure]] space and, for simplicity, assume that the function <math>f</math> is measurable. A number <math>a</math> is called an ''essential upper bound'' of ''f'' if the measurable set <math>f^{-1}(a, \\infty)</math> is  a set of measure zero,{{efn|For non measurable functions the definition has to be modified  by assuming that <math>f^{-1}(a, \\infty)</math> is ''contained'' in a set of measure zero. Alternatively, one can assume that the measure is [[complete measure|complete]]}} i.e., if <math>f(x)\\le a</math> for ''almost all'' <math>x</math> in <math>X</math>.  Let\n\n:<math>U^{\\operatorname{ess}}_f = \\{a \\in \\mathbb{R}:  \\mu(f^{-1}(a, \\infty)) = 0\\}\\,</math>\n \nbe the set of essential upper bounds. Then the essential supremum is defined similarly as\n\n:<math> \\operatorname{ess} \\sup f=\\inf U^{\\mathrm{ess}}_f  \\, </math>\n\nif <math> U^{\\operatorname{ess}}_f \\ne \\varnothing</math>, and <math>\\operatorname{ess}\\sup f = + \\infty</math> otherwise.\n\nAlternatively, if for some <math> a \\in \\mathbb{R}</math> we have <math> f(x) \\le a </math> for ''allmost all'' <math>x \\in X</math> then <math>\\operatorname{ess} \\sup f \\le a</math>. \n\nExactly in the same way one defines the '''essential infimum''' as the supremum of the  ''essential lower bounds'', that is,\n\n:<math> \\operatorname{ess} \\inf f=\\sup \\{b \\in \\mathbb{R}: \\mu(\\{x: f(x) < b\\}) = 0\\}\\, </math>\n\nif the set of essential lower bounds is nonempty, and as <math>-\\infty</math> otherwise.\n\n==Examples==\n\nOn the real line consider the [[Lebesgue measure]] and its corresponding σ-algebra Σ. Define a function ''f'' by the formula\n\n:<math> f(x)= \\begin{cases} 5, & \\text{if }  x=1  \\\\ \n                            -4, & \\text{if }  x = -1 \\\\\n                            2, & \\text{otherwise. }\n \\end{cases} </math>\n\nThe supremum of this function (largest value) is 5, and the infimum (smallest value) is −4. However, the function takes these values only on the sets {1} and {−1} respectively, which are of measure zero. Everywhere else, the function takes the value 2. Thus, the essential supremum and the essential infimum of this function are both 2.\n\nAs another example, consider the function \n:<math> f(x)= \\begin{cases} x^3, & \\text{if }  x\\in \\mathbb Q  \\\\ \n                            \\arctan x, & \\text{if } x\\in \\mathbb R\\smallsetminus \\mathbb Q \\\\\n \\end{cases} </math>\nwhere '''Q''' denotes the [[rational number]]s. This function is unbounded both from above and from below, so its supremum and infimum are ∞ and −∞ respectively. However, from the point of view of the Lebesgue measure, the set of rational numbers is of measure zero; thus, what really matters is what happens in the complement of this set, where the function is given as&nbsp;arctan&nbsp;''x''. It follows that the essential supremum is {{pi}}/2 while the essential infimum is&nbsp;−{{pi}}/2.\n\nOn the other hand, consider the function ''f''(''x'')&nbsp;=&nbsp;''x''<sup>3</sup> defined for all real ''x''. Its essential supremum is <math>+\\infty</math>, and its essential infimum is <math>-\\infty</math>.\n\nLastly, consider the function \n:<math> f(x)= \\begin{cases} 1/x, & \\text{if }  x \\ne 0  \\\\ \n                            0, & \\text{if } x = 0. \\\\\n \\end{cases} </math>\nThen for any <math>\\textstyle a \\in \\mathbb R</math>, we have <math>\\textstyle \\mu(\\{x \\in \\mathbb R : 1/x > a\\}) \\geq \\tfrac{1}{|a|}</math> and so <math>\\textstyle U_f = \\varnothing</math> and <math>\\operatorname{ess} \\sup f = + \\infty</math>.\n\n==Properties==\n* If <math> \\mu(X)>0 </math> we have <math>\\inf f \\le \\operatorname{ess} \\inf f \\le \\operatorname{ess}\\sup f \\le \\sup f</math>. If <math> X </math> has measure zero <math>\\operatorname{ess}\\sup f=-\\infty</math> and <math>\\operatorname{ess}\\inf f = +\\infty</math>.<ref>Dieudonne J.: Treatise On Analysis, Vol. II. Associated Press, New York 1976. p 172f.</ref>\n* <math>\\operatorname{ess}\\sup (fg) \\le (\\operatorname{ess}\\sup f)(\\operatorname{ess}\\sup g)</math> whenever both terms on the right are nonnegative.\n\n==See also==\n\n* [[Lp spaces|''L''<sup>''p''</sup> spaces]]\n\n==Notes==\n{{notelist}}\n\n== References ==\n\n{{reflist}}\n\n{{PlanetMath attribution|urlname=EssentialSupremum|title=Essential supremum}}\n\n[[Category:Measure theory]]\n[[Category:Integral calculus]]"
    },
    {
      "title": "Euler substitution",
      "url": "https://en.wikipedia.org/wiki/Euler_substitution",
      "text": "'''Euler substitution''' is a method for evaluating integrals of the form\n\n: <math>\\int R(x, \\sqrt{ax^2 + bx + c}) \\,\\mathrm dx,</math>\n\nwhere <math>R</math> is a rational function of <math>x</math> and <math>\\sqrt{ax^2 + bx + c}</math>. In such cases, the integrand can be changed to a rational function by using the substitutions of Euler.<ref>N. Piskunov, ''Diferentsiaal- ja integraalarvutus körgematele tehnilistele öppeasutustele. Viies, taiendatud trukk. Kirjastus Valgus'', Tallinn (1965). Note: Euler substitutions can be found in most Russian calculus textbooks.</ref>\n\n==The first substitution of Euler==\nThe first substitution of Euler is used when <math>a > 0</math>. We substitute\n<math>\n \\sqrt{ax^2 + bx + c} = \\pm x\\sqrt{a} + t\n</math>\nand solve the resulting expression for <math>x</math>. We have that <math>x = \\frac{c - t^2}{\\pm 2t\\sqrt{a} - b}</math> and that the <math>\\mathrm dx</math> term is expressible rationally in <math>t</math>.\n\nIn this substitution, either the positive sign or the negative sign can be chosen.\n\n==The second substitution of Euler==\nIf <math>c > 0</math>, we take\n<math>\n \\sqrt{ax^2 + bx + c} = xt \\pm \\sqrt{c}.\n</math>\nWe solve for <math>x</math> similarly as above and find\n<math>x = \\frac{\\pm 2t\\sqrt{c} - b}{a - t^2}.</math>\n\nAgain, either the positive or the negative sign can be chosen.\n\n==The third substitution of Euler==\nIf the polynomial <math>ax^2 + bx + c</math> has real roots <math>\\alpha</math> and <math>\\beta</math>, we may choose\n<math>\\sqrt{ax^2 + bx + c} = \\sqrt{a(x - \\alpha)(x - \\beta)} = (x - \\alpha)t</math>. This yields \n<math>x = \\frac{a\\beta - \\alpha t^2}{a - t^2},</math>\nand as in the preceding cases, we can express the entire integrand rationally in <math>t</math>.\n\n==Examples==\n{{Expand section|date=April 2018}}\n=== Examples for the first substitution of Euler ===\n\n==== One ====\nIn the integral <math>\\int\\! \\frac{\\mathrm dx}{\\sqrt{x^2+c}}</math> we can use the first substitution and set <math>\\sqrt{x^2+c} = -x+t</math>, thus\n:<math>x = \\frac{t^2-c}{2t} \\quad\\quad \\mathrm dx = \\frac{t^2+c}{2t^2}\\,\\mathrm dt</math>\n:<math>\\sqrt{x^2+c} = -\\frac{t^2-c}{2t}+t = \\frac{t^2+c}{2t}</math>\nAccordingly, we obtain:\n:<math>\\int \\frac{\\mathrm dx}{\\sqrt{x^2+c}} = \\int \\frac{\\frac{t^2+c}{2t^2}}{\\frac{t^2+c}{2t}}\\, \\mathrm dt</math>\n:<math> = \\int\\!\\frac{\\mathrm dt}{t} = \\ln|t|+C = \\ln|x+\\sqrt{x^2+c}|+C</math>\nThe cases <math>c = \\pm 1</math>, give the formulas\n:<math>\\int \\frac{\\mathrm dx}{\\sqrt{x^2+1}} = \\mbox{arsinh}(x) + C </math>\n:<math>\\int \\frac{\\mathrm dx}{\\sqrt{x^2-1}} = \\mbox{arcosh}(x) + C \\quad (x > 1)</math>\n\n==== Two ====\nFor finding the value of <math>\\int\\frac{1}{x\\sqrt{x^{2}+4x-4}}dx</math>, we find <math>t</math> using the first substitution of Euler: <math>\\sqrt{x^{2}+4x-4} = \\sqrt{1}x+t = x+t</math>. Squaring both sides of the equation gives us <math>x^{2}+4x-4 = x^{2} + 2xt +t^{2}</math>, from which the <math>x^2</math> terms will cancel out; solving for <math>x</math> yields <math>x=\\frac{t^{2}+4}{4-2t}</math>. From there, we can take derivatives of both sides of the equation, find <math>dx</math>, and substitute <math>x</math> and <math>dx</math> in the integral to find the answer.\n\n<math>dx=\\frac{-2t^{2}+8t+8}{(4-2t)^{2}}dt</math>; <math>\\int \\frac{dx}{x\\sqrt{x^{2}+4x-4}} = \\int \\frac{\\frac{-2t^{2}+8t+8}{(4-2t)^{2}}dt}{(\\frac{t^{2}+4}{4-2t})(\\frac{-t^{2}+4t+4}{4-2t})} = 2\\int \\frac{dt}{t^{2}+4}= \\tan^{-1}\\left(\\frac t2\\right) +C</math> and since <math>t=\\sqrt{x^{2}+4x-4}-x</math>, the value of the integral is <math>\\int \\frac{1}{x\\sqrt{x^{2}+4x-4}}dx = \\tan^{-1}\\left(\\frac{\\sqrt{x^{2}+4x-4}-x}{2}\\right)+C</math>\n\n=== Examples for the second substitution of Euler ===\n\nIn the integral <math>\\int\\! \\frac{\\mathrm dx}{x\\sqrt{-x^2+x+2}}</math> we can use the second substitution and set <math>\\sqrt{-x^2+x+2} = xt + \\sqrt{2}</math>, thus\n:<math>x = \\frac{1-2\\sqrt{2}t}{t^2+1} \\quad\\quad \\mathrm dx = \\frac{2\\sqrt{2}t^2-2t-2\\sqrt{2}}{(t^2+1)^2}\\,\\mathrm dt</math>\n:<math>\\sqrt{-x^2+x+2} = \\frac{1-2\\sqrt{2t}}{t^2+1}t + \\sqrt{2} = \\frac{-\\sqrt{2}t^2+t+\\sqrt{2}}{t^2+1}</math>\nAccordingly, we obtain:\n:<math>\\int \\frac{\\mathrm dx}{x\\sqrt{-x^2+x+2}} = \\int \\frac{\\frac{2\\sqrt{2}t^2-2t-2\\sqrt{2}}{(t^2+1)^2}}{\\frac{1-2\\sqrt{2}t}{t^2+1}\\frac{-\\sqrt{2}t^2+t+\\sqrt{2}}{t^2+1}} \\mathrm dt =</math>\n:<math> = \\int\\!\\frac{-2}{-2\\sqrt{2}t+1}\\mathrm dt = \\frac{1}{\\sqrt{2}}\\int\\frac{-2\\sqrt{2}}{-2\\sqrt{2}t+1}\\mathrm dt =</math>\n:<math> = \\frac{1}{\\sqrt{2}}\\ln|2\\sqrt{2}t-1|+C = \\frac{\\sqrt{2}}{2}\\ln|2\\sqrt{2}\\frac{\\sqrt{-x^2+x+2}}{x}-1|+C</math>\n\n=== Examples for the third substitution of Euler ===\nIn the integral <math>\\int\\! \\frac{x^2}{\\sqrt{-x^2+3x-2}}\\mathrm dx</math> we can use the third substitution and set <math>\\sqrt{-(x-2)(x-1)} = (x-2)t</math>, thus\n:<math>x = \\frac{-2t^2-1}{-t^2-1} \\quad\\quad \\mathrm dx = \\frac{2t}{(-t^2-1)^2}\\,\\mathrm dt</math>\n:<math>\\sqrt{-x^2+3x-2} = (x-2)t = \\frac{t}{-t^2-1}</math>\nAccordingly, we obtain:\n:<math>\\int \\frac{x^2}{\\sqrt{-x^2+3x-2}}\\mathrm dx = \\int\\frac{(\\frac{-2t^2-1}{-t^2-1})^2\\frac{2t}{(-t^2-1)^2}}{\\frac{t}{-t^2-1}}\\mathrm dt = \\int\\frac{2(-2t^2-1)^2}{((-t^2-1)^2)^3}\\mathrm dt</math>\nAs we can see this is a rational function which can be solved using partial fractions.\n\n==Generalizations==\n\nThe substitutions of Euler can be generalized by allowing the use of imaginary numbers. For example, in the integral <math>\\textstyle \\int \\frac{\\mathrm dx}{\\sqrt{-x^2 + c}}</math>, the substitution <math>\\sqrt{x^2 + c} = \\pm ix + t</math> can be used. Extensions to the complex numbers allows us to use every type of Euler substitution regardless of the coefficients on the quadratic.\n\nThe substitutions of Euler can be generalized to a larger class of functions. Consider integrals of the form\n\n: <math>\\int R_1\\Big(x, \\sqrt{ax^2 + bx + c}\\Big) \\, \\log\\Big(R_2\\Big(x, \\sqrt{ax^2 + bx + c}\\Big)\\Big) \\,\\mathrm dx,</math>\n\nwhere <math>R_1</math> and <math>R_2</math> are rational functions of <math>x</math> and <math>\\sqrt{ax^2 + bx + c}</math>. This integral can be transformed by the substitution <math>\\sqrt{ax^2 + bx + c} = \\sqrt{a} + xt</math> into another integral\n\n: <math>\\int \\tilde R_1(t) \\log\\big(\\tilde R_2(t)\\big) \\,\\mathrm dt,</math>\n\nwhere <math>\\tilde R_1(t)</math> and <math>\\tilde R_2(t)</math> are now simply rational functions of <math>t</math>. In principle, [[factorization]] and [[partial fraction decomposition]] can be employed to break the integral down into simple terms, which can be integrated analytically through use of the [[dilogarithm]] function.<ref>{{cite book |last1=Zwillinger |first1=Daniel |title=The Handbook of Integration |publisher=Jones and Bartlett |location=1992 |isbn=978-0867202939 |pages=145–146}}</ref>\n\n==References==\n{{reflist}}\n{{PlanetMath attribution|id=9681|title=Eulers Substitutions For Integration}}\n\n[[Category:Integral calculus]]"
    },
    {
      "title": "Functional integration",
      "url": "https://en.wikipedia.org/wiki/Functional_integration",
      "text": "{{distinguish|functional integration (neurobiology)}}\n\n'''Functional integration''' is a collection of results in [[mathematics]] and [[physics]] where the [[domain (mathematics)|domain]] of an [[integral]] is no longer a [[manifold|region of space]], but a [[Function space|space of functions]]. Functional integrals arise in [[probability]], in the study of [[partial differential equations]], and in the [[path integral formulation|path integral approach]] to the [[quantum mechanics]] of particles and fields.\n\nIn an [[Lebesgue integration|ordinary integral]] there is a function to be integrated (the integrand) and a region of space over which to integrate the function (the domain of integration). The process of integration consists of adding up the values of the integrand for each point of the domain of integration.  Making this procedure rigorous requires a limiting procedure, where the domain of integration is divided into smaller and smaller regions.  For each small region, the value of the integrand cannot vary much, so it may be replaced by a single value. In a functional integral the domain of integration is a space of functions.  For each function, the integrand returns a value to add up.  Making this procedure rigorous poses challenges that continue to be topics of current research.\n\nFunctional integration was developed by [[Percy John Daniell]] in an article of 1919<ref>{{Cite journal\n| volume = 20\n| issue = 4\n| pages = 281–288\n| last = Daniell\n| first = P. J.\n| title = Integrals in An Infinite Number of Dimensions\n| journal = The Annals of Mathematics\n| series = Second Series| date = July 1919\n| jstor = 1967122\n| doi = 10.2307/1967122\n}}</ref> and [[Norbert Wiener]] in a series of studies culminating in his articles of 1921 on [[Brownian motion]].  They developed a rigorous method (now known as the [[Wiener measure]]) for assigning a probability to a particle's random path.  [[Richard Feynman]] developed another functional integral, the [[path integral formulation|path integral]], useful for computing the quantum properties of systems.  In Feynman's path integral, the classical notion of a unique trajectory for a particle is replaced by an infinite sum of classical paths, each weighted differently according to its classical properties.\n\nFunctional integration is central to quantization techniques in theoretical physics.  The algebraic properties of functional integrals are used to develop series used to calculate properties in [[quantum electrodynamics]] and the [[standard model]] of particle physics.\n\n==Functional Integration==\n{{Confusing|section|date=January 2014}}\n{{unreferenced section|date=March 2017}}\nWhereas standard Riemann integration sums a function ''f''(''x'') over a continuous range of values of ''x'', functional integration sums a [[functional (mathematics)|functional]] ''G''[''f''], which can be thought of as a \"function of a function\" over a continuous range (or space) of functions ''f''. Most functional integrals cannot be evaluated exactly but must be evaluated using [[perturbation methods]]. The formal definition of a functional integral is\n\n:<math>\n\\int G[f] [Df] \\equiv \\int\\limits_{-\\infty}^\\infty \\cdots \\int\\limits_{-\\infty}^\\infty G[f] \\prod_x df(x).\n</math>\n\nHowever, in most cases the functions ''f''(''x'') can be written in terms of an infinite series of [[orthogonal functions]] such as <math>f(x) = f_n H_n(x)</math>, and then the definition becomes\n\n:<math>\n\\int G[f] [Df] \\equiv \\int\\limits_{-\\infty}^\\infty \\cdots \\int\\limits_{-\\infty}^\\infty G(f_1, f_2, \\ldots) \\prod_n df_n,\n</math>\n\nwhich is slightly more understandable. The integral is shown to be a functional integral with a capital ''D''. Sometimes it is written in square brackets: [''Df''] or ''D''[''f''], to indicate that ''f'' is a function.\n\n==Examples==\nMost functional integrals are actually infinite, but the [[quotient]] of two functional integrals can be finite.{{clarify|date=March 2017}} The functional integrals that can be solved exactly usually start with the following [[Gaussian integral]]:\n\n:<math>\n\\frac{\\int e^{i \\int -\\frac{1}{2}f(x) \\cdot K(x,y) \\cdot f(y) \\,dx\\,dy + \\int J(x) \\cdot f(x) \\,dx} [Df]}\n     {\\int e^{i \\int -\\frac{1}{2}f(x) \\cdot K(x,y) \\cdot f(y) \\,dx\\,dy} [Df]} =\n e^{i \\frac{1}{2}\\int J(x) \\cdot K^{-1}(x,y) \\cdot J(y) \\,dx\\,dy}.\n</math>\n\nBy functionally differentiating this with respect to ''J''(''x'') and then setting  to 0 this becomes an exponential multiplied by a polynomial in ''f''. For example, setting <math>K(x, y) = \\Box\\delta(x - y)</math>, we find:\n\n:<math>\n\\frac{\\int f(a) f(b) e^{i \\int f(x) \\Box f(x) \\,dx^4} [Df]}\n     {\\int e^{i \\int f(x) \\Box f(x) \\,dx^4} [Df]} =\n K^{-1}(a, b) = \\frac{1}{|a - b|^2},\n</math>\n\nwhere ''a'', ''b'' and ''x'' are 4-dimensional vectors. This comes from the formula for the propagation of a photon in quantum electrodynamics. Another useful integral is the functional [[delta function]]:\n\n:<math>\n\\int e^{i \\int f(x) g(x) \\,dx} [Df] = \\delta[g] = \\prod_x\\delta\\big(g(x)\\big),\n</math>\n\nwhich is useful to specify constraints. Functional integrals can also be done over [[Grassmann number|Grassmann-valued]] functions <math>\\psi(x)</math>, where <math>\\psi(x) \\psi(y) = -\\psi(y) \\psi(x)</math>, which is useful in quantum electrodynamics for calculations involving [[fermions]].\n\n==Approaches to path integrals==\n{{expand section|date=October 2009}}\nFunctional integrals where the space of integration consists of paths (''ν'' = 1) can be defined in many different ways.  The definitions fall in two different classes: the constructions derived from [[Wiener process|Wiener's theory]] yield an integral based on a [[Measure (mathematics)|measure]], whereas the constructions following Feynman's path integral do not.  Even within these two broad divisions, the integrals are not identical, that is, they are defined differently for different classes of functions.\n\n===The Wiener integral===\n\nIn the [[Wiener process|Wiener integral]], a probability is assigned to a class of [[Brownian motion]] paths.  The class consists of the paths ''w'' that are known to go through a small region of space at a given time.  The passage through different regions of space is assumed independent of each other, and the distance between any two points of the Brownian path is assumed to be [[Normal distribution|Gaussian-distributed]] with a [[variance]] that depends on the time ''t'' and on a diffusion constant ''D'':\n\n:<math>\\Pr\\big(w(s + t), t \\,\\big|\\, w(s), s\\big) = \\frac{1}{\\sqrt{2\\pi D t}} \\exp\\left(-\\frac{\\|w(s+t) - w(s)\\|^2}{2Dt}\\right).</math>\n\nThe probability for the class of paths can be found by multiplying the probabilities of starting in one region and then being at the next.  The Wiener measure can be developed by considering the limit of many small regions.\n\n* Itō and Stratonovich calculus\n\n===The Feynman integral===\n\n* Trotter formula, or [[Lie product formula]].\n* The Kac idea of Wick rotations.\n* Using x-dot-dot-squared or i S[x] + x-dot-squared.\n* The Cartier DeWitt-Morette relies on integrators rather than measures\n\n===The Lévy integral===\n\n* [[Fractional quantum mechanics]]\n* [[Fractional Schrödinger equation]]\n* [[Lévy process]]\n* [[Fractional statistical mechanics]]\n\n==See also==\n*[[Feynman path integral]]\n*[[Partition function (quantum field theory)]]\n*[[Saddle point approximation]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n*[http://www.scholarpedia.org/Path_integral    Jean Zinn-Justin (2009), ''Scholarpedia'' '''4'''(2):8674].\n* [[Hagen Kleinert|Kleinert, Hagen]], ''Path Integrals in Quantum Mechanics, Statistics, Polymer Physics, and Financial Markets'', 4th edition, World Scientific (Singapore, 2004); Paperback {{ISBN|981-238-107-4}}  '' (also available online: [http://www.physik.fu-berlin.de/~kleinert/b5 PDF-files])''\n*{{ cite journal|author-link=Nick Laskin|arxiv=0811.1769|doi=10.1103/PhysRevE.62.3135|title=Fractional quantum mechanics|year=2000|last1=Laskin|first1=Nick|journal=Physical Review E|volume=62|issue=3|pages=3135|bibcode = 2000PhRvE..62.3135L }}\n*{{ cite journal|author-link=Nick Laskin|arxiv=quant-ph/0206098 |doi=10.1103/PhysRevE.66.056108|title=Fractional Schrödinger equation|year=2002|last1=Laskin|first1=Nick|journal=Physical Review E|volume=66|issue=5|bibcode = 2002PhRvE..66e6108L }}\n* O. G. Smolyanov, E. T. Shavgulidze. ''Continual integrals''. Moscow, Moscow State University Press, 1990. (in Russian). http://lib.mexmat.ru/books/5132\n*[[Victor Popov]], Functional Integrals in Quantum Field Theory and Statistical Physics, Springer 1983\n\n\n\n\n\n\n\n[[Category:Integral calculus]]\n[[Category:Functional analysis]]\n[[Category:Mathematical physics]]\n[[Category:Quantum mechanics]]\n[[Category:Quantum field theory]]"
    },
    {
      "title": "Glasser's master theorem",
      "url": "https://en.wikipedia.org/wiki/Glasser%27s_master_theorem",
      "text": "In [[integral calculus]], '''Glasser's master theorem''' explains how a certain broad class of substitutions can simplify certain integrals over the whole interval from <math>-\\infty</math> to <math>+\\infty.</math> It is applicable in cases where the integrals must be construed as [[Cauchy principal value]]s, and ''a&nbsp;fortiori'' it is applicable when the integral [[absolute convergence|converges absolutely]]. It is named after M.&nbsp;L.&nbsp;Glasser, who introduced it in 1983.<ref>Glasser, M. L. \"A Remarkable Property of Definite Integrals.\" ''Mathematics of Computation'' 40, 561–563, 1983.</ref>\n\n== A special case: the Cauchy–Schlömilch transformation ==\nA special case called the Cauchy–Schlömilch substitution or Cauchy–Schlömilch transformation<ref>T. Amdeberhnan, M. L. Glasser, M. C. Jones, V. H. Moll, R. Posey, and D. Varela, \"The Cauchy–Schlömilch transformation\", arxiv.org/pdf/1004.2445.pdf</ref> was known to [[Augustin-Louis Cauchy|Cauchy]] in the early 19th century.<ref>A. L. Cauchy, \"Sur une formule generale relative a la transformation des integrales simples prises entre les limites 0 et ∞ de la variable.\" ''Oeuvres completes'', serie 2, ''Journal de l’ecole Polytechnique'', XIX cahier, tome XIII, 516–519, 1:275–357, 1823</ref> It states that if\n\n: <math> u = x - \\frac 1 x \\, </math>\n\nthen\n\n: <math> \\operatorname{PV} \\int_{-\\infty}^\\infty F(u)\\,dx = \\operatorname{PV} \\int_{-\\infty}^\\infty F(x)\\,dx \\qquad (\\text{Note: } F(u)\\,dx, \\text{ not } F(u)\\,du) </math>\n\nwhere PV denotes the Cauchy principal value.\n\n== The master theorem ==\n\nIf <math>a</math>, <math>a_i</math>, and <math>b_i</math> are real numbers and\n\n: <math> u = x - a - \\sum_{n=1}^N \\frac{|a_n|}{x-b_n} </math>\n\nthen\n\n: <math> \\operatorname{PV} \\int_{-\\infty}^\\infty F(u)\\,dx = \\operatorname{PV} \\int_{-\\infty}^\\infty F(x)\\,dx. </math>\n\n== Examples ==\n\n&nbsp;\n\n* <math> \\int_{-\\infty}^\\infty \\frac{x^2\\,dx}{x^4+1} = \\int_{-\\infty}^\\infty \\frac{dx}{\\left( x-\\frac 1 x \\right)^2 + 2} = \\int_{-\\infty}^\\infty \\frac{dx}{x^2 + 2} = \\frac \\pi {\\sqrt 2}. </math>\n\n== References ==\n\n{{reflist}}\n\n== External links ==\n\n* {{MathWorld |id=GlassersMasterTheorem  |title= Glasser's Master Theorem}}\n\n[[Category:Integral calculus]]"
    },
    {
      "title": "Improper integral",
      "url": "https://en.wikipedia.org/wiki/Improper_integral",
      "text": "{{Short description|Limit of a definite integral with as one or both limits approach infinity or values at which the integrand is undefined}}\n[[File:Improperintegral2.png|right|thumb|200px|An improper integral of the first kind. The integral may need to be defined on an unbounded domain.]]\n[[File:Improperintegral1.png|right|thumb|200px|An improper Riemann integral of the second kind. The integral may fail to exist because of a [[vertical asymptote]] in the function.]]\n{{Calculus |Integral}}\n\nIn [[mathematical analysis]], an '''improper integral''' is the [[limit (mathematics)|limit]] of a [[definite integral]] as an endpoint of the interval(s) of integration approaches either a specified [[real number]], <math>\\infty</math>, <math>-\\infty</math>, or in some instances as both endpoints approach limits. Such an integral is often written symbolically just like a standard definite integral, in some cases with ''infinity'' as a limit of integration.\n\nSpecifically, an improper integral is a limit of the form:\n:<math>\\lim_{b\\to\\infty} \\int_a^bf(x)\\, dx, \\qquad \\lim_{a\\to -\\infty} \\int_a^bf(x)\\, dx,</math>\nor\n:<math>\\lim_{c\\to b^-} \\int_a^cf(x)\\, dx,\\quad\n\\lim_{c\\to a^+} \\int_c^bf(x)\\, dx,</math>\nin which one takes a limit in one or the other (or sometimes both) endpoints {{harv|Apostol|1967|loc=§10.23}}.\n\nBy [[abuse of notation]], improper integrals are often written symbolically just like standard definite integrals, perhaps with ''infinity'' among the limits of integration. When the definite integral exists (in the sense of either the [[Riemann integral]] or the more advanced [[Lebesgue integral]]), this ambiguity is resolved as both the proper and improper integral will coincide in value.\n\nOften one is able to compute values for improper integrals, even when the function is not integrable in the conventional sense (as a [[Riemann integral]], for instance) because of a singularity in the function or because one of the bounds of integration is infinite.\n\n==Examples==\nThe original definition of the [[Riemann integral]] does not apply to a function such as <math>1/{x^2}</math> on the interval [1, ∞), because in this case the domain of integration is [[bounded set|unbounded]]. However, the Riemann integral can often be extended by [[continuous function|continuity]], by defining the improper integral instead as a [[Limit (mathematics)|limit]]\n\n:<math>\\int_1^\\infty \\frac{1}{x^2}\\,dx=\\lim_{b\\to\\infty} \\int_1^b\\frac{1}{x^2}\\,dx = \\lim_{b\\to\\infty} \\left(-\\frac{1}{b} + \\frac{1}{1}\\right) = 1. </math>\n\nThe narrow definition of the Riemann integral also does not cover the function <math>1/\\sqrt{x}</math> on the interval [0, 1]. The problem here is that the integrand is [[bounded function|unbounded]] in the domain of integration (the definition requires that both the domain of integration and the integrand be bounded). However, the improper integral does exist if understood as the limit\n\n:<math>\\int_0^1 \\frac{1}{\\sqrt{x}}\\,dx=\\lim_{a\\to 0^+}\\int_a^1\\frac{1}{\\sqrt{x}}\\, dx = \\lim_{a\\to 0^+}(2-2\\sqrt{a})=2.</math>\n\n[[File:Improper integral.svg|right|thumb|The improper integral<br/><math>\\int_{0}^{\\infty} \\frac{dx}{(x+1)\\sqrt{x}} = \\pi</math><br/> has unbounded intervals for both domain and range.]]Sometimes integrals may have two singularities where they are improper.  Consider, for example, the function {{math|1/((''x'' + 1){{sqrt|''x''}})}} integrated from 0 to {{math|∞}} (shown right). At the lower bound, as {{mvar|x}} goes to 0 the function goes to {{math|∞}}, and the upper bound is itself {{math|∞}}, though the function goes to 0. Thus this is a doubly improper integral. Integrated, say, from 1 to 3, an ordinary Riemann sum suffices to produce a result of {{pi}}/6. To integrate from 1 to {{math|∞}}, a Riemann sum is not possible. However, any finite upper bound, say {{mvar|t}} (with {{math|''t'' &gt; 1}}), gives a well-defined result, {{math|2 arctan({{sqrt|''t''}}) − {{pi}}/2}}. This has a finite limit as {{mvar|t}} goes to infinity, namely {{pi}}/2. Similarly, the integral from 1/3 to 1 allows a Riemann sum as well, coincidentally again producing {{pi}}/6. Replacing 1/3 by an arbitrary positive value {{mvar|s}} (with {{math|''s'' &lt; 1}}) is equally safe, giving {{math|{{pi}}/2 − 2 arctan({{sqrt|''s''}})}}. This, too, has a finite limit as {{mvar|s}} goes to zero, namely {{pi}}/2. Combining the limits of the two fragments, the result of this improper integral is\n:<math>\\begin{align}\n \\int_{0}^{\\infty} \\frac{dx}{(x+1)\\sqrt{x}} &{} = \\lim_{s \\to 0^+} \\int_{s}^{1} \\frac{dx}{(x+1)\\sqrt{x}}\n   + \\lim_{t \\to \\infty} \\int_{1}^{t} \\frac{dx}{(x+1)\\sqrt{x}} \\\\\n  &{} = \\lim_{s \\to 0^+} \\left(\\frac{\\pi}{2} - 2 \\arctan{\\sqrt{s}} \\right)\n   + \\lim_{t \\to \\infty} \\left(2 \\arctan{\\sqrt{t}} - \\frac{\\pi}{2} \\right) \\\\\n  &{} = \\frac{\\pi}{2} + \\left(\\pi - \\frac{\\pi}{2} \\right) \\\\\n  &{} = \\pi .\n\\end{align}</math>\nThis process does not guarantee success; a limit might fail to exist, or might be infinite. For example, over the bounded interval from 0 to 1 the integral of 1/{{mvar|x}} does not converge; and over the unbounded interval from 1 to {{math|∞}} the integral of 1/{{math|{{sqrt|''x''}}}} does not converge.\n\n[[File:Improper integral unbounded internally.svg|right|thumb|The improper integral<br/><math>\\int_{-1}^{1} \\frac{dx}{\\sqrt[3]{x^2}} = 6</math><br/> converges, since both left and right limits exist, though the integrand is unbounded near an interior point.]]\nIt might also happen that an integrand is unbounded near an interior point, in which case the integral must be split at that point. For the integral as a whole to converge, the limit integrals on both sides must exist and must be bounded. For example:\n:<math>\\begin{align}\n \\int_{-1}^{1} \\frac{dx}{\\sqrt[3]{x^2}} &{} = \\lim_{s \\to 0} \\int_{-1}^{-s} \\frac{dx}{\\sqrt[3]{x^2}}\n   + \\lim_{t \\to 0} \\int_{t}^{1} \\frac{dx}{\\sqrt[3]{x^2}} \\\\\n  &{} = \\lim_{s \\to 0} 3(1-\\sqrt[3]{s}) + \\lim_{t \\to 0} 3(1-\\sqrt[3]{t}) \\\\\n  &{} = 3 + 3 \\\\\n  &{} = 6.\n\\end{align}</math>\nBut the similar integral\n:<math>\\int_{-1}^{1} \\frac{dx}{x}</math>\ncannot be assigned a value in this way, as the integrals above and below zero do not independently converge. (However, see [[Cauchy principal value]].)\n\n== Convergence of the integral ==\nAn improper integral converges if the limit defining it exists.  Thus for example one says that the improper integral\n:<math>\\lim_{t\\to\\infty}\\int_a^t f(x)\\,dx</math>\nexists and is equal to ''L'' if the integrals under the limit exist for all sufficiently large ''t'', and the value of the limit is equal to ''L''.\n\nIt is also possible for an improper integral to diverge to infinity.  In that case, one may assign the value of ∞ (or -∞) to the integral. For instance \n:<math>\\lim_{b\\to\\infty}\\int_1^b \\frac{1}{x}\\,dx = \\infty.</math>\nHowever, other improper integrals may simply diverge in no particular direction, such as\n:<math>\\lim_{b\\to\\infty}\\int_1^b x\\sin(x)\\,dx,</math>\nwhich does not exist, even as an [[extended real number]]. This is called divergence by oscillation.\n\nA limitation of the technique of improper integration is that the limit must be taken with respect to one endpoint at a time.  Thus, for instance, an improper integral of the form\n\n:<math>\\int_{-\\infty}^\\infty f(x)\\,dx</math>\n\ncan be defined by taking two separate limits; to wit\n\n:<math>\\int_{-\\infty}^\\infty f(x)\\,dx = \\lim_{a\\to -\\infty} \\lim_{b\\to\\infty} \\int_a^bf(x)\\,dx</math>\n\nprovided the double limit is finite. It can also be defined as a pair of distinct improper integrals of the first kind:\n\n:<math>\\lim_{a\\to -\\infty}\\int_a^cf(x)\\,dx + \\lim_{b\\to\\infty} \\int_c^b f(x)\\,dx</math>\n\nwhere ''c'' is any convenient point at which to start the integration. This definition also applies when one of these integrals is infinite, or both if they have the same sign.\n\nAn example of an improper integrals where both endpoints are infinite is the [[Gaussian integral]] <math>\\int_{-\\infty}^\\infty e^{-x^2}\\,dx = \\sqrt{\\pi}</math>. An example which evaluates to infinity is <math>\\int_{-\\infty}^\\infty e^x\\,dx</math>. But one cannot even define other integrals of this kind unambiguously, such as <math>\\int_{-\\infty}^\\infty x\\,dx</math>, since the double limit is infinite and the two-integral method\n\n:<math>\\lim_{a\\to -\\infty}\\int_a^cx\\,dx + \\lim_{b\\to\\infty} \\int_c^b x\\,dx</math>\nyields <math>\\infty-\\infty</math>. In this case, one can however define an improper integral in the sense of [[Cauchy principal value]]:\n\n:<math> \\operatorname{p.v.} \\int_{-\\infty}^\\infty x\\,dx = \\lim_{b\\to\\infty}\\int_{-b}^b x\\,dx = 0.</math>\n\nThe questions one must address in determining an improper integral are:\n\n*Does the limit exist?\n*Can the limit be computed?\n\nThe first question is an issue of [[mathematical analysis]]. The second one can be addressed by calculus techniques, but also in some cases by [[contour integration]], [[Fourier transform]]s and other more advanced methods.\n\n==Types of integrals==\nThere is more than one theory of [[integral|integration]]. From the point of view of calculus, the [[Riemann integral]] theory is usually assumed as the default theory. In using improper integrals, it can matter which integration theory is in play.\n\n* For the Riemann integral (or the [[Darboux integral]], which is equivalent to it), improper integration is necessary ''both'' for unbounded intervals (since one cannot divide the interval into finitely many subintervals of finite length) ''and'' for unbounded functions with finite integral (since, supposing it is unbounded above, then the upper integral will be infinite, but the lower integral will be finite).\n* The [[Lebesgue integral]] deals differently with unbounded domains and unbounded functions, so that often an integral which only exists as an improper Riemann integral will exist as a (proper) Lebesgue integral, such as <math>\\int_1^\\infty \\frac{1}{x^2}\\,dx</math>.  On the other hand, there are also integrals that have an improper Riemann integral but do not have a (proper) Lebesgue integral, such as <math>\\int_0^\\infty \\frac{\\sin x}{x}\\,dx</math>. The Lebesgue theory does not see this as a deficiency: from the point of view of [[measure theory]], <math>\\int_0^\\infty \\frac{\\sin x}{x}\\,dx = \\infty - \\infty</math> and cannot be defined satisfactorily.  In some situations, however, it may be convenient to employ improper Lebesgue integrals as is the case, for instance, when defining the [[Cauchy principal value]]. The Lebesgue integral is more or less essential in the theoretical treatment of the [[Fourier transform]], with pervasive use of integrals over the whole real line.\n* For the [[Henstock–Kurzweil integral]], improper integration ''is not necessary'', and this is seen as a strength of the theory: it encompasses all Lebesgue integrable and improper Riemann integrable functions.\n\n== Improper Riemann integrals and Lebesgue integrals ==\n[[File:Improperintegral1.png|right|thumb|200px|Figure 1]]\n[[File:Improperintegral2.png|right|thumb|200px|Figure 2]]\n\nIn some cases, the integral\n\n:<math>\\int_a^c f(x)\\,dx</math>\n\ncan be defined as an integral (a [[Lebesgue integral]], for instance) without reference to the limit\n\n:<math>\\lim_{b\\to c^-}\\int_a^b f(x)\\,dx</math>\n\nbut cannot otherwise be conveniently computed.  This often happens when the function ''f'' being integrated from ''a'' to ''c'' has a [[vertical asymptote]] at ''c'', or if ''c''&nbsp;=&nbsp;∞ (see Figures 1 and 2).  In such cases, the improper Riemann integral allows one to calculate the Lebesgue integral of the function.  Specifically, the following theorem holds  {{harv|Apostol|1974|loc=Theorem 10.33}}:\n\n* If a function ''f'' is Riemann integrable on [''a'',''b''] for every ''b''&nbsp;≥&nbsp;''a'', and the partial integrals\n::<math>\\int_a^b|f(x)|\\,dx</math>\n:are bounded as ''b''&nbsp;&rarr;&nbsp;∞, then the improper Riemann integrals\n::<math>\\int_a^\\infty f(x)\\,dx,\\quad\\mbox{and }\\int_a^\\infty |f(x)|\\,dx</math>\n:both exist.  Furthermore, ''f'' is Lebesgue integrable on [''a'', ∞), and its Lebesgue integral is equal to its improper Riemann integral.\n\nFor example, the integral\n:<math>\\int_0^\\infty\\frac{dx}{1+x^2}</math>\ncan be interpreted alternatively as the improper integral\n:<math>\\lim_{b\\to\\infty}\\int_0^b\\frac{dx}{1+x^2}=\\lim_{b\\to\\infty}\\arctan{b}=\\frac{\\pi}{2},</math>\nor it may be interpreted instead as a [[Lebesgue integral]] over the set (0, ∞).  Since both of these kinds of integral agree, one is free to choose the first method to calculate the value of the integral, even if one ultimately wishes to regard it as a Lebesgue integral.  Thus improper integrals are clearly useful tools for obtaining the actual values of integrals.\n\nIn other cases, however, a Lebesgue integral between finite endpoints may not even be defined, because the integrals of the positive and negative parts of ''f'' are both infinite, but the improper Riemann integral may still exist.  Such cases are \"properly improper\" integrals, i.e. their values cannot be defined except as such limits.  For example,\n\n:<math>\\int_0^\\infty\\frac{\\sin(x)}{x}\\,dx</math>\n\ncannot be interpreted as a Lebesgue integral, since\n\n:<math>\\int_0^\\infty\\left|\\frac{\\sin(x)}{x}\\right|\\,dx=\\infty.</math>\n\nBut <math>f(x)=\\frac{\\sin(x)}{x}</math> is nevertheless integrable between any two finite endpoints, and its integral between 0 and ∞ is usually understood as the limit of the integral:\n\n:<math>\\int_0^\\infty\\frac{\\sin(x)}{x}\\,dx=\\lim_{b\\to\\infty}\\int_0^b\\frac{\\sin(x)}{x}\\,dx=\\frac{\\pi}{2}.</math>\n\n==Singularities==\nOne can speak of the ''singularities'' of an improper integral, meaning those points of the [[extended real number line]] at which limits are used.\n\n==Cauchy principal value==\n{{main article|Cauchy principal value}}\nConsider the difference in values of two limits:\n\n:<math>\\lim_{a\\to 0^+}\\left(\\int_{-1}^{-a}\\frac{dx}{x}+\\int_a^1\\frac{dx}{x}\\right)=0,</math>\n\n:<math>\\lim_{a\\to 0^+}\\left(\\int_{-1}^{-a}\\frac{dx}{x}+\\int_{2a}^1\\frac{dx}{x}\\right)=-\\ln 2.</math>\n\nThe former is the Cauchy principal value of the otherwise ill-defined expression\n\n:<math>\\int_{-1}^1\\frac{dx}{x}{\\  }\n\\left(\\mbox{which}\\  \\mbox{gives}\\  -\\infty+\\infty\\right).</math>\n\nSimilarly, we have\n\n:<math>\\lim_{a\\to\\infty}\\int_{-a}^a\\frac{2x\\,dx}{x^2+1}=0,</math>\n\nbut\n\n:<math>\\lim_{a\\to\\infty}\\int_{-2a}^a\\frac{2x\\,dx}{x^2+1}=-\\ln 4.</math>\n\nThe former is the principal value of the otherwise ill-defined expression\n\n:<math>\\int_{-\\infty}^\\infty\\frac{2x\\,dx}{x^2+1}{\\  }\n\\left(\\mbox{which}\\  \\mbox{gives}\\  -\\infty+\\infty\\right).</math>\n\nAll of the above limits are cases of the [[indeterminate form]] ∞ &minus; ∞.\n\nThese [[pathological (mathematics)|pathologies]] do not affect \"Lebesgue-integrable\" functions, that is, functions the integrals of whose [[absolute value]]s are finite.\n\n==Summability==\nAn improper integral may diverge in the sense that the limit defining it may not exist.  In this case, there are more sophisticated definitions of the limit which can produce a convergent value for the improper integral.  These are called [[summability]] methods.\n\nOne summability method, popular in [[Fourier analysis]], is that of [[Cesàro summation]].  The integral\n\n:<math>\\int_0^\\infty f(x)\\,dx</math>\n\nis Cesàro summable (C,&nbsp;α) if\n\n:<math>\\lim_{\\lambda\\to\\infty}\\int_0^\\lambda\\left(1-\\frac{x}{\\lambda}\\right)^\\alpha f(x)\\,dx</math>\n\nexists and is finite {{harv|Titchmarsh|1948|loc=§1.15}}.  The value of this limit, should it exist, is the (C,&nbsp;α) sum of the integral.\n\nAn integral is (C,&nbsp;0) summable precisely when it exists as an improper integral.  However, there are integrals which are (C,&nbsp;α) summable for α&nbsp;>&nbsp;0 which fail to converge as improper integrals (in the sense of Riemann or Lebesgue). One example is the integral\n\n:<math>\\int_0^\\infty\\sin x \\,dx</math>\n\nwhich fails to exist as an improper integral, but is (C,α) summable for every α&nbsp;>&nbsp;0.  This is an integral version of [[Grandi's series]].\n\n==Multivariable improper integrals==\nThe improper integral can also be defined for functions of several variables. The definition is slightly different, depending on whether one requires integrating over an unbounded domain, such as <math>\\R^2</math>, or is integrating a function with singularities, like <math>f(x,y)=\\log(x^2+y^2)</math>.\n\n===Improper integrals over arbitrary domains===\nIf <math>f:\\R^n\\to\\R</math> is a non-negative function that is Riemann integrable over every compact cube of the form <math>[-a,a]^n</math>, for <math>a>0</math>, then the improper integral of ''f'' over <math>\\R^n</math> is defined to be the limit\n:<math>\\lim_{a\\to\\infty}\\int_{[-a,a]^n}f,</math>\nprovided it exists.\n\nA function on an arbitrary domain ''A'' in <math>\\mathbb R^n</math> is extended to a function <math>\\tilde{f}</math> on <math>\\R^n</math> by zero outside of ''A'':\n:<math>\\tilde{f}(x)=\\begin{cases}f(x)& x\\in A\\\\\n0 & x\\not\\in A\n\\end{cases}</math>\nThe Riemann integral of a function over a bounded domain ''A'' is then defined as the integral of the extended function <math>\\tilde{f}</math> over a cube <math>[-a,a]^n</math> containing ''A'':  \n:<math>\\int_A f = \\int_{[-a,a]^n}\\tilde{f}.</math>\nMore generally, if ''A'' is unbounded, then the improper Riemann integral over an arbitrary domain in <math>\\mathbb R^n</math> is defined as the limit:\n:<math>\\int_Af=\\lim_{a\\to\\infty}\\int_{A\\cap [-a,a]^n}f=\\lim_{a\\to\\infty}\\int_{[-a,a]^n}\\tilde{f}.</math>\n\n===Improper integrals with singularities===\nIf ''f'' is a non-negative function which is unbounded in a domain ''A'', then the improper integral of ''f'' is defined by truncating ''f'' at some cutoff ''M'', integrating the resulting function, and then taking the limit as ''M'' tends to infinity.  That is for <math>M>0</math>, set <math>f_M=\\min\\{f,M\\}</math>.  Then define\n:<math>\\int_A f = \\lim_{M\\to\\infty}\\int_A f_M</math>\nprovided this limit exists.\n\n===Functions with both positive and negative values===\nThese definitions apply for functions that are non-negative.  A more general function ''f'' can be decomposed as a difference of its positive part <math>f_+=\\max\\{f,0\\}</math> and negative part <math>f_-=\\max\\{-f,0\\}</math>, so\n:<math>f=f_+-f_-</math>\nwith <math>f_+</math> and <math>f_-</math> both non-negative functions.  The function ''f'' has an improper Riemann integral if each of <math>f_+</math> and <math>f_-</math> has one, in which case the value of that improper integral is defined by\n:<math>\\int_Af = \\int_Af_+ - \\int_A f_-.</math>\nIn order to exist in this sense, the improper integral necessarily converges absolutely, since\n:<math>\\int_A|f| = \\int_Af_+ + \\int_Af_-.</math><ref>{{harvnb|Cooper|2005|loc=p. 538}}: \"We need to make this stronger definition of convergence in terms of |''f''(''x'')| because cancellation in the integrals can occur in so many different ways in higher dimensions.\"</ref><ref>{{harvnb|Ghorpade|Limaye|2010|loc=p. 448}}: \"The relevant notion here is that of unconditional convergence.\" ... \"In fact, for improper integrals of such functions, unconditional convergence turns out to be equivalent to absolute convergence.\"</ref>\n\n==Notes==\n<references />\n\n==Bibliography==\n* {{citation|last=Apostol|first=T|authorlink=Tom M. Apostol|title=Mathematical analysis|publisher=Addison-Wesley|year=1974|isbn=978-0-201-00288-1}}.\n* {{citation|last=Apostol|first=T|authorlink=Tom M. Apostol|title=Calculus, Vol. 1|publisher=Jon Wiley & Sons|edition=2nd|year=1967}}.\n*{{Citation\n |author=Autar Kaw, Egwu Kalu\n |year=2008\n |title=Numerical Methods with Applications\n |url=http://numericalmethods.eng.usf.edu/topics/textbook_index.html\n |edition=1st\n |publisher=autarkaw.com\n |isbn=\n}}\n* {{citation|last=Titchmarsh|first=E|authorlink=Edward Charles Titchmarsh|title=Introduction to the theory of Fourier integrals|isbn=978-0-8284-0324-5|year=1948|edition=2nd|publication-date=1986|publisher=Chelsea Pub. Co.|location=New York, N.Y.}}.\n*{{citation|last=Cooper|first=Jeffery|title=Working analysis|publisher=Gulf Professional|year=2005}}\n*{{citation|last1=Ghorpade|first1=Sudhir|last2=Limaye|first2=Balmohan|title=A course in multivariable calculus and analysis|publisher=Springer|year=2010}}\n\n==External links==\n* [http://numericalmethods.eng.usf.edu/topics/improper_integration.html Numerical Methods to Solve Improper Integrals] at Holistic Numerical Methods Institute\n* [http://www.lightandmatter.com/html_books/calc/ch06/ch06.html Improper integrals] – chapter from an online textbook\n\n{{integral}}\n\n[[Category:Integral calculus]]"
    },
    {
      "title": "Indicator function",
      "url": "https://en.wikipedia.org/wiki/Indicator_function",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Function that returns 1 if an element is present in a specified subset and 0 if absent; naturally isomorphic with a set's subsets}}\n{{More footnotes|date=December 2009}}\n{{About|the 0-1 indicator function|the 0-infinity indicator function|characteristic function (convex analysis)}}\n\n[[Image:Indicator function illustration.png|right|thumb|A three-dimensional plot of an indicator function, shown over a square two-dimensional domain (set X): the 'raised' portion overlays those two-dimensional points which are members of the 'indicated' subset (A).]]\nIn [[mathematics]], an '''indicator function''' or a '''characteristic function''' is a [[Function (mathematics)|function]] defined on a [[Set (mathematics)|set]] ''X'' that indicates membership of an [[Element (mathematics)|element]] in a [[subset]] ''A'' of ''X'', having the value 1 for all elements of ''A'' and the value 0 for all elements of ''X'' not in ''A''. It is usually denoted by a symbol 1 or ''I'', sometimes in boldface or  [[blackboard bold]]face, with a subscript specifying the subset.\n\nIn other contexts, such as [[computer science]], this would more often be described as a '''boolean [[Predicate (mathematical logic)|predicate]] function''' (to test set inclusion).\n\nThe [[Dirichlet function]] is an example of an indicator function and is the indicator of the [[rationals]].\n\n==Definition==\nThe indicator function of a subset ''A'' of a set ''X'' is a function\n\n:<math>\\mathbf{1}_A \\colon X \\to \\{ 0,1 \\} </math>\n\ndefined as\n\n:<math>\\mathbf{1}_A(x) :=\n\\begin{cases}\n1 &\\text{if } x \\in A, \\\\\n0 &\\text{if } x \\notin A.\n\\end{cases}\n</math>\n\nThe [[Iverson bracket]] allows the equivalent notation, <math>[x\\in A]</math>, to be used instead of <math>\\mathbf{1}_A(x)</math>.\n\nThe function <math>\\mathbf{1}_A</math> is sometimes denoted <math>I_A</math>, <math>\\chi_A</math>, ''K<sub>A</sub>'' or even just <math>A</math>. (The [[Greek alphabet|Greek letter]] <math>\\chi</math> appears because it is the initial letter of the Greek word χαρακτήρ, which is the ultimate origin of the word ''characteristic''.)\n\nThe set of all indicator functions on <math>X</math> can be identified with <math>\\mathcal{P}(X)</math>, the [[power set]] of <math>X</math>.  Consequently, both sets are sometimes denoted by <math>2^X</math>. This is a special case (<math>Y =\\{0,1\\}=2</math>) of the notation <math>Y^X</math> for the set of all functions <math>f:X\\to Y </math>.\n\n==Remark on notation and terminology==\nThe notation <math>\\chi_A</math> is also used to denote the [[Characteristic function (convex analysis)|characteristic function]] in [[convex analysis]], which is defined as if using the [[Multiplicative inverse|reciprocal]] of the standard definition of the indicator function.\n\nA related concept in [[statistics]] is that of a [[dummy variable (statistics)|dummy variable]]. (This must not be confused with \"dummy variables\" as that term is usually used in mathematics, also called a [[free variables and bound variables|bound variable]].)\n\nThe term \"[[characteristic function (probability theory)|characteristic function]]\" has an unrelated meaning in [[probability theory|classic probability theory]]. For this reason, [[List of probabilists|traditional probabilists]] use the term '''indicator function''' for the function defined here almost exclusively, while mathematicians in other fields are more likely to use the term ''characteristic function'' to describe the function that indicates membership in a set.\n\nIn [[fuzzy logic]] and [[Many-valued logic|modern many-valued logic]], predicates are the [[characteristic function (probability theory)|characteristic functions]] of a [[probability distribution]]. That is, the strict true/false valuation of the predicate is replaced by a quantity interpreted as the degree of truth.\n\n==Basic properties==\nThe ''indicator'' or ''characteristic'' [[function (mathematics)|function]] of a subset ''A'' of some set ''X'', [[Map (mathematics)|maps]] elements of ''X'' to the [[Range (mathematics)|range]] {0,1}.\n\nThis mapping is [[surjective]] only when ''A'' is a non-empty [[proper subset]] of ''X''. If ''A'' ≡ ''X'', then\n'''1'''<sub>''A''</sub> = 1. By a similar argument, if ''A'' ≡ Ø then '''1'''<sub>''A''</sub> = 0.\n\nIn the following, the dot represents multiplication, 1·1 = 1, 1·0 = 0 etc. \"+\" and \"−\" represent addition and subtraction. \"<math>\\cap </math>\" and \"<math>\\cup </math>\" is intersection and union, respectively.\n\nIf <math>A</math> and <math>B</math> are two subsets of <math>X</math>, then\n:<math>\\mathbf{1}_{A\\cap B} = \\min\\{\\mathbf{1}_A,\\mathbf{1}_B\\} = \\mathbf{1}_A \\cdot\\mathbf{1}_B,</math>\n:<math>\\mathbf{1}_{A\\cup B} = \\max\\{{\\mathbf{1}_A,\\mathbf{1}_B}\\} = \\mathbf{1}_A + \\mathbf{1}_B - \\mathbf{1}_A \\cdot\\mathbf{1}_B,</math>\nand the indicator function of the [[Complement (set theory)|complement]] of <math>A</math> i.e. <math>A^C</math> is:\n:<math>\\mathbf{1}_{A^\\complement} = 1-\\mathbf{1}_A</math>.\n\nMore generally, suppose <math>A_1, \\dotsc, A_n</math> is a collection of subsets of ''X''.  For any\n''x'' ∈ ''X'':\n\n:<math> \\prod_{k \\in I} ( 1 - \\mathbf{1}_{A_k}(x))</math>\n\nis clearly a product of 0s and 1s.  This product has the value 1 at\nprecisely those ''x'' ∈ ''X'' that belong to none of the sets ''A<sub>k</sub>'' and\nis 0 otherwise. That is\n\n:<math> \\prod_{k \\in I} ( 1 - \\mathbf{1}_{A_k}) = \\mathbf{1}_{X - \\bigcup_{k} A_k} = 1 - \\mathbf{1}_{\\bigcup_{k} A_k}.</math>\n\nExpanding the product on the left hand side,\n\n: <math> \\mathbf{1}_{\\bigcup_{k} A_k}= 1 - \\sum_{F \\subseteq \\{1, 2, \\dotsc, n\\}} (-1)^{|F|} \\mathbf{1}_{\\bigcap_F A_k} = \\sum_{\\emptyset \\neq F \\subseteq \\{1, 2, \\dotsc, n\\}} (-1)^{|F|+1} \\mathbf{1}_{\\bigcap_F A_k} </math>\n\nwhere |''F''| is the cardinality of ''F''. This is one form of the principle of [[inclusion-exclusion]].\n\nAs suggested by the previous example, the indicator function is a useful notational device in [[combinatorics]].  The notation is used in other places as well, for instance in [[probability theory]]: if <math>X</math> is a [[probability space]] with probability measure <math>\\operatorname{P}</math> and <math>A</math> is a [[Measure (mathematics)|measurable set]], then <math>\\mathbf{1}_A</math> becomes a [[random variable]] whose [[expected value]] is equal to the probability of <math>A</math>:\n\n:<math>\\operatorname{E}(\\mathbf{1}_A)= \\int_{X} \\mathbf{1}_A(x)\\,d\\operatorname{P} = \\int_{A} d\\operatorname{P} = \\operatorname{P}(A)</math>.\n\nThis identity is used in a simple proof of [[Markov's inequality]].\n\nIn many cases, such as [[order theory]], the inverse of the indicator function may be defined. This is commonly called the [[generalized Möbius function]], as a generalization of the inverse of the indicator function in elementary [[number theory]], the [[Möbius function]]. (See paragraph below about the use of the inverse in classical recursion theory.)\n\n==Mean, variance and covariance==\nGiven a [[probability space]] <math>\\textstyle (\\Omega, \\mathcal F, \\operatorname{P})</math> with <math>A \\in \\mathcal F</math>, the indicator random variable <math>\\mathbf{1}_A \\colon \\Omega \\rightarrow \\mathbb{R}</math> is defined by <math>\\mathbf{1}_A (\\omega) = 1 </math> if <math> \\omega \\in A,</math> otherwise <math>\\mathbf{1}_A (\\omega) = 0.</math>\n\n;[[Mean]]: <math>\\operatorname{E}(\\mathbf{1}_A (\\omega)) = \\operatorname{P}(A) </math>\n\n;[[Variance]]: <math>\\operatorname{Var}(\\mathbf{1}_A (\\omega)) = \\operatorname{P}(A)(1 - \\operatorname{P}(A)) </math>\n\n;[[Covariance]]: <math> \\operatorname{Cov}(\\mathbf{1}_A (\\omega), \\mathbf{1}_B (\\omega)) = \\operatorname{P}(A \\cap B) - \\operatorname{P}(A)\\operatorname{P}(B) </math>\n\n==Characteristic function in recursion theory, Gödel's and Kleene's ''representing function''==\n[[Kurt Gödel]] described the ''representing function'' in his 1934 paper \"On Undecidable Propositions of Formal Mathematical Systems\". (The paper appears on pp.&nbsp;41–74 in [[Martin Davis (mathematician)|Martin Davis]] ed. ''The Undecidable''):\n:\"There shall correspond to each class or relation R a representing function φ(x<sub>1</sub>, . . ., x<sub>n</sub>) = 0 if R(x<sub>1</sub>, . . ., x<sub>n</sub>) and φ(x<sub>1</sub>, . . ., x<sub>n</sub>) = 1 if ~R(x<sub>1</sub>, . . ., x<sub>n</sub>).\" (p. 42; the \"~\" indicates logical inversion i.e. \"NOT\")\n\n[[Stephen Kleene]] (1952) (p.&nbsp;227) offers up the same definition in the context of the [[primitive recursive function]]s as a function φ of a predicate P takes on values 0 if the predicate is true and 1 if the predicate is false.\n\nFor example, because the product of characteristic functions φ<sub>1</sub>*φ<sub>2</sub>* . . . *φ<sub>n</sub> = 0 whenever any one of the functions equals 0, it plays the role of logical OR: IF φ<sub>1</sub> = 0 OR φ<sub>2</sub> = 0 OR . . . OR φ<sub>n</sub> = 0 THEN their product is 0. What appears to the modern reader as the representing function's logical inversion, i.e. the representing function is 0 when the function R is \"true\" or satisfied\", plays a useful role in Kleene's definition of the logical  functions OR, AND, and IMPLY (p.&nbsp;228), the bounded- (p.&nbsp;228) and unbounded- (p.&nbsp;279ff) [[mu operator]]s (Kleene (1952)) and the CASE function (p.&nbsp;229).\n\n==Characteristic function in fuzzy set theory==\nIn classical mathematics, characteristic functions of sets only take values 1 (members) or 0 (non-members). In [[fuzzy set theory]], characteristic functions are generalized to take value in the real unit interval [0,&nbsp;1], or more generally, in some [[universal algebra|algebra]] or [[structure (mathematical logic)|structure]] (usually required to be at least a [[partially ordered set|poset]] or [[lattice (order)|lattice]]). Such generalized characteristic functions are more usually called [[membership function (mathematics)|membership function]]s, and the corresponding \"sets\" are called ''fuzzy'' sets. Fuzzy sets model the gradual change in the membership [[degree of truth|degree]] seen in many real-world [[predicate (mathematics)|predicate]]s like \"tall\", \"warm\", etc.\n\n==Derivatives of the indicator function==\nA particular indicator function is the [[Heaviside step function]]. The Heaviside step function  ''H''(''x'') is the indicator function of the one-dimensional positive half-line, i.e. the domain [0, ∞). The [[distributional derivative]] of the Heaviside step function is equal to the [[Dirac delta function]], i.e.\n\n:<math>\n\\delta(x)=\\tfrac{d H(x)}{dx},\n</math>\n\nwith the following property:\n\n:<math>\n\\int_{-\\infty}^\\infty f(x) \\, \\delta(x) dx =  f(0).\n</math>\n\nThe derivative of the Heaviside step function can be seen as the 'inward normal derivative' at the 'boundary' of the domain given by the positive half-line. In higher dimensions, the derivative naturally generalises to the inward normal derivative, while the Heaviside step function naturally generalises to the indicator function of some domain ''D''. The surface of ''D'' will be denoted by ''S''. Proceeding, it can be derived that the [[Laplacian of the indicator#Dirac surface delta function|inward normal derivative of the indicator]] gives rise to a 'surface delta function', which can be indicated by δ<sub>''S''</sub>('''x'''):\n\n:<math>\\delta_S(\\mathbf{x})=-\\mathbf{n}_x\\cdot\\nabla_x\\mathbf{1}_{\\mathbf{x}\\in D}</math>\n\nwhere ''n'' is the outward [[Normal (geometry)|normal]] of the surface ''S''. This 'surface delta function' has the following property:<ref>{{citation|last=Lange|first=Rutger-Jan|year=2012|title=Potential theory, path integrals and the Laplacian of the indicator|journal=Journal of High Energy Physics|volume=2012|pages=29–30|issue=11|bibcode=2012JHEP...11..032L|doi=10.1007/JHEP11(2012)032|arxiv = 1302.0864 }}</ref>\n\n:<math>\n-\\int_{\\mathbf{R}^n}f(\\mathbf{x})\\,\\mathbf{n}_x\\cdot\\nabla_x\\mathbf{1}_{\\mathbf{x}\\in D}\\;d^{n}\\mathbf{x}=\\oint_{S}\\,f(\\mathbf{\\beta})\\;d^{n-1}\\mathbf{\\beta}.\n</math>\n\nBy setting the function ''f'' equal to one, it follows that the [[Laplacian of the indicator#Dirac surface delta function|inward normal derivative of the indicator]] integrates to the numerical value of the [[surface area]] ''S''.\n\n==See also==\n{{Div col|colwidth=40em}}\n* [[Dirac measure]]\n* [[Laplacian of the indicator]]\n* [[Dirac delta]]\n* [[Extension (predicate logic)]]\n* [[Free variables and bound variables]]\n* [[Heaviside step function]]\n* [[Iverson bracket]]\n* [[Kronecker delta]], a function that can be viewed as an indicator for the [[Equality (mathematics)|identity relation]]\n* [[Macaulay brackets]]\n* [[Multiset]]\n* [[Membership function (mathematics)|Membership function]]\n* [[Simple function]]\n* [[Dummy variable (statistics)]]\n* [[Statistical classification]]\n* [[Zero-one loss function]]\n{{div col end}}\n\n==Notes==\n{{Reflist}}\n\n==References==\n* {{cite book | last = Folland | first = G.B. | title = Real Analysis: Modern Techniques and Their Applications | edition = Second | publisher = John Wiley & Sons, Inc. | year = 1999 | isbn=978-0-471-31716-6}}\n* {{cite book\n | first1 = Thomas H. | last1 = Cormen | authorlink1 = Thomas H. Cormen\n | first2 = Charles E. | last2 = Leiserson | authorlink2 = Charles E. Leiserson\n | first3 = Ronald L. | last3 = Rivest | authorlink3 = Ronald L. Rivest\n | first4 = Clifford | last4 = Stein | authorlink4 = Clifford Stein\n | title = Introduction to Algorithms\n | edition = Second\n | publisher = MIT Press and McGraw-Hill\n | year = 2001\n | isbn = 978-0-262-03293-3\n | chapter = Section 5.2: Indicator random variables\n | pages = 94–99\n | title-link = Introduction to Algorithms }}\n* {{cite book\n | editor1-first = Martin | editor1-last = Davis | editor1-link = Martin Davis (mathematician)\n | year = 1965\n | title = The Undecidable\n | publisher = Raven Press Books, Ltd. | location = New York\n }}\n* {{cite book\n | first = Stephen | last = Kleene | authorlink = Stephen Kleene\n | origyear = 1952 | title = Introduction to Metamathematics\n | publisher = Wolters-Noordhoff Publishing and North Holland Publishing Company | location = Netherlands\n | type = Sixth Reprint with corrections\n | year = 1971\n }}\n* {{cite book\n | first1 = George | last1 = Boolos | authorlink1 = George Boolos\n | first2 = John P. | last2 = Burgess | authorlink2 = John P. Burgess\n | first3 = Richard C. | last3 = Jeffrey | authorlink3 = Richard C. Jeffrey\n | year = 2002\n | title = Computability and Logic\n | publisher =  Cambridge University Press | location = Cambridge UK\n | isbn = 978-0-521-00758-0\n }}\n* {{cite journal\n |first       = Lotfi A.\n |last        = Zadeh\n |authorlink  = Lotfi A. Zadeh\n |date        = June 1965\n |title       = Fuzzy sets\n |journal     = [[Information and Control]]\n |volume      = 8\n |issue       = 3\n |pages       = 338–353\n |url         = http://www-bisc.cs.berkeley.edu/zadeh/papers/Fuzzy%20Sets-1965.pdf\n |doi         = 10.1016/S0019-9958(65)90241-X\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20070622151801/http://www-bisc.cs.berkeley.edu/zadeh/papers/Fuzzy%20Sets-1965.pdf\n |archivedate = 2007-06-22\n |df          = \n}}\n* {{cite journal\n | first = Joseph | last = Goguen | authorlink = Joseph Goguen\n | year = 1967\n | title = ''L''-fuzzy sets\n | journal = Journal of Mathematical Analysis and Applications\n | volume = 18 | issue = 1 | pages = 145–174\n | doi = 10.1016/0022-247X(67)90189-8\n }}\n\n[[Category:Measure theory]]\n[[Category:Integral calculus]]\n[[Category:Real analysis]]\n[[Category:Mathematical logic]]\n[[Category:Basic concepts in set theory]]\n[[Category:Probability theory]]\n[[Category:Types of functions]]"
    },
    {
      "title": "Integral of secant cubed",
      "url": "https://en.wikipedia.org/wiki/Integral_of_secant_cubed",
      "text": "The '''integral of secant cubed''' is a frequent and challenging<ref>{{cite book|last=Spivak|first=Michael|authorlink=Michael Spivak |title=Calculus|year=2008|chapter=Integration in Elementary Terms |quote=This is a tricky and important integral that often comes up. |page=382}}</ref> [[indefinite integral]] of elementary [[calculus]]:\n\n:<math>\\int \\sec^3 x \\, dx = \\frac{1}{2}(\\sec x \\tan x + \\ln \\left|\\sec x + \\tan x\\right|) + C.</math>\n\nThere are a number of reasons why this particular antiderivative is worthy of special attention:\n\n* The technique used for reducing integrals of higher odd powers of secant to lower ones is fully present in this, the simplest case.  The other cases are done in the same way.\n* The utility of hyperbolic functions in integration can be demonstrated in cases of odd powers of secant (powers of tangent can also be included).\n* This is one of several integrals usually done in a first-year calculus course in which the most natural way to proceed involves [[integrating by parts]] and returning to the same integral one started with (another is the integral of the product of an [[exponential function]] with a sine or cosine function; yet another the integral of a power of the sine or cosine function).\n* This integral is used in evaluating any integral of the form\n:: <math>\\int \\sqrt{a^2+x^2}\\,dx,</math>\n: where ''a'' is a constant.  In particular, it appears in the problems of:\n\n:* rectifying (i.e. finding the [[arc length]] of) the [[parabola]].\n:* rectifying the [[Archimedean spiral]].\n:* finding the [[surface area]] of the [[helicoid]].\n\n== Derivations ==\n\n=== Integration by parts ===\n\nThis [[antiderivative]] may be found by [[integration by parts]], as follows:\n\n:<math> \\int \\sec^3 x \\, dx = \\int u\\,dv </math>\n\nwhere\n\n:<math>\n\\begin{align}\ndv &{}= \\sec^2 x\\,dx, \\\\\nv &{}= \\tan x, \\\\\nu &{}= \\sec x, \\\\\ndu &{}= \\sec x \\tan x\\,dx.\n\\end{align}\n</math>\n\nThen\n\n:<math>\n\\begin{align}\n\\int \\sec^3 x \\, dx &{}= \\int u\\,dv \\\\\n&{}= uv - \\int v\\,du \\\\\n&{} = \\sec x \\tan x - \\int \\sec x \\tan^2 x\\,dx \\\\\n&{}= \\sec x \\tan x - \\int \\sec x\\, (\\sec^2 x - 1)\\,dx \\\\\n&{}= \\sec x \\tan x - \\left(\\int \\sec^3 x \\, dx - \\int \\sec x\\,dx.\\right) \\\\\n&{}= \\sec x \\tan x - \\int \\sec^3 x \\, dx + \\int \\sec x\\,dx.\n\\end{align}\n</math>\n\nHere we have assumed the [[integral of the secant function]] is known.\n\nNext we add <math>\\textstyle \\int\\sec^3 x\\,dx</math> to both sides of the equality just derived:{{efn|Combining two indefinite integrals that look the same is not generally valid, without accounting for a possible constant difference.  In this case, it is included in the remaining integral term.}}\n\n:<math>\n\\begin{align}\n2 \\int \\sec^3 x \\, dx & = \\sec x \\tan x + \\int \\sec x\\,dx \\\\\n& = \\sec x \\tan x + \\ln\\left|\\sec x + \\tan x\\right| + C.\n\\end{align}\n</math>\n\nThen divide both sides by 2:\n\n:<math>\\int \\sec^3 x \\, dx = \\frac{1}{2}(\\sec x \\tan x + \\ln \\left|\\sec x + \\tan x\\right|) + C.</math>\n\n=== Reduction to an integral of a rational function ===\n\n: <math>\n\\int \\sec^3 x \\, dx = \\int \\frac{dx}{\\cos^3 x} = \\int \\frac{\\cos x\\,dx}{\\cos^4 x} = \\int \\frac{\\cos x\\,dx}{(1-\\sin^2 x)^2} = \\int \\frac{du}{(1-u^2)^2}\n</math>\n\nwhere <math>u = \\sin x</math>, so that <math>du = \\cos x\\,dx</math>. This admits a decomposition by [[partial fractions in integration|partial fractions]]:\n\n: <math> \\frac{1}{(1-u^2)^2} = \\frac{1/4}{1-u} + \\frac{1/4}{(1-u)^2} + \\frac{1/4}{1+u} + \\frac{1/4}{(1+u)^2}. </math>\n\nAntidifferentiating term-by-term, one gets\n\n: <math>\n\\begin{align}\n& -\\frac 1 4\\ln (1-u) + \\frac{1/4}{1-u} + \\frac 1 4 \\ln(1+u) - \\frac{1/4}{1+u} + C = \\frac 1 4 \\ln \\frac{1+u}{1-u} + \\frac 1 2 \\frac{u}{1-u^2} + C \\\\[8pt]\n= {} & \\frac 1 4 \\ln\\frac{1+\\sin x}{1-\\sin x} + \\frac 1 2 \\frac{\\sin x}{\\cos^2 x} = \\frac 1 4 \\ln\\frac{1+\\sin x}{1-\\sin x} + \\frac 1 2 \\sec x \\tan x + C.\n\\end{align}\n</math>\n\n=== Hyperbolic functions ===\n\nIntegrals of the form: <math> \\int \\sec^n x \\tan^m x\\, dx </math> can be reduced using the Pythagorean identity if ''n'' is even or ''n'' and ''m'' are both odd. If ''n'' is odd and ''m'' is even, hyperbolic substitutions can be used to replace the nested integration by parts with hyperbolic power reducing formulas.\n\n:<math> \n\\begin{align}\n\\sec x &{}= \\cosh u \\\\\n\\tan x &{}= \\sinh u \\\\\n\\sec^2 x \\, dx &{}= \\cosh u \\, du \\text{ or } \\sec x \\tan x\\, dx = \\sinh u \\, du\\\\\n\\sec x \\, dx &{}= \\, du \\text{ or } dx = \\operatorname{sech} u \\, du \\\\\nu &{}= \\operatorname{arcosh} (\\sec x ) = \\operatorname{arsinh} ( \\tan x ) = \\ln|\\sec x + \\tan x|\n\\end{align}\n</math>\n\nNote that <math> \\int \\sec x \\, dx = \\ln|\\sec x + \\tan x| </math> follows directly from this substitution.\n\n:<math>\n\\begin{align}\n\\int \\sec^3 x \\, dx &{}= \\int \\cosh^2 u\\,du \\\\\n&{}= \\frac{1}{2}\\int ( \\cosh 2u +1) \\,du \\\\\n&{}= \\frac{1}{2} \\left( \\frac{1}{2}\\sinh2u + u\\right) + C\\\\\n&{}= \\frac{1}{2} ( \\sinh u \\cosh u + u ) + C \\\\\n&{}= \\frac{1}{2}(\\sec x \\tan x + \\ln \\left|\\sec x + \\tan x\\right|) + C\\\\\n\\end{align}\n</math>\n\n== Higher odd powers of secant ==\n\nJust as the integration by parts above reduced the integral of secant cubed to the integral of secant to the first power, so a similar process reduces the integral of higher odd powers of secant to lower ones. This is the secant reduction formula, which follows the syntax:\n\n: <math> \\int \\sec^n x \\, dx = \\frac{\\sec^{n-2} x \\tan x}{n-1} \\,+\\, \\frac{n-2}{n-1}\\int \\sec^{n-2} x \\, dx \\qquad \\text{ (for }n \\ne 1\\text{)}\\,\\! </math>\n\nAlternatively:\n\n: <math> \\int \\sec^n x \\, dx = \\frac{\\sec^{n-1} x \\sin x}{n-1} \\,+\\, \\frac{n-2}{n-1}\\int \\sec^{n-2} x \\, dx \\qquad \\text{ (for }n \\ne 1\\text{)}\\,\\! </math>\n\nEven powers of tangents can be accommodated by using binomial expansion to form an odd polynomial of secant and using these formulae on the largest term and combining like terms.\n\n== See also ==\n\n* [[Lists of integrals]]\n\n==Notes==\n{{notelist}}\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Integral Of Secant Cubed}}\n[[Category:Integral calculus]]"
    },
    {
      "title": "Integral of the secant function",
      "url": "https://en.wikipedia.org/wiki/Integral_of_the_secant_function",
      "text": "__NOTOC__\nThe '''integral of the secant function''' of [[trigonometry]] was the subject of one of the \"outstanding open problems of the mid-seventeenth century\", solved in 1668 by [[James Gregory (mathematician)|James Gregory]].<ref name=\"rickey-tuchinsky\">V. Frederick Rickey and Philip M. Tuchinsky, [https://www.jstor.org/stable/2690106 An Application of Geography to Mathematics: History of the Integral of the Secant] in  [[Mathematics Magazine]], volume 53, number 3, May 1980, pages 162–166.</ref>  In 1599, [[Edward Wright (mathematician)|Edward Wright]] evaluated the [[integral]] by [[numerical method]]s – what today we would call [[Riemann sum]]s.<ref>[[Edward Wright (mathematician)|Edward Wright]], ''Certaine Errors in Navigation, Arising either of the ordinaire erroneous making or vsing of the sea Chart, Compasse, Crosse staffe, and Tables of declination of the Sunne, and fixed Starres detected and corrected'', Valentine Simms, London, 1599.</ref>  He wanted the solution for the purposes of [[cartography]] – specifically for constructing an accurate [[Mercator projection]].<ref name=\"rickey-tuchinsky\"/>  In the 1640s, Henry Bond, a teacher of navigation, surveying, and other mathematical topics, compared Wright's numerically computed table of values of the integral of the [[Trigonometric functions#Reciprocal functions|secant]] with a table of logarithms of the tangent function, and consequently conjectured<ref name=\"rickey-tuchinsky\"/> that\n\n: <math> \\int_0^\\theta \\sec\\theta\\,d\\theta = \\ln\\left|\\tan\\left(\\frac{\\theta}{2} + \\frac{\\pi}{4}\\right)\\right|. </math>\n\nThat conjecture became widely known, and in 1665, [[Isaac Newton]] was aware of it.<ref>H. W. Turnbull, editor, ''The Correspondence of Isaac Newton'', Cambridge University Press, 1959–1960, volume 1, pages 13–16 and volume 2, pages 99–100.</ref><ref>[[D. T. Whiteside]], editor, ''The Mathematical Papers of Isaac Newton'', Cambridge University Press, 1967, volume 1, pages 466–467 and 473–475.</ref>\n\n==Evaluations==\n===Barrow's approach===\nAlthough Gregory proved the conjecture in 1668 in his ''Exercitationes Geometricae'', the proof was presented in a form that renders it nearly impossible for modern readers to comprehend; [[Isaac Barrow]], in his ''Geometrical Lectures'' of 1670,<ref>{{cite journal|author=Dresden, Arnold|authorlink=Arnold Dresden|title=Review: ''The Geometrical Lectures of Isaac Barrow'', translated, with notes and proofs, by James Mark Child|journal=Bull. Amer. Math. Soc.|year=1918|volume=24|issue=9|pages=454–456|url=http://www.ams.org/journals/bull/1918-24-09/S0002-9904-1918-03122-4/S0002-9904-1918-03122-4.pdf|doi=10.1090/s0002-9904-1918-03122-4}}</ref> gave the first \"intelligible\" proof, though even that was \"couched in the geometric idiom of the day.\"<ref name=\"rickey-tuchinsky\"/> Barrow's proof of the result was the earliest use of [[partial fraction]]s in integration.<ref name=\"rickey-tuchinsky\"/>  Adapted to modern notation, Barrow's proof began as follows:\n\n: <math> \\int \\sec \\theta \\, d\\theta = \\int \\frac{d\\theta}{\\cos\\theta} = \\int \\frac{\\cos\\theta \\, d\\theta}{\\cos^2\\theta} = \\int \\frac{\\cos\\theta \\, d\\theta}{1 - \\sin^2\\theta} \n</math>\nSubstituting <math>u</math> for <math>\\sin\\theta</math> reduces the integral to\n: <math>\n\\begin{align}\n\\int \\frac{du}{1 - u^2} & = \\int\\frac{du}{(1-u)(1+u)} = \\dfrac12\\int \\left(\\frac{1}{1+u} + \\frac{1}{1-u}\\right)\\,du \\\\[10pt]\n& = \\frac12 \\left( \\ln\\left|1 + u\\right|-\\ln \\left|1-u\\right| \\right) + C = \\frac12 \\ln\\left|\\frac{1+u}{1-u}\\right| + C\n\\end{align}\n</math>\n\nTherefore,\n: <math>\n\\int \\sec \\theta \\, d\\theta = \\left\\{\\begin{array}{l}\n\\dfrac12 \\ln \\left|\\dfrac{1+\\sin\\theta}{1-\\sin\\theta}\\right| + C \\\\[15pt]\n\\ln\\left|\\sec\\theta + \\tan\\theta\\right| + C \\\\[15pt]\n\\ln\\left| \\tan\\left(\\dfrac{\\theta}{2} + \\dfrac{\\pi}{4}\\right) \\right| + C\n\\end{array}\\right\\}\\text{ (equivalent forms)}\n</math>\nThe second of these follows by first multiplying top and bottom of the interior fraction by <math>(1+\\sin\\theta)</math>. This gives <math>\\cos^2\\theta</math> in the denominator and the result follows by moving the factor of 1/2 into the logarithm as a square root.\n\nThe third form follows by replacing <math>\\sin\\theta</math> by <math>-\\cos(\\theta+\\pi/2)</math> and expanding using the [[Trigonometric functions#Identities|identities]] for <math>\\cos2x</math>. It may also be obtained directly by means of the following substitutions:\n: <math>\n\\begin{align}\n\\sec\\theta=\\frac{1}{\\sin\\left(\\theta + \\dfrac{\\pi}{2}\\right)}\n=\\frac{1}{2\\sin\\left(\\dfrac{\\theta}{2} + \\dfrac{\\pi}{4}\\right)\n\\cos\\left(\\dfrac{\\theta}{2} + \\dfrac{\\pi}{4}\\right)}\n=\\frac{\\sec^2\\left(\\dfrac{\\theta}{2} + \\dfrac{\\pi}{4}\\right)}\n{2\\tan\\left(\\dfrac{\\theta}{2} + \\dfrac{\\pi}{4}\\right)}.\n\\end{align}\n</math>\n\nThe conventional solution for the [[Mercator projection]] ordinate may be written without the modulus signs since the latitude (''&phi;'') lies between &minus;{{pi}}/2 and {{pi}}/2:\n:<math>\ny= \\ln \\tan\\!\\left(\\frac \\varphi 2 + \\frac \\pi 4 \\right).\n</math>\n\n===By the Euler substitution===\nThe integral can also be derived by using the [[tangent half-angle substitution]], also known as the Euler or Weierstrass substitution. A somewhat non-standard version of the tangent half-angle substitution, which is simpler in the case of this particular integral, published in 2013,<ref>Michael Hardy, \"Efficiency in Antidifferentiation of the Secant Function\", ''American Mathematical Monthly'', June–July 2013, page 580.</ref> is as follows:\n\n: <math>\n\\begin{align}\n& x = \\tan \\left( \\frac \\pi 4 + \\frac \\theta 2 \\right) \\\\[10pt]\n& \\frac{2x}{1+x^2} = 2\\sin \\left( \\frac \\pi 4 + \\frac \\theta 2 \\right) \\cos \\left( \\frac \\pi 4 + \\frac \\theta 2 \\right)=\\sin \\left(\\frac{\\pi}{2}+\\theta \\right)=\\cos\\theta \\\\[10pt]\n& dx=\\frac{1}{2} \\sec^2 \\left(\\frac{\\pi}{4}+\\frac{\\theta}{2}\\right)d\\theta=\\frac{1}{2} (1+x^2) d\\theta\\\\[10pt]\n& \\frac{2\\,dx}{1+x^2} = d\\theta \\\\[10pt]\n\\int \\sec\\theta \\, d\\theta & = \\int \\frac{dx} x = \\ln\\left| \\tan\\left( \\frac \\pi 4 + \\frac \\theta 2 \\right) \\right| + C.\n\\end{align}\n</math>\n\n==Hyperbolic forms==\nLet\n:<math>\n\\begin{align}\n  \\psi          &=\\ln(\\sec\\theta+\\tan\\theta),\\\\\n  e^\\psi  &=\\sec\\theta+\\tan\\theta,\\\\\n\\sinh\\psi       &=\\frac12(e^\\psi-e^{-\\psi})=\\tan\\theta,\\\\\n\\cosh\\psi       &=\\sqrt{1+\\sinh^2\\psi}=\\sec\\theta,\\\\\n\\tanh\\psi       &=\\sin\\theta.\n  \\end{align}\n</math>\nTherefore,\n:<math>\n\\begin{align}\n  \\int \\sec \\theta \\, d\\theta& \n    =\\psi\n    =\\tanh^{-1}\\! \\left(\\sin\\theta\\right)\n    =\\sinh^{-1}\\! \\left(\\tan\\theta\\right)   \n    =\\cosh^{-1}\\! \\left(\\sec\\theta\\right).\n\\end{align}\n</math>\n\n==Gudermannian and lambertian==\nThe integral of the secant function defines the inverse of the [[Gudermannian function]]:\n:<math>\n\\begin{align}\n\\int \\sec \\theta \\, d\\theta&  = \\operatorname{gd}^{-1}(\\theta)=\\operatorname{lam}(\\theta).\n\\end{align}\n</math>\nThe lambertian function (lam) is a notation for the inverse of the gudermannian which is encountered in the theory of map projections. In particular the Mercator projection may be written<ref name=lee_exact>Lee, L.P. (1976). ''Conformal Projections Based on Elliptic Functions''. Supplement No. 1 to Canadian Cartographer, Vol 13. (Designated as Monograph 16)</ref> as\n:<math>\ny= \\operatorname{lam}(\\varphi).\n</math>\n\n== Notes and references ==\n{{reflist}}\n\n== See also ==\n* [[Integral of secant cubed]]\n* [[Gudermannian function]]\n\n[[Category:Integral calculus]]"
    },
    {
      "title": "Integral test for convergence",
      "url": "https://en.wikipedia.org/wiki/Integral_test_for_convergence",
      "text": "[[File:Integral Test.svg|thumb|right|300px|The integral test applied to the [[harmonic series (mathematics)|harmonic series]].  Since the area under the curve {{math|''y'' {{=}} 1/''x''}} for {{math|''x'' ∈ {{closed-open|1, ∞}}}} is infinite, the total area of the rectangles must be infinite as well.]]\n{{Calculus|Series}}\n\nIn [[mathematics]], the '''integral test for convergence''' is a [[convergence tests|method used to test]] infinite [[series (mathematics)|series]] of [[non-negative]] terms for [[convergent series|convergence]]. It was developed by [[Colin Maclaurin]] and [[Augustin-Louis Cauchy]] and is sometimes known as the '''Maclaurin–Cauchy test'''.\n\n==Statement of the test==\nConsider an [[integer]] {{math|''N''}} and a non-negative function {{math|''f''}} defined on the unbounded [[interval (mathematics)|interval]] {{closed-open|''N'', ∞}}, on which it is [[monotone decreasing]]. Then the infinite series\n\n:<math>\\sum_{n=N}^\\infty f(n)</math>\n\nconverges to a [[real number]] if and only if the [[improper integral]]\n\n:<math>\\int_N^\\infty f(x)\\,dx</math>\n\nis finite. In other words, if the integral diverges, then the [[divergent series|series diverges]] as well.\n\n===Remark===\nIf the improper integral is finite, then the proof also gives the [[upper and lower bounds|lower and upper bounds]]\n\n{{NumBlk|:|<math>\\int_N^\\infty f(x)\\,dx\\le\\sum_{n=N}^\\infty f(n)\\le f(N)+\\int_N^\\infty f(x)\\,dx</math>|{{EquationRef|1}}}}\n\nfor the infinite series.\n\n==Proof==\nThe proof basically uses the [[Direct comparison test|comparison test]], comparing the term {{math|''f''(''n'')}} with the integral of {{math|''f''}} over the intervals\n{{closed-open|''n'' − 1, ''n''}} and {{closed-open|''n'', ''n'' + 1}}, respectively.\n\nSince {{math|''f''}} is a monotone decreasing function, we know that\n\n:<math>\nf(x)\\le f(n)\\quad\\text{for all }x\\in[n,\\infty)\n</math>\n\nand\n\n:<math>\nf(n)\\le f(x)\\quad\\text{for all }x\\in[N,n].\n</math>\n\nHence, for every integer {{math|''n'' ≥ ''N''}},\n\n{{NumBlk|:|<math>\n\\int_n^{n+1} f(x)\\,dx\n\\le\\int_{n}^{n+1} f(n)\\,dx\n=f(n)</math>|{{EquationRef|2}}}}\n\nand, for every integer {{math|''n'' ≥ ''N'' + 1}},\n\n{{NumBlk|:|<math>\nf(n)=\\int_{n-1}^{n} f(n)\\,dx\n\\le\\int_{n-1}^n f(x)\\,dx.\n</math>|{{EquationRef|3}}}}\n\nBy summation over all {{math|''n''}} from {{math|''N''}} to some larger integer {{math|''M''}}, we get from ({{EquationNote|2}})\n\n:<math>\n\\int_N^{M+1}f(x)\\,dx=\\sum_{n=N}^M\\underbrace{\\int_n^{n+1}f(x)\\,dx}_{\\le\\,f(n)}\\le\\sum_{n=N}^Mf(n)\n</math>\n\nand from ({{EquationNote|3}})\n\n:<math>\n\\sum_{n=N}^Mf(n)\\le f(N)+\\sum_{n=N+1}^M\\underbrace{\\int_{n-1}^n f(x)\\,dx}_{\\ge\\,f(n)}=f(N)+\\int_N^M f(x)\\,dx.\n</math>\n\nCombining these two estimates yields\n\n:<math>\\int_N^{M+1}f(x)\\,dx\\le\\sum_{n=N}^Mf(n)\\le f(N)+\\int_N^M f(x)\\,dx.</math>\n\nLetting {{math|''M''}} tend to infinity, the bounds in ({{EquationNote|1}}) and the result follow.\n\n==Applications==\nThe [[harmonic series (mathematics)|harmonic series]]\n:<math>\n\\sum_{n=1}^\\infty \\frac1n\n</math>\ndiverges because, using the [[natural logarithm]], its [[antiderivative]], and the [[fundamental theorem of calculus]], we get\n:<math>\n\\int_1^M\\frac1n\\,dn=\\ln n\\Bigr|_1^M=\\ln M\\to\\infty\n\\quad\\text{for }M\\to\\infty.\n</math>\nContrary, the series\n:<math>\n\\zeta(1+\\varepsilon)=\\sum_{x=1}^\\infty \\frac1{x^{1+\\varepsilon}}\n</math>\n(cf. [[Riemann zeta function]])\nconverges for every {{math|''ε'' > 0}}, because by the [[power rule]]\n:<math>\n\\int_1^M\\frac1{x^{1+\\varepsilon}}\\,dx\n=-\\frac1{\\varepsilon x^\\varepsilon}\\biggr|_1^M=\n\\frac1\\varepsilon\\Bigl(1-\\frac1{M^\\varepsilon}\\Bigr)\n\\le\\frac1\\varepsilon<\\infty\n\\quad\\text{for all }M\\ge1.\n</math>\nFrom ({{EquationNote|1}}) we get the upper estimate\n:<math>\n\\zeta(1+\\varepsilon)=\\sum_{x=1}^\\infty \\frac1{x^{1+\\varepsilon}}\\le\\frac{1+\\varepsilon}\\varepsilon,\n</math>\nwhich can be compared with some of the [[particular values of Riemann zeta function]].\n\n==Borderline between divergence and convergence==\nThe above examples involving the harmonic series raise the question, whether there are monotone sequences such that {{math|''f''(''n'')}} decreases to 0 faster than {{math|1/''n''}} but slower than {{math|1/''n''<sup>1+''ε''</sup>}} in the sense that\n:<math>\n\\lim_{n\\to\\infty}\\frac{f(n)}{1/n}=0\n\\quad\\text{and}\\quad\n\\lim_{n\\to\\infty}\\frac{f(n)}{1/n^{1+\\varepsilon}}=\\infty\n</math>\nfor every {{math|''ε'' > 0}}, and whether the corresponding series of the {{math|''f''(''n'')}} still diverges. Once such a sequence is found, a similar question can be asked with {{math|''f''(''n'')}} taking the role of {{math|1/''n''}}, and so on. In this way it is possible to investigate the borderline between divergence and convergence of infinite series.\n\nUsing the integral test for convergence, one can show (see below) that, for every [[natural number]] {{math|''k''}}, the series\n{{NumBlk|:|<math>\n\\sum_{n=N_k}^\\infty\\frac1{n\\ln(n)\\ln_2(n)\\cdots \\ln_{k-1}(n)\\ln_k(n)}\n</math>|{{EquationRef|4}}}}\nstill diverges (cf. [[proof that the sum of the reciprocals of the primes diverges]] for {{math|''k'' {{=}} 1}}) but\n{{NumBlk|:|<math>\n\\sum_{n=N_k}^\\infty\\frac1{n\\ln(n)\\ln_2(n)\\cdots\\ln_{k-1}(n)(\\ln_k(n))^{1+\\varepsilon}}\n</math>|{{EquationRef|5}}}}\nconverges for every {{math|''ε'' > 0}}. Here {{math|ln<sub>''k''</sub>}} denotes the {{math|''k''}}-fold [[function composition|composition]] of the natural logarithm defined [[recursion|recursively]] by\n:<math>\n\\ln_k(x)=\n\\begin{cases}\n\\ln(x)&\\text{for }k=1,\\\\\n\\ln(\\ln_{k-1}(x))&\\text{for }k\\ge2.\n\\end{cases}\n</math>\nFurthermore, {{math|''N''<sub>''k''</sub>}} denotes the smallest natural number such that the {{math|''k''}}-fold composition is well-defined and {{math|ln<sub>''k''</sub>(''N''<sub>''k''</sub>) ≥ 1}}, i.e.\n:<math>\nN_k\\ge \\underbrace{e^{e^{\\cdot^{\\cdot^{e}}}}}_{k\\ e'\\text{s}}=e \\uparrow\\uparrow k\n</math>\nusing [[tetration]] or [[Knuth's up-arrow notation]].\n\nTo see the divergence of the series ({{EquationNote|4}}) using the integral test, note that by repeated application of the [[chain rule]]\n:<math>\n\\frac{d}{dx}\\ln_{k+1}(x)\n=\\frac{d}{dx}\\ln(\\ln_k(x))\n=\\frac1{\\ln_k(x)}\\frac{d}{dx}\\ln_k(x)\n=\\cdots\n=\\frac1{x\\ln(x)\\cdots\\ln_k(x)},\n</math>\nhence\n:<math>\n\\int_{N_k}^\\infty\\frac{dx}{x\\ln(x)\\cdots\\ln_k(x)}\n=\\ln_{k+1}(x)\\bigr|_{N_k}^\\infty=\\infty.\n</math>\nTo see the convergence of the series ({{EquationNote|5}}), note that by the [[power rule]], the chain rule and the above result\n:<math>\n-\\frac{d}{dx}\\frac1{\\varepsilon(\\ln_k(x))^\\varepsilon}\n=\\frac1{(\\ln_k(x))^{1+\\varepsilon}}\\frac{d}{dx}\\ln_k(x)\n=\\cdots\n=\\frac{1}{x\\ln(x)\\cdots\\ln_{k-1}(x)(\\ln_k(x))^{1+\\varepsilon}},\n</math>\nhence\n:<math>\n\\int_{N_k}^\\infty\\frac{dx}{x\\ln(x)\\cdots\\ln_{k-1}(x)(\\ln_k(x))^{1+\\varepsilon}}\n=-\\frac1{\\varepsilon(\\ln_k(x))^\\varepsilon}\\biggr|_{N_k}^\\infty<\\infty\n</math>\nand ({{EquationNote|1}}) gives bounds for the infinite series in ({{EquationNote|5}}).\n\n==See also==\n*[[Convergence tests]]\n*[[Convergence (mathematics)]]\n*[[Direct comparison test]]\n*[[Dominated convergence theorem]]\n*[[Euler-Maclaurin formula]]\n*[[Limit comparison test]]\n*[[Monotone convergence theorem]]\n\n==References==\n* [[Konrad Knopp|Knopp, Konrad]], \"Infinite Sequences and Series\", [[Dover Publications]], Inc.,  New York, 1956. (§ 3.3) {{ISBN|0-486-60153-6}}\n* [[Whittaker and Watson|Whittaker, E. T., and Watson, G. N., ''A Course in Modern Analysis'']], fourth edition, Cambridge University Press, 1963. (§ 4.43) {{ISBN|0-521-58807-3}}\n* Ferreira, Jaime Campos, Ed Calouste Gulbenkian, 1987,  {{ISBN|972-31-0179-3}}\n\n<references/>\n\n{{DEFAULTSORT:Integral Test For Convergence}}\n[[Category:Integral calculus]]\n[[Category:Convergence tests]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Integration by parts",
      "url": "https://en.wikipedia.org/wiki/Integration_by_parts",
      "text": "{{Calculus |Integral}}\n\nIn [[calculus]], and more generally in [[mathematical analysis]], '''integration by parts''' or '''partial integration''' is a process that finds the [[integral (mathematics)|integral]] of a [[product (mathematics)|product]] of functions in terms of the integral of their derivative and antiderivative. It is frequently used to transform the antiderivative of a product of functions into an antiderivative for which a solution can be more easily found. The rule can be readily derived by integrating the [[product rule]] of [[derivative|differentiation]].\n\nIf {{math|1=''u'' = ''u''(''x'')}} and {{math|1=''du'' = ''{{prime|u}}''(''x'') ''dx''}}, while {{math|1=''v'' = ''v''(''x'')}} and {{math|1=''dv'' = ''{{prime|v}}''(''x'') ''dx''}}, then integration by parts states that:\n\n:<math>\\begin{align}\n   \\int_a^b u(x) v'(x) \\, dx \n   &= \\Big[u(x) v(x)\\Big]_a^b - \\int_a^b u'(x) v(x) \\, dx\\\\\n   &= u(b) v(b) - u(a) v(a) - \\int_a^b u'(x) v(x) \\, dx \n \\end{align}</math>\n\nor more compactly:\n\n:<math>\\int u \\, dv = uv - \\int v \\, du.</math>\n\nMathematician [[Brook Taylor]] discovered integration by parts, first publishing the idea in [[1715 in science|1715]].<ref name=\"Brook Taylor biography, St. Andrews\">{{cite web |url=http://www-history.mcs.st-andrews.ac.uk/Biographies/Taylor.html |title=Brook Taylor |work=History.MCS.St-Andrews.ac.uk |accessdate= May 25, 2018}}</ref><ref name=\"Brook Taylor biography, Stetson\">{{cite web |url=https://www2.stetson.edu/~efriedma/periodictable/html/Tl.html |title=Brook Taylor |work=Stetson.edu |accessdate= May 25, 2018}}</ref> More general formulations of integration by parts exist for the [[Riemann–Stieltjes integral#Properties and relation to the Riemann integral|Riemann–Stieltjes]] and [[Lebesgue–Stieltjes integral#Integration by parts|Lebesgue–Stieltjes integrals]]. The discrete analogue for sequences is called [[summation by parts]].\n\n==Theorem==\n\n===Product of two functions===\nThe theorem can be derived as follows. Suppose ''u''(''x'') and ''v''(''x'') are two [[continuously differentiable]] [[function (mathematics)|functions]]. The [[product rule]] states (in [[Leibniz's notation]]):\n\n:<math>\\frac{d}{dx}\\Big(u(x)v(x)\\Big) = v(x) \\frac{d}{dx}\\left(u(x)\\right) + u(x) \\frac{d}{dx}\\left(v(x)\\right).</math>\n\nIntegrating both sides with respect to ''x'' (and using also [[Notation for differentiation#Lagrange's notation|Lagrange's notation]]),\n\n:<math>\\int \\frac{d}{dx}\\left(u(x)v(x)\\right)\\,dx = \\int u'(x)v(x)\\,dx + \\int u(x)v'(x) \\,dx, </math>\n\nthen applying the definition of [[indefinite integral]],\n\n:<math>u(x)v(x) = \\int u'(x)v(x)\\,dx + \\int u(x)v'(x)\\,dx,</math>\n\nyields the formula for '''integration by parts''':\n\n:<math>\\int u(x)v'(x)\\,dx = u(x)v(x) - \\int u'(x)v(x) \\,dx. </math>\n\nTaking ''du'' and ''dv'' as [[differentials of a function|differentials of functions]] of one variable ''x'',\n\n:<math>du=u'(x)\\,dx \\quad dv=v'(x)\\,dx, \\quad</math> and\n\n:<math>\\int u(x)\\,dv = u(x)v(x) - \\int v(x)\\,du.</math>\n\nThe original integral ∫''uv''′&nbsp;''dx'' contains ''v''′ ([[derivative]] of ''v''); in order to apply the theorem, ''v'' ([[antiderivative]] of ''v''′) must be found, and then the resulting integral ∫''vu''′&nbsp;''dx'' must be evaluated.\n\n===Extension to other cases===\nIt is not necessary for ''u'' and ''v'' to be continuously differentiable. Integration by parts works if ''u'' is [[absolutely continuous]] and the function designated ''v''′ is [[Lebesgue integrable]] (but not necessarily continuous).<ref>{{cite web|title=Integration by parts|url=https://www.encyclopediaofmath.org/index.php/Integration_by_parts|website=Encyclopedia of Mathematics}}</ref> (If ''v''′ has a point of discontinuity then its antiderivative ''v'' may not have a derivative at that point.)\n\nIf the interval of integration is not [[compact space|compact]], then it is not necessary for ''u'' to be absolutely continuous in the whole interval or for ''v''′ to be Lebesgue integrable in the interval, as a couple of examples (in which ''u'' and ''v'' are continuous and continuously differentiable) will show. For instance, if\n\n:<math>u(x)=\\exp(x)/x^2,\\,v'(x)=\\exp(-x)</math>\n\n''u'' is not absolutely continuous on the interval {{closed-open|1, +∞}}, but nevertheless\n\n:<math>\\int_1^\\infty u(x)v'(x)\\,dx = \\Big[u(x)v(x)\\Big]_1^\\infty - \\int_1^\\infty u'(x)v(x)\\,dx</math>\n\nso long as <math>\\left[u(x)v(x)\\right]_1^\\infty</math> is taken to mean the limit of <math>u(L)v(L)-u(1)v(1)</math> as <math>L\\to\\infty</math> and so long as the two terms on the right-hand side are finite. This is only true if we choose <math>v(x)=-\\exp(-x).</math> Similarly, if\n\n:<math>u(x)=\\exp(-x),\\,v'(x)=x^{-1}\\sin(x)</math>\n\n''v''′ is not Lebesgue integrable on the interval {{closed-open|1, +∞}}, but nevertheless\n\n:<math>\\int_1^\\infty u(x)v'(x)\\,dx = \\Big[u(x)v(x)\\Big]_1^\\infty - \\int_1^\\infty u'(x)v(x)\\,dx</math>\nwith the same interpretation.\n\nOne can also easily come up with similar examples in which ''u'' and ''v'' are ''not'' continuously differentiable.\n\nFurther, if <math>f(x)</math> is a function of bounded variation on the segment <math>[a,b],</math> and <math>\\varphi(x)</math> is differentiable on <math>[a,b],</math> then\n\n: <math>\\int\\limits_{a}^{b}f(x)\\varphi'(x)\\,dx=-\\int\\limits_{-\\infty}^{+\\infty} \\widetilde\\varphi(x)\\,d(\\widetilde\\chi_{[a,b]}(x)\\widetilde f(x)),</math>\n\nwhere by <math>d(\\chi_{[a,b]}(x)\\widetilde f(x))</math> I denoted the signed measure corresponding to the function of bounded variation <math>\\chi_{[a,b]}(x)f(x)</math>, and functions <math>\\widetilde f, \\widetilde \\varphi</math> are extensions of <math>f, \\varphi</math>\nto <math>\\mathbb{R},</math> which are respectively of bounded variation and differentiable.\n\n===Product of many functions===\n\nIntegrating the product rule for three multiplied functions, ''u''(''x''), ''v''(''x''), ''w''(''x''), gives a similar result:\n\n:<math>\\int_a^b u v \\, dw = \\Big[u v w\\Big]^b_a - \\int_a^b u w \\, dv - \\int_a^b v w \\, du.</math>\n\nIn general, for ''n'' factors\n\n:<math>\\frac{d}{dx} \\left(\\prod_{i=1}^n u_i(x) \\right)= \\sum_{j=1}^n \\prod_{i\\neq j}^n u_i(x) \\frac{du_j(x)}{dx}, </math>\n\nwhich leads to\n\n:<math> \\left[ \\prod_{i=1}^n u_i(x) \\right]_a^b = \\sum_{j=1}^n \\int_a^b \\prod_{i\\neq j}^n u_i(x) \\, du_j(x), </math>\n\nwhere the [[product (mathematics)|product]] is of all functions except for the one differentiated in the same term.\n\n==Visualization==\n[[Image:Integration by parts v2.svg|thumb|280px |Graphical interpretation of the theorem. The pictured curve is parametrized by the variable t.]]\nConsider a parametric curve by (''x'', ''y'') = (''f''(''t''), ''g''(''t'')). Assuming that the curve is locally [[Injective function|one-to-one]] and [[Locally integrable function|integrable]], we can define\n:<math>x(y) = f(g^{-1}(y))</math>\n:<math>y(x) = g(f^{-1}(x))</math>\n\nThe area of the blue region is\n\n:<math>A_1=\\int_{y_1}^{y_2}x(y) \\, dy</math>\n\nSimilarly, the area of the red region is\n:<math>A_2=\\int_{x_1}^{x_2}y(x)\\,dx</math>\n\nThe total area ''A''<sub>1</sub> + ''A''<sub>2</sub> is equal to the area of the bigger rectangle, ''x''<sub>2</sub>''y''<sub>2</sub>, minus the area of the smaller one, ''x''<sub>1</sub>''y''<sub>1</sub>:\n\n:<math>\\overbrace{\\int_{y_1}^{y_2}x(y) \\, dy}^{A_1}+\\overbrace{\\int_{x_1}^{x_2}y(x) \\, dx}^{A_2}=\\biggl.x \\cdot y(x)\\biggl|_{x_1}^{x_2} = \\biggl.y \\cdot x(y)\\biggl|_{y_1}^{y_2}.</math>\nOr, in terms of ''t'',\n:<math>\\int_{t_1}^{t_2}x(t) \\, dy(t) + \\int_{t_1}^{t_2}y(t) \\, dx(t) = \\biggl. x(t)y(t) \\biggl|_{t_1}^{t_2}</math>\nOr, in terms of indefinite integrals, this can be written as\n:<math>\\int x\\,dy + \\int y \\,dx = xy</math>\nRearranging:\n:<math>\\int x\\,dy = xy - \\int y \\,dx</math>\nThus integration by parts may be thought of as deriving the area of the blue region from the area of rectangles and that of the red region.\n\nThis visualization also explains why integration by parts may help find the integral of an inverse function ''f''<sup>−1</sup>(''x'') when the integral of the function ''f''(''x'') is known. Indeed, the functions ''x''(''y'') and ''y''(''x'') are inverses, and the integral ∫''x''&nbsp;''dy'' may be calculated as above from knowing the integral ∫''y''&nbsp;''dx''. In particular, this explains use of integration by parts to integrate [[logarithm]] and [[inverse trigonometric function]]s.\n\n==Applications==\n\n===Strategy===\nIntegration by parts is a [[heuristic]] rather than a purely mechanical process for solving integrals; given a single function to integrate, the typical strategy is to carefully separate this single function into a product of two functions ''u''(''x'')''v''(''x'') such that the residual integral from the integration by parts formula is easier to evaluate than the single function. The following form is useful in illustrating the best strategy to take:\n\n:<math>\\int uv\\ dx = u \\int v\\ dx - \\int\\left(u' \\int v\\ dx \\right)\\ dx.</math>\n\nNote that on the right-hand side, ''u'' is differentiated and ''v'' is integrated; consequently it is useful to choose ''u'' as a function that simplifies when differentiated, or to choose ''v'' as a function that simplifies when integrated. As a simple example, consider:\n\n:<math>\\int\\frac{\\ln(x)}{x^2}\\ dx\\ .</math>\n\nSince the derivative of ln(''x'') is {{sfrac|1|''x''}}, one makes (ln(''x'')) part ''u''; since the antiderivative of {{sfrac|1|''x''<sup>2</sup>}} is −{{sfrac|1|''x''}}, one makes {{sfrac|1|''x''<sup>2</sup>}}&nbsp;''dx'' part ''dv''. The formula now yields:\n\n:<math>\\int\\frac{\\ln(x)}{x^2}\\ dx = -\\frac{\\ln(x)}{x} - \\int \\biggl(\\frac1{x}\\biggr) \\biggl(-\\frac1{x}\\biggr)\\ dx\\ .</math>\n\nThe antiderivative of −{{sfrac|1|''x''<sup>2</sup>}} can be found with the [[power rule]] and is {{sfrac|1|''x''}}.\n\nAlternatively, one may choose ''u'' and ''v'' such that the product ''u''′ (∫''v''&nbsp;''dx'') simplifies due to cancellation. For example, suppose one wishes to integrate:\n\n:<math>\\int\\sec^2(x)\\cdot\\ln\\Big(\\bigl|\\sin(x)\\bigr|\\Big)\\ dx.</math>\n\nIf we choose ''u''(''x'') = ln(|sin(''x'')|) and ''v''(''x'') = sec<sup>2</sup>x, then ''u'' differentiates to 1/ tan ''x'' using the [[chain rule]] and ''v'' integrates to tan ''x''; so the formula gives:\n\n:<math>\\int\\sec^2(x)\\cdot\\ln\\Big(\\bigl|\\sin(x)\\bigr|\\Big)\\ dx = \\tan(x)\\cdot\\ln\\Big(\\bigl|\\sin(x)\\bigr|\\Big)-\\int\\tan(x)\\cdot\\frac1{\\tan(x)} \\, dx\\ .</math>\n\nThe integrand simplifies to 1, so the antiderivative is ''x''. Finding a simplifying combination frequently involves experimentation.\n\nIn some applications, it may not be necessary to ensure that the integral produced by integration by parts has a simple form; for example, in [[numerical analysis]], it may suffice that it has small magnitude and so contributes only a small error term. Some other special techniques are demonstrated in the examples below.\n\n====Polynomials and trigonometric functions====\n\nIn order to calculate\n\n:<math>I=\\int x\\cos(x)\\ dx\\ ,</math>\n\nlet:\n:<math>u = x\\ \\Rightarrow\\ du = dx</math>\n:<math>dv = \\cos(x)\\ dx\\ \\Rightarrow\\ v = \\int\\cos(x)\\ dx = \\sin(x)</math>\n\nthen:\n\n:<math>\\begin{align}\n  \\int x\\cos(x)\\ dx & = \\int u\\ dv \\\\\n  & = u\\cdot v - \\int v \\, du \\\\\n  & = x\\sin(x) - \\int \\sin(x)\\ dx \\\\\n  & = x\\sin(x) + \\cos(x) + C,\n \\end{align}</math>\n\nwhere ''C'' is a [[constant of integration]].\n\nFor higher powers of ''x'' in the form\n\n:<math>\\int x^n e^x\\ dx,\\ \\int x^n\\sin(x)\\ dx,\\ \\int x^n\\cos(x)\\ dx\\ ,</math>\n\nrepeatedly using integration by parts can evaluate integrals such as these; each application of the theorem lowers the power of ''x'' by one.\n\n====Exponentials and trigonometric functions====\n\nAn example commonly used to examine the workings of integration by parts is\n\n:<math>I=\\int e^x\\cos(x)\\ dx.</math>\n\nHere, integration by parts is performed twice. First let\n\n:<math>u = \\cos(x)\\ \\Rightarrow\\ du = -\\sin(x)\\ dx</math>\n:<math>dv = e^x\\ dx\\ \\Rightarrow\\ v = \\int e^x\\ dx = e^x</math>\n\nthen:\n\n:<math>\\int e^x\\cos(x)\\ dx = e^x\\cos(x) + \\int e^x\\sin(x)\\ dx.</math>\n\nNow, to evaluate the remaining integral, we use integration by parts again, with:\n\n:<math>u = \\sin(x)\\ \\Rightarrow\\ du = \\cos(x)\\ dx</math>\n:<math>dv = e^x\\ dx\\ \\Rightarrow\\ v = \\int e^x\\ dx = e^x.</math>\n\nThen:\n\n:<math>\\int e^x\\sin(x)\\ dx = e^x\\sin(x) - \\int e^x\\cos(x)\\ dx.</math>\n\nPutting these together,\n\n:<math>\\int e^x\\cos(x)\\ dx = e^x\\cos(x) + e^x\\sin(x) - \\int e^x\\cos(x)\\ dx.</math>\n\nThe same integral shows up on both sides of this equation. The integral can simply be added to both sides to get\n\n:<math>2\\int e^x\\cos(x)\\ dx = e^x\\bigl(\\sin(x)+\\cos(x)\\bigr) + C</math>\n\nwhich rearranges to:\n\n:<math>\\int e^x\\cos(x)\\ dx = \\frac{e^x\\bigl(\\sin(x)+\\cos(x)\\bigr)}{2} + C'</math>\n\nwhere again ''C'' (and ''C''′ = ''C''/2) is a [[constant of integration]].\n\nA similar method is used to find the [[integral of secant cubed]].\n\n====Functions multiplied by unity====\n\nTwo other well-known examples are when integration by parts is applied to a function expressed as a product of 1 and itself. This works if the derivative of the function is known, and the integral of this derivative times ''x'' is also known.\n\nThe first example is ∫ ln(''x'') d''x''. We write this as:\n\n:<math>I=\\int\\ln(x)\\cdot 1\\ dx\\ .</math>\n\nLet:\n\n:<math>u = \\ln(x)\\ \\Rightarrow\\ du = \\frac{dx}{x}</math>\n:<math>dv = dx\\ \\Rightarrow\\ v = x</math>\n\nthen:\n\n:<math>\n\\begin{align}\n\\int \\ln(x)\\ dx & = x\\ln(x) - \\int\\frac{x}{x}\\ dx \\\\\n& = x\\ln(x) - \\int 1\\ dx \\\\\n& = x\\ln(x) - x + C\n\\end{align}\n</math>\n\nwhere ''C'' is the [[constant of integration]].\n\nThe second example is the [[inverse tangent]] function arctan(''x''):\n\n:<math>I=\\int\\arctan(x)\\ dx.</math>\n\nRewrite this as\n\n:<math>\\int\\arctan(x)\\cdot 1\\ dx.</math>\n\nNow let:\n\n:<math>u = \\arctan(x)\\ \\Rightarrow\\ du = \\frac{dx}{1+x^2}</math>\n\n:<math>dv = dx\\ \\Rightarrow\\ v = x</math>\n\nthen\n\n:<math>\n\\begin{align}\n\\int\\arctan(x)\\ dx\n& = x\\arctan(x) - \\int\\frac{x}{1+x^2}\\ dx \\\\[8pt]\n& = x\\arctan(x) - \\frac{\\ln(1+x^2)}{2} + C\n\\end{align}\n</math>\n\nusing a combination of the [[inverse chain rule method]] and the [[natural logarithm integral condition]].\n\n===LIATE rule===\nA [[rule of thumb]] proposed by [[Herbert Kasube]] advises that whichever function comes first in the following list should be chosen as ''u'':<ref>{{Cite journal |jstor=2975556 |first=Herbert E. |last=Kasube |title=A Technique for Integration by Parts |journal=[[The American Mathematical Monthly]] |volume=90 |issue=3 |year=1983 |pages=210–211 |doi=10.2307/2975556}}</ref>\n:'''L''' – [[logarithmic function]]s: <math>\\ln(x),\\ \\log_b(x),</math> etc.\n:'''I''' – [[inverse trigonometric function]]s:  <math>\\arctan(x),\\ \\arcsec(x),</math> etc.\n:'''A''' – [[Polynomial|algebraic functions]]: <math>x^2,\\ 3x^{50},</math> etc.\n:'''T''' – [[trigonometric functions]]: <math>\\sin(x),\\ \\tan(x),</math> etc.\n:'''E''' – [[exponential function]]s: <math>e^x,\\ 19^x,</math> etc.\n\nThe function which is to be ''dv'' is whichever comes last in the list: functions lower on the list have easier [[antiderivative]]s than the functions above them. The rule is sometimes written as \"DETAIL\" where ''D'' stands for ''dv''.\n\nTo demonstrate the LIATE rule, consider the integral\n\n:<math>\\int x \\cdot \\cos(x) \\,dx.</math>\n\nFollowing the LIATE rule, ''u'' = ''x'', and ''dv'' = cos(''x'') ''dx'', hence ''du'' = ''dx'', and ''v'' = sin(''x''), which makes the integral become\n:<math>x \\cdot \\sin(x) - \\int 1 \\sin(x) \\,dx,</math>\nwhich equals\n:<math>x \\cdot \\sin(x) + \\cos(x) + C.</math>\n\nIn general, one tries to choose ''u'' and ''dv'' such that ''du'' is simpler than ''u'' and ''dv'' is easy to integrate. If instead cos(''x'') was chosen as ''u'', and ''x dx'' as ''dv'', we would have the integral\n\n:<math>\\frac{x^2}2 \\cos(x) + \\int \\frac{x^2}2 \\sin(x) \\,dx,</math>\n\nwhich, after recursive application of the integration by parts formula, would clearly result in an infinite recursion and lead nowhere.\n\nAlthough a useful rule of thumb, there are exceptions to the LIATE rule. A common alternative is to consider the rules in the \"ILATE\" order instead. Also, in some cases, polynomial terms need to be split in non-trivial ways. For example, to integrate\n\n:<math>\\int x^3 e^{x^2} \\,dx,</math>\n\none would set\n\n:<math>u = x^2, \\quad dv = x \\cdot e^{x^2} \\,dx,</math>\n\nso that\n\n:<math>du = 2x \\,dx, \\quad v = \\frac{e^{x^2}}{2}.</math>\n\nThen\n\n:<math>\\int x^3 e^{x^2} \\,dx = \\int (x^2) \\left(xe^{x^2}\\right) \\,dx = \\int u \\,dv\n= uv - \\int v \\,du = \\frac{x^2 e^{x^2}}{2} - \\int x e^{x^2} \\,dx.</math>\n\nFinally, this results in\n:<math>\\int x^3 e^{x^2} \\,dx = \\frac{e^{x^2}(x^2 - 1)}{2} + C.</math>\n\nIntegration by parts is often used as a tool to prove theorems in [[mathematical analysis]].\n\n===Use in special functions===\n\nThe [[gamma function]] is an example of a [[special function]], defined as an [[improper integral]] for {{math|''z'' &gt; 0}}. Integration by parts illustrates it to be an extension of the [[factorial]]:\n\n:<math>\\begin{align}\n\\Gamma(z) & = \\int_0^\\infty e^{-x} x^{z-1} \\,dx \\\\\n & = - \\int_0^\\infty x^{z-1} \\, d\\left(e^{-x}\\right) \\\\\n & = - \\left[e^{-x} x^{z-1}\\right]_0^\\infty + \\int_0^\\infty e^{-x} \\, d\\left(x^{z-1}\\right) \\\\\n & = 0 + \\int_0^\\infty \\left(z-1\\right) x^{z-2} e^{-x} \\, dx\\\\\n & = (z-1)\\Gamma(z-1).\n\\end{align} </math>\n\nSince\n\n:<math>\\Gamma(1) = \\int_0^\\infty e^{-x} \\, dx = 1,</math>\n\nfor integer {{mvar|z}}, applying this formula repeatedly gives the [[factorial]] (denoted by the !):\n\n:<math>\\Gamma(z+1) = z!</math>\n\n===Use in harmonic analysis===\n\nIntegration by parts is often used in [[harmonic analysis]], particularly [[Fourier analysis]], to show [[Riemann–Lebesgue lemma|that quickly oscillating integrals with sufficiently smooth integrands decay quickly]]. The most common example of this is its use in showing that the decay of function's Fourier transform depends on the smoothness of that function, as described below.\n\n====Fourier transform of derivative====\n\nIf ''f'' is a ''k''-times continuously differentiable function and all derivatives up to the ''k''th one decay to zero at infinity, then its [[Fourier transform]] satisfies\n\n:<math>(\\mathcal{F}f^{(k)})(\\xi) = (2\\pi i\\xi)^k \\mathcal{F}f(\\xi),</math>\n\nwhere {{nowrap|''f''<sup>(''k'')</sup>}} is the ''k''th derivative of ''f''. (The exact constant on the right depends on the [[Fourier transform#Other conventions|convention of the Fourier transform used]].) This is proved by noting that\n\n:<math>\\frac{d}{dy} e^{-2\\pi iy\\xi} = -2\\pi i\\xi e^{-2\\pi iy\\xi},</math>\n\nso using integration by parts on the Fourier transform of the derivative we get\n\n:<math>\\begin{align}\n(\\mathcal{F}f')(\\xi) &= \\int_{-\\infty}^\\infty e^{-2\\pi iy\\xi} f'(y)\\,dy \\\\\n&=\\left[e^{-2\\pi iy\\xi} f(y)\\right]_{-\\infty}^\\infty - \\int_{-\\infty}^\\infty (-2\\pi i\\xi e^{-2\\pi iy\\xi}) f(y)\\,dy \\\\[5pt]\n&=2\\pi i\\xi \\int_{-\\infty}^\\infty e^{-2\\pi iy\\xi} f(y)\\,dy \\\\[5pt]\n&=2\\pi i\\xi \\mathcal{F}f(\\xi).\n\\end{align}</math>\n\nApplying this [[Mathematical induction|inductively]] gives the result for general ''k''. A similar method can be used to find the [[Laplace transform]] of a derivative of a function.\n\n====Decay of Fourier transform====\n\nThe above result tells us about the decay of the Fourier transform, since it follows that if ''f'' and {{nowrap|''f''<sup>(''k'')</sup>}} are integrable then\n\n:<math>\\vert\\mathcal{F}f(\\xi)\\vert \\leq \\frac{I(f)}{1+\\vert 2\\pi\\xi\\vert^k}, \\text{ where } I(f) = \\int_{-\\infty}^\\infty \\Bigl(\\vert f(y)\\vert + \\vert f^{(k)}(y)\\vert\\Bigr) \\, dy.</math>\n\nIn other words, if ''f'' satisfies these conditions then its Fourier transform decays at infinity at least as quickly as {{nowrap|1/{{!}}''ξ''{{!}}<sup>''k''</sup>}}. In particular, if {{nowrap|''k'' ≥ 2}} then the Fourier transform is integrable.\n\nThe proof uses the fact, which is immediate from the [[Fourier transform#Definition|definition of the Fourier transform]], that\n\n:<math>\\vert\\mathcal{F}f(\\xi)\\vert \\leq \\int_{-\\infty}^\\infty \\vert f(y) \\vert \\,dy.</math>\n\nUsing the same idea on the equality stated at the start of this subsection gives\n\n:<math>\\vert(2\\pi i\\xi)^k \\mathcal{F}f(\\xi)\\vert \\leq \\int_{-\\infty}^\\infty \\vert f^{(k)}(y) \\vert \\,dy.</math>\n\nSumming these two inequalities and then dividing by {{nowrap|1 + {{!}}2{{pi}}''ξ''<sup>''k''</sup>{{!}}}} gives the stated inequality.\n\n===Use in operator theory===\n\nOne use of integration by parts in [[operator theory]] is that it shows that the {{nowrap|−∆}} (where ∆ is the [[Laplace operator]]) is a [[positive operator]] on {{nowrap|''L''<sup>2</sup>}} (see [[Lp space|''L''<sup>''p''</sup> space]]). If ''f'' is smooth and compactly supported then, using integration by parts, we have\n\n:<math>\\begin{align}\n\\langle -\\Delta f, f \\rangle_{L^2} &= -\\int_{-\\infty}^\\infty f''(x)\\overline{f(x)}\\,dx \\\\[5pt]\n&=-\\left[f'(x)\\overline{f(x)}\\right]_{-\\infty}^\\infty + \\int_{-\\infty}^\\infty f'(x)\\overline{f'(x)}\\,dx \\\\[5pt]\n&=\\int_{-\\infty}^\\infty \\vert f'(x)\\vert^2\\,dx \\geq 0.\n\\end{align}</math>\n\n===Other applications===\n<!---INCLUDING DERIVATIONS HERE WOULD BE TOO LENGTHLY, IDEALLY KEEP THIS AS A LIST--->\n* For determining [[boundary condition]]s in [[Sturm–Liouville theory]]\n* Deriving the [[Euler–Lagrange equation]] in the [[calculus of variations]]\n\n==Repeated integration by parts==\nConsidering a second derivative of <math>v</math> in the integral on the LHS of the formula for partial integration suggests a repeated application to the integral on the RHS:\n:<math>\\int u v''\\,dx = uv' - \\int u'v'\\,dx = uv' - \\left( u'v - \\int u''v\\,dx \\right).</math>\n\nExtending this concept of repeated partial integration to derivatives of degree {{mvar|n}} leads to\n:<math>\\begin{align}\n\\int u^{(0)} v^{(n)}\\,dx &= u^{(0)} v^{(n-1)} - u^{(1)}v^{(n-2)} + u^{(2)}v^{(n-3)} - \\cdots + (-1)^{n-1}u^{(n-1)} v^{(0)} + (-1)^n \\int u^{(n)} v^{(0)} \\,dx.\\\\[5pt]\n&= \\sum_{k=0}^{n-1}(-1)^k u^{(k)}v^{(n-1-k)} + (-1)^n \\int u^{(n)} v^{(0)} \\,dx.\n\\end{align}</math>\n\nThis concept may be useful when the successive integrals of <math>v^{(n)}</math> are readily available (e.g., plain exponentials or sine and cosine, as in  [[Laplace transform|Laplace]] or [[Fourier transform]]s), and when the {{mvar|n}}th derivative of <math>u</math> vanishes (e.g., as a polynomial function with degree <math>(n-1)</math>). The latter condition stops the repeating of partial integration, because the RHS-integral vanishes.\n\nIn the course of the above repetition of partial integrations the integrals \n:<math>\\int u^{(0)} v^{(n)}\\,dx \\quad</math> and <math>\\quad \\int u^{(\\ell)} v^{(n-\\ell)}\\,dx \\quad</math> and <math>\\quad \\int u^{(m)} v^{(n-m)}\\,dx \\quad\\text{ for } 1 \\le m,\\ell \\le n</math>\nget related. This may be interpreted as arbitrarily \"shifting\" derivatives between <math>v</math> and <math>u</math> within the integrand, and proves useful, too (see [[Rodrigues' formula]]).\n\n===Tabular integration by parts===\nThe essential process of the above formula can be summarized in a table; the resulting method is called \"tabular integration\"<ref>{{Cite book |first=G. B. |last=Thomas |authorlink=George B. Thomas |first2=R. L. |last2=Finney |title=Calculus and Analytic Geometry |publisher=Addison-Wesley |location=Reading, MA |edition=7th |year=1988 |isbn=0-201-17069-8 }}</ref> and was featured in the film ''[[Stand and Deliver]]''.<ref>{{Cite journal |url=https://www.maa.org/sites/default/files/pdf/mathdl/CMJ/Horowitz307-311.pdf |first=David |last=Horowitz |title=Tabular Integration by Parts |journal=[[The College Mathematics Journal]] |volume=21 |issue=4 |year=1990 |pages=307–311 |doi=10.2307/2686368 |jstor=2686368}}</ref>\n\nFor example, consider the integral\n\n:<math>\\int x^3 \\cos x \\,dx \\quad</math> and take <math>\\quad u^{(0)} = x^3, \\quad v^{(n)} = \\cos x.</math>\n\nBegin to list in column '''A''' the function <math>u^{(0)} = x^3</math> and its subsequent derivatives <math>u^{(i)}</math> until zero is reached. Then list in column '''B''' the function <math>v^{(n)} = \\cos x</math> and its subsequent integrals <math>v^{(n-i)}</math> until the size of column '''B''' is the same as that of column '''A'''. The result is as follows:\n\n:{| class=\"wikitable\" style=\"text-align:center\"\n!# ''i'' !! Sign !! A: derivatives ''u''<sup>(''i'')</sup>  !! B: integrals ''v''<sup>(''n''−''i'')</sup>\n|-\n| 0 || + || <math>x^3</math> || <math>\\cos x</math>\n|-\n| 1 || − || <math>3x^2</math> || <math>\\sin x</math>\n|-\n| 2 || + || <math>6x</math> || <math>-\\cos x</math>\n|-\n| 3 || − || <math>6</math> || <math>-\\sin x</math>\n|-\n| 4 || + || <math>0</math> || <math>\\cos x</math>\n|}\n\nThe product of the entries in {{nowrap|row {{mvar|i}}}} of columns '''A''' and '''B''' together with the respective sign give the relevant integrals in {{nowrap|step {{mvar|i}}}} in the course of repeated integration by parts. {{nowrap|Step {{math|''i'' {{=}} 0}}}} yields the original integral. For the complete result in {{nowrap|step {{math|''i'' > 0}}}} the {{nowrap|{{mvar|i}}th integral}} must be added to all the previous products ({{math|0 ≤ ''j'' < ''i''}}) of the {{nowrap|{{mvar|j}}th entry}} of column A and the {{nowrap|{{math|(''j'' + 1)}}st entry}} of column B (i.e., multiply the 1st entry of column A with the 2nd entry of column B, the 2nd entry of column A with the 3rd entry of column B, etc. ...) with the given {{nowrap|{{mvar|j}}th sign.}} This process comes to a natural halt, when the product, which yields the integral, is zero ({{math|''i'' {{=}} 4}} in the example).  The complete result is the following (notice the alternating signs in each term):\n\n:<math>\\underbrace{(+1)(x^3)(\\sin x)}_{j=0} + \\underbrace{(-1)(3x^2)(-\\cos x)}_{j=1} + \\underbrace{(+1)(6x)(-\\sin x)}_{j=2} +\\underbrace{(-1)(6)(\\cos x)}_{j=3}+ \\underbrace{\\int(+1)(0)(\\cos x) \\,dx}_{i=4: \\;\\to \\;C}.</math>\n\nThis yields\n\n:<math>\\underbrace{\\int x^3 \\cos x \\,dx}_{\\text{step 0}} = x^3\\sin x + 3x^2\\cos x - 6x\\sin x - 6\\cos x + C. </math>\n\nThe repeated partial integration also turns out useful, when in the course of respectively differentiating and integrating the functions <math>u^{(i)}</math> and  <math>v^{(n-i)}</math>  their product results in a multiple of the original integrand. In this case the repetition may also be terminated with this index {{mvar|i.}}This can happen, expectably, with exponentials and trigonometric functions. As an example consider\n\n:<math>\\int e^x \\cos x \\,dx. </math>\n\n:{| class=\"wikitable\" style=\"text-align:center\"\n!# ''i'' !! Sign !! A: derivatives ''u''<sup>(''i'')</sup> !! B: integrals ''v''<sup>(''n''−''i'')</sup>\n|-\n| 0 || + || <math>e^x</math> || <math>\\cos x</math>\n|-\n| 1 || − || <math>e^x</math> || <math>\\sin x</math>\n|-\n| 2 || + || <math>e^x</math> || <math>-\\cos x</math>\n|}\n\nIn this case the product of the terms in columns '''A''' and '''B''' with the appropriate sign for index {{math|''i'' {{=}} 2}} yields the negative of the original integrand (compare {{nowrap|rows {{math|''i'' {{=}} 0}}}} {{nowrap|and {{math|''i'' {{=}} 2}}).}}\n\n:<math> \\underbrace{\\int e^x \\cos x \\,dx}_{\\text{step 0}} = \\underbrace{(+1)(e^x)(\\sin x)}_{j=0} + \\underbrace{(-1)(e^x)(-\\cos x)}_{j=1} + \\underbrace{\\int(+1)(e^x)(-\\cos x) \\,dx}_{i= 2}. </math>\n\nObserving that the integral on the RHS can have its own constant of integration <math>C'</math>, and bringing the abstract integral to the other side, gives\n\n:<math> 2 \\int e^x \\cos x \\,dx = e^x\\sin x + e^x\\cos x + C', </math>\n\nand finally:\n\n:<math>\\int e^x \\cos x \\,dx = \\frac 12 \\left(e^x ( \\sin x + \\cos x ) \\right) + C,</math>\n\nwhere ''C'' = ''C''′/2.\n\n==Higher dimensions==\nThe formula for integration by parts can be extended to functions of several variables. These derivations are analogous to the one given above: a fundamental theorem of calculus is substituted into an appropriate product rule. There are several such pairings possible in multivariate calculus.<ref>{{Cite web|url=http://www.math.nagoya-u.ac.jp/~richard/teaching/s2016/Ref2.pdf|title=The Calculus of Several Variables|last=Rogers|first=Robert C.|date=September 29, 2011|website=|archive-url=|archive-date=|dead-url=}}</ref> For example, we may begin with a product rule for divergence followed by the divergence theorem.\n\nA [[Vector calculus identities#Properties|product rule for divergence]]:\n\n:<math>\\nabla \\cdot ( \\varphi \\vec v ) = \\varphi\\ ( \\nabla \\cdot \\vec v) \\ +\\  \\vec v\\cdot (\\nabla \\varphi)</math>\n\nInstead of an interval we integrate over an ''n''-dimensional domain <math>\n \\Omega\n</math>:\n\n:<math>\n\\int_\\Omega \\varphi\\, \\operatorname{div} \\vec v \\, dV = \\int_\\Omega \\nabla\\cdot (\\varphi\\, \\vec v) \\, dV - \\int_\\Omega \\vec v\\cdot \\operatorname{grad} \\varphi \\, dV\n</math>\n\nAfter substitution using the [[divergence theorem]] we arrive at:\n\n:<math>\n\\int_\\Omega \\varphi \\operatorname{div} \\vec v \\, dV = \\int_{\\partial \\Omega} \\varphi\\, \\vec v \\cdot d \\vec S - \\int_\\Omega \\vec v\\cdot \\operatorname{grad} \\varphi \\, dV.\n</math>\n\nMore specifically, suppose Ω is an [[Open set|open]] [[bounded set|bounded subset]] of ℝ<sup>''n''</sup> with a [[piecewise smooth]] [[boundary (topology)|boundary]] Γ. If ''u'' and ''v'' are two [[smooth function|continuously differentiable]] functions on the [[Closure (topology)|closure]] of Ω and we let <math>\\varphi =v</math> and <math>\\vec{v} = u\\vec{e}_i</math> (where <math>\\vec{e}_i</math>is the ''i''-th standard basis vector), the formula for integration by parts is\n\n:<math>\\int_\\Omega \\frac{\\partial u}{\\partial x_i} v \\,d\\Omega = \\int_{\\Gamma} u v \\, \\hat\\nu_i \\,d\\Gamma - \\int_\\Omega u \\frac{\\partial v}{\\partial x_i} \\, d\\Omega.</math>\n\nHere <math>\\hat{\\mathbf{\\nu}}</math> is the outward unit [[surface normal]] to Γ, <math>\\hat\\nu_i</math> is its ''i''-th component, and ''i'' ranges from 1 to ''n''. In vector form, the equation reads\n:<math>\\int_\\Omega v \\nabla u \\,d\\Omega = \\int_{\\Gamma} u v \\, \\mathbf{\\hat\\nu} \\,d\\Gamma - \\int_\\Omega u \\nabla v \\, d\\Omega,</math>\n\nReplacing ''v'' in the component formula with ''v''<sub>''i''</sub> and summing over ''i'' gives the vector formula\n\n:<math> \\int_\\Omega \\nabla u \\cdot \\mathbf{v}\\, d\\Omega = \\int_\\Gamma u (\\mathbf{v}\\cdot \\hat{\\nu})\\, d\\Gamma - \\int_\\Omega u\\, \\nabla\\cdot\\mathbf{v}\\, d\\Omega,</math>\n\nwhere '''v''' is a vector-valued function with components ''v''<sub>1</sub>, ..., ''v''<sub>''n.''</sub>\n\nFor <math>\\mathbf{v}=\\nabla v</math> where <math>v\\in C^2(\\bar{\\Omega})</math>, one gets\n\n:<math> \\int_\\Omega \\nabla u \\cdot \\nabla v\\, d\\Omega = \\int_\\Gamma u\\, \\nabla v\\cdot\\hat{\\nu}\\, d\\Gamma - \\int_\\Omega u\\, \\nabla^2 v\\, d\\Omega,</math>\nwhich is the [[Green's identities|first Green's identity]].\n\nThe [[Differentiability class|regularity]] requirements of the theorem can be relaxed. For instance, the boundary Γ need only be [[Lipschitz continuous]]. In the first formula above, only ''u'', ''v'' ∈ ''H''<sup>1</sup>(Ω) is necessary (where ''H''<sup>1</sup> is a [[Sobolev space]]); the other formulas have similarly relaxed requirements.\n\n==See also==\n* [[Lebesgue–Stieltjes integral#Integration by parts|Integration by parts for the Lebesgue–Stieltjes integral]]\n* [[Quadratic variation#Semimartingales|Integration by parts]] for [[semimartingale]]s, involving their quadratic covariation.\n* [[Integration by substitution]]\n* [[Legendre transformation]]\n\n==Notes==\n<references />\n\n==Further reading==\n*{{cite book |last=Hoffmann |first=Laurence D. |last2=Bradley |first2=Gerald L. |title=Calculus for Business, Economics, and the Social and Life Sciences |location= |publisher= |year=2004 |edition=8th |pages=450–464 |isbn=0-07-242432-X }}\n*{{cite book |first=Stephen |last=Willard |title=Calculus and its Applications |location=Boston |publisher=Prindle, Weber & Schmidt |year=1976 |isbn=0-87150-203-8 |pages=193–214 }}\n*{{cite book |first=Allyn J. |last=Washington |title=Technical Calculus with Analytic Geometry |location=Reading |publisher=Addison-Wesley |year=1966 |isbn=0-8465-8603-7 |pages=218–245 }}\n\n==External links==\n{{wikibooks|Calculus|Integration techniques/Integration by Parts|Integration by parts}}\n* {{springer|title=Integration by parts|id=p/i051730}}\n* [http://mathworld.wolfram.com/IntegrationbyParts.html Integration by parts—from MathWorld]\n\n[[Category:Integral calculus]]\n\n[[es:Métodos de integración#Método de integración por partes]]"
    },
    {
      "title": "Integration by parts operator",
      "url": "https://en.wikipedia.org/wiki/Integration_by_parts_operator",
      "text": "{{no footnotes|date=November 2014}}\nIn [[mathematics]], an '''integration by parts operator''' is a [[linear operator]] used to formulate [[integration by parts]] formulae; the most interesting examples of integration by parts operators occur in infinite-dimensional settings and find uses in [[stochastic analysis]] and its applications.\n\n==Definition==\n\nLet ''E'' be a [[Banach space]] such that both ''E'' and its [[continuous dual space]] ''E''<sup>∗</sup> are [[separable space]]s; let ''&mu;'' be a [[Borel measure]] on ''E''.  Let ''S'' be any (fixed) [[subset]] of the class of functions defined on ''E''.  A linear operator ''A''&nbsp;:&nbsp;''S''&nbsp;→&nbsp;''L''<sup>2</sup>(''E'',&nbsp;''&mu;'';&nbsp;'''R''') is said to be an '''integration by parts operator''' for ''&mu;'' if\n\n:<math>\\int_{E} \\mathrm{D} \\varphi(x) h(x) \\, \\mathrm{d} \\mu(x) = \\int_{E} \\varphi(x) (A h)(x) \\, \\mathrm{d} \\mu(x)</math>\n\nfor every [[smooth function|''C''<sup>1</sup> function]] ''&phi;''&nbsp;:&nbsp;''E''&nbsp;→&nbsp;'''R''' and all ''h''&nbsp;∈&nbsp;''S'' for which either side of the above equality makes sense.  In the above, D''&phi;''(''x'') denotes the [[Fréchet derivative]] of ''&phi;'' at ''x''.\n\n==Examples==\n\n* Consider an [[abstract Wiener space]] ''i''&nbsp;:&nbsp;''H''&nbsp;→&nbsp;''E'' with abstract Wiener measure ''&gamma;''.  Take ''S'' to be the set of all ''C''<sup>1</sup> functions from ''E'' into ''E''<sup>∗</sup>; ''E''<sup>∗</sup> can be thought of as a subspace of ''E'' in view of the inclusions\n\n::<math>E^{*} \\xrightarrow{i^{*}} H^{*} \\cong H \\xrightarrow{i} E.</math>\n\n:For ''h''&nbsp;&isin;&nbsp;''S'', define ''Ah'' by\n\n::<math>(A h)(x) = h(x) x - \\mathrm{trace}_{H} \\mathrm{D} h(x).</math>\n\n:This operator ''A'' is an integration by parts operator, also known as the [[divergence]] operator; a proof can be found in Elworthy (1974).\n\n* The [[classical Wiener space]] ''C''<sub>0</sub> of [[continuous function|continuous paths]] in '''R'''<sup>''n''</sup> starting at zero and defined on the [[interval (mathematics)|unit interval]] [0,&nbsp;1] has another integration by parts operator.  Let ''S'' be the collection\n\n::<math>S = \\left\\{ \\left. h \\colon C_{0} \\to L_{0}^{2, 1} \\right| h \\mbox{ is bounded and non-anticipating} \\right\\},</math>\n\n:i.e., all [[bounded function|bounded]], [[adapted process|adapted]] processes with [[absolutely continuous]] sample paths.  Let ''&phi;''&nbsp;:&nbsp;''C''<sub>0</sub>&nbsp;&rarr;&nbsp;'''R''' be any ''C''<sup>1</sup> function such that both ''&phi;'' and D''&phi;'' are bounded.  For ''h''&nbsp;&isin;&nbsp;''S'' and ''&lambda;''&nbsp;&isin;&nbsp;'''R''', the [[Girsanov theorem]] implies that\n\n::<math>\\int_{C_{0}} \\varphi (x + \\lambda h(x)) \\, \\mathrm{d} \\gamma(x) = \\int_{C_{0}} \\varphi(x) \\exp \\left( \\lambda \\int_{0}^{1} \\dot{h}_{s} \\cdot \\mathrm{d} x_{s} - \\frac{\\lambda^{2}}{2} \\int_{0}^{1} | \\dot{h}_{s} |^{2} \\, \\mathrm{d} s \\right) \\, \\mathrm{d} \\gamma(x).</math>\n\n:Differentiating with respect to ''&lambda;'' and setting ''&lambda;''&nbsp;=&nbsp;0 gives\n\n::<math>\\int_{C_{0}} \\mathrm{D} \\varphi(x) h(x) \\, \\mathrm{d} \\gamma(x) = \\int_{C_{0}} \\varphi(x) (A h) (x) \\, \\mathrm{d} \\gamma(x),</math>\n\n:where (''Ah'')(''x'') is the [[Itō integral]]\n\n::<math>\\int_{0}^{1} \\dot{h}_{s} \\cdot \\mathrm{d} x_{s}.</math>\n\n:The same relation holds for more general ''&phi;'' by an approximation argument; thus, the Itō integral is an integration by parts operator and can be seen as an infinite-dimensional divergence operator.  This is the same result as the [[Clark-Ocone theorem#Integration by parts on Wiener space|integration by parts formula derived from the Clark-Ocone theorem]].\n\n==References==\n\n* {{cite book\n| last = Bell\n| first = Denis R.\n| title = The Malliavin calculus\n| publisher = Dover Publications Inc.\n| location = Mineola, NY\n| year = 2006\n| pages = x+113\n| isbn = 0-486-44994-7\n}} {{MathSciNet|id=2250060}} (See section 5.3)\n* {{cite book\n| last = Elworthy\n| first =  K. David\n| chapter = Gaussian measures on Banach spaces and manifolds\n| title = Global analysis and its applications (Lectures, Internat. Sem. Course, Internat. Centre Theoret. Phys., Trieste, 1972), Vol. II\n| pages = 151&ndash;166\n| publisher = Internat. Atomic Energy Agency\n| location = Vienna\n| year = 1974\n}} {{MathSciNet|id=0464297}}\n\n[[Category:Integral calculus]]\n[[Category:Measure theory]]\n[[Category:Operator theory]]\n[[Category:Stochastic calculus]]"
    },
    {
      "title": "Integration by reduction formulae",
      "url": "https://en.wikipedia.org/wiki/Integration_by_reduction_formulae",
      "text": "{{Calculus |Integral}}\n\n'''Integration by reduction formula''' in [[integral calculus]] is a technique or procedure of integration, in the form of a [[recurrence relation]]. It is used when an [[expression (mathematics)|expression]] containing an [[integer]] [[parameter#Mathematical functions|parameter]], usually in the form of powers of elementary functions, or [[Product (mathematics)|product]]s of [[transcendental function]]s and [[polynomial]]s of arbitrary [[degree of a polynomial|degree]], can't be integrated directly. But using other [[integral#Methods|methods of integration]] a '''reduction formula''' can be set up to obtain the integral of the same or similar expression with a lower integer parameter, progressively simplifying the integral until it can be evaluated. <ref>Mathematical methods for physics and engineering, K.F. Riley, M.P. Hobson, S.J. Bence, Cambridge University Press, 2010, {{ISBN|978-0-521-86153-3}}</ref> This method of integration is one of the earliest used.\n\n== How to find the reduction formula ==\n\nThe reduction formula can be derived using any of the common methods of integration, like [[integration by substitution]], [[integration by parts]], [[Trigonometric substitution|integration by trigonometric substitution]], [[integration by partial fractions]], etc. The main idea is to express an integral involving an integer parameter (e.g. power) of a function, represented by I<sub>n</sub>, in terms of an integral that involves a lower value of the parameter (lower power) of that function, for example ''I''<sub>''n''-1</sub> or ''I''<sub>''n''-2</sub>. This makes the reduction formula a type of [[recurrence relation]]. In other words, the reduction formula expresses the integral \n:<math>I_n =\\int f(x,n) \\,\\text{d}x, </math> \nin terms of \n:<math>I_k =  \\int f(x,k) \\,\\text{d}x, </math> \nwhere \n:<math>k < n.</math>\n\n== How to compute the integral ==\n\nTo compute the integral, we set ''n'' to its value and use the reduction formula to calculate the (''n'' – 1) or (''n'' – 2) integral. The higher index integral can be used to calculate lower index ones; the process is continued repeatedly until we reach a point where the function to be integrated can be computed, usually when its index is 0 or 1. Then we back-substitute the previous results until we have computed ''I<sub>n</sub>''. <ref>Further Elementary Analysis, R.I. Porter, G. Bell & Sons Ltd, 1978, {{ISBN|0-7135-1594-5}}</ref>\n\n===Examples=== \n\nBelow are examples of the procedure.\n\n'''Cosine integral'''\n\nTypically, integrals like \n\n:<math>\\int \\cos^n x \\,\\text{d}x , \\,\\!</math>\n\ncan be evaluated by a reduction formula. \n\n[[Image:Cos to the n.png|thumb|<math>\\int \\cos^n (x) \\,\\text{d}x\\!</math>, for ''n'' = 1, 2 ... 30]]\n\nStart by setting:\n\n:<math>I_n = \\int \\cos^n x\\,\\text{d}x . \\,\\!</math>\n\nNow re-write as:\n\n:<math>I_n = \\int \\cos^ {n-1} x \\cos x \\,\\text{d}x , \\,\\!</math>\n\nIntegrating by this substitution:\n\n:<math>\\cos x \\,\\text{d}x = \\text{d} ( \\sin x) , \\,\\!</math>\n\n:<math>I_n = \\int \\cos^{n-1} x \\,\\text{d}(\\sin x) . \\!</math>\n\nNow integrating by parts:\n\n:<math> \\begin{align} \\int \\cos^n x \\,\\text{d}x & = \\cos^{n-1} x \\sin x - \\int \\sin x \\,\\text{d}(\\cos^{n-1} x) \\\\\n& = \\cos^{n-1} x \\sin x + (n-1) \\int \\sin x \\cos^{n-2} x\\sin x \\,\\text{d}x\\\\\n& = \\cos^{n-1} x \\sin x + (n-1) \\int \\cos^{n-2} x \\sin^2 x \\,\\text{d}x\\\\\n& = \\cos^{n-1} x \\sin x + (n-1) \\int \\cos^{n-2} x (1-\\cos^2 x )\\,\\text{d}x\\\\\n& = \\cos^{n-1} x \\sin x + (n-1) \\int \\cos^{n-2} x \\,\\text{d}x - (n-1)\\int \\cos^n x \\,\\text{d}x\\\\\n& = \\cos^{n-1} x \\sin x + (n-1) I_{n-2} - (n-1) I_n ,\n\\end{align} \\,</math>\n\nsolving for ''I<sub>n</sub>'':\n\n:<math>I_n \\ + (n-1) I_n\\ = \\cos^{n-1} x \\sin x\\ + \\ (n-1) I_{n-2} , \\,</math>\n\n:<math>n I_n\\ = \\cos^{n-1} (x) \\sin x\\ + (n-1) I_{n-2} , \\,</math>\n\n:<math>I_n \\ = \\frac{1}{n}\\cos^{n-1} x \\sin x\\ + \\frac{n-1}{n} I_{n-2} , \\,</math>\n\nso the reduction formula is:\n\n:<math>\\int \\cos^n x \\,\\text{d}x\\ = \\frac{1}{n}\\cos^{n-1} x \\sin x + \\frac{n-1}{n} \\int \\cos^{n-2} x \\,\\text{d}x . \\!</math>\n\nTo supplement the example, the above can be used to evaluate the integral for (say) ''n'' = 5;\n\n:<math> I_5 = \\int \\cos^5 x \\,\\text{d}x . \\,\\!</math>\n\nCalculating lower indices:\n\n:<math>n=5, \\quad I_5 = \\tfrac{1}{5} \\cos^4 x \\sin x + \\tfrac{4}{5} I_3 , \\,</math>\n\n:<math>n=3, \\quad I_3 = \\tfrac{1}{3} \\cos^2 x \\sin x + \\tfrac{2}{3} I_1, \\,</math>\n\nback-substituting:\n\n:<math>\\because I_1\\ = \\int \\cos x \\,\\text{d}x = \\sin x + C_1,\\,</math>\n\n:<math>\\therefore I_3\\ = \\tfrac{1}{3} \\cos^2 x \\sin x + \\tfrac{2}{3}\\sin x + C_2, \\quad C_2\\ = \\tfrac{2}{3} C_1,\\,</math>\n\n:<math>I_5\\ = \\frac{1}{5} \\cos^4 x \\sin x + \\frac{4}{5}\\left[\\frac{1}{3} \\cos^2 x \\sin x + \\frac{2}{3} \\sin x\\right] + C,\\,</math>\n\nwhere ''C'' is a constant.\n\n'''Exponential integral'''\n\nAnother typical example is:\n\n:<math>\\int x^n e^{ax} \\,\\text{d}x . \\,\\!</math>\n\nStart by setting:\n\n:<math>I_n = \\int x^n e^{ax} \\,\\text{d}x . \\,\\!</math>\n\nIntegrating by substitution:\n\n:<math> x^n \\,\\text{d}x = \\frac{\\text{d} ( x^{n+1})}{n+1} , \\,\\!</math>\n\n:<math>I_n = \\frac{1}{n+1} \\int e^{ax} \\,\\text{d}(x^{n+1}) , \\!</math>\n\nNow integrating by parts:\n\n:<math>\\begin{align} \\int e^{ax} \\,\\text{d}(x^{n+1}) & = x^{n+1}e^{ax} - \\int x^{n+1} \\,\\text{d}(e^{ax}) \\\\\n& = x^{n+1}e^{ax} - a \\int x^{n+1} e^{ax}\\,\\text{d}x ,\n\\end{align} \\!</math>\n\n:<math>(n+1) I_n = x^{n+1}e^{ax} - a I_{n+1} , \\!</math>\n\nshifting indices back by 1 (so ''n + 1'' → ''n'', ''n'' → ''n'' – 1):\n\n:<math>n I_{n-1} = x^ne^{ax} - a I_n , \\!</math>\n\nsolving for ''I<sub>n</sub>'':\n\n:<math> I_n = \\frac{1}{a} \\left ( x^ne^{ax} - n I_{n-1} \\right ) , \\,\\!</math>\n\nso the reduction formula is:\n\n:<math> \\int x^n e^{ax} \\,\\text{d}x = \\frac{1}{a} \\left ( x^ne^{ax} - n \\int x^{n-1} e^{ax} \\,\\text{d}x \\right ). \\!</math>\nAn alternative way in which the derivation could be done starts by substituting <math>e^{ax}</math>.\n\nIntegration by substitution:\n\n<math> e^{ax} \\,\\text{d}x = \\frac{\\text{d} ( e^{ax})}{a} , \\,\\!</math>\n\n<math>I_n = \\frac{1}{a} \\int x^{n} \\,\\text{d}(e^{ax}) , \\!</math>\n\nNow integrating by parts:\n\n<math>\\begin{align} \\int x^{n} \\,\\text{d}(e^{ax}) & = x^{n}e^{ax} - \\int e^{ax} \\,\\text{d}(x^{n}) \\\\\n& = x^{n}e^{ax} - n \\int e^{ax} x^{n-1}\\,\\text{d}x ,\n\\end{align} \\!</math>\n\nwhich gives the reduction formula when substituting back:\n\n<math> I_n = \\frac{1}{a} \\left ( x^ne^{ax} - n I_{n-1} \\right ) , \\,\\!</math>\n\nwhich is equivalent to:\n\n:<math> \\int x^n e^{ax} \\,\\text{d}x = \\frac{1}{a} \\left ( x^ne^{ax} - n \\int x^{n-1} e^{ax} \\,\\text{d}x \\right ). \\!</math>\n\n== Tables of integral reduction formulas ==\n\n===Rational functions===\n\nThe following integrals<ref>http://www.sosmath.com/tables/tables.html -> Indefinite integrals list</ref> contain:\n*Factors of the [[Linear equation|linear]] [[Square root|radical]] <math>\\sqrt{ax+b}\\,\\!</math>\n*Linear factors <math>{px+q}\\,\\!</math> and the linear radical <math>\\sqrt{ax+b}\\,\\!</math>\n*[[Quadratic polynomial|Quadratic]] factors <math>x^2+a^2\\,\\!</math>\n*Quadratic factors <math>x^2-a^2\\,\\!</math>, for <math>x>a\\,\\!</math>\n*Quadratic factors <math>a^2-x^2\\,\\!</math>, for <math>x<a\\,\\!</math>\n*([[Irreducible polynomial|Irreducible]]) quadratic factors <math>ax^2+bx+c\\,\\!</math>\n*Radicals of irreducible quadratic factors <math>\\sqrt{ax^2+bx+c}\\,\\!</math>\n\n{| class=\"wikitable\"\n|-\n! Integral!! Reduction formula\n|-\n| <math>I_n = \\int \\frac{x^n}{\\sqrt{ax+b}} \\,\\text{d}x\\,\\!</math> || <math>I_n = \\frac{2x^n\\sqrt{ax+b}}{a(2n+1)} - \\frac{2nb}{a(2n+1)} I_{n-1}\\,\\!</math>\n|-\n| <math>I_n = \\int \\frac{\\text{d}x}{x^n\\sqrt{ax+b}}\\,\\!</math> || <math>I_n = -\\frac{\\sqrt{ax+b}}{(n-1)bx^{n-1}}-\\frac{a(2n-3)}{2b(n-1)}I_{n-1}\\,\\!</math>\n|-\n| <math>I_n = \\int x^n\\sqrt{ax+b}\\,\\text{d}x\\,\\!</math> || <math>I_n = \\frac{2x^n\\sqrt{(ax+b)^3}}{a(2n+3)}-\\frac{2nb}{a(2n+3)}I_{n-1}\\,\\!</math>\n|-\n| <math>I_{m,n} = \\int \\frac{\\text{d}x}{(ax+b)^m(px+q)^n}\\,\\!</math> || <math>I_{m,n} = \n\\begin{cases}\n  -\\frac{1}{(n-1)(bp-aq)} \\left [ \\frac{1}{(ax+b)^{m-1}(px+q)^{n-1}}+a(m+n-2)I_{m,n-1} \\right ]   \\\\\n  \\frac{1}{(m-1)(bp-aq)} \\left [ \\frac{1}{(ax+b)^{m-1}(px+q)^{n-1}}+p(m+n-2)I_{m-1,n} \\right ]\n\\end{cases}\\,\\!</math>\n|-\n| <math>I_{m,n} = \\int  \\frac{(ax+b)^m}{(px+q)^n} \\,\\text{d}x\\,\\!</math> || <math>I_{m,n} =\n\\begin{cases}\n  -\\frac{1}{(n-1)(bp-aq)}\\left [ \\frac{(ax+b)^{m+1}}{(px+q)^{n-1}}+a(n-m-2)I_{m,n-1} \\right ]   \\\\\n  -\\frac{1}{(n-m-1)p}\\left [ \\frac{(ax+b)^m}{(px+q)^{n-1}}+m(bp-aq)I_{m-1,n} \\right ]  \\\\\n  -\\frac{1}{(n-1)p}\\left [ \\frac{(ax+b)^m}{(px+q)^{n-1}}-amI_{m-1,n-1} \\right ]\n\\end{cases}\\,\\!</math>\n|}\n\n{| class=\"wikitable\"\n|-\n! Integral!! Reduction formula\n|-\n| <math>I_n=\\int \\frac{(px+q)^n}{\\sqrt{ax+b}} \\,\\text{d}x\\,\\!</math> || <math>\\int (px+q)^n\\sqrt{ax+b} \\,\\text{d}x  = \\frac{2(px+q)^{n+1}\\sqrt{ax+b}}{p(2n+3)}+\\frac{bp-aq}{p(2n+3)}I_n\\,\\!</math>\n\n<math>I_n=\\frac{2(px+q)^n\\sqrt{ax+b}}{a(2n+1)}+\\frac{2n(aq-bp)}{a(2n+1)}I_{n-1}\\,\\!</math>\n|-\n| <math>I_n=\\int \\frac{\\text{d}x}{(px+q)^n\\sqrt{ax+b}}\\,\\!</math> || <math>\\int \\frac{\\sqrt{ax+b}}{(px+q)^n}\\,\\text{d}x = -\\frac{\\sqrt{ax+b}}{p(n-1)(px+q)^{n-1}}+\\frac{a}{2p(n-1)}I_{n}\\,\\!</math>\n\n<math>I_n= -\\frac{\\sqrt{ax+b}}{(n-1)(aq-bp)(px+q)^{n-1}}+\\frac{a(2n-3)}{2(n-1)(aq-bp)}I_{n-1}\\,\\!</math>\n|}\n\n{| class=\"wikitable\"\n|-\n! Integral!! Reduction formula\n|-\n| <math>I_n= \\int \\frac{\\text{d}x}{(x^2+a^2)^n}\\,\\!</math> || <math>I_n= \\frac{x}{2a^2(n-1)(x^2+a^2)^{n-1}}+\\frac{2n-3}{2a^2(n-1)}I_{n-1}\\,\\!</math>\n|-\n| <math>I_{n,m}= \\int \\frac{\\text{d}x}{x^m(x^2+a^2)^n}\\,\\!</math> || <math>a^2I_{n,m}= I_{m,n-1}-I_{m-2,n}\\,\\!</math>\n|-\n| <math>I_{n,m}= \\int \\frac{x^m}{(x^2+a^2)^n} \\,\\text{d}x\\,\\!</math> || <math>I_{n,m}= I_{m-2,n-1}-a^2I_{m-2,n}\\,\\!</math>\n|}\n\n{| class=\"wikitable\"\n|-\n! Integral!! Reduction formula\n|-\n| <math>I_n= \\int \\frac{\\text{d}x}{(x^2-a^2)^n}\\,\\!</math> || <math>I_n= -\\frac{x}{2a^2(n-1)(x^2-a^2)^{n-1}}-\\frac{2n-3}{2a^2(n-1)}I_{n-1}\\,\\!</math>\n|-\n| <math>I_{n,m}= \\int \\frac{\\text{d}x}{x^m(x^2-a^2)^n}\\,\\!</math> || <math>{a^2}I_{n,m}= I_{m-2,n}-I_{m,n-1}\\,\\!</math>\n|-\n| <math>I_{n,m}= \\int \\frac{x^m}{(x^2-a^2)^n} \\,\\text{d}x\\,\\!</math> || <math>I_{n,m}= I_{m-2,n-1}+a^2I_{m-2,n}\\,\\!</math>\n|}\n\n{| class=\"wikitable\"\n|-\n! Integral!! Reduction formula\n|-\n| <math>I_n= \\int \\frac{\\text{d}x}{(a^2-x^2)^n}\\,\\!</math> || <math>I_n= \\frac{x}{2a^2(n-1)(a^2-x^2)^{n-1}}+\\frac{2n-3}{2a^2(n-1)}I_{n-1}\\,\\!</math>\n|-\n| <math>I_{n,m}= \\int \\frac{\\text{d}x}{x^m(a^2-x^2)^n}\\,\\!</math>  || <math>{a^2}I_{n,m}= I_{m,n-1}+I_{m-2,n}\\,\\!</math>\n|-\n| <math>I_{n,m}= \\int \\frac{x^m}{(a^2-x^2)^n} \\,\\text{d}x\\,\\!</math> || <math>I_{n,m}= a^2I_{m-2,n}-I_{m-2,n-1}\\,\\!</math>\n|}\n\n{| class=\"wikitable\"\n|-\n! Integral!! Reduction formula\n|-\n| <math>I_n = \\int \\frac{\\text{d}x}{{x^n}(ax^2+bx+c)}\\,\\!</math> || <math>-cI_n =\\frac{1}{x^{n-1}(n-1)}+ bI_{n-1}+aI_{n-2}\\,\\!</math>\n|-\n| <math>I_{m,n}=\\int \\frac{x^m \\,\\text{d}x}{(ax^2+bx+c)^n}\\,\\!</math> || <math>I_{m,n}= -\\frac{x^{m-1}}{a(2n-m-1)(ax^2+bx+c)^{n-1}} - \\frac{b(n-m)}{a(2n-m-1)}I_{m-1,n} + \\frac{c(m-1)}{a(2n-m-1)}I_{m-2,n}\\,\\!</math>\n|-\n| <math>I_{m,n}= \\int \\frac{\\text{d}x}{x^m(ax^2+bx+c)^n}\\,\\!</math> || <math>-c(m-1)I_{m,n}= \\frac{1}{x^{m-1}(ax^2+bx+c)^{n-1}}+{a(m+2n-3)}I_{m-2,n}+{b(m+n-2)}I_{m-1,n}\\,\\!</math>\n|-\n|}\n\n{| class=\"wikitable\"\n|-\n! Integral!! Reduction formula\n|-\n| <math>I_n = \\int (ax^2+bx+c)^n\\,\\text{d}x\\,\\!</math> || <math>8a(n+1)I_{n+\\frac{1}{2}} = 2(2ax+b)(ax^2+bx+c)^{n+\\frac{1}{2}} + (2n+1)(4ac-b^2)I_{n-\\frac{1}{2}}\\,\\!</math>\n|-\n| <math>I_n = \\int \\frac{1}{(ax^2+bx+c)^n}\\,\\text{d}x\\,\\!</math> || <math>(2n-1)(4ac-b^2)I_{n+\\frac{1}{2}} = \\frac{2(2ax+b)}{(ax^2+bx+c)^{n-\\frac{1}{2}}}+{8a(n-1)}I_{n-\\frac{1}{2}}\\,\\!</math>\n|-\n|}\n\nnote that by the [[Exponentiation|laws of indices]]:\n\n:<math>I_{n+\\frac{1}{2}} =  I_{\\frac{2n+1}{2}} =\\int \\frac{1}{(ax^2+bx+c)^{\\frac{2n+1}{2}}}\\,\\text{d}x = \\int \\frac{1}{\\sqrt{(ax^2+bx+c)^{2n+1}}}\\,\\text{d}x\\,\\!</math>\n\n===Transcendental functions===\n{{main article|Transcendental function}}\n\nThe following integrals<ref>http://www.sosmath.com/tables/tables.html -> Indefinite integrals list</ref> contain:\n*Factors of sine\n*Factors of cosine\n*Factors of sine and cosine products and quotients\n*Products/quotients of exponential factors and powers of ''x''\n*Products of exponential and sine/cosine factors\n\n{| class=\"wikitable\"\n|-\n! Integral!! Reduction formula\n|-\n| <math>I_n=\\int x^n \\sin{ax} \\,\\text{d}x\\,\\!</math> || <math>a^2I_n=-ax^n \\cos{ax} + nx^{n-1} \\sin{ax} - n(n-1) I_{n-2} \\,\\!</math>\n|-\n| <math>J_n=\\int x^n \\cos{ax} \\,\\text{d}x \\,\\!</math> || <math>a^2J_n=ax^n \\sin{ax} + nx^{n-1} \\cos{ax} - n(n-1) J_{n-2} \\,\\!</math>\n|-\n| <math> I_n = \\int \\frac{\\sin{ax}}{x^n} \\,\\text{d}x\\,\\!</math>\n\n<math>J_n = \\int \\frac{\\cos{ax}}{x^n} \\,\\text{d}x \\,\\!</math>\n|| <math>I_n = -\\frac{\\sin{ax}}{(n-1)x^{n-1}}+\\frac{a}{n-1}J_{n-1}\\,\\!</math>\n\n<math>J_n = -\\frac{\\cos{ax}}{(n-1)x^{n-1}}-\\frac{a}{n-1}I_{n-1}\\,\\!</math>\n\nthe formulae can be combined to obtain separate equations in ''I<sub>n</sub>'':\n\n<math>J_{n-1} = -\\frac{\\cos{ax}}{(n-2)x^{n-2}}-\\frac{a}{n-2}I_{n-2}\\,\\!</math>\n\n<math>I_n = -\\frac{\\sin{ax}}{(n-1)x^{n-1}}-\\frac{a}{n-1}\\left [\\frac{\\cos{ax}}{(n-2)x^{n-2}}+\\frac{a}{n-2}I_{n-2}\\right ] \\,\\!</math>\n\n<math> \\therefore I_n = -\\frac{\\sin{ax}}{(n-1)x^{n-1}}-\\frac{a}{(n-1)(n-2)}\\left (\\frac{\\cos{ax}}{x^{n-2}}+aI_{n-2}\\right ) \\,\\!</math>\n\nand ''J<sub>n</sub>'':\n\n<math>I_{n-1} = -\\frac{\\sin{ax}}{(n-2)x^{n-2}}+\\frac{a}{n-2}J_{n-2}\\,\\!</math>\n\n<math>J_n = -\\frac{\\cos{ax}}{(n-1)x^{n-1}}-\\frac{a}{n-1}\\left [-\\frac{\\sin{ax}}{(n-2)x^{n-2}}+\\frac{a}{n-2}J_{n-2}  \\right ]\\,\\!</math>\n\n<math> \\therefore J_n = -\\frac{\\cos{ax}}{(n-1)x^{n-1}}-\\frac{a}{(n-1)(n-2)}\\left (-\\frac{\\sin{ax}}{x^{n-2}}+aJ_{n-2}  \\right )\\,\\!</math>\n|-\n| <math>I_n = \\int \\sin^n{ax} \\,\\text{d}x\\,\\!</math> || <math>anI_n = -\\sin^{n-1}{ax}\\cos{ax}+a(n-1)I_{n-2}\\,\\!</math>\n|-\n| <math>J_n = \\int \\cos^n{ax} \\,\\text{d}x\\,\\!</math> || <math>anJ_n = \\sin{ax}\\cos^{n-1}{ax}+a(n-1)J_{n-2}\\,\\!</math>\n|-\n| <math>I_n = \\int \\frac{\\text{d}x}{\\sin^n{ax}}\\,\\!</math> || <math>(n-1)I_n = - \\frac{\\cos{ax}}{a\\sin^{n-1}{ax}}+ (n-2)I_{n-2}\\,\\!</math>\n|-\n| <math>J_n = \\int \\frac{\\text{d}x}{\\cos^n{ax}}\\,\\!</math> || <math>(n-1)J_n = \\frac{\\sin{ax}}{a\\cos^{n-1}{ax}}+ (n-2)J_{n-2}\\,\\!</math>\n|-\n|}\n\n{| class=\"wikitable\"\n|-\n! Integral!! Reduction formula\n|-\n| <math>I_{m,n} = \\int \\sin^m{ax}\\cos^n{ax}\\,\\text{d}x\\,\\!</math> || <math>I_{m,n} = \\begin{cases}\n    -\\frac{\\sin^{m-1}{ax}\\cos^{n+1}{ax}}{a(m+n)}+\\frac{m-1}{m+n}I_{m-2,n} \\\\\n    \\frac{\\sin^{m+1}{ax}\\cos^{n-1}{ax}}{a(m+n)}+\\frac{n-1}{m+n}I_{m,n-2} \\\\\n\\end{cases}\\,\\!</math>\n|-\n| <math>I_{m,n} = \\int \\frac{\\text{d}x}{\\sin^m{ax}\\cos^n{ax}}\\,\\!</math> || <math>I_{m,n} = \\begin{cases}\n    \\frac{1}{a(n-1)\\sin^{m-1}{ax}\\cos^{n-1}{ax}}+\\frac{m+n-2}{n-1}I_{m,n-2} \\\\\n    -\\frac{1}{a(m-1)\\sin^{m-1}{ax}\\cos^{n-1}{ax}}+\\frac{m+n-2}{m-1}I_{m-2,n} \\\\\n\\end{cases}\\,\\!</math>\n|-\n| <math>I_{m,n} = \\int \\frac{\\sin^m{ax}}{\\cos^n{ax}}\\,\\text{d}x\\,\\!</math> || <math>I_{m,n} = \\begin{cases}\n    \\frac{\\sin^{m-1}{ax}}{a(n-1)\\cos^{n-1}{ax}}-\\frac{m-1}{n-1}I_{m-2,n-2} \\\\\n    \\frac{\\sin^{m+1}{ax}}{a(n-1)\\cos^{n-1}{ax}}-\\frac{m-n+2}{n-1}I_{m,n-2} \\\\\n    -\\frac{\\sin^{m-1}{ax}}{a(m-n)\\cos^{n-1}{ax}}+\\frac{m-1}{m-n}I_{m-2,n} \\\\\n\\end{cases}\\,\\!</math>\n|-\n| <math>I_{m,n} =  \\int \\frac{\\cos^m{ax}}{\\sin^n{ax}}\\,\\text{d}x\\,\\!</math> || <math>I_{m,n} = \\begin{cases}\n    -\\frac{\\cos^{m-1}{ax}}{a(n-1)\\sin^{n-1}{ax}}-\\frac{m-1}{n-1}I_{m-2,n-2} \\\\\n    -\\frac{\\cos^{m+1}{ax}}{a(n-1)\\sin^{n-1}{ax}}-\\frac{m-n+2}{n-1}I_{m,n-2} \\\\\n    \\frac{\\cos^{m-1}{ax}}{a(m-n)\\sin^{n-1}{ax}}+\\frac{m-1}{m-n}I_{m-2,n} \\\\\n\\end{cases}\\,\\!</math>\n|-\n|}\n\n{| class=\"wikitable\"\n|-\n! Integral!! Reduction formula\n|-\n| <math>I_{n} = \\int x^n e^{ax}\\,\\text{d}x\\,\\!</math>\n\n<math>n > 0\\,\\!</math>\n| <math> I_{n} = \\frac{x^n e^{ax}}{a} - \\frac{n}{a}I_{n-1} \\,\\!</math>\n|-\n| <math>I_{n} = \\int x^{-n} e^{ax} \\,\\text{d}x\\,\\!</math>\n\n<math>n > 0\\,\\!</math>\n\n<math>n \\neq 1\\,\\!</math>\n| <math> I_{n} = \\frac{- e^{ax}}{(n-1)x^{n-1}} + \\frac{a}{n-1}I_{n-1} \\,\\!</math>\n|-\n| <math>I_{n} = \\int e^{ax} \\sin^n{bx} \\,\\text{d}x\\,\\!</math>\n| <math> I_{n} = \\frac{e^{ax} \\sin^{n-1}{bx}}{a^2+(bn)^2}\\left ( a\\sin bx - bn\\cos bx \\right ) + \\frac{n(n-1)b^2}{a^2+(bn)^2}I_{n-2} \\,\\!</math>\n|-\n| <math>I_{n} = \\int e^{ax} \\cos^n{bx} \\,\\text{d}x\\,\\!</math>\n| <math> I_{n} = \\frac{e^{ax} \\cos^{n-1}{bx}}{a^2+(bn)^2}\\left ( a\\cos bx + bn\\sin bx \\right ) + \\frac{n(n-1)b^2}{a^2+(bn)^2}I_{n-2} \\,\\!</math>\n|-\n|}\n\n==References==\n\n{{Reflist}}\n\n==Bibliography==\n{{wikibooks|Calculus/Integration techniques/Reduction Formula}}\n*Anton, Bivens, Davis, Calculus, 7th edition.\n\n\n[[Category:Integral calculus]]"
    },
    {
      "title": "Integration by substitution",
      "url": "https://en.wikipedia.org/wiki/Integration_by_substitution",
      "text": "{{Calculus|Integral}}\n{{no footnotes|date=September 2017}}\nIn [[calculus]], '''integration by substitution''', also known as '''''u''-substitution''', is a method for finding [[integral]]s. Using the [[fundamental theorem of calculus]] often requires finding an [[antiderivative]]. For this and other reasons, integration by substitution is an important tool in mathematics. It is the counterpart to the [[chain rule]] for [[derivative|differentiation]].\n\n== Substitution for a single variable ==\n\n=== Proposition ===\nLet {{math|''I'' ⊆ '''R'''}} be an interval and {{math|''φ'' : [''a'',''b''] → ''I''}} be a differentiable function with integrable derivative. Suppose that {{math|''f'' : ''I'' → '''R'''}} is a [[continuous function]]. Then\n:<math>\n\\int_{\\varphi(a)}^{\\varphi(b)} f(u)\\,du = \\int_a^b f(\\varphi(x))\\varphi'(x)\\, dx.\n</math>\n\nIn Leibniz notation, the substitution {{math|''u'' {{=}} ''φ''(''x'')}} yields \n:<math>\\frac{du}{dx} = \\varphi'(x).</math>\nWorking heuristically with infinitesimals yields the equation\n:<math>du = \\varphi'(x)\\,dx,</math>\nwhich suggests the substitution formula above.  (This equation may be put on a rigorous foundation by interpreting it as a statement about [[differential form]]s.)  One may view the method of integration by substitution as a partial justification of [[Leibniz's notation]] for integrals and derivatives.\n\nThe formula is used to transform one integral into another integral that is easier to compute. Thus, the formula can be used from left to right or from right to left in order to simplify a given integral. When used in the latter manner, it is sometimes known as '''''u''-substitution''' or '''''w''-substitution'''.\n\n=== Proof ===\n\nIntegration by substitution can be derived from the [[fundamental theorem of calculus]] as follows.  Let {{math|''f''}} and {{math|''φ''}} be two functions satisfying the above hypothesis that {{math|''f''}} is continuous on {{math|''I''}} and {{math|''φ''′}} is integrable on the closed interval {{math|[''a'',''b'']}}.  Then the function {{math|''f''(''φ''(''x''))''φ''′(''x'')}} is also integrable on {{math|[''a'',''b'']}}.  Hence the integrals\n\n:<math>\n\\int_{\\varphi(a)}^{\\varphi(b)} f(u)\\,du\n</math>\n\nand\n\n:<math>\n\\int_a^b f(\\varphi(x))\\varphi'(x)\\,dx\n</math>\n\nin fact exist, and it remains to show that they are equal.\n\nSince {{math|''f''}} is continuous, it has an [[antiderivative]] {{math|''F''}}. The [[function composition|composite function]] {{math|''F'' ∘ ''φ''}} is then defined. Since {{math|''φ''}} is differentiable, combining the [[chain rule]] and the definition of an antiderivative gives\n\n:<math>(F \\circ \\varphi)'(x) = F'(\\varphi(x))\\varphi'(x) = f(\\varphi(x))\\varphi'(x).</math>\n\nApplying the [[fundamental theorem of calculus]] twice gives\n\n:<math>\n\\begin{align}\n\\int_a^b f(\\varphi(x))\\varphi'(x)\\,dx\n&= \\int_a^b (F \\circ \\varphi)'(x)\\,dx \\\\\n&= (F \\circ \\varphi)(b) - (F \\circ \\varphi)(a) \\\\\n&= F(\\varphi(b)) - F(\\varphi(a)) \\\\\n&= \\int_{\\varphi(a)}^{\\varphi(b)} f(u)\\,du,\n\\end{align}\n</math>\n\nwhich is the substitution rule.\n\n=== Examples ===\n\n==== Example 1: from right to left ====\nConsider the integral\n:<math>\\int_0^2 x \\cos(x^2+1) \\,dx.</math>\n\nIf we apply the formula from right to left and make the substitution {{math|1=''u'' =  ''φ''(''x'') =  ''x''<sup>2</sup> + 1}}, we obtain {{math|1=''du'' = 2''x'' ''dx''}} and hence {{math|1=''x'' ''dx'' = ½''du''}}.  Therefore,\n\n:<math>\\begin{align}\n\\int_{x=0}^{x=2} x \\cos(x^2+1) \\,dx\n&= \\frac{1}{2} \\int_{u=1}^{u=5}\\cos(u)\\,du \\\\\n&= \\frac{1}{2}(\\sin(5)-\\sin(1)).\n\\end{align}</math>\n\nSince the lower limit {{math|1=''x'' = 0}} was replaced with {{math|1=''u'' = 0<sup>2</sup> + 1 = 1}}, and the upper limit {{math|1=''x'' = 2}} replaced with {{math|1=''u'' = 2<sup>2</sup> + 1 = 5}}, a transformation back into terms of {{math|''x''}} was unnecessary.\n\n==== Example 2: from left to right ====\n\nFor the integral\n:<math>\\int_0^1 \\sqrt{1-x^2}\\,dx,</math>\nthe formula needs to be used from left to right.  The substitution {{math|1=''x'' = sin(''u'')}}, {{math|1=''dx'' = cos(''u'')&thinsp;''du''}} is useful because <math>\\sqrt{1-\\sin^2u} = \\cos(u)</math>:\n\n:<math>\\begin{align}\n\\int_0^1 \\sqrt{1-x^2}\\,dx\n&= \\int_0^{\\pi/2} \\sqrt{1-\\sin^2u} \\cos(u)\\,du \\\\\n&= \\int_0^{\\pi/2} \\cos^2u\\,du \\\\\n&= \\left(\\frac{u}{2} + \\frac{\\sin(2u)}{4}\\right)\\Bigg\\vert_0^{\\pi/2} \\\\\n&= \\frac{\\pi}{4} + 0 = \\frac{\\pi}{4}.\n\\end{align}</math>\n\nThe resulting integral can be computed using [[integration by parts]] or a [[List of trigonometric identities#Double-.2C triple-.2C and half-angle formulae|double angle formula]] followed by one more substitution. One can also note that the function being integrated is the upper right quarter of a circle with a radius of one, and hence integrating the upper right quarter from zero to one is the geometric equivalent to the area of one quarter of the unit circle, or {{math|π / 4}}.\n\n==== Example 3: antiderivatives ====\n\nSubstitution can be used to determine [[antiderivative]]s. One chooses a relation between {{math|''x''}} and {{math|''u''}}, determines the corresponding relation between {{math|''dx''}} and {{math|''du''}} by differentiating, and performs the substitutions. An antiderivative for the substituted function can hopefully be determined; the original substitution between {{math|''u''}} and {{math|''x''}} is then undone.\n\nSimilar to our first example above, we can determine the following antiderivative with this method:\n\n:<math>\\begin{align}\n\\int x \\cos(x^2+1) \\,dx\n&= \\frac{1}{2} \\int 2x \\cos(x^2+1) \\,dx \\\\\n&= \\frac{1}{2} \\int\\cos u\\,du \\\\\n&= \\frac{1}{2}\\sin u + C = \\frac{1}{2}\\sin(x^2+1) + C,\n\\end{align}</math>\n\nwhere ''C'' is an arbitrary [[constant of integration]].\n\nNote that there were no integral boundaries to transform, but in the last step we had to revert the original substitution {{math|1=''u'' = ''x''<sup>2</sup> + 1}}.\n\n== Substitution for multiple variables ==\n\nOne may also use substitution when integrating functions of several variables. \nHere the substitution function {{math|1=(''v''<sub>1</sub>,...,''v''<sub>''n''</sub>) = ''φ''(''u''<sub>1</sub>, ..., ''u''<sub>''n''</sub>)}} needs to be [[injective]] and continuously differentiable, and the differentials transform as\n\n:<math>dv_1 \\cdots dv_n = |\\det(D\\varphi)(u_1, \\ldots, u_n)| \\, du_1 \\cdots du_n,</math>\n\nwhere {{math|det(''Dφ'')(''u''<sub>1</sub>, ..., ''u''<sub>''n''</sub>)}} denotes the [[determinant]] of the [[Jacobian matrix]] of [[partial derivative]]s of {{math|''φ''}} at the point {{math|(''u''<sub>1</sub>, ..., ''u''<sub>''n''</sub>)}}. This formula expresses the fact that the [[absolute value]] of the determinant of a matrix equals the volume of the [[Parallelepiped#Parallelotope|parallelotope]] spanned by its columns or rows.\n\nMore precisely, the ''[[change of variables]]'' formula is stated in the next theorem:\n\n'''Theorem'''. Let {{math|''U''}} be an open set in {{math|'''R'''<sup>''n''</sup>}} and {{math|''φ'' : ''U'' → '''R'''<sup>''n''</sup>}} an [[Injective function|injective]] differentiable function with continuous partial derivatives, the Jacobian of which is nonzero for every {{math|''x''}} in {{math|''U''}}.  Then for any real-valued, compactly supported, continuous function {{math|''f''}}, with support contained in {{math|''φ''(''U'')}},\n\n:<math>\\int_{\\varphi(U)} f(\\mathbf{v})\\, d\\mathbf{v}\n= \\int_U f(\\varphi(\\mathbf{u})) \\left|\\det(D\\varphi)(\\mathbf{u})\\right| \\,d\\mathbf{u}.</math>\n\nThe conditions on the theorem can be weakened in various ways.  First, the requirement that {{math|''φ''}} be continuously differentiable can be replaced by the weaker assumption that {{math|''φ''}} be merely differentiable and have a continuous inverse {{harv|Rudin|1987|loc=Theorem 7.26}}.  This is guaranteed to hold if {{math|''φ''}} is continuously differentiable by the [[inverse function theorem]].  Alternatively, the requirement that {{math|det(''Dφ'') ≠ 0}} can be eliminated by applying [[Sard's theorem]] {{harv|Spivak|1965}}.\n\nFor Lebesgue measurable functions, the theorem can be stated in the following form {{harv|Fremlin|2010|loc=Theorem 263D}}:\n\n'''Theorem'''. Let {{math|''U''}} be a measurable subset of {{math|'''R'''<sup>''n''</sup>}} and {{math|''φ'' : ''U'' → '''R'''<sup>''n''</sup>}} an [[Injective function|injective]] function, and suppose for every {{math|''x''}} in {{math|''U''}} there exists {{math|''φ''&prime;(''x'')}} in {{math|'''R'''<sup>''n'',''n''</sup>}} such that {{math|1=''φ''(''y'') = ''φ''(''x'') + ''φ&prime;''(''x'')(''y'' − ''x'') + ''o''(<nowiki>||</nowiki>''y'' − ''x''<nowiki>||</nowiki>)}} as {{math|''y'' → ''x''}} (here {{math|''o''}} is [[Landau symbol#Related asymptotic notations|little-''o'' notation]]). Then {{math|''φ''(''U'')}} is measurable, and for any real-valued function {{math|''f''}} defined on {{math|''φ''(''U'')}},\n:<math>\\int_{\\varphi(U)} f(v)\\, dv = \\int_U f(\\varphi(u)) \\left|\\det \\varphi'(u)\\right| \\,du</math>\nin the sense that if either integral exists (including the possibility of being properly infinite), then so does the other one, and they have the same value.\n\nAnother very general version in [[measure theory]] is the following {{harv|Hewitt|Stromberg|1965|loc=Theorem 20.3}}:\n\n'''Theorem'''.  Let {{math|''X''}} be a [[locally compact]] [[Hausdorff space]] equipped with a finite [[Radon measure]] {{math|μ}}, and let {{math|''Y''}} be a [[Σ-compact space|&sigma;-compact]] Hausdorff space with a [[sigma finite measure|&sigma;-finite]] Radon measure {{math|ρ}}.  Let {{math|''φ'' : ''X'' → ''Y''}} be a [[continuous function|continuous]] and [[absolutely continuous]] function (where the latter means that {{math|1=''ρ''(''φ''(''E'')) = 0}} whenever {{math|1=''μ''(''E'') = 0}}).  Then there exists a real-valued [[Borel algebra|Borel measurable function]] {{math|''w''}} on {{math|''X''}} such that for every [[Lebesgue integral|Lebesgue integrable]] function {{math|''f'' : ''Y'' → '''R'''}}, the function {{math|(''f'' ∘ ''φ'') ⋅ ''w''}} is Lebesgue integrable on {{math|''X''}}, and\n:<math>\\int_Y f(y)\\,d\\rho(y) = \\int_X (f\\circ \\varphi)(x)\\,w(x)\\,d\\mu(x).</math>\nFurthermore, it is possible to write\n:<math>w(x) = (g\\circ \\varphi)(x)</math>\nfor some Borel measurable function {{math|''g''}} on {{math|''Y''}}.\n\nIn [[geometric measure theory]], integration by substitution is used with [[Lipschitz function]]s.  A bi-Lipschitz function is a Lipschitz function {{math|''φ'' : ''U'' → '''R'''<sup>n</sup>}} which is injective and whose inverse function {{math|''φ''<sup>&minus;1</sup> : ''φ''(''U'') → ''U''}} is also Lipschitz.  By [[Rademacher's theorem]] a bi-Lipschitz mapping is differentiable [[almost everywhere]].  In particular, the Jacobian determinant of a bi-Lipschitz mapping {{math|det ''Dφ''}} is well-defined almost everywhere.  The following result then holds:\n\n'''Theorem.''' Let {{math|''U''}} be an open subset of {{math|'''R'''<sup>n</sup>}} and {{math|''φ'' : ''U'' → '''R'''<sup>n</sup>}} be a bi-Lipschitz mapping.  Let {{math|''f'' : ''φ''(''U'') → '''R'''}} be measurable.  Then\n:<math>\\int_U (f\\circ \\varphi)(x) |\\det D\\varphi(x)|\\,dx = \\int_{\\varphi(U)} f(x)\\,dx</math>\nin the sense that if either integral exists (or is properly infinite), then so does the other one, and they have the same value.\n\nThe above theorem was first proposed by [[Euler]] when he developed the notion of [[double integrals]] in 1769. Although generalized to triple integrals by [[Lagrange]] in 1773, and used by [[Adrien-Marie Legendre|Legendre]], [[Laplace]], [[Gauss]], and first generalized to {{math|''n''}} variables by [[Mikhail Ostrogradski]] in 1836, it resisted a fully rigorous formal proof for a surprisingly long time, and was first satisfactorily resolved 125 years later, by [[Élie Cartan]] in a series of papers beginning in the mid-1890s ({{harnvb|Katz|1982}}; {{harvnb|Ferzola|1994}}).\n\n==Application in probability==\n\nSubstitution can be used to answer the following important question in probability: given a random variable <math>X</math> with probability density <math>p_X</math> and another random variable <math>Y</math> related to <math>X</math> by the equation <math>y=\\phi(x)</math>, what is the probability density for <math>Y</math>?\n\nIt is easiest to answer this question by first answering a slightly different question: what is the probability that <math>Y</math> takes a value in some particular subset <math>S</math>?  Denote this probability <math>P(Y \\in S)</math>.  Of course, if <math>Y</math> has probability density <math>p_Y</math> then the answer is\n\n:<math>P(Y \\in S) = \\int_S p_Y(y)\\,dy, </math>\n\nbut this isn't really useful because we don't know <math>p_Y</math>; it's what we're trying to find.  We can make progress by considering the problem in the variable <math>X</math>.  <math>Y</math> takes a value in <math>S</math> whenever <math>X</math> takes a value in <math>\\phi^{-1}(S)</math>, so\n\n:<math>P(Y \\in S) = \\int_{\\phi^{-1}(S)} p_X(x)\\,dx.</math>\n\nChanging from variable <math>x</math> to <math>y</math> gives\n\n:<math>P(Y \\in S) = \\int_{\\phi^{-1}(S)} p_X(x)\\,dx = \\int_S p_X(\\phi^{-1}(y)) \\left|\\frac{d\\phi^{-1}}{dy}\\right|\\,dy.</math>\n\nCombining this with our first equation gives\n\n:<math>\\int_S p_Y(y)\\,dy = \\int_S p_X(\\phi^{-1}(y)) \\left|\\frac{d\\phi^{-1}}{dy}\\right|\\,dy,</math>\n\nso\n\n:<math>p_Y(y) = p_X(\\phi^{-1}(y)) \\left|\\frac{d\\phi^{-1}}{dy}\\right|.</math>\n\nIn the case where <math>X</math> and <math>Y</math> depend on several uncorrelated variables, i.e. <math>p_X=p_X(x_1, \\ldots, x_n)</math> and <math>y=\\phi(x)</math>, <math>p_Y</math> can be found by substitution in several variables discussed above. The result is\n\n:<math>p_Y(y) = p_X(\\phi^{-1}(y)) \\left|\\det D\\phi ^{-1}(y) \\right|.</math>\n\n==See also==\n*[[Probability density function]]\n*[[Substitution of variables]]\n*[[Tangent half-angle substitution]]\n*[[Trigonometric substitution]]\n\n==References==\n*{{citation|first=Anthony P.|last=Ferzola|url=http://mathdl.maa.org/mathDL/22/?pa=content&sa=viewDocument&nodeId=2688|title=Euler and differentials|journal=[[The College Mathematics Journal]]|volume=25|issue=2|year=1994|pages=102&ndash;111|doi=10.2307/2687130}}\n* {{citation|first=D.H.|last=Fremlin|title=Measure Theory, Volume 2|publisher=Torres Fremlin|year=2010|isbn=978-0-9538129-7-4}}.\n* {{citation|first1=Edwin|last1=Hewitt|first2=Karl|last2=Stromberg|authorlink1=Edwin Hewitt|title=Real and Abstract Analysis|publisher=Springer-Verlag|year=1965|isbn=978-0-387-04559-7}}.\n* {{citation|first=V.|last=Katz|title=Change of variables in multiple integrals: Euler to Cartan|journal=[[Mathematics Magazine]]|volume=55|year=1982|pages=3&ndash;11|doi=10.2307/2689856|issue=1}}\n* {{citation|first=Walter|last=Rudin|authorlink=Walter Rudin|title=Real and Complex Analysis|publisher=McGraw-Hill|year=1987|isbn=978-0-07-054234-1}}.\n* {{citation|first=Michael|last=Spivak|authorlink=Michael Spivak|title=Calculus on Manifolds|publisher=Westview Press|year=1965|isbn=978-0-8053-9021-6}}.\n\n==External links==\n{{Wikibooks|Calculus|Integration#The_Substitution_Rule|The Substitution Rule}}\n{{Wikiversity|Integration by Substitution}}\n* [https://www.encyclopediaofmath.org/index.php/Integration_by_substitution Integration by substitution] at [[Encyclopedia of Mathematics]]\n* [https://www.encyclopediaofmath.org/index.php/Area_formula Area formula] at [[Encyclopedia of Mathematics]]\n\n[[Category:Articles containing proofs]]\n[[Category:Integral calculus]]\n\n[[es:Métodos de integración#Método de integración por sustitución]]"
    },
    {
      "title": "Integration using Euler's formula",
      "url": "https://en.wikipedia.org/wiki/Integration_using_Euler%27s_formula",
      "text": "{{unreferenced|date=October 2016}}\nIn [[integral calculus]], [[complex number]]s and [[Euler's formula]] may be used to evaluate [[integral]]s involving [[trigonometric functions]]. Using Euler's formula, any trigonometric function may be written in terms of {{math|''e''<sup>''ix''</sup>}} and {{math|''e''<sup>−''ix''</sup>}}, and then integrated. This technique is often simpler and faster than using [[trigonometric identities]] or [[integration by parts]], and is sufficiently powerful to integrate any [[rational fraction|rational expression]] involving trigonometric functions.\n\n==Euler's formula==\nEuler's formula states that <ref>Weisstein, Eric W.(June 14 2017)[http://mathworld.wolfram.com/EulerFormula.html]</ref>\n:<math>e^{ix} = \\cos x + i\\,\\sin x.</math>\nSubstituting {{math|−''x''}} for {{math|''x''}} gives the equation\n:<math>e^{-ix} = \\cos x - i\\,\\sin x.</math>\nThese two equations can be solved for the sine and cosine:\n:<math>\\cos x = \\frac{e^{ix} + e^{-ix}}{2}\\quad\\text{and}\\quad\\sin x = \\frac{e^{ix}-e^{-ix}}{2i}.</math>\n\n==Simple example==\nConsider the integral\n:<math>\\int \\cos^2 x \\, dx.</math>\nThe standard approach to this integral is to use a [[half-angle formula]] to simplify the integrand. We can use Euler's identity instead:\n:<math>\\begin{align}\n\\int \\cos^2 x \\, dx \\,&=\\, \\int \\left(\\frac{e^{ix}+e^{-ix}}{2}\\right)^2 dx \\\\[6pt]\n&=\\, \\frac14\\int \\left( e^{2ix} + 2 +e^{-2ix} \\right) dx\n\\end{align}</math>\nAt this point, it would be possible to change back to real numbers using the formula {{math|''e''<sup>2''ix''</sup> + ''e''<sup>−2''ix''</sup> {{=}} 2 cos 2''x''}}. Alternatively, we can integrate the complex exponentials and not change back to trigonometric functions until the end:\n:<math>\\begin{align}\n\\frac14\\int \\left( e^{2ix} + 2 + e^{-2ix} \\right) dx \n&= \\frac14\\left(\\frac{e^{2ix}}{2i} + 2x - \\frac{e^{-2ix}}{2i}\\right)+C \\\\[6pt]\n&= \\frac14\\left(2x + \\sin 2x\\right) +C.\n\\end{align}</math>\n\n==Second example==\nConsider the integral\n:<math>\\int \\sin^2 x \\cos 4x \\, dx.</math>\nThis integral would be extremely tedious to solve using trigonometric identities, but using Euler's identity makes it relatively painless:\n:<math>\\begin{align}\n\\int \\sin^2 x \\cos 4x \\, dx \n&= \\int \\left(\\frac{e^{ix}-e^{-ix}}{2i}\\right)^2\\left(\\frac{e^{4ix}+e^{-4ix}}{2}\\right) dx \\\\[6pt]\n&= -\\frac18\\int \\left(e^{2ix} - 2 + e^{-2ix}\\right)\\left(e^{4ix}+e^{-4ix}\\right) dx \\\\[6pt]\n&= -\\frac18\\int \\left(e^{6ix} - 2e^{4ix} + e^{2ix} + e^{-2ix} - 2e^{-4ix} + e^{-6ix}\\right) dx.\n\\end{align}</math>\nAt this point we can either integrate directly, or we can first change the integrand to {{math|2 cos 6''x'' − 4 cos 4''x'' + 2 cos 2''x''}} and continue from there.\nEither method gives\n:<math>\\int \\sin^2 x \\cos 4x \\, dx = -\\frac{1}{24} \\sin 6x + \\frac18\\sin 4x - \\frac18 \\sin 2x + C.</math>\n\n==Using real parts==\nIn addition to Euler's identity, it can be helpful to make judicious use of the [[real part]]s of complex expressions. For example, consider the integral\n:<math>\\int e^x \\cos x \\, dx.</math>\nSince {{math|cos ''x''}} is the real part of {{math|''e''<sup>''ix''</sup>}}, we know that\n:<math>\\int e^x \\cos x \\, dx = \\operatorname{Re}\\int e^x e^{ix}\\, dx.</math>\nThe integral on the right is easy to evaluate:\n:<math>\\int e^x e^{ix} \\, dx = \\int e^{(1+i)x}\\,dx = \\frac{e^{(1+i)x}}{1+i} + C.</math>\nThus:\n:<math>\\begin{align}\n\\int e^x \\cos x \\, dx &= \\operatorname{Re}\\left(\\frac{e^{(1+i)x}}{1+i}\\right) + C \\\\[6pt]\n&= e^x\\operatorname{Re}\\left(\\frac{e^{ix}}{1+i}\\right) +C \\\\[6pt]\n&= e^x\\operatorname{Re}\\left(\\frac{e^{ix}(1-i)}{2}\\right) +C \\\\[6pt]\n&= e^x \\frac{\\cos x + \\sin x}{2} +C.\n\\end{align}</math>\n\n==Fractions==\nIn general, this technique may be used to evaluate any fractions involving trigonometric functions. For example, consider the integral\n:<math>\\int \\frac{1+\\cos^2 x}{\\cos x + \\cos 3x} \\, dx.</math>\nUsing Euler's identity, this integral becomes\n:<math>\\frac12 \\int \\frac{12 + e^{2ix} + e^{-2ix} }{e^{ix} + e^{-ix} + e^{3ix} + e^{-3ix}} \\, dx.</math>\nIf we now make the [[integration by substitution|substitution]] {{math|''u'' {{=}} ''e''<sup>''ix''</sup>}}, the result is the integral of a [[rational function]]:\n:<math>-\\frac{i}{2}\\int \\frac{1+12u^2 + u^4}{1 + u^2 + u^4 + u^6}\\,du.</math>\nAny [[rational function]] is integrable (using, for example, [[partial fractions in integration|partial fractions]]), and therefore any fraction involving trigonometric functions may be integrated as well.\n\n==References==\n{{Reflist}}\n\n[[Category:Integral calculus]]"
    },
    {
      "title": "Integration using parametric derivatives",
      "url": "https://en.wikipedia.org/wiki/Integration_using_parametric_derivatives",
      "text": "{{multipleissues|\n{{Unreferenced|date=August 2008}}\n{{Cleanup|date=December 2008}}\n{{context|date=June 2019}}}}\n\nIn [[mathematics]], '''[[integral|integration]] by parametric derivatives''' is a method of integrating certain functions.\n\n==Example==\nFor example, suppose we want to find the integral\n\n: <math>\\int_0^\\infty x^2 e^{-3x} \\, dx.</math>\n\nSince this is a product of two functions that are simple to integrate separately, repeated [[integration by parts]] is certainly one way to evaluate it. However, we may also evaluate this by starting with a simpler integral and an added parameter, which in this case is ''t''&nbsp;=&nbsp;3:\n\n: <math>\n\\begin{align}\n& \\int_0^\\infty e^{-tx} \\, dx = \\left[ \\frac{e^{-tx}}{-t} \\right]_0^\\infty = \\left( \\lim_{x \\to \\infty} \\frac{e^{-tx}}{-t} \\right) - \\left( \\frac{e^{-t0}}{-t} \\right) \\\\\n& =  0 - \\left( \\frac{1}{-t} \\right) = \\frac{1}{t}.\n\\end{align}\n</math>\n\nThis converges only for ''t''&nbsp;>&nbsp;0, which is true of the desired integral. Now that we know\n\n: <math>\\int_0^\\infty e^{-tx} \\, dx = \\frac{1}{t},</math>\n\nwe can differentiate both sides twice with respect to ''t'' (not ''x'') in order to add the factor of ''x''<sup>2</sup> in the original integral.\n\n: <math>\n\\begin{align}\n& \\frac{d^2}{dt^2} \\int_0^\\infty e^{-tx} \\, dx = \\frac{d^2}{dt^2} \\frac{1}{t} \\\\[10pt]\n& \\int_0^\\infty \\frac{d^2}{dt^2} e^{-tx} \\, dx = \\frac{d^2}{dt^2} \\frac{1}{t} \\\\[10pt]\n& \\int_0^\\infty \\frac{d}{dt} \\left (-x e^{-tx}\\right) \\, dx = \\frac{d}{dt} \\left(-\\frac{1}{t^2}\\right) \\\\[10pt]\n& \\int_0^\\infty x^2 e^{-tx} \\, dx = \\frac{2}{t^3}.\n\\end{align}\n</math>\n\nThis is the same form as the desired integral, where ''t''&nbsp;=&nbsp;3.  Substituting that into the above equation gives the value:\n\n: <math>\\int_0^\\infty x^2 e^{-3x} \\, dx = \\frac{2}{3^3} = \\frac{2}{27}.</math>\n\n[[Category:Integral calculus]]\n\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Lebesgue's decomposition theorem",
      "url": "https://en.wikipedia.org/wiki/Lebesgue%27s_decomposition_theorem",
      "text": "In [[mathematics]], more precisely in [[measure theory]], '''Lebesgue's decomposition theorem'''<ref>{{harv|Halmos|1974|loc=Section&nbsp;32, Theorem&nbsp;C}}</ref><ref>{{harv|Hewitt|Stromberg|1965|loc=Chapter&nbsp;V, §&nbsp;19, (19.42) Lebesgue Decomposition Theorem}}</ref><ref>{{harv|Rudin|1974|loc=Section&nbsp;6.9, The Theorem of Lebesgue-Radon-Nikodym}}</ref> states that for every two [[sigma-finite measure|σ-finite]] [[signed measure]]s <math>\\mu</math> and <math>\\nu</math> on a [[measurable space]] <math>(\\Omega,\\Sigma),</math>  there exist two σ-finite signed measures <math>\\nu_0</math> and <math>\\nu_1</math> such that:\n\n* <math>\\nu=\\nu_0+\\nu_1\\, </math>\n* <math>\\nu_0\\ll\\mu</math> (that is, <math>\\nu_0</math> is [[absolutely continuous]] with respect to <math>\\mu</math>)\n* <math>\\nu_1\\perp\\mu</math> (that is, <math>\\nu_1</math> and <math>\\mu</math> are [[singular measure|singular]]).\n\nThese two measures are uniquely determined by <math>\\mu</math> and <math>\\nu</math>.\n\n==Refinement==\nLebesgue's decomposition theorem can be refined in a number of ways.\n\nFirst, the decomposition of the [[singular measure|singular]] part of a regular [[Borel measure]] on the [[real line]] can be refined:<ref>{{harv|Hewitt|Stromberg|1965|loc=Chapter&nbsp;V, §&nbsp;19, (19.61) Theorem}}</ref>\n:<math>\\, \\nu = \\nu_{\\mathrm{cont}} + \\nu_{\\mathrm{sing}} + \\nu_{\\mathrm{pp}}</math>\nwhere\n* ''ν''<sub>cont</sub> is the '''absolutely continuous''' part\n* ''ν''<sub>sing</sub> is the '''singular continuous''' part\n* ''ν''<sub>pp</sub> is the '''pure point''' part (a [[discrete measure]]).\n\nSecond, absolutely continuous measures are classified by the [[Radon–Nikodym theorem]], and discrete measures are easily understood. Hence (singular continuous measures aside), Lebesgue decomposition gives a very explicit description of measures.  The [[Cantor measure]] (the [[probability measure]] on the [[real line]] whose [[cumulative distribution function]] is the [[Cantor function]]) is an example of a singular continuous measure.\n\n==Related concepts==\n\n===Lévy–Itō decomposition===\n{{main|Lévy–Itō decomposition}}\nThe analogous{{citation needed|date=January 2017}} decomposition for a [[stochastic processes]] is the [[Lévy–Itō decomposition]]: given a [[Lévy process]] ''X,'' it can be decomposed as a sum of three independent [[Lévy process]]es <math>X=X^{(1)}+X^{(2)}+X^{(3)}</math> where:\n* <math>X^{(1)}</math> is a [[Brownian motion]] with drift, corresponding to the absolutely continuous part;\n* <math>X^{(2)}</math> is a [[compound Poisson process]], corresponding to the pure point part;\n* <math>X^{(3)}</math> is a [[square integrable]] pure jump [[Martingale (probability theory)|martingale]] that almost surely has a countable number of jumps on a finite interval, corresponding to the singular continuous part.\n\n==See also==\n* [[Decomposition of spectrum (functional analysis)|Decomposition of spectrum]]\n* [[Hahn decomposition theorem]] and the corresponding Jordan decomposition theorem\n\n==Citations==\n{{Reflist|2}}\n\n==References==\n* {{Citation\n | last = Halmos\n | first = Paul R.\n | author-link = Paul Halmos\n | title = Measure Theory\n | place = New York, Heidelberg, Berlin\n | publisher = Springer-Verlag\n | series = [[Graduate Texts in Mathematics]]\n | volume = 18\n | origyear = 1950\n | year = 1974\n | isbn = 978-0-387-90088-9\n | mr = 0033869\n | zbl = 0283.28001}}\n* {{Citation\n | last = Hewitt\n | first = Edwin\n | author-link = Edwin Hewitt\n | last2 = Stromberg\n | first2 = Karl\n | title = Real and Abstract Analysis. A Modern Treatment of the Theory of Functions of a Real Variable\n | place = Berlin, Heidelberg, New York\n | publisher = Springer-Verlag\n | series = Graduate Texts in Mathematics\n | volume = 25\n | year = 1965\n | isbn = 978-0-387-90138-1\n | mr = 0188387\n | zbl = 0137.03202}}\n* {{Citation\n | last = Rudin\n | first = Walter\n | author-link = Walter Rudin\n | title = Real and Complex Analysis\n | place = New York, Düsseldorf, Johannesburg\n | publisher = McGraw-Hill Book Comp.\n | series = McGraw-Hill Series in Higher Mathematics\n | edition = 2nd\n | year = 1974\n | isbn = 0-07-054233-3\n | mr = 0344043\n | zbl = 0278.26001}}\n\n{{PlanetMath attribution|id=4003|title=Lebesgue decomposition theorem}}\n\n[[Category:Integral calculus]]\n[[Category:Theorems in measure theory]]"
    },
    {
      "title": "Lebesgue's density theorem",
      "url": "https://en.wikipedia.org/wiki/Lebesgue%27s_density_theorem",
      "text": "In [[mathematics]], '''Lebesgue's density theorem''' states that for any [[Lebesgue measure|Lebesgue measurable set]] <math>A\\subset \\R^n</math>, the \"density\" of ''A'' is 0 or 1 at [[almost everywhere|almost every]] point in <math>\\R^n</math>. Additionally, the \"density\" of ''A'' is 1 at almost every point in ''A''.   Intuitively, this means that the \"edge\" of ''A'', the set of points in ''A'' whose \"neighborhood\" is partially in ''A'' and partially outside of ''A'', is [[null set|negligible]].\n\nLet μ be the Lebesgue measure on  the [[Euclidean space]] '''R'''<sup>''n''</sup> and ''A'' be a Lebesgue measurable subset of '''R'''<sup>''n''</sup>. Define the '''approximate density''' of ''A'' in a ε-neighborhood of a point ''x''  in '''R'''<sup>''n''</sup> as\n\n:<math> d_\\varepsilon(x)=\\frac{\\mu(A\\cap B_\\varepsilon(x))}{\\mu(B_\\varepsilon(x))}</math>\n\nwhere ''B''<sub>ε</sub> denotes the [[closed ball]] of radius ε centered at ''x''.\n\n'''Lebesgue's density theorem''' asserts that for almost every point ''x'' of ''A'' the '''density'''\n\n:<math> d(x)=\\lim_{\\varepsilon\\to 0} d_{\\varepsilon}(x)</math>\n\nexists and is equal to 1.\n\nIn other words, for every measurable set ''A'', the density of ''A'' is 0 or 1 [[almost everywhere]] in '''R'''<sup>''n''</sup>.<ref>{{cite book| last = Mattila| first = Pertti|author-link = Pertti Mattila| title = Geometry of Sets and Measures in Euclidean Spaces: Fractals and Rectifiability| year = 1999| isbn = 978-0-521-65595-8 }}</ref> However, it is a curious fact that if μ(''A'')&nbsp;>&nbsp;0 and  {{nowrap|μ('''R'''<sup>''n''</sup>&thinsp;\\&thinsp;''A'') > 0}}, then there are always points of  '''R'''<sup>''n''</sup> where the density is neither 0 nor&nbsp;1.\n\nFor example, given a square in the plane, the density at every point inside the square is 1, on the edges is 1/2, and at the corners is 1/4.  The set of points in the plane at which the density is neither 0 nor 1 is non-empty (the square boundary), but it is negligible.\n\nThe Lebesgue density theorem is a particular case of the [[Lebesgue differentiation theorem]].\n\nThus, this theorem is also true for every finite Borel measure on '''R'''<sup>''n''</sup> instead of Lebesgue measure, see [[Lebesgue differentiation theorem#Discussion|Discussion]].\n\n== See also ==\n* [[Lebesgue differentiation theorem]]\n\n== References ==\n{{reflist}}\n* Hallard T. Croft. Three lattice-point problems of Steinhaus. ''Quart. J. Math. Oxford (2)'', 33:71-83, 1982.\n\n{{PlanetMath attribution|id=3869|title=Lebesgue density theorem}}\n\n[[Category:Theorems in measure theory]]\n[[Category:Integral calculus]]"
    },
    {
      "title": "Leibniz integral rule",
      "url": "https://en.wikipedia.org/wiki/Leibniz_integral_rule",
      "text": "{{About|the integral rule|the convergence test of alternating series|Alternating series test}}\n\n{{more citations needed|date=October 2016}}\n\nIn [[calculus]], '''Leibniz's rule''' for differentiation under the integral sign, named after [[Gottfried Leibniz]], states that for an [[integral]] of the form\n\n:<math>\\int_{a(x)}^{b(x)} f(x,t)\\,dt,</math>\n\nwhere <math>-\\infty < a(x), b(x) < \\infty</math>, the derivative of this integral is expressible as\n\n:<math>\\frac{d}{dx} \\left (\\int_{a(x)}^{b(x)} f(x,t)\\,dt \\right )= f\\big(x,b(x)\\big)\\cdot \\frac{d}{dx} b(x) - f\\big(x,a(x)\\big)\\cdot \\frac{d}{dx} a(x) + \\int_{a(x)}^{b(x)}\\frac{\\partial}{\\partial x} f(x,t) \\,dt,</math>\n\nwhere the [[partial derivative]] indicates that inside the integral, only the variation of ''f''(''x'', ''t'') with ''x'' is considered in taking the derivative.<ref>{{cite book |first=Murray H. |last=Protter |first2=Charles B., Jr. |last2=Morrey |chapter=Differentiation under the Integral Sign |title=Intermediate Calculus |location=New York |publisher=Springer |edition=Second |year=1985 |isbn=978-0-387-96058-6 |pages=421–426 |chapterurl=https://books.google.com/books?id=3lTmBwAAQBAJ&pg=PA421 }}</ref> Notice that if <math>a(x)</math> and <math>b(x)</math> are constants rather than [[Function (mathematics)|functions]] of <math>x</math>, we have a special case of Leibniz's rule:\n\n:<math>\\frac{d}{dx} \\left(\\int_{a}^{b} f(x,t)\\,dt \\right)= \\int_{a}^{b}\\frac{\\partial}{\\partial x} f(x,t) \\,dt.</math>\n\nThus under certain conditions, one may interchange the integral and partial differential [[operator (mathematics)|operators]]. This important result is particularly useful in the differentiation of [[integral transform]]s. An example of such is the [[moment generating function]] in  [[probability]] theory, a variation of the [[Laplace transform]], which can be differentiated to generate the [[moment (mathematics)|moments]] of a [[random variable]]. Whether Leibniz's integral rule applies is essentially a question about the interchange of [[limit (mathematics)|limits]].\n\n== General form: Differentiation under the integral sign ==\n\n:'''Theorem.''' Let ''f''(''x'', ''t'') be a function such that both ''f''(''x'', ''t'') and its partial derivative ''f<sub>x</sub>''(''x'', ''t'') are continuous in ''t'' and ''x'' in some region of the (''x'', ''t'')-plane, including ''a''(''x'') ≤ ''t'' ≤ ''b''(''x''), ''x''<sub>0</sub> ≤ ''x'' ≤ ''x''<sub>1</sub>. Also suppose that the functions ''a''(''x'') and ''b''(''x'') are both continuous and both have continuous derivatives for ''x''<sub>0</sub> ≤ ''x'' ≤ ''x''<sub>1</sub>.  Then, for ''x''<sub>0</sub> ≤ ''x'' ≤ ''x''<sub>1</sub>,\n::<math>\\frac{d}{dx} \\left (\\int_{a(x)}^{b(x)}f(x,t)\\,dt \\right) = f\\big(x,b(x)\\big)\\cdot \\frac{d}{dx} b(x) - f\\big(x,a(x)\\big)\\cdot \\frac{d}{dx} a(x) + \\int_{a(x)}^{b(x)}\\frac{\\partial}{\\partial x} f(x,t) \\,dt.</math>\n\nThis formula is the general form of the Leibniz integral rule and can be derived using the [[fundamental theorem of calculus]]. The (first) fundamental theorem of calculus is just the particular case of the above formula where ''a''(''x'') = ''a'', a constant, ''b''(''x'') = ''x'', and ''f''(''x'', ''t'') = ''f''(''t'').\n\nIf both upper and lower limits are taken as constants, then the formula takes the shape of an [[Operator (mathematics)|operator]] equation:\n\n::<math>\\mathcal{I}_t \\partial_x = \\partial_x \\mathcal{I}_t</math>\n\nwhere <math>\\partial_x</math> is the [[partial derivative]] with respect to <math>x</math> and <math>\\mathcal{I}_t</math> is the integral operator with respect to <math>t</math> over a fixed [[Interval (mathematics)|interval]]. That is, it is related to the [[symmetry of second derivatives]], but involving integrals as well as derivatives. This case is also known as the Leibniz integral rule.\n\nThe following three basic theorems on the [[interchange of limiting operations|interchange of limits]] are essentially equivalent:\n* the interchange of a derivative and an integral (differentiation under the integral sign; i.e., Leibniz integral rule);\n* the change of order of partial derivatives;\n* the change of order of integration (integration under the integral sign; i.e., [[Fubini's theorem]]).\n\n== Three-dimensional, time-dependent case ==\n{{See also|#Higher dimensions}}\n\n[[File:Vector field on a surface.svg|right|thumb|250px|Figure 1: A vector field '''F'''('''r''', ''t'') defined throughout space, and a surface Σ bounded by curve ∂Σ moving with velocity '''v''' over which the field is integrated.]]\nA Leibniz integral rule for a [[#Higher dimensions|two dimensional surface]] moving in three dimensional space is<ref name=\"Flanders\">{{cite journal |last=Flanders|first=Harly|author-link=Harley Flanders|date=June–July 1973|title=Differentiation under the integral sign|journal=[[American Mathematical Monthly]]|volume= 80|issue=6|pages= 615–627|jstor=2319163 |url=http://sgpwe.izt.uam.mx/files/users/uami/jdf/proyectos/Derivar_inetegral.pdf|doi=10.2307/2319163}}</ref>\n\n:<math>\\frac {d}{dt} \\iint_{\\Sigma (t)} \\mathbf{F} (\\mathbf{r}, t) \\cdot d \\mathbf{A} = \\iint_{\\Sigma (t)}\\left(\\mathbf{F}_t (\\mathbf{r}, t) + \\left[\\nabla \\cdot \\mathbf{F} (\\mathbf{r}, t) \\right] \\mathbf{v} \\right) \\cdot d \\mathbf{A} - \\oint_{\\partial \\Sigma (t)} \\left[ \\mathbf{v} \\times \\mathbf{F} ( \\mathbf{r}, t) \\right] \\cdot d \\mathbf{s},</math>\n\nwhere:\n:'''F'''('''r''', ''t'') is a vector field at the spatial position '''r''' at time ''t'',\n:Σ is a surface bounded by the closed curve ∂Σ,\n:''d'''''A''' is a vector element of the surface Σ,\n:''d'''''s''' is a vector element of the curve ∂Σ,\n:'''v''' is the velocity of movement of the region Σ,\n:∇⋅ is the vector [[divergence]],\n:× is the [[vector cross product]],\n:The double integrals are [[surface integral]]s over the surface Σ, and the [[line integral]] is over the bounding curve ∂Σ.\n\n== {{anchor|Higher dimensions|higher dimensions}} Higher dimensions == <!-- anchor used to redirect here from other articles -->\nThe Leibniz integral rule can be extended to multidimensional integrals. In two and three dimensions, this rule is better known from the field of [[fluid dynamics]] as the [[Reynolds transport theorem]]:\n\n:<math>\\frac{d}{dt} \\int_{D(t)} F(\\vec{\\textbf x}, t) \\,dV = \\int_{D(t)} \\frac{\\partial}{\\partial t} F(\\vec{\\textbf x}, t)\\,dV + \\int_{\\partial D(t)} F(\\vec{\\textbf x}, t) \\vec{\\textbf v}_b \\cdot d\\mathbf{\\Sigma},</math>\n\nwhere <math>F(\\vec{\\textbf x}, t)</math> is a scalar function, ''D''(''t'') and ∂''D''(''t'') denote a time-varying connected region of '''R'''<sup>3</sup> and its boundary, respectively, <math>\\vec{\\textbf v}_b</math> is the Eulerian velocity of the boundary (see [[Lagrangian and Eulerian coordinates]]) and ''d''&thinsp;'''Σ''' = '''n''' ''dS'' is the unit normal component of the [[surface integral|surface]] [[volume element|element]].\n\nThe general statement of the Leibniz integral rule requires concepts from [[Differential geometry and topology|differential geometry]], specifically [[differential forms]], [[exterior derivative]]s, [[wedge product]]s and [[interior product]]s. With those tools, the Leibniz integral rule in ''p''-dimensions is<ref name=\"Flanders\" />\n\n:<math>\\frac{d}{dt}\\int_{\\Omega(t)}\\omega=\\int_{\\Omega(t)} i_{\\vec{\\textbf v}}(d_x\\omega)+\\int_{\\partial \\Omega(t)} i_{\\vec{\\textbf v}} \\omega+\\int_{\\Omega(t)}\\dot{\\omega},</math>\n\nwhere Ω(''t'') is a time-varying domain of integration, ω is a ''p''-form, <math>\\vec{\\textbf v}=\\frac{\\partial\\vec{\\textbf x}}{\\partial t}</math> is the vector field of the velocity, <math>i_{\\vec{\\textbf v}}</math> denotes the [[interior product]] with <math>\\vec{\\textbf v}</math>, ''d''<sub>''x''</sub>ω is the [[exterior derivative]] of ω with respect to the space variables only and <math>\\dot{\\omega}</math> is the time derivative of ω.\n\nHowever, all of these identities can be derived from a most general statement about Lie derivatives:\n\n:<math>\\left.\\frac{d}{dt}\\right|_{t=0}\\int_{\\text{im}_{\\psi_t}(\\Omega)} \\omega = \\int_{\\Omega} \\mathcal{L}_{\\Psi} \\omega,</math>\n\nHere, the ambient manifold on which the differential form <math>\\omega</math> lives includes both space and time.\n:<math>\\Omega</math> is the region of integration (a submanifold) at a given instant (it does not depend on <math>t</math>, since its parametrization as a submanifold defines its position in time),\n:<math>\\mathcal{L}</math> is the [[Lie derivative]],\n:<math>\\Psi</math> is the spacetime vector field obtained from adding the unitary vector field in the direction of time to the purely spatial vector field <math>\\vec{\\textbf v}</math> from the previous formulas (i.e, <math>\\Psi</math> is the spacetime velocity of <math>\\Omega</math>),\n:<math>\\psi_t</math> is a diffeomorphism from the [[one-parameter group]] generated by the [[Flow (mathematics)|flow]] of <math>\\Psi</math>, and\n:<math>\\text{im}_{\\psi_t}(\\Omega)</math> is the [[Image (mathematics)|image]] of <math>\\Omega</math> under such diffeomorphism.\n\nSomething remarkable about this form, is that it can account for the case when <math>\\Omega</math> changes its shape and size over time, since such deformations are fully determined by <math>\\Psi</math>.\n\n== Measure theory statement ==\n\nLet <math>X</math> be an open subset of <math>\\mathbf{R}</math>, and <math>\\Omega</math> be a [[measure space]]. Suppose <math>f\\colon X \\times \\Omega \\rightarrow \\mathbf{R} </math> satisfies the following conditions:\n\n#<math>f(x,\\omega)</math> is a Lebesgue-integrable function of <math>\\omega</math> for each <math>x \\in X</math>.\n#For [[almost all]] <math>\\omega \\in \\Omega</math> , the derivative <math>f_x</math> exists for all <math>x \\in X</math>.\n#There is an integrable function <math>\\theta \\colon \\Omega \\rightarrow \\mathbf{R}</math> such that <math>|f_x(x,\\omega)| \\leq \\theta ( \\omega)</math> for all <math>x \\in X</math> and almost every <math>\\omega \\in \\Omega</math>.\n\nThen by the [[dominated convergence theorem]] for all <math>x \\in X</math>,\n:<math>\\frac{d}{dx} \\int_\\Omega f(x, \\omega) \\, d\\omega = \\int_{\\Omega} f_x (x, \\omega) \\, d\\omega.</math>\n\n== Proofs ==\n\n=== Proof of basic form ===\nLet\n\n:<math>u(x) = \\int_a^b f(x, t) \\,dt. \\qquad (1)</math>\n\nBy the definition of the derivative,\n\n:<math>u'(x) = \\lim_{h \\rightarrow 0} \\frac{u(x + h) - u(x)}{h}. \\qquad (2)</math>\n\nSubstitute equation (1) into equation (2).  The difference of two integrals equals the integral of the difference, and 1/''h'' is a constant, so\n\n:<math>\\begin{align}\nu'(x) &= \\lim_{h \\rightarrow 0} \\frac{\\int_a^bf(x + h, t)\\,dt - \\int_a^b f(x, t)\\,dt}{h} \\\\\n&= \\lim_{h \\rightarrow 0} \\frac{\\int_a^b\\left( f(x + h, t) - f(x,t) \\right)\\,dt}{h} \\\\\n&= \\lim_{h \\rightarrow 0} \\int_a^b \\frac{f(x + h, t) - f(x, t)}{h} \\,dt.\n\\end{align}</math>\n\nProvided that the limit can be passed through the integral sign, we obtain\n\n:<math>u'(x) = \\int_a^b f_x(x, t)\\,dt.</math>\n\nWe claim that the passage of the limit under the integral sign is valid by the bounded convergence theorem (a corollary of the [[dominated convergence theorem]]).  For each &delta; > 0, consider the [[difference quotient]]\n:<math>f_\\delta(x, t) = \\frac{f(x + \\delta, t) - f(x, t)}{\\delta}.</math>\nFor ''t'' fixed, the [[mean value theorem]] implies there exists z in the interval [''x'', ''x'' + &delta;] such that\n:<math>f_\\delta(x, t) = f_x(z, t).</math>\nContinuity of ''f''<sub>''x''</sub>(''x'', ''t'') and compactness of the domain together imply that ''f''<sub>''x''</sub>(''x'', ''t'') is bounded.  The above application of the mean value theorem therefore gives a uniform (independent of &delta;) bound on <math>f_\\delta(x, t)</math>.  The difference quotients converge pointwise to the partial derivative ''f''<sub>''x''</sub> by the assumption that the partial derivative exists.\n\nThe above argument shows that for every sequence {&delta;<sub>''n''</sub>} &rarr; 0, the sequence <math>\\{f_{\\delta_n}(x, t)\\}</math> is uniformly bounded and converges pointwise to ''f''<sub>''x''</sub>.  The bounded convergence theorem states that if a sequence of functions on a set of finite measure is uniformly bounded and converges pointwise, then passage of the limit under the integral is valid.  In particular, the limit and integral may be exchanged for every sequence {&delta;<sub>''n''</sub>} &rarr; 0.  Therefore, the limit as &delta; &rarr; 0 may be passed through the integral sign.\n\nFor a simpler proof using [[Fubini's theorem]], see the references.\n\n=== Variable limits form ===\nFor a [[Continuous function|continuous]] [[Real-valued function|real valued function]] ''g'' of one [[Function of a real variable|real variable]], and real valued [[Differentiable function|differentiable]] functions <math> f_1 </math> and <math> f_2 </math> of one real variable,\n\n:<math>\\frac{d}{dx} \\left( \\int_{f_1(x)}^{f_2(x)} g(t) \\,dt \\right )= g\\left(f_2(x)\\right) {f_2'(x)} -  g\\left(f_1(x)\\right) {f_1'(x)}.</math>\n\nThis follows from the [[chain rule]] and the [[Fundamental theorem of calculus#First part|First Fundamental Theorem of Calculus]]. Define\n:<math> G(x) = \\int_{f_1(x)}^{f_2(x)} g(t) \\,dt </math>,\nand\n:<math> \\Gamma(x) = \\int_{0}^{x} g(t) \\,dt </math>.                 (The lower limit just has to be some number in the domain of <math> g </math>)\n\nThen, <math> G(x) </math> can be written as a [[Function composition|composition]]: <math> G(x) = (\\Gamma \\circ f_2)(x) - (\\Gamma \\circ f_1)(x) </math>.\nThe [[Chain rule#Statement|Chain Rule]] then implies that\n:<math> G'(x) = \\Gamma'\\left(f_2(x)\\right) f_2'(x) - \\Gamma'\\left(f_1(x)\\right) f_1'(x) </math>.\nBy the [[Fundamental theorem of calculus#First part|First Fundamental Theorem of Calculus]], <math> \\Gamma'(x) = g(x) </math>. Therefore, substituting this result above, we get the desired equation:\n:<math> G'(x) = g\\left(f_2(x)\\right) {f_2'(x)} -  g\\left(f_1(x)\\right) {f_1'(x)} </math>.\n\n=== General form with variable limits ===\nSet\n\n:<math>\\varphi(\\alpha) = \\int_a^b f(x,\\alpha)\\,dx,</math>\n\nwhere ''a'' and ''b'' are functions of α that exhibit increments Δ''a'' and Δ''b'', respectively, when α is increased by Δα. Then,\n\n:<math>\\begin{align}\n\\Delta\\varphi &= \\varphi(\\alpha + \\Delta\\alpha) - \\varphi(\\alpha) \\\\\n&= \\int_{a + \\Delta a}^{b + \\Delta b}f(x, \\alpha + \\Delta\\alpha)\\,dx - \\int_a^b f(x, \\alpha)\\,dx \\\\\n&= \\int_{a + \\Delta a}^af(x, \\alpha + \\Delta\\alpha)\\,dx + \\int_a^bf(x, \\alpha + \\Delta\\alpha)\\,dx + \\int_b^{b + \\Delta b} f(x, \\alpha+\\Delta\\alpha)\\,dx - \\int_a^b f(x, \\alpha)\\,dx \\\\\n&= -\\int_a^{a + \\Delta a} f(x, \\alpha + \\Delta\\alpha)\\,dx + \\int_a^b [f(x, \\alpha + \\Delta\\alpha) - f(x,\\alpha)]\\,dx + \\int_b^{b + \\Delta b} f(x, \\alpha + \\Delta\\alpha)\\,dx.\n\\end{align}</math>\n\nA form of the [[mean value theorem]], <math>\\int_a^b f(x)\\,dx = (b - a)f(\\xi)</math>, where ''a'' < ξ < ''b'', may be applied to the first and last integrals of the formula for Δφ above, resulting in\n\n:<math>\\Delta\\varphi = -\\Delta a f(\\xi_1, \\alpha + \\Delta\\alpha) + \\int_a^b [f(x, \\alpha + \\Delta\\alpha) - f(x,\\alpha)]\\,dx + \\Delta b f(\\xi_2, \\alpha + \\Delta\\alpha).</math>\n\nDivide by Δα and let Δα → 0.  Notice ξ<sub>1</sub> → ''a'' and ξ<sub>2</sub> → ''b''.  We may pass the limit through the integral sign:\n\n:<math>\\lim_{\\Delta\\alpha\\to 0}\\int_a^b \\frac{f(x,\\alpha + \\Delta\\alpha) - f(x,\\alpha)}{\\Delta\\alpha}\\,dx = \\int_a^b \\frac{\\partial}{\\partial\\alpha}f(x, \\alpha)\\,dx,</math>\n\nagain by the bounded convergence theorem.  This yields the general form of the Leibniz integral rule,\n\n:<math>\\frac{d\\varphi}{d\\alpha} = \\int_a^b \\frac{\\partial}{\\partial\\alpha}f(x, \\alpha)\\,dx + f(b, \\alpha) \\frac{db}{d\\alpha} - f(a, \\alpha)\\frac{da}{d\\alpha}.</math>\n\n=== Alternative Proof of General Form with Variable Limits, using the Chain Rule ===\n\nThe general form of Leibniz's Integral Rule with variable limits can be derived as a consequence of the [[Leibniz integral rule#Proof of basic form|basic form]] of Leibniz's Integral Rule, the [[Chain rule#Higher dimensions|Multivariable Chain Rule]], and the [[Fundamental theorem of calculus#First part|First Fundamental Theorem of Calculus]]. Suppose <math> f </math> is defined in a rectangle in the <math> x-t </math> plane, for <math> x \\in [x_1, x_2] </math> and <math> t \\in [t_1, t_2] </math>. Also, assume <math> f </math> and the partial derivative <math> \\dfrac{\\partial f}{\\partial x} </math> are both continuous functions on this rectangle. Suppose <math> a, b</math> are [[Differentiable function#Differentiability of real functions of one variable|differentiable]] real valued functions defined on <math> [x_1, x_2]</math>, with values in <math> [t_1, t_2] </math> (i.e. for every <math> x \\in [x_1, x_2], a(x) , b(x) \\in [t_1, t_2] </math>). Now, set\n\n:<math> F(x,y) = \\int_{t_1}^{y} f(x,t)\\,dt  </math>, {{pad|10px}}  for <math> x \\in [x_1, x_2] </math> and  <math> y \\in [t_1, t_2] </math>\nand \n:<math> G(x) = \\int_{a(x)}^{b(x)} f(x,t)\\,dt  </math>, {{pad|10px}} for  <math> x \\in [x_1, x_2] </math>\n\nThen, by  properties of [[Integral#Conventions|Definite Integrals]], we can write\n:<math> \\begin{align} \nG(x) &= \\int_{t_1}^{b(x)} f(x,t)\\,dt  - \\int_{t_1}^{a(x)} f(x,t)\\,dt \\\\\n&= F(x, b(x)) - F(x, a(x))\n\\end{align}</math>\n\nSince the functions <math> F, a, b </math> are all differentiable (see the remark at the end of the proof), by the [[Chain rule#Higher dimensions|Multivariable Chain Rule]], it follows that <math> G </math> is differentiable, and it's derivative is given by the formula:\n:<math> G'(x) = \\left(\\dfrac{\\partial F}{\\partial x} \\left(x, b(x) \\right) + \\dfrac{\\partial F}{\\partial y} \\left(x, b(x) \\right) b'(x) \\right) - \n\\left(\\dfrac{\\partial F}{\\partial x} \\left(x, a(x) \\right) + \\dfrac{\\partial F}{\\partial y} \\left(x, a(x) \\right) a'(x) \\right) </math> {{pad|10px}}\n\nNow, note that for every <math> x \\in [x_1, x_2] </math>, and for every <math> y \\in [t_1, t_2] </math>, we have that <math> \\dfrac{\\partial F}{\\partial x}(x, y) = \\int_{t_1}^{y} \\dfrac{\\partial f}{\\partial x}(x,t) dt </math>, because when taking the partial derivative with respect to <math> x </math> of <math> F </math>, we are keeping <math> y </math> fixed in the expression <math> \\int_{t_1}^{y} f(x,t)\\,dt  </math>; thus the [[Leibniz integral rule#Proof of basic form|basic form]] of Leibniz's Integral Rule with constant limits of integration applies. Next, by the [[Fundamental theorem of calculus#First part|First Fundamental Theorem of Calculus]], we have that <math> \\dfrac{\\partial F}{\\partial y}(x, y) = f(x,y) </math>; because when taking the partial derivative with respect to <math> y </math> of <math> F </math>, the first variable <math> x </math> is fixed, so the fundamental theorem can indeed be applied.\n\nSubstituting these results into the equation for <math> G'(x) </math> above gives:\n:<math> \n\\begin{align}\nG'(x) &= \\left(\\int_{t_1}^{b(x)} \\dfrac{\\partial f}{\\partial x}(x,t) dt + f\\left(x, b(x) \\right) b'(x) \\right) - \n\\left(\\int_{t_1}^{a(x)} \\dfrac{\\partial f}{\\partial x}(x,t) dt + f\\left(x, a(x) \\right) a'(x) \\right) \\\\\n&= f\\left(x,b(x)\\right) b'(x) - f\\left(x,a(x)\\right) a'(x) + \\int_{a(x)}^{b(x)} \\dfrac{\\partial f}{\\partial x}(x,t) dt,\n\\end{align}\n</math>\nas desired.\n\nThere is a technical point in the proof above which is worth noting: applying the Chain Rule to <math> G </math> requires that <math> F </math> already be [[Differentiable function#Differentiability in higher dimensions|Differentiable]]. This is where we use our assumptions about <math> f </math>. As mentioned above, the partial derivatives of <math> F </math> are given by the formulas  <math> \\dfrac{\\partial F}{\\partial x}(x, y) = \\int_{t_1}^{y} \\dfrac{\\partial f}{\\partial x}(x,t) dt </math> and <math> \\dfrac{\\partial F}{\\partial y}(x, y) = f(x,y) </math>. Since <math> \\dfrac{\\partial f}{\\partial x}</math> is continuous, its integral is also a continuous function,<ref>{{cite book |last1=Spivak |first1=Michael |title=Calculus |date=1994 |publisher=Publish or Perish, Inc |location=Houston, Texas |isbn=978-0-914098-89-8 |pages=267–268 |edition=3}}</ref> and since <math> f </math> is also continuous, these two results show that both the partial derivatives of <math> F </math> are continuous. Since continuity of partial derivatives implies differentiability of the function,<ref>{{cite book |last1=Spivak |first1=Michael |title=Calculus on Manifolds |date=1965 |publisher=Addison-Wesley Publishing Company |isbn=978-0-8053-9021-6 |page=31}}</ref> <math> F </math> is indeed differentiable.\n\n=== Three-dimensional, time-dependent form ===\n{{See also|#Higher dimensions}}\n\nAt time ''t'' the surface Σ in [[#Three-dimensional, time-dependent case|Figure 1]] contains a set of points arranged about a centroid <math>\\mathbf{C}(t)</math>.  The function <math>\\mathbf{F}(\\mathbf{r}, t)</math> can be written as\n\n:<math>\\mathbf{F}(\\mathbf{C}(t) + \\mathbf{r} - \\mathbf{C}(t), t) = \\mathbf{F}(\\mathbf{C}(t) +\\mathbf{I}, t),</math>\n\nwith <math>\\mathbf{I}</math> independent of time. Variables are shifted to a new frame of reference attached to the moving surface, with origin at <math>\\mathbf{C}(t)</math>. For a rigidly translating surface, the limits of integration are then independent of time, so:\n\n:<math>\\frac {d}{dt} \\left (\\iint_{\\Sigma (t)} d \\mathbf{A}_{\\mathbf{r}}\\cdot \\mathbf{F}(\\mathbf{r}, t) \\right) = \\iint_\\Sigma d \\mathbf{A}_{\\mathbf{I}} \\cdot \\frac {d}{dt}\\mathbf{F}(\\mathbf{C}(t) + \\mathbf{I}, t),</math>\n\nwhere the limits of integration confining the integral to the region Σ no longer are time dependent so differentiation passes through the integration to act on the integrand only:\n\n:<math>\\frac {d}{dt}\\mathbf{F}( \\mathbf{C}(t) + \\mathbf{I}, t) = \\mathbf{F}_t(\\mathbf{C}(t) + \\mathbf{I}, t) + \\mathbf{v \\cdot \\nabla F}(\\mathbf{C}(t) + \\mathbf{I}, t) = \\mathbf{F}_t(\\mathbf{r}, t) + \\mathbf{v} \\cdot \\nabla \\mathbf{F}(\\mathbf{r}, t),</math>\n\nwith the velocity of motion of the surface defined by\n\n:<math>\\mathbf{v} = \\frac {d}{dt} \\mathbf{C} (t).</math>\n\nThis equation expresses the [[material derivative]] of the field, that is, the derivative with respect to a coordinate system attached to the moving surface. Having found the derivative, variables can be switched back to the original frame of reference. We notice that (see [[Curl (mathematics)#Three common examples|article on curl]])\n\n:<math>\\nabla \\times \\left(\\mathbf{v} \\times \\mathbf{F}\\right) = (\\nabla \\cdot \\mathbf{F} + \\mathbf{F} \\cdot \\nabla) \\mathbf{v}- (\\nabla \\cdot  \\mathbf{v} + \\mathbf{v} \\cdot \\nabla) \\mathbf{F},</math>\n\nand that [[Stokes theorem#Kelvin–Stokes theorem|Stokes theorem]] equates the surface integral of the curl over Σ with a line integral over ∂Σ:\n\n:<math>\\frac{d}{dt} \\left(\\iint_{\\Sigma (t)} \\mathbf{F} (\\mathbf{r}, t) \\cdot d \\mathbf{A}\\right) = \\iint_{\\Sigma (t)} \\big(\\mathbf{F}_t (\\mathbf{r}, t) + \\left(\\mathbf{F \\cdot \\nabla} \\right)\\mathbf{v} + \\left(\\nabla \\cdot \\mathbf{F} \\right) \\mathbf{v} - (\\nabla \\cdot \\mathbf{v})\\mathbf{F}\\big)\\cdot d\\mathbf{A} - \\oint_{\\partial \\Sigma (t)}\\left(\\mathbf{v} \\times \\mathbf{F}\\right)\\cdot d\\mathbf{s}. </math>\n\nThe sign of the line integral is based on the [[right-hand rule]] for the choice of direction of line element ''d'''''s'''. To establish this sign, for example, suppose the field '''F''' points in the positive ''z''-direction, and the surface Σ is a portion of the ''xy''-plane with perimeter ∂Σ. We adopt the normal to Σ to be in the positive ''z''-direction. Positive traversal of ∂Σ is then counterclockwise (right-hand rule with thumb along ''z''-axis). Then the integral on the left-hand side determines a ''positive'' flux of '''F''' through Σ. Suppose Σ translates in the positive ''x''-direction at velocity '''v'''. An element of the boundary of Σ parallel to the ''y''-axis, say ''d'''''s''', sweeps out an area '''v'''''t'' × ''d'''''s''' in time ''t''. If we integrate around the boundary ∂Σ in a counterclockwise sense, '''v'''''t'' × ''d'''''s''' points in the negative ''z''-direction on the left side of ∂Σ (where ''d'''''s''' points downward), and in the positive ''z''-direction on the right side of ∂Σ (where ''d'''''s''' points upward), which makes sense because Σ is moving to the right, adding area on the right and losing it on the left. On that basis, the flux of '''F''' is increasing on the right of ∂Σ and decreasing on the left. However, the dot product '''v''' × '''F •''' ''d'''''s''' = −'''F''' × '''v''' • ''d'''''s''' = −'''F • v''' × ''d'''''s'''. Consequently, the sign of the line integral is taken as negative.\n\nIf '''v''' is a constant,\n\n:<math>\\frac {d}{dt} \\iint_{\\Sigma (t)} \\mathbf{F} (\\mathbf{r}, t) \\cdot d \\mathbf{A} = \\iint_{\\Sigma (t)} \\big(\\mathbf{F}_t (\\mathbf{r}, t) +  \\left(\\nabla \\cdot \\mathbf{F} \\right)  \\mathbf{v}\\big) \\cdot d \\mathbf{A} - \\oint_{\\partial \\Sigma (t)}\\left(\\mathbf{v} \\times \\mathbf{F}\\right) \\cdot \\,d\\mathbf{s},</math>\n\nwhich is the quoted result. This proof does not consider the possibility of the surface deforming as it moves.\n\n=== Alternative derivation ===\n'''Lemma.''' One has:\n:<math>\\frac{\\partial}{\\partial b} \\left (\\int_a^b f(x) \\,dx \\right ) = f(b), \\qquad \\frac{\\partial}{\\partial a} \\left (\\int_a^b f(x) \\,dx \\right )= -f(a).</math>\n\n'''Proof.''' From [[Fundamental theorem of calculus#Proof of the first part|proof of the fundamental theorem of calculus]],\n\n:<math>\\begin{align}\n  \\frac{\\partial}{\\partial b} \\left (\\int_a^b f(x) \\,dx \\right ) &= \\lim_{\\Delta b \\to 0} \\frac{1}{\\Delta b} \\left[ \\int_a^{b+\\Delta b} f(x)\\,dx - \\int_a^b f(x)\\,dx \\right] \\\\\n    &= \\lim_{\\Delta b \\to 0} \\frac{1}{\\Delta b} \\int_b^{b+\\Delta b} f(x)\\,dx \\\\\n    &= \\lim_{\\Delta b \\to 0} \\frac{1}{\\Delta b} \\left[ f(b) \\Delta b + O\\left(\\Delta b^2\\right) \\right]    \\\\\n    &= f(b),\n\\end{align}</math>\nand\n:<math>\\begin{align}\n  \\frac{\\partial}{\\partial a} \\left (\\int_a^b f(x) \\,dx \\right )&= \\lim_{\\Delta a \\to 0} \\frac{1}{\\Delta a} \\left[ \\int_{a+\\Delta a}^b f(x)\\,dx - \\int_a^b f(x)\\,dx \\right] \\\\\n    &= \\lim_{\\Delta a \\to 0} \\frac{1}{\\Delta a} \\int_{a+\\Delta a}^a f(x)\\,dx \\\\\n    &= \\lim_{\\Delta a \\to 0} \\frac{1}{\\Delta a} \\left[ -f(a) \\Delta a + O\\left(\\Delta a^2\\right) \\right]\\\\\n    &= -f(a).\n\\end{align}</math>\n\nSuppose ''a'' and ''b'' are constant, and that ''f''(''x'') involves a parameter α which is constant in the integration but may vary to form different integrals. Assume that ''f''(''x'', α) is a continuous function of ''x'' and α in the compact set {(''x'', α) : α<sub>0</sub> ≤ α ≤ α<sub>1</sub> and ''a'' ≤ ''x'' ≤ ''b''}, and that the partial derivative ''f''<sub>α</sub>(''x'', α) exists and is continuous.  If one defines:\n\n:<math>\\varphi(\\alpha) = \\int_a^b f(x,\\alpha)\\,dx,</math>\n\nthen <math>\\varphi</math> may be differentiated with respect to α by differentiating under the integral sign, i.e.,\n\n:<math>\\frac{d\\varphi}{d\\alpha}=\\int_a^b\\frac{\\partial}{\\partial\\alpha}f(x,\\alpha)\\,dx.</math>\n\nBy the [[Heine–Cantor theorem]] it is uniformly continuous in that set. In other words, for any ε > 0 there exists Δα such that for all values of ''x'' in [''a'', ''b''],\n\n:<math>|f(x,\\alpha+\\Delta \\alpha)-f(x,\\alpha)|<\\varepsilon.</math>\n\nOn the other hand,\n\n:<math>\\begin{align}\n\\Delta\\varphi &=\\varphi(\\alpha+\\Delta \\alpha)-\\varphi(\\alpha) \\\\\n&=\\int_a^b f(x,\\alpha+\\Delta\\alpha)\\,dx - \\int_a^b f(x,\\alpha)\\, dx \\\\\n&=\\int_a^b \\left (f(x,\\alpha+\\Delta\\alpha)-f(x,\\alpha) \\right )\\,dx \\\\\n&\\leq \\varepsilon (b-a).\n\\end{align}</math>\n\nHence φ(α) is a continuous function.\n\nSimilarly if <math>\\frac{\\partial}{\\partial\\alpha} f(x,\\alpha)</math> exists and is continuous, then for all ε > 0 there exists Δα such that:\n\n:<math>\\forall x \\in [a, b], \\quad \\left|\\frac{f(x,\\alpha+\\Delta \\alpha)-f(x,\\alpha)}{\\Delta \\alpha} - \\frac{\\partial f}{\\partial\\alpha}\\right|<\\varepsilon.</math>\n\nTherefore,\n\n:<math>\\frac{\\Delta \\varphi}{\\Delta \\alpha}=\\int_a^b\\frac{f(x,\\alpha+\\Delta\\alpha)-f(x,\\alpha)}{\\Delta \\alpha}\\,dx = \\int_a^b \\frac{\\partial f(x,\\alpha)}{\\partial \\alpha}\\,dx + R,</math>\n\nwhere\n\n:<math>|R| < \\int_a^b \\varepsilon\\, dx = \\varepsilon(b-a).</math>\n\nNow, ε → 0 as Δα → 0, so\n\n:<math>\\lim_{{\\Delta \\alpha} \\rarr 0}\\frac{\\Delta\\varphi}{\\Delta \\alpha}= \\frac{d\\varphi}{d\\alpha} = \\int_a^b \\frac{\\partial}{\\partial \\alpha} f(x,\\alpha)\\,dx.</math>\n\nThis is the formula we set out to prove.\n\nNow, suppose\n\n:<math>\\int_a^b f(x,\\alpha)\\,dx=\\varphi(\\alpha),</math>\n\nwhere ''a'' and ''b'' are functions of α which take increments Δ''a'' and Δ''b'', respectively, when α is increased by Δα. Then,\n\n:<math>\\begin{align}\n\\Delta\\varphi &=\\varphi(\\alpha+\\Delta\\alpha)-\\varphi(\\alpha) \\\\\n&=\\int_{a+\\Delta a}^{b+\\Delta b}f(x,\\alpha+\\Delta\\alpha)\\,dx -\\int_a^b f(x,\\alpha)\\,dx \\\\\n&=\\int_{a+\\Delta a}^af(x,\\alpha+\\Delta\\alpha)\\,dx+\\int_a^bf(x,\\alpha+\\Delta\\alpha)\\,dx+\\int_b^{b+\\Delta b}f(x,\\alpha+\\Delta\\alpha)\\,dx -\\int_a^b f(x,\\alpha)\\,dx \\\\\n&=-\\int_a^{a+\\Delta a} f(x,\\alpha+\\Delta\\alpha)\\,dx+\\int_a^b[f(x,\\alpha+\\Delta\\alpha)-f(x,\\alpha)]\\,dx+\\int_b^{b+\\Delta b} f(x,\\alpha+\\Delta\\alpha)\\,dx.\n\\end{align}</math>\n\nA form of the [[mean value theorem]], <math>\\int_a^b f(x)\\,dx=(b-a)f(\\xi),</math> where ''a'' < ξ < ''b'', can be applied to the first and last integrals of the formula for Δφ above, resulting in\n\n:<math>\\Delta\\varphi=-\\Delta a\\,f(\\xi_1,\\alpha+\\Delta\\alpha)+\\int_a^b[f(x,\\alpha+\\Delta\\alpha)-f(x,\\alpha)]\\,dx+\\Delta b\\,f(\\xi_2,\\alpha+\\Delta\\alpha).</math>\n\nDividing by Δα, letting Δα → 0, noticing ξ<sub>1</sub> → ''a'' and ξ<sub>2</sub> → ''b'' and using the above derivation for\n\n:<math>\\frac{d\\varphi}{d\\alpha} = \\int_a^b\\frac{\\partial}{\\partial \\alpha} f(x,\\alpha)\\,dx</math>\n\nyields\n\n:<math>\\frac{d\\varphi}{d\\alpha} = \\int_a^b\\frac{\\partial}{\\partial \\alpha} f(x,\\alpha)\\,dx+f(b,\\alpha)\\frac{\\partial b}{\\partial \\alpha}-f(a,\\alpha)\\frac{\\partial a}{\\partial \\alpha}. </math>\n\nThis is the general form of the Leibniz integral rule.\n\n== Examples ==\n\n=== General examples ===\n\n==== Example 1 ====\nConsider the function\n:<math>\\varphi(\\alpha)=\\int_0^1\\frac{\\alpha}{x^2+\\alpha^2}\\,dx.</math>\n\nThe function under the integral sign is not continuous at the point (''x'', α) = (0, 0), and the function φ(α) has a discontinuity at α = 0 because φ(α) approaches ±π/2 as α → 0<sup>±</sup>.\n\nIf we differentiate φ(α) with respect to α under the integral sign, we get\n\n:<math>\\frac{d}{d\\alpha} \\varphi(\\alpha)=\\int_0^1\\frac{\\partial}{\\partial\\alpha}\\left(\\frac{\\alpha}{x^2+\\alpha^2}\\right)\\,dx=\\int_0^1\\frac{x^2-\\alpha^2}{(x^2+\\alpha^2)^2} dx=-\\frac{x}{x^2+\\alpha^2}\\bigg|_0^1=-\\frac{1}{1+\\alpha^2},</math>\n\nwhich is, of course, true for all values of α except α = 0.  This may be integrated (with respect to &alpha;) to find\n\n:<math>\\varphi(\\alpha) = \\begin{cases}\n0, & \\alpha = 0, \\\\ -\\arctan({\\alpha})+\\frac{\\pi}{2}, \n& \\alpha \\neq 0.\n\\end{cases}</math>\n\n==== Example 2 ====\nAn example with variable limits:\n\n:<math>\\begin{align}\n    \\frac{d}{dx} \\int_{\\sin x}^{\\cos x} \\cosh t^2\\,dt &= \\cosh\\left(\\cos^2 x\\right) \\frac{d}{dx}(\\cos x) - \\cosh\\left(\\sin^2 x\\right) \\frac{d}{dx} (\\sin x) + \\int_{\\sin x}^{\\cos x} \\frac{\\partial}{\\partial x} \\left (\\cosh t^2\\right )dt \\\\\n    &= \\cosh\\left(\\cos^2 x\\right) (-\\sin x) - \\cosh\\left(\\sin^2 x\\right) (\\cos x) + 0 \\\\\n    &= - \\cosh\\left(\\cos^2 x\\right) \\sin x - \\cosh\\left(\\sin^2 x\\right) \\cos x.\n\\end{align}</math>\n\n=== Examples for evaluating a definite integral ===\n\n==== Example 3 ====\nThe principle of differentiating under the integral sign may sometimes be used to evaluate a definite integral. Consider:\n\n:<math>\\varphi(\\alpha)=\\int_0^\\pi \\ln \\left (1-2\\alpha\\cos(x)+\\alpha^2 \\right )\\,dx, \\qquad |\\alpha| > 1.</math>\n\nNow,\n\n:<math>\\begin{align}\n    \\frac{d}{d\\alpha} \\varphi(\\alpha) &=\\int_0^\\pi \\frac{-2\\cos(x)+2\\alpha }{1-2\\alpha \\cos(x)+\\alpha^2}\\,dx \\\\\n    &=\\frac{1}{\\alpha}\\int_0^\\pi \\left(1-\\frac{1-\\alpha^2}{1-2\\alpha \\cos(x)+\\alpha^2} \\right)\\,dx   \\\\\n    &=\\left. \\frac{\\pi}{\\alpha}-\\frac{2}{\\alpha}\\left\\{\\arctan\\left(\\frac{1+\\alpha}{1-\\alpha} \\tan\\left(\\frac{x}{2}\\right)\\right) \\right\\} \\right|_0^\\pi.\n\\end{align}</math>\n\nAs ''x'' varies from 0 to π, we have\n\n:<math>\\begin{cases} \\frac{1+\\alpha}{1-\\alpha} \\tan\\left(\\tfrac{x}{2}\\right) \\geq 0, & |\\alpha| < 1, \\\\ \\frac{1+\\alpha}{1-\\alpha} \\tan \\left( \\frac{x}{2}\\right) \\leq 0, & |\\alpha| > 1. \\end{cases}</math>\n\nHence,\n\n:<math>\\left. \\arctan\\left(\\frac{1+\\alpha}{1-\\alpha}\\tan\\left(\\tfrac{x}{2}\\right)\\right)\\right|_0^\\pi= \\begin{cases} \\frac{\\pi}{2}, & |\\alpha| < 1, \\\\ -\\frac{\\pi}{2}, & |\\alpha| > 1. \\end{cases}</math>\n\nTherefore,\n\n:<math>\\frac{d}{d\\alpha} \\varphi(\\alpha)= \\begin{cases} 0, & |\\alpha| < 1, \\\\ \\frac{2\\pi}{\\alpha}, & |\\alpha| > 1. \\end{cases}</math>\n\nIntegrating both sides with respect to {{mvar|α}}, we get:\n\n:<math>\\varphi (\\alpha) = \\begin{cases} C_1, & |\\alpha| < 1, \\\\ 2\\pi \\ln |\\alpha| + C_2, & |\\alpha| > 1. \\end{cases}</math>\n\n''C''<sub>1</sub> = 0 follows from evaluating {{math|''φ''(0)}}:\n\n:<math>\\varphi(0) =\\int_0^\\pi \\ln(1)\\,dx =\\int_0^\\pi 0\\,dx=0.</math>\n\nTo determine ''C''<sub>2</sub> in the same manner, we should need to substitute in a value of α greater than 1 in φ(α). This is somewhat inconvenient. Instead, we substitute α = 1/β, where |β| < 1. Then,\n\n:<math>\\begin{align}\n\\varphi(\\alpha)    &=\\int_0^\\pi\\left(\\ln \\left (1-2\\beta \\cos(x)+\\beta^2 \\right )-2\\ln|\\beta|\\right)\\,dx \\\\\n    &= \\int_0^\\pi \\ln \\left (1-2\\beta \\cos(x)+\\beta^2 \\right )\\,dx -\\int_0^\\pi 2\\ln|\\beta| \\,dx \\\\\n    &=0-2\\pi\\ln|\\beta|   \\\\\n    &=2\\pi\\ln|\\alpha|.\n \\end{align}</math>\n\nTherefore, ''C''<sub>2</sub> = 0.\n\nThe definition of {{math|''φ''(''α'')}} is now complete:\n\n:<math>\\varphi (\\alpha) = \\begin{cases} 0, & |\\alpha| < 1, \\\\ 2\\pi \\ln |\\alpha|, & |\\alpha| > 1. \\end{cases}</math>\n\nThe foregoing discussion, of course, does not apply when {{math|''α'' {{=}} ±1}}, since the conditions for differentiability are not met.\n\n==== Example 4 ====\n:<math>\\textbf I = \\int_0^{\\pi/2} \\frac{1}{(a\\cos^2 x +b\\sin^2 x)^2}\\,dx,\\qquad a,b > 0.</math>\n\nFirst we calculate:\n\n:<math>\\begin{align}\n\\textbf{J} &= \\int_0^{\\pi/2} \\frac{1}{a\\cos^2 x + b \\sin^2 x}\\,dx \\\\\n&= \\int_0^{\\pi/2} \\frac{\\frac{1}{\\cos^2 x}}{a + b \\frac{\\sin^2 x}{\\cos^2 x}} \\,dx \\\\\n&= \\int_0^{\\pi/2} \\frac{\\sec^2 x}{a +b \\tan^2 x}\\,dx \\\\\n&= \\frac{1}{b} \\int_0^{\\pi/2} \\frac{1}{\\left(\\sqrt{\\frac{a}{b}}\\right)^2+\\tan^2 x}\\,d(\\tan x) \\\\\n&= \\frac{1}{\\sqrt{ab}}\\arctan \\left(\\sqrt{\\frac{b}{a}}\\tan x\\right) \\Bigg|_0^{\\pi/2} \\\\\n&=\\frac{\\pi}{2\\sqrt{ab}}.\n\\end{align}</math>\n\nThe limits of integration being independent of ''a'', we have:\n\n:<math>\\frac{\\partial \\textbf J}{\\partial a}=-\\int_0^{\\pi/2} \\frac{\\cos^2 x}{\\left(a\\cos^2 x+b \\sin^2 x\\right)^2}\\,dx</math>\n\nOn the other hand:\n\n:<math>\\frac{\\partial \\textbf J}{\\partial a}= \\frac{\\partial}{\\partial a} \\left(\\frac{\\pi}{2\\sqrt{ab}}\\right) =-\\frac{\\pi}{4\\sqrt{a^3b}}.</math>\n\nEquating these two relations then yields\n\n:<math>\\int_0^{\\pi/2} \\frac{\\cos^2 x}{\\left(a \\cos^2 x+b \\sin^2 x\\right)^2}\\,dx=\\frac{\\pi}{4\\sqrt{a^3b}}.</math>\n\nIn a similar fashion, pursuing <math>\\frac{\\partial \\textbf J}{\\partial b}</math> yields\n\n:<math>\\int_0^{\\pi/2}\\frac{\\sin^2 x}{\\left(a\\cos^2 x+b\\sin^2 x\\right)^2}\\,dx = \\frac{\\pi}{4\\sqrt{ab^3}}.</math>\n\nAdding the two results then produces\n\n:<math>\\textbf I = \\int_0^{\\pi/2}\\frac{1}{\\left(a\\cos^2x+b\\sin^2x\\right)^2}\\,dx=\\frac{\\pi}{4\\sqrt{ab}}\\left(\\frac{1}{a}+\\frac{1}{b}\\right),</math>\n\nwhich computes <math>\\textbf I</math> as desired.\n\nThis derivation may be generalized.  Note that if we define\n\n:<math>\\textbf I_n = \\int_0^{\\pi/2} \\frac{1}{\\left(a\\cos^2 x+b\\sin^2 x\\right)^n}\\,dx,</math>\n\nit can easily be shown that\n\n:<math>\\frac{\\partial\\textbf I_{n-1}}{\\partial a} + \\frac{\\partial \\textbf I_{n-1}}{\\partial b} + (n-1)\\textbf I_n = 0.</math>\n\nGiven '''I'''<sub>1</sub>, this integral reduction formula can be used to compute all of the values of '''I'''<sub>''n''</sub> for ''n'' > 1.\n\n==== Example 5 ====\nHere, we consider the integral\n\n:<math>\\textbf I(\\alpha)=\\int_0^{\\pi/2} \\frac{\\ln (1+\\cos\\alpha \\cos x)}{\\cos x}\\,dx, \\qquad 0 < \\alpha < \\pi.</math>\n\nDifferentiating under the integral with respect to α, we have\n\n:<math>\\begin{align}\n\\frac{d}{d\\alpha} \\textbf{I}(\\alpha) &= \\int_0^{\\pi/2} \\frac{\\partial}{\\partial\\alpha} \\left(\\frac{\\ln(1 + \\cos\\alpha \\cos x)}{\\cos x}\\right)\\,dx \\\\\n&=-\\int_0^{\\pi/2}\\frac{\\sin \\alpha}{1+\\cos \\alpha \\cos x}\\,dx \\\\\n&=-\\int_0^{\\pi/2}\\frac{\\sin \\alpha}{\\left(\\cos^2 \\frac{x}{2}+\\sin^2 \\frac{x}{2}\\right)+\\cos \\alpha \\left(\\cos^2 \\frac{x}{2}-\\sin^2 \\frac{x}{2}\\right)}\\,dx \\\\\n&=-\\frac{\\sin\\alpha}{1-\\cos\\alpha} \\int_0^{\\pi/2} \\frac{1}{\\cos^2\\frac{x}{2}} \\frac{1}{\\frac{1+\\cos \\alpha}{1-\\cos \\alpha} +\\tan^2 \\frac{x}{2}}\\,dx \\\\\n&=-\\frac{2\\sin\\alpha}{1-\\cos\\alpha} \\int_0^{\\pi/2} \\frac{\\frac{1}{2} \\sec^2 \\frac{x}{2}}{\\frac{2 \\cos^2 \\frac{\\alpha}{2}}{2 \\sin^2\\frac{\\alpha}{2}} + \\tan^2 \\frac{x}{2}} \\,dx \\\\\n&=-\\frac{2\\left(2 \\sin \\frac{\\alpha}{2} \\cos \\frac{\\alpha}{2}\\right)}{2 \\sin^2 \\frac{\\alpha}{2}} \\int_0^{\\pi/2} \\frac{1}{\\cot^2\\frac{\\alpha}{2} + \\tan^2 \\frac{x}{2}} d\\left(\\tan \\frac{x}{2}\\right)\\\\\n&=-2\\cot \\frac{\\alpha}{2}\\int_0^{\\pi/2} \\frac{1}{\\cot^2\\frac{\\alpha}{2} + \\tan^2\\frac{x}{2}}\\,d\\left(\\tan \\frac{x}{2}\\right)\\\\\n&=-2\\arctan \\left(\\tan \\frac{\\alpha}{2} \\tan \\frac{x}{2} \\right) \\bigg|_0^{\\pi/2}\\\\\n&=-\\alpha.\n\\end{align}</math>\n\nTherefore:\n\n:<math>\\textbf{I}(\\alpha) = C - \\frac{\\alpha^2}{2}.</math>\n\nHowever, by definition, '''I'''(π/2) = 0, hence ''C'' = π<sup>2</sup>/8 and\n\n:<math>\\textbf I(\\alpha) = \\frac{\\pi^2}{8}-\\frac{\\alpha^2}{2}.</math>\n\n==== Example 6 ====\nHere, we consider the integral\n\n:<math>\\int_0^{2\\pi}e^{\\cos\\theta} \\cos(\\sin\\theta)\\,d\\theta.</math>\n\nWe introduce a new variable φ and rewrite the integral as\n\n:<math>f(\\varphi) = \\int_0^{2\\pi} e^{\\varphi\\cos\\theta} \\cos(\\varphi\\sin\\theta)\\,d\\theta.</math>\n\nWhen φ = 1 this equals the original integral.  However, this more general integral may be differentiated with respect to φ:\n\n:<math>\\begin{align}\n\\frac{df}{d\\varphi} &= \\int_0^{2\\pi} \\frac{\\partial}{\\partial\\varphi}\\left(e^{\\varphi\\cos\\theta} \\cos(\\varphi\\sin\\theta)\\right)\\,d\\theta \\\\\n    &= \\int_0^{2\\pi} e^{\\varphi\\cos\\theta} (\\cos\\theta\\cos(\\varphi\\sin\\theta)-\\sin\\theta\\sin(\\varphi\\sin\\theta))\\,d\\theta.\n\\end{align}</math>\n\nThis is the line integral of <math>F(x,y)=(e^{\\varphi x}\\sin(ty),e^{\\varphi x}\\cos(ty))</math>over the unit circle. By Green's Theorem, it equals the double integral over the unit disk of <math>\\partial F_2/\\partial x-\\partial F_1/\\partial y</math> which equals 0.This implies that ''f''(φ) is constant.  The constant may be determined by evaluating ''f'' at φ = 0:\n\n:<math>f(0) = \\int_0^{2\\pi} 1\\,d\\theta = 2\\pi.</math>\nTherefore, the original integral also equals 2π.\n\n==== Other problems to solve ====\nThere are innumerable other integrals that can be solved using the technique of differentiation under the integral sign. For example, in each of the following cases, the original integral may be replaced by a similar integral having a new parameter α:\n\n:<math>\\begin{align}\n\\int_0^\\infty \\frac{\\sin x}{x}\\,dx &\\to \\int_0^\\infty e^{-\\alpha x} \\frac{\\sin x}{x}\\,dx,\\\\\n\\int_0^{\\pi/2} \\frac{x}{\\tan x}\\,dx &\\to\\int_0^{\\pi/2} \\frac{\\tan^{-1}(\\alpha \\tan x)}{\\tan x}\\,dx,\\\\\n\\int_0^\\infty \\frac{\\ln (1+x^2)}{1+x^2}\\,dx &\\to\\int_0^\\infty \\frac{\\ln (1+\\alpha^2 x^2)}{1+x^2}\\,dx \\\\\n\\int_0^1 \\frac{x-1}{\\ln x}\\,dx &\\to \\int_0^1 \\frac{x^\\alpha-1}{\\ln x}\\,dx.\n\\end{align}</math>\n\nThe first integral, the [[Dirichlet integral]], is absolutely convergent for positive α but only conditionally convergent when α is 0. Therefore, differentiation under the integral sign is easy to justify when α > 0, but proving that the resulting formula remains valid when α is 0 requires some careful work.\n\n=== Applications to series ===\nThe measure-theoretic version of differentiation under the integral sign also applies to summation (finite or infinite) by interpreting summation as [[counting measure]].  An example of an application is the fact that power series are differentiable in their radius of convergence.\n\n== In popular culture ==\nDifferentiation under the integral sign is mentioned in the late [[physicist]] [[Richard Feynman]]'s best-selling memoir ''[[Surely You're Joking, Mr. Feynman!]]'' in the chapter \"A Different Box of Tools\".  He describes learning it, while in [[high school]], from an old text, ''Advanced Calculus'' (1926), by [[Frederick S. Woods]] (who was a professor of mathematics in the [[Massachusetts Institute of Technology]]).  The technique was not often taught when Feynman later received his formal education in [[calculus]], but using this technique, Feynman was able to solve otherwise difficult integration problems upon his arrival at graduate school at [[Princeton University]]:\n\n<blockquote>\nOne thing I never did learn was [[Methods of contour integration|contour integration]]. I had learned to do integrals by various methods shown in a book that my high school physics teacher Mr. Bader had given me. One day he told me to stay after class. \"Feynman,\" he said, \"you talk too much and you make too much noise. I know why. You're bored. So I'm going to give you a book. You go up there in the back, in the corner, and study this book, and when you know everything that's in this book, you can talk again.\" So every physics class, I paid no attention to what was going on with Pascal's Law, or whatever they were doing. I was up in the back with this book: [https://books.google.com/books/about/Advanced_calculus.html?id=94MZAQAAIAAJ \"Advanced Calculus\"], by Woods. Bader knew I had studied [https://archive.org/details/calulusforthepra000526mbp \"Calculus for the Practical Man\"] a little bit, so he gave me the real works—it was for a junior or senior course in college. It had [[Fourier series]], [[Bessel function]]s, [[determinant]]s, [[elliptic function]]s—all kinds of wonderful stuff that I didn't know anything about. That book also showed how to differentiate parameters under the integral sign—it's a certain operation. It turns out that's not taught very much in the universities; they don't emphasize it. But I caught on how to use that method, and I used that one damn tool again and again. So because I was self-taught using that book, I had peculiar methods of doing integrals. The result was, when guys at MIT or [[Princeton University|Princeton]] had trouble doing a certain integral, it was because they couldn't do it with the standard methods they had learned in school. If it was contour integration, they would have found it; if it was a simple series expansion, they would have found it. Then I come along and try differentiating under the integral sign, and often it worked. So I got a great reputation for doing integrals, only because my box of tools was different from everybody else's, and they had tried all their tools on it before giving the problem to me.\n</blockquote>\n\n== See also ==\n* [[Chain rule]]\n* [[Differentiation of integrals]]\n* [[Leibniz rule (generalized product rule)]]\n* [[Reynolds transport theorem]], a generalization of Leibniz rule\n\n== References ==\n{{Reflist}}\n\n== Further reading ==\n* {{cite book|title=Advanced Calculus|author=Frederick S. Woods|publisher=Ginn and Company| edition=New|year=1934|asin=B0006AMNBI}}\n* {{cite book|title=Advanced Calculus|author=Frederick S. Woods|publisher=Ginn and Company| edition=1st|year=1926|asin= B00085L67S}}\n* {{cite book |title=Advanced Calculus|author=David V. Widder|publisher=Dover Publications Inc.|edition= New |date=Jul 1990|isbn=978-0-486-66103-2}}\n\n== External links ==\n* [http://math.hawaii.edu/~rharron/teaching/MAT203/LeibnizRule.pdf \"The Leibniz Rule\" by Rob Harron]\n\n[[Category:Gottfried Leibniz]]\n[[Category:Multivariable calculus]]\n[[Category:Integral calculus]]\n[[Category:Differential calculus]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Limits of integration",
      "url": "https://en.wikipedia.org/wiki/Limits_of_integration",
      "text": "{{Unreferenced|date=July 2007}}\n\nIn [[calculus]] and [[mathematical analysis]] the '''limits of integration''' of the [[integral]]\n:<math> \\int_a^b f(x) \\, dx </math>\nof a [[Riemann integral|Riemann integrable]] [[function (mathematics)|function]] ''f'' defined on a [[closed set|closed]] and [[bounded set|bounded]] [interval] are the [[real number]]s ''a'' and ''b''.\n\n==Improper integrals==\n\n'''Limits of integration''' can also be defined for [[improper integral]]s, with the limits of integration of both\n:<math> \\lim_{z \\rightarrow a^+} \\int_z^b f(x) \\, dx</math>\nand\n:<math> \\lim_{z \\rightarrow b^-} \\int_a^z f(x) \\, dx</math>\nagain being ''a'' and ''b''. For an [[improper integral]]\n:<math> \\int_a^\\infty f(x) \\, dx </math>\nor\n:<math> \\int_{-\\infty}^b f(x) \\, dx </math>\nthe limits of integration are ''a'' and ∞, or &minus;∞ and ''b'', respectively.\n\n==See also==\n\n* [[Integral]]\n* [[Riemann integration]]\n\n[[Category:Integral calculus]]\n[[Category:Real analysis]]\n\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Linearity of integration",
      "url": "https://en.wikipedia.org/wiki/Linearity_of_integration",
      "text": "In [[calculus]], the [[integral]] of any [[linear combination]] of [[function (mathematics)|function]]s equals the same linear combination of the integrals of the functions; this property is known as '''linearity of integration'''.<ref>{{citation|title=Fourier and Laplace Transforms|first=R. J.|last=Beerends|publisher=Cambridge University Press|year=2003|isbn=9780521534413|url=https://books.google.com/books?id=frT5_rfyO4IC&pg=PA149|page=149}}.</ref> It is a fundamental property of the [[integral]] that encapsulates in a single rule two simpler rules of integration, the [[sum rule in integration|sum rule]] (the integral of the sum of two functions equals the sum of the integrals) and the [[constant factor rule in integration|constant factor rule]] (the integral of a constant multiple of a function equals a constant multiple of the integral).<ref>{{citation|title=Integral Calculus Made Easy|first=Deepak|last=Bhardwaj|publisher=Laxmi Publications|year=2006|isbn=9788170089339|page=8|url=https://books.google.com/books?id=CD966TcWrQIC&pg=PA8}}.</ref> Linearity of integration is related to the linearity of [[summation]], since integrals are thought of as infinite sums.\n\n==Statement and derivation==\nLet {{math|''f''}} and {{math|''g''}} be functions. Now consider:\n\n:<math>\\int (af(x)+bg(x))\\, dx.</math>\n\nBy the [[sum rule in integration]], this is\n\n:<math>\\int af(x)\\, dx+\\int bg(x)\\, dx.</math>\n\nBy the [[constant factor rule in integration]], this reduces to\n\n:<math>a\\int f(x)\\, dx+b\\int g(x)\\, dx.</math>\n\nHence we have\n\n:<math>\\int (af(x)+bg(x))\\, dx=a\\int f(x)\\, dx+b\\int g(x)\\, dx.</math>\n\nIt is also possible to infer linearity of integration as a consequence of [[linearity of differentiation]].<ref>{{citation|title=Practical Analysis in One Variable|series=[[Undergraduate Texts in Mathematics]]|first=Donald|last=Estep|publisher=Springer|year=2002|isbn=9780387954844|pages=303–304|url=https://books.google.com/books?id=trC-jTRffesC&pg=PA303}}.</ref>\n\n==Operator notation==\nThe [[differential operator]] is linear&nbsp;— if we use the Heaviside '''D''' notation to denote this, we may extend '''D'''<sup>−1</sup> to mean the first [[integral]].<ref>{{citation|title=Advanced Engineering Mathematics|first1=R. K.|last1=Jain|first2=S. R. K.|last2=Iyengar|publisher=Alpha Science Int'l Ltd.|year=2004|isbn=9781842651858|page=393|url=https://books.google.com/books?id=crOxJNLE5psC&pg=PA393}}.</ref> To say that '''D'''<sup>−1</sup> is ''therefore'' linear requires a moment to discuss the [[arbitrary constant of integration]];  '''D'''<sup>−1</sup> would be straightforward to show linear if the arbitrary constant of integration could be set to zero.\n\nAbstractly, we can say that '''D''' is a [[linear transformation]] from some vector space ''V'' to another one, ''W''. We know that '''D'''(''c'') = 0 for any constant function ''c''. We can by general theory ([[mean value theorem]])identify the subspace ''C'' of ''V'', consisting of all constant functions as the whole kernel of  '''D'''. Then by [[linear algebra]] we can establish that '''D'''<sup>−1</sup> is a well-defined linear transformation that is bijective on Im '''D''' and takes values in ''V''/''C''.\n\nThat is, we treat the ''arbitrary constant of integration'' as a notation for a [[coset]] ''f''&nbsp;+&nbsp;''C''; and all is well with the argument.\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Linearity Of Integration}}\n[[Category:Integral calculus]]"
    },
    {
      "title": "Localization theorem",
      "url": "https://en.wikipedia.org/wiki/Localization_theorem",
      "text": "In [[mathematics]], particularly in [[integral calculus]], the '''localization theorem''' allows, under certain conditions, to infer the nullity of a [[function (mathematics)|function]] given only information about its [[continuity (mathematics)|continuity]] and the value of its integral.\n\nLet {{math|<var>F</var>(<var>x</var>)}} be a [[real-valued function]] defined on some open [[interval (mathematics)|interval]] <var>Ω</var> of the [[real line]] that is [[Continuous function|continuous]] in <var>Ω</var>. Let <var>D</var> be an arbitrary subinterval contained in <var>Ω</var>. The theorem states the following implication:\n\n: <math>\\int\\limits_D F(x) \\, \\mathrm{d}x = 0 ~ \\forall D \\subset \\Omega ~ \\Rightarrow ~ F(x) = 0 ~ \\forall x \\in \\Omega</math>\n\n[[File:Localization Theorem.svg|float|right]]\nA simple proof is as follows: if there were a point <var>x</var><sub>0</sub> within  <var>Ω</var> for which {{math|<var>F</var>(<var>x</var><sub>0</sub>) &ne; 0}}, then the continuity of {{math|<var>F</var>}} would require the existence of a [[neighborhood (mathematics)|neighborhood]] of <var>x</var><sub>0</sub> in which the value of {{math|<var>F</var>}} was nonzero, and in particular of the same sign than in <var>x</var><sub>0</sub>. Since such a neighborhood <var>N</var>, which can be taken to be arbitrarily small, must however be of a nonzero width on the real line, the integral of {{math|<var>F</var>}} over <var>N</var> would evaluate to a nonzero value. However, since <var>x</var><sub>0</sub> is part of the ''open'' set <var>Ω</var>, all neighborhoods of <var>x</var><sub>0</sub> smaller than the distance of <var>x</var><sub>0</sub> to the frontier of <var>Ω</var> are included within it, and so the integral of {{math|<var>F</var>}} over them must evaluate to zero. Having reached the contradiction that {{math|∫<sub><var>N</var></sub><var>F</var>(<var>x</var>) <var>dx</var>}} must be both zero and nonzero, the initial hypothesis must be wrong, and thus there is no <var>x</var><sub>0</sub> in  <var>Ω</var> for which {{math|<var>F</var>(<var>x</var><sub>0</sub>) &ne; 0}}.\n\nThe theorem is easily generalized to [[multivariate calculus|multivariate function]]s, replacing intervals with the more general concept of connected [[open set]]s, that is, [[Domain (mathematics)#Real and complex analysis|domain]]s, and the original function with some {{math|<var>F</var>(<var>'''x'''</var>) : '''R'''<sup>''n''</sup>&rarr;'''R'''}}, with the constraints of continuity and nullity of its integral over any subdomain {{math|<var>D</var>&sub;<var>Ω</var>}}. The proof is completely analogous to the single variable case, and concludes with the impossibility of finding a point {{math|<var>'''x'''</var><sub>0</sub> &isin; <var>Ω</var>}} such that {{math|<var>F</var>(<var>'''x'''</var><sub>0</sub>) &ne; 0}}.\n\n{{clear}}\n\n==Example==\nAn example of the use of this theorem in physics is the law of [[conservation of mass]] for fluids, which states that the mass of any fluid volume must not change:\n\n: <math>\\frac{\\mathrm{d}}{\\mathrm{d}t}\\int\\limits_{V_f} \\rho(\\vec x, t) \\, \\mathrm{d}\\Omega = 0</math>\n\nApplying the [[Reynolds transport theorem]], one can change the reference to an arbitrary (non-fluid) [[control volume]] <var>V<sub>c</sub></var>.  Further assuming that the [[density|density function]] is continuous (i.e. that our fluid is monophasic and thermodynamically metastable) and that <var>V<sub>c</sub></var> is not moving relative to the chosen system of reference, the equation becomes:\n\n: <math>\\int\\limits_{V_c} \\left [ {{\\partial \\rho} \\over {\\partial t}} + \\nabla \\cdot (\\rho \\vec v) \\right ] \\, \\mathrm{d}\\Omega = 0</math>\n\nAs the equation holds for ''any'' such control volume, the localization theorem applies, rendering the common [[partial differential equation]] for the conservation of mass in monophase fluids:\n\n: <math>{\\partial \\rho \\over \\partial t} + \\nabla \\cdot (\\rho \\vec v) = 0</math>\n\n[[Category:Integral calculus]]"
    },
    {
      "title": "Locally integrable function",
      "url": "https://en.wikipedia.org/wiki/Locally_integrable_function",
      "text": "In [[mathematics]], a '''locally integrable function''' (sometimes also called '''locally summable function''')<ref>According to {{harvtxt|Gel'fand|Shilov|1964|p=3}}.</ref> is a [[function (mathematics)|function]] which is integrable (so its integral is finite) on every [[compact subset]] of its [[domain (mathematics)#Domain of a function|domain of definition]]. The importance of such functions lies in the fact that their [[function space]] is similar to [[Lp space|{{math|''L''<sup>''p''</sup>}} spaces]], but its members are not required to satisfy any growth restriction on their behavior at the boundary of their domain (at infinity if the domain is unbounded): in other words, locally integrable functions can grow arbitrarily fast at the domain boundary, but are still manageable in a way similar to ordinary integrable functions.\n\n== Definition ==\n\n===Standard definition===\n{{EquationRef|1|Definition 1}}.<ref name=\"ScVl\">See for example {{Harv|Schwartz|1998|p=18}} and {{Harv|Vladimirov|2002|p=3}}.</ref> Let {{math|Ω}} be an [[open set]] in  the [[Euclidean space]] {{math|ℝ''<sup>n</sup>''}} and  {{math|''f'' : Ω → ℂ}} be a [[Lebesgue measure|Lebesgue]] [[measurable function]]. If {{math|''f''}} on {{math|Ω}} is such that\n\n:<math> \\int_K | f |\\, \\mathrm{d}x <+\\infty,</math>\n\ni.e. its [[Lebesgue integral]] is finite on all [[compact set|compact subsets]] {{math|''K''}} of {{math|Ω}},<ref>Another slight variant of this definition, chosen by {{harvtxt|Vladimirov|2002|p=1}}, is to require only that {{math|''K'' ⋐ Ω}} (or, using the notation of {{harvtxt|Gilbarg|Trudinger|2001|p=9}}, {{math|''K'' ⊂⊂ Ω}}), meaning that {{math|''K''}} ''is strictly included in'' {{math|Ω}} i.e. it is a set having compact [[Closure (topology)|closure]] [[subset|strictly included]] in the given ambient set.</ref> then {{math|''f''}}&thinsp; is called ''locally integrable''. The [[Set (mathematics)|set]] of all such functions is denoted by {{math|''L''<sub>1,loc</sub>(Ω)}}:\n\n:<math>L_{1,\\mathrm{loc}}(\\Omega)=\\bigl\\{f:\\Omega\\to\\mathbb{C}\\text{ measurable}\\,\\big|\\, f|_K \\in L_1(K)\\ \\forall\\, K \\subset \\Omega,\\, K \\text{ compact}\\bigr\\},</math>\n\nwhere <math display=inline>\\left.f\\right|_K</math> denotes the [[restriction of a function|restriction]] of {{math|''f''}}&thinsp; to the set {{math|''K''}}.\n\nThe classical definition of a locally integrable function involves only [[Measure theory|measure theoretic]] and [[Topological space|topological]]<ref>The notion of compactness must obviously be defined on the given abstract measure space.</ref> concepts and can be carried over abstract to [[Complex number|complex-valued]] functions on a topological [[measure space]] {{math|(''X'',&thinsp;Σ,&thinsp;''μ'')}}:<ref>This is the approach developed for example by {{harvtxt|Cafiero|1959|pp=285–342}} and by {{harvtxt|Saks|1937|loc = chapter I}}, without dealing explicitly with the locally integrable case.</ref> however, since the most common application of such functions is to [[Distribution (mathematics)|distribution theory]] on Euclidean spaces,<ref name=\"ScVl\"/> all the definitions in this and the following sections deal explicitly only with this important case.\n\n===An alternative definition===\n{{EquationRef|2|Definition 2}}.<ref>See for example {{Harv|Strichartz|2003|pp=12–13}}.</ref> Let {{math|Ω}} be an open set in the Euclidean space {{math|ℝ''<sup>n</sup>''}}. Then a [[Function (mathematics)|function]] {{math|''f'' : Ω → ℂ}} such that\n\n:<math> \\int_\\Omega | f \\varphi|\\, \\mathrm{d}x <+\\infty,</math>\n\nfor each [[test function]] {{math|''φ'' ∈ {{SubSup|C|c|∞}}(Ω)}} is called ''locally integrable'', and the set of such functions is denoted  by {{math|''L''<sub>1,loc</sub>(Ω)}}. Here {{math|{{SubSup|C|c|∞}}(Ω)}} denotes the set of all infinitely differentiable functions {{math|''φ'' : Ω → ℝ}} with [[Support (mathematics)#Compact support|compact support]] contained in {{math|Ω}}.\n\nThis definition has its roots in the approach to measure and integration theory based on the concept of [[Continuous linear functional#Continuous linear functionals|continuous linear functional]] on a [[topological vector space]], developed by [[Nicolas Bourbaki]] and his school:<ref>This approach was praised by {{harvtxt|Schwartz|1998|pp=16–17}} who remarked also its usefulness, however using {{EquationNote|1|Definition&nbsp;1}} to define locally integrable functions.</ref> it is also the one adopted by {{Harvtxt|Strichartz|2003}} and by {{Harvtxt|Maz'ya|Shaposhnikova|2009|p=34}}.<ref>Be noted that Maz'ya and Shaposhnikova define explicitly only the \"localized\" version of the [[Sobolev space]] {{math|''W''<sup>''k'',''p''</sup>(Ω)}}, nevertheless explicitly asserting that the same method is used to define localized versions of all other [[Banach space]]s used in the cited book: in particular,  {{math|''L''<sub>''p'',loc</sub>(Ω)}} is introduced on page 44.</ref> This \"distribution theoretic\" definition is equivalent to the standard one, as the following lemma proves:\n\n{{EquationRef|3|Lemma 1}}. A given function {{math|''f'' : Ω → ℂ}} is locally integrable according to {{EquationNote|1|Definition&nbsp;1}} if and only if it is locally integrable according to {{EquationNote|2|Definition&nbsp;2}}, i.e.\n\n:<math> \\int_K | f |\\, \\mathrm{d}x <+\\infty \\quad \\forall\\, K \\subset \\Omega,\\, K \\text{ compact} \\quad \\Longleftrightarrow \\quad \n\\int_\\Omega | f \\varphi|\\, \\mathrm{d}x <+\\infty \\quad \\forall\\, \\varphi \\in C^\\infty_{\\mathrm{c}}(\\Omega).</math>\n\n<div style=\"clear:both;width:95%;\" class=\"NavFrame\">\n<div class=\"NavHead\" style=\"background-color:#FFFAF0; text-align:left; font-size:larger;\">Proof of {{EquationNote|3|Lemma&nbsp;1}}</div>\n<div class=\"NavContent\" style=\"text-align:left;display:none;\">\n\n'''If part''':  Let {{math|''φ'' ∈ {{SubSup|C|c|∞}}(Ω)}} be a test function. It is [[Extreme value theorem|bounded]] by its [[supremum norm]] {{math|<nowiki>||</nowiki>''φ''<nowiki>||</nowiki><sub>∞</sub>}}, measurable, and has a [[Support (mathematics)#Compact support|compact support]], let's call it {{math|''K''}}. Hence\n\n:<math>\\int_\\Omega | f \\varphi|\\, \\mathrm{d}x = \\int_K |f|\\,|\\varphi|\\, \\mathrm{d}x \\le\\|\\varphi\\|_\\infty\\int_K | f |\\, \\mathrm{d}x<\\infty</math>\n\nby {{EquationNote|1|Definition&nbsp;1}}.\n\n'''Only if part''': Let {{math|''K''}} be a compact subset of the open set {{math|Ω}}. We will first construct a test function {{math|''φ<sub>K</sub>'' ∈ {{SubSup|C|c|∞}}(Ω)}} which majorises the [[indicator function]] {{math|''χ<sub>K</sub>''}} of {{math|''K''}}.\nThe [[Distance#Distances between sets and between a point and a set|usual set distance]]<ref>Not to be confused with the [[Hausdorff distance]].</ref> between {{math|''K''}} and the [[Boundary (topology)|boundary]] {{math|∂Ω}} is strictly greater than zero, i.e.\n\n:<math>\\Delta:=d(K,\\partial\\Omega)>0,</math>\n\nhence it is possible to choose a [[real number]] {{math|''δ''}} such that {{math|Δ > 2''δ'' > 0}} (if {{math|∂Ω}} is the empty set, take {{math|Δ {{=}} ∞}}). Let {{math|''K<sub>δ</sub>''}} and {{math|''K''<sub>2''δ''</sub>}} denote the [[Closure (topology)#Closure of a set|closed]] [[Neighbourhood (mathematics)#In a metric space|{{math|''δ''}}-neighborhood]] and {{math|2''δ''}}-neighborhood of {{math|''K''}}, respectively.  They are likewise compact and satisfy\n\n:<math>K\\subset K_\\delta\\subset K_{2\\delta}\\subset\\Omega,\\qquad d(K_\\delta,\\partial\\Omega)=\\Delta-\\delta>\\delta>0.</math>\n\nNow use [[convolution]] to define the function {{math|''φ<sub>K</sub>'' : Ω → ℝ}} by\n\n:<math>\\varphi_K(x)={\\chi_{K_\\delta}\\ast\\varphi_\\delta(x)}=\n\\int_{\\mathbb{R}^n}\\chi_{K_\\delta}(y)\\,\\varphi_\\delta(x-y)\\,\\mathrm{d}y,</math>\n\nwhere {{math|''φ<sub>δ</sub>''}} is a [[mollifier]] constructed by using the [[Mollifier#Concrete example|standard positive symmetric one]]. Obviously {{math|''φ<sub>K</sub>''}} is non-negative in the sense that {{math|''φ<sub>K</sub>'' ≥ 0}}, infinitely differentiable, and its support is contained in {{math|''K''<sub>2''δ''</sub>}}, in particular it is a test function. Since {{math|''φ<sub>K</sub>''(''x'') {{=}} 1}} for all {{math|''x'' ∈ ''K''}}, we have that {{math|''χ<sub>K</sub>'' ≤ ''φ<sub>K</sub>''}}.\n\nLet {{math|''f''}}&thinsp; be a locally integrable function according to {{EquationNote|2|Definition&nbsp;2}}. Then\n\n:<math>\\int_K|f|\\,\\mathrm{d}x=\\int_\\Omega|f|\\chi_K\\,\\mathrm{d}x\n\\le\\int_\\Omega|f|\\varphi_K\\,\\mathrm{d}x<\\infty.\n</math>\n\nSince this holds for every compact subset {{math|''K''}} of {{math|Ω}}, the function {{math|''f''}}&thinsp; is locally integrable according to {{EquationNote|1|Definition&nbsp;1}}. □\n</div>\n</div>\n\n===Generalization: locally ''p''-integrable functions===\n{{EquationRef|4|Definition 3}}.<ref name=\"Vlp3\">See for example {{Harv|Vladimirov|2002|p=3}} and {{harv|Maz'ya|Poborchi|1997|p=4}}.</ref> Let {{math|Ω}} be an open set in  the Euclidean space ℝ''<sup>n</sup>'' and  {{math|''f'' : Ω → }}ℂ be a Lebesgue measurable function. If, for a given {{math|''p''}} with {{math|1 ≤ ''p'' ≤ +∞}}, {{math|''f''}} satisfies\n\n:<math> \\int_K | f|^p \\,\\mathrm{d}x <+\\infty,</math>\n\ni.e., it belongs to [[Lp space|{{math|''L''<sub>''p''</sub>(''K'')}}]] for all [[compact set|compact subsets]] {{math|''K''}} of {{math|Ω}}, then {{math|''f''}} is called ''locally'' {{math|''p''}}-''integrable'' or also {{math|''p''}}-''locally integrable''.<ref name=\"Vlp3\"/> The [[Set (mathematics)|set]] of all such functions is denoted by {{math|''L''<sub>''p'',loc</sub>(Ω)}}:\n\n:<math>L_{p,\\mathrm{loc}}(\\Omega)=\\left\\{f:\\Omega\\to\\mathbb{C}\\text{ measurable }\\left|\\ f|_K \\in L_p(K),\\ \\forall\\, K \\subset \\Omega, K \\text{ compact}\\right.\\right\\}.</math>\n\nAn alternative definition, completely analogous to the one given for locally integrable functions, can also be given for locally {{math|''p''}}-integrable functions: it can also be and proven equivalent to the one in this section.<ref>As remarked in the previous section, this is the approach adopted by {{harvtxt|Maz'ya|Shaposhnikova|2009}}, without developing the elementary details.</ref> Despite their apparent higher generality, locally {{math|''p''}}-integrable functions form a subset of locally integrable functions for every {{math|''p''}} such that {{math|1 < ''p'' ≤ +∞}}.<ref>Precisely, they form a [[vector subspace]] of {{math|''L''<sub>1,loc</sub>(Ω)}}: see {{EquationNote|7|Corollary&nbsp;1}} to {{EquationNote|6|Theorem&nbsp;2}}.</ref>\n\n=== Notation ===\nApart from the different [[glyph]]s which may be used for the uppercase \"L\",<ref>See for example {{Harv|Vladimirov|2002|p=3}}, where a calligraphic '''&#x2112;''' is used.</ref> there are few variants for the notation of the set of locally integrable functions\n*<math>L^p_{\\mathrm{loc}}(\\Omega),</math> adopted by {{harv|Hörmander|1990|p=37}}, {{Harv|Strichartz|2003|pp=12–13}} and {{Harv|Vladimirov|2002|p=3}}.\n*<math>L_{p,\\mathrm{loc}}(\\Omega),</math> adopted by {{harv|Maz'ya|Poborchi|1997|p=4}} and {{Harvtxt|Maz'ya|Shaposhnikova|2009|p=44}}.\n*<math>L_p(\\Omega,\\mathrm{loc}),</math> adopted by {{harv|Maz'ja|1985|p=6}} and {{harv|Maz'ya|2011|p=2}}.\n\n== Properties ==\n\n===''L''<sub>''p'',loc</sub> is a complete metric space for all ''p'' ≥ 1===\n{{EquationRef|5|Theorem 1}}.<ref>See {{harv|Gilbarg|Trudinger|1998|p=147}}, {{harv|Maz'ya|Poborchi|1997|p=5}} for a statement of this results, and also the brief notes in {{harv|Maz'ja|1985|p=6}} and {{harv|Maz'ya|2011|p=2}}.</ref> {{math|''L''<sub>''p'',loc</sub>}} is a [[Complete metric space|complete metrizable space]]: its topology can be generated by the following [[Metric (mathematics)|metric]]:\n:<math>d(u,v)=\\sum_{k\\geq 1}\\frac{1}{2^k}\\frac{\\Vert u - v\\Vert_{p,\\omega_k}}{1+\\Vert u - v\\Vert_{p,\\omega_k}}\\qquad u, v\\in L_{p,\\mathrm{loc}}(\\Omega),</math>\nwhere {{math|{''ω''<sub>''k''</sub>}<sub>''k''≥1</sub>}} is a family of non empty open sets such that\n* {{math|''ω''<sub>''k''</sub> ⊂⊂ ''ω''<sub>''k''+1</sub>}}, meaning that {{math|''ω''<sub>''k''</sub>}} ''is strictly included in'' {{math|''ω''<sub>''k''+1</sub>}} i.e. it is a set having compact closure strictly included in the set of higher index.\n* {{math|∪<sub>''k''</sub>''ω''<sub>''k''</sub> {{=}} Ω}}.\n* <math>\\scriptstyle{\\Vert\\cdot\\Vert_{p,\\omega_k}}\\to\\mathbb{R}^+</math>, ''k'' ∈ ℕ is an [[indexed family]] of [[seminorm]]s, defined as\n::<math> {\\Vert u \\Vert_{p,\\omega_k}} = \\left (\\int_{\\omega_k} | u(x)|^p \\,\\mathrm{d}x\\right)^{1/p}\\qquad\\forall\\, u\\in L_{p,\\mathrm{loc}}(\\Omega).</math>\n\nIn references {{harv|Gilbarg|Trudinger|1998|p=147}}, {{harv|Maz'ya|Poborchi|1997|p=5}}, {{harv|Maz'ja|1985|p=6}} and {{harv|Maz'ya|2011|p=2}}, this theorem is stated but not proved on a formal basis:<ref>{{harvtxt|Gilbarg|Trudinger|1998|p=147}} and {{harvtxt|Maz'ya|Poborchi|1997|p=5}} only sketch very briefly the method of proof, while in {{harv|Maz'ja|1985|p=6}} and {{harv|Maz'ya|2011|p=2}} it is assumed as a known result, from which the subsequent development starts.</ref> a complete proof of a more general result, which includes it, is found in {{harv|Meise|Vogt|1997|p=40}}.\n\n===''L''<sub>''p''</sub> is a subspace of ''L''<sub>1,loc</sub> for all ''p'' ≥ 1===\n{{EquationRef|6|Theorem 2}}. Every function {{math|''f''}} belonging to {{math|''L''<sub>''p''</sub>(Ω)}}, {{math|1 ≤ ''p'' ≤ +∞}}, where {{math|Ω}} is an [[open subset]] of ℝ''<sup>n</sup>'', is locally integrable.\n\n'''Proof'''. The case {{math|''p'' {{=}} 1}} is trivial, therefore in the sequel of the proof it is assumed that {{math|1 < ''p'' ≤ +∞}}. Consider the [[Indicator function|characteristic function]] {{math|''χ''<sub>''K''</sub>}} of a compact subset {{math|''K''}} of  {{math|Ω}}: then, for {{math|''p'' ≤ +∞}},\n\n:<math>\\left|{\\int_\\Omega|\\chi_K|^q\\,\\mathrm{d}x}\\right|^{1/q}=\\left|{\\int_K \\mathrm{d}x}\\right|^{1/q}=|K|^{1/q}<+\\infty,</math>\n\nwhere\n*{{math|''q''}} is a [[positive number]] such that {{math|1/''p'' + 1/''q''}} = {{math|1}} for a given {{math|1 ≤ ''p'' ≤ +∞}}\n*{{math|<nowiki>|</nowiki>''K''<nowiki>|</nowiki>}} is the [[Lebesgue measure]] of the [[compact set]] {{math|''K''}}\nThen by [[Hölder's inequality]], the [[Product (mathematics)|product]] {{math|''fχ''<sub>''K''</sub>}} is [[Integrable function|integrable]] i.e. belongs to {{math|''L''<sub>1</sub>(Ω)}} and\n\n:<math>{\\int_K|f|\\,\\mathrm{d}x}={\\int_\\Omega|f\\chi_K|\\,\\mathrm{d}x}\\leq\\left|{\\int_\\Omega|f|^p\\,\\mathrm{d}x}\\right|^{1/p}\\left|{\\int_K \\mathrm{d}x}\\right|^{1/q}=\\|f\\|_p|K|^{1/q}<+\\infty,</math>\n\ntherefore\n\n:<math>f\\in L_{1,\\mathrm{loc}}(\\Omega).</math>\n\nNote that since the following inequality is true\n\n:<math>{\\int_K|f|\\,\\mathrm{d}x}={\\int_\\Omega|f\\chi_K|\\,\\mathrm{d}x}\\leq\\left|{\\int_K|f|^p \\,\\mathrm{d}x}\\right|^{1/p}\\left|{\\int_K \\mathrm{d}x}\\right|^{1/q}=\\|f\\|_p|K|^{1/q}<+\\infty,</math>\n\nthe theorem is true also for functions {{math|''f''}} belonging only to the space of locally {{math|''p''}}-integrable functions, therefore the theorem implies also the following result.\n\n{{EquationRef|7|Corollary 1}}. Every function <math> f </math> in <math>L_{p,loc}(\\Omega)</math>, <math> 1<p\\leq\\infty </math>, is locally integrable, i. e. belongs to <math> L_{1,loc}(\\Omega) </math>.\n\n<b>Note:</b> If <math> \\Omega </math> is an [[open subset]] of <math> \\mathbb{R}^n</math> that is also bounded, then one has the standard inclusion <math> L_p(\\Omega) \\subset L_1(\\Omega)</math> which makes sense given the above inclusion <math> L_1(\\Omega)\\subset L_{1,loc}(\\Omega)</math>. But the first of these statements is not true if <math> \\Omega </math> is not bounded; then it is still true that <math> L_p(\\Omega) \\subset L_{1,loc}(\\Omega)</math> for any <math>p</math>, but not that <math> L_p(\\Omega)\\subset L_1(\\Omega) </math>. To see this, one typically considers the function <math> u(x)=1 </math>, which is in <math> L_{\\infty}(\\mathbb{R}^n) </math> but not in <math> L_p(\\mathbb{R}^n)</math> for any finite <math>p</math>.\n\n=== ''L''<sub>1,loc</sub> is the space of densities of absolutely continuous measures===\n\n{{EquationRef|7|Theorem 3}}. A function {{math|''f''}} is the [[Density function (measure theory)|density]] of an [[Absolute continuity#Absolute continuity of measures|absolutely continuous measure]] if and only if <math> f\\in L_{1,loc}</math>.\n\nThe proof of this result is sketched by {{harv|Schwartz|1998|p=18}}. Rephrasing its statement, this theorem asserts that every locally integrable function defines an absolutely continuous measure and conversely that every absolutely continuous measures defines a locally integrable function: this is also, in the abstract measure theory framework, the form of the important [[Radon–Nikodym theorem]] given by [[Stanisław Saks]] in his treatise.<ref>According to {{harvtxt|Saks|1937|p=36}}, \"''If {{math|E}} is a set of finite measure, or, more generally the sum of a sequence of sets of finite measure ''(''{{math|μ}}'')'', then, in order that an additive function of a set ''({{math|&#x1D51B;}})'' on {{math|E}} be absolutely continuous on {{math|E}}, it is necessary and sufficient that this function of a set be the indefinite integral of some integrable function of a point of {{math|E}}''\". Assuming ({{math|''μ''}}) to be the Lebesgue measure, the two statements can be seen to be equivalent.</ref>\n\n==Examples==\n*The constant function {{math|1}} defined on the real line is locally integrable but not globally integrable since the real line has infinite measure. More generally, [[constant (mathematics)|constants]], [[continuous function]]s<ref>See for example {{harv|Hörmander|1990|p=37}}.</ref> and [[integrable function]]s are locally integrable.<ref>See {{harv|Strichartz|2003|p=12}}.</ref>\n*The function <math>f(x) = 1/x</math> for x ∈ (0, 1) is locally but not globally integrable on (0, 1). It is locally integrable since any compact set K ⊆ (0, 1) has positive distance from 0 and f is hence bounded on K. This example underpins the initial claim that locally integrable functions do not require the satisfaction of growth conditions near the boundary in bounded domains.\n* The function\n\n::<math>\nf(x)=\n\\begin{cases}\n1/x &x\\neq 0,\\\\\n0 & x=0,\n\\end{cases} \\quad x \\in \\mathbb R\n</math>\n: is not locally integrable in {{math|''x'' {{=}} 0}}: it is indeed locally integrable near this point since its integral over every compact set not including it is finite. Formally speaking, {{math|1/''x'' ∈ ''L''<sub>1,loc</sub>}}(ℝ&nbsp;\\&nbsp;0):<ref>See {{harv|Schwartz|1998|p=19}}.</ref> however, this function can be extended to a distribution on the whole ℝ as a [[Cauchy principal value]].<ref>See {{Harv|Vladimirov|2002|pp=19–21}}.</ref>\n* The preceding example raises a question: does every function which is locally integrable in {{math|Ω}} ⊊ ℝ admit an extension to the whole ℝ as a distribution? The answer is negative, and a counterexample is provided by the following function:\n:: <math>\nf(x)= \n\\begin{cases}\ne^{1/x} &x\\neq 0,\\\\\n0 & x=0,\n\\end{cases}\n</math>\n: does not define any distribution on ℝ.<ref>See {{Harv|Vladimirov|2002|p=21}}.</ref>  \n* The following example, similar to the preceding one, is a function belonging to {{math|''L''<sub>1,loc</sub>}}(ℝ&nbsp;\\&nbsp;0) which serves as an elementary [[counterexample]] in the application of the theory of distributions to [[differential operator]]s with [[Irregular singularity|irregular singular coefficients]]:\n:: <math>\nf(x)= \n\\begin{cases}\nk_1 e^{1/x^2} &x>0,\\\\\n0 & x=0,\\\\\nk_2 e^{1/x^2} &x<0,\n\\end{cases}\n</math>\n:where {{math|''k''<sub>1</sub>}} and {{math|''k''<sub>2</sub>}} are [[Complex number|complex constants]], is a general solution of the following elementary [[Fuchsian differential equation|non-Fuchsian differential equation]] of first order \n::<math>x^3\\frac{\\mathrm{d}f}{\\mathrm{d}x}+2f=0.</math>\n:Again it does not define any distribution on the whole ℝ, if {{math|''k''<sub>1</sub>}} or {{math|''k''<sub>2</sub>}} are not zero: the only distributional global solution of such equation is therefore the zero distribution, and this shows how, in this branch of the theory of differential equations, the methods of the theory of distributions cannot be expected to have the same success achieved in other branches of the same theory, notably in the theory of linear differential equations with constant coefficients.<ref>For a brief discussion of this example, see {{harv|Schwartz|1998|pp=131–132}}.</ref>\n\n== Applications ==\n\nLocally integrable functions play a prominent role in [[Distribution (mathematics)|distribution theory]] and they occur in the definition of various classes of [[function (mathematics)|functions]] and [[function space]]s, like [[Bounded variation|functions of bounded variation]]. Moreover, they appear in the [[Radon–Nikodym theorem]] by characterizing the absolutely continuous part of every measure.\n\n== See also ==\n*[[Compact set]]\n*[[Distribution (mathematics)]]\n*[[Lebesgue's density theorem]]\n*[[Lebesgue differentiation theorem]]\n*[[Lebesgue integral]]\n*[[Lp space]]\n\n==Notes==\n{{reflist|29em}}\n\n==References==\n*{{Citation\n| last = Cafiero\n| first = Federico\n| author-link = Federico Cafiero\n| title = Misura e integrazione\n| place = [[Rome|Roma]]\n| publisher = Edizioni Cremonese\n| year = 1959\n| series = Monografie matematiche del [[Consiglio Nazionale delle Ricerche]]\n| volume = 5\n| pages =  VII+451\n| id = \n| mr = 0215954 \n| zbl = 0171.01503\n| language = Italian\n}}. ''Measure and integration'' (as the English translation of the title reads) is a definitive monograph on integration and measure theory: the treatment of the limiting behavior of the integral of various kind of [[sequences]] of measure-related structures (measurable functions, [[measurable set]]s, measures and their combinations) is somewhat conclusive.\n*{{Citation\n | last1 = Gel'fand\n | first1 = I. M. \n | author-link = Israel Gelfand\n | last2 = Shilov\n | first2 = G. E.\n | author2-link = Georgiy Shilov\n | title = Generalized functions. Vol. I: Properties and operations\n | place = New York–London\n | publisher = [[Academic Press]]\n | origyear = 1958\n | year = 1964\n | edition =\n | pages = xviii+423\n | url = https://books.google.com/books?id=QoWBSgAACAAJ\n | isbn = 978-0-12-279501-5 \n | mr = 0166596\n | zbl = 0115.33101}}. Translated from the original 1958 Russian edition by Eugene Saletan, this is an important monograph on the theory of [[generalized functions]], dealing both with distributions and analytic functionals.\n*{{Citation\n | last = Gilbarg\n | first = David\n | author-link = David Gilbarg\n | last2 = Trudinger\n | first2 = Neil S. \n | author2-link = Neil Trudinger\n | title = Elliptic partial differential equations of second order\n | place = Berlin – Heidelberg – New York\n | publisher = [[Springer Verlag]]\n | series = Classics in Mathematics\n | origyear = 1998\n | year = 2001\n | edition = Revised 3rd printing of 2nd \n | pages = xiv+517\n | language =\n | url = https://books.google.com/books?id=eoiGTf4cmhwC&printsec=frontcover&hl=it&source=gbs_ge_summary_r&cad=0#v=onepage&q&f=true\n | doi =\n | id =\n | isbn = 3-540-41160-7\n | mr = 1814364 \n | zbl = 1042.35002\n}}.\n*{{Citation\n| last = Hörmander\n| first = Lars\n| author-link = Lars Hörmander\n| title = The analysis of linear partial differential operators I\n| place = [[Berlin]]-[[Heidelberg]]-[[New York City]]\n| publisher = [[Springer-Verlag]]\n| year = 1990\n| series = Grundlehren der Mathematischen Wissenschaft\n| volume = 256\n| edition = 2nd\n| pages = xii+440\n| url = \n| doi = \n| id = \n| mr = 1065136 \n| zbl= 0712.35001\n| isbn = 0-387-52343-X}} (available also as {{ISBN|3-540-52343-X}}).\n*{{Citation\n| last = Maz'ja\n| first = Vladimir G. \n| authorlink = Vladimir Gilelevich Maz'ya\n| title = Sobolev Spaces\n| publisher = [[Springer-Verlag]]\n| place = Berlin–Heidelberg–New York\n| year = 1985\n| pages = xix+486\n| isbn = 3-540-13589-8\n| id = \n| mr = 817985 \n| zbl = 0692.46023\n}} (available also as {{ISBN|0-387-13589-8}}).\n*{{Citation\n | last = Maz'ya\n | first = Vladimir G. \n | authorlink = Vladimir Gilelevich Maz'ya\n | title = Sobolev Spaces. With Applications to Elliptic Partial Differential Equations.\n | place = Berlin–Heidelberg–New York\n | publisher = [[Springer Verlag]]\n | series = Grundlehren der Mathematischen Wissenschaften \n | volume = 342\n | origyear = 1985\n | year = 2011\n | edition = 2nd revised and augmented\n | pages = xxviii+866\n | language =\n | doi =\n | id =\n | isbn = 978-3-642-15563-5\n | mr = 2777530 \n | zbl = 1217.46002\n}}.\n*{{Citation\n| last = Maz'ya\n| first = Vladimir G. \n| authorlink = Vladimir Gilelevich Maz'ya\n| last2 = Poborchi\n| first2 = Sergei V.\n| author2-link =\n| title = Differentiable Functions on Bad Domains\n| publisher = [[World Scientific]]\n| place = Singapore–New Jersey–London–Hong Kong\n| year = 1997\n| pages = xx+481\n| isbn = 981-02-2767-1\n| id = \n| mr = 1643072\n| zbl = 0918.46033\n}}.\n*{{Citation\n  | last = Maz'ya\n  | first = Vladimir G.\n  | author-link = Vladimir Gilelevich Maz'ya\n  | last2 = Shaposhnikova\n  | first2 = Tatyana O.\n  | author2-link = Tatyana Shaposhnikova\n  | title = Theory of Sobolev multipliers. With applications to differential and integral operators\n  | place = [[Heidelberg]]\n  | publisher = [[Springer-Verlag]]\n  | series = Grundlehren der Mathematischen Wissenschaft\n  | volume = 337\n  | origyear =\n  | year = 2009\n  | pages = xiii+609\n  | language = \n  | url = https://books.google.com/books?id=QN8uP6Mn0yQC&printsec=frontcover#v=onepage&q&f=true\n  | doi = \n  | id = \n  | isbn = 978-3-540-69490-8\n  | mr = 2457601\n  | zbl = 1157.46001 \n}}.\n*{{Citation\n | last = Meise\n | first = Reinhold\n | author-link =\n | last2 = Vogt\n | first2 = Dietmar\n | author2-link =\n | title = Introduction to Functional Analysis\n | place = Oxford\n | publisher = [[Clarendon Press]]\n | series = Oxford Graduate Texts in Mathematics\n | volume = 2\n | year = 1997\n | pages = x+437\n | url =\n | doi =\n | id =\n | isbn = 0-19-851485-9\n | mr = 1483073\n | zbl = 0924.46002 \n}}.\n*{{Citation\n| last = Saks\n| first = Stanisław\n| author-link = Stanisław Saks\n| title = Theory of the Integral\n| place = [[Warszawa]]-[[Lwów]]\n| publisher = G.E. Stechert & Co.\n| year = 1937\n| series = [[Monografie Matematyczne]]\n| volume = 7\n| edition = 2nd\n| pages = VI+347\n| url = http://pldml.icm.edu.pl/mathbwn/element/bwmeta1.element.dl-catalog-42a56b61-37f4-4c6b-a42b-ea95a98e407a?q=0f71728c-851f-486a-bc43-e8507297cea3$1&qt=IN_PAGE\n| jfm = 63.0183.05 \n| mr = 0167578\n| zbl = 0017.30004}}. English translation by [[Laurence Chisholm Young]], with two additional notes by [[Stefan Banach]]: the [[Mathematical Reviews]] number refers to the [[Dover Publications]] 1964 edition, which is basically a reprint.\n*{{Citation\n | first = Laurent\n | last = Schwartz\n | authorlink = Laurent Schwartz\n | title = Théorie des distributions\n | place = Paris \n | publisher = Hermann Éditeurs \n | series = Publications de l'Institut de Mathématique de l'Université de Strasbourg\n | volume = No. IX–X\n | origyear = 1966\n | year = 1998\n | edition = Nouvelle\n | pages = xiii+420\n | language = French\n | url =\n | isbn = 2-7056-5551-4\n | mr = 0209834 \n | zbl = 0149.09501\n}}.\n*{{Citation\n| last = Strichartz\n| first = Robert S.\n| title = A Guide to Distribution Theory and Fourier Transforms\n| place = [[River Edge, NJ]]\n| publisher = [[World Scientific|World Scientific Publishers]]\n| year = 2003\n| edition = 2nd printing\n| pages = x+226\n| url = https://books.google.com/books?id=T7vEOGGDCh4C&printsec=frontcover&dq=A+Guide+to+Distribution+Theory+and+Fourier+Transforms#v=onepage&q=&f=false\n| doi = \n| mr = 2000535\n| zbl = 1029.46039\n| isbn = 981-238-430-8\n}}.\n*{{Citation\n| last = Vladimirov\n| first = V. S.\n| author-link = Vasilii Sergeevich Vladimirov\n| title = Methods of the theory of generalized functions\n| place = London–New York\n| publisher = [[Taylor & Francis]]\n| pages = XII+353\n| series = Analytical Methods and Special Functions\n| volume = 6\n| year = 2002\n| url = https://books.google.com/?id=hlumB8fkX0UC&pg=PR1&dq=Methods+of+the+theory+of+generalized+functions\n| id = \n| mr = 2012831\n| zbl = 1078.46029\n| isbn = 0-415-27356-0}}. A monograph on the theory of [[generalized function]]s written with an eye towards their applications to [[several complex variables]] and [[mathematical physics]], as is customary for the Author.\n\n== External links ==\n*{{MathWorld |title=Locally integrable|author=Rowland, Todd|urlname=LocallyIntegrable}}\n*{{springer\n | title=Locally integrable function\n | id= L/l060460\n | last= Vinogradova\n | first=I.A.\n }}\n\n{{PlanetMath attribution|id=4430|title=Locally integrable function}}\n\n[[Category:Measure theory]]\n[[Category:Integral calculus]]\n[[Category:Types of functions]]"
    },
    {
      "title": "Malliavin calculus",
      "url": "https://en.wikipedia.org/wiki/Malliavin_calculus",
      "text": "In probability theory and related fields, '''Malliavin calculus''' is a set of mathematical techniques and ideas that extend the mathematical field of [[calculus of variations]] from  deterministic functions to [[stochastic processes]]. In particular, it allows the computation of [[derivative]]s of [[random variable]]s. Malliavin calculus is also called the '''[[stochastic calculus]] of variations'''. \n\nMalliavin calculus is named after [[Paul Malliavin]] whose ideas led to a proof that [[Hörmander's condition]] implies the existence and smoothness of a [[probability density function|density]] for the solution of a [[stochastic differential equation]]; [[Lars Hörmander|Hörmander]]'s original proof was based on the theory of  [[partial differential equation]]s. The calculus has been applied to [[stochastic partial differential equation]]s as well.\n\nThe calculus allows [[integration by parts]] with [[random variable]]s; this operation is used in [[mathematical finance]] to compute the sensitivities of [[derivative (finance)|financial derivative]]s. The calculus has applications in, for example, [[stochastic filtering]].\n\n==Overview and history==\nMalliavin introduced Malliavin calculus to provide a stochastic proof that [[Hörmander's condition]] implies the existence of a [[probability density function|density]] for the solution of a [[stochastic differential equation]]; [[Lars Hörmander|Hörmander]]'s original proof was based on the theory of  [[partial differential equation]]s. His calculus enabled Malliavin to prove regularity bounds for the solution's density. The calculus has been applied to [[stochastic partial differential equation]]s.\n\n== Invariance principle ==\nThe usual invariance principle for [[Lebesgue integration]] over the whole real line is that, for any real number ε and integrable function ''f'', the\nfollowing holds\n\n:<math> \\int_{-\\infty}^\\infty f(x)\\, d \\lambda(x) = \\int_{-\\infty}^\\infty f(x+\\varepsilon)\\, d \\lambda(x) </math> and hence <math>\\int_{-\\infty}^\\infty f'(x)\\, d \\lambda(x)=0.</math>\n\nThis can be used to derive the [[integration by parts]] formula since, setting ''f'' = ''gh'', it implies\n\n:<math>0 = \\int_{-\\infty}^\\infty f' \\,d \\lambda = \\int_{-\\infty}^\\infty (gh)' \\,d \\lambda = \\int_{-\\infty}^\\infty g h'\\, d \\lambda +\n\\int_{-\\infty}^\\infty g' h\\, d \\lambda.</math>\n\nA similar idea can be applied in stochastic analysis for the differentiation along a Cameron-Martin-Girsanov direction. Indeed, let <math>h_s</math> be a square-integrable [[predictable process]] and set\n\n:<math> \\varphi(t) = \\int_0^t h_s\\, d s .</math>\n\nIf <math>X</math> is a [[Wiener process]], the [[Girsanov theorem]] then yields the following analogue of the invariance principle:\n\n:<math> E(F(X + \\varepsilon\\varphi))= E \\left [F(X) \\exp \\left ( \\varepsilon\\int_0^1 h_s\\, d X_s -\n\\frac{1}{2}\\varepsilon^2 \\int_0^1 h_s^2\\, ds \\right ) \\right ].</math>\n\nDifferentiating with respect to ε on both sides and evaluating at ε=0, one obtains the following integration by parts formula:\n\n:<math>E(\\langle DF(X), \\varphi\\rangle) = E\\Bigl[ F(X) \\int_0^1 h_s\\, dX_s\\Bigr].\n</math>\n\nHere, the left-hand side is the [[Malliavin derivative]] of the random variable <math>F</math> in the direction <math>\\varphi</math> and the integral appearing on the right hand side should be interpreted as an [[Itô integral]]. This expression also remains true (by definition) if <math>h</math> is not adapted, provided that the right hand side is interpreted as a [[Skorokhod integral]].{{Citation needed|date=August 2011}}\n\n== Clark-Ocone formula ==\n{{Main|Clark–Ocone theorem}}\n\nOne of the most useful results from Malliavin calculus is the [[Clark-Ocone theorem]], which allows the process in the [[martingale representation theorem]] to be identified explicitly. A simplified version of this theorem is as follows:\n\nFor <math>F: C[0,1] \\to \\R</math> satisfying <math> E(F(X)^2) < \\infty</math> which is Lipschitz and such that ''F'' has a strong derivative kernel, in the sense that\nfor <math>\\varphi</math> in ''C''[0,1]\n\n:<math> \\lim_{\\varepsilon \\to 0} (1/\\varepsilon)(F(X+\\varepsilon \\varphi) - F(X) ) = \\int_0^1 F'(X,dt) \\varphi(t)\\ \\mathrm{a.e.}\\ X</math>\n\nthen\n\n:<math>F(X) = E(F(X)) + \\int_0^1 H_t \\,d X_t ,</math>\n\nwhere ''H'' is the previsible projection of ''F''<nowiki>'</nowiki>(''x'', (''t'',1]) which may be viewed as the derivative of the function ''F'' with respect to a suitable parallel shift of the process ''X'' over the portion (''t'',1] of its domain.\n\nThis may be more concisely expressed by\n\n:<math>F(X) = E(F(X))+\\int_0^1 E (D_t F | \\mathcal{F}_t ) \\, d X_t .</math>\n\nMuch of the work in the formal development of the Malliavin calculus involves extending this result to the largest possible class of functionals ''F'' by replacing the derivative kernel used above by the \"[[Malliavin derivative]]\" denoted <math>D_t</math> in the above statement of the result. {{Citation needed|date=August 2011}}\n\n== Skorokhod integral ==\n{{Main|Skorokhod integral}}\nThe [[Skorokhod integral]] operator which is conventionally denoted δ is defined as the adjoint of the Malliavin derivative thus for u in the domain of the operator which is a subset of <math>L^2([0,\\infty) \\times \\Omega)</math>,\nfor '''F''' in the domain of the Malliavin derivative, we require\n\n: <math> E (\\langle DF, u \\rangle ) = E (F \\delta (u) ),</math>\n\nwhere the inner product is that on <math>L^2[0,\\infty)</math> viz\n\n: <math> \\langle f, g \\rangle = \\int_0^\\infty f(s) g(s) \\, ds.</math>\n\nThe existence of this adjoint follows from the [[Riesz representation theorem]] for linear operators on [[Hilbert spaces]].\n\nIt can be shown that if ''u'' is adapted then\n\n: <math> \\delta(u) = \\int_0^\\infty u_t\\, d W_t ,</math>\n\nwhere the integral is to be understood in the Itô sense.   Thus this provides a method of extending the Itô integral to non adapted integrands.\n\n==Applications==\nThe calculus allows [[integration by parts]] with [[random variable]]s; this operation is used in [[mathematical finance]] to compute the sensitivities of [[derivative (finance)|financial derivative]]s. The calculus has applications for example in [[stochastic control|stochastic filtering]].\n\n{{No footnotes|date=June 2011}}\n\n== References ==\n* Kusuoka, S. and Stroock, D. (1981) \"Applications of Malliavin Calculus I\", ''Stochastic Analysis, Proceedings Taniguchi International Symposium Katata and Kyoto'' 1982, pp 271–306\n* Kusuoka, S. and Stroock, D. (1985) \"Applications of Malliavin Calculus II\", ''J. Faculty Sci. Uni. Tokyo Sect. 1A Math.'', 32 pp 1–76\n* Kusuoka, S. and Stroock, D. (1987) \"Applications of Malliavin Calculus III\", ''J. Faculty Sci. Univ. Tokyo Sect. 1A Math.'', 34 pp 391–442\n* Malliavin, Paul and Thalmaier, Anton. ''Stochastic Calculus of Variations in Mathematical Finance'', Springer 2005, {{isbn|3-540-43431-3}}\n* {{cite book\n|     last = Nualart\n|    first = David\n|    title = The Malliavin calculus and related topics\n|  edition = Second\n|publisher = Springer-Verlag\n|     year = 2006\n|     isbn = 978-3-540-28328-7\n}}\n* Bell, Denis. (2007) ''The Malliavin Calculus'', Dover. {{isbn|0-486-44994-7}}\n* Schiller, Alex (2009) [https://web.archive.org/web/20110903171132/http://www.alexschiller.com/media/Thesis.pdf ''Malliavin Calculus for Monte Carlo Simulation with Financial Applications'']. Thesis, Department of Mathematics, Princeton University\n* [[Bernt Øksendal|Øksendal, Bernt K.]].(1997) [http://www.quantcode.com/modules/wflinks/visit.php?cid=11&lid=4 ''An Introduction To Malliavin Calculus With Applications To Economics'']. Lecture Notes, Dept. of Mathematics, University of Oslo (Zip file containing Thesis and addendum)\n*[[Giulia Di Nunno|Di Nunno, Giulia]], Øksendal, Bernt, Proske, Frank (2009) \"Malliavin Calculus for Lévy Processes with Applications to Finance\", Universitext, Springer. {{isbn|978-3-540-78571-2}}\n\n== External links ==\n\n* {{cite web\n|url = http://www.statslab.cam.ac.uk/~peter/malliavin/Malliavin2005/mall.pdf\n|title = An Introduction to Malliavin Calculus\n|accessdate = 2007-07-23\n|last = Friz\n|first = Peter K.\n|date = 2005-04-10\n|format = PDF\n|archiveurl = https://web.archive.org/web/20070417205303/http://www.statslab.cam.ac.uk/~peter/malliavin/Malliavin2005/mall.pdf |archivedate = 2007-04-17}} Lecture Notes, 43 pages\n\n* {{cite web\n|url = http://frank-oertel-math.de/PhD_thesis_on_Malliavin_Calculus_incl_copy_of_FO.pdf\n|title = The Malliavin Calculus\n|accessdate = 2004-11-11\n|last = Zhang\n|first = H.\n|date = 2004-11-11\n|format = PDF\n}} Thesis, 100 pages\n\n[[Category:Stochastic calculus]]\n[[Category:Integral calculus]]\n[[Category:Mathematical finance]]\n[[Category:Calculus of variations]]"
    },
    {
      "title": "Method of exhaustion",
      "url": "https://en.wikipedia.org/wiki/Method_of_exhaustion",
      "text": "{{about|the method of finding the area of a shape using limits|the method of proof|Proof by exhaustion}}\nThe '''method of exhaustion''' ({{lang|la|methodus exhaustionibus}}, or {{lang|fr|méthode des anciens}}) is a method of  finding the [[area]] of a [[shape]] by [[Inscribed figure|inscribing]] inside it a sequence of [[polygon]]s whose [[area]]s [[limit (mathematics)|converge]] to the area of the containing [[shape]]. If the sequence is correctly constructed, the difference in area between the ''n''-th polygon and the containing shape will become arbitrarily small as ''n'' becomes large. As this difference becomes arbitrarily small, the possible values for the area of the shape are systematically \"exhausted\" by the lower bound areas successively established by the sequence members.\n\nThe method of exhaustion typically required a form of [[proof by contradiction]], known as ''[[reductio ad absurdum]]''.  This amounts to finding an area of a region by first comparing it to the area of a second region (which can be “exhausted” so that its area becomes arbitrarily close to the true area). The proof involves assuming that the true area is greater than the second area, and then proving that assertion false, and then assuming that it is less than the second area, and proving that assertion false, too.\n\n== History ==\n[[File:Grégoire de Saint-Vincent (1584-1667).jpg|thumb|150px|right|Gregory of Saint Vincent]]\nThe idea originated in the late 5th century BC with [[Antiphon (person)|Antiphon]], although it is not entirely clear how well he understood it.<ref>[http://www-history.mcs.st-andrews.ac.uk/Biographies/Antiphon.html The MacTutor History of Mathematics archive]</ref> The theory was made rigorous a few decades later by [[Eudoxus of Cnidus]], who used it to calculate areas and volumes. It was later reinvented in [[Chinese mathematics|China]] by [[Liu Hui]] in the 3rd century AD in order to find the area of a circle.<ref>{{cite journal|series=Chinese studies in the history and philosophy of science and technology|volume=130|title=A comparison of Archimedes' and Liu Hui's studies of circles|first1=Liu|last1=Dun|first2=Dainian|last2=Fan|first3=Robert Sonné|last3=Cohen|publisher=Springer|year=1966|isbn=0-7923-3463-9|page=279|url=https://books.google.com/books?id=jaQH6_8Ju-MC}}, [https://books.google.com/books?id=jaQH6_8Ju-MC&pg=PA279 Chapter , p. 279]</ref> The first use of the term was in 1647 by [[Grégoire de Saint-Vincent|Gregory of Saint Vincent]] in ''Opus geometricum quadraturae circuli et sectionum''.\n\nThe method of exhaustion is seen as a precursor to the methods of [[calculus]]. The development of [[analytical geometry]] and rigorous [[integral calculus]] in the 17th-19th centuries subsumed the method of exhaustion so that it is no longer explicitly used to solve problems. An important alternative approach was [[Cavalieri's principle]], also termed the \"[[method of indivisibles]]\", which eventually evolved into the [[infinitesimal]] calculus of [[Gilles de Roberval|Roberval]], [[Evangelista Torricelli|Torricelli]], [[John Wallis|Wallis]], [[Gottfried Wilhelm Leibniz|Leibniz]], and others.\n\n=== Euclid ===\n[[Euclid]] used the method of exhaustion to prove the following six propositions in the book 12 of his ''[[Euclid's Elements|Elements]]''.\n; Proposition 2\n: The area of circles is proportional to the square of their diameters.<ref>{{cite web|url=http://aleph0.clarku.edu/~djoyce/java/elements/bookXII/propXII2.html|title=Euclid's Elements, Book XII, Proposition 2|website=aleph0.clarku.edu}}</ref>\n; Proposition 5\n: The volumes of two tetrahedra of the same height are proportional to the areas of their triangular bases.<ref>{{cite web|url=http://aleph0.clarku.edu/~djoyce/java/elements/bookXII/propXII5.html|title=Euclid's Elements, Book XII, Proposition 5|website=aleph0.clarku.edu}}</ref>\n; Proposition 10\n: The volume of a cone is a third of the volume of the corresponding cylinder which has the same base and height.<ref>{{cite web|url=http://aleph0.clarku.edu/~djoyce/java/elements/bookXII/propXII10.html|title=Euclid's Elements, Book XII, Proposition 10|website=aleph0.clarku.edu}}</ref>\n; Proposition 11\n: The volume of a cone (or cylinder) of the same height is proportional to the area of the base.<ref>{{cite web|url=http://aleph0.clarku.edu/~djoyce/java/elements/bookXII/propXII11.html|title=Euclid's Elements, Book XII, Proposition 11|website=aleph0.clarku.edu}}</ref>\n; Proposition 12\n: The volume of a cone (or cylinder) that is similar to another is proportional to the cube of the ratio of the diameters of the bases.<ref>{{cite web|url=http://aleph0.clarku.edu/~djoyce/java/elements/bookXII/propXII12.html|title=Euclid's Elements, Book XII, Proposition 12|website=aleph0.clarku.edu}}</ref>\n; Proposition 18\n: The volume of a sphere is proportional to the cube of its diameter.<ref>{{cite web|url=http://aleph0.clarku.edu/~djoyce/java/elements/bookXII/propXII18.html|title=Euclid's Elements, Book XII, Proposition 18|website=aleph0.clarku.edu}}</ref>\n\n=== Archimedes ===\n{{Main|Pi}}\n[[File:Archimedes pi.svg|thumb|right|300px|Archimedes used the method of exhaustion to compute the area inside a circle]]\n\n[[Archimedes]] used the method of exhaustion as a way to compute the area inside a circle by filling the [[circle]] with a [[polygon]] of a greater area and greater number of [[Edge (geometry)|sides]].  The quotient formed by the area of this polygon divided by the square of the circle radius can be made arbitrarily close to  π as the number of polygon sides becomes large, proving that the area inside the circle of radius r is πr<sup>2</sup>,  [[π]] being defined as the ratio of the circumference to the diameter (C/d) or of the area of the circle to the square of its radius (A/r²).\n\nHe also provided the bounds 3&nbsp;+&nbsp;<sup>10</sup>''/''<sub>71</sub>&nbsp;<&nbsp;''π''&nbsp;<&nbsp;3&nbsp;+&nbsp;<sup>10</sup>''/''<sub>70</sub>, (giving a range of <sup>1</sup>''/''<sub>497</sub>) by comparing the perimeters of the circle with the perimeters of the inscribed and circumscribed 96-sided regular polygons.\n\nOther results he obtained with the method of exhaustion included<ref>{{cite book\n | last = Smith\n | first = David E\n | year = 1958\n | title = History of Mathematics\n | publisher = Dover Publications\n | location = New York\n | isbn = 0-486-20430-8\n}}</ref>\n* The area bounded by the intersection of a line and a parabola is 4/3 that of the triangle having the same base and height;\n* The area of an ellipse is proportional to a rectangle having sides equal to its major and minor axes;\n* The volume of a sphere is 4 times that of a cone having a base of the same radius and height equal to this radius;\n* The volume of a cylinder having a height equal to its diameter is 3/2 that of a sphere having the same diameter;\n* The area bounded by one [[Archimedean spiral|spiral]] rotation and a line is 1/3 that of the circle having a radius equal to the line segment length;\n* Use of the method of exhaustion also led to the successful evaluation of an [[infinite geometric series]] (for the first time).\n\n== See also ==\n* ''[[The Method of Mechanical Theorems]]''\n* ''[[The Quadrature of the Parabola]]''\n* [[Trapezoidal rule]]\n\n== References ==\n{{reflist}}\n\n[[Category:Volume]]\n[[Category:Euclidean geometry]]\n[[Category:Integral calculus]]\n[[Category:History of mathematics]]\n[[Category:5th century BC in Greece]]"
    },
    {
      "title": "Multiple integral",
      "url": "https://en.wikipedia.org/wiki/Multiple_integral",
      "text": "{{Calculus |Multivariable}}\n[[File:Areabetweentwographs.svg|thumb|right|Integral as area between two curves.]]\n[[File:Volume under surface.png|right|thumb|Double integral as volume under a surface {{math|''z'' {{=}} 10 − {{sfrac|''x''<sup>2</sup> − ''y''<sup>2</sup>|8}}}}. The rectangular region at the bottom of the body is the domain of integration, while the surface is the graph of the two-variable function to be integrated.]]\n\nThe '''multiple integral''' is a [[definite integral]] of a [[Function (mathematics)|function]] of more than one real [[Variable (mathematics)|variable]], for example, {{math|''f''(''x'', ''y'')}} or {{math|''f''(''x'', ''y'', ''z'')}}. Integrals of a function of two variables over a region in {{math|'''R'''<sup>2</sup>}} are called [[double integrals]], and integrals of a function of three variables over a region of {{math|'''R'''<sup>3</sup>}} are called [[triple integrals]].<ref name= \"Stewart\">{{cite book|authorlink=James Stewart (mathematician) |last=Stewart |first=James |date=2008 |title=Calculus: Early Transcendentals |edition=6th |publisher=Brooks Cole Cengage Learning |ISBN=978-0-495-01166-8}}</ref>\n\n==Introduction==\nJust as the definite integral of a positive function of  one variable represents the [[area]] of the region between the graph of the function and the {{mvar|x}}-axis, the '''double integral''' of a positive function of two variables represents the [[volume]] of the region between the surface defined by the function (on the three-dimensional [[Cartesian plane]] where {{math|''z'' {{=}} ''f''(''x'', ''y'')}} and the plane which contains its [[Domain (mathematics)|domain]]. <ref name = \"Stewart\" /> If there are more variables, a multiple integral will yield [[hypervolume]]s of multidimensional functions.\n\nMultiple integration of a function in {{mvar|n}} variables: {{math|''f''(''x''<sub>1</sub>, ''x''<sub>2</sub>, ..., ''x''<sub>''n''</sub>)}} over a domain {{mvar|D}} is most commonly represented by nested integral signs in the reverse order of execution (the leftmost integral sign is computed last), followed by the function and integrand arguments in proper order (the integral with respect to the rightmost argument is computed last). The domain of integration is either represented symbolically for every argument over each integral sign, or is abbreviated by a variable at the rightmost integral sign:<ref>{{cite book|last1=Larson |last2=Edwards |date=2014 |title=Multivariable Calculus |edition=10th |publisher=Cengage Learning |ISBN= 978-1-285-08575-3}}</ref>\n\n:<math> \\int \\cdots \\int_\\mathbf{D}\\, f(x_1,x_2,\\ldots,x_n) \\,dx_1 \\!\\cdots dx_n </math>\n\nSince the concept of an [[antiderivative]] is only defined for functions of a single real variable, the usual definition of the [[indefinite integral]] does not immediately extend to the multiple integral.\n\n==Mathematical definition==\nFor {{math|''n'' > 1}}, consider a so-called \"half-open\" {{mvar|n}}-dimensional [[Hyperrectangle|hyperrectangular]] domain {{mvar|T}}, defined as:\n\n:<math>T= [ a_1, b_1) \\times [ a_2, b_2) \\times \\cdots \\times [ a_n, b_n) \\subseteq \\mathbf{R}^n.</math>\n\n[[partition (set theory)|Partition]] each interval {{math|[''a''<sub>''j''</sub>, ''b''<sub>''j''</sub>)}} into a finite family {{mvar|I<sub>j</sub>}} of non-overlapping subintervals {{mvar|i<sub>j<sub>α</sub></sub>}}, with each subinterval closed at the left end, and open at the right end.\n\nThen the finite family of subrectangles {{mvar|C}} given by\n\n:<math>C=I_1\\times I_2\\times \\cdots \\times I_n</math>\n\nis a [[partition (set theory)|partition]] of {{mvar|T}}; that is, the subrectangles {{mvar|C<sub>k</sub>}} are non-overlapping and their union is {{mvar|T}}.\n\nLet {{math|''f'' : ''T'' → '''R'''}} be a function defined on {{mvar|T}}. Consider a partition {{mvar|C}} of {{mvar|T}} as defined above, such that {{mvar|C}} is a family of {{mvar|m}} subrectangles {{mvar|C<sub>m</sub>}} and\n\n:<math>T=C_1\\cup C_2\\cup \\cdots \\cup C_m</math>\n\nWe can approximate the total {{math|(''n'' + 1)}}th-dimensional volume bounded below by the {{mvar|n}}-dimensional hyperrectangle {{mvar|T}} and above by the {{mvar|n}}-dimensional graph of {{mvar|f}} with the following [[Riemann sum]]:\n\n:<math>\\sum_{k=1}^m f(P_k)\\, \\operatorname{m}(C_k)</math>\n\nwhere {{mvar|P<sub>k</sub>}} is a point in {{mvar|C<sub>k</sub>}} and {{math|m(''C''<sub>''k''</sub>)}} is the product of the lengths of the intervals whose Cartesian product is {{mvar|C<sub>k</sub>}}, also known as the measure of {{mvar|C<sub>k</sub>}}.\n\nThe '''diameter''' of a subrectangle {{mvar|C<sub>k</sub>}} is the largest of the lengths of the intervals whose [[Cartesian product]] is {{mvar|C<sub>k</sub>}}. The diameter of a given partition of {{mvar|T}} is defined as the largest of the diameters of the subrectangles in the partition. Intuitively, as the diameter of the partition {{mvar|C}} is restricted smaller and smaller, the number of subrectangles {{mvar|m}} gets larger, and the measure {{math|m(''C''<sub>''k''</sub>)}} of each subrectangle grows smaller. The function {{mvar|f}} is said to be '''Riemann integrable''' if the [[limit (mathematics)|limit]]\n\n:<math>S=\\lim_{\\delta \\to 0} \\sum_{k=1}^m f(P_k)\\, \\operatorname{m}\\, (C_k)</math>\n\nexists, where the limit is taken over all possible partitions of {{mvar|T}} of diameter at most {{mvar|δ}}.<ref>{{cite book |last=Rudin |first=Walter |authorlink=Walter Rudin |title=Principles of Mathematical Analysis |series=Walter Rudin Student Series in Advanced Mathematics |edition=3rd |publisher=McGraw–Hill |isbn=978-0-07-054235-8}}</ref>\n\nIf {{mvar|f}} is Riemann integrable, {{mvar|S}} is called the '''Riemann integral''' of {{mvar|f}} over {{mvar|T}} and is denoted\n\n:<math> \\int \\cdots \\int_T\\, f(x_1,x_2,\\ldots,x_n) \\,dx_1 \\!\\cdots dx_n </math>\n\nFrequently this notation is abbreviated as\n\n: <math>\\int_T\\!f(\\mathbf{x})\\,d^n\\mathbf{x}.</math>\n\nwhere {{math|'''x'''}} represents the {{mvar|n}}-tuple {{math|(''x''<sub>1</sub>, ... ''x<sub>n</sub>'')}} and {{math|''d''{{isup|''n''}}'''x'''}} is the {{math|n}}-dimensional volume [[Differential (infinitesimal)|differential]].\n\nThe Riemann integral of a function defined over an arbitrary bounded {{math|n}}-dimensional set can be defined by extending that function to a function defined over a half-open rectangle whose values are zero outside the domain of the original function. Then the integral of the original function over the original domain is defined to be the integral of the extended function over its rectangular domain, if it exists.\n\nIn what follows the Riemann integral in {{math|n}} dimensions will be called the '''multiple integral'''.\n\n===Properties===\nMultiple integrals have many properties common to those of integrals of functions of one variable (linearity, commutativity, monotonicity, and so on). One important property of multiple integrals is that the value of an integral is independent of the order of integrands under certain conditions. This property is popularly known as [[Fubini's theorem]].<ref name=\"a\">{{cite book|last=Jones |first=Frank |date=2001 |title=Lebesgue Integration on Euclidean Space |publisher=Jones and Bartlett |pages=527–529 |ISBN=}}{{ISBN missing}}</ref>\n\n===Particular cases===\nIn the case of {{math|''T'' ⊆ '''R'''<sup>2</sup>}}, the integral\n\n:<math> l = \\iint_T f(x,y)\\, dx\\, dy </math>\n\nis the '''double integral''' of {{mvar|f}} on {{mvar|T}}, and if {{math|''T'' ⊆ '''R'''<sup>3</sup>}} the integral\n\n:<math> l = \\iiint_T f(x,y,z)\\, dx\\, dy\\, dz</math>\n\nis the '''triple integral''' of {{mvar|f}} on {{mvar|T}}.\n\nNotice that, by convention, the double integral has two integral signs, and the triple integral has three; this is a notational convention which is convenient when computing a multiple integral as an iterated integral, as shown later in this article.\n\n==Methods of integration==\nThe resolution of problems with multiple integrals consists, in most cases, of finding a way to reduce the multiple integral to an [[iterated integral]], a series of integrals of one variable, each being directly solvable. For continuous functions, this is justified by [[Fubini's theorem]]. Sometimes, it is possible to obtain the result of the integration by direct examination without any calculations.\n\nThe following are some simple methods of integration:<ref name=\"Stewart\" />\n\n===Integrating constant functions===\nWhen the integrand is a [[constant function]] {{mvar|c}}, the integral is equal to the product of {{mvar|c}} and the measure of the domain of integration. If {{math|''c'' {{=}} 1}} and the domain is a subregion of {{math|'''R'''<sup>2</sup>}}, the integral gives the area of the region, while if the domain is a subregion of {{math|'''R'''<sup>3</sup>}}, the integral gives the volume of the region.\n\n<blockquote>'''Example.''' Let {{math|''f''(''x'', ''y'') {{=}} 2}} and\n\n:<math>D = \\left\\{ (x,y) \\in \\mathbf{R}^2 \\ : \\ 2 \\le x \\le 4 \\ ; \\ 3 \\le y \\le 6 \\right\\}</math>\n\nin which case\n\n:<math>\\int_3^6 \\int_2^4 \\ 2 \\ dx\\, dy =2\\int_3^6 \\int_2^4 \\ 1 \\ dx\\, dy= 2\\cdot\\operatorname{area}(D) = 2 \\cdot (2 \\cdot 3) = 12,</math>\n\nsince by definition we have:\n\n:<math>\\int_3^6 \\int_2^4 \\ 1 \\ dx\\, dy=\\operatorname{area}(D).</math></blockquote>\n\n===Use of symmetry===\nWhen the domain of integration is symmetric about the origin with respect to at least one of the variables of integration and the integrand is [[even and odd functions|odd]] with respect to this variable, the integral is equal to zero, as the integrals over the two halves of the domain have the same absolute value but opposite signs. When the integrand is [[even and odd functions|even]] with respect to this variable, the integral is equal to twice the integral over one half of the domain, as the integrals over the two halves of the domain are equal.\n\n<blockquote>'''Example 1.''' Consider the function {{math|''f''(''x'',''y'') {{=}} 2 sin(''x'') − 3''y''<sup>3</sup> + 5}} integrated over the domain\n\n:<math>T=\\left \\{ ( x,y) \\in \\mathbf{R}^2 \\ : \\ x^2+y^2\\le 1 \\right \\},</math>\n\na disc with [[radius]]&nbsp;1 centered at the origin with the boundary included.\n\nUsing the linearity property, the integral can be decomposed into three pieces:\n\n:<math>\\iint_T \\left(2\\sin x - 3y^3 + 5\\right) \\, dx \\, dy = \\iint_T 2 \\sin x \\, dx \\, dy - \\iint_T 3y^3 \\, dx \\, dy + \\iint_T 5 \\, dx \\, dy</math>\n\nThe function {{math|2 sin(''x'')}} is an odd function in the variable {{mvar|x}} and the disc {{mvar|T}} is symmetric with respect to the {{mvar|y}}-axis, so the value of the first integral is 0. Similarly, the function {{math|3''y''<sup>3</sup>}} is an odd function of {{mvar|y}}, and {{mvar|T}} is symmetric with respect to the {{mvar|x}}-axis, and so the only contribution to the final result is that of the third integral. Therefore the original integral is equal to the area of the disk times 5, or 5{{pi}}.</blockquote>\n\n<blockquote>'''Example 2.''' Consider the function {{math|''f''(''x'', ''y'', ''z'') {{=}} ''x'' exp(''y''<sup>2</sup> + ''z''<sup>2</sup>)}} and as integration region the [[sphere]] with radius 2 centered at the origin,\n:<math>T = \\left \\{ ( x,y, z) \\in \\mathbf{R}^3 \\ : \\ x^2+y^2+z^2 \\le 4 \\right \\}.</math>\nThe \"ball\" is symmetric about all three axes, but it is sufficient to integrate with respect to {{mvar|x}}-axis to show that the integral is 0, because the function is an odd function of that variable.</blockquote>\n\n===Normal domains on {{math|'''R'''<sup>2</sup>}}===\n{{See also|Order of integration (calculus)}}\nThis method is applicable to any domain {{mvar|D}} for which:\n* the [[Orthographic projection|projection]] of {{mvar|D}} onto either the {{mvar|x}}-axis or the {{mvar|y}}-axis is bounded by the two values, {{mvar|a}} and {{mvar|b}}\n* any line perpendicular to this axis that passes between these two values intersects the domain in an interval whose endpoints are given by the graphs of two functions, {{mvar|α}} and {{mvar|β}}.\nSuch a domain will be here called a ''normal domain''.  Elsewhere in the literature, normal domains are sometimes called type I or type II domains, depending on which axis the domain is fibred over.  In all cases, the function to be integrated must be Riemann integrable on the domain, which is true (for instance) if the function is continuous.\n\n===={{mvar|x}}-axis====\nIf the domain {{mvar|D}} is normal with respect to the {{mvar|x}}-axis, and {{math|''f'' : ''D'' → '''R'''}} is a [[continuous function]]; then {{math|''α''(''x'')}} and {{math|''β''(''x'')}} (both of which are defined on the interval {{math|[''a'', ''b'']}}) are the two functions that determine {{mvar|D}}. Then, by Fubini's theorem:<ref>{{Cite book|title=Calculus, 8th Edition|last=Stewart|first=James|publisher=Cengage Learning|year=|isbn=978-1285740621|location=|pages=}}</ref>\n:<math>\\iint_D f(x,y)\\, dx\\, dy = \\int_a^b dx \\int_{ \\alpha (x)}^{ \\beta (x)} f(x,y)\\, dy.</math>\n\n===={{mvar|y}}-axis====\nIf {{mvar|D}} is normal with respect to the {{mvar|y}}-axis and {{math|''f'' : ''D'' → '''R'''}} is a continuous function; then {{math|''α''(''y'')}} and {{math|''β''(''y'')}} (both of which are defined on the interval {{math|[''a'', ''b'']}}) are the two functions that determine {{mvar|D}}. Again, by Fubini's theorem:\n\n:<math>\\iint_D f(x,y)\\, dx\\, dy = \\int_a^b dy \\int_{\\alpha (y)}^{ \\beta (y)} f(x,y)\\, dx.</math>\n\n====Normal domains on {{math|'''R'''<sup>3</sup>}}====\nIf {{mvar|T}} is a domain that is normal with respect to the {{mvar|xy}}-plane and determined by the functions {{math|''α''(''x'', ''y'')}} and {{math|''β''(''x'', ''y'')}}, then\n\n:<math>\\iiint_T f(x,y,z) \\, dx\\, dy\\, dz = \\iint_D \\int_{\\alpha (x,y)}^{\\beta (x,y)} f(x,y,z) \\, dz\\, dx\\, dy</math>\n\nThis definition is the same for the other five normality cases on {{math|'''R'''<sup>3</sup>}}.  It can be generalized in a straightforward way to domains in '''R'''<sup>''n''</sup>.\n\n===Change of variables===\n{{See also|Integration by substitution#Substitution for multiple variables}}\n\nThe limits of integration are often not easily interchangeable (without normality or with complex formulae to integrate). One makes a [[change of variables]] to rewrite the integral in a more \"comfortable\" region, which can be described in simpler formulae. To do so, the function must be adapted to the new coordinates.\n\n<blockquote>'''Example 1a.''' The function is {{math|''f''(''x'', ''y'') {{=}} (''x'' − 1)<sup>2</sup> + {{sqrt|''y''}}}}; if one adopts the substitution {{math|''x''′ {{=}} ''x'' − 1}}, {{math|''y''′ {{=}} ''y''}} therefore {{math|''x'' {{=}} ''x''′ + 1}}, {{math|''y'' {{=}} ''y''′}} one obtains the new function {{math|''f''<sub>2</sub>(''x'', ''y'') {{=}} (''x''′)<sup>2</sup> + {{sqrt|''y''}}}}.</blockquote>\n* Similarly for the domain because it is delimited by the original variables that were transformed before ({{mvar|x}} and {{mvar|y}} in example).\n* the differentials {{mvar|dx}} and {{mvar|dy}} transform via the absolute value of the [[Jacobian matrix and determinant|determinant of the Jacobian matrix]] containing the partial derivatives of the transformations regarding the new variable (consider, as an example, the differential transformation in polar coordinates).\n\nThere exist three main \"kinds\" of changes of variable (one in {{math|'''R'''<sup>2</sup>}}, two in {{math|'''R'''<sup>3</sup>}}); however, more general substitutions can be made using the same principle.\n\n====Polar coordinates====\n{{See also|Polar coordinate system}}\n[[File:Passaggio in coordinate polari.svg|thumb|270px|right|Transformation from cartesian to polar coordinates.]]\nIn {{math|'''R'''<sup>2</sup>}} if the domain has a circular symmetry and the function has some particular characteristics one can apply the ''transformation to polar coordinates'' (see the example in the picture) which means that the generic points {{math|''P''(''x'', ''y'')}} in Cartesian coordinates switch to their respective points in polar coordinates. That allows one to change the shape of the domain and simplify the operations.\n\nThe fundamental relation to make the transformation is the following:\n\n:<math>f(x,y) \\rightarrow f(\\rho \\cos \\varphi,\\rho \\sin \\varphi ).</math>\n\n<blockquote>'''Example 2a.''' The function is {{math|''f''(''x'', ''y'') {{=}} ''x'' + ''y''}} and applying the transformation one obtains\n:<math>f(\\rho, \\varphi) = \\rho \\cos \\varphi + \\rho \\sin \\varphi = \\rho(\\cos \\varphi + \\sin \\varphi ).</math></blockquote>\n\n<blockquote>'''Example 2b.''' The function is {{math|''f''(''x'', ''y'') {{=}} ''x''<sup>2</sup> + ''y''<sup>2</sup>}}, in this case one has:\n:<math>f(\\rho, \\varphi) = \\rho^2 \\left(\\cos^2 \\varphi + \\sin^2 \\varphi\\right) = \\rho^2</math>\nusing the [[Pythagorean trigonometric identity]] (very useful to simplify this operation).</blockquote>\n\nThe transformation of the domain is made by defining the radius' crown length and the amplitude of the described angle to define the {{math|''ρ'', ''φ''}} intervals starting from {{math|''x'', ''y''}}.\n[[File:Esempio trasformazione dominio da cartesiano polare.svg|thumb|230px|right|Example of a domain transformation from cartesian to polar.]]\n\n<blockquote>'''Example 2c.''' The domain is {{math|''D'' {{=}} {''x''<sup>2</sup> + ''y''<sup>2</sup> ≤ 4<nowiki>}</nowiki>}}, that is a circumference of radius 2; it's evident that the covered angle is the circle angle, so {{mvar|φ}} varies from 0 to 2{{pi}}, while the crown radius varies from 0 to 2 (the crown with the inside radius null is just a circle).</blockquote>\n\n<blockquote>'''Example 2d.''' The domain is {{math|''D'' {{=}} {''x''<sup>2</sup> + ''y''<sup>2</sup> ≤ 9, ''x''<sup>2</sup> + ''y''<sup>2</sup> ≥ 4, ''y'' ≥ 0<nowiki>}</nowiki>}}, that is the circular crown in the positive {{mvar|y}} half-plane (please see the picture in the example); {{mvar|φ}} describes a plane angle while {{mvar|ρ}} varies from 2 to 3. Therefore the transformed domain will be the following [[rectangle]]:\n\n:<math>T = \\{ 2 \\le \\rho \\le 3, \\ 0 \\le \\varphi \\le \\pi \\}.</math>\n\nThe [[Jacobian determinant]] of that transformation is the following:\n\n:<math>\\frac{\\partial (x,y)}{\\partial (\\rho, \\varphi)} = \\begin{vmatrix} \\cos \\varphi & - \\rho \\sin \\varphi \\\\ \\sin \\varphi & \\rho \\cos \\varphi \\end{vmatrix} = \\rho</math>\n\nwhich has been obtained by inserting the partial derivatives of {{math|''x'' {{=}} ''ρ'' cos(''φ'')}}, {{math|''y'' {{=}} ''ρ'' sin(''φ'')}} in the first column respect to {{mvar|ρ}} and in the second respect to {{mvar|φ}}, so the {{mvar|dx dy}} differentials in this transformation become {{mvar|ρ dρ dφ}}.\n\nOnce the function is transformed and the domain evaluated, it is possible to define the formula for the change of variables in polar coordinates:\n\n:<math>\\iint_D f(x,y) \\, dx\\, dy = \\iint_T f(\\rho \\cos \\varphi, \\rho \\sin \\varphi) \\rho \\, d \\rho\\, d \\varphi.</math>\n\n{{mvar|φ}} is valid in the {{math|[0, 2π]}} interval while {{mvar|ρ}}, which is a measure of a length, can only have positive values.</blockquote>\n\n<blockquote>'''Example 2e.''' The function is {{math|''f''(''x'', ''y'') {{=}} ''x''}} and the domain is the same as in Example 2d. From the previous analysis of {{mvar|D}} we know the intervals of {{mvar|ρ}} (from 2 to 3) and of {{mvar|φ}} (from 0 to {{pi}}). Now we change the function:\n\n:<math>f(x,y) = x \\longrightarrow f(\\rho,\\varphi) = \\rho \\cos \\varphi.</math>\n\nfinally let's apply the integration formula:\n\n:<math>\\iint_D x \\, dx\\, dy = \\iint_T \\rho \\cos \\varphi \\rho \\, d\\rho\\, d\\varphi.</math>\n\nOnce the intervals are known, you have\n\n:<math>\\int_0^\\pi \\int_2^3 \\rho^2 \\cos \\varphi \\, d \\rho \\, d \\varphi = \\int_0^\\pi \\cos \\varphi \\ d \\varphi \\left[ \\frac{\\rho^3}{3} \\right]_2^3 = \\Big[ \\sin \\varphi \\Big]_0^\\pi \\ \\left(9 - \\frac{8}{3} \\right) = 0.</math></blockquote>\n\n====Cylindrical coordinates====\n[[File:Cylindrical Coordinates.svg|thumb|right|190px|Cylindrical coordinates.]]\nIn {{math|'''R'''<sup>3</sup>}} the integration on domains with a circular base can be made by the ''passage to [[Cylindrical coordinate system|cylindrical coordinates]]''; the transformation of the function is made by the following relation:\n:<math>f(x,y,z) \\rightarrow f(\\rho \\cos \\varphi, \\rho \\sin \\varphi, z)</math>\n\nThe domain transformation can be graphically attained, because only the shape of the base varies, while the height follows the shape of the starting region.\n\n<blockquote>'''Example 3a.''' The region is {{math|''D'' {{=}} {''x''<sup>2</sup> + ''y''<sup>2</sup> ≤ 9, ''x''<sup>2</sup> + ''y''<sup>2</sup> ≥ 4, 0 ≤ ''z'' ≤ 5<nowiki>}</nowiki>}} (that is the \"tube\" whose base is the circular crown of Example 2d and whose height is 5); if the transformation is applied, this region is obtained:\n:<math>T = \\{ 2 \\le \\rho \\le 3, \\ 0 \\le \\varphi \\le 2 \\pi, \\ 0 \\le z \\le 5 \\}</math>\n(that is, the parallelepiped whose base is similar to the rectangle in Example 2d and whose height is 5).\n\nBecause the {{mvar|z}} component is unvaried during the transformation, the {{mvar|dx dy dz}} differentials vary as in the passage to polar coordinates: therefore, they become {{mvar|ρ dρ dφ dz}}.\n\nFinally, it is possible to apply the final formula to cylindrical coordinates:\n\n:<math>\\iiint_D f(x,y,z) \\, dx\\, dy\\, dz = \\iiint_T f(\\rho \\cos \\varphi, \\rho \\sin \\varphi, z) \\rho \\, d\\rho\\, d\\varphi\\, dz.</math>\n\nThis method is convenient in case of cylindrical or conical domains or in regions where it is easy to individuate the ''z'' interval and even transform the circular base and the function.</blockquote>\n\n<blockquote>'''Example 3b.''' The function is {{math|''f''(''x'', ''y'', ''z'') {{=}} ''x''<sup>2</sup> + ''y''<sup>2</sup> + ''z''}} and as integration domain this [[cylinder (geometry)|cylinder]]: {{math|''D'' {{=}} {''x''<sup>2</sup> + ''y''<sup>2</sup> ≤ 9, −5 ≤ ''z'' ≤ 5 <nowiki>}</nowiki>}}. The transformation of {{mvar|D}} in cylindrical coordinates is the following:\n\n:<math>T = \\{ 0 \\le \\rho \\le 3, \\ 0 \\le \\varphi \\le 2 \\pi, \\ -5 \\le z \\le 5 \\}.</math>\n\nwhile the function becomes\n\n:<math>f(\\rho \\cos \\varphi, \\rho \\sin \\varphi, z) = \\rho^2 + z</math>\n\nFinally one can apply the integration formula:\n\n:<math>\\iiint_D \\left(x^2 + y^2 +z\\right) \\, dx\\, dy\\, dz = \\iiint_T \\left( \\rho^2 + z\\right) \\rho \\, d\\rho\\, d\\varphi\\, dz;</math>\n\ndeveloping the formula you have\n\n:<math>\\int_{-5}^5 dz \\int_0^{2 \\pi} d\\varphi \\int_0^3 \\left( \\rho^3 + \\rho z \\right)\\, d\\rho = 2 \\pi \\int_{-5}^5 \\left[ \\frac{\\rho^4}{4} + \\frac{\\rho^2 z}{2} \\right]_0^3 \\, dz = 2 \\pi \\int_{-5}^5 \\left( \\frac{81}{4} + \\frac{9}{2} z\\right)\\, dz = \\cdots = 405 \\pi.</math></blockquote>\n\n====Spherical coordinates====\n[[File:Spherical Coordinates (Colatitude, Longitude).svg|thumb|right|190px|Spherical coordinates.]]\nIn {{math|'''R'''<sup>3</sup>}} some domains have a spherical symmetry, so it's possible to specify the coordinates of every point of the integration region by two angles and one distance. It's possible to use therefore the ''passage to [[Spherical coordinate system|spherical coordinates]]''; the function is transformed by this relation:\n:<math>f(x,y,z) \\longrightarrow f(\\rho \\cos \\theta \\sin \\varphi, \\rho \\sin \\theta \\sin \\varphi, \\rho \\cos \\varphi)</math>\n\nPoints on the {{mvar|z}}-axis do not have a precise characterization in spherical coordinates, so {{mvar|θ}} can vary between 0 and 2{{pi}}.\n\nThe better integration domain for this passage is the sphere.\n\n<blockquote>'''Example 4a.''' The domain is {{math|''D'' {{=}} ''x''<sup>2</sup> + ''y''<sup>2</sup> + ''z''<sup>2</sup> ≤ 16}} (sphere with radius 4 and center at the origin); applying the transformation you get the region\n:<math>T = \\{ 0 \\le \\rho \\le 4, \\ 0 \\le \\varphi \\le \\pi, \\ 0 \\le \\theta \\le 2 \\pi \\}.</math>\n\nThe Jacobian determinant of this transformation is the following:\n\n:<math>\\frac{\\partial (x,y,z)}{\\partial (\\rho, \\theta, \\varphi)} = \\begin{vmatrix}\n\\cos \\theta \\sin \\varphi & - \\rho \\sin \\theta \\sin \\varphi & \\rho \\cos \\theta \\cos \\varphi \\\\\n\\sin \\theta \\sin \\varphi & \\rho \\cos \\theta \\sin \\varphi & \\rho \\sin \\theta \\cos \\varphi \\\\\n\\cos \\varphi & 0 & - \\rho \\sin \\varphi \\end{vmatrix} = \\rho^2 \\sin \\varphi</math>\n\nThe {{mvar|dx dy dz}} differentials therefore are transformed to {{math|''ρ''<sup>2</sup> sin(''φ'') ''dρ'' ''dθ'' ''dφ''}}.\n\nThis yields the final integration formula:\n\n:<math>\\iiint_D f(x,y,z) \\, dx\\, dy\\, dz = \\iiint_T f(\\rho \\sin \\varphi \\cos \\theta, \\rho \\sin \\varphi \\sin \\theta, \\rho \\cos \\varphi) \\rho^2 \\sin \\varphi \\, d\\rho\\, d\\theta\\, d\\varphi.</math></blockquote>\n\nIt is better to use this method in case of spherical domains '''and''' in case of functions that can be easily simplified by the first fundamental relation of trigonometry extended to {{math|'''R'''<sup>3</sup>}} (see Example 4b); in other cases it can be better to use cylindrical coordinates (see Example 4c).\n\n:<math>\\iiint_T f(a,b,c) \\rho^2 \\sin \\varphi \\, d\\rho\\, d\\theta\\, d\\varphi.</math>\n\nThe extra {{math|''ρ''<sup>2</sup>}} and {{math|sin ''φ''}} come from the Jacobian.\n\nIn the following examples the roles of {{mvar|φ}} and {{mvar|θ}} have been reversed.\n\n<blockquote>'''Example 4b.''' {{mvar|D}} is the same region as in Example 4a and {{math|''f''(''x'', ''y'', ''z'') {{=}} ''x''<sup>2</sup> + ''y''<sup>2</sup> + ''z''<sup>2</sup>}} is the function to integrate. Its transformation is very easy:\n\n:<math>f(\\rho \\sin \\varphi \\cos \\theta, \\rho \\sin \\varphi \\sin \\theta, \\rho \\cos \\varphi) = \\rho^2,</math>\n\nwhile we know the intervals of the transformed region {{mvar|T}} from {{mvar|D}}:\n\n:<math>T=\\{0 \\le \\rho \\le 4, \\ 0 \\le \\varphi \\le \\pi, \\ 0 \\le \\theta \\le 2 \\pi\\}.</math>\n\nWe therefore apply the integration formula:\n\n:<math>\\iiint_D \\left(x^2 + y^2 + z^2\\right) \\, dx\\, dy\\, dz = \\iiint_T \\rho^2 \\, \\rho^2 \\sin \\theta \\, d\\rho\\, d\\theta\\, d\\varphi,</math>\n\nand, developing, we get\n\n:<math>\\iiint_T \\rho^4 \\sin \\theta \\, d\\rho\\, d\\theta\\, d\\varphi = \\int_0^{\\pi} \\sin \\varphi \\,d\\varphi \\int_0^4 \\rho^4 d \\rho \\int_0^{2 \\pi} d\\theta = 2 \\pi \\int_0^{\\pi} \\sin \\varphi \\left[ \\frac{\\rho^5}{5} \\right]_0^4 \\, d \\varphi = 2 \\pi \\left[ \\frac{\\rho^5}{5} \\right]_0^4 \\Big[- \\cos \\varphi \\Big]_0^{\\pi} = \\frac{4096 \\pi}{5}.</math></blockquote>\n\n<blockquote>'''Example 4c.''' The domain {{mvar|D}} is the ball with center at the origin and radius {{math|3''a''}},\n\n:<math>D = \\left \\{ x^2 + y^2 + z^2 \\le 9a^2 \\right \\}</math>\n\nand {{math|''f''(''x'', ''y'', ''z'') {{=}} ''x''<sup>2</sup> + ''y''<sup>2</sup>}} is the function to integrate.\n\nLooking at the domain, it seems convenient to adopt the passage to spherical coordinates, in fact, the intervals of the variables that delimit the new {{mvar|T}} region are obviously:\n\n:<math>T=\\{0 \\le \\rho \\le 3a, \\ 0 \\le \\varphi \\le 2 \\pi, \\ 0 \\le \\theta \\le \\pi\\}.</math>\n\nHowever, applying the transformation, we get\n\n:<math>f(x,y,z) = x^2 + y^2 \\longrightarrow \\rho^2 \\sin^2 \\theta \\cos^2 \\varphi + \\rho^2 \\sin^2 \\theta \\sin^2 \\varphi = \\rho^2 \\sin^2 \\theta</math>.\n\nApplying the formula for integration we obtain:\n\n:<math>\\iiint_T \\rho^2 \\sin^2 \\theta \\rho^2 \\sin \\theta \\, d\\rho\\, d\\theta\\, d\\varphi = \\iiint_T \\rho^4 \\sin^3 \\theta \\, d\\rho\\, d\\theta\\, d\\varphi</math>\n\nwhich is very hard to solve. This problem will be solved by using the passage to cylindrical coordinates. The new {{mvar|T}} intervals are\n\n:<math>T=\\left\\{0 \\le \\rho \\le 3a, \\ 0 \\le \\varphi \\le 2 \\pi, \\ - \\sqrt{9a^2 - \\rho^2} \\le z \\le \\sqrt{9a^2 - \\rho^2}\\right\\};</math>\n\nthe {{mvar|z}} interval has been obtained by dividing the ball into two [[sphere|hemisphere]]s simply by solving the [[inequality (mathematics)|inequality]] from the formula of {{mvar|D}} (and then directly transforming {{math|''x''<sup>2</sup> + ''y''<sup>2</sup>}} into {{math|''ρ''<sup>2</sup>}}). The new function is simply {{math|''ρ''<sup>2</sup>}}. Applying the integration formula\n\n:<math>\\iiint_T \\rho^2 \\rho \\, d \\rho \\, d \\varphi \\, dz.</math>\n\nThen we get\n\n:<math>\\begin{align} \\int_0^{2\\pi} d\\varphi \\int_0^{3a} \\rho^3 d\\rho \\int_{-\\sqrt{9a^2 - \\rho^2}}^{\\sqrt{9 a^2 - \\rho^2}}\\, dz &= 2 \\pi \\int_0^{3a} 2 \\rho^3 \\sqrt{9 a^2 - \\rho^2} \\, d\\rho \\\\\n&= -2 \\pi \\int_{9 a^2}^0 (9 a^2 - t) \\sqrt{t}\\, dt && t = 9 a^2 - \\rho^2 \\\\\n&= 2 \\pi \\int_0^{9 a^2} \\left ( 9 a^2 \\sqrt{t} - t \\sqrt{t} \\right ) \\, dt \\\\\n&= 2 \\pi \\left( \\int_0^{9 a^2} 9 a^2 \\sqrt{t} \\, dt - \\int_0^{9 a^2} t \\sqrt{t} \\, dt\\right) \\\\\n&= 2 \\pi \\left[9 a^2 \\frac23 t^{ \\frac32 } - \\frac{2}{5} t^{ \\frac{5}{2}} \\right]_0^{9 a^2} \\\\\n&= 2 \\cdot 27 \\pi a^5 \\left ( 6 - \\frac{18}{5} \\right ) \\\\\n&= \\frac{648 \\pi}{5} a^5. \\end{align}</math>\n\nThanks to the passage to cylindrical coordinates it was possible to reduce the triple integral to an easier one-variable integral.</blockquote>\n\nSee also the differential volume entry in [[nabla in cylindrical and spherical coordinates]].\n\n==Examples==\n\n=== Double integral over a rectangle ===\nLet us assume that we wish to integrate a multivariable function {{mvar|f}} over a region {{mvar|A}}:\n:<math>A = \\left \\{ (x,y) \\in \\mathbf{R}^2 \\ : \\ 11 \\le x \\le 14 \\ ; \\ 7 \\le y \\le 10 \\right \\} \\mbox{ and } f(x,y) = x^2 + 4y\\,</math>\n\nFrom this we formulate the iterated integral\n\n:<math>\\int_7^{10} \\int_{11}^{14} (x^2 + 4y) \\, dx\\, dy </math>\n\nThe inner integral is performed first, integrating with respect to {{mvar|x}} and taking {{mvar|y}} as a constant, as it is not the [[variable of integration]]. The result of this integral, which is a function depending only on {{mvar|y}}, is then integrated with respect to {{mvar|y}}.\n\n:<math>\\begin{align}\n\\int_{11}^{14} \\left(x^2 + 4y\\right) \\, dx & = \\left [\\frac13 x^3 + 4yx \\right]_{x=11}^{x=14} \\\\\n&= \\frac13(14)^3 + 4y(14) - \\frac13(11)^3 - 4y(11) \\\\\n&= 471 + 12y \\end{align}</math>\n\nWe then integrate the result with respect to {{mvar|y}}.\n\n:<math>\\begin{align}\n\\int_7^{10} (471 + 12y) \\ dy & = \\Big[471y + 6y^2\\Big]_{y=7}^{y=10} \\\\\n&= 471(10)+ 6(10)^2 - 471(7) - 6(7)^2 \\\\\n&= 1719 \\end{align}</math>\n\nIn cases where the double integral of the absolute value of the function is finite, the order of integration is interchangeable, that is, integrating with respect to ''x'' first and integrating with respect to ''y'' first produce the same result. That is [[Fubini's theorem]]. For example, doing the previous calculation with order reversed gives the same result:\n\n:<math> \\begin{align}\n\\int_{11}^{14} \\int_{7}^{10} \\, \\left(x^2 + 4y\\right) \\, dy\\, dx & = \\int_{11}^{14} \\Big[x^2 y + 2y^2 \\Big]_{y=7}^{y=10} \\, dx \\\\\n&= \\int_{11}^{14} \\, (3x^2 + 102) \\, dx \\\\\n&= \\Big[x^3 + 102x \\Big]_{x=11}^{x=14} \\\\\n&= 1719. \\end{align}</math>\n\n=== Double integral over a normal domain ===\n[[File:Esempio-formulediriduzione-r2.svg|thumb|160px|right|Example: double integral over the normal region ''D'']]\n\nConsider the region (please see the graphic in the example):\n:<math>D = \\{ (x,y) \\in \\mathbf{R}^2 \\ : \\ x \\ge 0, y \\le 1, y \\ge x^2 \\}</math> \nCalculate\n\n:<math>\\iint_D (x+y) \\, dx \\, dy.</math>\n\nThis domain is normal with respect to both the ''x''- and ''y''-axes. To apply the formulae it is required to find the functions that determine ''D'' and the intervals over which these functions are defined. In this case the two functions are:\n\n:<math>\\alpha (x) = x^2\\text{ and }\\beta (x) = 1</math>\n\nwhile the interval is given by the intersections of the functions with ''x''&nbsp;=&nbsp;0, so the interval is [''a'',&nbsp;''b''] = [0,&nbsp;1] (normality has been chosen with respect to the ''x''-axis for a better visual understanding).\n\nIt is now possible to apply the formula:\n\n:<math>\\iint_D (x+y) \\, dx \\, dy = \\int_0^1 dx \\int_{x^2}^1 (x+y) \\, dy = \\int_0^1 dx \\ \\left[xy + \\frac{y^2}{2} \\right]^1_{x^2}</math>\n\n(at first the second integral is calculated considering ''x'' as a constant). The remaining operations consist of applying the basic techniques of integration:\n\n:<math>\\int_0^1 \\left[xy + \\frac{y^2}{2}\\right]^1_{x^2} \\, dx = \\int_0^1 \\left(x + \\frac{1}{2} - x^3 - \\frac{x^4}{2} \\right) dx = \\cdots = \\frac{13}{20}.</math>\n\nIf we choose normality with respect to the ''y''-axis we could calculate\n\n:<math>\\int_0^1 dy \\int_0^{\\sqrt{y}} (x+y) \\, dx.</math>\n\nand obtain the same value.\n[[File:Dominio-normalità r3 esempio.svg|thumb|160px|right|Example of domain in '''R'''<sup>3</sup> that is normal with respect to the ''xy''-plane.]]\n\n=== Calculating volume ===\nUsing the methods previously described, it is possible to calculate the volumes of some common solids.\n* '''[[Cylinder (geometry)|Cylinder]]''': The volume of a cylinder with height {{mvar|h}} and circular base of radius {{mvar|R}} can be calculated by integrating the constant function {{mvar|h}} over the circular base, using polar coordinates.\n::<math>\\mathrm{Volume} = \\int_0^{2\\pi} d \\varphi\\, \\int_0^R h \\rho \\, d \\rho = 2 \\pi h \\left[\\frac{\\rho^2}{2}\\right]_0^R = \\pi R^2 h</math>\n\nThis is in agreement with the formula for the volume of a [[prism]]\n\n::<math>\\mathrm{Volume} = \\text{base area} \\times \\text{height}. </math>\n\n* '''[[Sphere]]''': The volume of a sphere with radius {{mvar|R}} can be calculated by integrating the constant function 1 over the sphere, using spherical coordinates.\n\n::<math>\\begin{align} \\text{Volume} &= \\iiint_D f(x,y,z) \\, dx\\, dy\\, dz \\\\\n&= \\iiint_D 1 \\, dV \\\\\n&= \\iiint_S \\rho^2 \\sin \\varphi \\, d\\rho\\, d\\theta\\, d\\varphi \\\\\n&= \\int_0^{2\\pi} \\, d \\theta \\int_0^{ \\pi } \\sin \\varphi\\, d \\varphi \\int_0^R \\rho^2\\, d \\rho \\\\\n&= 2 \\pi \\int_0^\\pi \\sin \\varphi\\, d \\varphi \\int_0^R \\rho^2\\, d \\rho \\\\\n&= 2 \\pi \\int_0^\\pi \\sin \\varphi \\frac{R^3}{3 }\\, d \\varphi \\\\\n&= \\frac23 \\pi R^3 \\Big[-\\cos \\varphi\\Big]_0^\\pi = \\frac43 \\pi R^3.\n\\end{align}</math>\n\n* '''[[Tetrahedron]]''' (triangular [[pyramid]] or 3-[[simplex]]): The volume of a tetrahedron with its apex at the origin and edges of length {{mvar|''ℓ''}} along the {{mvar|x}}-, {{mvar|y}}- and {{mvar|z}}-axes can be calculated by integrating the constant function 1 over the tetrahedron.\n\n::<math>\\begin{align} \\text{Volume} &= \\int_0^\\ell dx \\int_0^{\\ell-x}\\, dy \\int_0^{\\ell-x-y }\\, dz \\\\\n&= \\int_0^\\ell dx \\int_0^{\\ell-x } (\\ell - x - y)\\, dy \\\\\n&= \\int_0^\\ell \\left( l^2 - 2 \\ell x + x^2 - \\frac{(\\ell-x)^2 }{2}\\right)\\, dx \\\\\n&= \\ell^3 - \\ell \\ell^2 + \\frac{\\ell^3}{3 } - \\left[\\frac{\\ell^2 x}{2} - \\frac{ \\ell x^2}{2} + \\frac{x^3}{6 }\\right]_0^ \\ell \\\\\n&= \\frac{\\ell^3}{3} - \\frac{\\ell^3}{6} = \\frac{ \\ell^3}{6}\\end{align}</math>\n\n:This is in agreement with the formula for the volume of a [[pyramid]]\n\n::<math>\\mathrm{Volume} = \\frac13 \\times \\text{base area} \\times \\text{height} = \\frac13 \\times \\frac{\\ell^2}{2} \\times \\ell = \\frac{ \\ell^3}{6}.</math>\n\n[[File:Dominio improprio.svg|thumb|right|140px|Example of an improper domain.]]\n\n==Multiple improper integral==\nIn case of unbounded domains or functions not bounded near the boundary of the domain, we have to introduce the '''double [[improper integral]]''' or the '''triple improper integral'''.\n\n==Multiple integrals and iterated integrals==\n{{See also|Order of integration (calculus)}}\n[[Fubini's theorem]] states that if<ref name =\"a\" />\n:<math>\\iint_{A\\times B} \\left|f(x,y)\\right|\\,d(x,y)<\\infty,</math>\nthat is, if the integral is absolutely convergent, then the multiple integral will give the same result as either of the two iterated integrals:\n:<math>\\iint_{A\\times B} f(x,y)\\,d(x,y)=\\int_A\\left(\\int_B f(x,y)\\,dy\\right)\\,dx=\\int_B\\left(\\int_A f(x,y)\\,dx\\right)\\,dy.</math>\nIn particular this will occur if {{math|{{abs|''f''(''x'', ''y'')}}}} is a [[bounded function]] and {{mvar|A}} and {{mvar|B}} are [[bounded set]]s.\n\nIf the integral is not absolutely convergent, care is needed not to confuse the concepts of ''multiple integral'' and ''iterated integral'', especially since the same notation is often used for either concept. The notation\n\n:<math>\\int_0^1\\int_0^1 f(x,y)\\,dy\\,dx</math>\n\nmeans, in some cases, an iterated integral rather than a true double integral. In an iterated integral, the outer integral\n\n:<math>\\int_0^1 \\cdots \\, dx</math>\n\nis the integral with respect to {{mvar|x}} of the following function of {{mvar|x}}:\n\n:<math>g(x)=\\int_0^1 f(x,y)\\,dy.</math>\n\nA double integral, on the other hand, is defined with respect to area in the {{mvar|xy}}-plane. If the double integral exists, then it is equal to each of the two iterated integrals (either \"{{mvar|dy dx}}\" or \"{{mvar|dx dy}}\") and one often computes it by computing either of the iterated integrals. But sometimes the two iterated integrals exist when the double integral does not, and in some such cases the two iterated integrals are different numbers, i.e., one has\n\n:<math>\\int_0^1\\int_0^1 f(x,y)\\,dy\\,dx \\neq \\int_0^1\\int_0^1 f(x,y)\\,dx\\,dy.</math>\n\nThis is an instance of rearrangement of a [[conditional convergence|conditionally convergent]] integral.\n\nOn the other hand, some conditions ensure that the two iterated integrals are equal even though the double integral need not exist. By the [[Grigorii Fichtenholz|Fichtenholz]]–[[Leon Lichtenstein|Lichtenstein]] theorem, if {{mvar|f}} is bounded on {{math|[0, 1] × [0, 1]}} and both iterated integrals exist, then they are equal. Moreover, existence of the inner integrals ensures existence of the outer integrals.<ref>{{cite book |last=Lewin |first=Jonathan |title=An Interactive Introduction to Mathematical Analysis |publisher=Cambridge |year=2003 |at=Sect. 16.6|ISBN=978-1107694040}}</ref><ref>{{cite journal |last=Lewin |first=Jonathan |title=Some applications of the bounded convergence theorem for an introductory course in analysis |date=1987 |journal=The American Mathematical Monthly |publisher=AMS |volume=94 |issue=10 |pages=988–993 |doi=10.2307/2322609}}</ref><ref>{{cite journal |last=Sinclair |first=George Edward |title=A finitely additive generalization of the Fichtenholz–Lichtenstein theorem|date=1974 |journal=Transactions of the American Mathematical Society |publisher=AMS |volume=193 |pages=359–374 |doi=10.2307/1996919}}</ref> The double integral need not exist in this case even as [[Lebesgue integration|Lebesgue integral]], according to [[Wacław Sierpiński|Sierpiński]].<ref>{{cite book |last=Bogachev |first=Vladimir I. |title=Measure Theory |volume=1 |publisher=Springer |year=2006 |at=Item 3.10.49}}{{ISBN missing}}</ref>\n\nThe notation\n:<math>\\int_{[0,1]\\times[0,1]} f(x,y)\\,dx\\,dy</math>\n\nmay be used if one wishes to be emphatic about intending a double integral rather than an iterated integral.\n\n==Some practical applications==\nQuite generally, just as in one variable, one can use the multiple integral to find the average of a function over a given set. Given a set {{math|''D'' ⊆ '''R'''<sup>''n''</sup>}} and an integrable function {{mvar|f}} over {{mvar|D}}, the average value of {{mvar|f}} over its domain is given by\n\n:<math>\\bar{f} = \\frac{1}{m(D)} \\int_D f(x)\\, dx,</math>\n\nwhere {{math|''m''(''D'')}} is the [[Measure (mathematics)|measure]] of {{mvar|D}}.\n\nAdditionally, multiple integrals are used in many applications in [[physics]]. The examples below also show some variations in the notation.\n\nIn [[mechanics]], the [[moment of inertia]] is calculated as the volume integral (triple integral) of the [[density]] weighed with the square of the distance from the axis:\n\n:<math>I_z = \\iiint_V \\rho r^2\\, dV.</math>\n\nThe [[gravitational potential]] associated with a [[mass distribution]] given by a mass [[Borel measure|measure]] {{mvar|dm}} on three-dimensional [[Euclidean space]] {{math|'''R'''<sup>3</sup>}} is<ref>{{Cite book |title=Classical Mechanics |title-link=Classical Mechanics (Kibble and Berkshire) |edition=5th |publisher=[[Imperial College Press]] |year=2004 |isbn=978-1-86094-424-6 |author1 = Kibble |first1 = Tom W. B. |last2 = Berkshire |first2=Frank H.|author2-link=Frank H. Berkshire}}</ref>\n:<math>V(\\mathbf{x}) = -\\iiint_{\\mathbf{R}^3} \\frac{G}{|\\mathbf{x} - \\mathbf{y}|}\\,dm(\\mathbf{y}).</math>\n\nIf there is a continuous function {{math|''ρ''('''x''')}} representing the density of the distribution at {{math|'''x'''}}, so that {{math|''dm''('''x''') {{=}} ''ρ''('''x''')''d''{{isup|3}}'''x'''}}, where {{math|''d''{{isup|3}}'''x'''}} is the Euclidean [[volume element]], then the gravitational potential is\n:<math>V(\\mathbf{x}) = -\\iiint_{\\mathbf{R}^3} \\frac{G}{|\\mathbf{x}-\\mathbf{y}|}\\,\\rho(\\mathbf{y})\\,d^3\\mathbf{y}.</math>\n\nIn [[electromagnetism]], [[Maxwell's equations]] can be written using multiple integrals to calculate the total magnetic and electric fields.<ref>{{cite book | last = Jackson | first = John D. | title = Classical Electrodynamics |author-link=John David Jackson (physicist) |edition = 3rd | publisher = Wiley | year = 1998 | isbn = 0-471-30932-X}}</ref> In the following example, the [[electric field]] produced by a distribution of [[Electric charge|charges]] given by the volume [[charge density]] {{math|''ρ''( {{vec|''r''}} )}} is obtained by a ''triple integral'' of a vector function:\n\n:<math>\\vec E = \\frac {1}{4 \\pi \\varepsilon_0} \\iiint \\frac {\\vec r - \\vec r'}{\\left \\| \\vec r - \\vec r' \\right \\|^3} \\rho (\\vec r')\\, d^3 r'.</math>\n\nThis can also be written as an integral with respect to a [[signed measure]] representing the charge distribution.\n\n==See also==\n* Main [[real analysis|analysis]] theorems that relate multiple integrals:\n** [[Divergence theorem]]\n** [[Stokes' theorem]]\n** [[Green's theorem]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n* {{cite book|first=Robert A. |last=Adams |title=Calculus: A Complete Course |edition=5th |ISBN=0-201-79131-5}}\n* {{cite book|first1=R. K. |last1=Jain |first2=S. R. K. |last2=Iyengar |title=Advanced Engineering Mathematics |edition=3rd |date=2009 |publisher=Narosa Publishing House |ISBN=978-81-7319-730-7}}\n\n==External links==\n* {{MathWorld | urlname=MultipleIntegral | title=Multiple Integral}}\n* {{SpringerEOM | urlname=M/m065370 | title=Multiple integral | author=L.D. Kudryavtsev}}\n* [http://user.mendelu.cz/marik/maw/index.php?lang=en&form=integral2 Mathematical Assistant on Web] online evaluation of double integrals in [[Cartesian coordinate system|Cartesian coordinates]] and [[polar coordinate system|polar coordinates]] (includes intermediate steps in the solution, powered by [[Maxima (software)]])\n\n[[Category:Integral calculus]]\n[[Category:Multivariable calculus]]\n\n[[de:Integralrechnung#Mehrdimensionale Integration]]"
    },
    {
      "title": "Nonelementary integral",
      "url": "https://en.wikipedia.org/wiki/Nonelementary_integral",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Integrals not expressible in closed-form from elementary functions}}\n{{multiple issues|\n{{Expert needed|Mathematics|date=March 2011}}\n{{More citations needed|date=December 2009}}\n}}\nIn [[mathematics]], a '''nonelementary antiderivative''' of a given elementary function is an antiderivative that is, itself, not an ''[[elementary function]]'' (i.e. a function constructed from a finite number of quotients of constant, algebraic, exponential, and logarithmic functions using field operations).<ref name= Weisstein >Weisstein, Eric W. \"Elementary Function.\" From MathWorld--A Wolfram Web Resource. http://mathworld.wolfram.com/ElementaryFunction.html From [[MathWorld]] Accessed 24 Apr 2017.</ref>  A [[Liouville's theorem (differential algebra)|theorem by Liouville]] in 1835 provided the first proof that nonelementary antiderivatives exist.<ref>{{cite book| title=The Calculus Gallery| first=William| last=Dunham |isbn=978-0-691-13626-4 |publisher=Princeton |year=2005| page=119}}</ref> This theorem also provides a basis for the [[Risch algorithm]] for determining (with difficulty) which elementary functions have elementary antiderivatives. It can be shown that, if one is given a function of any complexity, the probability that it will have an elementary [[antiderivative]] is very low.{{Citation needed|date=July 2010}}\n\nSome examples of such functions are:\n\n*<math>\\sqrt{1 - x^4}</math><ref name= Weisstein/> (see [[Elliptic integral]])\n*<math>\\ln(\\ln x) \\,</math>\n*<math>\\frac{1}{\\ln x}</math><ref>[http://www2.maths.ox.ac.uk/cmi/library/academy/LectureNotes05/Conrad.pdf Impossibility theorems for elementary integration;] Brian Conrad. [[Clay Mathematics Institute]]: 2005 Academy Colloquium Series. Accessed 14 Jul 2014.</ref> (see [[Logarithmic integral function|Logarithmic integral]])\n*<math>\\frac{e^x}{x}</math> (see [[Exponential integral]])\n*<math>e^{e^x} \\,</math>\n*<math>e^{-\\frac{x^2}{2}} \\,</math><ref name= Weisstein/> (see [[Error function]] and [[Gaussian integral]])\n*<math>\\sin(x^2)</math> and <math>\\cos(x^2)</math> (see [[Fresnel integral]])\n*<math>\\frac{\\sin(x)}{x} = \\operatorname{sinc}(x) </math> (see [[Sine integral]] and [[Dirichlet integral]])\n\nThe evaluation of nonelementary antiderivatives can often be done using [[Taylor series]]. This is because [[Analytic function|Taylor series can ''always'' be integrated]] as one would an ordinary [[polynomial]] (using the fact that any Taylor series is uniformly convergent within its radius of convergence), even if there is no elementary antiderivative of the function that generated the Taylor series.\n\nHowever, in some cases it is not possible to rely on Taylor series. For example, if the function is not ''[[infinitely differentiable]]'', one cannot generate a Taylor series. Even if a Taylor series can be generated, there is a good possibility that it will [[divergent series|diverge]] and ''not'' represent the function one is attempting to antidifferentiate; there even exist non-analytic but infinitely differentiable real-valued functions (see [[bump function]]). Many functions which are infinitely differentiable have higher order [[derivative]]s that are unmanageable by hand. In these cases, it is not possible to evaluate [[indefinite integral]]s, but [[definite integral]]s can be evaluated numerically, for instance by [[Simpson's rule]]. There are yet other cases (such as the [[Gaussian integral]]) where definite integrals can be evaluated exactly without numerical methods, but indefinite integrals cannot, for lack of an elementary antiderivative.\n\nThe integrals for many of these functions can be written down if one allows so-called “special” (nonelementary) functions. For example, the first example's integral is expressible using [[Elliptic integral#Incomplete elliptic integral of the first kind|incomplete elliptic integrals of the first kind]], the second and third use the [[logarithmic integral]], the fourth the [[exponential integral]], and the sixth the [[error function]]. Still, there exist functions, such as <math>x^x</math> and <math>\\sin(\\sin(x))</math> for which no notation currently exists{{citation needed|date=April 2017}} to describe their integrals (other than the use of the integrals themselves).\n\nThe closure under integration of the set of the elementary functions is the set of the [[Liouvillian function]]s.\n\n==See also==\n* [[List of integrals]]\n* [[Derivative]]s\n* [[Symbolic integration]]s\n* [[Algebraic function]]s \n* [[Transcendental function]]s\n\n==References==\n<references/>\n* [http://www.sosmath.com/calculus/integration/fant/fant.html Integration of Nonelementary Functions], S.O.S MATHematics.com; accessed 7 Dec 2012.\n\n==Further reading==\n* Williams, Dana P., [http://www.math.dartmouth.edu/~dana/bookspapers/elementary.pdf NONELEMENTARY ANTIDERIVATIVES], 1 Dec 1993. Accessed January 24, 2014.\n\n{{DEFAULTSORT:Nonelementary Integral}}\n[[Category:Integral calculus]]\n[[Category:Integrals]]"
    },
    {
      "title": "Order of integration (calculus)",
      "url": "https://en.wikipedia.org/wiki/Order_of_integration_%28calculus%29",
      "text": "{{Short description|the order in which multiple or iterated integrals are computed}}\n{{for|the summary statistic in time series|Order of integration}}\n{{Calculus |Integral}}\n\nIn [[calculus]], interchange of the '''order of integration''' is a methodology that transforms [[iterated integral]]s (or [[multiple integral]]s through the use of [[Fubini's theorem]]) of functions into other, hopefully simpler, integrals by changing the order in which the integrations are performed.  In some cases, the order of integration can be validly interchanged; in others it cannot.\n\n== Problem statement ==\n\nThe problem for examination is evaluation of an integral of the form\n\n:<math> \\iint_D \\ f(x,y ) \\ dx \\,dy , </math>\n\nwhere ''D'' is some two-dimensional area in the ''xy''–plane. For some functions ''f'' straightforward integration is feasible, but where that is not true, the integral can sometimes be reduced to simpler form by changing the order of integration. The difficulty with this interchange is determining the change in description of the domain ''D''.\n\nThe method also is applicable to other [[multiple integrals]].<ref name=Dineen>{{cite book |title=Multivariate Calculus and Geometry |author=Seán Dineen |page=162 |url=https://books.google.com/books?id=1YNX3YAf1vMC&pg=PA165&dq=%22order+of+integration%22&lr=&as_brr=0&sig=hF51z9mPjvfq_lisOmHwuLUrM0Q#PPA162,M1\n|isbn=1-85233-472-X |publisher=Springer |year=2001}}</ref><ref name=Courant>{{cite book |title=Introduction to Calculus and Analysis: Vol. II/1, II/2. Classics in mathematics |author= Richard Courant & Fritz John |url=https://books.google.com/books?id=ngkQxS4eicgC&pg=RA3-PA891&dq=%22order+of+integration%22&lr=&as_brr=0&sig=EKZQrRpPezYV_67Bj71WN8awPyg#PRA3-PA897,M1\n|page=897 |isbn=3-540-66569-2 |year=2000 |publisher=Springer   }}</ref>\n\nSometimes, even though a full evaluation is difficult, or perhaps requires a numerical integration, a double integral can be reduced to a single integration, as illustrated next. Reduction to a single integration makes a [[Numerical integration|numerical evaluation]] much easier and more efficient.\n\n==Relation to integration by parts==\n[[File:Integration Order.svg|thumb|300px|left |Figure 1: Integration over the triangular area can be done using vertical or horizontal strips as the first step. This is an overhead view, looking down the z-axis onto the x-y plane. The sloped line is the curve ''y = x''.]]\nConsider the iterated integral \n:<math> \\int_a^z \\, \\int_a^x \\, h(y) \\, dy \\, dx \\ </math>,\nwhich we will write using the prefix notation commonly seen in physics:\n:<math> \\int_a^z dx \\, \\int_a^x \\, h(y) \\, dy </math>.\nIn this expression, the second integral is calculated first with respect to y and x is held constant—a strip of width ''dx'' is integrated first over the ''y''-direction (a strip of width dx in the x direction is integrated with respect to the y variable across the y direction), adding up an infinite amount of rectangles of width ''dy'' along the y-axis.  This forms a three dimensional slice ''dx'' wide along the x-axis, from y=a to y=x along the y axis, and in the z direction z=f(x,y). Notice that if the thickness dx is infinitesimal, x varies only infinitesimally on the slice. We can assume that x is constant.<ref name=OSU>{{cite web |publisher=Department of Mathematics, Oregon State University |title=Double Integrals |date=1996 |url=https://math.oregonstate.edu/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/255doub/255doub.html }}</ref> This integration is as shown in the left panel of Figure 1, but is inconvenient especially when the function ''h ( y )'' is not easily integrated. The integral can be reduced to a single integration by reversing the order of integration as shown in the right panel of the figure. To accomplish this interchange of variables, the strip of width ''dy'' is first integrated from the line ''x = y'' to the limit ''x = z'', and then the result is integrated from ''y = a'' to ''y = z'', resulting in:\n\n:<math> \\int_a^z \\ dx\\ \\int_a^x \\ h(y) \\ dy \\   = \\int_a^z \\ h(y)\\ dy \\  \\ \\int_y^z \\ dx = \\int_a^z \\ \\left(z-y\\right) h(y)\\, dy \\ .</math>\n\nThis result can be seen to be an example of the formula for [[integration by parts]], as stated below:<ref>The ''[[Prime (symbol)|prime]]'' \"''' <nowiki>′</nowiki> '''\" denotes a derivative in [[Lagrange's notation]].</ref>\n\n:<math>\\int_a^z f(x) g'(x)\\, dx = \\left[ f(x) g(x) \\right]_a^z - \\int_a^z  f'(x) g(x)\\, dx\\!</math>\n\nSubstitute:\n\n:<math> f (x) = \\int_a^x \\ h(y)\\, dy \\text{ and }g'(x) = 1 \\ . </math>\n\nWhich gives the result.\n\n==Principal-value integrals==\nFor application to [[Cauchy principal value|principal-value integrals]], see  Whittaker and Watson,<ref name=Whittaker>{{cite book |title=A course of modern analysis : an introduction to the general theory of infinite processes and of analytic functions, with an account of the principal transcendental functions |author=Edmund Taylor Whittaker & George Neville Watson |page= §4.51, p. 75 |url=https://books.google.com/books?id=ULVdGZmi9VcC&pg=PA75&dq=principal+value+%22order+of++integration%22&lr=&as_brr=0&sig=bx7JymRGBqKQ-K9KHRwRJk8XoYI\n|isbn=0-521-58807-3 |year=1927 |publisher=Cambridge University Press |edition=4th ed., repr   }}</ref> Gakhov,<ref name=Gakhov>{{cite book |title=Boundary Value Problems |page=46 |author=F. D. Gakhov |url=https://books.google.com/books?id=9G7sfwTDv8QC&pg=PA46&dq=%22order+of+integration%22&lr=&as_brr=0&sig=jw8C_74EzuwTmHXOAPRX9KrNi5g |isbn=0-486-66275-6 |publisher=Courier Dover Publications |year=1990}}</ref> Lu,<ref name=Lu>{{cite book |title=Boundary Value Problems for Analytic Functions |author=Jian-Ke Lu |page= 44 |url=https://books.google.com/books?id=RFafUfgB1dAC&pg=PA43&dq=principal+value+%22order+of++integration%22&lr=&as_brr=0&sig=IpfqQYcG__ljaZWIAqDy9Uji4e4#PPA44,M1\n|isbn=981-02-1020-5 |year=1993 |publisher=World Scientific |location=Singapore  }}</ref> or Zwillinger.<ref name=Zwillinger>{{cite book |title=Handbook of integration |author=Daniel Zwillinger |page=61 |url=https://books.google.com/books?id=DQd4wfV7fo0C&pg=PA61&dq=%22order+of+integration%22&lr=&as_brr=0&sig=pev_kroO46LEqZcxY1qJA01qv9U  |isbn=0-86720-293-9 |year=1992 |publisher=AK Peters Ltd.}}</ref> See also the discussion of the Poincaré-Bertrand transformation in Obolashvili.<ref name= Obolashvili>{{cite book |title=Higher order partial differential equations in Clifford analysis: effective solutions to problems |publisher=Birkhäuser |year=2003  |isbn=0-8176-4286-2 |author=Elena Irodionovna Obolashvili |url=https://books.google.com/books?id=HmvmB6NCyEAC&pg=PA101&dq=principal+value+%22order+of++integration%22&lr=&as_brr=0&sig=1ZcvrKq8hD6Zf-6PL2Bo6H0KEp0\n|page=101  }}</ref> An example where the order of integration cannot be exchanged is given by Kanwal:<ref name=Kanwal>{{cite book |author= Ram P. Kanwal |title=Linear Integral Equations: theory and technique |page= 194 |url =https://books.google.com/books?id=-bV9Qn8NpCYC&pg=PA194&lpg=PA194&dq=+%22Poincar%C3%A9-Bertrand+transformation%22&source=web&ots=iofB7oQccG&sig=2yieQ-eUpZTZtPcZrJJpBZAO-R4&hl=en#PPA194,M1\n|isbn=0-8176-3940-3 |year=1996 |publisher=Birkhäuser |location=Boston |edition=2nd}}</ref>\n\n:<math>\\frac {1}{(2\\pi i )^2} \\int_L^* \\frac{d{\\tau}_1}{{\\tau}_1 - t}\\ \\int_L^*\\ g(\\tau)\\frac{d \\tau}{\\tau-\\tau_1} = \\frac{1}{4} g(t) \\ , </math>\n\nwhile:\n\n:<math>\\frac {1}{(2\\pi i )^2} \\int_L^* g( \\tau ) \\ d \\tau  \\left(  \\int_L^* \\frac{d \\tau_1 } {\\left( \\tau_1 - t\\right) \\left( \\tau-\\tau_1 \\right)} \\right) = 0 \\ . </math>\n\nThe second form is evaluated using a [[Partial fractions in integration|partial fraction]] expansion and an evaluation using the [[Sokhotski&ndash;Plemelj theorem|Sokhotski&ndash;Plemelj formula]]:<ref name=Cima>For a discussion of the Sokhotski-Plemelj formula see, for example, {{cite book |title=The Cauchy Transform |author=Joseph A. Cima, Alec L. Matheson & William T. Ross |page= 56 |url=https://books.google.com/books?id=1sVLg512ffIC&pg=PA56&dq=%22Plemelj+formula%22&lr=&as_brr=0&sig=Li5rr1v7p1elwQGgf7m7m_dpZHE\n|isbn=0-8218-3871-7 |year=2006 |publisher=American Mathematical Society  }} or {{cite book |title=Linear integral equations |author=Rainer Kress |page= Theorem 7.6, p. 101 |url=https://books.google.com/books?id=R3BIOfKssQ4C&pg=PA115&dq=%22Plemelj+formula%22&lr=&as_brr=0&sig=n5qeOMJvwJm8ERxgO3A00VULPs0#PPA101,M1\n|isbn=0-387-98700-2 |year=1999 |publisher=Springer |edition=2nd  }}</ref>\n\n:<math>\\int_L^*\\frac{d \\tau_1}{\\tau_1-t} = \\int_L^* \\frac {d\\tau_1}{\\tau_1-t} = \\pi\\ i \\ . </math>\n\nThe notation <math>\\int_L^*</math> indicates a [[Cauchy principal value]]. See Kanwal.<ref name=Kanwal/>\n\n==Basic theorems==\nA discussion of the basis for reversing the order of integration is found in the book ''Fourier Analysis'' by T.W. Körner.<ref name=\"Körner\">{{cite book |title=Fourier Analysis |author=Thomas William Körner |page=Chapters 47 & 48 |url=https://books.google.com/books?id=DZTDtXs4OQAC&printsec=frontcover&dq=Fourier+analysis+subject:%22Fourier+analysis%22&lr=&as_brr=0&sig=KMLyUTINg93Htbfi3vy7GYoSBFw#PPA226,M1\n|isbn=0-521-38991-7 |publisher=Cambridge University Press |year=1988  }}</ref> He introduces his discussion with an example where interchange of integration leads to two different answers because the conditions of Theorem II below are not satisfied. Here is the example:\n\n:<math>\\int_1^{\\infty} \\frac {x^2-y^2}{\\left(x^2+y^2\\right)^2}\\ dy = \\left[\\frac{y}{x^2+y^2}\\right]_1^{\\infty} = -\\frac{1}{1+x^2} \\ \\left[x \\ge 1 \\right]\\ .</math>\n\n:::<math>\\int_1^{\\infty} \\left( \\int_1^{\\infty}\\frac {x^2-y^2}{\\left(x^2+y^2\\right)^2}\\ dy \\right)\\ dx = -\\frac{\\pi}{4} \\ .</math>\n:::<math>\\int_1^{\\infty} \\left( \\int_1^{\\infty}\\frac {x^2-y^2}{\\left(x^2+y^2\\right)^2}\\ dx \\right)\\ dy = \\frac{\\pi}{4} \\ .</math>\n\nTwo basic theorems governing admissibility of the interchange are quoted below from  Chaudhry and Zubair:<ref name=Chaudry>{{cite book |title=On a Class of Incomplete Gamma Functions with Applications |author=M. Aslam Chaudhry & Syed M. Zubair |page=Appendix C |url=https://books.google.com/books?id=Edf4KrG_vlYC&pg=PA458&dq=%22order+of+integration%22&lr=&as_brr=0&sig=5XNpVp-EoUsEXruZoX7jI0BakdY |isbn=1-58488-143-7 |publisher=CRC Press |year=2001}}</ref>\n{{math_theorem \n|name=Theorem I\n|math_statement= Let ''f''(''x'',&nbsp;''y'') be a continuous function of constant sign defined for ''a ≤ x < ∞'', ''c ≤ y < ∞'', and let the integrals\n{{Center|<math>J(y):= \\int_a^\\infty  dx \\ f(x,\\ y)</math>{{space|10}} and {{space|10}}<math>J^*(x) = \\int_c^\\infty dy\\ f(x, \\ y)</math>}} regarded as functions of the corresponding parameter be, respectively, continuous for ''c ≤ y < ∞'', ''a ≤ x < ∞''. Then if at least one of the iterated integrals\n{{Center|<math>\\int_c^\\infty dy \\ \\left(\\int_a^\\infty dx\\ f(x,\\ y) \\right )</math>{{space|10}} and {{space|10}}<math>\\int_a^\\infty dx \\ \\left(\\int_c^\\infty dy\\ f(x,\\ y) \\right )</math>}} converges, the other integral also converges and their values coincide.\n}}\n{{math_theorem \n|name=Theorem II\n|math_statement= Let ''f''(''x'',&nbsp;''y'') be continuous for ''a ≤ x < ∞'', ''c ≤ y < ∞'', and let the integrals\n{{Center|<math>J(y):= \\int_a^\\infty dx \\ f(x,\\ y)</math>{{space|10}} and {{space|10}}<math>J^*(x) = \\int_c^\\infty dy\\ f(x, \\ y)</math>}} be respectively, uniformly convergent on every finite interval ''c ≤ y < C''  and on every finite interval ''a ≤ x < A''. Then if at least one of the iterated integrals\n{{Center|<math>\\int_c^\\infty dy \\ \\left(\\int_a^\\infty dx\\ |f(x,\\ y)| \\right )</math>{{space|10}} and {{space|10}}<math>\\int_a^\\infty dx \\ \\left(\\int_c^\\infty dy\\ |f(x,\\ y)| \\right )</math>}} converges, the iterated integrals\n{{Center|<math>\\int_c^\\infty dy \\ \\left(\\int_a^{\\infty}dx\\ f(x,\\ y) \\right )</math>{{space|10}} and {{space|10}}<math>\\int_a^\\infty dx \\ \\left(\\int_c^\\infty dy\\ f(x,\\ y) \\right )</math>}} also converge and their values are equal.\n}}\n\nThe most important theorem for the applications is quoted from Protter and Morrey:<ref name=Protter>{{cite book |title=Intermediate Calculus |author=[[Murray H. Protter]] & [[Charles B. Morrey, Jr.]] |page=307 |url=https://books.google.com/books?id=AXw4a2_vzt4C&pg=PA307 |isbn=0-387-96058-9 |publisher=Springer |year=1985}}</ref>\n\n{{math_theorem \n|name=\n|math_statement=Suppose ''F'' is a region given by <math>F=\\left\\{(x,\\ y):a \\le x \\le b, p(x) \\le y \\le q(x) \\right\\} \\,</math>&emsp; where ''p'' and ''q'' are continuous and ''p''(''x'') ≤ ''q''(''x'') for ''a ≤ x ≤ b''. Suppose that ''f''(''x'',&nbsp;''y'') is continuous on ''F''. Then\n{{Center|<math> \\iint_F f(x,y) dA = \\int_a^b\\ \\int_{p(x)}^{q(x)} f(x,\\ y)\\,dy\\ dx \\ .</math>}} The corresponding result holds if the closed region ''F'' has the representation <math>F=\\left\\{(x,\\ y):c\\le y \\le d,\\ r(y) \\le x \\le s(y)\\right\\}</math>&emsp; where ''r''(''y'')&nbsp;≤&nbsp;''s''(''y'') for ''c ≤ y ≤ d''.&emsp;  In such a case,\n\n: <math> \\iint_F f(x,\\ y) dA = \\int_c^d \\ \\int_{r(y)}^{s(y)} f(x,\\ y)\\, dx\\ dy \\ . </math>\n\nIn other words, both iterated integrals, when computable, are equal to the double integral and therefore equal to each other.\n}}\n\n==See also==\n* [[Fubini's theorem]]\n\n==References and notes==\n{{reflist|2}}\n\n==External links==\n*[http://tutorial.math.lamar.edu/Classes/CalcIII/DIGeneralRegion.aspx Paul's Online Math Notes: Calculus III]\n*[https://math.oregonstate.edu/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/255doub/255doub.html Good 3D images showing the computation of \"Double Integrals\" using iterated integrals], the Department of Mathematics at Oregon State University.\n*[http://www.math.ucla.edu/~ronmiech/Calculus_Problems/32B/chap13/section3/ Ron Miech's UCLA Calculus Problems] More complex  examples of changing the order of integration  (see Problems 33, 35, 37, 39, 41  & 43)\n*[http://www.math.umn.edu/~nykamp/m2374/readings/doubleintchange/ Duane Nykamp's University of Minnesota website]\n\n[[Category:Integral calculus]]"
    },
    {
      "title": "Product measure",
      "url": "https://en.wikipedia.org/wiki/Product_measure",
      "text": "In [[mathematics]], given two [[measurable space]]s and [[measure (mathematics)|measures]] on them, one can obtain a '''product measurable space''' and a '''product measure''' on that space. Conceptually, this is similar to defining the [[Cartesian product]] of [[Set (mathematics)|sets]] and the [[product topology]] of two topological spaces, except that there can be many natural choices for the product measure.\n\nLet  <math>(X_1, \\Sigma_1)</math> and  <math>(X_2, \\Sigma_2)</math> be two [[measurable space]]s, that is, <math>\\Sigma_1</math> and <math>\\Sigma_2</math> are [[sigma algebra]]s on <math>X_1</math> and <math>X_2</math> respectively, and let <math>\\mu_1</math> and <math>\\mu_2</math> be measures on these spaces. Denote by  <math>\\Sigma_1 \\otimes \\Sigma_2</math> the sigma algebra on the [[Cartesian product]]  <math>X_1 \\times X_2</math> generated by [[subset]]s of the form  <math>B_1 \\times B_2</math>, where  <math>B_1 \\in \\Sigma_1</math> and  <math>B_2 \\in \\Sigma_2.</math> This sigma algebra is called the ''tensor-product σ-algebra'' on the product space.\n\nA ''product measure''  <math>\\mu_1 \\times \\mu_2</math> is defined to be a measure on the measurable space  <math>(X_1 \\times X_2, \\Sigma_1 \\otimes \\Sigma_2)</math> satisfying the property\n\n:<math> (\\mu_1 \\times \\mu_2)(B_1 \\times B_2) = \\mu_1(B_1) \\mu_2(B_2)</math>\n\nfor all\n\n:<math> B_1 \\in \\Sigma_1,\\ B_2 \\in \\Sigma_2 </math>.\n\n(In multiplying measures, some of which are infinite, we define the product to be zero if any factor is zero.)\n\nIn fact, when the spaces are <math>\\sigma</math>-finite, the product measure is uniquely defined, and for every measurable set ''E'',\n\n:<math>(\\mu_1 \\times \\mu_2)(E) = \\int_{X_2} \\mu_1(E^y)\\,d\\mu_2(y) = \\int_{X_1} \\mu_2(E_{x})\\,d\\mu_1(x),</math>\n\nwhere <math>E_x = \\{y \\in X_2 | (x, y) \\in E\\}</math> and <math>E^y = \\{x \\in X_1 | (x, y) \\in E\\}</math>, which are both measurable sets.\n\nThe existence of this measure is guaranteed by the [[Hahn–Kolmogorov theorem]]. The uniqueness of product measure is guaranteed only in the case that both <math>(X_1, \\Sigma_1, \\mu_1)</math> and <math>(X_2, \\Sigma_2, \\mu_2)</math> are [[σ-finite]].\n\nThe [[Borel measure]] on the [[Euclidean space]] '''R'''<sup>''n''</sup> can be obtained as the product of ''n'' copies of the Borel measure on the [[real line]] '''R'''.\n\nEven if the two factors of the product space are [[complete measure|complete measure spaces]], the product space may not be. Consequently, the completion procedure is needed to extend the Borel measure into the [[Lebesgue measure]], or to extend the product of two Lebesgue measures to give the Lebesgue measure on the product space.\n\nThe opposite construction to the formation of the product of two measures is [[disintegration theorem|disintegration]], which in some sense \"splits\" a given measure into a family of measures that can be integrated to give the original measure.\n\n==Examples==\n\n*Given two measure spaces, there is always a unique maximal product measure μ<sub>max</sub> on their product, with the property that if μ<sub>max</sub>(''A'') is finite for some measurable set ''A'', then μ<sub>max</sub>(''A'') = μ(''A'') for any product measure μ. In particular its value on any measurable set is at least that of any other product measure. This is the measure produced by the [[Carathéodory extension theorem]].\n*Sometimes there is also a unique minimal product measure μ<sub>min</sub>, given by μ<sub>min</sub>(''S'') = sup<sub>''A''&sub;''S'', μ<sub>max</sub>(''A'') finite</sub> μ<sub>max</sub>(''A''), where ''A'' and ''S'' are assumed to be measurable.\n*Here is an example where a product has more than one product measure. Take the product ''X''&times;''Y'', where ''X'' is the unit interval with Lebesgue measure, and ''Y'' is the unit interval with counting measure and all sets measurable. Then for the minimal product measure the measure of a set is the sum of the measures of its horizontal sections, while for the maximal product measure a set has measure infinity unless it is contained in the union of a countable number of sets of the form ''A''&times;''B'', where either ''A'' has Lebesgue measure 0 or ''B'' is a single point. (In this case the measure may be finite or infinite.) In particular, the diagonal has measure 0 for the minimal product measure and measure infinity for the maximal product measure.\n\n==See also==\n* [[Fubini's theorem]]\n\n== References ==\n* {{cite book|last=Loève|first=Michel|authorlink=Michel Loève|title=Probability Theory vol. I |edition=4th |publisher=Springer |year=1977 |isbn=0-387-90210-4 |chapter=8.2. Product measures and iterated integrals |pages=135&ndash;137 |ref=loe1978}}\n* {{cite book|last=Halmos|first=Paul|authorlink=Paul Halmos|title=Measure theory |publisher=Springer |year=1974 |isbn=0-387-90088-8 |chapter=35. Product measures |pages=143&ndash;145 |ref=loe1978}}\n\n{{PlanetMath attribution|id=952|title=Product measure}}\n\n[[Category:Measures (measure theory)]]\n[[Category:Integral calculus]]"
    },
    {
      "title": "Quadratic integral",
      "url": "https://en.wikipedia.org/wiki/Quadratic_integral",
      "text": "In [[mathematics]], a '''quadratic integral''' is an [[integral]] of the form\n\n:<math>\\int \\frac{dx}{a+bx+cx^2}. </math>\n\nIt can be evaluated by [[completing the square]] in the [[denominator]].\n\n:<math>\\int \\frac{dx}{a+bx+cx^2} = \\frac{1}{c} \\int  \\frac{dx}{\\left( x+ \\frac{b}{2c} \\right)^2 + \\left( \\frac{a}{c} - \\frac{b^2}{4c^2} \\right)}. </math>\n\n==Positive-discriminant case==\n\nAssume that the [[discriminant]] ''q'' = ''b''<sup>2</sup>&nbsp;&minus;&nbsp;4''ac'' is positive. In that case, define ''u'' and ''A'' by\n\n:<math>u = x + \\frac{b}{2c} </math>,\n\nand\n\n:<math> -A^2 = \\frac{a}{c} - \\frac{b^2}{4c^2} = \\frac{1}{4c^2} \\left( 4ac - b^2 \\right). </math>\n\nThe quadratic integral can now be written as\n\n:<math> \\int \\frac{dx}{a+bx+cx^2} = \\frac1c \\int \\frac{du}{u^2-A^2} = \\frac1c \\int \\frac{du}{(u+A)(u-A)}. </math>\n\nThe [[partial fraction decomposition]]\n\n:<math> \\frac{1}{(u+A)(u-A)} = \\frac{1}{2A} \\left( \\frac{1}{u-A} - \\frac{1}{u+A} \\right) </math>\n\nallows us to evaluate the integral:\n\n:<math> \\frac1c \\int \\frac{du}{(u+A)(u-A)} = \\frac{1}{2Ac} \\ln \\left( \\frac{u - A}{u + A} \\right) + \\text{constant}. </math>\n\nThe final result for the original integral, under the assumption that ''q'' > 0, is\n\n:<math> \\int \\frac{dx}{a+bx+cx^2} = \\frac{1}{ \\sqrt{q}} \\ln \\left( \\frac{2cx + b - \\sqrt{q}}{2cx+b+ \\sqrt{q}} \\right) + \\text{constant, where } q = b^2 - 4ac. </math>\n\n==Negative-discriminant case==\n\n:''This (hastily written) section may need attention.''\n\nIn case the [[discriminant]] ''q'' = ''b''<sup>2</sup>&nbsp;&minus;&nbsp;4''ac'' is negative, the second term in the denominator in\n\n:<math>\\int \\frac{dx}{a+bx+cx^2} = \\frac{1}{c} \\int  \\frac{dx}{\\left( x+ \\frac{b}{2c} \\right)^2 + \\left( \\frac{a}{c} - \\frac{b^2}{4c^2} \\right)}. </math>\n\nis positive.  Then the integral becomes\n\n:<math>\n\\begin{align}\n& {} \\qquad \\frac{1}{c} \\int \\frac{ du} {u^2 + A^2} \\\\[9pt]\n& = \\frac{1}{cA} \\int \\frac{du/A}{(u/A)^2 + 1 } \\\\[9pt]\n& = \\frac{1}{cA} \\int \\frac{dw}{w^2 + 1} \\\\[9pt]\n& = \\frac{1}{cA} \\arctan(w) + \\mathrm{constant} \\\\[9pt]\n& = \\frac{1}{cA} \\arctan\\left(\\frac{u}{A}\\right) + \\text{constant} \\\\[9pt]\n& = \\frac{1}{c\\sqrt{\\frac{a}{c} - \\frac{b^2}{4c^2}}} \\arctan\n\\left(\\frac{x + \\frac{b}{2c}}{\\sqrt{\\frac{a}{c} - \\frac{b^2}{4c^2}}}\\right) + \\text{constant} \\\\[9pt]\n& = \\frac{2}{\\sqrt{4ac - b^2\\, }}\n\\arctan\\left(\\frac{2cx + b}{\\sqrt{4ac - b^2}}\\right) + \\text{constant}.\n\\end{align}\n</math>\n\n==References==\n*Weisstein, Eric W. \"[http://mathworld.wolfram.com/QuadraticIntegral.html Quadratic Integral].\" From ''MathWorld''--A Wolfram Web Resource, wherein the following is referenced:\n*{{cite book |author-first1=Izrail Solomonovich |author-last1=Gradshteyn |author-link1=Izrail Solomonovich Gradshteyn |author-first2=Iosif Moiseevich |author-last2=Ryzhik |author-link2=Iosif Moiseevich Ryzhik |author-first3=Yuri Veniaminovich |author-last3=Geronimus |author-link3=Yuri Veniaminovich Geronimus |author-first4=Michail Yulyevich |author-last4=Tseytlin |author-link4=Michail Yulyevich Tseytlin |author-first5=Alan |author-last5=Jeffrey |editor-first1=Daniel |editor-last1=Zwillinger |editor-first2=Victor Hugo |editor-last2=Moll |translator=Scripta Technica, Inc. |title=Table of Integrals, Series, and Products |publisher=[[Academic Press, Inc.]] |date=2015 |orig-year=October 2014 |edition=8 |language=English |isbn=0-12-384933-0 |id={{isbn|978-0-12-384933-5}} |lccn=2014010276 <!-- |url=http://books.google.com/books?id=NjnLAwAAQBAJ |access-date=2016-02-21-->|title-link=Gradshteyn and Ryzhik}}\n\n[[Category:Integral calculus]]"
    },
    {
      "title": "Quadrature (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Quadrature_%28mathematics%29",
      "text": "{{Other uses|Quadrature (disambiguation)}}\n\nIn [[mathematics]], '''''quadrature''''' is a historical term which means the process of determining [[area]]. This term is still used nowadays in the context of [[differential equation]]s, where \"solving an equation by quadrature\" means expressing its solution in terms of [[antiderivative|integrals]].\n\nQuadrature problems served as one of the main sources of problems in the development of [[calculus]], and introduce important topics in [[mathematical analysis]].\n\n== History ==\n[[File:Geometric mean.svg|thumb|left|220px|<center>Antique method to find the [[geometric mean]]</center>]]\n\n[[Greek mathematics|Mathematicians of ancient Greece]], according to the [[Pythagoreanism|Pythagorean]] doctrine, understood determination of [[area]] of a figure as the process of geometrically constructing a [[square (geometry)|square]] having the same area  (''squaring''), thus the name ''quadrature'' for this process. The Greek geometers were not always successful (see [[quadrature of the circle]]), but they did carry out quadratures of some figures whose sides were not simply line segments, such as the [[lune of Hippocrates|lunes of Hippocrates]] and  [[The Quadrature of the Parabola|the quadrature of the parabola]]. By Greek tradition, these constructions had to be performed using only a [[Compass and straightedge constructions|compass and straightedge]].\n\nFor a quadrature of a [[rectangle]] with the sides ''a'' and ''b'' it is necessary to construct a square with the side <math>x =\\sqrt {ab}</math> (the [[geometric mean]] of ''a'' and ''b''). For this purpose it is possible to use the following: if one draws the circle with diameter made from joining line segments of lengths ''a'' and ''b'', then the height (''BH'' in the diagram) of the line segment drawn perpendicular to the diameter, from the point of their connection to the point where it crosses the circle, equals the geometric mean of ''a'' and ''b''. A similar geometrical construction solves the problems of quadrature of a parallelogram and of a triangle.\n\n[[File:Parabola and inscribed triangle.svg|thumb|200px|<center>The area of a segment of a parabola is 4/3 that of the area of a certain inscribed triangle.</center>]]\nProblems of quadrature for [[curvilinear]] figures are much more difficult. The quadrature of the circle with compass and straightedge was proved in the 19th century to be impossible. Nevertheless, for some figures (for example a lune of Hippocrates) a quadrature can be performed. The quadratures of the surface of a sphere and a [[parabola]] segment discovered by [[Archimedes]] became the highest achievement of analysis in antiquity.\n:* The area of the surface of a sphere is equal to four times the area of the circle formed by a [[great circle]] of this sphere.\n:* The area of a segment of a parabola determined by a straight line cutting it is 4/3 the area of a triangle inscribed in this segment.\nFor the proof of these results, Archimedes used the [[method of exhaustion]]<ref name=\"Katz\">{{cite book|last=Katz|first=Victor J.|title=A History of Mathematics: An Introduction|edition= 2nd|publisher=Addison Wesley Longman|year=1998|isbn=0321016181}}</ref>{{rp|113}} of [[Eudoxus of Cnidus|Eudoxus]].\n\nIn medieval Europe, quadrature meant the calculation of area by any method. Most often the [[method of indivisibles]] was used; it was less rigorous than the geometric constructions of the Greeks, but it was simpler and more powerful. With its help, [[Galileo Galilei]] and [[Gilles de Roberval]] found the area of a [[cycloid]] arch, [[Grégoire de Saint-Vincent]] investigated the area under a [[hyperbola]] (''Opus Geometricum'', 1647),<ref name=\"Katz\"/>{{rp|491}} and [[Alphonse Antonio de Sarasa]], de Saint-Vincent's pupil and commentator, noted the relation of this area to [[logarithm]]s.<ref name=Katz/>{{rp|492}}<ref>Enrique A. Gonzales-Velasco (2011) ''Journey through Mathematics'', § 2.4 Hyperbolic Logarithms, page 117</ref>\n\n[[John Wallis]] algebrised this method; he wrote in his ''Arithmetica Infinitorum'' (1656) some series which are equivalent to what is now called the [[definite integral]], and he calculated their values. [[Isaac Barrow]] and [[James Gregory (mathematician)|James Gregory]] made further progress: quadratures for some [[algebraic curves]] and [[spiral]]s. [[Christiaan Huygens]] successfully performed a quadrature of the surface area of some [[Solid of revolution|solids of revolution]].\n\nThe quadrature of the hyperbola by Saint-Vincent and de Sarasa provided a new [[function(mathematics)|function]], the [[natural logarithm]], of critical importance. With the invention of [[integral calculus]] came a universal method for area calculation. In response, the term ''quadrature'' has become traditional, and instead the modern phrase ''finding the area'' is more commonly used for what is technically the ''computation of a univariate definite integral''.\n\n== See also ==\n* [[Gaussian quadrature]]\n* [[Hyperbolic angle]]\n* [[Numerical integration]]\n* [[Quadrance]]\n* [[Quadratrix]]\n* [[Tanh-sinh quadrature]]\n\n==Notes==\n{{reflist}}\n\n== References ==\n* Boyer, C. B. (1989) ''A History of Mathematics'', 2nd ed. rev. by [[Uta Merzbach|Uta C. Merzbach]]. New York: Wiley, {{ISBN|0-471-09763-2}} (1991 pbk ed. {{ISBN|0-471-54397-7}}).\n* Eves, Howard (1990) ''An Introduction to the History of Mathematics'', Saunders, {{ISBN|0-03-029558-0}},\n* [[Christiaan Huygens]] (1651) ''Theoremata de Quadratura Hyperboles, Ellipsis et Circuli''\n* [[Jean-Etienne Montucla]] (1873) [http://babel.hathitrust.org/cgi/pt?id=nyp.33433069092124;view=1up;seq=19 History of the Quadrature of the Circle], J. Babin translator, William Alexander Myers editor, link from [[HathiTrust]].\n* [[Christoph Scriba]] (1983) \"Gregory's Converging Double Sequence: a new look at the controversy between Huygens and Gregory over the 'analytical' quadrature of the circle\", [[Historia Mathematica]] 10:274–85.\n\n[[Category:Integral calculus]]\n[[Category:History of mathematics]]\n[[Category:History of geometry]]\n[[Category:Mathematical terminology]]"
    },
    {
      "title": "Radius of curvature",
      "url": "https://en.wikipedia.org/wiki/Radius_of_curvature",
      "text": "{{about|the general mathematical concept|its optical applications|Radius of curvature (optics)}}\n[[File:Radius of curvature.svg|thumb|400px|Radius of curvature and [[center of curvature]]]]\n\nIn [[differential geometry]], the '''radius of curvature''', {{mvar|R}}, is the reciprocal of the [[curvature]]. For a [[curve]], it equals the [[radius]] of the [[circular arc]] which best approximates the curve at that point. For [[surface (mathematics)|surface]]s, the radius of curvature is the radius of a circle that best fits a [[normal section]] or [[Earth radius#Combinations|combinations]] thereof.<ref>{{Cite web|url=http://mathworld.wolfram.com/RadiusofCurvature.html|title=Radius of Curvature|last=Weisstien|first=Eric|date=|website=Wolfram Mathworld|publisher=|access-date=15 August 2016}}</ref><ref name=\":0\">{{Cite book|url=https://books.google.com.ph/books?redir_esc=y&id=90mk7qPAvb4C&q=radius+of+curvature#v=snippet&q=page%20210&f=false|title=Differential Calculus|last=Kishan|first=Hari|date=2007|publisher=Atlantic Publishers & Dist|isbn=9788126908202|language=en}}</ref><ref name=\":1\">{{Cite book |title=Differential and Integral Calculus |edition=Sixth |first=Clyde E. |last=Love |authorlink=Clyde E. Love |first2=Earl D. |last2=Rainville |authorlink2=Earl D. Rainville |date=1962 |publisher=MacMillan |location=New York |language=en |isbn= }}</ref>\n\n==Definition==\nIn the case of a [[space curve]], the radius of curvature is the length of the [[curvature vector]].\n\nIn the case of a [[plane curve]], then {{mvar|R}} is the [[absolute value]] of<ref name=\":1\" />\n\n: <math>R \\equiv \\left|\\frac{ds}{d\\varphi} \\right| = \\frac{1}{\\kappa},</math>\n\nwhere {{mvar|s}} is the arc length from a fixed point on the curve, {{mvar|φ}} is the [[tangential angle]] and {{mvar|κ}} is the [[curvature#Curvature|curvature]].\n\nIf the curve is given in [[Cartesian coordinates]] as {{math|''y''(''x'')}}, then the radius of curvature is (assuming the curve is differentiable up to order 2):\n\n: <math>R =\\left| \\frac { \\left(1 + y'^{\\,2}\\right)^\\frac32}{y''}\\right|, \\qquad\\mbox{where}\\quad y' = \\frac{dy}{dx},\\quad y'' = \\frac{d^2y}{dx^2},</math> \nand {{math|{{abs|''z''}}}} denotes the absolute value of {{mvar|z}}.\n\nIf the curve is given [[parametric equation|parametrically]] by functions {{math|''x''(''t'')}} and {{math|''y''(''t'')}}, then the radius of curvature is\n:<math>R = \\left|\\frac{ds}{d\\varphi}\\right| = \\left|\\frac {\\left({\\dot{x}^2 + \\dot{y}^2}\\right)^\\frac32}{\\dot {x}\\ddot{y} - \\dot{y}\\ddot{x}}\\right|, \\qquad\\mbox{where}\\quad \\dot{x} = \\frac{dx}{dt},\\quad\\ddot{x} = \\frac{d^2x}{dt^2},\\quad \\dot{y} = \\frac{dy}{dt},\\quad\\ddot{y} = \\frac{d^2y}{dt^2}.</math>\n\nHeuristically, this result can be interpreted as<ref name=\":0\" />\n\n:<math> R = \\frac{\\left|\\mathbf{v}\\right|^3}{\\left| \\mathbf{v} \\times \\mathbf{ \\dot v} \\right|}, \\qquad\\mbox{where}\\quad \\left| \\mathbf{v} \\right| = \\big| (\\dot x, \\dot y) \\big| = R \\frac{d\\varphi}{dt}.</math>\n\n==Formula==\n\nIf {{math| '''γ''' : ℝ → ℝ<sup>''n''</sup>}} is a parametrized curve in {{math|ℝ<sup>''n''</sup>}} then the radius of curvature at each point of the curve, {{math|''ρ'' : ℝ → ℝ}}, is given by<ref name=\":1\" />\n\n:<math>\\rho = \\frac{\\left|\\boldsymbol\\gamma'\\right|^3}{\\sqrt{\\left|\\boldsymbol\\gamma'\\right|^2 \\, \\left|\\boldsymbol\\gamma''\\right|^2 - \\left(\\boldsymbol\\gamma' \\cdot \\boldsymbol\\gamma''\\right)^2}}</math>.\n\nAs a special case, if {{math|''f''(''t'')}} is a function from {{math|ℝ}} to {{math|ℝ}}, then the radius of curvature of its [[graph of a function|graph]], {{math|'''γ'''(''t'') {{=}} (''t'', ''f''(''t''))}}, is\n\n:<math>\\rho(t)=\\frac{\\left|1+f'^{\\,2}(t)\\right|^\\frac32}{\\left|f''(t)\\right|}.</math>\n\n===Derivation===\n\nLet {{math|'''γ'''}} be as above, and fix {{mvar|t}}. We want to find the radius {{mvar|ρ}} of a parametrized circle which matches {{math|γ}} in its zeroth, first, and second derivatives at {{mvar|t}}. Clearly the radius will not depend on the position {{math|'''γ'''(''t'')}}, only on the velocity {{math|'''γ'''′(''t'')}} and acceleration {{math|'''γ'''″(''t'')}}. There are only three independent [[scalar product|scalars]] that can be obtained from two vectors {{math|'''v'''}} and {{math|'''w'''}}, namely {{math|'''v''' · '''v'''}}, {{math|'''v''' · '''w'''}}, and {{math|'''w''' · '''w'''}}. Thus the radius of curvature must be a function of the three scalars {{math|{{abs|'''γ'''′(''t'')}}{{isup|2}}}}, {{math|{{abs|'''γ'''″(''t'')}}{{isup|2}}}} and {{math|'''γ'''′(''t'') · '''γ'''″(''t'')}}.<ref name=\":1\" />\n\nThe general equation for a parametrized circle in {{math|ℝ<sup>''n''</sup>}} is\n:<math>\\mathbf{g}(u) = \\mathbf a \\cos h(u) + \\mathbf b \\sin h(u) + \\mathbf c</math>\nwhere {{math|'''c''' ∈ ℝ<sup>''n''</sup>}} is the center of the circle (irrelevant since it disappears in the derivatives), {{math|'''a''','''b''' ∈ ℝ<sup>''n''</sup>}} are perpendicular vectors of length {{mvar|ρ}} (that is, {{math|'''a''' · '''a''' {{=}} '''b''' · '''b''' {{=}} ''ρ''{{isup|2}}<sup></sup>}} and {{math|'''a''' · '''b''' {{=}} 0}}), and {{math|''h'' : ℝ → ℝ}} is an arbitrary function which is twice differentiable at {{mvar|t}}.\n\nThe relevant derivatives of {{math|'''g'''}} work out to be\n:<math>\\begin{align}\n|\\mathbf g'|^2 &= \\rho^2 (h')^2 \\\\\n\\mathbf g' \\cdot \\mathbf g'' &= \\rho^2 h' h'' \\\\\n|\\mathbf g''|^2 &= \\rho^2 \\left((h')^4 + (h'')^2 \\right)\n\\end{align}</math>\n\nIf we now equate these derivatives of {{math|'''g'''}} to the corresponding derivatives of {{math|'''γ'''}} at {{mvar|t}} we obtain\n\n:<math>\\begin{align}\n|\\boldsymbol\\gamma'(t)|^{2} &= \\rho^2 h'^{\\,2}(t) \\\\\n\\boldsymbol\\gamma'(t) \\cdot \\boldsymbol\\gamma''(t) &= \\rho^2 h'(t) h''(t) \\\\\n|\\boldsymbol\\gamma''(t)|^{2} &= \\rho^2 \\left(h'^{\\,4}(t) + h''^{\\,2}(t)\\right)\n\\end{align}</math>\n\nThese three equations in three unknowns ({{mvar|ρ}}, {{math|''h''′(''t'')}} and {{math|''h''″(''t'')}}) can be solved for {{mvar|ρ}}, giving the formula for the radius of curvature:\n\n:<math>\\rho(t) = \\frac{\\left|\\boldsymbol\\gamma'(t)\\right|^{3}}{\\sqrt{\\left|\\boldsymbol\\gamma'(t)\\right|^{2} \\, \\left|\\boldsymbol\\gamma''(t)\\right|^{2} - \\big(\\boldsymbol\\gamma'(t) \\cdot \\boldsymbol\\gamma''(t)\\big)^2}}</math>\n\nor, omitting the parameter {{mvar|t}} for readability,\n\n:<math>\\rho = \\frac{\\left|\\boldsymbol\\gamma'\\right|^3}{\\sqrt{\\left|\\boldsymbol\\gamma'\\right|^2 \\; \\left|\\boldsymbol\\gamma''\\right|^2 - \\left(\\boldsymbol\\gamma' \\cdot \\boldsymbol\\gamma''\\right)^2}}.</math>\n\n==Examples==\n\n===Semicircles and circles===\n\nFor a [[semi-circle]] of radius {{mvar|a}} in the upper half-plane\n:<math>y=\\sqrt{a^2-x^2}, \\quad y'=\\frac{-x}{\\sqrt{a^2-x^2}}, \\quad y''=\\frac{-a^2}{\\left(a^2-x^2\\right)^\\frac32},\\quad R=|-a| =a.</math>\n\n[[Image:Ellipse evolute.svg|right|thumb|240px|An ellipse (red) and its [[evolute]] (blue). The dots are the vertices of the ellipse, at the points of greatest and least curvature.]]\n\nFor a semi-circle  of radius {{mvar|a}} in the lower half-plane\n:<math> y=-\\sqrt{a^2-x^2}, \\quad R=|a|=a.</math>\n\nThe [[circle]] of radius {{mvar|a}} has a radius of curvature equal to {{mvar|a}}.\n\n===Ellipses===\n\nIn an [[ellipse]] with major axis {{math|2''a''}} and minor axis {{math|2''b''}}, the [[Vertex (curve)|vertices]] on the major axis have the smallest radius of curvature of any points, {{math|''R'' {{=}} {{sfrac|''b''<sup>2</sup>|''a''}}}}; and the vertices on the minor axis have the largest radius of curvature of any points, {{math|''R'' {{=}} {{sfrac|''a''<sup>2</sup>|''b''}}}}.\n\n==Applications==\n*For the use in [[differential geometry]], see [[Cesàro equation]].\n*For the radius of curvature of the earth (approximated by an oblate ellipsoid), see [[Earth radius#Radii of curvature|Radius of curvature of the earth]].\n*Radius of curvature is also used in a three part equation for bending of [[Beam (structure)|beams]].\n*[[Radius of curvature (optics)]]\n\n===Stress in semiconductor structures===\nStress in the semiconductor structure involving evaporated thin films usually results from the thermal expansion (thermal stress) during the manufacturing process.  Thermal stress occurs because film depositions are usually made above room temperature. Upon cooling from the deposition temperature to room temperature, the difference in the thermal expansion coefficients of the substrate and the film cause thermal stress.<ref>{{cite web|url=http://flipchips.com/tutorial/process/controlling-stress-in-thin-films/ |title=Controlling Stress in Thin Films |website=Flipchips.com |date= |accessdate=2016-04-22}}</ref>\n\nIntrinsic stress results from the microstructure created in the film as atoms are deposited on the substrate. Tensile stress results from microvoids in the thin film, because of the attractive interaction of atoms across the voids.\n\nThe stress in thin film semiconductor structures results in the buckling of the wafers. The radius of the curvature of the stressed structure is related to stress tensor in the structure, and can be described by modified Stoney formula.<ref>{{cite web|url=http://www.qucosa.de/fileadmin/data/qucosa/documents/5126/data/Stoney.pdf |format=PDF |title=On the determination of film stress from substrate bending : Stoney's formula and its limits |website=Qucosa.de |accessdate=2016-04-22}}</ref> The topography of the stressed structure including radii of curvature can be measured using optical scanner methods. The modern scanner tools have capability to measure full topography of the substrate and to measure both principal radii of curvature, while providing the accuracy of the order of 0.1% for radii of curvature of 90 meters and more.<ref>{{cite web|author=Peter Walecki |url=http://www.zebraoptical.com/ModelX.html |title=Model X |publisher=Zebraoptical.com |date= |accessdate=2016-04-22}}</ref>\n\n==See also==\n{{div col|colwidth=30em}}\n*[[AFM probe]]\n*[[Base curve radius]]\n*[[Bend radius]]\n*[[Curve]]\n*[[Curvature]]\n*[[Degree of curvature]] (civil engineering)\n*[[Diameter]]\n*[[Minimum railway curve radius]]\n*[[Osculating circle]]\n*[[Reverse curve]]\n*[[Track transition curve]]\n*[[Transition curve]]\n{{div col end}}\n\n==References==\n{{Reflist}}\n\n==Further reading==\n* {{cite book |title = Differential Geometry of Curves and Surfaces|first = Manfredo|last = do Carmo|authorlink=Manfredo do Carmo | isbn = 0-13-212589-7 | year = 1976}}\n\n==External links==\n* [http://www.geom.uiuc.edu/zoo/diffgeom/surfspace/concepts/curvatures/prin-curv.html The Geometry Center: Principal Curvatures]\n* [http://math.mit.edu/classes/18.013A/HTML/chapter15/section03.html 15.3 Curvature and Radius of Curvature]\n* {{MathWorld |title=Principal Curvatures |urlname=PrincipalCurvatures }}\n* {{MathWorld |title=Principal Radius of Curvature |urlname=PrincipalRadiusofCurvature }}\n\n{{curvature}}\n\n[[Category:Differential geometry]]\n[[Category:Curvature (mathematics)]]\n[[Category:Curves]]\n[[Category:Integral calculus]]\n[[Category:Multivariable calculus]]\n[[Category:Theoretical physics]]"
    },
    {
      "title": "Riemann sum",
      "url": "https://en.wikipedia.org/wiki/Riemann_sum",
      "text": "[[File:Riemann sum convergence.png|right|thumb|300px|Four of the Riemann summation [[#Methods|methods]] for approximating the area under curves. <span style=\"color:#0081cd\">'''Right''' </span> and <span style=\"color:#fec200\">'''left'''</span> methods make the approximation using the right and left endpoints of each subinterval, respectively. <span style=\"color:#009246\">'''Maximum'''</span> and <span style=\"color:#bc1e47\">'''minimum'''</span> methods make the approximation using the largest and smallest endpoint values of each subinterval, respectively. The values of the sums converge as the subintervals halve from top-left to bottom-right.]]\n\nIn [[mathematics]], a '''Riemann sum''' is a certain kind of [[approximation]] of an integral by a finite sum. It is named after nineteenth century German mathematician [[Bernhard Riemann]]. One very common application is approximating the area of functions or lines on a graph, but also the length of curves and other approximations. \n\nThe sum is calculated by [[Partition of an interval|partitioning]] the region into shapes ([[rectangle]]s, [[trapezoid]]s, [[parabola]]s, or [[cubic function|cubics]]) that together form a region that is similar to the region being measured, then calculating the area for each of these shapes, and finally adding all of these small areas together. This approach can be used to find a numerical approximation for a [[definite integral]] even if the [[fundamental theorem of calculus]] does not make it easy to find a [[Closed-form_expression|closed-form solution]].\n\nBecause the region filled by the small shapes is usually not exactly the same shape as the region being measured, the Riemann sum will differ from the area being measured. This error can be reduced by dividing up the region more finely, using smaller and smaller shapes. As the shapes get smaller and smaller, the sum approaches the [[Riemann integral]].\n\n==Definition==\nLet <math>f: [a,b] \\rightarrow \\mathbb R</math> be a function defined on a [[Interval (mathematics)#Terminology|closed interval]] <math>[a,b]</math> of the real numbers, <math>\\mathbb R</math>, and\n:<math>P = \\left \\{[x_0,x_1],[x_1,x_2],\\dots,[x_{n-1},x_n] \\right \\}</math>&#8288;,\nbe [[Partition of an interval|a partition of ''I'']], where\n:<math>a=x_0<x_1<x_2<\\cdots<x_n=b</math>.\nA '''Riemann sum''' <math>S</math> of ''f'' over ''I'' with partition ''P'' is defined as\n:<math>S = \\sum_{i=1}^{n} f(x_i^*)\\, \\Delta x_i</math>\nwhere <math>\\Delta x_i=x_i-x_{i-1}</math> and an <math>x_i^*\\in[x_{i-1},x_i]</math>.<ref>{{cite book |last1=Hughes-Hallett |first1=Deborah |last2=McCullum |first2=William G. |displayauthors=etal |year=2005 |title=Calculus |edition=4th |publisher=Wiley |page=252}} (Among many equivalent variations on the definition, this reference closely resembles the one given here.)</ref>\nNotice the use of \"an\" instead of \"the\" in the previous sentence. Another way of thinking about this asterisk is that you are choosing some random point in this slice, and it does not matter which one; as the difference or width of the slices approaches zero, the difference between any two points in our rectangle slice approaches zero as well. This is due to the fact that the choice of <math>x_i^*</math> in the interval <math>[x_{i-1},x_i]</math> is arbitrary, so for any given function ''f'' defined on an interval ''I'' and a fixed partition ''P'', one might produce different Riemann sums depending on which <math>x_i^*</math> is chosen, as long as <math>x_{i-1}\\le x_i^* \\le x_i</math> holds true.\n\n==Some specific types of Riemann sums==\nSpecific choices of <math>x_i^*</math> give us different types of Riemann sums:\n* If <math>x_i^*=x_{i-1}</math> for all ''i'', then ''S'' is called a '''left rule'''<ref name=rulenames>{{cite book |last1=Hughes-Hallett |first1=Deborah |last2=McCullum |first2=William G. |displayauthors=etal |year=2005 |title=Calculus |edition=4th |publisher=Wiley |page=340 |quote=So far, we have three ways of estimating an integral using a Riemann sum: 1. The '''left rule''' uses the left endpoint of each subinterval. 2. The '''right rule''' uses the right endpoint of each subinterval. 3. The '''midpoint rule''' uses the midpoint of each subinterval.}}</ref><ref name=rulenames2>{{cite book |last1=Ostebee |first1=Arnold |last2=Zorn |first2=Paul |year=2002 |title=Calculus from Graphical, Numerical, and Symbolic Points of View |edition=Second |page=M-33 |quote=Left-rule, right-rule, and midpoint-rule approximating sums all fit this definition.}}</ref> or '''left Riemann sum'''.\n* If <math>x_i^*=x_i</math> for all ''i'', then ''S'' is called a '''right rule'''<ref name=rulenames/><ref name=rulenames2/> or '''right Riemann sum'''.\n* If <math>x_i^*=(x_i+x_{i-1})/2</math> for all ''i'', then ''S'' is called the '''midpoint rule'''<ref name=rulenames/><ref name=rulenames2/> or '''middle Riemann sum'''.\n* If <math>f(x_i^*) = \\sup f([x_{i-1},x_i])</math> (that is, the [[supremum]] of ''f'' over <math>[x_{i-1},x_i]</math>), then ''S'' is defined to be an '''upper Riemann sum''' or '''upper Darboux sum'''.\n* If <math>f(x_i^*) = \\inf f([x_{i-1},x_i])</math> (that is, the [[infimum]] of ''f'' over <math>[x_{i-1},x_i]</math>), then ''S'' is defined to be an '''lower Riemann sum''' or '''lower Darboux sum'''.\n\nAll these methods are among the most basic ways to accomplish [[numerical integration]]. Loosely speaking, a function is [[Riemann integral|Riemann integrable]] if all Riemann sums converge as the partition \"gets finer and finer\".\n\nWhile not technically a Riemann sum, the average of the left and right Riemann sums is the '''[[trapezoidal rule|trapezoidal sum]]''' and is one of the simplest of a very general way of approximating integrals using weighted averages. This is followed in complexity by [[Simpson's rule]] and [[Newton–Cotes formulas]].\n\nAny Riemann sum on a given partition (that is, for any choice of <math>x_i^*</math> between <math>x_{i-1}</math> and <math>x_i</math>) is contained between the lower and upper Darboux sums. This forms the basis of the [[Darboux integral]], which is ultimately equivalent to the Riemann integral.\n\n==Methods==\nThe four methods of Riemann summation are usually best approached with partitions of equal size. The interval [''a'', ''b''] is therefore divided into ''n'' subintervals, each of length \n\n:<math>\\Delta x = \\frac{b-a}{n}.</math>\n\nThe points in the partition will then be \n\n: <math>a, a + \\Delta x, a + 2 \\,\\Delta x, \\ldots, a + (n-2) \\,\\Delta x, a + (n-1)\\,\\Delta x, b.</math>\n\n===Left Riemann sum===\n[[Image:LeftRiemann2.svg|thumb|right|Left Riemann sum of ''x''<sup>3</sup> over [0,2] using 4 subdivisions]]\nFor the left Riemann sum, approximating the function by its value at the left-end point gives multiple rectangles with base Δ''x'' and height ''f''(''a'' + ''i''Δ''x''). Doing this for ''i'' = 0, 1, ..., ''n''&nbsp;−&nbsp;1, and adding up the resulting areas gives\n\n:<math>\\Delta x \\left[f(a) + f(a + \\Delta x) + f(a + 2 \\,\\Delta x)+\\cdots+f(b - \\Delta x)\\right].</math>\n\nThe left Riemann sum amounts to an overestimation if ''f'' is [[monotonically decreasing]] on this interval, and an underestimation if it is [[monotonically increasing]].\n\n===Right Riemann sum===\n[[Image:RightRiemann2.svg|thumb|right|Right Riemann sum of ''x''<sup>3</sup> over [0,2] using 4 subdivisions]]\n''f'' is here approximated by the value at the right endpoint. This gives multiple rectangles with base Δ''x'' and height ''f''(''a'' + ''i''&nbsp;Δ''x''). Doing this for ''i'' = 1, ..., ''n'', and adding up the resulting areas produces \n:<math>\\Delta x \\left[ f( a + \\Delta x ) + f(a + 2 \\, \\Delta x)+\\cdots+f(b) \\right].</math>\n\nThe right Riemann sum amounts to an underestimation if ''f'' is [[monotonically decreasing]], and an overestimation if it is [[monotonically increasing]].\nThe error of this formula will be \n\n:<math>\\left \\vert \\int_{a}^{b} f(x) \\, dx - A_\\mathrm{right} \\right \\vert \\le \\frac{M_1 (b-a)^2}{2n}</math>,\n\nwhere <math>M_1</math> is the maximum value of the [[absolute value]] of <math>f^{\\prime}(x)</math> on the interval.\n\n===Midpoint rule===\n[[Image:MidRiemann2.svg|thumb|right|Midpoint Riemann sum of ''x''<sup>3</sup> over [0,2] using 4 subdivisions]]\nApproximating ''f'' at the midpoint of intervals gives ''f''(''a'' + Δ''x''/2) for the first interval, for the next one ''f''(''a'' + 3Δ''x''/2), and so on until ''f''(''b''&nbsp;−&nbsp;Δ''x''/2). Summing up the areas gives\n\n:<math>\\Delta x\\left[f(a + \\tfrac{\\Delta x}{2}) + f(a + \\tfrac{3\\,\\Delta x}{2}) + \\cdots+f(b-\\tfrac{\\Delta x}{2})\\right]</math>.\n\nThe error of this formula will be \n\n:<math>\\left \\vert \\int_a^b f(x) \\, dx - A_\\mathrm{mid} \\right \\vert \\le \\frac{M_2(b-a)^3}{24n^2}</math>,\n\nwhere <math>M_2</math> is the maximum value of the [[absolute value]] of <math>f^{\\prime\\prime}(x)</math> on the interval.\n\n===Trapezoidal rule===\n[[Image:TrapRiemann2.svg|thumb|right|Trapezoidal Riemann sum of ''x''<sup>3</sup> over [0,2] using 4 subdivisions]]\n{{main|Trapezoidal rule}}\nIn this case, the values of the function ''f'' on an interval are approximated by the average of the values at the left and right endpoints. In the same manner as above, a simple calculation using the area formula \n:<math>A=\\tfrac{1}{2}h(b_1+b_2)</math> \nfor a [[trapezoid|trapezium]]  with parallel sides ''b''<sub>1</sub>, ''b''<sub>2</sub>  and height ''h'' produces\n\n:<math>\\tfrac{1}{2}\\,\\Delta x\\left[f(a) + 2f(a+\\Delta x) + 2f(a+2\\,\\Delta x) + 2f(a+3\\,\\Delta x)+\\cdots+f(b)\\right].</math>\n\nThe error of this formula will be \n\n:<math>\\left \\vert \\int_{a}^{b} f(x) \\, dx - A_\\mathrm{trap} \\right \\vert \\le \\frac{M_2(b-a)^3}{12n^2},</math>\n\nwhere <math>M_2</math> is the maximum value of the absolute value of <math>f^{\\prime\\prime}(x)</math>.\n\nThe approximation obtained with the trapezoid rule for a function is the same as the average of the left hand and right hand sums of that function.\n\n==Connection with integration==\nFor a one-dimensional Riemann sum over domain <math>[a,b]</math>, as the maximum size of a partition element shrinks to zero (that is the limit of the norm of the partition goes to zero), some functions will have all Riemann sums converge to the same value. This limiting value, if it exists, is defined as the definite Riemann integral of the function over the domain,\n\n: <math>\\int_a^b \\! f(x) \\, dx = \\lim_{\\|\\Delta x\\|\\rightarrow0} \\sum_{i=1}^{n} f(x_i^*) \\,\\Delta x_i.</math>\n\nFor a finite-sized domain, if the maximum size of a partition element shrinks to zero, this implies the number of partition elements goes to infinity. For finite partitions, Riemann sums are always approximations to the limiting value and this approximation gets better as the partition gets finer. The following animations help demonstrate how increasing the number of partitions (while lowering the maximum partition element size) better approximates the \"area\" under the curve:\n<gallery>\nImage:Riemann sum (leftbox).gif|Left sum\nImage:Riemann sum (rightbox).gif|Right sum\nImage:Riemann sum (middlebox).gif|Middle sum\n</gallery>\nSince the red function here is assumed to be a smooth function, all three Riemann sums will converge to the same value as the number of partitions goes to infinity.\n\n==Example==\n{{multiple image\n | align = right\n | direction = vertical\n | width = 220\n | header = Comparison of right hand sums of the function ''y'' = ''x''<sup>2</sup> from 0&nbsp;to&nbsp;2 with the integral of it from 0&nbsp;to&nbsp;2.\n\n | image1 = Area-under-curve-for-x-squared.svg\n | alt1 = \n | caption1 = A visual representation of the area under the curve ''y'' = ''x''<sup>2</sup> for the interval from 0 to 2. Using antiderivatives this area is exactly 8/3.\n\n | image2 = Riemann sum (y=x^2).gif\n | alt2 = \n | caption2 = Approximating the area under <math>y=x^2</math> from 0 to 2 using right-rule sums. Notice that because the function is monotonically increasing, right-hand sums will always overestimate the area contributed by each term in the sum (and do so maximally).\n\n | image3 = Riemann sum error.svg\n | alt3 = \n | caption3 = The value of the Riemann sum under the curve ''y'' = ''x''<sup>2</sup> from 0 to 2. As the number of rectangles increases, it approaches the exact area of 8/3.\n\n}}\n\nTaking an example, the area under the curve of ''y'' = ''x''<sup>2</sup> between 0 and 2 can be procedurally computed using Riemann's method. \n\nThe interval [0, 2] is firstly divided into ''n'' subintervals, each of which is given a width of <math>\\tfrac{2}{n}</math>; these are the widths of the Riemann rectangles (hereafter \"boxes\").  Because the right Riemann sum is to be used, the sequence of ''x'' coordinates for the boxes will be <math>x_1, x_2, \\ldots, x_n</math>. Therefore, the sequence of the heights of the boxes will be <math>x_1^2, x_2^2, \\ldots, x_n^2</math>. It is an important fact that <math>x_i = \\tfrac{2i}{n}</math>, and <math>x_n = 2</math>.\n\nThe area of each box will be <math>\\tfrac{2}{n} \\times x_i^2</math> and therefore the ''n''th right Riemann sum will be: \n:<math>\\begin{align}\nS &= \\frac{2}{n} \\times \\left(\\frac{2}{n}\\right)^2 + \\cdots + \\frac{2}{n} \\times \\left(\\frac{2i}{n}\\right)^2 + \\cdots + \\frac{2}{n} \\times \\left(\\frac{2n}{n}\\right)^2 \\\\\n  &= \\frac{8}{n^3} \\left(1 + \\cdots + i^2 + \\cdots + n^2\\right)\\\\\n  &= \\frac{8}{n^3} \\left(\\frac{n(n+1)(2n+1)}{6}\\right)\\\\\n  &= \\frac{8}{n^3} \\left(\\frac{2n^3+3n^2+n}{6}\\right)\\\\\n  &= \\frac{8}{3} + \\frac{4}{n} + \\frac{4}{3n^2}\n\\end{align}</math>\n\nIf the limit is viewed as ''n'' → ∞, it can be concluded that the approximation approaches the actual value of the area under the curve as the number of boxes increases. Hence:\n:<math>\\lim_{n \\to \\infty} S = \\lim_{n \\to \\infty}\\left(\\frac{8}{3} + \\frac{4}{n} + \\frac{4}{3n^2}\\right) = \\frac{8}{3}</math>\n\nThis method agrees with the definite integral as calculated in more mechanical ways:\n\n:<math>\\int_0^2 x^2\\,  dx = \\frac{8}{3}</math>\n\nBecause the function is continuous and monotonically increasing on the interval, a right Riemann sum overestimates the integral by the largest amount (while a left Riemann sum would underestimate the integral by the largest amount). This fact, which is intuitively clear from the diagrams, shows how the nature of the function determines how accurate the integral is estimated. While simple, right and left Riemann sums are often less accurate than more advanced techniques of estimating an integral such as the [[Trapezoidal rule]] or [[Simpson's rule]].\n\nThe example function has an easy-to-find anti-derivative so estimating the integral by Riemann sums is mostly an academic exercise; however it must be remembered that not all functions have anti-derivatives so estimating their integrals by summation is practically important.\n\n{{clear}}\n\n==Higher dimensions==\nThe basic idea behind a Riemann sum is to \"break-up\" the domain via a partition into pieces, multiply the \"size\" of each piece by some value the function takes on that piece, and sum all these products. This can be generalized to allow Riemann sums for functions over domains of more than one dimension.\n\nWhile intuitively, the process of partitioning the domain is easy to grasp, the technical details of how the domain may be partitioned get much more complicated than the one dimensional case and involves aspects of the geometrical shape of the domain.<ref>{{cite book |last=Swokowski |first=Earl W. |year=1979 |title=Calculus with Analytic Geometry |edition=Second |publisher=Prindle, Weber & Schmidt |location=Boston, MA |isbn=0-87150-268-2 |pages=821–822}}</ref>\n\n===Two dimensions===\nIn two dimensions, the domain, <math>A</math> may be divided into a number of cells, <math>A_i</math> such that <math>A = \\cup_i A_i</math>. In two dimensions, each cell then can be interpreted as having an \"area\" denoted by <math>\\Delta A_i</math>.<ref>{{cite book |last1=Ostebee |first1=Arnold |last2=Zorn |first2=Paul |year=2002 |title=Calculus from Graphical, Numerical, and Symbolic Points of View |edition=Second |page=M-34 |quote=We chop the plane region ''R'' into ''m'' smaller regions ''R''<sub>1</sub>, ''R''<sub>2</sub>, ''R''<sub>3</sub>, ..., ''R''<sub>''m''</sub>, perhaps of different sizes and shapes. The 'size' of a subregion ''R''<sub>''i''</sub> is now taken to be its ''area'', denoted by Δ''A''<sub>''i''</sub>.}}</ref> The Riemann sum is\n:<math>S = \\sum_{i=1}^n f(x_i^*,y_i^*)\\,\\Delta A_i,</math>\nwhere <math>(x_i^*,y_i^*) \\in A_i</math>.\n\n===Three dimensions===\nIn three dimensions, it is customary to use the letter <math>V</math> for the domain, such that <math>V = \\cup_i V_i</math> under the partition and <math>\\Delta V_i</math> is the \"volume\" of the cell indexed by <math>i</math>. The three-dimensional Riemann sum may then be written as<ref>{{cite book |last=Swokowski |first=Earl W. |year=1979 |title=Calculus with Analytic Geometry |edition=Second |publisher=Prindle, Weber & Schmidt |location=Boston, MA |isbn=0-87150-268-2 |pages=857–858}}</ref>\n:<math>S = \\sum_{i=1}^{n} f(x_i^*,y_i^*,z_i^*) \\,\\Delta V_i</math>\nwith <math>(x_i^*,y_i^*,z_i^*) \\in V_i</math>.\n\n===Arbitrary number of dimensions===\nHigher dimensional Riemann sums follow a similar as from one to two to three dimensions. For an arbitrary dimension, n, a Riemann sum can be written as\n:<math>S = \\sum_i f(P_i^*)\\, \\Delta V_i</math>\nwhere <math>P_i^*\\in V_i</math>, that is, it's a point in the n-dimensional cell <math>V_i</math> with n-dimensional volume <math>\\Delta V_i</math>.\n\n===Generalization===\nIn high generality, Riemann sums can be written\n:<math>S = \\sum_i f(P_i^*) \\mu(V_i)</math>\nwhere <math>P_i^*</math> stands for any arbitrary point contained in the partition element <math>V_i</math> and <math>\\mu</math> is a [[Measure (mathematics)|measure]] on the underlying set. Roughly speaking, a measure is a function that gives a \"size\" of a set, in this case the size of the set <math>V_i</math>; in one dimension, this can often be interpreted as the length of the interval, in two dimensions, an area, in three dimensions, a volume, and so on.\n\n==See also==\n* [[Antiderivative]]\n* [[Euler method]] and [[midpoint method]], related methods for solving differential equations\n* [[Lebesgue integral]]\n* [[Riemann integral]], limit of Riemann sums as the partition becomes infinitely fine\n* [[Simpson's rule]], a powerful numerical method more powerful than basic Riemann sums or even the Trapezoidal rule\n* [[Trapezoidal rule]], numerical method based on the average of the left and right Riemann sum\n\n==References==\n{{Reflist}}\n\n==External links==\n*{{MathWorld |id=RiemannSum |title=Riemann Sum}}\n*[http://www.vias.org/simulations/simusoft_riemannsum.html A simulation showing the convergence of Riemann sums]\n\n[[Category:Integral calculus]]\n[[Category:Bernhard Riemann]]"
    },
    {
      "title": "Ring of periods",
      "url": "https://en.wikipedia.org/wiki/Ring_of_periods",
      "text": "{{Short description|Numbers expressible as integrals of algebraic functions}}\n{{dablink|For a more frequently used sense of the word \"period\" in mathematics, see [[Periodic function]].}}\n\nIn [[mathematics]], a '''period''' is a [[number]] that can be expressed as an [[integral]] of an [[algebraic function]] over an algebraic domain. Sums and products of periods [[Closure (mathematics)|remain]] periods, so the periods form a [[ring (mathematics)|ring]].\n\n{{harvs|txt|first=Maxim|last= Kontsevich|authorlink=Maxim Kontsevich|first2=Don |last2=Zagier|author2-link=Don Zagier|year=2001}} gave a survey of periods and introduced some conjectures about them.\n\n==Definition==\nA real number is called a period if it is the difference of volumes of regions of Euclidean space given by [[polynomial]] [[inequality (mathematics)|inequalities]] with rational coefficients.{{clarify|reason=expand on the definition. Is it any dimensional Euclidean space? How many volumes? etc|date=April 2019}} More generally a complex number is called a period if its real and imaginary parts are periods.\n\nPeriods are numbers that arise as integrals of algebraic functions over domains that are described by algebraic equations or by inequalities with rational coefficients {{harv|Weisstein|2019}}. Periods can be defined as complex numbers whose real and imaginary parts are values of [[absolutely convergent]] integrals of [[rational function]]s with rational coefficients, over domains in <math>\\mathbb{R}^n</math> given by [[polynomial]] [[inequality (mathematics)|inequalities]] with rational coefficients {{harv|Kontsevich|2001|p=3}}. The coefficients of the rational functions and polynomials can be generalised to algebraic numbers since integrals and irrational algebraic numbers are expressible in terms of areas of suitable domains.\n\n==Examples==\n\nBesides the algebraic numbers, the following numbers are known to be periods:\n* The [[natural logarithm]] of any positive algebraic number, a, which is <math>\\int_1^{a}\\frac{1}{x}\\ \\mathrm{d}x</math>\n* [[π|{{pi}}]]<math>=\\int_0^1\\frac{4}{x^2+1}\\ \\mathrm{d}x</math>\n* [[Elliptic integral]]s with rational arguments\n* All [[zeta constant]]s (the [[Riemann zeta function]] of an integer) and [[multiple zeta value]]s\n* Special values of [[hypergeometric function]]s at algebraic arguments\n* [[gamma function|Γ]](''p''/''q'')<sup>''q''</sup> for natural numbers ''p'' and ''q''.\n\nAn example of real number that is not a period is given by [[Chaitin's constant Ω]]. Currently there are no natural examples of [[computable number]]s that have been proved not to be periods, though it is easy to construct artificial examples using [[Cantor's diagonal argument]]. Plausible candidates for numbers that are not periods include ''[[e (mathematical constant)|e]]'', 1/π, and [[Euler–Mascheroni constant γ]].\n\n==Properties and motivation==\n\nThe periods are intended to bridge the gap between the [[algebraic number]]s and the [[transcendental numbers]]. The class of algebraic numbers is too narrow to include many common [[mathematical constant]]s, while the set of transcendental numbers is not [[countable]], and its members are not generally [[computable number|computable]].\n\nThe set of all periods is [[Countable set|countable]], and all periods are [[Computable number|computable]] {{harv|Tent|2010}}, and in particular [[definable number|definable]]. \n\n==Conjectures==\n\nMany of the constants known to be periods are also given by integrals of [[transcendental function]]s. Kontsevich and Zagier note that there \"seems to be no universal rule explaining why certain infinite sums or integrals of transcendental functions are periods\".\n\nKontsevich and Zagier conjectured that, if a period is given by two different integrals, then each integral can be transformed into the other using only the linearity of integrals, [[change of variables|changes of variables]], and the [[fundamental theorem of calculus|Newton&ndash;Leibniz formula]]\n\n: <math> \\int_a^b f'(x) \\, dx = f(b) - f(a) </math>\n\n(or, more generally, the [[Stokes' theorem|Stokes formula]]).\n\nA useful property of algebraic numbers is that equality between two algebraic expressions can be determined algorithmically. The conjecture of Kontsevich and Zagier would imply that equality of periods is also decidable: [[Computable_number#Non-computability_of_the_ordering| inequality of computable reals]] is known [[recursively enumerable]]; and conversely if two integrals agree, then an algorithm could confirm so by trying all possible ways to transform one of them into the other one.\n\nIt is not expected that [[Euler's number]] ''e'' and [[Euler&ndash;Mascheroni constant]] γ are periods. The periods can be extended to ''exponential periods'' by permitting the product of an algebraic function and the [[exponential function]] of an algebraic function as an integrand. This extension includes all algebraic powers of ''e'', the [[gamma function]] of rational arguments, and values of [[Bessel function]]s. If, further, Euler's constant γ is added as a new period, then according to Kontsevich and Zagier \"all classical constants are periods in the appropriate sense\".\n\n==References==\n*{{Citation | last1=Belkale | first1=Prakash | last2=Brosnan | first2=Patrick | title=Periods and Igusa local zeta functions | doi=10.1155/S107379280313142X | mr=2012522 | year=2003 | journal=International Mathematics Research Notices | volume=2003 | issn=1073-7928 | issue=49 | pages=2655–2670}}\n*{{Citation | last1=Kontsevich | first1=Maxim | last2=Zagier | first2=Don | editor1-last=Engquist | editor1-first=Björn | editor2-last=Schmid | editor2-first=Wilfried | title=Mathematics unlimited&mdash;2001 and beyond | chapter-url=http://www.ihes.fr/~maxim/TEXTS/Periods.pdf | publisher=[[Springer-Verlag]] | location=Berlin, New York | isbn=978-3-540-66913-5 | mr=1852188 | year=2001 | chapter=Periods | pages=771–808\n |ref={{harvid|Kontsevich|2001}}}}\n*{{Citation | last1=Waldschmidt | first1=Michel | title=Transcendence of periods: the state of the art | url=https://www.imj-prg.fr/~michel.waldschmidt/articles/pdf/TranscendencePeriods.pdf | doi=10.4310/PAMQ.2006.v2.n2.a3 | mr=2251476 | year=2006 | journal=Pure and Applied Mathematics Quarterly | issn=1558-8599 | volume=2 | issue=2 | pages=435–463}}\n*{{citation|first=Katrin |last=Tent|first2=Martin |last2=Ziegler|title=Computable functions of reals|journal=Münster Journal of Mathematics|volume=3|year=2010|pages=43–66|url=http://wwwmath.uni-muenster.de/mjm/vol_3/mjm_vol_3_04.pdf|doi=\n|ref={{harvid|Tent|2010}}\n}}\n*{{Cite web|url=http://mathworld.wolfram.com/Periods.html|title=Periods|last=Weisstein|first=Eric W.|website=mathworld.wolfram.com|language=en|access-date=2019-06-19\n|ref={{harvid|Weisstein|2019}}}}\n{{reflist}}\n\n==External links==\n* [http://planetmath.org/period PlanetMath: Period]\n\n[[Category:Mathematical constants]]\n[[Category:Algebraic geometry]]\n[[Category:Integral calculus]]\n\n{{Number systems}}"
    },
    {
      "title": "Risch algorithm",
      "url": "https://en.wikipedia.org/wiki/Risch_algorithm",
      "text": "In [[symbolic computation]] (or computer algebra), at the intersection of [[mathematics]] and [[computer science]], the '''Risch algorithm''' is an [[algorithm]] for [[Indefinite integral|indefinite integration]]. It is used in some [[computer algebra system]]s to find [[antiderivative]]s. It is named after the American [[mathematician]] [[Robert Henry Risch]], a specialist in computer algebra who developed it in 1968.\n\nThe algorithm transforms the problem of [[integration (calculus)|integration]] into a problem in [[differential algebra|algebra]]. It is based on the form of the function being integrated and on methods for integrating [[rational function]]s, [[Nth root|radical]]s, [[logarithm]]s, and [[exponential function]]s. Risch called it a [[decision procedure]], because it is a method for deciding whether a function has an [[elementary function]] as an indefinite integral, and if it does, for determining that indefinite integral.\n\nThe complete description of the Risch algorithm takes over 100 pages.<ref>{{harvnb|Geddes|Czapor|Labahn|1992}}.</ref> The '''Risch–Norman algorithm''' is a simpler, faster, but less powerful variant that was developed in 1976 by A. C. Norman.\n\n==Description==\nThe Risch algorithm is used to integrate [[elementary function]]s. These are functions obtained by composing exponentials, logarithms, radicals, trigonometric functions, and the four arithmetic operations ({{nowrap|+ − × ÷}}). [[Pierre-Simon Laplace|Laplace]] solved this problem for the case of [[rational functions]], as he showed that the indefinite integral of a rational function is a rational function and a finite number of constant multiples of logarithms of rational functions. The algorithm suggested by Laplace is usually described in calculus textbooks; as a computer program, it was finally implemented in the 1960s.\n\n[[Joseph Liouville|Liouville]] formulated the problem that is solved by the Risch algorithm. Liouville proved by analytical means that if there is an elementary solution {{math|''g''}} to the equation {{math|1=''g''′ = ''f''}} then there exist constants {{math|''α<sub>i</sub>''}} and functions {{math|''u<sub>i</sub>''}} and {{math|''v''}} in the field generated by {{math|''f''}} such that the solution is of the form\n\n:<math> g = v + \\sum_{i<n} \\alpha_i \\ln (u_i) </math>\n\nRisch developed a method that allows one to consider only a finite set of functions of Liouville's form.\n\nThe intuition for the Risch algorithm comes from the behavior of the exponential and logarithm functions under differentiation. For the function {{math|''f'' ''e<sup>g</sup>''}}, where {{math|''f''}} and {{math|''g''}} are [[differentiable function]]s, we have\n\n: <math> \\left(f \\cdot e^g\\right)^\\prime = \\left(f^\\prime + f\\cdot g^\\prime\\right) \\cdot e^g, \\, </math>\n\nso if {{math|''e<sup>g</sup>''}} were in the result of an indefinite integration, it should be expected to be inside the integral. Also, as\n\n: <math> \\left(f \\cdot(\\ln g)^n\\right)^\\prime =  f^\\prime \\left(\\ln g\\right)^n + n f  \\frac{g^\\prime}{g} \\left(\\ln g\\right)^{n - 1} </math>\n\nthen if {{math|(ln ''g'')<sup>''n''</sup>}} were in the result of an integration, then only a few powers of the logarithm should be expected.\n\n==Problem examples==\nFinding an elementary antiderivative is very sensitive to details. For instance, the following algebraic function has an elementary antiderivative:<ref>This example was posted by Manuel Bronstein to the [[Usenet]] forum ''comp.soft-sys.math.maple'' on 24 November 2000.[https://groups.google.com/d/msg/comp.soft-sys.math.maple/5CcPIR9Ft-Y/xYfGiyJauuoJ]</ref>\n\n: <math> f(x) = \\frac{x}{\\sqrt{x^4 + 10 x^2 - 96 x - 71}},</math>\n\nnamely:\n\n: <math>F(x) = - \\tfrac{1}{8}\\ln \\left( \\left(x^6+15 x^4-80 x^3+27 x^2-528 x+781\\right) \\sqrt{ x^4+10 x^2-96 x-71} - \\left(x^8 + 20 x^6 - 128 x^5 + 54 x^4 - 1408 x^3 + 3124 x^2 + 10001\\right) \\right) + C.</math>\n\nBut if the constant term 71 is changed to 72, it is not possible to represent the antiderivative in terms of elementary functions. Some [[computer algebra system]]s may here return an antiderivative in terms of ''non-elementary'' functions (i.e. [[elliptic integral]]s), which however are outside the scope of the Risch algorithm.\n\nThe following is a more complex example that involves both algebraic and transcendental functions:<ref>{{harvnb|Bronstein|1998}}.</ref>\n\n: <math>f(x) = \\frac{x^2+2x+1+ (3x+1)\\sqrt{x+\\ln x}}{x\\,\\sqrt{x+\\ln x}\\left(x+\\sqrt{x+\\ln x}\\right)}.</math>\n\nIn fact, the antiderivative of this function has a fairly short form:\n\n: <math>F(x) = 2 \\left(\\sqrt{x+\\ln x} + \\ln\\left(x+\\sqrt{x+\\ln x}\\right)\\right) + C.</math>\n\n==Implementation==\nTransforming Risch's theoretical algorithm into an algorithm that can be effectively executed by a computer was a complex task which took a long time.\n\nThe case of the purely transcendental functions (which do not involve roots of polynomials) is relatively easy and was implemented early in most [[computer algebra system]]s. The first implementation was done by [[Joel Moses]] in [[Macsyma]] soon after the publication of Risch's paper.<ref>{{harvnb|Moses|2012}}.</ref>\n\nThe case of purely algebraic functions was solved and implemented in [[Reduce (computer algebra system)|Reduce]] by [[James H. Davenport]].<ref>{{harvnb|Davenport|1981}}.</ref>\n\nThe general case was solved and implemented in Scratchpad, a precursor of [[Axiom (computer algebra system)|Axiom]], by Manuel Bronstein.<ref>{{harvnb|Bronstein|1990}}.</ref>\n\n==Decidability==\nThe Risch algorithm applied to general elementary functions is not an algorithm but a [[RE (complexity)|semi-algorithm]] because it needs to check, as a part of its operation, if certain expressions are equivalent to zero ([[constant problem]]), in particular in the constant field. For expressions that involve only functions commonly taken to be [[elementary function|elementary]] it is not known whether an algorithm performing such a check exists or not (current [[computer algebra system]]s use heuristics); moreover, if one adds the [[absolute value|absolute value function]] to the list of elementary functions, it is known that no such algorithm exists; see [[Richardson's theorem]].\n\nNote that this issue also arises in the [[polynomial division algorithm]]; this algorithm will fail if it cannot correctly determine whether coefficients vanish identically.<ref>{{Cite web| title= Mathematica 7 Documentation: PolynomialQuotient| url= http://reference.wolfram.com/mathematica/ref/PolynomialQuotient.html| work= Section: Possible Issues| accessdate= 17 July 2010}}</ref> Virtually every non-trivial algorithm relating to polynomials uses the polynomial division algorithm, the Risch algorithm included.  If the constant field is [[computable]], i.e., for elements not dependent on {{math|''x''}}, the problem of zero-equivalence is decidable, then the Risch algorithm is a complete algorithm. Examples of computable constant fields are {{math|'''Q'''}} and {{math|'''Q'''(''y'')}}, i.e., rational numbers and rational functions in {{mvar|''y''}} with rational number coefficients, respectively, where {{math|''y''}} is an indeterminate that does not depend on {{math|''x''}}.\n\nThis is also an issue in the [[Gaussian elimination]] matrix algorithm (or any algorithm that can compute the nullspace of a matrix), which is also necessary for many parts of the Risch algorithm.  Gaussian elimination will produce incorrect results if it cannot correctly determine if a pivot is identically zero{{Citation needed|date=January 2012}}.\n\n==See also==\n*[[Symbolic integration]]\n*[[Liouville's theorem (differential algebra)]]\n*[[Axiom (computer algebra system)]]\n*[[Incomplete gamma function]]\n*[[Nonelementary integral]]\n*[[Lists of integrals]]\n\n==Notes==\n{{reflist}}\n\n==References==\n\n*{{Cite journal\n | last = Bronstein\n | first = Manuel\n | title = Integration of elementary functions\n | journal = [[Journal of Symbolic Computation]]\n | volume = 9\n | issue = 2\n | year = 1990\n | pages = 117–173\n | doi = 10.1016/s0747-7171(08)80027-2\n | ref = harv\n}}\n\n*{{Cite journal\n | last = Bronstein\n | first = Manuel\n | title = Symbolic Integration Tutorial\n | year = 1998\n | url=http://www-sop.inria.fr/cafe/Manuel.Bronstein/publications/issac98.pdf\n | ref = harv\n}}\n\n*{{Cite book\n | last = Bronstein\n | first = Manuel\n | title = Symbolic Integration I\n | publisher = Springer\n | year = 2005\n | isbn = 3-540-21493-3\n | ref = harv\n}}\n\n*{{Cite book\n | last = Davenport\n | first = James H.\n | author-link = James H. Davenport\n | title = On the integration of algebraic functions\n | publisher = Springer\n | series = [[Lecture Notes in Computer Science]]\n | volume = 102\n | year = 1981\n | isbn =  978-3-540-10290-8\n | ref = harv\n}}\n\n*{{Cite book\n | last = Geddes\n | first = Keith O.\n | author1-link = Keith Geddes\n | last2 = Czapor\n | first2 = Stephen R.\n | last3 = Labahn\n | first3 = George\n | title = Algorithms for computer algebra\n | publisher = Kluwer Academic Publishers\n | location = Boston, MA\n | year = 1992\n | pages = xxii+585\n | isbn = 0-7923-9259-0\n | doi = 10.1007/b102438\n | ref = harv\n}}\n\n*{{Cite journal\n | last = Moses\n | first = Joel\n | title = Macsyma: A personal history\n | journal = [[Journal of Symbolic Computation]]\n | volume = 47\n | year = 2012\n | pages = 123–130\n | doi = 10.1016/j.jsc.2010.08.018\n | ref = harv\n}}\n\n*{{Cite journal\n | last = Risch\n | first = R. H.\n | title = The problem of integration in finite terms\n | journal = [[Transactions of the American Mathematical Society]]\n | year = 1969\n | volume = 139\n | pages = 167–189\n | publisher = American Mathematical Society\n | doi = 10.2307/1995313\n | jstor = 1995313\n | ref = harv\n}}\n\n*{{Cite journal\n | last = Risch\n | first = R. H.\n | title = The solution of the problem of integration in finite terms\n | journal = [[Bulletin of the American Mathematical Society]]\n | year = 1970\n | volume = 76\n | issue = 3\n | pages = 605–608\n | doi = 10.1090/S0002-9904-1970-12454-5\n | ref = harv\n}}\n\n*{{Cite journal\n | last = Rosenlicht\n | first = Maxwell\n | title = Integration in finite terms\n | journal = [[American Mathematical Monthly]]\n | year = 1972\n | volume = 79\n | issue = 9\n | pages = 963–972\n | publisher = Mathematical Association of America\n | doi = 10.2307/2318066\n | jstor = 2318066\n | ref = harv\n}}\n\n==External links==\n*{{MathWorld\n | urlname = RischAlgorithm\n | title = Risch Algorithm\n | author = Bhatt, Bhuvanesh\n}}\n\n{{DEFAULTSORT:Risch Algorithm}}\n[[Category:Computer algebra]]\n[[Category:Integral calculus]]\n[[Category:Differential algebra]]"
    },
    {
      "title": "Shell integration",
      "url": "https://en.wikipedia.org/wiki/Shell_integration",
      "text": "[[File:Shell integral undergraph - around y-axis.png|thumb|right|300px|A volume is approximated by a collection of hollow cylinders. As the cylinder walls get thinner the approximation gets better. The limit of this approximation is the shell integral.]]\n\n{{Calculus |Integral}}\n\n'''Shell integration''' (the '''shell method''' in [[integral calculus]]) is a means of [[calculation|calculating]] the [[volume]] of a [[solid of revolution]], when integrating along an axis ''perpendicular to'' the axis of revolution. This is in contrast to [[disc integration]] which integrates along the axis ''parallel'' to the axis of revolution.\n\n==Definition==\n\nThe shell method goes as follows: Consider a volume in three dimensions obtained by rotating a cross-section in the {{mvar|xy}}-plane around the {{mvar|y}}-axis. Suppose the cross-section is defined by the graph of the positive function {{math|''f''(''x'')}} on the interval {{math|[''a'', ''b'']}}. Then the formula for the volume will be:\n\n:<math>2 \\pi \\int_a^b x f(x)\\, dx</math>\n\nIf the function is of the {{mvar|y}} coordinate and the axis of rotation is the {{mvar|x}}-axis then the formula becomes:\n\n:<math>2 \\pi \\int_a^b y f(y)\\, dy</math>\n\nIf the function is rotating around the line {{math|''x'' {{=}} ''h''}} or {{math|''y'' {{=}} ''k''}}, the formulas become:<ref>{{Cite web|url=https://math.la.asu.edu/~dheckman/11%20-%20Volume%20-%20Shell%20Method.pdf|title=Volume – Shell Method|last=Heckman|first=Dave|date=2014|website=|publisher=|access-date=2016-09-28}}</ref>\n\n:<math>\\begin{cases}\n\\displaystyle 2 \\pi \\int_a^b (x-h) f(x)\\,dx, & \\text{if}\\ h \\le a < b\\\\\n\\displaystyle 2 \\pi \\int_a^b (h-x) f(x)\\,dx, & \\text{if}\\ a < b \\le h\n\\end{cases}</math> \nand\n:<math>\\begin{cases}\n\\displaystyle 2 \\pi \\int_a^b (y-k) f(y)\\,dy, & \\text{if}\\ k \\le a < b\\\\\n\\displaystyle 2 \\pi \\int_a^b (k-y) f(y)\\,dy, & \\text{if}\\ a < b \\le k\n\\end{cases}</math>\n\nThe formula is derived by computing the [[double integral]] in [[polar coordinates]].\n\n==Example==\n\nConsider the volume, depicted below, whose cross section on the interval [1, 2] is defined by:\n\n:<math>y = (x-1)^2(x-2)^2</math>\n\n{{Multiple image\n | align = none\n | total_width = 600\n | image1 = Shell_2d_example.png\n | width1 = 481 | height1 = 307\n | caption1 = Cross-section\n | image2 = Shell_3D_example.png\n | width2 = 632 | height2 = 463\n | caption2 = 3D volume\n | caption_align = center\n}}\n\nIn the case of disk integration we would need to solve for {{mvar|x}} given {{mvar|y}}. Because the volume is hollow in the middle we will find two functions, one that defines the inner solid and one that defines the outer solid. After integrating these two functions with the disk method we subtract them to yield the desired volume.\n\nWith the shell method all we need is the following formula:\n\n:<math>2 \\pi \\int_1^2 x (x-1)^2(x-2)^2 \\,dx </math>\n\nBy expanding the polynomial the integral becomes very simple. In the end we find the volume is {{sfrac|{{pi}}|10}} cubic units.\n\n==See also==\n*[[Solid of revolution]]\n*[[Disc integration]]\n\n==References==\n{{Reflist}}\n*{{MathWorld|title=Method of Shells|urlname=MethodofShells}}\n*[[Frank J. Ayres|Frank Ayres]], [[Elliott Mendelson]]. ''[[Schaum's Outlines]]: Calculus''. McGraw-Hill Professional 2008, {{isbn|978-0-07-150861-2}}. pp.&nbsp;244–248 ({{Google books|Ag26M8TII6oC|online copy|page=244}})\n\n{{DEFAULTSORT:Shell Integration}}\n[[Category:Integral calculus]]"
    },
    {
      "title": "Signed measure",
      "url": "https://en.wikipedia.org/wiki/Signed_measure",
      "text": "In [[mathematics]], '''signed measure''' is a generalization of the concept of [[measure (mathematics)|measure]] by allowing it to have [[negative and positive numbers|negative]] values.\n\n==Definition==\nThere are two slightly different concepts of a signed measure, depending on whether or not one allows it to take infinite values.  In research papers and advanced books signed measures are usually only allowed to take finite values, while undergraduate textbooks often allow them to take infinite values. To avoid confusion, this article will call these two cases \"finite signed measures\" and \"extended signed measures\".\n\nGiven a [[measurable space]] (''X'', Σ), that is, a [[Set (mathematics)|set]] ''X'' with a [[sigma algebra]] Σ on it, an '''extended signed measure''' is a [[function (mathematics)|function]]\n:<math>\\mu:\\Sigma\\to \\mathbb {R}\\cup\\{\\infty,-\\infty\\}</math>\nsuch that <math>\\mu(\\emptyset)=0</math> and <math> \\mu </math> is [[sigma additivity|sigma additive]], that is, it satisfies the equality\n:<math> \\mu\\left(\\bigcup_{n=1}^\\infty A_n\\right) = \\sum_{n=1}^\\infty \\mu(A_n)</math>\nwhere the series on the right must converge absolutely, for any [[sequence]]  ''A''<sub>1</sub>, ''A''<sub>2</sub>, ..., ''A''<sub>''n''</sub>, ... of [[disjoint set]]s in Σ. One consequence is that any extended signed measure can take +∞ as value, or it can take &minus;∞ as value, but both are not available. The expression ∞&nbsp;&minus;&nbsp;∞ is undefined<ref>See the article \"''[[Extended real number line#Arithmetic operations|Extended real number line]]''\" for more information.</ref> and must be avoided.\n\nA '''finite signed measure''' (aka. '''real measure''') is defined in the same way, except that it is only allowed to take real values. That is, it cannot take +∞ or &minus;∞.\n\nFinite signed measures form a vector space, while extended signed measures are not even closed under addition, which makes them rather hard to work with. On the other hand,  measures are extended signed measures, but are not in general finite signed measures.\n\n==Examples==\nConsider a nonnegative measure <math>\\nu</math> on the space (''X'', Σ) and a [[measurable function]] ''f'':''X''→ '''R''' such that\n\n:<math>\\int_X \\! |f(x)| \\, d\\nu (x) < \\infty. </math>\n\nThen, a finite signed measure is given by\n\n:<math>\\mu (A) = \\int_A \\! f(x) \\, d\\nu (x) </math>\n\nfor all ''A'' in Σ.\n\nThis signed measure takes only finite values. To allow it to take +∞ as a value, one needs to replace the assumption about ''f'' being absolutely integrable with the more relaxed condition\n\n:<math>\\int_X \\! f^-(x) \\, d\\nu (x) < \\infty, </math>\n\nwhere ''f''<sup>&minus;</sup>(''x'') = max(&minus;''f''(''x''), 0) is the [[positive and negative parts|negative part]] of ''f''.\n\n==Properties==\n\nWhat follows are two results which will imply that an extended signed measure is the difference of two nonnegative measures, and a finite signed measure is the difference of two finite non-negative measures.\n\nThe [[Hahn decomposition theorem]] states that given a signed measure μ, there exist two measurable sets ''P'' and ''N'' such that:\n\n#''P''∪''N'' = ''X'' and  ''P''∩''N'' = ∅;\n#μ(''E'') ≥ 0 for each ''E'' in Σ such that ''E'' ⊆ ''P'' &mdash; in other words, ''P'' is a [[positive and negative sets|positive set]];\n#μ(''E'') ≤ 0 for each ''E'' in Σ such that ''E'' ⊆ ''N'' &mdash; that is, ''N'' is a negative set.\nMoreover, this decomposition is unique [[up to]] adding to/subtracting μ-[[null set]]s from ''P'' and ''N''.\n\nConsider then two nonnegative measures μ<sup>+</sup> and μ<sup>−</sup> defined by\n\n:<math> \\mu^+(E) = \\mu(P\\cap E)</math>\n\nand\n\n:<math> \\mu^-(E)=-\\mu(N\\cap E)</math>\n\nfor all measurable sets ''E'', that is, ''E'' in Σ.\n\nOne can check that both μ<sup>+</sup> and μ<sup>−</sup> are nonnegative measures, with one taking only finite values, and are called the ''positive part'' and ''negative part'' of μ, respectively. One has that μ = μ<sup>+</sup> - μ<sup>−</sup>. The measure  |μ| = μ<sup>+</sup> + μ<sup>−</sup> is called the ''variation'' of μ, and its maximum possible value, ||μ|| = |μ|(''X''), is called the ''[[total variation]]'' of μ.\n\nThis consequence of the Hahn decomposition theorem is called the ''Jordan decomposition''. The measures μ<sup>+</sup>, μ<sup>−</sup> and  |μ| are independent of the choice of ''P'' and ''N'' in the Hahn decomposition theorem.\n\n==The space of signed measures==\n\nThe sum of two finite signed measures is a finite signed measure, as is the product of a finite signed measure by a real number: they are closed under [[linear combination]]. It follows that the set of finite signed measures on a measurable space (''X'', Σ) is a real [[vector space]]; this is in contrast to positive measures, which are only closed under [[conical combination]], and thus form a [[convex cone]] but not a vector space. Furthermore, the [[total variation]] defines a [[norm (mathematics)|norm]] in respect to which the space of finite signed measures becomes a [[Banach space]]. This space has even more structure, in that it can be shown to be a [[Dedekind complete]] [[Riesz space|Banach lattice]] and in so doing the [[Radon–Nikodym theorem]] can be shown to be a special case of the [[Freudenthal spectral theorem]].\n\nIf ''X'' is a compact separable space, then the space of finite signed Baire measures is the dual of the real Banach space of all continuous real-valued functions on ''X'', by the  [[Riesz–Markov–Kakutani representation theorem]].\n\n==See also==\n\n* [[Complex measure]]\n* [[Spectral measure]]\n* [[Vector measure]]\n* [[Riesz representation theorem]]\n* [[Total variation]]\n\n==Notes==\n<references/>\n\n==References==\n* {{Citation\n  | last = Bartle\n  | first = Robert G.\n  | title = The Elements of Integration\n  | place = New York-London-Sydney\n  | publisher =  [[John Wiley and Sons]]\n  | year = 1966\n  | pages = X+129\n  | url = \n  | doi = \n  | zbl = 0146.28201\n  | isbn = }}\n*{{Citation\n  | last = Bhaskara Rao\n  | first = K. P. S.\n  | last2 = Bhaskara Rao\n  | first2 = M.\n  | title = Theory of Charges: A Study of Finitely Additive Measures\n  | place = London\n  | publisher = [[Academic Press]]\n  | year = 1983\n  | series = Pure and Applied Mathematics\n  | volume = 109\n  | pages = x + 315\n  | url = https://books.google.com/books?id=mTNQvfe54CoC&printsec=frontcover#v=onepage&q&f=false\n  | doi = \n  | zbl = 0516.28001\n  | isbn = 0-12-095780-9}}\n*{{Citation\n  | last = Cohn\n  | first = Donald L.\n  | title = Measure theory\n  | place = Boston&ndash;Basel&ndash;Stuttgart\n  | publisher = [[Birkhäuser Verlag]]\n  | origyear = 1980\n  | year = 1997\n  | edition = reprint\n  | pages = IX+373\n  | url = https://books.google.com/books?id=vRxV2FwJvoAC&printsec=frontcover&dq=Measure+theory+Cohn&cd=1#v=onepage&q&f=false\n  | doi = \n  | zbl = 0436.28001\n  | isbn = 3-7643-3003-1\n}}\n*{{Citation\n  | last = Diestel\n  | first = J. E.\n  | last2 = Uhl\n  | first2 = J. J. Jr.\n  | title = Vector measures\n  | place = Providence, R.I.\n  | publisher = [[American Mathematical Society]]\n  | year = 1977\n  | series = Mathematical Surveys and Monographs\n  | volume = 15\n  | pages =\n  | url = https://books.google.com/books?id=NCm4E2By8DQC&printsec=frontcover#v=onepage&q&f=false\n  | doi = \n  | zbl = 0369.46039\n  | isbn = 0-8218-1515-6}}\n* {{Citation\n  | last = Dunford  | first = Nelson\n  | author-link = Nelson Dunford\n  | last2 = Schwartz  | first2 = Jacob T.\n  | author2-link = Jacob T. Schwartz\n  | year = 1959\n  | title = Linear Operators. Part I: General Theory. Part II: Spectral Theory. Self Adjoint Operators in Hilbert Space. Part III: Spectral Operators.\n  | place = New York and London\n  | publisher = [[John Wiley and Sons|Interscience Publishers]]\n  | series = Pure and Applied Mathematics\n  | volume = 6\n  | pages = XIV+858\n  | zbl = 0084.10402\n  | isbn = 0-471-60848-3\n}}\n* {{Citation\n  | last = Dunford  | first = Nelson\n  | author-link = Nelson Dunford\n  | last2 = Schwartz  | first2 = Jacob T.\n  | author2-link = Jacob T. Schwartz\n  | year = 1963\n  | title = Linear Operators. Part I: General Theory. Part II: Spectral Theory. Self Adjoint Operators in Hilbert Space. Part III: Spectral Operators.\n  | place = New York and London\n  | publisher = [[John Wiley and Sons|Interscience Publishers]]\n  | series = Pure and Applied Mathematics\n  | volume = 7\n  | pages = IX+859–1923\n  | zbl = 0128.34803\n  | isbn = 0-471-60847-5\n}}\n* {{Citation\n  | last = Dunford  | first = Nelson\n  | author-link = Nelson Dunford\n  | last2 = Schwartz  | first2 = Jacob T.\n  | author2-link = Jacob T. Schwartz\n  | year = 1971\n  | title = Linear Operators. Part I: General Theory. Part II: Spectral Theory. Self Adjoint Operators in Hilbert Space. Part III: Spectral Operators.\n  | place = New York and London\n  | publisher = [[John Wiley and Sons|Interscience Publishers]]\n  | series = Pure and Applied Mathematics\n  | volume = 8\n  | pages = XIX+1925–2592\n  | zbl = 0243.47001\n  | isbn = 0-471-60846-7\n}}\n* {{Citation\n  | last = Zaanen\n  | first = Adriaan C.\n  | year = 1996\n  | title = Introduction to Operator Theory in Riesz spaces\n  | publisher = [[Springer Publishing]]\n  | isbn = 3-540-61989-5\n}}\n\n----\n{{PlanetMath attribution| id=4013 | title=Signed measure | id2=4014 | title2=Hahn decomposition theorem | id3=4015 | title3=Jordan decomposition }}\n\n[[Category:Integral calculus]]\n[[Category:Measures (measure theory)]]\n[[Category:Wikipedia articles incorporating text from PlanetMath]]\n[[Category:Negative concepts]]"
    },
    {
      "title": "Simpson's rule",
      "url": "https://en.wikipedia.org/wiki/Simpson%27s_rule",
      "text": "{{about||Simpson's voting rule|Minimax Condorcet|Simpson's rules used in ship stability|Simpson's rules (ship stability)}}\n[[Image:simpsons method illustration.svg|thumb|right|Simpson's rule can be derived by approximating the integrand ''f ''(''x'') <span style=\"color:blue;\">(in blue)</span> by the quadratic interpolant ''P''(''x'') <span style=\"color:red;\">(in red)</span>.]]\n\n[[Image:simpsonsrule2.gif|thumb|right|An animation showing how Simpson's rule approximation improves with more strips.]]\n\nIn [[numerical analysis]], '''Simpson's  rule''' is a method for [[numerical integration]], the numerical approximation of [[definite integral]]s. Specifically, it is the following approximation for <math>n+1</math> equally spaced subdivisions (where <math>n</math> is even): (General Form)\n\n:<math>\\int_{a}^{b} f(x) \\, dx \\approx \n\\tfrac{\\Delta x}{3}\\left(f(x_0) + 4f(x_1)+2f(x_2)+ 4f(x_3)+2f(x_4)+\\cdots+4f(x_{n-1}) + f(x_{n})\\right)</math>,\nwhere <math>\\Delta x = \\frac{b-a}{n}</math> and <math>x_i=a+i\\Delta x</math>.\n\nSimpson's rule also corresponds to the three-point [[Newton–Cotes formulas|Newton-Cotes quadrature rule]].\n\nIn English, the method is credited to the mathematician [[Thomas Simpson]] (1710–1761) of Leicestershire, England. However, [[Johannes Kepler]] used similar formulas over 100 years prior, and for this reason the method is sometimes called '''Kepler's rule''', or ''Keplersche Fassregel'' (Kepler's barrel rule) in German.\n\n==Quadratic interpolation==\n\nOne derivation replaces the integrand  <math>f(x)</math> by the [[quadratic polynomial]] (i.e. parabola)<math>P(x)</math> which takes the same values as <math>f(x)</math> at the end points ''a'' and ''b'' and the midpoint ''m'' = (''a''&nbsp;+&nbsp;''b'') / 2. One can use [[Lagrange polynomial|Lagrange polynomial interpolation]] to find an expression for this polynomial,\n:<math> P(x) = f(a) \\tfrac{(x-m)(x-b)}{(a-m)(a-b)} + f(m) \\tfrac{(x-a)(x-b)}{(m-a)(m-b)} + f(b) \\tfrac{(x-a)(x-m)}{(b-a)(b-m)}.\n</math>\nUsing [[integration by substitution]] one can show that<ref>Atkinson, p. 256; Süli and Mayers, §7.2</ref>\n:<math> \\int_{a}^{b} P(x) \\, dx =\\tfrac{b-a}{6}\\left[f(a) + 4f\\left(\\tfrac{a+b}{2}\\right)+f(b)\\right].</math>\nIntroducing the step size <math>h=(b-a)/2</math> this is also commonly written as\n:<math> \\int_{a}^{b} P(x) \\, dx =\\tfrac{h}{3}\\left[f(a) + 4f\\left(\\tfrac{a+b}{2}\\right)+f(b)\\right].</math>\nBecause of the <math>1/3</math> factor Simpson's rule is also referred to as Simpson's 1/3 rule (see below for generalization).\n\nThe calculation above can be simplified if one observes that (by scaling) there is [[Without loss of generality|no loss of generality]] in assuming that <math>a=-1</math>。\n\n===Averaging the midpoint and the trapezoidal rules===\n\nAnother derivation constructs Simpson's rule from two simpler approximations: the [[rectangle rule|midpoint rule]]\n:<math> M = (b-a) f \\left( \\tfrac{a+b}{2} \\right) </math>\nand the [[trapezoidal rule]]\n:<math> T = \\tfrac12 (b-a) (f(a)+f(b)). </math>\nThe errors in these approximations are\n:<math> \\tfrac1{24} (b-a)^3 f''(a) + O((b-a)^4) \\quad\\text{and}\\quad -\\tfrac1{12} (b-a)^3 f''(a) + O((b-a)^4), </math>\nrespectively, where <math> O((b-a)^4) </math> denotes a term asymptotically proportional to <math> (b-a)^4 </math>. The two <math> O((b-a)^4) </math> terms are not equal; see [[Big O notation]] for more details. It follows from the above formulas for the errors of the midpoint and trapezoidal rule that the leading error term vanishes if we take the [[weighted average]]\n:<math> \\tfrac{2M+T}{3}. </math>\nThis weighted average is exactly Simpson's rule.\n\nUsing another approximation (for example, the trapezoidal rule with twice as many points), it is possible to take a suitable weighted average and eliminate another error term.  This is [[Romberg's method]].\n\n===Undetermined coefficients===\n\nThe third derivation starts from the ''[[ansatz]]''\n\n:<math> \\tfrac{1}{b-a} \\int_{a}^{b} f(x) \\, dx \\approx \\alpha f(a) + \\beta f\\left(\\tfrac{a+b}{2}\\right) + \\gamma f(b).</math>\n\nThe coefficients α, β and γ can be fixed by requiring that this approximation be exact for all quadratic polynomials. This yields Simpson's rule.\n\n==Error==\n\nThe error in approximating an integral by Simpson's rule for <math>n=2</math> is\n\n:<math> -\\tfrac{1}{90} \\left(\\tfrac{b-a}{2}\\right)^5 f^{(4)}(\\xi), </math>\n\nwhere <math>\\xi</math> (the [[Xi (letter)#Lowercase|Greek letter xi]]) is some number between <math>a</math> and <math>b</math>.<ref>Atkinson, equation (5.1.15); Süli and Mayers, Theorem 7.2</ref>\n\nThe error is asymptotically proportional to <math>(b-a)^5</math>. However, the above derivations suggest an error proportional to <math>(b-a)^4</math>. Simpson's rule gains an extra order because the points at which the integrand is evaluated are distributed symmetrically in the interval <math>[a,\\ b]</math>.\n\nSince the error term is proportional to the fourth derivative of <math>f</math> at <math>\\xi</math>, this shows that Simpson's rule provides exact results for any polynomial <math>f</math> of degree three or less, since the fourth derivative of such a polynomial is zero at all points.\n\nIf the second derivative <math>f''</math> exists and is [[convex function|convex]] in the interval <math>(a,\\ b)</math>:\n:<math> (b-a)f\\left(\\tfrac{a+b}{2}\\right) + \\tfrac{1}{3}\\left(\\tfrac{b-a}{2}\\right)^3 f''\\left(\\tfrac{a+b}{2}\\right) \\leq \\int_{a}^{b} f(x) \\, dx \\leq \\tfrac{b-a}{6}\\left[f(a)+4f\\left(\\tfrac{a+b}{2}\\right)+f(b)\\right].</math>\n\n==Composite Simpson's rule==\n\nIf the interval of integration <math>[a, b]</math> is in some sense \"small\", then Simpson's rule with <math>n=2</math> subintervals will provide an adequate approximation to the exact integral.  By small, what we really mean is that the function being integrated is relatively smooth over the interval <math>[a, b]</math>.  For such a function, a smooth quadratic interpolant like the one used in Simpson's rule will give good results.\n\nHowever, it is often the case that the function we are trying to integrate is not smooth over the interval.  Typically, this means that either the function is highly oscillatory, or it lacks derivatives at certain points.  In these cases, Simpson's rule may give very poor results.  One common way of handling this problem is by breaking up the interval <math>[a, b]</math> into <math>n>2</math> small subintervals.  Simpson's rule is then applied to each subinterval, with the results being summed to produce an approximation for the integral over the entire interval.  This sort of approach is termed the ''composite Simpson's rule''.\n\nSuppose that the interval <math>[a, b]</math> is split up into <math>n</math> sub-intervals, with <math>n</math> an even number. Then, the composite Simpson's rule is given by\n\n:<math>\n\\begin{align}\n\\int_a^b f(x) \\, dx & \\approx \\frac{h}{3}\n\\sum_{j=1}^{n/2}\\bigg[f(x_{2j-2})+4f(x_{2j-1})+f(x_{2j})\\bigg]\\\\\n{} & = \\frac{h}{3}\n\\bigg[f(x_0)+2\\sum_{j=1}^{n/2-1}f(x_{2j}) + 4\\sum_{j=1}^{n/2}f(x_{2j-1}) + f(x_n)\\bigg],\n\\end{align}</math>\n\nwhere <math>x_j=a+jh</math> for <math>j=0, 1, ..., n-1, n</math> with <math>h=(b-a)/n</math>; in particular, <math>x_0=a</math> and <math>x_n=b</math>. This composite rule with <math>n = 2</math> corresponds with the regular Simpson's Rule of the preceding section.\n\nThe error committed by the composite Simpson's rule is\n\n:<math>-\\frac{h^4}{180}(b-a)f^{(4)}(\\xi),</math>\n\nwhere <math>\\xi</math> is some number between <math>a</math> and <math>b</math> and <math>h=(b-a)/n</math> is the \"step length\".<ref>Atkinson, pp. 257+258; Süli and Mayers, §7.5</ref> The error is bounded (in absolute value) by\n\n:<math>\\tfrac{h^4}{180}(b-a)\\max_{\\xi\\in[a,b]} |f^{(4)}(\\xi)|.</math>\n\nThis formulation splits the interval <math>[a,b]</math> in subintervals of equal length. In practice, it is often advantageous to use subintervals of different lengths, and concentrate the efforts on the places where the integrand is less well-behaved. This leads to the [[adaptive Simpson's method]].\n\n==Simpson's 3/8 rule==\n\nSimpson's 3/8 rule is another method for numerical integration proposed by Thomas Simpson. It is based upon a cubic interpolation rather than a quadratic interpolation. Simpson's 3/8 rule is as follows:\n:<math> \\int_{a}^{b} f(x) \\, dx \\approx \\tfrac{3h}{8}\\left[f(a) + 3f\\left(\\tfrac{2a+b}{3}\\right) + 3f\\left(\\tfrac{a+2b}{3}\\right) + f(b)\\right] \n= \\tfrac{(b-a)}{8}\\left[f(a) + 3f\\left(\\tfrac{2a+b}{3}\\right) + 3f\\left(\\tfrac{a+2b}{3}\\right) + f(b)\\right],</math>\nwhere ''b''&nbsp;−&nbsp;''a''&nbsp;=&nbsp;3''h''. \nThe error of this method is:\n:<math>-\\tfrac{(b-a)^5}{6480} f^{(4)}(\\xi),</math>\nwhere <math>\\xi</math> is some number between <math>a</math> and <math>b</math>. Thus, the 3/8 rule is about twice as accurate as the standard method, but it uses one more function value. A composite 3/8 rule also exists, similarly as above.<ref>Matthews (2004)</ref>\n\nA further generalization of this concept for interpolation with arbitrary-degree polynomials are the [[Newton–Cotes formulas]].\n\n=== Composite Simpson's 3/8 rule ===\nDividing the interval <math>[a,b]</math> into <math>n</math> subintervals of length <math>h=(b-a)/n</math> and introducing the nodes <math>x_i = a + ih</math> we have\n\n:<math>\n\\begin{align}\n\\int_a^b f(x) \\, dx & \\approx \\tfrac{3h}{8}\\left[f(x_0) + 3f(x_1) + 3f(x_2) \n+ 2f(x_3) + 3f(x_4) + 3f(x_5) + 2f(x_6) + \\cdots + f(x_n)\\right]. \\\\\n& = \\frac{3h}{8} \\left[ f(x_0) + 3 \\sum_{i \\ne 3k}^{n-1} f(x_i) + 2 \\sum_{j=1}^{n/3 - 1} f(x_{3j}) + f(x_n) \\right] \\qquad \\text{For:  } k \\in \\mathbb{N}_0\n\\end{align}\n</math>\n\nWhile the remainder for the rule is shown as:\n\n:<math>-\\frac{h^4}{80}(b-a)f^{(4)}(\\xi),</math> <ref>Matthews (2004)</ref>\n\nNote, we can only use this if <math>n</math> is a multiple of three.\n\nThe 3/8th rule is also called [[Simpson's rules (ship stability)|Simpson's second rule]].\n\n== Alternative extended Simpson's rule ==\nThis is another formulation of a composite Simpson's rule: instead of applying Simpson's rule to disjoint segments of the integral to be approximated, Simpson's rule is applied to overlapping segments, yielding:<ref>Press (1989), p. 122</ref>\n\n:<math>\n\\int_a^b f(x) \\, dx\\approx\n\\tfrac{h}{48}\\bigg[17f(x_0)+59f(x_1)+43f(x_2)+49f(x_3)+48 \\sum_{i=4}^{n-4} f(x_i)+49f(x_{n-3})+43f(x_{n-2})+59f(x_{n-1})+17f(x_n)\\bigg].\n</math>\n\nThe formula above is obtained by combining the original composite Simpson's rule with the one consisting of using Simpson's 3/8 rule in the extreme subintervals and the standard 3-point rule in the remaining subintervals. The result is then obtained by taking the mean of the two formulas. \n\n=== Simpson's rules in the case of narrow peaks ===\nIn the task of estimation of full area of narrow peak-like functions, Simpson's rules are much less efficient than [[trapezoidal rule]]. Namely, composite Simpson's 1/3 rule requires 1.8 times more points to achieve the same accuracy<ref name=\":0\">{{Cite journal|last=Kalambet|first=Yuri|last2=Kozmin|first2=Yuri|last3=Samokhin|first3=Andrey|date=2018|title=Comparison of integration rules in the case of very narrow chromatographic peaks|url=https://linkinghub.elsevier.com/retrieve/pii/S0169743917305555|journal=Chemometrics and Intelligent Laboratory Systems|volume=179|pages=22–30|doi=10.1016/j.chemolab.2018.06.001|issn=0169-7439|via=}}</ref> as trapezoidal rule. Composite Simpson's 3/8 rule is even less accurate. Integral by Simpson's 1/3 rule can be represented as a sum of 2/3 of integral by trapezoidal rule with step h and 1/3 of integral by rectangle rule with step 2h. No wonder that error of the sum corresponds lo less accurate term. Averaging of Simpson's 1/3 rule composite sums with properly shifted frames produces following rules:\n\n<math>\n\\int_a^b f(x) \\, dx\\approx\n\\tfrac{h}{24}\\bigg[-f(x_{-1})+12f(x_0)+25f(x_1)+24 \\sum_{i=2}^{n-2} f(x_i)+25f(x_{n-1})+12f(x_{n})-f(x_{n+1})\\bigg]\n</math>\n\nwhere two points outside of integrated region are exploited and\n\n<math>\n\\int_a^b f(x) \\, dx\\approx\n\\tfrac{h}{24}\\bigg[9f(x_0)+28f(x_1)+23f(x_2)+24 \\sum_{i=3}^{n-3} f(x_i)+23f(x_{n-2})+28f(x_{n-1})+9f(x_{n})\\bigg]\n</math>\n\nThose rules are very much similar to Press's alternative extended Simpson's rule. Coefficients within the major part of the region being integrated equal one, differences are only at the edges. These three rules can be associated with [[Euler-MacLaurin formula]] with the first derivative term and named '''Euler-MacLaurin integration rules'''<ref name=\":0\" />. They differ only in how the first derivative at the region end is calculated.\n\n== Composite Simpson's rule for irregularly spaced data ==\nFor some applications, the integration interval <math>I=[a,b]</math> needs to be divided into uneven intervals &ndash; perhaps due to uneven sampling of data, or missing or corrupted data points. Suppose we divide the interval <math>I</math> into '''even number <math>N</math> of subintervals''' of widths <math>h_k</math>. Then the composite Simpson's rule is given by<ref>{{Cite conference|last=Kylänpää|first=Ilkka|title=Computational Physics course|location=Tampere University|year=2019}}</ref><ref>{{Cite journal|last=Cartwright|first=Kenneth V.|year=2016|title=Simpson's Rule Integration with MS Excel and Irregularly-spaced Data|url=http://msme.us/2016-2-3.pdf|journal=Journal of Mathematical Science and Mathematics Education|volume=11|issue=2|pages=34–42}}</ref>\n:<math>\n\\int_a^b f(x)\\,\\mathrm{d}x = \\sum_{i=0}^{N/2-1} \\left( \\alpha_i f_{2i+2} + \\beta_i f_{2i+1} + \\eta_i f_{2i} \\right),\n</math>\n\nwhere <math>f_k=f\\left(a+\\sum_{i=0}^{k-1} h_i \\right)</math> are the function values at the <math>k</math>th sampling point on the interval <math>I</math>, and the coefficients <math>\\alpha_i,\\,\\beta_i,</math> and <math>\\eta_i</math> are given by\n\n:<math>\n\\alpha_i = \\frac{2 h_{2i+1}^3-h_{2i}^3+3h_{2i}h_{2i+1}^2}{6h_{2i+1}(h_{2i+1}+h_{2i})},\n</math>\n:<math>\n\\beta_i = \\frac{h_{2i+1}^3+h_{2i}^3+3h_{2i+1}h_{2i}(h_{2i+1}+h_{2i})}{6h_{2i+1}h_{2i}}, \\text{and}\n</math>\n:<math>\n\\eta_i = \\frac{2h_{2i}^3-h_{2i+1}^3+3h_{2i+1}h_{2i}^2}{6 h_{2i}(h_{2i+1}+h_{2i})}.\n</math>\n\nIn case of '''odd number <math>N</math> of subintervals''', the above formula are used up to the second to last interval, \nand the last interval is handled separately by adding the following to the result:\n:<math>\n\\alpha f_N + \\beta f_{N-1} + \\eta f_{N-2},\n</math>\n\nwhere\n\n:<math>\n\\alpha = \\frac{2h_{N-1}^2+3h_{N-1}h_{N-2}}{6(h_{N-2}+h_{N-1})},\n</math>\n:<math>\n\\beta = \\frac{h_{N-1}^2+3h_{N-1}h_{N-2}}{6h_{N-2}},\\,\\text{and}\n</math>\n:<math>\n\\eta = - \\frac{h_{N-1}^3}{6 h_{N-2}(h_{N-2}+h_{N-1})}.\n</math>\n\n{| role=\"presentation\" class=\"wikitable mw-collapsible mw-collapsed\"\n|<strong>Example implementation in [[Python (programming language)|Python]]</strong>\n|-\n|<source lang=\"python\">\nimport numpy as np\n\ndef simpson_nonuniform(x, f):\n    \"\"\"\n    Simpson rule for irregularly spaced data.\n\n        Parameters\n        ----------\n        x : list or np.array of floats\n                Sampling points for the function values\n        f : list or np.array of floats\n                Function values at the sampling points\n\n        Returns\n        -------\n        float : approximation for the integral\n    \"\"\"\n    N = len(x) - 1\n    h = np.diff(x)\n\n    result = 0.0\n    for i in range(1, N, 2):\n        hph = h[i] + h[i - 1]\n        result += f[i] * ( h[i]**3 + h[i - 1]**3\n                           + 3. * h[i] * h[i - 1] * hph )\\\n                     / ( 6 * h[i] * h[i - 1] )\n        result += f[i - 1] * ( 2. * h[i - 1]**3 - h[i]**3\n                              + 3. * h[i] * h[i - 1]**2)\\\n                     / ( 6 * h[i - 1] * hph)\n        result += f[i + 1] * ( 2. * h[i]**3 - h[i - 1]**3\n                              + 3. * h[i - 1] * h[i]**2)\\\n                     / ( 6 * h[i] * hph )\n\n    if (N + 1) % 2 == 0:\n        result += f[N] * ( 2 * h[N - 1]**2\n                          + 3. * h[N - 2] * h[N - 1])\\\n                     / ( 6 * ( h[N - 2] + h[N - 1] ) )\n        result += f[N - 1] * ( h[N - 1]**2\n                           + 3*h[N - 1]* h[N - 2] )\\\n                     / ( 6 * h[N - 2] )\n        result -= f[N - 2] * h[N - 1]**3\\\n                     / ( 6 * h[N - 2] * ( h[N - 2] + h[N - 1] ) )\n    return result\n</source>\n|}\n\n==See also==\n\n* [[Gaussian quadrature]]\n* [[Rectangle method]]\n* [[Trapezoidal rule]]\n* [[Boole's rule]]\n\n==Notes==\n\n{{Reflist}}\n\n==References==\n\n* {{Cite book |last=Atkinson |first=Kendall E. |year=1989 |title=An Introduction to Numerical Analysis |edition=2nd |publisher=John Wiley & Sons |isbn=0-471-50023-2}}\n* {{Cite book |last1=Burden |first1=Richard L. |last2=Faires |first2=J. Douglas |year=2000 |title=Numerical Analysis |edition=7th |publisher=Brooks/Cole |isbn=0-534-38216-9}}\n* {{cite book |last=Pate |first=McCall |year=1918 |title=The naval artificer's manual: (The naval artificer's handbook revised) text, questions and general information for deck |publisher=United States. Bureau of Reconstruction and Repair |url=https://books.google.com/books?id=bQc9AAAAYAAJ&pg=PA198 |page=198}}\n* {{Cite web | author = Matthews, John H. | url = http://math.fullerton.edu/mathews/n2003/Simpson38RuleMod.html | title = Simpson's 3/8 Rule for Numerical Integration | work = Numerical Analysis - Numerical Methods Project | publisher = California State University, Fullerton | accessdate = 11 November 2008 | year = 2004 | deadurl = yes | archiveurl = https://web.archive.org/web/20081204012519/http://math.fullerton.edu/mathews/n2003/Simpson38RuleMod.html | archivedate = 4 December 2008 | df = dmy-all }}\n* {{Cite book |last1=Press |first1=William H. |last2=Flannery |first2=Brian P. |last3=Vetterling |first3=William T. |last4=Teukolsky |first4=Saul A. |year=1989 |title=Numerical Recipes in Pascal: The Art of Scientific Computing |publisher=Cambridge University Press |isbn=0-521-37516-9 |url=https://books.google.com/books?id=bh5w6E-M-PUC&pg=PA122&dq=extended-simpson%27s-rule }}\n* {{Cite book |last1=Süli |first1=Endre |last2=Mayers |first2=David |year=2003 |title=An Introduction to Numerical Analysis |publisher=Cambridge University Press |isbn=0-521-00794-1}}\n* {{Cite web |last1=Kaw | first1=Autar |last2=Kalu |first2=Egwu |last3=Nguyen |first3=Duc |year=2008 | title=Numerical Methods with Applications |url=http://nm.mathforcollege.com/topics/textbook_index.html}}\n* {{Cite web |last=Weisstein |first= Eric W. |year=2010 |title=Newton-Cotes Formulas |work=MathWorld--A Wolframtite Web Resource |publisher=MathWorld |url=http://mathworld.wolfram.com/Newton-CotesFormulas.html |accessdate=2 August 2010}}\n\n==External links==\n* {{springer|title=Simpson formula|id=p/s085450}}\n* {{mathworld|urlname=SimpsonsRule|title=Simpson's Rule}}\n* [http://www.howtoexcel.info/Civil_Engineering/Earthwork_Volume.htm Application of Simpson's Rule &mdash; Earthwork Excavation] (Note: The formula described in this page is correct but there are errors in the calculation which should give a result of 569m3 and not 623m3 as stated)\n* [http://numericalmethods.eng.usf.edu/topics/simpsons_13rd_rule.html Simpson's 1/3rd rule of integration &mdash; Notes, PPT, Mathcad, Matlab, Mathematica, Maple] at [http://numericalmethods.eng.usf.edu Numerical Methods for STEM undergraduate]\n* A detailed description of a computer implementation is described by Dorai Sitaram in ''Teach Yourself [[Scheme (programming language)|Scheme]] in Fixnum Days'', [http://www.ccs.neu.edu/home/dorai/t-y-scheme/t-y-scheme-Z-H-22.html#node_chap_C Appendix C]\n\n{{PlanetMath attribution|id=6518|title=Code for Simpson's rule}}\n\n{{Use dmy dates|date=September 2010}}\n\n{{DEFAULTSORT:Simpson's Rule}}\n[[Category:Integral calculus]]\n[[Category:Numerical integration (quadrature)]]\n[[Category:Numerical analysis]]\n[[Category:Articles with example Python code]]\n[[Category:Articles with example C code]]"
    },
    {
      "title": "Singular measure",
      "url": "https://en.wikipedia.org/wiki/Singular_measure",
      "text": "In [[mathematics]], two positive (or [[signed measure|signed]] or [[complex measure|complex]]) measures ''&mu;'' and ''&nu;'' defined on a [[measurable space]] (&Omega;, &Sigma;) are called '''singular''' if there exist two disjoint sets ''A'' and ''B'' in  &Sigma; whose [[set union|union]] is &Omega; such that ''&mu;'' is zero on all measurable subsets of ''B'' while ''&nu;'' is zero on all measurable subsets of ''A''. This is denoted by  <math>\\mu \\perp \\nu.</math>\n\nA refined form of [[Lebesgue's decomposition theorem]] decomposes a singular measure into a singular continuous measure and a [[discrete measure]]. See below for examples.\n\n==Examples on '''R'''<sup>''n''</sup>==\nAs a particular case, a measure defined on the [[Euclidean space]] <math>\\mathbb{R}^n</math> is called ''singular'', if it is singular with respect to the [[Lebesgue measure]] on this space. For example, the [[Dirac delta function]] is a singular measure.\n\n'''Example.''' A [[discrete measure]].\n\nThe [[Heaviside step function]] on the [[real line]],\n\n: <math>H(x) \\ \\stackrel{\\mathrm{def}}{=} \\begin{cases} 0, & x < 0; \\\\ 1, & x \\geq 0; \\end{cases}</math>\n\nhas the [[Dirac delta function|Dirac delta distribution]] <math>\\delta_0</math> as its [[distributional derivative]]. This is a measure on the real line, a \"[[point mass]]\" at 0. However, the [[Dirac measure]] <math>\\delta_0</math> is not absolutely continuous with respect to Lebesgue measure <math>\\lambda</math>, nor is <math>\\lambda</math> absolutely continuous with respect to <math>\\delta_0</math>: <math>\\lambda ( \\{ 0 \\} ) = 0</math> but <math>\\delta_0 ( \\{ 0 \\} ) = 1</math>; if <math>U</math> is any [[open set]] not containing 0, then <math>\\lambda (U) > 0</math> but <math>\\delta_0 (U) = 0</math>.\n\n'''Example.''' A singular continuous measure.\n\nThe [[Cantor distribution]] has a [[cumulative distribution function]] that is continuous but not [[absolutely continuous]], and indeed its absolutely continuous part is zero: it is singular continuous.\n\n'''Example.''' A singular continuous measure on '''R'''<sup>''2''</sup>.\n\nThe upper and lower [[Copula_(probability_theory)#Fréchet–Hoeffding copula bounds|Fréchet–Hoeffding bounds]] are singular distributions in two dimensions.\n\n==See also==\n* [[Lebesgue's decomposition theorem]]\n* [[Absolutely continuous]]\n* [[Singular distribution]]\n\n==References==\n* Eric W Weisstein, ''CRC Concise Encyclopedia of Mathematics'', CRC Press, 2002. {{ISBN|1-58488-347-2}}.\n* J Taylor, ''An Introduction to Measure and Probability'', Springer, 1996. {{ISBN|0-387-94830-9}}.\n\n{{PlanetMath attribution|id=4002|title=singular measure}}\n\n[[Category:Integral calculus]]\n[[Category:Measures (measure theory)]]"
    },
    {
      "title": "Solid of revolution",
      "url": "https://en.wikipedia.org/wiki/Solid_of_revolution",
      "text": "[[File:Rotationskoerper animation.gif|thumb|right|Rotating a curve. The surface formed is a [[surface of revolution]]; it encloses a solid of revolution.]]\n[[File:Revolução de poliedros 03.webm|thumb|300px|Solids of revolution ([[:pt:Matemateca_IME-USP|Matemateca Ime-Usp]])]]\nIn [[mathematics]], [[engineering]], and [[manufacturing]], a '''solid of revolution''' is a [[Shape|solid figure]] obtained by rotating a [[plane curve]] around some [[straight line]] (the ''[[axis of rotation|axis of revolution]]'') that lies on the same plane.\n\nAssuming that the curve does not cross the axis, the solid's [[volume]] is equal to the [[length]] of the [[circle]] described by the figure's [[centroid]] multiplied by the figure's [[area]] ([[Pappus's centroid theorem|Pappus's second centroid Theorem]]).\n\nA '''representative disk''' is a three-[[dimension]]al [[volume element]] of a solid of revolution.  The element is created by [[rotating]] a [[line segment]] (of [[length]] {{mvar|w}}) around some axis (located {{mvar|r}} units away), so that a [[cylinder (geometry)|cylindrical]] [[volume]] of {{math|π''r''<sup>2</sup>''w''}} units is enclosed.\n\n==Finding the volume==\nTwo common methods for finding the volume of a solid of revolution are the disc method and the shell method of integration. To apply these methods, it is easiest to draw the graph in question; identify the area that is to be revolved about the axis of revolution; determine the volume of either a disc-shaped slice of the solid, with thickness {{mvar|δx}}, or a cylindrical shell of width {{mvar|δx}}; and then find the limiting sum of these volumes as {{mvar|δx}} approaches 0, a value which may be found by evaluating a suitable integral.\n\n===Disk method===\n[[File:Disc integration.svg|thumb|right|Disk integration about the y-axis]]\n{{main|Disc integration}}\n\nThe disk method is used when the slice that was drawn is ''perpendicular to'' the axis of revolution; i.e. when integrating ''parallel to'' the axis of revolution.\n\nThe volume of the solid formed by rotating the area between the curves of {{math|''f''(''x'')}} and {{math|''g''(''x'')}} and the lines {{math|1=''x'' = ''a''}} and {{math|1=''x'' = ''b''}} about the {{mvar|x}}-axis is given by\n:<math>V = \\pi \\int_a^b \\left| f(x)^2 - g(x)^2\\right|\\,dx\\, .</math>\nIf {{math|1=''g''(''x'') = 0}} (e.g. revolving an area between the curve and the {{mvar|x}}-axis), this reduces to:\n:<math>V = \\pi \\int_a^b f(x)^2 \\,dx\\, .</math>\n\nThe method can be visualized by considering a thin horizontal rectangle at {{mvar|y}} between {{math|''f''(''y'')}} on top and {{math|''g''(''y'')}} on the bottom, and revolving it about the {{mvar|y}}-axis; it forms a ring (or disc in the case that {{math|1=''g''(''y'') = 0}}), with outer radius {{math|''f''(''y'')}} and inner radius {{math|''g''(''y'')}}.  The area of a ring is {{math|π(''R''<sup>2</sup> − ''r''<sup>2</sup>)}}, where {{mvar|R}} is the outer radius (in this case {{math|''f''(''y'')}}), and {{mvar|r}} is the inner radius (in this case {{math|''g''(''y'')}}). The volume of each infinitesimal disc is therefore {{math|π''f''(''y'')<sup>2</sup> ''dy''}}. The limit of the Riemann sum of the volumes of the discs between {{mvar|a}} and {{mvar|b}} becomes integral (1).\n\n===Cylinder method===\n{{main|Shell integration}}\n[[File:Shell integration.svg|thumb|right|Shell integration]]\n{{multiple image\n | align = right\n | direction = vertical\n | width = 800\n | header = Solid of revolution demonstration\n | image1 = Revolução de poliedros 01.jpg\n | alt1 = five coloured polyhedra mounted on vertical axes\n | caption1 = The shapes at rest\n | image2 = Revolução de poliedros 02.jpg\n | alt2 = five solids of rotation formed by rotating polyhedra\n | caption2 = The shapes in motion, showing the solids of revolution formed by each\n}}\nThe cylinder method is used when the slice that was drawn is ''parallel to'' the axis of revolution; i.e. when integrating ''perpendicular to'' the axis of revolution.\n\nThe volume of the solid formed by rotating the area between the curves of {{math|''f''(''x'')}} and {{math|''g''(''x'')}} and the lines {{math|1=''x'' = ''a''}} and {{math|1=''x'' = ''b''}} about the {{mvar|y}}-axis is given by\n:<math>V = 2\\pi \\int_a^b x |f(x) - g(x)|\\,dx\\, .</math>\nIf {{math|1=''g''(''x'') = 0}} (e.g. revolving an area between curve and {{mvar|y}}-axis), this reduces to:\n:<math>V = 2\\pi \\int_a^b x | f(x) | \\,dx\\, .</math>\n\nThe method can be visualized by considering a thin vertical rectangle at {{mvar|x}} with height {{math|''f''(''x'') − ''g''(''x'')}}, and revolving it about the {{mvar|y}}-axis; it forms a cylindrical shell.  The lateral surface area of a cylinder is {{math|2π''rh''}}, where {{mvar|r}} is the radius (in this case {{mvar|x}}), and {{mvar|h}} is the height (in this case {{math|''f''(''x'') − ''g''(''x'')}}).  Summing up all of the surface areas along the interval gives the total volume.\n\n==Parametric form==\n[[File:Paolo uccello, studio di vaso in prospettiva 02.jpg|thumb|[[Mathematics and art]]: study of a vase as a solid of revolution by [[Paolo Uccello]]. 15th century]]\n\nWhen a curve is defined by its parametric form {{math|(''x''(''t''),''y''(''t''))}} in some interval {{math|[''a'',''b'']}}, the volumes of the solids generated by revolving the curve around the {{mvar|x}}-axis or the {{mvar|y}}-axis are given by<ref>{{cite book\n|title=Application Of Integral Calculus\n|first=A.&nbsp;K.\n|last=Sharma\n|publisher=Discovery Publishing House\n|year=2005\n|isbn=81-7141-967-4\n|page=168\n|url=https://books.google.com/books?id=V_WxjYMKuUAC&pg=PA168}}</ref>\n:<math>V_x = \\int_a^b \\pi y^2 \\, \\frac{dx}{dt} \\, dt \\, ,</math>\n\n:<math>V_y = \\int_a^b \\pi x^2 \\, \\frac{dy}{dt} \\, dt \\, .</math>\n\nUnder the same circumstances the areas of the surfaces of the solids generated by revolving the curve around the {{mvar|x}}-axis or the {{mvar|y}}-axis are given by<ref>{{cite book\n|title=Engineering Mathematics\n|edition=6th\n|first=Ravish R.\n|last=Singh\n|publisher=Tata McGraw-Hill\n|year=1993\n|isbn=0-07-014615-2\n|page=6.90\n|url=https://books.google.com/books?id=oQ1y1HCpeowC&pg=SA6-PA90}}</ref>\n:<math>A_x = \\int_a^b 2 \\pi y \\, \\sqrt{ \\left( \\frac{dx}{dt} \\right)^2 + \\left( \\frac{dy}{dt} \\right)^2} \\, dt \\, ,</math>\n\n:<math>A_y = \\int_a^b 2 \\pi x \\, \\sqrt{ \\left( \\frac{dx}{dt} \\right)^2 + \\left( \\frac{dy}{dt} \\right)^2} \\, dt \\, .</math>\n\n==See also==\n{{commons category|Solids of revolution}}\n* [[Gabriel's Horn]]\n* [[Guldinus theorem]]\n* [[Pseudosphere]]\n* [[Surface of revolution]]\n* [[Ungula]]\n\n==Notes==\n{{reflist}}\n\n== References ==\n*{{cite web |website=CliffsNotes.com |title=Volumes of Solids of Revolution |date=12 Apr 2011 |url=http://www.cliffsnotes.com/study_guide/topicArticleId-39909,articleId-39907.html |deadurl=yes |archiveurl=https://web.archive.org/web/20120319195953/http://www.cliffsnotes.com/study_guide/topicArticleId-39909,articleId-39907.html |archivedate=2012-03-19 |df= }} \n*{{cite book|author1-link=Frank J. Ayres |first1=Frank |last1=Ayres |author2-link=Elliott Mendelson |first2=Elliott |last2=Mendelson |series=[[Schaum's Outlines]] |title=Calculus |publisher=McGraw-Hill Professional |date=2008 |ISBN=978-0-07-150861-2 |pages=244–248}} ({{Google books|Ag26M8TII6oC|online copy|page=244}})\n*{{MathWorld |id=SolidofRevolution |title=Solid of Revolution}}\n\n{{Authority control}}\n[[Category:Integral calculus]]"
    },
    {
      "title": "Stochastic calculus",
      "url": "https://en.wikipedia.org/wiki/Stochastic_calculus",
      "text": "{{Short description|calculus on stochastic processes}}\n{{No footnotes|date=August 2011}}\n{{Calculus |Specialized}}\n'''Stochastic calculus''' is a branch of [[mathematics]] that operates on [[stochastic process]]es. It allows a consistent theory of integration to be defined for [[integrals]] of stochastic processes with respect to stochastic processes. It is used to model systems that behave randomly.\n\nThe best-known stochastic process to which stochastic calculus is applied is the [[Wiener process]] (named in honor of [[Norbert Wiener]]), which is used for modeling [[Brownian motion]] as described by [[Louis Bachelier]] in 1900 and by [[Albert Einstein]] in 1905 and other physical [[diffusion]] processes in space of particles subject to random forces.  Since the 1970s, the Wiener process has been widely applied in [[financial mathematics]] and [[economics]] to model the evolution in time of stock prices and bond interest rates.\n\nThe main flavours of stochastic calculus are the [[Itô calculus]] and its variational relative the [[Malliavin calculus]].  For technical reasons the Itô integral is the most useful for general classes of processes, but the related [[Stratonovich integral]] is frequently useful in problem formulation (particularly in engineering disciplines). The Stratonovich integral can readily be expressed in terms of the Itô integral.  The main benefit of the Stratonovich integral is that it obeys the usual [[chain rule]] and therefore does not require [[Itô's lemma]]. This enables problems to be expressed in a coordinate system invariant form, which is invaluable when developing stochastic calculus on manifolds other than '''R'''<sup>''n''</sup>.\nThe [[dominated convergence theorem]] does not hold for the Stratonovich integral, consequently it is very difficult to prove results without re-expressing the integrals in Itô form.\n\n== Itô integral ==\n{{main|Itô calculus}}\n\nThe [[Itô integral]] is central to the study of stochastic calculus. The integral <math>\\int H\\,dX</math> is defined for a [[semimartingale]] ''X'' and locally bounded '''predictable''' process ''H''. {{Citation needed|date=August 2011}}\n\n== Stratonovich integral ==\n{{main|Stratonovich integral}}\n\nThe Stratonovich integral of a [[semimartingale]] <math>X</math> against another [[semimartingale]] ''Y'' can be defined in terms of the Itô integral as\n\n:<math> \\int_0^t X_{s-} \\circ d Y_s : = \\int_0^t X_{s-} d Y_s + \\frac{1}{2} \\left [ X, Y\\right]_t^c,</math>\n\nwhere [''X'',&nbsp;''Y'']<sub>''t''</sub><sup>''c''</sup> denotes the [[Quadratic variation|quadratic covariation]] of the continuous parts of ''X''\nand&nbsp;''Y''. The alternative notation\n\n:<math> \\int_0^t X_s \\, \\partial Y_s </math>\n\nis also used to denote the Stratonovich integral.\n\n== Applications ==\n\nAn important application of stochastic calculus is in [[quantitative finance]], in which asset prices are often assumed to follow [[stochastic differential equations]].  In the [[Black–Scholes model]], prices are assumed to follow [[geometric Brownian motion]].\n\n== References ==\n* Fima C Klebaner, 2012, Introduction to Stochastic Calculus with Application (3rd Edition). World Scientific Publishing, {{isbn|9781848168312}}\n* {{Cite journal | last1 = Szabados | first1 = T. S. | last2 = Székely | first2 = B. Z. | doi = 10.1007/s10959-007-0140-8 | title = Stochastic Integration Based on Simple, Symmetric Random Walks | journal = Journal of Theoretical Probability | volume = 22 | pages = 203 | year = 2008 | pmid =  | pmc = | arxiv = 0712.3908 }} [https://arxiv.org/PS_cache/arxiv/pdf/0712/0712.3908v2.pdf Preprint]\n\n[[Category:Stochastic calculus| ]]\n[[Category:Mathematical finance]]\n[[Category:Integral calculus]]"
    },
    {
      "title": "Sum rule in integration",
      "url": "https://en.wikipedia.org/wiki/Sum_rule_in_integration",
      "text": "{{Unreferenced|date=June 2009}}\nIn [[calculus]], the '''sum rule in integration''' states that the integral of a sum of two functions is equal to the sum of their integrals.  It is of particular use for the [[integral|integration]] of [[summation|sums]], and is one part of the [[linearity of integration]].\n\nAs with many properties of integrals in calculus, the sum rule applies both to [[definite integral]]s and [[indefinite integral]]s.  For indefinite integrals, the sum rule states\n\n:<math>\\int \\left(f + g\\right) \\,dx = \\int f \\,dx + \\int g \\,dx</math>\n\n==Application to indefinite integrals==\nFor example, if you know that the [[integral]] of exp(x) is exp(x) from [[exponential function|calculus with exponentials]] and that the [[integral]] of cos(x) is sin(x) from [[differentiation of trigonometric functions|calculus with trigonometry]] then:\n\n:<math>\\int \\left(e^x + \\cos{x}\\right) \\,dx = \\int e^x \\,dx + \\int \\cos{x}\\ \\,dx = e^x + \\sin{x} + C</math>\n\nSome other general results come from this rule. For example:\n\n{|\n|-\n|<math>\\int \\left(u-v\\right)dx</math>\n|<math>= \\int u+\\left(-v\\right) \\,dx</math>\n|-\n|\n|<math>= \\int u \\,dx + \\int \\left(-v\\right)\\,dx</math>\n|-\n|\n|<math>= \\int u \\,dx + \\left(-\\int v\\,dx\\right)</math>\n|-\n|\n|<math>= \\int u \\,dx - \\int v \\,dx</math>\n|}\nThe proof above relied on the special case of the [[constant factor rule in integration]] with k=-1.\n\nThus, the sum rule might be written as:\n\n:<math>\\int (u \\pm v) \\,dx = \\int u\\, dx \\pm \\int v\\, dx</math>\n\nAnother basic application is that sigma and integral signs can be changed around. That is:\n\n:<math>\\int \\sum^b_{r=a} f\\left(r,x\\right)\\, dx = \\sum^b_{r=a} \\int f\\left(r,x\\right) \\,dx</math>\n\nThis is simply because:\n\n:<math>\\int \\sum^b_{r=a} f(r,x)\\, dx</math>\n:<math> = \\int f\\left(a,x\\right) + f((a+1),x) + f((a+2),x) + \\dots </math>\n::::::<math>+ f((b-1),x) + f(b,x)\\, dx</math>\n:<math> = \\int f(a,x)\\,dx + \\int f((a+1),x)\\,  dx + \\int f((a+2),x) \\,dx + \\dots </math>\n::::::<math>+ \\int f((b-1),x)\\, dx + \\int f(b,x)\\, dx</math>\n:<math> = \\sum^b_{r=a} \\int f(r,x)\\, dx</math>\n\n==Application to definite integrals==\nPassing from the case of indefinite integrals to the case of integrals over an interval [a,b], we get exactly the same form of rule (the [[arbitrary constant of integration]] disappears).\n\n==The proof of the rule==\nFirst note that from the definition of [[integral|integration]] as the [[antiderivative]], the reverse process of [[derivative|differentiation]]:\n\n:<math>u = \\int \\frac{du}{dx} \\,dx</math>\n:<math>v = \\int \\frac{dv}{dx} \\,dx</math>\n\n[[Summation|Adding]] these,\n\n:<math>u + v = \\int \\frac{du}{dx} \\,dx + \\int \\frac{dv}{dx} \\,dx \\quad \\mbox{(1)}</math>\n\nNow take the [[sum rule in differentiation]]:\n\n:<math>\\frac{d}{dx} \\left(u+v\\right) = \\frac{du}{dx} + \\frac{dv}{dx}</math>\n\nIntegrate both sides with respect to x:\n\n:<math>u + v = \\int \\left(\\frac{du}{dx} + \\frac{dv}{dx}\\right) \\,dx \\quad \\mbox{(2)}</math>\n\nSo we have, looking at (1) and (2):\n\n:<math>u+v = \\int \\frac{du}{dx} \\,dx + \\int \\frac{dv}{dx}\\,dx</math>\n:<math>u+v = \\int \\left(\\frac{du}{dx} + \\frac{dv}{dx}\\right) \\,dx</math>\n\nTherefore:\n\n:<math>\\int \\left(\\frac{du}{dx} + \\frac{dv}{dx}\\right) \\,dx = \\int \\frac{du}{dx} \\,dx + \\int \\frac{dv}{dx} \\,dx</math>\n\nNow substitute:\n\n:<math>f = \\frac{du}{dx}</math>\n:<math>g = \\frac{dv}{dx}</math>\n\n[[Category:Integral calculus]]"
    },
    {
      "title": "Surface of revolution",
      "url": "https://en.wikipedia.org/wiki/Surface_of_revolution",
      "text": "[[File:Surface of revolution illustration.png|thumb|A portion of the curve {{math|1=''x'' = 2 + cos ''z''}} rotated around the {{mvar|z}}-axis]]\n\nA '''surface of revolution''' is a [[Surface (mathematics)|surface]] in [[Euclidean space]] created by rotating a [[curve]] (the '''generatrix''') around an  [[axis of rotation]].<ref>{{cite book|title=Analytic Geometry|last1=Middlemiss|last2=Marks|last3=Smart|edition=3rd|chapter=15-4. Surfaces of Revolution|LCCN=68015472|page=378}}</ref>\n\nExamples of surfaces of revolution generated by a straight line are [[cylinder (geometry)|cylindrical]] and [[conical surface]]s depending on whether or not the line is parallel to the axis. A circle that is rotated around any diameter generates a sphere of which it is then a [[great circle]], and if the circle is rotated around an axis that does not intersect the interior of a circle, then it generates a [[torus]] which does not intersect itself (a [[ring torus]]).\n\n==Properties==\nThe sections of the surface of revolution made by planes through the axis are called ''meridional sections''. Any meridional section can be considered to be the generatrix in the plane determined by it and the axis.<ref>{{citation|first1=W.A.|last1=Wilson|first2=J.I.|last2=Tracey|title=Analytic Geometry|edition=Revised|year=1925|publisher=D.C. Heath and Co.|page=227}}</ref>\n\nThe sections of the surface of revolution made by planes that are perpendicular to the axis are circles.\n\nSome special cases of [[hyperboloids]] (of either one or two sheets) and [[elliptic paraboloid]]s are surfaces of revolution. These may be identified as those quadratic surfaces all of whose [[Cross section (geometry)|cross sections]] perpendicular to the axis are circular.\n\n==Area formula==\nIf the curve is described by the [[parametric curve|parametric]] functions {{math|''x''(''t'')}}, {{math|''y''(''t'')}}, with {{mvar|t}} ranging over some interval {{math|[''a'',''b'']}}, and the axis of revolution is the {{mvar|y}}-axis, then the area {{mvar|A<sub>y</sub>}} is given by the [[integral]]\n\n:<math> A_y = 2 \\pi \\int_a^b x(t) \\, \\sqrt{\\left({dx \\over dt}\\right)^2 + \\left({dy \\over dt}\\right)^2} \\, dt, </math>\n\nprovided that {{math|''x''(''t'')}} is never negative between the endpoints {{mvar|a}} and {{mvar|b}}. This formula is the calculus equivalent of  [[Pappus's centroid theorem]].<ref>{{cite book|title=Calculus|first=George B.|last=Thomas|edition=3rd|chapter=6.7: Area of a Surface of Revolution; 6.11: The Theorems of Pappus|pages=206–209, 217–219|LCCN=69016407}}</ref> The quantity\n\n:<math>\\sqrt{ \\left({dx \\over dt}\\right)^2 + \\left({dy \\over dt}\\right)^2 }</math>\n\ncomes from the [[Pythagorean theorem]] and represents a small segment of the arc of the curve, as in the [[arc length]] formula. The quantity {{math|2π''x''(''t'')}} is the path of (the centroid of) this small segment, as required by Pappus' theorem.\n\nLikewise, when the axis of rotation is the {{mvar|x}}-axis and provided that {{math|''y''(''t'')}} is never negative, the area is given by<ref>{{cite book\n|title=Engineering Mathematics\n|edition=6\n|author=Singh\n|first=R.R.\n|publisher=Tata McGraw-Hill\n|year=1993\n|isbn=0-07-014615-2\n|page=6.90\n|url=https://books.google.com/books?id=oQ1y1HCpeowC&pg=SA6-PA90}}</ref>\n:<math> A_x = 2 \\pi \\int_a^b y(t) \\, \\sqrt{\\left({dx \\over dt}\\right)^2 + \\left({dy \\over dt}\\right)^2} \\, dt. </math>\n\nIf the continuous curve is described by the function {{math|1=''y'' = ''f''(''x'')}}, {{math|''a'' ≤ ''x'' ≤ ''b''}}, then the integral becomes\n\n:<math>A_x = 2\\pi\\int_a^b y \\sqrt{1+\\left(\\frac{dy}{dx}\\right)^2} \\, dx = 2\\pi\\int_a^bf(x)\\sqrt{1+\\big(f'(x)\\big)^2} \\, dx</math>\n\nfor revolution around the {{mvar|x}}-axis, and\n\n:<math>A_y =2\\pi\\int_a^b x \\sqrt{1+\\left(\\frac{dy}{dx}\\right)^2} \\, dx</math>\n\nfor revolution around the ''y''-axis (provided {{math|''a'' ≥ 0}}). These come from the above formula.<ref>{{citation|last=Swokowski|first=Earl W.|year=1983|title=Calculus with analytic geometry|edition=Alternate|page= 617|publisher=Prindle, Weber & Schmidt|isbn=0-87150-341-7}}</ref>\n\nFor example, the [[sphere|spherical surface]] with unit radius is generated by the curve {{math|1=''y''(''t'') = sin(''t'')}}, {{math|1=''x''(''t'') = cos(''t'')}}, when {{mvar|t}} ranges over {{math|[0,π]}}.  Its area is therefore\n:<math>\\begin{align}\nA\n &{}= 2 \\pi \\int_0^\\pi \\sin(t) \\sqrt{\\big(\\cos(t)\\big)^2 + \\big(\\sin(t)\\big)^2} \\, dt \\\\\n &{}= 2 \\pi \\int_0^\\pi \\sin(t) \\, dt \\\\\n &{}= 4\\pi.\n\\end{align}</math>\n\nFor the case of the spherical curve with radius {{mvar|r}}, {{math|1=''y''(''x'') = {{sqrt|''r''<sup>2</sup> − ''x''<sup>2</sup>}}}} rotated about the {{mvar|x}}-axis\n:<math>\\begin{align}\nA\n &{}= 2 \\pi \\int_{-r}^{r} \\sqrt{r^2 - x^2}\\,\\sqrt{1 + \\frac{x^2}{r^2 - x^2}}\\,dx \\\\\n &{}= 2 \\pi r\\int_{-r}^{r} \\,\\sqrt{r^2 - x^2}\\,\\sqrt{\\frac{1}{r^2 - x^2}}\\,dx \\\\\n &{}= 2 \\pi r\\int_{-r}^{r} \\,dx \\\\\n &{}= 4 \\pi r^2\\,\n\\end{align}</math>\n\nA [[minimal surface of revolution]] is the surface of revolution of the curve between two given points which [[mathematical optimization|minimizes]] [[surface area]].<ref name=\"Mathworld: Minimal Surface of Revolution\">{{MathWorld | id=MinimalSurfaceofRevolution | title=Minimal Surface of Revolution}}</ref> A basic problem in the [[calculus of variations]] is finding the curve between two points that produces this minimal surface of revolution.<ref name=\"Mathworld: Minimal Surface of Revolution\"/>\n\nThere are only two [[minimal surfaces of revolution]] ([[surfaces of revolution]] which are also minimal surfaces): the [[plane (geometry)|plane]] and the [[catenoid]].<ref>{{MathWorld|id=Catenoid|title=Catenoid}}</ref>\n\n==Rotating a function==\nTo generate a surface of revolution out of any 2-dimensional scalar function {{math|1=''y'' = ''f''(''x'')}}, simply make {{mvar|u}} the function's parameter, set the axis of rotation's function to simply {{mvar|u}}, then use {{mvar|v}} to rotate the function around the axis by setting the other two functions equal to {{math|''f''(''u'') sin ''v''}} and {{math|''f''(''u'') cos ''v''}}. For example, to rotate a function {{math|1=''y'' = ''f''(''x'')}} around the {{mvar|x}}-axis starting from the top of the {{mvar|xz}}-plane, parameterize it as\n:<math>\\vec r(u,v)=\\langle u,f(u)\\sin v,f(u)\\cos v\\rangle</math>\nfor {{math|1=''u'' = ''x''}} and {{math|''v'' ∈ [0,2π]}}.\n\n==Geodesics on a surface of revolution==\n[[Meridian (geography)|Meridian]]s are always geodesics on a surface of revolution. Other geodesics are governed by [[Clairaut's relation]].<ref>Pressley, Andrew. “Chapter 9 - Geodesics.” ''Elementary Differential Geometry'', 2nd ed., Springer, London, 2012, pp. 227–230.</ref>\n\n==Toroids==\n{{main article|Toroid}}\n[[File:Toroid by Zureks.svg|thumb|A toroid generated from a square]]\nA surface of revolution with a hole in, where the axis of revolution does not intersect the surface, is called a toroid.<ref>{{MathWorld|Toroid|Toroid}}</ref> For example, when a rectangle is rotated around an axis parallel to one of its edges, then a hollow square-section ring is produced. If the revolved figure is a [[circle]], then the object is called a [[torus]].\n\n==Applications of surfaces of revolution==\nThe use of surfaces of revolution is essential in many fields in physics and engineering. When certain objects are designed digitally, revolutions like these can be used to determine surface area without the use of measuring the length and radius of the object being designed.\n\n==See also==\n* [[Channel surface]], a generalisation of a surface of revolution\n* [[Gabriel's Horn]]\n* [[Liouville surface]], another generalization of a surface of revolution\n* [[Solid of revolution]]\n* [[Surface integral]]\n* [[Generalized helicoid]]\n* [[Translation surface (differential geometry)]]\n\n==References==\n<references/>\n\n==External links==\n*{{MathWorld|title=Surface of Revolution|urlname=SurfaceofRevolution}}\n*{{cite web|url=http://www.mathcurve.com/surfaces/revolution/revolution.shtml |title=Surface de révolution |website=Encyclopédie des Formes Mathématiques Remarquables |language=French}}\n\n{{DEFAULTSORT:Surface Of Revolution}}\n[[Category:Integral calculus]]\n[[Category:Surfaces]]"
    },
    {
      "title": "Tangent half-angle substitution",
      "url": "https://en.wikipedia.org/wiki/Tangent_half-angle_substitution",
      "text": "[[File:WeierstrassSubstitution.svg|thumb|right|400px|The Weierstrass substitution, here illustrated as [[stereographic projection]] of the circle.]]\nIn [[integral calculus]], the '''tangent half-angle substitution''' is a [[integration by substitution|substitution]] used for finding [[antiderivative]]s, and hence definite integrals, of [[rational function]]s of [[trigonometric function]]s.  [[Without loss of generality|No generality is lost]] by taking these to be rational functions of the sine and cosine.  [[Michael Spivak]] wrote that \"The world's sneakiest substitution is undoubtedly\" this technique.<ref>Michael Spivak, ''Calculus'', [[Cambridge University Press]], 2006, pages 382–383.</ref>\n\n== Euler and Weierstrass ==\n\nVarious books call this the '''Weierstrass substitution''', after [[Karl Weierstrass]] (1815&nbsp;–&nbsp;1897), without citing any occurrence of the substitution in Weierstrass' writings,<ref>Gerald L. Bradley and Karl J. Smith, ''Calculus'', Prentice Hall, 1995, pages 462, 465, 466</ref><ref>Christof Teuscher, ''Alan Turing: Life and Legacy of a Great Thinker'', Springer, 2004, pages 105–6</ref><ref>James Stewart, ''Calculus: Early Transcendentals'', Brooks/Cole, Apr 1, 1991, page 436</ref> but the technique appears well before Weierstrass was born, in the work of [[Leonhard Euler]] (1707&nbsp;–&nbsp;1783).<ref>Leonhard Euler, ''Institutiionum calculi integralis volumen primum'', 1768, E342, Caput V, paragraph 261. See [http://www.eulerarchive.org/ http://www.eulerarchive.org/]</ref>\n\n== The substitution ==\n[[File:Stereo.Weierstrass.svg|350px|thumb|right|How the Weierstrass substitution is related to the [[stereographic projection]].]]\nOne starts with the problem of finding an [[antiderivative]] of a rational function of the sine and cosine; and replaces sin&nbsp;''x'', cos&nbsp;''x'', and the [[Differential (mathematics)|differential]]&nbsp;''dx'' with rational functions of a variable&nbsp;''t'' and the product of a rational function of ''t'' with the differential&nbsp;''dt'', as follows:<ref>James Stewart, ''Calculus: Early Transcendentals'', Brooks/Cole, 1991, page 439</ref>\n\n:<math>\\begin{align}\n\\sin x &= \\frac{2t}{1 + t^2} \\\\[6pt]\n\\cos x &= \\frac{1 - t^2}{1 + t^2} \\\\[6pt]\ndx &= \\frac{2}{1 + t^2}\\,dt.\n\\end{align}</math>\n\n=== Derivation ===\n\nLet\n:<math>t = \\tan\\frac{x}{2}.</math>\n\nBy the [[double-angle formula]] for the sine function,\n:<math>\\begin{align}\n\\sin x &=2\\sin\\frac{x}{2}\\cos\\frac{x}{2} \\\\[6pt]\n&= 2t\\cos^2\\frac{x}{2}\\\\[6pt]\n&= \\frac{2t}{\\sec^2\\frac{x}{2}}\\\\[6pt]\n&= \\frac{2t}{1+t^2}.\n\\end{align}</math>\n\nBy the [[double-angle formula]] for the cosine function,\n:<math>\\begin{align}\n\\cos x &=1-2\\sin^2\\frac{x}{2} \\\\[6pt]\n&= 1-2t^2\\cos^2\\frac{x}{2} \\\\[6pt]\n&= 1-\\frac{2t^2}{\\sec^2\\frac{x}{2}} \\\\[6pt]\n&= 1-\\frac{2t^2}{1+t^2} \\\\[6pt]\n&= \\frac{1-t^2}{1+t^2}.\n\\end{align}</math>\n\nOne has \n:<math>dx = \\frac{2}{1+t^2}\\,dt,</math>\nsince\n:<math>\\begin{align}\n\\frac{dt}{dx} &= \\frac{1}{2}\\sec^2\\frac{x}{2} \\\\[6pt]\n&= \\frac{1+t^2}{2}.\n\\end{align}</math>\n\n== Examples ==\n[[File:Weierstrass substitution.png|thumb|right|400px|The tangent half-angle formula relates an angle to the slope of a line.]]\n\n=== First example ===\n:<math>\\begin{align}\n\\int\\csc x\\,dx&=\\int\\frac{dx}{\\sin x} \\\\\n&=\\int \\left(\\frac{1 + t^2}{2t}\\right) \\left(\\frac{2}{1 + t^2}\\,dt\\right)  & t &= \\tan\\frac{x}{2} \\\\\n&=\\int\\frac{dt}{t} \\\\\n&=\\ln t + C \\\\\n&=\\ln\\tan\\frac{x}{2} + C.\n\\end{align}</math>\n\n=== Second example: a definite integral ===\n:<math>\\begin{align}\n\\int_0^{2\\pi}\\frac{dx}{2+\\cos x}\n&= \\int_0^\\pi \\frac{dx}{2+\\cos x} + \\int_\\pi^{2\\pi} \\frac{dx}{2+\\cos x} \\\\\n&=\\int_0^\\infty \\frac{2\\,dt}{3 + t^2} + \\int_{-\\infty}^0 \\frac{2\\,dt}{3 + t^2} & t &= \\tan\\frac{x}{2} \\\\\n&=\\int_{-\\infty}^{\\infty} \\frac{2\\,dt}{3+t^2} \\\\\n&=\\frac{2}{\\sqrt 3}\\int_{-\\infty}^{\\infty} \\frac{du}{1+u^2} & t &= u\\sqrt 3 \\\\\n&=\\frac{2\\pi}{\\sqrt 3}.\n\\end{align}</math>\nIn the first line, one does not simply substitute <math>t=0</math> for both [[limits of integration]]. The [[Mathematical singularity|singularity]] (in this case, a [[Asymptote#Vertical asymptotes|vertical asymptote]]) of <math>t=\\tan\\frac{x}{2}</math> at <math>x=\\pi</math> must be taken into account.\n\n===Third example===\n:<math>\\begin{align} \\int \\frac{dx}{a\\cos x + b\\sin x +c} &= \\int \\frac{2dt}{a(1-t^2) + 2bt + c(t^2+1)} \\\\ \n&= \\int \\frac{2dt}{(c-a)t^2 +2bt+a+c} \\\\ \n&= \\frac{2}{\\sqrt{c^2-(a^2+b^2)}} \\arctan \\frac{(c-a)\\tan \\frac{x}{2} + b}{\\sqrt{c^2-(a^2+b^2)}} + C\n\\end{align} </math>\nIf <math> 4E = 4(c-a)(c+a) - (2b)^2 = 4(c^2-(a^2+b^2))>0</math>.\n\n== Geometry ==\n[[File:Weierstrass.substitution.svg|400px|thumb|right|The Weierstrass substitution parametrizes the [[unit circle]] centered at&nbsp;(0,&nbsp;0).  Instead of +&infin; and &minus;&infin;, we have only one &infin;, at both ends of the real line.  That is often appropriate when dealing with rational functions and with trigonometric functions.  (This is the [[one-point compactification]] of the line.)]]\n\nAs ''x'' varies, the point (cos&nbsp;''x'',&nbsp;sin&nbsp;''x'') winds repeatedly around the [[unit circle]] centered at&nbsp;(0,&nbsp;0).  The point\n\n:<math>\\left(\\frac{1-t^2}{1+t^2}, \\frac{2t}{1+t^2}\\right)</math>\n\ngoes only once around the circle as ''t'' goes from &minus;&infin; to&nbsp;+&infin;, and never reaches the point&nbsp;(&minus;1,&nbsp;0), which is approached as a limit as ''t'' approaches&nbsp;±&infin;.  As ''t'' goes from &minus;&infin; to &minus;1, the point determined by ''t'' goes through the part of the circle in the third quadrant, from (&minus;1,&nbsp;0) to&nbsp;(0,&nbsp;&minus;1).  As ''t'' goes from &minus;1 to&nbsp;0, the point follows the part of the circle in the fourth quadrant from (0,&nbsp;&minus;1) to&nbsp;(1,&nbsp;0).  As ''t'' goes from 0 to 1, the point follows the part of the circle in the first quadrant from (1,&nbsp;0) to&nbsp;(0,&nbsp;1).  Finally, as ''t'' goes from 1 to&nbsp;+&infin;, the point follows the part of the circle in the second quadrant from (0,&nbsp;1) to&nbsp;(&minus;1,&nbsp;0).\n\nHere is another geometric point of view.  Draw the unit circle, and let ''P'' be the point {{nowrap|(&minus;1, 0)}}.  A line through ''P'' (except the vertical line) is determined by its slope.  Furthermore, each of the lines (except the vertical line) intersects the unit circle in exactly two points, one of which is ''P''.  This determines a function from points on the unit circle to slopes.  The trigonometric functions determine a function from angles to points on the unit circle, and by combining these two functions we have a function from angles to slopes.\n\n==Hyperbolic functions==\n\nAs with other properties shared between the trigonometric functions and the hyperbolic functions, it is possible to use [[Tangent half-angle formula#Hyperbolic identities|hyperbolic identities]] to construct a similar form of the substitution:\n\n:<math>\\sinh x = \\frac{2t}{1 - t^2},</math>\n:<math>\\cosh x = \\frac{1 + t^2}{1 - t^2},</math>\n:<math>\\tanh x = \\frac{2t}{1 + t^2},</math>\n:<math>dx = \\frac{2}{1- t^2}\\,dt.</math>\n\n==See also==\n*[[Rational curve]]\n*[[Stereographic projection]]\n*[[Tangent half-angle formula]]\n*[[Trigonometric substitution]]\n\n== Notes and references ==\n{{reflist}}\n\n== External links ==\n* [http://planetmath.org/encyclopedia/WeierstrassSubstitutionFormulas.html Weierstrass substitution formulas] at [[PlanetMath]]\n\n[[Category:Integral calculus]]"
    },
    {
      "title": "Time evolution of integrals",
      "url": "https://en.wikipedia.org/wiki/Time_evolution_of_integrals",
      "text": "{{Short description|Change of time of the value of an integral}}\n{{context|date=July 2012}}\n\nIn many applications, one needs to calculate the [[derivative|rate of change]] of a [[volume integral|volume]] or [[surface integral]] whose domain of [[integral|integration]], as well as the [[integrand]], are [[Function (mathematics)|functions]] of a particular parameter. In physical applications, that parameter is frequently [[time]] ''t''.\n\n==Introduction==\nThe rate of change of one-dimensional integrals with sufficiently [[smooth function|smooth]] integrands, is governed by this [[Differentiation under the integral sign|extension]] of the [[fundamental theorem of calculus]]:\n\n:<math>\\frac{d}{dt}\\int_{a\\left( t\\right) }^{b\\left( t\\right) }f\\left( t,x\\right) dx=  \\int_{a\\left( t\\right) }^{b\\left( t\\right) }\\frac{\\partial f\\left( t,x\\right) }{\\partial t}dx+f\\left( t,b\\left( t\\right) \\right) b^{\\prime }\\left( t\\right) -f\\left( t,a\\left( t\\right) \\right) a^{\\prime }\\left( t\\right)</math>\n\nThe [[calculus of moving surfaces]]<ref name=\"Grinfeld1\" >Grinfeld, P. (2010). \"Hamiltonian Dynamic Equations for Fluid Films\". Studies in Applied Mathematics. {{doi|10.1111/j.1467-9590.2010.00485.x}}. {{ISSN|0022-2526}}.</ref> provides analogous [[formulas]] for volume integrals over [[Euclidean space|Euclidean domains]], and surface integrals over [[differential geometry of surfaces]], curved surfaces, including integrals over curved surfaces with moving contour [[Boundary (topology)|boundaries]].\n\n==Volume integrals==\n\nLet ''t'' be a time-like [[parameter]] and consider a time-dependent [[Domain of a function|domain]] &Omega; with a smooth [[Surface (topology)|surface]] boundary ''S''. Let ''F'' be a time-dependent [[Invariant (mathematics)|invariant]] field defined in the interior of &Omega;. Then the rate of change of the [[integral]] <math>\\int_\\Omega F \\, d\\Omega </math>\n\nis governed by the following law:<ref name=\"Grinfeld1\" />\n\n: <math> \\frac{d}{dt} \\int_\\Omega F \\, d\\Omega =\\int_\\Omega \\frac{\\partial F}{\\partial t} \\, d\\Omega + \\int_S  CF \\, dS</math>\n\nwhere ''C'' is the [[The Calculus of Moving Surfaces|velocity of the interface]]. The velocity of the interface ''C'' is the fundamental concept in the [[calculus of moving surfaces]]. In the above equation, ''C'' must be expressed with respect to the exterior [[Surface normal|normal]]. This law can be considered as the generalization of the [[fundamental theorem of calculus]].\n\n==Surface integrals==\n\nA related law governs the [[Derivative|rate of change]] of the [[surface integral]]\n\n: <math> \\int_S F \\, dS </math>\n\nThe law reads\n\n: <math> \\frac{d}{dt } \\int_S F \\, dS = \\int_S \\frac{\\delta F}{\\delta t} \\, dS - \\int_S CB^\\alpha_\\alpha F \\, dS</math>\n\nwhere the <math>{\\delta}/{\\delta} t</math>-[[derivative]] is the fundamental [[operator (mathematics)|operator]] in the [[calculus of moving surfaces]], originally proposed by [[Jacques Hadamard]]. <math>B^\\alpha _\\alpha</math> is the trace of the [[Curvature#Mean curvature|mean curvature tensor]]. In this law, ''C'' need not be expression with respect to the exterior normal, as long as the choice of the normal is consistent for ''C'' and <math>B^\\alpha_\\alpha</math>. The first term in the above equation captures the rate of change in ''F'' while the second corrects for expanding or shrinking area. The fact that mean curvature represents the rate of change in area follows from applying the above equation to <math>F\\equiv 1</math> since <math>\\int_S \\, dS </math> is area:\n\n: <math> \\frac{d}{dt} \\int_S  \\, dS = -\\int_S CB^\\alpha_\\alpha \\, dS</math>\n\nThe above equation shows that mean curvature <math>B^\\alpha_\\alpha</math> can be appropriately called the ''shape gradient'' of area. An evolution governed by\n\n:<math>C\\equiv B^\\alpha_\\alpha</math>\n\nis the popular [[mean curvature flow]] and represents [[steepest descent]] with respect to area. Note that for a [[sphere]] of radius ''R'', \n<math>B^\\alpha_\\alpha = -2/R</math>, and for a [[circle]] of radius ''R'', \n<math>B^\\alpha_\\alpha = -1/R</math>\nwith respect to the exterior normal.\n\n==Surface integrals with moving contour boundaries==\n[[Image:SurfaceIntegralLawExplained.png|400px|right|thumb|Illustration for the law for surface integrals with a moving contour. Change in area comes from two sources:\nexpansion by curvature <math>CB^\\alpha_\\alpha dt</math> and expansion by annexation <math>cdt</math>.]]\n\nSuppose that ''S'' is a moving surface with a moving contour &gamma;. Suppose that the velocity of the contour &gamma; with respect to ''S'' is ''c''. Then the rate of change of the time dependent integral:\n\n: <math>\\int_S F \\, dS</math>\n\nis\n\n: <math> \\frac{d}{dt} \\int_S F \\, dS = \\int_S \\frac{\\delta F}{\\delta t} \\, dS - \\int_S CB_\\alpha^\\alpha F \\, dS + \\int_\\gamma  c \\, d\\gamma </math>\n\nThe last term captures the change in area due to annexation, as the figure on the right illustrates.\n\n==References==\n{{reflist}}\n\n[[Category:Differential calculus]]\n[[Category:Integral calculus]]"
    },
    {
      "title": "Tonelli–Hobson test",
      "url": "https://en.wikipedia.org/wiki/Tonelli%E2%80%93Hobson_test",
      "text": "{{unreferenced|date=July 2013}}\nIn [[mathematics]], the '''Tonelli–Hobson test''' gives sufficient criteria for a function ''&fnof;'' on '''R'''<sup>2</sup> to be an [[integrable function]]. It is often used to establish that [[Fubini's theorem]] may be applied to ''&fnof;''. It is named for [[Leonida Tonelli]] and [[E. W. Hobson]].\n\nMore precisely, the Tonelli–Hobson test states that if ''&fnof;'' is a [[real-valued function|real-valued]] [[measurable function]] on '''R'''<sup>2</sup>, and either of the two [[iterated integral]]s\n\n:<math>\\int_\\mathbb{R}\\left(\\int_\\mathbb{R}|f(x,y)|\\,dx\\right)\\, dy</math>\n\nor\n\n:<math>\\int_\\mathbb{R}\\left(\\int_\\mathbb{R}|f(x,y)|\\,dy\\right)\\, dx</math>\n\nis finite, then ''&fnof;'' is [[Lebesgue-integrable]] on '''R'''<sup>2</sup>.\n\n{{DEFAULTSORT:Tonelli-Hobson test}}\n[[Category:Integral calculus]]\n[[Category:Theorems in analysis]]\n\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Volume element",
      "url": "https://en.wikipedia.org/wiki/Volume_element",
      "text": "In [[mathematics]], a '''volume element''' provides a means for [[Integral|integrating]] a [[function (mathematics)|function]] with respect to [[volume]] in various coordinate systems such as [[Spherical coordinate system|spherical coordinates]] and [[Cylindrical coordinate system|cylindrical coordinates]].  Thus a volume element is an expression of the form\n:<math>dV = \\rho(u_1,u_2,u_3)\\,du_1\\,du_2\\,du_3</math>\nwhere the <math>u_i</math> are the coordinates, so that the volume of any set <math>B</math> can be computed by\n:<math>\\operatorname{Volume}(B) = \\int_B \\rho(u_1,u_2,u_3)\\,du_1\\,du_2\\,du_3.</math>\nFor example, in spherical coordinates <math>dV = u_1^2\\sin u_2\\,du_1\\,du_2\\,du_3</math>, and so <math>\\rho = u_1^2\\sin u_2</math>.\n\nThe notion of a volume element is not limited to three dimensions: in two dimensions it is often known as the '''area element''', and in this setting it is useful for doing [[surface integral]]s.  Under changes of coordinates, the volume element changes by the absolute value of the [[Jacobian determinant]] of the coordinate transformation (by the [[integration by substitution#Substitution for multiple variables|change of variables formula]]).  This fact allows volume elements to be defined as a kind of [[measure (mathematics)|measure]] on a [[manifold]].  On an [[orientability|orientable]] [[differentiable manifold]], a volume element typically arises from a [[volume form]]: a top degree [[differential form]].  On a non-orientable manifold, the volume element is typically the [[absolute value]] of a (locally defined) volume form: it defines a [[density on a manifold|1-density]].\n\n==Volume element in Euclidean space==\nIn [[Euclidean space]], the volume element is given by the product of the differentials of the Cartesian coordinates\n:<math>dV = dx\\,dy\\,dz.</math>\nIn different coordinate systems of the form <math>x=x(u_1,u_2,u_3), y=y(u_1,u_2,u_3), z=z(u_1,u_2,u_3)</math>, the volume element [[integration by substitution|changes by the Jacobian]] of the coordinate change:\n:<math>dV = \\left|\\frac{\\partial (x,y,z)}{\\partial (u_1,u_2,u_3)}\\right|\\,du_1\\,du_2\\,du_3.</math>\nFor example, in spherical coordinates (mathematical convention) \n:<math>\\begin{align}\nx&=\\rho\\cos\\theta\\sin\\phi\\\\\ny&=\\rho\\sin\\theta\\sin\\phi\\\\\nz&=\\rho\\cos\\phi\n\\end{align}\n</math>\nthe Jacobian is\n:<math>\\left |\\frac{\\partial(x,y,z)}{\\partial (\\rho,\\theta,\\phi)}\\right| = \\rho^2\\sin\\phi</math>\nso that\n:<math>dV = \\rho^2\\sin\\phi\\,d\\rho\\,d\\theta\\,d\\phi.</math>\nThis can be seen as a special case of the fact that differential forms transform through a pullback <math>F^*</math> as\n\n:<math> F^*(u \\; dy^1 \\wedge \\cdots \\wedge dy^n) = (u \\circ F) \\det \\left(\\frac{\\partial F^j}{\\partial x^i}\\right) dx^1 \\wedge \\cdots \\wedge dx^n </math>\n\n== Volume element of a linear subspace ==\nConsider the [[linear subspace]] of the ''n''-dimensional [[Euclidean space]] '''R'''<sup>''n''</sup> that is spanned by a collection of [[linearly independent]] vectors\n:<math>X_1,\\dots,X_k.</math>\nTo find the volume element of the subspace, it is useful to know the fact from linear algebra that the volume of the parallelepiped spanned by the <math>X_i</math> is the square root of the [[determinant]] of the [[Gramian matrix]] of the <math>X_i</math>:\n:<math>\\sqrt{\\det(X_i\\cdot X_j)_{i,j=1\\dots k}}.</math>\n\nAny point ''p'' in the subspace can be given coordinates <math>(u_1,u_2,\\dots,u_k)</math> such that\n:<math>p = u_1X_1 + \\cdots + u_kX_k.</math>\nAt a point ''p'', if we form a small parallelepiped with sides <math>du_i</math>, then the volume of that parallelepiped is the square root of the determinant of the Grammian matrix\n:<math>\\sqrt{\\det\\left((du_i X_i)\\cdot (du_j X_j)\\right)_{i,j=1\\dots k}} = \\sqrt{\\det(X_i\\cdot X_j)_{i,j=1\\dots k}}\\; du_1\\,du_2\\,\\cdots\\,du_k.</math>\nThis therefore defines the volume form in the linear subspace.\n\n==Volume element of manifolds==\n{{See also|Riemannian volume form}}\nOn an ''oriented'' [[Riemannian manifold]] of dimension ''n'', the volume element is a volume form equal to the [[Hodge dual]] of the unit constant function, <math>f(x) = 1</math>:\n:<math>\\omega = \\star 1</math> .\nEquivalently, the volume element is precisely the [[Levi-Civita tensor]] <math>\\epsilon</math>.<ref>Carroll, Sean. ''Spacetime and Geometry''. Addison Wesley, 2004, p. 90</ref> In coordinates,\n:<math>\\omega = \\epsilon =\\sqrt{|\\det g|}\\, dx^1 \\wedge \\cdots  \\wedge dx^n</math>\nwhere <math>\\det g</math> is the [[determinant]] of the [[metric tensor]] ''g'' written in the coordinate system.\n\n=== Area element of a surface ===\nA simple example of a volume element can be explored by considering a two-dimensional surface embedded in ''n''-dimensional [[Euclidean space]].  Such a volume element is sometimes called an ''area element''.  Consider a subset <math>U \\subset \\mathbf{R}^2</math> and a mapping function\n\n:<math>\\varphi:U\\to \\mathbf{R}^n</math>\n\nthus defining a surface embedded in <math>\\mathbf{R}^n</math>.  In two dimensions, volume is just area, and a volume element gives a way to determine the area of parts of the surface.  Thus a volume element is an expression of the form\n\n:<math>f(u_1,u_2)\\,du_1\\,du_2</math>\n\nthat allows one to compute the area of a set ''B'' lying on the surface by computing the integral\n\n:<math>\\operatorname{Area}(B) = \\int_B f(u_1,u_2)\\,du_1\\,du_2.</math>\n\nHere we will find the volume element on the surface that defines area in the usual sense.  The [[Jacobian matrix]] of the mapping is\n\n:<math>\\lambda_{ij}=\\frac{\\partial \\varphi_i} {\\partial u_j}</math>\n\nwith index ''i'' running from 1 to ''n'', and ''j'' running from 1 to 2. The Euclidean [[metric (mathematics)|metric]] in the ''n''-dimensional space induces a metric <math>g=\\lambda^T\\lambda</math> on the set ''U'', with matrix elements\n\n:<math>g_{ij}=\\sum_{k=1}^n \\lambda_{ki} \\lambda_{kj}\n= \\sum_{k=1}^n\n\\frac{\\partial \\varphi_k} {\\partial u_i}\n\\frac{\\partial \\varphi_k} {\\partial u_j}.\n</math>\n\nThe [[determinant]] of the metric is given by\n\n:<math>\\det g = \\left|\n\\frac{\\partial \\varphi} {\\partial u_1} \\wedge\n\\frac{\\partial \\varphi} {\\partial u_2}\n\\right|^2 = \\det (\\lambda^T \\lambda)</math>\n\nFor a regular surface, this determinant is non-vanishing; equivalently, the Jacobian matrix has rank 2.\n\nNow consider a change of coordinates on ''U'', given by a [[diffeomorphism]]\n\n:<math>f \\colon U\\to U , \\,\\!</math>\n\nso that the coordinates <math>(u_1,u_2)</math> are given in terms of <math>(v_1,v_2)</math> by <math>(u_1,u_2)= f(v_1,v_2)</math>.  The Jacobian matrix of this transformation is given by\n\n:<math>F_{ij}= \\frac{\\partial f_i} {\\partial v_j}.</math>\n\nIn the new coordinates, we have\n\n:<math>\\frac{\\partial \\varphi_i} {\\partial v_j} =\n\\sum_{k=1}^2\n\\frac{\\partial \\varphi_i} {\\partial u_k}\n\\frac{\\partial f_k} {\\partial v_j}\n</math>\n\nand so the metric transforms as\n\n:<math>\\tilde{g} = F^T g F </math>\n\nwhere <math>\\tilde{g}</math> is the pullback metric in the ''v'' coordinate system.  The determinant is\n\n:<math>\\det \\tilde{g} = \\det g (\\det F)^2. </math>\n\nGiven the above construction, it should now be straightforward to understand how the volume element is invariant under an orientation-preserving change of coordinates. \n\nIn two dimensions, the volume is just the area. The area of a subset <math>B\\subset U</math> is given by the integral\n\n:<math>\\begin{align}\n \\mbox{Area}(B)\n &= \\iint_B \\sqrt{\\det g}\\; du_1\\; du_2 \\\\\n &= \\iint_B \\sqrt{\\det g} \\;|\\det F| \\;dv_1 \\;dv_2 \\\\\n &= \\iint_B \\sqrt{\\det \\tilde{g}} \\;dv_1 \\;dv_2.\n\\end{align}</math>\n\nThus, in either coordinate system, the volume element takes the same expression: the expression of the volume element is invariant under a change of coordinates.\n\nNote that there was nothing particular to two dimensions in the above presentation; the above trivially generalizes to arbitrary dimensions.\n\n===Example: Sphere===\nFor example, consider the sphere with radius ''r'' centered at the origin in '''R'''<sup>3</sup>.  This can be parametrized using [[spherical coordinates]] with the map\n:<math>\\phi(u_1,u_2) = (r\\cos u_1\\sin u_2,r\\sin u_1\\sin u_2,r\\cos u_2).</math>\nThen\n:<math>g = \\begin{pmatrix}r^2\\sin^2u_2 & 0 \\\\ 0 & r^2\\end{pmatrix},</math>\nand the area element is\n:<math> \\omega = \\sqrt{\\det g}\\; du_1 du_2 = r^2\\sin u_2\\, du_1 du_2.</math>\n\n==See also==\n* [[Cylindrical coordinate system#Line and volume elements]]\n* [[Spherical coordinate system#Integration and differentiation in spherical coordinates]]\n* [[Surface integral]]\n* [[Volume integral]]\n\n==References==\n* {{Citation|last1=Besse|first1=Arthur L.|title=Einstein manifolds|publisher=[[Springer-Verlag]]|location=Berlin, New York|series=Ergebnisse der Mathematik und ihrer Grenzgebiete (3) [Results in Mathematics and Related Areas (3)], vol. 10|isbn=978-3-540-15279-8|year=1987|pages=xii+510}}\n{{reflist}}\n\n[[Category:Measure theory]]\n[[Category:Integral calculus]]\n[[Category:Multivariable calculus]]"
    },
    {
      "title": "Bochner integral",
      "url": "https://en.wikipedia.org/wiki/Bochner_integral",
      "text": "In [[mathematics]], the '''Bochner integral''', named for [[Salomon Bochner]], extends the definition of [[Lebesgue integral]] to functions that take values in a [[Banach space]], as the limit of integrals of [[simple function]]s.\n\n==Definition==\n\nLet (''X'',&nbsp;Σ,&nbsp;μ) be a [[measure space]] and ''B'' a Banach space.  The Bochner integral is defined in much the same way as the Lebesgue integral.  First, a simple function is any finite sum of the form\n\n:<math>s(x) = \\sum_{i=1}^n \\chi_{E_i}(x) b_i</math>\n\nwhere the ''E''<sub>''i''</sub> are disjoint members of the σ-algebra Σ, the ''b''<sub>''i''</sub> are distinct elements of ''B'', and χ<sub>E</sub> is the [[Indicator function|characteristic function]] of ''E''.  If ''μ''(''E''<sub>''i''</sub>) is finite whenever ''b''<sub>''i''</sub>&nbsp;≠ 0, then the simple function is '''integrable''', and the integral is then defined by\n\n:<math>\\int_X \\left[\\sum_{i=1}^n \\chi_{E_i}(x) b_i\\right]\\, d\\mu = \\sum_{i=1}^n \\mu(E_i) b_i</math>\n\nexactly as it is for the ordinary Lebesgue integral.\n\nA measurable function ƒ&nbsp;: ''X''&nbsp;→ ''B'' is '''Bochner integrable''' if there exists a sequence of integrable simple functions ''s''<sub>''n''</sub> such that\n\n:<math>\\lim_{n\\to\\infty}\\int_X \\|f-s_n\\|_B\\,d\\mu = 0,</math>\n\nwhere the integral on the left-hand side is an ordinary Lebesgue integral.\n\nIn this case, the '''Bochner integral''' is defined by\n\n:<math>\\int_X f\\, d\\mu = \\lim_{n\\to\\infty}\\int_X s_n\\, d\\mu.</math>\n\nIt can be shown that a function is Bochner integrable if and only if it lies in the [[Bochner space]] <math>L^1</math>.\n\n==Properties==\n\nMany of the familiar properties of the Lebesgue integral continue to hold for the Bochner integral.  Particularly useful is Bochner's criterion for integrability, which states that if (''X'',&nbsp;Σ,&nbsp;μ) is a measure space, then a Bochner-measurable function ''ƒ''&nbsp;:&nbsp;''X''&nbsp;→&nbsp;''B'' is Bochner integrable if and only if\n\n:<math>\\int_X \\|f\\|_B\\, d\\mu < \\infty.</math>\n\nA function ''ƒ''&nbsp;:&nbsp;''X''&nbsp;→&nbsp;''B''  is called Bochner-measurable if it is equal μ-almost everywhere to a function ''g'' taking values in a separable subspace ''B''<sub>0</sub> of ''B'', and such that the inverse image ''g''<sup>−1</sup>(''U'') of every open set ''U''  in ''B''  belongs to Σ.  Equivalently,  ''ƒ'' is limit μ-almost everywhere of a sequence of simple functions.\n\nIf <math>T</math> is a continuous linear operator, and <math>f</math> is Bochner-integrable, then <math>T f</math> is Bochner-integrable and integration and <math>T</math> may be interchanged:\n\n:<math> \\int_X T f d\\mu = T \\int_X f d\\mu.</math>\n\nThis also holds for closed operators, given that <math>T f</math> be itself integrable (which, via the criterion mentioned above is trivially true for bounded <math>T</math>).\n\nA version of the [[dominated convergence theorem]] also holds for the Bochner integral.  Specifically, if ''ƒ''<sub>''n''</sub>&nbsp;: ''X''&nbsp;→ ''B'' is a sequence of measurable functions on a complete measure space tending almost everywhere to a limit function ''ƒ'', and if\n\n:<math>\\|f_n(x)\\|_B\\le g(x)</math>\n\nfor almost every ''x''&nbsp;∈&nbsp;''X'', and ''g''&nbsp;∈ [[Lp space|''L''<sup>1</sup>(μ)]], then\n\n:<math>\\int_X \\|f-f_n\\|_B\\,d\\mu \\to 0</math>\n\nas ''n''&nbsp;→&nbsp;∞ and\n\n:<math>\\int_E f_n\\,d\\mu \\to \\int_E f\\,d\\mu</math>\n\nfor all ''E''&nbsp;∈&nbsp;Σ.\n\nIf ''ƒ'' is Bochner integrable, then the inequality\n\n:<math>\\left\\|\\int_Ef\\,d\\mu\\right\\|_B \\le \\int_E \\|f\\|_B\\,d\\mu</math>\n\nholds for all ''E''&nbsp;∈&nbsp;Σ.  In particular, the set function\n\n:<math>E\\mapsto \\int_E f\\, d\\mu</math>\n\ndefines a countably-additive ''B''-valued [[vector measure]] on ''X'' which is [[absolutely continuous]] with respect to μ.\n\n==Radon–Nikodym property==\nAn important fact about the Bochner integral is that the [[Radon–Nikodym theorem]] ''fails'' to hold in general.  This results in an important property of Banach spaces known as the Radon–Nikodym property.  Specifically, if μ is a measure on (''X'',&nbsp;Σ), then ''B'' has the Radon–Nikodym property with respect to μ if, for every countably-additive [[vector measure]] <math>\\gamma</math> on (''X'',&nbsp;Σ) with values in ''B'' which has [[Vector measure#The variation of a vector measure|bounded variation]] and is absolutely continuous with respect to μ, there is a μ-integrable function ''g'' : ''X'' → ''B'' such that\n\n:<math>\\gamma(E) = \\int_E g\\, d\\mu </math>\n\nfor every measurable set ''E''&nbsp;∈ Σ.<ref>{{cite journal |url=http://www.emis.de/journals/DM/vXI1/art5.pdf |title=The Radon–Nikodym Theorem for Reflexive Banach Spaces |first=Diómedes |last=Bárcenas |journal=Divulgaciones Matemáticas |volume=11 |issue=1 |year=2003 |pages=55–59 [pp. 55–56] }}</ref>\n\nThe Banach space ''B'' has the '''Radon–Nikodym property''' if ''B'' has the Radon–Nikodym property with respect to every finite measure.  It is known that the space [[Lp space|<math>l_1</math>]] has the Radon–Nikodym property, but [[c0 space|<math>c_0</math>]] and the spaces <math>L^{\\infty}(\\Omega)</math>, <math>L^{1}(\\Omega)</math>, for <math>\\Omega</math> an open bounded subset of <math>\\mathbb{R}^n</math>, and <math>C(K)</math>, for ''K'' an infinite compact space, do not. Spaces with Radon–Nikodym property include separable dual spaces (this is the [[Dunford–Pettis theorem]]) and [[reflexive space]]s, which include, in particular, [[Hilbert space]]s.\n\n==See also==\n* [[Bochner space]]\n* [[Pettis integral]]\n* [[Vector measure]]\n\n==References==\n{{reflist}}\n*{{citation|last=Bochner|first=Salomon|title=Integration von Funktionen, deren Werte die Elemente eines Vektorraumes sind|journal=[[Fundamenta Mathematicae]]|year=1933|volume=20|pages=262–276| url=http://matwbn.icm.edu.pl/ksiazki/fm/fm20/fm20127.pdf}}\n*{{citation|first=Donald|last=Cohn|title=Measure Theory|publisher=Springer|year=2013|isbn=978-1-4614-6955-1|doi=10.1007/978-1-4614-6956-8|series=Birkhäuser Advanced Texts Basler Lehrbücher}}\n*{{citation|first=Kôsaku|last=Yosida|title=Functional Analysis|volume=123|publisher=Springer|year=1980|isbn=978-3-540-58654-8|doi=10.1007/978-3-642-61859-8|series=Classics in Mathematics}}\n*{{citation|first=Joseph|last=Diestel|title=Sequences and Series in Banach Spaces|volume=92|publisher=Springer|year=1984|isbn=978-0-387-90859-5|doi=10.1007/978-1-4612-5200-9|series=Graduate Texts in Mathematics}}\n*{{citation|last1=Diestel|last2=Uhl|title=Vector measures|publisher=[[American Mathematical Society]]|year=1977|isbn=978-0-8218-1515-1|url=http://projecteuclid.org/euclid.bams/1183540941}}\n*{{Citation|last1=Hille|first1=Einar|first2=Ralph|last2=Phillips|title=Functional Analysis and Semi-Groups|publisher=[[American Mathematical Society]]|year=1957|isbn=978-0-8218-1031-6}}\n*{{citation|last=Lang|first=Serge|title=Real and Functional Analysis|year=1993|publisher=Springer|isbn=978-0387940014|edition=3rd}}\n*{{springer|title=Bochner integral|id=B/b016710|first=V. I.|last=Sobolev}}\n*{{springer|title=Vector measures|id=V/v096490|first=D.|last=van Dulst}}\n\n{{integral}}\n{{Functional Analysis}}\n\n[[Category:Definitions of mathematical integration]]\n[[Category:Topological vector spaces]]\n[[Category:Integral representations]]"
    },
    {
      "title": "Burkill integral",
      "url": "https://en.wikipedia.org/wiki/Burkill_integral",
      "text": "In mathematics, the '''Burkill integral''' is an integral introduced by {{harvs|txt|last=Burkill|authorlink=John Charles Burkill|year1=1924a|year2=1924b}} for calculating areas. It is a special case of the [[Kolmogorov integral]].\n\n==References==\n\n*{{Citation | last1=Burkill | first1=J. C. | title=Functions of intervals | doi=10.1112/plms/s2-22.1.275  | year=1924a | journal=Proceedings of the London Mathematical Society | issn=0024-6115 | volume=22 | pages=275–310}}\n*{{Citation | last1=Burkill | first1=J. C. | title=The expression of area as an integral | doi=10.1112/plms/s2-22.1.311  | year=1924b | journal= Proceedings of the London Mathematical Society | volume=22 | pages=311–336}}\n*{{eom|id=b/b017780|first=V.A. |last=Skvortsov}}\n\n{{integral}}\n[[Category:Definitions of mathematical integration]]\n\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Daniell integral",
      "url": "https://en.wikipedia.org/wiki/Daniell_integral",
      "text": "In [[mathematics]], the '''Daniell integral''' is a type of integration that generalizes the concept of more elementary versions such as the [[Riemann integral]] to which students are typically first introduced. One of the main difficulties with the traditional formulation of the [[Lebesgue integral]] is that it requires the initial development of a workable measure theory before any useful results for the integral can be obtained. However, an alternative approach is available, developed by {{harvs|txt|authorlink=Percy John Daniell |first= Percy J. |last=Daniell|year= 1918}} that does not suffer from this deficiency, and has a few significant advantages over the traditional formulation, especially as the integral is generalized into higher-dimensional spaces and further generalizations such as the [[Stieltjes integral]]. The basic idea involves the [[axiomatization]] of the integral.\n\n==Axioms==\nWe start by choosing a family <math>H</math> of bounded real functions (called ''elementary functions'') defined over some set <math>X</math>, that satisfies these two axioms:\n\n* <math>H</math> is a linear space with the usual operations of addition and scalar multiplication.\n* If a function <math>h(x)</math> is in <math>H</math>, so is its [[absolute value]] <math>|h(x)|</math>.\n\nIn addition, every function ''h'' in ''H'' is assigned a real number <math>Ih</math>, which is called the ''elementary integral'' of ''h'', satisfying these three axioms:\n\n* Linearity\n: If ''h'' and ''k'' are both in H, and <math>\\alpha</math> and <math>\\beta</math> are any two real numbers, then <math>I(\\alpha h + \\beta k) = \\alpha Ih + \\beta Ik</math>.\n* Nonnegativity\n: If <math>h(x) \\ge 0</math>, then <math>Ih \\ge 0</math>.\n* Continuity\n: If <math>h_n</math> is a nonincreasing sequence (i.e. <math>h_1 \\ge \\cdots \\ge h_k \\ge \\cdots</math>) of functions in <math>H</math> that converges to 0 for all <math>x</math> in <math>X</math>, then <math>Ih_n \\to 0</math>.\nor (more commonly)\n: If <math>h_n</math> is an increasing sequence (i.e. <math>h_1 \\le \\cdots \\le h_k \\le \\cdots</math>) of functions in <math>H</math> that converges to h for all <math>x</math> in <math>X</math>, then <math>Ih_n \\to Ih</math>.\n\nThat is, we define a continuous non-negative [[linear functional]] <math>I</math> over the space of elementary functions.\n\nThese elementary functions and their elementary integrals may be any set of functions and definitions of integrals over these functions which satisfy these axioms. The family of all [[step function]]s evidently satisfies the above axioms for elementary functions. Defining the elementary integral of the family of step functions as the (signed) area underneath a step function evidently satisfies the given axioms for an elementary integral. Applying the construction of the Daniell integral described further below using step functions as elementary functions produces a definition of an integral equivalent to the Lebesgue integral. Using the family of all [[continuous function]]s as the elementary functions and the traditional [[Riemann integral]] as the elementary integral is also possible, however, this will yield an integral that is also equivalent to Lebesgue's definition. Doing the same, but using the [[Riemann–Stieltjes integral]], along with an appropriate function of [[bounded variation]], gives a definition of integral equivalent to the [[Lebesgue–Stieltjes integral]].\n\nSets of [[measure zero]] may be defined in terms of elementary functions as follows. A set <math>Z</math> which is a subset of <math>X</math> is a set of measure zero if for any <math>\\epsilon > 0</math>, there exists a nondecreasing sequence of nonnegative elementary functions <math>h_p(x)</math> in ''H'' such that <math>Ih_p < \\epsilon</math> and <math>\n\\sup_p h_p(x) \\ge 1\n</math> on <math>Z</math>. \n\nA set is called a set of [[Full measure (mathematics)|full measure]] if its complement, relative to <math>X</math>, is a set of measure zero. We say that if some property holds at every point of a set of full measure (or equivalently everywhere except on a set of measure zero), it holds [[almost everywhere]].\n\n==Definition==\nAlthough the end result is the same, different authors construct the integral differently. A common approach is to start with defining a larger class of functions, based on our chosen elementary functions, the class <math>L^+</math>, which is the family of all functions that are the limit of a nondecreasing sequence <math>h_n</math> of elementary functions, such that the set of integrals <math>Ih_n</math> is bounded. The integral of a function <math>f</math> in <math>L^+</math> is defined as:\n\n:<math>If = \\lim_{n \\to \\infty} Ih_n</math>\n\nIt can be shown that this definition of the integral is well-defined, i.e. it does not depend on the choice of sequence <math>h_n</math>.\n\nHowever, the class <math>L^+</math> is in general not closed under subtraction and scalar multiplication by negative numbers; one needs to further extend it by defining a wider class of functions <math>L</math> with these properties.\n\nDaniell's (1918) method, described in the book by Royden, amounts to defining the upper integral of a general function <math>\\phi</math> by\n\n:<math>I^+\\phi = \\inf_f If</math>\n\nwhere the infimum is taken over all <math>f</math> in <math>L^+</math> with <math>f \\ge \\phi</math>. The lower integral is defined in a similar fashion or shortly as <math> I^-\\phi = -I^+(-\\phi)</math>. Finally <math>L</math> consists of those functions whose upper and lower integrals are finite and coincide, and\n\n:<math>\\int_X \\phi(x) dx = I^+\\phi = I^-\\phi.</math>\n\nAn alternative route, based on a discovery by Frederic Riesz, is taken in the book by Shilov and Gurevich and in the article in Encyclopedia of Mathematics. Here <math>L</math> consists of those functions <math>\\phi(x)</math> that can be represented on a set of full measure (defined in the previous section) as the difference <math>\\phi = f - g</math>, for some functions <math>f</math> and <math>g</math> in the class <math>L^+</math>. Then the integral of a function <math>\\phi(x)</math> can be defined as:\n\n:<math>\\int_X \\phi(x) dx = If - Ig\\,</math>\n\nAgain, it may be shown that this integral is well-defined, i.e. it does not depend on the decomposition of <math>\\phi</math> into <math>f</math> and <math>g</math>. This turns out to be equivalent to the original Daniell integral.\n\n==Properties==\nNearly all of the important theorems in the traditional theory of the Lebesgue integral, such as [[Lebesgue's dominated convergence theorem]], the [[Riesz–Fischer theorem]], [[Fatou's lemma]], and [[Fubini's theorem]] may also readily be proved using this construction.  Its properties are identical to the traditional Lebesgue integral.\n\n==Measurement==\nBecause of the natural correspondence between sets and functions, it is also possible to use the Daniell integral to construct a [[measure theory]]. If we take the [[Indicator function|characteristic function]] <math>\\chi(x)</math> of some set, then its integral may be taken as the measure of the set. This definition of measure based on the Daniell integral can be shown to be equivalent to the traditional [[Lebesgue measure]].\n\n==Advantages over the traditional formulation==\nThis method of constructing the general integral has a few advantages over the traditional method of Lebesgue, particularly in the field of [[functional analysis]]. The Lebesgue and Daniell constructions are equivalent, as pointed out above, if ordinary finite-valued step functions are chosen as elementary functions. However, as one tries to extend the definition of the integral into more complex domains (e.g. attempting to define the integral of a [[linear functional]]), one runs into practical difficulties using Lebesgue's construction that are alleviated with the Daniell approach.\n\nThe Polish mathematician [[Jan Mikusinski]] has made an alternative and more natural formulation of Daniell integration by using the notion of absolutely convergent series. His formulation works for \n[[Bochner integral]] (Lebesgue integral for mappings taking values in [[Banach space]]s). Mikusinski's lemma allows one to define integral without mentioning [[null set]]s. He also proved change of variables theorem for multiple integral for Bochner integrals and Fubini's theorem for Bochner integrals using Daniell integration. The book by Asplund and Bungart carries a lucid treatment of this approach for real valued functions. It also offers a proof of an abstract [[Radon–Nikodym theorem]] using [[Daniell–Mikusinski approach]].\n\n==See also==\n* [[Lebesgue integral]]\n* [[Riemann integral]]\n* [[Lebesgue–Stieltjes integration]]\n\n==References==\n*{{cite book |first=Robert B. |last=Ash |title=Real Analysis and Probability |chapter=The Interplay between Measure Theory and Topology |location=New York |publisher=Academic Press |year=1972 |pages=168–200 |isbn=0-12-065201-3 }}\n*{{Cite journal | last1=Daniell | first1=P. J. | title=A General Form of Integral | jstor=1967495 | series=Second Series | year=1918 | journal=[[Annals of Mathematics]] | volume=19 | issue=4 | pages= 279–294| doi = 10.2307/1967495 }}\n*{{cite book |first=Shelby J. |last=Haberman |chapter=Construction of Daniell Integrals |pages=199–263 |title=Advanced Statistics |location=New York |publisher=Springer |year=1996 |isbn=0-387-94717-5 |chapterurl=https://books.google.com/books?id=4fn5WXUe2EQC&pg=PA199 }}\n*{{cite book |last=Royden |first=H. L. |year=1988 |title=Real Analysis |edition=3rd |location=Englewood Cliffs |publisher=Prentice Hall |isbn=0-02-404151-3 |chapter=The Daniell Integral |pages=419&ndash;434 }}\n*{{cite book |last=Shilov |first=G. E. |last2=Gurevich |first2=B. L. |year=1978 |title=Integral, Measure, and Derivative: A Unified Approach |translator-last1=Silverman |translator-first1=Richard A. |publisher=Dover Publications |isbn=0-486-63519-8 }}\n*{{cite book |last=Asplund |first=Edgar |last2=Bungart |first2=Lutz |year=1966 |title=A First Course in Integration |location=New York |publisher=Holt, Rinehart and Winston |isbn= }}\n*{{eom|id=D/d030110|first=V. I. |last=Sobolev}}\n*{{cite book |last=Taylor |first=A. E. |origyear=1965 |year=1985 |title=General Theory of Functions and Integration |location= |publisher=Dover |isbn=0-486-64988-1 }}\n\n{{integral}}\n\n[[Category:Definitions of mathematical integration]]"
    },
    {
      "title": "Darboux integral",
      "url": "https://en.wikipedia.org/wiki/Darboux_integral",
      "text": "{{Refimprove|date=February 2013}}\nIn [[real analysis]], a branch of [[mathematics]], the '''Darboux integral''' is constructed using '''Darboux sums''' and is one possible definition of the [[integral]] of a function. Darboux integrals are equivalent to [[Riemann integral]]s, meaning that a function is Darboux-integrable if and only if it is Riemann-integrable, and the values of the two integrals, if they exist, are equal.<ref>{{cite book|author1=David J. Foulis|author2=Mustafa A. Munem|title=After Calculus: Analysis|url=https://books.google.com/books?id=kSMnAQAAIAAJ|year=1989|publisher=Dellen Publishing Company|isbn=978-0-02-339130-9|page=396}}</ref> The definition of the Darboux integral has the advantage of being easier to apply in computations or proofs than that of the Riemann integral. Consequently, introductory textbooks on calculus and real analysis often develop Riemann integration using the Darboux integral, rather than the true Riemann integral.<ref>{{Cite book|title=Calculus (3rd. edition)|last=Spivak|first=M.|publisher=Publish Or Perish, Inc.|year=1994|isbn=0-914098-89-6|location=Houston, TX|pages=253–255|via=}}</ref>  Moreover, the definition is readily extended to defining Riemann-Stieltjes integration.<ref>{{Cite book|title=Principles of Mathematical Analysis (3rd. edition)|last=Rudin|first=W.|publisher=McGraw-Hill|year=1976|isbn=007054235X|location=New York|pages=120–122|via=}}</ref>  Darboux integrals are named after their inventor, [[Gaston Darboux]].\n\n==Definition==\nThe definition of the Darboux integral considers '''upper and lower (Darboux) integrals''', which exist for any bounded real-valued function ''f'' on the interval [''a'',''b'']. The '''Darboux integral''' exists if and only if the upper and lower integrals are equal.  The upper and lower integrals are in turn the infimum and supremum, respectively, of '''upper and lower (Darboux) sums''' which over- and underestimate, respectively, the \"area under the curve.\" In particular, for a given partition of the interval of integration, the upper and lower sums add together the areas of rectangular slices whose heights are the supremum and infimum, respectively, of ''f'' in each subinterval of the partition.  These ideas are made precise below:\n\n===Darboux sums===\nA [[partition of an interval]] [''a'',''b''] is a finite sequence of values ''x''<sub>''i''</sub> such that\n\n:<math>a = x_0 < x_1 < \\cdots < x_n = b . \\,\\!</math>\n\nEach interval [''x''<sub>''i''&minus;1</sub>,''x''<sub>''i''</sub>] is called a ''subinterval'' of the partition.  Let ƒ:[''a'',''b'']→ℝ be a bounded function, and let\n\n:<math>P = (x_0, \\ldots, x_n) \\,\\!</math>\n\nbe a partition of [''a'',''b''].  Let\n\n:<math>\\begin{align}\n M_i = \\sup_{x\\in[x_{i-1},x_{i}]} f(x) , \\\\\n m_i = \\inf_{x\\in[x_{i-1},x_{i}]} f(x) .\n\\end{align}</math>\n\n[[Image:Darboux.svg|thumb|right|Lower (green) and upper (green plus lavender) Darboux sums for four subintervals]]\n\nThe '''upper Darboux sum''' of ƒ with respect to ''P'' is\n\n:<math>U_{f, P} = \\sum_{i=1}^n (x_{i}-x_{i-1}) M_i . \\,\\!</math>\n\nThe '''lower Darboux sum''' of ƒ with respect to ''P'' is\n\n:<math>L_{f, P} = \\sum_{i=1}^n (x_{i}-x_{i-1}) m_i . \\,\\!</math>\n\nThe lower and upper Darboux sums are often called the lower and upper sums.\n\n===Darboux integrals===\nThe '''upper Darboux integral''' of ƒ is\n\n:<math>U_f = \\inf\\{U_{f,P} \\colon P \\text{ is a partition of } [a,b]\\} . \\,\\!</math>\n\nThe '''lower Darboux integral''' of ƒ is\n\n:<math>L_f = \\sup\\{L_{f,P} \\colon P \\text{ is a partition of } [a,b]\\} . \\,\\!</math>\n\nIn some literature an integral symbol with an underline and overline represent the lower and upper Darboux integrals respectively.\n\n:<math>\\begin{align}\nL_f \\equiv \\underline{\\int_{a}^{b}} f(x) \\, \\mathrm{d}x &\\quad U_f \\equiv \\overline{\\int_{a}^{b}} f(x) \\, \\mathrm{d}x\n\\end{align}</math>\n\nAnd like Darboux sums they are sometimes simply called the lower and upper integrals.\n\nIf ''U''<sub>ƒ</sub>&nbsp;=&nbsp;''L''<sub>ƒ</sub>, then we call the common value the '''Darboux Integral'''.<ref>Wolfram MathWorld</ref> We also say that ƒ is ''Darboux-integrable'' or simply ''integrable'' and set\n\n:<math>\\int_a^b {f(t)\\,dt} = U_f = L_f , \\,\\!</math>\n\nAn equivalent and sometimes useful criterion for the integrability of ''f'' is to show that for every ''ε > 0'' there exists a partition ''P''<sub>''ε''</sub> on  [''a'',''b''] such that<ref>Spivak 2008, chapter 13.</ref>\n\n:<math> U_{f,P_\\epsilon} - L_{f,P_\\epsilon} < \\epsilon </math>\n\n==Properties==\n*For any given partition, the upper Darboux sum is always greater than or equal to the lower Darboux sum. Furthermore, the lower Darboux sum is bounded below by the rectangle of width ''(b-a)'' and height ''inf(f)'' taken over [''a'',''b'']. Likewise, the upper sum is bounded above by the rectangle of width ''(b-a)'' and height ''sup(f)''.\n::<math>(b-a)\\inf_{x \\in [a,b]} f(x) \\leqslant L_{f,P} \\leqslant U_{f,P} \\leqslant (b-a)\\sup_{x \\in [a,b]} f(x)</math>\n\n*The lower and upper Darboux integrals satisfy\n::<math>\\underline{\\int_{a}^{b}} f(x) \\, dx  \\leqslant \\overline{\\int_{a}^{b}} f(x) \\, dx </math>\n\n*Given any ''c'' in (''a'',''b'')\n::<math>\\begin{align}\n\\underline{\\int_{a}^{b}} f(x) \\, dx  &= \\underline{\\int_{a}^{c}} f(x) \\, dx + \\underline{\\int_{c}^{b}} f(x) \\, dx\\\\[6pt]\n\\overline{\\int_{a}^{b}} f(x) \\, dx  &= \\overline{\\int_{a}^{c}} f(x) \\, dx + \\overline{\\int_{c}^{b}} f(x) \\, dx\n\\end{align}</math>\n\n*The lower and upper Darboux integrals are not necessarily linear. Suppose that ''g'':[''a'',''b'']→ℝ is also a bounded function, then the upper and lower integrals satisfy the following inequalities.\n\n::<math>\\begin{align}\n\\underline{\\int_{a}^{b}} f(x) \\, dx + \\underline{\\int_{a}^{b}} g(x) \\, dx &\\leqslant \\underline{\\int_{a}^{b}} f(x) + g(x) \\, dx\\\\[6pt]\n \\overline{\\int_{a}^{b}} f(x) \\, dx +  \\overline{\\int_{a}^{b}} g(x) \\, dx &\\geqslant  \\overline{\\int_{a}^{b}} f(x) + g(x) \\, dx \n\\end{align}</math>\n\n*For a constant ''c'' ≥ 0 we have\n::<math>\\begin{align}\n\\underline{\\int_{a}^{b}} cf(x) &= c\\underline{\\int_{a}^{b}} f(x)\\\\[6pt]\n\\overline{\\int_{a}^{b}} cf(x) &= c\\overline{\\int_{a}^{b}} f(x)\n\\end{align}</math>\n\n*For a constant ''c'' ≤ 0 we have\n::<math>\\begin{align}\n\\underline{\\int_{a}^{b}} cf(x) &= c\\overline{\\int_{a}^{b}} f(x)\\\\[6pt]\n\\overline{\\int_{a}^{b}} cf(x) &= c\\underline{\\int_{a}^{b}} f(x)\n\\end{align}</math>\n\n*Consider the function:\n::<math>\\begin{cases} F : [a, b] \\to \\R \\\\ F(x) = \\underline{\\int_{a}^{x}} f(t) \\, dt \\end{cases}</math>\n:then ''F'' is [[Lipschitz continuous]]. An identical result holds if ''F'' is defined using an upper Darboux integral.\n\n==Examples==\n\n===A Darboux-integrable function===\nSuppose we want to show that the function ''f(x) = x'' is Darboux-integrable on the interval [0,1] and determine its value. To do this we partition [0,1] into ''n'' equally sized subintervals each of length ''1/n''. We denote a partition of ''n'' equally sized subintervals as ''P''<sub>''n''</sub>.\n\nNow since ''f(x) = x'' is strictly increasing on [0,1], the infimum on any particular subinterval is given by its starting point. Likewise the supremum on any particular subinterval is given by its end point. The starting point of the ''k''th subinterval in ''P''<sub>''n''</sub> is ''(k-1)/n'' and the end point is ''k/n''. Thus the lower Darboux sum on a partition ''P''<sub>''n''</sub> is given by\n\n:<math>\\begin{align}\nL_{f,P_n} &= \\sum_{k = 1}^{n} f(x_{k-1})(x_{k} - x_{k-1})\\\\\n         &= \\sum_{k = 1}^{n} \\frac{k-1}{n} \\cdot \\frac{1}{n}\\\\\n         &= \\frac{1}{n^2} \\sum_{k = 1}^{n} [k-1]\\\\ \n         &= \\frac{1}{n^2}\\left[ \\frac{(n-1)n}{2} \\right]\n\\end{align}</math>\n\nsimilarly, the upper Darboux sum is given by\n\n:<math>\\begin{align}\nU_{f,P_n} &= \\sum_{k = 1}^{n} f(x_{k})(x_{k} - x_{k-1})\\\\\n         &= \\sum_{k = 1}^{n} \\frac{k}{n} \\cdot \\frac{1}{n}\\\\\n         &= \\frac{1}{n^2} \\sum_{k = 1}^{n} k\\\\ \n         &= \\frac{1}{n^2}\\left[ \\frac{(n+1)n}{2} \\right]\n\\end{align}</math>\n\nSince\n:<math>\\begin{align}\nU_{f,P_n} - L_{f,P_n} &= \\frac{1}{n}\n\\end{align}</math>\nThus for given any ''ε > 0'', we have that any partition ''P''<sub>''n''</sub> with ''n > 1/ε'' satisfies \n:<math>\\begin{align}\nU_{f,P_n} - L_{f,P_n} &< \\epsilon\n\\end{align}</math>\nwhich shows that ''f'' is Darboux integrable. To find the value of the integral note that\n\n:<math>\\begin{align}\n\\int_{0}^{1}f(x) \\, dx &= \\lim_{n \\to \\infty} U_{f,P_n} =\\lim_{n \\to \\infty} L_{f,P_n}  = \\frac{1}{2}\n\\end{align}</math>\n\n===An unintegrable function===\nSuppose we have the function ''f'':[0,1]→ℝ defined as \n\n:<math>\\begin{align}\nf(x) &=\n \\begin{cases}\n 0, & \\text{if }x\\text{ is rational} \\\\\n 1, & \\text{if }x\\text{ is irrational}\n \\end{cases}\n\\end{align}</math>\n\nSince the rational and irrational numbers are both [[dense subset]]s of ℝ, it follows that ''f'' takes on the value of 0 and 1 on every subinterval of any partition. Thus for any partition ''P'' we have\n\n:<math>\\begin{align}\nL_{f,P} &=\\sum_{k = 1}^{n}(x_{k} - x_{k-1})\\inf_{x \\in [x_{k-1},x_{k}]}f = 0\\\\\nU_{f,P} &=\\sum_{k = 1}^{n}(x_{k} - x_{k-1}) \\sup_{x \\in [x_{k-1},x_{k}]}f = 1\n\\end{align}</math>\nfrom which we can see that the lower and upper Darboux integrals are unequal.\n\n==Refinement of a partition and relation to Riemann integration==\n[[Image:Darboux refinement.svg|250px|thumb|right|When passing to a refinement, the lower sum increases and the upper sum decreases.]]\nA ''refinement'' of the partition\n\n:<math>x_0,\\ldots,x_n  \\,\\!</math>\n\nis a partition\n\n:<math>y_0, \\ldots, y_m \\,\\!</math>\n\nsuch that for every ''i'' with\n\n:<math>0 \\le i \\le n \\,\\!</math>\n\nthere is an integer ''r''(''i'') such that\n\n:<math> x_{i} = y_{r(i)} . \\,\\!</math>\n\nIn other words, to make a refinement, cut the subintervals into smaller pieces and do not remove any existing cuts.  If\n\n:<math>P' = (y_0,\\ldots,y_m) \\,\\!</math>\n\nis a refinement of\n\n:<math>P = (x_0,\\ldots,x_n) , \\,\\!</math>\n\nthen\n\n:<math>U_{f, P} \\ge U_{f, P'} \\,\\!</math>\n\nand\n\n:<math>L_{f, P} \\le L_{f, P'} . \\,\\!</math>\n\nIf ''P''<sub>1</sub>, ''P''<sub>2</sub> are two partitions of the same interval (one need not be a refinement of the other), then\n\n:<math>L_{f, P_1} \\le U_{f, P_2} . \\,\\!</math>\n\nIt follows that\n\n:<math>L_f \\le U_f . \\,\\!</math>\n\nRiemann sums always lie between the corresponding lower and upper Darboux sums.  Formally, if\n\n:<math>P = (x_0,\\ldots,x_n) \\,\\!</math>\n\nand\n\n:<math>T = (t_1,\\ldots,t_n) \\,\\!</math>\n\ntogether make a tagged partition\n\n:<math> x_0 \\le t_1 \\le x_1\\le \\cdots \\le x_{n-1} \\le t_n \\le x_n \\,\\!</math>\n\n(as in the definition of the [[Riemann integral]]), and if the Riemann sum of ƒ corresponding to ''P'' and ''T'' is ''R'', then\n\n:<math>L_{f, P} \\le R \\le U_{f, P}.\\,\\!</math>\n\nFrom the previous fact, Riemann integrals are at least as strong as Darboux integrals: if the Darboux integral exists, then the upper and lower Darboux sums corresponding to a sufficiently fine partition will be close to the value of the integral, so any Riemann sum over the same partition will also be close to the value of the integral. There is a tagged partition that comes arbitrarily close to the value of the upper Darboux integral or lower Darboux integral, and consequently, if the Riemann integral exists, then the Darboux integral must exist as well.\n\n==See also==\n* [[Regulated integral]]\n* [[Lebesgue integration]]\n* [[Minimum bounding rectangle]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n* {{cite web\n| url = http://mathworld.wolfram.com/DarbouxIntegral.html\n| title = Darboux Integral\n| accessdate = 2013-01-08\n| work=[[Wolfram MathWorld]]\n}}\n* [http://www.encyclopediaofmath.org/index.php/Darboux_integral ''Darboux integral'' at Encyclopaedia of Mathematics]\n* {{springer|title=Darboux sum|id=p/d030160}}\n* {{Citation\n | last = Spivak\n | first = Michael\n | title = Calculus\n | publisher = Publish or Perish\n | year = 2008\n | edition = 4\n | isbn = 978-0914098911\n}}\n\n{{Integral}}\n\n{{DEFAULTSORT:Darboux Integral}}\n[[Category:Definitions of mathematical integration]]"
    },
    {
      "title": "Hellinger integral",
      "url": "https://en.wikipedia.org/wiki/Hellinger_integral",
      "text": "In mathematics, the '''Hellinger integral''' is an  integral introduced by {{harvs|txt|last=Hellinger|authorlink=Ernst Hellinger|year=1909}} that is a special case of the [[Kolmogorov integral]]. It is used to define the [[Hellinger distance]] in probability theory.\n\n==References==\n\n*{{Citation \n| last = Hellinger \n| first = E. \n| author-link = Ernst Hellinger\n| title = Neue Begründung der Theorie quadratischer Formen von unendlichvielen Veränderlichen \n| url = http://resolver.sub.uni-goettingen.de/purl?GDZPPN002166941 \n| year = 1909 \n| journal = [[Journal für die reine und angewandte Mathematik]]\n| language = German\n| volume = 136 \n| pages = 210–271\n| jfm = 40.0393.01\n| doi=10.1515/crll.1909.136.210\n}}\n*{{Citation \n| last1 = Hobson \n| first1 = E. W. \n| title = The theory of functions of a real variable and the theory of Fourier's series. Vol. I \n| url=https://books.google.com/books?id=LCc9AAAAIAAJ \n| publisher = [[Dover Publications]] \n| location = New York \n| pages = XV+736\n| mr = 0092828\n| zbl = 0081.27702\n| year = 1958}}\n*{{eom|title=Hellinger integral|id=h/h046900|first=I. A.|last= Vinogradova}}\n{{integral}}\n\n[[Category:Definitions of mathematical integration]]"
    },
    {
      "title": "Henstock–Kurzweil integral",
      "url": "https://en.wikipedia.org/wiki/Henstock%E2%80%93Kurzweil_integral",
      "text": "{{inlinerefs|date=February 2016}}\nIn [[mathematics]], the '''Henstock–Kurzweil integral''' or '''Generalized Riemann integral''' or '''gauge integral''' – also known as the (narrow) '''Denjoy integral''' (pronounced {{IPA-fr|dɑ̃ˈʒwa|}}), '''Luzin integral''' or '''Perron integral''', but not to be confused with the more general [[Khinchin integral|wide Denjoy integral]] – is one of a number of definitions of the [[integral]] of a [[function (mathematics)|function]]. It is a generalization of the [[Riemann integral]], and in some situations is more general than the [[Lebesgue integration]].  In particular, a function is Lebesgue integrable if and only if the function and its absolute value are Henstock-Kurzweil integrable. \n\nThis integral was first defined by [[Arnaud Denjoy]] (1912). Denjoy was interested in a definition that would allow one to integrate functions like\n\n:<math>f(x)=\\frac{1}{x}\\sin\\left(\\frac{1}{x^3}\\right).</math>\n\nThis function has a [[singularity (mathematics)|singularity]] at 0, and is not Lebesgue integrable. However, it seems natural to calculate its integral except over the interval  [−ε,δ] and then let ε, δ → 0.\n\nTrying to create a general theory, Denjoy used [[transfinite induction]] over the possible types of singularities, which made the definition quite complicated. Other definitions were given by [[Nikolai Luzin]] (using variations on the notions of [[absolute continuity]]), and by [[Oskar Perron]], who was interested in continuous major and minor functions. It took a while to understand that the Perron and Denjoy integrals are actually identical.\n\nLater, in 1957, the Czech mathematician [[Jaroslav Kurzweil]] discovered a new definition of this integral elegantly similar in nature to [[Riemann]]'s original definition which he named the '''gauge integral'''; the theory was developed by [[Ralph Henstock]]. Due to these two important contributions, it is now commonly known as the '''Henstock–Kurzweil integral'''. The simplicity of Kurzweil's definition made some educators advocate that this integral should replace the Riemann integral in introductory calculus courses.<ref>{{cite web|title=An Open Letter to Authors of Calculus Books|url=http://www.math.vanderbilt.edu/~schectex/ccc/gauge/letter/|accessdate=27 February 2014}}</ref>\n\n==Definition==\nGiven a [[tagged partition]] ''P'' of [''a'', ''b''], that is,\n\n:<math>a = u_0 < u_1 < \\cdots < u_n = b </math>\n\ntogether with\n\n:<math>t_i \\in [u_{i-1}, u_i],</math>\n\nwe define the Riemann sum for a function\n\n:<math>f \\colon [a, b] \\to \\mathbb{R}</math> \n\nto be\n\n:<math> \\sum_P f = \\sum_{i = 1}^n f(t_i) \\Delta u_i.</math>\n\nwhere\n\n:<math> \\Delta u_i := u_i - u_{i-1}.</math>\n\nGiven a positive function\n\n:<math>\\delta \\colon [a, b] \\to (0, \\infty),\\,</math>\n\nwhich we call a ''gauge'', we say a partition ''P ''is <math>\\delta</math>-fine if\n\n:<math>\\forall i \\ \\ [u_{i-1}, u_i] \\subset [t_i-\\delta(t_i), t_i + \\delta (t_i)].</math>\n\nWe now define a number ''I'' to be the Henstock–Kurzweil integral of ''f'' if for every ε&nbsp;>&nbsp;0 there exists a gauge <math>\\delta</math> such that whenever ''P'' is <math>\\delta</math>-fine, we have\n\n:<math> {\\Big \\vert} \\sum_P f - I {\\Big \\vert} < \\varepsilon. </math>\n\nIf such an ''I'' exists, we say that ''f'' is Henstock–Kurzweil integrable on [''a'', ''b''].\n\n[[Cousin's theorem]] states that for every gauge <math>\\delta</math>, such a <math>\\delta</math>-fine partition ''P'' does exist, so this condition cannot be satisfied [[vacuous truth|vacuously]]. The Riemann integral can be regarded as the special case where we only allow constant gauges.\n\n==Properties==\n\nLet {{nowrap|''f'': [''a'', ''b''] → '''R'''}} be any function.\n\nIf {{nowrap|''a'' < ''c'' < ''b''}}, then ''f'' is Henstock–Kurzweil integrable on [''a'',&nbsp;''b''] if and only if it is Henstock–Kurzweil integrable on both [''a'',&nbsp;''c''] and [''c'',&nbsp;''b''], and then\n\n:<math>\\int_a^bf(x)\\,dx=\\int_a^cf(x)\\,dx+\\int_c^bf(x)\\,dx.</math>\n\nThe Henstock–Kurzweil integral is linear, i.e., if ''f'' and ''g'' are integrable, and α, β are reals, then α''f'' + β''g'' is integrable and\n\n:<math>\\int_a^b\\alpha f(x)+\\beta g(x)\\,dx=\\alpha\\int_a^bf(x)\\,dx+\\beta\\int_a^bg(x)\\,dx.</math>\n\nIf ''f'' is Riemann or Lebesgue integrable, then it is also Henstock–Kurzweil integrable, and the values of the integrals are the same. The important [[Hake's theorem]] states that\n\n:<math>\\int_a^bf(x)\\,dx=\\lim_{c\\to b^-}\\int_a^cf(x)\\,dx</math>\n\nwhenever either side of the equation exists, and symmetrically for the lower integration bound. This means that if ''f'' is \"[[improper integral|improperly]] Henstock–Kurzweil integrable\", then it is properly Henstock–Kurzweil integrable; in particular, improper Riemann or Lebesgue integrals such as\n\n:<math>\\int_0^1\\frac{\\sin(1/x)}x\\,dx</math>\n\nare also Henstock–Kurzweil integrals. This shows that there is no sense in studying an \"improper Henstock–Kurzweil integral\" with finite bounds. However, it makes sense to consider improper Henstock–Kurzweil integrals with infinite bounds such as\n\n:<math>\\int_a^{\\infty} f(x)\\,dx := \\lim_{b\\to\\infty}\\int_a^bf(x)\\,dx.</math>\n\nFor many types of functions the Henstock–Kurzweil integral is no more general than Lebesgue integral. For example, if ''f'' is bounded with compact support, the following are equivalent:\n*''f'' is Henstock–Kurzweil integrable,\n*''f'' is Lebesgue integrable,\n*''f'' is [[measurable function|Lebesgue measurable]].\nIn general, every Henstock–Kurzweil integrable function is measurable, and ''f'' is Lebesgue integrable if and only if both ''f'' and |''f''| are Henstock–Kurzweil integrable. This means that the Henstock–Kurzweil integral can be thought of as a \"[[absolute convergence|non-absolutely convergent]] version of Lebesgue integral\". It also implies that the Henstock–Kurzweil integral satisfies appropriate versions of the [[monotone convergence theorem#Lebesgue monotone convergence theorem|monotone convergence theorem]] (without requiring the functions to be nonnegative) and [[dominated convergence theorem]] (where the condition of dominance is loosened to ''g''(''x'') ≤ ''f<sub>n</sub>''(''x'') ≤ ''h''(''x'') for some integrable ''g'', ''h'').\n\nIf ''F'' is differentiable everywhere (or with countable many exceptions), the derivative ''F''′ is Henstock–Kurzweil integrable, and its indefinite Henstock–Kurzweil integral is ''F''.  (Note that ''F''′ need not be Lebesgue integrable.) In other words, we obtain a simpler and more satisfactory version of the [[second fundamental theorem of calculus]]: each differentiable function is, up to a constant, the integral of its derivative:\n\n: <math>F(x) - F(a) = \\int_a^x F'(t) \\,dt.</math>\n\nConversely, the [[Lebesgue differentiation theorem]] continues to hold for the Henstock–Kurzweil integral: if ''f'' is Henstock–Kurzweil integrable on [''a'',&nbsp;''b''], and\n\n:<math>F(x)=\\int_a^xf(t)\\,dt,</math>\n\nthen ''F''′(''x'') = ''f''(''x'') almost everywhere in [''a'',&nbsp;''b''] (in particular, ''F'' is almost everywhere differentiable).\n\nThe space of all Henstock–Kurzweil-integrable functions is often endowed with the [[Alexiewicz norm]], with respect to which it is [[barrelled space|barrelled]] but [[complete space|incomplete]].\n\n==McShane integral==\n[[Lebesgue integral]] on a line can also be presented in a similar fashion.\n\nIf we take the definition of the Henstock–Kurzweil integral from above, and we drop the condition\n\n:<math>t_i \\in [u_{i-1}, u_i],</math>\n\nthen we get a definition of the ''[[McShane integral]]'', which is equivalent to the Lebesgue integral. Note that the condition\n\n:<math>\\forall i \\ \\ [u_{i-1}, u_i] \\subset [t_i-\\delta(t_i), t_i + \\delta (t_i)]</math>\n\ndoes still apply, and we technically also require <math display=\"inline\">t_i \\in [a,b]</math> for <math display=\"inline\">f(t_i)</math> to be defined.\n\n==See also==\n*[[Pfeffer integral]]\n*[[Cauchy principal value]]\n*[[Hadamard finite part integral]]\n\n==References==\n===Footnotes===\n{{Reflist}}\n\n===General===\n* {{cite book | first = Robert G. |last= Bartle | author-link=Robert G. Bartle| title = A Modern Theory of Integration | series = [[Graduate Studies in Mathematics]] |volume = 32 |publisher=American Mathematical Society | year=2001 | isbn=978-0-8218-0845-0}}\n* [https://www.researchgate.net/publication/308903165_A_modern_Integration_Theory_for_21st_century?ev=prf_pub A Modern Integration Theory in 21st Century]\n* {{cite book | first1 = Robert G. |last1 = Bartle | author-link=Robert G. Bartle| first2= Donald R. |last2= Sherbert|title = Introduction to Real Analysis |publisher=Wiley |edition=3rd| year=1999 | isbn=978-0-471-32148-4}}\n* {{cite book | first1=V G |last1=Čelidze |first2= A G |last2= Džvaršeǐšvili|title= The Theory of the Denjoy Integral and Some Applications | series= Series in Real Analysis | volume=3| publisher=World Scientific Publishing Company | year = 1989 |isbn=978-981-02-0021-3}}\n* {{cite book |first1= A.G. |last = Das|title=The Riemann, Lebesgue, and Generalized Riemann Integrals |publisher=Narosa Publishers |year=2008 |isbn = 978-81-7319-933-2 }}\n* {{cite book | last=Gordon | first=Russell A. | title=The integrals of Lebesgue, Denjoy, Perron, and Henstock | series=Graduate Studies in Mathematics | volume=4  | publisher=American Mathematical Society | location=Providence, RI | year=1994 | isbn=978-0-8218-3805-1 }}\n* {{cite book | first=Ralph|last=Henstock| author-link=Ralph Henstock | title=Lectures on the Theory of Integration|series = Series in Real Analysis | volume=1| publisher=World Scientific Publishing Company | year=1988|isbn=978-9971-5-0450-2}}\n* {{cite book | first = Jaroslav | last= Kurzweil | author-link=Jaroslav Kurzweil | title=Henstock–Kurzweil Integration: Its Relation to Topological Vector Spaces |series = Series in Real Analysis | volume=7| publisher=World Scientific Publishing Company | year = 2000 |isbn=978-981-02-4207-7}}\n* {{cite book | first = Jaroslav | last= Kurzweil | author-link=Jaroslav Kurzweil | title=Integration Between the Lebesgue Integral and the Henstock–Kurzweil Integral: Its Relation to Locally Convex Vector Spaces|series = Series in Real Analysis | volume=8| publisher=World Scientific Publishing Company | year = 2002 |isbn=978-981-238-046-3}}\n* {{cite book |first=Solomon |last=Leader |title = The Kurzweil–Henstock Integral & Its Differentials | series= Pure and Applied Mathematics Series | publisher= CRC | year = 2001 | isbn = 978-0-8247-0535-0 }}\n* {{cite book | first=Peng-Yee |last=Lee | title=Lanzhou Lectures on Henstock Integration|series = Series in Real Analysis | volume=2| publisher=World Scientific Publishing Company | year = 1989 |isbn=978-9971-5-0891-3}}\n* {{cite book | last1=Lee |first1 = Peng-Yee |last2= Výborný|first2=Rudolf |title = Integral: An Easy Approach after Kurzweil and Henstock |series = Australian Mathematical Society Lecture Series | publisher= Cambridge University Press |year=2000|isbn=978-0-521-77968-5}}\n* {{cite book | last=McLeod | first=Robert M. | title=The generalized Riemann integral | series=Carus Mathematical Monographs|volume=20 | publisher=Mathematical Association of America | location=Washington, D.C. | year=1980 | isbn=978-0-88385-021-3 }}\n* {{cite book | last=Swartz | first=Charles W. |title=Introduction to Gauge Integrals | publisher= World Scientific Publishing Company |year=2001 | isbn=978-981-02-4239-8 }}\n* {{cite book | first1= Charles W. |last1= Swartz| first2 =Douglas S. | last2=Kurtz |title= Theories of Integration: The Integrals of Riemann, Lebesgue, Henstock–Kurzweil, and McShane|series= Series in Real Analysis |volume=9| publisher= World Scientific Publishing Company | year = 2004 |isbn = 978-981-256-611-9 }}\n\n==External links==\nThe following are additional resources on the web for learning more:\n* {{springer|title=Kurzweil-Henstock integral|id=p/k110200}}\n* [http://www.math.vanderbilt.edu/~schectex/ccc/gauge/ An Introduction to The Gauge Integral]\n* [http://www.math.vanderbilt.edu/~schectex/ccc/gauge/letter/ An Open Suggestion: To replace the Riemann integral with the gauge integral in calculus textbooks] signed by Bartle, Henstock, Kurzweil, Schechter, Schwabik, and Výborný\n\n\n{{integral}}\n\n{{DEFAULTSORT:Henstock-Kurzweil integral}}\n[[Category:Definitions of mathematical integration]]"
    },
    {
      "title": "Homological integration",
      "url": "https://en.wikipedia.org/wiki/Homological_integration",
      "text": "{{about|an extension of the theory of the [[Lebesgue integral]] to [[manifold]]s|numerical method|geometric integrator}}\nIn the [[mathematics|mathematical]] fields of [[differential geometry]] and [[geometric measure theory]], '''homological integration''' or '''geometric integration''' is a method for extending the notion of the [[integral]] to [[manifold]]s.  Rather than functions or [[differential form]]s, the integral is defined over [[current (mathematics)|currents]] on a manifold.\n\nThe theory is \"homological\" because currents themselves are defined by duality with differential forms.  To wit, the space {{math|''D''<sup>''k''</sup>}} of {{mvar|k}}-currents on a manifold {{mvar|M}} is defined as the [[dual space]], in the sense of [[distribution (mathematics)|distributions]], of the space of {{mvar|k}}-forms {{math|Ω<sup>''k''</sup>}} on {{mvar|M}}.  Thus there is a pairing between {{mvar|k}}-currents {{mvar|T}} and {{mvar|k}}-forms {{mvar|α}}, denoted here by\n:<math>\\langle T, \\alpha\\rangle.</math>\nUnder this duality pairing, the [[exterior derivative]] \n:<math>d : \\Omega^{k-1} \\to \\Omega^k</math>\ngoes over to a [[boundary operator]]\n:<math>\\partial : D^k \\to D^{k-1} </math>\ndefined by\n:<math>\\langle\\partial T,\\alpha\\rangle = \\langle T, d\\alpha\\rangle</math>\nfor all {{math|''α''&nbsp;∈&nbsp;Ω<sup>''k''</sup>}}.  This is a homological rather than [[cohomology theory|cohomological]] construction.\n\n==References==\n*{{citation\n|last = Federer\n|first = Herbert\n|authorlink = Herbert Federer\n|title = Geometric measure theory\n|publisher = Springer-Verlag New York Inc.\n|location = New York\n|year = 1969\n|pages = xiv+676\n|isbn = 978-3-540-60656-7\n|series = Die Grundlehren der mathematischen Wissenschaften\n|volume = 153 \n|mr=0257325\n|zbl=0176.00801}}.\n*{{citation\n|first=H.\n|last=Whitney\n|author-link=Hassler Whitney\n|title=Geometric Integration Theory\n|series=Princeton Mathematical Series\n|volume=21\n|publisher=[[Princeton University Press]] and [[Oxford University Press]]\n|place=Princeton, NJ and London\n|year=1957\n|pages= XV+387\n|mr=0087148\n|zbl=0083.28204\n}}.\n\n[[Category:Definitions of mathematical integration]]\n[[Category:Measure theory]]\n\n\n{{geometry-stub}}"
    },
    {
      "title": "Itô calculus",
      "url": "https://en.wikipedia.org/wiki/It%C3%B4_calculus",
      "text": "{{Short description|Calculus of stochastic differential equations}}\n[[File:ItoIntegralWienerProcess.svg|thumb|300px|Itô integral ''Y''<sub>''t''</sub>(''B'') ({{color|blue|blue}}) of a Brownian motion ''B'' ({{color|red|red}}) with respect to itself, i.e., both the integrand and the integrator are Brownian. It turns out ''Y''<sub>''t''</sub>(''B'') = (B<sup>2</sup> - ''t'')/2.]]\n'''Itô calculus''', named after [[Kiyoshi Itô]], extends the methods of calculus to [[stochastic process]]es such as [[Brownian motion]] (see [[Wiener process]]). It has important applications in [[mathematical finance]] and [[stochastic differential equation]]s.\n\nThe central concept is the Itô stochastic integral, a stochastic generalization of the [[Riemann–Stieltjes integral]] in analysis. The integrands and the integrators are now stochastic processes:\n:<math>Y_t=\\int_0^t H_s\\,dX_s,</math>\nwhere ''H'' is a locally square-integrable process adapted to the [[filtration (probability theory)|filtration]] generated by ''X'' {{Harv|Revuz|Yor|1999|loc=Chapter IV}}, which is a [[Brownian motion]] or, more generally, a [[semimartingale]]. The result of the integration is then another stochastic process. Concretely, the integral from 0 to any particular ''t'' is a [[random variable]], defined as a limit of a certain sequence of random variables. The paths of Brownian motion fail to satisfy the requirements to be able to apply the standard techniques of calculus. So with the integrand a stochastic process, the Itô stochastic integral amounts to an integral with respect to a function which is not differentiable at any point and has infinite [[Bounded variation|variation]] over every time interval. \nThe main insight is that the integral can be defined as long as the integrand ''H'' is [[adapted process|adapted]], which loosely speaking means that its value at time ''t'' can only depend on information available up until this time. Roughly speaking, one chooses a sequence of partitions of the interval from 0 to ''t'' and construct [[Riemann sum]]s. Every time we are computing a Riemann sum, we are using a particular instantiation of the integrator. It is crucial which point in each of the small intervals is used to compute the value of the function. The limit then is taken in probability as the [[Mesh (mathematics)|mesh]] of the partition is going to zero. Numerous technical details have to be taken care of to show that this limit exists and is independent of the particular sequence of partitions. Typically, the left end of the interval is used.\n\nImportant results of Itô calculus include the integration by parts formula and [[Itô's lemma]], which is a [[Integration by substitution|change of variables]] formula. These differ from the formulas of standard calculus, due to [[quadratic variation]] terms.\n\nIn [[mathematical finance]], the described evaluation strategy of the integral is conceptualized as that we are first deciding what to do, then observing the change in the prices. The integrand is how much stock we hold, the integrator represents the movement of the prices, and the integral is how much money we have in total  including what our stock is worth, at any given moment. The prices of stocks and other traded financial assets can be modeled by stochastic processes such as Brownian motion or, more often, [[geometric Brownian motion]] (see [[Black–Scholes]]). Then, the Itô stochastic integral represents the payoff of a continuous-time trading strategy consisting of holding an amount ''H<sub>t</sub>'' of the stock at time ''t''. In this situation, the condition that ''H'' is adapted corresponds to the necessary restriction that the trading strategy can only make use of the available information at any time. This prevents the possibility of unlimited gains through [[high-frequency trading]]: buying the stock just before each uptick in the market and selling before each downtick. Similarly, the condition that ''H'' is adapted implies that the stochastic integral will not diverge when calculated as a limit of [[Riemann sum]]s {{Harv|Revuz|Yor|1999|loc=Chapter IV}}.\n\n==Notation==\nThe process ''Y'' defined as before as\n\n: <math>Y_t = \\int_0^t H\\,dX\\equiv\\int_0^t H_s\\,dX_s ,</math>\n\nis itself a stochastic process with time parameter ''t'', which is also sometimes written as ''Y'' = ''H'' · ''X'' {{Harv|Rogers|Williams|2000}}. Alternatively, the integral is often written in differential form ''dY = H dX'', which is equivalent to ''Y''&nbsp;−&nbsp;''Y''<sub>0</sub> =&nbsp;''H''&nbsp;·&nbsp;''X''.  As Itô calculus is concerned with continuous-time stochastic processes, it is assumed that an underlying [[filtered probability space]] is given\n\n:<math>(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\ge 0},\\mathbb{P}) .</math>\n\nThe [[σ-algebra]] ''F<sub>t</sub>'' represents the information available up until time ''t'', and a process ''X'' is adapted if ''X<sub>t</sub>'' is ''F<sub>t</sub>''-measurable. A Brownian motion ''B'' is understood to be an ''F<sub>t</sub>''-Brownian motion, which is just a standard Brownian motion with the properties that ''B''<sub>''t''</sub> is ''F<sub>t</sub>''-measurable and that ''B''<sub>''t''+''s''</sub>&nbsp;−&nbsp;''B''<sub>''t''</sub> is independent of ''F<sub>t</sub>'' for all ''s'',''t''&nbsp;≥&nbsp;0 {{Harv|Revuz|Yor|1999}}.\n\n==Integration with respect to Brownian motion==\nThe Itô integral can be defined in a manner similar to the [[Riemann–Stieltjes integral]], that is as a [[Convergence of random variables|limit in probability]] of [[Riemann sum]]s; such a limit does not necessarily exist pathwise.  Suppose that ''B'' is a [[Wiener process]] (Brownian motion) and that ''H'' is a [[right-continuous]] ([[Càdlàg|cadlag]]), [[adapted process|adapted]] and locally bounded process. If <math>\\{\\pi_n\\}</math>  is a sequence of [[Partition of an interval|partition]]s of [0,&nbsp;''t''] with mesh going to zero, then the Itô integral of ''H'' with respect to ''B'' up to time ''t'' is a [[random variable]]\n\n:<math>\\int_0^t H \\,d B =\\lim_{n\\rightarrow\\infty} \\sum_{[t_{i-1},t_i]\\in\\pi_n}H_{t_{i-1}}(B_{t_i}-B_{t_{i-1}}).</math>\n\nIt can be shown that this limit [[Convergence of random variables|converges in probability]].\n\nFor some applications, such as [[martingale representation theorem]]s and [[local time (mathematics)|local times]], the integral is needed for processes that are not continuous.  The [[predictable process]]es form the smallest class that is closed under taking limits of sequences and contains all adapted left-continuous processes.  If ''H'' is any predictable process such that ∫<sub>0</sub><sup>''t''</sup>&nbsp;''H''<sup>2</sup>&nbsp;''ds''&nbsp;<&nbsp;∞ for every ''t''&nbsp;≥&nbsp;0 then the integral of ''H'' with respect to ''B'' can be defined, and ''H'' is said to be ''B''-integrable.  Any such process can be approximated by a sequence ''H<sub>n</sub>'' of left-continuous, adapted and locally bounded processes, in the sense that\n\n:<math> \\int_0^t (H-H_n)^2\\,ds\\to 0</math>\n\nin probability. Then, the Itô integral is\n\n:<math>\\int_0^t H\\,dB = \\lim_{n\\to\\infty}\\int_0^t H_n\\,dB</math>\n\nwhere, again, the limit can be shown to converge in probability. The stochastic integral satisfies the [[Itô isometry]]\n\n:<math>\\mathbb{E}\\left[ \\left(\\int_0^t H_s \\, dB_s\\right)^2\\right]=\\mathbb{E} \\left[ \\int_0^t H_s^2\\,ds\\right ]</math>\n\nwhich holds when ''H'' is bounded or, more generally, when the integral on the right hand side is finite.\n\n==Itô processes==\n[[File:ItoProcess1D.svg|thumb|A single realization of Itô process with μ = 0 and σ = ψ(t-5), where ψ is the [[Mexican hat wavelet|Ricker wavelet]]. Off the tide of wavelet, the motion of Itô process is stable.]]\nAn '''Itô process''' is defined to be an [[adapted process|adapted]] stochastic process that can be expressed as the sum of an integral with respect to Brownian motion and an integral with respect to time,\n\n:<math>X_t=X_0+\\int_0^t\\sigma_s\\,dB_s + \\int_0^t\\mu_s\\,ds.</math>\n\nHere, ''B'' is a Brownian motion and it is required that σ is a predictable ''B''-integrable process, and μ is predictable and ([[Lebesgue integration|Lebesgue]]) integrable. That is,\n\n:<math>\\int_0^t(\\sigma_s^2+|\\mu_s|)\\,ds<\\infty</math>\n\nfor each ''t''. The stochastic integral can be extended to such Itô processes,\n\n:<math>\\int_0^t H\\,dX =\\int_0^t H_s\\sigma_s\\,dB_s + \\int_0^t H_s\\mu_s\\,ds.</math>\n\nThis is defined for all locally bounded and predictable integrands. More generally, it is required that ''H''σ be ''B''-integrable and ''H''μ be Lebesgue integrable, so that\n:<math>\\int_0^t (H^2 \\sigma^2 + |H\\mu| )ds < \\infty.</math>\nSuch predictable processes ''H'' are called ''X''-integrable.\n\nAn important result for the study of Itô processes is [[Itô's lemma]].  In its simplest form, for any twice continuously differentiable function ''f'' on the reals and Itô process ''X'' as described above, it states that ''f''(''X'') is itself an Itô process satisfying\n\n:<math>df(X_t)=f^\\prime(X_t)\\,dX_t + \\frac{1}{2}f^{\\prime\\prime} (X_t) \\sigma_t^2 \\, dt.</math>\n\nThis is the stochastic calculus version of the [[integration by substitution|change of variables]] formula and [[chain rule]]. It differs from the standard result due to the additional term involving the second derivative of ''f'', which comes from the property that Brownian motion has non-zero [[quadratic variation]].\n\n==Semimartingales as integrators==\n\nThe Itô integral is defined with respect to a [[semimartingale]] ''X''. These are processes which can be decomposed as ''X''&nbsp;=&nbsp;''M''&nbsp;+&nbsp;''A'' for a [[local martingale]] ''M'' and [[bounded variation|finite variation]] process&nbsp;''A''. Important examples of such processes include [[Wiener process|Brownian motion]], which is a [[Martingale (probability theory)|martingale]], and [[Lévy process]]es. For a left continuous, locally bounded and adapted process ''H'' the integral ''H''&nbsp;·&nbsp;''X'' exists, and can be calculated as a limit of Riemann sums. Let π<sub>''n''</sub> be a sequence of [[Partition of an interval|partition]]s of [0,&nbsp;''t''] with mesh going to zero,\n\n:<math>\\int_0^t H\\,dX = \\lim_{n\\rightarrow\\infty} \\sum_{t_{i-1},t_i\\in\\pi_n}H_{t_{i-1}}(X_{t_i}-X_{t_{i-1}}).</math>\n\nThis limit converges in probability. The stochastic integral of left-continuous processes is general enough for studying much of stochastic calculus. For example, it is sufficient for applications of Itô's Lemma, changes of measure via [[Girsanov theorem|Girsanov's theorem]], and for the study of [[stochastic differential equation]]s. However, it is inadequate for other important topics such as [[martingale representation theorem]]s and [[local time (mathematics)|local times]].\n\nThe integral extends to all predictable and locally bounded integrands, in a unique way, such that the [[dominated convergence theorem]] holds. That is, if ''H<sub>n</sub>''&nbsp;→&nbsp;;''H'' and |''H<sub>n</sub>''|&nbsp;≤&nbsp;''J'' for a locally bounded process&nbsp;''J'', then\n:<math>\\int_0^t H_n dX \\to \\int_0^t H dX, </math>\nin probability. The uniqueness of the extension from left-continuous to predictable integrands is a result of the [[monotone class lemma]].\n\nIn general, the stochastic integral ''H''&nbsp;·&nbsp;''X'' can be defined even in cases where the predictable process ''H'' is not locally bounded.  If ''K''&nbsp;=&nbsp;1&nbsp;/&nbsp;(1&nbsp;+&nbsp;|''H''|) then ''K'' and ''KH'' are bounded.  Associativity of stochastic integration implies that ''H'' is ''X''-integrable, with integral ''H''&nbsp;·&nbsp;''X'' =&nbsp;''Y'', if and only if ''Y''<sub>0</sub>&nbsp;=&nbsp;0 and ''K''&nbsp;·&nbsp;''Y'' =&nbsp;(''KH'')&nbsp;·&nbsp;''X''. The set of ''X''-integrable processes is denoted by L(''X'').\n\n==Properties==\nThe following properties can be found in works such as {{Harv|Revuz|Yor|1999}} and {{Harv|Rogers|Williams|2000}}:\n\n* The stochastic integral is a [[càdlàg]] process. Furthermore, it is a [[semimartingale]].\n* The discontinuities of the stochastic integral are given by the jumps of the integrator multiplied by the integrand. The jump of a càdlàg process at a time ''t'' is ''X<sub>t</sub>''&nbsp;−&nbsp;''X''<sub>t−</sub>, and is often denoted by Δ''X<sub>t</sub>''. With this notation, Δ(''H''&nbsp;·&nbsp;''X'')&nbsp;=&nbsp;''H'' Δ''X''. A particular consequence of this is that integrals with respect to a continuous process are always themselves continuous.\n* '''[[Associativity]]'''. Let ''J'', ''K'' be predictable processes, and ''K'' be ''X''-integrable. Then, ''J'' is ''K''&nbsp;·&nbsp;''X'' integrable if and only if ''JK'' is ''X'' integrable, in which case\n*:<math> J\\cdot (K\\cdot X) = (JK)\\cdot X</math>\n* '''[[Dominated convergence theorem|Dominated convergence]]'''. Suppose that ''H<sub>n</sub>'' → ''H'' and ''|H<sub>n</sub>|'' ≤ ''J'', where ''J'' is an ''X''-integrable process. then ''H<sub>n</sub>''&nbsp;·&nbsp;''X'' →&nbsp;''H''&nbsp;·&nbsp;''X''. Convergence is in probability at each time&nbsp;''t''. In fact, it converges uniformly on compacts in probability.\n* The stochastic integral commutes with the operation of taking quadratic covariations. If ''X'' and ''Y'' are semimartingales then any ''X''-integrable process will also be [''X'',&nbsp;''Y'']-integrable, and [''H''&nbsp;·&nbsp;''X'',&nbsp;''Y''] = ''H''&nbsp;·&nbsp;[''X'',&nbsp;''Y'']. A consequence of this is that the quadratic variation process of a stochastic integral is equal to an integral of a quadratic variation process,\n*:<math>[H\\cdot X]=H^2\\cdot[X]</math>\n\n==Integration by parts==\nAs with ordinary calculus, [[integration by parts]] is an important result in stochastic calculus. The integration by parts formula for the Itô integral differs from the standard result due to the inclusion of a [[quadratic variation|quadratic covariation]] term. This term comes from the fact that Itô calculus deals with processes with non-zero quadratic variation, which only occurs for infinite variation processes (such as Brownian motion). If ''X'' and ''Y'' are semimartingales then\n:<math>X_tY_t = X_0Y_0+\\int_0^t X_{s-}\\,dY_s + \\int_0^t Y_{s-}\\,dX_s + [X,Y]_t</math>\nwhere [''X'',&nbsp;''Y''] is the quadratic covariation process.\n\nThe result is similar to the integration by parts theorem for the [[Riemann–Stieltjes integral]] but has an additional [[quadratic variation]] term.\n\n==Itô's lemma==\n{{Main article|Itô's lemma}}\n\nItô's lemma is the version of the [[chain rule]] or [[integration by substitution|change of variables]] formula which applies to the Itô integral. It is one of the most powerful and frequently used theorems in stochastic calculus. For a continuous ''n''-dimensional semimartingale ''X'' = (''X''<sup>1</sup>,...,''X''<sup>''n''</sup>) and twice continuously differentiable function ''f'' from '''R'''<sup>''n''</sup> to '''R''', it states that ''f''(''X'') is a semimartingale and,\n:<math>df(X_t)= \\sum_{i=1}^n f_{i}(X_t)\\,dX^i_t + \\frac{1}{2}\\sum_{i,j=1}^n f_{i,j}(X_{t})\\,d[X^i,X^j]_t.</math>\nThis differs from the chain rule used in standard calculus due to the term involving the quadratic covariation [''X''<sup>''i''</sup>,''X''<sup>''j''</sup>&nbsp;]. The formula can be generalized to non-continuous semimartingales by adding a pure jump term to ensure that the jumps of the left and right hand sides agree (see [[Itô's lemma]]).\n\n==Martingale integrators==\n\n===Local martingales===\nAn important property of the Itô integral is that it preserves the [[local martingale]] property. If ''M'' is a local martingale and ''H'' is a locally bounded predictable process then ''H''&nbsp;·&nbsp;''M'' is also a local martingale. For integrands which are not locally bounded, there are examples where ''H''&nbsp;·&nbsp;''M'' is not a local martingale. However, this can only occur when ''M'' is not continuous. If ''M'' is a continuous local martingale then a predictable process ''H'' is ''M''-integrable if and only if\n:<math>\\int_0^t H^2 d[M] <\\infty,</math>\nfor each ''t'', and ''H''&nbsp;·&nbsp;''M'' is always a local martingale.\n\nThe most general statement for a discontinuous local martingale ''M'' is that if (''H''<sup>2</sup>&nbsp;·&nbsp;[''M''])<sup>1/2</sup> is [[Stopping time#Localization|locally integrable]] then ''H''&nbsp;·&nbsp;''M'' exists and is a local martingale.\n\n===Square integrable martingales===\nFor bounded integrands, the Itô stochastic integral preserves the space of ''square integrable'' martingales, which is the set of [[càdlàg]] martingales ''M'' such that E[''M<sub>t</sub>''<sup>2</sup>] is finite for all ''t''. For any such square integrable martingale ''M'', the quadratic variation process [''M''] is integrable, and the '''Itô isometry''' states that\n:<math>\\mathbb{E}\\left [(H\\cdot M_t)^2\\right ]=\\mathbb{E}\\left [\\int_0^t H^2\\,d[M]\\right ].</math>\nThis equality holds more generally for any martingale ''M'' such that ''H''<sup>2</sup>&nbsp;·&nbsp;[''M'']<sub>''t''</sub> is integrable. The Itô isometry is often used as an important step in the construction of the stochastic integral, by defining ''H''&nbsp;·&nbsp;''M'' to be the unique extension of this isometry from a certain class of simple integrands to all bounded and predictable processes.\n\n===''p''-Integrable martingales===\nFor any ''p''&nbsp;>&nbsp;1, and bounded predictable integrand, the stochastic integral preserves the space of ''p''-integrable martingales. These are càdlàg martingales such that E(|''M<sub>t</sub>''|<sup>''p''</sup>) is finite for all&nbsp;''t''. However, this is not always true in the case where ''p''&nbsp;=&nbsp;1. There are examples of integrals of bounded predictable processes with respect to martingales which are not themselves martingales.\n\nThe maximum process of a càdlàg process ''M'' is written as ''M*<sub>t</sub>'' = sup<sub>''s''&nbsp;≤''t''</sub>&nbsp;|''M<sub>s</sub>''|. For any ''p''&nbsp;≥&nbsp;1 and bounded predictable integrand, the stochastic integral preserves the space of càdlàg martingales ''M'' such that E[(''M*<sub>t</sub>'')<sup>''p''</sup>] is finite for all ''t''. If ''p''&nbsp;>&nbsp;1 then this is the same as the space of ''p''-integrable martingales, by [[Doob's martingale inequality|Doob's inequalities]].\n\nThe '''Burkholder–Davis–Gundy inequalities''' state that, for any given ''p''&nbsp;≥&nbsp;1, there exist positive constants&nbsp;''c'',&nbsp;''C'' that depend on&nbsp;''p'', but not ''M'' or on ''t'' such that\n\n:<math>c\\mathbb{E} \\left [ [M]_t^{\\frac{p}{2}} \\right ] \\le \\mathbb{E}\\left [(M^*_t)^p \\right ]\\le C\\mathbb{E}\\left [ [M]_t^{\\frac{p}{2}} \\right ]</math>\n\nfor all càdlàg local martingales ''M''. These are used to show that if (''M*<sub>t</sub>'')<sup>p</sup> is integrable and ''H'' is a bounded predictable process then\n\n:<math>\\mathbb{E}\\left [ ((H\\cdot M)_t^*)^p \\right ] \\le C\\mathbb{E}\\left [(H^2\\cdot[M]_t)^{\\frac{p}{2}} \\right ]<\\infty</math>\n\nand, consequently, ''H''&nbsp;·&nbsp;''M'' is a ''p''-integrable martingale. More generally, this statement is true whenever (''H''<sup>2</sup>&nbsp;·&nbsp;[''M''])<sup>''p''/2</sup> is integrable.\n\n==Existence of the integral==\nProofs that the Itô integral is well defined typically proceed by first looking at very simple integrands, such as piecewise constant, left continuous and adapted processes where the integral can be written explicitly. Such ''simple predictable'' processes are linear combinations of terms of the form ''H<sub>t</sub>'' = ''A'''''1'''<sub>{''t'' > ''T''}</sub> for stopping times ''T'' and ''F<sub>T</sub>''-measurable random variables ''A'', for which the integral is\n:<math>H\\cdot X_t\\equiv \\mathbf{1}_{\\{t>T\\}}A(X_t-X_T).</math>\nThis is extended to all simple predictable processes by the linearity of ''H'' · ''X'' in ''H''.\n\nFor a Brownian motion ''B'', the property that it has [[independent increments]] with zero mean and variance Var(''B<sub>t</sub>'')&nbsp;=&nbsp;''t'' can be used to prove the Itô isometry for simple predictable integrands,\n:<math> \\mathbb{E} \\left [ (H\\cdot B_t)^2\\right ] = \\mathbb{E} \\left [\\int_0^tH_s^2\\,ds\\right ].</math>\nBy a [[continuous linear extension]], the integral extends uniquely to all predictable integrands satisfying\n:<math> \\mathbb{E} \\left[ \\int_0^t H^2 ds \\right ] < \\infty,</math>\nin such way that the Itô isometry still holds. It can then be extended to all ''B''-integrable processes by [[Stopping time#Localization|localization]]. This method allows the integral to be defined with respect to any Itô process.\n\nFor a general semimartingale ''X'', the decomposition ''X''&nbsp;=&nbsp;''M''&nbsp;+&nbsp;''A'' into a local martingale ''M'' plus a finite variation process ''A'' can be used. Then, the integral can be shown to exist separately with respect to ''M'' and ''A'' and combined using linearity, ''H'' · ''X''&nbsp;=&nbsp;''H'' · ''M''&nbsp;+&nbsp;''H'' · ''A'', to get the integral with respect to ''X''. The standard [[Lebesgue–Stieltjes integral]] allows integration to be defined with respect to finite variation processes, so the existence of the Itô integral for semimartingales will follow from any construction for local martingales.\n\nFor a càdlàg square integrable martingale ''M'', a generalized form of the Itô isometry can be used. First, the [[Doob–Meyer decomposition theorem]] is used to show that a decomposition ''M''<sup>2</sup>&nbsp;=&nbsp;''N''&nbsp;+&nbsp;<''M''> exists, where ''N'' is a martingale and <''M''> is a right-continuous, increasing and predictable process starting at zero. This uniquely defines <''M''>, which is referred to as the ''predictable quadratic variation'' of ''M''. The Itô isometry for square integrable martingales is then\n\n:<math>\\mathbb{E} \\left [(H\\cdot M_t)^2\\right ]= \\mathbb{E} \\left [\\int_0^tH^2_s\\,d\\langle M\\rangle_s\\right],</math>\n\nwhich can be proved directly for simple predictable integrands. As with the case above for Brownian motion, a continuous linear extension can be used to uniquely extend to all predictable integrands satisfying ''E''[''H''<sup>2</sup>&nbsp;·&nbsp;<''M''><sub>''t''</sub>]&nbsp;<&nbsp;∞. This method can be extended to all local square integrable martingales by localization. Finally, the Doob–Meyer decomposition can be used to decompose any local martingale into the sum of a local square integrable martingale and a finite variation process, allowing the Itô integral to be constructed with respect to any semimartingale.\n\nMany other proofs exist which apply similar methods but which avoid the need to use the Doob–Meyer decomposition theorem, such as the use of the quadratic variation [''M''] in the Itô isometry, the use of the [[Doléans measure]] for [[submartingale]]s, or the use of the [[Burkholder–Davis–Gundy inequalities]] instead of the Itô isometry. The latter applies directly to local martingales without having to first deal with the square integrable martingale case.\n\nAlternative proofs exist only making use of the fact that ''X'' is càdlàg, adapted, and the set {''H'' · ''X<sub>t</sub>'':&nbsp;|''H''| ≤ 1 is simple previsible} is bounded in probability for each time ''t'', which is an alternative definition for ''X'' to be a semimartingale. A continuous linear extension can be used to construct the integral for all left-continuous and adapted integrands with right limits everywhere (caglad or L-processes). This is general enough to be able to apply techniques such as Itô's lemma {{Harv|Protter|2004}}. Also, a [[Khintchine inequality]] can be used to prove the dominated convergence theorem and extend the integral to general predictable integrands {{Harv|Bichteler|2002}}.\n\n==Differentiation in Itô calculus==\nThe Itô calculus is first and foremost defined as an integral calculus as outlined above. However, there are also different notions of \"derivative\" with respect to Brownian motion:\n\n===Malliavin derivative===\n[[Malliavin calculus]] provides a theory of differentiation for random variables defined over [[Wiener space]], including an integration by parts formula {{Harv|Nualart|2006}}.\n\n===Martingale representation===\nThe following result allows to express martingales as Itô integrals: if ''M'' is a square-integrable martingale on a time interval [0,&nbsp;''T''] with respect to the filtration generated by a Brownian motion ''B'', then there is a unique [[adapted process|adapted]] square integrable process α on [0,&nbsp;''T''] such that\n\n:<math>M_{t} = M_{0} + \\int_{0}^{t} \\alpha_{s} \\, \\mathrm{d} B_{s}</math>\n\nalmost surely, and for all ''t''&nbsp;∈&nbsp;[0,&nbsp;''T''] {{Harv|Rogers|Williams|2000|loc=Theorem 36.5}}.  This representation theorem can be interpreted formally as saying that α is the \"time derivative\" of ''M'' with respect to Brownian motion ''B'', since α is precisely the process that must be integrated up to time ''t'' to obtain ''M''<sub>''t''</sub>&nbsp;−&nbsp;''M''<sub>0</sub>, as in deterministic calculus.\n\n==Itô calculus for physicists==\nIn physics, usually [[stochastic differential equation]]s (SDEs), such as [[Langevin equation]]s, are used, rather than stochastic integrals. Here an Itô stochastic differential equation (SDE) is often formulated via\n\n:<math> \\dot{x}_k=h_k+g_{kl} \\xi_l,</math>\n\nwhere <math>\\xi_j</math> is Gaussian white noise with\n\n:<math>\\langle\\xi_k(t_1)\\,\\xi_l(t_2)\\rangle=\\delta_{kl}\\delta(t_1-t_2)</math>\n\nand [[Einstein notation|Einstein's summation convention]] is used.\n\nIf <math>y=y(x_k)</math> is a function of the ''x<sub>k</sub>'', then [[Itô's lemma]] has to be used:\n\n:<math> \\dot{y}=\\frac{\\partial y}{\\partial x_j}\\dot{x}_j+\\frac{1}{2}\\frac{\\partial^2 y}{\\partial x_k \\, \\partial x_l} g_{km}g_{ml}. </math>\n\nAn Itô SDE as above also corresponds to a [[Stratonovich integral|Stratonovich SDE]] which reads\n\n:<math> \\dot{x}_k = h_k + g_{kl} \\xi_l - \\frac{1}{2} \\frac{\\partial g_{kl}}{\\partial {x_m}} g_{ml}.</math>\n\nSDEs frequently occur in physics in Stratonovich form, as limits of stochastic differential equations driven by [[colored noise]] if the correlation time of the noise term approaches zero.\nFor a recent treatment of different interpretations of stochastic differential equations see for example {{Harv|Lau|Lubensky|2007}}.\n\n== Itô interpretation and supersymmetric theory of SDEs ==\n{{Main|Supersymmetric theory of stochastic dynamics}}\nIn the [[Supersymmetric theory of stochastic dynamics|supersymmetric theory of SDEs]], stochastic evolution is defined via stochastic evolution operator (SEO) acting on [[Differential form|differential forms]] of the phase space. The Itô-Stratonovich dilemma takes the form of the ambiguity of the operator ordering that arises on the way from the path integral to the operator representation of stochastic evolution. The Itô interpretation corresponds to the operator ordering convention that all the momentum operators act after all the position operators. The SEO can be made unique by supplying it with its most natural mathematical definition of the [[Pullback (differential geometry)|pullback]] induced by the noise-configuration-dependent SDE-defined [[Diffeomorphism|diffeomorphisms]] and averaged over the noise configurations. This disambiguation leads to the [[Stratonovich integral|Stratonovich]] interpretation of SDEs that can be turned into the Itô interpretation by a specific shift of the flow vector field of the SDE.\n\n==See also==\n{{Portal|Mathematics|Statistics}}\n*[[Stochastic calculus]]\n*[[Wiener process]]\n*[[Itô's lemma]]\n*[[Stratonovich integral]]\n*[[Semimartingale]]\n\n==References==\n*{{Citation|last=Bichteler|first=Klaus|year=2002|title=Stochastic Integration With Jumps|publisher=[[Cambridge University Press]]|edition=1st|isbn=0-521-81129-5}}\n*{{Citation|last=Cohen|first=Samuel|last2=Elliott|first2=Robert|year=2015|title=Stochastic Calculus and Applications|publisher=[[Birkhaueser]]|edition=2nd|isbn=978-1-4939-2867-5}}\n*[[Hagen Kleinert]] (2004). ''Path Integrals in Quantum Mechanics, Statistics, Polymer Physics, and Financial Markets'', 4th edition, World Scientific (Singapore); Paperback {{ISBN|981-238-107-4}}. Fifth edition available online: [http://www.physik.fu-berlin.de/~kleinert/b5 PDF-files], with generalizations of Itô's lemma for non-Gaussian processes.\n*{{Citation|last=He|first=Sheng-wu|last2=Wang|first2=Jia-gang|last3=Yan|first3=Jia-an|year=1992|title=Semimartingale Theory and Stochastic Calculus|publisher=Science Press, CRC Press Inc.|isbn=978-0849377150}}\n*{{Citation|last=Karatzas|first=Ioannis|last2=Shreve|first2=Steven|year=1991|title=Brownian Motion and Stochastic Calculus|publisher=Springer|edition=2nd|isbn=0-387-97655-8}}\n*{{Citation|last=Lau|first=Andy|last2=Lubensky|first2=Tom|year=2007|title=State-dependent diffusion|pages=011123|journal=Phys. Rev. E|issue=1|volume=76|doi=10.1103/PhysRevE.76.011123|bibcode=2007PhRvE..76a1123L|arxiv=0707.2234}}\n*{{Citation|last=Nualart|first=David|title=The Malliavin calculus and related topics|year=2006|publisher=Springer|isbn=3-540-28328-5}}\n* {{Citation | author=Øksendal, Bernt K. | authorlink=Bernt Øksendal | title=Stochastic Differential Equations: An Introduction with Applications | publisher=Springer| location=Berlin | year=2003 | isbn=3-540-04758-1}}\n*{{Citation|last=Protter|first=Philip E.|year=2004|title=Stochastic Integration and Differential Equations|publisher=Springer|edition=2nd|isbn=3-540-00313-4}}\n* {{Citation|last=Revuz|first= Daniel|last2=Yor|first2=Marc | title=Continuous martingales and Brownian motion | publisher=Springer| location=Berlin | year=1999 | isbn=3-540-57622-3}}\n* {{Citation|last=Rogers|first=Chris|last2=Williams|first2=David | title=Diffusions, Markov processes and martingales - Volume 2: Itô calculus | publisher=Cambridge University Press| location=Cambridge | year=2000 | isbn=0-521-77593-0}}\n* Mathematical Finance Programming in TI-Basic, which implements Ito calculus for TI-calculators.\n\n{{integral}}\n{{Stochastic processes}}\n\n{{DEFAULTSORT:Ito Calculus}}\n[[Category:Definitions of mathematical integration]]\n[[Category:Stochastic calculus]]"
    },
    {
      "title": "Khinchin integral",
      "url": "https://en.wikipedia.org/wiki/Khinchin_integral",
      "text": "In mathematics, the '''Khinchin integral''' (sometimes spelled '''Khintchine integral'''), also known as the '''Denjoy–Khinchin integral''', '''generalized Denjoy integral''' or '''wide Denjoy integral''', is one of a number of definitions of the [[integral]] of a [[function (mathematics)|function]].  It is a generalization of the [[Riemann integral|Riemann]] and [[Lebesgue integral|Lebesgue]] integrals. It is named after [[Aleksandr Khinchin]] and [[Arnaud Denjoy]], but is not to be confused with the (narrow) [[Henstock–Kurzweil integral|Denjoy integral]].\n\n==Motivation==\n\nIf ''g''&nbsp;:&nbsp;''I''&nbsp;→&nbsp;'''R''' is a Lebesgue-integrable function on some interval ''I''&nbsp;=&nbsp;<nowiki>[</nowiki>''a'',''b''<nowiki>]</nowiki>, and if\n\n:<math>f(x) = \\int_a^x g(t)\\,dt</math>\n\nis its Lebesgue indefinite integral, then the following assertions are true:<ref>{{harv | Gordon | 1994 | loc=theorem 4.12 }}</ref>\n\n#''f'' is absolutely continuous (see below)\n#''f'' is differentiable [[almost everywhere]]\n#Its derivative coincides almost everywhere with ''g''(''x'').  (In fact, ''all'' absolutely continuous functions are obtained in this manner.<ref>{{harv | Gordon | 1994 | loc=theorem 4.14 }}</ref>)\n\nThe Lebesgue integral could be defined as follows: ''g'' is Lebesgue-integrable on ''I'' iff there exists a function ''f'' that is absolutely continuous whose derivative coincides with ''g'' almost everywhere.\n\nHowever, even if ''f''&nbsp;:&nbsp;''I''&nbsp;→&nbsp;'''R''' is differentiable ''everywhere'', and ''g'' is its derivative, it does not follow that ''f'' is (up to a constant) the Lebesgue indefinite integral of ''g'', simply because ''g'' can fail to be Lebesgue-integrable, i.e., ''f'' can fail to be absolutely continuous.  An example of this is given<ref>{{harv | Bruckner | 1994 | loc=chapter 5, §2 }}</ref> by the derivative ''g'' of the (differentiable but not absolutely continuous) function ''f''(''x'')=''x''²·sin(1/''x''²) (the function ''g'' is not Lebesgue-integrable around 0).\n\nThe Denjoy integral corrects this lack by ensuring that the derivative of any function ''f'' that is everywhere differentiable (or even differentiable everywhere except for at most countably many points) is integrable, and its integral reconstructs ''f'' up to a constant; the Khinchin integral is even more general in that it can integrate the ''approximate'' derivative of an approximately differentiable function (see below for definitions).  To do this, one first finds a condition that is weaker than absolute continuity but is satisfied by any approximately differentiable function. This is the concept of ''generalized'' absolute continuity; generalized absolutely continuous functions will be exactly those functions which are indefinite Khinchin integrals.\n\n==Definition==\n\n===Generalized absolutely continuous function===\n\nLet ''I''&nbsp;=&nbsp;<nowiki>[</nowiki>''a'',''b''<nowiki>]</nowiki> be an interval and ''f''&nbsp;:&nbsp;''I''&nbsp;→&nbsp;'''R''' be a real-valued function on ''I''.\n\nRecall that ''f'' is [[Absolute continuity|absolutely continuous]] on a subset ''E'' of ''I'' if and only if for every positive number ''ε'' there is a positive number ''δ'' such that whenever a finite collection <nowiki>[</nowiki>''x''<sub>''k''</sub>,''y''<sub>''k''</sub><nowiki>]</nowiki> of pairwise disjoint subintervals of ''I'' with endpoints in ''E'' satisfies\n\n:<math>\\sum_{k} \\left| y_k - x_k \\right| < \\delta</math>\n\nit also satisfies\n\n:<math> \\sum_k | f(y_k) - f(x_k) | < \\varepsilon.</math>\n\nDefine<ref>{{harv | Bruckner | 1994 | loc=chapter 5, §4 }}</ref><ref>{{harv | Gordon | 1994 | loc=definition 6.1 }}</ref> the function ''f'' to be ''generalized absolutely continuous'' on a subset ''E'' of ''I'' if the restriction of ''f'' to ''E'' is continuous (on ''E'') and ''E'' can be written as a countable union of subsets ''E''<sub>''i''</sub> such that ''f'' is absolutely continuous on each ''E''<sub>''i''</sub>.  This is equivalent<ref>{{harv | Gordon | 1994 | loc=theorem 6.10 }}</ref> to the statement that every nonempty [[Derived set (mathematics)|perfect]] subset of ''E'' contains a portion<ref>A ''portion'' of a perfect set ''P'' is a ''P''&nbsp;∩&nbsp;<nowiki>[</nowiki>''u'',&nbsp;''v''<nowiki>]</nowiki> such that this intersection is perfect and nonempty.</ref> on which ''f'' is absolutely continuous.\n\n===Approximate derivative===\n\nLet ''E'' be a [[Lebesgue measure|Lebesgue measurable]] set of reals. Recall that a real number ''x'' (not necessarily in ''E'') is said to be a ''[[Lebesgue's density theorem|point of density]]'' of ''E'' when\n\n: <math>\\lim_{\\varepsilon\\to 0} \\frac{\\mu(E \\cap [x-\\varepsilon,x+\\varepsilon])}{2\\varepsilon} = 1</math>\n\n(where ''μ'' denotes Lebesgue measure).  A Lebesgue-measurable function ''g''&nbsp;:&nbsp;''E''&nbsp;→&nbsp;'''R''' is said to have ''[[approximate limit]]''<ref>{{harv | Bruckner | 1994 | loc=chapter 10, §1 }}</ref> ''y'' at ''x'' (a point of density of ''E'') if for every positive number ''ε'', the point ''x'' is a point of density of <math>g^{-1}([y-\\varepsilon,y+\\varepsilon])</math>. (If furthermore ''g''(''x'') &nbsp;=&nbsp;''y'', we can say that ''g'' is ''approximately continuous'' at ''x''.<ref>{{harv | Gordon | 1994 | loc=theorem 14.5 }}</ref>) Equivalently, ''g'' has approximate limit ''y'' at ''x'' if and only if there exists a measurable subset ''F'' of ''E'' such that ''x'' is a point of density of ''F'' and the (usual) limit at ''x'' of the restriction of ''f'' to ''F'' is ''y''. Just like the usual limit, the approximate limit is unique if it exists.\n\nFinally, a Lebesgue-measurable function ''f''&nbsp;:&nbsp;''E''&nbsp;→&nbsp;'''R''' is said to have ''approximate derivative'' ''y'' at ''x'' iff\n\n:<math>\\frac{f(x')-f(x)}{x'-x}</math>\n\nhas approximate limit ''y'' at ''x''; this implies that ''f'' is approximately continuous at ''x''.\n\n===A theorem===\n\nRecall that it follows from [[Lusin's theorem]] that a Lebesgue-measurable function is approximately continuous almost everywhere (and conversely).<ref>{{harv | Bruckner | 1994 | loc=theorem 5.2 }}</ref><ref>{{harv | Gordon | 1994 | loc=theorem 14.7 }}</ref>  The key theorem in constructing the Khinchin integral is this: a function ''f'' that is generalized absolutely continuous (or even of \"generalized bounded variation\", a weaker notion) has an approximate derivative almost everywhere.<ref>{{harv | Bruckner | 1994 | loc=chapter 10, theorem 1.2 }}</ref><ref>{{harv | Gordon | 1994 | loc=theorem 14.11 }}</ref><ref>{{harv | Filippov | 1998 | loc=chapter IV, theorem 6.1 }}</ref>  Furthermore, if ''f'' is generalized absolutely continuous and its approximate derivative is nonnegative almost everywhere, then ''f'' is nondecreasing,<ref>{{harv | Gordon | 1994 | loc=theorem 15.2 }}</ref> and consequently, if this approximate derivative is zero almost everywhere, then ''f'' is constant.\n\n===The Khinchin integral===\n\nLet ''I''&nbsp;=&nbsp;<nowiki>[</nowiki>''a'',''b''<nowiki>]</nowiki> be an interval and ''g''&nbsp;:&nbsp;''I''&nbsp;→&nbsp;'''R''' be a real-valued function on ''I''.  The function ''g'' is said to be Khinchin-integrable on ''I'' iff there exists a function ''f'' that is generalized absolutely continuous whose approximate derivative coincides with ''g'' almost everywhere;<ref>{{harv | Gordon | 1994 | loc=definition 15.1 }}</ref> in this case, the function ''f'' is determined by ''g'' up to a constant, and the Khinchin-integral of ''g'' from ''a'' to ''b'' is defined as ''f''(''b'')&nbsp;−&nbsp;''f''(''a'').\n\n===A particular case===\n\nIf ''f''&nbsp;:&nbsp;''I''&nbsp;→&nbsp;'''R''' is continuous and has an approximate derivative everywhere on ''I'' except for at most countably many points, then ''f'' is, in fact, generalized absolutely continuous, so it is the (indefinite) Khinchin-integral of its approximate derivative.<ref>{{harv | Gordon | 1994 | loc=theorem 15.4 }}</ref>\n\nThis result does not hold if the set of points where ''f'' is not assumed to have an approximate derivative is merely of Lebesgue measure zero, as the [[Cantor function]] shows.\n\n==Notes==\n{{reflist|3}}\n\n==References==\n* [http://www.encyclopediaofmath.org/index.php/Denjoy_integral Springer Encyclopedia of Mathematics: article \"Denjoy integral\"]\n* [http://www.encyclopediaofmath.org/index.php/Approximate_derivative Springer Encyclopedia of Mathematics: article \"Approximate derivative\"]\n* {{Cite book | last=Bruckner | first=Andrew | title=Differentiation of Real Functions | year=1994 | publisher=[[American Mathematical Society]] | isbn=978-0-8218-6990-1 | ref=harv }}\n* {{Cite book | last=Gordon | first=Russell A. | title=The Integrals of Lebesgue, Denjoy, Perron, and Henstock | year=1994 | publisher=[[American Mathematical Society]] | isbn=978-0-8218-3805-1 | ref=harv }}\n* {{Cite book | last=Filippov | first=V.V. | title=Basic Topological Structures of Ordinary Differential Equations | year=1998 | isbn=978-0-7923-4951-8 | ref=harv }}\n\n{{integral}}\n\n[[Category:Definitions of mathematical integration]]"
    },
    {
      "title": "Kolmogorov integral",
      "url": "https://en.wikipedia.org/wiki/Kolmogorov_integral",
      "text": "In [[mathematics]], the '''Kolmogorov integral''' (or '''Kolmogoroff integral''') is a generalized  integral introduced by {{harvs|txt|last=Kolmogoroff|authorlink=Andrey Kolmogorov|year=1930}} including the [[Lebesgue–Stieltjes integral]], the [[Burkill integral]], and  the [[Hellinger integral]] as special cases.\n\n==References==\n\n*{{Citation | last1=Kolmogoroff | first1=A. | authorlink=Andrey Kolmogorov | title=Untersuchungen über den Integralbegriff | doi=10.1007/BF01455714 | year=1930 | journal=[[Mathematische Annalen]] | issn=0025-5831 | volume=103 | issue=1 | pages=654–696}}\n*{{eom|id=k/k055720|first=V. A. |last=Skvortsov}}\n\n{{integral}}\n[[Category:Definitions of mathematical integration]]"
    },
    {
      "title": "Lebesgue integration",
      "url": "https://en.wikipedia.org/wiki/Lebesgue_integration",
      "text": "[[File:Integral-area-under-curve.svg|thumb|The integral of a positive function can be interpreted as the area under a curve.]]\n{{Calculus |Integral}}\n\nIn [[mathematics]], the [[integral]] of a non-negative [[Function (mathematics)|function]] of a single variable can be regarded, in the simplest case, as the [[area]] between the [[Graph of a function|graph]] of that function and the {{math|''x''}}-axis. The '''Lebesgue integral''' extends the integral to a larger class of functions. It also extends the [[Domain (mathematics)|domain]]s on which these functions can be defined.\n\nLong before the 20th century, mathematicians already understood that for non-negative functions with a [[Smooth function|smooth]] enough graph—such as [[continuous function]]s on [[Closed set|closed]] [[Bounded set|bounded]] [[Interval (mathematics)|interval]]s—the ''area under the curve'' could be defined as the integral, and computed using approximation techniques on the region by [[polygon]]s.  However, as the need to consider more irregular functions arose—e.g., as a result of the [[Limit of a function|limiting]] processes of [[mathematical analysis]] and the mathematical [[theory of probability]]—it became clear that more careful approximation techniques were needed to define a suitable integral.  Also, one might wish to integrate on spaces more general than the real line. The Lebesgue integral provides abstractions needed to do this important job.\n\nThe Lebesgue integral plays an important role in probability theory, [[real analysis]], and many other fields in the mathematical sciences. It is named after [[Henri Lebesgue]] (1875–1941), who introduced the integral {{harv|Lebesgue|1904}}. It is also a pivotal part of the [[axiomatic theory of probability]].\n\nThe term ''Lebesgue integration'' can mean either the general theory of integration of a function with respect to a general [[measure (mathematics)|measure]], as introduced by Lebesgue, or the specific case of integration of a function defined on a sub-domain of the [[real line]] with respect to the [[Lebesgue measure]].\n\n== Introduction ==\nThe integral of a positive function {{math|''f''}} between limits {{math|''a''}} and {{math|''b''}} can be interpreted as the area under the graph of {{math|''f''}}. This is easy to understand for familiar functions such as [[polynomials]], but what does it mean for more exotic functions? In general, for which class of functions does \"area under the curve\" make sense? The answer to this question has great theoretical and practical importance.\n\nAs part of a general movement toward [[Mathematical rigor|rigor]] in mathematics in the nineteenth century, mathematicians attempted to put integral calculus on a firm foundation. The [[Riemann integral]]—proposed by [[Bernhard Riemann]] (1826–1866)—is a broadly successful attempt to provide such a foundation. Riemann's definition starts with the construction of a sequence of easily calculated areas that converge to the integral of a given function. This definition is successful in the sense that it gives the expected answer for many already-solved problems, and gives useful results for many other problems.\n\nHowever, Riemann integration does not interact well with taking limits of sequences of functions, making such limiting processes difficult to analyze. This is important, for instance, in the study of [[Fourier series]], [[Fourier transform]]s, and other topics. The Lebesgue integral is better able to describe how and when it is possible to take limits under the integral sign (via the powerful [[monotone convergence theorem]] and [[dominated convergence theorem]]).\n\nWhile the Riemann integral considers the area under a curve as made out of vertical rectangles, the Lebesgue definition considers horizontal slabs that are not necessarily just rectangles, and so it is more flexible. For this reason, the Lebesgue definition makes it possible to calculate integrals for a broader class of functions. For example, the [[Dirichlet function]], which is 0 where its argument is [[irrational number|irrational]] and 1 otherwise, has a Lebesgue integral, but does not have a Riemann integral.  Furthermore, the Lebesgue integral of this function is zero, which agrees with the intuition that when picking a real number uniformly at random from the unit interval, the probability of picking a rational number should be zero.\n\nLebesgue summarized his approach to integration in a letter to [[Paul Montel]]:\n{{quote|I have to pay a certain sum, which I have collected in my pocket.  I take the bills and coins out of my pocket and give them to the creditor in the order I find them until I have reached the total sum. This is the Riemann integral. But I can proceed differently. After I have taken all the money out of my pocket I order the bills and coins according to identical values and then I pay the several heaps one after the other to the creditor. This is my integral.|sign=<small>''Source'': {{harv|Siegmund-Schultze|2008}}</small>}}\n\nThe insight is that one should be able to rearrange the values of a function freely, while preserving the value of the integral.  This process of rearrangement can convert a very [[Pathological (mathematics)|pathological function]] into one that is \"nice\" from the point of view of integration, and thus let such pathological functions be integrated.\n\n=== Intuitive interpretation ===\n[[Image:RandLintegrals.png|thumb|250px|Riemann-Darboux's integration (in blue) and Lebesgue integration (in red).]]\nTo get some intuition about the different approaches to integration, let us imagine that we want to find a mountain's volume (above sea level).\n\n;The Riemann–Darboux approach: Divide the base of the mountain into a grid of 1 meter squares. Measure the altitude of the mountain at the center of each square. The volume on a single grid square is approximately 1&nbsp;m<sup>2</sup> × (that square's altitude), so the total volume is 1&nbsp;m<sup>2</sup> times the sum of the altitudes.\n\n;The Lebesgue approach: Draw a [[contour map]] of the mountain, where adjacent contours are 1&nbsp;meter of altitude apart. The volume of earth a single contour contains is approximately 1&nbsp;m × (that contour's area), so the total volume is the sum of these areas times 1&nbsp;m.\n\nFolland summarizes the difference between the Riemann and Lebesgue approaches thus: \"to compute the Riemann integral of {{math|''f''}}, one partitions the domain {{math|[''a'', ''b'']}} into subintervals\", while in the Lebesgue integral, \"one is in effect partitioning the range of {{math|''f''}} .\"<ref name=\"Folland\">{{cite book |first=Gerald B. |last=Folland |title=Real Analysis: Modern Techniques and Their Applications |location= |publisher=Wiley |year=1984 |page=56 |url=https://books.google.com/books?id=AnIPAQAAMAAJ&pg=PA56 }}</ref>\n\n=== Towards a formal definition ===\n[[File:Horizontal slice for Lebesgue.svg|thumb|right|A measurable function is shown, together with the set <math>\\{x|f(x)>t\\}</math> (on the x-axis).  The Lebesgue integral is obtained by slicing along the y-axis, using the 1-dimensional Lebesgue measure to measure the \"width\" of the slices.\n]]\nTo define the Lebesgue integral requires the formal notion of a [[measure (mathematics)|measure]] that, roughly, associates to each set {{math|''A''}} of real numbers a nonnegative number {{math|μ(''A'')}} representing the \"size\" of {{math|''A''}}.  This notion of \"size\" should agree with the usual length of an interval or disjoint union of intervals.  Suppose that {{math|''f'' : ℝ → ℝ<sup>+</sup>}} is a non-negative real-valued function.  Using the \"partitioning the range of {{math|''f''}} \" philosophy, the integral of {{math|''f''}} should be the sum over {{math|''t''}} of the elementary area contained in the thin horizontal strip between {{math|''y'' {{=}} ''t'' and ''y'' {{=}} ''t'' &minus; ''dt''}}.  This elementary area is just\n:<math>\\mu \\left (\\{x\\mid f(x)>t\\} \\right ) \\,dt.</math>\nLet\n:<math>f^*(t)=\\mu \\left (\\{x\\mid f(x)>t\\} \\right ).</math>\nThe Lebesgue integral of {{math|''f''}} is then defined by<ref>{{harvnb|Lieb|Loss|2001}}</ref>\n:<math>\\int f\\,d\\mu = \\int_0^\\infty f^*(t)\\,dt</math>\nwhere the integral on the right is an ordinary [[improper Riemann integral]].  Note that {{math|''f''<sup>∗</sup>}} is a non-negative decreasing function, and therefore has a well-defined improper Riemann integral with value in the interval {{math|[0,&infin;]}}.  For a suitable class of functions (the [[measurable function]]s), this defines the Lebesgue integral.\n\nA general (not necessarily positive) measurable function {{math|''f''}} is Lebesgue integrable if the area between the graph of {{math|''f''}} and the {{math|''x''}}-axis is finite:\n:<math>\\int |f|\\,d\\mu < + \\infty.</math>\nIn that case, as in the Riemannian case, the integral is the difference between the area above the {{math|''x''}}-axis and the area below the {{math|''x''}}-axis:\n:<math>\\int f \\,d\\mu = \\int f^+ \\,d\\mu - \\int f^- \\,d\\mu</math>\nwhere <math>f=f^+ - f^-</math> is the decomposition of ''f'' into the difference of two non-negative functions given by\n:<math>\\begin{align}\n f^+(x)&=\\max\\{f(x),0\\} &=&\\begin{cases}\n               f(x), & \\text{if } f(x) > 0, \\\\\n               0, & \\text{otherwise}\n             \\end{cases}\\\\\n f^-(x)&=\\max\\{-f(x),0\\} &=&\\begin{cases}\n               -f(x), & \\text{if } f(x) < 0, \\\\\n               0, & \\text{otherwise.}\n             \\end{cases}\n\\end{align}</math>\n\n== Construction ==\nThe theory of the Lebesgue integral requires a theory of measurable sets and measures on these sets, as well as a theory of measurable functions and integrals on these functions.\n\n=== Measure theory ===\n{{further|Measure (mathematics)}}\n[[Measure theory]] was initially created to provide a useful abstraction of the notion of length of subsets of the real line—and, more generally, area and volume of subsets of Euclidean spaces. In particular, it provided a systematic answer to the question of which subsets of {{math|ℝ}} have a length. As later [[set theory]] developments showed (see [[non-measurable set]]), it is actually impossible to assign a length to all subsets of {{math|ℝ}} in a way that preserves some natural additivity and translation invariance properties.  This suggests that picking out a suitable class of ''measurable'' subsets is an essential prerequisite.\n\nThe Riemann integral uses the notion of length explicitly. Indeed, the element of calculation for the Riemann integral is the rectangle {{math|[''a'', ''b''] × [''c'', ''d'']}}, whose area is calculated to be {{math|(''b'' − ''a'')(''d'' − ''c'')}}. The quantity {{math|''b'' − ''a''}} is the length of the base of the rectangle and {{math|''d'' − ''c''}}  is the height of the rectangle.  Riemann could only use planar rectangles to approximate the area under the curve, because there was no adequate theory for measuring more general sets.\n\nIn the development of the theory in most modern textbooks (after 1950), the approach to measure and integration is ''axiomatic''.  This means that a measure is any function μ defined on a certain class {{math|''X'' }} of subsets of a set {{math|''E''}}, which satisfies a certain list of properties. These properties can be shown to hold in many different cases.\n\n=== Measurable functions ===\nWe start with a [[measure space]] {{math|(''E'', ''X'', μ)}} where {{math|''E''}} is a [[Set (mathematics)|set]], {{math|''X''}} is a [[sigma-algebra|σ-algebra]] of subsets of {{math|''E''}}, and μ is a (non-[[Signed measure|negative]]) [[measure (mathematics)|measure]] on {{math|''E''}} defined on the sets of {{math|''X''}}.\n\nFor example,  {{math|''E''}} can be [[Euclidean space|Euclidean {{math|''n''}}-space]] {{math|ℝ<sup>''n''</sup>}} or some [[Lebesgue measure|Lebesgue measurable]] subset of it, {{math|''X''}} is the [[σ-algebra]] of all Lebesgue measurable subsets of {{math|''E''}}, and μ is the Lebesgue measure. In the mathematical theory of probability, we confine our study to a [[probability]] measure&nbsp;{{math|μ}}, which satisfies {{math|μ(''E'') {{=}} 1}}.\n\nLebesgue's theory defines integrals for a class of functions called [[measurable function]]s. A real-valued function {{math|''f''}} on {{math|''E''}} is measurable if the [[pre-image]] of every interval of the form {{math|(''t'', ∞)}} is in {{math|''X''}}:\n\n:<math> \\{x\\,\\mid\\,f(x) > t\\} \\in X\\quad \\forall t\\in\\mathbb{R}. </math>\n\nWe can show that this is equivalent to requiring that the pre-image of any [[Borel algebra|Borel]] subset of ℝ be in {{math|''X''}}. The set of measurable functions is closed under algebraic operations, but more importantly it is closed under various kinds of [[Limit superior and limit inferior|point-wise sequential limits]]:\n\n: <math> \\sup_{k \\in \\mathbb{N}} f_k, \\quad \\liminf_{k \\in \\mathbb{N}} f_k, \\quad \\limsup_{k \\in \\mathbb{N}} f_k </math>\n\nare measurable if the original sequence {{math|(''f''<sub>''k''</sub>)<sub>''k''</sub>}}, where {{math|''k'' ∈  ℕ}}, consists of measurable functions.\n\nThere are several approaches for defining an integral:\n\n: <math> \\int_E f \\, d \\mu = \\int_E f\\left(x\\right)\\, d\\mu\\left(x\\right)</math>\n\nfor measurable real-valued functions {{math|''f''}} defined on {{math|''E''}}.  \n\n===Constructing the integral===\n[[File:Lebesgueintegralsimplefunctions.svg|right|thumb|Approximating a function by simple functions.]]\nOne approach to constructing the Lebesgue integral is to make use of so-called ''simple functions'': finite real-linear combinations of ''indicator functions''.  Simple functions can be used to approximate a measurable function, by partitioning the range into layers.  The integral of a simple function is equal to the measure of a given layer, times the height of that layer.  The integral of a non-negative general measurable function is then defined as an appropriate [[supremum]] of approximations by simple functions, and the integral of a (not necessarily positive) measurable function is the difference of two integrals of non-negative measurable functions, as mentioned [[#Towards a formal definition|earlier]].\n\n==== Indicator functions ==== \nTo assign a value to the integral of the [[indicator function]] {{math|1<sub>''S''</sub>}} of a measurable set {{math|''S''}} consistent with the given measure μ, the only reasonable choice is to set:\n\n:<math>\\int 1_S  \\,  d\\mu = \\mu (S).</math>\n\nNotice that the result may be equal to {{math|+∞}}, unless {{math|μ}} is a ''finite'' measure.\n\n==== Simple functions ====\nA finite [[linear combination]] of indicator functions\n\n:<math>\\sum_k a_k 1_{S_k}</math>\n\nwhere the coefficients {{math|''a''<sub>''k''</sub>}} are real numbers and the sets {{math|''S<sub>k</sub>''}} are measurable, is called a measurable [[simple function]]. We extend the integral by linearity to ''non-negative'' measurable simple functions.  When the coefficients {{math|''a<sub>k</sub>''}} are non-negative, we set\n\n:<math>\\int \\left(\\sum_k a_k 1_{S_k}\\right) \\, d \\mu = \\sum_k a_k \\int 1_{S_k} \\, d \\mu = \\sum_k a_k \\, \\mu(S_k). </math>\n\nThe convention {{math|0 × ∞ {{=}} 0}} must be used, and the result may be infinite. Even if a simple function can be written in many ways as a linear combination of indicator functions, the integral is always the same. This can be shown using the additivity property of measures.\n\nSome care is needed when defining the integral of a ''real-valued'' simple function, to avoid the undefined expression {{math|∞ − ∞}}: one assumes that the representation\n\n:<math> f = \\sum_k a_k 1_{S_k}</math>\n\nis such that {{math|μ(''S''<sub>''k''</sub>) < ∞}} whenever {{math|''a''<sub>''k''</sub> ≠ 0}}. Then the above formula for the integral of ''f'' makes sense, and the result does not depend upon the particular representation of {{math|''f''}} satisfying the assumptions.\n\nIf {{math|''B''}} is a measurable subset of {{math|''E''}} and {{math|''s''}} is a measurable simple function one defines\n\n:<math> \\int_B s \\, d\\mu = \\int 1_B \\, s \\, d\\mu = \\sum_k a_k \\, \\mu(S_k \\cap B). </math>\n\n==== Non-negative functions ====\nLet {{math|''f''}} be a non-negative measurable function on {{math|''E''}}, which we allow to attain the value {{math|+∞}}, in other words, {{math|''f''}} takes non-negative values in the [[extended real number line]].  We define\n\n:<math>\\int_E f \\, d\\mu = \\sup\\left\\{\\,\\int_E s\\, d\\mu : 0 \\le s \\le f,\\ s\\ \\text{simple}\\,\\right\\}.</math>\n\nWe need to show this integral coincides with the preceding one, defined on the set of simple functions, when ''E''  is a segment [''a'',&nbsp;''b'']. There is also the question of whether this corresponds in any way to a Riemann notion of integration. It is possible to prove that the answer to both questions is yes.\n\nWe have defined the integral of ''f'' for any non-negative extended real-valued measurable function on&nbsp;''E''. For some functions, this integral  ∫<sub>''E''</sub>&nbsp;''f''&nbsp;dμ  is infinite.\n\nIt is often useful to have a particular sequence of simple functions that approximates the Lebesgue integral well (analogously to a Riemann sum).  For a non-negative measurable function {{math|''f''}}, let <math>s_n(x)</math> be the simple function whose value is <math>k/2^n</math> whenever <math>k/2^n\\le f(x)<(k+1)/2^n</math>, for ''k'' a non-negative integer less than (say) <math>4^n</math>.  Then it can be proven directly that\n:<math>\\int f\\,d\\mu = \\lim_{n\\to\\infty} \\int s_n\\,d\\mu</math>\nand that the limit on the right hand side exists as an extended real number.  This bridges the connection between the approach to the Lebesgue integral using simple functions, and the motivation for the Lebesgue integral using a partition of the range.\n\n==== Signed functions ====\nTo handle signed functions, we need a few more definitions. If {{math|''f''}} is a measurable function of the set {{math|''E''}} to the reals (including {{math|±∞}}), then we can write\n\n:<math> f = f^+ - f^-, \\quad </math>\n\nwhere\n\n:<math> f^+(x) = \\left\\{\\begin{matrix} f(x) & \\text{if } f(x) > 0 \\\\ 0 & \\text{otherwise} \\end{matrix}\\right. </math>\n:<math> f^-(x) = \\left\\{\\begin{matrix} -f(x) & \\text{if }  f(x) < 0 \\\\ 0 & \\text{otherwise} \\end{matrix}\\right. </math>\n\nNote that both {{math|''f''<sup>+</sup>}} and {{math|''f''<sup>−</sup>}} are non-negative measurable functions. Also note that\n\n:<math> |f| = f^+ + f^-. \\quad </math>\n\nWe say that the Lebesgue integral of the measurable function {{math|''f''}} ''exists'', or ''is defined'' if at least one of <math> \\int f^+ \\, d\\mu </math> and <math> \\int f^- \\, d\\mu </math> is finite:\n\n:<math> \\min\\left(\\int f^+ \\, d \\mu, \\int f^- \\, d \\mu\\right) < \\infty. </math>\n\nIn this case we ''define''\n\n:<math> \\int f \\, d \\mu  =  \\int f^+ \\, d\\mu - \\int f^- \\, d\\mu. </math>\n\nIf\n\n:<math> \\int |f| \\, d \\mu < \\infty, </math>\n\nwe say that {{math|''f''}} is ''Lebesgue integrable''.\n\nIt turns out that this definition gives the desirable properties of the integral.\n\n==== Complex valued functions ==== \n[[Complex number|Complex]] valued functions can be similarly integrated, by considering the real part and the imaginary part separately.\n\nIf ''h''=''f''+''ig'' for real-valued integrable functions ''f'', ''g'', then the integral of ''h'' is defined by\n\n:<math> \\int h \\, d \\mu = \\int f \\, d \\mu + i \\int g \\, d \\mu.</math>\n\nThe function is Lebesgue integrable if and only if its [[absolute value]] is Lebesgue integrable (see [[Absolutely integrable function]]).\n\n=== Example ===\nConsider the [[indicator function]] of the rational numbers, {{math|1<sub>'''Q'''</sub>}}.  This function is [[nowhere continuous]].\n\n* <math>1_{\\mathbf Q}</math> '''is not Riemann-integrable on'''  {{math|[0, 1]}}:  No matter how the set {{math|[0, 1]}} is partitioned into subintervals, each partition contains at least one rational and at least one irrational number, because rationals and irrationals are both dense in the reals.  Thus the upper [[Darboux sum]]s are all one, and the lower Darboux sums are all zero.\n* <math>1_{\\mathbf Q}</math> '''is Lebesgue-integrable on ''' {{math|[0, 1]}} using the [[Lebesgue measure]]:  Indeed, it is the indicator function of the rationals so by definition\n\n::<math> \\int_{[0,1]} 1_{\\mathbf Q} \\, d \\mu = \\mu(\\mathbf{Q} \\cap [0,1]) = 0,</math>\n\n:because {{math|'''Q'''}} is [[countable]].\n\n=== Domain of integration ===\nA technical issue in Lebesgue integration is that the domain of integration is defined as a ''set'' (a subset of a measure space), with no notion of orientation. In elementary calculus, one defines integration with respect to an [[orientation (manifold)|orientation]]:\n:<math>\\int_b^a f := - \\int_a^b f.</math>\nGeneralizing this to higher dimensions yields integration of [[differential form]]s. By contrast, Lebesgue integration provides an alternative generalization, integrating over subsets with respect to a measure; this can be notated as\n:<math>\\int_A f\\,d\\mu = \\int_{[a,b]} f\\,d\\mu</math>\nto indicate integration over a subset {{math|''A''}}. For details on the relation between these generalizations, see {{section link|Differential form|Relation with measures}}.\n\n== Limitations of the Riemann integral ==\nHere we discuss the limitations of the Riemann integral and the greater scope offered by the Lebesgue integral. This discussion presumes a working understanding of the [[Riemann integral]].\n\nWith the advent of [[Fourier series]], many analytical problems involving integrals came up whose satisfactory solution required interchanging limit processes and integral signs. However, the conditions under which the integrals\n\n: <math> \\sum_k \\int f_k(x) dx,  \\quad \\int \\left [\\sum_k f_k(x) \\right ] dx  </math>\n\nare equal proved quite elusive in the Riemann framework. There are some other technical difficulties with the Riemann integral. These are linked with the limit-taking difficulty discussed above.\n\n'''Failure of monotone convergence'''. As shown above, the [[indicator function]] {{math|1<sub>'''Q'''</sub>}} on the rationals is not Riemann integrable.  In particular, the [[Monotone convergence theorem]] fails. To see why, let {{math|{''a''<sub>''k''</sub>}}} be an enumeration of all the rational numbers in {{math|[0, 1]}} (they are [[countable]] so this can be done.) Then let\n:<math> g_k(x) = \\left\\{\\begin{matrix} 1 & \\text{if }  x = a_j, j\\leq k \\\\ 0 & \\text{otherwise} \\end{matrix} \\right. </math>\n\nThe function {{math|''g''<sub>''k''</sub>}} is zero everywhere, except on a finite set of points. Hence its Riemann integral is zero. Each {{math|''g''<sub>''k''</sub>}} is non-negative, and this sequence of functions is monotonically increasing, but its limit as {{math|''k'' → ∞}} is {{math|1<sub>'''Q'''</sub>}}, which is not Riemann integrable.\n\n'''Unsuitability for unbounded intervals'''. The Riemann integral can only integrate functions on a bounded interval. It can however be extended to unbounded intervals by taking limits, so long as this doesn't yield an answer such as {{math|∞ − ∞}}.\n\n'''Integrating on structures other than Euclidean space'''.  The Riemann integral is inextricably linked to the order structure of the real line.\n\n== Basic theorems of the Lebesgue integral ==\n\nThe Lebesgue integral has the following properties:\n\n[[Equivalence relation]] of equality [[almost everywhere|almost-everywhere]]: The Lebesgue integral does not distinguish between functions that differ only on a set of μ-measure zero. To make this precise, functions {{math|''f''}} and {{math|''g''}} are said to be equal almost everywhere (a.e.) if\n\n:<math> \\mu(\\{x \\in E: f(x) \\neq g(x)\\}) = 0. </math>\n\n* If {{math|''f'', ''g''}} are non-negative measurable functions (possibly assuming the value {{math|+∞}}) such that {{math|''f'' {{=}} ''g''}} almost everywhere, then\n\n:<math> \\int f \\, d \\mu =  \\int g \\, d \\mu. </math>\n\nTo wit, the integral respects the equivalence relation of almost-everywhere equality.\n\n* If {{math|''f'', ''g''}} are functions such that {{math|''f'' {{=}} ''g''}} almost everywhere, then {{math|''f''}} is Lebesgue integrable if and only if {{math|''g''}} is Lebesgue integrable, and the integrals of {{math|''f''}} and {{math|''g''}} are the same if they exist.\n\n[[linear transformation|Linearity]]: If {{math|''f''}} and {{math|''g''}} are Lebesgue integrable functions and {{math|''a''}} and {{math|''b''}} are real numbers, then {{math|''af'' + ''bg''}} is Lebesgue integrable and\n\n:<math> \\int (a f + bg) \\, d \\mu = a \\int f \\, d\\mu + b \\int g \\, d\\mu. </math>\n\n[[Monotonic]]ity: If {{math|''f'' ≤ ''g''}}, then\n\n:<math> \\int f \\, d \\mu \\leq  \\int g \\, d \\mu. </math>\n\n[[Lebesgue's monotone convergence theorem|Monotone convergence theorem]]: Suppose {{math|{ ''f''<sub>''k''</sub>}<sub>''k'' ∈ ℕ</sub>}}  is a sequence of non-negative measurable functions such that\n\n:<math>  f_k(x) \\leq f_{k+1}(x) \\quad \\forall k\\in \\mathbb{N}, \\, \\forall x \\in E. </math>\n\nThen, the pointwise limit {{math|''f''}} of {{math|''f''<sub>''k''</sub>}} is Lebesgue measurable and\n\n:<math> \\lim_k \\int f_k \\, d\\mu = \\int f \\, d \\mu. </math>\n\nThe value of any of the integrals is allowed to be infinite.\n\n[[Fatou's lemma]]: If {{math|{ ''f''<sub>''k''</sub>}<sub>''k'' ∈ '''N'''</sub>}} is a sequence of non-negative measurable functions, then\n\n:<math> \\int \\liminf_k f_k \\, d \\mu  \\leq  \\liminf_k \\int f_k \\, d \\mu.</math>\n\nAgain, the value of any of the integrals may be infinite.\n\n[[Dominated convergence theorem]]: Suppose {{math|{ ''f''<sub>''k''</sub>}<sub>''k'' ∈ '''N'''</sub>}} is a sequence of complex measurable functions with pointwise limit {{math|''f''}}, and there is a Lebesgue integrable function {{math|''g''}} (i.e., {{math|''g''}} belongs to the {{math|[[Lp space|space ''L''<sup>1</sup>]])}} such that {{math|{{!}} ''f''<sub>''k''</sub> {{!}} ≤ ''g''}} for all {{math|''k''}}.\n\nThen, {{math|''f''}} is Lebesgue integrable and\n\n:<math> \\lim_k \\int f_k \\, d \\mu = \\int f \\, d \\mu. </math>\n\n== Proof techniques ==\nTo illustrate some of the proof techniques used in Lebesgue integration theory, we sketch a proof of the above-mentioned Lebesgue monotone convergence theorem.  Let {{math|{ ''f''<sub>''k''</sub>}<sub>''k'' ∈ '''N'''</sub>}} be a non-decreasing sequence of non-negative measurable functions and put\n\n:<math> f = \\sup_{k \\in \\mathbb{N}} f_k = \\lim_{k \\in \\mathbb{N}} f_k. </math>\n\nBy the monotonicity property of the integral, it is immediate that:\n\n: <math> \\int f \\, d\\mu \\geq \\lim_k \\int f_k \\, d\\mu </math>\n\nand the limit on the right exists, because the sequence is monotonic. We now prove the inequality in the other direction.  It follows from the definition of integral that there is a non-decreasing sequence {{math|(''g''<sub>''n''</sub>)}} of non-negative simple functions such that {{math|''g''<sub>''n''</sub> ≤ ''f'' }} and\n\n:<math> \\lim_n \\int g_n \\, d\\mu = \\int f \\, d\\mu. </math>\n\nTherefore,  it suffices to prove that for each {{math|''n'' ∈  ℕ}},\n\n:<math>  \\int g_n \\, d\\mu \\leq \\lim_k \\int f_k \\, d\\mu. </math>\n\nWe will show that if {{math|''g''}} is a simple function and\n\n:<math> \\lim_k f_k(x) \\geq g(x) </math>\n\nalmost everywhere, then\n\n:<math> \\lim_k \\int f_k \\, d\\mu \\geq \\int g \\, d\\mu.</math>\n\nBy breaking up the function {{math|''g''}} into its constant value parts, this reduces to the case in which {{math|''g''}} is the indicator function of a set. The result we have to prove is then\n\n<blockquote>Suppose {{math|''A''}} is a measurable set and {{math|{ ''f''<sub>''k''</sub>}<sub>''k'' ∈  ℕ</sub>}} is a nondecreasing sequence of non-negative measurable functions on {{math|''E''}}  such that\n\n:<math> \\lim_k f_k (x) \\geq 1 </math>\n\nfor almost all {{math|''x'' ∈ ''A''}}. Then\n\n:<math> \\lim_k \\int f_k \\, d\\mu \\geq \\mu(A). </math></blockquote>\n\nTo prove this result, fix {{math|''ε'' > 0}} and define the sequence of measurable sets\n\n:<math> B_k = \\{x \\in A: f_k(x) \\geq 1 - \\varepsilon \\}. </math>\n\nBy monotonicity of the integral, it follows that for any {{math|''k'' ∈  ℕ}},\n\n:<math> (1 - \\varepsilon) \\mu(B_k) = \\int (1 - \\varepsilon) 1_{B_k} \\, d\\mu \\leq \\int f_k \\, d\\mu </math>\n\nBecause almost every {{math|''x''}} is in {{math|''B<sub>k</sub>''}} for large enough {{math|''k''}}, we have\n\n:<math> \\bigcup_k B_k = A, </math>\n\nup to a set of measure {{math|0}}. Thus by countable additivity of {{math|''μ''}}, and because {{math|''B<sub>k</sub>''}} increases with&nbsp;{{math|''k''}},\n\n:<math> \\mu(A) = \\lim_k \\mu(B_k) \\leq \\lim_k (1 - \\varepsilon)^{-1} \\int f_k \\, d\\mu. </math>\n\nAs this is true for any positive {{math|''ε''}} the result follows.\n\nFor another Proof of the Monotone Convergence Theorem, we follow:<ref name=\"Folland\" />\n\nLet {{math|(''X'', ''M'', ''μ'')}} be a measure space.\n\n{{math|{ ''f''<sub>''n''</sub>}{{null}}}} is an increasing sequence of numbers, therefore its limit exists, even if it is equal to {{math|∞}}. We know that\n\n:<math> \\int f_n \\leq \\int f</math>\n\nfor all {{math|''n''}}, so that\n\n:<math> \\lim \\limits_{n \\rightarrow \\infty} \\int f_n \\leq \\int f</math>.\n\nNow we need to establish the reverse inequality. Fix {{math|''α'' ∈ (0, 1)}}, let {{math|''ϕ''}} be a simple function with {{math|0 ≤ ''ϕ'' ≤ ''f''}} and let\n\n:<math> E_n = \\{x : f_n (x) \\geq \\alpha \\phi(x)\\} </math>.\n\nThen {{math|{E<sub>n</sub>}{{null}}}} is an increasing sequence of measurable sets with <math> \\bigcup \\limits^\\infty E_n = X</math>. We know that\n\n:<math> \\int f_n \\geq \\int \\limits_{E_n} f_n \\geq \\alpha \\int \\limits_{E_n} \\phi </math>.\n\nThis is true for all {{math|''n''}}, including the limit:\n\n:<math> \\lim \\int \\limits_{E_n} \\phi = \\int \\phi</math>.\n\nHence,\n\n:<math> \\lim \\int f_n \\geq \\alpha \\int \\phi </math>.\n\nThis was true for all {{math|''α'' ∈ (0, 1)}}, so it remains true for {{math|1=''α'' = 1}}, and taking the supremum over simple {{math|''ϕ'' ≤ ''f''}} by the definition of integration in {{math|''L''<sup>+</sup>}},\n\n:<math> \\lim \\int f_n \\geq \\int f</math>.\n\nNow we have both inequalities, so we've shown the Monotone Convergence theorem:\n\n:<math> \\lim \\int f_n = \\int f </math>\n\nfor {{math|''f''<sub>{''n''+1}</sub> ≥ ''f''<sub>''n''</sub>}}, and {{math|''f''<sub>n</sub> → ''f''}} pointwise, {{math|{''f''<sub>n</sub>} ∈ ''L''<sup>+</sup>}}, the set of positive measurable functions from {{math|''X'' → [0, ∞]}}.\n\n== Alternative formulations ==\n\nIt is possible to develop the integral with respect to the Lebesgue measure without relying on the full machinery of measure theory. One such approach is provided by the [[Daniell integral]].\n\nThere is also an alternative approach to developing the theory of integration via methods of [[functional analysis]].  The Riemann integral exists for any continuous function {{math|''f''}} of [[compact space|compact]] [[support (mathematics)|support]] defined on {{math|ℝ<sup>''n''</sup>}} (or a fixed open subset).  Integrals of more general functions can be built starting from these integrals.\n\nLet {{math|''C<sub>c</sub>''}} be the space of all real-valued compactly supported continuous functions of ℝ. Define a norm on {{math|''C<sub>c</sub>''}} by\n\n: <math> \\left\\| f \\right\\| = \\int |f(x)| \\, dx .</math>\n\nThen {{math|''C<sub>c</sub>''}} is a normed vector space (and in particular, it is a metric space.) All metric spaces have [[complete space|Hausdorff completions]], so let {{math|''L''<sup>1</sup>}} be its completion. This space is isomorphic to the space of Lebesgue integrable functions modulo the subspace of functions with integral zero. Furthermore, the Riemann integral {{math|∫}} is a [[uniformly continuous]] functional with respect to the norm on {{math|''C<sub>c</sub>''}}, which is dense in {{math|''L''<sup>1</sup>}}. Hence {{math|∫}} has a unique extension to all of {{math|''L''<sup>1</sup>}}. This integral is precisely the Lebesgue integral.\n\nMore generally, when the measure space on which the functions are defined is also a [[Locally compact space|locally compact]] [[topological space]] (as is the case with the real numbers ℝ), measures compatible with the topology in a suitable sense ([[Radon measure]]s, of which the Lebesgue measure is an example) an integral with respect to them can be defined in the same manner, starting from the integrals of [[continuous function]]s with [[compact support]]. More precisely, the compactly supported functions form a [[vector space]] that carries a natural [[topological space|topology]], and a (Radon) measure is defined as a continuous [[linear map|linear]] functional on this space.  The value of a measure at a compactly supported function is then also by definition the integral of the function. One then proceeds to expand the measure (the integral) to more general functions by continuity, and defines the measure of a set as the integral of its indicator function. This is the approach taken by {{Harvtxt|Bourbaki|2004}} and a certain number of other authors. For details see [[Radon measure#Radon measures on locally compact spaces|Radon measures]].\n\n== Limitations of Lebesgue integral ==\nThe main purpose of the Lebesgue integral is to provide an integral notion where limits of integrals hold under mild assumptions. There is no guarantee that every function is Lebesgue integrable. But it may happen that [[improper integral]]s exist for functions that are not Lebesgue integrable. One example would be\n:[[Sinc function|<math>\\frac{\\sin(x)}{x}</math>]]\nover the entire real line. This function is not Lebesgue integrable, as\n:<math> \\int_{-\\infty}^\\infty \\left|\\frac{\\sin(x)}{x}\\right| dx =\\infty.</math>\nOn the other hand, <math> \\int_{-\\infty}^\\infty\\frac{\\sin(x)}{x} dx</math> exists as an improper integral and can be computed to be finite; it is twice the [[Dirichlet integral]].\n\n== See also ==\n* [[Henri Lebesgue#Lebesgue's theory of integration|Henri Lebesgue]], for a non-technical description of Lebesgue integration\n* [[Null set]]\n* [[integral|Integration]]\n* [[measure (mathematics)|Measure]]\n* [[Sigma-algebra]]\n* [[Lebesgue space (disambiguation)|Lebesgue space]]\n* [[Lebesgue–Stieltjes integration]]\n* [[Henstock–Kurzweil integral]]\n\n== Notes ==\n{{reflist}}\n\n== References ==\n\n* {{cite book\n| last = Bartle\n| first = Robert G.\n| title = The elements of integration and Lebesgue measure\n| series = Wiley Classics Library\n| publisher = John Wiley &amp; Sons Inc.\n| location = New York\n| year = 1995\n| pages = xii+179\n| isbn = 0-471-04222-6\n| nopp = true\n| mr = 1312157}}\n\n* {{cite book\n| last = Bauer\n| first = Heinz\n| title = Measure and Integration Theory\n| series = De Gruyter Studies in Mathematics 26\n| publisher = De Gruyter\n| location = Berlin\n| year = 2001\n| page = 236\n| isbn = 978-3-11-016719-1\n| nopp = true}}\n\n* {{cite book\n| last = Bourbaki\n| first = Nicolas\n| authorlink = Nicolas Bourbaki\n| title = Integration. I. Chapters 1–6. Translated from the 1959, 1965 and 1967 French originals by Sterling K. Berberian\n| series = Elements of Mathematics (Berlin)\n| publisher= Springer-Verlag\n| location = Berlin\n| year = 2004\n| pages = xvi+472\n| isbn = 3-540-41129-1\n| nopp = true\n| mr = 2018901}}\n\n* {{cite book\n| last = Dudley\n| first = Richard M.\n| title = Real analysis and probability\n| series = The Wadsworth &amp; Brooks/Cole Mathematics Series\n| publisher = Wadsworth &amp; Brooks/Cole Advanced Books &amp; Software\n| location = Pacific Grove, CA\n| year = 1989\n| pages = xii+436\n| isbn = 0-534-10050-3\n| nopp = true\n| mr = 982264}} Very thorough treatment, particularly for probabilists with good notes and historical references.\n\n* {{cite book\n| last = Folland\n| first = Gerald B.\n| title = Real analysis: Modern techniques and their applications\n| series = Pure and Applied Mathematics (New York)\n| edition = Second\n| publisher = John Wiley &amp; Sons Inc.\n| location = New York\n| year = 1999\n| pages = xvi+386\n| isbn = 0-471-31716-0\n| nopp = true\n| mr = 1681462}}\n\n* {{cite book\n| last = Halmos\n| first = Paul R.\n| authorlink = Paul Halmos\n| title = Measure Theory\n| publisher = D. Van Nostrand Company, Inc.\n| location = New York, N. Y.\n| year = 1950\n| pages = xi+304\n| mr = 0033869}} A classic, though somewhat dated presentation.\n\n* {{springer|title=Lebesgue integral|id=p/l057860}}\n* {{Cite journal\n| last = Lebesgue\n| first = Henri\n| authorlink = Henri Lebesgue\n| title = Leçons sur l'intégration et la recherche des fonctions primitives\n| publisher = Gauthier-Villars\n| year = 1904\n| publication-place = Paris\n| postscript = <!--None-->\n| ref = harv}}\n\n* {{cite book\n| last = Lebesgue\n| first = Henri\n| authorlink = Henri Lebesgue\n| title = Oeuvres scientifiques (en cinq volumes)\n| publisher = Institut de Mathématiques de l'Université de Genève\n| location = Geneva\n| year = 1972\n| page = 405\n| language = French\n| mr = 0389523}}\n\n* {{cite book|last1=Lieb|first1=Elliott|authorlink1=Elliott H. Lieb|last2=Loss|first2=Michael|author2-link=Michael Loss|title=Analysis|year=2001|edition=2nd|publisher=[[American Mathematical Society]]|series=[[Graduate Studies in Mathematics]]|volume=14|isbn=978-0821827833}}\n* {{cite book\n| last = Loomis\n| first = Lynn H.\n| title = An introduction to abstract harmonic analysis\n| publisher = D. Van Nostrand Company, Inc.\n| location = Toronto-New York-London\n| year = 1953\n| pages = x+190\n| mr = 0054173}} Includes a presentation of the Daniell integral.\n\n* {{cite book\n| last = Munroe\n| first =  M. E.\n| title = Introduction to measure and integration\n| publisher = Addison-Wesley Publishing Company Inc.\n| location = Cambridge, Mass.\n| year = 1953\n| pages = x+310\n| mr = 0053186}} Good treatment of the theory of outer measures.\n\n* {{cite book\n| last = Royden\n| first = H. L.\n| title = Real analysis\n| edition = Third\n| publisher = Macmillan Publishing Company\n| location = New York\n| year = 1988\n| pages = xx+444\n| isbn = 0-02-404151-3\n| mr = 1013117}}\n\n* {{cite book\n| last = Rudin\n| first = Walter\n| authorlink = Walter Rudin\n| title = Principles of mathematical analysis\n| edition = Third\n| series = International Series in Pure and Applied Mathematics\n| publisher = McGraw-Hill Book Co.\n| location = New York\n| year = 1976\n| pages = x+342\n| mr = 0385023}}  Known as ''Little Rudin'', contains the basics of the Lebesgue theory, but does not treat material such as [[Fubini's theorem]].\n\n* {{cite book\n| last = Rudin\n| first = Walter\n| title = Real and complex analysis\n| publisher = McGraw-Hill Book Co.\n| location = New York\n| year = 1966\n| pages = xi+412\n| mr = 0210528}} Known as ''Big Rudin''. A complete and careful presentation of the theory.  Good presentation of the Riesz extension theorems. However, there is a minor flaw (in the first edition) in the proof of one of the extension theorems, the discovery of which constitutes exercise 21 of Chapter 2.\n* {{Cite journal\n| last = Saks\n| first = Stanisław\n| author-link = Stanislaw Saks\n| title = Theory of the Integral\n| place = [[Warszawa]]-[[Lwów]]\n| publisher = G.E. Stechert & Co.\n| year = 1937\n| series = [http://matwbn.icm.edu.pl/ksspis.php?wyd=10&jez=pl Monografie Matematyczne]\n| volume = 7\n| edition = 2nd\n| pages = VI+347\n| url = https://archive.org/details/theoryoftheinteg032192mbp\n| jfm = 63.0183.05 | zbl = 0017.30004\n| postscript = <!--None-->}}. English translation by [[Laurence Chisholm Young]], with two additional notes by [[Stefan Banach]].\n\n* {{cite book\n| last = Shilov\n| first = G. E.\n| last2 = Gurevich\n| first2 = B. L.\n| title = Integral, measure and derivative: a unified approach. Translated from the Russian and edited by Richard A. Silverman\n| series = Dover Books on Advanced Mathematics\n| publisher = Dover Publications Inc.\n| location = New York\n| year = 1977\n| pages = xiv+233\n| isbn = 0-486-63519-8\n| nopp = true\n| mr = 0466463}} Emphasizes the [[Daniell integral]].\n\n* {{citation|last=Siegmund-Schultze|first=Reinhard|chapter=Henri Lebesgue|title=Princeton Companion to Mathematics|editors=Timothy Gowers, June Barrow-Green, Imre Leader|year=2008|publisher=Princeton University Press}}.\n* {{cite book\n| last = Teschl\n| first = Gerald\n| authorlink = Gerald Teschl\n| title = Topics in Real and Functional Analysis\n| publisher = (lecture notes)\n| url = http://www.mat.univie.ac.at/~gerald/ftp/book-fa/index.html}}\n\n* {{cite book\n| last = Yeh\n| first = James\n| title = Real Analysis: Theory of Measure and Integral 2nd. Edition Paperback\n| publisher = World Scientific Publishing Company Pte. Ltd.\n| location = Singapore\n| year =2006\n| page = 760\n| isbn = 978-981-256-6}}\n\n{{Integral}}\n\n{{Authority control}}\n\n[[Category:Definitions of mathematical integration]]\n[[Category:Measure theory]]"
    },
    {
      "title": "Lebesgue–Stieltjes integration",
      "url": "https://en.wikipedia.org/wiki/Lebesgue%E2%80%93Stieltjes_integration",
      "text": "In [[measure theory|measure-theoretic]] [[Mathematical analysis|analysis]] and related branches of [[mathematics]], '''Lebesgue–Stieltjes integration''' generalizes [[Riemann–Stieltjes integral|Riemann–Stieltjes]] and [[Lebesgue integration]], preserving the many advantages of the former in a more general measure-theoretic framework.  The Lebesgue–Stieltjes integral is the ordinary Lebesgue integral with respect to a measure known as the Lebesgue–Stieltjes measure, which may be associated to any function of [[bounded variation]] on the real line.  The Lebesgue–Stieltjes measure is a [[regular Borel measure]], and conversely every regular Borel measure on the real line is of this kind.\n\nLebesgue–Stieltjes [[integral]]s, named for [[Henri Leon Lebesgue]] and [[Thomas Joannes Stieltjes]], are also known as '''Lebesgue–Radon integrals''' or just '''Radon integrals''', after [[Johann Radon]], to whom much of the theory is due.  They find common application in [[probability theory|probability]] and [[stochastic process]]es, and in certain branches of [[mathematical analysis|analysis]] including [[potential theory]].\n\n==Definition==\nThe Lebesgue–Stieltjes integral\n\n:<math>\\int_a^b f(x)\\,dg(x)</math>\n\nis defined when &thinsp;<math>f : \\left[a, b\\right] \\rightarrow \\mathbb R</math>&nbsp; is [[Borel measure|Borel]]-[[measurable function|measurable]]\nand [[bounded function|bounded]] and &thinsp;<math>g : \\left[a, b\\right] \\rightarrow \\mathbb R</math>&nbsp; is of [[bounded variation]] in {{math|[''a'', ''b'']}} and right-continuous, or when {{math|&thinsp;''f''&thinsp;}} is non-negative and {{mvar|g}} is [[monotone function|monotone]] and [[Continuous_function#Directional_and_semi-continuity|right-continuous]]. To start, assume that {{math|&thinsp;''f''&thinsp;}} is non-negative and {{mvar|g}} is monotone non-decreasing and right-continuous. Define {{math|''w''((''s'', ''t'']) {{=}} ''g''(''t'') − ''g''(''s'')}} and {{math|''w''({''a''}) {{=}} 0}} (Alternatively, the construction works for {{mvar|g}} left-continuous, {{math|''w''([''s'',''t'')) {{=}} ''g''(''t'') − ''g''(''s'')}} and {{math|''w''({''b''}) {{=}} 0}}).\n\nBy [[Carathéodory's extension theorem]], there is a unique Borel measure {{math|''μ<sub>g</sub>''}} on {{math|[''a'', ''b'']}} which agrees with {{mvar|w}} on every interval {{mvar|I}}.  The measure {{math|''μ<sub>g</sub>''}} arises from an [[outer measure]] (in fact, a [[metric outer measure]]) given by\n\n:<math>\\mu_g(E) = \\inf\\left\\{\\sum_i \\mu_g(I_i) \\ : \\  E\\subset \\bigcup_i I_i \\right\\}</math>\n\nthe [[infimum]] taken over all coverings of {{mvar|E}} by countably many semiopen intervals. This measure is sometimes called<ref>Halmos (1974), Sec. 15</ref> the '''Lebesgue–Stieltjes measure''' associated with {{mvar|g}}.\n\nThe Lebesgue–Stieltjes integral\n\n:<math>\\int_a^b f(x)\\,dg(x)</math>\n\nis defined as the [[Lebesgue integral]] of {{math|&thinsp;''f''&thinsp;}} with respect to the measure {{math|''μ<sub>g</sub>''}} in the usual way. If {{mvar|g}} is non-increasing, then define\n\n:<math>\\int_a^b f(x)\\,dg(x) := -\\int_a^b f(x) \\,d (-g)(x),</math>\n\nthe latter integral being defined by the preceding construction.\n\nIf {{mvar|g}} is of bounded variation and {{math|&thinsp;''f''&thinsp;}} is bounded, then it is possible to write\n\n:<math>dg(x)=dg_1(x)-dg_2(x)</math>\n\nwhere {{math|''g''<sub>1</sub>(''x'') {{=}} ''V''{{su|b=''a''|p=&nbsp;''x''}}''g''}} is the [[total variation]]\nof {{mvar|g}} in the interval {{math|[''a'', ''x'']}}, and {{math|''g''<sub>2</sub>(''x'') {{=}} ''g''<sub>1</sub>(''x'') − ''g''(''x'')}}. Both {{math|''g''<sub>1</sub>}} and {{math|''g''<sub>2</sub>}} are monotone non-decreasing. Now the Lebesgue–Stieltjes integral with respect to {{mvar|g}} is defined by\n\n:<math>\\int_a^b f(x)\\,dg(x) = \\int_a^b f(x)\\,dg_1(x)-\\int_a^b f(x)\\,dg_2(x),</math>\n\nwhere the latter two integrals are well-defined by the preceding construction.\n\n===Daniell integral===\nAn alternative approach {{harv|Hewitt|Stromberg|1965}} is to define the Lebesgue–Stieltjes integral as the [[Daniell integral]] that extends the usual Riemann–Stieltjes integral.  Let {{mvar|g}} be a non-decreasing right-continuous function on {{math|[''a'', ''b'']}}, and define {{math|''I''(&thinsp;''f''&thinsp;)}} to be the Riemann–Stieltjes integral\n\n:<math>I(f) = \\int_a^b f(x)\\,dg(x)</math>\n\nfor all continuous functions {{math|&thinsp;''f''&thinsp;}}. The [[functional (mathematics)|functional]] {{mvar|I}} defines a [[Radon measure]] on {{math|[''a'', ''b'']}}.  This functional can then be extended to the class of all non-negative functions by setting\n\n:<math>\\begin{align}\n\\overline{I}(h) &= \\sup \\left \\{I(f) \\ : \\ f\\in C[a,b], 0\\le f\\le h \\right \\} \\\\\n\\overline{\\overline{I}}(h) &= \\inf \\left \\{I(f) \\ : \\ f \\in C[a,b], h\\le f \\right \\}.\n\\end{align}</math>\n\nFor Borel measurable functions, one has\n\n:<math>\\overline{I}(h) = \\overline{\\overline{I}}(h),</math>\n\nand either side of the identity then defines the Lebesgue–Stieltjes integral of {{mvar|h}}.  The outer measure {{math|''μ<sub>g</sub>''}} is defined via\n\n:<math>\\mu_g(A) = \\overline{\\overline{I}}(\\chi_A)</math>\n\nwhere {{math|''χ<sub>A</sub>''}} is the [[indicator function]] of {{mvar|A}}.\n\nIntegrators of bounded variation are handled as above by decomposing into positive and negative variations.\n\n==Example==\nSuppose that {{math|&thinsp;''γ'' : [''a'', ''b''] → '''R'''<sup>2</sup>}} is a [[rectifiable curve]] in the plane and {{math|&thinsp;''ρ'' : '''R'''<sup>2</sup> → [0, ∞)}} is Borel measurable. Then we may define the length of {{mvar|γ}} with respect to the Euclidean metric weighted by ρ to be\n\n:<math>\\int_a^b \\rho(\\gamma(t))\\,d\\ell(t),</math>\n\nwhere <math>\\ell(t)</math> is the length of the restriction of {{mvar|γ}} to {{math|[''a'', ''t'']}}. This is sometimes called the {{mvar|ρ}}-length of {{mvar|γ}}. This notion is quite useful for various applications: for example, in muddy terrain the speed in which a person can move may depend on how deep the mud is. If {{math|&thinsp;''ρ''(''z'')}} denotes the inverse of the walking speed at or near {{mvar|z}}, then the {{mvar|ρ}}-length of {{mvar|γ}} is the time it would take to traverse {{mvar|γ}}. The concept of [[extremal length]] uses this notion of the {{mvar|ρ}}-length of curves and is useful in the study of [[conformal map]]pings.\n\n==Integration by parts==\nA function {{math|&thinsp;''f''&thinsp;}} is said to be \"regular\" at a point {{mvar|a}} if the right and left hand limits {{math|&thinsp;''f''&thinsp;(''a''+)}} and {{math|&thinsp;''f''&thinsp;(''a''−)}} exist, and the function takes the average value,\n\n:<math>f(a)=\\frac{f(a-)+f(a+)}{2},</math>\n\nat the limiting point. Given two functions {{mvar|U}} and {{mvar|V}} of finite variation, if at each point either {{mvar|U}} or {{mvar|V}} is continuous, or if both {{mvar|U}} and {{mvar|V}} are regular, then there is an [[integration by parts]] formula for the Lebesgue–Stieltjes integral:\n\n:<math>\\int_a^b U\\,dV+\\int_a^b V\\,dU=U(b+)V(b+)-U(a-)V(a-), \\qquad b > a.</math>\n\nUnder a slight generalization of this formula, the extra conditions on {{mvar|U}} and {{mvar|V}} can be dropped.<ref>{{cite journal |last=Hewitt |first=Edwin |date=May 1960  |title=Integration by Parts for Stieltjes Integrals |journal=The American Mathematical Monthly |volume=67 |issue=5 |pages=419–423  |jstor=2309287 |doi=10.2307/2309287 }}</ref>\n\nAn alternative result, of significant importance in the theory of [[stochastic calculus]] is the following. Given two functions {{mvar|U}} and {{mvar|V}} of finite variation, which are both right-continuous and have left-limits (they are [[cadlag]] functions) then\n\n:<math>U(t)V(t) = U(0)V(0) + \\int_{(0,t]} U(s-)\\,dV(s)+\\int_{(0,t]} V(s-)\\,dU(s)+\\sum_{u\\in (0,t]} \\Delta U_u \\Delta V_u,</math>\n\nwhere {{math|Δ''U<sub>t</sub>'' {{=}} ''U''(''t'') − ''U''(''t''−)}}. This result can be seen as a precursor to [[Itō's lemma]], and is of use in the general theory of stochastic integration. The final term is {{math|Δ''U''(''t'')Δ''V''(''t'') {{=}} ''d''[''U'', ''V''],}}which arises from the quadratic covariation of {{mvar|U}} and {{mvar|V}}. (The earlier result can then be seen as a result pertaining to the [[Stratonovich integral]].)\n\n==Related concepts==\n\n===Lebesgue integration===\nWhen {{math|''g''(''x'') {{=}} ''x''}} for all real {{mvar|x}}, then {{math|''μ<sub>g</sub>''}} is the [[Lebesgue measure]], and the Lebesgue–Stieltjes integral of {{math|&thinsp;''f''&thinsp;}} with respect to {{mvar|g}} is equivalent to the [[Lebesgue integral]] of {{math|&thinsp;''f''&thinsp;}}.\n\n===Riemann–Stieltjes integration and probability theory===\nWhere {{math|&thinsp;''f''&thinsp;}} is a [[continuous function|continuous]] real-valued function of a real variable and {{mvar|v}} is a non-decreasing real function, the Lebesgue–Stieltjes integral is equivalent to the [[Riemann–Stieltjes integral]], in which case we often write\n:<math>\\int_a^b f(x) \\, dv(x)</math>\nfor the Lebesgue–Stieltjes integral, letting the measure {{math|''μ<sub>v</sub>''}} remain implicit. This is particularly common in [[probability theory]] when {{mvar|v}} is the [[cumulative distribution function]] of a real-valued random variable {{mvar|X}}, in which case \n:<math>\\int_{-\\infty}^\\infty f(x) \\, dv(x) = \\mathrm{E}[f(X)].</math>\n(See the article on [[Riemann–Stieltjes integral|Riemann–Stieltjes integration]] for more detail on dealing with such cases.)\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{Citation|last1=Halmos|first1=Paul R.|author1-link=Paul R. Halmos|title=Measure Theory | publisher=[[Springer-Verlag]] | location=Berlin, New York | isbn=978-0-387-90088-9 | year=1974}}\n*{{citation|title=Real and abstract analysis|publisher=Springer-Verlag|first1=Edwin| last1=Hewitt| first2=Karl| last2=Stromberg |year=1965}}.\n*Saks, Stanislaw (1937) ''[http://matwbn.icm.edu.pl/kstresc.php?tom=7&wyd=10 Theory of the Integral.]''\n*Shilov, G. E., and Gurevich, B. L., 1978. ''Integral, Measure, and Derivative: A Unified Approach'', Richard A. Silverman, trans. Dover Publications. {{isbn|0-486-63519-8}}.\n\n{{integral}}\n\n{{DEFAULTSORT:Lebesgue-Stieltjes integration}}\n[[Category:Definitions of mathematical integration]]"
    },
    {
      "title": "Motivic integration",
      "url": "https://en.wikipedia.org/wiki/Motivic_integration",
      "text": "'''Motivic integration''' is a notion in [[algebraic geometry]] that was introduced by [[Maxim Kontsevich]] in 1995 and was developed by [[Jan Denef]] and [[François Loeser]]. Since its introduction it has proved to be quite useful in various branches of  [[algebraic geometry]], most notably [[birational geometry]] and [[singularity theory]]. Roughly speaking, motivic integration assigns to subsets of the arc space of an algebraic variety, a volume living in the [[Grothendieck group| Grothendieck ring]] of [[algebraic variety| algebraic varieties]]. The naming 'motivic' mirrors the fact that unlike ordinary [[integral|integration]], for which the values are [[real numbers]], in motivic integration the values are geometric in nature.\n\n==References==\n{{reflist}}\n\n==External links ==\n*[http://www.ams.org/bull/2005-42-02/S0273-0979-05-01053-0/S0273-0979-05-01053-0.pdf AMS Bulletin Vol. 42][[Thomas Callister Hales| Tom Hales]]\n** ''What is motivic measure?''\n* [http://xxx.lanl.gov/abs/math.AG/9911179 math.AG/9911179] A.Craw \n**''An introduction to [[Motivic Integration|motivic integration]]''\n*[http://www.math.jussieu.fr/~loeser/notes_seattle_09_04_2008.pdf Lecture Notes (version of 2008)] [[François Loeser]]\n** ''Seattle lecture notes on motivic integration'' \n*[http://perswww.kuleuven.be/~u0005725/articles/2006/001/ArcSpacesMotivicIntegrationAndStringyInvariants.pdf Lecture Notes] W.Veys\n**''Arc spaces, motivic integration and stringy invariants''\n\n[[Category:Algebraic geometry]]\n[[Category:Definitions of mathematical integration]]"
    },
    {
      "title": "Paley–Wiener integral",
      "url": "https://en.wikipedia.org/wiki/Paley%E2%80%93Wiener_integral",
      "text": "In [[mathematics]], the '''Paley–Wiener integral''' is a simple [[stochastic integral]]. When applied to [[Abstract Wiener space|classical Wiener space]], it is less general than the [[Itō integral]], but the two agree when they are both defined.\n\nThe integral is named after its discoverers, [[Raymond Paley]] and [[Norbert Wiener]].\n\n==Definition==\nLet ''i''&nbsp;:&nbsp;''H''&nbsp;→&nbsp;''E'' be an [[abstract Wiener space]] with abstract Wiener measure ''γ'' on ''E''. Let ''j''&nbsp;:&nbsp;''E''<sup>∗</sup>&nbsp;→&nbsp;''H'' be the [[adjoint of an operator|adjoint]] of ''i''. (We have abused notation slightly: strictly speaking, ''j''&nbsp;:&nbsp;''E''<sup>∗</sup>&nbsp;→&nbsp;''H''<sup>∗</sup>, but since ''H'' is a [[Hilbert space]], it is [[Isometry|isometrically isomorphic]] to its [[dual space]] ''H''<sup>∗</sup>, by the [[Riesz representation theorem]].)\n\nIt can be shown that ''j'' is an [[injective function]] and has [[dense (topology)|dense]] [[image (function)|image]] in ''H''.{{Citation needed|date=September 2010}} Furthermore, it can be shown that every [[linear functional]] ''f''&nbsp;∈&nbsp;''E''<sup>∗</sup> is also [[square-integrable]]: in fact,\n\n:<math>\\| f \\|_{L^{2} (E, \\gamma; \\mathbb{R})} = \\| j(f) \\|_{H}</math>\n\nThis defines a natural [[linear map]] from ''j''(''E''<sup>∗</sup>) to ''L''<sup>2</sup>(''E'',&nbsp;''γ'';&nbsp;'''R'''), under which ''j''(''f'')&nbsp;∈&nbsp;''j''(''E''<sup>∗</sup>)&nbsp;⊆&nbsp;''H'' goes to the [[equivalence class]] [''f''] of ''f'' in ''L''<sup>2</sup>(''E'',&nbsp;''γ'';&nbsp;'''R'''). This is well-defined since ''j'' is injective. This map is an [[isometry]], so it is [[continuous function|continuous]].\n\nHowever, since a continuous linear map between [[Banach space]]s such as ''H'' and ''L''<sup>2</sup>(''E'',&nbsp;''γ'';&nbsp;'''R''') is uniquely determined by its values on any dense subspace of its domain, there is a unique continuous linear extension ''I''&nbsp;:&nbsp;''H''&nbsp;→&nbsp;''L''<sup>2</sup>(''E'',&nbsp;''γ'';&nbsp;'''R''') of the above natural map ''j''(''E''<sup>∗</sup>)&nbsp;→&nbsp;''L''<sup>2</sup>(''E'',&nbsp;''γ'';&nbsp;'''R''') to the whole of ''H''.\n\nThis isometry ''I''&nbsp;:&nbsp;''H''&nbsp;→&nbsp;''L''<sup>2</sup>(''E'',&nbsp;''γ'';&nbsp;'''R''') is known as the '''Paley–Wiener map'''. ''I''(''h''), also denoted &lt;''h'',&nbsp;&minus;&gt;<sup>∼</sup>, is a function on ''E'' and is known as the '''Paley–Wiener integral''' (with respect to ''h''&nbsp;∈&nbsp;''H'').\n\nIt is important to note that the Paley–Wiener integral for a particular element ''h''&nbsp;∈&nbsp;''H'' is a [[Function (mathematics)|function]] on ''E''. The notation &lt;''h'',&nbsp;''x''&gt;<sup>∼</sup> does not really denote an inner product (since ''h'' and ''x'' belong to two different spaces), but is a convenient [[abuse of notation]] in view of the [[Cameron–Martin theorem]]. For this reason, many authors{{Citation needed|date=September 2010}} prefer to write &lt;''h'',&nbsp;&minus;&gt;<sup>∼</sup>(''x'') or ''I''(''h'')(''x'') rather than using the more compact but potentially confusing &lt;''h'',&nbsp;''x''&gt;<sup>∼</sup> notation.\n\n==See also==\nOther stochastic integrals:\n* [[Itō integral]]\n* [[Skorokhod integral]]\n* [[Stratonovich integral]]\n\n{{No footnotes|date=September 2010}}\n\n==References==\n*Park, C.; Skoug, D. (1988) \"A Note on Paley-Wiener-Zygmund Stochastic Integrals\", ''Proceedings of the American Mathematical Society', 103 (2), 591&ndash;601 {{JSTOR|2047184}}\n*Elworthy, D. (2008) [http://www.tjsullivan.org.uk/pdf/MA482_Stochastic_Analysis.pdf ''MA482 Stochastic Analysis''],  Lecture Notes, University of Warwick (Section 6)\n\n{{Functional analysis}}\n\n{{DEFAULTSORT:Paley-Wiener Integral}}\n[[Category:Definitions of mathematical integration]]\n[[Category:Stochastic calculus]]"
    },
    {
      "title": "Pfeffer integral",
      "url": "https://en.wikipedia.org/wiki/Pfeffer_integral",
      "text": "In mathematics, the '''Pfeffer integral''' is an integration technique created by [[Washek Pfeffer]] as an attempt to extend the [[Henstock–Kurzweil integral]] to a multidimensional domain. This was to be done in such a way that the [[fundamental theorem of calculus]] would apply analogously to the theorem in one dimension, with as few preconditions on the function under consideration as possible. The integral also permits analogues of the chain rule and other theorems of the integral calculus for higher dimensions.\n\n==Definition==\nThe construction is based on the Henstock or gauge integral, however Pfeffer proved that the integral, at least in the one dimensional case, is less general than the Henstock integral. It relies on what Pfeffer refers to as a '''set of bounded variation''', this is equivalent to a [[Caccioppoli set]]. The Riemann sums of the Pfeffer integral are taken over partitions made up of such sets, rather than intervals as in the Riemann or Henstock integrals. A gauge is used, exactly as in the Henstock integral, except that the gauge function may be zero on a negligible set.\n\n==Properties==\nPfeffer defined a notion of generalized absolute continuity <math>ACG^*</math>, close to but not equal to the definition of a function being <math>ACG_*</math>, and proved that a function is Pfeffer integrable if it is the derivative of an <math>ACG^*</math> function. He also proved a chain rule for the Pfeffer integral. In one dimension his work as well as similarities between the Pfeffer integral and the [[Henstock–Kurzweil integral#McShane integral|McShane integral]] indicate that the integral is more general than the [[Lebesgue integral]] and yet less general than the [[Henstock–Kurzweil integral]].\n\n==Bibliography==\n*{{Citation\n| surname1 = Bongiorno\n| given1 = Benedetto\n| surname2 = Pfeffer\n| given2 = Washek\n| title = A concept of absolute continuity and a Riemann type integral\n| journal = Comment. Math. Univ. Carolinae\n| volume = 33\n| issue = 2\n| year = 1992\n| page = 189&ndash;196}}\n*{{Citation\n| surname1 = Pfeffer\n| given1 = Washek\n| title = A Riemann type definition of a variational integral\n| journal = Proc. Amer. Math. Soc.\n| volume = 114\n| year = 1992\n| page = 99&ndash;106}}\n\n{{integral}}\n\n[[Category:Definitions of mathematical integration]]"
    },
    {
      "title": "Regulated integral",
      "url": "https://en.wikipedia.org/wiki/Regulated_integral",
      "text": "In [[mathematics]], the '''regulated integral''' is a definition of [[Integral|integration]] for [[regulated function]]s, which are defined to be [[uniform norm|uniform limits]] of [[step function]]s.  The use of the regulated integral instead of the [[Riemann integral]] has been advocated by [[Nicolas Bourbaki]] and [[Jean Dieudonné]].\n\n==Definition==\n\n===Definition on step functions===\nLet [''a'', ''b''] be a fixed [[closed set|closed]], [[bounded set|bounded]] [[interval (mathematics)|interval]] in the [[real line]] '''R'''. A real-valued function ''φ'' : [''a'',&nbsp;''b''] → '''R''' is called a '''step function''' if there exists a finite [[partition of an interval|partition]]\n\n:<math>\\Pi = \\{ a = t_0 < t_1 < \\cdots < t_k = b \\}</math>\n\nof [''a'', ''b''] such that ''φ'' is constant on each [[open set|open]] interval (''t''<sub>''i''</sub>, ''t''<sub>''i''+1</sub>) of Π; suppose that this constant value is ''c''<sub>''i''</sub> ∈ '''R'''. Then, define the '''integral''' of a step function ''φ'' to be\n\n:<math>\\int_a^b \\varphi(t) \\, \\mathrm{d} t := \\sum_{i = 0}^{k - 1} c_i | t_{i + 1} - t_i |.</math>\n\nIt can be shown that this definition is independent of the choice of partition, in that if Π<sub>1</sub> is another partition of [''a'',&nbsp;''b''] such that ''φ'' is constant on the open intervals of Π<sub>1</sub>, then the numerical value of the integral of ''φ'' is the same for Π<sub>1</sub> as for Π.\n\n===Extension to regulated functions===\nA function ''f'' : [''a'', ''b''] → '''R''' is called a '''[[regulated function]]''' if it is the uniform limit of a sequence of step functions on [''a'', ''b'']:\n* there is a sequence of step functions (''φ''<sub>''n''</sub>)<sub>''n''∈'''N'''</sub> such that || ''φ''<sub>''n''</sub> − ''f'' ||<sub>∞</sub> → 0 as ''n'' → ∞; or, equivalently,\n* for all ''ε'' &gt; 0, there exists a step function ''φ''<sub>''ε''</sub> such that || ''φ''<sub>''ε''</sub> − ''f'' ||<sub>∞</sub> &lt; ''ε''; or, equivalently,\n* ''f'' lies in the closure of the space of step functions, where the closure is taken in the space of all [[bounded function]]s [''a'', ''b''] → '''R''' and with respect to the [[supremum norm]] || - ||<sub>∞</sub>; or equivalently,\n* for every ''t''&nbsp;∈&nbsp;[''a'',&nbsp;''b''), the right-sided limit\n::<math>f(t+) = \\lim_{s \\downarrow t} f(s)</math>\n:exists, and, for every ''t''&nbsp;∈&nbsp;(''a'',&nbsp;''b''], the left-sided limit\n::<math>f(t-) = \\lim_{s \\uparrow t} f(s)</math>\n:exists as well.\n\nDefine the '''integral''' of a regulated function ''f'' to be\n\n:<math>\\int_{a}^{b} f(t) \\, \\mathrm{d} t := \\lim_{n \\to \\infty} \\int_{a}^{b} \\varphi_{n} (t) \\, \\mathrm{d} t,</math>\n\nwhere (''φ''<sub>''n''</sub>)<sub>''n''∈'''N'''</sub> is any sequence of step functions that converges uniformly to ''f''.\n\nOne must check that this limit exists and is independent of the chosen sequence, but this\nis an immediate consequence of the [[continuous linear extension]] theorem of elementary\nfunctional analysis: a [[bounded linear operator]] ''T''<sub>0</sub> defined on a [[dense (topology)|dense]] [[linear subspace]] ''E''<sub>0</sub> of a [[normed linear space]] ''E'' and taking values in a Banach space ''F'' extends uniquely to a bounded linear operator ''T'' : ''E'' → ''F'' with the same (finite) [[operator norm]].\n\n==Properties of the regulated integral==\n* The integral is a [[linear operator]]: for any regulated functions ''f'' and ''g'' and constants ''α'' and ''β'',\n\n::<math>\\int_{a}^{b} \\alpha f(t) + \\beta g(t) \\, \\mathrm{d} t = \\alpha \\int_{a}^{b} f(t) \\, \\mathrm{d} t + \\beta \\int_{a}^{b} g(t) \\, \\mathrm{d} t.</math>\n\n* The integral is also a [[bounded operator]]: every regulated function ''f'' is bounded, and if ''m'' ≤ ''f''(''t'') ≤ ''M'' for all ''t'' ∈ [''a'', ''b''], then\n\n::<math>m | b - a | \\leq \\int_{a}^{b} f(t) \\, \\mathrm{d} t \\leq M | b - a |.</math>\n\n: In particular:\n\n::<math>\\left| \\int_{a}^{b} f(t) \\, \\mathrm{d} t \\right| \\leq \\int_{a}^{b} | f(t) | \\, \\mathrm{d} t.</math>\n\n* Since step functions are integrable and the integrability and the value of a Riemann integral are compatible with uniform limits, the regulated integral is a special case of the Riemann integral.\n\n==Extension to functions defined on the whole real line==\nIt is possible to extend the definitions of step function and regulated function and the associated integrals to functions defined on the whole [[real line]]. However, care must be taken with certain technical points:\n* the partition on whose open intervals a step function is required to be constant is allowed to be a countable set, but must be a [[discrete set]], i.e. have no [[limit point]]s;\n* the requirement of uniform convergence must be loosened to the requirement of uniform convergence on [[compact space|compact sets]], i.e. [[closed set|closed]] and [[bounded set|bounded]] intervals;\n* not every [[bounded function]] is integrable (e.g. the function with constant value 1). This leads to a notion of [[Locally integrable function|local integrability]].\n\n==Extension to vector-valued functions==\nThe above definitions go through ''[[mutatis mutandis]]'' in the case of functions taking values in a [[normed vector space]] ''X''.\n\n==See also==\n* [[Lebesgue integration|Lebesgue integral]]\n* [[Riemann integral]]\n\n==References==\n*{{cite journal | author=Berberian, S.K. |  title=Regulated Functions: Bourbaki's Alternative to the Riemann Integral | journal=The American Mathematical Monthly | year=1979 | doi=10.2307/2321526 | volume=86 | pages=208 | jstor=2321526 | issue=3 | publisher=Mathematical Association of America }}\n*{{cite book | last=Gordon | first=Russell A. | title=The integrals of Lebesgue, Denjoy, Perron, and Henstock | series=[[Graduate Studies in Mathematics]], 4  | publisher=American Mathematical Society | location=Providence, RI | year=1994 | isbn=0-8218-3805-9 }}\n\n{{integral}}\n{{Functional Analysis}}\n\n[[Category:Definitions of mathematical integration]]"
    },
    {
      "title": "Riemann integral",
      "url": "https://en.wikipedia.org/wiki/Riemann_integral",
      "text": "{{Short description|Basic type of integral in elementary calculus}}\n[[File:Integral as region under curve.svg|thumb|right|The integral as the area of a region under a curve.]]\n[[File:Riemann integral regular.gif|thumb|right|A sequence of Riemann sums over a regular partition of an interval. The number on top is the total area of the rectangles, which converges to the integral of the function.]]\n[[File:Riemann integral irregular.gif|thumb|right|The partition does not need to be regular, as shown here. The approximation works as long as the width of each subdivision tends to zero.]]\n{{Calculus|Integral}}\n\nIn the branch of [[mathematics]] known as [[real analysis]], the '''Riemann integral''', created by [[Bernhard Riemann]], was the first rigorous definition of the [[integral]] of a [[function (mathematics)|function]] on an [[Interval (mathematics)|interval]].  It was presented to the faculty at the [[University of Göttingen]] in 1854, but not published in a journal until 1868.<ref>The Riemann integral was introduced in Bernhard Riemann's paper \"Über die Darstellbarkeit einer Function durch eine trigonometrische Reihe\" (On the representability of a function by a trigonometric series; i.e., when can a function be represented by a trigonometric series). This paper was submitted to the University of Göttingen in 1854 as Riemann's ''Habilitationsschrift'' (qualification to become an instructor). It was published in 1868 in ''Abhandlungen der Königlichen Gesellschaft der Wissenschaften zu Göttingen'' (Proceedings of the Royal Philosophical Society at Göttingen), vol. 13, pages 87-132. (Available online [https://books.google.com/books?id=PDVFAAAAcAAJ&pg=RA1-PA87 here].) For Riemann's definition of his integral, see section 4, \"Über den Begriff eines bestimmten Integrals und den Umfang seiner Gültigkeit\" (On the concept of a definite integral and the extent of its validity), pages 101–103.</ref> For many functions and practical applications, the Riemann integral can be evaluated by the [[fundamental theorem of calculus]] or approximated by [[numerical integration]].\n\nThe Riemann integral is unsuitable for many theoretical purposes. Some of the technical deficiencies in Riemann integration can be remedied with the [[Riemann–Stieltjes integral]], and most disappear with the [[Lebesgue integral]].\n\n== Overview ==\n\nLet {{mvar|f}} be a non-negative [[real number|real]]-valued function on the interval {{math|[''a'', ''b'']}}, and let\n\n:<math>S = \\left \\{ (x, y) \\, : \\ a \\leq x \\leq b, 0 < y < f(x) \\right \\}</math>\n\nbe the region of the plane under the graph of the function {{mvar|f}} and above the interval {{math|[''a'', ''b'']}} (see the figure on the top right). We are interested in measuring the area of {{mvar|S}}. Once we have measured it, we will denote the area by:\n\n:<math>\\int_{a}^{b}f(x)\\,dx.</math>\n\nThe basic idea of the Riemann integral is to use very simple approximations for the area of {{mvar|S}}. By taking better and better approximations, we can say that \"in the limit\" we get exactly the area of {{mvar|S}} under the curve.\n\nNote that where {{mvar|f}} can be both positive and negative, the definition of {{mvar|S}} is modified so that the integral corresponds to the ''signed area'' under the graph of {{mvar|f}}: that is, the area above the {{mvar|x}}-axis minus the area below the {{mvar|x}}-axis.\n\n== Definition ==\n\n=== Partitions of an interval ===\n\nA [[partition of an interval]] {{math|[''a'', ''b'']}} is a finite sequence of numbers of the form\n\n:<math>a = x_0 < x_1 < x_2 < \\dots < x_n = b</math>\n\nEach {{math|[''x<sub>i</sub>'', ''x''<sub>''i'' + 1</sub>]}} is called a '''sub-interval''' of the partition. The '''mesh''' or '''norm''' of a partition is defined to be the length of the longest sub-interval, that is,\n\n:<math>\\max \\left(x_{i+1}-x_i\\right), \\quad i \\in [0,n-1].</math>\n\nA '''tagged partition''' {{math|''P''(''x'', ''t'')}} of an interval {{math|[''a'', ''b'']}} is a partition together with a finite sequence of numbers {{math|''t''<sub>0</sub>, ..., ''t''<sub>''n'' − 1</sub>}} subject to the conditions that for each {{mvar|i}}, {{math|''t<sub>i</sub>'' ∈ [''x<sub>i</sub>'', ''x''<sub>''i'' + 1</sub>]}}. In other words, it is a partition together with a distinguished point of every sub-interval. The mesh of a tagged partition is the same as that of an ordinary partition.\n\nSuppose that two partitions {{math|''P''(''x'', ''t'')}} and {{math|''Q''(''y'', ''s'')}} are both partitions of the interval {{math|[''a'', ''b'']}}. We say that {{math|''Q''(''y'', ''s'')}} is a '''refinement''' of {{math|''P''(''x'', ''t'')}} if for each integer {{mvar|i}}, with {{math|''i'' ∈ [0, ''n'']}}, there exists an integer {{math|''r''(''i'')}} such that {{math|''x<sub>i</sub>'' {{=}} ''y''<sub>''r''(''i'')</sub>}} and such that {{math|''t<sub>i</sub>'' {{=}} ''s<sub>j</sub>''}} for some {{mvar|j}} with {{math|''j'' ∈ [''r''(''i''), ''r''(''i'' + 1))}}. Said more simply, a refinement of a tagged partition breaks up some of the sub-intervals and adds tags to the partition where necessary, thus it \"refines\" the accuracy of the partition.\n\nWe can define a [[partial order]] on the set of all tagged partitions by saying that one tagged partition is greater or equal to another if the former is a refinement of the latter.\n\n=== Riemann sums ===\nLet {{mvar|f}} be a real-valued function defined on the interval {{math|[''a'', ''b'']}}. The ''[[Riemann sum]]'' of {{mvar|f}} with respect to the tagged partition {{math|''x''<sub>0</sub>, ..., ''x<sub>n</sub>''}} together with {{math|''t''<sub>0</sub>, ..., ''t''<sub>''n'' − 1</sub>}} is<ref>{{cite book|author=Krantz|first=Steven G.|authorlink=Steven G. Krantz|title=Real Analysis and Foundations|publisher=CRC Press|year=1991|page=173|url=https://books.google.com/books/about/Real_Analysis_and_Foundations.html?id=OI-0vu1rb7MC&pg=PA173}}; {{cite book|title=2005 edition|isbn=9781584884835}}</ref>\n\n:<math>\\sum_{i=0}^{n-1} f(t_i) \\left(x_{i+1}-x_i\\right).</math>\n\nEach term in the sum is the product of the value of the function at a given point and the length of an interval. Consequently, each term represents the (signed) area of a rectangle with height {{math|''f''(''t<sub>i</sub>'')}} and width {{math|''x''<sub>''i'' + 1</sub> − ''x<sub>i</sub>''}}. The Riemann sum is the (signed) area of all the rectangles.\n\nA closely related concept are the ''lower and upper Darboux sums''. These are similar to Riemann sums, but the tags are replaced by the [[infimum and supremum]] (respectively) of {{mvar|f}} on each sub-interval:\n\n:<math>\\begin{align}\nL(f, P) &= \\sum_{i=0}^{n-1} \\inf_{t \\in [x_i, x_{i+1}]} f(t)(x_{i+1} - x_i), \\\\\nU(f, P) &= \\sum_{i=0}^{n-1} \\sup_{t \\in [x_i, x_{i+1}]} f(t)(x_{i+1} - x_i).\n\\end{align}</math>\n\nIf {{mvar|f}} is continuous, then the lower and upper Darboux sums for an untagged partition are equal to the Riemann sum for that partition, where the tags are chosen to be the minimum or maximum (respectively) of {{mvar|f}} on each subinterval. (When {{mvar|f}} is discontinuous on a subinterval, there may not be a tag that achieves the infimum or supremum on that subinterval.) The [[Darboux integral]], which is similar to the Riemann integral but based on Darboux sums, is equivalent to the Riemann integral.\n\n=== {{anchor|Riemann-integrable}} Riemann integral ===\nLoosely speaking, the Riemann integral is the limit of the Riemann sums of a function as the partitions get finer. If the limit exists then the function is said to be '''integrable''' (or more specifically '''Riemann-integrable'''). The Riemann sum can be made as close as desired to the Riemann integral by making the partition fine enough.<ref>{{Cite book|last=Taylor|first=Michael E.|authorlink=Michael E. Taylor|title=Measure Theory and Integration|publisher=American Mathematical Society|year=2006|isbn=9780821872468|page=1|url=https://books.google.com/books?id=P_zJA-E5oe4C&pg=PA1}}</ref>\n\nOne important requirement is that the mesh of the partitions must become smaller and smaller, so that in the limit, it is zero. If this were not so, then we would not be getting a good approximation to the function on certain subintervals. In fact, this is enough to define an integral. To be specific, we say that the Riemann integral of {{mvar|f}} equals {{mvar|s}} if the following condition holds:\n\n<blockquote>For all {{math|''ε'' > 0}}, there exists {{math|''δ'' > 0}} such that for any tagged partition {{math|''x''<sub>0</sub>, ..., ''x<sub>n</sub>''}} and {{math|''t''<sub>0</sub>, ..., ''t''<sub>''n'' − 1</sub>}} whose mesh is less than {{mvar|δ}}, we have\n\n:<math>\\left|\\sum_{i=0}^{n-1} f(t_i) (x_{i+1}-x_i) - s\\right| < \\varepsilon.</math></blockquote>\n\nUnfortunately, this definition is very difficult to use. It would help to develop an equivalent definition of the Riemann integral which is easier to work with. We develop this definition now, with a proof of equivalence following. Our new definition says that the Riemann integral of {{mvar|f}} equals {{mvar|s}} if the following condition holds:\n\n<blockquote>For all {{math|''ε'' > 0}}, there exists a tagged partition {{math|''y''<sub>0</sub>, ..., ''y<sub>m</sub>''}} and {{math|''r''<sub>0</sub>, ..., ''r''<sub>''m'' − 1</sub>}} such that for any tagged partition {{math|''x''<sub>0</sub>, ..., ''x<sub>n</sub>''}} and {{math|''t''<sub>0</sub>, ..., ''t''<sub>''n'' − 1</sub>}} which is a refinement of {{math|''y''<sub>0</sub>, ..., ''y<sub>m</sub>''}} and {{math|''r''<sub>0</sub>, ..., ''r''<sub>''m'' − 1</sub>}}, we have\n\n: <math>\\left|\\sum_{i=0}^{n-1} f(t_i) (x_{i+1}-x_i) - s\\right| < \\varepsilon.</math></blockquote>\n\nBoth of these mean that eventually, the Riemann sum of {{mvar|f}} with respect to any partition gets trapped close to {{mvar|s}}. Since this is true no matter how close we demand the sums be trapped, we say that the Riemann sums converge to {{mvar|s}}. These definitions are actually a special case of a more general concept, a [[net (mathematics)|net]].\n\nAs we stated earlier, these two definitions are equivalent. In other words, {{mvar|s}} works in the first definition if and only if {{mvar|s}} works in the second definition. To show that the first definition implies the second, start with an {{mvar|ε}}, and choose a {{mvar|δ}} that satisfies the condition. Choose any tagged partition whose mesh is less than {{mvar|δ}}. Its Riemann sum is within {{mvar|ε}} of {{mvar|s}}, and any refinement of this partition will also have mesh less than {{mvar|δ}}, so the Riemann sum of the refinement will also be within {{mvar|ε}} of {{mvar|s}}.\n\nTo show that the second definition implies the first, it is easiest to use the [[Darboux integral]]. First, one shows that the second definition is equivalent to the definition of the Darboux integral; for this see the article on Darboux integration. Now we will show that a Darboux integrable function satisfies the first definition. Fix {{mvar|ε}}, and choose a partition {{math|''y''<sub>0</sub>, ..., ''y<sub>m</sub>''}} such that the lower and upper Darboux sums with respect to this partition are within {{math|''ε''/2}} of the value {{mvar|s}} of the Darboux integral. Let\n\n:<math> r = 2\\sup_{x \\in [a, b]} |f(x)|.</math>\n\nIf {{math|''r'' {{=}} 0}}, then {{mvar|f}} is the zero function, which is clearly both Darboux and Riemann integrable with integral zero. Therefore, we will assume that {{math|''r'' > 0}}. If {{math|''m'' > 1}}, then we choose {{mvar|δ}} such that\n\n:<math>\\delta < \\min \\left \\{\\frac{\\varepsilon}{2r(m-1)}, \\left(y_1 - y_0\\right), \\left(y_2 - y_1\\right), \\cdots, \\left(y_m - y_{m-1}\\right) \\right \\}</math>\n\nIf {{math|''m'' {{=}} 1}}, then we choose {{mvar|δ}} to be less than one. Choose a tagged partition {{math|''x''<sub>0</sub>, ..., ''x<sub>n</sub>''}} and {{math|''t''<sub>0</sub>, ..., ''t''<sub>''n'' − 1</sub>}} with mesh smaller than {{mvar|δ}}. We must show that the Riemann sum is within {{mvar|ε}} of {{mvar|s}}.\n\nTo see this, choose an interval {{math|[''x<sub>i</sub>'', ''x''<sub>''i'' + 1</sub>]}}. If this interval is contained within some {{math|[''y<sub>j</sub>'', ''y''<sub>''j'' + 1</sub>]}}, then\n\n:<math> m_j < f(t_i) < M_j</math>\n\nwhere {{mvar|m<sub>j</sub>}} and {{mvar|M<sub>j</sub>}} are respectively, the infimum and the supremum of ''f'' on {{math|[''y<sub>j</sub>'', ''y''<sub>''j'' + 1</sub>]}}. If all intervals had this property, then this would conclude the proof, because each term in the Riemann sum would be bounded by a corresponding term in the Darboux sums, and we chose the Darboux sums to be near {{mvar|s}}. This is the case when {{math|''m'' {{=}} 1}}, so the proof is finished in that case.\n\nTherefore, we may assume that {{math|''m'' > 1}}. In this case, it is possible that one of the {{math|[''x<sub>i</sub>'', ''x''<sub>''i'' + 1</sub>]}} is not contained in any {{math|[''y<sub>j</sub>'', ''y''<sub>''j'' + 1</sub>]}}. Instead, it may stretch across two of the intervals determined by {{math|''y''<sub>0</sub>, ..., ''y<sub>m</sub>''}}. (It cannot meet three intervals because {{mvar|δ}} is assumed to be smaller than the length of any one interval.) In symbols, it may happen that\n\n:<math>y_j < x_i < y_{j+1} < x_{i+1} < y_{j+2}.</math>\n\n(We may assume that all the inequalities are strict because otherwise we are in the previous case by our assumption on the length of {{mvar|δ}}.) This can happen at most {{math|''m'' − 1}} times.\n\nTo handle this case, we will estimate the difference between the Riemann sum and the Darboux sum by subdividing the partition {{math|''x''<sub>0</sub>, ..., ''x<sub>n</sub>''}} at {{math|''y''<sub>''j'' + 1</sub>}}. The term {{math|''f''(''t<sub>i</sub>'')(''x''<sub>''i'' + 1</sub> − ''x<sub>i</sub>'')}} in the Riemann sum splits into two terms:\n\n:<math>f\\left(t_i\\right)\\left(x_{i+1}-x_i\\right) = f\\left(t_i\\right)\\left(x_{i+1}-y_{j+1}\\right)+f\\left(t_i\\right)\\left(y_{j+1}-x_i\\right).</math>\n\nSuppose, without loss of generality, that {{math|''t<sub>i</sub>'' ∈ [''y<sub>j</sub>'', ''y''<sub>''j'' + 1</sub>]}}. Then\n\n:<math>m_j < f(t_i) < M_j,</math>\n\nso this term is bounded by the corresponding term in the Darboux sum for {{mvar|y<sub>j</sub>}}. To bound the other term, notice that\n\n:<math>x_{i+1}-y_{j+1} < \\delta < \\frac{\\varepsilon}{2r(m-1)},</math>\n\nIt follows that, for some (indeed any) {{math|''t''{{su|b=''i''|p=*}} ∈ [''y''<sub>''j'' + 1</sub>, ''x''<sub>''i'' + 1</sub>]}},\n\n:<math>\\left|f\\left(t_i\\right)-f\\left(t_i^*\\right)\\right|\\left(x_{i+1}-y_{j+1}\\right) < \\frac{\\varepsilon}{2(m-1)}.</math>\n\nSince this happens at most {{math|''m'' − 1}} times, the distance between the Riemann sum and a Darboux sum is at most {{math|''ε''/2}}. Therefore, the distance between the Riemann sum and {{mvar|s}} is at most&nbsp;{{mvar|ε}}.\n\n== Examples ==\nLet {{math|''f'' : [0, 1] → '''R'''}} be the function which takes the value 1 at every point. Any Riemann sum of {{mvar|f}} on {{math|[0, 1]}} will have the value 1, therefore the Riemann integral of {{mvar|f}} on {{math|[0, 1]}} is 1.\n\nLet {{math|''I''<sub>'''Q'''</sub> : [0, 1] → '''R'''}} be the [[indicator function]] of the rational numbers in {{math|[0, 1]}}; that is, {{math|''I''<sub>'''Q'''</sub>}} takes the value 1 on rational numbers and 0 on irrational numbers. This function does not have a Riemann integral. To prove this, we will show how to construct tagged partitions whose Riemann sums get arbitrarily close to both zero and one.\n\nTo start, let {{math|''x''<sub>0</sub>, ..., ''x<sub>n</sub>''}} and {{math|''t''<sub>0</sub>, ..., ''t''<sub>''n'' − 1</sub>}} be a tagged partition (each {{mvar|t<sub>i</sub>}} is between {{mvar|x<sub>i</sub>}} and {{math|''x''<sub>''i'' + 1</sub>}}). Choose {{math|''ε'' > 0}}. The {{mvar|t<sub>i</sub>}} have already been chosen, and we can't change the value of {{mvar|f}} at those points. But if we cut the partition into tiny pieces around each {{mvar|t<sub>i</sub>}}, we can minimize the effect of the {{mvar|t<sub>i</sub>}}. Then, by carefully choosing the new tags, we can make the value of the Riemann sum turn out to be within {{mvar|ε}} of either zero or one.\n\nOur first step is to cut up the partition. There are {{mvar|n}} of the {{mvar|t<sub>i</sub>}}, and we want their total effect to be less than {{mvar|ε}}. If we confine each of them to an interval of length less than {{math|''ε''/''n''}}, then the contribution of each {{mvar|t<sub>i</sub>}} to the Riemann sum will be at least {{math|0 · ''ε''/''n''}} and at most {{math|1 · ''ε''/''n''}}. This makes the total sum at least zero and at most {{mvar|ε}}. So let {{mvar|δ}} be a positive number less than {{math|''ε''/''n''}}. If it happens that two of the {{mvar|t<sub>i</sub>}} are within {{mvar|δ}} of each other, choose {{mvar|δ}} smaller. If it happens that some {{mvar|t<sub>i</sub>}} is within {{mvar|δ}} of some {{mvar|x<sub>j</sub>}}, and {{mvar|t<sub>i</sub>}} is not equal to {{mvar|x<sub>j</sub>}}, choose {{mvar|δ}} smaller. Since there are only finitely many {{mvar|t<sub>i</sub>}} and {{mvar|x<sub>j</sub>}}, we can always choose {{mvar|δ}} sufficiently small.\n\nNow we add two cuts to the partition for each {{mvar|t<sub>i</sub>}}. One of the cuts will be at {{math|''t<sub>i</sub>'' − ''δ''/2}}, and the other will be at {{math|''t<sub>i</sub>'' + ''δ''/2}}. If one of these leaves the interval [0, 1], then we leave it out. {{mvar|t<sub>i</sub>}} will be the tag corresponding to the subinterval\n\n:<math>\\left [t_i - \\frac{\\delta}{2}, t_i + \\frac{\\delta}{2} \\right ].</math>\n\nIf {{mvar|t<sub>i</sub>}} is directly on top of one of the {{mvar|x<sub>j</sub>}}, then we let {{mvar|t<sub>i</sub>}} be the tag for both intervals:\n\n:<math>\\left [t_i - \\frac{\\delta}{2}, x_j \\right ], \\quad\\text{and}\\quad \\left [x_j,t_i + \\frac{\\delta}{2} \\right ].</math>\n\nWe still have to choose tags for the other subintervals. We will choose them in two different ways. The first way is to always choose a [[rational point]], so that the Riemann sum is as large as possible. This will make the value of the Riemann sum at least {{math|1 − ''ε''}}. The second way is to always choose an irrational point, so that the Riemann sum is as small as possible. This will make the value of the Riemann sum at most {{mvar|ε}}.\n\nSince we started from an arbitrary partition and ended up as close as we wanted to either zero or one, it is false to say that we are eventually trapped near some number {{mvar|s}}, so this function is not Riemann integrable. However, it is [[Lebesgue integral|Lebesgue integrable]]. In the Lebesgue sense its integral is zero, since the function is zero [[almost everywhere]]. But this is a fact that is beyond the reach of the Riemann integral.\n\nThere are even worse examples. {{math|''I''<sub>'''Q'''</sub>}} is equivalent (that is, equal almost everywhere) to a Riemann integrable function, but there are non-Riemann integrable bounded functions which are not equivalent to any Riemann integrable function. For example, let {{mvar|C}} be the [[Smith–Volterra–Cantor set]], and let {{math|''I''<sub>''C''</sub>}} be its indicator function. Because {{mvar|C}} is not [[Jordan measure|Jordan measurable]], {{math|''I''<sub>''C''</sub>}} is not Riemann integrable. Moreover, no function {{mvar|g}} equivalent to {{math|''I''<sub>''C''</sub>}} is Riemann integrable: {{mvar|g}}, like {{math|''I''<sub>''C''</sub>}}, must be zero on a dense set, so as in the previous example, any Riemann sum of {{mvar|g}} has a refinement which is within {{mvar|ε}} of 0 for any positive number&nbsp;{{mvar|ε}}. But if the Riemann integral of {{mvar|g}} exists, then it must equal the Lebesgue integral of {{math|''I''<sub>''C''</sub>}}, which is {{math|1/2}}. Therefore, {{mvar|g}} is not Riemann integrable.\n\n== Similar concepts ==\n\nIt is popular to define the Riemann integral as the [[Darboux integral]]. This is because the Darboux integral is technically simpler and because a function is Riemann-integrable if and only if it is Darboux-integrable.\n\nSome calculus books do not use general tagged partitions, but limit themselves to specific types of tagged partitions. If the type of partition is limited too much, some non-integrable functions may appear to be integrable.\n\nOne popular restriction is the use of \"left-hand\" and \"right-hand\" Riemann sums. In a left-hand Riemann sum, {{math|''t<sub>i</sub>'' {{=}} ''x<sub>i</sub>''}} for all {{mvar|i}}, and in a right-hand Riemann sum, {{math|''t<sub>i</sub>'' {{=}} ''x''<sub>''i'' + 1</sub>}} for all {{mvar|i}}. Alone this restriction does not impose a problem: we can refine any partition in a way that makes it a left-hand or right-hand sum by subdividing it at each {{mvar|t<sub>i</sub>}}. In more formal language, the set of all left-hand Riemann sums and the set of all right-hand Riemann sums is [[cofinal (mathematics)|cofinal]] in the set of all tagged partitions.\n\nAnother popular restriction is the use of regular subdivisions of an interval. For example, the {{mvar|n}}th regular subdivision of {{math|[0, 1]}} consists of the intervals \n:<math>\\left [0, \\frac{1}{n} \\right], \\left [\\frac{1}{n}, \\frac{2}{n} \\right], \\ldots, \\left[\\frac{n-1}{n}, 1 \\right].</math> \nAgain, alone this restriction does not impose a problem, but the reasoning required to see this fact is more difficult than in the case of left-hand and right-hand Riemann sums.\n\nHowever, combining these restrictions, so that one uses only left-hand or right-hand Riemann sums on regularly divided intervals, is dangerous. If a function is known in advance to be Riemann integrable, then this technique will give the correct value of the integral. But under these conditions the [[indicator function]] {{math|''I''<sub>'''Q'''</sub>}} will appear to be integrable on {{math|[0, 1]}} with integral equal to one: Every endpoint of every subinterval will be a rational number, so the function will always be evaluated at rational numbers, and hence it will appear to always equal one. The problem with this definition becomes apparent when we try to split the integral into two pieces. The following equation ought to hold:\n\n:<math>\\int_0^{\\sqrt{2}-1}\\! I_\\Q(x) \\,dx + \\int_{\\sqrt{2}-1}^1\\! I_\\Q(x) \\,dx = \\int_0^1\\! I_\\Q(x) \\,dx.</math>\n\nIf we use regular subdivisions and left-hand or right-hand Riemann sums, then the two terms on the left are equal to zero, since every endpoint except 0 and 1 will be irrational, but as we have seen the term on the right will equal 1.\n\nAs defined above, the Riemann integral avoids this problem by refusing to integrate {{math|''I''<sub>'''Q'''</sub>}}. The Lebesgue integral is defined in such a way that all these integrals are 0.\n\n== Properties ==\n\n=== Linearity ===\nThe Riemann integral is a linear transformation; that is, if {{mvar|f}} and {{mvar|g}} are Riemann-integrable on {{math|[''a'', ''b'']}} and {{mvar|α}} and {{mvar|β}} are constants, then\n\n:<math> \\int_{a}^{b}\\bigl( \\alpha f(x) + \\beta g(x)\\bigr)\\,dx = \\alpha \\int_{a}^{b}f(x)\\,dx + \\beta \\int_{a}^{b}g(x)\\,dx. </math>\n\nBecause the Riemann integral of a function is a number, this makes the Riemann integral a [[Linear form|linear functional]] on the [[vector space]] of Riemann-integrable functions.\n\n== Integrability ==\nA [[bounded function]] on a [[Compact space|compact interval]] {{math|[''a'', ''b'']}} is Riemann integrable if and only if it is [[continuous function|continuous]] [[almost everywhere]] (the set of its points of discontinuity has [[measure zero]], in the sense of [[Lebesgue measure]]). This is known as the '''{{visible anchor|Lebesgue's integrability condition|Lebesgue integrability condition}}''' or '''Lebesgue's criterion for Riemann integrability''' or the '''Riemann–Lebesgue theorem'''.<ref name=\"apostol169\">{{harvnb|Apostol|1974|pp=169–172}}</ref> The criterion has ''nothing to do'' with the [[Lebesgue integral]]. It is due to [[Henri Lebesgue|Lebesgue]] and uses his [[measure zero]], but makes use of neither Lebesgue's general measure or integral.\n\nThe integrability condition can be proven in various ways,<ref name=\"apostol169\" /><ref>{{Cite journal\n| issn = 0002-9890\n| volume = 43\n| issue = 7\n| pages = 396–398\n| last = Brown\n| first = A. B.\n| title = A Proof of the Lebesgue Condition for Riemann Integrability\n| journal = The American Mathematical Monthly| date = September 1936\n| jstor = 2301737\n| doi = 10.2307/2301737\n}}</ref><ref>Basic real analysis, by Houshang H. Sohrab, section 7.3, Sets of Measure Zero and Lebesgue’s Integrability Condition, [https://books.google.com/books?id=gBPI_oYZoMMC&pg=PA264 pp. 264–271]</ref><ref>''[http://ramanujan.math.trinity.edu/wtrench/texts/TRENCH_REAL_ANALYSIS.PDF Introduction to Real Analysis],'' updated April 2010, William F. Trench, 3.5 \"A More Advanced Look at the Existence of the Proper Riemann Integral\", pp. 171–177</ref> one of which is sketched below.\n\n:{| class=\"toccolours collapsible collapsed\" width=\"90%\" style=\"text-align:left\"\n!Proof\n|-\n|The proof is easiest using the [[Darboux integral]] definition of integrability (formally, the Riemann condition for integrability) – a function is Riemann integrable if and only if the upper and lower sums can be made arbitrarily close by choosing an appropriate partition.\n\nOne direction can be proven using the [[Oscillation (mathematics)|oscillation]] definition of continuity:<ref>[http://unapologetic.wordpress.com/2009/12/15/lebesgues-condition/ Lebesgue’s Condition], John Armstrong, December 15, 2009, The Unapologetic Mathematician</ref> For every positive {{mvar|ε}}, Let {{math|''X''<sub>''ε''</sub>}} be the set of points in {{math|[''a'', ''b'']}} with oscillation of at least {{mvar|ε}}. Since every point where {{mvar|f}} is discontinuous has a positive oscillation and vice versa, the set of points in {{math|[''a'', ''b'']}}, where {{mvar|f}} is discontinuous is equal to the union over {{math|{''X''<sub>1/''n''</sub>}|}} for all natural numbers {{mvar|n}}.\n\nIf this set does not have a zero [[Lebesgue measure]], then by [[countable additivity]] of the measure there is at least one such {{mvar|n}} so that {{math|''X''<sub>1/''n''</sub>}} does not have a zero measure. Thus there is some positive number {{mvar|c}} such that every [[countable]] collection of open intervals [[cover (topology)|covering]] {{math|''X''<sub>1/''n''</sub>}} has a total length of at least {{mvar|c}}. In particular this is also true for every such finite collection of intervals. Note that this remains true also for {{math|''X''<sub>1/''n''</sub>}} less a finite number of points (as a finite number of points can always be covered by a finite collection of intervals with arbitrarily small total length).\n\nFor every [[partition of an interval|partition of {{math|[''a'', ''b'']}}]], consider the set of intervals whose interiors include points from {{math|''X''<sub>1/''n''</sub>}}. These interiors consist of a finite open cover of {{math|''X''<sub>1/''n''</sub>}}, possibly up to a finite number of points (which may fall on interval edges). Thus these intervals have a total length of at least {{mvar|c}}. Since in these points {{mvar|f}} has oscillation of at least {{math|1/''n''}}, the [[infimum and supremum]] of {{mvar|f}} in each of these intervals differ by at least {{math|1/''n''}}. Thus the upper and lower sums of {{mvar|f}} differ by at least {{math|''c''/''n''}}. Since this is true for every partition, {{mvar|f}} is not Riemann integrable.\n\nWe now prove the converse direction using the sets {{math|''X''<sub>''ε''</sub>}} defined above.<ref>[http://unapologetic.wordpress.com/2009/12/09/jordan-content-integrability-condition/ Jordan Content Integrability Condition], John Armstrong, December 9, 2009, The Unapologetic Mathematician</ref> Note that for every {{mvar|ε}}, {{math|''X''<sub>''ε''</sub>}} is [[compact space|compact]], as it is bounded (by {{mvar|a}} and {{mvar|b}}) and closed:\n\n*For every series of points in {{math|''X''<sub>''ε''</sub>}} that is converging in {{math|[''a'', ''b'']}}, its limit is in {{math|''X''<sub>''ε''</sub>}} as well. This is because every neighborhood of the limit point is also a neighborhood of some point in {{math|''X''<sub>''ε''</sub>}}, and thus {{mvar|f}} has an oscillation of at least {{mvar|ε}} on it. Hence the limit point is in {{math|''X''<sub>''ε''</sub>}}.\n\nNow, suppose that {{mvar|f}} is continuous [[almost everywhere]]. Then for every {{mvar|ε}}, {{math|''X''<sub>''ε''</sub>}} has zero [[Lebesgue measure]]. Therefore, there is a countable collections of open intervals in {{math|[''a'', ''b'']}} which is an [[open cover]] of {{math|''X''<sub>''ε''</sub>}}, such that the sum over all their lengths is arbitrarily small. [[Compact space#Open cover definition|Since {{math|''X''<sub>''ε''</sub>}} is compact]], there is a finite [[subcover]] – a finite collections of open intervals in {{math|[''a'', ''b'']}} with arbitrarily small total length that together contain all points in {{math|''X''<sub>''ε''</sub>}}. We denote these intervals {{math|{''I''(''ε'')<sub>''i''</sub>}|}}, for {{math|1 ≤ ''i'' ≤ ''k''}}, for some natural {{mvar|k}}.\n\nThe [[complement (set theory)|complement]] of the union of these intervals is itself a union of a finite number of intervals, which we denote {{math|{''J''(''ε'')<sub>''i''</sub>}|}} (for {{math|1 ≤ ''i'' ≤ ''k'' − 1}} and possibly for {{math|1=''i'' = ''k'', ''k'' + 1}} as well).\n\nWe now show that for every {{math|''ε'' > 0}}, there are [[Darboux integral#Darboux sums|upper and lower sums]] whose difference is less than {{mvar|ε}}, from which Riemann integrability follows. To this end, we construct a [[partition of an interval|partition of {{math|[''a'', ''b'']}}]] as follows:\n\nDenote {{math|1=''ε''<sub>1</sub> = ''ε'' / 2(''b'' − ''a'')}} and {{math|1=''ε''<sub>2</sub> = ''ε'' / 2(''M'' − ''m'')}}, where {{mvar|m}} and {{mvar|M}} are the [[infimum and supremum]] of {{mvar|f}} on {{math|[''a'', ''b'']}}. Since we may choose intervals {{math|{''I''(''ε''<sub>1</sub>)<sub>''i''</sub>}|}} with arbitrarily small total length, we choose them to have total length smaller than {{math|''ε''<sub>2</sub>}}.\n\nEach of the intervals {{math|{''J''(''ε''<sub>1</sub>)<sub>''i''</sub>}|}} has an empty intersection with {{math|''X''<sub>''ε''<sub>1</sub></sub>}}, so each point in it has a neighborhood with oscillation smaller than {{math|''ε''<sub>1</sub>}}. These neighborhoods consist of an [[open cover]] of the interval, and since the interval is compact there is a finite subcover of them. This subcover is a finite collection of open intervals, which are subintervals of {{math|''J''(''ε''<sub>1</sub>)<sub>''i''</sub>}} (except for those that include an edge point, for which we only take their intersection with {{math|''J''(''ε''<sub>1</sub>)<sub>''i''</sub>)}}. We take the edge points of the subintervals for all {{math|''J''(''ε''<sub>1</sub>)<sub>''i''</sub> − ''s''}}, including the edge points of the intervals themselves, as our partition.\n\nThus the partition divides {{math|[''a'', ''b'']}} to two kinds of intervals:\n*Intervals of the latter kind (themselves subintervals of some {{math|''J''(''ε''<sub>1</sub>)<sub>''i''</sub>}}). In each of these, {{mvar|f}} oscillates by less than {{math|''ε''<sub>1</sub>}}. Since the total length of these is not larger than {{math|''b'' − ''a''}}, they together contribute at most {{math|1=''ε''{{su|b=1|p=∗}}(''b'' − ''a'') = ''ε''/2}} to the difference between the upper and lower sums of the partition.\n*The intervals {{math|{''I''(''ε'')<sub>''i''</sub>}|}}. These have total length smaller than {{math|''ε''<sub>2</sub>}}, and {{mvar|f}} oscillates on them by no more than {{math|''M'' − ''m''}}. Thus together they contribute less than {{math|1=''ε''{{su|b=2|p=∗}}(''M'' − ''m'') = ''ε''/2}} to the difference between the upper and lower sums of the partition.\n\nIn total, the difference between the upper and lower sums of the partition is smaller than {{mvar|ε}}, as required.\n|}\n\nIn particular, any set that is at most [[countable set|countable]] has [[Lebesgue measure]] zero, and thus a bounded function (on a compact interval) with only finitely or countably many discontinuities is Riemann integrable.\n\nAn [[indicator function]] of a bounded set is Riemann-integrable if and only if the set is [[Jordan measure|Jordan measurable]].<ref>[http://planetmath.org/encyclopedia/Volume.html PlanetMath Volume]</ref> The Riemann integral can be interpreted [[measure theory|measure-theoretically]] as the integral with respect to the Jordan measure.\n\nIf a real-valued function is [[monotone function|monotone]] on the interval {{math|[''a'', ''b'']}} it is Riemann-integrable, since its set of discontinuities is at most countable, and therefore of Lebesgue measure zero.\n\nIf a real-valued function on {{math|[''a'', ''b'']}} is Riemann-integrable, it is [[Lebesgue integral|Lebesgue-integrable]]. That is, Riemann-integrability is a ''stronger'' (meaning more difficult to satisfy) condition than Lebesgue-integrability.\n\nIf {{math|''f''<sub>''n''</sub>}} is a [[uniform convergence|uniformly convergent]] sequence on {{math|[''a'', ''b'']}} with limit {{mvar|f}}, then Riemann integrability of all {{math|''f''<sub>''n''</sub>}} implies Riemann integrability of {{mvar|f}}, and\n\n:<math> \\int_{a}^{b} f\\, dx = \\int_a^b{\\lim_{n \\to \\infty}{f_n}\\, dx} = \\lim_{n \\to \\infty} \\int_{a}^{b} f_n\\, dx.</math>\nHowever, the [[Lebesgue monotone convergence theorem]] (on a monotone pointwise limit) does not hold. In Riemann integration, taking limits under the integral sign is far more difficult to logically justify than in Lebesgue integration.<ref>{{cite journal|author=Cunningham|first= Frederick, Jr.|title=Taking limits under the integral sign|journal=Mathematics Magazine|volume=40|year=1967|pages=179–186|url=http://www.maa.org/programs/maa-awards/writing-awards/taking-limits-under-the-integral-sign|doi=10.2307/2688673}}</ref>\n\n== Generalizations ==\nIt is easy to extend the Riemann integral to functions with values in the Euclidean vector space {{math|'''R'''<sup>''n''</sup>}} for any {{mvar|n}}. The integral is defined component-wise; in other words, if {{math|1='''f''' = (''f''<sub>1</sub>, ..., ''f''<sub>''n''</sub>)}} then\n:<math>\\int\\mathbf{f} = \\left(\\int f_1,\\,\\dots, \\int f_n\\right).</math>\nIn particular, since the complex numbers are a real [[vector space]], this allows the integration of complex valued functions.\n\nThe Riemann integral is only defined on bounded intervals, and it does not extend well to unbounded intervals. The simplest possible extension is to define such an integral as a limit, in other words, as an [[improper integral]]:\n\n:<math>\\int_{-\\infty}^\\infty f(x)\\,dx = \\lim_{a \\to -\\infty \\atop b \\to \\infty}\\int_a^b f(x)\\,dx.</math>\n\nThis definition carries with it some subtleties, such as the fact that it is not always equivalent to compute the [[Cauchy principal value]]\n:<math>\\lim_{a\\to\\infty} \\int_{-a}^a f(x)\\,dx.</math>\nFor example, consider the function {{math|''f''(''x'')}} which is 0 at {{math|''x'' {{=}} 0}}, 1 for {{math|''x'' > 0}}, and −1 for {{math|''x'' < 0}}. By symmetry,\n:<math>\\int_{-a}^a f(x)\\,dx = 0</math>\nalways, regardless of {{mvar|a}}. But there are many ways for the interval of integration to expand to fill the real line, and other ways can produce different results; in other words, the multivariate limit does not always exist. We can compute\n:<math>\\begin{align} \\int_{-a}^{2a} f(x)\\,dx &= a, \\\\ \\int_{-2a}^a f(x)\\,dx &= -a. \\end{align}</math>\nIn general, this improper Riemann integral is undefined. Even standardizing a way for the interval to approach the real line does not work because it leads to disturbingly counterintuitive results. If we agree (for instance) that the improper integral should always be\n:<math>\\lim_{a\\to\\infty} \\int_{-a}^a f(x)\\,dx,</math>\nthen the integral of the translation {{math|''f''(''x'' − 1)}} is −2, so this definition is not invariant under shifts, a highly undesirable property. In fact, not only does this function not have an improper Riemann integral, its Lebesgue integral is also undefined (it equals {{math|∞ − ∞}}).\n\nUnfortunately, the improper Riemann integral is not powerful enough. The most severe problem is that there are no widely applicable theorems for commuting improper Riemann integrals with limits of functions. In applications such as [[Fourier series]] it is important to be able to approximate the integral of a function using integrals of approximations to the function. For proper Riemann integrals, a standard theorem states that if {{math|''f''<sub>''n''</sub>}} is a sequence of functions that [[uniform convergence|converge uniformly]] to {{mvar|f}} on a compact set {{math|[''a'', ''b'']}}, then\n:<math>\\lim_{n\\to\\infty} \\int_a^b f_n(x)\\,dx = \\int_a^b f(x)\\,dx.</math>\nOn non-compact intervals such as the real line, this is false. For example, take {{math|''f''<sub>''n''</sub>(''x'')}} to be {{math|''n''<sup>−1</sup>}} on {{math|[0, ''n'']}} and zero elsewhere. For all {{mvar|n}} we have:\n:<math>\\int_{-\\infty}^\\infty f_n\\,dx = 1.</math>\nThe sequence {{math|{''f''<sub>''n''</sub>}|}} converges uniformly to the zero function, and clearly the integral of the zero function is zero. Consequently,\n:<math>\\int_{-\\infty}^\\infty f\\,dx \\neq \\lim_{n\\to\\infty}\\int_{-\\infty}^\\infty f_n\\,dx.</math>\nThis demonstrates that for integrals on unbounded intervals, uniform convergence of a function is not strong enough to allow passing a limit through an integral sign. This makes the Riemann integral unworkable in applications (even though the Riemann integral assigns both sides the correct value), because there is no other general criterion for exchanging a limit and a Riemann integral, and without such a criterion it is difficult to approximate integrals by approximating their integrands.\n\nA better route is to abandon the Riemann integral for the [[Lebesgue integral]]. The definition of the Lebesgue integral is not obviously a generalization of the Riemann integral, but it is not hard to prove that every Riemann-integrable function is Lebesgue-integrable and that the values of the two integrals agree whenever they are both defined. Moreover, a function {{mvar|f}} defined on a bounded interval is Riemann-integrable if and only if it is bounded and the set of points where {{mvar|f}} is discontinuous has Lebesgue measure zero.\n\nAn integral which is in fact a direct generalization of the Riemann integral is the [[Henstock–Kurzweil integral]].\n\nAnother way of generalizing the Riemann integral is to replace the factors {{math|''x''<sub>''k'' + 1</sub> − ''x''<sub>''k''</sub>}} in the definition of a Riemann sum by something else; roughly speaking, this gives the interval of integration a different notion of length. This is the approach taken by the [[Riemann–Stieltjes integral]].\n\nIn [[multivariable calculus]], the Riemann integrals for functions from {{math|'''R'''<sup>''n''</sup> → '''R'''}} are [[multiple integral]]s.\n\n== See also ==\n* [[Area]]\n* [[Antiderivative]]\n* [[Lebesgue integration]]\n\n==Notes==\n{{reflist|30em}}\n\n== References ==\n* Shilov, G. E., and Gurevich, B. L., 1978. ''Integral, Measure, and Derivative: A Unified Approach'', Richard A. Silverman, trans. Dover Publications. {{isbn|0-486-63519-8}}.\n* {{citation|first=Tom|last=Apostol|authorlink=Tom Apostol|title=Mathematical Analysis|year=1974|publisher=Addison-Wesley}}\n\n==External links==\n* {{springer|title=Riemann integral|id=p/r081950}}\n\n{{integral}}\n\n[[Category:Definitions of mathematical integration]]\n[[Category:Bernhard Riemann]]"
    },
    {
      "title": "Riemann–Stieltjes integral",
      "url": "https://en.wikipedia.org/wiki/Riemann%E2%80%93Stieltjes_integral",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Generalization of the Riemann integral}}\nIn [[mathematics]], the '''Riemann–Stieltjes integral''' is a generalization of the [[Riemann integral]], named after [[Bernhard Riemann]] and [[Thomas Joannes Stieltjes]]. The definition of this integral was first published in 1894 by Stieltjes.{{sfnp|Stieltjes|1894|pp=68–71}} It serves as an instructive and useful precursor of the [[Lebesgue integral]], and an invaluable tool in unifying equivalent forms of statistical theorems that apply to discrete and continuous probability.\n\n==Formal definition==\nThe Riemann–Stieltjes [[integral]] of a [[real number|real]]-valued function <math>f</math> of a real variable with respect to a real function <math>g</math> is denoted by\n\n:<math>\\int_a^b f(x) \\, \\operatorname{d}g(x)</math>\n\nand defined to be the limit, as the [[partition of an interval|norm]] (or ''mesh'') of the [[partition of an interval|partition]] (i.e. the length of the longest subinterval)\n\n:<math>P=\\{ a = x_0 < x_1 < \\cdots < x_n = b\\}</math>\n\nof the interval [''a'',&nbsp;''b''] approaches zero, of the approximating sum\n\n:<math>S(P,f,g) = \\sum_{i=0}^{n-1} f(c_i)\\left[ g(x_{i+1}) - g(x_i) \\right]</math>\n\nwhere <math>c_i</math> is in the ''i''-th subinterval [''x''<sub>''i''</sub>,&nbsp;''x''<sub>''i''+1</sub>].  The two functions <math>f</math> and <math>g</math> are respectively called the integrand and the integrator. Typically <math>g</math> is taken to be monotone (or at least of bounded variation) and right-semicontinuous (however this last is essentially convention). We specifically do not require <math>g</math> to be continuous, which allows for integrals that have point mass terms. \n\nThe \"limit\" is here understood to be a number ''A'' (the value of the Riemann–Stieltjes integral) such that for every ''ε''&nbsp;>&nbsp;0, there exists ''δ''&nbsp;>&nbsp;0 such that for every partition ''P'' with mesh(''P'')&nbsp;<&nbsp;''δ'', and for every choice of points ''c''<sub>''i''</sub> in [''x''<sub>''i''</sub>,&nbsp;''x''<sub>''i''+1</sub>],\n\n:<math>|S(P,f,g)-A| < \\varepsilon \\, </math>\n\n==Properties==\nThe Riemann–Stieltjes integral admits [[integration by parts]] in the form\n\n:<math>\\int_a^b f(x) \\, \\operatorname{d}g(x)=f(b)g(b)-f(a)g(a)-\\int_a^b g(x) \\, \\operatorname{d}f(x)</math>\n\nand the existence of either integral implies the existence of the other.{{sfnp|Hille|Phillips|1974|loc=§3.3}}\n\nOn the other hand, a classical result{{sfnp|Young|1936}} shows that the integral is well-defined if ''f'' is ''α''-[[Hölder continuous]] and ''g'' is ''β''-Hölder continuous with {{nowrap|''α'' + ''β'' > 1}}&nbsp;.\n\n==Application to probability theory==<!-- This section is linked from [[Probability distribution]] -->\n\nIf ''g'' is the [[cumulative distribution function|cumulative probability distribution function]] of a [[random variable]] ''X'' that has a [[probability density function]] with respect to [[Lebesgue measure]], and ''f'' is any function for which the [[expected value]] <math>\\operatorname{E}\\left[\\,\\left|f(X)\\right|\\,\\right]</math> is finite, then the probability density function of ''X'' is the derivative of ''g'' and we have\n\n:<math>\\operatorname{E}\\left[f(X)\\right]=\\int_{-\\infty}^\\infty f(x)g'(x)\\,\\operatorname{d}x</math>.\n\nBut this formula does not work if ''X'' does not have a probability density function with respect to Lebesgue measure.  In particular, it does not work if the distribution of ''X'' is discrete (i.e., all of the probability is accounted for by point-masses), and even if the cumulative distribution function ''g'' is continuous, it does not work if ''g'' fails to be [[absolute continuity|absolutely continuous]] (again, the [[Cantor function]] may serve as an example of this failure).  But the identity\n\n:<math>\\operatorname{E}\\left[f(X)\\right]=\\int_{-\\infty}^\\infty f(x)\\, \\operatorname{d}g(x)</math>\n\nholds if ''g'' is ''any'' cumulative probability distribution function on the real line, no matter how ill-behaved.  In particular, no matter how ill-behaved the cumulative distribution function ''g'' of a random variable ''X'', if the [[moment (mathematics)|moment]] E(''X''<sup>''n''</sup>) exists, then it is equal to\n\n: <math>\\operatorname{E}\\left[X^n\\right] = \\int_{-\\infty}^\\infty x^n\\,\\operatorname{d}g(x) </math>\n\n==Application to functional analysis==\nThe Riemann–Stieltjes integral appears in the original formulation of [[Riesz-Markov-Kakutani representation theorem|F. Riesz's theorem]] which represents  the [[dual space]] of the [[Banach space]]  ''C''[''a'',''b''] of continuous functions in an interval [''a'',''b''] as Riemann–Stieltjes integrals against functions of [[bounded variation]].  Later, that theorem was reformulated in terms of measures.\n\nThe Riemann–Stieltjes integral also appears in the formulation of the [[spectral theorem]] for (non-compact) self-adjoint (or more generally, normal) operators in a Hilbert space.  In this theorem, the integral is considered with respect to a spectral family of projections.<ref>See {{harvp|Riesz|Sz. Nagy|1990}} for details.</ref>\n\n==Existence of the integral==\nThe best simple existence theorem states that if ''f'' is continuous and ''g'' is of [[bounded variation]] on [''a'', ''b''], then the integral exists.{{sfnp|Johnsonbaugh|Pfaffenberger|2010|p=219}}{{sfnp|Rudin|1964|pp=121–122}}{{sfnp|Kolmogorov|Fomin|1975|p=368}} A function ''g'' is of bounded variation if and only if it is the difference between two (bounded) monotone functions.  If ''g'' is not of bounded variation, then there will be continuous functions which cannot be integrated with respect to ''g''.  In general, the integral is not well-defined if ''f'' and ''g'' share any points of [[Discontinuity (mathematics)|discontinuity]], but there are other cases as well.\n\n==Generalization==\nAn important generalization is the [[Lebesgue&ndash;Stieltjes integral]], which generalizes the Riemann–Stieltjes integral in a way analogous to how the [[Lebesgue integral]] generalizes the Riemann integral. If [[improper integral|improper]] Riemann–Stieltjes integrals are allowed, then the Lebesgue integral is not strictly more general than the Riemann–Stieltjes integral.\n\nThe Riemann–Stieltjes integral also generalizes{{citation needed|date=March 2019}} to the case when either the integrand ''ƒ'' or the integrator ''g'' take values in a [[Banach space]].  If {{nowrap|''g'' : [''a'',''b''] &rarr; ''X''}} takes values in the Banach space ''X'', then it is natural to assume that it is of '''strongly bounded variation''', meaning that\n\n:<math>\\sup \\sum_i \\|g(t_{i-1})-g(t_i)\\|_X < \\infty </math>\nthe supremum being taken over all finite partitions\n:<math>a=t_0\\le t_1\\le\\cdots\\le t_n=b</math>\nof the interval [''a'',''b''].  This generalization plays a role in the study of [[c0-semigroup|semigroups]], via the [[Laplace–Stieltjes transform]].\n\n===Generalized Riemann–Stieltjes integral===\nA slight generalization<ref>Introduced by {{harvp|Pollard|1920}} and now standard in analysis.</ref> is to consider in the above definition partitions ''P'' that ''refine'' another partition ''P''<sub>''ε''</sub>, meaning that ''P'' arises from ''P''<sub>''ε''</sub> by the addition of points, rather than from partitions with a finer mesh. Specifically, the '''generalized Riemann–Stieltjes integral''' of ''f'' with respect to ''g'' is a number ''A'' such that for every ''ε''&nbsp;>&nbsp;0 there exists a partition ''P''<sub>''ε''</sub> such that for every partition ''P'' that refines ''P''<sub>''ε''</sub>,\n\n:<math>|S(P,f,g) - A| < \\varepsilon \\, </math>\n\nfor every choice of points ''c''<sub>''i''</sub> in [''x''<sub>''i''</sub>,&nbsp;''x''<sub>''i''+1</sub>].\n\nThis generalization exhibits the Riemann–Stieltjes integral as the [[Moore–Smith limit]] on the [[directed set]] of partitions of [''a'',&nbsp;''b'']&nbsp;.{{sfnp|McShane|1952}}<ref>{{harvp|Hildebrandt|1938}} calls it  the '''Pollard–Moore–Stieltjes integral'''.</ref>\n\n===Darboux sums===\nThe Riemann–Stieltjes integral can be efficiently handled using an appropriate generalization of [[Darboux sum]]s.  For a partition ''P'' and a nondecreasing function ''g'' on [''a'',&nbsp;''b''] define the upper Darboux sum of ''f'' with respect to ''g'' by\n\n:<math>U(P,f,g) = \\sum_{i=1}^n \\,\\, [\\,g(x_i)-g(x_{i-1})\\,] \\,\\sup_{x\\in [x_{i-1},x_i]} f(x)</math>\n\nand the lower sum by\n\n:<math>L(P,f,g) = \\sum_{i=1}^n \\,\\, [\\,g(x_i)-g(x_{i-1})\\,] \\,\\inf_{x\\in [x_{i-1},x_i]} f(x)</math>&nbsp;.\n\nThen the generalized Riemann–Stieltjes of ''f'' with respect to ''g'' exists if and only if, for every ε&nbsp;>&nbsp;0, there exists a partition ''P'' such that\n\n:<math>U(P,f,g)-L(P,f,g) < \\varepsilon.</math>\n\nFurthermore, ''f'' is Riemann–Stieltjes integrable with respect to ''g'' (in the classical sense) if\n\n:<math>\\lim_{\\operatorname{mesh}(P)\\to 0} [\\,U(P,f,g)-L(P,f,g)\\,] = 0\\quad</math>{{sfnp|Graves|1946|loc=Chap. XII, §3}}\n\n==Examples and special cases==\n=== Differentiable <math>g(x)</math> ===\nGiven a <math>g(x)</math> which is continuously [[Differentiable function|differentiable]] over <math>\\mathbb{R}</math> it can be shown that there is the equality\n:<math>\n\\int_a^b f(x)\\operatorname{d}g(x) = \\int_a^b f(x)g'(x)\\operatorname{d}x\n</math>\nwhere the integral on the right-hand side is the standard Riemann-integral, assuming that <math>f</math> can be integrated by the Riemann-Stieltjes integral.\n\nMore generally, the Riemann integral equals the Riemann–Stieltjes integral if <math>g</math> is the [[Lebesgue integral]] of its derivative; in this case <math>g</math> is said to be [[absolutely continuous]].\n\nIt may be the case that <math>g</math> has jump discontinuities, or may have derivative zero ''almost'' everywhere while still being continuous and increasing (for example, <math>g</math> could be the [[Cantor function]] or “Devil's staircase”), in either of which cases the Riemann–Stieltjes integral is not captured by any expression involving derivatives of ''g''.\n\n=== Riemann Integral ===\nThe standard Riemann integral is a special case of the Riemann-Stieltjes integral where <math>g(x) = x</math>.\n\n=== Rectifier ===\nConsider the function <math>g(x) = \\max\\{ 0, x \\}</math> used in the study of [[neural network]]s, called a [[Rectifier (neural networks)|a ''rectified linear unit'' (ReLU)]]. Then the Riemann-Stieltjes can be evaluated as\n:<math>\n\\int_a^b f(x)\\operatorname{d}g(x) = \\int_{g(a)}^{g(b)}f(x)\\operatorname{d}x\n</math>\nwhere the integral on the right-hand side is the standard Riemann integral.\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{cite book |last=Graves |first=Lawrence |title=The theory of functions of a real variable |publisher=McGraw-Hill |year=1946 |ref=harv}}\n\n*{{Cite journal |last1=Hildebrandt |first1=T.H. |title=Definitions of Stieltjes integrals of the Riemann type |mr=1524276 |year=1938 |journal=[[American Mathematical Monthly|The American Mathematical Monthly]] |issn=0002-9890 |volume=45 |issue=5 |pages=265–278 |jstor=2302540 |ref=harv}}\n\n*{{Cite book |last1=Hille |first1=Einar |authorlink1=Einar Hille |last2=Phillips |first2=Ralph S. |authorlink2=Ralph Phillips (mathematician) |title=Functional analysis and semi-groups |publisher=[[American Mathematical Society]] |location=Providence, RI |mr=0423094  |year=1974 |ref=harv}}\n\n* {{cite book |title=Foundations of mathematical analysis |last1=Johnsonbaugh |first1=Richard F. |author1-link=Richard Johnsonbaugh |last2=Pfaffenberger |first2=William Elmer |year=2010 |publisher=Dover Publications |location=Mineola, NY |isbn=978-0-486-47766-4 |ref=harv}}\n\n* {{cite book |title=Introductory Real Analysis |last1=Kolmogorov |first1=Andrey |authorlink1=Andrey Kolmogorov |last2=Fomin |first2=Sergei V. |authorlink2=Sergei Fomin |orig-year=1970 |edition=Revised English |translator-first=Richard A. |translator-last=Silverman |year=1975 |publisher=Dover Press |isbn=0-486-61226-0 |ref=harv}}\n\n* {{cite journal |first=E. J. |last=McShane |url=https://www.maa.org/sites/default/files/pdf/upload_library/22/Chauvenet/Mcshane.pdf |title=Partial orderings & Moore-Smith limit |accessdate=2 November 2010 |journal=The American Mathematical Monthly |volume=59 |year=1952 |pages=1&ndash;11|doi=10.2307/2307181 |jstor=2307181 |ref=harv}}\n\n* {{cite journal |first=Henry |last=Pollard |title=The Stieltjes integral and its generalizations |year=1920 |volume=19 |journal=[[The Quarterly Journal of Pure and Applied Mathematics]] |ref=harv}}\n\n* {{cite book |first1=F. |last1=Riesz |first2=B. |last2=Sz. Nagy |title=Functional Analysis |year=1990 |publisher=Dover Publications |isbn=0-486-66289-6 |ref=harv}}\n\n* {{cite book |title=Principles of mathematical analysis |last1=Rudin |first1=Walter |edition=Second |year=1964 |publisher=McGraw-Hill |location=New York, NY |ref=harv}}\n\n* {{cite book |last1=Shilov |first1=G. E. |last2=Gurevich |first2=B. L. |year=1978 |title=Integral, Measure, and Derivative: A unified approach <!-- |journal=Integral --> |publisher=Dover Publications |isbn=0-486-63519-8 |bibcode=1966imdu.book.....S |translator-first=Richard A. |translator-last=Silverman}}\n\n* {{Cite journal |last1=Stieltjes |first1=Thomas Jan |authorlink1=Thomas Joannes Stieltjes |title=Recherches sur les fractions continues |url=http://www.numdam.org/numdam-bin/item?id=AFST_1894_1_8_4_J1_0 |mr=1344720 |year=1894 |journal=Ann. Fac. Sci. Toulouse |volume=VIII |pages=1–122 |ref=harv}}\n\n* {{cite book |last=Stroock |first=Daniel W. |year=1998 |title=A Concise Introduction to the Theory of Integration |publisher=Birkhauser |edition=3rd |isbn=0-8176-4073-8}}\n\n* {{cite journal |last=Young |first=L.C. |title=An inequality of the Hölder type, connected with Stieltjes integration |journal=Acta Mathematica |volume=67 |year=1936 |issue=1 |pages=251–282 |doi=10.1007/bf02401743 |ref=harv}}\n\n{{integral}}\n\n{{DEFAULTSORT:Riemann-Stieltjes integral}}\n[[Category:Definitions of mathematical integration]]\n[[Category:Bernhard Riemann]]"
    },
    {
      "title": "Russo–Vallois integral",
      "url": "https://en.wikipedia.org/wiki/Russo%E2%80%93Vallois_integral",
      "text": "In [[mathematical analysis]], the '''Russo–Vallois integral''' is an extension to [[stochastic process]]es of the classical [[Riemann–Stieltjes integral]] \n\n:<math>\\int f \\, dg=\\int fg' \\, ds</math> \n\nfor suitable functions <math>f</math> and <math>g</math>. The idea is to replace the [[derivative]] <math>g'</math> by the difference quotient\n\n:<math>g(s+\\varepsilon)-g(s)\\over\\varepsilon</math> and to pull the limit out of the integral. In addition one changes the type of convergence.\n\n==Definitions==\n'''Definition:''' A sequence <math>H_n</math> of [[stochastic process]]es [[Convergence of random variables|converges]] uniformly on [[compact set]]s in probability to a process <math>H,</math>\n\n:<math>H=\\text{ucp-}\\lim_{n\\rightarrow\\infty}H_n,</math>\n\nif, for every <math>\\varepsilon>0</math> and <math>T>0,</math>\n\n:<math>\\lim_{n\\rightarrow\\infty}\\mathbb{P}(\\sup_{0\\leq t\\leq T}|H_n(t)-H(t)|>\\varepsilon)=0.</math>\n\nOne sets:\n:<math>I^-(\\varepsilon,t,f,dg)={1\\over\\varepsilon}\\int_0^tf(s)(g(s+\\varepsilon)-g(s))\\,ds</math> \n:<math>I^+(\\varepsilon,t,f,dg)={1\\over\\varepsilon}\\int_0^t f(s)(g(s)-g(s-\\varepsilon)) \\, ds</math> \n\nand\n\n:<math>[f,g]_\\varepsilon (t)={1\\over \\varepsilon}\\int_0^t(f(s+\\varepsilon)-f(s))(g(s+\\varepsilon)-g(s))\\,ds.</math>\n\n'''Definition:''' The forward integral is defined as the ucp-limit of \n\n:<math>I^-</math>: <math>\\int_0^t fd^-g=\\text{ucp-}\\lim_{\\varepsilon\\rightarrow\\infty (0?)}I^-(\\varepsilon,t,f,dg).</math>\n\n'''Definition:''' The backward integral is defined as the ucp-limit of \n\n:<math>I^+</math>: <math>\\int_0^t f \\, d^+g = \\text{ucp-}\\lim_{\\varepsilon\\rightarrow\\infty (0?)}I^+(\\varepsilon,t,f,dg).</math>\n\n'''Definition:''' The generalized bracket is defined as the ucp-limit of \n\n:<math>[f,g]_\\varepsilon</math>: <math>[f,g]_\\varepsilon=\\text{ucp-}\\lim_{\\varepsilon\\rightarrow\\infty}[f,g]_\\varepsilon (t).</math>\n\nFor continuous [[semimartingale]]s <math>X,Y</math> and a [[cadlag function]] H, the Russo–Vallois integral coincidences with the usual [[Ito integral]]: \n\n:<math>\\int_0^t H_s \\, dX_s=\\int_0^t H \\, d^-X.</math> \n\nIn this case the generalised bracket is equal to the classical covariation. In the special case, this means that the process \n\n:<math>[X]:=[X,X] \\, </math>\n\nis equal to the [[quadratic variation process]].\n\nAlso for the Russo-Vallois Integral an [[Ito formula]] holds: If <math>X</math> is a continuous semimartingale and \n\n:<math>f\\in C_2(\\mathbb{R}),</math>\n\nthen \n\n:<math>f(X_t)=f(X_0)+\\int_0^t f'(X_s) \\, dX_s + {1\\over 2}\\int_0^t f''(X_s) \\, d[X]_s.</math>\n\nBy a duality result of [[Triebel]] one can provide optimal classes of [[Besov space]]s, where the Russo–Vallois integral can be defined. The norm in the Besov space \n\n:<math>B_{p,q}^\\lambda(\\mathbb{R}^N)</math> \n\nis given by \n\n:<math>||f||_{p,q}^\\lambda=||f||_{L_p} + \\left(\\int_0^\\infty {1\\over |h|^{1+\\lambda q}}(||f(x+h)-f(x)||_{L_p})^q \\, dh\\right)^{1/q}</math> \n\nwith the well known modification for <math>q=\\infty</math>. Then the following theorem holds:\n\n'''Theorem:''' Suppose \n\n:<math>f\\in B_{p,q}^\\lambda,</math>\n:<math>g\\in B_{p',q'}^{1-\\lambda},</math> \n:<math>1/p+1/p'=1\\text{ and }1/q+1/q'=1.</math>\n\nThen the Russo–Vallois integral \n\n:<math>\\int f \\, dg</math> \n\nexists and for some constant <math>c</math> one has \n\n:<math>\\left| \\int f \\, dg \\right| \\leq c ||f||_{p,q}^\\alpha ||g||_{p',q'}^{1-\\alpha}.</math>\n\nNotice that in this case the Russo–Vallois integral coincides with the [[Riemann–Stieltjes integral]] and with the [[Young integra]]l for functions with [[finite p-variation]].\n\n{{no footnotes|date=January 2012}}\n\n==References==\n*{{cite journal|author=Russo, Francesco|author2=Vallois, Pierre|title=Forward, backward and symmetric integration|journal=Prob. Th. and Rel. Fields|volume=97|year=1993|pages=403–421|doi=10.1007/BF01195073}}\n*{{cite journal|author=Russo, F.|author2=Vallois, P.|title=The generalized covariation process and Ito-formula|journal=Stoch. Proc. and Appl.|volume=59|issue=1|pages=81–104|year=1995|doi=10.1016/0304-4149(95)93237-A}}\n*{{cite book|author=Zähle, Martina|chapter=Forward Integrals and Stochastic Differential Equations|title=''In:'' Seminar on Stochastic Analysis, Random Fields and Applications III|series=Progress in Prob. Vol. 52|year=2002|pages=293–302|publisher=Birkhäuser, Basel|doi=10.1007/978-3-0348-8209-5_20}}\n*{{cite book|author=Adams, Robert A.|author2=Fournier, John J. F.|title=Sobolev Spaces|publisher=Elsevier|edition=second|year=2003|url=https://books.google.com/books?id=R5A65Koh-EoC}}\n\n{{Integrals}}\n\n{{DEFAULTSORT:Russo-Vallois integral}}\n[[Category:Definitions of mathematical integration]]\n[[Category:Stochastic processes]]"
    },
    {
      "title": "Skorokhod integral",
      "url": "https://en.wikipedia.org/wiki/Skorokhod_integral",
      "text": "In [[mathematics]], the '''Skorokhod integral''', often denoted ''&delta;'', is an [[Operator (mathematics)|operator]] of great importance in the theory of [[stochastic processes]].  It is named after the [[Ukraine|Ukrainian]] [[mathematician]] [[Anatoliy Skorokhod]].  Part of its importance is that it unifies several concepts:\n* ''&delta;'' is an extension of the [[Itô integral]] to non-[[adapted process]]es;\n* ''&delta;'' is the [[adjoint operator|adjoint]] of the [[Malliavin derivative]], which is fundamental to the stochastic [[calculus of variations]] ([[Malliavin calculus]]);\n* ''&delta;'' is an infinite-dimensional generalization of the [[divergence]] operator from classical [[vector calculus]].\n\n==Definition==\n\n===Preliminaries: the Malliavin derivative===\n\nConsider a fixed [[probability space]] (&Omega;,&nbsp;&Sigma;,&nbsp;'''P''') and a [[Hilbert space]] ''H''; '''E''' denotes [[expected value|expectation]] with respect to '''P'''\n\n:<math>\\mathbf{E} [X] := \\int_{\\Omega} X(\\omega) \\, \\mathrm{d} \\mathbf{P}(\\omega).</math>\n\nIntuitively speaking, the Malliavin derivative of a random variable ''F'' in ''L''<sup>''p''</sup>(&Omega;) is defined by expanding it in terms of Gaussian random variables that are parametrized by the elements of ''H'' and differentiating the expansion formally; the Skorokhod integral is the adjoint operation to the Malliavin derivative.\n\nConsider a family of '''R'''-valued [[random variables]] ''W''(''h''), indexed by the elements ''h'' of the Hilbert space ''H''.  Assume further that each ''W''(''h'') is a Gaussian ([[normal distribution|normal]]) random variable, that the map taking ''h'' to ''W''(''h'') is a [[linear map]], and that the [[expected value|mean]] and [[covariance]] structure is given by\n\n:<math>\\mathbf{E} [W(h)] = 0,</math>\n:<math>\\mathbf{E} [W(g)W(h)] = \\langle g, h \\rangle_{H},</math>\n\nfor all ''g'' and ''h'' in ''H''.  It can be shown that, given ''H'', there always exists a probability space (&Omega;,&nbsp;&Sigma;,&nbsp;'''P''') and a family of random variables with the above properties.  The Malliavin derivative is essentially defined by formally setting the derivative of the random variable ''W''(''h'') to be ''h'', and then extending this definition to &ldquo;[[smooth function|smooth enough]]&rdquo; random variables.  For a random variable ''F'' of the form\n\n:<math>F = f(W(h_{1}), \\ldots, W(h_{n})),</math>\n\nwhere ''f''&nbsp;:&nbsp;'''R'''<sup>''n''</sup>&nbsp;&rarr;&nbsp;'''R''' is smooth, the '''Malliavin derivative''' is defined using the earlier &ldquo;formal definition&rdquo; and the chain rule:\n\n:<math>\\mathrm{D} F := \\sum_{i = 1}^{n} \\frac{\\partial f}{\\partial x_{i}} (W(h_{1}), \\ldots, W(h_{n})) h_{i}.</math>\n\nIn other words, whereas ''F'' was a real-valued random variable, its derivative D''F'' is an ''H''-valued random variable, an element of the space ''L''<sup>''p''</sup>(&Omega;;''H'').  Of course, this procedure only defines D''F'' for &ldquo;smooth&rdquo; random variables, but an approximation procedure can be employed to define D''F'' for ''F'' in a large subspace of ''L''<sup>''p''</sup>(&Omega;);  the [[domain (mathematics)|domain]] of D is the [[closure (topology)|closure]] of the smooth random variables in the [[seminorm]] :\n\n<math>\\| F \\|_{1, p} := \\big( \\mathbf{E}[|F|^{p}] + \\mathbf{E}[\\| \\mathrm{D}F \\|_{H}^{p}] \\big)^{1/p}.</math>\n\nThis space is denoted by '''D'''<sup>1,''p''</sup> and is called the [[Watanabe–Sobolev space]].\n\n===The Skorokhod integral===\n\nFor simplicity, consider now just the case ''p''&nbsp;=&nbsp;2.  The '''Skorokhod integral''' ''&delta;'' is defined to be the ''L''<sup>2</sup>-adjoint of the Malliavin derivative D.  Just as D was not defined on the whole of ''L''<sup>2</sup>(&Omega;), ''&delta;'' is not defined on the whole of ''L''<sup>2</sup>(&Omega;;&nbsp;''H''):  the domain of ''&delta;'' consists of those processes ''u'' in ''L''<sup>2</sup>(&Omega;;&nbsp;''H'') for which there exists a constant ''C''(''u'') such that, for all ''F'' in '''D'''<sup>1,2</sup>,\n\n:<math>\\big| \\mathbf{E} [ \\langle \\mathrm{D} F, u \\rangle_{H} ] \\big| \\leq C(u) \\| F \\|_{L^{2} (\\Omega)}.</math>\n\nThe '''Skorokhod integral''' of a process ''u'' in ''L''<sup>2</sup>(&Omega;;&nbsp;''H'') is a real-valued random variable ''&delta;u'' in ''L''<sup>2</sup>(&Omega;);  if ''u'' lies in the domain of ''&delta;'', then ''&delta;u'' is defined by the relation that, for all ''F''&nbsp;&isin;&nbsp;'''D'''<sup>1,2</sup>,\n\n:<math>\\mathbf{E} [F \\, \\delta u] = \\mathbf{E} [ \\langle \\mathrm{D}F, u \\rangle_{H} ].</math>\n\nJust as the Malliavin derivative D was first defined on simple, smooth random variables, the Skorokhod integral has a simple expression for &ldquo;simple processes&rdquo;:  if ''u'' is given by\n\n:<math>u = \\sum_{j = 1}^{n} F_{j} h_{j}</math>\n\nwith ''F''<sub>''j''</sub> smooth and ''h''<sub>''j''</sub> in ''H'', then\n\n:<math>\\delta u = \\sum_{j = 1}^{n} \\left( F_{j} W(h_{j}) - \\langle \\mathrm{D} F_{j}, h_{j} \\rangle_{H} \\right).</math>\n\n==Properties==\n\n* The [[isometry]] property:  for any process ''u'' in ''L''<sup>2</sup>(&Omega;;&nbsp;''H'') that lies in the domain of ''&delta;'',\n\n::<math>\\mathbf{E} \\big[ (\\delta u)^{2} \\big] = \\mathbf{E} \\int | u_t |^{2} dt + \\mathbf{E} \\int D_s u_t\\, D_t u_s\\,ds\\, dt.</math>\n\n:If ''u'' is an adapted process, then <math>D_s u_t = 0</math> for ''s > t'', so the second term on the right-hand side vanishes. The Skorokhod and Itô integrals coincide in that case, and the above equation becomes the [[Itô isometry]].\n\n* The derivative of a Skorokhod integral is given by the formula\n\n::<math>\\mathrm{D}_{h} (\\delta u) = \\langle u, h \\rangle_{H} + \\delta (\\mathrm{D}_{h} u),</math>\n\n:where D<sub>''h''</sub>''X'' stands for (D''X'')(''h''), the random variable that is the value of the process D''X'' at &ldquo;time&rdquo; ''h'' in ''H''.\n\n* The Skorokhod integral of the product of a random variable ''F'' in '''D'''<sup>1,2</sup> and a process ''u'' in dom(''&delta;'') is given by the formula\n\n::<math>\\delta (F u) = F \\, \\delta u - \\langle \\mathrm{D} F, u \\rangle_{H}.</math>\n\n==References==\n* {{springer|title=Skorokhod integral|id=p/s110170}}\n* {{cite book\n| last = Ocone\n| first = Daniel L.\n| chapter = A guide to the stochastic calculus of variations\n| title = Stochastic analysis and related topics (Silivri, 1986)\n| series = Lecture Notes in Math. 1316\n| pages = 1&ndash;79\n| publisher = Springer\n| location = Berlin\n| year = 1988\n}} {{MathSciNet|id=953793}}\n* {{cite web\n| last = Sanz-Solé | first = Marta | authorlink=Marta Sanz-Solé\n| title = Applications of Malliavin Calculus to Stochastic Partial Differential Equations (Lectures given at Imperial College London, 7&ndash;11 July 2008)\n| year = 2008\n| url = http://www.ma.ic.ac.uk/~dcrisan/lecturenotes-london.pdf\n| accessdate = 2008-07-09\n}}\n\n{{integral}}\n{{Stochastic processes}}\n\n[[Category:Definitions of mathematical integration]]\n[[Category:Stochastic calculus]]"
    },
    {
      "title": "Stratonovich integral",
      "url": "https://en.wikipedia.org/wiki/Stratonovich_integral",
      "text": "In [[stochastic process]]es, the '''Stratonovich integral''' (developed simultaneously by [[Ruslan Stratonovich]] and [[Donald Fisk]]) is a [[stochastic integral]], the most common alternative to the [[Itô calculus|Itô integral]].  Although the Itô integral is the usual choice in applied mathematics, the Stratonovich integral is frequently used in physics.\n\nIn some circumstances, integrals in the Stratonovich definition are easier to manipulate.  Unlike the [[Itô calculus]], Stratonovich integrals are defined such that the [[chain rule]] of ordinary calculus holds.\n\nPerhaps the most common situation in which these are encountered is as the solution to Stratonovich [[stochastic differential equation]]s (SDEs).  These are equivalent to Itô SDEs and it is possible to convert between the two whenever one definition is more convenient.\n\n==Definition==\n\nThe Stratonovich integral can be defined in a manner similar to the [[Riemann integral]], that is as a [[limit (mathematics)|limit]] of [[Riemann sum]]s.  Suppose that <math>W : [0, T] \\times \\Omega \\to \\mathbb{R}</math> is a [[Wiener process]] and <math>X : [0, T] \\times \\Omega \\to \\mathbb{R}</math> is a [[semimartingale]] [[adapted process|adapted]] to the natural [[filtration (abstract algebra)|filtration]] of the Wiener process. Then the '''Stratonovich integral'''\n\n:<math>\\int_0^T  X_{t} \\circ \\mathrm{d} W_t</math>\n\nis a random variable <math>: \\Omega \\to \\mathbb{R}</math> defined as the [[Convergence in mean#Convergence in mean|limit in mean square]] of<ref>Gardiner (2004), p. 98 and the comment on p. 101</ref>\n\n:<math>\\sum_{i = 0}^{k - 1} {X_{t_{i+1}} + X_{t_i}\\over 2} \\left( W_{t_{i+1}} - W_{t_i} \\right)</math>\n\nas the [[Partition of an interval|mesh]] of the partition <math>0 = t_{0} < t_{1} < \\dots < t_{k} = T</math> of <math>[0, T]</math> tends to 0 (in the style of a [[Riemann–Stieltjes integral]]).\n\n==Calculation==\nMany integration techniques of ordinary calculus can be used for the Stratonovich integral, e.g.: if ''f'':'''R'''&rarr;'''R''' is a smooth function, then\n:<math>\\int_0^T  f'(W_t) \\circ \\mathrm{d} W_t = f(W_T)-f(W_0)</math>\nand more generally, if ''f'':'''R'''&times;'''R'''&rarr;'''R''' is a smooth function, then\n:<math>\\int_0^T  {\\partial f\\over\\partial W}(W_t,t) \\circ \\mathrm{d} W_t + \\int_0^T  {\\partial f\\over\\partial t}(W_t,t)\\, \\mathrm{d}t = f(W_T,T)-f(W_0,0).</math>\nThis latter rule is akin to the chain rule of ordinary calculus.\n\n===Numerical methods===\n\nStochastic integrals can rarely be solved in analytic form, making [[Stochastic calculus|stochastic]] [[numerical integration]] an important topic in all uses of stochastic integrals. Various numerical approximations converge to the Stratonovich integral, and variations of these are used to solve Stratonovich SDEs {{harv|Kloeden|Platen|1992}}.\nNote however that the most widely used Euler scheme (the [[Euler–Maruyama method]]) for the numeric solution of \n[[Langevin equation]]s requires the equation to be in Itô form.<ref>{{ cite journal | last1 = Perez-Carrasco R. | last2 = Sancho J.M. | title = Stochastic algorithms for discontinuous multiplicative white noise | journal = Phys. Rev. E | volume = 81 | pages = 032104 | year = 2010 | url = https://journals.aps.org/pre/abstract/10.1103/PhysRevE.81.032104 | doi=10.1103/PhysRevE.81.032104}}</ref>\n\n==Differential notation==\nIf ''X<sub>t</sub>'', ''Y<sub>t</sub>'' and ''Z<sub>t</sub>'' are stochastic processes such that\n:<math>X_T-X_0=\\int_0^T  Y_{t} \\circ \\mathrm{d} W_t + \\int_0^T  Z_{t} \\,\\mathrm{d}t</math>\nfor all ''T''>0, we also write\n:<math>\\mathrm{d}X=Y\\circ\\mathrm{d}W + Z\\,\\mathrm{d}t.</math>\nThis notation is often used to formulate [[stochastic differential equation]]s (SDEs), which are really equations about stochastic integrals. It is compatible with the notation from ordinary calculus, for instance\n:<math>\\mathrm{d}(t^2\\,W^3)=3 t^2 W^2\\circ\\mathrm{d}W + 2t W^3\\,\\mathrm{d}t.</math>\n\n==Comparison with the Itô integral==\n{{main|Itô calculus}}\n\nThe [[Itô calculus|Itô integral]] of the process ''X'' with respect to the Wiener process ''W'' is denoted by \n::<math>\\int_0^T  X_{t} \\,\\mathrm{d} W_t</math>\n(without the circle). For its definition, the same procedure is used as above in the definition of the Stratonovich integral, except for choosing the value of the process <math>X</math> at the left-hand endpoint of each subinterval, i.e.\n\n:<math>X_{t_{i}}</math> in place of <math>(X_{t_{i+1}}+ X_{t_{i}})/ 2</math>\n\nThis integral does not obey the ordinary chain rule as the Stratonovich integral does; instead one has to use the slightly more complicated [[Itô's lemma]].\n\nConversion between Itô and Stratonovich integrals may be performed using the formula\n\n:<math>\\int_{0}^{T} f(W_{t},t) \\circ \\mathrm{d} W_{t} = \\frac{1}{2} \\int_{0}^{T} {\\partial f\\over\\partial W}(W_{t},t)  \\, \\mathrm{d} t + \\int_{0}^{T} f(W_{t},t) \\, \\mathrm{d} W_{t},</math>\n\nwhere ƒ is any continuously differentiable function of two variables ''W'' and ''t'' and the last integral is an Itô integral {{harv|Kloeden|Platen|1992|p=101}}.\n\nIt follows that if ''X''<sub>''t''</sub> is a time-homogeneous Itô diffusion with continuously differentiable diffusion coefficient ''σ'' (i.e. it satisfies the [[Stochastic differential equation|SDE]] <math>\\mathrm{d} X_t = \\mu(X_t)\\,\\mathrm{d} t + \\sigma(X_t)\\,\\mathrm{d} W_t</math> ), we have\n\n:<math>\\int_{0}^{T} \\sigma (X_{t}) \\circ \\mathrm{d} W_{t} = \\frac{1}{2} \\int_{0}^{T} \\frac{d \\sigma}{dx}(X_{t}) \\sigma(X_{t})  \\, \\mathrm{d} t + \\int_{0}^{T} \\sigma (X_{t}) \\, \\mathrm{d} W_{t}.</math>\n\nMore generally, for any two [[semimartingale]]s ''X'' and ''Y''\n:<math>\\int_{0}^{T} X_{s-} \\circ \\mathrm{d} Y_s = \\int_0^T X_{s-}\\,\\mathrm{d}Y_s+ \\frac{1}{2} [X,Y]_T^c,</math>\nwhere <math> [X,Y]_T^c</math> is the continuous part of the [[quadratic variation|covariation]].\n\n==Stratonovich integrals in applications==\n\nThe Stratonovich integral lacks the important property of the Itô integral, which does not \"look into the future\". In many real-world applications, such as modelling stock prices, one only has information about past events, and hence the Itô interpretation is more natural. In financial mathematics the Itô interpretation is usually used.\n\nIn physics, however, stochastic integrals occur as the solutions of [[Langevin equation]]s. A Langevin equation is a coarse-grained version of a more microscopic model; depending on the problem in consideration, Stratonovich or Itô interpretation or even more exotic interpretations such as the isothermal interpretation, are appropriate. The Stratonovich interpretation is the most frequently used interpretation within the physical sciences.\n\nThe [[Wong–Zakai theorem]] states that physical systems with non-white noise spectrum characterized by a finite noise correlation time τ can be approximated by a Langevin equations with white noise in Stratonovich interpretation in the limit where τ tends to zero.{{citation needed|date=September 2016}}\n\nBecause the Stratonovich calculus satisfies the ordinary chain rule, stochastic differential equations (SDEs) in the Stratonovich sense are more straightforward to define on [[differentiable manifold]]s, rather than just on '''R'''<sup>''n''</sup>. The tricky chain rule of the Itô calculus makes it a more awkward choice for manifolds.\n\n== Stratonovich interpretation and supersymmetric theory of SDEs == \n{{Main|Supersymmetric theory of stochastic dynamics}}\n\nIn supersymmetric theory of SDEs, the finite-time stochastic evolution operator is given its most natural mathematical meaning of the stochastically averaged pullback induced on the exterior algebra of the phase space by the noise-configuration-dependent SDE-defined diffeomorphisms. This operator is unique and corresponds to the Stratonovich interpretation of SDEs. In addition, Stratonovich approach is equivalent to the Weyl symmetrization convention needed for disambiguation of the stochastic evolution operator during the transition from path integral to its operator representation. Moreover, in the appendix of Ref.,<ref>{{ cite journal | last = Ovchinnikov, I.V. | title = Introduction to supersymmetric theory of stochastics | journal = Entropy | volume = 18 | pages = 108 | year = 2016 | url = http://www.mdpi.com/1099-4300/18/4/108 | doi=10.3390/e18040108}}</ref> it is shown that the wide-spread argumentation stating that, unlike Ito approach, Stratonovich approach \"looks\" into the future is a misconception. None of the approaches to SDEs \"look\" into the future. The only advantage of the Ito approach is that the coordinate change at each time step is given as an explicit function of the current coordinate, whereas all other approaches to SDEs this function is implicit. This advantage, however, has no mathematical or physical significance and, consequently, the Ito approach does not have any advantages over, say, the Stratonovich approach to SDEs. At the same time, the use of the Ito approach leads to a stochastic evolution operator with the shifted flow vector field as compared to that of the original SDE under consideration.\n\n== Notes ==\n{{Reflist}}\n\n==References==\n\n* {{cite book | author=Øksendal, Bernt K. | authorlink=Bernt Øksendal | title=Stochastic Differential Equations: An Introduction with Applications | publisher=Springer, Berlin | year=2003 | isbn=3-540-04758-1}}\n* {{cite book | author=Gardiner, Crispin W. | title=Handbook of Stochastic Methods | publisher=Springer, Berlin Heidelberg | year=2004 | edition=3 | isbn=3-540-20882-8}}\n* {{cite journal | last1 = Jarrow | first1 = Robert | last2 = Protter | first2 = Philip | year = 2004 | title = A short history of stochastic integration and mathematical finance: The early years, 1880–1970 | url = | journal = IMS Lecture Notes Monograph | volume = 45 | issue = | pages = 1–17 }}\n* {{Cite book | last1=Kloeden | first1=Peter E. | last2=Platen | first2=Eckhard | title=Numerical solution of stochastic differential equations | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Applications of Mathematics | isbn=978-3-540-54062-5 | year=1992 | ref=harv | postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->}}.\n\n{{DEFAULTSORT:Stratonovich Integral}}\n[[Category:Articles with inconsistent citation formats]]\n[[Category:Definitions of mathematical integration]]\n[[Category:Stochastic calculus]]"
    },
    {
      "title": "Fractional calculus",
      "url": "https://en.wikipedia.org/wiki/Fractional_calculus",
      "text": "{{short description|branch of mathematical analysis with fractional applications of derivatives and integrals}}\n{{Calculus |Specialized}}\n'''Fractional calculus''' is a branch of [[mathematical analysis]] that studies the several different possibilities of defining [[real number]] powers or [[complex number]] powers of the [[derivative|differentiation operator]] {{mvar|D}}\n\n:<math>D f(x) = \\frac{d}{dx} f(x)\\,,</math>\n\nand of the integration operator {{mvar|J}}<ref group=Note>The symbol {{mvar|J}} is commonly used instead of the intuitive {{mvar|I}} in order to avoid confusion with other concepts identified by similar {{mvar|I}}–like [[glyph]]s, such as [[identity (mathematics)|identities]].</ref>\n\n:<math>J f(x) = \\int_0^x f(s) \\,ds\\,,</math>\n\nand developing a [[calculus]] for such operators generalizing the classical one.\n\nIn this context, the term ''powers'' refers to iterative application of a linear operator ''D'' to a function ''f'', that is, repeatedly [[function composition|composing]] ''D'' with itself, as in<math>D^2(f) = (D\\circ D)(f) = D(D(f))</math>.\n\nFor example, one may ask for a meaningful interpretion of:\n\n:<math>\\sqrt{D} = D^\\frac12</math>\n\nas an analogue of the [[functional square root]] for the differentiation [[operator (mathematics)|operator]], that is, an expression for some linear operator that when applied ''twice'' to any function will have the same effect as [[derivative|differentiation]]. More generally, one can look at the question of defining a [[linear functional]]\n\n:<math>D^a</math>\n\nfor every real-number {{mvar|a}} in such a way that, when {{mvar|a}} takes an [[integer]] value {{math|''n'' ∈ ℤ}}, it coincides with the usual {{mvar|n}}-fold differentiation {{mvar|D}} if {{math|''n'' > 0}}, and with the {{mvar|−n}}th power of {{mvar|J}} when {{math|''n'' < 0}}.\n\nOne of the motivations behind the introduction and study of these sorts of extensions of the differentiation operator {{mvar|D}} is that the [[Set (mathematics)|sets]] of operator powers {{math|{ ''D''{{isup|''a''}} {{!}}''a'' ∈ ℝ <nowiki>}</nowiki>}} defined in this way are ''continuous'' semigroups with parameter {{mvar|a}}, of which the original ''discrete'' semigroup of {{math|{ ''D''{{isup|''n''}} {{!}} ''n'' ∈ ℤ <nowiki>}</nowiki>}} for integer {{mvar|n}} is a [[denumerable set|denumerable]] subgroup: since continuous semigroups have a well developed mathematical theory, they can be applied to other branches of mathematics.\n\nFractional differential equations, also known as extraordinary differential equations, are a generalization of [[differential equation]]s through the application of fractional calculus.\n\n==Historical notes==\nIn [[applied mathematics]] and [[mathematical analysis]], '''fractional derivative''' is a derivative of any arbitrary order, real or complex. Its first appearance is in a letter written to [[Guillaume de l'Hôpital]] by [[Gottfried Wilhelm Leibniz]] in 1695.<ref name=\"Derivative\">{{Cite journal |last=Katugampola |first=Udita N. |date=15 October 2014 |title=A New Approach To Generalized Fractional Derivatives |url=http://www.bmathaa.org/httpdocs/repository/docs/BMAA6-4-1.pdf |journal=[[Bulletin of Mathematical Analysis and Applications]] |volume=6 |issue=4 |pages=1–15 |arxiv=1106.0965|bibcode=2011arXiv1106.0965K }}</ref> As far as the existence of such a theory is concerned, the foundations of the subject were laid by [[Liouville]] in a paper from 1832.<ref>For the history of the subject, see the thesis (in French): Stéphane Dugowson, [http://s.dugowson.free.fr/recherche/dones/index.html ''Les différentielles métaphysiques''] (''histoire et philosophie de la généralisation de l'ordre de dérivation''), Thèse, Université Paris Nord (1994)\n</ref>\nThe autodidact [[Oliver Heaviside]] introduced the practical use of [[operational calculus|fractional differential operators]] in electrical transmission line analysis circa 1890.<ref>For a historical review of the subject up to the beginning of the 20th century, see: {{cite journal|doi=10.1016/0315-0860(77)90039-8 | title=The development of fractional calculus 1695-1900 |year=1977|journal=Historia Mathematica | pages=75–89|author=Bertram Ross | volume=4}}</ref>\n\n==Nature of the fractional derivative==\n{{distinguish|Fractal derivative}}\n\nThe {{mvar|a}}th derivative of a function {{math|''f''&thinsp;(''x'')}} at a point {{mvar|x}} is a ''local property'' only when {{mvar|a}} is an integer; this is not the case for non-integer power derivatives. In other words, it is not correct to say that the fractional derivative at {{mvar|x}} of a function {{math|''f''&thinsp;(''x'')}} depends only on values of {{mvar|f}} very near {{mvar|x}}, in the way that integer-power derivatives certainly do. Therefore, it is expected that the theory involves some sort of [[boundary condition]]s, involving information on the function further out.<ref>{{Cite web|url=http://www.mathpages.com/home/kmath616/kmath616.htm|title=Fractional Calculus|website=www.mathpages.com|access-date=2018-01-03}}</ref>\n\nThe fractional derivative of a function to order {{mvar|a}} is often now defined by means of the [[Fourier transform|Fourier]] or [[Mellin transform|Mellin]] integral transforms.\n\n==Heuristics==\nA fairly natural question to ask is whether there exists a linear operator {{mvar|H}}, or half-derivative, such that\n\n:<math>H^2 f(x) = D f(x) = \\dfrac{d}{dx} f(x) = f'(x) \\,.</math>\n\nIt turns out that there is such an operator, and indeed for any {{math|''a'' > 0}}, there exists an operator {{mvar|P}} such that\n\n:<math>\\left(P ^ a f\\right)(x) = f'(x),</math>\n\nor to put it another way, the definition of {{math|{{sfrac|''d<sup>n</sup>y''|''dx<sup>n</sup>''}}}} can be extended to all real values of {{mvar|n}}.\n\nLet {{math|''f''&thinsp;(''x'')}} be a function defined for {{math|''x'' > 0}}. Form the definite integral from 0 to {{mvar|x}}. Call this\n\n:<math> ( J f ) ( x ) = \\int_0^x f(t) \\, dt \\,.</math>\n\nRepeating this process gives\n\n:<math> \\left( J^2 f \\right) ( x ) = \\int_0^x ( J f ) ( t ) \\,dt = \\int_0^x \\left( \\int_0^t f(s) \\, ds \\right) \\, dt \\,,</math>\n\nand this can be extended arbitrarily.\n\nThe [[Cauchy formula for repeated integration]], namely\n\n:<math> \\left(J^n f\\right) ( x ) = \\frac{ 1 }{ (n-1) ! } \\int_0^x \\left(x-t\\right)^{n-1} f(t) \\, dt \\,,</math>\n\nleads in a straightforward way to a generalization for real {{mvar|n}}.\n\nUsing the [[gamma function]] to remove the discrete nature of the factorial function gives us a natural candidate for fractional applications of the integral operator.\n\n:<math> \\left(J^\\alpha f\\right) ( x ) = \\frac{ 1 }{ \\Gamma ( \\alpha ) } \\int_0^x \\left(x-t\\right)^{\\alpha-1} f(t) \\, dt \\,.</math>\n\nThis is in fact a well-defined operator.\n\nIt is straightforward to show that the {{mvar|J}} operator satisfies\n\n:<math> \\left(J^\\alpha\\right) \\left(J^\\beta f\\right)(x) = \\left(J^\\beta\\right) \\left(J^\\alpha f\\right)(x) = \\left(J^{\\alpha+\\beta} f\\right)(x) = \\frac{ 1 }{ \\Gamma ( \\alpha + \\beta) } \\int_0^x \\left(x-t\\right)^{\\alpha+\\beta-1} f(t) \\, dt \\,.</math>\n\n:{| class=\"toccolours collapsible collapsed\" width=\"60%\" style=\"text-align:left\"\n!Proof\n|-\n|\n:<math>\n\\begin{align}\n\\left(J^\\alpha\\right) \\left(J^\\beta f\\right)(x) & = \\frac{1}{\\Gamma(\\alpha)} \\int_0^x (x-t)^{\\alpha-1} \\left(J^\\beta f\\right)(t) \\, dt \\\\\n& = \\frac{1}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\int_0^x \\int_0^t \\left(x-t\\right)^{\\alpha-1} \\left(t-s\\right)^{\\beta-1} f(s) \\, ds \\, dt \\\\\n& = \\frac{1}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\int_0^x f(s) \\left( \\int_s^x \\left(x-t\\right)^{\\alpha-1} \\left(t-s\\right)^{\\beta-1} \\, dt \\right) \\, ds\n\\end{align}\n</math>\n\nwhere in the last step we exchanged the order of integration and pulled out the {{math|''f''&thinsp;(''s'')}} factor from the {{mvar|t}} integration. Changing variables to {{mvar|r}} defined by {{math|''t'' {{=}} ''s'' + (''x'' − ''s'')''r''}},\n\n:<math> \\left(J^\\alpha\\right) \\left(J^\\beta f\\right)(x) = \\frac{1}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\int_0^x \\left(x-s\\right)^{\\alpha + \\beta - 1} f(s) \\left( \\int_0^1 \\left(1-r\\right)^{\\alpha-1} r^{\\beta-1} \\, dr \\right)\\, ds</math>\n\nThe inner integral is the [[beta function]] which satisfies the following property:\n\n:<math>\\int_0^1 \\left(1-r\\right)^{\\alpha-1} r^{\\beta-1} \\, dr = B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\,\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}</math>\n\nSubstituting back into the equation\n\n:<math> \\left(J^\\alpha\\right) \\left(J^\\beta f\\right)(x) = \\frac{1}{\\Gamma(\\alpha + \\beta)} \\int_0^x \\left(x-s\\right)^{\\alpha + \\beta - 1} f(s) \\, ds = \\left(J^{\\alpha + \\beta} f\\right)(x)</math>\n\nInterchanging {{mvar|α}} and {{mvar|β}} shows that the order in which the {{mvar|J}} operator is applied is irrelevant and completes the proof.\n|}\n\nThis relationship is called the semigroup property of fractional [[differintegral]] operators. Unfortunately the comparable process for the derivative operator {{mvar|D}} is significantly more complex, but it can be shown that {{mvar|D}} is neither [[commutative]] nor [[additive map|additive]] in general.<ref>{{harvnb|Kilbas|Srivastava|Trujillo|2006|page=[{{google books|plainurl=yes|id=LhkO83ZioQkC|page=75}} 75 (Property 2.4)]}}</ref>\n\n==Fractional derivative of a basic power function==\n[[File:Half-derivative.svg|right|thumb|320px|The half derivative (purple curve) of the function {{math|''f''&thinsp;(''x'') {{=}} ''x''}} (blue curve) together with the first derivative (red curve).]]\n\n[[File:Fractional Derivative of Basic Power Function (2014).gif|right|thumb|320px|The animation shows the derivative operator oscillating between the [[antiderivative]] ({{math|''α'' {{=}} −1}}: {{math|''y'' {{=}} {{sfrac|1|2}}''x''<sup>2</sup>}}) and the derivative ({{math|''α'' {{=}} +1}}: {{math|''y'' {{=}} 1}}) of the simple [[power function]] {{math|''y'' {{=}} ''x''}} continuously.]]\n\nLet us assume that {{math|''f''&thinsp;(''x'')}} is a [[monomial]] of the form\n\n:<math> f(x)=x^k\\,.</math>\n\nThe first derivative is as usual\n\n:<math> f'(x)=\\frac{d}{dx}f(x)=k x^{k-1}\\,.</math>\n\nRepeating this gives the more general result that\n\n:<math> \\frac{d^a}{dx^a}x^k=\\dfrac{k!}{(k-a)!}x^{k-a}\\,,</math>\n\nWhich, after replacing the [[factorial]]s with the [[gamma function]], leads us to\n\n:<math> \\frac{d^a}{dx^a}x^k=\\dfrac{\\Gamma(k+1)}{\\Gamma(k-a+1)}x^{k-a}, \\qquad k \\ge 0</math>\n\nFor {{math|''k'' {{=}} 1}} and {{math|''a'' {{=}} {{sfrac|1|2}}}}, we obtain the half-derivative of the function {{mvar|x}} as\n\n:<math> \\frac{d^\\frac12}{dx^\\frac12}x=\\frac{\\Gamma(1+1)}{\\Gamma\\left(1-\\frac12 + 1\\right)} x^{1-\\frac12}=\\frac{\\Gamma(2)}{\\Gamma\\left(\\frac{3}{2}\\right)}x^\\frac12 = \\frac{1}{\\frac{\\sqrt{\\pi}}{2}}x^\\frac12.</math>\n\nTo demonstrate that this is, in fact, the \"half derivative\" (where {{math|''H''{{isup|2}}''f''&thinsp;(''x'') {{=}} ''Df''&thinsp;(''x'')}}), we repeat the process to get:\n\n:<math>\\dfrac{d^\\frac12}{dx^\\frac12} \\dfrac{2x^{\\frac12}}{\\sqrt{\\pi}}=\\frac{2}{\\sqrt{\\pi}}\\dfrac{\\Gamma(1+\\frac12)}{\\Gamma(\\frac12-\\frac12+1)}x^{\\frac12-\\frac12}=\\frac{2}{\\sqrt{\\pi}} \\frac{\\Gamma\\left(\\frac{3}{2}\\right)}{\\Gamma(1)} x^0 = \\frac{2 \\frac{\\sqrt{\\pi}}{2} x^0}{\\sqrt{\\pi}}=1\\,,</math>\n\n(because {{math|Γ({{sfrac|3|2}}) {{=}} {{sfrac|1|2}}{{sqrt|π}}}} and {{math|Γ(1) {{=}} 1}}) which is indeed the expected result of\n\n:<math> \\left(\\frac{d^\\frac12}{dx^\\frac12} \\frac{d^\\frac12}{dx^\\frac12}\\right)x=\\frac d {dx} x = 1\\,.</math>\n\nFor negative integer power k, the gamma function is undefined and we have to use the following relation:<ref>{{Citation | last1 = Bologna | first1 = Mauro | title = Short Introduction to Fractional Calculus | publisher = Universidad de Tarapaca, Arica, Chile | url = http://www.uta.cl/charlas/volumen19/Indice/MAUROrevision.pdf}}</ref>\n\n:<math> \\frac{d^a}{dx^a}x^{-k}=\\left(-1\\right)^a\\dfrac{\\Gamma(k+a)}{\\Gamma(k)}x^{-(k+a)} \\quad\\text{ for } k \\ge 0</math>\n\nThis extension of the above differential operator need not be constrained only to real powers. For example, the {{math|(1 + ''i'')}}th derivative of the {{math|(1 − ''i'')}}th derivative yields the 2nd derivative. Also notice that setting negative values for {{mvar|a}} yields integrals.\n\nFor a general function {{math|''f''&thinsp;(''x'')}} and {{math|0 < ''α'' < 1}}, the complete fractional derivative is\n\n:<math>D^\\alpha f(x)=\\frac{1}{\\Gamma(1-\\alpha)}\\frac{d}{dx} \\int_0^x \\frac{f(t)}{\\left(x-t\\right)^\\alpha} \\, dt</math>\n\nFor arbitrary {{mvar|α}}, since the gamma function is undefined for arguments whose real part is a negative integer and whose imaginary part is zero, it is necessary to apply the fractional derivative after the integer derivative has been performed. For example,\n\n:<math>D^\\frac32 f(x) = D^\\frac12 D^1 f(x)=D^\\frac12 \\frac d {dx} f(x)</math>\n\n==Laplace transform==\nWe can also come at the question via the [[Laplace transform]]. Knowing that\n\n:<math>\\mathcal L \\left\\{Jf\\right\\}(s) = \\mathcal L \\left\\{\\int_0^t f(\\tau)\\,d\\tau\\right\\}(s)=\\frac1s\\bigl(\\mathcal L\\left\\{f\\right\\}\\bigr)(s)</math>\n\nand\n\n:<math>\\mathcal L \\left\\{J^2f\\right\\}=\\frac1s\\bigl(\\mathcal L \\left\\{Jf\\right\\} \\bigr)(s)=\\frac1{s^2}\\bigl(\\mathcal L\\left\\{f\\right\\}\\bigr)(s)</math>\n\nand so on, we assert\n\n:<math>J^\\alpha f=\\mathcal L^{-1}\\left\\{s^{-\\alpha}\\bigl(\\mathcal L\\{f\\}\\bigr)(s)\\right\\}</math>.\n\nFor example,\n\n:<math>J^\\alpha(t^k) = \\mathcal L^{-1}\\left\\{\\frac{\\Gamma(k+1)}{s^{\\alpha+k+1}}\\right\\} = \\frac{\\Gamma(k+1)}{\\Gamma(\\alpha+k+1)} t^{\\alpha+k} </math>\n\nas expected. Indeed, given the [[convolution]] rule\n\n:<math>\\mathcal L\\{f*g\\}=\\bigl(\\mathcal L\\{f\\}\\bigr)\\bigl(\\mathcal L\\{g\\}\\bigr)</math>\n\nand shorthanding {{math|''p''(''x'') {{=}} ''x''<sup>''α'' − 1</sup>}} for clarity, we find that\n\n:<math>\\begin{align}\n\\left(J^\\alpha f\\right)(t) &= \\frac{1}{\\Gamma(\\alpha)}\\mathcal L^{-1}\\left\\{\\bigl(\\mathcal L\\{p\\}\\bigr)\\bigl(\\mathcal L\\{f\\}\\bigr)\\right\\}\\\\\n&=\\frac{1}{\\Gamma(\\alpha)}(p*f)\\\\\n&=\\frac{1}{\\Gamma(\\alpha)}\\int_0^t p(t-\\tau)f(\\tau)\\,d\\tau\\\\\n&=\\frac{1}{\\Gamma(\\alpha)}\\int_0^t\\left(t-\\tau\\right)^{\\alpha-1}f(\\tau)\\,d\\tau\\\\\n\\end{align}</math>\n\nwhich is what Cauchy gave us above.\n\nLaplace transforms \"work\" on relatively few functions, but they ''are'' often useful for solving fractional differential equations.\n\n==Fractional integrals==\n\n===Riemann–Liouville fractional integral===\nThe classical form of fractional calculus is given by the [[Riemann–Liouville integral]], which is essentially what has been described above. The theory for [[periodic function]]s (therefore including the 'boundary condition' of repeating after a period) is the [[Weyl integral]]. It is defined on [[Fourier series]], and requires the constant Fourier coefficient to vanish (thus, it applies to functions on the [[unit circle]] whose integrals evaluate to 0). The Riemann-Liouville integral exists in two forms, upper and lower. Considering the interval {{math|[''a'',''b'']}}, the integrals are defined as\n\n:<math>_aD_t^{-\\alpha} f(t)={}_aI_t^\\alpha f(t)=\\frac{1}{\\Gamma(\\alpha)}\\int_a^t \\left(t-\\tau\\right)^{\\alpha-1} f(\\tau) \\, d\\tau </math>\n:<math>_tD_b^{-\\alpha} f(t)={}_tI_b^\\alpha f(t)=\\frac{1}{\\Gamma(\\alpha)}\\int_t^b \\left(\\tau-t\\right)^{\\alpha-1} f(\\tau) \\, d\\tau </math>\n\nWhere the former is valid for {{math|''t'' > ''a''}} and the latter is valid for {{math|''t'' < ''b''}}.<ref>{{cite book |last=Hermann |first=Richard |date=2014 |title=Fractional Calculus: An Introduction for Physicists |edition=2nd |location=New Jersey |publisher=World Scientific Publishing |page=46 |isbn=978-981-4551-07-6 |doi=10.1142/8934 |bibcode=2014fcip.book.....H}}</ref>\n\nBy contrast the [[Grünwald–Letnikov derivative]] starts with the derivative instead of the integral.\n\n===Hadamard fractional integral===\nThe ''Hadamard fractional integral'' is introduced by [[Jacques Hadamard]]<ref>{{Cite journal |last=Hadamard |first=J. |date=1892 |title=Essai sur l'étude des fonctions données par leur développement de Taylor |url=http://sites.mathdoc.fr/JMPA/PDF/JMPA_1892_4_8_A4_0.pdf |journal=Journal de Mathématiques Pures et Appliquées |volume=4 |issue=8 |pp=101–186}}</ref> and is given by the following formula,\n\n:<math>_a\\mathbf{D}_t^{-\\alpha} f(t) = \\frac{1}{\\Gamma(\\alpha)} \\int_a^t \\left(\\log\\frac{t}{\\tau} \\right)^{\\alpha -1} f(\\tau)\\frac{d\\tau}{\\tau}, \\qquad t > a\\,.</math>\n\n===Atangana–Baleanu fractional integral===\nRecently, using the generalized Mittag-Leffler function, Atangana and Baleanu suggested a new formulation of the fractional derivative with a nonlocal and nonsingular kernel. The integral is defined as:\n\n: <math>^{AB}_aD_t^{-\\alpha} f(t)=^{AB}_aI_t^\\alpha f(t)=\\frac{1-\\alpha}{AB(\\alpha)}f(t)+\\frac{\\alpha}{AB(\\alpha)\\Gamma(\\alpha)}\\int_a^t \\left(t-\\tau\\right)^{\\alpha-1} f(\\tau) \\, d\\tau, </math>\n\nwhere {{math|''AB''(''α'')}} is a normalization function such that {{math|''AB''(0) {{=}} ''AB''(1) {{=}} 1}}.<ref name=\"ReferenceA\">{{ cite arxiv |eprint=1602.03408|last1=Atangana|first1=Abdon|title=New Fractional Derivatives with Nonlocal and Non-Singular Kernel: Theory and Application to Heat Transfer Model|last2=Baleanu|first2=Dumitru|class=math.GM|year=2016}}</ref>\n\n==Fractional derivatives==\nUnlike classical Newtonian derivatives, a fractional derivative is defined via a fractional integral.\n[[File:Fractionalderivative.gif|thumb|Fractional derivatives of a Gaussian, interpolating continuously between the function and its first derivative.]]\n\n===Riemann–Liouville fractional derivative===\nThe corresponding derivative is calculated using Lagrange's rule for differential operators. Computing {{mvar|n}}th order derivative over the integral of order {{math|(''n'' − ''α'')}}, the {{mvar|α}} order derivative is obtained. It is important to remark that {{mvar|n}} is the nearest integer greater than {{mvar|α}} ( that is, {{math|''n'' {{=}} ⌊''α''⌋}}). Similar to the definitions for the Riemann-Liouville integral, the derivative has upper and lower variants.<ref>{{cite book |editor-last=Hermann |editor-first=Richard |date=2014 |title=Fractional Calculus: An Introduction for Physicists |journal=Fractional Calculus: An Introduction for Physicists |edition=2nd |location=New Jersey |publisher=World Scientific Publishing Co. |page=54 |isbn=978-981-4551-07-6|doi=10.1142/8934 |bibcode=2014fcip.book.....H }}</ref>\n\n:<math> _aD_t^\\alpha f(t)=\\frac{d^n}{dt^n} {}_aD_t^{-(n-\\alpha)}f(t)=\\frac{d^n}{dt^n} {}_aI_t^{n-\\alpha} f(t)</math>\n:<math> _tD_b^\\alpha f(t)=\\frac{d^n}{dt^n} {}_tD_b^{-(n-\\alpha)}f(t)=\\frac{d^n}{dt^n} {}_tI_b^{n-\\alpha} f(t)</math>\n\n===Caputo fractional derivative===\nAnother option for computing fractional derivatives is the Caputo fractional derivative. It was introduced by Michele Caputo in his 1967 paper.<ref>{{cite journal|last=Caputo|first=Michele|title=Linear model of dissipation whose ''Q'' is almost frequency independent. II|journal=Geophysical Journal International|year=1967|volume=13|issue=5|pages=529–539|doi=10.1111/j.1365-246x.1967.tb02303.x|bibcode=1967GeoJ...13..529C}}.</ref> In contrast to the Riemann-Liouville fractional derivative, when solving differential equations using Caputo's definition, it is not necessary to define the fractional order initial conditions. Caputo's definition is illustrated as follows.\n\n:<math> {}_a^C D_t^\\alpha f(t)=\\frac{1}{\\Gamma(n-\\alpha)} \\int_a^t \\frac{f^{(n)}(\\tau) \\, d\\tau}{\\left(t-\\tau\\right)^{\\alpha+1-n}}.</math>\n\nThere is the Caputo fractional derivative defined as:\n:<math> {} D^\\nu f(t)=\\frac{1}{\\Gamma(n-\\nu)} \\int_0^t (t-u)^{(n-\\nu-1)}f^{(n)}(u)du \\qquad (n-1)<\\nu<n</math>\nwhich has the advantage that is zero when {{math|''f''&thinsp;(''t'')}} is constant and its Laplace Transform is expressed by means of the initial values of the function and its derivative. Moreover, there is the Caputo fractional derivative of distributed order defined as\n\n:<math> {}_a^b D^\\nu f(t)=\\int_a^b \\phi(\\nu)\\left[D^{(\\nu)}f(t)\\right]\\,d\\nu=\\int_a^b\\left[\\frac{\\phi(\\nu)}{\\Gamma(1-\\nu)}\\int_0^t \\left(t-u\\right)^{-\\nu}f'(u)\\,du \\right]\\,d\\nu  </math>\nwhere {{math|''φ''(''ν'')}} is a weight function and which is used to represent mathematically the presence of multiple memory formalisms.\n\n===Atangana–Baleanu derivative===\nLike the integral, there is also a fractional derivative using the general Mittag-Leffler function as a kernel.<ref name=\"ReferenceA\"/> The authors introduced two versions, the Atangana–Baleanu in Caputo sense (ABC) derivative, which is the convolution of a local derivative of a given function with the generalized Mittag-Leffler function, and the Atangana–Baleanu in Riemann–Liouville sense (ABR) derivative, which is the derivative of a convolution of a given function that is not differentiable with the generalized Mittag-Leffler function.<ref>{{ cite journal |doi=10.1016/j.chaos.2016.02.012 |title=Chaos in a simple nonlinear system with Atangana–Baleanu derivatives with fractional order |journal=Chaos, Solitons & Fractals |volume=89 |pages=447–454 |year=2016 |last1=Atangana |first1=Abdon |last2=Koca |first2=Ilknur |bibcode=2016CSF....89..447A }}</ref> The Atangana-Baleanu fractional derivative in Caputo sense is defined as:\n\n: <math> {}_a^{ABC} D_t^\\alpha f(t)=\\frac{AB(\\alpha)}{1-\\alpha} \\int_a^t f'(\\tau) E_\\alpha \\left(-\\alpha\\frac{\\left(t-\\tau\\right)^\\alpha}{1-\\alpha}\\right)\\,d\\tau\\,.</math>\n\nAnd the Atangana–Baleanu fractional derivative in Riemann–Liouville is defined as:\n:<math> {}_a^{ABR} D_t^\\alpha f(t)=\\frac{AB(\\alpha)}{1-\\alpha}\\frac {d} {dt} \\int_a^t f(\\tau) E_\\alpha \\left(-\\alpha\\frac{\\left(t-\\tau\\right)^\\alpha}{1-\\alpha}\\right)\\,d\\tau\\,.</math>\n\n===Riesz derivative===\n\n:<math> \\mathcal{F} \\left\\{ \\frac{\\partial^\\alpha u}{\\partial \\left|x\\right|^\\alpha} \\right\\}(k) = -\\left|k\\right|^{\\alpha} \\mathcal{F} \\{u \\}(k) </math>\n\nwhere {{mathcal|F}} denotes the [[Fourier transform]].<ref>{{Cite journal |last=Chen |first=YangQuan |last2=Li |first2=Changpin |last3=Ding |first3=Hengfei |date=22 May 2014 |title=High-Order Algorithms for Riesz Derivative and Their Applications |journal=[[Abstract and Applied Analysis]] |volume=2014 |pages=1–17 |language=en |doi=10.1155/2014/653797}}</ref><ref>{{Cite journal |last=Bayın |first=Selçuk Ş. |date=5 December 2016 |title=Definition of the Riesz derivative and its application to space fractional quantum mechanics |journal=Journal of Mathematical Physics |volume=57 |issue=12 |pages=123501 |arxiv=1612.03046 |doi=10.1063/1.4968819 |bibcode=2016JMP....57l3501B }}</ref>\n\n===Other types===\nClassical fractional derivatives include:\n* [[Grünwald–Letnikov derivative]] \n* Sonin–Letnikov derivative\n* Liouville derivative\n* Caputo derivative\n* Hadamard derivative\n* Marchaud derivative\n* Riesz derivative\n* Riesz–Miller derivative\n* Miller–Ross derivative\n* Weyl derivative\n* Erdélyi–Kober derivative\n\nNew fractional derivatives include:\n* Machado derivative (This derivative does not exist anywhere in the literature)\n* Chen–Machado derivative\n* Coimbra derivative\n* Katugampola derivative\n* Caputo–Katugampola derivative\n* Hilfer derivative\n* Hilfer–Katugampola derivative\n* Davidson derivative\n* Chen derivative\n* Caputo Fabrizio derivative\n* Atangana–Baleanu derivative \n* Pichaghchi derivative\n\n==Generalizations==\n\n===Erdélyi–Kober operator===\nThe [[Erdélyi–Kober operator]] is an integral operator introduced by [[Arthur Erdélyi]] (1940).<ref>{{cite journal | last= Erdélyi | first= Arthur | authorlink= Arthur Erdélyi | title= On some functional transformations | journal= Rendiconti del Seminario Matematico Dell'Università e del Politecnico di Torino | volume= 10 | pages= 217–234 | year= 1950–51 | mr= 0047818 | ref= harv}}</ref> and [[Hermann Kober]] (1940)<ref>{{cite journal | last= Kober | first= Hermann | title= On fractional integrals and derivatives | journal= The Quarterly Journal of Mathematics | volume=os-11 | issue= 1 | pages= 193–211 | year= 1940 | doi= 10.1093/qmath/os-11.1.193 | ref= harv| bibcode= 1940QJMat..11..193K }}</ref> and is given by\n\n:<math>\\frac{x^{-\\nu-\\alpha+1}}{\\Gamma(\\alpha)}\\int_0^x \\left(t-x\\right)^{\\alpha-1}t^{-\\alpha-\\nu}f(t) \\,dt\\,, </math>\n\nwhich generalizes the [[#fractional_integrals|Riemann–Liouville fractional integral]] and the [[Weyl integral]].\n\n===Katugampola operators===\n\nA recent generalization introduced by Udita Katugampola is the following, which generalizes the [[#fractional_integrals|Riemann–Liouville fractional integral]] and the [[#fractional_integrals|Hadamard fractional integral]]. The integral is now known as the [[Katugampola fractional operators#Katugampola fractional integral|Katugampola fractional integral]] and is given by,<ref name=\"Derivative\" /><ref name=\"Integral\">{{cite journal | doi=10.1016/j.amc.2011.03.062 | volume=218 | issue=3 | title=New approach to a generalized fractional integral | year=2011 | journal=Applied Mathematics and Computation | pages=860–865 | last1 = Katugampola | first1 = Udita N.| arxiv=1010.0742 | citeseerx=10.1.1.752.7886 }}</ref>\n\n:<math> \\left ({}^\\rho \\mathcal{I}^\\alpha_{a+}f \\right )(x) = \\frac{\\rho^{1- \\alpha }}{\\Gamma({\\alpha})} \\int^x_a \\frac{\\tau^{\\rho-1} f(\\tau) }{\\left(x^\\rho - \\tau^\\rho\\right)^{1-\\alpha}}\\, d\\tau\\,, \\qquad x > a\\,. </math>\n\nEven though the integral operator in question is a close resemblance of the famous [[Erdélyi–Kober operator]], it is not possible to obtain the Hadamard fractional integral as a direct consequence of the Erdélyi–Kober operator. Also, there is a Katugampola-type fractional derivative, which generalizes the Riemann–Liouville and the Hadamard fractional derivatives.<ref name=\"Derivative\" /> As with the case of fractional integrals, the same is not true for the [[Erdélyi–Kober operator]].<ref name=\"Derivative\" />\n\n==Functional calculus==\nIn the context of [[functional analysis]], functions {{math|''f''&thinsp;(''D'')}} more general than powers are studied in the [[functional calculus]] of [[Spectral theorem|spectral theory]]. The theory of [[pseudo-differential operator]]s also allows one to consider powers of {{mvar|D}}. The operators arising are examples of [[singular integral operator]]s; and the generalisation of the classical theory to higher dimensions is called the theory of [[Riesz potential]]s. So there are a number of contemporary theories available, within which ''fractional calculus'' can be discussed. See also [[Erdélyi–Kober operator]], important in [[special function]] theory {{harv|Kober|1940}}, {{harv|Erdélyi|1950–51}}.\n\n==Applications==\n\n===Fractional conservation of mass===\nAs described by Wheatcraft and Meerschaert (2008),<ref>{{Cite journal |last=Wheatcraft |first=Stephen W. |last2=Meerschaert |first2=Mark M. |date=October 2008 |title=Fractional conservation of mass |url=https://www.stt.msu.edu/users/mcubed/fCOM.pdf |journal=Advances in Water Resources |language=en |volume=31 |issue=10 |pages=1377–1381 |doi=10.1016/j.advwatres.2008.07.004 |issn=0309-1708 |bibcode=2008AdWR...31.1377W }}</ref> a fractional conservation of mass equation is needed to model fluid flow when the [[control volume]] is not large enough compared to the scale of [[heterogeneity]] and when the flux within the control volume is non-linear. In the referenced paper, the fractional conservation of mass equation for fluid flow is:\n\n:<math>-\\rho \\left(\\nabla^\\alpha \\cdot \\vec{u} \\right) = \\Gamma(\\alpha +1)\\Delta x^{1-\\alpha} \\rho \\left (\\beta_s+\\phi \\beta_w \\right ) \\frac{\\partial p}{\\partial t} </math>\n\n===Groundwater flow problem===\nIn 2013–2014 Atangana ''et al.'' described some groundwater flow problems using the concept of derivative with fractional order.<ref>\n{{cite journal\n  |first1=Abdon|last1=Atangana\n  |first2=Necdet|last2=Bildik\n  |title=The Use of Fractional Order Derivative to Predict the Groundwater Flow\n  |year=2013\n  |journal=Mathematical Problems in Engineering\n  |volume=2013\n  |pages=1–9\n  |doi=10.1155/2013/543026\n}}</ref><ref>\n{{cite journal\n  |first1=Abdon|last1=Atangana\n  |first2=P. D.|last2=Vermeulen\n  |title=Analytical Solutions of a Space-Time Fractional Derivative of Groundwater Flow Equation\n  |year=2014\n  |journal=Abstract and Applied Analysis\n  |volume=2014\n  |pages=1–11\n  |doi=10.1155/2014/381753\n}}</ref> In these works, The classical [[Darcy law]] is generalized by regarding the water flow as a function of a non-integer order derivative of the piezometric head. This generalized law and the law of conservation of mass are then used to derive a new equation for groundwater flow.\n\n===Fractional advection dispersion equation===\nThis equation{{clarify|date=January 2017}} has been shown useful for modeling contaminant flow in heterogenous porous media.<ref>{{Cite journal |last=Benson |first=D. |last2=Wheatcraft |first2=S. |last3=Meerschaert |first3=M. |year=2000 |title=Application of a fractional advection-dispersion equation |journal=Water Resources Res |volume=36 |issue=6 |pages=1403–1412 |bibcode=2000WRR....36.1403B |citeseerx=10.1.1.1.4838 |doi=10.1029/2000wr900031}}</ref><ref>{{Cite journal |last=Benson |first=D. |last2=Wheatcraft |first2=S. |last3=Meerschaert |first3=M. |year=2000 |title=The fractional-order governing equation of Lévy motion |journal=Water Resources Res |volume=36 |issue=6 |pages=1413–1423 |bibcode=2000WRR....36.1413B |doi=10.1029/2000wr900032}}</ref><ref>{{Cite journal |last=Wheatcraft |first=Stephen W. |last2=Meerschaert |first2=Mark M. |last3=Schumer |first3=Rina |last4=Benson |first4=David A. |date=2001-01-01 |title=Fractional Dispersion, Lévy Motion, and the MADE Tracer Tests |journal=[[Transport in Porous Media]] |language=en |volume=42 |issue=1–2 |pages=211–240 |citeseerx=10.1.1.58.2062 |doi=10.1023/A:1006733002131 |issn=1573-1634}}</ref>\n\nAtangana and Kilicman extended the fractional advection dispersion equation to a variable order equation. In their work, the hydrodynamic dispersion equation was generalized using the concept of a [[variational order derivative]]. The modified equation was numerically solved via the [[Crank–Nicolson method]]. The stability and convergence in numerical simulations showed that the modified equation is more reliable in predicting the movement of pollution in deformable aquifers than equations with constant fractional and integer derivatives<ref name=Atangana2014a>\n{{cite journal\n  |first1=Abdon|last1=Atangana\n  |first2=Adem|last2=Kilicman\n  |title=On the Generalized Mass Transport Equation to the Concept of Variable Fractional Derivative\n  |journal=Mathematical Problems in Engineering\n  |volume=2014\n  |year=2014\n  |page=9\n  |doi=10.1155/2014/542809\n}}</ref>\n\n===Time-space fractional diffusion equation models===\nAnomalous diffusion processes in complex media can be well characterized by using fractional-order diffusion equation models.<ref>{{cite journal | last1 = Metzler | first1 = R. | last2 = Klafter | first2 = J. | year = 2000 | title = The random walk's guide to anomalous diffusion: a fractional dynamics approach | url = | journal = Phys. Rep. | volume = 339 | issue = 1| pages = 1–77 | doi=10.1016/s0370-1573(00)00070-3| bibcode = 2000PhR...339....1M }}</ref><ref>{{cite journal | last1 = Mainardi | first1 = F. | authorlink2 = Yuri Luchko | last2 = Luchko | first2 = Y. | last3 = Pagnini | first3 = G. | year = 2001 | title = The fundamental solution of the space-time fractional diffusion equation | arxiv = cond-mat/0702419 | journal = Fractional Calculus and Applied Analysis | volume = 4 | issue = 2| pages = 153–192 | bibcode = 2007cond.mat..2419M }}</ref> The time derivative term is corresponding to long-time heavy tail decay and the spatial derivative for diffusion nonlocality. The time-space fractional diffusion governing equation can be written as\n\n:<math> \\frac{\\partial^\\alpha u}{\\partial t^\\alpha}=-K (-\\Delta)^\\beta u.</math>\n\nA simple extension of fractional derivative is the variable-order fractional derivative, {{mvar|α}} and {{mvar|β}} are changed into {{math|''α''(''x'', ''t'')}} and {{math|''β''(''x'', ''t'')}}. Its applications in anomalous diffusion modeling can be found in reference.<ref name=Atangana2014a/><ref>{{Cite book |title=Processes with Long-Range Correlations |journal=Processes with Long-Range Correlations |last=Atangana |first=Abdon |last2=Baleanu |first2=Dumitru |date=2007 |editor-last=Rangarajan |editor-first=G. |series=Lecture Notes in Physics |volume=621 |pages=148 |chapter=Fractional Diffusion Processes: Probability Distributions and Continuous Time Random Walk |arxiv=0709.3990 |editor-last2=Ding |editor-first2=M.|bibcode=2003LNP...621..148G }}</ref><ref>{{Cite journal |last=Colbrook |first=Matthew J. |last2=Ma |first2=Xiangcheng |last3=Hopkins |first3=Philip F. |last4=Squire |first4=Jonathan |year=2017 |title=Scaling laws of passive-scalar diffusion in the interstellar medium |journal=[[Monthly Notices of the Royal Astronomical Society]] |volume=467 |issue=2 |pages=2421–2429 |arxiv=1610.06590 |doi=10.1093/mnras/stx261|bibcode=2017MNRAS.467.2421C }}</ref>\n\n===Structural damping models===\nFractional derivatives are used to model [[viscoelastic]] [[damping]] in certain types of materials like polymers.<ref>{{Cite book |title=Fractional Calculus and Waves in Linear Viscoelasticity |last=Mainardi |first=Francesco |date=May 2010 |publisher=[[Imperial College Press]] |isbn=9781848163294 |language=en |doi=10.1142/p614}}</ref>\n\n===PID controllers===\nGeneralizing [[PID controller]]s to use fractional orders can increase their degree of freedom. The new equation relating the ''control variable'' {{math|''u''(''t'')}} in terms of a measured ''error value'' {{math|''e''(''t'')}} can be written as\n\n:<math>u(t) = K_\\mathrm{p} e(t) + K_\\mathrm{i} D_t^{-\\alpha} e(t) + K_\\mathrm{d} D_t^{\\beta} e(t)</math>\n\nwhere {{mvar|α}} and {{math|β}} are positive fractional orders and {{math|''K''<sub>p</sub>}}, {{math|''K''<sub>i</sub>}}, and {{math|''K''<sub>d</sub>}}, all non-negative, denote the coefficients for the [[Proportional control|proportional]], [[integral]], and [[derivative]] terms, respectively (sometimes denoted {{mvar|P}}, {{mvar|I}}, and {{mvar|D}}).<ref>{{Cite journal |last=Tenreiro Machado |first=J. A. |last2=Silva |first2=Manuel F. |last3=Barbosa |first3=Ramiro S. |last4=Jesus |first4=Isabel S. |last5=Reis |first5=Cecília M. |last6=Marcos |first6=Maria G. |last7=Galhano |first7=Alexandra F. |date=2010 |title=Some Applications of Fractional Calculus in Engineering |journal=[[Mathematical Problems in Engineering]] |language=en |volume=2010 |pages=1–34 |doi=10.1155/2010/639801 }}</ref>\n\n===Acoustical wave equations for complex media===\nThe propagation of acoustical waves in complex media, such as in biological tissue, commonly implies attenuation obeying a frequency power-law. This kind of phenomenon may be described using a causal wave equation which incorporates fractional time derivatives:\n\n:<math>\\nabla^2 u -\\dfrac 1{c_0^2} \\frac{\\partial^2 u}{\\partial t^2} + \\tau_\\sigma^\\alpha \\dfrac{\\partial^\\alpha}{\\partial t^\\alpha}\\nabla^2 u - \\dfrac {\\tau_\\epsilon^\\beta}{c_0^2} \\dfrac{\\partial^{\\beta+2} u}{\\partial t^{\\beta+2}} = 0\\,.</math>\n\nSee also Holm & Näsholm (2011)<ref>{{Cite journal |last=Holm |first=S. |last2=Näsholm |first2=S. P. |year=2011 |title=A causal and fractional all-frequency wave equation for lossy media |journal=Journal of the Acoustical Society of America |volume=130 |issue=4 |pages=2195–2201 |bibcode=2011ASAJ..130.2195H |doi=10.1121/1.3631626 |pmid=21973374}}</ref> and the references therein. Such models are linked to the commonly recognized hypothesis that multiple relaxation phenomena give rise to the attenuation measured in complex media. This link is further described in Näsholm & Holm (2011b)<ref>{{Cite journal |last=Näsholm |first=S. P. |last2=Holm |first2=S. |year=2011 |title=Linking multiple relaxation, power-law attenuation, and fractional wave equations |journal=Journal of the Acoustical Society of America |volume=130 |issue=5 |pages=3038–3045 |bibcode=2011ASAJ..130.3038N |doi=10.1121/1.3641457 |pmid=22087931}}</ref> and in the survey paper,<ref name=\"Nasholm2\">{{Cite journal |last=Näsholm |first=S. P. |last2=Holm |first2=S. |year=2012 |title=On a Fractional Zener Elastic Wave Equation |arxiv=1212.4024 |doi=10.2478/s13540-013--0003-1 |bibcode=2012arXiv1212.4024N |doi-broken-date=2019-05-07 }}</ref> as well as the [[acoustic attenuation]] article. See Holm & Nasholm (2013)<ref name=\"HolmNasholm2014\">{{Cite journal |last=Holm |first=S. |last2=Näsholm |first2=S. P. |year=2013 |title=Comparison of fractional wave equations for power law attenuation in ultrasound and elastography |journal=Ultrasound in Medicine & Biology |volume=40 |issue=4 |pages=695–703 |arxiv=1306.6507 |citeseerx=10.1.1.765.120 |doi=10.1016/j.ultrasmedbio.2013.09.033 |pmid=24433745|bibcode=2013arXiv1306.6507H }}</ref> for a recent paper which compares fractional wave equations which model power-law attenuation.\n\n===Fractional Schrödinger equation in quantum theory===\nThe [[fractional Schrödinger equation]], a fundamental equation of [[fractional quantum mechanics]], has the following form:<ref>{{Cite journal |last=Laskin |first=N. |year=2002 |title=Fractional Schrodinger equation |journal=Phys. Rev. E |volume=66 |issue=5 |pages=056108 |arxiv=quant-ph/0206098 |citeseerx=10.1.1.252.6732 |doi=10.1103/PhysRevE.66.056108 |pmid=12513557|bibcode=2002PhRvE..66e6108L }}</ref><ref>{{Cite book | doi=10.1142/10541|title = Fractional Quantum Mechanics|year = 2018|last1 = Laskin|first1 = Nick| isbn=978-981-322-379-0| citeseerx=10.1.1.247.5449}}</ref>\n\n:<math>i\\hbar \\frac{\\partial \\psi (\\mathbf{r},t)}{\\partial t}=D_{\\alpha } \\left(-\\hbar^2\\Delta \\right)^{\\frac{\\alpha}{2}}\\psi (\\mathbf{r},t)+V(\\mathbf{r},t)\\psi (\\mathbf{r},t)\\,.</math>\n\nwhere the solution of the equation is the [[wavefunction]] {{math|''ψ''('''r''', ''t'')}} – the quantum mechanical [[probability amplitude]] for the particle to have a given [[position vector]] {{math|'''r'''}} at any given time {{mvar|t}}, and {{mvar|ħ}} is the [[Planck constant|reduced Planck constant]]. The [[potential energy]] function {{math|''V''('''r''', ''t'')}} depends on the system.\n\nFurther, {{math|Δ {{=}} {{sfrac|∂<sup>2</sup>|∂'''r'''<sup>2</sup>}}}} is the [[Laplace operator]], and {{mvar|D<sub>α</sub>}} is a scale constant with physical [[dimensional analysis|dimension]] {{math|[''D<sub>α</sub>''] {{=}} J<sup>1 − ''α''</sup>·m<sup>''α''</sup>·s<sup>−''α''</sup> {{=}} kg<sup>1 − ''α''</sup>·m<sup>2 − ''α''</sup>·s<sup>''α'' − 2</sup>}}, (at {{math|''α'' {{=}} 2}}, {{math|''D''<sub>2</sub> {{=}} {{sfrac|1|2''m''}}}} for a particle of mass {{mvar|m}}), and the operator {{math|(−''ħ''<sup>2</sup>Δ)<sup>''α''/2</sup>}} is the 3-dimensional fractional quantum Riesz derivative defined by\n\n:<math> (-\\hbar^2\\Delta)^\\frac{\\alpha}{2}\\psi (\\mathbf{r},t) = \\frac 1 {(2\\pi \\hbar)^3} \\int d^3 p e^{\\frac{i}{\\hbar} \\mathbf{p}\\cdot\\mathbf{r}}|\\mathbf{p}|^\\alpha \\varphi (\\mathbf{p},t) \\,.</math>\n\nThe index {{mvar|α}} in the fractional Schrödinger equation is the Lévy index, {{math|1 < ''α'' ≤ 2}}.\n\n====Variable-order fractional Schrödinger equation====\nAs a natural generalization of the [[fractional Schrödinger equation]], the variable-order fractional Schrödinger equation has been exploited to study fractional quantum phenomena:<ref>{{Cite journal |last=Bhrawy |first=A.H. |last2=Zaky |first2=M.A. |year=2017 |title=An improved collocation method for multi-dimensional space–time variable-order fractional Schrödinger equations |journal=Applied Numerical Mathematics |volume=111 |pages=197–218 |doi=10.1016/j.apnum.2016.09.009}}</ref>\n\n:<math>i\\hbar \\frac{\\partial \\psi^{\\alpha(\\mathbf{r})}  (\\mathbf{r},t)}{\\partial t^{\\alpha(\\mathbf{r})} }= \\left(-\\hbar^2\\Delta \\right)^{\\frac{\\beta(t)}{2}}\\psi (\\mathbf{r},t)+V(\\mathbf{r},t)\\psi (\\mathbf{r},t) \\,.</math>\n\nwhere  {{math|Δ {{=}} {{sfrac|∂<sup>2</sup>|∂'''r'''<sup>2</sup>}}}} is the [[Laplace operator]] and the operator {{math|(−''ħ''<sup>2</sup>Δ)<sup>''β''(''t'')/2</sup>}} is the variable-order fractional quantum Riesz derivative.\n\n==See also==\n{{div col|colwidth=30em}}\n*[[Acoustic attenuation]]\n*[[Autoregressive fractionally integrated moving average]]\n*[[Differintegral]]\n*[[Differential equation]]\n*[[Erdelyi–Kober operator]]\n*[[Riemann–Liouville integral]]\n* [[Weyl integral]]\n*[[Neopolarogram]]\n{{div col end}}\n\n===Other fractional theories===\n*[[Fractional dynamics]]\n*[[Fractional Fourier transform]]\n*[[Fractional quantum mechanics]]\n\n==Notes==\n{{Reflist|group=Note}}\n\n==References==\n{{Reflist|30em}}\n\n===Sources===\n* {{cite book\n  |title=Theory and Applications of Fractional Differential Equations\n  |last1=Kilbas|first1=Anatolii Aleksandrovich\n  |last2=Srivastava|first2=Hari Mohan\n  |last3=Trujillo|first3=Juan J.\n  |location=Amsterdam, Netherlands\n  |publisher=Elsevier\n  |year=2006\n  |isbn=978-0-444-51832-3\n|ref=harv}}\n\n==Further reading==\n=== Articles regarding the history of fractional calculus ===\n<!-- Order list by year/author -->\n* {{cite book\n  |first=B.|last=Ross\n  |title=A brief history and exposition of the fundamental theory of fractional calculus\n  |journal=Fractional Calculus and its Applications. Lecture Notes in Mathematics\n  |volume=457\n  |year=1975\n  |pages=1–36\n|doi=10.1007/BFb0067096\n  |series=Lecture Notes in Mathematics\n  |isbn=978-3-540-07161-7\n  }}\n* {{cite journal\n  |first=L.|last=Debnath\n  |title=A brief historical introduction to fractional calculus\n  |journal=International Journal of Mathematical Education in Science and Technology\n  |volume=35|issue=4\n  |year=2004\n  |pages=487–501\n  |doi=10.1080/00207390410001686571\n}}\n* {{cite journal\n  |first1=J.|last1=Tenreiro Machado\n  |first2=V.|last2=Kiryakova\n  |first3=F.|last3=Mainardi\n  |title=Recent history of fractional calculus\n  |journal=Communications in Nonlinear Science and Numerical Simulation\n  |volume=16|number=3\n  |year=2011\n  |pages=1140–1153 |doi=10.1016/j.cnsns.2010.05.027\n|bibcode=2011CNSNS..16.1140M|hdl=10400.22/4149\n  }}\n* {{cite journal \n  |first1=J.A.|last1=Tenreiro Machado\n  |first2=A.M.|last2=Galhano\n  |first3=J.J.|last3=Trujillo\n  |title=Science metrics on fractional calculus development since 1966\n  |journal=Fractional Calculus and Applied Analysis\n  |volume=16|number=2\n  |year=2013\n  |pages=479–500 |doi=10.2478/s13540-013-0030-y\n}}\n* {{cite journal\n  |first1=J.A.|last1=Tenreiro Machado\n  |first2=A.M.S.F.|last2=Galhano\n  |first3=J.J.|last3=Trujillo\n  |title=On development of fractional calculus during the last fifty years\n  |journal=Scientometrics\n  |volume=98|number=1\n  |year=2014\n  |pages=577–582 |doi=10.1007/s11192-013-1032-6\n|hdl=10400.22/3769\n  }}\n\n===Review articles===\n\n* {{Cite journal |last=Ortigueira |first=Manuel D. |last2=Machado |first2=J. A. Tenreiro |date=15 July 2015 |title=What is a fractional derivative? |journal=[[Journal of Computational Physics]] |language=en |volume=293 |pages=4–13 |bibcode=2015JCoPh.293....4O |doi=10.1016/j.jcp.2014.07.019 |issn=0021-9991}}\n** {{Cite journal |last=Katugampola |first=Udita N. |date=15 September 2016 |title=Correction to \"What is a fractional derivative?\" by Ortigueira and Machado [Journal of Computational Physics, Volume 293, 15 July 2015, Pages 4–13. Special issue on Fractional PDEs] |journal=[[Journal of Computational Physics]] |language=en |volume=321 |pages=1255–1257 |doi=10.1016/j.jcp.2016.05.052 |issn=0021-9991 |bibcode=2016JCoPh.321.1255K }}\n* {{Cite journal |last=Tarasov |first=Vasily E. |last2=Baleanu |first2=Dumitru |year=2013 |title=No violation of the Leibniz rule. No fractional derivative |journal=[[Communications in Nonlinear Science and Numerical Simulation]] |volume=18 |issue=11 |pages=2945–2948 |arxiv=1402.7161 |citeseerx=10.1.1.760.8672 |doi=10.1016/j.cnsns.2013.04.001|bibcode=2013CNSNS..18.2945T }}\n\n=== Books ===\n<!-- Order list by year/first author) -->\n* {{cite book\n  |title=The Fractional Calculus; Theory and Applications of Differentiation and Integration to Arbitrary Order\n  |series=Mathematics in Science and Engineering\n  |volume=V\n  |first1=Keith B.|last1=Oldham\n  |first2=Jerome|last2=Spanier\n  |publisher=Academic Press\n  |year=1974\n  |isbn=978-0-12-525550-9\n}}\n* {{cite book\n  |title=An Introduction to the Fractional Calculus and Fractional Differential Equations\n  |editor1-first=Kenneth S.|editor1-last=Miller\n  |editor2-first=Bertram|editor2-last=Ross \n  |publisher=John Wiley & Sons\n  |year=1993\n  |isbn=978-0-471-58884-9\n}}\n* {{cite book\n  |title=Fractional Integrals and Derivatives: Theory and Applications\n  |last1=Samko|first1=S.\n  |last2=Kilbas|first2=A.A.\n  |last3=Marichev|first3=O.\n  |publisher=Taylor & Francis Books\n  |isbn=978-2-88124-864-1\n  |year=1993\n}}\n* {{cite book\n  |title=Fractals and Fractional Calculus in Continuum Mechanics\n  |editor1-first=A.|editor1-last=Carpinteri\n  |editor2-first=F.|editor2-last=Mainardi\n  |publisher=Springer-Verlag Telos\n  |year=1998\n  |isbn=978-3-211-82913-4\n}}\n* {{cite book\n  |title=Fractional Differential Equations. An Introduction to Fractional Derivatives, Fractional Differential Equations, Some Methods of Their Solution and Some of Their Applications\n  |series=Mathematics in Science and Engineering\n  |volume=198\n  |first=Igor|last=Podlubny\n  |publisher=Academic Press\n  |year=1998\n  |isbn=978-0-12-558840-9\n}}\n* {{cite book\n  |title=Physics of Fractal Operators\n  |journal=Physics Today|volume=56|issue=12|pages=65|first1=Bruce J.|last1=West\n  |first2=Mauro|last2=Bologna\n  |first3=Paolo|last3=Grigolini\n  |publisher=Springer Verlag\n  |year=2003\n  |isbn=978-0-387-95554-4\n|bibcode=2003PhT....56l..65W|doi=10.1063/1.1650234}}\n* {{cite book\n  |url=http://www.worldscibooks.com/mathematics/p614.html\n  |title=Fractional Calculus and Waves in Linear Viscoelasticity: An Introduction to Mathematical Models.\n  |first=F.|last=Mainardi\n  |publisher=Imperial College Press\n  |year=2010\n}}\n* {{cite book\n  |url=https://www.springer.com/physics/complexity/book/978-3-642-14003-7\n  |title=Fractional Dynamics: Applications of Fractional Calculus to Dynamics of Particles, Fields and Media.\n  |first=V.E.|last=Tarasov\n  |publisher=Springer\n  |year=2010\n|isbn=9783642140037\n  |series=Nonlinear Physical Science\n  }}\n* {{cite book\n  |title=Basic Theory of Fractional Differential Equations\n  |first=Y.|last=Zhou\n  |publisher=World Scientific\n  |location=Singapore\n  |year=2010\n|doi=10.1142/9069\n  |isbn = 978-981-4579-89-6}}\n* {{cite book\n  |url=https://www.springer.com/physics/theoretical,+mathematical+%26+computational+physics/book/978-3-642-33910-3\n  |title=Fractional Derivatives for Physicists and Engineers\n  |journal=Fractional Derivatives for Physicists and Engineers: Background and Theory\n  |first=V.V.|last=Uchaikin\n  |publisher=Higher Education Press\n  |year=2012\n|isbn=9783642339103\n  |series=Nonlinear Physical Science\n  |bibcode=2013fdpe.book.....U\n  |doi=10.1007/978-3-642-33911-0\n  }}\n* {{cite book\n  |title=Fractional Calculus: Theory and Applications\n  |first=Varsha|last=Daftardar-gejji\n  |publisher=Narosa Publishing House\n  |year=2013\n|isbn = 978-8184873337}}\n\n* {{cite book\n  |title=Special Functions in Fractional Calculus and Related Fractional Differintegral Equations\n  |first=Hari M|last=Srivastava\n  |publisher=World Scientific\n  |location=Singapore\n  |year=2014\n|doi=10.1142/8936\n  |isbn = 978-981-4551-10-6}}\n* {{cite book\n  |url=https://www.crcpress.com/Numerical-Methods-for-Fractional-Calculus/Li-Zeng/p/book/9781482253801\n  |title=Numerical Methods for Fractional Calcuus\n  |first1=C P|last1=Li\n  |first2=F H|last2=Zeng\n  |publisher=CRC Press\n  |location=USA\n  |year=2015\n}}\n\n* {{cite book\n  |title=Fractional Calculus - An Introduction for Physicists\n  |edition=3rd\n  |first=R.|last=Herrmann\n  |publisher=World Scientific|location=Singapore\n  |year=2018\n  |doi=10.1142/11107\n  |isbn = 978-981-3274-57-0\n}}\n\n==External links==\n*[http://mathworld.wolfram.com/FractionalDifferentialEquation.html Eric W. Weisstein. \"Fractional Differential Equation.\"] From [[MathWorld]] &mdash; A Wolfram Web Resource.\n*[http://mathworld.wolfram.com/FractionalCalculus.html MathWorld – Fractional calculus]\n*[http://mathworld.wolfram.com/FractionalDerivative.html MathWorld – Fractional derivative]\n*[http://www.mathpages.com/home/kmath616/kmath616.htm Fractional Calculus] at MathPages\n*Specialized journal: [http://www.diogenes.bg/fcaa/ Fractional Calculus and Applied Analysis (1998–2014)] and [http://www.degruyter.com/view/j/fca Fractional Calculus and Applied Analysis (from 2015)]\n*Specialized journal: [https://archive.is/20120712033445/http://fde.ele-math.com/ Fractional Differential Equations (FDE)]\n*Specialized journal: [http://naturalspublishing.com/show.asp?JorID=48&pgid=0 Progress in Fractional Differentiation and Applications]\n*Specialized journal: [http://www.nonlinearscience.com/journal_2218-3892.php Communications in Fractional Calculus] ({{issn|2218-3892}})\n*Specialized journal: [http://math-frac.org/Journals/JFCA/default.php Journal of Fractional Calculus and Applications (JFCA)]\n*[http://www.nasatech.com/Briefs/Oct02/LEW17139.html www.nasatech.com]\n*[http://www.tuke.sk/podlubny/fc_resources.html Igor Podlubny's collection of related books, articles, links, software, etc. ]\n*[http://www.gigahedron.de GigaHedron – Richard Herrmann's collection of books, articles, preprints, etc. ]\n*[http://s.dugowson.free.fr/recherche/dones/index.html s.dugowson.free.fr]\n*[https://web.archive.org/web/20051029113800/http://www.nd.edu/~msen/Teaching/UnderRes/FracCalc.pdf History, Definitions, and Applications for the Engineer] ([[PDF]]), by Adam Loverro, [[University of Notre Dame]]\n*[http://www.fracalmo.org/ Fractional Calculus Modelling]\n*[http://www.xuru.org/fc/TOC.asp Introductory Notes on Fractional Calculus]\n*[https://web.archive.org/web/20120614004922/http://www.ismm.ac.cn/ismmlink/PLFD/index.html Power Law & Fractional Dynamics]\n*[http://cronetoolbox.ims-bordeaux.fr The CRONE Toolbox, a Matlab and Simulink Toolbox dedicated to fractional calculus, which is freely downloadable]\n*{{cite journal | doi = 10.1007/s002200050299  | arxiv = funct-an/9608002 | volume=192 | issue = 2 | title=Operator of Fractional Derivative in the Complex Plane | year=1998 | journal=Communications in Mathematical Physics | pages=261–285 | last1 = Závada | first1 = Petr| bibcode=1998CMaPh.192..261Z }}\n*{{cite journal | doi = 10.1155/S1110757X02110102 | volume=2 | issue=4 | title=Relativistic wave equations with fractional derivatives and pseudodifferential operators | year=2002 | journal=Journal of Applied Mathematics | pages=163–197 | last1 = Závada | first1 = Petr| arxiv=hep-th/0003126 }}\n{{Differential equations topics}}\n\n{{DEFAULTSORT:Fractional Calculus}}\n[[Category:Fractional calculus| ]]\n[[Category:Generalizations]]"
    },
    {
      "title": "Katugampola fractional operators",
      "url": "https://en.wikipedia.org/wiki/Katugampola_fractional_operators",
      "text": "In [[mathematics]], '''Katugampola fractional operators''' are [[integral operators]] that generalize the ''Riemann–Liouville'' and the ''Hadamard'' fractional operators into a unique form.<ref name=\"Integral\">{{cite journal|doi=10.1016/j.amc.2011.03.062|pages=860–865|title=New approach to a generalized fractional integral|journal=Applied Mathematics and Computation|volume=218|issue=3|year=2011|last1=Katugampola|first1=Udita N.|arxiv=1010.0742}}</ref><ref name=\"dist\">Katugampola, Udita N. (2011).  [http://opensiuc.lib.siu.edu/dissertations/387/ On Generalized Fractional Integrals and Derivatives], Ph.D. Dissertation, Southern Illinois University, Carbondale, August, 2011.</ref><ref name=\"derivative\" /><ref name=\"tran\" /> The '''Katugampola fractional integral''' generalizes both the [[Fractional calculus#Riemann Liouville fractional integral|Riemann–Liouville fractional integral]] and the [[Fractional calculus#Hadamard fractional integral|Hadamard fractional integral]] into a single form and It is also closely related to the [[Erdelyi–Kober operator|Erdelyi–Kober]] <ref name=\"erdelyi\">{{cite journal | last= Erdélyi | first= Arthur | authorlink= Arthur Erdélyi | title= On some functional transformations | journal= Rendiconti del Seminario Matematico dell'Università e del Politecnico di Torino | volume= 10 | pages= 217–234 | year= 1950–51 | mr=  0047818| ref= harv}}</ref><ref name=\"Kober\">{{cite journal | last= Kober | first= Hermann | title= On fractional integrals and derivatives | journal= The Quarterly Journal of Mathematics (Oxford Series) | volume= 11 | issue= 1 | pages= 193–211 | year= 1940 | doi= 10.1093/qmath/os-11.1.193 | ref= harv| bibcode= 1940QJMat..11..193K }}</ref><ref name=\"Samko\">''Fractional Integrals and Derivatives: Theory and Applications'', by Samko, S.; Kilbas, A.A.; and Marichev, O. Hardcover: 1006 pages. Publisher: Taylor & Francis Books. {{ISBN|2-88124-864-0}}</ref><ref name=\"KilSri\">''Theory and Applications of Fractional Differential Equations'', by Kilbas, A. A.; Srivastava, H. M.; and Trujillo, J. J. Amsterdam, Netherlands, Elsevier, February 2006. {{ISBN|0-444-51832-0}}</ref> operator that generalizes the Riemann–Liouville fractional integral. '''Katugampola fractional derivative'''<ref name=\"dist\" /><ref name=\"derivative\" /><ref name=\"tran\" /> has been defined using the [[Fractional calculus#Further generalizations|Katugampola fractional integral]] <ref name=\"derivative\" /> and as with any other [[fractional calculus|fractional differential operator]], it also extends the possibility of taking [[real number]] powers or [[complex number]] powers of the integral and [[derivative|differential operators]].\n\n== Definitions ==\nThese operators have been defined on the following extended-Lebesgue space.\n\nLet <math>\\textit{X}^p_c(a,b), \\; c\\in \\mathbb{R}, \\, 1 \\leq p \\leq \\infty </math> be the space of those Lebesgue measurable functions <math> f </math> on <math> [a, b] </math> for which <math>\\|f\\|_{\\textit{X}^p_c} < \\infty </math>, where the norm is defined by <ref name=\"Integral\" />\n&nbsp;\n::<math>\n\\begin{align}\n\\|f\\|_{\\textit{X}^p_c} =\\Big(\\int^b_a |t^c f(t)|^p \\frac{dt}{t}\\Big)^{1/p} < \\infty,\n\\end{align}\n</math>\n&nbsp;\nfor <math> 1 \\leq p < \\infty,\\, c \\in \\mathbb{R} </math> and for the case <math> p=\\infty </math>\n&nbsp;\n::<math>\n\\begin{align}\n\\|f\\|_{\\textit{X}^\\infty_c} = \\text{ess sup}_{a \\leq t \\leq b} [t^c|f(t)|],  \\quad ( c \\in \\mathbb{R}).\n\\end{align}\n</math>\n\n== Katugampola fractional integral ==\nIt is defined via the following integrals <ref name=\"Integral\" /><ref name=\"dist\" /><ref name=\"langevin\">{{cite journal|doi=10.1186/s13662-015-0712-3|title=On the nonlocal Katugampola fractional integral conditions for fractional Langevin equation|journal=Advances in Difference Equations|volume=2015|year=2015|last1=Thaiprayoon|first1=Chatthai|last2=Ntouyas|first2=Sotiris K|last3=Tariboon|first3=Jessada}}</ref><ref name=\"appro \">{{cite journal|arxiv=1512.03791|title=An approximation formula for the Katugampola integral|journal= J. Math. Anal. |volume=7|issue=1|pages= 23–30 |year=2016|last1=Almeida|first1=R.|last2=Bastos|first2=N. |url=https://dl.dropboxusercontent.com/u/1639385/JMA7-1/JMA7-1-4.pdf|bibcode=2015arXiv151203791A}}</ref><ref name=\"googlesite\">{{cite journal|first=Udita|last=Katugampola|title=Google Site|url=https://sites.google.com/site/uditanalin/research-1|access-date=11 November 2017}}</ref>\n{{NumBlk|:|<math> ({}^\\rho \\mathcal{I}^\\alpha_{a+}f)(x) = \\frac{\\rho^{1- \\alpha }}{\\Gamma(\\alpha)} \\int^x_a \\frac{\\tau^{\\rho-1} f(\\tau) }{(x^\\rho - \\tau^\\rho)^{1-\\alpha}}\\, d\\tau, </math>|{{EquationRef|1}}}}\n&nbsp;\nfor <math> x > a </math> and <math> \\operatorname{Re}(\\alpha) > 0. </math> This integral is called the ''left-sided'' fractional integral. Similarly, the ''right-sided'' fractional integral is defined by,\n&nbsp;\n{{NumBlk|:|<math> ({}^\\rho \\mathcal{I}^\\alpha_{b-}f)(x) = \\frac{\\rho^{1- \\alpha }}{\\Gamma({\\alpha})} \\int^b_x \\frac{\\tau^{\\rho-1} f(\\tau) }{(\\tau^\\rho - x^\\rho)^{1-\\alpha}}\\, d\\tau. </math>|{{EquationRef|2}}}}\n&nbsp;\nfor <math>\\textstyle x < b</math> and <math>\\textstyle\\operatorname{Re}(\\alpha) > 0</math>.\n\nThese are the fractional generalizations of the <math>n</math>-fold left- and right-integrals of the form\n\n: <math> \\int_a^x t_1^{\\rho-1} \\, dt_1 \\int_a^{t_1} t_2^{\\rho-1} \\,dt_2 \\cdots \\int_a^{t_{n -1}} t_n^{\\rho-1} f(t_n)\\,dt_n</math>\n\nand\n\n: <math> \\int_x^b t_1^{\\rho-1} \\,dt_1 \\int^b_{t_1} t_2^{\\rho-1} \\,dt_2 \\cdots \\int^b_{t_{n -1}} t_n^{\\rho-1} f(t_n) \\, dt_n</math> for <math>\\textstyle n \\in \\mathbb{N},</math>\n\nrespectively. Even though the integral operators in question are close resemblance of the famous [[Erdélyi–Kober operator]], it is not possible to obtain the Hadamard fractional integrals as a direct consequence of the Erdélyi–Kober operators. Also, there is a corresponding fractional derivative, which generalizes the ''Riemann–Liouville'' and the ''Hadamard fractional derivatives''. As with the case of fractional integrals, the same is not true for the Erdélyi–Kober operator.\n\n== Katugampola fractional derivative ==\nAs with the case of other fractional derivatives, it is defined via the Katugampola fractional integral.<ref name=\"derivative\">{{citation | last= Katugampola |first=Udita N.| title= ''New Approach to Generalized Fractional Derivatives'' | journal= Bull. Math. Anal. App.| volume= 6|issue =4 | pages= 1–15| year= 2014| mr= 3298307 |url=http://www.emis.de/journals/BMAA/repository/docs/BMAA6-4-1.pdf}}</ref><ref name=\"langevin\" /><ref name=\"appro \" /><ref name=\"googlesite\"/>\n\nLet <math>\\alpha \\in \\mathbb{C},\\ \\operatorname{Re}(\\alpha) \\geq 0, n=[\\operatorname{Re}(\\alpha)]+1</math> and <math>\\rho >0.</math> The generalized fractional derivatives, corresponding to the generalized fractional integrals ({{EquationNote|1}}) and ({{EquationNote|2}}) are defined, respectively, for <math> 0 \\leq a < x < b \\leq \\infty </math>, by\n[[File:Fig1-Katugampola.jpg|right|thumb|320px|The half-derivative of the function <math> f(x) = x^{0.5}</math> for the Katugampola fractional derivative.]]\n[[File:Fig2-Katugampola.jpg|right|thumb|320px|The half derivative of the function <math> f(x) = x^\\nu</math> for the Katugampola fractional derivative for <math>\\alpha = 0.5</math> and <math>\\rho = 2</math>.]]\n:<math>\\begin{align}\n\\big({}^\\rho \\mathcal{D}^\\alpha_{a+}f\\big)(x)&= \\bigg(x^{1-\\rho} \\,\\frac{d}{dx}\\bigg)^n\\,\\, \\big({}^\\rho \\mathcal{I}^{n-\\alpha}_{a+}f\\big)(x)\\\\\n &= \\frac{\\rho^{\\alpha-n+1 }}{\\Gamma({n-\\alpha})} \\, \\bigg(x^{1-\\rho} \\,\\frac{d}{dx}\\bigg)^n \\int^x_a \\frac{\\tau^{\\rho-1} f(\\tau) }{(x^\\rho - \\tau^\\rho)^{\\alpha-n+1}}\\, d\\tau,\n\\end{align}</math>\nand\n:<math>\\begin{align}\n\\big({}^\\rho \\mathcal{D}^\\alpha_{b-}f\\big)(x) &= \\bigg(-x^{1-\\rho} \\,\\frac{d}{dx}\\bigg)^n\\,\\, \\big({}^\\rho \\mathcal{I}^{n-\\alpha}_{b-}f\\big)(x)\\\\\n &= \\frac{\\rho^{\\alpha-n+1 }}{\\Gamma({n-\\alpha})}\\bigg(-x^{1-\\rho}\\frac{d}{dx}\\bigg)^n \\int^b_x\\frac{\\tau^{\\rho-1} f(\\tau) }{(\\tau^\\rho - x^\\rho)^{\\alpha-n+1}}\\, d\\tau,\n\\end{align}</math>\nrespectively, if the integrals exist.\n\nThese operators generalize the Riemann–Liouville and Hadamard fractional derivatives into a single form, while the Erdelyi–Kober fractional is a generalization of the Riemann–Liouville fractional derivative.<ref name=\"derivative\" /> When, <math> b=\\infty </math>, the fractional derivatives are referred to as [[weyl integral|Weyl-type]] derivatives.\n\n=== Caputo–Katugampola fractional derivative ===\nThere is a Caputo-type modification of the Katugampola derivative that is now known as the Caputo–Katugampola fractional derivative.<ref>{{cite journal|doi=10.1007/s10957-016-0883-4|arxiv=1601.07376|title=Variational Problems Involving a Caputo-Type Fractional Derivative|journal=Journal of Optimization Theory and Applications|volume=174|issue=1|pages=276–294|year=2017|last=Almeida|first=Ricardo}}</ref><ref>{{cite journal|doi=10.1016/j.amc.2017.07.003|title=Fractional differential equations of Caputo–Katugampola type and numerical solutions|journal=Applied Mathematics and Computations|volume=315|pages=549–554|year=2017|last1=Zeng|first1=Sheng-Da|last2=Baleanu|first2=Dumitru|last3=Bai|first3=Yunru|first4=Guocheng|last4=Wu}}</ref>\nLet <math> f \\in L^1[a, b], \\alpha \\in (0, 1]</math> and <math> \\rho </math>. The C-K fractional derivative of order <math> \\alpha </math> of the function <math> f:[a,b] \\rightarrow \\mathbb{R},</math> with respect to parameter <math> \\rho </math> can be expressed as\n\n:<math> {}^C\\mathcal{D}^{\\alpha, \\rho}_{a+}f(t)=\\frac{\\rho^\\alpha t^{1-\\alpha}}{\\Gamma(1-\\alpha)}\\frac{d}{dt}\\int^t_a\\frac{s^{\\rho-1}}{(t^\\rho-s^\\rho)^\\alpha}\\big[f(s)-f(a)\\big]\\,ds. </math>\n\nIt satisfies the following result. Assume that <math> f \\in C^1[a, b]  </math>, then the C-K derivative has the following equivalent form {{cn|date=February 2019}}\n::<math>\n{}^C\\mathcal{D}^{\\alpha, \\rho}_{a+}f(t)=\\frac{\\rho^\\alpha }{\\Gamma(1-\\alpha)}\\int^t_a \\frac{f^\\prime(s)}{(t^\\rho-s^\\rho)^\\alpha}ds.\n</math>\n\n=== Hilfer–Katugampola fractional derivative ===\nAnother recent generalization is the ''Hilfer-Katugampola'' fractional derivative.<ref>{{cite arxiv|eprint=1705.07733|title=Hilfer-Katugampola fractional derivative|year=2017|last1=Oliveira|first1=D.S.|first2=E.|last2=Capelas de Oliveira|class=math.CA}}</ref><ref>{{cite arxiv|eprint=1709.08838|title=Existence and Stability of Fractional Differential Equations Involving Generalized Katugampola Derivative|year=2017|last1=Bhairat|first1=Sandeep P.|first2=D.B.|last2=Dhaigude|class=math.CA}}</ref> Let order <math>0<\\alpha<1</math> and type <math>0\\leq{\\beta}\\leq{1}</math>. The fractional derivative (left-sided/right-sided),\nwith respect to <math>x</math>, with <math>\\rho>0</math>, is defined by\n\n:<math>\\begin{align}\n({^{\\rho}\\mathcal{D}^{\\alpha,\\beta}_{a\\pm}}\\varphi)(x)&=\\left(\\pm\\,{^{\\rho}\\mathcal{J}_{a\\pm}^{\\beta(1-\\alpha)}}\\left(t^{\\rho-1}\\frac{d}{dt}\\right){^{\\rho}\\mathcal{J}_{a\\pm}^{(1-\\beta)(1-\\alpha)}}\\varphi\\right)(x)\\\\\n&=\\left(\\pm\\,{^{\\rho}\\mathcal{J}_{a\\pm}^{\\beta(1-\\alpha)}}\\delta_{\\rho}\\,{^{\\rho}\\mathcal{J}_{a\\pm}^{(1-\\beta)(1-\\alpha)}}\\varphi\\right)(x),\n\\end{align}\n</math>\nwhere <math>\\delta_{\\rho}= t^{\\rho-1}\\frac{d}{dt}</math>, for functions <math> \\varphi </math> in which the expression on the right hand side \nexists, where <math>\\mathcal{J}</math> is the generalized fractional integral \ngiven in ({{EquationNote|1}}).\n\n== Mellin transform ==\nAs in the case of [[Laplace transform]]s, [[Mellin transform]]s will be used specially when solving [[differential equation]]s. The Mellin transforms of the ''left-sided'' and ''right-sided'' versions of Katugampola Integral operators are given by <ref name=\"dist\" /><ref name=\"tran\">{{cite journal|pages=566–580|arxiv=1112.6031|doi=10.1016/j.amc.2014.12.067|title=Mellin transforms of generalized fractional integrals and derivatives|journal=Applied Mathematics and Computation|volume=257|year=2015|last1=Katugampola|first1=Udita N.}}</ref>\n\n=== Theorem ===\nLet <math>\\alpha \\in \\mathcal{C},\\  \\operatorname{Re}(\\alpha) > 0,</math> and <math>\\rho >0.</math> Then,\n::<math>\n\\begin{align}\n & \\mathcal{M}\\bigg({}^\\rho \\mathcal{I}^\\alpha_{a+}f\\bigg)(s) = \\frac{\\Gamma\\big(1-\\frac{s}{\\rho}-\\alpha\\big)}{\\Gamma\\big(1-\\frac{s}{\\rho}\\big)\\,\\rho^\\alpha}\\, \\mathcal{M}f(s + \\alpha\\rho), \\quad \\operatorname{Re}(s/\\rho + \\alpha) < 1, \\, x > a,    \\\\\n & \\mathcal{M}\\bigg({}^\\rho \\mathcal{I}^\\alpha_{b-}f\\bigg)(s) = \\frac{\\Gamma\\big(\\frac{s}{\\rho}\\big)}{\\Gamma\\big(\\frac{s}{\\rho} + \\alpha\\big)\\,\\rho^\\alpha}\\, \\mathcal{M}f(s + \\alpha\\rho), \\quad \\operatorname{Re}(s/\\rho) > 0, \\, x < b, \\\\\n\\end{align}\n</math>\n\nfor <math>f \\in \\textit{X}^1_{s + \\alpha\\rho}(\\mathbb{R^+})</math>, if <math>\\mathcal{M}f(s + \\alpha\\rho)</math> exists for <math> s\\in \\mathbb{C}</math>.\n\n==Hermite-Hadamard type inequalities==\nKatugampola operators satisfy the following Hermite-Hadamard type inequalities:<ref name=\"H-H\">{{cite journal|title =On Hermite-Hadamard Type Inequalities via Generalized Fractional Integrals| last1=M. Jleli |last2=D. O'Regan |last3=B. Samet | journal=Turkish Journal of Mathematics |year =2016|volume = 40|pages = 1221–1230|url=http://journals.tubitak.gov.tr/math/issues/mat-16-40-6/mat-40-6-4-1507-79.pdf| doi=10.3906/mat-1507-79 }}</ref>\n\n=== Theorem ===\nLet <math>\\alpha > 0 </math> and <math>\\rho >0.</math>. If <math> f </math> is a convex function on <math> [a, b] </math>, then \n::<math>\nf\\left(\\frac{a+b}{2}\\right) \\leq \\frac{\\rho^\\alpha\\Gamma(\\alpha +1)}{4(b^\\alpha -a^\\alpha)^\\alpha}\\left[{}^\\rho \\mathcal{I}^\\alpha_{a+}F(b)+{}^\\rho \\mathcal{I}^\\alpha_{b-}F(a)\\right] \\leq \\frac{f(a)+f(b)}{2},\n</math>\nwhere <math> F(x) = f(x) + f(a+b-x), \\; x \\in [a, b]. </math>.\n\nWhen <math> \\rho \\rightarrow 0^+ </math>, in the above result, the following Hadamard type inequality holds:<ref name=\"H-H\" />\n\n===Corollary===\nLet <math>\\alpha > 0 </math>. If <math> f </math> is a convex function on <math> [a, b] </math>, then \n::<math>\nf\\left(\\frac{a+b}{2}\\right) \\leq \\frac{\\Gamma(\\alpha +1)}{4\\left(\\ln \\frac{b}{a}\\right)^\\alpha}\\left[ \\mathbf{I}^\\alpha_{a+}F(b)+ \\mathbf{I}^\\alpha_{b-}F(a)\\right] \\leq \\frac{f(a)+f(b)}{2},\n</math>\nwhere <math> \\mathbf{I}^\\alpha_{a+}</math> and <math> \\mathbf{I}^\\alpha_{b-}</math> are left- and right-sided [[Fractional calculus#Hadamard fractional integral|Hadamard fractional integrals]].\n\n== Recent Development ==\nThese operators have been mentioned in the following works:\n# ''Fractional Calculus. An Introduction for Physicists'', by Richard Herrmann <ref>''Fractional Calculus. An Introduction for Physicists'', by Richard Herrmann. Hardcover. Publisher: World Scientific, Singapore; (February 2011) {{isbn|978-981-4340-24-3}}</ref>\n# ''Fractional Calculus of Variations in Terms of a Generalized Fractional Integral with Applications to Physics'', Tatiana Odzijewicz,  Agnieszka B. Malinowska and Delfim F. M. Torres, Abstract and Applied Analysis, Vol 2012 (2012), Article ID 871912, 24 pages'' <ref>{{cite journal|doi=10.1155/2012/871912|title=Fractional Calculus of Variations in Terms of a Generalized Fractional Integral with Applications to Physics|journal=Abstract and Applied Analysis|volume=2012|pages=1–24|year=2012|last1=Odzijewicz|first1=Tatiana|last2=Malinowska|first2=Agnieszka B.|last3=Torres|first3=Delfim F. M.}}</ref>\n# ''[https://books.google.com/books?id=0sD_ugAACAAJ Introduction to the Fractional Calculus of Variations]'', Agnieszka B Malinowska and Delfim F. M. Torres, Imperial College Press, 2015\n# ''[https://books.google.com/books?id=QkOMBgAAQBAJ Advanced Methods in the Fractional Calculus of Variations]'', Malinowska, Agnieszka B., Odzijewicz, Tatiana, Torres, Delfim F.M., Springer, 2015\n# ''Expansion formulas in terms of integer-order derivatives for the Hadamard fractional integral and derivative'', Shakoor Pooseh, Ricardo Almeida, and Delfim F. M. Torres, Numerical Functional Analysis and Optimization, Vol 33, Issue 3, 2012, pp 301–319.<ref>{{cite journal|doi= 10.1080/01630563.2011.647197|title= Expansion Formulas in Terms of Integer-Order Derivatives for the Hadamard Fractional Integral and Derivative|journal= Numerical Functional Analysis and Optimization|volume= 33|issue= 3|pages= 301|year= 2012|last1= Pooseh|first1= Shakoor|last2= Almeida|first2= Ricardo|last3= Torres|first3= Delfim F. M.|arxiv= 1112.0693}}</ref>\n\n== References ==\n{{reflist}}\n\n== Further reading ==\n{{refbegin}}\n*{{cite book |first=Kenneth S. |last=Miller |editor-first=Bertram |editor-last=Ross |title=An Introduction to the Fractional Calculus and Fractional Differential Equations |publisher=Wiley |year=1993 |isbn=0-471-58884-9 }}\n*{{cite book |first=Keith B. |last=Oldham |first2=Jerome |last2=Spanier |title=The Fractional Calculus; Theory and Applications of Differentiation and Integration to Arbitrary Order |publisher=Academic Press |series=Mathematics in Science and Engineering |volume=V |year=1974 |isbn=0-12-525550-0 }}\n*{{cite book |first=Igor |last=Podlubny |title=Fractional Differential Equations. An Introduction to Fractional Derivatives, Fractional Differential Equations, Some Methods of Their Solution and Some of Their Applications |publisher=Academic Press |series=Mathematics in Science and Engineering |volume=198 |year=1998 |isbn=0-12-558840-2 }}\n*{{cite book |first=Richard |last=Herrmann |title=Fractional Calculus. An Introduction for Physicists |publisher=World Scientific |year=2011 |isbn=978-981-4340-24-3 }}\n*{{cite journal |first=J.T. |last=Machado |first2=V. |last2=Kiryakova |first3=F. |last3=Mainardi |title=Recent history of fractional calculus |journal=Communications in Nonlinear Science and Numerical Simulations |volume=16 |issue=3 |pages=1140 |url=http://mechatronics.ece.usu.edu/foc/wcica2010tw/Recent%20History%20of%20Fractional%20Calculus-typeset.pdf|bibcode=2011CNSNS..16.1140M |year=2011 |doi=10.1016/j.cnsns.2010.05.027 |hdl=10400.22/4149 }}\n{{refend}}\n\n== Notes ==\nThe CRONE (R) Toolbox, a Matlab and Simulink Toolbox dedicated to fractional calculus, can be downloaded at http://cronetoolbox.ims-bordeaux.fr\n\n[[Category:Fractional calculus| ]]\n[[Category:Riemannian theory]]"
    },
    {
      "title": "Bessel potential",
      "url": "https://en.wikipedia.org/wiki/Bessel_potential",
      "text": "In [[mathematics]], the '''Bessel potential''' is a [[potential theory|potential]] (named after [[Friedrich Wilhelm Bessel]]) similar to the '''[[Riesz potential]]''' but with better decay properties at infinity.\n\nIf ''s'' is a complex number with positive real part then the Bessel potential of order ''s'' is the operator\n:<math>(I-\\Delta)^{-s/2}</math>\nwhere Δ is the [[Laplace operator]] and the [[fractional calculus|fractional power]] is defined using Fourier transforms.\n\n[[Yukawa potential]]s are particular cases of Bessel potentials for <math>s=2</math> in the 3-dimensional space.\n\n==Representation in Fourier space==\n\nThe Bessel potential acts by multiplication on the [[Fourier transform]]s: for each <math>\\xi \\in \\mathbb{R}^d</math>\n:<math> \n  \\mathcal{F}((I-\\Delta)^{-s/2} u) (\\xi)= \\frac{\\mathcal{F}u (\\xi)}{(1 + 4 \\pi^2 \\vert \\xi \\vert^2)^{s/2}}.\n</math>\n\n== Integral representations ==\n\nWhen <math>s > 0</math>, the Bessel potential on <math>\\mathbb{R}^d</math> can be represented by \n:<math>(I - \\Delta)^{-s/2} u = G_s \\ast u,</math> \nwhere the Bessel kernel <math>G_s</math> is defined for <math>x \\in \\mathbb{R}^d \\setminus \\{0\\} </math> by the integral formula <ref>{{cite book|last1=Stein|first1=Elias|title=Singular integrals and differentiability properties of functions|date=1970|publisher=Princeton University Press|isbn=0-691-08079-8|at=Chapter V eq. (26)}}</ref>\n:<math>\n  G_s (x) \n  = \\frac{1}{(4 \\pi)^{s/2}\\Gamma (s/2)} \n     \\int_0^\\infty \\frac{e^{-\\frac{\\pi \\vert x \\vert^2}{\\delta}-\\frac{\\delta}{4 \\pi}}}{\\delta^{1 + \\frac{d - s}{2}}}\\,\\mathrm{d}\\delta.\n</math>\nHere <math>\\Gamma</math> denotes the [[Gamma function]].\nThe Bessel kernel can also be represented for <math>x \\in \\mathbb{R}^d \\setminus \\{0\\} </math> by<ref>{{cite journal|last1=N. Aronszajn|last2=K. T. Smith|title=Theory of Bessel potentials I|journal=Ann. Inst. Fourier|date=1961|volume=11|at=385–475, (4,2)}}</ref>\n:<math>\nG_s (x) = \\frac{e^{-\\vert x \\vert}}{(2\\pi)^\\frac{d-1}{2} 2^\\frac{s}{2} \\Gamma (\\frac{s}{2}) \\Gamma (\\frac{d - s + 1}{2})}\n\\int_0^\\infty e^{-\\vert x \\vert t} \\Big(t + \\frac{t^2}{2}\\Big)^\\frac{d - s - 1}{2} \\,\\mathrm{d}t.\n</math>\n\n==Asymptotics==\n\nAt the origin, one has as <math>\\vert x\\vert \\to 0 </math>,<ref>{{cite journal|last1=N. Aronszajn|last2=K. T. Smith|title=Theory of Bessel potentials I|journal=Ann. Inst. Fourier|date=1961|volume=11|at=385–475, (4,3)}}</ref>\n:<math>\nG_s (x) = \\frac{\\Gamma (\\frac{d - s}{2})}{2^s \\pi^{s/2} \\vert x\\vert^{d - s}}(1 + o (1))  \\quad \\text{ if } 0 < s < d,\n</math>\n:<math>\nG_d (x) = \\frac{1}{2^{d - 1} \\pi^{d/2} }\\ln \\frac{1}{\\vert x \\vert}(1 + o (1)) ,\n</math>\n:<math>\nG_s (x) = \\frac{\\Gamma (\\frac{s - d}{2})}{2^s \\pi^{s/2} }(1 + o (1))  \\quad \\text{ if }s > d.\n</math>\nIn particular, when <math>0 < s < d</math> the Bessel potential behaves asymptotically as the [[Riesz potential]].\n\nAt infinity, one has, as <math>\\vert x\\vert \\to \\infty </math>, <ref>{{cite journal|last1=N. Aronszajn|last2=K. T. Smith|title=Theory of Bessel potentials I|journal=Ann. Inst. Fourier|date=1961|volume=11|pages=385–475}}</ref>\n:<math>\nG_s (x) = \\frac{e^{-\\vert x \\vert}}{2^\\frac{d + s - 1}{2} \\pi^\\frac{d - 1}{2} \\Gamma (\\frac{s}{2}) \\vert x \\vert^\\frac{d + 1 - s}{2}}(1 + o (1)).\n</math>\n\n==See also==\n* [[Riesz potential]]\n* [[Fractional integration]]\n* [[Sobolev space]]\n* [[Fractional Schrödinger equation]]\n*[[Yukawa potential]]\n\n==References==\n{{Reflist}}\n*{{eom|id=B/b110420|title=Bessel potential operator|first=R. |last=Duduchava}}\n*{{Citation | last1=Grafakos | first1=Loukas | title=Modern Fourier analysis | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=2nd | series=[[Graduate Texts in Mathematics]] | isbn=978-0-387-09433-5 | doi=10.1007/978-0-387-09434-2 | mr=2463316 | year=2009 | volume=250}}\n*{{eom|id=B/b120170|title=Bessel potential space|first=L.I. |last= Hedberg}}\n*{{eom|id=B/b015870|first=E.D.|last= Solomentsev}}\n* {{citation |first=Elias |last=Stein |authorlink=Elias Stein |title=Singular integrals and differentiability properties of functions |publisher=[[Princeton University Press]] |location=Princeton, NJ |year=1970 |isbn=0-691-08079-8}}\n\n[[Category:Fractional calculus]]\n[[Category:Partial differential equations]]\n[[Category:Potential theory]]\n[[Category:Singular integrals]]"
    },
    {
      "title": "Coopmans approximation",
      "url": "https://en.wikipedia.org/wiki/Coopmans_approximation",
      "text": "{{Unreferenced stub|auto=yes|date=December 2009}}\nThe '''Coopmans approximation'''<!--, named after [[?????? Coopmans]],---> is a method for approximating a [[fractional-order integrator]] in a continuous process with constant [[space complexity]]. The most correct and accurate methods for calculating the fractional integral require a record of all previous history, and therefore would require a linear space complexity solution O(''n''), where ''n'' is the number of samples measured for the complete history.\n\nThe [[fractor]] ( fractional capacitor ) is an analog component useful in [[control system]]s. In order to model the components behavior in a digital simulation, or replace the fractor in a digital controller, a linear solution is untenable. In order to reduce the space complexity however, it is necessary to lose information in some way. \n\nThe Coopmans approximation is a robust, simple method that uses a simple [[convolution]] to compute the fractional integral, then recycles old data back through the convolution. The convolution sets up a weighting table as described by the [[fractional calculus]], which varies based on the size of the table, the sampling rate of the system, and the order of the integral. Once computed the weighting table remains static.\n\nThe data table is initialized as all zeros, which represents a lack of activity for all previous time. New data is added to the data buffer in the fashion of a ring buffer, so that the newest point is written over the oldest data point.\nThe convolution is solved by multiplying corresponding elements from the weight and data tables, and summing the resulting products. As described, the loss of the old data by overwriting with new data will cause echoes in a continuous system as disturbances that were absorbed into the system are suddenly removed.\n\nThe solution to this is the crux of the Coopmans approximation, where the old data point, multiplied by its corresponding weight term, is added to the newest data point directly. This allows a smooth (though exponential, rather than power law) decay of the system history. This approximation has the desirable effect of removing the echo, while preserving the space complexity of the solution.\n\nThe negative effect of the approximation is that the phase character of the solution is lost as the system frequency approaches DC. However, all digital systems are guaranteed to suffer this flaw, as all digital systems have finite memory, and therefore will fail as the memory requirement approaches infinity.\n\n[[Category:Electronic design]]\n[[Category:Fractional calculus]]\n[[Category:Numerical analysis]]\n\n{{Electronics-stub}}"
    },
    {
      "title": "Differintegral",
      "url": "https://en.wikipedia.org/wiki/Differintegral",
      "text": "{{redirect-distinguish|Fractional integration|Autoregressive fractionally integrated moving average}}\n{{Calculus|expanded=Specialized calculi}}\n\nIn [[fractional calculus]], an area of [[applied mathematics]], the '''differintegral''' is a combined  [[Differential operator|differentiation]]/[[integral operator|integration]] operator. Applied to a [[function (mathematics)|function]] ƒ, the ''q''-differintegral of ''f'', here denoted by\n:<math>\\mathbb{D}^qf</math>\nis the fractional derivative (if ''q'' > 0) or fractional integral (if ''q'' < 0).  If ''q'' = 0, then the ''q''-th differintegral of a function is the function itself.  In the context of fractional integration and differentiation, there are several legitimate definitions of the differintegral.\n\n==Standard definitions==\n\nThe three most common forms are:\n\n*The [[Riemann–Liouville differintegral]]\n:This is the simplest and easiest to use, and consequently it is the most often used. It is a generalization of the [[Cauchy formula for repeated integration]] to arbitrary order. Here, <math>n = \\lceil q \\rceil</math>.\n: <math>\n\\begin{align}\n{}_a\\mathbb{D}^q_tf(t) & = \\frac{d^qf(t)}{d(t-a)^q} \\\\\n& =\\frac{1}{\\Gamma(n-q)} \\frac{d^n}{dt^n} \\int_{a}^t (t-\\tau)^{n-q-1}f(\\tau)d\\tau\n\\end{align}\n</math>\n\n\n\n\n*The [[Grunwald–Letnikov differintegral]]\n:The Grunwald–Letnikov differintegral is a direct generalization of the definition of a [[derivative]].  It is more difficult to use than the Riemann–Liouville differintegral, but can sometimes be used to solve problems that the Riemann–Liouville cannot.\n\n: <math>\n\\begin{align}\n{}_a\\mathbb{D}^q_tf(t) & = \\frac{d^qf(t)}{d(t-a)^q} \\\\\n& =\\lim_{N \\to \\infty}\\left[\\frac{t-a}{N}\\right]^{-q}\\sum_{j=0}^{N-1}(-1)^j{q \\choose j}f\\left(t-j\\left[\\frac{t-a}{N}\\right]\\right)\n\\end{align}\n</math>\n\n*The [[Weyl differintegral]]\n:This is formally similar to the Riemann–Liouville differintegral, but applies to [[periodic function]]s, with integral zero over a period.\n\n==Definitions via transforms==\n\nRecall the [[continuous Fourier transform]], here denoted <math> \\mathcal{F}</math> :\n\n:<math> F(\\omega) =  \\mathcal{F}\\{f(t)\\} = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty f(t) e^{- i\\omega t}\\,dt </math>\n\nUsing the continuous Fourier transform, in Fourier space, differentiation transforms into a multiplication:\n\n:<math>\\mathcal{F}\\left[\\frac{df(t)}{dt}\\right] = i \\omega \\mathcal{F}[f(t)]</math>\n\nSo,\n\n:<math>\\frac{d^nf(t)}{dt^n} = \\mathcal{F}^{-1}\\left\\{(i \\omega)^n\\mathcal{F}[f(t)]\\right\\}</math>\n\nwhich generalizes to\n\n:<math>\\mathbb{D}^qf(t)=\\mathcal{F}^{-1}\\left\\{(i \\omega)^q\\mathcal{F}[f(t)]\\right\\}.</math>\n\nUnder the [[bilateral Laplace transform]], here denoted by <math> \\mathcal{L}</math> and defined as <math> \\mathcal{L}[f(t)]=\\int_{-\\infty}^\\infty e^{-st} f(t)\\, dt</math>, differentiation transforms into a multiplication\n\n:<math>\\mathcal{L}\\left[\\frac{df(t)}{dt}\\right] = s\\mathcal{L}[f(t)].</math>\n\nGeneralizing to arbitrary order and solving for ''D''<sup>''q''</sup>''f''(''t''), one obtains\n:<math>\\mathbb{D}^qf(t)=\\mathcal{L}^{-1}\\left\\{s^q\\mathcal{L}[f(t)]\\right\\}.</math>\n\n==Basic formal properties==\n\n''Linearity rules''\n:<math>\\mathbb{D}^q(f+g)=\\mathbb{D}^q(f)+\\mathbb{D}^q(g)</math>\n:<math>\\mathbb{D}^q(af)=a\\mathbb{D}^q(f)</math>\n\n''Zero rule''\n:<math>\\mathbb{D}^0 f=f \\, </math>\n\n''Product rule''\n\n:<math>\\mathbb{D}^q_t(fg)=\\sum_{j=0}^{\\infty} {q \\choose j}\\mathbb{D}^j_t(f)\\mathbb{D}^{q-j}_t(g)</math>\n\nIn general, ''composition (or [[semigroup]]) rule'' is '''not satisfied'''<ref>See {{cite book |page=75 |chapter=2. Fractional Integrals and Fractional Derivatives §2.1 Property 2.4 |chapter-url=https://books.google.com/books?id=uxANOU0H8IUC&pg=PA75 |first=A. A. |last=Kilbas |first2=H. M. |last2=Srivastava |first3=J. J. |last3=Trujillo |title=Theory and Applications of Fractional Differential Equations |location= |publisher=Elsevier |year=2006 |isbn=9780444518323 }}</ref>:\n:<math>\\mathbb{D}^a\\mathbb{D}^{b}f \\neq \\mathbb{D}^{a+b}f</math>\n\n==A selection of basic formulæ==\n\n:<math>\\mathbb{D}^q(t^n)=\\frac{\\Gamma(n+1)}{\\Gamma(n+1-q)}t^{n-q}</math>\n:<math>\\mathbb{D}^q(\\sin(t))=\\sin \\left( t+\\frac{q\\pi}{2} \\right) </math>\n:<math>\\mathbb{D}^q(e^{at})=a^q e^{at}</math>\n\n==See also==\n* [[Fractional-order integrator]]\n\n==References==\n{{Reflist}}\n{{refbegin}}\n*{{cite book |first=Kenneth S. |last=Miller |editor-first=Bertram |editor-last=Ross |title=An Introduction to the Fractional Calculus and Fractional Differential Equations |publisher=Wiley |year=1993 |isbn=0-471-58884-9 }}\n*{{cite book |first=Keith B. |last=Oldham |first2=Jerome |last2=Spanier |title=The Fractional Calculus; Theory and Applications of Differentiation and Integration to Arbitrary Order |publisher=Academic Press |series=Mathematics in Science and Engineering |volume=V |year=1974 |isbn=0-12-525550-0 }}\n*{{cite book |first=Igor |last=Podlubny |title=Fractional Differential Equations. An Introduction to Fractional Derivatives, Fractional Differential Equations, Some Methods of Their Solution and Some of Their Applications |publisher=Academic Press  |series=Mathematics in Science and Engineering |volume=198 |year=1998 |isbn=0-12-558840-2 }}\n*{{cite book |editor-first=A. |editor-last=Carpinteri |editor2-first=F. |editor2-last=Mainardi |title=Fractals and Fractional Calculus in Continuum Mechanics |publisher=Springer-Verlag |year=1998 |isbn=3-211-82913-X }}\n*{{cite book |first=F. |last=Mainardi |title=Fractional Calculus and Waves in Linear Viscoelasticity: An Introduction to Mathematical Models |publisher=Imperial College Press |year=2010 |isbn=978-1-84816-329-4 |url=https://web.archive.org/web/20120519174508/http://www.worldscibooks.com/mathematics/p614.html}}\n*{{cite book |first=V.E. |last=Tarasov |title=Fractional Dynamics: Applications of Fractional Calculus to Dynamics of Particles, Fields and Media |publisher=Springer |year=2010 |isbn=978-3-642-14003-7 |url=https://www.springer.com/physics/complexity/book/978-3-642-14003-7|series=Nonlinear Physical Science }}\n*{{cite book |first=V.V. |last=Uchaikin |title=Fractional Derivatives for Physicists and Engineers |publisher=Springer |year=2012 |isbn=978-3-642-33910-3 |url=https://www.springer.com/physics/theoretical,+mathematical+%26+computational+physics/book/978-3-642-33910-3|series=Nonlinear Physical Science |bibcode=2013fdpe.book.....U }}\n*{{cite book |first=Bruce J. |last=West |first2=Mauro |last2=Bologna |first3=Paolo |last3=Grigolini |title=Physics of Fractal Operators |publisher=Springer Verlag |year=2003 |isbn=0-387-95554-2 |url=https://books.google.com/books?id=EgyTpQZOga0C&pg=PR7}}\n{{refend}}\n\n==External links==\n* [http://mathworld.wolfram.com/FractionalCalculus.html MathWorld – Fractional calculus]\n*[http://mathworld.wolfram.com/FractionalDerivative.html MathWorld – Fractional derivative]\n*Specialized journal: [http://www.diogenes.bg/fcaa/ Fractional Calculus and Applied Analysis (1998-2014)] and [http://www.degruyter.com/view/j/fca Fractional Calculus and Applied Analysis (from 2015)]\n*Specialized journal: [https://archive.today/20120712033445/http://fde.ele-math.com/ Fractional Differential Equations (FDE)]\n*Specialized journal: [http://naturalspublishing.com/show.asp?JorID=48&pgid=0 Progress in Fractional Differentiation and Applications]\n*Specialized journal: [http://www.nonlinearscience.com/journal_2218-3892.php Communications in Fractional Calculus] ({{issn|2218-3892}})\n* Specialized journal: [http://fcag-egypt.com/Journals/JFCA/ Journal of Fractional Calculus and Applications (JFCA)]\n*{{cite web |first=Carl F. |last=Lorenzo |first2=Tom T. |last2=Hartley |title=Initialized Fractional Calculus |date=2002 |work=Information Technology |publisher=Tech Briefs Media Group |url=https://www.techbriefs.com/component/content/article/tb/techbriefs/information-sciences/2264}}\n* https://web.archive.org/web/20040502170831/http://unr.edu/homepage/mcubed/FRG.html\n* [http://www.tuke.sk/podlubny/fc_resources.html Igor Podlubny's collection of related books, articles, links, software, etc. ]\n*{{cite journal |first=I. |last=Podlubny |title=Geometric and physical interpretation of fractional integration and fractional differentiation |journal=Fractional Calculus and Applied Analysis |volume=5 |issue=4 |pages=367–386 |year=2002 |doi= |url=http://www.tuke.sk/podlubny/pspdf/pifcaa_r.pdf |arxiv=math.CA/0110241|bibcode=2001math.....10241P }}\n*{{cite journal |first=P. |last=Zavada |title=Operator of fractional derivative in the complex plane |journal=Commun.Math.Phys. |volume=192 |issue= 2|pages=261–285 |year=1998 |doi=10.1007/s002200050299 |arxiv=funct-an/9608002|bibcode=1998CMaPh.192..261Z }}\n\n[[Category:Fractional calculus]]\n[[Category:Generalizations of the derivative]]\n[[Category:Linear operators in calculus]]"
    },
    {
      "title": "Erdelyi–Kober operator",
      "url": "https://en.wikipedia.org/wiki/Erdelyi%E2%80%93Kober_operator",
      "text": "{{Calculus|expanded=Fractional calculus}}\n\nIn mathematics, an '''Erdélyi–Kober operator''' is a [[fractional calculus|fractional integration operation]] introduced by {{harvs|txt|authorlink=Arthur Erdélyi|first=Arthur |last=Erdélyi|year=1940}} and {{harvs|txt|authorlink=Hermann Kober|first=Hermann |last=Kober|year=1940}}.\n\nThe Erdélyi–Kober fractional integral is given by \n:<math>\\frac{x^{-\\nu-\\alpha+1}}{\\Gamma(\\alpha)}\\int_0^x (t-x)^{\\alpha-1}t^{-\\alpha-\\nu}f(t) dt </math>\n\nwhich generalizes the [[Riemann fractional integral]] and the [[Weyl integral]].\n\n==Comparison==\nThere is a similar operator now known as the [[Katugampola_fractional_operators|Katugampola fractional operator]] which generalizes both the [[Fractional_calculus#Riemann_Liouville_fractional_integral|Riemann-Liouville]] and the [[Fractional_calculus#Hadamard_fractional_integral|Hadamard fractional integrals]] into a unique form.\n\n==References==\n\n*{{Citation | last1=Erdélyi | first1=A. | title=On fractional integration and its application to the theory of Hankel transforms | doi=10.1093/qmath/os-11.1.293  | mr=0003271 | year=1940 | journal=The Quarterly Journal of Mathematics. Oxford. Second Series | issn=0033-5606 | volume=11 | pages=293–303}}\n* {{citation | last= Erdélyi | first= Arthur | authorlink= Arthur Erdélyi | title= On some functional transformations | journal= Rendiconti del Seminario Matematico dell'Università e del Politecnico di Torino | volume= 10 | pages= 217–234 | year= 1950–51 | mr= 0047818 |id=}}\n*{{Citation | last1=Erdélyi | first1=A. | last2=Kober | first2=H. | title=Some remarks on Hankel transforms | doi=10.1093/qmath/os-11.1.212  | mr=0003270 | year=1940 | journal=The Quarterly Journal of Mathematics. Oxford. Second Series | issn=0033-5606 | volume=11 | pages=212–221}}\n* {{citation| last= Kober | first= Hermann | title= On fractional integrals and derivatives | journal= The Quarterly Journal of Mathematics (Oxford Series) | volume= 11 | issue= 1 | pages= 193–211 | year= 1940 | doi= 10.1093/qmath/os-11.1.193 }}\n*{{Citation | last1=Sneddon | first1=Ian Naismith | editor1-last=Ross | editor1-first=Bertram | title=Fractional calculus and its applications (Proc. Internat. Conf., Univ. New Haven, West Haven, Conn., 1974) | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Lecture Notes in Math. | isbn=978-3-540-07161-7 | doi=10.1007/BFb0067097 | mr=0487301 | year=1975 | volume=457 | chapter=The use in mathematical physics of Erdélyi-Kober operators and of some of their generalizations | pages=37–79}}\n\n{{DEFAULTSORT:Erdelyi-Kober operator}}\n[[Category:Fractional calculus]]"
    },
    {
      "title": "Fractional Laplacian",
      "url": "https://en.wikipedia.org/wiki/Fractional_Laplacian",
      "text": "In [[mathematics]], the '''fractional Laplacian''' is an operator which generalizes the notion of derivatives to fractional powers.\n\n==Definition==\n\nFor <math>0<s<1</math>, the fractional Laplacian of order s <math>(-\\Delta)^s</math> can be defined on functions <math>f:\\mathbb{R}^d\\rightarrow\\mathbb{R}</math> as a [[Fourier multiplier]] given by the formula\n\n:<math>\\mathcal{F}((-\\Delta)^sf))(\\xi) = |\\xi|^{2s}\\mathcal{F}(f)(\\xi)</math>\n\nwhere the [[Fourier transform]] <math>\\mathcal{F}(f)</math> of a function <math>f:\\mathbb{R}^d\\rightarrow\\mathbb{R}</math> is given by\n\n:<math>\\mathcal{F}(f)(\\xi) = \\int\\limits_{\\mathbb{R}^d}{f(x)e^{-ix\\cdot\\xi}\\,dx}.</math>\n\nMore concretely, the fractional Laplacian can be written as a [[singular integral operator]] defined by\n\n:<math> (-\\Delta)^sf(x) = c_{d,s}\\int\\limits_{\\mathbb{R}^d}{\\frac{f(x)-f(y)}{|x-y|^{d+2s}}\\,dy} </math>\n\nwhere <math>c_{d,s} = \\frac{4^s\\Gamma(d/2+s)}{\\pi^{d/2}|\\Gamma(-s)|} </math>. These two definitions, along with several other definitions,<ref>Kwasnicki, Mateusz. \"Ten equivalent definitions of the fractional Laplace operator\". https://arxiv.org/pdf/1507.07356.pdf\"</ref> are equivalent.\n\nSome authors prefer to adopt the convention of defining the fractional Laplacian of order s as <math>(-\\Delta)^{s/2}</math> (as defined above), where now <math>0<s<2</math>, so that the notion of order matches that of a [[pseudodifferential operator|(pseudo-)differential operator]].\n\n==See also==\n*[[Fractional calculus]]\n*[[Riemann-Liouville integral]]\n\n==References==\n{{reflist|60em}}\n\n==External links==\n* \"[https://www.ma.utexas.edu/mediawiki/index.php/Fractional_Laplacian Fractional Laplacian]\". Nonlocal Equations Wiki, Department of Mathematics, The University of Texas at Austin.\n\n[[Category:Fractional calculus]]"
    },
    {
      "title": "Fractional-order integrator",
      "url": "https://en.wikipedia.org/wiki/Fractional-order_integrator",
      "text": "{{further|Fractional calculus}}\n{{Unreferenced|date=June 2009}}\n{{calculus|expanded=Specialized calculi}}\nA '''fractional-order integrator''' or just simply '''fractional integrator''' is an [[integrator]] device that calculates the fractional-order integral or derivative (usually called a [[differintegral]]) of an input. Differentiation or integration is a real or complex parameter. The fractional integrator is useful in [[fractional-order control]] where the history of the system under control is important to the control system output.\n\n== Overview ==\nThe [[differintegral]] function,\n\n:<math>{}_a \\mathbb{D}^q_t \\left( f(x) \\right)</math>\n\nincludes the integer order differentiation and integration functions, and allows a continuous range of functions around them. The differintegral parameters are ''a'', ''t'', and ''q''. The parameters ''a'' and ''t'' describe the range over which to compute the result. The differintegral parameter ''q'' may be any real number or complex number. If ''q'' is greater than zero, the differintegral computes a derivative. If ''q'' is less than zero, the differintegral computes an integral.\nThe integer order integration can be computed as a [[Riemann–Liouville differintegral]], where the weight of each element in the sum is the constant unit value 1, which is equivalent to the [[Riemann sum]]. To compute an integer order derivative, the weights in the summation would be zero, with the exception of the most recent data points, where (in the case of the first unit derivative) the weight of the data point at ''t''&nbsp;−&nbsp;1 is −1 and the weight of the data point at ''t'' is&nbsp;1. The sum of the points in the input function using these weights results in the difference of the most recent data points.\nThese weights are computed using ratios of the [[Gamma function]] incorporating the number of data points in the range [''a'',''t''], and the parameter&nbsp;''q''.\n\n== Digital devices ==\nDigital devices have the advantage of being versatile, and are not susceptible to unexpected output variation due to heat or noise. The discrete nature of a computer however, does not allow for all of history to be computed. Some finite range [a,t] must exist. Therefore, the number of data points that can be stored in memory (''N''), determines the oldest data point in memory, so that the value a is never more than ''N'' samples old. The effect is that any history older than a is ''completely'' forgotten, and no longer influences the output.\n\nA solution to this problem is the [[Coopmans approximation]], which allows old data to be forgotten more gracefully (though still with exponential decay, rather than with the power law decay of a purely [[analog device]]).\n\n== Analog devices ==\nAnalog devices have the ability to retain history over longer intervals. This translates into the parameter a staying constant, while ''t'' increases. \n\nThere is no [[Round-off error|error due to round-off]], as in the case of digital devices, but there may be error in the device due to [[Leakage (electronics)|leakage]]s, and also unexpected variations in behavior caused by heat and noise.\n\nAn example fractional-order integrator is a modification of the standard [[integrator circuit]], where a [[capacitor]] is used as the [[feedback impedance]] on an [[opamp]]. By replacing the capacitor with an [[RC Ladder]] circuit, a half order integrator, that is, with\n\n:<math>q = -\\frac{1}{2},</math>\n\ncan be constructed.\n\n==See also==\n\n*[[Signal analysis]]\n*[[Fourier series]]\n\n[[Category:Cybernetics]]\n[[Category:Fractional calculus]]"
    }
  ]
}