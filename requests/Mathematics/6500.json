{
  "pages": [
    {
      "title": "Special ordered set",
      "url": "https://en.wikipedia.org/wiki/Special_ordered_set",
      "text": "In [[discrete optimization]], a '''special ordered set''' (SOS) is an ordered [[Set (mathematics)|set]] of variables, used as an additional way to specify integrality conditions in an optimization model. Special order sets are basically a device or tool used in [[branch and bound]] methods for branching on sets of variables, rather than individual variables, as in ordinary mixed [[integer programming]]. Knowing that a variable is part of a '''set''' and that it is '''ordered''' gives the branch and bound algorithm a more intelligent way to face the optimization problem, helping to speed up the search procedure. The members of a special ordered set individually may be continuous or discrete variables in any combination. However, even when all the members are themselves continuous, a model containing one or more special ordered sets becomes a discrete optimization problem requiring a [[Linear programming#Integer unknowns|mixed integer]] optimizer for its solution.\n\nThe ‘only’ benefit of using Special Ordered Sets compared with using only constraints, is that the search procedure will generally be noticeably faster.<ref>Christelle Gueret, Christian Prins, Marc Sevaux, \"Applications of optimization with Xpress-MP\", Editions Eyrolles, Paris, France (2000), {{ISBN|0-9543503-0-8}}, pag 39-42  [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.69.9634&rep=rep1&type=pdf Link to PDF]</ref>\n\nAs per J.A. Tomlin,<ref>J.A. Tomlin, \"Special Ordered Sets and an Application to Gas Supply Operations Planning\", Ketron Management Science, Inc., Mountain View, CA 94040-1266, USA</ref> Special Order Sets provide a powerful means of modeling nonconvex functions and discrete requirements, though there has been a tendency to think of them only in terms of multiple-choice zero-one programming.\n\n== Context of Applications ==\n* Multiple-choice programming\n* Global Optimization with continuous separable functions.\n\n== History ==\nThe origin of the concept was in the paper of Beale titled \"Two transportation problems\" (1963)<ref>E.M.L. Beale, \"Two transportation problems\", in: G.Kreweras and G.Morlat, eds., \"Proceedings of the Third International Conference on Operational Research\" (Dunod, Paris and English Universities Press, London, 1963) 780-788</ref> where he presented a bid evaluation model, however, the term was first explicitly introduced by Beale and Tomlin (1970).<ref>E.M.L. Beale and J.A. Tomlin, \"Special facilities in a general mathematical programming system for non-convex problems using ordered sets of variables\", in: J.Lawrence, ed., \"Proceedings of the Fifth International Conference on Operational Research\" (Tavistock Publications, London, 1970) 447-454</ref> The special order set were first implemented in Scicon's UMPIRE mathematical programming system.<ref>J.J.H. Forrest, J.P.H Hirst and J.A. Tomlin, \"Practical solution of large mixed integer programming problems with UMPIRE\", Management Sci. 20 (1974) 736-773</ref>\n\nSpecial Order sets were an important and recurring theme in Martin Beale's work,<ref>M.J.D. Powell, \"A biographical memoir of Evelyn Martin Lansdowne Beale, FRS\", Biographical Memoirs of Fellows of the Royal Society 33 (1987)</ref> and their value came to be recognized to the point where nearly all production mathematical programming systems (MPS's) implement some version, or subset, of SOS.\n\n== Types of SOS ==\nThere are two sorts of Special Ordered Sets:\n\n# '''Special Ordered Sets of type 1 (SOS1 or S1):''' are a set of variables, at most '''one''' of which can take a non-zero value, all others being at 0. They most frequently apply where a set of variables are actually 0-1 variables: in other words, we have to choose at most one from a set of possibilities. These might arise for instance where we are deciding on what size of factory to build, when we have a set of options, perhaps small, medium, large or no factory at all, and if we choose to build a factory, we have to choose one and only one size.\n# '''Special Ordered Sets of type 2 (SOS2 or S2):''' an ordered set of non-negative variables, of which at most '''two''' can be non-zero, and if two are non-zero these must be consecutive in their ordering. Special Ordered Sets of type 2 are typically used to model non-linear functions of a variable in a linear model. They are the natural extension of the concepts of [[Separable Programming]], but when embedded in a Branch and Bound code enable truly global optima to be found, and not just local optima.\n\n==Further Example==\n* [https://www.tu-chemnitz.de/mathematik/discrete/manuals/cplex/doc/pdf/cplex81userman.pdf Example sizing a warehouse (Example for SOS Type 1 from Manual of ILOG CPLEX 11.2)]\n\n==Notes==\n{{Reflist}}\n\n== References ==\n* The notion of Special Ordered Sets was introduced by E. M. L. Beale and J. A. Tomlin. Special Facilities in a General Mathematical Programming System for Nonconvex Problems Using Ordered Sets of Variables. In J. Lawrence, editor, Operational Research 69, pages 447–454. Tavistock Publishing, London, 1970.\n* E. M. L. Beale and J. J. H. Forrest. Global Optimization Using Special Ordered Sets. Mathematical Programming, 10(1):52–69, 1976.\n* [http://www.thetomlins.org/sosgsop.pdf \"Special Ordered Sets and an Application to Gas Supply Operations Planning\", J.A. Tomlin, Ketron Management Science, Inc., Mountain View, CA 94040-1266, USA]\n* [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.69.9634&rep=rep1&type=pdf Christelle Gueret, Christian Prins, Marc Sevaus, \"Applications of optimization with Xpress-MP\"], Editions Eyrolles, Paris, France (2000), {{ISBN|0-9543503-0-8}}, pag 39-42\n\n{{Optimization algorithms|combinatorial|state=expanded}}\n\n[[Category:Optimization of ordered sets]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Spiral optimization algorithm",
      "url": "https://en.wikipedia.org/wiki/Spiral_optimization_algorithm",
      "text": "{{Underlinked|date=November 2018}}\n\n[[File:Spiral image 17.jpg|thumb|The spiral shares the global (blue) and intensive (red) behavior.]]\nThe '''spiral optimization (SPO) algorithm''' is an uncomplicated search concept inspired by spiral phenomena in nature. \nThe first SPO algorithm was proposed for two-dimensional unconstrained optimization \n<ref name=tamura11a />\nbased on two-dimensional spiral models. This was extended to n-dimensional problems by generalizing the two-dimensional spiral model to an n-dimensional spiral model.<ref name=tamura11b />\nThere are effective settings for the SPO algorithm: the periodic descent direction setting \n<ref name=tamura16a />\nand the convergence setting \n.<ref name=tamura17a />\n\n== Metaphor ==\nThe motivation for focusing on [[spiral]] phenomena was due to the insight that the dynamics that generate logarithmic spirals share the diversification and intensification behavior. The diversification behavior can work for a global search (exploration) and the intensification behavior enables an intensive search around a current found good solution (exploitation).\n\n== Algorithm ==\n[[File:spo_movie4.gif|thumb|Spiral Optimization (SPO) algorithm]]\nThe SPO algorithm is a multipoint search algorithm that has no objective function gradient, which uses multiple spiral models that can be described as deterministic dynamical systems. As search points follow logarithmic \nspiral trajectories towards the common center, defined as the current best point, better solutions can be found and the common center can be updated.\nThe general SPO algorithm for a minimization problem under the maximum iteration <math>k_{\\max}</math> (termination criterion) is as follows:\n\n 0) Set the number of search points <math>m\\geq 2</math> and the maximum iteration number <math>k_{\\max}</math>.\n 1) Place the initial search points <math>x_i (0) \\in \\mathbb{R}^n~(i=1, \\ldots, m)</math> and determine the center <math> x^{\\star}(0)= x_{i_\\text{b} }(0) </math>, <math>\\displaystyle i_\\text{b} =\\mathop{\\text{argmin}}_{i=1,\\ldots,m} \\{ f(x_{i}(0)) \\} </math>,and then set <math> k = 0 </math>.\n 2) Decide the step rate <math>r(k)</math> by a rule.\n 3) Update the search points: <math> x_i(k+1) = x^{\\star}(k) + r(k) R(\\theta) (x_i(k) - x^{\\star}(k))\\quad(i=1, \\ldots, m).</math>\n 4) Update the center: <math>\nx^{\\star}(k+1) =\n\\begin{cases}\nx_{i_\\text{b}}(k+1) & \\big( \\text{if } f(x_{i_\\text{b}}(k+1)) < f(x^{\\star}(k)) \\big),\\\\\nx^{\\star}(k) & \\big(\\text{otherwise} \\big),\n\\end{cases}\n</math> where <math>\\displaystyle i_\\text{b} = \\mathop{\\text{argmin}}_{i=1,\\ldots,m} \\{ f(x_i (k+1)) \\} </math>.\n 5) Set <math>k := k+1</math>. If <math>k=k_{\\max}</math> is satisfied then terminate and output <math>x^{\\star}(k)</math>. Otherwise, return to Step 2).\n\n== Setting ==\nThe search performance depends on setting the composite rotation matrix <math>R(\\theta)</math>, the step rate <math>r(k)</math>, and the initial points <math>x_i(0)~(i=1,\\ldots,m)</math>. \nThe following settings are new and effective.\n\n=== Setting 1 (Periodic Descent Direction Setting)<ref name=tamura16a /> ===\n\nThis setting is an effective setting for high dimensional problems under the maximum iteration <math>k_{\\max}</math>. The conditions on <math>R(\\theta)</math> and <math>x_i(0)~(i=1,\\ldots,m)</math> together ensure that the spiral models generate descent directions periodically. The condition of <math>r(k)</math> works to utilize the periodic descent directions under the search termination <math>k_{\\max}</math>.\n\n* Set <math>R(\\theta)</math> as follows:<math>R(\\theta) = \\begin{bmatrix}\n0_{n-1}^\\top &-1\\\\\nI_{n-1}& 0_{n-1}\\\\\n\\end{bmatrix}\n</math> where <math>I_{n-1}</math> is the <math>(n-1)\\times (n-1)</math> identity matrix and <math>0_{n-1}</math> is the <math>(n-1)\\times 1</math> zero vector.\n* Place the initial points <math>x_i(0) \\in \\mathbb{R}^n</math> <math>(i = 1,\\ldots, m)</math> at random to satisfy the following condition:\n<math>\n\\min_{i=1,\\ldots,m} \\{ \\max_{j=1,\\ldots,m} \\bigl \\{ \\text{rank} \\bigl[ d_{j,i}(0)~R(\\theta)d_{j,i}(0)~~\n\\cdots~~R(\\theta)^{2n-1}d_{j,i}(0) \\bigr]\\bigr\\} \\bigr\\} = n \n</math>\nwhere <math>d_{j,i}(0) = x_{j}(0) - x_i(0)</math>. Note that this condition is almost all satisfied by a random placing and thus no check is actually fine.\n* Set <math>r(k)</math> at Step 2) as follows:<math>r(k) = r = \\sqrt[ k_{\\max} ]{\\delta }~~~~\\text{(constant value)}</math> where a sufficiently small <math>\\delta > 0</math> such as <math>\\delta = 1/k_{\\max}</math> or <math>\\delta = 10^{-3}</math>.\n\n\n=== Setting 2 (Convergence Setting)<ref name=tamura17a /> ===\n\nThis setting ensures that the SPO algorithm converges to a stationary point under the maximum iteration <math>k_{\\max} = \\infty</math>. The settings of <math>R(\\theta)</math> and the initial points <math>x_i(0)~(i=1,\\ldots,m)</math> are the same with the above Setting 1. The setting of <math>r(k)</math> is as follows.\n* Set <math>r(k)</math> at Step 2) as follows:<math>r(k) =\n\\begin{cases}\n1 & (k^\\star \\leqq k \\leqq  k^\\star + 2n-1), \\\\\nh & (k \\geqq k^\\star + 2n),\n\\end{cases}\n</math> where <math>k^\\star</math> is an iteration when the center is newly updated at Step 4) and <math>h = \\sqrt[ 2n ]{\\delta }, \\delta \\in (0,1)</math> such as <math>\\delta = 0.5</math>. Thus we have to add the following rules about <math>k^\\star</math> to the Algorithm:  \n:•(Step 1) <math>k^\\star = 0</math>.\n:•(Step 4) If <math>x^{\\star}(k+1)\\neq x^{\\star}(k)</math> then <math>k^\\star = k+1</math>.\n\n== Future works ==\n* The algorithms with the above settings are deterministic. Thus, incorporating some random operations would make this algorithm powerful for the global optimization.\n* To find an appropriate balance between diversification and intensification spirals depending on the target problem class (including <math>k_{\\max}</math>) is important to enhance the performance.\n\n== Extended works ==\nMany extended studies have been conducted on the SPO due to its simple structure and concept; these studies have helped improve its global search performance \nand proposed novel\napplications \n<ref name=nasir15a />\n<ref name=nasir16a />\n<ref name=ouadi13a />\n<ref name=benasla14a />\n<ref name=sidarto15a />\n.\n\n== References ==\n{{Reflist|refs=\n\n<ref name=tamura11a>\n{{cite journal\n| last1 = Tamura | first1 = K. \n| last2 = Yasuda | first2 = K.\n| title = Primary Study of Spiral Dynamics Inspired Optimization \n| journal = IEEJ Transactions on Electrical and Electronic Engineering \n| year=2011\n| volume=6\n| issue=S1\n| pages=98–100\n| doi=\n| url=\n}}\n</ref>\n\n<ref name=tamura11b>\n{{cite journal \n| last1 = Tamura | first1 = K. \n| last2 = Yasuda | first2 = K.\n| title = Spiral Dynamics Inspired Optimization \n| journal = Journal of Advanced Computational Intelligence and Intelligent Informatics\n| year = 2011\n| volume = 132 \n| issue = 5\n| pages = 1116–1121\n| doi=\n| url=\n}}\n</ref>\n\n<ref name=tamura16a>\n{{cite journal \n| last1 = Tamura | first1 = K. \n| last2 = Yasuda | first2 = K.\n| title = Spiral Optimization Algorithm Using Periodic Descent Directions \n| journal = SICE Journal of Control, Measurement, and System Integration \n| year = 2016 \n| volume = 6 \n| issue = 3\n| pages = 133–143 \n| doi=10.9746/jcmsi.9.134\n| url=\n}}\n</ref>\n\n<ref name=tamura17a>\n{{cite journal \n| last1 = Tamura | first1 = K. \n| last2 = Yasuda | first2 = K.\n| title = The Spiral Optimization Algorithm: Convergence Conditions and Settings \n| journal = IEEE Transactions on Systems, Man, and Cybernetics: Systems\n| year = 2017 \n| volume = PP \n| issue = 99\n| pages = 1–16 \n| doi = 10.1109/TSMC.2017.2695577\n| url = \n}}\n</ref>\n\n<ref name=nasir15a>\n{{cite journal \n| last1 = Nasir \n| first1 = A. N. K.  \n| last2 = Tokhi | first2 = M. O.\n| title = An improved spiral dynamic optimization algorithm with engineering application \n| journal = IEEE Trans. Syst.,Man, Cybern., Syst. \n| year = 2015 \n| volume = 45 \n| issue = 6\n| pages = 943–954\n| doi=\n| url=\n}}\n</ref>\n\n<ref name=nasir16a>\n{{cite journal \n| last1 = Nasir | first1 = A. N. K. \n| last2 = Ismail | first2 = R.M.T.R. \n| last3 = Tokhi | first3 = M. O.\n| title = Adaptive spiral dynamics metaheuristic algorithm for global optimisation with application to modelling of a flexible system| journal = Appl. Math. Modell. \n| year = 2016 \n| volume = 40 \n| issue = 9-10\n| pages = 5442–5461\n| doi=\n| url=\n}}\n</ref>\n\n<ref name=ouadi13a>\n{{cite journal \n| last1 = Ouadi | first1 = A. \n| last2 = Bentarzi | first2 = H. \n| last3 = Recioui | first3 = A.\n| title = multiobjective design of digital filters using spiral optimization technique\n| journal = SpringerPlus \n| year = 2013 \n| volume = 2 \n| issue = 461\n| pages = 697–707\n| doi=\n| url=\n}}\n</ref>\n\n<ref name=benasla14a>\n{{cite journal \n| last1 = Benasla | first1 = L. \n| last2 = Belmadani | first2 = A. \n| last3 = Rahli | first3 = M.\n| title = Spiral optimization algorithm for solving combined economic and Emission Dispatch\n| journal = Int. J. Elect. Power & Energy Syst. \n| year = 2014 \n| volume = 62\n| pages = 163–174\n| doi=\n| url=\n}}\n</ref>\n\n<ref name=sidarto15a>\n{{cite journal \n| last1 = Sidarto | first1 = K. A. \n| last2 = Kania | first2 = A. \n| title = Finding all solutions of systems of nonlinear equations using spiral dynamics inspired optimization with clustering\n| journal = JACIII \n| volume = 19\n| issue = 5\n| pages = 697–707\n| doi=\n| url=\n}}\n</ref>\n\n}}\n\n[[Category:Nature-inspired metaheuristics| ]]\n[[Category:Collective intelligence]]\n[[Category:Multi-agent systems]]\n[[Category:Evolutionary algorithms]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Stochastic dynamic programming",
      "url": "https://en.wikipedia.org/wiki/Stochastic_dynamic_programming",
      "text": "Originally introduced by [[Richard E. Bellman]] in {{harv|Bellman|1957}}, '''stochastic dynamic programming''' is a technique for modelling and solving problems of [[Decision theory#Choice under uncertainty|decision making under uncertainty]]. Closely related to [[stochastic programming]] and [[dynamic programming]], stochastic dynamic programming represents the problem under scrutiny in the form of a [[Bellman equation]]. The aim is to compute a [[Optimal decision#Under uncertainty in outcome|policy]] prescribing how to act optimally in the face of uncertainty.\n\n== A motivating example: Gambling game ==\n\nA gambler has $2, she is allowed to play a game of chance 4 times and her goal is to maximize her probability of ending up with a least $6. If the gambler bets $<math>b</math> on a play of the game, then with probability 0.4 she wins the game, recoup the initial bet, and she increases her capital position by $<math>b</math>; with probability 0.6, she loses the bet amount $<math>b</math>; all plays are [[Pairwise independence|pairwise independent]]. On any play of the game, the gambler may not bet more money than she has available at the beginning of that play.<ref>This problem is adapted from W. L. Winston, Operations Research: Applications and Algorithms (7th Edition), Duxbury Press, 2003, chap. 19, example 3.</ref>\n\nStochastic dynamic programming can be employed to model this problem and determine a betting strategy that, for instance, maximizes the gambler's probability of attaining a wealth of at least $6 by the end of the betting horizon.\n\nNote that if there is no limit to the number of games that can be played, the problem becomes a variant of the well known [[St. Petersburg paradox]].\n\n[[File:Gambling game optimal policy.png|thumb|center|750px|alt=Optimal betting strategy.|An optimal betting strategy that maximizes the gambler's probability of attaining a wealth of at least $6 by the end of the betting horizon; <math>b_t(\\$x)</math> represents the bet amount for game <math>t</math> when the gambler has $<math>x</math> at the beginning of that play. If the decision maker follows this policy, with probability 0.1984 she will attain a wealth of at least $6.]]\n\n== Formal background ==\nConsider a discrete system defined on <math>n</math> stages in which each stage <math>t=1,\\ldots,n</math> is characterized by\n*an '''initial state''' <math>s_t\\in S_t</math>, where <math>S_t</math> is the set of feasible states at the beginning of stage <math>t</math>;\n*a '''decision variable''' <math>x_t\\in X_t</math>, where <math>X_t</math> is the set of feasible actions at stage <math>t</math> – note that <math>X_t</math> may be a function of the initial state <math>s_t</math>;\n*an '''immediate cost/reward function''' <math>p_t(s_t,x_t)</math>, representing the cost/reward at stage <math>t</math> if <math>s_t</math> is the initial state and <math>x_t</math> the action selected;\n*a '''state transition function''' <math>g_t(s_t,x_t)</math> that leads the system towards state <math>s_{t+1}=g_t(s_t,x_t)</math>.\n\nLet <math>f_t(s_t)</math> represent the optimal cost/reward obtained by following an ''optimal policy'' over stages <math>t,t+1,\\ldots,n</math>. Without loss of generality in what follow we will consider a reward maximisation setting. In deterministic [[dynamic programming]] one usually deals with [[functional equation]]s taking the following structure\n: <math>\nf_t(s_t)=\\max_{x_t\\in X_t}\\{p_t(s_t,x_t)+f_{t+1}(s_{t+1})\\}\n</math>\nwhere <math>s_{t+1}=g_t(s_t,x_t)</math> and the boundary condition of the system is \n: <math>\nf_n(s_n)=\\max_{x_n\\in X_n}\\{p_n(s_n,x_n)\\}.\n</math>\n \nThe aim is to determine the set of optimal actions that maximise <math>f_1(s_1)</math>. Given the current state <math>s_t</math> and the current action <math>x_t</math>, we ''know with certainty'' the reward secured during the current stage and – thanks to the state transition function <math>g_t</math> – the future state towards which the system transitions.\n\nIn practice, however, even if we know the state of the system at the beginning of the current stage as well as the decision taken, the state of the system at the beginning of the next stage and the current period reward are often [[random variable]]s that can be observed only at the end of the current stage.\n\n'''Stochastic dynamic programming''' deals with problems in which the current period reward and/or the next period state are random, i.e. with multi-stage stochastic systems. The decision maker's goal is to maximise expected (discounted) reward over a given planning horizon.\n\nIn their most general form, stochastic dynamic programs deal with functional equations taking the following structure\n: <math>\nf_t(s_t)=\\max_{x_t\\in X_t(s_t)} \\left\\{(\\text{expected reward during stage } t\\mid s_t,x_t) + \\alpha\\sum_{s_{t+1}} \\Pr(s_{t+1}\\mid s_t,x_t)f_{t+1}(s_{t+1})\\right\\}\n</math>\nwhere\n*<math>f_t(s_t)</math> is the maximum expected reward that can be attained during stages <math>t,t+1,\\ldots,n</math>, given state <math>s_t</math> at the beginning of stage <math>t</math>;\n*<math>x_t</math> belongs to the set <math>X_t(s_t)</math> of feasible actions at stage <math>t</math> given initial state <math>s_t</math>;\n*<math>\\alpha</math> is the [[Discounting#Discount factor|discount factor]];\n*<math>\\Pr(s_{t+1}\\mid s_t,x_t)</math> is the conditional probability that the state at the beginning of stage <math>t</math> is <math>s_{t+1}</math> given current state <math>s_t</math> and selected action <math>x_t</math>.\n\n[[Markov decision process]] represent a special class of stochastic dynamic programs in which the underlying [[stochastic process]] is a [[stationary process]] that features the [[Markov property]].\n\n=== Gambling game as a stochastic dynamic program ===\n\nGambling game can be formulated as a Stochastic Dynamic Program as follows: there are <math>n=4</math> games (i.e. '''stages''') in the planning horizon\n*the '''state''' <math>s</math> in period <math>t</math> represents the initial wealth at the beginning of period <math>t</math>;\n*the '''action''' given state <math>s</math> in period <math>t</math> is the bet amount <math>b</math>;\n*the '''transition probability''' <math>p^a_{i,j}</math> from state <math>i</math> to state <math>j</math> when action <math>a</math> is taken in state <math>i</math> is easily derived from the probability of winning (0.4) or losing (0.6) a game.\n\nLet <math>f_t(s)</math> be the probability that, by the end of game 4, the gambler has at least $6, given that she has $<math>s</math> at the beginning of game <math>t</math>. \n*the '''immediate profit''' incurred if action <math>b</math> is taken in state <math>s</math> is given by the expected value <math>p_t(s,b)=0.4 f_{t+1}(s+b)+0.6 f_{t+1}(s-b)</math>.\n\nTo derive the '''functional equation''', define <math>b_t(s)</math> as a bet that attains <math>f_t(s)</math>, then at the beginning of game <math>t=4</math>\n*if <math>s<3</math> it is impossible to attain the goal, i.e. <math>f_4(s)=0</math> for <math>s<3</math>;\n*if <math>s\\geq 6</math> the goal is attained, i.e. <math>f_4(s)=1</math> for <math>s\\geq 6</math>;\n*if <math>3\\leq s\\leq 5</math> the gambler should bet enough to attain the goal, i.e. <math>f_4(s)=0.4</math> for <math>3\\leq s\\leq 5</math>.\n\nFor <math>t<4</math> the functional equation is <math>f_t(s)=\\max_{b_t(s)}\\{ 0.4 f_{t+1}(s+b)+0.6 f_{t+1}(s-b)\\}</math>, where <math>b_t(s)</math> ranges in <math>0,...,s</math>; the aim is to find <math>f_1(2)</math>.\n\nGiven the functional equation, an optimal betting policy can be obtained via forward recursion or backward recursion algorithms, as outlined below.\n\n== Solution methods ==\n\nStochastic dynamic programs can be solved to optimality by using [[Stochastic dynamic programming#Backward recursion|backward recursion]] or [[Stochastic dynamic programming#Forward recursion|forward recursion]] algorithms. [[Memoization]] is typically employed to enhance performance. However, like deterministic dynamic programming also its stochastic variant suffers from the [[curse of dimensionality]]. For this reason [[Stochastic dynamic programming#Approximate dynamic programming|approximate solution methods]] are typically employed in practical applications.\n\n=== Backward recursion ===\n\nGiven a bounded state space, ''backward recursion'' {{harv|Bertsekas|2000}} begins by tabulating <math>f_n(k)</math> for every possible state <math>k</math> belonging to the final stage <math>n</math>. Once these values are tabulated, together with the associated optimal state-dependent actions <math>x_n(k)</math>, it is possible to move to stage <math>n-1</math> and tabulate <math>f_{n-1}(k)</math> for all possible states belonging to the stage <math>n-1</math>. The process continues by considering in a ''backward'' fashion all remaining stages up to the first one. Once this tabulation process is complete, <math>f_1(s)</math> – the value of an optimal policy given initial state <math>s</math> – as well as the associated optimal action <math>x_1(s)</math> can be easily retrieved from the table. Since the computation proceeds in a backward fashion, it is clear that backward recursion may lead to computation of a large number of states that are not necessary for the computation of <math>f_1(s)</math>.\n\n==== Example: Gambling game ====\n\n{{Expand section|date=January 2017}}\n\n=== Forward recursion ===\n\nGiven the initial state <math>s</math> of the system at the beginning of period 1, ''forward recursion'' {{harv|Bertsekas|2000}} computes <math>f_1(s)</math> by progressively expanding the functional equation (''forward pass''). This involves recursive calls for all <math>f_{t+1}(\\cdot), f_{t+2}(\\cdot), \\ldots</math> that are necessary for computing a given <math>f_t(\\cdot)</math>. The value of an optimal policy and its structure are then retrieved via a (''backward pass'') in which these suspended recursive calls are resolved. A key difference from backward recursion is the fact that <math>f_t</math> is computed only for states that are relevant for the computation of <math>f_1(s)</math>.  [[Memoization]] is employed to avoid recomputation of states that have been already considered.\n\n==== Example: Gambling game ====\n\nWe shall illustrate forward recursion in the context of the Gambling game instance previously discussed. We begin the ''forward pass'' by considering\n<math>\nf_1(2)=\n\\min\\left\\{\n\\begin{array}{rr}\nb&\\text{success probability in periods 1,2,3,4}\\\\\n\\hline\n0&0.4f_{2}(2+0)+0.6f_{2}(2-0)\\\\\n1&0.4f_{2}(2+1)+0.6f_{2}(2-1)\\\\\n2&0.4f_{2}(2+2)+0.6f_{2}(2-2)\\\\\n\\end{array}\n\\right.\n</math>\n\nAt this point we have not computed yet <math>f_{2}(4),f_{2}(3), f_{2}(2), f_{2}(1), f_{2}(0)</math>, which are needed to compute <math>f_1(2)</math>; we proceed and compute these items. Note that <math>f_{2}(2+0)= f_{2}(2-0)=f_{2}(2)</math>, therefore one can leverage [[memoization]] and perform the necessary computations only once.\n\n;Computation of <math>f_{2}(4),f_{2}(3), f_{2}(2), f_{2}(1), f_{2}(0)</math>\n<math>\nf_2(0)=\n\\min\\left\\{\n\\begin{array}{rr}\nb&\\text{success probability in periods 2,3,4}\\\\\n\\hline\n0&0.4f_{3}(0+0)+0.6f_{3}(0-0)\\\\\n\\end{array}\n\\right.\n</math>\n\n<math>\nf_2(1)=\n\\min\\left\\{\n\\begin{array}{rr}\nb&\\text{success probability in periods 2,3,4}\\\\\n\\hline\n0&0.4f_{3}(1+0)+0.6f_{3}(1-0)\\\\\n1&0.4f_{3}(1+1)+0.6f_{3}(1-1)\\\\\n\\end{array}\n\\right.\n</math>\n\n<math>\nf_2(2)=\n\\min\\left\\{\n\\begin{array}{rr}\nb&\\text{success probability in periods 2,3,4}\\\\\n\\hline\n0&0.4f_{3}(2+0)+0.6f_{3}(2-0)\\\\\n1&0.4f_{3}(2+1)+0.6f_{3}(2-1)\\\\\n2&0.4f_{3}(2+2)+0.6f_{3}(2-2)\\\\\n\\end{array}\n\\right.\n</math>\n\n<math>\nf_2(3)=\n\\min\\left\\{\n\\begin{array}{rr}\nb&\\text{success probability in periods 2,3,4}\\\\\n\\hline\n0&0.4f_{3}(3+0)+0.6f_{3}(3-0)\\\\\n1&0.4f_{3}(3+1)+0.6f_{3}(3-1)\\\\\n2&0.4f_{3}(3+2)+0.6f_{3}(3-2)\\\\\n3&0.4f_{3}(3+3)+0.6f_{3}(3-3)\\\\\n\\end{array}\n\\right.\n</math>\n\n<math>\nf_2(4)=\n\\min\\left\\{\n\\begin{array}{rr}\nb&\\text{success probability in periods 2,3,4}\\\\\n\\hline\n0&0.4f_{3}(4+0)+0.6f_{3}(4-0)\\\\\n1&0.4f_{3}(4+1)+0.6f_{3}(4-1)\\\\\n2&0.4f_{3}(4+2)+0.6f_{3}(4-2)\n\\end{array}\n\\right.\n</math>\n\nWe have now computed <math>f_2(k)</math> for all <math>k</math> that are needed to compute <math>f_1(2)</math>. However, this has led to additional suspended recursions involving <math>f_{3}(4), f_{3}(3), f_{3}(2), f_{3}(1),  f_{3}(0)</math>. We proceed and compute these values.\n\n;Computation of <math>f_{3}(4), f_{3}(3), f_{3}(2), f_{3}(1),  f_{3}(0)</math>\n<math>\nf_3(0)=\n\\min\\left\\{\n\\begin{array}{rr}\nb&\\text{success probability in periods 3,4}\\\\\n\\hline\n0&0.4f_{4}(0+0)+0.6f_{4}(0-0)\\\\\n\\end{array}\n\\right.\n</math>\n\n<math>\nf_3(1)=\n\\min\\left\\{\n\\begin{array}{rr}\nb&\\text{success probability in periods 3,4}\\\\\n\\hline\n0&0.4f_{4}(1+0)+0.6f_{4}(1-0)\\\\\n1&0.4f_{4}(1+1)+0.6f_{4}(1-1)\\\\\n\\end{array}\n\\right.\n</math>\n\n<math>\nf_3(2)=\n\\min\\left\\{\n\\begin{array}{rr}\nb&\\text{success probability in periods 3,4}\\\\\n\\hline\n0&0.4f_{4}(2+0)+0.6f_{4}(2-0)\\\\\n1&0.4f_{4}(2+1)+0.6f_{4}(2-1)\\\\\n2&0.4f_{4}(2+2)+0.6f_{4}(2-2)\\\\\n\\end{array}\n\\right.\n</math>\n\n<math>\nf_3(3)=\n\\min\\left\\{\n\\begin{array}{rr}\nb&\\text{success probability in periods 3,4}\\\\\n\\hline\n0&0.4f_{4}(3+0)+0.6f_{4}(3-0)\\\\\n1&0.4f_{4}(3+1)+0.6f_{4}(3-1)\\\\\n2&0.4f_{4}(3+2)+0.6f_{4}(3-2)\\\\\n3&0.4f_{4}(3+3)+0.6f_{4}(3-3)\\\\\n\\end{array}\n\\right.\n</math>\n\n<math>\nf_3(4)=\n\\min\\left\\{\n\\begin{array}{rr}\nb&\\text{success probability in periods 3,4}\\\\\n\\hline\n0&0.4f_{4}(4+0)+0.6f_{4}(4-0)\\\\\n1&0.4f_{4}(4+1)+0.6f_{4}(4-1)\\\\\n2&0.4f_{4}(4+2)+0.6f_{4}(4-2)\n\\end{array}\n\\right.\n</math>\n\n<math>\nf_3(5)=\n\\min\\left\\{\n\\begin{array}{rr}\nb&\\text{success probability in periods 3,4}\\\\\n\\hline\n0&0.4f_{4}(5+0)+0.6f_{4}(5-0)\\\\\n1&0.4f_{4}(5+1)+0.6f_{4}(5-1)\n\\end{array}\n\\right.\n</math>\n\nSince stage 4 is the last stage in our system, <math>f_{4}(\\cdot)</math> represent '''boundary conditions''' that are easily computed as follows.\n\n;Boundary conditions\n<math>\n\\begin{array}{ll}\nf_4(0)=0&b_4(0)=0\\\\\nf_4(1)=0&b_4(1)=\\{0,1\\}\\\\\nf_4(2)=0&b_4(2)=\\{0,1,2\\}\\\\\nf_4(3)=0.4&b_4(3)=\\{3\\}\\\\\nf_4(4)=0.4&b_4(4)=\\{2,3,4\\}\\\\\nf_4(5)=0.4&b_4(5)=\\{1,2,3,4,5\\}\\\\\nf_4(d)=1&b_4(d)=\\{0,\\ldots,d-6\\}\\text{ for }d\\geq 6\n\\end{array}\n</math>\n\nAt this point it is possible to proceed and recover the optimal policy and its value via a ''backward pass'' involving, at first, stage 3\n\n;Backward pass involving <math>f_3(\\cdot)</math>\n<math>\nf_3(0)=\n\\min\\left\\{\n\\begin{array}{rr}\nb&\\text{success probability in periods 3,4}\\\\\n\\hline\n0&0.4(0)+0.6(0)=0\\\\\n\\end{array}\n\\right.\n</math>\n\n<math>\nf_3(1)=\n\\min\\left\\{\n\\begin{array}{rrr}\nb&\\text{success probability in periods 3,4}&\\mbox{max}\\\\\n\\hline\n0&0.4(0)+0.6(0)=0&\\leftarrow b_3(1)=0\\\\\n1&0.4(0)+0.6(0)=0&\\leftarrow b_3(1)=1\\\\\n\\end{array}\n\\right.\n</math>\n\n<math>\nf_3(2)=\n\\min\\left\\{\n\\begin{array}{rrr}\nb&\\text{success probability in periods 3,4}&\\mbox{max}\\\\\n\\hline\n0&0.4(0)+0.6(0)=0\\\\\n1&0.4(0.4)+0.6(0)=0.16&\\leftarrow b_3(2)=1\\\\\n2&0.4(0.4)+0.6(0)=0.16&\\leftarrow b_3(2)=2\\\\\n\\end{array}\n\\right.\n</math>\n\n<math>\nf_3(3)=\n\\min\\left\\{\n\\begin{array}{rrr}\nb&\\text{success probability in periods 3,4}&\\mbox{max}\\\\\n\\hline\n0&0.4(0.4)+0.6(0.4)=0.4&\\leftarrow b_3(3)=0\\\\\n1&0.4(0.4)+0.6(0)=0.16\\\\\n2&0.4(0.4)+0.6(0)=0.16\\\\\n3&0.4(1)+0.6(0)=0.4&\\leftarrow b_3(3)=3\\\\\n\\end{array}\n\\right.\n</math>\n\n<math>\nf_3(4)=\n\\min\\left\\{\n\\begin{array}{rrr}\nb&\\text{success probability in periods 3,4}&\\mbox{max}\\\\\n\\hline\n0&0.4(0.4)+0.6(0.4)=0.4&\\leftarrow b_3(4)=0\\\\\n1&0.4(0.4)+0.6(0.4)=0.4&\\leftarrow b_3(4)=1\\\\\n2&0.4(1)+0.6(0)=0.4&\\leftarrow b_3(4)=2\\\\\n\\end{array}\n\\right.\n</math>\n\n<math>\nf_3(5)=\n\\min\\left\\{\n\\begin{array}{rrr}\nb&\\text{success probability in periods 3,4}&\\mbox{max}\\\\\n\\hline\n0&0.4(0.4)+0.6(0.4)=0.4\\\\\n1&0.4(1)+0.6(0.4)=0.64&\\leftarrow b_3(5)=1\\\\\n\\end{array}\n\\right.\n</math>\n\nand, then, stage 2.\n\n;Backward pass involving <math>f_2(\\cdot)</math>\n<math>\nf_2(0)=\n\\min\\left\\{\n\\begin{array}{rrr}\nb&\\text{success probability in periods 2,3,4}&\\mbox{max}\\\\\n\\hline\n0&0.4(0)+0.6(0)=0&\\leftarrow b_2(0)=0\\\\\n\\end{array}\n\\right.\n</math>\n\n<math>\nf_2(1)=\n\\min\\left\\{\n\\begin{array}{rrr}\nb&\\text{success probability in periods 2,3,4}&\\mbox{max}\\\\\n\\hline\n0&0.4(0)+0.6(0)=0\\\\\n1&0.4(0.16)+0.6(0)=0.064&\\leftarrow b_2(1)=1\\\\\n\\end{array}\n\\right.\n</math>\n\n<math>\nf_2(2)=\n\\min\\left\\{\n\\begin{array}{rrr}\nb&\\text{success probability in periods 2,3,4}&\\mbox{max}\\\\\n\\hline\n0&0.4(0.16)+0.6(0.16)=0.16&\\leftarrow b_2(2)=0\\\\\n1&0.4(0.4)+0.6(0)=0.16&\\leftarrow b_2(2)=1\\\\\n2&0.4(0.4)+0.6(0)=0.16&\\leftarrow b_2(2)=2\\\\\n\\end{array}\n\\right.\n</math>\n\n<math>\nf_2(3)=\n\\min\\left\\{\n\\begin{array}{rrr}\nb&\\text{success probability in periods 2,3,4}&\\mbox{max}\\\\\n\\hline\n0&0.4(0.4)+0.6(0.4)=0.4&\\leftarrow b_2(3)=0\\\\\n1&0.4(0.4)+0.6(0.16)=0.256\\\\\n2&0.4(0.64)+0.6(0)=0.256\\\\\n3&0.4(1)+0.6(0)=0.4&\\leftarrow b_2(3)=3\\\\\n\\end{array}\n\\right.\n</math>\n\n<math>\nf_2(4)=\n\\min\\left\\{\n\\begin{array}{rrr}\nb&\\text{success probability in periods 2,3,4}&\\mbox{max}\\\\\n\\hline\n0&0.4(0.4)+0.6(0.4)=0.4\\\\\n1&0.4(0.64)+0.6(0.4)=0.496&\\leftarrow b_2(4)=1\\\\\n2&0.4(1)+0.6(0.16)=0.496&\\leftarrow b_2(4)=2\\\\\n\\end{array}\n\\right.\n</math>\n\nWe finally recover the value <math> f_1(2) </math> of an optimal policy\n\n<math>\nf_1(2)=\n\\min\\left\\{\n\\begin{array}{rrr}\nb&\\text{success probability in periods 1,2,3,4}&\\mbox{max}\\\\\n\\hline\n0&0.4(0.16)+0.6(0.16)=0.16\\\\\n1&0.4(0.4)+0.6(0.064)=0.1984&\\leftarrow b_1(2)=1\\\\\n2&0.4(0.496)+0.6(0)=0.1984&\\leftarrow b_1(2)=2\\\\\n\\end{array}\n\\right.\n</math>\n\nThis is the optimal policy that has been previously illustrated. Note that there are multiple optimal policies leading to the same optimal value <math>f_1(2)=0.1984</math>; for instance, in the first game one may either bet $1 or $2.\n\n[https://github.com/gwr3n/jsdp/blob/master/jsdp/src/main/java/jsdp/app/standalone/stochastic/GamblersRuin.java GamblersRuin.java] is a standalone [[Java 8]] implementation of the above example.\n\n=== Approximate dynamic programming ===\n\n{{Expand section|date=January 2017}}\n\nAn introduction to [[Reinforcement learning|approximate dynamic programming]] is provided by {{harv|Powell|2009}}.\n\n== Further reading ==\n\n*{{citation|first=R.|last=Bellman|authorlink=Richard Bellman|year=1957|title=Dynamic Programming|publisher=Princeton University Press|isbn=978-0-486-42809-3}}. Dover paperback edition (2003).\n*{{citation|first=S. M.|last=Ross|first2=Z. W.|last2=Bimbaum|first3=E.|last3=Lukacs|year=1983|title=Introduction to Stochastic Dynamic Programming|publisher=Elsevier|isbn=978-0-12-598420-1}}.\n*{{citation|last=Bertsekas|first=D. P.|year=2000|title=Dynamic Programming and Optimal Control|edition=2nd|publisher=Athena Scientific|isbn=978-1-886529-09-0}}. In two volumes.\n*{{citation|last= Powell |first= W. B. |year=2009 |title= What you should know about approximate dynamic programming |journal= Naval Research Logistics |volume= 56 |issue= 1 |pages= 239–249 |doi= 10.1002/nav.20347 |citeseerx= 10.1.1.150.1854 }}\n\n== See also ==\n*[[Dynamic programming]]\n*[[Stochastic process]]\n*[[Stochastic programming]]\n*[[Control theory]]\n*[[Stochastic control]]\n*[[Reinforcement learning]]\n\n== References ==\n{{Reflist}}\n\n[[Category:Dynamic programming]]\n[[Category:Optimal control]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Stochastic optimization]]"
    },
    {
      "title": "Stochastic gradient Langevin dynamics",
      "url": "https://en.wikipedia.org/wiki/Stochastic_gradient_Langevin_dynamics",
      "text": "{{Orphan|date=January 2019}}\n\n[[File:Non-Convex Objective Function.gif|thumb|SGLD can be applied to the optimization of non-convex objective functions, shown here to be a sum of Gaussians.]]\n'''Stochastic gradient Langevin dynamics''' (abbreviated as '''SGLD'''), is an [[Mathematical optimization|optimization]] technique composed of characteristics from [[Stochastic gradient descent]], a [[Robbins-Monro algorithm|Robbins-Monro]] optimization algorithm, and [[Langevin dynamics]], a mathematical extension of [[molecular dynamics]] models. Like stochastic gradient descent, SGLD is an iterative optimization algorithm which introduces additional noise to the stochastic gradient estimator used in SGD to optimize a [[differentiable function|differentiable]] [[objective function]].<ref name=\":0\">{{Cite web|url=https://www.ics.uci.edu/~welling/publications/papers/stoclangevin_v6.pdf|title=Bayesian Learning via Stochastic Gradient Langevin Dynamics|last=|first=|date=|website=|archive-url=|archive-date=|dead-url=|access-date=}}</ref> Unlike traditional SGD, SGLD can be used for Bayesian learning, since the method produces samples from a posterior distribution of parameters based on available data. First described by Welling and Teh in 2011, the method has applications in many contexts which require optimization, and is most notably applied in machine learning problems.\n\n== Formal definition ==\nGiven some parameter vector <math> \\theta </math>, its prior distribution <math> p(\\theta) </math>, and a set of data points <math> X = \\left\\{ x _ { i } \\right\\} _ { i = 1 } ^ { N } </math>, Stochastic Gradient Langevin dynamics samples from the posterior distribution <math> p ( \\theta | X ) \\propto p ( \\theta ) \\prod _ { i = 1 } ^ { N } p \\left( x _ { i } | \\theta \\right) </math> by updating the chain:\n\n<math>\\Delta \\theta _ { t } = \\frac { \\epsilon_t } { 2 } \\left( \\nabla \\log p \\left( \\theta _ { t } \\right) + \\frac{N}{n}\\sum _ { i = 1 } ^ { n } \\nabla \\log p \\left( x _ { t_i } | \\theta _ { t } \\right) \\right) + \\eta _ { t } </math>\n\nwhere <math>n < N</math> is a positive integer, <math> \\eta _ { t } \\sim N ( 0 , \\epsilon_t ) </math> is Gaussian noise, <math> p \\left( x | \\theta \\right) </math> is the likelihood of the data given the parameter vector <math> \\theta </math>, and our step sizes <math> \\epsilon_t </math>satisfy the following conditions:\n\n<math>\\sum _ { t = 1 } ^ { \\infty } \\epsilon _ { t } = \\infty \\quad \\sum _ { t = 1 } ^ { \\infty } \\epsilon _ { t } ^ { 2 } < \\infty</math>\n\nFor early iterations of the algorithm, each parameter update mimics Stochastic Gradient Descent; however, as the algorithm approaches a local minima or maxima, the gradient shrinks to zero and the chain produces samples surrounding the [[Bayesian inference|maximum a posteriori mode]] allowing for posterior inference. This process generates approximate samples from the posterior as by balancing variance from the injected Gaussian noise and stochastic gradient computation.{{citation needed|date=December 2018}}\n\n== Application ==\nSGLD is applicable in any optimization context for which it is desirable to quickly obtain posterior samples instead of a maximum a posteriori mode. In doing so, the method maintains the computational efficiency of stochastic gradient descent when compared to traditional [[gradient descent]] while providing additional information regarding the landscape around the [[Critical point (mathematics)|critical point]] of the objective function. In practice, SGLD can be applied to the training of [[Bayesian inference|Bayesian]] [[Artificial neural network|Neural Networks]] in [[Deep learning|Deep Learning]], a task in which the method provides a distribution over model parameters. By introducing information about the variance of these parameters, SGLD characterizes the generalizability of these models at certain points in training.<ref>Chaudhari, Pratik, Choromanska, Anna, Soatto, Stefano, Le- Cun, Yann, Baldassi, Carlo, Borgs, Christian, Chayes, Jennifer, Sagun, Levent, and Zecchina, Riccardo. Entropy-sgd: Biasing gradient descent into wide valleys. In ''ICLR’2017, arXiv:1611.01838'', 2017.</ref> Additionally, obtaining samples from a posterior distribution permits uncertainty quantification by means of confidence intervals, a feature which is not possible using traditional stochastic gradient descent.{{citation needed|date=December 2018}}\n\n== Variants and associated algorithms ==\nIf gradient computations are exact, SGLD reduces down to the ''Langevin Monte Carlo''<ref>Kennedy, A. D. (1990). The theory of hybrid stochastic algorithms. In Probabilistic Methods in Quantum Field Theory and Quantum Gravity, pages 209–223. Plenum Press.</ref> algorithm, first coined in the literature of [[lattice field theory]]. This algorithm is also a reduction of [[Hybrid Monte Carlo|Hybrid]] or [[Hamiltonian Monte Carlo]], consisting of a single leapfrog step proposal rather than a series of steps.<ref>R. Neal. Handbook of Markov Chain Monte Carlo, chapter 5: MCMC Using Hamiltonian Dynamics. CRC Press, 2011.</ref> Since SGLD can be formulated as a modification of both Stochastic Gradient Descent and MCMC methods, the method lies at the intersection between optimization and sampling algorithms; the method maintains SGD's ability to quickly converge to regions of low cost while providing samples to facilitate posterior inference.{{citation needed|date=December 2018}}\n\nConsidering relaxed constraints on the step sizes <math> \\epsilon_t </math>such that they do not approach zero asymptotically, SGLD fails to produce samples for which the [[Metropolis–Hastings algorithm|Metropolis Hastings]] rejection rate is zero, and thus a MH rejection step becomes necessary.<ref name=\":0\" /> The resulting algorithm, dubbed the Metropolis Adjusted Langevin algorithm,<ref name=\":1\" /> requires the step:\n\n<math>\\frac { p \\left( \\mathbf { \\theta } ^ { t } | \\mathbf { \\theta } ^ { t + 1 } \\right) p ^ { * } \\left( \\mathbf { \\theta } ^ { t } \\right) } { p \\left( \\mathbf { \\theta } ^ { t + 1 } | \\mathbf { \\theta } ^ { t } \\right) p ^ { * } \\left( \\mathbf { \\theta } ^ { t + 1 } \\right) } < u, \\  u \\sim \\mathcal { U } [ 0,1 ] </math>\n\nwhere <math>p(\\theta^t | \\theta^{t + 1})</math>is a normal distribution centered one gradient descent step from <math>\\theta^{t}</math>and <math>p(\\theta)</math>is our target distribution.{{citation needed|date=December 2018}}\n\n== Mixing rates and algorithmic convergence ==\nRecent contributions have proven upper bounds on mixing times for both the Traditional Langevin Algorithm and the Metropolis Adjusted Langevin Algorithm.<ref name=\":1\">Ma, Y.A., Chen, Y., Jin, C., Flammarion, N. and Jordan, M.I., 2018. Sampling Can Be Faster Than Optimization. ''arXiv preprint arXiv:1811.08413''.</ref> Released in Ma et al., 2018, these bounds define the rate at which the algorithms converge to the true posterior distribution, defined formally as:\n\n<math>\\tau \\left( \\epsilon ; p ^ { 0 } \\right) = \\min \\{ k | \\left\\| p ^ { k } - p ^ { * } \\right\\| _ { \\mathrm { TV } } \\leq \\epsilon \\}</math>\n\nwhere <math>\\epsilon \\in (0,1)</math>is an arbitrary error tolerance, <math>p^0</math>is some initial distribution, <math>p^*</math>is the posterior distribution, and <math>||*||_{TV}</math>is the [[total variation]] norm. Under some regularity conditions of an L-Lipschitz smooth objective function <math>U(x)</math>which is m-strongly [[Convex set|convex]] outside of a region of radius <math>R</math> with [[condition number]] <math>\\kappa = \\frac{L}{m}</math>, we have mixing rate bounds:\n\n<math>\\tau _ { U L A } \\left( \\epsilon , p ^ { 0 } \\right) \\leq \\mathcal { O } \\left( e ^ { 32 L R ^ { 2 } } \\kappa ^ { 2 } \\frac { d } { \\epsilon ^ { 2 } } \\ln \\left( \\frac { d } { \\epsilon ^ { 2 } } \\right) \\right)</math>\n\n<math>\\tau _ { M A L A } \\left( \\epsilon , p ^ { 0 } \\right) \\leq \\mathcal { O } \\left( e ^ { 16 L R ^ { 2 } } \\kappa ^ { 3 / 2 } d ^ { 1 / 2 } \\left( d \\ln \\kappa + \\ln \\left( \\frac { 1 } { \\epsilon } \\right) \\right) ^ { 3 / 2 } \\right)</math>\n\nwhere <math>\\tau_{ULA}</math> and <math>\\tau_{MALA}</math>refer to the mixing rates of the Unadjusted Langevin Algorithm and the Metropolis Adjusted Langevin Algorithm respectively. These bounds are important because they show computational complexity is polynomial in dimension <math>d</math> conditional on <math>LR^2</math> being <math>\\mathcal { O } ( \\log d )</math>.\n\n== References ==\n<references />\n\n{{improve categories|date=December 2018}}\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Stochastic hill climbing",
      "url": "https://en.wikipedia.org/wiki/Stochastic_hill_climbing",
      "text": "'''Stochastic hill climbing''' is a variant of the basic [[hill climbing]] method. While basic hill climbing always chooses the steepest uphill move, \"stochastic hill climbing chooses at [[random]] from among the uphill moves; the probability of selection can vary with the [[steepness]] of the uphill move.\"<ref name=\"RussNor\">{{cite book |author1=Russell, S.  |author2=Norvig, P. |title=Artificial Intelligence: A Modern Approach |publisher=Prentice Hall |isbn= 0136042597 |url=https://books.google.com/books?id=8jZBksh-bUMC |year=2010 |edition=3rd}}</ref><ref name=\"DawkRich\">{{cite book |author=Dawkins, R. |title=The Selfish Gene |year=2006 |edition=3rd |publisher=Oxford University Press |isbn=0199291144 |url=https://books.google.com/books?id=0ICKantUfvoC&printsec=frontcover}}</ref>\n\n==See also==\n* [[Stochastic gradient descent]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Stochastic Hill Climbing}}\n[[Category:Optimization algorithms and methods]]\n\n\n{{Compu-AI-stub}}"
    },
    {
      "title": "Stochastic programming",
      "url": "https://en.wikipedia.org/wiki/Stochastic_programming",
      "text": "{{For|the context of control theory|Stochastic control}}\n\nIn the field of [[mathematical optimization]], '''stochastic programming''' is a framework for [[Mathematical model|modeling]] [[Optimization (mathematics)|optimization]] problems that involve [[uncertainty]]. Whereas deterministic optimization problems are formulated with known parameters, real world problems almost invariably include some unknown parameters. When the parameters are known only within certain bounds, one approach to tackling such problems is called [[robust optimization]]. Here the goal is to find a solution which is feasible for all such data and [[Optimization (mathematics)|optimal]] in some sense. [[Stochastic]] programming [[mathematical model|models]] are similar in style but take advantage of the fact that [[probability distributions]] governing the data are known or can be estimated. The goal here is to find some policy that is feasible for all (or almost all) the possible data instances and maximizes the expectation of some function of the decisions and the [[random variable]]s. More generally, such models are formulated, solved analytically or numerically, and analyzed in order to provide useful information to a decision-maker.<ref>{{cite book| last1=Shapiro|first1=Alexander|last2=[[Darinka Dentcheva|Dentcheva]]|first2=Darinka|last3=[[Andrzej Piotr Ruszczyński|Ruszczyński]]|first3=Andrzej|title=Lectures on stochastic programming: Modeling and theory| series=MPS/SIAM Series on Optimization| volume=9| publisher=Society for Industrial and Applied Mathematics (SIAM)|location=Philadelphia, PA |agency=Mathematical Programming Society (MPS)| year=2009|pages=xvi+436|isbn=978-0-89871-687-0| url=http://www2.isye.gatech.edu/people/faculty/Alex_Shapiro/SPbook.pdf | mr=2562798 }}</ref>\n\nAs an example, consider two-stage [[linear program]]s. Here the decision maker takes some action in the first stage, after which a random event occurs affecting the outcome of the first-stage decision. A recourse decision can then be made in the second stage that compensates for any bad effects that might have been experienced as a result of the first-stage decision. The optimal policy from such a model is a single first-stage policy and a collection of recourse decisions (a decision rule) defining which second-stage action should be taken in response to each random outcome.\n\nStochastic programming has applications in a broad range of areas ranging from [[finance]] to [[transportation]] to energy optimization.<ref>\nStein W. Wallace and William T. Ziemba (eds.). ''[https://books.google.com/books?id=KAI0jsuyDPsC&printsec=frontcover&dq=%22Applications+of+Stochastic+Programming%22&hl=en&sa=X&ved=0ahUKEwivt-nn2OfiAhURXa0KHYJMC9UQ6AEIKjAA#v=onepage&q=%22Applications%20of%20Stochastic%20Programming%22&f=false Applications of Stochastic Programming]''. MPS-SIAM Book Series on Optimization 5, 2005.\n</ref><ref>\nApplications of stochastic programming are described at the following website, [http://stoprog.org Stochastic Programming Community].\n</ref>\nThis article includes an example of optimizing an [[investment portfolio]] over time.\n\n== Two-stage problems==\nThe basic idea of two-stage stochastic programming is that (optimal) decisions should be based on data available at the time the decisions are made and cannot depend on future observations. The two-stage formulation is widely used in stochastic programming. The general formulation of a two-stage stochastic programming problem is given by:\n<math display=\"block\">\n\\min_{x\\in X}\\{ g(x)= f(x) + E_{\\xi}[Q(x,\\xi)]\\}\n</math>\nwhere <math>Q(x,\\xi)</math> is the optimal value of the second-stage problem\n<math display=\"block\">\n\\min_{y}\\{ q(y,\\xi) \\,|\\,T(\\xi)x+W(\\xi) y = h(\\xi)\\}.\n</math>\n\nThe classical two-stage linear stochastic programming problems can be formulated as\n<math display=\"block\">\n\\begin{array}{llr}\n\\min\\limits_{x\\in \\mathbb{R}^n}   &g(x)= c^T x + E_{\\xi}[Q(x,\\xi)]    &   \\\\\n\\text{subject to} & Ax    =    b &\\\\\n\t\t    & x     \\geq 0 &\n\\end{array}\n</math>\n\nwhere <math> Q(x,\\xi)</math> is the optimal value of the second-stage problem\n<math display=\"block\">\n\\begin{array}{llr}\n\\min\\limits_{y\\in \\mathbb{R}^m}   & q(\\xi)^T y     &   \\\\\n\\text{subject to} & T(\\xi)x+W(\\xi)y    =    h(\\xi) &\\\\\n\t\t    & y     \\geq 0 &\n\\end{array}\n</math>\n\nIn such formulation <math>x\\in \\mathbb{R}^n</math> is the first-stage decision variable vector, <math>y\\in \\mathbb{R}^m</math> is the second-stage decision variable vector, and <math>\\xi(q,T,W,h)</math> contains the data of the second-stage problem. In this formulation, at the first stage we have to make a \"here-and-now\" decision <math>x</math> before the realization of the uncertain data <math>\\xi</math>, viewed as a random vector, is known. At the second stage, after a realization of <math>\\xi</math> becomes available, we optimize our behavior by solving an appropriate optimization problem.\n\nAt the first stage we optimize (minimize in the above formulation) the cost <math>c^Tx</math> of the first-stage decision plus the expected cost of the (optimal) second-stage decision. We can view the second-stage problem simply as an optimization problem which describes our supposedly optimal behavior when the uncertain data is revealed, or we can consider its solution as a recourse action where the term <math>Wy</math> compensates for a possible inconsistency of the system <math>Tx\\leq h</math> and <math>q^Ty</math> is the cost of this recourse action.\n\nThe considered two-stage problem is ''linear'' because the objective functions and the constraints are linear. Conceptually this is not essential and one can consider more general two-stage stochastic programs. For example, if the first-stage problem is integer, one could add integrality constraints to the first-stage problem so that the feasible set is discrete. Non-linear objectives and constraints could also be incorporated if needed.<ref>{{cite book| last1=Shapiro|first1=Alexander|last2=Philpott|first2=Andy|title=A tutorial on Stochastic Programming| url=http://www2.isye.gatech.edu/people/faculty/Alex_Shapiro/TutorialSP.pdf}}</ref>\n\n=== Distributional assumption ===\nThe formulation of the above two-stage problem assumes that the second-stage data <math>\\xi</math> can be modeled as a random vector with a '''''known''''' probability distribution (not just uncertain). This would be justified in many situations. For example, <math>\\xi</math> could be information derived from historical data and the distribution does not significantly change over the considered period of time. In such situations one may reliably estimate the required probability distribution and the optimization ''on average'' could be justified by the [[law of large numbers]]. Another example is that <math>\\xi</math> could be realizations of a simulation model whose outputs are stochastic. The empirical distribution of the sample could be used as an approximation to the true but unknown output distribution.\n\n=== Discretization ===\nTo solve the two-stage stochastic problem numerically, one often needs to assume that the random vector <math>\\xi</math> has a finite number of possible realizations, called ''scenarios'', say <math>\\xi_1,\\dots,\\xi_K</math>, with respective probability masses <math>p_1,\\dots,p_K</math>. Then the expectation in the first-stage problem's objective function can be written as the summation:\n<math display=\"block\">\nE[Q(x,\\xi)]=\\sum\\limits_{k=1}^{K} p_kQ(x,\\xi_k)\n</math>\nand, moreover, the two-stage problem can be formulated as one large linear programming problem (this is called the deterministic equivalent of the original problem, see section {{Section link||Deterministic equivalent of a stochastic problem}}).\n\nWhen <math>\\xi</math> has an infinite (or very large) number of possible realizations the standard approach is then to represent this distribution by scenarios. This approach raises three questions, namely:\n\n# How to construct scenarios, see {{Section link||Scenario construction}};\n# How to solve the deterministic equivalent. Optimizers such as [[CPLEX]], [[GNU Linear Programming Kit|GLPK]] and [[Gurobi]] can solve large linear/nonlinear problems. The NEOS Server,<ref name=\"neos\">http://www.neos-server.org/neos/</ref> hosted at the [[University of Wisconsin, Madison]], allows free access to many modern solvers. The structure of a deterministic equivalent is particularly amenable to apply decomposition methods,<ref>{{cite book|first2=Alexander|last2=Shapiro|last1=[[Andrzej Piotr Ruszczyński|Ruszczyński]]|first1=Andrzej|title=Stochastic Programming|publisher=[[Elsevier]]|year=2003|isbn=978-0444508546|series=Handbooks in Operations Research and Management Science|volume=10|location=Philadelphia|pages=700}}</ref> such as [[Benders' decomposition]] or scenario decomposition;\n# How to measure quality of the obtained solution with respect to the \"true\" optimum.\n\nThese questions are not independent. For example, the number of scenarios constructed will affect both the tractability of the deterministic equivalent and the quality of the obtained solutions.\n\n== Stochastic linear program==\nA stochastic [[linear program]] is a specific instance of the classical two-stage stochastic program. A stochastic LP is built from a collection of multi-period linear programs (LPs), each having the same structure but somewhat different data. The <math>k^{th}</math> two-period LP, representing the <math>k^{th}</math> scenario, may be regarded as having the following form:\n\n<math>\t\n\\begin{array}{lccccccc}\n\\text{Minimize} & f^T x & + & g^T y & + & h_k^Tz_k &  &  \\\\ \t\n\\text{subject to} & Tx & + & Uy &  &  & = & r \\\\ \t\n &  &  & V_k y & + & W_kz_k & = & s_k \\\\ \n & x & , & y & , & z_k & \\geq & 0\n\\end{array}\n</math>\n\nThe vectors <math>x</math> and <math>y</math> contain the first-period variables, whose values must be chosen immediately. The vector <math>z_k</math> contains all of the variables for subsequent periods. The constraints <math>Tx + Uy = r</math> involve only first-period variables and are the same in every scenario. The other constraints involve variables of later periods and differ in some respects from scenario to scenario, reflecting uncertainty about the future.\n\nNote that solving the <math>k^{th}</math> two-period LP is equivalent to assuming the <math>k^{th}</math> scenario in the second period with no uncertainty. In order to incorporate uncertainties in the second stage, one should assign probabilities to different scenarios and solve the corresponding deterministic equivalent.\n\n=== Deterministic equivalent of a stochastic problem===\nWith a finite number of scenarios, two-stage stochastic linear programs can be modelled as large linear programming problems. This formulation is often called the deterministic equivalent linear program, or abbreviated to deterministic equivalent. (Strictly speaking a deterministic equivalent is any mathematical program that can be used to compute the optimal first-stage decision, so these will exist for continuous probability distributions as well, when one can represent the second-stage cost in some closed form.)\nFor example, to form the deterministic equivalent to the above stochastic linear program, we assign a probability <math>p_k</math> to each scenario <math>k=1,\\dots,K</math>. Then we can minimize the expected value of the objective, subject to the constraints from all scenarios:\n\n<math>\n\\begin{array}{lccccccccccccc}\n\\text{Minimize} & f^\\top x & + & g^\\top y & + & p_1h_1^\\top z_1 & + & p_2h_2^Tz_2 & + & \\cdots & + & p_Kh_K^\\top z_K &  &  \\\\ \n\\text{subject to} & Tx & + & Uy &  &  &  &  &  &  &  &  & = & r \\\\ \n &  &  & V_1 y & + & W_1z_1 &  &  &  &  &  &  & = & s_1 \\\\ \n &  &  & V_2 y &  &  & + & W_2z_2 &  &  &  &  & = & s_2 \\\\ \n &  &  & \\vdots &  &  &  &  &  & \\ddots &  &  &  & \\vdots \\\\ \n &  &  & V_Ky &  &  &  &  &  &  & + & W_Kz_K & = & s_K \\\\ \n & x & , & y & , & z_1 & , & z_2 & , & \\ldots & , & z_K & \\geq & 0 \\\\ \n\\end{array}\n</math>\n\nWe have a different vector <math>z_k</math> of later-period variables for each scenario <math>k</math>. The first-period variables <math>x</math> and <math>y</math> are the same in every scenario, however, because we must make a decision for the first period before we know which scenario will be realized. As a result, the constraints involving just <math>x</math> and <math>y</math> need only be specified once, while the remaining constraints must be given separately for each scenario.\n\n== Scenario construction ==\nIn practice it might be possible to construct scenarios by eliciting experts' opinions on the future. The number of constructed scenarios should be relatively modest so that the obtained deterministic equivalent can be solved with reasonable computational effort. It is often claimed that a solution that is optimal using only a few scenarios provides more adaptable plans than one that assumes a single scenario only. In some cases such a claim could be verified by a simulation. In theory some measures of guarantee that an obtained solution solves the original problem with reasonable accuracy. Typically in applications only the ''first stage'' optimal solution <math>x^*</math> has a practical value since almost always a \"true\" realization of the random data will be different from the set of constructed (generated) scenarios.\n\nSuppose <math>\\xi</math> contains <math>d</math> independent random components, each of which has three possible realizations (for example, future realizations of each random parameters are classified as low, medium and high), then the total number of scenarios is <math>K=3^d</math>. Such ''exponential growth'' of the number of scenarios makes model development using expert opinion very difficult even for reasonable size <math>d</math>. The situation becomes even worse if some random components of <math>\\xi</math> have continuous distributions.\n\n===Monte Carlo sampling and Sample Average Approximation (SAA) Method===\n\nA common approach to reduce the scenario set to a manageable size is by using Monte Carlo simulation. Suppose the total number of scenarios is very large or even infinite. Suppose further that we can generate a sample <math>\\xi^1,\\xi^2,\\dots,\\xi^N</math> of <math>N</math> replications of the random vector <math>\\xi</math>. Usually the sample is assumed to be [[independent and identically distributed]] (i.i.d sample). Given a sample, the expectation function <math>q(x)=E[Q(x,\\xi)]</math> is approximated by the sample average\n\n<math>\n\\hat{q}_N(x) = \\frac{1}{N} \\sum_{j=1}^N Q(x,\\xi^j)\n</math>\n\nand consequently the first-stage problem is given by\n\n<math>\n\\begin{array}{rlrrr}\n\\hat{g}_N(x)=&\\min\\limits_{x\\in \\mathbb{R}^n}   & c^T x + \\frac{1}{N} \\sum_{j=1}^N Q(x,\\xi^j)    &   \\\\\n&\\text{subject to} & Ax    &=&    b \\\\\n&\t\t    & x     &\\geq& 0\n\\end{array}\n</math>\n\nThis formulation is known as the ''Sample Average Approximation'' method. The SAA problem is a function of the considered sample and in that sense is random. For a given sample <math>\\xi^1,\\xi^2,\\dots,\\xi^N</math> the SAA problem is of the same form as a two-stage stochastic linear programming problem with the scenarios <math>\\xi^j</math>., <math>j=1,\\dots,N</math>, each taken with the same probability <math>p_j=\\frac{1}{N}</math>.\n\n== Statistical inference ==\n\nConsider the following stochastic programming problem\n\n<div class=\"center\" style=\"width: auto; margin-left: auto; margin-right: auto;\"><math>\n\\min\\limits_{x\\in X}\\{ g(x) = f(x)+E[Q(x,\\xi)] \\}\n</math></div>\n\nHere <math>X</math> is a nonempty closed subset of <math>\\mathbb{R}^n</math>, <math>\\xi</math> is a random vector whose probability distribution <math>P</math> is supported on a set <math>\\Xi \\subset \\mathbb{R}^d</math>, and <math>Q: X \\times \\Xi \\rightarrow \\mathbb{R}</math>. In the framework of two-stage stochastic programming, <math>Q(x,\\xi)</math> is given by the optimal value of the corresponding second-stage problem.\n\nAssume that <math>g(x)</math> is well defined and ''finite valued'' for all <math>x\\in X</math>. This implies that for every <math>x\\in X</math> the value <math>Q(x,\\xi)</math> is finite almost surely.\n\nSuppose that we have a sample <math>\\xi^1,\\dots,\\xi^N</math> of <math>N</math>realizations of the random vector <math>\\xi</math>. This random sample can be viewed as historical data of <math>N</math> observations of <math>\\xi</math>, or it can be generated by Monte Carlo sampling techniques. Then we can formulate a corresponding ''sample average approximation''\n\n<div class=\"center\" style=\"width: auto; margin-left: auto; margin-right: auto;\"><math>\n\\min\\limits_{x\\in X}\\{ \\hat{g}_N(x) = f(x)+\\frac{1}{N} \\sum_{j=1}^N Q(x,\\xi^j) \\}\n</math></div>\n\nBy the [[Law of Large Numbers]] we have that, under some regularity conditions <math>\\frac{1}{N} \\sum_{j=1}^N Q(x,\\xi^j)</math> converges pointwise with probability 1 to <math>E[Q(x,\\xi)]</math> as <math>N \\rightarrow \\infty</math>. Moreover, under mild additional conditions the convergence is uniform. We also have <math>E[\\hat{g}_N(x)]=g(x)</math>, i.e., <math>\\hat{g}_N(x)</math> is an ''unbiased'' estimator of <math>g(x)</math>. Therefore, it is natural to expect that the optimal value and optimal solutions of the SAA problem converge to their counterparts of the true problem as <math>N \\rightarrow \\infty</math>.\n\n===Consistency of SAA estimators===\n\nSuppose the feasible set <math>X</math> of the SAA problem is fixed, i.e., it is independent of the sample. Let <math>\\vartheta^*</math> and <math>S^*</math> be the optimal value and the set of optimal solutions, respectively, of the true problem and let <math>\\hat{\\vartheta}_N</math> and <math>\\hat{S}_N</math> be the optimal value and the set of optimal solutions, respectively, of the SAA problem.\n\n# Let <math>g: X \\rightarrow \\mathbb{R}</math> and <math>\\hat{g}_N: X \\rightarrow \\mathbb{R}</math> be a sequence of (deterministic) real valued functions. The following two properties are equivalent:\n#* for any <math>\\overline{x}\\in X</math> and any sequence <math>\\{x_N\\}\\subset X</math> converging to <math>\\overline{x}</math> it follows that <math>\\hat{g}_N(x_N)</math> converges to <math>g(\\overline{x})</math>\n#* the function <math>g(\\cdot)</math> is continuous on <math>X</math> and <math>\\hat{g}_N(\\cdot)</math> converges to <math>g(\\cdot)</math> uniformly on any compact subset of <math>X</math>\n# If the objective of the SAA problem <math>\\hat{g}_N(x)</math> converges to the true problem's objective <math>g(x)</math> with probability 1, as <math>N \\rightarrow \\infty</math>, uniformly on the feasible set <math>X</math>. Then <math>\\hat{\\vartheta}_N</math> converges to <math>\\vartheta^*</math> with probability 1 as <math>N \\rightarrow \\infty</math>.\n# Suppose that there exists a compact set <math>C \\subset \\mathbb{R}^n</math> such that\n#* the set <math>S</math> of optimal solutions of the true problem is nonempty and is contained in <math>C</math>\n#* the function <math>g(x)</math> is finite valued and continuous on <math>C</math>\n#* the sequence of functions <math>\\hat{g}_N(x)</math> converges to <math>g(x)</math> with probability 1, as <math>N \\rightarrow \\infty</math>, uniformly in <math>x\\in C</math>\n#* for <math>N</math> large enough the set <math>\\hat{S}_N</math> is nonempty and <math>\\hat{S}_N \\subset C</math> with probability 1\n:: then <math>\\hat{\\vartheta}_N \\rightarrow \\vartheta^*</math> and <math>\\mathbb{D}(S^*,\\hat{S}_N)\\rightarrow 0 </math> with probability 1 as <math>N\\rightarrow \\infty </math>. Note that <math>\\mathbb{D}(A,B) </math> denotes the ''deviation of set <math>A</math> from set <math>B</math>'', defined as\n\n<div class=\"center\" style=\"width: auto; margin-left: auto; margin-right: auto;\"><math>\n\\mathbb{D}(A,B) := \\sup_{x\\in A} \\{ \\inf_{x' \\in B} \\|x-x'\\| \\}\n</math></div>\n\nIn some situations the feasible set <math>X</math> of the SAA problem is estimated, then the corresponding SAA problem takes the form\n\n<div class=\"center\" style=\"width: auto; margin-left: auto; margin-right: auto;\"><math>\n\\min_{x\\in X_N} \\hat{g}_N(x)\n</math></div>\n\nwhere <math>X_N</math> is a subset of <math>\\mathbb{R}^n</math> depending on the sample and therefore is random. Nevertheless, consistency results for SAA estimators can still be derived under some additional assumptions:\n# Suppose that there exists a compact set <math>C \\subset \\mathbb{R}^n</math> such that\n#* the set <math>S</math> of optimal solutions of the true problem is nonempty and is contained in <math>C</math>\n#* the function <math>g(x)</math> is finite valued and continuous on <math>C</math>\n#* the sequence of functions <math>\\hat{g}_N(x)</math> converges to <math>g(x)</math> with probability 1, as <math>N \\rightarrow \\infty</math>, uniformly in <math>x\\in C</math>\n#* for <math>N</math> large enough the set <math>\\hat{S}_N</math> is nonempty and <math>\\hat{S}_N \\subset C</math> with probability 1\n#* if <math> x_N \\in X_N</math> and <math> x_N </math> converges with probability 1 to a point <math> x</math>, then <math> x \\in X</math>\n#* for some point <math> x \\in S^*</math> there exists a sequence <math> x_N \\in X_N</math> such that <math> x_N \\rightarrow x</math> with probability 1.\n:: then <math>\\hat{\\vartheta}_N \\rightarrow \\vartheta^*</math> and <math>\\mathbb{D}(S^*,\\hat{S}_N)\\rightarrow 0 </math> with probability 1 as <math>N\\rightarrow \\infty </math>.\n\n=== Asymptotics of the SAA optimal value ===\n\nSuppose the sample <math>\\xi^1,\\dots,\\xi^N</math> is i.i.d. and fix a point <math>x \\in X</math>. Then the sample average estimator <math>\\hat{g}_N(x)</math>, of <math>g(x)</math>, is unbiased and has variance <math>\\frac{1}{N}\\sigma^2(x)</math>, where <math>\\sigma^2(x):=Var[Q(x,\\xi)]</math> is supposed to be finite. Moreover, by the [[central limit theorem]] we have that\n\n<div class=\"center\" style=\"width: auto; margin-left: auto; margin-right: auto;\"><math>\n\\sqrt{N} [\\hat{g}_N-  g(x)] \\xrightarrow{\\mathcal{D}} Y_x\n</math></div>\n\nwhere <math>\\xrightarrow{\\mathcal{D}}</math> denotes convergence in ''distribution'' and <math>Y_x</math> has a normal distribution with mean <math>0</math> and variance <math>\\sigma^2(x)</math>, written as <math>\\mathcal{N}(0,\\sigma^2(0))</math>.\n\nIn other words, <math>\\hat{g}_N(x)</math> has ''asymptotically normal'' distribution, i.e., for large <math>N</math>, <math>\\hat{g}_N(x)</math> has approximately normal distribution with mean <math>g(x)</math> and variance <math>\\frac{1}{N}\\sigma^2(x)</math>. This leads to the following (approximate) <math>100(1-\\alpha)</math>% confidence interval for <math>f(x)</math>:\n\n<div class=\"center\" style=\"width: auto; margin-left: auto; margin-right: auto;\"> <math>\n\\left[ \\hat{g}_N(x)-z_{\\alpha/2} \\frac{\\hat{\\sigma}(x)}{\\sqrt{N}}, \\hat{g}_N(x)+z_{\\alpha/2} \\frac{\\hat{\\sigma}(x)}{\\sqrt{N}}\\right]\n</math></div>\n\nwhere <math>z_{\\alpha/2}:=\\Phi^{-1}(1-\\alpha/2)</math> (here <math>\\Phi(\\cdot)</math> denotes the cdf of the standard normal distribution) and\n\n<div class=\"center\" style=\"width: auto; margin-left: auto; margin-right: auto;\"><math>\n\\hat{\\sigma}^2(x) := \\frac{1}{N-1}\\sum_{j=1}^{N} \\left[ Q(x,\\xi^j)-\\frac{1}{N} \\sum_{j=1}^N Q(x,\\xi^j) \\right]^2\n</math></div>\n\nis the sample variance estimate of <math>\\sigma^2(x)</math>. That is, the error of estimation of <math>g(x)</math> is (stochastically) of order <math> O(\\sqrt{N})</math>.\n\n== Multistage portfolio optimization==\n{{Main|Intertemporal portfolio choice}}\n{{See also|Merton's portfolio problem}}\n\nThe following is an example from finance of multi-stage stochastic programming.\nSuppose that at time <math>t=0</math> we have initial capital <math>W_0</math> to invest in <math>n</math> assets. Suppose further that we are allowed to rebalance our portfolio at times <math>t=1,\\dots,T-1</math> but without injecting additional cash into it. At each period <math>t</math> we make a decision about redistributing the current wealth <math>W_t</math> among the <math>n</math> assets. Let <math>x_0=(x_{10},\\dots,x_{n0})</math> be the initial amounts invested in the n assets. We require that each <math>x_{i0}</math> is nonnegative and that the balance equation <math>\\sum_{i=1}^{n}x_{i0}=W_0</math> should hold.\n\nConsider the total returns <math>\\xi_t=(\\xi_{1t},\\dots,\\xi_{nt})</math> for each period <math>t=1,\\dots,T</math>.  This forms a vector-valued random process <math>\\xi_1,\\dots,\\xi_T</math>. At time period <math>t=1</math>, we can rebalance the portfolio by specifying the amounts <math>x_1=(x_{11},\\dots,x_{n1})</math> invested in the respective assets. At that time the returns in the first period have been realized so it is reasonable to use this information in the rebalancing decision. Thus, the second-stage decisions, at time <math>t=1</math>, are actually functions of realization of the random vector <math>\\xi_1</math>, i.e., <math>x_1=x_1(\\xi_1)</math>. Similarly, at time <math>t</math> the decision <math>x_t=(x_{1t},\\dots,x_{nt})</math> is a function <math>x_t=x_t(\\xi_{[t]})</math> of the available information given by <math>\\xi_{[t]}=(\\xi_{1},\\dots,\\xi_{t})</math> the history of the random process up to time <math>t</math>. A sequence of functions <math>x_t=x_t(\\xi_{[t]})</math>, <math>t=0,\\dots,T-1</math>, with <math>x_0</math> being constant, defines an ''implementable policy'' of the decision process. It is said that such a policy is ''feasible'' if it satisfies the model constraints with probability 1, i.e., the nonnegativity  constraints <math>x_{it}(\\xi_{[t]})\\geq 0</math>, <math>i=1,\\dots,n</math>, <math>t=0,\\dots,T-1</math>, and the balance of wealth constraints,\n\n:<math>\n\\sum_{i=1}^{n}x_{it}(\\xi_{[t]}) = W_t,\n</math>\n\nwhere in period <math>t=1,\\dots,T</math> the wealth <math>W_t</math> is given by\n\n:<math>\nW_t = \\sum_{i=1}^{n}\\xi_{it} x_{i,t-1}(\\xi_{[t-1]}),\n</math>\n\nwhich depends on the realization of the random process and the decisions up to time <math>t</math>.\n\nSuppose the objective is to maximize the expected utility of this wealth at the last period, that is, to consider the problem\n\n:<math>\n\\max E[U(W_T)].\n</math>\n\nThis is a multistage stochastic programming problem, where stages are numbered from <math>t=0</math> to <math>t=T-1</math>. Optimization is performed over all implementable and feasible policies. To complete the problem description one also needs to define the probability distribution of the random process <math>\\xi_1,\\dots,\\xi_T</math>. This can be done in various ways. For example, one can construct a particular scenario tree defining time evolution of the process. If at every stage the random return of each asset is allowed to have two continuations, independent of other assets, then the total number of scenarios is <math>2^{nT}.</math>\n\nIn order to write [[dynamic programming]] equations, consider the above multistage problem backward in time. At the last stage <math>t=T-1</math>, a realization <math>\\xi_{[T-1]}=(\\xi_{1},\\dots,\\xi_{T-1})</math>  of the random process is known and <math>x_{T-2}</math> has been chosen. Therefore, one needs to solve the following problem\n\n:<math>\n\\begin{array}{lrclr}\n\\max\\limits_{x_{T-1}}   & E[U(W_T)|\\xi_{[T-1]}]    &   \\\\\n\\text{subject to} & W_T   &=&    \\sum_{i=1}^{n}\\xi_{iT}x_{i,T-1} \\\\\n                    &\\sum_{i=1}^{n}x_{i,T-1}&=&W_{T-1}\\\\\n\t\t    & x_{T-1}     &\\geq& 0\n\\end{array}\n</math>\n\nwhere <math>E[U(W_T)|\\xi_{[T-1]}]</math> denotes the conditional expectation of <math>U(W_T)</math> given <math>\\xi_{[T-1]}</math>. The optimal value of the above problem depends on <math>W_{T-1}</math> and <math>\\xi_{[T-1]}</math> and is denoted <math>Q_{T-1}(W_{T-1},\\xi_{[T-1]})</math>.\n\nSimilarly, at stages <math>t=T-2,\\dots,1</math>, one should solve the problem\n\n:<math>\n\\begin{array}{lrclr}\n\\max\\limits_{x_{t}}   & E[Q_{t+1}(W_{t+1},\\xi_{[t+1]})|\\xi_{[t]}]    &   \\\\\n\\text{subject to} & W_{t+1}   &=&    \\sum_{i=1}^{n}\\xi_{i,t+1}x_{i,t} \\\\\n                    &\\sum_{i=1}^{n}x_{i,t}&=&W_{t}\\\\\n\t\t    & x_{t}     &\\geq& 0\n\\end{array}\n</math>\n\nwhose optimal value is denoted by <math>Q_{t}(W_{t},\\xi_{[t]})</math>. Finally, at stage <math>t=0</math>, one solves the problem\n\n:<math>\n\\begin{array}{lrclr}\n\\max\\limits_{x_{0}}   & E[Q_{1}(W_{1},\\xi_{[1]})]    &   \\\\\n\\text{subject to} & W_{1}   &=&    \\sum_{i=1}^{n}\\xi_{i,1}x_{i0} \\\\\n                    &\\sum_{i=1}^{n}x_{i0}&=&W_{0}\\\\\n\t\t    & x_{0}     &\\geq& 0\n\\end{array}\n</math>\n\n=== Stagewise independent random process ===\n\nFor a general distribution of the process <math>\\xi_t</math>, it may be hard to solve these dynamic programming equations. The situation simplifies dramatically if the process <math>\\xi_t</math> is stagewise independent, i.e., <math>\\xi_t</math> is (stochastically) independent of <math>\\xi_1,\\dots,\\xi_{t-1}</math> for <math>t=2,\\dots,T</math>. In this case, the corresponding conditional expectations become unconditional expectations, and the function <math>Q_t(W_t)</math>, <math>t=1,\\dots,T-1</math> does not depend on <math>\\xi_{[t]}</math>. That is, <math>Q_{T-1}(W_{T-1})</math> is the optimal value of the problem\n\n:<math>\n\\begin{array}{lrclr}\n\\max\\limits_{x_{T-1}}   & E[U(W_T)]    &   \\\\\n\\text{subject to} & W_T   &=&    \\sum_{i=1}^{n}\\xi_{iT}x_{i,T-1} \\\\\n                    &\\sum_{i=1}^{n}x_{i,T-1}&=&W_{T-1}\\\\\n\t\t    & x_{T-1}     &\\geq& 0\n\\end{array}\n</math>\n\nand <math>Q_t(W_t)</math> is the optimal value of\n\n:<math>\n\\begin{array}{lrclr}\n\\max\\limits_{x_{t}}   & E[Q_{t+1}(W_{t+1})]    &   \\\\\n\\text{subject to} & W_{t+1}   &=&    \\sum_{i=1}^{n}\\xi_{i,t+1}x_{i,t} \\\\\n                    &\\sum_{i=1}^{n}x_{i,t}&=&W_{t}\\\\\n\t\t    & x_{t}     &\\geq& 0\n\\end{array}\n</math>\n\nfor <math>t=T-2,\\dots,1</math>.\n\n==Biological applications==\n[[Stochastic dynamic programming]] is frequently used to model [[ethology|animal behaviour]] in such fields as [[behavioural ecology]].<ref>Mangel, M. & Clark, C. W. 1988. ''Dynamic modeling in behavioral ecology.'' Princeton University Press {{ISBN|0-691-08506-4}}</ref><ref>Houston, A. I & McNamara, J. M. 1999. ''Models of adaptive behaviour: an approach based on state''. Cambridge University Press {{ISBN|0-521-65539-0}}</ref>  Empirical tests of models of [[Optimal foraging theory|optimal foraging]], [[Biological life cycle|life-history]] transitions such as [[Fledge|fledging in birds]] and egg laying in [[parasitoid]] wasps have shown the value of this modelling technique in explaining the evolution of behavioural decision making.  These models are typically many-staged, rather than two-staged.\n\n==Economic applications==\n[[Stochastic dynamic programming]] is a useful tool in understanding decision making under uncertainty. The accumulation of capital stock under uncertainty is one example; often it is used by resource economists to analyze [[Nicholas Georgescu-Roegen#Man.27s economic struggle and the social evolution of mankind .28bioeconomics.29|bioeconomic problems]]<ref>Howitt, R., Msangi, S., Reynaud, A and K. Knapp. 2002. [http://www.agecon.ucdavis.edu/aredepart/facultydocs/Howitt/Polyapprox3a.pdf \"Using Polynomial Approximations to Solve Stochastic Dynamic Programming Problems: or A \"Betty Crocker \" Approach to SDP.\"]  University of California, Davis, Department of Agricultural and Resource Economics Working Paper.</ref> where the uncertainty enters in such as weather, etc.\n\n==Software tools==\n\n===Modelling languages===\nAll discrete stochastic programming problems can be represented with any [[algebraic modeling language]], manually implementing explicit or implicit non-anticipativity to make sure the resulting model respects the structure of the information made available at each stage. \nAn instance of an SP problem generated by a general modelling language tends to grow quite large (linearly in the number of scenarios), and its matrix loses the structure that is intrinsic to this class of problems, which could otherwise be exploited at solution time by specific decomposition algorithms.\nExtensions to modelling languages specifically designed for SP are starting to appear, see:\n*[[AIMMS]] - supports the definition of SP problems\n*[[Extended Mathematical Programming (EMP)#EMP for Stochastic Programming|EMP SP]] (Extended Mathematical Programming for Stochastic Programming) - a module of [[General Algebraic Modeling System|GAMS]] created to facilitate stochastic programming (includes keywords for parametric distributions, chance constraints and risk measures such as  [[Value at risk]] and  [[Expected shortfall]]).\n*[[SAMPL]] - a set of extensions to [[AMPL]] specifically designed to express stochastic programs (includes syntax for chance constraints, integrated chance constraints and [[Robust optimization|Robust Optimization]] problems)\nThey both can generate SMPS instance level format, which conveys in a non-redundant form the structure of the problem to the solver.\n\n==See also==\n{{Portal|Computer science}}\n* [[Correlation gap]]\n* [[Extended Mathematical Programming (EMP)#EMP for Stochastic Programming|EMP for Stochastic Programming]]\n* [[Entropic value at risk]]\n* [[FortSP]]\n* [[SAMPL| SAMPL algebraic modeling language]]\n* [[Scenario optimization]]\n* [[Stochastic optimization]]\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n* John R. Birge and François V. Louveaux. ''[https://books.google.com/books?hl=en&lr=&id=Vp0Bp8kjPxUC&oi=fnd&pg=PR1&dq=%22Introduction+to+Stochastic+Programming%22&ots=q5DI3ZhhzB&sig=-RRlGMzT-vZVm6aNfHlO7oFIm2o#v=onepage&q=%22Introduction%20to%20Stochastic%20Programming%22&f=false Introduction to Stochastic Programming]''. Springer Verlag, New York, 1997.\n* {{cite book | last1=Kall|first1=Peter |last2=Wallace|first2=Stein W.| title=Stochastic programming |  series=Wiley-Interscience Series in Systems and Optimization| publisher=John Wiley & Sons, Ltd.| location=Chichester|year=1994|pages=xii+307|isbn=0-471-95158-7| url=http://stoprog.org/index.html?introductions.html |mr=1315300 }}\n* G. Ch. Pflug:  ''[https://books.google.com/books?hl=en&lr=&id=XJXhBwAAQBAJ&oi=fnd&pg=PR13&dq=%22Optimization+of+Stochastic+Models.+The+Interface+between+Simulation+and+Optimization%22&ots=6CUXWxyzWs&sig=OPvBdc_bmWUz-J1aPpLh1S-ngTQ#v=onepage&q=%22Stochastic%20programming%22&f=false Optimization of Stochastic Models. The Interface between Simulation and Optimization]''. Kluwer, Dordrecht, 1996.\n* [[András Prékopa]]. Stochastic Programming. Kluwer Academic Publishers, Dordrecht, 1995.\n* [[Andrzej Piotr Ruszczyński|Andrzej Ruszczynski]] and Alexander Shapiro (eds.) (2003) ''Stochastic Programming''. Handbooks in Operations Research and Management Science, Vol. 10, Elsevier.\n* {{cite book| last1=Shapiro|first1=Alexander|last2=[[Darinka Dentcheva|Dentcheva]]|first2=Darinka|last3=[[Andrzej Piotr Ruszczyński|Ruszczyński]]|first3=Andrzej|title=Lectures on stochastic programming: Modeling and theory| series=MPS/SIAM Series on Optimization| volume=9| publisher=Society for Industrial and Applied Mathematics (SIAM)|location=Philadelphia, PA |agency=Mathematical Programming Society (MPS)| year=2009|pages=xvi+436|isbn=978-0-89871-687-0| url=http://www2.isye.gatech.edu/people/faculty/Alex_Shapiro/SPbook.pdf | mr=2562798 }}\n* Stein W. Wallace and William T. Ziemba (eds.) (2005) ''Applications of Stochastic Programming''. MPS-SIAM Book Series on Optimization 5\n* {{cite book | last1=King|first1=Alan J.|last2=Wallace|first2=Stein W.| title=Modeling with Stochastic Programming |  series=Springer Series in Operations Research and Financial Engineering| publisher=Springer| location=New York|year=2012|isbn=978-0-387-87816-4| url=https://www.springer.com/mathematics/probability/book/978-0-387-87816-4 }}\n\n==External links==\n* [http://stoprog.org Stochastic Programming Community Home Page]\n\n{{DEFAULTSORT:Stochastic Programming}}\n[[Category:Stochastic optimization]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Subgradient method",
      "url": "https://en.wikipedia.org/wiki/Subgradient_method",
      "text": "'''Subgradient methods''' are [[iterative method]]s for solving [[convex optimization|convex minimization]] problems. Originally developed by [[Naum Z. Shor]] and others in the 1960s and 1970s, subgradient methods are convergent when applied even to a non-differentiable objective function. When the objective function is differentiable, sub-gradient methods for unconstrained problems use the same search direction as the method of [[gradient descent|steepest descent]].\n\nSubgradient methods are slower than Newton's method when applied to minimize twice continuously differentiable convex functions. However, Newton's method fails to converge on problems that have non-differentiable kinks. \n\nIn recent years, some [[interior-point methods]] have been suggested for convex minimization problems, but subgradient projection methods and related bundle methods of descent remain competitive. For convex minimization problems with very large number of dimensions, subgradient-projection methods are suitable, because they require little storage.   \n\nSubgradient projection methods are often applied to large-scale problems with decomposition techniques. Such decomposition methods often allow a simple distributed method for a problem.\n\n==Classical subgradient rules==\n\nLet <math>f:\\mathbb{R}^n \\to \\mathbb{R}</math> be a [[convex function]] with domain <math>\\mathbb{R}^n</math>.  A  classical subgradient method iterates\n:<math>x^{(k+1)} = x^{(k)} - \\alpha_k g^{(k)} \\ </math>\nwhere <math>g^{(k)}</math> denotes a [[subgradient]] of <math> f \\ </math> at <math>x^{(k)} \\ </math>, and <math>x^{(k)}</math> is the <math>k^{th}</math> iterate of <math>x</math>.  If  <math>f \\ </math> is differentiable, then its only subgradient is the gradient vector <math>\\nabla f</math> itself.\nIt may happen that <math>-g^{(k)}</math> is not a descent direction for <math>f \\ </math>  at <math>x^{(k)}</math>.  We therefore maintain a list <math>f_{\\rm{best}} \\ </math> that keeps track of the lowest objective function value found so far, i.e.\n:<math>f_{\\rm{best}}^{(k)} = \\min\\{f_{\\rm{best}}^{(k-1)} , f(x^{(k)}) \\}.</math>\n\n===Step size rules===\n\nMany different types of step-size rules are used by subgradient methods.  This article notes five classical step-size rules for which convergence [[mathematical proof|proof]]s are known:\n\n*Constant step size, <math>\\alpha_k = \\alpha.</math>\n*Constant step length, <math>\\alpha_k = \\gamma/\\lVert g^{(k)} \\rVert_2</math>, which gives <math>\\lVert x^{(k+1)} - x^{(k)} \\rVert_2 = \\gamma.</math>\n*Square summable but not summable step size, i.e. any step sizes satisfying\n:<math>\\alpha_k\\geq0,\\qquad\\sum_{k=1}^\\infty \\alpha_k^2 < \\infty,\\qquad \\sum_{k=1}^\\infty \\alpha_k = \\infty.</math>\n*Nonsummable diminishing, i.e. any step sizes satisfying\n:<math>\\alpha_k\\geq0,\\qquad \\lim_{k\\to\\infty} \\alpha_k = 0,\\qquad \\sum_{k=1}^\\infty \\alpha_k = \\infty.</math>\n*Nonsummable diminishing step lengths, i.e. <math>\\alpha_k = \\gamma_k/\\lVert g^{(k)} \\rVert_2</math>, where\n:<math>\\gamma_k\\geq0,\\qquad \\lim_{k\\to\\infty} \\gamma_k = 0,\\qquad \\sum_{k=1}^\\infty \\gamma_k = \\infty.</math>\nFor all five rules, the step-sizes are determined \"off-line\", before the method is iterated; the step-sizes do not depend on preceding iterations.  This \"off-line\" property of subgradient methods differs from the \"on-line\" step-size rules used for descent methods for differentiable functions: Many methods for minimizing differentiable functions satisfy Wolfe's sufficient conditions for convergence, where step-sizes typically depend on the current point and the current search-direction. An extensive discussion of stepsize rules for subgradient methods, including incremental versions, is given in the books by Bertsekas\n<ref>\n{{cite book\n  | last = Bertsekas\n  | first = Dimitri P.\n  | authorlink = Dimitri P. Bertsekas\n  | title = Convex Optimization Algorithms\n  | edition = Second\n  | publisher = Athena Scientific\n  | year = 2015\n  | location = Belmont, MA.\n  | isbn =  978-1-886529-28-1  \n}} \n</ref>\nand by Bertsekas, Nedic, and Ozdaglar. \n<ref>\n{{cite book\n|last1=Bertsekas\n|first1=Dimitri P.\n|last2=Nedic\n|first2=Angelia\n|last3 = Ozdaglar\n|first3 = Asuman\n| title = Convex Analysis and Optimization\n| edition = Second\n| publisher = Athena Scientific\n| year = 2003\n| location = Belmont, MA.\n| isbn =  1-886529-45-0  \n}} \n</ref>\n===Convergence results===\n\nFor constant step-length and scaled subgradients having [[Euclidean norm]] equal to one, the subgradient method converges to an arbitrarily close approximation to the minimum value, that is\n:<math>\\lim_{k\\to\\infty} f_{\\rm{best}}^{(k)} - f^* <\\epsilon</math> by a result of [[Naum Z. Shor|Shor]].<ref>\nThe approximate convergence of the constant step-size (scaled) subgradient method is stated as Exercise 6.3.14(a) in [[Dimitri P. Bertsekas|Bertsekas]] (page 636): {{cite book\n  | last = Bertsekas\n  | first = Dimitri P.\n  | authorlink = Dimitri P. Bertsekas\n  | title = Nonlinear Programming\n  | edition = Second\n  | publisher = Athena Scientific\n  | year = 1999\n  | location = Cambridge, MA.\n  | isbn = 1-886529-00-0 \n}} On page 636, Bertsekas attributes this result to Shor: {{cite book\n  | last = Shor\n  | first = Naum Z.\n  | authorlink = Naum Z. Shor\n  | title = Minimization Methods for Non-differentiable Functions\n  | publisher = [[Springer-Verlag]]\n  | isbn = 0-387-12763-1\n  | year = 1985\n}}\n</ref>\nThese classical subgradient methods have poor performance and are no longer recommended for general use.<ref name=\"Lem\"/><ref name=\"KLL\"/> However, they are still used widely in specialized applications because they are simple and they can be easily adapted to take advantage of the special structure of the problem at hand.\n\n==Subgradient-projection & bundle methods==\nDuring the 1970s, [[Claude Lemaréchal]] and Phil. Wolfe proposed \"bundle methods\" of descent for problems of convex minimization.<ref>\n{{cite book\n  | last = Bertsekas\n  | first = Dimitri P.\n  | authorlink = Dimitri P. Bertsekas\n  | title = Nonlinear Programming\n  | edition = Second\n  | publisher = Athena Scientific\n  | year = 1999\n  | location = Cambridge, MA.\n  | isbn = 1-886529-00-0 \n}} \n</ref> The meaning of the term \"bundle methods\" has changed significantly since that time. Modern versions and full convergence analysis were provided by Kiwiel.\n<ref>\n{{cite book|last=Kiwiel|first=Krzysztof|title=Methods of Descent for Nondifferentiable Optimization|publisher=[[Springer Verlag]]|location=Berlin|year=1985|pages=362|isbn=978-3540156420 |mr=0797754}}\n</ref> Contemporary bundle-methods often use \"[[level set|level]] control\" rules for choosing step-sizes, developing techniques from the \"subgradient-projection\" method of Boris T. Polyak (1969). However, there are problems on which bundle methods offer little advantage over subgradient-projection methods.<ref name=\"Lem\">\n{{cite book| last=Lemaréchal|first=Claude|authorlink=Claude Lemaréchal|chapter=Lagrangian relaxation|pages=112–156|title=Computational combinatorial optimization: Papers from the Spring School held in Schloß Dagstuhl, May 15–19, 2000|editor=Michael Jünger and Denis Naddef|series=Lecture Notes in Computer Science|volume=2241|publisher=Springer-Verlag| location=Berlin|year=2001|isbn=3-540-42877-1|mr=1900016|doi=10.1007/3-540-45586-8_4|ref=harv}}</ref><ref name=\"KLL\">\n{{cite journal|last1=Kiwiel|first1=Krzysztof&nbsp;C.|last2=Larsson |first2=Torbjörn|last3=Lindberg|first3=P.&nbsp;O.|title=Lagrangian relaxation via ballstep subgradient methods|url=http://mor.journal.informs.org/cgi/content/abstract/32/3/669 |journal=Mathematics of Operations Research|volume=32|date=August 2007|number=3|pages=669–686|mr=2348241|doi=10.1287/moor.1070.0261|ref=harv}}\n</ref>\n\n==Constrained optimization==\n===Projected subgradient===\nOne extension of the subgradient method is the '''projected subgradient method''', which solves the constrained optimization problem\n:minimize <math>f(x) \\ </math> subject to\n:<math>x\\in\\mathcal{C}</math>\n\nwhere <math>\\mathcal{C}</math> is a [[convex set]].  The projected subgradient method uses the iteration\n\n:<math>x^{(k+1)} = P \\left(x^{(k)} - \\alpha_k g^{(k)} \\right) </math>\n\nwhere <math>P</math> is projection on <math>\\mathcal{C}</math> and <math>g^{(k)}</math> is any subgradient of <math>f \\ </math> at <math>x^{(k)}.</math>\n\n===General constraints===\n\nThe subgradient method can be extended to solve the inequality constrained problem\n\n:minimize <math>f_0(x) \\ </math> subject to\n:<math>f_i (x) \\leq 0,\\quad i = 1,\\dots,m</math>\n\nwhere <math>f_i</math> are convex.  The algorithm takes the same form as the unconstrained case\n\n:<math>x^{(k+1)} = x^{(k)} - \\alpha_k g^{(k)} \\ </math>\n\nwhere <math>\\alpha_k>0</math> is a step size, and <math>g^{(k)}</math> is a subgradient of the objective or one of the constraint functions at <math>x. \\ </math>  Take\n\n:<math>g^{(k)} = \n\\begin{cases} \n  \\partial f_0 (x)  & \\text{ if } f_i(x) \\leq 0 \\; \\forall i = 1 \\dots m \\\\\n  \\partial f_j (x)  & \\text{ for some } j \\text{ such that } f_j(x) > 0 \n\\end{cases}</math>\n\nwhere <math>\\partial f</math> denotes the [[subdifferential]] of <math>f \\ </math>.  If the current point is feasible, the algorithm uses an objective subgradient; if the current point is infeasible, the algorithm chooses a subgradient of any violated constraint.\n\n==References==\n<references/> \n\n==Further reading==\n* {{cite book\n  | last = Bertsekas\n  | first = Dimitri P.\n  | authorlink = Dimitri P. Bertsekas\n  | title = Nonlinear Programming\n  | publisher = Athena Scientific\n  | year = 1999\n  | location = Belmont, MA.\n  | isbn = 1-886529-00-0 \n}}\n* {{cite book\n|last1=Bertsekas\n|first1=Dimitri P.\n|last2=Nedic\n|first2=Angelia\n|last3 = Ozdaglar\n|first3 = Asuman\n| title = Convex Analysis and Optimization\n| edition = Second\n| publisher = Athena Scientific\n| year = 2003\n| location = Belmont, MA.\n| isbn =  1-886529-45-0  \n}} \n* {{cite book\n  | last = Bertsekas\n  | first = Dimitri P.\n  | authorlink = Dimitri P. Bertsekas\n  | title = Convex Optimization Algorithms\n  | publisher = Athena Scientific\n  | year = 2015\n  | location = Belmont, MA.\n  | isbn = 978-1-886529-28-1 \n}}\n* {{cite book\n  | last = Shor\n  | first = Naum Z.\n  | authorlink = Naum Z. Shor\n  | title = Minimization Methods for Non-differentiable Functions\n  | publisher = [[Springer-Verlag]]\n  | isbn = 0-387-12763-1\n  | year = 1985\n}}\n\n* {{cite book|last=[[Andrzej Piotr Ruszczyński|Ruszczyński]]|first=Andrzej|title=Nonlinear Optimization|publisher=[[Princeton University Press]]|location=Princeton, NJ|year=2006|pages=xii+454|isbn=978-0691119151 |mr=2199043}}\n\n==External links==\n* [http://www.stanford.edu/class/ee364a/ EE364A] and [http://www.stanford.edu/class/ee364b/ EE364B], Stanford's convex optimization course sequence.\n{{optimization algorithms|convex}}\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Convex optimization]]"
    },
    {
      "title": "Successive linear programming",
      "url": "https://en.wikipedia.org/wiki/Successive_linear_programming",
      "text": "'''Successive Linear Programming''' ('''SLP'''), also known as '''Sequential Linear Programming''',  is an [[optimization]] technique for approximately solving [[nonlinear optimization]] problems.<ref>{{harv|Nocedal|Wright|2006|p=551}}</ref>\n\nStarting at some estimate of the optimal solution, the method is based on solving a sequence of first-order approximations (i.e. [[linearization]]s) of the model. The linearizations are linear programming problems, which can be solved efficiently. As the linearizations need not be bounded, [[trust region]]s or similar techniques are needed to ensure convergence in theory.  \n<ref>{{harv|Bazaraa |Sheraly|Shetty|1993|p=432}}</ref>\n\nSLP has been used widely in the [[petrochemical industry]] since the 1970s.\n<ref>{{harv|Palacios-Gomez|Lasdon|Enquist|October 1982}}</ref>\n\n==See also==\n* [[Sequential quadratic programming]]\n* [[Sequential linear-quadratic programming]]\n* [[Augmented Lagrangian method]]\n\n==References==\n{{Reflist}}\n\n==Sources==\n* {{Cite book | last1=Nocedal | first1=Jorge | last2=Wright | first2=Stephen J. | title=Numerical Optimization | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=2nd | isbn=978-0-387-30303-1 | year=2006 | ref=harv | postscript=<!--None-->}}\n* {{Cite book | last1=Bazaraa | first1=Mokhtar S. | last2=Sherali | first2=Hanif D. | last3=Shetty | first3=C.M. | title=Nonlinear Programming, Theory and Applications | publisher=[[John Wiley & Sons]] | edition=2nd | isbn=0-471-55793-5 | year=1993 |ref=harv }}\n* {{Cite article | last1=Palacios-Gomez| first1=F. | last2=Lasdon| first2=L. | last3=Enquist| first3=M. | title=Nonlinear Optimization by Successive Linear Programming | journal=Management Science | volume=28 | issue = 10 |date=October 1982 | doi=10.1287/mnsc.28.10.1106 |ref=harv }}\n\n{{optimization algorithms}}\n\n[[Category:Optimization algorithms and methods]]\n\n\n{{algorithm-stub}}"
    },
    {
      "title": "Symmetric rank-one",
      "url": "https://en.wikipedia.org/wiki/Symmetric_rank-one",
      "text": "The '''Symmetric Rank 1''' ('''SR1''') method is a [[quasi-Newton method]] to update the second derivative (Hessian)\nbased on the derivatives (gradients) calculated at two points. It is a generalization to the [[secant method]] for a multidimensional problem.\nThis update maintains the ''symmetry'' of the matrix but does ''not'' guarantee that the update be [[positive definite matrix|''positive definite'']].\n\nThe sequence of Hessian approximations generated by the SR1 method converges to the true Hessian under mild conditions, in theory; in practice, the approximate Hessians generated by the SR1 method show faster progress towards the true Hessian than do popular alternatives ([[BFGS]] or [[Davidon-Fletcher-Powell formula|DFP]]), in preliminary numerical experiments.<ref name=\"CGT\">{{harvtxt|Conn|Gould|Toint|1991}}</ref> The SR1 method has computational advantages for [[sparsity|sparse]] or [[partial separability|partially separable]] problems.\n\nA twice continuously differentiable function <math>x \\mapsto f(x)</math> has a [[gradient]] (<math>\\nabla f</math>) and [[Hessian matrix]] <math>B</math>: The function <math>f</math> has an expansion as a [[Taylor series]] at <math>x_0</math>, which can be truncated\n::<math>f(x_0+\\Delta x) \\approx f(x_0)+\\nabla f(x_0)^T \\Delta x+\\frac{1}{2} \\Delta x^T {B} \\Delta x </math>;\nits gradient has a Taylor-series approximation also\n::<math>\\nabla f(x_0+\\Delta x) \\approx \\nabla f(x_0)+B \\Delta x</math>,\nwhich is used to update <math>B</math>.  The above secant-equation need not have a unique solution  <math>B</math>.\nThe SR1 formula computes (via an update of [[Rank (linear algebra)|rank]] 1) the symmetric solution that is closest to the current approximate-value  <math>B_k</math>:\n::<math>B_{k+1}=B_{k}+\\frac {(y_k-B_k \\Delta x_k) (y_k-B_k \\Delta x_k)^T}{(y_k-B_k \\Delta x_k)^T \\Delta x_k}</math>,\nwhere\n::<math>y_k=\\nabla f(x_k+\\Delta x_k)-\\nabla f(x_k)</math>.\nThe corresponding update to the approximate inverse-Hessian <math>H_k=B_k^{-1}</math> is\n::<math>H_{k+1}=H_{k}+\\frac {(\\Delta x_k-H_k y_k)(\\Delta x_k-H_k y_k)^T}{(\\Delta x_k-H_k y_k)^T y_k}</math>.\n\nThe SR1 formula has been rediscovered a number of times. A drawback is that the denominator can vanish. Some authors have suggested that the update be applied only if\n::<math>|\\Delta x_k^T (y_k-B_k \\Delta x_k)|\\geq r \\|\\Delta x_k\\|\\cdot \\|y_k-B_k \\Delta x_k\\| </math>,\nwhere <math>r\\in(0,1)</math> is a small number, e.g. <math>10^{-8}</math>.<ref>{{harvtxt|Nocedal|Wright|1999}}</ref>\n\n==See also==\n* [[Quasi-Newton method]]\n* [[Newton's method in optimization]]\n* [[BFGS method|Broyden-Fletcher-Goldfarb-Shanno (BFGS) method]]\n* [[L-BFGS|L-BFGS method]]\n\n==Notes==\n<references/>\n\n==References==\n* Byrd, Richard H. (1996) Analysis of a Symmetric Rank-One Trust Region Method. ''SIAM Journal on Optimization'' 6(4)\n* {{cite journal|last1=Conn|first1=A. R.|last2=\nGould|first2=N. I. M.|last3=Toint|first3=Ph. L.|title=Convergence of quasi-Newton matrices generated by the symmetric rank one update|journal=Mathematical Programming|date=March 1991|publisher=Springer Berlin/ Heidelberg|\nissn=0025-5610|pages=177–195|volume=50|number=1|doi=10.1007/BF01594934|ref=harv|id=[ftp://ftp.numerical.rl.ac.uk/pub/nimg/pubs_old/ConnGoulToin91_mp.pdf PDF file at Nick Gould's website]|mr=}}\n* Khalfan, H. Fayez (1993) A Theoretical and Experimental Study of the Symmetric Rank-One Update. ''SIAM Journal on Optimization'' 3(1)\n* Nocedal, Jorge & Wright, Stephen J. (1999). ''Numerical Optimization''. Springer-Verlag. {{isbn|0-387-98793-2}}.\n\n{{Optimization algorithms|unconstrained}}\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Ternary search",
      "url": "https://en.wikipedia.org/wiki/Ternary_search",
      "text": "A '''ternary search algorithm''' is a technique in [[computer science]] for finding the [[maxima and minima|minimum or maximum]] of a [[Unimodality|unimodal]] function. A ternary search determines either that the minimum or maximum cannot be in the first third of the domain or that it cannot be in the last third of the domain, then repeats on the remaining two thirds. A ternary search is an example of a [[divide and conquer algorithm]] (see [[search algorithm]]).\n\n== The function ==\n\n== Algorithm ==\n\nLet {{math|1=''f''(''x'')}} be a [[unimodal]] function on some interval  [''l''; ''r'']. Take any two points {{math|1=''m''<sub>1</sub>}} and {{math|1=''m''<sub>2</sub>}} in this segment: {{math|1=''l'' < ''m''<sub>1</sub> < ''m''<sub>2</sub> < ''r''}}. Then there are three possibilities:\n* if {{math|1=''f''(''m''<sub>1</sub>) < ''f''(''m''<sub>2</sub>)}}, then the required maximum can not be located on the left side - {{math|1=[''l''; ''m''<sub>1</sub>]}}. It means that the maximum further makes sense to look only in the interval {{math|1=[''m''<sub>1</sub>;''r'']}}\n* if {{math|1=''f''(''m''<sub>1</sub>) > ''f''(''m''<sub>2</sub>)}}, that the situation is similar to the previous, up to symmetry. Now, the required maximum can not be in the right side - {{math|1=[''m''<sub>2</sub>; ''r'']}}, so go to the segment {{math|1=[''l''; ''m''<sub>2</sub>]}}\n* if {{math|1=''f''(''m''<sub>1</sub>) = f(''m''<sub>2</sub>)}}, then the search should be conducted in {{math|1=[''m''<sub>1</sub>; ''m''<sub>2</sub>]}}, but this case can be attributed to any of the previous two (in order to simplify the code). Sooner or later the length of the segment will be a little less than a predetermined constant, and the process can be stopped.\nchoice points {{math|1=''m''<sub>1</sub>}} and {{math|1=''m''<sub>2</sub>}}: \n* {{math|1=''m''<sub>1</sub> = ''l'' + (''r''-''l'')/3}}\n* {{math|1=''m''<sub>2</sub> = ''r'' - (''r''-''l'')/3}}\n\n; Run time order \n: <math>T(n) = T(2n/3) + 1\n            = \\Theta(\\log n)</math>\n\n=== Recursive algorithm ===\n<source lang=\"python\">\ndef ternarySearch(f, left, right, absolutePrecision):\n    '''\n    left and right are the current bounds; \n    the maximum is between them\n    '''\n    if abs(right - left) < absolutePrecision:\n        return (left + right)/2\n\n    leftThird = (2*left + right)/3\n    rightThird = (left + 2*right)/3\n\n    if f(leftThird) < f(rightThird):\n        return ternarySearch(f, leftThird, right, absolutePrecision) \n    else:\n        return ternarySearch(f, left, rightThird, absolutePrecision)\n</source>\n\n=== Iterative algorithm ===\n\n<source lang=\"python\">\ndef ternarySearch(f, left, right, absolutePrecision):\n    \"\"\"\n    Find maximum of unimodal function f() within [left, right]\n    To find the minimum, reverse the if/else statement or reverse the comparison.\n    \"\"\"\n    while True:\n        #left and right are the current bounds; the maximum is between them\n        if abs(right - left) < absolutePrecision:\n            return (left + right)/2\n\n        leftThird = left + (right - left)/3\n        rightThird = right - (right - left)/3\n\n        if f(leftThird) < f(rightThird):\n            left = leftThird\n        else:\n            right = rightThird\n</source>\n\n==See also==\n*[[Newton's method in optimization]] (can be used to search for where the derivative is zero)\n*[[Golden-section search]] (similar to ternary search, useful if evaluating f takes most of the time per iteration)\n*[[Binary search algorithm]] (can be used to search for where the derivative changes in sign)\n*[[Interpolation search]]\n*[[Exponential search]]\n*[[Linear search]]\n* [https://github.com/Dronee/vector3 N Dimensional Ternary Search Implementation]\n\n==References==\n{{Unreferenced|date=May 2007}}\n\n[[Category:Search algorithms]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "TOLMIN (optimization software)",
      "url": "https://en.wikipedia.org/wiki/TOLMIN_%28optimization_software%29",
      "text": "{{about|TOLMIN, an optimization algorithm/software by [[Michael J. D. Powell]]|other uses of the word \"Tolmin\"|Tolmin (disambiguation)}}\n\n'''TOLMIN''' <ref>{{cite journal|last=Powell|first=M. J. D. |title=A tolerant algorithm for linearly constrained optimization calculations |journal=Mathematical Programming |publisher= Springer |year=1989 |volume=45 |pages=547–566|doi=10.1007/BF01589118}}</ref><ref name=\"code\">{{cite web|url=http://mat.uc.pt/~zhang/software.html#tolmin |title=Source code of TOLMIN software |publisher= |date= |accessdate=2015-04-06}}</ref> is a [[numerical analysis|numerical]] [[optimization (mathematics)|optimization]] [[algorithm]] by [[Michael J. D. Powell]]. It is also the name of [[Michael J. D. Powell|Powell]]'s [[Fortran#FORTRAN 77|Fortran 77]] implementation of the algorithm.\n\nTOLMIN seeks the minimum of a differentiable nonlinear function subject to linear constraints (equality and/or inequality) and simple bounds on variables. Each search direction is calculated so that it does not intersect the boundary of any inequality constraint that is satisfied and that has a \"small\" residual at the beginning of the line search. The meaning of \"small\" depends on a parameter called TOL which is automatically adjusted, and which gives the name of the software.\n\nFeatures of the software include: quadratic approximations of the objective function whose second derivative matrices are updated by means of the [[BFGS]] formula, active sets technique, primal-dual quadratic programming procedure for calculation of the search direction.\n\nThe TOLMIN software is distributed under [[GNU Lesser General Public License|The GNU Lesser General Public License]] (LGPL).<ref name=\"code\"/>\n\n==See also==\n*[[COBYLA]]\n*[[UOBYQA]]\n*[[NEWUOA]]\n*[[BOBYQA]]\n*[[LINCOA]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [https://ccpforge.cse.rl.ac.uk/gf/project/powell Optimization software by Professor M. J. D. Powell at CCPForge] \n* [http://mat.uc.pt/~zhang/software.html#powell_software A repository of Professor M. J. D. Powell's software]\n* [http://camo.ici.ro/nonlin/tolmin.htm Nonlinear Programming Packages --- TOLMIN]\n\n{{optimization algorithms}}\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Tree rearrangement",
      "url": "https://en.wikipedia.org/wiki/Tree_rearrangement",
      "text": "'''Tree rearrangements''' are used in [[heuristic]] [[algorithm]]s devoted to searching for an [[Optimization (mathematics)|optimal]] [[tree structure]]. They can be applied to any set of data that are naturally arranged into a tree, but have most applications in [[computational phylogenetics]], especially in [[maximum parsimony]] and [[Maximum likelihood estimation with flow data|maximum likelihood]] searches of [[phylogenetic tree]]s, which seek to identify one among many possible trees that best explains the [[evolution]]ary history of a particular [[gene]] or [[species]].\n\n==Basic tree rearrangements==\n<gallery>\nImage:NNI.svg|Nearest neighbor interchange (NNI)\nImage:SPR.svg|Subtree pruning and regrafting (SPR)\nImage:TBR.svg|Tree bisection and reconnection (TBR)\n</gallery>\n\nThe simplest tree-rearrangement, known as '''nearest-neighbor interchange''', exchanges the connectivity of four subtrees within the main tree. Because there are three possible ways of connecting four subtrees,<ref name=\"felsenstein\">Felsenstein J. (2004). ''Inferring Phylogenies'' Sinauer Associates: Sunderland, MA.</ref> and one is the original connectivity, each interchange creates two new trees. Exhaustively searching the possible nearest-neighbors for each possible set of subtrees is the slowest but most optimizing way of performing this search. An alternative, more wide-ranging search, '''subtree pruning and regrafting''' (SPR), selects and removes a subtree from the main tree and reinserts it elsewhere on the main tree to create a new node. Finally, '''tree bisection and reconnection''' (TBR) detaches a subtree from the main tree at an interior node and then attempts all possible connections between edges of the two trees thus created. The increasing complexity of the tree rearrangement technique correlates with increasing computational time required for the search, although not necessarily with their performance.<ref name=\"takahashi\">Takahashi K, Nei M. (2000). Efficiencies of fast algorithms of phylogenetic inference under the criteria of maximum parsimony, minimum evolution, and maximum likelihood when a large number of sequences are used. ''Mol Biol Evol'' 17(8):1251-8.</ref>\n\nSPR can be further divided into uSPR: Unrooted SPR, rSPR: Rooted SPR. uSPR is applied to unrooted trees, and goes like this: break any edge. Join one end of the edge (selected arbitrarily) to any other edge in the tree. rSPR is applied to rooted trees*, and goes: break any edge except the edge leading to the root node. Join one end of the edge (specifically: the end of the edge that is FURTHEST from the root) and attach it to any other edge of the tree.<ref name=bs05> Bordewich M, Semple C. 2005. On the computational complexity of the rooted subtree prune and regraft distance Ann. Comb. 8:409–23</ref>\n\n<nowiki>*</nowiki> In this example the root of the tree is marked by a node of degree one, meaning that all nodes in the tree have either degree 1 or degree 3.  An alternative approach, used in Bordewich and Semple, is to consider the root node to have degree 2, and to have a special rule for rSPR.\n\nThe number of SPR<ref>WHIDDEN, C., BEIKO, R. G. and ZEH, N. 2016. Fixed-Parameter and Approximation Algorithms for Maximum [[Agreement Forest]]s of Multifurcating Trees. Algorithmica, 74, 1019–1054</ref> or TBR<ref>CHEN, J., FAN, J.-H. and SZE, S.-H. 2015. Parameterized and approximation algorithms for maximum agreement forest in multifurcating trees. Theoretical Computer Science, 562, 496–512.</ref> moves needed to get from one tree to another can be calculated by producing a Maximum Agreement Forest comprising (respectively) rooted or unrooted trees. This problem is NP hard but Fixed Parameter Tractable.\n\n==Tree fusion==\nThe simplest type of tree fusion begins with two trees already identified as near-optimal; thus, they most likely have the majority of their nodes correct but may fail to resolve individual tree \"leaves\" properly; for example, the separation ((A,B),(C,D)) at a branch tip versus ((A,C),(B,D)) may be unresolved.<ref name=\"felsenstein\" /> Tree fusion swaps these two solutions between two otherwise near-optimal trees. Variants of the method use standard [[genetic algorithm]]s with a defined [[objective function]] to swap high-scoring subtrees into main trees that are high-scoring overall.<ref name=\"matsuda\">Matsuda H. (1996). Protein phylogenetic inference using maximum likelihood with a genetic algorithm. ''Pacific Symposium on Biocomputing 1996'', pp512-23.</ref>\n\n== Sectorial search ==\nAn alternative strategy is to detach part of the tree (which can be selected at random, or using a more strategic approach) and to perform TBR/SPR/NNI on this sub-tree. This optimized sub-tree can then be replaced on the main tree, hopefully improving the p-score.<ref name=Goloboff1999>Goloboff, P. (1999). Analyzing Large Data Sets in Reasonable Times: Solutions for Composite Optima. Cladistics, 15(4), 415–428. doi:10.1006/clad.1999.0122</ref>\n\n== Tree drifting ==\nTo avoid entrapment in local optima, a 'simulated annealing' approach can be used, whereby the algorithm is occasionally permitted to entertain sub-optimal candidate trees, with a probability related to how far they are from the optimum.<ref name=Goloboff1999/>\n\n== Tree fusing ==\nOnce a range of equally-optimal trees have been gathered, it is often possible to find a better tree by combining the \"good bits\" of separate trees. Sub-groups with an identical composition but different topology can be switched and the resultant trees evaluated.<ref name=Goloboff1999/>\n\n==References==\n<references />\n\n[[Category:Phylogenetics]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Trees (data structures)]]"
    },
    {
      "title": "Truncated Newton method",
      "url": "https://en.wikipedia.org/wiki/Truncated_Newton_method",
      "text": "'''Truncated Newton methods''', also known as '''Hessian-free optimization''',<ref name=\"martens\">{{cite conference |last=Martens |first=James |title=Deep learning via Hessian-free optimization |url=http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf |conference=Proc. [[International Conference on Machine Learning]] |year=2010}}</ref> are a family of [[optimization algorithm]]s designed for optimizing non-linear functions with large numbers of [[independent variable]]s. A truncated Newton method consists of repeated application of an iterative optimization algorithm to approximately solve [[Newton's method in optimization|Newton's equations]], to determine an update to the function's parameters. The inner solver is ''truncated'', i.e., run for only a limited number of iterations. It follows that, for truncated Newton methods to work, the inner solver needs to produce a good approximation in a finite number of iterations;<ref name=\"nash\">{{cite journal |first=Stephen G. |last=Nash |doi=10.1016/S0377-0427(00)00426-X |title=A survey of truncated-Newton methods |journal=Journal of Computational and Applied Mathematics\n|volume=124 |issue=1–2 |year=2000 |pages=45–59}}</ref> [[conjugate gradient]] has been suggested and evaluated as a candidate inner loop.{{r|martens}} Another prerequisite is good [[preconditioning]] for the inner algorithm.<ref>{{cite journal |first1=Stephen G. |last1=Nash |title=Preconditioning of truncated-Newton methods |journal=SIAM J. Sci. Stat. Comput. |volume=6 |issue=3 |year=1985 |pages=599–616}}</ref>\n\n==References==\n{{reflist}}\n\n==Further reading==\n* {{cite journal |first1=L. |last1=Grippo |first2=F. |last2=Lampariello |first3=S. |last3=Lucidi |title=A Truncated Newton Method with Nonmonotone Line Search for Unconstrained Optimization |journal=J. Optimization Theory and Applications |volume=60 |issue=3 |year=1989 |citeseerx=10.1.1.455.7495}}\n* {{cite journal |first1=Stephen G. |last1=Nash |first2=Jorge |last2=Nocedal |title=A numerical study of the limited memory BFGS method and the truncated-Newton method for large scale optimization |journal=SIAM J. Optim. |volume=1 |issue=3 |pages=358–372 |year=1991 |citeseerx=10.1.1.474.3400}}\n\n{{optimization algorithms}}\n{{Isaac Newton}}\n\n[[Category:Optimization algorithms and methods]]\n\n\n{{applied-math-stub}}"
    },
    {
      "title": "Trust region",
      "url": "https://en.wikipedia.org/wiki/Trust_region",
      "text": "In [[mathematical optimization]], a '''trust region''' is the subset of the region of the [[objective function]] that is approximated using a model function (often a [[quadratic function|quadratic]]). If an adequate model of the objective function is found within the trust region, then the region is expanded; conversely, if the approximation is poor, then the region is contracted. Trust-region methods are also known as '''restricted-step methods'''.\n\nThe fit is evaluated by comparing the ratio of expected improvement from the model approximation with the actual improvement observed in the objective function. Simple thresholding of the ratio is used as the criterion for expansion and contraction—a model function is \"trusted\" only in the region where it provides a reasonable approximation.\n\nTrust-region methods are in some sense dual to [[line-search]] methods: trust-region methods first choose a step size (the size of the trust region) and then a step direction, while line-search methods first choose a step direction and then a step size.\n\nThe earliest use of the term seems to be by Sorensen (1982).\n\n==Example==\n\nConceptually, in the [[Levenberg–Marquardt algorithm]], the objective function is iteratively approximated by a [[quadratic surface]], then using a linear solver, the estimate is updated. This alone may not converge nicely if the initial guess is too far from the optimum. For this reason, the algorithm instead restricts each step, preventing it from stepping \"too far\". It operationalizes \"too far\" as follows. Rather than solving <math>A \\, \\Delta x = b</math> for <math>\\Delta x</math>, it solves <math>\\big(A + \\lambda \\operatorname{diag}(A)\\big) \\, \\Delta x = b</math>, where <math>\\operatorname{diag}(A)</math> is the diagonal matrix with the same diagonal as ''A'', and λ is a parameter that controls the trust-region size. Geometrically, this adds a paraboloid centered at <math>\\Delta x = 0</math> to the [[quadratic form]], resulting in a smaller step. \n\nThe trick is to change the trust-region size (λ). At each iteration, the damped quadratic fit predicts a certain reduction in the cost function, <math>\\Delta f_\\text{pred}</math>, which we would expect to be a smaller reduction than the true reduction. Given <math>\\Delta x</math>, we can evaluate\n: <math>\\Delta f_\\text{actual} = f(x) - f(x + \\Delta x).</math>\nBy looking at the ratio <math>\\Delta f_\\text{pred}/\\Delta f_\\text{actual}</math>, we can adjust the trust-region size. In general, we expect <math>\\Delta f_\\text{pred}</math> to be a bit larger than <math>\\Delta f_\\text{actual}</math>, and so the ratio would be between, say, 0.25 and 0.5. If the ratio is more than 0.5, then we are damping the step much, so expand the trust region (decrease λ) and iterate. If the ratio is smaller than 0.25, then the true function is diverging \"too much\" from the trust-region approximation, so shrink the trust region (increase λ) and try again.\n\n==References==\n\n* Andrew R. Conn, Nicholas I. M. Gould, Philippe L. Toint \"[https://books.google.com/books?id=5kNC4fqssYQC Trust-Region Methods (MPS-SIAM Series on Optimization)]\".\n* Byrd, R. H, R. B. Schnabel, and G. A. Schultz. \"[http://epubs.siam.org/doi/abs/10.1137/0724076 A trust region algorithm for nonlinearly constrained optimization]\", SIAM J. Numer. Anal., 24 (1987), pp.&nbsp;1152–1170.\n* Sorensen, D. C.: \"[http://epubs.siam.org/doi/abs/10.1137/0719026 Newton’s Method with a Model Trust Region Modification]\", SIAM J. Numer. Anal., 19(2), 409–426 (1982).\n* Yuan, Y. \"[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.9964 A review of trust region algorithms for optimization]\" in ICIAM 99: Proceedings of the Fourth International Congress on Industrial & Applied Mathematics, Edinburgh, 2000  Oxford University Press, USA.\n* Yuan, Y. \"[https://link.springer.com/article/10.1007%2Fs10107-015-0893-2 Recent Advances in Trust Region Algorithms]\", Math. Program., 2015\n\n== External links ==\n* [http://www.applied-mathematics.net/optimization/optimizationIntro.html Kranf site: Trust Region Algorithms]\n* [https://optimization.mccormick.northwestern.edu/index.php/Trust-region_methods Trust-region methods]\n\n{{Optimization algorithms}}\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "UOBYQA",
      "url": "https://en.wikipedia.org/wiki/UOBYQA",
      "text": "'''UOBYQA''' ('''U'''nconstrained '''O'''ptimization '''BY''' '''Q'''uadratic '''A'''pproximation)<ref name=\"report\">{{Cite report |author= Powell, M. J. D. |date= December 2000 | title=UOBYQA: unconstrained optimization by quadratic approximation | url=http://www.damtp.cam.ac.uk/user/na/NA_papers/NA2000_14.ps.gz |publisher=Department of Applied Mathematics and Theoretical Physics, Cambridge University |docket=DAMTP 2000/NA14|accessdate= 2015-04-06}}</ref><ref>{{cite journal|last=Powell|first=M. J. D. |title=UOBYQA: unconstrained optimization by quadratic approximation |journal=Mathematical Programming, Series B |year=2002 |volume=92 |issue=3 |pages=555–582|doi=10.1007/s101070100290|citeseerx=10.1.1.28.1756 }}</ref><ref name=\"code\">{{cite web|url=http://mat.uc.pt/~zhang/software.html#uobyqa |title=Source code of UOBYQA software |publisher= |date= |accessdate=2015-04-06}}</ref> is a [[numerical analysis|numerical]] [[optimization (mathematics)|optimization]] [[algorithm]] by [[Michael J. D. Powell]]. It is also the name of [[Michael J. D. Powell|Powell]]'s [[Fortran#FORTRAN 77|Fortran 77]] implementation of the algorithm.\n\nUOBYQA solves unconstrained [[optimization (mathematics)|optimization]] problems without using [[derivative]]s, which makes it a [[derivative-free optimization|derivative-free]] algorithm. The algorithm is [[Iterative method|iterative]] and exploits [[trust region|trust-region]] technique. On each iteration, the algorithm establishes a quadratic model <math> Q_k </math> by [[interpolation|interpolating]] the [[Optimization problem|objective function]] at <math> (n+1)(n+2)/2 </math> points and then minimizes <math> Q_k </math> within a [[trust region]].\n\nAfter UOBYQA, Powell developed [[NEWUOA]], which also solves unconstrained optimization problems without using derivatives. In general, [[NEWUOA]] is much more efficient than UOBYQA and is capable of solving much larger problems (with up to several hundreds of variables). A major difference between them is that [[NEWUOA]] constructs quadratic models by [[interpolation|interpolating]] the [[Optimization problem|objective function]]  at much less than <math> (n+1)(n+2)/2 </math> points (<math> 2n+1 </math> by default<ref name=\"newuoacode\">{{cite web|url=http://mat.uc.pt/~zhang/software.html#newuoa |title=Source code of NEWUOA software |publisher= |date= |accessdate=2014-01-14}}</ref>). For general usage, [[NEWUOA]] is recommended to replace UOBYQA.\n\nThe UOBYQA software is distributed under [[GNU Lesser General Public License|The GNU Lesser General Public License]] (LGPL).<ref name=\"code\"/>\n\n==See also==\n* [[TOLMIN (optimization software)|TOLMIN]]\n* [[COBYLA]]\n* [[NEWUOA]]\n* [[BOBYQA]]\n* [[LINCOA]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [https://ccpforge.cse.rl.ac.uk/gf/project/powell Optimization software by Professor M. J. D. Powell at CCPForge] \n* [http://mat.uc.pt/~zhang/software.html#powell_software A repository of Professor M. J. D. Powell's software]\n\n{{optimization algorithms}}\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Very large-scale neighborhood search",
      "url": "https://en.wikipedia.org/wiki/Very_large-scale_neighborhood_search",
      "text": "{{context|date=February 2009}}\nIn [[mathematical optimization]], [[Local search (optimization)|neighborhood search]] is a technique that tries to find good or near-optimal solutions to a combinatorial optimisation problem by repeatedly transforming a current solution into a different solution in the [[Neighbourhood (mathematics)|neighborhood]] of the current solution. The neighborhood of a solution is a set of similar solutions obtained by relatively simple modifications to the original solution. For a '''very large-scale neighborhood search''', the neighborhood is large and possibly exponentially sized.\n\nThe resulting algorithms can outperform algorithms using small neighborhoods because the local improvements are larger. If neighborhood searched is limited to just one or a very small number of changes from the current solution, then it can be difficult to escape from local minima, even with additional meta-heuristic techniques such as [[Simulated Annealing]] or [[Tabu search]]. In large neighborhood search techniques, the possible changes from one solution to its neighbor may allow tens or hundreds of values to change, and this means that the size of the neighborhood may itself be sufficient to allow the search process to avoid or escape local minima, though additional meta-heuristic techniques can still improve performance.\n\n== References ==\n*{{citation\n | last1 = Ahuja | first1 = Ravindra K. | author1-link = Ravindra K. Ahuja\n | last2 = Orlin | first2 = James B. | author2-link = James B. Orlin\n | last3 = Sharma | first3 = Dushyant\n | doi = 10.1111/j.1475-3995.2000.tb00201.x\n | issue = 4–5\n | journal = International Transactions in Operational Research\n | pages = 301–317\n | title = Very large-scale neighborhood search\n | url = http://jorlin.scripts.mit.edu/docs/publications/79-very%20large%20scale%20neighb.pdf\n | volume = 7\n | year = 2000}}.\n\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Zadeh's rule",
      "url": "https://en.wikipedia.org/wiki/Zadeh%27s_rule",
      "text": "In [[mathematical optimization]], '''Zadeh's rule''' (also known as the '''least-entered rule''') is an algorithmic refinement of the [[simplex method]] for [[linear programming|linear optimization]].\n\nThe rule was proposed around 1980 by [[Lotfi A. Zadeh]], and has entered the folklore of convex optimization since then. <ref>{{cite journal|first1=Norman|last1=Zadeh|title=What is the worst case behaviour of the simplex algorithm?|journal=Technical report, Department of Operations Research, Stanford|date=1980}}</ref>\n\nZadeh offered a reward of $1,000 to anyone who can show that the rule admits polynomially many iterations or to prove that there is a family of linear programs on which the pivoting rule requires subexponentially many iterations to find the optimum.<ref>{{cite journal|first1=Günter|last1=Ziegler|title=Typical and extremal linear programs|journal= The Sharpest Cut (MPS-Siam Series on Optimization|date=2004}}</ref>\n\n==Algorithm==\nZadeh's rule belongs to the family of history-based improvement rules which, during a run of the simplex algorithm, retain supplementary data in addition to the current basis of the linear program.\n\nIn particular, the rule chooses among all improving variables one which has entered the basis least often, intuitively ensuring that variables that might yield a substantive improvement in the long run but only a small improvement in a single step will be applied after a linear number of steps.\n\nThe supplementary data structure in Zadeh's algorithm can therefore be modeled as an occurrence record, mapping all variables to natural numbers, monitoring how often a particular variable has entered the basis. In every iteration, the algorithm then selects an improving variable that is minimal with respect to the retained occurrence record.\n\nNote that the rule does not explicitly specify which particular improving variable should enter the basis in case of a tie.\n\n==Superpolynomial lower bound==\nZadeh's rule has been shown to have at least [[super-polynomial time]] complexity in the worse-case by constructing a family of [[Markov Decision Processes]] on which the [[policy iteration]] algorithm requires a super-polynomial number of steps.\n\nRunning the simplex algorithm with Zadeh's rule on the induced linear program then yields a super-polynomial lower bound.\n\nThe result was presented at the [[Mathematical Optimization Society]]'s Integer Programming and Combinatorial Optimization conference in 2011 by [[Oliver Friedmann]]<ref>http://ipco2011.uai.cl/accepted.html</ref>. Zadeh, although not working in academia anymore at that time, attended the conference and honored his original proposal.<ref>https://gilkalai.wordpress.com/2011/01/20/gunter-ziegler-1000-from-beverly-hills-for-a-math-problem-ipam-remote-blogging</ref>\n\n== Notes ==\n{{Reflist}}\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Exchange algorithms]]\n[[Category:Oriented matroids]]\n[[Category:Linear programming]]"
    },
    {
      "title": "Zionts–Wallenius method",
      "url": "https://en.wikipedia.org/wiki/Zionts%E2%80%93Wallenius_method",
      "text": "{{context|date=December 2012}}\n\nThe '''Zionts–Wallenius method''' is an interactive method used to find a best solution to a [[Multiobjective optimization|multi-criteria optimization]] problem.\n\n==Detail==\nSpecifically it can help a user solve a [[linear programming]] problem having more than one (linear) objective. A user is asked to respond to comparisons between feasible solutions or to choose directions of change desired in each iteration. Providing certain mathematical assumptions hold, the method finds an optimal solution.\n\n==References==\n* Zionts, S. and J. Wallenius, “An Interactive Programming Method for Solving the Multiple Criteria Problem,” Management Science. Vol. 22, No. 6, pp.&nbsp;652–663, 1976.\n\n{{DEFAULTSORT:Zionts-Wallenius method}}\n[[Category:Optimization algorithms and methods]]\n\n\n{{compu-prog-stub}}\n{{mathapplied-stub}}"
    },
    {
      "title": "Benders decomposition",
      "url": "https://en.wikipedia.org/wiki/Benders_decomposition",
      "text": "'''Benders decomposition''' (or '''Benders' decomposition''')  is a technique in [[mathematical programming]] that allows the solution of very large [[linear programming]] problems that have a special [[block matrix|block structure]]. This block structure often occurs in applications such as [[stochastic programming]] as the uncertainty is usually represented with scenarios. The technique is named after [[Jacques F. Benders]].\n\nThe strategy behind Benders decomposition can be summarized as ''divide-and-conquer''. That is, in Benders decomposition, the variables of the original problem are divided into two subsets so that a first-stage master problem is solved over the first set of variables, and the values for the second set of variables are determined in a second-stage subproblem for a given first-stage solution. If the subproblem determines that the fixed first-stage decisions are in fact infeasible, then so-called ''Benders cuts'' are generated and added to the master problem, which is then re-solved until no cuts can be generated. Since Benders decomposition adds new ''constraints'' as it progresses towards a solution, the approach is called \"''row'' generation\".  In contrast, [[Dantzig–Wolfe decomposition]] uses \"[[column generation|''column'' generation]]\".\n\n== Applications ==\n\nAmong the most successful applications of the Benders decomposition is the [[facility location problem]]. Castro et al.<ref>{{Cite journal \n | last1= Castro |first1= J.\n | first2= S. | last2= Nasini\n | first3= F. | last3= Saldanha-da-Gama\n | title = A cutting-plane approach for large-scale capacitated multi-period facility location using a specialized interior-point method\n | journal = [[Mathematical Programming]]\n | year = 2017\n | volume = 163\n | issue = 1\n | pages = 411–444 | doi=10.1007/s10107-016-1067-6|hdl= 2117/80887\n }}</ref> show the Benders subproblems associated to the mathematical programming formulation of the facility location problem can be separated into block-angular structured linear programming problems. This allows efficiently solving large instances of those problems have been recently proposed by separating the variables of the original problem into binary decisions (corresponding to the placement of facilities) and transportation decisions (corresponding to the commodity flow).\n\n==See also==\n* [[FortSP]] solver uses Benders decomposition for solving stochastic programming problems\n\n==References==\n* Benders, J. F. (Sept. 1962), \"[https://link.springer.com/article/10.1007%2FBF01386316 Partitioning procedures for solving mixed-variables programming problems]\", ''[[Numerische Mathematik]]'' 4(3): 238–252. \n* {{citation|last=Lasdon|first=Leon&nbsp;S.|title=Optimization Theory for Large Systems|publisher=[[Dover Publications]]|location=Mineola, New York|year=2002|edition=reprint of the 1970 Macmillan|pages=xiii+523|mr=1888251}}.\n\n[[Category:Linear programming]]\n[[Category:Decomposition methods]]\n[[Category:Stochastic optimization]]"
    },
    {
      "title": "Dantzig–Wolfe decomposition",
      "url": "https://en.wikipedia.org/wiki/Dantzig%E2%80%93Wolfe_decomposition",
      "text": "{{short description|algorithm for solving linear programming problems with special structure}}\n\n'''Dantzig–Wolfe decomposition''' is an algorithm for solving [[linear programming]] problems with special structure.  It was originally developed by [[George Dantzig]] and [[Philip Wolfe (mathematician)|Philip Wolfe]] and initially published in 1960.<ref>{{cite journal|author=George B. Dantzig|author2=Philip Wolfe|title=Decomposition Principle for Linear Programs|journal=Operations Research|volume=8|year=1960|pages=101–111|doi=10.1287/opre.8.1.101}}</ref> Many texts on linear programming have sections dedicated to discussing this [[Decomposition method (constraint satisfaction)|decomposition algorithm]].<ref>{{cite book|author=Dimitris Bertsimas|author2=John N. Tsitsiklis|title=Linear Optimization|publisher=Athena Scientific|year=1997}}</ref><ref>{{cite book|author=George B. Dantzig|author2=Mukund N. Thapa|title=Linear Programming 2: Theory and Extensions|publisher=Springer|year=1997}}</ref><ref>{{cite book|author=Vašek Chvátal|title=Linear Programming|publisher=Macmillan|year=1983}}</ref><ref name=\"MarosMitra\" >{{cite book|last1=Maros|first1=István|last2=Mitra|first2=Gautam|chapter=Simplex algorithms|mr=1438309|title=Advances in linear and integer programming|pages=1–46|editor=J. E. Beasley|publisher=Oxford Science|year=1996}}</ref><ref>{{cite book|mr=1960274|last=Maros|first=István|title=Computational techniques of the simplex method|series=International Series in Operations Research & Management Science|volume=61|publisher=Kluwer Academic Publishers|location=Boston, MA|year=2003|pages=xx+325|isbn=1-4020-7332-1}}</ref><ref>{{cite book|last=Lasdon|first=Leon&nbsp;S.|title=Optimization theory for large systems|publisher=Dover Publications, Inc.|location=Mineola, New York|year=2002|edition=reprint of the 1970 Macmillan|pages=xiii+523|mr=1888251}}</ref>\n\nDantzig–Wolfe decomposition relies on [[delayed column generation]] for improving the [[tractable problem|tractability]] of large-scale linear programs.  For most linear programs solved via the revised [[simplex algorithm]],  at each  step, most columns (variables) are not in the basis. In such a scheme, a master problem containing at least the currently active columns (the basis) uses a subproblem or subproblems to generate columns for entry into the basis such that their inclusion improves the objective function.\n\n== Required form ==\n\nIn order to use Dantzig–Wolfe decomposition, the constraint matrix of the linear program must have a specific form.  A set of constraints must be identified as \"connecting\", \"coupling\", or \"complicating\" constraints wherein many of the variables contained in the constraints have non-zero coefficients.  The remaining constraints need to be grouped into independent submatrices such that if a variable has a non-zero coefficient within one submatrix, it will not have a non-zero coefficient in another submatrix.  This description is visualized below:\n\n[[File:DW Block Angular Matrix.jpg]]\n\nThe ''D'' matrix represents the coupling constraints and each ''F<sub>i</sub>'' represents the independent submatrices.  Note that it is possible to run the algorithm when there is only one ''F'' submatrix.\n\n== Problem reformulation ==\n\nAfter identifying the required form, the original problem is reformulated into a master program and ''n'' subprograms.  This reformulation relies on the fact that a non-empty, bounded [[convex polyhedron]] can be represented as a [[convex combination]] of its [[extreme points]] (or, in the case of an unbounded polyhedron, a convex combination of its extreme points and a weighted combination of its extreme rays).\n\nEach column in the new master program represents a solution to one of the subproblems.  The master program enforces that the coupling constraints are satisfied given the set of subproblem solutions that are currently available.  The master program then requests additional solutions from the subproblem such that the overall objective to the original linear program is improved.\n\n== The algorithm ==\n\nWhile there are several variations regarding implementation, the Dantzig–Wolfe decomposition algorithm can be briefly described as follows:\n\n# Starting with a feasible solution to the reduced master program, formulate new objective functions for each subproblem such that the subproblems will offer solutions that improve the current objective of the master program.\n# Subproblems are re-solved given their new objective functions.  An optimal value for each subproblem is offered to the master program.\n# The master program incorporates one or all of the new columns generated by the solutions to the subproblems based on those columns' respective ability to improve the original problem's objective.\n# Master program performs ''x'' iterations of the simplex algorithm, where ''x'' is the number of columns incorporated.\n# If objective is improved, goto step 1.  Else, continue.\n# The master program cannot be further improved by any new columns from the subproblems, thus return.\n\n== Implementation ==\n\nThere are examples of the implementation of Dantzig–Wolfe decomposition available in the [[AMPL]]<ref>{{cite web|url=http://www.ampl.com/NEW/LOOP2/index.html|title=AMPL code repository with Dantzig–Wolfe example|accessdate=December 26, 2008}}</ref> and [[General Algebraic Modeling System|GAMS]]<ref>{{citation|url=http://amsterdamoptimization.com/pdf/dw.pdf|title= Dantzig-Wolfe Decomposition with GAMS|first=Erwin|last=Kalvelagen|date=May 2003|accessdate=2014-03-31}}.</ref> mathematical modeling languages.  There is a general, parallel implementation available<ref>{{cite web|url=http://sourceforge.net/projects/dwsolver/|title=Open source Dantzig-Wolfe implementation|accessdate=October 15, 2010}}</ref> that leverages the open source [[GNU Linear Programming Kit]].\n\nThe algorithm can be implemented such that the subproblems are solved in parallel, since their solutions are completely independent.  When this is the case, there are options for the master program as to how the columns should be integrated into the master.  The master may wait until each subproblem has completed and then incorporate all columns that improve the objective or it may choose a smaller subset of those columns.  Another option is that the master may take only the first available column and then stop and restart all of the subproblems with new objectives based upon the incorporation of the newest column.\n\nAnother design choice for implementation involves columns that exit the basis at each iteration of the algorithm.  Those columns may be retained, immediately discarded, or discarded via some policy after future iterations (for example, remove all non-basic columns every 10 iterations).\n\nA recent (2001) computational evaluation of Dantzig-Wolfe in general and Dantzig-Wolfe and parallel computation is the PhD thesis by J. R. Tebboth<ref>\n{{cite book\n|last       = Tebboth\n|first      = James Richard\n|title      = A computational study of Dantzig-Wolfe decomposition\n|url        = http://www.blisworthhouse.co.uk/OR/Decomposition/tebboth.pdf\n|type       = PhD thesis\n|year       = 2001\n|location   = University of Buckingham, United Kingdom\n}}\n</ref>\n\n==See also==\n* [[Delayed column generation]]\n* [[Benders' decomposition]]\n\n== References ==\n\n{{reflist}}\n\n{{DEFAULTSORT:Dantzig-Wolfe decomposition}}\n[[Category:Linear programming]]\n[[Category:Decomposition methods]]"
    },
    {
      "title": "Decomposition (computer science)",
      "url": "https://en.wikipedia.org/wiki/Decomposition_%28computer_science%29",
      "text": "'''Decomposition''' in [[computer science]], also known as '''factoring''', is breaking a complex problem or system into parts that are easier to conceive, understand, program, and maintain.\n\n== Overview ==\nThere are different types of decomposition defined in computer sciences:\n\n* In [[structured programming]], ''algorithmic decomposition'' breaks a process down into well-defined steps.\n* [[Structured analysis]] breaks down a software system from the system context level to system functions and data entities as described by [[Tom DeMarco]].<ref>[[Tom DeMarco]] (1978). ''Structured Analysis and System Specification. New York, NY: Yourdon, 1978. {{ISBN|0-917072-07-3}}, {{ISBN|978-0-917072-07-9}}.</ref>\n* ''[[object-oriented programming|Object-oriented]] decomposition'', on the other hand, breaks a large system down into progressively smaller classes or objects that are responsible for some part of the problem domain.\n* According to [[Grady Booch|Booch]], algorithmic decomposition is a necessary part of object-oriented analysis and design, but object-oriented systems start with and emphasize decomposition into classes.<ref>[[Grady Booch]] (1994). ''Object-oriented Analysis and Design'' (2nd ed.). Redwood Cita, CA: Benjamin/Cummings. pp.16-20.</ref>\n\nMore generally, [[functional decomposition]] in computer science is a technique for mastering the complexity of the function of a model. A [[functional model]] of a system is thereby replaced by a series of functional models of subsystems.<ref name=\"Die06\">[[Jan Dietz]] (2006). ''Enterprise Ontology - Theory and Methodology''. Springer-Verlag Berlin Heidelberg.</ref>\n\n== Decomposition topics ==\n\n=== Decomposition paradigm ===\nA decomposition  paradigm in computer programming is a strategy for organizing a program as a number of parts, and it usually implies a specific way to organize a program text. Usually the aim of using a decomposition paradigm is to optimize some metric related to program complexity, for example the modularity of the program or its maintainability.\n\nMost decomposition paradigms suggest breaking down a program into parts so as to minimize the static dependencies among those parts, and to maximize the [[Cohesion (computer science)|cohesiveness]] of each part. Some popular decomposition paradigms are the procedural, modules, abstract data type and [[object oriented]] ones.\n\nThe concept of decomposition paradigm is entirely independent and different from that of [[model of computation]], but the two are often confused, most often in the cases of the [[functional model]] of computation being confused with procedural decomposition, and of the [[actor model]] of computation being confused with [[object oriented]] decomposition.\n\n=== Decomposition diagram ===\n<gallery class=\"center\">\nImage:6 Decomposition Structure.svg|Decomposition Structure\nImage:21 Negative Node-Numbered Context.svg|Negative Node-Numbered Context\nImage:Static, Dynamic, and Requirements Models for Sys Partition.jpg|Static, Dynamic, and Requirements Models for Systems Partition\nImage:Functions and Use Scenarios Mapping to Requirements and Goals.jpg|Functions and Use Scenarios Mapping to Requirements and Goals\n</gallery>\n\nA decomposition diagram shows a complex, process, organization, data subject area, or other type of object broken down into lower level, more detailed components. For example, decomposition diagrams may represent organizational structure or functional decomposition into processes. Decomposition diagrams  provide a logical hierarchical decomposition of a system.\n\n==See also==\n* [[Code refactoring]]\n* [[Component-based software engineering]]\n* [[Dynamization]]\n* [[Duplicate code]]\n* [[Event partitioning]]\n* [[How to Solve It]]\n* [[Integrated Enterprise Modeling]]\n* [[Personal information management]]\n* [[Readability]]\n* [[Subroutine]]\n\n== References ==\n{{reflist}}\n\n{{Refimprove|date=November 2008}}\n\n==External links==\n{{Commons category|Decomposition diagrams}}\n* [http://www.umsl.edu/~sauter/analysis/488_f01_papers/quillin.htm Object Oriented Analysis and Design]\n\n[[Category:Software design]]\n[[Category:Decomposition methods]]"
    },
    {
      "title": "Corpse decomposition",
      "url": "https://en.wikipedia.org/wiki/Corpse_decomposition",
      "text": "{{copy editing|reason=spelling errors, awkward grammar, repetition, adherence to Wikipedia standards, for example, of capitalization|date=May 2019}}\n{{merge|Forensic entomological decomposition|discuss=Talk:Corpse decomposition#Proposed merge with Forensic entomological decomposition|date=May 2019}}\n\nThe [[decomposition]] of [[Cadaver|human remains]] is the process of [[Organic compound|organic substances]] breaking down into simple [[organic matter]] over time. Different aspects, such as temperature, location, and body size, can change the decomposition process. The five general stages of decomposition within [[vertebrate|vertebrates]] include fresh, bloat, active decay, advanced decay, and dry/skeletonized.<ref>{{Cite journal|last=Payne|first=Jerry A.|date=September 1965|title=A Summer Carrion Study of the Baby Pig Sus Scrofa Linnaeus|url=http://dx.doi.org/10.2307/1934999|journal=Ecology|volume=46|issue=5|pages=592–602|doi=10.2307/1934999|issn=0012-9658}}</ref> The decomposition of human remains can vary within these five stages due to environmental conditions.\n\n== Stages and Characteristics ==\nThe five stages of decomposition—fresh, bloat, active decay, advanced decay, and dry/skeletonized—have specific characteristics that identify which stage the remains are in.<ref>{{Cite journal|last=Mądra|first=A.|last2=Frątczak|first2=K.|last3=Grzywacz|first3=A.|last4=Matuszewski|first4=S.|date=July 2015|title=Long-term study of pig carrion entomofauna|url=http://dx.doi.org/10.1016/j.forsciint.2015.04.013|journal=Forensic Science International|volume=252|pages=1–10|doi=10.1016/j.forsciint.2015.04.013|issn=0379-0738}}</ref> Technically, these steps are part of the stages of death.\n{{Signs of death}}\n\n=== Fresh ===\n[[File:Example of a pig carcass in the fresh stage of decomposition.jpg|thumb|180x180px|1: Fresh stage]]\n\nDuring the first stage, the remains are usually intact and clean of bugs or visible bacteria. Usually, the processes of [[algor mortis]] and [[rigor mortis]] are complete, so the remains are cold and [[livor mortis]] has begun, which means that blood has pooled on whichever side of the body is closest to the ground.<ref name=\":0\">{{Cite journal|last=Cerminara|first=Kathy L.|date=April 2011|title=After We Die|url=http://dx.doi.org/10.1080/01947648.2011.576635|journal=Journal of Legal Medicine|volume=32|issue=2|pages=239–244|doi=10.1080/01947648.2011.576635|issn=0194-7648}}</ref> Environment and temperature will usually affect the progression of the remains through the stages of death.<ref>{{Cite journal|last=Jiménez-Ruiz|first=Edgar I.|last2=Ocaño-Higuera|first2=Victor M.|last3=Maeda-Martínez|first3=Alfonso N.|last4=Varela-Romero|first4=Alejandro|last5=Márquez-Ríos|first5=Enrique|last6=Muhlia-Almazán|first6=Adriana T.|last7=Castillo-Yáñez|first7=Francisco J.|date=April 2013|title=Effect of seasonality and storage temperature on rigor mortis in the adductor muscle of lion's paw scallop Nodipecten subnodosus|url=http://dx.doi.org/10.1016/j.aquaculture.2013.01.006|journal=Aquaculture|volume=388-391|pages=35–41|doi=10.1016/j.aquaculture.2013.01.006|issn=0044-8486}}</ref>\n\n\n=== Bloat ===\n[[File:Example of a pig carcass in the bloat stage of decomposition.jpg|thumb|180x180px|2: Bloat stage]]\nIn the next stage, the remains begin to fill with gases emitted from bacteria inside the body. This causes remains to often purge from the nose or mouth. The body can also experience what is called [[skin slippage]], in which the first layer of skin begins to slough off of the carcass.{{Citation needed|date=June 2019}}\n\n=== Active Decay ===\n[[File:Example of a pig carcass in the active decay stage of decomposition.jpg|thumb|180x180px|3: Active Decay stage]]\nAfter the bloat stage, the remains begin active decay. During this stage, different bugs and insects often help in breaking down the remains, along with active bacteria living within the body. The skin will start to blacken along with the arrival of different insects. The most common specimens found within human remains are [[Maggot|maggots]], the larvae of flies. At this stage, it is possible to determine the [[Post-mortem interval|postmortem interval]] by measuring the length of present maggots. For example, if a [[maggot]] is 15 millimeters long, it is one week old.{{Citation needed|date=June 2019}} The age of the larvae is an accurate minimum time since death, but can certainly be lower than the actual time.\n\n=== Advanced Decay ===\nDuring advanced decay, most of the remains have discolored and often blackened. [[Putrefaction]], in which tissues and cells break down and liquidize as the body decays, will be almost complete.{{Citation needed|date=June 2019}} \n[[File:Example of a pig carcass in the advanced decay stage of decomposition.jpg|thumb|180x180px|4: Advanced Decay stage]]\n\n=== Dry/Skeletonized Remains ===\nOnce bloating has ceased, the soft tissue of remains typically collapses in on itself. At the end of active decay, the remains are often dried out and begin to [[Skeletonization|skeletonize]].\n\n[[File:Example of a pig carcass in the dry decay stage of decomposition.jpg|thumb|180x180px|5: Dry/Skeletonized Remains stage]]\n\n== Environmental Conditions == \n=== Hot and Humid Environments ===\nHigh temperatures can quicken the stages of decomposition. Heat helps organic material break down, which causes remains to move quickly through the fresh and bloating stages. In addition, active bacteria can grow faster and thrive in high heat, which also causes the remains to break down faster and begin to bloat quicker. Bloating is caused by gases released from the bacteria within the remains. A study performed by the Texas Tech University<ref>{{Cite journal|last=Stewart|first=T. D.|date=July 1935|title=Skeletal remains from southwestern Texas|url=http://dx.doi.org/10.1002/ajpa.1330200207|journal=American Journal of Physical Anthropology|volume=20|issue=2|pages=213–231|doi=10.1002/ajpa.1330200207|issn=0002-9483}}</ref> observed the decomposition of human remains within the Western Texan environment. After setting out fourteen human limbs, they found that whether the remains were left in areas of shade, vegetation, and insect and mammal feeding directly affected how quickly the remains skeletonized. [[Skeletonization]] began within two weeks and completed by the third month. The warm environment of Western [[Texas]] contributed to the fast decomposition of the remains, as well as other [[Taphonomy|taphonomic]] effects.\n\nSeasons like spring and summer; warmer or humid climates like those found on tropical islands; or enclosed spaces like cars, houses, or shelters can all increase the rate of decomposition. \n\n=== Cold Environments ===\n[[File:Progression_of_decomposition_by_ADD_score_for_outdoor_cases_95%25_CI_for_the_Mean.pdf|thumb|280x280px|This data, taken from \"The environmental variables that impact human decomposition in terrestrially exposed contexts within Canada\" published by the Science and Justice journal, represents the progression of decomposition using an ADD score.]]\nIn cold and dry environments, decomposition is often slowed down. The lower temperatures slow [[bacterial growth]], which lengthens the bloating stage significantly. In extreme temperatures, [[bacteria|bacterial]] activity can be almost nonexistent, as many bacteria cannot survive in low temperatures. An article observing the rate of decomposition in [[Canada]] found that temperatures of four degrees Celsius or lower correlated with a delayed onset of [[petrification]] and a greatly slowed decomposition process.<ref>{{Cite journal|last=Cockle|first=Diane Lyn|last2=Bell|first2=Lynne S|date=March 2017|title=The environmental variables that impact human decomposition in terrestrially exposed contexts within Canada|url=http://dx.doi.org/10.1016/j.scijus.2016.11.001|journal=Science & Justice|volume=57|issue=2|pages=107–117|doi=10.1016/j.scijus.2016.11.001|issn=1355-0306}}</ref> As shown in the graph, lower temperatures see less change in the human remains, while higher temperature result in more change. Colder temperatures slow decomposition and allow for better preserved remains.\n\nSeasons like fall and winter; high elevation locations like mountains; or shelters shielded from weather and animal activity will all slow decomposition.\n\n=== Wet Environments ===\nWater drastically affects the stages of decomposition and the breakdown of remains. Remains bloat much faster when sitting in water, as water is absorbed into the remains in addition to the gasses released by bacteria within the remains.  The effect of water on the decomposition process is highly variable and depends on many factors, such as water depth, temperature, tides, currents, seasons, dissolved oxygen, geology, acidity, salinity, sedimentation, and insect and scavenging activity.<ref>{{Cite journal |last=Heaton |first=Vivienne |last2=Lagden |first2=Abigail |last3=Moffatt |first3=Colin |last4=Simmons |first4=Tal |date=March 2010 |title=Predicting the Postmortem Submersion Interval for Human Remains Recovered from U.K. Waterways |url=http://dx.doi.org/10.1111/j.1556-4029.2009.01291.x |journal=Journal of Forensic Sciences |volume=55 |issue=2 |pages=302–307 |doi=10.1111/j.1556-4029.2009.01291.x |issn=0022-1198}}</ref> Due to these many variables, human remains found in aquatic surroundings are often incomplete and poorly preserved. Identifying and investigating the circumstances of death for remains that have been submerged in water is much more difficult.<ref>{{Cite journal |last=Delabarde |first=Tania |last2=Keyser |first2=Christine |last3=Tracqui |first3=Antoine |last4=Charabidze |first4=Damien |last5=Ludes |first5=Bertrand |date=May 2013 |title=The potential of forensic analysis on human bones found in riverine environment |url=http://dx.doi.org/10.1016/j.forsciint.2013.03.019 |journal=Forensic Science International |volume=228 |issue=1-3 |pages=e1–e5 |doi=10.1016/j.forsciint.2013.03.019 |issn=0379-0738}}</ref>\n\nConditions like wet seasons or flood zones, as well as the presence of lakes, pools, or rainfall, can all affect decomposition.\n\n== References ==\n<references />\n\n{{DEFAULTSORT:Decomposition of Human Remains within Different Environments}}\n[[Category:Decomposition methods]]"
    },
    {
      "title": "Modular design",
      "url": "https://en.wikipedia.org/wiki/Modular_design",
      "text": "{{about|a design approach|factory built structures moved in modules|modular building}}\n'''Modular design''', or \"[[modularity]] in design\", is an approach (design theory and practice) that subdivides a system into smaller parts called modules or [[modular process skid|skids]], that can be independently created and then used in different systems. A modular design can be characterized by functional partitioning into discrete scalable, reusable modules; rigorous use of well-defined modular interfaces; and making use of industry standards for interfaces. In this context modularity is at the component level, and has a single dimension, component slottability. A modular system with this limited modularity is generally known as a platform system that uses modular components.  Examples are Auto platforms or the USB port in CE platforms.  \n\nIn design theory this is distinct from a modular system which has higher dimensional modularity and degrees of freedom.  A modular system design has no distinct lifetime and exhibits flexibility in at least three dimensions. In this respect modular systems are very rare in markets.  Mero architectural systems are the closest example to a modular system in terms of hard products in markets.  Weapons platforms, especially in Aerospace, tend to be modular systems, wherein the airframe is designed to be upgraded multiple times during its lifetime, without the purchase of a completely new system. Modularity is best defined by the dimensions effected or the degrees of freedom in form, cost, or operation. \n\nModularity offers benefits such as reduction in cost (due to less customization), interoperability, shorter learning time, flexibility in design, non-generationally constrained augmentation or updating (adding new solution by merely plugging in a new module), and exclusion. Modularity in platform systems, offer benefits in returning margins to scale, reduced product development cost, reduced O&M costs, and time to market.  Platform systems have enabled the wide use of system design in markets and the ability for product companies to separate the rate of the product cycle from the R&D paths. The biggest drawback with modular systems is the designer or engineer.  Most designers are poorly trained in systems analysis and most engineers are poorly trained in design. The design complexity of a modular system is significantly higher than a platform system and requires experts in design and product strategy during the conception phase of system development. That phase must anticipate the directions and levels of flexibility necessary in the system to deliver the modular benefits. Modular systems could be viewed as more complete or holistic design whereas platforms systems are more reductionist, limiting modularity to components.  Complete or holistic modular design requires a much higher level of design skill and sophistication than the more common platform system.\n\n[[Car]]s, [[computers]], [[Modular process skid|process systems]], [[solar panel]]s, [[wind turbine]]s, [[elevator]]s, [[furniture]], [[loom]]s, [[railroad signal]]ing systems, [[telephone exchanges]], [[pipe organ]]s, [[Modular synthesizer|synthesizers]], [[electric power distribution]] systems and modular buildings are examples of platform systems using various levels of component modularity.  For example, one cannot assemble a solar cube from extant solar components or easily replace the engine on a truck or rearrange a modular housing unit into a different configuration after a few years, as would be the case in a modular system.  The only extant examples of modular systems in today's market are some software systems that have shifted away from versioning into a completely networked paradigm.  \n\nModular design inherently combines the mass production advantages of [[standardization]], since modularity is impossible without some level of standardization, (high volume normally equals low manufacturing costs) with those of [[Personalization|customization]].  The degree of modularity, dimensionally, determines the degree of customization possible. For example, solar panel systems have 2-dimensional modularity which allows adjustment of an array in the x and y dimensions.  Further dimensions of modularity would be introduced by making the panel itself and its auxiliary systems modular. Dimensions in modular systems are defined as the effected parameter such as shape or cost or lifecycle. Mero systems have 4-dimensional modularity, x, y, z, and structural load capacity. As can be seen in any modern convention space, the space frame's extra two dimensions of modularity allows far greater flexibility in form and function than solar's 2-d modularity. If modularity is properly defined and conceived in the design strategy, modular systems can create significant competitive advantage in markets.  A true modular system does not need to rely on product cycles to adapt its functionality to the current market state.  Properly designed modular systems also introduce the economic advantage of not carrying dead capacity, increasing the capacity utilization rate and its effect on cost and pricing flexibility.\n<!-- (I'm commenting this sentence out because it doesn't make sense and contains a lot of errors. I don't know enough about the topic to patch this up - if you can, please reinstate. Thanks!)  Modular design <ref>Baldwin C., and Clark K. (2006) “ Modularity in the design of complex engineering systems”, in understanding complex systems, pp 175–205, Springer</ref> often lack of optimization and due his success to the concept of product platform <ref>Muffatto M (1999) Platform strategies in international new product development. Int J Opera Prod Manag 19(5/6):449–460. {{doi|10.1108/01443579910260766}}</ref> that has been developed in automotive and electronic industry,.<ref>\nBrylawski M., “Uncommon knowledge: Automobile platform sharing’s potential impact on advanced technologies”, 1st international automotive conference. international society for the advancement of material and process engineering, society for the advancement of material and process engineering (SAMPE), 2010</ref><ref>Schlie E, Yip G (2004) Regional follows global: strategy mixes in the world automotive industry. Eur Manag J 18(4):343–354. {{doi|10.1016/S0263-2373(00)00019-0}}</ref>-->\n\n==In vehicles==\n{{See also|Electric vehicle|Wikispeed}}\n[[File:2010-04-07 Unimog at Arthur Ibbetts machinery dealership.jpg|thumb|The modular design of the [[Unimog]] offers attachment capabilities for various different [[List of agricultural machinery|implements]].]]\n\nAspects of modular design can be seen in cars or other [[vehicles]] to the extent of there being certain parts to the car that can be added or removed without altering the rest of the car.\n\nA simple example of modular design in cars is the fact that, while many cars come as a basic model, paying extra will allow for \"snap in\" upgrades such as a more powerful engine or seasonal tires; these do not require any change to other units of the car such as the chassis, steering, electric motor or battery systems.\n\n==In machines and architecture==\n{{main article|Modular building}}\nModular design can be seen in certain buildings. Modular buildings (and also modular homes) generally consist of universal parts (or modules) that are manufactured in a [[factory]] and then shipped to a build site where they are assembled into a variety of arrangements.<ref>{{cite web|url=http://architecture.about.com/cs/buildyourhouse/g/modular.htm|title=Modular home definition|accessdate=2010-08-19}}</ref>\n\nModular buildings can be added to or reduced in size by adding or removing certain components. This can be done without altering larger portions of the building. Modular buildings can also undergo changes in functionality using the same process of adding or removing components.\n\n[[File:Flexible Workplace Variability.jpg|thumb|left|275px|Modular workstations]]\n\nFor example, an [[office]] building can be built using modular parts such as walls, frames, doors, ceilings, and windows. The interior can then be partitioned (or divided) with more walls and furnished with desks, computers, and whatever else is needed for a functioning workspace. If the office needs to be expanded or redivided to accommodate employees, modular components such as wall panels can be added or relocated to make the necessary changes without altering the whole building. Later, this same office can be broken down and rearranged to form a [[retail]] space, [[conference hall]] or another type of building, using the same modular components that originally formed the office building. The new building can then be refurnished with whatever items are needed to carry out its desired functions.\n\nOther types of modular buildings that are offered from a company like Allied Modular include a [[guardhouse]], machine enclosure, [[press box]], [[conference room]], two-story building, [[clean room]] and many more applications.<ref>[http://www.alliedmodular.com/products Allied Modular Products] Allied Modular. Retrieved March 27, 2012</ref>\n\nMany misconceptions are held regarding modular buildings.<ref>{{cite web|url=http://icon-construction.com/2014/08/top-5-myths-modular-construction/|title=modular building|deadurl=yes|archiveurl=https://archive.is/20140917142328/http://icon-construction.com/2014/08/top-5-myths-modular-construction/|archivedate=2014-09-17|df=}}</ref> In reality modular construction is a viable method of construction for quick turnaround and fast growing companies. Industries that would benefit from this include healthcare, commercial, retail, military, and multi-family/student housing.\n\n==In televisions==\n\nIn 1963 [[Motorola]] introduced the first rectangular color picture tube, and in 1967 introduced the modular [[Quasar (brand)|Quasar]] brand. In 1964 it opened its first research and development branch outside of the United States, in Israel under the management of Moses Basin. In 1974 Motorola sold its television business to the Japan-based Matsushita, the parent company of Panasonic.\n\n==In computer hardware==\n[[File:Xi3 modular computer 01.jpg|thumb|right|200px|Modular computer design]] \nModular design in computer hardware is the same as in other things (e.g. cars, refrigerators, and furniture). The idea is to build computers with easily replaceable parts that use standardized interfaces. This technique allows a user to upgrade certain aspects of the computer easily without having to buy another computer altogether. This idea was also being implemented in [[Project Ara]], which provided a platform for manufactures to create modules for a smartphone which could then be customised by the end user.\n\nA computer is one of the best examples of modular design. Typical modules include [[Power supply unit (computer)|power supply units]], [[Central processing unit|processors]], [[mainboard]]s, [[graphics card]]s, [[hard drive]]s, and [[optical drive]]s. All of these parts should be easily [[interchangeable parts|interchangeable]] as long as the user uses parts that support the same standard interface. Similar to the computer's modularity, other tools have been developed to leverage modular design, such as [[LittleBits|littleBits Electronics]], which snap together with interoperable modules to create circuits.<ref name=\"PSFK 5.15\">{{cite web|title=How One Entrepreneur Is Bringing Fringe Maker Knowledge Mainstream|url=http://www.psfk.com/2014/08/one-entrepreneur-bringing-fringe-maker-knowledge-mainstream.html|website=PSFK|publisher=PSFK|accessdate=27 May 2015|ref=PSFK|date=2014-08-26}}</ref>\n\n==Modular Design, Digital Twin, and Industry 4.0 directions==\nDuring the presentation of the [[Product lifecycle management|PLM]] centre of University of Michigan, Grieves<ref>Grieves, M. (2005). “[https://www.inderscienceonline.com/doi/abs/10.1504/IJPD.2005.006669 Product Lifecycle Management: the new paradigm for enterprises].” Int. J. Product development 2, 71-84 </ref> launched inside the modular design contest the initial idea of \"Conceptual ideal for PLM\", that introduces: real space, virtual space, the link for data flow from real space to virtual space, the link for information flow from virtual space to real space and virtual sub-spaces.\nEgan <ref>Egan, M. (2004). “[https://www.designsociety.org/download-publication/27307/Implementing+A+Successful+Modular+Design+-+PTC%C2%B4S+Approach Implementing A Successful Modular Design-PTC´s Approach].” In Proceedings of the 7th Workshop on Product Structuring - Product Platform Development, Chalmers University, Göteborg, Sweden, 24.-25.03. 2004.</ref>([[PTC Inc]]) disclosed the strategy for an implementation of modular design in a PLM (Product Lifecycle Management) contest through a process that starts with a cross-functional input to the definition of the product architecture, and includes an architecture development program that keeps the integrity of the product during its lifecycle.  \nGrieves <ref>Grieves, M. (2006), “[[Product Lifecycle Management]], Driving the Next Generation of Lean Thinking”, New York, McGraw-Hill </ref>has produced an effective definition of [[digital twin]]: “A strategic business approach that applies a consistent set of business solutions in support of the collaborative creation, management, dissemination, and use of product definition information across the extended enterprise from concept to end of life –integrating people, processes, business systems, and information”.\n\n==Integrating Lifecycle and Energy assessments into modular design ==\nSome authors observe that modular design has generated in the vehicle industry a constant increase of weight over time. Trancossi <ref>Trancossi, M. [https://link.springer.com/article/10.1007/s12544-014-0150-4 A response to industrial maturity and energetic issues: a possible solution based on constructal law]. Eur. Transp. Res. Rev. (2015) 7: 2. {{doi|10.1007/s12544-014-0150-4}}</ref> advanced the hypothesis that modular design can be coupled by some optimization criteria derived from the [[constructal law]]. In fact, the constructal law is modular for his nature and can apply with interesting results in engineering simple systems.<ref>Bejan A., and Lorente S., “Constructal theory of generation of configuration in nature and engineering”, J. Appl. Phys., 100, 2006, {{doi|10.1063/1.2221896}}</ref> It applies with a typical bottom-up optimization schema: \n* a system can be divided into subsystems (elemental parts) using tree models;\n* any complex system can be represented in a modular way and it is possible to describe how different physical magnitudes flow through the system; \n* analysing the different flowpaths it is possible to identify the critical components that affect the performance of the system; \n* by optimizing those components and substituting them with more performing ones, it is possible to improve the performances of the system. \nA better formulation has been produced during the MAAT EU FP7 Project.<ref>{{Cite web | url=http://cordis.europa.eu/project/rcn/99650_en.html | title=Multibody Advanced Airship for Transport &#124; Projects &#124; FP7-TRANSPORT}}</ref> A new design method that couples the above bottom-up optimization with a preliminary system level top-down design has been formulated.<ref>Dumas A, Madonia M, Trancossi M, Vucinic D (2013) [http://www.academia.edu/download/43218871/Propulsion_of_Photovoltaic_Cruiser-Feede20160229-7290-15tzf2r.pdf Propulsion of photovoltaic cruiser-feeder airships dimensioning by constructal design for efficiency method]. SAE Int J Aerosp 6(1):273–285. {{doi|10.4271/2013-01-2303}} https://www.academia.edu/download/43218871/Propulsion_of_Photovoltaic_Cruiser-Feede20160229-7290-15tzf2r.pdf</ref> The two step design process has been motivated by considering that constructal and modular design does not refer to any objective to be reached in the design process. A theoretical formulation has been provided in a recent paper,<ref>Trancossi, M. Eur. Transp. Res. Rev. (2015) 7: 2. {{doi|10.1007/s12544-014-0150-4}} https://link.springer.com/article/10.1007/s12544-014-0150-4</ref> and applied with success to the design of a small aircraft,<ref>Trancossi, M., Bingham, C., Capuani, A., Das, S. et al., \"Multifunctional Unmanned Reconnaissance Aircraft for Low-Speed and STOL Operations,\" SAE Technical Paper 2015-01-2465, 2015, {{doi|10.4271/2015-01-2465}}. https://www.academia.edu/download/39296132/2015-01-2465.pdf</ref> the conceptual design of innovative commuter aircraft,<ref>Trancossi, M., Madonia, M., Dumas, A. et al. Eur. Transp. Res. Rev. (2016) 8: 11. {{doi|10.1007/s12544-016-0198-4}} https://www.researchgate.net/publication/297890415_A_new_aircraft_architecture_based_on_the_ACHEON_Coanda_effect_nozzle_flight_model_and_energy_evaluation</ref><ref>Trancossi, M., Dumas, A., Madonia, M., Subhash, M. et al., \"Preliminary Implementation Study of ACHEON Thrust and Vector Electrical Propulsion on a STOL Light Utility Aircraft,\" SAE Technical Paper 2015-01-2422, 2015, {{doi|10.4271/2015-01-2422}}. https://www.researchgate.net/publication/300470359_Preliminary_Implementation_Study_of_ACHEON_Thrust_and_Vector_Electrical_Propulsion_on_a_STOL_Light_Utility_Aircraft?ev=prf_pub</ref> the design of a new entropic wall,<ref>TRANCOSSI, M., et al. Constructal Design of an Entropic Wall With Circulating Water Inside. Journal of Heat Transfer, 2016, 138.8: 082801. https://www.researchgate.net/publication/301779272_Constructal_Design_of_an_Entropic_Wall_With_Circulating_Water_Inside</ref> and an innovative off-road vehicle designed for energy efficiency.<ref>Trancossi M., Pascoa J, Design of an Innovative Off Road Hybrid Vehicle by Energy Efficiency Criteria, International Journal of Heat and Technology, 2016. https://www.researchgate.net/publication/309732623_Design_of_an_Innovative_Off_Road_Hybrid_Vehicle_by_Energy_Efficiency_Criteria?ev=prf_pub</ref>\n\n==See also==\n{{Wiktionary|beam}}\n{{div col|colwidth=22em}}\n* [[3D printing]]\n* [[Configuration design]]\n* [[Holarchy]]\n* [[Holism]]\n* ''[[Kraftei]]''\n* [[Integrating functionality]]\n* [[Modular building]]\n* [[Modular function deployment]] (MFD)\n* [[Modular programming]]\n* [[Modular smartphone]]\n* [[Modularity]]\n* [[Open-design movement]]\n* [[Open-source hardware]]\n* [[OpenStructures]]\n* [[Separation of concerns]]\n* [[Systems design]]\n* [[Systems engineering]]\n{{div col end}}\n\n==References==\n{{reflist}}\n\n==Further reading==\n* Schilling, MA., \"Toward a general modular systems theory and its application to interfirm product modularity\" Academy of Management Review, 2000, Vol 25(2):312-334. [https://www.jstor.org/stable/259016?seq=1#page_scan_tab_contents]\n* Erixon, O.G. and Ericsson, A., \"''Controlling Design Variants''\" USA: Society of Manufacturing Engineers 1999 [https://web.archive.org/web/20090213192431/http://www.sme.org/cgi-bin/get-item.pl?BK99PUB16&2&SME] {{ISBN|0-87263-514-7}} [https://archive.is/20090213213540/http://users.du.se/~gex/index.htm]\n* Clark, K.B. and Baldwin, C.Y., \"''Design Rules. Vol. 1: The Power of Modularity''\" Cambridge, Massachusetts: MIT Press 2000 {{ISBN|0-262-02466-7}}\n* Baldwin, C.Y., Clark, K.B., \"''The Option Value of Modularity in Design''\" Harvard Business School, 2002 [http://www.people.hbs.edu/cbaldwin/DR2/DR1Option.pdf]\n* Levin, Mark Sh. \"''Modular systems design and evaluation''\". Springer, 2015.\n* [http://www.cs.drexel.edu/~yfcai/Presentations/Modularity%20in%20Design_CMU.ppt Modularity in Design Formal Modeling & Automated Analysis]\n* [http://www.connected.org/media/modular.html \"Modularity: upgrading to the next generation design architecture\"], an interview\n\n{{Design}}\n\n[[Category:Modular design| ]]\n[[Category:Systems engineering]]\n[[Category:Engineering concepts]]\n[[Category:Design]]\n[[Category:Holism]]\n[[Category:Decomposition methods]]\n[[Category:Open-source hardware]]"
    },
    {
      "title": "Network partition",
      "url": "https://en.wikipedia.org/wiki/Network_partition",
      "text": "{{about|networking hardware and its optimization|measurement in network science of graph structure|Modularity (networks)}}\n\nA '''network partition''' refers to network decomposition into relatively independent [[subnetwork|subnets]] for their separate optimization as well as network split due to the failure of network devices. In both cases the partition-tolerant behavior of subnets is expected. This means that even after the network is partitioned into multiple sub-systems, it still works correctly.\n\nFor example, in a network with multiple subnets where nodes A and B are located in one subnet and nodes C and D are in another, a partition occurs if the network switch device between the two subnets fails. In that case nodes A and B can no longer communicate with nodes C and D, but all nodes A-D  work the same as before.\n\n== Network Partition for Optimization ==\n[[File:Network Partition for Optimization.svg|alt=Network Partition|thumb|Fig. 1. Network partition with discarding of the most irrelevant interactions between elements.<ref name=\"Network partition\">{{cite journal|author1=Ignatov, D.Yu.|author2=Filippov, A.N.|author3=Ignatov, A.D.|author4=Zhang, X.|title=Automatic Analysis, Decomposition and Parallel Optimization of Large Homogeneous Networks|journal=Proc. ISP RAS|date=2016|volume=28|pages=141–152|doi=10.15514/ISPRAS-2016-28(6)-10|arxiv=1701.06595|url=https://arxiv.org/pdf/1701.06595.pdf}}</ref>]]\nTo decompose an [[NP-hardness|NP-hard]] network optimization task into subtasks, the network can be decomposed into relatively independent subnets. In order to partition the network, it is useful to visualize it as a weighted complete graph, where each vertex corresponds to a network element, and each edge has a weight equal to the rank of the correlation between each pair of corresponding elements. Then the most irrelevant interactions between elements of network are discarded. Based on the remaining connections, the network is then further split into relatively independent subnets.<ref name=\"Network partition\" /> Wherein different allocations of optimized elements predispose alternative splits of the network (Fig. 1). In the case of a large network, the optimization of each subnet can then be performed independently on different computer clusters.\n\n== As a CAP trade-off ==\nThe [[CAP Theorem]] is based on three trade-offs: [[Consistency (database systems)|Consistency]], [[Availability]], and Partition tolerance. Partition tolerance, in this context, means the ability of a data processing system to continue processing data even if a network partition causes communication errors between subsystems.<ref>{{Cite web|url = http://cacm.acm.org/blogs/blog-cacm/83396-errors-in-database-systems-eventual-consistency-and-the-cap-theorem/fulltext|title = Errors in Database Systems, Eventual Consistency, and the CAP Theorem|date = April 5, 2010|accessdate = |website = |publisher = Communications of the ACM|last = Stonebraker|first = Michael}}</ref>\n\n== External links ==\n* [https://www.slideshare.net/DmitryIgnatovPhD/network-optimization-82005426 Partition of the Large Network]  <span style=\"font-size:70%\">[[doi:10.13140/RG.2.2.20183.06565/6]]</span>\n\n==References==\n{{Reflist}}\n\n[[Category:Hardware partitioning]]\n[[Category:Networking hardware]]\n[[Category:Decomposition methods]]"
    },
    {
      "title": "Theory of two-level planning",
      "url": "https://en.wikipedia.org/wiki/Theory_of_two-level_planning",
      "text": "The '''theory of two-level planning''' (alternatively, '''Kornai–Liptak decomposition''') is a method that [[Matrix decomposition | decomposes]] large problems of [[Linear programming|linear optimization]] into sub-problems. This decomposition simplifies the solution of the overall problem. The method also models a method of coordinating economic decisions so that decentralized firms behave so as to produce a global optimum.  It was introduced by the Hungarian economist [[János Kornai]] and the mathematician Tamás Lipták in 1965.  It is an alternative to [[Dantzig–Wolfe decomposition]].\n\n==Description==\n\nThe LP problem must have a special structure, known as a block angular structure.  This is the same structure required for the Dantzig Wolfe decomposition:\n\n[[File:DW Block Angular Matrix.jpg]]\n\nThere are some constraints on overall resources (D) for which a central planning agency is assumed to be responsible, and n blocks of coefficients (F1 through Fn) that are the concern of individual firms.\n\nThe central agency starts the process by providing each firm with tentative resource allocations which satisfy the overall constraints D.  Each firm optimizes its local decision variables assuming the global resource allocations are as indicated.  The solution of the firm LP's yield Lagrange multipliers (prices) for the global resources which the firms transmit back to the planning agency.\n\nIn the next iteration, the central agency uses the information received from firms to come up with a revised resource allocation; for example if firm i reports a high shadow price for resource j, the agency will grant more of this resource to this firm and less to other firms.  The revised tentative allocations are sent back to the individual firms and the process continues.\n\nIt has been shown that this process will converge (though not necessarily in a finite number of steps) towards the global solution for the overall problem. (In contrast the Dantzig Wolfe method converges in a finite number of steps).\n\nThe DW and KL methods are dual: in DW the central market establishes prices (based on firm demands for resources) and sends these to the firms who then modify the quantities they demand, while in KL the central agency sends out quantity information to firms and receives bids (i.e. firm specific pricing information) from firms.\n\n==See also==\n* [[Dantzig–Wolfe decomposition]]\n* [[Benders' decomposition]]\n* [[Delayed column generation|Column generation]]\n\n==References==\n* J. Kornai, T. Liptak: ''Two-level Planning'', Econometrica, 1965, Vol. 33, pp.&nbsp;141–169. [http://www.kornai-janos.hu/Kornai-Liptak1965%20Two-level%20planning%20-%20Econometrica.pdf]\n\n{{instecon}}\n\n[[Category:Linear programming]]\n[[Category:Decomposition methods]]"
    },
    {
      "title": "Variable splitting",
      "url": "https://en.wikipedia.org/wiki/Variable_splitting",
      "text": "{{Orphan|date=September 2011}}\n\nIn [[applied mathematics]] and [[computer science]], '''variable splitting''' is a [[Matrix decomposition | decomposition]] method that [[relaxation technique (mathematics)|relaxes]] a set of [[Constraint (mathematics)|constraint]]s.\n\n==Details==\nWhen the variable ''x'' appears in two sets of constraints, it is possible to substitute the new variables ''x''1 in the first constraints and ''x''2 in the second, and then join the two variables with a new \"''linking''\" constraint,<ref name=\"Vanderbei\">{{harvtxt|Vanderbei|1991}}</ref> which requires that \n: ''x''1=''x''2.\n\nThis new linking constraint can be [[Lagrangian relaxation|relaxed]] with a [[Lagrange multiplier]]; in many applications, a Lagrange multiplier can be interpreted as the ''[[price]]'' of equality between ''x''1 and ''x''2 in the new constraint.\n\nFor many problems, when the equality of the split variables is relaxed, then the system is decomposed, and each subsystem can be solved independently, at substantial reduction of computing time and memory storage. A solution to the relaxed problem (with variable splitting) provides an approximate solution to the original problem: further, the approximate solution to the relaxed problem provides a \"warm start\", a good initialization of an iterative method for solving the original problem (having only the ''x'' variable).\n\nThis was first introduced by Kurt O. Jörnsten, Mikael Näsberg, Per A. Smeds in 1985. At the same time, M. Guignard and S. Kim introduced the same idea under the name Lagrangean Decomposition (their papers appeared in 1987). The original references are \n(1) Variable Splitting: A New Lagrangean Relaxation Approach to Some Mathematical Programming Models\nAuthors\tKurt O. Jörnsten, Mikael Näsberg, Per A. Smeds\nVolumes 84-85 of LiTH MAT R.: Matematiska InstitutionenPublisher  University of Linköping, Department of Mathematics, 1985\nLength\t52 pages\nand \n(2) Lagrangean  Decomposition:  A  Model  Yielding  Stronger  Bounds,  Authors Monique Guignard and Siwhan  Kim,  Mathematical Programming, 39(2), 1987, pp. 215-228.\n\n\n<ref name=\"Vanderbei\"/><ref>{{harvtxt|Alvarado|1990}}</ref><ref>{{harvtxt|Adlers|Björck|2000}} Reprinted as Appendix A, in Mikael Adlers, 2000, ''Topics in Sparse Least Squares Problems'', Linkoping Studies in Science and Technology\", Linkoping University, Sweden.</ref>\n\n==Notes==\n<references/>\n\n==Bibliography==\n* {{cite journal|last1=Adlers|first1=Mikael|last2=Björck|first2=Åke|authorlink2=Åke Björck|title=Matrix stretching for sparse least squares problems|year=2000|pages=51–65|journal=Numerical Linear Algebra with Applications|volume=7|issue=2|issn=1099-1506|doi=10.1002/(SICI)1099-1506(200003)7:2|ref=harv|doi-broken-date=2019-03-16}}\n* {{cite journal|last=Alvarado|first=Fernando|title=Matrix enlarging methods and their application|journal=BIT Numerical Mathematics|year=1997|pages=473–505|volume=37|issue=3|\ndoi=10.1007/BF02510237|ref=harv|citeseerx=10.1.1.24.5976}}\n* {{cite techreport |first=Joseph |last=Grcar |title=Matrix stretching for linear equations |institution=Sandia National Laboratories |number=SAND90-8723 |arxiv=1203.2377 |year=1990|bibcode=2012arXiv1203.2377G }}\n* {{cite journal|first=Robert J.|last=Vanderbei|authorlink=Robert J. Vanderbei|title=Splitting dense columns in sparse linear systems|journal=Linear Algebra and its Applications|volume=152|date=July 1991|pages=107–117|issn=0024-3795\n|doi= 10.1016/0024-3795(91)90269-3\n|url=http://www.sciencedirect.com/science/article/B6V0R-45G0K7M-6M/2/1b6056c04b5ba391be0164a8cd0e98d3\n|ref=harv}}\n\n[[Category:Decomposition methods]]"
    },
    {
      "title": "Approximate string matching",
      "url": "https://en.wikipedia.org/wiki/Approximate_string_matching",
      "text": "[[File:Did you mean andré emotions.png|thumb|300px|Fuzzy Mediawiki search for \"''angry emoticon''\": \"Did you mean: ''andré emotions''\"]]\nIn [[computer science]], '''approximate string matching''' (often colloquially referred to as '''fuzzy string searching''') is the technique of finding  [[String (computing)|strings]] that match a [[pattern]] approximately (rather than exactly). The problem of approximate string matching is typically divided into two sub-problems: finding approximate [[substring]] matches inside a given string and finding dictionary strings that match the pattern approximately.\n\n== Overview==\nThe closeness of a match is measured in terms of the number of primitive operations necessary to convert the string into an exact match. This number is called the [[edit distance]] between the string and the pattern. The usual primitive operations are:{{ref|CRMN01}}\n* insertion: ''cot'' → ''co'''a'''t''\n* deletion:  ''co'''a'''t'' → ''cot''\n* substitution: ''co'''a'''t'' → ''co'''s'''t''\nThese three operations may be generalized as forms of substitution by adding a NULL character (here symbolized by *) wherever a character has been deleted or inserted:\n* insertion: ''co'''*'''t'' → ''co'''a'''t''\n* deletion:  ''co'''a'''t'' → ''co'''*'''t''\n* substitution:  ''co'''a'''t'' → ''co'''s'''t''\n\nSome approximate matchers also treat ''transposition'', in which the positions of two letters in the string are swapped, to be a primitive operation.{{ref|CRMN01}}\n* transposition:  ''co'''st''''' → ''co'''ts'''''\n\nDifferent approximate matchers impose different constraints. Some matchers use a single global unweighted cost, that is, the total number of primitive operations necessary to convert the match to the pattern. For example, if the pattern is ''coil'', ''foil'' differs by one substitution, ''coils'' by one insertion, ''oil'' by one deletion, and ''foal'' by two substitutions. If all operations count as a single unit of cost and the limit is set to one, ''foil'', ''coils'', and ''oil'' will count as matches while ''foal'' will not.\n\nOther matchers specify the number of operations of each type separately, while still others set a total cost but allow different weights to be assigned to different operations. Some matchers permit separate assignments of limits and weights to individual groups in the pattern.\n\n== Problem formulation and algorithms ==\nOne possible definition of the approximate string matching problem is the following: Given a pattern string <math>P = p_1p_2...p_m</math> and a text string <math>T = t_1t_2\\dots t_n</math>, find a substring <math>T_{j',j} = t_{j'}\\dots t_j</math> in ''T'', which, of all substrings of ''T'', has the smallest edit distance to the pattern ''P''.\n\nA brute-force approach would be to compute the edit distance to P for all substrings of T, and then choose the substring with the minimum distance. However, this algorithm would have the running time [[Big O notation|''O'']](''n''<sup>3</sup>&nbsp;''m'').\n\nA better solution, which was proposed by Sellers{{ref|Sel80}}, relies on [[dynamic programming]]. It uses an alternative formulation of the problem: for each position ''j'' in the text ''T'' and each position ''i'' in the pattern ''P'', compute the minimum edit distance between the ''i'' first characters of the pattern, <math>P_i</math>, and any substring <math>T_{j',j}</math> of ''T'' that ends at position ''j''.\n\nFor each position ''j'' in the text ''T'', and each position ''i'' in the pattern ''P'', go through all substrings of ''T'' ending at position ''j'', and determine which one of them has the minimal\nedit distance to the ''i'' first characters of the pattern ''P''. Write this minimal distance as ''E''(''i'',&nbsp;''j''). After computing ''E''(''i'',&nbsp;''j'') for all ''i'' and ''j'', we can easily find a solution to the original problem: it is the substring for which ''E''(''m'',&nbsp;''j'') is minimal (''m'' being the length of the pattern ''P''.)\n\nComputing ''E''(''m'',&nbsp;''j'') is very similar to computing the edit distance between two strings. In fact, we can use the [[Levenshtein distance#Computing Levenshtein distance|Levenshtein distance computing algorithm]] for ''E''(''m'',&nbsp;''j''), the only difference being that we must initialize the first row with zeros, and save the path of computation, that is, whether we used ''E''(''i''&nbsp;&minus;&nbsp;1,''j''), E(''i'',''j''&nbsp;&minus;&nbsp;1) or ''E''(''i''&nbsp;&minus;&nbsp;1,''j''&nbsp;&minus;&nbsp;1) in computing ''E''(''i'',&nbsp;''j''). \n\nIn the array containing the ''E''(''x'',&nbsp;''y'') values, we then choose the minimal value in the last row, let it be ''E''(''x''<sub>2</sub>,&nbsp;''y''<sub>2</sub>), and follow the path of computation backwards, back to the row number 0. If the field we arrived at was ''E''(0,&nbsp;''y''<sub>1</sub>), then ''T''[''y''<sub>1</sub>&nbsp;+&nbsp;1]&nbsp;...&nbsp;''T''[''y''<sub>2</sub>] is a substring of T with the minimal edit distance to the pattern ''P''.\n\nComputing the ''E''(''x'',&nbsp;''y'') array takes [[Big O notation|''O'']](''mn'') time with the dynamic programming algorithm, while the backwards-working phase takes [[Big O notation|''O'']](''n''&nbsp;+&nbsp;''m'') time.\n\nAnother new idea recent years is the similarity join. When matching database relates to a large scale of data, the [[Big O notation|''O'']](''mn'') time with the dynamic programming algorithm cannot work within a limited time. So, the idea is, instead of computing the similarity of ''all'' pairs of strings, to reduce the number of candidate pairs. Widely used algorithms are based on filter-verification, hashing, [[Locality-sensitive hashing]] (LSH), [[Trie|Tries]] and other greedy and approximation algorithms. Most of them are designed to fit some framework (such as Map-Reduce) to compute concurrently.\n\n==On-line versus off-line==\nTraditionally, approximate string matching algorithms are classified into two categories: on-line and off-line. With on-line algorithms the pattern can be processed before searching but the text cannot.  In other words, on-line techniques do searching without an index. Early algorithms for on-line approximate matching were suggested by Wagner and Fisher{{ref|WF74}} and by Sellers{{ref|Sel80}}. Both algorithms are based on [[dynamic programming]]  but solve different problems. Sellers' algorithm searches approximately for a substring in a text while the algorithm of Wagner and Fisher calculates [[Levenshtein distance]], being appropriate for dictionary fuzzy search only. \n\nOn-line searching techniques have been repeatedly improved. Perhaps the most \nfamous improvement is the [[bitap algorithm]] (also known as the shift-or and shift-and algorithm), which is very efficient for relatively short pattern strings. The Bitap algorithm is the heart of the [[Unix]] searching [[programming tool|utility]] [[agrep]]. A review of on-line searching algorithms was done by G. Navarro.{{ref|Nav01}}\n\nAlthough very fast on-line techniques exist, their performance on large data is unacceptable. \nText preprocessing or [[index (search engine)|indexing]] makes searching dramatically faster.\nToday, a variety of indexing algorithms have been presented. Among them are [[suffix tree]]s{{ref|Gus97}}, [[metric tree]]s{{ref|NB98}} and [[n-gram]] methods.{{ref|NBST01}}{{ref|Zob95}} \nA detailed survey of indexing techniques that allows one to find an arbitrary substring in a text is given by Navarro ''et al.''{{ref|NBST01}} A computational survey of dictionary methods (i.e., methods that permit finding all dictionary words that approximately match a search pattern) is given by Boytsov{{ref|B11}}.\n\n== Applications ==\nThe most common application of approximate matchers until recently has been [[spell checking]].{{ref|Gus97}} With the availability of large amounts of DNA data, matching of [[nucleotide]] sequences has become an important application.{{ref|CRMN01}} Approximate matching is also used in [[spam filtering]].{{ref|Gus97}} String matching cannot be used for most binary data, such as images and music.  They require different algorithms, such as [[acoustic fingerprint]]ing.\n\n== See also ==\n* [[Concept search]]\n* [[Jaro–Winkler distance]]\n* [[Levenshtein distance]]\n* [[Locality-sensitive hashing]]\n* [[Metaphone]]\n* [[Needleman–Wunsch algorithm]]\n* [[Plagiarism detection]]\n* [[Regular expressions]] for fuzzy and non-fuzzy matching\n* [[Smith–Waterman algorithm]]\n* [[Soundex]]\n* [[String metric]]\n\n==References==\n{{reflist}}\n{{refbegin}}\n* {{note|BN96}} {{cite conference | last1=Baeza-Yates | first1=R. | last2=Navarro | first2=G. |title=A faster algorithm for approximate string matching |editor1=Dan Hirchsberg |editor2=Gene Myers |booktitle=Combinatorial Pattern Matching (CPM'96), LNCS 1075  |pages=1–23  |date=June 1996 |location=Irvine, CA |url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.42.1593&rep=rep1&type=pdf#page=119}}\n* {{note|NB98}} {{cite conference | last1=Baeza-Yates | first1=R. | last2=Navarro | first2=G. |title=Fast Approximate String Matching in a Dictionary |booktitle=Proc. SPIRE'98  |pages=14–22 |publisher=IEEE CS Press |url=http://www.dcc.uchile.cl/~gnavarro/ps/spire98.2.pdf}}\n* {{note|B11}} {{cite journal |last1=Boytsov | first1=Leonid |title=Indexing methods for approximate dictionary searching: Comparative analysis|journal=Jea ACM|volume=16 |issue=1 |pages=1–91 |year=2011|doi=10.1145/1963190.1963191}}\n*{{note|CRMN01}}{{cite book |last=Cormen |first=Thomas |authorlink=Thomas Cormen |author2=Leiserson, Rivest |title=Introduction to Algorithms |edition=2nd |year=2001 |publisher=MIT Press |isbn=978-0-262-03293-3 |pages=364–7}}\n*{{note|GLL01}}{{cite book | last1=Galil | first1=Zvi | last2=Apostolico | first2=Alberto |title=Pattern matching algorithms |publisher=Oxford University Press |location=Oxford [Oxfordshire] |year=1997 |isbn=978-0-19-511367-9 }}\n* {{note|Gus97}}{{cite book | last1=Gusfield | first1=Dan |title=Algorithms on strings, trees, and sequences: computer science and computational biology |publisher=Cambridge University Press |location=Cambridge, UK |year=1997 |isbn=978-0-521-58519-4 }}\n* {{note|M99}} {{cite journal | last1=Myers | first1=G. |title=A fast bit-vector algorithm for approximate string matching based on dynamic programming |journal=Journal of the ACM |volume=46 |issue=3 |pages=395–415 |date=May 1999 |doi=10.1145/316542.316550 }}\n* {{note|Nav01}} {{cite journal | last1=Navarro | first1=Gonzalo |title=A guided tour to approximate string matching |journal=ACM Computing Surveys |volume=33 |issue=1 |pages=31–88 |year=2001 |citeseerx=10.1.1.96.7225 |doi=10.1145/375360.375365}}\n* {{note|NBST01}} {{cite journal | last1=Navarro | first1=Gonzalo | first2=Ricardo | last2=Baeza-Yates | first3=Erkki | last3=Sutinen | first4=Jorma | last4=Tarhio |title=Indexing Methods for Approximate String Matching |journal=IEEE Data Engineering Bulletin |volume=24 |issue=4 |pages=19–27 |year=2001 |url=http://www.dcc.uchile.cl/~gnavarro/ps/deb01.pdf}}\n* {{note|Sel80}} {{cite journal |last1=Sellers |first1=Peter H. |title=The Theory and Computation of Evolutionary Distances: Pattern Recognition |journal=Journal of Algorithms |volume=1 |pages=359–73 |year=1980 |doi=10.1016/0196-6774(80)90016-4 |issue=4 }}\n*{{note|SKN01}}{{cite book |last=Skiena |first=Steve |title=Algorithm Design Manual |edition=1st |year=1998 |publisher=Springer |isbn=978-0-387-94860-7}}\n* {{note|Ukk85}} {{cite journal | last1=Ukkonen | first1=E. |title=Algorithms for approximate string matching |journal=Information and Control |volume=64 | issue=1–3 |pages=100–18 |year=1985 |doi=10.1016/S0019-9958(85)80046-2 }}\n* {{note|WF74}} {{cite journal | last1=Wagner | first1=R. | last2=Fischer | first2=M. | title=The string-to-string correction problem | journal=Journal of the ACM |volume=21 |pages=168–73 |year=1974 |doi=10.1145/321796.321811}}\n* {{note|Zob95}} {{cite journal | first1=Justin | last1=Zobel | first2=Philip | last2=Dart | title=Finding approximate matches in large lexicons | journal=Software-Practice & Experience | volume=25 | issue=3 | pages=331–345 | year=1995 | doi=10.1002/spe.4380250307 | citeseerx=10.1.1.14.3856 }}\n{{refend}}\n\n== External links ==\n* [http://flamingo.ics.uci.edu Flamingo Project]\n* [https://web.archive.org/web/20160319053237/http://www.cse.unsw.edu.au/~weiw/project/simjoin.html Efficient Similarity Query Processing Project] with recent advances in approximate string matching based on an edit distance threshold. \n* [http://rockymadden.com/stringmetric/ StringMetric project] a [[Scala programming language|Scala]] library of string metrics and phonetic algorithms\n* [https://github.com/NaturalNode/natural Natural project] a [[JavaScript]] natural language processing library which includes implementations of popular string metrics\n\n\n{{strings}}\n\n{{DEFAULTSORT:Approximate String Matching}}\n[[Category:String matching algorithms|*]]\n[[Category:Pattern matching]]\n[[Category:Dynamic programming]]"
    },
    {
      "title": "Automatic basis function construction",
      "url": "https://en.wikipedia.org/wiki/Automatic_basis_function_construction",
      "text": "{{multiple issues|{{orphan|date=December 2011}}\n{{context|date=January 2012}}\n{{technical|date=March 2019}}}}\n\n'''Automatic basis function construction''' (or '''basis discovery''') is the mathematical method of looking for a set of task-independent [[basis function]]s that map the [[State space|state space]] to a lower-dimensional embedding, while still representing the [[Bellman equation|value function]] accurately. Automatic basis construction is independent of prior knowledge of the domain, which allows it to perform well where expert-constructed basis functions are difficult or impossible to create.\n\n==Motivation==\n\nIn [[reinforcement learning]] (RL), most real-world [[Markov Decision Process]] (MDP) problems have large or continuous state spaces, which typically require some sort of approximation to be represented efficiently.\n\n [https://en.wikipedia.org/wiki/Linear_function Linear function] approximators<ref name=keller06>Keller,Philipp;Mannor,Shie;[[Doina Precup|Precup,Doina]]. (2006) Automatic Basis Function Construction for Approximate Dynamic Programming and Reinforcement Learning. Proceedings of the 23rd International Conference on Machine Learning, Pittsburgh, PA.</ref> (LFAs) are widely adopted for their low theoretical complexity. Two sub-problems needs to be solved for better approximation: weight optimization and basis construction. To solve the second problem, one way is to design special basis functions. Those basis functions work well in specific tasks but are significantly restricted to domains. Thus constructing basis construction functions automatically is preferred for broader applications.{{Citation needed|date=December 2011}}\n\n==Problem definition==\nA Markov decision process with finite state space and fixed policy is defined with a 5-tuple <math>{s,a,p,\\gamma,r}</math>, which includes the finite state space <math>S={{1,2,\\ldots,s}}</math>, the finite action space <math>A</math>, the reward function <math>r</math>, discount factor <math>\\gamma\\in [0,1)</math>, and the transition model <math>P</math>.\n\nBellman equation is defined as:\n\n: <math>v=r+\\gamma Pv. \\,</math>\n\nWhen the number of elements in <math>S</math> is small, <math>v</math> is usually maintained as tabular form. While <math>S</math> grows too large for this kind of representation. <math>v</math> is commonly being approximated via a linear combination of basis function <math>\\Phi={\\phi_1,\\phi_2,\\ldots,\\phi_n}</math>,<ref name=sutton_barto>Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction.(1998) MIT Press, Cambridge, MA, chapter 8</ref> so that we have:\n\n: <math>v\\approx\\hat{v}=\\sum_{i=1}^n\\theta_n\\phi_{n}</math>\n\nHere <math>\\Phi</math> is a <math>|S|\\times n</math> matrix in which every row contains a [[Feature (machine learning)|feature vector]] for corresponding row, <math>\\theta</math> is a weight vector with n parameters and usually <math>n\\ll |s|</math>.\n\nBasis construction looks for ways to automatically construct better basis function <math>\\Phi</math> which can represent the value function well. \n\nA good construction method should have the following characteristics:\n\n* Small error bounds between the estimate and real value function\n* Form [[Orthogonal basis|orthogonal basis]] in the value function space\n* Converge to stationary value function fast\n\n==Popular methods==\n\n===Proto-value basis===\n\nIn this approach, Mahadevan analyzes the connectivity graph between states to determine a set of basis functions.<ref name=mahadevan05/>\n\nThe normalized graph Laplacian is defined as:\n\n: <math>L=I-D^{-\\frac{1}{2}}WD^{-\\frac{1}{2}}</math>\n\nHere W is an [[Adjacency matrix|adjacency matrix]] which represents the states of fixed policy MDP which forms an [[Graph (discrete mathematics)|undirected graph]] (N,E). D is a [[Diagonal matrix|diagonal matrix]] related to nodes' degrees.\n\nIn discrete state space, the adjacency matrix <math>W</math> could be constructed by simply checking whether two states are connected, and D could be calculated by summing up every row of W. In continuous state space, we could take [[random walk]] Laplacian of W.\n\nThis spectral framework can be used for value function approximation (VFA). Given the fixed policy, the edge weights are determined by corresponding states' transition probability. To get smooth value approximation, [[Diffusion wavelets|diffusion wavelets]] are used.<ref name=mahadevan05>Mahadevan,Sridhar;Maggioni,Mauro. (2005) Value function approximation with diffusion wavelets and Laplacian eigenfuctions. Proceedings of Advances in Neural Information Processing Systems.</ref>\n\n===Krylov basis===\n\nKrylov basis construction uses the actual transition matrix instead of random walk Laplacian. The assumption of this method is that transition model ''P'' and reward ''r'' are available.\n\nThe vectors in Neumann series are denoted as \n<math>y_i=P^ir</math> for all <math>i\\in[0,infty)</math>.\n\nIt shows that Krylov space spanned by <math>y_0,y_1,\\ldots,y_{m-1}</math> is enough to represent any value function,<ref name=\"Ipsen_Meyer\">[[Ilse Ipsen|Ilse C. F. Ipsen]] and Carl D. Meyer. The idea behind Krylov methods. American Mathematical Monthly, 105(10):889–899, 1998.</ref> and m is the degree of minimal polynomial of <math>(I-\\gamma P)</math>.\n\nSuppose the minimal polynomial is <math>p(A)=\\frac{1}{\\alpha_0}\\sum_{i=0}^{m-1}\\alpha_{i+1}A^i</math>, and we have <math>BA=I</math>, the value function can be written as:\n\n: <math>v=Br=\\frac{1}{\\alpha_0}\\sum_{i=0}^{m-1}\\alpha_{i+1}(I-\\gamma P)^ir=\\sum_{i=0}^{m-1}\\alpha_{i+1}\\beta_i y_i.</math><ref name=krylov />\n\n:'''Algorithm''' Augmented Krylov Method<ref name=krylov>M. Petrik. An analysis of Laplacian methods for value function approximation in MDPs. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pages 2574–2579, 2007</ref>\n:<math>z_1,z_2,\\ldots,z_k</math> are top real [[Eigenvalues and eigenvectors|eigenvectors]] of P\n:<math>z_{k+1}:=r</math>\n:''for'' <math>i:=1:(l+k)</math> ''do''\n::''if'' <math>i>k+1</math> ''then''\n:::<math>z_i:=Pz_{i-1}</math>;\n::''end if''\n::''for'' <math>j:=1:(i-1)</math> ''do''\n:::<math>z_i:=z_i-<z_j,z_i>z_j;</math>\n::''end for''\n::''if'' <math>\\parallel z_i\\parallel\\approx 0</math> ''then''\n:::''break'';\n::''end if''\n:''end for''\n:* k: number of eigenvectors in basis\n:* l: total number of vectors\n\n===Bellman error basis===\nBellman error (or BEBFs) is defined as: <math>\\varepsilon=r+\\gamma P\\hat{v}-\\hat{v}=r+\\gamma P\\Phi\\theta-\\Phi\\theta</math>.\n\nLoosely speaking, Bellman error points towards the optimal value function.<ref name=parr07>R. Parr, C. Painter-Wakefield, L.-H. Li, and M. Littman. Analyzing feature generation for value-function approximation. In ICML’07, 2007.</ref> The sequence of BEBF form a basis space which is orthogonal to the real value function space; thus with sufficient number of BEBFs, any value function can be represented exactly.\n:'''Algorithm''' BEBF\n:stage stage i=1, <math>\\phi_{1}=r</math>;\n:stage <math>i\\in[2,N]</math>\n::compute the weight vector <math>\\theta_i</math> according to current basis function <math>\\Phi_i</math>;\n::compute new bellman error by <math>\\varepsilon=r+\\gamma P \\Phi_{i}\\theta_{i}-\\Phi_{i}\\theta_{i}</math>;\n::add bellman error to form new basis function: <math>\\Phi_{i+1}=[\\Phi_{i}:\\varepsilon]</math>;\n:* N represents the number of iterations till convergence.\n:* \":\" means juxtaposing matrices or vectors.\n\n===Bellman average reward bases===\nBellman Average Reward Bases (or BARBs)<ref name=mahadevan10>S. Mahadevan and B. Liu. Basis construction from power series expansions of value functions. In NIPS’10, 2010</ref> is similar to Krylov Bases, but the reward function is being dilated by the average adjusted transition matrix <math>P-P^*</math>. Here <math>P^*</math> can be calculated by many methods in.<ref name=willian97>William J. Stewart. Numerical methods for computing stationary distributions of finite irreducible [[Markov chain]]s. In Advances in Computational Probability. Kluwer Academic Publishers, 1997.</ref>\n\nBARBs converges faster than BEBFs and Krylov when <math>\\gamma</math> is close to 1.\n:'''Algorithm''' BARBs\n:stage stage i=1, <math>P^*r</math>;\n:stage <math>i\\in[2,N]</math>\n::compute the weight vector <math>\\theta_i</math> according to current basis function <math>\\Phi_i</math>;\n::compute new basis: <math>:\\phi_{i+1}=r-P^*r+P\\Phi_{i}\\theta_i-\\Phi_{i}\\theta_i</math>, and add it to form new bases matrix<math>\\Phi_{i+1}=[\\Phi_{i}:\\phi_{i+1}]</math>;\n:* N represents the number of iterations till convergence.\n:* \":\" means juxtaposing matrices or vectors.\n\n==Discussion and analysis==\nThere are two principal types of basis construction methods.\n\nThe first type of methods are reward-sensitive, like Krylov and BEBFs; they dilate the reward function geometrically through transition matrix. However, when discount factor <math>\\gamma</math> approaches to 1, Krylov and BEBFs converge slowly. This is because the error Krylov based methods are restricted by Chebyshev polynomial bound.<ref name=krylov/> To solve this problem, methods such as BARBs are proposed. BARBs is an incremental variant of Drazin bases, and converges faster than Krylov and BEBFs when <math>\\gamma</math> becomes large.\n\nAnother type is reward-insensitive proto value basis function derived from graph Lapalacian. This method uses graph information, but the construction of adjacency matrix makes this method hard to analyze.<ref name=krylov />\n\n==See also==\n\n* [[Dynamic programming]]\n* [[Bellman equation]]\n* [[Optimal control]]\n\n==References==\n\n{{Reflist}}\n\n== External links ==\n* [http://www-all.cs.umass.edu/] UMASS ALL lab\n\n[[Category:Optimal decisions]]\n[[Category:Dynamic programming]]\n[[Category:Stochastic control]]"
    },
    {
      "title": "Backward induction",
      "url": "https://en.wikipedia.org/wiki/Backward_induction",
      "text": "'''Backward induction''' is the process of reasoning backwards in time, from the end of a problem or situation, to determine a sequence of optimal actions. It proceeds by first considering the last time a decision might be made and choosing what to do in any situation at that time. Using this information, one can then determine what to do at the second-to-last time of decision. This process continues backwards until one has determined the best action for every possible situation (i.e. for every possible [[information set (game theory)|information set]]) at every point in time. It was first used by [[Ernst Zermelo|Zermelo]] in 1913, to prove that chess has pure optimal strategies.<ref>{{Cite web|url=https://www.ethz.ch/content/dam/ethz/special-interest/gess/chair-of-sociology-dam/documents/articles/Zermelo_Uber_eine_Anwendung_der_Mengenlehre_auf_die_Theorie_des_Schachspiels.pdf|title=Über eine Anwendung der Mengenlehre auf die Theorie des Schachspiels|last=Von E.|first=Zermelo|date=1913|website=www.ethz.ch|archive-url=|archive-date=|dead-url=|access-date=2018-12-31}}</ref><ref name=\":0\" />\n\nIn the mathematical [[optimization (mathematics)|optimization]] method of [[dynamic programming]], backward induction is one of the main methods for solving the [[Bellman equation]].<ref>Jerome Adda and Russell Cooper, \"Dynamic Economics: Quantitative Methods and Applications\", Section 3.2.1, page 28. MIT Press, 2003.</ref><ref>Mario Miranda and Paul Fackler, \"Applied Computational Economics and Finance\", Section 7.3.1, page 164. MIT Press, 2002.</ref> In [[game theory]], backward induction is a method used to compute [[subgame perfection|subgame perfect equilibria]] in [[sequential game]]s.<ref>Drew Fudenberg and Jean Tirole, \"Game Theory\", Section 3.5, page 92. MIT Press, 1991.</ref> The only difference is that optimization involves just one [[Decision theory|decision maker]], who chooses what to do at each point of time, whereas game theory analyzes how the decisions of several [[Player (game)|player]]s interact. That is, by anticipating what the last player will do in each situation, it is possible to determine what the second-to-last player will do, and so on.  In the related fields of [[automated planning and scheduling]] and [[automated theorem proving]], the method is called '''backward search''' or '''[[backward chaining]]'''. In chess it is called [[retrograde analysis]].\n\nBackward induction has been used to solve games as long as the field of game theory has existed. [[John von Neumann]] and [[Oskar Morgenstern]] suggested solving [[zero-sum]], two-person games by backward induction in their ''Theory of Games and Economic Behavior'' (1944), the book which established game theory as a field of study.<ref>John von Neumann and Oskar Morgenstern, \"Theory of Games and Economic Behavior\", Section 15.3.1. Princeton University Press. [https://archive.org/details/theoryofgamesand030098mbp Third edition, 1953.] (First edition, 1944.)</ref><ref name=\":0\">[http://www-groups.dcs.st-and.ac.uk/~history/Projects/MacQuarrie/Chapters/Ch4.html Mathematics of Chess], webpage by John MacQuarrie.</ref>\n\n==Backward induction in decision making: an optimal-stopping problem==\n\nConsider an unemployed person who will be able to work for ten more years ''t'' = 1,2,...,10. Suppose that each year in which he remains unemployed, he may be offered a 'good' job that pays $100, or a 'bad' job that pays $44, with equal probability (50/50). Once he accepts a job, he will remain in that job for the rest of the ten years. (Assume for simplicity that he cares only about his monetary earnings, and that he values earnings at different times equally, i.e., the [[Time preference|discount rate]] is zero.)\n\nShould this person accept bad jobs? To answer this question, we can reason backwards from time ''t'' = 10.\n*At time 10, the value of accepting a good job is $100; the value of accepting a bad job is $44; the value of rejecting the job that is available is zero. Therefore, if he is still unemployed in the last period, he should accept whatever job he is offered at that time.\n*At time 9, the value of accepting a good job is $200 (because that job will last for two years); the value of accepting a bad job is 2*$44 = $88. The value of rejecting a job offer is $0 now, plus the value of waiting for the next job offer, which will either be $44 with 50% probability or $100 with 50% probability, for an average ('expected') value of 0.5*($100+$44) = $72. Therefore, regardless of whether the job available at time 9 is good or bad, it is better to accept that offer than wait for a better one.\n*At time 8, the value of accepting a good job is $300 (it will last for three years); the value of accepting a bad job is 3*$44 = $132. The value of rejecting a job offer is $0 now, plus the value of waiting for a job offer at time 9. Since we have already concluded that offers at time 9 should be accepted, the expected value of waiting for a job offer at time 9 is 0.5*($200+$88) = $144. Therefore, at time 8, it is more valuable to wait for the next offer than to accept a bad job.\n\nIt can be verified by continuing to work backwards that bad offers should only be accepted if one is still unemployed at times 9 or 10; they should be rejected at all times up to ''t'' = 8. The intuition is that if one expects to work in a job for a long time, this makes it more valuable to be picky about what job to accept.\n\nA dynamic optimization problem of this kind is called an [[optimal stopping]] problem, because the issue at hand is when to stop waiting for a better offer. [[Search theory]] is the field of microeconomics that applies problems of this type to contexts like shopping, job search, and marriage.\n\n==Backward induction in game theory: the ultimatum game==\n\nConsider the [[ultimatum game]], where one player proposes to split a dollar with another.  The first player (the proposer) suggests a division of the dollar between the two players.  The second player is then given the option to either accept the split or reject it.  If the second player accepts, both get the amount suggested by the proposer.  If rejected, neither receives anything.\n\nConsider the actions of the second player given any arbitrary proposal by the first player (that gives the second player more than zero).  Since the only choice the second player has at each of these points in the game is to choose between something and nothing, one can expect that the second will accept.  Given that the second will accept all proposals offered by the first (that give the second anything at all), the first ought to propose giving the second as little as possible.  This is the unique [[subgame perfect equilibrium]] of the Ultimatum Game.  (However, the Ultimatum Game does have several other [[Nash equilibrium|Nash equilibria]] which are not subgame perfect.)\n\nSee also [[centipede game]].\n\n== Backward induction in economics: the entry-decision problem ==\n\nConsider a [[dynamic game]] in which the players are an incumbent firm in an industry and a potential entrant to that industry. As it stands, the incumbent has a [[monopoly]] over the industry and does not want to lose some of its market share to the entrant. If the entrant chooses not to enter, the payoff to the incumbent is high (it maintains its monopoly) and the entrant neither loses nor gains (its payoff is zero). If the entrant enters, the incumbent can \"fight\" or \"accommodate\" the entrant. It will fight by lowering its price, running the entrant out of business (and incurring exit costs &mdash; a negative payoff) and damaging its own profits. If it accommodates the entrant it will lose some of its sales, but a high price will be maintained and it will receive greater profits than by lowering its price (but lower than monopoly profits).\n\nConsider if the best response of the incumbent is to accommodate if the entrant enters. If the incumbent accommodates, the best response of the entrant is to enter (and gain profit). Hence the strategy profile in which the entrant enters and the incumbent accommodates if the entrant enters is a [[Nash equilibrium]] consistent with backward induction. However, if the incumbent is going to fight, the best response of the entrant is to not enter, and if the entrant does not enter, it does not matter what the incumbent chooses to do in the hypothetical case that the entrant does enter. Hence the strategy profile in which the incumbent fights if the entrant enters, but the entrant does not enter is also a Nash equilibrium. However, were the entrant to deviate and enter, the incumbent's best response is to accommodate—the threat of fighting is not credible. This second Nash equilibrium can therefore be eliminated by backward induction. \n\nFinding a Nash equilibrium in each decision-making process (subgame) constitutes as perfect subgame equilibria. Thus, these strategy profiles that depict subgame perfect equilibria exclude the possibility of actions like incredible threats that are used to \"scare off\" an entrant. If the incumbent threatens to start a price war [[Price war]] with an entrant, they are threatening to lower their prices from a monopoly price to slightly lower than the entrant's, which would be impractical, and incredible, if the entrant knew a price war would not actually happen since it would result in losses for both parties. Unlike a single agent optimization which includes equilibria that aren't feasible or optimal, a subgame perfect equilibrium accounts for the actions of another player, thus ensuring that no player reaches a subgame mistakenly. In this case, backwards induction yielding perfect subgame equilibria ensures that the entrant will not be convinced of the incumbent's threat knowing that it was not a best response in the strategy profile.<ref>Rust J. (2008) Dynamic Programming. In: Palgrave Macmillan (eds) The New Palgrave Dictionary of Economics. Palgrave Macmillan, London</ref>\n\n== Backward induction paradox: the unexpected hanging ==\n\nThe [[unexpected hanging paradox]] is a [[paradox]] related to backward induction.  Suppose a prisoner is told that she will be hanged sometime between Monday and Friday of next week.  However, the exact day will be a surprise (i.e. she will not know the night before that she will be executed the next day).  The prisoner, interested in outsmarting her executioner, attempts to determine which day the execution will occur.\n\nShe reasons that it cannot occur on Friday, since if it had not occurred by the end of Thursday, she would know the execution would be on Friday.  Therefore, she can eliminate Friday as a possibility.  With Friday eliminated, she decides that it cannot occur on Thursday, since if it had not occurred on Wednesday, she would know that it had to be on Thursday.  Therefore, she can eliminate Thursday.  This reasoning proceeds until she has eliminated all possibilities.  She concludes that she will not be hanged next week.\n\nTo her surprise, she is hanged on Wednesday. She made the mistake of assuming that she knew definitively whether the unknown future factor that would cause her execution was one that she could reason about.\n\nHere the prisoner reasons by backward induction, but seems to come to a false conclusion. Note, however, that the description of the problem assumes it is possible to surprise someone who is performing backward induction. The mathematical theory of backward induction does not make this assumption, so the paradox does not call into question the results of this theory. Nonetheless, this paradox has received some substantial discussion by philosophers.\n\n== Backward induction and common knowledge of rationality ==\nBackward induction works only if both players are [[Rationality|rational]], i.e, always select an action that maximizes their payoff. However, rationality is not enough: each player should also believe that all other players are rational. Even this is not enough: each player should believe that all other players know that all other players are rational. And so on ad infinitum. In other words, rationality should be [[common knowledge (logic)|common knowledge]].<ref>{{Cite journal|last=[[Yisrael Aumann]]|date=1995-01-01|title=Backward induction and common knowledge of rationality|url=https://www.sciencedirect.com/science/article/pii/S0899825605800156|journal=Games and Economic Behavior|language=en|volume=8|issue=1|pages=6–19|doi=10.1016/S0899-8256(05)80015-6|issn=0899-8256|via=}}</ref>\n\n==Notes==\n{{reflist}}\n\n{{Game theory}}\n\n[[Category:Dynamic programming]]\n[[Category:Game theory]]\n[[Category:Inductive reasoning]]"
    },
    {
      "title": "Bellman equation",
      "url": "https://en.wikipedia.org/wiki/Bellman_equation",
      "text": "{{short description|A necessary condition for optimality associated with dynamic programming}}\n{{Inline|date=April 2018}}\n\nA '''Bellman equation''', named after [[Richard E. Bellman]], is a [[necessary condition]] for optimality associated with the mathematical [[Optimization (mathematics)|optimization]] method known as [[dynamic programming]].<ref>{{cite book |first=Avinash K. |last=Dixit |title=Optimization in Economic Theory |location=Oxford |publisher=Oxford University Press |edition=Second |year=1990 |isbn=0-19-877211-4 |page=164 |url=https://books.google.com/books?id=dHrsHz0VocUC&pg=PA164 }}</ref> It writes the \"value\" of a decision problem at a certain point in time in terms of the payoff from some initial choices and the \"value\" of the remaining decision problem that results from those initial choices.{{Citation needed|date=September 2017}} This breaks a dynamic optimization problem into a [[sequence]] of simpler subproblems, as [[Bellman equation#Bellman's Principle of Optimality|Bellman's “principle of optimality”]] prescribes.<ref>{{cite book |first=Donald E. |last=Kirk |title=Optimal Control Theory: An Introduction |location=Englewood Cliffs, NJ |publisher=Prentice-Hall |year=1970 |isbn=0-13-638098-0 |page=55 |url=https://books.google.com/books?id=fCh2SAtWIdwC&pg=PA55 }}</ref>\n\nThe Bellman equation was first applied to engineering [[control theory]] and to other topics in applied mathematics, and subsequently became an important tool in [[economic theory]]; though the basic concepts of dynamic programming are prefigured in [[John von Neumann]] and [[Oskar Morgenstern]]'s ''[[Theory of Games and Economic Behavior]]'' and [[Abraham Wald]]'s ''[[sequential analysis]]''.{{Citation needed|date=September 2017}}\n\nAlmost any problem that can be solved using [[optimal control theory]] can also be solved by analyzing the appropriate Bellman equation.{{Why|date=September 2017}}{{Explain|date=September 2017}} However, the term 'Bellman equation' usually refers to the dynamic programming equation associated with [[discrete-time]] optimization problems.<ref>{{cite book |first=Donald E. |last=Kirk |title=Optimal Control Theory: An Introduction |location=Englewood Cliffs, NJ |publisher=Prentice-Hall |year=1970 |isbn=0-13-638098-0 |page=70 |url=https://books.google.com/books?id=fCh2SAtWIdwC&pg=PA70 }}</ref> In continuous-time optimization problems, the analogous equation is a [[partial differential equation]] that is usually called the [[Hamilton–Jacobi–Bellman equation]].<ref>{{cite book |first=Morton I. |last=Kamien |authorlink=Morton Kamien |first2=Nancy L. |last2=Schwartz |title=Dynamic Optimization: The Calculus of Variations and Optimal Control in Economics and Management |location=Amsterdam |publisher=Elsevier |edition=Second |year=1991 |isbn=0-444-01609-0 |page=261 |url=https://books.google.com/books?id=liLCAgAAQBAJ&pg=PA261 }}</ref><ref>{{cite book |first=Donald E. |last=Kirk |title=Optimal Control Theory: An Introduction |location=Englewood Cliffs, NJ |publisher=Prentice-Hall |year=1970 |isbn=0-13-638098-0 |page=88 |url=https://books.google.com/books?id=fCh2SAtWIdwC&pg=PA88 }}</ref>\n\n== Analytical concepts in dynamic programming ==\nTo understand the Bellman equation, several underlying concepts must be understood. First, any optimization problem has some objective: minimizing travel time, minimizing cost, maximizing profits, maximizing utility, etc. The mathematical function that describes this objective is called the ''[[Optimization (mathematics)#Optimization problems|objective function]]''.\n\nDynamic programming breaks a multi-period planning problem into simpler steps at different points in time. Therefore, it requires keeping track of how the decision situation is evolving over time. The information about the current situation that is needed to make a correct decision is called the \"state\".<ref name=BellmanDP>Bellman, R.E. 1957. ''Dynamic Programming''. Princeton University Press, Princeton, NJ. Republished 2003: Dover, {{ISBN|0-486-42809-5}}.</ref><ref name=dreyfus>S. Dreyfus (2002), [http://www.wu-wien.ac.at/usr/h99c/h9951826/bellman_dynprog.pdf 'Richard Bellman on the birth of dynamic programming'] {{webarchive |url=https://web.archive.org/web/20050110161049/http://www.wu-wien.ac.at/usr/h99c/h9951826/bellman_dynprog.pdf |date=January 10, 2005 }} ''Operations Research'' 50 (1), pp. 48–51.</ref> For example, to decide how much to consume and spend at each point in time, people would need to know (among other things) their initial wealth. Therefore, wealth <math>(W)</math> would be one of their ''[[state variable]]s'', but there would probably be others.\n\nThe variables chosen at any given point in time are often called the ''[[control variable (programming)|control variables]]''. For example, given their current wealth, people might decide how much to consume now. Choosing the control variables now may be equivalent to choosing the next state; more generally, the next state is affected by other factors in addition to the current control. For example, in the simplest case, today's wealth (the state) and consumption (the control) might exactly determine tomorrow's wealth (the new state), though typically other factors will affect tomorrow's wealth too.\n\nThe dynamic programming approach describes the optimal plan by finding a rule that tells what the controls should be, given any possible value of the state. For example, if consumption (''c'') depends ''only'' on wealth (''W''), we would seek a rule  <math>c(W)</math> that gives consumption as a function of wealth. Such a rule, determining the controls as a function of the states, is called a ''policy function'' (See Bellman, 1957, Ch. III.2).<ref name=BellmanDP />\n\nFinally, by definition, the optimal decision rule is the one that achieves the best possible value of the objective. For example, if someone chooses consumption, given wealth, in order to maximize happiness (assuming happiness ''H'' can be represented by a mathematical function, such as a [[utility]] function and is something defined by wealth), then each level of wealth will be associated with some highest possible level of happiness, <math>H(W)</math>. The best possible value of the objective, written as a function of the state, is called the ''value function''.\n\n[[Richard Bellman]] showed that a dynamic [[Optimization (mathematics)|optimization]] problem in [[discrete time]] can be stated in a [[recursion|recursive]], step-by-step form known as [[backward induction]] by writing down the relationship between the value function in one period and the value function in the next period. The relationship between these two value functions is called the \"Bellman equation\". In this approach, the optimal policy in the last time period is specified in advance as a function of the state variable's value at that time, and the resulting optimal value of the objective function is thus expressed in terms of that value of the state variable. Next, the next-to-last period's optimization involves maximizing the sum of that period's period-specific objective function and the optimal value of the future objective function, giving that period's optimal policy contingent upon the value of the state variable as of the next-to-last period decision. This logic continues recursively back in time, until the first period decision rule is derived, as a function of the initial state variable value, by optimizing the sum of the first-period-specific objective function and the value of the second period's value function, which gives the value for all the future periods. Thus, each period's decision is made by explicitly acknowledging that all future decisions will be optimally made.\n\n== Derivation ==\n\n=== A dynamic decision problem ===\nLet the state at time <math>t</math> be <math>x_t</math>. For a decision that begins at time 0, we take as given the initial state <math>x_0</math>. At any time, the set of possible actions depends on the current state; we can write this as <math> a_{t} \\in \\Gamma (x_t)</math>, where the action <math>a_t</math> represents one or more control variables. We also assume that the state changes from <math>x</math> to a new state <math>T(x,a)</math> when action <math>a</math> is taken, and that the current payoff from taking action <math>a</math> in state <math>x</math> is <math>F(x,a)</math>. Finally, we assume impatience, represented by a [[discount factor]] <math>0<\\beta<1</math>.\n\nUnder these assumptions, an infinite-horizon decision problem takes the following form:\n\n:<math> V(x_0) \\; = \\; \\max_{ \\left \\{ a_{t} \\right \\}_{t=0}^{\\infty} }  \\sum_{t=0}^{\\infty} \\beta^t F(x_t,a_{t}), </math>\n\nsubject to the constraints\n\n:<math> a_{t} \\in \\Gamma (x_t), \\; x_{t+1}=T(x_t,a_t), \\; \\forall t = 0, 1, 2, \\dots </math>\n\nNotice that we have defined notation <math>V(x_0)</math> to denote the optimal value that can be obtained by maximizing this objective function subject to the assumed constraints. This function is the ''value function''. It is a function of the initial state variable <math>x_0</math>, since the best value obtainable depends on the initial situation.\n\n=== Bellman's Principle of Optimality ===\nThe dynamic programming method breaks this decision problem into smaller subproblems. [[Richard Bellman]]'s ''principle of optimality'' describes how to do this:<blockquote>Principle of Optimality: An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. (See Bellman, 1957, Chap. III.3.)<ref name=BellmanDP /><ref name=dreyfus /><ref name=BellmanTheory>R Bellman, ''On the Theory of Dynamic Programming'', Proceedings of the National Academy of Sciences, 1952</ref></blockquote>\nIn computer science, a problem that can be broken apart like this is said to have [[optimal substructure]]. In the context of dynamic [[game theory]], this principle is analogous to the concept of [[subgame perfect equilibrium]], although what constitutes an optimal policy in this case is conditioned on the decision-maker's opponents choosing similarly optimal policies from their points of view.\n\nAs suggested by the ''principle of optimality'', we will consider the first decision separately, setting aside all future decisions (we will start afresh from time 1 with the new state <math>x_1 </math>). Collecting the future decisions in brackets on the right, the previous problem is equivalent to:{{Clarify|date=September 2017}}\n\n:<math> \\max_{ a_0 } \\left \\{ F(x_0,a_0)\n+ \\beta  \\left[ \\max_{ \\left \\{ a_{t} \\right \\}_{t=1}^{\\infty} }\n\\sum_{t=1}^{\\infty} \\beta^{t-1} F(x_t,a_{t}):\na_{t} \\in \\Gamma (x_t), \\; x_{t+1}=T(x_t,a_t), \\; \\forall t \\geq 1 \\right] \\right \\}</math>\n\nsubject to the constraints\n\n:<math> a_0 \\in \\Gamma (x_0), \\; x_1=T(x_0,a_0). </math>\n\nHere we are choosing <math>a_0</math>, knowing that our choice will cause the time 1 state to be <math>x_1=T(x_0,a_0)</math>. That new state will then affect the decision problem from time 1 on. The whole future decision problem appears inside the square brackets on the right.{{Clarify|date=September 2017}}{{Explain|date=September 2017}}\n\n=== The Bellman equation ===\nSo far it seems we have only made the problem uglier by separating today's decision from future decisions. But we can simplify by noticing that what is inside the square brackets on the right is ''the value'' of the time 1 decision problem, starting from state <math>x_1=T(x_0,a_0)</math>.\n\nTherefore, we can rewrite the problem as a [[Recursion|recursive]] definition of the value function:\n\n:<math>V(x_0) = \\max_{ a_0 } \\{ F(x_0,a_0) + \\beta V(x_1) \\} </math>, subject to the constraints: <math> a_0 \\in \\Gamma (x_0), \\; x_1=T(x_0,a_0). </math>\n\nThis is the Bellman equation. It can be simplified even further if we drop time subscripts and plug in the value of the next state:\n\n:<math>V(x) = \\max_{a \\in \\Gamma (x) } \\{ F(x,a) + \\beta V(T(x,a)) \\}.</math>\n\nThe Bellman equation is classified as a [[functional equation]], because solving it means finding the unknown function ''V'', which is the ''value function''. Recall that the value function describes the best possible value of the objective, as a function of the state ''x''. By calculating the value function, we will also find the function ''a''(''x'') that describes the optimal action as a function of the state; this is called the ''policy function''.\n\n=== In a stochastic problem ===\n{{See also|Markov decision process}}\n\nIn the deterministic setting, other techniques besides dynamic programming can be used to tackle the above [[optimal control]] problem. However, the Bellman Equation is often the most convenient method of solving ''stochastic'' optimal control problems.\n\nFor a specific example from economics, consider an infinitely-lived consumer with initial wealth endowment <math>{\\color{Red}a_0}</math> at period <math>0</math>. He has an instantaneous [[utility function]] <math>u(c)</math> where <math>c</math> denotes consumption and discounts the next period utility at a rate of <math>0< \\beta<1 </math>. Assume that what is not consumed in period <math>t</math> carries over to the next period with interest rate <math>r</math>. Then the consumer's utility maximization problem is to choose a consumption plan <math>\\{{\\color{OliveGreen}c_t}\\}</math> that solves\n\n:<math>\\max \\sum_{t=0} ^{\\infty} \\beta^t u ({\\color{OliveGreen}c_t})</math>\n\nsubject to\n\n:<math>{\\color{Red}a_{t+1}} = (1 + r) ({\\color{Red}a_t} - {\\color{OliveGreen}c_t}), \\; {\\color{OliveGreen}c_t} \\geq 0,</math>\n\nand\n\n:<math>\\lim_{t \\rightarrow \\infty} {\\color{Red}a_t} \\geq 0.</math>\n\nThe first constraint is the capital accumulation/law of motion specified by the problem, while the second constraint is a [[Transversality (mathematics)|transversality condition]] that the consumer does not carry debt at the end of his life. The Bellman equation is\n\n:<math>V(a) = \\max_{ 0 \\leq c \\leq a } \\{ u(c) + \\beta V((1+r) (a - c)) \\},</math>\n\nAlternatively, one can treat the sequence problem directly using, for example, the [[Hamiltonian (control theory)|Hamiltonian equations]].\n\nNow, if the interest rate varies from period to period, the consumer is faced with a stochastic optimization problem. Let the interest ''r'' follow a [[Markov process]] with probability transition function <math>Q(r, d\\mu_r)</math> where <math>d\\mu_r</math> denotes the [[probability measure]] governing the distribution of interest rate next period if current interest rate is <math>r</math>. The timing of the model is that the consumer decides his current period consumption after the current period interest rate is announced.\n\nRather than simply choosing a single sequence <math>\\{{\\color{OliveGreen}c_t}\\}</math>, the consumer now must choose a sequence <math>\\{{\\color{OliveGreen}c_t}\\}</math> for each possible realization of a <math>\\{r_t\\}</math> in such a way that his lifetime expected utility is maximized:\n\n:<math>\\max \\mathbb{E}\\bigg( \\sum_{t=0} ^{\\infty} \\beta^t u ({\\color{OliveGreen}c_t})   \\bigg).</math>\n\nThe expectation <math>\\mathbb{E}</math> is taken with respect to the appropriate probability measure given by ''Q'' on the sequences of ''r'' 's. Because ''r'' is governed by a Markov process, dynamic programming simplifies the problem significantly. Then Bellman equation is simply\n\n:<math>V(a, r) =  \\max_{ 0 \\leq c \\leq a } \\{ u(c) + \\beta \\int V((1+r) (a - c), r') Q(r, d\\mu_r) \\} .</math>\n\nUnder some reasonable assumption, the resulting optimal policy function ''g''(''a'',''r'') is [[measurable]].\n\nFor a general stochastic sequential optimization problem with Markovian shocks and where the agent is faced with his decision ''[[ex-post]]'', the Bellman equation takes a very similar form\n\n:<math>V(x, z) = \\max_{c \\in \\Gamma(x,z)} F(x, c, z) + \\beta \\int V( T(x,c), z') d\\mu_z(z'). </math>\n\n== Solution methods ==\n* The [[method of undetermined coefficients]], also known as 'guess and verify', can be used to solve some infinite-horizon, [[Autonomous system (mathematics)|autonomous]] Bellman equations.<ref>{{cite book |first=Lars |last=Ljungqvist |first2=Thomas J. |last2=Sargent |title=Recursive Macroeconomic Theory |location= |publisher=MIT Press |year=2004 |edition=2nd |pages=88–90 |isbn=0-262-12274-X |url=https://books.google.com/books?id=Xx-j-tYaPQUC&pg=PA88 }}</ref>\n* The Bellman equation can be solved by [[backwards induction]], either [[Closed-form expression|analytically]] in a few special cases, or [[numerical analysis|numerically]] on a computer. Numerical backwards induction is applicable to a wide variety of problems, but may be infeasible when there are many state variables, due to the [[curse of dimensionality]]. Approximate dynamic programming has been introduced by [[Dimitri Bertsekas|D. P. Bertsekas]] and [[John Tsitsiklis|J. N. Tsitsiklis]] with the use of [[artificial neural network]]s ([[multilayer perceptron]]s) for approximating the Bellman function.<ref name=\"NeuroDynProg\">{{cite book |last=Bertsekas |first=D. P. |last2=Tsitsiklis |first2=J. N. |title=Neuro-Dynamic Programming |location= |publisher=Athena Scientific |year=1996 }}</ref> This is an effective mitigation strategy for reducing the impact of dimensionality by replacing the memorization of the complete function mapping for the whole space domain with the memorization of the sole neural network parameters.\n* By calculating the first-order conditions associated with the Bellman equation, and then using the [[envelope theorem]] to eliminate the derivatives of the value function, it is possible to obtain a system of [[difference equation]]s or [[differential equation]]s called the '[[Euler–Lagrange equation|Euler equation]]s'.<ref>{{cite book |first=Jianjun |last=Miao |title=Economic Dynamics in Discrete Time |location= |publisher=MIT Press |year=2014 |page=134 |isbn=978-0-262-32560-8 |url=https://books.google.com/books?id=dh2EBAAAQBAJ&pg=PA134 }}</ref> Standard techniques for the solution of difference or differential equations can then be used to calculate the dynamics of the state variables and the control variables of the optimization problem.\n\n== Applications in economics ==\nThe first known application of a Bellman equation in economics is due to [[Martin Beckmann]] and [[Richard Muth]].<ref>{{cite journal |first=Martin |last=Beckmann |first2=Richard |last2=Muth |year=1954 |title=On the Solution to the ‘Fundamental Equation’ of inventory theory |work=Cowles Commission Discussion Paper 2116 |url=http://cowles.yale.edu/sites/default/files/files/pub/cdp/e-2116.pdf }}</ref> Martin Beckmann also wrote extensively on consumption theory using the Bellman equation in 1959. His work influenced [[Edmund S. Phelps]], among others.\n\nA celebrated economic application of a Bellman equation is [[Robert C. Merton]]'s seminal 1973 article on the [[ICAPM|intertemporal capital asset pricing model]].<ref>{{cite journal |first=Robert C. |last=Merton |year=1973 |title=An Intertemporal Capital Asset Pricing Model |journal=[[Econometrica]] |volume=41 |issue=5 |pages=867–887 |jstor=1913811 }}</ref> (See also [[Merton's portfolio problem]]).The solution to Merton's theoretical model, one in which investors chose between income today and future income or capital gains, is a form of Bellman's equation. Because economic applications of dynamic programming usually result in a Bellman equation that is a [[difference equation]], economists refer to dynamic programming as a \"recursive method\" and a subfield of [[recursive economics]] is now recognized within economics.\n\n[[Nancy Stokey]], [[Robert E. Lucas]], and [[Edward Prescott]] describe stochastic and nonstochastic dynamic programming in considerable detail, and develop theorems for the existence of solutions to problems meeting certain conditions. They also describe many examples of modeling theoretical problems in economics using recursive methods.<ref>{{cite book |first=Nancy |last=Stokey |first2=Robert E. |last2=Lucas |first3=Edward |last3=Prescott |year=1989 |title=Recursive Methods in Economic Dynamics |location= |publisher=Harvard Univ. Press. |isbn=0-674-75096-9 }}</ref> This book led to dynamic programming being employed to solve a wide range of theoretical problems in economics, including optimal [[economic growth]], [[resource extraction]], [[principal–agent problem]]s, [[public finance]], business [[investment]], [[asset pricing]], [[factor of production|factor]] supply, and [[industrial organization]]. [[Lars Ljungqvist]] and [[Thomas Sargent]] apply dynamic programming to study a variety of theoretical questions in [[monetary policy]], [[fiscal policy]], [[taxation]], [[economic growth]], [[search theory]], and [[labor economics]].<ref>{{cite book |first=Lars |last=Ljungqvist |first2=Thomas |last2=Sargent |year=2012 |title=Recursive Macroeconomic Theory |location= |publisher=MIT Press |edition=Third |isbn=978-0-262-01874-6 }}</ref> [[Avinash Dixit]] and [[Robert Pindyck]] showed the value of the method for thinking about [[capital budgeting]].<ref>{{cite book |first=Avinash |last=Dixit |first2=Robert |last2=Pindyck |year=1994 |title=Investment Under Uncertainty |location= |publisher=Princeton Univ. Press |isbn=0-691-03410-9 }}</ref> Anderson adapted the technique to business valuation, including privately held businesses.<ref>Anderson, Patrick L., Business Economics & Finance, CRC Press, 2004 (chapter 10), {{ISBN|1-58488-348-0}}; The Value of Private Businesses in the United States, ''Business Economics'' (2009) 44, 87–108. {{doi|10.1057/be.2009.4}}. ''Economics of Business Valuation'', Stanford University Press (2013); {{ISBN|9780804758307}}. [http://www.sup.org/book.cgi?id=11400 Stanford Press] {{Webarchive|url=https://web.archive.org/web/20130808132733/http://www.sup.org/book.cgi?id=11400 |date=2013-08-08 }}</ref>\n\nUsing dynamic programming to solve concrete problems is complicated by informational difficulties, such as choosing the unobservable discount rate. There are also computational issues, the main one being the [[curse of dimensionality]] arising from the vast number of possible actions and potential state variables that must be considered before an optimal strategy can be selected. For an extensive discussion of computational issues, see Miranda and Fackler,<ref>Miranda, M., & Fackler, P., 2002. ''Applied Computational Economics and Finance''. MIT Press</ref> and Meyn 2007.<ref>S. P. Meyn, 2007.  [http://decision.csl.uiuc.edu/~meyn/pages/CTCN/CTCN.html Control Techniques for Complex Networks] {{Webarchive|url=https://web.archive.org/web/20080513165615/http://decision.csl.uiuc.edu/~meyn/pages/CTCN/CTCN.html |date=2008-05-13 }}, Cambridge University Press, 2007.  Appendix contains abridged [http://decision.csl.uiuc.edu/~meyn/pages/book.html Meyn & Tweedie] {{webarchive|url=https://web.archive.org/web/20071012194420/http://decision.csl.uiuc.edu/~meyn/pages/book.html |date=2007-10-12 }}.</ref>\n\n== Example ==\nIn [[Markov decision process]]es, a Bellman equation is a [[recursion]] for expected rewards. For example, the expected reward for being in a particular state ''s'' and following some fixed policy <math>\\pi</math> has the Bellman equation:\n\n:<math> V^\\pi(s)= R(s,\\pi(s)) + \\gamma \\sum_{s'} P(s'|s,\\pi(s)) V^\\pi(s').\\ </math>\n\nThis equation describes the expected reward for taking the action prescribed by some policy <math>\\pi</math>.\n\nThe equation for the optimal policy is referred to as the ''Bellman optimality equation'':\n\n:<math> V^{\\pi*}(s)=  \\max_a \\{ {R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^{\\pi*}(s')} \\}.\\ </math>\n\nwhere <math>{\\pi*}</math> is the optimal policy and <math>V^{\\pi*}</math> refers to the value function of the optimal policy. The equation above describes the reward for taking the action giving the highest expected return.\n\n== See also ==\n* {{annotated link|Bellman pseudospectral method}}\n* {{annotated link|Dynamic programming}}\n* {{annotated link|Hamilton–Jacobi–Bellman equation}}\n* {{annotated link|Markov decision process}}\n* {{annotated link|Optimal control|Optimal control theory}}\n* {{annotated link|Optimal substructure}}\n* {{annotated link|Recursive competitive equilibrium}}\n* {{annotated link|Stochastic dynamic programming}}\n\n== References ==\n{{Reflist|30em}}\n\n{{DEFAULTSORT:Bellman Equation}}\n[[Category:Equations]]\n[[Category:Dynamic programming]]\n[[Category:Control theory]]"
    },
    {
      "title": "Bellman–Ford algorithm",
      "url": "https://en.wikipedia.org/wiki/Bellman%E2%80%93Ford_algorithm",
      "text": "{{Infobox Algorithm\n|class=[[Single-source shortest path problem]] (for weighted directed graphs)\n|image=\n|caption = \n|data=[[Graph (data structure)|Graph]]\n|time=<math>\\Theta (|V| |E|)</math>\n|best-time=<math>\\Theta (|E|)</math>\n|space=<math>\\Theta (|V|)</math>\n}}\n\n{{Tree search algorithm}}\n\nThe '''Bellman–Ford algorithm''' is an [[algorithm]] that computes [[shortest path]]s from a single source [[vertex (graph theory)|vertex]] to all of the other vertices in a [[weighted digraph]].<ref name=Bang>{{harvtxt|Bang-Jensen|Gutin|2000}}</ref>\nIt is slower than [[Dijkstra's algorithm]] for the same problem, but more versatile, as it is capable of handling graphs in which some of the edge weights are negative numbers.\nThe algorithm was first proposed by {{harvs|first=Alfonso|last=Shimbel|year=1955|txt}}, but is instead named after [[Richard Bellman]] and [[L. R. Ford, Jr.|Lester Ford, Jr.]], who published it in [[#{{harvid|Bellman|1958}}|1958]] and [[#{{harvid|Ford|1956}}|1956]], respectively.<ref name=\"Schrijver\">{{harvtxt|Schrijver|2005}}</ref> [[Edward F. Moore]] also published the same algorithm in 1957, and for this reason it is also sometimes called the '''Bellman–Ford–Moore algorithm'''.<ref name=Bang />\n\nNegative edge weights are found in various applications of graphs, hence the usefulness of this algorithm.{{sfnp|Sedgewick|2002}}\nIf a graph contains a \"negative cycle\" (i.e. a [[cycle (graph theory)|cycle]] whose edges sum to a negative value) that is reachable from the source, then there is no ''cheapest'' path: any path that has a point on the negative cycle can be made cheaper by one more [[Walk (graph theory)|walk]] around the negative cycle. In such a case, the Bellman–Ford algorithm can detect and report the negative cycle.<ref name=Bang />{{sfnp|Kleinberg|Tardos|2006}}\n\n== Algorithm ==\n[[File:Bellman-Ford worst-case example.svg|thumb|In this example graph, assuming that A is the source and edges are processed in the worst order, from right to left, it requires the full {{math|&#124;''V''&#124;−1}} or 4 iterations for the distance estimates to converge. Conversely, if the edges are processed in the best order, from left to right, the algorithm converges in a single iteration.]]\n\nLike [[Dijkstra's algorithm]], Bellman–Ford proceeds by [[Relaxation (iterative method)|relaxation]], in which approximations to the correct distance are replaced by better ones until they eventually reach the solution. In both algorithms, the approximate distance to each vertex is always an overestimate of the true distance, and is replaced by the minimum of its old value and the length of a newly found path.\nHowever, Dijkstra's algorithm uses a [[Priority queue|priority queue]] to [[Greedy algorithm|greedily]] select the closest vertex that has not yet been processed, and performs this relaxation process on all of its outgoing edges; by contrast, the Bellman–Ford algorithm simply relaxes ''all'' the edges, and does this <math>|V|-1</math> times, where <math>|V|</math> is the number of vertices in the graph. In each of these repetitions, the number of vertices with correctly calculated distances grows, from which it follows that eventually all vertices will have their correct distances. This method allows the Bellman–Ford algorithm to be applied to a wider class of inputs than Dijkstra.\n\nBellman–Ford runs in <math>O(|V|\\cdot |E|)</math> [[Big O notation|time]], where <math>|V|</math> and <math>|E|</math> are the number of vertices and edges respectively.\n\n  '''function''' BellmanFord(''list'' vertices, ''list'' edges, ''vertex'' source)\n    ::distance[],predecessor[]\n    ''\n    ''// This implementation takes in a graph, represented as''\n    ''// lists of vertices and edges, and fills two arrays''\n    ''// (distance and predecessor) about the shortest path''\n    ''// from the source to each vertex''\n    ''\n    ''// Step 1: initialize graph''\n    '''for each''' vertex v '''in''' vertices:\n        distance[v] := '''inf'''             // Initialize the distance to all vertices to infinity\n        predecessor[v] := '''null'''         // And having a null predecessor\n    ''\n    distance[source] := 0              // The distance from the source to itself is, of course, zero\n    ''\n    ''// Step 2: relax edges repeatedly''\n    <!-- The outer loop iterates |V|+1 times. See, e.g, CLRS Ch. 24.1.  The value of i is unused. -->\n    '''for''' i '''from''' 1 '''to''' size(vertices)-1:\n        '''for each''' edge (u, v) '''with''' weight w '''in''' edges:\n            '''if''' distance[u] + w < distance[v]:\n                distance[v] := distance[u] + w\n                predecessor[v] := u\n    ''\n    ''// Step 3: check for negative-weight cycles''\n    '''for each''' edge (u, v) '''with''' weight w '''in''' edges:\n        '''if''' distance[u] + w < distance[v]:\n            '''error''' \"Graph contains a negative-weight cycle\"\n    ''\n    '''return''' distance[], predecessor[]\n\nSimply put, the algorithm initializes the distance to the source to 0 and all other nodes to infinity. Then for all edges, if the distance to the destination can be shortened by taking the edge, the distance is updated to the new lower value. At each iteration {{mvar|i}} that the edges are scanned, the algorithm finds all shortest paths of at most length {{mvar|i}} edges (and possibly some paths longer than {{mvar|i}} edges). Since the longest possible path without a cycle can be <math>|V|-1</math> edges, the edges must be scanned <math>|V|-1</math> times to ensure the shortest path has been found for all nodes. A final scan of all the edges is performed and if any distance is updated, then a path of length <math>|V|</math> edges has been found which can only occur if at least one negative cycle exists in the graph.\n\n== Proof of correctness ==\n{{Unreferenced section|date=March 2019}}\nThe correctness of the algorithm can be shown by [[mathematical induction|induction]]:\n\n'''Lemma'''. After ''i'' repetitions of ''for'' loop,\n* if Distance(''u'') is not infinity, it is equal to the length of some path from ''s'' to ''u''; and\n* if there is a path from ''s'' to ''u'' with at most ''i'' edges, then Distance(u) is at most the length of the shortest path from ''s'' to ''u'' with at most ''i'' edges.\n\n'''Proof'''. For the base case of induction, consider <code>i=0</code> and the moment before ''for'' loop is executed for the first time. Then, for the source vertex, <code>source.distance = 0</code>, which is correct. For other vertices ''u'', <code>u.distance = '''infinity'''</code>, which is also correct because there is no path from ''source'' to ''u'' with 0 edges.\n\nFor the inductive case, we first prove the first part. Consider a moment when a vertex's distance is updated by\n<code>v.distance := u.distance + uv.weight</code>. By inductive assumption, <code>u.distance</code> is the length of some path from ''source'' to ''u''. Then <code>u.distance + uv.weight</code> is the length of the path from ''source'' to ''v'' that follows the path from  ''source'' to ''u'' and then goes to ''v''.\n\nFor the second part, consider a shortest path ''P'' (there may be more than one) from ''source'' to ''u'' with at most ''i'' edges. Let ''v'' be the last vertex before ''u'' on this path. Then, the part of the path from ''source'' to ''v'' is a shortest path from ''source'' to ''v'' with at most ''i-1'' edges, since if it were not, then there must be some strictly shorter path from ''source'' to ''v'' with at most ''i-1'' edges, and we could then append the edge ''uv'' to this path to obtain a path with at most ''i'' edges that is strictly shorter than ''P''—a contradiction. By inductive assumption, <code>v.distance</code> after ''i''−1 iterations is at most the length of this path from ''source'' to ''v''. Therefore, <code>uv.weight + v.distance</code> is at most the length of ''P''. In the ''i<sup>th</sup>'' iteration, <code>u.distance</code> gets compared with <code>uv.weight + v.distance</code>, and is set equal to it if <code>uv.weight + v.distance</code> is smaller. Therefore, after ''i'' iterations, <code>u.distance</code> is at most the length of ''P'', i.e., the length of the shortest path from ''source'' to ''u'' that uses at most ''i'' edges.\n\nIf there are no negative-weight cycles, then every shortest path visits each vertex at most once, so at step 3 no further improvements can be made. Conversely, suppose no improvement can be made. Then for any cycle with vertices ''v''[0], ..., ''v''[''k''−1],\n\n<code>v[i].distance <= v[i-1 (mod k)].distance + v[i-1 (mod k)]v[i].weight</code>\n\nSumming around the cycle, the ''v''[''i''].distance and ''v''[''i''−1 (mod ''k'')].distance terms cancel, leaving\n\n<code>0 <= sum from 1 to k of v[i-1 (mod k)]v[i].weight</code>\n\nI.e., every cycle has nonnegative weight.\n\n== Finding negative cycles ==\nWhen the algorithm is used to find shortest paths, the existence of negative cycles is a problem, preventing the algorithm from finding a correct answer. However, since it terminates upon finding a negative cycle, the Bellman–Ford algorithm can be used for applications in which this is the target to be sought – for example in [[cycle-cancelling]] techniques in [[Flow network|network flow]] analysis.<ref name=\"Bang\" />\n\n== Applications in routing ==\n\nA distributed variant of the Bellman–Ford algorithm is used in [[distance-vector routing protocol]]s, for example the [[Routing Information Protocol]] (RIP). The algorithm is distributed because it involves a number of nodes (routers) within an [[autonomous system (Internet)|Autonomous system]], a collection of IP networks typically owned by an ISP.\nIt consists of the following steps:\n\n# Each node calculates the distances between itself and all other nodes within the AS and stores this information as a table.\n# Each node sends its table to all neighboring nodes.\n# When a node receives distance tables from its neighbors, it calculates the shortest routes to all other nodes and updates its own table to reflect any changes.\nThe main disadvantages of the Bellman–Ford algorithm in this setting are as follows:\n\n* It does not scale well.\n* Changes in [[network topology]] are not reflected quickly since updates are spread node-by-node.\n* [[Count to infinity#Count-to-infinity problem|Count to infinity]] if link or node failures render a node unreachable from some set of other nodes, those nodes may spend forever gradually increasing their estimates of the distance to it, and in the meantime there may be routing loops.\n\n== Improvements ==\nThe Bellman–Ford algorithm may be improved in practice (although not in the worst case) by the observation that, if an iteration of the main loop of the algorithm terminates without making any changes, the algorithm can be immediately terminated, as subsequent iterations will not make any more changes. With this early termination condition, the main loop may in some cases use many fewer than {{math|{{abs|''V''}}&nbsp;−&nbsp;1}} iterations, even though the worst case of the algorithm remains unchanged.\n\n{{harvtxt|Yen|1970}} described two more improvements to the Bellman–Ford algorithm for a graph without negative-weight cycles; again, while making the algorithm faster in practice, they do not change its <math>O(|V|\\cdot |E|)</math> worst case time bound. His first improvement reduces the number of relaxation steps that need to be performed within each iteration of the algorithm. If a vertex ''v'' has a distance value that has not changed since the last time the edges out of ''v'' were relaxed, then there is no need to relax the edges out of ''v'' a second time. In this way, as the number of vertices with correct distance values grows, the number whose outgoing edges that need to be relaxed in each iteration shrinks, leading to a constant-factor savings in time for [[dense graph]]s.\n\nYen's second improvement first assigns some arbitrary linear order on all vertices and then partitions the set of all edges into two subsets. The first subset, ''E<sub>f</sub>'', contains all edges (''v<sub>i</sub>'', ''v<sub>j</sub>'') such that ''i'' < ''j''; the second, ''E<sub>b</sub>'', contains edges (''v<sub>i</sub>'', ''v<sub>j</sub>'') such that ''i'' > ''j''. Each vertex is visited in the order <span class=\"texhtml\">''v<sub>1</sub>'', ''v<sub>2</sub>'', ..., ''v''<sub>|''V''|</sub></span>, relaxing each outgoing edge from that vertex in ''E<sub>f</sub>''. Each vertex is then visited in the order <span class=\"texhtml\">''v''<sub>|''V''|</sub>, ''v''<sub>|''V''|−1</sub>, ..., ''v''<sub>1</sub></span>, relaxing each outgoing edge from that vertex in ''E<sub>b</sub>''. Each iteration of the main loop of the algorithm, after the first one, adds at least two edges to the set of edges whose relaxed distances match the correct shortest path distances: one from ''E<sub>f</sub>'' and one from ''E<sub>b</sub>''. This modification reduces the worst-case number of iterations of the main loop of the algorithm from {{math|{{abs|''V''}}&nbsp;−&nbsp;1}} to <math>|V|/2</math>.<ref>Cormen et al., 2nd ed., Problem 24-1, pp. 614–615.</ref><ref name=Sedweb />\n\nAnother improvement, by {{harvtxt|Bannister|Eppstein|2012}}, replaces the arbitrary linear order of the vertices used in Yen's second improvement by a [[random permutation]]. This change makes the worst case for Yen's improvement (in which the edges of a shortest path strictly alternate between the two subsets ''E<sub>f</sub>'' and ''E<sub>b</sub>'') very unlikely to happen. With a randomly permuted vertex ordering, the [[expected value|expected]] number of iterations needed in the main loop is at most <math>|V|/3</math>.<ref name=Sedweb>See Sedgewick's [http://algs4.cs.princeton.edu/44sp/ web exercises] for ''Algorithms'', 4th ed., exercises 5 and 12 (retrieved 2013-01-30).</ref>\n\nIn China, an algorithm which adds a first-in first-out queue to the Bellman–Ford algorithm, known as [[SPFA]], published by Fanding Duan in 1994, is popular with students who take part in [[:zh:全国青少年信息学奥林匹克联赛|NOIP]] and [[ACM-ICPC]].<ref name=\"duan\">{{Citation\n| last=Duan\n| first=Fanding\n| year=1994\n| title=关于最短路径的SPFA快速算法\n| journal=Journal of Southwest Jiaotong University\n| volume=29\n| issue=2\n| pages=207–212\n| url=http://wenku.baidu.com/view/3b8c5d778e9951e79a892705.html\n}}</ref>\n\n== Notes ==\n{{Reflist}}\n\n== References ==\n\n=== Original sources ===\n*{{cite conference\n| last = Shimbel | first = A.\n| title = Structure in communication nets\n| location = New York, New York\n| pages = 199–203\n| publisher = Polytechnic Press of the Polytechnic Institute of Brooklyn\n| conference = Proceedings of the Symposium on Information Networks\n| year = 1955\n| ref = harv}}\n*{{cite journal\n | last = Bellman | first = Richard | authorlink = Richard Bellman\n | mr = 0102435\n | journal = Quarterly of Applied Mathematics\n | pages = 87–90\n | title = On a routing problem\n | volume = 16\n | year = 1958\n | ref = harv}}\n*{{cite book\n |authorlink=L. R. Ford, Jr. | last=Ford | first=Lester R. Jr.\n |title=Network Flow Theory\n |date=August 14, 1956\n |series=Paper P-923\n |publisher=RAND Corporation\n |location=Santa Monica, California\n |url=http://www.rand.org/pubs/papers/P923.html\n |ref = harv}}\n*{{cite conference\n | last = Moore | first = Edward F. | authorlink = Edward F. Moore\n | title = The shortest path through a maze\n | location = Cambridge, Massachusetts\n | mr = 0114710\n | pages = 285–292\n | publisher = Harvard Univ. Press\n | conference = Proc. Internat. Sympos. Switching Theory 1957, Part II\n | year = 1959\n | ref = harv}}\n*{{cite journal\n | last = Yen | first = Jin Y.\n | mr = 0253822\n | journal = Quarterly of Applied Mathematics\n | pages = 526–530\n | title = An algorithm for finding shortest routes from all source nodes to a given destination in general networks\n | volume = 27\n | year = 1970\n | ref = harv}}\n*{{cite conference|title=Randomized speedup of the Bellman–Ford algorithm|first1=M. J.|last1=Bannister|first2=D.|last2=Eppstein|author2-link=David Eppstein|arxiv=1111.5414|conference=Analytic Algorithmics and Combinatorics (ANALCO12), Kyoto, Japan|year=2012|pages=41–47|url=https://arxiv.org/pdf/1111.5414.pdf|ref=harv|bibcode=2011arXiv1111.5414B}}\n\n=== Secondary sources ===\n*{{Cite book|first1=Jørgen |last1=Bang-Jensen|first2=Gregory|last2=Gutin|year=2000|title=Digraphs: Theory, Algorithms and Applications|edition=First |isbn=978-1-84800-997-4|chapter=Section 2.3.4: The Bellman-Ford-Moore algorithm|url=http://www.cs.rhul.ac.uk/books/dbook/|ref=harv}}\n*{{cite journal|first=Alexander|last=Schrijver|title=On the history of combinatorial optimization (till 1960)|pages=1–68|publisher=Elsevier|journal=Handbook of Discrete Optimization|year=2005|url=http://homepages.cwi.nl/~lex/files/histco.pdf|ref=harv}}\n*{{Introduction to Algorithms}}, Second Edition. MIT Press and McGraw-Hill, 2001. {{ISBN|0-262-03293-7}}. Section 24.1: The Bellman–Ford algorithm, pp.&nbsp;588–592. Problem 24-1, pp.&nbsp;614–615. Third Edition. MIT Press, 2009. {{ISBN|978-0-262-53305-8}}. Section 24.1: The Bellman–Ford algorithm, pp.&nbsp;651–655.\n*{{cite book | first1 = George T. | last1 = Heineman | first2 = Gary | last2 = Pollice | first3 = Stanley | last3 = Selkow | title= Algorithms in a Nutshell | publisher=[[O'Reilly Media]] | year=2008 | chapter=Chapter 6: Graph Algorithms | pages = 160–164 | isbn=978-0-596-51624-6 | ref = harv }}\n*{{cite book|last1=Kleinberg|first1=Jon|author1-link=Jon Kleinberg|last2=Tardos|first2=Éva|author2-link=Éva Tardos|year=2006|title=Algorithm Design|location=New York|publisher=Pearson Education, Inc.|ref=harv}}\n*{{Cite book|first=Robert|last=Sedgewick|authorlink=Robert Sedgewick (computer scientist)|year=2002|title=Algorithms in Java|edition=3rd|isbn=0-201-36121-3|chapter=Section 21.7: Negative Edge Weights|url=http://safari.oreilly.com/0201361213/ch21lev1sec7|ref=harv|access-date=2007-05-28|archive-url=https://web.archive.org/web/20080531142256/http://safari.oreilly.com/0201361213/ch21lev1sec7|archive-date=2008-05-31|dead-url=yes}}\n\n{{DEFAULTSORT:Bellman-Ford algorithm}}\n[[Category:Graph algorithms]]\n[[Category:Polynomial-time problems]]\n[[Category:Articles with example C code]]\n[[Category:Articles with example pseudocode]]\n[[Category:Dynamic programming]]"
    },
    {
      "title": "Bitonic tour",
      "url": "https://en.wikipedia.org/wiki/Bitonic_tour",
      "text": "[[Image:Bitonic tour.svg|thumb|A bitonic tour]]\nIn [[computational geometry]], a '''bitonic tour''' of a set of [[Point (geometry)|point]] sites in the [[Euclidean plane]] is a [[closed polygonal chain]] that has each site as one of its vertices, such that any vertical [[line (geometry)|line]] crosses the chain at most twice.\n\n==Optimal bitonic tours==\nThe '''optimal bitonic tour''' is a bitonic tour of minimum total length.  It is a standard exercise in [[dynamic programming]] to devise a [[polynomial time]] algorithm that constructs the optimal bitonic tour.<ref>''[[Introduction to Algorithms]]'', 3rd ed., [[Thomas H. Cormen|T. H. Cormen]], [[Charles E. Leiserson|C. E. Leiserson]], [[Ron Rivest|R. Rivest]], and [[Clifford Stein|C. Stein]], [[MIT Press]], 2009. Problem 15-3, p. 405.</ref><ref>{{citation|title=The Algebra of Programming|first1=Richard S.|last1=Bird|first2=Oege|last2=De Moor|publisher=Prentice Hall|year=1997|isbn=9780135072455|page=213}}.</ref>\n\nThe problem of constructing optimal bitonic tours is often credited to Jon L. Bentley, who published in 1990 an experimental comparison of many heuristics for the [[traveling salesman problem]];<ref>{{citation|last=Bentley|first=Jon L.|contribution=Experiments on traveling salesman heuristics|title=Proc. 1st ACM-SIAM Symp. Discrete Algorithms (SODA)|year=1990|pages=91–99|url=http://portal.acm.org/citation.cfm?id=320186}}.</ref> however, Bentley's experiments do not include bitonic tours. The first publication that describes the bitonic tour problem appears to be a different 1990 publication, the first edition of the textbook ''[[Introduction to Algorithms]]'' by [[Thomas H. Cormen]], [[Charles E. Leiserson]], and [[Ron Rivest]], which lists Bentley as the originator of the problem.\n\n==Properties==\nThe optimal bitonic tour has no self-crossings, because any two edges that cross can be replaced by an uncrossed pair of edges with shorter total length due to the triangle inequality.\n\nWhen compared to other tours that might not be bitonic,\nthe optimal bitonic tour is the  one that minimizes the total amount of horizontal motion, with ties broken by Euclidean distance.<ref name=\"sourd\">{{citation\n | last = Sourd | first = Francis\n | doi = 10.1007/s10878-008-9154-0\n | issue = 1\n | journal = Journal of Combinatorial Optimization\n | mr = 2579501\n | pages = 1–15\n | title = Lexicographically minimizing axial motions for the Euclidean TSP\n | volume = 19\n | year = 2010}}.</ref>\n\n==Other optimization criteria==\nThe same dynamic programming algorithm that finds the optimal bitonic tour may be used to solve other variants of the traveling salesman problem that minimize [[Lexicographical order|lexicographic]] combinations of motion in a fixed number of coordinate directions.<ref name=\"sourd\"/>\n\nAt the 5th [[International Olympiad in Informatics]], in [[Mendoza, Argentina]] in 1993, one of the contest problems involved bitonic tours: the contestants were to devise an algorithm that took as input a set of sites and a collection of allowed edges between sites and construct a bitonic tour using those edges that included as many sites as possible. As with the optimal bitonic tour, this problem may be solved by dynamic programming.<ref>[http://olympiads.win.tue.nl/ioi/ioi93/index.html IOI'93] contest problems and report.</ref><ref>{{citation|title=The Canadian Airline Problem and the Bitonic Tour: Is This Dynamic Programming?|first=Pedro|last=Guerreiro|date=December 2003|publisher=Departamento de Informática, Faculdade de Ciências e Tecnologia, Universidade Nova de Lisboa|url=https://www.researchgate.net/publication/228391119_The_Canadian_Airline_Problem_and_the_Bitonic_Tour_Is_This_Dynamic_Programming/file/79e4150c0f0cb543bd.pdf}}.</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Geometric algorithms]]\n[[Category:Dynamic programming]]"
    },
    {
      "title": "Damerau–Levenshtein distance",
      "url": "https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance",
      "text": "In [[information theory]] and [[computer science]], the '''Damerau–Levenshtein distance''' (named after [[Frederick J. Damerau]] and [[Vladimir Levenshtein|Vladimir I. Levenshtein]]<ref>{{cite conference|last1=Brill |first1=Eric |last2=Moore |first2=Robert C. |year=2000 |title=An Improved Error Model for Noisy Channel Spelling Correction |conference=Proceedings of the 38th Annual Meeting on Association for Computational Linguistics |pages=286–293 |doi=10.3115/1075218.1075255 |url=http://acl.ldc.upenn.edu/P/P00/P00-1037.pdf |deadurl=yes |archiveurl=https://web.archive.org/web/20121221172057/http://acl.ldc.upenn.edu/P/P00/P00-1037.pdf |archivedate=2012-12-21 |df= }}</ref><ref name=\"bard\">{{citation|last=Bard|first=Gregory V.|title=Proceedings of the Fifth Australasian Symposium on ACSW Frontiers : 2007, Ballarat, Australia, January 30 - February 2, 2007|url=http://dl.acm.org/citation.cfm?id=1274531.1274545|year=2007|series=Conferences in Research and Practice in Information Technology|volume=68|pages=117–124|contribution=Spelling-error tolerant, order-independent pass-phrases via the Damerau–Levenshtein string-edit distance metric|location=Darlinghurst, Australia|publisher=Australian Computer Society, Inc.|isbn=978-1-920682-49-1}}.</ref><ref>{{cite conference |last1=Li |year=2006 |title=Exploring distributional similarity based models for query spelling correction |conference=Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics |pages=1025–1032 |doi=10.3115/1220175.1220304 |url=http://acl.ldc.upenn.edu/P/P06/P06-1129.pdf |display-authors=etal |deadurl=yes |archiveurl=https://web.archive.org/web/20100401081500/http://acl.ldc.upenn.edu/P/P06/P06-1129.pdf |archivedate=2010-04-01 |df= }}</ref>) is a [[string metric]] for measuring the [[edit distance]] between two sequences. Informally, the Damerau–Levenshtein distance between two words is the minimum number of operations (consisting of insertions, deletions or substitutions of a single character, or [[transposition (mathematics)|transposition]] of two adjacent characters) required to change one word into the other.\n\nThe Damerau–Levenshtein distance differs from the classical [[Levenshtein distance]] by including transpositions among its allowable operations in addition to the three classical single-character edit operations (insertions, deletions and substitutions).<ref>{{citation|last=Levenshtein|first=Vladimir I.|title=Binary codes capable of correcting deletions, insertions, and reversals|date=February 1966|url=<!-- copylink? http://profs.sci.univr.it/~liptak/ALBioinfo/files/levenshtein66.pdf-->|journal=Soviet Physics Doklady|volume=10|issue=8|pages=707–710}}</ref><ref name=\"bard\" />\n\nIn his seminal paper,<ref>{{Citation |last=Damerau |first=Fred J. |author-link=Frederick J. Damerau |title=A technique for computer detection and correction of spelling errors |journal=Communications of the ACM |volume=7 |issue=3 |pages=171–176 |date=March 1964 |doi=10.1145/363958.363994}}</ref> Damerau stated that more than 80% of all human misspellings can be expressed by a single error of one of the four types. Damerau's paper considered only misspellings that could be corrected with at most one edit operation. While the original motivation was to measure distance between human misspellings to improve applications such as [[spell checker]]s, Damerau–Levenshtein distance has also seen uses in biology to measure the variation between protein sequences.<ref>The method used in: {{Citation\n| last1  = Majorek         | first1 = Karolina A.\n| last2  = Dunin-Horkawicz | first2 = Stanisław\n| last3  = Steczkiewicz    | first3 = Kamil\n| last4  = Muszewska       | first4 = Anna\n| last5  = Nowotny         | first5 = Marcin\n| last6  = Ginalski        | first6 = Krzysztof\n| last7  = Bujnicki        | first7 = Janusz M.\n| display-authors = 2\n| title   = The RNase H-like superfamily: new members, comparative structural analysis and evolutionary classification\n| journal = Nucleic Acids Research\n| volume  = 42\n| issue   = 7\n| pages   = 4160–4179\n| year    = 2013\n| doi     = 10.1093/nar/gkt1414\n| url     = http://nar.oxfordjournals.org/content/42/7/4160.full\n| pmid    = 24464998\n| pmc     = 3985635\n}}</ref>\n\n== Definition ==\nTo express the Damerau–Levenshtein distance between two strings <math>a</math> and <math>b</math> a function <math>d_{a,b}(i,j)</math> is defined, whose value is a distance between an <math>i</math>–symbol prefix (initial substring) of string <math>a</math> and a <math>j</math>–symbol prefix of <math>b</math>.\n\nThe ''restricted distance'' function is defined recursively as:,<ref name=\"Boytsov\" />{{rp|A:11}}\n\n<math>\\qquad d_{a,b}(i,j) = \n\\min \n\\begin{cases}\n0 & \\text{if } i = j = 0 \\\\\nd_{a,b}(i-1,j) + 1 & \\text{if } i > 0 \\\\\nd_{a,b}(i,j-1) + 1 & \\text{if } j > 0 \\\\\nd_{a,b}(i-1,j-1) + 1_{(a_i \\neq b_j)} &  \\text{if } i, j > 0\\\\\nd_{a,b}(i-2,j-2) + 1 & \\text{if } i, j > 1 \\text{ and } a[i] = b[j-1] \\text{ and } a[i-1] = b[j]\\\\\n\\end{cases}\n</math>\n\nwhere  <math>1_{(a_i \\neq b_j)}</math> is the [[indicator function]] equal to 0 when  <math>a_i = b_j</math> and equal to 1 otherwise.\n\nEach recursive call matches one of the cases covered by the Damerau–Levenshtein distance:\n* <math>d_{a,b}(i-1,j) + 1</math> corresponds to a deletion (from a to b).\n* <math>d_{a,b}(i,j-1) + 1</math> corresponds to an insertion (from a to b).\n* <math>d_{a,b}(i-1,j-1) + 1_{(a_i \\neq b_j)} </math> corresponds to a match or mismatch, depending on whether the respective symbols are the same.\n* <math>d_{a,b}(i-2,j-2) + 1 </math> corresponds to a [[transposition (mathematics)|transposition]] between two successive symbols.\n\nThe Damerau–Levenshtein distance between {{mvar|a}} and {{mvar|b}} is then given by the function value for full strings: <math>d_{a,b}(|a|,|b|)</math> where <math>i=|a|</math> denotes the length of string {{mvar|a}} and <math>j=|b|</math> is the length of {{mvar|b}}.\n\n== Algorithm ==\nPresented here are two algorithms: the first,<ref name=\"oommen\">{{cite journal |first1= B. J. |last1=Oommen |first2=R. K. S. |last2=Loke |title=Pattern recognition of strings with substitutions, insertions, deletions and generalized transpositions |citeseerx=10.1.1.50.1459 |doi=10.1016/S0031-3203(96)00101-X |volume=30 |issue=5 |journal=Pattern Recognition |pages=789–800|year=1997 }}</ref> simpler one, computes what is known as the ''optimal string alignment distance'' or ''restricted edit distance'',<ref name=\"Boytsov\">{{cite journal |last=Boytsov |first=Leonid |title=Indexing methods for approximate dictionary searching |journal=Journal of Experimental Algorithmics |volume=16 |page=1 |date=May 2011 |doi=10.1145/1963190.1963191}}</ref> while the second one<ref name=\"LW75\">{{Citation |first1=Roy |last1=Lowrance |first2=Robert A. |last2=Wagner |title=An Extension of the String-to-String Correction Problem |journal=J ACM |volume=22 |issue=2 |pages=177–183 |date=April 1975 |doi=10.1145/321879.321880}}</ref> computes the Damerau–Levenshtein distance with adjacent transpositions. Adding transpositions adds significant complexity. The difference between the two algorithms consists in that the ''optimal string alignment algorithm'' computes the number of edit operations needed to make the strings equal under the condition that '''no substring is edited more than once''', whereas the second one presents no such restriction.\n\nTake for example the edit distance between '''CA''' and '''ABC'''. The Damerau–Levenshtein distance LD('''CA''','''ABC''') = 2 because '''CA''' → '''AC''' → '''ABC''', but the optimal string alignment distance OSA('''CA''','''ABC''') = 3 because if the operation '''CA''' → '''AC''' is used, it is not possible to use '''AC''' → '''ABC''' because that would require the substring to be edited more than once, which is not allowed in OSA, and therefore the shortest sequence of operations is '''CA''' → '''A''' → '''AB''' → '''ABC'''. Note that for the optimal string alignment distance, the [[triangle inequality]] does not hold: OSA('''CA''','''AC''') + OSA('''AC''','''ABC''') < OSA('''CA''','''ABC'''), and so it is not a true metric.\n\n=== Optimal string alignment distance ===\nOptimal string alignment distance can be computed using a straightforward extension of the [[Wagner–Fischer algorithm|Wagner–Fischer]] [[dynamic programming]] algorithm that computes [[Levenshtein distance]]. In [[pseudocode]]:\n\n '''algorithm''' OSA-distance '''is'''\n     '''input''': strings a[1..length(a)], b[1..length(b)]\n     '''output''': distance, integer\n     \n     '''let''' d[0..length(a), 0..length(b)] be a 2-d array of integers, dimensions length(a)+1, length(b)+1\n     ''// note that d is zero-indexed, while a and b are one-indexed.''\n     \n     '''for''' i := 0 '''to''' length(a) '''inclusive''' '''do'''\n         d[i, 0] := i\n     '''for''' j := 0 '''to''' length(b) '''inclusive''' '''do'''\n         d[0, j] := j\n     \n     '''for''' i := 1 '''to''' length(a) '''inclusive''' '''do'''\n         '''for''' j := 1 '''to''' length(b) '''inclusive''' '''do'''\n             '''if''' a[i] = b[j] '''then'''\n                 cost := 0\n             '''else'''\n                 cost := 1\n             d[i, j] := minimum(d[i-1, j] + 1,     ''// deletion''\n                                d[i, j-1] + 1,     ''// insertion''\n                                d[i-1, j-1] + cost)  ''// substitution''\n             '''if''' i > 1 and j > 1 and a[i] = b[j-1] and a[i-1] = b[j] '''then'''\n                 d[i, j] := minimum(d[i, j],\n                                    d[i-2, j-2] + cost)  ''// transposition''\n     '''return''' d[length(a), length(b)]\n\nThe difference from the algorithm for Levenshtein distance is the addition of one recurrence:\n\n '''if''' i > 1 and j > 1 and a[i] = b[j-1] and a[i-1] = b[j] '''then'''\n     d[i, j] := minimum(d[i, j],\n                        d[i-2, j-2] + cost)  ''// transposition''\n\n=== Distance with adjacent transpositions ===\nThe following algorithm computes the true Damerau–Levenshtein distance with adjacent transpositions; this algorithm requires as an additional parameter the size of the alphabet {{math|Σ}}, so that all entries of the arrays are in {{math|[0, {{!}}Σ{{!}})}}:<ref name=\"Boytsov\" />{{Rp|A:93}}\n\n '''algorithm''' DL-distance '''is'''\n     '''input''': strings a[1..length(a)], b[1..length(b)]\n     '''output''': distance, integer\n     \n     da := new array of |Σ| integers\n     '''for''' i := 1 '''to''' |Σ| '''inclusive''' '''do'''\n         da[i] := 0\n     \n     '''let''' d[−1..length(a), −1..length(b)] be a 2-d array of integers, dimensions length(a)+2, length(b)+2\n     ''// note that d has indices starting at −1, while a, b and da are one-indexed.''\n     \n     maxdist := length(a) + length(b)\n     d[−1, −1] := maxdist\n     '''for''' i := 0 '''to''' length(a) '''inclusive''' '''do'''\n         d[i, −1] := maxdist\n         d[i, 0] := i\n     '''for''' j := 0 '''to''' length(b) '''inclusive''' '''do'''\n         d[−1, j] := maxdist\n         d[0, j] := j\n     \n     '''for''' i := 1 '''to''' length(a) '''inclusive''' '''do'''\n         db := 0\n         '''for''' j := 1 '''to''' length(b) '''inclusive''' '''do'''\n             k := da[b[j]]\n             ℓ := db\n             '''if''' a[i] = b[j] '''then'''\n                 cost := 0\n                 db := j\n             '''else'''\n                 cost := 1\n             d[i, j] := minimum(d[i−1, j−1] + cost,  ''//substitution''\n                                d[i,   j−1] + 1,     ''//insertion''\n                                d[i−1, j  ] + 1,     ''//deletion''\n                                d[k−1, ℓ−1] + (i−k−1) + 1 + (j-ℓ−1)) ''//transposition''\n         da[a[i]] := i\n     '''return''' d[length(a), length(b)]\n\nTo devise a proper algorithm to calculate unrestricted Damerau–Levenshtein distance note that there always exists an optimal sequence of edit operations, where once-transposed letters are never modified afterwards. (This holds as long as the cost of a transposition, <math>W_T</math>, is at least the average of the cost of an insertion and deletion, i.e., <math>2W_T \\ge W_I+W_D</math>.<ref name=\"LW75\"/>) Thus, we need to consider only two symmetric ways of modifying a substring more than once: (1) transpose letters and insert an arbitrary number of characters between them, or (2) delete a sequence of characters and transpose letters that become adjacent after deletion. The straightforward implementation of this idea gives an algorithm of cubic complexity: <math>O\\left (M \\cdot N \\cdot \\max(M, N) \\right )</math>, where ''M'' and ''N'' are string lengths. Using the ideas of Lowrance and Wagner,<ref name=\"LW75\"/> this naive algorithm can be improved to be <math>O\\left (M \\cdot N \\right)</math> in the worst case.\n\nIt is interesting that the [[bitap algorithm]] can be modified to process transposition. See the information retrieval section of{{ref|itman}} for an example of such an adaptation.\n\n== Applications ==\nDamerau–Levenshtein distance plays an important role in [[natural language processing]]. In natural languages, strings are short and the number of errors (misspellings) rarely exceeds 2. In such circumstances, restricted and real edit distance differ very rarely. Oommen and Loke<ref name=\"oommen\"/> even mitigated the limitation of the restricted edit distance by introducing ''generalized transpositions''. Nevertheless, one must remember that the restricted edit distance usually does not satisfy the [[triangle inequality]] and, thus, cannot be used with [[metric tree]]s.\n\n=== DNA ===\nSince [[DNA]] frequently undergoes insertions, deletions, substitutions, and transpositions, and each of these operations occurs on approximately the same timescale, the Damerau–Levenshtein distance is an appropriate metric of the variation between two strands of DNA. More common in DNA, protein, and other bioinformatics related alignment tasks is the use of closely related algorithms such as [[Needleman–Wunsch algorithm]] or [[Smith–Waterman algorithm]].\n\n=== Fraud detection ===\nThe algorithm can be used with any set of words, like vendor names. Since entry is manual by nature there is a risk of entering a false vendor. A fraudster employee may enter one real vendor such as \"Rich Heir Estate Services\" versus a false vendor \"Rich Hier State Services\". The fraudster would then create a false bank account and have the company route checks to the real vendor and false vendor. The Damerau–Levenshtein algorithm will detect the transposed and dropped letter and bring attention of the items to a fraud examiner.\n\n=== Export control ===\n\nThe U.S. Government uses the Damerau–Levenshtein distance with its Consolidated Screening List API.<ref>http://developer.trade.gov/consolidated-screening-list.html</ref>\n\n== See also ==\n* [[Ispell]] suggests corrections that are based on a Damerau–Levenshtein distance of 1\n* [[Typosquatting]]\n\n== References ==\n{{Reflist|30em}}\n\n== Further reading ==\n* {{Citation |first=Gonzalo |last=Navarro |title=A guided tour to approximate string matching |journal=ACM Computing Surveys |volume=33 |issue=1 |pages=31–88 |date=March 2001 |doi=10.1145/375360.375365 |citeseerx=10.1.1.452.6317 }}\n\n{{Strings}}\n\n{{DEFAULTSORT:Damerau-Levenshtein Distance}}\n[[Category:String similarity measures]]\n[[Category:Information theory]]\n[[Category:Dynamic programming]]\n[[Category:Similarity and distance measures]]"
    },
    {
      "title": "Differential dynamic programming",
      "url": "https://en.wikipedia.org/wiki/Differential_dynamic_programming",
      "text": "'''Differential dynamic programming (DDP)''' is an [[optimal control]] algorithm of the [[trajectory optimization]] class.  The algorithm was introduced in 1966 by [[David Mayne|Mayne]]<ref>{{Cite journal\n| volume = 3\n| pages = 85–95\n| last = Mayne\n| first = D. Q.\n| title = A second-order gradient method of optimizing non-linear discrete time systems\n| journal = Int J Control\n| year = 1966\n| doi = 10.1080/00207176608921369\n}}</ref> and subsequently analysed in Jacobson and Mayne's eponymous book.<ref>{{cite book|last=Mayne|first= David H. and Jacobson, David Q.|title=Differential dynamic programming|year=1970|publisher=American Elsevier Pub. Co.|location=New York|isbn=978-0-444-00070-5|url=https://books.google.com/books?id=tA-oAAAAIAAJ}}</ref>  The algorithm uses locally-quadratic models of the dynamics and cost functions, and displays [[Rate of convergence|quadratic convergence]]. It is closely related to Pantoja's step-wise Newton's method.<ref>{{Cite journal\n| doi = 10.1080/00207178808906114\n| issn = 0020-7179\n| volume = 47\n| issue = 5\n| pages = 1539–1553\n| last = de O. Pantoja\n| first = J. F. A.\n| title = Differential dynamic programming and Newton's method\n| journal = International Journal of Control\n| year = 1988\n}}</ref><ref>{{Cite document\n| last = Liao\n| first = L. Z.\n|author2=C. A Shoemaker\n| title = Advantages of differential dynamic programming over Newton's method for discrete-time optimal control problems\n| publisher = Cornell University, Ithaca, NY\n| year = 1992\n| hdl = 1813/5474\n}}</ref>\n\n== Finite-horizon discrete-time problems ==\nThe dynamics\n\n{{NumBlk|:|<math>\\mathbf{x}_{i+1} = \\mathbf{f}(\\mathbf{x}_i,\\mathbf{u}_i)</math>|{{EquationRef|1}}}}\n\ndescribe the evolution of the state <math>\\textstyle\\mathbf{x}</math> given the control <math>\\mathbf{u}</math> from time <math>i</math> to time <math>i+1</math>. The ''total cost'' <math>J_0</math> is the sum of running costs <math>\\textstyle\\ell</math> and final cost <math>\\ell_f</math>, incurred when starting from state <math>\\mathbf{x}</math> and applying the control sequence <math>\\mathbf{U} \\equiv \\{\\mathbf{u}_0,\\mathbf{u}_1\\dots,\\mathbf{u}_{N-1}\\}</math> until the horizon is reached:\n\n:<math>J_0(\\mathbf{x},\\mathbf{U})=\\sum_{i=0}^{N-1}\\ell(\\mathbf{x}_i,\\mathbf{u}_i) + \\ell_f(\\mathbf{x}_N),</math>\n\nwhere <math>\\mathbf{x}_0\\equiv\\mathbf{x}</math>, and the <math>\\mathbf{x}_i</math> for <math>i>0</math> are given by {{EquationNote|1|Eq. 1}}. The solution of the optimal control problem is the minimizing control sequence\n<math>\\mathbf{U}^*(\\mathbf{x})\\equiv \\operatorname{argmin}_{\\mathbf{U}} J_0(\\mathbf{x},\\mathbf{U}).</math>\n''Trajectory optimization'' means finding <math>\\mathbf{U}^*(\\mathbf{x})</math> for a particular <math>\\mathbf{x}_0</math>, rather than for all possible initial states.\n\n== Dynamic programming ==\nLet <math>\\mathbf{U}_i</math> be the partial control sequence <math>\\mathbf{U}_i \\equiv \\{\\mathbf{u}_i,\\mathbf{u}_{i+1}\\dots,\\mathbf{u}_{N-1}\\}</math> and define the ''cost-to-go'' <math>J_i</math> as the partial sum of costs from <math>i</math> to <math>N</math>:\n\n:<math>J_i(\\mathbf{x},\\mathbf{U}_i)=\\sum_{j=i}^{N-1}\\ell(\\mathbf{x}_j,\\mathbf{u}_j) + \\ell_f(\\mathbf{x}_N).</math>\n\nThe optimal cost-to-go or ''value function'' at time <math>i</math> is the cost-to-go given the minimizing control sequence:\n\n:<math>V(\\mathbf{x},i)\\equiv \\min_{\\mathbf{U}_i}J_i(\\mathbf{x},\\mathbf{U}_i).</math>\n\nSetting <math>V(\\mathbf{x},N)\\equiv \\ell_f(\\mathbf{x}_N)</math>, the [[Dynamic programming|dynamic programming principle]] reduces the minimization over an entire sequence of controls to a sequence of minimizations over a single control, proceeding backwards in time:\n\n{{NumBlk|:|<math>V(\\mathbf{x},i)= \\min_{\\mathbf{u}}[\\ell(\\mathbf{x},\\mathbf{u}) + V(\\mathbf{f}(\\mathbf{x},\\mathbf{u}),i+1)].</math>|{{EquationRef|2}}}}\n\nThis is the [[Bellman equation]].\n\n== Differential dynamic programming ==\nDDP proceeds by iteratively performing a backward pass on the nominal trajectory to generate a new control sequence, and then a forward-pass to compute and evaluate a new nominal trajectory. We begin with the backward pass. If \n\n:<math>\\ell(\\mathbf{x},\\mathbf{u}) + V(\\mathbf{f}(\\mathbf{x},\\mathbf{u}),i+1)</math>\n\nis the argument of the <math>\\min[]</math> operator in {{EquationNote|2|Eq. 2}}, let <math>Q</math> be the variation of this quantity around the <math>i</math>-th <math>(\\mathbf{x},\\mathbf{u})</math> pair:\n\n:<math>\\begin{align}Q(\\delta\\mathbf{x},\\delta\\mathbf{u})\\equiv &\\ell(\\mathbf{x}+\\delta\\mathbf{x},\\mathbf{u}+\\delta\\mathbf{u})&&{}+V(\\mathbf{f}(\\mathbf{x}+\\delta\\mathbf{x},\\mathbf{u}+\\delta\\mathbf{u}),i+1)\n\\\\\n-&\\ell(\\mathbf{x},\\mathbf{u})&&{}-V(\\mathbf{f}(\\mathbf{x},\\mathbf{u}),i+1)\n\\end{align}\n</math>\n\nand expand to second order\n\n{{NumBlk|:|<math>\n\\approx\\frac{1}{2}\n\\begin{bmatrix}\n1\\\\\n\\delta\\mathbf{x}\\\\\n\\delta\\mathbf{u}\n\\end{bmatrix}^\\mathsf{T}\n\\begin{bmatrix}\n0 & Q_\\mathbf{x}^\\mathsf{T} & Q_\\mathbf{u}^\\mathsf{T}\\\\\nQ_\\mathbf{x} & Q_{\\mathbf{x}\\mathbf{x}} & Q_{\\mathbf{x}\\mathbf{u}}\\\\\nQ_\\mathbf{u} & Q_{\\mathbf{u}\\mathbf{x}} & Q_{\\mathbf{u}\\mathbf{u}}\n\\end{bmatrix}\n\\begin{bmatrix}\n1\\\\\n\\delta\\mathbf{x}\\\\\n\\delta\\mathbf{u}\n\\end{bmatrix}\n</math>|{{EquationRef|3}}}}\n\nThe <math>Q</math> notation used here is a variant of the notation of Morimoto where subscripts denote differentiation in denominator layout.<ref>{{Cite conference\n| volume = 2\n| pages = 1927–1932\n| last = Morimoto\n| first = J. |author2=G. Zeglin |author3=C.G. Atkeson\n| title = Minimax differential dynamic programming: Application to a biped walking robot\n| booktitle = Intelligent Robots and Systems, 2003.(IROS 2003). Proceedings. 2003 IEEE/RSJ International Conference on\n| date = 2003\n}}</ref>\n<!-- | url = www.cs.cmu.edu/~cga/papers/morimoto-iros03.pdf -->\nDropping the index <math>i</math> for readability, primes denoting the next time-step  <math>V'\\equiv V(i+1)</math>, the expansion coefficients are\n\n:<math>\n\\begin{alignat}{2}\nQ_\\mathbf{x} &= \\ell_\\mathbf{x}+ \\mathbf{f}_\\mathbf{x}^\\mathsf{T} V'_\\mathbf{x} \\\\\nQ_\\mathbf{u} &= \\ell_\\mathbf{u}+ \\mathbf{f}_\\mathbf{u}^\\mathsf{T} V'_\\mathbf{x} \\\\\nQ_{\\mathbf{x}\\mathbf{x}} &= \\ell_{\\mathbf{x}\\mathbf{x}} + \\mathbf{f}_\\mathbf{x}^\\mathsf{T} V'_{\\mathbf{x}\\mathbf{x}}\\mathbf{f}_\\mathbf{x}+V_\\mathbf{x}'\\cdot\\mathbf{f}_{\\mathbf{x}\\mathbf{x}}\\\\\nQ_{\\mathbf{u}\\mathbf{u}} &= \\ell_{\\mathbf{u}\\mathbf{u}} + \\mathbf{f}_\\mathbf{u}^\\mathsf{T} V'_{\\mathbf{x}\\mathbf{x}}\\mathbf{f}_\\mathbf{u}+{V'_\\mathbf{x}} \\cdot\\mathbf{f}_{\\mathbf{u} \\mathbf{u}}\\\\\nQ_{\\mathbf{u}\\mathbf{x}} &= \\ell_{\\mathbf{u}\\mathbf{x}} + \\mathbf{f}_\\mathbf{u}^\\mathsf{T} V'_{\\mathbf{x}\\mathbf{x}}\\mathbf{f}_\\mathbf{x} + {V'_\\mathbf{x}} \\cdot \\mathbf{f}_{\\mathbf{u} \\mathbf{x}}.\n\\end{alignat}\n</math>\n\nThe last terms in the last three equations denote contraction of a vector with a tensor. Minimizing the quadratic approximation {{EquationNote|3|(3)}} with respect to <math>\\delta\\mathbf{u}</math> we have\n\n{{NumBlk|:|<math>\n{\\delta \\mathbf{u}}^* = \\operatorname{argmin}\\limits_{\\delta \\mathbf{u}}Q(\\delta \\mathbf{x},\\delta\n\\mathbf{u})=-Q_{\\mathbf{u}\\mathbf{u}}^{-1}(Q_\\mathbf{u}+Q_{\\mathbf{u}\\mathbf{x}}\\delta \\mathbf{x}),\n</math>|{{EquationRef|4}}}}\n\ngiving an open-loop term <math>\\mathbf{k}=-Q_{\\mathbf{u}\\mathbf{u}}^{-1}Q_\\mathbf{u}</math> and a feedback gain term <math>\\mathbf{K}=-Q_{\\mathbf{u}\\mathbf{u}}^{-1}Q_{\\mathbf{u}\\mathbf{x}}</math>. Plugging the result back into {{EquationNote|3|(3)}}, we now have a quadratic model of the value at time <math>i</math>:\n\n:<math>\n\\begin{alignat}{2}\n\\Delta V(i) &= &{} -\\tfrac{1}{2}Q_\\mathbf{u} Q_{\\mathbf{u}\\mathbf{u}}^{-1}Q_\\mathbf{u}\\\\\nV_\\mathbf{x}(i) &= Q_\\mathbf{x} & {}- Q_\\mathbf{xu} Q_{\\mathbf{u}\\mathbf{u}}^{-1}Q_{\\mathbf{u}}\\\\\nV_{\\mathbf{x}\\mathbf{x}}(i) &= Q_{\\mathbf{x}\\mathbf{x}} &{} - Q_{\\mathbf{x}\\mathbf{u}}Q_{\\mathbf{u}\\mathbf{u}}^{-1}Q_{\\mathbf{u}\\mathbf{x}}.\n\\end{alignat}\n</math>\n\nRecursively computing the local quadratic models of <math>V(i)</math> and the control modifications <math>\\{\\mathbf{k}(i),\\mathbf{K}(i)\\}</math>, from <math>i=N-1</math> down to <math>i=1</math>, constitutes the backward pass. As above, the Value is initialized with <math>V(\\mathbf{x},N)\\equiv \\ell_f(\\mathbf{x}_N)</math>. Once the backward pass is completed, a forward pass computes a new trajectory:\n\n:<math>\n\\begin{align}\n\\hat{\\mathbf{x}}(1)&=\\mathbf{x}(1)\\\\\n\\hat{\\mathbf{u}}(i)&=\\mathbf{u}(i) + \\mathbf{k}(i) +\\mathbf{K}(i)(\\hat{\\mathbf{x}}(i) - \\mathbf{x}(i))\\\\\n\\hat{\\mathbf{x}}(i+1)&=\\mathbf{f}(\\hat{\\mathbf{x}}(i),\\hat{\\mathbf{u}}(i))\n\\end{align}\n</math>\n\nThe backward passes and forward passes are iterated until convergence.\n\n== Regularization and line-search ==\nDifferential dynamic programming is a second-order algorithm like [[Newton's method]]. It therefore takes large steps toward the minimum and often requires [[regularization (mathematics)|regularization]] and/or [[line-search]] to achieve convergence\n<ref>\n{{Cite journal\n| volume = 36\n| issue = 6\n| pages = 692\n| last = Liao\n| first = L. Z\n|author2=C. A Shoemaker\n| title = Convergence in unconstrained discrete-time differential dynamic programming\n| journal = IEEE Transactions on Automatic Control\n| year = 1991\n| doi = 10.1109/9.86943\n}}\n</ref>\n.<ref>\n{{Cite thesis\n| publisher = Hebrew University\n| last = Tassa\n| first = Y.\n| title = Theory and implementation of bio-mimetic motor controllers\n| date = 2011\n| url = http://icnc.huji.ac.il/phd/theses/files/YuvalTassa.pdf\n}}\n</ref> Regularization in the DDP context means ensuring that the <math>Q_{\\mathbf{u}\\mathbf{u}}</math> matrix in {{EquationNote|4|Eq. 4}} is [[positive definite matrix|positive definite]]. Line-search in DDP amounts to scaling the open-loop control modification <math>\\mathbf{k}</math> by some <math>0<\\alpha<1</math>.\n\n== Monte Carlo version ==\nSampled differential dynamic programming (SaDDP) is a Monte Carlo variant of differential dynamic programming.<ref>{{Cite web|url=https://ieeexplore.ieee.org/document/7759229|title=Sampled differential dynamic programming - IEEE Conference Publication|website=ieeexplore.ieee.org|language=en-US|access-date=2018-10-19}}</ref><ref>{{Cite web|url=https://ieeexplore.ieee.org/document/8430799|title=Regularizing Sampled Differential Dynamic Programming - IEEE Conference Publication|website=ieeexplore.ieee.org|language=en-US|access-date=2018-10-19}}</ref><ref>{{Cite journal|last=Joose|first=Rajamäki|date=2018|title=Random Search Algorithms for Optimal Control|url=http://urn.fi/URN:ISBN:978-952-60-8156-4|language=en|issn=1799-4942}}</ref> It is based on treating the quadratic cost of differential dynamic programming as the energy of a [[Boltzmann distribution]]. This way the quantities of DDP can be matched to the statistics of a [[Multivariate normal distribution|multidimensional normal distribution]]. The statistics can be recomputed from sampled trajectories without differentiation.\n\n== See also ==\n* [[Optimal control]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://www.ros.org/wiki/color_DDP A Python implementation of DDP]\n* [http://www.mathworks.com/matlabcentral/fileexchange/52069-ilqg-ddp-trajectory-optimization A MATLAB implementation of DDP]\n\n<!--- Categories --->\n\n[[Category:Dynamic programming]]"
    },
    {
      "title": "Dynamic discrete choice",
      "url": "https://en.wikipedia.org/wiki/Dynamic_discrete_choice",
      "text": "'''Dynamic discrete choice (DDC) models''', also known as '''discrete choice models of''' [[dynamic programming]], model an agent's choices over discrete options that have future implications. Rather than assuming observed choices are the result of static utility maximization, observed choices in DDC models are assumed to result from an agent's maximization of the [[present value]] of utility, generalizing the [[utility theory]] upon which [[discrete choice]] models are based.<ref>{{cite journal\n\n| first1     = Michael P.\n| last1      = Keane\n| first2     = Kenneth I.\n| last2      = Wolpin\n| title      = Empirical applications of discrete choice dynamic programming models\n| journal    = Review of Economic Dynamics\n| volume     = 12\n| number     = 1\n| pages      = 1–22\n| date       = 2009\n| doi \t     = 10.1016/j.red.2008.07.001\n}}</ref>\n\nThe goal of DDC models is to estimate the [[Structural estimation|structural parameters]] of the agent's decision process. Once these parameters are known, the researcher can then use the estimates to simulate how the agent would behave in a counterfactual state of the world. (For example, how a prospective college student's enrollment decision would change in response to a tuition increase.)\n\n== Mathematical representation ==\nAgent <math>n</math>'s [[Bellman_equation#A_dynamic_decision_problem|maximization problem]] can be written mathematically as follows:\n\n: <math>\nV\\left(x_{n0}\\right)=\\max_{\\left\\{d_{nt}\\right\\}_{t=1}^T} \\mathbb{E} \\left(\\sum_{t^{\\prime}=t}^T \\sum_{i=1}^J \\beta^{t'-t} \\left(d_{nt}=i\\right)U_{nit} \\left(x_{nt}, \\varepsilon_{nit}\\right)\\right),\n</math>\n\nwhere \n* <math>x_{nt}</math> are [[State variable|state variables]], with <math>x_{n0}</math> the agent's [[initial condition]]\n* <math>d_{nt}</math> represents <math>n</math>'s decision from among <math>J</math> discrete alternatives\n* <math>\\beta \\in \\left(0,1\\right)</math> is the [[Discounting#Discount_factor|discount factor]]\n* <math>U_{nit}</math> is the [[Utility theory|flow utility]] <math>n</math> receives from choosing alternative <math>i</math> in period <math>t</math>, and depends on both the state <math>x_{nt}</math> and unobserved factors <math>\\varepsilon_{nit}</math>\n* <math>T</math> is the [[time horizon]]\n* The expectation <math>\\mathbb{E}\\left(\\cdot\\right)</math> is taken over both the <math>x_{nt}</math>'s and <math>\\varepsilon_{nit}</math>'s in <math>U_{nit}</math>. That is, the agent is uncertain about future transitions in the states, and is also uncertain about future realizations of unobserved factors.\n\n=== Simplifying assumptions and notation ===\nIt is standard to impose the following simplifying assumptions and notation of the dynamic decision problem:\n\n;1. Flow utility is additively separable and linear in parameters\n\nThe flow utility can be written as an additive sum, consisting of deterministic and stochastic elements. The deterministic component can be written as a linear function of the [[Structural estimation|structural parameters]].\n\n: <math>\\begin{alignat}{5}\nU_{nit}\\left(x_{nt},\\varepsilon_{nit}\\right) &&\\; = \\;&& u_{nit}          &&\\; + \\;&& \\varepsilon_{nit} \\\\\n                                             &&\\; = \\;&& X_{nt}\\alpha_{i} &&\\; + \\;&& \\varepsilon_{nit}\n\\end{alignat}</math>\n\n;2. The optimization problem can be written as a [[Bellman equation]]\n\nDefine by <math>V_{nt}(x_{nt})</math> the ''ex ante'' value function for individual <math>n</math> in period <math>t</math> just before <math>\\varepsilon_{nt}</math> is revealed:\n\n: <math>\nV_{nt}(x_{nt}) = \\mathbb{E} \\max_i \\left\\{ u_{nit}(x_{nt}) + \\varepsilon_{nit} + \\beta \\int_{x_{t+1}} V_{nt+1} (x_{nt+1}) \\, dF\\left(x_{t+1} \\mid x_t \\right) \\right\\}\n</math>\n\nwhere the expectation operator <math>\\mathbb{E}</math> is over the <math>\\varepsilon</math>'s, and where <math>dF\\left(x_{t+1} \\mid x_t \\right)</math> represents the probability distribution over <math>x_{t+1}</math> conditional on <math>x_{t}</math>. The expectation over state transitions is accomplished by taking the integral over this probability distribution.\n\nIt is possible to decompose <math>V_{nt}(x_{nt})</math> into deterministic and stochastic components:\n\n: <math>\nV_{nt}(x_{nt}) = \\mathbb{E} \\max_i \\left\\{ v_{nit}(x_{nt}) + \\varepsilon_{nit} \\right\\}\n</math>\n\nwhere <math>v_{nit}</math> is the value to choosing alternative <math>i</math> at time <math>t</math> and is written as\n\n: <math>\nv_{nit}(x_{nt}) = u_{nit}\\left(x_{nt}\\right) + \\beta \\int_{x_{t+1}} \\mathbb{E} \\max_{j} \\left\\{ v_{njt+1}(x_{nt+1}) + \\varepsilon_{njt+1} \\right\\} \\, dF(x_{t+1} \\mid x_t)\n</math>\n\nwhere now the expectation <math>\\mathbb{E}</math> is taken over the <math>\\varepsilon_{njt+1}</math>.\n\n;3. The optimization problem follows a [[Markov decision process]]\n\nThe states <math>x_{t}</math> follow a [[Markov chain]]. That is, attainment of state <math>x_{t}</math> depends only on the state <math>x_{t-1}</math> and not <math>x_{t-2}</math> or any prior state.\n\n== Conditional value functions and choice probabilities ==\nThe value function in the previous section is called the '''conditional value function''', because it is the value function conditional on choosing alternative <math>i</math> in period <math>t</math>. Writing the conditional value function in this way is useful in constructing formulas for the choice probabilities.\n\nTo write down the choice probabilities, the researcher must make an assumption about the distribution of the <math>\\varepsilon_{nit}</math>'s. As in static discrete choice models, this distribution can be assumed to be [[iid]] [[Type I extreme value distribution|Type I extreme value]], [[generalized extreme value distribution|generalized extreme value]], [[multinomial probit]], or [[mixed logit]].\n\nFor the case where <math>\\varepsilon_{nit}</math> is multinomial logit (i.e. drawn [[iid]] from the [[Type I extreme value distribution]]), the formulas for the choice probabilities would be:\n\n: <math>P_{nit} = \\frac{\\exp(v_{nit})}{\\sum_{j=1}^J \\exp(v_{njt})}</math>\n\n== Estimation ==\nEstimation of dynamic discrete choice models is particularly challenging, due to the fact that the researcher must solve the backwards recursion problem for each guess of the structural parameters.\n\nThe most common methods used to estimate the structural parameters are [[maximum likelihood estimation]] and [[method of simulated moments]]. \n\nAside from estimation methods, there are also solution methods. Different solution methods can be employed due to complexity of the problem. These can be divided into '''full-solution methods''' and '''non-solution methods'''.\n\n=== Full-solution methods ===\n\nThe foremost example of a full-solution method is the nested fixed point (NFXP) algorithm developed by [[John Rust]] in 1987.<ref name=\"Rust1987\">{{cite journal\n\n| last       = Rust\n| first      = John\n| title      = Optimal Replacement of GMC Bus Engines: An Empirical Model of Harold Zurcher\n| journal    = Econometrica\n| volume     = 55\n| issue      = 5\n| pages      = 999–1033\n|issn\t\t\t\t = 0012-9682\n| date       = 1987\n|jstor\t\t\t = 1911259\n| doi=10.2307/1911259\n}}</ref>\nThe NFXP algorithm is described in great detail in its documentation manual.<ref>{{cite journal\n| last       = Rust\n| first      = John\n| title      = Nested fixed point algorithm documentation manual\n| journal    = Unpublished\n| date       = 2008\n| url \t     = https://editorialexpress.com/jrust/nfxp.html\n}}</ref>\n\nA recent work by Che-Lin Su and [[Kenneth Judd]] in 2012<ref name=\"SuJudd2012\">\n{{cite journal\n| last1      = Su\n| first1     = Che-Lin\n| last2      = Judd\n| first2     = Kenneth L.\n| authorlink2= Kenneth Judd\n| title      = Constrained Optimization Approaches to Estimation of Structural Models\n| journal    = Econometrica\n| volume     = 80\n| issue      = 5\n| pages      = 2213–2230\n| date       = 2012\n|issn\t\t\t\t = 1468-0262\n| doi \t\t\t = 10.3982/ECTA7925\n| hdl     = 10419/59626\n}}\n</ref> implements another approach (dismissed as intractable by Rust in 1987), which uses [[constrained optimization]] of the likelihood function, a special case of [[mathematical programming with equilibrium constraints]] (MPEC).\nSpecifically, the likelihood function is maximized subject to the constraints imposed by the model, and expressed in terms of the additional variables that describe the model's structure. This approach requires powerful optimization software such as [[Artelys Knitro]] because of the high dimensionality of the optimization problem.\nOnce it is solved, both the structural parameters that maximize the likelihood, and the solution of the model are found. \n\nIn the later article<ref name=\"Iskhakov et al 2016\">\n{{cite journal\n| last1      = Iskhakov\n| first1     = Fedor\n| last2      = Lee\n| first2     = Jinhyuk\n| last3      = Rust\n| first3     = John\n| last4      = Schjerning\n| first4     = Bertel\n| last5      = Seo\n| first5     = Kyoungwon\n| title      = Comment on \"constrained optimization approaches to estimation of structural models\"\n| journal    = Econometrica\n| volume     = 84\n| issue      = 1\n| pages      = 365–370\n| date       = 2016\n| issn\t     = 0012-9682\n| doi        = 10.3982/ECTA12605 \n| url     = https://curis.ku.dk/portal/da/publications/constrained-optimization-approaches-to-estimation-of-structural-models(99a64534-6f7f-44e8-b680-d392e1f90027).html\n}}\n</ref> Rust and coauthors show that the speed advantage of MPEC compared to NFXP is not significant. Yet, because the computations required by MPEC do not rely on the structure of the model, its implementation is much less labor intensive.\n\nDespite numerous contenders, the NFXP maximum likelihood estimator remains the leading estimation method\nfor Markov decision models.<ref name=\"Iskhakov et al 2016\"/>\n\n=== Non-solution methods ===\n\nAn alternative to full-solution methods is non-solution methods. In this case, the researcher can estimate the structural parameters without having to fully solve the backwards recursion problem for each parameter guess. Non-solution methods are typically faster while requiring more assumptions, but the additional assumptions are in many cases realistic. \n\nThe leading non-solution method is conditional choice probabilities, developed by V. Joseph Hotz and Robert A. Miller.<ref name=\"Hotz Miller\">\n{{cite journal\n| last1   = Hotz\n| first1  = V. Joseph\n| last2   = Miller\n| first2  = Robert A.\n| title   = Conditional Choice Probabilities and the Estimation of Dynamic Models\n| journal = Review of Economic Studies\n| volume  = 60\n| issue   = 3\n| pages   = 497–529\n| date    = 1993\n| doi     = 10.2307/2298122\n| jstor  = 2298122\n}}\n</ref>\n\n== Examples ==\n=== Bus engine replacement model ===\n\nThe bus engine replacement model developed in the seminal paper Rust (1987)<ref name=\"Rust1987\"/> is one of the first dynamic stochastic models of discrete choice estimated using real data, and continues to serve as classical example of the problems of this type.<ref name=\"SuJudd2012\"/>\n\nThe model is a simple regenerative [[optimal stopping]] stochastic dynamic problem faced by the decision maker, Harold Zurcher, superintendent of maintenance at the [[Madison, Wisconsin]] Metropolitan Bus Company. For every [[bus]] in operation in each time period Harold Zurcher has to decide whether to replace the [[engine]] and bear the associated replacement cost, or to continue operating the bus at an ever raising cost of operation, which includes insurance and the cost of lost ridership in the case of a breakdown.\n\nLet <math>x_t</math> denote the [[odometer]] reading (mileage) at period <math>t</math>, <math>c(x_t,\\theta)</math> cost of operating the bus which depends on the vector of parameters <math>\\theta</math>, <math>RC</math> cost of replacing the engine, and <math>\\beta</math> the [[discount factor]]. Then the per-period utility is given by\n\n: <math>\nU(x_t,\\xi_t,d,\\theta)= \n\\begin{cases}\n-c(x_t,\\theta) + \\xi_{t,\\text{keep}}, & \\\\\n-RC-c(0,\\theta) + \\xi_{t,\\text{replace}}, &\n\\end{cases}\n=\nu(x_t,d,\\theta) +\n\\begin{cases}\n\\xi_{t,\\text{keep}}, & \\textrm{if }\\;\\; d=\\text{keep}, \\\\\n\\xi_{t,\\text{replace}}, & \\textrm{if }\\;\\; d=\\text{replace},\n\\end{cases}\n</math>\n\nwhere <math>d</math> denotes the decision (keep or replace) and <math>\\xi_{t,\\text{keep}}</math>  and <math>\\xi_{t,\\text{replace}}</math> represent the component of the utility observed by Harold Zurcher, but not John Rust. It is assumed that <math>\\xi_{t,\\text{keep}}</math> and <math>\\xi_{t,\\text{replace}}</math> are independent and identically distributed with the [[Type I extreme value distribution]], and that <math>\\xi_{t,\\bullet}</math> are independent of <math>\\xi_{t-1,\\bullet} </math> conditional on <math>x_t</math>.\n\nThen the optimal decisions satisfy the [[Bellman equation]]\n\n: <math>\nV(x,\\xi,\\theta) = \\max_{d=\\text{keep},\\text{replace}} \\left\\{ u(x,d,\\theta)+\\xi_d + \\iint V(x',\\xi',\\theta) q(d\\xi'\\mid x',\\theta) p(dx'\\mid x,d,\\theta) \\right\\}\n</math>\n\nwhere <math>p(dx'\\mid x,d,\\theta)</math> and <math>q(d\\xi'\\mid x',\\theta)</math> are respectively transition densities for the observed and unobserved states variables. Time indices in the Bellman equation are dropped because the model is formulated in the infinite horizon settings, the unknown optimal policy is [[stationary process|stationary]], i.e. independent of time.\n\nGiven the distributional assumption on <math>q(d\\xi'\\mid x',\\theta)</math>, the probability of particular choice <math>d</math> is given by\n\n: <math>\nP(d\\mid x,\\theta) =  \\frac{ \\exp\\{ u(x,d,\\theta)+\\beta EV(x,d,\\theta)\\}}{\\sum_{d' \\in D(x)} \n\\exp\\{ u(x,d',\\theta)+\\beta EV(x,d',\\theta)\\} }\n</math>\n\nwhere <math>EV(x,d,\\theta)</math> is a unique solution to the [[functional equation]]\n\n: <math>\nEV(x,d,\\theta)= \\int \\left[ \\log\\left( \\sum_{d=\\text{keep},\\text{replace}}  \\exp\\{u(x,d',\\theta)+\\beta EV(x',d',\\theta)\\}\\right) \\right] p(x'\\mid x,d,\\theta).\n</math>\n\nIt can be shown that the latter functional equation defines a [[contraction mapping]] if the state space <math>x_t</math> is bounded, so there will be a unique solution <math>EV(x,d,\\theta)</math> for any <math>\\theta</math>, and further the [[implicit function theorem]] holds, so <math>EV(x,d,\\theta)</math> is also a [[smooth function]] of <math>\\theta</math> for each <math>(x,d)</math>.\n\n==== Estimation with nested fixed point algorithm ====\nThe contraction mapping above can be solved numerically for the fixed point <math>EV(x,d,\\theta)</math> that yields choice probabilities  <math>P(d\\mid x,\\theta)</math> for any given value of <math>\\theta</math>. The [[log-likelihood]] function can then be formulated as\n\n: <math>\nL(\\theta) = \\sum_{i=1}^N \\sum_{t=1}^{T_i} \\log(P(d_{it}\\mid x_{it},\\theta))+\\log(p(x_{it}\\mid x_{it-1},d_{it-1},\\theta)),\n</math>\n\nwhere <math>x_{i,t}</math> and <math>d_{i,t}</math> represent data on state variables (odometer readings) and\ndecision (keep or replace) for <math>i=1,\\dots,N</math> individual buses, each in <math>t=1,\\dots,T_i</math> periods.\n\nThe joint algorithm for solving the fixed point problem given a particular value of parameter <math>\\theta</math> and maximizing the log-likelihood <math>L(\\theta)</math> with respect to <math>\\theta</math> was named by John Rust ''nested fixed point algorithm'' (NFXP).\n\nRust's implementation of the nested fixed point algorithm is highly optimized for this problem, using [[Newton's method#Nonlinear equations in a Banach space|Newton–Kantorovich iterations]] to calculate <math>P(d\\mid x,\\theta)</math> and [[quasi-Newton method]]s, such as the [[Berndt–Hall–Hall–Hausman algorithm]], for likelihood maximization.<ref name=\"Iskhakov et al 2016\"/>\n\n==== Estimation with MPEC ====\n\nIn the nested fixed point algorithm, <math>P(d\\mid x,\\theta)</math> is recalculated for each guess of the parameters {{math|''θ''}}. The MPEC method instead solves the [[constrained optimization]] problem:<ref name=\"SuJudd2012\"/>\n\n: <math>\n\\begin{align}\n\\max & \\qquad L(\\theta) & \\\\\n\\text{subject to} & \\qquad EV(x,d,\\theta)= \\int \\left[ \\log\\left( \\sum_{d=\\text{keep},\\text{replace}} \\exp\\{ u(x,d',\\theta) + \\beta EV(x',d',\\theta)\\}\\right) \\right] p(x'\\mid x,d,\\theta)\n\\end{align}\n</math>\n\nThis method is faster to compute than non-optimized implementations of the nested fixed point algorithm, and takes about as long as highly optimized implementations.<ref name=\"Iskhakov et al 2016\"/>\n\n==== Estimation with non-solution methods ====\nThe conditional choice probabilities method of Hotz and Miller can be applied in this setting. Hotz, Miller, Sanders, and Smith proposed a computationally simpler version of the method, and tested it on a study of the bus engine replacement problem. The method works by estimating conditional choice probabilities using [[simulation]], then backing out the implied differences in [[value function]]s.{{sfn|Aguirregabiria|Mira|2010}}<ref name=\"Hotz Miller Sanders Smith\">{{cite journal | last=Hotz | first=V. J. | last2=Miller | first2=R. A. | last3=Sanders | first3=S. | last4=Smith | first4=J. | title=A Simulation Estimator for Dynamic Models of Discrete Choice | journal=The Review of Economic Studies | publisher=Oxford University Press (OUP) | volume=61 | issue=2 | date=1994-04-01 | issn=0034-6527 | doi=10.2307/2297981 | pages=265–289}}</ref>\n\n== See also ==\n*[[Inverse reinforcement learning]]\n\n== References ==\n{{reflist|30em}}\n\n== Further reading ==\n* {{cite book | last=Rust | first=John | title=Handbook of Econometrics | chapter=Chapter 51 Structural estimation of markov decision processes | publisher=Elsevier | year=1994 | isbn=978-0-444-88766-5 | issn=1573-4412 | doi=10.1016/s1573-4412(05)80020-0}}\n* {{cite journal | last=Aguirregabiria | first=Victor | last2=Mira | first2=Pedro | title=Dynamic discrete choice structural models: A survey | journal=Journal of Econometrics | publisher=Elsevier BV | volume=156 | issue=1 | year=2010 | issn=0304-4076 | doi=10.1016/j.jeconom.2009.09.007 | pages=38–67|ref=harv}}\n[[Category:Choice modelling]]\n[[Category:Economics models]]\n[[Category:Mathematical and quantitative methods (economics)]]\n[[Category:Dynamic programming]]"
    },
    {
      "title": "Dynamic time warping",
      "url": "https://en.wikipedia.org/wiki/Dynamic_time_warping",
      "text": "{{distinguish|text=the Time Warp mechanism for discrete event simulation, or the Time Warp Operating System that used this mechanism}}\n[[File:Dynamic time warping.png|thumb|Dynamic time warping]]\n\nIn [[time series analysis]], '''dynamic time warping''' ('''DTW''') is one of the [[algorithms]] for measuring similarity between two temporal sequences, which may vary in speed.  For instance, similarities in walking could be detected using DTW, even if one person was walking faster than the other, or if there were [[acceleration]]s and decelerations during the course of an observation. DTW has been applied to temporal sequences of video, audio, and graphics data — indeed, any data that can be turned into a linear sequence can be analyzed with DTW. A well known application has been automatic [[speech recognition]], to cope with different speaking speeds. Other applications include [[speaker recognition]] and online [[signature recognition]]. Also it is seen that it can be used in partial [[Shape analysis (digital geometry)|shape matching]] application.\n\nIn general, DTW is a method that calculates an [[Optimal matching|optimal match]] between two given sequences (e.g. [[time series]]) with certain restriction and rules:\n* Every index from the first sequence must be matched with one or more indices from the other sequence, and vice versa\n* The first index from the first sequence must be matched with the first index from the other sequence (but it does not have to be its only match)\n* The last index from the first sequence must be matched with the last index from the other sequence (but it does not have to be its only match)\n* The mapping of the indices from the first sequence to indices from the other sequence must be monotonically increasing, and vice versa, i.e. if <math>j > i</math> are indices from the first sequence, then there must not be two indices <math>l > k</math> in the other sequence, such that index <math>i</math> is matched with index <math>l</math> and index <math>j</math> is matched with index <math>k</math>, and vice versa\n\nThe [[Optimal matching|optimal match]] is denoted by the match that satisfies all the restrictions and the rules and that has the minimal cost, where the cost is computed as the sum of absolute differences, for each matched pair of indices, between their values.\n\nThe sequences are \"warped\" [[non-linear]]ly in the time dimension to determine a measure of their similarity independent of certain non-linear variations in the time dimension. This [[sequence alignment]] method is often used in time series classification. Although DTW measures a distance-like quantity between two given sequences, it doesn't guarantee the [[triangle inequality]] to hold.\n\nIn addition to a similarity measure between the two sequences, a so called \"warping path\" is produced, by warping according to this path the two signals may be aligned in time. The signal with an original set of points ''X''(original), ''Y''(original) is transformed to ''X''(warped), ''Y''(warped). This finds applications in genetic sequence and audio synchronisation. In a related technique sequences of varying speed may be averaged using this technique see the [[#Average sequence|average sequence]] section.\n\nThis is conceptually very similar to the [[Needleman–Wunsch algorithm]], which is explained in greater details.\n\n== Implementation ==\n\nThis example illustrates the implementation of the dynamic time warping algorithm when the two sequences <var>s</var> and <var>t</var> are strings of discrete symbols. For two symbols <var>x</var> and <var>y</var>, <code>d(x, y)</code> is a distance between the symbols, e.g. <code>d(x, y)</code> = <math>| x - y |</math>.\n\n int DTWDistance(s: array [1..n], t: array [1..m]) {\n    DTW := array [0..n, 0..m]\n  \n    for i := 1 to n\n      for j := 1 to m\n        DTW[i, j] := infinity\n    DTW[0, 0] := 0\n  \n    for i := 1 to n\n        for j := 1 to m\n            cost := d(s[i], t[j])\n            DTW[i, j] := cost + minimum(DTW[i-1, j  ],    // insertion\n                                        DTW[i  , j-1],    // deletion\n                                        DTW[i-1, j-1])    // match\n  \n    return DTW[n, m]\n }\n\nwhere <code>DTW[i, j]</code> is the distance between <code>s[1:i]</code> and <code>t[1:j]</code> with the best alignment.\n\nWe sometimes want to add a locality constraint. That is, we require that if <code>s[i]</code> is matched with <code>t[j]</code>, then <math>| i - j |</math> is no larger than <var>w</var>, a window parameter.\n\nWe can easily modify the above algorithm to add a locality constraint (differences <mark>marked</mark>).\nHowever, the above given modification works only if <math>| n - m |</math> is no larger than <var>w</var>, i.e. the end point is within the window length from diagonal. In order to make the algorithm work, the window parameter <var>w</var> must be adapted so that <math>| n - m | \\le w</math> (see the line marked with (*) in the code).\n\n int DTWDistance(s: array [1..n], t: array [1..m]<mark>, w: int</mark>) {\n     DTW := array [0..n, 0..m]\n \n     <mark>w := max(w, abs(n-m))</mark> // adapt window size (*)\n \n     for i := 0 to n\n         for j:= 0 to m\n             DTW[i, j] := infinity\n     DTW[0, 0] := 0\n     <mark>for i := 1 to n</mark>\n         <mark>for j := max(1, i-w) to min(m, i+w)</mark>\n             <mark>DTW[i, j] := 0</mark>\n \n     for i := 1 to n\n         for j := <mark>max(1, i-w) to min(m, i+w)</mark>\n             cost := d(s[i], t[j])\n             DTW[i, j] := cost + minimum(DTW[i-1, j  ],    // insertion\n                                         DTW[i  , j-1],    // deletion\n                                         DTW[i-1, j-1])    // match\n \n     return DTW[n, m]\n }\n\n==Complexity==\nThe time complexity of DTW algorithm is <math>O(NM)</math>, where <math>N</math> and <math>M</math> are the lengths of the two input sequences. Assuming that <math>N \\geq M</math>, the time complexity can be said to be <math>O(N^2)</math>. The same is true for space complexity.\n\n==Fast computation==\n\nFast techniques for computing DTW include PrunedDTW,<ref>Silva, D. F., Batista, G. E. A. P. A. (2015). [http://sites.labic.icmc.usp.br/dfs/pdf/SDM_PrunedDTW.pdf Speeding Up All-Pairwise Dynamic Time Warping Matrix Calculation].</ref> SparseDTW,<ref>\t\nAl-Naymat, G., Chawla, S., Taheri, J. (2012). [https://arxiv.org/abs/1201.2969 SparseDTW: A Novel Approach to Speed up Dynamic Time Warping].</ref> FastDTW,<ref>Stan Salvador, Philip Chan, FastDTW: Toward Accurate Dynamic Time Warping in Linear Time and Space. KDD Workshop on Mining Temporal and Sequential Data, pp. 70–80, 2004.</ref> and the MultiscaleDTW.<ref>Meinard Müller, Henning Mattes, and Frank Kurth (2006). [https://www.audiolabs-erlangen.de/fau/professor/mueller/publications/2006_MuellerMattesKurth_MultiscaleAudioSynchronization_ISMIR.pdf An Efficient Multiscale Approach to Audio Synchronization]. Proceedings of the International Conference on Music Information Retrieval (ISMIR), pp. 192—197.</ref><ref>Thomas Prätzlich, Jonathan Driedger, and Meinard Müller (2016). Memory-Restricted Multiscale Dynamic Time Warping. Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 569—573.</ref>\nA common task, retrieval of similar time series, can be accelerated by using lower bounds such as LB_Keogh<ref>{{cite journal | last1 = Keogh | first1 = E. | last2 = Ratanamahatana | first2 = C. A. | year = 2005 | title = Exact indexing of dynamic time warping | url = | journal = Knowledge and Information Systems | volume = 7 | issue = 3| pages = 358–386 | doi=10.1007/s10115-004-0154-9}}</ref> or LB_Improved.<ref>{{cite journal | last1 = Lemire | first1 = D. | year = 2009 | title = Faster Retrieval with a Two-Pass Dynamic-Time-Warping Lower Bound | arxiv = 0811.3301| journal = Pattern Recognition | volume = 42 | issue = 9| pages = 2169–2180 | doi=10.1016/j.patcog.2008.11.030}}</ref> In a survey, Wang et al. reported slightly better results with the LB_Improved lower bound than the LB_Keogh bound, and found that other techniques were inefficient.<ref>{{cite journal | last1 = Wang | first1 = Xiaoyue | display-authors = etal   | year = | title = Experimental comparison of representation methods and distance measures for time series data | url = | journal = Data Mining and Knowledge Discovery | volume = 2010 | issue = | pages = 1–35 }}</ref>\n\n== Average sequence ==\nAveraging for dynamic time warping is the problem of finding an average sequence for a set of sequences. \nThe average sequence is the sequence that minimizes the sum of the squares to the set of objects.\nNLAAF<ref>{{Cite journal | last1 = Gupta | first1 = L. | last2 = Molfese | first2 = D. L. | last3 = Tammana | first3 = R. | last4 = Simos | first4 = P. G. | title = Nonlinear alignment and averaging for estimating the evoked potential | doi = 10.1109/10.486255 | journal = IEEE Transactions on Biomedical Engineering | volume = 43 | issue = 4 | pages = 348–356 | year = 1996 | pmid =  8626184| pmc = }}</ref> is the exact method for two sequences.\nFor more than two sequences, the problem is related to the one of the [[multiple alignment]] and requires heuristics.\nDBA<ref name=\"DBA\">{{Cite journal | last1 = Petitjean | first1 = F. O. | last2 = Ketterlin | first2 = A. | last3 = Gançarski | first3 = P. | doi = 10.1016/j.patcog.2010.09.013 | title = A global averaging method for dynamic time warping, with applications to clustering | journal = Pattern Recognition | volume = 44 | issue = 3 | pages = 678 | year = 2011 | pmid =  | pmc = }}</ref> is currently the reference method to average a set of sequences consistently with DTW.\nCOMASA<ref>{{Cite journal | last1 = Petitjean | first1 = F. O. | last2 = Gançarski | first2 = P. | doi = 10.1016/j.tcs.2011.09.029 | title = Summarizing a set of time series by averaging: From Steiner sequence to compact multiple alignment | journal = Theoretical Computer Science | volume = 414 | pages = 76–91 | year = 2012 | pmid =  | pmc = }}</ref> efficiently randomizes the search for the average sequence, using DBA as a local optimization process.\n\n== Supervised learning ==\n\nA [[k-nearest neighbors algorithm|nearest-neighbour classifier]] can achieve state-of-the-art performance when using dynamic time warping as a distance measure.<ref>{{cite journal | last1 = Ding | first1 = Hui | last2 = Trajcevski | first2 = Goce | last3 = Scheuermann | first3 = Peter | last4 = Wang | first4 = Xiaoyue | last5 = Keogh | first5 = Eamonn | year = 2008 | title = Querying and mining of time series data: experimental comparison of representations and distance measures | url = | journal = Proc. VLDB Endow | volume = 1 | issue = 2| pages = 1542–1552 | doi = 10.14778/1454159.1454226 }}</ref>\n\n== Alternative approach ==\n\nAn alternative technique for DTW is based on [[functional data analysis]], in which the time series are regarded as discretizations of smooth (differentiable) functions of time and therefore continuous mathematics is applied.<ref>{{Cite journal|title = On the Registration of Time and the Patterning of Speech Movements|last = Lucero|first = J. C.|last2 = Munhall|first2 = K. G.|last3 = Gracco|first3 = V. G.|last4 = Ramsay|first4 = J. O.|journal = Journal of Speech, Language, and Hearing Research|volume = 40|issue = 5|pages = 1111–1117|year = 1997|doi=10.1044/jslhr.4005.1111}}</ref> Optimal nonlinear time warping functions are computed by minimizing a measure of distance of the set of functions to their warped average. Roughness penalty terms for the warping functions may be added, e.g., by constraining the size of their curvature. The resultant warping functions are smooth, which facilitates further processing. This approach has been successfully applied to analyze patterns and variability of speech movements.<ref>{{Cite book|title = Speech Motor Control: New Developments in Basic and Applied Research|last = Howell|first = P.|publisher = Oxford University Press|year = 2010|isbn = 978-0199235797|location = |pages = 215–225|last2 = Anderson|first2 = A.|last3 = Lucero|first3 = J. C.|chapter = Speech motor timing and fluency|editor-last = Maassen|editor-first = B.|editor-last2 = van Lieshout|editor-first2 = P.}}</ref><ref>{{Cite journal|title = Speech production variability in fricatives of children and adults: Results of functional data analysis|journal = The Journal of the Acoustical Society of America|date = 2008|issn = 0001-4966|pmc = 2677351|pmid = 19045800|pages = 3158–3170|volume = 124|issue = 5|doi = 10.1121/1.2981639|first = Laura L.|last = Koenig|first2 = Jorge C.|last2 = Lucero|first3 = Elizabeth|last3 = Perlman|bibcode = 2008ASAJ..124.3158K}}</ref>\n\nAnother related approach are [[hidden Markov model]]s (HMM) and it has been shown that the [[Viterbi algorithm]] used to search for the most likely path through the HMM is equivalent to stochastic DTW.<ref>{{Cite journal|last=Nakagawa|first=Seiichi|last2=Nakanishi|first2=Hirobumi|date=1988-01-01|title=Speaker-Independent English Consonant and Japanese Word Recognition by a Stochastic Dynamic Time Warping Method|journal=IETE Journal of Research|volume=34|issue=1|pages=87–95|doi=10.1080/03772063.1988.11436710|issn=0377-2063}}</ref><ref>{{Cite web|url=http://eecs.ceas.uc.edu/~fangcg/course/FromDTWtoHMM_ChunshengFang.pdf|title=From Dynamic Time Warping (DTW) to Hidden Markov Model (HMM)|last=Fang|first=Chunsheng|date=|website=|archive-url=|archive-date=|dead-url=|access-date=}}</ref><ref>{{Cite journal|last=Juang|first=B. H.|date=September 1984|title=On the hidden Markov model and dynamic time warping for speech recognition #x2014; A unified view|journal=AT&T Bell Laboratories Technical Journal|volume=63|issue=7|pages=1213–1243|doi=10.1002/j.1538-7305.1984.tb00034.x|issn=0748-612X}}</ref>\n\n==Open-source software==\n\n* The [https://github.com/lemire/lbimproved lbimproved] C++ library implements Fast Nearest-Neighbor Retrieval algorithms under the GNU General Public License (GPL). It also provides a C++ implementation of dynamic time warping, as well as various lower bounds.\n* The [https://github.com/rmaestre/FastDTW FastDTW] library is a Java implementation of DTW and a FastDTW implementation that provides optimal or near-optimal alignments with an ''O''(''N'') time and memory complexity, in contrast to the ''O''(''N''<sup>2</sup>) requirement for the standard DTW algorithm. FastDTW uses a multilevel approach that recursively projects a solution from a coarser resolution and refines the projected solution.\n* [https://mvnrepository.com/artifact/com.github.davidmoten/fastdtw FastDTW fork] (Java) published to Maven Central.\n* The R package [http://dtw.r-forge.r-project.org/ dtw] implements most known variants of the DTW algorithm family, including a variety of recursion rules (also called step patterns), constraints, and substring matching.\n* The [[mlpy]] Python library implements DTW.\n* The [https://pypi.python.org/pypi/pydtw pydtw] Python library implements the Manhattan and Euclidean flavoured DTW measures including the LB_Keogh lower bounds.\n* The [https://gravitino.github.io/cudadtw/ cudadtw] C++/CUDA library implements subsequence alignment of Euclidean-flavoured DTW and ''z''-normalized Euclidean distance similar to the popular UCR-Suite on CUDA-enabled accelerators.\n* The [http://java-ml.sourceforge.net/ JavaML] machine learning library implements [https://sourceforge.net/p/java-ml/java-ml-code/ci/9f6726deab4e55b7617478bc51e29c20308bffb9/tree/net/sf/javaml/distance/dtw/FastDTW.java DTW].\n* The [https://github.com/doblak/ndtw ndtw] C# library implements DTW with various options.\n* [https://github.com/kirel/sketch-a-char Sketch-a-Char] uses Greedy DTW (implemented in JavaScript) as part of LaTeX symbol classifier program.\n* The [https://github.com/hfink/matchbox MatchBox] implements DTW to match mel-frequency cepstral coefficients of audio signals.\n* [https://github.com/fpetitjean/DBA Sequence averaging]: a GPL Java implementation of DBA.<ref name=\"DBA\"/>\n* The [https://github.com/nickgillian/grt/wiki Gesture Recognition Toolkit|GRT] C++ real-time gesture-recognition toolkit implements DTW.\n* The [http://biointelligence.hu/pyhubs/ PyHubs] software package implements DTW and nearest-neighbour classifiers, as well as their extensions (hubness-aware classifiers).\n* The [https://github.com/talcs/simpledtw simpledtw] Python library implements the classic ''O''(''NM'') Dynamic Programming algorithm and bases on Numpy. It supports values of any dimension, as well as using custom norm functions for the distances. It is licensed under the MIT license.\n\n==Applications==\n\n===Spoken-word recognition===\nDue to different speaking rates, a non-linear fluctuation occurs in speech pattern versus time axis, which needs to be eliminated.<ref>{{cite journal|last1=Sakoe|first1=Hiroaki|last2=Chiba|first2=Seibi|title=Dynamic programming algorithm optimization for spoken word recognition|journal=IEEE Transactions on Acoustics, Speech, and Signal Processing|volume=26|issue=1|pages=43–49|doi=10.1109/tassp.1978.1163055|year=1978}}</ref> DP matching is a pattern-matching algorithm based on [[Dynamic programming|dynamic programming (DP),]] which uses a time-normalization effect, where the fluctuations in the time axis are modeled using a non-linear time-warping function. Considering any two speech patterns, we can get rid of their timing differences by warping the time axis of one so that the maximal coincidence is attained with the other. Moreover, if the warping function is allowed to take any possible value, {{clarify span|very less|date=October 2017}} distinction can be made between words belonging to different categories. So, to enhance the distinction between words belonging to different categories, restrictions were imposed on the warping function slope.\n\n===Correlation power analysis===\nUnstable clocks are used to defeat naive [[power analysis]]. Several techniques are used to counter this defense, one of which is dynamic time warping.\n\n==See also==\n* [[Levenshtein distance]]\n* [[Elastic matching]]\n* [[Fréchet distance]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n* {{cite journal | last1 = Vintsyuk | first1 = T. K. | year = 1968 | title = Speech discrimination by dynamic programming | url = | journal = Kibernetika | volume = 4 | issue = | pages = 81–88 }}\n* {{cite journal | last1 = Sakoe | first1 = H. | last2 = Chiba | year = 1978 | title = Dynamic programming algorithm optimization for spoken word recognition | url = | journal = IEEE Transactions on Acoustics, Speech, and Signal Processing | volume = 26 | issue = 1| pages = 43–49 | doi=10.1109/tassp.1978.1163055}}\n* {{cite journal|last1=Myers|first1=C. S.|last2=Rabiner|first2=L. R.|title=A Comparative Study of Several Dynamic Time-Warping Algorithms for Connected-Word Recognition|journal=Bell System Technical Journal|volume=60|issue=7|year=1981|pages=1389–1409|issn=0005-8580|doi=10.1002/j.1538-7305.1981.tb00272.x}}\n* {{cite book | last1=Rabiner | first1=Lawrence | last2=Juang | first2= Biing-Hwang |title=Fundamentals of speech recognition | publisher=PTR Prentice Hall | location=Englewood Cliffs, N.J. | year=1993 | isbn=978-0-13-015157-5 | ref=harv | chapter= Chapter 4: Pattern-Comparison Techniques}}\n* {{cite book\n| last = Müller\n| first = Meinard\n| title = Dynamic Time Warping. In Information Retrieval for Music and Motion, chapter 4, pages 69-84\n| url = https://www.springer.com/cda/content/document/cda_downloaddocument/9783540740476-1.pdf?SGWID=0-0-45-452103-p173751818\n| publisher = Springer\n| year = 2007\n| doi = 10.1007/978-3-540-74048-3\n| isbn = 978-3-540-74047-6}}\n* <!--ref name=\"ACM TKDD 7:3\"-->{{cite journal | title=Addressing Big Data Time Series: Mining Trillions of Time Series Subsequences Under Dynamic Time Warping | author=Rakthanmanon, Thanawin | journal=ACM Transactions on Knowledge Discovery from Data |date=September 2013  | volume=7 | issue=3 | pages=10:1–10:31 | doi=10.1145/2510000/2500489| doi-broken-date=2019-02-15 }}\n\n[[Category:Dynamic programming]]\n[[Category:Articles with example pseudocode]]\n[[Category:Machine learning algorithms]]\n[[Category:Multivariate time series]]"
    },
    {
      "title": "Earley parser",
      "url": "https://en.wikipedia.org/wiki/Earley_parser",
      "text": "\nIn [[computer science]], the '''Earley parser''' is an [[algorithm]] for [[parsing]] [[String (computer science)|strings]] that belong to a given [[context-free language]], though (depending on the variant) it may suffer problems with certain nullable grammars.<ref>{{cite web|last=Kegler|first=Jeffrey|title=What is the Marpa algorithm?|url=http://blogs.perl.org/users/jeffrey_kegler/2011/11/what-is-the-marpa-algorithm.html|accessdate=20 August 2013}}</ref> The algorithm, named after its inventor, [[Jay Earley]], is a [[chart parser]] that uses [[dynamic programming]]; it is mainly used for parsing in [[computational linguistics]]. It was first introduced in his dissertation<ref name=Earley1>{{cite book\n | last=Earley\n | first=Jay\n | title=An Efficient Context-Free Parsing Algorithm\n | year=1968\n | publisher=Carnegie-Mellon Dissertation\n | url=http://reports-archive.adm.cs.cmu.edu/anon/anon/usr/ftp/scan/CMU-CS-68-earley.pdf}}</ref> in 1968 (and later appeared in an abbreviated, more legible, form in a journal<ref name=\"Earley2\">{{citation\n | last = Earley | first = Jay | authorlink = Jay Earley\n | doi = 10.1145/362007.362035 | url = https://web.archive.org/web/20040708052627/http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/cmt-55/lti/Courses/711/Class-notes/p94-earley.pdf\n | issue = 2\n | journal = [[Communications of the ACM]]\n | pages = 94–102\n | title = An efficient context-free parsing algorithm\n | volume = 13\n | year = 1970}}</ref>).\n\nEarley parsers are appealing because they can parse all context-free languages, unlike [[LR parser]]s and [[LL parser]]s, which are more typically used in [[compiler]]s but which can only handle restricted classes of languages.  The Earley parser executes in cubic time in the general case <math>{O}(n^3)</math>, where ''n'' is the length of the parsed string, quadratic time for unambiguous grammars <math>{O}(n^2)</math>,<ref>{{cite book | isbn=978-0-201-02988-8 | author=John E. Hopcroft and Jeffrey D. Ullman | title=Introduction to Automata Theory, Languages, and Computation | location=Reading/MA | publisher=Addison-Wesley | year=1979 }} p.145</ref> and linear time for all [[LR parser|LR(k) grammars]]. It performs particularly well when the rules are written [[left recursion|left-recursively]].\n\n== Earley recogniser ==\nThe following algorithm describes the Earley recogniser. The recogniser can be easily modified to create a parse tree as it recognises, and in that way can be turned into a parser.\n\n== The algorithm ==\nIn the following descriptions, α, β, and γ represent any [[string (computer science)|string]] of [[Terminal and nonterminal symbols|terminals/nonterminals]] (including the [[empty string]]), X and Y represent single nonterminals, and ''a'' represents a terminal symbol.\n\nEarley's algorithm is a top-down [[dynamic programming]] algorithm. In the following, we use Earley's dot notation: given a [[Formal grammar#The syntax of grammars|production]] X → αβ, the notation X → α • β represents a condition in which α has already been parsed and β is expected.\n\nInput position 0 is the position prior to input.  Input position ''n'' is the position after accepting the ''n''th token.  (Informally, input positions can be thought of as locations at [[Lexical analysis|token]] boundaries.)  For every input position, the parser generates a ''state set''.  Each state is a [[tuple]] (X → α • β, ''i''), consisting of\n\n* the production currently being matched (X → α β)\n* our current position in that production (represented by the dot)\n* the position ''i'' in the input at which the matching of this production began: the ''origin position''\n\n(Earley's original algorithm included a look-ahead in the state; later research showed this to have little practical effect on the parsing efficiency, and it has subsequently been dropped from most implementations.)\n\nThe state set at input position ''k'' is called S(''k'').  The parser is seeded with S(0) consisting of only the top-level rule.  The parser then repeatedly executes three operations:  ''prediction'', ''scanning'', and ''completion''.\n\n* ''Prediction'':  For every state in S(''k'') of the form (X → α • Y β, ''j'') (where ''j'' is the origin position as above), add (Y → • γ, ''k'') to S(''k'') for every production in the grammar with Y on the left-hand side (Y → γ).\n* ''Scanning'': If ''a'' is the next symbol in the input stream, for every state in S(''k'') of the form (X → α • ''a'' β, ''j''), add (X → α ''a'' • β, ''j'') to S(''k''+1).\n* ''Completion'': For every state in S(''k'') of the form (Y → γ •, ''j''), find all states in S(''j'') of the form (X → α • Y β, ''i'') and add (X → α Y • β, ''i'') to S(''k'').\n\nIt is important to note that duplicate states are not added to the state set, only new ones.  These three operations are repeated until no new states can be added to the set.  The set is generally implemented as a queue of states to process, with the operation to be performed depending on what kind of state it is.\n\nThe algorithm accepts if (X → γ •, 0) ends up in S(''n''), where (X → γ) is the top level-rule and ''n'' the input length, otherwise it rejects.\n\n== Pseudocode ==\nAdapted from Speech and Language Processing<ref name=Jurafsky>{{cite book|last=Jurafsky|first=D.|title=Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition|year=2009|publisher=Pearson Prentice Hall|isbn=9780131873216|url=https://books.google.com/books?id=fZmj5UNK8AQC}}</ref> by Daniel Jurafsky and James H. Martin, \n\n<syntaxhighlight lang=\"text\">\nDECLARE ARRAY S;\n\nfunction INIT(words)\n    S ← CREATE-ARRAY(LENGTH(words) + 1)\n    for k ← from 0 to LENGTH(words) do\n        S[k] ← EMPTY-ORDERED-SET\n\nfunction EARLEY-PARSE(words, grammar)\n    INIT(words)\n    ADD-TO-SET((γ → •S, 0), S[0])\n    for k ← from 0 to LENGTH(words) do\n        for each state in S[k] do  // S[k] can expand during this loop\n            if not FINISHED(state) then\n                if NEXT-ELEMENT-OF(state) is a nonterminal then\n                    PREDICTOR(state, k, grammar)         // non-terminal\n                else do\n                    SCANNER(state, k, words)             // terminal\n            else do\n                COMPLETER(state, k)\n        end\n    end\n    return chart\n\nprocedure PREDICTOR((A → α•Bβ, j), k, grammar)\n    for each (B → γ) in GRAMMAR-RULES-FOR(B, grammar) do\n        ADD-TO-SET((B → •γ, k), S[k])\n    end\n\nprocedure SCANNER((A → α•aβ, j), k, words)\n    if a ⊂ PARTS-OF-SPEECH(words[k]) then\n        ADD-TO-SET((A → αa•β, j), S[k+1])\n    end\n\nprocedure COMPLETER((B → γ•, x), k)\n    for each (A → α•Bβ, j) in S[x] do\n        ADD-TO-SET((A → αB•β, j), S[k])\n    end\n</syntaxhighlight>\n\n== Example ==\nConsider the following simple grammar for arithmetic expressions:\n<syntaxhighlight lang=\"text\">\n<P> ::= <S>      # the start rule\n<S> ::= <S> \"+\" <M> | <M>\n<M> ::= <M> \"*\" <T> | <T>\n<T> ::= \"1\" | \"2\" | \"3\" | \"4\"\n</syntaxhighlight>\nWith the input:\n 2 + 3 * 4\n\nThis is the sequence of state sets:\n{| class=\"wikitable\"\n! (state no.) !! Production !! (Origin) !! Comment\n|-----------------------------------------\n! scope=\"row\" colspan=\"4\" style=\"text-align:left; background:#e9e9e9;font-family:monospace\" | S(0): • 2 + 3 * 4\n|-\n| 1 || style=\"font-family:monospace\" |P → • S          || 0 ||  start rule \n|-\n| 2 || style=\"font-family:monospace\" |S → • S + M      || 0 ||  predict from (1) \n|-\n| 3 || style=\"font-family:monospace\" |S → • M          || 0 ||  predict from (1) \n|-\n| 4 || style=\"font-family:monospace\" |M → • M * T      || 0 ||  predict from (3) \n|-\n| 5 || style=\"font-family:monospace\" |M → • T          || 0 ||  predict from (3) \n|-\n| 6 || style=\"font-family:monospace\" |T → • number     || 0 ||  predict from (5) \n|-\n! scope=\"row\" colspan=\"4\" style=\"text-align:left; background:#e9e9e9;font-family:monospace\" | S(1): 2 • + 3 * 4\n|-\n| 1 || style=\"font-family:monospace\" |T → number •     || 0 ||  scan from S(0)(6) \n|-\n| 2 || style=\"font-family:monospace\" |M → T •          || 0 ||  complete from (1) and S(0)(5) \n|-\n| 3 || style=\"font-family:monospace\" |M → M • * T      || 0 ||  complete from (2) and S(0)(4) \n|-\n| 4 || style=\"font-family:monospace\" |S → M •          || 0 ||  complete from (2) and S(0)(3) \n|-\n| 5 || style=\"font-family:monospace\" |S → S • + M      || 0 ||  complete from (4) and S(0)(2) \n|-\n| 6 || style=\"font-family:monospace\" |P → S •          || 0 ||  complete from (4) and S(0)(1) \n|-\n! scope=\"row\" colspan=\"4\" style=\"text-align:left; background:#e9e9e9;font-family:monospace\" | S(2): 2 + • 3 * 4\n|-\n| 1 || style=\"font-family:monospace\" |S → S + • M      || 0 ||  scan from S(1)(5) \n|-\n| 2 || style=\"font-family:monospace\" |M → • M * T      || 2 ||  predict from (1) \n|-\n| 3 || style=\"font-family:monospace\" |M → • T          || 2 ||  predict from (1) \n|-\n| 4 || style=\"font-family:monospace\" |T → • number     || 2 ||  predict from (3) \n|-\n! scope=\"row\" colspan=\"4\" style=\"text-align:left; background:#e9e9e9;font-family:monospace\" | S(3): 2 + 3 • * 4\n|-\n| 1 || style=\"font-family:monospace\" |T → number •     || 2 ||  scan from S(2)(4) \n|-\n| 2 || style=\"font-family:monospace\" |M → T •          || 2 ||  complete from (1) and S(2)(3) \n|-\n| 3 || style=\"font-family:monospace\" |M → M • * T      || 2 ||  complete from (2) and S(2)(2) \n|-\n| 4 || style=\"font-family:monospace\" |S → S + M •      || 0 ||  complete from (2) and S(2)(1) \n|-\n| 5 || style=\"font-family:monospace\" |S → S • + M      || 0 ||  complete from (4) and S(0)(2) \n|-\n| 6 || style=\"font-family:monospace\" |P → S •          || 0 ||  complete from (4) and S(0)(1) \n|-\n! scope=\"row\" colspan=\"4\" style=\"text-align:left; background:#e9e9e9;font-family:monospace\" | S(4): 2 + 3 * • 4\n|-\n| 1 || style=\"font-family:monospace\" |M → M * • T      || 2 ||  scan from S(3)(3) \n|-\n| 2 || style=\"font-family:monospace\" |T → • number     || 4 ||  predict from (1) \n|-\n! scope=\"row\" colspan=\"4\" style=\"text-align:left; background:#e9e9e9;font-family:monospace\" | S(5): 2 + 3 * 4 •\n|-\n| 1 || style=\"font-family:monospace\" |T → number •     || 4 ||  scan from S(4)(2) \n|-\n| 2 || style=\"font-family:monospace\" |M → M * T •      || 2 ||  complete from (1) and S(4)(1) \n|-\n| 3 || style=\"font-family:monospace\" |M → M • * T      || 2 ||  complete from (2) and S(2)(2) \n|-\n| 4 || style=\"font-family:monospace\" |S → S + M •      || 0 ||  complete from (2) and S(2)(1) \n|-\n| 5 || style=\"font-family:monospace\" |S → S • + M      || 0 ||  complete from (4) and S(0)(2) \n|-\n| 6 || style=\"font-family:monospace\" |P → S •          || 0 ||  complete from (4) and S(0)(1) \n|-\n|}\nThe state (P → S •, 0) represents a completed parse.  This state also appears in S(3) and S(1), which are complete sentences.\n\n== Constructing the parse forest ==\nEarley's dissertation<ref name=Earley3>{{cite book\n | last=Earley\n | first=Jay\n | title=An Efficient Context-Free Parsing Algorithm\n | year=1968\n | publisher=Carnegie-Mellon Dissertation\n | page=106\n | url=http://reports-archive.adm.cs.cmu.edu/anon/anon/usr/ftp/scan/CMU-CS-68-earley.pdf}}</ref> briefly describes an algorithm for constructing parse trees by adding a set of pointers from each non-terminal in an Earley item back to the items that caused it to be recognized.  But [[Masaru Tomita|Tomita]] noticed<ref>{{cite book|last1=Tomita|first1=Masaru|title=Efficient Parsing for Natural Language: A Fast Algorithm for Practical Systems|date=April 17, 2013|publisher=Springer Science and Business Media|isbn=978-1475718850|page=74|url=https://books.google.com/?id=DAjkBwAAQBAJ&lpg=PP13&dq=Tomita%20Efficient%20Parsing%20for%20natural%20Language&pg=PA74#v=onepage&q&f=false|accessdate=16 September 2015}}</ref> that this does not take into account the relations between symbols, so if we consider the grammar S → SS | b and the string bbb, it only notes that each S can match one or two b's, and thus produces spurious derivations for bb and bbbb as well as the two correct derivations for bbb.\n\nAnother method<ref>{{cite journal|last1=Scott|first1=Elizabeth|title=SPPF-Style Parsing From Earley Recognizers|journal=Electronic Notes in Theoretical Computer Science|date=April 1, 2008|volume=203|issue=2|pages=53–67|doi=10.1016/j.entcs.2008.03.044}}</ref> is to build the parse forest as you go, augmenting each Earley item with a pointer to a shared packed parse forest (SPPF) node labelled with a triple (s, i, j) where s is a symbol or an LR(0) item (production rule with dot), and i and j give the section of the input string derived by this node. A node's contents are either a pair of child pointers giving a single derivation, or a list of \"packed\" nodes each containing a pair of pointers and representing one derivation.  SPPF nodes are unique (there is only one with a given label), but may contain more than one derivation for ambiguous parses.  So even if an operation does not add an Earley item (because it already exists), it may still add a derivation to the item's parse forest.\n\n* Predicted items have a null SPPF pointer.\n* The scanner creates an SPPF node representing the non-terminal it is scanning.\n* Then when the scanner or completer advance an item, they add a derivation whose children are the node from the item whose dot was advanced, and the one for the new symbol that was advanced over (the non-terminal or completed item).\n\nNote also that SPPF nodes are never labeled with a completed LR(0) item: instead they are labelled with the symbol that is produced so that all derivations are combined under one node regardless of which alternative production they come from.\n\n== See also ==\n* [[CYK algorithm]]\n* [[Context-free grammar]]\n* [[List of algorithms#Parsing|Parsing Algorithms]]\n\n== Citations ==\n{{Reflist}}\n\n== Other reference materials ==\n*{{cite journal\n | last1 = Aycock | first1 = John\n | last2 = Horspool | first2 = R. Nigel | author2-link = Nigel Horspool\n | doi = 10.1093/comjnl/45.6.620\n | issue = 6\n | journal = [[The Computer Journal]]\n | pages = 620–630\n | title = Practical Earley Parsing\n | volume = 45\n | year = 2002| citeseerx = 10.1.1.12.4254\n }}\n*{{citation\n | last = Leo | first = Joop M. I. M.\n | doi = 10.1016/0304-3975(91)90180-A\n | issue = 1\n | journal = [[Theoretical Computer Science (journal)|Theoretical Computer Science]]\n | mr = 1112117\n | pages = 165–176\n | title = A general context-free parsing algorithm running in linear time on every LR(''k'') grammar without using lookahead\n | volume = 82\n | year = 1991\n}}\n\n*{{cite conference |first= Masaru|last= Tomita|title= LR parsers for natural languages |conference= 10th International Conference on Computational Linguistics |booktitle= COLING|pages= 354–357|year= 1984|url=https://aclanthology.info/pdf/P/P84/P84-1073.pdf}}\n\n== Implementations ==\n\n=== C, C++ ===\n* [https://github.com/vnmakarov/yaep 'Yet Another Earley Parser (YAEP)'] – [[C (programming language)|C]]/[[C++]] libraries\n* [https://bitbucket.org/amirouche/c-earley-parser/src 'C Earley Parser'] – an Earley parser C\n\n=== Haskell ===\n* [https://hackage.haskell.org/package/Earley 'Earley'] – an Earley parser [[Domain-specific language|DSL]] in [[Haskell (programming language)|Haskell]]\n\n=== Java ===\n* [http://www.cs.umanitoba.ca/~comp4190/Earley/Earley.java] – a Java implementation of the Earley algorithm\n* [http://linguateca.dei.uc.pt/index.php?sep=recursos PEN] – a Java library that implements the Earley algorithm\n* [http://www.coffeeblack.org/#projects-pep Pep] – a Java library that implements the Earley algorithm and provides charts and parse trees as parsing artifacts\n* [https://github.com/digitalheir/java-probabilistic-earley-parser digitalheir/java-probabilistic-earley-parser] - a Java library that implements the probabilistic Earley algorithm, which is useful to determine the most likely parse tree from an ambiguous sentence\n\n=== C# ===\n\n* [https://github.com/coonsta/earley coonsta/earley] - An Earley parser in C#\n* [https://github.com/patrickhuber/Pliant patrickhuber/pliant] - An Earley parser that integrates the improvements adopted by Marpa and demonstrates Elizabeth Scott's tree building algorithm.\n* [https://github.com/ellisonch/CFGLib ellisonch/CFGLib] - Probabilistic Context Free Grammar (PCFG) Library for C# (Earley + SPPF, CYK)\n\n=== JavaScript ===\n* [https://github.com/Hardmath123/nearley Nearley] – an Earley parser that's starting to integrate the improvements that Marpa adopted\n* [https://joshuagrams.github.io/pep/ A Pint-sized Earley Parser] – a toy parser (with annotated pseudocode) to demonstrate Elizabeth Scott's technique for building the shared packed parse forest\n* [https://github.com/lagodiuk/earley-parser-js lagodiuk/earley-parser-js] – a tiny JavaScript implementation of Earley parser (including generation of the parsing-forest)\n* [https://github.com/digitalheir/probabilistic-earley-parser-javascript digitalheir/probabilistic-earley-parser-javascript] - JavaScript implementation of the probabilistic Earley parser\n\n=== OCaml ===\n* [https://github.com/tomjridge/tjr_simple_earley Simple Earley] - An implementation of a simple Earley-like parsing algorithm, with documentation.\n\n=== Perl ===\n* [https://metacpan.org/module/Marpa::R2 Marpa::R2] – a [[Perl]] module.  [https://jeffreykegler.github.com/Marpa-web-site/ Marpa] is an Earley's algorithm that includes the improvements made by Joop Leo, and by Aycock and Horspool.\n* [https://metacpan.org/module/Parse::Earley Parse::Earley] – a Perl module implementing Jay Earley's original algorithm\n\n=== Python ===\n* [https://github.com/erezsh/lark/blob/master/lark/parsers/earley.py Lark] – an object-oriented, procedural implementation of an Earley parser in <200 lines of code\n* [http://nltk.org/ NLTK] – a [[Python (programming language)|Python]] toolkit with an Earley parser\n* [http://pages.cpsc.ucalgary.ca/~aycock/spark/ Spark] – an object-oriented ''little language framework'' for Python implementing an Earley parser\n* [https://pypi.python.org/pypi/spark_parser spark_parser] – updated and packaged version of the Spark parser above, which runs in both Python 3 and Python 2\n* [https://github.com/tomerfiliba/tau/blob/master/earley3.py earley3.py] – a stand-alone implementation of the algorithm in less than 150 lines of code, including generation of the parsing-forest and samples\n* [https://github.com/tomjridge/tjr_python_earley_parser tjr_python_earley_parser] - a minimal Earley parser in Python\n\n=== Common Lisp ===\n* [http://www.cliki.net/CL-EARLEY-PARSER CL-Earley-parser] – a Common Lisp library implementing an Earley parser\n\n=== Scheme, Racket ===\n* [https://web.archive.org/web/20160401103410/http://www.cavar.me/damir/charty/scheme/ Charty-Racket] – a [[Scheme (programming language)|Scheme]]-[[Racket (programming language)|Racket]] implementation of an Earley parser\n\n=== Resources ===\n* [http://accent.compilertools.net/Entire.html The Accent compiler-compiler]\n{{parsers}}\n[[Category:Parsing algorithms]]\n[[Category:Dynamic programming]]"
    },
    {
      "title": "Floyd–Warshall algorithm",
      "url": "https://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm",
      "text": "{{Redirect|Floyd's algorithm|cycle detection|Floyd's cycle-finding algorithm|computer graphics|Floyd–Steinberg dithering}}\n{{Infobox Algorithm\n|class=[[All-pairs shortest path problem]] (for weighted graphs)\n|image=\n|caption =\n|data=[[Graph (data structure)|Graph]]\n|time=<math>\\Theta (|V|^3)</math>\n|average-time=<math>\\Theta (|V|^3)</math>\n|best-time=<math>\\Theta (|V|^3)</math>\n|space=<math>\\Theta(|V|^2)</math>\n}}\n\n{{Tree search algorithm}}\nIn [[computer science]], the '''Floyd–Warshall algorithm''' (also known as '''Floyd's algorithm''', the '''Roy–Warshall algorithm''', the '''Roy–Floyd algorithm''', or the '''WFI algorithm''') is an [[algorithm]] for finding [[shortest path problem|shortest paths]] in a [[weighted graph]] with positive or negative edge weights (but with no negative cycles).<ref>{{Introduction to Algorithms|1}} See in particular Section 26.2, \"The Floyd–Warshall algorithm\", pp.&nbsp;558–565 and Section 26.4, \"A general framework for solving path problems in directed graphs\", pp.&nbsp;570–576.</ref><ref>{{cite book | author=Kenneth H. Rosen | title=Discrete Mathematics and Its Applications, 5th Edition | publisher = Addison Wesley | year=2003 | isbn=978-0-07-119881-3 }}</ref> A single execution of the algorithm will find the lengths (summed weights) of shortest paths between all pairs of vertices. Although it does not return details of the paths themselves, it is possible to reconstruct the paths with simple modifications to the algorithm. Versions of the algorithm can also be used for finding the [[transitive closure]] of a relation <math>R</math>, or (in connection with the [[Schulze method|Schulze voting system]]) [[widest path problem|widest paths]] between all pairs of vertices in a weighted graph.\n\n==History and naming==\nThe Floyd–Warshall algorithm is an example of [[dynamic programming]], and was published in its currently recognized form by [[Robert Floyd]] in 1962.<ref>{{cite journal | first = Robert W. | last = Floyd | authorlink = Robert W. Floyd | title = Algorithm 97: Shortest Path | journal = [[Communications of the ACM]] | volume = 5 | issue = 6 | page = 345 | date=  June 1962 | doi = 10.1145/367766.368168 }}</ref>  However, it is essentially the same as algorithms previously published by [[Bernard Roy]] in 1959<ref>{{cite journal | first = Bernard | last = Roy |authorlink=Bernard Roy| title = Transitivité et connexité. | journal = [[C. R. Acad. Sci. Paris]] | volume = 249 | pages = 216–218 | year=  1959 ||url=https://gallica.bnf.fr/ark:/12148/bpt6k3201c/f222.image |language=French }}\n</ref> and also by [[Stephen Warshall]] in 1962<ref>{{cite journal | first = Stephen | last = Warshall | title = A theorem on Boolean matrices | journal = [[Journal of the ACM]] | volume = 9 | issue = 1 | pages = 11–12 | date=  January 1962 | doi = 10.1145/321105.321107 }}</ref> for finding the transitive closure of a graph,<ref>{{mathworld|id=Floyd-WarshallAlgorithm | title = Floyd-Warshall Algorithm}}</ref> and is closely related to [[Kleene's algorithm]] (published in 1956) for converting a [[deterministic finite automaton]] into a [[regular expression]].<ref>{{cite book | authorlink = Stephen Cole Kleene | first = S. C. | last = Kleene | chapter = Representation of events in nerve nets and finite automata | title = Automata Studies | editor = [[Claude Elwood Shannon|C. E. Shannon]] and [[John McCarthy (computer scientist)|J. McCarthy]] | pages = 3–42 | publisher = Princeton University Press | year=  1956 }}</ref> The modern formulation of the algorithm as three nested for-loops was first described by Peter Ingerman, also in 1962.<ref>{{cite journal | first = Peter Z. | last = Ingerman | title = Algorithm 141: Path Matrix | journal = [[Communications of the ACM]] | volume = 5 | number = 11 | page = 556 | date = November 1962 | doi = 10.1145/368996.369016 }}</ref>\n\n==Algorithm==\nThe Floyd–Warshall algorithm compares all possible paths through the graph between each pair of vertices. It is able to do this with <math>\\Theta(|V|^3)</math> comparisons in a graph, even though there may be up to <math>\\Omega (|V|^2)</math> edges in the graph, and every combination of edges is tested.  It does so by incrementally improving an estimate on the shortest path between two vertices, until the estimate is optimal.\n\nConsider a graph <math>G</math> with vertices <math>V</math> numbered 1 through&nbsp;<math>N</math>. Further consider a function <math>\\mathrm{shortestPath}(i,j,k)</math> that returns the shortest possible path from <math>i</math> to <math>j</math> using vertices only from the set <math>\\{1,2,\\ldots,k\\}</math> as intermediate points along the way.  Now, given this function, our goal is to find the shortest path from each <math>i</math> to each <math>j</math> using only vertices in <math>\\{1,2,\\ldots,N\\}</math>.\n\nFor each of these pairs of vertices, the <math>\\mathrm{shortestPath}(i,j,k)</math> could be either\n\n:(1) a path that does not go through <math>k</math> (only uses vertices in the set <math>\\{1,\\ldots,k-1\\}</math>.)\n\nor\n\n:(2) a path that does go through <math>k</math> (from <math>i</math> to <math>k</math> and then from <math>k</math> to <math>j</math>, both only using intermediate vertices in&nbsp;<math>\\{1,\\ldots,k-1\\}</math>)\n\nWe know that the best path from <math>i</math> to <math>j</math> that only uses vertices <math>1</math> through <math>k-1</math> is defined by <math>\\mathrm{shortestPath}(i,j,k-1)</math>, and it is clear that if there were a better path from <math>i</math> to <math>k</math> to <math>j</math>, then the length of this path would be the concatenation of the shortest path from <math>i</math> to <math>k</math> (only using intermediate vertices in <math>\\{1,\\ldots,k-1\\}</math>) and the shortest path from <math>k</math> to <math>j</math> (only using intermediate vertices in&nbsp;<math>\\{1,\\ldots,k-1\\}</math>).\n\nIf <math>w(i,j)</math> is the weight of the edge between vertices <math>i</math> and <math>j</math>, we can define <math>\\mathrm{shortestPath}(i,j,k)</math> in terms of the following [[Recursion|recursive]] formula: the base case is\n: <math>\\mathrm{shortestPath}(i,j,0) = w(i,j)</math>\nand the recursive case is\n: <math>\\mathrm{shortestPath}(i,j,k) =</math>\n:: <math>\\mathrm{min}\\Big(\\mathrm{shortestPath}(i,j,k-1),</math>\n::: <math>\\mathrm{shortestPath}(i,k,k-1)+\\mathrm{shortestPath}(k,j,k-1)\\Big)</math>.\n\nThis formula is the heart of the Floyd–Warshall algorithm. The algorithm works by first computing <math>\\mathrm{shortestPath}(i,j,k)</math> for all <math>(i,j)</math> pairs for <math>k=1</math>, then <math>k=2</math>, and so on.  This process continues until <math>k=N</math>, and we have found the shortest path for all <math>(i,j)</math> pairs using any intermediate vertices. Pseudocode for this basic version follows:{{or|date=June 2019}}\n\n 1 '''let''' dist be a |V| × |V| array of minimum distances initialized to ∞ (infinity)\n 2 '''for each''' edge (''u'',''v'')\n 3    dist[''u''][''v''] &larr; w(''u'',''v'')  ''// the weight of the edge (''u'',''v'')\n 4 '''for each''' vertex ''v''\n 5    dist[''v''][''v''] &larr; 0\n 6 '''for''' ''k'' '''from''' 1 '''to''' |V|\n 7    '''for''' ''i'' '''from''' 1 '''to''' |V|\n 8       '''for''' ''j'' '''from''' 1 '''to''' |V|\n 9          '''if''' dist[''i''][''j''] > dist[''i''][''k''] + dist[''k''][''j''] \n 10             dist[''i''][''j''] &larr; dist[''i''][''k''] + dist[''k''][''j'']\n 11         '''end if'''\n\n==Example==\nThe algorithm above is executed on the graph on the left below:\n\n[[File:Floyd-Warshall example.svg|600px]]\n\nPrior to the first recursion of the outer loop, labeled {{math|1=''k'' = 0}} above, the only known paths correspond to the single edges in the graph. At {{math|1=''k'' = 1}}, paths that go through the vertex 1 are found: in particular, the path [2,1,3] is found, replacing the path [2,3] which has fewer edges but is longer (in terms of weight). At {{math|1=''k'' = 2}}, paths going through the vertices {1,2} are found. The red and blue boxes show how the path [4,2,1,3] is assembled from the two known paths [4,2] and [2,1,3] encountered in previous iterations, with 2 in the intersection. The path [4,2,3] is not considered, because [2,1,3] is the shortest path encountered so far from 2 to 3. At {{math|1=''k'' = 3}}, paths going through the vertices {1,2,3} are found. Finally, at {{math|1=''k'' = 4}}, all shortest paths are found.\n\nThe distance matrix at each iteration of {{mvar|k}}, with the updated distances in '''bold''', will be:\n{| class=wikitable style=\"float:left; margin:10px; text-align:center;\"\n|+\n| colspan=\"2\" rowspan=\"2\" |{{math|1=''k'' = 0}}\n| colspan=\"4\" |{{mvar|j}}\n|-\n! 1 !! 2 !! 3 !! 4\n|-\n| rowspan=\"4\" |{{mvar|i}}\n! 1\n| 0 || &infin; || −2 || &infin;\n|-\n! 2\n| 4 || 0 || 3 || &infin;\n|-\n! 3\n| &infin; || &infin; || 0 || 2\n|-\n! 4\n| &infin; || −1 || &infin; || 0\n|}\n\n{| class=wikitable style=\"float:left; margin:10px; text-align:center;\"\n|+\n| colspan=\"2\" rowspan=\"2\" |{{math|1=''k'' = 1}}\n| colspan=\"4\" |{{mvar|j}}\n|-\n! 1 !! 2 !! 3 !! 4\n|-\n| rowspan=\"4\" |{{mvar|i}}\n! 1\n| 0 || &infin; || −2 || &infin;\n|-\n! 2\n| 4 || 0 || '''2''' || &infin;\n|-\n! 3\n| &infin; || &infin; || 0 || 2\n|-\n! 4\n| &infin; || −1 || &infin; || 0\n|}\n\n{| class=wikitable style=\"float:left; margin:10px; text-align:center;\"\n|+\n| colspan=\"2\" rowspan=\"2\" |{{math|1=''k'' = 2}}\n| colspan=\"4\" |{{mvar|j}}\n|-\n! 1 !! 2 !! 3 !! 4\n|-\n| rowspan=\"4\" |{{mvar|i}}\n! 1\n| 0 || &infin; || −2 || &infin;\n|-\n! 2\n| 4 || 0 || 2 || &infin;\n|-\n! 3\n| &infin; || &infin; || 0 || 2\n|-\n! 4\n| '''3''' || −1 || '''1''' || 0\n|}\n\n{| class=wikitable style=\"float:left; margin:10px; text-align:center;\"\n|+\n| colspan=\"2\" rowspan=\"2\" |{{math|1=''k'' = 3}}\n| colspan=\"4\" |{{mvar|j}}\n|-\n! 1 !! 2 !! 3 !! 4\n|-\n| rowspan=\"4\" |{{mvar|i}}\n! 1\n| 0 || &infin; || −2 || '''0'''\n|-\n! 2\n| 4 || 0 || 2 ||'''4'''\n|-\n! 3\n| &infin; || &infin; || 0 || 2\n|-\n! 4\n| 3 || −1 || 1 || 0\n|}\n\n{| class=wikitable style=\"float:left; margin:10px; text-align:center;\"\n|+\n| colspan=\"2\" rowspan=\"2\" |{{math|1=''k'' = 4}}\n| colspan=\"4\" |{{mvar|j}}\n|-\n! 1 !! 2 !! 3 !! 4\n|-\n| rowspan=\"4\" |{{mvar|i}}\n! 1\n| 0 || '''−1''' || −2 || 0\n|-\n! 2\n| 4 || 0 || 2 || 4\n|-\n! 3\n| '''5''' || '''1''' || 0 || 2\n|-\n! 4\n| 3 || −1 || 1 || 0\n|}\n{{clear}}\n\n==Behavior with negative cycles==\n\nA negative cycle is a cycle whose edges sum to a negative value.  There is no shortest path between any pair of vertices <math>i</math>, <math>j</math> which form part of a negative cycle,  because path-lengths from <math>i</math> to <math>j</math> can be arbitrarily small (negative).  For numerically meaningful output, the Floyd–Warshall algorithm assumes that there are no negative cycles.  Nevertheless, if there are negative cycles, the Floyd–Warshall algorithm can be used to detect them.  The intuition is as follows:\n\n* The Floyd–Warshall algorithm iteratively revises path lengths between all pairs of vertices <math>(i,j)</math>, including where <math>i=j</math>;\n* Initially, the length of the path <math>(i,i)</math> is zero;\n* A path <math>[i,k,\\ldots,i]</math> can only improve upon this if it has length less than zero, i.e. denotes a negative cycle;\n* Thus, after the algorithm, <math>(i,i)</math> will be negative if there exists a negative-length path from <math>i</math> back to <math>i</math>.\n\nHence, to detect negative [[Cycle (graph theory)|cycles]] using the Floyd–Warshall algorithm, one can inspect the diagonal of the path matrix, and the presence of a negative number indicates that the graph contains at least one negative cycle.<ref>{{cite web | first = Dorit | last = Hochbaum | authorlink = Dorit S. Hochbaum | url = http://www.ieor.berkeley.edu/~hochbaum/files/ieor266-2014.pdf | title = Section 8.9: Floyd-Warshall algorithm for all pairs shortest paths | work = Lecture Notes for IEOR 266: Graph Algorithms and Network Flows | date = 2014 | format = [[PDF]] | publisher = Department of Industrial Engineering and Operations Research, [[University of California, Berkeley]]}}</ref> To avoid numerical problems one should check for negative numbers on the diagonal of the path matrix within the inner for loop of the algorithm.<ref>\n{{cite journal\n | title =   The Floyd–Warshall algorithm on graphs with negative cycles\n | author =  Stefan Hougardy\n | journal = Information Processing Letters\n | url = http://www.sciencedirect.com/science/article/pii/S002001901000027X\n | volume = 110 \n | number = 8-9\n | date = April 2010\n | pages = 279–281\n | doi=10.1016/j.ipl.2010.02.001\n}}</ref> Obviously, in an undirected graph a negative edge creates a negative cycle <!-- \"Cycle\" might imply no repeated edges --> (i.e., a closed walk) involving its incident vertices. Considering all edges of the [[#Example|above]] example graph as undirected, e.g. the vertex sequence 4 – 2 – 4 is a cycle with weight sum −2.\n\n==Path reconstruction==\n\nThe Floyd–Warshall algorithm typically only provides the lengths of the paths between all pairs of vertices. With simple modifications, it is possible to create a method to reconstruct the actual path between any two endpoint vertices. While one may be inclined to store the actual path from each vertex to each other vertex, this is not necessary, and in fact, is very costly in terms of memory. Instead, the [[shortest-path tree]] can be calculated for each node in <math>\\Theta(|E|)</math> time using <math>\\Theta(|V|)</math> memory to store each tree which allows us to efficiently reconstruct a path from any two connected vertices.\n\n==== Pseudocode <ref>https://books.goalkicker.com/AlgorithmsBook/</ref> ====\n '''let''' dist be a <math>|V| \\times |V|</math> array of minimum distances initialized to <math>\\infty</math> (infinity)\n '''let''' next be a <math>|V| \\times |V|</math> array of vertex indices initialized to '''null'''\n \n '''procedure''' ''FloydWarshallWithPathReconstruction'' ()\n    '''for each''' edge (u,v)\n       dist[u][v] &larr; w(u,v)  ''// the weight of the edge (u,v)\n       next[u][v] &larr; v\n    '''for each''' vertex v\n       dist[''v''][''v''] &larr; 0\n       next[v][v] &larr; v\n    '''for''' k '''from''' 1 '''to''' |V| ''// standard Floyd-Warshall implementation\n       '''for''' i '''from''' 1 '''to''' |V|\n          '''for''' j '''from''' 1 '''to''' |V|\n             '''if''' dist[i][j] > dist[i][k] + dist[k][j] '''then'''\n                dist[i][j] &larr; dist[i][k] + dist[k][j]\n                next[i][j] &larr; next[i][k]\n\n '''procedure''' Path(u, v)\n    '''if''' next[u][v] = null '''then'''\n        '''return''' []\n    path = [u]\n    '''while u ≠ v'''\n        u &larr; next[u][v]\n        path.append(u)\n    '''return''' path\n\n==Analysis==\nLet <math>n</math> be <math>|V|</math>, the number of vertices. To find all <math>n^2</math> of \n<math>\\mathrm{shortestPath}(i,j,k)</math> (for all <math>i</math> and <math>j</math>) from those of\n<math>\\mathrm{shortestPath}(i,j,k-1)</math> requires <math>2n^2</math> operations. Since we begin with\n<math>\\mathrm{shortestPath}(i,j,0) = \\mathrm{edgeCost}(i,j)</math> and compute the sequence of <math>n</math> matrices <math>\\mathrm{shortestPath}(i,j,1)</math>, <math>\\mathrm{shortestPath}(i,j,2)</math>, <math>\\ldots</math>, <math>\\mathrm{shortestPath}(i,j,n)</math>, the total number of operations used is \n<math>n \\cdot 2n^2 = 2n^3</math>. Therefore, the [[Computational complexity theory|complexity]] of the algorithm is [[big theta|<math>\\Theta(n^3)</math>]].\n\n==Applications and generalizations==\nThe Floyd–Warshall algorithm can be used to solve the following problems, among others:\n* Shortest paths in directed graphs (Floyd's algorithm).\n* [[Transitive closure]] of directed graphs (Warshall's algorithm). In Warshall's original formulation of the algorithm, the graph is unweighted and represented by a Boolean adjacency matrix. Then the addition operation is replaced by [[logical conjunction]] (AND) and the minimum operation by [[logical disjunction]] (OR).\n* Finding a [[regular expression]] denoting the [[regular language]] accepted by a [[finite automaton]] ([[Kleene's algorithm]], a closely related generalization of the Floyd–Warshall algorithm)<ref>{{citation|title=Handbook of Graph Theory|series=Discrete Mathematics and Its Applications|first1=Jonathan L.|last1=Gross|first2=Jay|last2=Yellen|publisher=CRC Press|year=2003|page=65|url=https://books.google.com/books?id=mKkIGIea_BkC&pg=PA65|isbn=9780203490204}}.</ref>\n* [[invertible matrix|Inversion]] of [[real number|real]] [[matrix (mathematics)|matrices]] ([[Gauss–Jordan elimination|Gauss–Jordan algorithm]]) <ref>{{cite journal | title = Algebraic Structures for Transitive Closure | first = Rafael | last = Penaloza|citeseerx = 10.1.1.71.7650}}</ref>\n* Optimal routing. In this application one is interested in finding the path with the maximum flow between two vertices. This means that, rather than taking minima as in the pseudocode above, one instead takes maxima. The edge weights represent fixed constraints on flow. Path weights represent bottlenecks; so the addition operation above is replaced by the minimum operation.\n* Fast computation of [[Pathfinder network]]s.\n* [[Widest path problem|Widest paths/Maximum bandwidth paths]]\n* Computing canonical form of difference bound matrices (DBMs)\n* Computing the similarity between graphs\n\n==Implementations==\nImplementations are available for many [[programming language]]s.\n* For [[C++]], in the [http://www.boost.org/libs/graph/doc/ boost::graph] library\n* For [[C Sharp (programming language)|C#]], at [http://www.codeplex.com/quickgraph QuickGraph]\n* For [[C Sharp (programming language)|C#]], at [https://www.nuget.org/packages/QuickGraphPCL/3.6.61114.2 QuickGraphPCL] (A fork of QuickGraph with better compatibility with projects using Portable Class Libraries.)\n* For [[Java (programming language)|Java]], in the [http://commons.apache.org/sandbox/commons-graph/ Apache Commons Graph] library\n* For [[JavaScript]], in the [[Cytoscape]] library\n* For [[MATLAB]], in the [http://www.mathworks.com/matlabcentral/fileexchange/10922 Matlab_bgl] package\n* For [[Perl]], in the [https://metacpan.org/module/Graph Graph] module\n* For [[Python (programming language)|Python]], in the [[SciPy]] library (module [http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csgraph.floyd_warshall.html#scipy.sparse.csgraph.floyd_warshall scipy.sparse.csgraph]) or [[NetworkX]] library\n* For [[R programming language|R]], in packages [https://cran.r-project.org/web/packages/e1071/index.html e1071] and [https://cran.r-project.org/web/packages/Rfast/index.html Rfast]\n\n==Comparison with other shortest path algorithms==\nThe Floyd–Warshall algorithm is a good choice for computing paths between all pairs of vertices in [[dense graph]]s, in which most or all pairs of vertices are connected by edges. For [[sparse graph]]s with non-negative edge weights, a better choice is to use [[Dijkstra's algorithm]] from each possible starting vertex, since the running time of repeated Dijkstra (<math>O(|E||V|+|V|^2\\log|V|)</math> using [[Fibonacci heap]]s) is better than the <math>O(|V|^3)</math> running time of the Floyd–Warshall algorithm when <math>|E|</math> is significantly smaller than <math>|V|^2</math>. For sparse graphs with negative edges but no negative cycles, [[Johnson's algorithm]] can be used, with the same asymptotic running time as the repeated Dijkstra approach.\n\nThere are also known algorithms using [[fast matrix multiplication]] to speed up all-pairs shortest path computation in dense graphs, but these typically make extra assumptions on the edge weights (such as requiring them to be small integers).<ref>{{citation\n | last = Zwick | first = Uri | authorlink = Uri Zwick\n | date = May 2002\n | doi = 10.1145/567112.567114\n | issue = 3\n | journal = [[Journal of the ACM]]\n | pages = 289–317\n | title = All pairs shortest paths using bridging sets and rectangular matrix multiplication\n | volume = 49| arxiv = cs/0008011}}.</ref><ref>{{citation\n | last = Chan | first = Timothy M. | authorlink = Timothy M. Chan\n | date = January 2010\n | doi = 10.1137/08071990x\n | issue = 5\n | journal = [[SIAM Journal on Computing]]\n | pages = 2075–2089\n | title = More algorithms for all-pairs shortest paths in weighted graphs\n | volume = 39| citeseerx = 10.1.1.153.6864 }}.</ref> In addition, because of the high constant factors in their running time, they would only provide a speedup over the Floyd–Warshall algorithm for very large graphs.\n\n==References==\n{{Reflist}}\n\n==External links==\n{{commons category|Floyd-Warshall algorithm}}\n* [http://www.pms.informatik.uni-muenchen.de/lehre/compgeometry/Gosper/shortest_path/shortest_path.html#visualization Interactive animation of the Floyd–Warshall algorithm]\n* [https://www-m9.ma.tum.de/graph-algorithms/spp-floyd-warshall/index_en.html Interactive animation of the Floyd–Warshall algorithm (Technical University of Munich)]\n\n{{DEFAULTSORT:Floyd-Warshall algorithm}}\n[[Category:Graph algorithms]]\n[[Category:Routing algorithms]]\n[[Category:Polynomial-time problems]]\n[[Category:Articles with example pseudocode]]\n[[Category:Dynamic programming]]"
    },
    {
      "title": "Forward–backward algorithm",
      "url": "https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm",
      "text": "{{Inline|date=April 2018}}\n\nThe '''forward–backward algorithm''' is an [[Statistical_inference | inference]] [[algorithm]] for [[hidden Markov model]]s which computes the [[posterior probability|posterior]] [[marginal probability|marginals]] of all hidden state variables given a sequence of observations/emissions <math>o_{1:T}:= o_1,\\dots,o_T</math>, i.e. it computes, for all hidden state variables <math>X_t \\in \\{X_1, \\dots, X_T\\}</math>, the distribution <math>P(X_t\\ |\\ o_{1:T})</math>. This inference task is usually called ''smoothing''. The algorithm makes use of the principle of [[dynamic programming]] to efficiently compute the values that are required to obtain the posterior marginal distributions in two passes. The first pass goes forward in time while the second goes backward in time; hence the name ''forward–backward algorithm''.\n\nThe term ''forward–backward algorithm'' is also used to refer to any algorithm belonging to the general class of algorithms that operate on sequence models in a forward–backward manner. In this sense, the descriptions in the remainder of this article refer but to one specific instance of this class.\n\n==Overview ==\nIn the first pass, the forward–backward algorithm computes a set of forward probabilities which provide, for all <math>t \\in \\{1, \\dots, T\\}</math>, the probability of ending up in any particular state given the first <math>t</math> observations in the sequence, i.e. <math>P(X_t\\ |\\  o_{1:t})</math>. In the second pass, the algorithm computes a set of backward probabilities which provide the probability of observing the remaining observations given any starting point <math>t</math>, i.e. <math>P(o_{t+1:T}\\ |\\ X_t)</math>.  These two sets of probability distributions can then be combined to obtain the distribution over states at any specific point in time given the entire observation sequence:\n\n:<math>P(X_t\\ |\\ o_{1:T}) = P(X_t\\ |\\ o_{1:t}, o_{t+1:T}) \\propto P(o_{t+1:T}\\ |\\ X_t) P( X_t | o_{1:t})</math>\n\nThe last step follows from an application of the [[Bayes' rule]] and the [[conditional independence]] of <math>o_{t+1:T}</math> and <math>o_{1:t}</math> given <math>X_t</math>.\n\nAs outlined above, the algorithm involves three steps:\n\n# computing forward probabilities\n# computing backward probabilities\n# computing smoothed values.\n\nThe forward and backward steps may also be called \"forward message pass\" and \"backward message pass\" - these terms are due to the ''message-passing'' used in general [[belief propagation]] approaches. At each single observation in the sequence, probabilities to be used for calculations at the next observation are computed. The smoothing step can be calculated simultaneously during the backward pass. This step allows the algorithm to take into account any past observations of output for computing more accurate results.\n\nThe forward–backward algorithm can be used to find the most likely state for any point in time. It cannot, however, be used to find the most likely sequence of states (see [[Viterbi algorithm]]).\n\n==Forward probabilities==\nThe following description will use matrices of probability values rather than probability distributions, although in general the forward-backward algorithm can be applied to continuous as well as discrete probability models.\n\nWe transform the probability distributions related to a given [[hidden Markov model]] into matrix notation as follows.\nThe transition probabilities <math>\\mathbf{P}(X_t\\mid X_{t-1})</math> of a given random variable <math>X_t</math> representing all possible states in the hidden Markov model will be represented by the matrix <math>\\mathbf{T}</math> where the column index <math>i</math> will represent the target state and the row index <math>j</math> represents the start state. A transition from row-vector state <math>\\mathbf{\\pi_t}</math> to the incremental row-vector state <math>\\mathbf{\\pi_{t+1}}</math> is written as <math>\\mathbf{\\pi_{t+1}} = \\mathbf{\\pi_{t}} \\mathbf{T}</math>. The example below represents a system where the probability of staying in the same state after each step is 70% and the probability of transitioning to the other state is 30%.  The transition matrix is then:\n\n:<math>\\mathbf{T} = \\begin{pmatrix}\n  0.7 & 0.3 \\\\\n  0.3 & 0.7\n\\end{pmatrix}\n</math>\n\nIn a typical Markov model we would multiply a state vector by this matrix to obtain the probabilities for the subsequent state.  In a hidden Markov model the state is unknown, and we instead observe events associated with the possible states.  An event matrix of the form:\n\n:<math>\\mathbf{B} = \\begin{pmatrix}\n  0.9 & 0.1 \\\\\n  0.2 & 0.8\n\\end{pmatrix}\n</math>\n\nprovides the probabilities for observing events given a particular state.  In the above example, event 1 will be observed 90% of the time if we are in state 1 while event 2 has a 10% probability of occurring in this state.  In contrast, event 1 will only be observed 20% of the time if we are in state 2 and event 2 has an 80% chance of occurring.  Given an arbitrary row-vector describing the state of the system (<math>\\mathbf{\\pi}</math>), the probability of observing event j is then:\n\n:<math>\\mathbf{P}(O = j)=\\sum_{i} \\pi_i B_{i,j}</math>\n\nThis can be represented in matrix form by multiplying the state row-vector (<math>\\mathbf{\\pi}</math>) by an observation matrix (<math>\\mathbf{O_j} = \\mathrm{diag}(B_{*,o_j})</math>) containing only diagonal entries.  Each entry is the probability of the observed event given each state.  Continuing the above example, an observation of event 1 would be:\n\n:<math>\\mathbf{O_1} = \\begin{pmatrix}\n  0.9 & 0.0 \\\\\n  0.0 & 0.2\n\\end{pmatrix}\n</math>\n\nThis allows us to calculate the new unnormalized probabilities state vector <math>\\mathbf{\\pi '}</math> through Bayes rule, weighting by the likelihood that each element of <math>\\mathbf{\\pi}</math> generated event 1 as:\n\n:<math>\n\\mathbf{\\pi '} = \\mathbf{\\pi} \\mathbf{O_1}\n</math>\n\nWe can now make this general procedure specific to our series of observations. Assuming an initial state vector <math>\\mathbf{\\pi_0}</math>, (which can be optimized as a parameter through repetitions of the forward-back procedure), we begin with:\n\n:<math>\n\\mathbf{f_{0:0}} = \\mathbf{\\pi_0} \\mathbf{T} \\mathbf{O_{o(0)}} \n</math>\n\nThis process can be carried forward with additional observations using:\n\n:<math>\n\\mathbf{f_{0:t}} = \\mathbf{f_{0:t-1}} \\mathbf{T} \\mathbf{O_{o(t)}}  \n</math>\n\nThis value is the forward unnormalized probability vector.  The i'th entry of this vector provides:\n\n:<math>\n\\mathbf{f_{0:t}}(i) = \\mathbf{P}(o_1, o_2, \\dots, o_t, X_t=x_i | \\mathbf{\\pi} )\n</math>\n\nTypically, we will normalize the probability vector at each step so that its entries sum to 1.  A scaling factor is thus introduced at each step such that:\n\n:<math>\n\\mathbf{\\hat{f}_{0:t}} = c_t^{-1}\\ \\mathbf{\\hat{f}_{0:t-1}} \\mathbf{T} \\mathbf{O_{o(t)}}\n</math>\n\nwhere <math>\\mathbf{\\hat{f}_{0:t-1}}</math> represents the scaled vector from the previous step and <math>c_t</math> represents the scaling factor that causes the resulting vector's entries to sum to 1.  The product of the scaling factors is the total probability for observing the given events irrespective of the final states:\n\n:<math>\n\\mathbf{P}(o_1, o_2, \\dots, o_t|\\mathbf{\\pi}) = \\prod_{s=1}^t c_s\n</math>\n\nThis allows us to interpret the scaled probability vector as:\n\n:<math>\n\\mathbf{\\hat{f}_{0:t}}(i) =\n\\frac{\\mathbf{f_{0:t}}(i)}{\\prod_{s=1}^t c_s} =\n\\frac{\\mathbf{P}(o_1, o_2, \\dots, o_t, X_t=x_i | \\mathbf{\\pi} )}{\\mathbf{P}(o_1, o_2, \\dots, o_t|\\mathbf{\\pi})} =\n\\mathbf{P}(X_t=x_i | o_1, o_2, \\dots, o_t, \\mathbf{\\pi} )\n</math>\n\nWe thus find that the product of the scaling factors provides us with the total probability for observing the given sequence up to time t and that the scaled probability vector provides us with the probability of being in each state at this time.\n\n==Backward probabilities==\nA similar procedure can be constructed to find backward probabilities.  These intend to provide the probabilities:\n\n:<math>\n\\mathbf{b_{t:T}}(i) = \\mathbf{P}(o_{t+1}, o_{t+2}, \\dots, o_{T} | X_t=x_i )\n</math>\n\nThat is, we now want to assume that we start in a particular state (<math>X_t=x_i</math>), and we are now interested in the probability of observing all future events from this state.  Since the initial state is assumed as given (i.e. the prior probability of this state = 100%), we begin with:\n\n:<math>\n\\mathbf{b_{T:T}} = [1\\ 1\\ 1\\ \\dots]^T\n</math>\n\nNotice that we are now using a column vector while the forward probabilities used row vectors.  We can then work backwards using:\n\n:<math>\n\\mathbf{b_{t-1:T}} = \\mathbf{T}\\mathbf{O_t}\\mathbf{b_{t:T}}\n</math>\n\nWhile we could normalize this vector as well so that its entries sum to one, this is not usually done.  Noting that each entry contains the probability of the future event sequence given a particular initial state, normalizing this vector would be equivalent to applying Bayes' theorem to find the likelihood of each initial state given the future events (assuming uniform priors for the final state vector).  However, it is more common to scale this vector using the same <math>c_t</math> constants used in the forward probability calculations.  <math>\\mathbf{b_{T:T}}</math> is not scaled, but subsequent operations use:\n\n:<math>\n\\mathbf{\\hat{b}_{t-1:T}} = c_t^{-1} \\mathbf{T}\\mathbf{O_t}\\mathbf{\\hat{b}_{t:T}}\n</math>\n\nwhere <math>\\mathbf{\\hat{b}_{t:T}}</math> represents the previous, scaled vector.  This result is that the scaled probability vector is related to the backward probabilities by:\n\n:<math>\n\\mathbf{\\hat{b}_{t:T}}(i) =\n\\frac{\\mathbf{b_{t:T}}(i)}{\\prod_{s=t+1}^T c_s}\n</math>\n\nThis is useful because it allows us to find the total probability of being in each state at a given time, t, by multiplying these values:\n\n:<math>\n\\mathbf{\\gamma_t}(i) =\n\\mathbf{P}(X_t=x_i | o_1, o_2, \\dots, o_T, \\mathbf{\\pi}) =\n\\frac{ \\mathbf{P}(o_1, o_2, \\dots, o_T, X_t=x_i | \\mathbf{\\pi} ) }{ \\mathbf{P}(o_1, o_2, \\dots, o_T | \\mathbf{\\pi} ) } =\n\\frac{ \\mathbf{f_{0:t}}(i) \\cdot \\mathbf{b_{t:T}}(i) }{ \\prod_{s=1}^T c_s } =\n\\mathbf{\\hat{f}_{0:t}}(i) \\cdot \\mathbf{\\hat{b}_{t:T}}(i)\n</math>\n\nTo understand this, we note that <math>\\mathbf{f_{0:t}}(i) \\cdot \\mathbf{b_{t:T}}(i)</math> provides the probability for observing the given events in a way that passes through state <math>x_i</math> at time t.  This probability includes the forward probabilities covering all events up to time t as well as the backward probabilities which include all future events.  This is the numerator we are looking for in our equation, and we divide by the total probability of the observation sequence to normalize this value and extract only the probability that <math>X_t=x_i</math>.  These values are sometimes called the \"smoothed values\" as they combine the forward and backward probabilities to compute a final probability.\n\nThe values <math>\\mathbf{\\gamma_t}(i)</math> thus provide the probability of being in each state at time t.  As such, they are useful for determining the most probable state at any time.  It should be noted, however, that the term \"most probable state\" is somewhat ambiguous.  While the most probable state is the most likely to be correct at a given point, the sequence of individually probable states is not likely to be the most probable sequence.  This is because the probabilities for each point are calculated independently of each other. They do not take into account the transition probabilities between states, and it is thus possible to get states at two moments (t and t+1) that are both most probable at those time points but which have very little probability of occurring together, i.e. <math> \\mathbf{P}(X_t=x_i,X_{t+1}=x_j) \\neq \\mathbf{P}(X_t=x_i) \\mathbf{P}(X_{t+1}=x_j) </math>. The most probable sequence of states that produced an observation sequence can be found using the [[Viterbi algorithm]].\n\n==Example ==\nThis example takes as its basis the umbrella world in [[#RussellNorvig10|Russell & Norvig 2010 Chapter 15 pp. 567]] in which we would like to infer the weather given observation of another person either carrying or not carrying an umbrella.  We assume two possible states for the weather: state 1 = rain, state 2 = no rain.  We assume that the weather has a 70% chance of staying the same each day and a 30% chance of changing.  The transition probabilities are then:\n\n:<math>\\mathbf{T} = \\begin{pmatrix}\n  0.7 & 0.3 \\\\\n  0.3 & 0.7\n\\end{pmatrix}\n</math>\n\nWe also assume each state generates one of two possible events: event 1 = umbrella, event 2 = no umbrella.  The conditional probabilities for these occurring in each state are given by the probability matrix:\n\n:<math>\\mathbf{B} = \\begin{pmatrix}\n  0.9 & 0.1 \\\\\n  0.2 & 0.8\n\\end{pmatrix}\n</math>\n\nWe then observe the following sequence of events: {umbrella, umbrella, no umbrella, umbrella, umbrella} which we will represent in our calculations as:\n\n:<math>\n\\mathbf{O_1} = \\begin{pmatrix}0.9 & 0.0 \\\\  0.0 & 0.2 \\end{pmatrix}~~\\mathbf{O_2} = \\begin{pmatrix}0.9 & 0.0 \\\\  0.0 & 0.2 \\end{pmatrix}~~\\mathbf{O_3} = \\begin{pmatrix}0.1 & 0.0 \\\\  0.0 & 0.8 \\end{pmatrix}~~\\mathbf{O_4} = \\begin{pmatrix}0.9 & 0.0 \\\\  0.0 & 0.2 \\end{pmatrix}~~\\mathbf{O_5} = \\begin{pmatrix}0.9 & 0.0 \\\\  0.0 & 0.2 \\end{pmatrix}\n</math>\n\nNote that <math>\\mathbf{O_3}</math> differs from the others because of the \"no umbrella\" observation.\n\nIn computing the forward probabilities we begin with:\n\n:<math>\n\\mathbf{f_{0:0}}= \\begin{pmatrix}  0.5 & 0.5 \\end{pmatrix}\n</math>\n\nwhich is our prior state vector indicating that we don't know which state the weather is in before our observations.  While a state vector should be given as a row vector, we will use the transpose of the matrix so that the calculations below are easier to read.  Our calculations are then written in the form:\n\n:<math>\n(\\mathbf{\\hat{f}_{0:t}})^T = c_t^{-1}\\mathbf{O_t}(\\mathbf{T})^T(\\mathbf{\\hat{f}_{0:t-1}})^T\n</math>\n\ninstead of:\n\n:<math>\n\\mathbf{\\hat{f}_{0:t}} = c_t^{-1}\\mathbf{\\hat{f}_{0:t-1}} \\mathbf{T} \\mathbf{O_t}\n</math>\n\nNotice that the transformation matrix is also transposed, but in our example the transpose is equal to the original matrix.  Performing these calculations and normalizing the results provides:\n\n:<math>\n(\\mathbf{\\hat{f}_{0:1}})^T =\nc_1^{-1}\\begin{pmatrix}0.9 & 0.0 \\\\  0.0 & 0.2 \\end{pmatrix}\\begin{pmatrix}  0.7 & 0.3 \\\\  0.3 & 0.7 \\end{pmatrix}\\begin{pmatrix}0.5000 \\\\ 0.5000 \\end{pmatrix}=\nc_1^{-1}\\begin{pmatrix}0.4500 \\\\ 0.1000\\end{pmatrix}=\n\\begin{pmatrix}0.8182 \\\\ 0.1818 \\end{pmatrix}\n</math>\n\n:<math>\n(\\mathbf{\\hat{f}_{0:2}})^T =\nc_2^{-1}\\begin{pmatrix}0.9 & 0.0 \\\\  0.0 & 0.2 \\end{pmatrix}\\begin{pmatrix}  0.7 & 0.3 \\\\  0.3 & 0.7 \\end{pmatrix}\\begin{pmatrix}0.8182 \\\\ 0.1818 \\end{pmatrix}=\nc_2^{-1}\\begin{pmatrix}0.5645 \\\\ 0.0745\\end{pmatrix}=\n\\begin{pmatrix}0.8834 \\\\ 0.1166 \\end{pmatrix}\n</math>\n\n:<math>\n(\\mathbf{\\hat{f}_{0:3}})^T =\nc_3^{-1}\\begin{pmatrix}0.1 & 0.0 \\\\  0.0 & 0.8 \\end{pmatrix}\\begin{pmatrix}  0.7 & 0.3 \\\\  0.3 & 0.7 \\end{pmatrix}\\begin{pmatrix}0.8834 \\\\ 0.1166 \\end{pmatrix}=\nc_3^{-1}\\begin{pmatrix}0.0653 \\\\ 0.2772\\end{pmatrix}=\n\\begin{pmatrix}0.1907 \\\\ 0.8093 \\end{pmatrix}\n</math>\n\n:<math>\n(\\mathbf{\\hat{f}_{0:4}})^T =\nc_4^{-1}\\begin{pmatrix}0.9 & 0.0 \\\\  0.0 & 0.2 \\end{pmatrix}\\begin{pmatrix}  0.7 & 0.3 \\\\  0.3 & 0.7 \\end{pmatrix}\\begin{pmatrix}0.1907 \\\\ 0.8093 \\end{pmatrix}=\nc_4^{-1}\\begin{pmatrix}0.3386 \\\\ 0.1247\\end{pmatrix}=\n\\begin{pmatrix}0.7308 \\\\ 0.2692 \\end{pmatrix}\n</math>\n\n:<math>\n(\\mathbf{\\hat{f}_{0:5}})^T =\nc_5^{-1}\\begin{pmatrix}0.9 & 0.0 \\\\  0.0 & 0.2 \\end{pmatrix}\\begin{pmatrix}  0.7 & 0.3 \\\\  0.3 & 0.7 \\end{pmatrix}\\begin{pmatrix}0.7308 \\\\ 0.2692 \\end{pmatrix}=\nc_5^{-1}\\begin{pmatrix}0.5331 \\\\ 0.0815\\end{pmatrix}=\n\\begin{pmatrix}0.8673 \\\\ 0.1327 \\end{pmatrix}\n</math>\n\nFor the backward probabilities we start with:\n\n:<math>\n\\mathbf{b_{5:5}} = \\begin{pmatrix}  1.0 \\\\ 1.0\\end{pmatrix}\n</math>\n\nWe are then able to compute (using the observations in reverse order and normalizing with different constants):\n\n:<math>\n\\mathbf{\\hat{b}_{4:5}}  = \\alpha\\begin{pmatrix}  0.7 & 0.3 \\\\  0.3 & 0.7 \\end{pmatrix}\\begin{pmatrix}0.9 & 0.0 \\\\  0.0 & 0.2 \\end{pmatrix}\\begin{pmatrix}1.0000 \\\\ 1.0000 \\end{pmatrix}=\\alpha\\begin{pmatrix}0.6900 \\\\ 0.4100\\end{pmatrix}=\\begin{pmatrix}0.6273 \\\\ 0.3727 \\end{pmatrix}\n</math>\n\n:<math>\n\\mathbf{\\hat{b}_{3:5}}  = \\alpha\\begin{pmatrix}  0.7 & 0.3 \\\\  0.3 & 0.7 \\end{pmatrix}\\begin{pmatrix}0.9 & 0.0 \\\\  0.0 & 0.2 \\end{pmatrix}\\begin{pmatrix}0.6273 \\\\ 0.3727 \\end{pmatrix}=\\alpha\\begin{pmatrix}0.4175 \\\\ 0.2215\\end{pmatrix}=\\begin{pmatrix}0.6533 \\\\ 0.3467 \\end{pmatrix}\n</math>\n\n:<math>\n\\mathbf{\\hat{b}_{2:5}}  = \\alpha\\begin{pmatrix}  0.7 & 0.3 \\\\  0.3 & 0.7 \\end{pmatrix}\\begin{pmatrix}0.1 & 0.0 \\\\  0.0 & 0.8 \\end{pmatrix}\\begin{pmatrix}0.6533 \\\\ 0.3467 \\end{pmatrix}=\\alpha\\begin{pmatrix}0.1289 \\\\ 0.2138\\end{pmatrix}=\\begin{pmatrix}0.3763 \\\\ 0.6237 \\end{pmatrix}\n</math>\n\n:<math>\n\\mathbf{\\hat{b}_{1:5}}  = \\alpha\\begin{pmatrix}  0.7 & 0.3 \\\\  0.3 & 0.7 \\end{pmatrix}\\begin{pmatrix}0.9 & 0.0 \\\\  0.0 & 0.2 \\end{pmatrix}\\begin{pmatrix}0.3763 \\\\ 0.6237 \\end{pmatrix}=\\alpha\\begin{pmatrix}0.2745 \\\\ 0.1889\\end{pmatrix}=\\begin{pmatrix}0.5923 \\\\ 0.4077 \\end{pmatrix}\n</math>\n\n:<math>\n\\mathbf{\\hat{b}_{0:5}}  = \\alpha\\begin{pmatrix}  0.7 & 0.3 \\\\  0.3 & 0.7 \\end{pmatrix}\\begin{pmatrix}0.9 & 0.0 \\\\  0.0 & 0.2 \\end{pmatrix}\\begin{pmatrix}0.5923 \\\\ 0.4077 \\end{pmatrix}=\\alpha\\begin{pmatrix}0.3976 \\\\ 0.2170\\end{pmatrix}=\\begin{pmatrix}0.6469 \\\\ 0.3531 \\end{pmatrix}\n</math>\n\nFinally, we will compute the smoothed probability values.  These result also must be scaled so that its entries sum to 1 because we did not scale the backward probabilities with the <math>c_t</math>'s found earlier.  The backward probability vectors above thus actually represent the likelihood of each state at time t given the future observations.  Because these vectors are proportional to the actual backward probabilities, the result has to be scaled an additional time.\n\n:<math>\n(\\mathbf{\\gamma_0})^T  = \\alpha\\begin{pmatrix}0.5000 \\\\ 0.5000 \\end{pmatrix}\\circ \\begin{pmatrix}0.6469 \\\\ 0.3531 \\end{pmatrix}=\\alpha\\begin{pmatrix}0.3235 \\\\ 0.1765\\end{pmatrix}=\\begin{pmatrix}0.6469 \\\\ 0.3531 \\end{pmatrix}\n</math>\n\n:<math>\n(\\mathbf{\\gamma_1})^T  = \\alpha\\begin{pmatrix}0.8182 \\\\ 0.1818 \\end{pmatrix}\\circ \\begin{pmatrix}0.5923 \\\\ 0.4077 \\end{pmatrix}=\\alpha\\begin{pmatrix}0.4846 \\\\ 0.0741\\end{pmatrix}=\\begin{pmatrix}0.8673 \\\\ 0.1327 \\end{pmatrix}\n</math>\n\n:<math>\n(\\mathbf{\\gamma_2})^T  = \\alpha\\begin{pmatrix}0.8834 \\\\ 0.1166 \\end{pmatrix}\\circ \\begin{pmatrix}0.3763 \\\\ 0.6237 \\end{pmatrix}=\\alpha\\begin{pmatrix}0.3324 \\\\ 0.0728\\end{pmatrix}=\\begin{pmatrix}0.8204 \\\\ 0.1796 \\end{pmatrix}\n</math>\n\n:<math>\n(\\mathbf{\\gamma_3})^T  = \\alpha\\begin{pmatrix}0.1907 \\\\ 0.8093 \\end{pmatrix}\\circ \\begin{pmatrix}0.6533 \\\\ 0.3467 \\end{pmatrix}=\\alpha\\begin{pmatrix}0.1246 \\\\ 0.2806\\end{pmatrix}=\\begin{pmatrix}0.3075 \\\\ 0.6925 \\end{pmatrix}\n</math>\n\n:<math>\n(\\mathbf{\\gamma_4})^T  = \\alpha\\begin{pmatrix}0.7308 \\\\ 0.2692 \\end{pmatrix}\\circ \\begin{pmatrix}0.6273 \\\\ 0.3727 \\end{pmatrix}=\\alpha\\begin{pmatrix}0.4584 \\\\ 0.1003\\end{pmatrix}=\\begin{pmatrix}0.8204 \\\\ 0.1796 \\end{pmatrix}\n</math>\n\n:<math>\n(\\mathbf{\\gamma_5})^T  = \\alpha\\begin{pmatrix}0.8673 \\\\ 0.1327 \\end{pmatrix}\\circ \\begin{pmatrix}1.0000 \\\\ 1.0000 \\end{pmatrix}=\\alpha\\begin{pmatrix}0.8673 \\\\ 0.1327 \\end{pmatrix}=\\begin{pmatrix}0.8673 \\\\ 0.1327 \\end{pmatrix}\n</math>\n\nNotice that the value of <math>\\mathbf{\\gamma_0}</math> is equal to <math>\\mathbf{\\hat{b}_{0:5}}</math> and that <math>\\mathbf{\\gamma_5}</math> is equal to <math>\\mathbf{\\hat{f}_{0:5}}</math>.  This follows naturally because both <math>\\mathbf{\\hat{f}_{0:5}}</math> and <math>\\mathbf{\\hat{b}_{0:5}}</math> begin with uniform priors over the initial and final state vectors (respectively) and take into account all of the observations.  However, <math>\\mathbf{\\gamma_0}</math> will only be equal to <math>\\mathbf{\\hat{b}_{0:5}}</math> when our initial state vector represents a uniform prior (i.e. all entries are equal).  When this is not the case <math>\\mathbf{\\hat{b}_{0:5}}</math> needs to be combined with the initial state vector to find the most likely initial state.  We thus find that the forward probabilities by themselves are sufficient to calculate the most likely final state.  Similarly, the backward probabilities can be combined with the initial state vector to provide the most probable initial state given the observations.  The forward and backward probabilities need only be combined to infer the most probable states between the initial and final points.\n\nThe calculations above reveal that the most probable weather state on every day except for the third one was \"rain\".  They tell us more than this, however, as they now provide a way to quantify the probabilities of each state at different times.  Perhaps most importantly, our value at <math>\\mathbf{\\gamma_5}</math> quantifies our knowledge of the state vector at the end of the observation sequence.  We can then use this to predict the probability of the various weather states tomorrow as well as the probability of observing an umbrella.\n\n==Performance ==\nThe brute-force procedure for the solution of this problem is the generation of all possible <math>N^T</math> state sequences and calculating the joint probability of each state sequence with the observed series of events.  This approach has [[time complexity]] <math> O(T \\cdot N^T) </math>, where <math>T</math> is the length of sequences and <math>N</math> is the number of symbols in the state alphabet. This is intractable for realistic problems, as the number of possible hidden node sequences typically is extremely high. However, the forward–backward algorithm has time complexity <math> O(N^2 T)\\, </math>.\n\nAn enhancement to the general forward-backward algorithm, called the [[Island algorithm]], trades smaller memory usage for longer running time, taking <math> O(N^2 T \\log T)\\, </math> time and <math> O(N \\log T)\\, </math> memory. On a computer with an unlimited number of processors, this can be reduced to <math> O(N^2 T)\\, </math> total time, while still taking only <math> O(N \\log T)\\, </math> memory.<ref>J. Binder, K. Murphy and S. Russell.  Space-Efficient Inference in Dynamic Probabilistic Networks.  Int'l, Joint Conf. on Artificial Intelligence, 1997.</ref>\n\nIn addition, algorithms have been developed to compute <math>\\mathbf{f_{0:t+1}}</math> efficiently through online smoothing such as the fixed-lag smoothing (FLS) algorithm [[#RussellNorvig10|Russell & Norvig 2010 Figure 15.6 pp. 580]].\n\n==Pseudocode==\n<pre>\nBackward(guessState, sequenceIndex):\n    if sequenceIndex is past the end of the sequence, return 1\n    if (guessState, sequenceIndex) has been seen before, return saved result\n    result = 0\n    for each neighboring state n:\n        result = result + (transition probability from guessState to \n                           n given observation element at sequenceIndex)\n                        * Backward(n, sequenceIndex+1)\n    save result for (guessState, sequenceIndex)\n    return result\n</pre>\n\n==Python example==\nGiven HMM (just like in [[Viterbi algorithm]]) represented in the [[Python programming language]]:\n<source lang=\"python\">\nstates = ('Healthy', 'Fever')\nend_state = 'E'\n \nobservations = ('normal', 'cold', 'dizzy')\n \nstart_probability = {'Healthy': 0.6, 'Fever': 0.4}\n \ntransition_probability = {\n   'Healthy' : {'Healthy': 0.69, 'Fever': 0.3, 'E': 0.01},\n   'Fever' : {'Healthy': 0.4, 'Fever': 0.59, 'E': 0.01},\n   }\n \nemission_probability = {\n   'Healthy' : {'normal': 0.5, 'cold': 0.4, 'dizzy': 0.1},\n   'Fever' : {'normal': 0.1, 'cold': 0.3, 'dizzy': 0.6},\n   }\n</source>\n\nWe can write the implementation of the forward-backward algorithm like this:\n<source lang=\"python\">\ndef fwd_bkw(observations, states, start_prob, trans_prob, emm_prob, end_st):\n    # forward part of the algorithm\n    fwd = []\n    f_prev = {}\n    for i, observation_i in enumerate(observations):\n        f_curr = {}\n        for st in states:\n            if i == 0:\n                # base case for the forward part\n                prev_f_sum = start_prob[st]\n            else:\n                prev_f_sum = sum(f_prev[k]*trans_prob[k][st] for k in states)\n\n            f_curr[st] = emm_prob[st][observation_i] * prev_f_sum\n\n        fwd.append(f_curr)\n        f_prev = f_curr\n\n    p_fwd = sum(f_curr[k] * trans_prob[k][end_st] for k in states)\n\n    # backward part of the algorithm\n    bkw = []\n    b_prev = {}\n    for i, observation_i_plus in enumerate(reversed(observations[1:]+(None,))):\n        b_curr = {}\n        for st in states:\n            if i == 0:\n                # base case for backward part\n                b_curr[st] = trans_prob[st][end_st]\n            else:\n                b_curr[st] = sum(trans_prob[st][l] * emm_prob[l][observation_i_plus] * b_prev[l] for l in states)\n\n        bkw.insert(0,b_curr)\n        b_prev = b_curr\n\n    p_bkw = sum(start_prob[l] * emm_prob[l][observations[0]] * b_curr[l] for l in states)\n\n    # merging the two parts\n    posterior = []\n    for i in range(len(observations)):\n        posterior.append({st: fwd[i][st] * bkw[i][st] / p_fwd for st in states})\n\n    assert p_fwd == p_bkw\n    return fwd, bkw, posterior\n\n</source>\n\nThe function <code>fwd_bkw</code> takes the following arguments: \n<code>x</code> is the sequence of observations, e.g. <code>['normal', 'cold', 'dizzy']</code>; \n<code>states</code> is the set of hidden states; \n<code>a_0</code> is the start probability; \n<code>a</code> are the transition probabilities; \nand <code>e</code> are the emission probabilities.\n\nFor simplicity of code, we assume that the observation sequence <code>x</code> is non-empty and that  <code>a[i][j]</code> and <code>e[i][j]</code> is defined for all states i,j.\n\nIn the running example, the forward-backward algorithm is used as follows:\n<source lang=\"python\">\ndef example():\n    return fwd_bkw(observations,\n                   states,\n                   start_probability,\n                   transition_probability,\n                   emission_probability,\n                   end_state)\n</source>\n<source lang=\"python\">\n\n>>> for line in example():\n...     print(*line)\n... \n{'Healthy': 0.3, 'Fever': 0.04000000000000001} {'Healthy': 0.0892, 'Fever': 0.03408} {'Healthy': 0.007518, 'Fever': 0.028120319999999997}\n{'Healthy': 0.0010418399999999998, 'Fever': 0.00109578} {'Healthy': 0.00249, 'Fever': 0.00394} {'Healthy': 0.01, 'Fever': 0.01}\n{'Healthy': 0.8770110375573259, 'Fever': 0.1229889624426741} {'Healthy': 0.623228030950954, 'Fever': 0.3767719690490461} {'Healthy': 0.2109527048413057, 'Fever': 0.7890472951586943}\n</source>\n\n==See also ==\n* [[Baum-Welch algorithm]]\n* [[Viterbi algorithm]]\n* [[BCJR algorithm]]\n\n== References==\n{{reflist}}\n*[[Lawrence Rabiner|Lawrence R. Rabiner]], A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. ''Proceedings of the [[IEEE]]'', 77 (2), p.&nbsp;257–286, February 1989. [https://dx.doi.org/10.1109/5.18626 10.1109/5.18626]\n*{{cite journal |author=Lawrence R. Rabiner, B. H. Juang|title=An introduction to hidden Markov models|journal=IEEE ASSP Magazine |date=January 1986 |pages=4–15}}\n*{{cite book | author = Eugene Charniak|title = Statistical Language Learning|publisher = MIT Press| location=Cambridge, Massachusetts|year = 1993|isbn=978-0-262-53141-2}}\n<cite id = RussellNorvig10>\n*{{cite book | author = Stuart Russell and Peter Norvig|title = Artificial Intelligence A Modern Approach 3rd Edition|publisher = Pearson Education/Prentice-Hall|location = Upper Saddle River, New Jersey|year = 2010|isbn=978-0-13-604259-4}}\n\n==External links ==\n* [http://www.cs.jhu.edu/~jason/papers/#eisner-2002-tnlp An interactive spreadsheet for teaching the forward–backward algorithm] (spreadsheet and article with step-by-step walk-through)\n* [http://www.cs.brown.edu/research/ai/dynamics/tutorial/Documents/HiddenMarkovModels.html Tutorial of hidden Markov models including the forward–backward algorithm]\n* [http://code.google.com/p/aima-java/ Collection of AI algorithms implemented in Java] (including HMM and the forward–backward algorithm)\n\n{{DEFAULTSORT:Forward-backward algorithm}}\n[[Category:Dynamic programming]]\n[[Category:Error detection and correction]]\n[[Category:Machine learning algorithms]]\n[[Category:Markov models]]"
    },
    {
      "title": "Hamilton–Jacobi–Bellman equation",
      "url": "https://en.wikipedia.org/wiki/Hamilton%E2%80%93Jacobi%E2%80%93Bellman_equation",
      "text": "{{more footnotes|date=October 2010}}\nThe '''Hamilton–Jacobi–Bellman''' ('''HJB''') '''equation''' is a [[partial differential equation]] which is central to [[optimal control]] theory.<ref>{{cite book |first=Donald E. |last=Kirk |title=Optimal Control Theory: An Introduction |location=Englewood Cliffs, NJ |publisher=Prentice-Hall |year=1970 |isbn=0-13-638098-0 |page=86–90 |url=https://books.google.com/books?id=fCh2SAtWIdwC&pg=PA86 }}</ref> The solution of the HJB equation is the [[value function]] which gives the minimum cost for a given [[dynamical system]] with an associated cost function.\n\nWhen solved locally, the HJB is a necessary condition, but when solved over the whole of state space, the HJB equation is a [[necessary and sufficient condition]] for an optimum. The solution is open loop, but it also permits the solution of the closed loop problem. The HJB method can be generalized to [[stochastic]] systems as well.<ref>{{cite book |first=Fwu-Ranq |last=Chang |title=Stochastic Optimization in Continuous Time |location=Cambridge, UK |publisher=Cambridge University Press |year=2004 |isbn=0-521-83406-6 |pages=114–121 |url=https://books.google.com/books?id=PmIefn9u67AC&pg=PA114 }}</ref>\n\nClassical variational problems, for example the [[brachistochrone problem]], can be solved using this method.<ref>{{cite book |last=Kemajou-Brown |first=Isabelle |title=Probability on Algebraic and Geometric Structures |series=Contemporary Mathematics |volume=668 |editor-first=Gregory |editor-last=Budzban |editor2-first=Harry Randolph |editor2-last=Hughes |editor3-first=Henri |editor3-last=Schurz |year=2016 |chapter=Brief History of Optimal Control Theory and Some Recent Developments |pages=119–130 |doi=10.1090/conm/668/13400 }}</ref>\n\nThe equation is a result of the theory of [[dynamic programming]] which was pioneered in the 1950s by [[Richard Bellman]] and coworkers.<ref>{{cite book |first=R. E. |last=Bellman |title=Dynamic Programming |location=Princeton, NJ |publisher= |year=1957 |isbn= }}</ref> The corresponding discrete-time equation is usually referred to as the [[Bellman equation]]. In continuous time, the result can be seen as an extension of earlier work in [[classical physics]] on the [[Hamilton–Jacobi equation]] by [[William Rowan Hamilton]] and [[Carl Gustav Jacob Jacobi]].\n\n==Optimal control problems==\n\nConsider the following problem in deterministic optimal control over the time period <math>[0,T]</math>:\n\n:<math>V(x(0), 0) = \\min_u \\left\\{ \\int_0^T C[x(t),u(t)]\\,dt + D[x(T)] \\right\\}</math>\n\nwhere C[ ] is the scalar cost rate function and ''D''[ ] is a function that gives the economic value or utility at the final state, ''x''(''t'') is the system state vector, ''x''(0) is assumed given, and ''u''(''t'') for 0&nbsp;≤&nbsp;''t''&nbsp;≤&nbsp;''T'' is the control vector that we are trying to find.\n\nThe system must also be subject to\n\n:<math> \\dot{x}(t)=F[x(t),u(t)] \\, </math>\n\nwhere ''F''[ ] gives the vector determining physical evolution of the state vector over time.\n\n==The partial differential equation==\n\nFor this simple system, the Hamilton–Jacobi–Bellman partial differential equation is\n\n:<math>\n\\dot{V}(x,t) + \\min_u \\left\\{  \\nabla V(x,t) \\cdot F(x, u) + C(x,u) \\right\\} = 0\n</math>\n\nsubject to the terminal condition\n\n:<math>\nV(x,T) = D(x),\\,\n</math>\nwhere <math>\\dot{V}(x,t)</math> means the partial derivative of <math>V</math> wrt. the time variable <math>t</math>. <math>a \\cdot b</math> means the [[dot product]] of the vectors a and b and <math>\\nabla V(x,t) </math> the [[gradient]] of <math>V</math> wrt. the variables <math>x</math>.\n\nThe unknown scalar <math>V(x, t)</math> in the above partial differential equation is the Bellman [[value function]], which represents the cost incurred from starting in state <math>x</math> at time <math>t</math> and controlling the system optimally from then until time <math>T</math>.\n\n==Deriving the equation==\n\nIntuitively HJB can be derived as follows. If <math>V(x(t), t)</math> is the optimal cost-to-go function (also called the 'value function'), then by Richard Bellman's [[principle of optimality]], going from time ''t'' to ''t''&nbsp;+&nbsp;''dt'', we have\n\n:<math> V(x(t), t) = \\min_u \\left\\{V(x(t+dt), t+dt) + \\int_t^{t + dt} C(x(t), u(t)) \\, dt\\right\\}. </math>\n\nNote that the [[Taylor expansion]] of the first term is\n\n:<math> V(x(t+dt), t+dt) = V(x(t), t) + \\dot{V}(x(t), t) \\, dt + \\nabla V(x(t), t) \\cdot \\dot{x}(t) \\, dt + \\mathcal{o}(dt),</math>\n\nwhere <math>\\mathcal{o}(dt)</math> denotes the terms in the Taylor expansion of higher order than one in [[Little-o notation|little-''o'' notation]]. Then if we subtract <math>V(x(t), t)</math> from both sides, divide by ''dt'', and take the limit as ''dt'' approaches zero, we obtain the HJB equation defined above.\n\n==Solving the equation==\n\nThe HJB equation is usually [[Backward induction|solved backwards in time]], starting from <math>t = T</math> and ending at <math>t = 0</math>.\n\nWhen solved over the whole of state space and <math>V(x)</math> is continuously differentiable, the HJB equation is a [[necessary and sufficient condition]] for an optimum when the terminal state is unconstrained.<ref>{{cite book |first=Dimitri P. |last=Bertsekas |title=Dynamic Programming and Optimal Control |publisher=Athena Scientific |year=2005 |isbn= }}</ref> If we can solve for <math>V</math> then we can find from it a control <math>u</math> that achieves the minimum cost.\n\nIn general case, the HJB equation does not have a classical (smooth) solution. Several notions of generalized solutions have been developed to cover such situations, including [[viscosity solution]] ([[Pierre-Louis Lions]] and [[Michael G. Crandall|Michael Crandall]]), [[minimax solution]] ({{Interlanguage link multi|Andrei Izmailovich Subbotin|ru|3=Субботин,_Андрей_Измайлович}}), and others.\n\n==Extension to stochastic problems==\nThe idea of solving a control problem by applying Bellman's principle of optimality and then working out backwards in time an optimizing strategy can be generalized to stochastic control problems. Consider similar as above\n\n:<math> \\min_u \\mathbb E  \\left\\{ \\int_0^T C(t,X_t,u_t)\\,dt + D(X_T) \\right\\}</math>\n\nnow with <math>(X_t)_{t \\in [0,T]}\\,\\!</math> the stochastic process to optimize and <math>(u_t)_{t \\in [0,T]}\\,\\!</math> the steering. By first using Bellman and then expanding <math>V(X_t,t)</math> with [[Itô calculus|Itô's rule]], one finds the stochastic HJB equation\n\n:<math>\n\\min_u \\left\\{ \\mathcal{A} V(x,t) + C(t,x,u) \\right\\} = 0,\n</math>\n\nwhere <math>\\mathcal{A}</math> represents the stochastic differentiation operator, and subject to the terminal condition\n\n:<math>\nV(x,T) = D(x)\\,\\!.\n</math>\n\nNote that the randomness has disappeared. In this case a solution <math>V\\,\\!</math> of the latter does not necessarily solve the primal problem, it is a candidate only and a further verifying argument is required. This technique is widely used in Financial Mathematics to determine optimal investment strategies in the market (see for example [[Merton's portfolio problem]]).\n\n===Application to LQG Control===\n\nAs an example, we can look at a system with linear stochastic dynamics and quadratic cost. If the system dynamics is given by\n:<math>\ndx_t = (a x_t + b u_t) dt + \\sigma dw_t,\n</math>\nand the cost accumulates at rate <math>C(x_t,u_t) = r(t) u_t^2/2 + q(t) x_t^2/2</math>, the HJB equation is given by\n:<math>\n-\\frac{\\partial V(x,t)}{\\partial t} = \\frac{1}{2}q(t) x^2 + \\frac{\\partial V(x,t)}{\\partial x} a x - \\frac{b^2}{2 r(t)} \\left(\\frac{\\partial V(x,t)}{\\partial x}\\right)^2 + \\frac{\\sigma^2}{2} \\frac{\\partial^2 V(x,t)}{\\partial x^2}.\n</math>\nwith optimal action given by\n:<math>\nu_t = -\\frac{b}{r(t)}\\frac{\\partial V(x,t)}{\\partial x}\n</math>\nAssuming a quadratic form for the value function, we obtain the usual [[Riccati equation]] for the Hessian of the value function as is usual for [[Linear-quadratic-Gaussian control]].\n\n==See also==\n* [[Bellman equation]], discrete-time counterpart of the Hamilton–Jacobi–Bellman equation\n* [[Pontryagin's minimum principle]], necessary but not sufficient condition for optimum, by minimizing a Hamiltonian, but this has the advantage over HJB of only needing to be satisfied over the single trajectory being considered.\n\n==References==\n{{Reflist}}\n* {{cite journal |first= R. E. |last=Bellman |title=Dynamic Programming and a new formalism in the calculus of variations |journal=[[Proceedings of the National Academy of Sciences of the United States of America|Proc. Natl. Acad. Sci.]] |volume=40 |issue=4 |year=1954 |pages=231–235 |doi= 10.1073/pnas.40.4.231|pmc=527981 |pmid=16589462}}\n* {{cite book |first=R. E. |last=Bellman |title=Dynamic Programming |location=Princeton |publisher= |year=1957 |isbn= }}\n* {{cite journal |first=R. |last=Bellman |first2=S. |last2=Dreyfus |title=An Application of Dynamic Programming to the Determination of Optimal Satellite Trajectories |journal=J. Brit. Interplanet. Soc. |volume=17 |issue= |year=1959 |pages=78–83 |doi= }}\n\n==Further reading==\n* {{cite book\n |first= Dimitri P. |last=Bertsekas | authorlink = Dimitri P. Bertsekas\n | year = 2005\n | title = Dynamic programming and optimal control\n | publisher = Athena Scientific\n | isbn = \n}}\n\n{{DEFAULTSORT:Hamilton-Jacobi-Bellman equation}}\n[[Category:Partial differential equations]]\n[[Category:Optimal control]]\n[[Category:Dynamic programming]]\n[[Category:Stochastic control]]\n[[Category:William Rowan Hamilton]]"
    },
    {
      "title": "Held–Karp algorithm",
      "url": "https://en.wikipedia.org/wiki/Held%E2%80%93Karp_algorithm",
      "text": "The '''Held–Karp algorithm''', also called '''Bellman–Held–Karp algorithm''', is a [[dynamic programming]] algorithm proposed in 1962 independently by [[Richard E. Bellman|Bellman]]<ref>‘Dynamic programming treatment of the travelling salesman problem’, Richard Bellman, ''Journal of Assoc. Computing Mach.'' 9. 1962.</ref> and by Held and [[Richard M. Karp|Karp]]<ref>'A dynamic programming approach to sequencing problems’, Michael Held and Richard M. Karp, ''Journal for the Society for Industrial and Applied Mathematics'' 1:10. 1962</ref> to solve the [[Travelling salesman problem|Traveling Salesman Problem (TSP)]]. TSP is an extension of the [[Hamiltonian path problem|Hamiltonian circuit problem]]. The problem can be described as: find a tour of N cities in a country (assuming all cities to be visited are reachable), the tour should (a) visit every city just once, (b) return to the starting point and (c) be of minimum distance.<ref>http://www.cs.man.ac.uk/~david/algorithms/graphs.pdf</ref>\nBroadly, the TSP is classified as symmetric travelling salesman problem (sTSP), asymmetric travelling salesman problem (aTSP), and multi-travelling salesman problem (mTSP).The mTSP is generally treated as a relaxed [[vehicle routing problem]].\n\n==Graph model==\n'''sTSP''': Let ''V'' = {''v''<sub>1</sub> ,..., ''v''<sub>''n''</sub> } be a set of cities, ''E'' = {( ''r'', ''s'' ) : ''r'', ''s'' ∈ ''V'' } be the edge set, and ''d''<sub>''rs''</sub> = ''d''<sub>''s''r</sub> be a cost measure associated with edge ( ''r'', ''s'' ) ∈ ''E''.\n\n'''aTSP''': If ''d''<sub>''rs''</sub> ≠ ''d''<sub>''sr''</sub> for at least one ( ''r'', ''s'' ) then the sTSP becomes an aTSP.\nThe aTSP and sTSP are defined on different graphs – complete directed and undirected. sTSP can be considered, in many cases, as a subproblem of the aTSP.\n\n'''mTSP''': The mTSP is defined as: In a given set of nodes, let there be m salesmen located at a single depot node. The remaining nodes (cities) that are to be visited are intermediate nodes. Then, the mTSP consists of finding tours for all m salesmen, who all start and end at the depot, such that each intermediate node is visited exactly once and the total cost of visiting all nodes is minimized.\n\n==Algorithm==\n\n=== Description ===\nBelow is the dynamic programming procedure:\n\nThere is an optimization property for TSP:\n    ''Every subpath of a path of minimum distance is itself of minimum distance.''\n\nCompute the solutions of all subproblems starting with the smallest. \nWhenever computing a solution requires solutions for smaller problems using the above recursive equations, look up these solutions which are already computed.\nTo compute a minimum distance tour, use the final equation to generate the 1st node, and repeat for the other nodes.\nFor this problem, we cannot know which subproblems we need to solve, so we solve them all.\n\n===Recursive formulation===\n\nNumber the cities 1, 2, . . . , ''N'' and assume we start at city 1, and the distance between city ''i'' and city ''j'' is ''d''<sub>''i,j''</sub>. Consider subsets ''S'' ⊆ {2, . . . , ''N''} of cities and, for ''c'' ∈ ''S'', let ''D''(''S'', ''c'') be the minimum distance, starting at city 1, visiting all cities in ''S'' and finishing at city ''c''.\n\nFirst phase: if ''S'' = {''c''}, then ''D''(''S'', ''c'') = ''d''<sub>1,''c''</sub>. Otherwise: ''D''(''S'', ''c'') = min<sub>''x''∈''S''−''c''</sub> (''D''(''S'' − ''c'', ''x'') + ''d''<sub>''x'',''c''</sub> ).\n\nSecond phase: the minimum distance for a complete tour of all cities is\n''M'' = min<sub>c∈{2,...,''N''}</sub> (''D''({2, . . . , ''N''}, ''c'') + ''d''<sub>''c'',1</sub> )\n\nA tour ''n''<sub>1</sub> , . . ., ''n''<sub>''N''</sub> is of minimum distance just when it satisfies ''M'' = ''D''({2, . . . , ''N''}, ''n''<sub>''N''</sub> ) + ''d''<sub>''n''<sub>''N''</sub>,1</sub> .\n\n==Example<ref>http://www.mafy.lut.fi/study/DiscreteOpt/tspdp.pdf</ref>==\n\nDistance matrix:\n\n: <math>C=\n\\begin{pmatrix}\n0 & 2 & 9 & 10 \\\\\n1 & 0 & 6 & 4 \\\\\n15 & 7 & 0 & 8 \\\\\n6 & 3 & 12 & 0 \n\\end{pmatrix}</math>\n\nFunctions description:\n* ''g(x, S)'' - starting from 1, path min cost ends at vertex x, passing vertices in set S exactly once \n* ''c<sub>xy</sub>'' - edge cost ends at x from y\n* ''p(x, S)'' - the second-to-last vertex to x from set S. Used for constructing the TSP path back at the end.\n\n\n''k'' = 0, null set:\n\nSet ∅:\n        g(2, ∅) = c<sub>21</sub> = 1 \n        g(3, ∅) = c<sub>31</sub> = 15\n        g(4, ∅) = c<sub>41</sub> = 6\n\n''k'' = 1, consider sets of 1 element:\n\nSet {2}:\n        g(3,{2}) = c<sub>32</sub> + g(2, ∅ ) = c<sub>32</sub> + c<sub>21</sub> = 7 + 1 = 8       p(3,{2}) = 2\n        g(4,{2}) = c<sub>42</sub> + g(2, ∅ ) = c<sub>42</sub> + c<sub>21</sub> = 3 + 1 = 4       p(4,{2}) = 2\nSet {3}:\n        g(2,{3}) = c<sub>23</sub> + g(3, ∅ ) = c<sub>23</sub> + c<sub>31</sub> = 6 + 15 = 21     p(2,{3}) = 3\n        g(4,{3}) = c<sub>43</sub> + g(3, ∅ ) = c<sub>43</sub> + c<sub>31</sub> = 12 + 15 = 27    p(4,{3}) = 3\nSet {4}:\n        g(2,{4}) = c<sub>24</sub> + g(4, ∅ ) = c<sub>24</sub> + c<sub>41</sub> = 4 + 6 = 10      p(2,{4}) = 4\n        g(3,{4}) = c<sub>34</sub> + g(4, ∅ ) = c<sub>34</sub> + c<sub>41</sub> = 8 + 6 = 14      p(3,{4}) = 4\n\n''k'' = 2, consider sets of 2 elements:\n\nSet {2,3}:\n          g(4,{2,3}) = min {c<sub>42</sub> + g(2,{3}), c<sub>43</sub> + g(3,{2})} = min {3+21, 12+8}= min {24, 20}= 20\n          p(4,{2,3}) = 3\nSet {2,4}:\n          g(3,{2,4}) = min {c<sub>32</sub> + g(2,{4}), c<sub>34</sub> + g(4,{2})} = min {7+10, 8+4}= min {17, 12} = 12\n          p(3,{2,4}) = 4\nSet {3,4}:\n           g(2,{3,4}) = min {c<sub>23</sub> + g(3,{4}), c<sub>24</sub> + g(4,{3})} = min {6+14, 4+27}= min {20, 31}= 20\n           p(2,{3,4}) = 3\n\nLength of an optimal tour:\n\n  f = g(1,{2,3,4}) = min { c<sub>12</sub> + g(2,{3,4}), c<sub>13</sub> + g(3,{2,4}), c<sub>14</sub> + g(4,{2,3}) }\n                   = min {2 + 20, 9 + 12, 10 + 20} = min {22, 21, 30} = 21\n\nSuccessor of node 1: p(1,{2,3,4}) = 3\n\nSuccessor of node 3: p(3, {2,4}) = 4\n\nSuccessor of node 4: p(4, {2}) = 2\n\nBacktracking the optimal TSP tour reaches: 1 → 3 → 4 → 2 → 1\n\n==Pseudocode<ref>http://www.lsi.upc.edu/~mjserna/docencia/algofib/P07/dynprog.pdf</ref>==\n\n function algorithm TSP (G, n)\n   for k := 2 to n do\n     C({k}, k) := d<sub>1,k</sub>\n   end for\n \n   for s := 2 to n-1 do\n     for all S ⊆ {2, . . . , n}, |S| = s do\n       for all k ∈ S do\n         C(S, k) := min<sub>m≠k,m∈S</sub> [C(S\\{k}, m) + d<sub>m,k</sub>]\n       end for\n     end for\n   end for\n \n   opt := min<sub>k≠1</sub> [C({2, 3, . . . , n}, k) + d<sub>k,1</sub>]\n   return (opt)\n end\n\n==Complexity==\n\n===Exhaustive enumeration===\nThis brute-force method starting at any city, enumerate all possible [[permutation]]s of cities to visit, and find the distance of each permutation and choose one of minimum distance. The total number of possible routes covering all N cities can be given as (N − 1)! and (N − 1)!/2 in aTSP and sTSP respectively.<ref>Gutin, Gregory, and Abraham P. Punnen, eds. The traveling salesman problem and its variations. Vol. 12. Springer, 2002.</ref>\n[[Stirling's approximation]]:\n<math>N! \\approx \\sqrt{2 \\pi N} \\left(\\frac{N}{e}\\right)^N</math>\n\n===Dynamic programming approach===\nThis algorithm offers faster (but still exponential time) execution than exhaustive enumeration, with the disadvantage using a lot of space: the worst-case time complexity of this algorithm is <math>O(2^n n^2)</math> and the space <math>O(2^n n)</math>.\n\nTime: the fundamental operations employed in the computation are additions and comparisons. \nThe number of each in the first phase is given by\n<math>\\left(\\sum_{k=2}^{n-1}k(k-1)\\binom{n-1}{k}\\right)+(n-1)=(n-1)(n-2)2^{n-3}+(n-1)</math>\n\nand the number of occurrence of each in the second phase is  \n<math>\\sum_{k=2}^{n-1}k={n(n-1) \\over 2} -1</math>\n\nSpace: <math>\\left(\\sum_{k=2}^{n-1}k\\binom{n-1}{k}\\right)+(n-1)=(n-1)2^{n-2}</math>\n\nThe space complexity can be slightly improved by noticing that the calculation of minimum costs for subsets of size ''s'' only requires subsets of size ''s-1''. \n\nBy storing only subsets of size ''s-1'' and ''s'' at any point of the algorithm, the space complexity reduces to: <math>\\max_{2\\leq s<n} \\left( (s-1)\\binom{n-1}{s-1} + s\\binom{n-1}{s} \\right) +(n-1)= O\\left( n\\binom{n-1}{\\lfloor n/2 \\rfloor}\\right) = O(2^n\\sqrt{n})</math>\n\n==Related algorithms==\n\n===Precise algorithm for solving the TSP===\nBesides Dynamic Programming, [[Linear programming]] and Branch-bound algorithm are precise algorithms that can solve TSP. Linear programming applies to the cutting plane method in the [[integer programming]], i.e. solving the LP formed by two constraints in the model and then seeking the cutting plane by adding inequality constraint to gradually converge at an optimal solution. When people apply this method to find a cutting plane, they often depend on experience. So this method is seldom deemed as a general method.\n\nBranch-bound algorithm is a search algorithm widely used, although it's not good for solving the large-scale problem. It controls the searching process through effective restrictive boundary so that it can search for the optimal solution branch from the space state tree to find an optimal solution as soon as possible. The key point of this algorithm is the choice of the restrictive boundary. Different restrictive boundaries may form different branch-bound algorithms.\n\n===Approximate algorithm for solving the TSP===\nAs the application of precise algorithm to solve problem is very limited, we often use approximate algorithm or heuristic algorithm. The result of the algorithm can be assessed by C / C* ≤ ε . C is the total travelling distance generated from approximate algorithm; C* is the optimal travelling distance; ε is the upper limit for the ratio of the total travelling distance of approximate solution to optimal solution under the worst condition. The value of ε >1.0. The more it closes to 1.0, the better the algorithm is. These algorithms include: Interpolation algorithm, [[Nearest neighbour algorithm]], Clark & Wright algorithm, Double spanning tree algorithm, [[Christofides algorithm]], Hybrid algorithm, Probabilistic algorithm.\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Held-Karp algorithm}}\n[[Category:Dynamic programming]]"
    },
    {
      "title": "Hirschberg's algorithm",
      "url": "https://en.wikipedia.org/wiki/Hirschberg%27s_algorithm",
      "text": "In [[computer science]], '''Hirschberg's algorithm''', named after its inventor, [[Dan Hirschberg]], is a [[dynamic programming]] [[algorithm]] that finds the optimal [[sequence alignment]] between two [[string (computer science)|string]]s. Optimality is measured with the [[Levenshtein distance]], defined to be the sum of the costs of insertions, replacements, deletions, and null actions needed to change one string into the other.  Hirschberg's algorithm is simply described as a more space efficient version of the [[Needleman&ndash;Wunsch algorithm]] that uses [[divide and conquer algorithm|divide and conquer]].<ref>[http://www.csse.monash.edu.au/~lloyd/tildeAlgDS/Dynamic/Hirsch/ Hirschberg's algorithm<!-- Bot generated title -->]</ref>  Hirschberg's algorithm is commonly used in [[computational biology]] to find maximal global alignments of [[DNA]] and [[protein]] sequences.\n\n==Algorithm information==\nHirschberg's algorithm is a generally applicable algorithm for optimal sequence alignment. [[BLAST]] and [[FASTA]] are suboptimal [[Heuristic (computer science)|heuristics]].  If ''x'' and ''y'' are strings, where length(''x'') = ''n'' and length(''y'') = ''m'', the [[Needleman-Wunsch algorithm]] finds an optimal alignment in [[Big O Notation|O]](''nm'') time, using O(''nm'') space.  Hirschberg's algorithm is a clever modification of the Needleman-Wunsch Algorithm which still takes O(''nm'') time, but needs only O(min{''n'',''m''}) space and is much faster in practice.<ref>http://www.cs.tau.ac.il/~rshamir/algmb/98/scribe/html/lec02/node10.html</ref>\nOne application of the algorithm is finding sequence alignments of DNA or protein sequences.  It is also a space-efficient way to calculate the [[longest common subsequence problem|longest common subsequence]] between two sets of data such as with the common [[diff]] tool.\n\nThe Hirschberg algorithm can be derived from the Needleman-Wunsch algorithm by observing that:<ref>{{cite journal|last=Hirschberg|first=D. S.|authorlink=Dan Hirschberg|title=A linear space algorithm for computing maximal common subsequences|journal=[[Communications of the ACM]]|volume=18|issue=6|year=1975|pages=341–343|doi=10.1145/360825.360861|mr=0375829|citeseerx=10.1.1.348.4774}}</ref>\n# one can compute the optimal alignment score by only storing the current and previous row of the Needleman-Wunsch score matrix;\n# if <math>(Z,W) = \\operatorname{NW}(X,Y)</math> is the optimal alignment of <math>(X,Y)</math>, and <math>X = X^l + X^r</math> is an arbitrary partition of <math>X</math>, there exists a partition <math>Y^l + Y^r</math> of <math>Y</math> such that <math>\\operatorname{NW}(X,Y) = \\operatorname{NW}(X^l,Y^l) + \\operatorname{NW}(X^r,Y^r)</math>.\n\n== Algorithm description ==\n\n<math>X_i</math> denotes the i-th character of <math>X</math>, where <math>1 < i \\leqslant \\operatorname{length}(X)</math>. <math>X_{i:j}</math> denotes a substring of size <math>j-i+1</math>, ranging from i-th to the j-th character of <math>X</math>. <math>\\operatorname{rev}(X)</math> is the reversed version of <math>X</math>.\n\n<math>X</math> and <math>Y</math> are sequences to be aligned. Let <math>x</math> be a character from <math>X</math>, and <math>y</math> be a character from <math>Y</math>. We assume that <math>\\operatorname{Del}(x)</math>, <math>\\operatorname{Ins}(y)</math> and <math>\\operatorname{Sub}(x,y)</math> are well defined integer-valued functions. These functions represent the cost of deleting <math>x</math>, inserting <math>y</math>, and replacing <math>x</math> with <math>y</math>, respectively.\n\nWe define <math>\\operatorname{NWScore}(X,Y)</math>, which returns the last line of the Needleman-Wunsch score matrix <math>\\mathrm{Score}(i,j)</math>:\n\n  '''function''' NWScore(X,Y)\n    Score(0,0) = 0 // 2*length(Y) array\n    '''for''' j=1 '''to''' length(Y)\n      Score(0,j) = Score(0,j-1) + Ins(Y<sub>j</sub>)\n    '''for''' i=1 '''to''' length(X) //init array\n      Score(1,0) = Score(0,0) + Del(X<sub>i</sub>)\n      '''for''' j=1 '''to''' length(Y)\n        scoreSub = Score(0,j-1) + Sub(X<sub>i</sub>, Y<sub>j</sub>)\n        scoreDel = Score(0,j) + Del(X<sub>i</sub>)\n        scoreIns = Score(1,j-1) + Ins(Y<sub>j</sub>)\n        Score(1,j) = max(scoreSub, scoreDel, scoreIns)\n      '''end'''\n      //copy Score[1] to Score[0]\n      Score(0,:) = Score(1,:)\n    '''end'''\n    '''for''' j=0 '''to''' length(Y)\n      LastLine(j) = Score(1,j)\n    '''return''' LastLine\n\nNote that at any point, <math>\\operatorname{NWScore}</math> only requires the two most recent rows of the score matrix. Thus, <math>\\operatorname{NWScore}</math> is implemented in <math>O(\\operatorname{min}\\{\\operatorname{length}(X),\\operatorname{length}(Y)\\})</math> space\n\nThe Hirschberg algorithm follows:\n\n  '''function''' Hirschberg(X,Y)\n    Z = \"\"\n    W = \"\"\n    '''if''' length(X) == 0\n      '''for''' i=1 '''to''' length(Y)\n        Z = Z + '-'\n        W = W + Y<sub>i</sub>\n      '''end'''\n    '''else if''' length(Y) == 0\n      '''for''' i=1 '''to''' length(X)\n        Z = Z + X<sub>i</sub>\n        W = W + '-'\n      '''end'''\n    '''else if''' length(X) == 1 '''or''' length(Y) == 1\n      (Z,W) = NeedlemanWunsch(X,Y)\n    '''else'''\n      xlen = length(X)\n      xmid = length(X)/2\n      ylen = length(Y)\n  \n      ScoreL = NWScore(X<sub>1:xmid</sub>, Y)\n      ScoreR = NWScore(rev(X<sub>xmid+1:xlen</sub>), rev(Y))\n      ymid = [[arg max]] ScoreL + rev(ScoreR)\n  \n      (Z,W) = Hirschberg(X<sub>1:xmid</sub>, y<sub>1:ymid</sub>) + Hirschberg(X<sub>xmid+1:xlen</sub>, Y<sub>ymid+1:ylen</sub>)\n    '''end'''\n    '''return''' (Z,W)\n\nIn the context of Observation (2), assume that <math>X^l + X^r</math> is a partition of <math>X</math>. Index <math>\\mathrm{ymid}</math> is computed such that <math>Y^l = Y_{1:\\mathrm{ymid}}</math> and <math>Y^r = Y_{\\mathrm{ymid}+1:\\operatorname{length}(Y)}</math>.\n\n== Example ==\n\nLet\n\n<math>\n  \\begin{align}\n    X &= \\mathrm{AGTACGCA},\\\\\n    Y &= \\mathrm{TATGC},\\\\\n    \\operatorname{Del}(x) &= -2,\\\\\n    \\operatorname{Ins}(y) &= -2,\\\\\n    \\operatorname{Sub}(x,y) &= \\begin{cases} +2, & \\mbox{if } x = y \\\\ -1, & \\mbox{if } x \\neq y.\\end{cases}\n  \\end{align}\n</math>\n\nThe optimal alignment is given by\n\n  W = AGTACGCA\n  Z = --TATGC-\n\nIndeed, this can be verified by backtracking its corresponding Needleman-Wunsch matrix:\n\n          '''T   A   T   G   C'''\n      '''0'''  -2  -4  -6  -8 -10\n  '''A'''  '''-2'''  -1   0  -2  -4  -6\n  '''G'''  '''-4'''  -3  -2  -1   0  -2\n  '''T'''  -6  '''-2'''  -4   0  -2  -1\n  '''A'''  -8  -4   '''0'''  -2  -1  -3\n  '''C''' -10  -6  -2  '''-1'''  -3   1\n  '''G''' -12  -8  -4  -3   '''1'''  -1\n  '''C''' -14 -10  -6  -5  -1   '''3'''\n  '''A''' -16 -12  -8  -7  -3   '''1'''\n\nOne starts with the top level call to <math>\\operatorname{Hirschberg}(\\mathrm{AGTACGCA}, \\mathrm{TATGC})</math>, which splits the first argument in half: <math>X = \\mathrm{AGTA} + \\mathrm{CGCA}</math>. The call to <math>\\operatorname{NWScore}(\\mathrm{AGTA},Y)</math> produces the following matrix:\n\n         '''T   A   T   G   C'''\n     0  -2  -4  -6  -8 -10\n  '''A''' -2  -1   0  -2  -4  -6\n  '''G''' -4  -3  -2  -1   0  -2\n  '''T''' -6  -2  -4   0  -2  -1\n  '''A''' -8  -4   0  -2  -1  -3\n\nLikewise, <math>\\operatorname{NWScore}(\\operatorname{rev}(\\mathrm{CGCA}), \\operatorname{rev}(Y))</math> generates the following matrix:\n\n        '''C   G   T   A   T'''\n     0 -2  -4  -6  -8 -10\n  '''A''' -2 -1  -3  -5  -4  -6\n  '''C''' -4  0  -2  -4  -6  -5\n  '''G''' -6 -2   2   0  -2  -4\n  '''C''' -8 -4   0   1  -1  -3\n\nTheir last lines (after reversing the latter) and sum of those are respectively\n\n  ScoreL      = [ -8 -4  0  1 -1 -3 ]\n  rev(ScoreR) = [ -3 -1  1  0 -4 -8 ]\n  Sum         = [-11 -5  '''1'''  1 -5 -11]\n\nThe maximum (shown in bold) appears at <tt>ymid = 2</tt>, producing the partition <math>Y = \\mathrm{TA} + \\mathrm{TGC}</math>.\n\nThe entire Hirschberg recursion (which we omit for brevity) produces the following tree:\n\n                (AGTACGCA,TATGC)\n                /               \\\n         (AGTA,TA)             (CGCA,TGC)\n          /     \\              /        \\\n      (AG, )   (TA,TA)      (CG,TG)     (CA,C)\n               /   \\        /   \\       \n            (T,T) (A,A)  (C,T) (G,G) \n\nThe leaves of the tree contain the optimal alignment.\n\n==See also==\n* [[Longest common subsequence problem|Longest Common Subsequence]]\n\n==References==\n{{reflist}}\n\n{{Strings}}\n\n{{DEFAULTSORT:Hirschberg's Algorithm}}\n[[Category:Sequence alignment algorithms]]\n[[Category:Bioinformatics algorithms]]\n[[Category:Articles with example pseudocode]]\n[[Category:Dynamic programming]]"
    },
    {
      "title": "Hunt–McIlroy algorithm",
      "url": "https://en.wikipedia.org/wiki/Hunt%E2%80%93McIlroy_algorithm",
      "text": "In [[computer science]], the '''Hunt–McIlroy algorithm''' is a solution to the [[longest common subsequence problem]].  It was one of the first non-heuristic algorithms used in [[diff]].  To this day, variations of this algorithm are found in incremental [[version control system]]s, [[wiki software|wiki engine]]s, and [[molecular phylogenetics]] research software.\n\nThe research accompanying the final version of [[Unix]] <tt>[[diff]]</tt>, written by [[Douglas McIlroy]], was published in the 1976 paper \"An Algorithm for Differential File Comparison\", co-written with [[James W. Hunt]], who developed an initial prototype of diff.<ref>{{cite journal |first1=James W. |last1=Hunt |first2=M. Douglas |last2=McIlroy |author2-link=Douglas McIlroy |title=An Algorithm for Differential File Comparison |volume=41 |journal=Computing Science Technical Report |publisher=Bell Laboratories |date=June 1976|pages= |url=http://www.cs.dartmouth.edu/~doug/diff.pdf}}</ref>\n\n==Algorithm==\nThe Hunt–McIlroy algorithm is a modification to a basic solution for the longest common subsequence problem. The solution is modified so that there are lower time and space requirements for the algorithm when it is working with typical inputs.\n\n===Basic Longest Common Subsequence Solution===\n\n====Algorithm====\nLet ''A<sub>i</sub>'' be the ''i''th line of the first file.\n\nLet ''B<sub>j</sub>'' be the ''j''th line of the second file.\n\nLet ''P<sub>ij</sub>'' be the length of the longest common subsequence for the first ''i'' lines of the first file and the first ''j'' lines of the second file.\n\n:<math>\nP_{ij} =\n\\begin{cases}\n  0\n& \\text{ if }\\ i = 0 \\text{ or }  j = 0 \\\\\n  1 + P_{i-1, j-1}\n& \\text{ if } A_i = B_j \\\\\n  \\max(P_{i-1, j}, P_{i, j-1})\n& \\text{ if } A_i \\ne B_j \\\\\n\\end{cases}\n</math>\n\n====Example====\n[[File:Longest Common Subsequence Recursion.png|thumb|A table showing the recursive steps the basic longest common subsequence algorithm takes.]]\nConsider the files A and B.\n\nA contains three lines:\n* A<sub>1</sub> = a\n* A<sub>2</sub> = b\n* A<sub>3</sub> = c\n\nB contains three lines:\n* B<sub>1</sub> = a\n* B<sub>2</sub> = c\n* B<sub>3</sub> = b\n\nThe steps the above algorithm would perform to determine the length of the longest common subsequence for both files are shown in the diagram. The algorithm correctly reports that the longest common subsequence of the two files is two lines long.\n\n====Complexity====\nThe above algorithm has worst-case time and space complexities of <math>O(mn)</math> (see [[big O notation]]), where m is the number of lines in file A and n is the number of lines in file B. The Hunt–McIlroy algorithm modifies this algorithm to have a worst case time complexity of <math>O(mn \\log m)</math> and space complexity of <math>O(mn)</math>, though it regularly beats the worst-case with typical inputs.\n\n===Essential Matches===\n\n====k-candidates====\nThe Hunt–McIlroy algorithm only considers what the authors call essential matches, or k-candidates. k-candidates are pairs of indices (i, j) such that:\n* A<sub>i</sub> = B<sub>j</sub>\n* P<sub>ij</sub> > max(P<sub>i-1, j</sub>, P<sub>i, j-1</sub>)\n\nThe second point implies two properties of k-candidates:\n* There is a common subsequence of length k in the first i lines of file A and the first j lines of file B.\n* There are no common subsequences of length k for any fewer than i lines of file A or j lines of file B.\n\n====Connecting k-candidates====\n[[File:K Candidate Diagram.png|thumb|A diagram that shows how using k-candidates reduces the amount of time and space needed to find the longest common subsequence of two files.]]\nTo create the longest common subsequence from a collection of k-candidates, a grid with each file's contents on each axis is created. The k-candidates are marked on the grid. A common subsequence can be created by joining marked coordinates of the grid such that any increase in i is accompanied by an increase in j.\n\nThis is illustrated in the adjacent diagram.\n\nBlack dots represent candidates that would have to be considered by the simple algorithm and the black lines are connections that create common subsequences of length 3.\n\nRed dots represent k-candidates that are considered by the Hunt–McIlroy algorithm and the red line is the connection that creates a common subsequence of length 3.\n\n==See also==\n* [[Levenshtein distance]]\n* [[Longest common subsequence problem]]\n* [[Wagner–Fischer algorithm]]\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Hunt-McIlroy algorithm}}\n[[Category:Algorithms on strings]]\n[[Category:Combinatorics]]\n[[Category:Dynamic programming]]"
    },
    {
      "title": "Knapsack problem",
      "url": "https://en.wikipedia.org/wiki/Knapsack_problem",
      "text": "[[File:Knapsack.svg|thumb|right|250px|Example of a one-dimensional (constraint) knapsack problem: which boxes should be chosen to maximize the amount of money while still keeping the overall weight under or equal to 15&nbsp;kg? A [[List of knapsack problems#Multiple constraints|multiple constrained problem]] could consider both the weight and volume of the boxes. <br />\n(Solution: if any number of each box is available, then three yellow boxes and three grey boxes; if only the shown boxes are available, then all but the green box.)]]\n\nThe '''knapsack problem''' or '''rucksack problem''' is a problem in [[combinatorial optimization]]: Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible. It derives its name from the problem faced by someone who is constrained by a fixed-size [[knapsack]] and must fill it with the most valuable items.\n\nThe problem often arises in [[resource allocation]] where there are financial constraints and is studied in fields such as [[combinatorics]], [[computer science]], [[computational complexity theory|complexity theory]], [[cryptography]], [[applied mathematics]], and [[daily fantasy sports]].\n\nThe knapsack problem has been studied for more than a century, with early works dating as far back as 1897.<ref>{{cite journal | title = On the partition of numbers | author = Mathews, G. B. | journal = Proceedings of the London Mathematical Society | volume = 28 | pages = 486&ndash;490 | date = 25 June 1897 | url = http://plms.oxfordjournals.org/content/s1-28/1/486.full.pdf |doi = 10.1112/plms/s1-28.1.486}}</ref> The name \"knapsack problem\" dates back to the early works of mathematician [[Tobias Dantzig]] (1884&ndash;1956),<ref>Dantzig, Tobias. Numbers: The Language of Science, 1930.</ref> and refers to the commonplace problem of packing the most valuable or useful items without overloading the luggage.\n\n==Applications==\nA 1998 study of the [http://www.cs.sunysb.edu/~algorith/ Stony Brook University Algorithm Repository] showed that, out of 75 algorithmic problems, the knapsack problem was the 19th most popular and the third most needed after [[suffix tree]]s and the [[bin packing problem]].<ref>{{cite journal | title = Who is Interested in Algorithms and Why? Lessons from the Stony Brook Algorithm Repository | author = Skiena, S. S. |journal = ACM SIGACT News | volume = 30 | issue=3 |date = September 1999| pages= 65&ndash;74  |issn=0163-5700 | doi=10.1145/333623.333627| citeseerx = 10.1.1.41.8357 }}</ref>\n\nKnapsack problems appear in real-world decision-making processes in a wide variety of fields, such as finding the least wasteful way to cut raw materials,<ref>Kellerer, Pferschy, and Pisinger 2004, p. 449</ref> selection of [[investment]]s and [[Portfolio (finance)|portfolios]],<ref>Kellerer, Pferschy, and Pisinger 2004, p. 461</ref> selection of assets for [[Securitization|asset-backed securitization]],<ref>Kellerer, Pferschy, and Pisinger 2004, p. 465</ref> and generating keys for the [[Merkle–Hellman knapsack cryptosystem|Merkle–Hellman]]<ref>Kellerer, Pferschy, and Pisinger 2004, p. 472</ref> and other [[knapsack cryptosystems]].\n\nOne early application of knapsack algorithms was in the construction and scoring of tests in which the test-takers have a choice as to which questions they answer. For small examples, it is a fairly simple process to provide the test-takers with such a choice. For example, if an exam contains 12 questions each worth 10 points, the test-taker need only answer 10 questions to achieve a maximum possible score of 100 points. However, on tests with a heterogeneous distribution of point values, it is more difficult to provide choices. Feuerman and Weiss proposed a system in which students are given a heterogeneous test with a total of 125 possible points. The students are asked to answer all of the questions to the best of their abilities. Of the possible subsets of problems whose total point values add up to 100, a knapsack algorithm would determine which subset gives each student the highest possible score.<ref>{{cite journal | title = A Mathematical Programming Model for Test Construction and Scoring | journal = Management Science | volume = 19 | issue = 8 |date = April 1973|pages = 961–966 |author1=Feuerman, Martin |author2=Weiss, Harvey | jstor = 2629127 | doi=10.1287/mnsc.19.8.961}}</ref>\n\n== Definition ==\nThe most common problem being solved is the '''0-1 knapsack problem''', which restricts the number ''<math>x_i</math>'' of copies of each kind of item to zero or one. Given a set of ''<math>n</math>'' items numbered from 1 up to ''<math>n</math>'', each with a weight ''<math>w_i</math>'' and a value ''<math>v_i</math>'', along with a maximum weight capacity ''<math>W</math>'',\n: maximize <math>\\sum_{i=1}^n v_i x_i</math>\n: subject to <math>\\sum_{i=1}^n w_i x_i \\leq W</math> and <math>x_i \\in \\{0,1\\}</math>.\nHere ''<math>x_i</math>'' represents the number of instances of item ''<math>i</math>'' to include in the knapsack. Informally, the problem is to maximize the sum of the values of the items in the knapsack so that the sum of the weights is less than or equal to the knapsack's capacity.\n\nThe '''bounded knapsack problem''' ('''BKP''') removes the restriction that there is only one of each item, but restricts the number <math>x_i</math> of copies of each kind of item to a maximum non-negative integer value <math>c</math>:\n: maximize <math>\\sum_{i=1}^n v_i x_i</math>\n: subject to <math>\\sum_{i=1}^n w_i x_i \\leq W</math> and <math>0 \\leq x_i \\leq c</math>\n\nThe '''unbounded knapsack problem''' ('''UKP''') places no upper bound on the number of copies of each kind of item and can be formulated as above except for that the only restriction on <math>x_i</math> is that it is a non-negative integer.\n: maximize <math>\\sum_{i=1}^n v_i x_i</math>\n: subject to <math>\\sum_{i=1}^n w_i x_i \\leq W</math> and <math>x_i \\geq 0</math>\n\nOne example of the unbounded knapsack problem is given using the figure shown at the beginning of this article and the text \"if any number of each box is available\" in the caption of that figure.\n\n== Computational complexity ==\nThe knapsack problem is interesting from the perspective of computer science for many reasons:\n* The [[decision problem]] form of the knapsack problem (''Can a value of at least ''V'' be achieved without exceeding the weight ''W''?'') is [[NP-complete]], thus there is no known algorithm both correct and fast (polynomial-time) in all cases.\n* While the decision problem is NP-complete, the optimization problem is [[NP-hard]], its resolution is at least as difficult as the decision problem, and there is no known polynomial algorithm which can tell, given a solution, whether it is optimal (which would mean that there is no solution with a larger ''V'', thus solving the NP-complete decision problem).\n* There is a [[pseudo-polynomial time]] algorithm using [[dynamic programming]].\n* There is a [[FPTAS|fully polynomial-time approximation scheme]], which uses the pseudo-polynomial time algorithm as a subroutine, described below.\n* Many cases that arise in practice, and \"random instances\" from some distributions, can nonetheless be solved exactly.\n\nThere is a link between the \"decision\" and \"optimization\" problems in that if there exists a polynomial algorithm that solves the  \"decision\" problem, then one can find the maximum value for the optimization problem in polynomial time by applying this algorithm iteratively while increasing the value of k . On the other hand, if an algorithm finds the optimal value of the optimization problem in polynomial time, then the decision problem can be solved in polynomial time by comparing the value of the solution output by this algorithm with the value of k . Thus, both versions of the problem are of similar difficulty.\n\nOne theme in research literature is to identify what the \"hard\" instances of the knapsack problem look like,<ref name=\"pisinger200308\">Pisinger, D. 2003. [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.7431&rep=rep1&type=pdf Where are the hard knapsack problems?] Technical Report 2003/08, Department of Computer Science, University of Copenhagen, Copenhagen, Denmark.</ref><ref name=\"Cacetta2001\">{{cite journal | last1 = Caccetta | first1 = L. | last2 = Kulanoot | first2 = A. | year = 2001 | title = Computational Aspects of Hard Knapsack Problems | url = | journal = Nonlinear Analysis | volume = 47 | issue = 8| pages = 5547–5558 | doi=10.1016/s0362-546x(01)00658-7}}</ref> or viewed another way, to identify what properties of instances in practice might make them more amenable than their worst-case NP-complete behaviour suggests.<ref name=\"poirriez_et_all_2009\">{{cite journal|last1=Poirriez|first1=Vincent|last2=Yanev|first2=Nicola|last3=Andonov|first3=Rumen|title=A hybrid algorithm for the unbounded knapsack problem|journal=Discrete Optimization|volume=6|issue=1|year=2009|pages=110–124|issn=1572-5286|doi=10.1016/j.disopt.2008.09.004}}</ref> The goal in finding these \"hard\" instances is for their use in [[public key cryptography]] systems, such as the [[Merkle-Hellman knapsack cryptosystem]].\n\n== Solving ==\nSeveral algorithms are available to solve knapsack problems, based on dynamic programming approach,<ref name=\"eduk2000\">{{cite journal | last1 = Andonov | first1 = Rumen | last2 = Poirriez | first2 = Vincent | last3 = Rajopadhye | first3 = Sanjay | year = 2000 | title = Unbounded Knapsack Problem : dynamic programming revisited | url = http://www.univ-valenciennes.fr/limav/poirriez/pub/rech/PI-1152.ps.gz| journal = European Journal of Operational Research | volume = 123 | issue = 2| pages = 168–181 | doi = 10.1016/S0377-2217(99)00265-9 | citeseerx = 10.1.1.41.2135 }}</ref> branch and bound approach<ref name=\"martellototh90\">S. Martello, P. Toth, Knapsack Problems: Algorithms and Computer Implementations,\nJohn Wiley and Sons, 1990</ref> or [[hybrid algorithm|hybridizations]] of both approaches.<ref name=\"poirriez_et_all_2009\"/><ref name=\"martellopisingertoth99a\">S. Martello, D. Pisinger, P. Toth, [https://www.jstor.org/stable/pdf/2634886.pdf?casa_token=wd6Ykeu46i0AAAAA:egeiDBT-4ijIpmM5GUKA8Ywc67SyXsHHCpFR3QlIL5GGDvx546hVIjk9vbeJsgfh_1NyDcH8EMh9TkBoqRHvMHA8xtT4uUzG5x2mZJ4FzCE_wC0mEhp0Tg Dynamic programming and strong bounds for the 0-1 knapsack problem], ''Manag. Sci.'', 45:414–424, 1999.</ref><ref name=\"plateau85\">{{cite journal | last1 = Plateau | first1 = G. | last2 = Elkihel | first2 = M. | year = 1985 | title = A hybrid algorithm for the 0-1 knapsack problem | url = | journal = Methods of Oper. Res. | volume = 49 | issue = | pages = 277–293 }}</ref><ref name=\"martellotothe99\">{{cite journal | last1 = Martello | first1 = S. | last2 = Toth | first2 = P. | year = 1984| title = A mixture of dynamic programming and branch-and-bound for the subset-sum problem | url = | journal = Manag. Sci. | volume = 30 | issue = 6| pages = 765–771 | doi=10.1287/mnsc.30.6.765}}</ref>\n\n=== Dynamic programming in-advance algorithm ===\nThe '''unbounded knapsack problem''' ('''UKP''') places no restriction on the number of copies of each kind of item. Besides, here we assume that <math>x_i > 0</math>\n: <math>m[w']= \\max \\left( \\sum_{i=1}^n v_i x_i \\right)</math>\n: subject to <math>\\sum_{i=1}^n w_i x_i \\leq w'</math> and <math>x_i > 0</math>\n\nObserve that <math>m[w]</math> has the following properties:\n\n1. <math>m[0]=0\\,\\!</math> (the sum of zero items, i.e., the summation of the empty set).\n\n2. <math>m[w]=\\max (v_1+m[w-w_1],v_2+m[w-w_2],...,v_i+m[w-w_i])</math>\n, <math>w_i \\leq w</math>, where <math>v_i</math> is the value of the <math>i</math>-th kind of item.\n\nThe second property needs to be explained in detail. During the process of the running of this method, how do we get the weight <math>w</math>? There are only <math>i</math> ways and the previous weights are <math>w-w_1, w-w_2,..., w-w_i</math> where there are total <math>i</math> kinds of different item(by saying different, we mean that the weight and the value are not completely the same). If we know each value of these <math>i</math> items and the related maximum value previously, we just compare them to each other and get the maximum value ultimately and we are done.\n\nHere the maximum of the empty set is taken to be zero. Tabulating the results from <math>m[0]</math> up through <math>m[W]</math> gives the solution. Since the calculation of each <math>m[w]</math> involves examining at most <math>n</math> items, and there are at most <math>W</math> values of <math>m[w]</math> to calculate, the running time of the dynamic programming solution is [[Big O notation|<math>O(nW)</math>]]. Dividing <math>w_1,\\,w_2,\\,\\ldots,\\,w_n,\\,W</math> by their [[greatest common divisor]] is a way to improve the running time.\n\nEven [[P versus NP problem|P≠NP]], the <math>O(nW)</math> complexity does not contradict the fact that the knapsack problem is [[NP-complete]], since <math>W</math>, unlike <math>n</math>, is not polynomial in the length of the input to the problem. The length of the <math>W</math> input to the problem is proportional to the number of bits in <math>W</math>, <math>\\log W</math>, not to <math>W</math> itself. However, since this runtime is [[pseudopolynomial]], this makes the (decision version of the) knapsack problem a [[weak NP-completeness|weakly NP-complete problem]].\n\n==== 0/1 knapsack problem ====\nA similar dynamic programming solution for the 0/1 knapsack problem also runs in pseudo-polynomial time. Assume <math>w_1,\\,w_2,\\,\\ldots,\\,w_n,\\, W</math> are strictly positive integers. Define <math>m[i,w]</math> to be the maximum value that can be attained with weight less than or equal to <math>w</math> using items up to <math>i</math> (first <math>i</math> items).\n\nWe can define <math>m[i,w]</math> recursively as follows: '''(Definition A)\n'''\n* <math>m[0,\\,w]=0</math>\n* <math>m[i,\\,w]=m[i-1,\\,w]</math> if <math>w_i > w\\,\\!</math> (the new item is more than the current weight limit)\n* <math>m[i,\\,w]=\\max(m[i-1,\\,w],\\,m[i-1,w-w_i]+v_i)</math> if <math>w_i \\leqslant w</math>.\n\nThe solution can then be found by calculating <math>m[n,W]</math>. To do this efficiently, we can use a table to store previous computations.\n\nThe following is pseudo code for the dynamic program:\n\n<source lang=\"c\" line>\n// Input:\n// Values (stored in array v)\n// Weights (stored in array w)\n// Number of distinct items (n)\n// Knapsack capacity (W)\n// NOTE: The array \"v\" and array \"w\" are assumed to store all relevant values starting at index 1.\n\nfor j from 0 to W do:\n    m[0, j] := 0\n\nfor i from 1 to n do:\n    for j from 0 to W do:\n        if w[i] > j then:\n            m[i, j] := m[i-1, j]\n        else:\n            m[i, j] := max(m[i-1, j], m[i-1, j-w[i]] + v[i])\n\n</source>\n\nThis solution will therefore run in <math>O(nW)</math> time and <math>O(nW)</math> space.\n\nHowever, if we take it a step or two further, we should know that the method will run in the time between <math>O(nW)</math> and <math>O(2^n)</math>. From '''Definition A''', we can know that there is no need for computing all the weights when the number of items and the items themselves that we chose are fixed. That is to say, the program above computes more than expected because that the weight changes from 0 to W all the time. All we need to do is to compare m[i-1, j] and m[i-1, j-w[i]] + v[i] for m[i, j], and when m[i-1, j-w[i]] is out of range, we just give the value of  m[i-1, j] to m[i, j]. From this perspective, we can program this method so that it runs recursively!\n\n<source lang=\"c\" line=\"1\">\n// Input:\n// Values (stored in array v)\n// Weights (stored in array w)\n// Number of distinct items (n)\n// Knapsack capacity (W)\n// NOTE: The array \"v\" and array \"w\" are assumed to store all relevant values starting at index 1.\n\nDefine value[n, W]\n\nInitialize All value[i, j] = -1\n\nDefine m:=(i,j)         //Define function m so that it represents the maximum value we can get under the condition: use first i items, total weight limit is j\n{\n    if i = 0 or j <= 0 then:\n        return 0\n    \n    if (value[i-1,j] == -1) then:     //m[i-1, j] has not been calculated, we have to call function m\n        value[i-1,j] = m(i-1,j)         \n  \n    if w[i] > j:                      //item cannot fit in the bag (THIS WAS MISSING FROM THE PREVIOUS ALGORITHM)\n        value[i,j] = value[i-1,j]\n    else: \n        if (value[i-1, j-w[i]] == -1) then:     //m[i-1,j-w[i]] has not been calculated, we have to call function m\n            value[i-1, j-w[i]] = m(i-1, j-w[i])\n        value[i, j] = max(value[i-1,j], value[i-1, j-w[i]] + v[i])\n\n    return value[i,j]\n}\n\nRun m(n,W)\n</source>\n\nFor example, there are 10 different items and the weight limit is 67. So,\n:<math>\\begin{align}\n&w[  1]= 23 ,w[  2]= 26,w[  3]= 20,w[  4]= 18,w[  5]= 32,w[  6]= 27,w[  7]= 29,w[  8]= 26,w[  9]= 30,w[ 10]= 27 \\\\\n&v[  1]=505 ,v[  2]=352,v[  3]=458,v[  4]=220,v[  5]=354,v[  6]=414,v[  7]=498,v[  8]=545,v[  9]=473,v[ 10]=543 \\\\\n\\end{align}</math>\nIf you use above method to compute for <math>m(10,67)</math>, you will get (''excluding calls that produce m(i,j) = 0''):\n:<math>\\begin{align}\n&m(10, 67) = 1270\\\\\n&m(9, 67) = 1270, m(9, 40) = 678\\\\\n&m(8, 67) = 1270, m(8, 40) = 678, m(8, 37) = 545\\\\\n&m(7, 67) = 1183, m(7, 41) = 725, m(7, 40) = 678, m(7, 37) = 505\\\\\n&m(6, 67) = 1183, m(6, 41) = 725, m(6, 40) = 678, m(6, 38) = 678, m(6, 37) = 505\\\\\n&m(5, 67) = 1183, m(5, 41) = 725, m(5, 40) = 678, m(5, 38) = 678, m(5, 37) = 505\\\\\n&m(4, 67) = 1183, m(4, 41) = 725, m(4, 40) = 678, m(4, 38) = 678, m(4, 37) = 505, m(4, 35) = 505\\\\\n&m(3, 67) = 963, m(3, 49) = 963, m(3, 41) = 505, m(3, 40) = 505, m(3, 38) = 505, m(3, 37) = 505, m(3, 35) = 505, m(3, 23) = 505, m(3, 22) = 458, m(3, 20) = 458\\\\\n&m(2, 67) = 857, m(2, 49) = 857, m(2, 47) = 505, m(2, 41) = 505, m(2, 40) = 505, m(2, 38) = 505, m(2, 37) = 505, m(2, 35) = 505, m(2, 29) = 505, m(2, 23) = 505\\\\\n&m(1, 67) = 505, m(1, 49) = 505, m(1, 47) = 505, m(1, 41) = 505, m(1, 40) = 505, m(1, 38) = 505, m(1, 37) = 505, m(1, 35) = 505, m(1, 29) = 505, m(1, 23) = 505\\\\\n\\end{align}</math>\n\nBesides, we can break the recursion and convert it into a tree. Then we can cut some leaves and use parallel computing to expedite the running of this method!\n\n=== Meet-in-the-middle ===\nAnother algorithm for 0-1 knapsack, discovered in 1974<ref>{{citation\n | last1 = Horowitz | first1 = Ellis\n | last2 = Sahni | first2 = Sartaj | author2-link = Sartaj Sahni\n | doi = 10.1145/321812.321823\n | journal = Journal of the Association for Computing Machinery\n | mr = 0354006\n | pages = 277–292\n | title = Computing partitions with applications to the knapsack problem\n | volume = 21\n | issue = 2\n | year = 1974| hdl = 1813/5989\n }}</ref> and sometimes called \"meet-in-the-middle\" due to parallels to [[Meet-in-the-middle attack|a similarly named algorithm in cryptography]], is exponential in the number of different items but may be preferable to the DP algorithm when <math>W</math> is large compared to ''n''. In particular, if the <math>w_i</math> are nonnegative but not integers, we could still use the dynamic programming algorithm by scaling and rounding (i.e. using [[fixed-point arithmetic]]), but if the problem requires <math>d</math> fractional digits of precision to arrive at the correct answer, <math>W</math> will need to be scaled by <math>10^d</math>, and the DP algorithm will require <math>O(W10^d)</math> space and <math>O(nW10^d)</math> time.\n\n:'' Meet-in-the-middle algorithm\n\n   '''input:''' \n     a set of items with weights and values\n   '''output:''' \n     the greatest combined value of a subset\n   partition the set {1...n} into two sets A and B of approximately equal size\n   compute the weights and values of all subsets of each set\n   '''for''' each subset of A\n     find the subset of B of greatest value such that the combined weight is less than W\n   keep track of the greatest combined value seen so far\n\nThe algorithm takes <math>O(2^{n/2})</math> space, and efficient implementations of step 3 (for instance, sorting the subsets of B by weight, discarding subsets of B which weigh more than other subsets of B of greater or equal value, and using binary search to find the best match) result in a runtime of <math>O(n2^{n/2})</math>. As with the [[Meet-in-the-middle attack|meet in the middle attack]] in cryptography, this improves on the <math>O(n2^n)</math> runtime of a naive brute force approach (examining all subsets of <math>\\{1...n\\}</math>), at the cost of using exponential rather than constant space (see also [[baby-step giant-step]]).\n\n=== Approximation algorithms ===\nAs for most NP-complete problems, it may be enough to find workable solutions even if they are not optimal. Preferably, however, the approximation comes with a guarantee on the difference between the value of the solution found and the value of the optimal solution.\n\nAs with many useful but computationally complex algorithms, there has been substantial research on creating and analyzing algorithms that approximate a solution. The knapsack problem, though NP-Hard, is one of a collection of algorithms that can still be approximated to any specified degree. This means that the problem has a polynomial time approximation scheme. To be exact, the knapsack problem has a fully polynomial time approximation scheme (FPTAS).<ref name=\"Vazirani, Vijay 2003\">Vazirani, Vijay. Approximation Algorithms. Springer-Verlag Berlin Heidelberg, 2003.</ref>\n\n==== Greedy approximation algorithm ====\n[[George Dantzig]] proposed a [[greedy algorithm|greedy]] [[approximation algorithm]] to solve the unbounded knapsack problem.<ref name=\"dantzig1957\">{{cite journal | last1 = Dantzig | first1 = George B. | authorlink = George Dantzig | year = 1957 | title = Discrete-Variable Extremum Problems | url = | journal = Operations Research | volume = 5 | issue = 2| pages = 266–288 | doi = 10.1287/opre.5.2.266 }}</ref> His version sorts the items in decreasing order of value per unit of weight, <math>v_i/w_i</math>. It then proceeds to insert them into the sack, starting with as many copies as possible of the first kind of item until there is no longer space in the sack for more. Provided that there is an unlimited supply of each kind of item, if <math>m</math> is the maximum value of items that fit into the sack, then the greedy algorithm is guaranteed to achieve at least a value of <math>m/2</math>. However, for the bounded problem, where the supply of each kind of item is limited, the algorithm may be far from optimal.\n\n==== Fully polynomial time approximation scheme ====\nThe [[fully polynomial time approximation scheme]] (FPTAS) for the knapsack problem takes advantage of the fact that the reason the problem has no known polynomial time solutions is because the profits associated with the items are not restricted. If one rounds off some of the least significant digits of the profit values then they will be bounded by a polynomial and 1/ε where ε is a bound on the correctness of the solution. This restriction then means that an algorithm can find a solution in polynomial time that is correct within a factor of (1-ε) of the optimal solution.<ref name=\"Vazirani, Vijay 2003\"/>\n\n:'' An Algorithm for FPTAS\n\n  '''input''': \n    ε ∈ (0,1]\n    a list A of n items, specified by their values, <math>v_i</math>, and weights\n  '''output''':\n    S' the FPTAS solution\n\n  P := max <math>\\{v_i\\mid 1 \\leq i \\leq n\\} </math>  // the highest item value\n  K := ε <math>\\frac{P}{n}</math>\n  '''for''' i '''from''' 1 '''to''' n '''do'''\n     <math>v'_i</math> := <math>\\left\\lfloor \\frac{v_i}{K} \\right\\rfloor</math>\n  '''end for'''\n  '''return''' the solution, S', using the <math>v'_i</math> values in the dynamic program outlined above\n\n'''Theorem:''' The set <math>S'</math> computed by the algorithm above satisfies <math>\\mathrm{profit}(S') \\geq (1-\\varepsilon) \\cdot \\mathrm{profit}(S^*)</math>, where <math>S^*</math> is an optimal solution.\n\n===Dominance relations===\nSolving the unbounded knapsack problem can be made easier by throwing away items which will never be needed.  For a given item <math>i</math>, suppose we could find a set of items <math>J</math> such that their total weight is less than the weight of <math>i</math>, and their total value is greater than the value of <math>i</math>.  Then <math>i</math> cannot appear in the optimal solution, because we could always improve any potential solution containing <math>i</math> by replacing <math>i</math> with the set <math>J</math>.  Therefore, we can disregard the <math>i</math>-th item altogether.  In such cases, <math>J</math> is said to '''dominate''' <math>i</math>.  (Note that this does not apply to bounded knapsack problems, since we may have already used up the items in <math>J</math>.)\n\nFinding dominance relations allows us to significantly reduce the size of the search space.  There are several different types of [[dominance relations]],<ref name=\"poirriez_et_all_2009\" /> which all satisfy an inequality of the form:\n\n<math>\\qquad \\sum_{j \\in J} w_j\\,x_j \\ \\le  \\alpha\\,w_i</math>, and <math>\\sum_{j \\in J} v_j\\,x_j \\ \\ge \\alpha\\,v_i\\,</math> for some <math>x \\in Z _+^n </math>\n\nwhere\n<math>\\alpha\\in Z_+ \\,,J\\subsetneq N</math> and <math>i\\not\\in J</math>.  The vector <math>x</math> denotes the number of copies of each member of <math>J</math>.\n\n;Collective dominance:  The <math>i</math>-th item is  '''collectively dominated''' by <math>J</math>, written as <math>i\\ll J</math>, if the total weight of some combination of items in <math>J</math> is less than ''w<sub>i</sub>'' and their total value is greater than ''v<sub>i</sub>''.  Formally, <math>\\sum_{j \\in J} w_j\\,x_j \\ \\le  w_i</math> and <math>\\sum_{j \\in J} v_j\\,x_j \\ \\ge v_i</math> for some <math>x \\in Z _+^n </math>, i.e. <math>\\alpha=1</math>.  Verifying this dominance is computationally hard, so it can only be used with a dynamic programming approach.  In fact, this is equivalent to solving a smaller knapsack decision problem where <math>V = v_i</math>, <math>W = w_i</math>, and the items are restricted to <math>J</math>.\n;Threshold dominance: The <math>i</math>-th item is '''threshold dominated'''  by <math>J</math>, written as <math>i\\prec\\prec J</math>, if some number of copies of <math>i</math> are dominated by <math>J</math>.  Formally, <math>\\sum_{j \\in J} w_j\\,x_j \\ \\le  \\alpha\\,w_i</math>, and <math>\\sum_{j \\in J} v_j\\,x_j \\ \\ge \\alpha\\,v_i\\,</math> for some <math>x \\in Z _+^n </math> and <math>\\alpha\\geq 1</math>.  This is a generalization of collective dominance, first introduced in<ref name=\"eduk2000\"/> and used in the EDUK algorithm.  The smallest such <math>\\alpha</math> defines the '''threshold''' of the item <math>i</math>, written  <math>t_i =(\\alpha-1)w_i</math>.  In this case, the optimal solution could contain at most <math>\\alpha-1</math> copies of <math>i</math>.\n;Multiple dominance: The <math>i</math>-th item is  '''multiply dominated''' by a single item <math>j</math>, written as <math>i\\ll_{m} j</math>, if <math>i</math> is dominated by some number of copies of <math>j</math>.  Formally, <math>w_j\\,x_j \\ \\le  w_i</math>, and <math>v_j\\,x_j \\ \\ge v_i</math> for some <math>x_j \\in Z _+ </math> i.e. <math> J=\\{j\\}, \\alpha=1,  x_j=\\lfloor \\frac{w_i}{w_j}\\rfloor</math>. This dominance could be efficiently used during preprocessing because it can be detected relatively easily.\n;Modular dominance: Let <math>b</math> be the ''best item'', i.e. <math>\\frac{v_b}{w_b}\\ge\\frac{v_i}{w_i}\\, </math> for all <math>i</math>.  This is the item with the greatest density of value. The <math>i</math>-th item is '''modularly dominated''' by a single item <math>j</math>, written as <math>i\\ll_\\equiv j</math>, if <math>i</math> is dominated by <math>j</math> plus several copies of <math>b</math>.  Formally, <math> w_j+tw_b \\le w_i</math>, and  <math>v_j +tv_b \\ge v_i </math> i.e. <math>J=\\{b,j\\}, \\alpha=1,  x_b=t, x_j=1</math>.\n\n== Variations ==\nThere are many variations of the knapsack problem that have arisen from the vast number of applications of the basic problem. The main variations occur by changing the number of some problem parameter such as the number of items, number of objectives, or even the number of knapsacks.\n\n===Multi-objective knapsack problem===\nThis variation changes the goal of the individual filling the knapsack. Instead of one objective, such as maximizing the monetary profit, the objective could have several dimensions. For example, there could be environmental or social concerns as well as economic goals. Problems frequently addressed include portfolio and transportation logistics optimizations.<ref>Chang, T. J., et al. [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.109.6698&rep=rep1&type=pdf Heuristics for Cardinality Constrained Portfolio Optimization].\nTechnical Report, London SW7 2AZ, England: The Management School, Imperial\nCollege, May 1998</ref><ref>Chang, C. S., et al. \"[https://scholarbank.nus.edu.sg/handle/10635/72660 Genetic Algorithm Based Bicriterion Optimization for Traction Substations in DC Railway System].\" In Fogel [102], 11-16.</ref>\n\nAs an example, suppose you ran a cruise ship. You have to decide how many famous comedians to hire. This boat can handle no more than one ton of passengers and the entertainers must weigh less than 1000&nbsp;lbs. Each comedian has a weight, brings in business based on their popularity and asks for a specific salary. In this example, you have multiple objectives. You want, of course, to maximize the popularity of your entertainers while minimizing their salaries. Also, you want to have as many entertainers as possible.\n\n===Multi-dimensional knapsack problem===\nIn this variation, the weight of knapsack item <math>i</math> is given by a D-dimensional vector <math>\\overline{w_i}=(w_{i1},\\ldots,w_{iD})</math> and the knapsack has a D-dimensional capacity vector <math>(W_1,\\ldots,W_D)</math>. The target is to maximize the sum of the values of the items in the knapsack so that the sum of weights in each dimension <math>d</math> does not exceed <math>W_d</math>.\n\nMulti-dimensional knapsack is computationally harder than knapsack; even for <math>D=2</math>, the problem does not have [[EPTAS]] unless P<math>=</math>NP.<ref>{{cite journal | last1 = Kulik | first1 = A. | last2 = Shachnai | first2 = H. | year = 2010 | title = There is no EPTAS for two dimensional knapsack | url = http://www.cs.technion.ac.il/~hadas/PUB/multi_knap.pdf| journal = Inf. Process. Lett. | volume = 110 | issue = 16| pages = 707–710 | doi=10.1016/j.ipl.2010.05.031| citeseerx = 10.1.1.161.5838 }}</ref> However, the algorithm in<ref name=CohenGrebla>Cohen, R. and Grebla, G. 2014. [http://wimnet.ee.columbia.edu/wp-content/uploads/2013/03/paper_short.pdf \"Multi-Dimensional OFDMA Scheduling in a Wireless Network with Relay Nodes\"]. in ''Proc. IEEE INFOCOM’14'', 2427–2435.</ref> is shown to solve sparse instances efficiently. An instance of multi-dimensional knapsack is sparse if there is a set <math>J=\\{1,2,\\ldots,m\\}</math> for <math>m<D</math> such that for every knapsack item <math>i</math>, <math> \\exists z>m</math> such that <math>\\forall j\\in J\\cup \\{z\\},\\ w_{ij}\\geq 0</math> and <math>\\forall y\\notin J\\cup\\{z\\}, w_{iy}=0</math>. Such instances occur, for example, when scheduling packets in a wireless network with relay nodes.<ref name=CohenGrebla/> The algorithm from<ref name=CohenGrebla/> also solves sparse instances of the multiple choice variant, multiple-choice multi-dimensional knapsack.\n\nThe IHS (Increasing Height Shelf) algorithm is optimal for 2D knapsack (packing squares into a two-dimensional unit size square): when there are at most five square in an optimal packing.<ref>Yan Lan, György Dósa, Xin Han, Chenyang Zhou, Attila Benkő [https://scholar.google.com/citations?user=txyI5aAAAAAJ]: ''2D knapsack: Packing squares'', Theoretical Computer Science Vol. 508, pp. 35–40.</ref>\n\n===Multiple knapsack problem===\nThis variation is similar to the [[Bin packing problem|Bin Packing Problem]]. It differs from the Bin Packing Problem in that a subset of items can be selected, whereas, in the Bin Packing Problem, all items have to be packed to certain bins. The concept is that there are multiple knapsacks. This may seem like a trivial change, but it is not equivalent to adding to the capacity of the initial knapsack. This variation is used in many loading and scheduling problems in Operations Research and has a [[Polynomial-time approximation scheme]].<ref>{{Cite journal|title=A PTAS for the multiple knapsack problem|journal = SIAM Journal on Computing|volume = 35|issue = 3|pages = 713–728|last=Chandra Chekuri and Sanjeev Khanna|date=2005|doi=10.1137/s0097539700382820|citeseerx = 10.1.1.226.3387}}</ref>\n\n===Quadratic knapsack problem===\nAs described by Wu et al.:\n<blockquote>\nThe [[quadratic knapsack problem]] (QKP) maximizes a quadratic objective function subject to a binary and linear capacity constraint.<ref name=QKP>{{cite journal | title = Global Optimality Conditions and Optimization Methods for Quadratic Knapsack Problems |author1=Wu, Z. Y. |author2=Yang, Y. J. |author3=Bai, F. S. |author4=Mammadov, M. | journal = J Optim Theory Appl | year = 2011 | volume = 151 |issue=2 | pages = 241&ndash;259 | doi = 10.1007/s10957-011-9885-4 }}</ref>\n</blockquote>\nThe quadratic knapsack problem was discussed under that title by Gallo, Hammer, and Simeone in 1980.<ref>{{cite book | title = Quadratic knapsack problems |author1=Gallo, G. |author2=Hammer, P. L. |author3=Simeone, B. | journal = Mathematical Programming Studies | year = 1980 | volume = 12 | pages = 132&ndash;149 | doi = 10.1007/BFb0120892 |isbn=978-3-642-00801-6 }}</ref>  However, Gallo and Simeone<ref>{{cite journal | title = On the Supermodular Knapsack Problem |author1=Gallo, G. |author2=Simeone, B. | journal = Mathematical Programming Studies | year = 1988 | volume = 45 |issue=1–3 | pages = 295&ndash;309 | doi=10.1007/bf01589108}}</ref> attribute the first treatment of the problem to Witzgall<ref>{{cite book | title = Mathematical methods of site selection for Electronic Message Systems (EMS) | author = Witzgall, C. | publisher = NBS Internal report | year = 1975|url=http://adsabs.harvard.edu/abs/1975STIN...7618321W}}</ref> in 1975.\n\n===Subset-sum problem===\nThe [[subset sum problem]] is a special case of the decision and '''0-1''' problems where each kind of item, the weight equals the value: <math>w_i=v_i</math>.  In the field of [[cryptography]], the term ''knapsack problem'' is often used to refer specifically to the subset sum problem and is commonly known as one of [[Karp's 21 NP-complete problems]].<ref>Richard M. Karp (1972). \"[https://pdfs.semanticscholar.org/a3c3/7657822859549cd6b12b0d1f76f8ee3680a0.pdf Reducibility Among Combinatorial Problems]\". In R. E. Miller and J. W. Thatcher (editors). Complexity of Computer Computations. New York: Plenum. pp. 85–103</ref>\n\nThe generalization of subset sum problem is called multiple subset-sum problem, in which multiple bins exist with the same capacity. It has been shown that the generalization does not have an FPTAS.<ref>{{cite journal | last1 = Caprara | first1 = Alberto | last2 = Kellerer | first2 = Hans | last3 = Pferschy | first3 = Ulrich | year = 2000 | title = The Multiple Subset Sum Problem | url = http://www.or.deis.unibo.it/alberto/mssp_siam.ps| journal = SIAM J. Optim. | volume = 11 | issue = 2| pages = 308–319 | doi = 10.1137/S1052623498348481 | citeseerx = 10.1.1.21.9826 }}</ref>\n\n==In popular culture==\n* Neal Stephenson provides an example of the knapsack problem in chapter 70 of his novel ''[[Cryptonomicon]]'' to distribute family heirlooms.\n* The knapsack problem occurs commonly in role-playing games, both digital and paper-based (prominent examples include ''[[The Elder Scrolls]]'' series and the ''[[Dungeons and Dragons]]'' game, respectively), where the player character is constrained by their encumbrance threshold when carrying items and treasure, which regularly forces the player to evaluate the items' value-to-weight ratio in order to bring only the most value-dense items to a merchant.\n* Web comic [https://xkcd.com/287/ xkcd #287] - NP-Complete\n* In Charles Stross' ''[[Accelerando]]'', the main character Manfred refers to the 'blind knapsack problem' in chapter 2, presumably a generalization or more complex version of the regular knapsack problem.\n\n==See also==\n{{Portal|Computer programming|Computer science}}\n{{Div col|colwidth=25em}}\n* [[Change-making problem]]\n* [[Combinatorial auction]]\n* [[Combinatorial optimization]]\n* [[Continuous knapsack problem]]\n* [[Cutting stock problem]]\n* [[List of knapsack problems]]\n* [[Packing problem]]\n{{div col end}}\n\n==Notes==\n{{Reflist|30em}}\n\n==References==\n* {{Cite book\n | authorlink = Michael R. Garey\n | first = Michael R. | last = Garey\n |author2=[[David S. Johnson]] \n | year = 1979\n | title = Computers and Intractability: A Guide to the Theory of NP-Completeness\n | publisher = W.H. Freeman\n | isbn = 978-0-7167-1045-5\n | title-link = Computers and Intractability: A Guide to the Theory of NP-Completeness }} A6: MP9, pg.247.\n* <cite name=\"Kellerer\">{{cite book | title = Knapsack Problems |author1=Kellerer, Hans |author2=Pferschy, Ulrich |author3=Pisinger, David | publisher = Springer | year = 2004 | isbn = 978-3-540-40286-2|mr=2161720 | doi = 10.1007/978-3-540-24777-7}}</cite>\n* <cite name=\"Martello\">{{cite book | title = Knapsack problems: Algorithms and computer implementations | last1=Martello|first1=Silvano|last2=Toth|first2=Paolo| publisher =Wiley-Interscience | year = 1990 | isbn = 978-0-471-92420-3|mr=1086874 }}</cite>\n\n==External links==\n* [http://www.or.deis.unibo.it/knapsack.html Free download of the book \"Knapsack problems: Algorithms and computer implementations\", by Silvano Martello and Paolo Toth]\n* [http://www.cse.unl.edu/~goddard/Courses/CSCE310J/Lectures/Lecture8-DynamicProgramming.pdf Lecture slides on the knapsack problem]\n* [http://download.gna.org/pyasukp PYAsUKP: Yet Another solver for the Unbounded Knapsack Problem], with code taking advantage of the dominance relations in an hybrid algorithm, benchmarks and downloadable copies of some papers.\n* [http://www.diku.dk/~pisinger/ Home page of David Pisinger] with downloadable copies of some papers on the publication list (including  \"Where are the hard knapsack problems?\")\n* [http://rosettacode.org/wiki/Knapsack_Problem Knapsack Problem solutions in many languages] at [[Rosetta Code]]\n* [http://www.personal.kent.edu/~rmuhamma/Algorithms/MyAlgorithms/Dynamic/knapsackdyn.htm Dynamic Programming algorithm to 0/1 Knapsack problem]\n* [http://karaffeltut.com/NEWKaraffeltutCom/Knapsack/knapsack.html Knapsack Problem solver (online)]\n* [http://www.nils-haldenwang.de/computer-science/computational-intelligence/genetic-algorithm-vs-0-1-knapsack Solving 0-1-KNAPSACK with Genetic Algorithms in Ruby]\n* [http://www.adaptivebox.net/CILib/code/qkpcodes_link.html Codes for Quadratic Knapsack Problem]\n{{Use dmy dates|date=September 2010}}\n*[https://pdfs.semanticscholar.org/bb99/86af2f26f7726fcef1bc684eac8239c9b853.pdf?_ga=1.50320358.1394974689.1485463187 Optimizing Three-Dimensional Bin Packing]\n\n{{DEFAULTSORT:Knapsack Problem}}\n[[Category:Cryptography]]\n[[Category:Packing problems]]\n[[Category:NP-complete problems]]\n[[Category:Dynamic programming]]\n[[Category:Combinatorial optimization]]\n[[Category:Weakly NP-complete problems]]\n[[Category:Pseudo-polynomial time algorithms]]"
    },
    {
      "title": "Levenshtein distance",
      "url": "https://en.wikipedia.org/wiki/Levenshtein_distance",
      "text": "In [[information theory]], [[linguistics]] and [[computer science]], the '''Levenshtein distance''' is a [[string metric]] for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. It is named after the Soviet mathematician [[Vladimir Levenshtein]], who considered this distance in 1965.<ref>{{cite journal |author=Влади́мир И. Левенштейн |script-title=ru:Двоичные коды с исправлением выпадений, вставок и замещений символов |language=Russian |trans-title=Binary codes capable of correcting deletions, insertions, and reversals |journal=Доклады Академий Наук СCCP |volume=163 |issue=4 |pages=845–8 |year=1965}} Appeared in English as: {{cite journal |author=Levenshtein, Vladimir I. |title=Binary codes capable of correcting deletions, insertions, and reversals |journal=Soviet Physics Doklady |volume=10 |number=8 |pages=707–710 |date=February 1966  |url=<!--http://profs.sci.univr.it/~liptak/ALBioinfo/files/levenshtein66.pdf right to publish copy of journal unclear: see http://www.sherpa.ac.uk/romeo/search.php?issn=1028-3358&type=issn&la=en/&fIDnum=%7C&mode=simple ; in any event, liptak does not appear to be the author or the translator -->|bibcode=1966SPhD...10..707L }}</ref>\n\nLevenshtein distance may also be referred to as '''edit distance''', although that term may also denote a larger [[Edit distance|family of distance metrics]].<ref name=\"navarro\">{{Cite journal |last1=Navarro |first1=Gonzalo |doi=10.1145/375360.375365 |title=A guided tour to approximate string matching |journal=ACM Computing Surveys |volume=33 |issue=1 |pages=31–88 |year=2001 |url=http://repositorio.uchile.cl/bitstream/handle/2250/126168/Navarro_Gonzalo_Guided_tour.pdf|citeseerx=10.1.1.452.6317 }}</ref>{{rp|32}} It is closely related to [[Sequence alignment#Pairwise alignment|pairwise string alignments]].\n\n== Definition ==\nMathematically, the Levenshtein distance between two strings <math>a, b</math> (of length <math>|a|</math> and <math>|b|</math> respectively) is given by <math>\\operatorname{lev}_{a,b}(|a|,|b|)</math> where\n\n:<math>\\qquad\\operatorname{lev}_{a,b}(i,j) = \\begin{cases}\n  \\max(i,j) & \\text{ if } \\min(i,j)=0, \\\\\n  \\min \\begin{cases}\n          \\operatorname{lev}_{a,b}(i-1,j) + 1 \\\\\n          \\operatorname{lev}_{a,b}(i,j-1) + 1 \\\\\n          \\operatorname{lev}_{a,b}(i-1,j-1) + 1_{(a_i \\neq b_j)}\n       \\end{cases} & \\text{ otherwise.}\n\\end{cases}</math>\nwhere  <math>1_{(a_i \\neq b_j)}</math> is the [[indicator function]] equal to 0 when  <math>a_i = b_j</math> and equal to 1 otherwise, and <math>\\operatorname{lev}_{a,b}(i,j)</math> is the distance between the first <math>i</math> characters of <math>a</math> and the first <math>j</math> characters of <math>b</math>.\n\nNote that the first element in the minimum corresponds to deletion (from <math>a</math> to <math>b</math>), the second to insertion and the third to match or mismatch, depending on whether the respective symbols are the same.\n\n=== Example ===\nFor example, the Levenshtein distance between \"kitten\" and \"sitting\" is 3, since the following three edits change one into the other, and there is no way to do it with fewer than three edits:\n\n# '''k'''itten → '''s'''itten (substitution of \"s\" for \"k\")\n# sitt'''e'''n → sitt'''i'''n (substitution of \"i\" for \"e\")\n# sittin → sittin'''g''' (insertion of \"g\" at the end).\n\n===Upper and lower bounds===\nThe Levenshtein distance has several simple upper and lower bounds. These include:\n* It is at least the difference of the sizes of the two strings.\n* It is at most the length of the longer string.\n* It is zero if and only if the strings are equal.\n* If the strings are the same size, the [[Hamming distance]] is an upper bound on the Levenshtein distance.\n* The Levenshtein distance between two strings is no greater than the sum of their Levenshtein distances from a third string ([[triangle inequality]]).\n\nAn example where the Levenshtein distance between two strings of the same length is strictly less than the Hamming distance is given by the pair \"flaw\" and \"lawn\". Here the Levenshtein distance equals 2 (delete \"f\" from the front; insert \"n\" at the end). The [[Hamming distance]] is 4.\n\n==Applications==\nIn [[approximate string matching]], the objective is to find matches for short strings in many longer texts, in situations where a small number of differences is to be expected. The short strings could come from a dictionary, for instance. Here, one of the strings is typically short, while the other is arbitrarily long. This has a wide range of applications, for instance, [[spell checker]]s, correction systems for [[optical character recognition]], and software to assist natural language translation based on [[translation memory]].\n\nThe Levenshtein distance can also be computed between two longer strings, but the cost to compute it, which is roughly proportional to the product of the two string lengths, makes this impractical.  Thus, when used to aid in [[fuzzy string searching]] in applications such as [[record linkage]], the compared strings are usually short to help improve speed of comparisons.{{citation needed|date=January 2019}}\n\nIn linguistics, the Levenshtein distance is used as a metric to quantify the [[linguistic distance]], or how different two languages are from one another.<ref name=\"ref05xubej\">{{Citation | title=Receptive multilingualism: linguistic analyses, language policies, and didactic concepts |author1=Jan D. ten Thije |author2=Ludger Zeevaert | publisher=John Benjamins Publishing Company, 2007 | isbn=978-90-272-1926-8 | url=https://books.google.com/books?id=8gIEN068J3gC&q=Levenshtein#v=snippet&q=Levenshtein&f=false | quote=''... Assuming that intelligibility is inversely related to linguistic distance ... the content words the percentage of cognates (related directly or via a synonym) ... lexical relatedness ... grammatical relatedness ...''|date=2007-01-01 }}</ref> It is related to [[mutual intelligibility]], the higher the linguistic distance, the lower the mutual intelligibility, and the lower the linguistic distance, the higher the mutual intelligibility.\n\n==Relationship with other edit distance metrics==\n{{main|Edit distance}}\nThere are other popular measures of [[edit distance]], which are calculated using a different set of allowable edit operations. For instance,\n* the [[Damerau–Levenshtein distance]] allows insertion, deletion, substitution, and the [[Transposition (mathematics)|transposition]] of two adjacent characters;\n* the [[longest common subsequence problem|longest common subsequence]] (LCS) distance allows only insertion and deletion, not substitution;\n* the [[Hamming distance]] allows only substitution, hence, it only applies to strings of the same length.\n* the [[Jaro distance]] allows only [[Transposition (mathematics)|transposition]].\n\n[[Edit distance]] is usually defined as a parameterizable metric calculated with a specific set of allowed edit operations, and each operation is assigned a cost (possibly infinite). This is further generalized by DNA [[sequence alignment]] algorithms such as the [[Smith–Waterman algorithm]], which make an operation's cost depend on where it is applied.\n\n==Computing Levenshtein distance==\n===Recursive===\nThis is a straightforward, but inefficient, recursive [[C (programming language)|C]] implementation of a <code>LevenshteinDistance</code> function that takes two strings, ''s'' and ''t'', together with their lengths, and returns the Levenshtein distance between them:\n\n<!--\n  Please do not add an additional implementation in your language of choice.\n  Many of those have been added to and deleted from this article in the past.\n  See the talk page archive for relevant discussion\n-->\n<syntaxhighlight lang=\"C\">\n// len_s and len_t are the number of characters in string s and t respectively\nint LevenshteinDistance(const char *s, int len_s, const char *t, int len_t)\n{ \n  int cost;\n\n  /* base case: empty strings */\n  if (len_s == 0) return len_t;\n  if (len_t == 0) return len_s;\n\n  /* test if last characters of the strings match */\n  if (s[len_s-1] == t[len_t-1])\n      cost = 0;\n  else\n      cost = 1;\n\n  /* return minimum of delete char from s, delete char from t, and delete char from both */\n  return minimum(LevenshteinDistance(s, len_s - 1, t, len_t    ) + 1,\n                 LevenshteinDistance(s, len_s    , t, len_t - 1) + 1,\n                 LevenshteinDistance(s, len_s - 1, t, len_t - 1) + cost);\n}\n</syntaxhighlight>\n\nThis implementation is very inefficient because it recomputes the Levenshtein distance of the same substrings many times.\n\nA more efficient method would never repeat the same distance calculation. For example, the Levenshtein distance of all possible prefixes might be stored in an array {{code|lang=C|code=d[][]}} where {{code|lang=C|code=d[i][j]}} is the distance between the first <code>i</code> characters of string <code>s</code> and the first <code>j</code> characters of string <code>t</code>. The table is easy to construct one row at a time starting with row 0. When the entire table has been built, the desired distance is {{code|lang=C|code=d[len_s][len_t]}}.\n\n===Iterative with full matrix===\n{{main|Wagner–Fischer algorithm}}\n::{{small|Note: This section uses 1-based strings instead of 0-based strings}}\nComputing the Levenshtein distance is based on the observation that if we reserve a [[Matrix (mathematics)|matrix]] to hold the Levenshtein distances between all [[prefix (computer science)|prefix]]es of the first string and all prefixes of the second, then we can compute the values in the matrix in a [[dynamic programming]] fashion, and thus find the distance between the two full strings as the last value computed.\n\nThis algorithm, an example of bottom-up [[dynamic programming]], is discussed, with variants, in the 1974 article ''The [[String-to-string correction problem]]'' by Robert A. Wagner and Michael J. Fischer.<ref>{{citation |first=Robert A. |last=Wagner |first2=Michael J. |last2=Fischer |author2-link=Michael J. Fischer |title=The String-to-String Correction Problem |journal=Journal of the ACM |volume=21 |issue=1 |year=1974 |pages=168–173 |doi= 10.1145/321796.321811}}</ref>\n\nThis is a straightforward pseudocode implementation for a function <code>LevenshteinDistance</code> that takes two strings, ''s'' of length ''m'', and ''t'' of length ''n'', and returns the Levenshtein distance between them:\n\n<!--\n  Please do not add an additional implementation in your language of choice.\n  Many of those have been added to and deleted from this article in the past.\n  See the talk page archive for relevant discussion.\n\n  Before you \"improve\" the code below, please use this article's edit history\n\n    https://en.wikipedia.org/w/index.php?title=Levenshtein_distance&offset=&limit=500&action=history\n\n  to review some of the many previous \"corrections\" that have had to be undone in the past.\n\n  Thank you!\n-->\n<!-- choose random language for highlights -->\n<syntaxhighlight lang=\"C\">\nfunction LevenshteinDistance(char s[1..m], char t[1..n]):\n  // for all i and j, d[i,j] will hold the Levenshtein distance between\n  // the first i characters of s and the first j characters of t\n  // note that d has (m+1)*(n+1) values\n  declare int d[0..m, 0..n]\n \n  set each element in d to zero\n \n  // source prefixes can be transformed into empty string by\n  // dropping all characters\n  for i from 1 to m:\n      d[i, 0] := i\n \n  // target prefixes can be reached from empty source prefix\n  // by inserting every character\n  for j from 1 to n:\n      d[0, j] := j\n \n  for j from 1 to n:\n      for i from 1 to m:\n          if s[i] = t[j]:\n            substitutionCost := 0\n          else:\n            substitutionCost := 1\n\n          d[i, j] := minimum(d[i-1, j] + 1,                   // deletion\n                             d[i, j-1] + 1,                   // insertion\n                             d[i-1, j-1] + substitutionCost)  // substitution\n \n  return d[m, n]\n</syntaxhighlight>\n\nTwo examples of the resulting matrix (hovering over a tagged number reveals the operation performed to get that number):\n<center>\n{{col-begin|width=auto}}\n{{col-break}}\n{|class=\"wikitable\"\n|-\n| \n| \n!k\n!i\n!t\n!t\n!e\n!n\n|-\n| ||0 ||1 ||2 ||3 ||4 ||5 ||6\n|-\n!s\n|1 ||{{H:title|substitution of 'k' for 's'|1}} ||2 ||3 ||4 ||5 ||6\n|-\n!i\n|2 ||2 ||{{H:title|'i' equals 'i'|1}} ||2 ||3 ||4 ||5\n|-\n!t\n|3 ||3 ||2 ||{{H:title|'t' equals 't'|1}} ||2 ||3 ||4\n|-\n!t\n|4 ||4 ||3 ||2 ||{{H:title|'t' equals 't'|1}} ||2 ||3\n|-\n!i\n|5 ||5 ||4 ||3 ||2 ||{{H:title|substitution of 'e' for 'i'|2}} ||3\n|-\n!n\n|6 ||6 ||5 ||4 ||3 ||3 ||{{H:title|'n' equals 'n'|2}}\n|-\n!g\n|7 ||7 ||6 ||5 ||4 ||4 ||{{H:title|delete'g'|3}}\n|}\n{{col-break|gap=1em}}\n{|class=\"wikitable\"\n|\n|\n!S\n!a\n!t\n!u\n!r\n!d\n!a\n!y\n|-\n| \n|0 ||1 ||2 ||3 ||4 ||5 ||6 ||7 ||8\n|-\n!S\n|1 ||{{H:title|'S' equals 'S'|0}} ||{{H:title|insert 'a'|1}} ||{{H:title|insert 't'|2}} ||3 ||4 ||5 ||6 ||7\n|-\n!u\n|2 ||1 ||1 ||2 ||{{H:title|'u' equals 'u'|2}} ||3 ||4 ||5 ||6\n|-\n!n\n|3 ||2 ||2 ||2 ||3 ||{{H:title|substitution of 'r' for 'n'|3}} ||4 ||5 ||6\n|-\n!d\n|4 ||3 ||3 ||3 ||3 ||4 ||{{H:title|'d' equals 'd'|3}} ||4 ||5\n|-\n!a\n|5 ||4 ||3 ||4 ||4 ||4 ||4 ||{{H:title|'a' equals 'a'|3}} ||4\n|-\n!y\n|6 ||5 ||4 ||4 ||5 ||5 ||5 ||4 ||{{H:title|'y' equals 'y'|3}}\n|}\n{{col-end}}\n</center>\n\nThe [[invariant (mathematics)|invariant]] maintained throughout the algorithm is that we can transform the initial segment {{code|lang=C|code=s[1..i]}} into {{code|lang=C|code=t[1..j]}} using a minimum of {{code|lang=C|code=d[i,j]}} operations. At the end, the bottom-right element of the array contains the answer.\n\n===Iterative with two matrix rows===\nIt turns out that only two rows of the table are needed for the construction if one does not want to reconstruct the edited input strings (the previous row and the current row being calculated).\n\nThe Levenshtein distance may be calculated iteratively using the following algorithm:<ref>{{Citation |title=Fast, memory efficient Levenshtein algorithm |first=Sten |last=Hjelmqvist |date=26 Mar 2012 |url=http://www.codeproject.com/Articles/13525/Fast-memory-efficient-Levenshtein-algorithm}}</ref>\n<syntaxhighlight lang=\"c\">\nfunction LevenshteinDistance(char s[0..m-1], char t[0..n-1]):\n    // create two work vectors of integer distances\n    declare int v0[n + 1]\n    declare int v1[n + 1]\n\n    // initialize v0 (the previous row of distances)\n    // this row is A[0][i]: edit distance for an empty s\n    // the distance is just the number of characters to delete from t\n    for i from 0 to n:\n        v0[i] = i\n\n    for i from 0 to m-1:\n        // calculate v1 (current row distances) from the previous row v0\n\n        // first element of v1 is A[i+1][0]\n        //   edit distance is delete (i+1) chars from s to match empty t\n        v1[0] = i + 1\n\n        // use formula to fill in the rest of the row\n        for j from 0 to n-1:\n            // calculating costs for A[i+1][j+1]\n            deletionCost := v0[j + 1] + 1\n            insertionCost := v1[j] + 1\n            if s[i] = t[j]:\n                substitutionCost := v0[j]\n            else:\n                substitutionCost := v0[j] + 1;\n\n            v1[j + 1] := minimum(deletionCost, insertionCost, substitutionCost)\n\n        // copy v1 (current row) to v0 (previous row) for next iteration\n        swap v0 with v1\n    // after the last swap, the results of v1 are now in v0\n    return v0[n]\n</syntaxhighlight>\n\nThis two row variant is suboptimal—the amount of memory required may be reduced to one row and one word of overhead.<ref>{{cite web | url=https://bitbucket.org/clearer/iosifovich/ | title=Clearer / Iosifovich}}</ref>\n\n[[Hirschberg's algorithm]] combines this method with [[Divide and conquer algorithms|divide and conquer]]. It can compute the optimal edit sequence, and not just the edit distance, in the same asymptotic time and space bounds.<ref>{{cite journal|last=Hirschberg|first=D. S.|authorlink=Dan Hirschberg|title=A linear space algorithm for computing maximal common subsequences|journal=[[Communications of the ACM]]|volume=18|issue=6|year=1975|pages=341–343|doi=10.1145/360825.360861|mr=0375829|url=http://www.ics.uci.edu/~dan/pubs/p341-hirschberg.pdf|type=Submitted manuscript|bibcode=1985CACM...28...22S|citeseerx=10.1.1.348.4774}}</ref>\n\n=== Adaptive variant ===\n\nThe dynamic variant is not the ideal implementation. An adaptive approach may reduce the amount of memory required and, in the best case, may reduce the time complexity to linear in the length of the shortest string, and, in the worst case, no more than quadratic in the length of the shortest string. <ref>{{cite web | url=https://bitbucket.org/clearer/iosifovich/ | title=Clearer / Iosifovich}}</ref>\n\n===Approximation===\nThe Levenshtein distance between two strings of length {{mvar|n}} can be [[Approximation algorithm|approximated]] to within a factor\n\n:<math>(\\log n)^{O(1/\\varepsilon)}</math>\n\nwhere {{math|''ε'' > 0}} is a free parameter to be tuned, in time {{math|''O''(n<sup>1 + ''ε''</sup>)}}.<ref>{{cite conference |last1=Andoni |first1=Alexandr |first2=Robert |last2=Krauthgamer |first3=Krzysztof |last3=Onak |title=Polylogarithmic approximation for edit distance and the asymmetric query complexity |conference=IEEE Symp. Foundations of Computer Science (FOCS) |year=2010 |citeseerx=10.1.1.208.2079 |arxiv=1005.4033|bibcode=2010arXiv1005.4033A }}</ref>\n\n===Computational complexity===\nIt has been shown that the Levenshtein distance of two strings of length {{mvar|n}} cannot be computed in time {{math|''O''(n<sup>2 - ''ε''</sup>)}} for any ε greater than zero unless the [[strong exponential time hypothesis]] is false.<ref>{{cite conference |last1=Backurs |first1=Arturs |first2=Piotr |last2=Indyk |title= Edit Distance Cannot Be Computed in Strongly Subquadratic Time (unless SETH is false) |conference=Forty-Seventh Annual ACM on Symposium on Theory of Computing (STOC) |year=2015 | arxiv=1412.0348|bibcode=2014arXiv1412.0348B }}</ref>\n\n==See also==\n{{div col|colwidth=25em}}\n*[[agrep]]\n*[[Damerau–Levenshtein distance]]\n*[[diff]]\n*[[Dynamic time warping]]\n*[[Euclidean distance]]\n*[[Sequence homology|Homology of sequences in genetics]]\n*[[Hunt–McIlroy algorithm]]\n*[[Jaccard index]]\n*[[Locality-sensitive hashing]]\n*[[Longest common subsequence problem]]\n*[[Lucene]] (an open source search engine that implements edit distance)\n*[[Manhattan distance]]\n*[[Metric space]]\n*[[MinHash]]\n*[[Most frequent k characters]]\n*[[Optimal matching]] algorithm\n*[[Numerical taxonomy]]\n*[[Sørensen similarity index]]\n{{div col end}}\n\n==References==\n{{reflist|30em}}\n\n==External links==\n{{Wikibooks| Algorithm implementation|Strings/Levenshtein distance|Levenshtein distance}}\n*{{citation |contribution=Levenshtein distance |title=Dictionary of Algorithms and Data Structures [online] |editor-first=Paul E. |editor-last=Black |publisher=U.S. National Institute of Standards and Technology |date=14 August 2008 |accessdate=2 November 2016 |url=https://xlinux.nist.gov/dads/HTML/Levenshtein.html }}\n\n{{strings}}\n\n{{Use dmy dates|date=April 2017}}\n\n{{DEFAULTSORT:Levenshtein Distance}}\n[[Category:String similarity measures]]\n[[Category:Dynamic programming]]\n[[Category:Articles with example pseudocode]]\n[[Category:Quantitative linguistics]]\n[[Category:Similarity and distance measures]]"
    },
    {
      "title": "Line wrap and word wrap",
      "url": "https://en.wikipedia.org/wiki/Line_wrap_and_word_wrap",
      "text": "{{Selfref|For line/word-wrap handling on Wikipedia, see [[Wikipedia:Line-break handling]].}}\n\n{{multiple issues|\n{{external links|date=March 2015}}\n{{lead too short|date=March 2015}}\n}}\n'''Line breaking''', also known as '''word wrapping''', is the process of breaking a section of text into lines such that it will fit in the available width of a page, window or other display area.  In text display, '''line wrap''' is the feature of continuing on a new line when a line is full, such that each line fits in the viewable window, allowing text to be read from top to bottom without any horizontal [[scrolling]]. '''Word wrap''' is the additional feature of most [[text editor]]s, [[word processors]], and [[web browser]]s, of breaking lines between words rather than within words, when possible.  Word wrap makes it unnecessary to hard-code [[newline]] delimiters within [[paragraph]]s, and allows the display of text to adapt flexibly and dynamically to displays of varying sizes.\n\n==Soft and hard returns==\n\nA soft return or soft wrap is the break resulting from line wrap or word wrap (whether automatic or manual), whereas a hard return or hard wrap is an intentional break, creating a new paragraph. With a hard return, paragraph-break formatting can (and should) be applied (either [[Indentation (typesetting)|indenting]] or vertical whitespace). Soft wrapping allows line lengths to adjust automatically with adjustments to the width of the user's window or margin settings, and is a standard feature of all modern text editors, word processors, and [[email client]]s.  Manual soft breaks are unnecessary when word wrap is done automatically, so hitting the \"Enter\" key usually produces a hard return.\n\nAlternatively, \"soft return\" can mean an intentional, stored line break that is not a paragraph break. For example, it is common to print postal addresses in a multiple-line format, but the several lines are understood to be a single paragraph.  Line breaks are needed to divide the words of the address into lines of the appropriate length.\n\nIn the contemporary [[graphical user interface|graphical]] word processors [[Microsoft Word]] and [[OpenOffice.org]], users are expected to type a carriage return ({{Key press|[[Enter key|Enter]]}}) between each paragraph. Formatting settings, such as first-line indentation or spacing between paragraphs, take effect where the carriage return marks the break. A non-paragraph line break, which is a soft return, is inserted using {{Key press|[[shift key|Shift]]|[[Enter key|Enter]]}} or via the menus, and is provided for cases when the text should start on a new line but none of the other side effects of starting a new paragraph are desired.\n\nIn text-oriented markup languages, a soft return is typically offered as a markup tag. For example, in [[HTML]] there is a &lt;br&gt; tag that has the same purpose as the soft return in word processors described above.\n\n===Unicode===\nThe [[Unicode]] Line Breaking Algorithm determines a set of positions, known as ''break opportunities'', that are appropriate places in which to begin a new line. The actual line break positions are picked from among the break opportunities by the higher level software that calls the algorithm, not by the algorithm itself, because only the higher level software knows about the width of the display the text is displayed on and the width of the glyphs that make up the displayed text.<ref name=annex14>{{cite journal|editor1-last=Heninger|editor1-first=Andy|title=Unicode Line Breaking Algorithm|journal=Technical Reports|date=2013-01-25|volume=Annex #14|issue=Proposed Update Unicode Standard|page=2|url=https://www.unicode.org/L2/L2013/13022-uax14-31.pdf|accessdate=10 March 2015|format=PDF|quote=WORD JOINER should be used if the intent is to merely prevent a line break}}</ref> \n\nThe Unicode character set provides a line separator character as well as a paragraph separator to represent the semantics of the soft return and hard return.\n\n:0x2028 LINE SEPARATOR \n:         * may be used to represent this semantic unambiguously\n:0x2029 PARAGRAPH SEPARATOR\n:         * may be used to represent this semantic unambiguously\n\n==Word boundaries, hyphenation, and hard spaces==\n<!-- this section could probably use some example illustrations of text before and after wrapping.  Maybe pull from /RTF Pocket Guide/ -->\nThe soft returns are usually placed after the ends of complete words, or after the punctuation that follows complete words. However, word wrap may also occur following a [[hyphen]] inside of a word.  This is sometimes not desired, and can be blocked by using a [[non-breaking hyphen]], or [[hard hyphen]], instead of a regular hyphen.\n\nA word without hyphens can be made wrappable by having [[soft hyphen]]s in it.  When the word isn't wrapped (i.e., isn't broken across lines), the soft hyphen isn't visible.  But if the word is wrapped across lines, this is done at the soft hyphen, at which point it is shown as a visible hyphen on the top line where the word is broken.  (In the rare case of a word that is meant to be wrappable by breaking it across lines but ''without'' making a hyphen ever appear, a [[zero-width space]] is put at the permitted breaking point(s) in the word.)<!-- example? a URL maybe? those are so long that they often need breaking, but must never have a hyphen introduced into them. -->\n\nSometimes word wrap is undesirable between adjacent words.  In such cases, word wrap can usually be blocked by using a '''hard space''' or '''[[non-breaking space]]''' between the words, instead of regular spaces.\n\n==Word wrapping in text containing Chinese, Japanese, and Korean==\nIn [[Chinese language|Chinese]], [[Japanese language|Japanese]], and [[Korean language|Korean]], word wrapping can usually occur before and after any [[Han character]], but certain punctuation characters are not allowed to begin a new line.<ref>{{citation|title=CJKV Information Processing: Chinese, Japanese, Korean & Vietnamese Computing|first=Ken|last=Lunde|publisher\t=O'Reilly Media, Inc.|year=1999|isbn=9781565922242|page=352|url=https://books.google.com/books?id=Cn7jnk9WwZEC&pg=PA352}}.</ref> Japanese [[kana]], letters of the Japanese alphabet, are treated the same way as Han Characters ([[Kanji]]) by extension, meaning words can, and tend to be broken without any hyphen or other indication that this has happened.\n\nUnder certain circumstances, however, word wrapping is not desired. For instance,\n* word wrapping might not be desired within personal names, and\n* word wrapping might not be desired within any compound words (when the text is flush left but only in some styles).\n\nMost existing word processors and [[typesetting]] software cannot handle either of the above scenarios.\n\n[[CJK]] punctuation may or may not follow rules similar to the above-mentioned special circumstances. It is up to [[Line breaking rules in East Asian language|line breaking rules in CJK]].\n\nA special case of line breaking rules in CJK, however, always applies: line wrap must never occur inside the CJK dash and ellipsis. Even though each of these punctuation marks must be represented by two characters due to a limitation of all existing [[character encoding]]s, each of these are intrinsically a single punctuation mark that is two [[em (typography)|em]]s wide, not two one-em-wide punctuation marks.\n\n==Algorithm==\nWord wrapping is an [[optimization problem]]. Depending on what needs to be optimized for, different algorithms are used.\n\n=== Minimum number of lines ===\nA simple way to do word wrapping is to use a [[greedy algorithm]] that puts as many words on a line as possible, then moving on to the next line to do the same until there are no more words left to place. This method is used by many modern word processors, such as [[OpenOffice.org Writer]] and Microsoft Word{{Citation needed|date=January 2017}}. This algorithm always uses the minimum possible number of lines but may lead to lines of widely varying lengths. The following pseudocode implements this algorithm:\n\n SpaceLeft := LineWidth\n for each Word in Text\n     if (Width(Word) + SpaceWidth) > SpaceLeft\n         insert line break before Word in Text\n         SpaceLeft := LineWidth - Width(Word)\n     else\n         SpaceLeft := SpaceLeft - (Width(Word) + SpaceWidth)\n\nWhere <code>LineWidth</code> is the width of a line, <code>SpaceLeft</code> is the remaining width of space on the line to fill, <code>SpaceWidth</code> is the width of a single space character, <code>Text</code> is the input text to iterate over and <code>Word</code> is a word in this text.\n\n=== Minimum raggedness ===\n\nA different algorithm, used in [[TeX]], minimizes the sum of the squares of the lengths of the spaces at the end of lines to produce a more aesthetically pleasing result. The following example compares this method with the greedy algorithm, which does not always minimize squared space.\n\nFor the input text\n\n AAA BB CC DDDDD\n\nwith line width 6, the greedy algorithm would produce:\n\n ------    Line width: 6\n AAA BB    Remaining space: 0\n CC        Remaining space: 4\n DDDDD     Remaining space: 1\n\nThe sum of squared space left over by this method is <math>0^2 + 4^2 + 1^2 = 17</math>. However, the optimal solution achieves the smaller sum <math>3^2 + 1^2 + 1^2 = 11</math>:\n\n ------    Line width: 6\n AAA       Remaining space: 3\n BB CC     Remaining space: 1\n DDDDD     Remaining space: 1\n\nThe difference here is that the first line is broken before <code>BB</code> instead of after it, yielding a better right margin and a lower cost 11.\n\nBy using a [[dynamic programming]] algorithm to choose the positions at which to break the line, instead of choosing breaks greedily, the solution with minimum raggedness may be found in time <math>O(n^2)</math>, where <math>n</math> is the number of words in the input text. Typically, the cost function for this technique should be modified so that it does not count the space left on the final line of a paragraph; this modification allows a paragraph to end in the middle of a line without penalty. It is also possible to apply the same dynamic programming technique to minimize more complex cost functions that combine other factors such as the number of lines or costs for hyphenating long words.<ref name=\"knuth-plass\">{{citation\n | last1 = Knuth | first1 = Donald E. | author1-link = Donald Knuth\n | last2 = Plass | first2 = Michael F.\n | doi = 10.1002/spe.4380111102\n | issue = 11\n | journal = Software: Practice and Experience\n | pages = 1119–1184\n | title = Breaking paragraphs into lines\n | volume = 11\n | year = 1981}}.</ref> Faster but more complicated [[linear time]] algorithms based on the [[SMAWK algorithm]] are also known for the minimum raggedness problem, and for some other cost functions that have similar properties.<ref>{{citation\n | last = Wilber | first = Robert\n | doi = 10.1016/0196-6774(88)90032-6\n | mr = 955150\n | issue = 3\n | journal = Journal of Algorithms\n | pages = 418–425\n | title = The concave least-weight subsequence problem revisited\n | volume = 9\n | year = 1988}}.</ref><ref>{{citation\n | last1 = Galil | first1 = Zvi | author1-link = Zvi Galil\n | last2 = Park | first2 = Kunsoo\n | doi = 10.1016/0020-0190(90)90215-J\n | mr = 1045521\n | issue = 6\n | journal = Information Processing Letters\n | pages = 309–311\n | title = A linear-time algorithm for concave one-dimensional dynamic programming\n | volume = 33\n | year = 1990}}.</ref>\n\n===History===\nA primitive line-breaking feature was used in 1955 in a \"page printer control unit\" developed by [[Western Union]]. This system used relays rather than programmable digital computers, and therefore needed a simple algorithm that could be implemented without [[data buffer]]s. In the Western Union system, each line was broken at the first space character to appear after the 58th character, or at the 70th character if no space character was found.<ref>{{citation|url=http://massis.lcs.mit.edu/archives/technical/western-union-tech-review/10-1/p040.htm|journal=Western Union Technical Review|volume=10|issue=1|date=January 1956|first=Robert W.|last=Harris|title=Keyboard standardization|pages=37–42}}.</ref>\n\nThe greedy algorithm for line-breaking predates the dynamic programming method outlined by [[Donald Knuth]] in an unpublished 1977 memo describing his TeX typesetting system<ref>{{citation|first=Donald|last=Knuth|authorlink=Donald Knuth|url=https://www.saildart.org/TEXDR.AFT%5B1,DEK%5D|title=TEXDR.AFT|year=1977|accessdate=2013-04-07}}. Reprinted in {{citation|first=Donald|last=Knuth|authorlink=Donald Knuth|title=Digital Typography|location=Stanford, California|publisher=Center for the Study of Language and Information|year=1999|series=CSLI Lecture Notes|volume=78|isbn=1-57586-010-4}}.</ref> and later published in more detail by {{harvtxt|Knuth|Plass|1981}}.\n\n== See also ==\n\n* [[Word divider]]\n* [[Non-breaking space]]\n* [[Zero-width space]]\n* [[Word joiner]]\n\n==References==\n{{reflist}}\n\n== External links ==\n* [https://www.unicode.org/reports/tr14/ Unicode Line Breaking Algorithm]\n\n=== Knuth's algorithm ===\n* [http://defoe.sourceforge.net/folio/knuth-plass.html \"Knuth & Plass line-breaking Revisited\"]\n* [http://oedipus.sourceforge.net/texlib/ \"tex_wrap\": \"Implements TeX's algorithm for breaking paragraphs into lines.\"] Reference: \"Breaking Paragraphs into Lines\", D.E. Knuth and M.F. Plass, chapter 3 of _Digital Typography_, CSLI Lecture Notes #78.\n* [https://metacpan.org/module/Text::Reflow Text::Reflow - Perl module for reflowing text files using Knuth's paragraphing algorithm.] \"The reflow algorithm tries to keep the lines the same length but also tries to break at punctuation, and avoid breaking within a proper name or after certain connectives (\"a\", \"the\", etc.). The result is a file with a more \"ragged\" right margin than is produced by fmt or Text::Wrap but it is easier to read since fewer phrases are broken across line breaks.\"\n* [https://web.archive.org/web/20070930015603/http://www.nabble.com/Initial-soft-hyphen-support-t2970713.html adjusting the Knuth algorithm] to recognize the [[Hyphen#Hyphens_in_computing|\"soft hyphen\"]].\n* [http://wiki.apache.org/xmlgraphics-fop/KnuthsModel Knuth's breaking algorithm.] \"The detailed description of the model and the algorithm can be found on the paper \"Breaking Paragraphs into Lines\" by Donald E. Knuth, published in the book \"Digital Typography\" (Stanford, California: Center for the Study of Language and Information, 1999), (CSLI Lecture Notes, no. 78.)\" ; part of [http://wiki.apache.org/xmlgraphics-fop/GoogleSummerOfCode2006/FloatsImplementationProgress Google Summer Of Code 2006]\n* [http://citeseer.ist.psu.edu/23630.html \"Bridging the Algorithm Gap: A Linear-time Functional Program for Paragraph Formatting\"] by Oege de Moor, Jeremy Gibbons, 1999\n\n=== Other word-wrap links ===\n* [http://www.codecomments.com/message230162.html the reverse problem -- picking columns just wide enough to fit (wrapped) text]  ([https://archive.is/20070927021648/http://www.codecomments.com/message230162.html Archived version])\n* [http://api.kde.org/4.x-api/kdelibs-apidocs/kdeui/html/classKWordWrap.html KWordWrap Class Reference] used in the KDE GUI\n* [http://www.leverkruid.eu/GKPLinebreaking/elements.html \"Knuth linebreaking elements for Formatting Objects\"] by Simon Pepping 2006. Extends the Knuth model to handle a few enhancements.\n* [http://wiki.apache.org/xmlgraphics-fop/PageLayout/ \"Page breaking strategies\"] Extends the Knuth model to handle a few enhancements.\n* [http://www.techwr-l.com/archives/0504/techwhirl-0504-00203.html \"a Knuth-Plass-like linebreaking algorithm] ... The *really* interesting thing is how Adobe's algorithm differs from the Knuth-Plass algorithm. It must differ, since Adobe has managed to patent its algorithm (6,510,441).\"[http://www.techwr-l.com/archives/0504/techwhirl-0504-00206.html ]\n* [http://blogs.msdn.com/murrays/archive/2006/11/15/lineservices.aspx \"Murray Sargent: Math in Office\"]\n* [http://xxyxyz.org/line-breaking/ \"Line breaking\"] compares the algorithms of various time complexities.\n\n\n[[Category:Text editor features]]\n[[Category:Typography]]\n[[Category:Dynamic programming]]\n[[Category:Unicode algorithms]]"
    },
    {
      "title": "Longest alternating subsequence",
      "url": "https://en.wikipedia.org/wiki/Longest_alternating_subsequence",
      "text": "In [[combinatorics|combinatorial]] mathematics, [[probability]], and [[computer science]], in the '''longest alternating subsequence''' problem, one wants to find a subsequence of a given [[sequence]] in which the elements are in alternating order, and in which the sequence is as long as possible.\n\nFormally, if <math>\\mathbf{x} = \\{x_1, x_2, \\ldots, x_n\\}</math> is a sequence of distinct real numbers, then the subsequence <math>\\{x_{i_1}, x_{i_2}, \\ldots, x_{i_k}\\}</math> is ''alternating''<ref\nname=\"Stanleybook\">{{citation\n|first=Richard P.\n|last=Stanley\n|author-link=Richard P. Stanley\n|title=Enumerative Combinatorics, Volume I, second edition\n|publisher=Cambridge University Press\n|year=2011}}</ref> (or ''zigzag'' or ''down-up'')if\n\n:<math>x_{i_1} > x_{i_2} < x_{i_3} > \\cdots  x_{i_k}\\qquad \\text{and} \\qquad 1\\leq i_1 < i_2 < \\cdots < i_k \\leq n.</math>\n\nSimilarly, <math>\\mathbf{x}</math> is ''reverse alternating'' (or ''up-down'') if \n:<math>x_{i_1} < x_{i_2} > x_{i_3} < \\cdots  x_{i_k}\\qquad \\text{and} \\qquad 1\\leq i_1 < i_2 < \\cdots < i_k \\leq n.</math>\n\nLet <math>{\\rm as}_n(\\mathbf{x})</math> denote the length (number of terms) of the longest alternating subsequence of <math>\\mathbf{x}</math>. For example, if we consider some of the permutations of the integers 1,2,3,4,5, we have that\n* <math>{\\rm as}_5(1,2,3,4,5) = 2 </math>; because any sequence of 2 distinct digits are (by definition) alternating. (for example 1,2 or 1,4 or 3,5)\n* <math>{\\rm as}_5(1,5,3,2,4) = 4, </math> because 1,5,3,4 and 1,5,2,4 and 1,3,2,4 are all alternating, and there is no alternating subsequence with more elements;\n* <math>{\\rm as}_5(5,3,4,1,2) = 5, </math> because 5,3,4,1,2 is itself alternating.\n\n== Efficient algorithms ==\n\nThe longest alternating subsequence problem is solvable in time <math>O(n)</math>, where <math>n</math> is the length of the original sequence.{{Citation needed|date=October 2016}}\n\n== Distributional results ==\n\nIf <math>\\mathbf{x}</math> is a random permutation of the integers <math>1,2,\\ldots,n</math> and <math>A_n \\equiv {\\rm as}_n(\\mathbf{x})</math>, then it is possible to show<ref\nname=\"widom\">{{citation\n | url = http://www.combinatorics.org/Volume_13/Abstracts/v13i1r25.html\n | last1 = Widom | first1 = Harold\n | journal = Electron. J. Combin.\n | pages = Research Paper 25, 7\n | title = On the limiting distribution for the length of the longest alternating sequence in a random permutation\n | volume = 13\n | year = 2006}}</ref><ref\nname=\"stanley\">{{citation\n | doi = 10.1307/mmj/1220879431\n | last1 = Stanley | first1 = Richard P. | author1-link = Richard_P._Stanley\n | journal = Michigan Math. J.\n | pages = 675–687\n | title = Longest alternating subsequences of permutations\n | volume = 57\n | year = 2008| arxiv = math/0511419}}</ref><ref\nname=\"hr\">{{citation\n | url = http://www.combinatorics.org/Volume_17/Abstracts/v17i1r168.html\n | last1 = Houdré | first1 = Christian\n | last2 = Restrepo | first2 = Ricardo\n | journal = Electron. J. Combin.\n | title = A probabilistic approach to the asymptotics of the length of the longest alternating subsequence\n | volume = 17\n | pages = Research Paper 168, 19\n | year = 2010}}</ref>\nthat\n\n:<math> E[A_n] = \\frac{2 n}{3} + \\frac{1}{6}  \\qquad \\text{and} \\qquad \\operatorname{Var}[A_n] = \\frac{8 n}{45} - \\frac{13}{180}.  </math>\n\nMoreover, as <math>n \\rightarrow \\infty</math>, the random variable <math>A_n</math>, appropriately centered and scaled, converges to a standard normal distribution.\n\n== Online algorithms ==\n\nThe longest alternating subsequence problem has also been studied in the setting of [[online algorithm]]s, in which the elements of <math>\\mathbf{x}</math> are presented in an online fashion, and a decision maker needs to decide whether to include or exclude each element at the time it is first presented, without any knowledge of the elements that will be presented in the future,\nand without the possibility of recalling on preceding observations.\n\nGiven a sequence <math>X_1, X_2, \\ldots, X_n</math> of independent random variables with common continuous distribution <math>F</math>, it is possible to construct a selection procedure that maximizes the expected number of alternating selections. \nSuch expected values can be tightly estimated, and it equals <math>(2-\\sqrt{2})n + O(1)</math>.<ref name=\"acss2011\">{{citation\n | doi = 10.1239/jap/1324046022\n | last1 = Arlotto | first1 = Alessandro\n | last2 = Chen | first2 = Robert W.\n | last3 = Shepp | first3 = Lawrence A. | author3-link = Lawrence_Shepp\n | last4 = Steele | first4 = J. Michael | author4-link = J._Michael_Steele\n | journal = J. Appl. Probab.\n | pages = 1114–1132\n | title = Online selection of alternating subsequences from a random sample\n | volume = 48\n | issue = 4\n | year = 2011| arxiv = 1105.1558}}</ref>\n\nAs <math>n \\rightarrow \\infty</math>, the optimal number of online alternating selections appropriately centered and scaled converges to a normal distribution.<ref name=\"as2014\">{{citation\n | doi = 10.1239/aap/1401369706\n | last1 = Arlotto | first1 = Alessandro\n | last2 = Steele | first2 = J. Michael | author2-link = J._Michael_Steele\n | journal = Adv. Appl. Probab.\n | pages = 536–559\n | title = Optimal online selection of an alternating subsequence: a central limit theorem\n | volume = 46\n | issue = 2\n | year = 2014}}</ref>\n\n== See also ==\n\n* [[Alternating permutation]]\n* [[Permutation pattern]] and pattern avoidance\n* Counting local maxima and/or local minima in a given sequence\n* Turning point tests for testing statistical independence of <math>n</math> observations\n* Number of alternating runs\n* [[Longest increasing subsequence]]\n* [[Longest common subsequence problem]]\n\n== References ==\n\n{{reflist}}\n\n*\n*\n*\n*\n\n[[Category:Problems on strings]]\n[[Category:Permutations]]\n[[Category:Combinatorics]]\n[[Category:Dynamic programming]]"
    },
    {
      "title": "Longest common subsequence problem",
      "url": "https://en.wikipedia.org/wiki/Longest_common_subsequence_problem",
      "text": "{{Distinguish|longest common substring problem}}\nThe '''longest common subsequence''' ('''LCS''') '''problem''' is the problem of finding the longest [[subsequence]] common to all sequences in a set of sequences (often just two sequences). It differs from the [[longest common substring problem]]: unlike substrings, subsequences are not required to occupy consecutive positions within the original sequences. The longest common subsequence problem is a classic [[computer science]] problem, the basis of [[data comparison]] programs such as the [[diff utility|<tt>diff</tt> utility]], and has applications in [[computational linguistics]] and [[bioinformatics]]. It is also widely used by [[Revision control|revision control systems]] such as [[Git (software)|Git]] for [[Merge (revision control)|reconciling]] multiple changes made to a revision-controlled collection of files. \n<!-- todo: add definition and example -->\n\n== Complexity ==\nFor the general case of an arbitrary number of input sequences, the problem is [[NP-hard]].<ref>{{cite journal| author = David Maier| title = The Complexity of Some Problems on Subsequences and Supersequences| journal = J. ACM| volume = 25| year = 1978| pages = 322&ndash;336| doi = 10.1145/322063.322075| publisher = ACM Press| issue = 2}}</ref> When the number of sequences is constant, the problem is solvable in polynomial time by [[dynamic programming]] (see ''Solution'' below). Assume you have <math>N</math> sequences of lengths <math>n_1, ..., n_N</math>. A naive search would test each of the <math>2^{n_1}</math> subsequences of the first sequence to determine whether they are also subsequences of the remaining sequences; each subsequence may be tested in time linear in the lengths of the remaining sequences, so the time for this algorithm would be\n:<math>O\\left( 2^{n_1} \\sum_{i>1} n_i\\right).</math>\n\nFor the case of two sequences of ''n'' and ''m'' elements, the running time of the dynamic programming approach is [[Big O notation|O]](''n'' × ''m'').<ref>{{cite journal |last1=Wagner |first1=Robert |last2=Fischer |first2=Michael |date=January 1974 |title=The string-to-string correction problem |url=http://www.inrg.csie.ntu.edu.tw/algorithm2014/homework/Wagner-74.pdf |journal=[[Journal of the ACM]] |volume=21 |issue=1 |pages=168-173 |doi=10.1145/321796.321811 |access-date=2018-05-03 }}</ref> For an arbitrary number of input sequences, the dynamic programming approach gives a solution in\n\n:<math>O\\left(N \\prod_{i=1}^{N} n_i\\right).</math>\n\nThere exist methods with lower complexity,<ref name=\"BHR00\">\n{{cite journal | author = L. Bergroth and H. Hakonen and T. Raita | title = A Survey of Longest Common Subsequence Algorithms | journal = SPIRE | volume = 00 | year = 2000 | isbn = 0-7695-0746-8 | pages = 39&ndash;48 | doi = 10.1109/SPIRE.2000.878178 | publisher = IEEE Computer Society}}</ref>\nwhich often depend on the length of the LCS, the size of the alphabet, or both.\n\nNotice that the LCS is not necessarily unique; for example the LCS of \"ABC\" and \"ACB\" is both \"AB\" and \"AC\". Indeed, the LCS problem is often defined to be finding ''all'' common subsequences of a maximum length. This problem inherently has higher complexity, as the number of such subsequences is exponential in the worst case,<ref name=\"\">{{cite arXiv | author = Ronald I. Greenberg | title = Bounds on the Number of Longest Common Subsequences  | date = 2003-08-06  | eprint = cs.DM/0301030}}</ref> even for only two input strings.\n\n== Solution for two sequences ==\nThe LCS problem has an [[optimal substructure]]:  the problem can be broken down into smaller, simple \"subproblems\", which can be broken down into yet simpler subproblems, and so on, until, finally, the solution becomes trivial.  The LCS problem also has [[overlapping subproblems]]:  the solution to high-level subproblems often reuse lower level subproblems.  Problems with these two properties—optimal substructure and overlapping subproblems—can be approached by a problem-solving technique called [[dynamic programming]], in which subproblem solutions are memoized rather than computed over and over.  The procedure requires [[memoization]]—saving the solutions to one level of subproblem in a table (analogous to writing them to a ''memo'', hence the name) so that the solutions are available to the next level of subproblems.\nThis method is illustrated here.\n\n=== Prefixes ===\nThe subproblems become simpler as the sequences become shorter.  Shorter sequences are conveniently described using the term ''prefix''.  A prefix of a sequence is the sequence with the end cut off.  Let ''S'' be the sequence (AGCA).  Then, the sequence (AG) is one of the prefixes of ''S''.  Prefixes are denoted with the name of the sequence, followed by a subscript to indicate how many characters the prefix contains.<ref>{{cite book\n | last = Xia | first = Xuhua\n | title = Bioinformatics and the Cell:  Modern Computational Approaches in Genomics, Proteomics and Transcriptomics\n | year = 2007\n | publisher = Springer\n | location = New York\n | page = 24\n | isbn = 0-387-71336-0\n}}</ref>  The prefix (AG) is denoted ''S''<sub>2</sub>, since it contains the first 2 elements of ''S''.  The possible prefixes of ''S'' are\n:''S''<sub>1</sub> = (A)\n:''S''<sub>2</sub> = (AG)\n:''S''<sub>3</sub> = (AGC)\n:''S''<sub>4</sub> = (AGCA).\n\nThe solution to the LCS problem for two arbitrary sequences, ''X'' and ''Y'', amounts to constructing some function, ''LCS''(''X'', ''Y''), that gives the longest subsequences common to ''X'' and ''Y''.  That function relies on the following two properties.\n\n=== First property ===\nSuppose that two sequences both end in the same element.  To find their LCS, shorten each sequence by removing the last element, find the LCS of the shortened sequences, and to that LCS append the removed element.\n:For example, here are two sequences having the same last element:  (BANANA) and (ATANA).\n:Remove the same last element. Repeat the procedure until you find no common last element. The removed sequence will be (ANA).\n:The sequences now under consideration:  (BAN) and (AT)\n:The LCS of these last two sequences is, by inspection, (A).\n:Append the removed element, (ANA), giving (AANA), which, by inspection, is the LCS of the original sequences.\n\nIn general, for any sequences ''X'' and ''Y'' of length ''n'' and ''m'', if we denote their elements ''x<sub>1</sub>'' to ''x<sub>n</sub>'' and ''y<sub>1</sub>'' to ''y<sub>m</sub>'' and their prefixes ''X<sub>1</sub>'' to ''X<sub>n-1</sub>'' and ''Y<sub>1</sub>'' to ''Y<sub>m-1</sub>'', then we can say this:\n: If:  ''x<sub>n</sub>''=''y<sub>m</sub>''\n: then: ''LCS''(''X<sub>n</sub>'', ''Y<sub>m</sub>'') = ''LCS''( ''X<sub>n-1</sub>'', ''Y<sub>m-1</sub>'') ^ ''x<sub>n</sub>'' \nwhere the caret ^ indicates that the following element, ''x<sub>n</sub>'', is appended to the sequence.  Note that the LCS for ''X<sub>n</sub>'' and ''Y<sub>m</sub>'' involves determining the LCS of the shorter sequences, ''X<sub>n-1</sub>'' and ''Y<sub>m-1</sub>''.\n\n=== Second property ===\nSuppose that the two sequences X and Y do not end in the same symbol.\nThen the LCS of X and Y is the longer of the two sequences LCS(X<sub>n</sub>,Y<sub>m-1</sub>) and LCS(X<sub>n-1</sub>,Y<sub>m</sub>).\n\nTo understand this property, consider the two following sequences :\n\nsequence X: ABCDEFG (n elements)<br />\nsequence Y: BCDGK (m elements)\n\nThe LCS of these two sequences either ends with a G (the last element of sequence X) or does not.\n\n'''Case 1: the LCS ends with a G'''<br />\nThen it cannot end with a K. Thus it does not hurt to remove the K from sequence Y: if K were in the LCS, it would be its last character; as a consequence K is not in the LCS. We can then write: LCS(X<sub>n</sub>,Y<sub>m</sub>) = LCS(X<sub>n</sub>, Y<sub>m-1</sub>).\n\n'''Case 2: the LCS does not end with a G'''<br />\nThen it does not hurt to remove the G from the sequence X (for the same reason as above). And then we can write: LCS(X<sub>n</sub>,Y<sub>m</sub>) = LCS(X<sub>n-1</sub>, Y<sub>m</sub>).\n\nIn any case, the LCS we are looking for is one of LCS(X<sub>n</sub>, Y<sub>m-1</sub>) or LCS(X<sub>n-1</sub>, Y<sub>m</sub>). Those two last LCS are both common subsequences to X and Y. LCS(X,Y) is the longest. Thus its value is the longest sequence of LCS(X<sub>n</sub>, Y<sub>m-1</sub>) and LCS(X<sub>n-1</sub>, Y<sub>m</sub>).\n\n=== ''LCS'' function defined ===\nLet two sequences be defined as follows:  <math>X=(x_1 x_2 \\cdots x_m)</math> and <math>Y=(y_1 y_2 \\cdots y_n)</math>.  The prefixes of <math>X</math> are <math>X_{1,2,\\dots,m}</math>; the prefixes of <math>Y</math> are <math>Y_{1,2,\\dots,n}</math>.  Let <math>\\mathit{LCS}(X_i,Y_j)</math> represent the set of longest common subsequence of prefixes <math>X_i</math> and <math>Y_j</math>.  This set of sequences is given by the following.\n\n:<math>\n\\mathit{LCS}(X_i,Y_j)=\\begin{cases}\n  \\empty & \\mbox{if }i=0\\mbox{ or }j=0 \\\\\n  \\mathit{LCS}(X_{i-1},Y_{j-1}) \\hat{} x_i & \\mbox{if }i,j>0\\mbox{ and }x_i=y_j \\\\\n  \\operatorname{\\max}\\{\\mathit{LCS}(X_i,Y_{j-1}),\\mathit{LCS}(X_{i-1},Y_j)\\} & \\mbox{if }i,j>0\\mbox{ and }x_i\\ne y_j.\n\\end{cases}\n</math>\n\nTo find the longest subsequences common to <math>X_i</math> and <math>Y_j</math>, compare the elements <math>x_i</math> and <math>y_j</math>.  If they are equal, then the sequence <math>\\mathit{LCS}(X_{i-1},Y_{j-1})</math> is extended by that element, <math>x_i</math>.  If they are not equal, then the longer of the two sequences, <math>\\mathit{LCS}(X_i,Y_{j-1})</math>, and <math>\\mathit{LCS}(X_{i-1},Y_j)</math>, is retained.  (If they are both the same length, but not identical, then both are retained.)  Notice that the subscripts are reduced by 1 in these formulas.  That can result in a subscript of 0.  Since the sequence elements are defined to start at 1, it was necessary to add the requirement that the LCS is empty when a subscript is zero.\n\n=== Worked example ===\nThe longest subsequence common to ''R'' = (GAC), and ''C'' = (AGCAT) will be found.  Because the ''LCS'' function uses a \"zeroth\" element, it is convenient to define zero prefixes that are empty for these sequences:  ''R''<sub>0</sub> =  Ø; and ''C''<sub>0</sub> =  Ø.  All the prefixes are placed in a table with ''C'' in the first row (making it a <u>c</u>olumn header) and ''R'' in the first column (making it a <u>r</u>ow header).\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|+ LCS Strings\n|-\n!   || Ø || A || G || C || A || T\n|-\n! Ø\n| Ø || Ø || Ø || Ø || Ø || Ø\n|-\n! G\n| Ø\n| \n| \n| \n| \n| \n|-\n! A\n| Ø\n| \n| \n| \n| \n| \n|-\n! C\n| Ø\n| \n| \n| \n| \n| \n|-\n|}\n\nThis table is used to store the LCS sequence for each step of the calculation.  The second column and second row have been filled in with Ø, because when an empty sequence is compared with a non-empty sequence, the longest common subsequence is always an empty sequence.\n\n''LCS''(''R''<sub>1</sub>, ''C''<sub>1</sub>) is determined by comparing the first elements in each sequence.  G and A are not the same, so this LCS gets (using the \"second property\") the longest of the two sequences, ''LCS''(''R''<sub>1</sub>, ''C''<sub>0</sub>) and  ''LCS''(''R''<sub>0</sub>, ''C''<sub>1</sub>).  According to the table, both of these are empty, so ''LCS''(''R''<sub>1</sub>, ''C''<sub>1</sub>) is also empty, as shown in the table below.  The arrows indicate that the sequence comes from both the cell above, ''LCS''(''R''<sub>0</sub>, ''C''<sub>1</sub>) and the cell on the left, ''LCS''(''R''<sub>1</sub>, ''C''<sub>0</sub>).\n\n''LCS''(''R''<sub>1</sub>, ''C''<sub>2</sub>) is determined by comparing G and G.  They match, so G is appended to the upper left sequence, ''LCS''(''R''<sub>0</sub>, ''C''<sub>1</sub>), which is (Ø), giving (ØG), which is (G).\n\nFor ''LCS''(''R''<sub>1</sub>, ''C''<sub>3</sub>), G and C do not match.  The sequence above is empty; the one to the left contains one element, G.  Selecting the longest of these, ''LCS''(''R''<sub>1</sub>, ''C''<sub>3</sub>) is (G).  The arrow points to the left, since that is the longest of the two sequences.\n\n''LCS''(''R''<sub>1</sub>, ''C''<sub>4</sub>), likewise, is (G).\n\n''LCS''(''R''<sub>1</sub>, ''C''<sub>5</sub>), likewise, is (G).\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|+ \"G\" Row Completed\n|-\n!   || Ø || A || G || C || A || T\n|-\n! Ø\n| Ø || Ø || Ø || Ø || Ø || Ø\n|-\n! G\n| Ø\n| <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>Ø\n| <math>\\overset{\\nwarrow}{\\ }</math>(G)\n| <math>\\overset{\\ }{\\leftarrow}</math>(G)\n| <math>\\overset{\\ }{\\leftarrow}</math>(G)\n| <math>\\overset{\\ }{\\leftarrow}</math>(G)\n|-\n! A\n| Ø\n| \n| \n| \n| \n| \n|-\n! C\n| Ø\n| \n| \n| \n| \n| \n|-\n|}\n\nFor ''LCS''(''R''<sub>2</sub>, ''C''<sub>1</sub>), A is compared with A.  The two elements match, so A is appended to Ø, giving (A).\n\nFor ''LCS''(''R''<sub>2</sub>, ''C''<sub>2</sub>), A and G do not match, so the longest of ''LCS''(''R''<sub>1</sub>, ''C''<sub>2</sub>), which is (G), and ''LCS''(''R''<sub>2</sub>, ''C''<sub>1</sub>), which is (A), is used.  In this case, they each contain one element, so this LCS is given two subsequences:  (A) and (G).\n\nFor ''LCS''(''R''<sub>2</sub>, ''C''<sub>3</sub>), A does not match C.  ''LCS''(''R''<sub>2</sub>, ''C''<sub>2</sub>) contains sequences (A) and (G); LCS(''R''<sub>1</sub>, ''C''<sub>3</sub>) is (G), which is already contained in ''LCS''(''R''<sub>2</sub>, ''C''<sub>2</sub>).  The result is that ''LCS''(''R''<sub>2</sub>, ''C''<sub>3</sub>) also contains the two subsequences, (A) and (G).\n\nFor ''LCS''(''R''<sub>2</sub>, ''C''<sub>4</sub>), A matches A, which is appended to the upper left cell, giving (GA).\n\nFor ''LCS''(''R''<sub>2</sub>, ''C''<sub>5</sub>), A does not match T.  Comparing the two sequences, (GA) and (G), the longest is (GA), so ''LCS''(''R''<sub>2</sub>, ''C''<sub>5</sub>) is (GA).\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|+ \"G\" & \"A\" Rows Completed\n|-\n!   || Ø || A || G || C || A || T\n|-\n! Ø\n| Ø || Ø || Ø || Ø || Ø || Ø\n|-\n! G\n| Ø\n| <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>Ø\n| <math>\\overset{\\nwarrow}{\\ }</math>(G)\n| <math>\\overset{\\ }{\\leftarrow}</math>(G)\n| <math>\\overset{\\ }{\\leftarrow}</math>(G)\n| <math>\\overset{\\ }{\\leftarrow}</math>(G)\n|-\n! A\n| Ø\n| <math>\\overset{\\nwarrow}{\\ }</math>(A)\n| <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>(A) & (G)\n| <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>(A) & (G)\n| <math>\\overset{\\nwarrow}{\\ }</math>(GA)\n| <math>\\overset{\\ }{\\leftarrow}</math>(GA)\n|-\n! C\n| Ø\n| \n| \n| \n| \n| \n|-\n|}\n\nFor ''LCS''(''R''<sub>3</sub>, ''C''<sub>1</sub>), C and A do not match, so ''LCS''(''R''<sub>3</sub>, ''C''<sub>1</sub>) gets the longest of the two sequences, (A).\n\nFor ''LCS''(''R''<sub>3</sub>, ''C''<sub>2</sub>), C and G do not match.  Both ''LCS''(''R''<sub>3</sub>, ''C''<sub>1</sub>) and ''LCS''(''R''<sub>2</sub>, ''C''<sub>2</sub>) have one element.  The result is that ''LCS''(''R''<sub>3</sub>, ''C''<sub>2</sub>) contains the two subsequences, (A) and (G).\n\nFor ''LCS''(''R''<sub>3</sub>, ''C''<sub>3</sub>), C and C match, so C is appended to ''LCS''(''R''<sub>2</sub>, ''C''<sub>2</sub>), which contains the two subsequences, (A) and (G), giving (AC) and (GC).\n\nFor ''LCS''(''R''<sub>3</sub>, ''C''<sub>4</sub>), C and A do not match.  Combining ''LCS''(''R''<sub>3</sub>, ''C''<sub>3</sub>), which contains (AC) and (GC), and ''LCS''(''R''<sub>2</sub>, ''C''<sub>4</sub>), which contains (GA), gives a total of three sequences:  (AC), (GC), and (GA).\n\nFinally, for ''LCS''(''R''<sub>3</sub>, ''C''<sub>5</sub>), C and T do not match.  The result is that ''LCS''(''R''<sub>3</sub>, ''C''<sub>5</sub>) also contains the three sequences, (AC), (GC), and (GA).\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|+ Completed LCS Table\n|-\n!   || Ø || A || G || C || A || T\n|-\n! Ø\n| Ø || Ø || Ø || Ø || Ø || Ø\n|-\n! G\n| Ø\n| <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>Ø\n| <math>\\overset{\\nwarrow}{\\ }</math>(G)\n| <math>\\overset{\\ }{\\leftarrow}</math>(G)\n| <math>\\overset{\\ }{\\leftarrow}</math>(G)\n| <math>\\overset{\\ }{\\leftarrow}</math>(G)\n|-\n! A\n| Ø\n| <math>\\overset{\\nwarrow}{\\ }</math>(A)\n| <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>(A) & (G)\n| <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>(A) & (G)\n| <math>\\overset{\\nwarrow}{\\ }</math>(GA)\n| <math>\\overset{\\ }{\\leftarrow}</math>(GA)\n|-\n! C\n| Ø\n| <math>\\overset{\\ \\uparrow}{\\ }</math>(A)\n| <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>(A) & (G)\n| <math>\\overset{\\nwarrow}{\\ }</math>(AC) & (GC)\n| <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>(AC) & (GC) & (GA)\n| <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>(AC) & (GC) & (GA)\n|-\n|}\n\nThe final result is that the last cell contains all the longest subsequences common to (AGCAT) and (GAC); these are (AC), (GC), and (GA).  The table also shows the longest common subsequences for every possible pair of prefixes.  For example, for (AGC) and (GA), the longest common subsequence are (A) and (G).\n\n=== Traceback approach ===\nCalculating the LCS of a row of the LCS table requires only the solutions to the current row and the previous row.  Still, for long sequences, these sequences can get numerous and long, requiring a lot of storage space.  Storage space can be saved by saving not the actual subsequences, but the length of the subsequence and the direction of the arrows, as in the table below.\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|+ Storing length, rather than sequences\n|-\n!   || Ø || A || G || C || A || T\n|-\n! Ø\n| 0 || 0 || 0 || 0 || 0 || 0\n|-\n! G\n| 0\n| <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>0\n| <math>\\overset{\\nwarrow}{\\ }</math>1\n| <math>\\overset{\\ }{\\leftarrow}</math>1\n| <math>\\overset{\\ }{\\leftarrow}</math>1\n| <math>\\overset{\\ }{\\leftarrow}</math>1\n|-\n! A\n| 0\n| <math>\\overset{\\nwarrow}{\\ }</math>1\n| <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>1\n| <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>1\n| <math>\\overset{\\nwarrow}{\\ }</math>2\n| <math>\\overset{\\ }{\\leftarrow}</math>2\n|-\n! C\n| 0\n| <math>\\overset{\\ \\uparrow}{\\ }</math>1\n| <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>1\n| <math>\\overset{\\nwarrow}{\\ }</math>2\n| <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>2\n| <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>2\n|-\n|}\n\nThe actual subsequences are deduced in a \"traceback\" procedure that follows the arrows backwards, starting from the last cell in the table.  When the length decreases, the sequences must have had a common element.  Several paths are possible when two arrows are shown in a cell.  Below is the table for such an analysis, with numbers colored in cells where the length is about to decrease.  The bold numbers trace out the sequence, (GA).<ref>{{cite book | author = [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]] and [[Clifford Stein]] | title = [[Introduction to Algorithms]] | publisher = MIT Press and McGraw-Hill | year = 2001 | isbn = 0-262-53196-8 | edition = 2nd | chapter = 15.4 | pages = 350–355 }}</ref>\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|+ Traceback example\n|-\n!   || Ø || A || G || C || A || T\n|-\n! Ø\n| 0 || style=\"background:silver\" | '''0''' || 0 || 0 || 0 || 0\n|-\n! G\n| style=\"background:silver\" | 0\n| <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>0\n| style=\"background:silver;color:#FF6600\" | <math>\\overset{\\nwarrow}{\\ }</math>'''1'''\n| style=\"background:silver\" | <math>\\overset{\\ }{\\leftarrow}</math>'''1'''\n| <math>\\overset{\\ }{\\leftarrow}</math>1\n| <math>\\overset{\\ }{\\leftarrow}</math>1\n|-\n! A\n| 0\n| style=\"background:silver;color:#FF6600\" | <math>\\overset{\\nwarrow}{\\ }</math>1\n| style=\"background:silver\" | <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>1\n| <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>1\n| style=\"background:silver;color:#FF6600\" | <math>\\overset{\\nwarrow}{\\ }</math>'''2'''\n| style=\"background:silver\" | <math>\\overset{\\ }{\\leftarrow}</math>'''2'''\n|-\n! C\n| 0\n| <math>\\overset{\\ \\uparrow}{\\ }</math>1\n| <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>1\n| style=\"background:silver;color:#FF6600\" | <math>\\overset{\\nwarrow}{\\ }</math>2\n| style=\"background:silver\" | <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>2\n| style=\"background:silver\" | <math>\\overset{\\ \\ \\uparrow}{\\leftarrow}</math>'''2'''\n|-\n|}\n\n== Relation to other problems ==\nFor two strings <math>X_{1 \\dots m}</math> and <math>Y_{1 \\dots n}</math>, the length of the [[shortest common supersequence problem|shortest common supersequence]] is related to the length of the LCS by<ref name=\"BHR00\" />\n\n:<math>\\left|SCS(X,Y)\\right| = n + m - \\left|LCS(X,Y)\\right|.</math>\n\nThe [[edit distance]] when only insertion and deletion is allowed (no substitution), or when the cost of the substitution is the double of the cost of an insertion or  deletion, is:\n\n:<math>d'(X,Y) = n + m - 2 \\cdot \\left|LCS(X,Y)\\right|.</math>\n\n== Code for the dynamic programming solution ==\n{{unreferenced section|date=March 2013}}\n\n=== Computing the length of the LCS ===\nThe function below takes as input sequences <code>X[1..m]</code> and <code>Y[1..n]</code>, computes the LCS between <code>X[1..i]</code> and <code>Y[1..j]</code> for all <code>1 ≤ i ≤ m</code> and <code>1 ≤ j ≤ n</code>, and stores it in <code>C[i,j]</code>. <code>C[m,n]</code> will contain the length of the LCS of <code>X</code> and <code>Y</code>.\n\n '''function''' LCSLength(X[1..m], Y[1..n])\n     C = array(0..m, 0..n)\n     '''for''' i := 0..m\n        C[i,0] = 0\n     '''for''' j := 0..n\n        C[0,j] = 0\n     '''for''' i := 1..m\n         '''for''' j := 1..n\n             '''if''' X[i] = Y[j]\n                 C[i,j] := C[i-1,j-1] + 1\n             '''else'''\n                 C[i,j] := max(C[i,j-1], C[i-1,j])\n     '''return''' C[m,n]\n\nAlternatively, [[memoization]] could be used.\n\n=== Reading out a LCS ===\nThe following function [[backtracking|backtracks]] the choices taken when computing the <code>C</code> table. If the last characters in the prefixes are equal, they must be in an LCS. If not, check what gave the largest LCS of keeping <math>x_i</math> and <math>y_j</math>, and make the same choice. Just choose one if they were equally long. Call the function with <code>i=m</code> and <code>j=n</code>.\n\n '''function''' backtrack(C[0..m,0..n], X[1..m], Y[1..n], i, j)\n     '''if''' i = 0 '''or''' j = 0\n         '''return''' \"\"\n     '''if ''' X[i] = Y[j]\n         '''return''' backtrack(C, X, Y, i-1, j-1) + X[i]\n     '''if''' C[i,j-1] > C[i-1,j]\n         '''return''' backtrack(C, X, Y, i, j-1)\n     '''return''' backtrack(C, X, Y, i-1, j)\n\n=== Reading out all LCSs ===\nIf choosing <math>x_i</math> and <math>y_j</math> would give an equally long result, read out both resulting subsequences. This is returned as a set by this function. Notice that this function is not polynomial, as it might branch in almost every step if the strings are similar.\n\n '''function''' backtrackAll(C[0..m,0..n], X[1..m], Y[1..n], i, j)\n     '''if''' i = 0 '''or''' j = 0\n         '''return''' {\"\"}\n     '''if''' X[i] = Y[j]\n         '''return''' {Z + X[i] '''for all''' Z '''in''' backtrackAll(C, X, Y, i-1, j-1)}\n     '''R := {}\n     '''if''' C[i,j-1] ≥ C[i-1,j]\n         R := R ∪ backtrackAll(C, X, Y, i, j-1)\n     '''if''' C[i-1,j] ≥ C[i,j-1]\n         R := R ∪ backtrackAll(C, X, Y, i-1, j)\n     '''return''' R\n\n=== Print the diff ===\nThis function will backtrack through the C matrix, and print the [[diff]] between the two sequences. Notice that you will get a different answer if you exchange <code>≥</code> and <code>&lt;</code>, with <code>&gt;</code> and <code>≤</code> below.\n\n '''function''' printDiff(C[0..m,0..n], X[1..m], Y[1..n], i, j)\n     '''if''' i > 0 '''and''' j > 0 '''and''' X[i] = Y[j]\n         printDiff(C, X, Y, i-1, j-1)\n         print \"  \" + X[i]\n     '''else if''' j > 0 '''and''' (i = 0 '''or''' C[i,j-1] ≥ C[i-1,j])\n         printDiff(C, X, Y, i, j-1)\n         print \"+ \" + Y[j]\n     '''else if''' i > 0 '''and''' (j = 0 '''or''' C[i,j-1] < C[i-1,j])\n         printDiff(C, X, Y, i-1, j)\n         print \"- \" + X[i]\n     '''else'''\n         print \"\"\n\n=== Example ===\nLet <math>X</math> be “<code>XMJYAUZ</code>” and <math>Y</math> be “<code>MZJAWXU</code>”. The longest common subsequence between <math>X</math> and <math>Y</math> is “<code>MJAU</code>”. The table <code>C</code> shown below, which is generated by the function <code>LCSLength</code>, shows the lengths of the longest common subsequences between prefixes of <math>X</math> and <math>Y</math>. The <math>i</math>th row and <math>j</math>th column shows the length of the LCS between <math>X_{1..i}</math> and <math>Y_{1..j}</math>.\n\n{| class=\"wikitable\" style=\"text-align: center;\"\n|-\n! colspan=\"2\" rowspan=\"2\" |\n! 0 !! 1 !! 2 !! 3 !! 4 !! 5 !! 6 !! 7\n|-\n! Ø !! M !! Z !! J !! A !! W !! X !! U\n|-\n! 0 !! Ø\n| style=\"background:yellow\" | '''0''' || 0 || 0 || 0 || 0 || 0 || 0 || 0\n|-\n! 1 !! X\n| style=\"background:yellow\" | 0 || 0 || 0 || 0 || 0 || 0 || 1 || 1\n|-\n! 2 !! M\n| 0 || style=\"background:yellow\" | '''1''' || style=\"background:yellow\" | 1 || 1 || 1 || 1 || 1 || 1\n|-\n! 3 !! J\n| 0 || 1 || 1 || style=\"background:yellow\" | '''2''' || 2 || 2 || 2 || 2\n|-\n! 4 !! Y\n| 0 || 1 || 1 || style=\"background:yellow\" | 2 || 2 || 2 || 2 || 2\n|-\n! 5 !! A\n| 0 || 1 || 1 || 2 || style=\"background:yellow\" | '''3''' || style=\"background:yellow\" | 3 || style=\"background:yellow\" | 3 || 3\n|-\n! 6 !! U\n| 0 || 1 || 1 || 2 || 3 || 3 || 3 || style=\"background:yellow\" | '''4'''\n|-\n! 7 !! Z\n| 0 || 1 || 2 || 2 || 3 || 3 || 3 || style=\"background: yellow\" | 4\n|}\n\nThe <span style=\"background: yellow\">highlighted</span> numbers show the path the function <code>backtrack</code> would follow from the bottom right to the top left corner, when reading out an LCS. If the current symbols in <math>X</math> and <math>Y</math> are equal, they are part of the LCS, and we go both up and left (shown in '''bold'''). If not, we go up or left, depending on which cell has a higher number. This corresponds to either taking the LCS between <math>X_{1..i-1}</math> and <math>Y_{1..j}</math>, or <math>X_{1..i}</math> and <math>Y_{1..j-1}</math>.\n\n== Code optimization ==\nSeveral optimizations can be made to the algorithm above to speed it up for real-world cases.\n\n=== Reduce the problem set ===\nThe C matrix in the naive algorithm [[quadratic growth|grows quadratically]] with the lengths of the sequences.  For two 100-item sequences, a 10,000-item matrix would be needed, and 10,000 comparisons would need to be done.  In most real-world cases, especially source code diffs and patches, the beginnings and ends of files rarely change, and almost certainly not both at the same time.  If only a few items have changed in the middle of the sequence, the beginning and end can be eliminated.  This reduces not only the memory requirements for the matrix, but also the number of comparisons that must be done.\n\n '''function''' LCS(X[1..m], Y[1..n])\n     start := 1\n     m_end := m\n     n_end := n\n     ''trim off the matching items at the beginning''\n     '''while''' start ≤ m_end '''and''' start ≤ n_end '''and''' X[start] = Y[start]\n         start := start + 1\n     ''trim off the matching items at the end''\n     '''while''' start ≤ m_end '''and''' start ≤ n_end '''and''' X[m_end] = Y[n_end]\n         m_end := m_end - 1\n         n_end := n_end - 1\n     C = array(start-1..m_end, start-1..n_end)\n     ''only loop over the items that have changed''\n     '''for''' i := start..m_end\n         '''for''' j := start..n_end\n             ''the algorithm continues as before ...''\n\nIn the best-case scenario, a sequence with no changes, this optimization would completely eliminate the need for the C matrix.  In the worst-case scenario, a change to the very first and last items in the sequence, only two additional comparisons are performed.\n\n=== Reduce the comparison time ===\nMost of the time taken by the naive algorithm is spent performing comparisons between items in the sequences.  For textual sequences such as source code, you want to view lines as the sequence elements instead of single characters.  This can mean comparisons of relatively long strings for each step in the algorithm.  Two optimizations can be made that can help to reduce the time these comparisons consume.\n\n=== Reduce strings to hashes ===\nA [[hash function]] or [[checksum]] can be used to reduce the size of the strings in the sequences.  That is, for source code where the average line is 60 or more characters long, the hash or checksum for that line might be only 8 to 40 characters long.  Additionally, the randomized nature of hashes and checksums would guarantee that comparisons would short-circuit faster, as lines of source code will rarely be changed at the beginning.\n\nThere are three primary drawbacks to this optimization.  First, an amount of time needs to be spent beforehand to precompute the hashes for the two sequences.  Second, additional memory needs to be allocated for the new hashed sequences.  However, in comparison to the naive algorithm used here, both of these drawbacks are relatively minimal.\n\nThe third drawback is that of [[hash collision|collisions]].  Since the checksum or hash is not guaranteed to be unique, there is a small chance that two different items could be reduced to the same hash.  This is unlikely in source code, but it is possible.  A cryptographic hash would therefore be far better suited for this optimization, as its entropy is going to be significantly greater than that of a simple checksum.  However, the benefits may not be worth the setup and computational requirements of a cryptographic hash for small sequence lengths.\n\n=== Reduce the required space ===\nIf only the length of the LCS is required, the matrix can be reduced to a <math>2\\times \\min(n,m)</math> matrix with ease, or to a <math>\\min(m,n)+1</math> vector (smarter) as the dynamic programming approach only needs the current and previous columns of the matrix. [[Hirschberg's algorithm]] allows the construction of the optimal sequence itself in the same quadratic time and linear space bounds.<ref>{{cite journal|authorlink = Dan Hirschberg|author=Hirschberg, D. S.|title=A linear space algorithm for computing maximal common subsequences|journal=Communications of the ACM|volume=18|issue=6|year=1975|pages=341–343|doi=10.1145/360825.360861}}</ref>\n\n=== Further optimized algorithms ===\nSeveral algorithms exist that are worst-case faster than the presented dynamic programming approach.<ref>https://books.google.com/books?id=mFd_grFyiT4C&pg=PA132&lpg=PA132&dq=hunt+szymanski+algorithm&source=bl&ots=sMc-HtvNTQ&sig=FtrZ_b5JdJ25Ighwc1-XOfysaf8&hl=en&sa=X&ei=-BU9VPK7OpS7ggT0gYEQ&ved=0CDsQ6AEwAw#v=onepage&q&f=false</ref> For problems with a bounded alphabet size, the [[Method of Four Russians]] can be used to reduce the running time of the dynamic programming algorithm by a logarithmic factor.<ref>{{citation\n | last1 = Masek | first1 = William J.\n | last2 = Paterson | first2 = Michael S. | author2-link = Mike Paterson\n | doi = 10.1016/0022-0000(80)90002-1\n | issue = 1\n | journal = Journal of Computer and System Sciences\n | mr = 566639\n | pages = 18–31\n | title = A faster algorithm computing string edit distances\n | volume = 20\n | year = 1980}}.</ref> There is an algorithm that performs in <math>O((n + r)\\log(n))</math> time (for <math>n > m</math>), where <math>r</math> is the number of matches between the two sequences.<ref>http://www.cs.bgu.ac.il/~dpaa111/wiki.files/HuntSzymanski.pdf</ref>\n\n== Behavior on random strings ==\n{{main|Chvátal–Sankoff constants}}\nBeginning with {{harvtxt|Chvátal|Sankoff|1975}},<ref>{{citation\n | last1 = Chvatal | first1 = Václáv | author1-link = Václav Chvátal\n | last2 = Sankoff | first2 = David | author2-link = David Sankoff\n | journal = Journal of Applied Probability\n | mr = 0405531\n | pages = 306–315\n | title = Longest common subsequences of two random sequences\n | volume = 12\n | year = 1975 | doi=10.2307/3212444}}.</ref> a number of researchers have investigated the behavior of the longest common subsequence length when the two given strings are drawn randomly from the same alphabet. When the alphabet size is constant, the expected length of the LCS is proportional to the length of the two strings, and the constants of proportionality (depending on alphabet size) are known as the [[Chvátal–Sankoff constants]]. Their exact values are not known, but upper and lower bounds on their values have been proven,<ref>{{citation\n | last = Lueker | first = George S.\n | doi = 10.1145/1516512.1516519\n | issue = 3\n | journal = [[Journal of the ACM]]\n | mr = 2536132\n | at = A17\n | title = Improved bounds on the average length of longest common subsequences\n | volume = 56\n | year = 2009}}.</ref> and it is known that they grow inversely proportionally to the square root of the alphabet size.<ref>{{citation\n | last1 = Kiwi | first1 = Marcos\n | last2 = Loebl | first2 = Martin\n | last3 = Matoušek | first3 = Jiří | author3-link = Jiří Matoušek (mathematician)\n | doi = 10.1016/j.aim.2004.10.012\n | issue = 2\n | journal = Advances in Mathematics\n | mr = 2173842\n | pages = 480–498\n | title = Expected length of the longest common subsequence for large alphabets\n | volume = 197\n | year = 2005| arxiv = math/0308234\n }}.</ref> Simplified mathematical models of the longest common subsequence problem have been shown to be controlled by the [[Tracy–Widom distribution]].<ref>{{citation\n | last1 = Majumdar | first1 = Satya N.\n | last2 = Nechaev | first2 = Sergei\n | doi = 10.1103/PhysRevE.72.020901\n | issue = 2\n | journal = Physical Review E\n | mr = 2177365\n | pages = 020901, 4\n | title = Exact asymptotic results for the Bernoulli matching model of sequence alignment\n | volume = 72\n | year = 2005| arxiv = q-bio/0410012\n | bibcode = 2005PhRvE..72b0901M\n }}.</ref>\n\n== See also ==\n* [[Longest increasing subsequence]]\n* [[Longest alternating subsequence]]\n* [[Levenshtein distance]]\n* [[Hunt–McIlroy algorithm]]\n\n== References ==\n{{reflist}}\n<!-- Dead note \"GJ78\": {{cite book|author = [[Michael R. Garey]] and [[David S. Johnson]] | year = 1979 | title = Computers and Intractability: A Guide to the Theory of NP-Completeness| publisher = W.H. Freeman | isbn = 0-7167-1045-5| pages = 228}} A421: SR10. -->\n\n== External links ==\n{{Wikibooks|Algorithm implementation|Strings/Longest common subsequence|Longest common subsequence}}\n* [https://xlinux.nist.gov/dads/HTML/longestCommonSubsequence.html Dictionary of Algorithms and Data Structures: longest common subsequence]\n* [http://rosettacode.org/wiki/Longest_common_subsequence A collection of implementations of the longest common subsequence in many programming languages]\n* [http://www.codingalpha.com/longest-common-subsequence-c-program/ Implementation of Longest Common Subsequence Algorithm in C Programming Language]\n\n <!-- case of fixed number of input strings -->\n <!-- case of arbitrary number of input strings -->\n\n{{DEFAULTSORT:Longest Common Subsequence Problem}}\n[[Category:Problems on strings]]\n[[Category:Combinatorics]]\n[[Category:Dynamic programming]]\n[[Category:Polynomial-time problems]]\n[[Category:NP-complete problems]]"
    },
    {
      "title": "Longest common substring problem",
      "url": "https://en.wikipedia.org/wiki/Longest_common_substring_problem",
      "text": "{{Distinguish|longest common subsequence problem}}\nIn [[computer science]], the '''longest common substring problem''' is to find the longest [[string (computer science)|string]] (or strings) that is a [[substring]] (or are substrings) of two or more strings.\n\n==Example==\nThe longest common substring of the strings \"ABABC\", \"BABCA\" and \"ABCBA\" is string \"ABC\" of length 3. Other common substrings are \"A\", \"AB\", \"B\", \"BA\", \"BC\" and \"C\".\n\n   ABABC\n     |||\n    BABCA\n     |||\n     ABCBA\n\n==Problem definition==\nGiven two strings, <math>S</math> of length <math>m</math> and <math>T</math> of length <math>n</math>, find the longest strings which are substrings of both <math>S</math> and <math>T</math>.\n\nA generalization is the '''k-common substring problem'''. Given the set of strings <math>S = \\{S_1, ..., S_K\\}</math>, where <math>|S_i|=n_i</math> and <math>\\Sigma n_i = N</math>. Find for each <math>2 \\leq k \\leq K</math>, the longest strings which occur as substrings of at least <math>k</math> strings.\n\n==Algorithms==\nOne can find the lengths and starting positions of the longest common substrings of <math>S</math> and <math>T</math> in <math>\\Theta(n+m)</math> time with the help of a [[generalized suffix tree]]. Finding them by [[dynamic programming]] costs <math>\\Theta(nm)</math>. The solutions to the generalized problem take <math>\\Theta(n_1 + ... + n_K)</math> space and <math>\\Theta(n_1</math>·...·<math>n_K)</math> time with [[dynamic programming]] and take <math>\\Theta(N * K)</math> time with [[generalized suffix tree]].\n\n===Suffix tree===\n[[Image:Suffix tree ABAB BABA ABBA.svg|thumb|400px|right|[[Generalized suffix tree]] for the strings \"ABAB\", \"BABA\" and \"ABBA\", numbered 0, 1 and 2.]]\nThe longest common substrings of a set of strings can be found by building a [[generalized suffix tree]] for the strings, and then finding the deepest internal nodes which have leaf nodes from all the strings in the subtree below it. The figure on the right is the suffix tree for the strings \"ABAB\", \"BABA\" and \"ABBA\", padded with unique string terminators, to become \"ABAB$0\", \"BABA$1\" and \"ABBA$2\". The nodes representing \"A\", \"B\", \"AB\" and \"BA\" all have descendant leaves from all of the strings, numbered 0, 1 and 2.\n\nBuilding the suffix tree takes <math>\\Theta(N)</math> time (if the size of the alphabet is constant). If the tree is traversed from the bottom up with a bit vector telling which strings are seen below each node, the k-common substring problem can be solved in <math>\\Theta(NK)</math> time. If the suffix tree is prepared for constant time [[lowest common ancestor]] retrieval, it can be solved in <math>\\Theta(N)</math> time.<ref name=\"Gus97\">{{cite book | last = Gusfield | first = Dan | origyear = 1997 | year = 1999 | title = Algorithms on Strings, Trees and Sequences: Computer Science and Computational Biology | publisher = Cambridge University Press | location = USA | isbn = 0-521-58519-8}}</ref>\n\n==Pseudocode==\nThe following pseudocode finds the set of longest common substrings between two strings with dynamic programming:\n\n '''function''' LCSubstr(S[1..r], T[1..n])\n     L := '''array'''(1..r, 1..n)\n     z := 0\n     ret := {}\n     '''for''' i := 1..r\n         '''for''' j := 1..n\n             '''if''' S[i] == T[j]\n                 '''if''' i == 1 or j == 1\n                     L[i,j] := 1\n                 '''else'''\n                     L[i,j] := L[i-1,j-1] + 1\n                 '''if''' L[i,j] > z\n                     z := L[i,j]\n                     ret := {S[i-z+1..i]}\n                 '''else'''\n                 '''if''' L[i,j] == z\n                     ret := ret ∪ {S[i-z+1..i]}\n             '''else'''\n                 L[i,j] := 0\n     '''return''' ret\n\nThis algorithm runs in <math>O(n r)</math> time. The variable <code>z</code> is used to hold the length of the longest common substring found so far. The set <code>ret</code> is used to hold the set of strings which are of length <code>z</code>. The set <code>ret</code> can be saved efficiently by just storing the index <code>i</code>, which is the last character of the longest common substring (of size z) instead of <code>S[i-z+1..i]</code>. Thus all the longest common substrings would be, for each i in <code>ret</code>, <code>S[(ret[i]-z)..(ret[i])]</code>.\n\nThe following tricks can be used to reduce the memory usage of an implementation:\n* Keep only the last and current row of the DP table to save memory (<math>O(\\min(r, n))</math> instead of <math>O(n r)</math>)\n* Store only non-zero values in the rows. This can be done using hash tables instead of arrays. This is useful for large alphabets.\n\n==See also==\n* [[Data deduplication]]\n* [[Longest palindromic substring]]\n* [[n-gram|''n''-gram]], all the possible substrings of length ''n'' that are contained in a string\n* [[Plagiarism detection]]\n\n==References==\n<references />\n\n==External links==\n{{Wikibooks|Algorithm implementation|Strings/Longest common substring|Longest common substring}}\n* [http://nist.gov/dads/HTML/longestCommonSubstring.html Dictionary of Algorithms and Data Structures: longest common substring]\n* [http://metacpan.org/module/String::LCSS_XS Perl/XS implementation of the dynamic programming algorithm]\n* [http://metacpan.org/module/Tree::Suffix Perl/XS implementation of the suffix tree algorithm]\n* [[b:Algorithm implementation/Strings/Longest common substring|Dynamic programming implementations in various languages on wikibooks]]\n* [http://www.emanueleferonato.com/2010/12/01/solving-the-longest-common-substring-problem-with-as3/ working AS3 implementation of the dynamic programming algorithm]\n* [http://www.geeksforgeeks.org/suffix-tree-application-5-longest-common-substring-2/ Suffix Tree based C implementation of Longest common substring for two strings]\n\n[[Category:Problems on strings]]\n[[Category:Dynamic programming]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Longest increasing subsequence",
      "url": "https://en.wikipedia.org/wiki/Longest_increasing_subsequence",
      "text": "In [[computer science]], the '''longest increasing subsequence''' problem is to find a subsequence of a given [[sequence]] in which the subsequence's elements are in sorted order, lowest to highest, and in which the subsequence is as long as possible. This subsequence is not necessarily contiguous, or unique.\nLongest increasing subsequences are studied in the context of various disciplines related to [[mathematics]], including [[algorithmics]], [[random matrix theory]], [[representation theory]], and [[physics]].<ref>{{citation\n | last1 = Aldous | first1 = David | author1-link = David Aldous\n | last2 = Diaconis | first2 = Persi | author2-link = Persi Diaconis\n | doi = 10.1090/S0273-0979-99-00796-X\n | journal = Bulletin of the American Mathematical Society\n | pages = 413–432\n | issue = 04\n | title = Longest increasing subsequences: from patience sorting to the Baik–Deift–Johansson theorem\n | volume = 36\n | year = 1999}}.</ref> The longest increasing subsequence problem is solvable in time O(''n'' log ''n''), where ''n'' denotes the length of the input sequence.<ref name=\"schensted\"/>\n\n==Example==\nIn the first 16 terms of the binary [[Van der Corput sequence]]\n:0, 8, 4, 12, 2, 10, 6, 14, 1, 9, 5, 13, 3, 11, 7, 15\na longest increasing subsequence is\n:0, 2, 6, 9, 11, 15.\nThis subsequence has length six; the input sequence has no seven-member increasing subsequences. The longest increasing subsequence in this example is not unique: for instance,\n:0, 4, 6, 9, 11, 15 or  \n:0, 2, 6, 9, 13, 15 or\n:0, 4, 6, 9, 13, 15\nare other increasing subsequences of equal length in the same input sequence.\n\n== Relations to other algorithmic problems ==\nThe longest increasing subsequence problem is closely related to the [[longest common subsequence problem]], which has a quadratic time [[dynamic programming]] solution: the longest increasing subsequence of a sequence ''S'' is the longest common subsequence of ''S'' and ''T'', where ''T'' is the result of [[sorting]] ''S''. However, for the special case in which the input is a permutation of the integers 1, 2, ..., ''n'', this approach can be made much more efficient, leading to time bounds of the form O(''n'' log log ''n'').<ref>{{Citation |author1=Hunt, J. |author2=Szymanski, T. | title = A fast algorithm for computing longest common subsequences | journal = Communications of the ACM | year = 1977 | pages = 350–353 | doi = 10.1145/359581.359603 | issue = 5 | volume = 20 | postscript = .}}</ref>\n\nThe largest [[clique (graph theory)|clique]] in a [[permutation graph]] is defined by the longest decreasing subsequence of the permutation that defines the graph; the longest decreasing subsequence is equivalent in computational complexity, by negation of all numbers, to the longest increasing subsequence. Therefore, longest increasing subsequence algorithms can be used to solve the [[clique problem]] efficiently in permutation graphs.<ref>{{citation|first=M. C.|last=Golumbic|authorlink=Martin Charles Golumbic|title=Algorithmic Graph Theory and Perfect Graphs|series=Computer Science and Applied Mathematics|publisher=Academic Press|year=1980|page=159}}.</ref>\n\nIn the [[Robinson–Schensted correspondence]] between [[permutation]]s and [[Young tableau]]x, the length of the first row of the tableau corresponding to a permutation equals the length of the longest increasing subsequence of the permutation, and the length of the first column equals the length of the longest decreasing subsequence.<ref name=\"schensted\">{{Citation | doi=10.4153/CJM-1961-015-3 | authorlink=Craige Schensted | last1=Schensted | first1=C. | title=Longest increasing and decreasing subsequences | mr = 0121305 | year=1961 | journal=[[Canadian Journal of Mathematics]] | volume=13 | pages=179–191}}.</ref>\n\n== Efficient algorithms ==\nThe algorithm outlined below solves the longest increasing subsequence problem efficiently with arrays and [[binary search]]ing. \nIt processes the sequence elements in order, maintaining the longest increasing subsequence found so far. Denote the sequence values as X[0], X[1], etc. Then, after processing X[''i''], the algorithm will have stored values in two arrays:\n:M[''j''] — stores the index ''k'' of the smallest value X[''k''] such that there is an increasing subsequence of length ''j'' ending at X[''k''] on the range ''k'' ≤ ''i''. Note that ''j'' ≤ ''(i+1)'', because ''j'' ≥ 1 represents the length of the increasing subsequence, and ''k'' ≥ 0 represents the index of its termination.\n:P[''k''] — stores the index of the predecessor of X[''k''] in the longest increasing subsequence ending at X[''k''].\nIn addition the algorithm stores a variable L representing the length of the longest increasing subsequence found so far. Because the algorithm below uses [[zero-based numbering]], for clarity M is padded with M[0], which goes unused so that M[''j''] corresponds to a subsequence of length ''j''. A real implementation can skip M[0] and adjust the indices accordingly.\n\nNote that, at any point in the algorithm, the sequence\n:<nowiki>X[M[1]], X[M[2]], ..., X[M[L]]</nowiki>\nis increasing.  For, if there is an increasing subsequence of length ''j'' ≥ 2 ending at X[M[''j'']], then there is also a subsequence of length ''j''-1 ending at a smaller value: namely the one ending at X[P[M[''j'']]]. Thus, we may do binary searches in this sequence in logarithmic time.\n\nThe algorithm, then, proceeds as follows:\n[[File:LISDemo.gif|400px|thumb|A demo of the code.]]\n \n  P = array of length N\n  M = array of length N + 1\n \n  L = 0\n  '''for''' i '''in range''' 0 '''to''' N-1:\n    // Binary search for the largest positive j ≤ L\n    // such that X[M[j]] <= X[i]\n    lo = 1\n    hi = L\n    '''while''' lo ≤ hi:\n      mid = ceil((lo+hi)/2)\n      '''if''' X[M[mid]] <= X[i]:\n        lo = mid+1\n      '''else''':\n        hi = mid-1\n \n    // After searching, lo is 1 greater than the\n    // length of the longest prefix of X[i]\n    newL = lo\n \n    // The predecessor of X[i] is the last index of \n    // the subsequence of length newL-1\n    P[i] = M[newL-1]\n    M[newL] = i\n \n    '''if''' newL > L:\n      // If we found a subsequence longer than any we've\n      // found yet, update L\n      L = newL\n \n  // Reconstruct the longest increasing subsequence\n  S = array of length L\n  k = M[L]\n  '''for''' i '''in range''' L-1 '''to''' 0:\n    S[i] = X[k]\n    k = P[k]\n \n  '''return''' S\n\nBecause the algorithm performs a single binary search per sequence element, its total time can be expressed using [[Big O notation]] as O(''n''&nbsp;log&nbsp;''n''). {{harvtxt|Fredman|1975}} discusses a variant of this algorithm, which he credits to [[Donald Knuth]]; in the variant that he studies, the algorithm tests whether each value X[''i''] can be used to extend the current longest increasing sequence, in constant time, prior to doing the binary search. With this modification, the algorithm uses at most {{nowrap|''n'' log<sub>2</sub> ''n'' &minus; ''n'' log<sub>2</sub>log<sub>2</sub> ''n'' + O(''n'')}} comparisons in the worst case, which is optimal for a comparison-based algorithm up to the constant factor in the O(''n'') term.<ref>{{citation\n | last = Fredman | first = Michael L. | authorlink = Michael Fredman\n | doi = 10.1016/0012-365X(75)90103-X\n | issue = 1\n | journal = Discrete Mathematics\n | pages = 29–35\n | title = On computing the length of longest increasing subsequences\n | volume = 11\n | year = 1975}}.</ref>\n\n==Length bounds==\nAccording to the [[Erdős–Szekeres theorem]], any sequence of ''n''<sup>2</sup>+1 distinct integers has an increasing or a decreasing subsequence of length  {{nowrap|''n'' + 1.<ref>{{Citation\n  |author1=[[Paul Erdős|Erdős, Paul]] |author2=[[George Szekeres|Szekeres, George]]\n | title = A combinatorial problem in geometry\n  | journal = Compositio Mathematica\n  | volume = 2\n  | pages = 463–470\n  | year = 1935\n  | url = http://www.numdam.org/item?id=CM_1935__2__463_0}}.</ref><ref>{{citation\n | last = Steele | first = J. Michael | authorlink = J. Michael Steele\n | contribution = Variations on the monotone subsequence theme of Erdős and Szekeres\n | editor1-last = Aldous | editor1-first = David | editor1-link = David Aldous\n | editor2-last = Diaconis | editor2-first = Persi | editor2-link = Persi Diaconis\n | editor3-last = Spencer | editor3-first = Joel | editor3-link = Joel Spencer\n |display-editors = 3 | editor4-last = Steele | editor4-first = J. Michael | editor4-link = J. Michael Steele\n | pages = 111–131\n | publisher = Springer-Verlag\n | series = IMA Volumes in Mathematics and its Applications\n | title = Discrete Probability and Algorithms\n | url = http://www-stat.wharton.upenn.edu/~steele/Publications/PDF/VOTMSTOEAS.pdf\n | volume = 72\n | year = 1995}}.</ref>}} For inputs in which each permutation of the input is equally likely, the expected length of the longest increasing subsequence is approximately 2{{radic|''n''}}.\n<ref name=\"vk\">{{citation\n | last1 = Vershik | first1 = A. M. | author1-link = A._M._Vershik\n | last2 = Kerov | first2 = C. V. \n | journal = Dokl. Akad. Nauk SSSR\n | pages = 1024–1027\n | title = Asymptotics of the Plancheral measure of the symmetric group and a limiting form for Young tableaux\n | volume = 233\n | year = 1977}}.</ref> \nIn the limit as ''n'' approaches infinity, the length of the longest increasing subsequence of a randomly permuted sequence of ''n'' items has a distribution approaching the [[Tracy–Widom distribution]], the distribution of the largest eigenvalue of a random matrix in the [[Gaussian unitary ensemble]].<ref>{{citation|title=On the distribution of the length of the longest increasing subsequence of random permutations|first1=Jinho|last1=Baik|first2=Percy|last2=Deift|first3=Kurt|last3=Johansson|journal=Journal of the American Mathematical Society|volume=12|year=1999|issue=4|pages=1119–1178|doi=10.1090/S0894-0347-99-00307-0|arxiv=math/9810105}}.</ref>\n\n==Online algorithms==\nThe longest increasing subsequence has also been studied in the setting of [[online algorithm]]s, in which the elements of a sequence of independent random variables with continuous distribution ''F'' – or alternatively the elements of a [[random permutation]] – are presented one at a time to an algorithm that must decide whether to include or exclude each element, without knowledge of the later elements. In this variant of the problem, which allows for interesting applications in several contexts, it is possible to devise an optimal selection procedure that, given a random sample of size ''n'' as input, will generate an increasing sequence with maximal expected length of size approximately {{sqrt|''2n''}}.\n<ref\nname=\"ss81\">{{citation\n | doi = 10.1214/aop/1176994265\n | last1 = Samuels | first1 = Stephen. M.\n | last2 = Steele | first2 = J. Michael | author2-link = J._Michael_Steele\n | journal = Annals of Probability\n | pages = 937–947\n | title = Optimal Sequential Selection of a Monotone Sequence From a Random Sample\n | volume = 9\n | issue = 6\n | year = 1981}}</ref>\nThe length of the increasing subsequence selected by this optimal procedure has variance approximately equal to {{sqrt|''2n''}}''/3'', and its limiting distribution is asymptotically [[normal distribution|normal]] after the usual centering and scaling.<ref name=\"ans2015\">{{citation\n | doi = 10.1016/j.spa.2015.03.009\n | last1 = Arlotto | first1 = Alessandro\n | last2 = Nguyen | first2 = Vinh V.\n | last3 = Steele | first3 = J. Michael | author3-link = J._Michael_Steele\n | journal = Stochastic Processes and their Applications\n | pages = 3596–3622\n | title = Optimal online selection of a monotone subsequence: a central limit theorem\n | volume = 125 \n | issue = 9\n | year = 2015| arxiv = 1408.6750}}</ref>\nThe same asymptotic  results hold with more precise bounds for the corresponding problem in the setting of a Poisson arrival process.<ref>{{citation\n | last1 = Bruss | first1 = F. Thomas | author1-link = F._Thomas_Bruss\n | last2 = Delbaen | first2 = Freddy\n | doi =\n | issue = 2\n | journal = Stochastic Processes and their Applications\n | pages = 313–342\n | title = Optimal rules for the sequential selection of monotone subsequences of maximum expected length\n | volume = 96\n | year = 2001}}.</ref>\nA further refinement in the Poisson process setting is given through the proof of a [[central limit theorem]] for the optimal selection process\nwhich holds, with a suitable normalization, in a more complete sense than one would expect. The proof yields not only the \"correct\" functional limit theorem\nbut also the (singular) [[covariance matrix]] of the three-dimensional process summarizing all interacting processes.\n<ref>{{citation\n | last1 = Bruss | first1 = F. Thomas | author1-link = F._Thomas_Bruss\n | last2 = Delbaen | first2 = Freddy\n | doi = 10.1016/j.spa.2004.09.002\n | issue = 2\n | journal = Stochastic Processes and their Applications\n | pages = 287–311\n | title = A central limit theorem for the optimal selection process for monotone subsequences of maximum expected length\n | volume = 114\n | year = 2004}}.</ref>\n\n== Application ==\n{{Expand section|date=May 2019}}<br />\n== See also ==\n*[[Patience sorting]], an efficient technique for finding the length of the longest increasing subsequence\n*[[Plactic monoid]], an algebraic system defined by transformations that preserve the length of the longest increasing subsequence\n*[[Anatoly Vershik]], a Russian mathematician who studied applications of group theory to longest increasing subsequences\n*[[Longest common subsequence]]\n*[[Longest alternating subsequence]]\n\n==References==\n{{reflist}}\n\n==External links==\n*[http://www.algorithmist.com/index.php/Longest_Increasing_Subsequence Algorithmist's Longest Increasing Subsequence]\n*[https://kuuatt.blogspot.com/2017/06/longest-increasing-sub-sequence-longest.html Simplified Longest Increasing Subsequence]\n*[https://stackoverflow.com/questions/22923646/number-of-all-longest-increasing-subsequences/22945390#22945390 Finding count of longest increased subsequences]\n*[https://cms.di.unipi.it/#/task/poldo/statement Poldo's diet]\n\n[[Category:Problems on strings]]\n[[Category:Combinatorics]]\n[[Category:Formal languages]]\n[[Category:Dynamic programming]]"
    },
    {
      "title": "Markov decision process",
      "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
      "text": "{{citation style|date=April 2018}}\nA '''Markov decision process''' ('''MDP''') is a [[discrete time]] [[stochastic]] [[Optimal control theory|control]] process. It provides a mathematical framework for modeling [[decision making]] in situations where outcomes are partly [[Randomness#In mathematics|random]] and partly under the control of a decision maker. MDPs are useful for studying [[optimization problem]]s solved via [[dynamic programming]] and [[reinforcement learning]]. MDPs were known at least as early as the 1950s;<ref>{{harvnb|Bellman|1957}}</ref> a core body of research on Markov decision processes resulted from [[Ronald A. Howard|Ronald Howard]]'s 1960 book, ''Dynamic Programming and Markov Processes''.{{sfn|Howard|1960}} They are used in many disciplines, including [[robotics]], [[automatic control]], [[economics]] and [[manufacturing]]. The name of MDPs comes from the Russian mathematician [[Andrey Markov]]{{why|date=June 2019}}.\n\nAt each time step, the process is in some state <math>s</math>, and the decision maker may choose any action <math>a</math> that is available in state <math>s</math>. The process responds at the next time step by randomly moving into a new state <math>s'</math>, and giving the decision maker a corresponding reward <math>R_a(s,s')</math>.\n\nThe probability that the process moves into its new state <math>s'</math> is influenced by the chosen action. Specifically, it is given by the state transition function <math>P_a(s,s')</math>. Thus, the next state <math>s'</math> depends on the current state <math>s</math> and the decision maker's action <math>a</math>. But given <math>s</math> and <math>a</math>, it is conditionally independent of all previous states and actions; in other words, the state transitions of an MDP satisfies the [[Markov property]].\n\nMarkov decision processes are an extension of [[Markov chain]]s; the difference is the addition of actions (allowing choice) and rewards (giving motivation). Conversely, if only one action exists for each state (e.g. \"wait\") and all rewards are the same (e.g. \"zero\"), a Markov decision process reduces to a Markov chain.\n\n==Definition==\n[[File:Markov Decision Process.svg|thumb|400x400px|Example of a simple MDP with three states (green circles) and two actions (orange circles), with two rewards (orange arrows).]]\nA Markov decision process is a 4-[[tuple]] <math>(S, A, P_a, R_a)</math>, where\n\n* <math>S</math> is a finite set of states,\n* <math>A</math> is a finite set of actions (alternatively, <math>A_s</math> is the finite set of actions available from state <math>s</math>),\n* <math>P_a(s, s') = \\Pr(s_{t+1}=s' \\mid s_t = s, a_t=a)</math> is the probability that action <math>a</math> in state <math>s</math> at time <math>t</math> will lead to state <math>s'</math> at time <math>t+1</math>,\n*<math>R_a(s, s')</math> is the immediate reward (or expected immediate reward) received after transitioning from state <math>s</math> to state <math>s'</math>, due to action <math>a</math>\n\n(Note: The theory of Markov decision processes does not state that <math>S</math> or <math>A</math> are finite, but the basic algorithms below assume that they are finite.)\n\n==Problem==\n\nThe core problem of MDPs is to find a \"policy\" for the decision maker: a function <math>\\pi</math> that specifies the action <math>\\pi(s)</math> that the decision maker will choose when in state <math>s</math>. Once a Markov decision process is combined with a policy in this way, this fixes the action for each state and the resulting combination behaves like a [[Markov chain]] (since the action chosen in state <math>s</math> is completely determined by <math>\\pi(s)</math> and <math>\\Pr(s_{t+1}=s' \\mid s_t = s, a_t=a)</math> reduces to <math>\\Pr(s_{t+1}=s' \\mid s_t = s)</math>, a Markov transition matrix).\n\nThe goal is to choose a policy <math>\\pi</math> that will maximize some cumulative function of the random rewards, typically the expected discounted sum over a potentially infinite horizon:\n\n:<math>\\sum^{\\infty}_{t=0} {\\gamma^t R_{a_t} (s_t, s_{t+1})} </math> &nbsp;&nbsp;&nbsp;(where we choose <math>a_t = \\pi(s_t)</math>, i.e. actions given by the policy)\n\nwhere <math>\\ \\gamma \\ </math> is the discount factor and satisfies <math>0 \\le\\ \\gamma\\ \\le\\ 1</math>. (For example, <math> \\gamma = 1/(1+r) </math> when the discount rate is r.) <math> \\gamma </math> is typically close to 1.\n\nBecause of the Markov property, the optimal policy for this particular problem can indeed be written as a function of <math>s</math> only, as assumed above.\n\nThe discount factor is used so that the decision maker favours taking actions early and doesn't postpone them indefinitely.\n\n==Algorithms==\nThe solution for an MDP is a policy which describes the best action for each state in the MDP, known as the optimal policy.  This optimal policy can be found through a variety of methods, like [[dynamic programming]].\n\nSome dynamic programming solutions require knowledge of the state transition function <math>P</math> and the reward function <math>R</math>.  Others can solve for the optimal policy of an MDP using experimentation alone.\n\nConsider the case in which state transition function <math>P</math> and reward function <math>R</math> for an MDP are given, and we seek the optimal policy <math>\\pi^*</math> that maximizes the expected discounted reward.\n\nThe standard family of algorithms to calculate this optimal policy requires storage for two arrays indexed by state: ''value'' <math>V</math>, which contains real values, and ''policy'' <math>\\pi</math>, which contains actions. At the end of the algorithm, <math>\\pi</math> will contain the solution and <math>V(s)</math> will contain the discounted sum of the rewards to be earned (on average) by following that solution from state <math>s</math>.\n\nThe algorithm has two steps, (1) a value update and (2) a policy update, which are repeated in some order for all the states until no further changes take place.  Both recursively update \na new estimation of the optimal policy and state value using an older estimation of those values.\n\n:<math> V(s) := \\sum_{s'} P_{\\pi(s)} (s,s') \\left( R_{\\pi(s)} (s,s') + \\gamma V(s') \\right) </math>\n\n:<math> \\pi(s) := \\operatorname{argmax}_a \\left\\{ \\sum_{s'} P(s' \\mid s, a) \\left( R(s'\\mid s,a) + \\gamma V(s') \\right) \\right\\} </math>\n\nTheir order depends on the variant of the algorithm; one can also do them for all states at once or state by state, and more often to some states than others. As long as no state is permanently excluded from either of the steps, the algorithm will eventually arrive at the correct solution{{citation needed|date=July 2018}}.\n\n===Notable variants===\n\n====Value iteration====\nIn value iteration {{harv|Bellman|1957}}, which is also called [[backward induction]],\nthe <math>\\pi</math> function is not used; instead, the value of <math>\\pi(s)</math> is calculated within <math>V(s)</math> whenever it is needed. Substituting the calculation of <math>\\pi(s)</math> into the calculation of <math>V(s)</math> gives the combined step{{explain|reason=The derivation of the substituion is needed|date=July 2018}}:\n:<math> V_{i+1}(s) := \\max_a \\left\\{ \\sum_{s'} P_a(s,s') \\left( R_a(s,s') + \\gamma V_i(s') \\right) \\right\\}, </math>\n\nwhere <math>i</math> is the iteration number. Value iteration starts at <math>i = 0</math> and <math>V_0</math> as a guess of the value function. It then iterates, repeatedly computing <math>V_{i+1}</math> for all states <math>s</math>, until <math>V</math> converges with the left-hand side equal to the right-hand side (which is the \"[[Bellman equation]]\" for this problem{{clarify|date=January 2018}}). [[Lloyd Shapley]]'s 1953 paper on [[stochastic games]]{{sfn|Shapley|1953}} included as a special case the value iteration method for MDPs, but this was recognized only later on.{{sfn|Kallenberg|2002}}\n\n====Policy iteration====\nIn policy iteration {{harv|Howard|1960}}, step one is performed once, and then step two is repeated until it converges. Then step one is again performed once and so on.\n\nInstead of repeating step two to convergence, it may be formulated and solved as a set of linear equations. These equations are merely obtained by making <math>s = s'</math> in the step two equation{{what|date=January 2018}}. Thus, repeating step two to convergence can be interpreted as solving the linear equations by [[Relaxation (iterative method)]]\n\nThis variant has the advantage that there is a definite stopping condition: when the array <math>\\pi</math> does not change in the course of applying step 1 to all states, the algorithm is completed.\n\nPolicy iteration is usually slower than value iteration for a large number of possible states.\n\n====Modified policy iteration====\nIn modified policy iteration ({{harvnb|van Nunen|1976}}; {{harvnb|Puterman|Shin|1978}}), step one is performed once, and then step two is repeated several times. Then step one is again performed once and so on.\n\n====Prioritized sweeping====\nIn this variant, the steps are preferentially applied to states which are in some way important – whether based on the algorithm (there were large changes in <math>V</math> or <math>\\pi</math> around those states recently) or based on use (those states are near the starting state, or otherwise of interest to the person or program using the algorithm).\n\n==Extensions and generalizations==\nA Markov decision process is a [[stochastic game]] with only one player.\n\n===Partial observability===\n{{main|partially observable Markov decision process}}\nThe solution above assumes that the state <math>s</math> is known when action is to be taken; otherwise <math>\\pi(s)</math> cannot be calculated. When this assumption is not true, the problem is called a partially observable Markov decision process or POMDP.\n\nA major advance in this area was provided by Burnetas and Katehakis in \"Optimal adaptive policies for Markov decision processes\".{{sfn|Burnetas|Katehakis|1997}} In this work, a class of adaptive policies that possess uniformly maximum convergence rate properties for the total expected finite horizon reward were constructed under the assumptions of finite state-action spaces and irreducibility of the transition law. These policies prescribe that the choice of actions, at each state and time period, should be based on indices that are inflations of the right-hand side of the estimated average reward optimality equations.\n\n===Reinforcement learning===\n{{main|Reinforcement learning}}\n\nIf the probabilities or rewards are unknown, the problem is one of reinforcement learning<ref>{{cite journal|author1=Shoham, Y.|author2= Powers, R.|author3= Grenager, T. |year=2003|title= Multi-agent reinforcement learning: a critical survey |pp= 1–13|journal= Technical Report, Stanford University|url=http://jmvidal.cse.sc.edu/library/shoham03a.pdf|access-date=2018-12-12}}</ref>.\n\nFor this purpose it is useful to define a further function, which corresponds to taking the action <math>a</math> and then continuing optimally (or according to whatever policy one currently has):\n:<math>\\ Q(s,a) = \\sum_{s'} P_a(s,s') (R_a(s,s') + \\gamma V(s')).\\ </math>\n\nWhile this function is also unknown, experience during learning is based on <math>(s, a)</math> pairs (together with the outcome <math>s'</math>; that is, \"I was in state <math>s</math> and I tried doing <math>a</math> and <math>s'</math> happened\"). Thus, one has an array <math>Q</math> and uses experience to update it directly. This is known as Q-learning.\n\nReinforcement learning can solve Markov decision processes without explicit specification of the transition probabilities; the values of the transition probabilities are needed in value and policy iteration. In reinforcement learning, instead of explicit specification of the transition probabilities, the transition probabilities are accessed through a simulator that is typically restarted many times from a uniformly random initial state. Reinforcement learning can also be combined with function approximation to address problems with a very large number of states.\n\n=== Learning automata ===\n{{main|Learning automata}}\nAnother application of MDP process in [[machine learning]] theory is called learning automata. This is also one type of reinforcement learning if the environment is stochastic. The first detail '''learning automata''' paper is surveyed by [[Kumpati S. Narendra|Narendra]] and Thathachar (1974), which were originally described explicitly as [[finite state automata]].{{sfn|Narendra|Thathachar|1974}} Similar to reinforcement learning, a learning automata algorithm also has the advantage of solving the problem when probability or rewards are unknown. The difference between learning automata and Q-learning is that the former technique omits the memory of Q-values, but updates the action probability directly to find the learning result. Learning automata is a learning scheme with a rigorous proof of convergence.{{sfn|Narendra|Thathachar|1989}}\n\nIn learning automata theory, '''a stochastic automaton''' consists of:\n* a set ''x'' of possible inputs,\n* a set Φ = { Φ<sub>1</sub>, ..., Φ<sub>''s''</sub> } of possible internal states,\n* a set α = { α<sub>1</sub>, ..., α<sub>''r''</sub> } of possible outputs, or actions, with ''r''&nbsp;≤&nbsp;''s'',\n* an initial state probability vector ''p''(0) = ≪ ''p''<sub>1</sub>(0), ..., ''p<sub>s</sub>''(0) ≫,\n* a [[computable function]] ''A'' which after each time step ''t'' generates ''p''(''t''&nbsp;+&nbsp;1) from ''p''(''t''), the current input, and the current state, and\n* a function ''G'': Φ → α which generates the output at each time step.\nThe states of such an automaton correspond to the states of a \"discrete-state discrete-parameter [[Markov process]]\".{{sfn|Narendra|Thathachar|1974|loc=p.325 left}} At each time step ''t''&nbsp;=&nbsp;0,1,2,3,..., the automaton reads an input from its environment, updates P(''t'') to P(''t''&nbsp;+&nbsp;1) by ''A'', randomly chooses a successor state according to the probabilities P(''t''&nbsp;+&nbsp;1) and outputs the corresponding action. The automaton's environment, in turn, reads the action and sends the next input to the automaton.{{sfn|Narendra|Thathachar|1989}}\n\n===Category theoretic interpretation===\nOther than the rewards, a Markov decision process <math>(S,A,P)</math> can be understood in terms of [[Category theory]]. Namely, let <math>\\mathcal{A}</math> denote the [[free monoid]] with generating set ''A''. Let '''Dist''' denote the [[Kleisli category]] of the [http://ncatlab.org/nlab/show/Giry+monad Giry monad]. Then a functor <math>\\mathcal{A}\\to\\mathbf{Dist}</math> encodes both the set ''S'' of states and the probability function ''P''.\n\nIn this way, Markov decision processes could be generalized from monoids (categories with one object) to arbitrary categories. One can call the result <math>(\\mathcal{C}, F:\\mathcal{C}\\to \\mathbf{Dist})</math> a ''context-dependent Markov decision process'', because moving from one object to another in <math>\\mathcal{C}</math> changes the set of available actions and the set of possible states.\n\n===Fuzzy Markov decision processes (FMDPs)===\n\nIn the MDPs, an optimal policy is a policy which maximizes the probability-weighted summation of future rewards. Therefore, an optimal policy consists of several actions which belong to a finite set of actions. In fuzzy Markov decision processes (FMDPs), first, the value function is computed as regular MDPs (i.e., with a finite set of actions); then, the policy is extracted by a fuzzy inference system. In other words, the value function is utilized as an input for the fuzzy inference system, and the policy is the output of the fuzzy inference system.<ref>{{cite journal|last1=Fakoor|first1=Mahdi|last2=Kosari|first2=Amirreza|last3=Jafarzadeh|first3=Mohsen|title=Humanoid robot path planning with fuzzy Markov decision processes|journal=Journal of Applied Research and Technology|date=2016|doi=10.1016/j.jart.2016.06.006|url=http://www.sciencedirect.com/science/article/pii/S1665642316300700|volume=14|issue=5|pages=300–310}}</ref>\n\n==Continuous-time Markov decision process==\nIn discrete-time Markov Decision Processes, decisions are made at discrete time intervals. However, for '''continuous-time Markov decision processes''', decisions can be made at any time the decision maker chooses. In comparison to discrete-time Markov decision processes, continuous-time Markov decision processes can better model the decision making process for a system that has [[Continuous time|continuous dynamics]], i.e.,&nbsp;the system dynamics is defined by [[partial differential equation]]s (PDEs).\n\n===Definition===\nIn order to discuss the continuous-time Markov decision process, we introduce two sets of notations:\n\nIf the state space and action space are finite,\n* <math>\\mathcal{S}</math>: State space;\n* <math>\\mathcal{A}</math>: Action space;\n* <math>q(i\\mid j,a)</math>: <math>\\mathcal{S}\\times \\mathcal{A} \\rightarrow \\triangle \\mathcal{S}</math>, transition rate function;\n* <math>R(i,a)</math>: <math>\\mathcal{S}\\times \\mathcal{A} \\rightarrow \\mathbb{R}</math>, a reward function.\n\nIf the state space and action space are continuous,\n* <math>\\mathcal{X}</math>: state space;\n* <math>\\mathcal{U}</math>: space of possible control;\n* <math>f(x,u)</math>: <math>\\mathcal{X}\\times \\mathcal{U} \\rightarrow \\triangle \\mathcal{X}</math>, a transition rate function;\n* <math>r(x,u)</math>: <math>\\mathcal{X}\\times \\mathcal{U} \\rightarrow \\mathbb{R}</math>, a reward rate function such that <math>r(x(t),u(t))\\,dt = dR(x(t),u(t))</math>, where <math>R(x,u)</math> is the reward function we discussed in previous case.\n\n===Problem===\nLike the discrete-time Markov decision processes, in continuous-time Markov decision processes we want to find the optimal ''policy'' or ''control'' which could give us the optimal expected integrated reward:\n:<math>\\max \\operatorname{E}_u\\left[\\left. \\int_0^\\infty\\gamma^t r(x(t),u(t))) \\, dt \\;\\right| x_0 \\right]</math>\nwhere <math>0\\leq\\gamma< 1.</math>\n\n===Linear programming formulation===\nIf the state space and action space are finite, we could use linear programming to find the optimal policy, which was one of the earliest approaches applied. Here we only consider the ergodic model, which means our continuous-time MDP becomes an [[Ergodicity|ergodic]] continuous-time Markov chain under a stationary [[policy]]. Under this assumption, although the decision maker can make a decision at any time at the current state, he could not benefit more by taking more than one action. It is better for him to take an action only at the time when system is transitioning from the current state to another state. Under some conditions,(for detail check Corollary 3.14 of [https://www.springer.com/mathematics/applications/book/978-3-642-02546-4 ''Continuous-Time Markov Decision Processes'']), if our optimal value function <math>V^*</math> is independent of state <math>i</math>, we will have the following inequality:\n:<math>g\\geq R(i,a)+\\sum_{j\\in S}q(j\\mid i,a)h(j) \\quad \\forall i \\in S \\text{ and } a \\in A(i)</math>\nIf there exists a function <math>h</math>, then <math>\\bar V^*</math> will be the smallest <math>g</math> satisfying the above equation. In order to find <math>\\bar V^*</math>, we could use the following linear programming model:\n*Primal linear program(P-LP)\n:<math>\n\\begin{align}\n\\text{Minimize}\\quad &g\\\\\n\\text{s.t} \\quad & g-\\sum_{j \\in S}q(j\\mid i,a)h(j)\\geq R(i,a)\\,\\,\n\\forall i\\in S,\\,a\\in A(i)\n\\end{align}\n</math>\n*Dual linear program(D-LP)\n:<math>\n\\begin{align}\n\\text{Maximize} &\\sum_{i\\in S}\\sum_{a\\in A(i)}R(i,a)y(i,a)\\\\\n\\text{s.t.} &\\sum_{i\\in S}\\sum_{a\\in A(i)} q(j\\mid i,a)y(i,a)=0 \\quad\n\\forall j\\in S,\\\\\n& \\sum_{i\\in S}\\sum_{a\\in A(i)}y(i,a)=1,\\\\\n& y(i,a)\\geq 0 \\qquad \\forall a\\in A(i) \\text{ and } \\forall i\\in S\n\\end{align}\n</math>\n<math>y(i,a)</math> is a feasible solution to the D-LP if <math>y(i,a)</math> is\nnonnative and satisfied the constraints in the D-LP problem. A\nfeasible solution <math>y^*(i,a)</math> to the D-LP is said to be an optimal\nsolution if\n:<math>\n\\begin{align}\n\\sum_{i\\in S}\\sum_{a\\in A(i)}R(i,a)y^*(i,a) \\geq  \\sum_{i\\in S} \\sum_{a\\in A(i)} R(i,a) y(i,a)\n\\end{align}\n</math>\nfor all feasible solution <math>y(i,a)</math> to the D-LP.\nOnce we have found the optimal solution <math>y^*(i,a)</math>, we can use it to establish the optimal policies.\n\n===Hamilton–Jacobi–Bellman equation===\nIn continuous-time MDP, if the state space and action space are continuous, the optimal criterion could be found by solving [[Hamilton–Jacobi–Bellman equation|Hamilton–Jacobi–Bellman (HJB) partial differential equation]].\nIn order to discuss the HJB equation, we need to reformulate\nour problem\n:<math>\\begin{align} V(x(0),0)= {} & \\max_u\\int_0^T r(x(t),u(t)) \\, dt+D[x(T)] \\\\\n\\text{s.t.}\\quad & \\frac{dx(t)}{dt}=f[t,x(t),u(t)]\n\\end{align}\n</math>\n\n<math>D(\\cdot)</math> is the terminal reward function, <math>x(t)</math> is the\nsystem state vector, <math>u(t)</math> is the system control vector we try to\nfind. <math>f(\\cdot)</math> shows how the state vector changes over time.\nThe Hamilton–Jacobi–Bellman equation is as follows:\n:<math>0=\\max_u ( r(t,x,u) +\\frac{\\partial V(t,x)}{\\partial x}f(t,x,u)) </math>\nWe could solve the equation to find the optimal control <math>u(t)</math>, which could give us the optimal value <math>V^*</math>\n\n===Application===\nContinuous-time Markov decision processes have applications in [[queueing system]]s, epidemic processes, and [[population process]]es.\n\n==Alternative notations==\nThe terminology and notation for MDPs are not entirely settled. There are two main streams — one focuses on maximization problems from contexts like economics, using the terms action, reward, value, and calling the discount factor <math>\\beta</math> or <math>\\gamma</math>, while the other focuses on minimization problems from engineering and navigation, using the terms control, cost, cost-to-go, and calling the discount factor <math>\\alpha</math>. In addition, the notation for the transition probability varies.\n\n{| class=\"wikitable\"\n! in this article !! alternative !! comment\n|-\n| action <math>a</math> || control <math>u</math> ||\n|-\n| reward <math>R</math> || cost <math>g</math>\n| <math>g</math> is the negative of <math>R</math>\n|-\n| value <math>V</math> || cost-to-go <math>J</math>\n| <math>J</math> is the negative of <math>V</math>\n|-\n| policy <math>\\pi</math> || policy <math>\\mu</math> ||\n|-\n| discounting factor <math>\\ \\gamma \\ </math> || discounting factor <math>\\alpha</math> ||\n|-\n| transition probability <math>P_a(s,s')</math> || transition probability <math>p_{ss'}(a)</math> ||\n|}\n\nIn addition, transition probability is sometimes written <math>\\Pr(s,a,s')</math>, <math>\\Pr(s'\\mid s,a)</math> or, rarely, <math>p_{s's}(a).</math>\n\n== Constrained Markov decision processes ==\nConstrained Markov decision processes (CMDPs) are extensions to Markov decision process (MDPs). There are three fundamental differences between MDPs and CMDPs.{{sfn|Altman|1999}}\n\n* There are multiple costs incurred after applying an action instead of one.\n* CMDPs are solved with [[Linear programming|linear programs]] only, and [[dynamic programming]] does not work.\n* The final policy depends on the starting state.\n\nThere are a number of applications for CMDPs. It has recently been used in [[motion planning]] scenarios in robotics.{{sfn|Feyzabadi|Carpin|2014}}\n\n==See also==\n* [[Probabilistic automata]]\n* [[Quantum finite automata]]\n* [[Partially observable Markov decision process]]\n* [[Dynamic programming]]\n* [[Bellman equation]] for applications to economics.\n* [[Hamilton–Jacobi–Bellman equation]]\n* [[Optimal control]]\n* [[Recursive economics]]\n* [[Mabinogion sheep problem]]\n* [[Stochastic games]]\n* [[Q-learning]]\n\n==Notes==\n{{reflist}}\n\n==References==\n{{refbegin}}\n* {{cite book|last=Altman|first=Eitan|title=Constrained Markov decision processes|volume=7|publisher=CRC Press|year=1999|ref=harv}}\n* {{cite journal|first=R.|last=Bellman|url=http://www.iumj.indiana.edu/IUMJ/FULLTEXT/1957/6/56038|title=A Markovian Decision Process|journal=Journal of Mathematics and Mechanics|volume=6|year=1957|ref=harv}}\n* {{cite book|first=R. E.|last=Bellman.|title=Dynamic Programming|publisher=Princeton University Press|location=Princeton, NJ|orig-year=1957|edition=Dover paperback |year=2003|isbn=978-0-486-42809-3|ref=harv}}\n* {{cite book|first=D.|last=Bertsekas|title=Dynamic Programming and Optimal Control|volume=2|publisher=Athena|location=MA|year=1995|ref=harv}}\n* {{cite journal|last1=Burnetas|first1=A.N.|first2=M. N.|last2=Katehakis|title=Optimal Adaptive Policies for Markov Decision Processes|journal=Mathematics of Operations Research|volume=22|issue=1|year=1997|doi = 10.1287/moor.22.1.222|pages=222|ref=harv}}\n* {{cite book|first=C.|last=Derman|title=Finite state Markovian decision processes|publisher=Academic Press|year=1970|ref=harv}}\n* {{cite book|editor1-first=E.A.|editor1-last=Feinberg|editor2-first=A.|editor2-last=Shwartz|title=Handbook of Markov Decision Processes|publisher=Kluwer|location=Boston, MA|year=2002|ref=harv|url=https://books.google.com/books?id=TpwKCAAAQBAJ&printsec=frontcover#v=onepage&q&f=false|isbn=9781461508052}}\n* {{cite conference |last1=Feyzabadi |first1=S. |last2=Carpin |first2=S. |title=Risk-aware path planning using hierarchical constrained Markov Decision Processes |book-title=Automation Science and Engineering (CASE) |conference=IEEE International Conference |pages=297, 303 |date=18–22 Aug 2014 |ref=harv|url=https://www.researchgate.net/profile/Shams_Feyzabadi/publication/270105954_Risk-aware_Path_Planning_Using_Hierarchical_Constrained_Markov_Decision_Processes/links/54d13c480cf28370d0e05aea.pdf}}\n* {{cite book|first1=X.|last1=Guo|first2=O.|last2=Hernández-Lerma|url=https://www.springer.com/mathematics/applications/book/978-3-642-02546-4|title=Continuous-Time Markov Decision Processes|publisher=Springer|year=2009|ref=harv}}\n* {{cite book|first=Ronald A.|last=Howard|title=Dynamic Programming and Markov Processes|publisher=The M.I.T. Press|year=1960|url=http://web.mit.edu/dimitrib/www/dpchapter.pdf|ref=harv}}\n* {{cite book|first=Lodewijk|last=Kallenberg|chapter=Finite state and action MDPs|editor-first1=Eugene A.|editor-last1=Feinberg|editor-first2=Adam|editor-last2=Shwartz|title=Handbook of Markov decision processes: methods and applications|publisher=Springer|year=2002|isbn=978-0-7923-7459-6|ref=harv}}\n* {{cite book|first=S. P.|last=Meyn|url=https://netfiles.uiuc.edu/meyn/www/spm_files/CTCN/CTCN.html|archive-url=https://web.archive.org/web/20100619011046/https://netfiles.uiuc.edu/meyn/www/spm_files/CTCN/CTCN.html|archive-date=19 June 2010|title=Control Techniques for Complex Networks|publisher=Cambridge University Press|year=2007|isbn=978-0-521-88441-9|ref=harv}} Appendix contains abridged {{cite web|url=https://netfiles.uiuc.edu/meyn/www/spm_files/book.html|archive-url=https://web.archive.org/web/20121218173202/https://netfiles.uiuc.edu/meyn/www/spm_files/book.html|archive-date=18 December 2012|title=Meyn & Tweedie}}\n* {{Cite journal|last=Narendra|first=K. S.|authorlink=Kumpati S. Narendra|last2=Thathachar|first2=M. A. L.|date=1974-07-01|title=Learning Automata – A Survey|url=http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5408453|journal=IEEE Transactions on Systems, Man, and Cybernetics|volume=SMC-4|issue=4|pages=323–334|doi=10.1109/TSMC.1974.5408453|issn=0018-9472|ref=harv|citeseerx=10.1.1.295.2280}}\n* {{Cite book|url=https://books.google.com/books?id=hHVQAAAAMAAJ|title=Learning automata: An introduction|last=Narendra|first=Kumpati S.|authorlink=Kumpati S. Narendra|last2=Thathachar|first2=Mandayam A. L.|year=1989|publisher=Prentice Hall|isbn=9780134855585|language=en|ref=harv}}\n* {{cite journal|first1=M. L.|last1=Puterman|last2=Shin|first2=M. C.|title=Modified Policy Iteration Algorithms for Discounted Markov Decision Problems|journal=Management Science|volume=24|issue=11|year=1978|ref=harv|doi=10.1287/mnsc.24.11.1127|pages=1127–1137}}\n* {{cite book|first=M. L.|last=Puterman.|title=Markov Decision Processes|publisher=Wiley|year=1994|ref=harv}}\n* {{cite book|first=S. M.|last=Ross|year=1983|title=Introduction to stochastic dynamic programming|publisher=Academic press|ref=harv|url=http://www.deeplearningitalia.com/wp-content/uploads/2018/03/Introduction-to-Stochastic-Dynamic-Programming-Ross.pdf}}\n* {{cite journal|last=Shapley|first=Lloyd|authorlink=Lloyd Shapley|title=Stochastic Games|year=1953|journal=Proceedings of the National Academy of Sciences of the United States of America|volume=39|issue=10|pages=1095–1100|ref=harv|doi=10.1073/pnas.39.10.1095|pmid=16589380}}\n* {{cite book|last1=Sutton|first1=R. S.|last2=Barto|first2=A. G.|title=Reinforcement Learning: An Introduction|publisher=The MIT Press|location=Cambridge, MA|year=1998|url=http://incompleteideas.net/sutton/book/the-book-1st.html|ref=harv}}\n* {{cite book|last1=Sutton|first1=R. S.|last2=Barto|first2=A. G.|title=Reinforcement Learning: An Introduction|publisher=The MIT Press|location=Cambridge, MA|year=2017|url=http://incompleteideas.net/sutton/book/the-book-2nd.html|ref=harv}}\n* {{cite book|first=H.C.|last=Tijms.|title=A First Course in Stochastic Models|publisher=Wiley|year=2003|ref=harv|url=https://books.google.com/books?id=WibF8iVHaiMC&printsec=frontcover#v=onepage&q=%22Markov%20decision%22&f=false|isbn=9780470864289}}\n* {{cite journal|first=J.A. E. E|last=van Nunen|title=A set of successive approximation methods for discounted Markovian decision problems. Z|journal=Operations Research |volume=20|issue=5|pages=203–208|year=1976|ref=harv|doi=10.1007/bf01920264}}\n{{refend}}\n\n==External links==\n* [http://www7.inra.fr/mia/T/MDPtoolbox/ MDP Toolbox for MATLAB, GNU Octave, Scilab and R] The Markov Decision Processes (MDP) Toolbox.\n* [http://www.ai.mit.edu/~murphyk/Software/MDP/mdp.html MDP Toolbox for Matlab] – An excellent tutorial and Matlab toolbox for working with MDPs.\n* [https://pypi.python.org/pypi/pymdptoolbox MDP Toolbox for Python] A package for solving MDPs\n* [https://github.com/JuliaPOMDP/POMDPs.jl POMDPs.jl] A flexible interface for defining and solving MDPs in [[Julia (programming language)|Julia]] with a variety of solvers\n* [http://www.incompleteideas.net/book/ebook/the-book.html Reinforcement Learning] An Introduction by Richard S. Sutton and Andrew G. Barto\n* [http://www.cs.uwaterloo.ca/~jhoey/research/spudd/index.php SPUDD] A structured MDP solver for download by Jesse Hoey\n* [http://www.eecs.umich.edu/~baveja/Papers/Thesis.ps.gz Learning to Solve Markovian Decision Processes] by [http://www.eecs.umich.edu/~baveja/ Satinder P. Singh]\n* [https://www.jstor.org/stable/3690147 Optimal Adaptive Policies for Markov Decision Processes] by Burnetas and Katehakis (1997).\n\n[[Category:Optimal decisions]]\n[[Category:Dynamic programming]]\n[[Category:Markov processes]]\n[[Category:Stochastic control]]"
    },
    {
      "title": "Most frequent k characters",
      "url": "https://en.wikipedia.org/wiki/Most_frequent_k_characters",
      "text": "{{multiple issues|\n{{Third-party|date=March 2014}}\n{{Notability|date=March 2014}}\n}}\n{{DISPLAYTITLE:Most frequent ''k'' characters}}\nIn [[information theory]], '''MostFreqKDistance''' is a [[string metric]] technique for quickly estimating how [[Similarity measure|similar]] two [[Order theory|ordered sets]] or [[String (computer science)|strings]] are. The scheme was invented by {{harvs|first=Sadi Evren|last=Seker|year=2014|txt}},<ref name=\"mfkc\"/> and initially used in [[text mining]] applications like [[author recognition]].<ref name=\"mfkc\"/>\nThe method is originally based on a hashing function MaxFreqKChars<ref name=\"hashfunc\"/> classical author recognition problem and idea first came out while studying on [[data stream mining]].<ref name=\"author\"/> The algorithm is suitable for coding in any turing complete programming language.<ref name=\"Rosetta\"/>\n\n==Definition==\nMethod has two steps.\n* [[Hash function|Hash]] input strings str1 and str2 separately using MostFreqKHashing and output hstr1 and hstr2 respectively\n* Calculate string distance (or string similarity coefficient) of two hash outputs, hstr1 and hstr2 and output an integer value\n\n===Most frequent K hashing===\nThe first step of algorithm is calculating the hashing based on the most frequent k characters. The hashing algorithm has below steps:\n<syntaxhighlight lang=\"Java\">\nString function MostFreqKHashing (String inputString, int K)\n    def string outputString\n    for each distinct character\n        count occurrence of each character\n    for i := 0 to K\n        char c = next most freq ith character  (if two chars have same frequency then get the first occurrence in inputString)\n        int count = number of occurrence of the character\n        append to outputString, c and count\n    end for\n    return outputString\n</syntaxhighlight>\n\nAbove function, simply gets an input string and an integer K value and outputs the most frequent K characters from the input string. The only condition during the creation of output string is adding the first occurring character first, if the frequencies of two characters are equal. Similar to the most of [[hashing function]]s, ''Most Frequent K Hashing'' is also a [[one way function]].\n\n===Most frequent K distance===\nThe second step of algorithm works on two outputs from two different input strings and outputs the similarity coefficient (or distance metric).\n<syntaxhighlight lang=\"Java\">\nint function MostFreqKSimilarity (String inputStr1, String inputStr2, int limit)\n    def int similarity\n    for each c = next character from inputStr1\n        lookup c in inputStr2\n        if c is null\n             continue\n        similarity += frequency of c in inputStr1\n    return limit - similarity\n</syntaxhighlight>\nAbove function, simply gets two input strings, previously outputted from the <code>MostFreqKHashing</code> function. From the most frequent k hashing function, the characters and their frequencies are returned. So, the similarity function calculates the similarity based on characters and their frequencies by checking if the same character appears on both strings. The limit is usually taken to be 10 and in the end the function returns the result of the subtraction of the sum of similarities from limit.\n\nIn some implementations, the distance metric is required instead of similarity coefficient. In order to convert the output of above similarity coefficient to distance metric, the output can be subtracted from any constant value (like the maximum possible output value). For the case, it is also possible to implement a [[wrapper function]] over above two functions.\n\n===String distance wrapper function===\nIn order to calculate the distance between two strings, the below function can be implemented\n<syntaxhighlight lang=\"Java\">\nint function MostFreqKSDF (String inputStr1, String inputStr2, int K, int maxDistance)\n    return MostFreqKSimilarity(MostFreqKHashing(inputStr1, K), MostFreqKHashing(inputStr2, K), maxDistance)\n</syntaxhighlight>\n\nAny call to above string distance function will supply two input strings and a maximum distance value. The function will calculate the similarity and subtract that value from the maximum possible distance. It can be considered as a simple [[additive inverse]] of similarity.\n\n==Examples==\nConsider maximum 2 frequent hashing over two strings ‘research’ and ‘seeking’.\nMostFreqKHashing('research', 2) = r2e2\nbecause we have 2 'r' and 2 'e' characters with the highest frequency and we return in the order they appear in the string.\nMostFreqKHashing('seeking', 2) = e2s1\nAgain we have character 'e' with highest frequency and rest of the characters have same frequency of 1, so we return the first character of equal frequencies, which is 's'.\nFinally we make the comparison:\nMostFreqKSimilarity('r2e2', 'e2s1') = 2\nWe simply compared the outputs and only character occurring in both input is character 'e' and the occurrence in both input is 2.\nInstead running the sample step by step as above, we can simply run by using the string distance wrapper function as below:\nMostFreqKSDF('research', 'seeking', 2) = 2\n\nBelow table holds some sample runs between example inputs for K=2:\n{|class=\"wikitable\"\n|-\n! Inputs\n! Hash outputs\n! SDF output (max from 10)\n|-\n|\n 'night'\n 'nacht'\n|\n n1i1\n n1a1\n|9\n|-\n|\n 'my'\n 'a'\n|\n m1y1\n a1NULL0\n|10\n|-\n|\n ‘research’\n ‘research’\t\n|\n r2e2\n r2e2\t\n|6\n|-\n|\n ‘aaaaabbbb’\n ‘ababababa’\t\n|\n a5b4\n a5b4\t\n|1\n|-\n|\n ‘significant’\n ‘capabilities’\t\n|\n i3n2\n i3a2\t\n|7\n|}\n\nMethod is also suitable for bioinformatics to compare the genetic strings like in [[FASTA format]].\n\n:Str1 = LCLYTHIGRNIYYGSYLYSETWNTGIMLLLITMATAFMGYVLPWGQMSFWGATVITNLFSAIPYIGTNLV\n\n:Str2 = EWIWGGFSVDKATLNRFFAFHFILPFTMVALAGVHLTFLHETGSNNPLGLTSDSDKIPFHPYYTIKDFLG\n\n:MostFreqKHashing(str1, 2) = L9T8\n\n:MostFreqKHashing(str2, 2) = F9L8\n\n:MostFreqKSDF(str1, str2, 2, 100) = 83\n\n==Algorithm complexity and comparison==\nThe motivation behind algorithm is calculating the similarity between two input strings. So, the hashing function should be able to reduce the size of input and at the same time keep the characteristics of the input. Other hashing algorithms like [[MD5]] or [[SHA-1]], the output is completely unrelated with the input and those hashing algorithms are not suitable for string similarity check.\n\nOn the other hand, string similarity functions like [[Levenshtein distance]] have the algorithm complexity problem.\n\nAlso algorithms like [[Hamming distance]], [[Jaccard coefficient]] or [[Tanimoto coefficient]] have relatively low algorithm complexity but the success rate in [[text mining]] studies are also low.\n\n===Time complexity===\nThe calculation of time complexity of 'most frequent k char string similarity' is quite simple. In order to get the maximum frequent K characters from a string, the first step is sorting the string in a lexiconical manner. After this sort, the input with highest occurrence can be achieved with a simple pass in linear time complexity. Since major classical sorting algorithms are working in O(nlogn) complexity like [[merge sort]] or [[quick sort]], we can sort the first string in O(nlogn) and second string on O(mlogm) times. The total complexity would be O(nlog n ) + O (m log m) which is O(n log n) as the upper bound [[worst case analysis]].\n\n===Comparison===\nBelow table compares the complexity of algorithms:\n{|class=\"wikitable\"\n|-\n! Algorithm\n! Time complexity\n|-\n| [[Levenshtein distance]]\n| <math>O(nm) = O(n^2)</math>\n|-\n| [[Jaccard index]]\n| <math>O(n+m) = O(n)</math>\n|-\n| MostFreqKSDF\n| <math>O(n \\log n + m \\log m) = O(n \\log n)</math>\n|}\n\nFor the above table, n is the length of first string and m is the length of second string.\n\n==Success on text mining==\nThe success of string similarity algorithms are compared on a study. The study is based on IMDB62 dataset which is holding 1000 comment entries in [[Internet Movie Database]] from each 62 people. The data set is challenged for three string similarity functions and the success rates are as below:\n\n{|class=\"wikitable\"\n|-\n! Algorithm\n! Running time\n! Error (RMSE)\n! Error (RAE)\n|-\n|[[Levenshtein distance]]\n|3647286.54 sec\n|29\n|0.47\n|-\n|[[Jaccard index]]\n|228647.22 sec\n|45\n|0.68\n|-\n|MostFreqKSDF\n|2712323.51 sec\n|32\n|0.49\n|}\n\nThe running times for author recognition are in seconds and the error rates are [[root mean square error]] (RMSE) and [[relative absolute error]] (RAE).\n\nAbove table shows, the 'most frequent ''k'' similarity' is better than [[Levenshtein distance]] by time and [[Jaccard index]] by success rate.\n\nFor the time performance and the success rates, the bitwise similarity functions like [[Dice's coefficient|Sørensen–Dice index]], [[Tversky index]] or [[Hamming distance]] are all in the same category with similar success rates and running times. There are obviously slight differences but the idea behind bitwise operation, loses the string operations like deletion or addition. For example, a single bit addition to the front of one of the input strings would yield a catastrophic result on the similarity for bitwise operators while Levenshtein distance is successfully catching.\n\nUnfortunately, [[big data]] studies often require a faster algorithm that still has an acceptable success rate. In such situations, the 'max frequent ''k'' characters' is a conceptually simpler algorithm that is also straight forward to implement.\n\n==See also==\n* [[agrep]]\n* [[Approximate string matching]]\n* [[Bitap algorithm]]\n* [[Damerau–Levenshtein distance]]\n* [[diff]]\n* [[MinHash]]\n* [[Dynamic time warping]]\n* [[Euclidean distance]]\n* [[Fuzzy string searching]]\n* [[Hamming weight]]\n* [[Hirschberg's algorithm]]\n* [[Sequence homology|Homology of sequences in genetics]]\n* [[Hunt–McIlroy algorithm]]\n* [[Jaccard index]]\n* [[Jaro–Winkler distance]]\n* [[Levenshtein distance]]\n* [[Longest common subsequence problem]]\n* [[Lucene]] (an open source search engine that implements edit distance)\n* [[Manhattan distance]]\n* [[Metric space]]\n* [[Needleman–Wunsch algorithm]]\n* [[Optimal matching]] algorithm\n* [[Sequence alignment]]\n* Similarity space on [[Numerical taxonomy]]\n* [[Smith–Waterman algorithm]]\n* [[Sørensen similarity index]]\n* [[String distance metric]]\n* [[String similarity function]]\n* [[Wagner–Fischer algorithm]]\n* [[Locality-sensitive hashing]]\n\n==References==\n{{reflist|refs=\n<ref name=\"mfkc\">{{cite journal |author-last1=Seker |author-first1=Sadi Evren |author-last2=Altun |author-first2=Oguz |author-last3=Ayan |author-first3=Ugur |author-last4=Mert |author-first4=Cihan |title=A Novel String Distance Function based on Most Frequent K Characters |volume=4 |issue=2 |pages=177–183 |publisher=International Association of Computer Science and Information Technology Press (IACSIT Press) |journal=International Journal of Machine Learning and Computing (IJMLC) |arxiv=1401.6596 |date=2014|bibcode=2014arXiv1401.6596E }}</ref>\n<ref name=\"hashfunc\">{{cite journal |author-last1=Seker |author-first1=Sadi Evren |author-last2=Mert |author-first2=Cihan |title=A Novel Feature Hashing For Text Mining |url=http://journal.ibsu.edu.ge/index.php/jtst/article/view/428 |pages=37–41 |publisher=[[International Black Sea University]] |journal=Journal of Technical Science and Technologies |issn=2298-0032 |volume=2 |issue=1 |date=2013}}</ref>\n<ref name=\"author\">{{cite journal |author-last1=Seker |author-first1=Sadi Evren |author-last2=Al-Naami |author-first2=Khaled |author-last3=Khan |author-first3=Latifur |title=Author attribution on streaming data |doi=10.1109/IRI.2013.6642511 |url=http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6642511 |pages=497–503 |publisher=[[IEEE]] |journal=Information Reuse and Integration (IRI), 2013 IEEE 14th International Conference on, San Francisco, USA, August 14–16, 2013 |date=2013}}</ref>\n<ref name=\"Rosetta\">{{Citation |title=Rosetta Code: Most frequent k chars distance|access-date=2014-10-16 |url=http://rosettacode.org/wiki/Most_frequent_k_chars_distance}}</ref>\n}}\n\n[[Category:String similarity measures]]\n[[Category:Dynamic programming]]\n[[Category:Articles with example pseudocode]]\n[[Category:Quantitative linguistics]]\n[[Category:Hash functions]]\n[[Category:Hashing]]"
    },
    {
      "title": "Needleman–Wunsch algorithm",
      "url": "https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm",
      "text": "{{Technical|date=September 2013}}\n{{Infobox algorithm\n|name= <!-- Defaults to article name -->\n|class= [[Sequence alignment]]\n|image=\n|caption=\n|data=\n|time= <math>O(mn)</math>\n|best-time=\n|average-time=\n|space= <math>O(mn)</math>\n}}\n\nThe '''Needleman–Wunsch algorithm''' is an [[algorithm]] used in  [[bioinformatics]] to [[sequence alignment|align]] [[protein]] or [[nucleotide]] sequences. It was one of the first applications of [[dynamic programming]] to compare biological sequences. The algorithm was developed by Saul B. Needleman and Christian D. Wunsch and published in 1970.<ref name=Needleman>{{cite journal | journal=Journal of Molecular Biology | volume=48 | issue=3 | pages=443–53 | year=1970  |author1=Needleman, Saul B. |author2=Wunsch, Christian D. |last-author-amp=yes | title=A general method applicable to the search for similarities in the amino acid sequence of two proteins | url=http://linkinghub.elsevier.com/retrieve/pii/0022-2836(70)90057-4 | pmid=5420325 | doi = 10.1016/0022-2836(70)90057-4 }}</ref> The algorithm essentially divides a large problem (e.g. the full sequence) into a series of smaller problems, and it uses the solutions to the smaller problems to find an optimal solution to the larger problem.<ref>{{cite web|title=bioinformatics |url=http://www.britannica.com/EBchecked/topic/1334661/bioinformatics/285871/Goals-of-bioinformatics#ref1115380|accessdate=10 September 2014}}</ref> It is also sometimes referred to as the [[optimal matching]] algorithm and the [[Sequence alignment#Global and local alignments|global alignment]] technique. The Needleman–Wunsch algorithm is still widely used for optimal global alignment, particularly when the quality of the global alignment is of the utmost importance. The algorithm assigns a score to every possible alignment, and the purpose of the algorithm is to find all possible alignments having the highest score.\n\n[[File:Needleman-Wunsch pairwise sequence alignment.png|framed|right|Figure 1: Needleman-Wunsch pairwise sequence alignment\n\n<pre>\nResults:\n\nSequences    Best alignments\n---------    ----------------------\nGCATGCU      GCATG-CU      GCA-TGCU      GCAT-GCU\nGATTACA      G-ATTACA      G-ATTACA      G-ATTACA\n\nInterpretation of the initialization step:\n\nOne can interpret the leftmost column in the above figure like this (putting a \"handle\" before each sequence, annotated as + here):\n\nAlignment:  +GCATGCU\n            +GATTACA\nScore:      0  // Handle matches handle, doesn't win any score\n\nAlignment:  +GCATGCU\n           +GATTACA\nScore:      0  // 1 gap,  score -1\n\nAlignment:  +GCATGCU\n          +GATTACA\nScore:      0  // 2 gaps, score -2\n\nAlignment:  +GCATGCU\n         +GATTACA\nScore:      0  // 3 gaps, score -3\n\nAlignment:  +GCATGCU\n        +GATTACA\nScore:      0  // 4 gaps, score -4\n\n...\n\nThe same thing can be done for the uppermost row.\n</pre>]]\n__TOC__\n{{-}}\n\n==Introduction==\nThis algorithm can be used for any two [[String (computer science)|strings]]. This guide will use two small [[DNA sequences]] as examples as shown in the diagram:\n GCATGCU\n GATTACA\n\n===Constructing the grid===\nFirst construct a grid such as one shown in Figure 1 above. Start the first string in the top of the third column and start the other string at the start of the third row. Fill out the rest of the column and row headers as in Figure 1. There should be no numbers in the grid yet.\n\n{| class=\"wikitable\"\n|-\n!|  ||   || G || C || A || T || G || C || U\n|-\n! scope=\"row\" |\n| &nbsp; ||  ||  ||  ||  ||  ||  ||\n|-\n! scope=\"row\" | G\n|  ||   ||   ||   ||   ||   ||   || \n|-\n! scope=\"row\" | A\n|  ||   ||   ||   ||   ||   ||   || \n|-\n! scope=\"row\" | T\n|  ||   ||   ||   ||   ||   ||   ||  \n|-\n! scope=\"row\" | T\n|  ||   ||   ||   ||   ||   ||   || \n|-\n! scope=\"row\" | A\n|  ||   ||   ||   ||   ||   ||   || \n|-\n! scope=\"row\" | C\n|  ||   ||   ||   ||   ||   ||   || \n|-\n! scope=\"row\" | A\n|  ||   ||   ||   ||   ||   ||   || \n|}\n\n===Choosing a scoring system===\nNext, decide how to score each individual pair of letters. Using the example above, one possible alignment candidate might be:\n{{DNA sequence|\n 12345678\n GCATG-CU\n G-ATTACA\n}}\nThe letters may match, mismatch, or be matched to a gap (a deletion or insertion ([[indel]])):\n* Match: The two letters at the current index are the same.\n* Mismatch: The two letters at the current index are different.  \n* Indel (INsertion or DELetion): The best alignment involves one letter aligning to a gap in the other string.\n\nEach of these scenarios is assigned a score and the sum of the scores of all the pairings is the score of the whole alignment candidate. Different systems exist for assigning scores; some have been outlined in the [[#Scoring systems|Scoring systems]] section below. For now, the system used by Needleman and Wunsch<ref name=\"Needleman\"/> will be used:\n\n* Match: +1\n* Mismatch or Indel: −1\n\nFor the Example above, the score of the alignment would be 0:\n{{DNA sequence|\n GCATG-CU\n G-ATTACA}}\n +-++--+- -> 1*4 + (-1)*4 = 0\n\n===Filling in the table===\nStart with a zero in the second row, second column. Move through the cells row by row, calculating the score for each cell. The score is calculated by comparing the scores of the cells neighboring to the left, top or top-left (diagonal) of the cell and adding the appropriate score for match, mismatch or indel. Calculate the candidate scores for each of the three possibilities:\n* The path from the top or left cell represents an indel pairing, so take the scores of the left and the top cell, and add the score for indel to each of them.\n* The diagonal path represents a match/mismatch, so take the score of the top-left diagonal cell and add the score for match if the corresponding bases (letters) in the row and column are matching or the score for mismatch if they do not.\nThe resulting score for the cell is the highest of the three candidate scores.\n\nGiven there is no 'top' or 'top-left' cells for the second row only the existing cell to the left can be used to calculate the score of each cell. Hence -1 is added for each shift to the right as this represents an indelible from the previous score. This results in the first row being 0, -1, -2, -3, -4, -5, -6, -7. The same applies to the second column as only the existing score above each cell can be used. Thus the resulting table is:\n\n{| class=\"wikitable\"\n|-\n!|  ||   || G || C || A || T || G || C || U\n|-\n! scope=\"row\" |\n| 0 || -1 || -2 || -3 || -4 || -5 || -6 || -7\n|-\n! scope=\"row\" | G\n| -1 ||   ||   ||   ||   ||   ||   || \n|-\n! scope=\"row\" | A\n| -2 ||   ||   ||   ||   ||   ||   || \n|-\n! scope=\"row\" | T\n| -3 ||   ||   ||   ||   ||   ||   ||  \n|-\n! scope=\"row\" | T\n| -4 ||   ||   ||   ||   ||   ||   || \n|-\n! scope=\"row\" | A\n| -5 ||   ||   ||   ||   ||   ||   || \n|-\n! scope=\"row\" | C\n| -6 ||   ||   ||   ||   ||   ||   || \n|-\n! scope=\"row\" | A\n| -7 ||   ||   ||   ||   ||   ||   || \n|}\n\nThe first case with existing scores in all 3 directions is the intersection of our first letters (in this case G and G). The surrounding cells are below:\n{| class=\"wikitable\"\n|-\n!|  ||   || G\n|-\n! scope=\"row\" |\n| 0 || -1\n|-\n! scope=\"row\" | G\n| -1 || '''X'''\n|}\n\nThis cell has three possible candidate sums:\n\n* The diagonal top-left neighbor has score 0. The pairing of G and G is a match, so add the score for match: 0+1 = 1\n* The top neighbor has score -1 and moving from there represents an indel, so add the score for indel: (-1) + (-1) = (-2)\n* The left neighbor also has score -1, represents an indel and also produces (-2).\n\nThe highest candidate is 1 and is entered into the cell:\n\n{| class=\"wikitable\"\n|-\n!|  ||   || G\n|-\n! scope=\"row\" |\n| 0 || -1\n|-\n! scope=\"row\" | G\n| -1 || '''1'''\n|}\n\nThe cell which gave the highest candidate score must also be recorded. In the completed diagram in figure 1 above, this is represented as an arrow from the cell in row and column 3 to the cell in row and column 2.\n\nIn the next example, the diagonal step for both X and Y represents a mismatch:\n\n{| class=\"wikitable\"\n|-\n!|  || || G || C\n|-\n! scope=\"row\" |\n| 0 || -1 || -2\n|-\n! scope=\"row\" | G\n| -1 || 1 || '''X'''\n|-\n! scope=\"row\" | A\n| -2 || '''Y''' || \n|}\n\nX:\n* Top: (-2)+(-1) = (-3)\n* Left: (+1)+(-1) = (0)\n* Top-Left: (-1)+(-1) = (-2)\n\nY:\n* Top: (1)+(-1) = (0)\n* Left: (-2)+(-1) = (-3)\n* Top-Left: (-1)+(-1) = (-2)\n\nFor both X and Y, the highest score is zero:\n\n{| class=\"wikitable\"\n|-\n!|  || || G || C\n|-\n! scope=\"row\" |\n| 0 || -1 || -2\n|-\n! scope=\"row\" | G\n| -1 || 1 || '''0'''\n|-\n! scope=\"row\" | A\n| -2 || '''0''' || \n|}\n\nThe highest candidate score may be reached by two or all neighboring cells:\n\n{| class=\"wikitable\"\n|-\n!|  || T || G\n|-\n! scope=\"row\" | T\n| 1 || 1\n|-\n! scope=\"row\" | A\n| 0 || '''X'''\n|}\n\n* Top: (1)+(-1) = (0)\n* Top-Left: (1)+(-1) = (0)\n* Left: (0)+(-1) = (-1)\n\nIn this case, all directions reaching the highest candidate score must be noted as possible origin cells in the finished diagram in figure 1, e.g. in the cell in row and column 7.\n\nFilling in the table in this manner gives the scores or all possible alignment candidates, the score in the cell on the bottom right represents the alignment score for the best alignment.\n\n===Tracing arrows back to origin===\nMark a path from the cell on the bottom right back to the cell on the top left by following the direction of the arrows. From this path, the sequence is constructed by these rules:\n* A diagonal arrow represents a match or mismatch, so the letter of the column and the letter of the row of the origin cell will align.\n* A horizontal or vertical arrow represents an indel. Horizontal arrows will align a gap (\"-\") to the letter of the row (the \"side\" sequence), vertical arrows will align a gap to the letter of the column (the \"top\" sequence).\n* If there are multiple arrows to choose from, they represent a branching of the alignments. If two or more branches all belong to paths from the bottom left to the top right cell, they are equally viable alignments. In this case, note the paths as separate alignment candidates.\n\nFollowing these rules, the steps for one possible alignment candidate in figure 1 are:\n\n U → CU → GCU → -GCU → T-GCU → AT-GCU → CAT-GCU → '''GCAT-GCU'''\n A → CA → ACA → TACA → TTACA → ATTACA → -ATTACA → '''G-ATTACA'''\n              ↓\n     (branch) → TGCU → ...\n              → TACA → ...\n\n==Scoring systems==\n\n===Basic scoring schemes===\nThe simplest scoring schemes simply give a value for each match, mismatch and indel. The step-by-step guide above uses match = 1, mismatch  = −1, indel = −1. Thus the lower the alignment score the larger the [[Levenshtein distance|edit distance]], for this scoring system one wants a high score. Another scoring system might be:\n* Match = 0\n* Indel = 1\n* Mismatch = 1\nFor this system the alignment score will represent the edit distance between the two strings.\nDifferent scoring systems can be devised for different situations, for example if gaps are considered very bad for your alignment you may use a scoring system that penalises gaps heavily, such as:\n* Match = 0\n* Mismatch = 1\n* Indel = 10\n[https://blievrouw.github.io/needleman-wunsch/ Try it out].\n\n===Similarity matrix===\nMore complicated scoring systems attribute values not only for the type of alteration, but also for the letters that are involved. For example, a match between A and A may be given 1, but a match between T and T may be given 4. Here (assuming the first scoring system) more importance is given to the Ts matching than the As, i.e. the Ts matching is  assumed to be more significant to the alignment. This weighting based on letters also applies to mismatches.\n\nIn order to represent all the possible combinations of letters and their resulting scores  a similarity matrix is used. The similarity matrix for the most basic system is represented as:\n\n{| style=\"font-family: monospace; font-size: 150%;\" cellpadding=\"2\" class=\"wikitable\"\n! scope=\"col\" |\n! scope=\"col\" | A\n! scope=\"col\" | G\n! scope=\"col\" | C\n! scope=\"col\" | T\n|- style=\"text-align: right;\"\n! scope=\"row\" | A\n| 1 ||  -1|| -1 || -1\n|- style=\"text-align: right;\"\n! scope=\"row\" | G\n| -1 || 1 || -1 || -1\n|- style=\"text-align: right;\"\n! scope=\"row\" | C\n| -1 || -1 || 1 || -1\n|- style=\"text-align: right;\"\n! scope=\"row\" | T\n| -1 || -1 || -1 ||  1\n|}\nEach score represents a switch from one of the letters the cell matches to the other. Hence this represents all possible matches and deletions (for an alphabet of ACGT). Note all the matches go along the diagonal, also not all the table needs to be filled, only this triangle because the scores are reciprocal.= (Score for A → C = Score for C → A). If implementing the T-T = 4 rule from above the following similarity matrix is produced:\n\n{| style=\"font-family: monospace; font-size: 150%;\" cellpadding=\"2\" class=\"wikitable\"\n! scope=\"col\" |\n! scope=\"col\" | A\n! scope=\"col\" | G\n! scope=\"col\" | C\n! scope=\"col\" | T\n|- style=\"text-align: right;\"\n! scope=\"row\" | A\n| 1 || -1 || -1 || -1\n|- style=\"text-align: right;\"\n! scope=\"row\" | G\n| -1 || 1 || -1 || -1\n|- style=\"text-align: right;\"\n! scope=\"row\" | C\n| -1 || -1 || 1 || -1\n|- style=\"text-align: right;\"\n! scope=\"row\" | T\n| -1 || -1 || -1 ||  4\n|}\n\nDifferent scoring matrices have been statistically constructed which give weight to different actions appropriate to a particular scenario. Having weighted scoring matrices is particularly important in protein sequence alignment due to the varying frequency of the different amino acids. There are two broad families of scoring matrices, each with further alterations for specific scenarios:\n* [[Point accepted mutation|PAM]]\n* [[BLOSUM]]\n\n===Gap penalty===\nWhen aligning sequences there are often gaps (i.e. indels), sometimes large ones. Biologically, a large gap is more likely to occur as one large deletion as opposed to multiple single deletions. Hence two small indels should have a worse score than one large one. The simple and common way to do this is via a large gap-start score for a new indel and a smaller gap-extension score for every letter which extends the indel. For example, new-indel may cost -5 and extend-indel may cost -1. In this way an alignment such as:\n GAAAAAAT\n G--A-A-T\nwhich has multiple equal alignments, some with multiple small alignments will now align as:\n GAAAAAAT\n GAA----T\nor any alignment with a 4 long gap in preference over multiple small gaps.\n\n==Advanced presentation of algorithm==\nScores for aligned characters are specified by a [[similarity matrix]]. Here, {{math|''S''(''a'', ''b'')}} is the similarity of characters ''a'' and ''b''. It uses a linear [[gap penalty]], here called {{mvar|d}}.\n\nFor example, if the similarity matrix was\n{| style=\"font-family: monospace; font-size: 150%;\" cellpadding=\"2\" class=\"wikitable\"\n! scope=\"col\" |\n! scope=\"col\" | A\n! scope=\"col\" | G\n! scope=\"col\" | C\n! scope=\"col\" | T\n|- style=\"text-align: right;\"\n! scope=\"row\" | A\n| 10 || -1 || -3 || -4\n|- style=\"text-align: right;\"\n! scope=\"row\" | G\n| -1 || 7 || -5 || -3\n|- style=\"text-align: right;\"\n! scope=\"row\" | C\n| -3 || -5 || 9 || 0\n|- style=\"text-align: right;\"\n! scope=\"row\" | T\n| -4 || -3 || 0 || 8\n|}\n\nthen the alignment:\n AGACTAGTTAC\n CGA---GACGT\nwith a gap penalty of -5, would have the following score:\n:{{math|''S''(A,C) + ''S''(G,G) + ''S''(A,A) + (3 × ''d'') + ''S''(G,G) + ''S''(T,A) + ''S''(T,C) + ''S''(A,G) + ''S''(C,T)}}\n:= -3 + 7 + 10 - (3 × 5) + 7 + (-4) + 0 + (-1) + 0 = 1\n\nTo find the alignment with the highest score, a two-dimensional [[Array data structure|array]] (or [[Matrix (mathematics)|matrix]]) ''F'' is allocated. The entry in row ''i'' and column ''j'' is denoted here by\n<math>F_{ij}</math>. There is one row for each character in sequence ''A'', and one column for each character in sequence ''B''. Thus, if aligning sequences of sizes ''n'' and ''m'', the amount of memory used is in <math>O(nm)</math>. [[Hirschberg's algorithm]] only holds a subset of the array in memory and uses <math>\\Theta(\\min \\{n,m\\})</math> space, but is otherwise similar to Needleman-Wunsch (and still requires <math>O(nm)</math> time).\n\nAs the algorithm progresses, the <math>F_{ij}</math> will be assigned to be the optimal score for the alignment of the first <math>i=0,\\dotsc,n</math> characters in ''A'' and the first <math>j=0,\\dotsc,m</math> characters in ''B''. The [[Bellman equation#Bellman's Principle of Optimality|principle of optimality]] is then applied as follows:\n* Basis:\n:<math>F_{0j} = d*j</math>\n:<math>F_{i0} = d*i</math>\n* Recursion, based on the principle of optimality:\n:<math>F_{ij} = \\max(F_{i-1,j-1} + S(A_{i}, B_{j}), \\; F_{i,j-1} + d, \\; F_{i-1,j} + d)</math>\n\nThe pseudo-code for the algorithm to compute the F matrix therefore looks like this:\n d ← MismatchScore\n '''for''' i=0 '''to''' '''length'''(A)\n   F(i,0) ← d*i\n '''for''' j=0 '''to''' '''length'''(B)\n   F(0,j) ← d*j\n '''for''' i=1 '''to''' '''length'''(A)\n   '''for''' j=1 '''to''' '''length'''(B)\n   {\n     Match ← F(i-1,j-1) + S(A<sub>i</sub>, B<sub>j</sub>)\n     Delete ← F(i-1, j) + d\n     Insert ← F(i, j-1) + d\n     F(i,j) ← '''max'''(Match, Insert, Delete)\n   }\nOnce the ''F'' matrix is computed, the entry <math>F_{nm}</math> gives the maximum score among all possible alignments. To compute an alignment that actually gives this score, you start from the bottom right cell, and compare the value with the three possible sources (Match, Insert, and Delete above) to see which it came from. If Match, then <math>A_i</math> and <math>B_j</math> are aligned, if Delete, then <math>A_i</math> is aligned with a gap, and if Insert, then <math>B_j</math> is aligned with a gap. (In general, more than one choice may have the same value, leading to alternative optimal alignments.)\n AlignmentA ← \"\"\n AlignmentB ← \"\"\n i ← '''length'''(A)\n j ← '''length'''(B)\n '''while''' (i > 0 '''or''' j > 0)\n {\n   '''if''' (i > 0 '''and''' j > 0 '''and''' F(i,j) == F(i-1,j-1) + S(A<sub>i</sub>, B<sub>j</sub>))\n   {\n     AlignmentA ← A<sub>i</sub> + AlignmentA\n     AlignmentB ← B<sub>j</sub> + AlignmentB\n     i ← i - 1\n     j ← j - 1\n   }\n   '''else''' '''if''' (i > 0 '''and''' F(i,j) == F(i-1,j) + d)\n   {\n     AlignmentA ← A<sub>i</sub> + AlignmentA\n     AlignmentB ← \"-\" + AlignmentB\n     i ← i - 1\n   }\n   '''else'''\n   {\n     AlignmentA ← \"-\" + AlignmentA\n     AlignmentB ← B<sub>j</sub> + AlignmentB\n     j ← j - 1\n   }\n }\n\n== Complexity ==\nComputing the score <math>F_{ij}</math> for each cell in the table is an <math>O(1)</math> operation. Thus the time complexity of the algorithm for two sequences of length <math>n</math> and <math>m</math> is <math>O(mn)</math>.<ref name=\":0\">{{Cite book|title=Algorithms in bioinformatics : a practical introduction|last=Wing-Kin.|first=Sung|date=2010|publisher=Chapman & Hall/CRC Press|isbn=9781420070330|location=Boca Raton|pages=34–35|oclc=429634761}}</ref> It has been shown that it is possible to improve the running time to <math>O(mn/ \\log n)</math> using the [[Method of Four Russians]].<ref name=\":0\" /><ref>{{Cite journal|last=Masek|first=William|last2=Paterson|first2=Michael|date=February 1980|title=A faster algorithm computing string edit distances|url=https://www.sciencedirect.com/science/article/pii/0022000080900021|journal=Journal of Computer and System Sciences|volume=20|pages=18–31|doi=10.1016/0022-0000(80)90002-1|via=Elsevier Science Direct}}</ref> Since the algorithm fill an <math>n \\times m</math> table the space complexity is <math>O(mn)</math>.<ref name=\":0\" />\n\n==Historical notes and algorithm development==\nThe original purpose of the algorithm described by Needleman and Wunsch was to find similarities in the amino acid sequences of two proteins.<ref name=Needleman />\n\nNeedleman and Wunsch describe their algorithm explicitly for the case when the alignment is penalized solely by the matches and mismatches, and gaps have no penalty (''d''=0). The original publication from 1970 suggests the [[recursion]]\n<math>F_{ij} = \\max_{h<i,k<j} \\{ F_{h,j-1}+S(A_{i},B_{j}), F_{i-1,k}+S(A_i,B_j) \\}</math>.\n\nThe corresponding dynamic programming algorithm takes cubic time. The paper also points out that the recursion can accommodate arbitrary gap penalization formulas:\n\n<blockquote>\nA penalty factor, a number subtracted for every gap made, may be assessed as a barrier to allowing the gap. The penalty factor could be a function of the size and/or direction of the gap. [page 444]\n</blockquote>\n\nA better dynamic programming algorithm with quadratic running time for the same problem (no gap penalty) was first introduced<ref name=Sankoff>{{cite journal | doi=10.1073/pnas.69.1.4 | journal=Proceedings of the National Academy of Sciences of the USA | volume=69 | issue=1 | pages=4–6 | year=1972  | author=Sankoff D | title=Matching sequences under deletion/insertion constraints | pmid=4500555 | pmc=427531}}</ref> by David Sankoff in 1972.\nSimilar quadratic-time algorithms were discovered independently\nby T. K. Vintsyuk<ref name=Vintsyuk>{{cite journal | journal=Kibernetika | volume=4 | pages=81–88 | year=1968  | author=Vintsyuk TK | title=Speech discrimination by dynamic programming| doi=10.1007/BF01074755 }}</ref> in 1968 for speech processing\n([[Dynamic time warping|\"time warping\"]]), and by Robert A. Wagner and [[Michael J. Fischer]]<ref name=WagnerFischer>{{cite journal |vauthors=Wagner RA, Fischer MJ | journal = [[Journal of the ACM]] | title=The string-to-string correction problem | volume=21 | issue=1 | year=1974 | pages=168–173 | doi=10.1145/321796.321811}}</ref> in 1974 for string matching.\n\nNeedleman and Wunsch formulated their problem in terms of maximizing similarity. Another possibility is to minimize the [[Levenshtein distance|edit distance]] between sequences, introduced by [[Vladimir Levenshtein]]. Peter H. Sellers showed<ref name=Sellers>{{cite journal | doi=10.1137/0126070 | title=On the theory and computation of evolutionary distances | author=Sellers PH | journal = SIAM Journal on Applied Mathematics | volume = 26 | issue = 4 | pages = 787–793 | year = 1974}}</ref> in 1974 that the two problems are equivalent.\n\nThe Needleman–Wunsch algorithm is still widely used for optimal [[Sequence alignment#Global and local alignments|global alignment]], particularly when the quality of the global alignment is of the utmost importance. However, the algorithm is expensive with respect to time and space, proportional to the product of the length of two sequences and hence is not suitable for long sequences.\n\nRecent development has focused on improving the time and space cost of the algorithm while maintaining quality. For example, in 2013, a Fast Optimal Global Sequence Alignment Algorithm (FOGSAA),<ref>{{cite journal|last1=Chakraborty|first1=Angana|last2=Bandyopadhyay|first2=Sanghamitra|title=FOGSAA: Fast Optimal Global Sequence Alignment Algorithm|journal=Scientific Reports|date=29 April 2013|volume=3|pages=1746|doi=10.1038/srep01746|pmid=23624407|pmc=3638164}}</ref> suggested alignment of nucleotide/protein sequences faster than other optimal global alignment methods, including the Needleman–Wunsch algorithm. The paper claims that when compared to the Needleman–Wunsch algorithm, FOGSAA achieves a time gain of 70–90% for highly similar nucleotide sequences (with > 80% similarity), and 54–70% for sequences having 30–80% similarity.\n\n==Global alignment tools using the Needleman–Wunsch algorithm==\n* [http://www.ebi.ac.uk/Tools/psa EMBOSS Needle and EMBOSS Stretcher Global Alignment Tools]\n* [https://blast.ncbi.nlm.nih.gov/Blast.cgi?PAGE_TYPE=BlastSearch&PROG_DEF=blastn&BLAST_PROG_DEF=blastn&BLAST_SPEC=GlobalAln&LINK_LOC=BlastHomeLink Needleman-Wunsch alignment for two nucleotide sequences]\n* [http://www.mathworks.com.au/help/bioinfo/ref/nwalign.html MathWorks - Globally align two sequences using Needleman-Wunsch algorithm]\n* [https://github.com/bitkeeper-scm/bitkeeper/blob/master/src/libdiff.c#L948 BitKeeper – Source Control Management Software]\n\n==Applications outside bioinformatics==\n\n===[[Computer stereo vision]]===\nStereo matching is an essential step in the process of 3D reconstruction from a pair of stereo images. When images have been rectified, an analogy can be drawn between aligning nucleotide and protein sequences and matching [[pixels]] belonging to [[scan lines]], since both tasks aim at establishing optimal correspondence between two strings of characters. The ‘right’ image of a stereo pair can be seen as a mutated version of the ‘left’ image: noise and individual camera sensitivity alter pixel values (i.e. character substitutions); and different view angle reveals previously occluded data and introduces new occlusions (i.e. insertion and deletion of characters). As consequence, minor modifications of the Needleman–Wunsch algorithm make it suitable for stereo matching.<ref>Dieny R., Thevenon J., Martinez-del-Rincon J., Nebel J.-C. (2011) \"[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.220.9699 Bioinformatics inspired algorithm for stereo correspondence]\". International Conference on Computer Vision Theory and Applications, March 5–7, Vilamoura - Algarve, Portugal.</ref> Although performances in terms of accuracy are not state-of-the-art, the relative simplicity of the algorithm allows its implementation on [[embedded systems]].<ref>Madeo S., Pelliccia R., Salvadori C., Martinez-del-Rincon J., Nebel J.-C. (2014) \"[https://pure.qub.ac.uk/portal/files/12586298/stereovision_end.pdf An optimized stereo vision implementation for embedded systems: application to RGB and Infra-Red images]\". Journal of Real-Time Image Processing.</ref>\n\nAlthough in many applications image rectification can be performed, e.g. by [[camera resectioning]] or calibration, it is sometimes impossible or impractical since the computational cost of accurate rectification models prohibit their usage in [[Real-time computing|real-time]] applications. Moreover, none of these models is suitable when a camera lens displays unexpected [[distortions]], such as those generated by raindrops, weatherproof covers or dust. By extending the Needleman–Wunsch algorithm, a line in the ‘left’ image can be associated to a curve in the ‘right’ image by finding the alignment with the highest score in a three-dimensional array (or matrix). Experiments demonstrated that such extension allows dense pixel matching between unrectified or distorted images.<ref>Martinez-del-Rincon J., Thevenon J., Dieny R., Nebel J.-C. (2012) \"[https://www.researchgate.net/profile/Jean-Christophe_Nebel/publication/257928290_Dense_pixel_matching_between_unrectified_and_distorted_images_using_dynamic_programming/links/0c9605266f2d98c719000000.pdf Dense Pixel Matching Between Unrectified and Distorted Images Using Dynamic Programming]\". International Conference on Computer Vision Theory and Applications, 24–26 February, Rome, Italy.</ref>\n\n==See also==\n* [[Smith–Waterman algorithm]]\n* [[Sequence mining]]\n* [[Levenshtein distance]]\n* [[Dynamic time warping]]\n* [[Sequence alignment]]\n\n==References==\n{{Reflist}}\n\n==External links==\n{{External links|date=May 2017}}\n* [http://zhanglab.ccmb.med.umich.edu/NW-align NW-align: A protein sequence-to-sequence alignment program by Needleman-Wunsch algorithm (online server and source code)]\n* [[:File:ParallelNeedlemanAlgorithm.pdf | Parallel Needleman-Wunsch Algorithm for Grid]] - Implementation by Tahir Naveed, Imitaz Saeed Siddiqui and Shaftab Ahmed - Bahria University\n* [http://chneukirchen.org/blog/archive/2006/03/dynamic-programming-in-haskell.html Needleman-Wunsch Algorithm as Haskell Code]\n* [http://ds9a.nl/nwunsch A live Javascript-based demo of Needleman–Wunsch]\n* [http://alfehrest.org/sub/nwa/index.html An interactive Javascript-based visual explanation of Needleman-Wunsch Algorithm]\n* [http://baba.sourceforge.net/ B.A.B.A.] — an applet (with source) which visually explains the algorithm.\n* [https://web.archive.org/web/20140729133238/http://www.science.marshall.edu/murraye/Clearer%20Matrix%20slide%20show.pdf A clear explanation of NW and its applications to sequence alignment]\n* [http://technology66.blogspot.com/2008/08/sequence-alignment-techniques.html Sequence Alignment Techniques at Technology Blog]\n* [https://web.archive.org/web/20091106224548/http://svitsrv25.epfl.ch/R-doc/library/Biostrings/html/00Index.html Biostrings] R package implementing Needleman–Wunsch algorithm among others\n* [https://gist.github.com/jonasmalacofilho/5226596 Needleman-Wunsch Algorithm as Haxe Code]\n\n{{Strings}}\n\n{{DEFAULTSORT:Needleman-Wunsch Algorithm}}\n[[Category:Bioinformatics algorithms]]\n[[Category:Sequence alignment algorithms]]\n[[Category:Computational phylogenetics]]\n[[Category:Dynamic programming]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Optimal stopping",
      "url": "https://en.wikipedia.org/wiki/Optimal_stopping",
      "text": "In [[mathematics]], the theory of '''optimal stopping'''<ref name=\"ChowRobSig1971\">{{Cite book | last = Chow | first=Y.S. | last2 =Robbins | first2 =H. |  authorlink2 = Herbert Robbins | last3 = Siegmund | first3 = D. | doi = | title = Great Expectations: The Theory of Optimal Stopping | year = 1971 | publisher = [[Houghton Mifflin]] | location =  Boston | isbn = | pmid =  | pmc = |}}</ref> <ref name=\"Ferguson2007\">{{Cite book | last = [http://www.math.ucla.edu/people/ladder/tom Ferguson, Thomas S. ] | first = | authorlink1 =  | doi = | title = Optimal Stopping and Applications | year = 2007 | publisher = UCLA | isbn = | pmid =  | pmc = | url = http://www.math.ucla.edu/~tom/Stopping/Contents.html }}</ref> or '''early stopping'''<ref name=\"Hill2009\">{{Cite journal | first1 = Theodore P. | last1 = Hill| authorlink1 = Theodore P. Hill | doi = 10.1511/2009.77.126 | title = Knowing When to Stop | journal = [[American Scientist]] | year = 2009 | volume = 97 | pages = 126-133 | issn = 1545-2786  | pmid =  | pmc = | via = (For French translation, see [https://web.archive.org/web/20110721020330/http://www.pourlascience.fr/ewb_pages/f/fiche-article-savoir-quand-s-arreter-22670.php cover story] in the July issue of ''Pour la Science'' (2009)) }}</ref> is concerned with the problem of choosing a time to take a particular action, in order to [[Optimization (mathematics)|maximise]] an expected reward or minimise an expected cost. Optimal stopping problems can be found in areas of [[statistics]], [[economics]], and [[mathematical finance]] (related to the pricing of [[American options]]). A key example of an optimal stopping problem is the [[secretary problem]]. Optimal stopping problems can often be written in the form of a [[Bellman equation]], and are therefore often solved using [[dynamic programming]].\n\n==Definition==\n\n===Discrete time case===\n\nStopping rule problems are associated with two objects:\n\n#A sequence of random variables <math>X_1, X_2, \\ldots</math>, whose joint distribution is something assumed to be known\n#A sequence of 'reward' functions <math>(y_i)_{i\\ge 1}</math> which depend on the observed values of the random variables in 1.:\n#:<math>y_i=y_i (x_1, \\ldots ,x_i)</math>\n\nGiven those objects, the problem is as follows:\n* You are observing the sequence of random variables, and at each step <math>i</math>, you can choose to either stop observing or continue\n* If you stop observing at step <math>i</math>, you will receive reward <math>y_i</math>\n* You want to choose a [[stopping rule]] to maximize your expected reward (or equivalently, minimize your expected loss)\n\n===Continuous time case===\n\nConsider a gain processes <math>G=(G_t)_{t\\ge 0}</math> defined on a [[filtered probability space]] <math>(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\ge 0},\\mathbb{P})</math> and assume that <math>G</math> is [[adapted process|adapted]] to the filtration. The optimal stopping problem is to find the [[stopping time]] <math>\\tau^*</math> which maximizes the expected gain\n:<math> V_t^T = \\mathbb{E} G_{\\tau^*} = \\sup_{t\\le \\tau \\le T} \\mathbb{E} G_\\tau </math>\nwhere <math>V_t^T</math> is called the [[value function]]. Here <math>T</math> can take value <math>\\infty</math>.\n\nA more specific formulation is as follows. We consider an adapted strong [[Markov process]] <math>X = (X_t)_{t\\ge 0}</math> defined on a filtered probability space <math>(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\ge 0},\\mathbb{P}_x)</math> where <math>\\mathbb{P}_x</math> denotes the [[probability measure]] where the [[stochastic process]] starts at <math>x</math>. Given continuous functions <math>M,L</math>, and <math>K</math>, the optimal stopping problem is \n:<math> V(x) = \\sup_{0\\le \\tau \\le T} \\mathbb{E}_x \\left( M(X_\\tau) + \\int_0^\\tau L(X_t) dt + \\sup_{0\\le t\\le\\tau} K(X_t) \\right). </math>\nThis is sometimes called the MLS (which stand for Mayer, Lagrange, and supremum, respectively) formulation.<ref name=\"opt2006\">{{Cite journal | first1 = Goran | last1 = Peskir| first2 = Albert | last2 = Shiryaev | authorlink2 = Albert Shiryaev\n| doi = 10.1007/978-3-7643-7390-0 | title = Optimal Stopping and Free-Boundary Problems | series = Lectures in Mathematics. ETH Zürich | year = 2006 | isbn = 978-3-7643-2419-3 | pmid =  | pmc = }}</ref>\n\n==Solution methods==\nThere are generally two approaches to solving optimal stopping problems.<ref name=opt2006/> When the underlying process (or the gain process) is described by its unconditional [[finite-dimensional distribution]]s, the appropriate solution technique is the martingale approach, so called because it uses [[Martingale (probability theory)|martingale]] theory, the most important concept being the [[Snell envelope]]. In the discrete time case, if the planning horizon <math>T</math> is finite, the problem can also be easily solved by [[dynamic programming]].\n\nWhen the underlying process is determined by a family of (conditional) transition functions leading to a Markov family of transition probabilities, powerful analytical tools provided by the theory of [[Markov process]]es can often be utilized and this approach is referred to as the Markov method. The solution is usually obtained by solving the associated [[Free boundary problem|free-boundary problems]] ([[Stefan problem]]s).\n\n==A jump diffusion result==\nLet <math>Y_t</math> be a [[Lévy process|Lévy]] diffusion in <math>\\mathbb{R}^k</math> given by the [[Stochastic differential equation|SDE]]\n:<math> dY_t = b(Y_t) dt + \\sigma (Y_t) dB_t + \\int_{\\mathbb{R}^k} \\gamma (Y_{t-},z)\\bar{N}(dt,dz),\\quad Y_0 = y </math> \nwhere <math> B </math> is an  <math> m</math>-dimensional [[Brownian motion]], <math> \\bar{N} </math> is an <math> l </math>-dimensional compensated [[Poisson random measure]], <math> b:\\mathbb{R}^k \\to \\mathbb{R}^k </math>, <math> \\sigma:\\mathbb{R}^k \\to \\mathbb{R}^{k\\times m} </math>, and <math> \\gamma:\\mathbb{R}^k \\times \\mathbb{R}^k \\to \\mathbb{R}^{k\\times l} </math> are given functions such that a unique solution <math> (Y_t) </math> exists. Let <math> \\mathcal{S}\\subset \\mathbb{R}^k </math> be an open set (the solvency region) and \n:<math> \\tau_\\mathcal{S} = \\inf\\{ t>0: Y_t \\notin \\mathcal{S} \\} </math>\nbe the bankruptcy time. The optimal stopping problem is:\n:<math>V(y) = \\sup_{\\tau \\le \\tau_\\mathcal{S}} J^\\tau (y) = \\sup_{\\tau \\le \\tau_\\mathcal{S}} \\mathbb{E}_y \\left[ M(Y_\\tau) + \\int_0^\\tau L(Y_t) dt \\right]. </math>\nIt turns out that under some regularity conditions,<ref name=\"oksendal2007\">{{Cite journal | last1 = Øksendal | first1 = B. | authorlink1 = Bernt Øksendal| last2 = Sulem | first2 = A. S. | doi = 10.1007/978-3-540-69826-5 | title = Applied Stochastic Control of Jump Diffusions | year = 2007 | isbn = 978-3-540-69825-8 | pmid =  | pmc = }}</ref> the following verification theorem holds:\n\nIf a function <math>\\phi:\\bar{\\mathcal{S}}\\to \\mathbb{R}</math> satisfies\n* <math> \\phi \\in C(\\bar{\\mathcal{S}}) \\cap C^1(\\mathcal{S}) \\cap C^2(\\mathcal{S}\\setminus \\partial D) </math> where the continuation region is <math> D = \\{y\\in\\mathcal{S}: \\phi(y) > M(y) \\} </math>, \n* <math> \\phi \\ge M </math> on <math> \\mathcal{S} </math>, and\n* <math> \\mathcal{A}\\phi + L \\le 0 </math> on <math> \\mathcal{S} \\setminus \\partial D </math>, where <math> \\mathcal{A} </math> is the [[Infinitesimal generator (stochastic processes)|infinitesimal generator]] of <math> (Y_t) </math> \nthen <math> \\phi(y) \\ge V(y) </math> for all <math> y\\in \\bar{\\mathcal{S}} </math>. Moreover, if\n* <math> \\mathcal{A}\\phi + L = 0 </math> on <math> D </math>\nThen <math> \\phi(y) = V(y) </math> for all <math> y\\in \\bar{\\mathcal{S}} </math> and <math> \\tau^* = \\inf\\{ t>0: Y_t\\notin D\\} </math> is an optimal stopping time.\n\nThese conditions can also be written is a more compact form (the [[Variational inequality|integro-variational inequality]]):\n* <math> \\max\\left\\{ \\mathcal{A}\\phi + L, M-\\phi \\right\\} = 0 </math> on <math> \\mathcal{S} \\setminus \\partial D. </math>\n\n==Examples==\n\n=== Coin tossing ===\n(Example where <math>\\mathbb{E}(y_i)</math> converges)\n\nYou have a fair coin and are repeatedly tossing it. Each time, before it is tossed, you can choose to stop tossing it and get paid (in dollars, say) the average number of heads observed.\n\nYou wish to maximise the amount you get paid by choosing a stopping rule.\nIf ''X''<sub>''i''</sub> (for ''i'' ≥ 1) forms a sequence of independent, identically distributed random variables with [[Bernoulli distribution]]\n: <math>\\text{Bern}\\left(\\frac{1}{2}\\right),</math>\nand if\n: <math>y_i = \\frac 1 i \\sum_{k=1}^{i} X_k</math>\nthen the sequences <math>(X_i)_{i\\geq 1}</math>, and <math>(y_i)_{i\\geq 1}</math> are the objects associated with this problem.\n\n=== House selling ===\n(Example where <math>\\mathbb{E}(y_i)</math> does not necessarily converge)\n\nYou have a house and wish to sell it. Each day you are offered <math>X_n</math> for your house, and pay <math>k</math> to continue advertising it. If you sell your house on day <math>n</math>, you will earn <math>y_n</math>, where <math>y_n = (X_n - nk)</math>.\n\nYou wish to maximise the amount you earn by choosing a stopping rule.\n\nIn this example, the sequence (<math>X_i</math>) is the sequence of offers for your house, and the sequence of reward functions is how much you will earn.\n\n=== Secretary problem ===\n{{Main|Secretary problem}}\n(Example where <math>(X_i)</math> is a finite sequence)\n\nYou are observing a sequence of objects which can be ranked from best to worst. You wish to choose a stopping rule which maximises your chance of picking the best object.\n\nHere, if <math>R_1, \\ldots, R_n</math> (''n'' is some large number) are the ranks of the objects, and <math>y_i</math> is the chance you pick the best object if you stop intentionally rejecting objects at step i, then <math>(R_i)</math> and <math>(y_i)</math> are the sequences associated with this problem. This problem was solved in the early 1960s by several people. An elegant solution to the secretary problem and several modifications of this problem is provided by the more recent [[odds algorithm]]\nof optimal stopping (Bruss algorithm).\n\n=== Search theory ===\n{{Main|Search theory}}\nEconomists have studied a number of optimal stopping problems similar to the 'secretary problem', and typically call this type of analysis 'search theory'. Search theory has especially focused on a worker's search for a high-wage job, or a consumer's search for a low-priced good.\n\n=== Parking Problem ===\n{{Main|Parking problem}}\nA special example of the task of searching theory is the task of optimal selection of parking space by the driver who went to the opera (theater or to do shopping). Approaching the destination of your trip, he goes down the street along which there are parking spaces - usually, only some places in the parking lot are free. The goal is clearly visible, so it easily assesses its distance from the target. The driver's task is to choose a free parking space as close to the destination as possible without turning around so that the distance from this place to the destination is the shortest.<ref name=\"MacQueenMiller1960\">{{cite journal |first=J. |last=MacQueen |first2=R.G. |last2=Miller Jr. |year=1960 |title=Optimal persistence policies |journal=[[Operations Research]] |volume=8 |issue=3 |pages=362-380 |doi=10.1287/opre.8.3.362 |issn = 0030-364X| pmid =  | pmc = }}</ref>\n\n=== Option trading ===\nIn the trading of [[Option (finance)|options]] on [[financial market]]s, the holder of an [[Option style|American option]] is allowed to exercise the right to buy (or sell) the underlying asset at a predetermined price at any time before or at the expiry date. Therefore, the valuation of American options is essentially an optimal stopping problem. Consider a classical [[Black-Scholes]] set-up and let <math> r </math> be the [[risk-free interest rate]] and <math> \\delta </math> and <math> \\sigma </math> be the dividend rate and volatility of the stock. The stock price <math> S </math> follows geometric Brownian motion\n:<math> S_t = S_0 \\exp\\left\\{ \\left(r - \\delta - \\frac{\\sigma^2}{2}\\right) t + \\sigma B_t \\right\\} </math>\nunder the risk-neutral measure. \n\nWhen the option is perpetual, the optimal stopping problem is\n:<math> V(x) = \\sup_{\\tau} \\mathbb{E}_x \\left[ e^{-r\\tau} g(S_\\tau) \\right] </math>\nwhere the payoff function is <math> g(x) = (x-K)^+ </math> for a call option and <math> g(x) = (K-x)^+ </math> for a put option. The variational inequality is\n:<math> \\max\\left\\{ \\frac{1}{2} \\sigma^2 x^2 V''(x) + (r-\\delta) x V'(x) - rV(x), g(x) - V(x) \\right\\} = 0</math>\nfor all <math>x \\in (0,\\infty)\\setminus \\{b\\}</math>\nwhere <math> b </math> is the exercise boundary. The solution is known to be<ref name=\"karatzas1998\">{{Cite journal | first1 = Ioannis| last1 = Karatzas| first2 = Steven E.| last2 = Shreve| authorlink2 = Steven E. Shreve| doi = 10.1007/b98840 | title = Methods of Mathematical Finance | series = Stochastic Modelling and Applied Probability | volume = 39 | year = 1998 | isbn = 978-0-387-94839-3 | pmid =  | pmc = }}</ref>\n* (Perpetual call) <math> V(x) = \\begin{cases} (b-K)(x/b)^\\gamma & x\\in(0,b) \\\\ x-K & x\\in[b,\\infty)  \\end{cases} </math> where <math> \\gamma = (\\sqrt{\\nu^2 + 2r} - \\nu) / \\sigma</math> and <math> \\nu = (r-\\delta)/\\sigma - \\sigma / 2, \\quad b = \\gamma K / (\\gamma - 1). </math>\n* (Perpetual put) <math> V(x) = \\begin{cases} K - x & x\\in(0,c] \\\\(K-c)(x/c)^\\tilde{\\gamma} & x\\in(c,\\infty)  \\end{cases} </math> where <math> \\tilde{\\gamma} = -(\\sqrt{\\nu^2 + 2r} + \\nu) / \\sigma </math> and <math> \\nu = (r-\\delta)/\\sigma - \\sigma / 2, \\quad c = \\tilde{\\gamma} K / (\\tilde{\\gamma} - 1). </math>\n\nOn the other hand, when the expiry date is finite, the problem is associated with a 2-dimensional free-boundary problem with no known closed-form solution. Various numerical methods can, however, be used. See [[Black–Scholes model #American options]] for various valuation methods here, as well as [[Fugit]] for a discrete, [[Lattice model (finance)|tree based]], calculation of the optimal time to exercise.\n\n==See also==\n*[[Halting problem]]\n*[[Optional stopping theorem]]\n*[[Markov decision process]]\n*[[Stochastic control]]\n\n==References==\n{{Reflist}}\n* Thomas S. Ferguson.<ref>[http://www.math.ucla.edu/people/ladder/tom UCLA Department of Mathematics: Thomas Ferguson]</ref> [http://www.math.ucla.edu/~tom/Stopping/Contents.html Optimal Stopping and Applications], retrieved on 21 June 2007\n* Thomas S. Ferguson. \"Who solved the secretary problem?\" ''Statistical Science'', Vol. 4.,282-296, (1989)\n* [[F. Thomas Bruss]]. \"Sum the odds to one and stop.\" ''Annals of Probability'', Vol. 28, 1384–1391,(2000)\n* F. Thomas Bruss. \"The art of a right decision: Why decision makers want to know the odds-algorithm.\" ''[[Newsletter of the European Mathematical Society]]'', Issue 62, 14-20, (2006)\n* {{cite journal |first=R. |last=Rogerson |first2=R. |last2=Shimer |first3=R. |last3=Wright |year=2005 |title=Search-theoretic models of the labor market: a survey |journal=[[Journal of Economic Literature]] |volume=43 |issue=4 |pages=959–88 |jstor=4129380 }}\n{{Use dmy dates|date=September 2010}}\n\n==External links==\n* [http://www.spotlightmind.com/optimal-search Neil Bearden's Optimal Search Page]\n\n{{DEFAULTSORT:Optimal Stopping}}\n[[Category:Mathematical finance]]\n[[Category:Sequential methods]]\n[[Category:Dynamic programming]]"
    },
    {
      "title": "Optimal substructure",
      "url": "https://en.wikipedia.org/wiki/Optimal_substructure",
      "text": "[[Image:Shortest path optimal substructure.svg|200px|thumb|'''Figure 1'''. Finding the shortest path using optimal substructure.  Numbers represent the length of the path; straight lines indicate single [[Edge (graph theory)|edges]], wavy lines indicate shortest [[Path (graph theory)|paths]], i.e., there might be other vertices that are not shown here.]]\nIn [[computer science]], a problem is said to have '''optimal substructure''' if an optimal solution can be constructed from optimal solutions of its subproblems. This property is used to determine the usefulness of dynamic programming and greedy algorithms for a problem.<ref name=cormen>{{cite book|title=Introduction to Algorithms |edition=3rd|last1=Cormen|first1=Thomas H. |last2=Leiserson |first2=Charles E. |last3=Rivest|first3=Ronald L. |last4= Stein |first4=Clifford|date=2009 |isbn=978-0-262-03384-8|publisher=[[MIT Press]]|authorlink1=Thomas H. Cormen |authorlink2=Charles E. Leiserson|authorlink3=Ron Rivest |authorlink4=Clifford Stein}}</ref>  \n\nTypically, a [[greedy algorithm]] is used to solve a problem with optimal substructure if it can be proven by induction that this is optimal at each step.<ref name=cormen /> Otherwise, provided the problem exhibits [[overlapping subproblem]]s as well, [[dynamic programming]] is used. If there are no appropriate greedy algorithms and the problem fails to exhibit overlapping subproblems, often a lengthy but straightforward search of the solution space is the best alternative.\n<!-- A special case of optimal substructure is the case where a subproblem S<sub>ab</sub> has an activity P<sub>y</sub>, then it should contain optimal solutions to subproblems S<sub>ay</sub> and S<sub>yb</sub>. --> <!-- *TODO: Add Recursion, misc. -->\n\nIn the application of [[dynamic programming]] to [[Optimization (mathematics)|mathematical optimization]], [[Richard Bellman]]'s [[Principle of optimality|Principle of Optimality]] is based on the idea that in order to solve a dynamic optimization problem from some starting period ''t'' to some ending period ''T'', one implicitly has to solve subproblems starting from later dates ''s'', where ''t<s<T''. This is an example of optimal substructure. The Principle of Optimality is used to derive the [[Bellman equation]], which shows how the value of the problem starting from ''t'' is related to the value of the problem starting from ''s''.\n\n==Example==\nConsider finding a [[Shortest path problem|shortest path]] for travelling between two cities by car, as illustrated in Figure 1. Such an example is likely to exhibit optimal substructure. That is, if the shortest route from Seattle to Los Angeles passes through Portland and then Sacramento, then the shortest route from Portland to Los Angeles must pass through Sacramento too. That is, the problem of how to get from Portland to Los Angeles is nested inside the problem of how to get from Seattle to Los Angeles. (The wavy lines in the graph represent solutions to the subproblems.)\n\nAs an example of a problem that is unlikely to exhibit optimal substructure, consider the problem of finding the cheapest airline ticket from Buenos Aires to Kyiv. Even if that ticket involves stops in Miami and then London, we can't conclude that the cheapest ticket from Miami to Kyiv stops in London, because the price at which an airline sells a multi-flight trip is usually not the sum of the prices at which it would sell the individual flights in the trip.\n\n==Definition==\nA slightly more formal definition of optimal substructure can be given.  Let a \"problem\" be a collection of \"alternatives\", and let each alternative have an associated cost, ''c(a)''.  The task is to find a set of alternatives that minimizes ''c(a)''.  Suppose that the alternatives can be [[Partition of a set|partitioned]] into subsets, i.e. each alternative belongs to only one subset. Suppose each subset has its own cost function. The minima of each of these cost functions can be found, as can the minima of the global cost function, ''restricted to the same subsets''.  If these minima match for each subset, then it's almost obvious that a global minimum can be picked not out of the full set of alternatives, but out of only the set that consists of the minima of the smaller, local cost functions we have defined.  If minimizing the local functions is a problem of \"lower order\", and (specifically) if, after a finite number of these reductions, the problem becomes trivial, then the problem has an optimal substructure.\n\n== Problems with optimal substructure ==\n* [[Longest common subsequence problem]]\n* [[Longest increasing subsequence]]\n* [[Longest palindromic substring]]\n* [[Shortest_path_problem#All-pairs_shortest_paths|All-Pairs Shortest Path]]\n* Any problem that can be solved by [[dynamic programming]].\n\n== Problems ''without'' optimal substructure ==\n* [[Longest path problem]]\n* '''Least-cost airline fare.'''  (Using online flight search, we will frequently find that the cheapest flight from airport A to airport B involves a single connection through airport C, but the cheapest flight from airport A to airport C involves a connection through some other airport D.)\n\n== See also ==\n* [[Dynamic Programming]]\n* [[Principle of optimality]]\n\n== References ==\n<references />\n\n[[Category:Dynamic programming]]"
    },
    {
      "title": "Overlapping subproblems",
      "url": "https://en.wikipedia.org/wiki/Overlapping_subproblems",
      "text": "In [[computer science]], a [[problem]] is said to have '''overlapping subproblems''' if the problem can be broken down into subproblems which are reused several times or a recursive algorithm for the problem solves the same subproblem over and over rather than always generating new subproblems.<ref>[https://books.google.com/books?id=NLngYyWFl_YC&pg=PA15&dq=introduction+to+algorithms&psp=1&sig=jX-xfEDWJU3PprUwH8Qfxidli6M#PPP1,M1 Introduction to Algorithms], 2nd ed., (Cormen, Leiserson, Rivest, and Stein) 2001, p. 327. {{ISBN|0-262-03293-7}}.</ref><ref>[https://books.google.com/books?id=jUF9BAAAQBAJ&dq=introduction+to+algorithms+3rd+edition&hl=en&sa=X&ved=0CD8Q6AEwAmoVChMIjZyx4N7HxwIVjDs-Ch1mjAqh,M1 Introduction to Algorithms], 3rd ed., (Cormen, Leiserson, Rivest, and Stein) 2014, p. 384. {{ISBN|9780262033848}}.</ref>\n<ref>[https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-00-introduction-to-computer-science-and-programming-fall-2008/video-lectures/lecture-13/ Dynamic Programming: Overlapping Subproblems, Optimal Substructure], MIT Video.</ref>\n\nFor example, the problem of computing the [[Fibonacci sequence]] exhibits overlapping subproblems.  The problem of computing the ''n''th [[Fibonacci number]] ''F''(''n''), can be broken down into the subproblems of computing ''F''(''n''&nbsp;&minus;&nbsp;1) and ''F''(''n''&nbsp;&minus;&nbsp;2), and then adding the two.  The subproblem of computing ''F''(''n''&nbsp;&minus;&nbsp;1) can itself be broken down into a subproblem that involves computing&nbsp;''F''(''n''&nbsp;&minus;&nbsp;2).  Therefore, the computation of ''F''(''n''&nbsp;&minus;&nbsp;2) is reused, and the Fibonacci sequence thus exhibits overlapping subproblems.\n\nA naive [[recursive]] approach to such a problem generally fails due to an [[Exponential time|exponential complexity]]. If the problem also shares an [[optimal substructure]] property, [[dynamic programming]] is a good way to work it out.\n\n==Fibonacci Sequence Example in C==\n\nConsider the following [[C (programming language)|C]] code:\n<source lang=c>#include <stdio.h>\n\n#define N 5\n\nstatic int fibMem[N];\n\nint fibonacci(int n) {\n\tint r = 1;\n\tif(n > 2) {\n\t\tr = fibonacci(n - 1) + fibonacci(n - 2);\n\t}\n\tfibMem[n - 1] = r;\n\treturn r;\n}\n\nvoid printFibonacci() {\n    int i;\n    for(i = 1; i <= N; i++) {\n        printf(\"fibonacci(%d): %d\\n\", i, fibMem[i - 1]);\n    }\n}\n\nint main(void) {\n    fibonacci(N);\n\tprintFibonacci();\n\treturn 0;\n}\n\n/* Output:\n    fibonacci(1): 1\n    fibonacci(2): 1\n    fibonacci(3): 2\n    fibonacci(4): 3\n    fibonacci(5): 5 */</source>\nWhen executed, the <code>fibonacci</code> function computes the value of some of the numbers in the sequence many times over, following a pattern which can be visualized by this diagram:\n<source lang=text>f(5) = f(4) + f(3) = 5\n       |      |\n       |      f(3) = f(2) + f(1) = 2\n       |             |      |\n       |             |      f(1) = 1\n       |             |\n       |             f(2) = 1\n       |\n       f(4) = f(3) + f(2) = 3\n              |      |\n              |      f(2) = 1\n              |\n              f(3) = f(2) + f(1) = 2\n                     |      |\n                     |      f(1) = 1\n                     |\n                     f(2) = 1</source>\nHowever, we can take advantage of [[memoization]] and change the <code>fibonacci</code> function to make use of <code>fibMem</code> like so:\n<source lang=c>int fibonacci(int n) {\n\tint r = 1;\n\tif(fibMem[n - 1] != 0) {\n\t\tr = fibMem[n - 1];\n\t} else {\n\t\tif(n > 2) {\n\t\t\tr = fibonacci(n - 1) + fibonacci(n - 2);\n\t\t}\n\t\tfibMem[n - 1] = r;\n\t}\n\treturn r;\n}</source>\nThis is much more efficient because if the value <code>r</code> has already been calculated for a certain <code>n</code> and stored in <code>fibMem[n - 1]</code>, the function can just return the stored value rather than making more recursive function calls. This results in a pattern which can be visualized by this diagram:\n<source lang=text>f(5) = f(4) + f(3) = 5\n       |      |\n       f(4) = f(3) + f(2) = 3\n              |      |\n              f(3) = f(2) + f(1) = 2\n                     |      |\n                     |      f(1) = 1\n                     |\n                     f(2) = 1</source>\nThe difference may not seem too significant with an <code>N</code> of 5, but as its value increases, the complexity of the original <code>fibonacci</code> function increases exponentially, whereas the revised version increases more linearly.\n\n==See also==\n*[[Dynamic programming]]\n\n== References ==\n<references />\n\n{{DEFAULTSORT:Overlapping Subproblem}}\n[[Category:Dynamic programming]]"
    },
    {
      "title": "Partially observable Markov decision process",
      "url": "https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process",
      "text": "A '''partially observable Markov decision process''' ('''POMDP''') is a generalization of a [[Markov decision process]] (MDP). A POMDP models an agent decision process in which it is assumed that the system dynamics are determined by an MDP, but the agent cannot directly observe the underlying state. Instead, it must maintain a probability distribution over the set of possible states, based on a set of observations and observation probabilities, and the underlying MDP.\n\nThe POMDP framework is general enough to model a variety of real-world sequential decision processes. Applications include robot navigation problems, machine maintenance, and planning under uncertainty in general. The general framework of Markov decision processes with incomplete information was described by Karl Johan Åström in 1965 <ref>{{cite journal  |author=Åström, K.J.  |title=Optimal control of Markov processes with incomplete state information |journal=Journal of Mathematical Analysis and Applications |volume=10 |pages=174–205 |year=1965|url=https://www.sciencedirect.com/science/article/pii/0022247X6590154X|doi=10.1016/0022-247X(65)90154-X }}</ref> in the case of a discrete state space, and it was further studied in the [[operations research]] community where the acronym POMDP was coined. It was later adapted for problems in [[artificial intelligence]] and [[automated planning]] by [[Leslie P. Kaelbling]] and [[Michael L. Littman]].<ref name=\"Kaelbling98\">{{cite journal |doi=10.1016/S0004-3702(98)00023-X |author=Kaelbling, L.P., Littman, M.L., Cassandra, A.R. |title=Planning and acting in partially observable stochastic domains |journal=Artificial Intelligence |volume=101 |issue=1–2 |pages=99–134 |year=1998 }}</ref>\n\nAn exact solution to a POMDP yields the optimal action for each possible belief over the world states. The optimal action maximizes (or minimizes) the expected reward (or cost) of the agent over a possibly infinite horizon. The sequence of optimal actions is known as the optimal policy of the agent for interacting with its environment.\n\n==Definition==\n\n===Formal definition===\nA discrete-time POMDP models the relationship between an agent and its environment. Formally, a POMDP is a 7-tuple <math>(S,A,T,R,\\Omega,O,\\gamma)</math>, where\n* <math>S</math> is a set of states,\n* <math>A</math> is a set of actions,\n* <math>T</math> is a set of conditional transition probabilities between states,\n* <math>R: S \\times A \\to \\mathbb{R}</math> is the reward function.\n* <math>\\Omega</math> is a set of observations,\n* <math>O</math> is a set of conditional observation probabilities, and\n* <math>\\gamma \\in [0, 1]</math> is the discount factor.\n\nAt each time period, the environment is in some state <math>s \\in S</math>.  The agent takes an action <math>a \\in A</math>,\nwhich causes the environment to transition to state <math>s'</math> with probability <math>T(s'\\mid s,a)</math>. At the same time, the agent receives an observation <math>o \\in \\Omega</math> which depends on the new state of the environment, s', and on the just taken action, a, with probability <math>O(o \\mid s',a)</math>. Finally, the agent receives a reward <math>r</math> equal to <math>R(s, a)</math>. Then the process repeats. The goal is for the agent to choose actions at each time step that maximize its expected future discounted reward: <math>E \\left[ \\sum_{t=0}^\\infty \\gamma^t r_t \\right]</math>, where <math>r_t</math> is the reward earned at time <math>t</math>. The discount factor <math>\\gamma</math> determines how much immediate rewards are favored over more distant rewards. When <math>\\gamma=0</math> the agent only cares about which action will yield the largest expected immediate reward; when <math>\\gamma=1</math> the agent cares about maximizing the expected sum of future rewards.\n\n===Discussion===\nBecause the agent does not directly observe the environment's state, the agent must make decisions under uncertainty of the true environment state. However, by interacting with the environment and receiving observations, the agent may update its belief in the true state by updating the probability distribution of the current state. A consequence of this property is that the optimal behavior may often include (information gathering) actions that are taken purely because they improve the agent's estimate of the current state, thereby allowing it to make better decisions in the future.\n\nIt is instructive to compare the above definition with the definition of a [[Markov decision process#Definition|Markov decision process]]. An MDP does not include the observation set, because the agent always knows with certainty the environment's current state. Alternatively, an MDP can be reformulated as a POMDP by setting the observation set to be equal to the set of states and defining the observation conditional probabilities to deterministically select the observation that corresponds to the true state.\n\n==Belief update==\n\nAfter having taken the action <math>a</math> and observing <math>o</math>, an agent needs to update its belief in the state the environment may (or not) be in. Since the state is Markovian (by assumption), maintaining a belief over the states solely requires knowledge of the previous belief state, the action taken, and the current observation. The operation is denoted <math>b' = \\tau(b,a,o)</math>. Below we describe how this belief update is computed.\n\nAfter reaching <math>s'</math>, the agent observes <math>o \\in \\Omega</math> with probability <math>O(o\\mid s',a)</math>. Let <math>b</math> be a probability distribution over the state space <math>S</math>. <math>b(s)</math> denotes the probability that the environment is in state <math>s</math>. Given <math>b(s)</math>, then after taking action <math>a</math> and observing <math>o</math>,\n:<math>\nb'(s') = \\eta O(o\\mid s',a) \\sum_{s\\in S} T(s'\\mid s,a)b(s)\n</math>\nwhere <math>\\eta=1/\\Pr(o\\mid b,a)</math> is a normalizing constant with <math>\\Pr(o\\mid b,a) = \\sum_{s'\\in S}O(o\\mid s',a)\\sum_{s\\in S}T(s'\\mid s,a)b(s)</math>.\n\n==Belief MDP==\nA Markovian belief state allows a POMDP to be formulated as a [[Markov decision process]] where every belief is a state. The resulting ''belief MDP'' will thus be defined on a continuous state space (even if the \"originating\" POMDP has a finite number of states: there are infinite belief states (in <math>B</math>) because there are an infinite number of mixtures of the originating states (of <math>S</math>)), since there are infinite beliefs for any given POMDP.<ref name=\"Kaelbling98\" />\n\nFormally, the belief MDP is defined as a tuple <math>(B,A,\\tau,r,\\gamma)</math> where\n\n* <math>B</math> is the set of belief states over the POMDP states,\n* <math>A</math> is the same finite set of action as for the original POMDP,\n* <math>\\tau</math> is the belief state transition function,\n* <math>r:B \\times A \\to \\mathbb{R}</math> is the reward function on belief states,\n* <math>\\gamma</math> is the discount factor equal to the <math>\\gamma</math> in the original POMDP.\n\nOf these, <math>\\tau</math> and <math>r</math> need to be derived from the original POMDP. <math>\\tau</math> is\n\n<math>\\tau(b,a,b') = \\sum_{o\\in \\Omega} \\Pr(b'|b,a,o) \\Pr(o | a, b),</math>\n\nwhere <math>\\Pr(o | a,b)</math> is the value derived in the previous section and\n\n<math>Pr(b'|b,a,o) = \\begin{cases}\n1 &\\text{if the belief update with arguments } b,a,o \\text{ returns } b'  \\\\\n0 &\\text{otherwise }  \\end{cases}.</math>\n\nThe belief MDP reward function (<math>r</math>) is the expected reward from the POMDP reward function over the belief state distribution:\n\n<math>r(b,a) = \\sum_{s\\in S} b(s) R(s,a)</math>.\n\nThe belief MDP is not partially observable anymore, since at any given time the agent knows its belief, and by extension the state of the belief MDP.\n\n===Policy and value function===\nUnlike the \"originating\" POMDP (where each action is available from only one state), in the corresponding Belief MDP all belief states allow all actions, since you (almost) always have ''some'' probability of believing you are in any (originating) state. As such, <math>\\pi</math> specifies an action <math>a=\\pi(b)</math> for any belief <math>b</math>.\n\nHere it is assumed the objective is to maximize the expected total discounted reward over an infinite horizon. When <math>R</math> defines a cost, the objective becomes the minimization of the expected cost.\n\nThe expected reward for policy <math>\\pi</math> starting from belief <math>b_0</math> is defined as\n:<math>\nV^\\pi(b_0) = \\sum_{t=0}^\\infty  \\gamma^t r(b_t, a_t) = \\sum_{t=0}^\\infty \\gamma^t E\\Bigl[ R(s_t,a_t) \\mid b_0, \\pi \\Bigr]\n</math>\nwhere <math>\\gamma<1</math> is the discount factor. The optimal policy <math>\\pi^*</math> is obtained by optimizing the long-term reward.\n:<math>\n\\pi^* = \\underset{\\pi}{\\mbox{argmax}}\\ V^\\pi(b_0)\n</math>\nwhere <math>b_0</math> is the initial belief.\n\nThe optimal policy, denoted by <math>\\pi^*</math>, yields the highest expected reward value for each belief state, compactly represented by the optimal value function <math>V^*</math>. This value function is solution to the [[Bellman equation|Bellman optimality equation]]:\n:<math>\nV^*(b) = \\max_{a\\in A}\\Bigl[ r(b,a) + \\gamma\\sum_{o\\in \\Omega} \\Pr(o\\mid b,a) V^*(\\tau(b,a,o)) \\Bigr]\n</math>\nFor finite-horizon POMDPs, the optimal value function is piecewise-linear and convex.<ref>{{Cite thesis |degree=PhD |title=The optimal control of partially observable Markov processes |last=Sondik |first=E.J. |year=1971 |publisher=Stanford University }}</ref> It can be represented as a finite set of vectors. In the infinite-horizon formulation, a finite vector set can approximate <math>V^*</math> arbitrarily closely, whose shape remains convex. Value iteration applies dynamic programming update to gradually improve on the value until convergence to an <math>\\epsilon</math>-optimal value function, and preserves its piecewise linearity and convexity.<ref>{{cite journal |doi=10.1287/opre.21.5.1071 |author=Smallwood, R.D., Sondik, E.J. |title=The optimal control of partially observable Markov decision processes over a finite horizon |journal=Operations Research |volume=21 |issue=5 |pages=1071–88 |year=1973 }}</ref> By improving the value, the policy is implicitly improved. Another dynamic programming technique called policy iteration explicitly represents and improves the policy instead.<ref>{{cite journal |doi=10.1287/opre.26.2.282 |author=Sondik, E.J. |title=The optimal control of partially observable Markov processes over the infinite horizon: discounted cost |journal=Operations Research |volume=26 |issue=2 |pages=282–304 |year=1978 }}</ref><ref>{{cite conference |booktitle=Proceedings of the Fourteenth International Conference on Uncertainty In Artificial Intelligence (UAI-98) |title=Solving POMDPs by searching in policy space |first=E. |last=Hansen |year=1998 |url=https://arxiv.org/pdf/1301.7380}}</ref>\n\n==Approximate POMDP solutions==\nIn practice, POMDPs are often computationally [[Computational complexity theory#Intractability|intractable]] to solve exactly, so computer scientists have developed methods that approximate solutions for POMDPs.<ref>{{cite journal |doi=10.1613/jair.678 |author=Hauskrecht, M. |title=Value function approximations for partially observable Markov decision processes|journal=Journal of Artificial Intelligence Research|volume=13|pages=33–94|year=2000}}</ref>\n\nGrid-based algorithms<ref>{{cite journal |doi=10.1287/opre.39.1.162 |author=Lovejoy, W. |title=Computationally feasible bounds for partially observed Markov decision processes |journal=Operations Research |volume=39 |pages=162–175 |year=1991 }}</ref> comprise one approximate solution technique.  In this approach, the value function is computed for a set of points in the belief space, and interpolation is used to determine the optimal action to take for other belief states that are encountered which are not in the set of grid points. More recent work makes use of sampling techniques, generalization techniques and exploitation of problem structure, and has extended POMDP solving into large domains with millions of states.<ref name=hoey>{{cite conference |title=Assisting Persons with Dementia during Handwashing Using a Partially Observable Markov Decision Process |author1=Jesse Hoey |author2=Axel von Bertoldi |author3=Pascal Poupart |author4=Alex Mihailidis |booktitle=Proc. International Conference on Computer Vision Systems (ICVS) |date=2007 |doi=10.2390/biecoll-icvs2007-89}}</ref><ref name=hoeyCVIU>{{cite journal |doi=10.1016/j.cviu.2009.06.008 | title=Automated Handwashing Assistance For Persons With Dementia Using Video and a Partially Observable Markov Decision Process |author1=Jesse Hoey |author2=Pascal Poupart |author3=Axel von Bertoldi |author4=Tammy Craig |author5=Craig Boutilier |author6=Alex Mihailidis. |journal=Computer Vision and Image Understanding (CVIU) |volume=114 | issue=5 | pages=503–519 | year=2010| citeseerx=10.1.1.160.8351 }}</ref> For example, adaptive grids and point-based methods sample random reachable belief points to constrain the planning to relevant areas in the belief space.<ref>{{cite conference |title=Point-based value iteration: An anytime algorithm for POMDPs |author=Pineau, J., Gordon, G., Thrun, S. |booktitle=International Joint Conference on Artificial Intelligence (IJCAI). Acapulco, Mexico |date=August 2003 |pages=1025–32 |url=http://www.fore.robot.cc/papers/Pineau03a.pdf}}</ref><ref>{{cite conference |title=Incremental methods for computing bounds in partially observable Markov decision processes |author=Hauskrecht, M.|booktitle=Proceedings of the 14th National Conference on Artificial Intelligence (AAAI). Providence, RI |date=1997 |pages=734–739 }}</ref>\nDimensionality reduction using [[Principle component analysis|PCA]] has also been explored.<ref>{{cite book |author1=Roy, Nicholas |author2=Gordon, Geoffrey |chapter=Exponential Family PCA for Belief Compression in POMDPs |title=Advances in Neural Information Processing Systems |year=2003 }}</ref>\n\n==Uses==\nPOMDPs can be used to model many kinds of real-world problems. Notable applications include the use of a POMDP in management of patients with ischemic heart disease,<ref>{{cite journal |doi=10.1016/S0933-3657(99)00042-1| title=Planning treatment of ischemic heart disease with partially observable Markov decision processes|author=Hauskrecht, M. , Fraser, H. | journal=Artificial Intelligence in Medicine|volume=18|issue=3|pages=221–244|year=2000}}</ref> assistive technology for persons with dementia,<ref name=hoey/><ref name=hoeyCVIU/> the conservation of the critically endangered and difficult to detect Sumatran tigers<ref name=\"chades\">{{cite journal |author=Chadès, I., McDonald-Madden, E., McCarthy, M.A., Wintle, B., Linkie, M., Possingham, H.P. |title=When to stop managing or surveying cryptic threatened species |journal=Proc. Natl. Acad. Sci. U.S.A. |volume=105 |issue=37 |pages=13936–40 |date=16 September 2008 |pmid=18779594 |pmc=2544557 |doi=10.1073/pnas.0805265105 |bibcode=2008PNAS..10513936C}}</ref> and aircraft collision avoidance.<ref>{{cite book |author=Kochenderfer, Mykel J. |chapter=Optimized Airborne Collision Avoidance |title=Decision Making Under Uncertainty |publisher=The MIT Press |year=2015 |url=https://ieeexplore.ieee.org/abstract/document/7288641/}}</ref>\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://www.cassandra.org/pomdp/index.shtml Tony Cassandra's POMDP pages] with a tutorial, examples of problems modeled as POMDPs, and software for solving them.\n* [http://www.cs.cmu.edu/~trey/zmdp/ zmdp], a POMDP solver by Trey Smith\n* [http://bigbird.comp.nus.edu.sg/pmwiki/farm/appl/index.php?n=Main.HomePage APPL], a fast point-based POMDP solver\n* [http://www.cs.uwaterloo.ca/~jhoey/research/spudd/ SPUDD], a factored structured (PO)MDP solver that uses algebraic decision diagrams (ADDs).\n* [http://bitbucket.org/bami/pypomdp pyPOMDP], a (PO)MDP toolbox (simulator, solver, learner, file reader) for Python by Oliver Stollmann and Bastian Migge\n* [http://www.cs.uwaterloo.ca/~mgrzes/code/IsoFreeBB/ Finite-state Controllers using Branch-and-Bound] An Exact POMDP Solver for Policies of a Bounded Size\n* [https://github.com/JuliaPOMDP/POMDPs.jl POMDPs.jl], an interface for defining and solving MDPs and POMDPs in [[Julia (programming language)|Julia]] with a variety of solvers.\n\n[[Category:Dynamic programming]]\n[[Category:Markov processes]]\n[[Category:Stochastic control]]"
    },
    {
      "title": "Quadratic knapsack problem",
      "url": "https://en.wikipedia.org/wiki/Quadratic_knapsack_problem",
      "text": "The '''quadratic knapsack problem (QKP)''', first introduced in 19th century,<ref>{{cite journal | last1 = C.| first1 = Witzgall| year = 1975 | title = Mathematical methods of site selection for Electronic Message Systems (EMS) | url = | journal = NBS Internal Report | doi = 10.6028/nbs.ir.75-737}}</ref> is an extension of [[knapsack problem]] that allows for quadratic terms in the objective function: Given a set of items, each with a weight, a value, and an extra profit that can be earned if two items are selected, determine the number of item to include in a collection without exceeding capacity of the [[knapsack]], so as to maximize the overall profit. Usually, quadratic knapsack problems come with a restriction on the number of copies of each kind of item: either 0, or 1. This special type of QKP forms the [[0-1 quadratic knapsack problem]], which was first discussed by Gallo et al.<ref>{{cite book | last1 = Gallo| first1 = G.| last2 = Hammer| first2 = P.L.| last3 = Simeone| first3 = B.| year = 1980 | title = Quadratic knapsack problems | url = | journal = Mathematical Programming Studies Combinatorial Optimization | volume = 12| pages = 132–149 | doi = 10.1007/bfb0120892 | series = Mathematical Programming Studies| isbn = 978-3-642-00801-6}}</ref>\nThe 0-1 quadratic knapsack problem is a variation of knapsack problems, combining the features of unbounded knapsack problem, 0-1 knapsack problem and quadratic knapsack problem.\n\n== Definition ==\nSpecifically, the 0–1 quadratic knapsack problem has the following form: \n: <math>\\text{maximize } \\left\\{\\sum_{i=1}^n p_i x_i + \\sum_{i=1}^n\\sum_{j=1,i\\neq j}^n P_{ij}x_ix_j:  x\\in X, x \\text{ binary} \\right\\} </math>\n: <math> \\text{subject to } X\\equiv\\left\\{x \\in \\R^n: \\sum_{i=1}^n w_i x_i \\leq W; 0\\leq x_i\\leq 1 \\text{ for } i=1,\\ldots, n \\right\\}.</math>\n\nWhile the binary variable ''x<sub>i</sub>'' represents whether item ''i'' is included in the knapsack, <math>p_i</math> is the profit earned by selecting item ''i'' and <math>P_{ij}</math> is the profit achieved if both item ''i'' and ''j'' are added.\n\nInformally, the problem is to maximize the sum of the values of the items in the knapsack so that the sum of the weights is less than or equal to the knapsack's capacity.\n\n== Application ==\nAs one might expect, QKP has a wide range of applications including [[telecommunication]], [[transport network|transportation network]], [[computer science]] and [[economics]]. In fact, Witzgall first discussed QKP when selecting sites for satellite stations in order to maximize the global traffic with respect to a budget constraint. Similar model applies to problems like considering the location of airports, railway stations, or freight handling terminals.<ref>{{cite journal | last1 = Rhys| first1 = J.M.W.| year = 1970 | title = A Selection Problem of Shared Fixed Costs and Network Flows | url = | journal = Management Science | volume = 17 | issue = 3| pages = 200–207 | doi = 10.1287/mnsc.17.3.200}}</ref> Applications of QKP in the field of computer science is more common after the early days: [[compiler]] design problem,<ref>{{cite book | last1 = Helmberg| first1 = C.| last2 = Rendl | first2 = F.| last3 = Weismantel | first3 = R. | year = 1996| title = Quadratic knapsack relaxations using cutting planes and semidefinite programming | url = | journal = Integer Programming and Combinatorial Optimization Lecture Notes in Computer Science | volume = 1084| issue = | pages = 175–189 | doi = 10.1007/3-540-61310-2_14| series = Lecture Notes in Computer Science| isbn = 978-3-540-61310-7}}</ref> [[clique problem]],<ref>{{cite journal | last1 = Dijkhuizen | first1 = G.| last2 = Faigle| first2 = U.| year = 1993| title = A cutting-plane approach to the edge-weighted maximal clique problem | url = | journal = European Journal of Operational Research | volume = 69| issue = 1 | pages = 121–130 | doi = 10.1016/0377-2217(93)90097-7}}</ref><ref>{{cite journal | last1 = Park| first1 = Kyungchul | last2 = Lee| first2 = Kyungsik | last3 = Park| first3 = Sungsoo | year = 1996| title = An extended formulation approach to the edge-weighted maximal clique problem | url = | journal = European Journal of Operational Research | volume = 95| issue = 3 | pages = 671–682 | doi = 10.1016/0377-2217(95)00299-5}}</ref> [[very large scale integration]] (VLSI) design.<ref>{{cite journal | last1 = Ferreira| first1 = C.E.| last2 = Martin| first2 = A.| last3 = Souza| first3 =  C.C.De| last4 = Weismantel| first4 =  R. | last5 = Wolsey | first5 = L.A.| year = 1996| title = Formulations and valid inequalities for the node capacitated graph partitioning problem | url = | journal = Mathematical Programming | volume = 74| issue = 3 | pages = 247–266 | doi = 10.1007/bf02592198}}</ref> Additionally, pricing problems appear to be an application of QKP as described by Johnson et al.<ref>{{cite journal | last1 = Johnson| first1 = Ellis L.| last2 = Mehrotra| first2 = Anuj | last3 = Nemhauser| first3 = George L.| year = 1993| title = Min-cut clustering | url = | journal = Mathematical Programming | volume = 62| issue = 1–3 | pages = 133–151 | doi = 10.1007/bf01585164}}</ref>\n\n== Computational complexity ==\nIn general, the decision version of the knapsack problem (Can a value of at least V be achieved under a restriction of a certain capacity W?) is [[NP-complete]].<ref>{{cite book| last1 = Garey | first1 = Michael R.| last2 =Johnson| first2 = David S.| year = 1979 | title = Computers and intractibility: A guide to the theory of NP completeness | publisher = New York: Freeman and Co.}}</ref> Thus, a given solution can be verified to in polynomial time while no algorithm can identify a solution efficiently.\n\nThe optimization knapsack problem is [[NP-hard]] and there is no known algorithm that can solve the problem in polynomial time.\n\nAs a particular variation of the knapsack problem, the 0-1 quadratic knapsack problem is also NP-hard.\n\nWhile no available efficient algorithm exists in the literature, there is a [[pseudo-polynomial time]] based on [[dynamic programming]] and other [[heuristic]] algorithms that can always generate “good” solutions.\n\n== Solving ==\nWhile the knapsack problem is one of the most commonly solved [[operation research]] (OR) problems, there are limited efficient algorithms that can solve 0-1 quadratic knapsack problems. Available algorithms include but are not limited to [[brute force method|brute force]], [[linearization]],<ref>{{cite journal | last1 = Adams | first1 = Warren P.| last2 = Sherali | first2 = Hanif D. | year = 1986 | title = A Tight Linearization and an Algorithm for Zero-One Quadratic Programming Problems | url = | journal = Management Science | volume = 32 | issue = 10| pages = 1274–1290 | doi = 10.1287/mnsc.32.10.1274}}</ref> and convex reformulation. Just like other NP-hard problems, it is usually enough to find a workable solution even if it is not necessarily optimal. Heuristic algorithms based on [[greedy algorithm]], dynamic programming can give a relatively “good” solution to the 0-1 QKP efficiently.\n\n=== Brute force ===\nThe brute force algorithm to solve this problem is to identify all possible subsets of the items without exceeding the capacity and select the one with the optimal value. The pseudo-code is provided as follows:\n<source lang=\"c\" line>\n// Input:\n// Profits (stored in array p)\n// Quadratic profits (stored in matrix P)\n// Weights (stored in array w)\n// Number of items (n)\n// Knapsack capacity (W)\n\nint max =0\nfor all subset S do\n\tint value, weight = 0\n \tfor i from 0 to S.size-1 do:\n\t\tvalue = value +p[i]\n\t\tweight = weight +w[i]\n\t\t\tfor j from i+1 to S.size-1 do: \n\t\t\t\tvalue = value + P[i][j]\n\tif weight <= W then:\n\t\tif value > max then:\t\n\t\t\tmax = value\n\n</source>\n\nGiven ''n'' items, there will be at most <math>2^n</math> subsets and for each legal candidate set, the running time of computing the values earned is <math>O(n^2)</math>. Thus, the efficiency class of brute force algorithm is <math>(2^n n^2 )=\\lambda(2^n)</math>, being exponential.\n\n=== Linearization ===\nProblems of such form are difficult to solve directly using standard solvers and thus people try to reformulate it as a [[linear program]] using auxiliary variables and constraints so that the problem can be readily solved using commercial packages. Two well-known [[linearization]] approaches for the 0-1 QKP are the standard linearization and Glover’s linearization.<ref>{{cite journal | last1 = Adams | first1 = Warren P.| last2 = Forrester| first2 = Richard J.| last3 = Glover | first3 = Fred W.| year = 2004 | title = Comparisons and enhancement strategies for linearizing mixed 0-1 quadratic programs | url = | journal = Discrete Optimization | volume = 1| issue = 2| pages = 99–120 | doi = 10.1016/j.disopt.2004.03.006}}</ref><ref>{{cite journal | last1 = Adams | first1 = Warren P.| last2 = Forrester| first2 = Richard J.| year = 2005 | title = A simple recipe for concise mixed 0-1 linearizations | url = | journal = Operations Research Letters | volume = 33| issue = 1| pages = 55–61 | doi = 10.1016/j.orl.2004.05.001}}</ref><ref>{{cite journal | last1 = Adams | first1 = Warren P.| last2 = Forrester| first2 = Richard J.| year = 2007 | title = Linear forms of nonlinear expressions: New insights on old ideas | url = | journal = Operations Research Letters | volume = 35| issue = 4| pages = 510–518 | doi = 10.1016/j.orl.2006.08.008}}</ref>\n\n==== Standard linearization ====\nThe first one is the standard linearization strategy, as shown below:\n: LP1: maximize \n:<math>\\sum_{i=1}^n p_i x_i + \\sum_{i=1}^n \\left(\\sum_{j=1,i\\neq j}^n (P_{ij} + P_{ji}) z_{ij} \\right).</math>\n: subject to \n:<math>z_{ij}\\leq x_i </math> for all <math>(i,j), i<j</math> \n:<math>z_{ij}\\leq x_j </math> for all <math>(i,j), i<j</math> \n:<math>x_i+x_j-1\\leq z_{ij} </math> for all <math>(i,j), i<j</math> \n:<math>z_{ij}\\geq 0 </math> for all <math>(i,j), i<j</math>\n:<math>x \\in X, x</math> binary\n\nIn the formulation LP1, we have replaced the ''x<sub>i</sub>x<sub>j</sub>'' term with a continuous variable ''z<sub>ij</sub>''. This reformulates the QKP into a knapsack problem, which we can then solve optimally using standard solvers.\n\n==== Glover's linearization ====\nThe second reformulation, which is more concise, is called Glover’s linearization.<ref>{{cite journal | last1 = Glover | first1 = Fred| last2 = Woolsey| first2 = Eugene | year = 1974 | title = Technical Note—Converting the 0-1 Polynomial Programming Problem to a 0-1 Linear Program | url = | journal = Operations Research | volume = 22| issue = 1| pages = 180–182 | doi = 10.1287/opre.22.1.180}}</ref><ref>{{cite journal | last1 = Glover | first1 = Fred | year = 1975 | title = Improved Linear Integer Programming Formulations of Nonlinear Integer Problems | url = | journal = Management Science  | volume = 22| issue = 4| pages = 455–460 | doi = 10.1287/mnsc.22.4.455}}</ref><ref>{{cite journal | last1 = Glover | first1 = Fred| last2 = Woolsey| first2 = Eugene | year = 1973 | title = Further Reduction of Zero-One Polynomial Programming Problems to Zero-One linear Programming Problems | url = | journal = Operations Research | volume = 21| issue = 1| pages = 156–161 | doi = 10.1287/opre.21.1.156}}</ref> The Glover formulation is shown below, where ''L<sub>i</sub>'' and ''U<sub>i</sub>'' are lower and upper bounds on <math>\\sum_{j=1}^n P_{ij}x_ix_j</math>, respectively: \n: LP2: maximize \n:<math>\\sum_{i=1}^n p_i x_i + \\sum_{i=1}^nz_{ij}</math>\n: subject to\n:<math>L_i\\leq z_i \\leq U_i </math> for <math>i=1,\\ldots, n</math> \n:<math>\\sum_{j=1}^nP_{ij} x_i-U_i(1-x_i)\\leq z_i\\leq \\sum_{j=1}^nP_{ij} x_i-L_i(1-x_i) </math> for <math>i=1,\\ldots, n</math>\n:<math>x \\in X, x</math> binary\n\nIn the formulation LP2, we have replaced the expression (<math>\\sum_{j=1}^n P_{ij} x_i x_j</math>)  with a continuous variable ''z<sub>i</sub>''. Similarly, we can use standard solvers to solve the linearization problem. Note that Glover’s linearization only includes <math>n</math> auxiliary variables with <math>4n</math> constraints while standard linearization requires <math>{n \\choose 2}</math> auxiliary variables and <math>3{n \\choose 2}</math> constraints to achieve linearity.\n\n=== Convex quadratic reformulation ===\nNote that nonlinear programs are hard to solve due to the possibility of being stuck at a [[local maximum]]. However, when the program is [[convexity (mathematics)|convex]], any local maximum is the [[global maximum]]. A convex program is to maximize a [[concave function]] or minimize a [[convex function]] on a [[convex set]]. A set S is convex if <math>\\forall u,v\\in S</math>, <math>\\lambda u+(1-\\lambda)v\\in S</math> where <math>\\lambda \\in [0,1]</math>. That is to say, any point between two points in the set must also be an element of the set. A function ''f'' is concave if <math>f[\\lambda u+(1-\\lambda)v]\\leq \\lambda f(u)+(1-\\lambda)f(v)</math>. A function ''f'' is convex if <math>f[\\lambda u+(1-\\lambda)v]\\geq \\lambda f(u)+(1-\\lambda)f(v)</math>. Informally, a function is concave if the line segment connecting two points on the graph lies above or on the graph, while a function is convex if below or on the graph. Thus, by rewriting the objective function into an equivalent convex function, we can reformulate the program to be convex, which can be solved using optimization packages.\n\nThe objective function can be written as <math>c^Tx+x^tCx</math> using linear algebra notation. We need to make ''P'' a [[positive semi-definite matrix]] in order to reformulate a convex function. In this case, we modify the objective function to be <math>p^Tx+x^TPx+\\sum_{i=1}^n \\left(\\sum_{j=1,j\\neq i}^n|P_{ij}|\\right)(x_i^2-x_i)</math> by applying results from linear algebra, where ''P'' is a [[diagonally dominant matrix]] and thus a positive semi-definite. This reformulation can be solved using a standard commercial mixed-integer quadratic package.<ref>{{cite journal | last1 = Bliek| first1 = Christian | last2 = Bonami| first2 = Pierre | last3 = Lodi | first3 = Andrea | year = 2014 | title = Solving Mixed-Integer Quadratic Programming problems with IBM-CPLEX: a progress report | url = http://www.orsj.or.jp/ramp/2014/paper/4-3.pdf }}</ref>\n\n=== Greedy heuristic algorithm ===\nGeorge Dantzig<ref>{{cite journal | last1 = Dantzig| first1 = George B. | year = 1957 | title = Discrete-Variable Extremum Problems | url = | journal = Operations Research | volume = 5| issue = 2| pages = 266–288 | doi = 10.1016/j.disopt.2004.03.006}}</ref> proposed a greedy approximation algorithm to unbounded knapsack problem which can also be used to solve the 0-1 QKP. The algorithm consists of two phrases: identify an initial solution and improve it.\n\nFirst compute for each item, the total objective contribution realizable by selecting it, <math>p_i+\\sum_{i\\neq j}^n P_{ij}</math>, and sort the items in decreasing order of the potential value per unit of weight, <math>(p_i+\\sum_{i\\neq j}^nP_{ij})/w_i</math>. Then select the items with the maximal value-weight ratio into the knapsack until there is no space for more, which forms the initial solution. \nStarting with the initial solution, the improvement is conducted by pairwise exchange. For each item in the solution set, identify the items not in the set where swapping results in an improving objective. Select the pair with maximal improvement and swap. There are also possibilities that removing one from the set or adding one to the set will produce the greatest contribution. Repeat until there is no improving swapping. \nThe complexity class of this algorithm is <math>O(2^n)</math> since for the worst case every possible combination of items will be identified.\n\n=== Quadknap ===\nQuadknap is an exact [[branch-and-bound]] algorithm raised by Caprara et al.,<ref>{{cite journal | last1 = Caprara| first1 = Alberto| last2= Pisinger| first2 = David |last3 =Toth|first3 =Paolo  | year = 1999 | title = Exact Solution of the Quadratic Knapsack Problem | url = | journal = INFORMS Journal on Computing | volume = 11| issue = 2| pages = 125–137 | doi = 10.1287/ijoc.11.2.125| citeseerx = 10.1.1.22.2818}}</ref> where upper bounds are computed by considering a [[Lagrangian relaxation]] which approximate a difficult problem by a simpler problem and penalizes violations of constraints using [[Lagrange multiplier]] to impost a cost on violations. Quadknap releases the integer requirement when computing the upper bounds. Suboptimal Lagrangian multipliers are derived from sub-gradient optimization and provide a convenient reformulation of the problem. This algorithm is quite efficient since Lagrangian multipliers are stable, and suitable data structures are adopted to compute a tight upper bound in linear expected time in the number of variables. This algorithm was reported to generate exact solutions of instances with up to 400 binary variables, i.e., significantly larger than those solvable by other approaches. The code was written C language and available online.<ref>{{cite web|url=http://www.diku.dk/~pisinger/quadknap.c |title=Quadknap |accessdate=2016-12-03}}</ref>\n\n=== Dynamic programming heuristic ===\nWhile dynamic programming can generate optimal solutions to knapsack problems, dynamic programming approaches for QKP<ref>{{cite journal | last1 = Fomeni | first1 = Franklin Djeumou| last2= Letchford| first2 = Adam N. | year = 2014 | title =A Dynamic Programming Heuristic for the Quadratic Knapsack Problem| url = | journal = INFORMS Journal on Computing | volume = 26| issue = 1| pages = 173–182 | doi = 10.1287/ijoc.2013.0555}}</ref> can only yield a relatively good quality solution, which can serve as a lower bound to the optimal objectives. While it runs in pseudo-polynomial time, it has a large memory requirement.\n\n==== Dynamic programming algorithm ====\nFor simplicity, assume all weights are non-negative. The objective is to maximize total value subject to the constraint: that the total weight is less than or equal to ''W''. Then for each <math>w\\leq W</math>, define <math>f(m,w)</math> to be the value of the most profitable packing of the first m items found with a total weight of ''w''. That is, let\n\n: <math>f(m,w)=\\max\\left\\{\\sum_{i=1}^m p_i x_i + \\sum_{i=1}^m\\sum_{j=1,i\\neq j}^m P_{ij}x_ix_j:  \\sum_{i=1}^m w_i=w,1\\leq i\\leq m \\right\\}. </math>\n\nThen, <math>f(m,w)</math> is the solution to the problem. Note that by dynamic programming, the solution to a problem arises from the solution to its smaller sub-problems. In this particular case, start with the first item and try to find a better packing by considering adding items with an expected weight of 𝑤. If the weight of the item to be added exceeds ''𝑤'', then <math>f(m,w)</math> is the same with <math>f(m-1,w)</math>. Given that the item has a smaller weight compared with the desired weight, <math>f(m,w)</math> is either the same as <math>f(m-1,w)</math> if adding makes no contribution, or the same as the solution for a knapsack with smaller capacity, specifically one with the capacity reduced by the weight of that chosen item, plus the value of one correct item, i.e. <math>f(m-1,w-w_m)+p_m+\\sum_{i=1}^{m-1}P_{im}x_i</math>. To conclude, we have that\n\n: <math>f(m,w)=\n \\begin{cases}\n \\max f(m-1,w),f(m-1,w-w_m)+p_m+\\sum_{i=1}^{m-1}P_{im}x_i & \\text{if } w_m\\leq w\\\\\n f(m-1,w) & \\text{otherwise}\n \\end{cases}\n</math>\n\nNote on efficiency class: Clearly the running time of this algorithm is <math>O(Wn^2)</math>, based on the nested loop and the computation of the profit of new packing. This does not contradict the fact the QKP is NP-hard since ''W'' is not polynomial in the length of the input.\n\n==== Revised dynamic programming algorithm ====\nNote that the previous algorithm requires <math>O(Wn^2)</math> space for storing the current packing of items for all ''m,w'', which may not be able to handle large-size problems. In fact, this can be easily improved by dropping the index ''m'' from <math>f(m,w)</math> since all the computations depend only on the results from the preceding stage.\n\nRedefine <math>f(w)</math> to be the current value of the most profitable packing found by the heuristic. That is, \n\n: <math>f(w)=\\max\\left\\{\\sum_{i=1}^m p_i x_i + \\sum_{i=1}^m \\sum_{j=1,i\\neq j}^m P_{ij}x_ix_j:  \\sum_{i=1}^m w_i=w,m\\leq n \\right\\}. </math>\nAccordingly, by dynamic programming we have that\n\n: <math>f(m)=\n \\begin{cases}\n \\max f(w),f(w-w_m)+p_m+\\sum_{i=1}^{m-1}P_{im}x_i & \\text{if } w_m\\leq w, \\\\\n f(w) & \\text{otherwise.} \n \\end{cases}\n</math>\n\nNote this revised algorithm still runs in <math>O(Wn^2)</math> while only taking up <math>O(Wn)</math> memory compared to the previous <math>O(Wn^2)</math>.\n\n== Related research topics ==\nResearchers have been studied 0-1 quadratic knapsack problems for decades. One focus is to find effective algorithms or effective heuristics, especially those with an outstanding performance solving real world problems. The relationship between the decision version and the optimization version of the 0-1 QKP should not be ignored when working with either one. On one hand, if the [[decision problem]] can be solved in polynomial time, then one can find the optimal solution by applying this algorithm iteratively. On the other hand, if there exists an algorithm that can solve the [[optimization problem]] efficiently, then it can be utilized in solving the decision problem by comparing the input with the optimal value.\n\nAnother theme in literature is to identify what are the \"hard\" problems. Researchers who study the 0-1 QKP often perform computational studies<ref>{{cite journal | last1 = Forrester| first1 =  Richard J.| last2= Adams| first2 = Warren P. |last3=Hadavas|first3 = Paul T.| year = 2009| title =Concise RLT forms of binary programs: A computational study of the quadratic knapsack problem.| url = | journal = Naval Research Logistics | pages =  NA| doi = 10.1002/nav.20364}}</ref> to show the superiority of their strategies. Such studies can also be conducted to assess the performance of different solution methods. For the 0-1 QKP, those computational studies often rely on randomly generated data, introduced by Gallo et al. Essentially every computational study of the 0-1 QKP utilizes data that is randomly generated as follows. The weights are integers taken from a [[Discrete uniform distribution|uniform distribution]] over the interval [1, 50], and the capacity constraints is an integer taken from a uniform distribution between 50 and the sum of item weights. The objective coefficients, i.e. the values are randomly chosen from [1,100]. It has been observed that generating instances of this form yields problems with highly variable and unpredictable difficulty. Therefore, the computational studies presented in the literature may be unsound. Thus some researches aim to develop a methodology to generate instances of the 0-1 QKP with a predictable and consistent level of difficulty.\n\n==See also==\n{{Portal|Computer programming|Computer science}}\n{{Div col|colwidth=25em}}\n* [[Knapsack problem]]\n* [[Combinatorial auction]]\n* [[Combinatorial optimization]]\n* [[Continuous knapsack problem]]\n* [[List of knapsack problems]]\n* [[Packing problem]]\n{{div col end}}\n\n==Notes==\n{{Reflist|30em}}\n\n==External links==\n*[http://www.diku.dk/~pisinger/codes.html David Pisinger's Codes for different knapsack problem]\n*[http://www.adaptivebox.net/CILib/code/qkpcodes_link.html Codes for Quadratic Knapsack Problem]\n\n[[Category:Packing problems]]\n[[Category:NP-complete problems]]\n[[Category:Dynamic programming]]\n[[Category:Combinatorial optimization]]\n[[Category:Pseudo-polynomial time algorithms]]"
    },
    {
      "title": "Recursive economics",
      "url": "https://en.wikipedia.org/wiki/Recursive_economics",
      "text": "'''Recursive economics''' is a branch of modern [[economics]] based on a paradigm of individuals making a series of two-period optimization decisions over time.\n\n==Differences between recursive and neoclassical paradigms==\nThe neoclassical model assumes a one-period utility maximization for a consumer and one-period [[profit maximization]] by a producer. The adjustment that occurs within that single time period is a subject of considerable debate within the field, and is often left unspecified. A time-series path in the neoclassical model is a series of these one-period utility maximizations.\n\nIn contrast, a recursive model involves two or more periods, in which the consumer or producer trades off benefits and costs across the two time periods. This trade-off is sometimes represented in what is called an Euler equation. A time-series path in the recursive model is the result of a series of these two-period decisions.\n\nIn the neoclassical model, the consumer or producer maximizes utility (or profits). In the recursive model, the subject maximizes value or welfare, which is the sum of current rewards or benefits and discounted future expected value.\n\n==The recursive model==\nThe field is sometimes called '''recursive''' because the decisions can be represented by equations that can be transformed into a single functional equation sometimes called a [[Bellman equation]]. This equation relates the benefits or rewards that can be obtained in the current time period to the discounted value that is expected in the next period. The dynamics of recursive models can sometimes also be studied as [[differential equation]]s.\n\n==Pioneers in the field==\n\nThe recursive paradigm originated in control theory with the invention of  [[dynamic programming]] by the American mathematician [[Richard E. Bellman]] in the 1950s. Bellman described possible applications of the method in a variety of fields, including Economics, in the introduction to his 1957 book.<ref>''Dynamic Programming'' Princeton, 1957; reissued by Dover</ref> [[Stuart Dreyfus]], [[David Blackwell]], and [[Ronald A. Howard]] all made major contributions to the approach in the 1960s.\n\nIn addition, some scholars also cite the [[Kalman filter]] invented by [[Rudolf E. Kalman]] and the theory of the maximum formulated by [[Lev Semenovich Pontryagin]] as forerunners of the recursive approach in economics.\n\n==Applications in economics==\nSome scholars point to Martin Beckmann and Richard Muth<ref>Martin Beckmann and Richard Muth, 1954, \"On the solution to the fundamental equation of inventory theory,\" ''Cowles Commission Discussion Paper'' 2116.</ref> as the first application of an explicit recursive equation in economics. However, probably the earliest celebrated economic application of recursive economics was Robert Merton's seminal 1973 article on the [[ICAPM|intertemporal capital asset pricing model]].<ref>[[Robert C. Merton]], 1973, \"An Intertemporal Capital Asset Pricing Model,\" ''Econometrica 41'': 867–887.</ref> (See also [[Merton's portfolio problem]]). Merton's theoretical model, one in which investors chose between income today and future income or capital gains, has a recursive formulation.\n\nNancy Stokey, Robert Lucas and Edward Prescott describe stochastic and non-stochastic dynamic programming in considerable detail, giving many examples of how to employ dynamic programming to solve problems in economic theory.<ref>[[Nancy Stokey]], and [[Robert E. Lucas]], with [[Edward Prescott]], 1989. ''Recursive Methods in Economic Dynamics''. Harvard Univ. Press.</ref> This book led to dynamic programming being employed to solve a wide range of theoretical problems in economics, including optimal [[economic growth]], [[resource extraction]], [[principal–agent problem]]s, [[public finance]], business [[investment]], [[asset pricing]], [[factor of production|factor]] supply, and [[industrial organization]].\n\nThe approach gained further notice in macroeconomics from the extensive exposition by Ljungqvist & Sargent.<ref>[[Lars Ljungqvist]] & [[Thomas Sargent]], 2000, 2004, 2012. ''Recursive Macroeconomic Theory''. MIT Press.</ref> This book describes recursive models applied to theoretical questions in [[monetary policy]], [[fiscal policy]], [[taxation]], [[economic growth]], [[search theory]], and [[labor economics]].\n\nIn investment and finance, Avinash Dixit and Robert Pindyck showed the value of the method for thinking about [[capital budgeting]], in particular showing how it was theoretically superior to the standard neoclassical investment rule.<ref>[[Avinash Dixit]] & Robert Pindyck, 1994. ''Investment Under Uncertainty''. Princeton Univ. Press.</ref> Patrick Anderson adapted the method to the valuation of operating and start-up businesses <ref>Anderson, Patrick L., ''Business Economics & Finance'', CRC Press, 2004, {{ISBN|1-58488-348-0}}.</ref><ref>Anderson, Patrick L., ''Economics of Business Valuation'', Stanford University Press, 2013</ref> and to the estimation of the aggregate value of privately held businesses in the US.<ref>The Value of Private Businesses in the United States, ''Business Economics'' (2009) 44, 87–108. {{doi|10.1057/be.2009.4}}.</ref>\n\nThere are serious computational issues that have hampered the adoption of recursive techniques in practice, many of which originate in the [[curse of dimensionality]] first identified by Richard Bellman.\n\nApplied recursive methods, and discussion of the underlying theory and the difficulties, are presented in Mario Miranda & Paul Fackler (2002),<ref>Miranda, M., & Fackler, P., 2002. ''Applied Computational Economics and Finance''. MIT Press</ref> Meyn (2007)<ref>S. P. Meyn, 2007.  [http://decision.csl.uiuc.edu/~meyn/pages/CTCN/CTCN.html Control Techniques for Complex Networks] {{webarchive|url=https://web.archive.org/web/20080513165615/http://decision.csl.uiuc.edu/~meyn/pages/CTCN/CTCN.html |date=2008-05-13 }}, Cambridge University Press, 2007. [http://decision.csl.uiuc.edu/~meyn/pages/book.html Meyn & Tweedie] {{webarchive|url=https://web.archive.org/web/20071012194420/http://decision.csl.uiuc.edu/~meyn/pages/book.html |date=2007-10-12 }},</ref> Powell (2011)<ref>Warren B. Powell, ''Approximate Dynamic Programming, 2d ed.'' Wiley, 2011,</ref> and Bertsekas (2005).<ref>Dimitri Bertsekas, ''Dynamic Programming and Optimal Control'', Athena Scientific 2005, 2012</ref>\n\n==See also==\n* [[Dynamic programming]]\n* [[Hamilton–Jacobi–Bellman equation]]\n* [[Markov decision process]]\n* [[Optimal control theory]]\n* [[Optimal substructure]]\n* [[Recursive competitive equilibrium]]\n* [[Bellman pseudospectral method]]\n\n== References ==\n{{Reflist}}\n\n[[Category:Intertemporal economics]]\n[[Category:Equations]]\n[[Category:Dynamic programming]]\n[[Category:Control theory]]\n[[Category:Mathematical economics]]"
    },
    {
      "title": "Smith–Waterman algorithm",
      "url": "https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm",
      "text": "{{Infobox algorithm\n|name= <!-- Defaults to article name -->\n|class= [[Sequence alignment]]\n|image=\n|caption=\n|data=\n|time= <math>O(mn)</math>\n|best-time=\n|average-time=\n|space= <math>O(mn)</math>\n}}\n\n[[File:Smith-Waterman-Algorithm-Example-En.gif|frame|right|An animated example to show the steps progressively. See [[#Example|here]] for detailed steps.]]\nThe '''Smith–Waterman algorithm''' performs local [[sequence alignment]]; that is, for determining similar regions between two strings of [[nucleic acid sequence]]s or [[protein sequence]]s. Instead of looking at the [[Needleman–Wunsch algorithm|entire]] sequence, the Smith–Waterman algorithm compares segments of all possible lengths and [[Mathematical optimization|optimizes]] the [[similarity measure]].\n\nThe algorithm was first proposed by [[Temple F. Smith]] and [[Michael S. Waterman]] in 1981.<ref name=\"Smith1981\">{{cite journal |author1=Smith, Temple F. |author2=Waterman, Michael S. |last-author-amp=yes |title=Identification of Common Molecular Subsequences \n|journal=[[Journal of Molecular Biology]] |volume=147 |issue=1 |pages=195–197 |year=1981 |url=http://dornsife.usc.edu/assets/sites/516/docs/papers/msw_papers/msw-042.pdf |doi=10.1016/0022-2836(81)90087-5 |pmid=7265238|citeseerx=10.1.1.63.2897 }}</ref> Like the [[Needleman–Wunsch algorithm]], of which it is a variation, Smith–Waterman is a [[dynamic programming]] algorithm. As such, it has the desirable property that it is guaranteed to find the optimal local alignment with respect to the scoring system being used (which includes the [[substitution matrix]] and the [[Gap penalty|gap-scoring]] scheme). The main difference to the [[Needleman–Wunsch algorithm]] is that negative scoring matrix cells are set to zero, which renders the (thus positively scoring) local alignments visible. Traceback procedure starts at the highest scoring matrix cell and proceeds until a cell with score zero is encountered, yielding the highest scoring local alignment. Because of its quadratic complexity in time and space, it often cannot be practically applied to large-scale problems and is replaced in favor of less general but computationally more efficient alternatives such as (Gotoh, 1982),<ref name=\"Gotoh1982\">{{cite journal |author=Osamu Gotoh | title=An improved algorithm for matching biological sequences | journal=[[Journal of Molecular Biology]] |volume=162 | issue=3 |pages=705–708 |year=1982 |doi=10.1016/0022-2836(82)90398-9| citeseerx=10.1.1.204.203 }}</ref> (Altschul and Erickson, 1986),<ref name=\"Altschul1986\">{{cite journal |author1=Stephen F. Altschul |author2=Bruce W. Erickson |last-author-amp=yes |title=Optimal sequence alignment using affine gap costs |journal=[[Bulletin of Mathematical Biology]] |volume=48 |issue=5–6 |pages=603–616 |year=1986 |doi=10.1007/BF02462326 }}</ref> and (Myers and Miller, 1988).<ref name=\"Myers1988\">{{cite journal |author1=Miller, Webb |author2=Myers, Eugene |title=Optimal alignments in linear space |journal=[[Bioinformatics (journal)|Bioinformatics]] |volume=4 |pages=11–17 |year=1988 |doi=10.1093/bioinformatics/4.1.11|citeseerx=10.1.1.107.6989 }}</ref>\n\n==History==\nIn 1970, Saul B. Needleman and Christian D. Wunsch proposed a heuristic homology algorithm for sequence alignment, also referred to as the Needleman–Wunsch algorithm.<ref name=\"Needleman1970\">{{cite journal |author1=Saul B. Needleman |author2=Christian D. Wunsch |title=A general method applicable to the search for similarities in the amino acid sequence of two proteins |journal=[[Journal of Molecular Biology]] |volume=48 |issue=3 |pages=443–453 |year=1970 |doi=10.1016/0022-2836(70)90057-4 |pmid=5420325}}</ref> It is a global alignment algorithm that requires [[Big O notation|<math>O(mn)</math>]] calculation steps (<math>m</math> and <math>n</math> are the lengths of the two sequences being aligned). It uses the iterative calculation of a matrix for the purpose of showing global alignment. In the following decade, Sankoff,<ref name=\"Sankoff1972\">{{cite journal |author=Sankoff D. |title=Matching Sequences under Deletion/Insertion Constraints |journal=[[Proceedings of the National Academy of Sciences of the United States of America]] |volume=69 |issue=1 |pages=4–6 |year=1972 |doi=10.1073/pnas.69.1.4 |pmid=4500555 |pmc=427531}}</ref> Reichert,<ref name=\"Reichert1973\">{{cite journal |author1=Thomas A. Reichert |author2=Donald N. Cohen |author3=Andrew K.C. Wong |title=An application of information theory to genetic mutations and the matching of polypeptide sequences |journal=[[Journal of Theoretical Biology]] |volume=42 |issue=2 |pages=245–261 |year=1973 |doi=10.1016/0022-5193(73)90088-X}}</ref> Beyer<ref name=\"Beyer1974\">{{cite journal |author=William A. Beyer, Myron L. Stein, Temple F. Smith, and Stanislaw M. Ulam |title=A molecular sequence metric and evolutionary trees |journal=[[Mathematical Biosciences]] |volume=19 |issue=1–2 |pages=9–25 |year=1974 |doi=10.1016/0025-5564(74)90028-5 }}</ref> and others formulated alternative heuristic algorithms for analyzing gene sequences. Sellers introduced a system for measuring sequence distances.<ref name=\"Sellers1974\">{{cite journal |author=Peter H. Sellers |title=On the Theory and Computation of Evolutionary Distances |journal=[[Journal of Applied Mathematics]] |volume=26 |issue=4 |pages=787–793 |year=1974 |doi=10.1137/0126070}}</ref> In 1976, Waterman et al. added the concept of gaps into the original measurement system.<ref name=\"Waterman1976\">{{cite journal |author1=M.S Waterman |author2=T.F Smith |author3=W.A Beyer |title=Some biological sequence metrics |journal=[[Advances in Mathematics]] |volume=20 |issue=3 |pages=367–387 |year=1976 |doi=10.1016/0001-8708(76)90202-4}}</ref> In 1981, Smith and Waterman published their Smith–Waterman algorithm for calculating local alignment.\n\nThe Smith–Waterman algorithm is fairly demanding of time: To align two sequences of lengths <math>m</math> and <math>n</math>, <math>O(mn)</math> time is required. Gotoh<ref name=\"Gotoh1982\" /> and Altschul<ref name=\"Altschul1986\" /> optimized the algorithm to <math>O(mn)</math> steps. The space complexity was optimized by Myers and Miller<ref name=\"Myers1988\" /> from <math>O(mn)</math> to <math>O(n)</math> (linear), where <math>n</math> is the length of the shorter sequence, for the case where one only of the many possible optimal alignments is desired.\n\n==Motivation==\nIn recent years, [[genome project]]s conducted on a variety of organisms generated massive amounts of sequence data for genes and proteins, which requires computational analysis. Sequence alignment shows the relations between genes or between proteins, leading to a better understanding of their homology and functionality. Sequence alignment can also reveal [[Conserved sequence|conserved domains]] and [[Sequence motif|motifs]].\n\nOne motivation for local alignment is the difficulty of obtaining correct alignments in regions of low similarity between distantly related biological sequences, because mutations have added too much 'noise' over evolutionary time to allow for a meaningful comparison of those regions. Local alignment avoids such regions altogether and focuses on those with a positive score, i.e. those with an evolutionarily conserved signal of similarity. A prerequisite for local alignment is a negative expectation score. The expectation score is defined as the average score that the scoring system ([[substitution matrix]] and [[gap penalty|gap penalties]]) would yield for a random sequence.\n\nAnother motivation for using local alignments is that there is a reliable statistical model (developed by Karlin and Altschul) for optimal local alignments. The alignment of unrelated sequences tends to produce optimal local alignment scores which follow an extreme value distribution. This property allows programs to produce an [[expectation value]] for the optimal local alignment of two sequences, which is a measure of how often two unrelated sequences would produce an optimal local alignment whose score is greater than or equal to the observed score. Very low expectation values indicate that the two sequences in question might be [[Homology (biology)|homologous]], meaning they might share a common ancestor.\n\n==Algorithm==\n[[File:Smith-Waterman-Algorithm-Scoring-1.png|frame|right|Scoring method of the Smith–Waterman algorithm]]\nLet <math>A=a_1a_2...a_n</math> and <math>B=b_1b_2...b_m</math> be the sequences to be aligned, where <math>n</math> and <math>m</math> are the lengths of <math>A</math> and <math>B</math> respectively.\n# Determine the substitution matrix and the gap penalty scheme.\n#* <math>s(a,b)</math> - Similarity score of the elements that constituted the two sequences\n#* <math>W_k</math> - The penalty of a gap that has length <math>k</math>\n# Construct a scoring matrix <math>H</math> and initialize its first row and first column. The size of the scoring matrix is <math>(n+1) * (m+1)</math>. Note the 0-based indexing.\n#: <math>H_{k0}=H_{0l}=0 \\quad for \\quad 0\\le k\\le n \\quad and \\quad 0\\le l\\le m</math>\n# Fill the scoring matrix using the equation below.\n#: <math>\nH_{ij} = \\max\\begin{cases} \nH_{i-1,j-1} + s(a_i,b_j), \\\\\n\\max_{k \\ge 1} \\{ H_{i-k,j} - W_k \\}, \\\\\n\\max_{l \\ge 1} \\{ H_{i,j-l} - W_l \\}, \\\\\n0\n\\end{cases} \\qquad (1\\le i\\le n, 1\\le j\\le m)\n</math>\n#: where\n#: <math>H_{i-1,j-1} + s(a_i,b_j)</math> is the score of aligning <math>a_i</math> and <math>b_j</math>,\n#: <math>H_{i-k,j} - W_k</math> is the score if <math>a_i</math> is at the end of a gap of length <math>k</math>,\n#: <math>H_{i,j-l} - W_l</math> is the score if <math>b_j</math> is at the end of a gap of length <math>l</math>,\n#: <math>0</math> means there is no similarity up to <math>a_i</math> and <math>b_j</math>.\n# Traceback. Starting at the highest score in the scoring matrix <math>H</math> and ending at a matrix cell that has a score of 0, traceback based on the source of each score recursively to generate the best local alignment.\n\n==Explanation==\nSmith–Waterman algorithm aligns two sequences by matches/mismatches (also known as substitutions), insertions, and deletions. Both insertions and deletions are the operations that introduce gaps, which are represented by dashes. The Smith–Waterman algorithm has several steps:\n# '''Determine the substitution matrix and the gap penalty scheme'''. A substitution matrix assigns each pair of bases or amino acids a score for match or mismatch. Usually matches get positive scores, whereas mismatches get relatively lower scores. A gap penalty function determines the score cost for opening or extending gaps. It is suggested that users choose the appropriate scoring system based on the goals. In addition, it is also a good practice to try different combinations of substitution matrices and gap penalties.\n# '''Initialize the scoring matrix'''. The dimensions of the scoring matrix are 1+length of each sequence respectively. All the elements of the first row and the first column are set to 0. The extra first row and first column make it possible to align one sequence to another at any position, and setting them to 0 makes the terminal gap free from penalty.\n# '''Scoring'''. Score each element from left to right, top to bottom in the matrix, considering the outcomes of substitutions (diagonal scores) or adding gaps (horizontal and vertical scores). If none of the scores are positive, this element gets a 0. Otherwise the highest score is used and the source of that score is recorded.\n# '''Traceback'''. Starting at the element with the highest score, traceback based on the source of each score recursively, until 0 is encountered. The segments that have the highest similarity score based on the given scoring system is generated in this process. To obtain the second best local alignment, apply the traceback process starting at the second highest score outside the trace of the best alignment.\n\n===Comparison with the Needleman–Wunsch algorithm===\n[[File:Alignment-Comparison-En.png|300px|thumb|right|Global and local sequence alignment]]\nThe Smith–Waterman algorithm finds the segments in two sequences that have similarities while the Needleman–Wunsch algorithm aligns two complete sequences. Therefore, they serve different purposes. Both algorithms use the concepts of a substitution matrix, a gap penalty function, a scoring matrix, and a traceback process. Three main differences are:\n{| class=\"wikitable\"\n|-\n! !! Smith–Waterman algorithm !! Needleman–Wunsch algorithm\n|-\n| Initialization || First row and first column are set to 0 || First row and first column are subject to gap penalty\n|-\n| Scoring || Negative score is set to 0 || Score can be negative\n|-\n| Traceback || Begin with the highest score, end when 0 is encountered || Begin with the cell at the lower right of the matrix, end at top left cell\n|}\nOne of the most important distinctions is that no negative score is assigned in the scoring system of the Smith–Waterman algorithm, which enables local alignment. When any element has a score lower than zero, it means that the sequences up to this position have no similarities; this element will then be set to zero to eliminate influence from previous alignment. In this way, calculation can continue to find alignment in any position afterwards.\n\nThe initial scoring matrix of Smith–Waterman algorithm enables the alignment of any segment of one sequence to an arbitrary position in the other sequence. In Needleman–Wunsch algorithm, however, end gap penalty also needs to be considered in order to align the full sequences.\n\n===Substitution matrix===\nEach base substitution or amino acid substitution is assigned a score. In general, matches are assigned positive scores, and mismatches are assigned relatively lower scores. Take DNA sequence as an example. If matches get +1, mismatches get -1, then the substitution matrix is:\n{| style=\"font-family: monospace\" class=\"wikitable\"\n! !! A !! G !! C !! T\n|- style=\"text-align: right;\"\n! scope=\"row\" | A\n| 1 ||  -1|| -1 || -1 \n|- style=\"text-align: right;\"\n! scope=\"row\" | G\n| -1 || 1 || -1 || -1 \n|- style=\"text-align: right;\"\n! scope=\"row\" | C\n| -1 || -1 || 1 || -1 \n|- style=\"text-align: right;\"\n! scope=\"row\" | T\n| -1 || -1 || -1 ||  1\n|}\nThis substitution matrix can be described as:\n<math>s(a_i,b_j) = \\begin{cases}+1, \\quad a_i=b_j \\\\ -1, \\quad a_i\\ne b_j\\end{cases}</math>\n\nDifferent base substitutions or amino acid substitutions can have different scores. The substitution matrix of amino acids is usually more complicated than that of the bases. See [[Point accepted mutation|PAM]], [[BLOSUM]].\n\n===Gap penalty===\nGap penalty designates scores for insertion or deletion. A simple gap penalty strategy is to use fixed score for each gap. In biology, however, the score needs to be counted differently for practical reasons. On one hand, partial similarity between two sequences is a common phenomenon; on the other hand, a single gene mutation event can result in insertion of a single long gap. Therefore, connected gaps forming a long gap usually is more favored than multiple scattered, short gaps. In order to take this difference into consideration, the concepts of gap opening and gap extension have been added to the scoring system. The gap opening score is usually higher than the gap extension score. For instance, the default parameter in [http://www.ebi.ac.uk/Tools/psa/emboss_water/ EMBOSS Water] are: gap opening = 10, gap extension = 0.5.\n\nHere we discuss two common strategies for gap penalty. See [[Gap penalty]] for more strategies.\nLet <math>W_k</math> be the gap penalty function for a gap of length <math>k</math>:\n\n====Linear====\n[[File:Smith-Waterman-Algorithm-Scoring-2.png|frame|right|Simplified Smith–Waterman algorithm when linear gap penalty function is used]]\nA linear gap penalty has the same scores for opening and extending a gap:\n\n<math>W_k=kW_1</math>,\n\nwhere <math>W_1</math> is the cost of a single gap.\n\nThe gap penalty is directly proportional to the gap length. When linear gap penalty is used, the Smith–Waterman algorithm can be simplified to:\n\n<math>\nH_{ij}=\\max\\begin{cases}\nH_{i-1,j-1}+s(a_i,b_j),\\\\\nH_{i-1,j}-W_1,\\\\\nH_{i,j-1}-W_1,\\\\\n0\n\\end{cases}\n</math>\n\nThe simplified algorithm uses <math>O(mn)</math> steps. When an element is being scored, only the gap penalties from the elements that are directly adjacent to this element need to be considered.\n\n====Affine====\nAn affine gap penalty considers gap opening and extension separately:\n\n<math>W_k=uk+v \\quad (u>0, v>0)</math>,\n\nwhere <math>v</math> is the gap opening penalty, and <math>u</math> is the gap extension penalty. For example, the penalty for a gap of length 2 is <math>2u+v</math>.\n\nAn arbitrary gap penalty was used in the original Smith–Waterman algorithm paper. It uses <math>O(m^2n)</math> steps, therefore is quite demanding of time. Gotoh optimized the steps for an affine gap penalty to <math>O(mn)</math>,<ref name=\"Gotoh1982\" /> but the optimized algorithm only attempts to find one optimal alignment, and the optimal alignment is not guaranteed to be found.<ref name=\"Altschul1986\" /> Altschul modified Gotoh's algorithm to find all optimal alignments while maintaining the computational complexity.<ref name=\"Altschul1986\" /> Later, Myers and Miller pointed out that Gotoh and Altschul's algorithm can be further modified based on the method that was published by Hirschberg in 1975,<ref name=\"Hirschberg1975\">{{cite journal |author=D. S. Hirschberg |title=A linear space algorithm for computing maximal common subsequences |journal=[[Communications of the ACM]] |volume=18 |issue=6 |pages=341–343 |year=1975 |doi=10.1145/360825.360861|citeseerx=10.1.1.348.4774 }}</ref> and applied this method.<ref name=\"Myers1988\" /> Myers and Miller's algorithm can align two sequences using <math>O(n)</math> space, with <math>n</math> being the length of the shorter sequence.\n\n====Gap penalty example====\nTake the alignment of sequences <tt>TACGGGCCCGCTAC</tt> and <tt>TAGCCCTATCGGTCA</tt> as an example.\nWhen linear gap penalty function is used, the result is (Alignments performed by EMBOSS Water. Substitution matrix is DNAfull. Gap opening and extension both are 1.0):\n <tt>TACGGGCCCGCTA-C</tt>\n <tt>||   | || ||| |</tt>\n <tt>TA---G-CC-CTATC</tt>\nWhen affine gap penalty is used, the result is (Gap opening and extension are 5.0 and 1.0 respectively):\n <tt>TACGGGCCCGCTA</tt>\n <tt>||   |||  |||</tt>\n <tt>TA---GCC--CTA</tt>\nThis example shows that an affine gap penalty can help avoid scattered small gaps.\n\n===Scoring matrix===\nThe function of the scoring matrix is to conduct one-to-one comparisons between all components in two sequences and record the optimal alignment results. The scoring process reflects the concept of dynamic programming. The final optimal alignment is found by iteratively expanding the growing optimal alignment. In other words, the current optimal alignment is generated by deciding which path (match/mismatch or inserting gap) gives the highest score from the previous optimal alignment. The size of the matrix is the length of one sequence plus 1 by the length of the other sequence plus 1. The additional first row and first column serve the purpose of aligning one sequence to any positions in the other sequence. Both the first line and the first column are set to 0 so that end gap is not penalized. The initial scoring matrix is:\n{| class=\"wikitable\" style=\"font-family: monospace\"\n! scope=\"col\" style=\"width:18px;\" |\n! scope=\"col\" style=\"width:18px;\" |\n! scope=\"col\" style=\"width:18px;\" | b<sub>1</sub>\n! scope=\"col\" style=\"width:18px;\" | …\n! scope=\"col\" style=\"width:18px;\" | b<sub>j</sub>\n! scope=\"col\" style=\"width:18px;\" | …\n! scope=\"col\" style=\"width:18px;\" | b<sub>m</sub>\n|-\n! scope=\"row\" style=\"height:18px;\" |\n| 0 || 0 || … || 0 || … || 0\n|-\n! scope=\"row\" style=\"height:18px;\" | a<sub>1</sub>\n| 0 || || || || ||\n|-\n! scope=\"row\" style=\"height:18px;\" | …\n| … || || || || ||\n|-\n! scope=\"row\" style=\"height:18px;\" | a<sub>i</sub>\n| 0 || || || || ||\n|-\n! scope=\"row\" style=\"height:18px;\" | …\n| … || || || || ||\n|-\n! scope=\"row\" style=\"height:18px;\" | a<sub>n</sub>\n| 0 || || || || ||\n|}\n\n==Example==\nTake the alignment of DNA sequences <tt>TGTTACGG</tt> and <tt>GGTTGACTA</tt> as an example. Use the following scheme:\n* Substitution matrix: <math>s(a_i,b_j) = \\begin{cases}+3, \\quad a_i=b_j \\\\ -3, \\quad a_i\\ne b_j\\end{cases}</math>\n* Gap penalty: <math>W_k=2k</math> (a linear gap penalty of <math>W_1 = 2</math>)\nInitialize and fill the scoring matrix, shown as below. This figure shows the scoring process of the first three elements. The yellow color indicates the bases that are being considered. The red color indicates the highest possible score for the cell being scored.\n[[File:Smith-Waterman-Algorithm-Example-Step1.png|frame|none|Initialization of the scoring matrix (left 1) and the scoring process of the first three elements (left 2-4)]]\nThe finished scoring matrix is shown below on the left. The blue color shows the highest score. Note that an element can receive score from more than one element, each will form a different path if this element is traced back. In case of multiple highest scores, traceback should be done starting with each highest score. The traceback process is shown below on the right. The best local alignment is generated in the reverse direction.\n{| class=\"wikitable\"\n|-\n| [[File:Smith-Waterman-Algorithm-Example-Step2.png|none]]\n| [[File:Smith-Waterman-Algorithm-Example-Step3.png|none]]\n|-\n| Finished scoring matrix (the highest score is in blue)\n| Traceback process and alignment result\n|}\nThe alignment result is:\n <tt>G T T - A C</tt>\n <tt>| | |   | |</tt>\n <tt>G T T G A C</tt>\n\n==Implementation==\nAn implementation of the Smith–Waterman Algorithm, SSEARCH, is available in the [[FASTA]] sequence analysis package from [http://fasta.bioch.virginia.edu/fasta_www2/fasta_down.shtml]. This implementation includes [[Altivec]] accelerated code for [[PowerPC]] G4 and G5 processors that speeds up comparisons 10–20-fold, using a modification of the Wozniak, 1997 approach,<ref name=\"Wozniak1997\">{{cite journal |author=Wozniak, Andrzej \n|title=Using video-oriented instructions to speed up sequence comparison \n|journal=Computer Applications in Biosciences (CABIOS) |volume=13 |issue=2 |pages=145–50 |year=1997 |url=http://bioinformatics.oxfordjournals.org/cgi/reprint/13/2/145.pdf |doi=10.1093/bioinformatics/13.2.145}}</ref> and an SSE2 vectorization developed by Farrar<ref name=\"Farrar2007\">{{cite journal |author=Farrar, Michael S. \n|title=Striped Smith–Waterman speeds database searches six times over other SIMD implementations \n|journal=Bioinformatics |volume=23 |issue=2 |pages=156–161 |year=2007 |url=http://bioinformatics.oxfordjournals.org/cgi/reprint/23/2/156.pdf |doi=10.1093/bioinformatics/btl582 |pmid=17110365 }}</ref> making optimal protein [[sequence database]] searches quite practical. A library, SSW, extends Farrar's implementation to return alignment information in addition to the optimal Smith–Waterman score.<ref>{{cite journal |last=Zhao |first=Mengyao |last2=Lee |first2=Wan-Ping |last3=Garrison |first3=Erik P |last4=Marth |first4=Gabor T |date=4 December 2013 |title=SSW Library: An SIMD Smith-Waterman C/C++ Library for Use in Genomic Applications |journal=PLoS ONE |volume=8 |issue=12 |doi=10.1371/journal.pone.0082138 |pages=e82138 |pmid=24324759 |pmc=3852983}}</ref>\n\n==Accelerated versions==\n\n===FPGA===\n[[Cray]] demonstrated acceleration of the Smith–Waterman algorithm using a [[reconfigurable computing]] platform based on [[Field-programmable gate array|FPGA]] chips, with results showing up to 28x speed-up over standard microprocessor-based solutions. Another FPGA-based version of the Smith–Waterman algorithm shows FPGA (Virtex-4) speedups up to 100x<ref>FPGA 100x Papers: {{cite web |url=http://ft.ornl.gov/~olaf/pubs/OlafRSSI2July07.pdf |title=Archived copy |accessdate=2007-10-17 |deadurl=yes |archiveurl=https://web.archive.org/web/20080705191338/http://ft.ornl.gov/~olaf/pubs/OlafRSSI2July07.pdf |archivedate=2008-07-05 |df= }}, {{cite web |url=http://ft.ornl.gov/~olaf/pubs/CUG07Olaf17M07.pdf |title=Archived copy |accessdate=2007-10-17 |deadurl=yes |archiveurl=https://web.archive.org/web/20080705191314/http://ft.ornl.gov/~olaf/pubs/CUG07Olaf17M07.pdf |archivedate=2008-07-05 |df= }}, and {{cite web |url=http://ft.ornl.gov/~olaf/pubs/RSSIOlafDave.pdf |title=Archived copy |accessdate=2007-10-17 |deadurl=yes |archiveurl=https://web.archive.org/web/20110720080855/http://ft.ornl.gov/~olaf/pubs/RSSIOlafDave.pdf |archivedate=2011-07-20 |df= }}</ref> over a 2.2&nbsp;GHz Opteron processor.<ref name=Progeniq>Progeniq Pte. Ltd., \"[http://www.progeniq.com/news/BioBoost%20White%20Paper.pdf White Paper - Accelerating Intensive Applications at 10&times;–50&times; Speedup to Remove Bottlenecks in Computational Workflows]\".</ref>  The [http://www.timelogic.com TimeLogic] DeCypher and CodeQuest systems also accelerate Smith–Waterman and Framesearch using PCIe FPGA cards.\n\nA 2011 Master's thesis <ref name=\"Vermij2011\">{{Cite thesis |degree=M.Sc. |first=Erik |last=Vermij |title=Genetic sequence alignment on a supercomputing platform |publisher=Delft University of Technology |year=2011 |url=http://repository.tudelft.nl/assets/uuid:bbbbd3c8-7b27-4a1b-bfd6-67695eec7449/thesis.pdf }}</ref> includes an analysis of FPGA-based Smith–Waterman acceleration.\n\nIn a 2016 publication [https://forums.xilinx.com/t5/Xcell-Daily-Blog/OpenCL-code-compiled-with-Xilinx-SDAccel-accelerates-genome/ba-p/680764], a very efficient implementation was presented. Using one PCIe FPGA card equipped with a Xilinx Virtex-7 2000T FPGA, the performance per Watt level was better than CPU/GPU by 12-21x.\n\n===GPU===\n[[Lawrence Livermore National Laboratory]] and the US Department of Energy's [[Joint Genome Institute]] implemented an accelerated version of Smith–Waterman local sequence alignment searches using [[graphics processing units]] (GPUs) with preliminary results showing a 2x speed-up over software implementations.<ref>{{cite book |title=GPU Accelerated Smith–Waterman \n|publisher=SpringerLink | doi=10.1007/11758549_29 |journal=Lecture Notes in Computer Science |volume=3994 \n|pages=188–195|year=2006 \n|last1=Liu \n|first1=Yang \n|last2=Huang \n|first2=Wayne \n|last3=Johnson \n|first3=John \n|last4=Vaidya \n|first4=Sheila \n|isbn=978-3-540-34385-1 \n}}</ref> A similar method has already been implemented in the Biofacet software since 1997, with the same speed-up factor.<ref>{{cite web\n |url         = http://www.genomequest.com/contact-bioinformatics-ht.html\n |title       = Bioinformatics High Throughput Sequence Search and Analysis (white paper)\n |accessdate  = 2008-05-09\n |publisher   = GenomeQuest\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20080513152726/http://www.genomequest.com/contact-bioinformatics-ht.html\n |archivedate = May 13, 2008\n |df          = \n}}</ref>\n\nSeveral [[GPU]] implementations of the algorithm in [[NVIDIA]]'s [[CUDA]] C platform are also available.<ref name=\"Manavski2008\">{{cite journal |doi=10.1186/1471-2105-9-S2-S10 |author1=Manavski, Svetlin A. |author2=Valle, Giorgio |last-author-amp=yes |title=CUDA compatible GPU cards as efficient hardware accelerators for Smith–Waterman sequence alignment \n|journal=BMC Bioinformatics |volume=9 |issue=Suppl 2:S10 |year=2008 |page=S10 |pmid=18387198 |pmc=2323659 }}</ref> When compared to the best known CPU implementation (using SIMD instructions on the x86 architecture), by Farrar, the performance tests of this solution using a single [[GeForce 8 Series|NVidia GeForce 8800 GTX]] card show a slight increase in performance for smaller sequences, but a slight decrease in performance for larger ones. However the same tests running on dual [[GeForce 8 Series|NVidia GeForce 8800 GTX]] cards are almost twice as fast as the Farrar implementation for all sequence sizes tested.\n\nA newer GPU CUDA implementation of SW is now available that is faster than previous versions and also removes limitations on query lengths.  See [http://www.nvidia.com/object/swplusplus_on_tesla.html CUDASW++].\n\nEleven different SW implementations on CUDA have been reported, three of which report speedups of 30X.<ref>{{cite web |url=http://www.nvidia.com/object/cuda_showcase_html.html\n|title=CUDA Zone\n|accessdate=2010-02-25 |publisher=Nvidia}}</ref>\n\n===SIMD===\nIn 2000, a fast implementation of the Smith–Waterman algorithm using the [[SIMD]] technology available in [[Intel]] [[Pentium (brand)|Pentium]] [[MMX (instruction set)|MMX]] processors and similar technology was described in a publication by Rognes and Seeberg.<ref name=\"RognesSeeberg2000\">{{cite journal|author1=Rognes, Torbjørn |author2=Seeberg, Erling |last-author-amp=yes |title=Six-fold speed-up of Smith–Waterman sequence database searches using parallel processing on common microprocessors \n|journal=[http://bioinformaticsARRAYxfordjournalsARRAYrg/ Bioinformatics]|volume=16|pages=699–706|year=2000 |url=http://bioinformatics.oxfordjournals.org/cgi/reprint/16/8/699.pdf|issue=8 |doi=10.1093/bioinformatics/16.8.699}}</ref> In contrast to the Wozniak (1997) approach, the new implementation was based on vectors parallel with the query sequence, not diagonal vectors. The company [http://www.sencel.com/ Sencel Bioinformatics] has applied for a patent covering this approach. Sencel is developing the software further and provides executables for academic use free of charge.\n\nA [[SSE2]] vectorization of the algorithm (Farrar, 2007) is now available providing an 8-16-fold speedup on Intel/AMD processors with SSE2 extensions.<ref name=\"Farrar2007\"/> When running on Intel processor using the [[Core (microarchitecture)|Core microarchitecture]] the SSE2 implementation achieves a 20-fold increase.  Farrar's SSE2 implementation is available as the SSEARCH program in the [[FASTA]] sequence comparison package.  The SSEARCH is included in the [[European Bioinformatics Institute]]'s suite of [http://www.ebi.ac.uk/Tools/SSS/ similarity searching programs].\n\nDanish bioinformatics company [[CLC bio]] has achieved speed-ups of close to 200 over standard software implementations with SSE2 on an Intel 2.17&nbsp;GHz Core 2 Duo CPU, according to a [https://web.archive.org/web/20070811101052/http://www.clccell.com/download.html publicly available white paper].\n\nAccelerated version of the Smith–Waterman algorithm, on [[Intel]] and [[AMD]] based Linux servers, is supported by the [http://www.biocceleration.com/GenCore6-General.html GenCore 6] package, offered by [http://www.biocceleration.com/ Biocceleration]. Performance benchmarks of this software package show up to 10 fold speed acceleration relative to standard software implementation on the same processor.\n\nCurrently the only company in bioinformatics to offer both SSE and FPGA solutions accelerating Smith–Waterman, [[CLC bio]] has achieved speed-ups of more than 110 over standard software implementations with [http://www.clccube.com CLC Bioinformatics Cube] {{Citation needed|reason=please give a reliable source for this assertion. Commercial company's website. There are other companies who provide this solution.|date=November 2011}}\n\nThe fastest implementation of the algorithm on CPUs with [[SSSE3]] can be found the SWIPE software (Rognes, 2011),<ref name=\"Rognes2011\">{{cite journal|author=Rognes, Torbjørn|title=Faster Smith–Waterman database searches with inter-sequence SIMD parallelisation|journal=[[BMC Bioinformatics]] |volume=12|pages=221|year=2011|doi=10.1186/1471-2105-12-221|pmid=21631914|pmc=3120707}}</ref> which is available under the [[GNU Affero General Public License]]. In parallel, this software compares residues from sixteen different database sequences to one query residue. Using a 375 residue query sequence a speed of 106 billion cell updates per second (GCUPS) was achieved on a dual Intel [[Xeon]] X5650 six-core processor system, which is over six times more rapid than software based on Farrar's 'striped' approach. It is faster than [[BLAST]] when using the BLOSUM50 matrix.\n\nThere also exists diagonalsw, a C and C++ implementation of the Smith–Waterman algorithm with the SIMD instruction sets ([[SSE4|SSE4.1]] for the x86 platform and AltiVec for the PowerPC platform). It is licensed under the open-source MIT license.\n\n===Cell Broadband Engine===\nIn 2008, Farrar<ref name=\"Farrar2008\">{{cite journal\n |author = Farrar, Michael S.\n |title  = Optimizing Smith–Waterman for the Cell Broadband Engine\n |year   = 2008\n |url    = http://farrar.michael.googlepages.com/smith-watermanfortheibmcellbe\n|archive-url = https://archive.is/20120212044037/http://farrar.michael.googlepages.com/smith-watermanfortheibmcellbe\n|dead-url = yes\n|archive-date = 2012-02-12\n}}</ref> described a port of the Striped Smith–Waterman<ref name=\"Farrar2007\"/> to the [[Cell Broadband Engine]] and reported speeds of 32 and 12 GCUPS on an [[IBM BladeCenter#QS20|IBM QS20 blade]] and a Sony [[PlayStation 3]], respectively.\n\n==See also==\n*[[Bioinformatics]]\n*[[Sequence alignment]]\n*[[Sequence mining]]\n*[[Needleman–Wunsch algorithm]]\n*[[Levenshtein distance]]\n*[[BLAST]]\n*[[FASTA]]\n\n==References==\n{{reflist|2}}\n\n==External links==\n*[http://jaligner.sourceforge.net/ JAligner] &mdash; an open source Java implementation of the Smith–Waterman algorithm\n*[http://baba.sourceforge.net/ B.A.B.A.] &mdash; an applet (with source) which visually explains the algorithm\n*[http://www.ebi.ac.uk/Tools/fasta FASTA/SSEARCH] &mdash; services page at the [[European Bioinformatics Institute|EBI]]\n*[https://ugene.net/wiki/display/UUOUM15/Smith-Waterman+Search UGENE Smith–Waterman plugin] &mdash; an open source SSEARCH compatible implementation of the algorithm with graphical interface written in C++\n*[https://github.com/Martinsos/opal OPAL] &mdash; an SIMD C/C++ library for massive optimal sequence alignment\n*[http://diagonalsw.sourceforge.net/ diagonalsw] &mdash; an open-source C/C++ implementation with SIMD instruction sets (notably SSE4.1) under the MIT license\n*[https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library SSW] &mdash; an open-source C++ library providing an API to an SIMD implemention of the Smith–Waterman algorithm under the MIT license\n*[https://melodic-sequence-alignment.firebaseapp.com/#/ melodic sequence alignment] &mdash; a javascript implementation for melodic sequence alignment\n\n{{Bioinformatics}}\n{{Strings}}\n\n{{DEFAULTSORT:Smith-Waterman algorithm}}\n[[Category:Bioinformatics algorithms]]\n[[Category:Computational phylogenetics]]\n[[Category:Sequence alignment algorithms]]\n[[Category:Dynamic programming]]"
    },
    {
      "title": "Subset sum problem",
      "url": "https://en.wikipedia.org/wiki/Subset_sum_problem",
      "text": "{{refimprove|date=December 2008}}\n\nIn [[computer science]], the '''subset sum problem''' is an important [[decision problem]] in [[computational complexity theory|complexity theory]] and [[cryptography]]. There are several equivalent formulations of the problem. One of them is: given a set (or [[multiset]]) of integers, is there a non-empty subset whose sum is zero?  For example, given the set <math>\\{-7, -3, -2, 5, 8\\}</math>, the answer is ''yes'' because the subset <math>\\{-3, -2, 5\\}</math> sums to zero.  The problem is [[NP-complete]], meaning roughly that while it is easy to confirm whether a proposed solution is valid, it may inherently be prohibitively difficult to determine in the first place whether any solution exists.\n\nThe problem can be equivalently formulated as: given the integers or natural numbers <math>w_1,\\ldots,w_n</math> does any subset of them sum to precisely <math>W</math>?<ref name = \"kleinberg2006p491\">{{cite book|year=2006|edition=2nd|title=Algorithm Design|first1=Jon|last1=Kleinberg|first2=Éva|last2=Tardos|isbn=0-321-37291-3|page=491}}</ref> Subset sum can also be thought of as a special case of the [[knapsack problem]].<ref name=\"MartelloToth\">{{cite book |chapter=4 Subset-sum problem|pages=105–136| title = Knapsack problems: Algorithms and computer interpretations | last1=Martello|first1=Silvano|last2=Toth|first2=Paolo| publisher =Wiley-Interscience | year = 1990 | isbn = 0-471-92420-2|mr=1086874|ref=harv}}</ref>  One interesting special case of subset sum is the [[partition problem]], in which <math>W</math> is half of the sum of all elements in the set (i.e., <math> W = \\frac{1}{2}(w_1+\\dots+w_n)</math>).\n\n== Complexity ==\n\nThe [[Computational complexity theory|complexity]] of the subset sum problem can be viewed as depending on two parameters, ''N'', the number of decision variables, and ''P'', the precision of the problem (stated as the number of binary place values that it takes to state the problem). (Note: here the letters ''N'' and ''P'' mean something different from what they mean in the '''[[NP (complexity)|NP]]''' class of problems.)\n\nThe complexity of the best known algorithms is [[Exponential time|exponential]] in the smaller of the two parameters ''N'' and ''P''. Thus, the problem is most difficult if ''N'' and ''P'' are of the same order. It only becomes easy if either ''N'' or ''P'' becomes very small.\n\nIf ''N'' (the number of variables) is small, then an [[exhaustive search]] for the solution is practical. If ''P'' (the number of place values) is a small fixed number, then there are [[dynamic programming]] algorithms that can solve it exactly.\n\nEfficient algorithms for both small ''N'' and small ''P'' cases are given below.\n\n== Exponential time algorithm ==\n\nThere are several ways to solve subset sum in time exponential in <math>n</math>. The most [[naive solution|naïve algorithm]] would be to cycle through all subsets of <math>n</math> numbers and, for every one of them, check if the subset sums to the right number. The running time is of order <math>O(2^nn)</math>, since there are <math>2^n</math> subsets and, to check each subset, we need to sum at most <math>n</math> elements.\n\nA better exponential time algorithm is known which runs in time ''O''(2<sup>''N''/2</sup>). The algorithm splits arbitrarily the ''N'' elements into two sets of ''N''/2 each. For each of these two sets, it stores a list of the sums of all 2<sup>''N''/2</sup> possible subsets of its elements. Each of these two lists is then sorted. Using a standard comparison sorting algorithm for this step would take time ''O''(2<sup>''N''/2</sup>''N'').  However, given a sorted list of sums for ''k'' elements, the list can be expanded to two sorted lists with the introduction of a (''k''&nbsp;+&nbsp;1)st element, and these two sorted lists can be merged in time ''O''(2<sup>''k''</sup>).  Thus, each list can be generated in sorted form in time ''O''(2<sup>''N''/2</sup>).  Given the two sorted lists, the algorithm can check if an element of the first array and an element of the second array sum up to ''s'' in time ''O''(2<sup>''N''/2</sup>). To do that, the algorithm passes through the first array in decreasing order (starting at the largest element) and the second array in increasing order (starting at the smallest element). Whenever the sum of the current element in the first array and the current element in the second array is more than ''s'', the algorithm moves to the next element in the first array. If it is less than ''s'', the algorithm moves to the next element in the second array. If two elements with sum ''s'' are found, it stops.  Horowitz and [[Sartaj Sahni|Sahni]] first published this algorithm in a technical report in 1974.<ref>{{citation\n | last1 = Horowitz | first1 = Ellis\n | last2 = Sahni | first2 = Sartaj | author2-link = Sartaj Sahni\n | doi = 10.1145/321812.321823\n | journal = [[Journal of the Association for Computing Machinery]]\n | mr = 0354006\n | pages = 277–292\n | title = Computing partitions with applications to the knapsack problem\n | volume = 21\n | year = 1974}}</ref>\n\n== Pseudo-polynomial time dynamic programming solution ==\n\nThe problem can be solved in [[pseudo-polynomial time]] using [[dynamic programming]]. Suppose the sequence is \n:''x''<sub>1</sub>, ..., ''x<sub>N</sub>''\n\nsorted in the increasing order and we wish to determine if there is a nonempty subset which sums to zero.  Define the boolean-valued function ''Q''(''i'', ''s'') to be the value ('''true''' or '''false''') of \n:\"there is a nonempty subset of ''x''<sub>1</sub>, ..., ''x<sub>i</sub>'' which sums to ''s''\".\n\nThus, the solution to the problem \"Given a set of integers, is there a non-empty subset whose sum is zero?\" is the value of ''Q''(''N'', 0).\n\nLet ''A'' be the sum of the negative values and ''B'' the sum of the positive values.  Clearly, {{nowrap|''Q''(''i'', ''s'') {{=}} '''false'''}}, if {{nowrap|''s'' < ''A''}} or {{nowrap|''s'' > ''B''}}. So these values do not need to be stored or computed.\n\nCreate an array to hold the values ''Q''(''i'', ''s'') for {{nowrap|1 &le; ''i'' &le; ''N''}} and {{nowrap|''A'' &le; ''s'' &le; ''B''.}}\n\nThe array can now be filled in using a simple recursion.  Initially, for  {{nowrap|''A'' &le; ''s'' &le; ''B'',}} set\n:''Q''(1, ''s'') := (''x''<sub>1</sub> == ''s'')\n\nwhere ''=='' is a boolean function that returns true if ''x''<sub>1</sub> is equal to ''s'', false otherwise.\n\nThen, for ''i'' = 2, …, ''N'', set\n:''Q''(''i'', ''s'') := ''Q''(''i'' − 1, ''s'') '''or''' (''x<sub>i</sub>'' == ''s'') '''or''' ''Q''(''i'' − 1, ''s'' − ''x<sub>i</sub>''),  &nbsp;for ''A'' &le; ''s'' &le; ''B''.\n\nFor each assignment, the values of ''Q'' on the right side are already known, either because they were stored in the table for the previous value of ''i'' or because {{nowrap|''Q''(''i'' − 1,''s'' − ''x<sub>i</sub>'') {{=}} '''false'''}} if {{nowrap|''s'' − ''x<sub>i</sub>'' < ''A''}} or {{nowrap|''s'' − ''x<sub>i</sub>'' > ''B''.}} Therefore, the total number of arithmetic operations is {{nowrap|''O''(''N''(''B'' − ''A'')).}} For example, if all the values are ''O''(''N<sup>k</sup>'') for some ''k'', then the time required is ''O''(''N''<sup>''k''+2</sup>).\n\nThis algorithm is easily modified to return the subset with sum 0 if there is one.\n\nThe dynamic programming solution has runtime of <math>O(sN)</math> where <math>s</math> is the sum we want to find in set of <math>N</math> numbers. This solution does not count as polynomial time in complexity theory because {{nowrap|''B'' − ''A''}} is not polynomial in the ''size'' of the problem, which is the number of bits used to represent it.  This algorithm is polynomial in the values of ''A'' and ''B'', which are exponential in their numbers of bits.\n\nFor the case that each ''x<sub>i</sub>'' is positive and bounded by a fixed constant ''C'', [http://www.diku.dk/~pisinger/ Pisinger] found a linear time algorithm having time complexity {{nowrap|''O''(''NC'')}} (note that this is for the version of the problem where the target sum is not necessarily zero, otherwise the problem would be trivial).<ref name=Pisinger09>Pisinger D (1999). \"Linear Time Algorithms for Knapsack Problems with Bounded Weights\". ''Journal of Algorithms'', Volume 33, Number 1, October 1999, pp. 1–14</ref> In 2015, Koiliaris and Xu found the <math>\\tilde{O}(s \\sqrt N)</math> algorithm for the subset sum problem where <math>s</math> is the sum we need to find.<ref>{{Cite arxiv|title = A Faster Pseudopolynomial Time Algorithm for Subset Sum |arxiv = 1507.02318 |date = 2015-07-08|first = Konstantinos|last = Koiliaris|first2 = Chao|last2 = Xu}}</ref>\n\n== Polynomial time approximate algorithm ==\n\nAn [[approximation algorithm|approximate]] version of the subset sum would be: given a set of ''N'' numbers ''x''<sub>1</sub>, ''x''<sub>2</sub>, ..., ''x<sub>N</sub>'' and a number ''s'', output\n* yes, if there is a subset that sums up to ''s'';\n* no, if there is no subset summing up to a number between {{nowrap|(1 − ''c'')''s''}} and ''s'' for some small {{nowrap|''c'' > 0;}}\n* any answer, if there is a subset summing up to a number between {{nowrap|(1 − ''c'')''s''}} and ''s'' but no subset summing up to ''s''.\nIf all numbers are non-negative, the approximate subset sum is solvable in time polynomial in ''N'' and 1/''c''.\n\nThe solution for subset sum also provides the solution for the original subset sum problem in the case where the numbers are small (again, for nonnegative numbers). If any sum of the numbers can be specified with at most ''P'' bits, then solving the problem approximately with {{nowrap|''c'' {{=}} 2<sup>−''P''</sup>}} is equivalent to solving it exactly. Then, the polynomial time algorithm for approximate subset sum becomes an exact algorithm with running time polynomial in ''N'' and 2<sup>''P''</sup> (i.e., exponential in ''P'').\n\nThe algorithm for the approximate subset sum problem is as follows:\n  initialize a list ''S'' to contain one element 0.\n  for each ''i'' from 1 to ''N'' do\n    let ''T'' be a list consisting of ''x<sub>i</sub>'' + ''y'', for all ''y'' in ''S''\n    let ''U'' be the union of ''T'' and ''S''\n    sort ''U''\n    make ''S'' empty \n    let ''y'' be the smallest element of ''U'' \n    add ''y'' to ''S'' \n    for each element ''z'' of ''U'' in increasing order do\n       //trim the list by eliminating numbers close to one another\n       //and throw out elements greater than ''s''\n      if ''y'' + ''cs''/''N'' < ''z'' ≤ ''s'', set ''y'' = ''z'' and add ''z'' to ''S'' \n  if ''S'' contains a number between (1 − ''c'')''s'' and ''s'', output ''yes'', otherwise ''no''\nThe algorithm is polynomial time because the lists ''S'', ''T'' and ''U'' always remain of size polynomial in ''N'' and 1/''c'' and, as long as they are of polynomial size, all operations on them can be done in polynomial time. The size of lists is kept polynomial by the trimming step, in which we only include a number ''z'' into ''S'' if it is greater than the previous one by ''cs''/''N'' and not greater than ''s''.\n\nThis step ensures that each element in ''S'' is smaller than the next one by at least ''cs''/''N'' and do not contain elements greater than ''s''. Any list with that property consists of no more than ''N''/''c'' elements.\n\nThe algorithm is correct because each step introduces an additive error of at most ''cs''/''N'' and ''N'' steps together introduce the error of at most ''cs''.\n\n== See also ==\n* [[3SUM]]\n* [[Merkle–Hellman knapsack cryptosystem]]\n\n== References ==\n{{reflist}}\n\n==Further reading==\n* {{Introduction to Algorithms|2|chapter=35.5: The subset-sum problem}}\n* {{cite book|author = [[Michael R. Garey]] and [[David S. Johnson]] | year = 1979 | title = Computers and Intractability: A Guide to the Theory of NP-Completeness | publisher = W.H. Freeman | isbn = 0-7167-1045-5}} A3.2: SP13, pg.223.\n\n{{DEFAULTSORT:Subset Sum Problem}}\n[[Category:Weakly NP-complete problems]]\n[[Category:Dynamic programming]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Viscosity solution",
      "url": "https://en.wikipedia.org/wiki/Viscosity_solution",
      "text": "In [[mathematics]], the '''viscosity solution''' concept was introduced in the early 1980s by [[Pierre-Louis Lions]] and [[Michael G. Crandall]] as a generalization of the classical concept of what is meant by a 'solution' to a [[partial differential equation]] (PDE).  It has been found that the viscosity solution is the natural solution concept to use in many applications of PDE's, including for example first order equations arising in [[optimal control]] (the [[Hamilton–Jacobi equation]]), [[differential game]]s (the [[Isaacs equation]]) or front evolution problems,<ref>I. Dolcetta and P. Lions, eds., (1995), ''Viscosity Solutions and Applications.'' Springer, {{isbn|978-3-540-62910-8}}.</ref> as well as second-order equations such as the ones arising in stochastic optimal control or stochastic differential games.\n\nThe classical concept was that a PDE\n:<math> F(x,u,Du,D^2 u) = 0 </math>\nover a domain <math>x\\in\\Omega</math> has a solution if we can find a [[function (mathematics)|function]] ''u''(''x'') continuous and differentiable over the entire domain such that <math>x</math>, <math>u</math>, <math>Du</math>, <math> D^2 u</math> satisfy the above equation at every point.\n\nIf a scalar equation is degenerate elliptic (defined below), one can define a type of [[weak solution]] called ''viscosity solution''.\nUnder the viscosity solution concept, ''u'' need not be everywhere differentiable. There may be points where either <math>Du</math> or <math> D^2 u</math> does not exist and yet ''u'' satisfies the equation in an appropriate generalized sense. The definition allows only for certain kind of singularities, so that existence, uniqueness, and stability under uniform limits, hold for a large class of equations.\n\n== Definition ==\nThere are several equivalent ways to phrase the definition of viscosity solutions. See for example the section II.4 of Fleming and Soner's book<ref>\nWendell H. Fleming, H. M . Soner., eds., (2006), ''Controlled Markov Processes and Viscosity Solutions.'' Springer, {{isbn|978-0-387-26045-7}}.</ref> or the definition using semi-jets in the Users Guide.<ref name=\"CIL\">{{Citation | last1=Crandall | first1=Michael G. | last2=Ishii | first2=Hitoshi | last3=Lions | first3=Pierre-Louis | title=User's guide to viscosity solutions of second order partial differential equations | doi=10.1090/S0273-0979-1992-00266-5 | year=1992 | journal=American Mathematical Society. Bulletin. New Series | issn=0002-9904 | volume=27 | issue=1 | pages=1–67| arxiv=math/9207212 }}</ref>\n\n; Degenerate elliptic : An equation <math> F(x,u,Du,D^2 u) = 0 </math> in a domain <math> \\Omega </math> is defined to be ''degenerate elliptic'' if for any two symmetric matrices <math>X</math> and <math>Y</math> such that <math>Y-X</math> is [[Positive-definite matrix|positive definite]], and any values of <math>x \\in \\Omega</math>, <math>u \\in \\mathbb{R}</math> and <math>p \\in \\mathbb{R}^n</math>, we have  the inequality <math> F(x,u,p,X) \\geq F(x,u,p,Y) </math>. For example, <math> -\\Delta u = 0 </math> is degenerate elliptic since in this case, <math> F(x,u,p,X) = -\\text{trace}(X) </math>, and the [[trace (linear algebra) | trace]] of <math> X </math> is the sum of its eigenvalues. Any real first order equation is degenerate elliptic.\n\n; Subsolution : An [[upper semicontinuous]] function <math>u</math> in <math>\\Omega</math> is defined to be a ''subsolution'' of a degenerate elliptic equation in the ''viscosity sense'' if for any point <math>x_0 \\in \\Omega</math> and any <math>C^2</math> function <math>\\phi</math> such that <math>\\phi(x_0) = u(x_0)</math> and <math>\\phi \\geq u</math> in a [[neighborhood (topology)|neighborhood]] of <math>x_0</math>, we have <math> F(x_0,\\phi(x_0),D\\phi(x_0),D^2 \\phi(x_0)) \\leq 0 </math>.\n\n; Supersolution : A [[lower semicontinuous]] function <math>u</math> in <math>\\Omega</math> is defined to be a ''supersolution'' of a degenerate elliptic equation in the ''viscosity sense'' if for any point <math>x_0 \\in \\Omega</math> and any <math>C^2</math> function <math>\\phi</math> such that <math>\\phi(x_0) = u(x_0)</math> and <math>\\phi \\leq u</math> in a [[neighborhood (topology)|neighborhood]] of <math>x_0</math>, we have <math> F(x_0,\\phi(x_0),D\\phi(x_0),D^2 \\phi(x_0)) \\geq 0 </math>.\n\n; Viscosity solution : A [[continuous function|continuous]] function ''u'' is a ''viscosity solution'' of the PDE if it is both a supersolution and a subsolution.\n\n== Example ==\n\nConsider the boundary value problem <math>|u'(x)| = 1</math>, or <math>F(u') = |u'| -1 = 0</math>, on <math>(-1,1)</math> with boundary conditions <math>u(-1) = u(1) = 0</math>. The function <math>u(x) = 1-|x|</math> is the unique viscosity solution. To see this, note that the boundary conditions are satisfied, and <math>|u'(x)| = 1</math> is well-defined on the interior except at <math>x = 0</math>. Thus, it remains to show that the conditions for subsolution and supersolution hold at <math>x=0</math>.\n\nFirst, suppose that <math>\\phi(x)</math> is any function differentiable at <math>x=0</math> with <math>\\phi(0) = u(0) = 1</math> and <math>\\phi(x) \\geq u(x)</math> near <math>x=0</math>. From these assumptions, it follows that <math>\\phi(x) - \\phi(0) \\geq -|x|</math>. For positive <math>x</math>, this inequality implies <math>\\lim_{x \\to 0^+} \\frac{\\phi(x) - \\phi(0)}{x} \\geq -1</math>, using that <math>|x| / x = sgn(x) = 1</math> for <math>x > 0</math>. On the other hand, for <math>x < 0</math>, we have that <math>\\lim_{x \\to 0^-} \\frac{\\phi(x) - \\phi(0)}{x} \\leq 1</math>. Because <math>\\phi</math> is differentiable, the left and right limits agree and are equal to <math>\\phi'(0)</math>, and we therefore conclude that <math>|\\phi'(0)| \\leq 1</math>, i.e., <math>F(\\phi') \\leq 0</math>. Thus, <math>u</math> is a subsolution. Moreover, the fact that <math>u</math> is a supersolution holds vacuously, since there is no function <math>\\phi(x)</math> differentiable at <math>x=0</math> with <math>\\phi(0) = u(0) = 1</math> and <math>\\phi(x) \\leq u(x)</math> near <math>x=0</math>. This implies that <math>u</math> is a viscosity solution.\n\n=== Discussion ===\n\n[[File:Vanishing_viscosity_solutions.svg|thumb|top|right|Family of solutions <math>u_\\epsilon</math> converging toward <math>u(x) = 1-|x|</math>.]]\n\nThe previous boundary value problem is an [[Eikonal equation]] in a single spatial dimension with <math>f = 1</math>, where the solution is known to be the [[signed distance function]] to the boundary of the domain. Note also in the previous example, the importance of the sign of <math>F</math>. In particular, the viscosity solution to the PDE <math>-F = 0</math> with the same boundary conditions is <math>u(x) = |x| - 1</math>. This can be explained by observing that the solution <math>u(x) = 1-|x|</math> is the limiting solution of the vanishing viscosity problem <math>F(u') = [u']^2 - 1 = \\epsilon u''</math> as <math>\\epsilon</math> goes to zero, while <math>u(x) = |x| - 1</math> is the limit solution of the vanishing viscosity problem <math>-F(u') = 1 - [u']^2 = \\epsilon u''</math> <ref name=\"GB\">{{Citation | last1=Barles | first1=Guy | title=An introduction to the theory of viscosity solutions for first-order hamilton–jacobi equations and applications | year=2013 }}</ref>. One can readily confirm that <math>u_\\epsilon(x) = \\epsilon [\\ln(\\cosh(1/\\epsilon)) - \\ln(\\cosh(x/\\epsilon))</math> solves the PDE <math>F(u') = [u']^2 - 1 = \\epsilon u''</math> for each epsilon. Further, the family of solutions <math>u_\\epsilon</math> converge toward the solution <math>u = 1-|x|</math> as <math>\\epsilon</math> vanishes (see Figure).\n\n== Basic properties ==\n\nThe three basic properties of viscosity solutions are ''existence'', ''uniqueness'' and ''stability''.\n* The '''uniqueness''' of solutions requires some extra structural assumptions on the equation. Yet it can be shown for a very large class of degenerate elliptic equations.<ref name=\"CIL\"/> It is a direct consequence of the ''comparison principle''. Some simple examples where comparison principle holds are\n# <math>u+H(x,\\nabla u) = 0</math> with ''H'' [[uniformly continuous]] in ''x''.\n# (Uniformly elliptic case) <math>F(D^2 u, Du, u) = 0</math> so that <math>F</math> is Lipschitz with respect to all variables and for every <math>r \\leq s </math> and <math>X \\geq Y</math>, <math>F(Y,p,s) \\geq F(X,p,r) + \\lambda ||X-Y||</math> for some <math>\\lambda>0</math>.\n* The '''existence''' of solutions holds in all cases where the comparison principle holds and the boundary conditions can be enforced in some way (through [[barrier function]]s in the case of a [[Dirichlet boundary condition]]). For first order equations, it can be obtained using the [[vanishing viscosity]] method <ref name=\"CL\">{{Citation | last1=Crandall | first1=Michael G. | last2=Lions | first2=Pierre-Louis | title=Viscosity solutions of Hamilton-Jacobi equations | doi=10.2307/1999343 | year=1983 | journal=[[Transactions of the American Mathematical Society]] | issn=0002-9947 | volume=277 | issue=1 | pages=1–42| jstor=1999343 }}</ref> or for most equations using Perron's method.<ref name=\"I1\">{{Citation | last1=Ishii | first1=Hitoshi | title=Perron's method for Hamilton-Jacobi equations | doi=10.1215/S0012-7094-87-05521-9 | year=1987 | journal=[[Duke Mathematical Journal]] | issn=0012-7094 | volume=55 | issue=2 | pages=369–384}}</ref><ref name=\"I2\">{{Citation | last1=Ishii | first1=Hitoshi | title=On uniqueness and existence of viscosity solutions of fully nonlinear second-order elliptic PDEs | doi=10.1002/cpa.3160420103 | year=1989 | journal=[[Communications on Pure and Applied Mathematics]] | issn=0010-3640 | volume=42 | issue=1 | pages=15–45}}</ref> There is a generalized notion of boundary condition, ''in the viscosity sense''. The solution to a boundary problem with generalized boundary conditions is solvable whenever the comparison principle holds. <ref name=\"CIL\"/>\n* The '''stability''' of solutions in <math>L^\\infty</math> holds as follows: a locally [[uniform convergence|uniform limit]] of a sequence of solutions (or subsolutions, or supersolutions) is a solution (or subsolution, or supersolution). More generally, the notion of viscosity sub- and supersolution is also conserved by half-relaxed limits. <ref name=\"CIL\"/>\n\n== History ==\n\nThe term ''viscosity solutions'' first appear in the work of [[Michael G. Crandall]] and [[Pierre-Louis Lions]] in 1983 regarding the Hamilton–Jacobi equation.<ref name=\"CL\"/> The name is justified by the fact that the existence of solutions was obtained by the [[vanishing viscosity]] method. The definition of solution had actually been given earlier by [[Lawrence C. Evans]] in 1980.<ref name=\"E\">{{Citation | last1=Evans | first1=Lawrence C. | title=On solving certain nonlinear partial differential equations by accretive operator methods | doi=10.1007/BF02762047 | year=1980 | journal=Israel Journal of Mathematics | issn=0021-2172 | volume=36 | issue=3 | pages=225–247}}</ref> Subsequently the definition and properties of viscosity solutions for the Hamilton–Jacobi equation were refined in a joint work by Crandall, Evans and Lions in 1984.<ref name=\"CEL\">{{Citation | last1=Crandall | first1=Michael G. | last2=Evans | first2=Lawrence C. | last3=Lions | first3=Pierre-Louis | title=Some properties of viscosity solutions of Hamilton–Jacobi equations | doi=10.2307/1999247 | year=1984 | journal=[[Transactions of the American Mathematical Society]] | issn=0002-9947 | volume=282 | issue=2 | pages=487–502| jstor=1999247 }}</ref>\n\nFor a few years the work on viscosity solutions concentrated on first order equations because it was not known whether second order elliptic equations would have a unique viscosity solution except in very particular cases. The breakthrough result came with the method introduced by [[Robert Jensen (mathematician)|Robert Jensen]] in 1988 to prove the comparison principle using a regularized approximation of the solution which has a second derivative almost everywhere (in modern versions of the proof this is achieved with sup-convolutions and [[Alexandrov theorem]]).<ref name=\"J\">{{Citation | last1=Jensen | first1=Robert | title=The maximum principle for viscosity solutions of fully nonlinear second order partial differential equations | doi=10.1007/BF00281780 | year=1988 | journal=Archive for Rational Mechanics and Analysis | issn=0003-9527 | volume=101 | issue=1 | pages=1–27| bibcode=1988ArRMA.101....1J }}</ref>\n\nIn subsequent years the concept of viscosity solution has become increasingly prevalent in analysis of degenerate elliptic PDE. Based on their stability properties, Barles and Souganidis obtained a very simple and general proof of convergence of finite difference schemes.<ref name=\"BS\">{{Citation | last1=Barles | first1=G. | last2=Souganidis | first2=P. E. | title=Convergence of approximation schemes for fully nonlinear second order equations | year=1991 | journal=Asymptotic Analysis | issn=0921-7134 | volume=4 | issue=3 | pages=271–283}}</ref> Further regularity properties of viscosity solutions were obtained, especially in the uniformly elliptic case with the work of [[Luis Caffarelli]].<ref name=\"CC\">{{Citation | last1=Caffarelli | first1=Luis A. | last2=Cabré | first2=Xavier | title=Fully nonlinear elliptic equations | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=American Mathematical Society Colloquium Publications | isbn=978-0-8218-0437-7 | year=1995 | volume=43}}</ref> Viscosity solutions have become a central concept in the study of elliptic PDE.\n\nIn the modern approach, the existence of solutions is obtained most often through the Perron method.<ref name=\"CIL\"/> The vanishing viscosity method is not practical for second order equations in general since the addition of artificial viscosity does not guarantee the existence of a classical solution. Moreover, the definition of ''viscosity solutions'' does not involve any viscosity of any kind. The theory of viscosity solutions is completely unrelated to [[Viscous liquid|viscous fluids]]. Thus, it has been suggested that the name ''viscosity solution'' does not represent the concept appropriately. Yet, the name persists because of the history of the subject. Other names that were suggested were ''Crandall–Lions solutions'', in honor to their pioneers, ''<math>L^\\infty</math>-weak solutions'', referring to their stability properties, or ''comparison solutions'', referring to their most characteristic property.\n\n== References ==\n{{reflist|30em}}\n\n\n== External links ==\n* [https://www.scilag.net/problem/P-180731.2   N. Nadirashvili,  \tFully nonlinear elliptic equations]\n\n\n[[Category:Partial differential equations]]\n[[Category:Dynamic programming]]\n[[Category:Mathematical finance]]"
    },
    {
      "title": "Viterbi algorithm",
      "url": "https://en.wikipedia.org/wiki/Viterbi_algorithm",
      "text": "{{short description|Algorithm for finding the most likely sequence of hidden states}}\nThe '''Viterbi algorithm''' is a [[dynamic programming]] [[algorithm]] for finding the most [[likelihood function|likely]] sequence of hidden states&mdash;called the '''Viterbi path'''&mdash;that results in a sequence of observed events, especially in the context of [[Markov information source]]s and [[hidden Markov model]]s.\n\nThe algorithm has found universal application in decoding the [[convolutional code]]s used in both [[CDMA]] and [[GSM]] digital cellular, [[dial-up]] modems, satellite, deep-space communications, and [[802.11]] wireless LANs. It is now also commonly used in [[speech recognition]], [[speech synthesis]], [[diarization]],<ref>Xavier Anguera et al., [http://www1.icsi.berkeley.edu/~vinyals/Files/taslp2011a.pdf \"Speaker Diarization: A Review of Recent Research\"], retrieved 19. August 2010, IEEE TASLP</ref> [[keyword spotting]], [[computational linguistics]], and [[bioinformatics]]. For example, in [[speech-to-text]] (speech recognition), the acoustic signal is treated as the observed sequence of events, and a string of text is considered to be the \"hidden cause\" of the acoustic signal. The Viterbi algorithm finds the most likely string of text given the acoustic signal.\n\n==History==\nThe Viterbi algorithm is named after [[Andrew Viterbi]], who proposed it in 1967 as a decoding algorithm for [[Convolution code|convolutional codes]] over noisy digital communication links.<ref>[https://arxiv.org/abs/cs/0504020v2 29 Apr 2005, G. David Forney Jr: The Viterbi Algorithm: A Personal History]</ref> It has, however, a history of [[multiple invention]], with at least seven independent discoveries, including those by Viterbi, [[Needleman–Wunsch algorithm|Needleman and Wunsch]], and [[Wagner–Fischer algorithm|Wagner and Fischer]].<ref name=\"slp\">{{cite book |author1=Daniel Jurafsky |author2=James H. Martin |title=Speech and Language Processing |publisher=Pearson Education International |page=246}}</ref><!-- Jurafsky and Martin specifically refer to the papers that presented the Needleman–Wunsch and Wagner–Fischer algorithms, hence the wikilinks to those-->\n\n\"Viterbi path\" and \"Viterbi algorithm\" have become standard terms for the application of dynamic programming algorithms to maximization problems involving probabilities.<ref name=\"slp\"/>\nFor example, in [[statistical parsing]] a dynamic programming algorithm can be used to discover the single most likely context-free derivation (parse) of a string, which is commonly called the \"Viterbi parse\".<ref>{{Cite conference | doi = 10.3115/1220355.1220379| title = Efficient parsing of highly ambiguous context-free grammars with bit vectors| conference = Proc. 20th Int'l Conf. on Computational Linguistics (COLING)| pages = <!--162-->| year = 2004| last1 = Schmid | first1 = Helmut| url = http://www.aclweb.org/anthology/C/C04/C04-1024.pdf}}</ref><ref>{{Cite conference| doi = 10.3115/1073445.1073461| title = A* parsing: fast exact Viterbi parse selection| conference = Proc. 2003 Conf. of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL)| pages = 40–47| year = 2003| last1 = Klein | first1 = Dan| last2 = Manning | first2 = Christopher D.| url = http://ilpubs.stanford.edu:8090/532/1/2002-16.pdf}}</ref><ref>{{Cite journal | doi = 10.1093/nar/gkl200| title = AUGUSTUS: Ab initio prediction of alternative transcripts| journal = Nucleic Acids Research| volume = 34| issue = Web Server issue| pages = W435–W439| year = 2006| last1 = Stanke | first1 = M.| last2 = Keller | first2 = O.| last3 = Gunduz | first3 = I.| last4 = Hayes | first4 = A.| last5 = Waack | first5 = S.| last6 = Morgenstern | first6 = B. | pmid=16845043 | pmc=1538822}}</ref> Another application is in [[Optical motion tracking|target tracking]], where the track is computed that assigns a maximum likelihood to a sequence of observations.<ref>{{cite proceedings |author=Quach, T.; Farooq, M. |chapter=Maximum Likelihood Track Formation with the Viterbi Algorithm |title=Proceedings of 33rd IEEE Conference on Decision and Control |date=1994 |volume=1 |pages=271–276|url=https://ieeexplore.ieee.org/abstract/document/410918/}}</ref>\n\n==Extensions==\nA generalization of the Viterbi algorithm, termed the ''max-sum algorithm'' (or ''max-product algorithm'') can be used to find the most likely assignment of all or some subset of [[latent variable]]s in a large number of [[graphical model]]s, e.g. [[Bayesian network]]s, [[Markov random field]]s and [[conditional random field]]s.  The latent variables need in general to be connected in a way somewhat similar to an HMM, with a limited number of connections between variables and some type of linear structure among the variables.  The general algorithm involves ''message passing'' and is substantially similar to the [[belief propagation]] algorithm (which is the generalization of the [[forward-backward algorithm]]).\n\nWith the algorithm called [[iterative Viterbi decoding]] one can find the subsequence of an observation that matches best (on average) to a given [[hidden Markov model]]. This algorithm is proposed by Qi Wang et al. to deal with [[turbo code]].<ref>{{cite journal |author1=Qi Wang |author2=Lei Wei |author3=Rodney A. Kennedy |year=2002 |title=Iterative Viterbi Decoding, Trellis Shaping, and Multilevel Structure for High-Rate Parity-Concatenated TCM |journal=IEEE Transactions on Communications |volume=50 |pages=48–55 |doi=10.1109/26.975743}} \n</ref> Iterative Viterbi decoding works by iteratively invoking a modified Viterbi algorithm, reestimating the score for a filler until convergence.\n\nAn alternative algorithm, the [[Lazy Viterbi algorithm]], has been proposed.<ref>{{cite conference|url=http://people.csail.mit.edu/jonfeld/pubs/lazyviterbi.pdf |title=A fast maximum-likelihood decoder for convolutional codes |date=December 2002 |conference= Vehicular Technology Conference |conferenceurl=http://www.ieeevtc.org/ |pages=371–375 |format=PDF |doi=10.1109/VETECF.2002.1040367}}</ref> For many applications of practical interest, under reasonable noise conditions, the lazy decoder (using Lazy Viterbi algorithm) is much faster than the original [[Viterbi decoder]] (using Viterbi algorithm). While the original Viterbi algorithm calculates every node in the trellis of possible outcomes, the Lazy Viterbi algorithm maintains a prioritized list of nodes to evaluate in order, and the number of calculations required is typically fewer (and never more) than the ordinary Viterbi algorithm for the same result. However, it is not so easy{{clarify|date=November 2017}} to parallelize in hardware.\n\n==Pseudocode==\nThis algorithm generates a path <math> X=(x_1,x_2,\\ldots,x_T) </math>, which is a sequence of states <math>x_n \\in S=\\{s_1,s_2,\\dots,s_K\\}</math> that generate the observations <math> Y=(y_1,y_2,\\ldots, y_T) </math> with <math>y_n \\in  O=\\{o_1,o_2,\\dots,o_N\\}</math> (<math>N</math> being the count of observations (observation space, see below)).\n\nTwo 2-dimensional tables of size <math>K \\times T</math> are constructed:\n\n* Each element <math>T_1[i,j]</math> of <math>T_1</math> stores the probability of the most likely path so far <math> \\hat{X}=(\\hat{x}_1,\\hat{x}_2,\\ldots,\\hat{x}_j) </math> with <math>\\hat{x}_j=s_i </math> that generates <math> Y=(y_1,y_2,\\ldots, y_j)</math>.\n* Each element <math>T_2[i,j] </math> of <math>T_2 </math> stores <math>\\hat{x}_{j-1} </math> of the most likely path so far <math> \\hat{X}=(\\hat{x}_1,\\hat{x}_2,\\ldots,\\hat{x}_{j-1},\\hat{x}_j = s_i)</math> for <math>\\forall j, 2\\leq j \\leq T  </math>\n\nThe table entries <math> T_1[i,j],T_2[i,j]</math> are filled by increasing order of <math>K\\cdot j+i </math>.\n\n:<math>T_1[i,j]=\\max_{k}{(T_1[k,j-1]\\cdot A_{ki}\\cdot B_{iy_j})} </math>, and\n:<math> T_2[i,j]=\\operatorname{argmax}_{k}{(T_1[k,j-1]\\cdot A_{ki}}) </math>,\n\nwith <math>A_{ki}</math> and <math>B_{iy_j}</math> as defined below. Note that <math>B_{iy_j}</math> does not need to appear in the latter expression, as it's non-negative and independent of <math>k</math> and thus does not affect the argmax.\n\n;INPUT:\n*            The [[observation space]] <math> O=\\{o_1,o_2,\\dots,o_N\\}</math>\n*            the [[state space]] <math> S=\\{s_1,s_2,\\dots,s_K\\} </math>\n*            an array of initial probabilities <math> \\Pi = (\\pi_1,\\pi_2,\\dots,\\pi_K)</math> such that <math> \\pi_i </math> stores the probability that <math> x_1 ==  s_i </math>\n*            a sequence of observations  <math> Y=(y_1,y_2,\\ldots, y_T) </math> such that <math> y_t==i </math> if the observation at time <math> t </math> is <math> o_i </math>\n*            [[Stochastic matrix|transition matrix]] <math> A </math> of size <math> K\\times K </math> such that <math> A_{ij} </math> stores the [[transition probability]] of transiting from state <math> s_i </math> to state <math> s_j </math>\n*            [[Hidden Markov model|emission matrix]] <math> B </math> of size <math> K\\times N </math> such that <math> B_{ij} </math> stores the probability of observing <math> o_j </math> from  state <math> s_i </math> \n\n;OUTPUT\n*The most likely hidden state sequence <math> X=(x_1,x_2,\\ldots,x_T) </math>\n  '''function''' ''VITERBI''<math>(O,S,\\Pi,Y,A,B):X</math>\n      '''for''' each state <math>i=1,2,\\ldots,K</math> '''do'''\n          <math>T_1[i,1]\\leftarrow\\pi_i\\cdot B_{iy_1}</math>\n          <math>T_2[i,1]\\leftarrow  0</math>\n      '''end for'''\n      '''for''' each observation <math>j = 2,3,\\ldots,T</math> '''do'''\n          '''for''' each state <math>i =1,2,\\ldots,K</math> '''do'''\n              {{nowrap|<math>T_1[i,j] \\gets \\max_{k}{(T_1[k,j-1]\\cdot A_{ki} \\cdot B_{iy_j})} </math>}}\n              {{nowrap|<math>T_2[i,j] \\gets \\arg\\max_{k}{(T_1[k,j-1]\\cdot A_{ki})} </math>}}\n          '''end for'''\n      '''end for'''\n      {{nowrap|<math>z_T \\gets \\arg\\max_{k}{(T_1[k,T])} </math>}}\n      <math>x_T\\leftarrow s_{z_T}</math>\n      '''for''' <math>j=T,T-1,\\ldots,2</math> '''do'''\n          <math>z_{j-1}\\leftarrow T_2[z_j,j]</math>\n          <math>x_{j-1}\\leftarrow s_{z_{j-1}}</math>\n      '''end for'''\n      '''return''' <math>X</math>\n  '''end function'''\n\n;EXPLANATION:\nSuppose we are given a [[hidden Markov model]] (HMM) with state space <math>S</math>, initial probabilities <math>\\pi_i</math> of being in state <math>i</math> and transition probabilities <math>a_{i,j}</math> of transitioning from state <math>i</math> to state <math>j</math>.  Say we observe outputs <math>y_1,\\dots, y_T</math>.  The most likely state sequence <math>x_1,\\dots,x_T</math> that produces the observations is given by the recurrence relations:<ref>Xing E, slide 11</ref>\n\n:<math>\n\\begin{array}{rcl}\nV_{1,k} &=& \\mathrm{P}\\big( y_1 \\ | \\ k \\big) \\cdot \\pi_k \\\\\nV_{t,k} &=& \\max_{x \\in S} \\left(  \\mathrm{P}\\big( y_t \\ | \\ k \\big) \\cdot a_{x,k} \\cdot V_{t-1,x}\\right)\n\\end{array}\n</math>\n\nHere <math>V_{t,k}</math> is the probability of the most probable state sequence <math>\\mathrm{P}\\big(x_1,\\dots,x_t,y_1,\\dots, y_t\\big)</math> responsible for the first <math>t</math> observations that have <math>k</math> as its final state.  The Viterbi path can be retrieved by saving back pointers that remember which state <math>x</math> was used in the second equation.  Let <math>\\mathrm{Ptr}(k,t)</math> be the function that returns the value of <math>x</math> used to compute <math>V_{t,k}</math> if <math>t > 1</math>, or <math>k</math> if <math>t=1</math>.  Then:\n\n:<math>\n\\begin{array}{rcl}\nx_T &=& \\arg\\max_{x \\in S} (V_{T,x}) \\\\\nx_{t-1} &=& \\mathrm{Ptr}(x_t,t)\n\\end{array}\n</math>\n\nHere we're using the standard definition of [[arg max]].<br>\nThe complexity of this implementation is <math>O(T\\times\\left|{S}\\right|^2)</math>. A better estimation exists if the maximum in the internal loop is instead found by iterating only over states that directly link to the current state (i.e. there is an edge from <math>k</math> to <math>j</math>). Then using [[amortized analysis]] one can show that the complexity is <math>O(T\\times(\\left|{S}\\right| + \\left|{E}\\right|))</math>, where <math>E</math> is the number of edges in the graph.\n\n\n==Example==\nConsider a village where all villagers are either healthy or have a fever and only the village doctor can determine whether each has a fever. The doctor diagnoses fever by asking patients how they feel. The villagers may only answer that they feel normal, dizzy, or cold.\n\nThe doctor believes that the health condition of his patients operate as a discrete [[Markov chain]]. There are two states, \"Healthy\" and \"Fever\", but the doctor cannot observe them directly; they are ''hidden'' from him. On each day, there is a certain chance that the patient will tell the doctor he/she is \"normal\", \"cold\", or \"dizzy\", depending on their health condition.\n\nThe ''observations'' (normal, cold, dizzy) along with a ''hidden'' state (healthy, fever) form a hidden Markov model (HMM), and can be represented as follows in the [[Python (programming language)|Python programming language]]:\n<source lang=\"python\">\nobs = ('normal', 'cold', 'dizzy')\nstates = ('Healthy', 'Fever')\nstart_p = {'Healthy': 0.6, 'Fever': 0.4}\ntrans_p = {\n   'Healthy' : {'Healthy': 0.7, 'Fever': 0.3},\n   'Fever' : {'Healthy': 0.4, 'Fever': 0.6}\n   }\nemit_p = {\n   'Healthy' : {'normal': 0.5, 'cold': 0.4, 'dizzy': 0.1},\n   'Fever' : {'normal': 0.1, 'cold': 0.3, 'dizzy': 0.6}\n   }\n</source>\nIn this piece of code, <code>start_probability</code> represents the doctor's belief about which state the HMM is in when the patient first visits (all he knows is that the patient tends to be healthy). The particular probability distribution used here is not the equilibrium one, which is (given the transition probabilities) approximately <code>{'Healthy': 0.57, 'Fever': 0.43}</code>. The <code>transition_probability</code> represents the change of the health condition in the underlying Markov chain. In this example, there is only a 30% chance that tomorrow the patient will have a fever if he is healthy today. The <code>emission_probability</code> represents how likely each possible observation, normal, cold, or dizzy is given their underlying condition, healthy or fever. If the patient is healthy, there is a 50% chance that he feels normal; if he has a fever, there is a 60% chance that he feels dizzy.\n\n[[File:An example of HMM.png|thumb|center|300px|Graphical representation of the given HMM]]\n\nThe patient visits three days in a row and the doctor discovers that on the first day he feels normal, on the second day he feels cold, on the third day he feels dizzy. The doctor has a question: what is the most likely sequence of health conditions of the patient that would explain these observations? This is answered by the Viterbi algorithm.\n\n<source lang=\"python\" line=\"1\">\ndef viterbi(obs, states, start_p, trans_p, emit_p):\n    V = [{}]\n    for st in states:\n        V[0][st] = {\"prob\": start_p[st] * emit_p[st][obs[0]], \"prev\": None}\n    # Run Viterbi when t > 0\n    for t in range(1, len(obs)):\n        V.append({})\n        for st in states:\n            max_tr_prob = V[t-1][states[0]][\"prob\"]*trans_p[states[0]][st]\n            prev_st_selected = states[0]\n            for prev_st in states[1:]:\n                tr_prob = V[t-1][prev_st][\"prob\"]*trans_p[prev_st][st]\n                if tr_prob > max_tr_prob:\n                    max_tr_prob = tr_prob\n                    prev_st_selected = prev_st\n                    \n            max_prob = max_tr_prob * emit_p[st][obs[t]]\n            V[t][st] = {\"prob\": max_prob, \"prev\": prev_st_selected}\n                    \n    for line in dptable(V):\n        print line\n    opt = []\n    # The highest probability\n    max_prob = max(value[\"prob\"] for value in V[-1].values())\n    previous = None\n    # Get most probable state and its backtrack\n    for st, data in V[-1].items():\n        if data[\"prob\"] == max_prob:\n            opt.append(st)\n            previous = st\n            break\n    # Follow the backtrack till the first observation\n    for t in range(len(V) - 2, -1, -1):\n        opt.insert(0, V[t + 1][previous][\"prev\"])\n        previous = V[t + 1][previous][\"prev\"]\n\n    print 'The steps of states are ' + ' '.join(opt) + ' with highest probability of %s' % max_prob\n\ndef dptable(V):\n    # Print a table of steps from dictionary\n    yield \" \".join((\"%12d\" % i) for i in range(len(V)))\n    for state in V[0]:\n        yield \"%.7s: \" % state + \" \".join(\"%.7s\" % (\"%f\" % v[state][\"prob\"]) for v in V)\n</source>\nThe function <code>viterbi</code> takes the following arguments: <code>obs</code> is the sequence of observations, e.g. <code>['normal', 'cold', 'dizzy']</code>; <code>states</code> is the set of hidden states; <code>start_p</code> is the start probability; <code>trans_p</code> are the transition probabilities; and <code>emit_p</code> are the emission probabilities.  For simplicity of code, we assume that the observation sequence <code>obs</code> is non-empty and that  <code>trans_p[i][j]</code> and <code>emit_p[i][j]</code> is defined for all states i,j.\n\nIn the running example, the forward/Viterbi algorithm is used as follows:\n\n<source lang=\"python\">\nviterbi(obs,\n        states,\n        start_p,\n        trans_p,\n        emit_p)\n\n</source>\n\nThe output of the script is\n\n<source lang=\"console\">\n$ python viterbi_example.py\n         0          1          2\nHealthy: 0.30000 0.08400 0.00588\nFever: 0.04000 0.02700 0.01512\nThe steps of states are Healthy Healthy Fever with highest probability of 0.01512\n</source>\n\nThis reveals that the observations <code>['normal', 'cold', 'dizzy']</code> were most likely generated by states <code>['Healthy', 'Healthy', 'Fever']</code>. In other words, given the observed activities, the patient was most likely to have been healthy both on the first day when he felt normal as well as on the second day when he felt cold, and then he contracted a fever the third day.\n\nThe operation of Viterbi's algorithm can be visualized by means of a\n[[Trellis diagram#Trellis diagram|trellis diagram]]. The Viterbi path is essentially the shortest\npath through this trellis. \n\n==See also==\n* [[Expectation–maximization algorithm]]\n* [[Baum–Welch algorithm]]\n* [[Forward-backward algorithm]]\n* [[Forward algorithm]]\n* [[Error-correcting code]]\n* [[Soft output Viterbi algorithm]]\n* [[Viterbi decoder]]\n* [[Hidden Markov model]]\n* [[Part-of-speech tagging]]\n\n==References==\n<references />\n\n==General references==\n* {{cite journal |doi=10.1109/TIT.1967.1054010 |author=Viterbi AJ |title=Error bounds for convolutional codes and an asymptotically optimum decoding algorithm |journal=IEEE Transactions on Information Theory |volume=13 |issue=2 |pages=260–269 |date=April 1967 }} (note: the Viterbi decoding algorithm is described in section IV.) Subscription required.\n* {{cite book |vauthors=Feldman J, Abou-Faycal I, Frigo M |title=A Fast Maximum-Likelihood Decoder for Convolutional Codes |journal=Vehicular Technology Conference |volume=1 |pages=371–375 |year=2002 |doi=10.1109/VETECF.2002.1040367|isbn=978-0-7803-7467-6 |citeseerx=10.1.1.114.1314 }}\n* {{cite journal |doi=10.1109/PROC.1973.9030 |author=Forney GD |title=The Viterbi algorithm |journal=Proceedings of the IEEE |volume=61 |issue=3 |pages=268–278 |date=March 1973 }} Subscription required.\n* {{Cite book | last1=Press | first1=WH | last2=Teukolsky | first2=SA | last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press |  location=New York | isbn=978-0-521-88068-8 | chapter=Section 16.2. Viterbi Decoding | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=850}}\n* {{cite journal |author=Rabiner LR |title=A tutorial on hidden Markov models and selected applications in speech recognition |journal=Proceedings of the IEEE |volume=77 |issue=2 |pages=257–286 |date=February 1989 |doi=10.1109/5.18626|citeseerx=10.1.1.381.3454 }} (Describes the forward algorithm and Viterbi algorithm for HMMs).\n* Shinghal, R. and [[Godfried Toussaint|Godfried T. Toussaint]], \"Experiments in text recognition with the modified Viterbi algorithm,\" ''IEEE Transactions on Pattern Analysis and Machine Intelligence'', Vol. PAMI-l, April 1979, pp.&nbsp;184–193.\n* Shinghal, R. and [[Godfried Toussaint|Godfried T. Toussaint]], \"The sensitivity of the modified Viterbi algorithm to the source statistics,\" ''IEEE Transactions on Pattern Analysis and Machine Intelligence'', vol. PAMI-2, March 1980, pp.&nbsp;181–185.\n\n== Implementations ==\n* [http://susalib.org/ Susa] signal processing framework provides the C++ implementation for [[Forward error correction]] codes and channel equalization [https://github.com/behrooza/susa/blob/master/inc/susa/channel.h here].\n* [https://github.com/xukmin/viterbi C++]\n* [http://pcarvalho.com/forward_viterbi/ C#]\n* [http://www.cs.stonybrook.edu/~pfodor/viterbi/Viterbi.java Java]\n* [https://adrianulbona.github.io/hmm/ Java 8]\n* [https://metacpan.org/module/Algorithm::Viterbi Perl]\n* [http://www.cs.stonybrook.edu/~pfodor/viterbi/viterbi.P Prolog]\n* [https://hackage.haskell.org/package/hmm-0.2.1.1/docs/src/Data-HMM.html#viterbi Haskell]\n* [https://github.com/nyxtom/viterbi Go]\n* [http://tuvalu.santafe.edu/~simon/styled-8/ SFIHMM] includes code for Viterbi decoding.\n\n==External links==\n* [[Wikibooks:Algorithm Implementation/Viterbi algorithm|Implementations in Java, F#, Clojure, C# on Wikibooks]]\n* [http://pl91.ddns.net/viterbi/tutorial.html Tutorial] on convolutional coding with viterbi decoding, by Chip Fleming\n* [http://www.kanungo.com/software/hmmtut.pdf A tutorial for a Hidden Markov Model toolkit (implemented in C) that contains a description of the Viterbi algorithm]\n* [http://www.scholarpedia.org/article/Viterbi_algorithm Viterbi algorithm] by Dr. [[Andrew Viterbi|Andrew J. Viterbi]] (scholarpedia.org).\n\n[[Category:Error detection and correction]]\n[[Category:Dynamic programming]]\n[[Category:Markov models]]\n[[Category:Articles with example Python code]]"
    },
    {
      "title": "WASP (cricket calculation tool)",
      "url": "https://en.wikipedia.org/wiki/WASP_%28cricket_calculation_tool%29",
      "text": "{{orphan|date=January 2014}}\n'''Winning and Score Predictor (WASP)''' is a calculation tool used in [[cricket]] to predict scores and possible results of a [[List A cricket|limited overs]] match, e.g. One Day and [[Twenty20|Twenty 20]] matches.\n \nThe prediction is based upon factors like the ease of scoring on the day according to the pitch, weather and boundary size. For the team batting first, it gives the prediction of the final total.  For the team batting second, it gives the probability of the chasing team winning, although it does not just take the match situation into the equation. Predictions are based on the average team playing against the average team in those conditions.<ref>{{cite web|title=What's WASP all about?|url=http://www.blackcaps.co.nz/news/whats-wasp-all-about/11200/article.aspx|work=Corporate News|publisher=New Zealand Cricket|date=23 January 2014|accessdate=21 May 2014}}</ref>\n\nThe models are based on a database of all non-shortened [[One Day International]] (ODI) matches and [[Twenty20]] games played between top-eight countries since late 2006 (slightly further back for Twenty20 games). The batting-first model estimates the additional runs likely to be scored as a function of the number of balls and wickets remaining. The batting-second model estimates the probability of winning as a function of balls and wickets remaining, runs scored to date, and the target score. Projected score or required run-rate will not qualitatively show the real picture as they fail to take into the account the quality of the batting team and the quality of the bowling attack. WASP<ref>{{cite web|url=http://upcric.com/cric-updates/what-is-wasp-percentage-cricket.html |title=What is wasp |publisher=ESPN Upcricinfo |date= |accessdate=2014-09-03}}</ref> is a very good quantitative parameter.<ref name=\"blogspot1\">{{cite web|last=Hogan |first=Seamus |url=http://offsettingbehaviour.blogspot.in/2012/11/cricket-and-wasp-shameless-self.html |title=Offsetting Behaviour: Cricket and the Wasp: Shameless self promotion (Wonkish) |publisher=Offsettingbehaviour.blogspot.in |date=2012-11-22 |accessdate=2014-02-03}}</ref>\n\n==History==\nThe WASP technique is a product of some extensive research from PhD graduate Dr Scott Brooker and his supervisor Dr Seamus Hogan at the [[University of Canterbury]] (UC) in [[Christchurch]], New Zealand.  They worked on this project for four years and started after they received a phone call from the university's economics department asking them to investigate alternatives to the [[Duckworth–Lewis method]].<ref>{{cite news|author=Charley Mann |url=http://www.stuff.co.nz/the-press/news/6820689/Cricket-formula-fairer |title=Cricket formula 'fairer' |newspaper= The Press |publisher=Fairfax New Zealand Limited |date=2012-04-28 |accessdate=2014-02-03}}</ref>\n\nWASP was first introduced by [[Sky Sport (New Zealand)|Sky Sport New Zealand]] in November 2012 during [[Auckland cricket team|Auckland]]'s [[HRV Cup]] Twenty20 game against [[Wellington cricket team|Wellington]].<ref>{{cite news|url=http://www.cricketcountry.com/articles/wasp-winning-and-score-predictor-makes-for-an-interesting-watch-on-television-87588/ |first=Abhijit |last=Banare |title=WASP: Winning and Score Predictor makes for an interesting watch on television |newspaper=CricketCountry |date=January 20, 2014 |accessdate=2014-02-03}}</ref>\n\n==Theory==\nThe WASP system is grounded in the theory of [[dynamic programming]]. It looks at data from past matches and estimates the probability of runs and wickets in each game situation, and works backwards to calculate the total runs or probability of winning in any situation.\n\nThis is how Dr Seamus Hogan – one of the creators of WASP – described the system:\n\nLet ''V''(''b'',''w'') be the expected additional runs for the rest of the innings when ''b'' (legitimate) balls have been bowled and ''w'' wickets have been lost, and let ''r''(''b'',''w'') and ''p''(''b'',''w'') be, respectively, the estimated expected runs and the probability of a wicket on the next ball in that situation.\n\nWe can then write,\n\n: <math> V(b,w) = r(b,w) + p(b,w) V(b+1,w+1) + (1-p(b,w))V(b+1,w)</math>\n\nSince ''V''(''b*'',''w'')=0 where ''b''<sup>*</sup> equals the maximum number of legitimate deliveries allowed in the innings (300 in a 50 over game), we can solve the model backwards.\n\nThis means that the estimates for ''V''(''b'',''w'') in rare situations depends only slightly on the estimated runs and probability of a wicket on that ball, and mostly on the values of ''V''(''b''&nbsp;+&nbsp;1,''w'') and ''V''(''b''&nbsp;+&nbsp;1,''w''&nbsp;+&nbsp;1), which will be mostly determined by thick data points.\n\nThe batting second model is a bit more complicated, but uses essentially the same logic.<ref name=\"blogspot1\"/>\n\n==Drawbacks==\nIf a batsman gets retired hurt, the model will not work well as it does not know the position in which the retired hurt batsman will come to bat again. This happened in a match between England and New Zealand,<ref>{{cite web|url=http://www.espncricinfo.com/ci/engine/match/569240.html |title=1st ODI: New Zealand v England at Hamilton, Feb 17, 2013 &#124; Cricket Scorecard |publisher=ESPN Cricinfo |date= |accessdate=2014-02-03}}</ref> as injured [[Martin Guptill]] came to the crease again in the ninth position and gave a staggering performance which is unusual for a ninth position batsman and helped his team to win the match.<ref>{{cite web|last=Hogan |first=Seamus |url=http://offsettingbehaviour.blogspot.in/2013/11/more-cricket-return-of-wasp.html |title=Offsetting Behaviour: More Cricket: The Return of the Wasp |publisher=Offsettingbehaviour.blogspot.in |date=2013-11-01 |accessdate=2014-02-03}}</ref><ref>{{cite web|author=Mike Selvey |url=https://www.theguardian.com/sport/2013/feb/17/new-zealand-england-odi |title=England beaten after injured Martin Guptill digs in for New Zealand &#124; Sport |publisher=The Guardian |date= |accessdate=2014-02-03}}</ref>\n\nWASP also fails when a team beats the predicted score batting first, as was the case when Black Caps beat the predicted score at Otago University Oval of 270 in the fourth game of the seven-game series against Sri Lanka by a large margin, when [[Grant Elliott]] and [[Luke Ronchi]] set the record for their highest 6th wicket partnership to set a score of 360/5, at which case, runs per over rate will resume showing.  The team batting second  will not have WASP showing if the likelihood of winning falls below 5% before the 5th over. These reasons make this unpopular outside of Sky broadcasts.  Two days later, due to a greener surface, the predicted score at the same ground drops to 257 at the start of the match, which Black Caps still went on to post a score of 315/8.  After those matches, Sky abandoned showing this statistic in future matches, and it was not featured during 2015 Cricket World Cup.  WASP, however, was featured again in [[NatWest t20 Blast|NatWest T20 Blast]], ODI and Twenty20 broadcasts in England.\n\n==References==\n{{Reflist}}\n\n[[Category:Prediction]]\n[[Category:Dynamic programming]]\n[[Category:Cricket terminology]]"
    },
    {
      "title": "Artificial development",
      "url": "https://en.wikipedia.org/wiki/Artificial_development",
      "text": "{{Evolutionary algorithms}}\n'''Artificial development''',  also known as '''artificial embryogeny or machine intelligence''' or '''computational development''', is an area of [[computer science]] and [[engineering]] concerned with computational models motivated by genotype-phenotype mappings in biological systems. Artificial development is often considered a sub-field of [[evolutionary computation]], although the principles of artificial development have also been used within stand-alone computational models.\n\nWithin evolutionary computation, the need for artificial development techniques was motivated by the perceived lack of scalability and evolvability of direct solution encodings (Tufte, 2008). Artificial development entails indirect solution encoding. Rather than describing a solution directly, an indirect encoding describes (either explicitly or implicitly) the process by which a solution is constructed. Often, but not always, these indirect encodings are based upon biological principles of development such as [[morphogen|morphogen gradients]], [[cell division]] and [[cellular differentiation]] (e.g. Doursat 2008), [[gene regulatory networks]] (e.g. Guo ''et al.'', 2009), [[degeneracy (biology)| degeneracy]] (Whitacre ''et al.'', 2010), [[grammatical evolution]] (de Salabert ''et al.'', 2006), or analogous computational processes such as re-writing, iteration, and time. The influences of interaction with the environment, spatiality and physical constraints on differentiated multi-cellular development have been investigated more recently (e.g. Knabe et al. 2008).\n\nArtificial development approaches have been applied to a number of computational and design problems, including electronic circuit design (Miller and Banzhaf 2003), robotic controllers (e.g. Taylor 2004), and the design of physical structures (e.g. Hornby 2004).\n\n==Notes==\n* Rene Doursat, \"[https://pdfs.semanticscholar.org/7e95/e1b5964bae64046d7c1ecdef8c2f43a1b469.pdf Organically grown architectures: Creating decentralized, autonomous systems by embryomorphic engineering]\", Organic Computing, R. P. Würtz, (ed.), Springer-Verlag, Ch. 8, pp. 167-200, 2008.\n* Guo, H., Y. Meng and Y. Jin (2009). \"A cellular mechanism for multi-robot construction via evolutionary multi-objective optimization of a gene regulatory network.\" BioSystems 98(3): 193-203. (https://web.archive.org/web/20110719123923/http://www.ece.stevens-tech.edu/~ymeng/publications/BioSystems09_Meng.pdf)\n* Whitacre, J. M., P. Rohlfshagen, X. Yao and A. Bender (2010). The role of degenerate robustness in the evolvability of multi-agent systems in dynamic environments. PPSN XI, Kraków, Poland. (http://philipprohlfshagen.net/resources/WhitacreRole.pdf{{Dead link|date=May 2019 |bot=InternetArchiveBot |fix-attempted=yes }})\n* Gregory S. Hornby, \"Functional Scalability through Generative Representations: the Evolution of Table Designs\", Environment and Planning B: Planning and Design, 31(4), 569-587, July 2004. ([http://www.envplan.com/abstract.cgi?id=b3015 abstract])\n* Julian F. Miller and Wolfgang Banzhaf (2003): \"Evolving the Program for a Cell: From French Flags to Boolean Circuits\", On Growth, Form and Computers, S. Kumar and P. Bentley, (eds.), Elsevier Academic Press, 2003. {{ISBN|978-0-12-428765-5}}\n* Arturo de Salabert, Alfonso Ortega and Manuel Alfonseca, (2006) “Optimizing Ecology-friendly Drawing of Plans of Buildings by means of Grammatical Evolution,” Proc. ISC’2006, Eurosis, pp. 493-497. {{ISBN|90-77381-26-0}}\n* Kenneth Stanley and Risto Miikkulainen (2003): \"A Taxonomy for artificial embryogeny\", ''[[Artificial Life (journal)|Artificial Life]]'' 9(2):93-130, 2003.\n* Tim Taylor (2004): [http://www.tim-taylor.com/papers/taylor_grn.pdf \"A Genetic Regulatory Network-Inspired Real-Time Controller for a Group of Underwater Robots\"],  ''Intelligent Autonomous Systems 8'' (Proceedings of IAS8), F. Groen, N. Amato, A. Bonarini, E. Yoshida and B. Kröse (eds.), IOS Press, Amsterdam, 2004. {{ISBN|978-1-58603-414-6}}\n* Gunnar Tufte (2008): \"[https://pdfs.semanticscholar.org/1846/d622d1ea438f1e8c0acab27a90f415e018a1.pdf Phenotypic, Developmental and Computational Resources: Scaling in Artificial Development]\", Proc. Genetic and Evolutionary Computation Conf. (GECCO) 2008, ACM, 2008.\n* Knabe, J. F., Nehaniv, C. L. and Schilstra, M. J. [https://web.archive.org/web/20110629022041/http://mitpress.mit.edu/books/chapters/0262287196chap42.pdf \"Evolution and Morphogenesis of Differentiated Multicellular Organisms: Autonomously Generated Diffusion Gradients for Positional Information\"]. In ''Artificial Life XI: Proceedings of the Eleventh International Conference on the Simulation and Synthesis of Living Systems'', pages 321-328, MIT Press, 2008. [http://panmental.de/ALifeXIflag corr. web page]\n\n[[Category:Evolutionary algorithms]]"
    },
    {
      "title": "Cellular evolutionary algorithm",
      "url": "https://en.wikipedia.org/wiki/Cellular_evolutionary_algorithm",
      "text": "A '''cellular evolutionary algorithm''' ('''cEA''') is a kind of [[evolutionary algorithm]] (EA) in which individuals cannot mate arbitrarily, but every one interacts with its closer neighbors on which a basic EA is applied (selection, variation, replacement).\n\n[[File:evolution of several cEAs.png|x300px|thumb|'''Example evolution of a cEA depending on the shape of the population, from squared (left) to unidimensional ring (right). Darker colors mean better solutions. Observe how shapes different from the traditional square keep diversity (higher exploration) for a longer time. Four snapshots of cEAs at generations 0-50-100-150.''']]\n\nThe cellular model simulates natural evolution from the point of view of\nthe individual, which encodes a tentative (optimization, learning, search) problem solution. The essential idea of this model is to provide the EA population\nwith a special structure defined as a connected graph, in which each vertex is an individual who communicates with his\nnearest neighbors. Particularly, individuals are conceptually set in a toroidal\nmesh, and are only allowed to recombine with close individuals. This leads us\nto a kind of locality known as ''isolation by distance''. The set of potential mates\nof an individual is called its ''neighborhood''. It is known that, in this kind\nof algorithm, similar individuals tend to cluster creating niches, and these groups\noperate as if they were separate sub-populations (islands). Anyway, there is no\nclear borderline between adjacent groups, and close niches could be easily\ncolonized by competitive niches and maybe merge solution contents during the process. Simultaneously,\nfarther niches can be affected more slowly.\n\n==Introduction==\n\nA cellular evolutionary algorithm (cEA) usually evolves a structured bidimensional\ngrid of individuals, although other topologies are also possible. In this grid, clusters of similar individuals are naturally created during evolution, promoting exploration in their boundaries, while exploitation is mainly performed by direct competition and merging inside them.\n\n[[File:cEA neighborhood types.png|x200px|thumb|'''Example models of neighborhoods in cellular EAs: linear, compact, diamond and... any other!''']]\n\nThe grid is usually 2D toroidal structure, although\nthe number of dimensions can be easily extended (to 3D) or reduced (to 1D, e.g. a ring).\nThe neighborhood of a particular point of the grid (where an individual is\nplaced) is defined in terms of the ''Manhattan'' distance from it to others in the population. Each point of the grid has a neighborhood that overlaps the neighborhoods of nearby individuals. In the basic algorithm, all the neighborhoods have the same size and identical shapes. The two\nmost commonly used neighborhoods are L5, also called\n''Von Neumann'' or NEWS (North, East, West and South), and C9, also known as ''Moore'' neighborhood. Here, ''L'' stands for ''Linear'' while ''C'' stands for ''Compact''.\n\nIn cEAs, the individuals can only interact with their neighbors in the reproductive\ncycle where the variation operators are applied. This reproductive\ncycle is executed inside the neighborhood of each individual and, generally,\nconsists in selecting two parents among its neighbors according to a certain\ncriterion, applying the variation operators to them (recombination and mutation\nfor example), and replacing the considered individual by the recently\ncreated offspring following a given criterion, for instance, replace if the offspring\nrepresents a better solution than the considered individual.\n\n==Synchronous versus asynchronous==\n\nIn a regular '''synchronous''' cEA, the algorithm proceeds from the very first top left individual to the right and then to the several rows by using the information in the population to create a new temporary population. After finishing with the bottom-right last individual the temporary population is full with the newly computed individuals, and the replacement step starts. In it, the old population is completely and synchronously replaced with the newly computed one according to some criterion. Usually, the replacement keeps the best individual in the same position of both populations, that is, elitism is used.\n\nWe must notice that according to the update policy of the population used, we could also define an '''asynchronous''' cEA. This is also a well-known issue in [[cellular automata]]. In asynchronous cEAs the order in which the individuals in the grid are update changes depending on the criterion used: line sweep, fixed random sweep, new random sweep, and uniform choice. These are the four most usual ways of updating the population. All of them keep using the newly computed individual (or the original if better) for the computations of its neighbors immediately. This makes the population to hold at any time individual in different states of evolution, defining a very interesting new line of research.\n\n[[File:ratio concept in cEAs.png|x300px|thumb|'''The ratio between the radii of the neighborhood to the topology defines the exploration/exploitation capability of the cEA. This could be even tuned during the run of the algorithm, giving the researcher a unique mechanism to search in very complex landscapes.''']]\n\nThe overlap of the neighborhoods provides an implicit mechanism of solution migration\nto the cEA. Since the best solutions spread smoothly through the\nwhole population, genetic diversity in the population is preserved longer than\nin non structured EAs. This soft dispersion of the best solutions through the\npopulation is one of the main issues of the good tradeoff between ''exploration''\nand ''exploitation'' that cEAs perform during the search. It is then easy to see\nthat we could ''tune this tradeoff'' (and hence, tune the genetic diversity level along\nthe evolution) by modifying (for instance) the size of the neighborhood used, as\nthe overlap degree between the neighborhoods grows according to the size of\nthe neighborhood.\n\nA cEA can be seen as a [[cellular automaton]] (CA) with probabilistic\nrewritable rules, where the alphabet of the CA is equivalent to the potential\nnumber of solutions of the problem. Hence, if we see cEAs as a kind of CA,\nit is possible to import knowledge from the field of CAs to cEAs, and in fact this is an interesting open research line.\n\n==Parallelism==\n\nCellular EAs are very amenable to parallelism, thus usually found in the literature of [[parallel metaheuristics]]. In particular, fine grain parallelism can be use to assign independent threads of execution to every individual, thus allowing the whole cEA to run on a concurrent or actually parallel hardware platform. In this way, large time reductions can be obtained when running cEAs on [[FPGAs]] or [[GPUs]].\n\nHowever, it is important to stress that cEAs are a model of search, in many senses different from traditional EAs. Also, they can be run in sequential and parallel platforms, reinforcing the fact that the model and the implementation are two different concepts.\n\nSee [https://www.springer.com/business/operations+research/book/978-0-387-77609-5 here] for a complete description on the fundamentals for the understanding, design, and application of cEAs.\n\n==See also==\n* [[Cellular automaton]]\n* [[Dual-phase evolution]]\n* [[Enrique Alba]]\n* [[Evolutionary algorithm]]\n* [[Metaheuristic]]\n* [[Parallel metaheuristic]]\n\n== References ==\n<!--- See [[Wikipedia:Footnotes]] on how to create references using <ref></ref> tags which will then appear here automatically -->\n{{Reflist}}\n* [https://www.springer.com/business/operations+research/book/978-0-387-77609-5 E. Alba, B. Dorronsoro, ''Cellular Genetic Algorithms''], Springer-Verlag, {{ISBN|978-0-387-77609-5}}, 2008\n* A.J. Neighbor, J.J. Durillo, F. Luna, B. Dorronsoro, E. Alba, MOCell: A New Cellular Genetic Algorithm for Multiobjective Optimization, International Journal of Intelligent Systems, 24:726-746, 2009\n* E. Alba, B. Dorronsoro, F. Luna, A.J. Neighbor, P. Bouvry, L. Hogie, A Cellular Multi-Objective Genetic Algorithm for Optimal Broadcasting Strategy in Metropolitan MANETs, Computer Communications, 30(4):685-697, 2007\n* E. Alba, B. Dorronsoro, Computing Nine New Best-So-Far Solutions for Capacitated VRP with a Cellular GA, Information Processing Letters, Elsevier, 98(6):225-230, 30 June 2006\n* M. Giacobini, M. Tomassini, A. Tettamanzi, E. Alba, The Selection Intensity in Cellular Evolutionary Algorithms for Regular Lattices, IEEE Transactions on Evolutionary Computation, IEEE Press, 9(5):489-505, 2005\n* E. Alba, B. Dorronsoro, The Exploration/Exploitation Tradeoff in Dynamic Cellular Genetic Algorithms, IEEE Transactions on Evolutionary Computation, IEEE Press, 9(2)126-142, 2005\n\n== External links ==\n* [http://neo.lcc.uma.es/cEA-web/ The site on Cellular Evolutionary Algorithms]\n* [http://neo.lcc.uma.es NEO Research Group at University of Málaga, Spain]\n\n{{Evolutionary computation}}\n\n<!--- Categories --->\n[[Category:Evolutionary algorithms]]"
    },
    {
      "title": "Computer-automated design",
      "url": "https://en.wikipedia.org/wiki/Computer-automated_design",
      "text": "{{Format footnotes|date=March 2018}}\n\nDesign Automation usually refers to [[electronic design automation]], or [[Design Automation]] which is a [[Product Configurator]].  Extending [[Computer-Aided Design]] (CAD), automated design and '''Computer-Automated Design (CAutoD)'''<ref name=IBM>[http://domino.research.ibm.com/tchjr/journalindex.nsf/0/a5cb0910ea78194885256bfa00683e5a?OpenDocument Kamentsky, L.A., and Liu, C.-N. (1963). Computer-Automated Design of Multifont Print Recognition Logic, IBM Journal of Research and Development, 7(1), p.2]</ref><ref>[https://www.ncbi.nlm.nih.gov/pubmed/10989487 Brncick, M. (2000). Computer automated design and computer automated manufacture, Phys Med Rehabil Clin N Am, Aug, 11(3), 701-13.]</ref><ref>[https://link.springer.com/content/pdf/10.1007%2Fs11633-004-0076-8.pdf Li, Y., et al. (2004). CAutoCSD - Evolutionary search and optimisation enabled computer automated control system design] {{webarchive|url=https://web.archive.org/web/20150831032257/http://www.ijac.net/EN/article/downloadArticleFile.do?attachType=PDF&id=840 |date=2015-08-31 }}. International Journal of Automation and Computing, 1(1). 76-88. ISSN 1751-8520</ref> are more concerned with a broader range of applications, such as [[automotive engineering]], [[civil engineering]],<ref>KRAMER, GJE; GRIERSON, DE, (1989) COMPUTER AUTOMATED DESIGN OF STRUCTURES UNDER DYNAMIC LOADS,  COMPUTERS & STRUCTURES, 32(2), 313-325</ref><ref>MOHARRAMI, H; GRIERSON, DE, 1993, COMPUTER-AUTOMATED DESIGN OF REINFORCED-CONCRETE FRAMEWORKS, JOURNAL OF STRUCTURAL ENGINEERING-ASCE, 119(7), 2036-2058</ref><ref>XU, L; GRIERSON, DE, (1993) COMPUTER-AUTOMATED DESIGN OF SEMIRIGID STEEL FRAMEWORKS, JOURNAL OF STRUCTURAL ENGINEERING-ASCE, 119(6), 1740-1760</ref><ref>Barsan, GM; Dinsoreanu, M, (1997). Computer-automated design based on structural performance criteria, Mouchel Centenary Conference on Innovation in Civil and Structural Engineering, AUG 19-21, CAMBRIDGE ENGLAND, INNOVATION IN CIVIL AND STRUCTURAL ENGINEERING, 167-172</ref> [[composite material]] design, [[control engineering]],<ref>[https://www.tandfonline.com/doi/pdf/10.1080/00207179608921865 Li, Y., et al. (1996). Genetic algorithm automated approach to the design of sliding mode control systems, Int J Control, 63(4), 721-739.]</ref> dynamic [[system identification]] and optimization,<ref>[https://www.sciencedirect.com/science/article/pii/S1474667017451585/pdf?md5=b7aedf998282848dfcf44a1ea2f003dd&pid=1-s2.0-S1474667017451585-main.pdf Li, Y., et al. (1995). Automation of Linear and Nonlinear Control Systems Design by Evolutionary Computation, Proc. IFAC Youth Automation Conf., Beijing, China, August 1995, 53-58.]</ref> [[financial]] systems, industrial equipment, [[mechatronic]] systems, [[steel construction]],<ref>Barsan, GM, (1995) Computer-automated design of semirigid steel frameworks according to EUROCODE-3, Nordic Steel Construction Conference 95, JUN 19-21, 787-794</ref> structural [[optimization (mathematics)|optimisation]],<ref>[https://www.sciencedirect.com/science/article/pii/S0967066198000872/pdf?md5=5ad89d3029a3ebad83086271f3c78f75&pid=1-s2.0-S0967066198000872-main.pdf Gary J. Gray, David J. Murray-Smith, Yun Li, et al. (1998). Nonlinear model structure identification using genetic programming, Control Engineering Practice 6 (1998) 1341—1352]</ref> and the invention of novel systems.\n\nThe concept of CAutoD perhaps first appeared in 1963, in the IBM Journal of Research and Development,<ref name=IBM/> where a computer program was written.\n# to search for logic circuits having certain constraints on hardware design\n# to evaluate these logics in terms of their discriminating ability over samples of the character set they are expected to recognize.\n\nMore recently, traditional CAD simulation is seen to be transformed to CAutoD by biologically-inspired [[machine learning]],<ref>[https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6052374 Zhan, Z.H., et al. (2011). Evolutionary computation meets machine learning: a survey, IEEE Computational Intelligence Magazine, 6(4), 68-75.]</ref> including heuristic [[search algorithm|search techniques]] such as [[evolutionary computation]],<ref>[http://ti.arc.nasa.gov/m/pub-archive/768h/0768%20(Hornby).pdf Gregory S. Hornby (2003). Generative Representations for Computer-Automated Design Systems, NASA Ames Research Center, Mail Stop 269-3, Moffett Field, CA 94035-1000]</ref><ref>[https://www.msu.edu/~jclune/webfiles/publications/2011-CluneLipson-Evolving3DObjectsWithCPPNs-ECAL.pdf J. Clune and H. Lipson (2011). Evolving three-dimensional objects with a generative encoding inspired by developmental biology. Proceedings of the European Conference on Artificial Life. 2011.]</ref> and [[swarm intelligence]] algorithms.<ref>[https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4812104 Zhan, Z.H., et al. (2009). Adaptive Particle Swarm Optimization,  IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), Vol.39, No.6. 1362-1381]</ref></sup>\n\n==Guiding designs by performance improvements==\n[[File:Computer-automated design.png|thumb|right|350px|Interaction in computer-automated design]]\nTo meet the ever-growing demand of quality and competitiveness, iterative physical prototyping is now often replaced by '[[digital prototyping]]' of a 'good design', which aims to meet multiple objectives such as maximised output, energy efficiency, highest speed and cost-effectiveness. The design problem concerns both finding the best design within a known range (i.e., through 'learning' or 'optimisation') and finding a new and better design beyond the existing ones (i.e., through creation and invention). This is equivalent to a [[search problem]] in an almost certainly, multidimensional (multivariate), multi-modal space with a single (or weighted) objective or multiple objectives.\n\n==Normalized objective function: cost vs. fitness==\nUsing single-objective CAutoD as an example, if the objective function, either as a [[Loss function|cost function]] <math>J\\in[0, \\infty)</math>, or inversely, as a [[fitness function]] <math>f\\in(0,1]</math>, where\n\n:<math>f = \\tfrac{J}{1+J}</math>,\n\nis differentiable under practical constraints in the multidimensional space, the design problem may be solved analytically. Finding the parameter sets that result in a zero first-order derivative and that satisfy the second-order derivative conditions would reveal all local optima. Then comparing the values of the performance index of all the local optima, together with those of all boundary parameter sets, would lead to the global optimum, whose corresponding 'parameter' set will thus represent the best design.  However, in practice, the optimization usually involves multiple objectives and the matters involving derivatives are lot more complex.\n\n==Dealing with practical objectives==\nIn practice, the objective value may be noisy or even non-numerical, and hence its gradient information may be unreliable or unavailable. This is particularly true when the problem is multi-objective.  At present, many designs and refinements are mainly made through a manual trial-and-error process with the help of a CAD [[simulation]] package. Usually, such ''[[a posteriori]]'' learning or adjustments need to be repeated many times until a ‘satisfactory’ or ‘optimal’ design emerges.\n\n==Exhaustive search==\nIn theory, this adjustment process can be automated by computerised search, such as [[exhaustive search]].  As this is an [[exponential algorithm]], it may not deliver solutions in practice within a limited period of time.\n\n==Search in polynomial time==\nOne approach to [[virtual engineering]] and automated design is [[evolutionary computation]] such as [[evolutionary algorithm]]s.\n\n===Evolutionary algorithms===\nTo reduce the search time, the biologically-inspired evolutionary algorithm (EA) can be used instead, which is a (non-deterministic) [[Exponential algorithm#Polynomial time|polynomial algorithm]].  The EA based multi-objective \"search team\" can be interfaced with an existing CAD simulation package in a batch mode. The EA encodes the design parameters (encoding being necessary if some parameters are non-numerical) to refine multiple candidates through parallel and interactive search. In the search process, '[[natural selection|selection]]' is performed using '[[survival of the fittest]]' ''a posteriori'' learning. To obtain the next 'generation' of possible solutions, some parameter values are exchanged between two candidates (by an operation called '[[Crossover (genetic algorithm)|crossover]]') and new values introduced (by an operation called '[[mutation]]'). This way, the evolutionary technique makes use of past trial information in a similarly intelligent manner to the human designer.\n\nThe EA based optimal designs can start from the designer's existing design database or from an initial generation of candidate designs obtained randomly. A number of finally evolved top-performing candidates will represent several automatically optimized digital prototypes.\n\nThere are websites that demonstrate interactive evolutionary algorithms for design. [http://EndlessForms.com EndlessForms.com] allows you to evolve 3D objects online and have them 3D printed.  [http://www.picbreeder.org PicBreeder.org] allows you to do the same for 2D images.\n\n==See also==\n{{Portal|Design}}\n* [[Electronic design automation]]\n* [[Design Automation]]\n* [[Design Automation Conference]]\n* [[Generative design]]\n* [[Genetic algorithm#Applications|Genetic algorithm (GA) applications - automated design]]\n\n==References==\n{{Reflist|2}}\n\n==External links==\n* [http://www.i4ai.org/EA-demo/ An online interactive GA based CAutoD demonstrator.] Learn step by step or watch global convergence in 2-parameter CAutoD\n* [https://www.google.com/search?tbm=isch&q=evolutionary+design Images of practical examples.]\n\n{{DEFAULTSORT:Computer-Automated Design}}\n[[Category:Design]]\n[[Category:Computer-aided design]]\n[[Category:Evolutionary algorithms]]\n[[Category:Evolutionary computation]]"
    },
    {
      "title": "Constructive cooperative coevolution",
      "url": "https://en.wikipedia.org/wiki/Constructive_cooperative_coevolution",
      "text": "The '''constructive cooperative coevolutionary algorithm''' (also called C<sup>3</sup>) is a [[global optimisation]] algorithm in [[artificial intelligence]] based on the multi-start architecture of the [[greedy randomized adaptive search procedure]] (GRASP).<ref>T.A. Feo and M.G.C. Resende (1989) [http://www.sciencedirect.com/science/article/pii/0167637789900023 \"A probabilistic heuristic for a computationally difficult set covering problem\"]. ''Operations Research Letters'', 8:67&ndash;71, 1989.</ref><ref>T.A. Feo and M.G.C. Resende (1995) [https://link.springer.com/article/10.1007/BF01096763 \"Greedy randomized adaptive search procedures\"]. ''Journal of Global Optimization'', 6:109&ndash;133, 1995.</ref> It incorporates the existing [[Cooperative coevolution|cooperative coevolutionary algorithm]] (CC).<ref>M. A. Potter and K. A. D. Jong, [https://link.springer.com/chapter/10.1007/3-540-58484-6_269 \"A cooperative coevolutionary approach to function optimization\"], in ''PPSN III: Proceedings of the International Conference on Evolutionary Computation. The Third Conference on Parallel Problem Solving from Nature'' London, UK:Springer-Verlag, 1994, pp. 249–257.</ref> The considered problem is decomposed into subproblems. These subproblems are optimised separately while exchanging information in order to solve the complete problem. An optimisation algorithm, usually but not necessarily an [[evolutionary algorithm]], is embedded in C<sup>3</sup> for optimising those subproblems. The nature of the embedded optimisation algorithm determines whether C<sup>3</sup>'s behaviour is [[deterministic]] or [[stochastic]].\n\nThe C<sup>3</sup> optimisation algorithm was originally designed for [[Stochastic optimization|simulation-based optimisation]]<ref name=\"Glorieux et al 2014a\" /><ref name=\"Glorieux et al 2014b\" /> but it can be used for [[global optimization|global optimisation]] problems in general.<ref name=\"Glorieux et al 2017b\">Glorieux E., Svensson B., Danielsson F., Lennartson B.: [https://doi.org/10.1007/s10732-017-9351-z \"Constructive cooperative coevolution for large-scale global optimisation\"], ''Journal of Heuristics'', 2017.</ref> Its strength over other optimisation algorithms, specifically cooperative coevolution, is that it is better able to handle non-separable optimisation problems.<ref name=\"Glorieux et al 2014a\" /><ref>Glorieux E., Danielsson F., Svensson B., Lennartson B.: [https://link.springer.com/article/10.1007/s00170-015-7012-7 \"Constructive cooperative coevolutionary optimisation for interacting production stations\"], ''International Journal of Advanced Manufacturing Technology'', 2015.</ref>\n\nAn improved version was proposed later, called the Improved Constructive Cooperative Coevolutionary Differential Evolution (C<sup>3i</sup>DE), which removes several limitations with the previous version. A novel element of C<sup>3i</sup>DE is the advanced initialisation of the subpopulations. C<sup>3i</sup>DE initially optimises the subpopulations in a partially co-adaptive fashion. During the initial optimisation of a subpopulation, only a subset of the other subcomponents is considered for the co-adaptation. This subset increases stepwise until all subcomponents are considered. This makes C<sup>3i</sup>DE very effective on large-scale global optimisation problems (up to 1000 dimensions) compared to [[Cooperative coevolution|cooperative coevolutionary algorithm]] (CC) and [[Differential evolution]].<ref name=\"Glorieux et al 2015b\">Glorieux E., Svensson B., Danielsson F., Lennartson B., [http://ieeexplore.ieee.org/document/7376815/ \"Improved Constructive Cooperative Coevolutionary Differential Evolution for Large-Scale Optimisation\"], 2015 IEEE Symposium Series on Computational Intelligence, December 2015</ref>\n\nThe improved algorithm has then been adapted for [[multi-objective optimization]].<ref name=\"Glorieux et al 2017a\">Glorieux E., Svensson B., Danielsson F., Lennartson B., [http://www.tandfonline.com/10.1080/0305215X.2016.1264220 \"Multi-objective constructive cooperative coevolutionary optimization of robotic press-line tending\"], Engineering Optimization, Vol. 49, Iss. 10, 2017, pp 1685-1703</ref>\n\n==Algorithm==\n\nAs shown in the pseudo code below, an iteration of C<sup>3</sup> exists of two phases. In Phase I, the constructive phase, a feasible solution for the entire problem is constructed in a stepwise manner. Considering a different subproblem in each step. After the final step, all subproblems are considered and a solution for the complete problem has been constructed. This constructed solution is then used as the initial solution in Phase II, the local improvement phase. The CC algorithm is employed to further optimise the constructed solution. A cycle of Phase II includes optimising the subproblems separately while keeping the parameters of the other subproblems fixed to a central blackboard solution. When this is done for each subproblem, the found solution are combined during a \"collaboration\" step, and the best one among the produced combinations becomes the blackboard solution for the next cycle. In the next cycle, the same is repeated. Phase II, and thereby the current iteration, are terminated when the search of the CC algorithm stagnates and no significantly better solutions are being found. Then, the next iteration is started. At the start of the next iteration, a new feasible solution is constructed, utilising solutions that were found during the Phase I of the previous iteraton(s). This constructed solution is then used as the initial solution in Phase II in the same way as in the first iteration. This is repeated until one of the termination criteria for the optimisation is reached, e.g. a maximum number of evaluations.\n\n {'''S'''<sub>phase1</sub>} ← ∅\n '''WHILE''' termination criteria not satisfied  '''DO'''\n    '''IF''' {'''S'''<sub>phase1</sub>} == ∅ '''THEN'''\n       {'''S'''<sub>phase1</sub>} ← SubOpt(∅, 1)\n    '''END IF'''\n    '''WHILE''' '''p'''<sub>phase1</sub> not completely constructed '''DO'''\n       '''p'''<sub>phase1</sub> ← GetBest({'''S'''<sub>phase1</sub>})\n       {'''S'''<sub>phase1</sub>} ← SubOpt('''p'''<sub>phase1</sub>, i<sub>next subproblem</sub>)\n    '''END WHILE'''\n    '''p'''<sub>phase2</sub> ← GetBest({'''S'''<sub>phase1</sub>})\n    '''WHILE''' not stagnate  '''DO'''\n         {'''S'''<sub>phase2</sub>} ← ∅ \n         '''FOR''' each subproblem i '''DO'''\n           {'''S'''<sub>phase2</sub>} ← SubOpt('''p'''<sub>phase2</sub>,i)\n         '''END FOR'''\n         {'''S'''<sub>phase2</sub>} ← Collab({'''S'''<sub>phase2</sub>})\n         '''p'''<sub>phase2</sub> ← GetBest({'''S'''<sub>phase2</sub>})\n      '''END WHILE'''\n '''END WHILE'''\n\n==Multi-Objective Optimisation==\nThe multi-objective version of the C<sup>3</sup> algorithm <ref name=\"Glorieux et al 2017a\" /> is a Pareto-based algorithm which uses the same divide-and-conquer strategy as the single-objective C<sup>3</sup> optimisation algorithm . The algorithm again starts with the advanced constructive initial optimisations of the subpopulations, considering an increasing subset of subproblems. The subset increases until the entire set of all subproblems is included. During these initial optimisations, the subpopulation of the latest included subproblem is evolved by a multi-objective evolutionary algorithm. For the fitness calculations of the members of the subpopulation, they are combined with a collaborator solution from each of the previously optimised subpopulations.  Once all subproblems' subpopulations have been initially optimised, the multi-objective C<sup>3</sup> optimisation algorithm continues to optimise each subproblem in a [[Round-robin scheduling|round-robin fashion]], but now collaborator solutions from all other subproblems' subspopulations are combined with the member of the subpopulation that is being evaluated. The collaborator solution is selected randomly from the solutions that make up the Pareto-optimal front of the subpopulation. The fitness assignment to the collaborator solutions is done in an optimistic fashion (i.e. an \"old\" fitness value is replaced when the new one is better).\n\n==Applications==\n\nThe constructive cooperative coevolution algorithm has been applied to different types of problems, e.g. a set of standard benchmark functions,<ref name=\"Glorieux et al 2014a\">Glorieux E., Danielsson F., Svensson B., Lennartson B., [http://ieeexplore.ieee.org/document/6899345/ \"Optimisation of interacting production stations using a Constructive Cooperative Coevolutionary approach\"], 2014 IEEE International Conference on Automation Science and Engineering (CASE), pp.322-327, August 2014, Taipei, Taiwan</ref><ref name=\"Glorieux et al 2017b\"/> optimisation of sheet metal press lines<ref name=\"Glorieux et al 2014a\" /><ref name=\"Glorieux et al 2014b\">Glorieux E., Svensson B., Danielsson F., Lennartson B., [http://hv.diva-portal.org/smash/record.jsf?pid=diva2%3A754165&dswid=-2044 \"A Constructive Cooperative Coevolutionary Algorithm Applied to Press Line Optimisation\"], Proceedings of the 24th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM), pp.909-917, May 2014, San Antonio, Texas, USA</ref> and interacting production stations.<ref name=\"Glorieux et al 2014b\" /> The C<sup>3</sup> algorithm has been embedded with, amongst others, the [[Differential evolution|differential evolution algorithm]]<ref>Storn, Rainer, and Kenneth Price. [https://link.springer.com/article/10.1023/A:1008202821328 \"Differential evolution–a simple and efficient heuristic for global optimization over continuous spaces\"], Journal of global optimization 11.4 (1997): 341-359.</ref> and the [[Particle swarm optimization|particle swarm optimiser]]<ref>Eberhart, Russ C., and James Kennedy. [http://ieeexplore.ieee.org/document/494215/ \"A new optimizer using particle swarm theory\"], Proceedings of the sixth international symposium on micro machine and human science. Vol. 1. 1995.</ref> for the subproblem optimisations.\n\n== See also ==\n<!-- Please don't add a whole list of optimization algorithms, the categories serve that purpose. -->\n<!-- 27/3/11 Updated with more meaningful category organisation - I have tried to keep the list as brief as possible, relying on sub-pages for further breakdown of categories. Please try to maintain this spirit on edits. -->\n\n* [[Cooperative coevolution]]\n* [[Metaheuristic]]\n* [[Stochastic search]]\n* [[Differential evolution]]\n* [[Swarm intelligence]]\n* [[Genetic algorithms]]\n* [[Hyper-heuristics]]\n\n==References==\n{{reflist}}\n\n*\n\n[[Category:Evolutionary algorithms]]\n[[Category:Evolutionary computation]]"
    },
    {
      "title": "Cultural algorithm",
      "url": "https://en.wikipedia.org/wiki/Cultural_algorithm",
      "text": "{{Evolutionary algorithms}}\n'''Cultural algorithms (CA)''' are a branch of [[evolutionary computation]] where there is a knowledge component that is called the belief space in addition to the [[population]] component. In this sense, cultural algorithms can be seen as an extension to a conventional [[genetic algorithm]]. Cultural algorithms were introduced by Reynolds (see references).<!-- Deleted image removed:  [[Image:Cultural Algorithm.svg|400px|right]] -->\n\n== Belief space ==\nThe belief space of a cultural algorithm is divided into distinct categories. These categories represent different domains of knowledge that the population has of the [[search algorithm|search space]].\n\nThe belief space is updated after each [[iteration]] by the best individuals of the population. The best individuals can be selected using a [[fitness function]] that assesses the performance of each individual in population much like in genetic algorithms.\n\n=== List of belief space categories ===\n\n* '''[[Norm (sociology)|Normative]] knowledge''' A collection of desirable value ranges for the individuals in the population component e.g. acceptable behavior for the agents in population.\n* '''Domain specific knowledge''' Information about the domain of the cultural algorithm problem is applied to.\n* '''Situational knowledge''' Specific examples of important events - e.g. successful/unsuccessful solutions\n* '''Temporal knowledge''' History of the search space - e.g. the temporal patterns of the search process\n* '''Spatial knowledge''' Information about the topography of the search space\n\n== Population==\nThe population component of the cultural algorithm is approximately the same as that of the [[genetic algorithm]].\n\n== Communication protocol ==\nCultural algorithms require an [[wikt:interface|interface]] between the population and belief space. The best individuals of the population can update the belief space via the update function. Also, the knowledge categories of the belief space can affect the population component via the influence function. The influence function can affect population by altering the genome or the actions of the individuals.\n\n==Pseudo-code for cultural algorithms==\n# Initialize '''population space''' (choose initial [[population]])\n# Initialize '''belief space''' (e.g. set domain specific knowledge and normative value-ranges)\n# Repeat until termination condition is met\n## Perform actions of the individuals in '''population space'''\n## Evaluate each individual by using the '''fitness function'''\n## Select the parents to reproduce a new generation of offspring\n## Let the belief space alter the genome of the offspring by using the '''influence function'''\n## Update the belief space by using the '''accept function''' (this is done by letting the best individuals to affect the belief space)\n\n==Applications==\n*Various [[Process optimization|optimization]] problems\n*[[Social simulation]]\n*Real-parameter optimization<ref>M. Omran, A novel cultural algorithm for real-parameter optimization. International Journal of Computer Mathematics, DOI:10.1080/00207160.2015.1067309, 2015.</ref>\n\n== See also ==\n*[[Artificial intelligence]]\n*[[Artificial life]]\n*[[Evolutionary computation]]\n*[[Genetic algorithm]]\n*[[Harmony search]]\n*[[Machine learning]]\n*[[Memetic algorithm]]\n*[[Memetics]]\n*[[Metaheuristic]]\n*[[Social simulation]]\n*[[Sociocultural evolution]]\n*[[Stochastic optimization]]\n*[[Swarm intelligence]]\n\n== References ==\n{{Reflist}}\n* [http://www.cscs.umich.edu/swarmfest04/Program/PapersSlides/Kobti-SwarmFest04_kobti_reynolds_kohler.pdf Robert G. Reynolds, Ziad Kobti, Tim Kohler: Agent-Based Modeling of Cultural Change in Swarm Using Cultural Algorithms]\n* R. G. Reynolds, “An Introduction to Cultural Algorithms, ” in Proceedings of the 3rd Annual Conference on Evolutionary Programming, World Scienfific Publishing, pp 131–139, 1994.\n* Robert G. Reynolds, Bin Peng. Knowledge Learning and Social Swarms in Cultural Systems. Journal of Mathematical Sociology. 29:1-18, 2005\n* Reynolds, R. G., and Ali, M. Z, “Embedding a Social Fabric Component into Cultural Algorithms Toolkit for an Enhanced Knowledge-Driven Engineering Optimization”, International Journal of Intelligent Computing and Cybernetics (IJICC), Vol. 1, No 4, pp.&nbsp;356–378, 2008\n* Reynolds, R G., and Ali, M Z., Exploring Knowledge and Population Swarms via an Agent-Based Cultural Algorithms Simulation Toolkit (CAT), in proceedings of IEEE Congress on Computational Intelligence 2007.\n\n{{DEFAULTSORT:Cultural Algorithm}}\n[[Category:Evolutionary algorithms]]\n[[Category:Genetic algorithms]]\n[[Category:Nature-inspired metaheuristics]]"
    },
    {
      "title": "DarwinTunes",
      "url": "https://en.wikipedia.org/wiki/DarwinTunes",
      "text": "'''DarwinTunes''' is a research project into the use of [[natural selection]] to create [[music]] led by Bob MacCallum and [[Armand Leroi]], scientists at [[Imperial College London]]. The project asks volunteers on the Internet to listen to automatically generated sound loops and rate them based on aesthetic preference. After the volunteers rate the loops on a five-point scale, software permits the highest rated loops to 'reproduce sexually' and populate the next generation of musical loops.<ref>{{Cite web|url=http://blogs.discovermagazine.com/notrocketscience/2012/06/18/tunes-without-composers-music-naturally-evolves-on-darwintunes/|title=Tunes without composers: music naturally evolves on DarwinTunes|work=Not Exactly Rocket Science|publisher=Discover Magazine|first=Ed|last=Yong|date=18 June 2012|accessdate=19 June 2012}}</ref><ref>{{Cite news|url=https://www.bbc.co.uk/news/science-environment-18449939|title=Music evolution: Is this the end of the composer?|first=Rebecca|last=Morelle|publisher=BBC News Online|date=19 June 2012|accessdate=19 June 2012}}</ref><ref>{{Cite news|url=http://www.smh.com.au/technology/technology-news/with-darwintunes-who-needs-composers-20120619-20lch.html|title=With DarwinTunes, who needs composers?|first=Deborah|last=Smith|date=19 June 2012|accessdate=19 June 2012|publisher=Sydney Morning Herald}}</ref><ref>{{Cite news|url=http://www.latimes.com/news/science/la-sci-music-evolution-20120619,0,3879428.story|title=DarwinTunes software 'evolves' music without composers|first=Rosie|last=Mestel|date=19 June 2012|publisher=Los Angeles Times|accessdate=19 June 2012}}</ref>\n\nIn a paper published in the [[Proceedings of the National Academies of Science]], the DarwinTunes developers describe how their first experimental population derived from two randomly generated founding loops, allowed 100 generations of loops to evolve without any selection pressure before asking members of the public to rate the loops. The paper found that for the first 500 to 600 generations, aesthetic quality of the loops dramatically improved before reaching a stable equilibrium. They tested this using ratings by listeners and also by using sampling techniques used by [[music information retrieval]] technology—namely the Chordino and Rhythm Patterns algorithms, which measure the presence of chords used in Western music and the presence of rhythm respectively.<ref>{{Cite journal | last1 = MacCallum | first1 = R. M. | last2 = Mauch | first2 = M. | last3 = Burt | first3 = A. | last4 = Leroi | first4 = A. M. | title = Evolution of music by public choice | doi = 10.1073/pnas.1203182109 | journal = Proceedings of the National Academy of Sciences | year = 2012 | pmid =  22711832| pmc = 3409751| volume=109 | pages=12081–12086}}</ref>\n\n== See also ==\n*[[Evolutionary music]]\n\n== References ==\n{{reflist|2}}\n\n== External links ==\n*[http://darwintunes.org/ Official web page]\n\n[[Category:Evolutionary algorithms]]\n[[Category:Artificial life models]]\n[[Category:Computer music software]]\n\n\n{{music-software-stub}}\n{{music-theory-stub}}"
    },
    {
      "title": "Differential evolution",
      "url": "https://en.wikipedia.org/wiki/Differential_evolution",
      "text": "{{Evolutionary algorithms}}\n[[File:Ackley.gif|thumb|Differential Evolution optimizing the 2D Ackley function.]]\n\nIn [[evolutionary computation]], '''differential evolution''' ('''DE''') is a method that [[optimization (mathematics)|optimizes]] a problem by [[iterative method|iteratively]] trying to improve a [[candidate solution]] with regard to a given measure of quality. Such methods are commonly known as [[metaheuristic]]s as they make few or no assumptions about the problem being optimized and can search very large spaces of candidate solutions. However, metaheuristics such as DE do not guarantee an optimal solution is ever found.\n\nDE is used for multidimensional real-valued [[function (mathematics)|functions]] but does not use the [[gradient]] of the problem being optimized, which means DE does not require the optimization problem to be [[Differentiable function|differentiable]], as is required by classic optimization methods such as [[gradient descent]] and [[quasi-newton methods]]. DE can therefore also be used on optimization problems that are not even [[:wikt:continuous|continuous]], are noisy, change over time, etc.<ref name=elediadereview/>\n\nDE optimizes a problem by maintaining a population of candidate solutions and creating new candidate solutions by combining existing ones according to its simple formulae, and then keeping whichever candidate solution has the best score or fitness on the optimization problem at hand. In this way the optimization problem is treated as a black box that merely provides a measure of quality given a candidate solution and the gradient is therefore not needed.\n\nDE is originally due to Storn and Price.<ref name=storn97differential/><ref name=storn96usage/> Books have been published on theoretical and practical aspects of using DE in [[parallel computing]], [[multiobjective optimization]], [[constrained optimization]], and the books also contain surveys of application areas.<ref name=price05differential/><ref name=feoktistov06differential/><ref>G. C. Onwubolu and  B V Babu, {{cite web|url=https://www.springer.com/in/book/9783540201670|title=New Optimization Techniques in Engineering|publisher=|accessdate=17 September 2016}}</ref><ref name=chakraborty08advances/> Surveys on the multi-faceted research aspects of DE can be found in journal articles .<ref>S. Das and P. N. Suganthan, \"[https://www.researchgate.net/profile/Swagatam_Das/publication/220380793_Differential_Evolution_A_Survey_of_the_State-of-the-Art/links/00b7d52204106cb196000000/Differential-Evolution-A-Survey-of-the-State-of-the-Art.pdf Differential Evolution: A Survey of the State-of-the-art]\", IEEE Trans. on Evolutionary Computation, Vol. 15, No. 1, pp. 4-31, Feb. 2011, DOI: 10.1109/TEVC.2010.2059031.</ref><ref>S. Das, S. S. Mullick, P. N. Suganthan, \"[http://web.mysites.ntu.edu.sg/epnsugan/PublicSite/Shared%20Documents/PDFs/DE-Survey-2016.pdf Recent Advances in Differential Evolution - An Updated Survey],\" Swarm and Evolutionary Computation, doi:10.1016/j.swevo.2016.01.004, 2016.</ref>\n\n== Algorithm {{Anchor|algo}} ==\n<!-- There is no need for extra pseudo-code when this algorithm description is made as detailed as it is! -->\n\nA basic variant of the DE algorithm works by having a population of [[candidate solutions]] (called agents). These agents are moved around in the search-space by using simple mathematical [[formula]]e to combine the positions of existing agents from the population. If the new position of an agent is an improvement then it is accepted and forms part of the population, otherwise the new position is simply discarded. The process is repeated and by doing so it is hoped, but not guaranteed, that a satisfactory solution will eventually be discovered.\n\nFormally, let <math>f: \\mathbb{R}^n \\to \\mathbb{R}</math> be the fitness function which must be minimized (note that maximization can be performed by considering the function <math>h := -f</math> instead). The function takes a candidate solution as argument in the form of a [[Row vector|vector]] of [[real number]]s and produces a real number as output which indicates the fitness of the given candidate solution. The [[gradient]] of <math>f</math> is not known. The goal is to find a solution <math>\\mathbf{m}</math> for which <math>f(\\mathbf{m}) \\leq f(\\mathbf{p})</math> for all <math>\\mathbf{p}</math> in the search-space, which means that <math>\\mathbf{m}</math> is the global minimum.\n\nLet <math>\\mathbf{x} \\in \\mathbb{R}^n</math> designate a candidate solution (agent) in the population. The basic DE algorithm can then be described as follows:\n\n* Initialize all agents <math>\\mathbf{x}</math> with random positions in the search-space.\n* Until a termination criterion is met (e.g. number of iterations performed, or adequate fitness reached), repeat the following:\n** For each agent <math>\\mathbf{x}</math> in the population do:\n*** Pick three agents <math>\\mathbf{a},\\mathbf{b}</math>, and <math>\\mathbf{c}</math> from the population at random, they must be distinct from each other as well as from agent <math>\\mathbf{x}</math>\n*** Pick a random index <math>R \\in \\{1, \\ldots, n\\}</math> where <math>n</math> is the dimensionality of the problem being optimized.\n*** Compute the agent's potentially new position <math>\\mathbf{y} = [y_1, \\ldots, y_n]</math> as follows:\n**** For each <math>i \\in \\{1,\\ldots,n\\}</math>, pick a uniformly distributed random number <math>r_i \\sim U(0,1)</math>\n**** If <math>r_i < CR </math> or <math>i = R</math> {{Dubious|i can't be equal to R since it is not integer|date=June 2018}} then set <math>y_i = a_i + F \\times (b_i-c_i)</math> otherwise set <math>y_i = x_i</math>. The parameter <math>\\text{CR} \\in [0,1]</math> is called the ''crossover probability'' and the parameter <math>F \\in [0,2]</math> is called the ''differential weight'', both these parameters must be set by the user along with the population size <math>\\text{NP} \\geq 4</math> and may greatly impact the optimization performance, see below.\n*** If <math>f(\\mathbf{y}) < f(\\mathbf{x})</math> then replace the agent <math>\\mathbf{x}</math> in the population with the improved candidate solution <math>\\mathbf{y}</math>.\n* Pick the agent from the population that has the best fitness and return it as the best found candidate solution.\n\n== Parameter selection ==\n\n[[Image:DE Meta-Fitness Landscape (Sphere and Rosenbrock).JPG|thumb|Performance landscape showing how the basic DE performs in aggregate on the Sphere and Rosenbrock benchmark problems when varying the two DE parameters <math>\\text{NP}</math> and <math>\\text{F}</math>, and keeping fixed <math>\\text{CR}</math>=0.9.]]\n\nThe choice of DE parameters <math>F, \\text{CR}</math> and <math>\\text{NP}</math> can have a large impact on optimization performance. Selecting the DE parameters that yield good performance has therefore been the subject of much research. [[Rules of thumb]] for parameter selection were devised by Storn et al.<ref name=storn96usage/><ref name=price05differential/> and Liu and Lampinen.<ref name=liu02setting/> Mathematical convergence analysis regarding parameter selection was done by Zaharie.<ref name=zaharie02critical/> [[Meta-optimization]] of the DE parameters was done by Pedersen<ref name=pedersen08thesis/><ref name=pedersen10good-de/> and Zhang et al.<ref name=zhang11fitting/>. Adaptive population based DE was done by Wong et al.<ref name=wong17capr/>.\n\n== Variants ==\n\n<!-- Please add proper reference to conference-paper / journal-paper / tech report / master's or phd thesis. Please only add representative works. There must be hundreds of DE variants and Wikipedia is not the proper place to list them all. -->\n\nVariants of the DE algorithm are continually being developed in an effort to improve optimization performance. Many different schemes for performing crossover and mutation of agents are possible in the basic algorithm given above, see e.g.<ref name=storn96usage/> More advanced DE variants are also being developed with a popular research trend being to perturb or adapt the DE parameters during optimization, see e.g. Price et al.,<ref name=price05differential/> Liu and Lampinen,<ref name=liu05fuzzy/> Qin and Suganthan,<ref name=qin05selfadaptive/> Civicioglu<ref name=civici/> and Brest et al.<ref name=brest06selfadapting/> There are also some work in making a hybrid optimization method using DE combined with other optimizers.<ref name=zx03depso/>\n\n== See also ==\n<!-- Please only add optimizers conceptually related to DE. -->\n* [[Artificial bee colony algorithm]]\n* [[CMA-ES]]\n* [[Differential search algorithm]]<ref name=civici/>\n* [[Evolution strategy]]\n* [[Genetic algorithm]]\n\n==References==\n{{Reflist|refs=\n<ref name=storn97differential>\n{{cite journal\n|last=Storn\n|first=R.\n|author2=Price, K.\n|title=Differential evolution - a simple and efficient heuristic for global optimization over continuous spaces\n|journal=Journal of Global Optimization\n|year=1997\n|volume=11\n|issue=4\n|pages=341&ndash;359\n|doi=10.1023/A:1008202821328\n}}\n</ref>\n\n<ref name=storn96usage>\n{{cite conference\n|last=Storn\n|first=R.\n|title=On the usage of differential evolution for function optimization\n|booktitle=Biennial Conference of the North American Fuzzy Information Processing Society (NAFIPS)\n|year=1996\n|pages=519&ndash;523\n}}\n</ref>\n\n<ref name=price05differential>\n{{cite book\n|title=Differential Evolution: A Practical Approach to Global Optimization\n|url=https://www.springer.com/computer/theoretical+computer+science/foundations+of+computations/book/978-3-540-20950-8\n|last1=Price\n|first1=K.\n|last2=Storn\n|first2=R.M.\n|last3=Lampinen\n|first3=J.A.\n|year=2005\n|publisher=Springer\n|isbn=978-3-540-20950-8\n}}\n</ref>\n\n<ref name=feoktistov06differential>\n{{cite book\n|title=Differential Evolution: In Search of Solutions\n|url=https://www.springer.com/mathematics/book/978-0-387-36895-5\n|last=Feoktistov\n|first=V.\n|year=2006\n|publisher=Springer\n|isbn=978-0-387-36895-5\n}}\n</ref>\n\n<ref name=chakraborty08advances>\n{{citation\n|title=Advances in Differential Evolution\n|url=https://www.springer.com/engineering/book/978-3-540-68827-3\n|editor-last=Chakraborty\n|editor-first=U.K.\n|year=2008\n|publisher=Springer\n|isbn=978-3-540-68827-3\n}}\n</ref>\n\n<ref name=liu02setting>\n{{cite conference\n|title=On setting the control parameter of the differential evolution method\n|booktitle=Proceedings of the 8th International Conference on Soft Computing (MENDEL)\n|last=Liu\n|first=J.\n|last2=Lampinen\n|first2=J.\n|year=2002\n|pages=11&ndash;18\n|location=Brno, Czech Republic\n}}\n</ref>\n\n<ref name=zaharie02critical>\n{{cite conference\n|title=Critical values for the control parameters of differential evolution algorithms\n|booktitle=Proceedings of the 8th International Conference on Soft Computing (MENDEL)\n|last=Zaharie\n|first=D.\n|year=2002\n|pages=62&ndash;67\n|location=Brno, Czech Republic\n}}\n</ref>\n\n<ref name=brest06selfadapting>\n{{cite journal\n|last1=Brest\n|first1=J.\n|last2=Greiner\n|first2=S.\n|last3=Boskovic\n|first3=B.\n|last4=Mernik\n|first4=M.\n|last5=Zumer\n|first5=V.\n|title=Self-adapting control parameters in differential evolution: a comparative study on numerical benchmark functions\n|journal=IEEE Transactions on Evolutionary Computation\n|year=2006\n|volume=10\n|issue=6\n|pages=646&ndash;657\n|doi=10.1109/tevc.2006.872133\n}}\n</ref>\n\n<ref name=qin05selfadaptive>\n{{cite conference\n|last1=Qin\n|first1=A.K.\n|last2=Suganthan\n|first2=P.N.\n|title=Self-adaptive differential evolution algorithm for numerical optimization\n|booktitle=Proceedings of the IEEE congress on evolutionary computation (CEC)\n|year=2005\n|pages=1785&ndash;1791\n|url=https://sci2s.ugr.es/sites/default/files/files/TematicWebSites/EAMHCO/contributionsCEC05/quin05sad.pdf\n}}\n</ref>\n\n<ref name=liu05fuzzy>\n{{cite journal\n|last1=Liu\n|first1=J.\n|last2=Lampinen\n|first2=J.\n|title=A fuzzy adaptive differential evolution algorithm\n|journal=Soft Computing\n|year=2005\n|volume=9\n|issue=6\n|pages=448&ndash;462\n|doi=10.1007/s00500-004-0363-x\n}}\n</ref>\n\n<ref name=pedersen08thesis>\n{{cite book\n|type=PhD thesis\n|title=Tuning & Simplifying Heuristical Optimization\n|url=http://www.hvass-labs.org/people/magnus/thesis/pedersen08thesis.pdf\n|last=Pedersen\n|first=M.E.H.\n|year=2010\n|publisher=University of Southampton, School of Engineering Sciences, Computational Engineering and Design Group\n}}\n</ref>\n\n<ref name=pedersen10good-de>\n{{Cite journal\n|last=Pedersen\n|first=M.E.H.\n|url=http://www.hvass-labs.org/people/magnus/publications/pedersen10good-de.pdf\n|title=Good parameters for differential evolution\n|journal=Technical Report HL1002\n|publisher=Hvass Laboratories\n|year=2010\n}}\n</ref>\n\n<ref name=zhang11fitting>\n{{cite conference\n|last1=Zhang\n|first1=X.\n|last2=Jiang\n|first2=X.\n|last3=Scott\n|first3=P.J.\n|title=A Minimax Fitting Algorithm for Ultra-Precision Aspheric Surfaces\n|booktitle=The 13th International Conference on Metrology and Properties of Engineering Surfaces\n|year=2011\n|url=https://iopscience.iop.org/article/10.1088/1742-6596/311/1/012031/pdf\n}}\n</ref>\n\n<ref name=civici>\n{{cite journal\n|last=Civicioglu\n|first=P.\n|title=Transforming geocentric cartesian coordinates to geodetic coordinates by using differential search algorithm\n|journal=Computers & Geosciences\n|year=2012\n|volume=46\n|pages=229&ndash;247\n|doi=10.1016/j.cageo.2011.12.011\n}}\n</ref>\n\n<ref name=elediadereview>\n{{Cite journal\n|last1=Rocca\n|first1=P.\n|last2=Oliveri\n|first2=G.\n|last3=Massa\n|first3=A.\n|title=Differential Evolution as Applied to Electromagnetics\n|journal=IEEE Antennas and Propagation Magazine\n|year=2011\n|volume=53\n|issue=1\n|pages=38&ndash;49\n|doi=10.1109/MAP.2011.5773566 \n}}\n</ref>\n\n<ref name=\"zx03depso\">Zhang, Wen-Jun; Xie, Xiao-Feng (2003). [http://www.wiomax.com/team/xie/paper/SMCC03.pdf DEPSO: hybrid particle swarm with differential evolution operator]. ''IEEE International Conference on Systems, Man, and Cybernetics'' (SMCC), Washington, DC, USA: 3816-3821.</ref>\n\n<ref name=wong17capr>\n{{cite journal\n|last1=Wong\n|first1=I.\n|title=Continuous Adaptive Population Reduction (CAPR) for Differential Evolution Optimization\n|journal=SLAS Technology\n|year=2017\n|volume=22\n|issue=3\n|pages=289–305\n|doi=10.1177/2472630317690318\n}}\n</ref>\n\n}}\n\n==External links==\n* [http://www.icsi.berkeley.edu/~storn/code.html Storn's Homepage on DE] featuring source-code for several programming languages.\n\n{{Major subfields of optimization}}\n\n{{DEFAULTSORT:Differential Evolution}}\n[[Category:Evolutionary algorithms]]"
    },
    {
      "title": "Dispersive flies optimisation",
      "url": "https://en.wikipedia.org/wiki/Dispersive_flies_optimisation",
      "text": "{{Multiple issues|\n{{notability|date=March 2018}}\n{{primary sources|date=March 2018}}\n}}\n\n[[File:DFO.gif|thumb|Swarm behaviour in Dispersive Flies Optimisation]]\n'''Dispersive flies optimisation''' ('''DFO''') is a bare-bones [[swarm intelligence]] algorithm which is inspired by the swarming behaviour of flies hovering over food sources.<ref>{{cite journal|last1=Downes|first1=J. A.|title=The Swarming and Mating Flight of Diptera|journal=Annual Review of Entomology|date=January 1969|volume=14|issue=1|pages=271–298|doi=10.1146/annurev.en.14.010169.001415}}</ref> DFO is a simple [[Mathematical optimization|optimiser]] which works by [[Iterative method|iteratively]] trying to improve a [[candidate solution]] with regard to a numerical measure that is calculated by a [[fitness function]]. Each member of the population, a fly or an agent, holds a candidate solution whose suitability can be evaluated by their fitness value. Optimisation problems are often formulated as either minimisation or maximisation problems.\n\nDFO <ref>{{Cite book|last=al-Rifaie|first=Mohammad Majid|date=2014|title=Dispersive Flies Optimisation|url=https://fedcsis.org/proceedings/2014/drp/142.html|journal=Proceedings of the 2014 Federated Conference on Computer Science and Information Systems, IEEE|volume=2|pages=529–538|language=en|doi=10.15439/2014f142|series=Proceedings of the 2014 Federated Conference on Computer Science and Information Systems|isbn=978-83-60810-58-3}}</ref> was introduced with the intention of analysing a simplified swarm intelligence algorithm with the least number of tunable parameters and components. In the first work on DFO, this algorithm was compared against a few other existing swarm intelligence techniques using [[error]], efficiency and diversity measures. It is shown that despite the simplicity of the algorithm, which only uses agents’ position vectors at time ''t'' to generate the position vectors for time ''t''&nbsp;+&nbsp;1, it exhibits a competitive performance. Since its inception, DFO has been used in a variety of applications including medical imaging and image analysis as well as data mining and machine learning.\n\n== Algorithm ==\nDFO bears many similarities with other existing continuous, population-based optimisers (e.g. [[particle swarm optimization]] and [[differential evolution]]). In that, the swarming behaviour of the individuals consists of two tightly connected mechanisms, one is the formation of the swarm and the other is its breaking or weakening. DFO works by facilitating the information exchange between the members of the population (the swarming flies). Each fly <math>\\mathbf{x}</math> represents a position in a ''d''-dimensional search space: <math> \\mathbf{x} = (x_1,x_2,\\ldots,x_d)</math>, and the fitness of each fly is calculated by the fitness function <math>f(\\mathbf{x})</math>, which takes into account the flies' ''d'' dimensions: <math>f(\\mathbf{x}) = f(x_1,x_2,\\ldots,x_d) </math>.\n\nThe [[pseudocode]] below represents one iteration of the algorithm:\n   '''for''' i = 1 : N flies\n      <math> \\mathbf{x_i}.\\text{fitness} = f(\\mathbf{x}_i) </math>\n   '''end for''' i  \n   <math> \\mathbf{x}_s </math> = arg min <math display=\"inline\"> [f(\\mathbf{x}_i)], \\; i \\in \\{1,\\ldots,N\\}  </math>\n   '''for''' i = 1 : N and <math> i \\ne s </math>\n      '''for''' d = 1 : D dimensions\n         {{nowrap|'''if''' <math>U(0,1)<\\Delta </math>}}\n            <math>x_{id}^{t+1} = U(x_{\\min,d}, x_{\\max,d})</math>\n         '''else'''\n            <math>x_{id}^{t+1} = x_{i_{nd}}^t + U(0,1)( x_{sd}^t - x_{id}^t ) </math>\n         '''end if''' \n      '''end for''' d\n   '''end for''' i   \nIn the algorithm above, <math> x_{id}^{t+1} </math> represents fly <math> i </math> at dimension <math> d </math> and time <math> t+1 </math>; <math> x_{i_{nd}}^t </math> presents <math> x_i </math>'s best neighbouring fly in [[ring topology]] (left or right, using flies indexes), at dimension <math> d </math> and time <math> t </math>; and <math> x_{sd}^t </math> is the swarm's best fly. Using this update equation, the swarm's population update depends on each fly's best neighbour (which is used as the focus <math> \\mu </math>, and the difference between the current fly and the best in swarm represents the spread of movement, <math> \\sigma </math>).\n\nOther than the population size <math> N </math>, the only tunable parameter is the disturbance threshold <math> \\Delta </math>, which controls the dimension-wise restart in each fly vector. This mechanism is proposed to control the diversity of the swarm.\n\nOther notable minimalist swarm algorithm is Bare bones particle swarms (BB-PSO),<ref>{{cite book|last1=Kennedy|first1=J.|title=Bare bones particle swarms|journal=Proceedings of the 2003 IEEE Swarm Intelligence Symposium, 2003. SIS '03|date=2003|pages=80–87|doi=10.1109/SIS.2003.1202251|isbn=978-0-7803-7914-5}}</ref> which is based on particle swarm optimisation, along with bare bones differential evolution (BBDE) <ref>{{cite journal|last1=Omran|first1=Mahamed G.H.|last2=Engelbrecht|first2=Andries P.|last3=Salman|first3=Ayed|title=Bare bones differential evolution|journal=European Journal of Operational Research|date=July 2009|volume=196|issue=1|pages=128–139|doi=10.1016/j.ejor.2008.02.035|hdl=2263/8794}}</ref> which is a hybrid of the bare bones particle swarm optimiser and differential evolution, aiming to reduce the number of parameters. Alhakbani in her PhD thesis<ref>{{cite book |last1=Alhakbani |first1=Haya |title=Handling Class Imbalance Using Swarm Intelligence Techniques, Hybrid Data and Algorithmic Level Solutions |date= 2018 |publisher=[PhD Thesis] Goldsmiths, University of London |location=London, UK}}</ref> covers many aspects of the algorithms including several DFP applications in feature selection as well as parameter tuning.\n\n== Applications ==\n\nSome of the recent applications of DFO are listed below:\n\n* Optimising [[support vector machine]] kernel to classify imbalanced data <ref>{{cite book|last1=Alhakbani|first1=H. A.|last2=al-Rifaie|first2=M. M.|date=2017|title=Optimising SVM to classify imbalanced data using Dispersive Flies Optimisation|url=http://ieeexplore.ieee.org/document/8104572/|journal=2017 Federated Conference on Computer Science and Information Systems (FedCSIS), IEEE|volume=11|pages=399–402|doi=10.15439/2017F91|series=Proceedings of the 2017 Federated Conference on Computer Science and Information Systems|isbn=978-83-946253-7-5}}</ref>\n* Quantifying [[Symmetry|symmetrical complexity]] in [[Aesthetics|computational aesthetics]] <ref>{{cite book|last1=al-Rifaie|first1=Mohammad Majid|last2=Ursyn|first2=Anna|last3=Zimmer|first3=Robert|last4=Javaheri Javid|first4=Mohammad Ali|date=2017|title=On Symmetry, Aesthetics and Quantifying Symmetrical Complexity|journal=Computational Intelligence in Music, Sound, Art and Design|volume=10198|language=en|pages=17–32|doi=10.1007/978-3-319-55750-2_2|series=Lecture Notes in Computer Science|isbn=978-3-319-55749-6}}</ref>\n* Analysing computational [[autopoiesis]] and [[computational creativity]] <ref>{{cite journal|last1=al-Rifaie|first1=Mohammad Majid|last2=Fol Leymarie|first2=Frédéric|last3=Latham|first3=William|last4=Bishop|first4=Mark|date=2017|title=Swarmic autopoiesis and computational creativity|journal=Connection Science|volume=29|issue=4|pages=276–294|doi=10.1080/09540091.2016.1274960|url=http://research.gold.ac.uk/19762/1/ConnScien_2017cc.pdf}}</ref>\n* Identifying [[calcification]]s in [[Medical imaging|medical images]] <ref>{{cite book|last1=al-Rifaie|first1=Mohammad Majid|last2=Aber|first2=Ahmed|date=2016|title=Dispersive Flies Optimisation and Medical Imaging|journal=Recent Advances in Computational Optimization|language=English|volume=610|pages=183–203|doi=10.1007/978-3-319-21133-6_11|series=Studies in Computational Intelligence|isbn=978-3-319-21132-9|url=http://research.gold.ac.uk/17250/1/2016_DFO_Medical%20Imaging.pdf}}</ref>\n* Building non-identical organic structures for game's space development <ref>{{cite journal|last1=King|first1=Michael|last2=al-Rifaie|first2=Mohammad Majid|date=2017|title=Building simple non-identical organic structures with dispersive flies optimisation and a* path-finding|journal=AISB 2017: Games and AI|pages=336–340}}</ref>\n\n== References ==\n{{Reflist}}\n\n[[Category:Nature-inspired metaheuristics]]\n[[Category:Mathematical optimization]]\n[[Category:Evolutionary algorithms]]"
    },
    {
      "title": "Effective fitness",
      "url": "https://en.wikipedia.org/wiki/Effective_fitness",
      "text": "{{Evolutionary algorithms}}\nIn [[evolution|natural evolution]] and artificial evolution (e.g. [[artificial life]] and [[evolutionary computation]]) the  [[fitness (biology)|fitness]] (or performance or [[objective measure]]) of a [[Schema (genetic algorithms)|schema]] is rescaled to give its '''effective fitness''' which takes into account [[crossover (genetic algorithm)|crossover]] and [[Mutation (genetic algorithm)|mutation]]. That is '''effective fitness''' can be thought of as the fitness that the schema would need to have in order to increase or decrease as a fraction of the population as it actually does with crossover and mutation present but as if they were not.\n\n==References==\n* [http://www.cs.ucl.ac.uk/staff/W.Langdon/FOGP/ Foundations of Genetic Programming]\n\n[[Category:Evolutionary algorithms]]\n\n{{evolution-stub}}"
    },
    {
      "title": "Evolution strategy",
      "url": "https://en.wikipedia.org/wiki/Evolution_strategy",
      "text": "{{Evolutionary algorithms}}\nIn computer science, an '''evolution strategy (ES)''' is an [[optimization (mathematics)|optimization]] technique based on ideas of [[evolution]]. It belongs to the general class of [[evolutionary computation]] or [[artificial evolution]] methodologies.\n\n==History==\nThe 'evolution strategy' optimization technique was created in the early 1960s and developed further in the 1970s and later by [[Ingo Rechenberg]], [[Hans-Paul Schwefel]] and their co-workers.\n\n==Methods==\nEvolution strategies use natural problem-dependent representations, and primarily [[mutation]] and [[Selection (biology)|selection]], as search operators. In common with [[evolutionary algorithms]], the operators are applied in a loop. An iteration of the loop is called a generation. The sequence of generations is continued until a termination criterion is met.\n\nFor real-valued search spaces, mutation is performed by adding a [[normal distribution|normally distributed]] random value to each vector component. The step size or mutation strength (i.e. the standard deviation of the normal distribution) is often governed by self-adaptation (see [[evolution window]]). Individual step sizes for each coordinate or correlations between coordinates are governed either by self-adaptation or by covariance matrix adaptation ([[CMA-ES]]).\n\nThe (environmental) selection in evolution strategies is deterministic and only based on the fitness rankings, not on the actual fitness values. The resulting algorithm is therefore invariant with respect to monotonic transformations of the objective function. The simplest evolution strategy operates on a population of size two: the current point (parent) and the result of its mutation. Only if the mutant's fitness is at least as good as the parent one, it becomes the parent of the next generation. Otherwise the mutant is disregarded. This is a ''(1&nbsp;+&nbsp;1)-ES''. More generally, λ mutants can be generated and compete with the parent, called ''(1&nbsp;+&nbsp;λ)-ES''. In (1&nbsp;,&nbsp;λ)-ES the best mutant becomes the parent of the next generation while the current parent is always disregarded. For some of these variants, proofs of [[Rate of convergence|linear convergence]] (in a [[stochastic]] sense) have been derived on unimodal objective functions.<ref>{{cite journal\n| doi = 10.1016/j.tcs.2004.11.017\n| first = A.\n| last = Auger\n| title = Convergence results for the (1,λ)-SA-ES using the theory of &phi;-irreducible Markov chains\n| journal = Theoretical Computer Science\n| volume = 334\n| issue = 1-3\n| pages = 35–69\n| publisher = Elsevier\n| year = 2005\n| location = \n| url = \n| pmid = \n| accessdate = \n| id = \n}}</ref><ref>{{cite journal\n| doi = 10.1016/j.tcs.2006.04.004\n| first = J.\n| last = Jägersküpper\n| title = How the (1+1) ES using isotropic mutations minimizes positive definite quadratic forms\n| journal = Theoretical Computer Science\n| volume = 361\n| issue = 1\n| pages = 38–56\n| publisher = Elsevier\n| year = 2006\n| location = \n| url = \n| pmid = \n| accessdate = \n| id = \n}}</ref>\n\nContemporary derivatives of evolution strategy often use a population of μ parents and recombination as an additional operator, called ''(μ/ρ+,&nbsp;λ)-ES''. This makes them less prone to settle in local optima.<ref>{{cite conference\n| first = N.\n| last = Hansen\n|author2=S. Kern\n| title = Evaluating the CMA Evolution Strategy on Multimodal Test Functions\n| booktitle = Parallel Problem Solving from Nature - PPSN VIII\n| pages = 282–291\n| publisher = Springer\n| date = 2004\n| location = \n| accessdate = \n| id = \n| doi = 10.1007/978-3-540-30217-9_29\n}}</ref>\n\n==See also==\n\n* [[CMA-ES|Covariance matrix adaptation evolution strategy (CMA-ES)]]\n* [[Derivative-free optimization]]\n* [[Evolutionary computation]]\n* [[Genetic algorithm]]\n* [[Natural evolution strategy]]\n\n==References==\n{{Reflist}}\n\n==Bibliography==\n\n* [[Ingo Rechenberg]] (1971): [https://www.jstor.org/stable/23679080 Evolutionsstrategie &ndash; Optimierung technischer Systeme nach Prinzipien der biologischen Evolution] (PhD thesis). Reprinted by Frommann-Holzboog (1973).\n* [[Hans-Paul Schwefel]] (1974): Numerische Optimierung von Computer-Modellen (PhD thesis). Reprinted by Birkhäuser (1977).\n* H.-G. Beyer and H.-P. Schwefel. [https://journals.openedition.org/trivium/3664 Evolution Strategies: A Comprehensive Introduction]. Journal Natural Computing, 1(1):3&ndash;52, 2002.\n* Hans-Georg Beyer: The Theory of Evolution Strategies: Springer April 27, 2001.\n* Hans-Paul Schwefel: Evolution and Optimum Seeking: New York: Wiley & Sons 1995.\n* Ingo Rechenberg: Evolutionsstrategie '94. Stuttgart: Frommann-Holzboog 1994.\n* J. Klockgether and H. P. Schwefel (1970). [https://www.researchgate.net/profile/Hans-Paul_Schwefel/publication/236373493_TWO-PHASE_NOZZLE_AND_HOLLOW_CORE_JET_EXPERIMENTS/links/544bd4db0cf2d6347f43a164.pdf Two-Phase Nozzle And Hollow Core Jet Experiments]. AEG-Forschungsinstitut. MDH Staustrahlrohr Project Group. Berlin, Federal Republic of Germany. Proceedings of the 11th Symposium on Engineering Aspects of Magneto-Hydrodynamics, Caltech, Pasadena, Cal., 24.&ndash;26.3. 1970.\n\n==Research centers==\n* [http://www.bionik.tu-berlin.de/institut/xstart.htm Bionics & Evolutiontechnique at the Technical University Berlin]\n* [http://ls11-www.cs.uni-dortmund.de/ Chair of Algorithm Engineering (Ls11) &ndash; University of Dortmund]\n* [https://archive.is/20130106090846/http://sfbci.cs.uni-dortmund.de/ Collaborative Research Center 531 &ndash; University of Dortmund]\n\n{{Evolutionary computation}}\n\n{{DEFAULTSORT:Evolution Strategy}}\n[[Category:Evolutionary algorithms]]"
    },
    {
      "title": "Evolution window",
      "url": "https://en.wikipedia.org/wiki/Evolution_window",
      "text": "{{short description|A narrow band of mutation step size that is conducive to significant evolutionary progress}}\nIt was observed in [[evolution strategy|evolution strategies]] that significant progress toward the fitness/objective function's [[Optimization (mathematics)|optimum]], generally, can only happen in a narrow band of the mutation step size σ. That narrow band is called '''evolution window'''.\n\nThere are three well-known methods to adapt the mutation step size σ in evolution strategies:\n\n* (1/5-th) Success Rule\n* Self-Adaptation (for example through [[log-normal]] mutations)\n* Cumulative Step Size Adaptation (CSA)\n\nOn simple functions all of them have been empirically shown to keep the step size within the evolution window.\n\n==See also==\n\n* [[Bionics]]\n* [[Cybernetics]]\n* [[Evolutionary Algorithm]]\n* [[Evolution strategy]]\n* [[Optimization (mathematics)]]\n\n==References==\n\n* H.-G. Beyer. Toward a Theory of Evolution Strategies: Self-Adaptation. ''Evolutionary Computation, 3''(3), 311-347.\n* Ingo Rechenberg: ''Evolutionsstrategie '94''. Stuttgart: Frommann-Holzboog 1994.\n\n\n[[Category:Evolutionary algorithms]]"
    },
    {
      "title": "Evolutionary acquisition of neural topologies",
      "url": "https://en.wikipedia.org/wiki/Evolutionary_acquisition_of_neural_topologies",
      "text": "{{short description|A method that evolves both the topology and weights of artificial neural networks}}\n{{hatnote|This article is on evolutionary acquisition of artificial neural topologies, not of natural ones.}}\n\n'''Evolutionary acquisition of neural topologies''' ('''EANT'''/'''EANT2''') is an [[Evolutionary computation|evolutionary]] [[reinforcement learning]] method that evolves both the topology and weights of [[artificial neural network]]s. It is closely related to the works of Angeline et al.<ref>Peter J Angeline, Gregory M Saunders, and Jordan B Pollack. An evolutionary algorithm that constructs recurrent neural networks. IEEE Transactions on Neural Networks, 5:54–65, 1994. [http://demo.cs.brandeis.edu/papers/ieeenn.pdf]</ref> and Stanley and Miikkulainen.<ref>NeuroEvolution of Augmented Topologies (NEAT) by Stanley and Miikkulainen, 2005 [http://nn.cs.utexas.edu/downloads/papers/stanley.ieeetec05.pdf]</ref> Like the work of Angeline et al., the method uses a type of parametric mutation that comes from [[Evolution strategy|evolution strategies]] and [[evolutionary programming]] (now using the most advanced form of the evolution strategies [[CMA-ES]] in EANT2), in which adaptive step sizes are used for optimizing the weights of the neural networks.  Similar to the work of Stanley ([[Neuroevolution of augmenting topologies|NEAT]]), the method starts with minimal structures which gain complexity along the evolution path.\n\n==Contribution of EANT to neuroevolution==\n\nDespite sharing these two properties, the method has the following important features which distinguish it from previous works in [[neuroevolution]].\n\nIt introduces a genetic encoding called '''common genetic encoding''' (CGE) that handles both direct and indirect encoding of neural networks within the same theoretical framework. The encoding has important properties that makes it suitable for evolving neural networks: \n# It is ''complete'' in that it is able to represent all types of valid phenotype networks.\n# It is ''closed'', i.e. every valid genotype represents a valid phenotype. (Similarly, the encoding is ''closed under genetic operators'' such as structural mutation and crossover.)\n\nThese properties have been formally proven in <ref>Yohannes Kassahun, Mark Edgington, Jan Hendrik Metzen, Gerald Sommer and Frank Kirchner.  Common Genetic Encoding for Both Direct and Indirect Encodings of Networks. In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO 2007), London, UK, 1029–1036, 2007.[http://portal.acm.org/citation.cfm?id=1277162]</ref>.\n\nFor evolving the structure and weights of neural networks, an evolutionary process is used, where the ''exploration'' of structures is executed at a larger timescale (structural exploration), and the ''exploitation'' of existing structures is done at a smaller timescale (structural exploitation). In the structural exploration phase, new neural structures are developed by gradually adding new structures to an initially minimal network that is used as a starting point. In the structural exploitation phase, the weights of the currently available structures are optimized using an [[evolution strategy]].\n\n==Performance==\n\nEANT has been tested on some benchmark problems such as the double-pole balancing problem,<ref>Yohannes Kassahun and Gerald Sommer. Efficient reinforcement learning through evolutionary acquisition of neural topologies. In Proceedings of the 13th European Symposium on Artificial Neural Networks (ESANN 2005), pages 259–266, Bruges, Belgium, April 2005. {{cite web |url=http://www.ks.informatik.uni-kiel.de/~yk/ESANN2005EANT.pdf |title=Archived copy |accessdate=2008-02-11 |deadurl=yes |archiveurl=https://web.archive.org/web/20070613093500/http://www.ks.informatik.uni-kiel.de/~yk/ESANN2005EANT.pdf |archivedate=2007-06-13 |df= }}</ref> and the [[RoboCup]] keepaway benchmark.<ref>Jan Hendrik Metzen, Mark Edgington, Yohannes Kassahun and Frank Kirchner. Performance Evaluation of EANT in the RoboCup Keepaway Benchmark. In Proceedings of the Sixth International Conference on Machine Learning and Applications (ICMLA 2007), pages 342–347, USA, 2007 [http://portal.acm.org/citation.cfm?id=1336953.1337145&coll=GUIDE&dl=GUIDE]</ref> In all the tests, EANT was found to perform very well. Moreover, a newer version of EANT, called EANT2, was tested on a visual servoing task and found to outperform [[Neuroevolution of augmenting topologies|NEAT]] and the traditional iterative [[Gauss–Newton algorithm|Gauss–Newton]] method.<ref>Nils T Siebel and Gerald Sommer. Evolutionary reinforcement learning of artificial neural networks.  International Journal of Hybrid Intelligent Systems 4(3): 171–183, October 2007. [http://www.informatik.uni-kiel.de/inf/Sommer/doc/Publications/nts/SiebelSommer-IJHIS2007.pdf]</ref>  Further experiments include results on a classification problem <ref>Nils T Siebel and Gerald Sommer. Learning Defect Classifiers for Visual Inspection Images by Neuro-evolution using Weakly Labelled Training Data.  Proceedings of the IEEE Congress on Evolutionary Computation (CEC 2008), pages 3926–3932, Hong Kong, China, June 2008. [https://www.informatik.uni-kiel.de/inf/Sommer/doc/Publications/nts/SiebelSommer-EANT2DefectClassification-CEC2008.pdf].</ref>\n\n==References==\n{{reflist}}\n\n==External links==\n*[http://beacon-center.org/blog/2012/08/13/evolution-101-neuroevolution/ BEACON Blog: What is neuroevolution?]\n\n[[Category:Artificial neural networks]]\n[[Category:Evolutionary algorithms]]\n[[Category:Evolutionary computation]]"
    },
    {
      "title": "Evolutionary Algorithm for Landmark Detection",
      "url": "https://en.wikipedia.org/wiki/Evolutionary_Algorithm_for_Landmark_Detection",
      "text": "{{Multiple issues|\n{{expert-subject|date=December 2010}}\n{{cleanup|date=December 2010}}\n{{orphan|date=November 2010}}\n}}\n\nthere are several [[algorithm]]s for locating [[landmark]]s in images such as [[satellite map]]s, [[medical image]]s etc.<br />\nnowadays [[evolutionary algorithm]]s such as [[particle swarm optimization]] are so useful to perform this task. evolutionary algorithms generally have two phase, training and test.\n\nin the training phase, we try to learn the algorithm to locate landmark correctly. this phase performs in some [[iteration]]s and finally in the last iteration we hope to obtain a system that can locate the landmark, correctly. in the particle swarm optimization there are some [[particle]]s that search for the landmark. each particle uses a specific [[formula]] in each iteration to optimizes the [[landmark detecting]].\n\nThe fundamental particle swarm optimization algorithm used in training phase generally as follows:\n\nRandomly initialise 100 individuals in the [[Mathematical optimization#Optimization problems|search space]] in the range [-1,1]<br />\nLOOP UNTIL 100 iterations performed OR [[detection error]] of gbest is 0%<br />\nFOR each particle p<br />\nDetection errors at x = 0<br />\nFOR each image i in training set<br />\nFOR each pixel coordinate c in i<br />\nEvaluate x of p on visual features at c<br />\nIF evaluation is highest so far for i THEN<br />\nDetected position in i = c<br />\nIF distance between detected position and marked-up position > 2mm THEN<br />\nDetection errors at x = Detection errors at x + 1<br />\nFitness of p at x = 1- ( Detection errors at x /Total no. of images in training set)<br />\nIF new _tness of p at x > previous _tness of p at pbest THEN<br />\npbest _tness of p = new _tness of p at x<br />\npbest position of p = x of p\n\nIF new _tness of p at x > previous gbest _tness THEN<br />\ngbest _tness = new _tness of p at x<br />\ngbest position of p = x of p<br />\nFOR each particle p<br />\nCalculate v of p<br />\nIF magnitude of v > v max THEN<br />\nMagnitude of v = v max<br />\nMove x of p to next position using v<br />\nIF x of p outside [-1,1] range THEN<br />\nx of p = -1 or 1 as appropriate<br />\nREPEAT<br />\nOutput gbest of last iteration as trained detector d\n\n== References ==\n<!--- See [[Wikipedia:Footnotes]] on how to create references using <ref></ref> tags which will then appear here automatically -->\nhttps://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.72.3218\n\n[[Category:Evolutionary algorithms]]"
    },
    {
      "title": "Evolutionary art",
      "url": "https://en.wikipedia.org/wiki/Evolutionary_art",
      "text": "{{short description|A branch of generative art where a system generates the art with an iterated process of selection by the artist and modification}}\n[[Image:Imagebreeder example.png|thumb|200px|right|An image generated using an [[evolutionary algorithm]]]]\n\n'''Evolutionary art''' is a branch of [[generative art]], in which the artist does not do the work of constructing the artwork, but rather lets a system do the construction.  In evolutionary art, initially generated art is put through an iterated process of selection and modification to arrive at a final product, where it is the artist who is the selective agent.\n\nEvolutionary art is to be distinguished from [[BioArt]], which uses living organisms as the material medium instead of paint, stone, metal, etc.\n\n==Overview==\nIn common with biological [[evolution]] through [[natural selection]] or [[animal husbandry]], the members of a population undergoing artificial evolution modify their form or behavior over many reproductive generations in response to a selective regime.\n\nIn [[interactive evolutionary computation|interactive evolution]] the selective regime may be applied by the viewer explicitly by selecting individuals which are aesthetically pleasing. Alternatively a [[selection pressure]] can be generated implicitly, for example according to the length of time a viewer spends near a piece of evolving art.\n\nEqually, evolution may be employed as a mechanism for generating a dynamic world of adaptive individuals, in which the [[selection pressure]] is imposed by the program, and the viewer plays no role in selection, as in the [[Black Shoals]] project.\n\n==See also==\n* [[Digital morphogenesis]]\n* [[Electric Sheep]]\n* [[Evolutionary music]]\n* [[NEAT Particles]]\n* [[Universal Darwinism]]\n\n==Further reading==\n* Bentley, Peter, and David Corne. ''Creative Evolutionary Systems''. Morgan Kaufmann, 2002.\n* ''Metacreations: Art and Artificial Life'', M Whitelaw, 2004, MIT Press\n*[http://art-artificial-evolution.dei.uc.pt/ ''The Art of Artificial Evolution: A Handbook on Evolutionary Art and Music''], Juan Romero and Penousal Machado (eds.), 2007, Springer\n* ''Evolutionary Art and Computers'', W Latham, S Todd, 1992, Academic Press\n* ''Genetic Algorithms in Visual Art and Music'' Special Edition: Leonardo. VOL. 35, ISSUE 2 - 2002 (Part I), C Johnson, J Romero Cardalda (eds), 2002, MIT Press\n*[https://www.amazon.com/Evolved-Art-Turtles-Tim-Endres/dp/0615300340/ ''Evolved Art: Turtles - Volume One''], {{ISBN|978-0-615-30034-4}}, Tim Endres, 2009, EvolvedArt.biz\n\n==External links==\n*[http://www.behance.net/friedlich], Abstract Genomic Art: An Introduction by Avi L. Friedlich\n*[http://iasl.uni-muenchen.de/links/GCA-IV.3e.html Thomas Dreher: History of Computer Art, Chap. IV.3: Evolutionary Art]\n*[http://www.cse.fau.edu/~thomas \"Evolutionary Art Gallery\"], by [[Thomas Fernandez]]\n*[http://www.freethoughtdebater.com/ALifeBiomorphsAbout.htm \"Biomorphs\"], by [[Richard Dawkins]]\n*[https://franciscouzo.github.io/genetic_art/ Genetic Art], a site that evolves images\n*[http://EndlessForms.com EndlessForms.com], Collaborative interactive evolution allowing you to evolve 3D objects and have them 3D printed.\n*[http://www.musigenesis.com \"MusiGenesis\"], a program that evolves music on a [[personal computer|PC]]\n*[http://artdent.homelinux.net/evolve/about/ \"Evolve\"], a program by Josh Lee that evolves art through a voting process.\n*[http://w-shadow.com/li/ \"Living Image Project\"], a site where images are evolved based on votes of visitors.\n*[http://www.emoware.org/evolutionary_art.asp \"An evolutionary art program using Cartesian Genetic Programming\"]\n*[http://eartweb.vanhemert.co.uk/ Evolutionary Art on the Web] Interactively generate Mondriaan, Theo van Doesburg, Mandala and Fractal art.\n*[https://web.archive.org/web/20060412133402/http://www.codeasart.com/poetry/darwin.html \"Darwinian Poetry\"]\n*[http://www.ashleymills.com/ae \"One mans eyes?\"], Aesthetically evolved images by Ashley Mills.\n*[http://www.xs4all.nl/~notnot/E-volverLUMC/E-volverLUMC.html \"E-volver\"], interactive breeding units.\n*[http://www.xs4all.nl/~notnot/breed/Breed.html \"Breed\"], evolved sculptures produced by rapid manufacturing techniques.\n*[http://picbreeder.org \"Picbreeder\"], Collaborative breeder allowing branching from other users' creations that produces pictures like faces and spaceships.\n*[http://www.wickedbean.co.uk/cfdg/index.html \"CFDG Mutate\"], a tool for image evolution based on Chris Coyne's Context Free Design Grammar.\n*[http://www.pikiproductions.com/rui/xtnz/index.html \"xTNZ\"], a three-dimensional ecosystem, where creatures evolve shapes and sounds.\n*[http://art-artificial-evolution.dei.uc.pt/ The Art of Artificial Evolution: A Handbook on Evolutionary Art and Music]\n*[http://www.evolvedturtle.com/ Evolved Turtle Website] Evolved Turtle Website - Evolve art based on [[Turtle graphics|Turtle Logo]] using the Windows app BioLogo.\n*[http://www.bottlenose.demon.co.uk/share/evolvotron/index.htm Evolvotron] - Evolutionary art software ([http://www.bottlenose.demon.co.uk/share/evolvotron/gallery.htm example output]).\n*[http://www.istanbulmuseum.org/kibris/ae.html Artificial Evolution of the Cyprus Problem (2005)] is an evolutionary artwork created by [[Genco Gulan]]\n*[http://de.evo-art.org/index.php?title=Literatur_zur_Evolution%C3%A4ren_Kunst Evo Art bibliography] largest online bibliography to evolutionary art and related fields like evolutionary architecture and design, evolutionary [[image processing]], [[generative art]], [[computational aesthetics]] and [[computational creativity]] as part of the MediaWiki based [http://de.evo-art.org Encyclopedia Evolutionary Art]\n*[http://evostar.dei.uc.pt/2012/call-for-contributions/evomusart/ \"Evomusart. 1st International Conference and 10th European Event on Evolutionary and Biologically Inspired Music, Sound, Art and Design\"]\n\n[[Category:Computer art]]\n[[Category:Evolutionary algorithms]]"
    },
    {
      "title": "Evolutionary multimodal optimization",
      "url": "https://en.wikipedia.org/wiki/Evolutionary_multimodal_optimization",
      "text": "{{Evolutionary algorithms}}\nIn [[applied mathematics]], '''multimodal optimization''' deals with [[Mathematical optimization|optimization]] tasks that involve finding all or most of the multiple (at least locally optimal) solutions of a problem, as opposed to a single best solution.\nEvolutionary multimodal optimization is a branch of [[evolutionary computation]], which is closely related to [[machine learning]]. Wong provides a short survey,<ref>Wong, K. C. (2015), [https://arxiv.org/abs/1508.00457 Evolutionary Multimodal Optimization: A Short Survey] arXiv preprint arXiv:1508.00457</ref> wherein the chapter of Shir<ref>Shir, O.M. (2012), [https://link.springer.com/book/10.1007/978-3-540-92910-9 Niching in Evolutionary Algorithms]</ref> and the book of Preuss<ref>Preuss, Mike  (2015), [https://www.springer.com/de/book/9783319074061 Multimodal Optimization by Means of Evolutionary Algorithms]</ref> cover the topic in more detail.\n\n== Motivation ==\nKnowledge of multiple solutions to an optimization task is especially helpful in engineering, when due to physical (and/or cost) constraints, the best results may not always be realizable.  In such a scenario, if multiple solutions (locally and/or globally optimal) are known, the implementation can be quickly switched to another solution and still obtain the best possible system performance. Multiple solutions could also be analyzed to discover hidden properties (or relationships) of the underlying optimization problem, which makes them important for obtaining [[domain knowledge]]. \nIn addition, the algorithms for multimodal optimization usually not only locate multiple optima in a single run, but also preserve their population diversity, resulting in their global optimization ability on multimodal functions. Moreover, the techniques for multimodal optimization are usually borrowed as diversity maintenance techniques to other problems.<ref>Wong, K. C. et al. (2012), [https://dx.doi.org/10.1016/j.ins.2011.12.016 Evolutionary multimodal optimization using the principle of locality] Information Sciences</ref>\n\n== Background ==\n\nClassical techniques of optimization would need multiple restart points and multiple runs in the hope that a different solution may be discovered every run, with no guarantee however.  [[Evolutionary algorithm]]s (EAs) due to their population based approach, provide a natural advantage over classical optimization techniques. They maintain a population of possible solutions, which are processed every generation,  and if the multiple solutions can be preserved over all these generations, then at termination of the algorithm we will have multiple good solutions,  rather than only the best solution. Note that this is against the natural tendency of classical optimization techniques, which will always converge to the best solution, or a sub-optimal solution (in a rugged, “badly behaving” function).''' Finding''' and '''maintenance''' of multiple solutions is wherein lies the challenge of using EAs for multi-modal optimization. '''Niching'''<ref>Mahfoud, S. W. (1995), \"Niching methods for genetic algorithms\"</ref> is a generic term referred to as the technique of finding and preserving multiple stable ''niches'', or favorable  parts of the solution space possibly around multiple solutions, so as to prevent convergence to a single solution.\n\nThe field of [[Evolutionary algorithm]]s encompasses [[genetic algorithm]]s (GAs), [[evolution strategy]] (ES), [[differential evolution]] (DE), [[particle swarm optimization]] (PSO),  and other methods. Attempts have been made to solve multi-modal optimization in all these realms and most, if not all the various methods implement niching in some form or the other.\n\n== Multimodal optimization using genetic algorithms/evolution strategies ==\n\nDe Jong's crowding method,  Goldberg's sharing function approach, Petrowski's clearing method, restricted mating, maintaining multiple subpopulations are some of the popular approaches that have been proposed by the community. The first two methods are especially well studied, however, they do not perform explicit separation into solutions belonging to different basins of attraction.\n\nThe application of multimodal optimization within ES was not explicit for many years, and has been explored only recently.\nA niching framework utilizing derandomized ES was introduced by Shir,<ref>Shir, O.M. (2008), \"Niching in Derandomized Evolution Strategies and its Applications in Quantum Control\"</ref> '''proposing the [[CMA-ES]] as a niching optimizer for the first time'''. The underpinning of that framework was the selection of a peak individual per subpopulation in each generation, followed by its sampling to produce the consecutive dispersion of search-points. The ''biological analogy'' of this machinery is an ''alpha-male'' winning all the imposed competitions and dominating thereafter its ''ecological niche'', which then obtains all the sexual resources therein to generate its offspring.\n\nRecently, an evolutionary [[multiobjective optimization]] (EMO) approach was proposed,<ref>Deb, K., Saha, A. (2010) \"Finding Multiple Solutions for Multimodal Optimization Problems Using a Multi-Objective Evolutionary Approach\" (GECCO 2010, In press)</ref> in which a suitable second objective is added to the originally single objective multimodal optimization problem, so that the multiple solutions form a '' weak pareto-optimal'' front. Hence, the multimodal optimization problem can be solved for its multiple solutions using an EMO algorithm. Improving upon their work,<ref>Saha, A., Deb, K. (2010) \"A Bi-criterion Approach to Multimodal Optimization: Self-adaptive Approach \" (Lecture Notes in Computer Science, 2010, Volume 6457/2010, 95–104)</ref> the same authors have made their algorithm self-adaptive, thus eliminating the need for pre-specifying the parameters.\n\nAn approach that does not use any radius for separating the population into subpopulations (or species) but employs the space topology instead is proposed in.<ref>C. Stoean, M. Preuss, R. Stoean, D. Dumitrescu (2010) [http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=5491155 Multimodal Optimization by means of a Topological Species Conservation Algorithm]. In  IEEE Transactions on Evolutionary Computation, Vol. 14, Issue 6, pages 842–864, 2010.</ref>\n\n[[File:GA-Multi-modal.ogv|thumbtime=1|thumb |350px |alt= Finding multiple optima using Genetic Algorithms in a Multi-modal optimization task| Finding multiple optima using genetic algorithms in a multi-modal optimization task. (The algorithm demonstrated in this demo is the one proposed by Deb, Saha in the multi-objective approach to multimodal optimization.)]]\n\n== Multimodal optimization using DE ==\n\nThe niching methods used in GAs have also been explored with success in the DE community. DE based local selection and global selection approaches have also been attempted for solving multi-modal problems. DE's coupled with local search algorithms (Memetic DE) have been explored as an approach to solve multi-modal problems.\n\nFor a comprehensive treatment of multimodal optimization methods in DE, refer the Ph.D thesis Ronkkonen, J. (2009). ''Continuous Multimodal Global Optimization with Differential Evolution Based Methods''.<ref>Ronkkonen,J., (2009). [https://oa.doria.fi/bitstream/handle/10024/50498/isbn%209789522148520.pdf Continuous Multimodal Global Optimization with Differential Evolution Based Methods]</ref>\n\n== References ==\n<!--- See [[Wikipedia:Footnotes]] on how to create references using <ref></ref> tags which will then appear here automatically -->\n{{Reflist}}\n\n== Bibliography ==\n{{refbegin}}\n\n* D. Goldberg and J. Richardson. (1987) \"Genetic algorithms with sharing for multimodal function optimization\". In Proceedings of the Second International Conference on Genetic Algorithms on Genetic algorithms and their application table of contents, pages 41–49. L. Erlbaum Associates Inc. Hillsdale, NJ, USA, 1987.\n* A. Petrowski. (1996) \"A clearing procedure as a niching method for genetic algorithms\". In Proceedings of the 1996 IEEE International Conference on Evolutionary Computation, pages 798–803. Citeseer, 1996.\n* Deb, K., (2001) \"Multi-objective Optimization using Evolutionary Algorithms\", Wiley ([https://books.google.com/books?id=OSTn4GSy2uQC&printsec=frontcover&dq=multi+objective+optimization&source=bl&ots=tCmpqyNlj0&sig=r00IYlDnjaRVU94DvotX-I5mVCI&hl=en&ei=fHnNS4K5IMuLkAWJ8OgS&sa=X&oi=book_result&ct=result&resnum=8&ved=0CD0Q6AEwBw#v=onepage&q&f=false Google Books)]\n* F. Streichert, G. Stein, H. Ulmer, and A. Zell. (2004) \"A clustering based niching EA for multimodal search spaces\". Lecture Notes in Computer Science, pages 293–304, 2004.\n* Singh, G., Deb, K., (2006) \"Comparison of multi-modal optimization algorithms based on evolutionary algorithms\". In Proceedings of the 8th annual conference on Genetic and evolutionary computation, pages 8–12. ACM, 2006.\n* Ronkkonen, J., (2009). [https://oa.doria.fi/bitstream/handle/10024/50498/isbn%209789522148520.pdf Continuous Multimodal Global Optimization with Differential Evolution Based Methods]\n* Wong, K. C., (2009). [http://portal.acm.org/citation.cfm?id=1570027 An evolutionary algorithm with species-specific explosion for multimodal optimization. GECCO 2009: 923–930]\n* J. Barrera and C. A. C. Coello. \"A Review of Particle Swarm Optimization Methods used for Multimodal Optimization\", pages 9–37. Springer, Berlin, November 2009.\n* Wong, K. C., (2010). [http://www.springerlink.com/content/jn23t10366778017/ Effect of Spatial Locality on an Evolutionary Algorithm for Multimodal Optimization. EvoApplications (1) 2010: 481–490]\n* Deb, K., Saha, A. (2010) [http://portal.acm.org/citation.cfm?id=1830483.1830568 Finding Multiple Solutions for Multimodal Optimization Problems Using a Multi-Objective Evolutionary Approach. GECCO 2010: 447–454]\n* Wong, K. C., (2010). [http://portal.acm.org/citation.cfm?id=1830483.1830513 Protein structure prediction on a lattice model via multimodal optimization techniques. GECCO 2010: 155–162]\n* Saha, A., Deb, K. (2010), [http://www.springerlink.com/content/8676217j87173p60/ A Bi-criterion Approach to Multimodal Optimization: Self-adaptive Approach. SEAL 2010: 95–104]\n* Shir, O.M., Emmerich, M., Bäck, T. (2010), [http://www.mitpressjournals.org/doi/abs/10.1162/evco.2010.18.1.18104#.VoDu4l6Y7ro Adaptive Niche Radii and Niche Shapes Approaches for Niching with the CMA-ES. Evolutionary Computation Vol. 18, No. 1, pp.&nbsp; 97-126.]\n* C. Stoean, M. Preuss, R. Stoean, D. Dumitrescu (2010) [http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=5491155 Multimodal Optimization by means of a Topological Species Conservation Algorithm]. In  IEEE Transactions on Evolutionary Computation, Vol. 14, Issue 6, pages 842–864, 2010.\n* S. Das, S. Maity, B-Y Qu, P. N. Suganthan, \"Real-parameter evolutionary multimodal optimization — A survey of the state-of-the-art\", Vol. 1, No. 2,  pp.&nbsp;71–88, Swarm and Evolutionary Computation, June 2011.\n{{refend}}\n\n== External links ==\n* [http://tracer.uc3m.es/tws/pso/multimodal.html Multi-modal optimization using Particle Swarm Optimization (PSO)]\n* [http://cs.telhai.ac.il/~ofersh/NichingES/index.htm Niching in Evolution Strategies (ES)]\n* [http://ls11-www.cs.uni-dortmund.de/rudolph/multimodal/start Multimodal optimization page at Chair 11, Computer Science, TU Dortmund University]\n* [http://www.epitropakis.co.uk/ieee-mmo/ IEEE CIS Task Force on Multi-modal Optimization]\n\n{{Optimization algorithms|state=expanded}}\n{{Evolutionary computation}}\n\n[[Category:Cybernetics]]\n[[Category:Evolutionary algorithms]]\n[[Category:Machine learning algorithms]]\n[[Category:Articles containing video clips]]"
    },
    {
      "title": "Evolutionary music",
      "url": "https://en.wikipedia.org/wiki/Evolutionary_music",
      "text": "{{Distinguish|Evolutionary musicology}}\n'''Evolutionary music''' is the audio counterpart to [[evolutionary art]], whereby [[algorithmic music]] is created using an [[evolutionary algorithm]].  The process begins with a [[population]] of individuals which by some means or other produce audio (e.g. a piece, melody, or loop), which is either initialized randomly or based on human-generated music.  Then through the repeated application of computational steps analogous to biological [[Natural selection|selection]], [[Genetic recombination|recombination]] and [[mutation]] the aim is for the produced audio to become more musical.  [[Evolutionary sound synthesis]] is a related technique for generating sounds or [[synthesizer]] instruments.  Evolutionary music is typically generated using an [[interactive evolutionary computation|interactive evolutionary algorithm]] where the [[fitness function]] is the user or audience, as it is difficult to capture the aesthetic qualities of music computationally.  However, research into automated measures of musical quality is also active.  Evolutionary computation techniques have also been applied to [[harmonization]] and [[accompaniment]] tasks.  The most commonly used evolutionary computation techniques are [[genetic algorithm]]s and [[genetic programming]].\n\n==History==\nNEUROGEN ([http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=140338 Gibson & Byrne, 1991]) employed a genetic algorithm to produce and combine musical fragments and a neural network (trained on examples of \"real\" music) to evaluate their fitness.  A genetic algorithm is also a key part of the improvisation and accompaniment system [http://igm.rit.edu/~jabics/GenJam.html GenJam]  which has been developed since 1993 by Al Biles.  Biles and GenJam are together known as the [[Al Biles Virtual Quintet]] and have performed many times to human audiences. Genetic programming has been used to produce music since the work of Lee Spector and Alpern Alpern on evolved bebop musicians in 1994<ref>[http://dl.acm.org/citation.cfm?id=199287 Spector, L., and A. Alpern. 1994. Criticism, Culture, and the Automatic Generation of Artworks. In Proceedings of the Twelfth National Conference on Artificial Intelligence, AAAI-94, pp. 3-8. Menlo Park, CA and Cambridge, MA: AAAI Press/The MIT Press.]</ref> and 1995,<ref>[http://faculty.hampshire.edu/lspector/pubs/IJCAI95mus-toappear.pdf Spector, L., and A. Alpern. 1995. Induction and Recapitulation of Deep Musical Structure. In Working Notes of the IJCAI-95 Workshop on Artificial Intelligence and Music. pp. 41-48.]</ref> and in 1997 Brad Johanson and Riccardo Poli developed the [http://graphics.stanford.edu/~bjohanso/gp-music/ GP-Music System] which used genetic programming to breed melodies according to both human and automated ratings. Since 1996 [[Rodney Waschka II]] has been using genetic algorithms for music composition including works such as ''Saint Ambrose''<ref>[http://www.capstonerecords.org/CPS-8708.html Capstone Records:Rodney Waschka II - Saint Ambrose<!-- Bot generated title -->]</ref> and his string quartets.<ref>[http://www.springerlink.com/content/j1up38mn7205g552/?p=e54526113482447681a3114bed6f5eef&pi=5 SpringerLink - Book Chapter<!-- Bot generated title -->]</ref>  Several systems for drum loop evolution have been produced (including one commercial program called [http://www.geneffects.com/musing/ MuSing]).\n\n==Conferences==\n\nThe EvoMUSART Conference<ref>{{cite web|title=EvoMUSART|url=http://evostar.dei.uc.pt/2012/call-for-contributions/evomusart/}}</ref>  from 2012 (previously a workshop from 2003) was part of the Evo*<ref>{{cite web|title=Evo* (EvoStar)|url=http://www.evostar.org/}}</ref>  event annually from 2003.  This event on evolutionary music and art is one of the main outlets for work on evolutionary music.\n\nAn annual Workshop in Evolutionary Music<ref>{{cite web|title=GECCO workshops|url=http://www.sigevo.org/gecco-2012/workshops.html}}</ref> has been held at GECCO (Genetic and Evolutionary Computation Conference<ref>{{cite web|title=GECCO 2012|url=http://www.sigevo.org/gecco-2012/}}</ref>) since 2011.\n\n==Recent work==\nThe [http://evonet.lri.fr/eurogp2004/songcontest.html EuroGP Song Contest] (a [[pun]] on [[Eurovision Song Contest]]) was held at [http://evonet.lri.fr/eurogp2004/index.html EuroGP 2004].  In this experiment several tens of users were first tested for their ability to recognise musical differences, and then a short piano-based melody was evolved.\n\nAl Biles gave a [http://www.it.rit.edu/~jab/EvoMusic/BilesEvoMusicSlides.pdf tutorial on evolutionary music] at GECCO 2005 and co-edited a [https://www.springer.com/uk/home/generic/search/results?SGWID=3-40109-22-173674005-0 book] on the subject with contributions from many researchers in the field.\n\n[http://askory.phratry.net/projects/evolutune/ Evolutune] is a small [[Microsoft Windows|Windows]] application from 2005 for evolving simple loops of \"beeps and boops\".  It has a graphical interface where the user can select parents manually.\n\nThe  [http://phoenix.inf.upol.cz/~dostal/evm.html GeneticDrummer] is a Genetic Algorithm-based system for generating human-competitive rhythm accompaniment.\n\nThe [http://www.compose-music.com easy Song Builder] is an evolutionary composition program. The user decides which version of the song will be the germ for the next generation.\n\n[http://www.melomics.com Melomics], an artificial intelligence group based in Málaga, Spain, has used evolutionary algorithms to compose full pieces of music in specific genres, creating the first album composed by a computer and performed by human musicians in 2012.<ref>{{cite   journal | title= Computer composer honours Turing's centenary| journal= New Scientist| date =5 July 2012| url =https://www.newscientist.com/article/mg21528724.300-computer-composer-honours-turings-centenary.html}}</ref>  The music is then exported into [[mp3]], [[MIDI]], [[XML]], and [[PDF]] for application by the user.\n\nThe [[DarwinTunes]] project has been running since 2009 (and before that as \"Evolectronica\") - recently a multiplayer game version of DarwinTunes was demonstrated at science festivals<ref>{{Cite web | url=http://www.discoveryfestival.nl/ | title=Experiment / Music / Science / Art / Dance / 2006-2015}}</ref><ref>https://www.theguardian.com/artanddesign/jonathanjonesblog/2014/may/12/darwin-tunes-evolutionary-art-music</ref> and is now available on the web.\n\n==Books==\n* ''Evolutionary Computer Music.'' Miranda, Eduardo Reck; Biles, John Al (Eds.) London: Springer, 2007.<ref>[https://www.springer.com/computer/information+systems/book/978-1-84628-599-8?detailsPage=toc Evolutionary Computer Music - Multimedia Information Systems Journals, Books & Online Media | Springer<!-- Bot generated title -->]</ref> \n* ''The Art of Artificial Evolution: A Handbook on Evolutionary Art and Music'', Juan Romero and Penousal Machado (eds.), 2007, Springer<ref>[http://art-artificial-evolution.dei.uc.pt/ The Art of Artificial Evolution: A Handbook on Evolutionary Art and Music<!-- Bot generated title -->]</ref>\n* ''Creative Evolutionary Systems'' by David W. Corne, Peter J. Bentley<ref>{{cite book|title=Creative evolutionary systems|year=2002|publisher=Morgan Kaufmann|pages=576|url=https://books.google.com/books/about/Creative_evolutionary_systems.html?id=kJTUG2dbbMkC|isbn=9781558606739}}</ref>\n\n==See also==\n* [[Algorithmic composition]]\n* [[Generative music]]\n* [[Evolutionary art]]\n\n==References==\n<references/>\n\n==External links==\n*[http://igm.rit.edu/~jabics/EvoMusic/EvoMusBib.html Al Biles' Evolutionary Music Bibliography] – also includes pointers to work on evolutionary sound synthesis.\n*[http://evolectronica.com Evolectronica] interactive evolving streaming electronic music\n*[http://www.Melomics.com Melomics] official site\n\n[[Category:Electronic music]]\n[[Category:Evolutionary algorithms]]"
    },
    {
      "title": "Evolved antenna",
      "url": "https://en.wikipedia.org/wiki/Evolved_antenna",
      "text": "\n[[Image:St 5-xband-antenna.jpg|thumb|The 2006 NASA [[Space Technology 5|ST5]] spacecraft antenna.  This complicated shape was found by an evolutionary computer design program to create the best radiation pattern.]]\n\nIn [[radio communications]], an '''evolved antenna''' is an [[Antenna (radio)|antenna]] designed fully or substantially by an automatic computer design program that uses an [[Evolutionary computing|evolutionary algorithm]] that mimics [[Darwinism|Darwinian]] [[evolution]].   This sophisticated procedure has been used in recent years to design a few antennas for mission-critical applications involving stringent, conflicting, or unusual design requirements, such as unusual [[radiation pattern]]s, for which none of the many existing antenna types are adequate.\n\n==Process==\nThe computer program starts with simple antenna shapes, then adds or modifies elements in a semirandom manner to create a number of new candidate antenna shapes. These are then evaluated to determine how well they fulfill the design requirements, and a numerical score is computed for each.  Then, in a step similar to [[natural selection]], a portion of the candidate antennas with the worst scores are discarded, leaving a small population of the highest-scoring designs.  Using these antennas, the computer repeats the procedure, generating a population of even higher-scoring designs.  After a number of iterations, the population of antennas is evaluated and the highest-scoring design is chosen. The resulting antenna often outperforms the best manual designs, because it has a complicated asymmetric shape that could not have been found with traditional manual design methods.\n\nThe first evolved antenna designs appeared in the mid-1990s from the work of Michielssen, Altshuler, Linden, Haupt, and Rahmat-Samii.  Most practitioners use the [[genetic algorithm]] technique or some variant thereof to evolve antenna designs.\n\nAn example of an evolved antenna is an [[X-band]] antenna evolved for a 2006 [[NASA]] mission called Space Technology 5 (ST5).<ref name=\"Hornby\">{{cite paper\n  | first = Gregory S.\n  | last =  Hornby\n  | authorlink = \n  |author2=Al Globus |author3=Derek S. Linden |author4=Jason D. Lohn\n   | title = Automated antenna design with evolutionary algorithms\n  | version = \n  | publisher = American Institute of Aeronautics and Astronautics\n  | date = September 2006\n  | url = http://alglobus.net/NASAwork/papers/Space2006Antenna.pdf\n  | format = \n  | accessdate = 2012-02-19}}</ref> The mission consists of three [[satellite]]s that will take measurements in [[Earth]]'s [[magnetosphere]]. Each satellite has two communication antennas to talk to ground stations. The antenna has an unusual structure and was evolved to meet a challenging set of mission requirements, notably the combination of wide [[beamwidth]] for a circularly [[Polarization (waves)|polarized]] wave and wide [[wave impedance|impedance]] bandwidth to cover the up and down link frequencies at X-band. Each spacecraft had two antennas - an evolved unit and a more standard, quadrifilar helix antenna.  Both antennas were fabricated by the Physical Science Laboratory at New Mexico State University.  Their external appearance  was essentially identical in that a foam [[radome]] covered the radiating elements. The ST5 mission successfully launched on March 22, 2006 and operated for the mission period before being decommissioned by NASA, and so this evolved antenna represents the world's first artificially-evolved object to fly in space.  Other evolved antennas were subsequently used on the LADEE spacecraft.\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://ti.arc.nasa.gov/m/pub-archive/1244h/1244%20(Hornby).pdf] at NASA Ames Research Center\n* [https://link.springer.com/chapter/10.1007%2F0-387-23254-0_18?LI=true A paper] given by Lohn, Hornby, and Linden at the [http://cscs.umich.edu/old/calendar/conferences/gptp2004/ Genetic Programming Theory Practice 2004 Workshop].\n\n{{Antenna Types}}\n\n[[Category:Radio frequency antenna types]]\n[[Category:Evolutionary algorithms]]\n[[Category:Antennas (radio)]]"
    },
    {
      "title": "Fitness approximation",
      "url": "https://en.wikipedia.org/wiki/Fitness_approximation",
      "text": "{{No footnotes|date=April 2009}}\n\nIn function [[optimization (mathematics)|optimization]], '''fitness approximation''' is a method for decreasing the number of [[fitness function]] evaluations to reach a target solution. It belongs to the general class of [[evolutionary computation]] or [[artificial evolution]] methodologies.\n\n==Approximate models in function optimization==\n\n===Motivation===\nIn many real-world [[optimization problem]]s including engineering problems, the number of [[fitness function]] evaluations needed to obtain a good solution dominates the [[optimization (mathematics)|optimization]] cost. In order to obtain efficient optimization algorithms, it is crucial to use prior information gained during the optimization process. Conceptually, a natural approach to utilizing the known prior information is building a model of the fitness function to assist in the selection of candidate solutions for evaluation. A variety of techniques for constructing such a model, often also referred to as surrogates, metamodels or [[approximation]] models – for  computationally expensive optimization problems have been considered.\n\n===Approaches===\nCommon approaches to constructing approximate models based on learning and interpolation from known fitness values of a small population include:\n*[[Degree of a polynomial|low-degree]]\n*[[Polynomial]]s and [[regression analysis|regression]] models\n*[[Artificial neural networks]] including\n**[[Multilayer perceptron]]s\n**[[Radial basis function network]]s\n**[[Support vector machines]]\nDue to the limited number of training samples and high dimensionality encountered in engineering design optimization, constructing a globally valid approximate model remains difficult. As a result, evolutionary algorithms using such approximate fitness functions may converge to [[local optima]]. Therefore, it can be beneficial to selectively use the original [[fitness function]] together with the approximate model.\n\n{{Wiktionary|fitness}}\n\n==Adaptive fuzzy fitness granulation==\n{{Notability|date=July 2010}}\n'''Adaptive fuzzy fitness granulation (AFFG)''' is a proposed solution to constructing an approximate model of the fitness function in place of traditional computationally expensive large-scale problem analysis like (L-SPA) in the [[Finite element method]] or iterative fitting of a [[Bayesian network]] structure.\n\nIn adaptive fuzzy fitness granulation, an adaptive pool of solutions, represented by [[fuzzy logic|fuzzy]] granules, with an exactly computed fitness function result is maintained. If a new individual is sufficiently similar to an existing known fuzzy granule, then that granule’s fitness is used instead as an estimate. Otherwise, that individual is added to the pool as a new fuzzy granule. The pool size as well as each granule’s radius of influence is adaptive and will grow/shrink depending on the utility of each granule and the overall population fitness. To encourage fewer function evaluations, each granule’s radius of influence is initially large and is gradually shrunk in latter stages of evolution. This encourages more exact fitness evaluations when competition is fierce among more similar and converging solutions. Furthermore, to prevent the pool from growing too large, granules that are not used are gradually eliminated.\n\nAdditionally, AFFG mirrors two features of human cognition: (a) granularity (b) similarity analysis. This granulation-based fitness approximation scheme is applied to solve various engineering optimization problems including [[Watermark detection|detecting hidden information]] from a [[Digital watermarking|watermarked signal]] in addition to several structural optimization problems.\n{{Portal|Computer Science}}\n\n==See also==\n{{Portal|Artificial intelligence}}\n*[[Surrogate model]]\n\n==References==\n{{Reflist}}\n*[http://behsys.com/mohsen/Fitness-Approximation-Adaptive-Fuzzy-Fitness-Granulation-Evolutionary-Algorithm.html The cyber shack of Adaptive Fuzzy Fitness Granulation (AFFG)] That is designed to accelerate the convergence rate of EAs.\n*[http://www.soft-computing.de/amec_n.html A complete list of references on Fitness Approximation in Evolutionary Computation], by [http://www.soft-computing.de/jin.html Yaochu Jin].\n*[http://genf20plus.info/ 1]\n\n[[Category:Evolutionary algorithms]]\n[[Category:Genetic algorithms]]\n\n[[ca:Funció d'aptitud (algorisme genètic)]]\n[[de:Fitnessfunktion]]\n[[nl:Fitnessfunctie]]\n[[ja:適応度関数]]"
    },
    {
      "title": "Gaussian adaptation",
      "url": "https://en.wikipedia.org/wiki/Gaussian_adaptation",
      "text": "{{Evolutionary algorithms}}\n{{Multiple issues|\n{{refimprove|date=July 2008}}\n{{primary sources|date=July 2008}}\n{{expert-subject|Mathematics|date=January 2015}}\n{{COI|date=March 2009}}\n}}\n\n'''Gaussian adaptation (GA)''' (also referred to as normal or natural adaptation and sometimes abbreviated as NA) is an [[evolutionary algorithm]] designed for the maximization of manufacturing yield due to statistical deviation of component values of [[signal processing]] systems. In short, GA is a stochastic adaptive process where a number of samples of an ''n''-dimensional vector ''x''[''x''<sup>T</sup> = (''x''<sub>1</sub>, ''x''<sub>2</sub>, ..., ''x<sub>n</sub>'')] are taken from a [[multivariate Gaussian distribution]], ''N''(''m'',&nbsp;''M''), having mean ''m'' and [[covariance matrix|moment matrix]] ''M''. The samples are tested for fail or pass. The first- and second-order moments of the Gaussian restricted to the pass samples are ''m*'' and&nbsp;''M*''.\n\nThe outcome of ''x'' as a pass sample is determined by a function ''s''(''x''), 0&nbsp;<&nbsp;''s''(''x'')&nbsp;<&nbsp;''q''&nbsp;≤&nbsp;1, such that ''s''(''x'') is the probability that x will be selected as a pass sample. The average probability of finding pass samples (yield) is\n\n:<math> P(m) = \\int s(x) N(x - m)\\, dx </math>\n\nThen the theorem of GA states:\n\n<blockquote>For any ''s''(''x'') and for any value of ''P&nbsp;''<&nbsp;''q'', there always exist a Gaussian p. d. f. [ [[probability density function]] ] that is adapted for maximum dispersion. The necessary conditions for a local optimum are ''m''&nbsp;=&nbsp;''m''* and ''M'' proportional to ''M''*. The dual problem is also solved: ''P'' is maximized while keeping the dispersion constant (Kjellström, 1991).\n</blockquote>\n\nProofs of the theorem may be found in the papers by Kjellström, 1970, and Kjellström & Taxén, 1981.\n\nSince dispersion is defined as the exponential of entropy/disorder/[[average information]] it immediately follows that the theorem is valid also for those concepts. Altogether, this means that Gaussian adaptation may carry out a simultaneous maximisation of yield and [[average information]] (without any need for the yield or the [[average information]] to be defined as criterion functions).\n\n'''The theorem is valid for all regions of acceptability and all Gaussian distributions'''. It may be used by cyclic repetition of random variation and selection (like the natural evolution). In every cycle a sufficiently large number of Gaussian distributed points are sampled and tested for membership in the region of acceptability. The centre of gravity of the Gaussian, ''m'', is then moved to the centre of gravity of the approved (selected) points, ''m''*. Thus, the process converges to a state of equilibrium fulfilling the theorem. A solution is always approximate because the centre of gravity is always determined for a limited number of points.\n\nIt was used for the first time in 1969 as a pure optimization algorithm making the regions of acceptability smaller and smaller (in analogy to [[simulated annealing]], Kirkpatrick 1983). Since 1970 it has been used for both ordinary optimization and yield maximization.\n\n==Natural evolution and Gaussian adaptation==\nIt has also been compared to the natural evolution of populations of living organisms. In this case ''s''(''x'') is the probability that the individual having an array ''x'' of phenotypes will survive by giving offspring to the next generation; a definition of individual fitness given by Hartl 1981. The yield, ''P'', is replaced by the [[mean fitness]] determined as a mean over the set of individuals in a large population.\n\nPhenotypes are often Gaussian distributed in a large population and a necessary condition for the natural evolution to be able to fulfill the theorem of Gaussian adaptation, with respect to all Gaussian quantitative characters, is that it may push the centre of gravity of the Gaussian to the centre of gravity of the selected individuals. This may be accomplished by the [[Hardy–Weinberg law]]. This is possible because the theorem of Gaussian adaptation is valid for any region of acceptability independent of the structure (Kjellström, 1996).\n\nIn this case the rules of genetic variation such as crossover, inversion, transposition etcetera may be seen as random number generators for the phenotypes. So, in this sense Gaussian adaptation may be seen as a genetic algorithm.\n\n==How to climb a mountain==\nMean fitness may be calculated provided that the distribution of parameters and the structure of the landscape is known. The real landscape is not known, but figure below shows a fictitious profile (blue) of a landscape along a line (x) in a room spanned by such parameters. The red curve is the mean based on the red bell curve at the bottom of  figure. It is obtained by letting the bell curve slide along the ''x''-axis, calculating the mean at every location. As can be seen, small peaks and pits are smoothed out. Thus, if evolution is started at A with a relatively small variance (the red bell curve), then climbing will take place on the red curve. The process may get stuck for millions of years at B or C, as long as the hollows to the right of these points remain, and the mutation rate is too small.\n\n[[Image:Fraktal.gif]]\n\nIf the mutation rate is sufficiently high, the disorder or variance  may increase and the parameter(s) may become distributed like the green bell curve. Then the climbing will take place on the green curve, which is even more smoothed out. Because the hollows to the right of B and C have now disappeared, the process may continue up to the peaks at D. But of course the landscape puts a limit on the disorder or variability. Besides &mdash; dependent on the landscape &mdash; the process may become very jerky, and if the ratio between the time spent by the process at a local peak and the time of transition to the next peak is very high, it may as well look like a [[punctuated equilibrium]] as suggested by Gould (see Ridley).\n\n==Computer simulation of Gaussian adaptation==\nThus far the theory only considers mean values of continuous distributions corresponding to an infinite number of individuals. In reality however, the number of individuals is always limited, which gives rise to an uncertainty in the estimation of ''m'' and ''M'' (the moment matrix of the Gaussian). And this may also affect the efficiency of the process. Unfortunately very little is known about this, at least theoretically.\n\nThe implementation of normal adaptation on a computer is a fairly simple task. The adaptation of m may be done by one sample (individual) at a time, for example\n\n: ''m''(''i'' + 1) = (1 – ''a'') ''m''(''i'') + ''ax''\n\nwhere ''x'' is a pass sample, and ''a'' < 1 a suitable constant so that the inverse of a represents the number of individuals in the population.\n\n''M'' may in principle be updated after every step ''y'' leading to a feasible point\n\n: ''x'' = ''m'' + ''y'' according to:\n\n: ''M''(''i'' + 1) = (1 – 2''b'') ''M''(''i'') + 2''byy''<sup>T</sup>,\n\nwhere ''y''<sup>T</sup> is the transpose of ''y'' and ''b'' << 1 is another suitable constant. In order to guarantee a suitable increase of [[average information]], ''y'' should be [[normal distribution|normally distributed]] with moment matrix ''μ''<sup>2</sup>''M'', where the scalar ''μ'' > 1 is used to increase [[average information]] ([[information entropy]], disorder, diversity) at a suitable rate. But ''M'' will never be used in the calculations. Instead we use the matrix ''W'' defined by ''WW''<sup>T</sup> = ''M''.\n\nThus, we have ''y'' = ''Wg'', where ''g'' is normally distributed with the moment matrix ''μU'', and ''U'' is the unit matrix. ''W'' and ''W''<sup>T</sup> may be updated by the formulas\n\n: ''W'' = (1 – ''b'')''W'' + ''byg''<sup>T</sup>   and   ''W''<sup>''T''</sup> = (1 – ''b'')''W''<sup>T</sup> + ''bgy''<sup>T</sup>\n\nbecause multiplication gives\n\n: ''M'' = (1 – 2''b'')''M'' + 2''byy''<sup>T</sup>,\n\nwhere terms including ''b''<sup>2</sup> have been neglected. Thus, ''M'' will be indirectly adapted with good approximation. In practice it will suffice to update ''W'' only\n\n: ''W''(''i'' + 1)  = (1 – ''b'')''W''(''i'') + ''byg''<sup>T</sup>.\n\nThis is the formula used in a simple 2-dimensional model of a brain satisfying the Hebbian rule of associative learning; see the next section (Kjellström, 1996 and 1999).\n\nThe figure below illustrates the effect of increased [[average information]] in a Gaussian p.d.f. used to climb a mountain Crest (the two lines represent the contour line). Both the red and green cluster have equal mean fitness, about 65%, but the green cluster has a much higher [[average information]] making the green process much more efficient. The effect of this adaptation is not very salient in a 2-dimensional case, but in a high-dimensional case, the efficiency of the search process may be increased by many orders of magnitude.\n\n[[Image:Mountain crest.GIF]]\n\n==The evolution in the brain==\nIn the brain the evolution of DNA-messages is supposed to be replaced by an evolution of signal patterns and the phenotypic landscape is replaced by a mental landscape, the complexity of which will hardly be second to the former. The metaphor with the mental landscape is based on the assumption that certain signal patterns give rise to a better well-being or performance. For instance, the control of a group of muscles leads to a better pronunciation of a word or performance of a piece of music.\n\nIn this simple model it is assumed that the brain consists of interconnected components that may add, multiply and delay signal values.\n* A nerve cell kernel may add signal values,\n* a synapse may multiply with a constant and\n* An axon may delay values.\nThis is a basis of the theory of digital filters and neural networks consisting of components that may add, multiply and delay signalvalues and also of many brain models, Levine 1991.\n\nIn the figure below the brain stem is supposed to deliver Gaussian distributed signal patterns. This may be possible since certain neurons fire at random (Kandel et al.). The stem also constitutes a disordered structure surrounded by more ordered shells (Bergström, 1969), and according to the [[central limit theorem]] the sum of signals from many neurons may be Gaussian distributed. The triangular boxes represent synapses and the boxes with the + sign are cell kernels.\n\nIn the cortex signals are supposed to be tested for feasibility. When a signal is accepted the contact areas in the synapses are updated according to the formulas below in agreement with the Hebbian theory. The figure shows a 2-dimensional computer simulation of Gaussian adaptation according to the last formula in the preceding section.\n\n[[Image:Schematic of a neural network executing the Gaussian adaptation algorithm.GIF]]\n\n''m'' and ''W'' are updated according to:\n\n: ''m''<sub>1</sub> = 0.9 ''m''<sub>1</sub> + 0.1 ''x''1;   ''m''<sub>2</sub> = 0.9 ''m''<sub>2</sub> + 0.1 ''x''<sub>2</sub>;\n\n: ''w''<sub>11</sub> = 0.9 ''w''<sub>11</sub> + 0.1 ''y''<sub>1</sub>''g''<sub>1</sub>;    ''w''<sub>12</sub> = 0.9 ''w''<sub>12</sub> + 0.1 ''y''<sub>1</sub>''g''<sub>2</sub>;\n\n: ''w''<sub>21</sub> = 0.9 ''w''<sub>21</sub> + 0.1 ''y''<sub>2</sub>''g''<sub>1</sub>;    ''w''<sub>22</sub> = 0.9 ''w''<sub>22</sub> + 0.1 ''y''<sub>2</sub>''g''<sub>2</sub>;\n\nAs can be seen this is very much like a small brain ruled by the theory of [[Hebbian learning]] (Kjellström, 1996, 1999 and 2002).\n\n==Gaussian adaptation and free will==\nGaussian adaptation as an evolutionary model of the brain obeying the [[Hebbian theory]] of associative learning offers an alternative view of [[free will]] due to the ability of the process to maximize the mean fitness of signal patterns in the brain by climbing a mental landscape in analogy with phenotypic evolution.\n\nSuch a random process gives us lots of freedom of choice, but hardly any will. An illusion of will may, however, emanate from the ability of the process to maximize mean fitness, making the process goal seeking. I. e., it prefers higher peaks in the landscape prior to lower, or better alternatives prior to worse. In this way an illusive will may appear. A similar view has been given by Zohar 1990. See also Kjellström 1999.\n\n==A theorem of efficiency for random search==\nThe efficiency of Gaussian adaptation relies on the theory of information due to Claude E. Shannon (see [[information content]]). When an event occurs with probability ''P'', then the information &minus;log(''P'') may be achieved. For instance, if the mean fitness is ''P'', the information gained for each individual selected for survival will be &minus;log(''P'') – on the average - and the work/time needed to get  the information is proportional to 1/''P''. Thus, if efficiency, E, is defined as information divided by the work/time needed to get it we have:\n\n: ''E'' = &minus;''P'' log(''P'').\n\nThis function attains its maximum when ''P'' = 1/''e'' = 0.37. The same result has been obtained by Gaines with a different method.\n\n''E'' = 0 if ''P'' = 0, for a process with infinite mutation rate, and if ''P'' = 1, for a process with mutation rate = 0 (provided that the process is alive).\nThis measure of efficiency is valid for a large class of [[random search]] processes provided that certain conditions are at hand.\n\n1  The search should be statistically independent and equally efficient in different parameter directions. This condition may be approximately fulfilled when the moment matrix of the Gaussian has been adapted for maximum [[average information]] to some region of acceptability, because linear transformations of the whole process do not affect efficiency.\n\n2   All individuals have equal cost and the derivative at ''P'' = 1 is <&nbsp;0.\n\nThen, the following theorem may be proved:\n\n<blockquote>All measures of efficiency, that satisfy the conditions above, are asymptotically proportional to –''P'' log(''P/q'') when the number of dimensions increases, and are maximized by ''P'' = ''q'' exp(-1) (Kjellström, 1996 and 1999).</blockquote>\n[[Image:Efficiency.GIF]]\n\nThe figure above shows a possible efficiency function for a random search process such as Gaussian adaptation. To the left the process is most chaotic when ''P'' = 0, while there is perfect order to the right where ''P'' = 1.\n\nIn an example by Rechenberg, 1971, 1973, a random walk is pushed thru a corridor maximizing the parameter ''x''<sub>1</sub>. In this case the region of acceptability is defined as a (''n''&nbsp;&minus;&nbsp;1)-dimensional interval in the parameters ''x''<sub>2</sub>, ''x''<sub>3</sub>, ..., ''x''<sub>''n''</sub>, but a ''x''<sub>1</sub>-value below the last accepted will never be accepted. Since ''P'' can never exceed 0.5 in this case, the maximum speed towards higher ''x''<sub>1</sub>-values is reached for ''P'' = 0.5/''e'' = 0.18, in agreement with the findings of Rechenberg.\n\nA point of view that also may be of interest in this context is that no definition of information (other than that sampled points inside some region of acceptability gives information about the extension of the region) is needed for the proof of the theorem. Then, because, the formula may be interpreted as information divided by the work needed to get the information, this is also an indication that &minus;log(''P'') is a good candidate for being a measure of information.\n\n==The Stauffer and Grimson algorithm==\nGaussian adaptation has also been used for other purposes as for instance shadow removal by \"The Stauffer-Grimson algorithm\" which is equivalent to Gaussian adaptation as used in the section \"Computer simulation of Gaussian adaptation\" above. In both cases the maximum likelihood method is used for estimation of mean values by adaptation at one sample at a time.\n\nBut there are differences. In the Stauffer-Grimson case the information is not used for the control of a random number generator for centering, maximization of mean fitness, [[average information]] or manufacturing yield. The adaptation of the moment matrix also differs very much as compared to \"the evolution in the brain\" above.\n\n==See also==\n* [[Entropy in thermodynamics and information theory]]\n* [[Fisher's fundamental theorem of natural selection]]\n* [[Free will]]\n* [[Genetic algorithm]]\n* [[Hebbian learning]]\n* [[Information content]]\n* [[Simulated annealing]]\n* [[Stochastic optimization]]\n* [[CMA-ES|Covariance matrix adaptation evolution strategy (CMA-ES)]]\n* [[Unit of selection]]\n\n==References==\n*Bergström, R. M. An Entropy Model of the Developing Brain. ''[[Developmental Psychobiology]]'', 2(3): 139–152, 1969.\n*Brooks, D. R. & Wiley, E. O. ''Evolution as Entropy, Towards a unified theory of Biology''. The University of Chicago Press, 1986.\n*Brooks, D. R. Evolution in the Information Age: Rediscovering the Nature of the Organism. Semiosis, Evolution, Energy, Development, Volume 1, Number 1, March 2001\n*Gaines, Brian R. Knowledge Management in Societies of Intelligent Adaptive Agents. ''Journal of intelligent Information systems'' 9, 277–298 (1997).\n*Hartl, D. L. ''A Primer of Population Genetics''. Sinauer, Sunderland, Massachusetts, 1981.\n*Hamilton, WD. 1963. The evolution of altruistic behavior. American Naturalist 97:354–356\n*Kandel, E. R., Schwartz, J. H., Jessel, T. M. ''Essentials of Neural Science and Behavior''. Prentice Hall International, London, 1995.\n*S. Kirkpatrick and C. D. Gelatt and M. P. Vecchi, Optimization by Simulated Annealing, Science, Vol 220, Number 4598, pages 671–680, 1983.\n*Kjellström, G. Network Optimization by Random Variation of component values. ''Ericsson Technics'', vol. 25, no. 3, pp.&nbsp;133–151, 1969.\n*Kjellström, G. Optimization of electrical Networks with respect to Tolerance Costs. ''Ericsson Technics'', no. 3, pp.&nbsp;157–175, 1970.\n*Kjellström, G. & Taxén, L. Stochastic Optimization in System Design. IEEE Trans. on Circ. and Syst., vol. CAS-28, no. 7, July 1981.\n*Kjellström, G., Taxén, L. and Lindberg, P. O. Discrete Optimization of Digital Filters Using Gaussian Adaptation and Quadratic Function Minimization. IEEE Trans. on Circ. and Syst., vol. CAS-34, no 10, October 1987.\n*Kjellström, G. On the Efficiency of Gaussian Adaptation. ''Journal of Optimization Theory and Applications'', vol. 71, no. 3, December 1991.\n*Kjellström, G. & Taxén, L. Gaussian Adaptation, an evolution-based efficient global optimizer; Computational and Applied Mathematics, In, C. Brezinski & U. Kulish (Editors), Elsevier Science Publishers B. V., pp 267–276, 1992.\n*Kjellström, G. Evolution as a statistical optimization algorithm. ''Evolutionary Theory'' 11:105–117 (January, 1996).\n*Kjellström, G. The evolution in the brain. ''Applied Mathematics and Computation'', 98(2–3):293–300, February, 1999.\n*Kjellström, G. Evolution in a nutshell and some consequences concerning valuations. EVOLVE, {{ISBN|91-972936-1-X}}, Stockholm, 2002.\n*Levine, D. S. Introduction to Neural & Cognitive Modeling. Laurence Erlbaum Associates, Inc., Publishers, 1991.\n*MacLean, P. D. ''A Triune Concept of the Brain and Behavior''. Toronto, Univ. Toronto Press, 1973.\n*Maynard Smith, J. 1964. Group Selection and Kin Selection, Nature 201:1145–1147.\n*Maynard Smith, J. ''Evolutionary Genetics''. Oxford University Press, 1998.\n*Mayr, E. ''What Evolution is''. Basic Books, New York, 2001.\n*Müller, Christian L. and Sbalzarini Ivo F. Gaussian Adaptation revisited - an entropic view on Covariance Matrix Adaptation. Institute of Theoretical Computer Science and [[Swiss Institute of Bioinformatics]], [[ETH Zurich]], CH-8092 Zurich, Switzerland.\n*Pinel, J. F. and Singhal, K. Statistical Design Centering and Tolerancing Using Parametric Sampling. IEEE Transactions on Circuits and Systems, Vol. Das-28, No. 7, July 1981.\n*Rechenberg, I. (1971): Evolutionsstrategie &mdash; Optimierung technischer Systeme nach Prinzipien der biologischen Evolution (PhD thesis). Reprinted by Fromman-Holzboog (1973).\n*Ridley, M. ''Evolution''. Blackwell Science, 1996.\n*Stauffer, C. & Grimson, W.E.L. Learning Patterns of Activity Using Real-Time Tracking, IEEE Trans. on PAMI, 22(8), 2000.\n*Stehr, G. On the Performance Space Exploration of Analog Integrated Circuits. Technischen Universität Munchen, Dissertation 2005.\n*Taxén, L. A Framework for the Coordination of Complex Systems’ Development. Institute of Technology, Linköping University, Dissertation, 2003.\n*Zohar, D. ''The quantum self : a revolutionary view of human nature and consciousness rooted in the new physics''. London, Bloomsbury, 1990.\n\n{{DEFAULTSORT:Gaussian Adaptation}}\n[[Category:Evolutionary algorithms]]\n[[Category:Creativity]]\n[[Category:Creationism]]\n[[Category:Free will]]"
    },
    {
      "title": "Gene expression programming",
      "url": "https://en.wikipedia.org/wiki/Gene_expression_programming",
      "text": "{{Short description|evolutionary algorithm}}\n{{Use dmy dates|date=September 2017}}\n{{COI|date=November 2012}}\nIn [[computer programming]], '''gene expression programming (GEP)''' is an [[evolutionary algorithm]] that creates computer programs or models. These computer programs are complex [[tree structure]]s that learn and adapt by changing their sizes, shapes, and composition, much like a living organism. And like living organisms, the computer programs of GEP are also encoded in simple linear [[chromosome]]s of fixed length. Thus, GEP is a [[Genotype–phenotype distinction|genotype–phenotype system]], benefiting from a simple [[genotype|genome]] to keep and transmit the genetic information and a complex [[phenotype]] to explore the environment and adapt to it.\n\n==Background==\n[[Evolutionary algorithms]] use populations of individuals, select individuals according to fitness, and introduce genetic variation using one or more [[genetic operators]]. Their use in artificial computational systems dates back to the 1950s where they were used to solve optimization problems (e.g. Box 1957<ref>Box, G. E. P., 1957. Evolutionary operation: A method for increasing industrial productivity. Applied Statistics, 6, 81–101.</ref> and Friedman 1959<ref>Friedman, G. J., 1959. Digital simulation of an evolutionary process. General Systems Yearbook, 4, 171–184.</ref>). But it was with the introduction of [[evolution strategies]] by Rechenberg in 1965<ref>{{cite book|last=Rechenberg|first=Ingo|year=1973|title=Evolutionsstrategie|place=Stuttgart|publisher=Holzmann-Froboog|isbn=3-7728-0373-3}}</ref> that evolutionary algorithms gained popularity. A good overview text on evolutionary algorithms is the book \"An Introduction to Genetic Algorithms\" by Mitchell (1996).<ref>{{cite book|last= Mitchell|first= Melanie|year=1996|title='An Introduction to Genetic Algorithms|place= Cambridge, MA|publisher= MIT Press}}</ref>\n\nGene expression programming<ref>{{cite web|last=Ferreira|first=C.|year=2001|title=Gene Expression Programming: A New Adaptive Algorithm for Solving Problems|url= http://www.gene-expression-programming.com/webpapers/GEP.pdf|publisher= Complex Systems, Vol. 13, issue 2: 87–129}}</ref> belongs to the family of [[evolutionary algorithm]]s and is closely related to [[genetic algorithms]] and [[genetic programming]]. From genetic algorithms it inherited the linear chromosomes of fixed length; and from genetic programming it inherited the expressive [[parse tree]]s of varied sizes and shapes.\n\nIn gene expression programming the linear chromosomes work as the genotype and the parse trees as the phenotype, creating a [[Genotype-phenotype distinction|genotype/phenotype system]]. This genotype/phenotype system is [[Gene|multigenic]], thus encoding multiple parse trees in each chromosome. This means that the computer programs created by GEP are composed of multiple parse trees. Because these parse trees are the result of gene expression, in GEP they are called [[expression tree]]s.\n\n==Encoding: the genotype==\nThe genome of gene expression programming consists of a linear, symbolic string or chromosome of fixed length composed of one or more genes of equal size. These genes, despite their fixed length, code for expression trees of different sizes and shapes. An example of a chromosome with two genes, each of size 9, is the string (position zero indicates the start of each gene):\n\n:<code><nowiki>012345678012345678</nowiki></code>\n:\n:<code><nowiki>L+a-baccd**cLabacd</nowiki></code>\n\nwhere “L” represents the natural logarithm function and “a”, “b”, “c”, and “d” represent the variables and constants used in a problem.\n\n==Expression trees: the phenotype==\nAs shown [[gene expression programming#Encoding: the genotype|above]], the genes of gene expression programming have all the same size. However, these fixed length strings code for [[expression tree]]s of different sizes. This means that the size of the coding regions varies from gene to gene, allowing for adaptation and evolution to occur smoothly.\n\nFor example, the mathematical expression:\n:<math>\\sqrt{(a-b)(c+d)} \\, </math>\ncan also be represented as an expression tree:\n\n{| align=\"center\" border=\"0\" cellpadding=\"4\" cellspacing=\"0\"\n| [[File:GEP expression tree, k-expression Q*-+abcd.png]]\n|}\n\nwhere \"Q” represents the square root function.\n\nThis kind of expression tree consists of the phenotypic expression of GEP genes, whereas the genes are linear strings encoding these complex structures. For this particular example, the linear string corresponds to:\n\n:<code><nowiki>01234567</nowiki></code>\n:\n:<code><nowiki>Q*-+abcd</nowiki></code>\n\nwhich is the straightforward reading of the expression tree from top to bottom and from left to right. These linear strings are called k-expressions (from [[Karva notation]]).\n\nGoing from k-expressions to expression trees is also very simple. For example, the following k-expression:\n\n:<code><nowiki>01234567890</nowiki></code>\n:\n:<code><nowiki>Q*b**+baQba</nowiki></code>\n\nis composed of two different terminals (the variables “a” and “b”), two different functions of two arguments (“*” and “+”), and a function of one argument (“Q”). Its expression gives:\n\n{| align=\"center\" border=\"0\" cellpadding=\"4\" cellspacing=\"0\"\n| [[File:GEP expression tree, k-expression Q*b**+baQba.png]]\n|}\n\n==K-expressions and genes==\nThe k-expressions of gene expression programming correspond to the region of genes that gets expressed. This means that there might be sequences in the genes that are not expressed, which is indeed true for most genes. The reason for these noncoding regions is to provide a buffer of terminals so that all k-expressions encoded in GEP genes correspond always to valid programs or expressions.\n\nThe genes of gene expression programming are therefore composed of two different domains – a head and a tail – each with different properties and functions. The head is used mainly to encode the functions and variables chosen to solve the problem at hand, whereas the tail, while also used to encode the variables, provides essentially a reservoir of terminals to ensure that all programs are error-free.\n\nFor GEP genes the length of the tail is given by the formula:\n\n:<math>t = h(n_\\max-1)+1</math>\n\nwhere ''h'' is the head’s length and ''n''<sub>max</sub> is maximum arity. For example, for a gene created using the set of functions F = {Q, +, −, *, /} and the set of terminals T = {a, b}, ''n''<sub>max</sub> = 2. And if we choose a head length of 15, then ''t'' = 15 (2–1) + 1 = 16, which gives a gene length ''g'' of 15 + 16 = 31. The randomly generated string below is an example of one such gene:\n\n:<code><nowiki>0123456789012345678901234567890</nowiki></code>\n:\n:<code><nowiki>*b+a-aQab+//+b+babbabbbababbaaa</nowiki></code>\n\nIt encodes the expression tree:\n\n{| align=\"center\" border=\"0\" cellpadding=\"4\" cellspacing=\"0\"\n| [[File:GEP expression tree, k-expression *b+a-aQa.png]]\n|}\n\nwhich, in this case, only uses 8 of the 31 elements that constitute the gene.\n\nIt's not hard to see that, despite their fixed length, each gene has the potential to code for expression trees of different sizes and shapes, with the simplest composed of only one node (when the first element of a gene is a terminal) and the largest composed of as many nodes as there are elements in the gene (when all the elements in the head are functions with maximum arity).\n\nIt's also not hard to see that it is trivial to implement all kinds of [[genetic operator|genetic modification]] ([[mutation (genetic algorithm)|mutation]], inversion, insertion, [[crossover (genetic algorithm)|recombination]], and so on) with the guarantee that all resulting offspring encode correct, error-free programs.\n\n==Multigenic chromosomes==\nThe chromosomes of gene expression programming are usually composed of more than one gene of equal length. Each gene codes for a sub-expression tree (sub-ET) or sub-program. Then the sub-ETs can interact with one another in different ways, forming a more complex program. The figure shows an example of a program composed of three sub-ETs.\n\n[[File:Expression of 3 GEP genes, 1st k-expression *Qb+*-bbba.png|thumb|Expression of GEP genes as sub-ETs. a) A three-genic chromosome with the tails shown in bold. b) The sub-ETs encoded by each gene.]]\n\nIn the final program the sub-ETs could be linked by addition or some other function, as there are no restrictions to the kind of linking function one might choose. Some examples of more complex linkers include taking the average, the median, the midrange, thresholding their sum to make a binomial classification, applying the sigmoid function to compute a probability, and so on. These linking functions are usually chosen a priori for each problem, but they can also be evolved elegantly and efficiently by the [[gene expression programming#Cells and code reuse|cellular system]]<ref>{{cite web|last=Ferreira|first=C.|title=Gene Expression Programming: Mathematical Modeling by an Artificial Intelligence|url=http://www.gene-expression-programming.com/GepBook/Introduction.htm|publisher=Angra do Heroismo |location=Portugal|year=2002|isbn=972-95890-5-4}}</ref><ref>{{cite web|last=Ferreira|first=C.|year=2006|title=Automatically Defined Functions in Gene Expression Programming|url= http://www.gene-expression-programming.com/webpapers/Ferreira-GSP2006.pdf|publisher= In N. Nedjah, L. de M. Mourelle, A. Abraham, eds., ''Genetic Systems Programming: Theory and Experiences, Studies in Computational Intelligence'', Vol. 13, pp. 21–56, Springer-Verlag}}</ref> of gene expression programming.\n\n==Cells and code reuse==\nIn gene expression programming, [[gene expression programming#Homeotic genes and the cellular system|homeotic genes]] control the interactions of the different sub-ETs or modules of the main program. The expression of such genes results in different main programs or cells, that is, they determine which genes are expressed in each cell and how the sub-ETs of each cell interact with one another. In other words, homeotic genes determine which sub-ETs are called upon and how often in which main program or cell and what kind of connections they establish with one another.\n\n===Homeotic genes and the cellular system===\nHomeotic genes have exactly the same kind of structural organization as normal genes and they are built using an identical process. They also contain a head domain and a tail domain, with the difference that the heads contain now linking functions and a special kind of terminals – genic terminals – that represent the normal genes. The expression of the normal genes results as usual in different sub-ETs, which in the cellular system are called ADFs (automatically defined functions). As for the tails, they contain only genic terminals, that is, derived features generated on the fly by the algorithm.\n\nFor example, the chromosome in the figure has three normal genes and one homeotic gene and encodes a main program that invokes three different functions a total of four times, linking them in a particular way.\n\n[[File:Expression of a unicellular GEP system with three ADFs.png|thumb|Expression of a unicellular system with three ADFs. a) The chromosome composed of three conventional genes and one homeotic gene (shown in bold). b) The ADFs encoded by each conventional gene. c) The main program or cell.]]\n\nFrom this example it is clear that the cellular system not only allows the unconstrained evolution of linking functions but also code reuse. And it shouldn't be hard to implement [[Recursion (computer science)|recursion]] in this system.\n\n===Multiple main programs and multicellular systems===\nMulticellular systems are composed of more than one [[gene expression programming#Homeotic genes and the cellular system|homeotic gene]]. Each homeotic gene in this system puts together a different combination of sub-expression trees or ADFs, creating multiple cells or main programs.\n\nFor example, the program shown in the figure was created using a cellular system with two cells and three normal genes.\n\n[[File:Expression of a multicellular GEP system with 3 ADFs and 2 main programs.png|thumb|Expression of a multicellular system with three ADFs and two main programs. a) The chromosome composed of three conventional genes and two homeotic genes (shown in bold). b) The ADFs encoded by each conventional gene. c) Two different main programs expressed in two different cells.]]\n\nThe applications of these multicellular systems are multiple and varied and, like the [[gene expression programming#Multigenic chromosomes|multigenic systems]], they can be used both in problems with just one output and in problems with multiple outputs.\n\n==Other levels of complexity==\nThe head/tail domain of GEP genes (both normal and homeotic) is the basic building block of all GEP algorithms. However, gene expression programming also explores other chromosomal organizations that are more complex than the head/tail structure. Essentially these complex structures consist of functional units or genes with a basic head/tail domain plus one or more extra domains. These extra domains usually encode random numerical constants that the algorithm relentlessly fine-tunes in order to find a good solution. For instance, these numerical constants may be the weights or factors in a function approximation problem (see the [[gene expression programming#The GEP-RNC algorithm|GEP-RNC algorithm]] below); they may be the weights and thresholds of a neural network (see the [[gene expression programming#Neural networks|GEP-NN algorithm]] below); the numerical constants needed for the design of decision trees (see the [[gene expression programming#Decision trees|GEP-DT algorithm]] below); the weights needed for polynomial induction; or the random numerical constants used to discover the parameter values in a parameter optimization task.\n\n==The basic gene expression algorithm==\nThe fundamental steps of the basic gene expression algorithm are listed below in pseudocode:\n:1.\tSelect function set;\n:2.\tSelect terminal set;\n:3.\tLoad dataset for fitness evaluation;\n:4.\tCreate chromosomes of initial population randomly;\n:5.\tFor each program in population:\n::a)\tExpress chromosome;\n::b)\tExecute program;\n::c)\tEvaluate fitness;\n:6.\tVerify stop condition;\n:7.\tSelect programs;\n:8.\tReplicate selected programs to form the next population;\n:9.\tModify chromosomes using genetic operators;\n:10.\tGo to step 5.\nThe first four steps prepare all the ingredients that are needed for the iterative loop of the algorithm (steps 5 through 10). Of these preparative steps, the crucial one is the creation of the initial population, which is created randomly using the elements of the function and terminal sets.\n\n===Populations of programs===\nLike all evolutionary algorithms, gene expression programming works with populations of individuals, which in this case are computer programs. Therefore, some kind of initial population must be created to get things started. Subsequent populations are descendants, via [[gene expression programming#Selection and elitism|selection]] and [[gene expression programming#Reproduction with modification|genetic modification]], of the initial population.\n\nIn the genotype/phenotype system of gene expression programming, it is only necessary to create the simple linear chromosomes of the individuals without worrying about the structural soundness of the programs they code for, as their expression always results in syntactically correct programs.\n\n===Fitness functions and the selection environment===\nFitness functions and selection environments (called training datasets in [[machine learning]]) are the two facets of fitness and are therefore intricately connected. Indeed, the fitness of a program depends not only on the [[Loss function|cost function]] used to measure its performance but also on the training data chosen to evaluate fitness\n\n====The selection environment or training data====\nThe selection environment consists of the set of training records, which are also called fitness cases. These fitness cases could be a set of observations or measurements concerning some problem, and they form what is called the training dataset.\n\nThe quality of the training data is essential for the evolution of good solutions. A good training set should be representative of the problem at hand and also well-balanced, otherwise the algorithm might get stuck at some local optimum. In addition, it is also important to avoid using unnecessarily large datasets for training as this will slow things down unnecessarily. A good rule of thumb is to choose enough records for training to enable a good generalization in the validation data and leave the remaining records for validation and testing.\n\n====Fitness functions====\nBroadly speaking, there are essentially three different kinds of problems based on the kind of prediction being made:\n:1.\tProblems involving numeric (continuous) predictions;\n:2.\tProblems involving categorical or nominal predictions, both binomial and multinomial;\n:3.\tProblems involving binary or Boolean predictions.\nThe first type of problem goes by the name of [[Regression analysis|regression]]; the second is known as [[Statistical classification|classification]], with [[logistic regression]] as a special case where, besides the crisp classifications like \"Yes\" or \"No\", a probability is also attached to each outcome; and the last one is related to [[Boolean algebra (logic)|Boolean algebra]] and [[logic synthesis]].\n\n=====Fitness functions for regression=====\nIn [[Regression analysis|regression]], the response or dependent variable is numeric (usually continuous) and therefore the output of a regression model is also continuous. So it's quite straightforward to evaluate the fitness of the evolving models by comparing the output of the model to the value of the response in the training data.\n\nThere are several basic [[fitness function]]s for evaluating model performance, with the most common being based on the error or residual between the model output and the actual value. Such functions include the [[mean squared error]], [[root mean squared error]], [[mean absolute error]], relative squared error, root relative squared error, relative absolute error, and others.\n\nAll these standard measures offer a fine granularity or smoothness to the solution space and therefore work very well for most applications. But some problems might require a coarser evolution, such as determining if a prediction is within a certain interval, for instance less than 10% of the actual value. However, even if one is only interested in counting the hits (that is, a prediction that is within the chosen interval), making populations of models evolve based on just the number of hits each program scores is usually not very efficient due to the coarse granularity of the [[fitness landscape]]. Thus the solution usually involves combining these coarse measures with some kind of smooth function such as the standard error measures listed above.\n\nFitness functions based on the [[Pearson product-moment correlation coefficient|correlation coefficient]] and [[R-square]] are also very smooth. For regression problems, these functions work best by combining them with other measures because, by themselves, they only tend to measure [[Correlation and dependence|correlation]], not caring for the range of values of the model output. So by combining them with functions that work at approximating the range of the target values, they form very efficient fitness functions for finding models with good correlation and good fit between predicted and actual values.\n\n=====Fitness functions for classification and logistic regression=====\nThe design of fitness functions for [[Statistical classification|classification]] and [[logistic regression]] takes advantage of three different characteristics of classification models. The most obvious is just counting the hits, that is, if a record is classified correctly it is counted as a hit. This fitness function is very simple and works well for simple problems, but for more complex problems or datasets highly unbalanced it gives poor results.\n\nOne way to improve this type of hits-based fitness function consists of expanding the notion of correct and incorrect classifications. In a binary classification task, correct classifications can be 00 or 11. The \"00\" representation means that a negative case (represented by \"0”) was correctly classified, whereas the \"11\" means that a positive case (represented by \"1”) was correctly classified. Classifications of the type \"00\" are called true negatives (TN) and \"11\" true positives (TP).\n\nThere are also two types of incorrect classifications and they are represented by 01 and 10. They are called false positives (FP) when the actual value is 0 and the model predicts a 1; and false negatives (FN) when the target is 1 and the model predicts a 0. The counts of TP, TN, FP, and FN are usually kept on a table known as the [[confusion matrix]].\n\n[[File:Binary confusion matrix.png|thumb|[[Confusion matrix]] for a binomial classification task.]]\n\nSo by counting the TP, TN, FP, and FN and further assigning different weights to these four types of classifications, it is possible to create smoother and therefore more efficient fitness functions. Some popular fitness functions based on the confusion matrix include [[Sensitivity and specificity|sensitivity/specificity]], [[Precision and recall|recall/precision]], [[F-measure]], [[Jaccard similarity]], [[Matthews correlation coefficient]], and cost/gain matrix which combines the costs and gains assigned to the 4 different types of classifications.\n\nThese functions based on the confusion matrix are quite sophisticated and are adequate to solve most problems efficiently. But there is another dimension to classification models which is key to exploring more efficiently the solution space and therefore results in the discovery of better classifiers. This new dimension involves exploring the structure of the model itself, which includes not only the domain and range, but also the distribution of the model output and the classifier margin.\n\nBy exploring this other dimension of classification models and then combining the information about the model with the confusion matrix, it is possible to design very sophisticated fitness functions that allow the smooth exploration of the solution space. For instance, one can combine some measure based on the confusion matrix with the [[mean squared error]] evaluated between the raw model outputs and the actual values. Or combine the [[F-measure]] with the [[R-square]] evaluated for the raw model output and the target; or the cost/gain matrix with the [[Pearson product-moment correlation coefficient|correlation coefficient]], and so on. More exotic fitness functions that explore model granularity include the area under the [[Receiver operating characteristic|ROC curve]] and rank measure.\n\nAlso related to this new dimension of classification models, is the idea of assigning probabilities to the model output, which is what is done in [[logistic regression]]. Then it is also possible to use these probabilities and evaluate the [[mean squared error]] (or some other similar measure) between the probabilities and the actual values, then combine this with the confusion matrix to create very efficient fitness functions for logistic regression. Popular examples of fitness functions based on the probabilities include [[maximum likelihood estimation]] and [[hinge loss]].\n\n=====Fitness functions for Boolean problems=====\nIn logic there is no model structure (as defined [[gene expression programming#Fitness functions for classification and logistic regression|above]] for classification and logistic regression) to explore: the domain and range of logical functions comprises only 0’s and 1’s or false and true. So, the fitness functions available for [[Boolean algebra (logic)|Boolean algebra]] can only be based on the hits or on the confusion matrix as explained in the section [[gene expression programming#Fitness functions for classification and logistic regression|above]].\n\n===Selection and elitism===\n[[Roulette-wheel selection]] is perhaps the most popular selection scheme used in evolutionary computation. It involves mapping the fitness of each program to a slice of the roulette wheel proportional to its fitness. Then the roulette is spun as many times as there are programs in the population in order to keep the population size constant. So, with roulette-wheel selection programs are selected both according to fitness and the luck of the draw, which means that some times the best traits might be lost. However, by combining roulette-wheel selection with the cloning of the best program of each generation, one guarantees that at least the very best traits are not lost. This technique of cloning the best-of-generation program is known as simple elitism and is used by most stochastic selection schemes.\n\n===Reproduction with modification===\nThe reproduction of programs involves first the selection and then the reproduction of their genomes. Genome modification is not required for reproduction, but without it adaptation and evolution won't take place.\n\n====Replication and selection====\nThe selection operator selects the programs for the replication operator to copy. Depending on the selection scheme, the number of copies one program originates may vary, with some programs getting copied more than once while others are copied just once or not at all. In addition, selection is usually set up so that the population size remains constant from one generation to another.\n\nThe replication of genomes in nature is very complex and it took scientists a long time to discover the [[DNA double helix]] and propose a mechanism for its replication. But the replication of strings is trivial in artificial evolutionary systems, where only an instruction to copy strings is required to pass all the information in the genome from generation to generation.\n\nThe replication of the selected programs is a fundamental piece of all artificial evolutionary systems, but for evolution to occur it needs to be implemented not with the usual precision of a copy instruction, but rather with a few errors thrown in. Indeed, genetic diversity is created with [[genetic operator]]s such as [[Mutation (genetic algorithm)|mutation]], [[Crossover (genetic algorithm)|recombination]], [[Transposition (genetics)|transposition]], inversion, and many others.\n\n====Mutation====\nIn gene expression programming mutation is the most important genetic operator.<ref>{{cite web|last=Ferreira|first=C.|year=2002|title=Mutation, Transposition, and Recombination: An Analysis of the Evolutionary Dynamics|url= http://www.gene-expression-programming.com/webpapers/ferreira-fea02.pdf|publisher=  In H. J. Caulfield, S.-H. Chen, H.-D. Cheng, R. Duro, V. Honavar, E. E. Kerre, M. Lu, M. G. Romay, T. K. Shih, D. Ventura, P. P. Wang, Y. Yang, eds., Proceedings of the 6th Joint Conference on Information Sciences, 4th International Workshop on Frontiers in Evolutionary Algorithms, pages 614–617, Research Triangle Park, North Carolina, USA}}</ref> It changes genomes by changing an element by another. The accumulation of many small changes over time can create great diversity.\n\nIn gene expression programming mutation is totally unconstrained, which means that in each gene domain any domain symbol can be replaced by another. For example, in the heads of genes any function can be replaced by a terminal or another function, regardless of the number of arguments in this new function; and a terminal can be replaced by a function or another terminal.\n\n====Recombination====\n[[Crossover (genetic algorithm)|Recombination]] usually involves two parent chromosomes to create two new chromosomes by combining different parts from the parent chromosomes. And as long as the parent chromosomes are aligned and the exchanged fragments are homologous (that is, occupy the same position in the chromosome), the new chromosomes created by recombination will always encode syntactically correct programs.\n\nDifferent kinds of crossover are easily implemented either by changing the number of parents involved (there's no reason for choosing only two); the number of split points; or the way one chooses to exchange the fragments, for example, either randomly or in some orderly fashion. For example, gene recombination, which is a special case of recombination, can be done by exchanging homologous genes (genes that occupy the same position in the chromosome) or by exchanging genes chosen at random from any position in the chromosome.\n\n====Transposition====\n[[Transposition (genetics)|Transposition]] involves the introduction of an insertion sequence somewhere in a chromosome. In gene expression programming insertion sequences might appear anywhere in the chromosome, but they are only inserted in the heads of genes. This method guarantees that even insertion sequences from the tails result in error-free programs.\n\nFor transposition to work properly, it must preserve chromosome length and gene structure. So, in gene expression programming transposition can be implemented using two different methods: the first creates a shift at the insertion site, followed by a deletion at the end of the head; the second overwrites the local sequence at the target site and therefore is easier to implement. Both methods can be implemented to operate between chromosomes or within a chromosome or even within a single gene.\n\n====Inversion====\nInversion is an interesting operator, especially powerful for combinatorial optimization.<ref>{{cite web|last=Ferreira|first=C.|year=2002|title=Combinatorial Optimization by Gene Expression Programming: Inversion Revisited|url= http://www.gene-expression-programming.com/webpapers/Ferreira-ASAI02.pdf|publisher= In J. M. Santos and A. Zapico, eds., Proceedings of the Argentine Symposium on Artificial Intelligence, pages 160–174, Santa Fe, Argentina}}</ref> It consists of inverting a small sequence within a chromosome.\n\nIn gene expression programming it can be easily implemented in all gene domains and, in all cases, the offspring produced is always syntactically correct. For any gene domain, a sequence (ranging from at least two elements to as big as the domain itself) is chosen at random within that domain and then inverted.\n\n====Other genetic operators====\nSeveral other genetic operators exist and in gene expression programming, with its different genes and gene domains, the possibilities are endless. For example, genetic operators such as one-point recombination, two-point recombination, gene recombination, uniform recombination, gene transposition, root transposition, domain-specific mutation, domain-specific inversion, domain-specific transposition, and so on, are easily implemented and widely used.\n\n==The GEP-RNC algorithm==\nNumerical constants are essential elements of mathematical and statistical models and therefore it is important to allow their integration in the models designed by evolutionary algorithms.\n\nGene expression programming solves this problem very elegantly through the use of an extra gene domain – the Dc – for handling random numerical constants (RNC). By combining this domain with a special terminal placeholder for the RNCs, a richly expressive system can be created.\n\nStructurally, the Dc comes after the tail, has a length equal to the size of the tail ''t'', and is composed of the symbols used to represent the RNCs.\n\nFor example, below is shown a simple chromosome composed of only one gene a head size of 7 (the Dc stretches over positions 15–22):\n\n:<code><nowiki>01234567890123456789012</nowiki></code>\n:\n:<code><nowiki>+?*+?**aaa??aaa68083295</nowiki></code>\n\nwhere the terminal \"?” represents the placeholder for the RNCs. This kind of chromosome is expressed exactly as shown [[gene expression programming#Expression trees: the phenotype|above]], giving:\n\n{| align=\"center\" border=\"0\" cellpadding=\"4\" cellspacing=\"0\"\n| [[File:GEP expression tree with placeholder for RNCs.png]]\n|}\n\nThen the ?'s in the expression tree are replaced from left to right and from top to bottom by the symbols (for simplicity represented by numerals) in the Dc, giving:\n\n{| align=\"center\" border=\"0\" cellpadding=\"4\" cellspacing=\"0\"\n| [[File:GEP expression tree with symbols (numerals) for RNCs.png]]\n|}\n\nThe values corresponding to these symbols are kept in an array. (For simplicity, the number represented by the numeral indicates the order in the array.) For instance, for the following 10 element array of RNCs:\n\n:C = {0.611, 1.184, 2.449, 2.98, 0.496, 2.286, 0.93, 2.305, 2.737, 0.755}\n\nthe expression tree above gives:\n\n{| align=\"center\" border=\"0\" cellpadding=\"4\" cellspacing=\"0\"\n| [[File:GEP expression tree with RNCs.png]]\n|}\n\nThis elegant structure for handling random numerical constants is at the heart of different GEP systems, such as [[gene expression programming#Neural networks|GEP neural networks]] and [[gene expression programming#Decision trees|GEP decision trees]].\n\nLike the [[gene expression programming#The basic gene expression algorithm|basic gene expression algorithm]], the GEP-RNC algorithm is also multigenic and its chromosomes are decoded as usual by expressing one gene after another and then linking them all together by the same kind of linking process.\n\nThe genetic operators used in the GEP-RNC system are an extension to the genetic operators of the basic GEP algorithm (see [[gene expression programming#Reproduction with modification|above]]), and they all can be straightforwardly implemented in these new chromosomes. On the other hand, the basic operators of mutation, inversion, transposition, and recombination are also used in the GEP-RNC algorithm. Furthermore, special Dc-specific operators such as mutation, inversion, and transposition, are also used to aid in a more efficient circulation of the RNCs among individual programs. In addition, there is also a special mutation operator that allows the permanent introduction of variation in the set of RNCs. The initial set of RNCs is randomly created at the beginning of a run, which means that, for each gene in the initial population, a specified number of numerical constants, chosen from a certain range, are randomly generated. Then their circulation and mutation is enabled by the genetic operators.\n\n==Neural networks==\nAn [[artificial neural network]] (ANN or NN) is a computational device that consists of many simple connected units or neurons. The connections between the units are usually weighted by real-valued weights. These weights are the primary means of learning in neural networks and a learning algorithm is usually used to adjust them.\n\nStructurally, a neural network has three different classes of units: input units, hidden units, and output units. An activation pattern is presented at the input units and then spreads in a forward direction from the input units through one or more layers of hidden units to the output units. The activation coming into one unit from other unit is multiplied by the weights on the links over which it spreads. All incoming activation is then added together and the unit becomes activated only if the incoming result is above the unit’s threshold.\n\nIn summary, the basic components of a neural network are the units, the connections between the units, the weights, and the thresholds. So, in order to fully simulate an artificial neural network one must somehow encode these components in a linear chromosome and then be able to express them in a meaningful way.\n\nIn GEP neural networks (GEP-NN or GEP nets), the network architecture is encoded in the usual structure of a head/tail domain.<ref>{{cite web|last=Ferreira|first=C.|year=2006|title=Designing Neural Networks Using Gene Expression Programming|url= http://www.gene-expression-programming.com/webpapers/Ferreira-ASCT2006.pdf|publisher= In A. Abraham, B. de Baets, M. Köppen, and B. Nickolay, eds., Applied Soft Computing Technologies: The Challenge of Complexity, pages 517–536, Springer-Verlag}}</ref> The head contains special functions/neurons that activate the hidden and output units (in the GEP context, all these units are more appropriately called functional units) and terminals that represent the input units. The tail, as usual, contains only terminals/input units.\n\nBesides the head and the tail, these neural network genes contain two additional domains, Dw and Dt, for encoding the weights and thresholds of the neural network. Structurally, the Dw comes after the tail and its length ''d<sub>w</sub>'' depends on the head size ''h'' and maximum arity ''n''<sub>max</sub> and is evaluated by the formula:\n\n:<math>d_{w} = hn_\\max</math>\n\nThe Dt comes after Dw and has a length ''d<sub>t</sub>'' equal to ''t''. Both domains are composed of symbols representing the weights and thresholds of the neural network.\n\nFor each NN-gene, the weights and thresholds are created at the beginning of each run, but their circulation and adaptation are guaranteed by the usual genetic operators of [[gene expression programming#Mutation|mutation]], [[gene expression programming#Transposition|transposition]], [[gene expression programming#Inversion|inversion]], and [[gene expression programming#Recombination|recombination]]. In addition, special operators are also used to allow a constant flow of genetic variation in the set of weights and thresholds.\n\nFor example, below is shown a neural network with two input units (''i''<sub>1</sub> and ''i''<sub>2</sub>), two hidden units (''h''<sub>1</sub> and ''h''<sub>2</sub>), and one output unit (''o''<sub>1</sub>). It has a total of six connections with six corresponding weights represented by the numerals 1–6 (for simplicity, the thresholds are all equal to 1 and are omitted):\n\n{| align=\"center\" border=\"0\" cellpadding=\"4\" cellspacing=\"0\"\n| [[File:Neural network with 5 units.png]]\n|}\n\nThis representation is the canonical neural network representation, but neural networks can also be represented by a tree, which, in this case, corresponds to:\n\n{| align=\"center\" border=\"0\" cellpadding=\"4\" cellspacing=\"0\"\n| [[File:GEP neural network with 7 nodes.png]]\n|}\n\nwhere \"a” and \"b” represent the two inputs ''i''<sub>1</sub> and ''i''<sub>2</sub> and \"D” represents a function with connectivity two. This function adds all its weighted arguments and then thresholds this activation in order to determine the forwarded output. This output (zero or one in this simple case) depends on the threshold of each unit, that is, if the total incoming activation is equal to or greater than the threshold, then the output is one, zero otherwise.\n\nThe above NN-tree can be linearized as follows:\n\n:<code><nowiki>0123456789012</nowiki></code>\n:\n:<code><nowiki>DDDabab654321</nowiki></code>\n\nwhere the structure in positions 7–12 (Dw) encodes the weights. The values of each weight are kept in an array and retrieved as necessary for expression.\n\nAs a more concrete example, below is shown a neural net gene for the [[exclusive or|exclusive-or]] problem. It has a head size of 3 and Dw size of 6:\n\n:<code><nowiki>0123456789012</nowiki></code>\n:\n:<code><nowiki>DDDabab393257</nowiki></code>\n\nIts expression results in the following neural network:\n\n{| align=\"center\" border=\"0\" cellpadding=\"4\" cellspacing=\"0\"\n| [[File:Expression of a GEP neural network for the exclusive-or.png]]\n|}\n\nwhich, for the set of weights:\n\n: ''W'' = {−1.978, 0.514, −0.465, 1.22, −1.686, −1.797, 0.197, 1.606, 0, 1.753}\n\nit gives:\n\n{| align=\"center\" border=\"0\" cellpadding=\"4\" cellspacing=\"0\"\n| [[File:GEP neural network solution for the exclusive-or.png]]\n|}\n\nwhich is a perfect solution to the exclusive-or function.\n\nBesides simple Boolean functions with binary inputs and binary outputs, the GEP-nets algorithm can handle all kinds of functions or neurons (linear neuron, tanh neuron, atan neuron, logistic neuron, limit neuron, radial basis and triangular basis neurons, all kinds of step neurons, and so on). Also interesting is that the GEP-nets algorithm can use all these neurons together and let evolution decide which ones work best to solve the problem at hand. So, GEP-nets can be used not only in Boolean problems but also in [[logistic regression]], [[Statistical classification|classification]], and [[Regression analysis|regression]]. In all cases, GEP-nets can be implemented not only with [[gene expression programming#Multigenic chromosomes|multigenic systems]] but also [[gene expression programming#Cells and code reuse|cellular systems]], both unicellular and multicellular. Furthermore, multinomial classification problems can also be tackled in one go by GEP-nets both with multigenic systems and multicellular systems.\n\n==Decision trees==\n[[Decision trees]] (DT) are classification models where a series of questions and answers are mapped using nodes and directed edges.\n\nDecision trees have three types of nodes: a root node, internal nodes, and leaf or terminal nodes. The root node and all internal nodes represent test conditions for different attributes or variables in a dataset. Leaf nodes specify the class label for all different paths in the tree.\n\nMost decision tree induction algorithms involve selecting an attribute for the root node and then make the same kind of informed decision about all the nodes in a tree.\n\nDecision trees can also be created by gene expression programming,<ref>{{cite book |last=Ferreira |first=C. |title=Gene Expression Programming: Mathematical Modeling by an Artificial Intelligence |publisher=Springer-Verlag |year=2006 |isbn=3-540-32796-7}}</ref> with the advantage that all the decisions concerning the growth of the tree are made by the algorithm itself without any kind of human input.\n\nThere are basically two different types of DT algorithms: one for inducing decision trees with only nominal attributes and another for inducing decision trees with both numeric and nominal attributes. This aspect of decision tree induction also carries to gene expression programming and there are two GEP algorithms for decision tree induction: the evolvable decision trees (EDT) algorithm for dealing exclusively with nominal attributes and the EDT-RNC (EDT with random numerical constants) for handling both nominal and numeric attributes.\n\nIn the decision trees induced by gene expression programming, the attributes behave as function nodes in the [[gene expression programming#The basic gene expression algorithm|basic gene expression algorithm]], whereas the class labels behave as terminals. This means that attribute nodes have also associated with them a specific arity or number of branches that will determine their growth and, ultimately, the growth of the tree. Class labels behave like terminals, which means that for a ''k''-class classification task, a terminal set with ''k'' terminals is used, representing the ''k'' different classes.\n\nThe rules for encoding a decision tree in a linear genome are very similar to the rules used to encode mathematical expressions (see [[gene expression programming#K-expressions and genes|above]]). So, for decision tree induction the genes also have a head and a tail, with the head containing attributes and terminals and the tail containing only terminals. This again ensures that all decision trees designed by GEP are always valid programs. Furthermore, the size of the tail ''t'' is also dictated by the head size ''h'' and the number of branches of the attribute with more branches ''n''<sub>max</sub> and is evaluated by the equation:\n\n:<math>t = h(n_\\max-1)+1 \\, </math>\n\nFor example, consider the decision tree below to decide whether to play outside:\n\n{| align=\"center\" border=\"0\" cellpadding=\"4\" cellspacing=\"0\"\n| [[File:Decision tree for playing outside.png]]\n|}\n\nIt can be linearly encoded as:\n\n:<code><nowiki>01234567</nowiki></code>\n:\n:<code><nowiki>HOWbaaba</nowiki></code>\n\nwhere “H” represents the attribute Humidity, “O” the attribute Outlook, “W” represents Windy, and “a” and “b” the class labels \"Yes\" and \"No\" respectively. Note that the edges connecting the nodes are properties of the data, specifying the type and number of branches of each attribute, and therefore don’t have to be encoded.\n\nThe process of decision tree induction with gene expression programming starts, as usual, with an initial population of randomly created chromosomes. Then the chromosomes are expressed as decision trees and their fitness evaluated against a training dataset. According to fitness they are then selected to reproduce with modification. The genetic operators are exactly the same that are used in a conventional unigenic system, for example, [[gene expression programming#Mutation|mutation]], [[gene expression programming#Inversion|inversion]], [[gene expression programming#Transposition|transposition]], and [[gene expression programming#Recombination|recombination]].\n\nDecision trees with both nominal and numeric attributes are also easily induced with gene expression programming using the framework described [[gene expression programming#The GEP-RNC algorithm|above]] for dealing with random numerical constants. The chromosomal architecture includes an extra domain for encoding random numerical constants, which are used as thresholds for splitting the data at each branching node. For example, the gene below with a head size of 5 (the Dc starts at position 16):\n\n:<code><nowiki>012345678901234567890</nowiki></code>\n:\n:<code><nowiki>WOTHabababbbabba46336</nowiki></code>\n\nencodes the decision tree shown below:\n\n{| align=\"center\" border=\"0\" cellpadding=\"4\" cellspacing=\"0\"\n| [[File:GEP decision tree, k-expression WOTHababab.png]]\n|}\n\nIn this system, every node in the head, irrespective of its type (numeric attribute, nominal attribute, or terminal), has associated with it a random numerical constant, which for simplicity in the example above is represented by a numeral 0–9. These random numerical constants are encoded in the Dc domain and their expression follows a very simple scheme: from top to bottom and from left to right, the elements in Dc are assigned one-by-one to the elements in the decision tree. So, for the following array of RNCs:\n\n: ''C'' = {62, 51, 68, 83, 86, 41, 43, 44, 9, 67}\n\nthe decision tree above results in:\n\n{| align=\"center\" border=\"0\" cellpadding=\"4\" cellspacing=\"0\"\n| [[File:GEP decision tree with numeric and nominal attributes, k-expression WOTHababab.png]]\n|}\n\nwhich can also be represented more colorfully as a conventional decision tree:\n\n{| align=\"center\" border=\"0\" cellpadding=\"4\" cellspacing=\"0\"\n| [[File:GEP decision tree with numeric and nominal attributes.png]]\n|}\n\n==Criticism==\nGEP has been criticized for not being a major improvement over other [[genetic programming]] techniques. In many experiments, it did not perform better than existing methods.<ref>{{citation | title=A comparison of several linear genetic programming techniques |\n  author=Oltean, M. | author2=Grosan, C. |\n  journal=Complex Systems |\n  volume=14 |\n  issue=4 |\n  pages=285–314 |\n  year=2003 }}</ref>\n\n<!-- TODO: add more critical literature, for an unbiased view. See talk page. -->\n\n==Software==\n\n===Commercial applications===\n; [[GeneXproTools]]: GeneXproTools is a [[predictive analytics]] suite developed by [[Gepsoft]]. GeneXproTools modeling frameworks include [[logistic regression]], [[Statistical classification|classification]], [[Regression analysis|regression]], [[time series prediction]], and [[logic synthesis]]. GeneXproTools implements the basic [[gene expression programming#The basic gene expression algorithm|gene expression algorithm]] and the [[gene expression programming#The GEP-RNC algorithm|GEP-RNC algorithm]], both used in all the modeling frameworks of GeneXproTools.\n\n===Open-source libraries===\n; GEP4J – GEP for Java Project: Created by Jason Thomas, GEP4J is an open-source implementation of gene expression programming in [[Java (programming language)|Java]]. It implements different GEP algorithms, including evolving [[gene expression programming#Decision trees|decision trees]] (with nominal, numeric, or mixed attributes) and [[gene expression programming#Cells and code reuse|automatically defined functions]]. GEP4J is hosted at [[Google Code]].\n\n; PyGEP – Gene Expression Programming for Python:  Created by Ryan O'Neil with the goal to create a simple library suitable for the academic study of gene expression programming in [[Python (programming language)|Python]], aiming for ease of use and rapid implementation. It implements standard [[gene expression programming#Multigenic chromosomes|multigenic chromosomes]] and the genetic operators mutation, crossover, and transposition. PyGEP is hosted at [[Google Code]].\n\n; jGEP – Java GEP toolkit: Created by Matthew Sottile to rapidly build [[Java (programming language)|Java]] prototype codes that use GEP, which can then be written in a language such as [[C (programming language)|C]] or [[Fortran]] for real speed. jGEP is hosted at [[SourceForge]].\n\n==Further reading==\n*{{cite book |last=Ferreira |first=C. |title=Gene Expression Programming: Mathematical Modeling by an Artificial Intelligence |publisher=Springer-Verlag |year=2006 |isbn=3-540-32796-7}}\n*{{cite book |last=Ferreira |first=C. |title=Gene Expression Programming: Mathematical Modeling by an Artificial Intelligence |publisher=Angra do Heroismo |location=Portugal |year=2002 |isbn=972-95890-5-4 |url=http://www.gene-expression-programming.com/GepBook/Introduction.htm}}\n\n==See also==\n* [[Artificial intelligence]]\n* [[Decision trees]]\n* [[Evolutionary algorithms]]\n* [[Genetic algorithms]]\n* [[Genetic programming]]\n* [[GeneXproTools]]\n* [[Machine learning]]\n* [[Artificial neural network|Neural networks]]\n\n==References==\n{{Reflist|colwidth=30em}}\n\n==External links==\n* [http://www.gene-expression-programming.com/ GEP home page], maintained by the inventor of gene expression programming.\n* [http://www.gepsoft.com/ GeneXproTools], commercial GEP software.\n\n{{DEFAULTSORT:Gene expression programming}}\n[[Category:Gene expression programming| ]]\n[[Category:Evolutionary algorithms]]\n[[Category:Evolutionary computation]]\n[[Category:Genetic algorithms]]\n[[Category:Genetic programming]]"
    },
    {
      "title": "Genetic algorithm",
      "url": "https://en.wikipedia.org/wiki/Genetic_algorithm",
      "text": "{{Evolutionary algorithms}}\n{{Use dmy dates|date=July 2013}}\n[[Image:St 5-xband-antenna.jpg|thumb|The 2007 NASA [[Space Technology 5|ST5]] spacecraft antenna. This complicated shape was found by an evolutionary computer design program to create the best radiation pattern. It is known as an [[evolved antenna]].]]\n<!-- Deleted image removed: [[Image:ESA JAXA HUMIES Trajectory.png|thumb|The ESA/JAXA interplanetary Trajectory recipient of the [http://www.genetic-programming.org/combined.php 2013 gold HUMIES ] award. This complex tour of the Jovian Moons was found with the help of an evolutionary technique based on self-adaptation]] -->\nIn [[computer science]] and [[operations research]], a '''genetic algorithm''' ('''GA''') is a [[metaheuristic]] inspired by   the process of [[natural selection]] that belongs to the larger class of [[evolutionary algorithm]]s (EA). Genetic algorithms are commonly used to generate high-quality solutions to [[Optimization (mathematics)|optimization]] and [[Search algorithm|search problem]]s by relying on bio-inspired operators such as [[Mutation (genetic algorithm)|mutation]], [[crossover (genetic algorithm)|crossover]] and [[selection (genetic algorithm)|selection]].{{sfn|Mitchell|1996|p=2}} [[John Henry Holland|John Holland]] introduced genetic algorithms in 1960 based on the concept of [[Darwin's theory of evolution|Darwin’s theory of evolution]]; afterwards, his student [[David E. Goldberg]] extended GA in 1989.<ref>{{Cite journal|date=2014-07-10|title=Optimizing a hybrid vendor-managed inventory and transportation problem with fuzzy demand: An improved particle swarm optimization algorithm|url=https://www.sciencedirect.com/science/article/pii/S0020025514001819|journal=Information Sciences|language=en|volume=272|pages=126–144|doi=10.1016/j.ins.2014.02.075|issn=0020-0255|last1=Sadeghi|first1=Javad|last2=Sadeghi|first2=Saeid|last3=Niaki|first3=Seyed Taghi Akhavan}}</ref>\n\n== Methodology ==\n\n=== Optimization problems ===\nIn a genetic algorithm, a [[population]] of [[candidate solution]]s (called individuals, creatures, or [[phenotype]]s) to an optimization problem is evolved toward better solutions. Each candidate solution has a set of properties (its [[chromosome]]s or [[genotype]]) which can be mutated and altered; traditionally, solutions are represented in binary as strings of 0s and 1s, but other encodings are also possible.{{sfn|Whitley|1994|p=66}}\n\nThe evolution usually starts from a population of randomly generated individuals, and is an [[Iteration|iterative process]], with the population in each iteration called a ''generation''. In each generation, the [[fitness (biology)|fitness]] of every individual in the population is evaluated; the fitness is usually the value of the [[objective function]] in the optimization problem being solved. The more fit individuals are [[Stochastics|stochastically]] selected from the current population, and each individual's genome is modified ([[Crossover (genetic algorithm)|recombined]] and possibly randomly mutated) to form a new generation. The new generation of candidate solutions is then used in the next iteration of the [[algorithm]]. Commonly, the algorithm terminates when either a maximum number of generations has been produced, or a satisfactory fitness level has been reached for the population.\n\nA typical genetic algorithm requires:\n\n# a [[genetic representation]] of the solution domain,\n# a [[fitness function]] to evaluate the solution domain.\n\nA standard representation of each candidate solution is as an [[bit array|array of bits]].{{sfn|Whitley|1994|p=66}} Arrays of other types and structures can be used in essentially the same way. The main property that makes these genetic representations convenient is that their parts are easily aligned due to their fixed size, which facilitates simple [[Crossover (genetic algorithm)|crossover]] operations. Variable length representations may also be used, but crossover implementation is more complex in this case. Tree-like representations are explored in [[genetic programming]] and graph-form representations are explored in [[evolutionary programming]]; a mix of both linear chromosomes and trees is explored in [[gene expression programming]].\n\nOnce the genetic representation and the fitness function are defined, a GA proceeds to initialize a population of solutions and then to improve it through repetitive application of the mutation, crossover, inversion and selection operators.\n\n==== Initialization ====\nThe population size depends on the nature of the problem, but typically contains several hundreds or thousands of possible solutions. Often, the initial population is generated randomly, allowing the entire range of possible solutions (the [[Feasible region|search space]]). Occasionally, the solutions may be \"seeded\" in areas where optimal solutions are likely to be found.\n\n==== Selection ====\n{{Main|Selection (genetic algorithm)}}\nDuring each successive generation, a portion of the existing population is [[selection (genetic algorithm)|selected]] to breed a new generation. Individual solutions are selected through a ''fitness-based'' process, where [[Fitness (biology)|fitter]] solutions (as measured by a [[fitness function]]) are typically more likely to be selected. Certain selection methods rate the fitness of each solution and preferentially select the best solutions. Other methods rate only a random sample of the population, as the former process may be very time-consuming.\n\nThe fitness function is defined over the genetic representation and measures the ''quality'' of the represented solution. The fitness function is always problem dependent. For instance, in the [[knapsack problem]] one wants to maximize the total value of objects that can be put in a knapsack of some fixed capacity. A representation of a solution might be an array of bits, where each bit represents a different object, and the value of the bit (0 or 1) represents whether or not the object is in the knapsack. Not every such representation is valid, as the size of objects may exceed the capacity of the knapsack. The ''fitness'' of the solution is the sum of values of all objects in the knapsack if the representation is valid, or 0 otherwise.\n\nIn some problems, it is hard or even impossible to define the fitness expression; in these cases, a [[Computer simulation|simulation]] may be used to determine the fitness function value of a [[phenotype]] (e.g. [[computational fluid dynamics]] is used to determine the air resistance of a vehicle whose shape is encoded as the phenotype), or even [[Interactive evolutionary computation|interactive genetic algorithms]] are used.\n\n==== Genetic operators ====\n{{Main|Crossover (genetic algorithm)|Mutation (genetic algorithm)}}\n\nThe next step is to generate a second generation population of solutions from those selected through a combination of [[genetic operator]]s: [[crossover (genetic algorithm)|crossover]] (also called recombination), and [[mutation (genetic algorithm)|mutation]].\n\nFor each new solution to be produced, a pair of \"parent\" solutions is selected for breeding from the pool selected previously. By producing a \"child\" solution using the above methods of crossover and mutation, a new solution is created which typically shares many of the characteristics of its \"parents\". New parents are selected for each new child, and the process continues until a new population of solutions of appropriate size is generated.\nAlthough reproduction methods that are based on the use of two parents are more \"biology inspired\", some research<ref>Eiben, A. E. et al (1994). \"Genetic algorithms with multi-parent recombination\". PPSN III: Proceedings of the International Conference on Evolutionary Computation. The Third Conference on Parallel Problem Solving from Nature: 78&ndash;87. {{ISBN|3-540-58484-6}}.</ref><ref>Ting, Chuan-Kang (2005). \"On the Mean Convergence Time of Multi-parent Genetic Algorithms Without Selection\". Advances in Artificial Life: 403&ndash;412. {{ISBN|978-3-540-28848-0}}.</ref> suggests that more than two \"parents\" generate higher quality chromosomes.\n\nThese processes ultimately result in the next generation population of chromosomes that is different from the initial generation. Generally the average fitness will have increased by this procedure for the population, since only the best organisms from the first generation are selected for breeding, along with a small proportion of less fit solutions. These less fit solutions ensure genetic diversity within the genetic pool of the parents and therefore ensure the genetic diversity of the subsequent generation of children.\n\nOpinion is divided over the importance of crossover versus mutation. There are many references in [[David B. Fogel|Fogel]] (2006) that support the importance of mutation-based search.\n\nAlthough crossover and mutation are known as the main genetic operators, it is possible to use other operators such as regrouping, colonization-extinction, or migration in genetic algorithms.<ref>Akbari, Ziarati (2010). \"A multilevel evolutionary algorithm for optimizing numerical functions\" IJIEC 2 (2011): 419&ndash;430  [http://growingscience.com/ijiec/Vol2/IJIEC_2010_11.pdf]</ref>\n\nIt is worth tuning parameters such as the [[Mutation (genetic algorithm)|mutation]] probability, [[Crossover (genetic algorithm)|crossover]] probability and population size to find reasonable settings for the problem class being worked on. A very small mutation rate may lead to [[genetic drift]]  (which is non-[[Ergodicity|ergodic]] in nature). A recombination rate that is too high may lead to premature convergence of the genetic algorithm. A mutation rate that is too high may lead to loss of good solutions, unless [[#Elitism|elitist selection]] is employed.\n\n==== Heuristics ====\n\nIn addition to the main operators above, other heuristics may be employed to make the calculation faster or more robust. The ''speciation'' heuristic penalizes crossover between candidate solutions that are too similar; this encourages population diversity and helps prevent premature convergence to a less optimal solution.<ref>{{Cite book|chapter-url=https://pdfs.semanticscholar.org/a525/1192091cc6c138cf8010e43d72b9dfb0d022.pdf|title=Handbook of Evolutionary Computation|last=Deb|first=Kalyanmoy|last2=Spears|first2=William M.|publisher=Institute of Physics Publishing|year=1997|isbn=|location=|pages=|chapter=C6.2: Speciation methods}}</ref><ref>{{Cite book|title=Handbook of Natural Computing|last=Shir|first=Ofer M.|date=2012|publisher=Springer Berlin Heidelberg|isbn=9783540929093|editor-last=Rozenberg|editor-first=Grzegorz|location=|pages=1035–1069|language=en|chapter=Niching in Evolutionary Algorithms|doi=10.1007/978-3-540-92910-9_32|editor-last2=Bäck|editor-first2=Thomas|editor-last3=Kok|editor-first3=Joost N.}}</ref>\n\n==== Termination ====\nThis generational process is repeated until a termination condition has been reached. Common terminating conditions are:\n\n* A solution is found that satisfies minimum criteria\n* Fixed number of generations reached\n* Allocated budget (computation time/money) reached\n* The highest ranking solution's fitness is reaching or has reached a plateau such that successive iterations no longer produce better results\n* Manual inspection\n* Combinations of the above\n\n== The building block hypothesis ==\nGenetic algorithms are simple to implement, but their behavior is difficult to understand. In particular it is difficult to understand why these algorithms frequently succeed at generating solutions of high fitness when applied to practical problems. The building block hypothesis (BBH) consists of:\n\n# A description of a heuristic that performs adaptation by identifying and recombining \"building blocks\", i.e. low order, low defining-length [[Schema (genetic algorithms)|schemata]] with above average fitness.\n# A hypothesis that a genetic algorithm performs adaptation by implicitly and efficiently implementing this heuristic.\n\nGoldberg describes the heuristic as follows:\n\n:\"Short, low order, and highly fit schemata are sampled, [[crossover (genetic algorithm)|recombined]] [crossed over], and resampled to form strings of potentially higher fitness. In a way, by working with these particular schemata [the building blocks], we have reduced the complexity of our problem; instead of building high-performance strings by trying every conceivable combination, we construct better and better strings from the best partial solutions of past samplings.\n\n:\"Because highly fit schemata of low defining length and low order play such an important role in the action of genetic algorithms, we have already given them a special name: building blocks. Just as a child creates magnificent fortresses through the arrangement of simple blocks of wood, so does a genetic algorithm seek near optimal performance through the juxtaposition of short, low-order, high-performance schemata, or building blocks.\"{{sfn|Goldberg|1989|p=41}}\n\nDespite the lack of consensus regarding the validity of the building-block hypothesis, it has been consistently evaluated and used as reference throughout the years. Many [[estimation of distribution algorithm]]s, for example, have been proposed in an attempt to provide an environment in which the hypothesis would hold.<ref>{{cite book|last1=Harik|first1=Georges R.|last2=Lobo|first2=Fernando G.|last3=Sastry|first3=Kumara|title=Linkage Learning via Probabilistic Modeling in the Extended Compact Genetic Algorithm (ECGA)|journal=Scalable Optimization Via Probabilistic Modeling|volume=33|date=1 January 2006|pages=39–61|doi=10.1007/978-3-540-34954-9_3|language=en|series=Studies in Computational Intelligence|isbn=978-3-540-34953-2}}</ref><ref>{{cite book|last1=Pelikan|first1=Martin|last2=Goldberg|first2=David E.|last3=Cantú-Paz|first3=Erick|title=BOA: The Bayesian Optimization Algorithm|journal=Proceedings of the 1st Annual Conference on Genetic and Evolutionary Computation - Volume 1|date=1 January 1999|pages=525–532|url=http://dl.acm.org/citation.cfm?id=2933973|isbn=9781558606111|series=Gecco'99}}</ref> Although good results have been reported for some classes of problems, skepticism concerning the generality and/or practicality of the building-block hypothesis as an explanation for GAs efficiency still remains. Indeed, there is a reasonable amount of work that attempts to understand its limitations from the perspective of estimation of distribution algorithms.<ref>{{cite book|last1=Coffin|first1=David|last2=Smith|first2=Robert E.|title=Linkage Learning in Estimation of Distribution Algorithms|journal=Linkage in Evolutionary Computation|volume=157|date=1 January 2008|pages=141–156|doi=10.1007/978-3-540-85068-7_7|language=en|series=Studies in Computational Intelligence|isbn=978-3-540-85067-0}}</ref><ref>{{cite journal|last1=Echegoyen|first1=Carlos|last2=Mendiburu|first2=Alexander|last3=Santana|first3=Roberto|last4=Lozano|first4=Jose A.|title=On the Taxonomy of Optimization Problems Under Estimation of Distribution Algorithms|journal=Evolutionary Computation|date=8 November 2012|volume=21|issue=3|pages=471–495|doi=10.1162/EVCO_a_00095|pmid=23136917|issn=1063-6560}}</ref><ref>{{cite book|last1=Sadowski|first1=Krzysztof L.|last2=Bosman|first2=Peter A.N.|last3=Thierens|first3=Dirk|title=On the Usefulness of Linkage Processing for Solving MAX-SAT|journal=Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation|date=1 January 2013|pages=853–860|doi=10.1145/2463372.2463474|url=http://dl.acm.org/citation.cfm?id=2463474|isbn=9781450319638|series=Gecco '13|hdl=1874/290291}}</ref>\n\n== Limitations ==\nThere are limitations of the use of a genetic algorithm compared to alternative optimization algorithms:\n\n* Repeated [[fitness function]] evaluation for complex problems is often the most prohibitive and limiting segment of artificial evolutionary algorithms. Finding the optimal solution to complex high-dimensional, multimodal problems often requires very expensive [[fitness function]] evaluations. In real world problems such as structural optimization problems, a single function evaluation may require several hours to several days of complete simulation. Typical optimization methods cannot deal with such types of problem. In this case, it may be necessary to forgo an exact evaluation and use an [[fitness approximation|approximated fitness]] that is computationally efficient. It is apparent that amalgamation of [[fitness approximation|approximate models]] may be one of the most promising approaches to convincingly use GA to solve complex real life problems.\n* Genetic algorithms do not scale well with complexity. That is, where the number of elements which are exposed to mutation is large there is often an exponential increase in search space size. This makes it extremely difficult to use the technique on problems such as designing an engine, a house or a plane. In order to make such problems tractable to evolutionary search, they must be broken down into the simplest representation possible. Hence we typically see evolutionary algorithms encoding designs for fan blades instead of engines, building shapes instead of detailed construction plans, and airfoils instead of whole aircraft designs. The second problem of complexity is the issue of how to protect parts that have evolved to represent good solutions from further destructive mutation, particularly when their fitness assessment requires them to combine well with other parts.\n* The \"better\" solution is only in comparison to other solutions. As a result, the stop criterion is not clear in every problem.\n* In many problems, GAs have a tendency to converge towards [[local optimum|local optima]] or even arbitrary points rather than the [[global optimum]] of the problem. This means that it does not \"know how\" to sacrifice short-term fitness to gain longer-term fitness. The likelihood of this occurring depends on the shape of the [[fitness landscape]]: certain problems may provide an easy ascent towards a global optimum, others may make it easier for the function to find the local optima. This problem may be alleviated by using a different fitness function, increasing the rate of mutation, or by using selection techniques that maintain a diverse population of solutions,<ref>{{cite journal|last=Taherdangkoo|first=Mohammad|last2=Paziresh |first2=Mahsa |last3=Yazdi |first3=Mehran |last4= Bagheri |first4=Mohammad Hadi |title=An efficient algorithm for function optimization: modified stem cells algorithm|journal=Central European Journal of Engineering|date=19 November 2012|volume=3|issue=1|pages=36–50|doi=10.2478/s13531-012-0047-8}}</ref> although the [[No free lunch in search and optimization|No Free Lunch theorem]]<ref>Wolpert, D.H., Macready, W.G., 1995. No Free Lunch Theorems for Optimisation. Santa Fe Institute, SFI-TR-05-010, Santa Fe.</ref> proves that there is no general solution to this problem. A common technique to maintain diversity is to impose a \"niche penalty\", wherein, any group of individuals of sufficient similarity (niche radius) have a penalty added, which will reduce the representation of that group in subsequent generations, permitting other (less similar) individuals to be maintained in the population. This trick, however, may not be effective, depending on the landscape of the problem. Another possible technique would be to simply replace part of the population with randomly generated individuals, when most of the population is too similar to each other. Diversity is important in genetic algorithms (and [[genetic programming]]) because crossing over a homogeneous population does not yield new solutions. In [[Evolution strategy|evolution strategies]] and [[evolutionary programming]], diversity is not essential because of a greater reliance on mutation.\n* Operating on dynamic data sets is difficult, as genomes begin to converge early on towards solutions which may no longer be valid for later data. Several methods have been proposed to remedy this by increasing genetic diversity somehow and preventing early convergence, either by increasing the probability of mutation when the solution quality drops (called ''triggered hypermutation''), or by occasionally introducing entirely new, randomly generated elements into the gene pool (called ''random immigrants''). Again, [[Evolution strategy|evolution strategies]] and [[evolutionary programming]] can be implemented with a so-called \"comma strategy\" in which parents are not maintained and new parents are selected only from offspring. This can be more effective on dynamic problems.\n* GAs cannot effectively solve problems in which the only fitness measure is a single right/wrong measure (like [[decision problem]]s), as there is no way to converge on the solution (no hill to climb). In these cases, a random search may find a solution as quickly as a GA. However, if the situation allows the success/failure trial to be repeated giving (possibly) different results, then the ratio of successes to failures provides a suitable fitness measure.\n* For specific optimization problems and problem instances, other optimization algorithms may be more efficient than genetic algorithms in terms of speed of convergence. Alternative and complementary algorithms include [[Evolution strategy|evolution strategies]], [[evolutionary programming]], [[simulated annealing]], [[Gaussian adaptation]], [[hill climbing]], and [[swarm intelligence]] (e.g.: [[ant colony optimization]], [[particle swarm optimization]]) and methods based on [[integer linear programming]]. The suitability of genetic algorithms is dependent on the amount of knowledge of the problem; well known problems often have better, more specialized approaches.\n\n== Variants ==\n\n=== Chromosome representation ===\n{{main | genetic representation }}\nThe simplest algorithm represents each chromosome as a [[Bit array|bit string]]. Typically, numeric parameters can be represented by [[integer]]s, though it is possible to use [[floating point]] representations. The floating point representation is natural to [[Evolution strategy|evolution strategies]] and [[evolutionary programming]]. The notion of real-valued genetic algorithms has been offered but is really a misnomer because it does not really represent the building block theory that was proposed by [[John Henry Holland]] in the 1970s. This theory is not without support though, based on theoretical and experimental results (see below). The basic algorithm performs crossover and mutation at the bit level. Other variants treat the chromosome as a list of numbers which are indexes into an instruction table, nodes in a [[linked list]], [[associative array|hashes]], [[object (computer science)|objects]], or any other imaginable [[data structure]]. Crossover and mutation are performed so as to respect data element boundaries. For most data types, specific variation operators can be designed. Different chromosomal data types seem to work better or worse for different specific problem domains.\n\nWhen bit-string representations of integers are used, [[Gray coding]] is often employed. In this way, small changes in the integer can be readily affected through mutations or crossovers. This has been found to help prevent premature convergence at so called ''Hamming walls'', in which too many simultaneous mutations (or crossover events) must occur in order to change the chromosome to a better solution.\n\nOther approaches involve using arrays of real-valued numbers instead of bit strings to represent chromosomes. Results from the theory of schemata suggest that in general the smaller the alphabet, the better the performance, but it was initially surprising to researchers that good results were obtained from using real-valued chromosomes. This was explained as the set of real values in a finite population of chromosomes as forming a ''virtual alphabet'' (when selection and recombination are dominant) with a much lower cardinality than would be expected from a floating point representation.<ref name=Goldberg1991>{{cite book|last=Goldberg|first=David E.|title=The theory of virtual alphabets|journal=Parallel Problem Solving from Nature, Lecture Notes in Computer Science|year=1991|volume=496|pages=13–22|doi=10.1007/BFb0029726|series=Lecture Notes in Computer Science|isbn=978-3-540-54148-6}}</ref><ref name=Janikow1991>{{cite journal|last=Janikow|first=C. Z.|first2=Z. |last2=Michalewicz |title=An Experimental Comparison of Binary and Floating Point Representations in Genetic Algorithms|journal=Proceedings of the Fourth International Conference on Genetic Algorithms|year=1991|pages=31–36|url=http://www.cs.umsl.edu/~janikow/publications/1991/GAbin/text.pdf|accessdate=2 July 2013}}</ref>\n\nAn expansion of the Genetic Algorithm accessible problem domain can be obtained through more complex encoding of the solution pools by concatenating several types of heterogenously encoded genes into one chromosome.<ref name=Patrascu2014>{{cite journal|last=Patrascu|first=M.|last2=Stancu|first2=A.F.|last3=Pop|first3=F.|title=HELGA: a heterogeneous encoding lifelike genetic algorithm for population evolution modeling and simulation|journal=Soft Computing|year=2014|volume=18|issue=12|pages=2565–2576|doi=10.1007/s00500-014-1401-y}}</ref> This particular approach allows for solving optimization problems that require vastly disparate definition domains for the problem parameters. For instance, in problems of cascaded controller tuning, the internal loop controller structure can belong to a conventional regulator of three parameters, whereas the external loop could implement a linguistic controller (such as a fuzzy system) which has an inherently different description. This particular form of encoding requires a specialized crossover mechanism that recombines the chromosome by section, and it is a useful tool for the modelling and simulation of complex adaptive systems, especially evolution processes.\n\n=== Elitism ===\nA practical variant of the general process of constructing a new population is to allow the best organism(s) from the current generation to carry over to the next, unaltered. This strategy is known as ''elitist selection'' and guarantees that the solution quality obtained by the GA will not decrease from one generation to the next.<ref>{{cite conference |last1=Baluja |first1=Shumeet |first2=Rich |last2=Caruana |title=Removing the genetics from the standard genetic algorithm |conference=[[International Conference on Machine Learning|ICML]] |year=1995 |url=http://www.ri.cmu.edu/pub_files/pub2/baluja_shumeet_1995_1/baluja_shumeet_1995_1.pdf}}</ref>\n\n=== Parallel implementations ===\nParallel implementations of genetic algorithms come in two flavors. Coarse-grained parallel genetic algorithms assume a population on each of the computer nodes and migration of individuals among the nodes. Fine-grained parallel genetic algorithms assume an individual on each processor node which acts with neighboring individuals for selection and reproduction.\nOther variants, like genetic algorithms for [[online optimization]] problems, introduce time-dependence or noise in the fitness function.\n\n=== Adaptive GAs ===\nGenetic algorithms with adaptive parameters (adaptive genetic algorithms, AGAs) is another significant and promising variant of genetic algorithms. The probabilities of crossover (pc) and mutation (pm) greatly determine the degree of solution accuracy and the convergence speed that genetic algorithms can obtain. Instead of using fixed values of ''pc'' and ''pm'', AGAs utilize the population information in each generation and adaptively adjust the ''pc'' and ''pm'' in order to maintain the population diversity as well as to sustain the convergence capacity. In AGA (adaptive genetic algorithm),<ref>{{Cite journal |last=Srinivas |first=M. |last2=Patnaik |first2=L. |title=Adaptive probabilities of crossover and mutation in genetic algorithms |journal=IEEE Transactions on System, Man and Cybernetics |volume=24 |issue=4 |pages=656–667 |year=1994 |doi=10.1109/21.286385 |url=http://eprints.iisc.ac.in/6971/2/adaptive.pdf }}</ref> the adjustment of ''pc'' and ''pm'' depends on the fitness values of the solutions. In ''CAGA'' (clustering-based adaptive genetic algorithm),<ref>{{cite journal |last=Zhang |first=J. |last2=Chung |first2=H. |last3=Lo, W. L. |title=Clustering-Based Adaptive Crossover and Mutation Probabilities for Genetic Algorithms |journal=IEEE Transactions on Evolutionary Computation |volume=11 |issue=3 |pages=326&ndash;335 |year=2007 |doi=10.1109/TEVC.2006.880727 }}</ref> through the use of clustering analysis to judge the optimization states of the population, the adjustment of ''pc'' and ''pm'' depends on these optimization states.\nIt can be quite effective to combine GA with other optimization methods. GA tends to be quite good at finding generally good global solutions, but quite inefficient at finding the last few mutations to find the absolute optimum. Other techniques (such as simple hill climbing) are quite efficient at finding absolute optimum in a limited region. Alternating GA and hill climbing can improve the efficiency of GA {{Citation needed|date=July 2016}} while overcoming the lack of robustness of hill climbing.\n\nThis means that the rules of genetic variation may have a different meaning in the natural case. For instance &ndash; provided that steps are stored in consecutive order &ndash; crossing over may sum a number of steps from maternal DNA adding a number of steps from paternal DNA and so on. This is like adding vectors that more probably may follow a ridge in the phenotypic landscape. Thus, the efficiency of the process may be increased by many orders of magnitude. Moreover, the [[Chromosomal inversion|inversion operator]] has the opportunity to place steps in consecutive order or any other suitable order in favour of survival or efficiency.<ref>See for instance [http://www.thisurlisfalse.com/evolution-in-a-nutshell/ Evolution-in-a-nutshell] or example in [[travelling salesman problem]], in particular the use of an [[edge recombination operator]].</ref>\n\nA variation, where the population as a whole is evolved rather than its individual members, is known as gene pool recombination.\n\nA number of variations have been developed to attempt to improve performance of GAs on problems with a high degree of fitness epistasis, i.e. where the fitness of a solution consists of interacting subsets of its variables. Such algorithms aim to learn (before exploiting) these beneficial phenotypic interactions. As such, they are aligned with the Building Block Hypothesis in adaptively reducing disruptive recombination. Prominent examples of this approach include the mGA,<ref>{{cite journal |url=http://www.complex-systems.com/issues/03-5.html |first=D. E. |last=Goldberg |first2=B. |last2=Korb |first3=K. |last3=Deb |title=Messy Genetic Algorithms : Motivation Analysis, and First Results |journal=Complex Systems |volume=5 |issue=3 |pages=493–530 |year=1989 }}</ref> GEMGA<ref>[https://www.osti.gov/servlets/purl/524858 Gene expression: The missing link in evolutionary computation]</ref> and LLGA.<ref>{{cite thesis |last=Harik |first=G. |date=1997 |title=Learning linkage to efficiently solve problems of bounded difficulty using genetic algorithms |type=PhD |chapter= |publisher=Dept. Computer Science, University of Michigan, Ann Arbour |docket= |oclc= |url=http://portal.acm.org/citation.cfm?id=269517 |access-date=}}</ref>\n\n== Problem domains ==\nProblems which appear to be particularly appropriate for solution by genetic algorithms include [[Timeline|timetabling]] and scheduling problems, and many scheduling software packages are based on GAs{{Citation needed|date=December 2011}}. GAs have also been applied to [[engineering]].<ref>Tomoiagă B, Chindriş M, Sumper A, Sudria-Andreu A, Villafafila-Robles R. [http://www.mdpi.com/1996-1073/6/3/1439/pdf Pareto Optimal Reconfiguration of Power Distribution Systems Using a Genetic Algorithm Based on NSGA-II. ] Energies. 2013; 6(3):1439-1455.</ref> Genetic algorithms are often applied as an approach to solve [[global optimization]] problems.\n\nAs a general rule of thumb genetic algorithms might be useful in problem domains that have a complex [[fitness landscape]] as mixing, i.e., [[Mutation (genetic algorithm)|mutation]] in combination with [[Crossover (genetic algorithm)|crossover]], is designed to move the population away from [[local optima]] that a traditional [[hill climbing]] algorithm might get stuck in. Observe that commonly used crossover operators cannot change any uniform population. Mutation alone can provide ergodicity of the overall genetic algorithm process (seen as a [[Markov chain]]).\n\nExamples of problems solved by genetic algorithms include: mirrors designed to funnel sunlight to a solar collector,<ref>{{cite web|last=Gross|first=Bill|title=A solar energy system that tracks the sun|url=http://www.ted.com/talks/bill_gross_on_new_energy.html|work=TED|accessdate=20 November 2013}}</ref> antennae designed to pick up radio signals in space,<ref>{{citation |first=G. S. |last=Hornby |first2=D. S. |last2=Linden |first3=J. D. |last3=Lohn |url=http://ti.arc.nasa.gov/m/pub-archive/1244h/1244%20(Hornby).pdf |title=Automated Antenna Design with Evolutionary Algorithms}}</ref> walking methods for computer figures,<ref>{{Cite web | url=http://goatstream.com/research/papers/SA2013/index.html | title=Flexible Muscle-Based Locomotion for Bipedal Creatures}}</ref> optimal design of aerodynamic bodies in complex flowfields <ref>{{Cite journal|last=Evans|first=B.|last2=Walton|first2=S.P.|date=December 2017|title=Aerodynamic optimisation of a hypersonic reentry vehicle based on solution of the Boltzmann–BGK equation and evolutionary optimisation|journal=Applied Mathematical Modelling|volume=52|pages=215–240|doi=10.1016/j.apm.2017.07.024|issn=0307-904X|url=https://cronfa.swan.ac.uk/Record/cronfa34688}}</ref>\n\nIn his ''Algorithm Design Manual'', [[Steven Skiena|Skiena]] advises against genetic algorithms for any task:\n\n{{blockquote|[I]t is quite unnatural to model applications in terms of genetic operators like mutation and crossover on bit strings. The pseudobiology adds another level of complexity between you and your problem. Second, genetic algorithms take a very long time on nontrivial problems. [...] [T]he analogy with evolution—where significant progress require [sic] millions of years—can be quite appropriate.\n[...]\nI have never encountered any problem where genetic algorithms seemed to me the right way to attack it. Further, I have never seen any computational results reported using genetic algorithms that have favorably impressed me. Stick to [[simulated annealing]] for your heuristic search voodoo needs.|Steven Skiena<ref>{{cite book |last=Skiena |first=Steven |authorlink=Steven Skiena |title = The Algorithm Design Manual |publisher=[[Springer Science+Business Media]] |edition=2nd |year = 2010 |isbn=978-1-849-96720-4}}</ref>{{rp|267}}}}\n\n== History ==\nIn 1950, [[Alan Turing]] proposed a \"learning machine\" which would parallel the principles of evolution.<ref name=\"mind.oxfordjournals.org\">{{cite journal|last1=Turing|first1=Alan M.|title=Computing machinery and intelligence|journal=Mind|volume=LIX|issue=238|pages=433–460|doi=10.1093/mind/LIX.236.433|url=http://mind.oxfordjournals.org/content/LIX/236/433|date=October 1950}}</ref> Computer simulation of evolution started as early as in 1954 with the work of [[Nils Aall Barricelli]], who was using the computer at the [[Institute for Advanced Study]] in [[Princeton, New Jersey]].<ref name=\"Barricelli 1954 45–68\">{{cite journal|last=Barricelli|first=Nils Aall|year=1954|authorlink=Nils Aall Barricelli|title=Esempi numerici di processi di evoluzione|journal=Methodos|pages=45–68}}</ref><ref name=\"Barricelli 1957 143–182\">{{cite journal|last=Barricelli|first=Nils Aall|year=1957|authorlink=Nils Aall Barricelli|title=Symbiogenetic evolution processes realized by artificial methods|journal=Methodos|pages=143–182}}</ref>  His 1954 publication was not widely noticed. Starting in 1957,<ref name=\"Fraser 1957 484–491\">{{cite journal|last=Fraser|first=Alex|authorlink=Alex Fraser (scientist)|year=1957|title=Simulation of genetic systems by automatic digital computers. I. Introduction|journal=Aust. J. Biol. Sci.|volume=10|issue=4|pages=484–491|doi=10.1071/BI9570484}}</ref>  the Australian quantitative geneticist [[Alex Fraser (scientist)|Alex Fraser]] published a series of papers on simulation of [[artificial selection]] of organisms with multiple loci controlling a measurable trait. From these beginnings, computer simulation of evolution by biologists became more common in the early 1960s, and the methods were described in books by Fraser and Burnell (1970)<ref name=\"Fraser 1970\">{{cite book|last=Fraser|first=Alex|authorlink=Alex Fraser (scientist)|first2=Donald |last2=Burnell|year=1970|title=Computer Models in Genetics|publisher=McGraw-Hill|location=New York|isbn=978-0-07-021904-5}}</ref> and Crosby (1973).<ref name=\"Crosby 1973\">{{cite book|last=Crosby|first=Jack L.|year=1973|title=Computer Simulation in Genetics|publisher=John Wiley & Sons|location=London|isbn=978-0-471-18880-3}}</ref> Fraser's simulations included all of the essential elements of modern genetic algorithms. In addition, [[Hans-Joachim Bremermann]] published a series of papers in the 1960s that also adopted a population of solution to optimization problems, undergoing recombination, mutation, and selection. Bremermann's research also included the elements of modern genetic algorithms.<ref>[http://berkeley.edu/news/media/releases/96legacy/releases.96/14319.html 02.27.96 - UC Berkeley's Hans Bremermann, professor emeritus and pioneer in mathematical biology, has died at 69<!-- Bot generated title -->]</ref> Other noteworthy early pioneers include Richard Friedberg, George Friedman, and Michael Conrad. Many early papers are reprinted by [[David B. Fogel|Fogel]] (1998).<ref>{{cite book|last=Fogel|first=David B. (editor)|year=1998|title=Evolutionary Computation: The Fossil Record|publisher=IEEE Press|location=New York|isbn=978-0-7803-3481-6}}</ref>\n\nAlthough Barricelli, in work he reported in 1963, had simulated the evolution of ability to play a simple game,<ref>{{cite journal|last=Barricelli|first=Nils Aall|year=1963|title=Numerical testing of evolution theories. Part II. Preliminary tests of performance, symbiogenesis and terrestrial life|journal=Acta Biotheoretica|volume=16|issue=16|pages=99–126|doi=10.1007/BF01556602}}</ref> [[artificial evolution]] became a widely recognized optimization method as a result of the work of [[Ingo Rechenberg]] and [[Hans-Paul Schwefel]] in the 1960s and early 1970s &ndash; Rechenberg's group was able to solve complex engineering problems through [[Evolution strategy|evolution strategies]].<ref>{{cite book|last=Rechenberg|first=Ingo|year=1973|title=Evolutionsstrategie|place=Stuttgart|publisher=Holzmann-Froboog|isbn=978-3-7728-0373-4}}</ref><ref>{{cite book|last=Schwefel|first=Hans-Paul|year=1974|title=Numerische Optimierung von Computer-Modellen (PhD thesis)}}</ref><ref>{{cite book|last=Schwefel|first=Hans-Paul|year=1977|title=Numerische Optimierung von Computor-Modellen mittels der Evolutionsstrategie : mit einer vergleichenden Einführung in die Hill-Climbing- und Zufallsstrategie|place=Basel; Stuttgart | publisher=Birkhäuser| isbn=978-3-7643-0876-6}}</ref><ref>{{cite book|last=Schwefel|first=Hans-Paul|year=1981|title=Numerical optimization of computer models (Translation of 1977 Numerische Optimierung von Computor-Modellen mittels der Evolutionsstrategie|place=Chichester ; New York|publisher=Wiley|isbn=978-0-471-09988-8}}</ref>  Another approach was the evolutionary programming technique of [[Lawrence J. Fogel]], which was proposed for generating artificial intelligence. [[Evolutionary programming]] originally used finite state machines for predicting environments, and used variation and selection to optimize the predictive logics. Genetic algorithms in particular became popular through the work of [[John Henry Holland|John Holland]] in the early 1970s, and particularly his book ''Adaptation in Natural and Artificial Systems'' (1975). His work originated with studies of [[cellular automata]], conducted by [[John Henry Holland|Holland]] and his students at the [[University of Michigan]]. Holland introduced a formalized framework for predicting the quality of the next generation, known as [[Holland's Schema Theorem]]. Research in GAs remained largely theoretical until the mid-1980s, when The First International Conference on Genetic Algorithms was held in [[Pittsburgh, Pennsylvania]].\n\n===Commercial products===\nIn the late 1980s, General Electric started selling the world's first genetic algorithm product, a mainframe-based toolkit designed for industrial processes.<ref>{{Cite book|url=https://books.google.com/?id=-MszVdu_PAMC&dq=general+electric+genetic+algorithm+mainframe|title=An Approach to Designing an Unmanned Helicopter Autopilot Using Genetic Algorithms and Simulated Annealing|last=Aldawoodi|first=Namir|publisher=ProQuest|year=2008|isbn=978-0549773498|location=|pages=99|quote=|via=Google Books}}</ref> \nIn 1989, Axcelis, Inc. released [[Evolver (software)|Evolver]], the world's first commercial GA product for desktop computers. [[The New York Times]] technology writer [[John Markoff]] wrote<ref>{{cite news|last=Markoff|first=John|title=What's the Best Answer? It's Survival of the Fittest|newspaper=New York Times|url=https://www.nytimes.com/1990/08/29/business/business-technology-what-s-the-best-answer-it-s-survival-of-the-fittest.html|accessdate=2016-07-13|date=29 August 1990}}</ref> about Evolver in 1990, and it remained the only interactive commercial genetic algorithm until 1995.<ref>Ruggiero, Murray A.. (2009-08-01) [http://www.futuresmag.com/2009/08/01/fifteen-years-and-counting?t=technology&page=2 Fifteen years and counting]. Futuresmag.com. Retrieved on 2013-08-07.</ref> Evolver was sold to Palisade in 1997, translated into several languages, and is currently in its 6th version.<ref>[http://www.palisade.com/evolver/ Evolver: Sophisticated Optimization for Spreadsheets]. Palisade. Retrieved on 2013-08-07.</ref>\n\n== Related techniques ==\n{{See also|List of genetic algorithm applications}}\n\n===Parent fields===\nGenetic algorithms are a sub-field:\n*[[Evolutionary algorithms]]\n*[[Evolutionary computing]]\n*[[Metaheuristic]]s\n*[[Stochastic optimization]]\n*[[Optimization (mathematics)|Optimization]]\n\n===Related fields===\n\n====Evolutionary algorithms====\n{{Refimprove section|date=May 2011}}\n{{main|Evolutionary algorithm}}\nEvolutionary algorithms is a sub-field of [[Evolutionary Computation|evolutionary computing]].\n\n* [[Evolution strategy|Evolution strategies]] (ES, see Rechenberg, 1994) evolve individuals by means of mutation and intermediate or discrete recombination. ES algorithms are designed particularly to solve problems in the real-value domain.<ref>{{cite book|last=Cohoon|first=J|display-authors=etal|title=Evolutionary algorithms for the physical design of VLSI circuits|url= https://www.ifte.de/mitarbeiter/lienig/cohoon.pdf|journal=Advances in Evolutionary Computing: Theory and Applications|publisher= Springer, pp. 683-712, 2003|isbn=978-3-540-43330-9|date=2002-11-26}}</ref> They use self-adaptation to adjust control parameters of the search. De-randomization of self-adaptation has led to the contemporary Covariance Matrix Adaptation Evolution Strategy ([[CMA-ES]]).\n* [[Evolutionary programming]] (EP) involves populations of solutions with primarily mutation and selection and arbitrary representations. They use self-adaptation to adjust parameters, and can include other variation operations such as combining information from multiple parents.\n* [[Estimation of Distribution Algorithm]] (EDA) substitutes traditional reproduction operators by model-guided operators. Such models are learned from the population by employing machine learning techniques and represented as Probabilistic Graphical Models, from which new solutions can be sampled<ref>{{cite book|last1=Pelikan|first1=Martin|last2=Goldberg|first2=David E.|last3=Cantú-Paz|first3=Erick|title=BOA: The Bayesian Optimization Algorithm|journal=Proceedings of the 1st Annual Conference on Genetic and Evolutionary Computation - Volume 1|date=1 January 1999|pages=525–532|url=http://dl.acm.org/citation.cfm?id=2933973|isbn=9781558606111|series=Gecco'99}}</ref><ref>{{cite book|last1=Pelikan|first1=Martin|title=Hierarchical Bayesian optimization algorithm : toward a new generation of evolutionary algorithms|date=2005|publisher=Springer|location=Berlin [u.a.]|isbn=978-3-540-23774-7|edition=1st}}</ref> or generated from guided-crossover.<ref>{{cite book|last1=Thierens|first1=Dirk|title=The Linkage Tree Genetic Algorithm|journal=Parallel Problem Solving from Nature, PPSN XI|date=11 September 2010|pages=264–273|doi=10.1007/978-3-642-15844-5_27|language=en|isbn=978-3-642-15843-8}}</ref>\n* [[Gene expression programming]] (GEP) also uses populations of computer programs. These complex computer programs are encoded in simpler linear chromosomes of fixed length, which are afterwards expressed as expression trees. Expression trees or computer programs evolve because the chromosomes undergo mutation and recombination in a manner similar to the canonical GA. But thanks to the special organization of GEP chromosomes, these genetic modifications always result in valid computer programs.<ref>{{cite web|last=Ferreira|first=C|title=Gene Expression Programming: A New Adaptive Algorithm for Solving Problems|url= http://www.gene-expression-programming.com/webpapers/GEP.pdf|publisher= Complex Systems, Vol. 13, issue 2: 87-129.}}</ref>\n* [[Genetic programming]] (GP) is a related technique popularized by [[John Koza]] in which computer programs, rather than function parameters, are optimized. Genetic programming often uses [[Tree (data structure)|tree-based]] internal [[data structure]]s to represent the computer programs for adaptation instead of the [[List (computing)|list]] structures typical of genetic algorithms.\n* [[Grouping genetic algorithm]] (GGA) is an evolution of the GA where the focus is shifted from individual items, like in classical GAs, to groups or subset of items.<ref name=\"Falkenauer\">{{cite book|last=Falkenauer|first=Emanuel|authorlink=Emanuel Falkenauer|year=1997|title=Genetic Algorithms and Grouping Problems|publisher=John Wiley & Sons Ltd|location=Chichester, England|isbn=978-0-471-97150-4}}</ref> The idea behind this GA evolution proposed by [[Emanuel Falkenauer]] is that solving some complex problems, a.k.a. ''clustering'' or ''partitioning'' problems where a set of items must be split into disjoint group of items in an optimal way, would better be achieved by making characteristics of the groups of items equivalent to genes. These kind of problems include [[bin packing problem|bin packing]], line balancing, [[cluster analysis|clustering]] with respect to a distance measure, equal piles, etc., on which classic GAs proved to perform poorly. Making genes equivalent to groups implies chromosomes that are in general of variable length, and special genetic operators that manipulate whole groups of items. For bin packing in particular, a GGA hybridized with the Dominance Criterion of Martello and Toth, is arguably the best technique to date.\n* [[Interactive evolutionary algorithm]]s are evolutionary algorithms that use human evaluation. They are usually applied to domains where it is hard to design a computational fitness function, for example, evolving images, music, artistic designs and forms to fit users' aesthetic preference.\n\n====Swarm intelligence====\n{{main|Swarm intelligence}}\nSwarm intelligence is a sub-field of [[Evolutionary Computation|evolutionary computing]].\n\n* [[Ant colony optimization]] ('''ACO''') uses many ants (or agents) equipped with a pheromone model to traverse the solution space and find locally productive areas. Although considered an [[Estimation of distribution algorithm]],<ref>{{cite journal|last1=Zlochin|first1=Mark|last2=Birattari|first2=Mauro|last3=Meuleau|first3=Nicolas|last4=Dorigo|first4=Marco|title=Model-Based Search for Combinatorial Optimization: A Critical Survey|journal=Annals of Operations Research|date=1 October 2004|volume=131|issue=1–4|pages=373–395|doi=10.1023/B:ANOR.0000039526.52305.af|language=en|issn=0254-5330|citeseerx=10.1.1.3.427}}</ref>\n* [[Particle swarm optimization]] (PSO) is a computational method for multi-parameter optimization which also uses population-based approach. A population (swarm) of candidate solutions (particles) moves in the search space, and the movement of the particles is influenced both by their own best known position and swarm's global best known position. Like genetic algorithms, the PSO method depends on information sharing among population members. In some problems the PSO is often more computationally efficient than the GAs, especially in unconstrained problems with continuous variables.<ref>Rania Hassan, Babak Cohanim, Olivier de Weck, Gerhard Vente\nr (2005) [http://www.mit.edu/~deweck/PDF_archive/3%20Refereed%20Conference/3_50_AIAA-2005-1897.pdf A comparison of particle swarm optimization and the genetic algorithm]</ref>\n\n====Other evolutionary computing algorithms====\n\nEvolutionary computation is a sub-field of the [[metaheuristic]] methods.\n\n* [[Electimize algorithm]] is an evolutionary algorithm that simulates the phenomenon of electron flow and electrical conductivity. Some current research showed Electimize to be more efficient in solving NP-hard optimization problems than traditional evolutionary algorithms. The algorithm provides higher capacity in searching the solution space extensively, and identifying global optimal alternatives. Unlike other evolutionary algorithms, Electimize evaluates the quality of the values in the solution string independently. <ref>{{Cite journal|last=Khalafallah Ahmed|last2=Abdel-Raheem Mohamed|date=2011-05-01|title=Electimize: New Evolutionary Algorithm for Optimization with Application in Construction Engineering|url=https://ascelibrary.org/doi/full/10.1061/(ASCE)CP.1943-5487.0000080|journal=Journal of Computing in Civil Engineering|volume=25|issue=3|pages=192–201|doi=10.1061/(ASCE)CP.1943-5487.0000080}}</ref>\n* [[Memetic algorithm]] (MA), often called ''hybrid genetic algorithm'' among others, is a population-based method in which solutions are also subject to local improvement phases. The idea of memetic algorithms comes from [[meme]]s, which unlike genes, can adapt themselves. In some problem areas they are shown to be more efficient than traditional evolutionary algorithms.\n* [[Bacteriologic algorithm]]s (BA) inspired by [[evolutionary ecology]] and, more particularly, bacteriologic adaptation. Evolutionary ecology is the study of living organisms in the context of their environment, with the aim of discovering how they adapt. Its basic concept is that in a heterogeneous environment, there is not one individual that fits the whole environment. So, one needs to reason at the population level. It is also believed BAs could be successfully applied to complex positioning problems (antennas for cell phones, urban planning, and so on) or data mining.<ref>{{cite journal|url=http://www.irisa.fr/triskell/publis/2005/Baudry05d.pdf|first=Benoit|last=Baudry |author2=Franck Fleurey |author3=[[Jean-Marc Jézéquel]] |author4=Yves Le Traon|title=Automatic Test Case Optimization: A Bacteriologic Algorithm|date=March–April 2005|pages=76–82|journal=IEEE Software|issue=2|doi=10.1109/MS.2005.30|volume=22|accessdate=9 August 2009}}</ref>\n* [[Cultural algorithm]] (CA) consists of the population component almost identical to that of the genetic algorithm and, in addition, a knowledge component called the belief space.\n* [[Differential search algorithm]] (DS) inspired by migration of superorganisms.<ref>{{cite journal|last=Civicioglu|first=P.|title=Transforming Geocentric Cartesian Coordinates to Geodetic Coordinates by Using Differential Search Algorithm|journal=Computers &Geosciences|year=2012|volume=46|pages=229–247|doi=10.1016/j.cageo.2011.12.011}}</ref>\n* [[Gaussian adaptation]] (normal or natural adaptation, abbreviated NA to avoid confusion with GA) is intended for the maximisation of manufacturing yield of signal processing systems. It may also be used for ordinary parametric optimisation. It relies on a certain theorem valid for all regions of acceptability and all Gaussian distributions. The efficiency of NA relies on information theory and a certain theorem of efficiency. Its efficiency is defined as information divided by the work needed to get the information.<ref>{{cite journal|last=Kjellström|first=G.|title= On the Efficiency of Gaussian Adaptation|journal=Journal of Optimization Theory and Applications|volume=71|issue=3|pages=589–597|date=December 1991|doi= 10.1007/BF00941405}}</ref> Because NA maximises mean fitness rather than the fitness of the individual, the landscape is smoothed such that valleys between peaks may disappear. Therefore it has a certain \"ambition\" to avoid local peaks in the fitness landscape. NA is also good at climbing sharp crests by adaptation of the moment matrix, because NA may maximise the disorder ([[average information]]) of the Gaussian simultaneously keeping the [[mean fitness]] constant.\n\n====Other metaheuristic methods====\n\nMetaheuristic methods broadly fall within [[Stochastic optimization|stochastic]] optimisation methods.\n\n* [[Simulated annealing]] (SA) is a related global optimization technique that traverses the search space by testing random mutations on an individual solution. A mutation that increases fitness is always accepted. A mutation that lowers fitness is accepted probabilistically based on the difference in fitness and a decreasing temperature parameter. In SA parlance, one speaks of seeking the lowest energy instead of the maximum fitness. SA can also be used within a standard GA algorithm by starting with a relatively high rate of mutation and decreasing it over time along a given schedule.\n* [[Tabu search]] (TS) is similar to simulated annealing in that both traverse the solution space by testing mutations of an individual solution. While simulated annealing generates only one mutated solution, tabu search generates many mutated solutions and moves to the solution with the lowest energy of those generated. In order to prevent cycling and encourage greater movement through the solution space, a tabu list is maintained of partial or complete solutions. It is forbidden to move to a solution that contains elements of the tabu list, which is updated as the solution traverses the solution space.\n* [[Extremal optimization]] (EO) Unlike GAs, which work with a population of candidate solutions, EO evolves a single solution and makes [[local search (optimization)|local]] modifications to the worst components. This requires that a suitable representation be selected which permits individual solution components to be assigned a quality measure (\"fitness\"). The governing principle behind this algorithm is that of ''emergent'' improvement through selectively removing low-quality components and replacing them with a randomly selected component. This is decidedly at odds with a GA that selects good solutions in an attempt to make better solutions.\n\n====Other stochastic optimisation methods====\n\n* The [[Cross-entropy method|cross-entropy (CE) method]] generates candidates solutions via a parameterized probability distribution. The parameters are updated via cross-entropy minimization, so as to generate better samples in the next iteration.\n* Reactive search optimization (RSO) advocates the integration of sub-symbolic machine learning techniques into search heuristics for solving complex optimization problems. The word reactive hints at a ready response to events during the search through an internal online feedback loop for the self-tuning of critical parameters. Methodologies of interest for Reactive Search include machine learning and statistics, in particular [[reinforcement learning]], [[Active learning (machine learning)|active or query learning]], [[neural networks]], and [[metaheuristics]].\n\n==See also==\n* [[List of genetic algorithm applications]]\n* [[particle filter|Genetic algorithms in signal processing (a.k.a. particle filters)]]\n* [[Propagation of schema]]\n* [[Universal Darwinism]]\n* [[Metaheuristics]]\n* [[Learning classifier system]]\n* [[Rule-based machine learning]]\n\n== References ==\n{{Reflist|30em}}\n\n== Bibliography ==\n{{Refbegin}}\n* {{Cite book|title=Genetic Programming &ndash; An Introduction | last1=Banzhaf | first1=Wolfgang | last2=Nordin | first2=Peter | last3 = Keller | first3=Robert | last4=Francone | first4=Frank | year=1998 | isbn=978-1558605107 | publisher=Morgan Kaufmann | location=San Francisco, CA | ref=harv}}\n* {{Cite journal|last=Bies|first=Robert R. |last2=Muldoon |first2=Matthew F. |last3=Pollock |first3=Bruce G. |last4=Manuck |first4=Steven |last5=Smith |first5=Gwenn |last6=Sale |first6=Mark E. |year=2006|title=A Genetic Algorithm-Based, Hybrid Machine Learning Approach to Model Selection|journal=Journal of Pharmacokinetics and Pharmacodynamics|pages=196–221 |ref=harv}}\n* {{Cite journal|last=Cha|first=Sung-Hyuk|last2=Tappert |first2=Charles C. |year=2009|title=A Genetic Algorithm for Constructing Compact Binary Decision Trees|journal=Journal of Pattern Recognition Research |volume=4|issue=1|pages=1–13|url=http://www.jprr.org/index.php/jprr/article/view/44/25|doi=10.13176/11.44|citeseerx=10.1.1.154.8314 |ref=harv}}\n* {{Cite journal|last=Fraser|first=Alex S.|year=1957|title=Simulation of Genetic Systems by Automatic Digital Computers. I. Introduction|journal=Australian Journal of Biological Sciences|volume=10|issue=4|pages=484–491|doi=10.1071/BI9570484 |ref=harv}}\n* {{Cite book| last=Goldberg | first=David | year=1989 | title=Genetic Algorithms in Search, Optimization and Machine Learning | isbn=978-0201157673 | publisher=Addison-Wesley Professional | location=Reading, MA | ref=harv}}\n* {{Cite book| last=Goldberg | first=David | year=2002 | title=The Design of Innovation: Lessons from and for Competent Genetic Algorithms | publisher=Kluwer Academic Publishers | location=Norwell, MA | isbn=978-1402070983 | ref=harv}}\n* {{Cite book| last=Fogel | first=David | title=Evolutionary Computation: Toward a New Philosophy of Machine Intelligence | publisher=IEEE Press | location=Piscataway, NJ | edition=3rd | isbn=978-0471669517 | year=2006 |ref=harv}}\n* {{Cite book | last=Holland | first=John | title=Adaptation in Natural and Artificial Systems | publisher=MIT Press | location=Cambridge, MA | year=1992 | isbn=978-0262581110 | ref=harv}}\n* {{Cite book | last=Koza | first=John | title=Genetic Programming: On the Programming of Computers by Means of Natural Selection | year = 1992 | publisher=MIT Press | location=Cambridge, MA | isbn=978-0262111706 | ref=harv}}\n* {{Cite book | last=Michalewicz | first=Zbigniew | year=1996 | title=Genetic Algorithms + Data Structures = Evolution Programs | publisher=Springer-Verlag | isbn=978-3540606765 | ref=harv}}\n* {{Cite book | last=Mitchell | first=Melanie | title=An Introduction to Genetic Algorithms | year=1996 | publisher=MIT Press | location=Cambridge, MA | isbn = 9780585030944 | ref=harv}}\n* {{Cite book |last1=Poli |first1=R. |last2=Langdon |first2=W. B. |last3=McPhee |first3=N. F. |year=2008 |title=A Field Guide to Genetic Programming | publisher=Lulu.com, freely available from the internet | isbn = 978-1-4092-0073-4 |ref=harv}}\n* Rechenberg, Ingo (1994): Evolutionsstrategie '94, Stuttgart: Fromman-Holzboog.\n* {{cite journal |last1=Schmitt |first1=Lothar M. |last2=Nehaniv |first2=Chrystopher L. |last3=Fujii |first3=Robert H. |date=1998 |url=https://www.sciencedirect.com/science/article/pii/S0304397598000048/pdf?md5=28a658a4dc5aef635bbf3c8560129925&pid=1-s2.0-S0304397598000048-main.pdf&_valck=1 |title=Linear analysis of genetic algorithms |journal=Theoretical Computer Science |volume=208 |pages=111&ndash;148 |ref=harv}}\n* {{cite journal |last1=Schmitt |first1=Lothar M. |date=2001 |title=Theory of Genetic Algorithms |journal=Theoretical Computer Science |volume=259 |pages=1&ndash;61 |ref=harv}}\n* {{cite journal |last1=Schmitt |first1=Lothar M. |date=2004 |url=https://www.sciencedirect.com/science/article/pii/S0304397503003931/pdf?md5=7b975ebf9f658d581f84cc19e1db9cff&isDTMRedir=Y&pid=1-s2.0-S0304397503003931-main.pdf&_valck=1 |title=Theory of Genetic Algorithms II: models for genetic operators over the string-tensor representation of populations and convergence to global optima for arbitrary fitness function under scaling |journal=Theoretical Computer Science |volume=310 |pages=181&ndash;231 |ref=harv}}\n* Schwefel, Hans-Paul (1974): Numerische Optimierung von Computer-Modellen (PhD thesis). Reprinted by Birkhäuser (1977).\n* {{Cite book | last=Vose | first=Michael | year=1999 | title=The Simple Genetic Algorithm: Foundations and Theory | publisher=MIT Press | location=Cambridge, MA | isbn=978-0262220583 | ref=harv}}\n* {{Cite journal | last=Whitley | first=Darrell | title=A genetic algorithm tutorial | journal=Statistics and Computing | doi=10.1007/BF00175354 | volume=4 | issue=2 | pages=65–85 | year=1994 | ref=harv | url=http://www.cs.uga.edu/~potter/CompIntell/ga_tutorial.pdf | citeseerx=10.1.1.184.3999 }}<!--| accessdate=5 January 2013-->\n* {{Cite book | last1=Hingston | first1=Philip | last2=Barone | first2=Luigi | last3=Michalewicz | first3=Zbigniew | title=Design by Evolution: Advances in Evolutionary Design | year=2008 | publisher=Springer | isbn=978-3540741091 | ref=harv}}\n* {{Cite book | last1=Eiben | first1=Agoston | last2=Smith | first2=James | year=2003 | title=Introduction to Evolutionary Computing | publisher=Springer | isbn=978-3540401841 | ref=harv}}\n{{Refend}}\n\n== External links ==\n\n=== Resources ===\n* [https://web.archive.org/web/20160303215222/http://www.geneticprogramming.com/ga/index.htm] Provides a list of resources in the genetic algorithms field\n\n=== Tutorials ===\n*[http://diegogiacomelli.com.br/function-optimization-with-geneticsharp/ Function optimization with GeneticSharp]. An tutorial to learn GA using .NET (C#).\n*[http://www2.econ.iastate.edu/tesfatsi/holland.gaintro.htm Genetic Algorithms - Computer programs that \"evolve\" in ways that resemble natural selection can solve complex problems even their creators do not fully understand] An excellent introduction to GA by John Holland and with an application to the Prisoner's Dilemma\n* [http://www.i4ai.org/EA-demo/ An online interactive Genetic Algorithm tutorial for a reader to practise or learn how a GA works]: Learn step by step or watch global convergence in batch, change the population size, crossover rates/bounds, mutation rates/bounds and selection mechanisms, and add constraints.\n* [https://web.archive.org/web/20130615042000/http://samizdat.mines.edu/ga_tutorial/ga_tutorial.ps A Genetic Algorithm Tutorial by Darrell Whitley Computer Science Department Colorado State University] An excellent tutorial with lots of theory\n* [http://cs.gmu.edu/~sean/book/metaheuristics/ \"Essentials of Metaheuristics\"], 2009 (225 p). Free open text by Sean Luke.\n* [http://www.it-weise.de/projects/book.pdf Global Optimization Algorithms &ndash; Theory and Application]\n* [https://mpatacchiola.github.io/blog/2017/03/14/dissecting-reinforcement-learning-5.html Genetic Algorithms in Python] Tutorial with the intuition behind GAs and Python implementation.\n* [http://www-personal.umich.edu/~axe/research/Evolving.pdf Genetic Algorithms evolves to solve the prisoner's dilemma.] Written by Robert Axelrod.\n\n{{Authority control}}\n\n{{DEFAULTSORT:Genetic Algorithm}}\n[[Category:Genetic algorithms| ]]\n[[Category:Evolutionary algorithms]]\n[[Category:Search algorithms]]\n[[Category:Cybernetics]]\n[[Category:Digital organisms]]\n[[Category:Machine learning]]\n\n[[de:Genetische Algorithmen]]\n[[sv:Genetisk programmering#Genetisk algoritm]]"
    },
    {
      "title": "Genetic representation",
      "url": "https://en.wikipedia.org/wiki/Genetic_representation",
      "text": "{{Refimprove|date=December 2009}}\nIn [[computer programming]], '''genetic representation''' is a way of representing solutions/individuals in [[evolutionary computation]] methods. Genetic representation can encode appearance, behavior, physical qualities of individuals. Designing a good genetic representation that is expressive and evolvable is a hard problem in evolutionary computation. Difference in genetic representations is one of the major criteria drawing a line between known classes of evolutionary computation.\n\nTerminology often comes by analogy with natural [[genetics]]. The block of computer memory that represents one candidate solution is called an individual. The data in that block is called a chromosome. Each chromosome consists of genes. The possible values of a particular gene are called [[allele]]s. A programmer may represent all the individuals of a population using ''binary encoding'', ''permutational encoding'', ''encoding by tree'', or any one of several other representations.<ref>\nTomáš Kuthan and Jan Lánský.\n[http://ceur-ws.org/Vol-235/paper3.pdf \"Genetic Algorithms in Syllable-Based Text Compression\"].\n2007.\np. 26.\n</ref>\n\n[[Genetic algorithm]]s use linear binary representations. The most standard one is an array of [[bit]]s. Arrays of other types and structures can be used in essentially the same way. The main property that makes these genetic representations convenient is that their parts are easily aligned due to their fixed size. This facilitates simple crossover operation. Variable length representations were also explored in [[Genetic algorithm]]s, but crossover implementation is more complex in this case.\n\n[[Evolution strategy]] uses linear real-valued representations, e.g. an array of real values. It uses mostly [[gaussian]] mutation and blending/averaging crossover.\n\n[[Genetic programming]] (GP) pioneered tree-like representations and developed [[genetic operator]]s suitable for such representations. Tree-like representations are used in GP to represent and evolve functional programs with desired properties.<ref>[http://www.sover.net/~nichael/nlc-publications/icga85/index.html A Representation for the Adaptive Generation of Simple Sequential Programs], Nichael Lynn Cramer, ''Proceedings of an International Conference on Genetic Algorithms and their Applications'' (1985), pp.&nbsp;183-187</ref>\n\n[[Human-based genetic algorithm]] (HBGA) offers a way to avoid solving hard representation problems by outsourcing all genetic operators to outside agents, in this case, humans. The algorithm has no need for knowledge of a particular fixed genetic representation as long as there are enough external agents capable of handling those representations, allowing for free-form and evolving genetic representations.\n\n==Common genetic representations==\n* [[Genetic algorithm|binary array]]\n* [[binary tree]]\n* [[HBGA|natural language]]\n* [[parse tree]]\n* [[directed graph]]\n\n== References and notes ==\n{{reflist}}\n\n{{DEFAULTSORT:Genetic Representation}}\n[[Category:Evolutionary algorithms]]"
    },
    {
      "title": "GenI process",
      "url": "https://en.wikipedia.org/wiki/GenI_process",
      "text": "The '''GenI process'''<ref>{{Cite book|url=https://www.worldcat.org/oclc/1011107944|title=The Source of the Universe|last=Genreith|first=Siegfried|publisher=Books on Demand|others=|year=2017|isbn=9783848223572|location=Norderstedt|pages=44|oclc=}}</ref> ({{IPAc-en|dʒ|iː|n|aɪ}} for Generic Intelligence) models team decision making by launching a chaotic competition between so-called ideas, where one idea after another dwindles out until exactly one remains. The decision statistics are completely determined by the process start configuration and match those known from [[Measurement in quantum mechanics|quantum measurements]]. The random process accomplishes this in a natural way without any fine adjustment. Therefore, it is also of interest for the interpretation of physical processes.\n\nIn mathematical terms, the GenI process describes a time-discrete [[stochastic process]] <math>X: [0,1] \\times \\mathbb{N}_0 \\rightarrow 2^E</math> in the state space of the [[Set (mathematics)|finite subsets]] of a [[countable set]] ''E'', together with a mapping <math>2^E \\rightarrow \\mathbb{C}^n</math>  of the [[power set]] on ''E'' into an [[Vector space|n-dimensional complex vector space]]. In principle, it can be classified as a [[Markov chain]] of first order, with variable [[Markov chain|transition probabilities]] <math>P(X_{n+1} \\mid X_n)</math>.\n\nThere are two expressions of the GenI process, for <math>n \\geq 2</math> and for <math>n=2</math>.\n\n== GenI process on eigenvector swarms ==\n\n=== Background ===\nThe GenI [[Stochastic process|random process]] determines changes in the complex vector space from the random behavior of independent individuals within a [[Swarm behaviour|swarm]]-like construct. The swarm has a [[Quantum superposition|superposition]] state <math>\\sum_{j=1}^n \\beta_j e_j \\in \\mathbb{C}^n</math>, that controls the individual activities via a target function. The [[amplitude]]s <math>\\beta_j e_j</math> are also called ideas (cf \"Generalized Quantum Modeling\" <ref name=\"Gabora 2017\">{{Cite journal|last=Gabora|first=Liane|last2=Kitto|first2=Kirsty|date=2017|title=Toward a Quantum Theory of Humor|url=http://journal.frontiersin.org/article/10.3389/fphy.2016.00053/full|journal=Frontiers in Physics|language=English|volume=4|doi=10.3389/fphy.2016.00053|issn=2296-424X}}</ref>). The swarm takes one of the [[Eigenvalues and eigenvectors|eigenstates]] <math>\\gamma e_j</math>  after a finite number of steps, with the well-defined [[probability]] <math>\\frac {\\vert{\\beta_j}\\vert^2} {\\sum_k {\\vert \\beta_k \\vert}^2}</math>. The individuals follow defined rules and are allowed to make mistakes, based on the processes in simulated shoals of fish.<ref name=\"COUZIN\">{{Cite journal|last=COUZIN|first=IAIN D.|last2=KRAUSE|first2=JENS|last3=JAMES|first3=RICHARD|last4=RUXTON|first4=GRAEME D.|last5=FRANKS|first5=NIGEL R.|authorlink5=Nigel R. Franks|title=Collective Memory and Spatial Sorting in Animal Groups|url=http://linkinghub.elsevier.com/retrieve/pii/S0022519302930651|journal=Journal of Theoretical Biology|volume=218|issue=1|pages=1–11|doi=10.1006/jtbi.2002.3065}}</ref> The GenI [[algorithm]] starts a [[Chaotics|chaotic]] [[decision-making]] process as a [[competition]] of ideas, such as running in a team that has to choose from a limited number of solutions to a given task. In the course of the process, a selection mechanism leads to ideas becoming extinct one after the other until finally just one survives that represents the solution to the problem.\n\n=== Definition ===\n\n==== Terminology ====\n[[File:The_GenI_Process.jpg|alt=The GenI Process|thumb|The GenI Process: A GenI swarm constantly sucks null rings from its environment and burns them randomly. The GenI process sets a gradient towards reducing the excitation. The swarm's state finally arrives at one of the given options.]]\nLet ''E'' be a countable set and <math>\\tilde E := \\{ S \\subset E : \\vert S \\vert < \\infty \\} \\subset 2^E</math> the set of finite subsets of ''E''. Next <math>B = (e_1, \\ldots, e_n)</math> the [[canonical basis]] in <math>\\mathbb{C}^n</math> and <math>\\tilde B :=\n\\{i^k e_j : k = 0 \\ldots 3, j = 1 \\ldots n \\}</math>, where ''i'' is the [[imaginary unit]] in <math>\\mathbb{C}</math>.\n\nA given <math>\\rho : E  \\rightarrow \\tilde B</math> maps each element of ''E'' into <math>\\tilde B</math>, so that <math>\\forall e \\in \\tilde B : \\vert \\rho^{-1} (\\{e\\}) \\vert = \\infty</math>. For a set <math>S \\in \\tilde E</math>  the complex vector <math>\\rho(S) := \\sum_{s \\in S} \\rho(s)\n= \\sum_j \\beta_j e_j</math> denotes its state with complex '''amplitudes''' <math>\\beta_j</math>. Each such set ''S'' is called an Eigenvector-swarm or '''E-swarm'''.\n\nA [[Ordered pair|pair]] <math>s, t \\in E</math> with <math>\\rho(s) + \\rho(t) = 0</math> is a '''null pair'''. A [[tuple]] <math>(s_0, s_1, s_2, s_3)</math> is called a '''null ring''' generated by <math>s_0</math>, if <math>\\exists j : \\rho(s_k) = i^k e_j</math>.\n\nA set <math>N \\in \\tilde E</math> is called a '''null set''', if <math>\\rho(N) = 0</math>. A maximal null set <math>N \\subset S</math> is called the '''entropy''' of ''S'' and <math>S \\setminus N</math> the '''entropy free residual swarm'''.\n\nThe term <math>\\varepsilon_j (S) := 2 \\frac {\\vert \\beta_j \\vert \\sqrt{\\sum_{k \\neq j} \\vert \\beta_k \\vert^2}}\n{\\sum_{k=1}^n \\vert \\beta_k\\vert^2} \\in [0; 1]</math> denotes the '''excitation''' of the swarm in index ''j''.\n\n==== Algorithm ====\nLet <math>S^{(l)} = S_D^{(l)} + N_S^{(l)}</math> be a series of swarms (as an instantiation of <math>X(\\omega, l)</math>) with the respective separation into a maximal null swarm <math>N_S^{(l)}</math> and the entropy free residual swarm <math>S_D^{(l)}</math>, <math>\\rho(S^{(l)}) =\\sum_{k=1}^{n} \\beta_k^{(l)} e_k</math> the respective state and <math>\\varepsilon_j^{(l)} := \\varepsilon_j(S^{(l)})</math> the excitations.\n# '''Step''': Set <math>l \\leftarrow 0</math> and start with a given swarm <math>S^{(0)}</math>.\n# '''Step''': If <math>\\forall j : \\varepsilon_j^{(l)} = 0</math>, then finish the process. \n# '''Step''': Each element <math>s  \\in S_D^{(l)}</math> generates an additional null ring within the swarm.\n# '''Step''': Each null pair <math>r,t \\in N_S^{(l)}</math> (including the newly generated) with <math>\\rho( r)= i^k e_j</math>, <math>\\rho( t)= - i^k e_j</math> gets selected with probability <math>p= \\varepsilon_j^{(l) 2}</math> (and \"burned\" in the next step). \n# '''Step''': For each selected null pair <math>r,t \\in N_S^{(l)}</math>, ''t'' leaves the swarm with probability <math>\\frac {\\varepsilon_j(S \\setminus \\{r\\})} {\\varepsilon_j(S \\setminus \\{r\\}) + \\varepsilon_j(S \\setminus \\{t\\})}</math>. Otherwise ''t'' stays and ''r'' leaves the swarm.\n# '''Step''': The resulting swarm is <math>S^{(l+1)}</math>.\n# '''Step''': Set <math>l \\leftarrow l+1</math> and start over with step 2.\n\n==== Interpretation ====\n[[File:Competition_of_ideas_within_an_GenI_swarm.jpg|alt=Competition of ideas within an GenI swarm|thumb|Competition of ideas within an GenI swarm: Graph a shows the evolution of absolute amplitudes during a GenI process operating in a four options environment. Due to its intrinsically chaotic behavior, it is impossible to predict the GenI process evolution at any point. The option with the lowest initial chance finally wins.\nChart b shows the according evolution of entropy rising dramatically at the end for the winning idea. Figure c displays the native paths of each idea in the complex plane.\n\n]]\nAs soon as the excitation disappears in each index, the process naturally comes to rest in step 4 (except for the hard abort condition in step 2), since no null pair is \"burned\" and the state of the swarm no longer changes. The role of the excitation here reminds of the dynamics of a grain of sand in the formation of the [[Ernst Chladni|Chladnic sound figures]]. On the other hand, excitation as a target in step 5 leads to a systematic distortion from 50% likelihood for an individual to remain. This leads here to an improved tendency to reduce the excitation. The following interpretation is obvious based on biological swarm behavior:<ref name=\"COUZIN\" /> Each individual tends to follow the rule \"reduce the excitation\". It remains free in its decision to do nothing (step 4), to follow the rule, or to disregard it (step 5).\n\n=== Simulation ===\nThe reference implementation under JAVA<ref name=\"WSG 2018\">{{Citation|last=WSG|title=BZuS: simulation software and test data for the GenI model (Java sources)|date=2018-01-17|url=https://github.com/genreith/BZuS|accessdate=2018-02-14}}</ref> shows an excellent convergence of the process. The table shows an example of the result of 1000 simulation runs (each simulation run aborts after more than 500 iterations or for swarm sizes> 10 million for performance reasons):\n\n{| class=\"wikitable\"\n!\n!<math>e_1</math>\n!<math>e_2</math>\n!<math>e_3</math>\n!<math>e_4</math>\n!<math>e_5</math>\n!<math>e_6</math>\n!<math>e_7</math>\n!<math>e_8</math>\n!<math>e_9</math>\n!<math>e_{10}</math>\n|-\n| style=\"width: 5em\" |target\n| style=\"width: 5em\" |132\n| style=\"width: 5em\" |81\n| style=\"width: 5em\" |97\n| style=\"width: 5em\" |78\n| style=\"width: 5em\" |11\n| style=\"width: 5em\" |206\n| style=\"width: 5em\" |3\n| style=\"width: 5em\" |336\n| style=\"width: 5em\" |36\n| style=\"width: 5em\" |3\n|-\n|frequency\n|135\n|74\n|99\n|76\n|15\n|189\n|1\n|357\n|36\n|1\n|-\n|[[Variance|sigma]]\n|10.7\n|8.6\n|9.4\n|8.5\n|3.3\n|12.8\n|1.8\n|14.9\n|5.9\n|1.8\n|-\n| colspan=\"3\" |measurements scheduled\n|1000\n| colspan=\"2\" |of it divergent\n|17\n| colspan=\"3\" |convergent\n|983\n|-\n| colspan=\"11\" |statistics:\n\n[[Chi-squared test|chi square]] value: 7.85 ;                         [[Chi-squared test|chi critical]] value at 95% confidence: 16.9\n|-\n| colspan=\"5\" |medium swarm size: 300,418\n| colspan=\"2\" |sigma: 281,543\n| colspan=\"2\" |maximal: 1,008,512\n| colspan=\"2\" |minimal: 9,695\n|}\n\nThese results support the '''statement of convergence (hypothesis)''':\n\nLet <math>S^{(0)} = S</math> be a given E-swarm with <math>\\rho(S) = \\sum_{j=1}^n \\beta_j a_j</math>, <math>n \\ge 2</math>, <math>b_j =\n\\vert \\beta_j \\vert</math>,  <math>\\vert S \\vert = \\sqrt{\\sum b_j^2}</math>.\n\nLet <math>S : \\mathbb{N}_0 \\times [0; 1] \\rightarrow \\tilde E</math> be a GenI process with  <math>\\rho ( S^{(m)}(\\omega))=\\sum \\beta_j^{(m)}(\\omega) a_j</math>.\n\nThen <math>P \\left(\\tilde S^{(m)} \\underset{m \\rightarrow \\infty}\\longrightarrow \\gamma a_j\n\\right) =P \\left(\\sum_{k \\ne j} b_k^{(m) 2} \\underset{m \\rightarrow\n\\infty}\\longrightarrow 0 \\right) = \\frac{b_j^2}{\\vert {S} \\vert^2}.</math>\n\n== GenI process on Pauli swarms ==\nThe P(auli) process represents a special version of the GenI process for binary YES-NO decisions. It determines a time discrete [[stochastic process]] <math>X: [0;1] \\times \\mathbb{N}_0 \\mapsto 2^E</math> in the state space of finite subsets of a countable set E, together with a map <math>\\rho : 2^E \\rightarrow \\mathbb{C}^{2 \\times 2}</math> from the power set on E into the [[Complex number|complex]] [[Matrix ring|matrix algebra]] <math>\\mathbb{C}^{2 \\times 2}</math>, a perspective vector <math>v \\in \\mathbb{C}^2</math> and a basis <math>a_1, a_2 \\in \\mathbb{C}^2</math>.\n\n=== Background ===\n[[File:Competition within a GenI-swarm.jpg|alt=Competition within a GenI-swarm|thumb|Competition within a GenI-swarm: The diagrams a-c demonstrate the Pauli process (as a special expression of the GenI process) evolution using a fixed perspective p = (1;1) under an external environment defined by the observable p3.\nDiagrams d-f show another test case under the internal environment defined by the swarm itself. Graphs a/d show the evolution of absolute amplitudes. Charts b/e show the according evolution of entropy for each type according to the swarm member images in {p0, ... , p3}. Null pairs for P-swarm do not relate to environment options as is true for E-swarms. Figures c/f display the native paths of each idea in the complex plane.]]\nThe GenI random process determines changes in the complex vector space out of the random behavior of independent individuals within a swarm-like construct. The swarm ''S'' has an image <math> \\rho(S) \\in  \\mathbb{C}^{2 \\times 2}</math> in the complex matrix algebra. Together with a perspective <math>v \\in \\mathbb{C}^2</math>, the state of the swarm is determined by <math>\\tilde S = \\rho(S) v</math>. A basis <math>a_1, a_2 \\in \\mathbb{C}^2</math>  is called the environment and represents the options under which the swarm makes a decision. This can also be the [[Eigenvalues and eigenvectors|eigenvectors]] of <math> \\rho(S) \\in  \\mathbb{C}^{2 \\times 2}</math> itself and so create a [[self-reference]]. The environment, perspective and state of the swarm control the individual activities via a target variable (excitation). The amplitudes <math>\\beta_j a_j</math> in the decomposition are also referred to as ideas with respect to the environment (see regarding \"Generalized Quantum Modeling\" <ref name=\"Gabora 2017\" />). The swarm takes one of the eigenstates <math>\\gamma a_j</math>  after a finite number of steps, with the well-defined probability <math>\\frac {\\vert{\\beta_j}\\vert^2}  {{\\vert \\beta_1 \\vert}^2 + {\\vert \\beta_2 \\vert}^2}</math>. The individuals follow defined rules and are allowed to make mistakes, based on the processes in simulated shoals of fish.<ref name=\"COUZIN\" /> The GenI algorithm starts a chaotic decision-making process as a competition of ideas, such as running in a team that has two possible solutions to a given task. In the course of the process, a selection mechanism leads to the survival of only one of the two ideas that represents the solution to the problem. The model in principal allows a moving environment.\n\nThe special properties of the P-process also allow interpretations of physical processes (see in particular [[Carl Friedrich von Weizsäcker]]'s ur-alternatives (archetypal objects),<ref>{{citation|surname1=Weizsäcker, Carl Friedrich, Freiherr von, 1912–2007.|title=Aufbau der Physik|publisher=C. Hanser|publication-place=München|isbn=3446141421|date=1985|language=German}}</ref> which he outlined for the reconstruction of quantum mechanics).\n\n=== Definition ===\n\n==== Terminology ====\nLet ''E'' be a countable set and <math>\\tilde E := \\{ S \\subset E : \\vert S \\vert < \\infty \\} \\subset 2^E</math> the set of finite subsets of ''E''. Next <math>P = (p_0, p_1, p_2, p_3)\n\\subset \\mathbb{C}^{2 \\times 2}</math> the [[Pauli matrices]] and <math>\\tilde P :=\n\\{i^k p_j : k = 0, \\ldots, 3, j = 1, \\ldots, n \\}</math> the image of the '''[[Pauli matrices|Pauli group]]''' in the complex matrix algebra as its irreducible representation. Such a subset <math>S \\in \\tilde E</math> is called a Pauli-swarm or a '''P-swarm'''.\n\nA given <math>\\rho : E  \\rightarrow \\tilde P</math> maps each element of ''E'' onto one element of the Pauli group, so that <math>\\forall e \\in \\tilde P : \\vert \\rho^{-1} (\\{e\\}) \\vert = \\infty</math>.\n\nA basis <math>a_1, a_2 \\in \\mathbb{C}^2</math> is called an '''environment''', a non zero vector <math>v \\in \\mathbb{C}^2</math> a '''perspective'''.\n\nFor a swarm <math>S \\in \\tilde E</math> denotes <math>\\rho(S)  := \\sum_{s \\in S} \\rho(s)</math> its matrix image, <math>\\tilde S = \\rho(S) v := \\beta_1 a_1 + \\beta_2 a_2</math> its '''state''' with complex '''amplitudes''' <math>\\beta_j</math> at the given environment and perspective.\n\nA [[Ordered pair|pair]] <math>s, t \\in E</math> with <math>\\rho(s) + \\rho(t) = 0</math> is a '''null pair'''. A [[Tuple (mathematics)|tuple]] <math>(s_0, s_1, s_2, s_3)</math> is called a '''null ring''' generated by <math>s_0</math>, if <math>\\exists j : \\rho(s_k) = i^k p_j</math>.\n\nA set <math>N \\in \\tilde E</math> is called '''null set''', if <math>\\rho(N) = 0</math>. A maximal null set <math>N \\subset S</math> ist called the '''entropy''' of ''S'' and <math>S \\setminus N</math> its '''entropy freed residual swarm'''.\n\nThe term <math>\\varepsilon (S) := 2 \\frac {\\vert \\beta_1 \\vert   \\vert \\beta_2 \\vert}\n{\\vert \\beta_1\\vert^2 +  \\vert \\beta_2\\vert^2} \\in [0; 1]</math> denotes the '''excitation''' of the swarm.\n\n==== Algorithm ====\nLet <math>S^{(l)} = S_D^{(l)} + N_S^{(l)}</math> be a series of swarms (as an instance von <math>X(\\omega, l)</math>) with the respective separation in a maximal null swarm <math>N_S^{(l)}</math> and the entropy freed residual swarm <math>S_D^{(l)}</math>, <math>\\tilde S^{(l)} =\\beta_1^{(l)} a_1 + \\beta_2^{(l)} a_2</math> the respective states and <math>\\varepsilon^{(l)} := \\varepsilon(S^{(l)})</math> the excitations.\n# '''Step''': Set <math>l \\leftarrow 0</math> and start with a given swarm <math>S^{(0)}</math>.\n# '''Step''': If <math> \\varepsilon^{(l)} = 0</math>, then finish the process.\n# '''Step''': Each element <math>s  \\in S_D^{(l)}</math> generates an additional null ring within the swarm.\n# '''Step''': Each null pair <math>r,t \\in N_S^{(l)}</math> (including the newly generated) gets selected with probability <math>p= \\varepsilon^{(l) 2}</math> (and gets \"burned\" in next step).\n# '''Step''': For each selected null pair <math>r,t \\in N_S^{(l)}</math>, ''t'' leaves the swarm with probability <math>\\frac {\\varepsilon(S \\setminus \\{r\\})} {\\varepsilon(S \\setminus \\{r\\}) + \\varepsilon(S \\setminus \\{t\\})}</math>. Otherwise ''t'' stays and ''r'' leaves the swarm.\n# '''Step''': The resulting swarm is <math>S^{(l+1)}</math>.\n# '''Step''': Set <math>l \\leftarrow l+1</math> and start over with step 2.\n\n=== Simulation ===\n[[File:GenI swarms observations along target values from 0 to 1.jpg|alt=GenI swarms observations along target values from 0 to 1|thumb|GenI swarms observations along target values from 0 to 1: The sample comprises 1000 measurements each on 201 test points on P(auli)-swarms (a special expression of a Geni swarm). More than 97% of observed frequencies are in the 2 sigma interval around the target values expected from quantum measurements. The chi square test value 92.6 is much lower than the critical level 168 at 95% confidence and 200 degrees of freedom.]]\nThe reference implementation under JAVA<ref name=\"WSG 2018\" /> shows an excellent convergence of the process at any fixed environment and perspective according to the chart right hand.\n\nThe results support the '''statement of convergence (hypothesis)''':\n\nLet <math>S^{(0)} = S</math> be a given P-swarm with <math>\\tilde S = \\beta_1 a_1 + \\beta_2 a_2</math>, <math>b_j =\n\\vert \\beta_j \\vert</math>,  <math>\\vert S \\vert = \\sqrt{b_1^2 + b_2^2}</math>, at any fixed environment <math>(a_1, a_2)</math>and perspective ''v''.\n\nLet <math>S : \\mathbb{N}_0 \\times [0; 1] \\rightarrow \\tilde E</math> be a P-process with  <math>\\tilde S^{(m)}(\\omega)=\\beta_1^{(m)}(\\omega) a_1 + \\beta_2^{(m)}(\\omega) a_2</math>.\n\nThen <math>P \\left(\\tilde S^{(m)} \\underset{m \\rightarrow \\infty}\\longrightarrow \\gamma a_j\n\\right) =P \\left(\\sum_{k \\ne j} b_k^{(m) 2} \\underset{m \\rightarrow\n\\infty}\\longrightarrow 0 \\right) = \\frac{b_j^2}{\\vert {S} \\vert^2}</math>, where <math>j,k \\in \\{1,2\\}</math>.\n\n== References ==\n<references />\n\n[[Category:Artificial intelligence]]\n[[Category:Evolutionary algorithms]]\n[[Category:Stochastic processes]]"
    },
    {
      "title": "Grammatical evolution",
      "url": "https://en.wikipedia.org/wiki/Grammatical_evolution",
      "text": "{{Evolutionary algorithms}}\n'''Grammatical evolution''' is a relatively new [[evolutionary computation]] technique pioneered by Conor Ryan, JJ Collins and Michael O'Neill in 1998<ref>http://www.cs.bham.ac.uk/~wbl/biblio/gp-html/ryan_1998_geepal.html</ref> at the [http://bds.ul.ie BDS Group] in the [[University of Limerick]].\n\nIt is related to the idea of [[genetic programming]] in that the objective is to find an executable program or program fragment, that will achieve a good fitness value for the given [[objective function]]. In most published work on Genetic Programming, a [[LISP]]-style tree-structured expression is directly manipulated, whereas Grammatical Evolution applies [[genetic operator]]s to an integer string, subsequently mapped to a program (or similar) through the use of a grammar. One of the benefits of GE is that this mapping simplifies the application of search to different programming languages and other structures.\n\n== Problem addressed ==\n\nIn type-free, conventional [[John Koza|Koza]]-style GP, the function set must meet the requirement of closure: all functions must be capable of accepting as their arguments the output of all other functions in the function set.  Usually, this is implemented by dealing with a single data-type such as double-precision floating point. While modern Genetic Programming frameworks support typing, such type-systems have limitations that Grammatical Evolution does not suffer from.\n\n== GE's solution ==\n\nGE offers a solution to this {{which|date=January 2015}} issue by evolving solutions according to a user-specified grammar (usually a grammar in [[Backus-Naur form]]). Therefore the search space can be restricted, and domain knowledge of the problem can be incorporated. The inspiration for this approach comes from a desire to separate the \"genotype\" from the \"phenotype\": in GP, the objects the search algorithm operates on and what the fitness evaluation function interprets are one and the same. In contrast, GE's \"genotypes\" are ordered lists of integers which code for selecting rules from the provided context-free grammar. The phenotype, however, is the same as in Koza-style GP: a tree-like structure that is evaluated recursively. This model is more in line with how genetics work in nature, where there is a separation between an organism's genotype and the final expression of phenotype in proteins, etc.\n\nGE has a modular approach to it. In particular, the search portion of the GE paradigm needn't be carried out by any one particular algorithm or method. Observe that the objects GE performs search on are the same as that used in [[genetic algorithms]]. This means, in principle, that any existing genetic algorithm package, such as the popular [http://lancet.mit.edu/ga/ GAlib], can be used to carry out the search, and a developer implementing a GE system need only worry about carrying out the mapping from list of integers to program tree. It is also in principle possible to perform the search using some other method, such as [[particle swarm optimization]] (see the remark below); the modular nature of GE creates many opportunities for hybrids as the problem of interest to be solved dictates.\n\nBrabazon and O'Neill have successfully applied GE to predicting corporate bankruptcy, forecasting stock indices, [[bond credit rating]]s, and other financial applications.{{citation needed|date=January 2015}} GE has also been used with a classic [[predator-prey model]] to explore the impact of parameters such as predator efficiency, niche number, and random mutations on [[ecological stability]].<ref>{{cite journal|last1=Alfonseca|first1=Manuel|last2=Soler Gil|first2=Francisco José|title=Evolving a predator-prey ecosystem of mathematical expressions with grammatical evolution|journal=Complexity|date=2 January 2015|volume=20|issue=3|pages=66–83|doi=10.1002/cplx.21507}}</ref>\n\nIt is possible to structure a GE grammar that for a given function/terminal set is equivalent to genetic programming.\n\n== Criticism ==\n\nDespite its successes, GE has been the subject of some criticism. One issue is that as a result of its mapping operation, GE's genetic operators do not achieve high locality<ref name=\"rothlauf:2006\">http://www.springerlink.com/content/0125627h52766534/</ref><ref>http://www.cs.kent.ac.uk/pubs/2010/3004/index.html</ref> which is a highly regarded property of genetic operators in evolutionary algorithms.<ref name=\"rothlauf:2006\" />\n\n== Variants ==\n\nAlthough GE is fairly new, there are already enhanced versions and variants that have been worked out. GE researchers have experimented with using [[particle swarm optimization]] to carry out the searching instead of genetic algorithms with results comparable to that of normal GE; this is referred to as a \"grammatical swarm\"; using only the basic PSO model it has been found that PSO is probably equally capable of carrying out the search process in GE as simple genetic algorithms are. (Although PSO is normally a floating-point search paradigm, it can be discretized, e.g., by simply rounding each vector to the nearest integer, for use with GE.)\n\nYet another possible variation that has been experimented with in the literature is attempting to encode semantic information in the grammar in order to further bias the search process.\n\n==See also==\n* [[Genetic programming]]\n* [[Java Grammatical Evolution]]\n\n==Notes==\n\n<references/>\n\n=== Resources ===\n\n* [https://web.archive.org/web/20110721124315/http://www.grammaticalevolution.org/tutorial.pdf Grammatical Evolution Tutorial].\n* [http://ncra.ucd.ie/geva Grammatical Evolution in Java].\n* [https://web.archive.org/web/20101129085227/http://www.bangor.ac.uk/~eep201/jge/ jGE - Java Grammatical Evolution].\n* [http://bds.ul.ie The Biocomputing and Developmental Systems (BDS) Group] at the [http://www.ul.ie University of Limerick].\n* [http://www.grammatical-evolution.org Michael O'Neill's Grammatical Evolution Page], including a bibliography.\n* [http://drp.rubyforge.org/ DRP], Directed Ruby Programming, is an experimental system designed to let users create hybrid GE/GP systems. It is implemented in pure Ruby.\n* [http://geret.org/ GERET], Grammatical Evolution Ruby Exploratory Toolkit.\n* [https://cran.r-project.org/web/packages/gramEvol/index.html gramEvol], Grammatical Evolution for [[R (programming language)|R]].\n\n{{DEFAULTSORT:Grammatical Evolution}}\n[[Category:Evolutionary algorithms]]"
    },
    {
      "title": "HyperNEAT",
      "url": "https://en.wikipedia.org/wiki/HyperNEAT",
      "text": "[[File:HyperNEAT query connection.png|thumb|Querying the CPPN to determine the connection weight between two neurons as a function of their position in space. Note sometimes the distance between them is also passed as an argument. ]]\n\n'''Hypercube-based NEAT''', or '''HyperNEAT''',<ref>{{Cite journal|last=Stanley|first=Kenneth O.|last2=D'Ambrosio|first2=David B.|last3=Gauci|first3=Jason|date=2009-01-14|title=A Hypercube-Based Encoding for Evolving Large-Scale Neural Networks|journal=Artificial Life|volume=15|issue=2|pages=185–212|doi=10.1162/artl.2009.15.2.15202|pmid=19199382|issn=1064-5462}}</ref> is a generative encoding that evolves [[artificial neural networks]] (ANNs) with the principles of the widely used [[NeuroEvolution of Augmented Topologies]] (NEAT) algorithm.<ref>{{Cite journal|last=Stanley|first=Kenneth O.|last2=Miikkulainen|first2=Risto|date=2002-06-01|title=Evolving Neural Networks through Augmenting Topologies|journal=Evolutionary Computation|volume=10|issue=2|pages=99–127|doi=10.1162/106365602320169811|issn=1063-6560|pmid=12180173|citeseerx=10.1.1.638.3910}}</ref> It is a novel technique for evolving large-scale neural networks using the geometric regularities of the task domain. It uses Compositional Pattern Producing Networks <ref>{{Cite journal|last=Stanley|first=Kenneth O.|date=2007-05-10|title=Compositional pattern producing networks: A novel abstraction of development|journal=Genetic Programming and Evolvable Machines|language=en|volume=8|issue=2|pages=131–162|doi=10.1007/s10710-007-9028-8|issn=1389-2576|citeseerx=10.1.1.643.8179}}</ref> ([[CPPN]]s), which are used to generate the images for [http://PicBreeder.org Picbreeder.org] and shapes for [http://EndlessForms.com EndlessForms.com]. HyperNEAT has recently been extended to also evolve plastic ANNs <ref>{{Cite book|title=From Animals to Animats 11|last=Risi|first=Sebastian|last2=Stanley|first2=Kenneth O.|date=2010-08-25|publisher=Springer Berlin Heidelberg|isbn=9783642151927|editor-last=Doncieux|editor-first=Stéphane|series=Lecture Notes in Computer Science|pages=533–543|language=en|doi=10.1007/978-3-642-15193-4_50|editor-last2=Girard|editor-first2=Benoît|editor-last3=Guillot|editor-first3=Agnès|editor-last4=Hallam|editor-first4=John|editor-last5=Meyer|editor-first5=Jean-Arcady|editor-last6=Mouret|editor-first6=Jean-Baptiste|citeseerx = 10.1.1.365.5589}}</ref> and to evolve the location of every neuron in the network.<ref>{{Cite journal|last=Risi|first=Sebastian|last2=Stanley|first2=Kenneth O.|date=2012-08-31|title=An Enhanced Hypercube-Based Encoding for Evolving the Placement, Density, and Connectivity of Neurons|journal=Artificial Life|volume=18|issue=4|pages=331–363|doi=10.1162/ARTL_a_00071|pmid=22938563|issn=1064-5462}}</ref>\n\n== Applications to date ==\n\n* Multi-agent learning<ref>{{Cite book|last=D'Ambrosio|first=David B.|last2=Stanley|first2=Kenneth O.|date=2008-01-01|title=Generative Encoding for Multiagent Learning|journal=Proceedings of the 10th Annual Conference on Genetic and Evolutionary Computation|series=GECCO '08|location=New York, NY, USA|publisher=ACM|pages=819–826|doi=10.1145/1389095.1389256|isbn=9781605581309}}</ref>\n* Checkers board evaluation<ref>J. Gauci and K. O. Stanley, “A case study on the critical role of geometric regularity in machine learning,” in AAAI (D. Fox and C. P. Gomes, eds.), pp. 628–633, AAAI Press, 2008.</ref>\n* Controlling Legged Robots<ref>{{Cite book|last=Risi|first=Sebastian|last2=Stanley|first2=Kenneth O.|date=2013-01-01|title=Confronting the Challenge of Learning a Flexible Neural Controller for a Diversity of Morphologies|journal=Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation|series=GECCO '13|location=New York, NY, USA|publisher=ACM|pages=255–262|doi=10.1145/2463372.2463397|isbn=9781450319638|citeseerx=10.1.1.465.5068}}</ref><ref>{{Cite book|last=Clune|first=J.|last2=Beckmann|first2=B. E.|last3=Ofria|first3=C.|last4=Pennock|first4=R. T.|date=2009-05-01|title=Evolving coordinated quadruped gaits with the HyperNEAT generative encoding|journal=2009 IEEE Congress on Evolutionary Computation|pages=2764–2771|doi=10.1109/CEC.2009.4983289|isbn=978-1-4244-2958-5|citeseerx=10.1.1.409.3868}}</ref><ref>{{Cite book|last=Clune|first=Jeff|last2=Ofria|first2=Charles|last3=Pennock|first3=Robert T.|date=2009-01-01|title=The Sensitivity of HyperNEAT to Different Geometric Representations of a Problem|journal=Proceedings of the 11th Annual Conference on Genetic and Evolutionary Computation|series=GECCO '09|location=New York, NY, USA|publisher=ACM|pages=675–682|doi=10.1145/1569901.1569995|isbn=9781605583259}}</ref><ref>Yosinski J, Clune J, Hidalgo D, Nguyen S, Cristobal Zagal J, Lipson H (2011) Evolving Robot Gaits in Hardware: the HyperNEAT Generative Encoding Vs. Parameter Optimization. Proceedings of the European Conference on Artificial Life. ([http://jeffclune.com/publications/2011-YosinskiEtAl-EvolvingRoboticGaitsInHardware-ECAL.pdf pdf])</ref><ref>Lee S, Yosinski J, Glette K, Lipson H, Clune J (2013) Evolving gaits for physical robots with the HyperNEAT generative encoding: the benefits of simulation. Applications of Evolutionary Computing. Springer. [http://jeffclune.com/publications/2013-LeeEtal-HyperNEAT+Sim.pdf pdf]</ref><ref>{{Cite book|title=Applications of Evolutionary Computation|last=Lee|first=Suchan|last2=Yosinski|first2=Jason|last3=Glette|first3=Kyrre|last4=Lipson|first4=Hod|last5=Clune|first5=Jeff|date=2013-04-03|publisher=Springer Berlin Heidelberg|isbn=9783642371912|editor-last=Esparcia-Alcázar|editor-first=Anna I.|series=Lecture Notes in Computer Science|pages=540–549|language=en|doi=10.1007/978-3-642-37192-9_54|citeseerx = 10.1.1.364.8979}}</ref>[https://www.youtube.com/watch?v=V2ADU8YWIug video]\n* Comparing Generative vs. Direct Encodings<ref>{{Cite journal|last=Clune|first=J.|last2=Stanley|first2=K. O.|last3=Pennock|first3=R. T.|last4=Ofria|first4=C.|date=2011-06-01|title=On the Performance of Indirect Encoding Across the Continuum of Regularity|journal=IEEE Transactions on Evolutionary Computation|volume=15|issue=3|pages=346–367|doi=10.1109/TEVC.2010.2104157|issn=1089-778X|citeseerx=10.1.1.375.6731}}</ref><ref>{{Cite book|title=Parallel Problem Solving from Nature – PPSN X|last=Clune|first=Jeff|last2=Ofria|first2=Charles|last3=Pennock|first3=Robert T.|date=2008-09-13|publisher=Springer Berlin Heidelberg|isbn=9783540876991|editor-last=Rudolph|editor-first=Günter|series=Lecture Notes in Computer Science|pages=358–367|language=en|doi=10.1007/978-3-540-87700-4_36|editor-last2=Jansen|editor-first2=Thomas|editor-last3=Beume|editor-first3=Nicola|editor-last4=Lucas|editor-first4=Simon|editor-last5=Poloni|editor-first5=Carlo}}</ref><ref>{{Cite book|title=Advances in Artificial Life. Darwin Meets von Neumann|last=Clune|first=Jeff|last2=Beckmann|first2=Benjamin E.|last3=Pennock|first3=Robert T.|last4=Ofria|first4=Charles|date=2009-09-13|publisher=Springer Berlin Heidelberg|isbn=9783642213137|editor-last=Kampis|editor-first=George|series=Lecture Notes in Computer Science|pages=134–141|language=en|doi=10.1007/978-3-642-21314-4_17|editor-last2=Karsai|editor-first2=István|editor-last3=Szathmáry|editor-first3=Eörs|citeseerx = 10.1.1.409.741}}</ref>\n* Investigating the Evolution of Modular Neural Networks<ref>{{Cite book|last=Clune|first=Jeff|last2=Beckmann|first2=Benjamin E.|last3=McKinley|first3=Philip K.|last4=Ofria|first4=Charles|date=2010-01-01|title=Investigating Whether hyperNEAT Produces Modular Neural Networks|journal=Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computation|series=GECCO '10|location=New York, NY, USA|publisher=ACM|pages=635–642|doi=10.1145/1830483.1830598|isbn=9781450300728|citeseerx=10.1.1.409.4870}}</ref><ref>{{Cite book|last=Suchorzewski|first=Marcin|last2=Clune|first2=Jeff|date=2011-01-01|title=A Novel Generative Encoding for Evolving Modular, Regular and Scalable Networks|journal=Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation|series=GECCO '11|location=New York, NY, USA|publisher=ACM|pages=1523–1530|doi=10.1145/2001576.2001781|isbn=9781450305570|citeseerx=10.1.1.453.5744}}</ref><ref>{{Cite book|last=Verbancsics|first=Phillip|last2=Stanley|first2=Kenneth O.|date=2011-01-01|title=Constraining Connectivity to Encourage Modularity in HyperNEAT|journal=Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation|series=GECCO '11|location=New York, NY, USA|publisher=ACM|pages=1483–1490|doi=10.1145/2001576.2001776|isbn=9781450305570|citeseerx=10.1.1.379.1188}}</ref>\n* Evolving Objects that can be 3D-printed<ref>{{Cite journal|last=Clune|first=Jeff|last2=Lipson|first2=Hod|date=2011-11-01|title=Evolving 3D Objects with a Generative Encoding Inspired by Developmental Biology|journal=SIGEVOlution|volume=5|issue=4|pages=2–12|doi=10.1145/2078245.2078246|issn=1931-8499}}</ref>\n* Evolving the Neural Geometry and Plasticity of an ANN<ref>{{Cite book|last=Risi|first=S.|last2=Stanley|first2=K. O.|date=2012-06-01|title=A unified approach to evolving plasticity and neural geometry|journal=The 2012 International Joint Conference on Neural Networks (IJCNN)|pages=1–8|doi=10.1109/IJCNN.2012.6252826|isbn=978-1-4673-1490-9|citeseerx=10.1.1.467.8366}}</ref>\n\n==References==\n{{reflist}}\n\n\n{{bioinformatics-stub}}\n\n==External links==\n*[http://eplex.cs.ucf.edu/hyperNEATpage/HyperNEAT.html HyperNEAT Users Page]\n*[http://www.cs.ucf.edu/~kstanley/ Ken Stanley's website]\n*[http://eplex.cs.ucf.edu \"Evolutionary Complexity Research Group at UCF\"]\n*[http://www.cs.ucf.edu/~kstanley/neat.html NEAT Project Homepage]\n*[http://www.picbreeder.org PicBreeder.org] \n*[http://EndlessForms.com EndlessForms.com]\n*[http://beacon-center.org/blog/2012/08/13/evolution-101-neuroevolution/ BEACON Blog: What is neuroevolution?]\n\n[[Category:Artificial neural networks]]\n[[Category:Evolutionary algorithms]]\n[[Category:Evolutionary computation]]\n[[Category:Genetic algorithms]]"
    },
    {
      "title": "IPO underpricing algorithm",
      "url": "https://en.wikipedia.org/wiki/IPO_underpricing_algorithm",
      "text": "{{Multiple issues|\n{{original research|date=April 2011}}\n{{essay-like|date=April 2011}}\n{{cleanup|date=April 2011}}\n}}\n\n'''[[Initial public offering#Pricing|IPO underpricing]]''' is the increase in stock value from the initial offering price to the first-day closing price. Many believe that underpriced IPOs leave money on the table for corporations, but some believe that underpricing is inevitable. Investors state that underpricing signals high interest to the market which increases the demand. On the other hand, overpriced stocks will drop long-term as the price stabilizes so underpricing may keep the issuers safe from investor litigation.\n\n==IPO underpricing algorithms==\n[[Underwriters]] and investors and corporations going for an [[initial public offering]] (IPO), issuers, are interested in their market value. There is always tension that results since the underwriters want to keep the price low while the companies want a high IPO price.\n\nUnderpricing may also be caused by investor over-reaction causing spikes on the initial days of trading. The IPO pricing process is similar to pricing new and unique products where there is sparse data on market demand, product acceptance, or competitive response. Besides, underpricing is also affected by the firm idiosyncratic factors such as its business model.<ref>{{cite journal|last=Morricone|first=Serena |author2=Federico Munari |author3=Raffaele Oriani |author4=Gaétan de Rassenfosse |title=Commercialization Strategy and IPO Underpricing|journal=Research Policy|year=2017|volume=46|issue=6|pages=1133–1141 | url=https://doi.org/10.1016/j.respol.2017.04.006 |doi=10.1016/j.respol.2017.04.006}}</ref> Thus it is difficult to determine a clear price which is compounded by the different goals issuers and investors have.\n\nThe problem with developing algorithms to determine underpricing is dealing with [[Statistical noise|noisy]], complex, and unordered data sets. Additionally, people, environment, and various environmental conditions introduce irregularities in the data.  To resolve these issues, researchers have found various techniques from [[artificial intelligence]] that [[normalization (statistics)|normalizes]] the data.\n\n==Artificial neural network==\n[[Artificial neural networks]] (ANNs) resolves these issues by scanning the data to develop internal representations of the relationship between the data. By determining the relationship over time, ANNs are more responsive and adaptive to structural changes in the data. There are two models for ANNs: supervised learning and unsupervised learning.\n\nIn [[supervised learning]] models, there are tests that are needed to pass to reduce mistakes. Usually, when mistakes are encountered i.e. test output does not match test input, the algorithms use [[back propagation]] to fix mistakes. Whereas in [[unsupervised learning]] models, the input is classified based on which problems need to be resolved.\n\n==Evolutionary models==\n[[Evolutionary programming]] is often paired with other algorithms e.g. [[artificial neural network|ANN]] to improve the robustness, reliability, and adaptability. Evolutionary models reduce error rates by allowing the numerical values to change within the fixed structure of the program.  Designers provide their algorithms the variables, they then provide training data to help the program generate rules defined in the input space that make a prediction in the output variable space.\n\nIn this approach, the solution is made an individual and the population is made of alternatives. However, the outliers cause the individuals to act unexpectedly as they try to create rules to explain the whole set.\n\n===Rule-based system===\nFor example, Quintana<ref>{{cite journal|last=Quintana|first=David |author2=Cristóbal Luque |author3=Pedro Isasi|title=Evolutionary rule-based system for IPO underpricing prediction|journal=In&nbsp;Proceedings of the 2005 conference on Genetic and evolutionary computation&nbsp;(GECCO '05)|year=2005|pages=983–989}}</ref> first abstracts a model with 7 major variables. The rules evolved from the Evolutionary Computation system developed at Michigan and Pittsburgh: \n* Underwriter prestige – Is the underwriter prestigious in role of lead manager? 1 for  true, 0 otherwise.\n* Price range width – The width of the non-binding reference price range offered to  potential customers during the roadshow. This width can be interpreted as a sign of  uncertainty regarding the real value of the company and a therefore, as a factor that  could influence the initial return.\n* Price adjustment – The difference between the final offer price and the price range  width. It can be viewed as uncertainty if the adjustment is outside the previous  price range.\n* Offering price – The final offer price of the IPO\n* Retained stock – Ratio of number of shares sold at the IPO divided by post-offering  number of shares minus the number of shares sold at the IPO.\n* Offering size – Logarithm of the offering size in millions of dollars excluding the  over-allotment option\n* Technology – Is this a technology company? 1 for true, 0 otherwise.\n\nQuintana uses these factors as signals that investors focus on. The algorithm his team explains shows how a prediction with a high-degree of confidence is possible with just a subset of the data.\n\n===Two-layered evolutionary forecasting===\nLuque<ref>{{cite journal|last=Luque|first=Cristóbal|author2=David Quintana |author3=J. M. Valls |author4=Pedro Isasi |title=Two-layered evolutionary forecasting for IPO underpricing|journal=In&nbsp;Proceedings of the Eleventh conference on Congress on Evolutionary Computation&nbsp;(CEC'09)|year=2009|pages=2374–2378|publisher=IEEE Press|location=Piscatawy, NJ, USA|doi=10.1109/cec.2009.4983237|isbn=978-1-4244-2958-5}}</ref>  approaches the problem with outliers by performing linear regressions over the set of data points (input, output). The algorithm deals with the data by allocating regions for noisy data. The scheme has the advantage of isolating noisy patterns which reduces the effect outliers have on the rule-generation system. The algorithm can come back later to understand if the isolated data sets influence the general data. Finally, the worst results from the algorithm outperformed all other algorithms' predictive abilities.\n\n==Agent-based modelling==\nCurrently, many of the algorithms assume homogeneous and rational behavior among investors. However, there's an approach alternative to financial modeling, and it's called [[agent-based model]]ling (ABM). ABM uses different autonomous agents whose behavior evolves endogenously which lead to complicated system dynamics that are sometimes impossible to predict from the properties of individual agents.<ref>{{cite journal |last=Brabazon |first=Anthony |author2=Jiang Dang |author3=Ian Dempsy |author4=Michael O'Neill |author5=David M. Edelman  |title=Natural Computing in finance: a review |journal=Handbook of Natural Computing |year=2010 |url=http://irserver.ucd.ie/dspace/bitstream/10197/2737/1/NCinFinance_v8.pdf }} {{dead link |date=September 2013}}</ref> ABM is starting to be applied to computational finance. Though, for ABM to be more accurate, better models for rule-generation need to be developed.\n\n== References ==\n{{reflist}}\n\n{{Corporate finance and investment banking}}\n\n[[Category:Initial public offering]]\n[[Category:Artificial neural networks]]\n[[Category:Evolutionary algorithms]]"
    },
    {
      "title": "Java Grammatical Evolution",
      "url": "https://en.wikipedia.org/wiki/Java_Grammatical_Evolution",
      "text": "{{Peacock|date=July 2016}}\n\nIn [[computer science]], '''Java Grammatical Evolution''' is an implementation of [[grammatical evolution]] in the [[Java programming language]]. Examples include jGE library and GEVA.\n\n== jGE library ==\n\n'''jGE library''' was the first published implementation of grammatical evolution in the Java language.<ref>Georgiou, L. and Teahan, W. J. (2006a) “jGE - A Java implementation of Grammatical Evolution”. <u>10th WSEAS International Conference on Systems</u>, Athens, Greece, July 10–15, 2006.</ref> Today, another well-known published Java implementation exists, named '''[http://ncra.ucd.ie/Site/GEVA.html GEVA]'''. GEVA was developed at [[University College Dublin]]'s Natural Computing Research & Applications group under the guidance of one of the inventors of grammatical evolution, [http://www.csi.ucd.ie/users/michael-oneill Dr. Michael O'Neill.]\n\njGE library aims to provide not only an implementation of grammatical evolution, but also a free, open-source, and extendable framework for experimentation in the area of [[evolutionary computation]]. Namely, it supports the implementation (through additions and extensions) of any [[evolutionary algorithm]].<ref>Georgiou, L. and Teahan, W. J. (2008) “Experiments with Grammatical Evolution in Java”. <u>Knowledge-Driven Computing: Knowledge Engineering and Intelligent Computations, Studies in Computational Intelligence</u> (vol. 102), 45-62. Berlin, Germany: Springer Berlin / Heidelberg.</ref> Furthermore, its extendable architecture and design facilitates the implementation and incorporation of new experimental implementation inspired by natural evolution and biology.<ref>Georgiou, L. and Teahan, W. J. (2006b) “Implication of Prior Knowledge and Population Thinking in Grammatical Evolution: Toward a Knowledge Sharing Architecture”. <u>WSEAS Transactions on Systems</u> 5 (10), 2338-2345.</ref>\n\nThe jGE library binary file, the source code, the documentation, and an extension for the [http://ccl.northwestern.edu/netlogo NetLogo modeling environment], named jGE NetLogo extension, can be downloaded from the [https://web.archive.org/web/20101129085227/http://www.bangor.ac.uk/~eep201/jge/ jGE Official Web Site].\n\n== License ==\nThe jGE library is free software released under the [https://www.gnu.org/licenses GNU General Public License v3].\n\n== References ==\n{{Reflist}}\n\n==External links==\n* [https://web.archive.org/web/20101129085227/http://www.bangor.ac.uk/~eep201/jge/ jGE Official Web Site]\n\n[[Category:Genetic programming]]\n[[Category:Evolutionary algorithms]]"
    },
    {
      "title": "Learning classifier system",
      "url": "https://en.wikipedia.org/wiki/Learning_classifier_system",
      "text": "[[File:Function approximation with LCS rules.jpg|thumb|2D visualization of LCS rules learning to approximate a 3D function. Each blue ellipse represents an individual rule covering part of the solution space. (Adapted from images taken from XCSF<ref name=\":9\">{{Cite journal|last=Stalph|first=Patrick O.|last2=Butz|first2=Martin V.|date=2010-02-01|title=JavaXCSF: The XCSF Learning Classifier System in Java|journal=SIGEVOlution|volume=4|issue=3|pages=16–19|doi=10.1145/1731888.1731890|issn=1931-8499}}</ref> with permission from Martin Butz)]]\n'''Learning classifier systems''', or '''LCS''', are a paradigm of [[rule-based machine learning]] methods that combine a discovery component (e.g. typically a [[genetic algorithm]]) with a learning component (performing either [[supervised learning]], [[reinforcement learning]], or [[unsupervised learning]]).<ref name=\":1\">{{Cite journal|last=Urbanowicz|first=Ryan J.|last2=Moore|first2=Jason H.|date=2009-09-22|title=Learning Classifier Systems: A Complete Introduction, Review, and Roadmap|journal=Journal of Artificial Evolution and Applications|language=en|volume=2009|pages=1–25|doi=10.1155/2009/736398|issn=1687-6229}}</ref>  Learning classifier systems seek to identify a set of context-dependent rules that collectively store and apply knowledge in a [[piecewise]] manner in order to make predictions (e.g. [[behavior modeling]],<ref>{{Cite journal|last=Dorigo|first=Marco|title=Alecsys and the AutonoMouse: Learning to control a real robot by distributed classifier systems|journal=Machine Learning|language=en|volume=19|issue=3|pages=209–240|doi=10.1007/BF00996270|issn=0885-6125|year=1995}}</ref> [[Statistical classification|classification]],<ref>{{Cite journal|last=Bernadó-Mansilla|first=Ester|last2=Garrell-Guiu|first2=Josep M.|date=2003-09-01|title=Accuracy-Based Learning Classifier Systems: Models, Analysis and Applications to Classification Tasks|journal=Evolutionary Computation|volume=11|issue=3|pages=209–238|doi=10.1162/106365603322365289|pmid=14558911|issn=1063-6560}}</ref><ref name=\":0\">{{Cite journal|last=Urbanowicz|first=Ryan J.|last2=Moore|first2=Jason H.|date=2015-04-03|title=ExSTraCS 2.0: description and evaluation of a scalable learning classifier system|journal=Evolutionary Intelligence|language=en|volume=8|issue=2–3|pages=89–116|doi=10.1007/s12065-015-0128-8|issn=1864-5909|pmc=4583133|pmid=26417393}}</ref> [[data mining]],<ref name=\":0\" /><ref>{{Cite book|title=Advances in Learning Classifier Systems|last=Bernadó|first=Ester|last2=Llorà|first2=Xavier|last3=Garrell|first3=Josep M.|date=2001-07-07|publisher=Springer Berlin Heidelberg|isbn=9783540437932|editor-last=Lanzi|editor-first=Pier Luca|series=Lecture Notes in Computer Science|pages=115–132|language=en|doi=10.1007/3-540-48104-4_8|editor-last2=Stolzmann|editor-first2=Wolfgang|editor-last3=Wilson|editor-first3=Stewart W.}}</ref><ref>{{Cite book|title=Learning Classifier Systems|last=Bacardit|first=Jaume|last2=Butz|first2=Martin V.|date=2007-01-01|publisher=Springer Berlin Heidelberg|isbn=9783540712305|editor-last=Kovacs|editor-first=Tim|series=Lecture Notes in Computer Science|pages=282–290|language=en|doi=10.1007/978-3-540-71231-2_19|editor-last2=Llorà|editor-first2=Xavier|editor-last3=Takadama|editor-first3=Keiki|editor-last4=Lanzi|editor-first4=Pier Luca|editor-last5=Stolzmann|editor-first5=Wolfgang|editor-last6=Wilson|editor-first6=Stewart W.|citeseerx = 10.1.1.553.4679}}</ref> [[Regression analysis|regression]],<ref>{{Cite book|last=Urbanowicz|first=Ryan|last2=Ramanand|first2=Niranjan|last3=Moore|first3=Jason|date=2015-01-01|title=Continuous Endpoint Data Mining with ExSTraCS: A Supervised Learning Classifier System|journal=Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation|series=GECCO Companion '15|location=New York, NY, USA|publisher=ACM|pages=1029–1036|doi=10.1145/2739482.2768453|isbn=9781450334884}}</ref> [[function approximation]],<ref>{{Cite journal|last=Butz|first=M. V.|last2=Lanzi|first2=P. L.|last3=Wilson|first3=S. W.|date=2008-06-01|title=Function Approximation With XCS: Hyperellipsoidal Conditions, Recursive Least Squares, and Compaction|journal=IEEE Transactions on Evolutionary Computation|volume=12|issue=3|pages=355–376|doi=10.1109/TEVC.2007.903551|issn=1089-778X}}</ref> or [[Strategy (game theory)|game strategy]]).  This approach allows complex [[Feasible region|solution spaces]] to be broken up into smaller, simpler parts.\n\nThe founding concepts behind learning classifier systems came from attempts to model [[complex adaptive system]]s, using rule-based agents to form an artificial cognitive system (i.e. [[artificial intelligence]]).\n\n== Methodology ==\nThe architecture and components of a given learning classifier system can be quite variable.  It is useful to think of an LCS as a machine consisting of several interacting components.  Components may be added or removed, or existing components modified/exchanged to suit the demands of a given problem domain (like algorithmic building blocks) or to make the algorithm flexible enough to function in many different problem domains.  As a result, the LCS paradigm can be flexibly applied to many problem domains that call for [[machine learning]].  The major divisions among LCS implementations are as follows: (1) Michigan-style architecture vs. Pittsburgh-style architecture, (2) [[reinforcement learning]] vs. [[supervised learning]], (3) incremental learning vs. batch learning, (4) [[Online machine learning|online learning]] vs. [[offline learning]], (5) strength-based fitness vs. accuracy-based fitness, and (6) complete action mapping vs best action mapping.   These divisions are not necessarily mutually exclusive. For example, XCS,<ref name=\":10\" /> the best known and best studied LCS algorithm, is Michigan-style, was designed for reinforcement learning but can also perform supervised learning, applies incremental learning that can be either online or offline, applies accuracy-based fitness, and seeks to generate a complete action mapping.\n\n=== Elements of a generic LCS algorithm ===\n[[File:Generic Michigan-style Supervised LCS Schematic.png|thumb|A step-wise schematic illustrating a generic Michigan-style learning classifier system learning cycle performing supervised learning.]]\nKeeping in mind that LCS is a paradigm for genetic-based machine learning rather than a specific method, the following outlines key elements of a generic, modern (i.e. post-XCS) LCS algorithm.  For simplicity let us focus on Michigan-style architecture with supervised learning.  See the illustrations  on the right laying out the sequential steps involved in this type of generic LCS.\n\n==== Environment ====\nThe environment is the source of data upon which an LCS learns.  It can be an offline, finite [[Training data set|training dataset]] (characteristic of a [[data mining]], [[Statistical classification|classification]], or regression problem), or an online sequential stream of live training instances.  Each training instance is assumed to include some number of ''features'' (also referred to as ''attributes'', or [[Dependent and independent variables|''independent variables'']]), and a single ''endpoint'' of interest (also referred to as the [[Class (set theory)|class]], ''action'', ''[[phenotype]]'', ''prediction'', or [[Dependent and independent variables|''dependent variable'']]).  Part of LCS learning can involve [[feature selection]], therefore not all of the features in the training data need be informative.  The set of feature values of an instance is commonly referred to as the ''state''.  For simplicity let's assume an example problem domain with [[Boolean data type|Boolean]]/[[Binary number|binary]] features and a [[Boolean data type|Boolean]]/[[Binary number|binary]] class.  For Michigan-style systems, one instance from the environment is trained on each learning cycle (i.e. incremental learning).  Pittsburgh-style systems perform batch learning, where rule-sets are evaluated each iteration over much or all of the training data.\n\n==== Rule/classifier/population ====\nA rule is a context dependent relationship between state values and some prediction.  Rules typically take the form of an {IF:THEN} expression, (e.g.  {''IF 'condition' THEN 'action'},'' or as a more specific example, ''{IF 'red' AND 'octagon' THEN 'stop-sign'}'').   A critical concept in LCS and rule-based machine learning alike, is that an individual rule is not in itself a model, since the rule is only applicable when its condition is satisfied.  Think of a rule as a \"local-model\" of the solution space.\n\nRules can be represented in many different ways to handle different data types (e.g. binary, discrete-valued, ordinal, continuous-valued).  Given binary data LCS traditionally applies a ternary rule representation (i.e. rules can include either a 0, 1, or '#' for each feature in the data).  The 'don't care' symbol (i.e. '#') serves as a wild card within a rule's condition allowing rules, and the system as a whole to generalize relationships between features and the target endpoint to be predicted. Consider the following rule (#1###0 ~ 1) (i.e. condition ~ action).  This rule can be interpreted as: IF the second feature = 1 AND the sixth feature = 0 THEN the class prediction = 1.  We would say that the second and sixth features were specified in this rule, while the others were generalized. This rule, and the corresponding prediction are only applicable to an instance when the condition of the rule is satisfied by the instance.  This is more commonly referred to as matching.  In Michigan-style LCS, each rule has its own fitness, as well as a number of other rule-parameters associated with it that can describe the number of copies of that rule that exist (i.e. the ''numerosity''), the age of the rule, its accuracy, or the accuracy of its reward predictions, and other descriptive or experiential statistics.  A rule along with its parameters is often referred to as a ''classifier''.  In Michigan-style systems, classifiers are contained within a ''population'' [P] that has a user defined maximum number of classifiers.  Unlike most [[stochastic]] search algorithms (e.g. [[evolutionary algorithm]]s), LCS populations start out empty (i.e. there is no need to randomly initialize a rule population).  Classifiers will instead be initially introduced to the population with a covering mechanism.\n\nIn any LCS, the trained model is a set of rules/classifiers, rather than any single rule/classifier.  In Michigan-style LCS, the entire trained (and optionally, compacted) classifier population forms the prediction model.\n\n==== Matching ====\nOne of the most critical and often time consuming elements of an LCS is the matching process.  The first step in an LCS learning cycle takes a single training instance from the environment and passes it to [P] where matching takes place.  In step two, every rule in [P] is now compared to the training instance to see which rules match (i.e. are contextually relevant to the current instance).  In step three, any matching rules are moved to a ''match set'' [M].  A rule matches a training instance if all feature values specified in the rule condition are equivalent to the corresponding feature value in the training instance.  For example, assuming the training instance is (001001 ~ 0), these rules would match: (###0## ~ 0), (00###1 ~ 0), (#01001 ~ 1), but these rules would not (1##### ~ 0), (000##1 ~ 0), (#0#1#0 ~ 1).  Notice that in matching, the endpoint/action specified by the rule is not taken into consideration.  As a result, the match set may contain classifiers that propose conflicting actions.  In the fourth step, since we are performing supervised learning, [M] is divided into a correct set [C] and an incorrect set [I].  A matching rule goes into the correct set if it proposes the correct action (based on the known action of the training instance), otherwise it goes into [I].  In reinforcement learning LCS, an action set [A] would be formed here instead, since the correct action is not known.\n\n==== Covering ====\nAt this point in the learning cycle, if no classifiers made it into either [M] or [C] (as would be the case when the population starts off empty), the covering mechanism is applied (fifth step).  Covering is a form of ''online smart population initialization''. Covering randomly generates a rule that matches the current training instance (and in the case of supervised learning, that rule is also generated with the correct action.  Assuming the training instance is (001001 ~ 0), covering might generate any of the following rules:  (#0#0## ~ 0), (001001 ~ 0), (#010## ~ 0).  Covering not only ensures that each learning cycle there is at least one correct, matching rule in [C], but that any rule initialized into the population will match at least one training instance.  This prevents LCS from exploring the search space of rules that do not match any training instances.\n\n==== Parameter updates/credit assignment/learning ====\nIn the sixth step, the rule parameters of any rule in [M] are updated to reflect the new experience gained from the current training instance.  Depending on the LCS algorithm, a number of updates can take place at this step.  For supervised learning, we can simply update the accuracy/error of a rule.  Rule accuracy/error is different than model accuracy/error, since it is not calculated over the entire training data, but only over all instances that it matched.  Rule accuracy is calculated by dividing the number of times the rule was in a correct set [C] by the number of times it was in a match set [M].  Rule accuracy can be thought of as a 'local accuracy'.  Rule fitness is also updated here, and is commonly calculated as a function of rule accuracy.  The concept of fitness is taken directly from classic [[genetic algorithm]]s.  Be aware that there are many variations on how LCS updates parameters in order to perform credit assignment and learning.\n\n==== Subsumption ====\nIn the seventh step, a ''subsumption'' mechanism is typically applied.  Subsumption is an explicit generalization mechanism that merges classifiers that cover redundant parts of the problem space.  The subsuming classifier effectively absorbs the subsumed classifier (and has its numerosity increased).  This can only happen when the subsuming classifier is more general, just as accurate, and covers all of the problem space of the classifier it subsumes.\n\n==== Rule discovery/genetic algorithm ====\nIn the eighth step, LCS adopts a highly elitist [[genetic algorithm]] (GA) which will select two parent classifiers based on fitness (survival of the fittest).  Parents are selected from [C] typically using [[tournament selection]].  Some systems have applied [[roulette wheel selection]] or deterministic selection, and have differently selected parent rules from either [P] - panmictic selection, or from [M]).  [[Crossover (genetic algorithm)|Crossover]] and [[Mutation (genetic algorithm)|mutation]] operators are now applied to generate two new offspring rules.  At this point, both the parent and offspring rules are returned to [P].  The LCS [[genetic algorithm]] is highly elitist since each learning iteration, the vast majority of the population is preserved.  Rule discovery may alternatively be performed by some other method, such as an [[estimation of distribution algorithm]], but a GA is by far the most common approach.  Evolutionary algorithms like the GA employ a stochastic search, which makes LCS a stochastic algorithm.  LCS seeks to cleverly explore the search space, but does not perform an exhaustive search of rule combinations, and is not guaranteed to converge on an optimal solution.\n\n==== Deletion ====\nThe last step in a generic LCS learning cycle is to maintain the maximum population size. The deletion mechanism will select classifiers for deletion (commonly using roulette wheel selection).  The probability of a classifier being selected for deletion is inversely proportional to its fitness.  When a classifier is selected for deletion, its numerosity parameter is reduced by one.  When the numerosity of a classifier is reduced to zero, it is removed entirely from the population.\n\n==== Training ====\nLCS will cycle through these steps repeatedly for some user defined number of training iterations, or until some user defined termination criteria have been met.  For online learning, LCS will obtain a completely new training instance each iteration from the environment.  For offline learning, LCS will iterate through a finite training dataset.  Once it reaches the last instance in the dataset, it will go back to the first instance and cycle through the dataset again.\n\n==== Rule compaction ====\nOnce training is complete, the rule population will inevitably contain some poor, redundant and inexperienced rules.  It is common to apply a ''rule compaction'', or ''condensation'' heuristic as a post-processing step.  This resulting compacted rule population is ready to be applied as a prediction model (e.g. make predictions on testing instances), and/or to be interpreted for [[knowledge discovery]].\n\n==== Prediction ====\nWhether or not rule compaction has been applied, the output of an LCS algorithm is a population of classifiers which can be applied to making predictions on previously unseen instances.  The prediction mechanism is not part of the supervised LCS learning cycle itself, however it would play an important role in a reinforcement learning LCS learning cycle.  For now we consider how the prediction mechanism can be applied for making predictions to test data.  When making predictions, the LCS learning components are deactivated so that the population does not continue to learn from incoming testing data.  A test instance is passed to [P] where a match set [M] is formed as usual.  At this point the match set is differently passed to a prediction array.  Rules in the match set can predict different actions, therefore a voting scheme is applied.  In a simple voting scheme, the action with the strongest supporting 'votes' from matching rules wins, and becomes the selected prediction.  All rules do not get an equal vote.  Rather the strength of the vote for a single rule is commonly proportional to its numerosity and fitness.  This voting scheme and the nature of how LCS's store knowledge, suggests that LCS algorithms are implicitly ''ensemble learners''.\n\n==== Interpretation ====\nIndividual LCS rules are typically human readable IF:THEN expression.  Rules that constitute the LCS prediction model can be ranked by different rule parameters and manually inspected.  Global strategies to guide knowledge discovery using statistical and graphical have also been proposed.<ref name=\":11\" /><ref name=\":12\" />  With respect to other advanced machine learning approaches, such as [[artificial neural network]]s, [[random forest]]s, or [[genetic programming]], learning classifier systems are particularly well suited to problems that require interpretable solutions.\n\n== History ==\n\n=== Early years ===\n[[John Henry Holland]] was best known for his work popularizing [[genetic algorithm]]s (GA), through his ground-breaking book \"Adaptation in Natural and Artificial Systems\"<ref>{{Cite book|title=Adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence|last=Holland|first=John|publisher=Michigan Press|year=1975|isbn=9780262581110|location=|pages=|quote=|via=|url=https://books.google.com/books?id=5EgGaBkwvWcC&printsec=frontcover#v=onepage&q&f=false}}</ref> in 1975 and his formalization of [[Holland's schema theorem]].  In 1976, Holland conceptualized an extension of the GA concept to what he called a \"cognitive system\",<ref>Holland JH (1976) Adaptation. In: Rosen R, Snell F (eds) Progress in theoretical biology, vol 4. Academic Press, New York, pp 263–293</ref> and provided the first detailed description of what would become known as the first learning classifier system in the paper \"Cognitive Systems based on Adaptive Algorithms\".<ref name=\":2\">Holland JH, Reitman JS (1978) Cognitive systems based on\nadaptive algorithms Reprinted in: Evolutionary computation.\nThe fossil record. In: David BF (ed) IEEE Press, New York\n1998. {{ISBN|0-7803-3481-7}}\n</ref>  This first system, named '''Cognitive System One (CS-1)''' was conceived as a modeling tool, designed to model a real system (i.e. ''environment'') with unknown underlying dynamics using a population of human readable rules.  The goal was for a set of rules to perform [[online machine learning]] to adapt to the environment based on infrequent payoff/reward (i.e. reinforcement learning) and apply these rules to generate a behavior that matched the real system. This early, ambitious implementation was later regarded as overly complex, yielding inconsistent results.<ref name=\":1\" /><ref name=\":3\">{{Cite journal|last=Lanzi|first=Pier Luca|date=2008-02-08|title=Learning classifier systems: then and now|journal=Evolutionary Intelligence|language=en|volume=1|issue=1|pages=63–82|doi=10.1007/s12065-007-0003-3|issn=1864-5909}}</ref>\n\nBeginning in 1980, [[Kenneth A De Jong|Kenneth de Jong]] and his student Stephen Smith took a different approach to rule-based machine learning with '''(LS-1)''', where learning was viewed as an offline optimization process rather than an online adaptation process.<ref>Smith S (1980) A learning system based on genetic adaptive\nalgorithms. Ph.D. thesis, Department of Computer Science,\nUniversity of Pittsburgh\n</ref><ref>Smith S (1983) [https://www.researchgate.net/profile/Stephen_Smith14/publication/220815785_Flexible_Learning_of_Problem_Solving_Heuristics_Through_Adaptive_Search/links/0deec52c18dbd0dd53000000.pdf Flexible learning of problem solving heuristics through adaptive search]. In: Eighth international joint conference\non articial intelligence. Morgan Kaufmann, Los Altos, pp\n421–425\n</ref><ref>De Jong KA (1988) Learning with genetic algorithms: an overview. Mach Learn 3:121–138</ref>  This new approach was more similar to a standard genetic algorithm but evolved independent sets of rules.  Since that time LCS methods inspired by the online learning framework introduced by Holland at the University of Michigan have been referred to as '''Michigan-style LCS''', and those inspired by Smith and De Jong at the University of Pittsburgh have been referred to as '''Pittsburgh-style LCS'''.<ref name=\":1\" /><ref name=\":3\" />  In 1986, Holland developed what would be considered the standard Michigan-style LCS for the next decade.<ref name=\":4\">[http://dl.acm.org/citation.cfm?id=216016 Holland, John H. \"Escaping brittleness: the possibilities of general purpose learning algorithms applied to parallel rule-based system.\" ''Machine learning''(1986): 593-623.]</ref>\n\nOther important concepts that emerged in the early days of LCS research included (1) the formalization of a ''bucket brigade algorithm'' (BBA) for credit assignment/learning,<ref>{{Cite book|last=Holland|first=John H.|date=1985-01-01|title=Properties of the Bucket Brigade|url=http://dl.acm.org/citation.cfm?id=645511.657087|journal=Proceedings of the 1st International Conference on Genetic Algorithms|location=Hillsdale, NJ, USA|publisher=L. Erlbaum Associates Inc.|pages=1–7|isbn=978-0805804263}}</ref> (2) selection of parent rules from a common 'environmental niche' (i.e. the ''match set'' [M]) rather than from the whole ''population'' [P],<ref>{{Cite thesis|last=Booker|first=L|title=Intelligent Behavior as a Adaptation to the Task Environment|date=1982-01-01|publisher=University of Michigan|url=http://www.citeulike.org/group/664/article/431772}}</ref> (3) ''covering'', first introduced as a ''create'' operator,<ref name=\":5\">Wilson, S. W. \"[http://www.cs.sfu.ca/~vaughan/teaching/415/papers/wilson_animat.pdf Knowledge growth in an artificial animal]. Proceedings of the First International Conference on Genetic Algorithms and their Applications.\" (1985).</ref> (4) the formalization of an ''action set'' [A],<ref name=\":5\" /> (5) a simplified algorithm architecture,<ref name=\":5\" /> (6) ''strength-based fitness'',<ref name=\":4\" /> (7) consideration of single-step, or supervised learning problems<ref>{{Cite journal|last=Wilson|first=Stewart W.|title=Classifier systems and the animat problem|journal=Machine Learning|language=en|volume=2|issue=3|pages=199–228|doi=10.1007/BF00058679|issn=0885-6125|year=1987}}</ref> and the introduction of the ''correct set'' [C],<ref>{{Cite book|last=Bonelli|first=Pierre|last2=Parodi|first2=Alexandre|last3=Sen|first3=Sandip|last4=Wilson|first4=Stewart|date=1990-01-01|title=NEWBOOLE: A Fast GBML System|url=http://dl.acm.org/citation.cfm?id=101883.102047|journal=Proceedings of the Seventh International Conference (1990) on Machine Learning|location=San Francisco, CA, USA|publisher=Morgan Kaufmann Publishers Inc.|pages=153–159|isbn=978-1558601413}}</ref> (8) ''accuracy-based fitness''<ref>{{Cite journal|last=Frey|first=Peter W.|last2=Slate|first2=David J.|title=Letter recognition using Holland-style adaptive classifiers|journal=Machine Learning|language=en|volume=6|issue=2|pages=161–182|doi=10.1007/BF00114162|issn=0885-6125|year=1991}}</ref> (9) the combination of fuzzy logic with LCS<ref>Valenzuela-Rendón, Manuel. \"[http://sci2s.ugr.es/sites/default/files/files/TematicWebSites/GeneticFuzzySystems/(1991)_Valenzuela-Rendon.pdf The Fuzzy Classifier System: A Classifier System for Continuously Varying Variables].\" In ''ICGA'', pp. 346-353. 1991.</ref> (which later spawned a lineage of ''fuzzy LCS algorithms''), (10) encouraging ''long action chains'' and ''default hierarchies'' for improving performance on multi-step problems,<ref>{{Cite thesis|last=Riolo|first=Rick L.|title=Empirical Studies of Default Hierarchies and Sequences of Rules in Learning Classifier Systems|date=1988-01-01|publisher=University of Michigan|url=http://dl.acm.org/citation.cfm?id=914945|place=Ann Arbor, MI, USA}}</ref><ref>{{Cite journal|last=R.L.|first=Riolo|date=1987-01-01|title=Bucket brigade performance. I. Long sequences of classifiers|url=http://agris.fao.org/agris-search/search.do?recordID=US201301782174|journal=Genetic Algorithms and Their Applications : Proceedings of the Second International Conference on Genetic Algorithms : July 28–31, 1987 at the Massachusetts Institute of Technology, Cambridge, MA|language=English}}</ref><ref>{{Cite journal|last=R.L.|first=Riolo|date=1987-01-01|title=Bucket brigade performance. II. Default hierarchies|url=http://agris.fao.org/agris-search/search.do?recordID=US201301782175|journal=Genetic Algorithms and Their Applications : Proceedings of the Second International Conference on Genetic Algorithms : July 28–31, 1987 at the Massachusetts Institute of Technology, Cambridge, MA|language=English}}</ref> (11) examining [[latent learning]] (which later inspired a new branch of ''anticipatory classifier systems'' (ACS)<ref name=\":7\">W. Stolzmann, \"Anticipatory classifier systems,\" in Proceedings\n\nof the 3rd Annual Genetic Programming Conference, pp.\n\n658–664, 1998.\n</ref>), and (12) the introduction of the first [[Q-learning]]-like credit assignment technique.<ref>{{Cite book|last=Riolo|first=Rick L.|date=1990-01-01|title=Lookahead Planning and Latent Learning in a Classifier System|url=http://dl.acm.org/citation.cfm?id=116517.116553|journal=Proceedings of the First International Conference on Simulation of Adaptive Behavior on from Animals to Animats|location=Cambridge, MA, USA|publisher=MIT Press|pages=316–326|isbn=978-0262631389}}</ref>  While not all of these concepts are applied in modern LCS algorithms, each were landmarks in the development of the LCS paradigm.\n\n=== The revolution ===\nInterest in learning classifier systems was reinvigorated in the mid 1990s largely due to two events; the development of the [[Q-learning|Q-Learning]] algorithm<ref>Watkins, Christopher John Cornish Hellaby. \"Learning from delayed rewards.\" PhD diss., University of Cambridge, 1989.</ref> for [[reinforcement learning]], and the introduction of significantly simplified Michigan-style LCS architectures by Stewart Wilson.<ref name=\":10\">{{Cite journal|last=Wilson|first=Stewart W.|date=1995-06-01|title=Classifier Fitness Based on Accuracy|journal=Evol. Comput.|volume=3|issue=2|pages=149–175|doi=10.1162/evco.1995.3.2.149|issn=1063-6560|citeseerx=10.1.1.363.2210}}</ref><ref name=\":6\">{{Cite journal|last=Wilson|first=Stewart W.|date=1994-03-01|title=ZCS: A Zeroth Level Classifier System|journal=Evolutionary Computation|volume=2|issue=1|pages=1–18|doi=10.1162/evco.1994.2.1.1|issn=1063-6560|citeseerx=10.1.1.363.798}}</ref>  Wilson's '''Zeroth-level Classifier System (ZCS)'''<ref name=\":6\" /> focused on increasing algorithmic understandability based on Hollands standard LCS implementation.<ref name=\":4\" />  This was done, in part, by removing rule-bidding and the internal message list, essential to the original BBA credit assignment, and replacing it with a hybrid BBA/[[Q-learning|Q-Learning]] strategy.  ZCS demonstrated that a much simpler LCS architecture could perform as well as the original, more complex implementations.  However, ZCS still suffered from performance drawbacks including the proliferation of over-general classifiers.\n\nIn 1995, Wilson published his landmark paper, \"Classifier fitness based on accuracy\" in which he introduced the classifier system '''XCS'''.<ref name=\":10\" />  XCS took the simplified architecture of ZCS and added an accuracy-based fitness, a niche GA (acting in the action set [A]), an explicit generalization mechanism called ''subsumption'', and an adaptation of the [[Q-learning|Q-Learning]] credit assignment.  XCS was popularized by its ability to reach optimal performance while evolving accurate and maximally general classifiers as well as its impressive problem flexibility (able to perform both [[reinforcement learning]] and [[supervised learning]]) . XCS later became the best known and most studied LCS algorithm and defined a new family of ''accuracy-based LCS''.  ZCS alternatively became synonymous with ''strength-based LCS''.  XCS is also important, because it successfully bridged the gap between LCS and the field of [[reinforcement learning]].  Following the success of XCS, LCS were later described as reinforcement learning systems endowed with a generalization capability.<ref>{{Cite journal|last=Lanzi|first=P. L.|title=Learning classifier systems from a reinforcement learning perspective|journal=Soft Computing|language=en|volume=6|issue=3–4|pages=162–170|doi=10.1007/s005000100113|issn=1432-7643|year=2002}}</ref>  [[Reinforcement learning]] typically seeks to learn a value function that maps out a complete representation of the state/action space.  Similarly, the design of XCS drives it to form an all-inclusive and accurate representation of the problem space (i.e. a ''complete map'') rather than focusing on high payoff niches in the environment (as was the case with strength-based LCS).  Conceptually, complete maps don't only capture what you should do, or what is correct, but also what you shouldn't do, or what's incorrect.  Differently, most strength-based LCSs, or exclusively supervised learning LCSs seek a rule set of efficient generalizations in the form of a ''best action map'' (or a ''partial map'').   Comparisons between strength vs. accuracy-based fitness and complete vs. best action maps have since been examined in greater detail.<ref>Kovacs, Timothy Michael Douglas. ''A Comparison of Strength and Accuracy-based Fitness in Learning and Classifier Systems''. 2002.</ref><ref>[https://link.springer.com/chapter/10.1007/3-540-48104-4_6 Kovacs, Tim. \"Two views of classifier systems.\" In ''International Workshop on Learning Classifier Systems'', pp. 74-87. Springer Berlin Heidelberg, 2001]</ref>\n\n=== In the wake of XCS ===\nXCS inspired the development of a whole new generation of LCS algorithms and applications.  In 1995, Congdon was the first to apply LCS to real-world [[Epidemiology|epidemiological]] investigations of disease <ref name=\":8\" /> followed closely by Holmes who developed the '''BOOLE++''',<ref>{{Cite journal|last=Holmes|first=John H.|date=1996-01-01|title=A Genetics-Based Machine Learning Approach to Knowledge Discovery in Clinical Data|journal=Proceedings of the AMIA Annual Fall Symposium|pages=883|issn=1091-8280|pmc=2233061}}</ref> '''EpiCS''',<ref>Holmes, John H. \"[https://pdfs.semanticscholar.org/71e4/eb6c630dee4b762e74b2970f6dc638a351ab.pdf Discovering Risk of Disease with a Learning Classifier System].\" In ''ICGA'', pp. 426-433. 1997.</ref> and later '''EpiXCS'''<ref>Holmes, John H., and Jennifer A. Sager. \"[https://link.springer.com/10.1007%2F11527770_60 Rule discovery in epidemiologic surveillance data using EpiXCS: an evolutionary computation approach].\" In''Conference on Artificial Intelligence in Medicine in Europe'', pp. 444-452. Springer Berlin Heidelberg, 2005.</ref> for [[Epidemiology|epidemiological]] classification.  These early works inspired later interest in applying LCS algorithms to complex and large-scale [[data mining]] tasks epitomized by [[bioinformatics]] applications.  In 1998, Stolzmann introduced '''anticipatory classifier systems (ACS)''' which included rules in the form of 'condition-action-effect, rather than the classic 'condition-action' representation.<ref name=\":7\" />  ACS was designed to predict the perceptual consequences of an action in all possible situations in an environment.  In other words, the system evolves a model that specifies not only what to do in a given situation, but also provides information of what will happen after a specific action will be executed. This family of LCS algorithms is best suited to multi-step problems, planning, speeding up learning, or disambiguating perceptual aliasing (i.e. where the same observation is obtained in distinct states but requires different actions).  Butz later pursued this anticipatory family of LCS developing a number of improvements to the original method.<ref>Butz, Martin V. \"[https://pdfs.semanticscholar.org/3572/7a56fcce7a73ccc43e5bfa19389780e6d436.pdf Biasing exploration in an anticipatory learning classifier system].\" In ''International Workshop on Learning Classifier Systems'', pp. 3-22. Springer Berlin Heidelberg, 2001.</ref>  In 2002, Wilson introduced '''XCSF''', adding a computed action in order to perform function approximation.<ref>{{Cite journal|last=Wilson|first=Stewart W.|title=Classifiers that approximate functions|journal=Natural Computing|language=en|volume=1|issue=2–3|pages=211–234|doi=10.1023/A:1016535925043|issn=1567-7818|year=2002}}</ref>  In 2003, Bernado-Mansilla introduced a '''sUpervised Classifier System (UCS)''', which specialized the XCS algorithm to the task of [[supervised learning]], single-step problems, and forming a best action set.  UCS removed the [[reinforcement learning]] strategy in favor of a simple, accuracy-based rule fitness as well as the explore/exploit learning phases, characteristic of many reinforcement learners.  Bull introduced a simple accuracy-based LCS '''(YCS)'''<ref>Bull, Larry. \"[https://pdfs.semanticscholar.org/120c/8f5057995c36ee60ec320c2263b20af05444.pdf A simple accuracy-based learning classifier system].\" ''Learning Classifier Systems Group Technical Report UWELCSG03-005, University of the West of England, Bristol, UK'' (2003).</ref> and a simple strength-based LCS '''Minimal Classifier System (MCS)'''<ref>Bull, Larry. \"[https://link.springer.com/chapter/10.1007/978-3-540-30217-9_104 A simple payoff-based learning classifier system].\" In''International Conference on Parallel Problem Solving from Nature'', pp. 1032-1041. Springer Berlin Heidelberg, 2004.</ref> in order to develop a better theoretical understanding of the LCS framework.  Bacardit introduced '''GAssist'''<ref>Peñarroya, Jaume Bacardit. \"Pittsburgh genetic-based machine learning in the data mining era: representations, generalization, and run-time.\" PhD diss., Universitat Ramon Llull, 2004.</ref> and '''BioHEL''',<ref>{{Cite journal|last=Bacardit|first=Jaume|last2=Burke|first2=Edmund K.|last3=Krasnogor|first3=Natalio|date=2008-12-12|title=Improving the scalability of rule-based evolutionary learning|journal=Memetic Computing|language=en|volume=1|issue=1|pages=55–67|doi=10.1007/s12293-008-0005-4|issn=1865-9284}}</ref> Pittsburgh-style LCSs designed for [[data mining]] and [[scalability]] to large datasets in [[bioinformatics]] applications.  In 2008, Drugowitsch published the book titled \"Design and Analysis of Learning Classifier Systems\" including some theoretical examination of LCS algorithms.<ref>{{Cite book|title=Design and Analysis of Learning Classifier Systems - Springer|volume = 139|doi=10.1007/978-3-540-79866-8|series = Studies in Computational Intelligence|year = 2008|isbn = 978-3-540-79865-1|last1 = Drugowitsch|first1 = Jan}}</ref>  Butz introduced the first rule online learning visualization within a [[Graphical user interface|GUI]] for XCSF<ref name=\":9\" /> (see the image at the top of this page).  Urbanowicz extended the UCS framework and introduced '''ExSTraCS,''' explicitly designed for [[supervised learning]] in noisy problem domains (e.g. epidemiology and bioinformatics).<ref>Urbanowicz, Ryan J., Gediminas Bertasius, and Jason H. Moore. \"[http://www.seas.upenn.edu/~gberta/uploads/3/1/4/8/31486883/urbanowicz_2014_exstracs_algorithm.pdf An extended michigan-style learning classifier system for flexible supervised learning, classification, and data mining].\" In ''International Conference on Parallel Problem Solving from Nature'', pp. 211-221. Springer International Publishing, 2014.</ref>  ExSTraCS integrated (1) expert knowledge to drive covering and genetic algorithm towards important features in the data,<ref>Urbanowicz, Ryan J., Delaney Granizo-Mackenzie, and Jason H. Moore. \"[https://pdfs.semanticscholar.org/b407/8f8bb6aa9e39e84b0b20874662a6ed8b7df1.pdf Using expert knowledge to guide covering and mutation in a michigan style learning classifier system to detect epistasis and heterogeneity].\" In''International Conference on Parallel Problem Solving from Nature'', pp. 266-275. Springer Berlin Heidelberg, 2012.</ref> (2) a form of long-term memory referred to as attribute tracking,<ref>{{Cite book|last=Urbanowicz|first=Ryan|last2=Granizo-Mackenzie|first2=Ambrose|last3=Moore|first3=Jason|date=2012-01-01|title=Instance-linked Attribute Tracking and Feedback for Michigan-style Supervised Learning Classifier Systems|journal=Proceedings of the 14th Annual Conference on Genetic and Evolutionary Computation|series=GECCO '12|location=New York, NY, USA|publisher=ACM|pages=927–934|doi=10.1145/2330163.2330291|isbn=9781450311779}}</ref> allowing for more efficient learning and the characterization of heterogeneous data patterns, and (3) a flexible rule representation similar to Bacardit's mixed discrete-continuous attribute list representation.<ref>{{Cite book|last=Bacardit|first=Jaume|last2=Krasnogor|first2=Natalio|date=2009-01-01|title=A Mixed Discrete-continuous Attribute List Representation for Large Scale Classification Domains|journal=Proceedings of the 11th Annual Conference on Genetic and Evolutionary Computation|series=GECCO '09|location=New York, NY, USA|publisher=ACM|pages=1155–1162|doi=10.1145/1569901.1570057|isbn=9781605583259|citeseerx=10.1.1.158.7314}}</ref>  Both Bacardit and Urbanowicz explored statistical and visualization strategies to interpret LCS rules and perform knowledge discovery for data mining.<ref name=\":11\">{{Cite journal|last=Urbanowicz|first=R. J.|last2=Granizo-Mackenzie|first2=A.|last3=Moore|first3=J. H.|date=2012-11-01|title=An analysis pipeline with statistical and visualization-guided knowledge discovery for Michigan-style learning classifier systems|journal=IEEE Computational Intelligence Magazine|volume=7|issue=4|pages=35–45|doi=10.1109/MCI.2012.2215124|issn=1556-603X|pmc=4244006|pmid=25431544}}</ref><ref name=\":12\">{{cite journal | last1 = Bacardit | first1 = Jaume | last2 = Llorà | first2 = Xavier | year = 2013 | title = Large‐scale data mining using genetics‐based machine learning | url = | journal = Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery | volume = 3 | issue = 1| pages = 37–61 | doi=10.1002/widm.1078}}</ref>  Browne and Iqbal explored the concept of reusing building blocks in the form of code fragments and were the first to solve the 135-bit multiplexer benchmark problem by first learning useful building blocks from simpler multiplexer problems.<ref>{{Cite journal|last=Iqbal|first=Muhammad|last2=Browne|first2=Will N.|last3=Zhang|first3=Mengjie|date=2014-08-01|title=Reusing Building Blocks of Extracted Knowledge to Solve Complex, Large-Scale Boolean Problems|journal=IEEE Transactions on Evolutionary Computation|pages=465–480|volume=18|issue=4|doi=10.1109/tevc.2013.2281537}}</ref> '''ExSTraCS 2.0''' was later introduced to improve Michigan-style LCS scalability, successfully solving the 135-bit multiplexer benchmark problem for the first time directly.<ref name=\":0\"/>  The n-bit [[multiplexer]] problem is highly [[Epistasis|epistatic]] and [[Homogeneity and heterogeneity|heterogeneous]], making it a very challenging [[machine learning]] task.\n\n== Variants ==\n\n=== Michigan-Style Learning Classifier System ===\nMichigan-Style LCSs are characterized by a population of rules where the genetic algorithm operates at the level of individual rules and the solution is represented by the entire rule population.  Michigan style systems also learn incrementally which allows them to perform both reinforcement learning and supervised learning, as well as both online and offline learning.  Michigan-style systems have the advantage of being applicable to a greater number of problem domains, and the unique benefits of incremental learning.\n\n=== Pittsburgh-Style Learning Classifier System ===\nPittsburgh-Style LCSs are characterized by a population of variable length rule-sets where each rule-set is a potential solution.  The genetic algorithm typically operates at the level of an entire rule-set.  Pittsburgh-style systems can also uniquely evolve ordered rule lists, as well as employ a default rule.  These systems have the natural advantage of identifying smaller rule sets, making these systems more interpretable with regards to manual rule inspection.\n\n=== Hybrid systems ===\nSystems that seek to combine key strengths of both systems have also been proposed.\n\n== Advantages ==\n* Adaptive: They can acclimate to a changing environment in the case of online learning.\n* Model free: They make limited assumptions about the environment, or the patterns of association within the data.\n** The can model complex, epistatic, heterogeneous, or distributed underlying patterns without relying on prior knowledge.\n** They make no assumptions about the number of predictive vs. non-predictive features in the data.\n* Ensemble Learner: No single model is applied to a given instance that universally provides a prediction.  Instead a relevant and often conflicting set of rules contribute a 'vote' which can be interpreted as a fuzzy prediction.\n* Stochastic Learner: Non-deterministic learning is advantageous in large-scale or high complexity problems where deterministic or exhaustive learning becomes intractable.\n* Implicitly Multi-objective: Rules evolve towards accuracy with implicit and explicit pressures encouraging maximal generality/simplicity. This implicit generalization pressure is unique to LCS.  Effectively, more general rules, will appear more often in match sets.  In turn, they have a more frequent opportunity to be selected as parents, and pass on their more general (genomes) to offspring rules.  \n* Interpretable:In the interest of data mining and knowledge discovery individual LCS rules are logical, and can be made to be human interpretable IF:THEN statements.  Effective strategies have also been introduced to allow for global knowledge discovery identifying significant features, and patterns of association from the rule population as a whole.<ref name=\":11\" />\n* Flexible application\n** Single or multi-step problems\n** Supervised, Reinforcement or Unsupervised Learning\n** Binary Class and Multi-Class Classification\n** Regression\n** Discrete or continuous features (or some mix of both types)\n** Clean or noisy problem domains\n** Balanced or imbalanced datasets.\n** Accommodates missing data (i.e. missing feature values in training instances)\n\n== Disadvantages ==\n* Limited Software Availability: There are a limited number of open source, accessible LCS implementations, and even fewer that are designed to be user friendly or accessible to machine learning practitioners. \n* Interpretation:  While LCS algorithms are certainly more interpretable than some advanced machine learners, users must interpret a set of rules (sometimes large sets of rules to comprehend the LCS model.). Methods for rule compaction, and interpretation strategies remains an area of active research.\n* Theory/Convergence Proofs:  There is a relatively small body of theoretical work behind LCS algorithms.  This is likely due to their relative algorithmic complexity (applying a number of interacting components) as well as their stochastic nature. \n* Overfitting:  Like any machine learner, LCS can suffer from [[overfitting]] despite implicit and explicit generalization pressures.\n* Run Parameters: LCSs often have many run parameters to consider/optimize. Typically, most parameters can be left to the community determined defaults with the exception of two critical parameters: Maximum rule population size, and the maximum number of learning iterations.   Optimizing these parameters are likely to be very problem dependent. \n* Notoriety:  Despite their age, LCS algorithms are still not widely known even in machine learning communities.  As a result, LCS algorithms are rarely considered in comparison to other established machine learning approaches.  This is likely due to the following factors:  (1) LCS is a relatively complicated algorithmic approach, (2) LCS, rule-based modeling is a different paradigm of modeling than almost all other machine learning approaches. (3) LCS software implementations are not as common.\n* Computationally Expensive:  While certainly more feasible than some exhaustive approaches, LCS algorithms can be computationally expensive.  For simple, linear learning problems there is no need to apply an LCS.  LCS algorithms are best suited to complex problem spaces, or problem spaces in which little prior knowledge exists.\n\n== Problem domains ==\n* Adaptive-control\n* Data Mining\n* Engineering Design\n* Feature Selection\n* Function Approximation\n* Game-Play\n* Image Classification\n* Knowledge Handeling\n* Medical Diagnosis\n* Modeling\n* Navigation\n* Optimization\n* Prediction\n* Querying\n* Robotics\n* Routing\n* Rule-Induction\n* Scheduling\n* Strategy\n\n== Terminology ==\nThe name, \"Learning Classifier System (LCS)\", is a bit misleading since there are many [[machine learning]] algorithms that 'learn to classify' (e.g. [[decision tree]]s, [[artificial neural network]]s), but are not LCSs.  The term 'rule-based machine learning (RBML)' is useful, as it more clearly captures the essential 'rule-based' component of these systems, but it also generalizes to methods that are not considered to be LCSs (e.g. [[association rule learning]], or [[artificial immune system]]s). More general terms such as, 'genetics-based machine learning', and even 'genetic algorithm'<ref name=\":8\">Congdon, Clare Bates. \"A comparison of genetic algorithms and other machine learning systems on a complex classification task from common disease research.\" PhD diss., The University of Michigan, 1995.</ref> have also been applied to refer to what would be more characteristically defined as a learning classifier system.  Due to their similarity to [[genetic algorithm]]s, Pittsburgh-style learning classifier systems are sometimes generically referred to as 'genetic algorithms'.  Beyond this, some LCS algorithms, or closely related methods, have been referred to as 'cognitive systems',<ref name=\":2\" /> 'adaptive agents', '[[production system (computer science)|production system]]s', or generically as a 'classifier system'.<ref>{{Cite journal|last=Booker|first=L. B.|last2=Goldberg|first2=D. E.|last3=Holland|first3=J. H.|date=1989-09-01|title=Classifier systems and genetic algorithms|url=http://www.sciencedirect.com/science/article/pii/0004370289900507|journal=Artificial Intelligence|volume=40|issue=1|pages=235–282|doi=10.1016/0004-3702(89)90050-7}}</ref><ref>Wilson, Stewart W., and David E. Goldberg. \"A critical review of classifier systems.\" In ''Proceedings of the third international conference on Genetic algorithms'', pp. 244-255. Morgan Kaufmann Publishers Inc., 1989.</ref>   This variation in terminology contributes to some confusion in the field.\n\nUp until the 2000s nearly all learning classifier system methods were developed with reinforcement learning problems in mind. As a result, the term ‘learning classifier system’ was commonly defined as the combination of ‘trial-and-error’ reinforcement learning with the global search of a genetic algorithm. Interest in supervised learning applications, and even unsupervised learning have since broadened the use and definition of this term.\n\n== See also ==\n* Rule-based machine learning\n* [[Production system (computer science)|Production system]]\n* [[Expert system]]\n* [[Genetic algorithm]]\n* [[Association rule learning]]\n* [[Artificial immune system]]\n* [[Population-based incremental learning|Population-based Incremental Learning]]\n* [[Machine learning]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n\n=== Video tutorial ===\n* [https://www.youtube.com/watch?v=CRge_cZ2cJc Learning Classifier Systems in a Nutshell] - (2016) Go inside a basic LCS algorithm to learn their components and how they work.\n\n=== Webpages ===\n* [http://gbml.org/ LCS & GBML Central]\n* [http://www.cems.uwe.ac.uk/lcsg/ UWE Learning Classifier Research Group]\n* [http://prediction-dynamics.com/ Prediction Dynamics]\n\n[[Category:Evolutionary algorithms]]"
    },
    {
      "title": "Melomics",
      "url": "https://en.wikipedia.org/wiki/Melomics",
      "text": "{{Infobox website\n| name     = Melomics Media\n| logo     = LogoMelomics.svg\n| logo_size = 100px\n| company_type     = [[University spin-off]], [[University of Malaga]] \n| traded_as        = \n| foundation       = 2012\n| founder          = Francisco Vico\n| dissolved        =\n| location         = \n| incorporated     =\n| area_served      = Worldwide\n| key_people       = \n| industry         = Music\n| products         = \n| services         = {{Unbulleted list| on-line music | royalty-free music | music-based mobile apps }}\n| revenue          = \n| operating_income = \n| net_income       = \n| assets           = \n| equity           = \n| owner            = \n| parent           = \n| divisions        = \n| subsid           = \n| url              = [http://melomics.uma.es/ melomics.uma.es]\n| ipv6             =\n| alexa            = \n| advertising      = None\n| registration     = Optional\n| num_users        = \n| language         = English\n| launch_date      = July 2012\n| current_status   = Active\n| screenshot       = \n| caption          = Melomics landing page as of January 2013\n}}\n\n[[File:MelomicsShowroom.jpg|thumb|right|Melomics Media showroom at [[Andalusia Technology Park]] | 200px]]\n\n'''Melomics''' (derived from \"genomics of melodies\") is a computational system for the [[algorithmic composition|automatic composition of music]] (with no human intervention), based on bioinspired algorithms.<ref name=\"BBC\"/>\n\n==Technological aspects==\n\nMelomics applies an [[evolutionary music|evolutionary approach]] to music composition, i.e., music pieces are obtained by simulated evolution. These themes compete to better adapt to a proper fitness function, generally grounded on formal and aesthetic criteria. The Melomics system encodes each theme in a genome, and the entire population of music pieces undergoes [[evo-devo]] dynamics (i.e., pieces read-out mimicking a complex embryological development process).<ref name=\"AIM\">{{cite journal|last= Sánchez|first= C|last2= Moreno|first2= F|last3= Albarracin|first3= D|last4= Fernandez|first4= JD|last5= Vico|first5= FJ|date= 2013|title= Melomics: A Case-Study of AI in Spain|url= http://geb.uma.es/images/papers/AIMAGAZINE.pdf|journal= AI Magazine|volume= 34|issue= 3|pages= 99–103}}{{Dead link|date=May 2019 |bot=InternetArchiveBot |fix-attempted=yes }}</ref><ref>{{cite journal|last=Stieler|first=Wolfgang|title=Die Mozart-Maschine|journal=Technology Review (Germany)|year=2012|volume=December|pages=26–35| url =http://www.heise.de/tr/artikel/Die-Mozart-Maschine-1750720.html}}</ref><ref name=\"Nature\">{{cite journal | last=Ball|first=Philip| title= Algorithmic Rapture| journal= Nature| volume= 188|issue=7412| year =2012| pages= 456| doi = 10.1038/488458a}}</ref><ref>{{cite journal |last= Fernandez |first= JD|last2= Vico |first2= FJ|date= 2013| title= AI Methods in Algorithmic Composition: A Comprehensive Survey|url= http://www.jair.org/media/3908/live-3908-7454-jair.pdf|journal= Journal of Artificial Intelligence Research|volume= 48|pages= 513–582}}</ref> The system is fully autonomous: once programmed, it composes music without human intervention.\n\nThis technology has been transferred to industry as an academic spin-off, Melomics Media, which has provided and reprogrammed a new computer cluster that created a huge collection of popular music. The results of this evolutionary computation are being stored in Melomics' site,<ref>{{cite web|title=Melomics.com|url=http://Melomics.com|accessdate=6 December 2011}}</ref> which nowadays constitutes a vast repository of music content. A differentiating feature is that pieces are available in three types of formats: playable ([[MP3]]), editable ([[MIDI]] and [[MusicXML]]) and readable (score in [[PDF]]).\n\n==Computer clusters==\n\nThe Melomics computational system includes two computer clusters: [[Melomics109]] and [[Iamus (computer)|Iamus]], dedicated to popular and artistic music, respectively.<ref name=\"AIM\" /><ref>{{cite news | title= Artificial music: The computers that create melodies|publisher= BBC Future|last= Ball |first= Philip| publication-date =8 August 2014| url =http://www.bbc.com/future/story/20140808-music-like-never-heard-before/}}</ref>\n\n===Melomics109: The largest repository of popular music===\n\n[[Melomics109]] is cluster programmed and integrated in the Melomics system.<ref name=\"taz\">{{cite news | last=Lenhart|first=Christian| title= Die Mozart-Maschine| work= taz.de| date =13 January 2013| url =http://www.taz.de/Computer-komponiert-klassische-Musik/!108691/}}</ref> Its first product is a vast repository of popular music compositions (roughly 1 billion), covering all essential styles. In addition to [[MP3]], all songs are available in editable formats ([[MIDI]]);<ref name=\"Time\" /> and music is licensed under [[CC0#Zero .2F Public domain|CC0]], meaning that it is freely downloadable.<ref name=\"taz\" /><ref name=\"THP\">{{cite news | last=Bosker|first=Bianca| title= Life As Francisco Vico, Creator Of The Incredible Computer-Composer Iamus| work= The Huffington Post| date =13 January 2013| url =http://www.huffingtonpost.com/2013/01/13/francisco-vico-iamus-melomics_n_2457374.html}}</ref>\n\n[[0music (album)|0music]] is the first album published by [[Melomics109]], which is available in [[MP3]] and [[MIDI]] formats, under [[CC0#Zero .2F Public domain|CC0]] license.\n\nIt has been argued that, by making such amount of editable, original and [[royalty-free]] music accessible to people, Melomics may accelerate the process of [[commoditization]] of music, and change the way music is composed and consumed in the future.<ref name=\"BBC\">{{cite news | last=Smith|first=Sylvia| title= Iamus: Is this the 21st century's answer to Mozart?| work= BBC News Technology| date =3 January 2013| url =https://www.bbc.co.uk/news/technology-20889644}}</ref><ref name=\"Time\">{{cite news | last=Peckham|first=Matt| title= Finally, a computer that writes contemporary music without human help| work= Time Magazine| date =4 January 2013| url =http://techland.time.com/2013/01/04/finally-a-computer-that-writes-contemporary-music-without-human-help/}}</ref><ref name=\"THP\" />\n\n===Iamus: First album of professional contemporary music by a non-human intelligence===\n\nIn the first stages of the development of the Melomics system, [[Iamus (computer)|Iamus]] composed ''Opus one'' (on October 15, 2010), arguably the first fragment of professional contemporary classical music ever composed by a computer in its own style, rather than attempting to emulate the style of existing composers. The first full composition (also in contemporary classic style), ''[[Hello World! (composition)|Hello World!]]'', premiered exactly one year after the creation of ''Opus one'', on October 15, 2011. Four later works premiered on July 2, 2012, and were broadcast live<ref name=iamus-CD>{{cite news|last=Ball|first=Philip|title=Iamus, classical music's computer composer, live from Malaga|url=https://www.theguardian.com/music/2012/jul/01/iamus-computer-composes-classical-music|accessdate=2 July 2012|newspaper=The Guardian|date=1 July 2012}}</ref> from the School of Computer Science at Universidad de Málaga<ref>{{cite web|author=School of Computer Science (University of Malaga - Spain) |url=https://www.youtube.com/watch?v=ygMRtXxqEBw |title=Can machines be creative? (live from Malaga) |publisher=YouTube |date=2012-07-02 |accessdate=2012-10-05}}</ref> as part of the events included in the [[Alan Turing Year|Alan Turing year]]. The compositions performed at this event were later recorded by the [[London Symphony Orchestra]], creating [[Iamus (album)|Iamus' eponymous first album]], which New Scientist reported as the \"first complete album to be composed solely by a computer and recorded by human musicians.\"<ref>{{cite  journal | title=Computer composer honours Turing's centenary|journal=New Scientist|date=5 July 2012|url =https://www.newscientist.com/article/mg21528724.300-computer-composer-honours-turings-centenary.html}}</ref>\n\nCommenting on the quality and authenticity of the music, Stephen Smoliar, critic of classical music at ''[[The San Francisco Examiner]]'', commented \"What is primary is the act of making the music itself engaged by the performers and how the listener responds to what those performers do... what is most interesting about the documents generated by Iamus is their capacity to challenge the creative talents of performing musicians\".<ref>{{cite news|last=Smoliar|first=Stephen|title=Thoughts about Iamus and the composition of music by computer|newspaper=The Examiner|date=4 January 2013|url=http://geb.uma.es/otros-recuros/examiner|access-date=12 June 2014|archive-url=https://archive.is/20140613160748/http://geb.uma.es/otros-recuros/examiner|archive-date=13 June 2014|dead-url=yes|df=dmy-all}} Accessed: 10 January 2013.</ref>\n\n==Applications==\n\nMelomics' empathic music has been tested in a number of therapeutic [[clinical trials]],<ref>{{cite conference |last= Caparros-Gonzalez |first= R |last2= Torre-Luque |first2= A |last3= Buela-Casal |first3= G |last4= Vico |first4= F |title= MELOMICS relaxing music for premature infants: preliminary results testing its effects on physiological parameters|chapter=10th International Conference on Child and Adolescent Psychopathology |date=20{{ndash}}22 July 2015 |location=University of Roehampton (London)}}</ref><ref>{{cite conference |last= Torre-Luque |first= A |last2= Caparros-Gonzalez |first2= R |last3= Bastard |first3= T |last4= Buela-Casal |first4= G |last5= Vico |first5= F |title= Effects of relaxing music listening after the exposure to acute stress within laboratory settings|chapter=7th International Congress of Clinical Psychology |date=14{{ndash}}16 November 2014 |location=Seville |url=http://www.ugr.es/~aepc/WEBCLINICA/ENGLISH/}}</ref><ref>{{cite conference |last= Seinfeld |first= S |last2= Slater |first2= M |last3= Vico |first3= F |last4= Sanchez-Vives |first4= M |title= The influence of relaxing music on anxiety induced by fear of heights in an immersive virtual reality experience|chapter=120th APA Convention |date=1{{ndash}}5 August 2014 |location=Seville |url=http://listserv.tcu.edu/cgi-bin/wa.exe?A3=ind1204&L=ALB-ISCP&E=base64&P=443157&B=--20cf302d4dd42a2ad004bd846282&T=application%2Fpdf;%20name=%22APA%20Conv%20Div%203%206%202012.pdf%22&N=APA%20Conv%20Div%203%206%202012.pdf&attachment=q}}</ref><ref name=\"allergy\">{{cite journal |last= Requena |first= G |last2= Sanchez |first2= C |last3= Corzo-Higueras |first3= JL |last4= Reyes-Alvarado |first4= S |last5= Rivas-Ruiz |first5= F |last6= Vico |first6= F |last7= Raglio |first7= A|date= 2014| title= Melomics music medicine (M<sup>3</sup>) to lessen pain perception during pediatric prick test procedure|journal= Pediatric Allergy and Immunology|volume= 25 |issue= 7 |pages= 721–724 |doi= 10.1111/pai.12263|pmid= 25115240 }}</ref> evidencing positive effects in reducing fear of heights, acute stress and pain perception. One of the studies resulted in a reduction of almost two thirds of pain perception in children undergoing a standard Skin Prick Test during allergy testing, as compared to the standard procedure.<ref name=\"allergy\" /> Some of these experiments made use of free mobile apps to adapt music to daily activity,<ref>{{cite web |url=https://play.google.com/store/apps/developer?id=Melomics&hl=en |title=Melomics apps |publisher=Google Play |accessdate=21 June 2015}}</ref> such as jogging,<ref>{{cite web |url=https://play.google.com/store/apps/details?id=com.melomics.sport_free&hl=en |title=Sports free |publisher=Google Play |accessdate=21 June 2015}}</ref> or commuting,<ref>{{cite web |url=https://play.google.com/store/apps/details?id=com.melomics.car_free&hl=en |title=Commuting free |publisher=Google Play |accessdate=21 June 2015}}</ref> but also for therapeutic use, such as lessening stress before an exam,<ref>{{cite web |url=https://play.google.com/store/apps/details?id=com.melomics.school_free&hl=en |title=School free |publisher=Google Play |accessdate=21 June 2015}}</ref> reducing chronic pain,<ref>{{cite web |url=https://play.google.com/store/apps/details?id=com.melomics.pain_free&hl=en |title=Chronic Pain |publisher=Google Play |accessdate=21 June 2015}}</ref> insomnia,<ref>{{cite web |url=https://play.google.com/store/apps/details?id=com.melomics.sleep_free&hl=en |title=Fall asleep for adults |publisher=Google Play |accessdate=21 June 2015}}</ref> and to help children go to sleep.<ref>{{cite web |url=https://play.google.com/store/apps/details?id=com.melomics.sleep_kids_free&hl=en |title=Fall asleep for kids |publisher=Google Play |accessdate=21 June 2015}}</ref>\n\nOngoing efforts to allow Melomics to adapt music in real-time to changes in the physiological state of the listener, and to [[Sound trademark|music branding]] were also reported.<ref name=\"THP\" /><ref>{{cite web |url=http://geb.uma.es/melomics-music-applications/ |title=Music applications |publisher=Universidad de Malaga |accessdate=26 November 2014 |archive-url=https://web.archive.org/web/20160303213031/http://geb.uma.es/melomics-music-applications/ |archive-date=3 March 2016 |dead-url=yes |df=dmy-all }}(archived at )</ref>\n\n==References==\n{{Commons|Melomics logos and screenshots}}\n{{reflist|33em}}\n<!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. --->\n\n==External links==\n*[http://melomics.uma.es/ Melomics Homepage]\n*[https://web.archive.org/web/20140808054022/http://geb.uma.es/melomics Melomics page at University of Malaga (Spain)]\n\n[[Category:Biotechnology]]\n[[Category:Music technology]]\n[[Category:Evolutionary algorithms]]\n[[Category:Spanish Supercomputing Network]]"
    },
    {
      "title": "Memetic algorithm",
      "url": "https://en.wikipedia.org/wiki/Memetic_algorithm",
      "text": "{{Evolutionary algorithms}}\n{{Context|date=February 2011}}\n\nIn [[computer science]] and [[operations research]], a '''memetic algorithm''' (MA) is an extension of the traditional [[genetic algorithm]]. It uses a local search technique to reduce the likelihood of the premature convergence.<ref>{{cite journal|url=https://arxiv.org/ftp/arxiv/papers/1004/1004.0574.pdf|title= A Comparison between Memetic algorithm and Genetic algorithm for the cryptanalysis of Simplified Data Encryption Standard algorithm|author=Poonam Garg |journal=International Journal of Network Security & Its Applications (IJNSA)|volume=1|issue=1|date=April 2009}}</ref> \n\nMemetic algorithms represent one of the recent growing areas of research in [[evolutionary computation]]. The term MA is now widely used as a synergy of evolutionary or any population-based approach with separate individual learning or local improvement procedures for problem search. Quite often, MAs are also referred to in the literature as Baldwinian [[evolutionary algorithm]]s (EAs), Lamarckian EAs, cultural algorithms, or genetic local search.\n\n==Introduction==\nInspired by both Darwinian principles of natural evolution and [[Richard Dawkins|Dawkins']] notion of a [[meme]], the term “Memetic Algorithm” (MA) was introduced by Moscato in his technical report<ref name=martial_arts>{{cite journal|last=Moscato|first=P.|year=1989|title=On Evolution, Search, Optimization, Genetic Algorithms and Martial Arts: Towards Memetic Algorithms|journal=Caltech Concurrent Computation Program|volume=|issue=report 826}}</ref> in 1989\nwhere he viewed MA as being close to a form of population-based hybrid [[genetic algorithm]] (GA) coupled with an individual learning procedure capable of performing local refinements. The metaphorical parallels, on the one hand, to Darwinian evolution and, on the other hand, between memes and domain specific (local search) [[heuristic]]s are captured within memetic algorithms thus rendering a methodology that balances well between generality and problem specificity. This two-stage nature makes them a special case of [[Dual-phase evolution]].\n\nIn a more diverse context, memetic algorithms are now used under various names including Hybrid Evolutionary Algorithms, Baldwinian Evolutionary Algorithms, Lamarckian Evolutionary Algorithms, Cultural Algorithms, or Genetic Local Search. In the context of complex optimization, many different instantiations of memetic algorithms have been reported across a wide range of [[#Applications|application domains]], in general, converging to high-quality solutions more efficiently than their conventional evolutionary counterparts.\n\nIn general, using the ideas of memetics within a computational framework is called \"Memetic Computing or Memetic Computation\" (MC).<ref name=MC2011>{{cite journal|last=Chen|first=X. S. |last2=Ong|first2=Y. S.|last3=Lim|first3=M. H.|last4=Tan|first4=K. C.|year=2011|title=A Multi-Facet Survey on Memetic Computation|journal=[[IEEE Transactions on Evolutionary Computation]]|volume=15|issue=5|pages=591–607|doi=10.1109/tevc.2011.2132725}}</ref><ref name=MC2010>{{cite journal|last=Chen|first=X. S. |last2=Ong|first2=Y. S.|last3=Lim|first3=M. H.|year=2010|title=Research Frontier: Memetic Computation - Past, Present & Future|journal=[[IEEE Computational Intelligence Society#Publications|IEEE Computational Intelligence Magazine]]|volume=5|issue=2|pages=24–36|doi=10.1109/mci.2010.936309}}</ref>\nWith MC, the traits of Universal Darwinism are more appropriately captured. Viewed in this perspective, MA is a more constrained notion of MC. More specifically, MA covers one area of MC, in particular dealing with areas of evolutionary algorithms that marry other deterministic refinement techniques for solving optimization problems. MC extends the notion of memes to cover conceptual entities of knowledge-enhanced procedures or representations.\n\n==The development of MAs==\n\n===1st generation===\nThe first generation of MA refers to hybrid [[algorithm]]s, a marriage between a population-based global search (often in the form of an evolutionary algorithm) coupled with a cultural evolutionary stage. This first generation of MA although encompasses characteristics of cultural evolution (in the form of local refinement) in the search cycle, it may not qualify as a true evolving system according to [[Universal Darwinism]], since all the core principles of inheritance/memetic transmission, variation, and selection are missing. This suggests why the term MA stirred up criticisms and controversies among researchers when first introduced.<ref name=martial_arts/>\n\n;Pseudo code:\n    '''Procedure''' Memetic Algorithm\n    '''Initialize:''' Generate an initial population;\n    '''while''' Stopping conditions are not satisfied '''do'''\n        ''Evaluate'' all individuals in the population.\n        ''Evolve'' a new population using stochastic search operators.\n        {{nowrap|''Select'' the subset of individuals, <math>\\Omega_{il}</math>}}, that should undergo the individual improvement procedure.\n        {{nowrap|'''for''' each individual in <math>\\Omega_{il}</math> '''do'''}}\n            ''Perform'' individual learning using meme(s) {{nowrap|with frequency or probability of <math>f_{il}</math>}}, {{nowrap|for a period of <math>t_{il}</math>.}}\n            ''Proceed'' with Lamarckian or Baldwinian learning.\n        '''end for'''\n    '''end while'''\n\n===2nd generation===\nMulti-meme,<ref name=krasnogor1999cga>{{cite journal|author=Krasnogor N.|title=Coevolution of genes and memes in memetic algorithms|journal=Graduate Student Workshop|year=1999|pages=371}}</ref> [[Hyper-heuristic]]<ref name=kendall2002cfa>{{cite conference|author=Kendall G. and Soubeiga E. and Cowling P.|title=Choice function and random hyperheuristics|conference=4th Asia-Pacific Conference on Simulated Evolution and Learning. SEAL 2002|pages=667–671}}</ref><ref name=burke2013>{{cite journal|author1=Burke E. K. |author2=Gendreau M. |author3=Hyde M. |author4=Kendall G. |author5=Ochoa G. |author6=Ouml |author7=zcan E. |author8=Qu R. |title=Hyper-heuristics: A Survey of the State of the Art|journal=Journal of the Operational Research Society|year=2013|volume=64|pages=1695–1724|issue=12|doi=10.1057/jors.2013.71|citeseerx=10.1.1.384.9743}}</ref> \nand Meta-Lamarckian MA<ref name=ong2004mll>{{cite journal|author1=Ong Y. S.  |author2=Keane A. J. |lastauthoramp=yes |title=Meta-Lamarckian learning in memetic algorithms|journal=IEEE Transactions on Evolutionary Computation|year=2004|volume=8|pages=99–110|doi=10.1109/TEVC.2003.819944|issue=2}}</ref> are referred to as second generation MA exhibiting the principles of memetic transmission and selection in their design. In Multi-meme MA, the memetic material is encoded as part of the [[genotype]]. Subsequently, the decoded meme of each respective individual/[[chromosome]] is then used to perform a local refinement. The memetic material is then transmitted through a simple inheritance mechanism from parent to offspring(s). On the other hand, in hyper-heuristic and meta-Lamarckian MA, the pool\nof candidate memes considered will compete, based on their past merits in generating local improvements through a reward mechanism, deciding on which meme to be selected to proceed for future local refinements. Memes with a higher reward have a greater chance of being replicated or copied. For a review on second generation MA; i.e., MA considering multiple individual learning methods within\nan evolutionary system, the reader is referred to.<ref name=ong2006cam>{{cite journal|author=Ong Y. S. and Lim M. H. and Zhu N. and Wong K. W.|title=Classification of Adaptive Memetic Algorithms: A Comparative Study|journal=IEEE Transactions on Systems Man and Cybernetics -- Part B.|year=2006|volume=36|pages=141–152|doi=10.1109/TSMCB.2005.856143|issue=1}}</ref>\n\n===3rd generation===\nCo-evolution<ref name=smith2007cma>{{cite journal|author=Smith J. E.|title=Coevolving Memetic Algorithms: A Review and Progress Report|journal=IEEE Transactions on Systems Man and Cybernetics - Part B|year=2007|volume=37|pages=6–17|doi=10.1109/TSMCB.2006.883273|issue=1}}</ref> and self-generating MAs<ref name=krasnogor2002ttm>{{cite journal|author1=Krasnogor N.  |author2=Gustafson S. |lastauthoramp=yes |title=Toward truly \"memetic\" memetic algorithms: discussion and proof of concepts|journal=Advances in Nature-Inspired Computation: The PPSN VII Workshops. PEDAL (Parallel Emergent and Distributed Architectures Lab). University of Reading|year=2002}}</ref> may be regarded as 3rd generation MA where all three principles satisfying the definitions of a basic evolving system have been considered. In contrast to 2nd generation MA which assumes that the memes to be used are known a priori, 3rd generation MA utilizes a rule-based local search to supplement candidate solutions within the evolutionary system, thus capturing regularly repeated features or patterns in the problem space.\n\n==Some design notes==\nThe frequency and intensity of individual learning directly define the degree of evolution (exploration) against\nindividual learning (exploitation) in the MA search, for a given fixed limited computational budget. Clearly, a more intense\nindividual learning provides greater chance of convergence to the local optima but limits the amount of evolution that\nmay be expended without incurring excessive computational resources. Therefore, care should be taken when setting\nthese two parameters to balance the computational budget available in achieving maximum search performance. When only a portion of the population individuals undergo learning, the issue of which subset of individuals to improve need to be considered to maximize the utility of MA search. Last but not least, the individual learning procedure/meme used also favors a different neighborhood structure, hence the need to decide which meme or memes to use for a given optimization problem at hand would be required.\n\n===How often should individual learning be applied?===\nOne of the first issues pertinent to memetic algorithm design is to consider how often the individual learning should be applied; i.e., individual learning frequency. In one case,<ref name=hart1994ago>{{cite journal|author=Hart W. E.|title=Adaptive Global Optimization with Local Search|publisher=University of California|year=1994|url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.473.1370&rep=rep1&type=pdf}}</ref> the effect of individual learning frequency on MA search performance was considered where various configurations of the individual learning frequency at different stages of the MA search were investigated. Conversely, it was shown elsewhere<ref name=ku2000sle>{{cite journal|author=Ku K. W. C. and Mak M. W. and Siu W. C.|title=A study of the Lamarckian evolution of recurrent neural networks|journal=IEEE Transactions on Evolutionary Computation|year=2000|volume=4|pages=31–42|doi=10.1109/4235.843493|issue=1}}</ref> that it may be worthwhile to apply individual learning on every individual if the computational complexity of the individual learning is relatively low.\n\n===On which solutions should individual learning be used?===\nOn the issue of selecting appropriate individuals among the EA population that should undergo individual learning, fitness-based and distribution-based strategies were studied for adapting the probability of applying individual learning on the population of chromosomes in continuous parametric search problems with Land<ref name=land1998eal>{{cite journal|author=Land M. W. S.|title=Evolutionary Algorithms with Local Search for Combinatorial Optimization|publisher=UNIVERSITY OF CALIFORNIA|year=1998|url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.8986&rep=rep1&type=pdf}}</ref> extending the work to combinatorial optimization problems. Bambha et al. introduced a simulated heating technique for systematically integrating parameterized individual learning into evolutionary algorithms to achieve maximum solution quality.<ref name=bambha2004sip>{{cite journal|author=Bambha N. K. and Bhattacharyya S. S. and Teich J. and Zitzler E.|title=Systematic integration of parameterized local search into evolutionary algorithms|journal=IEEE Transactions on Evolutionary Computation|year=2004|volume=8|pages=137–155|doi=10.1109/TEVC.2004.823471|issue=2}}</ref>\n\n===How long should individual learning be run?===\nIndividual learning intensity, <math>t_{il}</math>, is the amount of computational budget allocated to an iteration of individual learning; i.e., the maximum computational budget allowable for individual learning to expend on improving a single solution.\n\n===What individual learning method or meme should be used for a particular problem or individual?===\nIn the context of continuous optimization, individual learning exists in the form of local heuristics or conventional exact enumerative methods.<ref name=schwefel1995eao>{{cite book|title=Evolution and optimum seeking|publisher=Wiley New York|year=1995|author=Schwefel H. P.}}</ref> Examples of individual learning strategies include the hill climbing, Simplex method, Newton/Quasi-Newton method, interior point methods, conjugate gradient method, line search, and other local heuristics. Note that most of the common individual learning methods are deterministic.\n\nIn combinatorial optimization, on the other hand, individual learning methods commonly exist in the form of heuristics (which can be deterministic or stochastic) that are tailored to a specific problem of interest. Typical heuristic procedures and schemes include the k-gene exchange, edge exchange, first-improvement, and many others.\n\n==Applications==\nMemetic algorithms have been successfully applied to a multitude of real-world problems. Although many people employ techniques closely related to memetic algorithms, alternative names such as ''hybrid genetic algorithms'' are also employed. Furthermore, many people term their memetic techniques as ''genetic algorithms''.{{Citation needed|date=September 2014}}\n\nResearchers have used memetic algorithms to tackle many classical [[NP (complexity)|NP]] problems. To cite some of them: [[graph partition]]ing, [[knapsack problem|multidimensional knapsack]], [[travelling salesman problem]], [[quadratic assignment problem]], [[set cover problem]], [[graph coloring#Algorithms|minimal graph coloring]], [[independent set problem|max independent set problem]], [[bin packing problem]], and [[Generalized Assignment Problem|generalized assignment problem]].\n\nMore recent applications include (but are not limited to) training of [[artificial neural network]]s,<ref name=training_ANN>{{cite conference\n|author1=Ichimura, T. |author2=Kuriyama, Y. |title=Learning of neural networks with parallel hybrid GA using a royal road function|conference=IEEE International Joint Conference on Neural Networks|volume=2|pages=1131–1136|year=1998|location=New York, NY\n|url=https://ieeexplore.ieee.org/abstract/document/685931/}}</ref> [[pattern recognition]],<ref name=pattern_recognition>{{cite journal|author1=Aguilar, J. |author2=Colmenares, A. |year=1998|title=Resolution of pattern recognition problems using a hybrid genetic/random neural network learning algorithm|journal=Pattern Analysis and Applications|volume=1|issue=1|pages=52–61|doi=10.1007/BF01238026}}</ref> robotic [[motion planning]],<ref name=motion_planning>{{cite book|author1=Ridao, M. |author2=Riquelme, J. |author3=Camacho, E. |author4=Toro, M. | year=1998|title=An evolutionary and local search algorithm for planning two manipulators motion|volume=1416| pages=105–114|publisher=Springer-Verlag|doi=10.1007/3-540-64574-8_396|series=Lecture Notes in Computer Science|isbn=978-3-540-64574-0|citeseerx=10.1.1.324.2668 }}</ref> [[charged particle beam|beam]] orientation,<ref name=beam_orientation>{{cite journal|author1=Haas, O. |author2=Burnham, K. |author3=Mills, J. |year=1998|title=Optimization of beam orientation in radiotherapy using planar geometry|journal=Physics in Medicine and Biology|volume=43|issue=8|pages=2179–2193|doi=10.1088/0031-9155/43/8/013|pmid=9725597}}</ref> [[circuit design]],<ref name=circuit_design>{{cite journal|author1=Harris, S. |author2=Ifeachor, E. |year=1998|title=Automatic design of frequency sampling filters by hybrid genetic algorithm techniques|journal=IEEE Transactions on Signal Processing|volume=46|issue=12|pages=3304–3314|doi=10.1109/78.735305}}</ref> electric service restoration,<ref name=service_restoration>{{cite journal|author1=Augugliaro, A. |author2=Dusonchet, L. |author3=Riva-Sanseverino, E. |year=1998|title=Service restoration in compensated distribution networks using a hybrid genetic algorithm|journal=Electric Power Systems Research|volume=46|issue=1|pages=59–66|doi=10.1016/S0378-7796(98)00025-X}}</ref> medical [[expert system]]s,<ref name=medical_expert_system>{{cite journal|author1=Wehrens, R. |author2=Lucasius, C. |author3=Buydens, L. |author4=Kateman, G. |year=1993|title=HIPS, A hybrid self-adapting expert system for nuclear magnetic resonance spectrum interpretation using genetic algorithms|journal=Analytica Chimica Acta|volume=277|issue=2|pages=313–324|doi=10.1016/0003-2670(93)80444-P}}</ref> [[single machine scheduling]],<ref name=single_machine_sched>{{cite conference|author1=França, P. |author2=Mendes, A. |author3=Moscato, P. |title=Memetic algorithms to minimize tardiness on a single machine with sequence-dependent setup times|conference=Proceedings of the 5th International Conference of the Decision Sciences Institute|pages=1708–1710|year=1999|location=Athens, Greece|url=https://pdfs.semanticscholar.org/c213/5d68ceb0fd8e924aabe97cac1858ff6a2ce4.pdf}}</ref> automatic timetabling (notably, the timetable for the [[NHL]]),<ref name=nhl_timetabling>{{cite journal|author=Costa, D.|title=An evolutionary tabu search algorithm and the NHL scheduling problem|journal=Infor 33|year=1995|pages=161–178}}</ref> [[Schedule (workplace)|manpower scheduling]],<ref name=nurse_rostering>{{cite conference|author=Aickelin, U.|title=Nurse rostering with genetic algorithms|conference=Proceedings of young operational research conference 1998|year=1998|location=Guildford, UK}}</ref> [[nurse rostering problem|nurse rostering optimisation]],<ref name=nurse_rostering_function_opt>{{cite book| author = Ozcan, E.|year=2007|title=Memes, Self-generation and Nurse Rostering|volume=3867|pages=85–104|publisher=Springer-Verlag|doi=10.1007/978-3-540-77345-0_6|series=Lecture Notes in Computer Science|isbn=978-3-540-77344-3}}</ref> [[processor allocation]],<ref name=proc_alloc>{{cite journal|author1=Ozcan, E. |author2=Onbasioglu, E. |year=2007|title=Memetic Algorithms for Parallel Code Optimization|journal=International Journal of Parallel Programming|volume=35|issue=1|pages=33–61|doi=10.1007/s10766-006-0026-x}}</ref> maintenance scheduling (for example, of an electric distribution network),<ref name=planned_maintenance>{{cite journal|author1=Burke, E. |author2=Smith, A. |year=1999|title=A memetic algorithm to schedule planned maintenance for the national grid| journal=Journal of Experimental Algorithmics|issue=4|pages=1–13|doi=10.1145/347792.347801|volume=4}}</ref> multidimensional knapsack problem,<ref name=mkp_ma>{{cite journal|author1=Ozcan, E. |author2=Basaran, C. |year=2009|title=A Case Study of Memetic Algorithms for Constraint Optimization|journal=Soft Computing: A Fusion of Foundations, Methodologies and Applications|volume=13|issue=8–9|pages=871–882|doi=10.1007/s00500-008-0354-4|citeseerx=10.1.1.368.7327 }}</ref> [[VLSI]] design,<ref name=vlsi_design>{{cite journal|author=Areibi, S., Yang, Z.|year=2004|title=Effective memetic algorithms for VLSI design automation = genetic algorithms + local search + multi-level clustering|journal=Evolutionary Computation|volume=12|issue=3|pages=327–353|doi=10.1162/1063656041774947|pmid=15355604}}</ref> [[cluster analysis|clustering]] of [[expression profiling|gene expression profiles]],<ref name=clustering_gene_expression >{{cite book|author1=Merz, P. |author2=Zell, A. |title = Parallel Problem Solving from Nature — PPSN VII|volume=2439 |year=2002|publisher=[[Springer Science+Business Media|Springer]]|doi=10.1007/3-540-45712-7_78|pages=811–820| chapter=Clustering Gene Expression Profiles with Memetic Algorithms|series=Lecture Notes in Computer Science |isbn=978-3-540-44139-7 }}</ref> feature/gene selection,<ref name=gene_selection1>{{cite journal|author=Zexuan Zhu, Y. S. Ong and M. Dash|title=Markov Blanket-Embedded Genetic Algorithm for Gene Selection|year=2007|journal=Pattern Recognition|volume=49|issue=11|pages=3236–3248|doi=10.1016/j.patcog.2007.02.007}}</ref><ref name=gene_selection2>{{cite journal|author=Zexuan Zhu, Y. S. Ong and M. Dash|title=Wrapper-Filter Feature Selection Algorithm Using A Memetic Framework|year=2007|journal=IEEE Transactions on Systems, Man and Cybernetics - Part B|volume=37|issue=1|pages=70–76|doi=10.1109/TSMCB.2006.883267}}</ref> and multi-class, multi-objective [[feature selection]].<ref name=feature_selection>{{cite journal|author=Zexuan Zhu, Y. S. Ong and M. Zurada|title=Simultaneous Identification of Full Class Relevant and Partial Class Relevant Genes|year=2008|journal=IEEE/ACM Transactions on Computational Biology and Bioinformatics}}</ref><ref name=feature_selection2>{{cite book|author1=G. Karkavitsas  |author2=G. Tsihrintzis |lastauthoramp=yes |title=Automatic Music Genre Classification Using Hybrid Genetic Algorithms|year=2011|journal=Intelligent Interactive Multimedia Systems and Services|volume=11|pages=323–335|publisher=Springer|doi=10.1007/978-3-642-22158-3_32|series=Smart Innovation, Systems and Technologies |isbn=978-3-642-22157-6 }}</ref>\n\n==Recent Activities in Memetic Algorithms==\n*IEEE Workshop on Memetic Algorithms (WOMA 2009). Program Chairs: Jim Smith, University of the West of England, U.K.; Yew-Soon Ong, Nanyang Technological University, Singapore; Gustafson Steven, University of Nottingham; U.K.; Meng Hiot Lim, Nanyang Technological University, Singapore; Natalio Krasnogor, University of Nottingham, U.K.\n*[https://www.springer.com/journal/12293 Memetic Computing Journal], first issue appeared in January 2009.\n*[http://www.wcci2008.org/ 2008 IEEE World Congress on Computational Intelligence (WCCI 2008)], Hong Kong, [http://users.jyu.fi/~neferran/MA2008/MA2008.htm Special Session on Memetic Algorithms].\n*[http://www.ntu.edu.sg/home/asysong/SC/Special-Issue-MA.htm Special Issue on 'Emerging Trends in Soft Computing - Memetic Algorithm'], Soft Computing Journal, Completed & In Press, 2008.\n*[http://www.ntu.edu.sg/home/asysong/ETTC/ETTC%20Task%20Force%20-%20Memetic%20Computing.htm IEEE Computational Intelligence Society Emergent Technologies Task Force on Memetic Computing]\n* [https://web.archive.org/web/20100306001555/http://cec2007.nus.edu.sg/ IEEE Congress on Evolutionary Computation (CEC 2007)], Singapore, [https://web.archive.org/web/20080216234225/http://ntu-cg.ntu.edu.sg/ysong/MA-SS/MA.htm Special Session on Memetic Algorithms].\n* [http://www.esi-topics.com/erf/2007/august07-Ong_Keane.html 'Memetic Computing'] by Thomson Scientific's Essential Science Indicators as an Emerging Front Research Area.\n* [http://ieeexplore.ieee.org/Xplore/login.jsp?url=/iel5/3477/4067063/04067075.pdf?tp=&isnumber=&arnumber=4067075 Special Issue on Memetic Algorithms], IEEE Transactions on Systems, Man and Cybernetics - Part B, Vol. 37, No. 1, February 2007.\n* [http://www.springeronline.com/sgw/cda/frontpage/0,11855,5-40356-72-34233226-0,00.html Recent Advances in Memetic Algorithms], Series: Studies in Fuzziness and Soft Computing, Vol. 166, {{ISBN|978-3-540-22904-9}}, 2005.\n* [http://www.mitpressjournals.org/doi/abs/10.1162/1063656041775009?prevSearch=allfield%3A%28memetic+algorithm%29 Special Issue on Memetic Algorithms], Evolutionary Computation Fall 2004, Vol. 12, No. 3: v-vi.\n\n==References==\n{{reflist|30em}}\n\n[[Category:Evolutionary algorithms]]"
    },
    {
      "title": "Meta-optimization",
      "url": "https://en.wikipedia.org/wiki/Meta-optimization",
      "text": "[[Image:Meta-Optimization Concept.JPG|thumb|Meta-optimization concept.]]\nIn [[numerical optimization]], '''meta-optimization''' is the use of one optimization method to tune another optimization method. Meta-optimization is reported to have been used as early as in the late 1970s by Mercer and Sampson<ref name=\"mercer78adaptive\"/> for finding optimal parameter settings of a [[genetic algorithm]].\n\nMeta-optimization and related concepts are also known in the literature as meta-evolution, super-optimization, automated parameter calibration, [[hyper-heuristics]], etc.\n\n== Motivation ==\n[[Image:DE Meta-Fitness Landscape (12 benchmark problems).JPG|thumb|Performance landscape for [[differential evolution]].]]\nOptimization methods such as [[genetic algorithm]] and [[differential evolution]] have several parameters that govern their behaviour and efficiency in optimizing a given problem and these parameters must be chosen by the practitioner to achieve satisfactory results. Selecting the behavioural parameters by hand is a laborious task that is susceptible to human misconceptions of what makes the optimizer perform well.\n\nThe behavioural parameters of an optimizer can be varied and the optimization performance plotted as a landscape. This is computationally feasible for optimizers with few behavioural parameters and optimization problems that are fast to compute, but when the number of behavioural parameters increases the time usage for computing such a performance landscape increases exponentially. This is the [[curse of dimensionality]] for the search-space consisting of an optimizer's behavioural parameters. An efficient method is therefore needed to search the space of behavioural parameters.\n\n== Methods ==\n[[Image:DE Meta-Optimization Progress (12 benchmark problems).JPG|thumb|Meta-optimization of [[differential evolution]].]]\nA simple way of finding good behavioural parameters for an optimizer is to employ another overlaying optimizer, called the [[meta]]-optimizer. There are different ways of doing this depending on whether the behavioural parameters to be tuned are [[real number|real-valued]] or [[discrete mathematics|discrete-valued]], and depending on what performance measure is being used, etc.\n\nMeta-optimizing the parameters of a [[genetic algorithm]] was done by Grefenstette <ref name=grefenstette86optimization/> and Keane,<ref name=keane95genetic/> amongst others, and experiments with meta-optimizing both the parameters and the [[genetic operators]] were reported by Bäck.<ref name=back94parallel/> Meta-optimization of the COMPLEX-RF algorithm was done by Krus and Andersson,<ref name=Krus03optimizing/> and,<ref name=Krus13optimizing/> where performance index of optimization based on information theory was introduced and further developed. Meta-optimization of [[particle swarm optimization]] was done by Meissner et al.,<ref name=meissner06optimized/> Pedersen and Chipperfield,<ref name=pedersen08simplifying/> and Mason et al.<ref name=mason2017meta/> Pedersen and Chipperfield applied meta-optimization to [[differential evolution]].<ref name=pedersen08thesis/> Birattari et al.<ref name=birattari02racing/><ref name=birattari04thesis/> meta-optimized [[ant colony optimization]]. [[Statistical models]] have also been used to reveal more about the relationship between choices of behavioural parameters and optimization performance, see for example Francois and Lavergne,<ref name=francois01design/> and Nannen and Eiben.<ref name=nannen06method/> A comparison of various meta-optimization techniques was done by Smit and Eiben.<ref name=smit09comparing/>\n\n== See also ==\n* [[Automated machine learning]] (AutoML)\n* [[Hyper-heuristics]]\n\n== Implementations ==\n<!-- Alphabetical order -->\n\n* [https://github.com/Hvass-Labs/MetaOps MetaOps] for [[Python (programming language)|Python]].\n* [http://homes.esat.kuleuven.be/~claesenm/optunity/ Optunity] for [[Python (programming language)|Python]] with wrappers for [[MATLAB]] and [[R (programming language)|R]].\n* [http://www.hvass-labs.org/projects/swarmops/ SwarmOps] for [[Python (programming language)|Python]], [[C Sharp (programming language)|C#]], [[C (programming language)|C]] and [[Java (programming language)|Java]].\n\n== References ==\n{{reflist|refs=\n<ref name=mercer78adaptive>\n{{cite journal\n|last=Mercer\n|first=R.E.\n|author2=Sampson, J.R.\n|title=Adaptive search using a reproductive metaplan\n|journal=Kybernetes\n|year=1978\n|volume=7\n|pages=215&ndash;228\n|doi=10.1108/eb005486\n|issue=3\n}}\n</ref>\n\n<ref name=grefenstette86optimization>\n{{cite journal\n|doi=10.1109/TSMC.1986.289288\n|last=Grefenstette\n|first=J.J.\n|title=Optimization of control parameters for genetic algorithms\n|journal=IEEE Transactions on Systems, Man, and Cybernetics\n|year=1986\n|volume=16\n|pages=122&ndash;128\n|issue=1\n}}\n</ref>\n\n<ref name=keane95genetic>\n{{cite journal\n|doi=10.1016/0954-1810(95)95751-Q\n|last=Keane\n|first=A.J.\n|title=Genetic algorithm optimization in multi-peak problems: studies in convergence and robustness\n|journal=Artificial Intelligence in Engineering\n|year=1995\n|volume=9\n|pages=75&ndash;83\n|issue=2\n}}\n</ref>\n\n<ref name=meissner06optimized>\n{{cite journal\n|last1=Meissner\n|first1=M.\n|last2=Schmuker\n|first2=M.\n|last3=Schneider\n|first3=G.\n|title=Optimized Particle Swarm Optimization (OPSO) and its application to artificial neural network training\n|journal=BMC Bioinformatics\n|year=2006\n|volume=7\n|doi=10.1186/1471-2105-7-125\n|pmid=16529661\n|pmc=1464136\n|issue=1\n|pages=125\n}}\n</ref>\n\n<ref name=pedersen08simplifying>\n{{cite journal   \n|doi=10.1016/j.asoc.2009.08.029   \n|last=Pedersen   \n|first=M.E.H.   \n|author2=Chipperfield, A.J.   \n|url=http://www.hvass-labs.org/people/magnus/publications/pedersen08simplifying.pdf   \n|title=Simplifying particle swarm optimization   \n|journal=Applied Soft Computing   \n|year=2010   \n|volume=10   \n|pages=618&ndash;628   \n|issue=2   \n|citeseerx=10.1.1.149.8300   \n}}   \n</ref>\n\n<ref name=mason2017meta>\n{{cite journal\n|last=Mason\n|first=Karl\n|author2=Duggan, Jim\n|author3=Howley, Enda\n|title=A Meta Optimisation Analysis of Particle Swarm Optimisation Velocity Update Equations for Watershed Management Learning\n|journal=Applied Soft Computing\n|year=2018\n|volume=62\n|pages=148–161\n|doi =10.1016/j.asoc.2017.10.018\n\n}}\n</ref>\n\n<ref name=pedersen08thesis>\n{{cite book   \n|type=PhD thesis   \n|title=Tuning & Simplifying Heuristical Optimization   \n|url=http://www.hvass-labs.org/people/magnus/thesis/pedersen08thesis.pdf   \n|last=Pedersen   \n|first=M.E.H.   \n|year=2010   \n|publisher=University of Southampton, School of Engineering Sciences, Computational Engineering and Design Group   \n}}\n</ref>\n\n<ref name=francois01design>\n{{cite journal   \n|last1=Francois\n|first1=O.\n|last2=Lavergne\n|first2=C.\n|title=Design of evolutionary algorithms - a statistical perspective\n|journal=IEEE Transactions on Evolutionary Computation\n|year=2001\n|volume=5\n|pages=129&ndash;148\n|issue=2   \n|doi=10.1109/4235.918434\n}}   \n</ref>\n\n<ref name=birattari02racing>\n{{cite conference\n|last1=Birattari\n|first1=M.\n|last2=Stützle\n|first2=T.\n|last3=Paquete\n|first3=L.\n|last4=Varrentrapp\n|first4=K.\n|title=A racing algorithm for configuring metaheuristics\n|booktitle=Proceedings of the Genetic and Evolutionary Computation Conference (GECCO)\n|year=2002\n|pages=11–18\n}}\n</ref>\n\n<ref name=Krus03optimizing>\n{{cite conference\n|last1=Krus \n|first1=PK.\n|last2=Andersson (Ölvander)\n|first2=J.\n|title=Optimizing optimization for design optimization\n|booktitle=Proceedings of DETC’03 2003 ASME Design Engineering Technical Conferences and Computers and Information in Engineering Conference Chicago, Illinois, USA\n|year=2003\n}}\n</ref>\n\n<ref name=Krus13optimizing>\n{{cite journal\n|last1=Krus \n|first1=PK.\n|last2=Ölvander(Andersson)\n|first2=J.\n|url=http://liu.diva-portal.org/smash/get/diva2:572570/FULLTEXT01.pdf\n|title=Performance index and meta-optimization of a direct search optimization method\n|journal=Engineering Optimization\n|year=2013\n|volume=45\n|pages=1167&ndash;1185\n|issue=10   \n|doi=10.1080/0305215X.2012.725052\n\n}}\n</ref>\n\n<ref name=birattari04thesis>\n{{cite book\n|type=PhD thesis\n|title=The Problem of Tuning Metaheuristics as Seen from a Machine Learning Perspective\n|url=http://iridia.ulb.ac.be/~mbiro/paperi/BirattariPhD.pdf\n|last=Birattari\n|first=M.\n|year=2004\n|publisher=Université Libre de Bruxelles\n}}\n</ref>\n\n<ref name=smit09comparing>\n{{cite conference\n|last1=Smit\n|first1=S.K.\n|last2=Eiben\n|first2=A.E.\n|title=Comparing parameter tuning methods for evolutionary algorithms\n|booktitle=Proceedings of the IEEE Congress on Evolutionary Computation (CEC)\n|year=2009\n|pages=399–406\n}}\n</ref>\n\n<ref name=back94parallel>\n{{cite conference\n|last1=Bäck\n|first1=T.\n|title=Parallel optimization of evolutionary algorithms\n|booktitle=Proceedings of the International Conference on Evolutionary Computation\n|year=1994\n|pages=418–427\n}}\n</ref>\n\n<ref name=nannen06method>\n{{cite conference\n|last1=Nannen\n|first1=V.\n|last2=Eiben\n|first2=A.E.\n|title=A method for parameter calibration and relevance estimation in evolutionary algorithms\n|booktitle=Proceedings of the 8th Annual Conference on Genetic and Evolutionary Computation (GECCO)\n|year=2006\n|pages=183–190\n}}\n</ref>\n}}\n\n[[Category:Evolutionary algorithms]]\n[[Category:Heuristics]]"
    },
    {
      "title": "Minimum Population Search",
      "url": "https://en.wikipedia.org/wiki/Minimum_Population_Search",
      "text": "{{more footnotes|date=December 2017}}\nIn [[evolutionary computation]], '''Minimum Population Search''' ('''MPS''') is a computational method that [[Mathematical optimization|optimizes]] a problem by iteratively trying to improve a set of candidate solutions with regard to a given measure of quality. It solves a problem by evolving a small population of candidate solutions by means of relatively simple arithmetical operations.\n\nMPS is a [[metaheuristic]] as it makes few or no assumptions about the problem being optimized and can search very large spaces of candidate solutions. For problems where finding the precise global optimum is less important than finding an acceptable local optimum in a fixed amount of time, using a metaheuristic such as MPS may be preferable to alternatives such as [[brute-force search]] or [[gradient descent]].\n\nMPS is used for multidimensional real-valued functions but does not use the [[gradient]] of the problem being optimized, which means MPS does not require for the optimization problem to be differentiable as is required by classic optimization methods such as [[gradient descent]] and [[Quasi-Newton method|quasi-newton methods]]. MPS can therefore also be used on [[Optimization problem|optimization problems]] that are not even [[Continuous function|continuous]], are noisy, change over time, etc.\n\n== Background ==\nIn a similar way to [[Differential evolution]], MPS uses difference vectors between the members of the population in order to generate new solutions. It attempts to provide an efficient use of function evaluations by maintaining a small population size. If the population size is smaller than the dimensionality of the search space, then the solutions generated through difference vectors will be constrained to the <math>n - 1</math> dimensional hyperplane. A smaller population size will lead to a more restricted subspace. With a population size equal to the dimensionality of the problem <math>(n = d)</math>, the “line/hyperplane points” in MPS will be generated within a <math>d - 1</math> dimensional hyperplane. Taking a step orthogonal to this hyperplane will allow the search process to cover all the dimensions of the search space. <ref name=mpslessons/>\n\nPopulation size is a fundamental parameter in the performance of population-based heuristics. Larger populations promote exploration, but they also allow fewer generations, and this can reduce the chance of convergence. Searching with a small population can increase the chances of convergence and the efficient use of function evaluations, but it can also induce the risk of premature convergence. If the risk of premature convergence can be avoided, then a population-based heuristic could benefit from the efficiency and faster convergence rate of a smaller population. To avoid premature convergence, it is important to have a diversified population. By including techniques for explicitly increasing diversity and exploration, it is possible to have smaller populations with less risk of premature convergence <ref name=mpslessons/>. \n\n=== Thresheld Convergence ===\n\n''Thresheld Convergence'' (TC) is a diversification technique which attempts to separate the processes of exploration and exploitation. TC uses a “threshold” function to establish a minimum search step, and managing this step makes it possible to influence the transition from exploration to exploitation, convergence is thus “held” back until the last stages of the search process <ref name=tcreview/>. The goal of a controlled transition is to avoid an early concentration of the population around a few search regions and avoid the loss of diversity which can cause premature convergence. Thresheld Convergence has been successfully applied to several population-based metaheuristics such as [[Particle swarm optimization|Particle Swarm Optimization]], [[Differential evolution]] <ref name=detc/>, [[Evolution strategies]] <ref name=estc/>, [[Simulated annealing]] <ref name=satc/> and [[Estimation of distribution algorithm|Estimation of Distribution Algorithms]].\n\nThe ideal case for Thresheld Convergence is to have one sample solution from each attraction basin, and for each sample solution to have the same relative fitness with respect to its local optimum. Enforcing a minimum step aims to achieve this ideal case. In MPS Thresheld Convergence is specifically used to preserve diversity and avoid premature convergence by establishing a minimum search step. By disallowing new solutions which are too close to members of the current population, TC forces a strong exploration during the early stages of the search while preserving the diversity of the (small) population.\n\n== Algorithm ==\nA basic variant of the MPS algorithm works by having a population of size equal to the dimension of the problem. New solutions are generated by exploring the hyperplane defined by the current solutions (by means of difference vectors) and performing an additional orthogonal step in order to avoid getting caught in this hyperplane. The step sizes are controlled by the Thresheld Convergence technique, which gradually reduces step sizes as the search process advances.\n\nAn outline for the algorithm is given below:\n* Generate the first initial population. Allowing these solutions to lie near the bounds of the search space generally gives good results: <math>s_k =(rs_1 * bound_1/2, rs_2 * bound_2/2, ..., rs_n*bound_n/2)</math> where <math>s_k</math> is the <math>k</math>-th population member, <math>rs_i</math> are random numbers which can be -1 or 1, and the <math>bound_i</math> are the lower and upper bounds on each dimension.\n* While a stop condition is not reached:\n* Update threshold convergence values (<math>min\\_step</math> and <math>max\\_step</math>)\n* Calculate the centroid of the current population (<math>x_c</math>)\n* For each member of the population (<math>x_i</math>), generate a new offspring as follows: \n** Uniformly generate a scaling factor (<math>F_i</math>) between <math>-max\\_step</math> and <math>max\\_step</math>  \n** Generate a vector (<math>x_o</math>) orthogonal to the difference vector between <math>x_i</math> and <math>x_c</math> \n** Calculate a scaling factor for the orthogonal vector: \n*** <math>min\\_orth = sqrt(max(min\\_step^2 -F_i^2,0))</math>\n*** <math>max\\_orth = sqrt(max(max\\_step^2 -F_i^2,0))</math>\n*** <math>orth\\_step = uniform(min\\_orth, max\\_orth)</math>\n** Generate the new solution by adding the difference and the orthogonal vectors to the original solution\n*** <math>new\\_solution = x_i + F_i * (x_i - x_c) * orth\\_step * x_o</math>\n* Pick the best members between the old population and the new one by discarding the least fit members.\n* Return the single best solution or the best population found as the final result.\n\n== References ==\n{{Reflist|refs=\n\n<ref name=mpslessons>\n{{cite conference\n|last1=Bolufé-Röhler\n|first1=Antonio\n|last2=Chen\n|first2=Stephen\n|title=Minimum Population Search - Lessons from building a heuristic technique with two population members.\n|booktitle=Congress on Evolutionary Computation (CEC'2013)\n|year=2013\n|pages=2061-2068\n}}\n</ref>\n\n<ref name=tcreview>\n{{cite conference\n|last1=Chen\n|first1=Stephen\n|last2=Montgomery\n|first2=James\n|last3=Bolufé-Röhler\n|first3=Antonio\n|last4=Gonzalez-Fernandez\n|first4=Yasser\n|title=A Review of Thresheld Convergence\n|journal=GECONTEC: Revista Internacional de Gestión del Conocimiento y la Tecnología\n|year=2015\n}}\n</ref>\n\n<ref name=detc>\n{{cite conference\n|last1=Bolufé-Röhler\n|first1=Antonio\n|last2=Estévez-Velarde\n|first2=Suilán\n|last3=Piad-Morffis\n|first3=Alejandro\n|last4=Chen\n|first4=Stephen\n|last5=Montgomery\n|first5=James\n|title=Differential Evolution with Thresheld Convergence\n|booktitle=Congress on Evolutionary Computation (CEC'2013)\n|year=2013\n|pages=40-47\n}}\n</ref>\n\n<ref name=estc>\n{{cite conference\n|last1=Piad-Morffis\n|first1=Alejandro\n|last2=Estévez-Velarde\n|first2=Suilán\n|last3=Bolufé-Röhler\n|first3=Antonio\n|last4=Montgomery\n|first4=James\n|last5=Chen\n|first5=Stephen\n|title=Evolution strategies with thresheld convergence\n|booktitle=Congress on Evolutionary Computation (CEC'2015)\n|year=2015\n|pages=2097-2104\n}}\n</ref>\n\n<ref name=satc>\n{{cite conference\n|last1=Chen\n|first1=S.\n|last2=Xudiera\n|first2=C.\n|last3=Montgomery\n|first3=J.\n|title=Simulated annealing with thresheld convergence\n|booktitle=IEEE Congress on Evolutionary Computation (CEC)\n|year=2012\n|pages=1-7\n}}\n</ref>\n\n}}\n\n== External links ==\n* [https://github.com/alxrcs/MPS Reference implementations]\n\n{{Major subfields of optimization}}\n{{Optimization algorithms|heuristic}}\n\n{{DEFAULTSORT:Minimum Population Search}}\n\n[[Category:Metaheuristics]]\n[[Category:Evolutionary algorithms]]"
    },
    {
      "title": "MOEA Framework",
      "url": "https://en.wikipedia.org/wiki/MOEA_Framework",
      "text": "{{Infobox software\n| name                   = MOEA Framework\n| logo                   = [[File:MOEA Framework Logo.png|frameless|upright=1.25|MOEA Framework Logo]]\n| developer              =\n| released               = {{start date|2011|11|21}}\n| status                 = Active\n| latest release version = 2.7\n| latest release date    = {{release date|2015|12|11}}\n| frequently_updated     = yes\n| programming language   = [[Java (programming language)|Java]]\n| platform               = [[Cross-platform]]\n| genre                  = [[Evolutionary computation]]\n| license                = [[GNU Lesser General Public License]]\n| website                = {{URL|1=http://www.moeaframework.org/}}\n}}\nThe '''MOEA Framework''' is an [[open-source software|open-source]] [[evolutionary computation]] library for [[Java (programming language)|Java]] that specializes in [[multi-objective optimization]].  It supports a variety of multiobjective evolutionary algorithms (MOEAs), including [[genetic algorithms]], [[genetic programming]], [[grammatical evolution]], [[differential evolution]], and [[particle swarm optimization]].  As a result, it has been used to conduct numerous comparative studies to assess the efficiency, reliability, and controllability of state-of-the-art MOEAs.\n\n== Features ==\nThe MOEA Framework is an extensible framework for rapidly designing, developing, executing, and statistically testing multiobjective evolutionary algorithms (MOEAs).  It features 25 different state-of-the-art MOEAs and over 80 analytical test problems.  It supports NSGA-II,<ref>{{cite journal|last=Deb|first=K.|title=A Fast Elitist Multi-Objective Genetic Algorithm: NSGA-II.|journal=IEEE Transactions on Evolutionary Computation|volume=6|pages=182–197|year=2000|display-authors=etal}}</ref> its recently introduced successor NSGA-III<ref>{{cite journal|last=Deb|first=K.|author2=Jain, H.|title=An Evolutionary Many-Objective Optimization Algorithm Using Reference-Point-Based Nondominated Sorting Approach, Part I: Solving Problems With Box Constraints.|journal=IEEE Transactions on Evolutionary Computation|volume=18|issue=4|pages=577–601|year=2014}}</ref> epsilon-MOEA,<ref>{{cite journal|last=Deb|title=A Fast Multi-Objective Evolutionary Algorithm for Finding Well-Spread Pareto-Optimal Solutions.|journal=KanGAL Report No 2003002|year=2003|display-authors=etal}}</ref> GDE3.,<ref>{{cite journal|last=Kukkonen|author2=Lampinen|year=2005|title=GDE3: The Third Evolution Step of Generalized Differential Evolution.|journal=KanGAL Report Number 2005013}}</ref> and MOEA/D.<ref>{{cite journal|last=Li|first=H.|author2=Zhang, Q. |title=Multiobjective Optimization problems with Complicated Pareto Sets, MOEA/D and NSGA-II.|journal=IEEE Transactions on Evolutionary Computation|volume=13|issue=2|pages=284–302|year=2009}}</ref>  natively.  In addition, it integrates with the JMetal,<ref>{{cite web|url=http://jmetal.sourceforge.org|title=JMetal Website}}{{dead link|date=January 2018 |bot=InternetArchiveBot |fix-attempted=yes }}</ref> Platform and Programming Language Independent Interface for Search Algorithms (PISA),<ref>{{cite web|url=http://www.tik.ee.ethz.ch/pisa/|title=PISA Website}}</ref> and Borg MOEA<ref>{{cite web|url=http://www.borgmoea.org|title=Borg MOEA Website}}</ref> libraries to provide access to all popular MOEAs.  Additionally, using Java's [[service provider interface]] (SPI), new MOEAs and problems can be introduced into the framework.  This supports the use of the MOEA Framework in scientific studies, allowing new MOEAs to be tested against a suite of state-of-the-art algorithms across a large collection of test problems.\n\nNew problems are defined in the MOEA Framework using one or more decision variables of a varying type.  This includes common representations such as binary strings, real-valued numbers, and permutations.  It additionally supports evolving grammars in [[Backus–Naur form]] and programs using an internal [[Turing completeness|Turing complete]] programming language.  Once the problem is defined, the user can optimize the problem using any of supported MOEAs.\n\n== Sensitivity analysis ==\nThe MOEA Framework is the only known framework for evolutionary computation that provides support for [[sensitivity analysis]].  Sensitivity analysis in this context studies how an MOEA's parameters impact its output (i.e., the quality of the results).  Alternatively, sensitivity analysis measures the robustness of an MOEA to changes in its parameters.  An MOEA whose behavior is sensitive to its parameterization will not be easily controllable; conversely, an MOEA that is insensitive to its parameters is controllable.<ref>{{cite journal|last=Hadka|first=D.|author2=Reed, P.|title=Diagnostic Assessment of Search Controls and Failure Modes in Many-Objective Evolutionary Optimization|journal=Evolutionary Computation|year=2012|volume=20|issue=3|pages=423–452|url=http://www.mitpressjournals.org/doi/abs/10.1162/EVCO_a_00053|doi=10.1162/evco_a_00053}}</ref>  By measuring the sensitivities of each MOEA, the MOEA Framework can identify the controlling parameters for each MOEA and provide guidance for fine-tuning the parameters.  Additionally, MOEAs that are consistently insensitive to parameter changes across an array of problem domains are regarded highly due to their robust ability to solve optimization problems.\n\n== See also ==\n{{Portal|Free and open-source software}}\n* [[Java Evolutionary Computation Toolkit|ECJ]], a toolkit to implement evolutionary algorithms\n* [[Paradiseo]], a metaheuristics framework\n\n== References ==\n{{Reflist}}\n\n==External links==\n* [http://www.moeaframework.org Official site]\n\n[[Category:Evolutionary algorithms]]\n[[Category:Free software programmed in Java (programming language)]]\n[[Category:Free mathematics software]]\n[[Category:Free application software]]\n[[Category:Free science software]]\n[[Category:Free computer libraries]]"
    },
    {
      "title": "Multi expression programming",
      "url": "https://en.wikipedia.org/wiki/Multi_expression_programming",
      "text": "{{Evolutionary algorithms}}\n{{notability|date=August 2016}}\n{{third-party|date=August 2016}}\n{{technical|date=August 2015}}\n{{coi|date=August 2016}}\n'''Multi Expression Programming''' (MEP) is a [[genetic programming]] variant encoding multiple solutions in the same chromosome. MEP representation is not specific (multiple representations have been tested). In the simplest variant, MEP chromosomes are linear strings of instructions. This representation was inspired by [[Three-address code]]. MEP strength consists in the ability to encode multiple solutions, of a problem, in the same chromosome. In this way one can explore larger zones of the search space. For most of the problems this advantage comes with no running-time penalty compared with [[genetic programming]] variants encoding a single solution in a chromosome.<ref name=mep2002>Oltean M.; Dumitrescu D.: \"[http://www.mep.cs.ubbcluj.ro/oltean_mep.pdf Multi Expression Programming]\", Technical report, Univ. Babes-Bolyai, Cluj-Napoca, 2002</ref><ref name=mep_ecal03>Oltean M.; Grosan C.: \"[http://www.mep.cs.ubbcluj.ro/oltean_ecal2003.pdf  Evolving Evolutionary Algorithms using Multi Expression Programming]\", The 7th European Conference on Artificial Life, September 14–17, 2003, Dortmund, Edited by W. Banzhaf (et al), LNAI 2801, pp. 651-658, Springer-Verlag, Berlin, 2003</ref><ref name=mep_eh04>Oltean M.; Grosan C.: \"[http://www.mep.cs.ubbcluj.ro/oltean_eh04.pdf Evolving Digital Circuits using Multi Expression Programming]\", NASA/DoD Conference on Evolvable Hardware, 24–26 June, Seattle, Edited by R. Zebulum (et. al), pages 87-90, IEEE Press, NJ, 2004</ref>\n\n==Example of MEP program==\n\nHere is a simple MEP program:\n<pre>\n1: a\n2: b\n3: + 1, 2\n4: c\n5: d\n6: + 4, 5\n7: * 3, 5\n</pre>\n\nOn each line we can have a terminal or a function. In the case of functions we also need pointers to its arguments.\n\nWhen we decode the chromosome we obtain multiple expressions:\n\n<pre>\nE1 = a,\nE2 = b,\nE4 = c,\nE5 = d,\nE3 = a + b.\nE6 = c + d.\nE7 = (a + b) * d.\n</pre>\nWhich expression will represent the chromosome? In MEP each expression is evaluated and the best of them will represent the chromosome. For most of the problems, this evaluation has the same complexity as in the case of encoding a single solution in each chromosome.\n\n== Software ==\n\n=== MEPX ===\nMEPX is a cross platform (Windows, Mac OSX, and Linux Ubuntu) free software for automatic generation of computer programs. It can be used for data analysis, particularly for solving regression and classification problems.\n\n[[File:Mepx_screenshot.png|Multi Expression Programming X screenshot]]\n\n=== libmep ===\n\n[https://www.github.com/mepx Libmep] is a free and open source library implementing Multi Expression Programming technique. It is written in C++.\n\n=== hmep ===\n\n[http://hackage.haskell.org/package/hmep hmep] is a new open source library implementing Multi Expression Programming technique in Haskell programming language.\n\n== See also ==\n* [[Genetic programming]]\n* [[Gene expression programming]]\n* [[Grammatical evolution]]\n* [[Linear genetic programming]]\n\n== Notes ==\n<references/>\n\n==External links==\n*[http://www.mepx.org Multi Expression Programming website]\n*[https://www.github.com/mepx Multi Expression Programming source code]\n\n[[Category:Genetic programming]]\n[[Category:Genetic algorithms]]\n[[Category:Evolutionary algorithms]]\n\n{{compu-AI-stub}}"
    },
    {
      "title": "Neuroevolution",
      "url": "https://en.wikipedia.org/wiki/Neuroevolution",
      "text": "{{Distinguish|Evolution of nervous systems|Neural development|Neural Darwinism}}\n\n'''Neuroevolution''', or '''neuro-evolution''', is a form of  [[artificial intelligence]] that uses [[evolutionary algorithm]]s to generate [[artificial neural network]]s (ANN), parameters, topology and rules.<ref>{{Cite news|url=https://www.oreilly.com/ideas/neuroevolution-a-different-kind-of-deep-learning|title=Neuroevolution: A different kind of deep learning|last=Stanley|first=Kenneth O.|date=2017-07-13|work=O'Reilly Media|access-date=2017-09-04|language=en}}</ref> It is most commonly applied in [[artificial life]], [[general game playing]]<ref>{{cite journal|last1=Risi |first1=Sebastian|last2=Togelius|first2=Julian |title= Neuroevolution in Games: State of the Art and Open Challenges |journal=IEEE Transactions on Computational Intelligence and AI in Games |volume=9|pages=25–41|year= 2017 |arxiv=1410.7326|doi=10.1109/TCIAIG.2015.2494596}}</ref> and [[evolutionary robotics]]. The main benefit is that neuroevolution can be applied more widely than [[supervised learning|supervised learning algorithms]], which require a syllabus of correct input-output pairs. In contrast, neuroevolution requires only a measure of a network's performance at a task. For example, the outcome of a game (i.e. whether one player won or lost) can be easily measured without providing labeled examples of desired strategies. Neuroevolution is commonly used as part of the [[reinforcement learning]] paradigm, and it can be contrasted with conventional deep learning techniques that use [[gradient descent]] on a neural network with a fixed topology.\n\n==Features==\n\nMany neuroevolution algorithms have been defined. One common distinction is between algorithms that evolve only the strength of the connection weights for a fixed network topology (sometimes called conventional neuroevolution), as opposed to those that evolve both the topology of the network and its weights (called TWEANNs, for Topology and Weight Evolving Artificial Neural Network algorithms).\n\nA separate distinction can be made between methods that evolve the structure of ANNs in parallel to its parameters (those applying standard evolutionary algorithms) and those that develop them separately (through [[memetic algorithm]]s).<ref>{{citation|last1= Togelius |first1=Julian |last2=Schaul |first2=Tom |last3=Schmidhuber |first3=Jurgen |last4= Gomez |first4=Faustino |contribution=Countering poisonous inputs with memetic neuroevolution |title=Parallel Problem Solving from Nature |year=2008 |contribution-url=https://www.academia.edu/download/30945872/poison.pdf}}</ref>\n\n==Comparison with gradient descent==\nMost neural networks use gradient descent rather than neuroevolution. However, around 2017 researchers at [[Uber]] stated they had found that simple structural neuroevolution algorithms were competitive with sophisticated modern industry-standard gradient-descent deep learning algorithms, in part because neuroevolution was found to be less likely to get stuck in local minimal. In ''[[Science (magazine)|Science]]'',\njournalist Matthew Hutson speculated that part of the reason neuroevolution is succeeding where it had failed before is due to the increased computational power available in the 2010s.<ref>{{cite news|title=Artificial intelligence can ‘evolve’ to solve problems|url=http://www.sciencemag.org/news/2018/01/artificial-intelligence-can-evolve-solve-problems|accessdate=7 February 2018|work=Science {{!}} AAAS|date=10 January 2018|language=en}}</ref>\n\n==Direct and indirect encoding==\n\nEvolutionary algorithms operate on a population of [[genotype]]s (also referred to as [[genome]]s). In neuroevolution, a genotype is mapped to a neural network [[phenotype]] that is evaluated on some task to derive its [[fitness function|fitness]].\n\nIn ''direct'' encoding schemes the genotype directly maps to the phenotype. That is, every neuron and connection in the neural network is specified directly and explicitly in the genotype. In contrast, in ''indirect'' encoding schemes the genotype specifies indirectly how that network should be generated.<ref name=\"cgegecco\">{{Citation |last1=Kassahun|first1=Yohannes|last2=Sommer|first2=Gerald|last3=Edgington|first3=Mark|last4=Metzen|first4=Jan Hendrik|last5=Kirchner|first5=Frank|date=2007|contribution=Common genetic encoding for both direct and indirect encodings of networks|title=Genetic and Evolutionary Computation Conference |publisher=ACM Press|pages=1029–1036|citeseerx=10.1.1.159.705}}</ref>\n\nIndirect encodings are often used to achieve several aims:<ref name=\"cgegecco\"/><ref name=hyperneat>{{citation|last=Gauci |first= Stanley |contribution=Generating Large-Scale Neural Networks Through Discovering Geometric Regularities |title=Genetic and Evolutionary Computation Conference|year=2007 |location=New York, NY |publisher=ACM |contribution-url=http://eplex.cs.ucf.edu/papers/gauci_gecco07.pdf}}</ref><ref name=gruau94>{{Cite book|title=Neural Network Synthesis Using Cellular Encoding And The Genetic Algorithm.|last=Gruau|first=Frédéric|last2=I|first2=L'universite Claude Bernard-lyon|last3=Doctorat|first3=Of A. Diplome De|last4=Demongeot|first4=M. Jacques|last5=Cosnard|first5=Examinators M. Michel|last6=Mazoyer|first6=M. Jacques|last7=Peretto|first7=M. Pierre|last8=Whitley|first8=M. Darell|date=1994|citeseerx = 10.1.1.29.5939}}</ref><ref>{{Cite journal|last=Clune|first=J.|last2=Stanley|first2=Kenneth O.|last3=Pennock|first3=R. T.|last4=Ofria|first4=C.|date=June 2011|title=On the Performance of Indirect Encoding Across the Continuum of Regularity|journal=IEEE Transactions on Evolutionary Computation|volume=15|issue=3|pages=346–367|doi=10.1109/TEVC.2010.2104157|issn=1089-778X|citeseerx=10.1.1.375.6731}}</ref><ref name=eshyperalife>{{cite journal|last=Risi |first1=Sebastian|last2=Stanley |first2=Kenneth O.|title=An Enhanced Hypercube-Based Encoding for Evolving the Placement, Density and Connectivity of Neurons |journal=Artificial Life |volume=18|issue=4|pages=331–363|year= 2012 |url=http://eplex.cs.ucf.edu/papers/risi_alife12.pdf |doi=10.1162/ARTL_a_00071|pmid=22938563}}</ref>\n* modularity and other regularities;\n* compression of phenotype to a smaller genotype, providing a smaller search space;\n* mapping the search space (genome) to the problem domain.\n\n===Taxonomy of embryogenic systems for indirect encoding===\n\nTraditionally indirect encodings that employ artificial [[embryology|embryogeny]] (also known as [[artificial development]]) have been categorised along the lines of a ''grammatical approach'' versus a ''cell chemistry approach''.<ref name=taxae >{{cite journal|last1=Stanley |first1=Kenneth O. |last2=Miikkulainen |first2=Risto |title=A Taxonomy for Artificial Embryogeny |journal=The MIT Press Journals |year=2003 |url=http://nn.cs.utexas.edu/downloads/papers/stanley.alife03.pdf}}</ref> The former evolves sets of rules in the form of grammatical rewrite systems. The latter attempts to mimic how physical structures emerge in biology through gene expression. Indirect encoding systems often use aspects of both approaches.\n\nStanley and Miikkulainen<ref name=taxae /> propose a taxonomy for embryogenic systems that is intended to reflect their underlying properties. The taxonomy identifies five continuous dimensions, along which any embryogenic system can be placed:\n* Cell (neuron) fate''':''' the final characteristics and role of the cell in the mature phenotype. This dimension counts the number of methods used for determining the fate of a cell.\n* Targeting''':''' the method by which connections are directed from source cells to target cells. This ranges from specific targeting (source and target are explicitly identified) to relative targeting (e.g. based on locations of cells relative to each other).\n* Heterochrony''':''' the timing and ordering of events during embryogeny. Counts the number of mechanisms for changing the timing of events.\n* Canalization''':''' how tolerant the genome is to mutations (brittleness). Ranges from requiring precise genotypic instructions to a high tolerance of imprecise mutation.\n* Complexification''':''' the ability of the system (including evolutionary algorithm and genotype to phenotype mapping) to allow complexification of the genome (and hence phenotype) over time. Ranges from allowing only fixed-size genomes to allowing highly variable length genomes.\n\n==Examples==\n\nExamples of neuroevolution methods (those with direct encodings are necessarily non-embryogenic):\n{| class=\"wikitable\" border=\"1\"\n|-\n! Method\n! Encoding\n! Evolutionary algorithm\n! Aspects evolved\n|-\n| Neuro-genetic evolution by E. Ronald, 1994<ref>{{citation |contribution=Genetic Lander: An experiment in accurate neuro-genetic control |first1=Edmund |last1=Ronald |first2= March |last2=Schoenauer |title=PPSN III 1994 Parallel Programming Solving from Nature |pages=452–461 |citeseerx=10.1.1.56.3139 |year=1994 }}</ref>\n| Direct\n| [[Genetic algorithm]]\n| Network Weights\n|-\n| Cellular Encoding (CE) by F. Gruau, 1994<ref name=gruau94/>\n| Indirect, embryogenic (grammar tree using [[S-expressions]])\n| [[Genetic programming]]\n| Structure and parameters (simultaneous, complexification)\n|-\n| GNARL by Angeline et al., 1994<ref>{{cite journal|first1=Peter J. |last1=Angeline |first2=Gregory M. |last2=Saunders |first3=Jordan B. |last3=Pollack |title=An evolutionary algorithm that constructs recurrent neural networks |journal=IEEE Transactions on Neural Networks |volume=5 |issue=5|pages=54–65 |year=1994 |url=http://demo.cs.brandeis.edu/papers/ieeenn.pdf|doi=10.1109/72.265960 |pmid=18267779 |citeseerx=10.1.1.64.1853 }}</ref>\n| Direct\n| [[Evolutionary programming]]\n| Structure and parameters (simultaneous, complexification)\n|-\n| EPNet by Yao and Liu, 1997<ref>{{cite journal|first=Xin |last=Yao |first2= Yong |last2=Liu |title=A new evolutionary system for evolving artificial neural networks |journal=IEEE Transactions on Neural Networks |volume=8 |issue=3|pp=694–713 |date=May 1997 |url=http://www.cs.bham.ac.uk/~axk/evoNN2.pdf}}</ref>\n| Direct\n| [[Evolutionary programming]] (combined with [[backpropagation]] and [[simulated annealing]])\n| Structure and parameters (mixed, complexification and simplification)\n|-\n| [[NeuroEvolution of Augmenting Topologies]] (NEAT) by Stanley and Miikkulainen, 2002<ref name=autogenerated1>{{cite web|title=Real-Time Neuroevolution in the NERO Video Game|first1=Kenneth O. |last1=Stanley |first2=Bobby D. |last2=Bryant |first3=Risto |last3=Miikkulainen|url=http://nn.cs.utexas.edu/downloads/papers/stanley.ieeetec05.pdf |date=December 2005 }}</ref><ref>{{cite journal|title=Evolving Neural Networks through Augmenting Topologies |first=Kenneth O. |last=Stanley |first2=Risto |last2=Miikkulainen|url=http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf |journal=Evolutionary Computation |volume =10 |issue= 2 |pages=99–127 |year=2002|doi=10.1162/106365602320169811 |pmid=12180173 |citeseerx=10.1.1.638.3910 }}</ref>\n| Direct\n| [[Genetic algorithm]]. Tracks genes with historical markings to allow crossover between different topologies, protects innovation via speciation.\n| Structure and parameters\n|-\n| [[HyperNEAT|Hypercube-based NeuroEvolution of Augmenting Topologies]] (HyperNEAT) by Stanley, D'Ambrosio, Gauci, 2008<ref name=hyperneat />\n| Indirect, non-embryogenic (spatial patterns generated by a [[Compositional pattern-producing network]] (CPPN) within a [[hypercube]] are interpreted as connectivity patterns in a lower-dimensional space)\n| [[Genetic algorithm]]. The NEAT algorithm (above) is used to evolve the CPPN.\n| Parameters, structure fixed (functionally fully connected)\n|-\n| [[ES-HyperNEAT|Evolvable Substrate Hypercube-based NeuroEvolution of Augmenting Topologies]] (ES-HyperNEAT) by Risi, Stanley 2012<ref name=eshyperalife/>\n| Indirect, non-embryogenic (spatial patterns generated by a [[Compositional pattern-producing network]] (CPPN) within a [[hypercube]] are interpreted as connectivity patterns in a lower-dimensional space)\n| [[Genetic algorithm]]. The NEAT algorithm (above) is used to evolve the CPPN.\n| Parameters and network structure\n|-\n| [[Evolutionary Acquisition of Neural Topologies]] (EANT/EANT2) by Kassahun and Sommer, 2005<ref>{{citation|first1=Yohannes |last1=Kassahun |first2=Gerald |last2=Sommer|contribution=Efficient reinforcement learning through evolutionary acquisition of neural topologies|title=13th European Symposium on Artificial Neural Networks |pp= 259–266|location= Bruges, Belgium |date=April 2005 |contribution-url=http://www.ks.informatik.uni-kiel.de/~yk/ESANN2005EANT.pdf}}</ref> / Siebel and Sommer, 2007<ref>{{cite journal |first1=Nils T. |last1=Siebel |first2=Gerald |last2=Sommer |title=Evolutionary reinforcement learning of artificial neural networks |journal=International Journal of Hybrid Intelligent Systems |volume=4 |issue=3 |pp=171–183 |date=October 2007 |url=http://www.ks.informatik.uni-kiel.de/~vision/doc/Publications/nts/SiebelSommer-IJHIS2007.pdf |deadurl=yes |archiveurl=https://web.archive.org/web/20080905102111/http://www.ks.informatik.uni-kiel.de/~vision/doc/Publications/nts/SiebelSommer-IJHIS2007.pdf |archivedate=2008-09-05 |df= }}</ref>\n| Direct and indirect, potentially embryogenic (Common Genetic Encoding<ref name=cgegecco />)\n| [[Evolutionary programming]]/[[Evolution strategies]]\n| Structure and parameters (separately, complexification)\n|-\n| [[Interactively Constrained Neuro-Evolution]] (ICONE) by Rempis, 2012<ref>{{cite web|first=Christian W. |last=Rempis |title=Evolving Complex Neuro-Controllers with Interactively Constrained Neuro-Evolution |type=PhD thesis |publisher=Osnabrück University |date=October 2012 |url=http://repositorium.uni-osnabrueck.de/handle/urn:nbn:de:gbv:700-2012101710370}} urn:nbn:de:gbv:700-2012101710370</ref>\n| Direct, includes constraint masks to restrict the search to specific topology / parameter manifolds.\n| [[Evolutionary algorithm]]. Uses constraint masks to drastically reduce the search space through exploiting [[domain knowledge]].\n| Structure and parameters (separately, complexification, interactive)\n|-\n| [[Deus Ex Neural Network]] (DXNN) by Gene Sher, 2012<ref>{{cite book|first=Gene I. |last=Sher |title=Handbook of Neuroevolution Through Erlang |publisher=Springer Verlag |date=November 2012 |url=https://www.springer.com/computer/swe/book/978-1-4614-4462-6|isbn=9781461444626 }}</ref>\n| Direct/Indirect, includes constraints, local tuning, and allows for evolution to integrate new sensors and actuators.\n| [[Memetic algorithm]]. Evolves network structure and parameters on different time-scales.  \n| Structure and parameters (separately, complexification, interactive)\n|-\n| [[Spectrum-diverse Unified Neuroevolution Architecture]] (SUNA) by Danilo Vasconcellos Vargas, Junichi Murata<ref>{{cite journal|first1=Danilo Vasconcellos |last1=Vargas |first2=Junichi |last2=Murata |journal=IEEE Transactions on Neural Networks and Learning Systems |volume=28 |issue=8 |pages=1759–1773 |title=Spectrum-Diverse Neuroevolution With Unified Neural Models|doi=10.1109/TNNLS.2016.2551748 |pmid=28113564 |year=2017 }}</ref> ([https://github.com/zweifel/Physis-Shard Download code])\n| Direct, introduces the [[Unified Neural Representation]] (representation integrating most of the neural network features from the literature).\n| Genetic Algorithm with a diversity preserving mechanism called [[Spectrum-diversity]] that scales well with chromosome size, is problem independent and focus more on obtaining diversity of high level behaviours/approaches. To achieve this diversity the concept of [[chromosome Spectrum]] is introduced and used together with a [[Novelty Map Population]].   \n| Structure and parameters (mixed, complexification and simplification)\n|-\n| [[Modular Agent-Based Evolver]] (MABE) by Clifford Bohm, Arend Hintze, and others.<ref>{{cite journal|first1=Jeffrey |last1=Edlund |first2=Nicolas |last2=Chaumont |first3=Arend |last3=Hintze |first4=Christof |last4=Koch |first5=Giulio \n|last5=Tononi |first6=Christoph |last6=Adami |journal=PLOS Computational Biology |volume=7 |issue=10 |pages=e1002236 |title=Integrated Information Increases with Fitness in the Evolution of Animats|doi=10.1371/journal.pcbi.1002236 |pmid=22028639 |pmc=3197648 |year=2011 |arxiv=1103.1791 |bibcode=2011PLSCB...7E2236E }}</ref> ([https://github.com/Hintzelab/MABE Download code])\n| Direct or indirect encoding of [[Markov network]]s, Neural Networks, genetic programming, and other arbitrarily customizable controllers.\n| Provides evolutionary algorithms, genetic programming algorithms, and allows customized algorithms, along with specification of arbitrary constraints.   \n| Evolvable aspects include the neural model and allows for the evolution of morphology and sexual selection among others.\n|-\n|Covariance Matrix Adaptation with Hypervolume Sorted Adaptive Grid Algorithm (CMA-HAGA) by Shahin Rostami, and others.,<ref>{{Cite journal|last=Rostami|first=Shahin|last2=Neri|first2=Ferrante|date=2017-06-01|title=A fast hypervolume driven selection mechanism for many-objective optimisation problems|journal=Swarm and Evolutionary Computation|volume=34|issue=Supplement C|pages=50–67|doi=10.1016/j.swevo.2016.12.002}}</ref><ref>{{Cite web|url=http://ieeexplore.ieee.org/abstract/document/8058553|title=Multi-objective evolution of artificial neural networks in multi-class medical diagnosis problems with class imbalance - IEEE Conference Publication|website=ieeexplore.ieee.org|language=en-US|access-date=2017-11-28}}</ref>\n|Direct, includes an [[atavism]] feature which enables traits to disappear and re-appear at different generations.\n|Multi-Objective [[Evolution strategy|Evolution Strategy]] with Preference Articulation\n|Structure, weights, and biases.\n|}\n\n==See also==\n* [[Automated machine learning]] (AutoML) \n* [[Evolutionary computation]]\n* [[Covariance Matrix Adaptation with Hypervolume Sorted Adaptive Grid Algorithm]] (CMA-HAGA)\n* [[NeuroEvolution of Augmented Topologies]] (NEAT)\n* [[Noogenesis]]\n* [[HyperNEAT]] (A Generative version of NEAT)\n* [[ES-HyperNEAT]] (A Generative version of NEAT that determines parameters and network structure)\n* [[Evolutionary Acquisition of Neural Topologies]] (EANT/EANT2)\n* [[Spectrum-diverse Unified Neuroevolution Architecture]] (SUNA)\n\n==References==\n{{reflist|30em}}\n\n==External links==\n* {{Cite web|url=http://beacon-center.org/blog/2012/08/13/evolution-101-neuroevolution/|title=Evolution 101: Neuroevolution {{!}} BEACON|website=beacon-center.org|language=en-US|access-date=2018-01-14}}\n* {{Cite web|url=http://nn.cs.utexas.edu/keyword?neuroevolution|title=NNRG Areas - Neuroevolution|website=nn.cs.utexas.edu|publisher=University of Texas |access-date=2018-01-14}}</ref> (has downloadable papers on NEAT and applications)\n* {{Cite web|url=http://sharpneat.sourceforge.net/|title=SharpNEAT Neuroevolution Framework|website=sharpneat.sourceforge.net|language=en|access-date=2018-01-14}} mature [[Open Source]] neuroevolution project implemented in C#/.Net.\n* [http://ANNEvolve.sourceforge.net ANNEvolve is an Open Source AI Research Project] (Downloadable source code in C and Python with a tutorial & miscellaneous writings and illustrations\n* {{Cite web|url=http://www.siebel-research.de/evolutionary_learning/|title=Nils T Siebel - EANT2 - Evolutionary Reinforcement Learning of Neural Networks|website=www.siebel-research.de|access-date=2018-01-14}}</ref> Web page on evolutionary learning with EANT/EANT2] (information and articles on EANT/EANT2 with applications to robot learning)\n* [http://nerd.x-bot.org/ NERD Toolkit.] The Neurodynamics and Evolutionary Robotics Development Toolkit. A free, open source software collection for various experiments on neurocontrol and neuroevolution. Includes a scriptable simulator, several neuro-evolution algorithms (e.g. ICONE), cluster support, visual network design and analysis tools.\n* {{Cite web|url=https://github.com/CorticalComputer|title=CorticalComputer (Gene)|website=GitHub|access-date=2018-01-14}} Source code for the DXNN Neuroevolutionary system.\n* {{Cite web|url=http://eplex.cs.ucf.edu/ESHyperNEAT|title=ES-HyperNEAT Users Page|website=eplex.cs.ucf.edu|language=en|access-date=2018-01-14}}\n\n{{Neuroscience}}\n\n[[Category:Evolutionary algorithms]]\n[[Category:Artificial neural networks]]"
    },
    {
      "title": "Neuroevolution of augmenting topologies",
      "url": "https://en.wikipedia.org/wiki/Neuroevolution_of_augmenting_topologies",
      "text": "'''NeuroEvolution of Augmenting Topologies''' ('''NEAT''') is a [[genetic algorithm]] (GA) for the generation of evolving [[artificial neural network]]s (a [[neuroevolution]] technique) developed by Ken Stanley in 2002 while at [[The University of Texas at Austin]]. It alters both the weighting parameters and structures of networks, attempting to find a balance between the fitness of evolved solutions and their diversity. It is based on applying three key techniques: tracking genes with history markers to allow crossover among topologies, applying speciation (the evolution of species) to preserve innovations, and developing topologies incrementally from simple initial structures (\"complexifying\").\n\n== Performance ==\nOn simple control tasks, the NEAT algorithm often arrives at effective networks more quickly than other contemporary neuro-evolutionary techniques and [[reinforcement learning]] methods.<ref name=\"stanley2002\">Kenneth O. Stanley and Risto Miikkulainen (2002). \"Evolving Neural Networks Through Augmenting Topologies\". Evolutionary Computation 10 (2): 99-127</ref><ref name=\"taylor2006\">Matthew E. Taylor, Shimon Whiteson, and Peter Stone (2006). \"Comparing Evolutionary and Temporal Difference Methods in a Reinforcement Learning Domain\". GECCO 2006: Proceedings of the Genetic and Evolutionary Computation Conference.</ref>\n\n== Algorithm  ==\nTraditionally a neural network topology is chosen by a human experimenter, and effective connection weight values are learned through a training procedure. This yields a situation whereby a trial and error process may be necessary in order to determine an appropriate topology. NEAT is an example of a topology and weight evolving artificial neural network (TWEAN) which attempts to simultaneously learn weight values and an appropriate topology for a neural network.\n\nIn order to encode the network into a phenotype for the GA, NEAT uses a direct encoding scheme which means every connection and neuron is explicitly represented. This is in contrast to indirect encoding schemes which define rules that allow the network to be constructed without explicitly representing every connection and neuron allowing for more compact representation.\n\nThe NEAT approach begins with a [[perceptron]]-like feed-forward network of only input neurons and output neurons. As evolution progresses through discrete steps, the complexity of the network's topology may grow, either by inserting a new neuron into a connection path, or by creating a new connection between (formerly unconnected) neurons.\n\n=== Competing conventions ===\nThe competing conventions problem arises when there is more than one way of representing information in a phenotype. For example, if a genome contains neurons ''A'', ''B'' and ''C'' and is represented by [A B C], if this genome is crossed with an identical genome (in terms of functionality) but ordered [C B A] crossover will yield children that are missing information ([A B A] or [C B C]), in fact 1/3 of the information has been lost in this example. NEAT solves this problem by tracking the history of genes by the use of a global innovation number which increases as new genes are added. When adding a new gene the global innovation number is incremented and assigned to that gene. Thus the higher the number the more recently the gene was added. For a particular generation if an identical mutation occurs in more than one genome they are both given the same number, beyond that however the mutation number will remain unchanged indefinitely.\n\nThese innovation numbers allow NEAT to match up genes which can be crossed with each other.<ref name=\"stanley2002\" />\n\n== Implementation ==\nThe original implementation by Ken Stanley is published under the [[GNU General Public License|GPL]]. It integrates with [[GNU Guile|Guile]], a GNU [[Scheme (programming language)|scheme]] interpreter. This implementation of NEAT is considered the conventional basic starting point for implementations of the NEAT algorithm.\n\n== Extensions ==\n\n=== rtNEAT ===\nIn 2003 Stanley devised an extension to NEAT that allows evolution to occur in real time rather than through the iteration of generations as used by most genetic algorithms. The basic idea is to put the population under constant evaluation with a \"lifetime\" timer on each individual in the population. When a network's timer expires its current fitness measure is examined to see whether it falls near the bottom of the population, and if so it is discarded and replaced by a new network bred from two high-fitness parents. A timer is set for the new network and it is placed in the population to participate in the ongoing evaluations.\n\nThe first application of rtNEAT is a video game called Neuro-Evolving Robotic Operatives, or NERO. In the first phase of the game, individual players deploy robots in a 'sandbox' and train them to some desired tactical doctrine. Once a collection of robots has been trained, a second phase of play allows players to pit their robots in a battle against robots trained by some other player, to see how well their training regimens prepared their robots for battle.\n\n=== Phased pruning ===\nAn extension of Ken Stanley's NEAT, developed by Colin Green, adds periodic pruning of the network topologies of candidate solutions during the evolution process. This addition addressed concern that unbounded automated growth would generate unnecessary structure.\n\n=== HyperNEAT ===\n\n{{Main|HyperNEAT}}\n\n[[HyperNEAT]] is specialized to evolve large scale structures. It was originally based on the [[CPPN]] theory and is an active field of research.\n\n=== cgNEAT ===\n\nContent-Generating NEAT (cgNEAT) evolves custom video game content based on user preferences. The first video game to implement cgNEAT is [[Galactic Arms Race]], a space-shooter game in which unique particle system weapons are evolved based on player usage statistics.<ref name=\"hastings2009\">Erin J. Hastings, Ratan K. Guha, and Kenneth O. Stanley (2009). \"Automatic Content Generation in the Galactic Arms Race Video Game \". IEEE Transactions on Computational Intelligence and AI in Games, volume 4, number 1, pages 245-263, New York: IEEE Press, 2009.</ref> Each particle system weapon in the game is controlled by an evolved [[CPPN]], similarly to the evolution technique in the [[NEAT Particles]] interactive art program.\n\n=== odNEAT ===\nodNEAT is an online and decentralized version of NEAT designed for multi-robot systems.<ref>{{Cite journal|title = odNEAT: An Algorithm for Decentralised Online Evolution of Robotic Controllers|journal = Evolutionary Computation|date = 2015-09-15|pages = 421–449|volume = 23|issue = 3|doi = 10.1162/evco_a_00141|pmid = 25478664|first = Fernando|last = Silva|first2 = Paulo|last2 = Urbano|first3 = Luís|last3 = Correia|first4 = Anders Lyhne|last4 = Christensen}}</ref>  odNEAT is executed onboard robots themselves during task execution to continuously optimize the parameters and the topology of the artificial neural network-based controllers. In this way, robots executing odNEAT have the potential to adapt to changing conditions and learn new behaviors as they carry out their tasks. The online evolutionary process is implemented according to a physically distributed island model. Each robot optimizes an internal population of candidate solutions (intra-island variation), and two or more robots exchange candidate solutions when they meet (inter-island migration). In this way, each robot is potentially self-sufficient and the evolutionary process capitalizes on the exchange of controllers between multiple robots for faster synthesis of effective controllers.\n\n==See also==\n* [[Evolutionary acquisition of neural topologies]]\n\n==References==\n{{reflist}}\n\n==Bibliography==\n{{refbegin}}\n*{{cite journal |author1=Kenneth O. Stanley  |author2=Risto Miikkulainen  |lastauthoramp=yes | year=2002 |\ntitle=Evolving Neural Networks Through Augmenting Topologies | journal=Evolutionary Computation | volume = 10 | issue = 2 | pages = 99–127 | \ndoi=10.1162/106365602320169811 | pmid=12180173 | url=http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf|citeseerx=10.1.1.638.3910  }}\n\n*{{cite journal |author1=Kenneth O. Stanley  |author2=Risto Miikkulainen  |lastauthoramp=yes | year=2002 |\ntitle=Efficient Reinforcement Learning Through Evolving Neural Network Topologies | journal=Proceedings of the Genetic and Evolutionary Computation Conference (GECCO-2002) | url=http://nn.cs.utexas.edu/downloads/papers/stanley.gecco02_1.pdf}}\n\n*{{cite journal |author1=Kenneth O. Stanley |author2=Bobby D. Bryant |author3=Risto Miikkulainen  |last-author-amp=yes | year=2003 |\ntitle=Evolving Adaptive Neural Networks with and without Adaptive Synapses | journal=Proceedings of the 2003 IEEE Congress on Evolutionary Computation (CEC-2003) | url=http://nn.cs.utexas.edu/downloads/papers/stanley.cec03.pdf}}\n\n*{{cite journal | author=Colin Green | year=2004 | title=Phased Searching with NEAT: Alternating Between Complexification And Simplification | url=http://sharpneat.sourceforge.net/phasedsearch.html}}\n*{{cite journal |author1=Kenneth O. Stanley |author2=Ryan Cornelius |author3=Risto Miikkulainen |author4=Thomas D’Silva |author5=Aliza Gold  |last-author-amp=yes | year=2005 |\ntitle=Real-Time Learning in the NERO Video Game | journal=Proceedings of the Artificial Intelligence and Interactive Digital Entertainment Conference (AIIDE 2005) Demo Papers | url=http://www.cs.utexas.edu/users/nn/downloads/papers/stanley.aiide05demo.pdf}}\n\n*{{cite journal |author1=Matthew E. Taylor |author2=Shimon Whiteson |author3=Peter Stone  |last-author-amp=yes | year=2006 |\ntitle=Comparing Evolutionary and Temporal Difference Methods in a Reinforcement Learning Domain | journal=GECCO 2006: Proceedings of the Genetic and Evolutionary Computation Conference | url=http://www.cs.utexas.edu/~ai-lab/pubs/GECCO06-matt.pdf}}\n\n*{{cite journal  |author1=Shimon Whiteson  |author2=Daniel Whiteson  |lastauthoramp=yes  |year=2007  |title=Stochastic Optimization for Collision Selection in High Energy Physics  |journal=IAAI 2007: Proceedings of the Nineteenth Annual Innovative Applications of Artificial Intelligence Conference  |url=http://www.cs.utexas.edu/~shimon/pubs/whitesoniaai07.pdf  }}{{dead link|date=February 2018 |bot=InternetArchiveBot |fix-attempted=yes }}\n{{refend}}\n\n==Implementations==\n* Stanley's [http://nn.cs.utexas.edu/soft-view.php?SoftID=4 original] and [http://www.cs.utexas.edu/users/nn/keyword?rtneat rtNEAT] for [[C++]]\n* [https://cs.gmu.edu/~eclab/projects/ecj/ ECJ], [http://nn.cs.utexas.edu/soft-view.php?SoftID=5 JNEAT], [http://neat4j.sourceforge.net/ NEAT 4J], [http://anji.sourceforge.net/ ANJI] for [[Java (programming language)|Java]]\n* [http://sharpneat.sourceforge.net/ SharpNEAT] for [[C Sharp (programming language)|C#]]\n* [http://multineat.com/features.html MultiNEAT] for [[C++]] and [[Python (programming language)|Python]]\n* [https://github.com/CodeReclaimers/neat-python neat-python] for [[Python (programming language)|Python]]\n* Maintained fork of [https://pypi.python.org/pypi/neat-python/ neat-python] for [[Python (programming language)|Python]]\n* [[Encog]] for [[Java (programming language)|Java]] and [[C Sharp (programming language)|C#]]\n* [https://github.com/noio/peas peas] for [[Python (programming language)|Python]]\n* [http://rubyneat.com RubyNEAT] for [[Ruby (programming language)|Ruby]]\n* [https://github.com/OptimusLime/neatjs neatjs] for [[Javascript (programming language)|Javascript]]\n* Not exact implementation, but [https://github.com/wagenaartje/neataptic Neataptic] for [[Javascript (programming language)|Javascript]]\n*[https://github.com/cazala/synaptic Synaptic] for [[JavaScript|Javascript]]\n* [https://gitlab.com/onnoowl/Neat-Ex Neat-Ex] for [[Elixir (programming language)|Elixir]]\n\n==External links==\n*[http://www.cs.ucf.edu/~kstanley/neat.html NEAT Homepage]\n*[http://eplex.cs.ucf.edu \"Evolutionary Complexity Research Group at UCF\"] - Ken Stanley's current research group\n*[http://nerogame.org/ NERO: Neuro-Evolving Robotic Operatives] - an example application of rtNEAT\n*[http://gar.eecs.ucf.edu/ GAR: Galactic Arms Race] - an example application of cgNEAT\n*[http://picbreeder.org/ \"PicBreeder.org\"] - Online, collaborative art generated by CPPNs evolved with NEAT.\n*[http://EndlessForms.com/ \"EndlessForms.com\"] - A 3D version of Picbreeder, where you interactively evolve 3D objects that are encoded with CPPNs and evolved with NEAT.\n*[http://beacon-center.org/blog/2012/08/13/evolution-101-neuroevolution/ BEACON Blog: What is neuroevolution?]\n*[https://www.youtube.com/watch?v=qv6UVOQ0F44 MarI/O - Machine Learning for Video Games], a [[YouTube]] video demonstrating an implementation of NEAT learning to play [[Super Mario World]]\n*[http://gekkoquant.com/2016/03/13/evolving-neural-networks-through-augmenting-topologies-part-1-of-4/ \"GekkoQuant.com\"] - A visual tutorial series on NEAT, including solving the classic pole balancing problem using NEAT in R\n\n[[Category:Artificial neural networks]]\n[[Category:Evolutionary algorithms]]\n[[Category:Evolutionary computation]]\n[[Category:Genetic algorithms]]"
    }
  ]
}