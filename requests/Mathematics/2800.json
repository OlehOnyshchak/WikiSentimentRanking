{
  "pages": [
    {
      "title": "Wright Omega function",
      "url": "https://en.wikipedia.org/wiki/Wright_Omega_function",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Mathematical function}}\n[[Image:WrightOmega.png|thumb|right|250px|The Wright Omega function along part of the real axis]]\n\nIn [[mathematics]], the '''Wright omega function''' or '''Wright function''',<ref group=\"note\">Not to be confused with the [[Fox–Wright function]], also known as Wright function.</ref> denoted ω, is defined in terms of the [[Lambert W function]] as:\n\n: <math>\\omega(z) = W_{\\big \\lceil \\frac{\\mathrm{Im}(z) - \\pi}{2 \\pi} \\big \\rceil}(e^z).</math>\n\n==Uses==\nOne of the main applications of this function is in the resolution of the equation ''z''&nbsp;=&nbsp;ln(''z''), as the only solution is given by ''z''&nbsp;=&nbsp;''e''<sup>&minus;ω(''&pi;''&nbsp;''i'')</sup>.\n\n''y'' = ω(''z'') is the unique solution, when <math>z \\neq x \\pm i \\pi</math> for ''x''&nbsp;&le;&nbsp;&minus;1, of the equation ''y''&nbsp;+&nbsp;ln(''y'')&nbsp;=&nbsp;''z''. Except on those two rays, the Wright omega function is [[continuous function|continuous]], even [[analytic function|analytic]].\n\n==Properties==\nThe Wright omega function satisfies the relation <math>W_k(z) = \\omega(\\ln(z) + 2 \\pi i k)</math>.\n\nIt also satisfies the [[differential equation]]\n\n: <math> \\frac{d\\omega}{dz} = \\frac{\\omega}{1 + \\omega}</math>\n\nwherever ω is analytic (as can be seen by performing [[separation of variables]] and recovering the equation <math>\\ln(\\omega)+\\omega = z</math>), and as a consequence its [[integral]] can be expressed as:\n\n: <math>\n\\int w^n \\, dz = \n\\begin{cases} \n  \\frac{\\omega^{n+1} -1 }{n+1} + \\frac{\\omega^n}{n}  & \\mbox{if } n \\neq -1, \\\\\n  \\ln(\\omega) - \\frac{1}{\\omega} & \\mbox{if } n = -1.\n\\end{cases}\n</math>\n\nIts [[Taylor series]] around the point <math> a = \\omega_a + \\ln(\\omega_a) </math> takes the form :\n\n: <math>\\omega(z) = \\sum_{n=0}^{+\\infty} \\frac{q_n(\\omega_a)}{(1+\\omega_a)^{2n-1}}\\frac{(z-a)^n}{n!}</math>\n\nwhere\n\n: <math>q_n(w) = \\sum_{k=0}^{n-1} \\bigg \\langle \\! \\! \\bigg \\langle \n\\begin{matrix}\n  n+1 \\\\\n  k\n\\end{matrix} \n\\bigg \\rangle \\! \\! \\bigg \\rangle (-1)^k w^{k+1}</math>\n\nin which\n\n: <math>\\bigg \\langle \\! \\! \\bigg \\langle \n\\begin{matrix}\n  n \\\\\n  k\n\\end{matrix} \n\\bigg \\rangle \\! \\! \\bigg \\rangle</math>\n\nis a second-order [[Eulerian number]].\n\n==Values==\n\n:<math>\n\\begin{array}{lll}\n\\omega(0) &= W_0(1) &\\approx 0.56714 \\\\\n\\omega(1) &= 1 & \\\\\n\\omega(-1 \\pm i \\pi) &= -1 & \\\\\n\\omega(-\\frac{1}{3} + \\ln \\left ( \\frac{1}{3} \\right ) + i \\pi ) &= -\\frac{1}{3} & \\\\\n\\omega(-\\frac{1}{3} + \\ln \\left ( \\frac{1}{3} \\right ) - i \\pi ) &= W_{-1} \\left ( -\\frac{1}{3} e^{-\\frac{1}{3}} \\right ) &\\approx -2.237147028 \\\\\n\\end{array}\n</math>\n\n==Plots==\n<gallery caption=\"Plots of the Wright Omega function on the complex plane\">\nImage:WrightOmegaRe.png|''z'' = Re(ω(''x'' + ''i'' ''y''))\nImage:WrightOmegaIm.png|''z'' = Im(ω(''x'' + ''i'' ''y''))\nImage:WrightOmegaAbs.png|ω(''x'' + ''i'' ''y'')\n</gallery>\n\n==Notes==\n{{reflist|group=note}}\n\n==References==\n* [http://www.orcca.on.ca/TechReports/TechReports/2000/TR-00-12.pdf \"On the Wright ω function\", Robert Corless and David Jeffrey]\n\n[[Category:Special functions]]"
    },
    {
      "title": "Antilogarithm",
      "url": "https://en.wikipedia.org/wiki/Antilogarithm",
      "text": "#REDIRECT [[Logarithm#Antilogarithm]]\n\n{{R to section}}\n<!--\nNote that the same redirect exists for the following terms;\n\n[[Anti-logarithm]]<br />\n[[Anti-Logarithm]]<br />\n[[Antilog]]\n-->\n\n[[Category:Exponentials]]\n[[Category:Elementary special functions]]"
    },
    {
      "title": "Binary antilogarithm",
      "url": "https://en.wikipedia.org/wiki/Binary_antilogarithm",
      "text": "#redirect [[Logarithm#Antilogarithm]] {{R to related topic}}\n\n[[Category:Exponentials]]\n[[Category:Elementary special functions]]"
    },
    {
      "title": "Common antilogarithm",
      "url": "https://en.wikipedia.org/wiki/Common_antilogarithm",
      "text": "#redirect [[Logarithm#Antilogarithm]] {{R to related topic}}\n\n[[Category:Exponentials]]\n[[Category:Elementary special functions]]"
    },
    {
      "title": "Constant function",
      "url": "https://en.wikipedia.org/wiki/Constant_function",
      "text": "{{distinguish|function constant}}\n{{Functions}}\n[[Image:wiki constant function 175 200.png|270px|right|Constant function ''y''=4]]\n\nIn [[mathematics]], a '''constant function''' is a [[Function (mathematics)|function]] whose (output) value is the same for every input value.<ref>{{cite book|title=Encyclopedia of Mathematics|last1=Tanton|first1=James|year=2005|publisher=Facts on File, New York|isbn=0-8160-5124-0|page=94}}</ref><ref>{{cite web | url=http://web.cortland.edu/matresearch/OxfordDictionaryMathematics.pdf |title=Oxford Concise Dictionary of Mathematics, Constant Function | author=C.Clapham, J.Nicholson | publisher =Addison-Wesley | year =2009|page=175|accessdate=January 12, 2014}}</ref><ref>{{cite book|title=CRC Concise Encyclopedia of Mathematics|last1=Weisstein|first1=Eric|publisher=CRC Press, London|isbn=0-8493-9640-9|year=1999|page=313}}</ref> For example, the function <math>y(x) = 4</math> is a constant function because the value of &nbsp;<math>y(x)</math> &nbsp;is 4 regardless of the input value <math>x</math> (see image).\n\n==Basic properties==\nAs a real-valued function of a real-valued argument, a constant function has the general form &nbsp;<math>y(x)=c</math> &nbsp;or just &nbsp;<math>y=c</math> &nbsp;.\n\n:'''Example:''' The function &nbsp;<math>y(x)=2</math> &nbsp;or just &nbsp;<math>y=2</math> &nbsp;is the specific constant function where the output value is &nbsp;<math>c=2</math>. The [[domain of a function|domain of this function]] is the set of all real numbers ℝ. The [[codomain]] of this function is just {2}. The independent variable ''x'' does not appear on the right side of the function expression and so its value is \"vacuously substituted\". Namely ''y''(0)=2, ''y''(−2.7)=2, ''y''(π)=2,.... No matter what value of ''x'' is input, the output is \"2\".\n\n:'''Real-world example:''' A store where every item is sold for the price of 1 euro.\n\nThe graph of the constant function <math>y=c</math> is a '''horizontal line''' in the [[plane (geometry)|plane]] that passes through the point <math>(0,c)</math>.<ref>{{cite web|title=College Algebra|last1=Dawkins|first1=Paul|year=2007|publisher= Lamar University|url=http://tutorial.math.lamar.edu/Classes/Alg/Alg.aspx|page=224|accessdate=January 12, 2014}}</ref>\n\nIn the context of a [[polynomial]] in one variable ''x'', the '''non-zero constant function''' is a polynomial of degree 0 and its general form is <math>f(x) = c \\, ,\\,\\, c \\neq 0</math>&nbsp;. This function has no intersection point with the ''x''-axis, that is, it has no [[Zero of a function|root (zero)]]. On the other hand, the polynomial &nbsp;<math>f(x)=0</math>  &nbsp;is the '''identically zero function'''. It is the (trivial) constant function and every ''x'' is a root. Its graph is the ''x''-axis in the plane.<ref>{{cite book|title=Advanced Mathematical Concepts - Pre-calculus with Applications, Student Edition|last1=Carter|first1=John A.|last4=Marks|first4=Daniel|last2=Cuevas|first2=Gilbert J.|last3=Holliday|first3=Berchie|last5=McClure|first5=Melissa S. |publisher=Glencoe/McGraw-Hill School Pub Co|year=2005|isbn=978-0078682278|chapter=1|edition=1|page=22}}</ref>\n\nA constant function is an [[Even and odd functions|even function]], i.e. the graph of a constant function is symmetric with respect to the ''y''-axis.\n\nIn the context where it is defined, the [[derivative]] of a function is a measure of the rate of change of function values with respect to change in input values. Because a constant function does not change, its derivative is 0.<ref>{{cite web|url=http://tutorial.math.lamar.edu/Classes/CalcI/DerivativeProofs.aspx|title=Derivative Proofs|year=2007|last1=Dawkins|first1=Paul|publisher= Lamar University|accessdate=January 12, 2014}}</ref> This is often written: &nbsp;<math>(c)'=0</math>&nbsp;. The converse is also true. Namely, if ''y''&#39;(''x'')=0 for all real numbers ''x'', then ''y''(''x'') is a constant function.<ref>{{cite web|url=http://www.proofwiki.org/wiki/Zero_Derivative_implies_Constant_Function|title=Zero Derivative implies Constant Function|accessdate=January 12, 2014}}</ref>\n\n:'''Example:''' Given the constant function  &nbsp; <math>y(x)=-\\sqrt{2}</math> &nbsp;. The derivative of ''y'' is the identically zero function &nbsp; <math>y'(x)=(-\\sqrt{2})'=0</math> &nbsp;.\n\n==Other properties==\nFor functions between [[preorder|preordered sets]], constant functions are both [[order-preserving]] and [[order-reversing]]; conversely, if ''f'' is both order-preserving and order-reversing, and if the [[Domain of a function|domain]] of ''f'' is a [[lattice (order)|lattice]], then ''f'' must be constant.\n\n* Every constant function whose [[Domain of a function|domain]] and [[codomain]] are the same is [[idempotent]].\n* Every constant function between [[topological space]]s is [[continuous function (topology)|continuous]].\n* A constant function factors through the [[singleton (mathematics)|one-point set]], the [[terminal object]] in the [[category of sets]]. This observation is instrumental for [[F. William Lawvere]]'s axiomatization of set theory, the Elementary Theory of the Category of Sets (ETCS).<ref>{{cite arXiv|last1=Leinster|first1=Tom|title=An informal introduction to topos theory|date=27 Jun 2011|eprint=1012.5647|class=math.CT}}</ref> \n* Every set X is [[isomorphic]] to the set of constant functions into it.  For each element x and any set Y, there is a unique function <math>\\tilde{x}: Y \\rightarrow X</math> such that <math>\\tilde{x}(y) = x</math> for all <math>y \\in Y</math>. Conversely, if a function <math>f: Y \\rightarrow X</math> satisfies <math>f(y) = f(y')</math> for all <math>y, y' \\in Y</math>, <math>f</math> is by definition a constant function.\n** As a corollary, the one-point set is a [[generator (category theory)|generator]] in the category of sets. \n** Every set <math>X</math> is canonically isomorphic to the function set <math>X^1</math>, or [[hom set]] <math>\\operatorname{hom}(1,X)</math> in the category of sets, where 1 is the one-point set. Because of this, and the adjunction between cartesian products and hom in the category of sets (so there is a canonical isomorphism between functions of two variables and functions of one variable valued in functions of another (single) variable, <math>\\operatorname{hom}(X \\times Y,Z) \\cong \\operatorname{hom}(X(\\operatorname{hom}(Y,Z))</math>) the category of sets is a [[closed monoidal category]] with the [[cartesian product]] of sets as tensor product and the one-point set as tensor unit. In the isomorphisms <math>\\lambda :1\\times X \\cong X\\cong X\\times1:\\rho</math> [[natural transformation|natural in X]], the left and right unitors are the projections <math>p_1</math> and <math>p_2</math> the [[ordered pair]]s <math>(*,x)</math> and <math>(x,*)</math> respectively to the element <math>x</math>, where <math>*</math> is the unique [[point (mathematics)|point]] in the one-point set.\n\nA function on a [[connected set]] is [[locally constant]] if and only if it is constant.\n<!--Lfahlberg 01.2014: Perhaps needs information contained in: http://mathworld.wolfram.com/ConstantMap.html, http://www.proofwiki.org/wiki/Definition:Constant_Mapping, http://math.stackexchange.com/questions/133257/show-that-a-constant-mapping-between-metric-spaces-is-continuous  and programming http://www.w3schools.com/php/func_misc_constant.asp, http://www2.math.uu.se/research/telecom/software/stcounting.html -->\n\n==References==\n{{reflist}}\n*Herrlich, Horst and Strecker, George E., ''Category Theory'', Heldermann Verlag  (2007).\n\n==External links==\n{{commons category|Constant functions}}\n*{{MathWorld |title=Constant Function |id=ConstantFunction}}\n*{{planetmath reference|id=4727|title=Constant function}}\n\n{{polynomials}}\n[[Category:Elementary mathematics]]\n[[Category:Elementary special functions]]\n[[Category:Polynomial functions]]"
    },
    {
      "title": "Cube root",
      "url": "https://en.wikipedia.org/wiki/Cube_root",
      "text": "{{Use dmy dates|date=July 2013}}\n[[File:Cube-root function.svg|thumb|288x288px|Plot of ''y'' = {{radic|''x''|3}}. The plot is symmetric with respect to origin, as it is an [[odd function]]. At ''x'' = 0 this graph has a [[vertical tangent]].]]\n[[File:Cube and doubled cube.svg|thumb|A unit cube (side = 1) and a cube with twice the volume (side = {{radic|2|3}} = 1.2599… {{OEIS2C|A002580}}).]]\n\nIn [[mathematics]], a '''cube root''' of a number ''x'' is a number ''y'' such that ''y''<sup>3</sup>&nbsp;=&nbsp;''x''.  All [[real number]]s, except zero, have exactly one real cube root and a pair of [[complex conjugate]] cube roots, and all nonzero [[complex number]]s have three distinct complex cube roots.  For example, the real cube root of 8, denoted {{radic|8|3}}, is 2, because 2<sup>3</sup>&nbsp;=&nbsp;8, while the other cube roots of 8 are −1&nbsp;+&nbsp;{{sqrt|3}}''i'' and −1&nbsp;−&nbsp;{{sqrt|3}}''i''. The three cube roots of −27''i'' are  \n:<math>3i, \\quad \\frac{3\\sqrt{3}}{2}-\\frac{3}{2}i, \\quad  \\text{and} \\quad -\\frac{3\\sqrt{3}}{2}-\\frac{3}{2}i.  </math>\n\nThe cube root operation is not [[distributivity|distributive]] with [[addition]] or [[subtraction]].  \n\nIn some contexts, particularly when the number whose cube root is to be taken is a real number, one of the cube roots (in this particular case the real one) is referred to as the ''principal cube root'', denoted with the radical sign {{radic||3}}. The cube root operation is associative with [[exponentiation]] and distributive with [[multiplication]] and [[division (mathematics)|division]] if considering only real numbers, but not always if considering complex numbers: for example, the cube of any cube root of 8 is 8, but the three cube roots of 8{{sup|3}} are 8, −4&nbsp;+&nbsp;4''i''{{sqrt|3}}, and −4&nbsp;−&nbsp;4''i''{{sqrt|3}}.\n\n==Formal definition==\nThe cube roots of a number ''x'' are the numbers ''y'' which satisfy the equation\n\n:<math>y^3 = x.\\ </math>\n\n==Properties==\n\n===Real numbers===\nFor any real number ''x'', there is ''one'' real number ''y'' such that ''y''<sup>3</sup>&nbsp;=&nbsp;''x''. The [[cube (algebra)|cube function]] is increasing, so does not give the same result for two different inputs, plus it covers all real numbers. In other words, it is a bijection, or one-to-one. Then we can define an inverse function that is also one-to-one. For real numbers, we can define a unique cube root of all real numbers. If this definition is used, the cube root of a negative number is a negative number.\n\n[[Image:3rd roots of unity.svg|thumb|right|The three cube roots of 1]]\nIf ''x'' and ''y'' are allowed to be [[complex number|complex]], then there are three solutions (if ''x'' is non-zero) and so ''x'' has three cube roots. A real number has one real cube root and two further cube roots which form a [[complex conjugate]] pair. For instance, the cube roots of [[1]] are:\n\n:<math> 1, \\quad -\\frac{1}{2}+\\frac{\\sqrt{3}}{2}i, \\quad -\\frac{1}{2}-\\frac{\\sqrt{3}}{2}i. </math>\n\nThe last two of these roots lead to a relationship between all roots of any real or complex number. If a number is one cube root of a particular real or complex number, the other two cube roots can be found by multiplying that cube root by one or the other of the two complex cube roots of 1.\n\n===Complex numbers===\n[[Image:Complex cube root.jpg|right|thumb|350px|Plot of the complex cube root together with its two additional leaves. The first image shows the main branch, which is described in the text.]]\n[[Image:Riemann surface cube root.svg|right|thumb|200px|[[Riemann surface]] of the cube root. One can see how all three leaves fit together.]]\n\nFor complex numbers, the principal cube root is usually defined as the cube root that has the greatest [[real part]], or, equivalently, the cube root whose [[argument (complex analysis)|argument]] has the least [[absolute value]]. It is related to the principal value of the [[natural logarithm]] by the formula\n\n:<math>x^{\\frac13} = \\exp \\left( \\frac13 \\ln{x} \\right).</math>\n\nIf we write ''x'' as\n\n:<math>x = r \\exp(i \\theta)\\,</math>\n\nwhere ''r'' is a non-negative real number and ''θ'' lies in the range\n\n:<math>-\\pi < \\theta \\le \\pi</math>,\n\nthen the principal complex cube root is\n\n:<math>\\sqrt[3]{x} = \\sqrt[3]{r}\\exp \\left(\\frac {i\\theta}{3} \\right).</math>\n\nThis means that in [[polar coordinates]], we are taking the cube root of the radius and dividing the polar angle by three in order to define a cube root. With this definition, the principal cube root of a negative number is a complex number, and for instance {{radic|−8|3}} will not be −2, but rather {{nowrap|1 + ''i''{{sqrt|3}}}}.\n\nThis difficulty can also be solved by considering the cube root as a [[multivalued function]]: if we write the original complex number ''x'' in three equivalent forms, namely\n\n:<math>x = \\begin{cases} r \\exp (i \\theta ), \\\\[3px] r \\exp (i \\theta + 2i\\pi ),  \\\\[3px] r \\exp ( i \\theta - 2i\\pi ). \\end{cases} </math>\n\n{{visualisation_complex_number_roots.svg}}\nThe principal complex cube roots of these three forms are then respectively\n\n:<math>\\sqrt[3]{x} = \\begin{cases} \n\\sqrt[3]{r}\\exp \\left( \\frac{i\\theta}{3}\\right), \n\\\\ \\sqrt[3]{r}\\exp \\left(\\frac{i\\theta}{3} + \\frac{2i \\pi}{3}  \\right), \n\\\\ \\sqrt[3]{r}\\exp \\left(\\frac{i\\theta}{3} - \\frac{2i \\pi}{3} \\right). \\end{cases} </math>\n\nUnless {{nowrap|1=''x'' = 0}}, these three complex numbers are distinct, even though the three representations of ''x'' were equivalent. For example, {{radic|−8|3}} may then be calculated to be −2, {{nowrap|1 + ''i''{{sqrt|3}}}}, or {{nowrap|1 − ''i''{{sqrt|3}}}}.\n\nThis is related with the concept of [[monodromy]]: if one follows by [[continuous function|continuity]] the function ''cube root'' along a closed path around zero, after a turn the value of the cube root is multiplied (or divided) by <math>e^{2i\\pi/3}.</math>\n\n==Impossibility of compass-and-straightedge construction==\n\nCube roots arise in the problem of finding an angle whose measure is one third that of a given angle ([[angle trisection]]) and in the problem of finding the edge of a cube whose volume is twice that of a cube with a given edge ([[doubling the cube]]). In 1837 [[Pierre Wantzel]] proved that neither of these can be done with a [[compass-and-straightedge construction]].\n\n==Numerical methods==\n[[Newton's method]] is an [[iterative method]] that can be used to calculate the cube root.  For real [[floating-point]] numbers this method reduces to the following iterative algorithm to produce successively better approximations of the cube root of ''a'':\n\n:<math>x_{n+1} = \\frac{1}{3} \\left(\\frac{a}{x_n^2} + 2x_n\\right).</math>\n\nThe method is simply averaging three factors chosen such that\n:<math> x_n \\times x_n \\times \\frac{a}{x_n^2}=a </math>\nat each iteration.\n\n[[Halley's method]] improves upon this with an algorithm that converges more quickly with each step, albeit consuming more multiplication operations:\n\n:<math>x_{n+1} = x_n \\left(\\frac{x_n^3 + 2a}{2x_n^3 + a}\\right).</math>\n\nWith either method a poor initial approximation of ''x''{{sub|0}} can give very poor algorithm performance, and coming up with a good initial approximation is somewhat of a black art. Some implementations manipulate the exponent bits of the floating-point number; i.e. they arrive at an\ninitial approximation by dividing the exponent by 3.\n\n==Appearance in solutions of third and fourth degree equations==\n\n[[Cubic equation]]s, which are [[polynomial equation]]s of the third degree (meaning the highest power of the unknown is 3) can always be solved for their three solutions in terms of cube roots and square roots (although simpler expressions only in terms of square roots exist for all three solutions, if at least one of them is a [[rational number]]). If two of the solutions are complex numbers, then all three solution expressions involve the real cube root of a real number, while if all three solutions are real numbers then they may be expressed in terms of the [[Casus irreducibilis|complex cube root of a complex number]]. \n\n[[Quartic equation]]s can also be solved in terms of cube roots and square roots.\n\n==History==\n\nThe calculation of cube roots can be traced back to [[Babylonian mathematics|Babylonian mathematicians]] from as early as 1800 BCE.<ref name=\"cbgr\">{{cite book|last=Saggs|first=H. W. F.|title=Civilization Before Greece and Rome|url=https://books.google.com/books?id=R28oab-7jLcC&pg=PA227|year=1989|publisher=Yale University Press|isbn=978-0-300-05031-8|page=227}}</ref> In the fourth century BCE [[Plato]] posed the problem of [[doubling the cube#History|doubling the cube]], which required a [[compass-and-straightedge construction]] of the edge of a [[cube (geometry)|cube]] with twice the volume of a given cube; this required the construction, now known to be impossible, of  the length {{radic|2|3}}.\n\nA method for extracting cube roots appears in ''[[The Nine Chapters on the Mathematical Art]]'', a [[Chinese mathematics|Chinese mathematical]] text compiled around the 2nd century BCE and commented on by [[Liu Hui]] in the 3rd century CE.<ref name=\"oxf\">{{cite book|last=Crossley|first=John|last2=W.-C. Lun|first2=Anthony|title=The Nine Chapters on the Mathematical Art: Companion and Commentary|url=https://books.google.com/books?id=eiTJHRGTG6YC&pg=PA213|year=1999|publisher=Oxford University Press|isbn=978-0-19-853936-0|page=213}}</ref> The [[Greek mathematics|Greek mathematician]] [[Hero of Alexandria]] devised a method for calculating cube roots in the 1st century CE. His formula is again mentioned by Eutokios in a commentary on [[Archimedes]].<ref>{{cite journal|last=Smyly|first=J. Gilbart|title=Heron's Formula for Cube Root|journal=Hermathena|year=1920|volume=19|issue=42|pages=64–67|publisher=Trinity College Dublin|jstor=23037103}}</ref> In 499 CE [[Aryabhata]], a [[mathematician]]-[[astronomer]] from the classical age of [[Indian mathematics]] and [[Indian astronomy]], gave a method for finding the cube root of numbers having many digits in the ''[[Aryabhatiya]]'' (section 2.5).<ref>''[http://www.flipkart.com/aryabhatiya-mohan-apte-book-8174344802 Aryabhatiya] {{lang-mr|आर्यभटीय}}'', Mohan Apte, Pune, India, Rajhans Publications, 2009, p.62, {{ISBN|978-81-7434-480-9}}</ref>\n\n==See also==\n* [[Methods of computing square roots]]\n* [[List of polynomial topics]]\n* [[Nth root]]\n* [[Square root]]\n* [[Nested radical]]\n* [[Root of unity]]\n* [[Shifting nth-root algorithm]]\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://www.mathwarehouse.com/arithmetic/cube-root-calculator.php  Cube root calculator reduces any number to simplest radical form]\n*[http://people.freebsd.org/~lstewart/references/apple_tr_kt32_cuberoot.pdf Computing the Cube Root, K. Turkowski, Apple Technical Report #KT-32, 1998]. Includes C source code.\n*{{planetmath reference|id=748|title=Cube root}}\n*{{mathworld|urlname=CubeRoot|title=Cube Root}}\n\n{{DEFAULTSORT:Cube Root}}\n[[Category:Elementary special functions]]\n[[Category:Elementary algebra]]"
    },
    {
      "title": "Cyclometric function",
      "url": "https://en.wikipedia.org/wiki/Cyclometric_function",
      "text": "#REDIRECT [[Inverse trigonometric functions]]\n\n[[Category:Trigonometry]]\n[[Category:Inverse trigonometric functions]]\n[[Category:Elementary special functions]]"
    },
    {
      "title": "Decadic antilogarithm",
      "url": "https://en.wikipedia.org/wiki/Decadic_antilogarithm",
      "text": "#redirect [[Logarithm#Antilogarithm]] {{R to related topic}}\n\n[[Category:Exponentials]]\n[[Category:Elementary special functions]]"
    },
    {
      "title": "Decimal antilogarithm",
      "url": "https://en.wikipedia.org/wiki/Decimal_antilogarithm",
      "text": "#redirect [[Logarithm#Antilogarithm]] {{R to related topic}}\n\n[[Category:Exponentials]]\n[[Category:Elementary special functions]]"
    },
    {
      "title": "Double exponential function",
      "url": "https://en.wikipedia.org/wiki/Double_exponential_function",
      "text": "[[Image:Double Exponential Function.PNG|right|thumb|320px|A double exponential function (red curve) compared to a single exponential function (blue curve).]]\n\nA '''double exponential''' function is a [[Constant (mathematics)|constant]] raised to the power of an [[Exponentiation|exponential function]].  The general formula is <math>f(x) = a^{b^x}=a^{(b^x)}</math>, which grows much more quickly than an exponential function. For example, if ''a'' = ''b'' = 10:\n*''f''(0) = 10\n*''f''(1) = 10<sup>10</sup>\n*''f''(2) = 10<sup>100</sup> = [[googol]]\n*''f''(3) = 10<sup>1000</sup>\n*''f''(100) = 10<sup>10<sup>100</sup></sup> = [[googolplex]].\n\n[[Factorial]]s grow more quickly than exponential functions, but much more slowly than doubly exponential functions. However, [[tetration]] and the [[Ackermann function]] grow faster. See [[Big O notation]] for a comparison of the rate of growth of various functions.\n\nThe inverse of the double exponential function is the [[Logarithm#double logarithm|double logarithm]] ln(ln(''x'')).\n\n==Doubly exponential sequences==\n[[Alfred Aho|Aho]] and [[Neil Sloane|Sloane]] observed that in several important [[integer sequence]]s, each term is a constant plus the square of the previous term. They show that such sequences can be formed by rounding to the nearest integer the values of a doubly exponential function in which the middle exponent is two.<ref>{{citation|first1=A. V.|last1=Aho|authorlink1=Alfred Aho|first2=N. J. A.|last2=Sloane|authorlink2=N. J. A. Sloane|url=http://neilsloane.com/doc/doubly.html|title=Some doubly exponential sequences|journal=[[Fibonacci Quarterly]]|volume=11|year=1973|pages=429–437}}.</ref> Integer sequences with this squaring behavior include\n\n* The [[Fermat number]]s\n::<math>F(m) = 2^{2^m}+1</math>\n* The harmonic primes: The primes p, in which the sequence 1/2+1/3+1/5+1/7+....+1/p exceeds 0,1,2,3,....\n:The first few numbers, starting with 0, are 2,5,277,5195977,... {{OEIS|A016088}}\n* The [[Double Mersenne number]]s\n::<math>MM(p) = 2^{2^p-1}-1</math>\n* The elements of [[Sylvester's sequence]] {{OEIS|A000058}}\n::<math>s_n = \\left\\lfloor E^{2^{n+1}}+\\frac12 \\right\\rfloor</math>\n:where ''E'' ≈ 1.264084735305302 is [[Vardi's constant]] {{OEIS|A076393}}.\n* The number of [[Arity|''k''-ary]] [[Boolean function]]s:\n::<math>2^{2^k}</math>\n\nMore generally, if the ''n''th value of an integer sequence is proportional to a double exponential function of ''n'', Ionaşcu and Stănică call the sequence \"almost doubly-exponential\" and describe conditions under which it can be defined as the floor of a doubly exponential sequence plus a constant.<ref>{{citation\n | last1 = Ionaşcu | first1 = Eugen-Julien\n | last2 = Stănică | first2 = Pantelimon\n | issue = 1\n | journal = Acta Mathematica Universitatis Comenianae\n | pages = 75–87\n | title = Effective asymptotics for some nonlinear recurrences and almost doubly-exponential sequences\n | volume = LXXIII\n | url = http://faculty.nps.edu/pstanica/research/AMUC04.pdf\n | year = 2004}}.</ref> Additional sequences of this type include\n\n* The prime numbers 2, 11, 1361, ... {{OEIS|A051254}}\n::<math>a(n) = \\left\\lfloor A^{3^n}\\right\\rfloor</math>\n:where ''A'' ≈ 1.306377883863 is [[Mills' constant]].\n\n==Applications==\n===Algorithmic complexity===\nIn [[computational complexity theory]], some algorithms take doubly exponential time:\n* Each decision procedure for [[Presburger arithmetic]] provably requires at least doubly exponential time <ref>[[Michael J. Fischer|Fischer, M. J.]], and [[Michael O. Rabin]], 1974, \"[http://www.lcs.mit.edu/publications/pubs/ps/MIT-LCS-TM-043.ps \"Super-Exponential Complexity of Presburger Arithmetic.]\" ''Proceedings of the SIAM-AMS Symposium in Applied Mathematics Vol. 7'': 27–41</ref>\n* Computing a [[Gröbner basis]] over a field. In the worst case, a Gröbner basis may have a number of elements which is doubly exponential in the number of variables. On the other hand, the [[worst-case complexity]] of Gröbner basis algorithms is doubly exponential in the number of variables as well as in the entry size.<ref>Dubé, Thomas W. The structure of polynomial ideals and Gröbner bases. ''SIAM Journal on Computing'', 1990, vol. 19, no 4, p. 750-773.</ref>\n* Finding a complete set of associative-commutative unifiers <ref>{{citation\n | last1 = Kapur | first1 = Deepak\n | last2 = Narendran | first2 = Paliath\n | contribution = Double-exponential complexity of computing a complete set of AC-unifiers\n | doi = 10.1109/LICS.1992.185515\n | pages = 11–21\n | title = Proc. 7th IEEE Symp. Logic in Computer Science (LICS 1992)\n | url = http://citeseer.ist.psu.edu/337363.html\n | year = 1992\n | isbn = 0-8186-2735-2}}.</ref>\n* Satisfying [[computation tree logic|CTL]]<sup>+</sup> (which is, in fact, [[2-EXPTIME]]-complete) <ref>{{citation\n | last1 = Johannsen | first1 = Jan\n | last2 = Lange | first2 = Martin\n | contribution = CTL<sup>+</sup> is complete for double exponential time\n | doi = 10.1007/3-540-45061-0_60\n | editor1-last = Baeten | editor1-first = Jos C. M.\n | editor2-last = Lenstra | editor2-first = Jan Karel | editor2-link = Jan Karel Lenstra\n | editor3-last = Parrow | editor3-first = Joachim\n | editor4-last = Woeginger | editor4-first = Gerhard J. | editor4-link = Gerhard J. Woeginger \n | pages = 767–775\n | publisher = Springer-Verlag\n | series = Lecture Notes in Computer Science\n | title = Proceedings of the 30th International Colloquium on Automata, Languages and Programming (ICALP 2003)\n | url = http://www.tcs.informatik.uni-muenchen.de/~mlange/papers/icalp03.pdf\n | volume = 2719\n | year = 2003\n | isbn = 978-3-540-40493-4}}.</ref>\n* [[Quantifier elimination]] on [[real closed field]]s takes doubly exponential time (see [[Cylindrical algebraic decomposition]]).\n* Calculating the [[Complement (set theory)|complement]] of a [[regular expression]] <ref>{{cite conference\n | last1 = Gruber | first1 = Hermann\n | last2 = Holzer | first2 = Markus\n | title = Finite Automata, Digraph Connectivity, and Regular Expression Size\n | pages = 39–50\n | booktitle = Proceedings of the 35th International Colloquium on Automata, Languages and Programming (ICALP 2008)\n | url = http://www.hermann-gruber.com/data/icalp08.pdf\n | year = 2008\n | doi = 10.1007/978-3-540-70583-3_4\n | volume = 5126\n | ref = harv\n | postscript = <!--None-->\n}}</ref>\n\nIn some other problems in the design and analysis of algorithms, doubly exponential sequences are used within the design of an algorithm rather than in its analysis. An example is [[Chan's algorithm]] for computing [[convex hull]]s, which performs a sequence of computations using test values ''h''<sub>''i''</sub>&nbsp;=&nbsp;2<sup>2<sup>''i''</sup></sup> (estimates for the eventual output size), taking time O(''n''&nbsp;log&nbsp;''h''<sub>''i''</sub>) for each test value in the sequence. Because of the double exponential growth of these test values, the time for each computation in the sequence grows singly exponentially as a function of ''i'', and the total time is dominated by the time for the final step of the sequence. Thus, the overall time for the algorithm is O(''n''&nbsp;log&nbsp;''h'') where ''h'' is the actual output size.<ref>{{citation\n | last = Chan | first = T. M. | authorlink = Timothy M. Chan\n | doi = 10.1007/BF02712873\n | issue = 4\n | journal = [[Discrete and Computational Geometry]]\n | mr = 1414961\n | pages = 361–368\n | title = Optimal output-sensitive convex hull algorithms in two and three dimensions\n | volume = 16\n | year = 1996}}</ref>\n\n===Number theory===\nSome [[number theory|number theoretical]] bounds are double exponential. [[Odd perfect number]]s with ''n'' distinct prime factors are known to be at most\n:<math>2^{4^n}</math>\na result of Nielsen (2003).<ref>{{citation\n | last = Nielsen | first = Pace P.\n | journal = INTEGERS: the Electronic Journal of Combinatorial Number Theory\n | pages = A14\n | title = An upper bound for odd perfect numbers\n | url = http://www.integers-ejcnt.org/vol3.html\n | volume = 3\n | year = 2003}}.</ref> The maximal volume of a ''d''-lattice [[polytope]] with ''k'' ≥ 1 [[Integer points in convex polyhedra|interior lattice points]] is at most\n:<math>(8d)^d\\cdot15^{d\\cdot2^{2d+1}}</math>\na result of Pikhurko.<ref>{{citation|last=Pikhurko\n | first=Oleg\n | title=Lattice points in lattice polytopes\n | journal=Mathematika\n | volume=48\n | year=2001\n | pages=15–24\n | arxiv=math/0008028\n|bibcode = 2000math......8028P\n | doi=10.1112/s0025579300014339}}</ref>\n\nThe [[largest known prime number]] in the electronic era has grown roughly as a double exponential function of the year since [[J. C. P. Miller|Miller]] and [[David Wheeler (computer scientist)|Wheeler]] found a 79-digit prime on [[EDSAC]]1 in 1951.<ref>{{citation\n | last1 = Miller | first1 = J. C. P.\n | last2 = Wheeler | first2 = D. J.\n | doi = 10.1038/168838b0\n | journal = [[Nature (journal)|Nature]]\n |page=838\n | title = Large prime numbers\n | volume = 168\n | year = 1951\n | issue=4280|bibcode = 1951Natur.168..838M }}.</ref>\n\n===Theoretical biology===\n\nIn [[population dynamics]] the growth of human population is sometimes supposed to be double exponential. Varfolomeyev and Gurevich<ref>{{citation\n | last1 = Varfolomeyev | first1 = S. D.\n | last2 = Gurevich | first2 = K. G.\n | doi = 10.1006/jtbi.2001.2384\n | issue = 3\n | journal = Journal of Theoretical Biology\n | pages = 367–372\n | title = The hyperexponential growth of the human population on a macrohistorical scale\n | volume = 212\n | year = 2001\n | pmid = 11829357}}.</ref> experimentally fit\n\n:<math> N(y)=375.6\\cdot 1.00185^{1.00737^{y-1000}} \\,</math>\n\nwhere ''N''(''y'') is the population in year ''y'' in millions.\n\n===Physics===\nIn the [[Toda oscillator]] model of [[self-pulsation]], the logarithm of amplitude varies exponentially with time (for large amplitudes), thus the amplitude varies as doubly exponential function of time.<ref name=\"kouz\">{{citation\n | last1 = Kouznetsov | first1 = D.\n | last2 = Bisson | first2 = J.-F.\n | last3 = Li | first3 = J.\n | last4 = Ueda | first4 = K.\n | doi = 10.1088/1751-8113/40/9/016\n | journal = [[Journal of Physics A]]\n | pages = 1–18\n | title = Self-pulsing laser as oscillator Toda: Approximation through elementary functions\n | url = http://www.iop.org/EJ/abstract/-search=15823442.1/1751-8121/40/9/016\n | volume = 40\n | year = 2007\n | issue = 9 | bibcode=2007JPhA...40.2107K}}.</ref>\n\n==References==\n{{reflist|2}}\n\n[[Category:Elementary special functions]]\n[[Category:Exponentials]]"
    },
    {
      "title": "Exponential minus 1",
      "url": "https://en.wikipedia.org/wiki/Exponential_minus_1",
      "text": "#redirect [[Exponential function#expm1]] {{R to related topic}}\n\n[[Category:Elementary special functions]]\n[[Category:Analytic functions]]\n[[Category:Exponentials]]"
    },
    {
      "title": "General antilogarithm",
      "url": "https://en.wikipedia.org/wiki/General_antilogarithm",
      "text": "#redirect [[Logarithm#Antilogarithm]] {{R to related topic}}\n\n[[Category:Exponentials]]\n[[Category:Elementary special functions]]"
    },
    {
      "title": "Goniometric function",
      "url": "https://en.wikipedia.org/wiki/Goniometric_function",
      "text": "#redirect [[Trigonometric functions]] {{R from alternative name}} {{R from singular}} {{R to plural}}\n\n[[Category:Trigonometry]]\n[[Category:Trigonometric functions]]\n[[Category:Elementary special functions]]\n[[Category:Analytic functions]]"
    },
    {
      "title": "Gudermannian function",
      "url": "https://en.wikipedia.org/wiki/Gudermannian_function",
      "text": "[[Image:Mplwp gudermann piaxis.svg|thumb|300px|right|[[Graph of a function|Graph]] of the Gudermannian function]]\nThe '''Gudermannian function''', named after [[Christoph Gudermann]] (1798–1852), relates the [[circular function]]s and [[hyperbolic function]]s without explicitly using [[complex numbers]].\n\nIt is defined for all ''x'' by<ref>\n{{citation |ref=NIST \n|editor1-first=F. W.J. |editor1-last=Olver | editor2-first =D.W.  |editor2-last= Lozier |editor3-first= R.F.  |editor3-last= Boisvert |editor4-first=C.W.  |editor4-last=Clark \n|year=2010\n|title=NIST Handbook of Mathematical Functions \n|publisher=Cambridge University Press \n|url=http://dlmf.nist.gov\n|postscript =. [http://dlmf.nist.gov/4.23#viii Section 4.23(viii)].\n}}\n</ref><ref>\nCRC ''Handbook of Mathematical Sciences'' 5th&nbsp;ed. pp. 323–325\n</ref><ref name=weinstein>\n {{mathworld|urlname=Gudermannian|title=Gudermannian}}\n</ref>\n:<math> \\operatorname{gd} x=\\int_0^x\\frac{1}{\\cosh t} \\, dt.\n</math>\n:&nbsp; <!-- fudge vertical spacing -->\n\n==Properties==\n\n===Alternative definitions===\n:<math>\\begin{align} \\operatorname{gd} x\n&=\\arcsin\\left(\\tanh x \\right)=\\arctan(\\sinh x) = \\operatorname{arccsc}(\\coth x) \\\\\n&=\\operatorname{sgn}(x)\\cdot\\arccos\\left(\\operatorname{sech} x \\right)=\\operatorname{sgn}(x)\\cdot\\operatorname{arcsec}(\\cosh x) \\\\\n&=2\\arctan\\left[\\tanh\\left(\\tfrac12x\\right)\\right]\\\\\n&=2\\arctan(e^x)-\\tfrac12\\pi.\n\\end{align} </math>\n\n===Some identities===\n:<math>\\begin{align}\n\\sin(\\operatorname{gd} x) =\\tanh x ;\\quad\n& \\csc(\\operatorname{gd} x)=\\coth x ;\\\\\n\\cos(\\operatorname{gd} x) =\\operatorname{sech} x ;\\quad &\n\\sec(\\operatorname{gd} x)=\\cosh x ;\\\\\n\\tan(\\operatorname{gd} x) =\\sinh x ;\\quad & \n\\cot(\\operatorname{gd} x)=\\operatorname{csch} x ;\\\\\n\\tan(\\tfrac{1}{2}\\operatorname{gd} x) =\\tanh(\\tfrac{1}{2}x).\n\\end{align}</math>\n\n===Inverse===\n[[Image:Mplwp gudermann inv piaxis.svg|thumb|300px|right|[[Graph of a function|Graph]] of the inverse Gudermannian function]]\n:<math>\n\\begin{align}\n\\operatorname{gd}^{-1} x \n& = \\int_0^x\\frac{1}{\\cos t}\\,dt \\qquad -\\pi/2<x<\\pi/2\\\\[8pt]\n& = \\ln\\left| \\frac{1 + \\sin x}{\\cos x} \\right| = \\frac12\\ln \\left| \\frac{1 + \\sin x}{1 - \\sin x} \\right|  = \\ln \\left| \\frac{1 + \\tan \\frac{x}{2}}{1 - \\tan \\frac{x}{2}} \\right| \\\\[8pt]\n& = \\ln\\left| \\tan x +\\sec x \\right| = \\ln \\left| \\tan\\left(\\frac{x}{2} + \\frac{\\pi}{4}\\right) \\right| \\\\[8pt]\n& = \\operatorname{artanh}(\\sin x) = \\operatorname{arsinh}(\\tan x)\\\\\n& = 2 \\operatorname{arctanh} \\left(\\tan \\frac{x}{2} \\right)\\\\\n& = \\operatorname{arcoth}(\\csc x) = \\operatorname{arcsch}(\\cot x)\\\\\n& = \\operatorname{sgn}(x)\\operatorname{arcosh}(\\sec x) = \\operatorname{sgn}(x) \\operatorname{arsech}(\\cos x)\\\\\n& = -i \\operatorname{gd}(ix)\n\\end{align}\n</math>\n(See [[inverse hyperbolic function]]s.)\n\n===Some identities===\n:<math>\\begin{align}\n\\sinh(\\operatorname{gd}^{-1} x) =\\tan x ;\\quad\n& \\operatorname{csch}(\\operatorname{gd}^{-1} x)=\\cot x ;\\\\\n\\cosh(\\operatorname{gd}^{-1} x ) =\\sec x ;\\quad\n& \\operatorname{sech}(\\operatorname{gd}^{-1} x)=\\cos x ;\\\\\n\\tanh(\\operatorname{gd}^{-1}x) =\\sin x ;\\quad\n& \\coth(\\operatorname{gd}^{-1} x)=\\csc x .\n\\end{align}</math>\n\n===Derivatives===\n:<math>\\frac{d}{dx} \\operatorname{gd} x = \\operatorname{sech} x;\n\\quad \\frac{d}{dx}\\;\\operatorname{gd}^{-1} x=\\sec x.</math>\n\n==History==\nThe function was introduced by [[Johann Heinrich Lambert]] in the 1760s at the same time as the [[hyperbolic functions]]. He called it the \"transcendent angle,\" and it went by various names until 1862 when [[Arthur Cayley]] suggested it be given its current name as a tribute to Gudermann's work in the 1830s on the theory of special functions.<ref>\nGeorge F. Becker, C. E. Van Orstrand. ''Hyperbolic functions.'' Read Books, 1931. Page xlix.\nScanned copy available at [https://archive.org/details/hyperbolicfuncti020206mbp archive.org]\n</ref> Gudermann had published articles in ''[[Crelle's Journal]]'' that were collected in ''Theorie der potenzial- oder {{sic|c|hide=y}}yklisch-hyperbolischen Fun{{sic|c|hide=y}}tionen'' (1833), a book which expounded ''sinh'' and ''cosh'' to a wide audience (under the guises of <math>\\mathfrak{Sin}</math> and <math>\\mathfrak{Cos}</math>).\n\nThe notation ''gd'' was introduced by Cayley<ref>{{cite journal\n|year = 1862\n|last = Cayley |first = A. |authorlink = Arthur Cayley\n|title = On the transcendent gd. u\n|journal = Philosophical Magazine |series=4th Series\n|volume = 24\n|issue = 158 |pages = 19–21\n|doi = 10.1080/14786446208643307\n|url = https://books.google.com/books?id=K1cEAAAAYAAJ&pg=PA19\n}}</ref> where he starts by calling ''gd. u'' the inverse of the [[integral of the secant function]]:\n\n:<math>u = \\int_0^\\phi \\sec t \\,dt = \\ln \\left(\\tan\\left(\\tfrac14\\pi+\\tfrac12\\phi\\right)\\right)</math>\n\nand then derives \"the definition\" of the transcendent:\n\n:<math>\\operatorname{gd} u = i^{-1}\\ln \\left(\\tan\\left(\\tfrac14\\pi+\\tfrac12ui\\right)\\right)</math>\n\nobserving immediately that it is a real function of ''u''.\n\n==Applications==\n\n*The [[angle of parallelism]] function in [[hyperbolic geometry]] is defined by\n:<math>\\tfrac{1}{2}\\pi - \\operatorname{gd} x</math>\n\n*On a [[Mercator projection]] a line of constant latitude is parallel to the equator (on the projection) and is displaced by an amount proportional to the inverse Gudermannian of the latitude.\n*The Gudermannian (with a complex argument) may be used  in the definition of the  [[transverse Mercator projection]].<ref>\n{{citation |last=Osborne |first=P |year=2013 \n|title=The Mercator projections\n|url=https://zenodo.org/record/35392#.VpwXk5oS9Mw\n|postscript=, p74\n}}</ref>\n\n*The Gudermannian appears in a non-periodic solution of the [[inverted pendulum]].<ref>\n{{Cite journal|jstor=2687148|\ntitle=Gudermann and the Simple Pendulum\n|author=John S. Robertson\n|journal=The College Mathematics Journal\n|volume=28|issue= 4 |date=1997|pages=271–276\n|postscript =.  [http://www.codee.org/library/reviews/summaries/gudermann-and-the-simple-pendulum Review].\n|\ndoi=10.2307/2687148\n}}</ref>\n\n*The Gudermannian also appears in a moving mirror solution of the dynamical [[Casimir effect]].<ref>{{Cite journal|arxiv=1303.6756|doi=10.1103/PhysRevD.88.025023|title=Time dependence of particle creation from accelerating mirrors|journal=Physical Review D|volume=88|issue=2|pages=025023|year=2013|last1=Good|first1=Michael R. R.|last2=Anderson|first2=Paul R.|last3=Evans|first3=Charles R.|bibcode=2013PhRvD..88b5023G}}</ref>\n\n==See also==\n*[[Hyperbolic secant distribution]]\n*[[Mercator projection]]\n*[[Tangent half-angle formula]]\n*[[Tractrix]]\n*[[Trigonometric identity]]\n\n==References==\n{{reflist}}\n\n[[Category:Trigonometry]]\n[[Category:Elementary special functions]]\n[[Category:Exponentials]]"
    },
    {
      "title": "Inverse trigonometric functions",
      "url": "https://en.wikipedia.org/wiki/Inverse_trigonometric_functions",
      "text": "{{Trigonometry}}\nIn [[mathematics]], the '''inverse trigonometric functions''' (occasionally also called '''arcus functions''',<ref name=\"Taczanowski_1978\"/><ref name=\"Hazewinkel_1994\"/><ref name=\"Ebner_2005\"/><ref name=\"Mejlbro_2010\"/><ref name=\"Duran_2012\"/> '''antitrigonometric functions'''<ref name=\"Hall_1909\"/> or '''cyclometric functions'''<ref name=\"Klein_1924\"/><ref name=\"Klein_2004\"/><ref name=\"Dörrie_1965\"/>) are the [[inverse function]]s of the [[trigonometric functions]] (with suitably restricted [[Domain of a function|domain]]s). Specifically, they are the inverses of the [[sine]], [[cosine]], [[tangent (trigonometry)|tangent]], [[cotangent]], [[secant (trigonometry)|secant]], and [[cosecant]] functions, and are used to obtain an angle from any of the angle's trigonometric ratios. Inverse trigonometric functions are widely used in [[engineering]], [[navigation]], [[physics]], and [[geometry]].\n\n==Notation==\nThere are several notations used for the inverse trigonometric functions.\n\nThe most common convention is to name inverse trigonometric functions using an arc- prefix: {{math|arcsin(''x'')}}, {{math|arccos(''x'')}}, {{math|arctan(''x'')}}, etc.<ref name=\"Hall_1909\"/> (This convention is used throughout this article.)  This notation arises from the following geometric relationships:{{citation needed|date=January 2019}}\nWhen measuring in radians, an angle of ''θ'' radians will correspond to an arc whose length is ''rθ'', where ''r'' is the radius of the circle. Thus, in the [[unit circle]], \"the arc whose cosine is ''x''\" is the same as \"the angle whose cosine is ''x''\", because the length of the arc of the circle in radii is the same as the measurement of the angle in radians.<ref name=\"Americana_1912\"/> In computer programming languages the inverse trigonometric functions are usually called by the abbreviated forms asin, acos, atan.{{citation needed|reason=\"usually\" is a claim that deserves defense.|date=January 2019}}\n\nThe notations {{math|sin<sup>&minus;1</sup>(''x'')}}, {{math|cos<sup>&minus;1</sup>(''x'')}}, {{math|tan<sup>&minus;1</sup>(''x'')}}, etc., as introduced by [[John Herschel]] in 1813,<ref name=\"Cajori\"/><ref name=\"Herschel_1813\"/> are often used as well in English-language<ref name=\"Hall_1909\"/> sources, and this convention complies with the notation of an [[inverse function]]. This might appear to conflict logically with the common semantics for expressions like {{math|sin<sup>2</sup>(''x'')}}, which refer to numeric power rather than function composition, and therefore may result in confusion between [[multiplicative inverse]] and [[Inverse function|compositional inverse]]. The confusion is somewhat ameliorated by the fact that each of the reciprocal trigonometric functions has its own name—for example, {{math|(cos(''x''))<sup>&minus;1</sup>}} = {{math|sec(''x'')}}. Nevertheless, certain authors advise against using it for its ambiguity.<ref name=\"Hall_1909\"/><ref name=\"Korn_2000\"/> Another convention used by a few authors is to use a [[majuscule]] (capital/upper-case) first letter along with a &minus;1 superscript: {{math|Sin<sup>&minus;1</sup>(''x'')}}, {{math|Cos<sup>&minus;1</sup>(''x'')}}, {{math|Tan<sup>&minus;1</sup>(''x'')}}, etc.<ref name=\"Bhatti_1999\"/>  This potentially avoids confusion with the multiplicative inverse, which should be represented by {{math|sin<sup>&minus;1</sup>(''x'')}}, {{math|cos<sup>&minus;1</sup>(''x'')}}, etc.\n\nSince 2009, the [[ISO 80000-2#Function symbols and definitions|ISO 80000-2]] standard has specified solely the \"arc\" prefix for the inverse functions.\n\n==Basic properties==\n\n===Principal values===\nSince none of the six trigonometric functions are [[One-to-one function|one-to-one]], they are restricted in order to have inverse functions. Therefore, the [[Range (mathematics)|range]]s of the inverse functions are proper [[subset]]s of the domains of the original functions.\n\nFor example, using ''function'' in the sense of [[multivalued function]]s, just as the [[square root]] function {{math|''y'' {{=}} {{sqrt|''x''}}}} could be defined from {{math|''y''<sup>2</sup> {{=}} ''x''}}, the function {{math|''y'' {{=}} arcsin(''x'')}} is defined so that sin(''y'') = ''x''. For a given real number ''x'', with −1 ≤ ''x'' ≤ 1, there are multiple (in fact, countably infinitely many) numbers ''y'' such that {{math|sin(''y'') {{=}} ''x''}}; for example, {{math|sin(0) {{=}} 0}}, but also {{math|sin(&pi;) {{=}} 0}}, {{math|sin(2&pi;) {{=}} 0}}, etc. When only one value is desired, the function may be restricted to its [[principal branch]]. With this restriction, for each ''x'' in the domain the expression {{math|arcsin(''x'')}} will evaluate only to a single value, called its [[principal value]]. These properties apply to all the inverse trigonometric functions.\n\nThe principal inverses are listed in the following table.\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|- \n!Name\n!Usual notation\n!Definition\n!Domain of ''x'' for real result\n!Range of usual principal value <br /> ([[radian]]s)\n!Range of usual principal value <br /> ([[Degree (angle)|degrees]])\n|-\n| '''arcsine''' || ''y'' = {{math|arcsin(''x'')}} || ''x'' = {{math|[[sine|sin]](''y'')}} || −1 ≤ ''x'' ≤ 1 || −{{sfrac|{{pi}}|2}} ≤ ''y'' ≤ {{sfrac|{{pi}}|2}} || −90° ≤ ''y'' ≤ 90°\n|-\n| '''arccosine''' || ''y'' = {{math|arccos(''x'')}} || ''x'' = {{math|[[cosine|cos]](''y'')}} || −1 ≤ ''x'' ≤ 1 || 0 ≤ ''y'' ≤ {{pi}} || 0° ≤ ''y'' ≤ 180°\n|-\n| '''arctangent''' || ''y'' = {{math|arctan(''x'')}} || ''x'' = {{math|[[Tangent (trigonometry)|tan]](''y'')}} || all real numbers || −{{sfrac|{{pi}}|2}} < ''y'' < {{sfrac|{{pi}}|2}} || −90° < ''y'' < 90°\n|-\n| '''arccotangent''' || ''y'' = {{math|arccot(''x'')}} ||''x'' = {{math|[[cotangent|cot]](''y'')}} || all real numbers\n| 0 < ''y'' < {{pi}} || 0° < ''y'' < 180°\n|-\n| '''arcsecant''' || ''y'' = {{math|arcsec(''x'')}} || ''x'' = {{math|[[Secant (trigonometry)|sec]](''y'')}} || ''x'' ≤ −1 or 1 ≤ ''x'' || 0 ≤ ''y'' < {{sfrac|{{pi}}|2}} or {{sfrac|{{pi}}|2}} < ''y'' ≤ {{pi}} || 0° ≤ ''y'' < 90° or 90° < ''y'' ≤ 180°\n|-\n| '''arccosecant''' || ''y'' = {{math|arccsc(''x'')}} || ''x'' = {{math|[[cosecant|csc]](''y'')}} || ''x'' ≤ −1 or 1 ≤ ''x'' || −{{sfrac|{{pi}}|2}} ≤ ''y'' < 0 or 0 < ''y'' ≤ {{sfrac|{{pi}}|2}} || −90° ≤ ''y'' < 0° or 0° < ''y'' ≤ 90°\n|-\n|}\n(Note: Some authors define the range of arcsecant to be ( 0 ≤ ''y'' < {{sfrac|{{pi}}|2}} or {{pi}} ≤ ''y'' < {{sfrac|3{{pi}}|2}} ), because the tangent function is nonnegative on this domain. This makes some computations more consistent. For example, using this range, {{math|tan(arcsec(''x'')) {{=}} {{sqrt|''x''<sup>2</sup> − 1}}}}, whereas with the range ( 0 ≤ ''y'' < {{sfrac|{{pi}}|2}} or {{sfrac|{{pi}}|2}} < ''y'' ≤ {{pi}} ), we would have to write {{math|tan(arcsec(''x'')) {{=}} ±{{sqrt|''x''<sup>2</sup> − 1}}}}, since tangent is nonnegative on 0 ≤ ''y'' < {{sfrac|{{pi}}|2}} but nonpositive on {{sfrac|{{pi}}|2}} < ''y'' ≤ {{pi}}. For a similar reason, the same authors define the range of arccosecant to be −{{pi}} < ''y'' ≤ −{{sfrac|{{pi}}|2}} or 0 < ''y'' ≤ {{sfrac|{{pi}}|2}}.)\n\nIf ''x'' is allowed to be a [[complex number]], then the range of ''y'' applies only to its real part.\n\n===Relationships between trigonometric functions and inverse trigonometric functions===\n\nTrigonometric functions of inverse trigonometric functions are tabulated below. A quick way to derive them is by considering the geometry of a right-angled triangle, with one side of length 1, and another side of length ''x'' (any real number between 0 and 1), then applying the [[Pythagorean theorem]] and definitions of the trigonometric ratios. Purely algebraic derivations are longer.{{citation needed|date=May 2016}}\n\n{|class=\"wikitable\"\n|-\n!<math>\\theta</math>\n!<math>\\sin(\\theta)</math>\n!<math>\\cos(\\theta)</math>\n!<math>\\tan(\\theta)</math>\n!Diagram\n|-\n!<math>\\arcsin(x)</math>\n|<math>\\sin(\\arcsin(x)) = x </math>\n|<math>\\cos(\\arcsin(x)) = \\sqrt{1-x^2}</math>\n|<math>\\tan(\\arcsin(x)) = \\frac{x}{\\sqrt{1-x^2}}</math>\n|[[File:Trigonometric functions and inverse3.svg|150px]]\n|-\n!<math>\\arccos(x)</math>\n|<math>\\sin(\\arccos(x)) = \\sqrt{1-x^2}</math>\n|<math>\\cos(\\arccos(x)) = x </math>\n|<math>\\tan(\\arccos(x)) = \\frac{\\sqrt{1-x^2}}{x}</math>\n|[[File:Trigonometric functions and inverse.svg|150px]]\n|-\n!<math>\\arctan(x)</math>\n|<math>\\sin(\\arctan(x)) = \\frac{x}{\\sqrt{1+x^2}}</math>\n|<math>\\cos(\\arctan(x)) = \\frac{1}{\\sqrt{1+x^2}}</math>\n|<math>\\tan(\\arctan(x)) = x</math>\n|[[File:Trigonometric functions and inverse2.svg|150px]]\n|-\n!<math>\\arccsc(x)</math>\n|<math>\\sin(\\arccsc(x)) = \\frac{1}{x}</math>\n|<math>\\cos(\\arccsc(x)) = \\frac{\\sqrt{x^2-1}}{x}</math>\n|<math>\\tan(\\arccsc(x)) = \\frac{1}{\\sqrt{x^2-1}}</math>\n|[[File:Trigonometric functions and inverse5.svg|150px]]\n|-\n!<math>\\arcsec(x)</math>\n|<math>\\sin(\\arcsec(x)) = \\frac{\\sqrt{x^2-1}}{x}</math>\n|<math>\\cos(\\arcsec(x)) = \\frac{1}{x}</math>\n|<math>\\tan(\\arcsec(x)) = \\sqrt{x^2-1}</math>\n|[[File:Trigonometric functions and inverse6.svg|150px]]\n|-\n!<math>\\arccot(x)</math>\n|<math>\\sin(\\arccot(x)) = \\frac{1}{\\sqrt{1+x^2}}</math>\n|<math>\\cos(\\arccot(x)) = \\frac{x}{\\sqrt{1+x^2}}</math>\n|<math>\\tan(\\arccot(x)) = \\frac{1}{x}</math>\n|[[File:Trigonometric functions and inverse4.svg|150px]]\n|-\n|}\n\n===Relationships among the inverse trigonometric functions===\n[[Image:Arcsine Arccosine.svg|168px|right|thumb|The usual principal values of the arcsin(''x'') (red) and arccos(''x'') (blue) functions graphed on the cartesian plane.]]\n[[Image:Arctangent Arccotangent.svg|294px|right|thumb|The usual principal values of the arctan(''x'') and arccot(''x'') functions graphed on the cartesian plane.]]\n[[Image:Arcsecant Arccosecant.svg|294px|right|thumb|Principal values of the arcsec(''x'') and arccsc(''x'') functions graphed on the cartesian plane.]]\n\nComplementary angles:\n:<math>\\begin{align}\n\\arccos(x) &= \\frac{\\pi}{2} - \\arcsin(x) \\\\[0.5em]\n\\arccot(x) &= \\frac{\\pi}{2} - \\arctan(x) \\\\[0.5em]\n\\arccsc(x) &= \\frac{\\pi}{2} - \\arcsec(x)\n\\end{align}</math>\n\nNegative arguments:\n:<math>\\begin{align}\n\\arcsin(-x) &= -\\arcsin(x) \\\\\n\\arccos(-x) &= \\pi -\\arccos(x) \\\\\n\\arctan(-x) &= -\\arctan(x) \\\\\n\\arccot(-x) &= \\pi -\\arccot(x) \\\\\n\\arcsec(-x) &= \\pi -\\arcsec(x) \\\\\n\\arccsc(-x) &= -\\arccsc(x)\n\\end{align}</math>\n\nReciprocal arguments:\n:<math>\\begin{align}\n\\arccos\\left(\\frac{1}{x}\\right) &= \\arcsec(x) \\\\[0.3em]\n\\arcsin\\left(\\frac{1}{x}\\right) &= \\arccsc(x) \\\\[0.3em]\n\\arctan\\left(\\frac{1}{x}\\right) &= \\frac{\\pi}{2} - \\arctan(x) = \\arccot(x) \\, , \\text{ if } x > 0 \\\\[0.3em]\n\\arctan\\left(\\frac{1}{x}\\right) &= -\\frac{\\pi}{2} - \\arctan(x) = \\arccot(x) - \\pi \\, , \\text{ if } x < 0 \\\\[0.3em]\n\\arccot\\left(\\frac{1}{x}\\right) &= \\frac{\\pi}{2} - \\arccot(x) = \\arctan(x) \\, , \\text{ if } x > 0 \\\\[0.3em]\n\\arccot\\left(\\frac{1}{x}\\right) &= \\frac{3\\pi}{2} - \\arccot(x) = \\pi + \\arctan(x) \\, , \\text{ if } x < 0 \\\\[0.3em]\n\\arcsec\\left(\\frac{1}{x}\\right) &= \\arccos(x) \\\\[0.3em]\n\\arccsc\\left(\\frac{1}{x}\\right) &= \\arcsin(x)\n\\end{align}</math>\n\nUseful identities if one only has a fragment of a sine table:\n:<math>\\begin{align}\n\\arccos(x) &= \\arcsin\\left(\\sqrt{1 - x^2}\\right) \\, , \\text{ if } 0 \\leq x \\leq 1 \\\\\n\\arccos(x) &= \\frac{1}{2}\\arccos\\left(2x^2-1\\right) \\, , \\text{ if } 0 \\leq x \\leq 1 \\\\\n\\arcsin(x) &= \\frac{1}{2}\\arccos\\left(1-2x^2\\right) \\, , \\text{ if } 0 \\leq x \\leq 1 \\\\\n\\arcsin(x) &= \\arctan\\left(\\frac{x}{\\sqrt{1 - x^2}}\\right) \\\\\n\\arctan(x) &= \\arcsin\\left(\\frac{x}{\\sqrt{1 + x^2}}\\right)\n\\end{align}</math>\n\nWhenever the square root of a complex number is used here, we choose the root with the positive real part (or positive imaginary part if the square was negative real).\n\nFrom the [[tangent half-angle formula|half-angle formula]], <math>\\tan\\left(\\tfrac{\\theta}{2}\\right) = \\tfrac{\\sin(\\theta)}{1 + \\cos(\\theta)}</math>, we get:\n:<math>\\begin{align}\n\\arcsin(x) &= 2 \\arctan\\left(\\frac{x}{1 + \\sqrt{1 - x^2}}\\right) \\\\[0.5em]\n\\arccos(x) &= 2 \\arctan\\left(\\frac{\\sqrt{1 - x^2}}{1 + x}\\right) \\, , \\text{ if } -1 < x \\leq + 1 \\\\[0.5em]\n\\arctan(x) &= 2 \\arctan\\left(\\frac{x}{1 + \\sqrt{1 + x^2}}\\right)\n\\end{align}</math>\n\n===Arctangent addition formula===\n:<math>\\arctan(u) \\pm \\arctan(v) = \\arctan\\left(\\frac{u \\pm v}{1 \\mp uv}\\right) \\pmod \\pi \\, , \\quad u v \\ne 1 \\, .</math>\nThis is derived from the tangent [[Angle sum and difference identities|addition formula]]\n:<math>\\tan(\\alpha \\pm \\beta) = \\frac{\\tan(\\alpha) \\pm \\tan(\\beta)}{1 \\mp \\tan(\\alpha)\\tan(\\beta)} \\, ,</math>\nby letting\n:<math>\\alpha = \\arctan(u) \\, , \\quad \\beta = \\arctan(v) \\, .</math>\n\n==In calculus==\n\n==={{anchor|Derivatives}}Derivatives of inverse trigonometric functions===\n:{{Main|Differentiation of trigonometric functions}}\n\nThe [[derivative]]s for complex values of ''z'' are as follows:\n:<math>\\begin{align}\n\\frac{d}{dz} \\arcsin(z) &{} = \\frac{1}{\\sqrt{1-z^2}} \\; ;   &z &{}\\neq -1, +1 \\\\\n\\frac{d}{dz} \\arccos(z) &{} = -\\frac{1}{\\sqrt{1-z^2}} \\; ;  &z &{}\\neq -1, +1 \\\\\n\\frac{d}{dz} \\arctan(z) &{} = \\frac{1}{1+z^2} \\; ;          &z &{}\\neq -i, +i\\\\\n\\frac{d}{dz} \\arccot(z) &{} = -\\frac{1}{1+z^2} \\; ;         &z &{}\\neq -i, +i \\\\\n\\frac{d}{dz} \\arcsec(z) &{} = \\frac{1}{z^2 \\sqrt{1 - \\frac{1}{z^{2}}}} \\; ;   &z &{}\\neq -1, 0, +1 \\\\\n\\frac{d}{dz} \\arccsc(z) &{} = -\\frac{1}{z^2 \\sqrt{1 - \\frac{1}{z^{2}}}} \\; ;  &z &{}\\neq -1, 0, +1\n\\end{align}</math>\nOnly for real values of ''x'':\n:<math>\\begin{align}\n\\frac{d}{dx} \\arcsec(x) &{} = \\frac{1}{|x| \\sqrt{x^2-1}} \\; ;  & |x| > 1\\\\\n\\frac{d}{dx} \\arccsc(x) &{} = -\\frac{1}{|x| \\sqrt{x^2-1}} \\; ; & |x| > 1\n\\end{align}</math>\n\nFor a sample derivation: if <math>\\theta = \\arcsin(x)</math>, we get:\n:<math>\\frac{d \\arcsin(x)}{dx} = \\frac{d \\theta}{d \\sin(\\theta)} = \\frac{d \\theta}{\\cos(\\theta)d \\theta} = \\frac{1}{\\cos(\\theta)} = \\frac{1}{\\sqrt{1-\\sin^2(\\theta)}} = \\frac{1}{\\sqrt{1-x^2}}</math>\n\n===Expression as definite integrals===\nIntegrating the derivative and fixing the value at one point gives an expression for the inverse trigonometric function as a definite integral:\n:<math>\\begin{align}\n\\arcsin(x) &{}= \\int_0^x \\frac{1}{\\sqrt{1 - z^2}} \\, dz \\; , & |x| &{} \\leq 1\\\\\n\\arccos(x) &{}= \\int_x^1 \\frac{1}{\\sqrt{1 - z^2}} \\, dz \\; , & |x| &{} \\leq 1\\\\\n\\arctan(x) &{}= \\int_0^x \\frac{1}{z^2 + 1} \\, dz \\; ,\\\\\n\\arccot(x) &{}= \\int_x^\\infty \\frac{1}{z^2 + 1} \\, dz \\; ,\\\\\n\\arcsec(x) &{}= \\int_1^x \\frac{1}{z \\sqrt{z^2 - 1}} \\, dz = \\pi + \\int_x^{-1} \\frac{1}{z \\sqrt{z^2 - 1}} \\, dz\\; , & x &{} \\geq 1\\\\\n\\arccsc(x) &{}= \\int_x^\\infty \\frac{1}{z \\sqrt{z^2 - 1}} \\, dz = \\int_{-\\infty}^x \\frac{1}{z \\sqrt{z^2 - 1}} \\, dz \\; , & x &{} \\geq 1\\\\\n\\end{align}</math>\nWhen ''x'' equals 1, the integrals with limited domains are [[improper integral]]s, but still well-defined.\n\n===Infinite series===\nLike the sine and cosine functions, the inverse trigonometric functions can be calculated using [[power series]], as follows. For arcsine, the series can be derived by expanding its derivative, <math>\\frac{1}{\\sqrt{1-z^2}}</math>, as a [[binomial series]], and integrating term by term (using the integral definition as above). The series for arctangent can similarly be derived by expanding its derivative <math>\\frac{1}{1+z^2}</math> in a [[geometric series]] and applying the integral definition above (see [[Leibniz series]]).\n\n: <math>\n\\begin{align}\n\\arcsin(z) & = z + \\left( \\frac{1}{2} \\right) \\frac{z^3}{3} + \\left( \\frac{1 \\cdot 3}{2 \\cdot 4} \\right) \\frac{z^5}{5} + \\left( \\frac{1 \\cdot 3 \\cdot 5}{2 \\cdot 4 \\cdot 6} \\right) \\frac{z^7}{7} + \\cdots \\\\[5pt]\n& = \\sum_{n=0}^\\infty \\frac{(2n-1)!!}{(2n)!!} \\cdot \\frac{z^{2n+1}}{2n+1} \\\\[5pt]\n& = \\sum_{n=0}^\\infty \\frac{(2n)! \\, z^{2n+1}}{(2n+1)\\left(2^{n}n!\\right)^{2}} \\, ; \\qquad |z| \\le 1\n\\end{align}\n</math>\n\n:<math>\\arctan(z)\n= z - \\frac{z^3}{3} +\\frac{z^5}{5} - \\frac{z^7}{7} + \\cdots\n= \\sum_{n=0}^\\infty \\frac{(-1)^n z^{2n+1}}{2n+1} \\, ; \\qquad |z| \\le 1 \\qquad z \\neq i,-i</math>\n\nSeries for the other inverse trigonometric functions can be given in terms of these according to the relationships given above.  For example, <math>\\arccos(x) = \\pi/2 - \\arcsin(x)</math>, <math>\\arccsc(x) = \\arcsin(1/x)</math>, and so on.  Another series is given by:<ref name=\"Borwein_2004\"/>\n\n:<math>2\\left(\\arcsin\\left(\\frac{x}{2}\\right) \\right)^2 = \\sum_{n=1}^\\infty \\frac{x^{2n}}{n^2\\binom {2n} n}</math>\n\n[[Leonhard Euler]] found a series for the arctangent that converges more quickly than its [[Taylor series]]:\n\n: <math>\\arctan(z) = \\frac z {1 + z^2} \\sum_{n=0}^\\infty \\prod_{k=1}^n \\frac{2k z^2}{(2k + 1)(1 + z^2)}.</math><ref>{{citation|title= An elementary derivation of Euler's series for the arctangent function|journal = The Mathematical Gazette | author = Hwang Chien-Lih | doi = 10.1017/S0025557200178404 | year = 2005 | volume = 89 | issue = 516|pages = 469–470 }}</ref>\n(The term in the sum for ''n'' = 0 is the [[empty product]], so is 1.)\n\nAlternatively, this can be expressed as\n\n:<math>\\arctan(z) = \\sum_{n=0}^\\infty \\frac{2^{2n} (n!)^2}{(2n + 1)!} \\; \\frac{z^{2n + 1}}{(1 + z^2)^{n + 1}}.</math>\n\nAnother series for the arctangent function is given by\n\n:<math>\\arctan(z) = i\\sum_{n=1}^\\infty\\frac{1}{2n - 1}\\left(\\frac{1}{(1 + 2i/z)^{2n-1}} - \\frac{1}{(1 - 2i/z)^{2n - 1}}\\right),</math>\n\nwhere <math>i=\\sqrt{-1}</math> is the [[imaginary unit]].{{citation needed|date=April 2019}}\n\n====Continued fractions for arctangent====\nTwo alternatives to the power series for arctangent are these [[generalized continued fraction]]s:\n\n: <math>\\arctan(z) =\n\\frac z {1 + \\cfrac{(1z)^2}{3 - 1z^2 + \\cfrac{(3z)^2}{5 - 3z^2 + \\cfrac{(5z)^2}{7 - 5z^2 + \\cfrac{(7z)^2}{9-7z^2 + \\ddots}}}}} =\n\\frac{z}{1 + \\cfrac{(1z)^2}{3 + \\cfrac{(2z)^2}{5 + \\cfrac{(3z)^2}{7 + \\cfrac{(4z)^2}{9 + \\ddots}}}}}\n</math>\n\nThe second of these is valid in the cut complex plane. There are two cuts, from &minus;'''i''' to the point at infinity, going down the imaginary axis, and from '''i''' to the point at infinity, going up the same axis. It works best for real numbers running from −1 to 1. The partial denominators are the odd natural numbers, and the partial numerators (after the first) are just (''nz'')<sup>2</sup>, with each perfect square appearing once. The first was developed by [[Leonhard Euler]]; the second by [[Carl Friedrich Gauss]] utilizing the [[Gaussian hypergeometric series]].\n\n===Indefinite integrals of inverse trigonometric functions===\n\nFor real and complex values of ''z'':\n:<math>\\begin{align}\n\\int \\arcsin(z) \\, dz &{}= z \\, \\arcsin(z) + \\sqrt{1 - z^2} + C\\\\\n\\int \\arccos(z) \\, dz &{}= z \\, \\arccos(z) - \\sqrt{1 - z^2} + C\\\\\n\\int \\arctan(z) \\, dz &{}= z \\, \\arctan(z) - \\frac{1}{2} \\ln \\left( 1 + z^2 \\right) + C\\\\\n\\int \\arccot(z) \\, dz &{}= z \\, \\arccot(z) + \\frac{1}{2} \\ln \\left( 1 + z^2 \\right) + C\\\\\n\\int \\arcsec(z) \\, dz &{}= z \\, \\arcsec(z) - \\ln \\left[ z \\left( 1 + \\sqrt{ \\frac{z^2-1}{z^2} } \\right) \\right] + C\\\\\n\\int \\arccsc(z) \\, dz &{}= z \\, \\arccsc(z) + \\ln \\left[ z \\left( 1 + \\sqrt{ \\frac{z^2-1}{z^2} } \\right) \\right] + C\n\\end{align}</math>\n\nFor real ''x'' ≥ 1:\n:<math>\\begin{align}\n\\int \\arcsec(x) \\, dx &{}= x \\, \\arcsec(x) - \\ln \\left( x + \\sqrt{x^2-1} \\right) + C\\\\\n\\int \\arccsc(x) \\, dx &{}= x \\, \\arccsc(x) + \\ln \\left( x + \\sqrt{x^2-1} \\right) + C\n\\end{align}</math>\n\nFor all real ''x'' not between -1 and 1:\n:<math>\\begin{align}\n\\int \\arcsec(x) \\, dx &{}= x \\, \\arcsec(x) - \\sgn(x) \\ln\\left(\\left| x + \\sqrt{x^2-1}\\right|\\right) + C\\\\\n\\int \\arccsc(x) \\, dx &{}= x \\, \\arccsc(x) + \\sgn(x) \\ln\\left(\\left| x + \\sqrt{x^2-1}\\right|\\right) + C\n\\end{align}</math>\n\nThe absolute value is necessary to compensate for both negative and positive values of the arcsecant and arccosecant functions. The signum function is also necessary due to the absolute values in the [[#Derivatives|derivative]]s of the two functions, which create two different solutions for positive and negative values of x. These can be further simplified using the logarithmic definitions of the [[Inverse hyperbolic functions#Logarithmic representation|inverse hyperbolic function]]s:\n:<math>\\begin{align}\n\\int \\arcsec(x) \\, dx &{}= x \\, \\arcsec(x) - \\operatorname{arcosh}(|x|) + C\\\\\n\\int \\arccsc(x) \\, dx &{}= x \\, \\arccsc(x) + \\operatorname{arcosh}(|x|) + C\\\\\n\\end{align}</math>\n\nThe absolute value in the argument of the arcosh function creates a negative half of its graph, making it identical to the signum logarithmic function shown above.\n\nAll of these antiderivatives can be derived using [[integration by parts]] and the simple derivative forms shown above.\n\n====Example====\nUsing <math>\\int u \\, dv = u v - \\int v \\, du</math> (i.e. [[integration by parts]]), set\n\n:<math>\\begin{align}\nu &= \\arcsin(x) & dv &= dx \\\\\ndu &= \\frac{dx}{\\sqrt{1-x^2}} & v &= x\n\\end{align}</math>\n\nThen\n\n:<math>\\int \\arcsin(x) \\, dx = x \\arcsin(x) - \\int \\frac{x}{\\sqrt{1-x^2}} \\, dx,</math>\n\nwhich by a simple [[Integration by substitution|substitution]] yields the final result:\n\n:<math>\\int \\arcsin(x) \\, dx = x \\arcsin(x) + \\sqrt{1-x^2} + C </math>\n\n== Extension to complex plane ==\n\n[[File:Riemann surface for Arg of ArcTan of x.svg|thumb|A [[Riemann surface]] for the argument of the Tan[z]=x function in the complex plane of x. The orange sheet in the middle is the principal sheet representing ArcTan(x). The blue sheet above and green sheet below are displaced by 2 &pi; and -2 &pi; respectively.]]\n\nSince the inverse trigonometric functions are [[analytic function]]s, they can be extended from the real line to the complex plane. This results in functions with multiple sheets and [[branch point]]s. One possible way of defining the extension is:\n:<math>\\arctan(z) = \\int_0^z \\frac{dx}{1 + x^2} \\quad z \\neq -i, +i </math>\nwhere the part of the imaginary axis which does not lie strictly between the branch points (&minus;i and +i) is the [[branch cut]] between the principal sheet and other sheets. The path of the integral must not cross a branch cut. For ''z'' not on a branch cut, a straight line path from 0 to ''z'' is such a path. For ''z'' on a branch cut, the path must approach from Re[x]>0 for the upper branch cut and from Re[x]<0 for the lower branch cut.\n\nThe arcsine function may then be defined as:\n\n:<math>\\arcsin(z) = \\arctan\\left(\\frac{z}{\\sqrt{1 - z^2}}\\right) \\quad z \\neq -1, +1 </math>\nwhere (the square-root function has its cut along the negative real axis and) the part of the real axis which does not lie strictly between &minus;1 and +1 is the branch cut between the principal sheet of arcsin and other sheets;\n:<math>\\arccos(z) = \\frac{\\pi}{2} - \\arcsin(z) \\quad z \\neq -1, +1 </math>\nwhich has the same cut as arcsin;\n:<math>\\arccot(z) = \\frac{\\pi}{2} - \\arctan(z) \\quad z \\neq -i, i </math>\nwhich has the same cut as arctan;\n:<math>\\arcsec(z) = \\arccos\\left(\\frac{1}{z}\\right) \\quad z \\neq -1, 0, +1 </math>\nwhere the part of the real axis between &minus;1 and +1 inclusive is the cut between the principal sheet of arcsec and other sheets;\n:<math>\\arccsc(z) = \\arcsin\\left(\\frac{1}{z}\\right) \\quad z \\neq -1, 0, +1 </math>\nwhich has the same cut as '''arcsec'''.\n\n===Logarithmic forms===\nThese functions may also be expressed using [[complex logarithm]]s. This extends their [[domain of a function|domains]] to the [[complex plane]] in a natural fashion.\n\n:<math>\\begin{align}\n\\arcsin(z) &{}= -i \\ln \\left( iz + \\sqrt{1-z^2} \\right) &{}= \\arccsc\\left(\\frac{1}{z}\\right) \\\\[10pt]\n\\arccos(z) &{}= -i \\ln \\left( z + \\sqrt{z^2-1} \\right) = \\frac{\\pi}{2} \\, + i \\ln \\left( iz + \\sqrt{1-z^2} \\right) = \\frac{\\pi}{2} - \\arcsin(z) &{}= \\arcsec\\left(\\frac{1}{z}\\right) \\\\[10pt]\n\\arctan(z) &{}= \\frac{i}{2}\\ln \\left(\\frac{1 - iz}{1 + iz}\\right) = \\frac{i}{2}\\left[\\ln (1 - iz) - \\ln (1 +iz)\\right] &{}= \\arccot\\left(\\frac{1}{z}\\right) \\\\[10pt]\n\\arccot(z) &{}= \\frac{i}{2}\\ln\\left(\\frac{z - i}{z + i}\\right) = \\frac{i}{2}\\left[\\ln (z - i) - \\ln (z + i)\\right] &{}= \\arctan\\left(\\frac{1}{z}\\right) \\\\[10pt]\n\\arcsec(z) &{}= -i \\ln \\left( \\sqrt{\\frac{1}{z^2} - 1} + \\frac{1}{z} \\right) = i \\, \\ln \\left( \\sqrt{1 - \\frac{1}{z^2}} + \\frac{i}{z} \\right) + \\frac{\\pi}{2} = \\frac{\\pi}{2} - \\arccsc(z) &{}= \\arccos\\left(\\frac{1}{z}\\right) \\\\[10pt]\n\\arccsc(z) &{}= -i \\ln \\left( \\sqrt{1 - \\frac{1}{z^2}} + \\frac{i}{z}\\right) &{}= \\arcsin\\left(\\frac{1}{z}\\right)\n\\end{align}</math>\n\nElementary proofs of these relations proceed via expansion to exponential forms of the trigonometric functions.\n\n====Example proof====\n\n:<math>\\sin(\\phi) = z </math>\n\n:<math>\\phi = \\arcsin(z)</math>\n\nUsing the [[Trigonometric functions#Relationship to exponential function and complex numbers|exponential definition of sine]], one obtains\n\n:<math>z = \\frac{e^{\\phi i} - e^{-\\phi i}}{2i}</math>\n\nLet\n\n:<math>\\xi = e^{\\phi i} </math>\n\nSolving for <math>\\phi</math>\n\n:<math>z = \\frac{\\xi - \\frac{1}{\\xi}}{2i}</math>\n\n:<math>2iz = {\\xi - \\frac{1}{\\xi}}</math>\n\n:<math>{\\xi - 2iz - \\frac{1}{\\xi}} = 0</math>\n\n:<math>\\xi^2 - 2i \\xi z - 1 \\, = \\, 0</math>\n\n:<math>\\xi = iz \\pm \\sqrt{1-z^2} = e^{\\phi i}</math>\n\n:<math>\\phi i = \\ln \\left( iz \\pm \\sqrt{1-z^2} \\right)</math>\n\n:<math>\\phi= -i \\ln \\left( iz \\pm \\sqrt{1-z^2} \\right)</math>\n\n(the positive branch is chosen)\n\n:<math>\\phi= \\arcsin(z) = -i \\ln \\left(iz + \\sqrt{1-z^2} \\right)</math>\n\n{| style=\"text-align:center\"\n |+ [[Domain coloring|color wheel graphs]] of '''Inverse trigonometric functions in the [[complex plane]]'''\n | [[Image:Complex arcsin.jpg|1000x140px|none]]\n | [[Image:Complex arccos.jpg|1000x140px|none]]\n | [[Image:Complex arctan.jpg|1000x140px|none]]\n | [[Image:Complex ArcCot.jpg|1000x140px|none]]\n | [[Image:Complex ArcSec.jpg|158x158px|none]]\n | [[Image:Complex ArcCsc.jpg|1000x140px|none]]\n |-\n | <math>\\arcsin(z)</math>\n | <math>\\arccos(z)</math>\n | <math>\\arctan(z)</math>\n | <math>\\arccot(z)</math>\n | <math>\\arcsec(z)</math>\n | <math>\\arccsc(z)</math>\n |}\n\n==Applications==\n\n===General solutions===\nEach of the trigonometric functions is periodic in the real part of its argument, running through all its values twice in each interval of 2{{pi}}. Sine and cosecant begin their period at 2{{pi}}''k'' − {{sfrac|{{pi}}|2}} (where ''k'' is an integer), finish it at 2{{pi}}''k'' + {{sfrac|{{pi}}|2}}, and then reverse themselves over 2{{pi}}''k'' + {{sfrac|{{pi}}|2}} to 2{{pi}}''k'' + {{sfrac|3{{pi}}|2}}. Cosine and secant begin their period at 2{{pi}}''k'', finish it at 2{{pi}}''k'' + {{pi}}, and then reverse themselves over 2{{pi}}''k'' + {{pi}} to 2{{pi}}''k'' + 2{{pi}}. Tangent begins its period at 2{{pi}}''k'' − {{sfrac|{{pi}}|2}}, finishes it at 2{{pi}}''k'' + {{sfrac|{{pi}}|2}}, and then repeats it (forward) over 2{{pi}}''k'' + {{sfrac|{{pi}}|2}} to 2{{pi}}''k'' + {{sfrac|3{{pi}}|2}}. Cotangent begins its period at 2{{pi}}''k'', finishes it at 2{{pi}}''k'' + {{pi}}, and then repeats it (forward) over 2{{pi}}''k'' + {{pi}} to 2{{pi}}''k'' + 2{{pi}}.\n\nThis periodicity is reflected in the general inverses where ''k'' is some integer:\n:<math>\\sin(y) = x \\; \\Leftrightarrow \\; y = \\arcsin(x) + 2\\pi k \\; \\text{ or } \\; y = \\pi - \\arcsin(x) + 2\\pi k</math>\n:which, written in one equation, is: <math>\\sin(y) = x \\; \\Leftrightarrow \\; y = (-1)^k\\arcsin(x) + \\pi k.</math>\n:<math>\\cos(y) = x \\; \\Leftrightarrow \\; y = \\arccos(x) + 2\\pi k \\; \\text{ or } \\; y = 2\\pi - \\arccos(x) + 2\\pi k</math>\n:which, written in one equation, is: <math>\\cos(y) = x \\; \\Leftrightarrow \\; y = \\pm \\arccos(x) + 2\\pi k.</math>\n:<math>\\tan(y) = x \\; \\Leftrightarrow \\; y = \\arctan(x) + \\pi k</math>\n:<math>\\cot(y) = x \\; \\Leftrightarrow \\; y = \\arccot(x) + \\pi k</math>\n:<math>\\sec(y) = x \\; \\Leftrightarrow \\; y = \\arcsec(x) + 2\\pi k \\text{ or } y = 2\\pi - \\arcsec (x) + 2\\pi k</math>\n:<math>\\csc(y) = x \\; \\Leftrightarrow \\; y = \\arccsc(x) + 2\\pi k \\text{ or } y = \\pi - \\arccsc(x) + 2\\pi k</math>\n\n====Application: finding the angle of a right triangle====\n[[Image:Trigonometry triangle.svg|right|thumb|A right triangle.]]\nInverse trigonometric functions are useful when trying to determine the remaining two angles of a [[right triangle]] when the lengths of the sides of the triangle are known. Recalling the right-triangle definitions of sine and cosine, it follows that\n\n:<math>\\theta = \\arcsin \\left( \\frac{\\text{opposite}}{\\text{hypotenuse}} \\right) = \\arccos \\left( \\frac{\\text{adjacent}}{\\text{hypotenuse}} \\right) .</math>\n\nOften, the hypotenuse is unknown and would need to be calculated before using arcsine or arccosine using the [[Pythagorean Theorem]]: <math>a^2+b^2=h^2</math> where <math>h</math> is the length of the hypotenuse. Arctangent comes in handy in this situation, as the length of the hypotenuse is not needed.\n\n:<math>\\theta = \\arctan \\left( \\frac{\\text{opposite}}{\\text{adjacent}} \\right) \\, .</math>\n\nFor example, suppose a roof drops 8 feet as it runs out 20 feet. The roof makes an angle ''θ'' with the horizontal, where ''θ'' may be computed as follows:\n\n:<math>\\theta\n= \\arctan \\left( \\frac{\\text{opposite}}{\\text{adjacent}} \\right)\n= \\arctan \\left( \\frac{\\text{rise}}{\\text{run}} \\right)\n= \\arctan \\left( \\frac{8}{20} \\right) \\approx 21.8^{\\circ} \\, .</math>\n\n===In computer science and engineering===\n\n====Two-argument variant of arctangent====\n{{anchor|Two-argument variant of arctangent}}\n{{main|atan2}}\nThe two-argument [[atan2]] function computes the arctangent of ''y'' / ''x'' given ''y'' and ''x'', but with a range of (−{{pi}},&nbsp;{{pi}}]. In other words, atan2(''y'',&nbsp;''x'') is the angle between the positive ''x''-axis of a plane and  the point (''x'',&nbsp;''y'') on it, with positive sign for counter-clockwise angles (upper half-plane, ''y''&nbsp;>&nbsp;0), and negative sign for clockwise angles (lower half-plane, ''y''&nbsp;<&nbsp;0). It was first introduced in many computer programming languages, but it is now also common in other fields of science and engineering.\n \nIn terms of the standard '''arctan''' function, that is with range of (−{{sfrac|{{pi}}|2}}, {{sfrac|{{pi}}|2}}), it can be expressed as follows:\n\n:<math>\\operatorname{atan2}(y, x) = \\begin{cases}\n  \\arctan(\\frac y x) & \\quad x > 0 \\\\\n  \\arctan(\\frac y x) + \\pi & \\quad y \\ge 0 \\; , \\; x < 0 \\\\\n  \\arctan(\\frac y x) - \\pi & \\quad y < 0 \\; , \\; x < 0 \\\\\n  \\frac{\\pi}{2} & \\quad y > 0 \\; , \\; x = 0 \\\\\n  -\\frac{\\pi}{2} & \\quad y < 0 \\; , \\; x = 0 \\\\\n  \\text{undefined} & \\quad y = 0 \\; , \\; x = 0\n\\end{cases}</math>\n\nIt also equals the [[principal value]] of the [[arg (mathematics)|arg]]ument of the [[complex number]] ''x''&nbsp;+&nbsp;'''i'''''y''.\n\nThis function may also be defined using the [[tangent half-angle formula]]e as follows:\n:<math>\\operatorname{atan2}(y, x) = 2\\arctan\\left(\\frac{y}{\\sqrt{x^2 + y^2} + x}\\right)</math>\nprovided that either ''x''&nbsp;>&nbsp;0 or ''y''&nbsp;≠&nbsp;0. However this fails if given x&nbsp;≤&nbsp;0 and y&nbsp;=&nbsp;0 so the expression is unsuitable for computational use.\n\nThe above argument order (''y'', ''x'') seems to be the most common, and in particular is used in [[ISO standard]]s such as the [[C (programming language)|C programming language]], but a few authors may use the opposite convention (''x'', ''y'') so some caution is warranted.  These variations are detailed at [[atan2#Realizations of the function in common programming languages|atan2]].\n\n====Arctangent function with location parameter====\nIn many applications{{which|date=March 2014}} the solution <math>y</math> of the equation <math>x=\\tan(y)</math> is to come as close as possible to a given value <math>-\\infty < \\eta < \\infty</math>. The adequate solution is produced by the parameter modified arctangent function \n:<math>\ny = \\arctan_\\eta(x) := \\arctan(x) + \\pi \\cdot \\operatorname{rni}\\left(\\frac{\\eta - \\arctan(x)}{\\pi} \\right)\\, .\n</math>\nThe function <math>\\operatorname{rni}</math> rounds to the nearest integer.\n\n====Numerical accuracy====\n\nFor angles near 0 and {{pi}}, arccosine is [[ill-conditioned]] and will thus calculate the angle with reduced accuracy in a computer implementation (due to the limited number of digits).<ref name=\"Gade_2010\"/> Similarly, arcsine is inaccurate for angles near &minus;{{pi}}/2 and {{pi}}/2.\n\n==See also==\n*[[Inverse exsecant]]\n*[[Inverse versine]]\n*[[Inverse hyperbolic functions]]\n*[[List of integrals of inverse trigonometric functions]]\n*[[List of trigonometric identities]]\n*[[Trigonometric function]]\n\n== References==\n{{Reflist\n|refs =\n<ref name=\"Hall_1909\">{{cite book |title=Trigonometry |volume=Part I: Plane Trigonometry |author1-first=Arthur Graham |author1-last=Hall |author2-first=Fred Goodrich |author2-last=Frink |date=January 1909 |location=Ann Arbor, Michigan, USA |chapter=Chapter II. The Acute Angle [14] Inverse trigonometric functions |publisher=[[Henry Holt and Company]] / Norwood Press / J. S. Cushing Co. - Berwick & Smith Co., Norwood, Massachusetts, USA |publication-place=New York, USA |page=15 |chapter-url = https://archive.org/stream/planetrigonometr00hallrich#page/n30/mode/1up |access-date=2017-08-12 |dead-url=no |quote=[…] <tt>α&nbsp;= arcsin&nbsp;''m''</tt>: It is frequently read \"[[arc-sine]] ''m''\" or \"[[anti-sine]] ''m'',\" since two mutually inverse functions are said each to be the [[anti-function]] of the other. […] A similar symbolic relation holds for the other [[trigonometric function]]s. […] This notation is universally used in Europe and is fast gaining ground in this country. A less desirable symbol, <tt>α&nbsp;= sin{{sup|-1}}''m''</tt>, is still found in English and American texts. The notation <tt>α&nbsp;= inv sin ''m''</tt> is perhaps better still on account of its general applicability. […]}}</ref>\n<ref name=\"Klein_1924\">{{cite book |title=Elementarmathematik vom höheren Standpunkt aus: Arithmetik, Algebra, Analysis |volume=1 |author-first=Christian Felix |author-last=Klein |author-link=Christian Felix Klein |date=1924<!-- 1927 --> |orig-year=1902<!-- 1908 --> |edition=3rd |publisher=[[J. Springer]] |location=Berlin |language=German}}</ref>\n<ref name=\"Klein_2004\">{{cite book |title=Elementary Mathematics from an Advanced Standpoint: Arithmetic, Algebra, Analysis |author-first=Christian Felix |author-last=Klein |author-link=Christian Felix Klein |date=2004 |orig-year=1932 |edition=Translation of 3rd German |publisher=[[Dover Publications, Inc.]] / [[The Macmillan Company]] |translator-first1=E. R. |translator-last1=Hedrick |translator-first2=C. A. |translator-last2=Noble |isbn=978-0-48643480-3 |url=https://books.google.com/books?id=8KuoxgykfbkC |access-date=2017-08-13 }}</ref>\n<ref name=\"Taczanowski_1978\">{{cite journal |title=On the optimization of some geometric parameters in 14 MeV neutron activation analysis |author-first=Stefan |author-last=Taczanowski |journal=[[Nuclear Instruments and Methods]] |volume=155 |issue=3 |publisher=[[ScienceDirect]] |date=1978-10-01 |pages=543–546 |url = http://www.sciencedirect.com/science/article/pii/0029554X78905414 |access-date=2017-07-26 |doi=10.1016/0029-554X(78)90541-4 }}</ref>\n<ref name=\"Hazewinkel_1994\">{{cite book |title=Encyclopaedia of Mathematics |title-link=Encyclopedia of Mathematics |author-first=Michiel |author-last=Hazewinkel |author-link=Michiel Hazewinkel |publisher=[[Kluwer Academic Publishers]] / [[Springer Science & Business Media]] |orig-year=1987 |date=1994 |edition=unabridged reprint |isbn=978-155608010-4 }}</ref>\n<ref name=\"Ebner_2005\">{{cite book |title=Preparatory Course in Mathematics |author-first=Dieter |author-last=Ebner |publisher=Department of Physics, [[University of Konstanz]] |date=2005-07-25 |edition=6 |url=https://www.math.uni-konstanz.de/numerik/personen/gubisch/de/teaching/ws0708/vorkurs-skript.pdf |access-date=2017-07-26 |dead-url=no |archive-url=https://web.archive.org/web/20170726195140/https://www.math.uni-konstanz.de/numerik/personen/gubisch/de/teaching/ws0708/vorkurs-skript.pdf |archive-date=2017-07-26 }}</ref>\n<ref name=\"Mejlbro_2010\">{{cite book |title=Stability, Riemann Surfaces, Conformal Mappings - Complex Functions Theory |author-first1=Leif |author-last1=Mejlbro |publisher=[[Ventus Publishing ApS]] / [[Bookboon]] |date=2010-11-11 |edition=1 |isbn=978-87-7681-702-2 |url=http://netsaver.myds.me/sym/pub/Netsaver%20Library/Mejlbro,%20Leif/Complex%20Functions%20Theory,%20vol.3%20-%20S%20(2365)/Complex%20Functions%20Theory,%20vol.3%20-%20Mejlbro,%20Leif.pdf |access-date=2017-07-26 |dead-url=no |archive-url=https://web.archive.org/web/20170726203321/http://netsaver.myds.me/sym/pub/Netsaver%20Library/Mejlbro,%20Leif/Complex%20Functions%20Theory,%20vol.3%20-%20S%20(2365)/Complex%20Functions%20Theory,%20vol.3%20-%20Mejlbro,%20Leif.pdf |archive-date=2017-07-26}}</ref>\n<ref name=\"Duran_2012\">{{cite book |title=Mathematical methods for wave propagation in science and engineering |volume=1: Fundamentals |author-first=Mario |author-last=Durán |publisher=Ediciones UC |date=2012 |edition=1 |isbn=978-956141314-6 |page=88}}</ref>\n<ref name=\"Dörrie_1965\">{{cite book |title=Triumph der Mathematik |author-first=Heinrich |author-last=Dörrie |translator-first=David |translator-last=Antin |publisher=[[Dover Publications]] |year=1965 |isbn=978-0-486-61348-2 |page=69}}</ref>\n<ref name=\"Americana_1912\">{{cite book |chapter=Inverse trigonometric functions |title=The Americana: a universal reference library |volume=21 |editor-first1=Frederick Converse |editor-last1=Beach |editor-first2=George Edwin |editor-last2=Rines |date=1912|title-link=The Americana }}</ref>\n<ref name=\"Cajori\">{{cite book |url = https://books.google.com/books?id=bBoPAAAAIAAJ |author-last=Cajori |author-first=Florian |author-link=Florian Cajori |title=A History of Mathematics |page=272 |edition=2 |year=1919 |publisher=[[The Macmillan Company]] |location=New York, NY }}</ref>\n<ref name=\"Herschel_1813\">{{cite journal |url=https://books.google.com/books?id=qpRJAAAAYAAJ&pg=PA8 |author-last=Herschel |author-first=John Frederick William |author-link=John Frederick William Herschel |title=On a remarkable Application of Cotes's Theorem |journal=Philosophical Transactions |page=8 |volume=103 |number=1 |date=1813 |publisher=Royal Society, London|doi=10.1098/rstl.1813.0005 }}</ref>\n<ref name=\"Korn_2000\">{{cite book |title=Mathematical handbook for scientists and engineers: Definitions, theorems, and formulars for reference and review |first1=Grandino Arthur |last1=Korn |first2=Theresa M. |last2=Korn |edition=3<!-- (based on 1968 edition by McGrawHill, Inc.) --> |year=2000 |orig-year=1961 |publisher=[[Dover Publications, Inc.]] |location=Mineola, New York, USA |chapter=21.2.-4. Inverse Trigonometric Functions |page=811 |isbn=978-0-486-41147-7}}</ref>\n<ref name=\"Bhatti_1999\">{{cite book |title=Calculus and Analytic Geometry |date=1999 |publisher=Punjab Textbook Board |location=[[Lahore]] |edition=1 |page=140 |author-first1=Sanaullah |author-last1=Bhatti |author-last2=Nawab-ud-Din |author-first3=Bashir |author-last3=Ahmed |author-first4=S. M. |author-last4=Yousuf |author-first5=Allah Bukhsh |author-last5=Taheem |editor-first1=Mohammad Maqbool |editor-last1=Ellahi |editor-first2=Karamat Hussain |editor-last2=Dar |editor-first3=Faheem |editor-last3=Hussain |language=en-PK |chapter=Differentiation of Trigonometric, Logarithmic and Exponential Functions }}</ref>\n<ref name=\"Borwein_2004\">{{cite book |title=Experimentation in Mathematics: Computational Paths to Discovery |author-first1=Jonathan |author-last1=Borwein |author-first2=David |author-last2=Bailey |author-first3=Roland |author-last3=Gingersohn |edition=1 |date=2004 |publisher=:[[A. K. Peters]] |page=51 |location=Wellesley, MA, USA |isbn=978-1-56881-136-9 }}</ref>\n<ref name=\"Gade_2010\">{{cite journal |author-last=Gade |author-first=Kenneth |date=2010 |title=A non-singular horizontal position representation |journal=The Journal of Navigation |publisher=[[Cambridge University Press]] |volume=63 |issue=3 |pages=395–417 |url=http://www.navlab.net/Publications/A_Nonsingular_Horizontal_Position_Representation.pdf |doi=10.1017/S0373463309990415}}</ref>\n}}\n\n==External links==\n* {{MathWorld |urlname=InverseTrigonometricFunctions |title=Inverse Trigonometric Functions}}\n* {{MathWorld |urlname=InverseTangent |title=Inverse Tangent}}\n\n{{DEFAULTSORT:Inverse Trigonometric Functions}}\n[[Category:Trigonometry]]\n[[Category:Inverse trigonometric functions| ]]\n[[Category:Elementary special functions]]\n[[Category:Mathematical relations]]\n[[Category:Ratios]]\n[[Category:Dimensionless numbers]]<!-- see also (RUSSIAN)  [[:ru:Обсуждение:Тригонометрические функции#Радианы]] -->"
    },
    {
      "title": "Kronecker delta",
      "url": "https://en.wikipedia.org/wiki/Kronecker_delta",
      "text": "{{distinguish|text=the [[Dirac delta function]], nor with the [[Kronecker symbol]]}}\n\nIn [[mathematics]], the '''Kronecker delta''' (named after [[Leopold Kronecker]]) is a [[Function (mathematics)|function]] of two [[Variable (mathematics)|variables]], usually just non-negative [[integer]]s. The function is 1 if the variables are equal, and 0 otherwise:\n:<math>\\delta_{ij} = \\begin{cases}\n0 &\\text{if } i \\neq j,   \\\\\n1 &\\text{if } i=j.   \\end{cases}</math>\nwhere the Kronecker delta {{mvar|δ<sub>ij</sub>}} is a [[piecewise]] function of variables {{mvar|i}} and {{mvar|j}}. For example, {{math|''δ''<sub>1&thinsp;2</sub> {{=}} 0}}, whereas {{math|''δ''<sub>3&thinsp;3</sub> {{=}} 1}}.\n\nThe Kronecker delta appears naturally in many areas of mathematics, physics and engineering, as a means of compactly expressing its definition above.\n\nIn  [[linear algebra]], the {{math|''n'' × ''n''}} [[identity matrix]] {{math|'''I'''}} has entries equal to the Kronecker delta:\n:<math> I_{ij} = \\delta_{ij} </math> \nwhere {{mvar|i}} and {{mvar|j}} take the values {{math|1, 2, ..., ''n''}}, and the [[inner product]] of [[Euclidean vector|vector]]s can be written as \n:<math> \\mathbf{a}\\cdot\\mathbf{b} = \\sum_{i,j=1}^n a_{i}\\delta_{ij}b_{j}.</math>\n\nThe restriction to positive integers is common, but there is no reason it cannot have [[negative integer]]s as well as positive, or any discrete [[rational number]]s. If {{mvar|i}} and {{mvar|j}} above take rational values, then for example\n:<math>\\begin{align} \\delta_{(-1)(-3)}&=0 &\\qquad \\delta_{(-2)(-2)}&=1 \\\\ \\delta_{\\left(\\frac12\\right)\\left(-\\frac32\\right)}&=0 &\\qquad \\delta_{\\left(\\frac53\\right)\\left(\\frac53\\right)}&=1. \\end{align}</math>\nThis latter case is for convenience.\n\n== Properties ==\nThe following equations are satisfied: \n:<math>\\begin{align}\n\\sum_{j} \\delta_{ij} a_j  &= a_i,\\\\\n\\sum_{i} a_i\\delta_{ij}   &= a_j,\\\\\n\\sum_{k} \\delta_{ik}\\delta_{kj} &= \\delta_{ij}.\n\\end{align}</math>\nTherefore, the matrix {{math|'''δ'''}} can be considered as an identity matrix.\n\nAnother useful representation is the following form:\n:<math>\\delta_{nm} = \\frac{1}{N} \\sum_{k = 1}^N e^{2 \\pi i \\frac{k}{N}(n-m)}</math>\nThis can be derived using the formula for the [[finite geometric series]].\n\n==Alternative notation==\nUsing the [[Iverson bracket]]:\n: <math>\\delta_{ij} = [i=j ].</math>\n\nOften, a single-argument notation {{mvar|δ<sub>i</sub>}} is used, which is equivalent to setting {{math|''j'' {{=}} 0}}:\n\n:<math>\\delta_{i} = \\begin{cases}\n0, & \\mbox{if } i \\ne 0  \\\\\n1, & \\mbox{if } i=0 \\end{cases}</math>\n\nIn [[linear algebra]], it can be thought of as a [[tensor]], and is written {{mvar|δ{{su|p=i|b=j}}}}. Sometimes the Kronecker delta is called the substitution tensor.<ref name=\"Trowbridge\">{{cite journal |last=Trowbridge |first=J. H. |year=1998 |title=On a Technique for Measurement of Turbulent Shear Stress in the Presence of Surface Waves |journal=[[Journal of Atmospheric and Oceanic Technology]] |volume=15 |issue=1 |page=291 |doi=10.1175/1520-0426(1998)015<0290:OATFMO>2.0.CO;2 }}</ref>\n\n==Digital signal processing==\n[[Image:unit impulse.gif|thumb|right|An impulse function]]\nSimilarly, in [[digital signal processing]], the same concept is represented as a sequence or discrete function on {{math|ℤ}} (the [[integer]]s):\n\n:<math>\\delta[n] = \\begin{cases} 0, & n \\ne 0 \\\\ 1, & n = 0.\\end{cases}</math>\n\nThe function is referred to as an ''impulse'', or ''[[unit impulse]]''.  When it is the input to a [[discrete-time signal]] processing element, the output is called the [[impulse response]] of the element.\n\n==Properties of the delta function==\n<!-- Please do not \"correct\" sifting to shifting.  The Kronecker delta acts as a sieve; that is, it *sifts*. -->\nThe Kronecker delta has the so-called ''sifting'' property that for {{math|''j'' ∈ ℤ}}:\n:<math>\\sum_{i=-\\infty}^\\infty a_i \\delta_{ij} =a_j.</math>\nand if the integers are viewed as a [[measure space]], endowed with the [[counting measure]], then this property coincides with the defining property of the [[Dirac delta function]]\n:<math>\\int_{-\\infty}^\\infty \\delta(x-y)f(x)\\, dx=f(y),</math>\nand in fact Dirac's delta was named after the Kronecker delta because of this analogous property.  In signal processing it is usually the context (discrete or continuous time) that distinguishes the Kronecker and Dirac \"functions\".  And by convention, {{math|''δ''(''t'')}} generally indicates continuous time (Dirac), whereas arguments like {{mvar|i}}, {{mvar|j}}, {{mvar|k}}, {{mvar|l}}, {{mvar|m}}, and {{mvar|n}} are usually reserved for discrete time (Kronecker).  Another common practice is to represent discrete sequences with square brackets; thus: {{math|''δ''[''n'']}}. It is important to note that the Kronecker delta is not the result of directly sampling the Dirac delta function.\n\nThe Kronecker delta forms the multiplicative [[identity element]] of an [[incidence algebra]].<ref>{{citation | first1=Eugene | last1=Spiegel | first2=Christopher J. | last2=O'Donnell | title=Incidence Algebras | publisher=Marcel Dekker | isbn=0-8247-0036-8 | year=1997 | series=Pure and Applied Mathematics | volume=206 }}.</ref>\n\n==Relationship to the Dirac delta function==\nIn [[probability theory]] and [[statistics]], the Kronecker delta and [[Dirac delta function]] can both be used to represent a [[discrete distribution]].  If the [[support (mathematics)|support]] of a distribution consists of points {{math|'''x''' {{=}} {''x''<sub>1</sub>, ..., ''x<sub>n</sub>''}|}}, with corresponding probabilities {{math|''p''<sub>1</sub>, ..., ''p<sub>n</sub>''}}, then the [[probability mass function]] {{math|''p''(''x'')}} of the distribution over {{math|'''x'''}} can be written, using the Kronecker delta, as\n\n:<math>p(x) = \\sum_{i=1}^n p_i \\delta_{x x_i}.</math>\n\nEquivalently, the [[probability density function]] {{math|''f''(''x'')}} of the distribution can be written using the Dirac delta function as\n\n:<math>f(x) = \\sum_{i=1}^n p_i \\delta(x-x_i).</math>\n\nUnder certain conditions, the Kronecker delta can arise from sampling a Dirac delta function.  For example, if a Dirac delta impulse occurs exactly at a sampling point and is ideally lowpass-filtered (with cutoff at the critical frequency) per the [[Nyquist–Shannon sampling theorem]], the resulting discrete-time signal will be a Kronecker delta function.\n\n==Generalizations==\nIf it is considered as a type {{math|(1,1)}} [[tensor]], the Kronecker tensor can be written\n{{math|''δ''{{su|p=''i''|b=''j''|lh=0.9em}}}} with a [[covariance and contravariance of vectors|covariant]] index {{mvar|j}} and [[Covariance and contravariance of vectors|contravariant]] index {{mvar|i}}:\n:<math>\\delta^{i}_{j} = \\begin{cases} 0 & (i \\ne j), \\\\ 1 & (i = j). \\end{cases}</math>\n\nThis tensor represents:\n* The identity mapping (or identity matrix), considered as a [[linear mapping]] {{math|''V'' → ''V''}} or {{math|''V''{{sup|∗}} → ''V''{{sup|∗}}}}\n* The [[trace (linear algebra)|trace]] or [[tensor contraction]], considered as a mapping {{math|''V''{{sup|∗}} ⊗ ''V'' → ''K''}}\n* The map {{math|''K'' → ''V''{{sup|∗}} ⊗ ''V''}}, representing scalar multiplication as a sum of [[outer product]]s.\n\nThe '''{{visible anchor|generalized Kronecker delta}}''' or '''multi-index Kronecker delta''' of order {{math|2''p''}} is a type {{math|(''p'',''p'')}} tensor that is a completely [[antisymmetric tensor|antisymmetric]] in its {{mvar|p}} upper indices, and also in its {{mvar|p}} lower indices.\n\nTwo definitions that differ by a factor of {{math|''p''!}} are in use. Below, the version is presented has nonzero components scaled to be {{math|±1}}. The second version has nonzero components that are {{math|±{{sfrac|1|''p''!}}}}, with consequent changes scaling factors in formulae, such as the scaling factors of {{math|{{sfrac|1|''p''!}}}} in ''{{section link||Properties of the generalized Kronecker delta}}'' below disappearing<!--This is worded awkwardly-->.<ref>{{cite web|url=http://people.physics.tamu.edu/pope/geom-group.pdf|first=Christopher|last=Pope|date=2008|title=Geometry and Group Theory}}</ref>\n\n=== Definitions of the generalized Kronecker delta ===\nIn terms of the indices:<ref>{{cite book|first=Theodore|last=Frankel|title=The Geometry of Physics: An Introduction|edition=3rd|date=2012|publisher=Cambridge University Press|ISBN=9781107602601}}</ref><ref>{{cite book|first=D. C.|last=Agarwal|title=Tensor Calculus and Riemannian Geometry|edition=22nd|date=2007|publisher=Krishna Prakashan Media|ISBN=}}{{ISBN missing}}</ref>\n:<math>\\delta^{\\mu_1 \\dots \\mu_p }_{\\nu_1 \\dots \\nu_p} = \\begin{cases}\n+1 & \\quad \\text{if } \\nu_1 \\dots \\nu_p \\text{ are distinct integers and are an even permutation of } \\mu_1 \\dots \\mu_p \\\\\n-1 & \\quad \\text{if } \\nu_1 \\dots \\nu_p \\text{ are distinct integers and are an odd permutation of } \\mu_1 \\dots \\mu_p \\\\\n\\;\\;0 & \\quad \\text{in all other cases}.\\end{cases}</math>\n\nLet {{math|S<sub>''p''</sub>}} be the [[symmetric group]] of degree {{mvar|p}}, then:\n:<math>\\delta^{\\mu_1 \\dots \\mu_p}_{\\nu_1 \\dots \\nu_p} \n= \\sum_{\\sigma \\in \\mathrm{S}_p} \\sgn(\\sigma)\\, \\delta^{\\mu_1}_{\\nu_{\\sigma(1)}}\\cdots\\delta^{\\mu_p}_{\\nu_{\\sigma(p)}}\n= \\sum_{\\sigma \\in \\mathrm{S}_p} \\sgn(\\sigma)\\, \\delta^{\\mu_{\\sigma(1)}}_{\\nu_1}\\cdots\\delta^{\\mu_{\\sigma(p)}}_{\\nu_p}. </math>\n\nUsing [[Antisymmetric tensor#Notation|anti-symmetrization]]:\n:<math>\\delta^{\\mu_1 \\dots \\mu_p}_{\\nu_1 \\dots \\nu_p} \n= p! \\delta^{\\mu_1}_{\\lbrack \\nu_1} \\dots \\delta^{\\mu_p}_{\\nu_p \\rbrack}\n= p! \\delta^{\\lbrack \\mu_1}_{\\nu_1} \\dots \\delta^{\\mu_p \\rbrack}_{\\nu_p}.</math>\n\nIn terms of a {{math|''p'' × ''p''}} [[determinant]]:<ref>{{cite book |first1=David |last1=Lovelock |first2=Hanno |last2=Rund |title=Tensors, Differential Forms, and Variational Principles |publisher=Courier Dover Publications |year=1989 |isbn=0-486-65840-6 }}</ref>\n:<math>\\delta^{\\mu_1 \\dots \\mu_p }_{\\nu_1 \\dots \\nu_p} =\n\\begin{vmatrix}\n\\delta^{\\mu_1}_{\\nu_1} & \\cdots & \\delta^{\\mu_1}_{\\nu_p} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\delta^{\\mu_p}_{\\nu_1} & \\cdots & \\delta^{\\mu_p}_{\\nu_p}\n\\end{vmatrix}.</math>\n\nUsing the [[Laplace expansion]] ([[Determinant#Laplace's formula and the adjugate matrix|Laplace's formula]]) of determinant, it may be defined [[Recursion|recursively]]:<ref>A recursive definition requires a first case, which may be taken as {{math|1=''δ'' = 1}} for {{math|1=''p'' = 0}}, or alternatively {{math|1=''δ''{{su|p=''μ''|b=''ν''|lh=0.9em}} = ''δ''{{su|p=''μ''|b=''ν''|lh=0.9em}}}} for {{math|1=''p'' = 1}} (generalized delta in terms of standard delta).</ref>\n:<math>\\begin{align}\n  \\delta^{\\mu_1 \\dots \\mu_p}_{\\nu_1 \\dots \\nu_p}\n    &= \\sum_{k=1}^p (-1)^{p+k} \\delta^{\\mu_p}_{\\nu_k} \\delta^{\\mu_1 \\dots \\mu_{k} \\dots \\check\\mu_p}_{\\nu_1 \\dots \\check\\nu_k \\dots \\nu_p} \\\\\n    &= \\delta^{\\mu_p}_{\\nu_p} \\delta^{\\mu_1 \\dots \\mu_{p - 1}}_{\\nu_1 \\dots \\nu_{p-1}} - \\sum_{k=1}^{p-1} \\delta^{\\mu_p}_{\\nu_k} \\delta^{\\mu_1 \\dots \\mu_{k-1}\\, \\mu_k\\, \\mu_{k+1} \\dots \\mu_{p-1}}_{\\nu_1 \\dots \\nu_{k-1}\\, \\nu_p\\, \\nu_{k+1} \\dots \\nu_{p-1}},\n\\end{align}</math>\nwhere the caron, {{math|ˇ}}, indicates an index that is omitted from the sequence.\n\nWhen {{math|1=''p'' = ''n''}} (the dimension of the vector space), in terms of the [[Levi-Civita symbol]]:\n:<math>\\delta^{\\mu_1 \\dots \\mu_n}_{\\nu_1 \\dots \\nu_n} = \\varepsilon^{\\mu_1 \\dots \\mu_n}\\varepsilon_{\\nu_1 \\dots \\nu_n}.</math>\n\n=== Properties of the generalized Kronecker delta ===\nThe generalized Kronecker delta may be used for [[Antisymmetric tensor#Notation|anti-symmetrization]]:\n:<math>\\begin{align}\n \\frac{1}{p!} \\delta^{\\mu_1 \\dots \\mu_p}_{\\nu_1 \\dots \\nu_p} a^{\\nu_1 \\dots \\nu_p} &= a^{\\lbrack \\mu_1 \\dots \\mu_p \\rbrack} , \\\\\n \\frac{1}{p!} \\delta^{\\mu_1 \\dots \\mu_p}_{\\nu_1 \\dots \\nu_p} a_{\\mu_1 \\dots \\mu_p} &= a_{\\lbrack \\nu_1 \\dots \\nu_p \\rbrack} . \\end{align}</math>\n\nFrom the above equations and the properties of [[anti-symmetric tensor]]s, we can derive the properties of the generalized Kronecker delta:\n:<math>\\begin{align}\n \\frac{1}{p!} \\delta^{\\mu_1 \\dots \\mu_p}_{\\nu_1 \\dots \\nu_p} a^{\\lbrack \\nu_1 \\dots \\nu_p \\rbrack} &= a^{\\lbrack \\mu_1 \\dots \\mu_p \\rbrack} , \\\\\n \\frac{1}{p!} \\delta^{\\mu_1 \\dots \\mu_p}_{\\nu_1 \\dots \\nu_p} a_{\\lbrack \\mu_1 \\dots \\mu_p \\rbrack} &= a_{\\lbrack \\nu_1 \\dots \\nu_p \\rbrack} , \\\\\n \\frac{1}{p!} \\delta^{\\mu_1 \\dots \\mu_p}_{\\nu_1 \\dots \\nu_p} \\delta^{\\nu_1 \\dots \\nu_p}_{\\rho_1 \\dots \\rho_p} &= \\delta^{\\mu_1 \\dots \\mu_p}_{\\rho_1 \\dots \\rho_p} ,\n\\end{align}</math>\nwhich are the generalized version of formulae written in ''{{section link||Properties}}''. The last formula is equivalent to the [[Cauchy–Binet formula]].\n\nReducing the order via summation of the indices may be expressed by the identity<ref>\n{{cite book |first=Sadri |last=Hassani |title=Mathematical Methods: For Students of Physics and Related Fields |edition=2nd |publisher=Springer-Verlag |year=2008 |isbn=978-0-387-09503-5 }}</ref>\n:<math> \\delta^{\\mu_1 \\dots \\mu_s \\, \\mu_{s+1} \\dots \\mu_p}_{\\nu_1 \\dots \\nu_s \\, \\mu_{s+1} \\dots \\mu_p} = \\frac{(n-s)!}{(n-p)!} \\delta^{\\mu_1 \\dots \\mu_s}_{\\nu_1 \\dots \\nu_s}.</math>\n\nUsing both the summation rule for the case {{math|1=''p'' = ''n''}} and the relation with the Levi-Civita symbol,\n[[Levi-Civita symbol#n dimensions|the summation rule of the Levi-Civita symbol]] is derived:\n:<math>\\delta^{\\mu_1 \\dots \\mu_s}_{\\nu_1 \\dots \\nu_s} = \\frac{1}{(n-s)!}\\varepsilon^{\\mu_1 \\dots \\mu_s \\, \\rho_{s+1} \\dots \\rho_n}\\varepsilon_{\\nu_1 \\dots \\nu_s \\, \\rho_{s+1} \\dots \\rho_n}.</math>\n\n==Integral representations==\nFor any integer {{mvar|n}}, using a standard [[Residue (complex analysis)|residue]] calculation we can write an integral representation for the Kronecker delta as the integral below, where the contour of the integral goes counterclockwise around zero. This representation is also equivalent to a definite integral by a rotation in the complex plane.\n\n:<math>  \\delta_{x,n} = \\frac1{2\\pi i} \\oint_{|z|=1} z^{x-n-1} \\,dz=\\frac1{2\\pi} \\int_0^{2\\pi} e^{i(x-n)\\varphi} \\,d\\varphi</math>\n\n==The Kronecker comb==\nThe Kronecker comb function with period {{mvar|N}} is defined (using [[digital signal processing|DSP]] notation) as:\n\n:<math>\\Delta_N[n]=\\sum_{k=-\\infty}^\\infty \\delta[n-kN],</math>\n\nwhere {{mvar|N}} and {{mvar|n}} are integers. The Kronecker comb thus consists of an infinite series of unit impulses {{mvar|N}} units apart, and includes the unit impulse at zero. It may be considered to be the discrete analog of the [[Dirac comb]].\n\n==Kronecker integral==\n\nThe Kronecker delta is also called degree of mapping of one surface into another.<ref>{{Cite book |title=Advanced Calculus |first=Wilfred |last=Kaplan |publisher=Pearson Education |year=2003 |isbn=0-201-79937-5 |page=364 |url=https://books.google.com/books?id=wywnAQAAIAAJ&pg=PA364 }}</ref> Suppose a mapping takes place from surface {{mvar|S<sub>uvw</sub>}} to {{mvar|S<sub>xyz</sub>}} that are boundaries of regions, {{mvar|R<sub>uvw</sub>}} and {{mvar|R<sub>xyz</sub>}} which is simply connected with one-to-one correspondence. In this framework, if {{mvar|s}} and {{mvar|t}} are parameters for {{mvar|S<sub>uvw</sub>}}, and {{mvar|S<sub>uvw</sub>}} to {{mvar|S<sub>uvw</sub>}} are each oriented by the outer normal {{math|'''n'''}}:\n\n:<math> u=u(s,t), \\quad v=v(s,t), \\quad w=w(s,t), </math>\n\nwhile the normal has the direction of\n\n:<math>(u_{s} \\mathbf{i} +v_{s} \\mathbf{j} + w_{s} \\mathbf{k}) \\times (u_{t}\\mathbf{i} +v_{t}\\mathbf{j} +w_{t}\\mathbf{k}).</math>\n\nLet {{math|''x'' {{=}} ''x''(''u'',''v'',''w'')}}, {{math|''y'' {{=}} ''y''(''u'',''v'',''w'')}}, {{math|''z'' {{=}} ''z''(''u'',''v'',''w'')}} be defined and smooth in a domain containing {{mvar|S<sub>uvw</sub>}}, and let these equations define the mapping of {{mvar|S<sub>uvw</sub>}} onto {{mvar|S<sub>xyz</sub>}}. Then the degree {{mvar|δ}} of mapping is {{math|{{sfrac|1|4π}}}} times the solid angle of the image {{mvar|S}} of {{mvar|S<sub>uvw</sub>}} with respect to the interior point of {{mvar|S<sub>xyz</sub>}}, {{math|''O''}}. If {{math|''O''}} is the origin of the region, {{mvar|R<sub>xyz</sub>}}, then the degree, {{mvar|δ}} is given by the integral:\n\n:<math>\\delta=\\frac{1}{4\\pi}\\iint_{R_{st}}\\left(x^2+y^2+z^2\\right)^{-\\frac32}\n\\begin{vmatrix} x & y & z \\\\\n\\frac{\\partial x}{\\partial s} & \\frac{\\partial y}{\\partial s} & \\frac{\\partial z}{\\partial s} \\\\\n\\frac{\\partial x}{\\partial t} & \\frac{\\partial y}{\\partial t} & \\frac{\\partial z}{\\partial t}\n\\end{vmatrix} \\, ds \\, dt.</math>\n\n==See also==\n*[[Dirac measure]]\n*[[Indicator function]]\n*[[Levi-Civita symbol]]\n*[[Unit function]]\n*[[XNOR gate]]\n\n==References==\n<references />\n\n{{Tensors}}\n\n[[Category:Mathematical notation]]\n[[Category:Elementary special functions]]"
    },
    {
      "title": "Logarithm",
      "url": "https://en.wikipedia.org/wiki/Logarithm",
      "text": "{{short description|Inverse function of exponentiation that also maps products to sums}}\n{{Calculation results}}\n[[File:Logarithm plots.png|right|thumb|upright=1.35|Plots of logarithm functions of three commonly used bases. The special points {{math|log<sub>''b''</sub> ''b'' {{=}} 1}} are indicated by dotted lines, and all curves intersect {{nowrap|in {{math|1= log<sub>''b''</sub> 1 = 0.}}}}]]\n[[File:Binary logarithm plot with ticks.svg|right|thumb|upright=1.35|alt=Graph showing a logarithmic curve, crossing the ''x''-axis at ''x''= 1 and approaching minus infinity along the ''y''-axis.|The [[graph of a function|graph]] of the logarithm to base 2 crosses the [[x axis|''x''-axis]] at {{math|''x'' {{=}} 1}} and passes through the points {{nowrap|(2, 1)}}, {{nowrap|(4, 2)}}, and {{nowrap|(8, 3)}}, depicting, e.g., {{math|log<sub>2</sub>(8) {{=}} 3}} and {{math|2<sup>3</sup> {{=}} 8}}. The graph gets arbitrarily close to the {{mvar|y}}-axis, but [[asymptotic|does not meet it]].]]\nIn [[mathematics]], the '''logarithm''' is the [[inverse function]] to [[exponentiation]]. That means the logarithm of a given number&nbsp;{{mvar|x}} is the [[exponent]] to which another fixed number, the ''[[base (exponentiation)|base]]''&nbsp;{{mvar|b}}, must be raised, to produce that number&nbsp;{{mvar|x}}. In the simplest case, the logarithm counts repeated multiplication of the same factor; e.g., since {{math|1000 {{=}} 10 × 10 × 10 {{=}} 10<sup>3</sup>}}, the \"logarithm to base {{math|10}}\" of {{math|1000}} is {{math|3}}. The logarithm of {{mvar|x}} to ''base'' {{mvar|b}} is denoted as {{math|log<sub>''b''</sub>&thinsp;(''x'')}} (or, without parentheses, as {{math|log<sub>''b''</sub>&thinsp;''x''}}, or even without explicit base as {{math|log&thinsp;''x''}}, when no confusion is possible).  More generally, exponentiation allows any positive [[real number]] to be raised to any real power, always producing a positive result, so {{math|log<sub>''b''</sub>&thinsp;(''x'')}} for any two positive real numbers&nbsp;{{mvar|b}} and&nbsp;{{mvar|x}} where&nbsp;{{mvar|b}} is not equal to&nbsp;{{math|1}}, is always a unique real number&nbsp;{{mvar|y}}. More explicitly, the defining relation between exponentiation and logarithm is:\n\n:<math> \\log_b(x) = y \\quad</math> exactly if <math>\\quad b^y = x. </math>\n\nFor example, {{math|1=log<sub>2</sub>&thinsp;64 = 6}}, as {{math|1=2<sup>6</sup> = 64}}.\n\nThe logarithm to base {{math|10}} (that is {{math|1=''b'' = 10}}) is called the [[common logarithm]] and has many applications in science and engineering. The [[natural logarithm]] has the [[e (mathematical constant)|number {{mvar|e}}]]&nbsp;(that is {{math|''b'' ≈ 2.718}}) as its base; its use is widespread in mathematics and [[physics]], because of its simpler [[derivative]]. The [[binary logarithm]] uses base {{math|2}} (that is {{math|1=''b'' = 2}}) and is commonly used in [[computer science]].  Logarithms are examples of [[concave function]]s.\n\nLogarithms were introduced by [[John Napier]] in the early 17th century as a means to simplify calculations. They were rapidly adopted by navigators, scientists, engineers, surveyors and others to perform high-accuracy computations more easily. Using [[Mathematical table|logarithm tables]], tedious multi-digit multiplication steps can be replaced by table look-ups and simpler addition. This is possible because of the fact—important in its own right—that the logarithm of a [[product (mathematics)|product]] is the [[summation|sum]] of the logarithms of the factors:\n:<math> \\log_b(xy) = \\log_b x + \\log_b y, \\,</math>\nprovided that {{mvar|b}}, {{mvar|x}} and {{mvar|y}} are all positive and {{math|''b'' ≠ 1}}. The [[slide rule]], also based on logarithms, allows quick calculations without tables, but at lower precision.\nThe present-day notion of logarithms comes from [[Leonhard Euler]], who connected them to the [[exponential function]] in the 18th century.\n\n[[Logarithmic scale]]s reduce wide-ranging quantities to tiny scopes. For example, the [[decibel]] (dB) is a [[Units of measurement|unit]] used to express [[Level (logarithmic quantity)|ratio as logarithms]], mostly for signal power and amplitude (of which [[sound pressure]] is a common example). In chemistry, [[pH]] is a logarithmic measure for the [[acid]]ity of an [[aqueous solution]]. Logarithms are commonplace in scientific [[formula]]e, and in measurements of the [[Computational complexity theory|complexity of algorithms]] and of geometric objects called [[fractal]]s. They help describing [[frequency]] ratios of [[Interval (music)|musical intervals]], appear in formulas counting [[prime number]]s or [[Stirling's approximation|approximating]] [[factorial]]s, inform some models in [[psychophysics]], and can aid in [[forensic accounting]].\n\nIn the same way as the logarithm reverses [[exponentiation]], the [[complex logarithm]] is the [[inverse function]] of the exponential function applied to [[complex number]]s. The [[discrete logarithm]] is another variant; it has uses in [[public-key cryptography]].\n\n==Motivation and definition==\n\n[[Addition]], [[multiplication]], and [[exponentiation]] are three fundamental arithmetic operations. Addition, the simplest of these, can be undone by [[subtraction]]: adding, for example, 2 to 3 gives 5. The process of adding 2 can be undone by subtracting 2: 5 − 2 = 3. Multiplication, the next-simplest operation, can be undone by [[division (mathematics)|division]]: doubling a number {{math|x}}, i.e., multiplying ''x'' by 2, the result is {{math|2x}}. To get back {{math|x}}, it is necessary to divide by 2. For example, <math>2 \\cdot 3 = 6</math> and the process of multiplying by 2 is undone by dividing by 2: <math>6 / 2 = 3</math>. The idea and purpose of logarithms is also to undo a fundamental arithmetic operation, namely raising a number to a certain power, an operation also known as exponentiation. For example, raising 2 to the third power yields 8, because 8 is the product of three factors of 2:\n\n: <math>2^3 = 2 \\times 2 \\times 2 = 8</math>\n\nThe logarithm (with respect to base 2) of 8 is 3, reflecting the fact that 2 was raised to the ''third'' power to get 8.\n\n===Exponentiation===\nThis subsection contains a short overview of the exponentiation operation, which is fundamental to understanding logarithms.\nRaising {{mvar|b}} to the {{nowrap|{{mvar|n}}-th}} power, where {{mvar|n}} is a [[natural number]], is done by multiplying {{mvar|n}} factors equal to {{mvar|b}}. The {{nowrap|{{mvar|n}}-th}} power of {{mvar|b}} is written {{math|''b''<sup>''n''</sup>}}, so that\n\n: <math>b^n = \\underbrace{b \\times b \\times \\cdots \\times b}_{n \\text{ factors}}</math>\n\nExponentiation may be extended to {{math|''b''<sup>''y''</sup>}}, where {{mvar|b}} is a positive number and the ''exponent'' {{mvar|y}} is any [[real number]].<ref>{{Citation|last1=Shirali| first1=Shailesh|title=A Primer on Logarithms|publisher=Universities Press|isbn=978-81-7371-414-6|year=2002|location=Hyderabad|url={{google books |plainurl=y |id=0b0igbb3WaQC}}}}, esp. section 2</ref> For example, {{math|''b''<sup>−1</sup>}} is the [[Multiplicative inverse|reciprocal]] of {{mvar|b}}, that is, {{math|1/''b''}}. Raising ''b'' to the power 1/2 is the [[square root]] of ''b''. More generally, raising ''b'' to a [[rational number|rational]] power ''p''/''q'', where ''p'' and ''q'' are integers, is given by\n:<math>b^{p / q} = \\sqrt[q]{b^p},</math>\nthe ''q''-th root of ''b''<sup>''p''</sup>. Finally, any [[irrational number]] (a real number which is not rational) ''y'' can be approximated to arbitrary precision by rational numbers. This can be used to compute the ''y''-th power of ''b'': for example <math>\\sqrt 2 \\approx 1.414 ...</math> and <math>b^{\\sqrt 2}</math> is increasingly well approximated by <math>b^1, b^{1.4}, b^{1.41}, b^{1.414}, ...</math>. A more detailed explanation, as well as the formula {{math|''b''<sup>''m'' + ''n''</sup> {{=}} {{mvar|b}}<sup>''m''</sup> · {{mvar|b}}<sup>{{mvar|n}}</sup>}} is contained in the article on [[exponentiation]].\n\n===Definition===\nThe ''logarithm'' of a positive real number {{mvar|x}} with respect to base {{mvar|b}}{{refn|The restrictions on {{mvar|x}} and {{mvar|b}} are explained in the section [[#Analytic properties|\"Analytic properties\"]].|group=nb}} is the exponent by which {{mvar|b}} must be raised to yield {{mvar|x}}. In other words, the logarithm of {{mvar|x}} to base {{mvar|b}} is the solution {{mvar|y}} to the equation<ref>{{Citation|last1=Kate|first1=S.K.|last2=Bhapkar|first2=H.R.|title=Basics Of Mathematics|location=Pune|publisher=Technical Publications|isbn=978-81-8431-755-8|year=2009|url={{google books |plainurl=y |id=v4R0GSJtEQ4C|page=1}} }}, chapter 1</ref>\n\n: <math>b^y = x.</math>\n\nThe logarithm is denoted \"{{math|log<sub>''b''</sub>&nbsp;''x''}}\" (pronounced as \"the logarithm of {{mvar|x}} to base {{mvar|b}}\" or  \"the {{nowrap|base-''b''}} logarithm of {{mvar|x}}\" or (most commonly) \"the log, base {{mvar|b}}, of {{mvar|x}}\").\n\nIn the equation {{math|1=''y'' = log<sub>''b''</sub>&nbsp;''x''}}, the value {{mvar|y}} is the answer to the question \"To what power must {{mvar|b}} be raised, in order to yield {{mvar|x}}?\".\n\n===Examples===\n* {{math|log<sub>2</sub> 16 {{=}} 4 }}, since {{math| 2<sup>4</sup> {{=}} 2 ×2 × 2 × 2 {{=}} 16}}.\n* Logarithms can also be negative: <math>\\quad \\log_2 \\! \\frac{1}{2} = -1 \\quad</math> since <math>\\quad 2^{-1} = \\frac{1}{2^1} = \\frac{1}{2}.</math>\n* {{math|log<sub>10</sub>150}} is approximately 2.176, which lies between 2 and 3, just as 150 lies between {{math|10<sup>2</sup> {{=}} 100}} and {{math|10<sup>3</sup> {{=}} 1000.}}\n* For any base {{mvar|b}}, {{math|log<sub>''b''</sub> {{mvar|b}} {{=}} 1}} and {{math|1= log<sub>''b''</sub> 1 = 0}}, since {{math|''b''<sup>1</sup> {{=}} {{mvar|b}}}} and {{math|''b''<sup>0</sup> {{=}} 1}}, respectively.\n\n==Logarithmic identities==\n{{Main|List of logarithmic identities}}\n\nSeveral important formulas, sometimes called ''logarithmic identities'' or ''logarithmic laws'', relate logarithms to one another.<ref>All statements in this section can be found in {{Harvard citations|last1=Shirali|first1=Shailesh|year=2002|loc=section 4|nb=yes}}, {{Harvard citations|last1=Downing| first1=Douglas |year=2003|loc=p. 275}}, or {{Harvard citations|last1=Kate|last2=Bhapkar|year=2009|loc=p. 1-1|nb=yes}}, for example.</ref>\n\n===Product, quotient, power, and root===\nThe logarithm of a product is the sum of the logarithms of the numbers being multiplied; the logarithm of the ratio of two numbers is the difference of the logarithms. The logarithm of the {{nowrap|''p''-th}} power of a number is ''p'' times the logarithm of the number itself; the logarithm of a {{nowrap|''p''-th}} root is the logarithm of the number divided by ''p''. The following table lists these identities with examples. Each of the identities can be derived after substitution of the logarithm definitions <math>x = b^{\\log_b x}</math> or <math>y = b^{\\log_b y}</math> in the left hand sides.\n\n{| class=\"wikitable\" style=\"margin: 0 auto;\"\n|-\n! !! Formula !! Example\n|-\n| Product|| <math>\\log_b(x y) = \\log_b x + \\log_b y</math>\n| <math>\\log_3 243 = \\log_3 (9 \\cdot 27) = \\log_3 9 + \\log_3 27 = 2 + 3 = 5</math>\n|-\n| Quotient || <math>\\log_b \\!\\frac{x}{y} = \\log_b x - \\log_b y</math>\n| <math>\\log_2 16 = \\log_2 \\!\\frac{64}{4} = \\log_2 64 - \\log_2 4 = 6 - 2 = 4</math>\n|-\n| Power || <math>\\log_b\\left(x^p\\right) = p \\log_b x</math>\n| <math>\\log_2 64 = \\log_2 \\left(2^6\\right) = 6 \\log_2 2 = 6</math>\n|-\n| Root || <math>\\log_b \\sqrt[p]{x} = \\frac{\\log_b x}{p}</math>\n| <math>\\log_{10} \\sqrt{1000} = \\frac{1}{2}\\log_{10} 1000 = \\frac{3}{2} = 1.5</math>\n|}\n\n===Change of base===<!-- This section is linked from [[Mathematica]] -->\nThe logarithm {{math|log<sub>''b''</sub>''x''}} can be computed from the logarithms of {{mvar|x}} and {{mvar|b}} with respect to an arbitrary base ''k'' using the following formula:\n:<math> \\log_b x = \\frac{\\log_k x}{\\log_k b}.\\, </math>\n\n{{Collapse top|title=Derivation of the conversion factor between logarithms of arbitrary base|width=80%}}\nStarting from the defining identity\n\n: <math> x = b^{\\log_b x} </math>\n\nwe can apply {{math|log<sub>''k''</sub>}} to both sides of this equation, to get\n\n: <math> \\log_k x = \\log_k \\left(b^{\\log_b x}\\right) = \\log_b x \\cdot \\log_k b</math>.\n\nSolving for <math>\\log_b x</math> yields:\n\n: <math> \\log_b x =  \\frac{\\log_k x}{\\log_k b}</math>,\n\nshowing the conversion  factor from given <math>\\log_k</math>-values to their corresponding <math>\\log_b </math>-values to be <math>(\\log_k b)^{-1}.</math>\n{{Collapse bottom}}\n\nTypical [[scientific calculators]] calculate the logarithms to bases 10 and [[e (mathematical constant)|{{mvar|e}}]].<ref>{{Citation | last1=Bernstein | first1=Stephen | last2=Bernstein | first2=Ruth | title=Schaum's outline of theory and problems of elements of statistics. I, Descriptive statistics and probability| publisher=[[McGraw-Hill]] | location=New York | series=Schaum's outline series | isbn=978-0-07-005023-5 | year=1999}}, p.&nbsp;21</ref> Logarithms with respect to any base {{mvar|b}} can be determined using either of these two logarithms by the previous formula:\n:<math> \\log_b x = \\frac{\\log_{10} x}{\\log_{10} b} = \\frac{\\log_{e} x}{\\log_{e} b}. \\,</math>\nGiven a number {{mvar|x}} and its logarithm {{math|log<sub>''b''</sub>''x''}} to an unknown base {{mvar|b}}, the base is given by:\n: <math> b = x^\\frac{1}{\\log_b x},</math> which can be seen from taking the defining equation <math> x = b^{\\log_b x} </math> to the power of <math>\\; \\tfrac{1}{\\log_b x}.</math>\n\n==Particular bases==\n[[File:Log4.svg|thumb|upright=1.2|Plots of logarithm for bases 0.5, 2, and {{mvar|e}}]]\n\nAmong all choices for the base, three are particularly common. These are {{math|1=''b'' = 10}}, {{math|1=''b'' = [[e (mathematical constant)|''e'']]}} (the [[Irrational number|irrational]] mathematical constant ≈ 2.71828), and {{math|1=''b'' = 2}} (the [[binary logarithm]]). In [[mathematical analysis]], the logarithm to base {{mvar|e}} is widespread because of its particular analytical properties explained below. On the other hand, {{nowrap|base-10}} logarithms are easy to use for manual calculations in the [[decimal]] number system:<ref>{{Citation|last1=Downing|first1=Douglas|title=Algebra the Easy Way|series=Barron's Educational Series|location=Hauppauge, NY|publisher=Barron's|isbn=978-0-7641-1972-9|year=2003}}, chapter 17, p.&nbsp;275</ref>\n:<math>\\log_{10}(10 x) = \\log_{10} 10 + \\log_{10} x = 1 + \\log_{10} x.\\ </math>\nThus, {{math|log<sub>10</sub>''x''}} is related to the number of [[decimal digit]]s of a positive integer {{mvar|x}}: the number of digits is the smallest [[integer]] strictly bigger than log<sub>10</sub>''x''.<ref>{{Citation|last1=Wegener|first1=Ingo| title=Complexity theory: exploring the limits of efficient algorithms|publisher=[[Springer-Verlag]]|location=Berlin, New York|isbn=978-3-540-21045-0|year=2005}}, p.&nbsp;20</ref> For example, {{math|log<sub>10</sub>1430}} is approximately 3.15. The next integer is 4, which is the number of digits of 1430. Both the natural logarithm and the logarithm to base two are used in [[information theory]], corresponding to the use of [[Nat (unit)|nats]] or [[bit]]s as the fundamental units of information, respectively.<ref>{{citation|title=Information Theory|first=Jan C. A.|last=Van der Lubbe|publisher=Cambridge University Press|year=1997|isbn=978-0-521-46760-5|page=3|url={{google books |plainurl=y |id=tBuI_6MQTcwC|page=3}}}}</ref> Binary logarithms are also used in [[computer science]], where the [[binary numeral system|binary system]] is ubiquitous, in [[music theory]], where a pitch ratio of two (the [[octave]]) is ubiquitous and the [[Cent (music)|cent]] is the binary logarithm (scaled by 1200) of the ratio between two adjacent equally-tempered pitches in European [[classical music]], and in [[photography]] to measure [[exposure value]]s.<ref>{{citation|title=The Manual of Photography|first1=Elizabeth|last1=Allen|first2=Sophie|last2=Triantaphillidou|publisher=Taylor & Francis|year=2011|isbn=978-0-240-52037-7|page=228|url={{google books |plainurl=y |id=IfWivY3mIgAC|page=228}}}}</ref>\n\nThe following table lists common notations for logarithms to these bases and the fields where they are used. Many disciplines write {{math|log''x''}} instead of {{math|log<sub>''b''</sub>''x''}}, when the intended base can be determined from the context. The notation {{math|<sup>''b''</sup>log''x''}} also occurs.<ref>{{Citation| url=http://www.mathe-online.at/mathint/lexikon/l.html |author1=Franz Embacher |author2=Petra Oberhuemer |title=Mathematisches Lexikon |publisher=mathe online: für Schule, Fachhochschule, Universität unde Selbststudium |accessdate=2011-03-22 |language=German}}</ref> The \"ISO notation\" column lists designations suggested by the [[International Organization for Standardization]] ([[ISO 31-11]]).<ref>{{Citation|title = Guide for the Use of the International System of Units (SI)|first = B.N.|last = Taylor|publisher = US Department of Commerce|year = 1995|url = http://physics.nist.gov/Pubs/SP811/sec10.html#10.1.2|access-date = 25 May 2007|archive-url = https://web.archive.org/web/20070629210131/http://physics.nist.gov/Pubs/SP811/sec10.html#10.1.2#10.1.2|archive-date = 29 June 2007|dead-url = yes|df = dmy-all}}</ref> Because the notation {{math|log {{mvar|x}}}} has been used for all three bases (or when the base is indeterminate or immaterial), the intended base must often be inferred based on context or discipline. In computer science and mathematics, log usually refers to {{math|log<sub>2</sub>}} and {{math|log<sub>e</sub>}}, respectively.<ref>{{citation|first1=Michael T.|last1=Goodrich|author1-link=Michael T. Goodrich|first2=Roberto|last2=Tamassia|author2-link=Roberto Tamassia|title=Algorithm Design: Foundations, Analysis, and Internet Examples|publisher=John Wiley & Sons|year=2002|page=23|quote=One of the interesting and sometimes even surprising aspects of the analysis of data structures and algorithms is the ubiquitous presence of logarithms&nbsp;... As is the custom in the computing literature, we omit writing the base {{mvar|b}} of the logarithm when {{math|1=''b'' = 2}}.}}</ref> In other contexts log often means {{math|log<sub>10</sub>}}.<ref>{{cite book |title=Introduction to Applied Mathematics for Environmental Science |edition=illustrated |first1=David F. |last1=Parkhurst |publisher=Springer Science & Business Media |year=2007 |isbn=978-0-387-34228-3 |page=288 |url={{google books |plainurl=y |id=h6yq_lOr8Z4C|page=288 }} }}</ref>\n\n{| class=\"wikitable\" style=\"text-align:center; margin:1em auto 1em auto;\"\n|-\n! scope=\"col\"|Base {{mvar|b}}\n! scope=\"col\"|Name for log<sub>''b''</sub>''x''\n! scope=\"col\"|ISO notation\n! scope=\"col\"|Other notations\n! scope=\"col\"|Used in\n|-\n! scope=\"row\"|2\n| [[binary logarithm]]\n| {{math|lb {{mvar|x}}}}<ref name=\"gullberg\">{{Citation|title = Mathematics: from the birth of numbers.|author =  Gullberg, Jan|location=New York|publisher = W. W. Norton & Co|year = 1997|isbn=978-0-393-04002-9}}</ref>\n| {{math|ld {{mvar|x}}}}, {{math|log {{mvar|x}}}}, {{math|lg {{mvar|x}}}},<ref>See footnote 1 in {{cite journal|last1=Perl|first1=Yehoshua|last2=Reingold|first2=Edward M.|title=Understanding the complexity of interpolation search|journal=Information Processing Letters|date=December 1977|volume=6|issue=6|pages=219–22|doi=10.1016/0020-0190(77)90072-2}}</ref> {{math|log<sub>2</sub>''x''}}\n| [[computer science]], [[information theory]], [[music theory]], [[photography]]\n|-\n! scope=\"row\"|{{mvar|e}}\n| [[natural logarithm]]\n| {{math|ln {{mvar|x}}}}{{refn|Some mathematicians disapprove of this notation. In his 1985 autobiography, [[Paul Halmos]] criticized what he considered the \"childish ln notation,\" which he said no mathematician had ever used.<ref>\n{{Citation\n|title = I Want to Be a Mathematician: An Automathography\n|author = Paul Halmos\n|publisher =  Springer-Verlag\n|location=Berlin, New York\n|year =  1985\n|isbn=978-0-387-96078-4\n}}</ref>\nThe notation was invented by [[Irving Stringham]], a mathematician.<ref>\n{{Citation\n|title =  Uniplanar algebra: being part I of a propædeutic to the higher mathematical analysis\n|author = Irving Stringham\n|publisher =  The Berkeley Press\n|year = 1893\n|page = xiii\n|url = {{google books |plainurl=y |id=hPEKAQAAIAAJ|page=13}}\n}}</ref><ref>\n{{Citation|title =  Introduction to Financial Technology|author =  Roy S. Freedman|publisher = Academic Press|location=Amsterdam|year =  2006|isbn=978-0-12-370478-8|page = 59|url = {{google books |plainurl=y |id=APJ7QeR_XPkC|page=5}}}}</ref>|name=adaa|group=nb}}\n| {{math|log {{mvar|x}}}}<br />(in mathematics <ref>See Theorem 3.29 in {{cite book|last1=Rudin|first1=Walter|title=Principles of mathematical analysis|date=1984|publisher=McGraw-Hill International|location=Auckland|isbn=978-0-07-085613-4|edition=3rd ed., International student}}</ref> and many [[programming language]]s{{refn|For example [[C (programming language)|C]], [[Java (programming language)|Java]], [[Haskell (programming language)|Haskell]], and [[BASIC programming language|BASIC]].|group=nb}})\n| mathematics, physics, chemistry,<br />[[statistics]], [[economics]], information theory, and engineering\n|-\n! scope=\"row\"|10\n| [[common logarithm]]\n| {{math|lg {{mvar|x}}}}\n| {{math|log {{mvar|x}}}}, {{math|log<sub>10</sub>''x''}}<br />(in engineering, biology, astronomy)\n| various [[engineering]] fields (see [[decibel]] and see below), <br />logarithm [[Mathematical table|tables]], handheld [[Scientific calculator|calculators]], [[spectroscopy]]\n|}\n\n==History==\n{{Main|History of logarithms}}\nThe '''history of logarithm''' in seventeenth-century Europe is the discovery of a new [[function (mathematics)|function]] that extended the realm of analysis beyond the scope of algebraic methods. The method of logarithms was publicly propounded by [[John Napier]] in 1614, in a book titled ''Mirifici Logarithmorum Canonis Descriptio'' (''Description of the Wonderful Rule of Logarithms'').<ref>{{citation |first=John |last=Napier |author-link=John Napier |title=Mirifici Logarithmorum Canonis Descriptio |trans-title=The Description of the Wonderful Rule of Logarithms |language=Latin |location=Edinburgh, Scotland |publisher=Andrew Hart |year=1614 |url=http://gdz.sub.uni-goettingen.de/dms/load/img/?PPN=PPN527914568&DMDID=DMDLOG_0001&LOGID=LOG_0001&PHYSID=PHYS_0001 }}</ref><ref>{{Citation|first=Ernest William |last=Hobson|title=John Napier and the invention of logarithms, 1614|year=1914|publisher=The University Press|location=Cambridge|url=https://archive.org/details/johnnapierinvent00hobsiala}}</ref> Prior to Napier's invention, there had been other techniques of similar scopes, such as the prosthaphaeresis or the use of tables of progressions, extensively developed by [[Jost Bürgi]] around 1600.<ref name=\"folkerts\">{{citation |first1=Menso |last1=Folkerts |first2=Dieter |last2=Launert |first3=Andreas |last3=Thom |date=October 2015 |title=Jost Bürgi's Method for Calculating Sines |arxiv=1510.03180|bibcode=2015arXiv151003180F }}</ref><ref>{{Cite web|url=http://www-history.mcs.st-and.ac.uk/Biographies/Burgi.html|title=Burgi biography|website=www-history.mcs.st-and.ac.uk|access-date=2018-02-14}}</ref>\n\nThe [[common logarithm]] of a number is the index of that power of ten which equals the number.<ref>William Gardner (1742) ''Tables of Logarithms''</ref> Speaking of a number as requiring so many figures is a rough allusion to common logarithm, and was referred to by [[Archimedes]] as the \"order of a number\".<ref>R.C. Pierce (1977) \"A brief history of logarithm\", [[Two-Year College Mathematics Journal]] 8(1):22–26.</ref> The first real logarithms were heuristic methods to turn multiplication into addition, thus facilitating rapid computation. Some of these methods used tables derived from trigonometric identities.<ref>Enrique Gonzales-Velasco (2011) ''Journey through Mathematics – Creative Episodes in its History'', §2.4 Hyperbolic logarithms, p. 117, Springer {{isbn|978-0-387-92153-2}}</ref>\nSuch methods are called [[prosthaphaeresis]].\n\nInvention of the [[function (mathematics)|function]] now known as [[natural logarithm]] began as an attempt to perform a [[quadrature (mathematics)|quadrature]] of a rectangular [[hyperbola]] by [[Grégoire de Saint-Vincent]], a Belgian Jesuit residing in Prague. Archimedes had written ''[[The Quadrature of the Parabola]]'' in the third century BC, but a quadrature for the hyperbola eluded all efforts until Saint-Vincent published his results in 1647. The relation that the logarithm provides between a [[geometric progression]] in its [[argument of a function|argument]] and an [[arithmetic progression]] of values, prompted [[A. A. de Sarasa]] to make the connection of Saint-Vincent's quadrature and the tradition of logarithms in prosthaphaeresis, leading to the term \"hyperbolic logarithm\", a synonym for natural logarithm. Soon the new function was appreciated by [[Christiaan Huygens]], Patavii, and [[James Gregory (mathematician)|James Gregory]]. The notation Log y was adopted by [[Gottfried Wilhelm Leibniz|Leibniz]] in 1675,<ref>[[Florian Cajori]] (1913) \"History of the exponential and logarithm concepts\", [[American Mathematical Monthly]] 20: 5, 35, 75, 107, 148, 173, 205.</ref> and the next year he connected it to the [[integral calculus|integral]]\n<math>\\int \\frac{dy}{y} .</math>\n\n==Logarithm tables, slide rules, and historical applications{{anchor|Antilogarithm}}==\n[[Image:Logarithms Britannica 1797.png|thumb|360px|right|The 1797 ''[[Encyclopædia Britannica]]'' explanation of logarithms]]\n\nBy simplifying difficult calculations before calculators and computers became available, logarithms contributed to the advance of science, especially [[astronomy]]. They were critical to advances in [[surveying]], [[celestial navigation]], and other domains. [[Pierre-Simon Laplace]] called logarithms\n\n::\"...[a]n admirable artifice which, by reducing to a few days the labour of many months, doubles the life of the astronomer, and spares him the errors and disgust inseparable from long calculations.\"<ref>{{Citation |last1=Bryant |first1=Walter W. |title=A History of Astronomy |url=https://archive.org/stream/ahistoryastrono01bryagoog#page/n72/mode/2up |publisher=Methuen & Co|location=London |year=1907 }}, p.&nbsp;44</ref>\n\nAs the function {{math|''f''(''x'') {{=}} {{mvar|b}}<sup>''x''</sup>}} is the inverse function of log<sub>''b''</sub>''x'', it has been called the '''antilogarithm'''.<ref>{{Citation|editor1-last=Abramowitz|editor1-first=Milton|editor1-link=Milton Abramowitz|editor2-last=Stegun|editor2-first=Irene A.|editor2-link=Irene Stegun|title=Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables|publisher=[[Dover Publications]]|location=New York|isbn=978-0-486-61272-0|edition=10th|year=1972|title-link=Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables}}, section 4.7., p.&nbsp;89</ref>\n\n===Log tables===\nA key tool that enabled the practical use of logarithms was the ''[[log table|table of logarithms]]''.<ref>{{Citation | last1=Campbell-Kelly | first1=Martin | title=The history of mathematical tables: from Sumer to spreadsheets | publisher=[[Oxford University Press]] | series=Oxford scholarship online | isbn=978-0-19-850841-0 | year=2003}}, section 2</ref> The first such table was compiled by [[Henry Briggs (mathematician)|Henry Briggs]] in 1617, immediately after Napier's invention but with the innovation of using 10 as the base. Briggs' first table contained the [[common logarithm]]s of all integers in the range 1–1000, with a precision of 14 digits. Subsequently, tables with increasing scope were written. These tables listed the values of {{math|log<sub>10</sub>''x''}}  for any number {{mvar|x}} in a certain range, at a certain precision. Base-10 logarithms were universally used for computation, hence the name common logarithm, since numbers that differ by factors of 10 have logarithms that differ by integers. The common logarithm of {{mvar|x}} can be separated into an [[integer part]] and a [[fractional part]], known as the characteristic and [[mantissa (logarithm)|mantissa]]. Tables of logarithms need only include the mantissa, as the characteristic can be easily determined by counting digits from the decimal point.<ref>{{Citation | last1=Spiegel | first1=Murray R. | last2=Moyer | first2=R.E. | title=Schaum's outline of college algebra | publisher=[[McGraw-Hill]] | location=New York | series=Schaum's outline series | isbn=978-0-07-145227-4 | year=2006}}, p.&nbsp;264</ref> The characteristic of {{math|10 · {{mvar|x}}}} is one plus the characteristic of {{mvar|x}}, and their mantissas are the same.  Thus using a three-digit log table, the logarithm of 3542 is approximated by\n\n:<math>\\log_{10}3542 = \\log_{10}(1000 \\cdot 3.542) = 3 + \\log_{10}3.542 \\approx 3 + \\log_{10}3.54 \\, </math>\n\nGreater accuracy can be obtained by [[interpolation]]:\n\n:<math>\\log_{10}3542  \\approx 3 + \\log_{10}3.54 + 0.2 (\\log_{10}3.55-\\log_{10}3.54)\\, </math>\n\nThe value of {{math| 10<sup>''x''</sup>}} can be determined by reverse look up in the same table, since the logarithm is a [[monotonic function]].\n\n===Computations===\nThe product and quotient of two positive numbers ''c'' and ''d'' were routinely calculated as the sum and difference of their logarithms. The product ''cd'' or quotient ''c''/''d'' came from looking up the antilogarithm of the sum or difference, via the same table:\n:<math> c d = 10^{\\ \\log_{10} c} \\, 10^{\\ \\log_{10} d} = 10^{\\ \\log_{10} c + \\ \\log_{10} d} \\,</math>\nand\n:<math>\\frac c d = c d^{-1} = 10^{\\ \\log_{10} c - \\ \\log_{10} d}. \\,</math>\n\nFor manual calculations that demand any appreciable precision, performing the lookups of the two logarithms, calculating their sum or difference, and looking up the antilogarithm is much faster than performing the multiplication by earlier methods such as [[prosthaphaeresis]], which relies on [[trigonometric identities]].\n\nCalculations of powers and [[nth root|roots]] are reduced to multiplications or divisions and look-ups by\n:<math>c^d = \\left(10^{\\ \\log_{10} c}\\right)^d =  10^{d \\ \\log_{10} c} \\,</math>\n\nand\n:<math>\\sqrt[d]{c} = c^\\frac{1}{d} = 10^{\\frac{1}{d} \\ \\log_{10} c}. \\,</math>\n\nTrigonometric calculations were facilitated by tables that contained the common logarithms of [[trigonometric function]]s.\n\n===Slide rules===\nAnother critical application was the [[slide rule]], a pair of logarithmically divided scales used for calculation. The non-sliding logarithmic scale, [[Gunter's rule]], was invented shortly after Napier's invention. [[William Oughtred]] enhanced it to create the slide rule—a pair of logarithmic scales movable with respect to each other. Numbers are placed on sliding scales at distances proportional to the differences between their logarithms. Sliding the upper scale appropriately amounts to mechanically adding logarithms, as illustrated here:\n\n[[Image:Slide rule example2 with labels.svg|center|thumb|550px|Schematic depiction of a slide rule. Starting from 2 on the lower scale, add the distance to 3 on the upper scale to reach the product 6. The slide rule works because it is marked such that the distance from 1 to {{mvar|x}} is proportional to the logarithm of {{mvar|x}}.|alt=A slide rule: two rectangles with logarithmically ticked axes, arrangement to add the distance from 1 to 2 to the distance from 1 to 3, indicating the product 6.]]\n\nFor example, adding the distance from 1 to 2 on the lower scale to the distance from 1 to 3 on the upper scale yields a product of 6, which is read off at the lower part. The slide rule was an essential calculating tool for engineers and scientists until the 1970s, because it allows, at the expense of precision, much faster computation than techniques based on tables.<ref name=\"ReferenceA\">\n{{Harvard citations\n |last1=Maor |year=2009 |nb=yes |loc=sections 1, 13}}</ref>\n\n==Analytic properties==\nA deeper study of logarithms requires the concept of a ''[[function (mathematics)|function]]''. A function is a rule that, given one number, produces another number.<ref>{{Cite book | last1=Devlin | first1=Keith | author1-link=Keith Devlin | title=Sets, functions, and logic: an introduction to abstract mathematics | publisher=Chapman & Hall/CRC | location=Boca Raton, Fla | edition=3rd | series=Chapman & Hall/CRC mathematics | isbn=978-1-58488-449-1 | year=2004 | url={{google books |plainurl=y |id=uQHF7bcm4k4C}}}}, or see the references in [[function (mathematics)|function]]</ref> An example is the function producing the {{nowrap|''x''-th}} power of {{mvar|b}} from any real number {{mvar|x}}, where the base {{mvar|b}} is a fixed number. This function is written: <math>f(x) = b^x. \\, </math>\n\n===Logarithmic function===\nTo justify the definition of logarithms, it is necessary to show that the equation\n:<math>b^x = y \\,</math>\nhas a solution {{mvar|x}} and that this solution is unique, provided that {{mvar|y}} is positive and that {{mvar|b}} is positive and unequal to 1. A proof of that fact requires the [[intermediate value theorem]] from elementary [[calculus]].<ref name=LangIII.3>{{Citation|last1=Lang|first1=Serge|author1-link=Serge Lang|title=Undergraduate analysis|publisher=[[Springer-Verlag]]|location=Berlin, New York|edition=2nd|series=[[Undergraduate Texts in Mathematics]]|isbn=978-0-387-94841-6|mr=1476913|year=1997|doi=10.1007/978-1-4757-2698-5}}, section III.3</ref> This theorem states that a [[continuous function]] that produces two values ''m'' and ''n'' also produces any value that lies between ''m'' and ''n''. A function is ''continuous'' if it does not \"jump\", that is, if its graph can be drawn without lifting the pen.\n\nThis property can be shown to hold for the function {{math|1=''f''(''x'') = {{mvar|b}}<sup>''x''</sup>}}. Because ''f'' takes arbitrarily large and arbitrarily small positive values, any number {{math|''y'' > 0}} lies between {{math|''f''(''x''<sub>0</sub>)}} and {{math|''f''(''x''<sub>1</sub>)}} for suitable {{math|''x''<sub>0</sub>}} and {{math|''x''<sub>1</sub>}}. Hence, the intermediate value theorem ensures that the equation {{math|1=''f''(''x'') = {{mvar|y}}}} has a solution. Moreover, there is only one solution to this equation, because the function ''f'' is [[monotonic function|strictly increasing]] (for {{math|''b'' > 1}}), or strictly decreasing (for {{math|0 < {{mvar|b}} < 1}}).<ref name=LangIV.2>{{Harvard citations|last1=Lang|year=1997 |nb=yes|loc=section IV.2}}</ref>\n\nThe unique solution {{mvar|x}} is the logarithm of {{mvar|y}} to base {{mvar|b}}, {{math|log<sub>''b''</sub>''y''}}. The function that assigns to {{mvar|y}} its logarithm is called ''logarithm function'' or ''logarithmic function'' (or just ''logarithm'').\n\nThe function {{math|log<sub>''b''</sub>''x''}} is essentially characterized by the above product formula\n:<math>\\log_b(xy) = \\log_b x + \\log_b y.</math>\nMore precisely, the logarithm to any base {{math|''b'' > 1}} is the only [[increasing function]] ''f'' from the positive reals to the reals satisfying {{math|1=''f''(''b'') = 1}} and <ref>{{cite book| title=Foundations of Modern Analysis |volume=1 |last=Dieudonné |first=Jean |page=84 |year=1969 |publisher=Academic Press }} item (4.3.1)</ref>\n:<math>f(xy)=f(x)+f(y).</math>\n\n===Inverse function===\n[[File:Logarithm inversefunctiontoexp.svg|right|thumb|The graph of the logarithm function {{math|log<sub>''b''</sub>(''x'')}} (blue) is obtained by [[Reflection (mathematics)|reflecting]] the graph of the function {{math|''b''<sup>''x''</sup>}} (red) at the diagonal line ({{math|1=''x'' = {{mvar|y}}}}).|alt=The graphs of two functions.]]\nThe formula for the logarithm of a power says in particular that for any number {{mvar|x}},\n:<math>\\log_b \\left (b^x \\right) = x \\log_b b = x.</math>\nIn prose, taking the {{math|''x''-th}} power of {{mvar|b}} and then the {{math|base-''b''}} logarithm gives back {{mvar|x}}. Conversely, given a positive number {{mvar|y}}, the formula\n:<math>b^{\\log_b y} = y</math>\nsays that first taking the logarithm and then exponentiating gives back {{mvar|y}}. Thus, the two possible ways of combining (or [[composition (mathematics)|composing]]) logarithms and exponentiation give back the original number. Therefore, the logarithm to base {{mvar|b}} is the ''[[inverse function]]'' of {{math|''f''(''x'') {{=}} {{mvar|b}}<sup>''x''</sup>}}.<ref>{{Citation | last1=Stewart | first1=James | title=Single Variable Calculus: Early Transcendentals | publisher=Thomson Brooks/Cole |location=Belmont|isbn=978-0-495-01169-9 | year=2007}}, section 1.6</ref>\n\nInverse functions are closely related to the original functions. Their [[graph of a function|graphs]] correspond to each other upon exchanging the {{mvar|x}}- and the {{mvar|y}}-coordinates (or upon reflection at the diagonal line {{mvar|x}} = {{mvar|y}}), as shown at the right: a point {{math|1=(''t'', ''u'' = {{mvar|b}}<sup>''t''</sup>)}} on the graph of ''f'' yields a point {{math|1=(''u'', ''t'' = log<sub>''b''</sub>''u'')}} on the graph of the logarithm and vice versa. As a consequence, log<sub>''b''</sub>(''x'') [[divergent sequence|diverges to infinity]] (gets bigger than any given number) if {{mvar|x}} grows to infinity, provided that {{mvar|b}} is greater than one. In that case, {{math|log<sub>''b''</sub>(''x'')}}  is an [[increasing function]]. For {{math|''b'' < 1}}, {{math|log<sub>''b''</sub>(''x'')}} tends to minus infinity instead. When {{mvar|x}} approaches zero, {{math|log<sub>''b''</sub>''x''}} goes to minus infinity for {{math|''b'' > 1}} (plus infinity for {{math|''b'' < 1}}, respectively).\n\n===Derivative and antiderivative===\n[[File:Logarithm derivative.svg|right|thumb|220px|The graph of the [[natural logarithm]] (green) and its tangent at {{math|''x'' {{=}} 1.5}} (black)|alt=A graph of the logarithm function and a line touching it in one point.]]\nAnalytic properties of functions pass to their inverses.<ref name=LangIII.3 /> Thus, as {{math|1=''f''(''x'') = {{mvar|b}}<sup>''x''</sup>}} is a continuous and [[differentiable function]], so is {{math|log<sub>''b''</sub>''y''}}. Roughly, a continuous function is differentiable if its graph has no sharp \"corners\". Moreover, as the [[derivative]] of {{math|''f''(''x'')}} evaluates to {{math|ln(''b'')''b''<sup>''x''</sup>}} by the properties of the [[exponential function]], the [[chain rule]] implies that the derivative of {{math|log<sub>''b''</sub>''x''}} is given by<ref name=\"LangIV.2\"/><ref>{{cite web |work=Wolfram Alpha |title=Calculation of ''d/dx(Log(b,x))'' |publisher=[[Wolfram Research]] |accessdate=15 March 2011 |url=http://www.wolframalpha.com/input/?i=d/dx(Log(b,x)) }}</ref>\n: <math>\\frac{d}{dx} \\log_b x = \\frac{1}{x\\ln b}. </math>\nThat is, the [[slope]] of the [[tangent]] touching the graph of the {{math|base-''b''}} logarithm at the point {{math|(''x'', log<sub>''b''</sub>(''x''))}} equals {{math|1/(''x'' ln(''b''))}}.\n\nThe derivative of ln {{mvar|x}} is 1/''x''; this implies that ln {{mvar|x}} is the unique [[antiderivative]] of {{math|1/''x''}} that has the value 0 for {{math|1=''x'' =1}}. It is this very simple formula that motivated to qualify as \"natural\" the natural logarithm; this is also one of the main reasons of the importance of the constant [[E (mathematical constant)|{{mvar|e}}]].\n\nThe derivative with a generalised functional argument {{math|''f''(''x'')}} is\n:<math>\\frac{d}{dx} \\ln f(x) = \\frac{f'(x)}{f(x)}.</math>\nThe quotient at the right hand side is called the [[logarithmic derivative]] of ''f''. Computing {{math|''f<nowiki>'</nowiki>''(''x'')}} by means of the derivative of {{math|ln(''f''(''x''))}} is known as [[logarithmic differentiation]].<ref>{{Citation | last1=Kline | first1=Morris | author1-link=Morris Kline | title=Calculus: an intuitive and physical approach | publisher=[[Dover Publications]] | location=New York | series=Dover books on mathematics | isbn=978-0-486-40453-0 | year=1998}}, p.&nbsp;386</ref> The antiderivative of the [[natural logarithm]] {{math|ln(''x'')}} is:<ref>{{cite web |work=Wolfram Alpha |title=Calculation of ''Integrate(ln(x))'' |publisher=Wolfram Research |accessdate=15 March 2011 |url=http://www.wolframalpha.com/input/?i=Integrate(ln(x)) }}</ref>\n: <math>\\int \\ln(x) \\,dx = x \\ln(x) - x + C.</math>\n[[List of integrals of logarithmic functions|Related formulas]], such as antiderivatives of logarithms to other bases can be derived from this equation using the change of bases.<ref>{{Harvard citations|editor1-last=Abramowitz|editor2-last=Stegun|year=1972 |nb=yes|loc=p. 69}}</ref>\n\n===Integral representation of the natural logarithm===\n[[File:Natural logarithm integral.svg|right|thumb|The [[natural logarithm]] of ''t'' is the shaded area underneath the graph of the function {{math|1=''f''(''x'') = 1/''x''}} (reciprocal of {{mvar|x}}).|alt=A hyperbola with part of the area underneath shaded in grey.]]\nThe [[natural logarithm]] of ''t'' equals the [[integral]] of 1/''x''&nbsp;''dx'' from 1 to ''t'':\n:<math>\\ln (t) = \\int_1^t \\frac{1}{x} \\, dx.</math>\nIn other words, {{math|ln(''t'')}} equals the area between the {{mvar|x}} axis and the graph of the function {{math|1/''x''}}, ranging from {{math|1=''x'' = 1}} to {{math|1=''x'' = ''t''}} (figure at the right). This is a consequence of the [[fundamental theorem of calculus]] and the fact that the derivative of {{math|ln(''x'')}} is {{math|1/''x''}}. The right hand side of this equation can serve as a definition of the [[natural logarithm]]. Product and power logarithm formulas can be derived from this definition.<ref>{{Citation|last1=Courant|first1=Richard|title=Differential and integral calculus. Vol. I|publisher=[[John Wiley & Sons]]|location=New York|series=Wiley Classics Library|isbn=978-0-471-60842-4|mr=1009558|year=1988}}, section III.6</ref> For example, the product formula {{math|1=ln(''tu'') = ln(''t'') + ln(''u'')}} is deduced as:\n\n:<math> \\ln(tu) = \\int_1^{tu} \\frac{1}{x} \\, dx \\ \\stackrel {(1)} = \\int_1^{t} \\frac{1}{x} \\, dx + \\int_t^{tu} \\frac{1}{x} \\, dx \\ \\stackrel {(2)} = \\ln(t) + \\int_1^u \\frac{1}{w} \\, dw = \\ln(t) + \\ln(u).</math>\n\nThe equality (1) splits the integral into two parts, while the equality (2) is a change of variable ({{math|1=''w'' = {{mvar|x}}/''t''}}). In the illustration below, the splitting corresponds to dividing the area into the yellow and blue parts. Rescaling the left hand blue area vertically by the factor ''t'' and shrinking it by the same factor horizontally does not change its size. Moving it appropriately, the area fits the graph of the function {{math|1=''f''(''x'') = 1/''x''}} again. Therefore, the left hand blue area, which is the integral of {{math|''f''(''x'')}} from ''t'' to ''tu'' is the same as the integral from 1 to ''u''. This justifies the equality (2) with a more geometric proof.\n\n[[File:Natural logarithm product formula proven geometrically.svg|thumb|center|500px|A visual proof of the product formula of the natural logarithm|alt=The hyperbola depicted twice. The area underneath is split into different parts.]]\n\nThe power formula {{math|1=ln(''t''<sup>''r''</sup>) = ''r'' ln(''t'')}} may be derived in a similar way:\n\n:<math>\n\\ln(t^r) = \\int_1^{t^r} \\frac{1}{x}dx = \\int_1^t \\frac{1}{w^r} \\left(rw^{r - 1} \\, dw\\right) = r \\int_1^t \\frac{1}{w} \\, dw = r \\ln(t).\n</math>\nThe second equality uses a change of variables ([[integration by substitution]]), {{math|1=''w'' = {{mvar|x}}<sup>1/''r''</sup>}}.\n\nThe sum over the reciprocals of natural numbers,\n:<math>1 + \\frac 1 2 + \\frac 1 3 + \\cdots + \\frac 1 n = \\sum_{k=1}^n \\frac{1}{k},</math>\nis called the [[harmonic series (mathematics)|harmonic series]]. It is closely tied to the [[natural logarithm]]: as ''n'' tends to [[infinity]], the difference,\n:<math>\\sum_{k=1}^n \\frac{1}{k} - \\ln(n),</math>\n[[limit of a sequence|converges]] (i.e., gets arbitrarily close) to a number known as the [[Euler–Mascheroni constant]] {{math|1 = ''γ'' = 0.5772...}}. This relation aids in analyzing the performance of algorithms such as [[quicksort]].<ref>{{Citation|last1=Havil|first1=Julian|title=Gamma: Exploring Euler's Constant|publisher=[[Princeton University Press]]|isbn=978-0-691-09983-5|year=2003}}, sections 11.5 and 13.8</ref>\n\nThere are also some other integral representations of the logarithm that are useful in some situations:\n\n:<math> \\ln(x) = -\\lim_{\\epsilon \\to 0} \\int_\\epsilon^\\infty \\frac{dt}{t}\\left( e^{-xt} - e^{-t} \\right)</math>\n:<math> \\ln(x) = \\int_0^\\infty\\,\\frac{dt}{t}\\,\\left[\\cos(t)-\\cos(xt)\\right]</math>\n\nThe first identity can be verified by showing that it has the same value at {{math|1=''x'' = 1}}, and the same derivative.\nThe second identity can be proven by writing\n:<math> \\frac1t =\\int_0^\\infty\\,dq\\,e^{-qt} </math>\nand then inserting the [[Laplace transform]] of {{math|1=cos(''xt'')}} (and {{math|1=cos(''t'')}}).\n\n===Transcendence of the logarithm===\n[[Real number]]s that are not [[Algebraic number|algebraic]] are called [[transcendental number|transcendental]];<ref>{{citation|title=Selected papers on number theory and algebraic geometry|volume=172|first1=Katsumi|last1=Nomizu|authorlink=Katsumi Nomizu|location=Providence, RI|publisher=AMS Bookstore|year=1996|isbn=978-0-8218-0445-2|page=21|url={{google books |plainurl=y |id=uDDxdu0lrWAC|page=21}}}}</ref> for example, [[Pi|{{pi}}]] and ''[[e (mathematical constant)|e]]'' are such numbers, but <math>\\sqrt{2-\\sqrt 3}</math> is not. [[Almost all]] real numbers are transcendental. The logarithm is an example of a [[transcendental function]]. The [[Gelfond–Schneider theorem]] asserts that logarithms usually take transcendental, i.e., \"difficult\" values.<ref>{{Citation|last1=Baker|first1=Alan|author1-link=Alan Baker (mathematician)|title=Transcendental number theory|publisher=[[Cambridge University Press]]|isbn=978-0-521-20461-3|year=1975}}, p.&nbsp;10</ref>\n\n==Calculation==\n[[File:Logarithm keys.jpg|thumb|The logarithm keys (LOG for base-10 and LN for base-{{mvar|e}}) on a typical scientific calculator]]\nLogarithms are easy to compute in some cases, such as {{math|1=log<sub>10</sub>(1000) = 3}}. In general, logarithms can be calculated using [[power series]] or the [[arithmetic–geometric mean]], or be retrieved from a precalculated [[logarithm table]] that provides a fixed precision.<ref>{{Citation | last1=Muller | first1=Jean-Michel | title=Elementary functions | publisher=Birkhäuser Boston | location=Boston, MA | edition=2nd | isbn=978-0-8176-4372-0 | year=2006}}, sections 4.2.2 (p. 72) and 5.5.2 (p. 95)</ref><ref>{{Citation |last1=Hart |last2=Cheney |last3=Lawson |year=1968|publisher=John Wiley|location=New York|title=Computer Approximations|series=SIAM Series in Applied Mathematics|display-authors=etal}}, section 6.3, pp.&nbsp;105–11</ref>\n[[Newton's method]], an iterative method to solve equations approximately, can also be used to calculate the logarithm, because its inverse function, the exponential function, can be computed efficiently.<ref>{{Citation|last1=Zhang |first1=M. |last2=Delgado-Frias |first2=J.G. |last3=Vassiliadis |first3=S. |title=Table driven Newton scheme for high precision logarithm generation |url=http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=326783 |doi=10.1049/ip-cdt:19941268 |journal=IEE Proceedings Computers & Digital Techniques |issn=1350-2387 |volume=141 |year=1994 |issue=5 |pages=281–92 |deadurl=unfit |archiveurl=https://web.archive.org/web/20150529214127/http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=326783 |archivedate=29 May 2015 }}, section 1 for an overview</ref> Using look-up tables, [[CORDIC]]-like methods can be used to compute logarithms if the only available operations are addition and [[Arithmetic shift|bit shifts]].<ref>{{Citation |url= |first=J.E.|last=Meggitt|title=Pseudo Division and Pseudo Multiplication Processes|journal=IBM Journal|date=April 1962|doi=10.1147/rd.62.0210|volume=6|issue=2|pages=210–26}}</ref><ref>{{Citation |last=Kahan |first=W. |authorlink= William Kahan |title=Pseudo-Division Algorithms for Floating-Point Logarithms and Exponentials |date= May 20, 2001 |publisher= |journal= |doi= }}</ref> Moreover, the [[Binary logarithm#Algorithm|binary logarithm algorithm]] calculates {{math|lb(''x'')}} [[recursion|recursively]] based on repeated squarings of {{mvar|x}}, taking advantage of the relation\n:<math>\\log_2\\left(x^2\\right) = 2 \\log_2 (x). \\,</math>\n\n===Power series===\n;Taylor series\n\n[[File:Taylor approximation of natural logarithm.gif|right|thumb|The Taylor series of&nbsp;{{math|ln(''z'')}} centered at&nbsp;{{math|''z'' {{=}} 1}}. The animation shows the first&nbsp;10 approximations along with the 99th and 100th. The approximations do not converge beyond a distance of 1 from the center.|alt=An animation showing increasingly good approximations of the logarithm graph.]]\nFor any real number {{mvar|z}} that satisfies {{math|0 < ''z'' < 2}}, the following formula holds:{{refn|The same series holds for the principal value of the complex logarithm for complex numbers {{mvar|z}} satisfying {{math|{{!}}''z'' − 1{{!}} < 1}}.|group=nb}}<ref name=AbramowitzStegunp.68>{{Harvard citations|editor1-last=Abramowitz|editor2-last=Stegun|year=1972 |nb=yes|loc=p. 68}}</ref>\n:<math>\n\\ln (z)  = \\frac{(z-1)^1}{1} - \\frac{(z-1)^2}{2} + \\frac{(z-1)^3}{3} - \\frac{(z-1)^4}{4} + \\cdots\n</math>\nThis is a shorthand for saying that {{math|ln(''z'')}} can be approximated to a more and more accurate value by the following expressions:\n:<math>\n\\begin{array}{lllll}\n(z-1) & & \\\\\n(z-1) & - &  \\frac{(z-1)^2}{2} & \\\\\n(z-1) & - &  \\frac{(z-1)^2}{2} & + & \\frac{(z-1)^3}{3} \\\\\n\\vdots &\n\\end{array}\n</math>\nFor example, with {{math|''z'' {{=}} 1.5}} the third approximation yields 0.4167, which is about 0.011 greater than {{math|ln(1.5) {{=}} 0.405465}}. This [[series (mathematics)|series]] approximates {{math|ln(''z'')}} with arbitrary precision, provided the number of summands is large enough. In elementary calculus, {{math|ln(''z'')}} is therefore the [[limit (mathematics)|limit]] of this series. It is the [[Taylor series]] of the [[natural logarithm]] at {{math|1=''z'' = 1}}. The Taylor series of {{math|ln(''z'')}} provides a particularly useful approximation to {{math|ln(1+''z'')}} when {{mvar|z}} is small, {{math|{{!}}''z''{{!}} < 1}}, since then\n:<math>\n\\ln (1+z) = z - \\frac{z^2}{2}  +\\frac{z^3}{3}\\cdots \\approx z.\n</math>\nFor example, with {{math|1=''z'' = 0.1}} the first-order approximation gives {{math|ln(1.1) ≈ 0.1}}, which is less than 5% off the correct value 0.0953.\n\n;More efficient series\nAnother series is based on the [[area hyperbolic tangent]] function:\n:<math>\n\\ln (z) = 2\\cdot\\operatorname{artanh}\\,\\frac{z-1}{z+1} = 2 \\left ( \\frac{z-1}{z+1} + \\frac{1}{3}{\\left(\\frac{z-1}{z+1}\\right)}^3 + \\frac{1}{5}{\\left(\\frac{z-1}{z+1}\\right)}^5 + \\cdots \\right ),\n</math>\nfor any real number {{math|''z'' > 0}}.{{refn|The same series holds for the principal value of the complex logarithm for complex numbers {{mvar|z}} with positive real part.|group=nb}}<ref name=AbramowitzStegunp.68 /> Using [[sigma notation]], this is also written as\n:<math>\\ln (z) = 2\\sum_{k=0}^\\infty\\frac{1}{2k+1}\\left(\\frac{z-1}{z+1}\\right)^{2k+1}.</math>\nThis series can be derived from the above Taylor series. It converges more quickly than the Taylor series, especially if {{mvar|z}} is close to 1. For example, for {{math|1=''z'' = 1.5}}, the first three terms of the second series approximate {{math|ln(1.5)}} with an error of about {{val|3|e=-6}}. The quick convergence for {{mvar|z}} close to 1 can be taken advantage of in the following way: given a low-accuracy approximation {{math|''y'' ≈ ln(''z'')}} and putting\n:<math>A = \\frac z{\\exp(y)}, \\,</math>\nthe logarithm of {{mvar|z}} is:\n:<math>\\ln (z)=y+\\ln (A). \\,</math>\nThe better the initial approximation {{mvar|y}} is, the closer {{mvar|A}} is to 1, so its logarithm can be calculated efficiently. {{mvar|A}} can be calculated using the [[exponential function|exponential series]], which converges quickly provided {{mvar|y}} is not too large. Calculating the logarithm of larger {{mvar|z}} can be reduced to smaller values of {{mvar|z}} by writing {{math|''z'' {{=}} ''a'' · 10<sup>''b''</sup>}}, so that {{math|ln(''z'') {{=}} ln(''a'') + {{mvar|b}} · ln(10)}}.\n\nA closely related method can be used to compute the logarithm of integers. Putting <math>\\textstyle z=\\frac{n+1}{n}</math> in the above series, it follows that:\n:<math>\\ln (n+1) = \\ln(n) + 2\\sum_{k=0}^\\infty\\frac{1}{2k+1}\\left(\\frac{1}{2 n+1}\\right)^{2k+1}.</math>\nIf the logarithm of a large integer {{mvar|n}} is known, then this series yields a fast converging series for {{math|log(''n''+1)}}, with a [[rate of convergence]] of <math>\\frac{1}{2 n+1}</math>.\n\n===Arithmetic–geometric mean approximation===\nThe [[arithmetic–geometric mean]] yields high precision approximations of the [[natural logarithm]]. Sasaki and Kanada showed in 1982 that it was particularly fast for precisions between 400 and 1000 decimal places, while Taylor series methods were typically faster when less precision was needed. In their work {{math|ln(''x'')}} is approximated to a precision of {{math|2<sup>−''p''</sup>}} (or ''p'' precise bits) by the following formula (due to [[Carl Friedrich Gauss]]):<ref>{{Citation |first1=T. |last1=Sasaki |first2=Y. |last2=Kanada |title=Practically fast multiple-precision evaluation of log(x) |journal=Journal of Information Processing |volume=5|issue=4 |pages=247–50 |year=1982 | url=http://ci.nii.ac.jp/naid/110002673332 | accessdate=30 March 2011}}</ref><ref>{{Citation |first1=Timm |title=Stacs 99|last1=Ahrendt|publisher=Springer|location=Berlin, New York|series=Lecture notes in computer science|doi=10.1007/3-540-49116-3_28|volume=1564|year=1999|pages=302–12|isbn=978-3-540-65691-3|chapter=Fast Computations of the Exponential Function}}</ref>\n\n:<math>\\ln (x) \\approx \\frac{\\pi}{2  M(1,2^{2-m}/x)} - m \\ln (2).</math>\n\nHere {{math|M(''x'',''y'')}} denotes the [[arithmetic–geometric mean]] of {{mvar|x}} and {{mvar|y}}. It is obtained by repeatedly calculating the average <math>(x+y)/2</math> ([[arithmetic mean]]) and <math>\\sqrt{xy}</math> ([[geometric mean]]) of {{mvar|x}} and {{mvar|y}} then let those two numbers become the next {{mvar|x}} and {{mvar|y}}. The two numbers quickly converge to a common limit which is the value of {{math|M(''x'',''y'')}}. ''m'' is chosen such that\n\n:<math>x \\,2^m > 2^{p/2}.\\, </math>\n\nto ensure the required precision. A larger ''m'' makes the {{math|M(''x'',''y'')}} calculation take more steps (the initial x and y are farther apart so it takes more steps to converge) but gives more precision. The constants {{math|pi}} and {{math|ln(2)}} can be calculated with quickly converging series.\n\n===Feynman's algorithm===\nWhile at [[Los Alamos National Laboratory]] working on the [[Manhattan Project]], [[Richard Feynman]] developed a bit processing algorithm that is similar to long division and was later used in the [[Connection Machine]]. The algorithm uses the fact that every real number <math> 1 < x < 2 </math> is uniquely representable as a product of distinct factors of the form <math> 1 + 2^{-k} </math>. The algorithm sequentially builds that product <math>P</math>: if <math>P \\cdot (1 + 2^{-k}) < x</math>, then it changes <math>P</math> to <math> P \\cdot (1 + 2^{-k}) </math>. It then increase <math>k</math> by one regardless. The algorithm stops when <math>k</math> is large enough to give the desired accuracy. Because <math>\\log(x)</math> is the sum of the terms of the form <math>\\log(1 + 2^{-k})</math> corresponding to those <math>k</math> for which the factor <math>1 + 2^{-k}</math> was included in the product <math>P</math>, <math>\\log(x)</math> may be computed by simple addition, using a table of <math>\\log(1 + 2^{-k})</math> for all <math>k</math>. Any base may be used for the logarithm table.<ref>{{cite journal |first=Danny |last=Hillis |authorlink=Danny Hillis |title=Richard Feynman and The Connection Machine |journal=Physics Today |volume= 42|issue= 2|page= 78|date=January 15, 1989 |doi=10.1063/1.881196|bibcode=1989PhT....42b..78H}}</ref>\n\n==Applications==\n[[File:NautilusCutawayLogarithmicSpiral.jpg|thumb|A [[nautilus]] displaying a logarithmic spiral|alt=A photograph of a nautilus' shell.]]\nLogarithms have many applications inside and outside mathematics. Some of these occurrences are related to the notion of [[scale invariance]]. For example, each chamber of the shell of a [[nautilus]] is an approximate copy of the next one, scaled by a constant factor. This gives rise to a [[logarithmic spiral]].<ref>{{Harvard citations\n |last1=Maor\n |year=2009\n |nb=yes\n |loc=p. 135\n }}</ref> [[Benford's law]] on the distribution of leading digits can also be explained by scale invariance.<ref>{{Citation | last1=Frey | first1=Bruce | title=Statistics hacks | publisher=[[O'Reilly Media|O'Reilly]]|location=Sebastopol, CA| series=Hacks Series |url={{google books |plainurl=y |id=HOPyiNb9UqwC|page=275}}| isbn=978-0-596-10164-0 | year=2006}}, chapter 6, section 64</ref> Logarithms are also linked to [[self-similarity]]. For example, logarithms appear in the analysis of algorithms that solve a problem by dividing it into two similar smaller problems and patching their solutions.<ref>{{Citation | last1=Ricciardi | first1=Luigi M. | title=Lectures in applied mathematics and informatics | url={{google books |plainurl=y |id=Cw4NAQAAIAAJ}} | publisher=Manchester University Press | location=Manchester | isbn=978-0-7190-2671-3 | year=1990}}, p.&nbsp;21, section 1.3.2</ref> The dimensions of self-similar geometric shapes, that is, shapes whose parts resemble the overall picture are also based on logarithms.\n[[Logarithmic scale]]s are useful for quantifying the relative change of a value as opposed to its absolute difference. Moreover, because the logarithmic function {{math|log(''x'')}} grows very slowly for large {{mvar|x}}, logarithmic scales are used to compress large-scale scientific data. Logarithms also occur in numerous scientific formulas, such as the [[Tsiolkovsky rocket equation]], the [[Fenske equation]], or the [[Nernst equation]].\n\n===Logarithmic scale===\n{{Main|Logarithmic scale}}\n[[File:Germany Hyperinflation.svg|A logarithmic chart depicting the value of one [[German gold mark|Goldmark]] in [[German Papiermark|Papiermarks]] during the [[Inflation in the Weimar Republic|German hyperinflation in the 1920s]]|right|thumb|alt=A graph of the value of one mark over time. The line showing its value is increasing very quickly, even with logarithmic scale.]]\nScientific quantities are often expressed as logarithms of other quantities, using a ''logarithmic scale''. For example, the [[decibel]] is a [[unit of measurement]] associated with [[logarithmic-scale]] [[level quantity|quantities]]. It is based on the common logarithm of [[ratio]]s—10 times the common logarithm of a [[power (physics)|power]] ratio or 20 times the common logarithm of a [[voltage]] ratio. It is used to quantify the loss of voltage levels in transmitting electrical signals,<ref>{{Citation|last1=Bakshi|first1=U.A.|title=Telecommunication Engineering |publisher=Technical Publications|location=Pune|isbn=978-81-8431-725-1|year=2009|url={{google books |plainurl=y |id=EV4AF0XJO9wC|page=A5}}}}, section 5.2</ref> to describe power levels of sounds in [[acoustics]],<ref>{{Citation|last1=Maling|first1=George C.|editor1-last=Rossing|editor1-first=Thomas D.|title=Springer handbook of acoustics|publisher=[[Springer-Verlag]]|location=Berlin, New York|isbn=978-0-387-30446-5|year=2007|chapter=Noise}}, section 23.0.2</ref> and the [[absorbance]] of light in the fields of [[spectrometer|spectrometry]] and [[optics]]. The [[signal-to-noise ratio]] describing the amount of unwanted [[noise (electronic)|noise]] in relation to a (meaningful) [[signal (information theory)|signal]] is also measured in decibels.<ref>{{Citation | last1=Tashev | first1=Ivan Jelev | title=Sound Capture and Processing: Practical Approaches | publisher=[[John Wiley & Sons]] | location=New York | isbn=978-0-470-31983-3 | year=2009|url={{google books |plainurl=y |id=plll9smnbOIC|page=48}}|page=98}}</ref> In a similar vein, the [[peak signal-to-noise ratio]] is commonly used to assess the quality of sound and [[image compression]] methods using the logarithm.<ref>{{Citation | last1=Chui | first1=C.K. | title=Wavelets: a mathematical tool for signal processing | publisher=[[Society for Industrial and Applied Mathematics]] | location=Philadelphia | series=SIAM monographs on mathematical modeling and computation | isbn=978-0-89871-384-8 | year=1997|url={{google books |plainurl=y |id=N06Gu433PawC|page=180}}|page=}}</ref>\n\nThe strength of an earthquake is measured by taking the common logarithm of the energy emitted at the quake. This is used in the [[moment magnitude scale]] or the [[Richter magnitude scale]]. For example, a 5.0 earthquake releases 32 times {{math|(10<sup>1.5</sup>)}} and a 6.0 releases 1000 times {{math|(10<sup>3</sup>)}} the energy of a 4.0.<ref>{{Citation|last1=Crauder|first1=Bruce|last2=Evans|first2=Benny|last3=Noell|first3=Alan|title=Functions and Change: A Modeling Approach to College Algebra|publisher=Cengage Learning|location=Boston|edition=4th|isbn=978-0-547-15669-9|year=2008}}, section 4.4.</ref> Another logarithmic scale is [[apparent magnitude]]. It measures the brightness of stars logarithmically.<ref>{{Citation|last1=Bradt|first1=Hale|title=Astronomy methods: a physical approach to astronomical observations|publisher=[[Cambridge University Press]]|series=Cambridge Planetary Science|isbn=978-0-521-53551-9|year=2004}}, section 8.3, p.&nbsp;231</ref> Yet another example is [[pH]] in [[chemistry]]; pH is the negative of the common logarithm of the [[Activity (chemistry)|activity]] of [[hydronium]] ions (the form [[hydrogen]] [[ion]]s {{chem|H|+|}} take in water).<ref>{{Citation|author=[[IUPAC]]|title=Compendium of Chemical Terminology (\"Gold Book\")|edition=2nd|editor=A. D. McNaught, A. Wilkinson|publisher=Blackwell Scientific Publications|location=Oxford|year=1997|url=http://goldbook.iupac.org/P04524.html|isbn=978-0-9678550-9-7|doi=10.1351/goldbook}}</ref> The activity of hydronium ions in neutral water is 10<sup>−7</sup>&nbsp;[[molar concentration|mol·L<sup>−1</sup>]], hence a pH of 7. Vinegar typically has a pH of about 3. The difference of 4 corresponds to a ratio of 10<sup>4</sup> of the activity, that is, vinegar's hydronium ion activity is about {{math|10<sup>−3</sup> mol·L<sup>−1</sup>}}.\n\n[[Semi-log plot|Semilog]] (log-linear) graphs use the logarithmic scale concept for visualization: one axis, typically the vertical one, is scaled logarithmically. For example, the chart at the right compresses the steep increase from 1 million to 1 trillion to the same space (on the vertical axis) as the increase from 1 to 1 million. In such graphs, [[exponential function]]s of the form {{math|1=''f''(''x'') = ''a'' · {{mvar|b}}<sup>''x''</sup>}} appear as straight lines with [[slope]] equal to the logarithm of {{mvar|b}}.\n[[Log-log plot|Log-log]] graphs scale both axes logarithmically, which causes functions of the form {{math|1=''f''(''x'') = ''a'' · {{mvar|x}}<sup>''k''</sup>}} to be depicted as straight lines with slope equal to the exponent ''k''. This is applied in visualizing and analyzing [[power law]]s.<ref>{{Citation|last1=Bird|first1=J.O.|title=Newnes engineering mathematics pocket book  |publisher=Newnes|location=Oxford|edition=3rd|isbn=978-0-7506-4992-6|year=2001}}, section 34</ref>\n\n===Psychology===\nLogarithms occur in several laws describing [[human perception]]:<ref>{{Citation | last1=Goldstein | first1=E. Bruce | title=Encyclopedia of Perception | url={{google books |plainurl=y |id=Y4TOEN4f5ZMC}} | publisher=Sage | location=Thousand Oaks, CA | series=Encyclopedia of Perception | isbn=978-1-4129-4081-8 | year=2009}}, pp.&nbsp;355–56</ref><ref>{{Citation | last1=Matthews | first1=Gerald | title=Human performance: cognition, stress, and individual differences | url={{google books |plainurl=y |id=0XrpulSM1HUC}} | publisher=Psychology Press | location=Hove | series=Human Performance: Cognition, Stress, and Individual Differences | isbn=978-0-415-04406-6 | year=2000}}, p.&nbsp;48</ref>\n[[Hick's law]] proposes a logarithmic relation between the time individuals take to choose an alternative and the number of choices they have.<ref>{{Citation|last1=Welford|first1=A.T.|title=Fundamentals of skill|publisher=Methuen|location=London|isbn=978-0-416-03000-6 |oclc=219156|year=1968}}, p.&nbsp;61</ref> [[Fitts's law]] predicts that the time required to rapidly move to a target area is a logarithmic function of the distance to and the size of the target.<ref>{{Citation|author=Paul M. Fitts|date=June 1954|title=The information capacity of the human motor system in controlling the amplitude of movement|journal=Journal of Experimental Psychology|volume=47|issue=6|pages=381–91 | pmid=13174710 | doi =10.1037/h0055392 }}, reprinted in {{Citation|journal=Journal of Experimental Psychology: General|volume=121|issue=3|pages=262–69|year=1992 | pmid=1402698 | url=http://sing.stanford.edu/cs303-sp10/papers/1954-Fitts.pdf | format=PDF | accessdate=30 March 2011 |title=The information capacity of the human motor system in controlling the amplitude of movement|author=Paul M. Fitts|doi=10.1037/0096-3445.121.3.262}}</ref> In [[psychophysics]], the [[Weber–Fechner law]] proposes a logarithmic relationship between [[stimulus (psychology)|stimulus]] and [[sensation (psychology)|sensation]] such as the actual vs. the perceived weight of an item a person is carrying.<ref>{{Citation | last1=Banerjee | first1=J.C. | title=Encyclopaedic dictionary of psychological terms | publisher=M.D. Publications | location=New Delhi | isbn=978-81-85880-28-0  | oclc=33860167 | year=1994|url={{google books |plainurl=y |id=Pwl5U2q5hfcC|page=306}} |page=304}}</ref> (This \"law\", however, is less realistic than more recent models, such as [[Stevens's power law]].<ref>{{Citation|last1=Nadel|first1=Lynn|author1-link=Lynn Nadel|title=Encyclopedia of cognitive science|publisher=[[John Wiley & Sons]]|location=New York|isbn=978-0-470-01619-0|year=2005}}, lemmas ''Psychophysics'' and ''Perception: Overview''</ref>)\n\nPsychological studies found that individuals with little mathematics education tend to estimate quantities logarithmically, that is, they position a number on an unmarked line according to its logarithm, so that 10 is positioned as close to 100 as 100 is to 1000. Increasing education shifts this to a linear estimate (positioning 1000 10x as far away) in some circumstances, while logarithms are used when the numbers to be plotted are difficult to plot linearly.<ref>\n{{Citation|doi=10.1111/1467-9280.02438|last1=Siegler|first1=Robert S.|last2=Opfer|first2=John E.|title=The Development of Numerical Estimation. Evidence for Multiple Representations of Numerical Quantity|volume=14|issue=3|pages=237–43|year=2003|journal=Psychological Science|url=http://www.psy.cmu.edu/~siegler/sieglerbooth-cd04.pdf|pmid=12741747|citeseerx=10.1.1.727.3696|access-date=7 January 2011|archive-url=https://web.archive.org/web/20110517002232/http://www.psy.cmu.edu/~siegler/sieglerbooth-cd04.pdf|archive-date=17 May 2011|dead-url=yes|df=dmy-all}}\n</ref><ref>{{Citation|last1=Dehaene| first1=Stanislas|last2=Izard|first2=Véronique |last3=Spelke| first3=Elizabeth|last4=Pica| first4=Pierre| title=Log or Linear? Distinct Intuitions of the Number Scale in Western and Amazonian Indigene Cultures|volume=320|issue=5880|pages=1217–20|doi=10.1126/science.1156540|pmc=2610411|pmid=18511690| year=2008|journal=Science|postscript=<!--None-->|bibcode=2008Sci...320.1217D| citeseerx=10.1.1.362.2390}}</ref>\n\n===Probability theory and statistics===\n[[File:PDF-log normal distributions.svg|thumb|right|alt=Three asymmetric PDF curves|Three [[probability density function]]s (PDF) of random variables with log-normal distributions.  The location parameter {{math|μ}}, which is zero for all three of the PDFs shown, is the mean of the logarithm of the random variable,  not the mean of the variable itself.]]\n[[File:Benfords law illustrated by world's countries population.png|Distribution of first digits (in %, red bars) in the [[List of countries by population|population of the 237 countries]] of the world. Black dots indicate the distribution predicted by Benford's law.|thumb|right|alt=A bar chart and a superimposed second chart. The two differ slightly, but both decrease in a similar fashion.]]\nLogarithms arise in [[probability theory]]: the [[law of large numbers]] dictates that, for a [[fair coin]],  as the number of coin-tosses increases to infinity, the observed proportion of heads [[binomial distribution|approaches one-half]]. The fluctuations of this proportion about one-half are described by the [[law of the iterated logarithm]].<ref>{{Citation | last1=Breiman | first1=Leo | title=Probability | publisher=[[Society for Industrial and Applied Mathematics]] | location=Philadelphia | series=Classics in applied mathematics | isbn=978-0-89871-296-4 | year=1992}}, section 12.9</ref>\n\nLogarithms also occur in [[log-normal distribution]]s. When the logarithm of a [[random variable]] has a [[normal distribution]], the variable is said to have a log-normal distribution.<ref>{{Citation|last1=Aitchison|first1=J.|last2=Brown|first2=J.A.C.|title=The lognormal distribution|publisher=[[Cambridge University Press]]|isbn=978-0-521-04011-2 |oclc=301100935|year=1969}}</ref> Log-normal distributions are encountered in many fields, wherever a variable is formed as the product of many independent positive random variables, for example in the study of turbulence.<ref>\n{{Citation\n | title = An introduction to turbulent flow\n | author = Jean Mathieu and Julian Scott\n | publisher = Cambridge University Press\n | year = 2000\n | isbn = 978-0-521-77538-0\n | page = 50\n | url = {{google books |plainurl=y |id=nVA53NEAx64C|page=50}}\n }}</ref>\n\nLogarithms are used for [[maximum-likelihood estimation]] of parametric [[statistical model]]s. For such a model, the [[likelihood function]] depends on at least one [[parametric model|parameter]] that must be estimated.  A maximum of the likelihood function occurs at the same parameter-value as a maximum of the logarithm of the likelihood (the \"''log&nbsp;likelihood''\"), because the logarithm is an increasing function. The log-likelihood is easier to maximize, especially for the multiplied likelihoods for [[independence (probability)|independent]] random variables.<ref>{{Citation|last1=Rose|first1=Colin|last2=Smith|first2=Murray D.|title=Mathematical statistics with Mathematica|publisher=[[Springer-Verlag]]|location=Berlin, New York|series=Springer texts in statistics|isbn=978-0-387-95234-5|year=2002}}, section 11.3</ref>\n\n[[Benford's law]] describes the occurrence of digits in many [[data set]]s, such as heights of buildings. According to Benford's law, the probability that the first decimal-digit of an item in the data sample is ''d'' (from 1 to 9) equals {{math|log<sub>10</sub>(''d'' + 1) − log<sub>10</sub>(''d'')}}, ''regardless'' of the unit of measurement.<ref>{{Citation|last1=Tabachnikov|first1=Serge|authorlink1=Sergei Tabachnikov|title=Geometry and Billiards|publisher=[[American Mathematical Society]]|location=Providence, RI|isbn=978-0-8218-3919-5|year=2005|pages=36–40}}, section 2.1</ref> Thus, about 30% of the data can be expected to have 1 as first digit, 18% start with 2, etc. Auditors examine deviations from Benford's law to detect fraudulent accounting.<ref>{{Cite journal |title=The Effective Use of Benford's Law in Detecting Fraud in Accounting Data |first=Cindy |last=Durtschi |first2=William |last2=Hillison |first3=Carl |last3=Pacini |url=http://faculty.usfsp.edu/gkearns/Articles_Fraud/Benford%20Analysis%20Article.pdf |volume=V |pages=17–34 |year=2004 |journal=Journal of Forensic Accounting |archive-url=https://web.archive.org/web/20170829062510/http://faculty.usfsp.edu/gkearns/Articles_Fraud/Benford%20Analysis%20Article.pdf |archive-date=29 August 2017 |access-date=28 May 2018}}</ref>\n\n===Computational complexity===\n[[Analysis of algorithms]] is a branch of [[computer science]] that studies the [[time complexity|performance]] of [[algorithm]]s (computer programs solving a certain problem).<ref name=Wegener>{{Citation|last1=Wegener|first1=Ingo| title=Complexity theory: exploring the limits of efficient algorithms|publisher=[[Springer-Verlag]]|location=Berlin, New York|isbn=978-3-540-21045-0|year=2005}}, pp. 1–2</ref> Logarithms are valuable for describing algorithms that [[Divide and conquer algorithm|divide a problem]] into smaller ones, and join the solutions of the subproblems.<ref>{{Citation|last1=Harel|first1=David|last2=Feldman|first2=Yishai A.|title=Algorithmics: the spirit of computing|location=New York|publisher=[[Addison-Wesley]]|isbn=978-0-321-11784-7|year=2004}}, p.&nbsp;143</ref>\n\nFor example, to find a number in a sorted list, the [[binary search algorithm]] checks the middle entry and proceeds with the half before or after the middle entry if the number is still not found. This algorithm requires, on average, {{math|log<sub>2</sub>(''N'')}} comparisons, where ''N'' is the list's length.<ref>{{citation  | last = Knuth  | first = Donald  | authorlink = Donald Knuth  | title = The Art of Computer Programming  | publisher = Addison-Wesley  |location=Reading, MA | year=  1998| isbn = 978-0-201-89685-5 | title-link = The Art of Computer Programming  }}, section 6.2.1, pp. 409–26</ref> Similarly, the [[merge sort]] algorithm sorts an unsorted list by dividing the list into halves and sorting these first before merging the results. Merge sort algorithms typically require a time [[big O notation|approximately proportional to]] {{math|''N'' · log(''N'')}}.<ref>{{Harvard citations|last = Knuth  | first = Donald|year=1998|loc=section 5.2.4, pp. 158–68|nb=yes}}</ref> The base of the logarithm is not specified here, because the result only changes by a constant factor when another base is used. A constant factor is usually disregarded in the analysis of algorithms under the standard [[uniform cost model]].<ref name=Wegener20>{{Citation|last1=Wegener|first1=Ingo| title=Complexity theory: exploring the limits of efficient algorithms|publisher=[[Springer-Verlag]]|location=Berlin, New York|isbn=978-3-540-21045-0|year=2005|page=20}}</ref>\n\nA function {{math|''f''(''x'')}} is said to [[Logarithmic growth|grow logarithmically]] if {{math|''f''(''x'')}} is (exactly or approximately) proportional to the logarithm of {{mvar|x}}. (Biological descriptions of organism growth, however, use this term for an exponential function.<ref>{{Citation |last1=Mohr|first1=Hans|last2=Schopfer|first2=Peter|title=Plant physiology|publisher=Springer-Verlag|location=Berlin, New York|isbn=978-3-540-58016-4|year=1995}}, chapter 19, p.&nbsp;298</ref>) For example, any [[natural number]] ''N'' can be represented in [[Binary numeral system|binary form]] in no more than {{math|log<sub>2</sub>(''N'') + 1}} [[bit]]s. In other words, the amount of [[memory (computing)|memory]] needed to store ''N'' grows logarithmically with ''N''.\n\n===Entropy and chaos===\n[[File:Chaotic Bunimovich stadium.png|right|thumb|[[Dynamical billiards|Billiards]] on an oval [[billiard table]]. Two particles, starting at the center with an angle differing by one degree, take paths that diverge chaotically because of [[reflection (physics)|reflections]] at the boundary.|alt=An oval shape with the trajectories of two particles.]]\n\n[[Entropy]] is broadly a measure of the disorder of some system. In [[statistical thermodynamics]], the entropy ''S'' of some physical system is defined as\n:<math> S = - k \\sum_i p_i \\ln(p_i).\\, </math>\nThe sum is over all possible states ''i'' of the system in question, such as the positions of gas particles in a container. Moreover, {{math|''p''<sub>''i''</sub>}} is the probability that the state ''i'' is attained and ''k'' is the [[Boltzmann constant]]. Similarly, [[entropy (information theory)|entropy in information theory]] measures the quantity of information. If a message recipient may expect any one of ''N'' possible messages with equal likelihood, then the amount of information conveyed by any one such message is quantified as {{math|log<sub>2</sub>(''N'')}} bits.<ref>{{Citation|last1=Eco|first1=Umberto|author1-link=Umberto Eco|title=The open work  |publisher=[[Harvard University Press]]|isbn=978-0-674-63976-8|year=1989}}, section III.I</ref>\n\n[[Lyapunov exponent]]s use logarithms to gauge the degree of chaoticity of a [[dynamical system]]. For example, for a particle moving on an oval billiard table, even small changes of the initial conditions result in very different paths of the particle. Such systems are [[chaos theory|chaotic]] in a [[Deterministic system|deterministic]] way, because small measurement errors of the initial state predictably lead to largely different final states.<ref>{{Citation | last1=Sprott | first1=Julien Clinton | title=Elegant Chaos: Algebraically Simple Chaotic Flows | journal=Elegant Chaos: Algebraically Simple Chaotic Flows. Edited by Sprott Julien Clinton. Published by World Scientific Publishing Co. Pte. Ltd | url={{google books |plainurl=y |id=buILBDre9S4C}} | publisher=[[World Scientific]] |location=New Jersey|isbn=978-981-283-881-0| year=2010| bibcode=2010ecas.book.....S | doi=10.1142/7183 }}, section 1.9</ref> At least one Lyapunov exponent of a deterministically chaotic system is positive.\n\n===Fractals===\n[[File:Sierpinski dimension.svg|The Sierpinski triangle (at the right) is constructed by repeatedly replacing [[equilateral triangle]]s by three smaller ones.|right|thumb|400px|alt=Parts of a triangle are removed in an iterated way.]]\n\nLogarithms occur in definitions of the [[fractal dimension|dimension]] of [[fractal]]s.<ref>{{Citation|last1=Helmberg|first1=Gilbert|title=Getting acquainted with fractals|publisher=Walter de Gruyter|series=De Gruyter Textbook|location=Berlin, New York|isbn=978-3-11-019092-2|year=2007}}</ref> Fractals are geometric objects that are [[self-similarity|self-similar]]: small parts reproduce, at least roughly, the entire global structure. The [[Sierpinski triangle]] (pictured) can be covered by three copies of itself, each having sides half the original length. This makes the [[Hausdorff dimension]] of this structure {{math|1=ln(3)/ln(2) ≈ 1.58}}. Another logarithm-based notion of dimension is obtained by [[box-counting dimension|counting the number of boxes]] needed to cover the fractal in question.\n\n===Music===\n{{multiple image\n| direction = vertical\n| width     = 350\n| footer    = Four different octaves shown on a linear scale, then shown on a logarithmic scale (as the ear hears them).\n| image1    = 4octavesAndfrequencies.jpg\n| alt1      = Four different octaves shown on a linear scale.\n| image2    = 4octavesAndfrequenciesEars.jpg\n| alt2      = Four different octaves shown on a logarithmic scale.\n}}\n\nLogarithms are related to musical tones and [[interval (music)|intervals]]. In [[equal temperament]], the frequency ratio depends only on the interval between two tones, not on the specific frequency, or [[pitch (music)|pitch]], of the individual tones. For example, the [[a (musical note)|note ''A'']] has a frequency of 440 [[Hertz|Hz]] and [[B♭ (musical note)|''B-flat'']] has a frequency of 466&nbsp;Hz. The interval between ''A'' and ''B-flat'' is a [[semitone]], as is the one between ''B-flat'' and [[b (musical note)|''B'']] (frequency 493&nbsp;Hz). Accordingly, the frequency ratios agree:\n:<math>\\frac{466}{440} \\approx \\frac{493}{466} \\approx 1.059 \\approx \\sqrt[12]2.</math>\nTherefore, logarithms can be used to describe the intervals: an interval is measured in semitones by taking the {{math|base-2<sup>1/12</sup>}} logarithm of the [[frequency]] ratio, while the {{math|base-2<sup>1/1200</sup>}} logarithm of the frequency ratio expresses the interval in [[cent (music)|cents]], hundredths of a semitone. The latter is used for finer encoding, as it is needed for non-equal temperaments.<ref>{{Citation|last1=Wright|first1=David|title=Mathematics and music|location=Providence, RI|publisher=AMS Bookstore|isbn=978-0-8218-4873-9|year=2009}}, chapter 5</ref>\n\n{| class=\"wikitable\" style=\"text-align:center; margin:1em auto 1em auto;\"\n|-\n||'''Interval'''<br />(the two tones are played at the same time)\n||[[72 tone equal temperament|1/12 tone]] {{audio|1_step_in_72-et_on_C.mid|play}}\n||[[Semitone]] {{audio|help=no|Minor_second_on_C.mid|play}}\n||[[Just major third]] {{audio|help=no|Just_major_third_on_C.mid|play}}\n||[[Major third]] {{audio|help=no|Major_third_on_C.mid|play}}\n||[[Tritone]] {{audio|help=no|Tritone_on_C.mid|play}}\n||[[Octave]] {{audio|help=no|Perfect_octave_on_C.mid|play}}\n|-\n|| '''Frequency ratio''' ''r''\n|| <math>2^{\\frac 1 {72}} \\approx 1.0097</math>\n|| <math>2^{\\frac 1 {12}} \\approx 1.0595</math>\n|| <math>\\tfrac 5 4 = 1.25</math>\n|| <math>\\begin{align} 2^{\\frac 4 {12}} & = \\sqrt[3] 2 \\\\ & \\approx 1.2599 \\end{align} </math>\n|| <math>\\begin{align} 2^{\\frac 6 {12}} & = \\sqrt 2 \\\\ & \\approx 1.4142 \\end{align} </math>\n|| <math> 2^{\\frac {12} {12}} = 2 </math>\n|-\n|| '''Corresponding number of semitones'''<br /><math>\\log_{\\sqrt[12] 2}(r) = 12 \\log_2 (r)</math>\n|| <math>\\tfrac 1 6 \\,</math>\n|| <math>1 \\,</math>\n|| <math>\\approx 3.8631 \\,</math>\n|| <math>4 \\,</math>\n|| <math>6 \\,</math>\n|| <math>12 \\,</math>\n|-\n|| '''Corresponding number of cents'''<br /><math>\\log_{\\sqrt[1200] 2}(r) = 1200 \\log_2 (r)</math>\n|| <math>16 \\tfrac 2 3 \\,</math>\n|| <math>100 \\,</math>\n|| <math>\\approx 386.31 \\,</math>\n|| <math>400 \\,</math>\n|| <math>600 \\,</math>\n|| <math>1200 \\,</math>\n|}\n\n===Number theory===\n[[Natural logarithm]]s are closely linked to [[prime-counting function|counting prime numbers]] (2, 3, 5, 7, 11, ...), an important topic in [[number theory]]. For any [[integer]] {{mvar|x}}, the quantity of [[prime number]]s less than or equal to {{mvar|x}} is denoted [[prime-counting function|{{math|{{pi}}(''x'')}}]]. The [[prime number theorem]] asserts that {{math|{{pi}}(''x'')}} is approximately given by\n:<math>\\frac{x}{\\ln(x)},</math>\nin the sense that the ratio of {{math|{{pi}}(''x'')}} and that fraction approaches 1 when {{mvar|x}} tends to infinity.<ref>{{Citation|last1=Bateman|first1=P.T.|last2=Diamond|first2=Harold G.|title=Analytic number theory: an introductory course|publisher=[[World Scientific]]|location=New Jersey|isbn=978-981-256-080-3 |oclc=492669517|year=2004}}, theorem 4.1</ref> As a consequence, the probability that a randomly chosen number between 1 and {{mvar|x}} is prime is inversely [[proportionality (mathematics)|proportional]] to the number of decimal digits of {{mvar|x}}. A far better estimate of {{math|{{pi}}(''x'')}} is given by the\n[[logarithmic integral function|offset logarithmic integral]] function {{math|Li(''x'')}}, defined by\n:<math> \\mathrm{Li}(x) = \\int_2^x \\frac1{\\ln(t)} \\,dt.  </math>\nThe [[Riemann hypothesis]], one of the oldest open mathematical [[conjecture]]s, can be stated in terms of comparing {{math|{{pi}}(''x'')}} and {{math|Li(''x'')}}.<ref>{{Harvard citations|last1=Bateman|first1=P. T.|last2=Diamond|year=2004|nb=yes |loc=Theorem 8.15}}</ref> The [[Erdős–Kac theorem]] describing the number of distinct [[prime factor]]s also involves the [[natural logarithm]].\n\nThe logarithm of ''n'' [[factorial]], {{math|1=''n''! = 1 · 2 · ... · ''n''}}, is given by\n:<math> \\ln (n!) = \\ln (1) + \\ln (2) + \\cdots + \\ln (n). \\,</math>\nThis can be used to obtain [[Stirling's formula]], an approximation of {{math|''n''!}} for large ''n''.<ref>{{Citation|last1=Slomson|first1=Alan B.|title=An introduction to combinatorics|publisher=[[CRC Press]]|location=London|isbn=978-0-412-35370-3|year=1991}}, chapter 4</ref>\n\n==Generalizations==\n\n===Complex logarithm===\n{{Main|Complex logarithm}}\n[[File:Complex number illustration multiple arguments.svg|thumb|right|Polar form of {{math|''z {{=}} x + iy''}}. Both {{mvar|φ}} and {{mvar|φ'}} are arguments of {{mvar|z}}.|alt=An illustration of the polar form: a point is described by an arrow or equivalently by its length and angle to the {{mvar|x}} axis.]]\n\nAll the [[complex number]]s {{mvar|a}} that solve the equation\n\n:<math>e^a=z</math>\n\nare called ''complex logarithms'' of {{mvar|z}}, when {{mvar|z}} is (considered as) a complex number. A complex number is commonly represented as {{math|''z {{=}} x + iy''}}, where {{mvar|x}} and {{mvar|y}} are real numbers and {{mvar|i}} is an [[imaginary unit]], the square of which is −1. Such a number can be visualized by a point in the [[complex plane]], as shown at the right. The [[polar form]] encodes a non-zero complex number {{mvar|z}} by its [[absolute value]], that is, the (positive, real) distance {{math|r}} to the [[origin (mathematics)|origin]], and an angle between the real ({{mvar|x}}) axis ''Re'' and the line passing through both the origin and {{mvar|z}}. This angle is called the [[Argument (complex analysis)|argument]] of {{mvar|z}}.\n\nThe absolute value {{mvar|r}} of {{mvar|z}} is given by\n\n:<math>\\textstyle r=\\sqrt{x^2+y^2}.</math>\n\nUsing the geometrical interpretation of <math>\\sin</math> and <math>\\cos</math> and their periodicity in <math>2\\pi,</math> any complex number {{mvar|z}} may be denoted as\n\n:<math>z = x + iy = r (\\cos \\varphi + i \\sin \\varphi )= r (\\cos (\\varphi + 2k\\pi) + i \\sin (\\varphi + 2k\\pi)),</math>\n\nfor any integer number {{mvar|k}}. Evidently the argument of {{mvar|z}} is not uniquely specified: both {{mvar|φ}} and {{mvar|φ}}' = {{mvar|φ}} + 2''k''{{pi}} are valid arguments of {{mvar|z}} for all integers {{mvar|k}}, because adding 2''k''{{pi}} [[radian]] or ''k''⋅360°{{refn|See [[radian]] for the conversion between 2[[pi|{{pi}}]] and 360 [[degree (angle)|degree]].|group=nb}} to {{mvar|φ}} corresponds to \"winding\" around the origin counter-clock-wise by {{mvar|k}} [[Turn (geometry)|turns]]. The resulting complex number is always {{mvar|z}}, as illustrated at the right for {{math|''k'' {{=}} 1}}. One may select exactly one of the possible arguments of {{mvar|z}} as the so-called ''principal argument'', denoted {{math|Arg(''z'')}}, with a capital {{math|A}}, by requiring {{mvar|φ}} to belong to one, conveniently selected turn, e.g., <math>-\\pi < \\varphi \\le \\pi</math><ref>{{Citation|last1=Ganguly|location=Kolkata|first1=S.|title=Elements of Complex Analysis|publisher=Academic Publishers|isbn=978-81-87504-86-3|year=2005}}, Definition 1.6.3</ref> or  <math>0 \\le \\varphi < 2\\pi.</math><ref>{{Citation|last1=Nevanlinna|first1=Rolf Herman|author1-link=Rolf Nevanlinna|last2=Paatero|first2=Veikko|title=Introduction to complex analysis|journal=London: Hilger|location=Providence, RI|publisher=AMS Bookstore|isbn=978-0-8218-4399-4|year=2007|bibcode=1974aitc.book.....W}}, section 5.9</ref> These regions, where the argument of {{mvar|z}} is uniquely determined are called [[principal branch|''branches'']] of the argument function.\n\n[[File:Complex log.jpg|right|thumb|The principal branch (-{{pi}}, {{pi}}]] of the complex logarithm, {{math|Log(''z'')}}. The black point at {{math|''z'' {{=}} 1}} corresponds to absolute value zero and brighter (more [[saturation (color theory)|saturated]]) colors refer to bigger absolute values. The [[hue]] of the color encodes the argument of {{math|Log(''z'')}}.|alt=A density plot. In the middle there is a black point, at the negative axis the hue jumps sharply and evolves smoothly otherwise.]]\n\n[[Euler's formula]] connects the [[trigonometric functions]] [[sine]] and [[cosine]] to the [[complex exponential]]:\n:<math>e^{i\\varphi} = \\cos \\varphi + i\\sin \\varphi .</math>\n\nUsing this formula, and again the periodicity, the following identities hold:<ref>{{Citation|last1=Moore|first1=Theral Orvis|last2=Hadlock|first2=Edwin H.|title=Complex analysis|publisher=[[World Scientific]]|location=Singapore|isbn=978-981-02-0246-0|year=1991}}, section 1.2</ref>\n\n:<math> \\begin{array}{lll}z& = & r \\left (\\cos \\varphi + i \\sin \\varphi\\right) \\\\\n& = & r \\left (\\cos(\\varphi + 2k\\pi) + i \\sin(\\varphi + 2k\\pi)\\right) \\\\\n& = & r e^{i (\\varphi + 2k\\pi)} \\\\\n& = & e^{\\ln(r)} e^{i (\\varphi + 2k\\pi)}  \\\\\n& = & e^{\\ln(r) + i(\\varphi + 2k\\pi)} = e^{a_k},\n\\end{array}\n</math>\n\nwhere {{math|ln(''r'')}} is the unique real natural logarithm, {{math|''a''<sub>''k''</sub>}} denote the complex logarithms of {{mvar|z}}, and {{mvar|k}} is an arbitrary integer. Therefore, the complex logarithms of {{mvar|z}}, which are all those complex values {{math|''a''<sub>''k''</sub>}} for which the {{math|''a''<sub>''k''</sub>-th}} power of {{mvar|e}} equals {{mvar|z}}, are the infinitely many values\n\n:<math>a_k = \\ln (r) + i ( \\varphi + 2 k \\pi ),\\quad</math> for arbitrary integers {{mvar|k}}.\n\nTaking {{mvar|k}} such that <math>\\varphi + 2 k \\pi</math> is within the defined interval for the principal arguments, then {{math|''a''<sub>''k''</sub>}} is called the ''principal value'' of the logarithm, denoted {{math|Log(''z'')}}, again with a capital {{math|L}}. The principal argument of any positive real number {{mvar|x}} is 0; hence {{math|Log(''x'')}} is a real number and equals the real (natural) logarithm. However, the above formulas for logarithms of products and powers [[Exponentiation#Failure of power and logarithm identities|do ''not'' generalize]] to the principal value of the complex logarithm.<ref>{{Citation | last1=Wilde | first1=Ivan Francis | title=Lecture notes on complex analysis | publisher=Imperial College Press | location=London | isbn=978-1-86094-642-4 | year=2006|url=https://books.google.com/?id=vrWES2W6vG0C&pg=PA97&dq=complex+logarithm#v=onepage&q=complex%20logarithm&f=false}}, theorem 6.1.</ref>\n\nThe illustration at the right depicts {{math|Log(''z'')}}, confining the arguments of {{mvar|z}} to the interval {{math|(-{{pi}}, {{pi}}]}}. This way the corresponding branch of the complex logarithm has discontinuities all along the negative real {{mvar|x}} axis, which can be seen in the jump in the hue there. This discontinuity arises from jumping to the other boundary in the same branch, when crossing a boundary, i.e., not changing to the corresponding {{mvar|k}}-value of the continuously neighboring branch. Such a locus is called a [[branch cut]]. Dropping the range restrictions on the argument makes the relations \"argument of {{mvar|z}}\", and consequently the \"logarithm of {{mvar|z}}\", [[multi-valued function]]s.\n\n===Inverses of other exponential functions===\nExponentiation occurs in many areas of mathematics and its inverse function is often referred to as the logarithm. For example, the [[logarithm of a matrix]] is the (multi-valued) inverse function of the [[matrix exponential]].<ref>{{Citation|last1=Higham|first1=Nicholas|author1-link=Nicholas Higham|title=Functions of Matrices. Theory and Computation|location=Philadelphia, PA|publisher=[[Society for Industrial and Applied Mathematics|SIAM]]|isbn=978-0-89871-646-7|year=2008}}, chapter 11.</ref> Another example is the [[p-adic logarithm function|''p''-adic logarithm]], the inverse function of the [[p-adic exponential function|''p''-adic exponential]]. Both are defined via Taylor series analogous to the real case.<ref>{{Neukirch ANT}}, section II.5.</ref> In the context of [[differential geometry]], the [[exponential map (Riemannian geometry)|exponential map]] maps the [[tangent space]] at a point of a [[differentiable manifold|manifold]] to a [[neighborhood (mathematics)|neighborhood]] of that point. Its inverse is also called the logarithmic (or log) map.<ref>{{Citation|last1=Hancock|first1=Edwin R.|last2=Martin|first2=Ralph R.|last3=Sabin|first3=Malcolm A.|title=Mathematics of Surfaces XIII: 13th IMA International Conference York, UK, September 7–9, 2009 Proceedings|url=https://books.google.com/books?id=0cqCy9x7V_QC&pg=PA379|publisher=Springer|year=2009|page=379|isbn=978-3-642-03595-1}}</ref>\n\nIn the context of [[finite groups]] exponentiation is given by repeatedly multiplying one group element {{mvar|b}} with itself. The [[discrete logarithm]] is the integer ''n'' solving the equation\n:<math>b^n = x,\\,</math>\nwhere {{mvar|x}} is an element of the group. Carrying out the exponentiation can be done efficiently, but the discrete logarithm is believed to be very hard to calculate in some groups. This asymmetry has important applications in [[public key cryptography]], such as for example in the [[Diffie–Hellman key exchange]], a routine that allows secure exchanges of [[cryptography|cryptographic]] keys over unsecured information channels.<ref>{{Citation|last1=Stinson|first1=Douglas Robert|title=Cryptography: Theory and Practice|publisher=[[CRC Press]]|location=London|edition=3rd|isbn=978-1-58488-508-5|year=2006}}</ref> [[Zech's logarithm]] is related to the discrete logarithm in the multiplicative group of non-zero elements of a [[finite field]].<ref>{{Citation|last1=Lidl|first1=Rudolf|last2=Niederreiter|first2=Harald|author2-link = Harald Niederreiter |title=Finite fields|publisher=Cambridge University Press|isbn=978-0-521-39231-0|year=1997}}</ref>\n\n{{anchor|double logarithm}}Further logarithm-like inverse functions include the ''double logarithm'' ln(ln(''x'')), the ''[[super-logarithm|super- or hyper-4-logarithm]]'' (a slight variation of which is called [[iterated logarithm]] in computer science), the [[Lambert W function]], and the [[logit]]. They are the inverse functions of the [[double exponential function]], [[tetration]], of {{math|''f''(''w'') {{=}} ''we<sup>w</sup>''}},<ref>{{Citation | last1=Corless | first1=R. | last2=Gonnet | first2=G. | last3=Hare | first3=D. | last4=Jeffrey | first4=D. | last5=Knuth | first5=Donald | author5-link=Donald Knuth | title=On the Lambert ''W'' function | url=http://www.apmaths.uwo.ca/~djeffrey/Offprints/W-adv-cm.pdf | year=1996 | journal=Advances in Computational Mathematics | issn=1019-7168 | volume=5 | pages=329–59 | doi=10.1007/BF02124750 | access-date=13 February 2011 | archive-url=https://web.archive.org/web/20101214110615/http://www.apmaths.uwo.ca/~djeffrey/Offprints/W-adv-cm.pdf | archive-date=14 December 2010 | dead-url=yes | df=dmy-all }}</ref> and of the [[logistic function]], respectively.<ref>{{Citation | last1=Cherkassky | first1=Vladimir | last2=Cherkassky | first2=Vladimir S. | last3=Mulier | first3=Filip | title=Learning from data: concepts, theory, and methods | publisher=[[John Wiley & Sons]] | location=New York | series=Wiley series on adaptive and learning systems for signal processing, communications, and control | isbn=978-0-471-68182-3 | year=2007}}, p.&nbsp;357</ref>\n\n===Related concepts===\nFrom the perspective of [[group theory]], the identity {{math|log(''cd'') {{=}} log(''c'') + log(''d'')}} expresses a [[group isomorphism]] between positive [[real number|reals]] under multiplication and reals under addition. Logarithmic functions are the only continuous isomorphisms between these groups.<ref>{{Citation|last1=Bourbaki|first1=Nicolas|author1-link=Nicolas Bourbaki|title=General topology. Chapters 5–10|publisher=[[Springer-Verlag]]|location=Berlin, New York|series=Elements of Mathematics|isbn=978-3-540-64563-4|mr=1726872|year=1998}}, section V.4.1</ref> By means of that isomorphism, the [[Haar measure]] ([[Lebesgue measure]]) ''dx'' on the reals corresponds to the Haar measure {{math|''dx''/''x''}} on the positive reals.<ref>{{Citation|last1=Ambartzumian|first1=R.V.|authorlink=Rouben V. Ambartzumian|title=Factorization calculus and geometric probability|publisher=[[Cambridge University Press]]|isbn=978-0-521-34535-4|year=1990}}, section 1.4</ref> The non-negative reals not only have a multiplication, but also have addition, and form a [[semiring]], called the [[probability semiring]]; this is in fact a [[semifield]]. The logarithm then takes multiplication to addition (log multiplication), and takes addition to log addition ([[LogSumExp]]), giving an [[isomorphism]] of semirings between the probability semiring and the [[log semiring]].\n\n[[logarithmic form|Logarithmic one-forms]] {{math|''df''/''f''}} appear in [[complex analysis]] and [[algebraic geometry]] as [[differential form]]s with logarithmic [[Pole (complex analysis)|poles]].<ref>{{Citation|last1=Esnault|first1=Hélène|last2=Viehweg|first2=Eckart|title=Lectures on vanishing theorems|location=Basel, Boston|publisher=Birkhäuser Verlag|series=DMV Seminar|isbn=978-3-7643-2822-1|mr=1193913|year=1992|volume=20|doi=10.1007/978-3-0348-8600-0|citeseerx=10.1.1.178.3227}}, section 2</ref>\n\nThe [[polylogarithm]] is the function defined by\n:<math>\n\\operatorname{Li}_s(z) = \\sum_{k=1}^\\infty {z^k \\over k^s}.\n</math>\nIt is related to the [[natural logarithm]] by {{math|1=Li<sub>1</sub>(''z'') = −ln(1 − ''z'')}}. Moreover, {{math|Li<sub>''s''</sub>(1)}} equals the [[Riemann zeta function]] {{math|ζ(''s'')}}.<ref>{{dlmf|id= 25.12|first= T.M.|last= Apostol|ref= harv}}</ref>\n\n==See also==\n* [[Cologarithm]]\n* [[Decimal exponent]] (dex)\n* [[Exponential function]]\n* [[Index of logarithm articles]]\n\n==Notes==\n{{reflist|group=nb|30em}}\n\n==References==\n{{Reflist}}\n\n==External links==\n* {{Commons category inline}}\n* {{Wiktionary-inline}}\n* [https://web.archive.org/web/20121218200616/http://www.khanacademy.org/math/algebra/logarithms-tutorial Khan Academy: Logarithms, free online micro lectures]\n* {{springer|title=Logarithmic function|id=p/l060600}}\n* {{Citation|author=Colin Byfleet|url=http://mediasite.oddl.fsu.edu/mediasite/Viewer/?peid=003298f9a02f468c8351c50488d6c479|title=Educational video on logarithms|accessdate=2010-10-12}}\n* {{Citation|author=Edward Wright |url=http://www.johnnapier.com/table_of_logarithms_001.htm |title=Translation of Napier's work on logarithms |accessdate=2010-10-12 |deadurl=unfit |archiveurl=https://web.archive.org/web/20021203005508/http://www.johnnapier.com/table_of_logarithms_001.htm |archivedate=3 December 2002 }}\n* {{Cite EB1911|wstitle=Logarithm |volume=16 |pages=868–77 |first=James Whitbread Lee |last=Glaisher}}\n{{Use dmy dates|date=February 2011}}\n{{featured article}}\n\n{{Hyperoperations}}\n{{Authority control}}\n\n[[Category:Logarithms| ]]\n[[Category:Elementary special functions]]\n[[Category:Scottish inventions]]\n[[Category:Additive function]]"
    },
    {
      "title": "Natural antilogarithm",
      "url": "https://en.wikipedia.org/wiki/Natural_antilogarithm",
      "text": "#redirect [[Logarithm#Antilogarithm]] {{R to related topic}}\n\n[[Category:Exponentials]]\n[[Category:Elementary special functions]]"
    },
    {
      "title": "Natural logarithm",
      "url": "https://en.wikipedia.org/wiki/Natural_logarithm",
      "text": "{{redirect|Base e|the numbering system which uses \"e\" as its base|Non-integer representation#Base&nbsp;e}}\n\n[[File:Log.svg|thumb|right|300 px|Graph of the natural logarithm function. The function slowly grows to positive infinity as ''x'' increases and slowly goes to negative infinity as ''x'' approaches 0 (\"slowly\" as compared to any [[power law]] of ''x''); the ''y''-axis is an [[asymptote]].]]\n\nThe '''natural logarithm''' of a number is its [[logarithm]] to the [[base (exponentiation)|base]] of the [[mathematical constant]] [[e (mathematical constant)|''e'']], where ''e'' is an [[Irrational number|irrational]] and [[Transcendental number|transcendental]] number approximately equal to {{val|2.718281828459}}. The natural logarithm of ''x'' is generally written as {{nowrap|ln ''x''}}, {{nowrap|log<sub>''e''</sub> ''x''}},  or sometimes, if the base ''e'' is implicit, simply {{nowrap|log ''x''}}.<ref>{{cite book |title=Mathematics for physical chemistry |edition=3rd |author-first=Robert G. |author-last=Mortimer |publisher=[[Academic Press]] |date=2005 |isbn=0-12-508347-5 |page=9 |url=https://books.google.com/books?id=nGoSv5tmATsC}} [https://books.google.com/books?id=nGoSv5tmATsC&pg=PA9 Extract of page 9]</ref> [[Parentheses]] are sometimes added for clarity, giving ln(''x''), log<sub>''e''</sub>(''x'') or log(''x''). This is done in particular when the argument to the logarithm is not a single symbol, to prevent ambiguity.\n\nThe natural logarithm of ''x'' is the [[exponentiation|power]] to which ''e'' would have to be raised to equal ''x''. For example, ln(7.5) is 2.0149..., because {{nowrap|1=''e''<sup>2.0149...</sup> = 7.5}}. The natural log of ''e'' itself, ln(''e''), is 1, because {{nowrap|1=''e''<sup>1</sup> = ''e''}}, while the natural logarithm of 1, ln(1), is 0, since {{nowrap|1=''e''<sup>0</sup> = 1}}.\n\nThe natural logarithm can be defined for any positive [[real number]] ''a'' as the [[Integral|area under the curve]] {{nowrap|1=''y'' = 1/''x''}}  from 1 to ''a'' (the area being taken as negative when ''a'' < 1). The simplicity of this definition, which is matched in many other formulas involving the natural logarithm, leads to the term \"natural\". The definition of the natural logarithm can be extended to give logarithm values for negative numbers and for all non-zero [[complex number]]s, although this leads to a [[multi-valued function]]: see [[Complex logarithm]].\n\nThe natural logarithm function, if considered as a [[real-valued function]] of a real variable, is the [[inverse function]] of the [[exponential function]], leading to the identities:\n:<math>e^{\\ln x} = x \\qquad \\text{if }x > 0</math>\n\n:<math>\\ln(e^x) = x.</math>\n\nLike all logarithms, the natural logarithm maps multiplication into addition:\n\n:<math> \\ln(xy) = \\ln x + \\ln y.</math>\n\nThus, the logarithm function is a [[group isomorphism]] from [[positive real numbers]] under multiplication to the [[group (mathematics)|group]] of real numbers under addition, represented as a [[function (mathematics)|function]]:\n:<math>\\ln \\colon \\mathbb{R}^+ \\to \\mathbb{R}.</math>\n\nLogarithms can be defined to any positive base other than 1, not only ''e''. However, logarithms in other bases differ only by a constant multiplier from the natural logarithm, and are usually defined in terms of the latter. For instance, the [[binary logarithm]] is the natural logarithm divided by ln(2), the [[natural logarithm of 2]].  Logarithms are useful for solving equations in which the unknown appears as the exponent of some other quantity.  For example, logarithms are used to solve for the [[half-life]], decay constant, or unknown time in [[exponential decay]] problems.  They are important in many branches of mathematics and the sciences and are used in finance to solve problems involving [[compound interest]].\n\nBy [[Lindemann–Weierstrass theorem]], the natural logarithm of any positive [[algebraic number]] other than 1 is a [[transcendental number]].\n\n{| class=infobox width=247px\n|colspan=2 align=center| Natural logarithm\n|-\n|'''Representation''' ||<math>\\ln x</math>\n|-\n|'''Inverse''' ||<math>e^x</math>\n|-\n|'''Derivative''' ||<math>\\frac{1}{x}</math>\n|-\n|'''nth Derivative'''\n|<math>-(n-1)!\\cdot\\frac{(-1)^{n}}{x^n}</math>\n|-\n|'''Indefinite Integral''' ||<math>x\\ln x - x + C</math>\n|}\n{{E (mathematical constant)}}\n\n==History==\n{{Main|History of logarithms}}\n\nThe concept of the natural logarithm was worked out by [[Gregoire de Saint-Vincent]] and [[Alphonse Antonio de Sarasa]] before 1649.<ref>{{cite book |author-first=R. P. |author-last=Burn |date=2001 |title=Alphonse Antonio de Sarasa and Logarithms |publisher=[[Historia Mathematica]] |pages=28:1–17}}</ref> Their work involved [[quadrature (mathematics)|quadrature]] of the [[hyperbola]] {{nowrap|1=''xy'' = 1}} by determination of the area of [[hyperbolic sector]]s. Their solution generated the requisite \"hyperbolic logarithm\" [[function (mathematics)|function]] having properties now associated with the natural logarithm.\n\nAn early mention of the natural logarithm was by [[Nicholas Mercator]] in his work ''Logarithmotechnia'' published in 1668,<ref>{{cite web |author-first1=J. J. |author-last1=O'Connor |author-first2=E. F. |author-last2=Robertson |url=http://www-history.mcs.st-and.ac.uk/HistTopics/e.html |title=The number e |publisher=The MacTutor History of Mathematics archive |date=September 2001 |access-date=2009-02-02}}</ref> although the mathematics teacher [[John Speidell]] had already in 1619 compiled a table of what in fact were effectively natural logarithms.<ref name = Cajori>{{cite book |author-last=Cajori |author-first=Florian |author-link=Florian Cajori |title=A History of Mathematics |edition=5th |page=152 |publisher=AMS Bookstore |date=1991 |isbn=0-8218-2102-4 |url=https://books.google.com/?id=mGJRjIC9fZgC}}</ref> It has been said that Speidell's logarithms were to the base e, but this is not entirely true due to complications with the values being expressed as integers.<ref name = Cajori/>{{rp|152}}\n\n==Notational conventions==\nThe notations {{nowrap|\"ln ''x''\"}} and {{nowrap|\"log<sub>''e''</sub> ''x''\"}} both refer unambiguously to the natural logarithm of ''x''.\n{{nowrap|\"log ''x''\"}} without an explicit base may also refer to the natural logarithm. This usage is common in mathematics and some scientific contexts as well as in many [[programming language]]s.<ref group=\"nb\">Including [[C (programming language)|C]], [[C++]], [[SAS System|SAS]], [[MATLAB]], [[Mathematica]], <!--[[Pascal programming language|Pascal]], -->[[Fortran]], and [[BASIC programming language|BASIC]]</ref> In some other contexts, however, {{nowrap|\"log ''x''\"}} can be used to denote the [[common logarithm|common (base 10) logarithm]].\n\nHistorically, the notations \"{{math|l.}}\" and  \"{{math|l}}\" were in use at least since the 1730s,<ref name=\"Euler_1737\">{{cite journal |title=Variae observationes circa series infinitas |author-first=Leonhard |author-last=Euler |author-link=Leonhard Euler |journal=Commentarii academiae scientiarum imperialis Petropolitanae (CASP) |volume=9 |date=1737 |publication-date=1744 |pages=160–188 |id=E72}}</ref><ref name=\"Euler\">{{cite book |author-first=Leonhard |author-last=Euler |author-link=Leonhard Euler |title=Opera Omnia, Series Prima: Opera Mathematica |orig-year= |date=1925 |volume=Quartum Decimum |publisher=[[Teubner]]}}</ref> and until at least the 1840s,<ref name=\"Cauchy\">{{cite book |author-first=Augustin |author-last=Cauchy |author-link=Augustin Cauchy |title=Exercices d'analyse et de physique mathématique |volume=3 |page=380 |url=https://books.google.com/books?hl=fr&id=zQo7AAAAcAAJ |access-date=2015-10-31}}</ref> then \"log.\"<ref name=\"Legendre_179x\">{{cite book |author-first=Adrien-Marie |author-last=Legendre |author-link=Adrien-Marie Legendre |title=Essai sur la théorie des nombres |location=Paris, France |publisher=Duprat, libraire pour les mathématiques, quai des Augustins |volume=VI |date=1798}}</ref> or \"log\",<ref name=\"Landau_1909\">{{cite book |author-first=Edmund |author-last=Landau |author-link=Edmund Landau |title=Handbuch der Lehre von der Verteilung der Primzahlen |location=Berlin |orig-year=1909 |date=1953 |edition=2 |publisher=Chelsea, New York}}</ref> at least since the 1790s. Finally, in the twentieth century, the notations \"Log\"<ref name=\"Piskounov_1972\">{{cite book |author-first=Nikolaï |author-last=Piskounov |author-link=:fr:Nikolaï Piskounov |title=Calcul différentiel et intégral |edition=5 |date=1972 |publisher=Editions Mir |location=Moskow |page=91 |url=http://fr.scribd.com/doc/26412181/Calcul-Differentiel-Et-Integral-Tome1-N-Piskounov-Mir}}</ref> and \"logh\"<ref name=\"Jolley_1961\">{{cite book |author-first=L. B. W. |author-last=Jolley |date=1961 |title=Summation of Series |edition=2 (revised) |publisher=[[Dover Publications, Inc.]] |location=New York, USA |lccn=61-65274 |url=http://plouffe.fr/simon/math/SummationofSeries.pdf |access-date=2015-10-31}}</ref> are attested.\n\n==Origin of the term ''natural logarithm''==\n[[Image:hyperbola E.svg|thumb|One unit of area characterizes Euler's number. Add and subtract triangles of area one-half for a hyperbolic sector.]]\nThe function <math>n \\mapsto (e^n , e^{-n} )</math> for ''n'' ∈ ℤ produces a bi-infinite sequence of hyperbolic points. When two adjacent points are joined to (0, 0) by hyperbolic radii, the [[hyperbolic sector]] so formed has unit area. Thus the total area inside the hyperbola and its asymptotes is infinite, consistent with divergence of the [[harmonic series (mathematics)|harmonic series]]. [[Area|Area measure]] accords with the arc measure in both the circle and right hyperbola: in a circle of radius {{radic|2}}, the arc of a [[circular sector]] has an [[angle]] equal to the sector area. Likewise, the [[hyperbolic angle]] of a hyperbolic arc is measured by the area of the corresponding [[hyperbolic sector]] of ''xy'' = 1.\n\nTribute is paid to [[Leonhard Euler]] who profiled the importance of [[Euler's number]] ''e'' = 2.71828... as the [[base (exponentiation)|base]] of the [[exponential function]] and natural logarithm. He introduced the idea of a [[transcendental function]] to classify the trigonometric and exponential functions in the [[precalculus]] [[textbook]] [[Introduction to the Analysis of the Infinite]] (1748). The [[quadrature (mathematics)|quadrature]] of the hyperbola requires the natural logarithm, so [[integral calculus]] was inhibited by the lack of an expression for hyperbolic quadrature until [[Gregoire de Saint-Vincent]] (1647) described it with a logarithmic feature: the correspondence of an arithmetic sequence of areas with a geometric sequence on the asymptote. Expositions by [[Nicholas Mercator]], [[Christiaan Huygens]] and others led to Euler's ''Introduction'' that detailed the [[circular function]]s in terms of [[infinite series]].\n\nThe connection between area and the arcs of circular and [[hyperbolic function]]s demonstrates the naturalness of this logarithm.<ref>{{cite web |author-last=Ballew |author-first=Pat |title=Math Words, and Some Other Words, of Interest |url=http://www.pballew.net/arithme1.html#ln |access-date=2018-01-18}}</ref>\n\n==Definitions==\n[[File:Log-pole-x 1.svg|thumb|ln(''a'') illustrated as the area under the curve ''f''(''x'') = 1/''x'' from 1 to ''a''. If ''a'' is less than 1, the area from ''a'' to 1 is counted as negative.]]\n[[File:Log.gif|The area under the hyperbola satisfies the logarithm rule.  Here ''A''(''s'',''t'') denotes the area under the hyperbola between ''s'' and ''t''.|right|thumb]]\n\nFormally, ln(''a'') may be defined as the area under the [[Hyperbola#Rectangular hyperbola|hyperbola]] 1/''x''.  This is the [[integral]],\n:<math>\\ln a = \\int_1^a \\frac{1}{x}\\,dx.</math>\n\nThis function is a logarithm because it satisfies the fundamental property of a logarithm:\n:<math>\\ln(ab) = \\ln a + \\ln b.</math>\n\nThis can be demonstrated by splitting the integral that defines ln(''ab'') into two parts and then making the [[Integration by substitution|variable substitution]] {{nowrap|1=''x'' = ''ta''}} in the second part, as follows:\n:<math>\\begin{align}\n\\ln(ab)= \\int_1^{ab}\\frac{1}{x} \\, dx\n&=\\int_1^a \\frac 1 x \\, dx + \\int_a^{ab} \\frac{1}{x} \\, dx\\\\[5pt]\n&=\\int_1^a \\frac 1 x \\, dx + \\int_1^b \\frac{1}{at} \\, d(at)\\\\[5pt]\n&=\\int_1^a \\frac 1 x \\, dx + \\int_1^b \\frac{1}{t} \\, dt\\\\[5pt]\n&= \\ln a + \\ln b.\n\\end{align}</math>\n\nIn elementary terms, this is simply scaling by 1/''a'' in the horizontal direction and by ''a'' in the vertical direction.  Area does not change under this transformation, but the region between ''a'' and ''ab'' is reconfigured.  Because the function ''a''/(''ax'') is equal to the function 1/''x'', the resulting area is precisely ln(''b'').\n\nThe number ''[[E (mathematical constant)|e]]'' is defined as the unique real number ''a'' such that ln(''a'')&nbsp;=&nbsp;1.\n\nAlternatively, if the [[exponential function]] has been defined first, say by using an [[infinite series]], the natural logarithm may be defined as its [[inverse function]], i.e., ln is that function such that exp(ln(''x''))&nbsp;=&nbsp;''x''.  Since the range of the exponential function on real arguments is all positive real numbers and since the exponential function is strictly increasing, this is well-defined for all positive&nbsp;''x''.\n\n==Properties==\n* <math>\\ln 1 = 0</math>\n* <math>\\ln e = 1</math>\n* <math>\\ln(xy) = \\ln x + \\ln y \\quad \\text{for }\\; x > 0\\;\\text{and }\\; y > 0</math>\n* <math>\\ln(x^y) = y \\ln x \\quad \\text{for }\\; x > 0</math>\n* <math>\\ln x < \\ln y \\quad\\text{for }\\; 0 < x < y</math>\n* <math>\\lim_{x \\to 0} \\frac{\\ln(1+x)}{x} = 1</math>\n\n{{Collapse top|title=Proof|width=80%|left=yes}}\n:<math>\\lim_{h \\to 0} \\frac{\\ln(1+h)}{h} = \\lim_{h \\to 0} \\frac{\\ln(1+h)-\\ln 1}{h} = \\frac{d}{dx} \\ln x \\Bigg|_{x=1} = 1</math>\n{{Collapse bottom}}\n\n* <math>\\lim_{\\alpha \\to 0} \\frac{x^\\alpha-1}{\\alpha} = \\ln x\\quad \\text{for }\\; x > 0</math>\n\n{{Collapse top|title=Proof|width=80%|left=yes}}\nSince <math>\\ln</math> is the inverse of the exponential function <math>e^x</math>, one can write\n:<math> \\lim_{\\alpha \\to 0} \\frac{x^\\alpha-1}{\\alpha}\n    = \\ln x \\cdot \\lim_{\\alpha \\to 0} \\frac{e^{\\ln x\\cdot\\alpha}-1}{\\ln x\\cdot\\alpha}\n  = \\ln x \\cdot \\lim_{\\beta \\to 0} \\frac{e^{\\beta}-1}{\\beta}  = \\ln x,\n</math>\nwhich proves the claim.\n{{Collapse bottom}}\n\n* <math>\\frac{x-1}{x} \\leq \\ln x \\leq x-1 \\quad\\text{for}\\quad x > 0</math>\n* <math>\\ln{( 1+x^\\alpha )} \\leq \\alpha x \\quad\\text{for}\\quad x \\ge 0\\;\\text{and }\\; \\alpha \\ge 1</math>\n\n{{Collapse top|title=Proof|width=80%|left=yes}}\nThe statement is true for <math>x=0</math>, and we now show that <math>\\frac{d}{dx} \\ln{( 1+x^\\alpha )} \\leq \\frac{d}{dx} ( \\alpha x ) </math> for all <math>x</math>, which completes the proof by the [[fundamental theorem of calculus#First part|fundamental theorem of calculus]]. Hence, we want to show that\n\n:<math>\\frac{d}{dx} \\ln{( 1+x^\\alpha )} = \\frac{\\alpha x^{\\alpha - 1}}{1 + x^\\alpha} \\leq \\alpha = \\frac{d}{dx} ( \\alpha x ) </math>\n\n(Note that we have not yet proved that this statement is true.) If this is true, then by multiplying the middle statement by the positive quantity <math>(1+x^\\alpha) / \\alpha</math> and subtracting <math>x^\\alpha</math> we would obtain\n\n:<math> x^{\\alpha-1} \\leq x^\\alpha + 1 </math>\n:<math> x^{\\alpha-1} (1-x) \\leq 1 </math>\n\nThis statement is trivially true for <math>x \\ge 1</math> since the left hand side is negative or zero. For <math>0 \\le x < 1</math> it is still true since both factors on the left are less than 1 (recall that <math>\\alpha \\ge 1</math>). Thus this last statement is true and by repeating our steps in reverse order we find that <math>\\frac{d}{dx} \\ln{( 1+x^\\alpha )} \\leq \\frac{d}{dx} ( \\alpha x ) </math> for all <math>x</math>. This completes the proof.\n\nAn alternate proof is to observe that <math>(1+x^\\alpha)\\leq (1+x)^\\alpha </math> under the given conditions. This can be proved, e.g., by the norm inequalities. Taking logarithms and using <math> \\ln(1+x)\\leq x </math> completes the proof.\n{{Collapse bottom}}\n\n== Derivative ==\n\nThe [[derivative]] of the natural logarithm as a real-valued function on the positive reals is given by\n:<math>\\frac{d}{dx} \\ln x = \\frac{1}{x}.</math>\n\nHow to establish this derivative of the natural logarithm depends on how it is defined firsthand.  If the natural logarithm is defined as the integral\n:<math>\\ln x = \\int_1^x \\frac{1}{t}\\,dt,</math>\nthen the derivative immediately follows from the first part of the [[Fundamental theorem of calculus#First part|fundamental theorem of calculus]].\n\nIf the natural logarithm is defined as the inverse of the (natural) exponential function, then the derivative for ''x'' > 0 can be found by using the properties of the logarithm and a definition of the exponential function. \n:<math>\\begin{align}\n   \\frac{d}{dx} \\ln x &= \\lim_{h\\to 0} \\frac{\\ln(x+h) - \\ln x}{h} \\\\\n   &= \\lim_{h\\to 0}\\left( \\frac{1}{h} \\ln\\left(\\frac{x+h}{x}\\right)\\right) \\\\\n   &= \\lim_{h\\to 0} \\ln\\left(1 + \\frac{h}{x}\\right)^{\\frac{1}{h}} \\quad &&\\text{all above for logarithmic properties}\\\\\n   &= \\ln \\lim_{h\\to 0} \\left(1 + \\frac{h}{x}\\right)^{\\frac{1}{h}}\\quad &&\\text{for continuity of the logarithm} \\\\\n   &= \\ln e^{1/x} \\quad &&\\text{for the definition of } e^x= \\lim_{h\\to 0}(1 + hx)^{(1/h)}\\\\\n   &= \\frac{1}{x} \\quad &&\\text{for the definition of the ln as inverse function.}\n \\end{align}</math>\n\n== Series ==\n\n[[File:LogTay.svg|290px|thumb|right|The Taylor polynomials for ln(1&nbsp;+&nbsp;''x'') only provide accurate approximations in the range −1&nbsp;&lt;&nbsp;''x''&nbsp;≤&nbsp;1. Note that, beyond some ''x''&nbsp;&gt;&nbsp;1, the Taylor polynomials of higher degree are increasingly ''worse'' approximations.]]\n\nIf <math>\\vert x - 1 \\vert \\leq 1 \\text{ and } x \\neq 0, </math> then<ref>[http://www.math2.org/math/expansion/log.htm \"Logarithmic Expansions\" at Math2.org]</ref>\n\n:<math>\\begin{align}\n   \\ln x &= \\int_1^x \\frac{1}{t} \\, dt = \\int_0^{x - 1} \\frac{1}{1 + u} \\, du \\\\\n   &= \\int_0^{x - 1} (1 - u + u^2 - u^3 + \\cdots) \\, du \\\\\n   &= (x - 1) - \\frac{(x - 1)^2}{2} + \\frac{(x - 1)^3}{3} - \\frac{(x - 1)^4}{4} + \\cdots \\\\\n   &= \\sum_{k=1}^\\infty \\frac{(-1)^{k-1} (x-1)^k}{k}.\n \\end{align}</math>\n\nThis is the [[Taylor series]] for ln&nbsp;''x'' around 1. A change of variables yields the [[Mercator series]]:\n\n:<math>\\ln(1+x)=\\sum_{k=1}^\\infty \\frac{(-1)^{k-1}}{k} x^k = x - \\frac{x^2}{2} + \\frac{x^3}{3} - \\cdots,</math>\n\nvalid for |''x''|&nbsp;&le;&nbsp;1 and ''x''&nbsp;≠&nbsp;&minus;1.\n\n[[Leonhard Euler]],<ref>[[Leonhard Euler]], Introductio in Analysin Infinitorum. Tomus Primus. Bousquet, Lausanne 1748. Exemplum 1, p. 228; quoque in: Opera Omnia, Series Prima, Opera Mathematica, Volumen Octavum, Teubner 1922</ref> disregarding <math>x\\ne -1</math>, nevertheless applied this series to ''x''&nbsp;=&nbsp;−1, in order to show that the [[harmonic series (mathematics)|harmonic series]] equals the (natural) logarithm of 1/(1&nbsp;−&nbsp;1), that is the logarithm of infinity. Nowadays, more formally, one can prove that the harmonic series truncated at ''N'' is close to the logarithm of ''N'', when ''N'' is large.\n\nAt right is a picture of ln(1&nbsp;+&nbsp;''x'') and some of its [[Taylor polynomial]]s around 0. These approximations converge to the function only in the region −1&nbsp;&lt;&nbsp;''x''&nbsp;≤&nbsp;1; outside of this region the higher-degree Taylor polynomials evolve to ''worse'' approximations for the function.\n\nA useful special case for positive integers ''n'', taking <math>x=\\tfrac{1}{n}</math>, is:\n:<math> \\ln \\left(\\frac{n + 1}{n}\\right) = \\sum_{k=1}^\\infty \\frac{(-1)^{k-1}}{k n^k}  = \\frac{1}{n} - \\frac{1}{2 n^2} + \\frac{1}{3 n^3} - \\frac{1}{4 n^4} + \\cdots</math>\n\nIf <math>\\operatorname{Re}(x) \\ge 1/2,</math> then\n:<math>\\begin{align}\n   \\ln (x) &= - \\ln \\left(\\frac{1}{x}\\right) = - \\sum_{k=1}^\\infty \\frac{(-1)^{k-1} (\\frac{1}{x} - 1)^k}{k} = \\sum_{k=1}^\\infty \\frac{(x - 1)^k}{k x^k} \\\\\n   &= \\frac{x - 1}{x} + \\frac{(x - 1)^2}{2 x^2} +  \\frac{(x - 1)^3}{3 x^3} + \\frac{(x - 1)^4}{4 x^4} + \\cdots\n \\end{align}</math>\n\nNow taking  <math>x=\\tfrac{n+1}{n}</math> for positive integers ''n'', yields:\n:<math> \\ln \\left(\\frac{n + 1}{n}\\right) = \\sum_{k=1}^\\infty \\frac{1}{k (n + 1)^k} = \\frac{1}{n + 1} + \\frac{1}{2 (n + 1)^2} + \\frac{1}{3 (n + 1)^3} + \\frac{1}{4 (n + 1)^4} + \\cdots</math>\n\nIf <math>\\operatorname{Re}(x) \\ge 0 \\text{ and } x \\neq 0,</math> then\n:<math> \\ln (x) = \\ln \\left(\\frac{2x}{2}\\right) = \\ln\\left(\\frac{1 + \\frac{x - 1}{x + 1}}{1 - \\frac{x - 1}{x + 1}}\\right) = \\ln \\left(1 + \\frac{x - 1}{x + 1}\\right) - \\ln \\left(1 - \\frac{x - 1}{x + 1}\\right). </math>\nSince\n:<math>\\begin{align}\n\\ln(1+y) - \\ln(1-y)&= \\sum^\\infty_{i=1}\\frac{1}{i}\\left((-1)^{i-1}y^i - (-1)^{i-1}(-y)^i\\right) = \\sum^\\infty_{i=1}\\frac{y^i}{i}\\left((-1)^{i-1} +1\\right)  \\\\\n&= y\\sum^\\infty_{i=1}\\frac{y^{i-1}}{i}\\left((-1)^{i-1} +1\\right)\\overset{i-1\\to 2k}{=}\\; 2y\\sum^\\infty_{k=0}\\frac{y^{2k}}{2k+1},\n\\end{align}</math> \nwe arrive at\n:<math>\\begin{align}\n   \\ln (x) &= \\frac{2(x - 1)}{x + 1} \\sum_{k = 0}^\\infty \\frac{1}{2k + 1} {\\left(\\frac{(x - 1)^2}{(x + 1)^2}\\right)}^k \\\\\n&= \\frac{2(x - 1)}{x + 1} \\left( \\frac{1}{1} + \\frac{1}{3} \\frac{(x - 1)^2}{(x + 1)^2} + \\frac{1}{5} {\\left(\\frac{(x - 1)^2}{(x + 1)^2}\\right)}^2 + \\cdots \\right) .\n \\end{align}</math>\nSubstituting again <math>x=\\tfrac{n+1}{n}</math> for positive integers ''n'', yields: \n:<math>\\begin{align}\n   \\ln \\left(\\frac{n + 1}{n}\\right) &= \\frac{2}{2n + 1} \\sum_{k=0}^\\infty \\frac{1}{(2k + 1) ((2n + 1)^2)^k}\\\\\n&= 2 \\left(\\frac{1}{2n + 1} + \\frac{1}{3 (2n + 1)^3} + \\frac{1}{5 (2n + 1)^5} + \\cdots \\right).\n\\end{align}</math>\n\nThis is, by far, the fastest converging of the series described here.\n\n==The natural logarithm in integration==\nThe natural logarithm allows simple [[integral|integration]] of functions of the form ''g''(''x'') = ''f''&nbsp;'(''x'')/''f''(''x''): an [[antiderivative]] of ''g''(''x'') is given by ln(|''f''(''x'')|).  This is the case because of the [[chain rule]] and the following fact:\n\n:<math>\\frac{d}{dx}\\ln \\left| x \\right| = \\frac{1}{x}.</math>\n\nIn other words,\n\n:<math>\\int \\frac{1}{x} \\,dx = \\ln|x| + C</math>\n\nand\n\n:<math>\\int { \\frac{f'(x)}{f(x)}\\,dx} = \\ln|f(x)| + C.</math>\n\nHere is an example in the case of ''g''(''x'') = tan(''x''):\n\n: <math>\n\\begin{align}\n& \\int \\tan x \\,dx = \\int \\frac{\\sin x}{\\cos x} \\,dx \\\\[6pt]\n& \\int \\tan x \\,dx = \\int \\frac{-\\frac{d}{dx} \\cos x}{\\cos x} \\,dx.\n\\end{align}\n</math>\nLetting ''f''(''x'') = cos(''x''):\n:<math>\\int \\tan x \\,dx = -\\ln \\left| \\cos x \\right| + C</math>\n:<math>\\int \\tan x \\,dx = \\ln \\left| \\sec x \\right| + C</math>\n\nwhere ''C'' is an [[arbitrary constant of integration]].\n\nThe natural logarithm can be integrated using [[integration by parts]]:\n\n:<math>\\int \\ln x \\,dx = x \\ln x - x + C.</math>\n\nLet:\n\n:<math>u = \\ln x \\Rightarrow du = \\frac{dx}{x}</math>\n:<math>dv = dx  \\Rightarrow v = x</math>\n\nthen:\n\n: <math>\n\\begin{align}\n\\int \\ln x \\,dx & = x \\ln x - \\int \\frac{x}{x} \\,dx \\\\\n& = x \\ln x - \\int 1 \\,dx \\\\\n& = x \\ln x - x + C\n\\end{align}\n</math>\n\n==Numerical value==<!-- This section is linked from [[Common logarithm]] -->\n\nFor ln(''x'') where ''x''&nbsp;>&nbsp;1, the closer the value of ''x'' is to 1, the faster the rate of convergence. The identities associated with the logarithm can be leveraged to exploit this:\n:<math>\\begin{align}\n\\ln 123.456 &= \\ln(1.23456 \\cdot 10^2)\\\\\n&= \\ln 1.23456 + \\ln(10^2)\\\\\n&= \\ln 1.23456 + 2 \\ln 10\\\\\n&\\approx \\ln 1.23456 + 2 \\cdot 2.3025851.\n\\end{align}</math>\n\nSuch techniques were used before calculators, by referring to numerical tables and performing manipulations such as those above.\n\n===Natural logarithm of 10===\nThe natural logarithm of 10, which has the decimal expansion 2.30258509...,<ref>{{OEIS2C|A002392}}</ref> plays a role for example in the computation of natural logarithms of numbers represented in [[scientific notation]], as a mantissa multiplied by a power of 10:\n\n: <math>\\ln(a\\cdot 10^n) = \\ln a + n \\ln 10.</math>\n\nThis means that one can effectively calculate the logarithms of numbers with very large or very small [[magnitude (mathematics)|magnitude]] using the logarithms of a relatively small set of decimals in the range <math>[1,10)</math>.\n\n==={{anchor|lnp1}}High precision===\nTo compute the natural logarithm with many digits of precision, the Taylor series approach is not efficient since the convergence is slow. Especially if {{math|''x''}} is near 1, a good alternative is to use [[Halley's method]] or [[Newton's method]] to invert the exponential function, because the series of the exponential function converges more quickly. For finding the value of {{math|''y''}} to give {{math|1=exp(''y'') − ''x'' = 0}} using Halley's method, or equivalently to give {{math|1=exp(''y''/2) − ''x'' exp(−''y''/2) = 0}} using Newton's method, the iteration simplifies to\n:<math> y_{n+1} = y_n + 2 \\cdot \\frac{ x - \\exp ( y_n ) }{ x + \\exp ( y_n ) } </math>\nwhich has [[cubic convergence]] to {{math|ln(''x'')}}.\n\nAnother alternative for extremely high precision calculation is the formula<ref>{{cite journal |author-first1=T. |author-last1=Sasaki |author-first2=Y. |author-last2=Kanada |title=Practically fast multiple-precision evaluation of log(x) |journal=Journal of Information Processing |volume=5 |issue=4 |pages=247–250 |date=1982 |url=http://ci.nii.ac.jp/naid/110002673332 |access-date=2011-03-30}}</ref>\n<ref>{{cite journal |author-first1=Timm |author-last1=Ahrendt |work=Stacs 99 |series=Lecture Notes in Computer Science |doi=10.1007/3-540-49116-3_28 |volume=1564 |date=1999 |pages=302–312 |title=Fast Computations of the Exponential Function |isbn=978-3-540-65691-3}}</ref>\n\n:<math>\\ln x \\approx \\frac{\\pi}{2 M(1,4/s)} - m \\ln 2,</math>\n\nwhere {{math|''M''}} denotes the [[arithmetic-geometric mean]] of 1 and {{math|4/''s''}}, and\n\n:<math>s = x 2^m > 2^{p/2},</math>\n\nwith {{math|''m''}} chosen so that {{math|''p''}} bits of precision is attained. (For most purposes, the value of 8 for m is sufficient.) In fact, if this method is used, Newton inversion of the natural logarithm may conversely be used to calculate the exponential function efficiently. (The constants ln 2 and [[pi|π]] can be pre-computed to the desired precision using any of several known quickly converging series.)\n\nBased on a proposal by [[William Kahan]] and first implemented in the [[Hewlett-Packard]] [[HP-41C]] calculator in 1979 (referred to under \"LN1\" in the display, only), some calculators, [[computer algebra system]]s and programming languages (for example [[C99]]<ref name=\"Beebe_2002\"/>) provide a special '''natural logarithm plus 1''' function, alternatively named '''LNP1''',<ref name=\"HP48_AUR\">{{cite book |title=HP&nbsp;48G Series – Advanced User's Reference Manual (AUR) |publisher=[[Hewlett-Packard]] |edition=4 |date=December 1994 |id=HP 00048-90136, 0-88698-01574-2 |orig-year=1993<!-- edition 1 (1993-07) --> |url=http://www.hpcalc.org/details.php?id=6036 |access-date=2015-09-06}}</ref><ref name=\"HP50_AUR\">{{cite book |title=HP 50g / 49g+ / 48gII graphing calculator advanced user's reference manual (AUR) |publisher=[[Hewlett-Packard]] |edition=2 |date=2009-07-14 |orig-year=2005<!-- first published: Edition 1 (2005–09) --> |id=HP F2228-90010 |url=http://www.hpcalc.org/details.php?id=7141 | accessdate=2015-10-10}} [http://holyjoe.net/hp/HP_50g_AUR_v2_English_searchable.pdf Searchable PDF]</ref> or '''log1p'''<ref name=\"Beebe_2002\"/> to give more accurate results for logarithms close to zero by passing arguments ''x'', also close to zero, to a function log1p(''x''), which returns the value ln(1+''x''), instead of passing a value ''y'' close to 1 to a function returning ln(''y'').<ref name=\"Beebe_2002\">{{cite web |title=Computation of expm1 = exp(x)−1 |author-first=Nelson H. F. |author-last=Beebe |publisher=Department of Mathematics, Center for Scientific Computing, [[University of Utah]] |location=Salt Lake City, Utah, USA |date=2002-07-09 |version=1.00 |url=http://www.math.utah.edu/~beebe/reports/expm1.pdf |access-date=2015-11-02}}</ref><ref name=\"HP48_AUR\"/><ref name=\"HP50_AUR\"/> The function log1p avoids in the floating point arithmetic a near cancelling of the absolute term 1 with the second term from the Taylor expansion of the ln, thereby allowing for a high accuracy for both the argument and the result near zero.<ref name=\"HP48_AUR\"/><ref name=\"HP50_AUR\"/> Similar inverse functions named \"[[expm1]]\",<ref name=\"Beebe_2002\"/> \"expm\"<ref name=\"HP48_AUR\"/><ref name=\"HP50_AUR\"/> or \"exp1m\" exist as well, all with the meaning of {{nowrap|expm1(''x'') {{=}} exp(''x'') - 1.}}<ref group=\"nb\" name=\"Alternative_funcs\">For a similar approach to reduce [[round-off error]]s of calculations for certain input values see [[trigonometric function]]s like [[versine]], [[vercosine]], [[coversine]], [[covercosine]], [[haversine]], [[havercosine]], [[hacoversine]], [[hacovercosine]], [[exsecant]] and [[excosecant]].</ref>\n\nAn identity in terms of the [[artanh|inverse hyperbolic tangent]],\n:<math>\\mathrm{log1p}(x) = \\log(1+x) = 2 ~ \\mathrm{artanh}\\left(\\frac{x}{2+x}\\right)\\,,</math>\ngives a high precision value for small values of {{math|''x''}} on systems that do not implement {{math|log1p(''x'')}}.\n\n===Computational complexity===\n{{main|Computational complexity of mathematical operations}}\nThe [[Computational complexity theory|computational complexity]] of computing the natural logarithm (using the arithmetic-geometric mean) is O(''M''(''n'') ln ''n''). Here ''n'' is the number of digits of precision at which the natural logarithm is to be evaluated and ''M''(''n'') is the computational complexity of multiplying two ''n''-digit numbers.\n\n==Continued fractions==\nWhile no simple [[continued fraction]]s are available, several [[generalized continued fraction]]s are, including:\n\n:<math>\n\\begin{align}\n\\ln(1+x) & =\\frac{x^1}{1}-\\frac{x^2}{2}+\\frac{x^3}{3}-\\frac{x^4}{4}+\\frac{x^5}{5}-\\cdots \\\\[5pt]\n& = \\cfrac{x}{1-0x+\\cfrac{1^2x}{2-1x+\\cfrac{2^2x}{3-2x+\\cfrac{3^2x}{4-3x+\\cfrac{4^2x}{5-4x+\\ddots}}}}}\n\\end{align}\n</math>\n\n: <math>\n\\begin{align}\n\\ln\\left(1+\\frac{x}{y}\\right) & = \\cfrac{x} {y+\\cfrac{1x} {2+\\cfrac{1x} {3y+\\cfrac{2x} {2+\\cfrac{2x} {5y+\\cfrac{3x} {2+\\ddots}}}}}} \\\\[5pt]\n& = \\cfrac{2x} {2y+x-\\cfrac{(1x)^2} {3(2y+x)-\\cfrac{(2x)^2} {5(2y+x)-\\cfrac{(3x)^2} {7(2y+x)-\\ddots}}}}\n\\end{align}\n</math>\n\nThese continued fractions—particularly the last—converge rapidly for values close to&nbsp;1. However, the natural logarithms of much larger numbers can easily be computed by repeatedly adding those of smaller numbers, with similarly rapid convergence.\n\nFor example, since 2 = 1.25<sup>3</sup> × 1.024, the [[natural logarithm of 2]] can be computed as:\n\n: <math>\n\\begin{align}\n\\ln 2 & = 3 \\ln\\left(1+\\frac{1}{4}\\right) + \\ln\\left(1+\\frac{3}{125}\\right) \\\\[8pt]\n& = \\cfrac{6} {9-\\cfrac{1^2} {27-\\cfrac{2^2} {45-\\cfrac{3^2} {63-\\ddots}}}}\n+ \\cfrac{6} {253-\\cfrac{3^2} {759-\\cfrac{6^2} {1265-\\cfrac{9^2} {1771-\\ddots}}}}.\n\\end{align}\n</math>\n\nFurthermore, since 10 = 1.25<sup>10</sup> × 1.024<sup>3</sup>, even the natural logarithm of 10 similarly can be computed as:\n\n: <math>\n\\begin{align}\n\\ln 10 & = 10 \\ln\\left(1+\\frac{1}{4}\\right) + 3\\ln\\left(1+\\frac{3}{125}\\right) \\\\[10pt]\n& = \\cfrac{20} {9-\\cfrac{1^2} {27-\\cfrac{2^2} {45-\\cfrac{3^2} {63-\\ddots}}}}\n+ \\cfrac{18} {253-\\cfrac{3^2} {759-\\cfrac{6^2} {1265-\\cfrac{9^2} {1771-\\ddots}}}}.\n\\end{align}\n</math>\n\n==Complex logarithms==\n{{Main|Complex logarithm}}\nThe exponential function can be extended to a function which gives a [[complex number]] as {{math|''e''<sup>''x''</sup>}} for any arbitrary complex number ''x''; simply use the infinite series with ''x'' complex. This exponential function can be inverted to form a complex logarithm that exhibits most of the properties of the ordinary logarithm. There are two difficulties involved: no ''x'' has {{math|''e''<sup>''x''</sup> {{=}} 0}}; and it turns out that {{math|''e''<sup>2{{pi}}''i''</sup> {{=}} 1 {{=}} ''e''<sup>0</sup>}}. Since the multiplicative property still works for the complex exponential function, {{math|''e''<sup>''z''</sup> {{=}} ''e''<sup>''z''+2{{pi}}''ki''</sup>}}, for all complex ''z'' and integers&nbsp;''k''.\n\nSo the logarithm cannot be defined for the whole [[complex plane]], and even then it is [[multi-valued]] – any complex logarithm can be changed into an \"equivalent\" logarithm by adding any integer multiple of 2{{pi}}''i'' at will. The complex logarithm can only be single-valued on the [[complex plane#Cutting the plane|cut plane]].  For example, {{math|ln(''i'')}} = {{sfrac|{{pi}}''i''|2}} or {{sfrac|5{{pi}}''i''|2}} or -{{sfrac|3{{pi}}''i''|2}}, etc.; and although {{math|''i''<sup>4</sup> {{=}} 1, 4 log(''i'')}} can be defined as 2{{pi}}''i'', or 10{{pi}}''i'' or −6{{pi}}''i'', and so on.\n\n<gallery mode=\"packed\" caption=\"Plots of the natural logarithm function on the complex plane ([[principal branch]])\">\nImage:NaturalLogarithmRe.png| ''z'' = Re(ln(''x'' + ''yi''))\nImage:NaturalLogarithmImAbs.png| ''z'' = abs(Im(ln(''x'' + ''yi'')))\nImage:NaturalLogarithmAbs.png| ''z'' = abs(ln(''x'' + ''yi''))\nImage:NaturalLogarithmAll.png| Superposition of the previous three graphs\n</gallery>\n\n==See also==\n* [[Mental calculation#nat-exp|Approximating natural exponents (log base e)]]\n* [[Napierian logarithm]]\n* [[Logarithm of a matrix]]\n* [[Logarithmic integral function]]\n* [[Nicholas Mercator]] – first to use the term natural logarithm\n* [[Polylogarithm]]\n* [[Von Mangoldt function]]\n* [[e (mathematical constant)|The number ''e'']]\n\n==Notes==\n{{Reflist|group=\"nb\"}}\n\n==References==\n{{Reflist|30em}}\n\n{{DEFAULTSORT:Natural Logarithm}}\n[[Category:Logarithms]]\n[[Category:Elementary special functions]]\n[[Category:E (mathematical constant)]]\n[[Category:Unary operations]]\n\n[[de:Logarithmus#Natürlicher Logarithmus]]"
    },
    {
      "title": "Natural logarithm plus 1",
      "url": "https://en.wikipedia.org/wiki/Natural_logarithm_plus_1",
      "text": "#REDIRECT [[Natural logarithm#lnp1]]\n\n{{Redirect category shell|\n{{Redirect to related topic}}\n}}\n\n[[Category:Logarithms]]\n[[Category:Elementary special functions]]"
    },
    {
      "title": "Ptolemy's table of chords",
      "url": "https://en.wikipedia.org/wiki/Ptolemy%27s_table_of_chords",
      "text": "The '''table of chords''', created by the astronomer, geometer, and geographer [[Ptolemy]] in [[Egypt]] during the 2nd century AD, is a [[trigonometric table]] in Book&nbsp;I, chapter&nbsp;11 of Ptolemy's ''[[Almagest]]'',<ref name=toomer>{{Citation|title=Ptolemy's Almagest|last1=Toomer|first1=G. J.|authorlink=Gerald J. Toomer|publisher=[[Princeton University Press]]|year= 1998|ISBN =0-691-00260-6}}</ref> a treatise on [[mathematical astronomy]].  It is essentially equivalent to a table of values of the [[sine]] function.  It was the earliest trigonometric table extensive enough for many practical purposes, including those of astronomy (an earlier table of chords by [[Hipparchus]] gave chords only for arcs that were multiples of {{nowrap|{{sfrac|7|1|2}}° {{=}} {{sfrac|{{pi}}|24}} radians}}).<ref>Thurston, [https://books.google.com/books?id=rNpHjqxQQ9oC&pg=PA235 pp. 235&ndash;236].</ref>  Centuries passed before more extensive trigonometric tables were created. One such table is the ''[[Canon Sinuum (Bürgi)|Canon Sinuum]]'' created at the end of the 16th century.\n\n== The chord function and the table ==\n\n[[File:Chords.svg|thumb|right|350px|Example: The length of the chord subtending a ({{sfrac|109|1|2}})° arc is approximately&nbsp;98.]]\n\nA [[chord (geometry)|chord]] of a [[circle]] is a line segment whose endpoints are on the circle.  Ptolemy used a circle whose diameter is&nbsp;120.  He tabulated the length of a chord whose endpoints are separated by an arc of ''n''&nbsp;degrees, for ''n'' ranging from {{sfrac|1|2}} to 180 by increments of&nbsp;{{sfrac|1|2}}.  In modern notation, the length of the chord corresponding to an arc of ''θ''&nbsp;degrees is\n\n: <math>\n\\begin{align}\n& \\operatorname{chord}(\\theta) = 120\\sin\\left(\\frac{\\theta^\\circ} 2 \\right) \\\\\n= {} & 60 \\cdot \\left( 2 \\, \\sin\\left(\\frac{\\pi\\theta}{360} \\text{ radians} \\right) \\right).\n\\end{align}\n</math>\n\nAs ''θ'' goes from 0 to 180, the chord of a ''θ''° arc goes from 0 to&nbsp;120.  For tiny arcs, the chord is to the arc angle in degrees as {{pi}} is to&nbsp;3, or more precisely, the ratio can be made as close as desired to {{sfrac|{{pi}}|3}}&nbsp;≈&nbsp;{{val|1.04719755}} by making ''θ'' small enough.  Thus, for the arc of {{sfrac|1|2}}°, the chord length is slightly more than the arc angle in degrees.  As the arc increases, the ratio of the chord to the arc decreases.  When the arc reaches 60°, the chord length is exactly equal to the number of degrees in the arc, i.e. chord&nbsp;60°&nbsp;=&nbsp;60.  For arcs of more than 60°, the chord is less than the arc, until an arc of 180° is reached, when the chord is only&nbsp;120.\n\nThe fractional parts of chord lengths were expressed in [[sexagesimal]] (base 60) numerals.  For example, where the length of a chord subtended by a 112° arc is reported to be 99&nbsp;29&nbsp;5, it has a length of\n\n: <math> 99 + \\frac{29}{60} + \\frac{5}{60^2} = 99.4847\\overline{2}, </math>\n\nrounded to the nearest&nbsp;{{sfrac|1|60<sup>2</sup>}}.<ref name=toomer />\n\nAfter the columns for the arc and the chord, a third column is labeled \"sixtieths\".  For an arc of&nbsp;''θ''°, the entry in the \"sixtieths\" column is\n\n: <math> \\frac{\\operatorname{chord} \\left(\\theta + \\tfrac12^\\circ \\right) - \\operatorname{chord} \\left( \\theta^\\circ\\right)}{30}. </math>\n\nThis is the average number of sixtieths of a unit that must be added to chord(''θ''°) each time the angle increases by one minute of arc, between the entry for&nbsp;''θ''° and that for&nbsp;(''θ''&nbsp;+&nbsp;{{sfrac|1|2}})°. Thus, it is used for [[linear interpolation]].  Glowatzki and Göttsche showed that Ptolemy must have calculated chords to five sexigesimal places in order to achieve the degree of accuracy found in the \"sixtieths\" column.<ref>Ernst Glowatzki and Helmut Göttsche, ''Die Sehnentafel des Klaudios Ptolemaios.  Nach den historischen Formelplänen neuberechnet.'', München, 1976.</ref>\n\n: <math>\n\\begin{array}{|l|rrr|rrr|}\n\\hline\n\\text{arc}^\\circ & \\text{chord} & & & \\text{sixtieths} & & \\\\\n\\hline\n{}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, \\tfrac12 &  0 & 31 & 25 & 0 \\quad 1 & 2 & 50 \\\\\n{}\\,\\,\\,\\,\\,\\,\\, 1 & 1 & 2 & 50 & 0 \\quad 1 & 2 & 50 \\\\\n{}\\,\\,\\,\\,\\,\\,\\, 1\\tfrac12 & 1 & 34 & 15 & 0 \\quad 1 & 2 & 50 \\\\\n{}\\,\\,\\,\\,\\,\\,\\, \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n109 & 97 & 41 & 38 & 0 \\quad 0 & 36 & 23 \\\\\n109\\tfrac12 & 97 & 59 & 49 & 0 \\quad 0 & 36 & 9 \\\\\n110 & 98 & 17 & 54 & 0 \\quad 0 & 35 & 56 \\\\\n110\\tfrac12 & 98 & 35 & 52 & 0 \\quad 0 & 35 & 42\\\\\n111 & 98 & 53 & 43 & 0 \\quad 0 & 35 & 29 \\\\\n111\\tfrac12 & 99 & 11 & 27 & 0 \\quad 0 & 35 & 15 \\\\\n112 & 99 & 29 & 5 & 0 \\quad 0 & 35 & 1\\\\\n112\\tfrac12 & 99 & 46 & 35 & 0 \\quad 0 & 34 & 48 \\\\\n113 & 100 & 3 & 59 & 0 \\quad 0 & 34 & 34 \\\\\n{}\\,\\,\\,\\,\\,\\,\\, \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n179 & 119 & 59 & 44 & 0 \\quad 0 & 0 & 25 \\\\\n179\\frac12 & 119 & 59 & 56 & 0 \\quad 0 & 0 & 9 \\\\\n180 & 120 & 0 & 0 & 0 \\quad 0 & 0 & 0 \\\\\n\\hline\n\\end{array}\n</math>\n\n== How Ptolemy computed chords ==\n\nChapter 10 of Book&nbsp;I of the ''Almagest'' presents [[Euclidean geometry|geometric]] theorems used for computing chords.  Ptolemy used geometric reasoning based on Proposition&nbsp;10 of Book&nbsp;XIII of [[Euclid|Euclid's]] ''[[Euclid's Elements|Elements]]'' to find the chords of 72° and&nbsp;36°.  That Proposition states that if an equilateral [[pentagon]] is inscribed in a circle, then the area of the square on the side of the pentagon equals the sum of the areas of the squares on the sides of the [[hexagon]] and the [[decagon]] inscribed in the same circle.\n\nHe used [[Ptolemy's theorem]] on quadrilaterals inscribed in a circle to derive formulas for the chord of a half-arc, the chord of the sum of two arcs, and the chord of a difference of two arcs.  The theorem states that for a [[quadrilateral]] inscribed in a [[circle]], the product of the lengths of the diagonals equals the sum of the products of the two pairs of lengths of opposite sides.  The derivations of trigonometric identities rely on a [[cyclic quadrilateral]] in which one side is a diameter of the circle.\n\nTo find the chords of arcs of 1° and {{sfrac|1|2}}° he used approximations based on [[Aristarchus's inequality]].  The inequality states that for arcs ''α'' and ''β'', if 0&nbsp;<&nbsp;''β''&nbsp;<&nbsp;''α''&nbsp;<&nbsp;90°, then\n\n: <math> \\frac{\\sin \\alpha}{\\sin \\beta} < \\frac\\alpha\\beta < \\frac{\\tan\\alpha}{\\tan\\beta}.</math>\n\nPtolemy showed that for arcs of 1° and {{sfrac|1|2}}°, the approximations correctly give the first two sexigesimal places after the integer part.\n\n== The numeral system and the appearance of the untranslated table ==\n\n{{main|Greek numerals}}\n\nLengths of arcs of the circle, in degrees, and the integer parts of chord lengths, were expressed in a [[base 10]] [[numeral system]] that used 21 of the letters of the [[Greek alphabet]] with the meanings given in the following table, and a symbol, \"∠′&thinsp;\", that means {{sfrac|1|2}} and a raised circle \"○\" that fills a blank space (effectively representing zero). Two of the letters, labeled \"archaic\" in the table below, had not been in use in the Greek language for some centuries before the ''Almagest'' was written, but were still in use as numerals and [[Ancient Greek music|musical notes]].\n: <math>\n\\begin{array}{|rlr|rlr|rlr|}\n\\hline\n\\alpha & \\mathrm{alpha} &  1 & \\iota & \\mathrm{iota} & 10 & \\rho & \\mathrm{rho} & 100 \\\\  \\beta & \\mathrm{beta} & 2 & \\kappa & \\mathrm{kappa} & 20 & \\vdots  & \\vdots & \\vdots \\\\  \\gamma & \\mathrm{gamma} & 3 & \\lambda & \\mathrm{lambda} & 30 & & & \\\\  \\delta & \\mathrm{delta} & 4 & \\mu & \\mathrm{mu} & 40 & & & \\\\  \\varepsilon & \\mathrm{epsilon} & 5 & \\nu & \\mathrm{nu} & 50 & & & \\\\  \\stigma & \\mathrm{stigma\\ (archaic)} & 6 & \\xi & \\mathrm{xi} & 60 & & & \\\\  \\zeta & \\mathrm{zeta} & 7 & \\omicron & \\mathrm{omicron} & 70 & & & \\\\  \\eta & \\mathrm{eta} & 8 & \\pi & \\mathrm{pi} & 80 & & & \\\\  \\theta & \\mathrm{theta} & 9 & \\koppa & \\mathrm{koppa\\ (archaic)} & 90 & & & \\\\  \\hline\n\\end{array}\n</math>\nThus, for example, an arc of {{sfrac|143|1|2}}° is expressed as ''ρμγ''∠′. (As the table only reaches 180°, the Greek numerals for 200 and above are not used.)\n\nThe fractional parts of chord lengths required great accuracy, and were given in two columns in the table: The first column gives an integer multiple of {{sfrac|1|60}}, in the range 0–59, the second an integer multiple of {{sfrac|1|60<sup>2</sup>}}&nbsp;=&nbsp;{{sfrac|1|3600}}, also in the range 0–59.\n\nThus in  Heiberg's [http://www.wilbourhall.org/pdfs/HeibergAlmagestComplete.pdf edition of the ''Almagest'' with the table of chords on pages 48–63], the beginning of the table, corresponding to arcs from {{sfrac|1|2}}° to {{sfrac|7|1|2}}°, looks like this:\n: <math>\n\\begin{array}{ccc} \\pi\\varepsilon\\rho\\iota\\varphi\\varepsilon\\rho\\varepsilon\\iota\\tilde\\omega\\nu & \\varepsilon\\overset{\\text{'}}\\nu\\theta\\varepsilon\\iota\\tilde\\omega\\nu & \\overset{\\text{`}}\\varepsilon\\xi\\eta\\kappa\\omicron\\sigma\\tau\\tilde\\omega\\nu \\\\\n\\begin{array}{|l|} \\hline \\quad \\angle' \\\\ \\alpha \\\\  \\alpha\\;\\angle' \\\\  \\hline\\beta \\\\  \\beta\\;\\angle' \\\\  \\gamma \\\\  \\hline\\gamma\\;\\angle' \\\\  \\delta \\\\  \\delta\\;\\angle' \\\\  \\hline\\varepsilon \\\\  \\varepsilon\\;\\angle' \\\\  \\stigma \\\\  \\hline\\stigma\\;\\angle' \\\\  \\zeta \\\\  \\zeta\\;\\angle' \\\\  \\hline \\end{array} & \\begin{array}{|r|r|r|} \\hline\\circ & \\lambda\\alpha & \\kappa\\varepsilon \\\\  \\alpha & \\beta & \\nu \\\\  \\alpha & \\lambda\\delta & \\iota\\varepsilon \\\\  \\hline \\beta & \\varepsilon & \\mu \\\\  \\beta & \\lambda\\zeta & \\delta \\\\  \\gamma & \\eta & \\kappa\\eta \\\\  \\hline \\gamma & \\lambda\\theta & \\nu\\beta \\\\  \\delta & \\iota\\alpha & \\iota\\stigma \\\\  \\delta & \\mu\\beta & \\mu \\\\  \\hline \\varepsilon & \\iota\\delta & \\delta \\\\  \\varepsilon & \\mu\\varepsilon & \\kappa\\zeta \\\\  \\stigma & \\iota\\stigma & \\mu\\theta \\\\  \\hline \\stigma & \\mu\\eta & \\iota\\alpha \\\\  \\zeta & \\iota\\theta & \\lambda\\gamma \\\\  \\zeta & \\nu & \\nu\\delta \\\\  \\hline \\end{array} & \\begin{array}{|r|r|r|r|} \\hline \\circ & \\alpha & \\beta & \\nu \\\\  \\circ & \\alpha & \\beta & \\nu \\\\  \\circ & \\alpha & \\beta & \\nu \\\\  \\hline \\circ & \\alpha & \\beta & \\nu \\\\  \\circ & \\alpha & \\beta & \\mu\\eta \\\\  \\circ & \\alpha & \\beta & \\mu\\eta \\\\  \\hline\\circ & \\alpha & \\beta & \\mu\\eta \\\\  \\circ & \\alpha & \\beta & \\mu\\zeta \\\\  \\circ & \\alpha & \\beta & \\mu\\zeta \\\\  \\hline \\circ & \\alpha & \\beta & \\mu\\stigma \\\\  \\circ & \\alpha & \\beta & \\mu\\varepsilon \\\\  \\circ & \\alpha & \\beta & \\mu\\delta \\\\  \\hline \\circ & \\alpha & \\beta & \\mu\\gamma \\\\  \\circ & \\alpha & \\beta & \\mu\\beta \\\\  \\circ & \\alpha & \\beta & \\mu\\alpha \\\\  \\hline \\end{array}\n\\end{array}\n</math>\n\nLater in the table, one can see the base-10 nature of the numerals expressing the integer parts of the arc and the chord length.  Thus an arc of 85° is written as ''πε'' (''π'' for 80 and ''ε'' for 5) and not broken down into 60&nbsp;+&nbsp;25. The corresponding chord length is 81 plus a fractional part. The integer part begins with ''πα'', likewise not broken into 60&nbsp;+&nbsp;21. But the fractional part, {{sfrac|4|60}}&nbsp;+&nbsp;{{sfrac|15|60<sup>2</sup>}}, is written as ''δ'', for 4, in the {{sfrac|1|60}} column, followed by ''ιε'', for 15, in the {{sfrac|1|60<sup>2</sup>}} column.\n: <math>\n\\begin{array}{ccc} \\pi\\varepsilon\\rho\\iota\\varphi\\varepsilon\\rho\\varepsilon\\iota\\tilde\\omega\\nu & \\varepsilon\\overset{\\text{'}}\\nu\\theta\\varepsilon\\iota\\tilde\\omega\\nu & \\overset{\\text{`}}\\varepsilon\\xi\\eta\\kappa\\omicron\\sigma\\tau\\tilde\\omega\\nu \\\\\n\\begin{array}{|l|} \\hline \\pi\\delta\\angle' \\\\  \\pi\\varepsilon \\\\  \\pi\\varepsilon\\angle' \\\\  \\hline  \\pi\\stigma \\\\  \\pi\\stigma\\angle' \\\\  \\pi\\zeta \\\\  \\hline \\end{array} & \\begin{array}{|r|r|r|} \\hline \\pi & \\mu\\alpha & \\gamma \\\\  \\pi\\alpha & \\delta & \\iota\\varepsilon \\\\  \\pi\\alpha & \\kappa\\zeta & \\kappa\\beta \\\\  \\hline \\pi\\alpha & \\nu & \\kappa\\delta \\\\  \\pi\\beta & \\iota\\gamma & \\iota\\theta \\\\  \\pi\\beta & \\lambda\\stigma & \\theta \\\\  \\hline \\end{array} & \\begin{array}{|r|r|r|r|} \\hline \\circ & \\circ & \\mu\\stigma & \\kappa\\varepsilon \\\\  \\circ & \\circ & \\mu\\stigma & \\iota\\delta \\\\  \\circ & \\circ & \\mu\\stigma & \\gamma \\\\  \\hline \\circ & \\circ & \\mu\\varepsilon & \\nu\\beta \\\\  \\circ & \\circ & \\mu\\varepsilon & \\mu \\\\  \\circ & \\circ & \\mu\\varepsilon & \\kappa\\theta \\\\  \\hline \\end{array}\n\\end{array}\n</math>\nThe table has 45&nbsp;lines on each of eight pages, for a total of 360&nbsp;lines.\n\n== See also ==\n* [[Exsecant]]\n* ''[[Fundamentum Astronomiae]]'', a book setting forth an algorithm for precise computation of sines, published in the late 1500s\n* [[Scale of chords]]\n* [[Versine]]\n\n== References ==\n{{Reflist}}\n\n* {{Citation|title=Episodes from the Early History of Mathematics|last1=Aaboe|first1=Asger|authorlink=Asger Aaboe|publisher=Mathematical Association of America|year= 1997|ISBN =978-0-88385-613-0}}\n* {{Citation|title=Greek Science in Antiquity|last1=Clagett|first1=Marshall|authorlink=Marshall Clagett |publisher=Courier Dover Publications|year=2002|ISBN =978-0-8369-2150-2}}\n* {{Citation|title=A History of Ancient Mathematical Astronomy|last1=Neugebauer|first1=Otto|authorlink=Otto Neugebauer|publisher=Springer-Verlag|year= 1975|ISBN =978-0-387-06995-1}}\n* [[Olaf Pedersen]] (1974) ''A Survey of the Almagest'', [[Odense University Press]] {{ISBN|87-7492-087-1}}\n* {{Citation|title=Early Astronomy|last1=Thurston|first1=Hugh|authorlink=Hugh Thurston |publisher=Springer|year=1996|ISBN =978-0-387-94822-5}}\n\n== External links ==\n* [[Johan Ludvig Heiberg (historian)|J. L. Heiberg]] [http://www.wilbourhall.org/pdfs/HeibergAlmagestComplete.pdf ''Almagest''], Table of chords on pages 48–63.\n* Glenn Elert [https://hypertextbook.com/eworld/chords/ Ptolemy's Table of Chords: Trigonometry in the Second Century]\n* [https://archive.org/details/bub_gb_a9nvvbG-OOIC  ''Almageste''] in Greek and French, at the internet archive.\n\n\n[[Category:Trigonometry]]\n[[Category:History of mathematics|Trigonometry]]\n[[Category:History of astronomy]]\n[[Category:Elementary special functions]]\n[[Category:Ptolemy]]\n[[Category:Mathematical tables]]"
    },
    {
      "title": "Sigmoid function",
      "url": "https://en.wikipedia.org/wiki/Sigmoid_function",
      "text": "{{refimprove|date=May 2008}}\n\n[[Image:Logistic-curve.svg|thumb|320px|right|The [[logistic curve]]]]\n[[Image:Error Function.svg|thumb|right|320px|Plot of the [[error function]]]]\n\nA '''sigmoid function''' is a  [[function (mathematics)|mathematical function]] having a characteristic \"S\"-shaped curve or '''sigmoid curve'''.  Often, ''sigmoid function'' refers to the special case of the [[logistic function]] shown in the first figure and defined by the formula\n:<math>S(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{e^x + 1}.</math>\nSpecial cases of the sigmoid function include the [[Gompertz curve]] (used in modeling systems that saturate at large values of x) and the [[ogee curve]] (used in the [[spillway]] of some [[dam]]s). Sigmoid functions have domain of all [[real number]]s, with return value [[monotonic function|monotonically increasing]] most often from 0 to 1 or alternatively from −1 to 1, depending on convention.\n\nA wide variety of sigmoid functions including the logistic and [[hyperbolic tangent]] functions have been used as the [[activation function]] of [[artificial neuron]]s. Sigmoid curves are also common in statistics as [[cumulative distribution function]]s (which go from 0 to 1), such as the integrals of the [[logistic distribution]], the [[normal distribution]], and [[Student's t-distribution|Student's ''t'' probability density functions]].\n\n==Definition==\nA sigmoid function is a [[Bounded function|bounded]], differentiable, real function that is defined for all real input values and has a non-negative derivative at each point.<ref>{{Cite book |last1=Han |first1=Jun |last2=Morag |first2=Claudio |title=From Natural to Artificial Neural Computation |volume=930 |chapter=The influence of the sigmoid function parameters on the speed of backpropagation learning |editor1-last=Mira |editor1-first=José |editor2-last=Sandoval |editor2-first=Francisco |pages=195–201 |year=1995 |doi=10.1007/3-540-59497-3_175 |series=Lecture Notes in Computer Science |isbn=978-3-540-59497-0 }}</ref>\n\n==Properties==\nIn general, a sigmoid function is [[monotonic function|monotonic]], and has a first [[derivative]] which is [[bell shaped function|bell shaped]]. A sigmoid function is constrained by a pair of [[horizontal asymptotes]] as <math>x \\rightarrow \\pm \\infty</math>.\n\nThe sigmoid function is convex for values less than 0, and it is concave for values more than 0. Because of this, the sigmoid function and its affine compositions can possess multiple optima.\n\n==Examples==\n[[File:Gjl-t(x).svg|thumb|320px|right|Some sigmoid functions compared. In the drawing all functions are normalized in such a way that their slope at the origin is 1.]]\n\n*[[Logistic function]]\n::<math> f(x) = \\frac{1}{1 + e^{-x}} </math>\n\n*[[Hyperbolic function|Hyperbolic tangent]] (shifted and scaled version of the logistic function, above)\n::<math> f(x) = \\tanh x = \\frac{e^x-e^{-x}}{e^x+e^{-x}} </math>\n\n*[[Inverse trigonometric function|Arctangent function]]\n::<math> f(x) = \\arctan x </math>\n\n*[[Gudermannian function]]\n\n::<math> f(x) = \\operatorname{gd}(x) = \\int_{0}^{x} \\frac{1}{\\cosh t} \\, dt = 2\\arctan\\left(\\tanh\\left(\\frac{x}{2}\\right)\\right)\n </math>\n\n*[[Error function]]\n::<math> f(x) = \\operatorname{erf}(x) = \\frac{2}{\\sqrt{\\pi}}\\int_0^x e^{-t^2} \\, dt </math>\n\n*[[Generalised logistic function]]\n::<math> f(x) = (1+e^{-x})^{-\\alpha}, \\quad \\alpha > 0 </math>\n\n*[[Smoothstep]] function\n\n::<math> f(x) = \\begin{cases}\n\\displaystyle{\n\\frac{\\int_{0}^{x} \\bigl(1 - u^2 \\bigr)^N \\ du}\n{\\int_{0}^{1} {\\bigl(1 - u^2 \\bigr)^N \\ du}}\n}, & |x| \\le 1 \\\\\n\\sgn(x) & |x| \\ge 1 \\\\\n\\end{cases} \\, \\quad N \\ge 1 </math>\n\n*Some [[algebraic function|algebraic functions]], for example\n::<math> f(x) = \\frac{x}{\\sqrt{1+x^2}} </math>\n\nThe [[integral]] of any continuous, non-negative, \"bump-shaped\" function will be sigmoidal, thus the [[cumulative distribution function]]s for many common [[probability distribution]]s are sigmoidal. One such example is the error function, which is related to the cumulative distribution function of a [[normal distribution]].\n\n==Applications==\n[[File:Gohana inverted S-curve.png|thumb|right|320px|Inverted logistic S-curve to model the relation between wheat yield and soil salinity. <ref>Software to fit an S-curve to a data set [https://www.waterlog.info/sigmoid.htm]</ref> ]]\n\nMany natural processes, such as those of complex system [[learning curve]]s, exhibit a progression from small beginnings that accelerates and approaches a climax over time.   When a specific mathematical model is lacking, a sigmoid function is often used.<ref>{{cite journal\n | last = Gibbs\n | first = M.N.\n |date=Nov 2000\n | title = Variational Gaussian process classifiers\n | journal = IEEE Transactions on Neural Networks\n | volume = 11\n | issue = 6\n | pages = 1458–1464\n | doi = 10.1109/72.883477\n| pmid = 18249869\n }}</ref>\n\nThe [[van Genuchten–Gupta model]] is based on an inverted S-curve and applied to the response of crop yield to [[soil salinity]].\n\nExamples of the application of the logistic S-curve to the response of crop yield (wheat) to both the soil salinity and depth to [[water table]] in the soil are shown in [[logistic function#In agriculture: modeling crop response]].\n\nIn [[artificial neural network]]s, sometimes non-smooth functions are used instead for efficiency; these are known as [[hard sigmoid]]s.\n\nIn [[biochemistry]] and [[pharmacology]], the [[Hill equation (biochemistry)|Hill equation]] and Hill-Langmuir equation are sigmoid functions.\n\n{{clear}}\n\n==See also==\n{{commons category|Sigmoid functions}}\n{{div col|colwidth=30em}}\n* [[Activation function]]\n* [[Cumulative distribution function]]\n* [[Generalized logistic curve]]\n* [[Gompertz function]]\n* [[Heaviside step function]]\n* [[Hyperbolic function]]\n* [[Logistic distribution]]\n* [[Logistic function]]\n* [[Logistic regression]]\n* [[Logit]]\n* [[Rectifier (neural networks)|Softplus function]]\n* [[Smoothstep]] function (Graphics)\n* [[Softmax function]]\n* [[Weibull distribution]]\n* [[Fermi–Dirac statistics]]\n{{div col end}}\n\n== References ==\n{{reflist}}\n\n* {{ cite book | first1=Tom M. |last1= Mitchell | title=Machine Learning | publisher=WCB–McGraw–Hill |year=1997\n|isbn=978-0-07-042807-2}}. In particular see \"Chapter 4: Artificial Neural Networks\" (in particular pp.&nbsp;96–97) where Mitchell uses the word \"logistic function\" and the \"sigmoid function\" synonymously – this function he also calls the \"squashing function\" – and the sigmoid (aka logistic) function is used to compress the outputs of the \"neurons\" in multi-layer neural nets.\n* {{cite web|first1= Mark | last1= Humphrys | url =http://www.computing.dcu.ie/~humphrys/Notes/Neural/sigmoid.html\n|title= Continuous output, the sigmoid function}} Properties of the sigmoid, including how it can shift along axes and how its domain may be transformed.\n\n[[Category:Elementary special functions]]\n[[Category:Artificial neural networks]]"
    },
    {
      "title": "Sinc function",
      "url": "https://en.wikipedia.org/wiki/Sinc_function",
      "text": "{{Redirect|Sinc|the designation used in the United Kingdom for areas of wildlife interest|Site of Importance for Nature Conservation|the signal processing filter based on this function|Sinc filter}}\n{{Use American English|date = March 2019}}\n{{Short description|Special mathematical function defined as sin(x)/x}}\n\nIn [[mathematics]], [[physics]] and [[engineering]], the '''cardinal sine function''' or '''sinc function''', denoted by {{math|sinc(''x'')}}, has two slightly different definitions.<ref name=\"dlmf\">{{dlmf|title=Numerical methods|id=3.3}}</ref>\n\nIn mathematics, the historical '''unnormalized sinc function''' is defined for {{math|''x'' ≠ 0}}  by\n::<math>\\operatorname{sinc}(x) = \\frac{\\sin(x)}{x}~.</math>\n\nIn [[digital signal processing]] and [[information theory]], the '''normalized sinc function''' is commonly defined for  {{math|''x'' ≠ 0}}  by\n[[File:Si sinc.svg|thumb|350px|right|The normalized sinc (blue) and unnormalized sinc function (red) shown on the same scale.]]\n::<math>\\operatorname{sinc}(x) = \\frac{\\sin(\\pi x)}{\\pi x}~.</math>\n\nIn either case, the value at {{math|''x'' {{=}} 0}} is defined to be the limiting value\n::<math>\\operatorname{sinc}(0):=\\lim_{x\\to 0}\\frac{\\sin(a x)}{a x}= 1</math> for all real {{math|''a'' ≠ 0}}.\n\nThe [[Normalizing constant|normalization]] causes the [[integral|definite integral]] of the function over the real numbers to equal 1 (whereas the same integral of the unnormalized sinc function has a value of [[pi|{{pi}}]]). As a further useful property, the zeros of the normalized sinc function are the nonzero integer values of {{mvar|x}}.\n\nThe normalized sinc function is the [[Fourier transform]] of the [[rectangular function]] with no scaling. It is used in the concept of [[Whittaker–Shannon interpolation formula|reconstructing]] a continuous bandlimited signal from uniformly spaced [[Nyquist–Shannon sampling theorem|samples]] of that signal.\n\nThe only difference between the two definitions is in the scaling of the [[independent variable]] (the [[x-axis|{{mvar|x}}-axis]]) by a factor of {{pi}}. In both cases, the value of the function at the [[removable singularity]] at zero is understood to be the limit value 1. The sinc function is then [[Analytic function|analytic]] everywhere and hence an [[entire function]].\n\nThe term ''sinc'' {{IPAc-en|ˈ|s|ɪ|ŋ|k}} is a contraction of the function's full Latin name, the {{lang|la|sinus cardinalis}} (cardinal sine).<ref name=Poynton /> It was introduced by Philip M. Woodward in his 1952 paper \"Information theory and inverse probability in telecommunication\", in which he said the function \"occurs so often in Fourier analysis and its applications that it does seem to merit some notation of its own\",<ref>{{cite journal| last1=Woodward|first1=  P. M.|last2= Davies|first2= I. L.| url=http://www.norbertwiener.umd.edu/crowds/documents/Woodward52.pdf|title=Information theory and inverse probability in telecommunication|journal=Proceedings of the IEE – Part III: Radio and Communication Engineering|volume=99|issue=58|pages=37–44|date= March 1952| doi=10.1049/pi-3.1952.0011}}</ref> and his 1953 book ''Probability and Information Theory, with Applications to Radar''.<ref name=\"Poynton\">{{Cite book|first=Charles A. |last=Poynton|title=Digital video and HDTV|page=147|publisher= Morgan Kaufmann Publishers|year= 2003| isbn =978-1-55860-792-7}}</ref><ref>{{cite book|first=Phillip M. |last=Woodward|title=Probability and information theory, with applications to radar|page=29|location=London|publisher= Pergamon Press|year= 1953|oclc=488749777|isbn=978-0-89006-103-9}}</ref>\n\n== Properties ==\n\n[[File:Si cos.svg|thumb|350px|right|The local maxima and minima (small white dots) of the unnormalized, red sinc function correspond to its intersections with the blue [[cosine function]].]]\n[[File:Sinc re.svg|thumb|The real part of complex sinc {{math|Re(sinc ''z'') {{=}} Re({{sfrac|sin ''z''|''z''}})}}.]]\n[[File:Sinc im.svg|thumb|The imaginary part of complex sinc {{math|Im(sinc ''z'') {{=}} Im({{sfrac|sin ''z''|''z''}})}}.]]\n[[File:Sinc abs.svg|thumb|The absolute value {{math|{{abs|sinc ''z''}} {{=}} {{abs|{{sfrac|sin ''z''|''z''}}}}}}.]]\nThe [[zero crossing]]s of the unnormalized sinc are at non-zero integer multiples of {{pi}}, while zero crossings of the normalized sinc occur at non-zero integers.\n\nThe local maxima and minima of the unnormalized sinc correspond to its intersections with the cosine function.  That is, {{math|{{sfrac|sin(''ξ'')|''ξ''}} {{=}} cos(''ξ'')}} for all points {{mvar|ξ}} where the derivative of {{math|{{sfrac|sin(''x'')|''x''}}}} is zero and thus a local extremum is reached. This follows from the derivative of the sinc function,\n:<math>\\frac{d\\operatorname{sinc}(x)}{dx} = \\frac{\\cos(x) - \\operatorname{sinc}(x)}{x} </math>\n\nThe first few terms of the infinite series for the {{mvar|x}}-coordinate of the {{mvar|n}}th extremum with positive {{mvar|x}}-coordinate are\n:<math>x_n = q - q^{-1} - \\frac{2}{3} q^{-3} - \\frac{13}{15} q^{-5} - \\frac{146}{105} q^{-7} -\\cdots </math>\nwhere\n:<math> q = \\left(n+\\frac{1}{2}\\right)\\pi </math>\nand where odd {{math|''n''}} lead to a local minimum and even {{math|''n''}} to a local maximum. Because of symmetry around the {{math|''y''}}-axis, there exist extrema with {{math|''x''}}-coordinates {{math|−''x<sub>n</sub>''}}. In addition, there is an absolute maximum at {{math|''ξ''<sub>0</sub> {{=}} (0,1)}}.\n\nThe normalized sinc function has a simple representation as the [[infinite product]]\n\n:<math>\\frac{\\sin(\\pi x)}{\\pi x} = \\prod_{n=1}^\\infty \\left(1 - \\frac{x^2}{n^2}\\right)</math>\nand is related to the [[gamma function]] {{math|Γ(''x'')}} through [[Euler's reflection formula]],\n:<math>\\frac{\\sin(\\pi x)}{\\pi x} = \\frac{1}{\\Gamma(1+x)\\Gamma(1-x)}~.</math>\n\n[[Euler]] discovered<ref>{{cite arxiv|last=Euler|first=Leonhard|title=On the sums of series of reciprocals|year=1735|eprint=math/0506415}}</ref> that\n:<math>\\frac{\\sin(x)}{x} = \\prod_{n=1}^\\infty \\cos\\left(\\frac{x}{2^n}\\right)</math>\nand because of the product-to-sum identity<ref>{{cite journal|author1=Luis Ortiz-Gracia|author2=Cornelis W. Oosterlee|title=A highly efficient Shannon wavelet inverse Fourier technique for pricing Europe|year=2016|journal=SIAM J. Sci. Comput.  |volume=38 |issue=1 |pages=B118–B143|doi=10.1137/15M1014164}}</ref>\n:<math>\\prod_{n=1}^k\\cos\\left(\\frac{x}{2^n}\\right)=\\frac{1}{2^{k-1}}\\sum_{n=1}^{2^{k-1}}\\cos\\left(\\frac{n-1/2}{2^{k-1}}x \\right)\\,,\\qquad \\qquad \\forall k\\ge 1\\,,</math>\nthe Euler's product can be recast as a sum\n:<math>\\frac{\\sin(x)}{x} = \\lim_{N\\to\\infty}\\frac{1}{N}\\sum_{n=1}^N\\cos\\left(\\frac{n-1/2}{N}x \\right)\\,.</math>\n\nThe [[continuous Fourier transform]] of the normalized sinc (to ordinary frequency) is {{math|[[rectangular function|rect]]( ''f'' )}},\n:<math>\\int_{-\\infty}^\\infty \\operatorname{sinc}(t) \\, e^{-i 2 \\pi f t}\\,dt = \\operatorname{rect}(f)~,</math>\nwhere the [[rectangular function]] is 1 for argument between −{{sfrac|1|2}} and {{sfrac|1|2}}, and zero otherwise. This corresponds to the fact that the [[sinc filter]] is the ideal ([[brick-wall filter|brick-wall]], meaning rectangular frequency response) [[low-pass filter]].\n\nThis Fourier integral, including the special case\n:<math>\\int_{-\\infty}^\\infty \\frac{\\sin(\\pi x)}{\\pi x} \\, dx = \\operatorname{rect}(0) = 1\\,\\!</math>\nis an [[improper integral]] (cf. [[Dirichlet integral]]) and not a convergent [[Lebesgue integral]], as\n:<math>\\int_{-\\infty}^\\infty \\left|\\frac{\\sin(\\pi x)}{\\pi x} \\right|\\, dx = +\\infty ~.</math>\n\nThe normalized sinc function has properties that make it ideal in relationship to [[interpolation]] of [[sampling (signal processing)|sampled]] [[bandlimited]] functions:\n* It is an interpolating function, i.e., {{math|sinc(0) {{=}} 1}}, and {{math|sinc(''k'') {{=}} 0}} for nonzero [[Number#Integers|integer]] {{math|''k''}}.\n* The functions {{math|''x<sub>k</sub>''(''t'') {{=}} sinc(''t'' − ''k'')}} ({{math|''k''}} integer) form an [[orthonormal basis]] for [[bandlimited]] functions in the [[Lp space|function space]] {{math|'''''L'''''<sup>2</sup>('''R''')}}, with highest angular frequency {{math|''ω''<sub>H</sub> {{=}} π}} (that is, highest cycle frequency {{math|''f''<sub>H</sub> {{=}} {{sfrac|1|2}}}}).\n\nOther properties of the two sinc functions include:\n* The unnormalized sinc is the zeroth-order spherical [[Bessel function]] of the first kind, {{math|''j''<sub>0</sub>(''x'')}}.  The normalized sinc is  {{math| ''j''<sub>0</sub>(π''x'')}}.\n* <math> \\int_0^x \\frac{\\sin(\\theta)}{\\theta}\\,d\\theta = \\operatorname{Si}(x) \\,\\!</math>\n:where {{math|Si(''x'')}} is the [[sine integral]].\n* {{math|''λ'' sinc(''λx'')}} (not normalized) is one of two linearly independent solutions to the linear [[ordinary differential equation]]\n::<math>x \\frac{d^2 y}{d x^2} + 2 \\frac{d y}{d x} + \\lambda^2 x y = 0.\\,\\!</math>\n:The other is {{math|{{sfrac|cos(''λx'')|''x''}}}}, which is not bounded at {{math|''x'' {{=}} 0}}, unlike its sinc function counterpart.\n* <math> \\int_{-\\infty}^\\infty \\frac{\\sin^2(\\theta)}{\\theta^2}\\,d\\theta = \\pi \\,\\! \\rightarrow \\int_{-\\infty}^\\infty \\operatorname{sinc}^2(x)\\,dx = 1~, </math>\n:where the normalized sinc is meant.\n* <math> \\int_{-\\infty}^\\infty \\frac{\\sin(\\theta)}{\\theta}\\,d\\theta = \\int_{-\\infty}^\\infty \\left ( \\frac{\\sin(\\theta)}{\\theta} \\right )^2 \\,d\\theta = \\pi \\,\\!</math>\n* <math> \\int_{-\\infty}^\\infty \\frac{\\sin^3(\\theta)}{\\theta^3}\\,d\\theta = \\frac{3\\pi}{4} \\,\\!</math>\n* <math> \\int_{-\\infty}^\\infty \\frac{\\sin^4(\\theta)}{\\theta^4}\\,d\\theta = \\frac{2\\pi}{3} ~.</math>\n* The following improper integral involves the (not normalized) sinc function:\n* <math> \\int_{0}^{\\infty} \\frac{dx}{x^n+1} = 1+2\\sum_{k=1}^{\\infty} \\frac{(-1)^{k+1}}{(kn)^2-1} = \\frac{1}{\\operatorname{sinc}(\\frac{\\pi}{n})} </math>\n\n== Relationship to the Dirac delta distribution ==\n\nThe normalized sinc function can be used as a ''[[Dirac delta function#Representations of the delta function|nascent delta function]]'', meaning that the following [[weak convergence (Hilbert space)|weak limit]] holds,\n\n:<math>\\lim_{a\\rightarrow 0}\\frac{\\sin\\left(\\frac{\\pi x}{a}\\right)}{\\pi x} = \\lim_{a\\rightarrow 0}\\frac{1}{a}\\operatorname{sinc}\\left(\\frac{x}{a}\\right)=\\delta(x)~.</math>\n\nThis is not an ordinary limit, since the left side does not converge.  Rather, it means that\n\n:<math>\\lim_{a\\rightarrow 0}\\int_{-\\infty}^\\infty \\frac{1}{a} \\operatorname{sinc}\\left(\\frac{x}{a}\\right)\\varphi(x)\\,dx\n            = \\varphi(0)~,</math>\n\nfor every [[Schwartz space|Schwartz function]], as can be seen from the [[Fourier inversion theorem]].\nIn the above expression, as {{math|''a'' → 0}}, the number of oscillations per unit length of the sinc function approaches infinity. Nevertheless, the expression always oscillates inside an envelope of  {{math|±{{sfrac|1|π''x''}}}}, regardless of the value of {{mvar|a}}.\n\nThis complicates the informal picture of {{math|''δ''(''x'')}} as being zero for all {{mvar|x}}  except at the point {{math|''x'' {{=}} 0}}, and illustrates the problem of thinking of the delta function as a function rather than as a distribution. A similar situation is found in the [[Gibbs phenomenon]].\n\n== Summation ==\nAll sums in this section refer to the unnormalized sinc function.\n\nThe sum of {{math|sinc(''n'')}} over integer {{mvar|n}} from 1 to {{math|∞}} equals {{math|{{sfrac|{{pi}} − 1|2}}}}.\n\n:<math>\\sum_{n=1}^\\infty \\operatorname{sinc}(n) = \\operatorname{sinc}(1) + \\operatorname{sinc}(2) + \\operatorname{sinc}(3) + \\operatorname{sinc}(4) +\\cdots = \\frac{\\pi-1}{2}</math>\n\nThe sum of the squares also equals {{math|{{sfrac|{{pi}} − 1|2}}}}.<ref name=\"BBB\">{{cite journal\n|author1=Robert Baillie|author2-link=David Borwein|author2=David Borwein|author3-link=Jonathan M. Borwein|author3=Jonathan M. Borwein\n|title=Surprising Sinc Sums and Integrals\n|journal=American Mathematical Monthly\n|date=December 2008|volume=115|issue=10|pages=888–901 |jstor = 27642636\n|doi=10.1080/00029890.2008.11920606\n}}</ref>\n\n:<math>\\sum_{n=1}^\\infty \\operatorname{sinc}^2(n) = \\operatorname{sinc}^2(1) + \\operatorname{sinc}^2(2) + \\operatorname{sinc}^2(3) + \\operatorname{sinc}^2(4) +\\cdots = \\frac{\\pi-1}{2}</math>\n\nWhen the signs of the [[addend]]s alternate and begin with +, the sum equals {{sfrac|1|2}}.\n:<math>\\sum_{n=1}^\\infty (-1)^{n+1}\\,\\operatorname{sinc}(n) = \\operatorname{sinc}(1) - \\operatorname{sinc}(2) + \\operatorname{sinc}(3) - \\operatorname{sinc}(4) +\\cdots = \\frac{1}{2}</math>\n\nThe alternating sums of the squares and cubes also equal {{sfrac|1|2}}.<ref name=\"FWFS\">{{cite arXiv |last=Baillie |first=Robert |eprint=0806.0150v2 |class=math.CA |title=Fun with Fourier series |date=2008}}</ref>\n:<math>\\sum_{n=1}^\\infty (-1)^{n+1}\\,\\operatorname{sinc}^2(n) = \\operatorname{sinc}^2(1) - \\operatorname{sinc}^2(2) + \\operatorname{sinc}^2(3) - \\operatorname{sinc}^2(4) +\\cdots = \\frac{1}{2}</math>\n\n:<math>\\sum_{n=1}^\\infty (-1)^{n+1}\\,\\operatorname{sinc}^3(n) = \\operatorname{sinc}^3(1) - \\operatorname{sinc}^3(2) + \\operatorname{sinc}^3(3) - \\operatorname{sinc}^3(4) +\\cdots = \\frac{1}{2}</math>\n\n== Series expansion ==\nUnnormalized {{math|sinc(''x'')}}:\n\n:<math> \\operatorname{sinc}(x) = \\frac{\\sin(x)}{x} = \\sum_{n=0}^\\infty \\frac{\\left( -x^2 \\right)^n}{(2n+1)!} </math>\n\n== Higher dimensions ==\nThe product of 1-D sinc functions readily provides a [[multivariable calculus|multivariate]] sinc function for the square, Cartesian, grid ([[Lattice graph|lattice]]): {{math|sinc<sub>C</sub>(''x'', ''y'') {{=}} sinc(''x'')sinc(''y'')}} whose [[Fourier transform]] is the [[indicator function]] of a square in the frequency space (i.e., the brick wall defined in 2-D space). The sinc function for a non-Cartesian [[Lattice (group)|lattice]] (e.g., [[hexagonal lattice]]) is a function whose [[Fourier transform]] is the [[indicator function]] of the [[Brillouin zone]] of that lattice. For example, the sinc function for the hexagonal lattice is a function whose [[Fourier transform]] is the [[indicator function]] of the unit hexagon in the frequency space. For a non-Cartesian lattice this function can not be obtained by a simple tensor-product. However, the explicit formula for the sinc function for the [[hexagonal lattice|hexagonal]], [[body centered cubic]], [[face centered cubic]] and other higher-dimensional lattices can be explicitly derived<ref name=\"multiD\">{{cite journal| last1=Ye|first1=  W.|last2= Entezari|first2= A. |title=A Geometric Construction of Multivariate Sinc Functions|journal=IEEE Transactions on Image Processing|volume=21|issue=6|pages=2969–2979|date= June 2012| doi=10.1109/TIP.2011.2162421| pmid=21775264|bibcode=2012ITIP...21.2969Y}}</ref> using the geometric properties of [[Brillouin zone]]s and their connection to [[zonohedron|zonotopes]].\n\nFor example, a [[hexagonal lattice]] can be generated by the (integer) [[linear span]] of the vectors\n\n:<math>\n  \\mathbf{u}_1 = \\begin{bmatrix} \\frac{1}{2} \\\\[4pt]  \\frac{\\sqrt{3}}{2} \\end{bmatrix} \\quad \\text{and} \\quad\n  \\mathbf{u}_2 = \\begin{bmatrix} \\frac{1}{2} \\\\[4pt] -\\frac{\\sqrt{3}}{2} \\end{bmatrix}.\n</math>\n\nDenoting\n\n:<math>\n  \\boldsymbol{\\xi}_1 =  \\tfrac{2}{3} \\mathbf{u}_1, \\quad\n  \\boldsymbol{\\xi}_2 =  \\tfrac{2}{3} \\mathbf{u}_2, \\quad\n  \\boldsymbol{\\xi}_3 = -\\tfrac{2}{3} (\\mathbf{u}_1 + \\mathbf{u}_2), \\quad\n          \\mathbf{x} = \\begin{bmatrix} x\\\\ y\\end{bmatrix},\n</math>\n\none can derive<ref name=\"multiD\" /> the sinc function for this hexagonal lattice as:\n\n:<math>\\begin{align}\n  \\operatorname{sinc}_\\text{H}(\\mathbf{x}) = \\tfrac{1}{3} \\big(\n    &      \\cos\\left(\\pi\\boldsymbol{\\xi}_1\\cdot\\mathbf{x}\\right) \\operatorname{sinc}\\left(\\boldsymbol{\\xi}_2\\cdot\\mathbf{x}\\right) \\operatorname{sinc}\\left(\\boldsymbol{\\xi}_3\\cdot\\mathbf{x}\\right) \\\\\n    & {} + \\cos\\left(\\pi\\boldsymbol{\\xi}_2\\cdot\\mathbf{x}\\right) \\operatorname{sinc}\\left(\\boldsymbol{\\xi}_3\\cdot\\mathbf{x}\\right) \\operatorname{sinc}\\left(\\boldsymbol{\\xi}_1\\cdot\\mathbf{x}\\right) \\\\\n    & {} + \\cos\\left(\\pi\\boldsymbol{\\xi}_3\\cdot\\mathbf{x}\\right) \\operatorname{sinc}\\left(\\boldsymbol{\\xi}_1\\cdot\\mathbf{x}\\right) \\operatorname{sinc}\\left(\\boldsymbol{\\xi}_2\\cdot\\mathbf{x}\\right)\n  \\big)\n\\end{align}</math>\n\nThis construction can be used to design [[Lanczos window]] for general multidimensional lattices.<ref name=\"multiD\" />\n\n== See also ==\n* [[Anti-aliasing filter]]\n* [[Sinc filter]]\n* [[Lanczos resampling]]\n* [[Whittaker–Shannon interpolation formula]]\n* [[Shannon wavelet]]\n* [[Winkel tripel projection]] (cartography)\n* [[Trigonometric integral]]\n* [[Trigonometric functions of matrices]]\n* [[Borwein integral]]\n* [[Dirichlet integral]]\n\n== References ==\n{{Reflist|30em}}\n\n== External links ==\n* {{MathWorld|title=Sinc Function|urlname=SincFunction}}\n\n[[Category:Signal processing]]\n[[Category:Elementary special functions]]"
    },
    {
      "title": "Sombrero function",
      "url": "https://en.wikipedia.org/wiki/Sombrero_function",
      "text": "[[File:Sombrero function 3d.png|thumb|Sombrero function 3D]]\nA '''sombrero function''' (sometimes called '''besinc function''' or '''jinc function'''<ref name=\"Blahut\"/>) is the 2-dimensional [[polar coordinate]] analog of the [[sinc function]], and is so-called because it is shaped like a [[sombrero]] hat.  This function is frequently used in [[image processing]].<ref name=\"Hendee\"/> It can be defined through the [[Bessel function|Bessel function of the first kind]] where {{math|''ρ''<sup>2</sup> {{=}} ''x''<sup>2</sup> + ''y''<sup>2</sup>}}.\n\n: <math>\\operatorname{somb} (\\rho) = \\frac{2 J_1(\\pi \\rho)}{\\pi \\rho}</math>.\n\nThe normalization factor {{math|2}} makes {{math|somb(0) {{=}} 1}}. Sometimes the {{math|''π''}} factor is omitted, giving the following alternative definition:\n\n: <math>\\operatorname{somb} (\\rho) = \\frac{2 J_1(\\rho)}{\\rho}</math>.\n\nThe factor of 2 is also often omitted, giving yet another definition and causing the function maximum to be 0.5:<ref name=\"Wolfram\"/>\n\n: <math>\\operatorname{somb} (\\rho) = \\frac{ J_1(\\rho)}{\\rho}</math>.\n\n==References==\n{{reflist|refs=\n<ref name=\"Blahut\">{{cite book\n  | title     = Theory of Remote Image Formation\n  | author    = Richard E. Blahut\n  | page      = 82\n  | date      = 2004-11-18\n  | publisher = [[Cambridge University Press]]\n  | isbn      = 9781139455305\n  | url       = https://books.google.com/books?id=d8FMlHewYp0C&pg=PA82&dq=%22jinc+function%22\n  }}</ref>\n<ref name=\"Hendee\">{{cite book\n  | url    = https://books.google.com/books?id=T_KNSWU4uz4C&pg=PA204\n  | title  = The perception of visual information\n  | author = William R. Hendee, Peter Neil Temple Wells\n  | page   = 204\n  | date   = 1997-06-27\n  | isbn   = 978-0-387-94910-9\n  }}</ref>\n<ref name=Wolfram>{{cite web\n |url=http://mathworld.wolfram.com/JincFunction.html\n |title= Jinc Function\n |last= Weisstein\n |first= Eric W. \n |date= \n |website= MathWorld--A Wolfram Web Resource\n |publisher=\n |access-date= 1 Jan 2019\n |quote= }}</ref>\n\n\n\n}}\n\n[[Category:Signal processing]]\n[[Category:Elementary special functions]]"
    },
    {
      "title": "Square root",
      "url": "https://en.wikipedia.org/wiki/Square_root",
      "text": "{{Redirect|Square roots|other uses|Square Roots (disambiguation)}}\n\n[[File:Nuvola apps edu mathematics blue-p.svg|thumb|right|168px|The mathematical expression \"The (principal) square root of x\"]]\n[[Image:Five Squared.svg|thumb|right|168px<!-- at 160px and 200px lines render with unequal widths -->|For example, {{math|{{sqrt|25}} {{=}} 5}}, since {{math|25 {{=}} 5 &sdot; 5}}, or {{math|5<sup>2</sup>}} (5 squared).]]\n\nIn [[mathematics]], a '''square root''' of a number ''a'' is a number ''y'' such that {{nowrap|1=''y''<sup>2</sup> = ''a''}}; in other words, a number ''y'' whose ''[[square (algebra)|square]]'' (the result of multiplying the number by itself, or {{nowrap|''y'' &sdot; ''y''}}) is ''a''.<ref>Gel'fand, [https://books.google.com/books?id=Z9z7iliyFD0C&pg=PA120 p. 120] {{webarchive|url=https://web.archive.org/web/20160902151740/https://books.google.com/books?id=Z9z7iliyFD0C&pg=PA120 |date=2016-09-02 }}\n</ref> For example, 4 and −4 are square roots of 16 because {{nowrap|1=4<sup>2</sup> = (−4)<sup>2</sup> = 16}}.\nEvery nonnegative [[real number]] ''a'' has a unique nonnegative square root, called the ''principal square root'', which is denoted by {{sqrt|''a''}}, where √ is called the ''[[radical sign]]'' or ''radix''. For example, the principal square root of 9 is 3, which is denoted by {{sqrt|9}} = 3, because {{nowrap|1=3<sup>2</sup> = 3 · 3 = 9}} and 3 is nonnegative.  The term (or number) whose square root is being considered is known as the ''radicand''. The radicand is the number or expression underneath the radical sign, in this example 9.\n\nEvery [[positive number]] ''a'' has two square roots: {{sqrt|''a''}}, which is positive, and −{{sqrt|''a''}}, which is negative. Together, these two roots are denoted as ±{{sqrt|''a''}} (see [[Plus-minus_sign|±&nbsp;shorthand]]). Although the principal square root of a positive number is only one of its two square roots, the designation \"''the'' square root\" is often used to refer to the ''principal square root''. For positive ''a'', the principal square root can also be written in [[Exponentiation|exponent]] notation, as ''a''<sup>1/2</sup>.<ref>{{cite book |title=A First Course in Complex Analysis With Applications |edition=2nd |first1=Dennis G. |last1=Zill |first2=Patrick |last2=Shanahan |publisher=Jones & Bartlett Learning |year=2008 |isbn=0-7637-5772-1 |page=78 |url=https://books.google.com/books?id=YKZqY8PCNo0C |deadurl=no |archiveurl=https://web.archive.org/web/20160901081936/https://books.google.com/books?id=YKZqY8PCNo0C |archivedate=2016-09-01 |df= }} [https://books.google.com/books?id=YKZqY8PCNo0C&pg=PA78 Extract of page 78] {{webarchive|url=https://web.archive.org/web/20160901091148/https://books.google.com/books?id=YKZqY8PCNo0C&pg=PA78 |date=2016-09-01 }}</ref>\n\nSquare roots of negative numbers can be discussed within the framework of [[complex number]]s. More generally, square roots can be considered in any context in which a notion of \"squaring\" of some mathematical objects is defined (including [[Matrix (mathematics)|algebras of matrices]], [[endomorphism ring]]s, etc.)\n\n==History==\nThe [[Yale Babylonian Collection]] [[YBC&nbsp;7289]] clay tablet was created between 1800&nbsp;BC and 1600&nbsp;BC, showing {{sqrt|2}} and {{sqrt|2}}/2 = 1/{{sqrt|2}}  as 1;24,51,10 and 0;42,25,35 [[Sexagesimal |base 60]] numbers on a square crossed by two diagonals.<ref>{{cite web|url=http://www.math.ubc.ca/~cass/Euclid/ybc/analysis.html|title=Analysis of YBC 7289|work=ubc.ca|accessdate=19 January 2015}}</ref> (1;24,51,10) base 60 corresponds to 1.41421296 which is a correct value to 5 decimal points (1.41421356...).\n\nThe [[Rhind Mathematical Papyrus]] is a copy from 1650&nbsp;BC of an earlier [[Berlin Papyrus 6619|Berlin Papyrus]] and other texts{{snd}}possibly the [[Kahun Papyrus]]{{snd}}that shows how the Egyptians extracted square roots by an inverse proportion method.<ref>Anglin, W.S. (1994). ''Mathematics: A Concise History and Philosophy''. New York: Springer-Verlag.</ref>\n\nIn [[History of India|Ancient India]], the knowledge of theoretical and applied aspects of square and square root was at least as old as the ''[[Sulba Sutras]]'', dated around 800–500&nbsp;BC (possibly much earlier).{{citation needed|date=July 2010|reason=no manuscript dates back that far and reliable secondary sources disagree}} A method for finding very good approximations to the square roots of 2 and 3 are given in the ''[[Baudhayana Sulba Sutra]]''.<ref>Joseph, ch.8.</ref>  [[Aryabhata]] in the ''[[Aryabhatiya]]'' (section 2.4), has given a method for finding the square root of numbers having many digits.\n\nIt was known to the ancient Greeks that square roots of [[Natural number|positive whole numbers]] that are not [[Square number|perfect square]]s are always [[irrational number]]s: numbers not expressible as a [[ratio]] of two integers (that is to say they cannot be written exactly as ''m/n'', where ''m'' and ''n'' are integers). This is the theorem [[Euclid's Elements|''Euclid X, 9'']] almost certainly due to [[Theaetetus (mathematician)|Theaetetus]] dating back to circa 380&nbsp;BC.<ref>{{cite book\n|first= Sir Thomas L. \n|last= Heath\n|editor= \n|title= The Thirteen Books of The Elements, Vol. 3\n|url=https://archive.org/stream/thirteenbookseu03heibgoog#page/n14/mode/1up\n|year=1908\n|publisher=Cambridge University Press\n|page=3\n}}</ref>\nThe particular case [[Square root of 2|{{sqrt|2}}]] is assumed to date back earlier to the [[Pythagoreanism|Pythagoreans]] and is traditionally attributed to [[Hippasus]].{{Citation needed|date=October 2012}} It is exactly the length of the [[diagonal]] of a [[unit square|square with side length 1]].\n\nIn the Chinese mathematical work ''[[Suàn shù shū|Writings on Reckoning]]'', written between 202&nbsp;BC and 186&nbsp;BC during the early [[Han Dynasty]], the square root is approximated by using an \"excess and deficiency\" method, which says to \"...combine the excess and deficiency as the divisor; (taking) the deficiency numerator multiplied by the excess denominator and the excess numerator times the deficiency denominator, combine them as the dividend.\"<ref>Dauben (2007), p. 210.</ref>\n\nA symbol for square roots, written as an elaborate R, was invented by [[Regiomontanus]] (1436–1476). An R was also used for Radix to indicate square roots in [[Gerolamo Cardano]]'s ''[[Ars Magna (Gerolamo Cardano)|Ars Magna]]''.<ref>{{cite web|url=http://nrich.maths.org/6546|title=The Development of Algebra - 2|work=maths.org|accessdate=19 January 2015|deadurl=no|archiveurl=https://web.archive.org/web/20141124102946/http://nrich.maths.org/6546|archivedate=24 November 2014|df=}}</ref>\n\nAccording to historian of mathematics [[David Eugene Smith|D.E. Smith]], Aryabhata's method for finding the square root was first introduced in Europe by [[Pietro di Giacomo Cataneo|Cataneo]] in 1546.\n\nAccording to Jeffrey A. Oaks, Arabs used the letter ''[[Gimel#Arabic ĝīm|jīm/ĝīm]]'' ({{lang|ar|ج}}), the first letter of the word “{{lang|ar|جذر}}” (variously transliterated as ''jaḏr'', ''jiḏr'', ''ǧaḏr'' or ''ǧiḏr'', “root”), placed in its initial form ({{lang|ar|&#65183;}}) over a number to indicate its square root. The letter ''jīm'' resembles the present square root shape. Its usage goes as far as the end of the twelfth century  in the works of the Moroccan mathematician [[Ibn al-Yasamin]].<ref>* {{cite thesis | title=Algebraic Symbolism in Medieval Arabic Algebra | first1=Jeffrey A. | last1=Oaks | publisher=Philosophica | year=2012 | page=36 | url=http://logica.ugent.be/philosophica/fulltexts/87-2.pdf | deadurl=no | archiveurl=https://web.archive.org/web/20161203134229/http://logica.ugent.be/philosophica/fulltexts/87-2.pdf | archivedate=2016-12-03 | df= }}</ref>\n\nThe symbol '√' for the square root was first used in print in 1525 in [[Christoph Rudolff]]'s ''Coss''.<ref>{{Cite book| last=Manguel|first=Alberto| chapter=Done on paper: the dual nature of numbers and the page | title=The Life of Numbers | year=2006 | isbn=84-86882-14-1}}</ref>\n\n==Properties and uses==\n[[Image:Square root 0 25.svg|thumb|400px|The graph of the function ''f''(''x'') = {{sqrt|''x''}}, made up of half a [[parabola]] with a vertical [[Directrix (conic section)#Eccentricity, focus and directrix|directrix]] ]]\nThe principal square root function ''f''(''x'') = {{sqrt|''x''}} (usually just referred to as the \"square root function\") is a [[function (mathematics)|function]] that maps the [[Set (mathematics)|set]] of nonnegative real numbers onto itself. In [[geometry|geometrical]] terms, the square root function maps the [[area]] of a square to its side length.\n\nThe square root of ''x'' is rational if and only if ''x'' is a [[rational number]] that can be represented as a ratio of two perfect squares. (See [[square root of 2]] for proofs that this is an irrational number, and [[quadratic irrational]] for a proof for all non-square natural numbers.) The square root function maps rational numbers into [[algebraic number]]s (a [[superset]] of the rational numbers).\n\nFor all real numbers ''x'',\n\n:<math>\n\\sqrt{x^2} = \\left|x\\right| = \n\\begin{cases} \n  x,  & \\mbox{if }x \\ge 0 \\\\\n  -x, & \\mbox{if }x < 0. \n\\end{cases}\n</math> &nbsp;&nbsp;&nbsp;&nbsp;(see [[absolute value]])\n\nFor all nonnegative real numbers ''x'' and ''y'',\n\n:<math>\\sqrt{xy} = \\sqrt x \\sqrt y</math>\n\nand\n\n:<math>\\sqrt x = x^{1/2}.</math>\n\nThe square root function is [[Continuous function|continuous]] for all nonnegative ''x'' and [[derivative|differentiable]] for all positive ''x''. If ''f'' denotes the square root function, its derivative is given by:\n:<math>f'(x) = \\frac{1}{2\\sqrt x}.</math>\n\nThe [[Taylor series]] of {{sqrt|1 + ''x''}} about ''x'' = 0 converges for {{abs|''x''}} ≤ 1 and is given by\n\n:<math>\\sqrt{1 + x} = \\sum_{n=0}^\\infty \\frac{(-1)^n(2n)!}{(1-2n)(n!)^2(4^n)}x^n = 1 + \\frac{1}{2}x - \\frac{1}{8}x^2 + \\frac{1}{16} x^3 - \\frac{5}{128} x^4 + \\cdots,</math>\n\nThe square root of a nonnegative number is used in the definition of [[Euclidean norm]] (and [[Euclidean distance|distance]]), as well as in generalizations such as [[Hilbert space]]s. It defines an important concept of [[standard deviation]] used in probability theory and statistics. It has a major use in the formula for roots of a [[quadratic equation]]; [[quadratic field]]s and rings of [[quadratic integer]]s, which are based on square roots, are important in algebra and have uses in geometry. Square roots frequently appear in mathematical formulas elsewhere, as well as in many [[physics|physical]] laws.\n\n==Computation==\n{{Main article|Methods of computing square roots}}\nSquare roots of positive numbers are not in general [[rational number]]s, and so cannot be written as a terminating or recurring decimal expression. Therefore in general any attempt to compute a square root expressed in decimal form can only yield an approximation, though a sequence of increasingly accurate approximations can be obtained.\n\nMost [[pocket calculator]]s have a square root key. Computer [[spreadsheet]]s and other [[software]] are also frequently used to calculate square roots. Pocket calculators typically implement efficient routines, such as the [[Newton's method]] (frequently with an initial guess of 1), to compute the square root of a positive real number.<ref>{{cite book|last=Parkhurst|first=David F.|title=Introduction to Applied Mathematics for Environmental Science|year=2006|publisher=Springer|isbn=9780387342283|pages=241}}</ref><ref>{{cite book|last=Solow|first=Anita E.|title=Learning by Discovery: A Lab Manual for Calculus|year=1993|publisher=Cambridge University Press|isbn=9780883850831|pages=48}}</ref>  When computing square roots with [[Common logarithm|logarithm table]]s or [[slide rule]]s, one can exploit the identities\n\n:<math>\\sqrt{a} = e^{(\\ln a)/2} = 10^{(\\log_{10} a)/2},</math>\n\nwhere {{math|ln}} and {{math|log}}<sub>10</sub> are the [[Natural logarithm|natural]] and [[base-10 logarithm]]s.\n\nBy trial-and-error,<ref>{{cite book |title=Mathematics for Biological Scientists |first1=Mike |last1=Aitken |first2=Bill |last2=Broadhurst |first3=Stephen |last3=Hladky |publisher=Garland Science |year=2009 |isbn=978-1-136-84393-8 |page=41 |url=https://books.google.com/books?id=KywWBAAAQBAJ |deadurl=no |archiveurl=https://web.archive.org/web/20170301101038/https://books.google.com/books?id=KywWBAAAQBAJ |archivedate=2017-03-01 |df= }} [https://books.google.com/books?id=KywWBAAAQBAJ&pg=PA41 Extract of page 41] {{webarchive|url=https://web.archive.org/web/20170301100516/https://books.google.com/books?id=KywWBAAAQBAJ&pg=PA41 |date=2017-03-01 }}</ref> one can square an estimate for {{sqrt|''a''}} and raise or lower the estimate until it agrees to sufficient accuracy. For this technique it is prudent to use the identity\n\n:<math>(x + c)^2 = x^2 + 2xc + c^2,</math>\n\nas it allows one to adjust the estimate ''x'' by some amount ''c'' and measure the square of the adjustment in terms of the original estimate and its square. Furthermore, (''x'' + ''c'')<sup>2</sup> ≈ ''x''<sup>2</sup> + 2''xc'' when ''c'' is close to 0, because the [[tangent line]] to the graph of ''x''<sup>2</sup> + 2''xc'' + ''c''<sup>2</sup> at ''c'' = 0, as a function of ''c'' alone, is ''y'' = 2''xc'' + ''x''<sup>2</sup>. Thus, small adjustments to ''x'' can be planned out by setting 2''xc'' to ''a'', or ''c'' = ''a''/(2''x'').\n\nThe most common [[iterative method]] of square root calculation by hand is known as the \"[[Babylonian method]]\" or \"Heron's method\" after the first-century Greek philosopher [[Hero of Alexandria|Heron of Alexandria]], who first described it.<ref>{{cite book\n  | last = Heath\n  | first = Sir Thomas L.\n  | authorlink = \n  | coauthors = \n  | title = A History of Greek Mathematics, Vol. 2\n  | publisher = Clarendon Press\n  | year = 1921\n  | location = Oxford\n  | pages = 323–324\n  | url = https://books.google.com/?id=LOA5AAAAMAAJ&pg=PR323\n  | doi = \n  | id = \n  | isbn = }}</ref>\nThe method uses the same iterative scheme as the [[Newton–Raphson method]] yields when applied to the function y = ''f''(''x'') = ''x''<sup>2</sup> − ''a'', using the fact that its slope at any point is ''dy''/''dx'' = ''{{prime|f}}''(''x'') = 2''x'', but predates it by many centuries.<ref>{{Cite book\n |title      = Elementary functions: algorithms and implementation\n |first1     = Jean-Mic\n |last1      = Muller\n |publisher  = Springer\n |year       = 2006\n |isbn       = 0-8176-4372-9\n |pages      = 92–93\n |url        = https://books.google.com/?id=g3AlWip4R38C\n |postscript = <!--None-->\n}}, [https://books.google.com/books?id=g3AlWip4R38C&pg=PA92 Chapter 5, p 92] {{webarchive|url=https://web.archive.org/web/20160901091516/https://books.google.com/books?id=g3AlWip4R38C&pg=PA92 |date=2016-09-01 }}\n</ref>\nThe algorithm is to repeat a simple calculation that results in a number closer to the actual square root each time it is repeated with its result as the new input. The motivation is that if ''x'' is an overestimate to the square root of a nonnegative real number ''a'' then ''a''/''x''<!-- please avoid intermixing <math> with a bare text in one paragraph: it is the ugliest pair among 3 existing styles --> will be an underestimate and so the average of these two numbers is a better approximation than either of them. However, the [[inequality of arithmetic and geometric means]] shows this average is always an overestimate of the square root (as noted [[Square root#Geometric construction of the square root|below]]), and so it can serve as a new overestimate with which to repeat the process, which [[Limit of a sequence|converges]] as a consequence of the successive overestimates and underestimates being closer to each other after each iteration. To find ''x'':\n\n# Start with an arbitrary positive start value ''x''. The closer to the square root of ''a'', the fewer the iterations that will be needed to achieve the desired precision.\n# Replace ''x'' by the average (''x'' + ''a''/''x'') / 2 between ''x'' and ''a''/''x''.\n# Repeat from step 2, using this average as the new value of ''x''.\n\nThat is, if an arbitrary guess for {{sqrt|''a''}} is ''x''<sub>0</sub>, and {{nowrap|1 = ''x''<sub>''n'' + 1</sub> = (''x<sub>n</sub>'' + ''a''/''x<sub>n</sub>'') / 2}}, then each x<sub>n</sub> is an approximation of {{sqrt|''a''}} which is better for large ''n'' than for small ''n''. If ''a'' is positive, the convergence is [[Rate of convergence|quadratic]], which means that in approaching the limit, the number of correct digits roughly doubles in each next iteration. If {{nowrap|1 =''a'' = 0}}, the convergence is only linear.\n\nUsing the identity\n\n:<math>\\sqrt{a} = 2^{-n}\\sqrt{4^n a},</math>\n\nthe computation of the square root of a positive number can be reduced to that of a number in the range {{closed-open|1,4}}. This simplifies finding a start value for the iterative method that is close to the square root, for which a [[Polynomial function|polynomial]] or [[Piecewise linear function|piecewise-linear]] [[Approximation theory|approximation]] can be used.\n\nThe [[Computational complexity theory|time complexity]] for computing a square root with ''n'' digits of precision is equivalent to that of multiplying two ''n''-digit numbers.\n\nAnother useful method for calculating the square root is the [[shifting nth root algorithm]], applied for {{nowrap|1= ''n'' = 2}}.\n\nThe name of the square root [[Function (programming)|function]] varies from [[programming language]] to programming language, with <code>sqrt</code><ref>{{cite web |title=Function sqrt |work=CPlusPlus.com |date=2016 |publisher=The C++ Resources Network |url=http://www.cplusplus.com/reference/clibrary/cmath/sqrt/ |access-date=June 24, 2016 |deadurl=no |archiveurl=https://web.archive.org/web/20121122050619/http://www.cplusplus.com/reference/clibrary/cmath/sqrt/ |archivedate=November 22, 2012 |df= }}</ref> (often pronounced \"squirt\" <ref>{{cite book |title=C++ for the Impatient |first=Brian |last=Overland |page=338 |publisher=Addison-Wesley |date=2013 |isbn=9780133257120 |oclc=850705706 |url=https://books.google.com/books?id=eJFpV-_t4WkC&pg=PA338&dq=%22squirt%22+sqrt+C%2B%2B&hl=en&sa=X&ved=0ahUKEwjEwfj04sHNAhUY0GMKHatGDnsQ6AEIKDAC#v=onepage&q=%22squirt%22%20sqrt%20C%2B%2B&f=false |access-date=June 24, 2016 |deadurl=no |archiveurl=https://web.archive.org/web/20160901082021/https://books.google.com/books?id=eJFpV-_t4WkC&pg=PA338&dq=%22squirt%22+sqrt+C%2B%2B&hl=en&sa=X&ved=0ahUKEwjEwfj04sHNAhUY0GMKHatGDnsQ6AEIKDAC#v=onepage&q=%22squirt%22%20sqrt%20C%2B%2B&f=false |archivedate=September 1, 2016 |df= }}</ref>) being common, used in [[C (programming language)|C]], [[C++]], and derived languages like [[JavaScript]], [[PHP]], and [[Python (programming language)|Python]].\n\n==Square roots of negative and complex numbers==<!-- This section is linked from [[Complex number]] -->\n{{multiple image |align=left |direction=horizontal \n |image1=Complex sqrt leaf1.jpg |caption1=First leaf of the complex square root\n |image2=Complex sqrt leaf2.jpg |caption2=Second leaf of the complex square root\n |image3=Riemann surface sqrt.svg |caption3=Using the [[Riemann surface]] of the square root, it is shown how the two leaves fit together\n}}\n{{clear}}\nThe square of any positive or negative number is positive, and the square of 0 is 0. Therefore, no negative number can have a [[real number|real]] square root.  However, it is possible to work with a more inclusive set of numbers, called the [[complex number]]s, that does contain solutions to the square root of a negative number.  This is done by introducing a new number, denoted by ''i'' (sometimes ''j'', especially in the context of [[electric current|electricity]] where \"''i''\" traditionally represents electric current) and called the [[imaginary unit]], which is ''defined'' such that {{nowrap|1=''i''<sup>2</sup> = −1}}. Using this notation, we can think of ''i'' as the square root of −1, but notice that we also have {{nowrap|1=(−''i'')<sup>2</sup> = ''i''<sup>2</sup> = −1}} and so −''i'' is also a square root of −1. By convention, the principal square root of −1 is ''i'', or more generally, if ''x'' is any nonnegative number, then the principal square root of −''x'' is\n\n:<math>\\sqrt{-x} = i \\sqrt x.</math>\n\nThe right side (as well as its negative) is indeed a square root of −''x'', since\n\n:<math>(i\\sqrt x)^2 = i^2(\\sqrt x)^2 = (-1)x = -x.</math>\n\nFor every non-zero complex number ''z'' there exist precisely two numbers ''w'' such that {{nowrap|1=''w''<sup>2</sup> = ''z''}}: the principal square root of ''z'' (defined below), and its negative.\n\n===Square root of an imaginary number===\n[[Image:Imaginary2Root.svg|right|thumb|The square roots of {{mvar|i}}]]\n\nThere are two complex numbers that square to a given arbitrary non-zero imaginary number <math>z= ai</math> with real <math>a\\ne 0:</math>\n:<math>\\pm\\sqrt{ai}=\\textstyle \\pm\\sqrt{|a|\\sgn(a)i}= \\pm\\sqrt{|a|}\\sqrt{\\sgn(a)i}.</math>\n\nSince <math>\\textstyle\\sqrt{|a|}</math> is a uniquely determined positive real number, determining the square roots of an imaginary number boils down to calculating <math>\\sqrt{\\pm i}.</math>\n\nThe principal square roots of <math>\\pm i</math> are given by\n:<math>\\sqrt{+i} = \\tfrac{1}{\\sqrt{2}} + i\\tfrac{1}{\\sqrt{2}} = \\tfrac{\\sqrt{2}}{2}(1+i)\\quad</math> and\n:<math>\\sqrt{-i} = \\tfrac{1}{\\sqrt{2}} - i\\tfrac{1}{\\sqrt{2}} = \\tfrac{\\sqrt{2}}{2}(1-i)\\quad</math> (selected for positive real part), or\n:<math>\\sqrt{-i} = -\\tfrac{1}{\\sqrt{2}} + i\\tfrac{1}{\\sqrt{2}} = \\tfrac{\\sqrt{2}}{2}(-1+i)\\quad</math> (for least positive argument).\n\n{{Collapse top|title=Derivation of the roots of <math>\\pm i</math>|width=80%}}\n\nThe result can be obtained [[algebra]]ically by finding real numbers ''a'' and ''b'' such that\n\n:<math>\\pm i = (a+bi)^2</math>\n\nor equivalently\n\n:<math>\\pm i = a^2 + 2abi - b^2.</math>\n\nThis gives the two [[simultaneous equations]]\n\n:<math>\\left\\{\\begin{align}\n   2ab &= \\pm 1 \\\\\n   a^2 - b^2 &= 0\n \\end{align}\\right.</math>\n\nwith solutions\n\n:<math>a = b = \\pm \\tfrac{\\sqrt{2}}{2}\\quad</math> for <math>\\sqrt{+i}\\quad</math> and\n:<math>a = -b = \\pm \\tfrac{\\sqrt{2}}{2}\\quad</math> for <math>\\sqrt{-i}.</math>\n\nThe choice of the [[principal root]]s yields the solutions given above.\n\nThe result can also be obtained by using [[de Moivre's formula]] and setting\n\n:<math>\\pm i = \\cos(\\pm\\tfrac{\\pi}{2} + 2k\\pi) + i\\sin(\\pm\\tfrac{\\pi}{2} + 2k\\pi),</math>\n\nfor an arbitrary integer <math>k,</math> which yields\n\n:<math>\\begin{align}\n \\left(\\pm i \\right)^{\\frac 1 2} & \\in \\left\\{ \\left ( \\cos (\\pm \\tfrac{\\pi}{2} + 2k\\pi) + i\\sin(\\pm\\tfrac{\\pi}{2}+ 2k\\pi) \\right )^{\\frac{1}{2}},\\; k \\in \\mathbb Z\\right \\}  \\\\\n              & = \\left\\{ \\cos(\\pm\\tfrac{\\pi}{4}) + i\\sin(\\pm\\tfrac{\\pi}{4}),\\; \\cos(\\pm\\tfrac{\\pi}{4} + \\pi) + i\\sin(\\pm \\tfrac{\\pi}{4} + \\pi)\\right\\} \\\\\n              & = \\left\\{\\tfrac{1}{\\sqrt{2}} \\pm i \\tfrac{1}{\\sqrt{2}},\\;-\\tfrac{1}{\\sqrt{2}} \\mp i \\tfrac{1}{\\sqrt{2}} \\right\\}\\\\\n              & = \\left\\{\\tfrac{\\sqrt{2}}{2}(1\\pm i),\\;\\tfrac{\\sqrt{2}}{2}(-1\\mp i)  \\right\\}. \\\\\n\\end{align}\n</math>\n\n{{cob}}\n\n===Principal square root of a complex number===\n{{Visualisation complex number roots}}\nTo find a definition for the square root that allows us to consistently choose a single value, called the [[principal value]], we start by observing that any complex number ''x'' + ''iy'' can be viewed as a point in the plane, (''x'', ''y''), expressed using [[Cartesian coordinate system|Cartesian coordinates]]. The same point may be reinterpreted using [[polar coordinates]] as the pair <math>(r, \\varphi</math>), where ''r'' ≥ 0 is the distance of the point from the origin, and  <math>\\varphi</math> is the angle that the line from the origin to the point makes with the positive real (''x'') axis. In complex analysis, the location of this point is conventionally written <math>re^{i\\varphi}.</math> If\n\n:<math> z=r e^{i \\varphi} \\text{ with } -\\pi < \\varphi \\le \\pi, </math>\n\nthen we define the principal square root of ''z'' as follows:\n\n:<math>\\sqrt{z} = \\sqrt{r} e^{i \\varphi / 2}.</math>\n\nThe principal square root function is thus defined using the nonpositive real axis as a [[branch cut]].  The principal square root function is [[holomorphic function|holomorphic]] everywhere except on the set of non-positive real numbers (on strictly negative reals it isn't even [[continuous function|continuous]]). The above Taylor series for {{sqrt|1 + ''x''}} remains valid for complex numbers ''x'' with {{nowrap|{{abs|''x''}} < 1}}.\n\nThe above can also be expressed in terms of [[trigonometric function]]s:\n:<math>\\sqrt{r \\left(\\cos \\varphi + i \\sin \\varphi \\right)} = \\sqrt{r} \\left ( \\cos \\frac{\\varphi}{2} + i \\sin \\frac{\\varphi}{2} \\right ) .</math>\n\n===Algebraic formula===\nWhen the number is expressed using Cartesian coordinates the following formula can be used for the principal square root:<ref>{{cite book\n |title       = Handbook of mathematical functions with formulas, graphs, and mathematical tables\n |edition     = \n |first1      = Milton\n |last1       = Abramowitz\n |first2      = Irene A.\n |last2       = Stegun\n |publisher   = Courier Dover Publications\n |year        = 1964\n |isbn        = 0-486-61272-4\n |page        = 17\n |url         = https://books.google.com/books?id=MtU8uP7XMvoC\n |deadurl     = no\n |archiveurl  = https://web.archive.org/web/20160423180235/https://books.google.com/books?id=MtU8uP7XMvoC\n |archivedate = 2016-04-23\n |df          = \n}}, [http://www.math.sfu.ca/~cbm/aands/page_17.htm Section 3.7.27, p. 17] {{webarchive|url=https://web.archive.org/web/20090910094533/http://www.math.sfu.ca/~cbm/aands/page_17.htm |date=2009-09-10 }}\n</ref><ref>{{cite book\n |title       = Classical algebra: its nature, origins, and uses\n |first1      = Roger\n |last1       = Cooke\n |publisher   = John Wiley and Sons\n |year        = 2008\n |isbn        = 0-470-25952-3\n |page        = 59\n |url         = https://books.google.com/books?id=lUcTsYopfhkC&pg=PA59\n |deadurl     = no\n |archiveurl  = https://web.archive.org/web/20160423183239/https://books.google.com/books?id=lUcTsYopfhkC&pg=PA59\n |archivedate = 2016-04-23\n |df          = \n}}\n</ref>\n:<math>\\sqrt{z} = \\sqrt{\\frac{|z| + \\operatorname{Re}(z)}{2}} \\pm i\\ \\sqrt{\\frac{|z| - \\operatorname{Re}(z)}{2}},</math>\nwhere the [[Sign function|sign]] of the imaginary part of the root is taken to be the same as the sign of the imaginary part of the original number, or positive when zero. The real part of the principal value is always nonnegative.\n\n===Notes===\n\nIn the following, the complex ''z'' and ''w'' may be expressed as:\n\n* <math> z=|z|e^{i \\theta_z}</math>\n* <math> w=|w|e^{i \\theta_w}</math>\n\nwhere <math>-\\pi<\\theta_z\\le\\pi</math> and <math>-\\pi<\\theta_w\\le\\pi</math>.\n\nBecause of the discontinuous nature of the square root function in the complex plane, the following laws are '''not true''' in general.\n\n*  <math>\\sqrt{zw} = \\sqrt{z} \\sqrt{w}</math> (counterexample for the principal square root: {{math|1=''z'' = −1}} and  {{math|1=''w'' = −1}}) This equality is valid only when <math>-\\pi<\\theta_z+\\theta_w\\le\\pi</math>\n* <math>\\frac{\\sqrt{w}}{\\sqrt z} = \\sqrt{\\frac{w}{z}}</math> (counterexample for the principal square root: {{math|1=''w'' = 1}} and {{math|1=''z'' = −1}})This equality is valid only when <math>-\\pi<\\theta_w-\\theta_z\\le\\pi</math>\n*<math>\\sqrt{z^*} = \\left( \\sqrt z \\right)^*</math> (counterexample for the principal square root: {{math|1=''z'' = −1}})This equality is valid only when <math>\\theta_z\\ne\\pi</math>\n\nA similar problem appears with other complex functions with branch cuts, e.g., the [[complex logarithm]] and the relations {{math|1=log&thinsp;''z'' + log&thinsp;''w'' = log(''zw'')}} or {{math|1=log&thinsp;(''z''<sup>*</sup>) = log&thinsp;(''z'')<sup>*</sup>}} which are not true in general.\n\nWrongly assuming one of these laws underlies several faulty \"proofs\", for instance the following one showing that {{math|1=−1 = 1}}:\n\n:<math>\n\\begin{align}\n-1 &= i \\cdot i \\\\\n&= \\sqrt{-1} \\cdot \\sqrt{-1} \\\\\n&= \\sqrt{\\left(-1\\right)\\cdot\\left(-1\\right)} \\\\\n&= \\sqrt{1} \\\\\n&= 1\n\\end{align}\n</math>\n\nThe third equality cannot be justified (see [[invalid proof]]). It can be made to hold by changing the meaning of √ so that this no longer represents the principal square root (see above) but selects a branch for the square root that contains ({{sqrt|−1}})·({{sqrt|−1}}). The left-hand side becomes either\n\n:<math>\\sqrt{-1} \\cdot \\sqrt{-1}=i \\cdot i=-1</math>\n\nif the branch includes +''i'' or\n\n:<math>\\sqrt{-1} \\cdot \\sqrt{-1}=(-i) \\cdot (-i)=-1</math>\n\nif the branch includes −''i'', while the right-hand side becomes\n\n:<math>\\sqrt{\\left(-1\\right)\\cdot\\left(-1\\right)}=\\sqrt{1}=-1,</math>\n\nwhere the last equality, {{math|1={{sqrt|1}} = −1}}, is a consequence of the choice of branch in the redefinition of √.\n\n==Square roots of matrices and operators==\n{{Main article|Square root of a matrix}}\n{{See also|Square root of a 2 by 2 matrix}}\n\nIf ''A'' is a [[positive-definite matrix]] or operator, then there exists precisely one positive definite matrix or operator ''B'' with {{nowrap|1=''B''<sup>2</sup> = ''A''}}; we then define {{nowrap|1=''A''<sup>1/2</sup> = ''B''}}. In general matrices may have multiple square roots or even an infinitude of them.  For example, the {{nowrap|2 × 2}} [[identity matrix]] has an infinity of square roots,<ref>Mitchell, Douglas W., \"Using Pythagorean triples to generate square roots of I<sub>2</sub>\", ''Mathematical Gazette'' 87, November 2003, 499–500.</ref> though only one of them is positive definite.\n\n==In integral domains, including fields==\nEach element of an [[integral domain]] has no more than 2 square roots. The [[difference of two squares]] identity {{math|1=''u''<sup>2</sup> − ''v''<sup>2</sup> = (''u'' − ''v'')(''u'' + ''v'')}} is proved using the [[commutative ring|commutativity of multiplication]]. If {{mvar|u}} and {{mvar|v}} are square roots of the same element, then {{math|1=''u''<sup>2</sup> − ''v''<sup>2</sup> = 0}}. Because there are no [[zero divisors]] this implies {{math|1=''u'' = ''v''}} or {{math|1=''u'' + ''v'' = 0}}, where the latter means that two roots are [[additive inverse]]s of each other. In other words if an element a square root {{mvar|u}} of an element {{mvar|a}} exists, then the only square roots of {{mvar|a}} are {{mvar|u}} and {{mvar|&minus;u}}. The only square root of 0 in an integral domain is 0 itself.\n\nIn a field of [[characteristic (algebra)|characteristic]]&nbsp;2, an element either has one square root or does not have any at all, because each element is its own additive inverse, so that {{math|1=&minus;''u'' = ''u''}}. If the field is [[finite field|finite]] of characteristic&nbsp;2 then every element has a unique square root. In a [[field (mathematics)|field]] of any other characteristic, any non-zero element either has two square roots, as explained above, or does not have any.\n\nGiven an odd [[prime number]] {{mvar|p}}, let {{math|1=''q'' = ''p''<sup>''e''</sup>}} for some positive integer {{mvar|e}}. A non-zero element of the field [[finite field|{{math|'''F'''<sub>''q''</sub>}}]] with {{mvar|q}} elements is a [[quadratic residue]] if it has a square root in {{math|'''F'''<sub>''q''</sub>}}. Otherwise, it is a quadratic non-residue. There are {{math|(''q'' − 1)/2}} quadratic residues and {{math|(''q'' − 1)/2}} quadratic non-residues; zero is not counted in either class. The quadratic residues form a [[group (mathematics)|group]] under multiplication. The properties of quadratic residues are widely used in [[number theory]].\n\n==In rings in general==\nUnlike in an integral domain, a square root in an arbitrary (unital) ring need not be unique up to sign.  For example, in the ring <math>\\mathbb{Z}/8\\mathbb{Z}</math> of integers [[modular arithmetic|modulo 8]] (which is commutative, but has zero divisors), the element 1 has four distinct square roots: ±1 and ±3.\n\nAnother example is provided by the ring of [[quaternion]]s <math>\\mathbb{H},</math> which has no zero divisors, but is not commutative.  Here, the element −1 has [[quaternion#Square roots of −1|infinitely many square roots]], including {{math|±''i''}}, {{math|±''j''}}, and {{math|±''k''}}.  In fact, the set of square roots of −1 is exactly\n\n:<math>\\{ai + bj + ck \\mid a^2 + b^2 + c^2 = 1\\} .</math>\n\nA square root of 0 is either 0 or a zero divisor.  Thus in rings where zero divisors do not exist, it is uniquely 0.  However, rings with zero divisors may have multiple square roots of 0.  For example, in <math>\\mathbb{Z}/n^2\\mathbb{Z},</math> any multiple of {{mvar|n}} is a square root of 0.\n\n==Principal square roots of the positive integers==\n\n===As decimal expansions===\nThe square roots of the [[square number|perfect square]]s (0, 1, 4, 9, 16, etc.) are [[integers]]. In all other cases, the square roots of positive integers are [[irrational number]]s, and therefore their [[decimal representation]]s are non-[[repeating decimal]]s.  Decimal approximations of the square roots of the first few natural numbers are given in the following table.\n\n:{|class=\"wikitable\"\n! {{mvar|n}} !! {{math|{{sqrt|''n''}}}}, truncated to 50 decimal places\n|- \n|align=\"right\" | 0 || 0\n|- \n|align=\"right\" | 1 || 1\n|-\n|align=\"right\" | 2 || {{gaps|1.4142135623|7309504880|1688724209|6980785696|7187537694}}\n|-\n|align=\"right\" | 3 || {{gaps|1.7320508075|6887729352|7446341505|8723669428|0525381038}}\n|-\n|align=\"right\" | 4 || 2\n|-\n|align=\"right\" | 5 || {{gaps|2.2360679774|9978969640|9173668731|2762354406|1835961152}}\n|-\n|align=\"right\" | 6 || {{gaps|2.4494897427|8317809819|7284074705|8913919659|4748065667}}\n|-\n|align=\"right\" | 7 || {{gaps|2.6457513110|6459059050|1615753639|2604257102|5918308245}}\n|-\n|align=\"right\" | 8 || {{gaps|2.8284271247|4619009760|3377448419|3961571393|4375075389}}\n|-\n|align=\"right\" | 9 || 3\n|-\n|align=\"right\" | 10 || {{gaps|3.1622776601|6837933199|8893544432|7185337195|5513932521}}\n|-\n|}\n\n===As expansions in other numeral systems===\nThe square roots of the [[square number|perfect square]]s (1, 4, 9, 16, etc.) are integers. In all other cases, the square roots of positive integers are [[irrational number]]s, and therefore their representations in any standard [[positional notation]] system are non-repeating.\n\nThe square roots of small integers are used in both the [[SHA-1]] and [[SHA-2]] hash function designs to provide [[nothing up my sleeve number]]s.\n\n===As periodic continued fractions===\nOne of the most intriguing results from the study of [[irrational number]]s as [[continued fraction]]s was obtained by [[Joseph Louis Lagrange]] {{circa}} 1780. Lagrange found that the representation of the square root of any non-square positive integer as a continued fraction is [[Periodic continued fraction|periodic]]. That is, a certain pattern of partial denominators repeats indefinitely in the continued fraction. In a sense these square roots are the very simplest irrational numbers, because they can be represented with a simple repeating pattern of integers.\n\n:{|\n|- \n|align=\"right\"|{{sqrt|2}}|| = [1; 2, 2, ...]\n|-\n|align=\"right\"|{{sqrt|3}}|| = [1; 1, 2, 1, 2, ...]\n|-\n|align=\"right\"|{{sqrt|4}}|| = [2]\n|-\n|align=\"right\"|{{sqrt|5}}|| = [2; 4, 4, ...]\n|-\n|align=\"right\"|{{sqrt|6}}|| = [2; 2, 4, 2, 4, ...]\n|-\n|align=\"right\"|{{sqrt|7}}|| = [2; 1, 1, 1, 4, 1, 1, 1, 4, ...]\n|-\n|align=\"right\"|{{sqrt|8}}||= [2; 1, 4, 1, 4, ...]\n|-\n|align=\"right\"|{{sqrt|9}}|| = [3]\n|-\n|align=\"right\"|{{sqrt|10}}|| = [3; 6, 6, ...]\n|- \n|align=\"right\"|{{sqrt|11}}|| = [3; 3, 6, 3, 6, ...]\n|-\n|align=\"right\"|{{sqrt|12}}|| = [3; 2, 6, 2, 6, ...]\n|-\n|align=\"right\"|{{sqrt|13}}|| = [3; 1, 1, 1, 1, 6, 1, 1, 1, 1, 6, ...]\n|-\n|align=\"right\"|{{sqrt|14}}|| = [3; 1, 2, 1, 6, 1, 2, 1, 6, ...]\n|-\n|align=\"right\"|{{sqrt|15}}|| = [3; 1, 6, 1, 6, ...]\n|-\n|align=\"right\"|{{sqrt|16}}|| = [4]\n|-\n|align=\"right\"|{{sqrt|17}}|| = [4; 8, 8, ...]\n|-\n|align=\"right\"|{{sqrt|18}}|| = [4; 4, 8, 4, 8, ...]\n|-\n|align=\"right\"|{{sqrt|19}}|| = [4; 2, 1, 3, 1, 2, 8, 2, 1, 3, 1, 2, 8, ...]\n|-\n|align=\"right\"|{{sqrt|20}}|| = [4; 2, 8, 2, 8, ...]\n|}\n\nThe square bracket notation used above is a sort of mathematical shorthand to conserve space. Written in more traditional notation the simple continued fraction for the square root of 11, [3; 3, 6, 3, 6, ...], looks like this:\n\n:<math>\n\\sqrt{11} = 3 + \\cfrac{1}{3 + \\cfrac{1}{6 + \\cfrac{1}{3 + \\cfrac{1}{6 + \\cfrac{1}{3 + \\ddots}}}}}\n</math>\n\nwhere the two-digit pattern {3, 6} repeats over and over again in the partial denominators. Since {{nowrap|1=11 = 3<sup>2</sup> + 2}}, the above is also identical to the following [[generalized continued fraction#Roots of positive numbers|generalized continued fractions]]:\n\n:<math>\n\\sqrt{11} = 3 + \\cfrac{2}{6 + \\cfrac{2}{6 + \\cfrac{2}{6 + \\cfrac{2}{6 + \\cfrac{2}{6 + \\ddots}}}}} = 3 + \\cfrac{6}{20 - 1 - \\cfrac{1}{20 - \\cfrac{1}{20 - \\cfrac{1}{20 - \\cfrac{1}{20 - \\ddots}}}}}.\n</math>\n\n==Geometric construction of the square root==\n[[File:Euclid Corollary 5.svg|thumb|The [[Spiral of Theodorus]] up to the triangle with a hypotenuse of {{math|{{radic|4}}}} ({{math|1 {{=}} {{sqrt|1}}}})]]\n\nThe square root of a positive number is usually defined as the side length of a [[square]] with the [[area]] equal to the given number. But the square shape is not necessary for it: if one of two [[similarity (geometry)|similar]] [[Euclidean plane|planar Euclidean]] objects has the area ''a'' times greater than another, then the ratio of their linear sizes is {{sqrt|''a''}}.\n\nA square root can be constructed with a compass and straightedge. In his [[Euclid's Elements|Elements]], [[Euclid]] ([[floruit|fl.]] 300&nbsp;BC) gave the construction of the [[geometric mean]] of two quantities in two different places: [http://aleph0.clarku.edu/~djoyce/java/elements/bookII/propII14.html Proposition II.14] and [http://aleph0.clarku.edu/~djoyce/java/elements/bookVI/propVI13.html Proposition VI.13].  Since the geometric mean of ''a'' and ''b'' is {{sqrt|''ab''}}, one can construct {{sqrt|''a''}} simply by taking {{nowrap|1=''b'' = 1}}.\n\nThe construction is also given by [[Descartes]] in his ''[[La Géométrie]]'', see figure 2 on [http://historical.library.cornell.edu/cgi-bin/cul.math/docviewer?did=00570001&seq=12&frames=0&view=50 page 2]. However, Descartes made no claim to originality and his audience would have been quite familiar with Euclid.\n\nEuclid's second proof in Book VI depends on the theory of [[Similar triangles#Similar triangles|similar triangles]]. Let AHB be a line segment of length {{nowrap|''a'' + ''b''}} with {{nowrap|1=AH = ''a''}} and {{nowrap|1=HB = ''b''}}. Construct the circle with AB as diameter and let C be one of the two intersections of the perpendicular chord at H with the circle and denote the length CH as ''h''. Then, using [[Thales' theorem]] and, as in the [[Pythagorean theorem#Proof using similar triangles|proof of Pythagoras' theorem by similar triangles]], triangle AHC is similar to triangle CHB (as indeed both are to triangle ACB, though we don't need that, but it is the essence of the proof of Pythagoras' theorem) so that AH:CH is as HC:HB, i.e. {{nowrap|1= ''a''/''h'' = ''h''/''b''}}, from which we conclude by cross-multiplication that {{nowrap|1= ''h''<sup>2</sup> = ''ab''}}, and finally that {{nowrap|1=''h'' = {{sqrt|''ab''}}}}. When marking the midpoint O of the line segment AB and drawing the radius OC of length {{nowrap|(''a'' + ''b'')/2}}, then clearly OC > CH, i.e. {{nowrap|(''a'' + ''b'')/2 ≥ {{sqrt|''ab''}}}} (with equality if and only if {{nowrap|1=''a'' = ''b''}}), which is the [[inequality of arithmetic and geometric means|arithmetic–geometric mean inequality for two variables]] and, as noted [[Square root#Computation|above]], is the basis of the [[Greek Mathematics|Ancient Greek]] understanding of \"Heron's method\".\n\nAnother method of geometric construction uses [[right triangle]]s and [[Mathematical induction|induction]]: {{sqrt|1}} can, of course, be constructed, and once {{sqrt|''x''}} has been constructed, the right triangle with legs 1 and {{sqrt|''x''}} has a [[hypotenuse]] of {{sqrt|''x'' + 1}}. Constructing successive square roots in this manner yields the [[Spiral of Theodorus]] depicted above.\n\n==See also==\n* [[Apotome (mathematics)]]\n* [[Cube root]]\n* [[Integer square root]]\n* [[Nested radical]]\n* [[Nth root]]\n* [[Root of unity]]\n* [[Solving quadratic equations with continued fractions]]\n* [[Square root principle]]\n* [[Quantum gate#Square root of NOT gate (√NOT)|The square root of NOT gate ({{sqrt|NOT}})]], one of the [[logic gate]]s used in [[quantum computer]]s (doesn't exist for non-quantum where [[NOT gate]]s are used)\n\n==Notes==\n{{Reflist}}\n\n==References==\n* {{cite book | last = Dauben | first = Joseph W. | author-link = Joseph Dauben | editor-last = Katz | editor-first = Victor J. | title = The Mathematics of Egypt, Mesopotamia, China, India, and Islam | chapter = Chinese Mathematics I | publisher = Princeton University Press | location = Princeton | year = 2007 | url = https://books.google.com/books?id=3ullzl036UEC | isbn = 0-691-11485-4}}\n* {{cite book|title=Algebra|edition=3rd|first1=Izrael M.|last1=Gel'fand|first2=Alexander|author1-link=Israel Gelfand|last2=Shen|publisher=Birkhäuser|year=1993|isbn=0-8176-3677-3|page=120|url=https://books.google.com/books?id=Z9z7iliyFD0C}}\n* {{cite book | last = Joseph | first = George | title = The Crest of the Peacock | publisher = Princeton University Press | location = Princeton | year = 2000 | isbn = 0-691-00659-8 }}\n* {{cite book | last = Smith | first = David | author-link = David Eugene Smith| title = History of Mathematics | publisher = Dover Publications | location = New York | year = 1958 | isbn = 978-0-486-20430-7 | volume = 2}}\n* {{citation|last=Selin|first=Helaine|author-link=Helaine Selin|title=Encyclopaedia of the History of Science, Technology, and Medicine in Non-Western Cultures|url=https://books.google.com/books?id=kt9DIY1g9HYC&pg=PA1268|year=2008|publisher=Springer|isbn=978-1-4020-4559-2}}.\n\n==External links==\n{{commons category}}\n* [http://www.azillionmonkeys.com/qed/sqroot.html Algorithms, implementations, and more]{{snd}}Paul Hsieh's square roots webpage\n* [http://johnkerl.org/doc/square-root.html How to manually find a square root]\n* [http://www.ams.org/samplings/feature-column/fc-2013-05 AMS Featured Column, Galileo's Arithmetic by Tony Philips]{{snd}}includes a section on how Galileo found square roots\n\n{{DEFAULTSORT:Square Root}}\n[[Category:Elementary special functions]]\n[[Category:Elementary mathematics]]\n[[Category:Unary operations]]"
    },
    {
      "title": "Trigonometric functions",
      "url": "https://en.wikipedia.org/wiki/Trigonometric_functions",
      "text": "{{Redirect|Cosine|the similarity measure|Cosine similarity}}\n{{Trigonometry}}\n[[File:Academ Base of trigonometry.svg|thumb|300px|upright=2|Basis of trigonometry: if two [[right triangle]]s have equal [[Types of angles|acute angles]], they are [[Similarity (geometry)|similar]], so their side lengths [[Proportionality (mathematics)|are proportional]]. Proportionality [[constant (mathematics)|constant]]s are written within the image: {{math|sin ''θ''}}, {{math|cos ''θ''}}, {{math|tan ''θ''}}, where {{mvar|θ}} is the common measure of five acute angles.]]\nIn [[mathematics]], the '''trigonometric functions''' (also called '''circular functions''', '''angle functions''' or '''goniometric functions'''<ref name=\"Klein_1924\"/><ref name=\"Klein_2004\"/>) are [[real function]]s which relate an angle of a [[right-angled triangle]] to ratios of two side lengths. They are widely used in all sciences that are related to [[geometry]], such as [[navigation]], [[solid mechanics]], [[celestial mechanics]], [[geodesy]], and many others. They are among the simplest [[periodic function]]s, and as such are also widely used for studying periodic phenomena, through [[Fourier analysis]].\n\nThe most familiar trigonometric functions are the [[sine]], the '''cosine''', and the '''tangent'''. Their [[multiplicative inverse|reciprocal]]s are respectively the '''cosecant''', the '''secant''', and the '''cotangent''', which are less used in modern mathematics.\n\nThe oldest definitions of trigonometric functions, related to right-angle triangles, define them only for [[acute angle]]s. For extending these definitions to functions whose [[domain of a function|domain]] is the whole [[projectively extended real line]], one can use geometrical definitions using the standard [[unit circle]] (a circle with [[radius]] 1 unit). Modern definitions express trigonometric functions as [[Series (mathematics)|infinite series]] or as solutions of [[differential equation]]s. This allows extending the domain of the sine and the cosine functions to the whole [[complex plane]], and the domain of the other trigonometric functions to the complex plane from which some isolated points are removed.\n\n==Right-angled triangle definitions==\n[[File:Trigonometry triangle.svg|right|200px|A right triangle always includes a 90° ({{sfrac|{{pi}}|2}} radians) angle, here labeled ''C''. Angles ''A'' and ''B'' may vary. Trigonometric functions specify the relationships among side lengths and interior angles of a right triangle.]]\n[[File:Periodic sine.PNG|thumb|'''Top:''' Trigonometric function {{math|sin ''θ''}} for selected angles {{math|''θ''}}, {{math|{{pi}} − ''θ''}}, {{math|{{pi}} + ''θ''}}, and {{math|2{{pi}} − ''θ''}} in the four quadrants.<br>'''Bottom:''' Graph of sine function versus angle. Angles from the top panel are identified.]]\n[[File:TrigFunctionDiagram.svg|thumb|Plot of the six trigonometric functions and the unit circle for an angle of 0.7 radians]]\n\n''In this section, the same upper-case letter denotes a vertex of a triangle and the measure of the corresponding angle; the same lower case letter denotes an edge of the triangle and its length.''\n\nGiven an [[acute angle]] {{mvar|A}} of a [[right-angled triangle]] (see figure) the [[hypotenuse]] {{mvar|h}} is the side that connects the two acute angles. The side {{mvar|b}} ''adjacent'' to {{mvar|A}} is the side of the triangle that connects {{mvar|A}} to the right angle. The third side {{mvar|a}} is said ''opposite'' to {{mvar|A}}.\n\nIf the angle {{mvar|A}} is given, then all sides of the right-angled triangle are well defined up to a scaling factor. This means that the ratio of any two side lengths depends only on {{mvar|A}}. These six ratios define thus six functions of {{mvar|A}}, which are the trigonometric functions. More precisely, the six trigonometric functions are:\n* '''sine:''' <math>\\sin A= \\frac a h = \\frac \\mathrm{opposite}\\mathrm{hypotenuse}</math>\n* '''cosine:''' <math>\\cos A= \\frac b h = \\frac \\mathrm{adjacent}\\mathrm{hypotenuse}</math>\n* '''tangent:''' <math>\\tan A= \\frac a b = \\frac \\mathrm{opposite}\\mathrm{adjacent}</math>\n* '''cosecant:''' <math>\\csc A= \\frac h a = \\frac \\mathrm{hypotenuse}\\mathrm{opposite}</math>\n* '''secant:''' <math>\\sec A= \\frac h b  = \\frac \\mathrm{hypotenuse}\\mathrm{adjacent}</math>\n* '''cotangent:''' <math>\\cot A= \\frac b a = \\frac \\mathrm{adjacent}\\mathrm{opposite}</math>\n\nIn a right angled triangle, the sum of the two acute angles is a right angle, that is 90° or <math>\\frac \\pi 2</math> [[radian]]s. This induces relationships between trigonometric functions that are summarized in the following table, where the angle is denoted by <math>\\theta</math> instead of {{mvar|A}}.\n\n{| class=\"wikitable\"\n|-\n! Function\n! Abbreviation\n! Description\n! [[List of trigonometric identities|Relationship]] (using [[radian]]s)\n|-\n! sine\n| sin\n|align=center|{{sfrac|opposite|hypotenuse}}\n| <math>\\sin \\theta = \\cos\\left(\\frac{\\pi}{2} - \\theta \\right) = \\frac{1}{\\csc \\theta}</math>\n|-\n! cosine\n| cos\n|align=center|{{sfrac|adjacent|hypotenuse}}\n| <math>\\cos \\theta = \\sin\\left(\\frac{\\pi}{2} - \\theta \\right) = \\frac{1}{\\sec \\theta}\\,</math>\n|-\n! tangent\n| tan (or&nbsp;tg)\n|align=center|{{sfrac|opposite|adjacent}}\n| <math>\\tan \\theta = \\frac{\\sin \\theta}{\\cos \\theta} = \\cot\\left(\\frac{\\pi}{2} - \\theta \\right) = \\frac{1}{\\cot \\theta} </math>\n|-\n! cotangent\n| cot (or&nbsp;cotan or&nbsp;cotg or&nbsp;ctg or&nbsp;ctn)\n|align=center|{{sfrac|adjacent|opposite}}\n| <math>\\cot \\theta = \\frac{\\cos \\theta}{\\sin \\theta} = \\tan\\left(\\frac{\\pi}{2} - \\theta \\right) = \\frac{1}{\\tan \\theta} </math>\n|-\n! secant\n| sec\n|align=center|{{sfrac|hypotenuse|adjacent}}\n| <math>\\sec \\theta = \\csc\\left(\\frac{\\pi}{2} - \\theta \\right) = \\frac{1}{\\cos \\theta} </math>\n|-\n! cosecant\n| csc (or&nbsp;cosec)\n|align=center|{{sfrac|hypotenuse|opposite}}\n| <math>\\csc \\theta = \\sec\\left(\\frac{\\pi}{2} - \\theta \\right) = \\frac{1}{\\sin \\theta} </math>\n|}\n\n==Radians versus degrees==\nIn geometric applications, the argument of a trigonometric function is generally the measure of an [[angle]]. For this purpose, any [[angular unit]] is convenient, and angles are most commonly measured in [[degree (angle)|degrees]].\n\nWhen using trigonometric function in [[calculus]], their argument is generally not an angle, but rather a [[real number]]. In this case, it is more suitable to express the argument of the trigonometric as the length of the [[arc (geometry)|arc]] of the [[unit circle]] delimited by an angle with the center of the circle as vertex. Therefore, one uses the [[radian]] as angular unit: a radian is the angle that delimites an arc of length {{val|1}} on the unit circle. A complete [[turn (angle)|turn]] is thus an angle of {{math|2''{{pi}}''}} radians.\n\nA great advantage of radians is that many formulas are much simpler when using them, typically all formulas relative to [[derivative]]s and [[integral]]s.\n\nThis is thus a general convention that, when the angular unit is not explicitly specified, ''the arguments of trigonometric functions are always expressed in radians.''\n\n==Unit-circle definitions==\n[[File:Unit Circle Definitions of Six Trigonometric Functions.png|thumb|300x300px|In this illustration, the six trigonometric functions of an arbitrary angle {{math|''θ''}} are represented as [[Cartesian coordinates]] of points related to the [[unit circle]]. The ordinates of {{math|A}}, {{math|B}} and {{math|D}} are {{math|sin ''θ''}}, {{math|tan ''θ''}} and {{math|csc ''θ''}}, respectively, while the abscissas of {{math|A}}, {{math|C}} and {{math|E}} are {{math|cos ''θ''}}, {{math|cot ''θ''}} and {{math|sec ''θ''}}, respectively.]]\n[[File:trigonometric function quadrant sign.svg|thumb|Signs of trigonometric functions in each quadrant. The mnemonic \"'''all''' '''s'''cience '''t'''eachers (are) '''c'''razy\" lists the functions which are positive from quadrants I to IV.<ref name=\"Heng\"/> This is a variation on the mnemonic \"[[All Students Take Calculus]]\".]]\n\nThe six trigonometric functions can be defined as [[Cartesian coordinate system|coordinate values]] of points on the [[Euclidean plane]] that are related to the [[unit circle]], which is the [[circle]] of radius one centered at the origin {{math|O}} of this coordinate system. While [[#Right-angled triangle definitions|right-angled triangle definitions]] permit the definition of the trigonometric functions for angles between {{math|0}} and <math>\\frac{\\pi}{2}</math> [[radian]] {{math|(90°),}} the unit circle definitions allow to extend the [[Domain of a function|domain]] of the trigonometric functions to all positive and negative real numbers.\n\nRotating a [[Line (geometry)#Ray|ray]] from the direction of the positive half of the ''x''-axis by an angle {{mvar|θ}} ([[counterclockwise]] for <math>\\theta > 0,</math> and  clockwise for <math>\\theta < 0</math>) yields intersection points of this ray (see the figure) with the unit {{nowrap|circle: <math>\\mathrm{A} = (x_\\mathrm{A},y_\\mathrm{A})</math>,}} and, by extending the ray to a line if necessary, with the {{nowrap|line <math> \\text{“}x=1\\text{”}:\\;\\mathrm{B} = (x_\\mathrm{B},y_\\mathrm{B}),</math>}} and with the {{nowrap|line <math> \\text{“}y=1\\text{”}:\\;\\mathrm{C} = (x_\\mathrm{C},y_\\mathrm{C}).</math>}} The tangent line to the unit circle in point {{math|A}}, which is orthogonal to this ray, intersects the ''y''- and ''x''-axis in points <math>\\mathrm{D} = (0,y_\\mathrm{D})</math> and <math>\\mathrm{E} = (x_\\mathrm{E},0)</math>. The coordinate values of these points give all the existing values of the trigonometric functions for arbitrary real values of {{mvar|θ}} in the following manner.\n\nThe trigonometric functions {{math|cos}} and {{math|sin}} are defined, respectively, as the ''x''- and ''y''-coordinate values of point {{math|A}}, i.e.,\n:<math>\\cos(\\theta) = x_\\mathrm{A} \\quad</math> and <math>\\quad \\sin(\\theta) = y_\\mathrm{A}.</math><ref>{{Cite web|url=https://www.encyclopediaofmath.org/index.php/Trigonometric_functions|title=Trigonometric Functions|last=Bityutskov|first=V.I.|date=2011-02-07|website=Encyclopedia of Mathematics|language=en|archive-url=https://web.archive.org/web/20171229231821/https://www.encyclopediaofmath.org/index.php/Trigonometric_functions|archive-date=2017-12-29|dead-url=no|access-date=2017-12-29|df=}}</ref>\n\nIn the range <math>0 \\le \\theta \\le \\pi/2</math> this definition coincides with the right-angled triangle definition by taking the right-angled triangle to have the unit radius {{math|OA}} as [[hypotenuse]], and since for all points <math>\\mathrm{P} = (x,y)</math> on the unit circle the equation <math>x^2+y^2=1</math> holds, this definition of cosine and sine also satisfies the '''Pythagorean identity''' \n:<math>\\cos^2\\theta+\\sin^2\\theta=1.</math>\n\nThe other trigonometric functions can be found along the unit circle as \n:<math>\\tan(\\theta) = y_\\mathrm{B} \\quad</math> and <math> \\quad\\cot(\\theta) = x_\\mathrm{C},</math>\n:<math>\\csc(\\theta)\\ = y_\\mathrm{D} \\quad</math> and <math> \\quad\\sec(\\theta) = x_\\mathrm{E}.</math>\n\nBy applying the Pythagorean identity and geometric proof methods, these definitions can readily be shown to coincide with the definitions of tangent, cotangent, secant and cosecant in terms of sine and cosine, that is\n: <math>\\tan \\theta =\\frac{\\sin \\theta}{\\cos\\theta},\\quad \\cot\\theta=\\frac{\\cos\\theta}{\\sin\\theta},\\quad \\sec\\theta=\\frac{1}{\\cos\\theta},\\quad \\csc\\theta=\\frac{1}{\\sin\\theta}.</math>\n\n[[File:Trigonometric functions.svg|right|thumb|300px|Trigonometric functions:\n{{color|#00A|Sine}},\n{{color|#0A0|Cosine}},\n{{color|#A00|Tangent}},\n{{color|#00A|Cosecant (dotted)}},\n{{color|#0A0|Secant (dotted)}},\n{{color|#A00|Cotangent (dotted)}}]]\n\nAs a rotation of an angle of <math>\\pm2\\pi</math> does not change the position or size of a shape, the points {{math|A}}, {{math|B}}, {{math|C}}, {{math|D}}, and {{math|E}} are the same for two angles whose difference is an integer multiple of <math>2\\pi</math>. Thus trigonometric functions are [[periodic function]]s with period <math>2\\pi</math>. That is, the equalities\n: <math> \\sin\\theta = \\sin\\left(\\theta + 2 k \\pi \\right)\\quad</math> and <math>\\quad \\cos\\theta = \\cos\\left(\\theta + 2 k \\pi \\right)</math>\nhold for any angle {{mvar|θ}} and any [[integer]] {{mvar|k}}. The same is true for the four other trigonometric functions. Observing the sign and the monotonicity of the functions sine, cosine, cosecant, and secant in the four quadrants, shows that {{math|2{{pi}}}} is the smallest value for which they are periodic, i.e., {{math|2{{pi}}}} is the [[periodic function|fundamental period]] of these functions. However, already after a rotation by an angle <math>\\pi</math> the points {{mvar|B}} and {{mvar|C}} return to their original position, so that the tangent function and the cotangent function have a fundamental period of {{pi}}. That is, the equalities\n: <math> \\tan\\theta = \\tan(\\theta + k\\pi) \\quad</math> and <math>\\quad \\cot\\theta = \\cot(\\theta + k\\pi)</math>\nhold for any angle {{mvar|θ}} and any integer {{mvar|k}}.\n\n==Algebraic values==\n[[File:Unit circle angles color.svg|right|thumb|300px|The [[unit circle]], with some points labeled with their cosine and sine (in this order), and the corresponding angles in radians and degrees.]]\nThe [[algebraic expression]]s for {{math|1=sin 0, sin {{sfrac|{{pi}}|6}} = sin 30°, sin {{sfrac|{{pi}}|4}} = sin 45°, sin {{sfrac|{{pi}}|3}} = sin 60°}} and {{math|1=sin {{sfrac|{{pi}}|2}} = sin 90°}} are\n\n:<math>0, \\quad \\frac{1}{2},\\quad \\frac{\\sqrt{2}}{2},\\quad \\frac{\\sqrt{3}}{2},\\quad 1,</math>\n\nrespectively. Writing the numerators as square roots of consecutive natural numbers <math>\\frac{\\sqrt{0}}{2}, \\frac{\\sqrt{1}}{2}, \\frac{\\sqrt{2}}{2}, \\frac{\\sqrt{3}}{2}, \\frac{\\sqrt{4}}{2}</math> provides an easy way to remember the values.<ref name=\"Larson_2013\"/>\n\nSuch simple expressions generally do not exist for other angles which are rational multiples of a straight angle.\nFor an angle which, measured in degrees, is a multiple of three, the sine and the cosine may be expressed in terms of [[square root]]s, see [[Trigonometric constants expressed in real radicals]]. These values of the sine and the cosine may thus be constructed by [[Compass-and-straightedge construction|ruler and compass]].\n\nFor an angle of an integer number of degrees, the sine and the cosine may be expressed in terms of [[square root]]s and the [[cube root]] of a non-real [[complex number]]. [[Galois theory]] allows proving that, if the angle is not a multiple of 3°, non-real cube roots are unavoidable.\n\nFor an angle which, measured in degrees, is a [[rational number]], the sine and the cosine are [[algebraic number]]s, which may be expressed in terms of [[nth root|{{mvar|n}}th roots]]. This results from the fact that the [[Galois group]]s of the [[cyclotomic polynomial]]s are [[cyclic group|cyclic]].\n\nFor an angle which, measured in degrees, is not a rational number, then either the angle or both the sine and the cosine are [[transcendental number]]s. This is a corollary of [[Baker's theorem]], proved in 1966.\n\n===Simple algebraic values===\nThe following table summarizes the simplest algebraic values of trigonometric functions.<ref name=\"Abramowitz and Stegun\">Abramowitz, Milton and Irene A. Stegun, p.74</ref> The symbol {{math|&infin;}} represents the [[point at infinity]] on the [[projectively extended real line]]; it is not signed, because, when it appears in the table, the corresponding trigonometric function tends to {{math|+&infin;}} on one side, and to {{math|–&infin;}} on the other side, when the argument tends to the value in the table.\n\n:<math>\n\\begin{array}{|c|cccccccc|}\n\\hline\n\\begin{matrix}\\text{Radian}\\\\ \\text{Degree}\\end{matrix} &\n\\begin{matrix}0\\\\ 0^\\circ\\end{matrix} &\n\\begin{matrix}\\frac{\\pi}{12}\\\\ 15^\\circ\\end{matrix} &\n\\begin{matrix}\\frac{\\pi}{8}\\\\ 22.5^\\circ\\end{matrix} &\n\\begin{matrix}\\frac{\\pi}{6}\\\\ 30^\\circ\\end{matrix} &\n\\begin{matrix}\\frac{\\pi}{4}\\\\ 45^\\circ\\end{matrix} &\n\\begin{matrix}\\frac{\\pi}{3}\\\\ 60^\\circ\\end{matrix} &\n\\begin{matrix}\\frac{5\\pi}{12}\\\\ 75^\\circ\\end{matrix} &\n\\begin{matrix}\\frac{\\pi}{2}\\\\ 90^\\circ\\end{matrix} \\\\\n\\hline\n\\sin &\n0 &\n\\frac{ \\sqrt{6} - \\sqrt{2} } {4} &\n\\frac{ \\sqrt{2 - \\sqrt{2}} } {2} &\n\\frac{1}{2} &\n\\frac{\\sqrt{2}}{2} &\n\\frac{\\sqrt{3}}{2} &\n\\frac{ \\sqrt{6} + \\sqrt{2} } {4} &\n1 \\\\\n\\cos &\n1 &\n\\frac{\\sqrt{6}+\\sqrt{2}}{4} &\n\\frac{ \\sqrt{2 + \\sqrt{2}} } {2} &\n\\frac{\\sqrt{3}}{2} &\n\\frac{\\sqrt{2}}{2} &\n\\frac{1}{2} &\n\\frac{ \\sqrt{6} - \\sqrt{2}} {4} &\n0 \\\\\n\\tan &\n0 &\n2-\\sqrt{3} &\n\\sqrt{2} - 1 &\n\\frac{\\sqrt{3}}{3} &\n1 &\n\\sqrt{3} &\n2+\\sqrt{3} &\n\\infty \\\\\n\\cot &\n\\infty &\n2+\\sqrt{3} &\n\\sqrt{2} + 1 &\n\\sqrt{3} &\n1 &\n\\frac{\\sqrt{3}}{3} &\n2-\\sqrt{3} &\n0 \\\\\n\\sec &\n1 &\n\\sqrt{6} - \\sqrt{2} &\n\\sqrt{2} \\sqrt{ 2 - \\sqrt{2} } &\n\\frac{2\\sqrt{3}}{3} &\n\\sqrt{2} &\n2 &\n\\sqrt{6}+\\sqrt{2} &\n\\infty \\\\\n\\csc &\n\\infty &\n\\sqrt{6}+\\sqrt{2} &\n\\sqrt{2} \\sqrt{ 2 + \\sqrt{2} } &\n2 &\n\\sqrt{2} &\n\\frac{2\\sqrt{3}}{3} &\n\\sqrt{6} - \\sqrt{2} &\n1 \\\\\\hline\n\\end{array}\n</math>\n\n==In calculus==\n[[File:Taylorsine.svg|thumb|right|The sine function (blue) is closely approximated by its [[Taylor's theorem|Taylor polynomial]] of degree 7 (pink) for a full cycle centered on the origin.]]\n[[File:Taylor cos.gif|thumb|Animation for the approximation of cosine via Taylor polynomials.]]\n[[File:Taylorreihenentwicklung des Kosinus.svg|thumb|<math>\\cos(x)</math> together with the first Taylor polynomials <math>p_n(x)=\\sum_{k=0}^n (-1)^k \\frac{x^{2k}}{(2k)!}</math>]]\n \nTrigonometric functions are [[differentiable function|differentiable]]. This is not immediately evident from the above geometrical definitions. Moreover, the modern trends, in mathematics is to build [[geometry]] from [[calculus]] rather than the converse. Therefore, except at a very elementary level, trigonometric functions are defined using the methods of calculus.\n\nFor defining trigonometric functions inside calculus, there are two equivalent possibilities, either using [[power series]] or [[differential equation]]s. These definitions are equivalent, as starting from one of them, it is easy to retrieve the other as a property. However the definition through differential equations is somehow more natural, since, for example, the choice of the coefficients of the power series may appear as quite arbitrary, and the [[Pythagorean identity]] is much easier to deduce from the differential equations.\n\n===Definition by differential equations===\n\nSine and cosine are the unique [[differentiable function]]s such that\n:<math>\n\\begin{align}\n\\frac{d}{dx}\\sin x&= \\cos x,\\\\\n\\frac{d}{dx}\\cos x&= -\\sin x,\\\\\n\\sin 0&=0,\\\\\n\\cos 0&=1.\n\\end{align}\n</math>\n\nDifferentiating these equations, one gets that both sine and cosine are solutions of the [[differential equation]]\n:<math>y''+y=0.</math>\n\nApplying the [[quotient rule]] to the definition of the tangent as the quotient of the sine by the cosine, one gets that the tangent function verifies\n:<math>\\frac{d}{dx}\\tan x = 1+\\tan^2 x.</math>\n\n===Power series expansion===\nApplying the differential equations to [[power series]] with indeterminate coefficients, one may deduce [[recurrence relation]]s for the coefficients of the [[Taylor series]] of the sine and cosine functions. These recurrence relations are easy to solve, and give the series expansions<ref>See Ahlfors, pages 43–44.</ref> \n:<math>\n\\begin{align}\n\\sin x & = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\cdots \\\\\n& = \\sum_{n=0}^\\infty \\frac{(-1)^n x^{2n+1}}{(2n+1)!} \\\\\n\\end{align}\n</math>\n\n:<math>\n\\begin{align}\n\\cos x & = 1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\frac{x^6}{6!} + \\cdots \\\\\n& = \\sum_{n=0}^\\infty \\frac{(-1)^n x^{2n}}{(2n)!}\n\\end{align}\n</math>\n\nThe [[radius of convergence]] of these series is infinite. Therefore, the sine and the cosine can be extended to [[entire function]]s (also called \"sine\" and \"cosine\"), which are (by definition) [[complex-valued function]]s that are defined and [[holomorphic]] on the whole [[complex plane]].\n\nBeing defined as fractions of entire functions, the other trigonometric functions may be extended to [[meromorphic function]]s, that is functions that are holomorphic in the whole complex plane, except some isolated points called [[zeros and poles|poles]]. Here, the poles are the numbers of the form <math>(2k+1)\\frac \\pi 2</math> for the tangent and the secant, or <math>k\\pi</math> for the cotangent and the cosecant, where {{mvar|k}} is an arbitrary integer.\n\nRecurrences relations may also be computed for the coefficients of the [[Taylor series]] of the other trigonometric functions. These series have a finite [[radius of convergence]]. Their coefficients have a [[combinatorics|combinatorial]] interpretation: they enumerate [[alternating permutation]]s of finite sets.<ref>Stanley, Enumerative Combinatorics, Vol I., page 149</ref>\n\nMore precisely, defining\n: {{mvar|U<sub>n</sub>}}, the {{mvar|n}}th [[up/down number]],\n: {{mvar|B<sub>n</sub>}}, the {{mvar|n}}th [[Bernoulli number]], and\n: {{mvar|E<sub>n</sub>}}, is the {{mvar|n}}th [[Euler number]],\none has the following series expansions:<ref>Abramowitz; Weisstein.</ref> \n: <math>\n\\begin{align}\n\\tan x & {} = \\sum_{n=0}^\\infty \\frac{U_{2n+1} x^{2n+1}}{(2n+1)!} \\\\\n& {} = \\sum_{n=1}^\\infty \\frac{(-1)^{n-1} 2^{2n} \\left(2^{2n}-1\\right) B_{2n} x^{2n-1}}{(2n)!} \\\\\n& {} = x + \\frac{1}{3}x^3 + \\frac{2}{15}x^5 + \\frac{17}{315}x^7 + \\cdots, \\qquad \\text{for } |x| < \\frac{\\pi}{2}.\n\\end{align}\n</math>\n\n: <math>\n\\begin{align}\n\\csc x & {} = \\sum_{n=0}^\\infty \\frac{(-1)^{n+1} 2 \\left(2^{2n-1}-1\\right) B_{2n} x^{2n-1}}{(2n)!} \\\\\n& {} = x^{-1} + \\frac{1}{6}x + \\frac{7}{360}x^3 + \\frac{31}{15120}x^5 + \\cdots, \\qquad \\text{for } 0 < |x| < \\pi.\n\\end{align}\n</math>\n\n: <math>\n\\begin{align}\n\\sec x & {} = \\sum_{n=0}^\\infty \\frac{U_{2n} x^{2n}}{(2n)!}\n= \\sum_{n=0}^\\infty \\frac{(-1)^n E_{2n} x^{2n}}{(2n)!} \\\\\n& {} = 1 + \\frac{1}{2}x^2 + \\frac{5}{24}x^4 + \\frac{61}{720}x^6 + \\cdots, \\qquad \\text{for } |x| < \\frac{\\pi}{2}.\n\\end{align}\n</math>\n\n: <math>\n\\begin{align}\n\\cot x & {} = \\sum_{n=0}^\\infty \\frac{(-1)^n 2^{2n} B_{2n} x^{2n-1}}{(2n)!} \\\\\n& {} = x^{-1} - \\frac{1}{3}x - \\frac{1}{45}x^3 - \\frac{2}{945}x^5 - \\cdots, \\qquad \\text{for } 0 < |x| < \\pi.\n\\end{align}\n</math>\n\nThere is a series representation as [[partial fraction expansion]] where just translated [[Multiplicative inverse|reciprocal function]]s are summed up, such that the [[Pole (complex analysis)|pole]]s of the cotangent function and the reciprocal functions match:<ref name=\"Aigner_2000\"/>\n: <math>\n\\pi \\cdot \\cot(\\pi x) = \\lim_{N\\to\\infty}\\sum_{n=-N}^N \\frac{1}{x+n}.\n</math>\nThis identity can be proven with the [[Gustav Herglotz|Herglotz]] trick.<ref name=\"Remmert_1991\"/>\nCombining the {{math|(–''n'')}}th with the {{math|''n''}}th term lead to [[absolute convergence|absolutely convergent]] series:\n:<math>\n\\pi \\cdot \\cot(\\pi x) = \\frac{1}{x} + 2x\\sum_{n=1}^\\infty \\frac{1}{x^2-n^2}\\ , \\quad \\frac{\\pi}{\\sin(\\pi x)} = \\frac{1}{x} + 2x\\sum_{n=1}^\\infty \\frac{(-1)^n}{x^2-n^2}.\n</math>\n\n===Relationship to exponential function (Euler's formula)===\n[[File:Sinus und Kosinus am Einheitskreis 3.svg|thumb|<math>\\cos(\\theta)</math> and <math>\\sin(\\theta)</math> are the real and imaginary part of <math>e^{i\\theta}</math> respectively.]]\n\n[[Euler's formula]] relates sine and cosine to the [[exponential function]]:\n:<math> e^{ix}  = \\cos x + i\\sin x. </math>\nThis formula is commonly considered for real values of {{mvar|x}}, but it remains true for all complex values.\n\n''Proof'': Let <math>f_1(x)=\\cos x + i\\sin x,</math> and <math>f_2(x)=e^{ix}.</math> One has <math>\\frac{d}{dx}f_j(x)= if_j(x)</math> for {{math|1=''j'' = 1, 2}}. The [[quotient rule]] implies thus that <math>\\frac{d}{dx}\\left(\\frac{f_1(x)}{f_2(x)}\\right)=0</math>. Therefore, <math>\\frac{f_1(x)}{f_2(x)}</math> is a constant function, which equals {{val|1}}, as <math>f_1(0)=f_2(0)=1.</math> This proves the formula.\n\nOne has\n:<math>\\begin{align}\ne^{ix}  &= \\cos x + i\\sin x\\\\\ne^{-ix}  &= \\cos x - i\\sin x.\n\\end{align}</math>\n\nSolving this [[linear system]] in sine and cosine, one can express them in terms of the exponential function:\n: <math>\\begin{align}\\sin x &= \\frac{e^{i x} - e^{-i x}}{2i}\\\\\n\\cos x &= \\frac{e^{i x} + e^{-i x}}{2}.\n\\end{align}</math>\n\nWhen {{mvar|x}} is real, this may be rewritten as \n: <math>\\cos x = \\operatorname{Re}\\left(e^{i x}\\right), \\qquad \\sin x = \\operatorname{Im}\\left(e^{i x}\\right).</math>\n\nMost [[List of trigonometric identities|trigonometric identities]] can be proved by expressing trigonometric functions in terms of the complex exponential function by using above formulas, and then using the identity <math>e^{a+b}=e^ae^b</math> for simplifying the result.\n\n===Definitions using functional equations===\nOne can also define the trigonometric functions using various [[functional equation]]s.\n\nFor example,<ref name=\"Kannappan_2009\"/> the sine and the cosine form the unique pair of [[continuous function]]s that satisfy the difference formula\n: <math>\\cos(x- y) = \\cos x\\cos y + \\sin x\\sin y\\,</math>\nand the added condition\n: <math>0 < x\\cos x < \\sin x < x\\quad\\text{ for }\\quad 0 < x < 1.</math>\n\n==Basic identities==\nMany [[identity (mathematics)|identities]] interrelate the trigonometric functions. This section contains the most basic ones; for more identities, see [[List of trigonometric identities]]. These identities may be proved geometrically from the unit-circle definitions or the right-angled-triangle definitions (although, for the latter definitions, care must be taken for angles that are not in the interval {{math|[0, {{pi}}/2]}}, see  [[Proofs of trigonometric identities]]). For non-geometrical proofs using only tools of [[calculus]], one may use directly the differential equations, in a way that is similar to that of the [[#Relationship to exponential function (Euler's formula)|above proof]] of Euler's identity. One can also use Euler's identity for expressing all trigonometric functions in terms of complex exponentials and using properties of the exponential function.\n\n===Parity===\nThe cosine and the secant are [[even function]]s; the other trigonometric functions are [[odd function]]s. That is:\n:<math>\\begin{align}\n\\sin(-x) &=-\\sin x\\\\\n\\cos(-x) &=\\cos x\\\\\n\\tan(-x) &=-\\tan x\\\\\n\\cot(-x) &=-\\cot x\\\\\n\\csc(-x) &=-\\csc x\\\\\n\\sec(-x) &=\\sec x.\n\\end{align}</math>\n\n===Periods===\nAll trigonometric functions are [[periodic function]]s of period {{math|2{{pi}}}}. This is the smallest period, except for the tangent and the cotangent, which have {{pi}} as smallest period. This means that, for every integer {{mvar|k}}, one has\n:<math>\\begin{align}\n\\sin (x+2k\\pi) &=\\sin x\\\\\n\\cos (x+2k\\pi) &=\\cos x\\\\\n\\tan (x+k\\pi) &=\\tan x\\\\\n\\cot (x+k\\pi) &=\\cot x\\\\\n\\csc (x+2k\\pi) &=\\csc x\\\\\n\\sec (x+2k\\pi) &=\\sec x.\n\\end{align}</math>\n\n===Pythagorean identity===\n\nThe [[Pythagorean identity]], is the expression of the [[Pythagorean theorem]] in terms of trigonometric functions. It is \n:<math>\\sin^2 x  + \\cos^2 x  = 1 . </math>\n\n===Sum and difference formulas===\n\nThe sum and difference formulas allow expanding the sine, the cosine, and the tangent of a sum or a difference of two angles in terms of sines and cosines and tangents of the angles themselves. These can be derived geometrically, using arguments that date to [[Ptolemy]]. One can also produce them algebraically using [[Euler's formula]].\n; Sum\n:<math>\\begin{align}\n\\sin\\left(x+y\\right)&=\\sin x \\cos y + \\cos x \\sin y,\\\\\n\\cos\\left(x+y\\right)&=\\cos x \\cos y - \\sin x \\sin y,\\\\\n\\tan(x + y) &= \\frac{\\tan x + \\tan y}{1 - \\tan x\\tan y}.\n\\end{align}</math>\n; Difference\n:<math>\\begin{align}\n\\sin\\left(x-y\\right)&=\\sin x \\cos y - \\cos x \\sin y, \\\\\n\\cos\\left(x-y\\right)&=\\cos x \\cos y + \\sin x \\sin y,\\\\ \n\\tan(x - y) &= \\frac{\\tan x - \\tan y}{1 + \\tan x\\tan y}.\n\\end{align}</math>\n\nWhen the two angles are equal, the sum formulas reduce to simpler equations known as the '''double-angle formulae'''.\n\n:<math>\\begin{align}\n\\sin 2x &= 2 \\sin x \\cos x = \\frac{2\\tan x}{1+\\tan^2 x}, \\\\\n\\cos 2x &= \\cos^2 x - \\sin^2 x = 2 \\cos^2 x - 1 = 1 - 2 \\sin^2 x = \\frac{1-\\tan^2 x}{1+\\tan^2 x},\\\\\n\\tan 2x &= \\frac{2\\tan x}{1-\\tan^2 x}.\n\\end{align}</math>\n\nThese identities can be used to derive the [[product-to-sum identities]].\n\nBy setting <math>\\theta=2x</math> and <math>t=\\tan x,</math> this allows expressing all trigonometric functions of <math>\\theta</math> as a [[rational fraction]] of <math>t=\\tan(\\theta/2):</math>\n:<math>\\begin{align}\n\\sin \\theta &= \\frac{2t}{1+t^2}, \\\\\n\\cos \\theta &= \\frac{1-t^2}{1+t^2},\\\\\n\\tan \\theta &= \\frac{2t}{1-t^2}.\n\\end{align}</math>\nTogether with \n:<math>d\\theta = \\frac{2}{1+t^2}dt,</math>\nthis is the [[tangent half-angle substitution]], which allows reducing the computation of [[integral]]s and [[antiderivative]]s of trigonometric functions to that of rational fractions.\n\n===Derivatives and antiderivatives===\nThe [[derivative]]s of trigonometric functions result from those of sine and cosine by applying [[quotient rule]]. The values given for the [[antiderivative]]s in the following table can be verified by differentiating them. The number&nbsp;{{mvar|C}} is a [[constant of integration]].\n\n:<math>\n\\begin{array}{|c|c|c|}\\hline\nf(x) &\nf'(x) &\n\\int f(x)\\,dx \\\\\n\\hline\n\\sin x &\n\\cos x &\n-\\cos x + C \\\\\n\\cos x &\n-\\sin x &\n\\sin x + C \\\\\n\\tan x &\n\\sec^2 x = 1+\\tan^2 x &\n-\\ln \\left( |\\cos x|\\right ) + C \\\\\n\\cot x &\n-\\csc^2 x = -(1+\\cot^2 x) &\n\\ln \\left (|\\sin x|\\right ) + C \\\\\n\\sec x &\n\\sec x\\tan x &\n\\ln \\left (|\\sec x + \\tan x|\\right ) + C \\\\\n\\csc x &\n-\\csc x \\cot x &\n-\\ln \\left (|\\csc x + \\cot x|\\right ) + C \\\\\n\\hline\n\\end{array}\n</math>\n\n==Inverse functions==\n{{Main|Inverse trigonometric functions}}\nThe trigonometric functions are periodic, and hence not [[injective function|injective]], so strictly speaking, they do not have an [[inverse function]]. However, on each interval on which a trigonometric function is [[monotonic]], one can define an inverse function, and this defines inverse trigonometric functions as [[multivalued function]]s. To define a true inverse function, one must restrict the domain to an interval where the function is monotonic, and is thus [[bijection|bijective]] from this interval to its image by the function. The common choice for this interval, called the set of [[principal value]]s, is given in the following table. As usual, the inverse trigonometric functions are denoted with the prefix \"arc\" before the name or its abbreviation of the function.\n:<math>\n\\begin{array}{|c|c|c|c|}\n\\hline\n \\text{Function} & \\text{Definition} & \\text{Domain} &\\text{Set of principal values}\n \\\\\n \\hline\n y = \\arcsin x & \\sin y = x & -1 \\le x \\le 1           & -\\frac{\\pi}{2} \\le y \\le \\frac{\\pi}{2} \\\\\n y = \\arccos x & \\cos y = x & -1 \\le x \\le 1           & 0 \\le y \\le \\pi \\\\\n y = \\arctan x & \\tan y = x & -\\infty \\le x \\le \\infty & -\\frac{\\pi}{2} < y < \\frac{\\pi}{2} \\\\\n y = \\arccot x & \\cot y = x & -\\infty \\le x \\le \\infty & 0 < y < \\pi \\\\\n y = \\arcsec x & \\sec y = x & x<-1 \\text{ or } x>1     & 0 \\le y \\le \\pi,\\; y \\ne \\frac{\\pi}{2} \\\\\n y = \\arccsc x & \\csc y = x & x<-1 \\text{ or } x>1     & -\\frac{\\pi}{2} \\le y \\le \\frac{\\pi}{2},\\; y \\ne 0\n\\\\\\hline\n\\end{array}\n</math>\n\nThe notations sin<sup>−1,</sup> cos<sup>−1,</sup> etc. are often used for arcsin and arccos, etc. When this notation is used, inverse functions could be confused with multiplicative inverses. The notation with the \"arc\" prefix avoids such a confusion, though \"arcsec\" for arcsecant can be confused with \"[[arcsecond]]\".\n\nJust like the sine and cosine, the inverse trigonometric functions can also be expressed in terms of infinite series. They can also be expressed in terms of [[complex logarithm]]s.  See [[Inverse trigonometric functions]] for details.\n\n==Applications==\n{{Main|Uses of trigonometry}}\n\n===Angles and sides of a triangle===\nIn this sections {{math|''A'', ''B'', ''C''}} denote the three (interior) angles of a triangle, and {{math|''a'', ''b'', ''c''}} denote the lengths of the respective opposite edges. They are related by various formulas, which are named by the trigonometric functions they involve.\n\n====Law of sines====\nThe '''[[law of sines]]''' states that for an arbitrary triangle with sides {{mvar|a}}, {{mvar|b}}, and {{mvar|c}} and angles opposite those sides {{mvar|A}}, {{mvar|B}} and {{mvar|C}}:\n\n:<math>\\frac{\\sin A}{a} = \\frac{\\sin B}{b} = \\frac{\\sin C}{c} = \\frac{2\\Delta}{abc},</math>\n\nwhere {{math|Δ}} is the area of the triangle,\nor, equivalently,\n\n:<math>\\frac{a}{\\sin A} = \\frac{b}{\\sin B} = \\frac{c}{\\sin C} = 2R,</math>\n\nwhere {{mvar|R}} is the triangle's [[circumscribed circle|circumradius]].\n\nIt can be proven by dividing the triangle into two right ones and using the above definition of sine. The law of sines is useful for computing the lengths of the unknown sides in a triangle if two angles and one side are known. This is a common situation occurring in ''[[triangulation]]'', a technique to determine unknown distances by measuring two angles and an accessible enclosed distance.\n\n====Law of cosines====\nThe '''[[law of cosines]]''' (also known as the cosine formula or cosine rule) is an extension of the [[Pythagorean theorem]]:\n\n:<math>c^2=a^2+b^2-2ab\\cos C, \\, </math>\nor equivalently,\n:<math>\\cos C=\\frac{a^2+b^2-c^2}{2ab}.</math>\n\nIn this formula the angle at {{mvar|C}} is opposite to the side&nbsp;{{mvar|c}}. This theorem can be proven by dividing the triangle into two right ones and using the [[Pythagorean theorem]].\n\nThe law of cosines can be used to determine a side of a triangle if two sides and the angle between them are known.  It can also be used to find the cosines of an angle (and consequently the angles themselves) if the lengths of all the sides are known.\n\n====Law of tangents====\n{{main|Law of tangents}}\nThe following all form the law of tangents<ref name=\"Allen_1976\"/>\n\n: <math>\n\\frac{\\tan \\frac{A-B}{2 }}{\\tan \\frac{A+B}{2 } } = \\frac{a-b}{a+b}\\,; \\qquad\n\\frac{\\tan \\frac{A-C}{2 }}{\\tan \\frac{A+C}{2 } } = \\frac{a-c}{a+c}\\,; \\qquad\n\\frac{\\tan \\frac{B-C}{2 }}{\\tan \\frac{B+C}{2 } } = \\frac{b-c}{b+c}</math>\n\nThe explanation of the formulae in words would be cumbersome, but the patterns of sums and differences, for the lengths and corresponding opposite angles, are apparent in the theorem.\n\n====Law of cotangents====\n{{main|Law of cotangents}}\nIf\n\n: <math> \\zeta = \\sqrt{\\frac{1}{s} (s-a)(s-b)(s-c)}\\ </math> (the radius of the inscribed circle for the triangle)\n\nand\n\n: <math> s = \\frac{a+b+c}{2 }\\ </math> (the semi-perimeter for the triangle),\n\nthen the following all form the law of cotangents<ref name=\"Allen_1976\"/>\n\n: <math>\n\\cot{ \\frac{A}{2 }} = \\frac{s-a}{\\zeta }\\,; \\qquad\n\\cot{ \\frac{B}{2 }} = \\frac{s-b}{\\zeta }\\,; \\qquad\n\\cot{ \\frac{C}{2 }} = \\frac{s-c}{\\zeta }</math>\n\nIt follows that\n\n: <math> \\frac{\\cot \\dfrac{A}{2}}{s-a} = \\frac{\\cot \\dfrac{B}{2}}{s-b} = \\frac{\\cot \\dfrac{C}{2}}{s-c}. </math>\n\nIn words the theorem is: the cotangent of a half-angle equals the ratio of the semi-perimeter minus the opposite side to the said angle, to the inradius for the triangle.\n\n[[File:Lissajous curve 5by4.svg|thumb|right|A [[Lissajous curve]], a figure formed with a trigonometry-based function.]]\n\n===Periodic functions===\n[[File:Synthesis square.gif|thumb|340px|right|An animation of the [[additive synthesis]] of a [[square wave]] with an increasing number of harmonics]]\n[[File:Sawtooth Fourier Animation.gif|thumb|280px|Sinusoidal basis functions (bottom) can form a sawtooth wave (top) when added. All the basis functions have nodes at the nodes of the sawtooth, and all but the fundamental ({{math|1=''k'' = 1}}) have additional nodes. The oscillation seen about the sawtooth when {{mvar|k}} is large is called the [[Gibbs phenomenon]]]]\n\nThe trigonometric functions are also important in physics. The sine and the cosine functions, for example, are used to describe [[simple harmonic motion]], which models many natural phenomena, such as the movement of a mass attached to a spring and, for small angles, the pendular motion of a mass hanging by a string. The sine and cosine functions are one-dimensional projections of [[uniform circular motion]].\n\nTrigonometric functions also prove to be useful in the study of general [[periodic function]]s. The characteristic wave patterns of periodic functions are useful for modeling recurring phenomena such as sound or light [[wave]]s.<ref name=\"Farlow_1993\"/>\n\nUnder rather general conditions, a periodic function {{math|''f''(''x'')}} can be expressed as a sum of sine waves or cosine waves in a [[Fourier series]].<ref name=\"Folland_1992\"/> Denoting the sine or cosine [[basis functions]] by {{mvar|φ<sub>k</sub>}}, the expansion of the periodic function {{math|''f''(''t'')}} takes the form:\n\n:<math>f(t) = \\sum _{k=1}^\\infty c_k \\varphi_k(t). </math>\n\nFor example, the [[square wave]] can be written as the [[Fourier series]]\n\n:<math> f_\\text{square}(t) = \\frac{4}{\\pi} \\sum_{k=1}^\\infty {\\sin \\big( (2k-1)t \\big) \\over 2k-1}.</math>\n\nIn the animation of a square wave at top right it can be seen that just a few terms already produce a fairly good approximation. The superposition of several terms in the expansion of a [[sawtooth wave]] are shown underneath.\n\n==History==\n{{Main|History of trigonometric functions}}\nWhile the early study of trigonometry can be traced to antiquity, the trigonometric functions as they are in use today were developed in the medieval period. The [[Chord (geometry)|chord]] function was discovered by [[Hipparchus]] of [[İznik|Nicaea]] (180–125&nbsp;BCE) and [[Ptolemy]] of [[Egypt (Roman province)|Roman Egypt]] (90–165&nbsp;CE).\n\nThe functions sine and cosine can be traced to the [[Jyā, koti-jyā and utkrama-jyā|''jyā'' and ''koti-jyā'']] functions used in [[Gupta period]] [[Indian astronomy]] (''[[Aryabhatiya]]'', ''[[Surya Siddhanta]]''), via translation from Sanskrit to Arabic and then from Arabic to Latin.<ref name=\"Boyer_1991\"/>\n\nAll six trigonometric functions in current use were known in [[Islamic mathematics]] by the 9th century, as was the law of sines, used in [[solving triangles]].<ref name=\"Gingerich_1986\"/> [[Al-Khwārizmī]] produced tables of sines, cosines and tangents.\nThey were studied by authors including [[Omar Khayyám]], [[Bhāskara II]], [[Nasir al-Din al-Tusi]], [[Jamshīd al-Kāshī]] (14th century), [[Ulugh Beg]] (14th century), [[Regiomontanus]] (1464), [[Georg Joachim Rheticus|Rheticus]], and Rheticus' student [[Valentinus Otho]].\n\n[[Madhava of Sangamagrama]] (c. 1400) made early strides in the [[mathematical analysis|analysis]] of trigonometric functions in terms of [[series (mathematics)|infinite series]].<ref name=\"mact-biog\"/>\n\nThe terms ''tangent'' and ''secant'' were first introduced by the [[Denmark|Danish]] mathematician [[Thomas Fincke]] in his book ''Geometria rotundi'' (1583).<ref name=\"Fincke\"/>\n\nThe first published use of the abbreviations ''sin'', ''cos'', and ''tan'' is probably by the [[16th century]] French mathematician [[Albert Girard]].{{citation needed|date=April 2018}}\n\nIn a paper published in 1682, [[Gottfried Leibniz|Leibniz]] proved that {{math|sin ''x''}} is not an [[algebraic function]] of {{mvar|x}}.<ref name=\"Bourbaki_1994\"/>\n\n[[Leonhard Euler]]'s ''Introductio in analysin infinitorum'' (1748) was mostly responsible for establishing the analytic treatment of trigonometric functions in Europe, also defining them as infinite series and presenting \"[[Euler's formula]]\", as well as near-modern abbreviations (''sin.'', ''cos.'', ''tang.'', ''cot.'', ''sec.'', and ''cosec.'').<ref name=\"Boyer_1991\"/>\n\nA few functions were common historically, but are now seldom used, such as the [[chord (geometry)|chord]] ({{math|1=crd(''θ'') = 2 sin({{sfrac|''θ''|2}})}}), the [[versine]] ({{math|1=versin(''θ'') = 1 − cos(''θ'') = 2 sin<sup>2</sup>({{sfrac|''θ''|2}})}}) (which appeared in the earliest tables<ref name=\"Boyer_1991\"/>), the [[coversine]] ({{math|1=coversin(''θ'') = 1 − sin(''θ'') = versin({{sfrac|{{pi}}|2}} − ''θ'')}}), the [[haversine]] ({{math|1=haversin(''θ'') = {{sfrac|1|2}}versin(''θ'') = sin<sup>2</sup>({{sfrac|''θ''|2}})}}),<ref>{{harvtxt|Nielsen|1966|pp=xxiii–xxiv}}</ref> the [[exsecant]] ({{math|1=exsec(''θ'') = sec(''θ'') − 1}}), and the [[excosecant]] ({{math|1=excsc(''θ'') = exsec({{sfrac|{{pi}}|2}} − ''θ'') = csc(''θ'') − 1}}). See [[List of trigonometric identities]] for more relations between these functions.\n\n==Etymology==\nThe word ''sine'' derives<ref>The anglicized form is first recorded in 1593 in [[Thomas Fale]]'s ''Horologiographia, the Art of Dialling''.</ref> from [[Latin]] ''[[wikt:sinus|sinus]]'', meaning \"bend; bay\", and more specifically \"the hanging fold of the upper part of a [[toga]]\", \"the bosom of a garment\", which was chosen as the translation of what was interpreted as the Arabic word ''jaib'', meaning \"pocket\" or \"fold\" in the twelfth-century translations of works by [[Al-Battani]] and [[Muḥammad ibn Mūsā al-Khwārizmī|al-Khwārizmī]] into [[Medieval Latin]].<ref>Various sources credit the first use of ''sinus'' to either \n* [[Plato Tiburtinus]]'s 1116 translation of the ''Astronomy'' of [[Al-Battani]]\n* [[Gerard of Cremona]]'s translation of the ''Algebra'' of [[Muḥammad ibn Mūsā al-Khwārizmī|al-Khwārizmī]]\n* [[Robert of Chester]]'s 1145 translation of the tables of al-Khwārizmī\nSee Merlet, [https://link.springer.com/chapter/10.1007/1-4020-2204-2_16#page-1 ''A Note on the History of the Trigonometric Functions''] in Ceccarelli (ed.), ''International Symposium on History of Machines and Mechanisms'', Springer, 2004<br>See Maor (1998), chapter 3, for an earlier etymology crediting Gerard.<br>See {{cite book |last=Katx |first=Victor |date=July 2008 |title=A history of mathematics |edition=3rd |location=Boston |publisher=[[Pearson (publisher)|Pearson]] |page=210 (sidebar) |isbn= 978-0321387004 |language=English }}</ref>\nThe choice was based on a misreading of the Arabic written form ''j-y-b'' ({{lang|ar|[[:wikt:جيب|جيب]]}}), which itself originated as a [[transliteration]] from Sanskrit ''{{IAST|jīvā}}'', which along with its synonym ''{{IAST|jyā}}''  (the standard Sanskrit term for the sine) translates to \"bowstring\", being in turn adopted from [[Ancient Greek language|Ancient Greek]] {{lang|grc|[[Chord (geometry)|χορδή]]}} \"string\".<ref name=\"Plofker_2009\"/>\n\nThe word ''tangent'' comes from Latin ''tangens'' meaning \"touching\", since the line ''touches'' the circle of unit radius, whereas ''secant'' stems from Latin ''secans''—\"cutting\"—since the line ''cuts'' the circle.<ref>Oxford English Dictionary</ref>\n\nThe prefix \"[[co (function prefix)|co-]]\" (in \"cosine\", \"cotangent\", \"cosecant\") is found in [[Edmund Gunter]]'s ''Canon triangulorum'' (1620), which defines the ''cosinus'' as an abbreviation for the ''sinus complementi'' (sine of the [[complementary angle]]) and proceeds to define the ''cotangens'' similarly.<ref name=\"Gunter_1620\"/><ref name=\"Roegel_2010\"/>\n\n==See also==\n{{colbegin|colwidth=25em}}\n* [[All Students Take Calculus]] — a mnemonic for recalling the signs of trigonometric functions in a particular quadrant of a Cartesian plane\n* [[Aryabhata's sine table]]\n* [[Bhaskara I's sine approximation formula]]\n* [[Generalized trigonometry]]\n* [[Generating trigonometric tables]]\n* [[Hyperbolic function]]\n* [[List of periodic functions]]\n* [[List of trigonometric identities]]\n* [[Madhava series]]\n* [[Madhava's sine table]]\n* [[Polar sine]] — a generalization to vertex angles\n* [[Proofs of trigonometric identities]]\n* [[Versine]] — for several less used trigonometric functions\n{{colend}}\n\n==Notes==\n{{reflist|refs=\n<ref name=\"Klein_1924\">{{cite book |title=Elementarmathematik vom höheren Standpunkt aus: Arithmetik, Algebra, Analysis |volume=1 |author-first=Christian Felix |author-last=Klein |author-link=Christian Felix Klein |date=1924<!-- 1927 --> |orig-year=1902<!-- 1908 --> |edition=3rd |publisher=[[J. Springer]] |location=Berlin |language=German}}</ref>\n<ref name=\"Klein_2004\">{{cite book |title=Elementary Mathematics from an Advanced Standpoint: Arithmetic, Algebra, Analysis |author-first=Christian Felix |author-last=Klein |author-link=Christian Felix Klein |date=2004 |orig-year=1932 |edition=Translation of 3rd German |publisher=[[Dover Publications, Inc.]] / [[The Macmillan Company]] |translator-first1=E. R. |translator-last1=Hedrick |translator-first2=C. A. |translator-last2=Noble |isbn=978-0-48643480-3 |url=https://books.google.com/books?id=8KuoxgykfbkC |access-date=2017-08-13 |deadurl=no |archiveurl=https://web.archive.org/web/20180215144848/https://books.google.com/books?id=8KuoxgykfbkC |archivedate=2018-02-15 |df= }}</ref>\n<ref name=\"Heng\">Heng, Cheng and Talbert, [https://books.google.com/books?id=ZZoxLiJBwOUC&amp;pg=PA228 \"Additional Mathematics\"] {{webarchive|url=https://web.archive.org/web/20150320011115/http://books.google.com/books?id=ZZoxLiJBwOUC&pg=PA228 |date=2015-03-20 }}, page 228</ref>\n<ref name=\"Larson_2013\">{{cite book |title=Trigonometry |edition=9th |first1=Ron |last1=Larson |publisher=Cengage Learning |date=2013 |isbn=978-1-285-60718-4 |page=153 |url=https://books.google.com/books?id=zbgWAAAAQBAJ |deadurl=no |archiveurl=https://web.archive.org/web/20180215144848/https://books.google.com/books?id=zbgWAAAAQBAJ |archivedate=2018-02-15 |df= }} [https://books.google.com/books?id=zbgWAAAAQBAJ&pg=PA153 Extract of page 153] {{webarchive|url=https://web.archive.org/web/20180215144848/https://books.google.com/books?id=zbgWAAAAQBAJ&pg=PA153 |date=2018-02-15 }}</ref>\n<ref name=\"Aigner_2000\">{{cite book |author-last1=Aigner |author-first1=Martin |author1-link=Martin Aigner |author-last2=Ziegler |author-first2=Günter M. |author-link2=Günter Ziegler |title=Proofs from THE BOOK |publisher=[[Springer-Verlag]] |edition=Second |date=2000 |isbn=978-3-642-00855-9 |page=149 |url=https://www.springer.com/mathematics/book/978-3-642-00855-9 |deadurl=no |archiveurl=https://web.archive.org/web/20140308034453/http://www.springer.com/mathematics/book/978-3-642-00855-9 |archivedate=2014-03-08 |df= }}</ref>\n<ref name=\"Remmert_1991\">{{cite book |title=Theory of complex functions |author-first1=Reinhold |author-last1=Remmert |publisher=Springer |date=1991 |isbn=978-0-387-97195-7 |page=327 |url=https://books.google.com/books?id=CC0dQxtYb6kC |deadurl=no |archiveurl=https://web.archive.org/web/20150320010718/http://books.google.com/books?id=CC0dQxtYb6kC |archivedate=2015-03-20 |df= }} [https://books.google.com/books?id=CC0dQxtYb6kC&pg=PA327 Extract of page 327] {{webarchive|url=https://web.archive.org/web/20150320010448/http://books.google.com/books?id=CC0dQxtYb6kC&pg=PA327 |date=2015-03-20 }}</ref>\n<ref name=\"Kannappan_2009\">{{cite book |author-last=Kannappan |author-first=Palaniappan |title=Functional Equations and Inequalities with Applications |date=2009 |publisher=Springer |isbn=978-0387894911}}</ref>\n<ref name=\"Allen_1976\">The Universal Encyclopaedia of Mathematics, Pan Reference Books, 1976, page 529-530. English version George Allen and Unwin, 1964. Translated from the German version Meyers Rechenduden, 1960.</ref>\n<ref name=\"Farlow_1993\">{{cite book |title=Partial differential equations for scientists and engineers |url=https://books.google.com/books?id=DLUYeSb49eAC&pg=PA82 |author-first=Stanley J. |author-last=Farlow |page=82 |isbn=978-0-486-67620-3 |publisher=Courier Dover Publications |edition=Reprint of Wiley 1982 |date=1993 |deadurl=no |archiveurl=https://web.archive.org/web/20150320011420/http://books.google.com/books?id=DLUYeSb49eAC&pg=PA82 |archivedate=2015-03-20 |df= }}</ref>\n<ref name=\"Folland_1992\">See for example, {{cite book |author-first=Gerald B. |author-last=Folland |title=Fourier Analysis and its Applications |publisher=American Mathematical Society |edition=Reprint of Wadsworth & Brooks/Cole 1992 |chapter-url=https://books.google.com/books?id=idAomhpwI8MC&pg=PA77 |pages=77ff |chapter=Convergence and completeness |date=2009 |isbn=978-0-8218-4790-9 |deadurl=no |archiveurl=https://web.archive.org/web/20150319230954/http://books.google.com/books?id=idAomhpwI8MC&pg=PA77 |archivedate=2015-03-19 |df= }}</ref>\n<ref name=\"Boyer_1991\">Boyer, Carl B. (1991). A History of Mathematics (Second ed.). John Wiley & Sons, Inc. {{isbn|0-471-54397-7}}, p. 210.</ref>\n<ref name=\"Gingerich_1986\">{{cite magazine |title=Islamic Astronomy |author-first=Owen |author-last=Gingerich |magazine=[[Scientific American]] |date=1986 |volume=254 |page=74 |url=http://faculty.kfupm.edu.sa/PHYS/alshukri/PHYS215/Islamic_astronomy.htm |access-date=2010-07-13 |archive-url=https://web.archive.org/web/20131019140821/http://faculty.kfupm.edu.sa/PHYS/alshukri/PHYS215/Islamic_astronomy.htm |archive-date=2013-10-19}}</ref>\n<ref name=\"mact-biog\">{{cite web |publisher=School of Mathematics and Statistics University of St Andrews, Scotland |title=Madhava of Sangamagrama |author-first1=J. J. |author-last1=O'Connor |author-first2=E. F. |author-last2=Robertson |url=http://www-gap.dcs.st-and.ac.uk/~history/Biographies/Madhava.html |archive-url=https://web.archive.org/web/20060514012903/http://www-gap.dcs.st-and.ac.uk/~history/Biographies/Madhava.html |dead-url=yes |archive-date=2006-05-14 |access-date=2007-09-08 }}</ref>\n<ref name=\"Fincke\">{{cite web |url=http://www-history.mcs.st-andrews.ac.uk/Biographies/Fincke.html |title=Fincke biography |access-date=2017-03-15 |deadurl=no |archiveurl=https://web.archive.org/web/20170107035144/http://www-history.mcs.st-andrews.ac.uk/Biographies/Fincke.html |archivedate=2017-01-07 |df= }}</ref>\n<ref name=\"Bourbaki_1994\">{{cite book |title=Elements of the History of Mathematics |author-first=Nicolás |author-last=Bourbaki |publisher=Springer |date=1994}}</ref>\n<ref name=\"Gunter_1620\">{{cite book |author-first=Edmund |author-last=Gunter |author-link=Edmund Gunter |title=Canon triangulorum |date=1620}}</ref>\n<ref name=\"Roegel_2010\">{{cite web |title=A reconstruction of Gunter's Canon triangulorum (1620) |editor-first=Denis |editor-last=Roegel |type=Research report |publisher=HAL |date=2010-12-06 |id=inria-00543938 |url=https://hal.inria.fr/inria-00543938/document |access-date=2017-07-28 |dead-url=no |archive-url=https://web.archive.org/web/20170728192238/https://hal.inria.fr/inria-00543938/document |archive-date=2017-07-28}}</ref>\n<ref name=\"Plofker_2009\">See Plofker, ''Mathematics in India'', Princeton University Press, 2009, p. 257<br>See {{cite web |url=http://www.clarku.edu/~djoyce/trig/ |title=Clark University |deadurl=no |archiveurl=https://web.archive.org/web/20080615133310/http://www.clarku.edu/~djoyce/trig/ |archivedate=2008-06-15 |df= }}<br>See Maor (1998), chapter 3, regarding the etymology.</ref>\n}}\n\n==References==\n{{refbegin}}\n* {{AS ref}}\n* [[Lars Ahlfors]], ''Complex Analysis: an introduction to the theory of analytic functions of one complex variable'', second edition, [[McGraw-Hill Book Company]], New York, 1966.\n* [[Carl Benjamin Boyer|Boyer, Carl B.]], ''A History of Mathematics'', John Wiley & Sons, Inc., 2nd edition. (1991). {{isbn|0-471-54397-7}}.\n* Gal, Shmuel and Bachelis, Boris. An accurate elementary mathematical library for the IEEE floating point standard, ACM Transactions on Mathematical Software (1991).\n* Joseph, George G., ''The Crest of the Peacock: Non-European Roots of Mathematics'', 2nd ed. [[Penguin Books]], London. (2000). {{isbn|0-691-00659-8}}.\n* Kantabutra, Vitit, \"On hardware for computing exponential and trigonometric functions,\" ''IEEE Trans. Computers'' '''45''' (3), 328–339 (1996).\n* Maor, Eli, ''[https://web.archive.org/web/20040404234808/http://www.pupress.princeton.edu/books/maor/ Trigonometric Delights]'', Princeton Univ. Press. (1998). Reprint edition (February 25, 2002): {{isbn|0-691-09541-8}}.{{dead link|date = April 2014}}\n* Needham, Tristan, [https://web.archive.org/web/20040602145226/http://www.usfca.edu/vca/PDF/vca-preface.pdf \"Preface\"]\" to ''[http://www.usfca.edu/vca/ Visual Complex Analysis]''. Oxford University Press, (1999). {{isbn|0-19-853446-9}}.\n* {{citation |last1=Nielsen |first1=Kaj L. |title=Logarithmic and Trigonometric Tables to Five Places |edition=2nd |location=New York, USA |publisher=[[Barnes & Noble]] |date=1966 |lccn=61-9103}}\n* O'Connor, J. J., and E. F. Robertson, [https://www.webcitation.org/6Auwxe6v3?url=http://www-gap.dcs.st-and.ac.uk/~history/HistTopics/Trigonometric_functions.html \"Trigonometric functions\"], ''[[MacTutor History of Mathematics archive]]''. (1996).\n* O'Connor, J. J., and E. F. Robertson, [http://www-groups.dcs.st-and.ac.uk/~history/Mathematicians/Madhava.html \"Madhava of Sangamagramma\"], ''[[MacTutor History of Mathematics archive]]''. (2000).\n* Pearce, Ian G., [http://www-history.mcs.st-andrews.ac.uk/history/Projects/Pearce/Chapters/Ch9_3.html \"Madhava of Sangamagramma\"], ''[[MacTutor History of Mathematics archive]]''. (2002).\n* Weisstein, Eric W., [http://mathworld.wolfram.com/Tangent.html \"Tangent\"] from ''[[MathWorld]]'', accessed 21 January 2006.\n{{refend}}\n\n==External links==\n{{Wikibooks|Trigonometry}}\n* {{springer |title=Trigonometric functions|id=p/t094210}}\n* [http://www.visionlearning.com/library/module_viewer.php?mid=131&l=&c3= Visionlearning Module on Wave Mathematics]\n* [https://web.archive.org/web/20071006172054/http://glab.trixon.se/ GonioLab] Visualization of the unit circle, trigonometric and hyperbolic functions\n* [http://mathworld.wolfram.com/q-Sine.html q-Sine] Article about the [[q-analog]] of sin at [[MathWorld]]\n* [http://mathworld.wolfram.com/q-Cosine.html q-Cosine] Article about the [[q-analog]] of cos at [[MathWorld]]\n{{Authority control}}\n\n{{DEFAULTSORT:Trigonometric Functions}}\n[[Category:Angle]]\n[[Category:Trigonometry]]\n[[Category:Trigonometric functions| ]]\n[[Category:Elementary special functions]]\n[[Category:Analytic functions]]\n[[Category:Ratios]]\n[[Category:Dimensionless numbers]]"
    },
    {
      "title": "List of exponential topics",
      "url": "https://en.wikipedia.org/wiki/List_of_exponential_topics",
      "text": "This is a '''list of exponential topics''', by Wikipedia page. See also [[list of logarithm topics]].\n* [[Accelerating change]]\n* [[Mental calculation|Approximating natural exponents (log base e)]]\n* [[Artin–Hasse exponential]] [[Talk:Artin–Hasse exponential| ]]\n* [[Bacterial growth]] [[Talk:Bacterial growth| ]]\n* [[Baker–Campbell–Hausdorff formula]]\n* [[Cell growth]] [[Talk:Cell growth| ]]\n* [[Barometric formula]] [[Talk:Barometric formula| ]]\n* [[Beer–Lambert law]] [[Talk:Beer–Lambert law| ]]\n* [[Characterizations of the exponential function]] [[Talk:Characterizations of the exponential function| ]]\n* [[Catenary]] [[Talk:Catenary| ]]\n* [[Compound interest]] [[Talk:Compound interest| ]]\n* [[De Moivre's formula]] [[Talk:de Moivre's formula| ]]\n* [[Derivative of the exponential map]] [[Talk:Derivative of the exponential map| ]]\n* [[Doléans-Dade exponential]] [[Talk:Doléans-Dade exponential| ]]\n* [[Doubling time]] [[Talk:Doubling time| ]]\n* [[e-folding|''e''-folding]] [[Talk:e-folding| ]]\n* [[Elimination half-life]] [[Talk:Elimination half-life| ]]\n* [[Error exponent]] [[Talk:Error exponent| ]]\n* [[Euler's formula]] [[Talk:Euler's formula| ]]\n* [[Euler's identity]] [[Talk:Euler's identity| ]]\n* [[e (mathematical constant)]] [[Talk:e (mathematical constant)| ]]\n* [[Exponent]] [[Talk:Exponent| ]]\n* [[Exponent bias]] [[Talk:Exponent bias| ]]\n* [[Exponential (disambiguation)]] [[Talk:Exponential| ]]\n* [[Exponential backoff]] [[Talk:Exponential backoff| ]]\n* [[Exponential decay]] [[Talk:Exponential decay| ]]\n* [[Exponential dichotomy]] [[Talk:Exponential dichotomy| ]]\n* [[Exponential discounting]] [[Talk:Exponential discounting| ]]\n* [[Exponential diophantine equation]] [[Talk:Exponential diophantine equation| ]]\n* [[Exponential dispersion model]] [[Talk:Exponential dispersion model| ]]\n* [[Exponential distribution]] [[Talk:Exponential distribution| ]]\n* [[Exponential error]] [[Talk:Exponential error| ]]\n* [[Exponential factorial]] [[Talk:Exponential factorial| ]]\n* [[Exponential family]] [[Talk:Exponential family| ]]\n* [[Exponential field]] [[Talk:Exponential field| ]]\n* [[Exponential formula]] [[Talk:Exponential formula| ]]\n* [[Exponential function]] [[Talk:Exponential function| ]]\n* [[Exponential generating function]] [[Talk:Exponential generating function| ]]\n* [[Exponential-Golomb coding]] [[Talk:Exponential-Golomb coding| ]]\n* [[Exponential growth]] [[Talk:Exponential growth| ]]\n* [[Exponential hierarchy]] [[Talk:Exponential hierarchy| ]]\n* [[Exponential integral]] [[Talk:Exponential integral| ]]\n* [[Exponential integrator]] [[Talk:Exponential integrator| ]]\n* [[Exponential map (Lie theory)]] [[Talk:Exponential map  (Lie theory)| ]]\n* [[Exponential map (Riemannian geometry)]] [[Talk:Exponential map (Riemannian geometry)| ]]\n* [[Exponential notation]] [[Talk:Exponential notation| ]]\n* [[Exponential object]] [[Talk:Exponential object| ]] ([[category theory]])\n* [[Exponential polynomials]]&mdash;see also [[Touchard polynomials]] [[Talk:Touchard polynomials| ]] ([[combinatorics]])\n* [[Exponential response formula]] [[Talk:Exponential response formula| ]]\n* [[Exponential sheaf sequence]] [[Talk:Exponential sheaf sequence| ]]\n* [[Exponential smoothing]] [[Talk:Exponential smoothing| ]]\n* [[Exponential stability]] [[Talk:Exponential stability| ]]\n* [[Exponential sum]] [[Talk:Exponential sum| ]]\n* [[Exponential time]] [[Talk:Exponential time| ]]\n** [[Sub-exponential time]] [[Talk:Sub-exponential time| ]]\n* [[Exponential tree]] [[Talk:Exponential tree| ]]\n* [[Exponential type]] [[Talk:Exponential type| ]]\n* [[Exponentially equivalent measures]] [[Talk:Exponentially equivalent measures| ]]\n* [[Exponentiating by squaring]] [[Talk:Exponentiating by squaring| ]]\n* [[Exponentiation]] [[Exponentiation| ]]\n* [[Fermat's Last Theorem]]\n* [[Forgetting curve]]\n* [[Gaussian function]] [[Talk:Gaussian function| ]]\n* [[Gudermannian function]] [[Talk:Gudermannian function| ]]\n* [[Half-exponential function]] [[Talk:Half-exponential function| ]]\n* [[Half-life]] [[Talk:Half-life| ]]\n* [[Hyperbolic function]] [[Talk:Hyperbolic function| ]]\n* [[Inflation]], [[inflation rate]] [[Talk:Inflation| ]] [[Talk:inflation rate| ]]\n* [[Interest]] [[Talk:Interest| ]]\n* [[Lifetime (physics)]] [[Talk:Lifetime (physics)| ]]\n* [[Limiting factor]] [[Talk:Limiting factor| ]]\n* [[Lindemann–Weierstrass theorem]] [[Talk:Lindemann–Weierstrass theorem| ]]\n* [[List of integrals of exponential functions]] [[Talk:List of integrals of exponential functions| ]]\n* [[List of integrals of hyperbolic functions]] [[Talk:List of integrals of hyperbolic functions| ]]\n* [[Lyapunov exponent]] [[Talk:Lyapunov exponent| ]]\n* [[Malthusian catastrophe]] [[Talk:Malthusian catastrophe| ]]\n* [[Malthusian growth model]] [[Talk:Malthusian growth model| ]]\n* [[Marshall–Olkin exponential distribution]] [[Talk:Marshall–Olkin exponential distribution| ]]\n* [[Matrix exponential]] [[Talk:Matrix exponential| ]]\n* [[Moore's law]] [[Talk:Moore's law| ]]\n* [[Nachbin's theorem]] [[Talk:Nachbin's theorem| ]]\n* [[Piano key frequencies]]\n* [[p-adic exponential function]] [[Talk:p-adic exponential function| ]]\n* [[Power law]] [[Talk:Power law| ]]\n* [[Proof that e is irrational]] [[Talk:Proof that e is irrational| ]]\n* [[Lindemann–Weierstrass theorem|Proof that e is transcendental]] [[Talk:Lindemann–Weierstrass theorem| ]]\n* [[Q-exponential]] [[Talk:Q-exponential| ]]\n* [[Radioactive decay]] [[Talk:Radioactive decay| ]]\n* [[Rule of 70]], [[Rule of 72]] [[Talk:Rule of 70| ]] [[Talk:Rule of 72| ]]\n* [[Scientific notation]]\n* [[Six exponentials theorem]] [[Talk:Six exponentials theorem| ]]\n* [[Spontaneous emission]] [[Talk:Spontaneous emission| ]]\n* [[Super-exponentiation]] [[Talk:Super-exponentiation| ]]\n* [[Tetration]] [[Talk:Tetration| ]]\n* [[Versor]] [[Talk:Versor| ]]\n* [[Wilkie's theorem]] [[Talk:Wilkie's theorem| ]]\n* [[Zenzizenzizenzic]] [[Talk:Zenzizenzizenzic| ]]\n\n[[Category:Exponentials|*]]\n[[Category:Mathematics-related lists|Exponentials]]\n[[Category:Lists of topics|Exponential]]"
    },
    {
      "title": "Addition-chain exponentiation",
      "url": "https://en.wikipedia.org/wiki/Addition-chain_exponentiation",
      "text": "In [[mathematics]] and [[computer science]], optimal '''addition-chain exponentiation''' is a method of [[exponentiation]] by positive [[integer]] powers that requires a minimal number of multiplications. This corresponds to the sequence [https://oeis.org/A003313 A003313] on the [[Online Encyclopedia of Integer Sequences]]. It works by creating the shortest [[addition chain]] that generates the desired exponent. Each exponentiation in the chain can be evaluated by multiplying  two of the earlier exponentiation results. More generally, ''addition-chain exponentiation'' may also refer to exponentiation by non-minimal addition chains constructed by a variety of algorithms (since a shortest addition chain is very difficult to find).\n\nThe shortest addition-chain [[algorithm]] requires no more multiplications than [[binary exponentiation]] and usually less. The first example of where it does better is for ''a''<sup>15</sup>, where the binary method needs six multiplications but a shortest addition chain requires only five:\n\n:<math>a^{15} = a \\times (a \\times [a \\times a^2]^2)^2  \\!</math>  (binary, 6 multiplications)\n:<math>a^{15} = a^3 \\times ([a^3]^2)^2  \\!</math> (shortest addition chain, 5 multiplications).\n{|class=wikitable\n|+Table demonstrating how to do ''Exponentiation'' using ''Addition Chains''\n|-\n!Number of<br>Multiplications || Actual<br>Exponentiation || Specific implementation of<br>''Addition Chains'' to do Exponentiation\n|-\n|0|| a<sup>1</sup> || a\n|-\n|1|| a<sup>2</sup> || a × a\n|-\n|2|| a<sup>3</sup> || a × a × a\n|-\n|2|| a<sup>4</sup> || (a × a→b) × b\n|-\n|3|| a<sup>5</sup> || (a × a→b) × b × a\n|-\n|3|| a<sup>6</sup> || (a × a→b) × b × b\n|-\n|4|| a<sup>7</sup> || (a × a→b) × b × b × a\n|-\n|3|| a<sup>8</sup> || ((a × a→b) × b→d) × d\n|-\n|4|| a<sup>9</sup> || (a × a × a→c) × c × c\n|-\n|4|| a<sup>10</sup> || ((a × a→b) × b→d) × d × b \n|-\n|5|| a<sup>11</sup> || ((a × a→b) × b→d) × d × b × a\n|-\n|4|| a<sup>12</sup> || ((a × a→b) × b→d) × d × d\n|-\n|5|| a<sup>13</sup> || ((a × a→b) × b→d) × d × d × a\n|-\n|5|| a<sup>14</sup> || ((a × a→b) × b→d) × d × d × b\n|-\n|5|| a<sup>15</sup> || ((a × a→b) × b × a→e) × e × e\n|-\n|4|| a<sup>16</sup> || (((a × a→b) × b→d) × d→h) × h\n|}\nOn the other hand, the determination of a shortest addition chain is hard: no efficient optimal methods are currently known for arbitrary exponents, and the related problem of finding a shortest addition chain for a given set of exponents has been proven [[NP-complete]].<ref>{{Cite journal|first1=Peter|last1=Downey|first2=Benton|last2=Leong|first3=Ravi|last3=Sethi|title=Computing sequences with addition chains|journal=SIAM Journal on Computing|volume=10|issue=3|year=1981|pages=638–646|doi=10.1137/0210047}}</ref> Even given a shortest chain, addition-chain exponentiation requires more memory than the binary method, because it must potentially store many previous exponents from the chain.  So in practice, shortest addition-chain exponentiation is primarily used for small fixed exponents for which a shortest chain can be precomputed and is not too large.\n\nThere are also several methods to ''approximate'' a shortest addition chain, and which often require fewer multiplications than binary exponentiation; binary exponentiation itself is a suboptimal addition-chain algorithm. The optimal algorithm choice depends on the context (such as the relative cost of the multiplication and the number of times a given exponent is re-used).<ref>{{cite journal |author=Gordon, D. M. |year=1998 |title=A survey of fast exponentiation methods |journal=J. Algorithms |volume=27 |pages=129–146 |doi=10.1006/jagm.1997.0913 |url=http://saluc.engr.uconn.edu/refs/sidechannel/gordon97asurvey.pdf |citeseerx=10.1.1.17.7076 |access-date=2013-11-11 |archive-url=https://web.archive.org/web/20131111155543/http://saluc.engr.uconn.edu/refs/sidechannel/gordon97asurvey.pdf |archive-date=2013-11-11 |dead-url=yes |df= }}</ref>\n\nThe problem of finding the shortest addition chain cannot be solved by [[dynamic programming]], because it does not satisfy the assumption of [[optimal substructure]].  That is, it is not sufficient to decompose the power into smaller powers, each of which is computed minimally, since the addition chains for the smaller powers may be related (to share computations).  For example, in the shortest addition chain for ''a''<sup>15</sup> above, the subproblem for ''a''<sup>6</sup> must be computed as (''a''<sup>3</sup>)<sup>2</sup> since ''a''<sup>3</sup> is re-used (as opposed to, say, ''a''<sup>6</sup>&nbsp;=&nbsp;''a''<sup>2</sup>(''a''<sup>2</sup>)<sup>2</sup>, which also requires three multiplies).\n\n==Addition-subtraction–chain exponentiation==\nIf both multiplication and division are allowed, then an [[addition-subtraction chain]] may be used to obtain even fewer total multiplications+divisions (where subtraction corresponds to division). However, the slowness of division compared to multiplication makes this technique unattractive in general.  For exponentiation to [[negative number|negative]] integer powers, on the other hand, since one division is required anyway, an addition-subtraction chain is often beneficial.  One such example is ''a''<sup>&minus;31</sup>, where computing 1/''a''<sup>31</sup> by a shortest addition chain for ''a''<sup>31</sup> requires 7 multiplications and one division, whereas the shortest addition-subtraction chain requires 5 multiplications and one division:\n\n:<math>a^{-31} = a / ((((a^2)^2)^2)^2)^2 \\!</math> (addition-subtraction chain, 5 mults + 1 div).\n\nFor exponentiation on [[elliptic curve]]s, the inverse of a point (''x'',&nbsp;''y'') is available at no cost, since it is simply (''x'',&nbsp;&minus;''y''), and therefore addition-subtraction chains are optimal in this context even for positive integer exponents.<ref>François Morain and Jorge Olivos, \"[http://archive.numdam.org/ARCHIVE/ITA/ITA_1990__24_6/ITA_1990__24_6_531_0/ITA_1990__24_6_531_0.pdf Speeding up the computations on an elliptic curve using addition-subtraction chains],\" ''RAIRO Informatique théoretique et application'' '''24''', pp. 531-543 (1990).</ref>\n\n==References==\n{{reflist}}\n\n* [[Donald E. Knuth]], ''The Art of Computer Programming, Volume 2: Seminumerical Algorithms'', 3rd edition, §4.6.3 (Addison-Wesley: San Francisco, 1998).\n* Daniel J. Bernstein, \"[http://cr.yp.to/papers/pippenger.pdf Pippenger's Algorithm],\" to be incorporated into author's ''High-speed cryptography'' book. (2002)\n\n[[Category:Addition chains]]\n[[Category:Exponentials]]\n[[Category:Computer arithmetic algorithms]]"
    },
    {
      "title": "Base (exponentiation)",
      "url": "https://en.wikipedia.org/wiki/Base_%28exponentiation%29",
      "text": "{{short description|(in exponentiation), number b in an expression of the form b^n}}\n{{Calculation results}}\n\nIn [[exponentiation]], the '''base''' is the number <var>b</var> in an expression of the form <var>b<sup>n</sup></var>.\n\n==Related terms== \nThe number <var>n</var> is called the [[exponent]] and the expression is known formally as exponentiation of <var>b</var> by <var>n</var> or the exponential of <var>n</var> with base <var>b</var>. It is more commonly expressed as \"the <var>n</var>th power of <var>b</var>\", \"<var>b</var> to the <var>n</var>th power\" or \"<var>b</var> to the power <var>n</var>\". For example, the fourth power of 10 is 10,000 because {{nowrap|10<sup>4</sup> {{=}} 10 × 10 × 10 × 10 {{=}} 10,000}}. The term ''power'' strictly refers to the entire expression, but is sometimes used to refer to the exponent.\n\n[[Radix]] is the traditional term for ''base'', but usually refers then to one of the common bases: decimal (10), binary (2), hexadecimal (16), or sexagesimal (60). When the concepts of [[variable (mathematics)|variable]] and [[constant (mathematics)|constant]] came to be distinguished, the process of exponentiation was seen to transcend the [[algebraic function]]s.\n\nIn his 1748 ''Introductio in analysin infinitorum'', [[Leonhard Euler]] referred to \"base a = 10\" in an example. He referred to ''a'' as a \"constant number\" in an extensive consideration of the function F(''z'') = ''a''<sup>z</sup>. First ''z'' is a positive integer, then negative, then a fraction, or rational number.<ref>[[Leonard Euler]] (1748) [http://www.17centurymaths.com/contents/euler/introductiontoanalysisvolone/ch6vol1.pdf Chapter 6: Concerning Exponential and Logarithmic Quantities] of [[Introduction to the Analysis of the Infinite]], translated by Ian Bruce (2013), lk from 17centurymaths.</ref>{{rp|155}}\n\n==Roots==\nWhen the <var>n</var>th power of <var>b</var> equals a number <var>a</var>, or <var>a</var>&nbsp;=&nbsp;<var>b<sup>n</sup></var>, then <var>b</var> is called an \"[[nth root|<var>n</var>th root]]\" of <var>a</var>. For example, 10 is a fourth root of 10,000.\n\n==Logarithms==\nThe [[inverse function]] to exponentiation with base <var>b</var> (when it is [[well-defined]]) is called the [[logarithm]] to base <var>b</var>, denoted log<sub><var>b</var></sub>. Thus:\n\n:log<sub>''b''</sub> ''a'' = ''n''.\n\nFor example, log<sub>10</sub>&nbsp;10,000&nbsp;=&nbsp;4.\n\n==References==\n{{Reflist}}\n\n[[Category:Exponentials]]\n[[Category:Mathematical terminology]]"
    },
    {
      "title": "Catenary",
      "url": "https://en.wikipedia.org/wiki/Catenary",
      "text": "{{short description|Plane curve formed by a hanging cable}}\n{{About|the mathematical curve}}\n{{Redirect|Chainette|the wine grape also known as Chainette|Cinsaut}}\n\n[[File:Kette Kettenkurve Catenary 2008 PD.JPG|thumb|180px|right|A chain hanging from points forms a catenary.]]\n\n[[File:PylonsSunset-5982.jpg|thumb|180px|right|Freely-hanging electric [[power cable]]s (for example, those used on electrified railways) can also form a catenary.]]\n\n[[File:SpiderCatenary.jpg|thumb|180px|right|The silk on a [[spider's web]] forming multiple [[Elastic deformation|elastic]] catenaries.]]\n\nIn [[physics]] and [[geometry]], a '''catenary''' ({{IPAc-en|US|ˈ|k|æ|t|ən|ɛr|i}}, {{IPAc-en|UK|k|ə|ˈ|t|iː|n|ər|i}}) is the [[curve]] that an idealized hanging [[chain]] or [[Wire rope|cable]] assumes under its own [[weight]] when supported only at its ends.\n\nThe catenary curve has a U-like shape, superficially similar in appearance to a [[parabolic arch]], but it is not a [[parabola]].\n\nThe curve appears in the design of certain types of [[Catenary arch|arch]]es and  as a cross section of the [[catenoid]]—the shape assumed by a soap film bounded by two parallel circular rings.\n\nThe catenary is also called the '''alysoid''', '''chainette''',<ref name=\"MathWorld\">[[#MathWorld|MathWorld]]</ref> or, particularly in the materials sciences, '''funicular'''.<ref>''e.g.'':  {{cite book| last = Shodek| first = Daniel L.| title = Structures| edition = 5th| year = 2004| publisher = Prentice Hall| isbn = 978-0-13-048879-4| oclc = 148137330| page = 22 }}</ref>\n\nMathematically, the catenary curve is the [[Graph of a function|graph]] of the [[hyperbolic cosine]] function. The [[surface of revolution]] of the catenary curve, the [[catenoid]], is a [[minimal surface]], specifically a [[minimal surface of revolution]]. A hanging chain will assume a shape of least potential energy which is a catenary.<ref>{{cite web|url=http://galileoandeinstein.physics.virginia.edu/7010/CM_02_CalculusVariations.html |title=\"The Calculus of Variations\" |date=2015 |accessdate=2019-05-03}}</ref> The mathematical properties of the catenary curve were first studied by [[Robert Hooke]] in the 1670s, and its equation was derived by [[Leibniz]], [[Christiaan Huygens|Huygens]] and [[Johann Bernoulli]] in 1691.\n\nCatenaries and related curves are used in architecture and engineering, in the design of bridges and [[Catenary arch|arches]], so that forces do not result in bending moments. In the offshore oil and gas industry, \"catenary\" refers to a [[steel catenary riser]], a pipeline suspended between a production platform and the seabed that adopts an approximate catenary shape.\n\nIn optics and electromagnetics, the hyperbolic cosine and sine functions are basic solutions to Maxwell's equations.<ref>{{cite book |last1=Luo |first1=Xiangang |title=Catenary optics |date=2019 |publisher=Springer |location=Singapore |isbn=978-981-13-4818-1 |doi=10.1007/978-981-13-4818-1 }}</ref> The symmetric modes consisting of two [[evanescent waves]] would form a catenary shape.<ref>{{Cite journal|last=Bourke|first=Levi|last2=Blaikie|first2=Richard J.|date=2017-12-01|title=Herpin effective media resonant underlayers and resonant overlayer designs for ultra-high NA interference lithography|url=https://www.osapublishing.org/josaa/abstract.cfm?uri=josaa-34-12-2243|journal=JOSA A|language=EN|volume=34|issue=12|pages=2243–2249|doi=10.1364/JOSAA.34.002243|pmid=29240100|issn=1520-8532}}</ref><ref>{{Cite journal|last=Pu|first=Mingbo|last2=Guo|first2=Yinghui|last3=Li|first3=Xiong|last4=Ma|first4=Xiaoliang|last5=Luo|first5=Xiangang|date=2018-07-05|title=Revisitation of Extraordinary Young's Interference: from Catenary Optical Fields to Spin–Orbit Interaction in Metasurfaces|journal=ACS Photonics|volume=5|issue=8|pages=3198–3204|language=en|doi=10.1021/acsphotonics.8b00437|issn=2330-4022}}</ref><ref>{{Cite journal|last=Pu|first=Mingbo|last2=Ma|first2=XiaoLiang|last3=Guo|first3=Yinghui|last4=Li|first4=Xiong|last5=Luo|first5=Xiangang|date=2018-07-23|title=Theory of microscopic meta-surface waves based on catenary optical fields and dispersion|url=https://www.osapublishing.org/oe/abstract.cfm?uri=oe-26-15-19555|journal=Optics Express|language=EN|volume=26|issue=15|pages=19555–19562|doi=10.1364/OE.26.019555|pmid=30114126|issn=1094-4087}}</ref>\n\n==History==\n[[File:GaudiCatenaryModel.jpg|thumb|250px|[[Antoni Gaudí]]'s catenary model at [[Casa Milà]]]]\n\nThe word \"catenary\" is derived from the Latin word ''catēna'', which means \"[[chain]]\". The English word \"catenary\" is usually attributed to [[Thomas Jefferson]],<ref>{{cite web|url=http://www.pballew.net/arithme8.html#catenary |title=\"Catenary\" at Math Words |publisher=Pballew.net |date=1995-11-21 |accessdate=2010-11-17}}</ref><ref>{{cite book| last = Barrow| first = John D.| title = 100 Essential Things You Didn't Know You Didn't Know: Math Explains Your World| year = 2010| publisher = W. W. Norton & Company| isbn = 978-0-393-33867-6| page = 27 }}</ref>\nwho wrote in a letter to [[Thomas Paine]] on the construction of an arch for a bridge:\n\n{{quote|I have lately received from Italy a treatise on the [[Mechanical equilibrium|equilibrium]] of arches, by the Abbé Mascheroni. It appears to be a very scientifical work. I have not yet had time to engage in it; but I find that the conclusions of his demonstrations are, that every part of the catenary is in perfect equilibrium.<ref>{{cite book| last = Jefferson| first = Thomas| title = Memoirs, Correspondence and Private Papers of Thomas Jefferson| url = https://books.google.com/?id=wFlq_7_IAEUC&pg=PA419| year = 1829| publisher = Henry Colbura and Richard Bertley| page = 419 }}</ref>}}\n\nIt is often said<ref name=\"Lockwood124\"/> that [[Galileo Galilei|Galileo]] thought the curve of a hanging chain was parabolic. In his ''[[Two New Sciences]]'' (1638), Galileo says that a hanging cord is an approximate parabola, and he correctly observes that this approximation improves as the curvature gets smaller and is almost exact when the elevation is less than 45°.<ref>{{cite book| last = Fahie| first = John Joseph| title = Galileo, His Life and Work| url = https://books.google.com/?id=iX0RAAAAYAAJ&pg=PA359| year = 1903| publisher = J. Murray| pages = 359–360 }}</ref> That the curve followed by a chain is not a parabola was proven by [[Joachim Jungius]] (1587–1657); this result was published posthumously in 1669.<ref name=\"Lockwood124\">[[#Lockwood|Lockwood]] p. 124</ref>\n\nThe application of the catenary to the construction of arches is attributed to [[Robert Hooke]], whose \"true mathematical and mechanical form\" in the context of the rebuilding of [[St Paul's Cathedral]]  alluded to a catenary.<ref>{{Cite journal|jstor=532102 |title=Monuments and Microscopes: Scientific Thinking on a Grand Scale in the Early Royal Society |journal=Notes and Records of the Royal Society of London |volume=55 |issue=2 |pages=289–308 |first=Lisa |last=Jardine|year=2001 |doi=10.1098/rsnr.2001.0145 }}</ref> Some much older arches approximate catenaries, an example of which is the Arch of [[Taq-i Kisra]] in [[Ctesiphon]].<ref>{{cite book| last = Denny| first = Mark| title = Super Structures: The Science of Bridges, Buildings, Dams, and Other Feats of Engineering| year = 2010| publisher = JHU Press| isbn = 978-0-8018-9437-4| pages = 112–113 }}</ref>\n\nIn 1671, Hooke announced to the [[Royal Society]] that he had solved the problem of the optimal shape of an arch, and in 1675 published an encrypted solution as a Latin [[anagram]]<ref>[[cf.]] the anagram for [[Hooke's law]], which appeared in the next paragraph.</ref> in an appendix to his ''Description of Helioscopes,''<ref>{{cite web |url=http://www.lindahall.org/events_exhib/exhibit/exhibits/civil/design.shtml |title=Arch Design |publisher=Lindahall.org |date=2002-10-28 |accessdate=2010-11-17 |deadurl=yes |archiveurl=https://web.archive.org/web/20101113210736/http://www.lindahall.org/events_exhib/exhibit/exhibits/civil/design.shtml |archivedate=2010-11-13 |df= }}</ref> where he wrote that he had found \"a true mathematical and mechanical form of all manner of Arches for Building.\" He did not publish the solution to this anagram<ref>The original anagram was ''abcccddeeeeefggiiiiiiiillmmmmnnnnnooprrsssttttttuuuuuuuux'': the letters of the Latin phrase, alphabetized.</ref> in his lifetime, but in 1705 his executor provided it as ''ut pendet continuum flexile, sic stabit contiguum rigidum inversum'', meaning \"As hangs a flexible cable so, inverted, stand the touching pieces of an arch.\"\n\nIn 1691, [[Gottfried Leibniz]], [[Christiaan Huygens]], and [[Johann Bernoulli]] derived the [[equation]] in response to a challenge by [[Jakob Bernoulli]];<ref name=\"Lockwood124\"/> their solutions were published in the ''[[Acta Eruditorum]]'' for June 1691.<ref>{{citation|first=C.|last=Truesdell|title=The Rotational Mechanics of Flexible Or Elastic Bodies 1638–1788: Introduction to Leonhardi Euleri Opera Omnia Vol. X et XI Seriei Secundae|location=Zürich|\nurl=https://books.google.co.uk/books?id=gxrzm6y10EwC&pg=PA66#v=onepage&q&f=false|page=66|publisher=Orell Füssli|date=1960|isbn=9783764314415}}</ref><ref name=\"calladine\" >{{citation|first=C. R.|last=Calladine|title=An amateur's contribution to the design of Telford's Menai Suspension Bridge: a commentary on Gilbert (1826) 'On the mathematical theory of suspension bridges'|journal=Philosophical Transactions of the Royal Society A|date=2015-04-13|volume=373|issue=2039|page=20140346|doi=10.1098/rsta.2014.0346|pmid=25750153|pmc=4360092}}</ref> [[David Gregory (mathematician)|David Gregory]] wrote a treatise on the catenary in 1697<ref name=\"Lockwood124\"/><ref>{{citation|first=Davidis|last=Gregorii|title=Catenaria|journal=Philosophical Transactions|volume=19|issue=231|date=August 1697|pages=637–652|doi=10.1098/rstl.1695.0114}}</ref> in which he provided an incorrect derivation of the correct differential equation.<ref name=\"calladine\" />\n\n[[Euler]] proved in 1744 that the catenary is the curve which, when rotated about the {{mvar|x}}-axis, gives the surface of minimum [[surface area]] (the [[catenoid]]) for the given bounding circles.<ref name=\"MathWorld\"/> [[Nicolas Fuss]] gave equations describing the equilibrium of a chain under any [[force]] in 1796.<ref>[[#Routh|Routh]] Art. 455, footnote</ref>\n\n==Inverted catenary arch==<!-- This section is linked from [[Park Güell]] -->\n\n[[Catenary arches]] are often used in the construction of [[kiln]]s. To create the desired curve, the shape of a hanging chain of the desired dimensions is transferred to a form which is then used as a guide for the placement of bricks or other building material.<ref>{{cite book| last1 = Minogue| first1 = Coll| last2 = Sanderson| first2 = Robert| title = Wood-fired Ceramics: Contemporary Practices| year = 2000| publisher = University of Pennsylvania| isbn = 978-0-8122-3514-2| page = 42 }}</ref><ref>\n{{cite book| last1 = Peterson| first1 = Susan| last2 = Peterson| first2 = Jan| title = The Craft and Art of Clay: A Complete Potter's Handbook| url = https://books.google.com/?id=PAZR-A9Ra6EC&pg=PA208| year = 2003| publisher = Laurence King| isbn = 978-1-85669-354-7| page = 224 }}</ref>\n\nThe [[Gateway Arch]] in [[St. Louis, Missouri]], United States is sometimes said to be an (inverted) catenary, but this is incorrect.<ref>{{Citation | last1=Osserman | first1=Robert | title=Mathematics of the Gateway Arch | url=http://www.ams.org/notices/201002/index.html | year=2010 | journal=[[Notices of the American Mathematical Society]] | issn=0002-9920 | volume=57 | issue=2 | pages=220–229}}</ref> It is close to a more general curve called a flattened catenary, with equation {{math|1=''y'' = ''A'' cosh(''Bx'')}}, which is a catenary if {{math|1=''AB'' = 1}}. While a catenary is the ideal shape for a freestanding arch of constant thickness, the Gateway Arch is narrower near the top. According to the U.S. [[National Historic Landmark]] nomination for the arch, it is a \"[[weighted catenary]]\" instead. Its shape corresponds to the shape that a weighted chain, having lighter links in the middle, would form.<ref>{{cite journal| last = Hicks| first = Clifford B.| title = The Incredible Gateway Arch: America's Mightiest National Monument| url = https://books.google.com/?id=BuMDAAAAMBAJ&pg=PA89| volume = 120|date=December 1963| page = 89| issn = 0032-4558| issue = 6| journal = [[Popular Mechanics]] }}</ref><ref name=\"nrhpinv2\">{{citation|url={{NHLS url|id=87001423}}|title=National Register of Historic Places Inventory-Nomination: Jefferson National Expansion Memorial Gateway Arch / Gateway Arch; or \"The Arch\"|year=1985 |first=Laura Soullière |last=Harrison |publisher=National Park Service }} and {{NHLS url|id=87001423|title=''Accompanying one photo, aerial, from 1975''|photos=y}}&nbsp;{{small|(578&nbsp;KB)}}</ref>\n\n<gallery mode=\"packed\" heights=\"200px\"> \nFile:LaPedreraParabola.jpg|Catenary<ref>{{cite book| last = Sennott| first = Stephen| title = Encyclopedia of Twentieth Century Architecture| year = 2004| publisher = Taylor & Francis| isbn = 978-1-57958-433-7| page = 224 }}</ref> arches under the roof of [[Gaudí]]'s ''[[Casa Milà]]'', [[Barcelona]], Spain.\nFile:Sheffield Winter Garden.jpg|The [[Sheffield Winter Garden]] is enclosed by a series of [[catenary arches]].<ref>{{cite book| last = Hymers| first = Paul| title = Planning and Building a Conservatory| year = 2005| publisher = New Holland| isbn = 978-1-84330-910-9| page = 36 }}</ref>\nFile:Gateway Arch.jpg|The [[Gateway Arch]] (looking East) is a flattened catenary.\nFile:CatenaryKilnConstruction06025.JPG|Catenary arch kiln under construction over temporary form\nFile:Budapest_Keleti_teto 1.jpg|Cross-section of the roof of the [[Budapest Keleti railway station|Keleti Railway Station]] (Budapest, Hungary)\nFile:Budapest_Keleti_teto_2.svg|The cross-section of the roof of the Keleti Railway Station forms a catenary.\n</gallery>\n\n{{Clear}}\n\n==Catenary bridges==\n[[File:Soderskar-bridge.jpg|thumb|right|250px|[[Simple suspension bridge]]s are essentially thickened cables, and follow a catenary curve.]]\n[[File:Puentedelabarra(below).jpg|thumb|right|250px|[[Stressed ribbon bridge]]s, like this one in [[Maldonado, Uruguay]], also follow a catenary curve, with cables embedded in a rigid deck.]]\n\nIn free-hanging chains, the force exerted is uniform with respect to length of the chain, and so the chain follows the catenary curve.<ref>{{cite book| first= Owen |last1=Byer|first2=Felix |last2=Lazebnik|first3=Deirdre L. |last3=Smeltzer| title = Methods for Euclidean Geometry| url = https://books.google.com/?id=QkuVb672dWgC&pg=PA210| date = 2010-09-02| publisher = MAA| isbn = 978-0-88385-763-2| page = 210 }}</ref> The same is true of a [[simple suspension bridge]] or \"catenary bridge,\" where the roadway follows the cable.<ref>{{cite book| first= Leonardo|last= Fernández Troyano| title = Bridge Engineering: A Global Perspective| url = https://books.google.com/?id=0u5G8E3uPUAC&pg=PA514| year = 2003| publisher = Thomas Telford| isbn = 978-0-7277-3215-6| page = 514 }}</ref><ref>{{cite book| first1= W. |last1=Trinks|first2=M. H. |last2=Mawhinney|first3=R. A. |last3=Shannon|first4=R. J. |last4=Reed|first5=J. R. |last5=Garvey| title = Industrial Furnaces| url = https://books.google.com/?id=EqRTAAAAMAAJ&pg=PA132| date = 2003-12-05| publisher = Wiley| isbn = 978-0-471-38706-0| page = 132 }}</ref>\n\nA [[stressed ribbon bridge]] is a more sophisticated structure with the same catenary shape.<ref>{{cite book| first= John S. |last=Scott| title = Dictionary Of Civil Engineering| date = 1992-10-31| publisher = Springer| isbn = 978-0-412-98421-1| page = 433 }}</ref><ref>{{cite journal|journal=The Architects' Journal |volume=207 |date=1998 |page=51}}</ref>\n\nHowever, in a [[suspension bridge]] with a suspended roadway, the chains or cables support the weight of the bridge, and so do not hang freely. In most cases the roadway is flat, so when the weight of the cable is negligible compared with the weight being supported, the force exerted is uniform with respect to horizontal distance, and the result is a [[parabolic arch|parabola]], as discussed below (although the term \"catenary\" is often still used, in an informal sense). If the cable is heavy then the resulting curve is between a catenary and a parabola.<ref name=\"Lockwood122\">[[#Lockwood|Lockwood]] p. 122</ref><ref>\n{{cite web\n|title=Hanging With Galileo\n|date=June 30, 2006\n|first=Paul\n|last=Kunkel\n|publisher=Whistler Alley Mathematics\n|url=http://whistleralley.com/hanging/hanging.htm\n|accessdate=March 27, 2009\n}}</ref>\n[[File:Comparison catenary parabola.svg|thumb|none|400px|Comparison of a [[catenary arch]] (black dotted curve) and a [[parabolic arch]] (red solid curve) with the same span and sag. The catenary represents the profile of a simple suspension bridge, or the cable of a suspended-deck suspension bridge on which its deck and hangers have negligible mass compared to its cable. The parabola represents the profile of the cable of a suspended-deck suspension bridge on which its cable and hangers have negligible mass compared to its deck. The profile of the cable of a real suspension bridge with the same span and sag lies between the two curves. The catenary and parabola equations are respectively, {{math|1=''y'' = cosh(''x'')}} and {{math|1=''y'' =''x''<sup>2</sup>}} ]]\n\n==Anchoring of marine objects==\n[[File:Catenary.PNG|thumb|right|250px|A heavy [[anchor]] chain forms a catenary, with a low angle of pull on the anchor.]]\nThe catenary produced by gravity provides an advantage to heavy [[wikt:rode#Noun|anchor rodes]]. An anchor rode (or anchor line) usually consists of chain or cable or both. Anchor rodes are used by ships, oil rigs, docks, [[floating wind turbine]]s, and other marine equipment which must be anchored to the seabed.\n\nWhen the rode is slack, the catenary curve presents a lower angle of pull on the [[anchor]] or mooring device than would be the case if it were nearly straight. This enhances the performance of the anchor and raises the level of force it will resist before dragging. To maintain the catenary shape in the presence of wind, a heavy chain is needed, so that only larger ships in deeper water can rely on this effect. Smaller boats also rely on catenary to maintain maximum holding power.<ref>{{cite web|url=http://www.petersmith.net.nz/boat-anchors/catenary.php |title=Chain, Rope, and Catenary – Anchor Systems For Small Boats |publisher=Petersmith.net.nz |accessdate=2010-11-17}}</ref>\n\n==Mathematical description==\n\n===Equation===\n[[Image:catenary-pm.svg|thumb|350px|right|Catenaries for different values of {{mvar|a}}]]\n[[Image:Catenary-tension.png|350px|thumb|Three catenaries through the same two points, depending on the horizontal force {{mvar|T<sub>H</sub>}}, being {{math|1=''a'' = ''{{sfrac|T<sub>H</sub>|λH}}''}} and {{mvar|λ}} mass per unit length.]]\n\nThe equation of a catenary in [[Cartesian coordinate system|Cartesian coordinates]] has the form<ref name=\"Lockwood122\"/>\n\n:<math>y = a \\cosh \\left(\\frac{x}{a}\\right) = \\frac{a}{2}\\left(e^\\frac{x}{a} + e^{-\\frac{x}{a}}\\right)</math>\n\nwhere {{math|cosh}} is the [[hyperbolic function|hyperbolic cosine function]]. All catenary curves are [[Similarity (geometry)|similar]] to each other; changing the [[parameter]] {{mvar|a}} is equivalent to a uniform [[Scaling (geometry)|scaling]] of the curve.<ref>{{cite web|url=http://xahlee.org/SpecialPlaneCurves_dir/Catenary_dir/catenary.html |title=Catenary |publisher=Xahlee.org |date=2003-05-28 |accessdate=2010-11-17}}</ref>\n\nThe [[Whewell equation]] for the catenary is<ref name=\"Lockwood122\"/>\n\n:<math>\\tan \\varphi = \\frac{s}{a}\\,.</math>\n\nDifferentiating gives\n\n:<math>\\frac{d\\varphi}{ds} = \\frac{\\cos^2\\varphi}{a}</math>\n\nand eliminating {{mvar|φ}} gives the [[Cesàro equation]]<ref>[[#MathWorld|MathWorld]], eq. 7</ref>\n\n:<math>\\kappa=\\frac{a}{s^2+a^2}\\,.</math>\n\nThe [[Osculating circle|radius of curvature]] is then\n\n:<math>\\rho = a \\sec^2 \\varphi</math>\n\nwhich is the length of the [[Tangent#Normal line to a curve|line normal to the curve]] between it and the {{mvar|x}}-axis.<ref>[[#Routh|Routh]] Art. 444</ref>\n\n===Relation to other curves===\nWhen a [[parabola]] is rolled along a straight line, the [[roulette (curve)|roulette]] curve traced by its [[Conic section#Eccentricity, focus and directrix|focus]] is a catenary.<ref name=\"Yates 13\"/> The [[Envelope (mathematics)|envelope]] of the [[Conic section#Eccentricity, focus and directrix|directrix]] of the parabola is also a catenary.<ref>Yates p. 80</ref> The [[involute]] from the vertex, that is the roulette formed traced by a point starting at the vertex when a line is rolled on a catenary, is the [[tractrix]].<ref name=\"Yates 13\"/>\n\nAnother roulette, formed by rolling a line on a catenary, is another line. This implies that [[square wheel]]s can roll perfectly smoothly on a road made of a series of bumps in the shape of an inverted catenary curve. The wheels can be any [[regular polygon]] except a triangle, but the catenary must have parameters corresponding to the shape and dimensions of the wheels.<ref>{{cite journal \n|last1=Hall|first1=Leon|last2=Wagon|first2=Stan|author2-link=Stan Wagon|year=1992|title=Roads and Wheels\n|journal=Mathematics Magazine|volume=65|issue= 5|pages=283–301 |jstor=2691240|doi=10.2307/2691240}}\n</ref>\n\n===Geometrical properties===\nOver any horizontal interval, the ratio of the area under the catenary to its length equals {{mvar|a}}, independent of the interval selected. The catenary is the only plane curve other than a horizontal line with this property. Also, the geometric centroid of the area under a stretch of catenary is the midpoint of the perpendicular segment connecting the centroid of the curve itself and the {{mvar|x}}-axis.<ref>{{cite journal|last=Parker |first=Edward |date=2010 |title=A Property Characterizing the Catenary |journal=Mathematics Magazine |volume=83 |pages=63–64|doi=10.4169/002557010X485120 }}</ref>\n\n===Science===\nA moving [[electric charge|charge]] in a uniform [[electric field]] travels along a catenary (which tends to a [[parabola]] if the charge velocity is much less than the [[speed of light]] {{mvar|c}}).<ref>{{cite book| last = Landau| first = Lev Davidovich| url=https://books.google.com/books?id=X18PF4oKyrUC&pg=PA56|title =  The Classical Theory of Fields| year = 1975| publisher = Butterworth-Heinemann| isbn = 978-0-7506-2768-9| page = 56 }}</ref>\n\nThe [[surface of revolution]] with fixed radii at either end that has minimum surface area is a catenary revolved about the {{mvar|x}}-axis.<ref name=\"Yates 13\">{{cite book |title=Curves and their Properties\n|first=Robert C.|last=Yates|publisher=NCTM|year=1952|pages=13}}</ref>\n\n==Analysis==\n\n===Model of chains and arches===\nIn the [[mathematical model]] the chain (or cord, cable, rope, string, etc.) is idealized by assuming that it is so thin that it can be regarded as a [[curve]] and that it is so flexible any force of [[Tension (physics)|tension]] exerted by the chain is parallel to the chain.<ref>[[#Routh|Routh]] Art. 442, p. 316</ref> The analysis of the curve for an optimal arch is similar except that the forces of tension become forces of [[Compression (physics)|compression]] and everything is inverted.<ref>{{cite book| last = Church| first = Irving Porter| title = Mechanics of Engineering| url = https://books.google.com/?id=-iAPAAAAYAAJ&pg=PA387| year = 1890| publisher = Wiley| page = 387 }}</ref>\nAn underlying principle is that the chain may be considered a rigid body once it has attained equilibrium.<ref>[[#Whewell|Whewell]] p. 65</ref> Equations which define the shape of the curve and the tension of the chain at each point may be derived by a careful inspection of the various forces acting on a segment using the fact that these forces must be in balance if the chain is in [[static equilibrium]].\n\nLet the path followed by the chain be given [[parametric equations|parametrically]] by {{math|1='''r''' = (''x'', ''y'') = (''x''(''s''), ''y''(''s''))}} where {{mvar|s}} represents [[arc length]] and {{math|'''r'''}} is the [[position vector]]. This is the [[Unit speed parametrization|natural parameterization]] and has the property that\n\n:<math>\\frac{d\\mathbf{r}}{ds}=\\mathbf{u}</math>\n\nwhere {{math|'''u'''}} is a [[unit tangent vector]].\n\n[[File:CatenaryForceDiagram.svg|thumb|Diagram of forces acting on a segment of a catenary from {{math|'''c'''}} to {{math|'''r'''}}. The forces are the tension {{math|'''T'''<sub>0</sub>}} at {{math|'''c'''}}, the tension {{math|'''T'''}} at {{math|'''r'''}}, and the weight of the chain {{math|(0, −''λgs'')}}. Since the chain is at rest the sum of these forces must be zero.]]\nA [[differential equation]] for the curve may be derived as follows.<ref>Following [[#Routh|Routh]] Art. 443 p. 316</ref> Let {{math|'''c'''}} be the lowest point on the chain, called the vertex of the catenary.<ref>[[#Routh|Routh]] Art. 443 p. 317</ref> The slope  {{math|1=''dy/dx''}} of the curve is zero at C since it is a minimum point. Assume {{math|'''r'''}} is to the right of {{math|'''c'''}} since the other case is implied by symmetry. The forces acting on the section of the chain from '''c''' to '''r''' are the tension of the chain at {{math|'''c'''}}, the tension of the chain at '''r''', and the weight of the chain. The tension at {{math|'''c'''}} is tangent to the curve at {{math|'''c'''}} and is therefore horizontal without any vertical component and it pulls the section to the left so it may be written {{math|(−''T''<sub>0</sub>, 0)}} where {{math|''T''<sub>0</sub>}} is the magnitude of the force. The tension at {{math|'''r'''}} is parallel to the curve at {{math|'''r'''}} and pulls the section to the right. The tension at {{math|'''r'''}} can be split into two components so it may be written {{math|1=''T'''''u''' = (''T'' cos ''φ'', ''T'' sin ''φ'')}}, where {{mvar|T}} is the magnitude of the force and {{mvar|φ}} is the angle between the curve at {{math|'''r'''}} and the {{mvar|x}}-axis (see [[tangential angle]]). Finally, the weight of the chain is represented by {{math|(0, −''λgs'')}} where {{mvar|λ}} is the mass per unit length, {{mvar|g}} is the acceleration of gravity and {{mvar|s}} is the length of the segment of chain between {{math|'''c'''}} and {{math|'''r'''}}.\n\nThe chain is in equilibrium so the sum of three forces is {{math|'''0'''}}, therefore\n\n:<math>T \\cos \\varphi = T_0</math>\nand\n:<math>T \\sin \\varphi = \\lambda gs\\,,</math>\n\nand dividing these gives\n\n:<math>\\frac{dy}{dx}=\\tan \\varphi = \\frac{\\lambda gs}{T_0}\\,.</math>\n\nIt is convenient to write\n\n:<math>a = \\frac{T_0}{\\lambda g}</math>\n\nwhich is the length of chain whose weight on Earth is equal in magnitude to the tension at {{math|'''c'''}}.<ref>[[#Whewell|Whewell]] p. 67</ref>  Then\n\n:<math>\\frac{dy}{dx}=\\frac{s}{a}</math>\n\nis an equation defining the curve.\n\nThe horizontal component of the tension, {{math|1=''T'' cos ''φ'' = ''T''<sub>0</sub>}} is constant and the vertical component of the tension, {{math|1=''T'' sin ''φ'' = ''λgs''}} is proportional to the length of chain between the {{math|'''r'''}} and the vertex.<ref name=\"Routh Art 443 318\"/>\n\n===Derivation of equations for the curve===\nThe differential equation given above can be solved to produce equations for the curve.<ref>Following [[#Routh|Routh]] Art. 443 p/ 317</ref>\n\nFrom\n\n:<math>\\frac{dy}{dx} = \\frac{s}{a}\\,,</math>\n\nthe formula for [[Arc length#Finding arc lengths by integrating|arc length]] gives\n:<math>\\frac{ds}{dx} = \\sqrt{1+\\left(\\dfrac{dy}{dx}\\right)^2} = \\frac{\\sqrt{a^2+s^2}}{a}\\,.</math>\n\nThen\n\n:<math>\\frac{dx}{ds} = \\frac{1}{\\frac{ds}{dx}} = \\frac{a}{\\sqrt{a^2+s^2}}</math>\n\nand\n\n:<math>\\frac{dy}{ds} = \\frac{\\frac{dy}{dx}}{\\frac{ds}{dx}} = \\frac{s}{\\sqrt{a^2+s^2}}\\,.</math>\n\nThe second of these equations can be integrated to give\n\n:<math>y = \\sqrt{a^2+s^2} + \\beta</math>\n\nand by shifting the position of the {{mvar|x}}-axis, {{mvar|β}} can be taken to be 0. Then\n\n:<math>y = \\sqrt{a^2+s^2}\\,,\\quad y^2=a^2+s^2\\,.</math>\n\nThe {{mvar|x}}-axis thus chosen is called the ''directrix'' of the catenary.\n\nIt follows that the magnitude of the tension at a point {{math|(''x'', ''y'')}} is {{math|1=''T'' = ''λgy''}}, which is proportional to the distance between the point and the directrix.<ref name=\"Routh Art 443 318\">[[#Routh|Routh]] Art. 443 p. 318</ref>\n\nThe integral of the expression for {{mvar|{{sfrac|dx|ds}}}} can be found using [[List of integrals of irrational functions|standard techniques]], giving<ref>Use of hyperbolic functions follows Maurer p. 107</ref>\n\n:<math>x = a\\operatorname{arsinh}\\left(\\frac{s}{a}\\right) + \\alpha\\,.</math>\n\nand, again, by shifting the position of the {{mvar|y}}-axis, {{mvar|α}} can be taken to be 0. Then\n\n:<math>x = a\\operatorname{arsinh}\\left(\\frac{s}{a}\\right)\\,,\\quad s=a \\sinh\\left(\\frac{x}{a}\\right)\\,.</math>\n\nThe {{mvar|y}}-axis thus chosen passes through the vertex and is called the axis of the catenary.\n\nThese results can be used to eliminate {{mvar|s}} giving\n\n:<math>y = a \\cosh\\left(\\frac{x}{a}\\right)\\,.</math>\n\n===Alternative derivation===\nThe differential equation can be solved using a different approach.<ref>Following Lamb p. 342</ref> From\n\n:<math>s = a \\tan \\varphi</math>\n\nit follows that\n\n:<math>\\frac{dx}{d\\varphi} = \\frac{dx}{ds}\\frac{ds}{d\\varphi}=\\cos \\varphi \\cdot a \\sec^2 \\varphi= a \\sec \\varphi</math>\nand\n:<math>\\frac{dy}{d\\varphi} = \\frac{dy}{ds}\\frac{ds}{d\\varphi}=\\sin \\varphi \\cdot a \\sec^2 \\varphi= a \\tan \\varphi \\sec \\varphi\\,.</math>\n\nIntegrating gives,\n\n:<math>x = a \\ln(\\sec \\varphi + \\tan \\varphi) + \\alpha</math>\nand\n:<math>y = a \\sec \\varphi + \\beta\\,.</math>\n\nAs before, the {{mvar|x}} and {{mvar|y}}-axes can be shifted so {{mvar|α}} and {{mvar|β}} can be taken to be 0. Then\n\n:<math>\\sec \\varphi + \\tan \\varphi = e^\\frac{x}{a}\\,,</math>\nand taking the reciprocal of both sides\n:<math>\\sec \\varphi - \\tan \\varphi = e^{-\\frac{x}{a}}\\,.</math>\n\nAdding and subtracting the last two equations then gives the solution\n:<math>y = a \\sec \\varphi = a \\cosh\\left(\\frac{x}{a}\\right)\\,,</math>\nand\n:<math>s = a \\tan \\varphi = a \\sinh\\left(\\frac{x}{a}\\right)\\,.</math>\n\n===Determining parameters===\nIn general the parameter {{mvar|a}} is the position of the axis. The equation can be determined in this case as follows:<ref>Following Todhunter Art. 186</ref>\nRelabel if necessary so that {{math|''P''<sub>1</sub>}} is to the left of {{math|''P''<sub>2</sub>}} and let {{mvar|h}} be the horizontal and {{mvar|v}} be the vertical distance from {{math|''P''<sub>1</sub>}} to {{math|''P''<sub>2</sub>}}. [[Translation (geometry)|Translate]] the axes so that the vertex of the catenary lies on the {{mvar|y}}-axis and its height {{mvar|a}} is adjusted so the catenary satisfies the standard equation of the curve\n \n:<math>y = a \\cosh\\left(\\frac{x}{a}\\right)</math>\n\nand let the coordinates of {{math|''P''<sub>1</sub>}} and {{math|''P''<sub>2</sub>}} be {{math|(''x''<sub>1</sub>, ''y''<sub>1</sub>)}} and {{math|(''x''<sub>2</sub>, ''y''<sub>2</sub>)}} respectively. The curve passes through these points, so the difference of height is\n\n:<math>v = a \\cosh\\left(\\frac{x_2}{a}\\right) - a \\cosh\\left(\\frac{x_1}{a}\\right)\\,.</math>\n\nand the length of the curve from {{math|''P''<sub>1</sub>}} to {{math|''P''<sub>2</sub>}} is\n\n:<math>s = a \\sinh\\left(\\frac{x_2}{a}\\right) - a \\sinh\\left(\\frac{x_1}{a}\\right)\\,.</math>\n\nWhen {{math|''s''<sup>2</sup> − ''v''<sup>2</sup>}} is expanded using these expressions the result is\n\n:<math>s^2-v^2=2a^2\\left(\\cosh\\left(\\frac{x_2-x_1}{a}\\right)-1\\right)=4a^2\\sinh^2\\left(\\frac{h}{2a}\\right)\\,,</math>\nso\n:<math>\\sqrt{s^2-v^2}=2a\\sinh\\left(\\frac{h}{2a}\\right)\\,.</math>\n\nThis is a transcendental equation in {{mvar|a}} and must be solved [[Numerical analysis|numerically]]. It can be shown with the methods of calculus<ref>See [[#Routh|Routh]] art. 447</ref> that there is at most one solution with {{math|''a'' > 0}} and so there is at most one position of equilibrium.\n\n==Generalizations with vertical force==\n\n===Nonuniform chains===\nIf the density of the chain is variable then the analysis above can be adapted to produce equations for the curve given the density, or given the curve to find the density.<ref>Following [[#Routh|Routh]] Art. 450</ref>\n\nLet {{mvar|w}} denote the weight per unit length of the chain, then the weight of the chain has magnitude\n\n:<math>\\int_\\mathbf{c}^\\mathbf{r} w\\, ds\\,,</math>\n\nwhere the limits of integration are {{math|'''c'''}} and {{math|'''r'''}}. Balancing forces as in the uniform chain produces\n\n:<math>T \\cos \\varphi = T_0</math>\nand\n:<math>T \\sin \\varphi = \\int_\\mathbf{c}^\\mathbf{r} w\\, ds\\,,</math>\nand therefore\n:<math>\\frac{dy}{dx}=\\tan \\varphi = \\frac{1}{T_0} \\int_\\mathbf{c}^\\mathbf{r} w\\, ds\\,.</math>\n\nDifferentiation then gives\n\n:<math>w=T_0 \\frac{d}{ds}\\frac{dy}{dx} = \\frac{T_0 \\dfrac{d^2y}{dx^2}}{\\sqrt{1+\\left(\\dfrac{dy}{dx}\\right)^2}}\\,.</math>\n\nIn terms of {{mvar|φ}} and the radius of curvature {{mvar|ρ}} this becomes\n\n:<math>w= \\frac{T_0}{\\rho \\cos^2 \\varphi}\\,.</math>\n\n===Suspension bridge curve===\n[[File:Golden Gate Bridge, SF.jpg|thumb|right|480px|[[Golden Gate Bridge]]. Most [[suspension bridge]] cables follow a parabolic, not a catenary curve, due to the weight of the roadway being much greater than that of the cable.]]\n\nA similar analysis can be done to find the curve followed by the cable supporting a [[suspension bridge]] with a horizontal roadway.<ref>Following [[#Routh|Routh]] Art. 452</ref> If the weight of the roadway per unit length is {{mvar|w}} and the weight of the cable and the wire supporting the bridge is negligible in comparison, then the weight on the cable from {{math|'''c'''}} to {{math|'''r'''}} is {{mvar|wx}} where {{mvar|x}} is the horizontal distance between {{math|'''c'''}} and {{math|'''r'''}}. Proceeding as before gives the differential equation\n\n:<math>\\frac{dy}{dx}=\\tan \\varphi = \\frac{w}{T_0}x\\,. </math>\n\nThis is solved by simple integration to get\n\n:<math>y=\\frac{w}{2T_0}x^2 + \\beta</math>\n\nand so the cable follows a parabola. If the weight of the cable and supporting wires is not negligible then the analysis is more complex.<ref>Ira Freeman investigated the case where only the cable and roadway are significant, see the External links section. [[#Routh|Routh]] gives the case where only the supporting wires have significant weight as an exercise.</ref>\n\n===Catenary of equal strength===\nIn a catenary of equal strength, the cable is strengthened according to the magnitude of the tension at each point, so its resistance to breaking is constant along its length. Assuming that the strength of the cable is proportional to its density per unit length, the weight, {{mvar|w}}, per unit length of the chain can be written {{mvar|{{sfrac|T|c}}}}, where {{mvar|c}} is constant, and the analysis for nonuniform chains can be applied.<ref>Following [[#Routh|Routh]] Art. 453</ref>\n\nIn this case the equations for tension are\n\n:<math>T \\cos \\varphi = T_0\\,,</math>\n:<math>T \\sin \\varphi = \\frac{1}{c}\\int T\\, ds\\,.</math>\n\nCombining gives\n\n:<math>c \\tan \\varphi = \\int \\sec \\varphi\\, ds</math>\n\nand by differentiation\n\n:<math>c = \\rho \\cos \\varphi</math>\n\nwhere {{mvar|ρ}} is the radius of curvature.\n\nThe solution to this is\n\n:<math>y = c \\ln\\left(\\sec\\left(\\frac{x}{c}\\right)\\right)\\,.</math>\n\nIn this case, the curve has vertical asymptotes and this limits the span to {{math|π''c''}}. Other relations are\n\n:<math>x = c\\varphi\\,,\\quad s = \\ln\\left(\\tan\\left(\\frac{\\pi+2\\varphi}{4}\\right)\\right)\\,.</math>\n\nThe curve was studied 1826 by [[Davies Gilbert]] and, apparently independently, by [[Gaspard-Gustave Coriolis]] in 1836.\n\nRecently, it was shown that this type of catenary could act as a building block of [[electromagnetic metasurface]] and was known as \"catenary of equal phase gradient\".<ref>{{cite journal \n|last1=Pu|first1=Mingbo|last2=Li|first2=Xiong|last3=Ma|first3=Xiaoliang|last4=Luo|first4=Xiangang|year=2015|title=Catenary Optics for Achromatic Generation of Perfect Optical Angular Momentum\n|journal=Science Advances|volume=1|issue= 9|pages=e1500396 |url=http://advances.sciencemag.org/content/1/9/e1500396.abstract|doi=10.1126/sciadv.1500396|pmid=26601283|pmc=4646797}}\n</ref>\n\n===Elastic catenary===\nIn an [[Elasticity (physics)|elastic]] catenary, the chain is replaced by a [[Spring (device)|spring]] which can stretch in response to tension. The spring is assumed to stretch in accordance with [[Hooke's Law]]. Specifically, if {{math|p}} is the natural length of a section of spring, then the length of the spring with tension {{math|T}} applied has length\n\n:<math>s=\\left(1+\\frac{T}{E}\\right)p\\,,</math>\n\nwhere {{math|E}} is a constant equal to {{math|kp}}, where {{math|k}} is the [[stiffness]] of the spring.<ref>[[#Routh|Routh]] Art. 489</ref> In the catenary the value of {{math|T}} is variable, but ratio remains valid at a local level, so<ref>[[#Routh|Routh]] Art. 494</ref>\n:<math>\\frac{ds}{dp}=1+\\frac{T}{E}\\,.</math>\nThe curve followed by an elastic spring can now be derived following a similar method as for the inelastic spring.<ref>Following [[#Routh|Routh]] Art. 500</ref>\n\nThe equations for tension of the spring are\n\n:<math>T \\cos \\varphi = T_0\\,,</math>\nand\n:<math>T \\sin \\varphi = \\lambda_0 gp\\,,</math>\n\nfrom which\n\n:<math>\\frac{dy}{dx}=\\tan \\varphi = \\frac{\\lambda_0 gp}{T_0}\\,,\\quad T=\\sqrt{T_0^2+\\lambda_0^2 g^2p^2}\\,,</math>\n\nwhere {{mvar|p}} is the natural length of the segment from {{math|'''c'''}} to {{math|'''r'''}} and {{math|''λ''<sub>0</sub>}} is the mass per unit length of the spring with no tension and {{mvar|g}} is the acceleration of gravity. Write\n\n:<math>a = \\frac{T_0}{\\lambda_0 g}</math>\n\nso\n\n:<math>\\frac{dy}{dx}=\\tan \\varphi = \\frac{p}{a}\\,,\\quad T=\\frac{T_0}{a}\\sqrt{a^2+p^2}\\,.</math>\n\nThen \n:<math>\\frac{dx}{ds} = \\cos \\varphi = \\frac{T_0}{T}</math>\nand \n:<math>\\frac{dy}{ds} = \\sin \\varphi = \\frac{\\lambda_0 gp}{T}\\,,</math>\nfrom which\n:<math>\\frac{dx}{dp} = \\frac{T_0}{T}\\frac{ds}{dp} = T_0\\left(\\frac{1}{T}+\\frac{1}{E}\\right)=\\frac{a}{\\sqrt{a^2+p^2}}+\\frac{T_0}{E}</math>\nand \n:<math>\\frac{dy}{dp} = \\frac{\\lambda_0 gp}{T}\\frac{ds}{dp} = \\frac{T_0p}{a}\\left(\\frac{1}{T}+\\frac{1}{E}\\right)=\\frac{p}{\\sqrt{a^2+p^2}}+\\frac{T_0p}{Ea}\\,.</math>\n\nIntegrating gives the parametric equations\n\n:<math>x=a\\operatorname{arcsinh}\\left(\\frac{p}{a}\\right)+\\frac{T_0}{E}p + \\alpha\\,,</math>\n:<math>y=\\sqrt{a^2+p^2}+\\frac{T_0}{2Ea}p^2+\\beta\\,.</math>\n\nAgain, the {{mvar|x}} and {{mvar|y}}-axes can be shifted so {{mvar|α}} and {{mvar|β}} can be taken to be 0. So\n\n:<math>x=a\\operatorname{arcsinh}\\left(\\frac{p}{a}\\right)+\\frac{T_0}{E}p\\,,</math>\n:<math>y=\\sqrt{a^2+p^2}+\\frac{T_0}{2Ea}p^2</math>\n\nare parametric equations for the curve. At the rigid [[Limit (mathematics)|limit]] where {{mvar|E}} is large, the shape of the curve reduces to that of a non-elastic chain.\n\n==Other generalizations==\n\n===Chain under a general force===\nWith no assumptions have been made regarding the force {{math|'''G'''}} acting on the chain, the following analysis can be made.<ref>Follows [[#Routh|Routh]] Art. 455</ref>\n\nFirst, let {{math|1='''T''' = '''T'''(''s'')}} be the force of tension as a function of {{mvar|s}}. The chain is flexible so it can only exert a force parallel to itself. Since tension is defined as the force that the chain exerts on itself, {{math|'''T'''}} must be parallel to the chain. In other words,\n\n:<math>\\mathbf{T} = T \\mathbf{u}\\,,</math>\n\nwhere {{mvar|T}} is the magnitude of {{math|'''T'''}} and {{math|'''u'''}} is the unit tangent vector.\n\nSecond, let {{math|1='''G''' = '''G'''(''s'')}} be the external force per unit length acting on a small segment of a chain as a function of {{mvar|s}}. The forces acting on the segment of the chain between {{mvar|s}} and {{math|''s'' + Δ''s''}} are the force of tension {{math|'''T'''(''s'' + Δ''s'')}} at one end of the segment, the nearly opposite force {{math|−'''T'''(''s'')}} at the other end, and the external force acting on the segment which is approximately {{math|'''G'''Δ''s''}}. These forces must balance so\n\n:<math>\\mathbf{T}(s+\\Delta s)-\\mathbf{T}(s)+\\mathbf{G}\\Delta s \\approx \\mathbf{0}\\,.</math>\n\nDivide by {{math|Δ''s''}} and take the limit as {{math|Δ''s'' → 0}} to obtain\n\n:<math>\\frac{d\\mathbf{T}}{ds} + \\mathbf{G} = \\mathbf{0}\\,.</math>\n\nThese equations can be used as the starting point in the analysis of a flexible chain acting under any external force. In the case of the standard catenary, {{math|1='''G''' = (0, −''λg'')}} where the chain has mass {{mvar|λ}} per unit length and {{mvar|g}} is the acceleration of gravity.\n\n==See also==\n* [[Catenary arch]]\n* [[Chain fountain]] or self-siphoning beads\n* [[Overhead line]] – power lines suspended over rail or tram vehicles\n* [[Roulette (curve)]] – an elliptic/hyperbolic catenary\n* [[Troposkein]] – the shape of a spun rope\n* [[Weighted catenary]]\n\n==Notes==\n{{Reflist}}\n\n==Bibliography==\n* {{anchor|Lockwood}}{{cite book |title=A Book of Curves|first=E.H.|last=Lockwood|publisher=Cambridge|year=1961\n|chapter=Chapter 13: The Tractrix and Catenary|chapter-url=https://archive.org/details/bookofcurves006299mbp}}\n* {{cite book|last=Salmon|first=George | title=Higher Plane Curves\n|publisher=Hodges, Foster and Figgis|year=1879|pages=287–289\n}}\n* {{anchor|Routh}}{{cite book| last = Routh| first = Edward John| authorlink = Edward Routh| title = A Treatise on Analytical Statics| chapter-url = https://books.google.com/?id=3N5JAAAAMAAJ&pg=PA315| year = 1891| publisher = University Press| chapter = Chapter X: On Strings }}\n* {{cite book| last = Maurer| first = Edward Rose| title = Technical Mechanics| chapter-url = https://books.google.com/?id=L98uAQAAIAAJ&pg=PA107| year = 1914| publisher = J. Wiley & Sons| chapter = Art. 26 Catenary Cable }}\n* {{cite book| last = Lamb| first = Sir Horace| title = An Elementary Course of Infinitesimal Calculus| chapter-url = https://books.google.com/?id=eDM6AAAAMAAJ&pg=PA342| year = 1897| publisher = University Press| chapter = Art. 134 Transcendental Curves; Catenary, Tractrix }}\n* {{cite book| last = Todhunter| first = Isaac| authorlink = Isaac Todhunter| title = A Treatise on Analytical Statics| chapter-url = https://books.google.com/?id=-iEuAAAAYAAJ&pg=PA199| year = 1858| publisher = Macmillan| chapter = XI Flexible Strings. Inextensible, XII Flexible Strings. Extensible }}\n* {{anchor|Whewell}}{{cite book| last = Whewell| first = William| authorlink = William Whewell| title = Analytical Statics| chapter-url = https://books.google.com/?id=BF8JAAAAIAAJ&pg=PA65| year = 1833| publisher = J. & J.J. Deighton| page = 65| chapter = Chapter V: The Equilibrium of a Flexible Body }}\n* {{anchor|MathWorld}}{{mathworld|Catenary|Catenary}}\n\n==Further reading==\n* {{cite book| last = Swetz| first = Frank| title = Learn from the Masters| url = https://books.google.com/?id=gqGLoh-WYrEC&pg=PA128| year = 1995| publisher = MAA| isbn = 978-0-88385-703-8| pages = 128–9 }}\n* {{cite book| last = Venturoli| first = Giuseppe| others = Trans. Daniel Cresswell| title = Elements of the Theory of Mechanics| chapter-url = https://books.google.com/?id=kHhBAAAAYAAJ&pg=PA67| year = 1822| publisher = J. Nicholson & Son| chapter = Chapter XXIII: On the Catenary }}\n\n==External links==\n{{Commons category}}\n{{wikiquote}}\n{{Wikisource1911Enc|Catenary}}\n* {{MacTutor|class=Curves|id=Catenary|title=Catenary}}\n* {{PlanetMath|urlname=Catenary|title=Catenary}}\n* [http://www.fxsolver.com/browse/formulas/Catenary+curve Catenary curve calculator]\n* [http://www.geom.uiuc.edu/zoo/diffgeom/surfspace/catenoid/catenary.html Catenary] at [[The Geometry Center]]\n* [http://xahlee.org/SpecialPlaneCurves_dir/Catenary_dir/catenary.html \"Catenary\" at Visual Dictionary of Special Plane Curves]\n* [http://www.maththoughts.com/blog/2013/catenary The Catenary - Chains, Arches, and Soap Films.]\n* [http://www.spaceagecontrol.com/calccabl.htm Cable Sag Error Calculator] – Calculates the deviation from a straight line of a catenary curve and provides derivation of the calculator and references.\n* [http://www.subhrajit.net/files/Projects-Work/OilBoom_Catenary_2010/catenary.pdf Dynamic as well as static cetenary curve equations derived] – The equations governing the shape (static case) as well as dynamics (dynamic case) of a centenary is derived. Solution to the equations discussed.\n* [https://arxiv.org/pdf/1401.2660.pdf The straight line, the catenary, the brachistochrone, the circle, and Fermat] Unified approach to some geodesics.\n* [http://www.ams.org/journals/bull/1925-31-08/S0002-9904-1925-04083-5/S0002-9904-1925-04083-5.pdf Ira Freeman \"A General Form of the Suspension Bridge Catenary\" ''Bulletin of the AMS'']\n\n[[Category:Curves]]\n[[Category:Differential equations]]\n[[Category:Exponentials]]\n[[Category:Analytic geometry]]"
    },
    {
      "title": "Characterizations of the exponential function",
      "url": "https://en.wikipedia.org/wiki/Characterizations_of_the_exponential_function",
      "text": "In [[mathematics]], the [[exponential function]] can be [[characterization (mathematics)|characterized]] in many ways. The following characterizations (definitions) are most common. This article discusses why each characterization makes sense, and why the characterizations are independent of and equivalent to each other. As a special case of these considerations, we will see that the three most common definitions given for the [[E (mathematical constant)|mathematical constant ''e'']] are also equivalent to each other.\n\n== Characterizations ==\n\nThe six most common definitions of the exponential function exp(''x'') = ''e''<sup>''x''</sup> for real ''x'' are:\n\n:1. Define ''e''<sup>''x''</sup> by the [[limit (mathematics)|limit]]\n\n::: <math>e^x = \\lim_{n\\to\\infty} \\left(1+\\frac x n \\right)^n.</math>\n\n:2. Define ''e''<sup>''x''</sup> as the value of the [[infinite series]]\n\n::: <math>e^x = \\sum_{n=0}^\\infty {x^n \\over n!} = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\frac{x^4}{4!} + \\cdots</math>\n\n::(Here ''n''! denotes the [[factorial]] of ''n''. One [[proof that e is irrational|proof that ''e'' is irrational]] uses this representation.)\n\n:3. Define ''e''<sup>''x''</sup> to be the unique number ''y'' > 0 such that\n\n::: <math>\\int_1^y \\frac{dt}{t} = x.</math>\n::This is as the inverse of the [[natural logarithm]] function, which is defined by this integral.\n\n:4. Define ''e''<sup>''x''</sup> to be the unique solution to the [[initial value problem]]\n\n:::<math>y'=y,\\quad y(0)=1.</math>\n\n::(Here, ''y''′ denotes the [[derivative]] of ''y''.)\n\n:5. The exponential function ''f''(''x'') = ''e''<sup>''x''</sup> is the '''unique Lebesgue-[[measurable function]]''' with ''f''(1) = ''e'' that satisfies\n:::<math>f(x+y) = f(x) f(y) \\text{ for all } x \\text{ and } y </math>\n::(Hewitt and Stromberg, 1965, exercise 18.46).  Alternatively, it is the '''unique anywhere-[[continuous function]]''' with these properties (Rudin, 1976, chapter 8, exercise 6).  The term \"anywhere-continuous\" means that there exists at least a single point <math>x</math> at which <math>f(x)</math> is continuous. As shown below, if <math>f(x+y) = f(x) f(y)</math> for all <math>x</math> and <math>y</math> and <math>f(x)</math> is continuous at ''any'' single point <math>x</math> then <math>f(x)</math> is necessarily continuous ''everywhere''.\n::(As a counterexample, if one does ''not'' assume continuity or measurability, it is possible to prove the existence of an everywhere-discontinuous, non-measurable function with this property by using a [[Hamel basis]] for the real numbers over the rationals, as described in Hewitt and Stromberg.)\n::Because ''f''(''x'') = ''e''<sup>''x''</sup> is guaranteed for rational ''x'' by the above properties (see below), one could also use [[monotonicity]] or other properties to enforce the choice of ''e''<sup>''x''</sup> for irrational ''x'',{{Citation needed|date=April 2010}} but such alternatives appear to be uncommon.\n\n::One could also replace the conditions that <math>f(1)=e</math> and that <math>f</math> be Lebesgue-measurable or anywhere-continuous with the single condition that <math>f'(0)=1</math>. This condition, along with the condition <math>f(x+y) = f(x) f(y)</math> easily implies both conditions in characterization 4. Indeed, one gets the initial condition <math>f(0) = 1</math> by dividing both sides of the equation\n:::<math>f(0) = f(0+0) = f(0) f(0)</math>\n::by <math>f(0)</math>, and the condition that <math>f'(x)=f(x)</math> follows from the condition that <math>f'(0)=1</math> and the definition of the derivative as follows:\n\n:::<math>\n\\begin{align}\nf'(x) & = \\lim_{h\\to 0}\\frac{f(x+h)-f(x)} h\n\\\\ & = \\lim_{h\\to 0}\\frac{f(x)f(h)-f(x)} h\n\\\\ & = \\lim_{h\\to 0}f(x)\\frac{f(h)-1} h\n\\\\ & = f(x)\\lim_{h\\to 0}\\frac{f(h)-1} h\n\\\\ & = f(x)\\lim_{h\\to 0}\\frac{f(0+h)-f(0)} h\n\\\\ & = f(x)f'(0) = f(x).\n\\end{align}\n</math>\n\n:6. Let ''e'' be the unique real number satisfying\n\n::: <math>\\lim_{h\\to 0} \\frac{e^h-1} h = 1.</math>\n\n:: This limit can be shown to exist. This definition is particularly suited to computing the derivative of the exponential function. Then define ''e''<sup>''x''</sup> to be the exponential function with this base.\n<!--\nAnother proof that the exponential function is identical to its derivative follows from the definition of the exponential function by differentiating the power series term by term:\n\n: <math>e^x = \\sum_{n=0}^\\infty {x^n \\over n!} = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\frac{x^4}{4!} + \\cdots</math>\n\nDifferentiating term by term, \n\n: <math>\n\\begin{align}\n(e^x)' & = \\sum_{n=0}^\\infty {x^n \\over n!} = 0 + 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\frac{x^4}{4!} + \\cdots \\\\[4pt]\n& = \\sum_{n=0}^\\infty {x^n \\over n!} = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\frac{x^4}{4!} + \\cdots\n\\end{align}\n</math>\n\nwhich is identical to <math>(e^x)</math> and thus equal, so\n\n: <math>(e^x)' = (e^x)'</math>\n-->\n\n== Larger domains ==\nOne way of defining the exponential function for domains larger than the domain of real numbers is to first define it for the domain of real numbers using one of the above characterizations and then extend it to larger domains in a way which would work for any [[analytic function]].\n\nIt is also possible to use the characterisations directly for the larger domain, though some problems may arise. (1), (2), and (4) all make sense for arbitrary [[Banach algebra]]s. (3) presents a problem for complex numbers, because there are non-equivalent paths along which one could integrate, and (5) is not sufficient. For example, the function ''f'' defined (for ''x'' and ''y'' real) as\n\n: <math> f(x + iy) = e^x(\\cos(2y) + i\\sin(2y)) = e^{x + 2iy} </math>\n\nsatisfies the conditions in (5) without being the exponential function of&nbsp;''x''&nbsp;+&nbsp;''iy''.  To make (5) sufficient for the domain of complex numbers, one may either stipulate that there exists a point at which ''f'' is a [[conformal map]] or else stipulate that\n\n:  <math> f(i) = \\cos(1) + i\\sin(1). </math>\n\nIn particular, the alternate condition in (5) that <math>f'(0)=1</math> is sufficient since it implicitly stipulates that ''f'' be conformal.\n\n== Proof that each characterization makes sense ==\n\nSome of these definitions require justification to demonstrate that they are [[well-defined]]. For example, when the value of the function is defined as the result of a [[limit of a function|limiting process]] (i.e. an [[infinite sequence]] or [[series (mathematics)|series]]), it must be demonstrated that such a limit always exists.\n\n=== Characterization 2 ===\nSince\n\n: <math>\\lim_{n\\to\\infty} \\left|\\frac{x^{n+1}/(n+1)!}{x^n/n!}\\right|\n   = \\lim_{n\\to\\infty} \\left|\\frac{x}{n+1}\\right|\n   = 0 < 1.</math>\n\nit follows from the [[ratio test]] that <math>\\sum_{n=0}^\\infty \\frac{x^n}{n!}</math> converges for all ''x''.\n\n=== Characterization 3 ===\nSince the integrand is an [[integrable function]] of ''t'', the integral expression is well-defined.  Now we must show that the function from <math>\\mathbb{R}^+</math> to <math>\\mathbb{R}</math> defined by\n\n: <math>\\int_1^{(\\cdot)} \\frac{dt}{t}</math>\n\nis a [[bijection]].  As <math>t^{-1}</math> is positive for positive ''t'', this function is [[Monotonic function|monotone increasing]], hence [[bijection|one-to-one]].  If the two integrals\n: <math>\n\\begin{align}\n\\int_1^\\infty \\frac{dt} t & = \\infty \\\\[8pt]\n\\int_1^0 \\frac{dt} t & = -\\infty\n\\end{align}\n</math>\nhold, then it is clearly onto as well.  Indeed, these integrals ''do'' hold; they follow from the [[integral test]] and the divergence of the [[harmonic series (mathematics)|harmonic series]].\n\n== Equivalence of the characterizations ==<!-- This section is linked from [[E (mathematical constant)]] -->\n\nThe following proof demonstrates the equivalence of the first three characterizations given for ''e'' above. The proof consists of two parts. First, the equivalence of characterizations 1 and 2 is established, and then the equivalence of characterizations 1 and 3 is established.\n\n===Equivalence of characterizations 1 and 2===\n\nThe following argument is adapted from a proof in Rudin, theorem 3.31, p.&nbsp;63–-5.\n\nLet <math>x\\geq0</math> be a fixed non-negative real number. Define\n\n:<math>s_n = \\sum_{k=0}^n\\frac{x^k}{k!},\\ t_n=\\left(1+\\frac x n \\right)^n.</math>\n\nBy the [[binomial theorem]],\n\n:<math>\n\\begin{align}\nt_n & =\\sum_{k=0}^n{n \\choose k}\\frac{x^k}{n^k}=1+x+\\sum_{k=2}^n\\frac{n(n-1)(n-2)\\cdots(n-(k-1))x^k}{k!\\,n^k} \\\\[8pt]\n& = 1+x+\\frac{x^2}{2!}\\left(1-\\frac{1}{n}\\right)+\\frac{x^3}{3!}\\left(1-\\frac{1}{n}\\right)\\left(1-\\frac{2}{n}\\right)+\\cdots \\\\[8pt]\n& {}\\qquad \\cdots +\\frac{x^n}{n!}\\left(1-\\frac{1}{n}\\right)\\cdots\\left(1-\\frac{n-1}{n}\\right)\\le s_n\n\\end{align}\n</math>\n\n(using ''x''&nbsp;≥&nbsp;0 to obtain the final inequality) so that\n\n:<math>\\limsup_{n\\to\\infty}t_n \\le \\limsup_{n\\to\\infty}s_n = e^x</math>\n\nwhere ''e''<sup>''x''</sup> is in the sense of definition&nbsp;2. Here, we must use [[limit superior and limit inferior|limsups]], because we don't yet know that ''t''<sub>''n''</sub> actually [[limit (mathematics)|converges]]. Now, for the other direction, note that by the above expression of ''t''<sub>''n''</sub>, if 2&nbsp;≤&nbsp;''m'' ≤ ''n'', we have\n\n:<math>1+x+\\frac{x^2}{2!}\\left(1-\\frac{1}{n}\\right)+\\cdots+\\frac{x^m}{m!}\\left(1-\\frac{1}{n}\\right)\\left(1-\\frac{2}{n}\\right)\\cdots\\left(1-\\frac{m-1}{n}\\right)\\le t_n.</math>\n\nFix ''m'', and let ''n'' approach infinity. We get\n\n:<math>s_m = 1+x+\\frac{x^2}{2!}+\\cdots+\\frac{x^m}{m!} \\le \\liminf_{n\\to\\infty}t_n</math>\n\n(again, we must use [[limit superior and limit inferior|liminf]]'s because we don't yet know that ''t''<sub>''n''</sub> converges). Now, take the above inequality, let ''m'' approach infinity, and put it together with the other inequality. This becomes\n\n:<math>\\limsup_{n\\to\\infty}t_n \\le e^x \\le \\liminf_{n\\to\\infty}t_n </math>\n\nso that\n\n:<math>\\lim_{n\\to\\infty}t_n = e^x. </math>\n\nWe can then extend this equivalence to the negative real numbers by noting <math>\\left(1 - \\frac r n \\right)^n  \\left(1+\\frac{r}{n}\\right)^n = \\left(1-\\frac{r^2}{n^2}\\right)^n </math> and taking the limit as n goes to infinity.\n\nThe error term of this limit-expression is described by\n\n:<math>\\left(1+\\frac x n \\right)^n=e^x \\left(1-\\frac{x^2}{2n}+\\frac{x^3(8+3x)}{24n^2}+\\cdots \\right),</math>\n\nwhere the polynomial's degree (in ''x'') in the term with denominator ''n''<sup>''k''</sup> is&nbsp;2''k''.\n\n===Equivalence of characterizations 1 and 3===\n\nHere, we define the [[natural logarithm]] function in terms of a definite integral as above. By the first part of [[fundamental theorem of calculus]],\n\n: <math>\\frac d {dx}\\ln x=\\frac{d}{dx} \\int_1^x \\frac1 t \\,dt = \\frac 1 x.</math>\n\nBesides, <math>\\ln 1 = \\int_1^1 \\frac{1}{t}\\,dt = 0</math>\n\nNow, let ''x'' be any fixed real number, and let\n\n: <math>y=\\lim_{n\\to\\infty}\\left(1+\\frac{x}{n}\\right)^n.</math>\n\nWe will show that ln(''y'') = ''x'', which implies that ''y'' = ''e''<sup>''x''</sup>, where ''e''<sup>''x''</sup> is in the sense of definition 3. We have\n\n: <math>\\ln y=\\ln\\lim_{n\\to\\infty}\\left(1+\\frac{x}{n} \\right)^n = \\lim_{n\\to\\infty} \\ln\\left(1+\\frac{x}{n}\\right)^n.</math>\n\nHere, we have used the continuity of ln(''y''), which follows from the continuity of 1/''t'':\n\n: <math>\\ln y=\\lim_{n\\to\\infty}n\\ln \\left(1+\\frac{x}{n} \\right) = \\lim_{n\\to\\infty} \\frac{x\\ln\\left(1+(x/n)\\right)}{(x/n)}.</math>\n\nHere, we have used the result ln''a''<sup>''n''</sup> = ''n''ln''a''.  This result can be established for ''n'' a natural number by induction, or using integration by substitution.  (The extension to real powers must wait until ''ln'' and ''exp'' have been established as inverses of each other, so that ''a''<sup>''b''</sup> can be defined for real ''b'' as ''e''<sup>''b'' ln''a''</sup>.)\n\n: <math>=x\\cdot\\lim_{h\\to 0}\\frac{\\ln\\left(1+h\\right)}{h} \\quad \\text{ where } h = \\frac{x}{n}</math>\n\n: <math>=x\\cdot\\lim_{h\\to 0}\\frac{\\ln\\left(1+h\\right)-\\ln 1}{h}</math>\n\n: <math>=x\\cdot\\frac{d}{dt} \\ln t \\Bigg|_{t=1}</math>\n\n: <math>\\!\\, = x.</math>\n\n===Equivalence of characterizations 2 and 4===\nLet n be a non-negative integer. In the sense of definition 4 and by induction, <math>\\frac{d^ny}{dx^n}=y</math>. \n\nTherefore <math>\\frac{d^ny}{dx^n}\\Bigg|_{x=0}=y(0)=1.</math>\n\nUsing [[Taylor series]],\n<math>y= \\sum_{n=0}^\\infty \\frac {f^{(n)}(0)}{n!} \\, x^n = \\sum_{n=0}^\\infty \\frac {1}{n!} \\, x^n = \\sum_{n=0}^\\infty \\frac {x^n}{n!}.</math> This shows that definition 4 implies definition 2.\n\nIn the sense of definition 2,\n\n: <math>\n\\begin{align}\n\\frac{d}{dx}e^x & = \\frac{d}{dx} \\left(1+\\sum_{n=1}^\\infty \\frac {x^n}{n!} \\right) = \\sum_{n=1}^\\infty \\frac {nx^{n-1}}{n!} =\\sum_{n=1}^\\infty \\frac {x^{n-1}}{(n-1)!} \\\\[6pt]\n& =\\sum_{k=0}^\\infty \\frac {x^k}{k!}, \\text{ where } k=n-1 \\\\[6pt]\n& =e^x\n\\end{align}\n</math>\n\nBesides, <math>e^0=1+0+\\frac{0^2}{2!}+\\frac{0^3}{3!}+\\cdots=1.</math> This shows that definition 2 implies definition&nbsp;4.\n\n===Equivalence of characterizations 1 and 5===\n\nThe following proof is a simplified version of the one in Hewitt and Stromberg, exercise 18.46.  First, one proves that measurability (or here, Lebesgue-integrability) implies continuity for a non-zero function <math>f(x)</math> satisfying <math>f(x+y)=f(x)f(y)</math>, and then one proves that continuity implies <math>f(x) = e^{kx}</math> for some ''k'', and finally <math>f(1)=e</math> implies ''k''=1.\n\nFirst, we prove a few elementary properties from <math>f(x)</math> satisfying <math>f(x+y)=f(x)f(y)</math> and the assumption that <math>f(x)</math> is not identically zero:\n* If <math>f(x)</math> is nonzero anywhere (say at ''x''=''y''), then it is non-zero everywhere.  Proof: <math>f(y) = f(x) f(y - x) \\neq 0</math> implies <math>f(x) \\neq 0</math>.\n* <math>f(0)=1</math>.  Proof: <math>f(x)= f(x+0) = f(x) f(0)</math> and <math>f(x)</math> is non-zero.\n* <math>f(-x)=1/f(x)</math>. Proof: <math>1 = f(0)= f(x-x) = f(x) f(-x)</math>.\n* If <math>f(x)</math> is continuous anywhere (say at ''x'' = ''y''), then it is continuous everywhere.  Proof: <math>f(x+\\delta)-f(x) = f(x-y) [ f(y+\\delta) - f(y)] \\rightarrow 0</math> as <math>\\delta\\rightarrow 0</math> by continuity at&nbsp;''y''.\n\nThe second and third properties mean that it is sufficient to prove <math>f(x)=e^x</math> for positive&nbsp;''x''.\n\nIf <math>f(x)</math> is a [[Lebesgue-integrable function]], then we can define\n\n:<math>g(x) = \\int_0^x f(x')\\, dx'.</math>\n\nIt then follows that\n\n:<math>g(x+y)-g(x) = \\int_x^{x+y} f(x')\\, dx' = \\int_0^y f(x+x')\\, dx' = f(x) g(y). </math>\n\nSince <math>f(x)</math> is nonzero, we can choose some ''y'' such that <math>g(y) \\neq 0</math> and solve for <math>f(x)</math> in the above expression.  Therefore:\n\n:<math>\n\\begin{align}\nf(x+\\delta)-f(x) & = \\frac{[g(x+\\delta+y)-g(x+\\delta)]-[g(x+y)-g(x)]}{g(y)} \\\\\n& =\\frac{[g(x+y+\\delta)-g(x+y)]-[g(x+\\delta)-g(x)]}{g(y)} \\\\\n& =\\frac{f(x+y)g(\\delta)-f(x)g(\\delta)}{g(y)}=g(\\delta)\\frac{f(x+y)-f(x)}{g(y)}.\n\\end{align}\n</math>\n\nThe final expression must go to zero as <math>\\delta\\rightarrow 0</math> since <math>g(0)=0</math> and <math>g(x)</math> is continuous.  It follows that <math>f(x)</math> is continuous.\n\nNow, we prove that <math>f(q) = e^{kq}</math>, for some ''k'', for all positive rational numbers ''q''.  Let ''q''=''n''/''m'' for positive integers ''n'' and ''m''.  Then\n:<math>f\\left(\\frac{n}{m}\\right)=f\\left(\\frac{1}{m}+\\cdots+\\frac{1}{m} \\right)=f\\left(\\frac{1}{m}\\right)^n</math>\nby elementary induction on ''n''.  Therefore, <math>f(1/m)^m = f(1)</math> and thus\n\n:<math>f\\left(\\frac{n}{m}\\right)=f(1)^{n/m}=e^{k(n/m)}.</math>\n\nfor <math>k = \\ln [f(1)]</math>.  Note that if we are restricting ourselves to real-valued <math>f(x)</math>, then <math>f(x) = f(x/2)^2</math> is everywhere positive and so ''k'' is real.\n\nFinally, by continuity, since <math>f(x) = e^{kx}</math> for all rational ''x'', it must be true for all real ''x'' since the [[Closure (mathematics)|closure]] of the rationals is the reals (that is, we can write any real ''x'' as the limit of a sequence of rationals).  If <math>f(1) = e</math> then ''k'' = 1.  This is equivalent to characterization 1 (or 2, or 3), depending on which equivalent definition of [[e (mathematical constant)|e]] one uses.\n\n===Characterization 2 implies characterization 6===\n\nIn the sense of definition 2,<ref>[https://www.youtube.com/watch?v=HHenriJHNMM&index=30&list=PLzDe9mOi1K8o2lveHTSM04WAhaGEZE7xB]</ref> \n::<math>\\lim_{h\\to 0} \\frac{e^h-1}{h}</math>\n:<math>=\\lim_{h\\to 0} \\frac{1}{h} \\left (\\left (1+h+ \\frac{h^2}{2!}+\\frac{h^3}{3!}+\\frac{h^4}{4!}+\\cdots \\right) -1 \\right)</math>\n:<math>=\\lim_{h\\to 0} \\left(1+ \\frac{h}{2!}+\\frac{h^2}{3!}+\\frac{h^3}{4!}+\\cdots \\right)</math>\n:<math>=1</math>\n\n===Characterization 6 implies characterization 4===\n\nIn the sense of definition 6, <math>\\frac{d}{dx}e^x=\\lim_{h \\to 0} \\frac{e^{x+h}-e^x}{h}=e^x \\cdot \\lim_{h \\to 0}\\frac{e^h-1}{h}=e^x.</math>\nBy the way <math>e^0=1</math>, therefore definition 6 implies definition 4.\n\n== References ==\n{{Reflist}}\n\n* [[Walter Rudin]], ''Principles of Mathematical Analysis'', 3rd edition (McGraw–Hill, 1976), chapter 8.\n* [[Edwin Hewitt]] and Karl Stromberg, ''Real and Abstract Analysis'' (Springer, 1965).\n\n[[Category:Mathematical analysis]]\n[[Category:Exponentials]]\n[[Category:Definitions|Exponential function]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Compound interest",
      "url": "https://en.wikipedia.org/wiki/Compound_interest",
      "text": "{{short description|A compounding sum paid for the use of money}}\n[[File:Compound interest (English).gif|thumb|300px|Effective interest rates]]\n[[File:Compound Interest with Varying Frequencies.svg|thumb|right|310px|The effect of earning 20% annual interest on an initial $1,000 investment at various compounding frequencies]]\n{{E (mathematical constant)}}\n\n'''Compound interest''' is the addition of [[interest]] to the [[principal sum]] of a loan or deposit, or in other words, interest on interest. It is the result of reinvesting interest, rather than paying it out, so that interest in the next period is then earned on the principal sum plus previously accumulated interest. Compound interest is standard in [[finance]] and [[economics]].\n\nCompound interest is contrasted with ''simple interest'', where previously accumulated  interest is not added to the principal amount of the current period, so there is no compounding. The ''simple annual interest rate'' is the interest amount per period, multiplied by the number of periods per year. The simple annual interest rate is also known as the '''[[nominal interest rate]]''' (not to be confused with [[real versus nominal value (economics)|the interest rate not adjusted for inflation]], which goes by the same name).\nCompound Interest = P (1+(r/n))^nt\n\n==Compounding frequency==\nThe ''compounding frequency'' is the number of times per year (or other unit of time) the accumulated interest is paid out, or ''capitalized'' (credited to the account), on a regular basis. The frequency could be yearly, half-yearly, quarterly, monthly, weekly, daily, or [[continuous compounding|continuously]] (or not at all, until maturity).\n\nFor example, monthly capitalization with annual rate of interest means that the compounding frequency is 12, with time periods measured in months.\n\nThe effect of compounding depends on:\n# The nominal interest rate which is applied and\n# The frequency interest is compounded.\nA compound interest calculator is a tool that allows calculating such compounding effect on loans or investments.<ref>{{Cite news|url=https://www.mycheckweb.com/calculators/compound-interest-calculator/|title=Compound Interest Calculatorz - MyCheckWeb.Com|work=MyCheckWeb.Com|access-date=2018-03-30|language=en-US}}</ref>\n\n==Annual equivalent rate==\nThe nominal rate cannot be directly compared between loans with different compounding frequencies. Both the nominal interest rate and the compounding frequency are required in order to compare interest-bearing financial instruments.\n\nTo help consumers compare retail financial products more fairly and easily, many countries require financial institutions to disclose the annual compound interest rate on deposits or advances on a comparable basis. The interest rate on an annual equivalent basis may be referred to variously in different markets as ''[[annual percentage rate]]'' (APR), ''[[annual equivalent rate]]'' (AER), ''[[effective interest rate]]'', ''[[effective annual rate]]'', ''[[annual percentage yield]]''  and other terms. The effective annual rate is the total accumulated interest that would be payable up to the end of one year, divided by the principal sum.\n\nThere are usually two aspects to the rules defining these rates:\n\n#The rate is the annualised compound interest rate, and\n#There may be charges other than interest. The effect of fees or taxes which the customer is charged, and which are directly related to the product, may be included. Exactly which fees and taxes are included or excluded varies by country, may or may not be comparable between different jurisdictions, because the use of such terms may be inconsistent, and vary according to local practice.\n\n==Examples==\n* 1,000 Brazilian real (BRL) is deposited into a Brazilian savings account paying 20% per annum, compounded annually. At the end of one year, 1,000 x 20% = 200 BRL interest is credited to the account. The account then earns 1,200 x 20% = 240 BRL in the second year.\n* A rate of 1% per month is equivalent to a simple annual interest rate (nominal rate) of 12%, but allowing for the effect of compounding, the annual equivalent compound rate is 12.68% per annum (1.01<sup>12</sup> − 1).\n* The interest on corporate bonds and government bonds is usually payable twice yearly. The amount of interest paid (each six months) is the disclosed interest rate divided by two and multiplied by the principal. The yearly compounded rate is higher than the disclosed rate.\n* Canadian [[mortgage loan]]s are generally compounded semi-annually with monthly (or more frequent) payments.<ref>http://laws.justice.gc.ca/en/showdoc/cs/I-15/bo-ga:s_6//en#anchorbo-ga:s_6 Interest Act (Canada), ''Department of Justice''. The Interest Act specifies that interest is not recoverable unless the mortgage loan contains a statement showing the rate of interest chargeable, \"calculated yearly or half-yearly, not in advance.\" In practice, banks use the half-yearly rate.</ref>\n* U.S. mortgages use an [[amortizing loan]], not compound interest. With these loans, an [[amortization schedule]] is used to determine how to apply payments toward principal and interest.  Interest generated on these loans is not added to the principal, but rather is paid off monthly as the payments are applied.\n* It is sometimes mathematically simpler, for example, in the valuation of [[Derivative (finance)|derivatives]], to use ''continuous compounding'', which is the [[Limit of a function|limit]] as the compounding period approaches zero. Continuous compounding in pricing these instruments is a natural consequence of [[Itō calculus]], where [[Derivative (finance)|financial derivatives]] are valued at ever increasing frequency, until the limit is approached and the derivative is valued in continuous time.\n\n==Discount instruments==\n* US and Canadian T-Bills (short term Government debt) have a different convention. Their interest is calculated on a discount basis as (100 − ''P'')/''Pbnm'',{{clarify|date=July 2013}} where ''P'' is the price paid. Instead of normalizing it to a year, the interest is prorated by the number of days ''t'': (365/''t'')×100. (See [[day count convention]]).\n\n==Calculation==\n{{See also|Time value of money|Interest#Calculation}}\n\n===Periodic compounding===\nThe total accumulated value, including the principal sum <math>P</math> plus compounded interest <math>I</math>, is given by the formula:<ref>{{Cite web|url=https://qrc.depaul.edu/StudyGuide2009/Notes/Savings%20Accounts/Compound%20Interest.htm|title=Compound Interest Formula|last=|first=|date=|website=qrc.depaul.edu|archive-url=|archive-date=|dead-url=|access-date=2018-12-05}}</ref><ref>{{Cite web|url=https://www.investopedia.com/terms/c/continuouscompounding.asp|title=Continuous Compounding|last=Staff|first=Investopedia|date=2003-11-19|website=Investopedia|language=en|archive-url=|archive-date=|dead-url=|access-date=2018-12-05}}</ref> \n:<math>P' = P \\left(1 + \\frac{r}{n}\\right)^{nt}</math>\n\nwhere:\n:''P'' is the original principal sum\n:''P''' is the new principal sum\n:''r'' is the [[nominal annual interest rate]]\n:''n'' is the compounding frequency\n:''t'' is the overall length of time the interest is applied (expressed using the same time units as ''r'', usually years).\n\nThe total compound interest generated is the final value minus the initial principal:<ref>{{Cite web|url=https://www.thecalculatorsite.com/articles/finance/compound-interest-formula.php|title=Compound Interest Formula - Explained|last=|first=|date=|website=www.thecalculatorsite.com|archive-url=|archive-date=|dead-url=|access-date=2018-12-05}}</ref>\n\n:<math>I = P \\left(1 + \\frac{r}{n}\\right)^{nt} - P</math>\n\n====Example 1====\nSuppose a principal amount of $1,500 is deposited in a bank paying an annual interest rate of 4.3%, compounded quarterly.<br> Then the balance after 6 years is found by using the formula above, with ''P'' = 1500, ''r'' = 0.043 (4.3%), ''n'' = 4, and ''t'' = 6:\n\n:<math>P' =1\\,500\\times \\left(1 + \\frac{0.043}{4}\\right)^{4 \\times 6}\\approx 1\\,938.84</math>\n\nSo the new principal <math>P'</math> after 6 years is approximately $1,938.84.\n\nSubtracting the original principal from this amount gives the amount of interest received:\n\n:<math>1\\,938.84 - 1\\,500 = 438.84</math>\n\n====Example 2====\n\nSuppose the same amount of $1,500 is compounded biennially (every 2 years).<br> Then the balance after 6 years is found by using the formula above, with ''P'' = 1500, ''r'' = 0.043 (4.3%), ''n'' = 1/2 (the interest is compounded every two years), and ''t'' = 6 :\n\n:<math>P' = 1\\,500\\times \\left(1 + (0.043\\times 2)\\right)^{\\frac 6 2}\\approx 1\\,921.24</math> \n\nSo, the balance after 6 years is approximately $1,921.24.\n\nThe amount of interest received can be calculated by subtracting the principal from this amount.\n\n:<math>1\\,921.24 - 1\\,500 = 421.24</math>\n\nThe interest is less compared with the previous case, as a result of the lower compounding frequency.\n\n===Accumulation function===\n\nSince the principal ''P'' is simply a coefficient, it is often dropped for simplicity, and the resulting [[accumulation function]] is used instead. The accumulation function shows what $1 grows to after any length of time. Accumulation functions for simple and compound interest are\n\n:<math>a(t)=1+t r\\,</math>\n:<math>a(t) = \\left(1 + \\frac {r} {n}\\right) ^ {nt} </math>\n\n===Continuous compounding===<!-- This section is linked from [[Interest]] and from [[Continuously compounded interest]] -->\n{{see also|Logarithmic return}}\n\nAs ''n'', the number of compounding periods per year, increases without limit, the case is known as continuous compounding, in which case the effective annual rate approaches an upper limit of {{math|e<sup>''r''</sup>&nbsp;−&nbsp;1}}, where [[e (mathematical constant)|{{mvar|e}}]] is a [[mathematical constant]] that is the base of the [[natural logarithm]].\n\nContinuous compounding can be thought of as making the compounding period infinitesimally small, achieved by taking the [[Limit (mathematics)|limit]] as ''n'' goes to [[infinity]]. See [[definitions of the exponential function]] for the mathematical proof of this limit. The amount after ''t'' periods of continuous compounding can be expressed in terms of the initial amount ''P''<sub>0</sub> as\n\n:<math>P(t)=P_0 e ^ {rt}.</math>\n\n===Force of interest===\nAs the number of compounding periods <math>n</math> reaches infinity in continuous compounding, the continuous compound interest is referred to as the force of interest <math>\\delta</math>.\n\nIn mathematics, the accumulation functions are often expressed in terms of ''[[E (mathematical constant)|e]]'', the base of the [[natural logarithm]]. This facilitates the use of calculus to manipulate interest formulae.\n\nFor any continuously differentiable [[accumulation function]] ''a(t)'', the force of interest, or more generally the [[Rate of return#Logarithmic or continuously compounded return|logarithmic or continuously compounded return]] is a function of time defined as follows:\n\n:<math>\\delta_{t}=\\frac{a'(t)}{a(t)}=\\frac{d}{dt} \\ln a(t)</math>\n\nThis is the [[logarithmic derivative]] of the accumulation function.\n\nConversely:\n:<math>a(t)=e^{\\int_0^t \\delta_s\\, ds}\\ ,</math> (since <math>a(0) = 1</math>; this can be viewed as a particular case of a [[product integral]]).\n\nWhen the above formula is written in differential equation format, then the force of interest is simply the coefficient of amount of change:\n:<math>da(t)=\\delta_{t}a(t)\\,dt\\,</math>\n\nFor compound interest with a constant annual interest rate ''r'', the force of interest is a constant, and the accumulation function of compounding interest in terms of force of interest is a simple power of e:\n:<math>\\delta=\\ln(1+r)\\,</math> or\n:<math>a(t)=e^{t\\delta}\\,</math>\n\nThe force of interest is less than the annual effective interest rate, but more than the [[annual effective discount rate]]. It is the reciprocal of the [[e-folding]] time. See also [[Actuarial notation#Interest rates|notation of interest rates]].\n\nA way of modeling the force of inflation is with Stoodley's formula:&nbsp;<math>\\delta_t = p + {s \\over {1+rse^{st}}}</math> where p, r and s are estimated. <!-- c.f. Stochastic Modeling in Economics and Finance, By Jitka Dupacova, J. Hurt, J. Stepan -->\n\n===Compounding basis===\n{{See also|Day count convention}}\n\nTo convert an interest rate from one compounding basis to another compounding basis, use\n\n:<math>r_2=\\left[\\left(1+\\frac{r_1}{n_1}\\right)^\\frac{n_1}{n_2}-1\\right]{n_2},</math>\n\nwhere\n''r''<sub>1</sub> is the interest rate with compounding frequency ''n''<sub>1</sub>, and\n''r''<sub>2</sub> is the interest rate with compounding frequency ''n''<sub>2</sub>.\n\nWhen interest is [[#Continuous compounding|continuously compounded]], use\n\n:<math>\\delta=n\\ln{\\left(1+\\frac{r}{n}\\right)},</math>\n\nwhere\n''<math>\\delta</math>'' is the interest rate on a continuous compounding basis, and\n''r'' is the stated interest rate with a compounding frequency ''n''.\n\n===Monthly amortized loan or mortgage payments===\n{{sources|section|date=June 2019}}\n{{See also|Mortgage calculator#Monthly payment formula}}\nThe interest on loans and mortgages that are amortized—that is, have a smooth monthly payment until the loan has been paid off—is often compounded monthly. The formula for payments is found from the following argument.\n\n====Exact formula for monthly payment====\nAn exact formula for the monthly payment (<math>c</math>) is\n:<math>\nc= \\frac{Pr}{1-\\frac{1}{(1+r)^n}}\n</math>\nor equivalently\n:<math>\nc= \\frac{Pr}{1-e^{-n\\ln(1+r)}}\n</math>\n\nwhere:\n: <math>c</math> = monthly payment\n: <math>P</math> = principal\n: <math>r</math> = monthly interest rate\n: <math>n</math> = number of payment periods\n\nThis can be derived by considering how much is left to be repaid after each month. <br>The Principal remaining after the first month is\n:<math>\nP_1=(1+r)P - c,</math>\nthat is, the initial amount has increased less the payment. <br>If the whole loan is repaid after one month then\n:<math>P_1=0</math>, so <math>P=\\frac{c}{1+r}</math>\nAfter the second month <math>P_2=(1+r) P_1 - c</math> is left, so\n:<math>P_2=(1+r)((1+r)P-c)-c</math>\n\nIf the whole loan was repaid after two months,\n:<math>P_2=0</math>, so <math>P = \\frac{c}{1+r}+\\frac{c}{(1+r)^2}</math>\n\nThis equation generalises for a term of n months, <math> P = c \\sum_{j=1}^n \\frac{1}{(1+r)^j} </math>. This is a [[geometric series]] which has the sum\n:<math>P=\\frac{c}{r}\\left(1-\\frac{1}{(1+r)^n}\\right)</math>\nwhich can be rearranged to give\n:<math>\nc= \\frac{Pr}{1-\\frac{1}{(1+r)^n}}=\\frac{Pr}{1-e^{-n\\ln(1+r)}}\n</math>\n\n;Spreadsheet formula\nIn spreadsheets, the '''PMT()''' function is used.  The syntax is:\n\n:''PMT( interest_rate, number_payments, present_value, future_value,[Type] )''\nSee [https://support.office.com/en-us/article/PMT-function-0214da64-9a63-4996-bc20-214433fa6441 Excel], [https://www.apple.com/ca/mac/numbers/compatibility/functions.html#financial Mac Numbers], [https://help.libreoffice.org/Calc/Financial_Functions_Part_Two#PMT Libreoffice], [https://wiki.openoffice.org/wiki/Documentation/How_Tos/Calc:_PMT_function Open Office] for more details.\n\nFor example, for interest rate of 6% (0.06/12), 25 years * 12 p.a., PV of $150,000, FV of 0, type of 0 gives:\n\n:= PMT( 0.06/12, 25 * 12, -150000, 0, 0 )\n:= $966.45\n\n====Approximate formula for monthly payment====\nA formula that is accurate to within a few percent can be found by\nnoting that for typical U.S. note rates (<math>I<8\\%</math> and terms <math>T</math>=10–30 years), the monthly note rate is small compared to 1:\n<math>r<<1</math> so that the <math>\\ln(1+r)\\approx r</math> which yields\na simplification so that\n\n:<math>c\\approx \\frac{Pr}{1-e^{-nr}}= \\frac{P}{n}\\frac{nr}{1-e^{-nr}}</math>\n\nwhich suggests defining auxiliary variables\n\n:<math>Y\\equiv n r = IT</math>\n\n:<math>c_0\\equiv \\frac{P}{n} </math>.\n\nHere <math>c_0</math> is the monthly payment required for a zero–interest loan paid off in <math>n</math> installments. In terms of these variables the\napproximation can be written\n\n:<math>c\\approx c_0 \\frac{Y}{1-e^{-Y}},</math>\n\nThe function <math>f(Y)\\equiv \\frac{Y}{1-e^{-Y}}-\\frac{Y}{2}</math> is even:\n\n:<math>f(Y)=f(-Y)</math>\n\nimplying that it can be expanded in even powers of <math>Y</math>.\n\nIt follows immediately that <math>\\frac{Y}{1-e^{-Y}}</math> can be expanded in even powers\nof <math>Y</math> plus the single term: <math>Y/2 .</math>\n\nIt will prove convenient then to define\n\n:<math>X=\\frac{1}{2}Y = \\frac{1}{2}IT</math>\n\nso that\n\n:<math>c\\approx c_0 \\frac{2X}{1-e^{-2X}}</math>\nwhich can be expanded:\n:<math>\nc\\approx c_0 \\left(1 + X + \\frac{X^2}{3} - \\frac{1}{45} X^4 + ...\\right)\n</math>\n\nwhere the ellipses indicate terms that are higher order in even powers of <math>X</math>. The expansion\n\n:<math>\nP\\approx P_0 \\left(1 + X + \\frac{X^2}{3}\\right)\n</math>\n\nis valid to better than 1% provided <math>X\\le 1 </math>.\n\n====Example of mortgage payment====\nFor a $10,000 mortgage with a term of 30 years and a note rate of 4.5%, payable yearly, we find:\n\n:<math>T=30</math>\n\n:<math>I=0.045</math>\n\nwhich gives\n\n:<math>X=\\frac{1}{2}IT=.675</math>\n\nso that\n\n:<math>\nP\\approx P_0 \\left(1 + X + \\frac{1}{3}X^2 \\right)=\\$333.33 (1+.675+.675^2/3)=\\$608.96\n</math>\n\nThe exact payment amount is <math>P=\\$608.02</math> so the approximation is an overestimate of about a sixth of a percent.\n\n==History==\nCompound interest was once regarded as the worst kind of [[usury]] and was severely condemned by [[Roman law]] and the [[common law]]s of many other countries.<ref name=\"r1728\">{{1728}}</ref>\n\nThe Florentine merchant [[Francesco Balducci Pegolotti]] provided a [http://www.medievalacademy.org/resource/resmgr/maa_books_online/evans_0024.htm#hd_ma0024_head_755 table of compound interest] in his book ''[[Pratica della mercatura]]'' of about 1340. It gives the interest on 100 lire, for rates from 1% to 8%, for up to 20 years.<ref>{{cite book | last = Evans |first = Allan | title = Francesco Balducci Pegolotti, La Pratica della Mercatura | place = Cambridge, Massachusetts | year = 1936 |pages = 301–2}}</ref> The ''[[Summa de arithmetica]]'' of [[Luca Pacioli]] (1494) gives the [[Rule of 72]], stating that to find the number of years for an investment at compound interest to double, one should divide the interest rate into 72.\n\n[[Richard Witt]]'s book ''Arithmeticall Questions'', published in 1613, was a landmark in the history of compound interest. It was wholly devoted to the subject (previously called '''[https://en.wiktionary.org/wiki/anatocism#Etymology anatocism]'''), whereas previous writers had usually treated compound interest briefly in just one chapter in a mathematical textbook. Witt's book gave tables based on 10% (the then maximum rate of interest allowable on loans) and on other rates for different purposes, such as the valuation of property leases. Witt was a London mathematical practitioner and his book is notable for its clarity of expression, depth of insight and accuracy of calculation, with 124 worked examples.<ref>{{cite journal | last = Lewin | first = C G | year = 1970 | title = An Early Book on Compound Interest - Richard Witt's Arithmeticall Questions| journal = Journal of the Institute of Actuaries | volume = 96 | issue = 1 | pages = 121–132 }}</ref><ref>{{cite journal | last = Lewin | first = C G | year = 1981 | title = Compound Interest in the Seventeenth Century | journal = Journal of the Institute of Actuaries | volume = 108 | issue = 3 | pages = 423–442 }}</ref>\n\n[[Jacob Bernoulli]] discovered the constant [[e (mathematical constant)#Compound interest|<math>e</math>]] in 1683 by studying a question about compound interest.\n\n==See also==\n{{wikiquote}}\n{{wiktionary|interest}}\n* [[Credit card interest]]\n* [[Exponential growth]]\n* [[Fisher equation]]\n* [[Interest]]\n* [[Interest rate]]\n* [[Rate of return]]\n* [[Rate of return on investment]]\n* [[Real versus nominal value (economics)]]\n* [[Yield curve]]\n* [[e (mathematical constant)]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [https://play.google.com/store/apps/details?id=com.oshemb.bitconnectlendingcalculator Compound Daily Interest Calculator]\n\n{{Authority control}}\n\n[[Category:Interest]]\n[[Category:Exponentials]]\n[[Category:Mathematical finance]]\n[[Category:Actuarial science]]\n\n[[it:Anatocismo]]"
    },
    {
      "title": "Doubling time",
      "url": "https://en.wikipedia.org/wiki/Doubling_time",
      "text": "The '''doubling time''' is the period of time required for a quantity to double in size or value. It is applied to [[population growth]], [[inflation]], [[resource extraction]], [[Consumption (economics)|consumption]] of goods, [[compound interest]], the volume of [[Cancer|malignant tumours]], and many other things that tend to grow over time.  When the [[Relative growth rate|relative growth rate]] (not the absolute growth rate) is constant, the quantity undergoes [[exponential growth]] and has a constant doubling time or period, which can be calculated directly from the growth rate.\n\nThis time can be calculated by dividing the [[natural logarithm]] of 2 by the exponent of growth, or approximated by dividing 70 by the percentage growth rate<ref>[[Donella Meadows]], ''Thinking in Systems: A Primer'', [[Chelsea Green Publishing]], 2008, page 33 (box \"Hint on reinforcing feedback loops and doubling time\").</ref> (more roughly but roundly, dividing 72; see the [[rule of 72]] for details and [[Rule of 72#Derivation|a derivation of this formula]]).\n\nThe doubling time is a [[characteristic unit]] (a natural unit of scale) for the exponential growth equation, and its converse for [[exponential decay]] is the [[half-life]].\n\nFor example, given Canada's net population growth of 0.9% in the year 2006, dividing 70 by 0.9 gives an approximate doubling time of 78 years.  Thus if the growth rate remains constant, Canada's population would double from its 2006 figure of 33 million to 66 million by 2084.\n\n== History ==\nThe notion of doubling time dates to interest on loans in [[Babylonian mathematics]]. Clay tablets from circa 2000 BCE include the exercise \"Given an interest rate of 1/60 per month (no compounding), come the doubling time.\" This yields an annual interest rate of 12/60 = 20%, and hence a doubling time of 100% growth/20% growth per year = 5 years.<ref name=\"hudson\">[http://michael-hudson.com/2007/08/why-the-miracle-of-compound-interest-leads-to-financial-crises/ Why the “Miracle of Compound Interest” leads to Financial Crises], by Michael Hudson</ref><ref>[http://plus.maths.org/issue11/features/compound/ Have we caught your interest?] by John H. Webb</ref> Further, repaying double the initial amount of a loan, after a fixed time, was common commercial practice of the period: a common Assyrian loan of 1900 BCE consisted of loaning 2 minas of gold, getting back 4 in five years,<ref name=\"hudson\"/> and an Egyptian proverb of the time was \"If wealth is placed where it bears interest, it comes back to you redoubled.\"<ref name=\"hudson\"/><ref>Miriam Lichtheim, Ancient Egyptian Literature, II:135.</ref>\n\n== Examination ==\n\nExamining the doubling time can give a more intuitive sense of the long-term impact of growth than simply viewing the percentage growth rate\n\nFor a constant growth rate of ''r%'' within time t, the formula for the doubling time ''T''<sub>''d''</sub> is given by\n\n:<math> T_{d} = t \\frac{\\ln(2)}{\\ln(1+\\frac{r}{100})} \\approx t \\frac{70}{r}</math>\n\nSome doubling times calculated with this formula are shown in this table.\n\nSimple doubling time formula:\n\n:<math>N(t) = N_0 2^{t/T_d}</math>\n\n* ''N''(''t'') = the number of objects at time ''t''\n* ''T<sub>d</sub>'' = doubling period (time it takes for object to double in number)\n* ''N<sub>0</sub>'' = initial number of objects\n* ''t'' = time\n\n{|\n|+ '''Doubling times ''T''<sub>''d''</sub> given constant ''r''% growth'''\n|- style=\"vertical-align:top\"\n|\n{| class=\"wikitable\"\n!''r''% !! ''T<sub>d</sub>''\n|-\n| &nbsp;0.1 || style=\"text-align:right\" | 693.49\n|-\n| &nbsp;0.2 || style=\"text-align:right\" | 346.92\n|-\n| &nbsp;0.3 || style=\"text-align:right\" | 231.40\n|-\n| &nbsp;0.4 || style=\"text-align:right\" | 173.63\n|-\n| &nbsp;0.5 || style=\"text-align:right\" | 138.98\n|-\n| &nbsp;0.6 || style=\"text-align:right\" | 115.87\n|-\n| &nbsp;0.7 || style=\"text-align:right\" | 99.36\n|-\n| &nbsp;0.8 || style=\"text-align:right\" | 86.99\n|-\n| &nbsp;0.9 || style=\"text-align:right\" | 77.36\n|-\n| &nbsp;1.0 || style=\"text-align:right\" | 69.66\n|}\n|\n{| class=\"wikitable\"\n!''r''% !! ''T<sub>d</sub>''\n|-\n| &nbsp;1.1 || style=\"text-align:right\" | 63.36\n|-\n| &nbsp;1.2 || style=\"text-align:right\" | 58.11\n|-\n| &nbsp;1.3 || style=\"text-align:right\" | 53.66\n|-\n| &nbsp;1.4 || style=\"text-align:right\" | 49.86\n|-\n| &nbsp;1.5 || style=\"text-align:right\" | 46.56\n|-\n| &nbsp;1.6 || style=\"text-align:right\" | 43.67\n|-\n| &nbsp;1.7 || style=\"text-align:right\" | 41.12\n|-\n| &nbsp;1.8 || style=\"text-align:right\" | 38.85\n|-\n| &nbsp;1.9 || style=\"text-align:right\" | 36.83\n|-\n| &nbsp;2.0 || style=\"text-align:right\" | 35.00\n|}\n|\n{| class=\"wikitable\"\n!''r''% !! ''T<sub>d</sub>''\n|-\n| &nbsp;2.1 || style=\"text-align:right\" | 33.35\n|-\n| &nbsp;2.2 || style=\"text-align:right\" | 31.85\n|-\n| &nbsp;2.3 || style=\"text-align:right\" | 30.48\n|-\n| &nbsp;2.4 || style=\"text-align:right\" | 29.23\n|-\n| &nbsp;2.5 || style=\"text-align:right\" | 28.07\n|-\n| &nbsp;2.6 || style=\"text-align:right\" | 27.00\n|-\n| &nbsp;2.7 || style=\"text-align:right\" | 26.02\n|-\n| &nbsp;2.8 || style=\"text-align:right\" | 25.10\n|-\n| &nbsp;2.9 || style=\"text-align:right\" | 24.25\n|-\n| &nbsp;3.0 || style=\"text-align:right\" | 23.45\n|}\n|\n{| class=\"wikitable\"\n!''r''% !! ''T<sub>d</sub>''\n|-\n| &nbsp;3.1 || style=\"text-align:right\" | 22.70\n|-\n| &nbsp;3.2 || style=\"text-align:right\" | 22.01\n|-\n| &nbsp;3.3 || style=\"text-align:right\" | 21.35\n|-\n| &nbsp;3.4 || style=\"text-align:right\" | 20.73\n|-\n| &nbsp;3.5 || style=\"text-align:right\" | 20.15\n|-\n| &nbsp;3.6 || style=\"text-align:right\" | 19.60\n|-\n| &nbsp;3.7 || style=\"text-align:right\" | 19.08\n|-\n| &nbsp;3.8 || style=\"text-align:right\" | 18.59\n|-\n| &nbsp;3.9 || style=\"text-align:right\" | 18.12\n|-\n| &nbsp;4.0 || style=\"text-align:right\" | 17.67\n|}\n|\n{| class=\"wikitable\"\n!''r''% !! ''T<sub>d</sub>''\n|-\n| &nbsp;4.1 || style=\"text-align:right\" | 17.25\n|-\n| &nbsp;4.2 || style=\"text-align:right\" | 16.85\n|-\n| &nbsp;4.3 || style=\"text-align:right\" | 16.46\n|-\n| &nbsp;4.4 || style=\"text-align:right\" | 16.10\n|-\n| &nbsp;4.5 || style=\"text-align:right\" | 15.75\n|-\n| &nbsp;4.6 || style=\"text-align:right\" | 15.41\n|-\n| &nbsp;4.7 || style=\"text-align:right\" | 15.09\n|-\n| &nbsp;4.8 || style=\"text-align:right\" | 14.78\n|-\n| &nbsp;4.9 || style=\"text-align:right\" | 14.49\n|-\n| &nbsp;5.0 || style=\"text-align:right\" | 14.21\n|}\n|\n{| class=\"wikitable\"\n!''r''% !! ''T<sub>d</sub>''\n|-\n| &nbsp;5.5 || style=\"text-align:right\" | 12.95\n|-\n| &nbsp;6.0 || style=\"text-align:right\" | 11.90\n|-\n| &nbsp;6.5 || style=\"text-align:right\" | 11.01\n|-\n| &nbsp;7.0 || style=\"text-align:right\" | 10.24\n|-\n| &nbsp;7.5 || style=\"text-align:right\" | 9.58\n|-\n| &nbsp;8.0 || style=\"text-align:right\" | 9.01\n|-\n| &nbsp;8.5 || style=\"text-align:right\" | 8.50\n|-\n| &nbsp;9.0 || style=\"text-align:right\" | 8.04\n|-\n| &nbsp;9.5 || style=\"text-align:right\" | 7.64\n|-\n| 10.0 || style=\"text-align:right\" | 7.27\n|}\n|\n{| class=\"wikitable\"\n!''r''% !! ''T<sub>d</sub>''\n|-\n| 11.0 || style=\"text-align:right\" | 6.64\n|-\n| 12.0 || style=\"text-align:right\" | 6.12\n|-\n| 13.0 || style=\"text-align:right\" | 5.67\n|-\n| 14.0 || style=\"text-align:right\" | 5.29\n|-\n| 15.0 || style=\"text-align:right\" | 4.96\n|-\n| 16.0 || style=\"text-align:right\" | 4.67\n|-\n| 17.0 || style=\"text-align:right\" | 4.41\n|-\n| 18.0 || style=\"text-align:right\" | 4.19\n|-\n| 19.0 || style=\"text-align:right\" | 3.98\n|-\n| 20.0 || style=\"text-align:right\" | 3.80\n|}\n|}\n\nFor example, with an annual growth rate of 4.8% the doubling time is 14.78 years, and a doubling time of 10 years corresponds to a growth rate between 7% and 7.5% (actually about 7.18%).\n\nWhen applied to the constant growth in consumption of a resource, the total amount consumed in one doubling period equals the total amount consumed in all previous periods.\nThis enabled US President Jimmy Carter to note in a speech in 1977 that in each of the previous two decades the world had used more oil than in all of previous history (The roughly exponential growth in world oil consumption between 1950 and 1970 had a doubling period of under a decade).\n\nGiven two measurements of a growing quantity, ''q''<sub>1</sub> at time ''t''<sub>1</sub> and ''q''<sub>2</sub> at time ''t''<sub>2</sub>, and assuming a constant growth rate, you can calculate the doubling time as\n\n:<math> T_{d} = (t_{2} - t_{1}) \\cdot \\frac{\\ln(2)}{\\ln(\\frac{q_{2}}{q_{1}})}.</math>\n\n==Where is it useful?==\nA constant relative growth rate means simply that the increase per unit time is proportional to the current quantity, i.e. the addition rate per unit amount is constant.  It naturally occurs when the existing material generates or is the main determinant of new material.  For example, population growth in virgin territory, or [[fractional-reserve banking]] creating inflation.  With unvarying growth the doubling calculation may be applied for many doubling periods or generations.\n\nIn practice eventually other constraints become important, exponential growth stops and the doubling time changes or becomes inapplicable. Limited food supply or other resources at high population densities will reduce growth, or needing a wheel-barrow full of notes to buy a loaf of bread will reduce the acceptance of paper money.  While using doubling times is convenient and simple, we should not apply the idea without considering factors which may affect future growth.  In the 1950s Canada's population growth rate was over 3% per year, so extrapolating the current growth rate of 0.9% for many decades (implied by the doubling time) is unjustified unless we have examined the underlying causes of the growth and determined they will not be changing significantly over that period.\n\n==Related concepts==\nThe equivalent concept to ''doubling time'' for a material undergoing a constant negative relative growth rate or [[exponential decay]] is the [[half-life]].\n\nThe equivalent concept in base-[[e (mathematical constant)|''e'']] is [[e-folding|''e''-folding]].\n\n{{wide image|doubling_time_vs_half_life.svg|640px|Graphs comparing doubling times and half lives of exponential growths (bold lines) and decay (faint lines), and their 70/''t'' and 72/''t'' approximations. In the [http://upload.wikimedia.org/wikipedia/commons/8/88/Doubling_time_vs_half_life.svg SVG version], hover over a graph to highlight it and its complement.}}\n\n== Cell culture doubling time ==\n\nCell doubling time can be calculated in the following way using growth rate (amount of doubling in one unit of time)\n\nGrowth rate:\n\n:<math>N(t) = N(0) e^{gr*t}</math>\nor\n:<math>gr = \\frac{\\ln\\left(N(t)/N(0)\\right)}{t}</math>\n\n* <math>N(t)</math> = the number of cells at time ''t''\n* <math>N(0)</math> = the number of cells at time ''0''\n* <math>gr</math> = growth rate\n* <math>t</math> = time (usually in hours)\n\nDoubling time:\n\n:<math> \\text{doubling time} = \\frac{\\ln(2)}{\\text{growth rate}}</math>\n\nThe following is the known doubling time for the following cells:\n{| class=\"wikitable\"\n!Cell types\n!Source\n!Doubling time\n|-\n|Mesenchymal Stem Cell\n|Mouse\n|21–23 hours<ref>{{Cite web|url = http://tools.lifetechnologies.com/content/sfs/manuals/GIBCO_Mouse_(C57)_MSCs.pdf|title = Life Technologies|date = |accessdate = |website = |publisher = |last = |first = }}</ref>\n|-\n|Cardiac/heart stem cell\n|Human\n|29 ± 10 hours<ref>{{Cite journal|url = http://www.pnas.org/content/104/35/14068.full|title = Human cardiac stem cells|last = |first = |date = |journal = |doi = |pmid = |access-date = }}</ref>\n|}\n\n==See also==\n* [[Albert Allen Bartlett]]\n* [[e-folding|''e''-folding]]\n* [[Exponential decay]]\n* [[Exponential growth]]\n* [[Half-life]]\n* [[Relative growth rate]]\n* [[Rule of 72]]\n\n==References==\n<references />\n\n== External links ==\n* [http://www.miniwebtool.com/doubling-time-calculator/ Doubling Time Calculator]\n* http://geography.about.com/od/populationgeography/a/populationgrow.htm\n\n{{Systems science}}\n{{Portal bar|Mathematics|Business and economics}}\n\n[[Category:Exponentials]]\n[[Category:Economic growth]]"
    },
    {
      "title": "List of representations of e",
      "url": "https://en.wikipedia.org/wiki/List_of_representations_of_e",
      "text": "{{DISPLAYTITLE:List of representations of {{mvar|e}}}}\n{{E (mathematical constant)}}\n{{refimprove|date=December 2007}}\nThe [[mathematical constant]] [[E (mathematical constant)|{{math|''e''}}]] can be represented in a variety of ways as a [[real number]].  Since {{math|''e''}} is an [[irrational number]] (see [[proof that e is irrational]]), it cannot be represented as the [[quotient]] of two [[integer]]s, but it can be represented as a [[continued fraction]].  Using [[calculus]], {{math|''e''}} may also be represented as an [[infinite series]], [[infinite product]], or other sort of [[limit of a sequence]].\n\n==As a continued fraction==\n\n[[Leonhard Euler|Euler]] proved that the number {{math|''e''}} is represented as the infinite [[simple continued fraction]]<ref>{{cite web|url=http://eulerarchive.maa.org/hedi/HEDI-2006-02.pdf|title=How Euler Did It: Who proved ''e'' is Irrational?|last=Sandifer|first=Ed|date=Feb 2006|publisher=MAA Online|accessdate=2017-04-23}}</ref> {{OEIS|id=A003417}}:\n\n:<math>e = [2; 1, 2, 1, 1, 4, 1, 1, 6, 1, 1, 8, 1, \\ldots, 1, 2n, 1, \\ldots]. </math>\n\nIts convergence can be tripled{{clarify|reason=By what measure?|date = April 2017}}{{cn|date = April 2017}} by allowing just one fractional number:\n\n:<math> e = [1; 1/2, 12, 5, 28, 9, 44, 13, 60, 17, \\ldots, 4(4n-1), 4n+1, \\ldots]. </math>\n\nHere are some infinite [[generalized continued fraction]] expansions of {{math|''e''}}. The second is generated from the first by a simple [[generalized continued fraction#The equivalence transformation|equivalence transformation]].\n\n:<math>\ne= 2+\\cfrac{1}{1+\\cfrac{1}{2+\\cfrac{2}{3+\\cfrac{3}{4+\\cfrac{4}{5+\\ddots}}}}} = 2+\\cfrac{2}{2+\\cfrac{3}{3+\\cfrac{4}{4+\\cfrac{5}{5+\\cfrac{6}{6+\\ddots\\,}}}}}\n</math>\n\n:<math>e = 2+\\cfrac{1}{1+\\cfrac{2}{5+\\cfrac{1}{10+\\cfrac{1}{14+\\cfrac{1}{18+\\ddots\\,}}}}} = 1+\\cfrac{2}{1+\\cfrac{1}{6+\\cfrac{1}{10+\\cfrac{1}{14+\\cfrac{1}{18+\\ddots\\,}}}}}</math>\n\nThis last, equivalent to [1; 0.5, 12, 5, 28, 9, ...], is a special case of a general formula for the [[exponential function]]:\n\n:<math>e^{x/y} = 1+\\cfrac{2x} {2y-x+\\cfrac{x^2} {6y+\\cfrac{x^2} {10y+\\cfrac{x^2} {14y+\\cfrac{x^2} {18y+\\ddots}}}}}</math>\n\n==As an infinite series==\nThe number {{math|''e''}} can be expressed as the sum of the following [[infinite series]]:\n\n:<math>e^x = \\sum_{k=0}^\\infty \\frac{x^k}{k!} </math> for any real number ''x''.\n\nIn the [[special case]] where ''x''&nbsp;=&nbsp;1 or &minus;1, we have:\n\n:<math>e = \\sum_{k=0}^\\infty \\frac{1}{k!}</math>,<ref>{{cite web|url=http://oakroadsystems.com/math/loglaws.htm|title=It's the Law Too — the Laws of Logarithms|last=Brown|first=Stan|date=2006-08-27|publisher=Oak Road Systems|accessdate=2008-08-14|deadurl=yes|archiveurl=https://web.archive.org/web/20080813175402/http://oakroadsystems.com/math/loglaws.htm|archivedate=2008-08-13|df=}}</ref> and\n\n:<math>e^{-1} = \\sum_{k=0}^\\infty \\frac{(-1)^k}{k!}</math>\n\nOther series include the following:\n\n:<math>e = \\left [ \\sum_{k=0}^\\infty \\frac{1-2k}{(2k)!} \\right ]^{-1}</math> <ref>Formulas 2–7: [[Harlan J. Brothers|H. J. Brothers]],  [http://www.brotherstechnology.com/docs/Improving_Convergence_(CMJ-2004-01).pdf Improving the convergence of Newton's series approximation for ''e''],  ''The College Mathematics Journal'', Vol. 35, No. 1, (2004),  pp. 34–39.</ref>\n\n:<math>e =  \\frac{1}{2} \\sum_{k=0}^\\infty \\frac{k+1}{k!}</math>\n\n:<math>e =  2 \\sum_{k=0}^\\infty \\frac{k+1}{(2k+1)!}</math>\n\n:<math>e =   \\sum_{k=0}^\\infty \\frac{3-4k^2}{(2k+1)!}</math>\n\n:<math>e =   \\sum_{k=0}^\\infty \\frac{(3k)^2+1}{(3k)!} = \\sum_{k=0}^\\infty \\frac{(3k+1)^2+1}{(3k+1)!} = \\sum_{k=0}^\\infty \\frac{(3k+2)^2+1}{(3k+2)!}</math>\n\n:<math>e =   \\left [ \\sum_{k=0}^\\infty \\frac{4k+3}{2^{2k+1}\\,(2k+1)!} \\right ]^2</math>\n\n:<math>e =  \\sum_{k=0}^\\infty \\frac{k^n}{B_n(k!)}</math> where <math>B_n</math> is the {{mvar|n}}th [[Bell number]].\n\nConsideration of how to put upper bounds on ''e'' leads to this descending series:\n:<math>e = 3 + \\sum_{k=2}^\\infty \\frac{-1}{k! (k-1) k} = 3 - \\frac{1}{4} - \\frac{1}{36} - \\frac{1}{288} - \\frac{1}{2400} - \\frac{1}{21600} - \\frac{1}{211680} - \\frac{1}{2257920} - \\cdots </math>\nwhich gives at least one correct (or rounded up) digit per term. That is, if 1 ≤ ''n'', then\n:<math>e < 3 + \\sum_{k=2}^n \\frac{-1}{k! (k-1) k} < e + 0.6 \\cdot 10^{1-n} \\,.</math>\nMore generally, if ''x'' is not in {2, 3, 4, 5, ...}, then\n:<math>e^x = \\frac{2+x}{2-x} + \\sum_{k=2}^\\infty \\frac{- x^{k+1}}{k! (k-x) (k+1-x)} \\,.</math>\n\n==As an infinite product==\nThe number {{math|''e''}} is also given by several [[infinite product]] forms including [[Nick Pippenger|Pippenger]]'s product\n\n:<math> e= 2 \\left ( \\frac{2}{1} \\right )^{1/2} \\left ( \\frac{2}{3}\\; \\frac{4}{3} \\right )^{1/4} \\left ( \\frac{4}{5}\\; \\frac{6}{5}\\; \\frac{6}{7}\\; \\frac{8}{7} \\right )^{1/8} \\cdots </math>\n\nand Guillera's product <ref>J. Sondow, [https://arxiv.org/abs/math/0401406 A faster product for pi and a new integral for ln pi/2,] ''Amer. Math. Monthly'' 112 (2005) 729–734.</ref><ref>J. Guillera and J. Sondow, [https://arxiv.org/abs/math.NT/0506319 Double integrals and infinite products for some classical constants via analytic continuations of Lerch's transcendent,]''Ramanujan Journal'' 16 (2008), 247–270.</ref>\n:<math> e = \\left ( \\frac{2}{1} \\right )^{1/1} \\left (\\frac{2^2}{1 \\cdot 3} \\right )^{1/2} \\left (\\frac{2^3 \\cdot 4}{1 \\cdot 3^3} \\right )^{1/3} \n\\left (\\frac{2^4 \\cdot 4^4}{1 \\cdot 3^6 \\cdot 5} \\right )^{1/4}  \\cdots ,</math>\nwhere the ''n''th factor is the ''n''th root of the product\n:<math>\\prod_{k=0}^n (k+1)^{(-1)^{k+1}{n \\choose k}},</math>\n\nas well as the infinite product\n\n:<math> e = \\frac{2\\cdot 2^{(\\ln(2)-1)^2} \\cdots}{2^{\\ln(2)-1}\\cdot 2^{(\\ln(2)-1)^3}\\cdots }.</math>\n\nMore generally, if 1 < ''B'' < ''e''<sup>2</sup> (which includes ''B'' = 2, 3, 4, 5, 6, or 7), then\n\n:<math> e = \\frac{B\\cdot B^{(\\ln(B)-1)^2} \\cdots}{B^{\\ln(B)-1}\\cdot B^{(\\ln(B)-1)^3}\\cdots }.</math>\n\n==As the limit of a sequence==\nThe number {{math|''e''}} is equal to the [[limit of a sequence|limit]] of several [[infinite sequences]]:\n\n:<math> e= \\lim_{n \\to \\infty} n\\cdot\\left ( \\frac{\\sqrt{2 \\pi n}}{n!} \\right )^{1/n}   </math> and\n\n:<math> e=\\lim_{n \\to \\infty} \\frac{n}{\\sqrt[n]{n!}} </math> (both by [[Stirling's formula]]).\n\nThe symmetric limit,<ref>[[Harlan J. Brothers|H. J. Brothers]] and J. A. Knox,  [http://www.brotherstechnology.com/docs/Closed-Form_Approximations_(MI-1998-12).pdf New closed-form approximations to the Logarithmic Constant ''e'',] ''The Mathematical Intelligencer'', Vol. 20, No. 4, (1998), pp. 25–29.</ref>\n\n:<math>e=\\lim_{n \\to \\infty} \\left [ \\frac{(n+1)^{n+1}}{n^n}- \\frac{n^n}{(n-1)^{n-1}} \\right ]</math>\n\nmay be obtained by manipulation of the basic limit definition of {{math|''e''}}.\n\nThe next two definitions are direct corollaries of the [[prime number theorem]]<ref>S. M. Ruiz 1997</ref>\n\n:<math>e= \\lim_{n \\to \\infty}(p_n \\#)^{1/p_n} </math>\n\nwhere <math> p_n </math> is the ''n''th [[prime number|prime]] and <math> p_n \\# </math> is the [[primorial]] of the ''n''th prime.\n\n:<math>e= \\lim_{n \\to \\infty}n^{\\pi(n)/n} </math>\n\nwhere <math> \\pi(n) </math> is the [[prime counting function]].\n\nAlso:\n:<math>e^x= \\lim_{n \\to \\infty}\\left (1+ \\frac{x}{n} \\right )^n.</math>\n\nIn the special case that <math>x = 1</math>, the result is the famous statement:\n\n:<math>e= \\lim_{n \\to \\infty}\\left (1+ \\frac{1}{n} \\right )^n.</math>\n\n== In trigonometry ==\nTrigonometrically, {{math|''e''}} can be written in terms of the sum of two [[hyperbolic function]]s,\n\n: <math>e^x = \\sinh(x) + \\cosh(x) ,</math>\n\nat {{math|1=''x'' = 1}}.\n\n==Notes==\n{{reflist}}\n\n{{DEFAULTSORT:e, Representations}}\n[[Category:Transcendental numbers]]\n[[Category:Mathematical constants]]\n[[Category:Exponentials]]\n[[Category:Logarithms]]\n[[Category:E (mathematical constant)]]\n[[Category:Real transcendental numbers]]"
    },
    {
      "title": "Euler's identity",
      "url": "https://en.wikipedia.org/wiki/Euler%27s_identity",
      "text": "{{Other uses|List of topics named after Leonhard Euler#Euler's identities}}\n{{E (mathematical constant)}}\nIn mathematics, '''Euler's identity'''{{#tag:ref |The term \"Euler's identity\" (or \"Euler identity\") is also used elsewhere to refer to other concepts, including the related general formula {{math|e<sup>''ix''</sup> {{=}} cos ''x'' + ''i'' sin ''x''}},<ref>Dunham, 1999, [https://books.google.com/books?id=uKOVNvGOkhQC&pg=PR24 p. xxiv].</ref> and the [[Riemann zeta function#Euler product formula|Euler product formula]].<ref name=EOM>{{cite encyclopedia |last=Stepanov |first=S. A. |encyclopedia=[[Encyclopedia of Mathematics]] |title=Euler identity |publisher= |url=http://www.encyclopediaofmath.org/index.php?title=Euler_identity&oldid=11612 |date=7 February 2011 |accessdate=7 September 2018}}</ref> |group=n}} (also known as [[List of things named after Leonhard Euler|Euler's equation]]) is the [[Equality (mathematics)|equality]]\n\n:<math>e^{i \\pi} + 1 = 0</math>\n\nwhere\n:{{mvar|e}} is [[E (mathematical constant)|Euler's number]], the base of [[natural logarithm]]s,\n:{{mvar|i}} is the [[imaginary unit]], which satisfies {{math|''i''<sup>2</sup> {{=}} −1}}, and\n:{{mvar|&pi;}} is [[pi]], the [[ratio]] of the circumference of a [[circle]] to its diameter.\nEuler's identity is named after the Swiss mathematician [[Leonhard Euler]]. It is considered to be an exemplar of [[mathematical beauty]] as it shows a profound connection between the most fundamental numbers in mathematics.\n\n==Mathematical beauty==\nEuler's identity is often cited as an example of deep [[mathematical beauty]].<ref name=Gallagher2014>{{cite news |last=Gallagher |first=James |title=Mathematics: Why the brain sees maths as beauty |url=https://www.bbc.co.uk/news/science-environment-26151062 |accessdate=26 December 2017 |work=[[BBC News Online]] |date=13 February 2014}}</ref> Three of the basic [[arithmetic]] operations occur exactly once each: [[addition]], [[multiplication]], and [[exponentiation]]. The identity also links five fundamental [[mathematical constant]]s:<ref>Paulos, 1992, p. 117.</ref>\n* The [[0 (number)|number 0]].\n* The [[1 (number)|number 1]].\n* The [[pi|number {{mvar|&pi;}}]] ({{mvar|&pi;}} = 3.141...).\n* The [[e (mathematical constant)|number {{math|''e''}}]] ({{math|''e''}} = 2.718...), which occurs widely in [[mathematical analysis]].\n* The [[imaginary unit|number {{math|''i''}}]], the imaginary unit of the [[complex number]]s.\n\nFurthermore, the equation is given in the form of an expression set equal to zero, which is common practice in several areas of mathematics.\n\n[[Stanford University]] mathematics professor [[Keith Devlin]] has said, \"like a Shakespearean [[sonnet]] that captures the very essence of love, or a painting that brings out the beauty of the human form that is far more than just skin deep, Euler's equation reaches down into the very depths of existence\".<ref>Nahin, 2006, [https://books.google.com/books?id=GvSg5HQ7WPcC&pg=PA1 p. 1].</ref> And [[Paul Nahin]], a professor emeritus at the [[University of New Hampshire]], who has written a book dedicated to [[Euler's formula]] and its applications in [[Fourier analysis]], describes Euler's identity as being \"of exquisite beauty\".<ref>Nahin, 2006, p. xxxii.</ref>\n\nMathematics writer [[Constance Reid]] has opined that Euler's identity is \"the most famous formula in all mathematics\".<ref>Reid, chapter ''e''.</ref> And [[Benjamin Peirce]], a 19th-century American [[philosopher]], mathematician, and professor at [[Harvard University]], after proving Euler's identity during a lecture, stated that the identity \"is absolutely paradoxical; we cannot understand it, and we don't know what it means, but we have proved it, and therefore we know it must be the truth\".<ref>Maor, [https://books.google.com/books?id=eIsyLD_bDKkC&pg=PA160 p. 160], and Kasner & Newman, [https://books.google.com/books?id=Ad8hAx-6m9oC&pg=PA103 p. 103–104].</ref>\n\nA poll of readers conducted by ''[[The Mathematical Intelligencer]]'' in 1990 named Euler's identity as the \"most beautiful theorem in mathematics\".<ref>Wells, 1990.</ref> In another poll of readers that was conducted by ''[[Physics World]]'' in 2004, Euler's identity tied with [[Maxwell's equations]] (of [[electromagnetism]]) as the \"greatest equation ever\".<ref>Crease, 2004.</ref>\n\nA study of the brains of sixteen mathematicians found that the \"emotional brain\" (specifically, the medial [[orbitofrontal cortex]], which lights up for beautiful music, poetry, pictures, etc.) lit up more consistently for Euler's identity than for any other formula.<ref>Zeki et al., 2014.</ref>\n\nAt least two books in [[popular mathematics]] have been published about Euler's identity. One is ''A Most Elegant Equation: Euler's formula and the beauty of mathematics'', by David Stipp (2017). Another is ''Euler's Pioneering Equation: The most beautiful theorem in mathematics'', by [[Robin Wilson (mathematician)|Robin Wilson]] (2018).\n\n==Explanations==\n===Imaginary exponents===\n{{main|Euler's formula}}\n{{See also|Exponentiation#Complex_exponents_with_a_positive_real_base|l1=Complex exponents with a positive real base}}\n[[File:ExpIPi.gif|thumb|right|In this animation {{mvar|N}} takes various increasing values from 1 to 100. The computation of {{math|(1 + {{sfrac|''iπ''|''N''}})<sup>''N''</sup>}} is displayed as the combined effect of {{mvar|N}} repeated multiplications in the [[complex plane]], with the final point being the actual value of {{math|(1 + {{sfrac|''iπ''|''N''}})<sup>''N''</sup>}}. It can be seen that as {{mvar|N}} gets larger {{math|(1 + {{sfrac|''iπ''|''N''}})<sup>''N''</sup>}} approaches a limit of −1.]]\nFundamentally, Euler's identity asserts that <math>e^{i\\pi}</math> is equal to -1. The expression <math>e^{i\\pi}</math> is a special case of the expression <math>e^z</math>, where {{math|''z''}} is any complex number. In general, <math>e^z</math> is defined for complex {{math|''z''}} by extending one of the [[characterizations of the exponential function|definitions of the exponential function]] from real exponents to complex exponents. For example, one common definition is:\n\n:<math>e^z = \\lim_{n\\to\\infty} \\left(1+\\frac z n \\right)^n.</math>\n\nEuler's identity therefore states that the limit, as {{math|''n''}} approaches infinity, of <math>(1 + i\\pi/n)^n</math> is equal to -1. This limit is illustrated in the animation to the right.\n\n[[File:Euler's formula.svg|thumb|right|Euler's formula for a general angle]]\nEuler's identity is a [[special case]] of [[Euler's formula]], which states that for any [[real number]] {{math|''x''}},\n\n: <math>e^{ix} = \\cos x + i\\sin x</math>\n\nwhere the inputs of the [[trigonometry|trigonometric functions]] sine and cosine are given in [[radian]]s.\n\nIn particular, when {{math|''x'' {{=}} ''π''}},\n\n: <math>e^{i \\pi} = \\cos \\pi + i\\sin \\pi.</math>\n\nSince\n\n:<math>\\cos \\pi = -1</math>\n\nand\n\n:<math>\\sin \\pi = 0,</math>\n\nit follows that\n\n: <math>e^{i \\pi} = -1 + 0 i,</math>\n\nwhich yields Euler's identity:\n\n: <math>e^{i \\pi} +1 = 0.</math>\n\n===Geometric interpretation===\nAny complex number <math>z = x + iy</math> can be represented by the point <math>(x, y)</math> on the [[complex plane]]. This point can also be represented in [[Complex_number#Polar_complex_plane|polar coordinates]] as <math>(r, \\theta)</math>, where ''r'' is the absolute value of ''z'' (distance from the origin), and <math>\\theta</math> is the argument of ''z'' (angle counterclockwise from the positive ''x''-axis). By the definitions of sine and cosine, this point has cartesian coordinates of <math>(r \\cos \\theta, r \\sin \\theta)</math>, implying that <math>z = r(\\cos \\theta + i \\sin \\theta)</math>. According to Euler's formula, this is equivalent to saying <math>z = r e^{i\\theta}</math>.\n\nEuler's identity says that <math>-1 = e^{i\\pi}</math>. Since <math>e^{i\\pi}</math> is <math>r e^{i\\theta}</math> for ''r'' = 1 and <math>\\theta = \\pi</math>, this can be interpreted as a fact about the number -1 on the complex plane: its distance from the origin is 1, and its angle from the positive ''x''-axis is <math>\\pi</math> radians.\n\nAdditionally, when any complex number ''z'' is [[Complex number#Multiplication and division in polar form|multiplied]] by <math>e^{i\\theta}</math>, it has the effect of rotating ''z'' counterclockwise by an angle of <math>\\theta</math> on the complex plane. Since multiplication by -1 reflects a point across the origin, Euler's identity can be interpreted as saying that rotating any point <math>\\pi</math> radians around the origin has the same effect as reflecting the point across the origin.\n\n==Generalizations==\nEuler's identity is also a special case of the more general identity that the {{mvar|n}}th [[roots of unity]], for {{math|''n'' > 1}}, add up to 0:\n\n:<math>\\sum_{k=0}^{n-1} e^{2 \\pi i \\frac{k}{n}} = 0 .</math>\n\nEuler's identity is the case where {{math|''n'' {{=}} 2}}.\n\nIn another field of mathematics, by using [[quaternion]] exponentiation, one can show that a similar identity also applies to quaternions. Let {{math|{{mset|''i'', ''j'', ''k''}}}} be the basis elements; then,\n\n:<math>e^{\\frac{1}{\\sqrt 3}(i \\pm j \\pm k)\\pi} + 1 = 0. </math>\n\nIn general, given [[real numbers|real]] {{math|''a''<sub>1</sub>}}, {{math|''a''<sub>2</sub>}}, and {{math|''a''<sub>3</sub>}} such that {{math|''a''<sub>1</sub><sup>2</sup> + ''a''<sub>2</sub><sup>2</sup> + ''a''<sub>3</sub><sup>2</sup> {{=}} 1}}, then,\n\n:<math>e^{\\left(a_1i+a_2j+a_3k\\right)\\pi} + 1 = 0. </math>\n\nFor [[octonions]], with real {{math|''a''<sub>''n''</sub>}} such that {{math|''a''<sub>1</sub><sup>2</sup> + ''a''<sub>2</sub><sup>2</sup> + ... + ''a''<sub>7</sub><sup>2</sup> {{=}} 1}}, and with the octonion basis elements {{math|{{mset|''i''<sub>1</sub>, ''i''<sub>2</sub>, ..., ''i''<sub>7</sub>}}}},\n\n:<math>e^{\\left(a_1i_1+a_2i_2+\\dots+a_7i_7\\right)\\pi} + 1 = 0. </math>\n\n==History==\nIt has been claimed that Euler's identity appears in his monumental work of mathematical analysis published in 1748, ''[[Introductio in analysin infinitorum]]''.<ref>Conway & Guy, p. 254–255.</ref> However, it is questionable whether this particular concept can be attributed to Euler himself, as he may never have expressed it.<ref name=Sandifer2007>Sandifer, p. 4.</ref> Moreover, while Euler did write in the ''Introductio'' about what we today call [[Euler's formula]],<ref>Euler, p. 147.</ref> which relates {{mvar|e}} with cosine and sine terms in the field of complex numbers, the English mathematician [[Roger Cotes]] (who died in 1716, when Euler was only 9 years old) also knew of this formula and Euler may have acquired the knowledge through his Swiss compatriot [[Johann Bernoulli]].<ref name=Sandifer2007/>\n\n[[Robin Wilson (mathematician)|Robin Wilson]] states the following.<ref>Wilson, p. 151-152.</ref>\n{{quote|text= \nWe've seen how it [Euler's identity] can easily be deduced from results of Johann Bernoulli and Roger Cotes, but that neither of them seems to have done so. Even Euler does not seem to have written it down explicitly – and certainly it doesn't appear in any of his publications – though he must surely have realized that it follows immediately from his identity [i.e. [[Euler's formula]]], {{nowrap|''e<sup>ix</sup>'' {{=}} cos ''x'' + ''i'' sin ''x''}}. Moreover, it seems to be unknown who first stated the result explicitly&hellip;.\n}}\n\n== See also ==\n*[[De Moivre's formula]] \n*[[Exponential function]]\n*[[Gelfond's constant]]\n\n==Notes==\n{{reflist|group=n}}\n\n==References==\n{{Reflist|25em}}\n\n==Sources==\n* [[John Horton Conway|Conway, John H.]], and [[Richard K. Guy|Guy, Richard K.]] (1996), ''[https://books.google.com/books?id=0--3rcO7dMYC&pg=PA254 The Book of Numbers]'', Springer {{ISBN|978-0-387-97993-9}}\n* [[Robert P. Crease|Crease, Robert P.]] (10&nbsp;May 2004), \"[http://physicsworld.com/cws/article/print/2004/may/10/the-greatest-equations-ever The greatest equations ever]\", ''[[Physics World]]'' [registration required]\n* [[William Dunham (mathematician)|Dunham, William]] (1999), ''Euler: The Master of Us All'', [[Mathematical Association of America]] {{ISBN|978-0-88385-328-3}}\n* Euler, Leonhard (1922), ''[http://gallica.bnf.fr/ark:/12148/bpt6k69587.image.r=%22has+celeberrimas+formulas%22.f169.langEN Leonhardi Euleri opera omnia. 1, Opera mathematica. Volumen VIII, Leonhardi Euleri introductio in analysin infinitorum. Tomus primus]'', Leipzig: B. G. Teubneri\n* [[Edward Kasner|Kasner, E.]], and [[James R. Newman|Newman, J.]] (1940), ''[[Mathematics and the Imagination]]'', [[Simon & Schuster]]\n* [[Eli Maor|Maor, Eli]] (1998), ''{{mvar|e}}: The Story of a number'', [[Princeton University Press]] {{ISBN|0-691-05854-7}}\n* Nahin, Paul J. (2006), ''Dr. Euler's Fabulous Formula: Cures Many Mathematical Ills'', [[Princeton University Press]] {{ISBN|978-0-691-11822-2}}\n* [[John Allen Paulos|Paulos, John Allen]] (1992), ''Beyond Numeracy: An Uncommon Dictionary of Mathematics'', [[Penguin Books]] {{ISBN|0-14-014574-5}}\n* Reid, Constance (various editions), ''From Zero to Infinity'', [[Mathematical Association of America]]\n* Sandifer, C. Edward (2007), ''[https://books.google.com/books?id=sohHs7ExOsYC&pg=PA4 Euler's Greatest Hits]'', [[Mathematical Association of America]] {{ISBN|978-0-88385-563-8}}\n*{{citation |title= A Most Elegant Equation: Euler's formula and the beauty of mathematics |first= David |last= Stipp |year=2017 |publisher= [[Basic Books]]}}\n*Wells, David (1990), \"Are these the most beautiful?\", ''[[The Mathematical Intelligencer]]'', 12: 37–41, {{doi|10.1007/BF03024015}}\n*{{citation |first= Robin |last= Wilson |author-link= Robin Wilson (mathematician) |title= Euler's Pioneering Equation: The most beautiful theorem in mathematics |publisher= [[Oxford University Press]] |year= 2018}}\n*{{Citation |last1= Zeki |first1= S. |last2= Romaya |first2= J. P. |last3= Benincasa |first3= D. M. T. |last4= Atiyah |first4= M. F. |authorlink1= Semir Zeki |authorlink4= Michael Atiyah |title= The experience of mathematical beauty and its neural correlates |journal= Frontiers in Human Neuroscience |volume= 8 | year= 2014 |doi= 10.3389/fnhum.2014.00068|pmc= 3923150 }}\n\n==External links==\n{{Wikiquote|Euler's identity}}\n* [https://www.youtube.com/watch?v=UcGDNUDQCc4 Complete derivation of Euler's identity]\n* [http://betterexplained.com/articles/intuitive-understanding-of-eulers-formula/ Intuitive understanding of Euler's formula]\n\n{{DEFAULTSORT:Euler's identity}}\n[[Category:Exponentials]]\n[[Category:Mathematical identities]]\n[[Category:E (mathematical constant)]]\n[[Category:Theorems in complex analysis]]\n[[Category:Leonhard Euler]]\n\n[[de:Eulersche Formel#Eulersche Identit.C3.A4t]]\n[[pl:Wzór Eulera#Tożsamość Eulera]]"
    },
    {
      "title": "Exponential decay",
      "url": "https://en.wikipedia.org/wiki/Exponential_decay",
      "text": "[[Image:Plot-exponential-decay.svg|thumb|A quantity undergoing exponential decay. Larger decay constants make the quantity vanish much more rapidly. This plot shows decay for decay constant (λ) of 25, 5, 1, 1/5, and 1/25 for x from 0 to 5.]]\n\nA quantity is subject to '''exponential decay''' if it decreases at a rate [[Proportionality (mathematics)|proportional]] to its current value.  Symbolically, this process can be expressed by the following [[differential equation]], where ''N'' is the quantity and λ (lambda) is a positive rate called the '''exponential decay constant''':\n\n:<math>\\frac{dN}{dt} = -\\lambda N.</math>\n\nThe solution to this equation (see [[#Solution_of_the_differential_equation|derivation]] below) is:\n\n:<math>N(t) = N_0 e^{-\\lambda t}, </math>\n\nwhere ''N''(''t'') is the quantity at time ''t'', and ''N''<sub>0</sub> = ''N''(0) is the initial quantity, i.e. the quantity at time ''t'' = 0.\n\n== Measuring rates of decay==\n\n=== Mean lifetime ===\nIf the decaying quantity, ''N''(''t''), is the number of discrete elements in a certain [[set (mathematics)|set]], it is possible to compute the average length of time that an element remains in the set.  This is called the '''mean lifetime''' (or simply the '''lifetime'''), where the '''exponential [[time constant]]''', <math>\\tau</math>, relates to the decay rate, &lambda;, in the following way:\n:<math>\\tau = \\frac{1}{\\lambda}.</math>\n\nThe mean lifetime can be looked at as a \"scaling time\", because the exponential decay equation can be written in terms of the mean lifetime, <math>\\tau</math>, instead of the decay constant, &lambda;:\n:<math>N(t) = N_0 e^{-t/\\tau}, </math>\nand that <math>\\tau</math> is the time at which the population of the assembly is reduced to [[e (mathematical constant)|1/''e'' ≈ 0.367879441]] times its initial value.\n\nFor example, if the initial population of the assembly, ''N''(0), is 1000, then the population at time <math>\\tau</math>, <math>N(\\tau)</math>, is 368.\n\nA very similar equation will be seen below, which arises when the base of the exponential is chosen to be 2, rather than ''e''. In that case the scaling time is the \"half-life\".\n\n===Half-life===\n{{main|Half-life}}\n\nA more intuitive characteristic of exponential decay for many people is the time required for the decaying quantity to fall to one half of its initial value.  This time is called the ''[[half-life]]'', and often denoted by the symbol ''t''<sub>1/2</sub>.  The half-life can be written in terms of the decay constant, or the mean lifetime, as:\n\n:<math>t_{1/2} = \\frac{\\ln (2)}{\\lambda} = \\tau \\ln (2).</math>\n\nWhen this expression is inserted for <math>\\tau</math> in the exponential equation above, and ln&nbsp;2 is absorbed into the base, this equation becomes:\n\n:<math>N(t) = N_0 2^{-t/t_{1/2}}. \\,</math>\n\nThus, the amount of material left is 2<sup>−1</sup> =&nbsp;1/2 raised to the (whole or fractional) number of half-lives that have passed. Thus, after 3 half-lives there will be 1/2<sup>3</sup>&nbsp;=&nbsp;1/8 of the original material left.\n\nTherefore, the mean lifetime <math>\\tau</math> is equal to the half-life divided by the natural log of 2, or:\n\n: <math>\\tau = \\frac{t_{1/2}}{\\ln (2)} \\approx 1.44 \\cdot t_{1/2}.</math>\n\nE.g. [[polonium]]-210 has a half-life of 138 days, and a mean lifetime of 200 days.\n\n== Solution of the differential equation ==\n\nThe equation that describes exponential decay is\n:<math>\\frac{dN}{dt} = -\\lambda N</math>\nor, by rearranging (applying the technique called [[separation of variables]]),\n:<math>\\frac{dN}{N} = -\\lambda dt.</math>\n\nIntegrating, we have\n:<math>\\ln N = -\\lambda t + C \\,</math>\nwhere C is the [[constant of integration]], and hence\n:<math>N(t) = e^C e^{-\\lambda t} = N_0 e^{-\\lambda t} \\,</math>\nwhere the final substitution, ''N''<sub>0</sub> = ''e''<sup>''C''</sup>, is obtained by evaluating the equation at ''t'' = 0, as ''N''<sub>0</sub> is defined as being the quantity at ''t'' = 0.\n\nThis is the form of the equation that is most commonly used to describe exponential decay.  Any one of decay constant, mean lifetime, or half-life is sufficient to characterise the decay.  The notation λ for the decay constant is a remnant of the usual notation for an [[eigenvalue]].  In this case, λ is the eigenvalue of the [[additive inverse|negative]] of the [[differential operator]] with ''N''(''t'') as the corresponding [[eigenfunction]]. The units of the decay constant are s<sup>−1</sup>{{Citation needed|date=November 2016}}.\n\n=== Derivation of the mean lifetime ===\nGiven an assembly of elements, the number of which decreases ultimately to zero, the '''mean lifetime''', <math>\\tau</math>, (also called simply the '''lifetime''') is the [[expected value]] of the amount of time before an object is removed from the assembly.  Specifically, if the ''individual lifetime'' of an element of the assembly is the time elapsed between some reference time and the removal of that element from the assembly, the mean lifetime is the [[arithmetic mean]] of the individual lifetimes.\n\nStarting from the population formula\n\n:<math>N = N_0 e^{-\\lambda t}, \\,</math>\n\nfirst let ''c'' be the normalizing factor to convert to a [[probability density function]]:\n\n:<math>1 = \\int_0^\\infty c \\cdot N_0 e^{-\\lambda t}\\, dt = c \\cdot \\frac{N_0}{\\lambda}</math>\n\nor, on rearranging,\n\n:<math>c = \\frac{\\lambda}{N_0}.</math>\n\nExponential decay is a [[scalar multiplication|scalar multiple]] of the [[exponential distribution]] (i.e. the individual lifetime of each object is exponentially distributed), which has a [[Exponential distribution#Properties|well-known expected value]].  We can compute it here using [[integration by parts]].\n\n:<math>\\tau = \\langle t \\rangle = \\int_0^\\infty t \\cdot c \\cdot N_0 e^{-\\lambda t}\\, dt = \\int_0^\\infty \\lambda t e^{-\\lambda t}\\, dt = \\frac{1}{\\lambda}.</math>\n\n=== Decay by two or more processes ===<!-- This section is linked from [[Half-life]] -->\n{{see also|Branching fraction}}\nA quantity may decay via two or more different processes simultaneously. In general, these processes (often called \"decay modes\", \"decay channels\", \"decay routes\" etc.) have different probabilities of occurring, and thus occur at different rates with different half-lives, in parallel. The total decay rate of the quantity&nbsp;''N'' is given by the ''sum'' of the decay routes; thus, in the case of two processes:\n\n:<math>-\\frac{dN(t)}{dt} = N\\lambda _1 + N\\lambda _2 = (\\lambda _1 + \\lambda _2)N.</math>\n\nThe solution to this equation is given in the previous section, where the sum of <math>\\lambda _1 + \\lambda _2\\,</math> is treated as a new total decay constant <math>\\lambda _c</math>.\n\n:<math>N(t) = N_0 e^{-(\\lambda _1 + \\lambda _2) t} = N_0 e^{-(\\lambda _c) t}.</math>\n\n'''Partial mean life''' associated with individual processes is by definition the [[multiplicative inverse]] of corresponding partial decay constant: <math>\\tau = 1/\\lambda</math>. A combined <math>\\tau_c</math> can be given in terms of <math>\\lambda</math>s:\n\n:<math>\\frac{1}{\\tau_c} = \\lambda_c = \\lambda_1 + \\lambda_2 = \\frac{1}{\\tau_1} + \\frac{1}{\\tau_2}</math>\n\n:<math>\\tau_c = \\frac{\\tau_1 \\tau_2}{\\tau_1 + \\tau_2}. </math>\n\nSince half-lives differ from mean life <math>\\tau</math> by a constant factor, the same equation holds in terms of the two corresponding half-lives:\n\n:<math>T_{1/2} = \\frac{t_1 t_2}{t_1 + t_2} </math>\n\nwhere <math>T _{1/2}</math> is the combined or total half-life for the process, <math>t_1</math> and <math>t_2</math> are so-named '''partial half-lives''' of corresponding processes. Terms \"partial half-life\" and \"partial mean life\" denote quantities derived from a decay constant as if the given decay mode were the only decay mode for the quantity. The term \"partial half-life\" is misleading, because it cannot be measured as a time interval for which a certain quantity is [[one half|halved]].\n\nIn terms of separate decay constants, the total half-life <math>T _{1/2}</math> can be shown to be\n\n:<math>T_{1/2} = \\frac{\\ln 2}{\\lambda _c} = \\frac{\\ln 2}{\\lambda _1 + \\lambda _2}.</math>\n\nFor a decay by three simultaneous exponential processes the total half-life can be computed as above:\n\n:<math>T_{1/2} = \\frac{\\ln 2}{\\lambda _c} = \\frac{\\ln 2}{\\lambda_1 + \\lambda_2 + \\lambda_3} = \\frac{t_1 t_2 t_3}{(t_1 t_2) + (t_1 t_3) + (t_2 t_3)}.</math>\n\n== Applications and examples ==\n\nExponential decay occurs in a wide variety of situations.  Most of these fall into the domain of the [[natural science]]s.\n\nMany decay processes that are often treated as exponential, are really only exponential so long as the sample is large and the [[law of large numbers]] holds.  For small samples, a more general analysis is necessary, accounting for a [[Poisson process]].\n\n=== Natural sciences ===<!-- This section is linked from [[Methicillin-resistant Staphylococcus aureus]] -->\n* '''[[Chemical reactions]]:''' The [[reaction rate|rate]]s of certain types of [[chemical reaction]]s depend on the concentration of one or another [[reactant]]. Reactions whose rate depends only on the concentration of one reactant (known as [[Rate equation#First-order reactions|first-order reactions]]) consequently follow exponential decay. For instance, many [[enzyme]]-[[catalysis|catalyzed]] reactions behave this way.\n* '''[[Electrostatics]]:''' The [[electric charge]] (or, equivalently, the [[electric potential|potential]]) contained in a [[capacitor]] (capacitance ''C'') changes exponentially, if the capacitor experiences a constant [[External electric load|external load]] (resistance ''R'').  The exponential time-constant τ for the process is ''R'' ''C'', and the half-life is therefore ''R'' ''C'' ln2. This applies to both charging and discharging, i.e. a capacitor charges or discharges according to the same law. The same equations can be applied to the current in an inductor. (Furthermore, the particular case of a capacitor or inductor changing through several [[Series and parallel circuits#Parallel circuits|parallel]] [[resistor]]s makes an interesting example of multiple decay processes, with each resistor representing a separate process.  In fact, the expression for the [[resistor#Series and parallel circuits|equivalent resistance]] of two resistors in parallel mirrors the equation for the half-life with two decay processes.)\n* '''[[Geophysics]]:''' [[Atmospheric pressure]] decreases approximately exponentially with increasing height above sea level, at a rate of about 12% per 1000m.{{citation needed|date=November 2017}}\n* '''[[Heat transfer]]:''' If an object at one [[temperature]] is exposed to a medium of another temperature, the temperature difference between the object and the medium follows exponential decay (in the limit of slow processes; equivalent to \"good\" heat conduction inside the object, so that its temperature remains relatively uniform through its volume).  See also [[Newton's law of cooling]].\n* '''[[Luminescence]]:''' After excitation, the emission intensity – which is proportional to the number of excited atoms or molecules – of a luminescent material decays exponentially. Depending on the number of mechanisms involved, the decay can be mono- or multi-exponential.\n* '''[[Pharmacology]] and [[toxicology]]:''' It is found that many administered substances are distributed and [[metabolism|metabolize]]d (see ''[[clearance (medicine)|clearance]]'') according to exponential decay patterns.  The [[biological half-life|biological half-lives]] \"alpha half-life\" and \"beta half-life\" of a substance measure how quickly a substance is distributed  and eliminated.\n* '''[[Physical optics]]:''' The intensity of [[electromagnetic radiation]] such as light or X-rays or gamma rays in an absorbent medium, follows an exponential decrease with distance into the absorbing medium.  This is known as the [[Beer-Lambert]] law.\n* '''[[Radioactivity]]:''' In a sample of a [[radionuclide]] that undergoes [[radioactive decay]] to a different state, the number of atoms in the original state follows exponential decay as long as the remaining number of atoms is large. The decay product is termed a [[radiogenic]] nuclide.\n* '''[[Thermoelectricity]]:''' The decline in resistance of a Negative Temperature Coefficient [[Thermistor]] as temperature is increased.\n* '''[[Vibrations]]:''' Some vibrations may decay exponentially; this characteristic is often found in [[Harmonic oscillator|damped mechanical oscillators]], and used in creating [[ADSR envelope]]s in [[Synthesizer#Sound basics|synthesizers]].  An [[overdamped]] system will simply return to equilibrium via an exponential decay.\n* '''Beer froth:''' Arnd Leike, of the [[Ludwig Maximilian University of Munich]], won an [[List of Ig Nobel Prize winners|Ig Nobel Prize]] for demonstrating that [[beer]] froth obeys the law of exponential decay.<ref>{{Cite journal| last1 = Leike | first1 = A.| title = Demonstration of the exponential decay law using beer froth| journal = European Journal of Physics| volume = 23| pages = 21| year = 2002| doi = 10.1088/0143-0807/23/1/304|bibcode = 2002EJPh...23...21L | citeseerx = 10.1.1.693.5948}}</ref>\n\n=== Social sciences ===\n* '''[[Finance]]:''' a retirement fund will decay exponentially being subject to discrete payout amounts, usually monthly, and an input subject to a continuous interest rate. A differential equation dA/dt = input – output can be written and solved to find the time to reach any amount A, remaining in the fund.\n* In simple '''[[glottochronology]]''', the (debatable) assumption of a constant decay rate in languages allows one to estimate the age of single languages. (To compute the time of split between ''two'' languages requires additional assumptions, independent of exponential decay).\n\n=== Computer science ===\n* The core '''[[Routing|routing protocol]]''' on the [[Internet]], [[BGP]], has to maintain a [[routing table]] in order to remember the paths a [[Packet (information technology)|packet]] can be deviated to. When one of these paths repeatedly changes its state from ''available'' to ''not available'' (and ''vice versa''), the BGP [[router (computing)|router]] controlling that path has to repeatedly add and remove the path record from its routing table (''flaps'' the path), thus spending local resources such as [[CPU]] and [[Random-access memory|RAM]] and, even more, broadcasting useless information to peer routers. To prevent this undesired behavior, an algorithm named ''route flapping damping'' assigns each route a weight that gets bigger each time the route changes its state and decays exponentially with time. When the weight reaches a certain limit, no more flapping is done, thus suppressing the route.\n\n{{wide image|doubling_time_vs_half_life.svg|640px|Graphs comparing doubling times and half lives of exponential growths (bold lines) and decay (faint lines), and their 70/''t'' and 72/''t'' approximations. In the [http://upload.wikimedia.org/wikipedia/commons/8/88/Doubling_time_vs_half_life.svg SVG version], hover over a graph to highlight it and its complement.}}\n\n==See also==\n* [[Exponential formula]]\n* [[Exponential growth]]\n* [[Radioactive decay]] for the mathematics of chains of exponential processes with differing constants\n\n==References==\n{{Reflist}}\n\n==External links==\n* [https://www.fxsolver.com/browse/formulas/Exponential+decay Exponential decay calculator]\n* [http://vam.anest.ufl.edu/simulations/stochasticonecompartment.php A stochastic simulation of exponential decay]\n* [https://web.archive.org/web/20060617205436/http://www.facstaff.bucknell.edu/mastascu/elessonshtml/SysDyn/SysDyn3TCBasic.htm Tutorial on time constants]\n\n[[Category:Exponentials]]"
    },
    {
      "title": "Exponential distribution",
      "url": "https://en.wikipedia.org/wiki/Exponential_distribution",
      "text": "{{distinguish|text=the [[exponential family]] of probability distributions}}\n{{Probability distribution\n  | name       = Exponential\n  | type       = continuous\n  | pdf_image  = [[File:exponential pdf.svg|325px|Probability density function]]\n  | cdf_image  = [[File:exponential cdf.svg|325px|Cumulative distribution function]]\n  | parameters = {{nowrap|''λ'' > 0}} rate, or inverse [[scale parameter|scale]]\n  | support    = {{nowrap|''x'' ∈ [0, ∞)}}\n  | pdf        = λ&thinsp;''e''<sup>−''λx''</sup>\n  | cdf        = {{nowrap|1 − ''e''<sup>−''λx''</sup>}}\n  | quantile   = {{nowrap|−ln(1 − ''F'') / ''λ''}}\n  | mean       = ''λ''<sup>−1</sup> (= ''β'')\n  | median     = {{nowrap|''λ''<sup>−1</sup>ln(2)}}\n  | mode       = 0\n  | variance   = ''λ''<sup>−2</sup> (= ''β''<sup>2</sup>)\n  | skewness   = 2\n  | kurtosis   = 6\n  | entropy    = {{nowrap|1 − ln(''λ'')}}\n  | mgf        = <math>\\frac{\\lambda}{\\lambda-t}, \\text{ for } t < \\lambda</math>\n  | char       = <math>\\frac{\\lambda}{\\lambda-it}</math>\n  | rate       = <math>\\lambda z - \\log(\\lambda x) - 1,</math>\n  | fisher     = <math> \\lambda^{-2} </math>\n  | KLDiv      = <math>D_{KL}(exp_0||exp_1) = \\log(\\lambda_0) - \\log(\\lambda) + \\frac{\\lambda}{\\lambda_0} - 1</math>\n  }}\n\nIn [[probability theory]] and [[statistics]], the '''exponential distribution''' (known as the '''negative exponential distribution''') is the [[probability distribution]] of the time between events in a [[Poisson point process]], i.e., a process in which events occur continuously and independently at a constant average rate. It is a particular case of the [[gamma distribution]]. It is the continuous analogue of the [[geometric distribution]], and it has the key property of being [[memoryless]]. In addition to being used for the analysis of Poisson point processes it is found in various other contexts.\n\nThe exponential distribution is not the same as the class of [[exponential family|exponential families]] of distributions, which is a large class of probability distributions that includes the exponential distribution as one of its members, but also includes the [[normal distribution]], [[binomial distribution]], [[gamma distribution]], [[Poisson distribution|Poisson]], and many others.\n\n==Characterization==\n\n===Probability density function===\nThe [[probability density function]] (pdf) of an exponential distribution is\n\n:<math> f(x;\\lambda) = \\begin{cases}\n\\lambda e^{-\\lambda x} & x \\ge 0, \\\\\n0 & x < 0.\n\\end{cases}</math>\n\nAlternatively, this can be defined using the right-continuous [[Heaviside step function]], ''H''(''x'') where ''H''(0)&nbsp;=&nbsp;1:\n\n:<math>f(x;\\lambda) = \\lambda e^{-\\lambda x} H(x)</math>\n\nHere ''λ'' > 0 is the parameter of the distribution, often called the ''rate parameter''. The distribution is supported on the interval&nbsp;[0,&nbsp;∞). If a [[random variable]] ''X'' has this distribution, we write&nbsp;''X''&nbsp;~&nbsp;Exp(''λ'').\n\nThe exponential distribution exhibits [[infinite divisibility (probability)|infinite divisibility]].\n\n===Cumulative distribution function===\nThe [[cumulative distribution function]] is given by\n\n:<math>F(x;\\lambda) = \\begin{cases}\n1-e^{-\\lambda x} & x \\ge 0, \\\\\n0 & x < 0.\n\\end{cases}</math>\n\nAlternatively, this can be defined using the [[Heaviside step function]], ''H''(''x'').\n\n:<math>F(x;\\lambda) = \\mathrm (1-e^{-\\lambda x}) H(x)</math>\n\n===Alternative parametrization===\nA commonly used alternative parametrization is to define the [[probability density function]] (pdf) of an exponential distribution as\n\n:<math>f(x;\\beta) = \\begin{cases}\n\\frac{1}{\\beta} e^{-\\frac{x}{\\beta}} & x \\ge 0, \\\\\n0 & x < 0.\n\\end{cases}</math>\n\nwhere β > 0 is [[mean]], [[standard deviation]], and [[scale parameter]] of the distribution, the [[multiplicative inverse|reciprocal]] of the ''rate parameter'', λ, defined above. In this specification, β is a ''survival parameter'' in the sense that if a [[random variable]] ''X'' is the duration of time that a given biological or mechanical system manages to survive and ''X'' ~ Exp(β) then E[''X''] = β. That is to say, the expected duration of survival of the system is β units of time. The parametrization involving the \"rate\" parameter arises in the context of events arriving at a rate λ, when the time between events (which might be modeled using an exponential distribution) has a mean of β = λ<sup>−1</sup>.\n\nThe alternative specification is sometimes more convenient than the one given above, and some authors will use it as a standard definition. This alternative specification is not used here. Unfortunately this gives rise to a [[Mathematical notation|notational]] ambiguity. In general, the reader must check which of these two specifications is being used if an author writes \"''X'' ~ Exp(λ)\", since either the notation in the previous (using λ) or the notation in this section (here, using ''β'' to avoid confusion) could be intended. An example of this notational switch: reference<ref>David Olive, ''[http://lagrange.math.siu.edu/Olive/ch4.pdf Chapter 4. Truncated Distributions]'', \"Lemma 4.3\", [[Southern Illinois University]], February 18, 2010, p.107.</ref> uses λ for β.\n\n==Properties==\n\n===Mean, variance, moments and median===\n[[File:Mean exp.svg|thumb|The mean is the probability mass centre, that is the [[first moment]].]]\n[[File:Median exp.svg|thumb|The median is the [[preimage]] ''F''<sup>−1</sup>(1/2).]]\nThe mean or [[expected value]] of an exponentially distributed random variable ''X'' with rate parameter λ is given by\n\n:<math>\\operatorname{E}[X] = \\frac{1}{\\lambda} = \\beta,</math> see above.\n\nIn light of the examples given above, this makes sense: if you receive phone calls at an average rate of 2 per hour, then you can expect to wait half an hour for every call.\n\nThe [[variance]] of ''X'' is given by\n\n:<math>\\operatorname{Var}[X] = \\frac{1}{\\lambda^2} = \\beta^2,</math>\n\nso the [[standard deviation]] is equal to the mean.\n\nThe [[Moment (mathematics)|moments]] of ''X'', for <math>n\\in\\mathbb{N}</math> are given by\n\n:<math>\\operatorname{E}\\left [X^n \\right ] = \\frac{n!}{\\lambda^n}.</math>\n\nThe [[central moment]]s of ''X'', for <math>n\\in\\mathbb{N}</math> are given by\n\n:<math>\\mu_n = \\frac{!n}{\\lambda^n}=\\frac{n!}{\\lambda^n}\\sum^n_{k=0}\\frac{(-1)^k}{k!}.</math>\nwhere !''n'' is the [[Derangement|subfactorial]] of ''n''\n\nThe [[median]] of ''X'' is given by\n\n:<math>\\operatorname{m}[X] = \\frac{\\ln(2)}{\\lambda} < \\operatorname{E}[X],</math>\n\nwhere ln refers to the [[natural logarithm]].  Thus the [[absolute difference]] between the mean and median is\n\n:<math>|\\operatorname{E}[X]- \\operatorname{m}[X]| = \\frac{1- \\ln(2)}{\\lambda}< \\frac{1}{\\lambda} = \\text{standard deviation},</math>\n\nin accordance with the [[Chebyshev's inequality#An application: distance between the mean and the median|median-mean inequality]].\n\n===Memorylessness===\nAn exponentially distributed random variable ''T'' obeys the relation\n\n:<math>\\Pr \\left (T > s + t \\mid T > s \\right ) = \\Pr(T > t), \\qquad \\forall s, t \\ge 0.</math>\n\nThis can be seen by considering the [[Cumulative distribution function#Complementary cumulative distribution function (tail distribution)|complementary cumulative distribution function]]:\n\n:<math> \\begin{align}\n\\Pr \\left (T > s + t \\mid T > s \\right ) &= \\frac{\\Pr \\left (T > s + t \\cap T > s \\right )}{\\Pr \\left (T > s \\right )} \\\\[4pt]\n&= \\frac{\\Pr \\left (T > s + t \\right )}{\\Pr \\left (T > s \\right )} \\\\[4pt]\n&= \\frac{e^{-\\lambda(s+t)}}{e^{-\\lambda s}} \\\\[4pt]\n&= e^{-\\lambda t} \\\\[4pt]\n&= \\Pr(T > t).\n\\end{align} </math>\n\nWhen ''T'' is interpreted as the waiting time for an event to occur relative to some initial time, this relation implies that, if ''T'' is conditioned on a failure to observe the event over some initial period of time ''s'', the distribution of the remaining waiting time is the same as the original unconditional distribution. For example, if an event has not occurred after 30 seconds, the [[conditional probability]] that occurrence will take at least 10 more seconds is equal to the unconditional probability of observing the event more than 10 seconds after the initial time.\n\nThe exponential distribution and the [[geometric distribution]] are the only memoryless probability distributions.\n\nThe exponential distribution is consequently also necessarily the only continuous probability distribution that has a constant [[failure rate]].\n\n===Quantiles===\n[[File:Tukey anomaly criteria for Exponential PDF.png|thumb|alt=Tukey anomaly criteria for exponential probability distribution function.| Tukey criteria for anomalies.{{citation needed|date=September 2017}}]]\n\nThe [[quantile function]] (inverse cumulative distribution function) for Exp(''λ'') is\n\n:<math>F^{-1}(p;\\lambda) = \\frac{-\\ln(1-p)}{\\lambda},\\qquad 0 \\le p < 1</math>\n\nThe [[quartile]]s are therefore:\n\n*first quartile: ln(4/3)/''λ''\n*[[median]]: ln(2)/''λ''\n*third quartile: ln(4)/''λ''\n\nAnd as a consequence the [[interquartile range]] is ln(3)/''λ''.\n\n===Kullback–Leibler divergence===\nThe directed [[Kullback–Leibler divergence]] of <math>e^\\lambda</math> (\"approximating\" distribution) from <math>e^{\\lambda_0}</math> ('true' distribution) is given by\n\n:<math>\\begin{align}\n\\Delta(\\lambda_0 || \\lambda) \n&= \\mathbb{E}_{\\lambda_0}\\left( \\log \\frac{p_{\\lambda_0}(x)}{p_\\lambda(x)}\\right)\\\\\n&= \\mathbb{E}_{\\lambda_0}\\left( \\log \\frac{\\lambda_0 e^{-\\lambda_0 x}}{\\lambda e^{-\\lambda x}}\\right)\\\\\n&= \\log(\\lambda_0) - \\log(\\lambda) - (\\lambda_0 - \\lambda)E_{\\lambda_0}(x)\\\\\n&= \\log(\\lambda_0) - \\log(\\lambda) + \\frac{\\lambda}{\\lambda_0} - 1.\n\\end{align}\n\n</math>\n\n===Maximum entropy distribution===\nAmong all continuous probability distributions with [[Support (mathematics)#In probability and measure theory|support]] [0, &infin;) and mean μ, the exponential distribution with &lambda; = 1/μ has the largest [[differential entropy]]. In other words, it is the [[maximum entropy probability distribution]] for a [[random variate]] ''X'' which is greater than or equal to zero and for which E[''X''] is fixed.<ref>{{cite journal |last1=Park |first1=Sung Y. |last2=Bera |first2=Anil K. |year=2009 |title=Maximum entropy autoregressive conditional heteroskedasticity model |journal=Journal of Econometrics |volume= |issue= |pages=219–230 |publisher=Elsevier |doi= |url=http://www.wise.xmu.edu.cn/Master/Download/..%5C..%5CUploadFiles%5Cpaper-masterdownload%5C2009519932327055475115776.pdf |accessdate=2011-06-02 }}</ref>\n\n===Distribution of the minimum of exponential random variables===\nLet ''X''<sub>1</sub>, ..., ''X''<sub>''n''</sub> be [[Independent random variables|independent]] exponentially distributed random variables with rate parameters λ<sub>1</sub>, ..., λ<sub>''n''</sub>.  Then\n\n:<math> \\min \\left \\{X_1,\\dotsc,X_n \\right \\} </math>\n\nis also exponentially distributed, with parameter\n\n:<math> \\lambda = \\lambda_1+\\dotsb+\\lambda_n.</math>\n\nThis can be seen by considering the [[Cumulative distribution function#Complementary cumulative distribution function (tail distribution)|complementary cumulative distribution function]]:\n\n:<math> \\begin{align}\n\\Pr \\left (\\min\\{X_1,\\dotsc,X_n \\} > x \\right ) & = \\Pr\\left(X_1 > x, \\dotsc, X_n > x\\right) \\\\\n&= \\prod_{i=1}^n \\Pr(X_i > x) \\\\\n&= \\prod_{i=1}^n \\exp(-x\\lambda_i) = \\exp\\left(-x\\sum_{i=1}^n \\lambda_i\\right).\n\\end{align} </math>\n\nThe index of the variable which achieves the minimum is distributed according to the law\n\n:<math>\\Pr \\left (k\\mid X_k=\\min\\{X_1,\\dotsc,X_n\\} \\right )=\\frac{\\lambda_k}{\\lambda_1+\\dotsb+\\lambda_n}.</math>\n\nNote that\n\n:<math> \\max\\{X_1,\\dotsc,X_n\\} </math>\n\nis not exponentially distributed.<ref>{{cite web|last1=Michael|first1=Lugo|title=The expectation of the maximum of exponentials|url=http://www.stat.berkeley.edu/~mlugo/stat134-f11/exponential-maximum.pdf|accessdate=13 December 2016}}</ref>\n\n===Joint moments of i.i.d. exponential order statistics===\n\nLet <math> X_1,\\dotsc,X_n </math> be <math> n </math> [[Independent and identically distributed random variables|independent and identically distributed]] exponential random variables with rate parameter ''λ''. \nLet <math> X_{(1)},\\dotsc,X_{(n)} </math> denote the corresponding [[order statistic]]s. \nFor <math> i < j </math> , the joint moment <math> E[X_{(i)} X_{(j)}] </math> of the order statistics <math> X_{(i)} </math> and <math> X_{(j)} </math> is given by\n\n:<math> E[X_{(i)} X_{(j)}] = \\frac{1}{(n-j+1)\\lambda}  E[X_{(i)}] +  E[X_{(i)}^2] = \\frac{1}{(n-j+1)\\lambda}  \\frac{1}{(n-i+1)\\lambda} +  \\frac{2}{((n-i+1)\\lambda)^2}. </math>\n\nThis can be seen by invoking the [[law of total expectation]] and the memoryless property:\n:<math>\n\\begin{align}\n\\operatorname E[X_{(i)} X_{(j)}] & = \\int _0^\\infty \\operatorname E[X_{(i)} X_{(j)} \\mid X_{(i)}=x] f_{X_{(i)}}(x) \\, dx \\\\\n&= \\int_{x=0}^\\infty x \\operatorname E[ X_{(j)} \\mid X_{(j)} \\geq x ] f_{X_{(i)}}(x) \\, dx \\qquad (\\textrm{since}~X_{(i)} = x \\implies X_{(j)} \\geq x) \\\\\n&= \\int_{x=0}^\\infty x \\left[ \\frac{1}{(n-j+1)\\lambda} + x \\right] f_{X_{(i)}}(x) \\, dx \\qquad (\\text{by the memoryless property}) \\\\\n&= \\frac{1}{(n-j+1)\\lambda} \\operatorname E[X_{(i)}] + \\operatorname E[X_{(i)}^2].\n\\end{align} </math>\nThe first equation follows from the [[law of total expectation]].\nThe second equation exploits the fact that once we condition on <math> X_{(i)} = x </math>, it must follow that <math> X_{(j)} \\geq x </math>.\nThe third equation relies on the memoryless property to replace <math>\\operatorname E[ X_{(j)} \\mid X_{(j)} \\geq x ] </math> with <math> \\operatorname E[ X_{(j)} ] + x </math>.\n\n===Sum of two independent exponential random variables===\nThe sum of two independent variables corresponds to the [[convolution of probability distributions]].\nIf <math>X_1</math> and <math>X_2</math> are independent exponential random variables of independent observations, with respective rate parameters <math>\\lambda_1</math> and <math>\\lambda_2</math>, then the probability density of <math>Z=X_1+X_2</math> is given by,\n:<math> \\begin{align}\nf_Z(z) =& \\int_0^z f_{X_1}(x_1) f_{X_2}(z - x_1)\\,dx_1\\\\\n =& \\lambda_1 \\lambda_2 \\exp[-\\lambda_2 z] \\int_0^z \\exp[(\\lambda_2 - \\lambda_1)x_1]\\,dx_1\\\\\n =& \\frac{\\lambda_1 \\lambda_2}{\\lambda_2-\\lambda_1} \\left(\\exp[-\\lambda_1 z] - \\exp[-\\lambda_2 z]\\right)\n\\end{align} </math>\n\nThe mean and variance of <math>Z=X_1+X_2</math> are found to be,\n:<math> \\begin{align}\n\\operatorname E[Z] =& \\int z\\,df_Z(z)\\\\[4pt]\n=& \\lambda_1^{-1} + \\lambda_2^{-1}\\\\[4pt]\n\\operatorname{var}(Z) =& \\operatorname E[Z^2] - \\operatorname E[Z]^2\\\\[4pt]\n=& \\int z^2\\,df_Z(z) - (\\lambda_1^{-1} + \\lambda_2^{-1})^2\\\\[4pt]\n=& \\lambda_1^{-2} + \\lambda_2^{-2}\n\\end{align} </math>\n\nThe cumulative distribution function of the sum of two independent exponential random variables then follows as,\n:<math> \\begin{align}\nF_Z(z) = \\Pr(Z\\le z) =& \\int_0^z f_Z(t)\\,dt\\\\[4pt]\n =& 1 + \\frac{\\lambda_1}{\\lambda_2-\\lambda_1} \\exp[-\\lambda_2 z] - \\frac{\\lambda_2}{\\lambda_2-\\lambda_1} \\exp[-\\lambda_1 z]\n\\end{align} </math>\n\n==Parameter estimation==\nBelow, suppose random variable ''X'' is exponentially distributed with rate parameter λ, and <math>x_1, \\dotsc, x_n</math> are ''n'' independent samples from ''X'', with sample mean <math>\\bar{x}</math>.\n\n===Maximum likelihood===\nThe [[likelihood function]] for λ, given an [[independent identically-distributed random variables|independent and identically distributed]] sample ''x'' = (''x''<sub>1</sub>, ..., ''x''<sub>''n''</sub>) drawn from the variable, is:\n\n:<math> L(\\lambda) = \\prod_{i=1}^n \\lambda \\exp(-\\lambda x_i) = \\lambda^n \\exp \\left(-\\lambda \\sum_{i=1}^n x_i\\right)=\\lambda^n\\exp\\left(-\\lambda n \\overline{x}\\right), </math>\n\nwhere:\n\n:<math>\\overline{x}={1 \\over n}\\sum_{i=1}^n x_i</math>\n\nis the sample mean.\n\nThe derivative of the likelihood function's logarithm is:\n\n:<math>\\frac{\\mathrm{d}}{\\mathrm{d}\\lambda} \\ln (L(\\lambda)) = \\frac{\\mathrm{d}}{\\mathrm{d}\\lambda} \\left( n \\ln(\\lambda) - \\lambda n\\overline{x} \\right) = \\frac{n}{\\lambda}-n\\overline{x}\\ \\begin{cases} > 0, & 0 < \\lambda < \\frac{1}{\\overline{x}}, \\\\[8pt] = 0, & \\lambda = \\frac{1}{\\overline{x}}, \\\\[8pt] < 0, & \\lambda > \\frac{1}{\\overline{x}}. \\end{cases} </math>\n\nConsequently, the [[maximum likelihood]] estimate for the rate parameter is:\n\n:<math>\\widehat{\\lambda} = \\frac{1}{\\overline{x}} = \\frac{n}{\\sum_i x_i}</math>\n\nAlthough this is ''not'' an [[unbiased estimator]] of <math>\\lambda</math>, <math>\\overline{x}</math> ''is'' an unbiased<ref name=\"JohnsonWichern2007\">{{cite book|author1=Richard Arnold Johnson|author2=Dean W. Wichern|title=Applied Multivariate Statistical Analysis|url=https://books.google.com/books?id=gFWcQgAACAAJ|accessdate=10 August 2012|year=2007|publisher=Pearson Prentice Hall|isbn=978-0-13-187715-3}}</ref> MLE<ref>''[http://www.itl.nist.gov/div898/handbook/eda/section3/eda3667.htm NIST/SEMATECH e-Handbook of Statistical Methods]''</ref> estimator of <math>1/\\lambda = \\beta,</math> where <math>\\beta</math> is the scale parameter defined in the [[#Alternative parameterization|'Alternative parameterization' section]] above and the distribution mean.\n\nThe bias of <math> \\hat \\lambda_\\mathrm{mle} </math> is equal to \n: <math>\n    b \\equiv \\operatorname{E}\\bigg[\\;(\\hat \\lambda_\\mathrm{mle} - \\lambda)\\;\\bigg]\n        = \\frac{\\lambda}{n} \n  </math>\n\nwhich yields the '''[[Maximum_likelihood_estimation#Higher-order_properties|bias-corrected maximum likelihood estimator]]'''\n\n: <math>\n    \\hat{\\lambda\\,}^*_\\text{mle} = \\hat{\\lambda\\,}_\\text{mle} - \\hat{b\\,}\n  </math>\n\n===Approximate minimizer of expected squared error===\nAssume you have at least three samples. If we seek a minimizer of expected [[mean squared error]] (see also: [[Bias–variance tradeoff]]) that is similar to the maximum likelihood estimate (i.e. a multiplicative correction to the likelihood estimate) we have:\n\n:<math>\\widehat{\\lambda} = \\left ( \\frac{n-2}{n} \\right ) \\left(\\frac{1}{\\bar{x}}\\right) = \\frac{n-2}{\\sum_{i} x_i}</math>\n\nThis is derived from the mean and variance of the [[Inverse-gamma distribution]]: <math display=\"inline\">\\mbox{Inv-Gamma}(n, \\lambda)</math>.<ref>{{cite journal |first=Abdulaziz |last=Elfessi |first2=David M. |last2=Reineke |url=http://www.amstat.org/publications/jse/v9n1/elfessi.html |title=A Bayesian Look at Classical Estimation: The Exponential Distribution |journal=Journal of Statistics Education |volume=9 |issue=1 |year=2001 |pages= }}</ref>\n\n===Fisher Information===\nThe [[Fisher information]], denoted <math>\\mathcal{I}(\\lambda)</math>, for an estimator of the rate parameter <math>\\lambda</math> is given as:\n:<math> \\mathcal{I}(\\lambda)=\\operatorname{E} \\left[\\left. \\left(\\frac{\\partial}{\\partial\\lambda} \\log f(x;\\lambda)\\right)^2\\right|\\lambda\\right] = \\int \\left(\\frac{\\partial}{\\partial\\lambda} \\log f(x;\\lambda)\\right)^2 f(x; \\lambda)\\,dx</math>\nPlugging in the distribution and solving gives:\n\n:<math> \\mathcal{I}(\\lambda)=\\int_{0}^{\\infty} \\left(\\frac{\\partial}{\\partial\\lambda} \\log \\lambda e^{-\\lambda x}\\right)^2 \\lambda e^{-\\lambda x}\\,dx=\\int_{0}^{\\infty} \\left(\\frac{1}{\\lambda}-x\\right)^2 \\lambda e^{-\\lambda x}\\,dx=\\lambda^{-2}.</math>\n\nThis determines the amount of information each independent sample of an exponential distribution carries about the unknown rate parameter <math>\\lambda</math>.\n\n===Confidence intervals===\nThe 100(1 − α)% confidence interval for the rate parameter of an exponential distribution is given by:<ref>{{cite book|title=Introduction to probability and statistics for engineers and scientists|first=Sheldon M.|last=Ross|page=267|url=https://books.google.com/books?id=mXP_UEiUo9wC&pg=PA267| edition=4th|year=2009| publisher=Associated Press|isbn=978-0-12-370483-2}}</ref>\n\n:<math>\\frac{2n}{\\widehat{\\lambda} \\chi^2_{\\frac{\\alpha}{2},2n}}< \\frac{1}{\\lambda} < \\frac{2n}{\\widehat{\\lambda} \\chi^2_{1-\\frac{\\alpha}{2},2n}}</math>\n\nwhich is also equal to:\n\n: <math>\\frac{2n\\overline{x}}{\\chi^2_{\\frac{\\alpha}{2},2n}} < \\frac{1}{\\lambda} < \\frac{2n\\overline{x}}{\\chi^2_{1-\\frac{\\alpha}{2},2n}}</math>\n\nwhere {{math|χ{{su|p=2|b=''p'',''v''}}}} is the {{math|100(''p'')}} [[percentile]] of the  [[chi squared distribution]] with ''v'' [[degrees of freedom (statistics)|degrees of freedom]], n is the number of observations of inter-arrival times in the sample, and x-bar is the sample average. A simple approximation to the exact interval endpoints can be derived using a normal approximation to the {{math|''χ''{{su|p=2|b=''p'',''v''}}}} distribution. This approximation gives the following values for a 95% confidence interval:\n\n:<math> \\lambda_\\text{lower}=\\widehat{\\lambda}  \\left (1-\\frac{1.96}{\\sqrt{n}} \\right ) </math>\n:<math> \\lambda_\\text{upper}=\\widehat{\\lambda}  \\left (1+\\frac{1.96}{\\sqrt{n}} \\right ) </math>\n\nThis approximation may be acceptable for samples containing at least 15 to 20 elements.<ref name=\"Guerriero\">{{Cite journal\n| first1 = V.|last1= Guerriero | year = 2012  | title = Power Law Distribution: Method of Multi-scale Inferential Statistics| journal = Journal of Modern Mathematics Frontier (JMMF)| url =http://www.sjmmf.org/paperInfo.aspx?ID=886 | volume = 1  | pages = 21–28}}</ref>\n\n===Bayesian inference===\nThe [[conjugate prior]] for the exponential distribution is the [[gamma distribution]] (of which the exponential distribution is a special case).  The following parameterization of the gamma probability density function is useful:\n\n:<math> \\operatorname{Gamma}(\\lambda; \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} \\exp(-\\lambda\\beta). </math>\n\nThe [[posterior distribution]] ''p'' can then be expressed in terms of the likelihood function defined above and a gamma prior:\n\n:<math> \\begin{align}\np(\\lambda) &\\propto L(\\lambda) \\times \\mathrm{Gamma}(\\lambda; \\alpha, \\beta) \\\\\n&= \\lambda^n \\exp\\left (-\\lambda n\\overline{x} \\right) \\times \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} \\exp(-\\lambda \\beta) \\\\\n&\\propto \\lambda^{(\\alpha+n)-1} \\exp(-\\lambda \\left (\\beta + n\\overline{x} \\right)).\n\\end{align}</math>\n\nNow the posterior density ''p'' has been specified up to a missing normalizing constant.  Since it has the form of a gamma pdf, this can easily be filled in, and one obtains:\n\n:<math> p(\\lambda) = \\mathrm{Gamma}(\\lambda; \\alpha + n, \\beta + n \\overline{x}). </math>\n\nHere the [[hyperparameter]] α can be interpreted as the number of prior observations, and β as the sum of the prior observations.\nThe posterior mean here is:\n\n:<math> \\frac{\\alpha + n}{\\beta + n \\overline{x}}. </math>\n\n==Generating exponential variates==<!-- This section is linked from [[Gamma distribution]] -->\nA conceptually very simple method for generating exponential [[random variate|variate]]s is based on [[inverse transform sampling method|inverse transform sampling]]: Given a random variate ''U'' drawn from the [[uniform distribution (continuous)|uniform distribution]] on the unit interval (0,&nbsp;1), the variate\n\n:<math>T = F^{-1}(U)</math>\n\nhas an exponential distribution, where ''F''<sup>&nbsp;−1</sup> is the [[quantile function]], defined by\n\n:<math>F^{-1}(p)=\\frac{-\\ln(1-p)}{\\lambda}.</math>\n\nMoreover, if ''U'' is uniform on (0, 1), then so is 1 − ''U''.  This means one can generate exponential variates as follows:\n\n:<math>T = \\frac{-\\ln(U)}{\\lambda}.</math>\n\nOther methods for generating exponential variates are discussed by Knuth<ref>[[Donald Knuth|Donald E. Knuth]] (1998). ''[[The Art of Computer Programming]]'', volume 2: ''Seminumerical Algorithms'', 3rd edn. Boston: Addison–Wesley. {{ISBN|0-201-89684-2}}. ''See section 3.4.1, p. 133.''</ref> and Devroye.<ref name=\"devroye\">Luc Devroye (1986). ''[http://luc.devroye.org/rnbookindex.html Non-Uniform Random Variate Generation]''. New York: Springer-Verlag. {{ISBN|0-387-96305-7}}. ''See [http://luc.devroye.org/chapter_nine.pdf chapter IX], section 2, pp. 392–401.''</ref>\n\nA fast method for generating a set of ready-ordered exponential variates without using a sorting routine is also available.<ref name=\"devroye\"/>\n\n==Related distributions==\n{{More footnotes|section|date=March 2011}}\n\n* Exponential distribution is closed under scaling by a positive factor. If ''X'' ~ Exp(''λ'') then ''kX'' ~ Exp(''λ''/''k'').\n* If ''X'' ~ Exp(1/2) then {{nowrap|''X'' ∼ χ{{su|b=2|p=2}}}}, i.e. ''X'' has a [[chi-squared distribution]] with 2 [[degrees of freedom (statistics)|degrees of freedom]]. Hence:\n\n:: <math>\\operatorname{Exp}(\\lambda) = \\frac{1}{2 \\lambda} \\operatorname{Exp} \\left( \\frac{1}{2}  \\right) \\sim \\frac{1}{2\\lambda } \\chi_2^2\\Rightarrow \\sum_{i=1}^n \\operatorname{Exp}(\\lambda) \\sim \\frac{1}{2\\lambda }\\chi_{2n}^2</math>\n* If ''X<sub>i</sub>'' ~ Exp(''λ''<sub>''i''</sub>) then min{''X''<sub>1</sub>, ..., ''X<sub>n</sub>''} ~ Exp(''λ''<sub>1</sub>+ ... +''λ''<sub>''n''</sub>).\n* The [[Benktander Weibull distribution]] reduces to a truncated exponential distribution. If ''X'' ~ Exp(''λ'') then 1&nbsp;+&nbsp;''X'' ~ [[Benktander Weibull distribution|BenktanderWeibull(''λ'', 1)]].\n* The exponential distribution is a limit of a scaled [[beta distribution]]: \n::<math>\\lim_{n \\to \\infty} n \\operatorname{Beta}(1,n) = \\operatorname{Exp}(1).</math>\n* If ''X<sub>i</sub>'' ~ Exp(''λ'') then the sum <math display=\"inline\"> X_1 + \\cdots + X_k = \\sum_i X_i \\sim </math> [[Erlang distribution|Erlang(''k'', ''λ'')]]  which is just a [[gamma distribution|Gamma(''k'', ''λ''<sup>−1</sup>)]] (in (''k'', ''θ'') parametrization) or Gamma(''k'', ''λ'') (in (''α'',''β'') parametrization) with an integer shape parameter k.\n* If ''X'' ~ Exp(1) then ''μ'' − ''σ'' log(''X'') ~ [[Generalized extreme value distribution|GEV(μ, ''σ'', 0)]].\n* If ''X'' ~ Exp(''λ'') then ''X'' ~ [[gamma distribution|Gamma(1, ''λ''<sup>−1</sup>)]] (in (''k'', ''θ'') parametrization) or Gamma(1, ''λ'') (in (''α'', ''β'') parametrization).\n* If ''X'' ~ Exp(λ) and ''Y'' ~ Exp(''ν'') then ''λX'' − ''νY'' ~ [[Laplace distribution|Laplace(0, 1)]].\n* If ''X'', ''Y'' ~ Exp(''λ'') then ''X'' − ''Y'' ~ Laplace(0, ''λ''<sup>−1</sup>).\n* If ''X'' ~ Laplace(''μ'', ''β''<sup>−1</sup>) then |''X'' − ''μ''| ~ Exp(''β'').\n* If ''X'' ~ Exp(1) then ([[logistic distribution]]):\n::<math>\\mu-\\beta\\log \\left(\\tfrac{e^{-X}}{1-e^{-X}}\\right) \\sim \\mathrm{Logistic}(\\mu,\\beta) </math>\n* If ''X'', ''Y'' ~ Exp(1) then ([[logistic distribution]]):\n::<math>\\mu-\\beta\\log\\left(\\tfrac{X}{Y}\\right) \\sim \\mathrm{Logistic}(\\mu,\\beta) </math>\n* If ''X'' ~ Exp(λ) then ''ke<sup>X</sup>'' ~ [[Pareto distribution|Pareto(''k'', λ)]].\n* If ''X'' ~ Pareto(1, λ) then log(''X'') ~ Exp(λ).\n* Exponential distribution is a special case of type 3 [[Pearson distribution]].\n* If ''X'' ~ Exp(λ) then ''e<sup>−X</sup>'' ~ [[Beta distribution|Beta(λ, 1)]].\n* If ''X'' ~ Exp(λ) then <math>\\tfrac{e^{-X}}{k} \\sim \\mathrm{PowerLaw}(k, \\lambda)</math> ([[power law]])\n* If ''X'' ~ Exp(''λ'') then <math>\\sqrt{X} \\sim \\operatorname{Rayleigh} \\left(1/\\sqrt{2\\lambda}\\right).</math> ([[Rayleigh distribution]])\n* If ''X'' ~ Exp(''λ'') then <math> X \\sim \\mathrm{Weibull}(\\tfrac{1}{\\lambda},1)</math> ([[Weibull distribution]])\n* If ''X<sub>i</sub>'' ~ [[Uniform distribution (continuous)|''U''(0, 1)]] then \n::<math>\\lim_{n \\to \\infty}n \\min \\left (X_1, \\ldots, X_n \\right ) \\sim \\textrm{Exp}(1)</math>\n* If ''Y|X'' ~ [[Poisson distribution|Poisson(''X'')]] where ''X'' ~ Exp(λ<sup>−1</sup>) then <math>Y \\sim \\mathrm{Geometric}(\\tfrac{1}{1+\\lambda})</math> ([[geometric distribution]])\n* If ''X'' ~ Exp(1) and <math>Y \\sim \\Gamma(\\alpha,\\tfrac{\\beta}{\\alpha})</math> then <math>\\sqrt{XY} \\sim \\mathrm{K}(\\alpha,\\beta)</math> ([[K-distribution]])\n* The [[Hoyt distribution]] can be obtained from Exponential distribution and [[Arcsine distribution]]\n* If ''X'' ~ Exp(λ) and ''Y'' ~ Erlang(''n'', λ) then: \n::<math>\\frac{X}{Y}+1 \\sim \\mathrm{Pareto}(1,n)</math>\n* If ''X'' ~ Exp(λ) and <math>Y \\sim \\Gamma(n,\\tfrac{1}{\\lambda})</math> then <math>\\tfrac{X}{Y}+1 \\sim \\mathrm{Pareto}(1,n)</math>\n* If ''X'' ~  [[skew-logistic distribution|SkewLogistic(θ)]], then log(1 + ''e<sup>−X</sup>'') ~ Exp(θ).\n* If ''X'' ~ Exp(λ) and {{nowrap|''Y'' {{=}} μ − β log(''X''λ)}} then ''Y'' ∼ [[Gumbel distribution|Gumbel(μ, β)]].\n* Let {{nowrap|''X'' ∼ Exp(λ<sub>''X''</sub>)}} and {{nowrap|''Y'' ∼ Exp(λ<sub>''Y''</sub>)}} be independent. Then <math>Z = \\frac{\\lambda_X X}{\\lambda_Y Y}</math> has probability density function <math>f_Z(z) = \\frac{1}{(z + 1)^2}</math>. This can be used to obtain a [[confidence interval]] for <math>\\frac{\\lambda_X}{\\lambda_Y}</math>.\n* Gamma [[Compound_distribution|mixture]]: If ''λ'' ~ [[gamma distribution|Gamma]](shape&nbsp;=&nbsp;''k'', scale&nbsp;=&nbsp;''θ'') and ''X'' ~ Exponential(rate&nbsp;=&nbsp;''λ'') then the marginal distribution of ''X'' is [[Lomax distribution|Lomax]](shape&nbsp;=&nbsp;''k'', scale&nbsp;=&nbsp;1/''θ'')\n\nOther related distributions:\n*[[Hyper-exponential distribution]] – the distribution whose density is a weighted sum of exponential densities.\n*[[Hypoexponential distribution]] – the distribution of a general sum of exponential random variables.\n*[[exGaussian distribution]] – the sum of an exponential distribution and a [[normal distribution]].\n\n==Applications of exponential distribution==\n\n===Occurrence of events===\nThe exponential distribution occurs naturally when describing the lengths of the inter-arrival times in a homogeneous [[Poisson process]].\n\nThe exponential distribution may be viewed as a continuous counterpart of the [[geometric distribution]], which describes the number of [[Bernoulli trial]]s necessary for a ''discrete'' process to change state. In contrast, the exponential distribution describes the time for a continuous process to change state.\n\nIn real-world scenarios, the assumption of a constant rate (or probability per unit time) is rarely satisfied. For example, the rate of incoming phone calls differs according to the time of day. But if we focus on a time interval during which the rate is roughly constant, such as from 2 to 4 p.m. during work days, the exponential distribution can be used as a good approximate model for the time until the next phone call arrives. Similar caveats apply to the following examples which yield approximately exponentially distributed variables:\n\n* The time until a radioactive [[particle decay]]s, or the time between clicks of a [[geiger counter]]\n* The time it takes before your next telephone call\n* The time until default (on payment to company debt holders) in reduced form credit risk modeling\n\nExponential variables can also be used to model situations where certain events occur with a constant probability per unit length, such as the distance between [[mutation]]s on a [[DNA]] strand, or between [[roadkill]]s on a given road.\n\nIn [[queuing theory]], the service times of agents in a system (e.g. how long it takes for a bank teller etc. to serve a customer) are often modeled as exponentially distributed variables.  (The arrival of customers for instance is also modeled by the [[Poisson distribution]] if the arrivals are independent and distributed identically.)  The length of a process that can be thought of as a sequence of several independent tasks follows the [[Erlang distribution]] (which is the distribution of the sum of several independent exponentially distributed variables).\n[[Reliability theory]] and [[reliability engineering]] also make extensive use of the exponential distribution. Because of the ''[[#Memorylessness|memoryless]]'' property of this distribution, it is well-suited to model the constant [[hazard rate]] portion of the [[bathtub curve]] used in reliability theory. It is also very convenient because it is so easy to add [[failure rate]]s in a reliability model. The exponential distribution is however not appropriate to model the overall lifetime of organisms or technical devices, because the \"failure rates\" here are not constant: more failures occur for very young and for very old systems.\n[[File:FitExponDistr.tif|thumb|260px|Fitted cumulative exponential distribution to annually maximum 1-day rainfalls using [[CumFreq]]<ref>{{cite web |url=http://www.waterlog.info/cumfreq.htm| title=Cumfreq, a free computer program for cumulative frequency analysis}}</ref>]]\n\nIn [[physics]], if you observe a [[gas]] at a fixed [[temperature]] and [[pressure]] in a uniform [[gravitational field]], the heights of the various molecules also follow an approximate exponential distribution, known as the [[Barometric formula]]. This is a consequence of the entropy property mentioned below.\n\nIn [[hydrology]], the exponential distribution is used to analyze extreme values of such variables as monthly and annual maximum values of daily rainfall and river discharge volumes.<ref>{{cite book|last=Ritzema (ed.)|first=H.P.|title=Frequency and Regression Analysis|year=1994|publisher=Chapter 6 in: Drainage Principles and Applications, Publication 16, International Institute for Land Reclamation and Improvement (ILRI), Wageningen, The Netherlands|pages=175–224|url=http://www.waterlog.info/pdf/freqtxt.pdf|isbn=90-70754-33-9}}</ref>\n\n:The blue picture illustrates an example of fitting the exponential distribution to ranked annually maximum one-day rainfalls showing also the 90% [[confidence belt]] based on the [[binomial distribution]]. The rainfall data are represented by [[plotting position]]s as part of the [[cumulative frequency analysis]].\n\n===Prediction===\nHaving observed a sample of ''n'' data points from an unknown exponential distribution a common task is to use these samples to make predictions about future data from the same source. A common predictive distribution over future samples is the so-called plug-in distribution, formed by plugging a suitable estimate for the rate parameter ''λ'' into the exponential density function. A common choice of estimate is the one provided by the principle of maximum likelihood, and using this yields the predictive density over a future sample ''x''<sub>''n''+1</sub>, conditioned on the observed samples ''x'' = (''x''<sub>1</sub>, ..., ''x<sub>n</sub>'') given by\n\n:<math>p_{\\rm ML}(x_{n+1} \\mid x_1, \\ldots, x_n) = \\left( \\frac1{\\overline{x}} \\right) \\exp \\left( - \\frac{x_{n+1}}{\\overline{x}} \\right)</math>\n\nThe Bayesian approach provides a predictive distribution which takes into account the uncertainty of the estimated parameter, although this may depend crucially on the choice of prior.\n\nA predictive distribution free of the issues of choosing priors that arise under the subjective Bayesian approach is\n\n:<math>p_{\\rm CNML}(x_{n+1} \\mid x_1, \\ldots, x_n) = \\frac{ n^{n+1} \\left( \\overline{x} \\right)^n }{ \\left( n \\overline{x} + x_{n+1} \\right)^{n+1} },</math>\n\nwhich can be considered as\n# a frequentist [[confidence distribution]], obtained from the distribution of the pivotal quantity <math>{x_{n+1}}/{\\overline{x}}</math>;<ref>{{cite journal |last=Lawless |first=J. F. |last2=Fredette |first2=M. |title=Frequentist predictions intervals and predictive distributions |journal=Biometrika |year=2005 |volume=92 |issue=3 |pages=529–542 |doi=10.1093/biomet/92.3.529 }}</ref>\n# a profile predictive likelihood, obtained by eliminating the parameter ''λ'' from the joint likelihood of ''x''<sub>''n''+1</sub> and ''λ'' by maximization;<ref>{{cite journal | last1 = Bjornstad | first1 = J.F. | year = 1990 | title = Predictive Likelihood: A Review | url = | journal = Statist. Sci. | volume = 5 | issue = 2| pages = 242–254 | doi=10.1214/ss/1177012175}}</ref>\n# an objective Bayesian predictive posterior distribution, obtained using the non-informative [[Jeffreys prior]] 1/''λ'';\n# the Conditional Normalized Maximum Likelihood (CNML) predictive distribution, from information theoretic considerations.<ref>D. F. Schmidt and E. Makalic, \"[http://www.emakalic.org/blog/wp-content/uploads/2010/04/SchmidtMakalic09b.pdf Universal Models for the Exponential Distribution]\", ''[[IEEE Transactions on Information Theory]]'', Volume 55, Number 7, pp. 3087–3090, 2009 {{doi|10.1109/TIT.2009.2018331}}</ref>\n\nThe accuracy of a predictive distribution may be measured using the distance or divergence between the true exponential distribution with rate parameter, ''λ''<sub>0</sub>, and the predictive distribution based on the sample ''x''. The [[Kullback–Leibler divergence]] is a commonly used, parameterisation free measure of the difference between two distributions. Letting Δ(''λ''<sub>0</sub>||''p'') denote the Kullback–Leibler divergence between an exponential with rate parameter ''λ''<sub>0</sub> and a predictive distribution ''p'' it can be shown that\n\n:<math>\\begin{align}\n\\operatorname{E}_{\\lambda_0} \\left[ \\Delta(\\lambda_0\\parallel p_{\\rm ML}) \\right] &= \\psi(n) + \\frac{1}{n-1} - \\log(n) \\\\\n\\operatorname{E}_{\\lambda_0} \\left[ \\Delta(\\lambda_0\\parallel p_{\\rm CNML}) \\right] &= \\psi(n) + \\frac{1}{n} - \\log(n)\n\\end{align}</math>\n\nwhere the expectation is taken with respect to the exponential distribution with rate parameter {{nowrap|''λ''<sub>0</sub> ∈ (0, ∞)}}, and {{nowrap|ψ( · )}} is the digamma function. It is clear that the CNML predictive distribution is strictly superior to the maximum likelihood plug-in distribution in terms of average Kullback–Leibler divergence for all sample sizes {{nowrap|''n'' > 0}}.\n\n==See also==\n* [[Dead time]] – an application of exponential distribution to particle detector analysis.\n* [[Laplace distribution]], or the \"double exponential distribution\".\n* [[Relationships among probability distributions]]\n* [[Marshall–Olkin exponential distribution]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* {{springer|title=Exponential distribution|id=p/e036900}}\n*[http://www.elektro-energetika.cz/calculations/ex.php?language=english Online calculator of Exponential Distribution]\n\n{{ProbDistributions|continuous-semi-infinite}}\n\n{{DEFAULTSORT:Exponential Distribution}}\n[[Category:Continuous distributions]]\n[[Category:Exponentials]]\n[[Category:Poisson point processes]]\n[[Category:Conjugate prior distributions]]\n[[Category:Exponential family distributions]]\n[[Category:Infinitely divisible probability distributions]]\n[[Category:Survival analysis]]"
    },
    {
      "title": "Exponential family",
      "url": "https://en.wikipedia.org/wiki/Exponential_family",
      "text": "{{distinguish|text=the [[exponential distribution]]}}\n: ''\"Natural parameter\" links here.  For the usage of this term in differential geometry, see [[differential geometry of curves]].''\n\nIn [[theory of probability|probability]] and [[statistics]], an '''exponential family''' is a [[parametric model|parametric]] set of [[probability distribution]]s of a certain form, specified below. This special form is chosen for mathematical convenience, based on some useful algebraic properties, as well as for generality, as exponential families are in a sense very natural sets of distributions to consider. The term '''exponential class''' is sometimes used in place of \"exponential family\",<ref>Kupperman, M. (1958) \"Probabilities of Hypotheses and Information-Statistics in Sampling from Exponential-Class Populations\", [[Annals of Mathematical Statistics]], 9 (2), 571&ndash;575 {{JSTOR|2237349}}</ref> or the older term '''Koopman-Darmois family'''. The terms \"distribution\" and \"family\" are often used loosely: properly, ''an'' exponential family is a ''set'' of distributions, where the specific distribution varies with the parameter;{{NoteTag|For example, the family of normal distributions includes the standard normal distribution ''N''(0, 1) with mean 0 and variance 1, as well as other normal distributions with different mean and variance.}} however, a parametric ''family'' of distributions is often referred to as \"''a'' distribution\" (like \"the normal distribution\", meaning \"the family of normal distributions\"), and the set of all exponential families is sometimes loosely referred to as \"the\" exponential family.\n\nThe concept of exponential families is credited to<ref>{{cite journal\n | last = Andersen\n | first = Erling\n | date = September 1970\n | title = Sufficiency and Exponential Families for Discrete Sample Spaces\n | journal = [[Journal of the American Statistical Association]]\n | volume = 65\n | issue = 331\n | pages = 1248–1255\n | mr = 268992\n | doi = 10.2307/2284291\n | jstor = 2284291\n | publisher = Journal of the American Statistical Association, Vol. 65, No. 331\n }}</ref> [[E. J. G. Pitman]],<ref>{{cite journal\n | last = Pitman | first = E. |author-link = E. J. G. Pitman\n | year = 1936\n | title = Sufficient statistics and intrinsic accuracy\n | journal = [[Mathematical Proceedings of the Cambridge Philosophical Society]]\n | volume = 32\n | pages = 567–579\n | doi = 10.1017/S0305004100019307\n | last2 = Wishart\n | first2 = J.\n | issue = 4\n | bibcode = 1936PCPS...32..567P\n }}</ref> [[Georges Darmois|G. Darmois]],<ref>{{cite journal\n | last = Darmois\n | first = G.\n | year = 1935\n | title = Sur les lois de probabilites a estimation exhaustive\n | journal = C. R. Acad. Sci. Paris\n | volume = 200\n | pages = 1265–1266\n | language = fr\n }}</ref> and [[Bernard Koopman|B. O. Koopman]]<ref>{{cite journal\n | last = Koopman | first = B |authorlink=Bernard Koopman\n | year = 1936\n | title = On distribution admitting a sufficient statistic\n | journal = [[Transactions of the American Mathematical Society]]\n | volume = 39 |number = 3\n | pages = 399–409\n | mr = 1501854\n | doi = 10.2307/1989758\n | jstor = 1989758\n | publisher = [[American Mathematical Society]]\n }}</ref> in 1935&ndash;36. Exponential families of distributions provides a general framework for selecting a possible alternative parameterisation of a [[parametric family]] of distributions, in terms of '''natural parameters''', and for defining useful [[sample statistic]]s, called the '''natural sufficient statistics''' of the family.\n\n== Definition ==\nMost of the commonly used distributions form an exponential family or subset of an exponential family, listed in the subsection below. The subsections following it are a sequence of increasingly more general mathematical definitions of an exponential family. A casual reader may wish to restrict attention to the first and simplest definition, which corresponds to a single-parameter family of [[discrete probability distribution|discrete]] or [[continuous probability distribution|continuous]] probability distributions.\n\n===Examples of exponential family distributions===\nExponential families include many of the most common distributions. Among many others, exponential families includes the following:\n{{Div col |colwidth = 20em }}\n*[[normal distribution|normal]]\n*[[exponential distribution|exponential]]\n*[[gamma distribution|gamma]]\n*[[chi-squared distribution|chi-squared]]\n*[[beta distribution|beta]]\n*[[Dirichlet distribution|Dirichlet]]\n*[[Bernoulli distribution|Bernoulli]]\n*[[categorical distribution|categorical]]\n*[[Poisson distribution|Poisson]]\n*[[Wishart distribution|Wishart]]\n*[[Inverse Wishart distribution|inverse Wishart]]\n*[[Geometric distribution|geometric]]\n{{Div col end}}\n\nA number of common distributions are exponential families, but only when certain parameters are fixed and known. For example:\n*[[binomial distribution|binomial]] (with fixed number of trials)\n*[[multinomial distribution|multinomial]] (with fixed number of trials)\n*[[negative binomial distribution|negative binomial]] (with fixed number of failures)\nNotice that in each case, the parameters which must be fixed determine a limit on the size of observation values.\n\nExamples of common distributions that are ''not'' exponential families are [[Student's t distribution|Student's ''t'']], most [[mixture distribution]]s, and even the family of [[uniform distribution (continuous)|uniform distribution]]s when the bounds are not fixed. See the section below on [[#Examples|examples]] for more discussion.\n\n=== Scalar parameter ===\nA single-parameter exponential family is a set of probability distributions whose [[probability density function]] (or [[probability mass function]], for the case of a [[discrete distribution]]) can be expressed in the form\n\n: <math> f_X(x\\mid\\theta) = h(x) \\exp \\left (\\eta(\\theta) \\cdot T(x) -A(\\theta)\\right )</math>\n\nwhere ''T''(''x''), ''h''(''x''), ''η''(''θ''), and ''A''(''θ'') are known functions.\n\nAn alternative, equivalent form often given is\n\n: <math> f_X(x\\mid\\theta) = h(x) g(\\theta) \\exp \\left ( \\eta(\\theta) \\cdot T(x) \\right )</math>\n\nor equivalently\n\n: <math> f_X(x\\mid\\theta) = \\exp \\left (\\eta(\\theta) \\cdot T(x) - A(\\theta) + B(x) \\right )</math>\n\nThe value ''θ'' is called the parameter of the family.\n\nIn addition, the [[Support of a distribution#In probability and measure theory|support]] of <math>f_X \\left( x \\mid \\theta \\right)</math> (i.e. the set of all <math>x</math> for which <math>f_X \\left( x \\mid \\theta \\right)</math> is greater than 0) does not depend on <math>\\theta</math>.<ref>{{cite book |title=Statistical Theory: A Concise Introduction|last=Abramovich & Ritov|first=|publisher=Chapman & Hall |year=2013|isbn=978-1439851845|location=|pages=}}</ref> This can be used to exclude a parametric family distribution from being an exponential family. For example, the [[Pareto distribution|Pareto]] [[Pareto distribution|distribution]] has a pdf which is defined for <math>x \\geq x_m </math> (<math>x_m</math> being the scale parameter) and its support, therefore, has a lower limit of <math>x_m </math>. Since the support of <math>f_{\\alpha, x_m}(x)</math> is dependent on the value of the parameter, the family of [[Pareto distribution]]s does not form an exponential family of distributions.\n\nNote that ''x'' is often a vector of measurements, in which case ''T''(''x'') may be a function from the space of possible values of ''x'' to the real numbers. More generally, ''η''(''θ'') and ''T''(''x'') can each be vector-valued such that <math>\\eta(\\theta)'\\cdot T(x)</math> is real-valued.\n\nIf ''η''(''θ'')&nbsp;=&nbsp;''θ'', then the exponential family is said to be in ''[[canonical form]]''. By defining a transformed parameter ''η''&nbsp;=&nbsp;''η''(''θ''), it is always possible to convert an exponential family to canonical form. The canonical form is non-unique, since ''η''(''θ'') can be multiplied by any nonzero constant, provided that ''T''(''x'') is multiplied by that constant's reciprocal, or a constant ''c'' can be added to ''η''(''θ'') and ''h''(''x'') multiplied by <math>\\exp (-c \\cdot T(x)) </math> to offset it.\n\nEven when ''x'' is a scalar, and there is only a single parameter, the functions ''η''(''θ'') and ''T''(''x'') can still be vectors, as described below.\n\nNote also that the function ''A''(''θ''), or equivalently ''g''(''θ''), is automatically determined once the other functions have been chosen, since it must assume a form that causes the distribution to be [[normalizing constant|normalized]] (sum or integrate to one over the entire domain).  Furthermore, both of these functions can always be written as functions of ''η'', even when ''η''(''θ'') is not a [[bijection|one-to-one]] function, i.e. two or more different values of ''θ'' map to the same value of ''η''(''θ''), and hence ''η''(''θ'') cannot be inverted.  In such a case, all values of ''θ'' mapping to the same ''η''(''θ'') will also have the same value for ''A''(''θ'') and ''g''(''θ'').\n\n=== Factorization of the variables involved ===\nWhat is important to note, and what characterizes all exponential family variants, is that the parameter(s) and the observation variable(s) must [[factorize]] (can be separated into products each of which involves only one type of variable), either directly or within either part (the base or exponent) of an [[exponentiation]] operation.  Generally, this means that all of the factors constituting the density or mass function must be of one of the following forms:\n\n: <math>f(x), g(\\theta), c^{f(x)}, c^{g(\\theta)}, {[f(x)]}^c, {[g(\\theta)]}^c, {[f(x)]}^{g(\\theta)}, {[g(\\theta)]}^{f(x)}, {[f(x)]}^{h(x)g(\\theta)}, \\text{ or } {[g(\\theta)]}^{h(x)j(\\theta)},</math>\n\nwhere ''f'' and ''h'' are arbitrary functions of ''x''; ''g'' and ''j'' are arbitrary functions of ''θ''; and ''c'' is an arbitrary \"constant\" expression (i.e. an expression not involving ''x'' or ''θ'').\n\nThere are further restrictions on how many such factors can occur.  For example, the two expressions:\n\n: <math>{[f(x) g(\\theta)]}^{h(x)j(\\theta)}, \\qquad {[f(x)]}^{h(x)j(\\theta)} [g(\\theta)]^{h(x)j(\\theta)},</math>\n\nare the same, i.e. a product of two \"allowed\" factors.  However, when rewritten into the factorized form,\n\n: <math>{[f(x) g(\\theta)]}^{h(x)j(\\theta)} = {[f(x)]}^{h(x)j(\\theta)} [g(\\theta)]^{h(x)j(\\theta)} = e^{[h(x) \\log f(x)] j(\\theta) + h(x) [j(\\theta) \\log g(\\theta)]},</math>\n\nit can be seen that it cannot be expressed in the required form. (However, a form of this sort is a member of a ''curved exponential family'', which allows multiple factorized terms in the exponent.{{citation needed|date=June 2011}})\n\nTo see why an expression of the form\n\n: <math>{[f(x)]}^{g(\\theta)}</math>\n\nqualifies, note that\n\n: <math>{[f(x)]}^{g(\\theta)} = e^{g(\\theta) \\log f(x)}</math>\n\nand hence factorizes inside of the exponent. Similarly,\n\n: <math>{[f(x)]}^{h(x)g(\\theta)} = e^{h(x)g(\\theta)\\log f(x)} =  e^{[h(x) \\log f(x)] g(\\theta)}</math>\n\nand again factorizes inside of the exponent.\n\nNote also that a factor consisting of a sum where both types of variables are involved (e.g. a factor of the form <math>1+f(x)g(\\theta)</math>) cannot be factorized in this fashion (except in some cases where occurring directly in an exponent); this is why, for example, the [[Cauchy distribution]] and [[Student's t distribution|Student's ''t'' distribution]] are not exponential families.\n\n=== Vector parameter ===\nThe definition in terms of one ''real-number'' parameter can be extended to one ''real-vector'' parameter\n\n: <math>{\\boldsymbol \\theta} = \\left(\\theta_1, \\theta_2, \\ldots, \\theta_s \\right )^T.</math>\n\nA family of distributions is said to belong to a vector exponential family if the probability density function (or probability mass function, for discrete distributions) can be written as\n\n: <math> f_X(x\\mid\\boldsymbol \\theta) = h(x) \\exp\\left(\\sum_{i=1}^s \\eta_i({\\boldsymbol \\theta}) T_i(x) - A({\\boldsymbol \\theta}) \\right)</math>\n\nOr in a more compact form,\n\n: <math> f_X(x\\mid\\boldsymbol \\theta) = h(x) \\exp\\Big(\\boldsymbol\\eta({\\boldsymbol \\theta}) \\cdot \\mathbf{T}(x) - A({\\boldsymbol \\theta}) \\Big) </math>\n\nThis form writes the sum as a [[dot product]] of vector-valued functions <math>\\boldsymbol\\eta({\\boldsymbol \\theta})</math> and <math>\\mathbf{T}(x)</math>.\n\nAn alternative, equivalent form often seen is\n\n:<math> f_X(x\\mid\\boldsymbol \\theta) = h(x) g(\\boldsymbol \\theta) \\exp\\Big(\\boldsymbol\\eta({\\boldsymbol \\theta}) \\cdot \\mathbf{T}(x)\\Big)</math>\n\nAs in the scalar valued case, the exponential family is said to be in canonical form if\n\n:<math>\\forall i: \\quad \\eta_i({\\boldsymbol \\theta}) = \\theta_i.</math>\n\nA vector exponential family is said to be ''curved'' if the dimension of\n\n: <math>{\\boldsymbol \\theta} = \\left (\\theta_1, \\theta_2, \\ldots, \\theta_d \\right )^T</math>\n\nis less than the dimension of the vector\n\n: <math>{\\boldsymbol \\eta}(\\boldsymbol \\theta) = \\left (\\eta_1(\\boldsymbol \\theta), \\eta_2(\\boldsymbol \\theta), \\ldots, \\eta_s(\\boldsymbol \\theta) \\right )^T.</math>\n\nThat is, if the ''dimension'' of the parameter vector is less than the ''number of functions'' of the parameter vector in the above representation of the probability density function.  Note that most common distributions in the exponential family are ''not'' curved, and many algorithms designed to work with any exponential family implicitly or explicitly assume that the distribution is not curved.\n\nNote that, as in the above case of a scalar-valued parameter, the function <math>A(\\boldsymbol \\theta)</math> or equivalently <math>g(\\boldsymbol \\theta)</math> is automatically determined once the other functions have been chosen, so that the entire distribution is normalized.  In addition, as above, both of these functions can always be written as functions of <math>\\boldsymbol\\eta</math>, regardless of the form of the transformation that generates <math>\\boldsymbol\\eta</math> from <math>\\boldsymbol\\theta</math>.  Hence an exponential family in its \"natural form\" (parametrized by its natural parameter) looks like\n\n: <math> f_X(x\\mid\\boldsymbol \\eta) = h(x) \\exp\\Big(\\boldsymbol\\eta \\cdot \\mathbf{T}(x) - A({\\boldsymbol \\eta})\\Big)</math>\n\nor equivalently\n\n: <math> f_X(x\\mid\\boldsymbol \\eta) = h(x) g(\\boldsymbol \\eta) \\exp\\Big(\\boldsymbol\\eta \\cdot \\mathbf{T}(x)\\Big)</math>\n\nNote that the above forms may sometimes be seen with <math>\\boldsymbol\\eta^T \\mathbf{T}(x)</math> in place of <math>\\boldsymbol\\eta \\cdot \\mathbf{T}(x)</math>. These are exactly equivalent formulations, merely using different notation for the [[dot product]].\n\n=== Vector parameter, vector variable ===\nThe vector-parameter form over a single scalar-valued random variable can be trivially expanded to cover a joint distribution over a vector of random variables.  The resulting distribution is simply the same as the above distribution for a scalar-valued random variable with each occurrence of the scalar ''x'' replaced by the vector\n\n:<math>\\mathbf{x} = \\left (x_1, x_2, \\cdots, x_k \\right).</math>\n\nNote that the dimension ''k'' of the random variable need not match the dimension ''d'' of the parameter vector, nor (in the case of a curved exponential function) the dimension ''s'' of the natural parameter <math>\\boldsymbol\\eta</math> and [[sufficient statistic]] ''T''('''x''').\n\nThe distribution in this case is written as\n\n: <math>f_X(\\mathbf{x}\\mid\\boldsymbol \\theta) = h(\\mathbf{x})\\exp\\left(\\sum_{i=1}^s \\eta_i({\\boldsymbol \\theta}) T_i(\\mathbf{x}) - A({\\boldsymbol \\theta}) \\right)</math>\n\nOr more compactly as\n\n: <math> f_X(\\mathbf{x}\\mid\\boldsymbol \\theta) = h(\\mathbf{x}) \\exp\\Big(\\boldsymbol\\eta({\\boldsymbol \\theta}) \\cdot \\mathbf{T}(\\mathbf{x}) - A({\\boldsymbol \\theta})\\Big)</math>\n\nOr alternatively as\n\n: <math> f_X(\\mathbf{x}\\mid\\boldsymbol \\theta) = h(\\mathbf{x})\\ g(\\boldsymbol \\theta)\\ \\exp\\Big(\\boldsymbol\\eta({\\boldsymbol \\theta}) \\cdot \\mathbf{T}(\\mathbf{x})\\Big)</math>\n\n=== Measure-theoretic formulation ===\nWe use [[cumulative distribution function]]s (cdf) in order to encompass both discrete and continuous distributions.\n\nSuppose ''H'' is a non-decreasing function of a real variable.  Then [[Lebesgue–Stieltjes integral]]s with respect to ''dH''(''x'') are integrals with respect to the \"reference measure\" of the exponential family generated by ''H''.\n\nAny member of that exponential family has cumulative distribution function\n\n:<math>dF(\\mathbf{x}\\mid\\boldsymbol\\eta) = e^{\\boldsymbol\\eta^{\\rm T} \\mathbf{T}(\\mathbf{x}) - A(\\boldsymbol\\eta)} dH(\\mathbf{x}).</math>\n\nIf ''F'' is a continuous distribution with a density, one can write ''dF''(''x'') = ''f''(''x'')&nbsp;''dx''.\n\n''H''(''x'') is a [[Lebesgue–Stieltjes integral|Lebesgue–Stieltjes integrator]] for the ''reference measure''. When the reference measure is finite, it can be normalized and ''H'' is actually the [[cumulative distribution function]] of a probability distribution.  If ''F'' is absolutely continuous with a density, then so is ''H'', which can then be written ''dH''(''x'') = ''h''(''x'')&nbsp;''dx''. If ''F'' is discrete, then ''H'' is a [[step function]] (with steps on the [[support (mathematics)|support]] of ''F'').\n\n== Interpretation ==\nIn the definitions above, the functions ''T''(''x''), ''η''(''θ'') and ''A''(''η'') were apparently arbitrarily defined. However, these functions play a significant role in the resulting probability distribution.\n\n* ''T''(''x'') is a ''[[sufficiency (statistics)|sufficient statistic]]'' of the distribution. For exponential families, the sufficient statistic is a function of the data that holds all information the data <math>x</math> provides with regard to the unknown parameter values. This means that, for any data sets <math>x</math> and <math>y</math>, the likelihood ratio is the same (that is, <math>\\frac{f(x;\\theta_1)}{f(x;\\theta_2)} = \\frac{f(y;\\theta_1)}{f(y;\\theta_2)}</math>) if <math>T(x)=T(y)</math>. This is true even if ''x'' and ''y'' are quite different—that is, <math> d(x,y)>0 </math>.  The dimension of ''T''(''x'') equals the number of parameters of ''θ'' and encompasses all of the information regarding the data related to the parameter ''θ''. The sufficient statistic of a set of [[independent identically distributed]] data observations is simply the sum of individual sufficient statistics, and encapsulates all the information needed to describe the [[posterior distribution]] of the parameters, given the data (and hence to derive any desired estimate of the parameters). This important property is further discussed [[#Classical estimation: sufficiency|below]].\n* ''η'' is called the ''natural parameter''. The set of values of ''η'' for which the function <math>f_X(x;\\theta)</math> is finite is called the ''natural parameter space''. It can be shown that the natural parameter space is always [[convex set|convex]].\n* ''A''(''η'') is called the {{anchor|log-partition function}}'''log-[[partition function (mathematics)|partition function]]''' because it is the [[logarithm]] of a [[normalization factor]], without which <math>f_X(x;\\theta)</math> would not be a probability distribution (\"partition function\" is often used in statistics as a synonym of \"normalization factor\"):\n::<math> A(\\eta) =  \\log\\left ( \\int_x h(x) \\exp (\\eta(\\theta) \\cdot T(x)) \\, \\mathrm{d}x \\right )</math>\nThe function ''A'' is important in its own right, because the [[mean]], [[variance]] and other [[moment (mathematics)|moment]]s of the sufficient statistic ''T''(''x'') can be derived simply by differentiating ''A''(''η''). For example, because log(''x'') is one of the components of the sufficient statistic of the [[gamma distribution]], <math>\\operatorname{E}[\\log x]</math> can be easily determined for this distribution using ''A''(''η''). Technically, this is true because \n::<math>K(u\\mid\\eta) = A(\\eta+u) - A(\\eta),</math> \nis the [[cumulant generating function]] of the sufficient statistic.\n\n== Properties ==\nExponential families have a large number of properties that make them extremely useful for statistical analysis.  In many cases, it can be shown that, except in a few exceptional cases, ''only'' exponential families have these properties.  Examples:\n*Exponential families have [[sufficient statistic]]s that can summarize arbitrary amounts of [[independent identically distributed]] data using a fixed number of values.\n*Exponential families have [[conjugate prior]]s, an important property in [[Bayesian statistics]].\n*The [[posterior predictive distribution]] of an exponential-family random variable with a conjugate prior can always be written in closed form (provided that the [[normalizing factor]] of the exponential-family distribution can itself be written in closed form). Note that these distributions are often not themselves exponential families. Common examples of non-exponential families arising from exponential ones are the [[Student's t-distribution|Student's ''t''-distribution]], [[beta-binomial distribution]] and [[Dirichlet-multinomial distribution]].\n*In the mean-field approximation in [[variational Bayes]] (used for approximating the [[posterior distribution]] in large [[Bayesian network]]s), the best approximating posterior distribution of an exponential-family node (a node is a random variable in the context of Bayesian networks) with a conjugate prior is in the same family as the node.<ref>{{cite web|last1=Blei|first1=David|title=Variational Inference|url=https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf|website=Princeton}}</ref>\n\n== Examples ==\nIt is critical, when considering the examples in this section, to remember the discussion above about what it means to say that a \"distribution\" is an exponential family, and in particular to keep in mind that the set of parameters that are allowed to vary is critical in determining whether a \"distribution\" is or is not an exponential family.\n\nThe [[normal distribution|normal]], [[exponential distribution|exponential]], [[log-normal distribution|log-normal]], [[gamma distribution|gamma]], [[chi-squared distribution|chi-squared]], [[beta distribution|beta]], [[Dirichlet distribution|Dirichlet]], [[Bernoulli distribution|Bernoulli]], [[categorical distribution|categorical]], [[Poisson distribution|Poisson]], [[geometric distribution|geometric]], [[inverse Gaussian distribution|inverse Gaussian]], [[von Mises distribution|von Mises]] and [[von Mises-Fisher distribution|von Mises-Fisher]] distributions are all exponential families.\n\nSome distributions are exponential families only if some of their parameters are held fixed.  The family of [[Pareto distribution]]s with a fixed minimum bound ''x''<sub>m</sub> form an exponential family.  The families of [[binomial distribution|binomial]] and [[multinomial distribution|multinomial]] distributions with fixed number of trials ''n'' but unknown probability parameter(s) are exponential families.  The family of [[negative binomial distribution]]s with fixed number of failures (a.k.a. stopping-time parameter) ''r'' is an exponential family.  However, when any of the above-mentioned fixed parameters are allowed to vary, the resulting family is not an exponential family.\n\nAs mentioned above, as a general rule, the [[support (mathematics)|support]] of an exponential family must remain the same across all parameter settings in the family.  This is why the above cases (e.g. binomial with varying number of trials, Pareto with varying minimum bound) are not exponential families — in all of the cases, the parameter in question affects the support (particularly, changing the minimum or maximum possible value).  For similar reasons, neither the [[discrete uniform distribution]] nor [[continuous uniform distribution]] are exponential families as one or both bounds vary. If both bounds are held fixed, the result is a single distribution; this can be considered a zero-dimensional exponential family, and is the only zero-dimensional exponential family with a given support, but this is generally considered too trivial to consider as a family.\n\nThe [[Weibull distribution]] with fixed shape parameter ''k'' is an exponential family.  Unlike in the previous examples, the shape parameter does not affect the support; the fact that allowing it to vary makes the Weibull non-exponential is due rather to the particular form of the Weibull's [[probability density function]] (''k'' appears in the exponent of an exponent).\n\nIn general, distributions that result from a finite or infinite [[mixture distribution|mixture]] of other distributions, e.g. [[mixture model]] densities and [[compound probability distribution]]s, are ''not'' exponential families.  Examples are typical Gaussian [[mixture model]]s as well as many [[heavy-tailed distribution]]s that result from [[compound probability distribution|compounding]] (i.e. infinitely mixing) a distribution with a [[prior distribution]] over one of its parameters, e.g. the [[Student's t-distribution|Student's ''t''-distribution]] (compounding a [[normal distribution]] over a [[gamma distribution|gamma-distributed]] precision prior), and the [[beta-binomial distribution|beta-binomial]] and [[Dirichlet-multinomial distribution|Dirichlet-multinomial]] distributions.  Other examples of distributions that are not exponential families are the [[F-distribution]], [[Cauchy distribution]], [[hypergeometric distribution]] and [[logistic distribution]].\n\nFollowing are some detailed examples of the representation of some useful distribution as exponential families.\n\n=== Normal distribution: unknown mean, known variance ===\nAs a first example, consider a random variable distributed normally with unknown mean ''μ'' and ''known'' variance ''σ''<sup>2</sup>. The probability density function is then\n\n:<math>f_\\sigma(x;\\mu) = \\frac 1 {\\sqrt{2 \\pi \\sigma^2}} e^{-(x-\\mu)^2/(2\\sigma^2)}.</math>\n\nThis is a single-parameter exponential family, as can be seen by setting\n\n:<math>\\begin{align}\nh_\\sigma(x) &= \\frac 1 {\\sqrt{2\\pi\\sigma^2}} e^{-x^2/(2\\sigma^2)} \\\\[4pt]\nT_\\sigma(x) &= \\frac x \\sigma \\\\[4pt]\nA_\\sigma(\\mu) &= \\frac{\\mu^2}{2\\sigma^2}\\\\[4pt]\n\\eta_\\sigma(\\mu) &= \\frac \\mu \\sigma.\n\\end{align}</math>\n\nIf ''σ'' = 1 this is in canonical form, as then&nbsp;''η''(''μ'')&nbsp;=&nbsp;''μ''.\n\n=== Normal distribution: unknown mean and unknown variance ===\nNext, consider the case of a normal distribution with unknown mean and unknown variance. The probability density function is then\n\n:<math>f(x;\\mu,\\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}}.</math>\n\nThis is an exponential family which can be written in canonical form by defining\n\n:<math>\\begin{align} \n\\boldsymbol {\\eta} &= \\left(\\frac{\\mu}{\\sigma^2}, -\\frac{1}{2\\sigma^2} \\right)^{\\rm T} \\\\\nh(x) &= \\frac{1}{\\sqrt{2 \\pi}} \\\\\nT(x) &= \\left( x, x^2 \\right)^{\\rm T} \\\\\nA({\\boldsymbol \\eta}) &= \\frac{\\mu^2}{2 \\sigma^2} + \\log |\\sigma| = -\\frac{\\eta_1^2}{4\\eta_2} + \\frac{1}{2}\\log\\left|\\frac{1}{2\\eta_2} \\right|\n\\end{align}</math>\n\n=== Binomial distribution ===\nAs an example of a discrete exponential family, consider the [[binomial distribution]] with ''known'' number of trials ''n''. The [[probability mass function]] for this distribution is\n:<math>f(x)={n \\choose x}p^x (1-p)^{n-x}, \\quad x \\in \\{0, 1, 2, \\ldots, n\\}.</math>\nThis can equivalently be written as\n:<math>f(x)={n \\choose x}\\exp\\left(x \\log\\left(\\frac{p}{1-p}\\right) + n \\log(1-p)\\right),</math>\nwhich shows that the binomial distribution is an exponential family, whose natural parameter is\n:<math>\\eta = \\log\\frac{p}{1-p}.</math>\nThis function of ''p'' is known as [[logit]].\n\n== Table of distributions ==\nThe following table shows how to rewrite a number of common distributions as exponential-family distributions with natural parameters. Refer  to the flashcards<ref>{{cite arXiv\n | last1 = Nielsen\n | first1 = Frank\n | last2 = Garcia\n | first2 = Vincent\n | year = 2009\n | title = Statistical exponential families: A digest with flash cards\n | arxiv =  0911.4863}}</ref> for main exponential families.\n\nFor a scalar variable and scalar parameter, the form is as follows:\n\n:<math> f_X(x\\mid \\theta) = h(x) \\exp\\Big(\\eta({\\theta}) T(x) - A({ \\eta})\\Big) </math>\n\nFor a scalar variable and vector parameter:\n\n:<math> f_X(x\\mid\\boldsymbol \\theta) = h(x) \\exp\\Big(\\boldsymbol\\eta({\\boldsymbol \\theta}) \\cdot \\mathbf{T}(x) - A({\\boldsymbol \\eta})\\Big)</math>\n:<math> f_X(x\\mid\\boldsymbol \\theta) = h(x) g(\\boldsymbol \\theta) \\exp\\Big(\\boldsymbol\\eta({\\boldsymbol \\theta}) \\cdot \\mathbf{T}(x)\\Big)</math>\n\nFor a vector variable and vector parameter:\n\n:<math> f_X(\\mathbf{x}\\mid\\boldsymbol \\theta) = h(\\mathbf{x}) \\exp\\Big(\\boldsymbol\\eta({\\boldsymbol \\theta}) \\cdot \\mathbf{T}(\\mathbf{x}) - A({\\boldsymbol \\eta})\\Big)</math>\n\nThe above formulas choose the functional form of the exponential-family with a log-partition function <math>A({\\boldsymbol \\eta})</math>.  The reason for this is so that the [[#Moments and cumulants of the sufficient statistic|moments of the sufficient statistics]] can be calculated easily, simply by differentiating this function.  Alternative forms involve either parameterizing this function in terms of the normal parameter <math>\\boldsymbol\\theta</math> instead of the natural parameter, and/or using a factor <math>g(\\boldsymbol\\eta)</math> outside of the exponential.  The relation between the latter and the former is:\n:<math>A(\\boldsymbol\\eta) = -\\log g(\\boldsymbol\\eta)</math> \n:<math>g(\\boldsymbol\\eta) = e^{-A(\\boldsymbol\\eta)}</math> \nTo convert between the representations involving the two types of parameter, use the formulas below for writing one type of parameter in terms of the other.\n\n{|class=\"wikitable\"\n! Distribution !! Parameter(s) <math>\\boldsymbol\\theta</math> !! Natural parameter(s) <math>\\boldsymbol\\eta</math> !! Inverse parameter mapping !! Base measure <math>h(x)</math> !! Sufficient statistic <math>T(x)</math> !! Log-partition <math>A(\\boldsymbol\\eta)</math> !! Log-partition <math>A(\\boldsymbol\\theta)</math>\n|-\n| [[Bernoulli distribution]] || p\n| <math>\\log\\frac{p}{1-p}</math>\n*This is the [[logit function]].\n| <math>\\frac{1}{1+e^{-\\eta}} = \\frac{e^\\eta}{1+e^{\\eta}}</math>\n*This is the [[logistic function]].\n| <math> 1 </math>\n| <math> x </math>\n| <math> \\log (1+e^{\\eta})</math>\n| <math> -\\log (1-p)</math>\n|-\n| [[binomial distribution]]<br />with known number of trials ''n'' || p\n| <math>\\log\\frac{p}{1-p}</math>\n| <math>\\frac{1}{1+e^{-\\eta}} = \\frac{e^\\eta}{1+e^{\\eta}}</math>\n| <math> {n \\choose x} </math>\n| <math> x </math>\n| <math> n \\log (1+e^{\\eta})</math>\n| <math> -n \\log (1-p)</math>\n|-\n| [[Poisson distribution]] || &lambda;\n| <math>\\log\\lambda</math>\n| <math>e^\\eta</math>\n| <math> \\frac{1}{x!} </math>\n| <math> x </math>\n| <math> e^{\\eta}</math>\n| <math> \\lambda</math>\n|-\n| [[negative binomial distribution]]<br />with known number of failures ''r'' || p\n| <math>\\log p</math>\n| <math>e^\\eta</math>\n| <math> {x+r-1 \\choose x} </math>\n| <math> x </math>\n| <math> -r \\log (1-e^{\\eta})</math>\n| <math> -r \\log (1-p)</math>\n|-\n| [[exponential distribution]] || &lambda;\n| <math>-\\lambda </math>\n| <math>-\\eta </math>\n| <math> 1 </math>\n| <math> x </math>\n| <math> -\\log(-\\eta)</math>\n| <math> -\\log\\lambda</math>\n|-\n| [[Pareto distribution]]<br />with known minimum value ''x''<sub>m</sub> || &alpha;\n| <math>-\\alpha-1</math>\n| <math>-1-\\eta</math>\n| <math> 1 </math>\n| <math> \\log x </math>\n| <math> -\\log (-1-\\eta) + (1+\\eta) \\log x_{\\mathrm m}</math>\n| <math> -\\log \\alpha - \\alpha \\log x_{\\mathrm m}</math>\n|-\n| [[Weibull distribution]]<br />with known shape ''k'' || &lambda;\n| <math>-\\frac{1}{\\lambda^k}</math>\n| <math>(-\\eta)^{-\\frac{1}{k}}</math>\n| <math> x^{k-1} </math>\n| <math> x^k </math>\n| <math> -\\log(-\\eta) -\\log k</math>\n| <math> k\\log\\lambda -\\log k</math>\n|-\n| [[Laplace distribution]]<br />with known mean ''&mu;'' || b\n| <math>-\\frac{1}{b}</math>\n| <math>-\\frac{1}{\\eta}</math>\n| <math> 1 </math>\n| <math> |x-\\mu| </math>\n| <math> \\log\\left(-\\frac{2}{\\eta}\\right)</math>\n| <math> \\log 2b</math>\n|-\n| [[chi-squared distribution]] || &nu;\n| <math>\\frac{\\nu}{2}-1 </math>\n| <math>2(\\eta+1) </math>\n| <math> e^{-\\frac{x}{2}} </math>\n| <math> \\log x </math>\n| <math> \\log \\Gamma(\\eta+1)+(\\eta+1)\\log 2</math>\n| <math> \\log \\Gamma\\left(\\frac{\\nu}{2}\\right)+\\frac{\\nu}{2}\\log 2</math>\n|-\n| [[normal distribution]]<br />known variance || &mu;\n| <math>\\frac{\\mu}{\\sigma} </math>\n| <math>\\sigma\\eta </math>\n| <math> \\frac{e^{-\\frac{x^2}{2\\sigma^2}}}{\\sqrt{2\\pi}\\sigma}  </math>\n| <math> \\frac{x}{\\sigma} </math>\n| <math> \\frac{\\eta^2}{2}</math>\n| <math> \\frac{\\mu^2}{2\\sigma^2}</math>\n|-\n| [[normal distribution]] || &mu;,&sigma;<sup>2</sup>\n| <math>\\begin{bmatrix} \\dfrac{\\mu}{\\sigma^2} \\\\[10pt] -\\dfrac{1}{2\\sigma^2} \\end{bmatrix} </math>\n| <math>\\begin{bmatrix} -\\dfrac{\\eta_1}{2\\eta_2} \\\\[15pt] -\\dfrac{1}{2\\eta_2} \\end{bmatrix} </math>\n| <math> \\frac{1}{\\sqrt{2\\pi}} </math>\n| <math> \\begin{bmatrix} x \\\\ x^2 \\end{bmatrix} </math>\n| <math> -\\frac{\\eta_1^2}{4\\eta_2} - \\frac12\\log(-2\\eta_2)</math>\n| <math> \\frac{\\mu^2}{2\\sigma^2} + \\log \\sigma</math>\n|-\n| [[lognormal distribution]] || &mu;,&sigma;<sup>2</sup>\n| <math>\\begin{bmatrix} \\dfrac{\\mu}{\\sigma^2} \\\\[10pt] -\\dfrac{1}{2\\sigma^2} \\end{bmatrix} </math>\n| <math>\\begin{bmatrix} -\\dfrac{\\eta_1}{2\\eta_2} \\\\[15pt] -\\dfrac{1}{2\\eta_2} \\end{bmatrix} </math>\n| <math> \\frac{1}{\\sqrt{2\\pi}x} </math>\n| <math> \\begin{bmatrix} \\log x \\\\ (\\log x)^2 \\end{bmatrix} </math>\n| <math> -\\frac{\\eta_1^2}{4\\eta_2} - \\frac12\\log(-2\\eta_2)</math>\n| <math> \\frac{\\mu^2}{2\\sigma^2} + \\log \\sigma</math>\n|-\n| [[inverse Gaussian distribution]] || &mu;,&lambda;\n| <math>\\begin{bmatrix} -\\dfrac{\\lambda}{2\\mu^2} \\\\[15pt] -\\dfrac{\\lambda}{2} \\end{bmatrix} </math>\n| <math>\\begin{bmatrix} \\sqrt{\\dfrac{\\eta_2}{\\eta_1}} \\\\[15pt] -2\\eta_2 \\end{bmatrix} </math>\n| <math> \\frac{1}{\\sqrt{2\\pi}x^{\\frac{3}{2}}} </math>\n| <math> \\begin{bmatrix} x \\\\[5pt] \\dfrac{1}{x} \\end{bmatrix} </math>\n| <math> 2\\sqrt{\\eta_1\\eta_2} -\\frac12\\log(-2\\eta_2)</math>\n| <math> -\\frac{\\lambda}{\\mu} -\\frac12\\log\\lambda</math>\n|-\n| rowspan=2|[[gamma distribution]] || &alpha;,&beta;\n| <math>\\begin{bmatrix} \\alpha-1 \\\\ -\\beta \\end{bmatrix} </math>\n| <math>\\begin{bmatrix} \\eta_1+1 \\\\ -\\eta_2 \\end{bmatrix} </math>\n| rowspan=2|<math> 1 </math>\n| rowspan=2|<math> \\begin{bmatrix} \\log x \\\\ x \\end{bmatrix} </math>\n| rowspan=2|<math> \\log \\Gamma(\\eta_1+1)-(\\eta_1+1)\\log(-\\eta_2)</math>\n| <math> \\log \\Gamma(\\alpha)-\\alpha\\log\\beta</math>\n|-\n| ''k'', ''θ''\n| <math>\\begin{bmatrix} k-1 \\\\[5pt] -\\dfrac{1}{\\theta} \\end{bmatrix} </math>\n| <math>\\begin{bmatrix} \\eta_1+1 \\\\[5pt] -\\dfrac{1}{\\eta_2} \\end{bmatrix} </math>\n| <math> \\log \\Gamma(k)+k\\log\\theta</math>\n|-\n| [[inverse gamma distribution]] || &alpha;,&beta;\n| <math>\\begin{bmatrix} -\\alpha-1 \\\\ -\\beta \\end{bmatrix} </math>\n| <math>\\begin{bmatrix} -\\eta_1-1 \\\\ -\\eta_2 \\end{bmatrix} </math>\n| <math> 1 </math>\n| <math> \\begin{bmatrix} \\log x \\\\ \\frac{1}{x} \\end{bmatrix} </math>\n| <math> \\log \\Gamma(-\\eta_1-1)-(-\\eta_1-1)\\log(-\\eta_2)</math>\n| <math> \\log \\Gamma(\\alpha)-\\alpha\\log\\beta</math>\n|-\n| [[scaled inverse chi-squared distribution]] || &nu;,&sigma;<sup>2</sup>\n| <math>\\begin{bmatrix} -\\dfrac{\\nu}{2}-1 \\\\[10pt] -\\dfrac{\\nu\\sigma^2}{2} \\end{bmatrix} </math>\n| <math>\\begin{bmatrix} -2(\\eta_1+1) \\\\[10pt] \\dfrac{\\eta_2}{\\eta_1+1} \\end{bmatrix} </math>\n| <math> 1 </math>\n| <math>\\begin{bmatrix} \\log x \\\\ \\frac{1}{x} \\end{bmatrix} </math>\n| <math> \\log \\Gamma(-\\eta_1-1)-(-\\eta_1-1)\\log(-\\eta_2)</math>\n| <math> \\log \\Gamma\\left(\\frac{\\nu}{2}\\right)-\\frac{\\nu}{2}\\log\\frac{\\nu\\sigma^2}{2}</math>\n|-\n| [[beta distribution]] || &alpha;,&beta;\n| <math>\\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix} </math>\n| <math>\\begin{bmatrix} \\eta_1 \\\\ \\eta_2 \\end{bmatrix} </math>\n| <math> \\frac{1}{x(1-x)} </math>\n| <math> \\begin{bmatrix} \\log x \\\\ \\log (1-x)  \\end{bmatrix} </math>\n| <math> \\log \\Gamma(\\eta_1) + \\log \\Gamma(\\eta_2) - \\log \\Gamma(\\eta_1+\\eta_2)</math>\n| <math> \\log \\Gamma(\\alpha) + \\log \\Gamma(\\beta) - \\log \\Gamma(\\alpha+\\beta)</math>\n|-\n| [[multivariate normal distribution]] || '''&mu;''','''&Sigma;'''\n| <math>\\begin{bmatrix} \\boldsymbol\\Sigma^{-1}\\boldsymbol\\mu \\\\[5pt] -\\frac12\\boldsymbol\\Sigma^{-1} \\end{bmatrix}</math>\n| <math>\\begin{bmatrix} -\\frac12\\boldsymbol\\eta_2^{-1}\\boldsymbol\\eta_1 \\\\[5pt] -\\frac12\\boldsymbol\\eta_2^{-1} \\end{bmatrix}</math>\n| <math>(2\\pi)^{-\\frac{k}{2}}</math>\n| <math>\\begin{bmatrix} \\mathbf{x} \\\\[5pt] \\mathbf{x}\\mathbf{x}^\\mathrm{T} \\end{bmatrix}</math>\n| <math> -\\frac{1}{4}\\boldsymbol\\eta_1^{\\rm T}\\boldsymbol\\eta_2^{-1}\\boldsymbol\\eta_1 - \\frac12\\log\\left|-2\\boldsymbol\\eta_2\\right|</math>\n| <math> \\frac12\\boldsymbol\\mu^{\\rm T}\\boldsymbol\\Sigma^{-1}\\boldsymbol\\mu + \\frac12 \\log |\\boldsymbol\\Sigma|</math>\n|-\n| [[categorical distribution]] (variant 1) || p<sub>1</sub>,...,p<sub>k</sub><br /><br />where <math>\\textstyle\\sum_{i=1}^k p_i=1</math>\n| <math>\\begin{bmatrix} \\log p_1 \\\\ \\vdots \\\\ \\log p_k \\end{bmatrix}</math>\n| <math>\\begin{bmatrix} e^{\\eta_1} \\\\ \\vdots \\\\ e^{\\eta_k} \\end{bmatrix}</math><br /><br />where <math>\\textstyle\\sum_{i=1}^k e^{\\eta_i}=1</math>\n| <math> 1 </math>\n| <math>\\begin{bmatrix} [x=1] \\\\ \\vdots \\\\ {[x=k]} \\end{bmatrix} </math>\n*<math>[x=i]</math> is the [[Iverson bracket]] (1 if <math>x=i</math>, 0 otherwise).\n| <math> 0</math>\n| <math> 0</math>\n|-\n| [[categorical distribution]] (variant 2) || p<sub>1</sub>,...,p<sub>k</sub><br /><br />where <math>\\textstyle\\sum_{i=1}^k p_i=1</math>\n| <math>\\begin{bmatrix} \\log p_1+C \\\\ \\vdots \\\\ \\log p_k+C \\end{bmatrix}</math>\n| <math>\\begin{bmatrix} \\dfrac{1}{C}e^{\\eta_1} \\\\ \\vdots \\\\ \\dfrac{1}{C}e^{\\eta_k} \\end{bmatrix} =</math><br />\n<math>\\begin{bmatrix} \\dfrac{e^{\\eta_1}}{\\sum_{i=1}^{k}e^{\\eta_i}} \\\\[10pt] \\vdots \\\\[5pt] \\dfrac{e^{\\eta_k}}{\\sum_{i=1}^{k}e^{\\eta_i}} \\end{bmatrix}</math>\n\nwhere <math>\\textstyle\\sum_{i=1}^k e^{\\eta_i}=C</math>\n| <math> 1 </math>\n| <math>\\begin{bmatrix} [x=1] \\\\ \\vdots \\\\ {[x=k]} \\end{bmatrix} </math>\n*<math>[x=i]</math> is the [[Iverson bracket]] (1 if <math>x=i</math>, 0 otherwise).\n| <math> 0</math>\n| <math> 0</math>\n|-\n| [[categorical distribution]] (variant 3) || p<sub>1</sub>,...,p<sub>k</sub><br /><br />where <math>p_k = 1 - \\textstyle\\sum_{i=1}^{k-1} p_i</math>\n| <math>\\begin{bmatrix} \\log \\dfrac{p_1}{p_k} \\\\[10pt] \\vdots \\\\[5pt] \\log \\dfrac{p_{k-1}}{p_k} \\\\[15pt] 0 \\end{bmatrix} =</math><br /><br /><math>\\begin{bmatrix} \\log \\dfrac{p_1}{1-\\sum_{i=1}^{k-1}p_i} \\\\[10pt] \\vdots \\\\[5pt] \\log \\dfrac{p_{k-1}}{1-\\sum_{i=1}^{k-1}p_i} \\\\[15pt] 0 \\end{bmatrix}</math>\n*This is the inverse [[softmax function]], a generalization of the [[logit function]].\n| <math>\\begin{bmatrix} \\dfrac{e^{\\eta_1}}{\\sum_{i=1}^{k}e^{\\eta_i}} \\\\[10pt] \\vdots \\\\[5pt] \\dfrac{e^{\\eta_k}}{\\sum_{i=1}^{k}e^{\\eta_i}} \\end{bmatrix} =</math><br /><br />\n<math>\\begin{bmatrix} \\dfrac{e^{\\eta_1}}{1+\\sum_{i=1}^{k-1}e^{\\eta_i}} \\\\[10pt] \\vdots \\\\[5pt] \\dfrac{e^{\\eta_{k-1}}}{1+\\sum_{i=1}^{k-1}e^{\\eta_i}} \\\\[15pt] \\dfrac{1}{1+\\sum_{i=1}^{k-1}e^{\\eta_i}} \\end{bmatrix}</math>\n*This is the [[softmax function]], a generalization of the [[logistic function]].\n| <math> 1 </math>\n| <math>\\begin{bmatrix} [x=1] \\\\ \\vdots \\\\ {[x=k]} \\end{bmatrix} </math>\n*<math>[x=i]</math> is the [[Iverson bracket]] (1 if <math>x=i</math>, 0 otherwise).\n| <math> \\log \\left(\\sum_{i=1}^{k} e^{\\eta_i}\\right) = \\log \\left(1+\\sum_{i=1}^{k-1} e^{\\eta_i}\\right)\n</math>\n| <math> -\\log p_k = -\\log \\left(1 - \\sum_{i=1}^{k-1} p_i\\right)</math>\n|-\n| [[multinomial distribution]] (variant 1)<br />with known number of trials ''n'' || p<sub>1</sub>,...,p<sub>k</sub><br /><br />where <math>\\textstyle\\sum_{i=1}^k p_i=1</math>\n| <math>\\begin{bmatrix} \\log p_1 \\\\ \\vdots \\\\ \\log p_k \\end{bmatrix}</math>\n| <math>\\begin{bmatrix} e^{\\eta_1} \\\\ \\vdots \\\\ e^{\\eta_k} \\end{bmatrix}</math><br /><br />where <math>\\textstyle\\sum_{i=1}^k e^{\\eta_i}=1</math>\n| <math> \\frac{n!}{\\prod_{i=1}^{k} x_i!} </math>\n| <math>\\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_k \\end{bmatrix} </math>\n| <math> 0</math>\n| <math> 0</math>\n|-\n| [[multinomial distribution]] (variant 2)<br />with known number of trials ''n'' || p<sub>1</sub>,...,p<sub>k</sub><br /><br />where <math>\\textstyle\\sum_{i=1}^k p_i=1</math>\n| <math>\\begin{bmatrix} \\log p_1+C \\\\ \\vdots \\\\ \\log p_k+C \\end{bmatrix}</math>\n| <math>\\begin{bmatrix} \\dfrac{1}{C}e^{\\eta_1} \\\\ \\vdots \\\\ \\dfrac{1}{C}e^{\\eta_k} \\end{bmatrix} =</math><br />\n<math>\\begin{bmatrix} \\dfrac{e^{\\eta_1}}{\\sum_{i=1}^{k}e^{\\eta_i}} \\\\[10pt] \\vdots \\\\[5pt] \\dfrac{e^{\\eta_k}}{\\sum_{i=1}^{k}e^{\\eta_i}} \\end{bmatrix}</math>\n\nwhere <math>\\textstyle\\sum_{i=1}^k e^{\\eta_i}=C</math>\n| <math> \\frac{n!}{\\prod_{i=1}^{k} x_i!} </math>\n| <math>\\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_k \\end{bmatrix} </math>\n| <math> 0</math>\n| <math> 0</math>\n|-\n| [[multinomial distribution]] (variant 3)<br />with known number of trials ''n''\n| p<sub>1</sub>,...,p<sub>k</sub><br /><br />where <math>p_k = 1 - \\textstyle\\sum_{i=1}^{k-1} p_i</math>\n| <math>\\begin{bmatrix} \\log \\dfrac{p_1}{p_k} \\\\[10pt] \\vdots \\\\[5pt] \\log \\dfrac{p_{k-1}}{p_k} \\\\[15pt] 0 \\end{bmatrix} =</math><br /><br /><math>\\begin{bmatrix} \\log \\dfrac{p_1}{1-\\sum_{i=1}^{k-1}p_i} \\\\[10pt] \\vdots \\\\[5pt] \\log \\dfrac{p_{k-1}}{1-\\sum_{i=1}^{k-1}p_i} \\\\[15pt] 0 \\end{bmatrix}</math>\n| <math>\\begin{bmatrix} \\dfrac{e^{\\eta_1}}{\\sum_{i=1}^{k}e^{\\eta_i}} \\\\[10pt] \\vdots \\\\[5pt] \\dfrac{e^{\\eta_k}}{\\sum_{i=1}^{k}e^{\\eta_i}} \\end{bmatrix} =</math><br /><br />\n<math>\\begin{bmatrix} \\dfrac{e^{\\eta_1}}{1+\\sum_{i=1}^{k-1}e^{\\eta_i}} \\\\[10pt] \\vdots \\\\[5pt] \\dfrac{e^{\\eta_{k-1}}}{1+\\sum_{i=1}^{k-1}e^{\\eta_i}} \\\\[15pt] \\dfrac{1}{1+\\sum_{i=1}^{k-1}e^{\\eta_i}} \\end{bmatrix}</math>\n| <math> \\frac{n!}{\\prod_{i=1}^{k} x_i!} </math>\n| <math>\\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_k \\end{bmatrix} </math>\n| <math> n\\log \\left(\\sum_{i=1}^{k} e^{\\eta_i}\\right) = n\\log \\left(1+\\sum_{i=1}^{k-1} e^{\\eta_i}\\right)</math>\n| <math> -n\\log p_k = -n\\log \\left(1 - \\sum_{i=1}^{k-1} p_i\\right)</math>\n|-\n| [[Dirichlet distribution]] || &alpha;<sub>1</sub>,...,&alpha;<sub>k</sub>\n| <math>\\begin{bmatrix} \\alpha_1 \\\\ \\vdots \\\\ \\alpha_k \\end{bmatrix}</math>\n| <math>\\begin{bmatrix} \\eta_1 \\\\ \\vdots \\\\ \\eta_k \\end{bmatrix}</math>\n| <math> \\frac{1}{\\prod_{i=1}^k x_i} </math>\n| <math> \\begin{bmatrix} \\log x_1 \\\\ \\vdots \\\\ \\log x_k \\end{bmatrix} </math>\n| <math> \\sum_{i=1}^k \\log \\Gamma(\\eta_i) - \\log \\Gamma\\left(\\sum_{i=1}^k \\eta_i \\right)\n</math>\n| <math> \\sum_{i=1}^k \\log \\Gamma(\\alpha_i) - \\log \\Gamma\\left(\\sum_{i=1}^k\\alpha_i\\right)\n</math>\n|-\n| rowspan=2|[[Wishart distribution]] || '''V''',n\n| <math>\\begin{bmatrix} -\\frac12\\mathbf{V}^{-1} \\\\[5pt] \\dfrac{n-p-1}{2} \\end{bmatrix}</math>\n| <math>\\begin{bmatrix} -\\frac12{\\boldsymbol\\eta_1}^{-1} \\\\[5pt] 2\\eta_2+p+1 \\end{bmatrix}</math>\n| <math> 1 </math>\n| <math> \\begin{bmatrix} \\mathbf{X} \\\\ \\log|\\mathbf{X}| \\end{bmatrix} </math>\n| rowspan=2|<math>-\\left(\\eta_2+\\frac{p+1}{2}\\right)\\log|-\\boldsymbol\\eta_1|</math><br />\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math>+ \\log\\Gamma_p\\left(\\eta_2+\\frac{p+1}{2}\\right) =</math><br />\n<math>-\\frac{n}{2}\\log|-\\boldsymbol\\eta_1| + \\log\\Gamma_p\\left(\\frac{n}{2}\\right) =</math><br />\n<math>\\left(\\eta_2+\\frac{p+1}{2}\\right)(p\\log 2 + \\log|\\mathbf{V}|)</math><br />\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math>+ \\log\\Gamma_p\\left(\\eta_2+\\frac{p+1}{2}\\right)</math>\n*Three variants with different parameterizations are given, to facilitate computing moments of the sufficient statistics.\n|rowspan=2|<math> \\frac{n}{2}(p\\log 2 + \\log|\\mathbf{V}|) + \\log\\Gamma_p\\left(\\frac{n}{2}\\right)</math>\n|-\n| colspan=5|'''NOTE''': Uses the fact that <math>{\\rm tr}(\\mathbf{A}^{\\rm T}\\mathbf{B}) = \\operatorname{vec}(\\mathbf{A}) \\cdot \\operatorname{vec}(\\mathbf{B}),</math> i.e. the [[trace (linear algebra)|trace]] of a [[matrix product]] is much like a [[dot product]].  The matrix parameters are assumed to be [[vectorization (mathematics)|vectorized]] (laid out in a vector) when inserted into the exponential form. Also, '''V''' and '''X''' are symmetric, so e.g. <math>\\mathbf{V}^{\\rm T} = \\mathbf{V}.</math>\n|-\n| [[inverse Wishart distribution]] || '''&Psi;''',m\n| <math>\\begin{bmatrix} -\\frac12\\boldsymbol\\Psi \\\\[5pt] -\\dfrac{m+p+1}{2} \\end{bmatrix}</math>\n| <math>\\begin{bmatrix} -2\\boldsymbol\\eta_1 \\\\[5pt] -(2\\eta_2+p+1) \\end{bmatrix}</math>\n| <math> 1 </math>\n| <math> \\begin{bmatrix} \\mathbf{X}^{-1} \\\\ \\log|\\mathbf{X}| \\end{bmatrix} </math>\n| <math> \\left(\\eta_2 + \\frac{p + 1}{2}\\right)\\log|-\\boldsymbol\\eta_1|</math><br />\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math> + \\log\\Gamma_p\\left(-\\Big(\\eta_2 + \\frac{p + 1}{2}\\Big)\\right) =</math><br />\n<math> -\\frac{m}{2}\\log|-\\boldsymbol\\eta_1| + \\log\\Gamma_p\\left(\\frac{m}{2}\\right) =</math><br />\n<math> -\\left(\\eta_2 + \\frac{p + 1}{2}\\right)(p\\log 2 - \\log|\\boldsymbol\\Psi|)</math><br />\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math> + \\log\\Gamma_p\\left(-\\Big(\\eta_2 + \\frac{p + 1}{2}\\Big)\\right)</math>\n|<math>\\frac{m}{2}(p\\log 2 - \\log|\\boldsymbol\\Psi|) + \\log\\Gamma_p\\left(\\frac{m}{2}\\right)</math>\n|-\n| [[normal-gamma distribution]] || &alpha;,&beta;,&mu;,&lambda;\n| <math>\\begin{bmatrix} \\alpha-\\frac12 \\\\ -\\beta-\\dfrac{\\lambda\\mu^2}{2} \\\\ \\lambda\\mu \\\\ -\\dfrac{\\lambda}{2}\\end{bmatrix} </math>\n| <math>\\begin{bmatrix} \\eta_1+\\frac12 \\\\ -\\eta_2 + \\dfrac{\\eta_3^2}{4\\eta_4} \\\\ -\\dfrac{\\eta_3}{2\\eta_4} \\\\ -2\\eta_4 \\end{bmatrix} </math>\n| <math> \\dfrac{1}{\\sqrt{2\\pi}} </math>\n| <math> \\begin{bmatrix} \\log \\tau \\\\ \\tau \\\\ \\tau x \\\\ \\tau x^2 \\end{bmatrix} </math>\n| <math> \\log \\Gamma\\left(\\eta_1+\\frac12\\right) - \\frac12\\log\\left(-2\\eta_4\\right) - </math><br />\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math> - \\left(\\eta_1+\\frac12\\right)\\log\\left(-\\eta_2 + \\dfrac{\\eta_3^2}{4\\eta_4}\\right)</math>\n| <math> \\log \\Gamma\\left(\\alpha\\right)-\\alpha\\log\\beta-\\frac12\\log\\lambda</math>\n|}\n\nThe three variants of the [[categorical distribution]] and [[multinomial distribution]] are due to the fact that the parameters <math>p_i</math> are constrained, such that\n\n:<math>\\sum_{i=1}^{k} p_i = 1.</math>\n\nThus, there are only ''k''−1 independent parameters.\n*Variant 1 uses ''k'' natural parameters with a simple relation between the standard and natural parameters; however, only ''k''−1 of the natural parameters are independent, and the set of ''k'' natural parameters is [[nonidentifiable]].  The constraint on the usual parameters translates to a similar constraint on the natural parameters.\n*Variant 2 demonstrates the fact that the entire set of natural parameters is nonidentifiable: Adding any constant value to the natural parameters has no effect on the resulting distribution.  However, by using the constraint on the natural parameters, the formula for the normal parameters in terms of the natural parameters can be written in a way that is independent on the constant that is added.\n*Variant 3 shows how to make the parameters identifiable in a convenient way by setting <math>C = -\\log p_k .</math> This effectively \"pivots\" around ''p<sub>k</sub>'' and causes the last natural parameter to have the constant value of 0.  All the remaining formulas are written in a way that does not access ''p<sub>k</sub>'', so that effectively the model has only ''k''−1 parameters, both of the usual and natural kind.\nNote also that variants 1 and 2 are not actually standard exponential families at all.  Rather they are ''curved exponential families'', i.e. there are ''k''−1 independent parameters embedded in a ''k''-dimensional parameter space.<ref>{{cite journal |first=Kees Jan |last=van Garderen |title=Curved Exponential Models in Econometrics |journal=[[Econometric Theory]] |volume=13 |issue=6 |year=1997 |pages=771–790 |doi=10.1017/S0266466600006253 }}</ref> Many of the standard results for exponential families do not apply to curved exponential families.  An example is the log-partition function ''A''(''x''), which has the value of 0 in the curved cases.  In standard exponential families, the derivatives of this function correspond to the moments (more technically, the [[cumulant]]s) of the sufficient statistics, e.g. the mean and variance.  However, a value of 0 suggests that the mean and variance of all the sufficient statistics are uniformly 0, whereas in fact the mean of the ''i''th sufficient statistic should be ''p<sub>i</sub>''. (This does emerge correctly when using the form of ''A''(''x'') in variant 3.)\n\n== Moments and cumulants of the sufficient statistic ==\n\n=== Normalization of the distribution ===\n\nWe start with the normalization of the probability distribution. In general, any non-negative function ''f''(''x'') that serves as the [[kernel (statistics)|kernel]] of a probability distribution (the part encoding all dependence on ''x'') can be made into a proper distribution by [[normalization factor|normalizing]]: i.e.\n\n:<math>p(x) = \\frac{1}{Z} f(x)</math>\n\nwhere\n\n:<math>Z = \\int_x f(x) \\,dx.</math>\n\nThe factor ''Z'' is sometimes termed the ''normalizer'' or ''[[partition function (mathematics)|partition function]]'', based on an analogy to [[statistical physics]].\n\nIn the case of an exponential family where \n:<math>p(x; \\boldsymbol\\eta) = g(\\boldsymbol\\eta) h(x) e^{\\boldsymbol\\eta \\cdot \\mathbf{T}(x)},</math>\n\nthe kernel is\n:<math>K(x) = h(x) e^{\\boldsymbol\\eta \\cdot \\mathbf{T}(x)}</math>\nand the partition function is\n:<math>Z = \\int_x h(x) e^{\\boldsymbol\\eta \\cdot \\mathbf{T}(x)} \\,dx.</math>\n\nSince the distribution must be normalized, we have\n\n:<math>1 = \\int_x g(\\boldsymbol\\eta) h(x) e^{\\boldsymbol\\eta \\cdot \\mathbf{T}(x)}\\, dx = g(\\boldsymbol\\eta) \\int_x h(x) e^{\\boldsymbol\\eta \\cdot \\mathbf{T}(x)} \\,dx = g(\\boldsymbol\\eta) Z.</math>\n\nIn other words,\n:<math>g(\\boldsymbol\\eta) = \\frac{1}{Z}</math>\nor equivalently\n:<math>A(\\boldsymbol\\eta) = - \\log g(\\boldsymbol\\eta) = \\log Z.</math>\n\nThis justifies calling ''A'' the ''log-normalizer'' or ''log-partition function''.\n\n=== Moment-generating function of the sufficient statistic ===\nNow, the [[moment-generating function]] of ''T''(''x'') is\n\n:<math>M_T(u) \\equiv E[e^{u^{\\rm t} T(x)}\\mid\\eta] = \\int_x h(x) e^{(\\eta+u)^{\\rm t} T(x)-A(\\eta)} \\,dx = e^{A(\\eta + u)-A(\\eta)}</math>\n\nwhere t means transpose, proving the earlier statement that\n\n:<math>K(u\\mid\\eta) = A(\\eta+u) - A(\\eta)</math>\n\nis the [[cumulant generating function]] for ''T''.\n\nAn important subclass of exponential families are the [[natural exponential family|natural exponential families]], which have a similar form for the moment-generating function for the distribution of ''x''.\n\n==== Differential identities for cumulants ====\nIn particular, using the properties of the cumulant generating function,\n\n:<math> \\operatorname{E}(T_{j}) = \\frac{ \\partial A(\\eta) }{ \\partial \\eta_{j} } </math>\n\nand\n\n:<math> \\operatorname{cov}\\left (T_i,T_j \\right) = \\frac{ \\partial^2 A(\\eta) }{ \\partial \\eta_i \\, \\partial \\eta_j }. </math>\n\nThe first two raw moments and all mixed second moments can be recovered from these two identities. Higher-order moments and cumulants are obtained by higher derivatives. This technique is often useful when ''T'' is a complicated function of the data, whose moments are difficult to calculate by integration.\n\nAnother way to see this that does not rely on the theory of [[cumulant]]s is to begin from the fact that the distribution of an exponential family must be normalized, and differentiate.  We illustrate using the simple case of a one-dimensional parameter, but an analogous derivation holds more generally.\n\nIn the one-dimensional case, we have\n:<math>p(x) = g(\\eta) h(x) e^{\\eta T(x)} .</math>\n\nThis must be normalized, so\n\n:<math>1 = \\int_x p(x) \\,dx = \\int_x g(\\eta) h(x) e^{\\eta T(x)} \\,dx = g(\\eta) \\int_x h(x) e^{\\eta T(x)} \\,dx .</math>\n\nTake the [[derivative]] of both sides with respect to ''η'':\n\n:<math>\\begin{align}\n0 &= g(\\eta) \\frac{d}{d\\eta} \\int_x h(x) e^{\\eta T(x)} \\,dx + g'(\\eta)\\int_x h(x) e^{\\eta T(x)} \\,dx \\\\\n&= g(\\eta) \\int_x h(x) \\left(\\frac{d}{d\\eta} e^{\\eta T(x)}\\right) \\,dx + g'(\\eta)\\int_x h(x) e^{\\eta T(x)} \\,dx \\\\\n&= g(\\eta) \\int_x h(x) e^{\\eta T(x)} T(x) \\,dx + g'(\\eta)\\int_x h(x) e^{\\eta T(x)} \\,dx \\\\\n&= \\int_x T(x) g(\\eta) h(x) e^{\\eta T(x)} \\,dx + \\frac{g'(\\eta)}{g(\\eta)}\\int_x g(\\eta) h(x) e^{\\eta T(x)} \\,dx \\\\\n&= \\int_x T(x) p(x) \\,dx + \\frac{g'(\\eta)}{g(\\eta)}\\int_x p(x) \\,dx \\\\\n&= \\operatorname{E}[T(x)] + \\frac{g'(\\eta)}{g(\\eta)} \\\\\n&= \\operatorname{E}[T(x)] + \\frac{d}{d\\eta} \\log g(\\eta)\n\\end{align}</math>\n\nTherefore,\n:<math>\\operatorname{E}[T(x)] = - \\frac{d}{d\\eta} \\log g(\\eta) = \\frac{d}{d\\eta} A(\\eta).</math>\n\n==== Example 1 ====\n\nAs an introductory example, consider the [[gamma distribution]], whose distribution is defined by\n\n:<math>p(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1}e^{-\\beta x}.</math>\n\nReferring to the above table, we can see that the natural parameter is given by\n\n:<math>\\eta_1 = \\alpha-1,</math>\n:<math>\\eta_2 = -\\beta,</math>\n\nthe reverse substitutions are\n\n:<math>\\alpha = \\eta_1+1,</math>\n:<math>\\beta = -\\eta_2,</math>\n\nthe sufficient statistics are <math>(\\log x, x),</math> and the log-partition function is\n\n:<math>A(\\eta_1,\\eta_2) = \\log \\Gamma(\\eta_1+1)-(\\eta_1+1)\\log(-\\eta_2).</math>\n\nWe can find the mean of the sufficient statistics as follows.  First, for ''η''<sub>1</sub>:\n\n:<math>\\begin{align}\n\\operatorname{E}[\\log x] &= \\frac{ \\partial A(\\eta_1,\\eta_2) }{ \\partial \\eta_1 } = \\frac{ \\partial }{ \\partial \\eta_1 } \\left(\\log\\Gamma(\\eta_1+1) - (\\eta_1+1) \\log(-\\eta_2)\\right) \\\\\n&= \\psi(\\eta_1+1) - \\log(-\\eta_2) \\\\\n&= \\psi(\\alpha) - \\log \\beta,\n\\end{align}</math>\n\nWhere <math>\\psi(x)</math> is the [[digamma function]] (derivative of log gamma), and we used the reverse substitutions in the last step.\n\nNow, for ''η''<sub>2</sub>:\n\n:<math>\\begin{align}\n\\operatorname{E}[x] &= \\frac{ \\partial A(\\eta_1,\\eta_2) }{ \\partial \\eta_2 } = \\frac{ \\partial }{ \\partial \\eta_2 } \\left(\\log \\Gamma(\\eta_1+1)-(\\eta_1+1)\\log(-\\eta_2)\\right) \\\\\n&= -(\\eta_1+1)\\frac{1}{-\\eta_2}(-1) = \\frac{\\eta_1+1}{-\\eta_2} \\\\\n&= \\frac{\\alpha}{\\beta},\n\\end{align}</math>\n\nagain making the reverse substitution in the last step.\n\nTo compute the variance of ''x'', we just differentiate again:\n\n:<math>\\begin{align}\n\\operatorname{Var}(x) &= \\frac{\\partial^2 A\\left(\\eta_1,\\eta_2 \\right)}{\\partial \\eta_2^2} = \\frac{\\partial}{\\partial \\eta_2} \\frac{\\eta_1+1}{-\\eta_2} \\\\\n&= \\frac{\\eta_1+1}{\\eta_2^2} \\\\\n&= \\frac{\\alpha}{\\beta^2}.\n\\end{align}</math>\n\nAll of these calculations can be done using integration, making use of various properties of the [[gamma function]], but this requires significantly more work.\n\n==== Example 2 ====\nAs another example consider a real valued random variable ''X'' with density\n\n:<math> p_\\theta (x) = \\frac{ \\theta e^{-x} }{\\left(1 + e^{-x} \\right)^{\\theta + 1} } </math>\n\nindexed by shape parameter <math> \\theta \\in (0,\\infty) </math> (this is called the [[skew-logistic distribution]]). The density can be rewritten as\n\n:<math> \\frac{ e^{-x} } { 1 + e^{-x} } \\exp\\left( -\\theta \\log\\left(1 + e^{-x} \\right) + \\log(\\theta)\\right) </math>\n\nNotice this is an exponential family with natural parameter\n\n:<math> \\eta = -\\theta,</math>\n\nsufficient statistic\n\n:<math> T = \\log\\left (1 + e^{-x} \\right),</math>\n\nand log-partition function\n\n:<math> A(\\eta) = -\\log(\\theta) = -\\log(-\\eta)</math>\n\nSo using the first identity,\n\n:<math> \\operatorname{E}(\\log(1 + e^{-X})) = \\operatorname{E}(T) = \\frac{ \\partial A(\\eta) }{ \\partial \\eta } = \\frac{ \\partial }{ \\partial \\eta } [-\\log(-\\eta)] = \\frac{1}{-\\eta} = \\frac{1}{\\theta}, </math>\n\nand using the second identity\n\n:<math> \\operatorname{var}(\\log\\left(1 + e^{-X} \\right)) = \\frac{ \\partial^2 A(\\eta) }{ \\partial \\eta^2 } = \\frac{ \\partial }{ \\partial \\eta } \\left[\\frac{1}{-\\eta}\\right] = \\frac{1}{(-\\eta)^2} = \\frac{1}{\\theta^2}.</math>\n\nThis example illustrates a case where using this method is very simple, but the direct calculation would be nearly impossible.\n\n==== Example 3 ====\nThe final example is one where integration would be extremely difficult.  This is the case of the [[Wishart distribution]], which is defined over matrices.  Even taking derivatives is a bit tricky, as it involves [[matrix calculus]], but the respective identities are listed in that article.\n\nFrom the above table, we can see that the natural parameter is given by\n\n: <math>\\boldsymbol\\eta_1 = -\\frac12\\mathbf{V}^{-1},</math>\n: <math>\\eta_2 = \\frac{n-p-1}{2},</math>\n\nthe reverse substitutions are\n\n: <math>\\mathbf{V} = -\\frac12{\\boldsymbol\\eta_1}^{-1},</math>\n: <math>n = 2\\eta_2+p+1,</math>\n\nand the sufficient statistics are <math>(\\mathbf{X}, \\log|\\mathbf{X}|).</math>\n\nThe log-partition function is written in various forms in the table, to facilitate differentiation and back-substitution.  We use the following forms:\n\n: <math>A(\\boldsymbol\\eta_1, n) = -\\frac{n}{2}\\log|-\\boldsymbol\\eta_1| + \\log\\Gamma_p\\left(\\frac{n}{2}\\right),</math>\n: <math>A(\\mathbf{V},\\eta_2) = \\left(\\eta_2+\\frac{p+1}{2}\\right)(p\\log 2 + \\log|\\mathbf{V}|) + \\log\\Gamma_p\\left(\\eta_2+\\frac{p+1}{2}\\right).</math>\n\n; Expectation of '''X''' (associated with '''η'''<sub>1</sub>)\nTo differentiate with respect to '''η'''<sub>1</sub>, we need the following [[matrix calculus]] identity:\n\n: <math>\\frac{\\partial \\log |a\\mathbf{X}|}{\\partial \\mathbf{X}} =(\\mathbf{X}^{-1})^{\\rm T}</math>\n\nThen:\n\n: <math>\\begin{align}\n\\operatorname{E}[\\mathbf{X}] &= \\frac{ \\partial A\\left(\\boldsymbol\\eta_1,\\cdots \\right) }{ \\partial \\boldsymbol\\eta_1 } \\\\\n&= \\frac{ \\partial }{ \\partial \\boldsymbol\\eta_1 } \\left[-\\frac{n}{2}\\log|-\\boldsymbol\\eta_1| + \\log\\Gamma_p\\left(\\frac{n}{2}\\right) \\right] \\\\\n&= -\\frac{n}{2}(\\boldsymbol\\eta_1^{-1})^{\\rm T} \\\\\n&= \\frac{n}{2}(-\\boldsymbol\\eta_1^{-1})^{\\rm T} \\\\\n&= n(\\mathbf{V})^{\\rm T} \\\\\n&= n\\mathbf{V}\n\\end{align}</math>\n\nThe last line uses the fact that '''V''' is symmetric, and therefore it is the same when transposed.\n\n;Expectation of log |'''X'''| (associated with ''η''<sub>2</sub>)\nNow, for ''η''<sub>2</sub>, we first need to expand the part of the log-partition function that involves the [[multivariate gamma function]]:\n\n: <math> \\log \\Gamma_p(a)= \\log \\left(\\pi^{\\frac{p(p-1)}{4}}\\prod_{j=1}^p \\Gamma\\left(a+\\frac{1-j}{2}\\right)\\right) = \\frac{p(p-1)}{4} \\log \\pi + \\sum_{j=1}^p \\log \\Gamma\\left[ a+\\frac{1-j}{2}\\right] </math>\n\nWe also need the [[digamma function]]:\n\n: <math>\\psi(x) = \\frac{d}{dx} \\log \\Gamma(x).</math>\n\nThen:\n\n: <math>\\begin{align}\n\\operatorname{E}[\\log |\\mathbf{X}|] &= \\frac{\\partial A\\left (\\ldots,\\eta_2 \\right)}{\\partial \\eta_2} \\\\\n&= \\frac{\\partial}{\\partial \\eta_2} \\left[-\\left(\\eta_2+\\frac{p+1}{2}\\right)(p\\log 2 + \\log|\\mathbf{V}|) + \\log\\Gamma_p\\left(\\eta_2+\\frac{p+1}{2}\\right) \\right] \\\\\n&= \\frac{\\partial}{\\partial \\eta_2} \\left[ \\left(\\eta_2+\\frac{p+1}{2}\\right)(p\\log 2 + \\log|\\mathbf{V}|) + \\frac{p(p-1)}{4} \\log \\pi + \\sum_{j=1}^p \\log \\Gamma\\left(\\eta_2+\\frac{p+1}{2}+\\frac{1-j}{2}\\right) \\right] \\\\\n&= p\\log 2 + \\log|\\mathbf{V}| + \\sum_{j=1}^p \\psi\\left(\\eta_2+\\frac{p+1}{2}+\\frac{1-j}{2}\\right) \\\\\n&= p\\log 2 + \\log|\\mathbf{V}| + \\sum_{j=1}^p \\psi\\left(\\frac{n-p-1}{2}+\\frac{p+1}{2}+\\frac{1-j}{2}\\right) \\\\\n&= p\\log 2 + \\log|\\mathbf{V}| + \\sum_{j=1}^p \\psi\\left(\\frac{n+1-j}{2}\\right)\n\\end{align}</math>\n\nThis latter formula is listed in the [[Wishart distribution#Log-expectation|Wishart distribution]] article.  Both of these expectations are needed when deriving the [[variational Bayes]] update equations in a [[Bayes network]] involving a Wishart distribution (which is the [[conjugate prior]] of the [[multivariate normal distribution]]).\n\nComputing these formulas using integration would be much more difficult.  The first one, for example, would require matrix integration.\n\n== Entropy ==\n=== Relative entropy ===\nThe [[relative entropy]] ([[Kullback–Leibler divergence]], KL divergence) of two distributions in an exponential family has a simple expression as the [[Bregman divergence]] between the natural parameters with respect to the log-normalizer.{{sfn|Nielsen|Nock|2010|loc=4. Bregman Divergences and Relative Entropy of Exponential Families}} The relative entropy is defined in terms of an integral, while the Bregman divergence is defined in terms of a derivative and inner product, and thus is easier to calculate and has a [[closed-form expression]] (assuming the derivative has a closed-form expression). Further, the Bregman divergence in terms of the natural parameters and the log-normalizer equals the Bregman divergence of the dual parameters (expectation parameters), in the opposite order, for the [[convex conjugate]] function.\n\nFixing an exponential family with log-normalizer {{tmath|A}} (with convex conjugate {{tmath|A^*}}), writing <math>P_{A,\\theta}</math> for the distribution in this family corresponding a fixed value of the natural parameter {{tmath|\\theta}} (writing {{tmath|\\theta'}} for another value, and with {{tmath|\\eta, \\eta'}} for the corresponding dual expectation/moment parameters), writing {{math|KL}} for the KL divergence, and {{tmath|B_A}} for the Bregman divergence, the divergences are related as:\n:<math>\\mathrm{KL}(P_{A,\\theta} \\parallel P_{A,\\theta'}) = B_A(\\theta' \\parallel \\theta) = B_{A^*}(\\eta \\parallel \\eta').</math>\n\nThe KL divergence is conventionally written with respect to the ''first'' parameter, while the Bregman divergence is conventionally written with respect to the ''second'' parameter, and thus this can be read as \"the relative entropy is equal to the Bregman divergence defined by the log-normalizer on the swapped natural parameters\", or equivalently as \"equal to the Bregman divergence defined by the dual to the log-normalizer on the expectation parameters\".\n\n=== Maximum entropy derivation ===\nExponential families arise naturally as the answer to the following question: what is the [[principle of maximum entropy|maximum-entropy]] distribution consistent with given constraints on expected values?\n\nThe [[information entropy]] of a probability distribution ''dF''(''x'') can only be computed with respect to some other probability distribution (or, more generally, a positive measure), and both [[measure (mathematics)|measure]]s must be mutually [[absolutely continuous]]. Accordingly, we need to pick a ''reference measure'' ''dH''(''x'') with the same support as ''dF''(''x'').\n\nThe entropy of ''dF''(''x'') relative to ''dH''(''x'') is\n\n:<math>S[dF\\mid dH]=-\\int \\frac{dF}{dH}\\log\\frac{dF}{dH}\\,dH</math>\n\nor\n\n:<math>S[dF\\mid dH]=\\int\\log\\frac{dH}{dF}\\,dF</math>\n\nwhere ''dF''/''dH'' and ''dH''/''dF'' are [[Radon–Nikodym derivative]]s. Note that the ordinary definition of entropy for a discrete distribution supported on a set ''I'', namely\n\n:<math>S=-\\sum_{i\\in I} p_i\\log p_i</math>\n\n''assumes'', though this is seldom pointed out, that ''dH'' is chosen to be the [[counting measure]] on ''I''.\n\nConsider now a collection of observable quantities (random variables) ''T<sub>i</sub>''. The probability distribution ''dF'' whose entropy with respect to ''dH'' is greatest, subject to the conditions that the expected value of ''T<sub>i</sub>'' be equal to ''t<sub>i</sub>'', is an exponential family with ''dH'' as reference measure and (''T''<sub>1</sub>, ..., ''T<sub>n</sub>'') as sufficient statistic.\n\nThe derivation is a simple [[calculus of variations|variational calculation]] using [[Lagrange multipliers]]. Normalization is imposed by letting ''T''<sub>0</sub> = 1 be one of the constraints. The natural parameters of the distribution are the Lagrange multipliers, and the normalization factor is the Lagrange multiplier associated to ''T''<sub>0</sub>.\n\nFor examples of such derivations, see [[Maximum entropy probability distribution]].\n\n== Role in statistics ==\n=== Classical estimation: sufficiency ===\nAccording to the '''[[E. J. G. Pitman|Pitman]]&ndash;[[Bernard Koopman|Koopman]]&ndash;[[Georges Darmois|Darmois]] theorem''', among families of probability distributions whose domain does not vary with the parameter being estimated, only in exponential families is there a [[sufficient statistic]] whose dimension remains bounded as sample size increases.\n\nLess tersely, suppose ''X<sub>k</sub>'', (where ''k'' = 1, 2, 3, ... ''n'') are [[statistical independence|independent]], identically distributed random variables. Only if their distribution is one of the ''exponential family'' of distributions is there a [[sufficient statistic]] '''''T'''''(''X''<sub>1</sub>, ..., ''X<sub>n</sub>'') whose [[dimension|number]] of [[Random variable#Introduction|scalar components]] does not increase as the sample size ''n'' increases; the statistic '''''T''''' may be a [[Multivariate random variable|vector]] or a [[Random variable#Introduction|single scalar number]], but whatever it is, its [[dimension|size]] will neither grow nor shrink when more data are obtained.\n\nAs a counterexample if these conditions are relaxed, note that the family of [[uniform distribution (disambiguation)|uniform distributions]] (either [[Discrete uniform distribution|discrete]] or [[Uniform distribution (continuous)|continuous]], with either or both bounds unknown) has a sufficient statistic, namely the sample maximum, sample minimum, and sample size, but does not form an exponential family, as the domain varies with the parameters.\n\n=== Bayesian estimation: conjugate distributions ===\nExponential families are also important in [[Bayesian statistics]]. In Bayesian statistics a [[prior distribution]] is multiplied by a [[likelihood function]] and then normalised to produce a [[posterior distribution]]. In the case of a likelihood which belongs to an exponential family there exists a [[conjugate prior]], which is often also in an exponential family. A conjugate prior π for the parameter <math>\\boldsymbol\\eta</math> of an exponential family\n\n: <math> f(x\\mid\\boldsymbol\\eta) = h(x) \\exp \\left ( {\\boldsymbol\\eta}^{\\rm T}\\mathbf{T}(x) -A(\\boldsymbol\\eta)\\right )</math>\n\nis given by\n\n: <math>p_\\pi(\\boldsymbol\\eta\\mid\\boldsymbol\\chi,\\nu) = f(\\boldsymbol\\chi,\\nu) \\exp \\left (\\boldsymbol\\eta^{\\rm T} \\boldsymbol\\chi - \\nu A(\\boldsymbol\\eta) \\right ),</math>\n\nor equivalently\n\n:<math>p_\\pi(\\boldsymbol\\eta\\mid\\boldsymbol\\chi,\\nu) = f(\\boldsymbol\\chi,\\nu) g(\\boldsymbol\\eta)^\\nu \\exp \\left (\\boldsymbol\\eta^{\\rm T} \\boldsymbol\\chi \\right ), \\qquad \\boldsymbol\\chi \\in \\mathbb{R}^s</math>\n\nwhere ''s'' is the dimension of <math>\\boldsymbol\\eta</math> and <math>\\nu > 0 </math> and <math>\\boldsymbol\\chi</math> are [[hyperparameter]]s (parameters controlling parameters). <math>\\nu</math> corresponds to the effective number of observations that the prior distribution contributes, and <math>\\boldsymbol\\chi</math> corresponds to the total amount that these pseudo-observations contribute to the [[sufficient statistic]] over all observations and pseudo-observations. <math>f(\\boldsymbol\\chi,\\nu)</math> is a [[normalization constant]] that is automatically determined by the remaining functions and serves to ensure that the given function is a [[probability density function]] (i.e. it is [[Normalization (statistics)|normalized]]). <math>A(\\boldsymbol\\eta)</math> and equivalently <math>g(\\boldsymbol\\eta)</math> are the same functions as in the definition of the distribution over which π is the conjugate prior.\n\nA conjugate prior is one which, when combined with the likelihood and normalised, produces a posterior distribution which is of the same type as the prior. For example, if one is estimating the success probability of a binomial distribution, then if one chooses to use a beta distribution as one's prior, the posterior is another beta distribution. This makes the computation of the posterior particularly simple. Similarly, if one is estimating the parameter of a [[Poisson distribution]] the use of a gamma prior will lead to another gamma posterior. Conjugate priors are often very flexible and can be very convenient. However, if one's belief about the likely value of the theta parameter of a binomial is represented by (say) a bimodal (two-humped) prior distribution, then this cannot be represented by a beta distribution. It can however be represented by using a [[mixture density]] as the prior, here a combination of two beta distributions; this is a form of [[hyperprior]].\n\nAn arbitrary likelihood will not belong to an exponential family, and thus in general no conjugate prior exists. The posterior will then have to be computed by numerical methods.\n\nTo show that the above prior distribution is a conjugate prior, we can derive the posterior.\n\nFirst, assume that the probability of a single observation follows an exponential family, parameterized using its natural parameter:\n\n:<math> p_F(x\\mid\\boldsymbol \\eta) = h(x) g(\\boldsymbol\\eta) \\exp\\left(\\boldsymbol\\eta^{\\rm T} \\mathbf{T}(x)\\right)</math>\n\nThen, for data <math>\\mathbf{X} = (x_1,\\ldots,x_n)</math>, the likelihood is computed as follows:\n\n:<math>p(\\mathbf{X}\\mid\\boldsymbol\\eta) =\\left(\\prod_{i=1}^n h(x_i) \\right) g(\\boldsymbol\\eta)^n \\exp\\left(\\boldsymbol\\eta^{\\rm T}\\sum_{i=1}^n \\mathbf{T}(x_i) \\right)</math>\n\nThen, for the above conjugate prior:\n\n: <math>\\begin{align}p_\\pi(\\boldsymbol\\eta\\mid\\boldsymbol\\chi,\\nu) &= f(\\boldsymbol\\chi,\\nu) g(\\boldsymbol\\eta)^\\nu \\exp(\\boldsymbol\\eta^{\\rm T} \\boldsymbol\\chi) \\propto g(\\boldsymbol\\eta)^\\nu \\exp(\\boldsymbol\\eta^{\\rm T} \\boldsymbol\\chi)\\end{align}</math>\n\nWe can then compute the posterior as follows:\n\n:<math>\\begin{align}\np(\\boldsymbol\\eta\\mid\\mathbf{X},\\boldsymbol\\chi,\\nu)& \\propto p(\\mathbf{X}\\mid\\boldsymbol\\eta) p_\\pi(\\boldsymbol\\eta\\mid\\boldsymbol\\chi,\\nu) \\\\\n&= \\left(\\prod_{i=1}^n h(x_i) \\right) g(\\boldsymbol\\eta)^n \\exp\\left(\\boldsymbol\\eta^{\\rm T} \\sum_{i=1}^n \\mathbf{T}(x_i)\\right)\nf(\\boldsymbol\\chi,\\nu) g(\\boldsymbol\\eta)^\\nu \\exp(\\boldsymbol\\eta^{\\rm T} \\boldsymbol\\chi) \\\\\n&\\propto g(\\boldsymbol\\eta)^n \\exp\\left(\\boldsymbol\\eta^{\\rm T}\\sum_{i=1}^n \\mathbf{T}(x_i)\\right) g(\\boldsymbol\\eta)^\\nu \\exp(\\boldsymbol\\eta^{\\rm T} \\boldsymbol\\chi) \\\\\n&\\propto g(\\boldsymbol\\eta)^{\\nu + n} \\exp\\left(\\boldsymbol\\eta^{\\rm T} \\left(\\boldsymbol\\chi + \\sum_{i=1}^n \\mathbf{T}(x_i)\\right)\\right)\n\\end{align}</math>\n\nThe last line is the [[kernel (statistics)|kernel]] of the posterior distribution, i.e.\n\n: <math>p(\\boldsymbol\\eta\\mid\\mathbf{X},\\boldsymbol\\chi,\\nu) = p_\\pi\\left(\\boldsymbol\\eta\\mid\\boldsymbol\\chi + \\sum_{i=1}^n \\mathbf{T}(x_i), \\nu + n \\right)</math>\n\nThis shows that the posterior has the same form as the prior.\n\nNote in particular that the data '''X''' enters into this equation ''only'' in the expression\n\n: <math>\\mathbf{T}(\\mathbf{X}) = \\sum_{i=1}^n \\mathbf{T}(x_i),</math>\n\nwhich is termed the [[sufficient statistic]] of the data.  That is, the value of the sufficient statistic is sufficient to completely determine the posterior distribution.  The actual data points themselves are not needed, and all sets of data points with the same sufficient statistic will have the same distribution.  This is important because the dimension of the sufficient statistic does not grow with the data size — it has only as many components as the components of <math>\\boldsymbol\\eta</math> (equivalently, the number of parameters of the distribution of a single data point).\n\nThe update equations are as follows:\n\n: <math>\\begin{align}\n\\boldsymbol\\chi' &= \\boldsymbol\\chi + \\mathbf{T}(\\mathbf{X}) \\\\\n&= \\boldsymbol\\chi + \\sum_{i=1}^n \\mathbf{T}(x_i) \\\\\n\\nu' &= \\nu + n\n\\end{align} </math>\n\nThis shows that the update equations can be written simply in terms of the number of data points and the [[sufficient statistic]] of the data.  This can be seen clearly in the various examples of update equations shown in the [[conjugate prior]] page.  Note also that because of the way that the sufficient statistic is computed, it necessarily involves sums of components of the data (in some cases disguised as products or other forms — a product can be written in terms of a sum of [[logarithm]]s). The cases where the update equations for particular distributions don't exactly match the above forms are cases where the conjugate prior has been expressed using a different [[parameterization]] than the one that produces a conjugate prior of the above form — often specifically because the above form is defined over the natural parameter <math>\\boldsymbol\\eta</math> while conjugate priors are usually defined over the actual parameter <math>\\boldsymbol\\theta .</math>\n\n=== Hypothesis testing: uniformly most powerful tests ===\n{{further|Uniformly most powerful test#Important case: exponential family|l1=Uniformly most powerful test}}\n\nA one-parameter exponential family has a monotone non-decreasing likelihood ratio in the [[Sufficiency (statistics)|sufficient statistic]] ''T''(''x''), provided that ''η''(''θ'') is non-decreasing. As a consequence, there exists a [[uniformly most powerful test]] for [[hypothesis testing|testing the hypothesis]] ''H''<sub>0</sub>: ''θ'' ≥ ''θ''<sub>0</sub>  ''vs''. ''H''<sub>1</sub>: ''θ'' < ''θ''<sub>0</sub>.\n\n=== Generalized linear models ===\nExponential families form the basis for the distribution functions used in [[generalized linear model]]s, a class of model that encompass many of the commonly used regression models in statistics.\n\n== See also ==\n* [[Natural exponential family]]\n* [[Exponential dispersion model]]\n* [[Gibbs measure]]\n\n== Notes ==\n{{NoteFoot}}\n\n== References ==\n=== Citations ===\n{{Reflist}}\n\n{{more footnotes|date=November 2010}}\n\n=== Sources ===\n{{refbegin}}\n* {{cite article\n  | last1 = Nielsen | first1 = Frank\n  | last2 = Garcia | first2 = Vincent\n  | title = Statistical exponential families: A digest with flash cards\n  | arxiv = 0911.4863\n  | year = 2009 |bibcode = 2009arXiv0911.4863N\n  | ref = harv\n  }}\n* {{cite conference\n| last1 = Nielsen | first1 = Frank\n| last2 =  Nock | first2 = Richard\n| year = 2010\n| title = Entropies and cross-entropies of exponential families\n| conference = IEEE International Conference on Image Processing\n| doi = 10.1109/ICIP.2010.5652054\n| url = https://www.lix.polytechnique.fr/~nielsen/EntropyEF-ICIP2010.pdf\n| archive-url = https://web.archive.org/web/20190331194854/https://www.lix.polytechnique.fr/~nielsen/EntropyEF-ICIP2010.pdf\n| archive-date = 2019-03-31\n| ref = harv\n}}\n{{refend}}\n\n== Further reading ==\n* {{cite book\n  | last = Fahrmeir\n  | first = Ludwig\n  |author2=Tutz, G.\n  | title = Multivariate Statistical Modelling based on Generalized Linear Models\n  | publisher = Springer\n  | year = 1994\n  | isbn = 0-387-94233-5\n  | pages = 18&ndash;22, 345&ndash;349 }}\n* {{cite book\n  | last = Keener\n  | first = Robert W.\n  | title = Theoretical Statistics: Topics for a Core Course \n  | publisher = Springer\n  | year = 2006\n  | isbn = 978-0-387-93838-7\n  | pages = 27&ndash;28, 32&ndash;33 }}\n* {{cite book\n  | last = Lehmann\n  | first = E. L.\n  |author2=Casella, G.\n  | title = Theory of Point Estimation\n  | year = 1998\n  | edition = 2nd\n  | isbn = 0-387-98502-6\n  | at = sec. 1.5 }}\n\n==External links==\n* [http://www.casact.org/pubs/dpp/dpp04/04dpp117.pdf A primer on the exponential family of distributions]\n* [http://jeff560.tripod.com/e.html Exponential family of distributions] on the [http://jeff560.tripod.com/mathword.html Earliest known uses of some of the words of mathematics]\n* [https://vincentfpgarcia.github.com/jMEF/ jMEF: A Java library for exponential families]\n{{Statistics|inference|collapsed}}\n{{ProbDistributions|families|collapsed}}\n\n{{DEFAULTSORT:Exponential Family}}\n[[Category:Exponentials]]\n[[Category:Continuous distributions]]\n[[Category:Discrete distributions]]\n[[Category:Types of probability distributions]]"
    },
    {
      "title": "Exponential formula",
      "url": "https://en.wikipedia.org/wiki/Exponential_formula",
      "text": "In [[combinatorics|combinatorial]] [[mathematics]], the '''exponential formula''' (called the '''polymer expansion''' in [[physics]]) states that the exponential generating function for structures on finite sets is the exponential of the exponential generating function for connected structures.\nThe exponential formula is a power-series version of a special case of [[Faà di Bruno's formula]].\n\n==Statement==\nFor any [[formal power series]] of the form\n\n:<math>f(x)=a_1 x+{a_2 \\over 2}x^2+{a_3 \\over 6}x^3+\\cdots+{a_n \\over n!}x^n+\\cdots\\,</math>\n\nwe have\n\n:<math>\\exp f(x)=e^{f(x)}=\\sum_{n=0}^\\infty {b_n \\over n!}x^n,\\,</math>\n\nwhere\n\n:<math>b_n=\\sum_{\\pi=\\left\\{\\,S_1,\\,\\dots,\\,S_k\\,\\right\\}} a_{\\left|S_1\\right|}\\cdots a_{\\left|S_k\\right|}, </math>\n\nand the index {{pi}} runs through the list of all [[partition of a set|partitions]] { ''S''<sub>1</sub>, ..., ''S''<sub>''k''</sub> } of the set { 1, ..., ''n'' }. (When ''k''&nbsp;=&nbsp;0, the product is [[Empty product|empty]] and by definition equals 1.)\n\nOne can write the formula in the following form:\n:<math>b_n = B_n(a_1,a_2,\\dots,a_n),</math>\nand thus\n:<math>\\exp\\left(\\sum_{n=1}^\\infty {a_n \\over n!} x^n \\right) = \\sum_{n=0}^\\infty {B_n(a_1,\\dots,a_n) \\over n!} x^n,</math>\nwhere ''B''<sub>''n''</sub>(''a''<sub>1</sub>, ..., ''a''<sub>''n''</sub>) is the ''n''th complete [[Bell polynomial]].\n\n==Examples==\n* <math>b_3=B_3(a_1,a_2,a_3)=a_3+3a_2 a_1 + a_1^3,</math> because there is one partition of the set { 1, 2, 3 } that has a single block of size 3, there are three partitions of { 1, 2, 3 } that split it into a block of size 2 and a block of size 1, and there is one partition of { 1, 2, 3 } that splits it into three blocks of size 1.\n* If ''b''<sub>''n''</sub> = 2<sup>''n''(''n''&minus;1)/2</sup> is the number of graphs whose vertices are a given ''n''-point set, then ''a''<sub>''n''</sub> is the number of connected graphs whose vertices are a given ''n''-point set.\n*There are numerous variations of the previous example where the graph has certain properties: for example, if ''b''<sub>''n''</sub> counts graphs without cycles, then ''a''<sub>''n''</sub> counts trees (connected graphs without cycles).\n*If ''b''<sub>''n''</sub> counts directed graphs whose ''edges'' (rather than vertices) are a given ''n'' point set, then ''a''<sub>''n''</sub> counts connected directed graphs with this edge s\n\n==Applications==\nIn applications, the numbers ''a''<sub>''n''</sub> often count the number of some sort of \"connected\" structure on an ''n''-point set, and the numbers ''b''<sub>''n''</sub> count the number of (possibly disconnected) structures. The numbers ''b''<sub>''n''</sub>/''n''! count the number of isomorphism classes of structures on ''n'' points, with each structure being weighted by the reciprocal of its automorphism group, and the numbers ''a''<sub>''n''</sub>/''n''! count isomorphism classes of connected structures in the same way.\n\nIn quantum field theory and statistical mechanics, the [[partition function (mathematics)|partition function]]s ''Z'', or more generally [[correlation function]]s, are given by a formal sum over [[Feynman diagram]]s. The exponential formula shows that log(''Z'') can be written as a sum over connected Feynman diagrams, in terms of [[connected correlation function]]s.\n\n==References==\n*{{Citation | authorlink=Richard P. Stanley | last1=Stanley | first1=Richard P. | title=Enumerative combinatorics. Vol. 2 | url=http://www-math.mit.edu/~rstan/ec/ | publisher=[[Cambridge University Press]] | series=Cambridge Studies in Advanced Mathematics | isbn=978-0-521-56069-6 | id={{ISBN|978-0-521-78987-5}} | mr=1676282 | year=1999 | volume=62}} Chapter 5\n\n[[Category:Exponentials]]\n[[Category:Enumerative combinatorics]]"
    },
    {
      "title": "Exponential growth",
      "url": "https://en.wikipedia.org/wiki/Exponential_growth",
      "text": "{{Short description|Growth of quantities at rate proportional to the current amount\n}}\n[[File:Exponential.svg|thumb|300px|right|The graph illustrates how exponential growth (green) surpasses both linear (red) and cubic (blue) growth. {{legend|green|Exponential growth}} {{legend|blue|[[Polynomial|Cubic growth]]}} {{legend|red|[[Linear growth]]}}]]\n\n'''Exponential growth''' is exhibited when the [[Rate (mathematics)#Of change|rate of change]]—the change per instant or unit of time—of the value of a mathematical function of time is [[proportionality (mathematics)|proportional]] to the function's current value, resulting in its value at any time being an [[exponential function]] of time, i.e., a function in which the time value is the exponent.\n[[Exponential decay]] occurs in the same way when the growth rate is negative.  In the case of a discrete [[Domain of a function|domain]] of definition with equal intervals, it is also called '''geometric growth''' or '''geometric decay''', the function values forming a [[geometric progression]].  In either exponential growth or exponential decay, the ratio of the rate of change of the quantity to its current size remains constant over time.\n\nThe formula for exponential growth of a variable ''x'' at the growth rate ''r'', as time ''t'' goes on in discrete intervals (that is, at integer times&nbsp;0,&nbsp;1,&nbsp;2,&nbsp;3,&nbsp;...), is\n\n:<math>x_t = x_0(1+r)^t</math>\n\nwhere ''x''<sub>0</sub> is the value of ''x'' at time 0. This formula is transparent when the exponents are converted to multiplication.  For instance, with a starting value of 50 and a growth rate of {{nowrap|1=''r'' = 5% = 0.05}} per interval, the passage of one interval would give {{nowrap|1=50 × 1.05<sup>1</sup> = 50 × 1.05}}; two intervals would give {{nowrap|1=50 × 1.05<sup>2</sup> = 50 × 1.05 × 1.05}}; and three intervals would give {{nowrap|1=50 × 1.05<sup>3</sup> = 50 × 1.05 × 1.05 × 1.05}}.  In this way, each increase in the exponent by a full interval can be seen to increase the previous total by another five percent.  (The order of multiplication does not change the result based on the [[associative property]] of multiplication.)\n\nSince the time variable, which is the input to this function, occurs as the exponent, this is an [[exponential function]].  This contrasts with growth based on a [[power function]], where the time variable is the base value raised to a fixed exponent, such as cubic growth (or in general terms denoted as [[Growth_rate_(group_theory)|polynomial growth]]).\n\n==Examples==\n[[File:e.coli-colony-growth.gif|right|frame|150px|[[Bacteria]] exhibit exponential growth under optimal conditions.]]\n{{refimprove section|date=August 2013}}\n===Biology===\n* The number of [[microorganism]]s in a [[microbiological culture|culture]] will increase exponentially until an essential nutrient is exhausted. Typically the first organism [[cell division|splits]] into two daughter organisms, who then each split to form four, who split to form eight, and so on. Because exponential growth indicates constant growth rate, it is frequently assumed that exponentially growing cells are at a steady-state. However, cells can grow exponentially at a constant rate while remodeling their metabolism  and gene expression.<ref name=\"SlavovBudnik2014\">{{cite journal|last1=Slavov|first1=Nikolai|last2=Budnik|first2=Bogdan A.|last3=Schwab|first3=David|last4=Airoldi|authorlink4=Edoardo Airoldi|first4=Edoardo M.|last5=van Oudenaarden|first5=Alexander|title=Constant Growth Rate Can Be Supported by Decreasing Energy Flux and Increasing Aerobic Glycolysis|journal=Cell Reports|volume=7|issue=3|year=2014|pages=705–714|issn=2211-1247|doi=10.1016/j.celrep.2014.03.057|pmid=24767987|pmc=4049626}}</ref>    \n* A virus (for example [[SARS]], or [[smallpox]]) typically will spread exponentially at first, if no artificial [[immunization]] is available. Each infected person can infect multiple new people.\n* [[World population|Human population]], if the number of births and deaths per person per year were to remain at current levels (but also see [[logistic growth]]). For example, according to the [[United States Census Bureau]], over the last 100 years (1910 to 2010), the population of the United States of America is exponentially increasing at an average rate of one and a half percent a year (1.5%). This means that the [[doubling time]] of the American population (depending on the yearly growth in population) is approximately 50 years.<ref>2010 Census Data, \"U.S. Census Bureau\", 20 Dec 2012, Internet Archive: https://web.archive.org/web/20121220035511/http://2010.census.gov/2010census/data/index.php</ref>\n===Physics===\n* [[Avalanche breakdown]] within a [[dielectric]] material. A free [[electron]] becomes sufficiently accelerated by an externally applied [[electrical field]] that it frees up additional electrons as it collides with [[atom]]s or [[molecule]]s of the dielectric media. These ''secondary'' electrons also are accelerated, creating larger numbers of free electrons. The resulting exponential growth of electrons and ions may rapidly lead to complete [[dielectric breakdown]] of the material.\n* [[Nuclear chain reaction]] (the concept behind [[nuclear reactors]] and [[nuclear weapons]]). Each [[uranium]] [[atomic nucleus|nucleus]] that undergoes [[Nuclear fission|fission]] produces multiple [[neutron]]s, each of which can be [[absorption (chemistry)|absorbed]] by adjacent uranium atoms, causing them to fission in turn. If the [[probability]] of neutron absorption exceeds the probability of neutron escape (a [[function (mathematics)|function]] of the [[shape]] and [[mass]] of the uranium), ''k''&nbsp;>&nbsp;0 and so the production rate of neutrons and induced uranium fissions increases exponentially, in an uncontrolled reaction. \"Due to the exponential rate of increase, at any point in the chain reaction 99% of the energy will have been released in the last 4.6 generations. It is a reasonable approximation to think of the first 53 generations as a latency period leading up to the actual explosion, which only takes 3–4 generations.\"<ref>{{cite web|url=http://nuclearweaponarchive.org/Nwfaq/Nfaq2.html|title=Introduction to Nuclear Weapon Physics and Design|publisher=Nuclear Weapons Archive|last=Sublette|first=Carey|accessdate=2009-05-26}}</ref>\n* [[Positive feedback]] within the linear range of electrical or electroacoustic [[Amplifier|amplification]] can result in the exponential growth of the amplified signal, although [[resonance]] effects may favor some [[component frequency|component frequencies]] of the signal over others.\n\n===Economics===\n* [[Economic growth]] is expressed in percentage terms, implying exponential growth.\n\n===Finance===\n* [[Compound interest]] at a constant interest rate provides exponential growth of the capital. See also [[rule of 72]].\n* [[Pyramid scheme]]s or [[Ponzi scheme]]s also show this type of growth resulting in high profits for a few initial investors and losses among great numbers of investors.\n===Computer technology===\n* [[Clock rate|Processing power]] of computers. See also [[Moore's law]] and [[technological singularity]]. (Under exponential growth, there are no singularities. The singularity here is a metaphor, meant to convey an unimaginable future. The link of this hypothetical concept with exponential growth is most vocally made by futurist [[Raymond Kurzweil|Ray Kurzweil]].)\n* In [[computational complexity theory]], computer algorithms of exponential complexity require an exponentially increasing amount of resources (e.g. time, computer memory) for only a constant increase in problem size. So for an algorithm of time complexity 2<sup>''x''</sup>, if a problem of size {{nowrap|1=''x'' = 10}} requires 10 seconds to complete, and a problem of size {{nowrap|1=''x'' = 11}} requires 20 seconds, then a problem of size {{nowrap|1=''x'' = 12}} will require 40 seconds. This kind of algorithm typically becomes unusable at very small problem sizes, often between 30 and 100 items (most computer algorithms need to be able to solve much larger problems, up to tens of thousands or even millions of items in reasonable times, something that would be physically impossible with an exponential algorithm). Also, the effects of [[Moore's Law]] do not help the situation much because doubling processor speed merely allows you to increase the problem size by a constant. E.g. if a slow processor can solve problems of size ''x'' in time ''t'', then a processor twice as fast could only solve problems of size ''x'' + constant in the same time ''t''. So exponentially complex algorithms are most often impractical, and the search for more efficient algorithms is one of the central goals of computer science today.\n\n==Basic formula==\nA quantity ''x'' depends exponentially on time ''t'' if\n\n:<math>x(t)=a\\cdot b^{t/\\tau}</math>\n\nwhere the constant ''a'' is the initial value of ''x'',\n\n:<math>x(0)=a\\, ,</math>\n\nthe constant ''b'' is a positive growth factor, and ''τ'' is the [[time constant]]—the time required for ''x'' to increase by one factor of ''b'':\n\n:<math>x(t+\\tau)=a \\cdot b^{\\frac{t+\\tau}{\\tau}} = a \\cdot b^{\\frac{t}{\\tau}} \\cdot b^{\\frac{\\tau}{\\tau}} = x(t)\\cdot b\\, .</math>\n\nIf {{nowrap|''τ'' > 0}} and {{nowrap|''b'' > 1}}, then ''x'' has exponential growth. If {{nowrap|''τ'' < 0}} and {{nowrap|''b'' > 1}}, or {{nowrap|''τ'' > 0}}  and 0 < {{nowrap|''b'' < 1}}, then ''x'' has [[exponential decay]].\n\nExample: ''If a species of bacteria doubles every ten minutes, starting out with only one bacterium, how many bacteria would be present after one hour?''  The question implies ''a''&nbsp;=&nbsp;1, ''b''&nbsp;=&nbsp;2 and ''τ''&nbsp;=&nbsp;10&nbsp;min.\n\n:<math>x(t)=a\\cdot b^{t/\\tau}=1\\cdot 2^{(60\\text{ min})/(10\\text{ min})}</math>\n\n:<math>x(1\\text{ hr})= 1 \\cdot 2^6 =64.</math>\n\nAfter one hour, or six ten-minute intervals, there would be sixty-four bacteria.\n\nMany pairs (''b'',&nbsp;''τ'') of a [[dimensionless]] non-negative number ''b'' and an amount of time ''τ'' (a [[physical quantity]] which can be expressed as the product of a number of units and a unit of time) represent the same growth rate, with ''τ'' proportional to log&nbsp;''b''. For any fixed ''b'' not equal to 1 (e.g. ''[[E (mathematical constant)|e]]'' or 2), the growth rate is given by the non-zero time ''τ''. For any non-zero time ''τ'' the growth rate is given by the dimensionless positive number&nbsp;''b''.\n\nThus the law of exponential growth can be written in different but mathematically equivalent forms, by using a different [[exponentiation|base]].  The most common forms are the following:\n\n:<math>x(t) = x_0\\cdot e^{kt} = x_0\\cdot e^{t/\\tau} = x_0 \\cdot 2^{t/T}\n= x_0\\cdot \\left( 1 + \\frac{r}{100} \\right)^{t/p},</math>\n\nwhere ''x''<sub>0</sub> expresses the initial quantity ''x''(0).\n\nParameters (negative in the case of exponential decay):\n* The ''growth constant'' ''k'' is the [[frequency]] (number of times per unit time) of growing by a factor ''e''; in finance it is also called the logarithmic return,  [[continuous compounding|continuously compounded return]], or [[Compound interest#Force of interest|force of interest]].\n* The ''[[e-folding|e-folding time]]'' ''τ'' is the time it takes to grow by a factor ''[[E (mathematical constant)|e]]''.\n* The ''[[doubling time]]'' ''T'' is the time it takes to double.\n* The percent increase ''r'' (a dimensionless number) in a period ''p''.\nThe quantities ''k'', ''τ'', and ''T'', and for a given ''p'' also ''r'', have a one-to-one connection given by the following equation (which can be derived by taking the natural logarithm of the above):\n\n:<math>k = \\frac{1}{\\tau} = \\frac{\\ln 2}{T} = \\frac{\\ln \\left( 1 + \\frac{r}{100} \\right)}{p}</math>\n\nwhere ''k'' = 0 corresponds to ''r'' = 0 and to ''τ'' and ''T'' being infinite.\n\nIf ''p'' is the unit of time the quotient ''t''/''p'' is simply the number of units of time. Using the notation ''t'' for the (dimensionless) number of units of time rather than the time itself, ''t''/''p'' can be replaced by ''t'', but for uniformity this has been avoided here. In this case the division by ''p'' in the last formula is not a numerical division either, but converts a dimensionless number to the correct quantity including unit.\n\nA popular approximated method for calculating the doubling time from the growth rate is the [[rule of 70]],\ni.e. <math>T \\simeq 70 / r</math>.\n\n{{wide image|doubling_time_vs_half_life.svg|640px|Graphs comparing doubling times and half lives of exponential growths (bold lines) and decay (faint lines), and their 70/''t'' and 72/''t'' approximations. In the [http://upload.wikimedia.org/wikipedia/commons/8/88/Doubling_time_vs_half_life.svg SVG version], hover over a graph to highlight it and its complement.}}\n\n==Reformulation as log-linear growth==\nIf a variable ''x'' exhibits exponential growth according to <math>x(t)=x_0(1+r)^t</math>, then the log (to any base) of ''x'' [[linear function|grows linearly]] over time, as can be seen by taking [[logarithm]]s of both sides of the exponential growth equation:\n\n:<math>\\log x(t) = \\log x_0 + t \\cdot \\log (1+r).</math>\n\nThis allows an exponentially growing variable to be modeled with a [[Nonlinear regression#Linearization|log-linear model]]. For example, if one wishes to empirically estimate the growth rate from intertemporal data on ''x'', one can [[linear regression|linearly regress]] log ''x'' on ''t''.\n\n==Differential equation==\nThe [[exponential function]] <math>x(t)=x(0) e^{kt}</math> satisfies the [[linear differential equation]]:\n\n:<math> \\!\\, \\frac{dx}{dt} = kx</math>\n\nsaying that the change per instant of time of ''x'' at time ''t'' is proportional to the value of ''x''(''t''), and ''x''(''t'') has the [[initial value]]\n:<math>x(0).</math>\n\nThe differential equation is solved by direct integration:\n\n: <math>\n\\begin{align}\n\\frac{dx}{dt} & = kx \\\\[5pt]\n\\frac{dx} x & = k\\, dt \\\\[5pt]\n\\int_{x(0)}^{x(t)} \\frac{dx}{x} & = k \\int_0^t  \\, dt \\\\[5pt]\n\\ln \\frac{x(t)}{x(0)} & =  kt.\n\\end{align}\n</math>\n\nso that\n\n: <math> x(t) =  x(0) e^{kt}</math>\n\nIn the above differential equation, if {{nowrap|''k'' < 0}}, then the quantity experiences [[exponential decay]].\n\nFor a [[nonlinear]] variation of this growth model see [[logistic function]].\n\n==Difference equation==\nThe [[difference equation]]\n\n:<math>x_t = a \\cdot x_{t-1}</math>\n\nhas solution\n\n:<math>x_t = x_0 \\cdot a^t,</math>\n\nshowing that ''x'' experiences exponential growth.\n\n== Other growth rates ==\nIn the long run, exponential growth of any kind will overtake linear growth of any kind (the basis of the [[Malthusian catastrophe]]) as well as any [[polynomial]] growth, i.e., for all α:\n\n:<math>\\lim_{t\\rightarrow\\infty} {t^\\alpha \\over ae^t} =0.</math>\n\nThere is a whole hierarchy of conceivable growth rates that are slower than exponential and faster than linear (in the long run). See [[Degree of a polynomial#The degree computed from the function values]].\n\nGrowth rates may also be faster than exponential. In the most extreme case, when growth increases without bound in finite time, it is called [[hyperbolic growth]]. In between exponential and hyperbolic growth lie more classes of growth behavior, like the [[hyperoperation]]s beginning at [[tetration]], and <math>A(n,n)</math>, the diagonal of the [[Ackermann function]].\n\n==Limitations of models==\nExponential growth models of physical phenomena only apply within limited regions, as unbounded growth is not physically realistic. Although growth may initially be exponential, the modelled phenomena will eventually enter a region in which previously ignored [[negative feedback]] factors become significant (leading to a [[logistic growth]] model) or other underlying assumptions of the exponential growth model, such as continuity or instantaneous feedback, break down.\n\n{{further information|Limits to Growth|Malthusian catastrophe|Apparent infection rate}}\n\n==Exponential stories==\n\n===Rice on a chessboard===\n{{see also|Wheat and chessboard problem}}\nAccording to an old legend, vizier Sissa Ben Dahir presented an Indian King Sharim with a beautiful, hand-made [[chessboard]]. The king asked what he would like in return for his gift and the courtier surprised the king by asking for one grain of rice on the first square, two grains on the second, four grains on the third etc. The king readily agreed and asked for the rice to be brought. All went well at first, but the requirement for 2<sup>''n''−1</sup> grains on the ''n''th square demanded over a million grains on the 21st square, more than a million million ({{aka}} [[Orders of magnitude (numbers)#1012|trillion]]) on the 41st and there simply was not enough rice in the whole world for the final squares. (From Swirski, 2006)<ref name=Porritt-2005>{{cite book|last=Porritt|first=Jonathan|title=Capitalism: as if the world matters|year=2005|publisher=Earthscan|location=London|isbn=1-84407-192-8|page=49}}</ref>\n\nThe [[second half of the chessboard]] is the time when an exponentially growing influence is having a significant economic impact on an organization's overall business strategy.\n\n===Water lily===\nFrench children are told a story in which they imagine having a pond with [[Nymphaeaceae|water lily]] leaves floating on the surface. The lily population doubles in size every day and, if left unchecked, it will smother the pond in 30 days killing all the other living things in the water. Day after day, the plant's growth is small and so it is decided that it shall be cut down when the water lilies cover half of the pond. The children are then asked on what day will half of the pond be covered in water lilies.  The solution is simple when one considers that the water lilies must double to completely cover the pond on the 30th day.  Therefore, the water lilies will cover half of the pond on the 29th day. There is only one day to save the pond. (From Meadows ''et al''. 1972)<ref name=Porritt-2005/>\n\n==See also==\n{{div col|colwidth=20em}}\n* [[Accelerating change]]\n* [[Albert Allen Bartlett]]\n* [[Arthrobacter]]\n* [[Asymptotic notation]]\n* [[Bacterial growth]]\n* [[Bounded growth]]\n* [[Cell growth]]\n* [[Exponential algorithm]]\n* [[EXPSPACE]]\n* [[EXPTIME]]\n* [[Hausdorff dimension]]\n* [[Hyperbolic growth]]\n* [[Information explosion]]\n* [[Law of accelerating returns]]\n* [[List of exponential topics]]\n* [[Logarithmic growth]]\n* [[Logistic function]]\n* [[Malthusian growth model]]\n* [[Menger sponge]]\n* [[Moore's law]]\n* [[Multiplicative calculus]]\n* [[Polynomial growth]]\n* [[Stein's law]]\n{{div col end}}\n\n==References and footnotes==\n{{reflist}}\n\n===Sources===\n* Meadows, Donella H., Dennis L. Meadows, Jørgen Randers, and William W. Behrens III. (1972) ''[[The Limits to Growth]]''. New York: University Books. {{ISBN|0-87663-165-0}}\n* Porritt, J. ''Capitalism as if the world matters'', Earthscan 2005. {{ISBN|1-84407-192-8}}\n* Swirski, Peter. ''Of Literature and Knowledge: Explorations in Narrative Thought Experiments, Evolution, and Game Theory''. New York: Routledge. {{ISBN|0-415-42060-1}}\n* Thomson, David G. ''Blueprint to a Billion: 7 Essentials to Achieve Exponential Growth'', Wiley Dec 2005, {{ISBN|0-471-74747-5}}\n* Tsirel, S. V. 2004. [http://www.mmsed.narod.ru/articles/artTsirel.ps On the Possible Reasons for the Hyperexponential Growth of the Earth Population]. ''Mathematical Modeling of Social and Economic Dynamics'' / Ed. by M. G. Dmitriev and A. P. Petrov, pp.&nbsp;367–9. Moscow: Russian State Social University, 2004.\n\n==External links==\n\n* [http://www.slideshare.net/amenning/growth-in-a-finite-world-sustainability-and-the-exponential-function Growth in a Finite World – Sustainability and the Exponential Function] — Presentation\n* [http://www.energybulletin.net/media/2004-08-29/dr-albert-bartlett-arithmetic-population-and-energy Dr. Albert Bartlett: Arithmetic, Population and Energy] — streaming video and audio 58 min\n* [http://engineeringunits.com/exponential-growth-calculator/ exponential growth calculator] — Online exponential growth Calculator\n\n\n\n[[Category:Ordinary differential equations]]\n[[Category:Exponentials]]\n[[Category:Mathematical modeling]]\n[[Category:Growth curves]]"
    },
    {
      "title": "Exponential sum",
      "url": "https://en.wikipedia.org/wiki/Exponential_sum",
      "text": "In [[mathematics]], an '''exponential sum''' may be a finite [[Fourier series]] (i.e. a [[trigonometric polynomial]]), or other finite sum formed using the [[exponential function]], usually expressed by means of the function\n\n:<math>e(x) = \\exp(2\\pi ix).\\,</math>\n\nTherefore, a typical exponential sum may take the form\n\n:<math>\\sum_n e(x_n),</math>\n\nsummed over a finite [[sequence]] of [[real number]]s ''x''<sub>''n''</sub>.\n\n==Formulation==\n\nIf we allow some real coefficients ''a''<sub>''n''</sub>, to get the form\n\n:<math>\\sum_n a_n e(x_n)</math>\n\nit is the same as allowing exponents that are [[complex number]]s. Both forms are certainly useful in applications. A large part of twentieth century [[analytic number theory]] was devoted to finding good estimates for these sums, a trend started by basic work of [[Hermann Weyl]] in [[diophantine approximation]].\n\n==Estimates==\n\nThe main thrust of the subject is that a sum\n\n:<math>S=\\sum_n e(x_n)</math>\n\nis ''trivially'' estimated by the number ''N'' of terms. That is, the [[absolute value]]\n\n:<math>|S| \\le N\\,</math>\n\nby the [[triangle inequality]], since each summand has absolute value 1. In applications one would like to do better. That involves proving some cancellation takes place, or in other words that this sum of complex numbers on the [[unit circle]] is not of numbers all with the same [[Parameter|argument]]. The best that is reasonable to hope for is an estimate of the form\n\n:<math>|S|= O(\\sqrt{N})\\,</math>\n\nwhich signifies, up to the implied constant in the [[big O notation]], that the sum resembles a [[random walk]] in two dimensions.\n\nSuch an estimate can be considered ideal; it is unattainable in many of the major problems, and estimates\n\n:<math>|S|= o(N)\\,</math>\n\nhave to be used, where the o(''N'') function represents only a ''small saving'' on the trivial estimate. A typical 'small saving' may be a factor of log(''N''), for example. Even such a minor-seeming result in the right direction has to be referred all the way back to the structure of the initial sequence ''x''<sub>''n''</sub>, to show a degree of [[randomness]]. The techniques involved are ingenious and subtle.\n\nA variant of 'Weyl differencing' investigated by Weyl involving a generating exponential sum\n\n<math> G(\\tau)= \\sum_n e^{iaf(x)+ia\\tau n} </math>\n\nWas previously studied by Weyl himself, he developed a method to express the sum as the value <math> G(0)</math>, where 'G' can be defined via a linear differential equation similar to [[Dyson equation]] obtained via summation by parts.\n\n==History==\n\nIf the sum is of the form\n\n:<math> S(x)= e^{ia f(x) } </math>\n\nwhere ''ƒ'' is a smooth function, we could use the [[Euler–Maclaurin formula]] to convert the series into an integral, plus some corrections involving derivatives of ''S''(''x''), then for large values of ''a'' you could use \"stationary phase\" method to calculate the integral and give an approximate evaluation of the sum. Major advances in the subject were ''[[Van der Corput's method]]'' (c. 1920), related to the [[principle of stationary phase]], and the later ''[[Vinogradov method]]'' (c.1930).\n\nThe [[large sieve method]] (c.1960), the work of many researchers, is a relatively transparent general principle; but no one method has general application.\n\n==Types of exponential sum==\n\nMany types of sums are used in formulating particular problems; applications require usually a reduction to some known type, often by ingenious manipulations. [[Partial summation]] can be used to remove coefficients ''a''<sub>''n''</sub>, in many cases.\n\nA basic distinction is between a '''complete exponential sum''', which is typically a sum over all [[residue class]]es ''[[modular arithmetic|modulo]]'' some integer ''N'' (or more general [[finite ring]]), and an '''incomplete exponential sum''' where the range of summation is restricted by some [[inequality (mathematics)|inequality]]. Examples of complete exponential sums are [[Gauss sum]]s and [[Kloosterman sum]]s; these are in some sense [[finite field]] or finite ring analogues of the [[gamma function]] and some sort of [[Bessel function]], respectively, and have many 'structural' properties. An example of an incomplete sum is the partial sum of the quadratic Gauss sum (indeed, the case investigated by [[Carl Friedrich Gauss|Gauss]]). Here there are good estimates for sums over shorter ranges than the whole set of residue classes, because, in geometric terms, the partial sums approximate a [[Cornu spiral]]; this implies massive cancellation.\n\nAuxiliary types of sums occur in the theory, for example [[character sum]]s; going back to [[Harold Davenport]]'s thesis. The [[Weil conjectures]] had major applications to complete sums with domain restricted by polynomial conditions (i.e., along an [[algebraic variety]] over a finite field).\n\n==Weyl sums==\nOne of the most general types of exponential sum is the '''Weyl sum''', with exponents 2π''if''(''n'') where ''f'' is a fairly general real-valued [[smooth function]]. These are the sums involved in the distribution of the values\n\n:''&fnof;''(''n'') modulo 1,\n\naccording to [[Weyl's equidistribution criterion]]. A basic advance was [[Weyl's inequality]] for such sums, for polynomial ''f''.\n\nThere is a general theory of [[exponent pair]]s, which formulates estimates. An important case is where ''f'' is logarithmic, in relation with the [[Riemann zeta function]].  See also [[equidistribution theorem]].<ref name=Mont39>Montgomery (1994) p.39</ref>\n\n==Example: the quadratic Gauss sum==\n\nLet ''p'' be an odd prime and let <math>\\xi = e^{2\\pi i / p}</math>. Then\nthe [[Quadratic Gauss sum]] is given by\n\n:<math>\\sum_{n=0}^{p-1}\\xi^{n^2} =\n\\begin{cases}\n\\sqrt{p}, & p = 1 \\mod 4 \\\\\ni\\sqrt{p}, & p = 3 \\mod 4\n\\end{cases}\n</math>\n\nwhere the square roots are taken to be positive.\n\nThis is the ideal degree of cancellation one could hope for without any ''a priori'' knowledge of the structure of the sum, since it matches the scaling of a [[random walk]].\n\n==See also==\n* [[Hua's lemma]]\n\n==References==\n{{reflist}}\n* {{cite book | last=Montgomery | first=Hugh L. | authorlink=Hugh Montgomery (mathematician) | title=Ten lectures on the interface between analytic number theory and harmonic analysis | series=Regional Conference Series in Mathematics | volume=84 | location=Providence, RI | publisher=[[American Mathematical Society]] | year=1994 | isbn=0-8218-0737-4 | zbl=0814.11001 }}\n* {{cite book | editor1-last=Sándor | editor1-first=József | editor2-last=Mitrinović | editor2-first=Dragoslav S. | editor3-last=Crstici |editor3-first=Borislav | title=Handbook of number theory I | location=Dordrecht | publisher=[[Springer-Verlag]] | year=2006 | isbn=1-4020-4215-9 | zbl=1151.11300 }}\n\n==Further reading==\n* {{cite book | zbl=0754.11022 | last=Korobov | first=N.M. | title=Exponential sums and their applications | others=Translated from the Russian by Yu. N. Shakhov | series=Mathematics and Its Applications. Soviet Series. | volume=80 | location=Dordrecht | publisher=Kluwer Academic Publishers | year=1992 | isbn=0-7923-1647-9 }}\n\n==External links==\n* [http://mathworld.wolfram.com/WeylSum.html A brief introduction to Weyl sums on Mathworld]\n\n[[Category:Exponentials]]\n[[Category:Analytic number theory]]"
    },
    {
      "title": "Exponential tree",
      "url": "https://en.wikipedia.org/wiki/Exponential_tree",
      "text": "{{Infobox data structure\n|name=Exponential tree\n|type=tree\n|invented_by=[[Arne_Andersson_(computer_scientist)|Arne Andersson]]\n|invented_year=1995\n|space_avg=O(''n'')\n|space_worst=O(''n'')\n|search_avg=O(min({{radic|log&nbsp;''n''}}, log&nbsp;''n''/log&nbsp;''w''+ log&nbsp;log&nbsp;''n'', log&nbsp;''w'' log&nbsp;log&nbsp;''n''))\n|search_worst=O(min({{radic|log&nbsp;''n''}}, log&nbsp;''n''/log&nbsp;''w''+ log&nbsp;log&nbsp;''n'', log&nbsp;''w'' log&nbsp;log&nbsp;''n''))\n|insert_avg=O(min({{radic|log&nbsp;''n''}}, log&nbsp;''n''/log&nbsp;''w''+ log&nbsp;log&nbsp;''n'', log&nbsp;''w'' log&nbsp;log&nbsp;''n''))\n|insert_worst=O(min({{radic|log&nbsp;''n''}}, log&nbsp;''n''/log&nbsp;''w''+ log&nbsp;log&nbsp;''n'', log&nbsp;''w'' log&nbsp;log&nbsp;''n''))\n|delete_avg=O(min({{radic|log&nbsp;''n''}}, log&nbsp;''n''/log&nbsp;''w''+ log&nbsp;log&nbsp;''n'', log&nbsp;''w'' log&nbsp;log&nbsp;''n''))\n|delete_worst=O(min({{radic|log&nbsp;''n''}}, log&nbsp;''n''/log&nbsp;''w''+ log&nbsp;log&nbsp;''n'', log&nbsp;''w'' log&nbsp;log&nbsp;''n''))\n}}\n\nAn '''exponential tree''' is almost identical to a [[binary search tree]], with the exception that the dimension of the tree is not the same at all levels. In a normal binary search tree, each node has a dimension (''d'') of 1, and has 2<sup>''d''</sup> children. In an exponential tree, the dimension equals the depth of the node, with the root node having a ''d''&nbsp;=&nbsp;1. So the second level can hold four nodes, the third can hold eight nodes, the fourth 16 nodes, and so on.\n\n==Layout==\n\n\"Exponential Tree\" can also refer to a method of laying out the nodes of a tree structure in n (typically 2) dimensional space. Nodes are placed closer to a baseline than their parent node, by a factor equal to the number of child nodes of that parent node (or by some sort of weighting), and scaled according to how close they are.  Thus, no matter how \"deep\" the tree may be, there is always room for more nodes, and the geometry of a subtree is unrelated to its position in the whole tree. The whole has a [[fractal]] structure.\n\nIn fact, this method of laying out a tree can be viewed as an application of the [[upper half-plane]] model of [[hyperbolic geometry]], with isometries limited to translations only.\n\n==See also==\n* [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.7109 Faster deterministic sorting and searching in linear space] (Original paper from '95)\n* [http://www2.parc.com/csl/groups/sda/publications/papers/Lamping-UIST94/for-web.pdf Laying out and Visualizing Large Trees Using a Hyperbolic Space]{{dead link|date=June 2019}}\n* [http://www.ijcaonline.org/volume24/number3/pxc3873876.pdf Implementation and Performance Analysis of Exponential Tree Sorting] (This paper is not correct)\n\n{{CS-Trees}}\n\n[[Category:Exponentials]]\n[[Category:Trees (data structures)]]\n\n{{datastructure-stub}}"
    },
    {
      "title": "Exponentiation",
      "url": "https://en.wikipedia.org/wiki/Exponentiation",
      "text": "{{short description|Mathematical operation}}\n{{Redirect|Exponent}}\n\n[[Image:Expo02.svg|thumb|315px|Graphs of {{math|1=''y'' = ''b''<sup>''x''</sup>}} for various bases ''b'':\n{{legend-line|inline=yes|green solid 2px|[[#Powers of ten|base&nbsp;10]]}},\n{{legend-line|inline=yes|red solid 2px|[[#The exponential function|base&nbsp;''e'']]}},\n{{legend-line|inline=yes|blue solid 2px|[[#Powers of two|base&nbsp;2]]}}, and\n{{legend-line|inline=yes|cyan solid 2px|base&nbsp;{{sfrac|2}}}}.\nEach curve passes through the point {{math|(0, 1)}} because any nonzero number raised to the power of 0 is 1. At {{math|1=''x'' = 1}}, the value of ''y'' equals the base because any number raised to the power of 1 is the number itself.]]\n<div class=\"tright\">{{Calculation results}}</div>\n\n'''Exponentiation''' is a [[mathematics|mathematical]] [[operation (mathematics)|operation]], written as {{math|''b''<sup>''n''</sup>}}, involving two numbers, the ''[[Base (exponentiation)|base]]'' {{mvar|b}} and the ''exponent'' or ''power'' {{mvar|n}}. When {{mvar|n}} is a positive [[integer]], exponentiation corresponds to repeated [[multiplication]] of the base: that is, {{math|''b''<sup>''n''</sup>}} is the [[product (mathematics)|product]] of multiplying {{mvar|n}} bases:\n\n:<math>b^n = \\underbrace{b \\times \\dots \\times b}_{n\\, \\textrm{times}}.</math>\n\nThe exponent [[#History of the notation|is usually shown]] as a [[superscript]] to the right of the base. In that case, {{math|''b''<sup>''n''</sup>}} is called \"b raised to the n-th power\", \"b raised to the power of n\", \"the n-th power of b\", \"b to the nth\", or most briefly as \"b to the n\".\n\nFor any positive integers {{mvar|m}} and {{mvar|n}}, one has {{math|1=''b''<sup>''n''</sup> ⋅ ''b''<sup>''m''</sup> = ''b''<sup>''n''+''m''</sup>}}. To extend this property to non-positive integer exponents, {{math|''b''<sup>0</sup>}} is defined to be 1, and {{math|''b''<sup>−''n''</sup>}} with {{mvar|n}} a positive integer and {{mvar|b}} not zero is defined as {{math|{{sfrac|1|''b''<sup>''n''</sup>}}}}. In particular, {{math|''b''<sup>−1</sup>}} is equal to {{math|{{sfrac|1|''b''}}}}, the ''[[multiplicative inverse|reciprocal]]'' of {{mvar|b}}.\n\nThe definition of exponentiation can be extended to allow any real or [[complex numbers|complex]] exponent. Exponentiation by integer exponents can also be defined for a wide variety of algebraic structures, including [[matrix (mathematics)|matrices]].\n\nExponentiation is used extensively in many fields, including [[economics]], [[biology]], [[chemistry]], [[physics]], and [[computer science]], with applications such as [[compound interest]], [[population growth]],  [[chemical reaction kinetics]], [[wave]] behavior, and [[public-key cryptography]].\n\n== History of the notation ==\nThe term ''power'' was used by the [[Greek mathematics|Greek]] mathematician [[Euclid]] for the square of a line,<ref name=\"MacTutor\" /> following [[Hippocrates of Chios]].<ref>[[W. W. Rouse Ball]], ''[https://books.google.com/books?id=U9zuAAAAMAAJ&pg=PA36 A Short Account of the History of Mathematics]'' (1888) p. 36.</ref> [[Archimedes]] discovered and proved the law of exponents, {{math|1=10<sup>''a''</sup> ⋅ 10<sup>''b''</sup> = 10<sup>''a''+''b''</sup>}}, necessary to manipulate powers of {{math|10}}.<ref>For further analysis see [[The Sand Reckoner]].</ref>{{better source|date=July 2018}} In the 9th century, the Persian mathematician [[Muhammad ibn Mūsā al-Khwārizmī]] used the terms ''mal'' for a [[Square (algebra)|square]] and ''kahb'' for a [[Cube (algebra)|cube]], which later [[Mathematics in medieval Islam|Islamic]] mathematicians represented in [[mathematical notation]] as ''m'' and ''k'', respectively, by the 15th century, as seen in the work of [[Abū al-Hasan ibn Alī al-Qalasādī]].<ref>{{MacTutor|id=Al-Qalasadi|title= Abu'l Hasan ibn Ali al Qalasadi}}</ref>\n\nIn the late 16th century, [[Joost Burgi|Jost Bürgi]] used Roman numerals for exponents.<ref>Cajori, Florian (2007). A History of Mathematical Notations; Vol I. Cosimo Classics. p. 344 {{ISBN|1-60206-684-1}}</ref>\n\nEarly in the 17th century, the first form of our modern exponential notation was introduced by [[Rene Descartes]] in his text titled ''[[La Géométrie]]''; there, the notation is introduced in Book I.<ref name=\"Descartes\">René Descartes, ''Discourse de la Méthode'' ... (Leiden, (Netherlands):  Jan Maire, 1637), appended book:  ''La Géométrie'', book one, [http://gallica.bnf.fr/ark:/12148/btv1b86069594/f383.image page 299.]  From page 299:  ''\" ... Et ''aa'', ou ''a''<sup>2</sup>, pour multiplier ''a'' par soy mesme; Et ''a''<sup>3</sup>, pour le multiplier encore une fois par ''a'', & ainsi a l'infini ; ... \"''  ( ... and ''aa'', or ''a''<sup>2</sup>, in order to multiply ''a'' by itself; and ''a''<sup>3</sup>, in order to multiply it once more by ''a'', and thus to infinity ; ... )</ref>\n\n[[Nicolas Chuquet]] used a form of exponential notation in the 15th century, which was later used by [[Henricus Grammateus]] and [[Michael Stifel]] in the 16th century. The word \"exponent\" was coined in 1544 by Michael Stifel.<ref>See:\n*  [http://jeff560.tripod.com/e.html Earliest Known Uses of Some of the Words of Mathematics]\n*  Michael Stifel, ''Arithmetica integra'' (Nuremberg (\"Norimberga\"), (Germany):  Johannes Petreius, 1544), Liber III (Book 3), Caput III (Chapter 3):  De Algorithmo numerorum Cossicorum. (On algorithms of algebra.), [https://books.google.com/books?id=fndPsRv08R0C&vq=exponens&pg=RA7-PA231#v=onepage&q&f=false page 236.]  Stifel was trying to conveniently represent the terms of geometric progressions.  He devised a cumbersome notation for doing that.  On page 236, he presented the notation for the first eight terms of a geometric progression (using 1 as a base) and then he wrote:  ''\"Quemadmodum autem hic vides, quemlibet terminum progressionis cossicæ, suum habere exponentem in suo ordine (ut 1ze habet 1. 1ʓ habet 2 &c.) sic quilibet numerus cossicus, servat exponentem suæ denominationis implicite, qui ei serviat & utilis sit, potissimus in multiplicatione & divisione, ut paulo inferius dicam.\"''  (However, you see how each term of the progression has its exponent in its order (as 1ze has a 1, 1ʓ has a 2, etc.), so each number is implicitly subject to the exponent of its denomination, which [in turn] is subject to it and is useful mainly in multiplication and division, as I will mention just below.)  [Note:  Most of Stifel's cumbersome symbols were taken from [[Christoff Rudolff]], who in turn took them from Leonardo Fibonacci's ''Liber Abaci'' (1202), where they served as shorthand symbols for the Latin words ''res''/''radix'' (x), ''census''/''zensus'' (''x''<sup>2</sup>), and ''cubus'' (''x''<sup>3</sup>).]</ref> [[Samuel Jeake]] introduced the term ''indices'' in 1696.<ref name=\"MacTutor\">{{MacTutor|class=Miscellaneous|id=Mathematical_notation|title=Etymology of some common mathematical terms}}</ref> In the 16th century [[Robert Recorde]] used the terms square, cube, zenzizenzic ([[fourth power]]), sursolid (fifth), zenzicube (sixth), second sursolid (seventh), and [[zenzizenzizenzic]] (eighth).<ref>{{Cite web|url=http://www.worldwidewords.org/weirdwords/ww-zen1.htm|title=Zenzizenzizenzic – the eighth power of a number|publisher=World Wide Words|first=Michael|last=Quinion|accessdate=2010-03-19}}</ref> ''Biquadrate'' has been used to refer to the fourth power as well.\n\nSome mathematicians (e.g., [[Isaac Newton]]) used exponents only for powers greater than two, preferring to represent squares as repeated multiplication. Thus they would write [[polynomial]]s, for example, as {{math|''ax'' + ''bxx'' + ''cx''<sup>3</sup> + ''d''}}.\n\nAnother historical synonym, '''involution''',<ref>This definition of \"involution\" appears in the OED second edition, 1989, and Merriam-Webster online dictionary [http://www.m-w.com/dictionary/involution].  The most recent usage in this sense cited by the OED is from 1806.</ref> is now rare and should not be confused with [[Involution (mathematics)|its more common meaning]].\n\nIn 1748 [[Leonhard Euler]] wrote \"consider exponentials or powers in which the exponent itself is a variable. It is clear that quantities of this kind are not [[algebraic function]]s, since in those the exponents must be constant.\"<ref>[[Leonhard Euler]] (1748) [[Introduction to the Analysis of the Infinite]], English version, page 75</ref> With this introduction of [[transcendental function]]s, Euler laid the foundation for the modern introduction of [[natural logarithm]] as the [[inverse function]] for the [[natural exponential function]], {{math|1=''f''(''x'') = ''e''<sup>''x''</sup>}}.\n\n==Terminology==\nThe expression {{math|1=''b''<sup>2</sup> = ''b'' ⋅ ''b''}} is called \"the [[Square (algebra)|square]] of ''b''\" or \"''b'' squared\" because the area of a square with side-length {{math|''b''}} is {{math|''b''<sup>2</sup>}}.\n\nThe expression {{math|1=''b''<sup>3</sup> = ''b'' ⋅ ''b'' ⋅ ''b''}}  is called \"the [[Cube (algebra)|cube]] of ''b''\" or \"''b'' cubed\" because the volume of a cube with side-length {{math|''b''}} is {{math|''b''<sup>3</sup>}}.\n\nWhen it is a [[positive integer]], the exponent indicates how many copies of the base are multiplied together. For example, {{math|1=3<sup>5</sup> = 3 ⋅ 3 ⋅ 3 ⋅ 3 ⋅ 3 = 243}}. The base {{math|3}} appears {{math|5}} times in the repeated multiplication, because the exponent is {{math|5}}. Here, {{math|3}} is the ''base'', {{math|5}} is the ''exponent'', and {{math|243}} is the ''power'' or, more specifically, ''3 raised to the 5th power''.\n\nThe word \"raised\" is usually omitted, and sometimes \"power\" as well, so {{math|3<sup>5</sup>}} can also be read \"3 to the 5th\" or \"3 to the 5\". Therefore, the exponentiation {{math|''b''<sup>''n''</sup>}} can be expressed as \"''b'' to the power of ''n''\", \"''b'' to the ''n''th power\", \"''b'' to the ''n''th\", or most briefly as \"''b'' to the ''n''\".\n\n==Integer exponents==\nThe exponentiation operation with integer exponents may be defined directly from elementary [[arithmetic operation]]s.\n\n===Positive exponents===\nFormally, powers with positive integer exponents may be defined by the initial condition<ref>{{cite book |url=https://books.google.com/books?id=qToTAgAAQBAJ&lpg=PA93&pg=PA94#v=onepage&f=false |title=Abstract Algebra: an inquiry based approach |first1=Jonathan K. |last1=Hodge |first2=Steven |last2=Schlicker |first3=Ted |last3=Sundstorm |page=94 |year=2014 |publisher=CRC Press |isbn=978-1-4665-6706-1}}</ref>\n:<math>b^1 = b</math>\nand the [[recurrence relation]]\n:<math>b^{n+1} = b^n \\cdot b.</math>\n\nFrom the [[Associative property|associativity]] of multiplication, it follows that for any positive integers {{mvar|m}} and {{mvar|n}},\n:<math>b^{m+n} = b^m \\cdot b^n.</math>\n\n===Zero exponent===\nAny nonzero number raised to the {{math|0}} power is {{math|1}}:<ref>{{cite book|url=https://books.google.com/?id=YOdtemSmzQQC&pg=PA101#v=onepage&f=false |title=Technical Shop Mathematics |first1=Thomas |last1=Achatz |page=101 |year=2005 |edition=3rd |publisher=Industrial Press |isbn=978-0-8311-3086-2}}</ref>\n\n:<math>b^0=1.</math>\n\nOne interpretation of such a power is as an [[empty product]].\n\nThe case of {{math|0<sup>0</sup>}} is more complicated, and the choice of whether to assign it a value and what value to assign may depend on context. {{Crossref|For more details, see [[Zero to the power of zero]].}}\n\n===Negative exponents===\nThe following identity holds for an arbitrary integer {{mvar|n}} and nonzero {{mvar|b}}:\n:<math>b^{-n} = \\frac{1}{b^n}.</math>\nRaising 0 to a negative exponent is undefined, but in some circumstances, it may be interpreted as infinity ({{math|∞}}).\n\nThe identity above may be derived through a definition aimed at extending the range of exponents to negative integers.\n\nFor non-zero {{mvar|b}} and positive {{mvar|n}}, the recurrence relation above can be rewritten as\n:<math>b^n = \\frac{b^{n+1}}{b}, \\quad n \\ge 1 .</math>\n\nBy defining this relation as valid for all integer {{mvar|n}} and nonzero {{mvar|b}}, it follows that\n:<math>\\begin{align}\n  b^0    &= \\frac{b^1}{b} = 1, \\\\[3pt]\n  b^{-1} &= \\frac{b^0}{b} = \\frac{1}{b},\n\\end{align}</math>\n\nand more generally for any nonzero {{mvar|b}} and any nonnegative integer {{mvar|n}},\n:<math>b^{-n} = \\frac{1}{b^n}.</math>\n\nThis is then readily shown to be true for every integer {{mvar|n}}.\n\n===Identities and properties===\nThe following [[identity (mathematics)|identities]] hold for all integer exponents, provided that the base is non-zero:\n:<math>\\begin{align}\n           b^{m + n} &= b^m \\cdot b^n \\\\\n  \\left(b^m\\right)^n &= b^{m \\cdot n} \\\\\n       (b \\cdot c)^n &= b^n \\cdot c^n\n\\end{align}</math>\n\nUnlike addition and multiplication:\n{{bulleted list\n| Exponentiation is not [[commutative]]. For example, {{math|1=2<sup>3</sup> = 8 ≠ 3<sup>2</sup> = 9}}.\n| Exponentiation is not [[associative]]. For example, {{math|1=(2<sup>3</sup>)<sup>4</sup> = 8<sup>4</sup> {{=}} 4096}}, whereas {{math|2<sup>(3<sup>4</sup>)</sup> {{=}} 2<sup>81</sup> {{=}} {{val|2417851639229258349412352}}}}. Without parentheses, the conventional [[order of operations]] in superscript notation is top-down (or ''right''-associative), not bottom-up<ref>{{Cite journal\n | title=A report on primes of the form k · 2<sup>n</sup> + 1 and on factors of Fermat numbers\n | author=Raphael M. Robinson\n | journal= Proc. Amer. Math. Soc.\n | volume=9\n | issue=5\n | page=677\n | date=1958\n | url=http://www.ams.org/journals/proc/1958-009-05/S0002-9939-1958-0096614-7/S0002-9939-1958-0096614-7.pdf\n | doi=10.1090/s0002-9939-1958-0096614-7\n}}</ref> (or ''left''-associative). That is, \n:<math>b^{p^q} = b^{\\left(p^q\\right)},</math>\nwhich, in general, is different from \n:<math>\\left(b^p\\right)^q = b^{p q} .</math>}}\n\n===Combinatorial interpretation===\n{{see also|#Exponentiation over sets|l1=Exponentiation over sets}}\n\nFor nonnegative integers {{mvar|n}} and {{mvar|m}}, the value of {{math|''n''<sup>''m''</sup>}} is the number of [[function (mathematics)|functions]] from a [[Set (mathematics)|set]] of {{mvar|m}} elements to a set of {{mvar|n}} elements (see [[Cardinal number#Cardinal exponentiation|cardinal exponentiation]]). Such functions can be represented as {{mvar|m}}-[[tuple]]s from an {{mvar|n}}-element set (or as {{mvar|m}}-letter words from an {{mvar|n}}-letter alphabet).  Some examples for particular values of {{mvar|m}} and {{mvar|n}} are given in the following table:\n\n:{| class=\"wikitable\"\n!{{math|''n''<sup>''m''</sup>}}\n!The {{math|''n''<sup>''m''</sup>}} possible {{mvar|m}}-tuples of elements from the set {{math|{{mset|1, ..., ''n''}}}}\n|-\n|<math>0^5 = 0</math>\n|none\n|-\n|<math>1^4 = 1</math>\n|<math>(1,1,1,1)</math>\n|-\n|<math>2^3 = 8</math>\n|<math>(1,1,1),(1,1,2),(1,2,1),(1,2,2),(2,1,1),(2,1,2),(2,2,1),(2,2,2)</math>\n|-\n|<math>3^2 = 9</math>\n|<math>(1,1),(1,2),(1,3),(2,1),(2,2),(2,3),(3,1),(3,2),(3,3)</math>\n|-\n|<math>4^1 = 4</math>\n|<math>(1),(2),(3),(4)</math>\n|-\n|<math>5^0 = 1</math>\n|<math>()</math>\n|}\n\n===Particular bases===\n\n===={{anchor|Base 10}}Powers of ten====\n{{see also|Scientific notation}}\nIn the base ten ([[decimal]]) number system, integer powers of {{math|10}} are written as the digit {{math|1}} followed or preceded by a number of zeroes determined by the sign and magnitude of the exponent.  For example, {{math|1={{val|e=3}} = {{val|1000}}}} and {{math|1={{val|e=-4}} = {{val|0.0001}}}}.\n\nExponentiation with base [[10 (number)|{{math|10}}]] is used in [[scientific notation]] to denote large or small numbers. For instance, {{val|299792458|u=m/s}} (the [[speed of light]] in vacuum, in [[metres per second]]) can be written as {{val|2.99792458|e=8|u=m/s}} and then [[approximation|approximated]] as {{val|2.998|e=8|u=m/s}}.\n\n[[SI prefix]]es based on powers of {{math|10}} are also used to describe small or large quantities. For example, the prefix [[Kilo-|kilo]] means {{math|1={{val|e=3}} = {{val|1000}}}}, so a kilometre is {{val|1000|u=metres}}.\n\n===={{anchor|Base 2}}Powers of two====\nThe first negative powers of {{math|2}} are commonly used, and have special names, e.g.: ''[[One half|half]]'' and ''[[4 (number)|quarter]]''.\n\nPowers of {{math|2}} appear in [[set theory]], since a set with {{math|''n''}} members has a [[power set]], the set of all of its [[subset]]s, which has {{math|2<sup>''n''</sup>}} members.\n\nInteger [[Power of two|powers of {{math|2}}]] are important in [[computer science]]. The positive integer powers {{math|2<sup>''n''</sup>}} give the number of possible values for an {{math|''n''}}-[[bit]] integer [[binary number]]; for example, a [[byte]] may take {{math|1=2<sup>8</sup> = 256}} different values. The [[binary number system]] expresses any number as a sum of powers of {{math|2}}, and denotes it as a sequence of {{math|0}} and {{math|1}}, separated by a [[binary point]], where {{math|1}} indicates a power of {{math|2}} that appears in the sum; the exponent is determined by the place of this {{math|1}}: the nonnegative exponents are the rank of the {{math|1}} on the left of the point (starting from {{math|0}}), and the negative exponents are determined by the rank on the right of the point.\n\n====Powers of one====\nThe powers of one are all one: {{math|1=1<sup>''n''</sup> = 1}}.\n\n====Powers of zero====\nIf the exponent {{mvar|n}} is positive ({{math|''n'' > 0}}), the {{mvar|n}}th power of zero is zero: {{math|1=0<sup>''n''</sup> = 0}}.\n\nIf the exponent {{mvar|n}} is negative ({{math|''n'' < 0}}), the {{mvar|n}}th power of zero {{math|0<sup>''n''</sup>}} is undefined, because it must equal <math>1/0^{-n}</math> with {{math|-''n'' > 0}}, and this would be <math>1/0</math> according to above.\n\nThe expression {{math|0<sup>0</sup>}} is either defined as 1, or it is left undefined (''see [[Zero to the power of zero]]'').\n\n====Powers of negative one====\nIf {{math|''n''}} is an even integer, then {{math|1=(−1)<sup>''n''</sup> = 1}}.\n\nIf {{math|''n''}} is an odd integer, then {{math|1=(−1)<sup>''n''</sup> = −1}}.\n\nBecause of this, powers of {{math|−1}} are useful for expressing alternating [[sequence]]s.  For a similar discussion of powers of the complex number {{math|''i''}}, see {{section link||Powers of complex numbers}}.\n\n===Large exponents===\nThe [[limit of a sequence]] of powers of a number greater than one diverges; in other words, the sequence grows without bound:\n:{{math|''b''<sup>''n''</sup> → ∞}} as {{math|''n'' → ∞}} when {{math|''b'' > 1}}\n\nThis can be read as \"''b'' to the power of ''n'' tends to [[Extended real number line|+∞]] as ''n'' tends to infinity when ''b'' is greater than one\".\n\nPowers of a number with [[absolute value]] less than one tend to zero:\n:{{math|''b''<sup>''n''</sup> → 0}} as {{math|''n'' → ∞}} when {{math|{{abs|''b''}} < 1}}\n\nAny power of one is always one:\n:{{math|1=''b''<sup>''n''</sup> = 1}} for all {{math|''n''}} if {{math|1=''b'' = 1}}\n\nPowers of {{math|–1}} alternate between {{math|1}} and {{math|–1}} as {{math|''n''}} alternates between even and odd, and thus do not tend to any limit as {{math|''n''}} grows.\n\nIf {{math|''b'' < –1}}, {{math|1=''b''<sup>''n''</sup>}}, alternates between larger and larger positive and negative numbers as {{math|''n''}} alternates between even and odd, and thus does not tend to any limit as {{math|''n''}} grows.\n\nIf the exponentiated number varies while tending to {{math|1}} as the exponent tends to infinity, then the limit is not necessarily one of those above. A particularly important case is\n:{{math|(1 + 1/''n'')<sup>''n''</sup> → ''e''}} as {{math|''n'' → ∞}}\n\nSee ''{{section link||The exponential function}}'' below.\n\nOther limits, in particular those of expressions that take on an [[indeterminate form]], are described in {{section link||Limits of powers}} below.\n\n===Power functions===\n\n[[File:Potenssi 1 3 5.png|thumb|left|Power functions for <math>n=1,3,5</math> ]]\n[[File:Potenssi 2 4 6.png|thumb|Power functions for <math>n=2,4,6</math>]]\n\nReal functions of the form <math>f(x) = cx^n</math>, where <math>c \\ne 0</math>, are sometimes called power functions.{{cn|date=November 2017}} When <math>n</math> is an [[integer]] and <math>n \\ge 1</math>, two primary families exist: for <math>n</math> even, and for <math>n</math> odd.  In general for <math>c > 0</math>, when <math>n</math> is even <math>f(x) = cx^n</math> will tend towards positive [[infinity (mathematics)|infinity]] with increasing <math>x</math>, and also towards positive infinity with decreasing <math>x</math>.  All graphs from the family of even power functions have the general shape of <math>y=cx^2</math>, flattening more in the middle as <math>n</math> increases.<ref>{{cite book|last1=Anton|first1=Howard|last2=Bivens|first2=Irl|last3=Davis|first3=Stephen|title=Calculus: Early Transcendentals|publisher=John Wiley & Sons|page=28|edition=9th}}</ref>  Functions with this kind of [[symmetry]] {{nobr|(<math>f(-x)= f(x)</math>)}} are called [[even functions]].\n\nWhen <math>n</math> is odd, <math>f(x)</math>'s [[asymptotic]] behavior reverses from positive <math>x</math> to negative <math>x</math>.  For <math>c > 0</math>, <math>f(x) = cx^n</math> will also tend towards positive [[infinity (mathematics)|infinity]] with increasing <math>x</math>, but towards negative infinity with decreasing <math>x</math>. All graphs from the family of odd power functions have the general shape of <math>y=cx^3</math>, flattening more in the middle as <math>n</math> increases and losing all flatness there in the straight line for <math>n=1</math>. Functions with this kind of symmetry {{nobr|(<math>f(-x)= -f(x)</math>)}} are called [[odd functions]].\n\nFor <math>c < 0</math>, the opposite asymptotic behavior is true in each case.<ref>{{cite book|last1=Anton|first1=Howard|last2=Bivens|first2=Irl|last3=Davis|first3=Stephen|title=Calculus: Early Transcendentals|publisher=John Wiley & Sons|page=28|edition=9th}}</ref>\n\n===List of whole-number powers===\n{|class=\"wikitable\" style=\"text-align:right\"\n!''n'' !!''n''<sup>2</sup> !!''n''<sup>3</sup> !!''n''<sup>4</sup> !!''n''<sup>5</sup> !!''n''<sup>6</sup> !!''n''<sup>7</sup> !!''n''<sup>8</sup> !!''n''<sup>9</sup> !!''n''<sup>10</sup>\n|-\n|width=\"8%\"|'''2''' || width=\"8%\"|4|| width=\"7%\"|8|| width=\"8%\"|16|| width=\"9%\"|32|| width=\"10%\"|64|| width=\"11%\"|128|| width=\"12%\"|256|| width=\"12%\"|512|| width=\"14%\"|1,024\n|-\n|'''3''' ||9||27||81||243||729||2,187||6,561||19,683||59,049\n|-\n|'''4''' ||16||64||256||1,024||4,096||16,384||65,536||262,144||1,048,576\n|-\n|'''5''' ||25||125||625||3,125||15,625||78,125||390,625||1,953,125||9,765,625\n|-\n|'''6''' ||36||216||1,296||7,776||46,656||279,936||1,679,616||10,077,696||60,466,176\n|-\n|'''7''' ||49||343||2,401||16,807||117,649||823,543||5,764,801||40,353,607||282,475,249\n|-\n|'''8''' ||64||512||4,096||32,768||262,144||2,097,152||16,777,216||134,217,728||1,073,741,824\n|-\n|'''9''' ||81||729||6,561||59,049||531,441||4,782,969||43,046,721||387,420,489||3,486,784,401\n|-\n|'''10'''||100||1,000||10,000||100,000||1,000,000||10,000,000||100,000,000||1,000,000,000||10,000,000,000\n|}\n\n==Rational exponents==\n{{Main article|nth root}}\n[[Image:Mplwp roots 01.svg|right|thumb|300px|From top to bottom: ''x''<sup>1/8</sup>, ''x''<sup>1/4</sup>, ''x''<sup>1/2</sup>, ''x''<sup>1</sup>, ''x''<sup>2</sup>, ''x''<sup>4</sup>, ''x''<sup>8</sup>.]]\n\nAn '''''n''th root''' of a [[number]] ''b'' is a number ''x'' such that {{math|1=''x<sup>n</sup>'' = ''b''}}.\n\nIf ''b'' is a positive real number and ''n'' is a positive integer, then there is exactly one positive real solution to {{math|1=''x<sup>n</sup>'' = ''b''}}. This solution is called the '''principal [[Nth root|''n''th root]]''' of ''b''. It is denoted {{radic|''b''|''n''}}, where {{radic|&nbsp;&nbsp;}} is the '''radical''' [[symbol]]; alternatively, the principal root may be written ''b''<sup>1/''n''</sup>. For example: {{math|1=9<sup>1/2</sup> = {{radic|9}} = 3}} and {{math|1=8<sup>1/3</sup> = {{radic|8|3}} = 2}}.\n\nThe fact that <math>x = b^\\frac{1}{n}</math> solves <math>x^n = b</math> follows from noting that\n:<math>\\begin{align}\n  x^n &= \\left(b^\\frac{1}{n}\\right)^n\n       = \\underbrace{b^\\frac{1}{n} \\times b^\\frac{1}{n} \\times \\cdots \\times b^\\frac{1}{n}}_{n \\, \\textrm{times}} \\\\\n      &=  b^{\\underbrace{\\left(\\frac{1}{n} + \\frac{1}{n} + \\cdots + \\frac{1}{n}\\right)}_{n \\, \\textrm{times}}}\n       = b^\\frac{n}{n} = b^1 = b.\n\\end{align}</math>\n\nIf ''n'' is [[parity (mathematics)|even]] and ''b'' is positive, then {{math|1=''x<sup>n</sup>'' = ''b''}} has two real solutions, which are the positive and negative ''n''th roots of ''b'', that is, {{math|''b''<sup>1/''n''</sup> > 0}} and {{math|−(''b''<sup>1/''n''</sup>) < 0.}} \n\nIf ''n'' is even and ''b'' is negative, the equation has no solution in real numbers.\n\nIf ''n'' is odd, then {{math|1=''x<sup>n</sup>'' = ''b''}} has exactly one real solution, which is positive if ''b'' is positive ({{math|''b''<sup>1/''n''</sup> > 0}}) and negative if ''b'' is negative ({{math|''b''<sup>1/''n''</sup> < 0}}).\n\nTaking a positive real number ''b'' to a [[rational number|rational]] exponent ''u''/''v'', where ''u'' is an integer and ''v'' is a positive integer, and considering principal roots only, yields\n\n:<math>b^\\frac{u}{v} = \\left(b^u\\right)^\\frac{1}{v} = \\sqrt[v]{b^u} =  \\left(b^\\frac{1}{v}\\right)^u = \\left(\\sqrt[v]{b}\\right)^u.</math>\n\nTaking a negative real number ''b'' to a rational power ''u''/''v'', where ''u''/''v'' is in lowest terms, yields a positive real result if ''u'' is even, and hence ''v'' is odd, because then ''b''<sup>''u''</sup> is positive; and yields a negative real result, if ''u'' and ''v'' are both odd, because then ''b''<sup>''u''</sup> is negative. The case of even ''v'' (and, hence,  odd ''u'') cannot be treated this way within the reals, since there is no real number ''x'' such that {{math|1=''x''<sup>2''k''</sup> = −1}}, the value of ''b''<sup>''u''/''v''</sup> in this case must use the [[imaginary unit]] ''i'', as described more fully in the section [[#Powers of complex numbers|§&nbsp;Powers of complex numbers]].\n\nThus we have {{math|1=(−27)<sup>1/3</sup> = −3}} and {{math|1=(−27)<sup>2/3</sup> = 9}}. The number 4 has two 3/2''th'' powers, namely 8 and −8; however, by convention the notation 4<sup>3/2</sup> employs the '''principal root''', and results in 8. For employing the ''v''-th root the ''u''/''v''-th power is also called the ''u''/''v''-th root, and for even ''v'' the term ''principal root'' denotes also the positive result.\n\nThis sign ambiguity needs to be taken care of when applying the power identities. For instance:\n\n:<math>-27 = (-27)^{\\left(\\left(\\frac{2}{3}\\right)\\left(\\frac{3}{2}\\right)\\right)} = \\left((-27)^\\frac{2}{3}\\right)^\\frac{3}{2} = 9^\\frac{3}{2} = 27</math>\n\nis clearly wrong. The problem starts already in the first equality by introducing a ''standard'' notation for an inherently ambiguous situation –asking for an even root– and simply relying wrongly on only one, the ''conventional'' or ''principal'' interpretation. The same problem occurs also with an inappropriately introduced surd-notation, inherently enforcing a positive result:\n:<math>\\left((-27)^\\frac{2}{3}\\right)^\\frac{3}{2} = \\sqrt{\\left(\\sqrt[3]{(-27)^2}\\right)^3} = \\sqrt{(-27)^{2}} \\ne -27</math>\n\ninstead of\n:<math>\\left((-27)^\\frac{2}{3}\\right)^\\frac{3}{2} = -\\sqrt{\\left(\\sqrt[3]{(-27)^2}\\right)^3} = -\\sqrt{(-27)^2} = -27.</math>\n\nIn general the same sort of problems occur for complex numbers as described in the section {{section link||Failure of power and logarithm identities}}.\n\n==Real exponents==\nExponentiation to real powers of positive real numbers can be defined either by extending the rational powers to reals by continuity, or more usually as given in {{section link||Powers via logarithms}} below. The result is always a positive real number, and the [[#Identities and properties|identities and properties]] shown above for integer exponents are true for positive real bases with non-integer exponents as well. \n\nOn the other hand, exponentiation to a real power of a negative real number is much more difficult to define consistently, as it may be non-real and have several values (see {{section link||Real exponents with negative bases}}). One may choose one of these values, called the [[principal value]], but there is no choice of the principal value for which an identity such as\n:<math>\\left(b^r\\right)^s = b^{r\\cdot s}</math>\nis true; see {{section link||Failure of power and logarithm identities}}. Therefore, exponentiation with a basis that is not a positive real number is generally viewed as a [[multivalued function]].\n\n===Limits of rational exponents===\n[[File:Continuity of the Exponential at 0.svg|thumb|Because the exponential function is continuous we find <math>\\lim_{n\\to\\infty} e^{x_n} = e^{\\lim_{n\\to\\infty} x_n}</math> for convergent sequences (''x<sub>n</sub>''). This is shown here for ''x<sub>n</sub>''&nbsp;=&nbsp;{{sfrac|1|''n''}}.]]\nSince any [[irrational number]] can be expressed as the [[limit of a sequence]] of rational numbers, exponentiation of a positive real number ''b'' with an arbitrary real exponent ''x'' can be defined by [[continuous function|continuity]] with the rule<ref name=Denlinger>{{cite book |title=Elements of Real Analysis |last=Denlinger |first=Charles G. |publisher=Jones and Bartlett |year=2011 |pages=278–283 |isbn=978-0-7637-7947-4}}</ref>\n:<math> b^x = \\lim_{r (\\in\\mathbb{Q})\\to x} b^r\\quad(b \\in\\mathbb{R}^+,\\,x\\in\\mathbb{R})</math>\n\nwhere the limit as ''r'' gets close to ''x'' is taken only over rational values of ''r''. This limit only exists for positive ''b''. The [[(ε, δ)-definition of limit|(''ε'',&nbsp;''δ'')-definition of limit]] is used; this involves showing that for any desired accuracy of the result ''b''<sup>''x''</sup> one can choose a sufficiently small interval around {{mvar|x}} so all the rational powers in the interval are within the desired accuracy.\n\nFor example, if {{math|1=''x'' = ''π''}}, the nonterminating decimal representation {{math|1=''π'' = 3.14159…}} can be used (based on strict monotonicity of the rational power) to obtain the intervals bounded by rational powers\n:<math>\\left[b^3, b^4\\right]</math>, <math>\\left[b^{3.1}, b^{3.2}\\right]</math>, <math>\\left[b^{3.14}, b^{3.15}\\right]</math>, <math>\\left[b^{3.141}, b^{3.142}\\right]</math>, <math>\\left[b^{3.1415}, b^{3.1416}\\right]</math>, <math>\\left[b^{3.14159}, b^{3.14160}\\right]</math>, …\n\nThe bounded intervals converge to a unique real number, denoted by <math>b^\\pi</math>. This technique can be used to obtain the power of a positive real number ''b'' for any irrational exponent. The function {{math|1=''f''<sub>''b''</sub>(''x'') = ''b''<sup>''x''</sup>}} is thus defined for any real number ''x''.\n\n===The exponential function===\n{{Main article|Exponential function}}\n\nThe important mathematical constant [[E (mathematical constant)|{{mvar|e}}]], sometimes called [[Euler's number]], is approximately equal to 2.718 and is the base of the [[natural logarithm]]. Although exponentiation of ''e'' could, in principle, be treated the same as exponentiation of any other real number, such exponentials turn out to have particularly elegant and useful properties. Among other things, these properties allow exponentials of ''e'' to be generalized in a natural way to other types of exponents, such as complex numbers or even matrices, while coinciding with the familiar meaning of exponentiation with rational exponents.\n\nAs a consequence, the notation ''e''<sup>''x''</sup> usually denotes a generalized exponentiation definition called the '''exponential function''', exp(''x''), which can be defined [[Characterizations of the exponential function|in many equivalent ways]], for example by:\n:<math>\\exp(x) = \\lim_{n \\rightarrow \\infty} \\left(1+\\frac x n \\right)^n </math>\n\nAmong other properties, exp satisfies the exponential identity\n:<math>\\exp(x + y) = \\exp(x) \\cdot \\exp(y) .</math>\n\nThe exponential function is defined for all integer, fractional, real, and [[complex number|complex]] values of ''x''. In fact, the [[matrix exponential]] is well-defined for [[square matrix|square matrices]] (in which case this exponential identity only holds when ''x'' and ''y'' commute), and is useful for solving systems of [[linear differential equation]]s.\n\nSince exp(1) is equal to ''e'' and exp(''x'') satisfies this exponential identity, it immediately follows that exp(''x'') coincides with the repeated-multiplication definition of ''e''<sup>''x''</sup> for integer ''x'', and it also follows that rational powers denote (positive) roots as usual, so exp(''x'') coincides with the ''e''<sup>''x''</sup> definitions in the previous section for all real ''x'' by continuity.\n\n===Powers via logarithms===\nWhen {{math|''e''<sup>''x''</sup>}} is defined as the exponential function, {{math|''b''<sup>''x''</sup>}} can be defined, for other positive real numbers {{math|''b''}}, in terms of {{math|''e''<sup>''x''</sup>}}.  Specifically, the [[natural logarithm]] {{math|ln(''x'')}} is the [[inverse function|inverse]] of the exponential function {{math|''e''<sup>''x''</sup>}}. It is defined for {{math|''b'' > 0}}, and satisfies\n:<math>b = e^{\\ln b}</math>\n\nIf ''b''<sup>''x''</sup> is to preserve the logarithm and exponent rules, then one must have\n:<math>b^x = \\left(e^{\\ln b}\\right)^x = e^{x \\cdot\\ln b}</math>\n\nfor each real number {{math|''x''}}.\n\nThis can be used as an alternative definition of the real number power {{math|''b''<sup>''x''</sup>}} and agrees with the definition given above using rational exponents and continuity.  The definition of exponentiation using logarithms is more common in the context of complex numbers, as discussed below.\n\n===Real exponents with negative bases===\nPowers of a positive real number are always positive real numbers. The solution of x<sup>2</sup>&nbsp;=&nbsp;4, however, can be either 2 or −2. The principal value of 4<sup>1/2</sup> is 2, but −2 is also a valid square root. If the definition of exponentiation of real numbers is extended to allow negative results then the result is no longer well-behaved.\n\nNeither the logarithm method nor the rational exponent method can be used to define ''b''<sup>''r''</sup> as a real number for a negative real number ''b'' and an arbitrary real number ''r''. Indeed, ''e''<sup>''r''</sup> is positive for every real number ''r'', so ln(''b'') is not defined as a real number for {{math|''b'' ≤ 0}}.\n\nThe rational exponent method cannot be used for negative values of ''b'' because it relies on [[continuous function|continuity]]. The function {{math|1=''f''(''r'') = ''b''<sup>''r''</sup>}} has a unique continuous extension<ref name=Denlinger /> from the rational numbers to the real numbers for each {{math|''b'' > 0}}.  But when {{math|''b'' < 0}}, the function ''f'' is not even continuous on the set of rational numbers ''r'' for which it is defined.\n\nFor example, consider {{math|1=''b'' = −1}}. The ''n''th root of −1 is −1 for every odd natural number ''n''. So if ''n'' is an odd positive integer, {{math|1=(−1)<sup>(''m''/''n'')</sup> = −1}} if ''m'' is odd, and {{math|1=(−1)<sup>(''m''/''n'')</sup> = 1}} if ''m'' is even.  Thus the set of rational numbers ''q'' for which {{math|1=(−1)<sup>''q''</sup> = 1}} is [[dense set|dense]] in the rational numbers, as is the set of ''q'' for which {{math|1=(−1)<sup>''q''</sup> = −1}}. This means that the function (−1)<sup>''q''</sup> is not continuous at any rational number ''q'' where it is defined.\n\nOn the other hand, arbitrary [[#Powers of complex numbers|complex powers]] of negative numbers ''b'' can be defined by choosing a [[complex logarithm|''complex'' logarithm]] of ''b''.\n\n===Irrational exponents===\n{{main article|Gelfond–Schneider theorem}}\nIf ''b'' is a positive real [[algebraic number]], and ''x'' is a rational number, it has been shown above that ''b''<sup>''x''</sup> is an algebraic number. This remains true even if one accepts any algebraic number for ''b'', with the only difference that ''b''<sup>''x''</sup> may take several values (a finite number, see below), which are all algebraic. The Gelfond–Schneider theorem provides some information on the nature of ''b''<sup>''x''</sup> when ''x'' is [[irrational number|irrational]] (that is, ''not rational''). It states:\n{{quote|If ''b'' is an algebraic number different from 0 and 1, and ''x'' an irrational algebraic number, then all values of ''b''<sup>''x''</sup> (there are infinitely many) are [[transcendental number|transcendental]] (that is, not algebraic).}}\n\n==Complex exponents with a positive real base==\n\nIf {{mvar|b}} is a positive real number, and {{mvar|z}} is any [[complex number]], the power {{math|''b''<sup>''z''</sup>}} is defined as\n:<math>b^z = \\left(e^{\\ln b}\\right)^z = e^{(z\\ln b)},</math>\nwhere {{math|''x'' {{=}} ln(''b'')}} is the unique real solution to the equation {{math|''e''<sup>''x''</sup> {{=}} ''b''}}, and the complex power of {{math|''e''}} is defined by the [[exponential function]], which is the unique [[function of a complex variable]] that is equal to its derivative and takes the value 1 for {{math|1=''x'' = 0}}.\n\nAs, in general, {{math|''b''<sup>''z''</sup>}} is not a real number, an expression such as {{math|(''b''<sup>''z''</sup>)<sup>''w''</sup>}} is not defined by the previous definition. It must be interpreted via the rules for [[#Powers of complex numbers|powers of complex numbers]], and, unless {{mvar|z}} is real or {{mvar|w}} is integer, does not generally equal {{math|''b''<sup>''zw''</sup>}}, as one might expect.\n\nThere are various [[characterizations of the exponential function|definitions of the exponential function]] but they extend compatibly to complex numbers and satisfy the exponential property. For any complex numbers ''z'' and ''w'', the exponential function satisfies <math>e^{z+w} = e^z e^w</math>. In particular, for any complex number <math>z = x+iy</math>\n:<math>e^z = e^{x+iy} = e^x \\cdot e^{iy},</math>\n\nThe second term <math>e^{iy}</math> has a value given by [[Euler's formula]]\n:<math>e^{iy} = \\cos y + i \\sin y.</math>\n\nThis formula links problems in [[trigonometry]] and algebra.\n\nTherefore, for any complex number <math>z = x+iy,</math>\n:<math>e^z = e^{x+iy} = e^x \\cdot e^{iy} = e^x (\\cos y + i \\sin y).</math>\n\nBecause of the [[Pythagorean trigonometric identity]], the [[absolute value]] of <math>\\cos y + i \\sin y</math> is {{math|1}}. Therefore the real factor <math>e^x</math> is the absolute value of <math>e^z</math> and the imaginary part <math>y</math> of the exponent identifies the [[Argument (complex analysis)|argument]] (angle) of the complex number <math>e^z</math>.\n\n===Series definition===\n\nThe exponential function being equal to its derivative  and satisfying <math>e^0=1,</math> its [[Taylor series]] must be\n:<math>e^z = \\sum_{n=0}^\\infty {z^n \\over n!} = 1 + z + \\frac{z^2}{2!} + \\frac{z^3}{3!} + \\frac{z^4}{4!} + \\cdots.</math>\n\nThis [[infinite series]], which is often taken as the definition of the exponential function {{math|''e''<sup>''z''</sup>}} for arbitrary complex exponents, is [[absolute convergence|absolutely convergent]] for all complex numbers {{mvar|z.}}\n\nWhen ''z'' is purely [[imaginary number|imaginary]], that is, ''z'' = ''iy'' for a real number ''y'', the series above becomes\n:<math>e^{iy} = 1 + iy + \\frac{(iy)^2}{2!} + \\frac{(iy)^3}{3!} + \\frac{(iy)^4}{4!} + \\cdots</math>,\n\nwhich (because it converges absolutely) may be reordered to\n:<math>e^{iy} = \\left(1 - \\frac{y^2}{2!} + \\frac{y^4}{4!} - \\frac{y^6}{6!} + \\cdots\\right) + i \\left(y - \\frac{y^3}{3!} + \\frac{y^5}{5!} - \\cdots\\right)</math>.\n\nThe real and the imaginary parts of this expression are [[Taylor series#Trigonometric functions|Taylor expansions of cosine and sine]], respectively, centered at zero, implying Euler's formula:\n:<math>e^{iy} = \\cos y + i \\sin y</math>.\n\n===Limit definition===\n\n[[Image:ExpIPi.gif|300px|thumb|right|This animation shows by repeated multiplications in the [[complex plane]] for values of {{mvar|n}} (denoted as {{math|N}} in the picture), increasing from {{math|1}} to {{math|100,}} how <math>(1 + i\\pi/n)^n</math> approaches {{math|-1.}} The values of <math>(1 + i\\pi/n)^k,</math> for ''k'' = 0 ... ''n'', are the vertices of a polygonal path whose leftmost endpoint is <math>(1 + i\\pi/k)^k</math> for the actual {{mvar|k.}} It can be seen that as {{mvar|k}} gets larger <math>(1 + i\\pi/k)^k</math> approaches the limit {{math|−1,}} illustrating  [[Euler's identity]]: <math>e^{i\\pi} = -1.</math>]]\n\nAnother characterization of the exponential function <math>e^z</math> is as the [[limit of a sequence|limit]] of <math>(1 + z/n)^n</math>, as ''n'' approaches infinity.  By thinking of the ''n''th power in this definition as repeated multiplication in [[polar form]], it can be used to visually illustrate Euler's formula.  Any complex number can be represented in polar form as <math>(r,\\theta)</math>, where ''r'' is the absolute value and ''θ'' is its argument.  The product of two complex numbers <math>\\left(r_1, \\theta_1\\right)</math> and <math>\\left(r_2, \\theta_2\\right)</math> is <math>\\left(r_1 r_2, \\theta_1 + \\theta_2\\right)</math>.\n\nConsider the [[right triangle]] in the [[complex plane]] which has <math>0</math>, <math>1</math>, and <math>1 + ix/n</math> as vertices. For large values of ''n'', the triangle is almost a [[circular sector]] with a radius of 1 and a small central angle equal to <math>x/n</math> [[radian]]s. 1 + <math>ix/n</math> may then be approximated by the number with polar form <math>(1, x/n)</math>. So, in the limit as ''n'' approaches infinity, <math>(1 + ix/n)^n</math> approaches <math>(1, x/n)^n = \\left(1^n, nx/n\\right) = (1, x)</math>, the point on the [[unit circle]] whose angle from the [[positive real axis]] is ''x'' radians. The [[Cartesian coordinate]]s of this point are <math>(\\cos x, \\sin x)</math>, so <math>e^{ix} = \\cos x + i \\sin x</math>; this is −again− Euler's formula, allowing for the same connections to the trigonometric functions as elaborated with the series definition.\n\n===Periodicity===\n\nThe solutions to the equation <math>e^z = 1</math> are the integer multiples of <math>2 \\pi i</math>:\n:<math>\\left\\{ z : e^z = 1 \\right\\} = \\{ 2k\\pi i : k \\in \\mathbb{Z} \\}</math>\n\nThus, if <math>v</math> is a complex number such that <math>e^v = w</math>, then every <math>z</math> that also satisfies <math>e^z = w</math> can be obtained from <math>e^z = e^v\\cdot 1 =  e^{v+i2k\\pi}</math>, i.e., by adding an arbitrary integer multiple of <math>2 \\pi i</math> to <math>v</math>:\n:<math>\\left\\{ z : e^z = w \\right\\} = \\{ v + 2k\\pi i : k \\in \\mathbb{Z} \\}</math>\n\nThat is, the complex exponential function <math>e^z = \\exp(z) = \\exp(z + 2k\\pi i)</math> for any integer {{mvar|k}} is a [[periodic function]] with period <math>2 \\pi i</math>.\n\n===Examples===\n:<math>\\begin{align}\n                      2^i &= e^{i \\ln(2)} = \\cos(\\ln(2)) + i \\sin(\\ln(2)) \\approx 0.76924 + 0.63896 i \\\\\n                      e^i &= \\cos(1) + i\\sin(1) \\approx 0.54030 + 0.84147 i \\\\\n  \\left(e^{2\\pi}\\right)^i &= {(535.49165\\dots)}^i = e^{i\\ln\\left(e^{2\\pi}\\right)} = e^{i(2\\pi)} = \\cos(2\\pi) + i\\sin(2\\pi)= 1.\n\\end{align}</math>\n\n==Powers of complex numbers==\nInteger powers of nonzero complex numbers are defined by repeated multiplication or division as above. If ''i'' is the [[imaginary unit]] and ''n'' is an integer, then ''i''<sup>''n''</sup> equals 1, ''i'', −1, or −''i'', according to whether the integer ''n'' is congruent to 0, 1, 2, or 3 modulo 4. Because of this, the powers of ''i'' are useful for expressing [[sequence]]s of [[Root of unity#Periodicity|period 4]].\n\nComplex powers of positive reals are defined via ''e''<sup>''x''</sup> as in section [[#Complex exponents with positive real bases|Complex exponents with positive real bases]] above. These are continuous functions.\n\nTrying to extend these functions to the general case of noninteger powers of complex numbers that are not positive reals leads to difficulties. Either we define discontinuous functions or [[multivalued function]]s. Neither of these options is entirely satisfactory.\n\nThe rational power of a complex number must be the solution to an algebraic equation. Therefore, it always has a finite number of possible values. For example, {{math|1=''w'' = ''z''<sup>1/2</sup>}} must be a solution to the equation {{math|1=''w''<sup>2</sup> = ''z''}}. But if ''w'' is a solution, then so is −''w'', because {{math|1=(−1)<sup>2</sup> = 1}}. A unique but somewhat arbitrary solution called the [[principal value]] can be chosen using a general rule which also applies for nonrational powers.\n\nComplex powers and logarithms are more naturally handled as single valued functions on a [[Riemann surface]]. Single valued versions are defined by choosing a sheet. The value has a discontinuity along a [[branch cut]]. Choosing one out of many solutions as the principal value leaves us with functions that are not continuous, and the usual rules for manipulating powers can lead us astray.\n\nAny nonrational power of a complex number has an infinite number of possible values because of the multi-valued nature of the [[complex logarithm]]. The principal value is a single value chosen from these by a rule which, amongst its other properties, ensures powers of complex numbers with a positive real part and zero imaginary part give the same value as does the rule defined [[Exponentiation#Complex exponents with positive real bases 2|above]] for the corresponding real base.\n\nExponentiating a real number to a complex power is formally a different operation from that for the corresponding complex number. However, in the common case of a positive real number the principal value is the same.\n\nThe powers of negative real numbers are not always defined and are discontinuous even where defined. In fact, they are only defined when the exponent is a rational number with the denominator being an odd integer. When dealing with complex numbers the complex number operation is normally used instead.\n\n===Complex exponents with complex bases===\nFor complex numbers ''w'' and ''z'' with {{math|''w'' ≠ 0}}, the notation ''w''<sup>''z''</sup> is ambiguous in the same sense that [[complex logarithm|log&nbsp;''w'']] is.\n\nTo obtain a value of ''w''<sup>''z''</sup>, first choose a logarithm of ''w''; call it {{math|log ''w''}}.  Such a choice may be the [[complex logarithm#Definition of principal value|principal value]] {{math|Log ''w''}} (the default, if no other specification is given), or perhaps a value given by some other [[complex logarithm#Branches of the complex logarithm|branch of log&nbsp;''w'']] fixed in advance.  Then, using the complex exponential function one defines\n\n:<math>w^z = e^{z \\log w}</math>\n\nbecause this agrees with the [[#Real exponents|earlier definition]] in the case where ''w'' is a positive real number and the (real) principal value of {{math|log ''w''}} is used.\n\nIf ''z'' is an [[integer]], then the value of ''w''<sup>''z''</sup> is independent of the choice of {{math|log ''w''}}, and it agrees with the [[#Positive integer exponents|earlier definition of exponentiation with an integer exponent]].\n\nIf ''z'' is a [[rational number]] ''m''/''n'' in lowest terms with {{math|''z'' > 0}}, then the [[countably infinite]]ly many choices of {{math|log ''w''}} yield only ''n'' different values for ''w''<sup>''z''</sup>; these values are the ''n'' complex solutions ''s'' to the equation {{math|1=''s''<sup>''n''</sup> = ''w''<sup>''m''</sup>}}.\n\nIf ''z'' is an [[irrational number]], then the countably infinitely many choices of {{math|log ''w''}} lead to infinitely many distinct values for ''w''<sup>''z''</sup>.\n\nThe computation of complex powers is facilitated by converting the base ''w'' to [[polar form]], as described in detail [[#Computing complex powers|below]].\n\nA similar construction is employed in [[Quaternion#Exponential, logarithm, and power|quaternions]].\n\n===Complex roots of unity===\n{{Main article|Root of unity}}\n\n[[File:One3Root.svg|thumb|right|The three 3rd roots of 1]]\nA complex number ''w'' such that {{math|1=''w''<sup>''n''</sup> = 1}} for a positive integer ''n'' is an '''''n''th root of unity'''.  Geometrically, the ''n''th roots of unity lie on the unit circle of the complex plane at the vertices of a regular ''n''-gon with one vertex on the real number 1.\n\nIf {{math|1=''w''<sup>''n''</sup> = 1}} but {{math|''w''<sup>''k''</sup> ≠ 1}} for all natural numbers ''k'' such that {{math|0 < ''k'' < ''n''}}, then ''w'' is called a '''primitive ''n''th root of unity'''.  The negative unit −1 is the only primitive square root of unity. The [[imaginary unit]] ''i'' is one of the two primitive 4th roots of unity; the other one is −''i''.\n\nThe number ''e''<sup>{{sfrac|2''πi''|''n''}}</sup> is the primitive ''n''th root of unity with the smallest positive [[Argument (complex analysis)|argument]].  (It is sometimes called the '''principal ''n''th root of unity''', although this terminology is not universal and should not be confused with the [[principal value]] of {{radic|1|''n''}}, which is 1.<ref>This definition of a principal root of unity can be found in:\n* {{cite book | title = Introduction to Algorithms | edition = second |author1=Thomas H. Cormen |author2=Charles E. Leiserson |author3=Ronald L. Rivest |author4=Clifford Stein | publisher = MIT Press | year = 2001 | isbn = 978-0-262-03293-3}} [http://highered.mcgraw-hill.com/sites/0070131511/student_view0/chapter30/glossary.html Online resource] {{webarchive|url=https://web.archive.org/web/20070930201902/http://highered.mcgraw-hill.com/sites/0070131511/student_view0/chapter30/glossary.html |date=2007-09-30 }}\n* {{cite book | title = Difference Equations: From Rabbits to Chaos | edition = [[Undergraduate Texts in Mathematics]] |author1=Paul Cull |author2=Mary Flahive|author2-link= Mary Flahive |author3=Robby Robson | year = 2005 | publisher = Springer | isbn = 978-0-387-23234-8 }} Defined on p. 351\n* \"[http://mathworld.wolfram.com/PrincipalRootofUnity.html Principal root of unity]\", MathWorld.</ref>)\n\nThe other ''n''th roots of unity are given by\n:<math>\\left( e^{ \\frac{2}{n} \\pi i } \\right) ^k = e^{ \\frac{2}{n} \\pi i k }</math>\n\nfor {{math|2 ≤ ''k'' ≤ ''n''}}.\n\n===Roots of arbitrary complex numbers===\nAlthough there are infinitely many possible values for a general complex logarithm, there are only a finite number of values for the power {{math|''w<sup>q</sup>''}} in the important special case where {{math|1=''q'' = 1/''n''}} and {{math|''n''}} is a positive integer. These are the {{math|''n''}}''th roots'' of {{math|''w''}}; they are solutions of the equation {{math|1=''z<sup>n</sup>'' = ''w''}}.  As with real roots, a second root is also called a square root and a third root is also called a cube root.\n\nIt is usual in mathematics to define {{math|''w''<sup>1/''n''</sup>}} as the [[principal value]] of the root, which is, conventionally, the {{math|''n''}}th root whose argument has the smallest [[absolute value]]. When {{math|''w''}} is a positive real number, this is coherent with the usual convention of defining {{math|''w''<sup>1/''n''</sup>}} as the unique positive real {{math|''n''}}th root. On the other hand, when {{math|''w''}} is a negative real number, and {{mvar|n}} is an odd integer, the unique real {{math|''n''}}th root is not one of the two {{math|''n''}}th roots whose argument has the smallest absolute value. In this case, the meaning of {{math|''w''<sup>1/''n''</sup>}} may depend on the context, and some care may be needed for avoiding errors.\n\nThe set of {{math|''n''}}th roots of a complex number {{math|''w''}} is obtained by multiplying the principal value {{math|''w''<sup>1/''n''</sup>}} by each of the {{math|''n''}}th roots of unity. For example, the fourth roots of 16 are 2, −2, 2{{math|''i''}}, and −2{{math|''i''}}, because the principal value of the fourth root of 16 is 2 and the fourth roots of unity are 1, −1, {{math|''i''}}, and −{{math|''i''}}.\n\n===Computing complex powers===\n<!-- {{main|Polar coordinate system}} This is not the MAIN article on complex powers -->\nIt is often easier to compute complex powers by writing the number to be exponentiated in [[Principal argument|polar form]].  Every complex number ''z'' can be written in the polar form\n:<math>z = re^{i\\theta} = e^{\\log(r) + i\\theta}</math>\n\nwhere ''r'' is a nonnegative real number and ''θ'' is the (real) [[complex argument|argument]] of ''z''.  The polar form has a simple geometric interpretation: if a complex number {{math|''u'' + ''iv''}} is thought of as representing a point {{math|(''u'', ''v'')}} in the [[complex plane]] using [[Cartesian coordinate system|Cartesian coordinates]], then {{math|(''r'', ''θ'')}} is the same point in [[polar coordinates]].  That is, ''r'' is the \"radius\" {{math|1=''r''<sup>2</sup> = ''u''<sup>2</sup> + ''v''<sup>2</sup>}} and ''θ'' is the \"angle\" {{math|1=''θ'' = [[atan2]](''v'', ''u'')}}.  The polar angle ''θ'' is ambiguous since any integer multiple of 2π could be added to ''θ'' without changing the location of the point.  Each choice of ''θ'' gives in general a different possible value of the power.  A [[branch cut]] can be used to choose a specific value.  The principal value (the most common branch cut), corresponds to ''θ'' chosen in the interval {{open-closed|−π, π}}. For complex numbers with a positive real part and zero imaginary part using the principal value gives the same result as using the corresponding real number.\n\nIn order to compute the complex power ''w''<sup>''z''</sup>, write ''w'' in polar form:\n:<math>w = r e^{i\\theta}</math>\n\nThen\n:<math>\\log(w) = \\log(r) + i \\theta</math>\n\nand thus\n:<math>w^z = e^{z \\log(w)} = e^{z(\\log(r) + i\\theta)}</math>\n\nIf ''z'' is decomposed as {{math|''c'' + ''di''}}, then the formula for ''w''<sup>''z''</sup> can be written more explicitly as\n:<math>\\left( r^c e^{-d\\theta} \\right) e^{i (d \\log(r) + c\\theta)} = \\left( r^c e^{-d\\theta} \\right) \\left[ \\cos(d \\log(r) + c\\theta) + i \\sin(d \\log(r) + c\\theta) \\right]</math><!-- e^{c \\log(r) - d\\theta + i (d \\log(r) + c\\theta)}  -->\n\nThis final formula allows complex powers to be computed easily from decompositions of the base into polar form and the exponent into Cartesian form.  It is shown here both in polar form and in Cartesian form (via Euler's identity).\n\nThe following examples use the principal value, the branch cut which causes ''θ'' to be in the interval {{math|(−π, π]}}. To compute ''i''<sup>''i''</sup>, write ''i'' in polar and Cartesian forms:\n:<math>\\begin{align}\n  i &= 1 \\cdot e^{\\frac{1}{2} i \\pi} \\\\\n  i &= 0 + 1i\n\\end{align}</math>\n\nThen the formula above, with {{math|1=''r'' = 1}}, {{math|1=''θ'' = {{sfrac|π|2}}}}, {{math|1=''c'' = 0}}, and {{math|1=''d'' = 1}}, yields:\n:<math>i^i = \\left( 1^0 e^{-\\frac{1}{2}\\pi} \\right) e^{i \\left[1 \\cdot \\log(1) + 0 \\cdot \\frac{1}{2}\\pi \\right]} = e^{-\\frac{1}{2}\\pi} \\approx 0.2079</math>\n\nSimilarly, to find {{math|(−2)<sup>3 + 4''i''</sup>}}, compute the polar form of −2,\n:<math>-2 = 2e^{i \\pi}</math>\n\nand use the formula above to compute\n:<math>(-2)^{3 + 4i} = \\left( 2^3 e^{-4\\pi} \\right) e^{i[4\\log(2) + 3\\pi]} \\approx (2.602 - 1.006 i) \\cdot 10^{-5}</math>\n\nThe value of a complex power depends on the branch used.  For example, if the polar form {{math|1=''i'' = 1''e''<sup>5''πi''/2</sup>}} is used to compute ''i''<sup>''i''</sup>, the power is found to be ''e''<sup>−5''π''/2</sup>; the principal value of ''i''<sup>''i''</sup>, computed above, is ''e''<sup>−π/2</sup>. The set of all possible values for ''i''<sup>''i''</sup> is given by:<ref>[http://www.cut-the-knot.org/do_you_know/complex.shtml Complex number to a complex power may be real] at Cut The Knot gives some references to ''i''<sup>''i''</sup></ref>\n:<math>\\begin{align}\n  i   &= 1 \\cdot e^{\\frac{1}{2} i\\pi + i 2 \\pi k} \\mid k \\isin \\mathbb{Z} \\\\\n  i^i &= e^{i \\left(\\frac{1}{2} i\\pi + i 2 \\pi k\\right)} \\\\\n      &= e^{-\\left(\\frac{1}{2} \\pi + 2 \\pi k\\right)}\n\\end{align}</math>\n\nSo there is an infinity of values which are possible candidates for the value of ''i''<sup>''i''</sup>, one for each integer ''k''. All of them have a zero imaginary part so one can say ''i''<sup>''i''</sup> has an infinity of valid real values.\n\n===Failure of power and logarithm identities===\nSome identities for powers and logarithms for positive real numbers will fail for complex numbers, no matter how complex powers and complex logarithms are defined ''as single-valued functions''. For example:\n\n{{bulleted list\n| The identity {{math|1=log(''b''<sup>''x''</sup>) = ''x'' ⋅ log ''b''}} holds whenever {{mvar|b}} is a positive real number and {{mvar|x}} is a real number.  But for the [[principal branch]] of the complex logarithm one has\n: <math> i\\pi = \\log(-1) = \\log\\left[(-i)^2\\right] \\neq 2\\log(-i) = 2\\left(-\\frac{i\\pi}{2}\\right) = -i\\pi</math>\n\nRegardless of which branch of the logarithm is used, a similar failure of the identity will exist. The best that can be said (if only using this result) is that:\n: <math>\\log(w^z) \\equiv z \\cdot \\log(w) \\pmod{2 \\pi i}</math>\n\nThis identity does not hold even when considering log as a multivalued function. The possible values of {{math|log(''w''<sup>''z''</sup>)}} contain those of {{math|''z'' ⋅ log ''w''}} as a subset. Using {{math|Log(''w'')}} for the principal value of {{math|log(''w'')}} and {{mvar|m}}, {{mvar|n}} as any integers the possible values of both sides are:\n: <math>\\begin{align}\n            \\left\\{\\log(w^z)\\right\\} &= \\left\\{ z \\cdot \\operatorname{Log}(w) + z \\cdot 2 \\pi i n + 2 \\pi i m \\right\\} \\\\\n      \\left\\{z \\cdot \\log(w)\\right\\} &= \\left\\{ z \\cdot \\operatorname{Log}(w) + z \\cdot 2 \\pi i n \\right\\}\n    \\end{align}</math>\n| The identities {{math|1=(''bc'')<sup>''x''</sup> = ''b''<sup>''x''</sup>''c''<sup>''x''</sup>}} and {{math|1=(''b''/''c'')<sup>''x''</sup> = ''b''<sup>''x''</sup>/''c''<sup>''x''</sup>}} are valid when {{mvar|b}} and {{mvar|c}} are positive real numbers and {{mvar|x}} is a real number.  But a calculation using principal branches shows that\n: <math>1 = (-1 \\cdot -1)^\\frac{1}{2} \\not = (-1)^\\frac{1}{2}(-1)^\\frac{1}{2} = -1</math>\n\nand\n: <math>i = (-1)^\\frac{1}{2} = \\left(\\frac{1}{-1}\\right)^\\frac{1}{2} \\not = \\frac{1^\\frac{1}{2}}{(-1)^\\frac{1}{2}} = \\frac{1}{i} = -i</math>\n\nOn the other hand, when {{mvar|x}} is an integer, the identities are valid for all nonzero complex numbers.\n\nIf exponentiation is considered as a multivalued function then the possible values of {{math|(−1 ⋅ −1)<sup>1/2</sup>}} are {{math|{1, −1}}}. The identity holds, but saying {{math|1={1} = {(−1 ⋅ −1)<sup>1/2</sup>}}} is wrong.\n| The identity {{math|1=(''e''<sup>''x''</sup>)<sup>''y''</sup> = ''e''<sup>''xy''</sup>}} holds for real numbers {{mvar|x}} and {{mvar|y}}, but assuming its truth for complex numbers leads to the following [[Mathematical fallacy|paradox]], discovered in 1827 by [[Thomas Clausen (mathematician)|Clausen]]:<ref name=\"Clausen1827\">{{cite journal |vauthors=Steiner J, Clausen T, Abel NH |title=Aufgaben und Lehrsätze, erstere aufzulösen, letztere zu beweisen |trans-title=Problems and propositions, the former to solve, the later to prove |url=http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=270662 |journal=[[Crelle's Journal|Journal für die reine und angewandte Mathematik]] |volume=2 |year=1827 |pages=286–287}}</ref>\n\nFor any integer {{mvar|n}}, we have:\n# <math>e^{1 + 2 \\pi i n} = e^1 e^{2 \\pi i n} = e \\cdot 1 = e</math>\n# <math>\\left(e^{1 + 2\\pi i n}\\right)^{1 + 2 \\pi i n} = e\\qquad</math> (taking the <math>(1 + 2 \\pi i n)</math>-th power of both sides)\n# <math>e^{1 + 4 \\pi i n - 4 \\pi^2 n^2} = e\\qquad</math> (using <math>\\left(e^x\\right)^y = e^{xy}</math> and expanding the exponent)\n# <math>e^1 e^{4 \\pi i n} e^{-4 \\pi^2 n^2} = e\\qquad</math> (using <math>e^{x+y} = e^x e^y</math>)\n# <math>e^{-4 \\pi^2 n^2} = 1\\qquad</math> (dividing by {{mvar|e}})\n\nbut this is false when the integer {{mvar|n}} is nonzero.\n\nThe error is the following: by definition, <math>e^y</math> is a notation for <math>\\exp(y),</math> a true function, and <math>x^y</math> is a notation for <math>\\exp(y\\log x),</math> which is a multi-valued function. Thus the notation is ambiguous when {{math|1=''x'' = ''e''}}. Here, before expanding the exponent, the second line should be\n: <math>\\exp\\left((1 + 2\\pi i n)\\log \\exp(1 + 2\\pi i n)\\right) = \\exp(1 + 2\\pi i n).</math>\n\nTherefore, when expanding the exponent, one has implicitly supposed that <math>\\log \\exp z =z</math> for complex values of {{mvar|z}}, which is wrong, as the complex logarithm is multivalued. In other words, the wrong identity {{math|1=(''e''<sup>''x''</sup>)<sup>''y''</sup> = ''e''<sup>''xy''</sup>}} must be replaced by the identity\n: <math>\\left(e^x\\right)^y = e^{y\\log e^x},</math>\n\nwhich is a true identity between multivalued functions.\n}}\n\n==Generalizations==\n\n===Monoids===\nExponentiation with integer exponents can be defined in any multiplicative [[monoid]].<ref>{{cite book|author=Nicolas Bourbaki|title=Algèbre|year=1970|publisher=Springer}}, I.2</ref>  A monoid is an [[algebraic structure]] consisting of a set ''X'' together with a rule for composition (\"multiplication\") satisfying an [[associative law]] and a [[multiplicative identity]], denoted by 1.  Exponentiation is defined inductively by:\n* <math>x^0=1</math> for all <math>x\\in X</math>\n* <math>x^{n+1}=x^nx</math> for all <math>x\\in X</math> and non-negative integers ''n''\n* If ''n'' is a negative integer then <math>x^n</math> is only defined<ref>{{cite book |author=David M. Bloom |title=Linear Algebra and Geometry |year=1979 |isbn=978-0-521-29324-2 |page=45}}</ref> if <math>x</math> has an inverse in ''X''.\n\nMonoids include many structures of importance in mathematics, including [[group (mathematics)|groups]] and [[ring (mathematics)|rings]] (under multiplication), with more specific examples of the latter being [[matrix ring]]s and [[field (mathematics)|fields]].\n\n===Matrices and linear operators===\nIf ''A'' is a square matrix, then the product of ''A'' with itself ''n'' times is called the [[matrix power]].  Also <math>A^0</math> is defined to be the identity matrix,<ref>Chapter 1, Elementary Linear Algebra, 8E, Howard Anton</ref> and if ''A'' is invertible, then <math>A^{-n} = \\left(A^{-1}\\right)^n</math>.\n\nMatrix powers appear often in the context of [[discrete dynamical system]]s, where the matrix ''A'' expresses a transition from a state vector ''x'' of some system to the next state ''Ax'' of the system.<ref>{{citation|first=Gilbert|last=Strang|title=Linear algebra and its applications|publisher=Brooks-Cole|year=1988|edition=3rd}}, Chapter 5.</ref>  This is the standard interpretation of a [[Markov chain]], for example.  Then <math>A^2x</math> is the state of the system after two time steps, and so forth: <math>A^nx</math> is the state of the system after ''n'' time steps.  The matrix power <math>A^n</math> is the transition matrix between the state now and the state at a time ''n'' steps in the future.  So computing matrix powers is equivalent to solving the evolution of the dynamical system.  In many cases, matrix powers can be expediently computed by using [[eigenvalues and eigenvectors]].\n\nApart from matrices, more general [[linear operator]]s can also be exponentiated.  An example is the [[derivative]] operator of calculus, <math>d/dx</math>, which is a linear operator acting on functions <math>f(x)</math> to give a new function <math>(d/dx)f(x) = f'(x)</math>.  The ''n''-th power of the differentiation operator is the ''n''-th derivative:\n:<math>\\left(\\frac{d}{dx}\\right)^nf(x) = \\frac{d^n}{dx^n}f(x) = f^{(n)}(x).</math>\n\nThese examples are for discrete exponents of linear operators, but in many circumstances it is also desirable to define powers of such operators with continuous exponents.  This is the starting point of the mathematical theory of [[c0-semigroup|semigroups]].<ref>E Hille, R S Phillips: ''Functional Analysis and Semi-Groups''. American Mathematical Society, 1975.</ref>  Just as computing matrix powers with discrete exponents solves discrete dynamical systems, so does computing matrix powers with continuous exponents solve systems with continuous dynamics.  Examples include approaches to solving the [[heat equation]], [[Schrödinger equation]], [[wave equation]], and other partial differential equations including a time evolution.  The special case of exponentiating the derivative operator to a non-integer power is called the [[fractional derivative]] which, together with the [[fractional integral]], is one of the basic operations of the [[fractional calculus]].\n\n===Finite fields===\nA [[field (mathematics)|field]] is an algebraic structure in which multiplication, addition, subtraction, and division are all well-defined and satisfy their familiar properties.  The real numbers, for example, form a field, as do the complex numbers and rational numbers.  Unlike these familiar examples of fields, which are all [[infinite set]]s, some fields have only finitely many elements.  The simplest example is the field with two elements <math>F_2=\\{0,1\\}</math> with addition defined by <math>0+1=1+0=1</math> and <math>0+0=1+1=0</math>, and multiplication <math>0\\cdot 0=1\\cdot 0 = 0\\cdot 1=0</math> and <math>1\\cdot 1=1</math>.\n\nExponentiation in finite fields has applications in [[public key cryptography]].  For example, the [[Diffie–Hellman key exchange]] uses the fact that exponentiation is computationally inexpensive in finite fields, whereas the [[discrete logarithm]] (the inverse of exponentiation) is computationally expensive.\n\nAny finite field ''F'' has the property that there is a unique [[prime number]] ''p'' such that <math>px=0</math> for all ''x'' in ''F''; that is, ''x'' added to itself ''p'' times is zero.  For example, in <math>F_2</math>, the prime number {{math|1=''p'' = 2}} has this property.  This prime number is called the [[characteristic of a field|characteristic]] of the field.  Suppose that ''F'' is a field of characteristic ''p'', and consider the function <math>f(x) = x^p</math> that raises each element of ''F'' to the power ''p''.  This is called the [[Frobenius automorphism]] of ''F''.  It is an automorphism of the field because of the [[Freshman's dream]] identity <math>(x+y)^p = x^p+y^p</math>.  The Frobenius automorphism is important in [[number theory]] because it generates the [[Galois group]] of ''F'' over its prime subfield.\n\n===In abstract algebra===\nExponentiation for integer exponents can be defined for quite general structures in  [[abstract algebra]].\n\nLet ''X'' be a [[Set (mathematics)|set]] with a [[power-associative]] [[binary operation]] which is written multiplicatively. Then ''x''<sup>''n''</sup> is defined for any element ''x'' of ''X'' and any nonzero [[natural number]] ''n'' as the product of ''n'' copies of ''x'', which is recursively defined by\n:<math>\\begin{align}\n  x^1 &= x \\\\\n  x^n &= x^{n-1}x \\quad\\hbox{for }n>1\n\\end{align}</math>\n\nOne has the following properties\n:<math>\\begin{align}\n  \\left(x^i x^j\\right) x^k &= x^i \\left(x^j x^k\\right) & &\\text{(power-associative property)} \\\\\n                   x^{m+n} &= x^m x^n \\\\\n        \\left(x^m\\right)^n &= x^{mn}\n\\end{align}</math>\n\nIf the operation has a two-sided [[identity element]] 1, then ''x''<sup>0</sup> is defined to be equal to 1 for any ''x''.\n:<math>\\begin{align}\n   x1 &= 1x = x & &\\text{(two-sided identity)} \\\\\n  x^0 &= 1\n\\end{align}</math>{{citation needed|date=April 2014}}\n\nIf the operation also has two-sided [[inverse element|inverses]] and is associative, then the [[Magma (algebra)|magma]] is a [[group (mathematics)|group]]. The inverse of ''x'' can be denoted by ''x''<sup>−1</sup> and follows all the usual rules for exponents.\n:<math>\\begin{align}\n  x x^{-1} &= x^{-1} x = 1          & &\\text{(two-sided inverse)} \\\\\n   (x y) z &= x (y z)               & &\\text{(associative)} \\\\\n    x^{-n} &= \\left(x^{-1}\\right)^n \\\\\n   x^{m-n} &= x^m x^{-n}\n\\end{align}</math>\n\nIf the multiplication operation is [[commutative]] (as for instance in [[abelian group]]s), then  the following holds:\n: <math>(xy)^n = x^n y^n </math>\n\nIf the binary operation is written additively, as it often is for [[abelian groups]], then \"exponentiation is repeated multiplication\" can be reinterpreted as \"[[multiplication]] is repeated [[addition]]\". Thus, each of the laws of exponentiation above has an [[analogy|analogue]] among laws of multiplication.\n\nWhen there are several power-associative binary operations defined on a set, any of which might be iterated, it is common to indicate which operation is being repeated by placing its symbol in the superscript. Thus, ''x''<sup>∗''n''</sup> is {{math|''x'' ∗ ... ∗ ''x''}}, while ''x''<sup>#''n''</sup> is {{math|''x'' # ... # ''x''}}, whatever the operations ∗ and # might be.\n\nSuperscript notation is also used, especially in [[group theory]], to indicate [[Conjugacy class|conjugation]]. That is, {{math|1=''g''<sup>''h''</sup> = ''h''<sup>−1</sup>''gh''}}, where ''g'' and ''h'' are elements of some [[group (math)|group]]. Although conjugation obeys some of the same laws as exponentiation, it is not an example of repeated multiplication in any sense. A [[quandle]] is an [[algebraic structure]] in which these laws of conjugation play a central role.\n\n==={{Anchor|Exponentiation over sets}}Over sets===\n{{Main article|Cartesian product}}\n\nIf ''n'' is a natural number and ''A'' is an arbitrary set, the expression ''A''<sup>''n''</sup> is often used to denote the set of ordered [[n-tuple|''n''-tuples]] of elements of ''A''. This is equivalent to letting ''A''<sup>''n''</sup> denote the set of functions from the set {{math|{0, 1, 2, ..., ''n''−1}{{null}}}} to the set ''A''; the ''n''-tuple {{math|(''a''<sub>0</sub>, ''a''<sub>1</sub>, ''a''<sub>2</sub>, ..., a<sub>''n''−1</sub>)}} represents the function that sends ''i'' to ''a''<sub>''i''</sub>.\n\nFor an infinite [[cardinal number]] κ and a set ''A'', the notation ''A''<sup>κ</sup> is also used to denote the set of all functions from a set of size κ to ''A''.   This is sometimes written <sup>κ</sup>''A'' to distinguish it from cardinal exponentiation, defined below.\n\nThis generalized exponential can also be defined for operations on sets or for sets with extra [[Mathematical structure|structure]]. For example, in [[linear algebra]], it makes sense to index [[Direct sum of modules|direct sums]] of [[vector space]]s over arbitrary index sets. That is, we can speak of\n: <math>\\bigoplus_{i \\in \\mathbb{N}} V_{i}</math>\n\nwhere each ''V''<sub>''i''</sub> is a vector space.\n\nThen if ''V''<sub>''i''</sub> = ''V'' for each ''i'', the resulting direct sum can be written in exponential notation as ''V''<sup>⊕'''N'''</sup>, or simply ''V''<sup>'''N'''</sup> with the understanding that the direct sum is the default. We can again replace the set '''N''' with a cardinal number ''n'' to get ''V''<sup>''n''</sup>, although without choosing a specific standard set with cardinality ''n'', this is defined only [[up to]] [[isomorphism]]. Taking ''V'' to be the [[field (algebra)|field]] '''R''' of [[real number]]s (thought of as a vector space over itself) and ''n'' to be some [[natural number]], we get the vector space that is most commonly studied in linear algebra, the real vector space '''R'''<sup>''n''</sup>.\n\nIf the base of the exponentiation operation is a set, the exponentiation operation is the [[Cartesian product]] unless otherwise stated. Since multiple Cartesian products produce an ''n''-[[tuple]], which can be represented by a function on a set of appropriate cardinality, ''S''<sup>''N''</sup> becomes simply the set of all [[Function (mathematics)|functions]] from ''N'' to ''S'' in this case:\n: <math>S^N \\equiv \\{ f\\colon N \\to S \\}</math>\n\nThis fits in with the [[Cardinal number#Cardinal exponentiation|exponentiation of cardinal numbers]], in the sense that {{math|1={{abs|''S''<sup>''N''</sup>}} = {{abs|''S''}}<sup>{{abs|''N''}}</sup>}}, where {{abs|''X''}} is the cardinality of ''X''. When \"2\" is defined as {{math|{0, 1},}} we have {{math|1={{abs|2<sup>''X''</sup>}} = 2<sup>{{abs|''X''}}</sup>}}, where 2<sup>''X''</sup>, usually denoted by '''P'''(''X''), is the [[power set]] of ''X''; each [[subset]] ''Y'' of ''X'' corresponds uniquely to a function on ''X'' taking the value 1 for {{math|''x'' ∈ ''Y''}} and 0 for {{math|''x'' ∉ ''Y''}}.<!-- (This is where the term \"power set\" comes from. This needs a source -->\n\n===In category theory===\n{{Main article|Cartesian closed category}}\nIn a [[Cartesian closed category]], the [[exponential (category theory)|exponential]] operation can be used to raise an arbitrary object to the power of another object.  This generalizes the [[Cartesian product]] in the category of sets. If 0 is an [[initial object]] in a Cartesian closed category, then the [[exponential object]] 0<sup>0</sup> is isomorphic to any terminal object 1.\n\n===Of cardinal and ordinal numbers===\n{{Main article|Cardinal number#Cardinal arithmetic|l1=Cardinal arithmetic|Ordinal arithmetic}}\n\nIn [[set theory]], there are exponential operations for [[cardinal number|cardinal]] and [[ordinal number]]s.\n\nIf ''κ'' and ''λ'' are cardinal numbers, the expression ''κ''<sup>''λ''</sup> represents the cardinality of the set of functions from any set of cardinality ''λ'' to any set of cardinality ''κ''.<ref name=\"Bourbaki\">N. Bourbaki, Elements of Mathematics, Theory of Sets, Springer-Verlag, 2004, III.§3.5.</ref>  If ''κ'' and ''λ'' are finite, then this agrees with the ordinary arithmetic exponential operation.  For example, the set of 3-tuples of elements from a 2-element set has cardinality {{math|1=8 = 2<sup>3</sup>}}. In cardinal arithmetic, ''κ''<sup>0</sup> is always 1 (even if ''κ'' is an infinite cardinal or zero).\n\nExponentiation of cardinal numbers is distinct from exponentiation of ordinal numbers, which is defined by a [[limit (mathematics)|limit]] process involving [[transfinite induction]].\n\n==Repeated exponentiation==\n{{main|Tetration|Hyperoperation}}\nJust as exponentiation of natural numbers is motivated by repeated multiplication, it is possible to define an operation based on repeated exponentiation; this operation is sometimes called [[hyper-4]] or [[tetration]].  Iterating tetration leads to another operation, and so on, a concept named [[hyperoperation]].  This sequence of operations is expressed by the [[Ackermann function]] and [[Knuth's up-arrow notation]]. Just as exponentiation grows faster than  multiplication, which is faster-growing than addition, tetration is faster-growing than exponentiation. Evaluated at {{math|(3, 3)}}, the functions addition, multiplication, exponentiation, and tetration yield 6, 9, 27, and {{val|7625597484987}} ({{math|1== 3<sup>27</sup> = 3<sup>3<sup>3</sup></sup> = <sup>3</sup>3}}) respectively.\n\n==Limits of powers==\n[[Zero to the power of zero]] gives a number of examples of limits that are of the [[indeterminate form]] 0<sup>0</sup>. The limits in these examples exist, but have different values, showing that the two-variable function {{math|''x''<sup>''y''</sup>}} has no limit at the point {{math|(0, 0)}}. One may consider at what points this function does have a limit.\n\nMore precisely, consider the function {{math|1=''f''(''x'', ''y'') = ''x''<sup>''y''</sup>}} defined on {{math|1=''D'' = {(''x'', ''y'') ∈ '''R'''<sup>2</sup> : ''x'' > 0}.}} Then {{math|''D''}} can be viewed as a subset of {{math|{{overline|'''R'''}}<sup>2</sup>}} (that is, the set of all pairs {{math|(''x'', ''y'')}} with {{math|''x''}}, {{math|''y''}} belonging to the [[extended real number line]] {{math|1={{overline|'''R'''}} = [−∞, +∞]}}, endowed with the [[product topology]]), which will contain the points at which the function {{math|''f''}} has a limit.\n\nIn fact, {{math|''f''}} has a limit at all [[accumulation point]]s of {{math|''D''}}, except for {{math|(0, 0)}}, {{math|(+∞, 0)}}, {{math|(1, +∞)}} and {{math|(1, −∞)}}.<ref>N. Bourbaki, ''Topologie générale'', V.4.2.</ref> Accordingly, this allows one to define the powers {{math|''x''<sup>''y''</sup>}} by continuity whenever {{math|0 ≤ ''x'' ≤ +∞}}, {{math|−∞ ≤ y ≤ +∞}}, except for 0<sup>0</sup>, (+∞)<sup>0</sup>, 1<sup>+∞</sup> and 1<sup>−∞</sup>, which remain indeterminate forms.\n\nUnder this definition by continuity, we obtain:\n* {{math|1=''x''<sup>+∞</sup> = +∞}} and {{math|1=''x''<sup>−∞</sup> = 0}}, when {{math|1 < ''x'' ≤ +∞}}.\n* {{math|1=''x''<sup>+∞</sup> = 0}} and {{math|1=''x''<sup>−∞</sup> = +∞}}, when {{math|0 ≤ ''x'' < 1}}.\n* {{math|1=0<sup>''y''</sup> = 0}} and {{math|1=(+∞)<sup>''y''</sup> = +∞}}, when {{math|0 < ''y'' ≤ +∞}}.\n* {{math|1=0<sup>''y''</sup> = +∞}} and {{math|1=(+∞)<sup>''y''</sup> = 0}}, when {{math|−∞ ≤ ''y'' < 0}}.\n\nThese powers are obtained by taking limits of {{math|''x''<sup>''y''</sup>}} for ''positive'' values of {{math|''x''}}. This method does not permit a definition of {{math|''x''<sup>''y''</sup>}} when {{math|''x'' < 0}}, since pairs {{math|(''x'', ''y'')}} with {{math|''x'' < 0}} are not accumulation points of {{math|''D''}}.\n\nOn the other hand, when {{math|''n''}} is an integer, the power {{math|''x''<sup>''n''</sup>}} is already meaningful for all values of {{math|''x''}}, including negative ones. This may make the definition {{math|1=0<sup>''n''</sup> = +∞}} obtained above for negative {{math|''n''}} problematic when {{math|''n''}} is odd, since in this case {{math|''x''<sup>''n''</sup> → +∞}} as {{math|''x''}} tends to {{math|0}} through positive values, but not negative ones.\n\n==Efficient computation with integer exponents==\nComputing ''b''<sup>''n''</sup> using iterated multiplication requires {{math|''n'' − 1}} multiplication operations, but it can be computed more efficiently than that, as illustrated by the following example.  To compute 2<sup>100</sup>, note that {{math|1=100 = 64 + 32 + 4}}.  Compute the following in order:\n# 2<sup>2</sup> = 4\n# (2<sup>2</sup>)<sup>2</sup> = 2<sup>4</sup> = 16\n# (2<sup>4</sup>)<sup>2</sup> = 2<sup>8</sup> = 256\n# (2<sup>8</sup>)<sup>2</sup> = 2<sup>16</sup> = 65,536\n# (2<sup>16</sup>)<sup>2</sup> = 2<sup>32</sup> = 4,294,967,296\n# (2<sup>32</sup>)<sup>2</sup> = 2<sup>64</sup> = 18,446,744,073,709,551,616\n# 2<sup>64</sup> 2<sup>32</sup> 2<sup>4</sup> = 2<sup>100</sup> = 1,267,650,600,228,229,401,496,703,205,376\nThis series of steps only requires 8 multiplication operations instead of 99 (since the last product above takes 2 multiplications).\n\nIn general, the number of multiplication operations required to compute ''b''<sup>''n''</sup> can be reduced to [[asymptotic notation|Θ]](log ''n'') by using [[exponentiation by squaring]] or (more generally) [[addition-chain exponentiation]].  Finding the ''minimal'' sequence of multiplications (the minimal-length addition chain for the exponent) for ''b''<sup>''n''</sup> is a difficult problem for which no efficient algorithms are currently known (see [[Subset sum problem]]), but many reasonably efficient heuristic algorithms are available.<ref>{{Cite journal | last1 = Gordon | first1 = D. M. | doi = 10.1006/jagm.1997.0913 | title = A Survey of Fast Exponentiation Methods | journal = Journal of Algorithms | volume = 27 | pages = 129–146 | year = 1998 | pmid =  | pmc = | url = http://www.ccrwest.org/gordon/jalg.pdf | citeseerx = 10.1.1.17.7076 }}</ref>\n\n==Exponential notation for function names==\nPlacing an integer superscript after the name or symbol of a function, as if the function were being raised to a power, commonly refers to repeated [[function composition]] rather than repeated multiplication. Thus, {{math|''f''{{i sup|3}}(''x'')}} may mean {{math|''f''(''f''(''f''(''x'')))}}; in particular, {{math|''f''{{i sup|−1}}(''x'')}} usually denotes the [[inverse function]] of {{math|''f''}}. [[Iterated function]]s are of interest in the study of [[fractal]]s and [[dynamical systems]]. [[Babbage]] was the first to study the problem of finding a [[functional square root]] {{math|''f''{{i sup|1/2}}(''x'')}}.\n\nFor historical reasons, this notation applied to the [[trigonometric functions|trigonometric]] and [[hyperbolic functions|hyperbolic]] functions has a specific and diverse interpretation: a positive exponent applied to the function's abbreviation means that the result is raised to that power, while an exponent of {{math|−1}} denotes the inverse function. That is, {{math|sin<sup>2</sup> ''x''}} is just a shorthand way to write {{math|(sin ''x'')<sup>2</sup>}} without using parentheses, whereas {{math|sin<sup>−1</sup> ''x''}} refers to the inverse function of the [[sine]], also called {{math|arcsin ''x''}}. Each trigonometric and hyperbolic has its own name and abbreviation both for the reciprocal; for example, {{math|1=1/(sin ''x'') = (sin ''x'')<sup>−1</sup> = csc ''x''}}, as well as for its inverse, for example {{math|1=cosh<sup>−1</sup> ''x'' = arcosh ''x''}}.  A similar convention applies to logarithms, where {{math|log<sup>2</sup> ''x''}} usually means {{math|(log ''x'')<sup>2</sup>}}, not {{math|log log ''x''}}.\n\n==In programming languages==\n[[Programming language]]s generally express exponentiation either as an infix operator or as a (prefix) function, as they are linear notations which do not support superscripts:\n* <code>x ↑ y</code>: [[Algol programming language|Algol]], [[Commodore BASIC]]\n* <code>x ^ y</code>: [[AWK]], [[BASIC]], [[J programming language|J]], [[MATLAB]], [[Wolfram Language]] ([[Wolfram Mathematica|Mathematica]]), [[R (programming language)|R]], [[Microsoft Excel]], [[Analytica (software)|Analytica]], [[TeX]] (and its derivatives), [[TI-BASIC]], [[bc programming language|bc]] (for integer exponents), [[Haskell (programming language)|Haskell]] (for nonnegative integer exponents), [[Lua (programming language)|Lua]] and most [[computer algebra system]]s. Conflicting uses of the symbol <code>^</code> include: [[XOR]] (in POSIX Shell arithmetic expansion, AWK, C, C++, C#, D, Go, Java, JavaScript, Perl, PHP, Python, Ruby and Tcl), [[indirection]] (Pascal), and string concatenation (OCaml and Standard ML).\n* <code>x ^^ y</code>: Haskell (for fractional base, integer exponents), [[D (programming language)|D]]\n* <code>x ** y</code>: [[Ada (programming language)|Ada]], [[Z shell]], [[Korn shell]], [[Bash (Unix shell)|Bash]], [[COBOL]], [[CoffeeScript]], [[Fortran]], [[FoxPro 2|FoxPro]], [[Gnuplot]], [[Apache Groovy|Groovy]], [[JavaScript]], [[OCaml]], [[F Sharp (programming language)|F#]], [[Perl]], [[PHP]], [[PL/I]], [[Python (programming language)|Python]], [[Rexx]], [[Ruby (programming language)|Ruby]], [[SAS programming language|SAS]], [[Seed7]], [[Tcl]], [[ABAP]], [[Mercury (programming language)|Mercury]], Haskell (for floating-point exponents), [[Turing (programming language)|Turing]], [[VHDL]]\n* <code>pown x y</code>: F# (for integer base, integer exponent)\n* <code>x⋆y</code>: [[APL (programming language)|APL]]\n\nMany other programming languages lack syntactic support for exponentiation, but provide library functions:\n* <code>pow(x, y)</code>: [[C (programming language)|C]], [[C++]]\n* <code>Math.Pow(x, y)</code>: [[C Sharp (programming language)|C#]]\n* <code>math:pow(X, Y)</code>: [[Erlang (programming language)|Erlang]]\n\nFor certain exponents there are special ways to compute ''x''<sup>''y''</sup> much faster than through generic exponentiation. These cases include small positive and negative integers (prefer ''x''*''x'' over ''x''<sup>2</sup>; prefer 1/''x'' over ''x''<sup>−1</sup>) and roots (prefer sqrt(''x'') over ''x''<sup>0.5</sup>, prefer cbrt(''x'') over ''x''<sup>1/3</sup>).\n\nNot all programming languages adhere to the same association convention for exponentiation: while the [[Wolfram language]], [[Google Search]] and others use right-association (i.e. <code>a^b^c</code> is evaluated as <code>a^(b^c)</code>), many computer programs such as [[Microsoft Office Excel]] and [[Matlab]] associate to the left (i.e. <code>a^b^c</code> is evaluated as <code>(a^b)^c</code>).\n\n==See also==\n{{Portal|Mathematics}}\n<!-- Please keep entries in alphabetical order & add a short description [[WP:SEEALSO]] -->\n{{div col|colwidth=20em}}\n* [[Double exponential function]]\n* [[Exponential decay]]\n* [[Exponential growth]]\n* [[List of exponential topics]]\n* [[Modular exponentiation]]\n* [[Scientific notation]]\n* [[Unicode subscripts and superscripts]]\n* [[Equation xʸ = yˣ|''x''<sup>''y''</sup> = ''y''<sup>''x''</sup>]]\n* [[Zero to the power of zero]]\n{{div col end}}\n<!-- please keep entries in alphabetical order -->\n\n==References==\n{{Reflist}}\n\n==External links==\n* {{planetmath reference|id=3948|title=Introducing 0th power}}\n* [http://www.mathsisfun.com/algebra/exponent-laws.html Laws of Exponents] with derivation and examples\n{{Good article}}\n\n{{Hyperoperations}}\n[[Category:Exponentials]]\n[[Category:Binary operations]]<!-- when taking x^y  with 2 variables as exponentiate(x,y) -->\n[[Category:Unary operations]]<!-- when taking either x or y in x^y  as fixed; e.g. in exp(y) or cube(x) -->"
    },
    {
      "title": "Exponentiation by squaring",
      "url": "https://en.wikipedia.org/wiki/Exponentiation_by_squaring",
      "text": "{{short description|Algorithm for fast exponentiation}}\n{{multiple issues|\n{{cleanup HTML|date=February 2019}}\n{{more citations needed|date=February 2018}}\n}}\nIn [[mathematics]] and [[computer programming]], '''[[Exponentiation|exponentiating]] by squaring''' is a general method for fast computation of large [[positive integer]] powers of a [[number]], or more generally of an element of a [[semigroup]], like a [[polynomial]] or a [[square matrix]]. Some variants are commonly referred to as '''square-and-multiply''' algorithms or '''binary exponentiation'''. These can be of quite general use, for example in [[modular arithmetic]] or powering of matrices. For semigroups for which [[Abelian group#Notation|additive notation]] is commonly used, like [[elliptic curve]]s used in [[cryptography]], this method is also referred to as '''double-and-add'''.\n\n==Basic method==\n{{confusing section|small=no|reason=the example describes an different algorithm than the remaining of the section, as the bits of the exponent are considered in an opposite order|date=April 2019}}\nThe method is based on the observation that, for a positive integer ''n'', we have\n:<math> x^n=     \n    \\begin{cases}\n                x \\, ( x^{2})^{\\frac{n - 1}{2}}, & \\mbox{if } n \\mbox{ is odd} \\\\\n                (x^{2})^{\\frac{n}{2}} , & \\mbox{if } n \\mbox{ is even}.\n     \\end{cases}\n</math>\n\nThis method uses the bits of the exponent to determine which powers are computed.\n\nThis example shows how to compute <math>x^{13}</math> using this method.\nThe exponent, 13, is 1101 in binary. The bits are used in left to right order.\nThe exponent has 4 bits, so there are 4 iterations.\n\nFirst, initialize the result to 1: <math>r \\leftarrow 1 \\, ( = x^0)</math>.\n: Step 1) <math>r \\leftarrow r^2 \\, ( = x^0)</math>; bit 1 = 1, so compute <math>r \\leftarrow r \\cdot x \\,( = x^1)</math>;\n: Step 2) <math>r \\leftarrow r^2 \\, ( = x^2)</math>; bit 2 = 1, so compute <math>r \\leftarrow r \\cdot x \\, ( = x^3)</math>;\n: Step 3) <math>r \\leftarrow r^2 \\, ( = x^6)</math>; bit 3 = 0, so we are done with this step (equivalently, this corresponds to <math>r \\leftarrow r \\cdot x^0 \\, ( = x^6)</math>);\n: Step 4) <math>r \\leftarrow r^2 \\, ( = x^{12})</math>; bit 4 = 1, so compute <math>r \\leftarrow r \\cdot x \\, ( = x^{13})</math>.\nIf we write <math>n</math> in binary as <math>b_k \\cdots b_0</math>, then this is equivalent to defining a sequence <math>r_{k+1}, \\ldots, r_0</math> by letting <math>r_{k+1} = 1</math> and then defining <math>r_i = r_{i+1}^2 x^{b_i}</math> for <math>i = k, \\ldots, 0</math>, where <math>r_0</math> will equal <math>x^n</math>.\n\nThis may be implemented as the following [[recursion (computer science)|recursive algorithm]]: \n<source lang=\"pascal\">\n  Function exp_by_squaring(x, n)\n    if n < 0  then return exp_by_squaring(1 / x, -n);\n    else if n = 0  then return  1;\n    else if n = 1  then return  x ;\n    else if n is even  then return exp_by_squaring(x * x,  n / 2);\n    else if n is odd  then return x * exp_by_squaring(x * x, (n - 1) / 2);\n</source>\n\nAlthough not [[Tail call|tail-recursive]], this algorithm may be rewritten into a tail recursive algorithm by introducing an auxiliary function:\n\n<source lang=\"pascal\">\n  Function exp_by_squaring(x, n)\n    exp_by_squaring2(1, x, n)\n  Function exp_by_squaring2(y, x, n)\n    if n < 0  then return exp_by_squaring2(y, 1 / x, - n);\n    else if n = 0  then return  y;\n    else if n = 1  then return  x * y;\n    else if n is even  then return exp_by_squaring2(y, x * x,  n / 2);\n    else if n is odd  then return exp_by_squaring2(x * y, x * x, (n - 1) / 2).\n</source>\n\nThe iterative version of the algorithm also uses a bounded auxiliary space, and is given by\n\n<source lang=\"pascal\">\n  Function exp_by_squaring_iterative(x, n)\n    if n < 0 then\n      x := 1 / x;\n      n := -n;\n    if n = 0 then return 1\n    y := 1;\n    while n > 1 do\n      if n is even then \n        x := x * x;\n        n := n / 2;\n      else\n        y := x * y;\n        x := x * x;\n        n := (n – 1) / 2;\n    return x * y\n</source>\n\n==Computational complexity==\n\nA brief analysis shows that such an algorithm uses <math>\\lfloor \\log_2n\\rfloor</math> squarings and at most <math>\\lfloor \\log_2n\\rfloor</math> multiplications, where <math>\\lfloor\\;\\rfloor</math> denotes the [[floor function]]. More precisely, the number of multiplications is one less than the number of ones present in the [[binary expansion]] of ''n''. For ''n'' greater than about 4 this is computationally more efficient than naively multiplying the base with itself repeatedly.\n\nEach squaring results in approximately double the number of digits of the previous, and so, if multiplication of two ''d'' digit numbers is implemented in O(''d''<sup>''k''</sup>) operations for some fixed ''k'' then the complexity of computing ''x''<sup>''n''</sup> is given by:\n\n:<math> \\sum\\limits_{i=0}^{O(\\log(n))} (2^i O(\\log(x)))^k = O((n \\log(x))^k)  \n</math>\n\n==2<sup>k</sup>-ary method==\nThis algorithm calculates the value of x<sup>n</sup> after expanding the exponent in base  2<sup>k</sup>. It was first proposed by [[Brauer]] in 1939. In the algorithm below we make use of the following function f(0) = (k,0) and f(m) = (s,u) where m = u·2<sup>s</sup>\nwith ''u'' odd.\n\nAlgorithm:\n\n;Input: An element x of G, a parameter k > 0, a non-negative integer {{math|1=''n'' = (''n''<sub>''l''−1</sub>, ''n''<sub>''l''−2</sub>, ..., ''n''<sub>0</sub>)<sub>2<sup>''k''</sup></sub>}} and the precomputed values <math style=\"vertical-align:baseline;\">x^3, x^5, ... , x^{2^k-1}</math>.\n\n;Output: The element x<sup>n</sup> in ''G''\n\n y := 1; i := l-1\n '''while''' i>=0 do\n     (s,u) := f(n<sub>i</sub>)\n     '''for''' j:=1 '''to''' k-s '''do'''\n         y := y<sup>2</sup> \n     y := y*x<sup>u</sup>\n     '''for''' j:=1 '''to''' s '''do'''\n         y := y<sup>2</sup>\n     i := i-1\n '''return''' y\n\nFor optimal efficiency, ''k'' should be the smallest integer satisfying <ref name=\"frey\">{{cite book |editor1-last=Cohen |editor1-first=H. |editor2-last=Frey, G. |date=2006 |title=Handbook of Elliptic and Hyperelliptic Curve Cryptography |series=Discrete Mathematics and Its Applications |publisher=Chapman & Hall/CRC |isbn=9781584885184}}</ref>\n\n:<math>\\log(n) < \\frac{k(k+1) \\cdot 2^{2k}}{2^{k+1} - k - 2} + 1.</math>\n\n==Sliding window method==\nThis method is an efficient variant of the 2<sup>k</sup>-ary method. For example, to calculate the exponent 398 which has binary expansion (110 001 110)<sub>2</sub>, we take a window of length 3 using the 2<sup>k</sup>-ary method algorithm we calculate 1,x<sup>3</sup>,x<sup>6</sup>,x<sup>12</sup>,x<sup>24</sup>,x<sup>48</sup>,x<sup>49</sup>,x<sup>98</sup>,x<sup>99</sup>,x<sup>198</sup>,x<sup>199</sup>,x<sup>398</sup>.  \nBut, we can also compute 1,x<sup>3</sup>,x<sup>6</sup>,x<sup>12</sup>,x<sup>24</sup>,x<sup>48</sup>,x<sup>96</sup>,x<sup>192</sup>,x<sup>198</sup>,x<sup>199</sup>,\nx<sup>398</sup> which saves one multiplication and amounts to evaluating (110 001 110)n<sub>2</sub>\n\nHere is the general algorithm:\n\nAlgorithm:\n\n;Input:An element ''x'' of ''G'',a non negative integer {{math|1=''n''=(''n''<sub>l-1</sub>,''n''<sub>l-2</sub>,...,''n''<sub>0</sub>)<sub>2</sub>}}, a parameter k>0 and the pre-computed values <math style=\"vertical-align:baseline;\">x^3, x^5, ... ,x^{2^k-1}</math>.\n\n;Output: The element ''x<sup>n</sup>'' &isin; ''G''\n\nAlgorithm:\n\n y := 1; i := l-1\n '''while''' i > -1 '''do'''\n     '''if''' n<sub>i</sub>=0 '''then'''\n         y:=y<sup>2</sup>' i:=i-1\n     '''else'''\n         s:=max{i-k+1,0}\n         '''while''' n<sub>s</sub>=0 '''do'''\n             s:=s+1 <ref group=notes>In this line, the loop finds the longest string of length less than or equal to 'k' which ends in a non zero value. And not all odd powers of 2 up to <math style=\"vertical-align:baseline;\">x^{2^k-1}</math> need be computed and only those specifically involved in the computation need be considered.</ref>\n         '''for''' h:=1 '''to''' i-s+1 '''do'''\n             y:=y<sup>2</sup>\n         u:=(n<sub>i</sub>,n<sub>i-1</sub>,....,n<sub>s</sub>)<sub>2</sub>\n         y:=y*x<sup>u</sup>\n         i:=s-1\n '''return''' y\n\n==Montgomery's ladder technique==\nMany algorithms for exponentiation do not provide defence against [[side-channel attack]]s. Namely, an attacker observing the sequence of squarings and multiplications can (partially) recover the exponent involved in the computation. This is a problem if the exponent should remain secret, as with many [[Public-key cryptography|public-key cryptosystems]]. A technique called [[Peter Montgomery (mathematician)|Montgomery's]] Ladder<ref name=\"ladder\">{{cite journal |last=Montgomery |first=Peter L. |date=1987 |title=Speeding the Pollard and Elliptic Curve Methods of Factorization |journal=Math. Comput. |volume=48 |number=177 |pages=243–264 |url=http://www.ams.org/journals/mcom/1987-48-177/S0025-5718-1987-0866113-7/S0025-5718-1987-0866113-7.pdf |format=PDF}}</ref> addresses this concern.\n\nGiven the [[binary expansion]] of a positive, non-zero integer n=(n<sub>k-1</sub>...n<sub>0</sub>)<sub>2</sub> with n<sub>k-1</sub>=1 we can compute x<sup>n</sup> as follows:\n x<sub>1</sub>=x; x<sub>2</sub>=x<sup>2</sup>\n for i=k-2 to 0 do\n   If n<sub>i</sub>=0 then\n     x<sub>2</sub>=x<sub>1</sub>*x<sub>2</sub>; x<sub>1</sub>=x<sub>1</sub><sup>2</sup>\n   else\n     x<sub>1</sub>=x<sub>1</sub>*x<sub>2</sub>; x<sub>2</sub>=x<sub>2</sub><sup>2</sup>\n return x<sub>1</sub>\n\nThe algorithm performs a fixed sequence of operations ([[up to]] log n): a multiplication and squaring takes place for each bit in the exponent, regardless of the bit's specific value.\n\nThis specific implementation of Montgomery's ladder is not yet protected against cache [[timing attack]]s: memory access latencies might still be observable to an attacker as you access different variables depending on the value of bits of the secret exponent.\n\n==Fixed base exponent==\nThere are several methods which can be employed to calculate x<sup>n</sup> when the base is fixed and the exponent varies. As one can see, [[precomputation]]s play a key role in these algorithms.\n\n===Yao's method===\nYao's method is orthogonal to the {{math|2<sup>''k''</sup>}}-ary method where the exponent is expanded in radix {{math|1=''b''=2<sup>''k''</sup>}} and the computation is as performed in the algorithm above. Let \"{{mvar|n}}\", \"{{mvar|n<sub>i</sub>}}\", \"{{mvar|b}}\", and \"{{mvar|b<sub>i</sub>}}\" be integers.\n\nLet the exponent \"{{mvar|n}}\" be written as\n:<math> n = \\sum_{i=0}^{w-1} n_ib_i</math> where <math> 0 \\leqslant n_i < h</math> for all <math>i \\in [0,w-1] </math>\n\nLet {{math|1=''x<sub>i</sub>'' = ''x<sup>b<sub>i</sub></sup>''}}. Then the algorithm uses the equality\n:<math> x^n = \\prod_{i=0}^{w-1} {x_i}^{n_i} = \\prod_{j=1}^{h-1}{\\bigg[\\prod_{n_i=j} x_i\\bigg]}^j </math>\n\nGiven the element '{{mvar|x}}' of {{mvar|G}}, and the exponent '{{mvar|n}}' written in the above form, along with the precomputed values {{math|1=''x''<sup>''b''<sub>0</sub></sup>....''x''<sup>''b''<sub>''w''-1</sub></sup>}} the element {{mvar|x<sup>n</sup>}} is calculated using the algorithm below.\n\n  y=1,u=1 and j=h-1\n  while j > 0 do\n    for i=0 to w-1 do\n      if n<sub>i</sub>=j then u=u*x<sup>b<sub>i</sub></sup>\n    y=y*u\n    j=j-1\n  return y\n\nIf we set {{math|1=''h''=2<sup>''k''</sup>}} and {{math|1=''b<sub>i</sub>'' = ''h<sup>i</sup>''}} then the {{mvar|n<sub>i</sub>}}'s are simply the digits of {{mvar|n}} in base {{mvar|h}}. Yao's method collects in u first those {{mvar|x<sub>i</sub>}} which appear to the highest power {{tmath|h-1}}; in the next round those with power {{tmath|h-2}} are collected in {{mvar|u}} as well etc. The variable y is multiplied {{tmath|h-1}} times with the initial {{mvar|u}}, {{tmath|h-2}} times with the next highest powers etc.\nThe algorithm uses {{tmath|w+h-2}} multiplications and {{tmath|w+1}} elements must be stored to compute {{mvar|x<sup>n</sup>}}.<ref name=frey />\n\n===Euclidean method===\nThe Euclidean method was first introduced in ''Efficient exponentiation using precomputation and vector addition chains'' by P.D Rooij.\n\nThis method for computing <math>x^n</math> in group {{math|'''G'''}}, where {{mvar|n}} is a natural integer, whose algorithm is given below, is using the following equality recursively:\n: <math>{x_0}^{n_0} \\cdot {x_1}^{n_1} = {\\left( x_0 \\cdot {x_1}^{q} \\right)}^{n_0} \\cdot {x_1}^{n_1 \\mod {n_0}}</math>, where <math>q = \\left\\lfloor \\frac {n_1} {n_0} \\right\\rfloor</math>\n: (in other words a Euclidean division of the exponent {{math|''n''<sub>1</sub>}} by {{math|''n''<sub>0</sub>}} is used to return a quotient {{mvar|q}} and a rest {{math|''n''<sub>1</sub> mod ''n''<sub>0</sub>}}).\n\nGiven the base element {{mvar|x}} in group {{math|'''G'''}}, and the exponent <math>n</math> written as in Yao's method, the element <math>x^n</math> is calculated using <math>l</math> precomputed values <math>x^{b_0}, ..., x^{b_{l_i}}</math> and then the algorithm below.\n\n     '''Begin loop'''   \n         {{nowrap|Find <math>M \\in \\left[ 0, l - 1 \\right]</math>,}} {{nowrap|such that <math>\\forall i \\in \\left[ 0, l - 1 \\right], {n_M} \\ge {n_i}</math>;}}\n         {{nowrap|Find <math>N \\in \\left( \\left[ 0, l - 1 \\right] - M \\right)</math>,}} {{nowrap|such that <math>\\forall i \\in \\left( \\left[ 0, l - 1 \\right] - M \\right), {n_N} \\ge {n_i}</math>;}}\n         '''Break loop''' {{nowrap|if <math>{n_N} = 0</math>;}}\n         {{nowrap|'''Let''' <math>q = \\left\\lfloor {n_M} / {n_N} \\right\\rfloor</math>,}} {{nowrap|and then '''let''' <math>{n_N} = \\left( {n_M} \\bmod {n_N} \\right)</math>;}}\n         {{nowrap|Compute recursively <math>{x_M}^q</math>,}} {{nowrap|and then '''let''' <math>{x_N} = {x_N} \\cdot {x_M}^q</math>;}}\n     '''End loop''';\n     {{nowrap|'''Return''' <math>x^n = {x_M}^{n_M}</math>.}}\n\nThe algorithm first finds the largest value amongst the {{math|''n''<sub>''i''</sub>}} and then the supremum within the set of {{math|{{(}} ''n''<sub>''i''</sub> \\ ''i'' ≠ ''M'' {{)}}}}.\nThen it raises {{math|''x''<sub>''M''</sub>}} to the power {{mvar|q}}, multiplies this value with {{math|''x''<sub>''N''</sub>}}, and then assigns {{math|''x''<sub>''N''</sub>}} the result of this computation and {{math|''n''<sub>''M''</sub>}} the value {{math|''n''<sub>''M''</sub>}} modulo {{math|''n''<sub>''N''</sub>}}.\n\n==Further applications==\nThe same idea allows fast computation of large [[Modular exponentiation|exponents modulo]] a number. Especially in [[cryptography]], it is useful to compute powers in a [[Ring (mathematics)|ring]] of [[modular arithmetic|integers modulo ''q'']]. It can also be used to compute integer powers in a [[group (mathematics)|group]], using the rule\n\n:Power(''x'', −''n'') = (Power(''x'', ''n''))<sup>−1</sup>.\n\nThe method works in every [[semigroup]] and is often used to compute powers of [[matrix (math)|matrices]].\n\nFor example, the evaluation of\n\n:13789<sup>722341</sup> (mod 2345) = 2029\n\nwould take a very long time and lots of storage space if the naïve method were used: compute 13789<sup>722341</sup> then take the [[remainder]] when divided by 2345. Even using a more effective method will take a long time: square 13789, take the remainder when divided by 2345, multiply the [[result]] by 13789, and so on. This will take less than <math>2\\log_2(722340)\\leq 40</math> modular multiplications.\n\nApplying above ''exp-by-squaring'' algorithm, with \"*\" interpreted as ''x''*''y'' = ''xy'' mod 2345 (that is a multiplication followed by a division with remainder) leads to only 27 multiplications and divisions of integers which may all be stored in a single machine word.\n\n==Example implementations==\n\n===Computation by powers of 2===\nThis is a non-recursive implementation of the above algorithm in [[Ruby (programming language)|Ruby]].\n\nn=n-1 is redundant when n=n/2 implicitly rounds towards zero, as lower level languages would do.\nn[0] is the rightmost bit of the binary representation of n, so if it is 1, the number is odd, if it is zero, the number is even. It is also n modulo 2.\n\n<source lang=\"ruby\">\ndef power(x,n)\n  result = 1\n  while n.nonzero?\n    if n[0].nonzero?\n      result *= x\n      n -= 1\n    end\n    x *= x\n    n /= 2\n  end\n  return result\nend\n</source>\n\n====Runtime example: compute 3<sup>10</sup>====\n parameter x =  3\n parameter n = 10\n result := 1\n \n '''Iteration 1'''\n   n = 10 -&gt; n is even\n   x := x<sup>2</sup> = 3<sup>2</sup> = 9\n   n := n / 2 = 5\n \n '''Iteration 2'''\n   n = 5 -&gt; n is odd\n       -&gt; result := result * x = 1 * x = 1 * 3<sup>2</sup> = 9\n          n := n - 1 = 4\n   x := x<sup>2</sup> = 9<sup>2</sup> = 3<sup>4</sup> = 81\n   n := n / 2 = 2\n \n '''Iteration 3'''\n   n = 2 -&gt; n is even\n   x := x<sup>2</sup> = 81<sup>2</sup> = 3<sup>8</sup> = 6561\n   n := n / 2 = 1\n \n '''Iteration 4'''\n   n = 1 -&gt; n is odd\n       -&gt; result := result * x = 3<sup>2</sup> * 3<sup>8</sup> = 3<sup>10</sup> = 9 * 6561 = 59049\n          n := n - 1 = 0\n \n return result\n\n====Runtime example: compute 3<sup>10</sup>====\n result := 3\n bin := \"1010\"\n \n '''Iteration for digit 4:'''\n   result := result<sup>2</sup> = 3<sup>2</sup> = 9\n   101'''0'''<sub>bin</sub> - Digit equals \"0\"\n   \n '''Iteration for digit 3:'''\n   result := result<sup>2</sup> = (3<sup>2</sup>)<sup>2</sup> = 3<sup>4</sup>  = 81\n   10'''1'''0<sub>bin</sub> - Digit equals \"1\" --&gt;&nbsp;result := result*3 = (3<sup>2</sup>)<sup>2</sup>*3 = 3<sup>5</sup>  = 243\n \n '''Iteration for digit 2:'''\n   result := result<sup>2</sup> = ((3<sup>2</sup>)<sup>2</sup>*3)<sup>2</sup> = 3<sup>10</sup>  = 59049\n   1'''0'''10<sub>bin</sub> - Digit equals \"0\"   \n \n return result\n\n(This example is based on the algorithm above. If calculated by hand, should go from left to right. If the start number is 1, just ignore it. Then if the next is one, square and multiply. If the next is zero, only square. )\n\n===Calculation of products of powers===\nExponentiation by squaring may also be used to calculate the product of 2 or more powers.\nIf the underlying group or semigroup is [[commutative]] then it is often possible to reduce the\nnumber of multiplications by computing the product simultaneously.\n\n====Example====\nThe formula a<sup>7</sup>×b<sup>5</sup> may be calculated within 3 steps:\n:((a)<span style=\"color:red;\"><sup>2</sup>×</span>a)<span style=\"color:red;\"><sup>2</sup>×</span>a   (four multiplications for calculating a<sup>7</sup>)\n:((b)<span style=\"color:red;\"><sup>2</sup></span>)<span style=\"color:red;\"><sup>2</sup>×</span>b     (three multiplications for calculating b<sup>5</sup>)\n: (a<sup>7</sup>)<span style=\"color:red;\">×</span>(b<sup>5</sup>) (one multiplication to calculate the product of the two)\nso one gets eight multiplications in total.\n\nA faster solution is to calculate both powers simultaneously:\n:((a<span style=\"color:red;\">×</span>b)<span style=\"color:red;\"><sup>2</sup>×</span>a)<span style=\"color:red;\"><sup>2</sup>×</span>a<span style=\"color:red;\">×</span>b\nwhich needs only 6 multiplications in total. Note that a×b is calculated twice; the result could be stored after the first calculation, which reduces the count of multiplication to 5.\n\nExample with numbers:\n:2<sup>7</sup>×3<sup>5</sup>&nbsp;=&nbsp;((2×3)<sup>2</sup>×2)<sup>2</sup>×2×3&nbsp;=&nbsp;(6<sup>2</sup>×2)<sup>2</sup>×6&nbsp;=&nbsp;72<sup>2</sup>×6&nbsp;=&nbsp;31,104\n\nCalculating the powers simultaneously instead of calculating them separately always reduces the\ncount of multiplications if at least two of the exponents are greater than 1.\n\n====Using transformation====\nThe example above a<sup>7</sup>×b<sup>5</sup> may also be calculated with only 5\nmultiplications if the expression is transformed before calculation:\n\na<sup>7</sup>×b<sup>5</sup> = a<sup>2</sup>×(ab)<sup>5</sup> with ab := a×b\n<dl>\n<dd>ab := a<span style=\"color:red;\">×</span>b (one multiplication)</dd>\n<dd>a<sup>2</sup>×(ab)<sup>5</sup> = ((ab)<span style=\"color:red;\"><sup>2</sup>×</span>a)<span style=\"color:red;\"><sup>2</sup>×</span>ab (four multiplications)</dd>\n</dl>\n\nGeneralization of transformation shows the following scheme:<br>\nFor calculating a<sup>A</sup>×b<sup>B</sup>×...×m<sup>M</sup>×n<sup>N</sup><br>\n1st: define ab := a×b, abc = ab×c, ...<br>\n2nd: calculate the transformed expression a<sup>A−B</sup>×ab<sup>B−C</sup>×...×abc..m<sup>M−N</sup>×abc..mn<sup>N</sup>\n\nTransformation before calculation often reduces the count of multiplications\nbut in some cases it also increases the count (see the last one of the examples below),\nso it may be a good idea to check the count of multiplications before using the transformed expression for calculation.\n\n====Examples====\nFor the following expressions the count of multiplications is shown for calculating each power separately,\ncalculating them simultaneously without transformation, and calculating them simultaneously after transformation.\n{|class=\"wikitable\"\n!Example \n! a<sup>7</sup>×b<sup>5</sup>×c<sup>3</sup>\n! a<sup>5</sup>×b<sup>5</sup>×c<sup>3</sup>\n! a<sup>7</sup>×b<sup>4</sup>×c<sup>1</sup>\n|-\n!separate\n| [((a)<span style=\"color:red;\"><sup>2</sup>×</span>a)<span style=\"color:red;\"><sup>2</sup>×</span>a] <span style=\"color:red;\">×</span> [((b)<span style=\"color:red;\"><sup>2</sup></span>)<span style=\"color:red;\"><sup>2</sup>×</span>b] <span style=\"color:red;\">×</span> [(c)<span style=\"color:red;\"><sup>2</sup>×</span>c]<br/>( '''11''' multiplications )\n| [((a)<span style=\"color:red;\"><sup>2</sup></span>)<span style=\"color:red;\"><sup>2</sup>×</span>a] <span style=\"color:red;\">×</span> [((b)<span style=\"color:red;\"><sup>2</sup></span>)<span style=\"color:red;\"><sup>2</sup>×</span>b] <span style=\"color:red;\">×</span> [(c)<span style=\"color:red;\"><sup>2</sup>×</span>c]<br/>( '''10''' multiplications )\n| [((a)<span style=\"color:red;\"><sup>2</sup>×</span>a)<span style=\"color:red;\"><sup>2</sup>×</span>a] <span style=\"color:red;\">×</span> [((b)<span style=\"color:red;\"><sup>2</sup></span>)<span style=\"color:red;\"><sup>2</sup></span>] <span style=\"color:red;\">×</span> [c]<br/>( '''8''' multiplications )\n|-\n!simultaneous\n| ((a<span style=\"color:red;\">×</span>b)<span style=\"color:red;\"><sup>2</sup>×</span>a<span style=\"color:red;\">×</span>c)<span style=\"color:red;\"><sup>2</sup>×</span>a<span style=\"color:red;\">×</span>b<span style=\"color:red;\">×</span>c<br/>( '''8''' multiplications )\n| ((a<span style=\"color:red;\">×</span>b)<span style=\"color:red;\"><sup>2</sup>×</span>c)<span style=\"color:red;\"><sup>2</sup>×</span>a<span style=\"color:red;\">×</span>b<span style=\"color:red;\">×</span>c<br/>( '''7''' multiplications )\n| ((a<span style=\"color:red;\">×</span>b)<span style=\"color:red;\"><sup>2</sup>×</span>a)<span style=\"color:red;\"><sup>2</sup>×</span>a<span style=\"color:red;\">×</span>c<br/>( '''6''' multiplications )\n|-\n!transformation\n| a := 2 &nbsp; ab := a<span style=\"color:red;\">×</span>b &nbsp; abc := ab<span style=\"color:red;\">×</span>c<br/>( 2 multiplications )\n| a := 2 &nbsp; ab := a<span style=\"color:red;\">×</span>b &nbsp; abc := ab<span style=\"color:red;\">×</span>c<br/>( 2 multiplications )\n| a := 2 &nbsp; ab := a<span style=\"color:red;\">×</span>b &nbsp; abc := ab<span style=\"color:red;\">×</span>c<br/>( 2 multiplications )\n|-\n!calculation after that \n| (a<span style=\"color:red;\">×</span>ab<span style=\"color:red;\">×</span>abc)<span style=\"color:red;\"><sup>2</sup>×</span>abc<br/>( 4 multiplications ⇒ '''6''' in total )\n| (ab<span style=\"color:red;\">×</span>abc)<span style=\"color:red;\"><sup>2</sup>×</span>abc<br/>( 3 multiplications ⇒ '''5''' in total )\n| (a<span style=\"color:red;\">×</span>ab)<span style=\"color:red;\"><sup>2</sup>×</span>a<span style=\"color:red;\">×</span>ab<span style=\"color:red;\">×</span>abc<br/>( 5 multiplications ⇒ '''7''' in total )\n|}\n\n==Signed-digit recoding==\nIn certain computations it may be more efficient to allow negative coefficients and hence use the inverse of the base, provided inversion in {{mvar|'''G'''}} is 'fast' or has been precomputed. For example, when computing {{math|''x''<sup>2<sup>''k''</sup>−1</sup>}} the binary method requires {{math|''k''−1}} multiplications and {{math|''k''−1}} squarings. However one could perform {{mvar|k}} squarings to get {{math|''x''<sup>2<sup>''k''</sup></sup>}} and then multiply by {{math|''x''<sup>−1</sup>}} to obtain {{math|''x''<sup>2<sup>''k''</sup>−1</sup>}}.\n\nTo this end we define the [[signed-digit representation]] of an integer {{mvar|n}} in radix {{mvar|b}} as \n:<math>n=\\sum_{i=0}^{l-1}n_ib^i \\text{  with  } |n_i|<b</math>\n\n''Signed binary representation'' corresponds to the particular choice {{math|1=''b''=2}} and <math>n_i \\in \\{-1,0,1\\}</math>. It is denoted by <math>(n_{l-1}\\dots n_0)_s</math>. There are several methods for computing this representation. The representation is not unique, for example take {{math|1=''n''=478}}. Two distinct signed-binary representations are given by <math>(10\\bar 1 1100\\bar 1 10)_s</math> and <math>(100\\bar 1 1000\\bar 1 0)_s</math>, where <math>\\bar 1</math> is used to denote {{math|-1}}. Since the binary method computes a multiplication for every non-zero entry in the base 2 representation of {{mvar|n}}, we are interested in finding the signed-binary representation with the smallest number of non-zero entries, that is, the one with ''minimal'' [[Hamming weight]]. One method of doing this is to compute the representation in [[non-adjacent form]], or NAF for short, which is one that satisfies <math>n_in_{i+1}=0\\text{ for all }i\\geqslant 0</math> and denoted by <math>(n_{l-1}\\dots n_0)_{\\text{NAF}}</math>. For example, the NAF representation of 478 is equal to <math>(1000\\bar 1 000\\bar 1 0)_{\\text{NAF}}</math>. This representation always has minimal Hamming weight. A simple algorithm to compute the NAF representation of a given integer <math>n=(n_ln_{l-1}\\dots n_0)_2</math> with <math>n_l=n_{l-1}=0</math> is the following:\n\n {{nowrap|<math>c_0=0</math>}}\n for {{math|1=''i'' = 0}} to {{math|''l'' - 1}} do\n   {{nowrap|<math>c_{i+1}=\\left\\lfloor\\frac{1}{2}(c_i+n_i+n_{i+1})\\right\\rfloor</math>}}\n   {{nowrap|<math>n_i'=c_i+n_i-2c_{i+1}</math>}}\n {{nowrap|return <math>(n_{l-1}'\\dots n_0')_{\\text{NAF}}</math>}}\n\nAnother algorithm by Koyama and Tsuruoka does not require the condition that <math>n_i=n_{i+1}=0</math>; it still minimizes the Hamming weight.\n\n==Alternatives and generalizations==\n{{main|Addition-chain exponentiation}}\nExponentiation by squaring can be viewed as a suboptimal [[addition-chain exponentiation]] algorithm: it computes the exponent via an [[addition chain]] consisting of repeated exponent doublings (squarings) and/or incrementing exponents by ''one'' (multiplying by ''x'') only.  More generally, if one allows ''any'' previously computed exponents to be summed (by multiplying those powers of ''x''), one can sometimes perform the exponentiation using fewer multiplications (but typically using more memory).  The smallest power where this occurs is for ''n''=15:\n\n:<math>x^{15} = x \\times (x \\times [x \\times x^2]^2)^2  \\!</math>  (squaring, 6 multiplies)\n:<math>x^{15} = x^3 \\times ([x^3]^2)^2  \\!</math> (optimal addition chain, 5 multiplies if ''x''<sup>3</sup> is re-used)\n\nIn general, finding the ''optimal'' addition chain for a given exponent is a hard problem, for which no efficient algorithms are known, so optimal chains are typically only used for small exponents (e.g. in [[compiler]]s where the chains for small powers have been pre-tabulated).  However, there are a number of [[heuristic]] algorithms that, while not being optimal, have fewer multiplications than exponentiation by squaring at the cost of additional bookkeeping work and memory usage.  Regardless, the number of multiplications never grows more slowly than [[Big-O notation|&Theta;]](log ''n''), so these algorithms only improve asymptotically upon exponentiation by squaring by a constant factor at best.\n\n==See also==\n*[[Modular exponentiation]]\n*[[Vectorial addition chain]]\n*[[Montgomery reduction]]\n*[[Non-adjacent form]]\n*[[Addition chain]]\n\n==Notes==\n{{Reflist|group=notes}}\n\n==References==\n{{Reflist}}\n\n[[Category:Exponentials]]\n[[Category:Computer arithmetic algorithms]]\n[[Category:Computer arithmetic]]"
    },
    {
      "title": "Four exponentials conjecture",
      "url": "https://en.wikipedia.org/wiki/Four_exponentials_conjecture",
      "text": "In [[mathematics]], specifically the field of [[transcendental number theory]], the '''four exponentials conjecture''' is a [[conjecture]] which, given the right conditions on the exponents, would guarantee the transcendence of at least one of four exponentials.  The conjecture, along with two related, stronger conjectures, is at the top of a hierarchy of conjectures and theorems concerning the arithmetic nature of a certain number of values of the [[exponential function]].\n\n==Statement==\n\nIf ''x''<sub>1</sub>, ''x''<sub>2</sub> and ''y''<sub>1</sub>, ''y''<sub>2</sub> are two pairs of [[complex numbers]], with each pair being [[linearly independent]] over the [[rational numbers]], then at least one of the following four numbers is [[Transcendental number|transcendental]]:\n:<math>e^{x_1y_1}, e^{x_1y_2}, e^{x_2y_1}, e^{x_2y_2}.</math>\n\nAn alternative way of stating the conjecture in terms of logarithms is the following.  For 1&nbsp;&le;&nbsp;''i'',''j''&nbsp;&le;&nbsp;2 let λ<sub>''ij''</sub> be complex numbers such that exp(λ<sub>''ij''</sub>) are all algebraic.  Suppose λ<sub>11</sub> and λ<sub>12</sub> are linearly independent over the rational numbers, and λ<sub>11</sub> and λ<sub>21</sub> are also linearly independent over the rational numbers, then\n:<math>\\lambda_{11}\\lambda_{22}\\neq\\lambda_{12}\\lambda_{21}.\\,</math>\n\nAn equivalent formulation in terms of [[linear algebra]] is the following.  Let ''M'' be the 2&times;2 [[matrix (mathematics)|matrix]]\n:<math>M=\\begin{pmatrix}\\lambda_{11}&\\lambda_{12} \\\\ \\lambda_{21}&\\lambda_{22}\\end{pmatrix},</math>\nwhere exp(λ<sub>''ij''</sub>) is algebraic for 1&nbsp;&le;&nbsp;''i'',''j''&nbsp;&le;&nbsp;2.  Suppose the two rows of ''M'' are linearly independent over the rational numbers, and the two columns of ''M'' are linearly independent over the rational numbers.  Then the [[Rank (linear algebra)|rank]] of ''M'' is 2.\n\nWhile a 2&times;2 matrix having linearly independent rows and columns usually means it has rank 2, in this case we require linear independence over a smaller field so the rank isn't forced to be 2.  For example, the matrix\n:<math>\\begin{pmatrix}1&\\pi \\\\ \\pi&\\pi^2\\end{pmatrix}</math>\nhas rows and columns that are linearly independent over the rational numbers, since ''π'' is irrational.  But the rank of the matrix is 1.  So in this case the conjecture would imply that at least one of ''e'', ''e''<sup>''π''</sup>, and ''e''<sup>''π''&nbsp;²</sup> is transcendental (which in this case is already known since ''e'' is transcendental).\n\n==History==\n\nThe conjecture was considered as early as the early 1940s by [[Atle Selberg]] who never formally stated the conjecture.<ref>Waldschmidt, (2006).</ref>  A special case of the conjecture is mentioned in a 1944 paper of [[Leonidas Alaoglu]] and [[Paul Erdős]] who suggest that it had been considered by [[Carl Ludwig Siegel]].<ref>Alaoglu and Erdős, (1944), p.455: \"It is very likely that ''q''<sup> ''x''</sup> and ''p''<sup> ''x''</sup> cannot be rational at the same time except if ''x'' is an integer. &hellip; At present we can not show this.  Professor Siegel has communicated to us the result that ''q''<sup> ''x''</sup>, ''r''<sup> ''x''</sup> and ''s''<sup> ''x''</sup> can not be simultaneously rational except if ''x'' is an integer.\"</ref>  An equivalent statement was first mentioned in print by [[Theodor Schneider]] who set it as the first of eight important, open problems in transcendental number theory in 1957.<ref>Schneider, (1957).</ref>\n\nThe related [[six exponentials theorem]] was first explicitly mentioned in the 1960s by [[Serge Lang]]<ref>Lang, (1966), chapter 2 section 1.</ref> and [[Kanakanahalli Ramachandra]],<ref>Ramachandra, (1967/8).</ref> and both also explicitly conjecture the above result.<ref>Waldschmidt, (2000), p.15.</ref>  Indeed, after proving the six exponentials theorem Lang mentions the difficulty in dropping the number of exponents from six to four &mdash; the proof used for six exponentials “just misses” when one tries to apply it to four.\n\n==Corollaries==\n\nUsing [[Euler's identity]] this conjecture implies the transcendence of many numbers involving [[E (mathematical constant)|''e'']] and [[Pi|π]].  For example, taking ''x''<sub>1</sub>&nbsp;=&nbsp;1, ''x''<sub>2</sub>&nbsp;=&nbsp;{{radic|2}}, ''y''<sub>1</sub>&nbsp;=&nbsp;''iπ'', and ''y''<sub>2</sub>&nbsp;=&nbsp;''iπ''{{radic|2}}, the conjecture &mdash; if true &mdash; implies that one of the following four numbers is transcendental:\n:<math>e^{i\\pi}, e^{i\\pi\\sqrt{2}}, e^{i\\pi\\sqrt{2}}, e^{2i\\pi}.</math>\nThe first of these is just &minus;1, and the fourth is 1, so the conjecture implies that ''e''<sup>''iπ''{{radic|2}}</sup> is transcendental (which is already known, by consequence of the [[Gelfond–Schneider theorem]]).\n\nAn open problem in [[number theory]] settled by the conjecture is the question of whether there exists a non-[[Integer|integral]] real number ''t'' such that both 2<sup>''t''</sup> and 3<sup>''t''</sup> are integers, or indeed such that ''a''<sup>''t''</sup> and ''b''<sup>''t''</sup> are both integers for some pair of integers ''a'' and ''b'' that are multiplicatively independent over the integers.  Values of ''t'' such that 2<sup>''t''</sup> is an integer are all of the form ''t''&nbsp;=&nbsp;log<sub>2</sub>''m'' for some integer ''m'', while for 3<sup>''t''</sup> to be an integer, ''t'' must be of the form ''t''&nbsp;=&nbsp;log<sub>3</sub>''n'' for some integer ''n''.  By setting ''x''<sub>1</sub>&nbsp;=&nbsp;1, ''x''<sub>2</sub>&nbsp;=&nbsp;''t'', ''y''<sub>1</sub>&nbsp;=&nbsp;log2, and ''y''<sub>2</sub>&nbsp;=&nbsp;log3, the four exponentials conjecture implies that if ''t'' is irrational then one of the following four numbers is transcendental:\n:<math>2, 3, 2^t, 3^t.\\,</math>\nSo if 2<sup>''t''</sup> and 3<sup>''t''</sup> are both integers then the conjecture implies that ''t'' must be a rational number.  Since the only rational numbers ''t'' for which 2<sup>''t''</sup> is also rational are the integers, this implies that there are no non-integral real numbers ''t'' such that both 2<sup>''t''</sup> and 3<sup>''t''</sup> are integers.  It is this consequence, for any two primes not just 2 and 3, that Alaoglu and Erdős desired in their paper as it would imply the conjecture that the quotient of two consecutive [[colossally abundant number]]s is [[Prime number|prime]], extending [[Srinivasa Ramanujan|Ramanujan's]] results on the quotients of consecutive [[superior highly composite number]].<ref>Ramanujan, (1915), section IV.</ref>\n\n==Sharp four exponentials conjecture==\n\nThe four exponentials conjecture reduces the pair and triplet of complex numbers in the hypotheses of the six exponentials theorem to two pairs.  It is conjectured that this is also possible with the sharp six exponentials theorem, and this is the '''sharp four exponentials conjecture'''.<ref>Waldschmidt, \"Hopf algebras&hellip;\" (2005), p.200.</ref>  Specifically, this conjecture claims that if ''x''<sub>1</sub>, ''x''<sub>2</sub>, and ''y''<sub>1</sub>, ''y''<sub>2</sub> are two pairs of complex numbers with each pair being linearly independent over the rational numbers, and if β<sub>''ij''</sub> are four algebraic numbers for 1&nbsp;&le;&nbsp;''i'',''j''&nbsp;&le;&nbsp;2 such that the following four numbers are algebraic:\n:<math>e^{x_1 y_1-\\beta_{11}}, e^{x_1 y_2-\\beta_{12}}, e^{x_2 y_1-\\beta_{21}}, e^{x_2 y_2-\\beta_{22}},</math>\nthen ''x''<sub>''i''</sub>&nbsp;''y''<sub>''j''</sub>&nbsp;=&nbsp;β<sub>''ij''</sub> for 1&nbsp;&le;&nbsp;''i'',''j''&nbsp;&le;&nbsp;2.  So all four exponentials are in fact 1.\n\nThis conjecture implies both the sharp six exponentials theorem, which requires a third ''x'' value, and the as yet unproven sharp five exponentials conjecture that requires a further exponential to be algebraic in its hypotheses.\n\n==Strong four exponentials conjecture==\n\n[[File:N-Exponentials Conjecture.png|frame|alt=Logical implications between the various n-exponentials problems|The logical implications between the various problems in this circle.  Those in red are as yet unproven while those in blue are known results.  The top most result refers to that discussed at [[Baker's theorem#Extensions|Baker's theorem]], while the lower two rows are detailed at the [[six exponentials theorem]] article.]]\nThe strongest result that has been conjectured in this circle of problems is the '''strong four exponentials conjecture'''.<ref>Waldschmidt, (2000), conjecture 11.17.</ref>  This result would imply both aforementioned conjectures concerning four exponentials as well as all the five and six exponentials conjectures and theorems, as illustrated to the right, and all the three exponentials conjectures detailed below.  The statement of this conjecture deals with the [[vector space]] over the algebraic numbers generated by 1 and all logarithms of non-zero algebraic numbers, denoted here as ''L''<sup>∗</sup>.  So ''L''<sup>∗</sup> is the set of all complex numbers of the form\n:<math>\\beta_0+\\sum_{i=1}^n \\beta_i\\log\\alpha_i,</math>\nfor some ''n''&nbsp;&ge;&nbsp;0, where all the β<sub>''i''</sub> and α<sub>''i''</sub> are algebraic and every [[Complex logarithm#Branches of the complex logarithm|branch of the logarithm]] is considered.  The statement of the strong four exponentials conjecture is then as follows.  Let ''x''<sub>1</sub>, ''x''<sub>2</sub>, and ''y''<sub>1</sub>, ''y''<sub>2</sub> be two pairs of complex numbers with each pair being linearly independent over the algebraic numbers, then at least one of the four numbers ''x''<sub>''i''</sub>&nbsp;''y''<sub>''j''</sub> for 1&nbsp;&le;&nbsp;''i'',''j''&nbsp;&le;&nbsp;2 is not in ''L''<sup>∗</sup>.\n\n==Three exponentials conjecture==\n\nThe four exponentials conjecture rules out a special case of non-trivial, [[Homogeneous polynomial|homogeneous]], quadratic relations between logarithms of algebraic numbers.  But a conjectural extension of [[Baker's theorem]] implies that there should be no non-trivial algebraic relations between logarithms of algebraic numbers at all, homogeneous or not.  One case of non-homogeneous quadratic relations is covered by the still open '''three exponentials conjecture'''.<ref>Waldschmidt, \"Variations&hellip;\" (2005), consequence 1.9.</ref>  In its logarithmic form it is the following conjecture.  Let λ<sub>1</sub>, λ<sub>2</sub>, and λ<sub>3</sub> be any three logarithms of algebraic numbers and γ be a non-zero algebraic number, and suppose that λ<sub>1</sub>λ<sub>2</sub>&nbsp;=&nbsp;γλ<sub>3</sub>.  Then λ<sub>1</sub>λ<sub>2</sub>&nbsp;=&nbsp;γλ<sub>3</sub>&nbsp;=&nbsp;0.\n\nThe exponential form of this conjecture is the following.  Let ''x''<sub>1</sub>, ''x''<sub>2</sub>, and ''y'' be non-zero complex numbers and let γ be a non-zero algebraic number.  Then at least one of the following three numbers is transcendental:\n:<math>e^{x_1y}, e^{x_2y}, e^{\\gamma x_1/x_2}.</math>\n\nThere is also a '''sharp three exponentials conjecture''' which claims that if ''x''<sub>1</sub>, ''x''<sub>2</sub>, and ''y'' are non-zero complex numbers and α, β<sub>1</sub>, β<sub>2</sub>, and γ are algebraic numbers such that the following three numbers are algebraic\n:<math>e^{x_1 y-\\beta_1}, e^{x_2 y-\\beta_2}, e^{(\\gamma x_1/x_2)-\\alpha},</math>\nthen either ''x''<sub>2</sub>''y''&nbsp;=&nbsp;β<sub>2</sub> or γ''x''<sub>1</sub>&nbsp;=&nbsp;α&nbsp;''x''<sub>2</sub>.\n\nThe '''strong three exponentials conjecture''' meanwhile states that if ''x''<sub>1</sub>, ''x''<sub>2</sub>, and ''y'' are non-zero complex numbers with ''x''<sub>1</sub>''y'', ''x''<sub>2</sub>''y'', and ''x''<sub>1</sub>/''x''<sub>2</sub> all transcendental, then at least one of the three numbers ''x''<sub>1</sub>''y'', ''x''<sub>2</sub>''y'', ''x''<sub>1</sub>/''x''<sub>2</sub> is not in ''L''<sup>∗</sup>.\n\nAs with the other results in this family, the strong three exponentials conjecture implies the sharp three exponentials conjecture which implies the three exponentials conjecture.  However, the strong and sharp three exponentials conjectures are implied by their four exponentials counterparts, bucking the usual trend.  And the three exponentials conjecture is neither implied by nor implies the four exponentials conjecture.\n\nThe three exponentials conjecture, like the sharp five exponentials conjecture, would imply the transcendence of ''e''<sup>π²</sup> by letting (in the logarithmic version) λ<sub>1</sub>&nbsp;=&nbsp;''i''π, λ<sub>2</sub>&nbsp;=&nbsp;&minus;''i''π, and γ&nbsp;=&nbsp;1.\n\n==Bertrand's conjecture==\n\nMany of the theorems and results in transcendental number theory concerning the exponential function have analogues involving the modular function [[j-invariant|''j'']].  Writing ''q''&nbsp;=&nbsp;''e''<sup>2π''i''{{math|τ}}</sup> for the [[Nome (mathematics)|nome]] and ''j''({{math|τ}})&nbsp;=&nbsp;''J''(''q''), Daniel Bertrand conjectured that if ''q''<sub>1</sub> and ''q''<sub>2</sub> are non-zero algebraic numbers in the complex [[unit disc]] that are multiplicatively independent, then ''J''(''q''<sub>1</sub>) and ''J''(''q''<sub>2</sub>) are algebraically independent over the rational numbers.<ref>Bertrand, (1997), conjecture 2 in section 5.</ref>  Although not obviously related to the four exponentials conjecture, Bertrand's conjecture in fact implies a special case known as the '''weak four exponentials conjecture'''.<ref>Diaz, (2001), section 4.</ref>  This conjecture states that if ''x''<sub>1</sub> and ''x''<sub>2</sub> are two positive real algebraic numbers, neither of them equal to 1, then π² and the product {{nowrap|(log''x''<sub>1</sub>)(log''x''<sub>2</sub>)}} are linearly independent over the rational numbers.  This corresponds to the special case of the four exponentials conjecture whereby ''y''<sub>1</sub>&nbsp;=&nbsp;''i''π, ''y''<sub>2</sub>&nbsp;=&nbsp;&minus;''i''π, and ''x''<sub>1</sub> and ''x''<sub>2</sub> are real.  Perhaps surprisingly, though, it is also a corollary of Bertrand's conjecture, suggesting there may be an approach to the full four exponentials conjecture via the modular function ''j''.\n\n==Notes==\n{{reflist}}\n\n==References==\n\n*{{cite journal | last = Alaoglu | first = Leonidas | author-link = Leonidas Alaoglu | last2 = Erdős | first2 = Paul | author2-link = Paul Erdős | title = On highly composite and similar numbers | journal = [[Transactions of the American Mathematical Society|Trans. Amer. Math. Soc.]] | volume = 56 | pages = 448&ndash;469 | year = 1944 | mr = 0011087 | doi=10.2307/1990319}}\n*{{cite journal | last = Bertrand | first = Daniel | title = Theta functions and transcendence | journal = The Ramanujan Journal | volume = 1 | issue = 4 | pages = 339&ndash;350 | year = 1997 | mr = 1608721}}\n*{{Cite book| last = Diaz | first = Guy | editor-last = Nesterenko | editor-first = Yuri V. | editor-link = Yuri Valentinovich Nesterenko | editor2-last = Philippon | editor2-first = Patrice | contribution = Mahler's conjecture and other transcendence results | publisher = Springer | series = Lecture Notes in Math. | volume = 1752 | year = 2001 | title = Introduction to algebraic independence theory | pages = 13&ndash;26 | isbn = 3-540-41496-7 | mr = 1837824| postscript = <!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->&#123;&#123;inconsistent citations&#125;&#125; }}.\n*{{cite book | last = Lang | first = Serge | authorlink = Serge Lang | title = Introduction to transcendental numbers | publisher = [[Addison-Wesley|Addison-Wesley Publishing Co.]] | year = 1966 | location = Reading, Mass. | mr = 0214547}}\n*{{cite journal | last = Ramachandra | first = Kanakanahalli | authorlink = Kanakanahalli Ramachandra | title = Contributions to the theory of transcendental numbers. I, II. | journal = [[Acta Arithmetica|Acta Arith.]] | volume = 14 | pages = 65&ndash;72, 73&ndash;88 | year = 1967–1968 | mr = 0224566}}\n*{{cite journal | last = Ramanujan | first = Srinivasa | authorlink = Srinivasa Ramanujan | title = Highly Composite Numbers | journal = Proc. London Math. Soc. | volume = 14 | issue = 2 | pages = 347&ndash;407 | year = 1915 | mr = 2280858 | doi = 10.1112/plms/s2_14.1.347 }}\n*{{cite book | last = Schneider | first = Theodor | authorlink = Theodor Schneider | title = Einführung in die transzendenten Zahlen | publisher = Springer | year = 1957 | location = Berlin-Göttingen-Heidelberg | language = German | mr = 0086842}}\n*{{cite book | last = Waldschmidt | first = Michel | title = Diophantine approximation on linear algebraic groups | publisher = Springer | series = Grundlehren der Mathematischen Wissenschaften | volume = 326 | year = 2000 | location = Berlin | isbn = 3-540-66785-7 | mr = 1756786}}\n*{{cite conference | first = Michel | last = Waldschmidt | editor-last = Aoki | editor-first = Takashi | editor2-last = Kanemitsu\n  | editor2-first = Shigeru | editor3-last = Nakahara | editor3-first = Mikio |display-editors = 3 | editor4-last = Ohno | editor4-first = Yasuo | title = Hopf algebras and transcendental numbers | booktitle = Zeta functions, topology, and quantum physics: Papers from the symposium held at Kinki University, Osaka, March 3–6, 2003 | pages = 197&ndash;219 | publisher = Springer | year = 2005 | series = Developments in mathematics | volume = 14 | mr = 2179279}}\n*{{Cite book| last = Waldschmidt | first = Michel | editor-last = Tandon | editor-first = Rajat | contribution = Variations on the six exponentials theorem | year = 2005 | title = Algebra and number theory | pages = 338&ndash;355 | place = Delhi | publisher = Hindustan Book Agency | mr = 2193363| postscript = <!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->&#123;&#123;inconsistent citations&#125;&#125; }}.\n*{{Cite book| last = Waldschmidt | first = Michel | editor-last = Balasubramanian | editor-first = B. | editor2-last = Srinivas | editor2-first = K. | contribution = On Ramachandra's contributions to transcendental number theory | year = 2006 | title = The Riemann zeta function and related themes: papers in honour of Professor K. Ramachandra | series = Ramanujan Math. Soc. Lect. Notes Ser. | volume = 2 | pages = 155&ndash;179 | place = Mysore | publisher = Ramanujan Math. Soc. | mr = 2335194| postscript = <!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->&#123;&#123;inconsistent citations&#125;&#125; }}.\n\n==External links==\n*{{planetmath reference|id=4349|title=Four exponentials conjecture}}\n*{{MathWorld|urlname=FourExponentialsConjecture|title=Four Exponentials Conjecture}}\n\n[[Category:Conjectures]]\n[[Category:Transcendental numbers]]\n[[Category:Exponentials]]"
    },
    {
      "title": "Gaussian function",
      "url": "https://en.wikipedia.org/wiki/Gaussian_function",
      "text": "{{Redirect|Gaussian Curve|the band|Gaussian Curve (band)}}\n{{refimprove|date=August 2009}}\nIn [[mathematics]], a '''Gaussian function''', often simply referred to as a '''Gaussian''', is a [[function (mathematics)|function]] of the form:\n\n:<math>f(x) = a e^{-(x-b)^2/(2c^2)}</math>\n\nfor arbitrary [[real number|real]] constants {{math|<var>a</var>}}, {{math|<var>b</var>}} and non zero {{math|<var>c</var>}}.  It is named after the mathematician [[Carl Friedrich Gauss]]. The [[graph of a function|graph]] of a Gaussian is a characteristic symmetric \"[[Normal distribution|bell curve]]\" shape. The parameter {{math|<var>a</var>}} is the height of the curve's peak, {{math|<var>b</var>}} is the position of the center of the peak and {{math|<var>c</var>}} (the [[standard deviation]], sometimes called the Gaussian [[Root mean square|RMS]] width) controls the width of the \"bell\".\n\nGaussian functions are often used to represent the [[probability density function]] of a [[normal distribution|normally distributed]] [[random variable]] with [[expected value]] {{math|1=<var>μ</var>&nbsp;=&nbsp;<var>b</var>}} and [[variance]] {{math|1=<var>σ</var>{{sup|2}}&nbsp;=&nbsp;<var>c</var>{{sup|2}}}}. In this case, the Gaussian is of the form:\n\n:<math> g(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{1}{2}\\left((x-\\mu)/\\sigma\\right)^2 }. </math>\n\nGaussian functions are widely used in [[statistics]] to describe the [[normal distribution]]s, in [[signal processing]] to define [[Gaussian filter]]s, in [[image processing]] where two-dimensional Gaussians are used for [[Gaussian blur]]s, and in mathematics to solve [[heat equation]]s and [[diffusion equation]]s and to define the [[Weierstrass transform]].\n\n== Properties ==\nGaussian functions arise by composing the [[exponential function]] with a [[Concave function|concave]] [[quadratic function]]. The Gaussian functions are thus those functions whose [[logarithm]] is a concave quadratic function.\n\nThe parameter {{math|<var>c</var>}} is related to the [[full width at half maximum]] (FWHM) of the peak according to\n\n: <math>\\mathrm{FWHM} = 2 \\sqrt{2 \\ln 2}\\ c \\approx 2.35482 c.</math>{{NoteTag|Using the [[List of logarithmic identities|logarithmic identity]] <math>\\log x = -\\log \\left( \\frac 1 x \\right)</math>, this expression can be transformed to <math>\\mathrm{FWHM} = 2 \\sqrt{-2 \\ln 0.5}\\ c</math>.}}\n\nThe function may then be expressed in terms of the FWHM, represented by {{math|<var>w</var>}}:\n: <math>f(x) = a e^{-4 (\\ln 2) (x-b)^2/w^2 }</math>\n\nAlternatively, the parameter {{math|<var>c</var>}} can be interpreted by saying that the two [[inflection point]]s of the function occur at {{math|1=<var>x</var>&nbsp;=&nbsp;<var>b</var>&nbsp;−&nbsp;<var>c</var>}} and {{math|1=<var>x</var>&nbsp;=&nbsp;<var>b</var>&nbsp;+&nbsp;<var>c</var>}}.\n\nThe ''full width at tenth of maximum'' (FWTM) for a Gaussian could be of interest and is \n: <math>\\mathrm{FWTM} = 2 \\sqrt{2 \\ln 10}\\ c \\approx 4.29193 c.</math>\n\nGaussian functions are [[analytic function|analytic]], and their [[limit (mathematics)|limit]] as {{math|<var>x</var>&nbsp;→&nbsp;∞}} is&nbsp;0 (for the above case of {{math|1=<var>b</var>&nbsp;=&nbsp;0}}).\n\nGaussian functions are among those functions that are [[Elementary function (differential algebra)|elementary]] but lack elementary [[antiderivative]]s; the [[integral]] of the Gaussian function is the [[error function]].  Nonetheless their improper integrals over the whole real line can be evaluated exactly, using the [[Gaussian integral]]\n\n:<math>\\int_{-\\infty}^\\infty e^{-x^2}\\,dx=\\sqrt{\\pi}</math>\n\nand one obtains\n\n:<math>\\int_{-\\infty}^\\infty a e^{-(x-b)^2/(2c^2)}\\,dx=ac\\cdot\\sqrt{2\\pi}.</math>\n\nThis integral is 1 if and only if <math>a = \\tfrac{1}{c\\sqrt{2\\pi}}</math>, and in this case the Gaussian is the [[probability density function]] of a [[normal distribution|normally distributed]] [[random variable]] with [[expected value]] {{math|1=<var>μ</var>&nbsp;=&nbsp;<var>b</var>}} and [[variance]] {{math|1=<var>σ</var>{{sup|2}}&nbsp;=&nbsp;<var>c</var>{{sup|2}}}}:\n\n:<math> g(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -(1/2)\\left((x-\\mu)/\\sigma\\right)^2 }. </math>\n\nThese Gaussians are plotted in the accompanying figure.\n[[Image:Normal Distribution PDF.svg|thumb|360px|right|[[Normalizing constant|Normalized]] Gaussian curves with [[expected value]] {{math|<var>μ</var>}} and [[variance]] {{math|<var>σ</var>{{sup|2}}}}. The corresponding parameters are <math>a = \\tfrac{1}{\\sigma\\sqrt{2\\pi}}</math>,  {{math|1=<var>b</var> = <var>μ</var>}} and {{math|1=<var>c</var> = <var>σ</var>}}.]]\n\nGaussian functions centered at zero minimize the Fourier [[Fourier transform#Uncertainty principle|uncertainty principle]].\n\nThe product of two Gaussian functions is a Gaussian, and the [[convolution]] of two Gaussian functions is also a Gaussian, with variance being the sum of the original variances: <math>c^2 = c_1^2 + c_2^2</math>.  The product of two Gaussian probability density functions, though, is not in general a Gaussian PDF.\n\nTaking the [[Fourier transform#Other conventions|Fourier transform (unitary, angular frequency convention)]] of a Gaussian function with parameters {{math|1=<var>a</var>&nbsp;=&nbsp;1}}, {{math|1=<var>b</var>&nbsp;=&nbsp;0}} and {{math|<var>c</var>}} yields another Gaussian function, with parameters <math>c</math>, {{math|1=<var>b</var>&nbsp;=&nbsp;0}} and <math>\\frac{1}{c}</math>.<ref>{{cite web |last=Weisstein|first=Eric&nbsp;W.|title=Fourier Transform&nbsp;– Gaussian |url = http://mathworld.wolfram.com/FourierTransformGaussian.html |publisher=[[MathWorld]]|accessdate=19 December 2013 }}</ref> So in particular the Gaussian functions with {{math|1=<var>b</var>&nbsp;=&nbsp;0}} and <math>c = 1</math> are kept fixed by the Fourier transform (they are [[eigenfunction]]s of the Fourier transform with eigenvalue&nbsp;1).\n<!-- The way the Fourier transform is currently defined in its article (with pi in the exponent, also the way that I prefer), the Gaussian must also have a pi in its exponent. ~~~~ -->\nA physical realization is that of the [[Fraunhofer diffraction#Diffraction by an aperture with a Gaussian profile|diffraction pattern]]: for example, a [[photographic slide]] whose [[transmittance]] has a Gaussian variation is also a Gaussian function.\n\n<!--\nUsing [[periodic summation]] and [[discretization]] you can construct vectors from the Gaussian function,\nthat behave similarly under the [[Discrete Fourier transform]].\nComparing the zeroth coefficient of the Discrete Fourier transform of such a vector\nwith the periodic summation and discretization of the Continuous Fourier transform of the Gaussian yields the interesting identity:\n-->\nThe fact that the Gaussian function is an eigenfunction of the continuous Fourier transform\nallows us to derive the following interesting{{clarify|date=August 2016}} identity from the [[Poisson summation formula]]:\n: <math>\\sum_{k\\in\\mathbb{Z}}\\exp\\left(-\\pi\\cdot\\left(\\frac{k}{c}\\right)^2\\right) = c\\cdot\\sum_{k\\in\\mathbb{Z}}\\exp(-\\pi\\cdot(kc)^2).</math>\n\n==Integral of a Gaussian function==\nThe integral of an arbitrary Gaussian function is\n\n:<math>\\int_{-\\infty}^\\infty a\\,e^{-\\left( x-b \\right)^2/2c^2}\\,dx=\\sqrt{2} a \\, \\left\\vert c \\right\\vert \\, \\sqrt{\\pi}</math>\n\nAn alternative form is\n\n:<math>\\int_{-\\infty}^\\infty k\\,e^{-f x^2 + g x + h}\\,dx=\\int_{-\\infty}^\\infty k\\,e^{-f \\left( x-g/(2f)\\right)^2 +g^2/(4f) + h}\\,dx=k\\,\\sqrt{\\frac{\\pi}{f}}\\,\\exp\\left(\\frac{g^2}{4f} + h\\right)</math>\n\nwhere ''f'' must be strictly positive for the integral to converge.\n\n===Relation to standard Gaussian integral===\n\nThe integral\n\n:<math>\\int_{-\\infty}^\\infty ae^{-(x-b)^2/2c^2}\\,dx</math>\n\nfor some [[real number|real]] constants a, b, c > 0 can be calculated by putting it into the form of a [[Gaussian integral]].  First, the constant ''a'' can simply be factored out of the integral. Next, the variable of integration is changed from ''x'' to ''y''&nbsp;=&nbsp;''x''&nbsp;-&nbsp;''b''.\n\n:<math>a\\int_{-\\infty}^\\infty e^{-y^2/2c^2}\\,dy</math>\n\nand then to <math>z=y/\\sqrt{2 c^2}</math>\n\n:<math>a\\sqrt{2 c^2} \\int_{-\\infty}^\\infty e^{-z^2}\\,dz</math>\n\nThen, using the [[Gaussian integral|Gaussian integral identity]]\n\n:<math>\\int_{-\\infty}^\\infty e^{-z^2}\\,dz = \\sqrt{\\pi}</math>\n\nwe have\n\n:<math>\\int_{-\\infty}^\\infty ae^{-(x-b)^2/2c^2}\\,dx=a\\sqrt{2\\pi c^2}</math>\n\n== Two-dimensional Gaussian function ==\n[[Image:Gaussian 2d.svg|thumb|300px|Gaussian curve with a two-dimensional domain]]\nIn two dimensions, the power to which ''e'' is raised in the Gaussian function is any negative-definite quadratic form.  Consequently, the level sets of the Gaussian will always be ellipses.\n\nA particular example of a two-dimensional Gaussian function is\n<!-- This makes the formula consistent with the 1d formula above -->\n\n:<math>f(x,y) = A \\exp\\left(- \\left(\\frac{(x-x_o)^2}{2\\sigma_X^2} + \\frac{(y-y_o)^2}{2\\sigma_Y^2} \\right)\\right).</math>\n\nHere the coefficient ''A'' is the amplitude, ''x''<sub>o</sub>,y<sub>o</sub> is the center and σ<sub>''x''</sub>, σ<sub>''y''</sub> are the ''x'' and ''y'' spreads of the blob.  The figure on the right was created using ''A'' = 1, ''x''<sub>o</sub> = 0, ''y''<sub>o</sub> = 0, σ<sub>''x''</sub> = σ<sub>''y''</sub> = 1.\n\nThe volume under the Gaussian function is given by\n:<math>V = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty f(x,y)\\,dx \\,dy=2 \\pi A \\sigma_X \\sigma_Y.</math>\n\nIn general, a two-dimensional elliptical Gaussian function is expressed as\n\n:<math>f(x,y) = A \\exp\\left(- \\left(a(x - x_o)^2 + 2b(x-x_o)(y-y_o) + c(y-y_o)^2 \\right)\\right)</math>\n\nwhere the matrix\n\n:<math>\\left[\\begin{matrix} a & b \\\\ b & c \\end{matrix}\\right] </math>\n\nis [[positive-definite matrix|positive-definite]].\n\nUsing this formulation, the figure on the right can be created using ''A'' = 1, (''x''<sub>o</sub>, ''y''<sub>o</sub>) = (0, 0), ''a'' = ''c'' = 1/2, ''b'' = 0.\n\n=== Meaning of parameters for the general equation ===\nFor the general form of the equation the coefficient ''A'' is the height of the peak and (''x''<sub>o</sub>,&nbsp;''y''<sub>o</sub>) is the center of the blob.\n\nIf we set\n\n:<math>\n\\begin{align}\na & = \\frac{\\cos^2\\theta}{2\\sigma_X^2} + \\frac{\\sin^2\\theta}{2\\sigma_Y^2} \\\\[4pt]\nb & = -\\frac{\\sin2\\theta}{4\\sigma_X^2} + \\frac{\\sin2\\theta}{4\\sigma_Y^2} \\\\[4pt]\nc & = \\frac{\\sin^2\\theta}{2\\sigma_X^2} + \\frac{\\cos^2\\theta}{2\\sigma_Y^2}\n\\end{align}\n</math>\n\nthen we rotate the blob by a clockwise angle <math>\\theta</math> (for counterclockwise rotation invert the signs in the ''b'' coefficient). This can be seen in the following examples:\n\n{|\n| [[Image:Gaussian 2d 1.svg|thumb|200px|<math>\\theta = 0</math>]]\n| [[Image:Gaussian 2d 2.svg|thumb|200px|<math>\\theta = \\pi/6</math>]]\n| [[Image:Gaussian 2d 3.svg|thumb|200px|<math>\\theta = \\pi/3</math>]]\n|}\n\nUsing the following [[GNU Octave|Octave]] code, one can easily see the effect of changing the parameters\n\n<source lang=\"octave\">\nA = 1;\nx0 = 0; y0 = 0;\n\nsigma_X = 1;\nsigma_Y = 2;\n\n[X, Y] = meshgrid(-5:.1:5, -5:.1:5);\n\nfor theta = 0:pi/100:pi\n    a = cos(theta)^2/(2*sigma_X^2) + sin(theta)^2/(2*sigma_Y^2);\n    b = -sin(2*theta)/(4*sigma_X^2) + sin(2*theta)/(4*sigma_Y^2);\n    c = sin(theta)^2/(2*sigma_X^2) + cos(theta)^2/(2*sigma_Y^2);\n\n    Z = A*exp( - (a*(X-x0).^2 + 2*b*(X-x0).*(Y-y0) + c*(Y-y0).^2));\n\nsurf(X,Y,Z);shading interp;view(-36,36)\nwaitforbuttonpress\nend\n\n</source>\n\nSuch functions are often used in [[image processing]] and in computational models of [[visual system]] function—see the articles on [[scale space]] and [[affine shn]].\n\nAlso see [[multivariate normal distribution]].\n\n=== Higher-order Gaussian or super-Gaussian function ===\nA more general formulation of a Gaussian function with a flat-top and Gaussian fall-off can be taken by raising the content of the exponent to a power, <math>P</math>:\n\n<math>f(x) = A \\exp\\left(-\\left(\\frac{(x-x_o)^2}{2\\sigma_X^2}\\right)^P\\right).</math>\nThis function is known as a super-Gaussian function and is often used for Gaussian beam formulation.<ref>Parent, A., M. Morin, and P. Lavigne. \"Propagation of super-Gaussian field distributions.\" ''Optical and quantum electronics'' 24.9 (1992): S1071-S1079.</ref> In a two-dimensional formulation, a Gaussian function along <math>x</math> and <math>y</math> can be combined with potentially different <math>P_X</math> and <math>P_X</math> to form an elliptical Gaussian distribution,<math>f(x,y) = A \\exp\\left(- \\left(\\frac{(x-x_o)^2}{2\\sigma_X^2} + \\frac{(y-y_o)^2}{2\\sigma_Y^2} \\right)^P\\right)</math> or a rectangular Gaussian distribution, <math>f(x,y) = A \\exp\\left(- \\left(\\frac{(x-x_o)^2}{2\\sigma_X^2}\\right)^{P_X} - \\left(\\frac{(y-y_o)^2}{2\\sigma_Y^2} \\right)^{P_Y}\\right)</math>.<ref>{{Cite web|url=http://www.aor.com/anonymous/pub/commands.pdf|title=GLAD optical software commands manual, Entry on GAUSSIAN command|last=|first=|date=2016-12-15|website=Applied Optics Research|access-date=}}</ref>\n\n== Multi-dimensional Gaussian function ==\n{{main article|Multivariate normal distribution}}\nIn an <math>n</math>-dimensional space a Gaussian function can be defined as\n:<math>\nf(x) = \\exp(-x^TAx) \\;,\n</math>\nwhere <math>x=\\{x_1,\\dots,x_n\\}</math> is a column of <math>n</math> coordinates, <math>A</math> is a [[positive-definite matrix|positive-definite]] <math>n\\times n</math> matrix, and <math>{}^T</math> denotes [[transpose|matrix transposition]].\n\nThe integral of this Gaussian function over the whole <math>n</math>-dimensional space is given as\n:<math>\n\\int_{\\mathbb{R}^n} \\exp(-x^TAx) \\, dx = \\sqrt{\\frac{\\pi^n}{\\det A}} \\;.\n</math>\nIt can be easily calculated by diagonalizing the matrix <math>A</math> and changing the integration variables to the eigenvectors of  <math>A</math>.\n\nMore generally a shifted Gaussian function is defined as\n:<math>\nf(x) = \\exp(-x^TAx+s^Tx) \\;,\n</math>\nwhere <math>s=\\{s_1,\\dots,s_n\\}</math> is the shift vector and the matrix <math>A</math> can be assumed to be symmetric, <math>A^T=A</math>, and positive-definite. The following integrals with this function can be calculated with the same technique,\n\n:<math>\n\\begin{align}\n& \\int_{\\mathbb{R}^n} e^{-x^T A x+v^Tx} \\, dx = \\sqrt{\\frac{\\pi^n}{\\det{A}}} \\exp\\left(\\frac{1}{4}v^T A^{-1}v\\right)\\equiv \\mathcal{M}\\;. \\\\[6pt]\n& \\int_{\\mathbb{R}^n} e^{- x^T A x + v^T x}  \\left( a^T x \\right) \\, dx = (a^T u) \\cdot \\mathcal{M}\\;,\\text{ where } u = \\frac{1}{2} A^{- 1} v \\;. \\\\[6pt]\n& \\int_{\\mathbb{R}^n} e^{- x^T A x + v^T x} (x^T D x) \\, dx = \\left( u^T D u +\n\\frac{1}{2} \\operatorname{tr} (D A^{- 1}) \\right) \\cdot \\mathcal{M}\\;. \\\\[6pt]\n& \\int_{\\mathbb{R}^n} e^{- x^T A' x + s'^T x} \\left( -\\frac{\\partial}{\\partial x} \\Lambda \\frac{\\partial}{\\partial x} \\right) e^{-x^T A x + s^T x} \\, dx \\\\[6pt]\n= {} & \\left( 2 \\operatorname{tr} (A' \\Lambda A B^{- 1}) + 4 u^T A' \\Lambda A u - 2 u^T (A' \\Lambda s + A \\Lambda s') + s'^T \\Lambda s \\right) \\cdot \\mathcal{M}\\;,\\\\[6pt]\n& \\text{where } u = \\frac{1}{2} B^{- 1} v, v = s + s', B = A + A' \\;.\n\\end{align}\n</math>\n\n== Gaussian profile estimation ==\nA number of fields such as [[Photometry (astronomy)|stellar photometry]], [[Gaussian beam]] characterization, and [[emission spectrum#Emission spectroscopy|emission/absorption line spectroscopy]] work with sampled Gaussian functions and need to accurately estimate the height, position, and width parameters of the function. These are <math>a</math>, <math>b</math>, and <math>c</math> for a 1D Gaussian function, <math>A</math>, <math>(x_0,y_0)</math>, and <math>(\\sigma_X,\\sigma_Y)</math> for a 2D Gaussian function. The most common method for estimating the profile parameters is to take the logarithm of the data and fit a parabola to the resulting data set.<ref name=\"Guo\">[https://dx.doi.org/10.1109/MSP.2011.941846 Hongwei Guo, \"A simple algorithm for fitting a Gaussian function,\" IEEE Sign. Proc. Mag. 28(9): 134-137 (2011).]</ref> While this provides a simple [[least squares]] fitting procedure, the resulting algorithm is biased by excessively weighting small data values, and this can produce large errors in the profile estimate. One can partially compensate for this through [[least squares#Weighted least squares|weighted least squares]] estimation, in which the small data values are given small weights, but this too can be biased by allowing the tail of the Gaussian to dominate the fit. In order to remove the bias, one can instead use an iterative procedure in which the weights are updated at each iteration (see [[Iteratively reweighted least squares]]).<ref name=\"Guo\" />\n\nOnce one has an algorithm for estimating the Gaussian function parameters, it is also important to know how accurate those estimates are. While an estimation algorithm can provide numerical estimates for the variance of each parameter (i.e. the variance of the estimated height, position, and width of the function), one can use [[Cramér–Rao bound]] theory to obtain an analytical expression for the lower bound on the parameter variances, given some assumptions about the data.<ref name=\"Hagen1\">[https://dx.doi.org/10.1364/AO.46.005374 N. Hagen, M. Kupinski, and E. L. Dereniak, \"Gaussian profile estimation in one dimension,\" Appl. Opt. 46:5374–5383 (2007)]</ref><ref name=\"Hagen2\">[https://dx.doi.org/10.1364/AO.47.006842 N. Hagen and E. L. Dereniak, \"Gaussian profile estimation in two dimensions,\" Appl. Opt. 47:6842–6851 (2008)]</ref>\n# The noise in the measured profile is either [[Independent and identically-distributed random variables|i.i.d.]] Gaussian, or the noise is [[Poisson distribution|Poisson-distributed]].\n# The spacing between each sampling (i.e. the distance between pixels measuring the data) is uniform.\n# The peak is \"well-sampled\", so that less than 10% of the area or volume under the peak (area if a 1D Gaussian, volume if a 2D Gaussian) lies outside the measurement region.\n# The width of the peak is much larger than the distance between sample locations (i.e. the detector pixels must be at least 5 times smaller than the Gaussian FWHM).\nWhen these assumptions are satisfied, the following covariance matrix '''K''' applies for the 1D profile parameters <math>a</math>, <math>b</math>, and <math>c</math> under i.i.d. Gaussian noise and under Poisson noise:<ref name=\"Hagen1\" />\n\n:<math> \\mathbf{K}_{\\text{Gauss}} = \\frac{\\sigma^2}{\\sqrt{\\pi} \\delta_X Q^2} \\begin{pmatrix} \\frac{3}{2c} &0 &\\frac{-1}{a} \\\\ 0 &\\frac{2c}{a^2} &0 \\\\ \\frac{-1}{a} &0 &\\frac{2c}{a^2} \\end{pmatrix} \\ , \\qquad \\mathbf{K}_\\text{Poiss} = \\frac{1}{\\sqrt{2 \\pi}} \\begin{pmatrix} \\frac{3a}{2c} &0 &-\\frac{1}{2} \\\\ 0 &\\frac{c}{a} &0 \\\\ -\\frac{1}{2} &0 &\\frac{c}{2a} \\end{pmatrix} \\ ,</math>\n\nwhere <math>\\delta_X</math> is the width of the pixels used to sample the function, <math>Q</math> is the quantum efficiency of the detector, and <math>\\sigma</math> indicates the standard deviation of the measurement noise. Thus, the individual variances for the parameters are, in the Gaussian noise case,\n\n:<math>\\begin{align} \\operatorname{var} (a) &= \\frac{3 \\sigma^2}{2 \\sqrt{\\pi} \\, \\delta_X Q^2 c} \\\\ \\operatorname{var} (b) &= \\frac{2 \\sigma^2 c}{\\delta_X \\sqrt{\\pi} \\, Q^2 a^2} \\\\ \\operatorname{var} (c) &= \\frac{2 \\sigma^2 c}{\\delta_X \\sqrt{\\pi} \\, Q^2 a^2} \\end{align}</math>\n\nand in the Poisson noise case,\n\n:<math>\\begin{align} \\operatorname{var} (a) &= \\frac{3a}{2 \\sqrt{2 \\pi} \\, c} \\\\ \\operatorname{var} (b) &= \\frac{c}{\\sqrt{2 \\pi} \\, a} \\\\ \\operatorname{var} (c) &= \\frac{c}{2 \\sqrt{2 \\pi} \\, a}. \\end{align} </math>\n\nFor the 2D profile parameters giving the amplitude <math>A</math>, position <math>(x_0,y_0)</math>, and width <math>(\\sigma_X,\\sigma_Y)</math> of the profile, the following covariance matrices apply:<ref name=\"Hagen2\" />\n\n:<math>\n\\begin{align}\n\\mathbf{K}_\\text{Gauss} = \\frac{\\sigma^2}{\\pi \\delta_X \\delta_Y Q^2} & \\begin{pmatrix} \\frac{2}{\\sigma_X \\sigma_Y} &0 &0 &\\frac{-1}{A \\sigma_Y} &\\frac{-1}{A \\sigma_X} \\\\ 0\n      &\\frac{2 \\sigma_X}{A^2 \\sigma_Y} &0 &0 &0 \\\\ 0 &0 &\\frac{2 \\sigma_Y}{A^2 \\sigma_X} &0 &0 \\\\ \\frac{-1}{A \\sigma_y} &0 &0 &\\frac{2 \\sigma_X}{A^2 \\sigma_y} &0 \\\\\n      \\frac{-1}{A \\sigma_X} &0 &0 &0 &\\frac{2 \\sigma_Y}{A^2 \\sigma_X} \\end{pmatrix} \\\\[6pt]\n\\mathbf{K}_{\\operatorname{Poisson}} = \\frac{1}{2 \\pi} & \\begin{pmatrix} \\frac{3A}{\\sigma_X \\sigma_Y} &0 &0 &\\frac{-1}{\\sigma_Y} &\\frac{-1}{\\sigma_X} \\\\ 0\n      &\\frac{\\sigma_X}{A \\sigma_Y} &0 &0 &0 \\\\ 0 &0 &\\frac{\\sigma_Y}{A \\sigma_X} &0 &0 \\\\ \\frac{-1}{\\sigma_Y} &0 &0 &\\frac{2 \\sigma_X}{3A \\sigma_Y} &\\frac{1}{3A} \\\\\n      \\frac{-1}{\\sigma_X} &0 &0 &\\frac{1}{3A} &\\frac{2 \\sigma_Y}{3A \\sigma_X} \\end{pmatrix}.\n\\end{align}      \n</math>\n\nwhere the individual parameter variances are given by the diagonal elements of the covariance matrix.\n\n== Discrete Gaussian ==\n{{main article|Discrete Gaussian kernel}}\n[[File:Discrete Gaussian kernel.svg|thumb|The [[discrete Gaussian kernel]] (solid), compared with the [[sampled Gaussian kernel]] (dashed) for scales <math>t=0.5,1,2,4.</math>]]\nOne may ask for a discrete analog to the Gaussian;\nthis is necessary in discrete applications, particularly [[digital signal processing]]. A simple answer is to sample the continuous Gaussian, yielding the [[sampled Gaussian kernel]]. However, this discrete function does not have the discrete analogs of the properties of the continuous function, and can lead to undesired effects, as described in the article [[scale space implementation]].\n\nAn alternative approach is to use [[discrete Gaussian kernel]]:<ref name=\"tpl90\">[http://www.nada.kth.se/~tony/abstracts/Lin90-PAMI.html Lindeberg, T., \"Scale-space for discrete signals,\" PAMI(12), No. 3, March 1990, pp. 234–254.]</ref>\n:<math>T(n, t) = e^{-t} I_n(t)</math>\nwhere <math>I_n(t)</math> denotes the [[modified Bessel function]]s of integer order.\n\nThis is the discrete analog of the continuous Gaussian in that it is the solution to the discrete [[diffusion equation]] (discrete space, continuous time), just as the continuous Gaussian is the solution to the continuous diffusion equation.<ref>Campbell, J, 2007, ''[https://dx.doi.org/10.1016/j.tpb.2007.08.001 The SMM model as a boundary value problem using the discrete diffusion equation]'', Theor Popul Biol. 2007 Dec;72(4):539–46.</ref>\n\n==Applications==\nGaussian functions appear in many contexts in the [[natural sciences]], the [[social sciences]], [[mathematics]], and [[engineering]].  Some examples include:\n* In [[statistics]] and [[probability theory]], Gaussian functions appear as the density function of the [[normal distribution]], which is a limiting [[probability distribution]] of complicated sums, according to the [[central limit theorem]].\n* Gaussian functions are the [[Green's function]] for the (homogeneous and isotropic) [[diffusion equation]] (and to the [[heat equation]], which is the same thing), a [[partial differential equation]] that describes the time evolution of a mass-density under [[diffusion]]. Specifically, if the mass-density at time ''t''=0 is given by a [[Dirac delta]], which essentially means that the mass is initially concentrated in a single point, then the mass-distribution at time ''t'' will be given by a Gaussian function, with the parameter ''a'' being linearly related to 1/{{radic|''t''}} and ''c'' being linearly related to {{radic|''t''}}; this time-varying Gaussian is described by the [[heat kernel]]. More generally, if the initial mass-density is &phi;(''x''), then the mass-density at later times is obtained by taking the [[convolution]] of &phi; with a Gaussian function. The convolution of a function with a Gaussian is also known as a [[Weierstrass transform]].\n* A Gaussian function is the [[wave function]] of the [[ground state]] of the [[quantum harmonic oscillator]].\n* The [[molecular orbital]]s used in [[computational chemistry]] can be [[linear combination]]s of Gaussian functions called [[Gaussian orbital]]s (see also [[basis set (chemistry)]]).\n* Mathematically, the [[derivative]]s of the Gaussian function can be represented using [[Hermite functions]].  The ''n''-th derivative of the Gaussian is the Gaussian function itself multiplied by the ''n''-th [[Hermite polynomial]], up to scale.  \n* Consequently, Gaussian functions are also associated with the [[vacuum state]] in [[quantum field theory]].\n* [[Gaussian beam]]s are used in optical systems, microwave systems and lasers.\n* In [[scale space]] representation, Gaussian functions are used as smoothing kernels for generating multi-scale representations in [[computer vision]] and [[image processing]]. Specifically, derivatives of Gaussians ([[Hermite functions]]) are used as a basis for defining a large number of types of visual operations.\n* Gaussian functions are used to define some types of [[artificial neural network]]s.\n* In [[fluorescence microscopy]] a 2D Gaussian function is used to approximate the [[Airy disk]], describing the intensity distribution produced by a [[point source]].\n* In [[signal processing]] they serve to define [[Gaussian filter]]s, such as in [[image processing]] where 2D Gaussians are used for [[Gaussian blur]]s. In [[digital signal processing]], one uses a [[discrete Gaussian kernel]], which may be defined by sampling a Gaussian, or in a different way.\n* In [[geostatistics]] they have been used for understanding the variability between the patterns of a complex [[training image]]. They are used with kernel methods to cluster the patterns in the feature space.<ref>Honarkhah, M and Caers, J, 2010, ''[https://dx.doi.org/10.1007/s11004-010-9276-7 Stochastic Simulation of Patterns Using Distance-Based Pattern Modeling]'', Mathematical Geosciences, 42: 487–517</ref>\n\n==See also==\n*[[Normal distribution]]\n*[[Lorentzian function]]\n*[[Radial basis function kernel]]\n\n== Notes ==\n{{NoteFoot}}\n\n== References ==\n{{Reflist}}\n\n==External links==\n* [http://mathworld.wolfram.com/GaussianFunction.html Mathworld, includes a proof for the relations between c and FWHM]\n* {{MathPages|id=home/kmath045/kmath045|title=Integrating The Bell Curve}}\n* [https://github.com/frecker/gaussian-distribution/ Haskell, Erlang and Perl implementation of Gaussian distribution]\n* [https://upload.wikimedia.org/wikipedia/commons/a/a2/Cumulative_function_n_dimensional_Gaussians_12.2013.pdf Bensimhoun Michael, ''N''-Dimensional Cumulative Function, And Other Useful Facts About Gaussians and Normal Densities (2009)]\n*[https://github.com/dwaithe/generalMacros/tree/master/gaussian_fitting Code for fitting Gaussians in ImageJ and Fiji.]\n\n[[Category:Exponentials]]\n[[Category:Gaussian function]]\n[[Category:Articles containing proofs]]\n[[Category:Articles with example MATLAB/Octave code]]"
    },
    {
      "title": "Growth curve (statistics)",
      "url": "https://en.wikipedia.org/wiki/Growth_curve_%28statistics%29",
      "text": "{{expert needed|1=Statistics|date=November 2018}}\n{{lead too short|date=November 2018}}\n[[File:PSM V85 D564 Table of height and weight for boys.png|thumb|350px|Table of height and weight for boys over time. The growth curve model (also known as GMANOVA) is used to analyze data such as this, where multiple observations are made on collections of individuals over time.]]\n\nThe '''growth curve model''' in [[statistics]] is a specific multivariate linear model, also known as GMANOVA (Generalized Multivariate ANalysis-Of-VAriance).<ref>{{cite book\n|author1=Kim, Kevin  |author2=Timm, Neil\n|chapter=\"Restricted MGLM and growth curve model\" (Chapter 7)\n|title=Univariate and multivariate general linear models: Theory and applications with '''SAS''' (with 1 CD-ROM for Windows and UNIX).\n|edition=Second\n|series=Statistics: Textbooks and Monographs\n|publisher=Chapman & Hall/CRC\n|location=Boca Raton, Florida\n|year=2007\n|isbn=978-1-58488-634-1\n}}</ref> It generalizes [[MANOVA]] by allowing post-matrices, as seen in the definition.\n\n==Definition==\n'''Growth curve model''':<ref>{{cite book\n|author1=Kollo, Tõnu  |author2=von Rosen, Dietrich\n|chapter=\"Multivariate linear models\" (chapter 4), especially \"The Growth curve model and extensions\" (Chapter 4.1)\n|title=Advanced multivariate statistics with matrices\n|series=Mathematics and its applications\n|location=Dordrecht\n|volume=579\n|publisher=Springer\n|year=2005\n|isbn=978-1-4020-3418-3\n}}</ref> Let '''X''' be a ''p''×''n'' [[random matrix]]  corresponding to the observations, '''A''' a ''p''×''q'' within design matrix with ''q''&nbsp;≤&nbsp;''p'', '''B''' a ''q''×''k'' parameter matrix, '''C''' a ''k''×''n'' between individual design matrix with rank(''C'')&nbsp;+&nbsp;''p''&nbsp;≤&nbsp;''n'' and let '''Σ''' be a positive-definite ''p''×''p'' matrix. Then\n\n: <math>X=ABC+\\Sigma^{1/2}E</math>\n\ndefines the growth curve model, where '''A''' and '''C''' are known, '''B''' and '''Σ''' are unknown, and '''E''' is a [[random matrix]] distributed as ''N''<sub>''p'',''n''</sub>(0,''I''<sub>''p''</sub>,<sub>''n''</sub>).\n\nThis differs from standard [[MANOVA]] by the addition of '''C''', a \"postmatrix\".<ref name=\"orig\" />\n\n==History==\nMany writers have considered the growth curve analysis, among them Wishart (1938),<ref>{{Cite journal|last=Wishart|first=John|year=1938|title=Growth rate determinations in nutrition studies with the bacon pig, and their analysis|url=|journal=Biometrika|volume=30|pages=16–28|via=|doi=10.1093/biomet/30.1-2.16}}</ref> Box (1950) <ref>{{Cite journal|last=Box|first=G.E.P.|year=1950|title=Problems in the analysis of growth and wear curves|url=|journal=Biometrics|volume=6|pages=362–89|via=|doi=10.2307/3001781}}</ref> and Rao (1958).<ref>{{Cite journal|last=Radhakrishna|first=Rao|year=1958|title=Some statistical methods for comparison of growth curves.|url=|journal=Biometrics|volume=14|pages=1–17|via=|doi=10.2307/2527726}}</ref>  Potthoff and Roy in 1964;<ref name=\"orig\">R.F. Potthoff and S.N. Roy, “A generalized multivariate analysis of variance model useful especially for growth curve problems,”\n''Biometrika'', vol. 51, pp. 313–326, 1964</ref> were the first in analyzing [[longitudinal data]] applying  GMANOVA models.\n\n==Applications==\nGMANOVA is frequently used for the analysis of surveys, clinical trials, and agricultural data,<ref>{{cite book\n|author1=Pan, Jian-Xin  |author2=Fang, Kai-Tai\n|title=Growth curve models and statistical diagnostics\n|series=Springer Series in Statistics\n|publisher=Springer-Verlag\n|location=New York\n|year=2002\n|isbn=0-387-95053-2\n}}</ref> as well as more recently in the context of Radar adaptive detection.<ref>{{cite journal|last1 = Ciuonzo|first1 = D.|last2 = De Maio|first2 = A.|last3 = Orlando|first3 = D.|title = A Unifying Framework for Adaptive Radar Detection in Homogeneous plus Structured Interference-Part I: On the Maximal Invariant Statistic|journal = IEEE Transactions on Signal Processing|date = 2016|volume = PP|issue = 99|pages = 1-1|doi = 10.1109/TSP.2016.2519003|arxiv = 1507.05263|bibcode = 2016ITSP...64.2894C}}</ref><ref>{{cite journal|last1 = Ciuonzo|first1 = D.|last2 = De Maio|first2 = A.|last3 = Orlando|first3 = D.|title = A Unifying Framework for Adaptive Radar Detection in Homogeneous plus Structured Interference-Part II: Detectors Design|journal = IEEE Transactions on Signal Processing|date = 2016|volume = PP|issue = 99|pages = 1-1|doi = 10.1109/TSP.2016.2519005|arxiv = 1507.05266|bibcode = 2016ITSP...64.2907C}}</ref>\n\n==Other uses==\nIn [[mathematical statistics]], [[growth curve (biology)|growth curves such as those used in biology]] are often modeled as being [[continuous stochastic process|continuous]] [[stochastic process]]es, e.g. as being [[Sample-continuous process|sample paths]] that [[almost surely]] solve  [[stochastic differential equation]]s.<ref>\n{{cite book\n|author1=Seber, G. A. F.  |author2=Wild, C. J.\n|chapter=\"Growth models (Chapter 7)\"\n|pages=325–367\n|title=Nonlinear regression\n|series=Wiley Series in Probability and Mathematical Statistics: Probability and Mathematical Statistics\n|publisher=John Wiley & Sons, Inc.\n|location=New York\n|year=1989\n|isbn=0-471-61760-1\n}}\n</ref>  Growth curves have been also applied in forecasting market development.<ref>{{Cite journal|last=Meade|first=Nigel|year=1984|title=The use of growth curves in forecasting market development—a review and appraisal|url=|journal=Journal of Forecasting|volume=3|pages=429–451|via=|doi=10.1002/for.3980030406}}</ref>\n\n==Footnotes==\n{{reflist}}\n\n==References==\n* {{cite book\n|title=Nonlinear Models for Repeated Measurement Data\n|first=Marie\n|last=Davidian | authorlink = Marie Davidian\n|author2=David M. Giltinan\n|series=Chapman & Hall/CRC Monographs on Statistics & Applied Probability\n|isbn=978-0-412-98341-2\n|year=1995\n}}\n*{{cite book\n|author1=Kshirsagar, Anant M.  |author2=Smith, William Boyce\n|title=Growth curves\n|series=Statistics: Textbooks and Monographs\n|volume=145\n|publisher=Marcel Dekker, Inc.\n|location=New York\n|year=1995\n|isbn=0-8247-9341-2\n}}\n*{{cite book\n|last=Pan\n|first=Jianxin\n|author2=Fang, Kaitai\n|title=Growth curve models and statistical diagnostics\n|series=Mathematical Monograph Series\n|volume=8\n|publisher=Science Press\n|location=Beijing\n|year=2007\n|isbn=9780387950532\n}}\n\n*{{cite book\n|author=Timm, Neil H.\n|chapter=\"The general MANOVA model (GMANOVA)\" (Chapter 3.6.d)\n|title=Applied multivariate analysis\n|series=Springer Texts in Statistics\n|publisher=Springer-Verlag\n|location=New York\n|year=2002\n|isbn=0-387-95347-7\n}}\n\n*{{cite book  |author1=Vonesh, Edward F.  |author2=Chinchilli, Vernon G. | title = [[Linear model|Linear]] and [[Nonlinear regression|Nonlinear]] Models for the Analysis of Repeated Measurements| publisher = London: Chapman and Hall | year = 1997\n}}\n{{Least Squares and Regression Analysis}}\n\n[[Category:Analysis of variance]]\n[[Category:Statistical forecasting]]\n[[Category:Multivariate time series]]\n[[Category:Ordinary differential equations]]\n[[Category:Exponentials]]\n[[Category:Biostatistics]]\n[[Category:Growth curves]]"
    },
    {
      "title": "Half time (physics)",
      "url": "https://en.wikipedia.org/wiki/Half_time_%28physics%29",
      "text": "{{short description| The time take for a quantity to decrease by half in an exponential decay process, also half-life. }}\n{{other uses|Half time (disambiguation)}}\n[[File:Half times.svg|right|200px]]\nThe ''''half time'''' is the time taken by a quantity to reach one half of its extremal value, where the rate of change is proportional to the difference between the present value and the extremal value (i.e. in [[exponential decay]] processes). It is synonymous with [[half-life]], but used in slightly different contexts.\n\nThe diagram shows the increase in the quantity (red) in response to a step-change in the motive force that changes it (blue). The time-axis is in multiples of the half time. It can be seen that the quantity increases to one-half of its final value after one half time, to three-quarters after two half times, to seven-eighths after three half times, and so on.\n\nThe relationship between the quantity (''Q'') and time (''t'') is described by the mathematical formula:\n\n: <math> Q(t) = Q_\\mathrm{f}(1 - e^{-\\lambda t}) </math>\n\nwhere ''Q''<sub>f</sub> is the extremal value and ''&lambda;'' is a constant, approximately equal to 0.69 divided by the half time – more precisely: log<sub>e</sub>(2) / (half time).\n\nWhere the quantity decreases in response to a step-decrease in the motive force that changes it, the curve is mirrored in the time-axis and may be referred to as [[exponential decay]].\n\nThe concept of half time is used in [[Underwater diving|diving]] physiology where [[Tissue (biology)|body tissues]] take up and release [[inert gases]] (usually [[nitrogen]]) following changes in depth. Different tissue types have different half times for a given inert gas, and modelling the uptake and release of gases by the tissues is important to avoid [[decompression sickness]].\n\n==Examples==\n*[[File:RC charging.svg|right|thumb|The voltage (v) on the capacitor (C) changes with time as the capacitor is charged or discharged via the resistor (R)]]In electronics, when a [[capacitor]] is charged or discharged via a [[resistor]], the voltage on the capacitor follows the above formula, with the half time approximately equal to 0.69 times the [[time constant]], which is equal to the product of the resistance and the capacitance.\n*The first models of nitrogen uptake and release in the body of a diver used five parallel compartments with half times from 5 minutes to 75 minutes.<ref name=\"BE2003-439\">{{cite book |title=Bennett and Elliott's physiology and medicine of diving, 5th Revised edition |author1=Tikuisis, Peter |author2=Gerth, Wayne A. |editor1=Brubakk, Alf O |editor2=Neuman, Tom S |year=2003 |publisher=Saunders Ltd |location=United States |chapter=10.1: Decompression Theory |page=439 |isbn=0-7020-2571-2 |oclc=51607923}}</ref><ref name=Haldane1908>{{cite journal |author=Boycott A. E., Damant G. C. C., [[John Scott Haldane|Haldane John Scott]] |title=Prevention of compressed air illness |journal=Journal of Hygiene |volume=8 |issue=3 |pages=342–443 |year=1908 |url=http://archive.rubicon-foundation.org/7489 |pmid=20474365 |accessdate=2009-06-15 |pmc=2167126 |doi=10.1017/S0022172400003399 }}</ref> Later models refined this by considering more compartments and a wider range of half times. The U.S. Navy tables used six compartments with half times of 5, 10, 20, 40, 80 and 120 minutes.<ref name=\"DiD2005-215\">{{cite book |author1=Lippmann, John |author2=[[Simon Mitchell|Mitchell, Simon J]]  |title=Deeper into Diving |publisher=J.L. Publications |location=Victoria, Australia |date=October 2005 |edition=2 |page=215 |chapter=15 |isbn=0-9752290-1-X |oclc=66524750 }}</ref><ref name=NEDU57>{{cite journal |author=des Granges, M |title=Standard Air Decompression Table |journal=[[United States Navy Experimental Diving Unit]] Technical Report |volume=NEDU-RR-5-57 |year=1957 |url=http://archive.rubicon-foundation.org/3331 |accessdate=2009-06-15 }}</ref> The [[Bühlmann tables]] use twelve of the sixteen compartments in the ZH-L<sub>16</sub> algorithm, which uses half times from 4 to 635 minutes.<ref name=\"DiD2005-226\">{{cite book |author1=Lippmann, John |author2=Mitchell, Simon |title=Deeper into Diving |publisher=J.L. Publications |location=Victoria, Australia |date=October 2005 |edition=2 |page=226 |chapter=17 |isbn=0-9752290-1-X |oclc=66524750 }}</ref><ref name=decoDCS>{{cite book |title=Decompression-Decompression Sickness |author=[[Albert A. Bühlmann|Bühlmann, Albert A.]] |year=1984 |publisher=Springer-Verlag |location=Berlin New York |isbn=0-387-13308-9 }}</ref>\n\n==References==\n{{reflist}}\n\n==See also==\n*[[Half-life]]\n*[[Dive tables]]\n*[[Bühlmann tables]]\n*[[Dive computer]]\n\n[[Category:Exponentials]]"
    },
    {
      "title": "Half-life",
      "url": "https://en.wikipedia.org/wiki/Half-life",
      "text": "{{About|the scientific and mathematical concept|the video game|Half-Life (video game)|other uses|Half-Life (disambiguation)}}\n\n{| class=\"wikitable\" align=right\n! Number of<br />half-lives<br />elapsed !! Fraction<br />remaining !! colspan=2| Percentage<br />remaining\n|-\n| 0 || <sup>1</sup>⁄<sub>1</sub> ||align=right style=\"border-right-width: 0; padding-right:0\"| 100||style=\"border-left-width: 0\"|\n|-\n| 1 || <sup>1</sup>⁄<sub>2</sub> ||align=right style=\"border-right-width: 0; padding-right:0\"| 50||style=\"border-left-width: 0\"|\n|-\n| 2 || <sup>1</sup>⁄<sub>4</sub> ||align=right style=\"border-right-width: 0; padding-right:0\"| 25||style=\"border-left-width: 0\"|\n|-\n| 3 || <sup>1</sup>⁄<sub>8</sub> ||align=right style=\"padding-right:0; border-right-width: 0\"| 12||style=\"border-left-width: 0; padding-left:0\"|.5\n|-\n| 4 || <sup>1</sup>⁄<sub>16</sub> ||align=right style=\"border-right-width: 0; padding-right:0\"| 6||style=\"border-left-width: 0; padding-left:0\"|.25\n|-\n| 5 || <sup>1</sup>⁄<sub>32</sub> || align=right style=\"border-right-width: 0; padding-right:0\"|3||style=\"border-left-width: 0; padding-left:0\"|.125\n|-\n| 6 || <sup>1</sup>⁄<sub>64</sub> || align=right style=\"border-right-width: 0; padding-right:0\"|1||style=\"border-left-width: 0; padding-left:0\"|.563\n|-\n| 7 || <sup>1</sup>⁄<sub>128</sub> ||align=right style=\"border-right-width: 0; padding-right:0\"| 0||style=\"border-left-width: 0; padding-left:0\"|.781\n|-\n| ... || ... ||colspan=2| ...\n|-\n| ''n'' ||<sup>1</sup>/<sub>2<sup>''n''</sup></sub> || colspan=2|<sup>100</sup>/<sub>2<sup>''n''</sup></sub>\n|}\n\n'''Half-life''' (symbol '''''t''<sub>1⁄2</sub>''') is the time required for a quantity to reduce to half of its initial value. The term is commonly used in [[nuclear physics]] to describe how quickly unstable atoms undergo, or how long stable atoms survive, [[radioactive decay]]. The term is also used more generally to characterize any type of [[exponential decay|exponential]] or [[rate law|non-exponential]] decay. For example, the medical sciences refer to the [[biological half-life]] of drugs and other chemicals in the human body. The converse of half-life is [[doubling time]].\n\nThe original term, ''half-life period'', dating to [[Ernest Rutherford]]'s discovery of the principle in 1907, was shortened to ''half-life'' in the early 1950s.<ref>John Ayto, ''20th Century Words'' (1989), Cambridge University Press.</ref> Rutherford applied the principle of a radioactive [[chemical element|element's]] half-life to studies of age determination of rocks by measuring the decay period of [[radium]] to [[lead-206]].\n\nHalf-life is constant over the lifetime of an exponentially decaying quantity, and it is a [[characteristic unit]] for the exponential decay equation. The accompanying table shows the reduction of a quantity as a function of the number of half-lives elapsed.\n\n== Probabilistic nature ==\n[[File:Halflife-sim.gif|thumb|right|Simulation of many identical atoms undergoing radioactive decay, starting with either 4 atoms per box (left) or 400 (right). The number at the top is how many half-lives have elapsed. Note the consequence of the [[law of large numbers]]: with more atoms, the overall decay is more regular and more predictable.]]\n\nA half-life usually describes the decay of discrete entities, such as radioactive atoms. In that case, it does not work to use the definition that states \"half-life is the time required for exactly half of the entities to decay\". For example, if there is just one radioactive atom, and its half-life is one second, there will ''not'' be \"half of an atom\" left after one second.\n\nInstead, the half-life is defined in terms of [[probability]]: \"Half-life is the time required for exactly half of the entities to decay ''[[expected value|on average]]''\". In other words, the ''probability'' of a radioactive atom decaying within its half-life is 50%.\n\nFor example, the image on the right is a simulation of many identical atoms undergoing radioactive decay. Note that after one half-life there are not ''exactly'' one-half of the atoms remaining, only ''approximately'', because of the random variation in the process. Nevertheless, when there are many identical atoms decaying (right boxes), the [[law of large numbers]] suggests that it is a ''very good approximation'' to say that half of the atoms remain after one half-life.\n\nThere are various simple exercises that demonstrate probabilistic decay, for example involving flipping coins or running a statistical [[computer program]].<ref>{{cite web\n | url=http://www.madsci.org/posts/archives/Mar2003/1047912974.Ph.r.html | title=Re: What happens durring half lifes &#91;sic&#93; when there is only one atom left?|publisher=MADSCI.org|author=Chivers, Sidney | date=March 16, 2003}}\n</ref><ref>\n{{cite web | url=http://www.exploratorium.edu/snacks/radioactive_decay/index.html | title=Radioactive-Decay Model|publisher=Exploratorium.edu | accessdate=2012-04-25}}\n</ref><ref>\n{{cite web| url=http://astro.gmu.edu/classes/c80196/hw2.html| title=Assignment #2: Data, Simulations, and Analytic Science in Decay| publisher=Astro.GLU.edu| date=September 1996| author=Wallin, John| deadurl=bot: unknown| archiveurl=https://web.archive.org/web/20110929005007/http://astro.gmu.edu/classes/c80196/hw2.html| archivedate=2011-09-29| df=}}</ref>\n\n== Formulas for half-life in exponential decay ==\n{{Main|Exponential decay}}\n\nAn exponential decay  can be described by any of the following three equivalent formulas:\n{{block indent|<math>\\begin{align}\n  N(t) &= N_0 \\left(\\frac {1}{2}\\right)^{\\frac{t}{t_{1/2}}} \\\\\n  N(t) &= N_0 e^{-\\frac{t}{\\tau}} \\\\\n  N(t) &= N_0 e^{-\\lambda t}\n\\end{align}</math>}}\nwhere\n* ''N''<sub>0</sub> is the initial quantity of the substance that will decay (this quantity may be measured in grams, moles, number of atoms, etc.),\n* ''N''(''t'') is the quantity that still remains and has not yet decayed after a time ''t'',\n* {{math|''t''<sub>1⁄2</sub>}} is the half-life of the decaying quantity,\n* {{mvar|τ}} is a [[positive number]] called the [[mean lifetime]] of the decaying quantity,\n* {{mvar|λ}} is a positive number called the [[decay constant]] of the decaying quantity.\n\nThe three parameters {{math|''t''<sub>1⁄2</sub>}}, {{mvar|τ}}, and {{mvar|λ}}  are all directly related in the following way:\n{{block indent|<math>t_{1/2} = \\frac{\\ln (2)}{\\lambda} = \\tau \\ln(2)</math>}}\nwhere ln(2) is the [[natural logarithm]] of 2 (approximately 0.693).\n\n=== Decay by two or more processes ===\nSome quantities decay by two exponential-decay processes simultaneously. In this case, the actual half-life {{math|''T''<sub>1⁄2</sub>}} can be related to the half-lives ''t''<sub>1</sub> and ''t''<sub>2</sub> that the quantity would have if each of the decay processes acted in isolation:\n:<math>\\frac{1}{T_{1/2}} = \\frac{1}{t_1} + \\frac{1}{t_2}</math>\n\nFor three or more processes, the analogous formula is:\n:<math>\\frac{1}{T_{1/2}} = \\frac{1}{t_1} + \\frac{1}{t_2} + \\frac{1}{t_3} + \\cdots</math>\nFor a proof of these formulas, see [[Exponential decay#Decay by two or more processes|Exponential decay § Decay by two or more processes]].\n\n===Examples===\n[[File:Dice_half-life_decay.jpg|thumb|Half life demonstrated using dice in a [[v:Physics and Astronomy Labs/Radioactive decay with dice|classroom experiment]]]]\n{{further|Exponential decay#Applications and examples}}\nThere is a half-life describing any exponential-decay process. For example:\n*  As noted above, in [[radioactive decay]] the half-life is the length of time after which there is a 50% chance that an atom will have undergone [[Atomic nucleus|nuclear]] decay. It varies depending on the atom type and [[isotope]], and is usually determined experimentally. See [[List of nuclides]].\n*  The current flowing through an [[RC circuit]] or [[RL circuit]] decays with a half-life of ln(2)''RC'' or ln(2)''L/R'', respectively. For this example the term [[half time (physics)|half time]] tends to be used, rather than \"half life\", but they mean the same thing.\n*  In a [[chemical reaction]], the half-life of a species is the time it takes for the concentration of that substance to fall to half of its initial value.  In a first-order reaction the half-life of the reactant is ln(2)/{{mvar|λ}}, where {{mvar|λ}} is the [[reaction rate constant]].\n\n==In non-exponential decay==\nThe term \"half-life\" is almost exclusively used for decay processes that are exponential (such as radioactive decay or the other examples above), or approximately exponential (such as [[biological half-life]] discussed below). In a decay process that is not even close to exponential, the half-life will change dramatically while the decay is happening. In this situation it is generally uncommon to talk about half-life in the first place, but sometimes people will describe the decay in terms of its \"first half-life\", \"second half-life\", etc., where the first half-life is defined as the time required for decay from the initial value to 50%, the second half-life is from 50% to 25%, and so on.<ref>{{cite book|title=Chemistry for the Biosciences: The Essential Concepts |authors=Jonathan Crowe, Tony Bradshaw |page=568 |url=https://books.google.com/books?id=VxMNBAAAQBAJ&pg=PA568|isbn=9780199662883 |year=2014 }}</ref>\n\n== In biology and pharmacology ==\n{{also|Biological half-life}}\nA biological half-life or elimination half-life is the time it takes for a substance (drug, radioactive nuclide, or other) to lose one-half of its pharmacologic, physiologic, or radiological activity. In a medical context, the half-life may also describe the time that it takes for the concentration of a substance in [[blood plasma]] to reach one-half of its steady-state value (the \"plasma half-life\").\n\nThe relationship between the biological and plasma half-lives of a substance can be complex, due to factors including accumulation in [[tissue (biology)|tissues]], active [[metabolite]]s, and [[Receptor (biochemistry)|receptor]] interactions.<ref name=\"SCM\">{{cite book|title=Spinal cord medicine|author1=Lin VW|author2=Cardenas DD|publisher=Demos Medical Publishing, LLC|page=251|url=https://books.google.com/books?id=3anl3G4No_oC&pg=PA251&lpg=PA251|year=2003|isbn=978-1-888799-61-3}}</ref>\n\nWhile a radioactive isotope decays almost perfectly according to so-called \"first order kinetics\" where the rate constant is a fixed number, the elimination of a substance from a living organism usually follows more complex chemical kinetics.\n\nFor example, the biological half-life of water in a human being is about 9 to 10 days,<ref>{{cite book|last1=Pang|first1=Xiao-Feng|title=Water: Molecular Structure and Properties|date=2014|publisher=World Scientific|location=New Jersey|isbn=9789814440424|page=451}}</ref> though this can be altered by behavior and various other conditions. The biological half-life of [[caesium]] in human beings is between one and four months.\n\nThe concept of a half-life has also been utilized for [[pesticides]] in [[plants]],<ref name=tebuau>{{cite web|last1=Australian Pesticides and Veterinary Medicines Authority|title=Tebufenozide in the product Mimic 700 WP Insecticide, Mimic 240 SC Insecticide|url=https://apvma.gov.au/node/14051|publisher=Australian Government|accessdate=30 April 2018|language=en|date=31 March 2015}}</ref> and certain authors maintain that [[Environmental impact of pesticides|pesticide risk and impact assessment models]] rely on and are sensitive to information describing dissipation from plants.<ref name=acs>{{cite journal|last1=Fantke|first1=Peter|last2=Gillespie|first2=Brenda W.|last3=Juraske|first3=Ronnie|last4=Jolliet|first4=Olivier|title=Estimating Half-Lives for Pesticide Dissipation from Plants|journal=Environmental Science & Technology|date=11 July 2014|volume=48|issue=15|pages=8588–8602|doi=10.1021/es500434p|pmid=24968074|bibcode=2014EnST...48.8588F}}</ref>\n\n== See also ==\n* [[Half time (physics)]]\n* [[List of isotopes by half-life]]\n* [[Mean lifetime]]\n\n== References ==\n{{reflist}}\n\n== External links ==\n{{Wiktionary|half-life}}\n{{commons category|Half times}}\n* [http://www.nucleonica.net Nucleonica.net], Nuclear Science Portal\n* [http://www.nucleonica.net/wiki/index.php/Help:Decay_Engine Nucleonica.net], wiki: Decay Engine\n* [https://web.archive.org/web/20060617205436/http://www.facstaff.bucknell.edu/mastascu/elessonshtml/SysDyn/SysDyn3TCBasic.htm Bucknell.edu], System Dynamics – Time Constants\n* [https://www.nikhef.nl/en/news/researchers-nikhef-and-uva-measure-slowest-radioactive-decay-ever/] Researchers Nikhef and UvA measure slowest radioactive decay ever: Xe-124 with 18 billion trillion years.\n* [http://www.subotex.com/SuboxoneTaperChart.aspx Subotex.com], Half-Life elimination of drugs in blood plasma – Simple Charting Tool\n{{Radiation}}\n\n\n{{Authority control}}\n\n{{DEFAULTSORT:Half-Life}}\n[[Category:Radioactivity]]\n[[Category:Exponentials]]\n[[Category:Chemical kinetics]]"
    },
    {
      "title": "Hyperbolic function",
      "url": "https://en.wikipedia.org/wiki/Hyperbolic_function",
      "text": "{{Redirect|Hyperbolic curve|the geometric curve|Hyperbola}}\n{{Anchor|Sinh|Cosh|Tanh|Sech|Csch|Coth}}\n[[File:sinh cosh tanh.svg|300px|thumb]]\n\nIn [[mathematics]], '''hyperbolic functions''' are analogs of the ordinary [[trigonometric function|trigonometric]], or [[unit circle|circular]], functions.\n\nThe basic hyperbolic functions are:\n* '''hyperbolic sine''' \"sinh\" ({{IPAc-en|s|ɪ|n|tʃ|,_|ʃ|aɪ|n}}),<ref>(1999) ''Collins Concise Dictionary'', 4th edition, HarperCollins, Glasgow, {{ISBN|0 00 472257 4}}, p. 1386</ref>\n* '''hyperbolic cosine''' \"cosh\" ({{IPAc-en|k|ɒ|ʃ|,_|k|oʊ|ʃ}}),<ref name=\"Collins Concise Dictionary p. 328\">''Collins Concise Dictionary'', p. 328</ref>\nfrom which are derived:\n* '''hyperbolic tangent''' \"tanh\" ({{IPAc-en|t|æ|n|tʃ|,_|θ|æ|n}}),<ref>''Collins Concise Dictionary'', p. 1520</ref>\n* '''hyperbolic cosecant''' \"csch\" or \"cosech\" ({{IPAc-en|ˈ|k|oʊ|ʃ|ɛ|k}}<ref name=\"Collins Concise Dictionary p. 328\"/> or {{IPAc-en|ˈ|k|oʊ|s|ɛ|tʃ}})\n* '''hyperbolic secant''' \"sech\" ({{IPAc-en|ʃ|ɛ|k|,_|s|ɛ|tʃ}}),<ref>''Collins Concise Dictionary'', p. 1340</ref>\n* '''hyperbolic cotangent''' \"coth\" ({{IPAc-en|k|oʊ|θ|,_|k|ɒ|θ}}),<ref>''Collins Concise Dictionary'', p. 329</ref><ref>[http://www.mathcentre.ac.uk/resources/workbooks/mathcentre/hyperbolicfunctions.pdf tanh]</ref> \ncorresponding to the derived trigonometric functions.\n\nThe [[inverse hyperbolic function]]s are:\n* '''area hyperbolic sine''' \"arsinh\" (also denoted \"sinh<sup>−1</sup>\", \"asinh\" or sometimes \"arcsinh\")<ref>{{Citation | last=Woodhouse | first = N. M. J. | author-link = N. M. J. Woodhouse | title = Special Relativity | publisher = Springer | place = London | date = 2003 | page = 71 | isbn = 978-1-85233-426-0}}</ref><ref>{{Citation | editor1-last=Abramowitz | editor1-first=Milton | editor1-link=Milton Abramowitz | editor2-last=Stegun | editor2-first=Irene A. | editor2-link=Irene Stegun | title=Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables | publisher=[[Dover Publications]] | location=New York | isbn=978-0-486-61272-0 | year=1972| title-link=Abramowitz and Stegun }}</ref><ref>[https://www.google.com/books?q=arcsinh+-library Some examples of using '''arcsinh'''] found in [[Google Books]].</ref>\n* and so on.\n\n[[Image:Hyperbolic functions-2.svg|thumb|300px|right|A ray through the [[unit hyperbola]] {{math|''x''<sup>2</sup> − ''y''<sup>2</sup> {{=}} 1}} in the point {{math|(cosh ''a'', sinh ''a'')}}, where {{mvar|a}} is twice the area between the ray, the hyperbola, and the {{mvar|x}}-axis. For points on the hyperbola below the {{mvar|x}}-axis, the area is considered negative (see [[:Image:HyperbolicAnimation.gif|animated version]] with comparison with the trigonometric (circular) functions).]]\n\nJust as the points {{math|(cos ''t'', sin ''t'')}} form a circle with a unit radius, the points {{math|(cosh ''t'', sinh ''t'')}} form the right half of the equilateral [[hyperbola]].  The hyperbolic functions take a [[Real number|real argument]] called a [[hyperbolic angle]].  The size of a hyperbolic angle is twice the area of its [[hyperbolic sector]]. The hyperbolic functions may be defined in terms of the [[hyperbolic sector#Hyperbolic triangle|legs of a right triangle]] covering this sector.\n\nHyperbolic functions occur in the solutions of many linear [[differential equation]]s (for example, the equation defining a [[catenary]]), of some [[Cubic function#Hyperbolic solution for one real root|cubic equations]], in calculations of angles and distances  in [[hyperbolic geometry]], and of [[Laplace's equation]] in [[Cartesian coordinates]]. [[Laplace's equation]]s are important in many areas of [[physics]], including [[electromagnetic theory]], [[heat transfer]], [[fluid dynamics]], and [[special relativity]].\n\nIn [[complex analysis]], the hyperbolic functions arise as the imaginary parts of sine and cosine. The hyperbolic sine and the hyperbolic cosine are [[entire function]]s. As a result, the other hyperbolic functions are [[meromorphic function|meromorphic]] in the whole complex plane.\n\nBy [[Lindemann–Weierstrass theorem]], the hyperbolic functions have a [[transcendental number|transcendental value]] for every non-zero [[algebraic number|algebraic value]] of the argument.<ref>{{Cite journal | jstor=10.4169/j.ctt5hh8zn| doi=10.4169/j.ctt5hh8zn| title=Irrational Numbers| volume=11| last1=Niven| first1=Ivan| year=1985| publisher=Mathematical Association of America| doi-broken-date=2019-02-22}}</ref>\n\nHyperbolic functions were introduced in the 1760s independently by [[Vincenzo Riccati]] and [[Johann Heinrich Lambert]].<ref>Robert E. Bradley, Lawrence A. D'Antonio, Charles Edward Sandifer. ''Euler at 300: an appreciation.'' Mathematical Association of America, 2007. Page 100.</ref> Riccati used {{math|''Sc.''}} and {{math|''Cc.''}} (''sinus/cosinus circulare'') to refer to circular functions and {{math|''Sh.''}} and {{math|''Ch.''}} (''sinus/cosinus hyperbolico'') to refer to hyperbolic functions. Lambert adopted the names but altered the abbreviations to what they are today.<ref>Georg F. Becker. ''Hyperbolic functions.'' Read Books, 1931. Page xlviii.</ref> The abbreviations {{math|sh}}, {{math|ch}}, {{math|th}}, {{math|cth}} are also at disposition, their use depending more on personal preference of mathematics of influence than on the local language.\n\n==Definitions==\n[[File:sinh cosh tanh.svg|thumb|<span style=\"color:#b30000;\">sinh</span>, <span style=\"color:#00b300;\">cosh</span> and <span style=\"color:#0000b3;\">tanh</span>]]\n[[File:csch sech coth.svg|thumb|<span style=\"color:#b30000;\">csch</span>, <span style=\"color:#00b300;\">sech</span> and <span style=\"color:#0000b3;\">coth</span>]]\n\nThere are various equivalent ways for defining the hyperbolic functions. \n\n=== Exponential definitions ===\n[[File:Hyperbolic and exponential; sinh.svg|thumb|right|{{math|sinh ''x''}} is half the [[Subtraction|difference]] of {{math|''e<sup>x</sup>''}} and {{math|''e''<sup>−''x''</sup>}}]]\n[[File:Hyperbolic and exponential; cosh.svg|thumb|right|{{math|cosh ''x''}} is the [[Arithmetic mean|average]] of {{math|''e<sup>x</sup>''}} and {{math|''e''<sup>−''x''</sup>}}]]\n\nThey may be defined in terms of the [[exponential function]]:\n\n* Hyperbolic sine: the [[odd part of a function|odd part]] of the exponential function, that is\n*:<math>\\sinh x = \\frac {e^x - e^{-x}} {2} = \\frac {e^{2x} - 1} {2e^x} = \\frac {1 - e^{-2x}} {2e^{-x}}.</math>\n* Hyperbolic cosine: the [[even part of a function|even part]] of the exponential function, that is\n*:<math>\\cosh x = \\frac {e^x + e^{-x}} {2} = \\frac {e^{2x} + 1} {2e^x} = \\frac {1 + e^{-2x}} {2e^{-x}}.</math>\n* Hyperbolic tangent:\n*:<math>\\tanh x = \\frac{\\sinh x}{\\cosh x} = \\frac {e^x - e^{-x}} {e^x + e^{-x}}\n  = \\frac{e^{2x} - 1} {e^{2x} + 1}</math>\n* Hyperbolic cotangent: for {{math|''x'' ≠ 0}},\n*:<math>\\coth x = \\frac{\\cosh x}{\\sinh x} = \\frac {e^x + e^{-x}} {e^x - e^{-x}}\n  = \\frac{e^{2x} + 1} {e^{2x} - 1}</math>\n* Hyperbolic secant:\n*:<math>\\operatorname{sech} x = \\frac{1}{\\cosh x} = \\frac {2} {e^x + e^{-x}}\n  = \\frac{2e^x} {e^{2x} + 1}</math>\n* Hyperbolic cosecant:  for {{math|''x'' ≠ 0}},\n*:<math>\\operatorname{csch} x = \\frac{1}{\\sinh x} = \\frac {2} {e^x - e^{-x}}\n  = \\frac{2e^x} {e^{2x} - 1}</math>\n\n=== Differential definitions ===\n\nThe hyperbolic functions may be defined as solutions of [[differential equation]]s: The hyperbolic sine and cosine are the unique solution {{math|(''s'', ''c'')}} of the system\n:<math>\\begin{align}\nc'(x)&=s(x)\\\\\ns'(x)&=c(x)\\end{align}</math>\nsuch that \n{{math|1=''s''(0) = 0}} and {{math|1=''c''(0) = 1}}.\n\nThey are also the unique solution of the equation {{math|1=''f''&thinsp;″(''x'') = ''f''&thinsp;(''x'')}},\nsuch that {{math|1=''f''&thinsp;(0) = 1}}, {{math|1=''f''&thinsp;′(0) = 0}} for the hyperbolic cosine, and {{math|1=''f''&thinsp;(0) = 0}}, {{math|1=''f''&thinsp;′(0) = 1}} for the hyperbolic sine.\n\n=== Trigonometric definitions ===\n\nHyperbolic functions may also be deduced from [[trigonometric function]]s with [[complex number|complex]] arguments:\n\n* Hyperbolic sine:\n*:<math>\\sinh x = -i \\sin (i x)</math>\n* Hyperbolic cosine:\n*:<math>\\cosh x = \\cos (i x)</math>\n* Hyperbolic tangent:\n*:<math>\\tanh x = -i \\tan (i x)</math>\n* Hyperbolic cotangent:\n*:<math>\\coth x = i \\cot (i x)</math>\n* Hyperbolic secant:\n*:<math>\\operatorname{sech} x = \\sec (i x)</math>\n* Hyperbolic cosecant:\n*:<math>\\operatorname{csch} x = i \\csc (i x)</math>\nwhere {{mvar|i}} is the [[imaginary unit]] with the property that {{math|1=''i''<sup>2</sup> = −1}}.\n\nThe [[complex number|complex]] forms in the definitions above derive from [[Euler's formula]].\n\n== Characterizing properties==\n\n=== Hyperbolic cosine ===\n\nIt can be shown that the area under the curve of the hyperbolic cosine over a finite interval is always equal to the arc length corresponding to that interval:<ref>{{cite book\n |title=Golden Integral Calculus\n |first1=Bali  |last1=N.P.\n |publisher=Firewall Media\n |year=2005\n |isbn=81-7008-169-6\n |page=472\n |url=https://books.google.com/books?id=hfi2bn2Ly4cC&pg=PA472\n}}</ref>\n:<math>\\text{area} = \\int_a^b \\cosh x \\,dx = \\int_a^b\\sqrt{1 + \\left(\\frac{d}{dx} \\cosh x \\right)^2} \\,dx = \\text{arc length.}</math>\n\n{{Anchor|Tanh}}\n\n===Hyperbolic tangent ===\n\nThe hyperbolic tangent is the solution to the [[differential equation]] {{math|1=''f''&thinsp;′ = 1 − ''f''&thinsp;<sup>2</sup>}} with {{math|1=''f''&thinsp;(0) = 0}} and the [[nonlinear]] [[boundary value problem]]:<ref>{{MathWorld | file = HyperbolicTangent | title = Hyperbolic Tangent}}</ref><ref>\n{{cite web|title=Derivation of tanh solution to {{sfrac|1|2}}''f''″ = ''f''<sup>3</sup> − ''f''|url=http://math.stackexchange.com/q/1670143/88985|website=Math [[StackExchange]]|accessdate=18 March 2016}}</ref>\n\n:<math>\\tfrac{1}{2} f'' = f^3 - f ; \\quad f(0) = f'(\\infty) = 0.</math>\n\n==Useful relations==\nOdd and even functions:\n:<math>\\begin{align}\n  \\sinh (-x) &= -\\sinh x \\\\\n  \\cosh (-x) &=  \\cosh x\n\\end{align}</math>\n\nHence:\n:<math>\\begin{align}\n                \\tanh (-x) &= -\\tanh x \\\\\n                \\coth (-x) &= -\\coth x \\\\\n  \\operatorname{sech} (-x) &=  \\operatorname{sech} x \\\\\n  \\operatorname{csch} (-x) &= -\\operatorname{csch} x\n\\end{align}</math>\n\nIt can be seen that {{math|cosh ''x''}} and {{math|sech ''x''}} are [[even function]]s; the others are [[odd functions]].\n\n:<math>\\begin{align}\n  \\operatorname{arsech} x &= \\operatorname{arcosh} \\left(\\frac{1}{x}\\right) \\\\\n  \\operatorname{arcsch} x &= \\operatorname{arsinh} \\left(\\frac{1}{x}\\right) \\\\\n  \\operatorname{arcoth} x &= \\operatorname{artanh} \\left(\\frac{1}{x}\\right)\n\\end{align}</math>\n\nHyperbolic sine and cosine satisfy: \n:<math>\\begin{align}\n  \\cosh x + \\sinh x &= e^x \\\\\n  \\cosh x - \\sinh x &= e^{-x} \\\\\n  \\cosh^2 x - \\sinh^2 x &= 1\n\\end{align}</math>\n\nthe last of which is similar to the [[Pythagorean trigonometric identity]].\n\nOne also has\n:<math>\\begin{align}\n  \\operatorname{sech} ^{2} x &= 1 - \\tanh^{2} x \\\\\n  \\operatorname{csch} ^{2} x &= \\coth^{2} x - 1\n\\end{align}</math>\n\nfor the other functions.\n\n===Sums of arguments===\n:<math>\\begin{align}\n  \\sinh(x + y) &= \\sinh x \\cosh y + \\cosh x \\sinh y \\\\\n  \\cosh(x + y) &= \\cosh x \\cosh y + \\sinh x \\sinh y \\\\[6px]\n  \\tanh(x + y) &= \\frac{\\tanh x +\\tanh y}{1+ \\tanh x \\tanh y } \\\\\n\\end{align}</math>\n\nparticularly\n:<math>\\begin{align}\n  \\cosh (2x) &= \\sinh^2{x} + \\cosh^2{x} = 2\\sinh^2 x + 1 = 2\\cosh^2 x - 1\\\\\n  \\sinh (2x) &= 2\\sinh x \\cosh x \\\\\n  \\tanh (2x) &= \\frac{2\\tanh x}{1+ \\tanh^2 x } \\\\\n\\end{align}</math>\n\nAlso:\n:<math>\\begin{align}\n  \\sinh x + \\sinh y &= 2 \\sinh \\left(\\frac{x+y}{2}\\right) \\cosh \\left(\\frac{x-y}{2}\\right)\\\\\n  \\cosh x + \\cosh y &= 2 \\cosh \\left(\\frac{x+y}{2}\\right) \\cosh \\left(\\frac{x-y}{2}\\right)\\\\\n\n\\end{align}</math>\n\n===Subtraction formulas===\n:<math>\\begin{align}\n  \\sinh(x - y) &= \\sinh x \\cosh y - \\cosh x \\sinh y \\\\\n  \\cosh(x - y) &= \\cosh x \\cosh y - \\sinh x \\sinh y \\\\\n  \\tanh(x - y) &= \\frac{\\tanh x -\\tanh y}{1- \\tanh x \\tanh y } \\\\\n\\end{align}</math>\n\nAlso:<ref>{{cite book|last1=Martin|first1=George E.|title=The foundations of geometry and the non-euclidean plane|date=1986|publisher=Springer-Verlag|location=New York|isbn=3-540-90694-0|page=416|edition=1st corr.}}</ref>\n:<math>\\begin{align}\n  \\sinh x - \\sinh y &= 2 \\cosh \\left(\\frac{x+y}{2}\\right) \\sinh \\left(\\frac{x-y}{2}\\right)\\\\\n  \\cosh x - \\cosh y &= 2 \\sinh \\left(\\frac{x+y}{2}\\right) \\sinh \\left(\\frac{x-y}{2}\\right)\\\\\n\\end{align}</math>\n\n===Half argument formulas===\n\n:<math>\\begin{align}\n  \\sinh\\left(\\frac{x}{2}\\right) &= \\frac{\\sinh (x)}{\\sqrt{2 (\\cosh x + 1)} } &&= \\sgn x \\, \\sqrt \\frac{\\cosh x - 1}{2} \\\\[6px]\n  \\cosh\\left(\\frac{x}{2}\\right) &= \\sqrt \\frac{\\cosh x + 1}{2}\\\\[6px]\n  \\tanh\\left(\\frac{x}{2}\\right) &= \\frac{\\sinh x}{\\cosh x + 1} &&= \\sgn x \\, \\sqrt \\frac{\\cosh x-1}{\\cosh x+1} = \\frac{e^x - 1}{e^x + 1} \n\\end{align}</math>\n\nwhere {{math|sgn}} is the [[sign function]].\n\nIf {{math|''x'' ≠ 0}}, then<ref>{{cite web|title=math.stackexchange.com/q/1565753/88985|url=http://math.stackexchange.com/q/1565753/88985|website=[[StackExchange]] (mathematics)|accessdate=24 January 2016}}</ref>\n\n:<math> \\tanh\\left(\\frac{x}{2}\\right) = \\frac{\\cosh x - 1}{\\sinh x} = \\coth x - \\operatorname{csch} x </math>\n\n==Inverse functions as logarithms==\n{{main|Inverse hyperbolic function}}\n\n:<math>\\begin{align}\n  \\operatorname {arsinh} (x) &= \\ln \\left(x + \\sqrt{x^{2} + 1} \\right) \\\\\n  \\operatorname {arcosh} (x) &= \\ln \\left(x + \\sqrt{x^{2} - 1} \\right) && x \\geqslant 1 \\\\\n  \\operatorname {artanh} (x) &= \\frac{1}{2}\\ln \\left( \\frac{1 + x}{1 - x} \\right) && | x | < 1 \\\\\n  \\operatorname {arcoth} (x) &= \\frac{1}{2}\\ln \\left( \\frac{x + 1}{x - 1} \\right) && |x| > 1 \\\\\n  \\operatorname {arsech} (x) &= \\ln \\left( \\frac{1}{x} + \\sqrt{\\frac{1}{x^2} - 1}\\right) = \\ln \\left( \\frac{1+ \\sqrt{1 - x^2}}{x} \\right) && 0 < x \\leqslant 1 \\\\\n  \\operatorname {arcsch} (x) &= \\ln \\left( \\frac{1}{x} + \\sqrt{\\frac{1}{x^2} +1}\\right) && x \\ne 0\n\n\\end{align}</math>\n\n==Derivatives==\n:<math>\\begin{align}\n   \\frac{d}{dx}\\sinh x &= \\cosh x \\\\\n   \\frac{d}{dx}\\cosh x &= \\sinh x \\\\\n   \\frac{d}{dx}\\tanh x &= 1 - \\tanh^2 x = \\operatorname{sech}^2 x = \\frac{1}{\\cosh^2 x} \\\\ \n   \\frac{d}{dx}\\coth x &= 1 - \\coth^2 x = -\\operatorname{csch}^2 x = -\\frac{1}{\\sinh^2 x} && x \\neq 0 \\\\\n   \\frac{d}{dx}\\operatorname{sech} x &= - \\tanh x \\operatorname{sech} x \\\\\n   \\frac{d}{dx}\\operatorname{csch} x &= - \\coth x \\operatorname{csch} x && x \\neq 0 \\\\\n   \\frac{d}{dx}\\operatorname{arsinh} x &= \\frac{1}{\\sqrt{x^2+1}} \\\\\n   \\frac{d}{dx}\\operatorname{arcosh} x &= \\frac{1}{\\sqrt{x^2 - 1}} && 1 < x \\\\\n   \\frac{d}{dx}\\operatorname{artanh} x &= \\frac{1}{1-x^2} && |x| < 1 \\\\\n   \\frac{d}{dx}\\operatorname{arcoth} x &= \\frac{1}{1-x^2} && 1 < |x| \\\\\n   \\frac{d}{dx}\\operatorname{arsech} x &= -\\frac{1}{x\\sqrt{1-x^2}} && 0 < x < 1 \\\\\n   \\frac{d}{dx}\\operatorname{arcsch} x &= -\\frac{1}{|x|\\sqrt{1+x^2}} && x \\neq 0\n \\end{align}</math>\n\n<!-- from: http://www.pen.k12.va.us/Div/Winchester/jhhs/math/lessons/calculus/tableof.html  and http://thesaurus.maths.org/mmkb/entry.html?action=entryById&id=2664 -->\n\n==Second derivatives==\nSinh and cosh are both equal to their [[second derivative]], that is:\n:<math> \\frac{d^2}{dx^2}\\sinh x = \\sinh x \\,</math>\n:<math> \\frac{d^2}{dx^2}\\cosh x = \\cosh x \\, .</math>\n\nAll functions with this property are [[linear combination]]s of sinh and cosh, in particular the [[exponential function]]s <math> e^x </math> and <math> e^{-x} </math>, and the [[zero function]] <math> f(x) = 0 </math>.\n\n==Standard integrals==\n{{For|a full list|list of integrals of hyperbolic functions}}\n\n:<math>\\begin{align}\n  \\int \\sinh (ax)\\,dx &= a^{-1} \\cosh (ax) + C \\\\\n  \\int \\cosh (ax)\\,dx &= a^{-1} \\sinh (ax) + C \\\\\n  \\int \\tanh (ax)\\,dx &= a^{-1} \\ln (\\cosh (ax)) + C \\\\\n  \\int \\coth (ax)\\,dx &= a^{-1} \\ln (\\sinh (ax)) + C \\\\\n  \\int \\operatorname{sech} (ax)\\,dx &= a^{-1} \\arctan (\\sinh (ax)) + C \\\\\n  \\int \\operatorname{csch} (ax)\\,dx &= a^{-1} \\ln \\left( \\tanh \\left( \\frac{ax}{2} \\right) \\right) + C = a^{-1} \\ln\\left|\\operatorname{csch} (ax) - \\coth (ax)\\right| + C\n\\end{align}</math>\n\nThe following integrals can be proved using [[hyperbolic substitution]]:\n\n:<math>\\begin{align}\n   \\int {\\frac{1}{\\sqrt{a^2 + u^2}}\\,du} & = \\operatorname{arsinh} \\left( \\frac{u}{a} \\right) + C \\\\\n   \\int {\\frac{1}{\\sqrt{u^2 - a^2}}\\,du} &= \\operatorname{arcosh} \\left( \\frac{u}{a} \\right) + C \\\\\n   \\int {\\frac{1}{a^2 - u^2}}\\,du & =  a^{-1}\\operatorname{artanh} \\left( \\frac{u}{a} \\right) + C && u^2 < a^2 \\\\\n   \\int {\\frac{1}{a^2 - u^2}}\\,du & =  a^{-1}\\operatorname{arcoth} \\left( \\frac{u}{a} \\right) + C && u^2 > a^2 \\\\\n   \\int {\\frac{1}{u\\sqrt{a^2 - u^2}}\\,du} & = -a^{-1}\\operatorname{arsech}\\left( \\frac{u}{a} \\right) + C \\\\\n   \\int {\\frac{1}{u\\sqrt{a^2 + u^2}}\\,du} & = -a^{-1}\\operatorname{arcsch}\\left| \\frac{u}{a} \\right| + C\n\\end{align}</math>\n\nwhere ''C'' is the [[constant of integration]].\n\n==Taylor series expressions==\nIt is possible to express the above functions as [[Taylor series]]:\n\n:<math>\\sinh x = x + \\frac {x^3} {3!} + \\frac {x^5} {5!} + \\frac {x^7} {7!} +\\cdots = \\sum_{n=0}^\\infty \\frac{x^{2n+1}}{(2n+1)!}</math>\nThe function sinh&nbsp;''x'' has a Taylor series expression with only odd exponents for ''x''. Thus it is an [[odd function]], that is, −sinh&nbsp;''x''&nbsp;=&nbsp;sinh(−''x''), and sinh&nbsp;0&nbsp;=&nbsp;0.\n\n:<math>\\cosh x = 1 + \\frac {x^2} {2!} + \\frac {x^4} {4!} + \\frac {x^6} {6!} + \\cdots = \\sum_{n=0}^\\infty \\frac{x^{2n}}{(2n)!}</math>\n\nThe function cosh&nbsp;''x'' has a Taylor series expression with only even exponents for ''x''. Thus it is an [[even function]], that is, symmetric with respect to the ''y''-axis. The sum of the sinh and cosh series is the [[infinite series]] expression of the [[exponential function]].\n\n:<math>\\begin{align}\n\n                   \\tanh x &= x - \\frac {x^3} {3} + \\frac {2x^5} {15} - \\frac {17x^7} {315} + \\cdots = \\sum_{n=1}^\\infty \\frac{2^{2n}(2^{2n}-1)B_{2n} x^{2n-1}}{(2n)!}, \\left |x \\right | < \\frac {\\pi} {2} \\\\\n\n                   \\coth x &= x^{-1} + \\frac {x} {3} - \\frac {x^3} {45} + \\frac {2x^5} {945} + \\cdots = x^{-1} + \\sum_{n=1}^\\infty \\frac{2^{2n} B_{2n} x^{2n-1}} {(2n)!}, 0 < \\left |x \\right | < \\pi \\\\\n\n  \\operatorname {sech}\\, x &= 1 - \\frac {x^2} {2} + \\frac {5x^4} {24} - \\frac {61x^6} {720} + \\cdots = \\sum_{n=0}^\\infty \\frac{E_{2 n} x^{2n}}{(2n)!} , \\left |x \\right | < \\frac {\\pi} {2} \\\\\n\n  \\operatorname {csch}\\, x &= x^{-1} - \\frac {x} {6} +\\frac {7x^3} {360} -\\frac {31x^5} {15120} + \\cdots = x^{-1} + \\sum_{n=1}^\\infty \\frac{ 2 (1-2^{2n-1}) B_{2n} x^{2n-1}}{(2n)!} , 0 < \\left |x \\right | < \\pi\n\n\\end{align}</math>\n\nwhere:\n:<math>B_n \\,</math> is the ''n''th [[Bernoulli number]]\n:<math>E_n \\,</math> is the ''n''th [[Euler number]]\n\n==Comparison with circular functions==\n\n[[File:Circular and hyperbolic angle.svg|right|250px|thumb|Circle and hyperbola tangent at (1,1) display geometry of circular functions in terms of [[sector of a circle|circular sector]] area ''u'' and hyperbolic functions depending on [[hyperbolic sector]] area ''u''.]]\nThe hyperbolic functions represent an expansion of [[trigonometry]] beyond the [[circular function]]s. Both types depend on an [[argument of a function|argument]], either [[angle|circular angle]] or [[hyperbolic angle]].\n\nSince the [[circular sector#Area|area of a circular sector]] with radius ''r'' and angle ''u'' is ''r''<sup>2</sup>''u''/2, it will be equal to ''u'' when ''r'' = {{sqrt|2}}. In the diagram such a circle is tangent to the hyperbola ''xy'' = 1 at (1,1). The yellow sector depicts an area and angle magnitude. Similarly, the yellow and red sectors together depict an area and [[hyperbolic sector|hyperbolic angle magnitude]].\n\nThe legs of the two [[right triangle]]s with hypotenuse on the ray defining the angles are of length {{radic|2}} times the circular and hyperbolic functions.\n\nThe hyperbolic angle is an [[invariant measure]] with respect to the [[squeeze mapping]], just as the circular angle is invariant under rotation.<ref>[[Mellen W. Haskell]], \"On the introduction of the notion of hyperbolic functions\",  [[Bulletin of the American Mathematical Society]] '''1''':6:155–9, [http://www.ams.org/journals/bull/1895-01-06/S0002-9904-1895-00266-9/S0002-9904-1895-00266-9.pdf full text]</ref>\n\n=={{anchor|Osborn}}Identities==\nThe hyperbolic functions satisfy many identities, all of them similar in form to the [[trigonometric identity|trigonometric identities]]. In fact, '''Osborn's rule'''<ref name=\"Osborn, 1902\" /> states that one can convert any trigonometric identity into a hyperbolic identity by expanding it completely in terms of integral powers of sines and cosines, changing sine to sinh and cosine to cosh, and switching the sign of every term which contains a product of 2, 6, 10, 14, ... sinhs. This yields for example the addition theorems\n:<math>\\begin{align}\n  \\sinh(x + y) &= \\sinh (x) \\cosh (y) + \\cosh (x) \\sinh (y) \\\\\n  \\cosh(x + y) &= \\cosh (x) \\cosh (y) + \\sinh (x) \\sinh (y) \\\\\n  \\tanh(x + y) &= \\frac{\\tanh (x) + \\tanh (y)}{1 + \\tanh (x) \\tanh (y)}\n\\end{align}</math>\n\nthe \"double argument formulas\"\n:<math>\\begin{align}\n  \\sinh (2x) &= 2\\sinh x \\cosh x \\\\\n  \\cosh (2x) &= \\cosh^2 x + \\sinh^2 x = 2\\cosh^2 x - 1 = 2\\sinh^2 x + 1 \\\\\n  \\tanh (2x) &= \\frac{2\\tanh x}{1 + \\tanh^2 x}\\\\\n  \\sinh (2x) &= \\frac{2\\tanh x}{1-\\tanh^2 x}\\\\\n  \\cosh (2x) &= \\frac{1+ \\tanh^2 x}{1-\\tanh^2 x}\n\\end{align}</math>\n\nand the \"half-argument formulas\"<ref>\n{{cite book\n |title=Technical mathematics with calculus\n |edition=3rd\n |first1=John Charles\n |last1=Peterson\n |publisher=Cengage Learning\n |year=2003\n |isbn=0-7668-6189-9\n |page=1155\n |url=https://books.google.com/books?id=PGuSDjHvircC}}, [https://books.google.com/books?id=PGuSDjHvircC&pg=PA1155 Chapter 26, page 1155]\n</ref>\n:<math>\\sinh \\frac{x}{2} = \\sqrt{ \\frac{1}{2}(\\cosh x - 1)} \\,</math> &nbsp;&nbsp;&nbsp;Note:  This is equivalent to its circular counterpart multiplied by −1.\n:<math>\\cosh \\frac{x}{2} = \\sqrt{ \\frac{1}{2}(\\cosh x + 1)} \\,</math> &nbsp;&nbsp;&nbsp;Note:  This corresponds to its circular counterpart.\n:<math> \\tanh \\frac{x}{2} = \\sqrt \\frac{\\cosh x - 1}{\\cosh x + 1} = \\frac{\\sinh x}{\\cosh x + 1} = \\frac{\\cosh x - 1}{\\sinh x} = \\coth x - \\operatorname{csch}x.</math>\n:<math> \\coth \\frac{x}{2} = \\coth x + \\operatorname{csch}x.</math>\n\nThe [[derivative]] of sinh&nbsp;''x'' is cosh&nbsp;''x'' and the derivative of cosh&nbsp;''x'' is sinh&nbsp;''x''; this is similar to trigonometric functions, albeit the sign is different (i.e., the derivative of cos&nbsp;''x'' is −sin&nbsp;''x'').\n\nThe [[Gudermannian function]] gives a direct relationship between the circular functions and the hyperbolic ones that does not involve complex numbers.\n\nThe graph of the function ''a''&nbsp;cosh(''x''/''a'') is the [[catenary]], the curve formed by a uniform flexible chain hanging freely between two fixed points under uniform gravity.\n\n==Relationship to the exponential function==\n\nThe decomposition of the exponential function in its [[even–odd decomposition|even and odd parts]] gives the identities\n:<math>e^x = \\cosh x + \\sinh x,</math>\nand\n:<math>e^{-x} = \\cosh x - \\sinh x.</math>\nThe first one is analogous to [[Euler's formula]]\n:<math>e^{ix} = \\cos x + i\\sin x.</math>\n\nAdditionally,\n:<math>e^x = \\sqrt{\\frac{1 + \\tanh x}{1 - \\tanh x}} = \\frac{1 + \\tanh \\frac{x}{2}}{1 - \\tanh \\frac{x}{2}}</math>\n\n==Hyperbolic functions for complex numbers==\nSince the [[exponential function]] can be defined for any [[complex number|complex]] argument, we can extend the definitions of the hyperbolic functions also to complex arguments. The functions sinh&nbsp;''z'' and cosh&nbsp;''z'' are then [[Holomorphic function|holomorphic]].\n\nRelationships to ordinary trigonometric functions are given by [[Euler's formula]] for complex numbers:\n\n:<math>\\begin{align}\n   e^{i x} &= \\cos x + i \\sin x \\\\\n  e^{-i x} &= \\cos x - i \\sin x\n\\end{align}</math>\n\nso:\n\n:<math>\\begin{align}\n    \\cosh(ix) &= \\frac{1}{2} \\left(e^{i x} + e^{-i x}\\right) = \\cos x \\\\\n    \\sinh(ix) &= \\frac{1}{2} \\left(e^{i x} - e^{-i x}\\right) = i \\sin x \\\\\n \\cosh(x+iy) &= \\cosh(x) \\cos(y) + i \\sinh(x) \\sin(y) \\\\\n \\sinh(x+iy) &= \\sinh(x) \\cos(y) + i \\cosh(x) \\sin(y) \\\\\n    \\tanh(ix) &= i \\tan x \\\\\n     \\cosh x &= \\cos(ix) \\\\\n     \\sinh x &= - i \\sin(ix) \\\\\n     \\tanh x &= - i \\tan(ix)\n\\end{align}</math>\n\nThus, hyperbolic functions are [[periodic function|periodic]] with respect to the imaginary component, with period <math>2 \\pi i</math> (<math>\\pi i</math> for hyperbolic tangent and cotangent).\n\n{|  style=\"text-align:center\"\n|+ Hyperbolic functions in the complex plane\n|[[Image:Complex Sinh.jpg|1000x140px|none]]\n|[[Image:Complex Cosh.jpg|1000x140px|none]]\n|[[Image:Complex Tanh.jpg|1000x140px|none]]\n|[[Image:Complex Coth.jpg|1000x140px|none]]\n|[[Image:Complex Sech.jpg|1000x140px|none]]\n|[[Image:Complex Csch.jpg|1000x140px|none]]\n|-\n|<math>\\operatorname{sinh}(z)</math>\n|<math>\\operatorname{cosh}(z)</math>\n|<math>\\operatorname{tanh}(z)</math>\n|<math>\\operatorname{coth}(z)</math>\n|<math>\\operatorname{sech}(z)</math>\n|<math>\\operatorname{csch}(z)</math>\n|}\n\n==See also==\n{{Commons category|Hyperbolic functions}}\n* [[e (mathematical constant)]]\n* [[Equal incircles theorem]], based on sinh\n* [[Inverse hyperbolic function]]s\n* [[List of integrals of hyperbolic functions]]\n* [[Poinsot's spirals]]\n* [[Sigmoid function]]\n* [[Trigonometric functions]]\n\n==References==\n{{Reflist|35em|refs=\n\n<ref name=\"Osborn, 1902\" >{{Cite journal\n  |first=G.  |last=Osborn\n  |jstor=3602492 \n  |title=Mnemonic for hyperbolic formulae\n  |journal=[[The Mathematical Gazette]]\n  |page=189\n  |volume=2  |issue=34\n  |date=July 1902\n}}</ref>\n\n}}\n\n==External links==\n*{{springer|title=Hyperbolic functions|id=p/h048250}}\n*[http://planetmath.org/hyperbolicfunctions Hyperbolic functions] on [[PlanetMath]]\n*[http://mathworld.wolfram.com/HyperbolicFunctions.html Hyperbolic functions] entry at [[MathWorld]]\n*[https://web.archive.org/web/20071006172054/http://glab.trixon.se/ GonioLab]: Visualization of the unit circle, trigonometric and hyperbolic functions ([[Java Web Start]])\n*[http://www.calctool.org/CALC/math/trigonometry/hyperbolic Web-based calculator of hyperbolic functions]\n\n{{Authority control}}\n\n{{DEFAULTSORT:Hyperbolic Function}}\n[[Category:Hyperbolic functions| ]]\n[[Category:Exponentials]]\n[[Category:Hyperbolic geometry]]\n[[Category:Analytic functions]]"
    },
    {
      "title": "Index of logarithm articles",
      "url": "https://en.wikipedia.org/wiki/Index_of_logarithm_articles",
      "text": "This is a '''list of [[logarithm]] topics''', by Wikipedia page. See also the [[list of exponential topics]].\n* [[Acoustic power]]\n* [[Antilogarithm]]\n* [[Apparent magnitude]]\n* [[Baker's theorem]]\n* [[Bel (unit)|Bel]]\n* [[Benford's law]]\n* [[Binary logarithm]]\n* [[Bode plot]]\n* [[Henry Briggs (mathematician)|Henry Briggs]]\n* [[Bygrave slide rule]]\n* [[Cologarithm]]\n* [[Common logarithm]]\n* [[Complex logarithm]]\n* [[Discrete logarithm]]\n** [[Discrete logarithm records]]\n* [[e (mathematical constant)|e]]\n** [[Representations of e]]\n* [[El Gamal discrete log cryptosystem]]\n* [[Harmonic series (mathematics)|Harmonic series]]\n* [[History of logarithms]]\n* [[Hyperbolic sector]]\n* [[Iterated logarithm]]\n* [[Otis King]]\n* [[Law of the iterated logarithm]]\n* [[Linear form in logarithms]]\n* [[Linearithmic]]\n* [[List of integrals of logarithmic functions]]\n* [[Logarithmic growth]]\n* [[Logarithmic timeline]]\n* [[Log-likelihood ratio]]\n* [[Log-log|Log-log graph]]\n* [[Log-normal distribution]]\n* [[Log-periodic antenna]]\n* [[Log-Weibull distribution]]\n* [[Logarithmic algorithm]]\n* [[Logarithmic convolution]]\n* [[Logarithmic decrement]]\n* [[Logarithmic derivative]]\n* [[Logarithmic differential]]\n* [[Logarithmic differentiation]]\n* [[Logarithmic distribution]]\n* [[Logarithmic form]]\n* [[Logarithmic graph paper]]\n* [[Logarithmic growth]]\n* [[Logarithmic identities]]\n* [[Logarithmic number system]]\n* [[Logarithmic scale]]\n* [[Logarithmic spiral]]\n* [[Logarithmic timeline]]\n* [[Logit]]\n* [[LogSumExp]]\n* ''[[Mantissa (disambiguation)|Mantissa]]'' is a disambiguation page; see [[common logarithm]] for the traditional concept of ''mantissa''; see [[significand]] for the modern concept used in computing.\n* [[Matrix logarithm]]\n* [[Mel scale]]\n* [[Mercator projection]]\n* [[Mercator series]]\n* [[Moment magnitude scale]]\n* [[John Napier]]\n* [[Napierian logarithm]]\n* [[Natural logarithm]]\n** [[Natural logarithm of 2]]\n* [[Neper]]\n* [[Offset logarithmic integral]]\n* [[pH]]\n* [[Pollard's kangaroo algorithm]]\n* [[Pollard's rho algorithm for logarithms]]\n* [[Polylogarithm]]\n* [[Polylogarithmic function]]\n* [[Prime number theorem]]\n* [[Richter magnitude scale]]\n* [[Grégoire de Saint-Vincent]]\n* [[Alphonse Antonio de Sarasa]]\n* [[Schnorr signature]]\n* [[Semi-log graph]]\n* [[Significand]]\n* [[Slide rule]]\n* [[Smearing retransformation]]\n* [[Sound intensity level]]\n* [[Super-logarithm]]\n* [[Table of logarithms]]\n* [[Weber-Fechner law]]\n\n[[Category:Exponentials]]\n[[Category:Logarithms|*]]\n[[Category:Mathematics-related lists|Logarithm topics]]"
    },
    {
      "title": "Infra-exponential",
      "url": "https://en.wikipedia.org/wiki/Infra-exponential",
      "text": "{{See also|Time complexity#Sub-exponential time}}\n\nA growth rate is said to be '''infra-exponential''' or '''subexponential''' if it is dominated by all [[exponential growth]] rates, however great the [[doubling time]]. A continuous function with infra-exponential growth rate will have a [[Fourier transform]] that is a Fourier [[hyperfunction]].\n\nExamples of sub-exponential growth rates arise in the [[analysis of algorithms]], where they give rise to [[Time complexity#Sub-exponential time|sub-exponential time complexity]], and in the [[Growth rate (group theory)|growth rate of groups]], where a subexponential growth rate implies that a group is [[amenable group|amenable]].\n\n==References==\n*[http://eom.springer.de/F/f120110.htm Springer Online Mathematics Encyclopedia]\n\n[[Category:Exponentials]]\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Lindemann–Weierstrass theorem",
      "url": "https://en.wikipedia.org/wiki/Lindemann%E2%80%93Weierstrass_theorem",
      "text": "{{stack|{{Pi box}}|{{E (mathematical constant)}}}}\nIn [[transcendental number theory]]<!--[[mathematics]]-->, the '''Lindemann–Weierstrass theorem''' is a result that is very useful in establishing the [[transcendental number|transcendence]] of numbers. It states the following. {{math_theorem|name=Lindemann–Weierstrass theorem|if {{math| α<sub>1</sub>,&nbsp;...,&nbsp;α<sub>''n''</sub> }} are [[algebraic number]]s that are [[linearly independent]] over the [[rational number]]s {{math| &#x211A;, }} then {{math| ''e''<sup>α<sub>1</sub></sup>,&nbsp;...,&nbsp;''e''<sup>α<sub>''n''</sub></sup> }} are [[algebraically independent]] over {{math| &#x211A;.}}}} \nIn other words the [[extension field]] {{math| &#x211A;(''e''<sup>α<sub>1</sub></sup>,&nbsp;...,&nbsp;''e''<sup>α<sub>''n''</sub></sup>) }} has [[transcendence degree]] {{math| ''n'' }} over {{math| &#x211A;. }}\n\nAn equivalent formulation {{Harv|Baker|1990|loc=Chapter 1, Theorem 1.4}}, is the following. {{math_theorem|name=An equivalent formulation|If {{math| α<sub>1</sub>,&nbsp;...,&nbsp;α<sub>''n''</sub> }} are distinct algebraic numbers, then the exponentials {{math| ''e''<sup>α<sub>1</sub></sup>,&nbsp;...,&nbsp;''e''<sup>α<sub>''n''</sub></sup> }} are linearly independent over the algebraic numbers.}}  This equivalence transforms a linear relation over the algebraic numbers into an algebraic relation over {{math| &#x211A;:}} by using the fact that a [[symmetric polynomial]] whose arguments are all [[algebraic conjugate|conjugates]] of one another gives a rational number.\n\nThe theorem is named for [[Ferdinand von Lindemann]] and [[Karl Weierstrass]]. Lindemann proved in 1882 that {{math| ''e''<sup>α</sup> }} is transcendental for every non-zero algebraic number {{math| α, }} thereby establishing that {{pi}} is transcendental (see below).<ref name=\"Lindemann1882a\" /> Weierstrass proved the above more general statement in 1885.<ref name=\"Weierstrass1885\" />\n\nThe theorem, along with the [[Gelfond–Schneider theorem]], is extended by [[Baker's theorem]], and all of these are further generalized by [[Schanuel's conjecture]].\n\n==Naming convention==\nThe theorem is also known variously as the '''Hermite–Lindemann theorem''' and the '''Hermite–Lindemann–Weierstrass theorem'''. [[Charles Hermite]] first proved the simpler theorem where the {{math| α<sub>''i''</sub> }} exponents are required to be [[rational integer]]s and linear independence is only assured over the rational integers,<ref>{{Harvnb|Hermite|1873|pp=18–24}}.</ref><ref>{{Harvnb|Hermite|1874}}</ref> a result sometimes referred to as Hermite's theorem.<ref>{{Harvnb|Gelfond|2015}}.</ref> Although apparently a rather special case of the above theorem, the general result can be reduced to this simpler case.  Lindemann was the first to allow algebraic numbers into Hermite's work in 1882.<ref name=\"Lindemann1882a\">{{Harvnb|Lindemann|1882a}}, {{Harvnb|Lindemann|1882b}}.</ref>  Shortly afterwards Weierstrass obtained the full result,<ref name=\"Weierstrass1885\">{{Harvnb|Weierstrass|1885|pp=1067–1086}},</ref> and further simplifications have been made by several mathematicians, most notably by [[David Hilbert]]<ref>{{Harvnb|Hilbert|1893|pp=216–219}}.</ref> and [[Paul Gordan]].<ref>{{Harvnb|Gordan|1893|pp=222–224}}.</ref>\n\n== {{anchor|Transcendence of ''e'' and π}} Transcendence of {{math| ''e'' }} and {{pi}} ==\nThe [[transcendental number|transcendence]] of {{math| [[e (mathematical constant)|''e'']] }} and {{pi}} are direct corollaries of this theorem.\n\nSuppose {{math| α }} is a non zero algebraic number; then {{math| {α} }} is a linearly independent set over the rationals, and therefore by the first formulation of the theorem {{math| {''e''<sup>α</sup>} }} is an algebraically independent set; or in other words {{math| ''e''<sup>α</sup> }} is transcendental. In particular, {{math| ''e''<sup>1</sup> {{=}} ''e'' }} is transcendental. (A more elementary proof that {{math| ''e'' }} is transcendental is outlined in the article on [[transcendental number]]s.)\n\nAlternatively, by the second formulation of the theorem, if {{math| α }} is a nonzero algebraic number, then {{math| {0, α} }} is a set of distinct algebraic numbers, and so the set {{math| {''e''<sup>0</sup>,&nbsp;''e''<sup>α</sup>}&nbsp;{{=}}&nbsp;{1,&nbsp;''e''<sup>α</sup>} }} is linearly independent over the algebraic numbers and in particular {{math| ''e''<sup>α</sup> }} cannot be algebraic and so it is transcendental.\n\nTo prove that {{pi}} is transcendental, we prove that it is not algebraic. If {{pi}} were algebraic, {{pi}}''i'' would be algebraic as well, and then by the Lindemann–Weierstrass theorem {{math| ''e''<sup>{{pi}}''i''</sup> {{=}} −1 }} (see [[Euler's identity]]) would be transcendental, a contradiction. Therefore {{pi}} is not algebraic, which means that it is transcendental.\n\nA slight variant on the same proof will show that if {{math| α }} is a nonzero algebraic number then {{math| sin(α), cos(α), tan(α) }} and their [[hyperbolic function|hyperbolic]] counterparts are also transcendental.\n\n== {{anchor|''p''-adic conjecture}} &thinsp;{{math|''p''}}-adic conjecture ==\n{{math_theorem|name={{math|''p''}}-adic Lindemann–Weierstrass Conjecture.|math_statement=Suppose {{math| ''p'' }} is some [[prime number]] and {{math| α<sub>1</sub>,&nbsp;...,&nbsp;α<sub>''n''</sub> }} are [[p-adic numbers|{{math|''p''}}-adic numbers]] which are algebraic and linearly independent over {{math| &#x211A;, }} such that {{math| {{!}}&thinsp;α<sub>''i''</sub>&thinsp;{{!}}<sub>''p''</sub>&nbsp;<&nbsp;1/''p'' }} for all {{math| ''i''; }} then the [[p-adic exponential function|{{math|''p''}}-adic exponential]]s {{math| exp<sub>''p''</sub>(α<sub>1</sub>),&nbsp;.&nbsp;.&nbsp;.&nbsp;,&nbsp;exp<sub>''p''</sub>(α<sub>''n''</sub>) }} are {{math|''p''}}-adic numbers that are algebraically independent over {{math| &#x211A;. }} }}\n\n==Modular conjecture==\nAn analogue of the theorem involving the [[modular function]] [[j-invariant|{{math| ''j'' }}]] was conjectured by Daniel Bertrand in 1997, and remains an open problem.<ref>{{Harvnb|Bertrand|1997|pp=339–350}}.</ref> Writing {{math| ''q''&nbsp;{{=}}&nbsp;''e''<sup>2{{pi}}''i''τ</sup> }} for the [[Nome (mathematics)|nome]] and {{math| ''j''(τ)&nbsp;{{=}}&nbsp;''J''(''q''), }} the conjecture is as follows.  {{math_theorem|name=Modular conjecture|Let {{math| ''q''<sub>1</sub>, ..., ''q''<sub>''n''</sub> }} be non-zero algebraic numbers in the complex [[unit disc]] such that the {{math| 3''n'' }} numbers\n\n:<math>\\left \\{ J(q_1), J'(q_1), J''(q_1), \\ldots, J(q_n), J'(q_n), J''(q_n) \\right \\}</math>\n\nare algebraically dependent over {{math| &#x211A;. }} Then there exist two indices {{math| 1&nbsp;≤&nbsp;''i''&nbsp;<&nbsp;''j''&nbsp;≤&nbsp;''n'' }} such that {{math| ''q<sub>i</sub>'' }} and {{math| ''q''<sub>''j''</sub> }} are multiplicatively dependent.}}\n\n==Lindemann–Weierstrass theorem ==\n{{math_theorem|name=Lindemann–Weierstrass Theorem (Baker's reformulation).|math_statement=If {{math| ''a''<sub>1</sub>, ..., ''a''<sub>''n''</sub> }} are non-zero algebraic numbers, and {{math| α<sub>1</sub>, ..., α<sub>''n''</sub> }} are distinct algebraic numbers, then<ref>{{fr icon}} [http://fan2cube.fr/mathtador/pdf/lindermann-weierstrass.pdf french Proof's Lindemann-Weierstrass (pdf)]{{Dead link|date=July 2017}}</ref>\n\n:<math>a_1 e^{\\alpha_1} +\\cdots + a_n e^{\\alpha_n}\\ne 0.</math>}}\n\n===Proof===\nThe proof relies on two preliminary lemmas. Notice that Lemma B itself is already sufficient to deduce the original statement of Lindemann-Weierstrass theorem.\n\n====Preliminary lemmas====\n{{math_theorem|name=Lemma A.|math_statement=Let {{math|''c''(1), ..., ''c''(''r'')}} be non-zero integers and, for every {{mvar|k}} between {{math|1}} and {{mvar|r}}, let {{math|{''γ''(''k'')<sub>1</sub>, ..., ''γ''(''k'')<sub>''m''(''k'')</sub>} }} be the roots of a non-zero polynomial with integer coefficients <math>T_k(x)</math>. If {{math|''γ''(''k'')<sub>''i''</sub>&nbsp;≠&nbsp;''γ''(''u'')<sub>''v''</sub> }} whenever {{math|(''k'',&nbsp;''i'')&nbsp;≠&nbsp;(''u'',&nbsp;''v'')}}, then\n\n:<math>c(1)\\left (e^{\\gamma(1)_1}+\\cdots+ e^{\\gamma(1)_{m(1)}} \\right ) + \\cdots + c(r) \\left (e^{\\gamma(r)_1}+\\cdots+ e^{\\gamma(r)_{m(r)}} \\right) \\ne 0.</math>}}\n\n'''Proof of Lemma A.''' To simplify the notation set:\n \n:<math>\n\\begin{align}\n& n_0 =0, & & \\\\\n& n_i =\\sum\\nolimits_{k=1}^i m(k), & & i=1,\\ldots,r \\\\\n& n=n_r, & & \\\\\n& \\alpha_{n_{i-1}+j} =\\gamma(i)_j, & & 1\\leq i\\leq r,\\  1\\leq j\\leq m(i) \\\\\n& \\beta_{n_{i-1}+j} =c(i).\n\\end{align}\n</math>\n\nThen the statement becomes\n\n:<math>\\sum_{k=1}^n \\beta_k e^{\\alpha_k}\\neq 0.</math>\n\nLet {{mvar|p}} be a [[prime number]] and define the following polynomials:\n\n: <math>f_i(x) = \\frac {\\ell^{np} (x-\\alpha_1)^p \\cdots (x-\\alpha_n)^p}{(x-\\alpha_i)},</math>\n\nwhere {{mvar|ℓ}} is a non-zero integer such that <math>\\ell\\alpha_1,\\ldots,\\ell\\alpha_n</math> are all algebraic integers. Define<ref>Up to a factor, this is the same integral appearing in [[Transcendental number#Sketch of a proof that ''e'' is transcendental|the proof that {{mvar|e}} is a transcendental number]], where {{math|''β''<sub>1</sub> {{=}} 1, ..., ''β<sub>m</sub>'' {{=}} ''m''.}} The rest of the proof of the Lemma is analog to that proof.</ref>\n\n: <math>I_i(s) = \\int^s_0 e^{s-x} f_i(x) \\, dx.</math>\n\nUsing [[integration by parts]] we arrive at\n\n: <math>I_i(s) = e^s \\sum_{j=0}^{np-1} f_i^{(j)}(0) - \\sum_{j=0}^{np-1} f_i^{(j)}(s),</math>\n\nwhere <math>np-1</math> is the [[Degree of a polynomial|degree]] of <math>f_i</math>, and <math>f_i^{(j)}</math> is the ''j''-th derivative of <math>f_i</math>. This also holds for ''s'' complex (in this case the integral has to be intended as a contour integral, for example along the straight segment from 0 to ''s'') because\n\n:<math>-e^{s-x} \\sum_{j=0}^{np-1} f_i^{(j)}(x)</math>\n\nis a primitive of <math>e^{s-x} f_i(x)</math>.\n\nConsider the following sum:\n\n:<math>\\begin{align}\nJ_i &=\\sum_{k=1}^n\\beta_k I_i(\\alpha_k)\\\\[5pt]\n&= \\sum_{k=1}^n\\beta_k \\left ( e^{\\alpha_k} \\sum_{j=0}^{np-1} f_i^{(j)}(0) - \\sum_{j=0}^{np-1} f_i^{(j)}(\\alpha_k)\\right ) \\\\[5pt]\n&=\\left(\\sum_{j=0}^{np-1}f_i^{(j)}(0)\\right)\\left(\\sum_{k=1}^n \\beta_k e^{\\alpha_k}\\right)-\\sum_{k=1}^n\\sum_{j=0}^{np-1} \\beta_kf_i^{(j)}(\\alpha_k)\\\\[5pt]\n&= -\\sum_{k=1}^n \\sum_{j=0}^{np-1} \\beta_kf_i^{(j)}(\\alpha_k)\n\\end{align}</math>\n\nIn the last line we assumed that the conclusion of the Lemma is false. In order to complete the proof we need to reach a contradiction. We will do so by estimating <math>|J_1\\cdots J_n|</math> in two different ways.\n\nFirst <math>f_i^{(j)}(\\alpha_k)</math> is an algebraic integer which is divisible by ''p''! for <math>j\\geq p</math> and vanishes for <math>j<p</math> unless <math>j=p-1</math> and <math>j=i</math>, in which case it equals\n\n:<math>\\ell^{np}(p-1)!\\prod_{k\\neq i}(\\alpha_i-\\alpha_k)^p.</math>\n\nThis is not divisible by ''p'' when ''p'' is large enough because otherwise, putting\n\n:<math>\\delta_i=\\prod_{k\\neq i}(\\ell\\alpha_i-\\ell\\alpha_k)</math>\n\n(which is a non-zero algebraic integer) and calling <math>d_i\\in\\mathbb Z</math> the product of its conjugates (which is still non-zero), we would get that ''p'' divides <math>\\ell^p(p-1)!d_i^p</math>, which is false.\n\nSo <math>J_i</math> is a non-zero algebraic integer divisible by (''p''&nbsp;−&nbsp;1)!. Now\n\n:<math>J_i=-\\sum_{j=0}^{np-1}\\sum_{t=1}^r c(t)\\left(f_i^{(j)}(\\alpha_{n_{t-1}+1}) + \\cdots + f_i^{(j)}(\\alpha_{n_t})\\right).</math>\n\nSince each <math>f_i(x)</math> is obtained by dividing a fixed polynomial with integer coefficients by <math>(x-\\alpha_i)</math>, it is of the form\n\n:<math>f_i(x)=\\sum_{m=0}^{np-1}g_m(\\alpha_i)x^m, </math>\n\nwhere <math>g_m</math> is a polynomial (with integer coefficients) independent of ''i''. The same holds for the derivatives <math>f_i^{(j)}(x)</math>.\n\nHence, by the fundamental theorem of symmetric polynomials,\n\n:<math>f_i^{(j)}(\\alpha_{n_{t-1}+1})+\\cdots+f_i^{(j)}(\\alpha_{n_t})</math>\n\nis a fixed polynomial with rational coefficients evaluated in <math>\\alpha_i</math> (this is seen by grouping the same powers of <math>\\alpha_{n_{t-1}+1},\\dots,\\alpha_{n_t}</math> appearing in the expansion and using the fact that these algebraic numbers are a complete set of conjugates). So the same is true of <math>J_i</math>, i.e. it equals <math>G(\\alpha_i)</math>, where ''G'' is a polynomial with rational coefficients independent of ''i''.\n\nFinally <math>J_1\\cdots J_n=G(\\alpha_1)\\cdots G(\\alpha_n)</math> is rational (again by the fundamental theorem of symmetric polynomials) and is a non-zero algebraic integer divisible by <math>(p-1)!^n</math> (since the <math>J_i</math>'s are algebraic integers divisible by <math>(p-1)!</math>). Therefore\n\n:<math>|J_1\\cdots J_n|\\geq (p-1)!^n.</math>\n\nHowever one clearly has:\n\n:<math>|I_i(\\alpha_k)| \\leq |\\alpha_k|e^{|\\alpha_k|}F_i(|\\alpha_k|),</math>\n\nwhere {{mvar|F<sub>i</sub>}} is the polynomial whose coefficients are the absolute values of those of ''f''<sub>''i''</sub> (this follows directly from the definition of <math>I_i(s)</math>). Thus\n\n:<math>|J_i|\\leq \\sum_{k=1}^n \\left |\\beta_k\\alpha_k \\right |e^{|\\alpha_k|}F_i \\left ( \\left |\\alpha_k \\right| \\right )</math>\n\nand so by the construction of the <math>f_i</math>'s we have <math>|J_1\\cdots J_n|\\le C^p</math> for a sufficiently large ''C'' independent of ''p'', which contradicts the previous inequality. This proves Lemma A. &#8718;\n\n{{math_theorem|name=Lemma B.|math_statement=If ''b''(1), ..., ''b''(''n'') are non-zero integers and ''γ''(1), ..., ''γ''(''n''), are distinct [[algebraic number]]s, then\n:<math>b(1)e^{\\gamma(1)}+\\cdots+ b(n)e^{\\gamma(n)}\\ne 0.</math>}}\n\n'''Proof of Lemma B:''' Assuming\n\n:<math>b(1)e^{\\gamma(1)}+\\cdots+ b(n)e^{\\gamma(n)}= 0,</math>\n\nwe will derive a contradiction, thus proving Lemma B.\n\nLet us choose a polynomial with integer coefficients which vanishes on all the <math>\\gamma(k)</math>'s and let <math>\\gamma(1),\\ldots,\\gamma(n),\\gamma(n+1),\\ldots,\\gamma(N)</math> be all its distinct roots. Let ''b''(''n''&nbsp;+&nbsp;1)&nbsp;=&nbsp;...&nbsp;=&nbsp;''b''(''N'')&nbsp;=&nbsp;0.\n\nThe polynomial\n\n:<math>P(x_1,\\dots,x_N)=\\prod_{\\sigma\\in S_N}(b(1) x_{\\sigma(1)}+\\cdots+b(N) x_{\\sigma(N)})</math>\n\nvanishes at <math>(e^{\\gamma(1)},\\dots,e^{\\gamma(N)})</math> by assumption. Since the product is symmetric, for any <math>\\tau\\in S_N</math> the monomials <math>x_{\\tau(1)}^{h_1}\\cdots x_{\\tau(N)}^{h_N}</math> and <math>x_1^{h_1}\\cdots x_N^{h_N}</math> have the same coefficient in the expansion of ''P''.\n\nThus, expanding <math>P(e^{\\gamma(1)},\\dots,e^{\\gamma(N)})</math> accordingly and grouping the terms with the same exponent, we see that the resulting exponents <math>h_1\\gamma(1)+\\dots+h_N\\gamma(N)</math> form a complete set of conjugates and, if two terms have conjugate exponents, they are multiplied by the same coefficient.\n\nSo we are in the situation of Lemma A. To reach a contradiction it suffices to see that at least one of the coefficients is non-zero. This is seen by equipping {{math|'''C'''}} with the lexicographic order and by choosing for each factor in the product the term with non-zero coefficient which has maximum exponent according to this ordering: the product of these terms has non-zero coefficient in the expansion and does not get simplified by any other term. This proves Lemma B. &#8718;\n\n====Final step====\nWe turn now to prove the theorem: Let ''a''(1), ..., ''a''(''n'') be non-zero [[algebraic number]]s, and ''α''(1), ..., ''α''(''n'') distinct algebraic numbers. Then let us assume that:\n\n: <math>a(1)e^{\\alpha(1)}+\\cdots + a(n)e^{\\alpha(n)} =  0.</math>\n\nWe will show that this leads to contradiction and thus prove the theorem. The proof is very similar to that of Lemma B, except that this time the choices are made over the ''a''(''i'')'s:\n\nFor every ''i'' ∈ {1, ..., ''n''}, ''a''(''i'') is algebraic, so it is a root of an irreducible polynomial with integer coefficients of degree ''d''(''i''). Let us denote the distinct roots of this polynomial ''a''(''i'')<sub>1</sub>, ..., ''a''(''i'')<sub>''d''(''i'')</sub>, with ''a''(''i'')<sub>1</sub> = ''a''(''i'').\n\nLet S be the functions σ which choose one element from each of the sequences (1, ..., ''d''(1)), (1, ..., ''d''(2)), ..., (1, ..., ''d''(''n'')), so that for every 1&nbsp;≤&nbsp;''i''&nbsp;≤&nbsp;''n'', σ(''i'') is an integer between 1 and ''d''(''i''). We form the polynomial in the variables <math>x_{11},\\dots,x_{1d(1)},\\dots,x_{n1},\\dots,x_{nd(n)},y_1,\\dots,y_n</math>\n\n: <math>Q(x_{11},\\dots,x_{nd(n)},y_1,\\dots,y_n)=\\prod\\nolimits_{\\sigma\\in S}\\left(x_{1\\sigma(1)}y_1+\\dots+x_{n\\sigma(n)}y_n\\right).</math>\n\nSince the product is over all the possible choice functions σ, ''Q'' is symmetric in <math>x_{i1},\\dots,x_{id(i)}</math> for every ''i''. Therefore ''Q'' is a polynomial with integer coefficients in elementary symmetric polynomials of the above variables, for every ''i'', and in the variables ''y''<sub>''i''</sub>. Each of the latter symmetric polynomials is a rational number when evaluated in <math>a(i)_i,\\dots,a(i)_{d(i)}</math>.\n\nThe evaluated polynomial <math>Q(a(1)_1,\\dots,a(n)_{d(n)},e^{\\alpha(1)},\\dots,e^{\\alpha(n)})</math> vanishes because one of the choices is just σ(''i'') = 1 for all ''i'', for which the corresponding factor vanishes according to our assumption above. Thus, the evaluated polynomial is a sum of the form\n\n: <math>b(1)e^{\\beta(1)}+ b(2)e^{\\beta(2)}+ \\cdots + b(N)e^{\\beta(N)}= 0,</math>\n\nwhere we already grouped the terms with the same exponent. So in the left-hand side we have distinct values β(1), ..., β(''N''), each of which is still algebraic (being a sum of algebraic numbers) and coefficients <math>b(1),\\dots,b(N)\\in\\mathbb Q</math>.\nThe sum is nontrivial: if <math>\\alpha(i)</math> is maximal in the lexicographic order, the coefficient of <math>e^{|S|\\alpha(i)}</math> is just a product of ''a''(''i'')<sub>''j''</sub>'s (with possible repetitions), which is nonzero.\n\nBy multiplying the equation with an appropriate integer factor, we get an identical equation except that now ''b''(1), ..., ''b''(''N'') are all integers. Therefore, according to Lemma B, the equality cannot hold, and we are led to a contradiction which completes the proof. &#8718;\n\nNote that Lemma A is sufficient to prove that ''e'' is irrational, since otherwise we may write ''e'' = ''p'' / ''q'', where both ''p'' and ''q'' are nonzero integers, but by Lemma A we would have ''qe''&nbsp;−&nbsp;''p'' ≠ 0, which is a contradiction. Lemma A also suffices to prove that {{pi}} is irrational, since otherwise we may write {{pi}} = ''k'' / ''n'', where both ''k'' and ''n'' are integers) and then ±''i''{{pi}} are the roots of ''n''<sup>2</sup>''x''<sup>2</sup> + ''k''<sup>2</sup> = 0; thus 2 − 1 − 1 = 2''e''<sup>0</sup> + ''e''<sup>''i''{{pi}}</sup> + ''e''<sup>−''i''{{pi}}</sup> ≠ 0; but this is false.\n\nSimilarly, Lemma B is sufficient to prove that ''e'' is transcendental, since Lemma B says that if ''a''<sub>0</sub>, ..., ''a''<sub>''n''</sub> are integers not all of which are zero, then\n\n: <math>a_ne^n+\\cdots+a_0e^0\\ne 0.</math>\n\nLemma B also suffices to prove that {{pi}} is transcendental, since otherwise we would have 1&nbsp;+&nbsp;''e''<sup>''i''{{pi}}</sup>&nbsp;≠&nbsp;0.\n\n==See also==\n* [[Gelfond–Schneider theorem]]\n* [[Baker's theorem]]; an extension of Gelfond–Schneider theorem\n* [[Schanuel's conjecture]]; if proven, it would imply both the Gelfond–Schneider theorem and the Lindemann–Weierstrass theorem\n\n== Notes ==\n{{Reflist|2}}\n\n== References ==\n*{{Citation | last=Gordan | first=P. | author-link=Paul Gordan | year=1893 | title=Transcendenz von {{math|''e''}} und {{math|&pi;}}. | journal=Mathematische Annalen | volume=43 | pages=222–224 | url=https://gdz.sub.uni-goettingen.de/dms/load/img/?PID=GDZPPN002254557&physid=PHYS_0223 | doi=10.1007/bf01443647}}\n*{{Citation | last=Hermite | first=C. | author-link=Charles Hermite | year=1873 | title=Sur la fonction exponentielle. | journal=Comptes rendus de l'Académie des Sciences de Paris | volume=77 | pages=18–24 | url=http://gallica.bnf.fr/ark:/12148/bpt6k3034n/f18.image}}\n*{{Citation | last=Hermite | first=C. | author-link=Charles Hermite | year=1874 | title=Sur la fonction exponentielle. | publisher=Gauthier-Villars | place=Paris | url=https://archive.org/details/surlafonctionexp00hermuoft}}\n*{{Citation | last=Hilbert | first=D. | author-link=David Hilbert | year=1893 | title=Ueber die Transcendenz der Zahlen {{math|''e''}} und {{math|&pi;}}. | journal=Mathematische Annalen | volume=43 | pages=216–219 | url=https://gdz.sub.uni-goettingen.de/index.php?id=11&PID=GDZPPN002254565 | doi=10.1007/bf01443645 | access-date=2018-12-24 | archive-url=https://web.archive.org/web/20171006113711/https://gdz.sub.uni-goettingen.de/index.php?id=11&PID=GDZPPN002254565 | archive-date=2017-10-06 | dead-url=yes | df= }}\n*{{Citation | last=Lindemann | first=F. | author-link=Ferdinand Lindemann | year=1882 | title=Über die Ludolph'sche Zahl. | journal=Sitzungsberichte der Königlich Preussischen Akademie der Wissenschaften zu Berlin | volume=2 | pages=679–682 | url=https://books.google.co.jp/books?id=YxkYAAAAYAAJ&pg=PA679 |ref={{Harvid|Lindemann|1882a}}}}\n*{{Citation | last=Lindemann | first=F. | author-link=Ferdinand Lindemann | year=1882 | title=Über die Zahl {{math|&pi;}}. | journal=Mathematische Annalen | volume=20 | pages=213–225 | url=https://gdz.sub.uni-goettingen.de/index.php?id=11&PID=GDZPPN002246910 | ref={{Harvid|Lindemann|1882b}} | doi=10.1007/bf01446522 | access-date=2018-12-24 | archive-url=https://web.archive.org/web/20171006120026/https://gdz.sub.uni-goettingen.de/index.php?id=11&PID=GDZPPN002246910 | archive-date=2017-10-06 | dead-url=yes | df= }}\n*{{Citation | last=Weierstrass | first=K. | author-link=Karl Weierstrass | year=1885 | title=Zu Lindemann's Abhandlung. \"Über die Ludolph'sche Zahl\". | journal=Sitzungsberichte der Königlich Preussischen Akademie der Wissen-schaften zu Berlin | volume=5 | pages=1067–1085 | url=https://books.google.co.jp/books?id=jhlEAQAAMAAJ&pg=PA1067}}\n\n== Further reading ==\n*{{Citation | last1=Baker | first1=Alan |authorlink=Alan Baker (mathematician) | title=Transcendental number theory | url={{Google books|SmsCqiQMvvgC|Transcendental number theory|plainurl=yes}} | publisher=[[Cambridge University Press]] | edition=2nd | series=Cambridge Mathematical Library | isbn=978-0-521-39791-9 | mr=0422171 | year=1990}}\n*{{Citation |last=Bertrand |first=D. |author-link=Daniel Bertrand |year=1997 |title=Theta functions and transcendence, |journal=The Ramanujan Journal |volume=1 |issue=4 |pages=339–350 |doi=10.1023/A:1009749608672 }}\n*{{Citation | last=Gelfond | first=A.O. | author-link=Alexander Gelfond | translator-last=Boron | translator-first=Leo F. | translator-link=Leo F. Boron | origyear=1960 | year=2015 | title=Transcendental and Algebraic Numbers, | publisher=[[Dover Publications]] |location=New York |series=Dover Books on Mathematics | isbn=978-0-486-49526-2 |mr=0057921 | url={{Google books|408wBgAAQBAJ|Transcendental and Algebraic Numbers|plainurl=yes}} }}\n\n==External links==\n*{{MathWorld|title=Hermite-Lindemann Theorem|urlname=Hermite-LindemannTheorem}}\n*{{MathWorld|title=Lindemann-Weierstrass Theorem|urlname=Lindemann-WeierstrassTheorem}}\n\n{{DEFAULTSORT:Lindemann-Weierstrass theorem}}\n[[Category:Articles containing proofs]]\n[[Category:E (mathematical constant)]]\n[[Category:Exponentials]]\n[[Category:Pi]]\n[[Category:Theorems in number theory]]\n[[Category:Transcendental numbers]]"
    },
    {
      "title": "List of integrals of exponential functions",
      "url": "https://en.wikipedia.org/wiki/List_of_integrals_of_exponential_functions",
      "text": "The following is a list of [[integral]]s of [[exponential function]]s. For a complete list of integral functions, please see the [[list of integrals]].\n\n== Indefinite integral ==\n\nIndefinite integrals are [[antiderivative]] functions. A constant (the [[constant of integration]]) may be added to the right hand side of any of these formulas, but has been suppressed here in the interest of brevity.\n\n===Integrals of polynomials===\n\n: <math>\\int xe^{cx}\\,dx = e^{cx}\\left(\\frac{cx-1}{c^{2}}\\right)</math>\n\n: <math>\\int x^2 e^{cx}\\,dx = e^{cx}\\left(\\frac{x^2}{c}-\\frac{2x}{c^2}+\\frac{2}{c^3}\\right)</math>\n\n: <math>\\begin{align}\n\\int x^n e^{cx}\\,dx &= \\frac{1}{c} x^n e^{cx} - \\frac{n}{c}\\int x^{n-1} e^{cx} \\,dx \\\\\n&= \\left( \\frac{\\partial}{\\partial c} \\right)^n \\frac{e^{cx}}{c} \\\\\n&= e^{cx}\\sum_{i=0}^n (-1)^i\\frac{n!}{(n-i)!c^{i+1}}x^{n-i} \\\\\n&= e^{cx}\\sum_{i=0}^n (-1)^{n-i}\\frac{n!}{i!c^{n-i+1}}x^i\n\\end{align}</math>\n\n: <math>\\int\\frac{e^{cx}}{x}\\,dx = \\ln|x| +\\sum_{n=1}^\\infty\\frac{(cx)^n}{n\\cdot n!}</math>\n\n: <math>\\int\\frac{e^{cx}}{x^n}\\,dx = \\frac{1}{n-1}\\left(-\\frac{e^{cx}}{x^{n-1}}+c\\int\\frac{e^{cx} }{x^{n-1}}\\,dx\\right) \\qquad\\text{(for }n\\neq 1\\text{)}</math>\n\n===Integrals involving only exponential functions===\n\n: <math>\\int  f'(x)e^{f(x)}\\,dx = e^{f(x)}</math>\n\n: <math>\\int e^{cx}\\,dx = \\frac{1}{c} e^{cx}</math>\n\n: <math>\\int a^{cx}\\,dx = \\frac{1}{c\\cdot \\ln a} a^{cx}\\qquad\\text{ for }a > 0,\\ a \\ne 1</math>\n\n===Integrals involving exponential and trigonometric functions===\n\n: <math>\\begin{align}\n\\int e^{cx}\\sin bx\\,dx &= \\frac{e^{cx}}{c^2+b^2}(c\\sin bx - b\\cos bx) \\\\\n&= \\frac{e^{cx}}{\\sqrt{c^2+b^2}}\\sin(bx-\\phi)\\qquad \\text{where }\\cos(\\phi) = \\frac{c}{\\sqrt{c^2+b^2}}\n\\end{align}</math>\n\n: <math>\\begin{align}\n\\int e^{cx}\\cos bx\\,dx &= \\frac{e^{cx}}{c^2+b^2}(c\\cos bx + b\\sin bx) \\\\\n&= \\frac{e^{cx}}{\\sqrt{c^2+b^2}}\\cos(bx-\\phi)\\qquad \\text{where }\\cos(\\phi) = \\frac{c}{\\sqrt{c^2+b^2}}\n\\end{align}</math>\n\n: <math>\\int e^{cx}\\sin^n x\\,dx = \\frac{e^{cx}\\sin^{n-1} x}{c^2+n^2}(c\\sin x-n\\cos x)+\\frac{n(n-1)}{c^2+n^2}\\int e^{cx}\\sin^{n-2} x\\,dx</math>\n\n: <math>\\int e^{cx}\\cos^n x\\,dx = \\frac{e^{cx}\\cos^{n-1} x}{c^2+n^2}(c\\cos x+n\\sin x)+\\frac{n(n-1)}{c^2+n^2}\\int e^{cx}\\cos^{n-2} x\\,dx</math>\n\n===Integrals involving the error function===\n\nIn the following formulas, {{math|erf}} is the [[error function]] and {{math|Ei}} is the [[exponential integral]].\n\n: <math>\\int e^{cx}\\ln x\\,dx = \\frac{1}{c}\\left(e^{cx}\\ln|x|-\\operatorname{Ei}(cx)\\right)</math>\n\n:<math>\\int x e^{c x^2 }\\,dx= \\frac{1}{2c} e^{c x^2}</math>\n\n:<math>\\int e^{-c x^2 }\\,dx= \\sqrt{\\frac{\\pi}{4c}} \\operatorname{erf}(\\sqrt{c} x)</math>\n\n:<math>\\int xe^{-c x^2 }\\,dx=-\\frac{1}{2c}e^{-cx^2} </math>\n\n: <math>\\int\\frac{e^{-x^2}}{x^2}\\,dx = -\\frac{e^{-x^2}}{x} - \\sqrt{\\pi} \\operatorname{erf} (x) </math>\n\n:<math>\\int {\\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2 }}\\,dx= \\frac{1}{2}\\operatorname{erf}\\left(\\frac{x-\\mu}{\\sigma \\sqrt{2}}\\right)</math>\n\n===Other integrals===\n\n:<math>\\int e^{x^2}\\,dx = e^{x^2}\\left( \\sum_{j=0}^{n-1}c_{2j}\\frac{1}{x^{2j+1}} \\right )+(2n-1)c_{2n-2} \\int \\frac{e^{x^2}}{x^{2n}}\\,dx  \\quad \\text{valid for any } n > 0,  </math>\n::where <math> c_{2j}=\\frac{ 1 \\cdot 3 \\cdot 5 \\cdots (2j-1)}{2^{j+1}}=\\frac{(2j)!}{j!2^{2j+1}} \\ . </math>\n::(Note that the value of the expression is ''independent'' of the value of {{mvar|n}}, which is why it does not appear in the integral.)\n\n:<math> {\\int \\underbrace{x^{x^{\\cdot^{\\cdot^{x}}}}}_mdx= \\sum_{n=0}^m\\frac{(-1)^n(n+1)^{n-1}}{n!}\\Gamma(n+1,- \\ln x) + \\sum_{n=m+1}^\\infty(-1)^na_{mn}\\Gamma(n+1,-\\ln x)  \\qquad\\text{(for }x> 0\\text{)}}</math> \n:: where <math>a_{mn}=\\begin{cases}1   &\\text{if } n = 0, \\\\ \\\\ \\dfrac{1}{n!} &\\text{if } m=1, \\\\ \\\\ \\dfrac{1}{n}\\sum_{j=1}^{n}ja_{m,n-j}a_{m-1,j-1}  &\\text{otherwise} \\end{cases}</math>\n:: and {{math|Γ(''x'',''y'')}} is the [[gamma function]].\n\n:<math>\\int \\frac{1}{ae^{\\lambda x} + b} \\,dx = \\frac{x}{b} - \\frac{1}{b \\lambda} \\ln\\left(a e^{\\lambda x} + b \\right) </math> when <math>b \\neq 0</math>, <math>\\lambda \\neq 0</math>, and <math>ae^{\\lambda x} + b > 0.</math>\n\n:<math>\\int \\frac{e^{2\\lambda x}}{ae^{\\lambda x} + b} \\,dx = \\frac{1}{a^2 \\lambda} \\left[a e^{\\lambda x} + b - b \\ln\\left(a e^{\\lambda x} + b \\right) \\right] </math>  when <math>a \\neq 0</math>, <math>\\lambda \\neq 0</math>, and <math>ae^{\\lambda x} + b > 0.</math>\n\n:<math>\\int \\frac{ae^{cx}-1}{be^{cx}-1}\\,dx=\\frac{(a-b)\\log(1-be^{cx})}{bc}+x.</math>\n\n== Definite integrals ==\n\n: <math>\\begin{align}\n\\int_0^1 e^{x\\cdot \\ln a + (1-x)\\cdot \\ln b}\\,dx\n&= \\int_0^1 \\left(\\frac{a}{b}\\right)^{x}\\cdot b\\,dx \\\\\n&= \\int_0^1 a^{x}\\cdot b^{1-x}\\,dx \\\\\n&= \\frac{a-b}{\\ln a - \\ln b} \\qquad\\text{for } a > 0,\\ b > 0,\\ a \\neq b\n\\end{align}</math>\nThe last expression is the [[logarithmic mean]].\n\n:<math>\\int_0^{\\infty} e^{-ax}\\,dx=\\frac{1}{a} \\quad (\\operatorname{Re}(a)>0)</math>\n\n:<math>\\int_0^{\\infty} e^{-ax^2}\\,dx=\\frac{1}{2} \\sqrt{\\pi \\over a} \\quad  (a>0)</math> (the [[Gaussian integral]])\n\n:<math>\\int_{-\\infty}^{\\infty} e^{-ax^2}\\,dx=\\sqrt{\\pi \\over a} \\quad (a>0)</math>\n:\n:<math>\\int_{-\\infty}^{\\infty} e^{-ax^2 + bx}\\,dx= \\sqrt{\\pi \\over a}e^{\\tfrac{b^2}{4a}} \\quad(a > 0)</math>\n\n:<math>\\int_{-\\infty}^{\\infty} e^{-ax^2} e^{-2bx}\\,dx=\\sqrt{\\frac{\\pi}{a}}e^{\\frac{b^2}{a}} \\quad (a>0)</math> (see [[Integral of a Gaussian function]])\n\n:<math>\\int_{-\\infty}^{\\infty} x e^{-a(x-b)^2}\\,dx= b \\sqrt{\\frac{\\pi}{a}} \\quad (\\operatorname{Re}(a)>0)</math>\n\n:<math>\\int_{-\\infty}^{\\infty} x e^{-ax^2+bx}\\,dx= \\frac{ \\sqrt{\\pi} b }{2a^{3/2}} e^{\\frac{b^2}{4a}} \\quad (\\operatorname{Re}(a)>0)</math>\n\n:<math>\\int_{-\\infty}^{\\infty} x^2 e^{-ax^2}\\,dx=\\frac{1}{2} \\sqrt{\\pi \\over a^3} \\quad (a>0)</math>\n\n:<math>\\int_{-\\infty}^{\\infty} x^2 e^{-ax^2+bx}\\,dx=\\frac{\\sqrt{\\pi}(2a+b^2)}{4a^{5/2}} e^{\\frac{b^2}{4a}} \\quad (\\operatorname{Re}(a)>0)</math>\n\n:<math>\\int_{-\\infty}^{\\infty} x^3 e^{-ax^2+bx}\\,dx=\\frac{\\sqrt{\\pi}(6a+b^2)b}{8a^{7/2}} e^{\\frac{b^2}{4a}} \\quad (\\operatorname{Re}(a)>0)</math>\n\n:<math>\\int_0^{\\infty} x^{n} e^{-ax^2}\\,dx = \n\\begin{cases}\n       \\dfrac{\\Gamma \\left(\\frac{n+1}{2}\\right)}{2\\left(a^\\frac{n+1}{2}\\right) } & (n>-1,\\ a>0) \\\\ \\\\\n       \\dfrac{(2k-1)!!}{2^{k+1}a^k}\\sqrt{\\dfrac{\\pi}{a}} & (n=2k,\\ k \\text{ integer},\\ a>0) \\ \\text{(!! is the double factorial)} \\\\ \\\\\n       \\dfrac{k!}{2(a^{k+1})} & (n=2k+1,\\ k \\text{ integer},\\ a>0)\n\\end{cases} </math>\n(the operator <math>!!</math> is the [[Double factorial]])\n\n:<math>\\int_0^{\\infty} x^n e^{-ax}\\,dx = \n\\begin{cases}\n       \\dfrac{\\Gamma(n+1)}{a^{n+1}} & (n>-1,\\ a>0) \\\\ \\\\\n       \\dfrac{n!}{a^{n+1}} & (n=0,1,2,\\ldots,\\ a>0)\n\\end{cases}</math>\n\n:<math>\\int_0^{1} x^n e^{-ax}\\,dx = \n\\frac{n!}{a^{n+1}}\\left[\n                    1-e^{-a}\\sum_{i=0}^{n} \\frac{a^i}{i!}\n                   \\right]</math>\n\n:<math>\\int_0^\\infty e^{-ax^b} dx =  \\frac{1}{b}\\ a^{-\\frac{1}{b}}\\Gamma\\left(\\frac{1}{b}\\right)</math>\n\n:<math>\\int_0^\\infty x^n e^{-ax^b} dx = \\frac{1}{b}\\ a^{-\\frac{n+1}{b}}\\Gamma\\left(\\frac{n+1}{b}\\right)</math>\n\n:<math>\\int_0^{\\infty} e^{-ax}\\sin bx\\,dx = \\frac{b}{a^2+b^2} \\quad (a>0)</math>\n\n:<math>\\int_0^{\\infty} e^{-ax}\\cos bx\\,dx = \\frac{a}{a^2+b^2} \\quad (a>0)</math>\n\n:<math>\\int_0^{\\infty} xe^{-ax}\\sin bx\\,dx = \\frac{2ab}{(a^2+b^2)^2} \\quad (a>0)</math>\n\n:<math>\\int_0^{\\infty} xe^{-ax}\\cos bx\\,dx = \\frac{a^2-b^2}{(a^2+b^2)^2} \\quad (a>0)</math>\n\n:<math>\\int_0^{2 \\pi} e^{x \\cos \\theta} d \\theta = 2 \\pi I_0(x)</math> ({{math|''I''<sub>0</sub>}} is the [[Bessel function#Modified Bessel functions : I.CE.B1.2C K.CE.B1|modified Bessel function]] of the first kind)\n\n:<math>\\int_0^{2 \\pi} e^{x \\cos \\theta + y \\sin \\theta} d \\theta = 2 \\pi I_0 \\left( \\sqrt{x^2 + y^2} \\right)</math>\n\n==See also==\n*[[Gradshteyn and Ryzhik]]\n\n==Further reading==\n*{{cite book |first=Victor Hugo |last=Moll |title=Special Integrals of Gradshteyn and Ryzhik: the Proofs – Volume I |volume=I |edition=1 |work=Series: Monographs and Research Notes in Mathematics |publisher=[[Chapman and Hall]]/[[CRC Press]] |date=2014-11-12 |isbn=978-1-48225-651-2 |url=http://www.crcpress.com/Special-Integrals-of-Gradshteyn-and-Ryzhik-the-Proofs---Volume-I/Moll/9781482256512 |access-date=2016-02-12}}\n*{{cite book |first=Victor Hugo |last=Moll |title=Special Integrals of Gradshteyn and Ryzhik: the Proofs – Volume II |volume=II |edition=1 |work=Series: Monographs and Research Notes in Mathematics |publisher=[[Chapman and Hall]]/[[CRC Press]] |date=2015-10-27 |isbn=978-1-48225-653-6 |url=http://www.crcpress.com/Special-Integrals-of-Gradshteyn-and-Ryzhik-the-Proofs---Volume-II/Moll/9781482256536 |access-date=2016-02-12}}\n\n==External links==\n* [http://www.wolframalpha.com/calculators/integral-calculator/ Wolfram Mathematica Online Integrator]\n* {{cite web |first=Victor Hugo |last=Moll |title=<!-- This is a -->List with the formulas and proofs in GR |url=http://www.math.tulane.edu/~vhm/Table.html |access-date=2016-02-12}}\n\n{{Lists of integrals}}\n\n{{DEFAULTSORT:List Of Integrals Of Exponential Functions}}\n[[Category:Exponentials]]\n[[Category:Integrals|Exponential functions]]\n[[Category:Mathematics-related lists|Integrals of exponential functions]]"
    },
    {
      "title": "List of integrals of hyperbolic functions",
      "url": "https://en.wikipedia.org/wiki/List_of_integrals_of_hyperbolic_functions",
      "text": "The following is a list of [[integral]]s ([[anti-derivative]] functions) of [[hyperbolic function]]s. For a complete list of integral functions, see [[list of integrals]].\n\nIn all formulas the constant ''a'' is assumed to be nonzero, and ''C''\ndenotes the [[constant of integration]].\n\n==Integrals involving only hyperbolic sine functions==\n\n<math>\\int\\sinh ax\\,dx = \\frac{1}{a}\\cosh ax+C</math>\n\n<math>\\int\\sinh^2 ax\\,dx = \\frac{1}{4a}\\sinh 2ax - \\frac{x}{2}+C</math>\n\n<math>\\int\\sinh^n ax\\,dx = \\frac{1}{an}(\\sinh^{n-1} ax)(\\cosh ax) - \\frac{n-1}{n}\\int\\sinh^{n-2} ax\\,dx \\qquad\\mbox{(for }n>0\\mbox{)}</math>\n\n: also: <math>\\int\\sinh^n ax\\,dx = \\frac{1}{a(n+1)}(\\sinh^{n+1} ax)(\\cosh ax) - \\frac{n+2}{n+1}\\int\\sinh^{n+2}ax\\,dx \\qquad\\mbox{(for }n<0\\mbox{, }n\\neq -1\\mbox{)}</math>\n\n\n<math>\\int\\frac{dx}{\\sinh ax} = \\frac{1}{a} \\ln\\left|\\tanh\\frac{ax}{2}\\right|+C</math>\n\n: also: <math>\\int\\frac{dx}{\\sinh ax} = \\frac{1}{a} \\ln\\left|\\frac{\\cosh ax - 1}{\\sinh ax}\\right|+C</math>\n\n: <math>\\int\\frac{dx}{\\sinh ax} = \\frac{1}{a} \\ln\\left|\\frac{\\sinh ax}{\\cosh ax + 1}\\right|+C</math>\n\n: <math>\\int\\frac{dx}{\\sinh ax} = \\frac{1}{2a} \\ln\\left|\\frac{\\cosh ax - 1}{\\cosh ax + 1}\\right|+C</math>\n\n\n<math>\\int\\frac{dx}{\\sinh^n ax} = -\\frac{\\cosh ax}{a(n-1)\\sinh^{n-1} ax}-\\frac{n-2}{n-1}\\int\\frac{dx}{\\sinh^{n-2} ax} \\qquad\\mbox{(for }n\\neq 1\\mbox{)}</math>\n\n<math>\\int x\\sinh ax\\,dx = \\frac{1}{a} x\\cosh ax - \\frac{1}{a^2}\\sinh ax+C</math>\n\n<math>\\int (\\sinh ax)(\\sinh bx)\\,dx = \\frac{1}{a^2-b^2} \\big(a(\\sinh bx)(\\cosh ax) - b(\\cosh bx)(\\sinh ax)\\big)+C \\qquad\\mbox{(for }a^2\\neq b^2\\mbox{)}</math>\n\n==Integrals involving only hyperbolic cosine functions==\n\n<math>\\int\\cosh ax\\,dx = \\frac{1}{a}\\sinh ax+C</math>\n\n<math>\\int\\cosh^2 ax\\,dx = \\frac{1}{4a}\\sinh 2ax + \\frac{x}{2}+C</math>\n\n<math>\\int\\cosh^n ax\\,dx = \\frac{1}{an}(\\sinh ax)(\\cosh^{n-1} ax) + \\frac{n-1}{n}\\int\\cosh^{n-2} ax\\,dx \\qquad\\mbox{(for }n>0\\mbox{)}</math>\n\n: also: <math>\\int\\cosh^n ax\\,dx = -\\frac{1}{a(n+1)}(\\sinh ax)(\\cosh^{n+1} ax) + \\frac{n+2}{n+1}\\int\\cosh^{n+2}ax\\,dx \\qquad\\mbox{(for }n<0\\mbox{, }n\\neq -1\\mbox{)}</math>\n\n\n<math>\\int\\frac{dx}{\\cosh ax} = \\frac{2}{a} \\arctan e^{ax}+C</math>\n\n: also: <math>\\int\\frac{dx}{\\cosh ax} = \\frac{1}{a} \\arctan (\\sinh ax)+C</math>\n\n\n<math>\\int\\frac{dx}{\\cosh^n ax} = \\frac{\\sinh ax}{a(n-1)\\cosh^{n-1} ax}+\\frac{n-2}{n-1}\\int\\frac{dx}{\\cosh^{n-2} ax} \\qquad\\mbox{(for }n\\neq 1\\mbox{)}</math>\n\n<math>\\int x\\cosh ax\\,dx = \\frac{1}{a} x\\sinh ax - \\frac{1}{a^2}\\cosh ax+C</math>\n\n<math>\\int x^2 \\cosh ax\\,dx = -\\frac{2x \\cosh ax}{a^2} + \\left(\\frac{x^2}{a}+\\frac{2}{a^3}\\right) \\sinh ax+C</math>\n\n<math>\\int (\\cosh ax)(\\cosh bx)\\,dx = \\frac{1}{a^2-b^2} \\big(a(\\sinh ax)(\\cosh bx) - b(\\sinh bx)(\\cosh ax)\\big)+C \\qquad\\mbox{(for }a^2\\neq b^2\\mbox{)}</math>\n\n==Other integrals==\n\n===Integrals of hyperbolic tangent, cotangent, secant, cosecant functions===\n\n<math>\\int \\tanh x \\, dx = \\ln \\cosh x + C</math>\n\n<math>\\int\\tanh^2 ax\\,dx = x - \\frac{\\tanh ax}{a}+C</math>\n\n<math>\\int \\tanh^n ax\\,dx = -\\frac{1}{a(n-1)}\\tanh^{n-1} ax+\\int\\tanh^{n-2} ax\\,dx \\qquad\\mbox{(for }n\\neq 1\\mbox{)}</math>\n\n<math>\\int \\coth x \\, dx = \\ln| \\sinh x | + C , \\text{ for } x \\neq 0 </math>\n\n<math>\\int \\coth^n ax\\,dx = -\\frac{1}{a(n-1)}\\coth^{n-1} ax+\\int\\coth^{n-2} ax\\,dx \\qquad\\mbox{(for }n\\neq 1\\mbox{)}</math>\n\n<math>\\int \\operatorname{sech}\\,x \\, dx = \\arctan\\,(\\sinh x) + C</math>\n\n<math>\\int \\operatorname{csch}\\,x \\, dx = \\ln\\left| \\tanh {x \\over2}\\right| + C , \\text{ for } x \\neq 0 </math>\n\n===Integrals involving hyperbolic sine and cosine functions===\n\n<math>\\int (\\cosh ax)(\\sinh bx)\\,dx = \\frac{1}{a^2-b^2} \\big(a(\\sinh ax)(\\sinh bx) - b(\\cosh ax)(\\cosh bx)\\big)+C \\qquad\\mbox{(for }a^2\\neq b^2\\mbox{)}</math>\n\n<math>\\int\\frac{\\cosh^n ax}{\\sinh^m ax} dx = \\frac{\\cosh^{n-1} ax}{a(n-m)\\sinh^{m-1} ax} + \\frac{n-1}{n-m}\\int\\frac{\\cosh^{n-2} ax}{\\sinh^m ax} dx \\qquad\\mbox{(for }m\\neq n\\mbox{)}</math>\n\n: also: <math>\\int\\frac{\\cosh^n ax}{\\sinh^m ax} dx = -\\frac{\\cosh^{n+1} ax}{a(m-1)\\sinh^{m-1} ax} + \\frac{n-m+2}{m-1}\\int\\frac{\\cosh^n ax}{\\sinh^{m-2} ax} dx \\qquad\\mbox{(for }m\\neq 1\\mbox{)}</math>\n\n: <math>\\int\\frac{\\cosh^n ax}{\\sinh^m ax} dx = -\\frac{\\cosh^{n-1} ax}{a(m-1)\\sinh^{m-1} ax} + \\frac{n-1}{m-1}\\int\\frac{\\cosh^{n-2} ax}{\\sinh^{m-2} ax} dx \\qquad\\mbox{(for }m\\neq 1\\mbox{)}</math>\n\n: <math>\\int\\frac{\\sinh^m ax}{\\cosh^n ax} dx = \\frac{\\sinh^{m-1} ax}{a(m-n)\\cosh^{n-1} ax} + \\frac{m-1}{n-m}\\int\\frac{\\sinh^{m-2} ax}{\\cosh^n ax} dx \\qquad\\mbox{(for }m\\neq n\\mbox{)}</math>\n\n: <math>\\int\\frac{\\sinh^m ax}{\\cosh^n ax} dx = \\frac{\\sinh^{m+1} ax}{a(n-1)\\cosh^{n-1} ax} + \\frac{m-n+2}{n-1}\\int\\frac{\\sinh^m ax}{\\cosh^{n-2} ax} dx \\qquad\\mbox{(for }n\\neq 1\\mbox{)}</math>\n\n: <math>\\int\\frac{\\sinh^m ax}{\\cosh^n ax} dx = -\\frac{\\sinh^{m-1} ax}{a(n-1)\\cosh^{n-1} ax} + \\frac{m-1}{n-1}\\int\\frac{\\sinh^{m -2} ax}{\\cosh^{n-2} ax} dx \\qquad\\mbox{(for }n\\neq 1\\mbox{)}</math>\n\n===Integrals involving hyperbolic and trigonometric functions===\n\n<math>\\int \\sinh (ax+b)\\sin (cx+d)\\,dx = \\frac{a}{a^2+c^2}\\cosh(ax+b)\\sin(cx+d)-\\frac{c}{a^2+c^2}\\sinh(ax+b)\\cos(cx+d)+C</math>\n\n<math>\\int \\sinh (ax+b)\\cos (cx+d)\\,dx = \\frac{a}{a^2+c^2}\\cosh(ax+b)\\cos(cx+d)+\\frac{c}{a^2+c^2}\\sinh(ax+b)\\sin(cx+d)+C</math>\n\n<math>\\int \\cosh (ax+b)\\sin (cx+d)\\,dx = \\frac{a}{a^2+c^2}\\sinh(ax+b)\\sin(cx+d)-\\frac{c}{a^2+c^2}\\cosh(ax+b)\\cos(cx+d)+C</math>\n\n<math>\\int \\cosh (ax+b)\\cos (cx+d)\\,dx = \\frac{a}{a^2+c^2}\\sinh(ax+b)\\cos(cx+d)+\\frac{c}{a^2+c^2}\\cosh(ax+b)\\sin(cx+d)+C</math>\n\n{{Lists of integrals}}\n\n[[Category:Exponentials]]\n[[Category:Integrals|Hyperbolic functions]]\n[[Category:Mathematics-related lists|Integrals of hyperbolic functions]]"
    },
    {
      "title": "Marshall–Olkin exponential distribution",
      "url": "https://en.wikipedia.org/wiki/Marshall%E2%80%93Olkin_exponential_distribution",
      "text": "{{Probability distribution\n  | name       = Marshall–Olkin exponential\n  | type       = continuous\n  | pdf_image  = \n  | cdf_image  = \n  | parameters = \n  | support    = <math>x\\in  [0, \\infty)^b</math>\n  | pdf        = \n  | cdf        = \n  | quantile   = \n  | mean       = \n  | median     = \n  | mode       = \n  | variance   = \n  | skewness   = \n  | kurtosis   = \n  | entropy    = \n  | mgf        = \n  | char       = \n  | rate       = \n  | fisher     = \n  }}\n\nIn applied statistics, the '''Marshall–Olkin exponential distribution''' is any member of a certain family of continuous multivariate probability distributions with positive-valued components. It was introduced by Albert W. Marshall and [[Ingram Olkin]].<ref>{{citation|first1=Albert W.|last1= Marshall|first2= Ingram|last2= Olkin|author2-link=Ingram Olkin| title=A multivariate exponential distribution|journal=[[Journal of the American Statistical Association]] |volume= 62|issue=317|pages= 30–49|year=1967|doi=10.2307/2282907|mr=0215400}}</ref> \nOne of its main uses is in reliability theory, where the Marshall–Olkin copula models the dependence between random variables subjected to external shocks. \n<ref>{{citation|first1=Z.|last1= Botev|first2=P.|last2= L'Ecuyer|first3=R.|last3= Simard|first4=B.|last4= Tuffin|title=Static network reliability estimation under the Marshall-Olkin copula|journal=ACM Transactions on Modeling and Computer Simulation (TOMACS) |volume= 26|issue=2|pages=No.14|year=2016|doi=10.1145/2775106}}</ref>\n<ref>{{citation|first1=F.|last1= Durante|first2=S.|last2= Girard|first3=G.|last3= Mazo|title=Marshall--Olkin type copulas generated by a global shock|journal=Journal of Computational and Applied Mathematics|volume= 296|pages=638-648|year=2016}}</ref>\n\n==Definition==\nLet <math> \\{E_B : \\varnothing \\ne B\\subset \\{1,2,\\ldots,b\\}\\}</math> be a set of independent, [[exponential distribution|exponentially distributed]] [[random variable]]s, where <math> E_B </math> has mean <math> 1/\\lambda_B </math>. Let\n\n:<math> T_j=\\min\\{E_B:j\\in B\\},\\ \\ j=1,\\ldots,b. </math>\n\nThe joint distribution of <math> T=(T_1,\\ldots,T_b) </math> is called the Marshall–Olkin exponential distribution with parameters <math> \\{\\lambda _B,B\\subset \\{1,2,\\ldots,b\\}\\}.</math>\n\n=== Concrete example ===\n\nSuppose ''b''&nbsp;=&nbsp;3. Then there are seven nonempty subsets of {&nbsp;1,&nbsp;...,&nbsp;''b''&nbsp;} =&nbsp;{&nbsp;1,&nbsp;2,&nbsp;3&nbsp;}; hence seven different exponential random variables:\n: <math>\nE_{\\{1\\}}, E_{\\{2\\}}, E_{\\{3\\}}, E_{\\{1,2\\}}, E_{\\{1,3\\}}, E_{\\{2,3\\}}, E_{\\{1,2,3\\}}\n</math>\nThen we have:\n: <math>\n\\begin{align}\nT_1 & = \\min\\{ E_{\\{1\\}}, E_{\\{1,2\\}}, E_{\\{1,3\\}}, E_{\\{1,2,3\\}} \\} \\\\\nT_2 & = \\min\\{ E_{\\{2\\}}, E_{\\{1,2\\}}, E_{\\{2,3\\}}, E_{\\{1,2,3\\}} \\} \\\\\nT_3 & = \\min\\{ E_{\\{3\\}}, E_{\\{1,3\\}}, E_{\\{2,3\\}}, E_{\\{1,2,3\\}} \\} \\\\\n\\end{align}\n</math>\n\n==References==\n\n{{reflist}}\n* Xu M, Xu S. \"An Extended Stochastic Model for Quantitative Security Analysis of Networked Systems\". ''Internet Mathematics'', 2012, 8(3): 288–320.\n\n{{DEFAULTSORT:Marshall-Olkin exponential distribution}}\n[[Category:Statistics articles needing expert attention]]\n[[Category:Continuous distributions]]\n[[Category:Exponentials]]\n[[Category:Exponential family distributions]]"
    },
    {
      "title": "Moore's second law",
      "url": "https://en.wikipedia.org/wiki/Moore%27s_second_law",
      "text": "'''Rock's law''' or '''Moore's second law''', named for [[Arthur Rock]] or [[Gordon Moore]], says that the cost of a [[integrated circuit|semiconductor chip]] [[semiconductor fabrication plant|fabrication plant]] doubles every four years.<ref>{{cite web |first= |last= |title=FAQs |date= |work= |publisher=India Electronics & Semiconductor Association |url=http://www.iesaonline.org/aboutus/faq_pg3.html}}</ref> As of 2015, the price had already reached about 14 billion US dollars.<ref>{{cite web |first=Lucian |last=Armasu |title=Samsung's New $14 Billion Chip Plant To Manufacture DRAM, Processors In 2017 |agency=Reuters |date=8 May 2015 |work= |publisher=Tom's Hardware |url=http://www.tomshardware.com/news/samsung-14-billion-chip-plant,29058.html}}</ref>\n\nRock's law can be seen as the economic flip side to [[Moore's law|Moores (first) law]] – that the number of transistors in a dense integrated circuit doubles every two years. The latter is a direct consequence of the ongoing growth of the capital-intensive semiconductor industry&mdash; innovative and popular products mean more profits, meaning more capital available to invest in ever higher levels of [[Integrated circuit#LSI|large-scale integration]], which in turn leads to the creation of even more innovative products.\n\nThe semiconductor industry has always been extremely capital-intensive, with ever-dropping manufacturing [[unit cost]]s. Thus, the ultimate [[limits to growth]] of the industry will constrain the maximum amount of capital that can be invested in new products; at some point, Rock's Law will collide with Moore's Law.<ref>{{cite web |url=http://www.edavision.com/200111/feature.pdf |format=PDF |archive-date=6 May 2006 |last=Dorsch |first=Jeff |title=Does Moore's Law Still Hold Up? |publisher=Edavision.com |deadurl=bot: unknown |archiveurl=https://web.archive.org/web/20060506114410/http://www.edavision.com/200111/feature.pdf |df= }}</ref><ref>{{cite web |url=http://research.microsoft.com/~gray/Moore_Law.html |archive-date=13 November 2008 |last=Schaller |first=Bob |date=1996 |title=The Origin, Nature, and Implications of 'Moore's Law' |publisher=Research.Microsoft.com |deadurl=bot: unknown |archiveurl=https://web.archive.org/web/20081113014641/http://research.microsoft.com/~gray/Moore_Law.html |df= }}</ref><ref>{{cite journal |first=Jean-François |last=Tremblay |title=Riding On Flat Panels |journal=Chemical & Engineering News |volume=84 |issue=26 |pages=13–16 |date=26 June 2006 |doi= 10.1021/cen-v084n026.p013|url=http://cen.acs.org/articles/84/i26/Riding-Flat-Panels.html}}</ref>\n\nIt has been suggested that fabrication plant costs have not increased as quickly as predicted by Rock's law – indeed plateauing in the late 1990s<ref name=Ross03>{{cite journal |first=Philip E. |last=Ross |title=5 Commandments |journal=IEEE Spectrum |volume= |issue= |pages= |year=2003 |doi= |url=http://spectrum.ieee.org/semiconductors/materials/5-commandments/2}}</ref> – and also that the fabrication plant cost ''per transistor'' (which has shown a pronounced downward trend<ref name=Ross03/>) may be more relevant as a constraint on Moore's Law.\n\n==See also==\n*[[Semiconductor device fabrication]]\n*[[Fabless manufacturing]]\n*[[Semiconductor consolidation]]\n\n==References==\n<references/>\n\n==External links==\n*{{cite web |first=Michael |last=Kanellos |title=Soaring costs of chipmaking recast industry |date=2003 |work=CNET |publisher=News.com |url=http://news.com.com/Semi+survival/2009-1001_3-981418.html |deadurl=bot: unknown |archiveurl=https://archive.is/20130119214941/http://news.com.com/Semi+survival/2009-1001_3-981418.html |archivedate=2013-01-19 |df= }}\n\n{{Computer laws}}\n\n[[Category:Adages]]\n[[Category:Computer architecture statements]]\n[[Category:Computer industry]]\n[[Category:Computing culture]]\n[[Category:Electronic design]]\n[[Category:Exponentials]]\n[[Category:Rules of thumb]]"
    },
    {
      "title": "Natural exponential family",
      "url": "https://en.wikipedia.org/wiki/Natural_exponential_family",
      "text": "In [[probability]] and [[statistics]], a '''natural exponential family (NEF)''' is a class of [[probability distribution]]s that is a special case of an [[exponential family]] (EF). Every distribution possessing a [[moment-generating function]] is a member of a natural exponential family, and the use of such distributions simplifies the theory and computation of [[generalized linear models]].\n\n== Definition ==\n\n=== Probability distribution function (PDF) of the univariate case (scalar domain, scalar parameter) ===\n\nThe natural exponential families (NEF) are a subset of the [[exponential families]]. A NEF is an exponential family in which the natural parameter ''&eta;'' and the natural statistic ''T''(''x'') are both the identity.  A distribution in an [[exponential family]] with parameter ''&theta;'' can be written with [[probability density function]] (PDF)\n:<math> f_X(x\\mid  \\theta) = h(x)\\ \\exp\\Big(\\ \\eta(\\theta) T(x) - A(\\theta)\\ \\Big) \\,\\! ,</math>\nwhere <math>h(x)</math> and <math>A(\\theta)</math> are known functions.\nA distribution in a natural exponential family with parameter &theta; can thus be written with PDF\n:<math> f_X(x\\mid \\theta) = h(x)\\ \\exp\\Big(\\ \\theta x - A(\\theta)\\ \\Big) \\,\\! .</math>\n \n[Note that slightly different notation is used by the originator of the NEF, Carl Morris.<ref name=Morris2006>Morris C. (2006) \"Natural exponential families\", ''Encyclopedia of Statistical Sciences''.</ref>  Morris uses ''&omega;'' instead of ''&eta;'' and ''&psi;'' instead of ''A''.]\n\n=== Probability distribution function (PDF) of the general case (multivariate domain and/or parameter) ===\nSuppose that <math>\\mathbf{x} \\in \\mathcal{X} \\subseteq \\mathbb{R}^p</math>, then a natural exponential family of order ''p'' has density or mass function of the form:\n:<math> f_X(\\mathbf{x} \\mid \\boldsymbol\\theta) = h(\\mathbf{x})\\ \\exp\\Big(\\boldsymbol\\theta^{\\rm T} \\mathbf{x} - A(\\boldsymbol\\theta)\\ \\Big) \\,\\! ,</math>\nwhere in this case the parameter <math>\\boldsymbol\\theta \\in \\mathbb{R}^p .</math>\n\n=== Moment and cumulant generating function ===\nA member of a natural exponential family has [[moment generating function]] (MGF) of the form\n\n:<math>M_X(\\mathbf{t}) = \\exp\\Big(\\ A(\\boldsymbol\\theta + \\mathbf{t}) - A(\\boldsymbol\\theta)\\ \\Big) \\, .</math>\n\nThe [[cumulant generating function]] is by definition the logarithm of the MGF, so it is\n\n:<math>K_X(\\mathbf{t}) =  A(\\boldsymbol\\theta + \\mathbf{t}) - A(\\boldsymbol\\theta) \\, .</math>\n\n== Examples ==\nThe five most important univariate cases are:\n* [[normal distribution]] with known variance\n* [[Poisson distribution]]\n* [[gamma distribution]] with known shape parameter ''&alpha;'' (or ''k'' depending on notation set used)\n* [[binomial distribution]] with known number of trials, ''n''\n* [[negative binomial distribution]] with known <math>r</math>\n\nThese five examples &ndash; Poisson, binomial, negative binomial, normal, and gamma &ndash; are a special subset of NEF, called NEF with quadratic [[variance function]] (NEF-QVF) because the variance can be written as a quadratic function of the mean. NEF-QVF are discussed below.\n\nDistributions such as the [[exponential distribution|exponential]], [[chi-squared distribution|chi-squared]], [[Rayleigh distribution|Rayleigh]], [[Weibull distribution|Weibull]], [[Bernoulli distribution|Bernoulli]], and [[geometric distribution]]s are special cases of the above five distributions. Many common distributions are either NEF or can be related to the NEF. For example: the [[chi-squared distribution]] is a special case of the [[gamma distribution]]. The [[Bernoulli distribution]] is a [[binomial distribution]] with ''n''&nbsp;=&nbsp;1&nbsp;trial. The [[exponential distribution]] is a gamma distribution with shape parameter &alpha;&nbsp;=&nbsp;1 (or ''k''&nbsp;=&nbsp;1&nbsp;). The [[Rayleigh distribution|Rayleigh]] and [[Weibull distribution]]s can each be written in terms of an exponential distribution.\n\nSome exponential family distributions are not NEF. The [[lognormal]] and [[Beta distribution]] are in the exponential family, but not the natural exponential family.\n\nThe parameterization of most of the above distributions has been written differently from the parameterization commonly used in textbooks and the above linked pages. For example, the above parameterization differs from the parameterization in the linked article in the Poisson case. The two parameterizations are related by <math> \\theta = \\log(\\lambda) </math>, where &lambda; is the mean parameter, and so that the density may be written as\n:<math>f(k;\\theta) = \\frac{1}{k!} \\exp\\Big(\\ \\theta\\ k - \\exp(\\theta)\\ \\Big) \\ ,</math>\nfor <math> \\theta \\in \\mathbb{R}</math>, so \n:<math>h(k) = \\frac{1}{k!}, \\text{ and } A(\\theta) =  \\exp(\\theta)\\ .</math>\n\nThis alternative parameterization can greatly simplify calculations in [[mathematical statistics]]. For example, in [[Bayesian inference]], a [[posterior probability distribution]] is calculated as the product of two distributions. Normally this calculation requires writing out the probability distribution functions (PDF) and integrating; with the above parameterization, however, that  calculation can be avoided. Instead, relationships between distributions can be abstracted due to the properties of the NEF described below.\n   \nAn example of the multivariate case is the [[multinomial distribution]] with known number of trials.\n\n== Properties ==\n\nThe properties of the natural exponential family can be used to simplify calculations involving these distributions.\n\n=== Univariate case ===\n1.  The cumulants of an NEF can be calculated as derivatives of the NEF's cumulant generating function.  The nth cumulant is the nth derivative of the cumulant generating function with respect to ''t'' evaluated at ''t'' = 0.\n\nThe [[cumulant generating function]] is\n:<math>K_X(t) =  A(\\theta + t) - A(\\theta) \\, .</math>\nThe first cumulant is \n:<math> \\kappa_1 = K_X'(t)\\Big|_{t = 0} = \\left. \\frac{d}{d t} A(\\theta + t) \\right|_{t = 0} \\, .</math> \nThe mean is the first moment and always equal to the first cumulant, so \n\n:<math> \\mu_1 = \\kappa_1 = \\operatorname{E}[X] = K'_X(0) = A'(\\theta)\\, .</math>\n\nThe variance is always the second cumulant, and it is always related to the first and second moments by \n:<math> \\operatorname{Var}[X] = \\kappa_2 = \\mu_2 - \\mu_1^2 \\, ,</math>\nso that\n:<math> \\operatorname{Var}[X] = K''_X(0) = A''(\\theta) \\, .</math>\n\nLikewise, the ''n''th cumulant is \n:<math> \\kappa_n = A^{(n)}(\\theta) \\, .</math>\n\n2. Natural exponential families (NEF) are closed under convolution.{{citation needed|date=July 2012}}\n\nGiven  [[independent identically distributed]] (iid) <math>X_1,\\ldots,X_n</math> with distribution from an NEF, then <math>\\sum_{i=1}^n X_i\\,</math> is an NEF, although not necessarily the original NEF.  This follows from the properties of the cumulant generating function.\n\n3.  The [[variance function]] for random variables with an NEF distribution can be written in terms of the mean.{{citation needed|date=July 2012}}\n\n:<math>\\operatorname{Var}(X) = V(\\mu).</math>\n\n4.  The first two moments of a NEF distribution uniquely specify the distribution within that family of distributions.{{citation needed|date=July 2012}}\n\n: <math> X \\sim \\operatorname{NEF} [\\mu, V(\\mu)] .</math>\n\n=== Multivariate case ===\n\nIn the multivariate case, the mean vector and covariance matrix are{{citation needed|date=July 2012}}\n\n:<math> \\operatorname{E}[X] = \\nabla A(\\boldsymbol\\theta) \\text{ and } \\operatorname{Cov}[X] = \\nabla \\nabla^{\\rm T}  A(\\boldsymbol\\theta)\\, ,</math>\n\nwhere<math>\\nabla</math> is the [[gradient]] and <math>\\nabla \\nabla^{\\rm T} </math> is the [[Hessian matrix]].\n\n==<span id=\"NEF-QVF\"></span>Natural exponential families with quadratic variance functions (NEF-QVF)==\n\nA special case of the natural exponential families are those with quadratic variance functions.\nSix NEFs have quadratic variance functions (QVF) in which the variance of the distribution can be written as a quadratic function of the mean.  These are called NEF-QVF. The properties of these distributions were first described by [[Carl Morris (statistician)|Carl Morris]].<ref>Morris C. (1982) \"Natural exponential families with quadratic variance functions\".  ''Ann. Stat.'', 10(1), 65&ndash;80.</ref>\n\n: <math> \\operatorname{Var}(X) = V(\\mu) = \\nu_0 + \\nu_1 \\mu + \\nu_2 \\mu^2.</math>\n\n=== The six NEF-QVFs ===\n\nThe six NEF-QVF are written here in increasing complexity of the relationship between variance and mean.\n\n1. The normal distribution with fixed variance <math>X \\sim  N(\\mu, \\sigma^2) </math> is NEF-QVF because the variance is constant.  The variance can be written <math> \\operatorname{Var}(X) = V(\\mu) = \\sigma^2</math>, so variance is a degree 0 function of the mean.\n\n2. The Poisson distribution <math>X \\sim \\operatorname{Poisson}(\\mu) </math> is NEF-QVF because all Poisson distributions have variance equal to the mean <math>\\operatorname{Var}(X) = V(\\mu) = \\mu</math>, so variance is a linear function of the mean.\n\n3. The Gamma distribution <math>X \\sim \\operatorname{Gamma}(r, \\lambda) </math> is NEF-QVF because the mean of the Gamma distribution is <math>\\mu = r\\lambda</math> and the variance of the Gamma distribution is <math>\\operatorname{Var}(X) = V(\\mu) = \\mu^2/r</math>, so the variance is a quadratic function of the mean.\n\n4. The binomial distribution <math> X \\sim \\operatorname{Binomial}(n, p) </math> is NEF-QVF because the mean is <math>\\mu = np</math> and the variance is <math> \\operatorname{Var}(X) = np(1-p) </math> which can be written in terms of the mean as\n<math>V(X) = - np^2 + np  = -\\mu^2/n + \\mu.</math>\n\n5. The negative binomial distribution <math> X \\sim \\operatorname{NegBin}(n, p) </math> is NEF-QVF because the mean is <math>\\mu = np/(1-p)</math> and the variance is\n<math>V(\\mu) = \\mu^2/n + \\mu.</math>\n\n6.  The (not very famous) distribution generated by the generalized{{clarify|date=July 2012}} [[hyperbolic secant distribution]] (NEF-GHS) has{{citation needed|date=July 2012}}  <math>V(\\mu) = \\mu^2/n +n</math> and <math>\\mu > 0.</math>\n\n=== Properties of NEF-QVF ===\n\nThe properties of NEF-QVF can simplify calculations that use these distributions.\n\n1.  Natural exponential families with quadratic variance functions (NEF-QVF) are closed under convolutions of a linear transformation.{{citation needed|date=July 2012}}   That is, a convolution of a linear transformation of an NEF-QVF is also an NEF-QVF, although not necessarily the original one.\n\nGiven  [[independent identically distributed]] (iid) <math>X_1,\\ldots,X_n</math> with distribution from a NEF-QVF.  A convolution of a linear transformation of an NEF-QVF is also an NEF-QVF.\n\nLet <math>Y = \\sum_{i=1}^n (X_i - b)/c \\,</math> be the convolution of a linear transformation of ''X''. \nThe mean of ''Y'' is <math> \\mu^* = n(\\mu - b)/c \\,</math>.  The variance of ''Y'' can be written in terms of the variance function of the original NEF-QVF. If the original NEF-QVF had variance function\n\n: <math> \\operatorname{Var}(X) = V(\\mu) = \\nu_0 + \\nu_1 \\mu + \\nu_2 \\mu^2,</math>\n\nthen the new NEF-QVF has variance function\n:<math> \\operatorname{Var}(Y) = V^*(\\mu^*) = \\nu^*_0 + \\nu^*_1 \\mu + \\nu^*_2 \\mu^2 ,</math>\nwhere\n:<math> \\nu^*_0 = nV(b)/c^2 \\, ,</math>\n:<math> \\nu^*_1 = V'(b)/c \\, ,</math>\n:<math> \\nu^*_2/n = \\nu_2/n \\, .</math>\n\n2. Let <math> X_1</math> and <math>X_2</math> be independent NEF with the same parameter &theta; and let <math> Y = X_1 + X_2 </math>.  Then the conditional distribution of <math>X_1</math> given <math>Y</math> has quadratic variance in <math>Y</math> if and only if <math> X_1</math> and <math>X_2</math> are NEF-QVF.  Examples of such conditional distributions are the [[normal distribution|normal]], [[binomial distribution|binomial]], [[beta distribution|beta]], [[hypergeometric distribution|hypergeometric]] and [[geometric distribution]]s, which are not all NEF-QVF.<ref name=Morris2006/>\n\n3. NEF-QVF have [[conjugate prior distribution]]s on &mu; in the Pearson system of distributions (also called the [[Pearson distribution]] although the Pearson system of distributions is actually a family of distributions rather than a single distribution.)  Examples of conjugate prior distributions of NEF-QVF distributions are the [[normal distribution|normal]], [[gamma distribution|gamma]], reciprocal gamma, [[Beta distribution|beta]], [[F-distribution|F-]], and [[Student's t-distribution|t-]] distributions.  Again, these conjugate priors are not all NEF-QVF.<ref name=Morris2006/>\n\n4. If <math> X \\mid \\mu </math> has an NEF-QVF distribution and &mu; has a conjugate prior distribution then the marginal distributions are well-known distributions.<ref name=Morris2006/>\n\nThese properties together with the above notation can simplify calculations in [[mathematical statistics]] that would normally be done using complicated calculations and calculus.\n\n{{one source|date=June 2012}}\n{{refimprove|date=June 2012}}\n\n== References ==\n{{Reflist}}\n\n* Morris C.  (1982) ''Natural exponential families with quadratic variance functions:  statistical theory''.  Dept of mathematics, Institute of Statistics, University of Texas, Austin.\n\n{{ProbDistributions|families}}\n\n[[Category:Exponentials]]\n[[Category:Types of probability distributions]]"
    },
    {
      "title": "P-adic exponential function",
      "url": "https://en.wikipedia.org/wiki/P-adic_exponential_function",
      "text": "{{DISPLAYTITLE:''p''-adic exponential function}}\nIn [[mathematics]], particularly [[P-adic analysis|''p''-adic analysis]], the '''''p''-adic exponential function''' is a ''p''-adic analogue of the usual [[exponential function]] on the [[complex numbers]]. As in the complex case, it has an inverse function, named the '''''p''-adic logarithm'''.\n\n==Definition==\nThe usual exponential function on '''C''' is defined by the infinite series\n:<math>\\exp(z)=\\sum_{n=0}^\\infty \\frac{z^n}{n!}.</math>\nEntirely analogously, one defines the exponential function on '''C'''<sub>''p''</sub>, the completion of the algebraic closure of '''Q'''<sub>''p''</sub>, by\n:<math>\\exp_p(z)=\\sum_{n=0}^\\infty\\frac{z^n}{n!}.</math>\nHowever, unlike exp which converges on all of '''C''', exp<sub>''p''</sub> only converges on the disc\n:<math>|z|_p<p^{-1/(p-1)}.</math>\nThis is because ''p''-adic series converge if and only if the summands tend to zero, and since the ''n''! in the denominator of each summand tends to make them very large ''p''-adically, rather a small value of ''z'' is needed in the numerator.\n\n==''p''-adic logarithm function==\n\nThe power series\n:<math>\\log(1+x)=\\sum_{n=1}^\\infty \\frac{(-1)^{n+1}x^n}{n},</math>\nconverges for ''x'' in '''C'''<sub>''p''</sub> satisfying |''x''|<sub>''p''</sub>&nbsp;&lt;&nbsp;1 and so defines the '''''p''-adic logarithm function''' log<sub>''p''</sub>(''z'') for |''z''&nbsp;&minus;&nbsp;1|<sub>''p''</sub>&nbsp;&lt;&nbsp;1 satisfying the usual property log<sub>''p''</sub>(''zw'')&nbsp;=&nbsp;log<sub>''p''</sub>''z''&nbsp;+&nbsp;log<sub>''p''</sub>''w''. The function log<sub>''p''</sub> can be extended to all of {{SubSup|'''C'''|''p''|×}} (the set of nonzero elements of '''C'''<sub>''p''</sub>) by imposing that it continues to satisfy this last property and setting log<sub>''p''</sub>(''p'')&nbsp;=&nbsp;0. Specifically, every element ''w'' of {{SubSup|'''C'''|''p''|×}} can be written as ''w''&nbsp;=&nbsp;''p<sup>r</sup>''·ζ·''z'' with ''r'' a rational number, ζ a root of unity, and |''z''&nbsp;&minus;&nbsp;1|<sub>''p''</sub>&nbsp;&lt;&nbsp;1,<ref>{{harvnb|Cohen|2007|loc=Proposition 4.4.44}}</ref> in which case log<sub>''p''</sub>(''w'')&nbsp;=&nbsp;log<sub>''p''</sub>(''z'').<ref>In factoring ''w'' as above, there is a choice of a root involved in writing ''p<sup>r</sup>'' since ''r'' is rational; however, different choices differ only by multiplication by a root of unity, which gets absorbed into the factor ζ.</ref> This function on {{SubSup|'''C'''|''p''|×}} is sometimes called the '''Iwasawa logarithm''' to emphasize the choice of log<sub>''p''</sub>(''p'')&nbsp;=&nbsp;0. In fact, there is an extension of the logarithm from |''z''&nbsp;&minus;&nbsp;1|<sub>''p''</sub>&nbsp;&lt;&nbsp;1 to all of {{SubSup|'''C'''|''p''|×}} for each choice of log<sub>''p''</sub>(''p'') in '''C'''<sub>''p''</sub>.<ref>{{harvnb|Cohen|2007|loc=§4.4.11}}</ref>\n\n==Properties==\n\nIf ''z'' and ''w'' are both in the radius of convergence for exp<sub>''p''</sub>, then their sum is too and we have the usual addition formula: exp<sub>''p''</sub>(''z''&nbsp;+&nbsp;''w'')&nbsp;=&nbsp;exp<sub>''p''</sub>(''z'')exp<sub>''p''</sub>(''w'').\n\nSimilarly if ''z'' and ''w'' are nonzero elements of '''C'''<sub>''p''</sub> then log<sub>''p''</sub>(''zw'')&nbsp;=&nbsp;log<sub>''p''</sub>''z''&nbsp;+&nbsp;log<sub>''p''</sub>''w''.\n\nFor ''z'' in the domain of exp<sub>''p''</sub>, we have exp<sub>''p''</sub>(log<sub>''p''</sub>(1+''z''))&nbsp;=&nbsp;1+''z'' and log<sub>''p''</sub>(exp<sub>''p''</sub>(''z''))&nbsp;=&nbsp;''z''.\n\nThe roots of the Iwasawa logarithm log<sub>''p''</sub>(''z'') are exactly the elements of '''C'''<sub>''p''</sub> of the form ''p<sup>r</sup>''·ζ where ''r'' is a rational number and ζ is a root of unity.<ref>{{harvnb|Cohen|2007|loc=Proposition 4.4.45}}</ref>\n\nNote that there is no analogue in '''C'''<sub>''p''</sub> of [[Euler's identity]], ''e''<sup>2''πi''</sup>&nbsp;=&nbsp;1.  This is a corollary of [[Strassmann's theorem]].\n\nAnother major difference to the situation in '''C''' is that the domain of convergence of exp<sub>''p''</sub> is much smaller than that of log<sub>''p''</sub>.  A modified exponential function &mdash; the [[Artin–Hasse exponential]] &mdash; can be used instead which converges on |''z''|<sub>''p''</sub>&nbsp;&lt;&nbsp;1.\n\n==Notes==\n\n{{reflist}}\n\n==References==\n* Chapter 12 of {{cite book | last=Cassels | first=J. W. S. | authorlink=J. W. S. Cassels | title=Local fields | series=[[London Mathematical Society|London Mathematical Society Student Texts]] | publisher=[[Cambridge University Press]] | year=1986 | isbn=0-521-31525-5 }}\n*{{Citation\n| last=Cohen\n| first=Henri\n| author-link=Henri Cohen (number theorist)\n| title=Number theory, Volume I: Tools and Diophantine equations\n| publisher=Springer\n| location=New York\n| series=[[Graduate Texts in Mathematics]]\n| volume=239\n| year=2007\n| isbn=978-0-387-49922-2\n| mr=2312337\n| doi=10.1007/978-0-387-49923-9\n}}\n\n==External links==\n* {{planetmath reference|id=7000|title=p-adic exponential and p-adic logarithm}}\n\n[[Category:Exponentials]]\n[[Category:p-adic numbers]]"
    },
    {
      "title": "Pentation",
      "url": "https://en.wikipedia.org/wiki/Pentation",
      "text": "[[File:Pentation.jpg|thumb|The first three values of the expression ''x''[5]2. The value of 3[5]2 is about 7.626 × 10<sup>12</sup>; values for higher ''x'' are much too large to appear on the graph.]]\nIn [[mathematics]], '''pentation''' is the next [[hyperoperation]] after [[tetration]] but before hexation. It is defined as [[iterated]] (repeated) tetration, just as tetration is iterated [[exponentiation]].<ref>{{citation\n | last = Perstein | first = Millard H.\n | date = June 1962\n | doi = 10.1145/367766.368160\n | issue = 6\n | journal = [[Communications of the ACM]]\n | page = 344\n | title = Algorithm 93: General Order Arithmetic\n | volume = 5}}.</ref> It is a [[binary operation]] defined with two numbers ''a'' and ''b'', where ''a'' is tetrated to itself ''b'' times. For instance, using [[hyperoperation]] notation for pentation and tetration, <math>2[5]3</math> means tetrating 2 to itself 3 times, or <math>2[4](2[4]2)</math>. This can then be reduced to <math>2[4](2^2)=2[4]4=2^{2^{2^2}}=2^{2^4}=2^{16}=65536.</math>\n\n==Etymology==\nThe word \"pentation\" was coined by [[Reuben Goodstein]] in 1947 from the roots [[penta-]] (five) and [[iterated function|iteration]]. It is part of his general naming scheme for [[hyperoperation]]s.<ref>{{citation\n | last = Goodstein | first = R. L. | authorlink = Reuben Goodstein\n | journal = [[The Journal of Symbolic Logic]]\n | mr = 0022537\n | pages = 123–129\n | title = Transfinite ordinals in recursive number theory\n | volume = 12\n | year = 1947}}.</ref>\n\n==Notation==\nThere is little consensus on the notation for pentation; as such, there are many different ways to write the operation. However, some are more used than others, and some have clear advantages or disadvantages compared to others.\n\n*Pentation can be written as a [[hyperoperation]] as <math>a[5]b</math>. In this format, <math>a[3]b</math> may be interpreted as the result of [[Iterated function|repeatedly applying]] the function <math>x\\mapsto a[2]x</math>, for <math>b</math> repetitions, starting from the number&nbsp;1. Analogously, <math>a[4]b</math>, tetration, represents the value obtained by repeatedly applying the function <math>x\\mapsto a[3]x</math>, for <math>b</math> repetitions, starting from the number&nbsp;1, and the pentation <math>a[5]b</math> represents the value obtained by repeatedly applying the function <math>x\\mapsto a[4]x</math>, for <math>b</math> repetitions, starting from the number&nbsp;1.<ref>{{citation\n | last = Knuth | first = D. E. | author-link = Donald Knuth\n | doi = 10.1126/science.194.4271.1235\n | journal = [[Science (journal)|Science]]\n | pages = 1235–1242\n | title = Mathematics and computer science: Coping with finiteness\n | volume = 194\n | year = 1976\n | pmid=17797067\n | issue=4271}}.</ref><ref>{{citation\n | last1 = Blakley | first1 = G. R.\n | last2 = Borosh | first2 = I.\n | doi = 10.1016/0001-8708(79)90052-5\n | issue = 2\n | journal = [[Advances in Mathematics]]\n | mr = 549780\n | pages = 109–136\n | title = Knuth's iterated powers\n | volume = 34\n | year = 1979}}.</ref> This will be the notation used in the rest of the article.\n\n*In [[Knuth's up-arrow notation]], <math>a[5]b</math> is represented as <math>a \\uparrow \\uparrow \\uparrow b</math> or <math>a \\uparrow^{3}b</math>. In this notation, <math>a\\uparrow b</math> represents the exponentiation function <math>a^b</math> and <math>a\\uparrow \\uparrow b</math> represents tetration.The operation can be easily adapted for hexation by adding another arrow.\n\n*In [[Conway chained arrow notation]], <math>a[5]b = a\\rightarrow b\\rightarrow 3</math>.<ref>{{citation|title=The Book of Numbers|first1=John Horton|last1=Conway|author1-link=John Horton Conway|first2=Richard|last2=Guy|author2-link=Richard K. Guy|publisher=Springer|year=1996|isbn=9780387979939|page=61|url=https://books.google.com/books?id=0--3rcO7dMYC&pg=PA61}}.</ref> \n\n*Another proposed notation is <math>{_{b}a}</math>, though this is not extensible to higher hyperoperations.<ref>http://www.tetration.org/Tetration/index.html</ref>\n\n== Examples ==\nThe values of the pentation function may also be obtained from the values in the fourth row of the table of values of a variant of the [[Ackermann function]]: if <math>A(n,m)</math> is defined by the Ackermann recurrence <math>A(m-1,A(m,n-1))</math> with the initial conditions <math>A(1,n)=an</math> and <math>A(m,1)=a</math>, then <math>a[5]b=A(4,b)</math>.<ref>{{citation\n | last = Nambiar | first = K. K.\n | doi = 10.1016/0893-9659(95)00084-4\n | issue = 6\n | journal = Applied Mathematics Letters\n | mr = 1368037\n | pages = 51–53\n | title = Ackermann functions and transfinite ordinals\n | volume = 8\n | year = 1995}}.</ref>\n\nAs tetration, its base operation, has not been extended to non-integer heights, pentation <math>a[5]b</math> is currently only defined for integer values of ''a'' and ''b'' where ''a'' > 0 and ''b'' ≥ −1, and a few other integer values which ''may'' be uniquely defined. As with all hyperoperations of order 3 ([[exponentiation]]) and higher, pentation has the following trivial cases (identities) which holds for all values of ''a'' and ''b'' within its domain:\n\n* <math>1[5]b = 1</math>\n* <math>a[5]1 = a</math>\n\nAdditionally, we can also define:\n\n* <math>a[5]0 = 1</math>\n* <math>a[5](-1) = 0</math>\n\nOther than the trivial cases shown above, pentation generates extremely large numbers very quickly such that there are only a few non-trivial cases that produce numbers that can be written in conventional notation, as illustrated below:\n* <math>2[5]2 = 2[4]2 = 2^2 = 4</math>\n* <math>2[5]3 = 2[4](2[4]2) = 2[4]4 = 2^{2^{2^2}} = 2^{2^4} = 2^{16} = 65,536</math>\n* <math>2[5]4 = 2[4](2[4](2[4]2)) = 2[4](2[4]4) = 2[4]65536 = 2^{2^{2^{\\cdot^{\\cdot^{\\cdot^{2}}}}}} \\mbox{ (a power tower of height 65,536) } \\approx \\exp_{10}^{65,533}(4.29508)</math> (shown here in iterated exponential notation as it is far too large to be written in conventional notation. Note <math> \\exp_{10}(n) = 10^n </math>)\n* <math>3[5]2 = 3[4]3 = 3^{3^3} = 3^{27} = 7,625,597,484,987</math>\n* <math>3[5]3 = 3[4](3[4]3) = 3[4]7,625,597,484,987 = 3^{3^{3^{\\cdot^{\\cdot^{\\cdot^{3}}}}}} \\mbox{ (a power tower of height 7,625,597,484,987) } \\approx \\exp_{10}^{7,625,597,484,986}(1.09902)</math>\n* <math>4[5]2 = 4[4]4 =  4^{4^{4^4}} = 4^{4^{256}} \\approx \\exp_{10}^3(2.19)</math> (a number with over 10<sup>153</sup> digits)\n* <math>5[5]2 = 5[4]5 = 5^{5^{5^{5^5}}} = 5^{5^{5^{3125}}} \\approx \\exp_{10}^4(3.33928)</math> (a number with more than 10<sup>10<sup>2184</sup></sup> digits)\n\n==See also==\n*[[Ackermann function]]\n*[[Bowers's operators]]\n*[[Large numbers]]\n*[[Graham's number]]\n*[[History of large numbers]]\n\n==References==\n{{reflist}}\n\n{{Hyperoperations}}\n{{Large numbers}}\n\n[[Category:Exponentials]]\n[[Category:Large numbers]]\n[[Category:Binary operations]]\n[[Category:Arithmetic]]"
    },
    {
      "title": "Power law",
      "url": "https://en.wikipedia.org/wiki/Power_law",
      "text": "{{cleanup HTML|date=February 2019}}<!-- article uses <cite> tag; see [[Wikipedia:HTML 5#cite]] for cleanup help -->\n{{distinguish|Force (law)}}\n{{Other uses|Power (disambiguation){{!}}Power}}\n[[Image:Long tail.svg|thumb|300px|right|An example power-law graph, being used to demonstrate ranking of popularity. To the right is the [[long tail]], and to the left are the few that dominate (also known as the [[Pareto principle|80–20 rule]]).]]\n\nIn [[statistics]], a '''power law''' is a functional relationship between two quantities, where a relative change in one quantity results in a proportional relative change in the other quantity, independent of the initial size of those quantities: one quantity varies as a [[Exponentiation|power]] of another. For instance, considering the area of a square in terms of the length of its side, if the length is doubled, the area is multiplied by a factor of four.<ref>{{cite web | url=http://www.necsi.edu/guide/concepts/powerlaw.html | title=Concepts: Power Law | publisher=New England Complex Systems Institute | accessdate=18 August 2015 | author=Yaneer Bar-Yam}}</ref>\n\n==Empirical examples==\nThe distributions of a wide variety of physical, biological, and man-made phenomena approximately follow a power law over a wide range of magnitudes: these include the sizes of craters on the [[moon]] and of [[solar flare]]s,<ref name= Newman/> the foraging pattern of various species,<ref name=Humphries>{{cite journal |vauthors=Humphries NE, Queiroz N, Dyer JR, Pade NG, Musyl MK, Schaefer KM, Fuller DW, Brunnschweiler JM, Doyle TK, Houghton JD, Hays GC, Jones CS, Noble LR, Wearmouth VJ, Southall EJ, Sims DW | year = 2010| title = Environmental context explains Lévy and Brownian movement patterns of marine predators | url = | journal = Nature | volume = 465 | issue = 7301| pages = 1066–1069| doi = 10.1038/nature09116 | pmid = 20531470 |bibcode = 2010Natur.465.1066H }}</ref> the sizes of activity patterns of neuronal populations,<ref name=Klaus>{{cite journal |vauthors=Klaus A, Yu S, Plenz D | year = 2011 | title = Statistical Analyses Support Power Law Distributions Found in Neuronal Avalanches | journal = PLoS ONE | volume = 6 | issue = 5| pages = e19779 | doi = 10.1371/journal.pone.0019779 | editor1-last = Zochowski | editor1-first = Michal | pmid = 21720544 | pmc = 3102672|bibcode = 2011PLoSO...619779K }}</ref> the frequencies of [[word]]s in most languages, frequencies of [[family name]]s, the [[species richness]] in [[clades]] of organisms,<ref>{{cite book\n|editor1-last=Albert|editor1-first=J. S.\n|editor2-first=R. E.|editor2-last=Reis\n|year=2011\n|title=Historical Biogeography of Neotropical Freshwater Fishes\n|publisher=University of California Press\n|location=Berkeley\n|url=http://www.ucpress.edu/book.php?isbn=9780520268685\n}}</ref> the sizes of [[power outage]]s, criminal charges per convict, volcanic eruptions,<ref>{{Cite journal|last=Cannavò|first=Flavio|last2=Nunnari|first2=Giuseppe|date=2016-03-01|title=On a Possible Unified Scaling Law for Volcanic Eruption Durations|journal=Scientific Reports|language=en|volume=6|doi=10.1038/srep22289|issn=2045-2322|pmc=4772095|pmid=26926425|page=22289|bibcode=2016NatSR...622289C}}</ref> human judgements of stimulus intensity<ref>Stevens, S. S. (1957). On the psychophysical law. Psychological Review, 64, 153-181</ref><ref>Staddon, J. E. R.  (1978).  Theory of behavioral power functions. Psychological Review, 85, 305-320.</ref> and many other quantities.{{sfn|Clauset|Shalizi|Newman|2009}} Few empirical distributions fit a power law for all their values, but rather follow a power law in the tail.\n[[Acoustic attenuation]] follows frequency power-laws within wide frequency bands for many complex media. [[allometric scaling| Allometric scaling laws ]] for relationships between biological variables are among the best known power-law functions in nature.\n\n==Properties==\n\n===Scale invariance===\nOne attribute of power laws is their [[scale invariance#Scale invariance of functions and self-similarity|scale invariance]]. Given a relation <math>f(x) = ax^{-k}</math>, scaling the argument <math>x</math> by a constant factor <math>c</math> causes only a proportionate scaling of the function itself. That is,\n\n:<math>f(c x) = a(c x)^{-k} = c^{-k} f(x) \\propto f(x),\\!</math>\n\nwhere <math>\\propto</math> denotes [[direct proportionality]]. That is, scaling by a constant <math>c</math> simply multiplies the original power-law relation by the constant <math>c^{-k}</math>. Thus, it follows that all power laws with a particular scaling exponent are equivalent up to constant factors, since each is simply a scaled version of the others. This behavior is what produces the linear relationship when logarithms are taken of both <math>f(x)</math> and <math>x</math>, and the straight-line on the log–log plot is often called the ''signature'' of a power law. With real data, such straightness is a necessary, but not sufficient, condition for the data following a power-law relation. In fact, there are many ways to generate finite amounts of data that mimic this signature behavior, but, in their asymptotic limit, are not true power laws (e.g., if the generating process of some data follows a [[Log-normal distribution]]).{{citation needed|date=July 2015}} Thus, accurately fitting and [[#Validating power laws|validating power-law]] models is an active area of research in statistics; see below.\n\n===Lack of well-defined average value===\nA power-law <math>x^{-k}</math> has a well-defined [[mean]] over <math>x \\in [1,\\infty)</math> only if <math> k > 2 </math>, and it has a finite [[variance]] only if <math>k >3</math>; most identified power laws in nature have exponents such that the mean is well-defined but the variance is not, implying they are capable of [[black swan theory|black swan]] behavior.<ref>{{Cite journal|arxiv=cond-mat/0412004|title=Power laws, Pareto distributions and Zipf's law|journal=Cities|volume=30|issue=2005|pages=323–351|last1= Newman|first1=M. E. J.|last2=Reggiani|first2=Aura|last3=Nijkamp|first3=Peter|year=2005|doi=10.1016/j.cities.2012.03.001|bibcode=2005ConPh..46..323N}}</ref> This can be seen in the following thought experiment:<ref name=\"CCSSCS9\">9na CEPAL Charlas Sobre Sistemas Complejos Sociales (CCSSCS): Leyes de potencias, https://www.youtube.com/watch?v=4uDSEs86xCI</ref> imagine a room with your friends and estimate the average monthly income in the room. Now imagine the [[world's richest person]] entering the room, with a monthly income of about 1 [[1,000,000,000|billion]] US$. What happens to the average income in the room? Income is distributed according to a power-law known as the [[Pareto distribution]] (for example, the net worth of Americans is distributed according to a power law with an exponent of 2).\n\nOn the one hand, this makes it incorrect to apply traditional statistics that are based on [[variance]] and [[standard deviation]] (such as [[regression analysis]]).{{citation needed|date=July 2015}} On the other hand, this also allows for cost-efficient interventions.<ref name=\"CCSSCS9\"/> For example, given that car exhaust is distributed according to a power-law among cars (very few cars contribute to most contamination) it would be sufficient to eliminate those very few cars from the road to reduce total exhaust substantially.<ref>Malcolm Gladwell (2006), Million-Dollar Murray; {{cite web |url=http://gladwell.com/million-dollar-murray/ |title=Archived copy |accessdate=2015-06-14 |deadurl=yes |archiveurl=https://web.archive.org/web/20150318142026/http://gladwell.com/million-dollar-murray/ |archivedate=2015-03-18 |df= }}</ref>\n\nThe median does exist, however: for a power law ''x''<sup> –''k''</sup>, with exponent {{tmath|k > 1}}, it takes the value 2<sup>1/(''k'' – 1)</sup>''x''<sub>min</sub>, where ''x''<sub>min</sub> is the minimum value for which the power law holds.<ref>[https://arxiv.org/pdf/cond-mat/0412004.pdf Newman, Mark EJ. \"Power laws, Pareto distributions and Zipf's law.\" Contemporary physics 46.5 (2005): 323-351.]</ref>\n\n===Universality===\nThe equivalence of power laws with a particular scaling exponent can have a deeper origin in the dynamical processes that generate the power-law relation. In physics, for example, [[phase transition]]s in thermodynamic systems are associated with the emergence of power-law distributions of certain quantities, whose exponents are referred to as the [[critical exponent]]s of the system. Diverse systems with the same critical exponents—that is, which display identical scaling behaviour as they approach [[critical point (thermodynamics)|criticality]]—can be shown, via [[renormalization group]] theory, to share the same fundamental dynamics. For instance, the behavior of water and CO<sub>2</sub> at their boiling points fall in the same universality class because they have identical critical exponents.{{citation needed|date=July 2015}}{{clarify|date=July 2015}} In fact, almost all material phase transitions are described by a small set of universality classes. Similar observations have been made, though not as comprehensively, for various [[self-organized criticality|self-organized critical]] systems, where the critical point of the system is an [[attractor]].  Formally, this sharing of dynamics is referred to as [[universality (dynamical systems)|universality]], and systems with precisely the same critical exponents are said to belong to the same [[renormalization group#Relevant and irrelevant operators and universality classes|universality class]].\n<!--\nCOMMENT: Rather than spin-glasses I'd like a concrete reference of two distinct systems that share the same universality class.  I'd also like a reference about the how-common-is-universality issue. -->\n<!--RESPONSE: Discussion of water and CO2 satisfies first request. Can someone else satisfy the second part? -->\n<!--COMMENT: Note that scale-invariance is not necessarily observed for power-law-''tailed'' equations.  For example, the [[Lévy distribution]] does not display the above property.-->\n<!--RESPONSE: Any function that asymptotically follows a power law relation is scale invariant, by the definition given in this article. -->\n\n==Power-law functions==\nScientific interest in power-law relations stems partly from the ease with which certain general classes of mechanisms generate them.{{sfn|Sornette|2006}} The demonstration of a power-law relation in some data can point to specific kinds of mechanisms that might underlie the natural phenomenon in question, and can indicate a deep connection with other, seemingly unrelated systems;{{sfn|Simon|1955}} see also [[#Universality|universality]] above. The ubiquity of power-law relations in physics is partly due to [[dimensional analysis|dimensional constraints]], while in [[complex systems]], power laws are often thought to be signatures of hierarchy or of specific [[stochastic processes]]. A few notable examples of power laws are [[Pareto principle|Pareto's law]] of income distribution, structural self-similarity of [[fractals]], and [[allometric law|scaling laws in biological systems]]. Research on the origins of power-law relations, and efforts to observe and validate them in the real world, is an active topic of research in many fields of science, including [[physics]], [[computer science]], [[linguistics]], [[geophysics]], [[neuroscience]], [[sociology]], [[economics]] and more.\n\nHowever, much of the recent interest in power laws comes from the study of [[probability distributions]]: The distributions of a wide variety of quantities seem to follow the power-law form, at least in their upper tail (large events). The behavior of these large events connects these quantities to the study of [[extreme value theory|theory of large deviations]] (also called [[extreme value theory]]), which considers the frequency of extremely rare events like [[stock market crash]]es and large [[natural disaster]]s. It is primarily in the study of statistical distributions that the name \"power law\" is used.\n\nIn empirical contexts, an approximation to a power-law <math>o(x^k)</math> often includes a deviation term <math>\\varepsilon</math>, which can represent uncertainty in the observed values (perhaps measurement or sampling errors) or provide a simple way for observations to deviate from the power-law function (perhaps for [[stochastic process|stochastic]] reasons):\n\n:<math>y = ax^k + \\varepsilon.\\!</math>\n\nMathematically, a strict power law cannot be a probability distribution, but a distribution that is a truncated [[power function]] is possible: <math>p(x) = C x^{-\\alpha}</math> for <math>x > x_\\text{min}</math> where the exponent <math>\\alpha</math> (Greek letter [[alpha]], not to be confused with scaling factor <math>a</math> used above) is greater than 1 (otherwise the tail has infinite area), the minimum value <math>x_\\text{min}</math> is needed otherwise the distribution has infinite area as ''x'' approaches 0, and the constant ''C'' is a scaling factor to ensure that the total area is 1, as required by a probability distribution. More often one uses an asymptotic power law – one that is only true in the limit; see [[#Power-law probability distributions|power-law probability distributions]] below for details. Typically the exponent falls in the range <math>2 < \\alpha < 3</math>, though not always.{{sfn|Clauset|Shalizi|Newman|2009}}\n\n===Examples===\nMore than a hundred power-law distributions have been identified in physics (e.g. sandpile avalanches), biology (e.g. species extinction and body mass), and the social sciences (e.g. city sizes and income).<ref>{{cite journal | last1 = Andriani | first1 = P. | last2 = McKelvey | first2 = B. | year = 2007 | title = Beyond Gaussian averages: redirecting international business and management research toward extreme events and power laws | url = | journal = Journal of International Business Studies | volume = 38 | issue = 7| pages = 1212–1230 | doi = 10.1057/palgrave.jibs.8400324 }}</ref> Among them are:\n====Astronomy====\n*[[Kepler's third law]]\n* The [[initial mass function]] of stars\n*The differential energy spectrum of [[Cosmic ray|cosmic-ray]] nuclei\n*The [[M-sigma relation]]\n\n====Physics====\n*The [[Angstrom exponent]] in aerosol optics\n*The frequency-dependency of [[acoustic attenuation]] in complex media\n*The [[Stevens' power law]] of psychophysics\n*The [[Stefan–Boltzmann law]]\n*The input-voltage–output-current curves of [[field-effect transistor]]s and [[vacuum tubes]] approximate a [[Electronic amplifier#Square-law|square-law]] relationship, a factor in \"[[tube sound]]\".\n*[[Square-cube law]] (ratio of surface area to volume)\n*A 3/2-power law can be found in the [[Current–voltage characteristic|plate characteristic curves]] of [[triode]]s.\n*The [[inverse-square law]]s of [[Newtonian gravity]] and [[electrostatics]], as evidenced by the [[gravitational potential]] and [[Electrostatic potential]], respectively.\n*[[Self-organized criticality]] with a [[Critical point (thermodynamics)|critical point]] as an [[attractor]]\n*Model of [[van der Waals force]]\n*Force and potential in [[simple harmonic motion]]\n*[[Gamma correction]] relating light intensity with voltage\n*[[Phase transition#Critical exponents and universality classes|Behaviour near second-order phase transitions]] involving [[critical exponent]]s\n*The [[safe operating area]] relating to maximum simultaneous current and voltage in power semiconductors.\n*Supercritical [[state of matter]] and [[supercritical fluids]], such as supercritical exponents of [[heat capacity]] and [[viscosity]].<ref>{{Cite journal|doi=10.1038/ncomms3331 |pmid=23949085|title=Thermodynamic behaviour of supercritical matter|year=2013|last1=Bolmatov|first1=D.|last2=Brazhkin|first2=V. V.|last3=Trachenko|first3=K.|journal=Nature Communications|volume=4|pages=2331|arxiv = 1303.3153 |bibcode=2013NatCo...4E2331B}}</ref>\n*The [[Curie-von Schweidler law]] in dielectric responses to step DC voltage input.\n* The damping force over speed relation in antiseismic dampers calculus\n\n====Biology====\n*[[Kleiber's law]] relating animal metabolism to size, and [[allometric law]]s in general\n* The two-thirds power law, relating speed to curvature in the human [[motor system]].\n* The [[Taylor's law]] relating mean population size and variance of populations sizes in ecology\n*Neuronal avalanches<ref name=Klaus/>\n* The species richness (number of species) in clades of freshwater fishes<ref>{{cite book|author=Albert, J. S., H. J. Bart, & R. E. Reis|year=2011|chapter=Species richness & cladal diversity|pages=89–104|title=Historical Biogeography of Neotropical Freshwater Fishes|editor=Albert, J. S., & R. E. Reis|publisher=University of California Press|location=Berkeley}}</ref>\n*The Harlow Knapp effect, where a subset of the kinases found in the human body compose a majority of published research<ref>{{Cite journal|last=Yu|first=Frank H.|last2=Willson|first2=Timothy|last3=Frye|first3=Stephen|last4=Edwards|first4=Aled|last5=Bader|first5=Gary D.|last6=Isserlin|first6=Ruth|date=2011-02-02|title=The human genome and drug discovery after a decade. Roads (still) not taken|journal=Nature|volume=470|issue=7333|pages=163–5|language=en|doi=10.1038/470163a|pmid=21307913|arxiv=1102.0448v2}}</ref>\n\n====Meteorology====\n* The size of rain-shower cells,<ref name=Machado>{{cite journal|vauthors=Machado L, Rossow, WB| title=Structural characteristics and radial properties of tropical cloud clusters | journal= Monthly Weather Review | volume = 121 | issue=12 | pages=3234–3260 | doi=10.1175/1520-0493(1993)121<3234:scarpo>2.0.co;2 | year=1993}}</ref> energy dissipation in cyclones,<ref name=Corral>{{cite journal|vauthors= Corral, A, Osso, A, Llebot, JE| year=2010| title=Scaling of tropical cyclone dissipation | journal = Nature Physics | volume = 6| issue=9| pages= 693–696| doi=10.1038/nphys1725| arxiv=0910.0054| bibcode=2010NatPh...6..693C}}</ref> and the diameters of [[dust devils]] on Earth and Mars <ref name=Lorenz>{{cite journal|vauthors=Lorenz RD| year=2009| title=Power Law of Dust Devil Diameters on Earth and Mars | journal=Icarus | volume = 203 | issue=2| pages=683–684 | doi=10.1016/j.icarus.2009.06.029 | bibcode=2009Icar..203..683L}}</ref>\n\n====General science====\n*[[Exponential growth]] and random observation (or killing)<ref name=\"ReedHughes\">Reed W. J.; Hughes B. D. [http://www.math.uvic.ca/faculty/reed/PhysRevPowerLawTwoCol.pdf \"From gene families and genera to incomes and internet file sizes: Why power laws are so common in nature\"]. Phys Rev E, 2002, 66, 067103</ref>\n*Progress through [[exponential growth]] and exponential [[diffusion of innovations]]<ref name=\"HilbertPowerLaw\">{{cite journal|doi=10.1002/cplx.21485 | volume=19 | issue=4 | title=Scale-free power-laws as interaction between progress and diffusion | year=2013 | journal=Complexity | pages=56–65 | last1 = Hilbert | first1 = Martin| bibcode=2014Cmplx..19d..56H | url=http://www.escholarship.org/uc/item/1nb8n94b | type=Submitted manuscript }}</ref>\n*[[Highly optimized tolerance]]\n*Proposed form of [[Experience curve effects#The experience curve|experience curve effects]]\n*[[Pink noise]]\n*The law of stream numbers, and the law of stream lengths ([[Robert E. Horton|Horton]]'s laws describing river systems)<ref>{{Cite web|url=http://www.engr.colostate.edu/~ramirez/ce_old/classes/cive322-Ramirez/CE322_Web/Example_Horton_html.htm|title=Horton's Laws – Example|website=www.engr.colostate.edu|access-date=2018-09-30}}</ref>\n*Populations of cities ([[Gibrat's law]]){{citation needed|date=June 2012}}\n*[[Bibliogram]]s, and frequencies of words in a text ([[Zipf's law]])<ref>{{Cite journal|last=Li|first=W.|date=November 1999|title=Random texts exhibit Zipf's-law-like word frequency distribution|journal=IEEE Transactions on Information Theory|volume=38|issue=6|pages=1842–1845|doi=10.1109/18.165464|issn=0018-9448}}</ref>\n*[[90–9–1 principle]] on [[wiki]]s (also referred to as the [[1% rule (Internet culture)|1% rule]]){{citation needed|date=June 2012}}\n*Richardson's Law for the severity of violent conflicts (wars and terrorism)<ref>{{cite book|author=Lewis Fry Richardson|title=The Statistics of Deadly Quarrels|year=1950}}</ref>\n*The relationship between a CPU's cache size and the number of cache misses follows the [[power law of cache misses]].\n*The spectral density of the weight matrices of deep neural networks<ref>{{cite arxiv|last=Martin|first=Charles H.|last2=Mahoney|first2=Michael W.|date=2018-10-02|title=Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning|eprint=1810.01075|class=cs.LG}}</ref>\n\n====Mathematics====\n*[[Fractal]]s\n*[[Pareto distribution]] and the [[Pareto principle]] also called the \"80–20 rule\"\n*[[Zipf's law]] in corpus analysis and population distributions amongst others, where frequency of an item or event is inversely proportional to its frequency rank (i.e. the second most frequent item/event occurs half as often as the most frequent item, the third most frequent item/event occurs one third as often as the most frequent item, and so on).\n*[[Zeta distribution]] (discrete)\n*[[Yule–Simon distribution]] (discrete)\n*[[Student's t-distribution]] (continuous), of which the [[Cauchy distribution]] is a special case\n*[[Lotka's law]]\n*The [[scale-free network]] model\n\n====Economics====\n*Distribution of artists by the average price of their artworks.<ref>{{cite journal | last1 = Etro | first1 = F. | last2 = Stepanova | first2 = E. | year = 2018 | title = Power-laws in art | url = | journal = Physica A: Statistical Mechanics and its Applications | volume = 506 | pages = 217–220 | doi = 10.1016/j.physa.2018.04.057 | bibcode = 2018PhyA..506..217E }}</ref>\n\n*Distribution of income in a market economy.\n\n*Distribution of degrees in banking networks.\n\n===Variants===\n\n====Broken power law====\n[[File:Plot of various initial mass functions.svg|thumb|Some models of the [[initial mass function]] use a broken power law; here Kroupa (2001) in red.]]\nA broken power law is a [[piecewise function]], consisting of two or more power laws, combined with a threshold. For example, with two power laws:<ref>{{cite journal |url=http://iopscience.iop.org/1538-4357/640/1/L5/fulltext/20296.text.html |title=Afterglow Light Curves and Broken Power Laws: A Statistical Study |journal=The Astrophysical Journal |volume=640 |issue=1 |pages=L5 |accessdate=2013-07-07|bibcode=2006ApJ...640L...5J |author1=Jóhannesson |first1=Gudlaugur |last2=Björnsson |first2=Gunnlaugur |last3=Gudmundsson |first3=Einar H. |year=2006 |doi=10.1086/503294 |arxiv=astro-ph/0602219 }}</ref>\n\n:<math>f(x) \\propto x^{\\alpha_1}</math> for <math>x<x_\\text{th},</math>\n:<math>f(x) \\propto x^{\\alpha_1-\\alpha_2}_\\text{th}x^{\\alpha_2}\\text{ for } x>x_\\text{th}</math>.\n\n====Power law with exponential cutoff====\nA power law with an exponential cutoff is simply a power law multiplied by an exponential function:<ref>{{cite journal |arxiv=0706.1062|title=POWER-LAW DISTRIBUTIONS IN EMPIRICAL DATA | doi=10.1137/070710111 | bibcode=2009SIAMR..51..661C |volume=51 |issue=4 |year=2009 |journal=SIAM Review |pages=661–703 | last1 = Clauset | first1 = Aaron}}</ref>\n\n:<math>f(x) \\propto x^{\\alpha}e^{\\beta x}.</math>\n\n====Curved power law====\n:<math>f(x) \\propto x^{\\alpha + \\beta x}</math><ref>{{cite web |url=http://www.mpe.mpg.de/xray/wave/rosat/doc/users-guide/node-files/node188.php |title=Curved-power law |accessdate=2013-07-07}}</ref>\n\n==Power-law probability distributions==\nIn a looser sense, a power-law [[probability distribution]] is a distribution whose density function (or mass function in the discrete case) has the form, for large values of <math>x</math>,<ref>N. H. Bingham, C. M. Goldie, and J. L. Teugels, Regular variation. Cambridge University Press, 1989</ref>\n\n:<math>P(X>x) \\sim L(x) x^{-(\\alpha+1)}</math>\n\nwhere <math>\\alpha > 0</math>, and <math>L(x)</math> is a [[slowly varying function]], which is any function that satisfies <math>\\lim_{x\\rightarrow\\infty} L(r\\,x) / L(x) = 1</math> for any positive factor <math>r</math>. This property of <math>L(x)</math> follows directly from the requirement that <math>p(x)</math> be asymptotically scale invariant; thus, the form of <math>L(x)</math> only controls the shape and finite extent of the lower tail. For instance, if <math>L(x)</math> is the constant function, then we have a power law that holds for all values of <math>x</math>. In many cases, it is convenient to assume a lower bound <math>x_{\\mathrm{min}}</math> from which the law holds. Combining these two cases, and where <math>x</math> is a continuous variable, the power law has the form\n\n:<math>p(x) = \\frac{\\alpha-1}{x_\\min} \\left(\\frac{x}{x_\\min}\\right)^{-\\alpha},</math>\n\nwhere the pre-factor to <math>\\frac{\\alpha-1}{x_\\min}</math> is the [[normalizing constant]]. We can now consider several properties of this distribution. For instance, its [[Moment (mathematics)|moments]] are given by\n\n:<math>\\langle x^{m} \\rangle = \\int_{x_\\min}^\\infty x^{m} p(x) \\,\\mathrm{d}x = \\frac{\\alpha-1}{\\alpha-1-m}x_\\min^m</math>\n\nwhich is only well defined for <math>m < \\alpha -1</math>. That is, all moments <math>m \\geq \\alpha - 1</math> diverge: when <math>\\alpha\\leq 2</math>, the average and all higher-order moments are infinite; when <math>2<\\alpha<3</math>, the mean exists, but the variance and higher-order moments are infinite, etc. For finite-size samples drawn from such distribution, this behavior implies that the [[central moment]] estimators (like the mean and the variance) for diverging moments will never converge – as more data is accumulated, they continue to grow. These power-law probability distributions are also called [[Pareto distribution|Pareto-type distributions]], distributions with Pareto tails, or distributions with regularly varying tails.\n\nA modification, which does not satisfy the general form above, with an exponential cutoff,{{sfn|Clauset|Shalizi|Newman|2009}} is\n\n:<math>p(x) \\propto L(x) x^{-\\alpha} \\mathrm{e}^{-\\lambda x}.</math>\n\nIn this distribution, the exponential decay term <math>\\mathrm{e}^{-\\lambda x}</math> eventually overwhelms the power-law behavior at very large values of <math>x</math>. This distribution does not scale and is thus not asymptotically as a power law; however, it does approximately scale over a finite region before the cutoff. (Note that the pure form above is a subset of this family, with <math>\\lambda=0</math>.) This distribution is a common alternative to the asymptotic power-law distribution because it naturally captures finite-size effects.\n\nThe [[Tweedie distributions]] are a family of statistical models characterized by [[Closure (mathematics)|closure]] under additive and reproductive convolution as well as under scale transformation.  Consequently, these models all express a power-law relationship between the variance and the mean.  These models have a fundamental role as foci of mathematical [[Limit (mathematics)|convergence]] similar to the role  that the [[normal distribution]] has as a focus in the [[central limit theorem]].  This convergence effect explains why the variance-to-mean power law manifests so widely in natural processes, as with [[Taylor's law]] in ecology and with fluctuation scaling<ref name=Kendal2011a>{{cite journal | last1 = Kendal | first1 = WS | last2 = Jørgensen | first2 = B | year = 2011 | title = Taylor's power law and fluctuation scaling explained by a central-limit-like convergence | url = | journal = Phys. Rev. E | volume = 83 | issue = 6| page = 066115 | doi=10.1103/physreve.83.066115| pmid = 21797449 | bibcode = 2011PhRvE..83f6115K }}</ref> in physics.  It can also be shown that this variance-to-mean power law, when demonstrated by the [[Tweedie distributions|method of expanding bins]], implies the presence of 1/''f'' noise and that 1/''f'' noise can arise as a consequence of this [[Tweedie distributions|Tweedie convergence effect]].<ref name=Kendal2011b>{{cite journal | last1 = Kendal | first1 = WS | last2 = Jørgensen | first2 = BR | year = 2011 | title = Tweedie convergence: a mathematical basis for Taylor's power law, 1/''f'' noise and multifractality | url = | journal = Phys. Rev. E | volume = 84 | issue = 6| page = 066120 | doi=10.1103/physreve.84.066120| pmid = 22304168 | bibcode = 2011PhRvE..84f6120K }}</ref>\n\n===Graphical methods for identification===\n\nAlthough more sophisticated and robust methods have been proposed, the most frequently used graphical methods of identifying power-law probability distributions  using random samples are Pareto quantile-quantile plots (or Pareto [[Q–Q plot]]s),{{citation needed|date=May 2012}}  mean residual life plots<ref>Beirlant, J., Teugels, J. L., Vynckier, P. (1996a) ''Practical Analysis of Extreme Values'', Leuven: Leuven University Press</ref><ref>Coles, S. (2001) ''An introduction to statistical modeling of extreme values''. Springer-Verlag, London.</ref> and [[log–log plot]]s. Another, more robust graphical method uses bundles of residual quantile functions.<ref name=Diaz>{{cite journal | last1 = Diaz |first1=F. J. | year = 1999 | title = Identifying Tail Behavior by Means of Residual Quantile Functions | url = | journal = Journal of Computational and Graphical Statistics | volume = 8 | issue = 3| pages = 493–509 | doi = 10.2307/1390871 |jstor=1390871 }}</ref> (Please keep in mind that power-law distributions are also called Pareto-type distributions.) It is assumed here that a random sample is obtained from a probability distribution, and that we want to know if the tail of the distribution follows a power law (in other words, we want to know if the distribution has a \"Pareto tail\"). Here, the random sample is called \"the data\".\n\nPareto Q–Q plots compare the [[quantile]]s of the log-transformed data to the corresponding quantiles of an exponential distribution with mean 1 (or to the quantiles of a standard Pareto distribution) by plotting the former versus the latter. If the resultant scatterplot suggests that the plotted points \" asymptotically converge\" to a straight line, then a power-law distribution should be suspected.  A limitation of Pareto Q–Q plots is that they behave poorly when the tail index <math>\\alpha</math> (also called Pareto index) is close to 0, because Pareto Q–Q plots are not designed to identify distributions with slowly varying tails.<ref name=Diaz/>\n\nOn the other hand, in its version for identifying power-law probability distributions, the mean residual life plot consists of first log-transforming the data, and then  plotting the average of those log-transformed data that are higher than the ''i''-th order statistic versus the ''i''-th order statistic, for ''i''&nbsp;=&nbsp;1,&nbsp;...,&nbsp;''n'', where n is the size of the random sample. If the resultant scatterplot suggests that the plotted points tend to \"stabilize\" about a horizontal straight line, then a power-law distribution should be suspected. Since the mean residual life plot is very sensitive to outliers (it is not robust), it usually produces plots that are difficult to interpret; for this reason, such plots are usually called Hill horror plots <ref>{{cite journal | last1 = Resnick | first1 = S. I. | year = 1997 | title = Heavy Tail Modeling and Teletraffic Data | url = | journal = The Annals of Statistics | volume = 25 | issue = 5| pages = 1805–1869 | doi=10.1214/aos/1069362376}}</ref>\n\n[[File:Log-log plot example.svg|thumb|A straight line on a log–log plot is necessary but insufficient evidence for power-laws, the slope of the straight line corresponds to the power law exponent.]]\n\n[[Log–log plot]]s are an alternative way of graphically examining the tail of a distribution using a random sample. Caution has to be exercised however as a log–log plot is necessary but insufficient evidence for a power law relationship, as many non power-law distributions will appear as straight lines on a log–log plot.<ref>{{cite web|url=http://bactra.org/weblog/491.html|title=So You Think You Have a Power Law — Well Isn't That Special?|author=|date=|website=bactra.org|accessdate=27 March 2018}}</ref><ref>{{cite journal|title=Power-law distributions in empirical data|first1=Aaron|last1=Clauset|first2=Cosma Rohilla|last2=Shalizi|first3=M. E. J.|last3=Newman|date=4 November 2009|journal=SIAM Review|volume=51|issue=4|pages=661–703|doi=10.1137/070710111|arxiv=0706.1062|bibcode=2009SIAMR..51..661C}}</ref> This method consists of plotting the logarithm of an estimator of the probability that a particular number of the distribution occurs versus the logarithm of that particular number. Usually, this estimator is the proportion of times that the number occurs in the data set. If the points in the plot tend to \"converge\" to a straight line for large numbers in the x axis, then the researcher concludes that the distribution has a power-law tail. Examples of the application of these types of plot have been published.<ref>{{cite journal | last1 = Jeong|first1= H|last2= Tombor|first2= B. Albert|last3= Oltvai|first3= Z.N.|last4= Barabasi|first4= A.-L. | year = 2000 | title = The large-scale organization of metabolic networks | url = | journal = Nature | volume = 407 | issue = 6804| pages = 651–654 | doi = 10.1038/35036627 | pmid = 11034217 |arxiv = cond-mat/0010278 |bibcode = 2000Natur.407..651J }}</ref> A disadvantage of these plots is that, in order for them to provide reliable results, they require huge amounts of data. In addition, they are appropriate only for discrete (or grouped) data.\n\nAnother graphical method for the identification of power-law probability distributions using random samples has been proposed.<ref name=Diaz/> This methodology consists of plotting a ''bundle for the log-transformed sample''. Originally proposed as a tool to explore the existence of moments and the moment generation function using random samples, the bundle methodology is based on residual [[quantile function]]s (RQFs), also called residual percentile functions,<ref>{{cite journal | last1 = Arnold | first1 = B. C. | last2 = Brockett | first2 = P. L. | year = 1983 | title = When does the βth percentile residual life function determine the distribution? | url = | journal = Operations Research | volume = 31 | issue = 2| pages = 391–396 | doi=10.1287/opre.31.2.391}}</ref><ref>{{cite journal | last1 = Joe | first1 = H. | last2 = Proschan | first2 = F. | year = 1984 | title = Percentile residual life functions | url = | journal = Operations Research | volume = 32 | issue = 3| pages = 668–678 | doi=10.1287/opre.32.3.668}}</ref><ref>Joe, H. (1985), \"Characterizations of life distributions from percentile residual lifetimes\", ''Ann. Inst. Statist. Math.'' 37, Part A, 165–172.</ref><ref>{{cite journal | last1 = Csorgo | first1 = S. | last2 = Viharos | first2 = L. | year = 1992 | title = Confidence bands for percentile residual lifetimes | url = | journal = Journal of Statistical Planning and Inference | volume = 30 | issue = 3| pages = 327–337 | doi=10.1016/0378-3758(92)90159-p}}</ref><ref>{{cite journal | last1 = Schmittlein | first1 = D. C. | last2 = Morrison | first2 = D. G. | year = 1981 | title = The median residual lifetime: A characterization theorem and an application | url = | journal = Operations Research | volume = 29 | issue = 2| pages = 392–399 | doi=10.1287/opre.29.2.392}}</ref><ref>{{cite journal | last1 = Morrison | first1 = D. G. | last2 = Schmittlein | first2 = D. C. | year = 1980 | title = Jobs, strikes, and wars: Probability models for duration | url = | journal = Organizational Behavior and Human Performance | volume = 25 | issue = 2| pages = 224–251 | doi=10.1016/0030-5073(80)90065-3}}</ref><ref>{{cite journal | last1 = Gerchak | first1 = Y | year = 1984 | title = Decreasing failure rates and related issues in the social sciences | url = | journal = Operations Research | volume = 32 | issue = 3| pages = 537–546 | doi=10.1287/opre.32.3.537}}</ref> which provide a full characterization of the tail behavior of many well-known probability distributions, including power-law distributions, distributions with other types of heavy tails, and even non-heavy-tailed distributions. Bundle plots do not have the disadvantages of Pareto Q–Q plots, mean residual life plots and log–log plots mentioned above (they are robust to outliers,  allow visually identifying power laws with small values of <math>\\alpha</math>, and do not demand the collection of much data).{{citation needed|date=May 2012}} In addition, other types of tail behavior can be identified using bundle plots.\n\n===Plotting power-law distributions===\nIn general, power-law distributions are plotted on [[log–log graph|doubly logarithmic axes]], which emphasizes the upper tail region. The most convenient way to do this is via the (complementary) [[cumulative distribution function#Complementary cumulative distribution function (tail distribution)|cumulative distribution]] (cdf), <math>P(x) = \\mathrm{Pr}(X > x)</math>,\n\n:<math>P(x) = \\Pr(X > x) =  C \\int_x^\\infty p(X)\\,\\mathrm{d}X =  \\frac{\\alpha-1}{x_\\min^{-\\alpha+1}} \\int_x^\\infty X^{-\\alpha}\\,\\mathrm{d}X = \\left(\\frac{x}{x_\\min} \\right)^{-\\alpha+1}.</math>\n\nNote that the cdf is also a power-law function, but with a smaller scaling exponent. For data, an equivalent form of the cdf is the rank-frequency approach, in which we first sort the <math>n</math> observed values in ascending order, and plot them against the vector <math>\\left[1,\\frac{n-1}{n},\\frac{n-2}{n},\\dots,\\frac{1}{n}\\right]</math>.\n\nAlthough it can be convenient to log-bin the data, or otherwise smooth the probability density (mass) function directly, these methods introduce an implicit bias in the representation of the data, and thus should be avoided.<ref>{{cite journal|title=Parameter estimation for power-law distributions by maximum likelihood methods|journal= European Physical Journal B|volume=58|issue=2|pages=167–173|author=Bauke, H.|doi=10.1140/epjb/e2007-00219-y|year = 2007|arxiv = 0704.1867|bibcode= 2007EPJB...58..167B}}</ref><ref>{{cite journal|title=Power-Law Distributions in Empirical Data|journal=SIAM Review|volume=51|issue=4|pages=661–703|author=Clauset, A., Shalizi, C. R., Newman, M. E. J.|doi=10.1137/070710111|year = 2009|arxiv=0706.1062|bibcode=2009SIAMR..51..661C}}</ref> The cdf, on the other hand, is more robust to (but not without) such biases in the data and preserves the linear signature on doubly logarithmic axes. Though a cdf representation is favored over that of the pdf while fitting a power law to the data with the linear least square method, it is not devoid of mathematical inaccuracy. Thus, while estimating exponents of a power law distribution, maximum likelihood estimator is recommended.\n\n===Estimating the exponent from empirical data===\nThere are many ways of estimating the value of the scaling exponent for a power-law tail, however not all of them yield [[Maximum likelihood#Asymptotics|unbiased and consistent answers]]. Some of the most reliable techniques are often based on the method of [[maximum likelihood estimation|maximum likelihood]]. Alternative methods are often based on making a linear regression on either the log–log probability, the log–log cumulative distribution function, or on log-binned data, but these approaches should be avoided as they can all lead to highly biased estimates of the scaling exponent.{{sfn|Clauset|Shalizi|Newman|2009}}\n\n====Maximum likelihood====\n\nFor real-valued, [[independent and identically distributed]] data, we fit a power-law distribution of the form\n\n: <math>p(x) = \\frac{\\alpha-1}{x_\\min} \\left(\\frac{x}{x_\\min}\\right)^{-\\alpha}</math>\n\nto the data <math>x\\geq x_\\min</math>, where the coefficient <math>\\frac{\\alpha-1}{x_\\min}</math> is included to ensure that the distribution is [[Normalizing constant|normalized]].  Given a choice for <math>x_\\min</math>, the log likelihood function becomes:\n\n:<math>\\mathcal{L}(\\alpha)=\\log  \\prod _{i=1}^n \\frac{\\alpha-1}{x_\\min} \\left(\\frac{x_i}{x_\\min}\\right)^{-\\alpha}</math> \nThe maximum of this likelihood is found by differentiating with respect to parameter <math>\\alpha</math>, setting the result equal to zero. Upon rearrangement, this yields the estimator equation:\n\n:<math>\\hat{\\alpha} = 1 + n \\left[ \\sum_{i=1}^n \\ln \\frac{x_i}{x_\\min} \\right]^{-1}</math>\n\nwhere <math>\\{x_i\\}</math> are the <math>n</math> data points <math>x_{i}\\geq x_\\min</math>.<ref name=Newman/><ref name=Hall/> This estimator exhibits a small finite sample-size bias of order <math>O(n^{-1})</math>, which is small when ''n''&nbsp;>&nbsp;100. Further, the standard error of the estimate is <math>\\sigma = \\frac{\\hat{\\alpha}-1}{\\sqrt{n}} + O(n^{-1})</math>. This estimator is equivalent to the popular{{citation needed|date=June 2012}} [[Hill estimator]] from [[quantitative finance]] and [[extreme value theory]].{{citation needed|date=June 2012}}\n\nFor a set of ''n'' integer-valued data points <math>\\{x_i\\}</math>, again where each <math>x_i\\geq x_\\min</math>, the maximum likelihood exponent is the solution to the transcendental equation\n\n: <math>\\frac{\\zeta'(\\hat\\alpha,x_\\min)}{\\zeta(\\hat{\\alpha},x_\\min)} = -\\frac{1}{n} \\sum_{i=1}^n \\ln \\frac{x_i}{x_\\min} </math>\n\nwhere <math>\\zeta(\\alpha,x_{\\mathrm{min}})</math> is the [[Riemann zeta function#Generalizations|incomplete zeta function]]. The uncertainty in this estimate follows the same formula as for the continuous equation. However, the two equations for <math>\\hat{\\alpha}</math> are not equivalent, and the continuous version should not be applied to discrete data, nor vice versa.\n\nFurther, both of these estimators require the choice of <math>x_\\min</math>. For functions with a non-trivial <math>L(x)</math> function, choosing <math>x_\\min</math> too small produces a significant bias in <math>\\hat\\alpha</math>, while choosing it too large increases the uncertainty in <math>\\hat{\\alpha}</math>, and reduces the [[statistical power]] of our model. In general, the best choice of <math>x_\\min</math> depends strongly on the particular form of the lower tail, represented by <math>L(x)</math> above.\n\nMore about these methods, and the conditions under which they can be used, can be found in .{{sfn|Clauset|Shalizi|Newman|2009}} Further, this comprehensive review article provides [http://www.santafe.edu/~aaronc/powerlaws/ usable code] (Matlab, Python, R and C++) for estimation and testing routines for power-law distributions.\n\n====Kolmogorov–Smirnov estimation====\n\nAnother method for the estimation of the power-law exponent, which does not assume [[independent and identically distributed]] (iid) data, uses the minimization of the [[Kolmogorov–Smirnov statistic]], <math>D</math>, between the cumulative distribution functions of the data and the power law:\n\n: <math>\\hat{\\alpha} = \\underset{\\alpha}{\\operatorname{arg\\,min}} \\, D_\\alpha </math>\n\nwith\n\n: <math> D_\\alpha = \\max_x | P_\\mathrm{emp}(x) - P_\\alpha(x) | </math>\n\nwhere <math>P_\\mathrm{emp}(x)</math> and <math>P_\\alpha(x)</math> denote the cdfs of the data and the power law with exponent <math>\\alpha</math>, respectively. As this method does not assume iid data, it provides an alternative way to determine the power-law exponent for data sets in which the temporal correlation can not be ignored.<ref name=Klaus/>\n\n====Two-point fitting method====\nThis criterion{{clarify|reason=need to give sme detail of method|date=May 2012}} can be applied for the estimation of power-law exponent in the case of scale free distributions and provides a more convergent estimate than the maximum likelihood method.<ref name=Guerriero>{{Cite journal\n | first1 = V.|last1= Guerriero\n | year = 2012\n | title = Power Law Distribution: Method of Multi-scale Inferential Statistics\n | journal = Journal of Modern Mathematics Frontier (JMMF)   | url =http://www.seipub.org/sjmmf/paperInfo.aspx?ID=5093\n | volume = 1\n | pages = 21–28}}</ref> It has been applied to study probability distributions of fracture apertures.<ref name=Guerriero/> In some contexts the probability distribution is described, not by the [[cumulative distribution function]], by the [[cumulative frequency analysis|cumulative frequency]] of a property ''X'', defined as the number of elements per meter (or area unit, second etc.) for which ''X''&nbsp;>&nbsp;''x'' applies, where ''x'' is a variable real number. As an example,<ref name=Guerriero/> the cumulative distribution of the fracture aperture, ''X'', for a sample of ''N'' elements is defined as 'the number of fractures per meter having aperture greater than ''x'' . Use of cumulative frequency has some advantages, e.g. it allows one to put on the same diagram data gathered from sample lines of different lengths at different scales (e.g. from outcrop and from microscope).\n\n====R function====\nThe following function estimates the exponent in R, plotting the log–log data and the fitted line.\n<source lang=\"r\">\n    pwrdist <- function(u,...) {\n        # u is vector of event counts, e.g. how many\n        # crimes was a given perpetrator charged for by the police\n        fx <- table(u)\n        i <- as.numeric(names(fx))\n        y <- rep(0,max(i))\n        y[i] <- fx\n        m0 <- glm(y~log(1:max(i)),family=quasipoisson())\n        print(summary(m0))\n        sub <- paste(\"s=\",round(m0$coef[2],2),\"lambda=\",sum(u),\"/\",length(u))\n        plot(i,fx,log=\"xy\",xlab=\"x\",sub=sub,ylab=\"counts\",...)\n        grid()\n        lines(1:max(i),(fitted(m0)),type=\"b\")\n        return(m0)\n    }\n</source>\n\n==Validating power laws==\nAlthough power-law relations are attractive for many theoretical reasons, demonstrating that data does indeed follow a power-law relation requires more than simply fitting a particular model to the data.<ref name=\"HilbertPowerLaw\"/> This is important for understanding the mechanism that gives rise to the distribution: superficially similar distributions may arise for significantly different reasons, and different models yield different predictions, such as extrapolation.\n\nFor example, [[log-normal distribution]]s are often mistaken for power-law distributions:{{sfn|Mitzenmacher|2004}} a data set drawn from a lognormal distribution will be approximately linear for large values (corresponding to the upper tail of the lognormal being close to a power law){{clarify|date=July 2015}}, but for small values the lognormal will drop off significantly (bowing down), corresponding to the lower tail of the lognormal being small (there are very few small values, rather than many small values in a power law).{{citation needed|date=July 2015}}\n\nFor example, [[Gibrat's law]] about proportional growth processes produce distributions that are lognormal, although their log–log plots look linear over a limited range. An explanation of this is that although the logarithm of the [[Log-normal distribution#Probability density function|lognormal density function]] is quadratic in {{math|log(<var>x</var>)}}, yielding a \"bowed\" shape in a log–log plot, if the quadratic term is small relative to the linear term then the result can appear almost linear, and the lognormal behavior is only visible when the quadratic term dominates, which may require significantly more data. Therefore, a log–log plot that is slightly \"bowed\" downwards can reflect a log-normal distribution – not a power law.\n\nIn general, many alternative functional forms can appear to follow a power-law form for some extent.{{sfn|Laherrère|Sornette|1998}}  Stumpf<ref name=Stumpf/> proposed plotting the empirical cumulative distribution function in the log-log domain and claimed that a candidate power-law should cover at least two orders of magnitude. Also, researchers usually have to face the problem of deciding whether or not a real-world probability distribution follows a power law. As a solution to this problem, Diaz<ref name=Diaz/> proposed a graphical methodology based on random samples that allow visually discerning between different types of tail behavior. This methodology uses bundles of residual quantile functions, also called percentile residual life functions, which characterize many different types of distribution tails, including both heavy and non-heavy tails. However, Stumpf<ref name=Stumpf/> claimed the need for both a statistical and a theoretical background in order to support a power-law in the underlying mechanism driving the data generating process.\n\nOne method to validate a power-law relation tests many orthogonal predictions of a particular generative mechanism against data. Simply fitting a power-law relation to a particular kind of data is not considered a rational approach. As such, the validation of power-law claims remains a very active field of research in many areas of modern science.{{sfn|Clauset|Shalizi|Newman|2009}}\n\n==See also==\n{{col-begin}}\n{{col-break}}\n*[[Acoustic attenuation]]\n*[[Allometry]]\n*[[Fat-tailed distribution]]\n*[[Finite-time singularity]]\n*[[Fractional calculus]]\n*[[Fractional dynamics]]\n*[[Heavy-tailed distribution]]s\n*[[Hyperbolic growth]]\n{{col-break}}\n*[[Lévy flight]]\n*[[Long tail]]\n*[[Power law fluid]]\n*[[Simon model]]\n*[[Stable distribution]]\n*[[Stevens's power law]]\n*[[Wealth concentration]]\n*[[Webgraph]]\n{{col-break}}\n{{col-end}}\n\n==References==\n'''Notes'''\n{{reflist|3|refs=\n\n<ref name=Hall>{{Cite journal\n | author = Hall, P.  | year = 1982\n | title = On Some Simple Estimates of an Exponent of Regular Variation\n | journal = [[Journal of the Royal Statistical Society, Series B]]\n | volume = 44  | issue = 1 | pages = 37–42\n | jstor = 2984706\n}}</ref>\n\n<ref name=Newman>{{Cite journal\n | author = Newman, M. E. J.\n | year = 2005\n | title = Power laws, Pareto distributions and Zipf's law\n | journal = [[Contemporary Physics]]\n | volume = 46  | issue = 5 | pages = 323–351\n | arxiv =cond-mat/0412004  | doi = 10.1080/00107510500052444\n |bibcode = 2005ConPh..46..323N }}</ref>\n\n<ref name=Stumpf>{{Cite journal\n | author = Stumpf, M.P.H.\n | year = 2012\n | title = Critical Truths about Power Laws\n | journal = [[Science (journal)|Science]]\n | volume = 335  | issue =  6069| pages = 665–666\n | doi = 10.1126/science.1216142| pmid = 22323807\n | bibcode = 2012Sci...335..665S}}</ref>\n}}\n\n'''Bibliography'''\n\n* Bak, Per (1997) ''How nature works'', Oxford University Press {{isbn|0-19-850164-1}}\n*{{Cite journal | last1 = Clauset | first1 = A. | last2 = Shalizi | first2 = C. R. | last3 = Newman | first3 = M. E. J. | doi = 10.1137/070710111 | title = Power-Law Distributions in Empirical Data | journal = SIAM Review | volume = 51 | issue = 4 | pages = 661–703 | year = 2009 | arxiv = 0706.1062 | pmid =  | pmc = | ref = harv |bibcode = 2009SIAMR..51..661C }}\n*{{Cite journal | last1 = Laherrère | first1 = J. | last2 = Sornette | first2 = D. | doi = 10.1007/s100510050276 | title = Stretched exponential distributions in nature and economy: \"fat tails\" with characteristic scales | journal = European Physical Journal B | volume = 2 | issue = 4 | pages = 525–539 | year = 1998 | arxiv = cond-mat/9801293 | ref = harv | pmid =  | pmc = |bibcode = 1998EPJB....2..525L }}\n*{{Cite journal | last1 = Mitzenmacher | first1 = M. | title = A Brief History of Generative Models for Power Law and Lognormal Distributions | doi = 10.1080/15427951.2004.10129088 | url = http://www.eecs.harvard.edu/~michaelm/postscripts/im2004a.pdf| journal = Internet Mathematics | volume = 1 | issue = 2 | pages = 226–251 | year = 2004 | ref = harv | pmid =  | pmc = }}\n* Alexander Saichev, Yannick Malevergne and Didier Sornette (2009) ''Theory of  Zipf's law and beyond'', Lecture Notes in Economics and Mathematical Systems, Volume 632, Springer (November 2009), {{isbn|978-3-642-02945-5}}\n*{{Cite journal| last = Simon | first = H. A. | title = On a Class of Skew Distribution Functions | doi = 10.2307/2333389 | year = 1955 | journal = [[Biometrika]] | volume = 42 | pages = 425–440 | issue = 3/4 | jstor = 2333389 | ref = harv }}\n*{{cite book |title=Critical Phenomena in Natural Sciences: Chaos, Fractals, Self-organization and Disorder: Concepts and Tools |last=Sornette |first=Didier |authorlink=Didier Sornette |year=2006 |edition=2nd |publisher=Springer |location=Heidelberg |series=Springer Series in Synergetics |isbn=978-3-540-30882-9 |ref=harv }}\n* Mark Buchanan (2000) ''Ubiquity'', Weidenfeld & Nicolson {{isbn|0-297-64376-2}}\n* {{cite journal | last1 = Stumpf | first1 = M.P.H. | last2 = Porter | first2 = M.A. | year = 2012 | title = Critical Truths about Power Laws| journal = Science | volume = 335 | issue = 6069 | pages = 665–6 | doi=10.1126/science.1216142| pmid = 22323807 | bibcode = 2012Sci...335..665S }}\n\n==External links==\n*[http://www.nslij-genetics.org/wli/zipf/ Zipf's law]\n*[http://www.hpl.hp.com/research/idl/papers/ranking/ranking.html Zipf, Power-laws, and Pareto – a ranking tutorial]\n*[http://www.physicalgeography.net/fundamentals/10ab.html Stream Morphometry and Horton's Laws]\n*[[Clay Shirky]] on [https://www.youtube.com/watch?v=sPQViNNOAkw Institutions & Collaboration: Power law in relation to the internet-based social networks]\n*[[Clay Shirky]] on [http://www.shirky.com/writings/herecomeseverybody/powerlaw_weblog.html Power Laws, Weblogs, and Inequality]\n*[http://archive.fortune.com/magazines/fortune/fortune_archive/2005/07/11/8265256/index.htm \"How the Finance Gurus Get Risk All Wrong\"] by Benoit Mandelbrot & Nassim Nicholas Taleb. ''Fortune'', July 11, 2005.\n*[http://www.newyorker.com/magazine/2006/02/13/million-dollar-murray \"Million-dollar Murray\":] power-law distributions in homelessness and other social problems; by [[Malcolm Gladwell]]. ''The New Yorker'', February 13, 2006.\n*Benoit Mandelbrot & Richard Hudson: <cite>The Misbehaviour of Markets (2004) </cite>\n*Philip Ball: [http://www.agrfoto.com/philipball/criticalmass.php Critical Mass: How one thing leads to another] (2005)\n*[http://econophysics.blogspot.com/2006/07/tyranny-of-power-law-and-why-we-should.html ''Tyranny of the Power Law''] from [http://econophysics.blogspot.com The Econophysics Blog]\n*[http://www.stat.cmu.edu/~cshalizi/2010-10-18-Meetup.pdf ''So You Think You Have a Power Law – Well Isn't That Special?''] from [http://www.cscs.umich.edu/~crshalizi/weblog/ Three-Toed Sloth], the blog of [[Cosma Shalizi]], Professor of Statistics at Carnegie-Mellon University.\n*[http://www.mathworks.com/matlabcentral/fileexchange/27176-log-binning-of-data  Simple MATLAB script] which bins data to illustrate power-law distributions (if any) in the data.\n*[http://web-graph.org The Erdős Webgraph Server] visualizes the distribution of the degrees of the webgraph on the [http://web-graph.org/index.php/download download page].\n\n{{DEFAULTSORT:Power Law}}\n[[Category:Exponentials]]\n[[Category:Power laws|*]]\n[[Category:Theory of probability distributions]]\n[[Category:Statistical laws]]\n\n[[nl:Machtsfunctie]]"
    },
    {
      "title": "Prime power",
      "url": "https://en.wikipedia.org/wiki/Prime_power",
      "text": "{{For|the electrical generator power rating|Prime power (electrical)}}\n\nIn [[mathematics]], a '''prime power''' is a [[positive integer]] [[exponentiation|power]] of a single [[prime number]].\nFor example: {{nowrap|1=7 = 7<sup>1</sup>}}, {{nowrap|1= 9 = 3<sup>2</sup>}} and {{nowrap|1=32 = 2<sup>5</sup>}} are prime powers, while\n{{nowrap|1=6 = 2 × 3}}, {{nowrap|1=12 = 2<sup>2</sup> × 3}} and {{nowrap|1=36 = 6<sup>2</sup> = 2<sup>2</sup> × 3<sup>2</sup>}} are not. (The number 1 is not counted as a prime power.) The prime powers are:\n2, 3, 4, 5, 7, 8, 9, 11, 13, 16, 17, 19, 23, 25, 27, 29, 31, 32, 37, 41, 43, 47, 49, 53, 59, 61, 64, 67, 71, 73, 79, 81, 83, 89, 97, 101, 103, 107, 109, 113, 121, 125, 127, 128, 131, 137, 139, 149, 151, 157, 163, 167, 169, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 243, 251, 256, ... {{OEIS|id=A246655}}\n\nThe prime powers are those positive integers that are divisible by exactly one prime number; prime powers and related concepts are also called '''primary numbers''', as in the [[primary decomposition]].\n\n==Properties==\n===Algebraic properties===\nPrime powers are powers of prime numbers. Every prime power (except powers of 2) has a [[primitive root modulo n|primitive root]]; thus the [[multiplicative group of integers modulo n|multiplicative group]] of integers modulo ''p<sup>n</sup>'' (or equivalently, the [[group of units]] of the [[ring (mathematics)|ring]] '''Z'''/''p<sup>n</sup>'''''Z''') is [[cyclic group|cyclic]].\n\nThe number of elements of a [[finite field]] is always a prime power and conversely, every\nprime power occurs as the number of elements in some finite field (which is unique up to [[isomorphism]]).\n\n===Combinatorial properties===\nA property of prime powers used frequently in [[analytic number theory]] is that the set of prime powers which are not prime is a [[small set (combinatorics)|small set]] in the sense that the [[series (mathematics)|infinite sum]] of their reciprocals [[convergent series|converges]], although the primes are a large set.\n\n===Divisibility properties===\nThe [[Euler's totient function|totient function]] (''φ'') and [[divisor function|sigma functions]] (''σ''<sub>0</sub>) and (''σ''<sub>1</sub>) of a prime power are calculated by the formulas:\n:<math>\\varphi(p^n) = p^{n-1} \\varphi(p) = p^{n-1} (p - 1) = p^n - p^{n-1} = p^n \\left(1 - \\frac{1}{p}\\right),</math>\n:<math>\\sigma_0(p^n) = \\sum_{j=0}^{n} p^{0\\cdot j} = \\sum_{j=0}^{n} 1 = n+1,</math>\n:<math>\\sigma_1(p^n) = \\sum_{j=0}^{n} p^{1\\cdot j} = \\sum_{j=0}^{n} p^{j} = \\frac{p^{n+1} - 1}{p - 1}.</math>\n\nAll prime powers are [[deficient number]]s. A prime power ''p<sup>n</sup>'' is an ''n''-[[almost prime]]. It is not known whether a prime power ''p<sup>n</sup>'' can be an [[amicable number]]. If there is such a number, then ''p<sup>n</sup>'' must be greater than 10<sup>1500</sup> and ''n'' must be greater than 1400.\n\n==Popular media==\nIn the 1997 film ''[[Cube (film)|Cube]]'', prime powers play a key role, acting as indicators of lethal dangers in a maze-like cube structure.\n\n==See also==\n*[[Almost prime]]\n*[[Perfect power]]\n*[[Semiprime]]\n\n==References==\n*''Elementary Number Theory''. Jones, Gareth A. and Jones, J. Mary. Springer-Verlag London Limited. 1998.\n\n{{Classes of natural numbers}}\n[[Category:Prime numbers]]\n[[Category:Exponentials]]"
    },
    {
      "title": "Proof that e is irrational",
      "url": "https://en.wikipedia.org/wiki/Proof_that_e_is_irrational",
      "text": "{{E (mathematical constant)}}\n{{DISPLAYTITLE:Proof that {{mvar|e}} is irrational}}\nThe [[e (mathematical constant)|number ''e'']] was introduced by [[Jacob Bernoulli]] in 1683. More than half a century later, [[Leonhard Euler|Euler]], who had been a student of Jacob's younger brother [[Johann Bernoulli|Johann]], proved that ''e'' is [[Irrational number|irrational]]; that is, that it cannot be expressed as the quotient of two integers.\n\n==Euler's proof==\nEuler wrote the first proof of the fact that ''e'' is irrational in 1737 (but the text was only published seven years later).<ref>{{cite journal | last = Euler | first = Leonhard | year = 1744 | title = De fractionibus continuis dissertatio | url = http://www.math.dartmouth.edu/~euler/docs/originals/E071.pdf | journal = Commentarii academiae scientiarum Petropolitanae | volume = 9 | pages = 98–137 |trans-title=A dissertation on continued fractions}}</ref><ref>{{cite journal | last = Euler | first = Leonhard | title = An essay on continued fractions | journal = Mathematical Systems Theory | volume = 18 | pages = 295–398 | url = https://kb.osu.edu/dspace/handle/1811/32133 | publication-date = 1985 | doi=10.1007/bf01699475}}</ref><ref>{{cite book | last1 = Sandifer | first1 = C. Edward | title = How Euler did it | chapter = Chapter 32: Who proved ''e'' is irrational? | publisher = [[Mathematical Association of America]] | pages = 185–190 | year = 2007 | isbn = 978-0-88385-563-8 | lccn = 2007927658}}</ref> He computed the representation of ''e'' as a [[simple continued fraction]], which is\n\n:<math>e = [2; 1, 2, 1, 1, 4, 1, 1, 6, 1, 1, 8, 1, 1, \\ldots, 2n, 1, 1, \\ldots]. </math>\n\nSince this continued fraction is infinite and every rational number has a terminating continued fraction, ''e'' is irrational. A short proof of the previous equality is known.<ref>[https://arxiv.org/pdf/math/0601660.pdf A Short Proof of the Simple Continued Fraction Expansion of e]</ref><ref>{{cite journal | last = Cohn | first = Henry | journal = [[American Mathematical Monthly]] | volume = 113 | issue = 1 | pages = 57–62 | publisher = [[Mathematical Association of America]] | year = 2006 | title = A short proof of the simple continued fraction expansion of ''e'' | jstor = 27641837 | doi=10.2307/27641837| arxiv = math/0601660 }}</ref> Since the simple continued fraction of ''e'' is not [[Periodic continued fraction|periodic]], this also proves that ''e'' is not a root of second degree polynomial with rational coefficients; in particular, ''e''<sup>2</sup> is irrational.\n\n==Fourier's proof==\nThe most well-known proof is [[Joseph Fourier]]'s [[proof by contradiction]],<ref>{{Cite book | last1 = de Stainville | first1 = Janot | year = 1815 | title = Mélanges d'Analyse Algébrique et de Géométrie |trans-title=A mixture of Algebraic Analysis and Geometry | publisher = Veuve Courcier | pages = 340–341}}</ref> which is based upon the equality\n\n: <math>e = \\sum_{n = 0}^{\\infty} \\frac{1}{n!}\\cdot</math>\n\nInitially ''e'' is assumed to be a rational number of the form <sup>''a''</sup>⁄<sub>''b''</sub>.  Note that ''b'' could not be equal to 1 as ''e'' is not an integer. It can be shown using the above equality that ''e'' is strictly between 2 and 3:\n\n:<math>2 = 1 + \\tfrac{1}{1!} < e = 1 + \\tfrac{1}{1!} + \\tfrac{1}{2!} + \\tfrac{1}{3!} + \\cdots < 1 +  \\left (1 + \\tfrac{1}{2} + \\tfrac{1}{2^2} + \\tfrac{1}{2^3} + \\cdots \\right ) = 3.</math>\n\nWe then analyze a blown-up difference ''x'' of the series representing ''e'' and its strictly smaller {{nowrap|''b''<sup> th</sup>}} partial sum, which approximates the limiting value ''e''.  By choosing the magnifying factor to be the [[factorial]] of&nbsp;''b'', the fraction <sup>''a''</sup>⁄<sub>''b''</sub> and the {{nowrap|''b''<sup> th</sup>}} partial sum are turned into [[integer]]s, hence ''x'' must be a positive integer. However, the fast convergence of the series representation implies that the magnified approximation error  ''x'' is still strictly smaller than&nbsp;1. From this contradiction we deduce that ''e'' is irrational.\n\nSuppose that ''e'' is a [[rational number]]. Then there exist positive integers ''a'' and ''b'' such that ''e''&nbsp;=&nbsp;<sup>''a''</sup>⁄<sub>''b''</sub>. Define the number\n\n:<math>x = b!\\left(e - \\sum_{n = 0}^{b} \\frac{1}{n!}\\right).</math>\n\nTo see that if ''e'' is rational, then ''x'' is an integer, substitute ''e'' = <sup>''a''</sup>⁄<sub>''b''</sub> into this definition to obtain\n\n:<math>x = b!\\left (\\frac{a}{b} - \\sum_{n = 0}^{b} \\frac{1}{n!}\\right) = a(b - 1)! - \\sum_{n = 0}^{b} \\frac{b!}{n!}.</math>\n\nThe first term is an integer, and every fraction in the sum is actually an integer because ''n''&nbsp;≤&nbsp;''b'' for each term. Therefore, ''x'' is an integer.\n\nWe now prove that {{nowrap|0 < ''x'' < 1}}. First, to prove that ''x'' is strictly positive, we insert the above series representation of ''e'' into the definition of ''x'' and obtain\n\n:<math>x = b!\\left(\\sum_{n = 0}^{\\infty} \\frac{1}{n!} - \\sum_{n = 0}^{b} \\frac{1}{n!}\\right) = \\sum_{n = b+1}^{\\infty} \\frac{b!}{n!}>0,</math>\n\nbecause all the terms are strictly positive.\n\nWe now prove that ''x''&nbsp;<&nbsp;1. For all terms with {{nowrap|''n'' ≥ ''b'' + 1}} we have the upper estimate\n\n:<math>\\frac{b!}{n!} =\\frac1{(b+1)(b+2)\\cdots(b+(n-b))} <\\frac1{(b+1)^{n-b}}.</math>\n\nThis inequality is strict for every ''n''&nbsp;≥&nbsp;''b''&nbsp;+&nbsp;2. Changing the index of summation to ''k''&nbsp;=&nbsp;''n''&nbsp;–&nbsp;''b'' and using the formula for the [[Geometric series#Infinite geometric series|infinite geometric series]], we obtain\n\n:<math> x =\\sum_{n = b+1}^\\infty \\frac{b!}{n!}\n< \\sum_{n=b+1}^\\infty \\frac1{(b+1)^{n-b}}\n=\\sum_{k=1}^\\infty \\frac1{(b+1)^k}\n=\\frac{1}{b+1} \\left (\\frac1{1-\\frac1{b+1}}\\right )\n= \\frac{1}{b} < 1.</math>\n\nSince there is no integer strictly between 0 and 1, we have reached a contradiction, and so ''e'' must be irrational. [[Q.E.D.]]\n\n==Alternate proofs==\nAnother proof<ref>{{Citation | last1 = MacDivitt | first1 = A. R. G. | last2 = Yanagisawa | first2 = Yukio | title = An elementary proof that ''e'' is irrational | journal = [[The Mathematical Gazette]] | volume = 71 | issue = 457 | pages = 217 | year = 1987 | publisher =[[Mathematical Association]] | place = London | jstor = 3616765 | doi=10.2307/3616765}}</ref> can be obtained from the previous one by noting that\n\n:<math>(b+1)x=1+\\frac1{b+2}+\\frac1{(b+2)(b+3)}+\\cdots<1+\\frac1{b+1}+\\frac1{(b+1)(b+2)}+\\cdots=1+x,</math>\n\nand this inequality is equivalent to the assertion that ''bx''&nbsp;<&nbsp;1. This is impossible, of course, since ''b'' and ''x'' are natural numbers.\n\nStill another proof <ref>{{cite journal | last = Penesi | first = L. L. | year = 1953 | title = Elementary proof that ''e'' is irrational | journal = [[American Mathematical Monthly]] | publisher = [[Mathematical Association of America]] | volume = 60 | issue = 7 | pages = 474 | jstor = 2308411 | doi = 10.2307/2308411 }}</ref><ref>Apostol, T. (1974). Mathematical analysis (2nd ed., Addison-Wesley series in mathematics). Reading, Mass.: Addison-Wesley.</ref> can be obtained from the fact that\n\n:<math>\\frac{1}{e} =e^{-1}=\\sum_{n=0}^\\infty\\frac{(-1)^n}{n!}\\cdot</math>\n\nDefine <math>s_{n}</math> as follows:\n\n:<math>s_{n}=\\sum_{k=0}^{n} \\frac{(-1)^{k}}{k!}.</math>\n\nThen:\n\n:<math> e^{-1}-s_{2n-1}=\\sum_{k=0}^{\\infty} \\frac{(-1)^{k}}{k!} -\\sum_{k=0}^{2n-1} \\frac{(-1)^{k}}{k!} <\\frac{1}{(2n)!},</math>\n\nwhich implies:\n\n:<math> 0<(2n-1)! \\left (e^{-1}-s_{2n-1} \\right ) < \\frac{1}{2n} \\leq \\frac{1}{2} </math> \n\nfor any integer <math>n \\geq 2.</math>\n\nNote that <math>(2n-1)!s_{2n-1}</math> is always an integer. Assume <math>e^{-1}</math> is rational, so, <math>e^{-1}=\\tfrac{p}{q}</math> where <math>p, q</math> are co-prime and <math>q \\neq 0.</math> It's possible to appropriately choose <math>n</math> so that <math>(2n-1)!e^{-1}</math> is an integer i.e. <math>n \\geq \\tfrac{q+1}{2}.</math> Hence, for this choice, the difference between <math>(2n-1)!e^{-1}</math> and <math>(2n-1)!s_{2n-1}</math> would be an integer. But from the above inequality, that's impossible. So, <math>e^{-1}</math> is irrational. This means that <math>e</math> is irrational.\n\n==Generalizations==\nIn 1840, [[Joseph Liouville|Liouville]] published a proof of the fact that ''e''<sup>2</sup> is irrational<ref>{{cite journal | last = Liouville | first = Joseph | journal = [[Journal de Mathématiques Pures et Appliquées]] | title = Sur l'irrationalité du nombre ''e'' = 2,718… | series = 1 | volume = 5 | pages = 192 | year = 1840 | language = french}}</ref> followed by a proof that ''e''<sup>2</sup> is not a root of a second degree polynomial with rational coefficients.<ref>{{cite journal | last = Liouville | first = Joseph | journal = [[Journal de Mathématiques Pures et Appliquées]] | title = Addition à la note sur l'irrationnalité du nombre ''e'' | series = 1 | volume = 5 | pages = 193–194 | year = 1840 | language = french}}</ref> This last fact implies that ''e''<sup>4</sup> is irrational. His proofs are similar to Fourier's proof of the irrationality of ''e''. In 1891, [[Adolf Hurwitz|Hurwitz]] explained how it is possible to prove along the same line of ideas that ''e'' is not a root of a third degree polynomial with rational coefficients.<ref>{{cite book | last1 = Hurwitz | first1 = Adolf | year = 1933 | origyear = 1891 | title = Mathematische Werke | volume = 2 | language = german | chapter = Über die Kettenbruchentwicklung der Zahl ''e'' | publisher = [[Birkhäuser]] | location = Basel | pages = 129–133}}</ref> In particular, ''e''<sup>3</sup> is irrational.\n\nMore generally, ''e''<sup>''q''</sup> is irrational for any non-zero rational ''q''.<ref>{{Citation | last1=Aigner | first1=Martin | author1-link = Martin Aigner | last2=Ziegler | first2=Günter M. | author2-link=Günter M. Ziegler | title=[[Proofs from THE BOOK]] | publisher=[[Springer-Verlag]] | location=Berlin, New York | year=1998|pages=27–36|isbn=978-3-642-00855-9|doi=10.1007/978-3-642-00856-6|edition=4th}}.</ref>\n\n==See also==\n* [[Characterizations of the exponential function]]\n* [[Transcendental number]], including a [[Transcendental number#Sketch of a proof that e is transcendental|proof that ''e'' is transcendental]]\n* [[Lindemann–Weierstrass theorem]]\n\n==References==\n<references/>\n\n[[Category:Diophantine approximation]]\n[[Category:Exponentials]]\n[[Category:Article proofs]]\n[[Category:E (mathematical constant)]]\n[[Category:Irrational numbers]]"
    },
    {
      "title": "Q-exponential",
      "url": "https://en.wikipedia.org/wiki/Q-exponential",
      "text": "{{distinguish|text=the [[Tsallis statistics#q-exponential|Tsallis q-exponential]]}}\n{{DISPLAYTITLE:''q''-exponential}}\n\nIn [[combinatorics|combinatorial]] [[mathematics]], a '''''q''-exponential''' is a [[q-analog|''q''-analog]] of the [[exponential function]],\nnamely the [[eigenfunction]] of a ''q''-derivative. There are many  ''q''-derivatives, for example, the classical  [[q-derivative|''q''-derivative]], the Askey-Wilson operator, etc. Therefore, unlike the classical exponentials, ''q''-exponentials are not unique.  For example, <math>e_q(z)</math> is the ''q''-exponential corresponding to the classical  [[q-derivative|''q''-derivative]] while  <math>\\mathcal{E}_q(z)</math> are eigenfunctions of the Askey-Wilson operators.\n\n==Definition==\nThe ''q''-exponential <math>e_q(z)</math> is defined as\n:<math>e_q(z)=\n\\sum_{n=0}^\\infty \\frac{z^n}{[n]_q!} = \n\\sum_{n=0}^\\infty \\frac{z^n (1-q)^n}{(q;q)_n} = \n\\sum_{n=0}^\\infty z^n\\frac{(1-q)^n}{(1-q^n)(1-q^{n-1}) \\cdots (1-q)}</math>\n\nwhere <math>[n]_q!</math> is the [[q-factorial|''q''-factorial]] and \n:<math>(q;q)_n=(1-q^n)(1-q^{n-1})\\cdots (1-q)</math>\n\nis the [[q-Pochhammer symbol|''q''-Pochhammer symbol]]. That this is the ''q''-analog of the exponential follows from the property\n\n:<math>\\left(\\frac{d}{dz}\\right)_q e_q(z) = e_q(z)</math>\n\nwhere the derivative on the left is the [[q-derivative|''q''-derivative]]. The above is easily verified by considering the ''q''-derivative of the [[monomial]]\n\n:<math>\\left(\\frac{d}{dz}\\right)_q z^n = z^{n-1} \\frac{1-q^n}{1-q}\n=[n]_q z^{n-1}.</math>\n\nHere, <math>[n]_q</math> is the [[q-bracket|''q''-bracket]].\nFor other definitions of the ''q''-exponential function, see {{harvtxt|Exton|1983}}, {{harvtxt|Ismail|Zhang|1994}}, {{harvtxt|Suslov|2003}} and {{harvtxt|Cieslinski|2011}}.\n\n==Properties==\nFor real <math>q>1</math>, the function <math>e_q(z)</math> is an [[entire function]] of <math>z</math>. For <math>q<1</math>, <math>e_q(z)</math> is regular in the disk <math>|z|<1/(1-q)</math>.\n\nNote the inverse,  <math>~e_q(z)  ~   e_{1/q} (-z)        =1</math>.\n===Addition Formula===\nIf <math>xy=qyx</math>, <math>e_q(x)e_q(y)=e_q(x+y)</math> holds.\n==Relations==\nFor  <math>-1<q<1</math>, a function that is closely related is <math>E_q(z).</math>  It  is a special case of the [[basic hypergeometric series]],\n\n:<math>E_{q}(z)=\\;_{1}\\phi_{1}\\left({\\scriptstyle{0\\atop 0}}\\, ;\\,z\\right)=\\sum_{n=0}^{\\infty}\\frac{q^{\\binom{n}{2}}(-z)^{n}}{(q;q)_{n}}=\\prod_{n=0}^{\\infty}(1-q^{n}z)=(z;q)_\\infty. </math>\n\nClearly,\n:<math>\\lim_{q\\to1}E_{q}\\left(z(1-q)\\right)=\\lim_{q\\to1}\\sum_{n=0}^{\\infty}\\frac{q^{\\binom{n}{2}}(1-q)^{n}}{(q;q)_{n}}\n(-z)^{n}=e^{-z} .~ </math>\n===Relation with Dilogarithm===\n<math>e_q(x)</math> has the following infinite product representation:\n:<math>e_q(x)=\\left(\\prod_{k=0}^\\infty(1-q^k(1-q)x)\\right)^{-1}. </math>\nOn the other hand, <math>\\log(1-x)=-\\sum_{n=1}^\\infty\\frac{x^n}{n}</math> holds. \nWhen <math>|q|<1</math>,\n:<math>\\log e_q(x)=-\\sum_{k=0}^\\infty\\log(1-q^k(1-q)x)=\\sum_{k=0}^\\infty\\sum_{n=1}^\\infty\\frac{(q^k(1-q)x)^n}{n}=\\sum_{n=1}^\\infty\\frac{((1-q)x)^n}{(1-q^n)n}=\\frac{1}{1-q}\\sum_{n=1}^\\infty\\frac{((1-q)x)^n}{[n]_qn}.</math>\nBy taking the limit <math>q\\to 1</math>,\n:<math>\\lim_{q\\to 1}(1-q)\\log e_q(x/(1-q))=\\mathrm{Li}_2(x), </math>\nwhere <math>\\mathrm{Li}_2(x)</math> is the [[dilogarithm]].\n\n==References==\n* [[Harold Exton|Exton]], H. (1983), ''q-Hypergeometric Functions and Applications'', New York:  Halstead Press, Chichester: Ellis Horwood, {{ISBN|0853124914}},  {{ISBN|0470274530}}, {{ISBN|978-0470274538}}\n* [[George Gasper|Gasper]], G. & [[Mizan Rahman|Rahman]], M.  (2004), ''Basic Hypergeometric Series'', Cambridge University Press, {{ISBN|0521833574}}\n* [[Mourad Ismail|Ismail]], M. E. H. (2005), ''Classical and Quantum Orthogonal Polynomials in One Variable'',  Cambridge University Press.\n* Jackson, F. H. (1908), \"On q-functions and a certain difference operator\", ''Transactions of the Royal Society of Edinburgh'',  '''46''', 253-281. \n\n{{DEFAULTSORT:Q-Exponential}}\n[[Category:Q-analogs]]\n[[Category:Exponentials]]"
    },
    {
      "title": "Radioactive decay",
      "url": "https://en.wikipedia.org/wiki/Radioactive_decay",
      "text": "{{about||particle decay in a more general context|Particle decay|more information on hazards of various kinds of radiation from decay|Ionizing radiation}}\n{{Redirect2|Radioactive|Radioactivity|other uses|Radioactive (disambiguation)|and|Radioactivity (disambiguation)}}\n{{short description|Process by which an unstable atom emits radiation}}\n{{Use British English|date=May 2014}}\n{{Use dmy dates|date=May 2014}}\n{{Nuclear physics}}\n[[File:Alpha Decay.svg|thumb|[[Alpha decay]] is one type of radioactive decay, in which an atomic nucleus emits an [[alpha particle]], and thereby transforms (or \"decays\") into an atom with a [[mass number]] decreased by 4 and [[atomic number]] decreased by 2.]]\n'''Radioactive decay''' (also known as '''nuclear decay''', '''radioactivity''' or '''nuclear radiation''') is the process by which an unstable [[atomic nucleus]] loses energy (in terms of mass in its [[rest frame]]) by emitting [[radiation]], such as an [[alpha particle]], [[beta particle]] with [[neutrino]] or only a neutrino in the case of [[electron capture]], or a [[gamma ray]] or [[electron]] in the case of [[internal conversion]]. A material containing unstable nuclei is considered '''radioactive'''. Certain highly excited short-lived nuclear states can decay through [[neutron emission]], or more rarely, [[proton emission]].\n\nRadioactive decay is a [[stochastic]] (i.e. random) process at the level of single atoms. According to [[quantum mechanics|quantum theory]], it is impossible to predict when a particular atom will decay,<ref name=\"IntroductionToHealthPhysics\">{{cite book |title=Radiation Protection and Dosimetry: An Introduction to Health Physics |last1=Stabin |first1=Michael G. |isbn=978-0-387-49982-6 |year=2007 |publisher=[[Springer Publishing|Springer]] |chapter=3 |doi=10.1007/978-0-387-49983-3|chapter-url=http://cds.cern.ch/record/1105894 }}</ref><ref name=\"RadiationOncologyPrimer\">{{cite book |title=Radiation Oncology Primer and Review |isbn=978-1-62070-004-4 |last1=Best |first1=Lara |last2=Rodrigues |first2=George |last3=Velker |first3=Vikram |publisher=[[Demos Medical Publishing]] |year=2013 |chapter=1.3}}</ref><ref>{{cite book |title=Modern Nuclear Chemistry |isbn=978-0-471-11532-8 |last1=Loveland |first1=W. |last2=Morrissey |first2=D. |last3=[[Glenn T. Seaborg|Seaborg]] |first3=G.T. |publisher=Wiley-Interscience |year=2006 |page=57|bibcode=2005mnc..book.....L }}</ref> regardless of how long the atom has existed. However, for a collection of atoms, the expected decay rate is characterized in terms of <!--refers to atoms, not collection, which may include various nuclides, reflecting following pluralities... --> measured [[decay constant]]s or [[Half-life|half-lives]]. This is the basis of [[radiometric dating]]. The half-lives of radioactive atoms have no known upper limit, spanning a time range of over 55 [[Time (Orders of magnitude)|orders of magnitude]], from nearly instantaneous to far longer than the [[age of the universe]].\n\nA radioactive nucleus with zero [[particle spin|spin]] can have no defined orientation, and hence emits the total [[momentum (physics)|momentum]] of its decay products [[isotropy|isotropically]] (all directions and without bias). If there are multiple particles produced during a single decay, as in [[beta decay]], their ''relative'' angular distribution, or spin directions may not be isotropic. Decay products from a nucleus with spin may be distributed non-isotropically with respect to that spin direction, either because of an external influence such as an [[electromagnetic field]], or because the nucleus was produced in a dynamic process that constrained the direction of its spin. Such a parent process could be a previous decay, or a [[nuclear reaction]].<ref name=\"LitherlandFerguson1961\">{{cite journal|title=Gamma-Ray Angular Correlations from Aligned Nuclei Produced by Nuclear Reactions |first1=A.E. |last1=Litherland |first2=A.J. |last2=Ferguson |journal=[[Canadian Journal of Physics]] |year=1961 |volume=39 |issue=6 |pages=788–824 |doi=10.1139/p61-089 |issn=0008-4204 |bibcode=1961CaJPh..39..788L }}</ref><ref>{{cite book| series=Methods in Experimental Physics |journal=<!-- -->|chapter=3. Nuclear and Atomic Spectroscopy |title=Spectroscopy|volume=13|year=1976|pages=115–346|doi=10.1016/S0076-695X(08)60643-2|bibcode=1976MExP...13..115.|isbn=9780124759138}}</ref><ref name=\"B.R. Martin\">{{cite book |last1=Martin |first1=B.R. |title=Nuclear and particle physics: An introduction |edition=2nd |publisher=John Wiley & Sons |date=31 August 2011 |isbn= 978-1-1199-6511-4 |page=240}}</ref><ref group=note>See [[Wu experiment]] among other counterexamples when the decaying atom is influenced by external factors.</ref>\n\nThe decaying nucleus is called the ''parent [[radionuclide]]'' (or ''parent radioisotope''<ref group=note>Radionuclide is the more correct term, but radioisotope is also used. The difference between isotope and nuclide is explained at [[Isotope#Isotope vs. nuclide]].</ref>), and the process produces at least one ''daughter nuclide''. Except for gamma decay or internal conversion from a nuclear [[excited state]], the decay is a [[nuclear transmutation]] resulting in a daughter containing a different number of [[proton]]s or [[neutron]]s (or both). When the number of protons changes, an atom of a different [[chemical element]] is created.\n\nThe first decay processes to be discovered were alpha decay, beta decay, and gamma decay. [[Alpha decay]] occurs when the nucleus ejects an alpha particle (helium nucleus). This is the most common process of emitting [[nucleon]]s, but highly excited nuclei can eject single nucleons, or in the case of [[cluster decay]], specific light nuclei of other elements. [[Beta decay]] occurs in two ways:\n(i) beta-minus decay, when the nucleus emits an electron and an antineutrino in a process that changes a neutron to a proton, or\n(ii) beta-plus decay, when the nucleus emits a [[positron]] and a neutrino in a process that changes a proton to a neutron.\nHighly excited neutron-rich nuclei, formed as the product of other types of decay, occasionally lose energy by way of neutron emission, resulting in a change from one [[isotope]] to another of the same element. The nucleus may capture an orbiting electron, causing a proton to convert into a neutron in a process called electron capture. All of these processes result in a well-defined nuclear transmutation.\n\nBy contrast, there are radioactive decay processes that do not result in a nuclear transmutation. The energy of an excited nucleus may be emitted as a gamma ray in a process called [[gamma decay]], or that energy may be lost when the nucleus interacts with an orbital electron causing its ejection from the atom, in a process called [[internal conversion]].\n\nAnother type of radioactive decay results in products that vary, appearing as two or more \"fragments\" of the original nucleus with a range of possible masses. This decay, called spontaneous [[nuclear fission|fission]], happens when a large unstable nucleus spontaneously splits into two (or occasionally three) smaller daughter nuclei, and generally leads to the emission of gamma rays, neutrons, or other particles from those products.\n\nFor a summary table showing the number of stable and radioactive nuclides in each category, see [[radionuclide]]. There are 28 naturally occurring chemical elements on Earth that are radioactive, consisting of 33 radionuclides (5 elements have 2 different radionuclides) that date before the time of formation of the solar system. These 33 are known as [[primordial nuclide]]s. Well-known examples are [[uranium]] and [[thorium]], but also included are naturally occurring long-lived radioisotopes, such as [[potassium-40]]. Another 50 or so shorter-lived radionuclides, such as [[radium]] and [[radon]], found on Earth, are the products of [[decay chain]]s that began with the primordial nuclides, or are the product of ongoing [[cosmogenic nuclide|cosmogenic]] processes, such as the production of [[carbon-14]] from [[nitrogen-14]] in the atmosphere by [[cosmic rays]]. Radionuclides may also be [[Synthetic element|produced artificially]] in [[particle accelerators]] or [[nuclear reactors]], resulting in 650 of these with half-lives of over an hour, and several thousand more with even shorter half-lives. (See [[List of nuclides]] for a list of these sorted by half-life.)\n\n==History of discovery==\n[[File:Pierre and Marie Curie.jpg|thumb|upright=0.9|Pierre and Marie Curie in their Paris laboratory, before 1907]]\n\nRadioactivity was discovered in 1896 by the [[France|French]] scientist [[Henri Becquerel]], while working with [[Phosphorescence|phosphorescent]] materials.<ref>{{cite book |last=Mould |first=Richard F. |title=A century of X-rays and radioactivity in medicine : with emphasis on photographic records of the early years |date=1995 |publisher=Inst. of Physics Publ. |location=Bristol |isbn=978-0-7503-0224-1 |page=12 |edition=Reprint. with minor corr}}</ref> These materials glow in the dark after exposure to light, and he suspected that the glow produced in [[cathode ray tube]]s by [[X-ray]]s might be associated with phosphorescence. He wrapped a photographic plate in black paper and placed various phosphorescent [[salt (chemistry)|salts]] on it. All results were negative until he used [[uranium]] salts. The uranium salts caused a blackening of the plate in spite of the plate being wrapped in black paper. These radiations were given the name \"Becquerel Rays\".\n\nIt soon became clear that the blackening of the plate had nothing to do with phosphorescence, as the blackening was also produced by non-phosphorescent [[salts]] of uranium and by metallic uranium. It became clear from these experiments that there was a form of invisible radiation that could pass through paper and was causing the plate to react as if exposed to light.\n\nAt first, it seemed as though the new radiation was similar to the then recently discovered X-rays. Further research by Becquerel, [[Ernest Rutherford]], [[Paul Villard]], [[Pierre Curie]], [[Marie Curie]], and others showed that this form of radioactivity was significantly more complicated. Rutherford was the first to realize that all such elements decay in accordance with the same mathematical exponential formula. Rutherford and his student [[Frederick Soddy]] were the first to realize that many decay processes resulted in the [[nuclear transmutation|transmutation]] of one element to another. Subsequently, the [[radioactive displacement law of Fajans and Soddy]] was formulated to describe the products of [[alpha decay|alpha]] and [[beta decay]].<ref>Kasimir Fajans, \"Radioactive transformations and the periodic system of the elements\". [[Chemische Berichte|Berichte der Deutschen Chemischen Gesellschaft]], Nr. 46, 1913, pp. 422–439</ref><ref>Frederick Soddy, \"The Radio Elements and the Periodic Law\", Chem. News, Nr. 107, 1913, pp. 97–99</ref>\n\nThe early researchers also discovered that many other [[chemical element]]s, besides uranium, have [[Radionuclide|radioactive isotopes]]. A systematic search for the total radioactivity in uranium ores also guided Pierre and Marie Curie to isolate two new elements: [[polonium]] and [[radium]]. Except for the radioactivity of radium, the chemical similarity of radium to [[barium]] made these two elements difficult to distinguish.\n\nMarie and Pierre Curie's study of radioactivity is an important factor in science and medicine. After their research on Becquerel's rays led them to the discovery of both radium and polonium, they coined the term \"radioactivity\".<ref name=\":0\">{{Cite book|title=Radioactivity: Introduction and History|last=L'Annunziata|first=Michael F.|publisher=Elsevier Science|year=2007|isbn=9780080548883|location=Amsterdam, Netherlands|pages=2}}</ref> Their research on the penetrating rays in uranium and the discovery of radium launched an era of using radium for the treatment of cancer. Their exploration of radium could be seen as the first peaceful use of nuclear energy and the start of modern [[nuclear medicine]].<ref name=\":0\" />\n\n==Early health dangers==\n{{main|Ionizing radiation}}\n[[File:Crookes tube xray experiment.jpg|thumb|upright=1.8|Taking an X-ray image with early [[Crookes tube]] apparatus in 1896. The Crookes tube is visible in the centre. The standing man is viewing his hand with a [[fluoroscope]] screen; this was a common way of setting up the tube. No precautions against radiation exposure are being taken; its hazards were not known at the time.]]\nThe dangers of [[ionizing radiation]] due to radioactivity and X-rays were not immediately recognized.\n\n===X-rays===\nThe discovery of x‑rays by [[Wilhelm Röntgen]] in 1895 led to widespread experimentation by scientists, physicians, and inventors. Many people began recounting stories of burns, hair loss and worse in technical journals as early as 1896. In February of that year, Professor Daniel and Dr. Dudley of [[Vanderbilt University]] performed an experiment involving X-raying Dudley's head that resulted in his hair loss. A report by Dr. H.D. Hawks, of his suffering severe hand and chest burns in an X-ray demonstration, was the first of many other reports in ''Electrical Review''.<ref name=\"SansareKhanna2011\">{{cite journal |last1=Sansare |first1=K. |last2=Khanna |first2=V. |last3=Karjodkar |first3=F. |title=Early victims of X-rays: a tribute and current perception |journal=Dentomaxillofacial Radiology |volume=40 |issue=2 |year=2011 |pages=123–125 |issn=0250-832X |doi=10.1259/dmfr/73488299 |pmc=3520298 |pmid=21239576}}</ref>\n\nOther experimenters, including [[Elihu Thomson]] and [[Nikola Tesla]], also reported burns. Thomson deliberately exposed a finger to an X-ray tube over a period of time and suffered pain, swelling, and blistering.<ref name=\"physics.isu.edu\">[http://www.physics.isu.edu/radinf/50yrs.htm Ronald L. Kathern and Paul L. Ziemer, he First Fifty Years of Radiation Protection, physics.isu.edu]</ref> Other effects, including ultraviolet rays and ozone, were sometimes blamed for the damage,<ref>{{Cite journal |title=Nikola Tesla and the Discovery of X-rays |journal=RadioGraphics |date=July 2008 |volume=28 |issue=4 |pmid=18635636 |pages=1189–92 |doi=10.1148/rg.284075206 |last1=Hrabak |first1=M. |last2=Padovan |first2=R.S. |last3=Kralik |first3=M. |last4=Ozretic |first4=D. |last5=Potocki |first5=K.}}</ref> and many physicians still claimed that there were no effects from X-ray exposure at all.<ref name=\"physics.isu.edu\" />\n\nDespite this, there were some early systematic hazard investigations, and as early as 1902 [[William Herbert Rollins]] wrote almost despairingly that his warnings about the dangers involved in the careless use of X-rays were not being heeded, either by industry or by his colleagues. By this time, Rollins had proved that X-rays could kill experimental animals, could cause a pregnant guinea pig to abort, and that they could kill a fetus.<ref name=\"taming\">{{citation |title=Taming the Rays - A history of Radiation and Protection. |author=Geoff Meggitt |publisher=[[Lulu.com]] |year=2008 |isbn=978-1-4092-4667-1}}</ref> He also stressed that \"animals vary in susceptibility to the external action of X-light\" and warned that these differences be considered when patients were treated by means of X-rays.\n\n===Radioactive substances===\n[[File:Periodic Table Stability & Radioactivity.svg|upright=1.8|right|thumb\n|Radioactivity is characteristic of elements with large atomic number. Elements with at least one stable isotope are shown in light blue. Green shows elements of which the most stable isotope has a half-life measured in millions of years. Yellow and orange are progressively less stable, with half-lives in thousands or hundreds of years, down toward one day. Red and purple show highly and extremely radioactive elements where the most stable isotopes exhibit half-lives measured on the order of one day and much less.]]\nHowever, the biological effects of radiation due to radioactive substances were less easy to gauge. This gave the opportunity for many physicians and corporations to market radioactive substances as [[patent medicine]]s. Examples were radium [[enema]] treatments, and radium-containing waters to be drunk as tonics. [[Marie Curie]] protested against this sort of treatment, warning that the effects of radiation on the human body were not well understood. Curie later died from [[aplastic anaemia]], likely caused by exposure to ionizing radiation. By the 1930s, after a number of cases of bone necrosis and death of radium treatment enthusiasts, radium-containing medicinal products had been largely removed from the market ([[radioactive quackery]]).\n\n===Radiation protection===\n{{main|Radiation protection}}\n{{see also|Sievert|Ionizing radiation}}\nOnly a year after [[Wilhelm Röntgen|Röntgen's]] discovery of X rays, the American engineer Wolfram Fuchs (1896) gave what is probably the first protection advice, but it was not until 1925 that the first International Congress of Radiology (ICR) was held and considered establishing international protection standards. The effects of radiation on genes, including the effect of cancer risk, were recognized much later. In 1927, [[Hermann Joseph Muller]] published research showing genetic effects and, in 1946, was awarded the [[Nobel Prize in Physiology or Medicine]] for his findings.\n\nThe second ICR was held in Stockholm in 1928 and proposed the adoption of the rontgen unit, and the 'International X-ray and Radium Protection Committee' (IXRPC) was formed. [[Rolf Maximilian Sievert|Rolf Sievert]] was named Chairman, but a driving force was George Kaye of the British [[National Physical Laboratory (United Kingdom)|National Physical Laboratory]]. The committee met in 1931, 1934 and 1937.\n\nAfter [[World War II]], the increased range and quantity of [[radioactive]] substances being handled as a result of military and civil nuclear programmes led to large groups of occupational workers and the public being potentially exposed to harmful levels of ionising radiation. This was considered at the first post-war ICR convened in London in 1950, when the present [[International Commission on Radiological Protection]] (ICRP) was born.<ref>{{cite journal|last=Clarke|first=R.H.|author2=J. Valentin|title=The History of ICRP and the Evolution of its Policies|journal=Annals of the ICRP|year=2009|volume=39|series=ICRP Publication 109|issue=1|pages=75–110|doi=10.1016/j.icrp.2009.07.009|url=http://www.icrp.org/docs/The%20History%20of%20ICRP%20and%20the%20Evolution%20of%20its%20Policies.pdf|accessdate=12 May 2012}}</ref>\nSince then the ICRP has developed the present international system of radiation protection, covering all aspects of radiation hazard.\n\n==Units of radioactivity==\n[[File:Radioactivity and radiation.png|thumb|Graphic showing relationships between radioactivity and detected ionizing radiation]]\nThe [[International System of Units]] (SI) unit of radioactive activity is the [[becquerel]] (Bq), named in honor of the scientist [[Henri Becquerel]]. One Bq is defined as one transformation (or decay or disintegration) per second.\n\nAn older unit of radioactivity is the [[curie]], Ci, which was originally defined as \"the quantity or mass of [[radium emanation]] in [[secular equilibrium|equilibrium]] with one gram of [[radium]] (element)\".<ref>{{cite journal |last1=Rutherford |first1=Ernest |title=Radium Standards and Nomenclature |journal=Nature |date=6 October 1910 |volume=84 |issue=2136 |pages=430–431 |url=https://archive.org/details/nature841910lock |doi=10.1038/084430a0|bibcode = 1910Natur..84..430R }}</ref> Today, the curie is defined as {{val|3.7|e=10}} disintegrations per second, so that 1&nbsp;[[curie]] (Ci) = {{val|3.7|e=10|u=Bq}}.\nFor radiological protection purposes, although the United States Nuclear Regulatory Commission permits the use of the unit [[curie]] alongside SI units,<ref>{{cite book\n |title=10 CFR 20.1005\n |year=2009\n |publisher=US Nuclear Regulatory Commission\n |url=https://www.nrc.gov/reading-rm/doc-collections/cfr/part020/part020-1005.html}}\n</ref> the [[European Union]] [[European units of measurement directives]] required that its use for \"public health ... purposes\" be phased out by 31 December 1985.<ref>{{cite web\n| url = http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:31980L0181:EN:NOT\n| author = The Council of the European Communities\n| title = Council Directive 80/181/EEC of 20 December 1979 on the approximation of the laws of the Member States relating to Unit of measurement and on the repeal of Directive 71/354/EEC\n| date=1979-12-21\n| accessdate=19 May 2012}}</ref>\n\nThe effects of ionizing radiation are often measured in units of [[Gray (unit)|gray]] for mechanical or [[sievert]] for damage to tissue.\n\n==Types of decay==\n[[File:Alfa beta gamma radiation.svg|upright=0.7|thumb|[[Alpha particle]]s may be completely stopped by a sheet of paper, [[beta particle]]s by aluminium shielding. [[Gamma ray]]s can only be reduced by much more substantial mass, such as a very thick layer of [[lead]].]]\n{{Main|Nuclear drip line|Gamma decay|Internal conversion|Electron capture|Alpha decay|Nuclear fission|Neutron emission|Cluster emission}}\nEarly researchers found that an [[Electric field|electric]] or [[magnetic field]] could split radioactive emissions into three types of beams. The rays were given the names [[Alpha particle|alpha]], [[Beta particle|beta]], and [[Gamma ray|gamma]], in increasing order of their ability to penetrate matter. Alpha decay is observed only in heavier elements of atomic number 52 ([[tellurium]]) and greater, with the exception of [[Isotopes of beryllium|beryllium-8]] which decays to two alpha particles. The other two types of decay are produced by all of the elements. Lead, [[atomic number]] 82, is the heaviest element to have any isotopes stable (to the limit of measurement) to radioactive decay. Radioactive decay is seen in all isotopes of all elements of atomic number 83 ([[bismuth]]) or greater. Bismuth-209, however, is only very slightly radioactive, with a half-life greater than the age of the universe; radioisotopes with extremely long half-lives are considered effectively stable for practical purposes.\n[[File:Radioactive decay modes.svg|upright=0.9|thumb|Transition diagram for decay modes of a [[radionuclide]], with neutron number ''N'' and [[atomic number]] ''Z'' (shown are [[alpha particle|α]], [[electron|β<sup>±</sup>]], [[proton|p<sup>+</sup>]], and [[neutron|n<sup>0</sup>]] emissions, EC denotes [[electron capture]]).]]\n[[File:Table isotopes en.svg|thumb|right|Types of radioactive decay related to N and Z numbers]]\n\nIn analysing the nature of the decay products, it was obvious from the direction of the [[electromagnetic force]]s applied to the radiations by external magnetic and electric fields that [[alpha particle]]s carried a positive charge, [[beta decay|beta particles]] carried a negative charge, and [[gamma ray]]s were neutral. From the magnitude of deflection, it was clear that [[alpha particles]] were much more massive than [[beta particles]]. Passing alpha particles through a very thin glass window and trapping them in a [[neon lamp|discharge tube]] allowed researchers to study the [[emission spectrum]] of the captured particles, and ultimately proved that alpha particles are [[helium]] nuclei. Other experiments showed beta radiation, resulting from decay and [[cathode ray]]s, were high-speed [[electrons]]. Likewise, gamma radiation and X-rays were found to be high-energy [[electromagnetic radiation]].\n\nThe relationship between the types of decays also began to be examined: For example, gamma decay was almost always found to be associated with other types of decay, and occurred at about the same time, or afterwards. Gamma decay as a separate phenomenon, with its own half-life (now termed [[isomeric transition]]), was found in natural radioactivity to be a result of the gamma decay of excited metastable [[nuclear isomer]]s, which were in turn created from other types of decay.\n\nAlthough alpha, beta, and gamma radiations were most commonly found, other types of emission were eventually discovered. Shortly after the discovery of the [[positron]] in cosmic ray products, it was realized that the same process that operates in classical [[beta decay]] can also produce positrons ([[positron emission]]), along with [[neutrino]]s (classical beta decay produces antineutrinos). In a more common analogous process, called [[electron capture]], some proton-rich nuclides were found to capture their own atomic electrons instead of emitting positrons, and subsequently these nuclides emit only a neutrino and a gamma ray from the excited nucleus (and often also [[Auger electron]]s and [[characteristic X-ray]]s, as a result of the re-ordering of electrons to fill the place of the missing captured electron). These types of decay involve the nuclear capture of electrons or emission of electrons or positrons, and thus acts to move a nucleus toward the ratio of neutrons to protons that has the least energy for a given total number of [[nucleon]]s. This consequently produces a more stable (lower energy) nucleus.\n\n(A theoretical process of [[positron capture]], analogous to electron capture, is possible in antimatter atoms, but has not been observed, as complex antimatter atoms beyond [[antihelium]] are not experimentally available.<ref>[http://chemed.chem.purdue.edu/genchem/topicreview/bp/ch23/modes.php#fission Radioactive Decay<!-- Bot generated title -->]</ref> Such a decay would require antimatter atoms at least as complex as [[beryllium-7]], which is the lightest known isotope of normal matter to undergo decay by electron capture.)\n\nShortly after the discovery of the [[neutron]] in 1932, [[Enrico Fermi]] realized that certain rare beta-decay reactions immediately yield neutrons as a decay particle ([[neutron emission]]). Isolated [[proton emission]] was eventually observed in some elements. It was also found that some heavy elements may undergo [[spontaneous fission]] into products that vary in composition. In a phenomenon called [[cluster decay]], specific combinations of neutrons and protons other than alpha particles (helium nuclei) were found to be spontaneously emitted from atoms.\n\nOther types of radioactive decay were found to emit previously-seen particles, but via different mechanisms. An example is [[internal conversion]], which results in an initial electron emission, and then often further [[characteristic X-ray]]s and [[Auger electron]]s emissions, although the internal conversion process involves neither beta nor gamma decay. A neutrino is not emitted, and none of the electron(s) and photon(s) emitted originate in the nucleus, even though the energy to emit all of them does originate there. Internal conversion decay, like [[isomeric transition]] gamma decay and neutron emission, involves the release of energy by an excited nuclide, without the transmutation of one element into another.\n\nRare events that involve a combination of two beta-decay type events happening simultaneously are known (see below). Any decay process that does not violate the conservation of energy or momentum laws (and perhaps other particle conservation laws) is permitted to happen, although not all have been detected. An interesting example discussed in a final section, is [[Beta decay#Bound-state β− decay|bound state beta decay]] of [[rhenium-187]]. In this process, beta electron-decay of the parent nuclide is not accompanied by beta electron emission, because the beta particle has been captured into the K-shell of the emitting atom. An antineutrino is emitted, as in all negative beta decays.\n\nRadionuclides can undergo a number of different reactions. These are summarized in the following table. A nucleus with [[mass number]] ''A'' and [[atomic number]] ''Z'' is represented as (''A'', ''Z''). The column \"Daughter nucleus\" indicates the difference between the new nucleus and the original nucleus. Thus, (''A''&nbsp;−&nbsp;1, ''Z'') means that the mass number is one less than before, but the atomic number is the same as before.\n\nIf energy circumstances are favorable, a given radionuclide may undergo many competing types of decay, with some atoms decaying by one route, and others decaying by another. An example is [[copper-64]], which has 29 protons, and 35 neutrons, which decays with a half-life of about 12.7 hours. This isotope has one unpaired proton and one unpaired neutron, so either the proton or the neutron can decay to the other particle, which has opposite [[isospin]]. This particular nuclide (though not all nuclides in this situation) is almost equally likely to decay through [[positron emission]] (18%), or through [[electron capture]] (43%), as it does through electron emission (39%). The excited energy states resulting from these decays which fail to end in a ground energy state, also produce later [[internal conversion]] and [[gamma decay]] in almost 0.5% of the time.\n\nMore common in heavy nuclides is competition between alpha and beta decay. The daughter nuclides will then normally decay through beta or alpha, respectively, to end up in the same place.\n\n{{anchor|Decay modes in table form}}\n{|class=\"wikitable\"\n|- style=\"background:#eee0e0; white-space:nowrap;\"\n!Mode of decay !! Participating particles!!Daughter nucleus\n|-\n| style=\"background:#ccc;\" colspan=\"3\"|'''Decays with emission of nucleons:'''\n|-\n|[[Alpha decay]] || An [[alpha particle]] (''A''&nbsp;=&nbsp;4, ''Z''&nbsp;=&nbsp;2) emitted from nucleus || (''A''&nbsp;−&nbsp;4, ''Z''&nbsp;−&nbsp;2)\n|-\n|[[Proton emission]] || A [[proton]] ejected from nucleus || (''A''&nbsp;−&nbsp;1, ''Z''&nbsp;−&nbsp;1)\n|-\n|[[Neutron emission]] || A [[neutron]] ejected from nucleus || (''A''&nbsp;−&nbsp;1, ''Z'')\n|-\n|[[Proton emission|Double proton emission]] || Two protons ejected from nucleus simultaneously|| (''A''&nbsp;−&nbsp;2, ''Z''&nbsp;−&nbsp;2)\n|-\n|[[Spontaneous fission]] || Nucleus disintegrates into two or more smaller nuclei and other particles || —\n|-\n|[[Cluster decay]] || Nucleus emits a specific type of smaller nucleus (''A''<sub>1</sub>, ''Z''<sub>1</sub>) which is larger than an alpha particle || (''A''&nbsp;−&nbsp;''A''<sub>1</sub>, ''Z''&nbsp;−&nbsp;''Z''<sub>1</sub>) + (''A''<sub>1</sub>, ''Z''<sub>1</sub>)\n|-\n| style=\"background:#ccc;\" colspan=\"3\"| '''Different modes of beta decay:'''\n|-\n|[[Beta decay|β<sup>−</sup> decay]] || A nucleus emits an [[electron]] and an [[electron antineutrino]] || (''A'', ''Z''&nbsp;+&nbsp;1)\n|-\n|[[Positron emission]] ([[beta decay|β<sup>+</sup> decay]]) || A nucleus emits a [[positron]] and an [[electron neutrino]] || (''A'', ''Z''&nbsp;−&nbsp;1)\n|-\n|[[Electron capture]] || A nucleus captures an orbiting electron and emits a neutrino; the daughter nucleus is left in an excited unstable state || (''A'', ''Z''&nbsp;−&nbsp;1)\n|-\n|[[Beta decay#Bound-state β− decay|Bound state beta decay]] || A free neutron or nucleus beta decays to electron and antineutrino, but the electron is not emitted, as it is captured into an empty K-shell; the daughter nucleus is left in an excited and unstable state. This process is a minority of free neutron decays (0.0004%) due to the low energy of hydrogen ionization, and is suppressed except in ionized atoms that have K-shell vacancies. || (''A'', ''Z''&nbsp;+&nbsp;1)\n|-\n|[[Double beta decay]] || A nucleus emits two electrons and two antineutrinos || (''A'', ''Z''&nbsp;+&nbsp;2)\n|-\n|[[Double electron capture]] || A nucleus absorbs two orbital electrons and emits two neutrinos&nbsp;– the daughter nucleus is left in an excited and unstable state || (''A'', ''Z''&nbsp;−&nbsp;2)\n|-\n|[[Electron capture]] with [[positron emission]] || A nucleus absorbs one orbital electron, emits one positron and two neutrinos || (''A'', ''Z''&nbsp;−&nbsp;2)\n|-\n|[[Double positron decay]] || A nucleus emits two positrons and two neutrinos || (''A'', ''Z''&nbsp;−&nbsp;2)\n|-\n| style=\"background:#ccc;\" colspan=\"3\"| '''Transitions between states of the same nucleus''':\n|-\n|[[Isomeric transition]] || Excited nucleus releases a high-energy [[photon]] ([[gamma ray]]) || (''A'', ''Z'')\n|-\n|[[Internal conversion]] || Excited nucleus transfers energy to an orbital electron, which is subsequently ejected from the atom || (''A'', ''Z'')\n|}\n\nRadioactive decay results in a reduction of summed rest [[mass]], once the released energy (the ''disintegration energy'') has escaped in some way. Although decay energy is sometimes defined as associated with the difference between the mass of the parent nuclide products and the mass of the decay products, this is true only of rest mass measurements, where some energy has been removed from the product system. This is true because the decay energy must always carry mass with it, wherever it appears (see [[mass in special relativity]]) according to the formula [[E=mc2|''E''&nbsp;=&nbsp;''mc''<sup>2</sup>]]. The decay energy is initially released as the energy of emitted photons plus the kinetic energy of massive emitted particles (that is, particles that have rest mass). If these particles come to [[thermal equilibrium]] with their surroundings and photons are absorbed, then the decay energy is transformed to thermal energy, which retains its mass.\n\nDecay energy therefore remains associated with a certain measure of mass of the decay system, called [[invariant mass]], which does not change during the decay, even though the energy of decay is distributed among decay particles. The energy of photons, the kinetic energy of emitted particles, and, later, the thermal energy of the surrounding matter, all contribute to the [[invariant mass]] of the system. Thus, while the sum of the rest masses of the particles is not conserved in radioactive decay, the ''system'' mass and system [[invariant mass]] (and also the system total energy) is conserved throughout any decay process. This is a restatement of the equivalent laws of [[conservation of energy]] and [[conservation of mass]].\n\n==Radioactive decay rates==\n<!-- \"Neutron activation analysis\" links here -->\n\nThe ''decay rate'', or ''activity'', of a radioactive substance is characterized by:\n\n'''Constant quantities''':\n* The ''[[half-life]]''{{mdash}}{{math|<big>''t''</big><sub>1/2</sub>}}, is the time taken for the activity of a given amount of a radioactive substance to decay to half of its initial value; see [[List of nuclides]].\n* The ''[[decay constant]]''{{mdash}} {{bigmath|''λ''}}, \"[[lambda]]\" the reciprocal of the mean lifetime, sometimes referred to as simply ''decay rate''.\n* The ''[[mean lifetime]]''{{mdash}} {{bigmath|''τ''}}, \"[[tau]]\" the average lifetime (1/[[E (mathematical constant)|e]] life) of a radioactive particle before decay.\n\nAlthough these are constants, they are associated with the [[Statistical mechanics|statistical behavior of populations]] of atoms. In consequence, predictions using these constants are less accurate for minuscule samples of atoms.\n\nIn principle a half-life, a third-life, or even a (1/{{sqrt|2}})-life, can be used in exactly the same way as half-life; but the mean life and half-life {{math|<big>''t''</big><sub>1/2</sub>}} have been adopted as standard times associated with exponential decay.\n\n'''Time-variable quantities''':\n* ''Total activity''{{mdash}}{{bigmath| ''A''}}, is the number of decays per unit time of a radioactive sample.\n* ''Number of particles''{{mdash}}{{bigmath|''N''}}, is the total [[number of particles]] in the sample.\n* ''Specific activity''{{mdash}}{{bigmath|''S<sub>A</sub>''}}, number of decays per unit time per amount of substance of the sample at time set to zero (''t'' = 0). \"Amount of substance\" can be the mass, volume or moles of the initial sample.\n\nThese are related as follows:\n:<math> t_{1/2} = \\frac{\\ln(2)}{\\lambda} = \\tau \\ln(2)</math>\n:<math> A = - \\frac{\\mathrm{d}N}{\\mathrm{d}t} = \\lambda N </math>\n:<math> S_A a_0 = - \\frac{\\mathrm{d}N}{\\mathrm{d}t}\\bigg|_{t=0} = \\lambda N_0 </math>\nwhere ''N''<sub>0</sub> is the initial amount of active substance&nbsp;— substance that has the same percentage of unstable particles as when the substance was formed.\n\n==Mathematics of radioactive decay==\n{{textbook|date=December 2016}}\n{{hatnote|For the mathematical details of exponential decay in general context, see [[exponential decay]].}}\n{{hatnote|For related derivations with some further details, see [[half-life]].}}\n{{hatnote|For the analogous mathematics in 1st order chemical reactions, see [[Rate equation#Consecutive reactions|Consecutive reactions]].}}\n\n===Universal law of radioactive decay===\n\nRadioactivity is one very frequently given example of [[exponential decay]]. The law describes the statistical behaviour of a large number of nuclei, rather than one individual nucleus. In the following formalism, the number of nuclei or the nuclei population ''N'', is of course a discrete variable (a [[natural number]])—but for any physical sample ''N'' is so large that it can be treated as a continuous variable. [[Differential calculus]] is used to model the behaviour of nuclear decay.\n\nThe mathematics of radioactive decay depend on a key assumption that a nucleus of a radionuclide has no \"memory\" or way of translating its history into its present behavior. A nucleus does not \"age\" with the passage of time. Thus, the probability of its breaking down does not increase with time, but stays constant no matter how long the nucleus has existed. This constant probability may vary greatly between different types of nuclei, leading to the many different observed decay rates. However, whatever the probability is, it does not change. This is in marked contrast to complex objects which do show aging, such as automobiles and humans. These systems do have a chance of breakdown per unit of time, that increases from the moment they begin their existence.\n\n====One-decay process====\nConsider the case of a nuclide {{math|''A''}} that decays into another {{math|''B''}} by some process {{math|''A → B''}} (emission of other particles, like [[electron neutrino]]s {{SubatomicParticle|Electron antineutrino}} and [[electron]]s e<sup>−</sup> as in [[beta decay]], are irrelevant in what follows). The decay of an unstable nucleus is entirely random in time so it is impossible to predict when a particular atom will decay. However, it is equally likely to decay at any instant in time. Therefore, given a sample of a particular radioisotope, the number of decay events {{math|−d''N''}} expected to occur in a small interval of time {{math|d''t''}} is proportional to the number of atoms present {{math|''N''}}, that is<ref name=\"Patel\">{{cite book |last=Patel |first=S.B. |title=Nuclear physics: an introduction |year=2000 |publisher=New Age International |location=New Delhi |isbn=978-81-224-0125-7 |pages=62–72}}</ref>\n\n:<math>-\\frac{\\mathrm{d}N}{\\mathrm{d}t} \\propto N.</math>\n\nParticular radionuclides decay at different rates, so each has its own decay constant {{bigmath|λ}}. The expected decay {{math|−d''N''/''N''}} is proportional to an increment of time, {{math|d''t''}}:\n\n{{Equation box 1\n|indent=:\n|title=\n|equation=<math> -\\frac{\\mathrm{d}N}{N} = \\lambda \\mathrm{d}t</math>\n|cellpadding\n|border\n|border colour = #50C878\n|background colour = #ECFCF4}}\n\nThe negative sign indicates that {{math|''N''}} decreases as time increases, as the decay events follow one after another. The solution to this first-order [[differential equation]] is the [[mathematical function|function]]:\n\n:<math>N(t) = N_0\\,e^{-{\\lambda}t}</math>\n\nwhere {{math|''N''<sub>0</sub>}} is the value of {{math|''N''}} at time {{math|''t''}} = 0, with the decay constant expressed as {{math|λ}}<ref name=\"Patel\" />\n\nWe have for all time {{math|''t''}}:\n\n:<math> N_A + N_B = N_\\mathrm{total} = N_{A0}, </math>\n\nwhere {{math|''N''{{sub|total}}}} is the constant number of particles throughout the decay process, which is equal to the initial number of {{math|''A''}} nuclides since this is the initial substance.\n\nIf the number of non-decayed {{math|''A''}} nuclei is:\n\n:<math>N_A = N_{A0}e^{-{\\lambda}t} </math>\n\nthen the number of nuclei of {{math|''B''}}, i.e. the number of decayed {{math|''A''}} nuclei, is\n\n:<math> N_B = N_{A0} - N_A = N_{A0} - N_{A0}e^{-{\\lambda}t} = N_{A0} \\left ( 1 - e^{-{\\lambda}t} \\right ).</math>\n\nThe number of decays observed over a given interval obeys [[Poisson statistics]]. If the average number of decays is {{math|{{angbr|''N''}}}}, the probability of a given number of decays {{math|''N''}} is<ref name=\"Patel\" />\n:<math> P(N) = \\frac{\\langle N \\rangle^N \\exp(-\\langle N\\rangle)}{N!} .</math>\n\n====Chain-decay processes====\n\n'''Chain of two decays'''\n\nNow consider the case of a chain of two decays: one nuclide {{math|''A''}} decaying into another {{math|''B''}} by one process, then {{math|''B''}} decaying into another {{math|''C''}} by a second process, i.e. ''{{math|A → B → C}}''. The previous equation cannot be applied to the decay chain, but can be generalized as follows. Since {{math|''A''}} decays into {{math|''B''}}, ''then'' {{math|''B''}} decays into {{math|''C''}}, the activity of {{math|''A''}} adds to the total number of {{math|''B''}} nuclides in the present sample, ''before'' those {{math|''B''}} nuclides decay and reduce the number of nuclides leading to the later sample. In other words, the number of second generation nuclei {{math|''B''}} increases as a result of the first generation nuclei decay of {{math|''A''}}, and decreases as a result of its own decay into the third generation nuclei {{math|''C''}}.<ref name=\"Introductory Nuclear Physics 1988\">Introductory Nuclear Physics, K.S. Krane, 1988, John Wiley & Sons Inc, {{ISBN|978-0-471-80553-3}}</ref> The sum of these two terms gives the law for a decay chain for two nuclides:\n\n:<math>\\frac{\\mathrm{d}N_B}{\\mathrm{d}t} = -\\lambda_B N_B + \\lambda_A N_A.</math>\n\nThe rate of change of {{math|''N<sub>B</sub>''}}, that is {{math|d''N{{sub|B}}''/d''t''}}, is related to the changes in the amounts of {{math|''A''}} and {{math|''B''}}, {{math|''N<sub>B</sub>''}} can increase as {{math|''B''}} is produced from {{math|''A''}} and decrease as {{math|''B''}} produces {{math|''C''}}.\n\nRe-writing using the previous results:\n\n{{Equation box 1\n|indent=:\n|title=\n|equation=<math> \\frac{\\mathrm{d}N_B}{\\mathrm{d}t} = - \\lambda_B N_B + \\lambda_A N_{A0} e^{-\\lambda_A t} </math>\n|cellpadding\n|border\n|border colour = #50C878\n|background colour = #ECFCF4}}\n\nThe subscripts simply refer to the respective nuclides, i.e. {{math|''N{{sub|A}}''}} is the number of nuclides of type {{math|''A''}}, {{math|''N''{{sub|''A''0}}}} is the initial number of nuclides of type {{math|''A''}}, {{math|''λ{{sub|A}}''}} is the decay constant for {{math|''A''}} - and similarly for nuclide {{math|''B''}}. Solving this equation for {{math|''N<sub>B</sub>''}} gives:\n\n:<math> N_B = \\frac{N_{A0}\\lambda_A}{\\lambda_B - \\lambda_A} \\left ( e^{-\\lambda_A t} - e^{-\\lambda_B t}\\right ) . </math>\n\nIn the case where {{math|''B''}} is a stable nuclide ({{math|''λ{{sub|B}}''}} = 0), this equation reduces to the previous solution:\n\n:<math> \\lim_{\\lambda_B\\rightarrow 0} \\left [ \\frac{N_{A0}\\lambda_A}{\\lambda_B - \\lambda_A} \\left ( e^{-\\lambda_A t} - e^{-\\lambda_B t}\\right ) \\right ] = \\frac{N_{A0}\\lambda_A}{0 - \\lambda_A} \\left ( e^{-\\lambda_A t} - 1 \\right ) = N_{A0} \\left ( 1- e^{-\\lambda_A t} \\right ), </math>\n\nas shown above for one decay. The solution can be found by the [[integrating factor|integration factor]] method, where the integrating factor is {{math|''e''{{sup|λ{{sub|''B''}}''t''}}}}. This case is perhaps the most useful, since it can derive both the one-decay equation (above) and the equation for multi-decay chains (below) more directly.\n\n'''Chain of any number of decays'''\n\nFor the general case of any number of consecutive decays in a decay chain, i.e. {{math|A<sub>1</sub> → A<sub>2</sub> ··· → A<sub>''i''</sub> ··· → A<sub>''D''</sub>}}, where {{math|''D''}} is the number of decays and {{math|''i''}} is a dummy index ({{math|''i'' {{=}} {{sub|1, 2, 3, ...''D''}}}}), each nuclide population can be found in terms of the previous population. In this case {{math|''N''<sub>2</sub> {{=}} 0}}, {{math|''N''<sub>3</sub> {{=}} 0}},..., {{math|''N<sub>D</sub>'' {{=}} 0}}. Using the above result in a recursive form:\n\n:<math> \\frac{\\mathrm{d}N_j}{\\mathrm{d}t} = - \\lambda_j N_j + \\lambda_{j-1} N_{(j-1)0} e^{-\\lambda_{j-1} t}. </math>\n\nThe general solution to the recursive problem is given by '''''Bateman's equations''''':<ref name=\"general solution of Bateman\">{{cite journal|last=Cetnar|first=Jerzy|title=General solution of Bateman equations for nuclear transmutations|journal=Annals of Nuclear Energy|date=May 2006|volume=33|issue=7|pages=640–645|url=http://www.sciencedirect.com/science/article/pii/S0306454906000284|doi=10.1016/j.anucene.2006.02.004}}</ref>\n\n{{Equation box 1\n|indent=:\n|title='''Bateman's equations'''\n|equation=<math> N_D = \\frac{N_1(0)}{\\lambda_D} \\sum_{i=1}^D \\lambda_i c_i e^{-\\lambda_i t} </math>\n\n<math> c_i = \\prod_{j=1, i\\neq j}^D \\frac{\\lambda_j}{\\lambda_j - \\lambda_i} </math>\n|cellpadding\n|border\n|border colour = #0073CF\n|background colour=#F5FFFA}}\n\n====Alternative decay modes====\n\nIn all of the above examples, the initial nuclide decays into just one product.<ref>{{cite book|title=Introductory Nuclear Physics|author=K.S. Krane|year=1988|publisher=John Wiley & Sons Inc|page=164|isbn=978-0-471-80553-3}}</ref> Consider the case of one initial nuclide that can decay into either of two products, that is ''{{math|A → B}}'' and ''{{math|A → C}}'' in parallel. For example, in a sample of [[potassium-40]], 89.3% of the nuclei decay to [[calcium-40]] and 10.7% to [[argon-40]]. We have for all time {{math|''t''}}:\n\n:<math> N = N_A + N_B + N_C </math>\n\nwhich is constant, since the total number of nuclides remains constant. Differentiating with respect to time:\n\n:<math> \\begin{align}\n\\frac{\\mathrm{d}N_A}{\\mathrm{d}t} & = - \\left(\\frac{\\mathrm{d}N_B}{\\mathrm{d}t} + \\frac{\\mathrm{d}N_C}{\\mathrm{d}t} \\right) \\\\\n- \\lambda N_A & = - N_A \\left ( \\lambda_B + \\lambda_C \\right ) \\\\\n\\end{align}</math>\n\ndefining the ''total decay constant'' {{math|λ}} in terms of the sum of ''partial decay constants'' {{math|λ<sub>''B''</sub>}} and {{math|λ<sub>''C''</sub>}}:\n\n:<math> \\lambda = \\lambda_B + \\lambda_C . </math>\n\nNotice that\n:<math> \\frac{\\mathrm{d}N_A}{\\mathrm{d}t} < 0,\\frac{\\mathrm{d}N_B}{\\mathrm{d}t} > 0,\n\\frac{\\mathrm{d}N_C}{\\mathrm{d}t} > 0.\n</math>\n\nSolving this equation for {{math|''N{{sub|A}}''}}:\n\n:<math> N_A = N_{A0} e^{-\\lambda t} .</math>\n\nwhere {{math|''N''<sub>''A''0</sub>}} is the initial number of nuclide A. When measuring the production of one nuclide, one can only observe the total decay constant {{math|''λ''}}. The decay constants {{math|''λ{{sub|B}}''}} and {{math|''λ{{sub|C}}''}} determine the probability for the decay to result in products {{math|''B''}} or {{math|''C''}} as follows:\n\n:<math> N_B = \\frac{\\lambda_B}{\\lambda} N_{A0} \\left ( 1 - e^{-\\lambda t} \\right ),</math>\n\n:<math> N_C = \\frac{\\lambda_C}{\\lambda} N_{A0} \\left ( 1 - e^{-\\lambda t} \\right ).</math>\n\nbecause the fraction {{math|''λ{{sub|B}}''/''λ''}} of nuclei decay into {{math|''B''}} while the fraction {{math|''λ{{sub|C}}''/''λ''}} of nuclei decay into {{math|''C''}}.\n\n===Corollaries of the decay laws===\n\nThe above equations can also be written using quantities related to the number of nuclide particles {{math|''N''}} in a sample;\n* The activity: {{math|''A'' {{=}} ''λN''}}.\n* The [[amount of substance]]: {{math|''n'' {{=}} ''N''/''L''}}.\n* The [[mass]]: {{math|1= ''m'' = ''Mn'' = ''nN''/''L''}}.\n\nwhere {{math|''L''}} = {{physconst|NA}} is the [[Avogadro constant]], {{math|''M''}} is the [[molar mass]] of the substance in kg/mol, and the amount of the substance {{math|''n''}} is in [[Mole (unit)|moles]].\n\n===Decay timing: definitions and relations===\n\n====Time constant and mean-life====\n\nFor the one-decay solution ''{{math|A → B}}'':\n\n:<math>N = N_0\\,e^{-{\\lambda}t} = N_0\\,e^{-t/ \\tau}, \\,\\!</math>\n\nthe equation indicates that the [[decay constant]] {{math|''λ''}} has units of ''{{math|t<sup>−1</sup>}}'', and can thus also be represented as 1/{{bigmath|''τ''}}, where {{bigmath|''τ''}} is a characteristic time of the process called the ''[[time constant]]''.\n\nIn a radioactive decay process, this time constant is also the [[mean lifetime]] for decaying atoms. Each atom \"lives\" for a finite amount of time before it decays, and it may be shown that this mean lifetime is the [[arithmetic mean]] of all the atoms' lifetimes, and that it is {{math|''τ''}}, which again is related to the decay constant as follows:\n\n:<math>\\tau = \\frac{1}{\\lambda}.</math>\n\nThis form is also true for two-decay processes simultaneously ''{{math|A → B + C}}'', inserting the equivalent values of decay constants (as given above)\n\n:<math> \\lambda = \\lambda_B + \\lambda_C \\,</math>\n\ninto the decay solution leads to:\n\n:<math>\\frac{1}{\\tau} = \\lambda = \\lambda_B + \\lambda_C = \\frac{1}{\\tau_B} + \\frac{1}{\\tau_C}\\,</math>\n\n[[File:Halflife-sim.gif|thumb|right|Simulation of many identical atoms undergoing radioactive decay, starting with either 4 atoms (left) or 400 (right). The number at the top indicates how many [[half-life|half-lives]] have elapsed.]]\n\n====Half-life====\n\nA more commonly used parameter is the [[half-life]]. Given a sample of a particular radionuclide, the half-life is the time taken for half the radionuclide's atoms to decay. For the case of one-decay nuclear reactions:\n\n:<math>N = N_0\\,e^{-{\\lambda}t} = N_0\\,e^{-t/ \\tau}, \\,\\!</math>\n\nthe half-life is related to the decay constant as follows: set ''{{math|1=N = N{{sub|0}}/2}}'' and {{math|''t''}} = {{math|''T''<sub>1/2</sub>}} to obtain\n\n:<math>t_{1/2} = \\frac{\\ln 2}{\\lambda} = \\tau \\ln 2. </math>\n\nThis relationship between the half-life and the decay constant shows that highly radioactive substances are quickly spent, while those that radiate weakly endure longer. Half-lives of known radionuclides vary widely, from more than [[1 E19 s and more|10<sup>19</sup> years]], such as for the very nearly stable nuclide <sup>209</sup>Bi, to 10<sup>−23</sup> seconds for highly unstable ones.\n\nThe factor of {{math|ln(2)}} in the above relations results from the fact that the concept of \"half-life\" is merely a way of selecting a different base other than the natural base {{math|''e''}} for the lifetime expression. The time constant {{bigmath|''τ''}} is the {{subSup|''e''||-1}}-life, the time until only 1/''e'' remains, about 36.8%, rather than the 50% in the half-life of a radionuclide. Thus, {{bigmath|''τ''}} is longer than {{math|''t''{{sub|1/2}}}}. The following equation can be shown to be valid:\n\n:<math>N(t) = N_0\\,e^{-t/ \\tau} =N_0\\,2^{-t/t_{1/2}}. \\,\\!</math>\n\nSince radioactive decay is exponential with a constant probability, each process could as easily be described with a different constant time period that (for example) gave its \"(1/3)-life\" (how long until only 1/3 is left) or \"(1/10)-life\" (a time period until only 10% is left), and so on. Thus, the choice of {{bigmath|''τ''}} and ''{{math|t{{sub|1/2}}}}'' for marker-times, are only for convenience, and from convention. They reflect a fundamental principle only in so much as they show that the ''same proportion'' of a given radioactive substance will decay, during any time-period that one chooses.\n\nMathematically, the {{math|''n''{{sup|th}}}} life for the above situation would be found in the same way as above{{mdash}}by setting ''{{math|1=N = N{{sub|0}}/n}}'', {{math|1=''t'' = ''T''{{sub|1/''n''}}}} and substituting into the decay solution to obtain\n\n:<math>t_{1/n} = \\frac{\\ln n}{\\lambda} = \\tau \\ln n. </math>\n\n===Example===\n\nA sample of <sup>14</sup>C has a half-life of 5,730 years and a decay rate of 14 disintegration per minute (dpm) per gram of natural [[carbon]].\n\nIf an artifact is found to have radioactivity of 4 dpm per gram of its present C, we can find the approximate age of the object using the above equation:\n:<math> N = N_0\\,e^{-t/ \\tau}, </math>\nwhere: <math> \\frac{N}{ N_0} = 4/14 \\approx 0.286,</math>\n:<math> \\tau = \\frac{T_{1/2}}{\\ln 2} \\approx 8267</math> years,\n:<math> t = -\\tau\\,\\ln\\frac{N}{ N_0} \\approx 10356</math> years.\n\n==Changing decay rates==\nThe radioactive decay modes of [[electron capture]] and [[internal conversion]] are known to be slightly sensitive to chemical and environmental effects that change the electronic structure of the atom, which in turn affects the presence of '''1s''' and '''2s''' electrons that participate in the decay process. A small number of mostly light nuclides are affected. For example, [[chemical bonds]] can affect the rate of electron capture to a small degree (in general, less than 1%) depending on the proximity of electrons to the nucleus. In <sup>7</sup>Be, a difference of 0.9% has been observed between half-lives in metallic and insulating environments.<ref name=\"WangYan2006\">{{cite journal |last1=Wang |first1=B. |display-authors=etal |title=Change of the 7Be electron capture half-life in metallic environments |journal=The European Physical Journal A |volume=28 |issue=3 |year=2006 |pages=375–377 |issn=1434-6001 |doi=10.1140/epja/i2006-10068-x |bibcode=2006EPJA...28..375W}}</ref> This relatively large effect is because beryllium is a small atom whose valence electrons are in '''2s''' [[atomic orbital]]s, which are subject to electron capture in <sup>7</sup>Be because (like all '''s''' atomic orbitals in all atoms) they naturally penetrate into the nucleus.\n\nIn 1992, Jung et al. of the Darmstadt Heavy-Ion Research group observed an accelerated β<sup>−</sup>&nbsp;decay of <sup>163</sup>Dy<sup>66+</sup>. Although neutral <sup>163</sup>Dy is a stable isotope, the fully ionized <sup>163</sup>Dy<sup>66+</sup> undergoes β<sup>−</sup>&nbsp;decay [[Bound-state beta decay|into the K and L shells]] to <sup>163</sup>Ho<sup>66+</sup> with a half-life of 47&nbsp;days.<ref name=\"JungBosch1992\">{{cite journal |last1=Jung |first1=M. |display-authors=etal |title=First observation of bound-state β<sup>−</sup> decay |journal=Physical Review Letters |volume=69 |issue=15 |year=1992 |pages=2164–2167 |issn=0031-9007 |doi=10.1103/PhysRevLett.69.2164 |pmid=10046415 |bibcode=1992PhRvL..69.2164J}}</ref>\n\n[[Rhenium-187]] is another spectacular example. <sup>187</sup>Re normally [[beta decay]]s to <sup>187</sup>Os with a [[half-life]] of 41.6 × 10<sup>9</sup>&nbsp;years,<ref>{{cite journal |doi=10.1126/science.271.5252.1099 |last1=Smoliar |first1=M.I. |last2=Walker |first2=R.J. |last3=Morgan |first3=J.W. |year=1996 |title=Re-Os ages of group IIA, IIIA, IVA, and IVB iron meteorites |journal=Science |volume=271 |pages=1099–1102 |bibcode=1996Sci...271.1099S |issue=5252}}</ref> but studies using fully ionised <sup>187</sup>[[rhenium|Re]] atoms (bare nuclei) have found that this can decrease to only 33&nbsp;years. This is attributed to \"[[Beta decay#Bound-state β- decay|bound-state β<sup>−</sup> decay]]\" of the fully ionised atom – the electron is emitted into the \"K-shell\" ('''1s''' atomic orbital), which cannot occur for neutral atoms in which all low-lying bound states are occupied.<ref name=\"Bosch1996\">{{cite journal |doi=10.1103/PhysRevLett.77.5190 |title=Observation of bound-state β– decay of fully ionized <sup>187</sup>Re:<sup>187</sup>Re-<sup>187</sup>Os Cosmochronometry |year=1996 |journal=Physical Review Letters |volume=77 |issue=26 |pages=5190–5193 |pmid=10062738 |first1=F. |last1=Bosch |display-authors=etal |bibcode=1996PhRvL..77.5190B}}</ref>\n\n[[File:DecayRate vs Solar Time.png|thumb|Example of diurnal and seasonal variations in gamma ray detector response.]]\nA number of experiments have found that decay rates of other modes of artificial and naturally occurring radioisotopes are, to a high degree of precision, unaffected by external conditions such as temperature, pressure, the chemical environment, and electric, magnetic, or gravitational fields.<ref>{{cite journal |last1=Emery |first1=G.T. |year=1972 |title=Perturbation of Nuclear Decay Rates |journal=Annual Review of Nuclear Science |volume=22 |pages=165–202 |doi=10.1146/annurev.ns.22.120172.001121 |bibcode=1972ARNPS..22..165E}}</ref> Comparison of laboratory experiments over the last century, studies of the Oklo [[Natural nuclear fission reactor|natural nuclear reactor]] (which exemplified the effects of thermal neutrons on nuclear decay), and astrophysical observations of the luminosity decays of distant supernovae (which occurred far away so the light has taken a great deal of time to reach us), for example, strongly indicate that unperturbed decay rates have been constant (at least to within the limitations of small experimental errors) as a function of time as well.{{citation needed|reason=Entire paragraph needs sourcing|date=October 2014}}\n\nRecent results suggest the possibility that decay rates might have a weak dependence on environmental factors. It has been suggested that measurements of decay rates of [[silicon-32]], [[manganese-54]], and [[radium-226]] exhibit small seasonal variations (of the order of 0.1%).<ref>{{cite web |title=The mystery of varying nuclear decay |work=Physics World |date=2 October 2008 |url=http://physicsworld.com/cws/article/news/36108}}</ref><ref>{{cite journal |title=Perturbation of Nuclear Decay Rates During the Solar Flare of 13 December 2006 |journal=Astroparticle Physics |volume=31 |issue=6 |year=2009 |pages=407–411 |arxiv=0808.3156 |bibcode=2009APh....31..407J |doi=10.1016/j.astropartphys.2009.04.005 |last1=Jenkins |first1=Jere H. |last2=Fischbach |first2=Ephraim}}</ref><ref>{{cite journal |last1=Jenkins |first1=J.H. |last2=Fischbach |first2=Ephraim |last3=Buncher |first3=John B. |last4=Gruenwald |first4=John T. |last5=Krause |first5=Dennis E. |last6=Mattes |first6=Joshua J. |title=Evidence of correlations between nuclear decay rates and Earth–Sun distance |journal=Astroparticle Physics |volume=32 |issue=1 |pages=42–46 |year=2009 |arxiv=0808.3283 |bibcode=2009APh....32...42J |doi=10.1016/j.astropartphys.2009.05.004}}</ref>  However, such measurements are highly susceptible to systematic errors, and a subsequent paper<ref>{{cite journal|last1=Norman|first1=E.B.|last2=Browne|first2=Edgardo|last3=Shugart|first3=Howard A.|last4=Joshi|first4=Tenzing H.|last5=Firestone|first5=Richard B.|year=2009|title=Evidence against correlations between nuclear decay rates and Earth–Sun distance|url=http://donuts.berkeley.edu/papers/EarthSun.pdf|journal=Astroparticle Physics|volume=31|issue=2|pages=135–137|arxiv=0810.3265|bibcode=2009APh....31..135N|doi=10.1016/j.astropartphys.2008.12.004|access-date=23 September 2009|archive-url=https://web.archive.org/web/20100629033415/http://donuts.berkeley.edu/papers/EarthSun.pdf|archive-date=29 June 2010|dead-url=yes}}</ref> has found no evidence for such correlations in seven other isotopes (<sup>22</sup>Na, <sup>44</sup>Ti, <sup>108</sup>Ag, <sup>121</sup>Sn, <sup>133</sup>Ba, <sup>241</sup>Am, <sup>238</sup>Pu), and sets upper limits on the size of any such effects. The decay of [[radon-222]] was once reported to exhibit large 4% peak-to-peak seasonal variations (see plot),<ref name=\"SturrockSteinitz2012\">{{cite journal|last1=Sturrock|first1=P.A.|last2=Steinitz|first2=G.|last3=Fischbach|first3=E.|last4=Javorsek|first4=D.|last5=Jenkins|first5=J.H.|title=Analysis of gamma radiation from a radon source: Indications of a solar influence|journal=Astroparticle Physics|volume=36|issue=1|year=2012|pages=18–25|issn=0927-6505|doi=10.1016/j.astropartphys.2012.04.009 |arxiv=1205.0205|bibcode=2012APh....36...18S}}</ref>  which were proposed to be related to either [[solar flare]] activity or the distance from the Sun, but detailed analysis of the experiment's design flaws, along with comparisons to other, much more stringent and systematically controlled, experiments refute this claim.<ref>{{Cite journal|date=2018-01-01|title=On the claim of modulations in radon decay and their association with solar rotation|url=https://www.sciencedirect.com/science/article/pii/S0927650517302323|journal=Astroparticle Physics|language=en|volume=97|pages=38–45|doi=10.1016/j.astropartphys.2017.10.011|issn=0927-6505|bibcode=2018APh....97...38P|last1=Pommé|first1=S.|last2=Lutter|first2=G.|last3=Marouli|first3=M.|last4=Kossert|first4=K.|last5=Nähle|first5=O.}}</ref>\n\n===GSI anomaly===\n{{Main|GSI anomaly}}\nAn unexpected series of experimental results for the rate of decay of heavy [[Highly charged ion|highly charged]] [[radioactive]] [[ion]]s circulating in a [[storage ring]] has provoked theoretical activity in an effort to find a convincing explanation. The rates of [[Weak interaction|weak]] decay of two radioactive species with half lives of about 40 s and 200 s are found to have a significant [[oscillation|oscillatory]] [[modulation]], with a period of about 7 s.<ref name=\"KienleBosch2013\">{{cite journal |vauthors=Kienle P, Bosch F, Bühler P, Faestermanna T, ((Litvinov Yu.A.)), Winckler N, Sanjari MS, Shubina DB, Atanasov D, Geissel H, Ivanova V, Yan XL, Boutin D, Brandau C, Dillmann I, Dimopoulou C, Hess R, Hillebrand PM, Izumikawa T, Knöbel R, Kurcewicz J, Kuzminchuk N, Lestinsky M, Litvinov SA, Ma XW, Maier L, Mazzocco M, Mukha I, Nociforo C, Nolden F, Scheidenberger C, Spillmann U, Steck M, ((Stöhlker Th)), Sun BH, Suzaki F, Suzuki T, ((Torilov S.Yu)), Trassinelli M, Tu XL, Wang M, Weick H, Winters DF, Winters N, Woods PJ, Yamaguchi T, Zhang GL, Ohtsubo T|display-authors=6 |title=High-resolution measurement of the time-modulated orbital electron capture and of the β<sup>+</sup> decay of hydrogen-like <sup>142</sup>Pm<sub>60</sub><sup>+</sup> ions |journal=Physics Letters B |volume=726 |issue=4–5 |year=2013 |pages=638–645 |issn=0370-2693 |doi=10.1016/j.physletb.2013.09.033 |bibcode=2013PhLB..726..638K|arxiv=1309.7294 }}</ref>\nThe observed phenomenon is known as the [[GSI anomaly]], as the storage ring is a facility at the [[GSI Helmholtz Centre for Heavy Ion Research]] in [[Darmstadt]], [[Germany]].  As the decay process produces an [[electron neutrino]], some of the proposed explanations for the observed rate oscillation invoke neutrino properties. Initial ideas related to [[Neutrino#Flavor oscillations|flavour oscillation]] met with skepticism.<ref name=\"Giunti2009\">{{cite journal |last1=Giunti|first1=Carlo |title=The GSI Time Anomaly: Facts and Fiction |journal=Nuclear Physics B: Proceedings Supplements |volume=188 |year=2009 |pages=43–45 |issn=0920-5632 |doi=10.1016/j.nuclphysbps.2009.02.009 |arxiv=0812.1887 |bibcode=2009NuPhS.188...43G }}</ref>  A more recent proposal involves mass differences between neutrino mass [[eigenstates]].<ref name=\"Gal2016\">{{cite journal |last1=Gal|first1=Avraham |title=Neutrino Signals in Electron-Capture Storage-Ring Experiments |journal=Symmetry |volume=8 |issue=6 |year=2016 |pages=49 |issn=2073-8994 |doi=10.3390/sym8060049|arxiv=1407.1789 }}</ref>\n\n==Theoretical basis of decay phenomena==\n{{Original research section|date=October 2014}}\nThe [[neutron]]s and [[proton]]s that constitute nuclei, as well as other particles that approach close enough to them, are governed by several interactions. The [[Nuclear force|strong nuclear force]], not observed at the familiar [[macroscopic]] scale, is the most powerful force over subatomic distances. The [[Coulomb's law|electrostatic force]] is almost always significant, and, in the case of [[beta decay]], the [[Weak interaction|weak nuclear force]] is also involved.\n\nThe combined effects of these forces produces a number of different phenomena in which energy may be released by rearrangement of particles in the nucleus, or else the change of one type of particle into others. These rearrangements and transformations may be hindered energetically, so that they do not occur immediately. In certain cases, random [[quantum fluctuation|quantum vacuum fluctuations]] are theorized to promote relaxation to a lower energy state (the \"decay\") in a phenomenon known as [[quantum tunneling]]. Radioactive decay [[half-life]] of nuclides has been measured over timescales of 55 orders of magnitude, from 2.3 × 10<sup>−23</sup> seconds (for [[hydrogen-7]]) to 6.9 × 10<sup>31</sup> seconds (for [[tellurium-128]]).<ref>[http://amdc.in2p3.fr/nubase/Nubase2003.pdf NUBASE evaluation of nuclear and decay properties] {{webarchive|url=https://web.archive.org/web/20110720233206/http://amdc.in2p3.fr/nubase/Nubase2003.pdf |date=20 July 2011 }}</ref> The limits of these timescales are set by the sensitivity of instrumentation only, and there are no known natural limits to how brief{{need citation|date=March 2019}} or long a decay [[half-life]] for radioactive decay of a [[radionuclide]] may be.\n\nThe decay process, like all hindered energy transformations, may be analogized by a snowfield on a mountain. While [[friction]] between the ice crystals may be supporting the snow's weight, the system is inherently unstable with regard to a state of lower potential energy. A disturbance would thus facilitate the path to a state of greater [[entropy]]; the system will move towards the ground state, producing heat, and the total energy will be distributable over a larger number of [[quantum states]] thus resulting in an [[avalanche]]. The ''total'' energy does not change in this process, but, because of the [[second law of thermodynamics]], avalanches have only been observed in one direction and that is toward the \"[[ground state]]\" — the state with the largest number of ways in which the available energy could be distributed.\n\nSuch a collapse (a gamma-ray ''decay event'') requires a specific [[activation energy]]. For a snow avalanche, this energy comes as a disturbance from outside the system, although such disturbances can be arbitrarily small. In the case of an excited [[atomic nucleus]] decaying by gamma radiation in a [[spontaneous emission]] of electromagnetic radiation, the arbitrarily small disturbance comes from [[quantum fluctuation|quantum vacuum fluctuations]].<ref>[http://www.famaf.unc.edu.ar/~vmarconi/moderna1/emision_estimulada_AJP.pdf Discussion of the quantum underpinnings of spontaneous emission, as first postulated by Dirac in 1927]</ref>\n\nA radioactive nucleus (or any excited system in quantum mechanics) is unstable, and can, thus, ''spontaneously'' stabilize to a less-excited system. The resulting transformation alters the structure of the nucleus and results in the emission of either a photon or a high-velocity particle that has mass (such as an electron, [[alpha particle]], or other type).{{citation needed|reason=No sourcing Entire section is original research|date=October 2014}}\n\n==Occurrence and applications==\nAccording to the [[Big Bang theory]], stable isotopes of the lightest five elements ([[hydrogen|H]], [[helium|He]], and traces of [[lithium|Li]], [[beryllium|Be]], and [[boron|B]]) were produced very shortly after the emergence of the universe, in a process called [[Big Bang nucleosynthesis]]. These lightest stable nuclides (including [[deuterium]]) survive to today, but any radioactive isotopes of the light elements produced in the Big Bang (such as [[tritium]]) have long since decayed. Isotopes of elements heavier than boron were not produced at all in the Big Bang, and these first five elements do not have any long-lived radioisotopes. Thus, all radioactive nuclei are, therefore, relatively young with respect to the birth of the universe, having formed later in various other types of [[nucleosynthesis]] in [[star]]s (in particular, [[supernova]]e), and also during ongoing interactions between stable isotopes and energetic particles. For example, [[carbon-14]], a radioactive nuclide with a half-life of only 5,730 years, is constantly produced in Earth's upper atmosphere due to interactions between cosmic rays and nitrogen.\n\nNuclides that are produced by radioactive decay are called [[radiogenic nuclide]]s, whether they themselves are [[Stable isotope|stable]] or not. There exist stable radiogenic nuclides that were formed from short-lived [[extinct radionuclide]]s in the early solar system.<ref>{{cite book\n | first=Donald D. | last=Clayton | year=1983\n | title=Principles of Stellar Evolution and Nucleosynthesis\n | page= 75 | edition=2nd\n | publisher=University of Chicago Press\n | isbn=978-0-226-10953-4\n}}</ref><ref>{{cite web\n |author1=Bolt, B.A. |author2=Packard, R.E. |author3=Price, P.B. | year=2007\n | url=http://content.cdlib.org/xtf/view?docId=hb1r29n709&doc.view=content&chunk.id=div00061&toc.depth=1&brand=oac&anchor.id=0\n | title=John H. Reynolds, Physics: Berkeley\n | publisher=The University of California, Berkeley\n | accessdate=2007-10-01\n}}</ref> The extra presence of these stable radiogenic nuclides (such as Xe-129 from primordial I-129) against the background of primordial [[stable nuclide]]s can be inferred by various means.\n\nRadioactive decay has been put to use in the technique of [[radioisotopic labeling]], which is used to track the passage of a chemical substance through a complex system (such as a living [[organism]]). A sample of the substance is synthesized with a high concentration of unstable atoms. The presence of the substance in one or another part of the system is determined by detecting the locations of decay events.\n\nOn the premise that radioactive decay is truly [[random]] (rather than merely [[chaos theory|chaotic]]), it has been used in [[hardware random-number generator]]s. Because the process is not thought to vary significantly in mechanism over time, it is also a valuable tool in estimating the absolute ages of certain materials. For geological materials, the radioisotopes and some of their decay products become trapped when a rock solidifies, and can then later be used (subject to many well-known qualifications) to estimate the date of the solidification. These include checking the results of several simultaneous processes and their products against each other, within the same sample. In a similar fashion, and also subject to qualification, the rate of formation of carbon-14 in various eras, the date of formation of organic matter within a certain period related to the isotope's half-life may be estimated, because the carbon-14 becomes trapped when the organic matter grows and incorporates the new carbon-14 from the air. Thereafter, the amount of carbon-14 in organic matter decreases according to decay processes that may also be independently cross-checked by other means (such as checking the carbon-14 in individual tree rings, for example).\n\n===Szilard–Chalmers effect===\nThe ''Szilard–Chalmers effect'' is defined as the breaking of a chemical bond between an atom and the molecule that the atom is part of, as a result of a nuclear reaction of the atom. The effect can be used to separate isotopes by chemical means. The discovery of this effect is due to [[Leó Szilárd]] and Thomas A. Chalmers.<ref>{{cite journal |journal=Nature |volume=134 |issue=3386 |pages=462 |year=1934 |first=Leó |last=Szilard |first2=Thomas A. |last2=Chalmers |title=Chemical separation of the radioactive element from its bombarded isotope in the Fermi effect |doi=10.1038/134462b0 |bibcode=1934Natur.134..462S }}</ref>\n\n==Origins of radioactive nuclides==\n{{Main|Nucleosynthesis}}\nRadioactive [[primordial nuclide]]s found in the [[Earth]] are residues from ancient [[supernova nucleosynthesis|supernova]] explosions that occurred before the formation of the [[solar system]]. They are the fraction of radionuclides that survived from that time, through the formation of the primordial solar [[nebula]], through planet [[accretion (astrophysics)|accretion]], and up to the present time. The naturally occurring short-lived [[radiogenic]] [[radionuclide]]s found in today's [[rocks]], are the daughters of those radioactive [[primordial nuclide]]s. Another minor source of naturally occurring radioactive nuclides are [[cosmogenic nuclide]]s, that are formed by cosmic ray bombardment of material in the Earth's [[atmosphere]] or [[crust (geology)|crust]]. The decay of the radionuclides in rocks of the Earth's [[mantle (geology)|mantle]] and [[crust (geology)|crust]] contribute significantly to [[Earth's internal heat budget]].\n\n==Decay chains and multiple modes==\n{{See also|Valley of stability}}\nThe daughter nuclide of a decay event may also be unstable (radioactive). In this case, it too will decay, producing radiation. The resulting second daughter nuclide may also be radioactive. This can lead to a sequence of several decay events called a ''[[decay chain]]'' (see this article for specific details of important natural decay chains). Eventually, a stable nuclide is produced. Any decay daughters that are the result of an alpha decay will also result in helium atoms being created.\n\n[[File:Gammaspektrum Uranerz.jpg|thumb|upright=1.15|[[Gamma spectroscopy|Gamma-ray energy spectrum]] of uranium ore (inset). Gamma-rays are emitted by decaying [[nuclide]]s, and the gamma-ray energy can be used to characterize the decay (which nuclide is decaying to which). Here, using the gamma-ray spectrum, several nuclides that are typical of the decay chain of <sup>238</sup>U have been identified: <sup>226</sup>Ra, <sup>214</sup>Pb, <sup>214</sup>Bi.]]\n\nAn example is the natural decay chain of <sup>238</sup>U:\n* Uranium-238 decays, through alpha-emission, with a [[half-life]] of 4.5 billion years to [[thorium-234]]\n* which decays, through beta-emission, with a half-life of 24 days to [[protactinium-234]]\n* which decays, through beta-emission, with a half-life of 1.2 minutes to [[uranium-234]]\n* which decays, through alpha-emission, with a half-life of 240 thousand years to [[thorium-230]]\n* which decays, through alpha-emission, with a half-life of 77 thousand years to [[radium-226]]\n* which decays, through alpha-emission, with a half-life of 1.6 thousand years to [[radon-222]]\n* which decays, through alpha-emission, with a half-life of 3.8 days to [[polonium-218]]\n* which decays, through alpha-emission, with a half-life of 3.1 minutes to [[lead-214]]\n* which decays, through beta-emission, with a half-life of 27 minutes to [[bismuth-214]]\n* which decays, through beta-emission, with a half-life of 20 minutes to [[polonium-214]]\n* which decays, through alpha-emission, with a half-life of 160 microseconds to [[lead-210]]\n* which decays, through beta-emission, with a half-life of 22 years to [[bismuth-210]]\n* which decays, through beta-emission, with a half-life of 5 days to [[polonium-210]]\n* which decays, through alpha-emission, with a half-life of 140 days to [[lead-206]], which is a stable nuclide.\n\nSome radionuclides may have several different paths of decay. For example, approximately 36% of [[bismuth-212]] decays, through alpha-emission, to [[thallium-208]] while approximately 64% of [[bismuth-212]] decays, through beta-emission, to [[polonium-212]]. Both [[thallium-208]] and [[polonium-212]] are radioactive daughter products of [[bismuth-212]], and both decay directly to stable [[lead-208]].\n\n==Associated hazard warning signs==\n<gallery>\nfile:Radioactive.svg|The trefoil symbol used to indicate ionising radiation.<!-- The [[Unicode]] encoding of this symbol is U+2622 ({{unicode|☢}}) -->\nFile:Logo iso radiation.svg|2007 ISO radioactivity danger symbol intended for IAEA Category 1, 2 and 3 sources defined as dangerous sources capable of death or serious injury.<ref>[http://www.iaea.org/newscenter/news/2007/radiationsymbol.html IAEA news release Feb 2007]</ref>\nFile:Dangclass7.svg|The dangerous goods transport classification sign for radioactive materials\n</gallery>\n\n==See also==\n{{Portal|Nuclear technology|Physics}}\n{{commons category|Radioactive decay by mode}}\n{{colbegin|colwidth=}}\n* [[Actinides in the environment]]\n* [[Background radiation]]\n* [[Chernobyl disaster]]\n* [[Crimes involving radioactive substances]]\n* [[Decay chain]]\n* [[Decay correct]]\n* [[Fallout shelter]]\n* [[Geiger counter]]\n* [[Half-life]]\n* [[Induced radioactivity]]\n* [[Lists of nuclear disasters and radioactive incidents]]\n* [[Multiplicative calculus]]\n* [[National Council on Radiation Protection and Measurements]]\n* [[Nuclear engineering]]\n* [[Nuclear medicine]]\n* [[Nuclear pharmacy]]\n* [[Nuclear physics]]\n* [[Nuclear power]]\n* [[Particle decay]]\n* [[Poisson process]]\n* [[Radiation]]\n* [[Radiation therapy]]\n* [[Radioactive contamination]]\n* [[Radioactivity in biology]]\n* [[Radiometric dating]]\n* [[Radionuclide]] a.k.a. \"radio-isotope\"\n* [[Secular equilibrium]]\n* [[Transient equilibrium]]\n{{colend}}\n\n==Notes==\n{{reflist|group=note}}\n\n==References==\n\n===Inline===\n{{reflist}}\n\n===General===\n* [http://search.eb.com/eb/article-9110413 \"Radioactivity\"], Encyclopædia Britannica. 2006. [[Encyclopædia Britannica Online]]. December 18, 2006\n* Radio-activity by Ernest Rutherford Phd, [[Encyclopædia Britannica Eleventh Edition]]\n\n==External links==\n{{Wikibooks |Historical Geology|Radioactive decay}}\n{{Wiktionary|radioactivity}}\n* [https://web.archive.org/web/20130108122246/http://ie.lbl.gov/toi/abouttoi.htm The Lund/LBNL Nuclear Data Search] – Contains tabulated information on radioactive decay types and energies.\n* [http://www.radiochemistry.org/nomenclature/ Nomenclature of nuclear chemistry]\n* [http://www.iem-inc.com/prhlfr.html Specific activity and related topics].\n* [http://www-nds.iaea.org/livechart The Live Chart of Nuclides – IAEA]\n* [http://www.nndc.bnl.gov/chart/ Interactive Chart of Nuclides]\n* [http://www.radiationanswers.org/ Health Physics Society Public Education Website]\n* {{Cite NSRW|wstitle=Becquerel Rays}}\n* [http://alsos.wlu.edu/qsearch.aspx?browse=science/Radioactivity Annotated bibliography for radioactivity from the Alsos Digital Library for Nuclear Issues]\n* [http://chair.pa.msu.edu/applets/decay/a.htm Stochastic Java applet on the decay of radioactive atoms] by Wolfgang Bauer\n* [http://www.upscale.utoronto.ca/GeneralInterest/Harrison/Flash/Nuclear/Decay/NuclearDecay.html Stochastic Flash simulation on the decay of radioactive atoms] by David M. Harrison\n* \"Henri Becquerel: The Discovery of Radioactivity\", Becquerel's 1896 articles online and analyzed on ''[http://www.bibnum.education.fr/physique/radioactivite/sur-les-radiations-invisibles-emises-par-les-corps-phosphorescents BibNum]'' <small>[click 'à télécharger' for English version]</small>.\n* \"Radioactive change\", Rutherford & Soddy article (1903), online and analyzed on ''[http://www.bibnum.education.fr/physique/radioactivite/la-transformation-radioactive Bibnum]'' <small>[click 'à télécharger' for English version]</small>.\n\n{{Radiation|state=uncollapsed}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Radioactive Decay}}\n[[Category:Radioactivity| ]]\n[[Category:Exponentials]]\n[[Category:Poisson point processes]]"
    },
    {
      "title": "Rule of 72",
      "url": "https://en.wikipedia.org/wiki/Rule_of_72",
      "text": "{{distinguish|72-year rule}}\n\nIn [[finance]], the '''rule of 72''', the '''rule of 70'''<ref name=Meadows/> and the '''rule of 69.3''' are methods for estimating an [[investment]]'s doubling time. The rule number (e.g., 72) is divided by the interest percentage per period (usually years) to obtain the approximate number of periods required for doubling.  Although [[scientific calculator]]s and [[spreadsheet]] programs have functions to find the accurate doubling time, the rules are useful for [[mental calculation]]s and when only a basic [[calculator]] is available.<ref>{{Cite book|title=All the Math You'll Ever Need|author=Slavin, Steve|publisher=[[John Wiley & Sons]]|year=1989|isbn=0-471-50636-2|pages=153–154}}</ref>\n\nThese rules apply to [[exponential growth]] and are therefore used for [[compound interest]] as opposed to [[simple interest]] calculations. They can also be used for [[exponential decay|decay]] to obtain a halving time. The choice of number is mostly a matter of preference: 69 is more accurate for continuous compounding, while 72 works well in common interest situations and is more easily divisible.\nThere are a number of variations to the rules that improve accuracy. For periodic compounding, the ''exact'' doubling time for an interest rate of ''r'' percent per period is\n:<math>t = \\frac{\\ln(2)}{\\ln(1+r/100)}\\approx \\frac{72}{r}</math>,\nwhere ''t'' is the number of periods required. The formula above can be used for more than calculating the doubling time. If one wants to know the tripling time, for example, replace the constant 2 in the numerator with 3. As another example, if one wants to know the number of periods it takes for the initial value to rise by 50%, replace the constant 2 with 1.5.\n\n== Using the rule to estimate compounding periods ==\n\nTo estimate the number of periods required to double an original investment, divide the most convenient \"rule-quantity\" by the expected growth rate, expressed as a percentage.\n*For instance, if you were to invest $100 with compounding interest at a rate of 9% per annum, the rule of 72 gives 72/9 = 8 years required for the investment to be worth $200; an exact calculation gives [[Natural logarithm of 2|ln(2)]]/ln(1+0.09) = 8.0432 years.\n\nSimilarly, to determine the time it takes for the value of money to halve at a given rate, divide the rule quantity by that rate.\n\n*To determine the time for [[money]]'s [[Purchasing power|buying power]] to halve, financiers divide the rule-quantity by the [[inflation rate]]. Thus at 3.5% [[inflation]] using the '''rule of 70''', it should take approximately 70/3.5 = 20 years for the value of a unit of currency to halve<ref name=Meadows>[[Donella Meadows]], ''Thinking in Systems: A Primer'', [[Chelsea Green Publishing]], 2008, page 33 (box \"Hint on reinforcing feedback loops and doubling time\").</ref>.\n*To estimate the impact of additional fees on financial policies (e.g., [[mutual fund fees and expenses]], loading and expense charges on [[variable universal life insurance]] investment portfolios), divide 72 by the fee.  For example, if the Universal Life policy charges an annual 3% fee over and above the cost of the underlying investment fund, then the total account value will be cut to 1/2 in 72 / 3 = 24 years, and then to just 1/4 the value in 48 years, compared to holding exactly the same investment outside the policy.\n\n== Choice of rule ==\nThe value 72 is a convenient choice of numerator, since it has many small [[divisor]]s: 1, 2, 3, 4, 6, 8, 9, and 12. It provides a good approximation for annual compounding, and for compounding at typical rates (from 6% to 10%).  The approximations are less accurate at higher interest rates.\n\nFor continuous compounding, 69 gives accurate results for any rate. This is because [[natural logarithm|ln]](2) is about 69.3%; see derivation below. Since daily compounding is close enough to continuous compounding, for most purposes 69, 69.3 or 70 are better than 72 for daily compounding. For lower annual rates than those above, 69.3 would also be more accurate than 72.<ref>Kalid Azad [http://betterexplained.com/articles/demystifying-the-natural-logarithm-ln/ Demystifying the Natural Logarithm (ln)] from BetterExplained</ref>\n\n{{wide image|doubling_time_vs_half_life.svg|640px|Graphs comparing doubling times and half lives of exponential growths (bold lines) and decay (faint lines), and their 70/''t'' and 72/''t'' approximations. In the [http://upload.wikimedia.org/wikipedia/commons/8/88/Doubling_time_vs_half_life.svg SVG version], hover over a graph to highlight it and its complement.}}\n\n{| class=\"wikitable sortable\"\n! Rate\n! Actual Years\n! Rate * Actual Years\n! Rule of 72\n! Rule of 70\n! Rule of 69.3\n! 72 adjusted\n! E-M rule\n|-\n| 0.25% || 277.605 || 69.401 || 288.000 || 280.000 || 277.200 || 277.667 || 277.547\n|-\n| 0.5% || 138.976 || 69.488 || 144.000 || 140.000 || 138.600 || 139.000 || 138.947\n|-\n| 1% || 69.661 || 69.661 || 72.000 || 70.000 || 69.300 || 69.667 || 69.648\n|-\n| 2% || 35.003 || 70.006 || 36.000 || 35.000 || 34.650 || 35.000 || 35.000\n|-\n| 3% || 23.450 || 70.349 || 24.000 || 23.333 || 23.100 || 23.444 || 23.452\n|-\n| 4% || 17.673 || 70.692 || 18.000 || 17.500 || 17.325 || 17.667 || 17.679\n|-\n| 5% || 14.207 || 71.033 || 14.400 || 14.000 || 13.860 || 14.200 || 14.215\n|-\n| 6% || 11.896 || 71.374 || 12.000 || 11.667 || 11.550 || 11.889 || 11.907\n|-\n| 7% || 10.245 || 71.713 || 10.286 || 10.000 || 9.900 || 10.238 || 10.259\n|-\n| 8% || 9.006 || 72.052 || 9.000 || 8.750 || 8.663 || 9.000 || 9.023\n|-\n| 9% || 8.043 || 72.389 || 8.000 || 7.778 || 7.700 || 8.037 || 8.062\n|-\n| 10% || 7.273 || 72.725 || 7.200 || 7.000 || 6.930 || 7.267 || 7.295\n|-\n| 11% || 6.642 || 73.061 || 6.545 || 6.364 || 6.300 || 6.636 || 6.667\n|-\n| 12% || 6.116 || 73.395 || 6.000 || 5.833 || 5.775 || 6.111 || 6.144\n|-\n| 15% || 4.959 || 74.392 || 4.800 || 4.667 || 4.620 || 4.956 || 4.995\n|-\n| 18% || 4.188 || 75.381 || 4.000 || 3.889 || 3.850 || 4.185 || 4.231\n|-\n| 20% || 3.802 || 76.036 || 3.600 || 3.500 || 3.465 || 3.800 || 3.850\n|-\n| 25% || 3.106 || 77.657 || 2.880 || 2.800 || 2.772 || 3.107 || 3.168\n|-\n| 30% || 2.642 || 79.258 || 2.400 || 2.333 || 2.310 || 2.644 || 2.718\n|-\n| 40% || 2.060 || 82.402 || 1.800 || 1.750 || 1.733 || 2.067 || 2.166\n|-\n| 50% || 1.710 || 85.476 || 1.440 || 1.400 || 1.386 || 1.720 || 1.848\n|-\n| 60% || 1.475 || 88.486 || 1.200 || 1.167 || 1.155 || 1.489 || 1.650\n|-\n| 70% || 1.306 || 91.439 || 1.029 || 1.000 || 0.990 || 1.324 || 1.523\n|}'''\n\n==History==\nAn early reference to the rule is in the ''[[Summa de arithmetica]]'' (Venice, 1494. Fol. 181, n. 44) of [[Luca Pacioli]] (1445–1514). He presents the rule in a discussion regarding the estimation of the doubling time of an investment, but does not derive or explain the rule, and it is thus assumed that the rule predates Pacioli by some time.\n{{cquote|'' A voler sapere ogni quantità a tanto per 100 l'anno, in quanti anni sarà tornata doppia tra utile e capitale, '''tieni per regola 72''', a mente, il quale sempre partirai per l'interesse, e quello che ne viene, in tanti anni sarà raddoppiato. Esempio: Quando l'interesse è a 6 per 100 l'anno, dico che si parta 72 per 6; ne vien 12, e in 12 anni sarà raddoppiato il capitale.'' (emphasis added).}}\nRoughly translated:\n{{cquote|In wanting to know of any capital, at a given yearly percentage, in how many years it will double adding the interest to the capital, '''keep as a rule [the number] 72''' in mind, which you will always divide by the interest, and what results, in that many years it will be doubled.  Example: When the interest is 6 percent per year, I say that one divides 72 by 6; 12 results, and in 12 years the capital will be doubled.}}\n\n==Adjustments for higher accuracy==\nFor higher rates, a bigger [[numerator]] would be better (e.g., for 20%, using 76 to get 3.8 years would be only about 0.002 off, where using 72 to get 3.6 would be about 0.2 off).  This is because, as above, the rule of 72 is only an approximation that is accurate for interest rates from 6% to 10%. For every three percentage points away from 8% the value 72 could be adjusted by 1.\n\n:<math> t \\approx \\frac{72 + (r - 8)/3}{r} </math>\n\nor for the same result, but simpler:\n\n:<math> t \\approx \\frac{70 + (r - 2)/3}{r} </math>\n:<math> t \\approx \\frac{69.3}{r} + 0.33</math>\n\n===E-M rule===\nThe Eckart–McHale second-order rule (the E-M rule) provides a multiplicative correction for the rule of 69.3 that is very accurate for rates from 0% to 20%. The rule of 69.3 is normally only accurate at the lowest end of interest rates, from 0% to about 5%. To compute the E-M approximation, multiply the rule of 69.3 result by 200/(200−''r'') as follows:\n\n:<math> t \\approx \\frac{69.3}{r} \\times \\frac{200}{200-r}</math>.\n\nFor example, if the interest rate is 18%, the rule of 69.3 says ''t'' = 3.85 years.  The E-M rule multiplies this by 200/(200−18), giving a doubling time of 4.23 years, where the actual doubling time at this rate is 4.19 years. (The E-M rule thus gives a closer approximation than the rule of 72.)\n\nTo obtain a similar correction for the rule of 70 or 72, one of the numerators can be set and the other adjusted to keep their product approximately the same. The E-M rule could thus be written also as\n\n:<math> t \\approx \\frac{70}{r} \\times \\frac{198}{200-r}</math> or <math> t \\approx \\frac{72}{r} \\times \\frac{192}{200-r}</math>\n\nIn these variants, the multiplicative correction becomes 1 respectively for r=2 and r=8, the values for which the rules of 70 and 72 are most precise.\n\nSimilarly, the third-order [[Padé approximant]] gives a more accurate answer over an even larger range of ''r'', but it has a slightly more complicated formula:\n\n:<math> t \\approx \\frac{69.3}{r} \\times \\frac{600+4r}{600+r}</math>.\n\n==Derivation==\n\n===Periodic compounding ===\nFor [[Compound interest#Periodic compounding|periodic compounding]], [[future value]] is given by:\n:<math>FV = PV \\cdot (1+r)^t</math>\nwhere <math>PV</math> is the [[present value]], <math>t</math> is the number of time periods, and <math>r</math> stands for the interest rate per time period.\n\nThe future value is double the present value when the following condition is met:\n:<math>(1+r)^t = 2\\,</math>\nThis equation is easily solved for <math>t</math>:\n:<math>\n  \\begin{array}{ccc}\n            \\ln((1+r)^t) & = & \\ln 2                  \\\\\n    t\\ln(1+r)   & = & \\ln 2                  \\\\\n    t                  & = & \\frac{\\ln 2}{\\ln(1+r)}\n  \\end{array}\n</math>\n\n\nA simple rearrangement shows:\n\n:<math>\n\\frac{\\ln{2}}{\\ln{(1+r)}}=\\bigg(\\frac{\\ln2}{r}\\bigg)\\bigg(\\frac{r}{ln(1+r)}\\bigg)\n</math>\n\n\nIf ''r'' is small, then ln(1 + ''r'') [[Natural logarithm#Derivative.2C Taylor series|approximately equals ''r'']] (this is the first term in the [[Taylor series#Calculation of Taylor series|Taylor series]]). That is, the latter term grows slowly when <math>r</math> is close to zero. \n\nCalling this latter term <math>f(r)</math>, the function <math>f(r)</math> is shown to be accurate in the approximation of <math>t</math> for a small, positive interest rate when <math>r=.08</math> (see derivation below). <math>f(.08)\\approx1.03949</math>, and we therefore approximate time <math>t</math> as:\n:<math>\nt=\\bigg(\\frac{\\ln2}{r}\\bigg)f(.08) \\approx \\frac{.72}{r}\n</math>\n\nWritten as a percentage:\n:<math>\n\\frac{.72}{r}=\\frac{72}{100r}\n</math>\n\n\n\nThis approximation increases in accuracy as the [[Compound Interest|compounding of interest]] becomes continuous (see derivation below).  <math>100 r</math> is <math>r</math> written as a [[percentage]].\n\nIn order to derive the more precise adjustments presented above, it is noted that <math>\\ln(1+r)\\,</math> is more closely approximated by <math>r - \\frac{r^2}{2}</math> (using the second term in the [[Taylor series]]).  <math>\\frac{0.693}{r - r^2/2}</math> can then be further simplified by Taylor approximations:\n\n:<math>\n  \\begin{array}{ccc}\n    \\frac{0.693}{r - r^2/2} & = & \\frac{69.3}{R - R^2/200} \\\\  & & \\\\\n      &    =    & \\frac{69.3}{R}    \\frac{1}{1-R/200}      \\\\  & & \\\\\n      & \\approx & \\frac{69.3 (1+R/200)}{R}                 \\\\  & & \\\\\n      &    =    & \\frac{69.3}{R}+\\frac{69.3}{200}          \\\\  & & \\\\\n      &    =    & \\frac{69.3}{R}+0.34\\end{array}\n</math>\n\nReplacing the \"R\" in ''R/200'' on the third line with 7.79 gives 72 on the numerator.  This shows that the rule of 72 is most precise for periodically composed interests around 8%.\n\nAlternatively, the E-M rule is obtained if the second-order Taylor approximation is used directly.\n\n=== Continuous compounding ===\nFor [[Compound interest#Continuous compounding|continuous compounding]], the derivation is simpler and yields a more accurate rule:\n:<math>\n  \\begin{array}{ccc}\n        (e^r)^p &    =    &           2        \\\\\n        e^{rp}  &    =    &           2        \\\\\n    \\ln e^{rp}  &    =    &       \\ln 2        \\\\\n           rp   &    =    &       \\ln 2        \\\\\n            p   &    =    & \\frac{\\ln 2}{r}    \\\\\n                &         &                    \\\\\n            p   & \\approx & \\frac{0.693147}{r}\n  \\end{array}\n</math>\n\n==See also==\n\n* [[Exponential growth]]\n* [[Time value of money]]\n* [[Interest]]\n* [[Discounting|Discount]]\n* [[Volatility (finance)#Crude volatility estimation|Rule of 16]]\n* [[Rule of three (statistics)]]\n\n== References ==\n{{Reflist}}\n\n==External links==\n* [http://members.optusnet.com.au/exponentialist/The_Scales_Of_70.htm The Scales Of 70] –  extends the rule of 72 beyond fixed-rate growth to variable rate compound growth including positive and negative rates.\n\n{{Portal bar|Mathematics|Business and economics}}\n\n{{DEFAULTSORT:Rule Of 72}}\n[[Category:Debt]]\n[[Category:Exponentials]]\n[[Category:Interest]]\n[[Category:Rules of thumb]]\n[[Category:Mathematical finance]]\n[[Category:Mental calculation]]"
    },
    {
      "title": "Schanuel's conjecture",
      "url": "https://en.wikipedia.org/wiki/Schanuel%27s_conjecture",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Conjecture on the transcendence degree of field extensions to the rational numbers}}\n{{E (mathematical constant)}}\nIn [[mathematics]], specifically [[transcendental number theory]], '''Schanuel's conjecture''' is a conjecture made by  [[Stephen Schanuel]] in the 1960s concerning the [[transcendence degree]] of certain [[field extension]]s of the [[rational numbers]].\n\n==Statement==\n\nThe conjecture is as follows:\n:Given any {{math| ''n'' }} [[complex number]]s {{math|''z''<sub>1</sub>, ..., ''z''<sub>''n''</sub>}} that are [[linear independence|linearly independent]] over the [[rational number]]s {{math|&#x211A;}}, the [[field extension]] {{math|&#x211A;(''z''<sub>1</sub>, ..., ''z''<sub>''n''</sub>, ''e''<sup>''z''<sub>1</sub></sup>, ..., ''e''<sup>''z''<sub>''n''</sub></sup>}}) has [[transcendence degree]] at least {{math|''n''}} over {{math|&#x211A;}}.\n\nThe conjecture can be found in Lang (1966).<ref>{{Cite book |authorlink=Serge Lang |first=Serge |last=Lang |title=Introduction to Transcendental Numbers |location= |publisher=Addison–Wesley |year=1966 |pages=30–31 |isbn= }}</ref>\n\n==Consequences==\n\nThe conjecture, if proven, would generalize most known results in [[transcendental number theory]].  The special case where the numbers ''z''<sub>1</sub>,...,''z''<sub>''n''</sub> are all [[algebraic number|algebraic]] is the [[Lindemann–Weierstrass theorem]].  If, on the other hand, the numbers are chosen so as to make exp(''z''<sub>1</sub>),...,exp(''z''<sub>''n''</sub>) all algebraic then one would prove that linearly independent logarithms of algebraic numbers are algebraically independent, a strengthening of [[Baker's theorem]].\n\nThe [[Gelfond–Schneider theorem]] follows from this strengthened version of Baker's theorem, as does the currently unproven [[four exponentials conjecture]].\n\nSchanuel's conjecture, if proved, would also settle whether numbers such as ''e''&nbsp;+&nbsp;{{pi}} and ''e''<sup>''e''</sup> are algebraic or transcendental, and prove that ''e'' and {{pi}} are algebraically independent simply by setting ''z''<sub>1</sub>&nbsp;=&nbsp;1 and ''z''<sub>2</sub>&nbsp;=&nbsp;{{pi}}''i'', and using [[Euler's identity]].\n\nEuler's identity states that ''e''<sup>{{pi}}''i''</sup>&nbsp;+&nbsp;1&nbsp;=&nbsp;0.  If Schanuel's conjecture is true then this is, in some precise sense involving [[exponential ring]]s, the ''only'' relation between ''e'', {{pi}}, and ''i'' over the complex numbers.<ref>{{Cite journal |first=Giuseppina |last=Terzo |title=Some consequences of Schanuel's conjecture in exponential rings |journal=Communications in Algebra |volume=36 |issue=3 |pages=1171–1189 |doi=10.1080/00927870701410694 |year=2008 }}</ref>\n\nAlthough ostensibly a problem in number theory, the conjecture has implications in [[model theory]] as well.  [[Angus Macintyre]] and [[Alex Wilkie]], for example, proved that the theory of the real field with exponentiation, {{math|&#x211D;}}<sub>exp</sub>, is [[Decidability (logic)|decidable]] provided Schanuel's conjecture is true.<ref name=\"MW96\">{{Cite book |first=A. |last=Macintyre |lastauthoramp=yes |first2=A. J. |last2=Wilkie |chapter=On the decidability of the real exponential field |editor-first=Piergiorgio |editor-last=Odifreddi |title=Kreiseliana: About and Around Georg Kreisel |year=1996 |location=Wellesley |publisher=Peters |pages=441–467 |isbn=978-1-56881-061-4 }}</ref>  In fact they only needed the real version of the conjecture, defined below, to prove this result, which would be a positive solution to [[Tarski's exponential function problem]].\n\n==Related conjectures and results==\n\nThe '''converse Schanuel conjecture'''<ref>Scott W. Williams, [http://www.math.buffalo.edu/~sww/0papers/million.buck.problems.mi.pdf Million Bucks Problems]</ref> is the following statement: \n:Suppose ''F'' is a [[countable]] [[field (mathematics)|field]] with [[characteristic (algebra)|characteristic]] 0, and ''e'' : ''F'' &rarr; ''F'' is a [[group homomorphism|homomorphism]] from the additive group (''F'',+) to the multiplicative group (''F'',&middot;) whose [[kernel (algebra)|kernel]] is [[cyclic group|cyclic]]. Suppose further that for any ''n'' elements ''x''<sub>1</sub>,...,''x''<sub>''n''</sub> of ''F'' which are linearly independent over {{math|&#x211A;}}, the extension field {{math|&#x211A;}}(''x''<sub>1</sub>,...,''x''<sub>''n''</sub>,''e''(''x''<sub>1</sub>),...,''e''(''x''<sub>''n''</sub>)) has transcendence degree at least ''n'' over {{math|&#x211A;}}. Then there exists a field homomorphism ''h'' : ''F'' &rarr; {{math|&#x2102;}} such that ''h''(''e''(''x''))&nbsp;=&nbsp;exp(''h''(''x'')) for all ''x'' in ''F''.\n\nA version of Schanuel's conjecture for [[formal power series]], also by Schanuel, was proven by [[James Ax]] in 1971.<ref>{{Cite journal |first=James |last=Ax |title=On Schanuel's conjectures |journal=[[Annals of Mathematics]] |issue=2 |volume=93 |year=1971 |pages=252–268 |jstor=1970774|doi=10.2307/1970774 }}</ref> It states:\n:Given any ''n'' formal power series ''f''<sub>1</sub>,...,''f''<sub>''n''</sub> in ''t''{{math|&#x2102;}}<nowiki>[[</nowiki>''t''<nowiki>]]</nowiki> which are linearly independent over {{math|&#x211A;}}, then the field extension {{math|&#x2102;}}(''t'',''f''<sub>1</sub>,...,''f''<sub>''n''</sub>,exp(''f''<sub>1</sub>),...,exp(''f''<sub>''n''</sub>)) has transcendence degree at least ''n'' over {{math|&#x2102;}}(''t'').\n\nAs stated above, the decidability of {{math|&#x211D;}}<sub>exp</sub> follows from the real version of Schanuel's conjecture which is as follows:<ref name=\"KZ06\">{{Cite journal |first=Jonathan |last=Kirby |lastauthoramp=yes |first2=Boris |last2=Zilber |title=The uniform Schanuel conjecture over the real numbers |journal=Bull. London Math. Soc. |volume=38 |year=2006 |issue= 4|pages=568–570 |doi=10.1112/S0024609306018510|citeseerx=10.1.1.407.5667 }}</ref>\n:Suppose ''x''<sub>1</sub>,...,''x''<sub>''n''</sub> are [[real number]]s and the transcendence degree of the field {{math|&#x211A;}}(''x''<sub>1</sub>,...,''x''<sub>''n''</sub>, [[exponential function|exp]](''x''<sub>1</sub>),...,exp(''x''<sub>''n''</sub>)) is strictly less than ''n'', then there are integers ''m''<sub>1</sub>,...,''m''<sub>''n''</sub>, not all zero, such that ''m''<sub>1</sub>''x''<sub>1</sub>&nbsp;+...+&nbsp;''m''<sub>''n''</sub>''x''<sub>''n''</sub>&nbsp;=&nbsp;0.\nA related conjecture called the uniform real Schanuel's conjecture essentially says the same but puts a bound on the integers ''m''<sub>''i''</sub>.  The uniform real version of the conjecture is equivalent to the standard real version.<ref name=\"KZ06\"/>  Macintyre and Wilkie showed that a consequence of Schanuel's conjecture, which they dubbed the Weak Schanuel's conjecture, was equivalent to the decidability of {{math|&#x211D;}}<sub>exp</sub>.  This conjecture states that there is a computable upper bound on the norm of non-singular solutions to systems of [[exponential polynomial]]s; this is, non-obviously, a consequence of Schanuel's conjecture for the reals.<ref name=\"MW96\"/>\n\nIt is also known that Schanuel's conjecture would be a consequence of conjectural results in the theory of [[motive (algebraic geometry)|motives]]. There [[Grothendieck's period conjecture]] for an [[abelian variety]] ''A'' states that the transcendence degree of its [[period matrix]] is the same as the dimension of the associated [[Mumford–Tate group]], and what is known by work of [[Pierre Deligne]] is that the dimension is an upper bound for the transcendence degree. Bertolin has shown how a generalised period conjecture includes Schanuel's conjecture.<ref>{{Cite journal |first=Cristiana |last=Bertolin |title=Périodes de 1-motifs et transcendance |journal=Journal of Number Theory |volume=97 |issue=2 |year=2002 |pages=204–221 |doi=10.1016/S0022-314X(02)00002-1 }}</ref>\n\n==Zilber's pseudo-exponentiation==\n\nWhile a proof of Schanuel's conjecture with number theoretic tools seems a long way off,<ref>{{Cite book |first=Michel |last=Waldschmidt |title=Diophantine approximation on linear algebraic groups |location=Berlin |publisher=[[Springer Science+Business Media|Springer]] |year=2000 |isbn= }}</ref> connections with model theory have prompted a surge of research on the conjecture.\n\nIn 2004, [[Boris Zilber]] systematically constructed [[exponential field]]s ''K''<sub>exp</sub> that are algebraically closed and of characteristic zero, and such that one of these fields exists for each [[uncountable]] [[cardinality]].<ref>{{Cite journal |first=Boris |last=Zilber |title=Pseudo-exponentiation on algebraically closed fields of characteristic zero |journal=[[Annals of Pure and Applied Logic]] |volume=132 |year=2004 |issue=1 |pages=67–95 |doi=10.1016/j.apal.2004.07.001 }}</ref>  He axiomatised these fields and, using [[Hrushovski construction|Hrushovski's construction]] and techniques inspired by work of [[Saharon Shelah | Shelah]] on [[categoricity]] in [[infinitary logic]]s, proved that this theory of \"pseudo-exponentiation\" has a unique model in each uncountable cardinal.  Schanuel's conjecture is part of this axiomatisation, and so the natural conjecture that the unique model of cardinality continuum is actually isomorphic to the complex exponential field implies Schanuel's conjecture. In fact, Zilber showed that this conjecture holds if and only if both Schanuel's conjecture and another unproven condition on the complex exponentiation field, which Zilber calls exponential-algebraic closedness, hold.<ref>{{Cite journal |first=Boris |last=Zilber |title=Exponential sums equations and the Schanuel conjecture |journal=J. London Math. Soc. |issue=2 |volume=65 |year=2002 |pages=27–44 |doi= 10.1112/S0024610701002861}}</ref>\n\n==References==\n{{Reflist}}\n\n==External links==\n*{{MathWorld|urlname=SchanuelsConjecture|title=Schanuel's Conjecture}}\n\n[[Category:Conjectures]]\n[[Category:Exponentials]]\n[[Category:Number theory]]\n[[Category:Transcendental numbers]]"
    },
    {
      "title": "Six exponentials theorem",
      "url": "https://en.wikipedia.org/wiki/Six_exponentials_theorem",
      "text": "In [[mathematics]], specifically [[transcendental number theory]], the '''six exponentials theorem''' is a result that, given the right conditions on the exponents, guarantees the transcendence of at least one of a set of exponentials.\n\n==Statement==\n\nIf ''x''<sub>1</sub>, ''x''<sub>2</sub>,..., ''x''<sub>''d''</sub> are ''d'' [[complex numbers]] that are [[linearly independent]] over the [[rational numbers]], and ''y''<sub>1</sub>, ''y''<sub>2</sub>,...,''y''<sub>''l''</sub> are ''l'' complex numbers that are also linearly independent over the rational numbers, and if ''dl''&nbsp;&gt;&nbsp;''d''&nbsp;+&nbsp;''l'', then at least one of the following ''dl'' numbers is [[transcendental number|transcendental]]:\n:<math>\\exp(x_i y_j),\\quad (1\\leq i\\leq d, 1\\leq j\\leq l).</math>\n\nThe most interesting case is when ''d''&nbsp;=&nbsp;3 and ''l''&nbsp;=&nbsp;2, in which case there are six exponentials, hence the name of the result.  The theorem is weaker than the related but thus far unproved [[four exponentials conjecture]], whereby the strict inequality ''dl''&nbsp;>&nbsp;''d''&nbsp;+&nbsp;''l'' is replaced with ''dl''&nbsp;&ge;&nbsp;''d''&nbsp;+&nbsp;''l'', thus allowing ''d''&nbsp;=&nbsp;''l''&nbsp;=&nbsp;2.\n\nThe theorem can be stated in terms of logarithms by introducing the set ''L'' of logarithms of [[algebraic number]]s:\n:<math>\\mathcal{L}=\\{\\lambda\\in\\mathbb{C}\\,:\\,e^\\lambda\\in\\overline{\\mathbb{Q}}\\}.</math>\nThe theorem then says that if λ<sub>''ij''</sub> are elements of ''L'' for ''i''&nbsp;=&nbsp;1, 2 and ''j''&nbsp;=&nbsp;1, 2, 3, such that  λ<sub>11</sub>, λ<sub>12</sub>, and λ<sub>13</sub> are linearly independent over the rational numbers, and λ<sub>11</sub> and λ<sub>21</sub> are also linearly independent over the rational numbers, then the [[Matrix (mathematics)|matrix]]\n:<math>M=\\begin{pmatrix}\\lambda_{11}&\\lambda_{12}&\\lambda_{13} \\\\ \\lambda_{21}&\\lambda_{22}&\\lambda_{23}\\end{pmatrix}</math>\nhas [[Rank (linear algebra)|rank]] 2.\n\n==History==\n\nA special case of the result where ''x''<sub>1</sub>, ''x''<sub>2</sub>, and ''x''<sub>3</sub> are logarithms of positive integers, ''y''<sub>1</sub>&nbsp;=&nbsp;1, and ''y''<sub>2</sub> is real, was first mentioned in a paper by [[Leonidas Alaoglu]] and [[Paul Erdős]] from 1944 in which they try to prove that the ratio of consecutive [[colossally abundant number]]s is always [[Prime number|prime]]. They claimed that [[Carl Ludwig Siegel]] knew of a proof of this special case, but it is not recorded.<ref>Alaoglu and Erdős, (1944), p.455: \"Professor Siegel has communicated to us the result that ''q''<sup>&nbsp;''x''</sup>, ''r''<sup>&nbsp;''x''</sup> and ''s''<sup>&nbsp;''x''</sup> can not be simultaneously rational except if ''x'' is an integer.\"</ref>  Using the special case they manage to prove that the ratio of consecutive colossally abundant numbers is always either a prime or a [[semiprime]].\n\nThe theorem was first explicitly stated and proved in its complete form independently by [[Serge Lang]]<ref>Lang, (1966), chapter 2, section 1.</ref> and [[Kanakanahalli Ramachandra]]<ref>Ramachandra, (1967/68).</ref> in the 1960s.\n\n==Five exponentials theorem==\n\nA stronger, related result is the '''five exponentials theorem''',<ref>Waldschmidt, (1988), corollary 2.2.</ref> which is as follows.  Let ''x''<sub>1</sub>, ''x''<sub>2</sub> and ''y''<sub>1</sub>, ''y''<sub>2</sub> be two pairs of complex numbers, with each pair being linearly independent over the rational numbers, and let γ be a non-zero algebraic number.  Then at least one of the following five numbers is transcendental:\n:<math>e^{x_1 y_1}, e^{x_1 y_2}, e^{x_2 y_1}, e^{x_2 y_2}, e^{\\gamma x_2/x_1}.</math>\nThis theorem implies the six exponentials theorem and in turn is implied by the as yet unproven four exponentials conjecture, which says that in fact one of the first four numbers on this list must be transcendental.\n\n==Sharp six exponentials theorem==\n\nAnother related result that implies both the six exponentials theorem and the five exponentials theorem is the '''sharp six exponentials theorem'''.<ref>Waldschmidt, (2005), theorem 1.4.</ref>  This theorem is as follows.  Let ''x''<sub>1</sub>, ''x''<sub>2</sub>, and ''x''<sub>3</sub> be complex numbers that are linearly independent over the rational numbers, and let ''y''<sub>1</sub> and ''y''<sub>2</sub> be a pair of complex numbers that are linearly independent over the rational numbers, and suppose that β<sub>''ij''</sub> are six algebraic numbers for 1&nbsp;&le;&nbsp;''i''&nbsp;&le;&nbsp;3 and 1&nbsp;&le;&nbsp;''j''&nbsp;&le;&nbsp;2 such that the following six numbers are algebraic:\n:<math>e^{x_1 y_1-\\beta_{11}}, e^{x_1 y_2-\\beta_{12}}, e^{x_2 y_1-\\beta_{21}}, e^{x_2 y_2-\\beta_{22}}, e^{x_3 y_1-\\beta_{31}}, e^{x_3 y_2-\\beta_{32}}.</math>\nThen ''x''<sub>''i''</sub>&nbsp;''y''<sub>''j''</sub>&nbsp;=&nbsp;β<sub>''ij''</sub> for 1&nbsp;&le;&nbsp;''i''&nbsp;&le;&nbsp;3 and 1&nbsp;&le;&nbsp;''j''&nbsp;&le;&nbsp;2.  The six exponentials theorem then follows by setting β<sub>''ij''</sub>&nbsp;=&nbsp;0 for every ''i'' and ''j'', while the five exponentials theorem follows by setting ''x''<sub>3</sub>&nbsp;=&nbsp;γ/''x''<sub>1</sub> and using [[Baker's theorem]] to ensure that the ''x''<sub>''i''</sub> are linearly independent.\n\nThere is a sharp version of the five exponentials theorem as well, although it as yet unproven so is known as the '''sharp five exponentials conjecture'''.<ref>Waldschmidt, (2005), conjecture 1.5</ref>  This conjecture implies both the sharp six exponentials theorem and the five exponentials theorem, and is stated as follows.  Let ''x''<sub>1</sub>, ''x''<sub>2</sub> and ''y''<sub>1</sub>, ''y''<sub>2</sub> be two pairs of complex numbers, with each pair being linearly independent over the rational numbers, and let α, β<sub>11</sub>, β<sub>12</sub>, β<sub>21</sub>, β<sub>22</sub>, and γ be six algebraic numbers with γ&nbsp;≠&nbsp;0 such that the following five numbers are algebraic:\n:<math>e^{x_1 y_1-\\beta_{11}}, e^{x_1 y_2-\\beta_{12}}, e^{x_2 y_1-\\beta_{21}}, e^{x_2 y_2-\\beta_{22}}, e^{(\\gamma x_2/x_1)-\\alpha}.</math>\nThen ''x''<sub>''i''</sub>&nbsp;''y''<sub>''j''</sub>&nbsp;=&nbsp;β<sub>''ij''</sub> for 1&nbsp;&le;&nbsp;''i'', ''j''&nbsp;&le;&nbsp;2 and γ''x''<sub>2</sub>&nbsp;=&nbsp;α''x''<sub>1</sub>.\n\nA consequence of this conjecture that isn't currently known would be the transcendence of ''e''<sup>π²</sup>, by setting ''x''<sub>1</sub>&nbsp;=&nbsp;''y''<sub>1</sub>&nbsp;=&nbsp;β<sub>11</sub>&nbsp;=&nbsp;1, ''x''<sub>2</sub>&nbsp;=&nbsp;''y''<sub>2</sub>&nbsp;=&nbsp;''i''π, and all the other values in the statement to be zero.\n\n==Strong six exponentials theorem==\n\n[[File:N-Exponentials Conjecture.png|frame|alt=Logical implications between the various n-exponentials problems|The logical implications between the various problems in this circle.  Those in red are as yet unproven while those in blue are known results.  The top most result refers to that discussed at [[Baker's theorem#Extensions|Baker's theorem]], while the four exponentials conjectures are detailed at the [[four exponentials conjecture]] article.]]\nA further strengthening of the theorems and conjectures in this area are the strong versions.  The '''strong six exponentials theorem''' is a result proved by Damien Roy that implies the sharp six exponentials theorem.<ref>Roy, (1992), section 4, corollary 2.</ref>  This result concerns the [[vector space]] over the algebraic numbers generated by 1 and all logarithms of algebraic numbers, denoted here as ''L''<sup>∗</sup>.  So ''L''<sup>∗</sup> is the set of all complex numbers of the form\n:<math>\\beta_0+\\sum_{i=1}^n \\beta_i\\log\\alpha_i,</math>\nfor some ''n''&nbsp;&ge;&nbsp;0, where all the β<sub>''i''</sub> and α<sub>''i''</sub> are algebraic and every [[Complex logarithm#Branches of the complex logarithm|branch of the logarithm]] is considered.  The strong six exponentials theorem then says that if ''x''<sub>1</sub>, ''x''<sub>2</sub>, and ''x''<sub>3</sub> are complex numbers that are linearly independent over the algebraic numbers, and if ''y''<sub>1</sub> and ''y''<sub>2</sub> are a pair of complex numbers that are also linearly independent over the algebraic numbers then at least one of the six numbers ''x''<sub>''i''</sub>&nbsp;''y''<sub>''j''</sub> for 1&nbsp;&le;&nbsp;''i''&nbsp;&le;&nbsp;3 and 1&nbsp;&le;&nbsp;''j''&nbsp;&le;&nbsp;2 is not in ''L''<sup>∗</sup>.  This is stronger than the standard six exponentials theorem which says that one of these six numbers is not simply the logarithm of an algebraic number.\n\nThere is also a '''strong five exponentials conjecture''' formulated by [[Michel Waldschmidt]]<ref>Waldschmidt, (1988).</ref> It would imply both, the strong six exponentials theorem and the sharp five exponentials conjecture.  This conjecture claims that if ''x''<sub>1</sub>, ''x''<sub>2</sub> and ''y''<sub>1</sub>, ''y''<sub>2</sub> are two pairs of complex numbers, with each pair being linearly independent over the algebraic numbers, then at least one of the following five numbers is not in ''L''<sup>∗</sup>:\n:<math>x_1y_1,\\,x_1y_2,\\,x_2y_1,\\,x_2y_2,\\,x_1/x_2.</math>\n\nAll the above conjectures and theorems are consequences of the unproven extension of [[Baker's theorem]], that logarithms of algebraic numbers that are linearly independent over the rational numbers are automatically algebraically independent too.  The diagram on the right shows the logical implications between all these results.\n\n==Generalization to commutative group varieties==\nThe exponential function {{mvar|e<sup>z</sup>}} uniformizes the exponential map of the multiplicative group {{math|'''G'''<sub>''m''</sub>}}. Therefore, we can reformulate the six exponential theorem more abstractly as follows:\n\n:Let {{math|''G'' {{=}} '''G'''<sub>''m''</sub> × '''G'''<sub>''m''</sub>}} and take {{math|''u'' : '''C''' → ''G''('''C''')}} to be a non-zero complex-analytic group homomorphism. Define {{mvar|L}} to be the set of complex numbers {{mvar|l}} for which {{math|''u''(''l'')}} is an algebraic point of {{mvar|G}}. If a minimal generating set of {{mvar|L}} over {{math|'''Q'''}} has more than two elements then the image {{math|''u''('''C''')}} is an algebraic subgroup of {{math|''G''('''C''')}}.\n\n(In order to derive the classical statement, set {{math|''u''(''z'') }}={{mvar|(e<sup> ''y''<sub>1</sub> z</sup>; e<sup> ''y''<sub>2</sub> z</sup>)}} and note that {{math|'''Q'''''x''<sub>1</sub> + '''Q'''''x''<sub>2</sub> + '''Q'''''x''<sub>3</sub>}} is a subset of {{math|''L''}}).\n\nIn this way, the statement of the six exponentials theorem can be generalized to an arbitrary commutative group variety {{mvar|G}} over the field of algebraic numbers. This '''generalized six exponential conjecture''', however, seems out of scope at the current state of [[transcendental number theory]].\n\nFor the special, but interesting cases {{math|''G'' {{=}} '''G'''<sub>''m''</sub> × ''E''}} and {{math|''G'' {{=}} ''E'' × ''E′''}}, where {{math|''E'', ''E′''}} are elliptic curves over the field of algebraic numbers, results towards the generalized six exponential conjecture were proven by Aleksander Momot.<ref>Momot, ch. 7</ref> These results involve the exponential function {{mvar|e<sup>z</sup>}} and a Weierstrass function <math>\\wp</math> resp. two Weierstrass functions <math>\\wp, \\wp'</math> with algebraic invariants <math>g_2, g_3, g_2', g_3'</math>, instead of the two exponential functions <math>e^{y_1z}, e^{y_2z}</math> in the classical statement.\n\nLet {{math|''G'' {{=}} '''G'''<sub>''m''</sub> × ''E''}} and suppose {{mvar|E}} is not isogenous to a curve over a real field and that {{math|''u''('''C''')}} is not an algebraic subgroup of {{math|''G''('''C''')}}. Then {{mvar|L}} is generated over {{math|'''Q'''}} either by two elements {{math|''x''<sub>1</sub>, ''x''<sub>2</sub>}}, or three elements {{math|''x''<sub>1</sub>, ''x''<sub>2</sub>, ''x''<sub>3</sub>}} which are not all contained in a real line {{math|'''R'''''c''}}, where {{mvar|c}} is a non-zero complex number. A similar result is shown for {{math|''G'' {{=}} ''E'' × ''E′''}}.<ref>Momot, ch. 7</ref>\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{cite journal | last = Alaoglu | first = Leonidas | author-link = Leonidas Alaoglu | last2 = Erdős | first2 = Paul | author2-link = Paul Erdős | title = On highly composite and similar numbers | journal = [[Transactions of the American Mathematical Society|Trans. Amer. Math. Soc.]] | volume = 56 | pages = 448&ndash;469 | year = 1944 | mr = 0011087 | doi=10.2307/1990319}}\n*{{cite book | last = Lang | first = Serge | authorlink = Serge Lang | title = Introduction to transcendental numbers | publisher = [[Addison-Wesley|Addison-Wesley Publishing Co.]] | year = 1966 | location = Reading, Mass. | mr = 0214547}}\n*{{cite arxiv | last = Momot | first = Aleksander | title = Density of rational points on commutative group varieties and small transcendence degree|arxiv=1011.3368 | year = 2011}}\n*{{cite journal | last = Ramachandra | first = Kanakanahalli | authorlink = Kanakanahalli Ramachandra | title = Contributions to the theory of transcendental numbers. I, II. | journal = [[Acta Arithmetica|Acta Arith.]] | volume = 14 | pages = 65&ndash;72, 73&ndash;88 | year = 1967{{ndash}}1968 | mr = 0224566}}\n*{{cite journal | last = Roy | first = Damien | title = Matrices whose coefficients are linear forms in logarithms | journal = [[Journal of Number Theory|J. Number Theory]] | volume = 41 | issue = 1 | pages = 22&ndash;47| year = 1992 | mr = 1161143 | doi=10.1016/0022-314x(92)90081-y}}\n*{{Cite book| last = Waldschmidt | first = Michel | editor1-first=Alan | editor1-last=Baker| chapter = On the transcendence methods of Gel'fond and Schneider in several variables | title = New advances in transcendence theory | pages = 375&ndash;398 | year = 1988 | publisher = [[Cambridge University Press]] | mr = 0972013 }}\n*{{cite conference | first = Michel | last = Waldschmidt | editor-last = Aoki | editor-first = Takashi | editor2-last = Kanemitsu | editor2-first = Shigeru | editor3-last = Nakahara | editor3-first = Mikio |display-editors = 3 | editor4-last = Ohno | editor4-first = Yasuo | title = Hopf algebras and transcendental numbers | booktitle = Zeta functions, topology, and quantum physics: Papers from the symposium held at Kinki University, Osaka, March 3–6, 2003 | pages = 197&ndash;219 | publisher = Springer | year = 2005 | series = Developments in mathematics | volume = 14 | mr = 2179279}}\n\n==External links==\n*{{MathWorld|urlname=SixExponentialsTheorem|title=Six exponentials theorem}}\n*{{planetmath reference|id=4348|title=Six exponentials theorem}}\n\n[[Category:Transcendental numbers]]\n[[Category:Exponentials]]\n[[Category:Theorems in number theory]]\n[[Category:Conjectures]]"
    },
    {
      "title": "Softmax function",
      "url": "https://en.wikipedia.org/wiki/Softmax_function",
      "text": "{{about|the smooth approximation of arg max|the smooth approximation of max|LogSumExp}}\nIn [[mathematics]], the '''softmax function,''' also known as '''softargmax'''{{sfn|Goodfellow|Bengio|Courville|2016|p=184}} or '''normalized exponential function''',<ref name=\"bishop\" />{{rp|198}} is a function that takes as input a vector of ''K'' real numbers, and normalizes it into a [[Probability distribution|probability distribution]] consisting of ''K'' probabilities. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the [[Interval (mathematics)|interval]] <math>(0, 1)</math>, and the components will add up to 1, so that they can be interpreted as probabilities. Furthermore, the larger input components will correspond to larger probabilities. Softmax is often used in [[Artificial neural network|neural networks]], to map the non-normalized output of a network to a probability distribution over predicted output classes.\n\nThe standard (unit) softmax function <math>\\sigma : \\R^K\\to\\R^K</math>is defined by the formula\n\n:<math>\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}} \\text{ for } i = 1, \\dotsc , K \\text{ and } \\mathbf z=(z_1,\\dotsc,z_K) \\in\\R^K</math>\n\nIn words: we apply the standard [[exponential function]] to each element <math>z_i</math> of the input vector <math>\\mathbf z</math> and normalize these values by dividing by the sum of all these exponentials; this normalization ensures that the sum of the components of the output vector <math>\\sigma(\\mathbf z)</math>is 1.\n\nInstead of {{mvar|''e''}}, a different [[Base (exponentiation)|base]] {{mvar|''b''}}>0 can be used; choosing a larger value of ''{{mvar|''b''}}'' will create a probability distribution that is more concentrated around the positions of the largest input values. Writing <math>b = e^\\beta</math> or <math>b = e^{-\\beta}</math>{{efn|Positive {{mvar|''β''}} corresponds to the maximum convention, and is usual in machine learning, corresponding to the highest score having highest probability. The negative {{math|−''β''}} corresponds to the minimum convention, and is conventional in thermodynamics, corresponding to the lowest energy state having the highest probability; this matches the convention in the [[Gibbs distribution]], interpreting {{mvar|''β''}} as [[coldness]].}} (for real {{mvar|''β''}}){{efn|1=The notation {{mvar|''β''}} is for the [[thermodynamic beta]], which is inverse [[temperature]]: <math>\\beta = 1/T</math>, <math>T = 1/\\beta.</math>}} yields the expressions:{{efn|For <math>\\beta = 0</math> ([[coldness]] zero, infinite temperature), <math>b = e^\\beta = e^0 = 1</math>, and this becomes the constant function {{tmath|(1/n, \\dots, 1/n)}}, corresponding to the [[discrete uniform distribution]].}}\n\n:<math>\\sigma(\\mathbf{z})_i = \\frac{e^{\\beta z_i}}{\\sum_{j=1}^K e^{\\beta z_j}} \\text{ or } \\sigma(\\mathbf{z})_i = \\frac{e^{-\\beta z_i}}{\\sum_{j=1}^K e^{-\\beta z_j}} \\text{ for } i = 1,\\dotsc , K </math>.\n\nIn some fields, the base is fixed, corresponding to a fixed scale,{{efn|In statistical mechanics, fixing {{mvar|''β''}} is interpreted as having coldness and temperature of 1.}} while in others the parameter {{mvar|''β''}} is varied.\n\n== Interpretations ==\n=== Smooth arg max ===\n{{see|Arg max}}\nThe name \"softmax\" is misleading; the function is not a [[smooth maximum]] (a [[smooth approximation]] to the [[maximum]] function), but is rather a smooth approximation to the [[arg max]] function: the function whose value is ''which'' index has the maximum. In fact, the term \"softmax\" is also used for the closely related [[LogSumExp]] function, which is a smooth maximum. For this reason, some prefer the more accurate term \"softargmax\", but the term \"softmax\" is conventional in machine learning.<ref name=\"sako2018\"/>{{sfn|Goodfellow|Bengio|Courville|2016|pp=183–184|ps=: The name “softmax” can be somewhat confusing. The function is more closely related to the arg max function than the max function. The term “soft” derives\nfrom the fact that the softmax function is continuous and differentiable. The arg max function, with its result represented as a one-hot vector, is not continuous or differentiable. The softmax function thus provides a “softened” version of the arg max. The corresponding soft version of the maximum function is <math>\\operatorname{softmax}(\\mathbf{z})^\\top \\mathbf{z}</math>. It would perhaps be better to call the softmax function “softargmax,” but the current name is an entrenched convention.}} For this section, the term \"softargmax\" is used to emphasize this interpretation.\n\nFormally, instead of considering the arg max as a function with categorical output <math>1, \\dots, n</math> (corresponding to the index), consider the arg max function with [[one-hot]] representation of the output (assuming there is a unique max arg):\n:<math>\\operatorname{arg\\,max}(z_1, \\dots, z_n) = (y_1, \\dots, y_n) = (0, \\dots, 0, 1, 0, \\dots, 0),</math>\nwhere the output coordinate <math>y_i = 1</math> if and only if <math>i</math> is the arg max of <math>(z_1, \\dots, z_n)</math>, meaning <math>z_i</math> is the unique maximum value of <math>(z_1, \\dots, z_n)</math>. For example, in this encoding <math>\\operatorname{arg\\,max}(1, 5, 10) = (0, 0, 1),</math> since the third argument is the maximum.\n\nThis can be generalized to multiple arg max values (multiple equal <math>z_i</math> being the maximum) by dividing the 1 between all max args; formally {{math|1/''k''}} where {{mvar|k}} is the number of arguments assuming the maximum. For example, <math>\\operatorname{arg\\,max}(1, 5, 5) = (0, 1/2, 1/2),</math> since the second and third argument are both the maximum. In case all arguments are equal, this is simply <math>\\operatorname{arg\\,max}(z, \\dots, z) = (1/n, \\dots, 1/n).</math> Points {{mvar|'''z'''}} with multiple arg max values are [[Singular point of an algebraic variety|singular points]] (or singularities, and form the singular set) – these are the points where arg max is discontinuous (with a [[jump discontinuity]]) – while points with a single arg max are known as non-singular or regular points.\n\nWith this representation, softargmax is now a smooth approximation of arg max: as {{tmath|\\beta \\to \\infty}}, softargmax converges to arg max. There are various notions of convergence of a function; softargmax converges to arg max [[pointwise convergence|pointwise]], meaning for each fixed input {{math|'''z'''}} as {{tmath|\\beta \\to \\infty}}, <math>\\sigma_\\beta(\\mathbf{z}) \\to \\operatorname{arg\\,max}(\\mathbf{z}).</math> However, softargmax does not [[uniform convergence|converge uniformly]] to arg max, meaning intuitively that different points converge at different rates, and may converge arbitrarily slowly. In fact, softargmax is continuous, but arg max is not continuous at the singular set where two coordinates are equal, while the uniform limit of continuous functions is continuous. The failure to converge uniformly is because for inputs where two coordinates are almost equal (and one is the maximum), the arg max is the index of one or the other, so a small change in input yields a large change in output. For example,\n<math>\\sigma_\\beta(1, 1.0001) \\to (0, 1),</math> but <math>\\sigma_\\beta(1, 0.9999) \\to (1, 0),</math> and <math>\\sigma_\\beta(1, 1) = 1/2</math> for all inputs: the closer the points are to the singular set <math>(x, x)</math>, the slower they converge. However, softargmax does [[compact convergence|converge compactly]] on the non-singular set.\n\nConversely, as {{tmath|\\beta \\to -\\infty}}, softargmax converges to arg min in the same way, where here the singular set is points with two arg ''min'' values. In the language of [[tropical analysis]], the softmax is a [[deformation theory|deformation]] or \"quantization\" of arg max and arg min, corresponding to using the [[log semiring]] instead of the [[max-plus semiring]] (respectively [[min-plus semiring]]), and recovering the arg max or arg min by taking the limit is called \"tropicalization\" or \"dequantization\".\n\nIt is also the case that, for any fixed {{mvar|''β''}}, if one input {{tmath|z_i}} is much larger than the others ''relative'' to the temperature, {{tmath|T = 1/β}}, the output is approximately the arg max. For example, a difference of 10 is large relative to a temperature of 1:\n:<math>\\sigma(0, 10) := \\sigma_1(0, 10) = \\left(1/(1 + e^{10}), e^{10}/(1 + e^{10})\\right) \\approx (0.00005, 0.99995)</math>\nHowever, if the difference is small relative to the temperature, the value is not close to the arg max. For example, a difference of 10 is small relative to a temperature of 100:\n:<math>\\sigma_{1/100}(0, 10) \\left(1/(1 + e^{1/10}), e^{1/10}/(1 + e^{1/10})\\right) \\approx (0.475, 0.525).</math>\nAs {{tmath|\\beta \\to \\infty}}, temperature goes to zero, <math>T = 1/\\beta \\to 0</math>, so eventually all differences become large (relative to a shrinking temperature), which gives another interpretation for the limit behavior.\n\n=== Probability theory ===\nIn [[probability theory]], the output of the softmax function can be used to represent a [[categorical distribution]] – that is, a [[probability distribution]] over {{mvar|K}} different possible outcomes.\n\n=== Statistical mechanics ===\nIn [[statistical mechanics]], the softmax function is known as the [[Boltzmann distribution]] (or [[Gibbs distribution]]):{{sfn|LeCun|Chopra|Hadsell|Ranzato|Huang|2006|p=7}} the index set <math>{1, \\dots, k}</math> are the [[Microstate (statistical mechanics)|microstates]] of the system; the inputs <math>z_i</math> are the energies of that state; the denominator is known as the [[partition function (statistical mechanics)|partition function]], often denoted by {{mvar|''Z''}}; and the factor {{mvar|''β''}} is called the [[coldness]] (or [[thermodynamic beta]], or [[inverse temperature]]).\n\n== Applications ==\nThe softmax function is used in various [[multiclass classification]] methods, such as [[multinomial logistic regression]] (also known as softmax regression)<ref name=\"bishop\">{{cite book |first=Christopher M. |last=Bishop |year=2006 |title=Pattern Recognition and Machine Learning |publisher=Springer}}</ref>{{rp|206–209}} [http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/], multiclass [[linear discriminant analysis]], [[naive Bayes classifier]]s, and [[artificial neural network]]s.<ref>ai-faq [http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-12.html What is a softmax activation function?]</ref> Specifically, in multinomial logistic regression and linear discriminant analysis, the input to the function is the result of {{mvar|K}} distinct [[linear function]]s, and the predicted probability for the {{mvar|j}}'th class given a sample vector {{math|'''x'''}} and a weighting vector {{math|'''w'''}} is:\n\n:<math>P(y=j\\mid \\mathbf{x}) = \\frac{e^{\\mathbf{x}^\\mathsf{T}\\mathbf{w}_j}}{\\sum_{k=1}^K e^{\\mathbf{x}^\\mathsf{T}\\mathbf{w}_k}}</math>\n\nThis can be seen as the [[function composition|composition]] of {{mvar|K}} linear functions <math>\\mathbf{x} \\mapsto \\mathbf{x}^\\mathsf{T}\\mathbf{w}_1, \\ldots, \\mathbf{x} \\mapsto \\mathbf{x}^\\mathsf{T}\\mathbf{w}_K</math> and the softmax function (where <math>\\mathbf{x}^\\mathsf{T}\\mathbf{w}</math> denotes the inner product of <math>\\mathbf{x}</math> and <math>\\mathbf{w}</math>). The operation is equivalent to applying a linear operator defined by <math>\\mathbf{w}</math> to vectors <math>\\mathbf{x}</math>, thus transforming the original, probably highly-dimensional, input to vectors in a {{mvar|K}}-dimensional space <math>\\mathbb{R}^K</math>.\n\n=== Neural networks ===\nThe softmax function is often used in the final layer of a neural network-based classifier. Such networks are commonly trained under a [[Cross entropy|log loss]] (or [[cross-entropy]]) regime, giving a non-linear variant of multinomial logistic regression.\n\nSince the function maps a vector and a specific index ''i'' to a real value, the derivative needs to take the index into account:\n\n:<math> \\frac{\\partial}{\\partial q_k}\\sigma(\\textbf{q}, i) = \\cdots =  \\sigma(\\textbf{q}, i)(\\delta_{ik} - \\sigma(\\textbf{q}, k))</math>\n\nHere, the [[Kronecker delta]] is used for simplicity (cf. the derivative of a [[sigmoid function]], being expressed via the function itself).\n\nSee [[Multinomial logit]] for a probability model which uses the softmax activation function.\n\n===Reinforcement learning===\nIn the field of [[reinforcement learning]], a softmax function can be used to convert values into action probabilities. The function commonly used is:<ref>Sutton, R. S. and Barto A. G. ''Reinforcement Learning: An Introduction''. The MIT Press, Cambridge, MA, 1998. [http://incompleteideas.net/book/ebook/node17.html Softmax Action Selection]</ref>\n:<math>\nP_t(a) = \\frac{\\exp(q_t(a)/\\tau)}{\\sum_{i=1}^n\\exp(q_t(i)/\\tau)} \\text{,}\n</math>\n\nwhere the action value <math>q_t(a)</math> corresponds to the expected reward of following action a and <math>\\tau</math> is called a temperature parameter (in allusion to [[statistical mechanics]]). For high temperatures (<math>\\tau\\to \\infty</math>), all actions have nearly the same probability and the lower the temperature, the more expected rewards affect the probability. For a low temperature (<math>\\tau\\to 0^+</math>), the probability of the action with the highest expected reward tends to 1.\n\n== Properties ==\nGeometrically the softmax function maps the vector space <math>\\mathbb{R}^K</math> to the [[Interior (topology)|interior]] of the [[standard simplex|standard <math>(K-1)</math>-simplex]], cutting the dimension by one (the range is a <math>(K-1)</math>-dimensional simplex in <math>K</math>-dimensional space), due to the [[linear constraint]] that all output sum to 1 meaning it lies on a [[hyperplane]].\n\nAlong the main diagonal <math>(x, x, \\dots, x),</math> softmax is just the uniform distribution on outputs, <math>(1/n, \\dots, 1/n)</math>: equal scores yield equal probabilities.\n\nMore generally, softmax is invariant under translation by the same value in each coordinate: adding <math>\\mathbf{c} = (c, \\dots, c)</math> to the inputs <math>\\mathbf{z}</math> yields <math>\\sigma(\\mathbf{z} + \\mathbf{c}) = \\sigma(\\mathbf{z})</math>, because it multiplies each exponent by the same factor, <math>e^c</math> (because <math>e^{z_i + c} = e^{z_i} \\cdot e^c</math>), so the ratios do not change:\n\n:<math>\\sigma(\\mathbf{z} + \\mathbf{c})_j = \\frac{e^{z_j + c}}{\\sum_{k=1}^K e^{z_k + c}} = \\frac{e^{z_j} \\cdot e^c}{\\sum_{k=1}^K e^{z_k} \\cdot e^c} = \\sigma(\\mathbf{z})_j.</math>\n\nGeometrically, softmax is constant along diagonals: this is the dimension that is eliminated, and corresponds to the softmax output being independent of a translation in the input scores (a choice of 0 score). One can normalize input scores by assuming that the sum is zero (subtract the average: <math>\\mathbf{c}</math> where <math display=\"inline\">c = \\frac{1}{n} \\sum z_i</math>), and then the softmax takes the hyperplane of points that sum to zero, <math display=\"inline\">\\sum z_i = 0</math>, to the open simplex of positive values that sum to 1<math display=\"inline\">\\sum \\sigma(\\mathbf{z})_i = 1</math>, analogously to how the exponent takes 0 to 1, <math>e^0 = 1</math> and is positive.\n\nBy contrast, softmax is not invariant under scaling. For instance, <math>\\sigma\\bigl((0, 1)\\bigr) = \\bigl(1/(1 + e), e/(1+e)\\bigr)</math> but <math>\\sigma\\bigl((0, 2)\\bigr) = \\bigl(1/(1 + e^2), e^2/(1+e^2)\\bigr).</math>\n\nThe [[standard logistic function]] is the special case for a 1-dimensional axis in 2-dimensional space, say the ''x''-axis in the {{math|(x, y)}} plane. One variable is fixed at 0 (say <math>z_2 = 0</math>), so <math>e^0 = 1</math>, and the other variable can vary, denote it <math>z_1 = x</math>, so <math display=\"inline\">e^{z_1}/\\sum_{k=1}^2 e^{z_k} = e^x/(e^x + 1),</math> the standard logistic function, and <math display=\"inline\">e^{z_2}/\\sum_{k=1}^2 e^{z_k} = 1/(e^x + 1),</math> its complement (meaning they add up to 1). The 1-dimensional input could alternatively be expressed as the line <math>(x/2, -x/2)</math>, with outputs <math>e^{x/2}/(e^{x/2} + e^{-x/2}) = e^x/(e^x + 1)</math> and <math>e^{-x/2}/(e^{x/2} + e^{-x/2}) = 1/(e^x + 1).</math>\n\nThe softmax function is also the gradient of the [[LogSumExp]] function, a [[smooth maximum]]; defining:\n:<math>\\mathrm{LSE}(z_1, \\dots, z_n) = \\log\\left( \\exp(z_1)+ \\cdots + \\exp(z_n) \\right),</math>\nthe partial derivatives are:\n:<math display=\"inline\">\\partial_i LSE(\\mathbf{x}) = \\exp x_i /\\bigl(\\sum_i \\exp x_i\\bigr).</math>\nExpressing the partial derivatives as a vector with the [[gradient]] yields the softmax.\n\n== History ==\nThe softmax function was used in [[statistical mechanics]] as the [[Boltzmann distribution]] in the foundational paper {{harvtxt|Boltzmann|1868}}, formalized and popularized in the influential textbook {{harvtxt|Gibbs|1902}}.\n\nThe use of the softmax in [[decision theory]] is credited to {{harvtxt|Luce|1959}},{{sfn|Gao|Pavel|2017|p=1}} who used the axiom of [[independence of irrelevant alternatives]] in [[rational choice theory]] to deduce the softmax in [[Luce's choice axiom]] for relative preferences.\n\nIn machine learning, the term \"softmax\" is credited to John S. Bridle in two 1989 conference papers, {{harvtxt|Bridle|1990a}}:{{sfn|Gao|Pavel|2017|p=1}} and {{harvtxt|Bridle|1990b}}:<ref name=\"sako2018\"/>\n{{quote\n|We are concerned with feed-forward non-linear networks (multi-layer perceptrons, or MLPs) with multiple outputs. We wish to treat the outputs of the network as probabilities of alternatives (''e.g.'' pattern classes), conditioned on the inputs. We look for appropriate output non-linearities and for appropriate criteria for adaptation of the parameters of the network (''e.g.'' weights). We explain two modifications: probability scoring, which is an alternative to squared error minimisation, and a normalised exponential ('''softmax''') multi-input generalisation of the logistic non-linearity.{{sfn|Bridle|1990a|p=227}}}}\n{{quote\n|For any input, the outputs must all be positive and they must sum to unity. ...\n\nGiven a set of unconstrained values, {{tmath|V_j(x)}}, we can ensure both conditions by using a Normalised Exponential transformation:\n:<math>Q_j(x) = e^{V_j(x)} / \\sum_k e^{V_k(x)}</math>\nThis transformation can be considered a multi-input generalisation of the logistic, operating on the whole output layer. It preserves the rank order of its input values, and is a differentiable generalisation of the ‘winner-take-all’ operation of picking the maximum value. For this reason we like to refer to it as '''softmax'''.{{sfn|Bridle|1990b|p=213}}}}\n\n== Example ==\n\nIf we take an input of [1, 2, 3, 4, 1, 2, 3], the softmax of that is [0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]. The output has most of its weight where the '4' was in the original input. This is what the function is normally used for: to highlight the largest values and suppress values which are significantly below the maximum value. But note: softmax is not scale invariant, so if the input were [0.1, 0.2, 0.3, 0.4, 0.1, 0.2, 0.3] (which sums to 1.6) the softmax would be [0.125, 0.138, 0.153, 0.169, 0.125, 0.138, 0.153]. This shows that for values between 0 and 1 softmax, in fact, de-emphasizes the maximum value (note that 0.169 is not only less than 0.475, it is also less than the initial proportion of 0.4/1.6=0.25).\n\nComputation of this example using simple Python code:\n<syntaxhighlight lang=\"pycon\">\n>>> import math\n>>> z = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]\n>>> z_exp = [math.exp(i) for i in z]\n>>> print([round(i, 2) for i in z_exp])\n[2.72, 7.39, 20.09, 54.6, 2.72, 7.39, 20.09]\n>>> sum_z_exp = sum(z_exp)\n>>> print(round(sum_z_exp, 2))\n114.98\n>>> softmax = [i / sum_z_exp for i in z_exp]\n>>> print([round(i, 3) for i in softmax])\n[0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]\n</syntaxhighlight>\n\nHere is an example of [[Julia (programming language)|Julia]] code:\n<syntaxhighlight lang=\"jlcon\">\njulia> A = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]\n7-element Array{Float64,1}:\n 1.0\n 2.0\n 3.0\n 4.0\n 1.0\n 2.0\n 3.0\n\njulia> exp.(A) ./ sum(exp.(A))\n7-element Array{Float64,1}:\n 0.0236405\n 0.0642617\n 0.174681\n 0.474833\n 0.0236405\n 0.0642617\n 0.174681\n</syntaxhighlight>\n\nHere is an example of R code:\n<syntaxhighlight lang=\"r\">\n> z <- c(1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0)\n> softmax <- exp(z)/sum(exp(z))\n> softmax\n[1] 0.02364054 0.06426166 0.17468130 0.47483300 0.02364054 0.06426166 0.17468130\n</syntaxhighlight>\n\n==See also==\n* [[Softplus]]\n* [[Multinomial logistic regression]]\n* [[Dirichlet distribution]] – an alternative way to sample categorical distributions\n* [[Partition function (statistical mechanics)|Partition function]]\n\n==Notes==\n{{notelist}}\n\n==References==\n{{reflist|refs=\n<ref name=\"sako2018\">{{cite web|first=Yusaku |last=Sako\n|title=Is the term “softmax” driving you nuts?\n|url=https://medium.com/@u39kun/is-the-term-softmax-driving-you-nuts-ee232ab4f6bd\n|date=2018-06-02\n|publisher=[[Medium (website)|Medium]]\n}}</ref>\n}}\n{{refbegin}}\n* {{cite journal |last=Boltzmann |first=Ludwig |authorlink=Ludwig Boltzmann\n|year=1868\n|title=Studien über das Gleichgewicht der lebendigen Kraft zwischen bewegten materiellen Punkten\n|trans-title=Studies on the balance of living force between moving material points\n|journal=Wiener Berichte |volume=58 |pages=517–560\n}}\n* {{cite book\n|first=Josiah Willard |last=Gibbs |authorlink=Josiah Willard Gibbs\n|year=1902\n|title=[[Elementary Principles in Statistical Mechanics]]\n}}\n* {{cite conference\n|first=John S. |last=Bridle\n|year=1990a\n|title=Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition\n|pages=227–236\n|conference=Neurocomputing: Algorithms, Architectures and Applications (1989)\n|editor1=Soulié F.F.\n|editor2=Hérault J. \n|series=NATO ASI Series (Series F: Computer and Systems Sciences)\n|volume=68\n|publisher=Springer\n|location=Berlin, Heidelberg\n|doi=10.1007/978-3-642-76153-9_28\n}}\n* {{cite conference\n|first=John S. |last=Bridle\n|year=1990b\n|title=Training Stochastic Model Recognition Algorithms as Networks can Lead to Maximum Mutual Information Estimation of Parameters\n|url=https://papers.nips.cc/paper/195-training-stochastic-model-recognition-algorithms-as-networks-can-lead-to-maximum-mutual-information-estimation-of-parameters\n|conference=[[Advances in Neural Information Processing Systems]] 2 (1989)\n|editor = D. S. Touretzky\n|publisher = Morgan-Kaufmann\n}}\n* {{cite book\n|first1=Yann |last1=LeCun\n|author-link1=Yann LeCun\n|first2=Sumit |last2=Chopra\n|first3=Raia |last3=Hadsell\n|first4=Marc’Aurelio |last4=Ranzato\n|first5=Fu Jie |last5=Huang\n|chapter=A Tutorial on Energy-Based Learning\n|chapter-url=http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf\n|isbn=978-0-26202617-8\n|publisher=MIT Press\n|year=2006\n|title=Predicting Structured Data\n|url=https://mitpress.mit.edu/books/predicting-structured-data\n|series=Neural Information Processing series\n|editors=Gökhan Bakır, Thomas Hofmann, Bernhard Schölkopf, Alexander J. Smola, Ben Taskar and S.V.N Vishwanathan\n}}\n* {{cite book |title=Deep Learning |year=2016\n|first1=Ian |last1=Goodfellow |authorlink1=Ian Goodfellow\n|first2=Yoshua |last2=Bengio |authorlink2=Yoshua Bengio\n|first3=Aaron |last3=Courville\n|publisher=MIT Press\n|url=http://www.deeplearningbook.org\n|isbn=978-0-26203561-3\n|chapter-url=https://www.deeplearningbook.org/contents/mlp.html\n|section=6.2.2.3 Softmax Units for Multinoulli Output Distributions\n|pages=180–184\n}}\n* {{cite arXiv|eprint=1704.00805|last1=Gao|first1=Bolin|title=On the Properties of the Softmax Function with Application in Game Theory and Reinforcement Learning|last2=Pavel|first2=Lacra|class=math.OC|year=2017}}\n{{refend}}\n\n[[Category:Computational neuroscience]]\n[[Category:Logistic regression]]\n[[Category:Artificial neural networks]]\n[[Category:Functions and mappings]]\n[[Category:Articles with example Python code]]\n[[Category:Exponentials]]"
    },
    {
      "title": "Logarithmic spiral",
      "url": "https://en.wikipedia.org/wiki/Logarithmic_spiral",
      "text": "{{Redirect|Spira mirabilis|the orchestra|Spira Mirabilis (orchestra)|for the Italian film|Spira Mirabilis (film)}}\n{|align=right\n|[[Image:Logarithmic Spiral Pylab.svg|260px|thumb|Logarithmic spiral (pitch 10°)]]\n|-\n|[[File:Nautilus Cutaway with Logarithmic Spiral.png|200px|thumb|Cutaway of a [[nautilus]] shell showing the chambers arranged in an approximately logarithmic spiral.  The plotted spiral (dashed blue curve) is based on growth rate parameter <math>b=0.1759</math>, resulting in a pitch of <math>\\arctan b \\approx 10^\\circ</math>.]]\n|-\n|[[File:Fractal Broccoli.jpg|200px|thumb|[[Romanesco broccoli]], which grows in a logarithmic spiral]]\n|-\n|[[Image:Mandel zoom 04 seehorse tail.jpg|thumb|200px|A section of the [[Mandelbrot set]] following a logarithmic spiral]]\n|-\n|[[Image:Low pressure system over Iceland.jpg|thumb|200px|An [[extratropical cyclone]] over [[Iceland]] shows an approximately logarithmic spiral pattern]]\n|-\n|[[File:Messier51 sRGB.jpg|thumb|200px|The arms of [[Spiral galaxy|spiral galaxies]] often have the shape of a logarithmic spiral, here the [[Whirlpool Galaxy]]]]\n|-\n|[[Image:Polygon spiral.svg|thumb|200px|Polygon spiral]]\n|}\n\nA '''logarithmic spiral''', '''equiangular spiral''' or '''growth spiral''' is a [[self-similarity|self-similar]] [[spiral]] [[curve]] which often appears in nature. The logarithmic spiral was first described by [[René Descartes|Descartes]] and later extensively investigated by [[Jacob Bernoulli]], who called it ''Spira mirabilis'', \"the marvelous spiral\".\n\n==Definition==\nIn [[polar coordinates]] <math>(r, \\theta)</math> the [[logarithm]]ic curve can be written as<ref>{{cite book | title = Divine Proportion: Φ Phi in Art, Nature, and Science | author = Priya Hemenway | isbn = 978-1-4027-3522-6 | publisher = Sterling Publishing Co | year = 2005}}</ref>\n\n:<math>r = ae^{b\\theta}</math>\n\nor\n\n:<math>\\theta = \\frac{1}{b} \\ln \\frac{r}{a},</math>\n\nwith [[e (mathematical constant)|<math>e</math>]] being the base of natural logarithms, and <math>a</math> and <math>b</math> being arbitrary positive real constants.\n\nIn parametric form, the curve is\n\n:<math>x(t) = r(t) \\cos t = ae^{bt} \\cos t,</math>\n:<math>y(t) = r(t) \\sin t = ae^{bt} \\sin t</math>\n\nwith [[real number]]s <math>a</math> and <math>b</math>.\n\nThe spiral has the property that the angle <math>\\phi</math> between the [[tangent]] and [[radial line]] at the point <math>(r, \\theta)</math> is constant.  This property can be expressed in [[differential geometry of curves|differential geometric terms]] as\n\n:<math>\\arccos \\frac{\\langle \\mathbf{r}(\\theta), \\mathbf{r}'(\\theta) \\rangle}{\\|\\mathbf{r}(\\theta)\\|\\|\\mathbf{r}'(\\theta)\\|} = \\arctan \\frac{1}{b} = \\phi.</math>\n\nThe [[derivative]] of <math>\\mathbf{r}(\\theta)</math> is proportional to the parameter <math>b</math>.  In other words, it controls how \"tightly\" and in which direction the spiral spirals.  In the extreme case that <math>b = 0</math> (<math>\\phi = \\tfrac{\\pi}{2}</math>) the spiral becomes a [[circle]] of radius <math>a</math>.  Conversely, in the [[Limit of a function|limit]] that <math>b</math> approaches [[Extended real number line|infinity]] (<math>\\phi \\to 0</math>) the spiral tends toward a straight half-line.  The [[Complementary angles|complement]] of <math>\\phi</math> is called the ''pitch''.\n\n==''Spira mirabilis'' and Jacob Bernoulli==\n'''''Spira mirabilis''''', [[Latin]] for \"miraculous spiral\", is another name for the logarithmic spiral. Although this curve had already been named by other mathematicians, the specific name (\"miraculous\" or \"marvelous\" spiral) was given to this curve by [[Jacob Bernoulli]], because he was fascinated by one of its unique mathematical properties: the size of the spiral increases but its shape is unaltered with each successive curve, a property known as [[self-similarity]]. Possibly as a result of this unique property, the spira mirabilis has evolved in nature, appearing in certain growing forms such as [[nautilus]] shells and [[sunflower]] heads.  Jacob Bernoulli wanted such a spiral engraved on his [[headstone]] along with the phrase \"[[Eadem mutata resurgo]]\" (\"Although changed, I shall arise the same.\"), but, by error, an [[Archimedean spiral]] was placed there instead.<ref name=\"livio\">{{cite book |last=Livio |first=Mario |year=2002 |title=The Golden Ratio: The Story of Phi, The World's Most Astonishing Number |publisher=Broadway Books |location=New York |isbn=978-0-7679-0815-3}}</ref><ref>Yates, R. C.: ''A Handbook on Curves and Their Properties'', J. W. Edwards (1952), \"Evolutes\". p. 206.</ref>\n\n==Properties==\nThe logarithmic spiral can be distinguished from the [[Archimedean spiral]] by the fact that the distances between the turnings of a logarithmic spiral increase in [[geometric progression]], while in an Archimedean spiral these distances are constant.\n\nLogarithmic spirals are self-similar in that the result of applying any [[similarity (geometry)|similarity transformation]] to the spiral is [[congruence (geometry)|congruent]] to the original untransformed spiral. Scaling by a factor <math>e^{2 \\pi b}</math>, where ''b'' is the parameter from the definition of the spiral, with the center of scaling at the origin, gives the same curve as the original; other scale factors give a curve that is rotated from the original position of the spiral. Logarithmic spirals are also congruent to their own [[involute]]s, [[evolute]]s, and the [[pedal curve]]s based on their centers.\n\nStarting at a point <math>P</math> and moving inward along the spiral, one can circle the origin an unbounded number of times without reaching it; yet, the total distance covered on this path is finite; that is, the [[limit (mathematics)|limit]] as <math>\\theta</math> goes toward <math>-\\infty</math> is finite. This property was first realized by [[Evangelista Torricelli]] even before [[calculus]] had been invented.<ref>\n{{cite book\n | title = The history of the calculus and its conceptual development\n | author = Carl Benjamin Boyer\n | publisher = Courier Dover Publications\n | year = 1949\n | isbn = 978-0-486-60509-8\n | page = 133\n | url = https://books.google.com/books?id=KLQSHUW8FnUC&pg=PA133\n }}</ref>\nThe total distance covered is <math>\\textstyle\\frac{r}{\\cos(\\phi)}</math>, where <math>r</math> is the straight-line distance from <math>P</math> to the origin.\n\nThe [[exponential function]] exactly maps all lines not parallel with the real or imaginary axis in the complex plane, to all logarithmic spirals in the complex plane with centre at 0. ([[Up to]] adding integer multiples of <math>2\\pi i</math> to the lines, the mapping of all lines to all logarithmic spirals is [[onto]].) The pitch angle of the logarithmic spiral is the angle between the line and the imaginary axis.\n\nThe function <math>x \\mapsto x^k</math>, where the constant <math>k</math> is a [[complex number]] with a non-zero [[imaginary unit]], maps the [[real line]] to a logarithmic spiral in the complex plane.\n\nThe [[golden spiral]] is a logarithmic spiral that grows outward by a factor of the [[golden ratio]] for every 90 degrees of rotation (pitch about 17.03239 degrees).  It can be approximated by a \"Fibonacci spiral\", made of a sequence of quarter circles with radii proportional to [[Fibonacci number]]s.\n\n==In nature==\n{{further|Patterns in nature#Spirals}}\n\nIn several natural phenomena one may find curves that are close to being logarithmic spirals. Here follow some examples and reasons:\n\n*The approach of a [[hawk]] to its prey in [[classical pursuit]], assuming the prey travels in a straight line. Their sharpest view is at an angle to their direction of flight; this angle is the same as the spiral's pitch.<ref>{{Citation |first=Gilbert J. |last=Chin |date=8 December 2000 |title=Organismal Biology: Flying Along a Logarithmic Spiral |journal=[[Science (journal)|Science]] |volume=290 |issue=5498 |page=1857 |doi=10.1126/science.290.5498.1857c}}</ref>\n*The approach of an insect to a light source. They are used to having the light source at a constant angle to their flight path. Usually the sun (or moon for nocturnal species) is the only light source and flying that way will result in a practically straight line.<ref>\n{{cite book\n | title = Discovering Moths: Nighttime Jewels in Your Own Backyard\n | author = John Himmelman\n | publisher = Down East Enterprise Inc\n | year = 2002\n | isbn = 978-0-89272-528-1\n | page = 63\n | url = https://books.google.com/books?id=iGn6ohfKhbAC&pg=PA63\n }}</ref>\n\n*The arms of spiral [[galaxy|galaxies]].<ref>\n{{cite book\n | title = Spiral structure in galaxies: a density wave theory\n | author = G. Bertin and C. C. Lin\n | publisher = MIT Press\n | year = 1996\n | isbn = 978-0-262-02396-2\n | page = 78\n | url = https://books.google.com/books?id=06yfwrdpTk4C&pg=PA78\n }}</ref> Our own galaxy, the [[Milky Way]], has several spiral arms, each of which is roughly a logarithmic spiral with pitch of about 12 degrees.<ref>\n{{cite book\n | title = The universal book of mathematics: from Abracadabra to Zeno's paradoxes\n | author = David J. Darling\n | publisher = John Wiley and Sons\n | year = 2004\n | isbn = 978-0-471-27047-8\n | page = 188\n | url = https://books.google.com/books?id=nnpChqstvg0C&pg=PA188\n }}</ref>\n\n*The nerves of the [[cornea]] (this is, corneal nerves of the subepithelial layer terminate near superficial epithelial layer of the cornea in a logarithmic spiral pattern).<ref name=\"Yu\">C. Q. Yu CQ and M. I. Rosenblatt, \"Transgenic corneal neurofluorescence in mice: a new model for in vivo investigation of nerve structure and regeneration,\"\nInvest Ophthalmol Vis Sci. 2007 Apr;48(4):1535-42.</ref>\n\n*The [[rainband|bands]] of [[tropical cyclone]]s, such as hurricanes.<ref>\n{{cite book\n | title = Treatise on physics, Volume 1\n | author = Andrew Gray\n | publisher = Churchill\n | year = 1901\n | pages = 356–357\n | url = https://books.google.com/books?id=ArELAAAAYAAJ&pg=PA357\n }}</ref>\n\n*Many [[Biology|biological]] structures including the shells of [[Mollusca|mollusk]]s.<ref>\n{{cite book\n | title = Spiral symmetry\n | chapter = The form, function, and synthesis of the molluscan shell\n | author = Michael Cortie\n | editor = István Hargittai and Clifford A. Pickover\n | publisher = World Scientific\n | year = 1992\n | isbn = 978-981-02-0615-4\n | page = 370\n | chapter-url = https://books.google.com/books?id=Ga8aoiIUx1gC&pg=PA370\n }}</ref>  In these cases, the reason may be construction from expanding similar shapes, as shown for [[polygon]]al figures in the accompanying graphic.\n\n*[[Logarithmic spiral beaches]] can form as the result of wave refraction and diffraction by the coast. [[Half Moon Bay (California)]] is an example of such a type of beach.<ref>\n{{cite book\n | title = Beach management: principles and practice\n | author = Allan Thomas Williams and Anton Micallef\n | publisher = Earthscan\n | year = 2009\n | isbn = 978-1-84407-435-8\n | page = 14\n | url = https://books.google.com/books?id=z_vKEMeJXKYC&pg=PA14\n }}</ref>\n\n==See also==\n*[[Archimedean spiral]]\n*[[Epispiral]]\n*[[Golden spiral]]\n*[[List of spirals]]\n\n==References==\n{{reflist}}\n* {{mathworld|urlname=LogarithmicSpiral|title=Logarithmic Spiral}}\n* Jim Wilson, [http://jwilson.coe.uga.edu/EMT668/EMAT6680.F99/Erbas/KURSATgeometrypro/related%20curves/related%20curves.html Equiangular Spiral (or Logarithmic Spiral) and Its Related Curves], University of Georgia (1999)\n* [[Alexander Bogomolny]], [http://www.cut-the-knot.org/Curriculum/Geometry/Mirabilis.shtml Spira Mirabilis - Wonderful Spiral], at cut-the-knot\n\n==External links==\n{{commons category|Logarithmic spirals}}\n* [http://jwilson.coe.uga.edu/EMT668/EMAT6680.F99/Erbas/KURSATgeometrypro/golden%20spiral/logspiral-history.html Spira mirabilis] history and math\n* {{APOD |date=25 September 2003 |title=Hurricane Isabel vs. the Whirlpool Galaxy}}\n* {{APOD |date=17 May 2008 |title=Typhoon Rammasun vs. the Pinwheel Galaxy}}\n* [http://SpiralZoom.com/ ''SpiralZoom.com''],  an educational website about the science of pattern formation, spirals in nature, and spirals in the mythic imagination.\n* [http://jsxgraph.uni-bayreuth.de/wiki/index.php/Logarithmic_spiral Online exploration using JSXGraph (JavaScript)]\n\n{{Patterns in nature}}\n{{Spirals}}\n\n[[Category:Spirals]]\n[[Category:Logarithms|Spiral]]\n[[Category:Exponentials|Spiral]]\n[[Category:Curves]]"
    },
    {
      "title": "Stretched exponential function",
      "url": "https://en.wikipedia.org/wiki/Stretched_exponential_function",
      "text": "[[Image:Pibmasterplot.png|325px|thumb|'''Figure 1'''. Illustration of a stretched exponential fit (with ''&beta;''=0.52) to an empirical master curve. For comparison, a least squares single and a [[Laplace distribution|double exponential]] fit are also shown. The data are rotational [[anisotropy]] of [[anthracene]] in [[polyisobutylene]] of several [[molecular mass]]es. The plots have been made to overlap by dividing time (''t'') by the respective characteristic [[time constant]].]]\n\nThe '''stretched exponential function'''\n:<math>f_\\beta (t) = e^{ -t^\\beta }</math>\nis obtained by inserting a fractional [[power law]] into the [[exponential function]].\nIn most applications, it is meaningful only for arguments ''t'' between 0 and +∞. With ''β''&nbsp;=&nbsp;1, the usual exponential function is recovered. With a ''stretching exponent'' ''β'' between 0 and 1, the graph of log&nbsp;''f'' versus ''t'' is characteristically ''stretched'', hence the name of the function. The '''compressed exponential function''' (with ''β''&nbsp;>&nbsp;1) has less practical importance, with the notable exception of ''β''&nbsp;=&nbsp;2, which gives the [[normal distribution]].\n\nIn mathematics, the stretched exponential is also known as the [[Cumulative distribution function#Complementary cumulative distribution function (tail distribution)|complementary cumulative]] [[Weibull distribution]]. The stretched exponential is also the [[characteristic function (probability theory)|characteristic function]], basically the [[Fourier transform]], of the [[stable distribution|Lévy symmetric alpha-stable distribution]].\n\nIn physics, the stretched exponential function is often used as a phenomenological description of [[Relaxation (physics)|relaxation]] in disordered systems. It was first introduced by [[Rudolf Kohlrausch]] in 1854 to describe the discharge of a capacitor;<ref>{{cite journal\n | author = Kohlrausch, R.\n | year = 1854\n | title = Theorie des elektrischen Rückstandes in der Leidner Flasche\n | journal = [[Annalen der Physik und Chemie]]\n | volume = 91\n | pages = 56–82, 179–213\n | url = http://gallica.bnf.fr/ark:/12148/bpt6k15176w.pagination}}.</ref>\ntherefore it is also called the '''Kohlrausch function'''. In 1970, G. Williams and D.C. Watts used the [[Fourier transform]] of the stretched exponential to describe [[dielectric spectroscopy|dielectric spectra]] of polymers;<ref>{{cite journal\n |author1=Williams, G.  |author2=Watts, D. C.\n  |lastauthoramp=yes | year = 1970\n | title = Non-Symmetrical Dielectric Relaxation Behavior Arising from a Simple Empirical Decay Function\n | journal = Transactions of the [[Faraday Society]]\n | volume = 66\n | pages = 80–85\n | doi = 10.1039/tf9706600080\n }}.</ref>\nin this context, the stretched exponential or its Fourier transform are also called the '''Kohlrausch-Williams-Watts (KWW) function'''.\n\nIn phenomenological applications, it is often not clear whether the stretched exponential function should apply to the differential or to the integral distribution function—or to neither. \nIn each case one gets the same asymptotic decay, but a different power law prefactor, which makes  fits more ambiguous than for simple exponentials. In a few cases<ref>{{cite journal\n |author1=Donsker, M. D.  |author2=Varadhan, S. R. S.\n  |lastauthoramp=yes | journal = Comm. Pure Appl. Math. \n | volume = 28\n | pages = 1–47\n | year = 1975\n | title = Asymptotic evaluation of certain Markov process expectations for large time\n | doi=10.1002/cpa.3160280102\n}}</ref><ref>{{cite journal\n | author = Takano, H. and Nakanishi, H. and Miyashita, S.\n | journal = Phys. Rev. B\n | volume = 37\n | pages = 3716–3719\n | year = 1988\n | title = Stretched exponential decay of the spin-correlation function in the kinetic Ising model below the critical temperature\n|bibcode = 1988PhRvB..37.3716T |doi = 10.1103/PhysRevB.37.3716 }}</ref>\n<ref>{{cite journal\n | author = Shore, John E. and Zwanzig, Robert\n | journal = The Journal of Chemical Physics\n | volume = 63\n | pages = 5445\n | year = 1975\n | title = Dielectric relaxation and dynamic susceptibility of a one-dimensional model for perpendicular-dipole polymers\n|doi = 10.1063/1.431279}}</ref>\n<ref>{{cite journal\n | author = Brey, J. J. and Prados, A.\n | journal = Physica A\n | volume = 197\n | pages = 569-582\n | year = 1993\n | title = Stretched exponential decay at intermediate times in the one-dimentional Ising model at low temperatures\n|doi = 10.1016/0378-4371(93)90015-V}}</ref>\nit can be shown that the asymptotic decay is a stretched exponential, but the prefactor is usually an unrelated power.\n\n== Mathematical properties ==\n\n=== Moments ===\n\nFollowing the usual physical interpretation, we interpret the function argument ''t'' as a time, and ''f''<sub>β</sub>(''t'') is the differential distribution. The area under the curve \nis therefore interpreted as a ''mean relaxation time''. One finds\n\n:<math>\\langle\\tau\\rangle \\equiv \\int_0^\\infty dt\\, e^{-(t/\\tau_K)^\\beta} = {\\tau_K  \\over \\beta } \\Gamma \\left({1 \\over \\beta }\\right)</math>\n\nwhere Γ is the [[gamma function]]. For exponential decay, 〈''τ''〉&nbsp;=&nbsp;''τ''<sub>''K''</sub> is recovered.\n\nThe higher [[moment (mathematics)|moments]] of the stretched exponential function are:\n<ref name=\"Zwillinger_2014\">{{cite book |author-first1=Izrail Solomonovich |author-last1=Gradshteyn |author-link1=Izrail Solomonovich Gradshteyn |author-first2=Iosif Moiseevich |author-last2=Ryzhik |author-link2=Iosif Moiseevich Ryzhik |author-first3=Yuri Veniaminovich |author-last3=Geronimus |author-link3=Yuri Veniaminovich Geronimus |author-first4=Michail Yulyevich |author-last4=Tseytlin |author-link4=Michail Yulyevich Tseytlin |author-first5=Alan |author-last5=Jeffrey |editor-first1=Daniel |editor-last1=Zwillinger |editor-first2=Victor Hugo |editor-last2=Moll |translator=Scripta Technica, Inc. |title=Table of Integrals, Series, and Products |publisher=[[Academic Press, Inc.]] |date=2015 |orig-year=October 2014 |edition=8 |language=English |isbn=978-0-12-384933-5 |lccn=2014010276 <!-- |url=http://books.google.com/books?id=NjnLAwAAQBAJ |access-date=2016-02-21-->|title-link=Gradshteyn and Ryzhik |chapter=3.478. |page=372}}</ref>\n\n: <math>\\langle\\tau^n\\rangle \\equiv \\int_0^\\infty dt\\, t^{n-1}\\, e^{-(t/\\tau_K)^\\beta} = {{\\tau_K}^n  \\over \\beta }\\Gamma \\left({n \\over \\beta }\\right).</math>\n\n===Distribution function===\n\nIn physics, attempts have been made to explain stretched exponential behaviour as a linear superposition of simple exponential decays. This requires a nontrivial distribution of relaxation times, ''ρ(u)'', which is implicitly defined by \n\n: <math>e^{-t^\\beta} = \\int_0^\\infty du\\,\\rho(u)\\, e^{-t/u}.</math>\n\nAlternatively, a distribution\n\n: <math>G=u \\rho (u)\\,</math>\n\nis used.\n\n''ρ'' can be computed from the series expansion:<ref>{{cite journal\n |author1=Lindsey, C. P.  |author2=Patterson, G. D.\n  |lastauthoramp=yes | year = 1980\n | title = Detailed comparison of the Williams-Watts and Cole-Davidson functions\n | journal = [[Journal of Chemical Physics]]\n | volume = 73\n | pages = 3348–3357\n | doi = 10.1063/1.440530|bibcode = 1980JChPh..73.3348L }}.\nFor a more recent and general discussion, see {{cite journal\n | author = Berberan-Santos, M.N., Bodunov, E.N. and Valeur, B.\n | year = 2005\n | title = Mathematical functions for the analysis of luminescence decays with underlying distributions 1. Kohlrausch decay function (stretched exponential)\n | journal = [[Chemical Physics]]\n | volume = 315\n | pages = 171–182\n | doi = 10.1016/j.chemphys.2005.04.006\n|bibcode = 2005CP....315..171B }}.</ref>\n: <math> \\rho (u ) =  -{ 1 \\over \\pi u} \\sum\\limits_{k = 0}^\\infty {(-1)^k  \\over k!}\\sin (\\pi \\beta k)\\Gamma (\\beta k + 1) u^{\\beta k}\n</math>\n\nFor rational values of ''β'', ''ρ''(''u'') can be calculated in terms of elementary functions. But the expression is in general too complex to be useful except for the case ''β''&nbsp;=&nbsp;1/2 where\n\n: <math>\nG(u) = u \\rho(u) = { 1 \\over 2\\sqrt{\\pi}} \\sqrt{u} \\exp(-u/4)\n</math>\n\nFigure 2 shows the same results plotted in both a [[linear]] and a [[Logarithm|log]] representation. The curves converge to a [[Dirac delta function]] peaked at ''u''&nbsp;=&nbsp;1 as ''β'' approaches 1, corresponding to the simple exponential function.\n{| class=\"wikitable\" style=\"margin: 1em auto 1em auto\"\n|\n{| \n|-\n| [[Image:KWW dist. function linear.png|300px]] || [[Image:KWW dist. funct. log.png|300px]]\n|}\n|-\n|'''Figure 2'''. Linear and log-log plots of the stretched exponential distribution function <math>G</math> vs <math>t/\\tau</math>\nfor values of the stretching parameter ''β'' between 0.1 and 0.9.\n|}\n\nThe moments of the original function can be expressed as\n: <math>\\langle\\tau^n\\rangle = \\Gamma(n) \\int_0^\\infty d\\tau\\, t^n \\, \\rho(\\tau).</math>\n\nThe first logarithmic moment of the distribution of simple-exponential relaxation times is\n: <math>\\langle\\ln\\tau\\rangle = \\left( 1 - {1 \\over \\beta} \\right) {\\rm Eu} + \\ln \\tau_K </math>\n\nwhere Eu is the [[Euler constant]].<ref>{{cite journal\n | doi = 10.1063/1.1446035\n | author = Zorn, R.\n | year = 2002\n | title = Logarithmic moments of relaxation time distributions\n | journal = [[Journal of Chemical Physics]]\n | volume = 116\n | pages = 3204–3209\n |bibcode = 2002JChPh.116.3204Z | url = http://juser.fz-juelich.de/record/1954/files/10418.pdf\n }}</ref>\n\n== Fourier transform ==\n\nTo describe results from spectroscopy or inelastic scattering, the sine or cosine Fourier transform of the stretched exponential is needed. It must be calculated either by numeric integration, or from a series expansion.<ref>Dishon et al. 1985.</ref> The series here as well as the one for the distribution function are special cases of the [[Fox-Wright function]].<ref>{{cite journal\n | author = Hilfer, J.\n | year = 2002\n | title = ''H''-function representations for stretched exponential relaxation and non-Debye susceptibilities in glassy systems\n | journal = [[Physical Review]] E\n | volume = 65\n | pages = 061510\n | doi=10.1103/physreve.65.061510\n  | bibcode = 2002PhRvE..65f1510H\n }}</ref> For practical purposes, the Fourier transform may be approximated by the [[Havriliak-Negami relaxation|Havriliak-Negami function]],<ref>{{cite journal\n | author = Alvarez, F., Alegría, A. and Colmenero, J.\n | year = 1991\n | title = Relationship between the time-domain Kohlrausch-Williams-Watts and frequency-domain Havriliak-Negami relaxation functions\n | journal = [[Physical Review]] B\n | volume = 44\n | pages = 7306–7312\n | doi = 10.1103/PhysRevB.44.7306\n |bibcode = 1991PhRvB..44.7306A }}</ref>\nthough nowadays the numeric computation can be done so efficiently<ref>{{cite journal\n| author = Wuttke, J.\n| year = 2012\n| title = Laplace–Fourier Transform of the Stretched Exponential Function: Analytic Error Bounds, Double Exponential Transform, and Open-Source Implementation \"libkww\"\n|journal = [[Algorithms (journal)|Algorithms]]\n| volume = 5\n| pages = 604–628\n| doi = 10.3390/a5040604| arxiv = 0911.4796\n}}</ref> that there is no longer any reason not to use the Kohlrausch-Williams-Watts function in the frequency domain.\n\n== History and further applications ==\n\nAs said in the introduction, the stretched exponential was introduced by the [[Germans|German]] [[physics|physicist]] [[Rudolf Kohlrausch]] in 1854 to describe the discharge of a capacitor ([[Leyden jar]]) that used glass as dielectric medium. The next documented usage is by [[Friedrich Kohlrausch (physicist)|Friedrich Kohlrausch]], son of Rudolf, to describe torsional relaxation. [[A. Werner]] used it in 1907 to describe complex luminescence decays; [[Theodor Förster]] in 1949 as the fluorescence decay law of electronic energy donors.\n\nOutside condensed matter physics, the stretched exponential has been used to describe the removal rates of small, stray bodies in the solar system,<ref>{{cite journal\n | author = Dobrovolskis, A., Alvarellos, J. and Lissauer, J.\n | year = 2007\n | title = Lifetimes of small bodies in planetocentric (or heliocentric) orbits\n | journal = [[Icarus (journal)|Icarus]]\n | volume = 188\n | pages = 481–505\n | doi = 10.1016/j.icarus.2006.11.024|bibcode = 2007Icar..188..481D }}</ref> the diffusion-weighted MRI signal in the brain,<ref>{{cite journal\n | author = Bennett, K.| year = 2003 \n | title = Characterization of Continuously Distributed Water Diffusion Rates in Cerebral Cortex with a Stretched Exponential Model\n | journal = Magn. Reson. Med.\n | volume = 50\n | pages = 727–734\n | doi = 10.1002/mrm.10581|display-authors=etal}}</ref> and the production from unconventional gas wells.<ref>{{Cite journal|last=Valko|first=Peter P.|last2=Lee|first2=W. John|date=2010-01-01|title=A Better Way To Forecast Production From Unconventional Gas Wells|url=https://doi.org/10.2118/134231-MS|journal=SPE Annual Technical Conference and Exhibition|language=english|publisher=Society of Petroleum Engineers|doi=10.2118/134231-ms|isbn=9781555633004}}</ref>\n\n=== In probability, ===\n\nIf the integrated distribution is a stretched exponential, the normalized [[Probability distribution|probability density function]] is given by\n\n:<math> p(\\tau \\mid \\lambda, \\beta)~d\\tau = \\frac{\\lambda}{\\Gamma(1 + \\beta^{-1})}~e^{-(\\tau \\lambda)^\\beta}~d\\tau</math>\n\nNote that confusingly some authors<ref>{{cite book\n | author = Sornette, D.\n | year = 2004\n | title = Critical Phenomena in Natural Science: Chaos, Fractals, Self-organization, and Disorder}}.</ref>\nhave been known to use the name \"stretched exponential\" to refer to the [[Weibull distribution]].\n\n=== Modified functions ===\n\nA modified stretched exponential function\n:<math>f_\\beta (t) = e^{ -t^{\\beta(t)} }</math>\nwith a slowly ''t''-dependent exponent ''&beta;'' has been used for biological survival curves.<ref>{{cite journal\n |author1=B. M. Weon  |author2=J. H. Je\n  |lastauthoramp=yes | year = 2009\n | title = Theoretical estimation of maximum human lifespan\n | journal = Biogerontology\n | volume = 10\n | pages = 65–71\n | doi = 10.1007/s10522-008-9156-4}}</ref>\n<ref>\n{{cite journal\n | author = B. M. Weon\n | year = 2016\n | title = Tyrannosaurs as long-lived species\n | journal = Scientific Reports\n | volume = 6\n | pages = 19554 \n | doi = 10.1038/srep19554\n | pmid=26790747\n | pmc=4726238| bibcode = 2016NatSR...619554W\n }}</ref>\n\n==References==\n<references/>\n\n==External links==\n* J. Wuttke: [http://apps.jcns.fz-juelich.de/kww libkww] C library to compute the Fourier transform of the stretched exponential function\n\n{{DEFAULTSORT:Stretched Exponential Function}}\n[[Category:Exponentials]]"
    },
    {
      "title": "Tetration",
      "url": "https://en.wikipedia.org/wiki/Tetration",
      "text": "{{distinguish|titration}}\n[[Image:TetrationComplexColor.png|thumb|border|268px|[[Domain coloring]] of the [[holomorphic function|holomorphic]] tetration <math>{}^{z}e</math>, with [[hue]] representing the function [[Argument (complex analysis)|argument]] and [[brightness]] representing magnitude]]\n[[Image:TetrationConvergence2D.svg|thumb|<math>{}^{n}x</math>, for {{math|1=''n'' = 2, 3, 4, …}}, showing convergence to the infinitely iterated exponential between the two dots]]\n\nIn [[mathematics]], '''tetration''' (or '''hyper-4''') is [[iterated]], or repeated, exponentiation. It is the next [[hyperoperation]] after [[exponentiation]], but before [[pentation]]. The word was coined by [[Reuben Louis Goodstein]] from [[tetra-]] (four) and [[iterated function|iteration]]. Tetration is used for the [[Large numbers#Standardized system of writing very large numbers|notation of very large numbers]]. The notation <math>{^{n}a}</math> means <math>{a^{a^{\\cdot^{\\cdot^{a}}}}}</math>, which is the application of exponentiation <math>n-1</math> times.\n\nThe first four [[hyperoperation]]s are shown here, with tetration being the fourth of these (in this case, the [[unary operation]] [[Successor function|succession]], <math>a' = a + 1</math>, is considered to be the zeroth operation).\n#[[Addition]]\n#:<math>a + n = a + \\underbrace{1 + 1 + \\cdots + 1}_n</math>\n#::''n'' copies of 1 added to ''a''.\n#[[Multiplication]]\n#:<math>a \\times n = \\underbrace{a + a + \\cdots + a}_n</math>\n#::''n'' copies of ''a'' combined by addition.\n#[[Exponentiation]]\n#:<math>a^n = \\underbrace{a \\times a \\times \\cdots \\times a}_n</math>\n#::''n'' copies of ''a'' combined by multiplication.\n#Tetration\n#:<math>{^{n}a} = \\underbrace{a^{a^{\\cdot^{\\cdot^{a}}}}}_n</math>\n#::''n'' copies of ''a'' combined by exponentiation, right-to-left.\n\nHere, succession {{math|(''a′'' {{=}} ''a'' + 1)}} is the most basic operation; addition ({{math|''a'' + ''n''}}) is a primary operation, though for natural numbers it can be thought of as a chained succession of {{mvar|n}} successors of {{mvar|a}}; multiplication ({{math|(''a'' × ''n''}}) is also a primary operation, though for natural numbers it can be thought of as a chained addition involving {{mvar|n}} numbers {{mvar|a}}. Exponentiation (<math>a^n</math>) can be thought of as a chained multiplication involving {{mvar|n}} numbers {{mvar|a}}, and analogously, tetration (<math>^{n}a</math>) can be thought of as a chained power involving {{mvar|n}} numbers {{mvar|a}}. Each of the operations above are defined by iterating the previous one; however, unlike the operations before it, tetration is not an [[elementary function]].\n\nThe parameter {{mvar|a}} may be called the base-parameter in the following, while the parameter {{mvar|n}} in the following may be called the ''height''-parameter (which is integral in the first approach but may be generalized to fractional, real and complex ''heights'', see below). Tetration is read as \"the {{mvar|n}}th tetration of {{mvar|a}}\".\n\n== Formal definition ==\nFor any positive [[real number|real]] <math> a > 0 </math> and non-negative [[integer]] <math> n \\ge 0 </math>, we can define <math>\\,\\! {^{n}a} </math> recursively as:\n:<math>{^{n}a} := \\begin{cases} 1 &\\text{if }n=0 \\\\ a^{\\left(^{(n-1)}a\\right)} &\\text{if }n>0 \\end{cases} </math>\nThis formal definition is equivalent to repeated exponentiation for [[Natural number|natural]] heights; however, this definition allows for extensions to other heights such as <math>^{0}a</math>, <math>^{-1}a</math>, and <math>^{i}a</math>.\n\n== Terminology ==\nThere are many terms for tetration, each of which has some logic behind it, but some have not become commonly used for one reason or another. Here is a comparison of each term with its rationale and counter-rationale.\n\n* The term ''tetration'', introduced by Goodstein in his 1947 paper ''Transfinite Ordinals in Recursive Number Theory''<ref>{{cite journal |author=R. L. Goodstein |title=Transfinite ordinals in recursive number theory |jstor=2266486 |journal=Journal of Symbolic Logic |volume=12 |year=1947 |issue=4 |doi=10.2307/2266486 |pages=123–129}}</ref> (generalizing the recursive base-representation used in [[Goodstein's theorem]] to use higher operations), has gained dominance. It was also popularized in [[Rudy Rucker]]'s ''[[Infinity and the Mind]]''.\n* The term ''superexponentiation'' was published by Bromer in his paper ''Superexponentiation'' in 1987.<ref>{{cite journal |author=N. Bromer |title=Superexponentiation |journal=Mathematics Magazine |volume=60 |issue=3 |year=1987 |pages=169–174 |jstor=2689566}}</ref> It was used earlier by Ed Nelson in his book Predicative Arithmetic, Princeton University Press, 1986.\n* The term ''hyperpower''<ref>{{cite journal |author=J. F. MacDonnell |title=Somecritical points of the hyperpower function <math>x^{x^{\\dots}}</math> |journal=International Journal of Mathematical Education |year=1989 |volume=20 |issue=2 |pages=297–305 |mr=994348 |url=http://www.faculty.fairfield.edu/jmac/ther/tower.htm |doi=10.1080/0020739890200210}}</ref> is a natural combination of ''hyper'' and ''power'', which aptly describes tetration. The problem lies in the meaning of ''hyper'' with respect to the [[hyperoperation]] sequence. When considering hyperoperations, the term ''hyper'' refers to all ranks, and the term ''super'' refers to rank 4, or tetration. So under these considerations ''hyperpower'' is misleading, since it is only referring to tetration.\n* The term ''power tower''<ref>{{MathWorld |urlname=PowerTower |title=Power Tower}}</ref> is occasionally used, in the form \"the power tower of order {{mvar|n}}\" for <math>{\\ \\atop {\\ }} {{\\underbrace{a^{a^{\\cdot^{\\cdot^{a}}}}}} \\atop n}</math>. This is a misnomer, however, because tetration cannot be expressed with iterated ''power'' functions (see above), since it is an iterated ''exponential'' function.\n* The term ''snap'' is occasionally used in informal contexts, in the form \"{{mvar|a}} snap {{mvar|n}}\" for <math>{\\ \\atop {\\ }} {{\\underbrace{a^{a^{\\cdot^{\\cdot^{a}}}}}} \\atop n}</math>. This term is not yet widely accepted, although it is used within select communities.{{citation needed|date=November 2018}} It is believed to be a reference to [[jounce]], the fourth derivative of position in physics, as tetration is the fourth hyperoperation and jounce is also known as snap.\n<!-- (No such article)\n* Ultra exponential is also used, see [[Ultra exponential function]].\n-->\n\nOwing in part to some shared terminology and similar [[Mathematical notation|notational symbolism]], tetration is often confused with closely related functions and expressions. Here are a few related terms:\n:{|class=\"wikitable\"\n! Form\n! Terminology\n|-\n|<math>a^{a^{\\cdot^{\\cdot^{a^a}}}}</math>\n|Tetration\n|-\n|<math>a^{a^{\\cdot^{\\cdot^{a^x}}}}</math>\n|Iterated exponentials\n|-\n|<math>a_1^{a_2^{\\cdot^{\\cdot^{a_n}}}}</math>\n|Nested exponentials (also towers)\n|-\n|<math>a_1^{a_2^{a_3^{\\cdot^{\\cdot^\\cdot}}}}</math>\n|Infinite exponentials (also towers)\n|}\n\nIn the first two expressions {{mvar|a}} is the ''base'', and the number of times {{mvar|a}} appears is the ''height'' (add one for {{mvar|x}}). In the third expression, {{mvar|n}} is the ''height'', but each of the bases is different.\n\nCare must be taken when referring to iterated exponentials, as it is common to call expressions of this form iterated exponentiation, which is ambiguous, as this can either mean [[iterated function|iterated]] [[power (mathematics)|powers]] or iterated [[exponential function|exponentials]].\n\n== Notation ==\nThere are many different notation styles that can be used to express tetration. Some notations can also be used to describe other [[hyperoperation]]s, while some are limited to tetration and have no immediate extension.\n:{|class=\"wikitable\"\n! Name\n! Form\n! Description\n|-\n| Rudy Rucker notation\n| <math>\\,{}^{n}a</math>\n| Used by Maurer [1901] and Goodstein [1947]; [[Rudy Rucker]]'s book ''[[Infinity and the Mind]]'' popularized the notation.\n|-\n| [[Knuth's up-arrow notation]]\n| <math>a {\\uparrow\\uparrow} n</math>\n| Allows extension by putting more arrows, or, even more powerfully, an indexed arrow.\n|-\n| [[Conway chained arrow notation]]\n| <math>a \\rightarrow n \\rightarrow 2</math>\n| Allows extension by increasing the number 2 (equivalent with the extensions above), but also, even more powerfully, by extending the chain\n|-\n| [[Ackermann function]]\n| <math>{}^{n}2 = \\operatorname{A}(4, n - 3) + 3</math>\n| Allows the special case <math>a=2</math> to be written in terms of the Ackermann function.\n|-\n| Iterated exponential notation\n| <math>\\exp_a^n(1)</math>\n| Allows simple extension to iterated exponentials from initial values other than 1.\n|-\n| Hooshmand notations<ref name=\"uxp\">\n {{cite journal\n  | author=M. H. Hooshmand,\n  | year=2006\n  | title=Ultra power and ultra exponential functions\n  | journal=[[Integral Transforms and Special Functions]]\n  | volume=17\n  | issue=8\n  | pages=549–558\n  | doi= 10.1080/10652460500422247\n }}</ref>\n| <math>\\begin{align}\n  &\\operatorname{uxp}_a n \\\\[2pt]\n  &a^{\\frac{n}{}}\n\\end{align}</math>\n| Used by M. H. Hooshmand [2006].\n|-\n| [[Hyperoperation]] notations\n| <math>\\begin{align}\n  &a [4] n \\\\[2pt]\n  &H_4(a, n)\n\\end{align}</math>\n| Allows extension by increasing the number 4; this gives the family of [[hyperoperation]]s.\n|-\n| Double caret notation\n| <code>a^^n</code>\n| Since the up-arrow is used identically to the caret (<code>^</code>), tetration may be written as (<code>^^</code>); convenient for [[ASCII]].\n|-\n| [[Bowers's operators]]\n| {{ubl\n | {a,b,4}\n | {a,b,4,1}\n | a {4} b\n }}\n| Similar to the hyperoperation notations.\n|}\n\nOne notation above uses iterated exponential notation; in general this is defined as follows:\n:<math>\\exp_a^n(x) = a^{a^{\\cdot^{\\cdot^{a^x}}}}</math> with {{mvar|n}} {{mvar|a}}s.\n\nThere are not as many notations for iterated exponentials, but here are a few:\n:{| class=\"wikitable\"\n! Name\n! Form\n! Description\n|-\n| Standard notation\n| <math>\\exp_a^n(x)</math>\n| [[Leonhard Euler|Euler]] coined the notation <math>\\exp_a(x) = a^x</math>, and iteration notation <math>f^n(x)</math> has been around about as long.\n|-\n| Knuth's up-arrow notation\n| <math>(a{\\uparrow})^n(x)</math>\n| Allows for super-powers and super-exponential function by increasing the number of arrows; used in the article on [[large numbers]].\n|-\n| Text notation\n| {{code|2=tex|exp_a^n(x)}}\n| Based on standard notation; convenient for [[ASCII]].\n|-\n| J Notation\n| {{code|2=j|x^^:(n-1)x}}\n| Repeats the exponentiation. See [[J (programming language)]]<ref>{{cite web|title=Power Verb|url=http://www.jsoftware.com/help/dictionary/d202n.htm|work=J Vocabulary|publisher=J Software|accessdate=28 October 2011}}</ref>\n|}\n\n== Examples ==\nBecause of the extremely fast growth of tetration, most values in the following table are too large to write in scientific notation. In these cases, iterated exponential notation is used to express them in base 10. The values containing a decimal point are approximate.\n\n:{| class=\"wikitable\"\n! <math>x</math>\n! <math>{}^{2}x</math>\n! <math>{}^{3}x</math>\n! <math>{}^{4}x</math>\n! <math>{}^{5}x</math>\n|- align=right\n| 1\n| 1\n| 1\n| 1\n| 1\n|- align=right\n| 2\n| 4\n| 16\n| [[65536 (number)|65,536]]\n| 2<sup>65,536</sup> or (2.00353 × 10<sup>19,728</sup>)\n|- align=right\n| 3\n| 27\n| 7,625,597,484,987\n| <math>\\exp_{10}^3(1.09902)</math> (3.6 × 10<sup>12</sup> digits)\n| <math>\\exp_{10}^4(1.09902)</math>\n|- align=right\n| 4\n| 256\n| 1.34078 × 10<sup>154</sup>\n| <math>\\exp_{10}^3(2.18726)</math> (8.1 × 10<sup>153</sup> digits)\n| <math>\\exp_{10}^4(2.18726)</math>\n|- align=right\n| 5\n| 3,125\n| 1.91101 × 10<sup>2,184</sup>\n| <math>\\exp_{10}^3(3.33928)</math> (1.3 × 10<sup>2,184</sup> digits)\n| <math>\\exp_{10}^4(3.33928)</math>\n|- align=right\n| 6\n| 46,656\n| 2.65912 × 10<sup>36,305</sup>\n| <math>\\exp_{10}^3(4.55997)</math> (2.1 × 10<sup>36,305</sup> digits)\n| <math>\\exp_{10}^4(4.55997)</math>\n|- align=right\n| 7\n| 823,543\n| 3.75982 × 10<sup>695,974</sup>\n| <math>\\exp_{10}^3(5.84259)</math> (3.2 × 10<sup>695,974</sup> digits)\n| <math>\\exp_{10}^4(5.84259)</math>\n|- align=right\n| 8\n| 16,777,216\n| 6.01452 × 10<sup>15,151,335</sup>\n| <math>\\exp_{10}^3(7.18045)</math> (5.4 × 10<sup>15,151,335</sup> digits)\n| <math>\\exp_{10}^4(7.18045)</math>\n|- align=right\n| 9\n| 387,420,489\n| 4.28125 × 10<sup>369,693,099</sup>\n| <math>\\exp_{10}^3(8.56784)</math> (4.1 × 10<sup>369,693,099</sup> digits)\n| <math>\\exp_{10}^4(8.56784)</math>\n|- align=right\n| 10\n| 10,000,000,000\n| 10<sup>10,000,000,000</sup>\n| <math>\\exp_{10}^3(10)</math> (10<sup>10<sup>10</sup></sup> digits)\n| <math>\\exp_{10}^4(10)</math>\n|}\n\n==Properties==\nTetration has several properties that are similar to exponentiation, as well as properties that are specific to the operation and are lost or gained from exponentiation. Because exponentiation does not [[commutative law|commute]], the product and power rules do not have an analogue with tetration; the statements <math display=\"inline\">{}^a \\left({}^b x\\right) = \\left({}^{ab} x\\right)</math> and <math display=\"inline\">{}^a \\left(xy\\right) = {}^a x {}^a y</math> are not necessarily true for all cases.<ref>Alexander Meiburg. (2014). [http://web.math.ucsb.edu/~padraic/ucsb_2013_14/mathcs103_s2014/mathcs103_s2014_alex_presentation.pdf Analytic Extension of Tetration Through the Product Power-Tower] Retrieved November 29, 2018</ref>\n\nHowever, tetration does follow a different property, in which <math display=\"inline\">{}^a x = x^{\\left({}^{a-1} x\\right)}</math>. This fact is most clearly shown using the recursive definition. From this property, a proof follows that <math>\\left({}^b a\\right)^{\\left({}^c a\\right)} = \\left({}^{c+1} a\\right)^{\\left({}^{b-1} a\\right)}</math>, which allows for switching ''b'' and ''c'' in certain equations. The proof goes as follows:\n: <math>\\begin{align}\n      &\\left({}^b a\\right)^{\\left({}^c a\\right)}         \\\\\n  ={} &\\left(a^{{}^{b-1} a}\\right)^{\\left({}^c a\\right)} \\\\\n  ={} &a^{\\left({}^{b-1} a\\right)\\left({}^c a\\right)}    \\\\\n  ={} &a^{\\left({}^c a\\right)\\left({}^{b-1} a\\right)}    \\\\\n  ={} &\\left({}^{c+1} a\\right)^{\\left({}^{b-1} a\\right)}\n\\end{align}</math>\n\nWhen a number {{mvar|x}} and 10 are [[coprime]], it is possible to compute the last {{mvar|m}} decimal digits of <math>\\,\\!\\ ^{a}x</math> using [[Euler's theorem]], for any integer {{mvar|m}}.\n\n===Direction of evaluation===\nWhen evaluating tetration expressed as an \"exponentiation tower\", the exponentiation is done at the deepest level first (in the notation, at the apex). For example:\n:<math>\\,\\!\\ ^{4}2 = 2^{2^{2^2}} = 2^{\\left(2^{\\left(2^2\\right)}\\right)} = 2^{\\left(2^4\\right)} = 2^{16} = 65,\\!536</math>\n\nThis order is important because exponentiation is not [[associative]], and evaluating the expression in the opposite [[Bracket (mathematics)#Algebra|order]] will lead to a different answer:\n:<math>\\,\\! 2^{2^{2^2}} \\ne \\left({\\left(2^2\\right)}^2\\right)^2 = 4^{2 \\cdot2} = 256</math>\n\nEvaluating the expression the left to right is considered less interesting; evaluating left to right, any expression <math>^{n}a</math> can be simplified to be <math>a^{\\left(a^{n-1}\\right)}</math>.<ref name=\"tetration extensions\">{{citeweb |url=http://www.mpmueller.net/reihenalgebra.pdf |last=Müller |first= M. |title= Reihenalgebra: What comes beyond exponentiation? |accessdate= 12 December 2018}}</ref> Because of this, the towers must be evaluated from right to left (or top to bottom). Computer programmers refer to this choice as [[right-associative]].\n\n==Extensions==\nTetration can be extended in two different ways; in the equation <math>^na</math>, both the base {{mvar|a}} and the height {{mvar|n}} can be generalized using the definition and properties of tetration. Although the base and the height can be extended beyond the positive integers to different [[domain of a function|domain]]s, including <math>{^n 0}</math>, complex functions such as <math>{}^{n}i</math>, and heights of infinite {{mvar|n}}, the more limited properties of tetration reduce the ability to extend tetration.\n\n===Extension of domain for bases===\n====Base zero====\nThe exponential <math>0^0</math> is not consistently defined. Thus, the tetrations <math>\\,{^{n}0}</math> are not clearly defined by the formula given earlier. However, <math>\\lim_{x\\rightarrow0} {}^{n}x</math> is well defined, and exists:\n:<math>\\lim_{x\\rightarrow0} {}^{n}x = \\begin{cases}\n  1, & n \\text{ even} \\\\\n  0, & n \\text{ odd}\n\\end{cases}</math>\n\nThus we could consistently define <math>{}^{n}0 = \\lim_{x\\rightarrow 0} {}^{n}x</math>. This is equivalent to defining <math>0^0 = 1</math>.\n\nUnder this extension, <math>{}^{0}0 = 1</math>, so the rule <math>{^{0}a} = 1</math> from the original definition still holds.\n\n====Complex bases====\n[[Image:Tetration period.png|thumbnail|Tetration by period]]\n[[Image:Tetration escape.png|thumbnail|Tetration by escape]]\n\nSince [[complex number]]s can be raised to powers, tetration can be applied to ''bases'' of the form {{math|''z'' {{=}} ''a'' + ''bi''}} (where {{mvar|a}} and {{mvar|b}} are real). For example, in {{math|<sup>''n''</sup>''z''}} with {{math|''z'' {{=}} ''i''}}, tetration is achieved by using the [[principal branch]] of the natural logarithm; using [[Euler's formula]] we get the relation:\n\n:<math>i^{a+bi} = e^{\\frac{1}{2}{\\pi i} (a + bi)} = e^{-\\frac{1}{2}{\\pi b}} \\left(\\cos{\\frac{\\pi a}{2}} + i \\sin{\\frac{\\pi a}{2}}\\right)</math>\n\nThis suggests a recursive definition for {{math|<sup>''n''+1</sup>''i'' {{=}} ''a′'' + ''b′i''}} given any {{math|<sup>''n''</sup>''i'' {{=}} ''a'' + ''bi''}}:\n:<math>\\begin{align}\n  a' &= e^{-\\frac{1}{2}{\\pi b}} \\cos{\\frac{\\pi a}{2}} \\\\[2pt]\n  b' &= e^{-\\frac{1}{2}{\\pi b}} \\sin{\\frac{\\pi a}{2}}\n\\end{align}</math>\n\nThe following approximate values can be derived:\n{| class=\"wikitable\"\n|-\n! <math display=\"inline\">{}^{n}i</math>\n! Approximate value\n|-\n| <math display=\"inline\">{}^{1}i = i</math>\n| {{math|''i''}}\n|-\n| <math display=\"inline\">{}^{2}i = i^{\\left({}^{1}i\\right)}</math>\n| {{math|0.2079}}\n|-\n| <math display=\"inline\">{}^{3}i = i^{\\left({}^{2}i\\right)}</math>\n| {{math|0.9472 + 0.3208''i''}}\n|-\n| <math display=\"inline\">{}^{4}i = i^{\\left({}^{3}i\\right)}</math>\n| {{math|0.0501 + 0.6021''i''}}\n|-\n| <math display=\"inline\">{}^{5}i = i^{\\left({}^{4}i\\right)}</math>\n| {{math|0.3872 + 0.0305''i''}}\n|-\n| <math display=\"inline\">{}^{6}i = i^{\\left({}^{5}i\\right)}</math>\n| {{math|0.7823 + 0.5446''i''}}\n|-\n| <math display=\"inline\">{}^{7}i = i^{\\left({}^{6}i\\right)}</math>\n| {{math|0.1426 + 0.4005''i''}}\n|-\n| <math display=\"inline\">{}^{8}i = i^{\\left({}^{7}i\\right)}</math>\n| {{math|0.5198 + 0.1184''i''}}\n|-\n| <math display=\"inline\">{}^{9}i = i^{\\left({}^{8}i\\right)}</math>\n| {{math|0.5686 + 0.6051''i''}}\n|}\n\nSolving the inverse relation, as in the previous section, yields the expected {{math|<sup>0</sup>''i'' {{=}} 1}} and {{math|<sup>−1</sup>''i'' {{=}} 0}}, with negative values of {{mvar|n}} giving infinite results on the imaginary axis. Plotted in the [[complex plane]], the entire sequence spirals to the limit {{math|0.4383 + 0.3606''i''}}, which could be interpreted as the value where {{mvar|n}} is infinite.\n\nSuch tetration sequences have been studied since the time of Euler, but are poorly understood due to their chaotic behavior. Most published research historically has focused on the convergence of the infinitely iterated exponential function. Current research has greatly benefited by the advent of powerful computers with [[fractal]] and symbolic mathematics software. Much of what is known about tetration comes from general knowledge of complex dynamics and specific research of the exponential map.\n\n===Extensions of the domain for different \"heights\"===\n\n====Infinite heights====\n[[Image:Infinite_power_tower.svg|thumb|<math>\\textstyle \\lim_{n\\rightarrow \\infty} {}^nx</math> of the infinitely iterated exponential converges for the bases <math>\\textstyle \\left(e^{-1}\\right)^e \\le x \\le e^{\\left(e^{-1}\\right)} </math>]]\n[[Image:TetrationConvergence3D.png|thumbnail|The function <math>\\left| \\frac{\\mathrm{W}(-\\ln{z})}{-\\ln{z}} \\right|</math> on the complex plane, showing the real-valued infinitely iterated exponential function (black curve)]]\n\nTetration can be extended to [[Infinity|infinite]] heights;<ref>{{cite web | url=http://math.blogoverflow.com/2015/01/05/climbing-the-ladder-of-hyper-operators-tetration/ | title=Climbing the ladder of hyper operators: tetration | publisher=George Daccache | date=January 5, 2015 | accessdate=18 February 2016}}</ref> i.e., for certain {{mvar|a}} and {{mvar|n}} values in <math>{}^{n}a</math>, there exists a well defined result for an infinite {{mvar|n}}. This is because for bases within a certain interval, tetration converges to a finite value as the height tends to [[infinity]]. For example, <math>\\sqrt{2}^{\\sqrt{2}^{\\sqrt{2}^{\\cdot^{\\cdot^{\\cdot}}}}}</math> converges to 2, and can therefore be said to be equal to 2. The trend towards 2 can be seen by evaluating a small finite tower:\n\n:<math>\\begin{align}\n  \\sqrt{2}^{\\sqrt{2}^{\\sqrt{2}^{\\sqrt{2}^{\\sqrt{2}^{1.414}}}}}\n    &\\approx \\sqrt{2}^{\\sqrt{2}^{\\sqrt{2}^{\\sqrt{2}^{1.63}}}} \\\\\n    &\\approx \\sqrt{2}^{\\sqrt{2}^{\\sqrt{2}^{1.76}}} \\\\\n    &\\approx \\sqrt{2}^{\\sqrt{2}^{1.84}} \\\\\n    &\\approx \\sqrt{2}^{1.89} \\\\\n    &\\approx 1.93\n\\end{align}</math>\n\nIn general, the infinitely iterated exponential <math>x^{x^{\\cdot^{\\cdot^{\\cdot}}}}</math>, defined as the limit of <math>{}^{n}x</math> as {{mvar|n}} goes to infinity, converges for {{math|''e''<sup>−''e''</sup> ≤ ''x'' ≤ ''e''<sup>1/''e''</sup>}}, roughly the interval from 0.066 to 1.44, a result shown by [[Leonhard Euler]].<ref>Euler, L. \"De serie Lambertina Plurimisque eius insignibus proprietatibus.\" ''Acta Acad. Scient. Petropol. 2'', 29–51, 1783. Reprinted in Euler, L. ''Opera Omnia, Series Prima, Vol. 6: Commentationes Algebraicae''. Leipzig, Germany: Teubner, pp. 350–369, 1921. ([http://math.dartmouth.edu/~euler/docs/originals/E532.pdf facsimile])</ref> The limit, should it exist, is a positive real solution of the equation {{math|1=''y'' = ''x''<sup>''y''</sup>}}. Thus, {{math|1=''x'' = ''y''<sup>1/''y''</sup>}}. The limit defining the infinite tetration of {{mvar|x}} fails to converge for  {{math|''x'' > ''e''<sup>1/''e''</sup>}} because the maximum of {{math|''y''<sup>1/''y''</sup>}} is {{math|''e''<sup>1/''e''</sup>}}.\n\nThis may be extended to complex numbers {{mvar|z}} with the definition:\n:<math>{}^{\\infty}z = z^{z^{\\cdot^{\\cdot^{\\cdot}}}} = \\frac{\\mathrm{W}(-\\ln{z})}{-\\ln{z}} ~,</math>\nwhere {{math|W}} represents [[Lambert's W function]].\n\nAs the limit {{math|1=''y'' = <sup>∞</sup>''x''}} (if existent, i.e. for {{math|''e''<sup>−''e''</sup> < ''x'' < ''e''<sup>1/''e''</sup>}}) must satisfy {{math|1=''x''<sup>''y''</sup> = ''y''}} we see that {{math|1=''x'' ↦ ''y'' = <sup>∞</sup>''x''}} is (the lower branch of) the inverse function of {{math|1=''y'' ↦ ''x'' = ''y''<sup>1/''y''</sup>}}.\n\n====Negative heights====\nWe can use the recursive rule for tetration,\n:<math> {^{k+1}a} = a^{\\left({^{k}a}\\right)}, </math>\n\nto prove <math>{}^{-1}a</math>:\n:<math> ^{k}a = \\log_a \\left(^{k+1}a\\right);</math>\n\nSubstituting −1 for {{mvar|k}} gives\n:<math> {}^{-1}a = \\log_{a} \\left({}^0 a\\right) = \\log_a 1 = 0</math>.<ref name=\"tetration extensions\"/>\n\nSmaller negative values cannot be well defined in this way. Substituting −2 for {{mvar|k}} in the same equation gives\n:<math> {}^{-2}a = \\log_{a} \\left( {}^{-1}a \\right) = \\log_a 0 </math>\n\nwhich is not well defined. They can, however, sometimes be considered sets.<ref name=\"tetration extensions\"/>\n\nFor <math> n = 1 </math>, any definition of <math>\\,\\! {^{-1}1} </math> is consistent with the rule because\n:<math> {^{0}1} = 1 = 1^n </math> for any <math>\\,\\! n = {^{-1}1} </math>.\n\n====Real heights====\n\nAt this time there is no commonly accepted solution to the general problem of extending tetration to the real or complex values of {{mvar|n}}. There have, however, been multiple approaches towards the issue, and different approaches are outlined below.\n\nIn general the problem is finding, for any real {{math|''a'' > 0}}, a ''super-exponential function'' <math>\\,f(x) = {}^{x}a</math> over real {{math|''x'' > −2}} that satisfies\n*<math>\\,{}^{-1}a = 0</math>\n*<math>\\,{}^{0}a = 1</math>\n*<math>\\,{}^{x}a = a^{\\left({}^{x-1}a\\right)}</math>for all real <math>x > -1.</math><ref>{{cite web |url=https://www.researchgate.net/profile/Dmitrii_Kouznetsov/publication/265066072_5_methods_for_real_analytic_tetration/links/544e57db0cf26dda08900c3d.pdf|title=5+ methods for real analytic tetration|last=Trappmann|first=Henryk|last2=Kouznetsov|first2=Dmitrii||date=June 28, 2010|access-date= 5 December 2018}}</ref>\n\nTo find a more natural extension, one or more extra requirements are usually required. This is usually some collection of the following:\n*A ''continuity'' requirement (usually just that <math>{}^{x}a</math> is continuous in both variables for <math>x > 0</math>).\n*A ''differentiability'' requirement (can be once, twice, {{mvar|k}} times, or infinitely differentiable in {{mvar|x}}).\n*A ''regularity'' requirement (implying twice differentiable in {{mvar|x}}) that:\n:<math>\\left( \\frac{d^2}{dx^2}f(x) > 0\\right)</math> for all <math>x > 0</math>\n\nThe fourth requirement differs from author to author, and between approaches. There are two main approaches to extending tetration to real heights; one is based on the ''regularity'' requirement, and one is based on the ''differentiability'' requirement. These two approaches seem to be so different that they may not be reconciled, as they produce results inconsistent with each other.\n\nWhen <math>\\,{}^{x}a</math> is defined for an interval of length one, the whole function easily follows for all {{math|''x'' > −2}}.\n\n=====Linear approximation for real heights=====\n[[Image:Real-tetration.png|thumbnail|<math>\\,{}^{x}e</math> using linear approximation.]]\nA [[linear approximation]] (solution to the continuity requirement, approximation to the differentiability requirement) is given by:\n:<math>{}^{x}a \\approx \\begin{cases}\n  \\log_a\\left(^{x+1}a\\right) &  x \\le -1 \\\\\n                       1 + x & -1 < x \\le 0 \\\\\n    a^{\\left(^{x-1}a\\right)} &  0 < x\n\\end{cases}</math>\n\nhence:\n{| class=\"wikitable\"\n! Approximation\n! Domain\n|-\n|  <math display=\"inline\">{}^x a \\approx x + 1</math>\n| for {{math|−1 < ''x'' < 0}}\n|-\n|  <math display=\"inline\">{}^x a \\approx a^x</math>\n| for {{math|0 < ''x'' < 1}}\n|-\n|  <math display=\"inline\">{}^x a \\approx a^{a^{(x-1)}}</math>\n| for {{math|1 < ''x'' < 2}}\n|}\n\nand so on. However, it is only piecewise differentiable; at integer values of {{mvar|x}} the derivative is multiplied by <math>\\ln{a}</math>. It is continuously differentiable for <math>x > -2</math> if and only if <math>a = e</math>. For example, using these methods <math>{}^\\frac{\\pi}{2}e \\approx 5.868...</math> and <math>{}^{-4.3}0.5 \\approx 4.03335...</math>\n\nA main theorem in Hooshmand's paper<ref name=\"uxp\"/> states: Let <math>0 < a \\neq 1</math>. If <math>f:(-2, +\\infty)\\rightarrow \\mathbb{R}</math> is continuous and satisfies the conditions:\n\n*<math>f(x) = a^{f(x-1)} \\;\\; \\text{for all} \\;\\; x > -1, \\; f(0) = 1,</math>\n*<math>f</math> is differentiable on {{open-open|−1, 0}},\n*<math>f^\\prime</math> is a nondecreasing or nonincreasing function on {{open-open|−1, 0}},\n*<math>f^\\prime \\left(0^+\\right) = (\\ln a) f^\\prime \\left(0^-\\right) \\text{ or } f^\\prime \\left(-1^+\\right) = f^\\prime \\left(0^-\\right).</math>\n\nthen <math>f</math> is uniquely determined through the equation\n:<math>f(x) = \\exp^{[x]}_a \\left(a^{(x)}\\right) = \\exp^{[x+1]}_a((x)) \\quad \\text{for all} \\; \\; x > -2,</math>\n\nwhere <math> (x) = x - [x] </math> denotes the fractional part of {{mvar|x}} and <math> \\exp^{[x]}_a </math> is the <math> [x] </math>-[[iterated function]] of the function <math> \\exp_a </math>.\n\nThe proof is that the second through fourth conditions trivially imply that {{mvar|f}} is a linear function on {{closed-closed|−1, 0}}.\n\nThe linear approximation to natural tetration function <math>{}^xe</math> is continuously differentiable, but its second derivative does not exist at integer values of its argument. Hooshmand derived another uniqueness theorem for it which states:\n\nIf <math> f: (-2, +\\infty)\\rightarrow \\mathbb{R}</math> is a continuous function that satisfies:\n\n*<math> f(x) = e^{f(x-1)} \\;\\; \\text{for all} \\;\\; x > -1, \\; f(0) = 1,</math>\n*<math>f</math> is convex on {{open-open|−1, 0}},\n*<math>f^\\prime \\left(0^-\\right) \\leq f^\\prime \\left(0^+\\right).</math>\n\nthen <math>f = \\text{uxp}</math>. [Here <math>f = \\text{uxp}</math> is Hooshmand's name for the linear approximation to the natural tetration function.]\n\nThe proof is much the same as before; the recursion equation ensures that <math>f^\\prime (-1^+) = f^\\prime (0^+),</math> and then the convexity condition implies that <math>f</math> is linear on {{open-open|−1, 0}}.\n\nTherefore, the linear approximation to natural tetration is the only solution of the equation <math>f(x) = e^{f(x-1)} \\;\\; (x > -1)</math> and <math>f(0) = 1</math> which is [[convex function|convex]] on {{open-open|−1, +∞}}. All other sufficiently-differentiable solutions must have an [[inflection point]] on the interval {{open-open|−1, 0}}.\n\n===== Higher order approximations for real heights =====\n[[File:Approximations of 0.5 tetratrated to the x.png|thumb|a comparison of the linear and quadratic approximations (in red and blue respectively) of the function <math>^{x}0.5</math>, from {{math|1=''x'' = −2}} to {{math|1=''x'' = 2}}.]]\nBeyond linear approximations, a [[quadratic approximation]] (to the differentiability requirement) is given by:\n:<math>{}^{x}a \\approx \\begin{cases}\n  \\log_a\\left({}^{x+1}a\\right) & x \\le -1 \\\\\n  1 + \\frac{2\\ln(a)}{1 \\;+\\; \\ln(a)}x - \\frac{1 \\;-\\; \\ln(a)}{1 \\;+\\; \\ln(a)}x^2 & -1 < x \\le 0 \\\\\n  a^{\\left({}^{x-1}a\\right)} & x >0\n\\end{cases}</math>\n\nwhich is differentiable for all <math>x > 0</math>, but not twice differentiable. For example, <math>{}^\\frac{1}{2}2 \\approx 1.45933...</math> If <math>a = e</math> this is the same as the linear approximation.<ref name=\"uwu\">Neyrinck, Mark. [http://skysrv.pha.jhu.edu/~neyrinck/extessay.pdf An Investigation of Arithmetic Operations.] Retrieved 9 January 2019.</ref>\n\nNote that because of the way it is calculated, this function does not \"cancel out\", contrary to exponents, where <math>\\left(a^\\frac{1}{n}\\right)^n = a</math>. Namely,\n:<math>\n  {}^n\\left({}^\\frac{1}{n} a\\right)\n  = \\underbrace{\n      \\left({}^\\frac{1}{n}a\\right)^{\n        \\left({}^\\frac{1}{n}a\\right)^{\n          \\cdot^{\\cdot^{\\cdot^{\\cdot^{\n            \\left({}^\\frac{1}{n}a\\right)\n          }}}}\n        }\n      }\n    }_n\n  \\neq a\n</math>.\n\nJust as there is a quadratic approximation, cubic approximations and methods for generalizing to approximations of degree {{mvar|n}} also exist, although they are much more unwieldy.<ref name=\"uwu\"/><ref name=SolveAnalyt>Andrew Robbins. [https://web.archive.org/web/20090201164821/http://tetration.itgo.com/paper.html Solving for the Analytic Piecewise Extension of Tetration and the Super-logarithm]. The extensions are found in part two of the paper, \"Beginning of Results\".</ref>\n\n====Complex heights====\n[[File:Tetration analytic extension.svg|400px|right|thumb|Drawing of the analytic extension <math>f = F(x+{\\rm i}y)</math> of tetration to the complex plane. Levels <math>|f| = 1, e^{\\pm 1}, e^{\\pm 2}, \\ldots</math> and levels <math>\\arg(f) = 0, \\pm 1, \\pm 2, \\ldots</math> are shown with thick curves.]]\n<!--\nThe existence of an analytic extension of <math>{}^{z}a</math> to complex values of <math>z</math> is not yet established. For <math>a = e</math>, it could be a solution of the [[functional equation]] <math>F(z + 1) = \\exp(F(z))</math> with the additional conditions that <math>F(0) = 1</math> and <math>F(z)</math> remains finite as <math>z\\to\\pm{\\rm i}\\infty</math>.\nUpdate: The existence of such an analytic extension of <math>{}^{z}a</math> has now been established. \n!-->\n\nIt has now been proven<ref name=\"PAU10\">{{cite journal\n|author=W. Paulsen and S. Cowgill\n|title=Solving <math>F(z+1) = b^{F(z)}</math> in the complex plane\n|journal=Advances in Computational Mathematics\n|pages=1–22\n|date=March 2017\n|url=http://myweb.astate.edu/wpaulsen/tetration2.pdf\n|doi=10.1007/s10444-017-9524-1\n|volume=43\n}}</ref> that there exists a unique function {{mvar|F}} which is a solution of the equation {{math|1=''F''(''z'' + 1) = exp(''F''(''z''))}} and satisfies the additional conditions that {{math|1=''F''(0) = 1}} and {{math|''F''(''z'')}} approaches the [[fixed point (mathematics)|fixed points]] of the logarithm (roughly {{math|0.318 ± 1.337''i''}}) as {{mvar|z}} approaches {{math|±''i''∞}} and that {{mvar|F}} is [[holomorphic function|holomorphic]] in the whole complex {{mvar|z}}-plane, except the part of the real axis at {{math|''z'' ≤ −2}}. This proof confirms a previous [[conjecture]].<ref name=\"MOC09\">{{cite journal\n|author=D. Kouznetsov\n|title=Solution of <math>F(z + 1) = \\exp(F(z))</math> in complex <math>z</math>-plane\n|journal=[[Mathematics of Computation]]\n|volume=78\n|issue=267\n|pages=1647–1670\n|date=July 2009\n|url=http://www.ams.org/mcom/2009-78-267/S0025-5718-09-02188-7/S0025-5718-09-02188-7.pdf\n|doi=10.1090/S0025-5718-09-02188-7\n}}</ref> The complex map of this function is shown in the figure at right. The proof also works for other bases besides ''e'', as long as the base is bigger than <math>e^\\frac{1}{e}</math>. The complex double precision approximation of this function is available online.{{citation needed|date=February 2016}}<!-- This function is not [[entire function|entire]], as there are singularities of <math>F(z)</math> on the real axis at the points <math>z=-2,-3,-4,\\ldots</math>.!-->\n\nThe requirement of the tetration being holomorphic is important for its uniqueness. Many functions {{mvar|S}} can be constructed as\n: <math>S(z) = F\\!\\left(~z~\n    + \\sum_{n=1}^{\\infty} \\sin(2\\pi nz)~ \\alpha_n\n    + \\sum_{n=1}^{\\infty} \\Big(1 - \\cos(2\\pi nz)\\Big) ~\\beta_n\n  \\right)\n</math>\n\nwhere {{mvar|α}} and {{mvar|β}} are real sequences which decay fast enough to provide the [[Limit (mathematics)#Limit of a sequence|convergence of the series]], at least at moderate values of {{math|Im ''z''}}.\n\nThe function {{mvar|S}} satisfies the tetration equations {{math|1=''S''(''z'' + 1) = exp(''S''(''z''))}}, {{math|1=''S''(0) = 1}}, and if {{math|''α<sub>n</sub>''}} and {{math|''β<sub>n</sub>''}} approach 0 fast enough it will be analytic on a neighborhood of the positive real axis. However, if some elements of {{math|{''α''}<nowiki/>}} or {{math|{''β''}<nowiki/>}} are not zero, then function {{mvar|S}} has multitudes of additional singularities and cutlines in the complex plane, due to the exponential growth of sin and cos along the imaginary axis; the smaller the coefficients {{math|{''α''}<nowiki/>}} and {{math|{''β''}<nowiki/>}} are, the further away these singularities are from the real axis.\n\nThe extension of tetration into the complex plane is thus essential for the uniqueness; the [[real-analytic]] tetration is not unique.\n\n== Non-elementary recursiveness ==\nTetration (restricted to <math>\\mathbb{N}^2</math>) is not an [[ELEMENTARY|elementary recursive function]]. One can prove by induction that for every elementary recursive function {{mvar|f}}, there is a constant {{mvar|c}} such that\n:<math>f(x) \\leq \\underbrace{2^{2^{\\cdot^{\\cdot^{x}}}}}_c.</math>\nWe denote the right hand side by <math>g(c, x)</math>. Suppose on the contrary that tetration is elementary recursive. <math>g(x, x)+1</math> is also elementary recursive. By the above inequality, there is a constant {{mvar|c}} such that <math>g(x,x) +1 \\leq g(c, x)</math>. By letting <math>x=c</math>, we have that <math>g(c,c) + 1 \\leq g(c, c)</math>, a contradiction.\n\n== Inverse operations ==\n[[Exponentiation]] has two inverse operations; [[Nth root|roots]] and [[logarithm]]s. Analogously, the [[Inverse function|inverse]]s of tetration are often called the ''super-root'', and the ''super-logarithm'' (In fact, all hyperoperations greater than or equal to 3 have analogous inverses); e.g., in the function <math>{^3}y=x</math>, the two inverses are the cube super-root of {{mvar|y}} and the super logarithm base&nbsp;{{mvar|y}} of {{mvar|x}}.\n\n===Super-root===\n{{redir|Super-root|the directory supported by some Unixes|super-root (computing)}}\nThe super-root is the inverse operation of tetration with respect to the base: if <math>^n y = x</math>, then {{mvar|y}} is an {{mvar|n}}th super root of {{mvar|x}} (<math>\\sqrt[n]{x}_s</math> or <math>\\sqrt[n]{x}_4</math>).\n\nFor example,\n:<math>^4 2 = 2^{2^{2^{2}}} = 65{,}536</math>\n\nso 2 is the 4th super-root of 65,536.\n\n==== Square super-root ====\n[[Image:The graph y = √x(s).png|thumb|right|The graph <math>y = \\sqrt{x}_s</math>.]]\nThe ''2nd-order super-root'', ''square super-root'', or ''super square root'' has two equivalent notations, <math>\\mathrm{ssrt}(x)</math> and <math>\\sqrt{x}_s</math>. It is the inverse of <math>^2 x = x^x</math> and can be represented with the [[Lambert W function]]:<ref name = \"Corless\">\n{{cite journal\n  | last1 = Corless  | first1 = R. M.\n  | last2 = Gonnet | first2 = G. H.\n  | last3 = Hare | first3 = D. E. G.\n  | last4 = Jeffrey | first4 = D. J.\n  | last5 = Knuth | first5 = D. E.\n  | author5-link = Donald Knuth\n  | title = On the Lambert W function\n  | journal = Advances in Computational Mathematics\n  | volume = 5\n  | page = 333\n  | year = 1996\n  | url = http://www.apmaths.uwo.ca/~rcorless/frames/PAPERS/LambertW/LambertW.ps <!-- or http://www.apmaths.uwo.ca/~djeffrey/Offprints/W-adv-cm.pdf -->\n  | format = [[PostScript]] <!-- or PDF -->\n  | doi = 10.1007/BF02124750\n| arxiv = 1809.07369\n  }}</ref>\n\n:<math>\\mathrm{ssrt}(x)=e^{W(\\ln x)}=\\frac{\\ln x}{W(\\ln x)}</math>\n\nThe function also illustrates the reflective nature of the root and logarithm functions as the equation below only holds true when <math>y = \\mathrm{ssrt}(x)</math>:\n\n: <math>\\sqrt[y]{x} = \\log_y x</math>\n\nLike [[square root]]s, the square super-root of {{mvar|x}} may not have a single solution. Unlike square roots, determining the number of square super-roots of {{mvar|x}} may be difficult. In general, if <math>e^{-1/e}<x<1</math>, then {{mvar|x}} has two positive square super-roots between 0 and 1; and if <math>x > 1</math>, then {{mvar|x}} has one positive square super-root greater than 1. If {{mvar|x}} is positive and less than <math>e^{-1/e}</math> it doesn't have any [[real number|real]] square super-roots, but the formula given above yields countably infinitely many [[complex number|complex]] ones for any finite {{mvar|x}} not equal to 1.<ref name = \"Corless\" /> The function has been used to determine the size of [[data cluster]]s.<ref>Krishnam R. (2004), \"[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.10.8594 Efficient Self-Organization Of Large Wireless Sensor Networks]\" - Dissertation, BOSTON UNIVERSITY, COLLEGE OF ENGINEERING. pp. 37–40</ref>\n\n==== Other super-roots ====\n[[File:Cube super root.png|thumb|The graph <math>y=\\sqrt[3]{x}_s</math>.]]\nFor each integer {{math|''n'' > 2}}, the function {{math|''<sup>n</sup>x''}} is defined and increasing for {{math|''x'' ≥ 1}}, and {{math|1=<sup>''n''</sup>1 = 1}}, so that the {{mvar|n}}th super-root of {{mvar|x}}, <math>\\sqrt[n]{x}_s</math>, exists for {{math|''x'' ≥ 1}}.\n\nHowever, if the [[Tetration#Linear approximation for the extension to real heights|linear approximation above]] is used, then <math> ^y x = y + 1</math> if {{math|−1 < ''y'' ≤ 0}}, so <math> ^y \\sqrt{y + 1}_s </math> cannot exist.\n\nIn the same way as the square super-root, terminology for other super roots can be based on the [[nth root|normal roots]]: \"cube super-roots\" can be expressed as <math>\\sqrt[3]{x}_s</math>; the \"4th super-root\" can be expressed as <math>\\sqrt[4]{x}_s</math>; and the \"{{mvar|n}}th super-root\" is <math>\\sqrt[n]{x}_s</math>. Note that <math>\\sqrt[n]{x}_s</math> may not be uniquely defined, because there may be more than one {{mvar|n}}<sup>th</sup> root. For example, {{mvar|x}} has a single (real) super-root if {{mvar|n}} is ''odd'', and up to two if {{mvar|n}} is ''even''.{{Citation needed|date=October 2009}}\n\nJust as with the extension of tetration to infinite heights, the super-root can be extended to {{math|1=n = ∞}}, being well-defined if {{math|1/''e'' ≤ ''x'' ≤ ''e''}}. Note that <math> x = {^\\infty y} = y^{\\left[^\\infty y\\right]} = y^x,</math> and thus that <math> y = x^{1/x} </math>. Therefore, when it is well defined, <math> \\sqrt[\\infty]{x}_s = x^{1/x} </math> and, unlike normal tetration, is an [[elementary function]]. For example, <math>\\sqrt[\\infty]{2}_s = 2^{1/2} = \\sqrt{2}</math>.\n\nIt follows from the [[Gelfond–Schneider theorem]] that super-root <math>\\sqrt{n}_s</math> for any positive integer {{mvar|n}} is either integer or [[Transcendental number|transcendental]], and <math>\\sqrt[3]{n}_s</math> is either integer or irrational.<ref name=\"condor.depaul.edu\"/> It is still an open question whether irrational super-roots are transcendental in the latter case.\n\n===Super-logarithm===\n{{main|Super-logarithm}}\nOnce a continuous increasing (in {{mvar|x}}) definition of tetration, {{math|<sup>''x''</sup>''a''}}, is selected, the corresponding super-logarithm <math>\\operatorname{slog}_ax</math> or <math>\\log^4_ax</math> is defined for all real numbers {{mvar|x}}, and {{math|''a'' > 1}}.\n\nThe function {{math|slog<sub>''a''</sub> ''x''}} satisfies:\n:<math>\\begin{array}{lcl}\n\\operatorname{slog}_a {^x a} &=& x \\\\\n\\operatorname{slog}_a a^x &=& 1 + \\operatorname{slog}_a x \\\\\n\\operatorname{slog}_a x &=& 1 + \\operatorname{slog}_a \\log_a x \\\\\n\\operatorname{slog}_a x &>& -2\n\\end{array}\n</math>\n\n==Open questions==\nOther than the problems with the extensions of tetration, there are several open questions concerning tetration, particularly when concerning the relations between number systems such as [[integer]]s and [[irrational number]]s:\n*It is not known whether there is a positive integer {{mvar|n}} for which {{math|<sup>''n''</sup>[[pi|&pi;]]}} or {{math|<sup>''n''</sup>[[e (mathematical constant)|''e'']]}} is an integer. In particular, it is not known whether {{math|<sup>4</sup>&pi;}} is an integer.\n*It is not known whether {{math|<sup>''n''</sup>''q''}} is an integer for any positive integer {{mvar|n}} and positive non-integer rational {{mvar|q}}.<ref name=\"condor.depaul.edu\">[http://condor.depaul.edu/mash/atotheamg.pdf Marshall, Ash J., and Tan, Yiren, \"A rational number of the form {{math|''a''<sup>''a''</sup>}} with {{mvar|a}} irrational\", Mathematical Gazette 96, March 2012, pp. 106–109.]</ref> Particularly, it is not known whether the positive root of the equation {{math|1=<sup>4</sup>''x'' = 2}} is a rational number.\n\n== See also ==\n*[[Ackermann function]]\n*[[Big O notation]]\n*[[Double exponential function]]\n*[[Hyperoperation]]\n*[[Iterated logarithm]]\n*[[Symmetric level-index arithmetic]]\n\n== References ==\n{{reflist|2}}\n\n* Daniel Geisler, ''[http://www.iteratedfunctions.com/ iteratedfunctions.com]''\n* Ioannis Galidakis, ''[https://web.archive.org/web/20090520164620/http://ioannis.virtualcomposer2000.com/math/exponents4.html On extending hyper4 to nonintegers]'' (undated, 2006 or earlier) ''(A simpler, easier to read review of the next reference)''\n* Ioannis Galidakis, ''[https://web.archive.org/web/20060525195301/http://ioannis.virtualcomposer2000.com/math/papers/Extensions.pdf On Extending hyper4 and Knuth's Up-arrow Notation to the Reals]'' (undated, 2006 or earlier).\n* Robert Munafo, ''[http://mrob.com/pub/math/hyper4.html#real-hyper4 Extension of the hyper4 function to reals]'' ''(An informal discussion about extending tetration to the real numbers.)''\n* Lode Vandevenne, ''[http://groups.google.com/group/sci.math/browse_frm/thread/39a7019f9051c5d7/8c1c4facb7e4bd6d#8c1c4facb7e4bd6d Tetration of the Square Root of Two]'', (2004). ''(Attempt to extend tetration to real numbers.)''\n* Ioannis Galidakis, ''[https://web.archive.org/web/20090420132250/http://ioannis.virtualcomposer2000.com/math/index.html Mathematics]'', ''(Definitive list of references to tetration research. Lots of information on the Lambert W function, Riemann surfaces, and analytic continuation.)''\n* Galidakis, Ioannis and Weisstein, Eric W. [http://mathworld.wolfram.com/PowerTower.html Power Tower]\n* Joseph MacDonell, ''[http://www.faculty.fairfield.edu/jmac/ther/tower.htm Some Critical Points of the Hyperpower Function]''.\n* Dave L. Renfro, ''[http://mathforum.org/discuss/sci.math/t/350321 Web pages for infinitely iterated exponentials]''\n* {{cite journal | author = Knobel R | year = 1981 | title = Exponentials Reiterated | url = | journal = [[American Mathematical Monthly]] | volume = 88 | issue = | pages = 235–252 }}\n* Hans Maurer, \"Über die Funktion <math>y=x^{[x^{[x(\\cdots)]}]}</math> für ganzzahliges Argument (Abundanzen).\" ''Mittheilungen der Mathematische Gesellschaft in Hamburg'' '''4''', (1901), p.&nbsp;33–50. ''(Reference to usage of <math>\\ {^{n} a}</math> from Knobel's paper.)''\n{{Commons category|tetration}}\n* ''[http://ariwatch.com/VS/Algorithms/TheFourthOperation.htm The Fourth Operation]'' \n\n{{Hyperoperations}}\n{{Large numbers}}\n\n[[Category:Exponentials]]\n[[Category:Binary operations]]\n[[Category:Large numbers]]"
    },
    {
      "title": "Theory of fructification",
      "url": "https://en.wikipedia.org/wiki/Theory_of_fructification",
      "text": "In [[economics]], the '''theory of fructification''' is a theory of the [[interest]] rate which was proposed by French economist and finance minister [[Anne Robert Jacques Turgot]]. The term ''theory of fructification'' is due to [[Eugen von Böhm-Bawerk]] who considered Turgot as the first economist who tried to develop a scientific explanation of the interest rate.<ref>[http://oll.libertyfund.org/titles/bawerk-capital-and-interest-a-critical-history-of-economic-theory Böhm-Bawerk, E. (1884) Capital and Interest: A Critical History of Economic Theory. London.], p. 61</ref>\n\nAccording to Turgot, a capitalist can either lend his money, or employ it in the purchase of a plot of land. Because fruitful land yields an annual [[Economic rent|rent]] forever, its price is given by the formula of a [[perpetuity|perpetual annuity]]: If ''A'' denotes the land's annual rent and ''r'' denotes the interest rate, the land price is simply ''A''/''r''. From this formula, Turgot concluded that \"the lower the interest rate, the more valuable is the land.\"<ref name=\"Turgot\">[http://oll.libertyfund.org/titles/turgot-reflections-on-the-formation-and-distribution-of-riches Turgot, J.(1770) Reflections on the Formation and Distribution of Riches, english translation 1898]</ref>{{rp|§89}} Specifically, if the interest rate approached zero, the land price would become infinite. Because land prices must be finite, it follows that the interest rate is strictly positive. Turgot argued also that the mechanism which keeps interest rates above zero crowds out inefficient capital formation.<ref name=\"Turgot\"/>{{rp|§90}}\n\nBöhm-Bawerk, who sponsored a different interest theory, considered Turgot's approach as circular. However, according to [[Joseph Schumpeter]], the eminent economic historian, \"Turgot's contribution is not only by far the greatest performance in the field of interest theory the eighteenth century produced but it clearly foreshadowed much of the best thought of the last decades of the nineteenth.\"<ref>[[Joseph A. Schumpeter]] (1954) ''History of Economic Analysis''. New York: Oxford University Press.</ref>\n\nMuch later, economists demonstrated that the theory of fructification can be stated rigorously in a [[general equilibrium theory|general equilibrium model]].<ref>[[Edmond Malinvaud]] (1953) Capital Accumulation and Efficient Allocation of Resources. ''Econometrica'' 21, p. 257.</ref> They also generalized Turgot's proposition in two respects. First, land which is useful for residential or industrial purposes can be substituted for agricultural land. Second, in a growing economy, the existence of land implies that the interest rate exceeds the growth rate if the land's income share is bounded away from zero. <ref>[[Stefan Homburg]] (1992) [https://ideas.repec.org/b/zbw/esmono/92903.html ''Efficient Economic Growth'', Berlin.]</ref> The latter result is notable because it states that land ensures [[dynamic efficiency]].\n\n==References==\n{{reflist}}\n\n[[Category:Interest]]\n[[Category:Finance theories]]\n[[Category:Exponentials]]\n[[Category:Mathematical finance]]\n[[Category:Actuarial science]]\n[[Category:Economic history studies]]\n\n\n{{finance-stub}}"
    },
    {
      "title": "Turán's method",
      "url": "https://en.wikipedia.org/wiki/Tur%C3%A1n%27s_method",
      "text": "{{no footnotes|date=December 2013}}\n\nIn mathematics, '''Turán's method''' provides lower bounds for [[exponential sum]]s and complex [[power sum]]s.  The method has been applied to problems in [[equidistribution]].\n\nThe method  applies to sums of the form\n\n:<math> s_\\nu = \\sum_{n=1}^N b_n z_n^\\nu \\ </math>\n\nwhere the ''b'' and ''z'' are complex numbers and ''ν'' runs over a range of integers.  There are two main results, depending on the size of the complex numbers&nbsp;''z''.\n\n==Turán's first theorem==\nThe first result applies to sums ''s''<sub>ν</sub> where <math>|z_n| \\ge 1</math> for all&nbsp;''n''.  For any range of ''ν'' of length ''N'', say ''ν''&nbsp;=&nbsp;''M''&nbsp;+&nbsp;1,&nbsp;...,&nbsp;''M''&nbsp;+&nbsp;''N'', there is some ''ν'' with |''s''<sub>''ν''</sub>| at least ''c''(''M'',&nbsp;''N'')|''s''<sub>0</sub>| where\n\n:<math> c(M,N) = \\left({ \\sum_{k=0}^{N-1} \\binom{M+k}{k} 2^k }\\right)^{-1} \\ . </math>\n\nThe sum here may be replaced by the weaker but simpler <math>\\left({ \\frac{N}{2e(M+N)} }\\right)^{N-1}</math>.\n\nWe may deduce the [[Fabry gap theorem]] from this result.\n\n==Turán's second theorem==\nThe second result applies to sums ''s''<sub>ν</sub> where <math>|z_n| \\le 1</math> for all&nbsp;''n''.  Assume that the ''z'' are ordered in decreasing absolute value and scaled so that |''z''<sub>1</sub>| = 1.  Then there is some ν with\n\n:<math> |s_\\nu| \\ge 2 \\left({ \\frac{N}{8e(M+N)} }\\right)^N \\min_{1\\le j\\le N} \\left\\vert{\\sum_{n=1}^j b_n }\\right\\vert \\ . </math>\n\n==See also==\n* [[Turán's theorem]] in graph theory\n\n==References==\n* {{cite book | last=Montgomery | first=Hugh L. | authorlink=Hugh Montgomery (mathematician) | title=Ten lectures on the interface between analytic number theory and harmonic analysis | series=Regional Conference Series in Mathematics | volume=84 | location=Providence, RI | publisher=[[American Mathematical Society]] | year=1994 | isbn=0-8218-0737-4 | zbl=0814.11001 }}\n\n{{DEFAULTSORT:Turan's method}}\n[[Category:Exponentials]]\n[[Category:Analytic number theory]]"
    },
    {
      "title": "Van der Corput's method",
      "url": "https://en.wikipedia.org/wiki/Van_der_Corput%27s_method",
      "text": "In mathematics, '''van der Corput's method''' generates estimates for [[exponential sum]]s.  The method applies two processes, the '''van der Corput processes A''' and '''B''' which relate the sums into simpler sums which are easier to estimate.\n\nThe processes apply to exponential sums of the form \n\n:<math> \\sum_{n=a}^b e(f(n)) \\  </math>\n\nwhere ''f'' is a sufficiently [[smooth function]] and ''e''(''x'') denotes exp(2πi''x'').\n\n==Process A==\nTo apply process A, write the first difference ''f''<sub>''h''</sub>(''x'') for ''f''(''x''+''h'')−''f''(''x'').\n\nAssume there is ''H'' ≤ ''b''−''a'' such that\n\n:<math> \\sum_{h=1}^H \\left\\vert{ \\sum_{n=a}^{b-h} e(f_h(n)) }\\right\\vert \\le b-a \\ . </math>\n\nThen\n\n:<math> \\left\\vert{ \\sum_{n=a}^b e(f(n)) }\\right\\vert \\ll \\frac{b-a}{\\sqrt H} \\ . </math>\n\n==Process B==\nProcess B transforms the sum involving ''f'' into one involving a function ''g'' defined in terms of the derivative of f.  Suppose that ''f''' is monotone increasing with ''f''<nowiki>'</nowiki>(''a'') = α, ''f''<nowiki>'</nowiki>(''b'') = β.  Then ''f''<nowiki>'</nowiki> is invertible on [α,β] with inverse ''u'' say.  Further suppose ''f''<nowiki>''</nowiki> ≥ λ > 0.  Write\n\n:<math> g(y) = f(u(y)) - y u(y) \\ . </math>\n\nWe have \n\n:<math> \\left\\vert{ \\sum_{n=a}^b e(f(n)) }\\right\\vert \\ll \\frac{1}{\\sqrt \\lambda} \\max_{\\alpha \\le \\gamma \\le \\beta} \\left\\vert{ \\sum_{\\nu=\\alpha}^\\gamma e(g(\\nu)) }\\right\\vert \\ . </math>\n\nApplying Process B again to the sum involving ''g'' returns to the sum over ''f'' and so yields no further information.\n\n==Exponent pairs==\nThe method of '''exponent pairs''' gives a class of estimates for functions with a particular smoothness property.  Fix parameters ''N'',''R'',''T'',''s'',δ.  We consider functions ''f'' defined on an interval [''N'',2''N''] which are ''R'' times continuously differentiable, satisfying\n\n:<math>\\left\\vert{ f^{(r+1)}(x) - (-1)^r s(s+1)\\cdots(s+r)T x^{-s-r} }\\right\\vert \\le \\delta s(s+1)\\cdots(s+r)T x^{-s-r} \\ </math>\n\nuniformly on [''a'',''b''] for 0 ≤ ''r'' < ''R''.\n\nWe say that a pair of real numbers (''k'',''l'') with 0 ≤ ''k'' ≤ 1/2 ≤ ''l'' ≤ 1 is an ''exponent pair'' if for each σ > 0 there exists δ and ''R'' depending on ''k'',''l'',σ such that\n\n:<math> \\left\\vert{ \\sum_{n=a}^b e(f(n)) }\\right\\vert \\ll \\left({\\frac{T}{N^\\sigma}}\\right)^k  N^l \\  </math>\n\nuniformly in ''f''.\n\nBy Process A we find that if (''k'',''l'') is an exponent pair then so is <math>\\left({\\frac{k}{2k+2},\\frac{k+l+1}{2k+2}}\\right)</math>.\nBy Process B we find that so is <math>\\left({l-1/2,k+1/2}\\right)</math>.\n\nA trivial bound shows that (0,1) is an exponent pair.\n\nThe set of exponents pairs is convex.  \n\nIt is known that if (''k'',''l'') is an exponent pair then the [[Riemann zeta function]] on the [[critical line (mathematics)|critical line]] satisfies\n\n<math> \\zeta(1/2+i t) \\ll t^\\theta \\log t </math>\n\nwhere <math>\\theta = (k+l-1/2)/2</math>.\n\nThe '''exponent pair conjecture''' states that for all ε > 0, the pair (ε,1/2+ε) is an exponent pair.  This conjecture implies the [[Lindelöf hypothesis]].\n\n==References==\n* {{cite book | last=Ivić | first=Aleksandar | title=The Riemann zeta-function. The theory of the Riemann zeta-function with applications | location=New York etc. | publisher=John Wiley & Sons | year=1985 | isbn=0-471-80634-X | zbl=0556.10026 }}\n* {{cite book | last=Montgomery | first=Hugh L. | authorlink=Hugh Montgomery (mathematician) | title=Ten lectures on the interface between analytic number theory and harmonic analysis | series=Regional Conference Series in Mathematics | volume=84 | location=Providence, RI | publisher=[[American Mathematical Society]] | year=1994 | isbn=0-8218-0737-4 | zbl=0814.11001 }}\n* {{cite book | editor1-last=Sándor | editor1-first=József | editor2-last=Mitrinović | editor2-first=Dragoslav S. | editor3-last=Crstici |editor3-first=Borislav | title=Handbook of number theory I | location=Dordrecht | publisher=[[Springer-Verlag]] | year=2006 | isbn=1-4020-4215-9 | zbl=1151.11300 }}\n\n[[Category:Exponentials]]\n[[Category:Analytic number theory]]"
    },
    {
      "title": "Volatility tax",
      "url": "https://en.wikipedia.org/wiki/Volatility_tax",
      "text": "{{sources exist|date=March 2018}}\n\nThe '''volatility tax''' is a [[mathematical finance]] term, formalized by [[hedge fund]] manager [[Mark Spitznagel]], describing the effect of large investment losses (or [[Volatility (finance)|volatility]]) on [[Compound interest|compound returns]].<ref name=\"VolTax1\">[http://www.pionline.com/article/20171120/ONLINE/171119874/commentary-not-all-risk-mitigation-is-created-equal Not all risk mitigation is created equal], ''Pensions & Investments'', November 20, 2017</ref> It has also been called “volatility drag”.<ref>https://blogs.cfainstitute.org/investor/2015/03/23/the-myth-of-volatility-drag-part-1/</ref>\n\n==Overview==\nAs Spitznagel wrote:\n{{quote|It is well known that steep portfolio losses crush long-run [[Compound annual growth rate|compound annual growth rates (CAGRs)]]. It just takes too long to recover from a much lower starting point: lose 50% and you need to make 100% to get back to even. I call this cost that transforms, in this case, a portfolio’s +25% average arithmetic return into a zero CAGR (and hence leaves the portfolio with zero profit) the “volatility tax”: it is a hidden, deceptive fee levied on investors by the negative compounding of the markets’ swings.<ref name=\"VolTax1\"/>}}\n\nQuantitatively, the volatility tax is the difference between the [[Arithmetic mean|arithmetic]] and [[Geometric mean|geometric average]] (or “[[ensemble average]]” and “time average”) returns of an asset or portfolio. It thus represents the degree of “[[Ergodic process|non-ergodicity]]” of the geometric average.\n\nStandard quantitative finance assumes that a portfolio’s [[net asset value]] changes follow a [[geometric Brownian motion]] (and thus are [[Log-normal distribution|log-normally distributed]]) with arithmetic average return (or “[[Stochastic drift|drift]]”) <math>\\mu</math>, [[standard deviation]] (or “volatility”) <math>\\sigma</math>, and geometric average return\n\n:<math>\\mu-\\sigma^2/2</math>\n\nSo the geometric average return is the difference between the arithmetic average return and a function of volatility. This function of volatility\n\n:<math>\\sigma^2/2</math>\n\nrepresents the volatility tax. (Though this is under the assumption of log-normality, the volatility tax is in fact independent of the assumed or actual return distribution.)\n\nThe mathematics behind the volatility tax is such that a very large portfolio loss has a disproportionate impact on the volatility tax that it pays and, as Spitznagel wrote, this is why the most effective risk mitigation focuses on large losses:\n{{quote|We can see how this works by considering that the compound (or geometric) average return is mathematically just the average of the [[logarithms]] of the arithmetic price changes. Because the logarithm is a [[concave function]] (it curves down), it increasingly penalizes negative arithmetic returns the more negative they are, and thus the more negative they are, the more they lower the compound average relative to the arithmetic average—and raise the volatility tax.<ref name=\"VolTax3\">[http://www.pionline.com/article/20180309/ONLINE/180309846/commentary-thanks-to-volatility-you-cant-always-get-what-you-want-in-investing Thanks to volatility, you can’t always get what you want in investing], ''Pensions & Investments'', March 9, 2018</ref>}}\n\nAccording to Spitznagel, the goal of risk mitigation strategies is to solve this “vexing non-ergodicity, volatility tax problem” and thus raise a portfolio’s geometric average return, or CAGR, by lowering its volatility tax (and “narrow the gap between our ensemble and time averages”).<ref name=\"VolTax3\"/> This is “the very name of the game in successful investing. It is the key to the kingdom, and explains in a nutshell [[Warren Buffett]]’s cardinal rule, ‘Don’t lose money.’”<ref name=\"VolTax2\">[https://www.scribd.com/document/370967187/Universa-Mark-Spitznagel-Volatility-Tax ''The Volatility Tax''], Universa Investments, February 2018</ref> Moreover, “the good news is the entire hedge fund industry basically exists to help with this—to help save on volatility taxes paid by portfolios. The bad news is they haven't done that, not at all.”<ref name=\"VolTax3\"/>\n\nAs [[Nassim Nicholas Taleb]] wrote in his 2018 book ''[[Skin in the Game (book)|Skin in the Game]]'', “more than two decades ago, practitioners such as Mark Spitznagel and myself built our entire business careers around the effect of the difference between ensemble and time.”<ref>{{cite book|last1=Taleb|first1=Nassim Nicholas|title=Skin in the Game: Hidden Asymmetries in Daily Life|date=2018|publisher=[[Random House]]|isbn=9780425284629|language=en}}</ref>\n\n== See also ==\n* [[Annual growth %]]\n* [[Arithmetic mean]]\n* [[Compound interest]]\n* [[Exponential growth]]\n* [[Geometric Brownian motion]]\n* [[Geometric mean]]\n* [[Log-normal distribution]]\n* [[Mathematical finance]]\n* [[Rate of return]]\n\n==References==\n{{reflist}}\n\n[[Category:Interest]]\n[[Category:Mathematical finance]]\n[[Category:Exponentials]]\n[[Category:Risk management]]"
    },
    {
      "title": "Wheat and chessboard problem",
      "url": "https://en.wikipedia.org/wiki/Wheat_and_chessboard_problem",
      "text": "[[File:Wheat and chessboard problem.jpg|thumb|By the time that the fifth square is reached on the chessboard, the board contains a total of 31, or <math display=\"inline\">2^5 - 1</math>, grains of wheat.]]\n\nThe '''wheat and chessboard problem''' (sometimes expressed in terms of rice grains) is a [[mathematical problem]] expressed in [[word problem (mathematics education)|textual form]] as:\n\n{{quote|If a [[chessboard]] were to have [[wheat]] placed upon each square such that one grain were placed on the first square, two on the second, four on the third, and so on (doubling the number of grains on each subsequent square), how many grains of wheat would be on the chessboard at the finish?|sign=|source=}}\n\nThe problem may be solved using simple [[addition]]. With 64 squares on a chessboard, if the number of grains doubles on successive squares, then the sum of grains on all 64 squares is: 1&nbsp;+&nbsp;2&nbsp;+&nbsp;4&nbsp;+&nbsp;8&nbsp;+&nbsp;... and so forth for the 64 squares. The total number of grains equals 18,446,744,073,709,551,615<!-- if you think this is wrong, please read the article's \"Solution\" section and double-check before changing it -->, much higher than most expect.\n\nThis exercise can be used to demonstrate how quickly exponential sequences grow, as well as to introduce exponents, zero power, capital-sigma notation and [[geometric series]]. Updated for modern times using pennies and the hypothetical question, \"Would you rather have a million dollars or the sum of a penny doubled every day for a month?\", the formula has been used to explain compounded interest. (In this case, the total value of the resulting pennies would surpass two million dollars in February or ten million dollars in other months.)<ref>{{cite web|url=https://www.bloomberg.com/news/videos/b/92966fc7-c54d-4405-8fa6-cbefd05bbd6f|title=A Penny Doubled Every Day for 30 Days = $10.7M|publisher=|via=www.bloomberg.com}}</ref><ref>{{cite web|url=http://mathforum.org/dr.math/faq/faq.doubling.pennies.html|title=Doubling Pennies |website=Mathforum.org|accessdate=2017-08-09}}</ref>\n\n== Origins ==\nThe problem appears in different stories about the invention of [[chess]]. One of them includes the geometric progression problem. The story is first known to have been recorded in 1256 by [[Ibn Khallikan]].<ref>Clifford A. Pickover (2009), ''The Math Book: From Pythagoras to the 57th Dimension'', New York : Sterling. {{isbn|9781402757969}}. p. 102</ref> Another version has the inventor of chess (in some tellings [[Sessa (chaturanga)|Sessa]], an [[History of India|ancient Indian Minister]]) request his ruler give him wheat according to the wheat and chessboard problem. The ruler laughs it off as a meager prize for a brilliant invention, only to have court treasurers report the unexpectedly huge number of wheat grains would outstrip the ruler's resources. Versions differ as to whether the inventor becomes a high-ranking advisor or is executed.<ref>{{cite book|last1=Tahan|first1=Malba|title=The Man Who Counted: A Collection of Mathematical Adventures|date=1993|publisher=W.W. Norton & Co.|location=New York|isbn=0393309347|pages=113–115|url={{Google books|WMv_2aSlXOoC|page=113|plainurl=yes}}|accessdate=2015-04-05}}</ref>\n\nMacdonnell also investigates the earlier development of the theme.<ref>{{cite journal|last1=Macdonell|first1=A. A.|title=Art. XIII.—The Origin and Early History of Chess|journal=Journal of the Royal Asiatic Society of Great Britain & Ireland|date=2011-03-15|volume=30|issue=01|pages=117–141|doi=10.1017/S0035869X00146246|url=https://journals.cambridge.org/abstract_S0035869X00146246|accessdate=2015-04-05}}</ref>\n\n:[According to al-Masudi's early history of India], shatranj, or chess was invented under an Indian king, who expressed his preference for this game over [[backgammon]]. [...] The Indians, he adds, also calculated an arithmetical progression with the squares of the chessboard. [...] The early fondness of the Indians for enormous calculations is well known to students of their mathematics, and is exemplified in the writings of the great astronomer Āryabaṭha (born 476&nbsp;A.D.). [...] An additional argument for the Indian origin of this calculation is supplied by the Arabic name for the square of the chessboard, (بيت, \"beit\"), 'house'. [...] For this has doubtless a historical connection with its Indian designation koṣṭhāgāra, 'store-house', 'granary' [...].\n\n\n== Solutions ==\n\nThe simple, brute-force solution is just to manually double and add each step of the series:\n\n: <math>T_{64} = 1 + 2 + 4 + \\cdots + 9,223,372,036,854,775,808 = 18,446,744,073,709,551,615 </math>\n\n::where <math>T_{64}</math> is the total number of grains.\n\nThe series may be expressed using exponents:\n\n: <math>T_{64} = 2^0 + 2^1 + 2^2 + \\cdots + 2^{63}</math>\n\nand, represented with capital-sigma notation as:\n\n:<math>\\sum_{i=0}^{63} 2^i.\\, </math>\n\nIt can also be solved much more easily using:\n\n: <math>T_{64} = 2^{64}- 1. \\, </math>\n\nA proof of which is:\n\n: <math>s = 2^0 + 2^1 + 2^2 + \\cdots + 2^{63}.</math>\n\nMultiply each side by 2:\n\n: <math>2s = 2^1 + 2^2 + 2^3 + \\cdots + 2^{63} + 2^{64}.</math>\n\nSubtract original series from each side:\n\n: <math>2s - s = 2^{64} - 2^0</math>\n\n: <math>\\therefore  s = 2^{64}- 1. \\, </math>\n\nThe solution above is a particular case of the sum of a geometric series, given by\n\n:<math>a + ar + a r^2 + a r^3 + \\cdots + a r^{n-1} = \\sum_{k=0}^{n-1} ar^k= a \\, \\frac{1-r^{n}}{1-r},</math>\n\nwhere <math>a</math> is the first term of the series, <math>r</math> is the common ratio and <math>n</math> is the number of terms.\n\nIn this problem  <math>a = 1</math>, <math>r = 2</math> and <math>n = 64</math>.\n\nThe exercise of working through this problem may be used to explain and demonstrate [[exponentiation|exponents]] and the quick growth of [[exponential growth|exponential]] and [[geometric growth|geometric]] sequences. It can also be used to illustrate [[capital-sigma notation|sigma notation]].\nWhen expressed as exponents, the [[geometric series]] is:  2<sup>0</sup>&nbsp;+&nbsp;2<sup>1</sup>&nbsp;+&nbsp;2<sup>2</sup>\n&nbsp;+&nbsp;2<sup>3</sup>&nbsp;+&nbsp;... and so forth, up to 2<sup>63</sup>. The base of each exponentiation, \"2\", expresses the doubling at each square, while the exponents represent the position of each square (0 for the first square, 1 for the second, etc.).\n\nThe number of grains is the 64th [[Mersenne number]].\n\n==Second half of the chessboard==\n[[Image:Wheat Chessboard with line.svg|thumb|right|alt=A chessboard with each square labeled with the number of wheat grains according to the problem. A red line divides the chessboard in half. |An illustration of [[Ray Kurzweil]]'s second half of the chessboard principle. The letters are abbreviations for the SI [[metric prefix]]es.]]\n\nIn [[technology strategy]], the \"second half of the chessboard\" is a phrase, coined by [[Raymond Kurzweil|Ray Kurzweil]],<ref>{{cite book|last1=Kurzweil|first1=Ray|title=The Age of Spiritual Machines: When Computers Exceed Human Intelligence|date=1999|publisher=Penguin|location=New York|isbn=0-670-88217-8|page=37|url={{Google books|ldAGcyh0bkUC|page=637|plainurl=yes}}|accessdate=2015-04-06}}</ref> in reference to the point where an [[exponential growth|exponentially growing]] factor begins to have a significant economic impact on an organization's overall business strategy. While the number of grains on the first half of the chessboard is large, the amount on the second half is vastly (2<sup>32</sup> > 4 billion times) larger.\n\nThe number of grains of wheat on the first half of the chessboard is {{nowrap|1 + 2 + 4 + 8 + ... + 2,147,483,648}}, for a total of 4,294,967,295 (2<sup>32</sup>&nbsp;&minus;&nbsp;1) grains, or about 279&nbsp;tonnes of wheat (assuming 65&nbsp;mg as the mass of one grain of wheat).<ref>{{cite web |url=https://www.britannica.com/science/grain-unit-of-weight |title=Encyclopedia Britannica: Grain, unit of weight |date=29 April 2004 |accessdate=2 March 2017 }}</ref>\n\nThe number of grains of wheat on the ''second'' half of the chessboard is {{nowrap|2<sup>32</sup> + 2<sup>33</sup> + 2<sup>34</sup> + ... + 2<sup>63</sup>}}, for a total of 2<sup>64</sup>&nbsp;&minus;&nbsp;2<sup>32</sup> grains. This is equal to the square of the number of grains on the first half of the board, plus itself. The first square of the second half alone contains more grains than the entire first half. On the 64th square of the chessboard alone there would be 2<sup>63</sup> = 9,223,372,036,854,775,808 grains, more than two billion times as many as on the first half of the chessboard.<!-- 9,223,372,036,854,780,000 ÷ 4,294,967,295 = 2,147,483,649-->\n\nOn the entire chessboard there would be 2<sup>64</sup>&nbsp;&minus;&nbsp;1 = 18,446,744,073,709,551,615 grains of wheat, weighing about 1,199,000,000,000 [[metric tons]]. This is about 1,645 times the [[International wheat production statistics|global production of wheat]] in 2014 (729,000,000 metric tons).<ref>{{Cite web|title = FAOSTAT |url=http://faostat3.fao.org |website = faostat3.fao.org|access-date = 2 March 2017}}</ref>\n\n==Use==\n\n[[Carl Sagan]] titled the second chapter of [[Billions and Billions|his final book]] ''The Persian Chessboard'' and wrote that when referring to bacteria, \"Exponentials can't go on forever, because they will gobble up everything.\"<ref>{{cite book|last1=Sagan|first1=Carl|title=Billions and Billions: Thoughts On Life And Death At the Brink Of The Millennium|date=1997|publisher=Ballantine Books|location=New York|isbn=0-345-37918-7|page=17}}</ref> Similarly, ''[[The Limits to Growth]]'' uses the story to present suggested consequences of exponential growth: \"Exponential growth never can go on very long in a finite space with finite resources.\"<ref>Meadows, Donella H., Dennis L. Meadows, Jørgen Randers, and William W. Behrens III (1972). {{Google books|QRyQiINGW6oC|The Limits to Growth|page=21}}. New York: University Books. {{isbn|0-87663-165-0}}. Retrieved 2015-04-05.</ref>\n\n==See also==\n* [[Malthusian growth model]]\n* [[Moore's law]]\n* [[Technology strategy]]\n* [[Orders of magnitude (data)]]\n* [[Ambalappuzha_Sri_Krishna_Temple#Legend_of_the_Ambalappuzha_Paal_Payasam|Legend of the Ambalappuzha Paal Payasam]]\n\n==References==\n{{Reflist|35em}}\n\n==External links==\n{{wiktionary}}\n* {{MathWorld | urlname=WheatandChessboardProblem | title=Wheat and Chessboard Problem}}\n* [http://mathforum.org/~sanders/geometry/GP11Fable.html One telling of the fable]\n* [https://web.archive.org/web/20110707192804/http://www.averypickford.com/Third/salt.htm Salt and chessboard problem] - A variation on the wheat and chessboard problem with measurements of each square.\n\n[[Category:Mathematical chess problems]]\n[[Category:Exponentials]]\n[[Category:Wheat|Chess]]"
    },
    {
      "title": "Zero to the power of zero",
      "url": "https://en.wikipedia.org/wiki/Zero_to_the_power_of_zero",
      "text": "'''Zero to the power of zero''', denoted by '''0<sup>0</sup>''', is a [[Expression (mathematics)|mathematical expression]] with no agreed-upon [[Value (mathematics)|value]]. The most common possibilities are 1 or leaving the expression undefined, with justifications existing for each, depending on context.\nIn [[algebra]], [[combinatorics]], or [[set theory]], the generally agreed upon value is&nbsp;{{nowrap|1=0<sup>0</sup> = 1}}, whereas in [[mathematical analysis]], the expression is generally left undefined. [[#Treatment on computers|Computer program]]s also have differing ways of handling this expression.\n\n==Discrete exponents==\nThere are many widely used formulas having terms involving [[natural number|natural-number]] exponents that require 0<sup>0</sup> to be evaluated to 1.  For example, regarding ''b''<sup>0</sup> as an [[empty product]] assigns it the value 1, even when {{math|1=''b'' = 0}}.  Alternatively, the [[Exponentiation#Combinatorial interpretation|combinatorial interpretation]] of ''b''<sup>0</sup> is the number of [[empty tuple]]s of elements from a set with ''b'' elements; there is exactly one empty tuple, even if {{math|1=''b'' = 0}}.  Equivalently, the [[Exponentiation#Over sets|set-theoretic interpretation]] of 0<sup>0</sup> is the number of functions from the empty set to the empty set; there is exactly one such function, the [[empty function]].<ref name=\"Bourbaki\">N. Bourbaki, Elements of Mathematics, Theory of Sets, Springer-Verlag, 2004, III.§3.5.</ref>\n\n==Polynomials and power series==\nLikewise, when working with [[polynomial]]s, it is often necessary to assign {{math|0<sup>0</sup>}} the value 1.  A polynomial is an expression of the form <math>a_0x^0+\\cdots+a_nx^n</math> where {{math|''x''}} is an indeterminate, and the coefficients <math>a_n</math> are real numbers (or, more generally, elements of some [[ring (mathematics)|ring]]).  The set of all real polynomials in {{math|''x''}} is denoted by <math>\\mathbb R[x]</math>.  Polynomials are added termwise, and multiplied by the applying the usual rules for exponents in the indeterminate {{math|''x''}} (see [[Cauchy product]]).  With these algebraic rules for manipulation, polynomials form a [[polynomial ring]].  The polynomial <math>x^0</math> is the [[identity element]] of the polynomial ring, meaning that it is the (unique) element such that the product of <math>x^0</math> with any polynomial <math>p(x)</math> is just <math>p(x)</math>.<ref>{{cite book|author=[[Nicolas Bourbaki]]|title=Algèbre|publisher=Springer|year=1970}}, §III.2 No. 9: \"L'unique monôme de degré 0 est l'élément unité de <math>A[(X_i)_{i\\in I}]</math>; on l'identifie souvent à l'élément unité 1 de {{math|''A''}}\".</ref>  Polynomials can be evaluated by specializing the indeterminate {{math|''x''}} to be a real number.  More precisely, for any given real number <math>x_0</math> there is a unique unital [[ring homomorphism]] <math>\\operatorname{ev}_{x_0}:\\mathbb R[x]\\to\\mathbb R</math> such that <math>\\operatorname{ev}_{x_0}(x^1)=x_0</math>.<ref>{{cite book|author=[[Nicolas Bourbaki]]|title=Algèbre|publisher=Springer|year=1970}}, §IV.1 No. 3.</ref>  This is called the ''evaluation homomorphism''.  Because it is a unital homomorphism, we have <math>\\operatorname{ev}_{x_0}(x^0) = 1.</math>  That is, <math>x^0=1</math> for all specializations of {{math|''x''}} to a real number (including zero).\n\nThis perspective is significant for many polynomial identities appearing in combinatorics.  For example, the [[binomial theorem]] <math>(1 + x)^n = \\sum_{k = 0}^n \\binom{n}{k} x^k</math> is not valid for {{math|1=''x'' = 0}} unless {{math|1=0<sup>0</sup> = 1}}.<ref>\"Some textbooks leave the quantity {{math|0<sup>0</sup>}} undefined, because the functions {{math|''x''<sup>0</sup>}} and {{math|0<sup>''x''</sup>}} have different limiting values when {{math|''x''}} decreases to 0. But this is a mistake. We must define {{math|1=''x''<sup>0</sup> = 1}}, for all {{math|''x''}}, if the binomial theorem is to be valid when {{math|1=''x'' = 0}}, {{math|1=''y'' = 0}}, and/or {{math|1=''x'' = −''y''}}. The binomial theorem is too important to be arbitrarily restricted! By contrast, the function {{math|0<sup>''x''</sup>}} is quite unimportant\".{{cite book|title=[[Concrete Mathematics]]|edition=1st|publisher=Addison Wesley Longman Publishing Co|date=1989-01-05|isbn=0-201-14236-8|author=[[Ronald Graham]], [[Donald Knuth]], and [[Oren Patashnik]]|page=162|chapter=Binomial coefficients}}</ref>  Similarly, rings of [[power series]] require <math>x^0=1</math> to be true for all specializations of {{math|''x''}}.  Thus identities like <math>\\frac{1}{1-x} = \\sum_{n=0}^{\\infty} x^n</math> and <math>e^{x} = \\sum_{n=0}^{\\infty} \\frac{x^n}{n!}</math> are only true as functional identities (including at {{math|1=''x'' = 0}}) if {{math|1=0<sup>0</sup> = 1}}.\n\nIn [[differential calculus]], the [[power rule]] <math>\\frac{d}{dx} x^n = nx^{n-1}</math> is not valid for {{math|1=''n'' = 1}} at {{math|1=''x'' = 0}} unless {{math|1=0<sup>0</sup> = 1}}.\n\n==Continuous exponents==\n[[Image:X^y.png|right|thumb|300px|Plot of {{math|1=''z'' = ''x''{{i sup|''y''}}}}. The red curves (with {{math|''z''}} constant) yield different limits as {{math|(''x'', ''y'')}} approaches {{math|(0, 0)}}. The green curves (of finite constant slope, {{math|1=''y'' = ''ax''}}) all yield a limit of 1.]]\n\nLimits involving algebraic operations can often be evaluated by replacing subexpressions by their limits; if the resulting expression does not determine the original limit, the expression is known as an [[indeterminate form]].<ref>{{cite book|first=S. C.|last=Malik |author2=Savita Arora|year=1992|title=Mathematical Analysis|page=223|isbn=978-81-224-0323-7|quote=In general the limit of {{math|''φ''(''x'')/''ψ''(''x'')}} when {{math|1=''x'' = ''a''}} in case the limits of both the functions exist is equal to the limit of the numerator divided by the denominator. But what happens when both limits are zero? The division ({{math|0/0}}) then becomes meaningless. A case like this is known as an indeterminate form. Other such forms are {{math|∞/∞}}, {{math|0 × ∞}}, {{math|∞ − ∞}}, {{math|0<sup>0</sup>}}, {{math|1<sup>∞</sup>}} and {{math|∞<sup>0</sup>}}.|publisher=Wiley|location=New York}}</ref> In fact, when {{math|''f''(''t'')}} and {{math|''g''(''t'')}} are real-valued functions both approaching {{math|0}} (as {{math|''t''}} approaches a real number or {{math|±∞}}), with {{math|''f''(''t'') > 0}}, the function {{math|''f''(''t'')<sup>''g''(''t'')</sup>}} need not approach {{math|1}}; depending on {{math|''f''}} and {{math|''g''}}, the limit of {{math|''f''(''t'')<sup>''g''(''t'')</sup>}} can be any non-negative real number or {{math|+∞}}, or it can [[limit (mathematics)|diverge]]. For example, the functions below are of the form {{math|''f''(''t'')<sup>''g''(''t'')</sup>}} with {{math|''f''(''t''), ''g''(''t'') → 0}} as {{math|''t'' → 0<sup>+</sup>}} (a [[one-sided limit]]), but the limits are different:\n::<math> \\lim_{t \\to 0^+} {t}^{t} = 1, \\quad \\lim_{t \\to 0^+} \\left(e^{-\\frac{1}{t^2}}\\right)^t = 0, \\quad \\lim_{t \\to 0^+} \\left(e^{-\\frac{1}{t^2}}\\right)^{-t} = +\\infty, \\quad \\lim_{t \\to 0^+} \\left(e^{-\\frac{1}{t}}\\right)^{at} = e^{-a}</math>.\n\nThus, the two-variable function {{math|''x''<sup>''y''</sup>}}, though continuous on the set {{math|{(''x'', ''y'') : ''x'' > 0},}} [[Classification of discontinuities#Essential discontinuity|cannot be extended]] to a [[continuous function]] on <math>\\{(x,y):x > 0\\} \\cup \\{(0, 0)\\}</math>, no matter how one chooses to define {{math|0<sup>0</sup>}}.<ref>{{cite journal|author=L. J. Paige|title=A note on indeterminate forms|jstor=2307224|journal=American Mathematical Monthly|volume=61|issue=3|date=March 1954|pages=189–190|doi=10.2307/2307224}}</ref> However, under certain conditions, such as when {{math|''f''}} and {{math|''g''}} are both [[analytic functions]] and {{math|''f''}} is positive on the open interval {{math|(0, ''b'')}} for some positive {{math|''b''}}, the limit approaching from the right is always {{math|1}}.<ref>[http://www.faqs.org/faqs/sci-math-faq/specialnumbers/0to0/ sci.math FAQ: What is 0^0?]</ref><ref>{{Cite journal | doi = 10.2307/2689754| jstor = 2689754  | pages = 41–42  | title = The Indeterminate Form {{math|0<sup>0</sup>}}  | journal = [[Mathematics Magazine]] | volume = 50  | issue = 1  | publisher = [[Mathematical Association of America]] | year = 1977 | author1 = Rotando | first1 = Louis M. | author2 = Korn | first2 = Henry }}</ref><ref>{{Cite journal | doi = 10.2307/3595845 | jstor = 3595845  | pages = 55–56  | title = On the Indeterminate Form {{math|0<sup>0</sup>}} | journal = [[The College Mathematics Journal]] | volume = 34  | issue = 1  | publisher = [[Mathematical Association of America]]  | year = 2003 | author1 = Lipkin | first1 = Leonard J.}}</ref>\n\n==Complex exponents==\nIn the [[complex domain]], the function {{math|''z''{{i sup|''w''}}}} may be defined for nonzero {{math|''z''}} by choosing a [[complex logarithm#Branches of the complex logarithm|branch]] of {{math|log ''z''}} and defining {{math|''z''{{i sup|''w''}}}} as {{math|''e''{{i sup|''w'' log ''z''}}}}. This does not define {{math|0<sup>''w''</sup>}} since there is no branch of {{math|log ''z''}} defined at {{math|1=''z'' = 0}}, let alone in a neighborhood of {{math|0}}.<ref>\"Since {{math|log(0)}} does not exist, {{math|0<sup>''z''</sup>}} is undefined. For {{math|Re(''z'') > 0}}, we define it arbitrarily as {{math|0}}.\" George F. Carrier, Max Krook and Carl E. Pearson, ''Functions of a Complex Variable: Theory and Technique '', 2005, p.&nbsp;15</ref><ref>\"For {{math|1=''z'' = 0}}, {{math|''w'' ≠ 0}}, we define {{math|1=0<sup>''w''</sup> = 0}}, while {{math|0<sup>0</sup>}} is not defined.\" Mario Gonzalez, ''Classical Complex Analysis'', Chapman & Hall, 1991, p.&nbsp;56.</ref><ref>\"... Let's start at {{math|1=''x'' = 0}}. Here {{math|''x''<sup>''x''</sup>}} is undefined.\"  Mark D. Meyerson, The {{math|''x''<sup>''x''</sup>}} Spindle, ''Mathematics Magazine'' '''69''', no. 3 (June 1996), 198-206.</ref>\n\n==History of differing points of view==\nThe debate over the definition of  <math>0^0</math> has been going on at least since the early 19th century. At that time, most mathematicians agreed that <math>0^0 = 1</math>, until in 1821 [[Cauchy]]<ref>Augustin-Louis Cauchy, ''Cours d'Analyse de l'École Royale Polytechnique'' (1821).  In his ''Oeuvres Complètes'', series 2, volume 3.</ref> listed <math>0^0</math> along with expressions like <math>\\textstyle \\frac{0}{0}</math> in a [[Indeterminate form#List of indeterminate forms|table of indeterminate forms]]. In the 1830s Libri<ref>Guillaume Libri, Note sur les valeurs de la fonction 0<sup>0<sup>x</sup></sup>, ''[[Crelle's Journal|Journal für die reine und angewandte Mathematik]]'' '''6''' (1830), 67–72.</ref><ref>Guillaume Libri, Mémoire sur les fonctions discontinues, ''Journal für die reine und angewandte Mathematik'' '''10''' (1833), 303–316.</ref> published an unconvincing argument for <math>0^0 = 1</math>, and [[August Ferdinand Möbius|Möbius]]<ref name=\"Möbius1834\">{{cite journal |author =  A. F. Möbius |title = Beweis der Gleichung 0<sup>0</sup> = 1, nach J. F. Pfaff |trans-title = Proof of the equation 0<sup>0</sup> = 1, according to J. F. Pfaff|url = http://babel.hathitrust.org/cgi/pt?id=mdp.39015036988163;view=1up;seq=142 |journal = Journal für die reine und angewandte Mathematik |volume = 12 |year = 1834 |pages = 134–136}}</ref> sided with him, erroneously claiming that <math>\\textstyle \\lim_{t \\to 0^+} f(t)^{g(t)} \\;=\\; 1</math> whenever <math>\\textstyle \\lim_{t \\to 0^+} f(t) \\;=\\; \\lim_{t \\to 0^+} g(t) \\;=\\; 0</math>. A commentator who signed his name simply as \"S\" provided the counterexample of <math>\\textstyle (e^{-1/t})^t</math>, and this quieted the debate for some time. More historical details can be found in Knuth (1992).<ref name=\"Knuth1992\" />\n\nMore recent authors interpret the situation above in different ways:\n* Some argue that the best value for <math>0^0</math> depends on context, and hence that [[defined and undefined|defining]] it once and for all is problematic.<ref>Examples include Edwards and Penny (1994). ''Calculus'', 4th ed, Prentice-Hall, p. 466, and Keedy, Bittinger, and Smith (1982). ''Algebra Two.'' Addison-Wesley, p. 32.</ref>  According to Benson (1999), \"The choice whether to define <math>0^0</math> is based on convenience, not on correctness. If we refrain from defining <math>0^0</math>, then certain assertions become unnecessarily awkward. [...] The consensus is to use the definition <math>0^0=1</math>, although there are textbooks that refrain from defining {{nowrap begin}}<math>0^0</math>.\"<ref>Donald C. Benson, ''The Moment of Proof : Mathematical Epiphanies.'' New York Oxford University Press (UK), 1999. {{ISBN|978-0-19-511721-9}}</ref>{{nowrap end}}\n* Others argue that <math>0^0</math> should be defined as 1. [[Donald Knuth|Knuth]] (1992) contends strongly that <math>0^0</math> \"''has'' to be 1\", drawing a distinction between the ''value'' <math>0^0</math>, which should equal 1 as advocated by Libri, and the ''limiting form'' <math>0^0</math> (an abbreviation for a limit of <math>\\textstyle f(x)^{g(x)}</math> where <math>\\textstyle f(x), g(x) \\to 0</math>), which is necessarily an indeterminate form as listed by Cauchy: \"Both Cauchy and Libri were right, but Libri and his defenders did not understand why truth was on their side.\"<ref name=\"Knuth1992\">Donald E. Knuth, Two notes on notation, ''[[American Mathematical Monthly|Amer. Math. Monthly]]'' '''99''' no. 5 (May 1992), 403–422 ([https://arxiv.org/abs/math/9205211 arXiv:math/9205211 &#91;math.HO&#93;]).</ref>\n\n==Treatment on computers==\n\n===IEEE floating-point standard===\nThe [[IEEE 754-2008]] floating-point standard is used in the design of most floating-point libraries. It recommends a number of operations for computing a power:<ref name=\"Muller_2010\">{{cite book |author-last1=Muller |author-first1=Jean-Michel |author-last2=Brisebarre |author-first2=Nicolas |author-last3=de Dinechin |author-first3=Florent |author-last4=Jeannerod |author-first4=Claude-Pierre |author-last5=Lefèvre |author-first5=Vincent |author-last6=Melquiond |author-first6=Guillaume |author-last7=Revol |author-first7=Nathalie |author-last8=Stehlé |author-first8=Damien |author-last9=Torres |author-first9=Serge |title=Handbook of Floating-Point Arithmetic |year=2010 |publisher=[[Birkhäuser]] |edition=1 |doi=10.1007/978-0-8176-4705-6 |lccn=2009939668|page=216}}  {{isbn|978-0-8176-4705-6}} (online), {{isbn|0-8176-4704-X}} (print)</ref> \n*<tt>pow</tt> treats 0<sup>0</sup> as 1. If the power is an exact integer the result is the same as for <tt>pown</tt>, otherwise the result is as for <tt>powr</tt> (except for some exceptional cases).\n*<tt>pown</tt> treats 0<sup>0</sup> as 1. The power must be an exact integer. The value is defined for negative bases; e.g., <tt>pown(−3,5)</tt> is −243.\n*<tt>powr</tt> treats 0<sup>0</sup> as [[NaN]] (Not-a-Number – undefined). The value is also NaN for cases like <tt>powr(−3,2)</tt> where the base is less than zero. The value is defined by ''e''<sup>power×log(base)</sup>.\n\nThe <tt>pow</tt> variant is inspired by the <tt>pow</tt> function from [[C99]], mainly for compatibility.<ref>{{cite web |url=http://grouper.ieee.org/groups/754/email/msg03270.html |title=More transcendental questions |website=grouper.ieee.org |archive-url=https://web.archive.org/web/20171114040441/http://grouper.ieee.org/groups/754/email/msg03270.html |archive-date=2017-11-14 |access-date=2019-05-27}} (Beginning of the discussion about the power functions for the revision of the IEEE 754 standard, May 2007.)</ref> It is useful mostly for languages with a single power function. The <tt>pown</tt> and <tt>powr</tt> variants have been introduced due to conflicting usage of the power functions and the different points of view (as stated above).<ref>{{cite web |url=http://grouper.ieee.org/groups/754/email/msg03292.html |title=Re: A vague specification |website=grouper.ieee.org |archive-url=https://web.archive.org/web/20171114040300/http://grouper.ieee.org/groups/754/email/msg03292.html |archive-date=2017-11-14 |access-date=2019-05-27}} (Suggestion of variants in the discussion about the power functions for the revision of the IEEE 754 standard, May 2007.)</ref>\n\n===Programming languages===\nThe C and C++ standards do not specify the result of 0<sup>0</sup> (a domain error may occur), but as of [[C99]], if the [[normative]] annex&nbsp;F is supported, the result is required to be 1 because this value is more useful than [[NaN]] for significant applications<ref>{{cite journal|title=Rationale for International Standard—Programming Languages—C|version=Revision 5.10|date=April 2003|author=John Benito|url=http://www.open-std.org/jtc1/sc22/wg14/www/C99RationaleV5.10.pdf|page=182|format=PDF}}</ref> (for instance, with [[#Discrete_exponents|discrete exponents]]). The [[Java (programming language)|Java]] standard<ref>{{cite web|url=http://docs.oracle.com/javase/8/docs/api/java/lang/Math.html#pow-double-double- |title=Math (Java Platform SE 8) pow |publisher=Oracle}}</ref> and the [[.NET Framework]] [[method (computer science)|method]] <code>System.Math.Pow</code><ref>{{cite web |url=http://msdn.microsoft.com/en-us/library/system.math.pow.aspx |title=.NET Framework Class Library Math.Pow Method |publisher=Microsoft}}</ref> also treat 0<sup>0</sup> as 1. Some languages document that their exponentiation operation corresponds to the <code>pow</code> function from the [[C mathematical functions|C mathematical library]]; this is the case of [[Lua (programming language)|Lua]]<ref>{{cite web |url=https://www.lua.org/manual/5.3/manual.html |title=Lua 5.3 Reference Manual |access-date=2019-05-27}}</ref> and [[Perl]]'s <code>**</code> operator<ref>{{cite web |url=https://perldoc.perl.org/perlop.html#Exponentiation |title=perlop – Exponentiation |access-date=2019-05-27}}</ref> (where it is explicitly mentioned that the result of <code>0**0</code> is platform-dependent).\n\n===Mathematical and scientific software===\n*[[SageMath]] simplifies ''b''<sup>0</sup> to 1,  even if no constraints are placed on ''b''. It takes 0<sup>0</sup> to be 1, but does not simplify 0<sup>''x''</sup> for other ''x''.{{cn|date=June 2018}}\n*[[Maple (software)|Maple]] distinguishes between ''integers'' 0, 1, ... and the corresponding ''[[floating point|floats]]'' 0.0, 1.0, ... (usually denoted 0., 1., ...). If ''x'' does not evaluate to a number, then ''x''<sup>0</sup> and ''x''<sup>0.0</sup> are respectively evaluated to 1 (''integer'') and 1.0 (''float''); on the other hand, 0<sup>''x''</sup> is evaluated to the integer 0, while 0.0<sup>''x''</sup> is evaluated as 0.<sup>''x''</sup>. If both the base and the exponent are zero (or are evaluated to zero), the result is ''Float(undefined)'' if the exponent is the ''float'' 0.0; with an integer as exponent, the evaluation of 0<sup>0</sup> results in the ''integer'' 1, while that of 0.<sup>0</sup> results in the ''float'' 1.0.{{cn|date=June 2018}}\n*[[Macsyma]] also simplifies ''b''<sup>0</sup> to 1 even if no constraints are placed on ''b'', but issues an error for 0<sup>0</sup>. For ''x'' > 0, it simplifies 0<sup>''x''</sup> to 0. {{Citation needed|date=July 2010}}\n*[[Mathematica]] distinguishes between integers as exact, and floating point numbers as approximate. It simplifies ''b''<sup>0</sup> to 1, even if no constraints are placed on ''b'', does not simplify 0<sup>''x''</sup>, and takes 0<sup>0</sup> to be \"Indeterminate\".<ref>{{cite web |url=http://reference.wolfram.com/language/ref/Power.html |title=Wolfram Language & System Documentation: Power  |publisher=Wolfram| access-date =August 2, 2018}}</ref>\n*[[PARI/GP]] also distinguishes between integer and floating-point types: When the exponent 0 is of integer type, the expression, such as 0^0 or 0.^0, is simplified to 1.<ref>{{cite web |url=http://pari.math.u-bordeaux.fr/cgi-bin/gitweb.cgi?p=pari.git;a=commitdiff;h=c2fac9a15 |title=pari.git / commitdiff – 10- x ^ t_FRAC: return an exact result if possible; e.g. 4^(1/2) is now 2 |access-date=September 10, 2018}}</ref> When the exponent is not of integer type, PARI/GP treats ''x''^''n'' as the transcendental function <math>e^{n \\log x}</math>, so that 0^0. yields a domain error.<ref>{{cite web |url=https://pari.math.u-bordeaux.fr/pub/pari/manuals/2.11.0/users.pdf |title=Users' Guide to PARI/GP (version 2.11.0) |author=The PARI Group |format=PDF |pages=10,122 |year=2018 |quote=\"There is also the exponentiation operator ^, when the exponent is of type integer; otherwise, it is considered as a transcendental function. [...] If the exponent ''n'' is an integer, then exact operations are performed using binary (left-shift) powering techniques. [...] If the exponent ''n'' is not an integer, powering is treated as the transcendental function exp(''n'' log ''x'').\" |access-date=September 4, 2018}}</ref>\n*[[MATLAB|Matlab]], [[Python (programming language)|Python]], [[Magma computer algebra system|Magma]], [[GAP computer algebra system|GAP]], [[SINGULAR|singular]], and [[GNU Octave]] evaluate 0<sup>0</sup> as 1.{{cn|date=June 2018}}\n*[[Microsoft Excel]] treats =0^0 as #NUM!.<ref>Microsoft Docs, [https://docs.microsoft.com/en-us/openspecs/office_standards/ms-oe376/11087d65-0fe1-44c6-bbeb-61380bc34bad \"{{bracket|MS-OE376}}: Part 4 Section 3.17.7.252, POWER\"]</ref>\n\n==References==\n\n{{reflist}}\n\n==External links==\n* [http://www.faqs.org/faqs/sci-math-faq/specialnumbers/0to0/ sci.math FAQ: What is 0<sup>0</sup>?]\n* [http://www.askamathematician.com/?p=4524 What does 0^0 (zero to the zeroth power) equal?] on AskAMathematician.com\n\n[[Category:Exponentials]]\n[[Category:Mathematical analysis]]\n[[Category:Computer arithmetic]]\n[[Category:Computer errors]]\n[[Category:0 (number)]]\n[[Category:Software anomalies]]"
    },
    {
      "title": "Faddeeva function",
      "url": "https://en.wikipedia.org/wiki/Faddeeva_function",
      "text": "The '''Faddeeva function''' or '''Kramp function''' is a scaled complex complementary [[error function]],\n:<math>w(z):=e^{-z^2}\\operatorname{erfc}(-iz) = \\operatorname{erfcx}(-iz)\n  =e^{-z^2}\\left(1+\\frac{2i}{\\sqrt{\\pi}}\\int_0^z e^{t^2}\\text{d}t\\right).</math>\nIt is related to the [[Fresnel integral]], to [[Dawson's integral]], and to the [[Voigt function]].\n\nThe function arises in various physical problems in describing electromagnetic response in complicated media.\n* problems involving small-amplitude waves propagating through [[Maxwell–Boltzmann distribution|Maxwellian]] [[Plasma (physics)|plasmas]], and in particular appears in the plasma's [[permittivity]] from which [[dispersion relation]]s are derived, hence it is sometimes referred to as the '''plasma dispersion function'''<ref name=\"errorfun\">http://nlpc.stanford.edu/nleht/Science/reference/errorfun.pdf</ref><ref name=Zaghloul11/> (although this name is sometimes used instead for the rescaled function <math>Z(z)=i\\sqrt{\\pi}w(z)</math> defined by Fried and Conte<ref name=\"errorfun\" /><ref>[http://farside.ph.utexas.edu/ Richard Fitzpatrick], [http://farside.ph.utexas.edu/teaching/plasma/lectures/node87.html Plasma Dispersion Function], ''[http://farside.ph.utexas.edu/teaching/plasma/plasma.html Plasma Physics]'' lecture notes, University of Texas at Austin (2011/3/31).</ref>).\n* the infrared [[permittivity]] functions of amorphous oxides have resonances (due to [[phonon]]s) that are sometimes too complicated to fit using simple harmonic oscillators. The Brendel–Borman oscillator form uses an infinite superposition of oscillators having slightly different frequencies, with a Gaussian distribution.<ref name=\"BrendelBormann1992\">{{cite journal|last1=Brendel|first1=R.|last2=Bormann|first2=D.|title=An infrared dielectric function model for amorphous solids|journal=Journal of Applied Physics|volume=71|issue=1|year=1992|pages=1|issn=0021-8979|doi=10.1063/1.350737}}</ref> The integrated response can be written in terms of the Faddeeva function.\n* the Faddeeva function is also used in the analysis of electromagnetic waves of the type used in AM radio.{{citation needed|date=July 2016}} Groundwaves are verticaly polarised waves propagating over a lossy ground with finite resistivity and permittivity.\n\n== Properties ==\n\n=== Real and imaginary parts ===\n\nThe decomposition into real and imaginary parts is usually written\n:<math>w(x+iy)=V(x,y)+iL(x,y)</math>,\nwhere ''V'' and ''L'' are called the real and imaginary ''Voigt functions'', since ''V(x,y)'' is the [[Voigt profile]] (up to prefactors).\n\n=== Sign inversion ===\n\nFor sign-inverted arguments, the following both apply:\n:<math>w(-z)=2e^{-z^2} - w(z)</math>\nand\n:<math>w(-z)=w(z^*)^*</math>\nwhere * denotes complex conjugate.\n\n== Integral representation ==\n\nThe Faddeeva function occurs as\n:<math>w(z)=\\frac{i}{\\pi}\\int_{-\\infty}^{\\infty} \\frac{e^{- t^2}}{z - t} dt = \\frac{2iz}{\\pi} \\int_{0}^{\\infty} \\frac{e^{- t^2}}{z^2 - t^2} dt, \\qquad \\operatorname{Im}z > 0 </math>\nmeaning that it is a convolution of a Gaussian with a simple pole.\n\n==History==\n\nThe function was tabulated by [[Vera Faddeeva|Faddeeva]] and Terent'ev in 1954.<ref>[[Vera Faddeeva|V. N. Faddeeva]] and N. N. Terent'ev: Tables of values of the function <math>w(z)=\\exp(-z^2)(1+2i/\\sqrt{\\pi}\\int_0^z\\exp(t^2)\\text{d}t)</math> for complex argument. ''Gosud. Izdat. Teh.-Teor. Lit.'', Moscow, 1954; English transl., Pergamon Press, New York, 1961. Unverified citation, copied from Poppe and Wijers (1990).</ref> It appears as nameless function ''w(z)'' in [[Abramowitz and Stegun]] (1964), formula 7.1.3. The name ''Faddeeva function'' was apparently introduced by Poppe and Wijers in 1990;<ref>Earliest search result in Google Scholar as of Oct 2012.</ref> previously, it was known as Kramp's function (probably after [[Christian Kramp]]).<ref>For instance in Al'pert, Space Science Reviews 6, 781 (1967), formula (3.13), with reference to Faddeeva and Terent'ev.</ref>\n\nEarly implementations used methods by Gautschi (1969/70; [[ACM Transactions on Mathematical Software|ACM Algorithm]] 363)<ref>See references 3 and 4 in Poppe and Wijers (1990).</ref> or by Humlicek (1982).<ref>J. Humlicek, J. Quant. Spectrosc. Radiat. Transfer 27, 437-444 (1982).</ref> A more efficient algorithm was proposed by Poppe and Wijers (1990; ACM Algorithm 680).<ref>G. P. M. Poppe and C. M. J. Wijers, [[ACM Transactions on Mathematical Software]] 16, 38-46 (1990).</ref> Weideman (1994) proposed a particularly short algorithm that takes no more than eight lines of [[Matlab]] code.<ref>J. A. C. Weideman, SIAM J. Numer. Anal. 31, 1497-1518 (1994).</ref> Zaghloul and Ali pointed out deficiencies of previous algorithms and proposed a new one (2011; ACM Algorithm 916).<ref name=Zaghloul11>M. R. Zaghloul and A. N. Ali, ACM Transactions on Mathematical Software 38(2)15 (2011)</ref> Another algorithm has been proposed by Abrarov and Quine (2011/2012).<ref>S. M. Abrarov and B. M. Quine, Appl. Math. Comp. 218, 1894-1902 (2011) and arXiv:1205.1768v1 (2012).</ref>\n\n==Implementations==\n\nTwo software implementations, which are free for non-commercial use only,<ref>{{Cite web | url=http://www.acm.org/publications/policies/softwarecrnotice | title=Software Copyright Notice}}; hence they are not ''free'' in the sense of [[free and open-source software]]</ref> were published in [[ACM Transactions on Mathematical Software]] (TOMS) as Algorithm 680 (in [[Fortran]],<ref>http://www.cs.kent.ac.uk/people/staff/trh/CALGO/680.gz</ref> later translated into [[C (programming language)|C]]<ref>http://spec.jpl.nasa.gov/ftp/pub/calpgm/collisions/ZWOFZ.C</ref>) and Algorithm 916 by Zaghloul and Ali (in [[MATLAB]]).<ref>Mofreh R. Zaghloul and Ahmed N. Ali, \"[https://dx.doi.org/10.1145/2049673.2049679 Algorithm 916: Computing the Faddeyeva and Voigt Functions],\" ''ACM Trans. Math. Soft.'' '''38''' (2), 15 (2011).  Preprint available at [https://arxiv.org/abs/1106.0151 arXiv:1106.0151].</ref>\n\nA [[free and open source]] C or C++ implementation derived from a combination of Algorithm 680 and Algorithm 916 (using different algorithms for different ''z'') is also available under the [[MIT License]],<ref name=Faddeeva_w>[http://ab-initio.mit.edu/Faddeeva Faddeeva Package], free/open-source C++ implementation, accessed 13 October 2012.</ref> , and is maintained as a library package ''libcerf''.<ref>{{Cite web | url=https://jugit.fz-juelich.de:mlz/libcerf | title=Libcerf &#91;MLZ Scientific Computing Group&#93;}}</ref>\nThis implementation is also available as a [[Plug-in (computing)|plug-in]] for Matlab,<ref name=Faddeeva_w/> [[GNU Octave]],<ref name=Faddeeva_w/> and in [[Python (programming language)|Python]] via [[Scipy]] as <code>scipy.special.wofz</code> (which was originally the TOMS 680 code, but was replaced due to copyright concerns<ref>{{Cite web | url=https://github.com/scipy/scipy/issues/2260 | title=SciPy's complex erf code is not free/open-source? (Trac #1741) · Issue #2260 · scipy/scipy}}</ref>).\n\n==References==\n{{Reflist}}\n\n[[Category:Gaussian function]]\n[[Category:Analytic functions]]"
    },
    {
      "title": "Gaussian blur",
      "url": "https://en.wikipedia.org/wiki/Gaussian_blur",
      "text": "[[File:Cappadocia Gaussian Blur.svg|thumb|250px|The difference between a small and large Gaussian blur]]\n\nIn [[image processing]], a '''Gaussian blur''' (also known as '''Gaussian smoothing''') is the result of blurring an image by a [[Gaussian function]] (named after mathematician and scientist [[Carl Friedrich Gauss]]). It is a widely used effect in graphics software,  typically to reduce [[image noise]] and reduce detail. The visual effect of this blurring technique is a smooth blur resembling that of viewing the [[image]] through a translucent screen, distinctly different from the [[bokeh]] effect produced by an out-of-focus lens or the shadow of an object under usual illumination. Gaussian smoothing is also used as a pre-processing stage in [[computer vision]] algorithms in order to enhance image structures at different scales—see [[scale space representation]] and [[scale space implementation]].\n\nMathematically, applying a Gaussian blur to an image is the same as [[convolution|convolving]] the image with a [[Gaussian function]]. This is also known as a two-dimensional [[Weierstrass transform]]. By contrast, convolving by a circle (i.e., a circular [[box blur]]) would more accurately reproduce the [[bokeh]] effect. Since the [[Fourier transform]] of a Gaussian is another Gaussian, applying a Gaussian blur has the effect of reducing the image's high-frequency components; a Gaussian blur is thus a [[low pass filter]].\n\n==Mathematics==\n[[File:Halftone, Gaussian Blur.jpg|thumb|right|A [[halftone]] print rendered smooth through Gaussian blur]]\nThe Gaussian blur is a type of image-blurring filter that uses a Gaussian function (which also expresses the [[normal distribution]] in statistics) for calculating the [[transformation (mathematics)|transformation]] to apply to each [[pixel]] in the image. The formula of a Gaussian function in one dimension is\n\n:<math>G(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{x^2}{2 \\sigma^2}}</math>\n\nIn two dimensions, it is the product of two such Gaussian functions, one in each dimension:\n\n:<math>G(x,y) = \\frac{1}{{2\\pi \\sigma^2}} e^{-\\frac{x^2 + y^2}{2 \\sigma^2}}</math><ref name=\"ShapiroStockman\">[[Linda Shapiro|Shapiro, L. G.]] &amp; Stockman, G. C: \"Computer Vision\", page 137, 150. Prentice Hall, 2001</ref><ref name=\"NixonAguado\">Mark S. Nixon and Alberto S. Aguado. ''Feature Extraction and Image Processing''. Academic Press, 2008, p. 88.</ref>\n\nwhere ''x'' is the distance from the origin in the horizontal axis, ''y'' is the distance from the origin in the vertical axis, and ''σ'' is the [[standard deviation]] of the Gaussian distribution. When applied in two dimensions, this formula produces a surface whose [[:wikt:contour|contour]]s are [[concentric circles]] with a Gaussian distribution from the center point. Values from this distribution are used to build a [[convolution]] matrix which is applied to the original image. This convolution process is illustrated visually in the figure on the right. Each pixel's new value is set to a [[weighted average]] of that pixel's neighborhood. The original pixel's value receives the heaviest weight (having the highest Gaussian value) and neighboring pixels receive smaller weights as their distance to the original pixel increases. This results in a blur that preserves boundaries and edges better than other, more uniform blurring filters; see also [[scale space implementation]].\n\nIn theory, the Gaussian function at every point on the image will be non-zero, meaning that the entire image would need to be included in the calculations for each pixel. In practice, when computing a discrete approximation of the Gaussian function, pixels at a distance of more than 3''σ'' have a small enough influence to be considered effectively zero. Thus contributions from pixels outside that range can be ignored. Typically, an image processing program need only calculate a matrix with dimensions <math>\\lceil6\\sigma\\rceil</math> × <math>\\lceil6\\sigma\\rceil</math> (where <math>\\lceil \\cdot \\rceil</math> is the [[Floor and ceiling functions|ceiling function]]) to ensure a result sufficiently close to that obtained by the entire Gaussian distribution.\n\nIn addition to being circularly symmetric, the Gaussian blur can be applied to a two-dimensional image as two independent one-dimensional calculations, and so is termed [[separable filter]]. That is, the effect of applying the two-dimensional matrix can also be achieved by applying a series of single-dimensional Gaussian matrices in the horizontal direction, then repeating the process in the vertical direction. In computational terms, this is a useful property, since the calculation can be performed in <math>O\\left(w_\\text{kernel}  w_\\text{image}  h_\\text{image}\\right) + O\\left(h_\\text{kernel}  w_\\text{image}  h_\\text{image}\\right)</math> time (where ''h'' is height and ''w'' is width; see [[Big O notation]]), as opposed to <math>O\\left(w_\\text{kernel} h_\\text{kernel} w_\\text{image} h_\\text{image}\\right)</math> for a non-separable kernel.\n\nApplying multiple, successive Gaussian blurs to an image has the same effect as applying a single, larger Gaussian blur, whose radius is the square root of the sum of the squares of the blur radii that were actually applied. For example, applying successive Gaussian blurs with radii of 6 and 8 gives the same results as applying a single Gaussian blur of radius 10, since <math>\\sqrt{6^2 + 8^2} = 10</math>. Because of this relationship, processing time cannot be saved by simulating a Gaussian blur with successive, smaller blurs — the time required will be at least as great as performing the single large blur.\n\n[[Image:Gaussian blur before downscaling.png|thumb|128x128px|Two downscaled images of the [[Flag of the Commonwealth of Nations]]. Before downscaling, a Gaussian blur was applied to the bottom image but not to the top image. The blur makes the image less sharp, but prevents the formation of [[moiré pattern]] aliasing artifacts.]]\nGaussian blurring is commonly used when reducing the size of an image. When [[downsampling]] an image, it is common to apply a low-pass filter to the image prior to resampling. This is to ensure that spurious high-frequency information does not appear in the downsampled image ([[aliasing]]). Gaussian blurs have nice properties, such as having no sharp edges, and thus do not introduce ringing into the filtered image.\n\n== Low-pass filter ==\n{{Expand section|date=March 2009}}\nGaussian blur is a [[low-pass filter]], attenuating high frequency signals.\n\nIts amplitude [[Bode plot]] (the [[log scale]] in the [[frequency domain]]) is a [[parabola]].\n\n== Variance reduction ==\nHow much does a Gaussian filter with standard deviation <math>\\sigma_f</math> smooth the picture?  In other words, how much does it reduce the standard deviation of pixel values in the picture? Assume the grayscale pixel values have a standard deviation <math>\\sigma_X</math>, then after applying the filter the reduced standard deviation <math>\\sigma_r</math> can be approximated as\n:<math>\\sigma_r \\approx \\frac{\\sigma_X}{\\sigma_f 2 \\sqrt \\pi}</math>. {{citation needed|reason=Original Research?|date=November 2014}}\n\n==Sample Gaussian matrix==\n\nThis is a sample matrix, produced by sampling the Gaussian filter kernel (with σ = 0.84089642) at the midpoints of each pixel and then normalizing. Note that the center element (at [4, 4]) has the largest value, decreasing symmetrically as distance from the center increases.\n<!-- This is contradictory, above it says ceil(6*sigma) dimension size, which is 6, here we use 7 -->\n{| cellpadding=\"2\" style=\"text-align:center;width:100%;max-width:49em;\"\n|0.00000067 || 0.00002292 || '''0.00019117''' || 0.00038771 || '''0.00019117 '''|| 0.00002292 || 0.00000067\n|-\n|0.00002292 || 0.00078633 || 0.00655965 || 0.01330373 || 0.00655965 || 0.00078633 || 0.00002292\n|-\n|'''0.00019117''' || 0.00655965 || 0.05472157 || 0.11098164 || 0.05472157 || 0.00655965 || '''0.00019117'''\n|-\n|0.00038771 || 0.01330373 || 0.11098164 || '''0.22508352''' || 0.11098164 || 0.01330373 || 0.00038771\n|-\n|'''0.00019117''' || 0.00655965 || 0.05472157 || 0.11098164 || 0.05472157 || 0.00655965 || '''0.00019117'''\n|-\n|0.00002292 || 0.00078633 || 0.00655965 || 0.01330373 || 0.00655965 || 0.00078633 || 0.00002292\n|-\n|0.00000067 || 0.00002292 || '''0.00019117''' || 0.00038771 || '''0.00019117 '''|| 0.00002292 || 0.00000067\n|}\n\nNote that 0.22508352 (the central one) is 1177 times larger than 0.00019117 which is just outside 3σ.\n\n==Implementation==\n\nA Gaussian blur effect is typically generated by convolving an image with a kernel of Gaussian values. In practice, it is best to take advantage of the Gaussian blur’s separable property by dividing the process into two passes. In the first pass, a one-dimensional kernel is used to blur the image in only the horizontal or vertical direction. In the second pass, the same one-dimensional kernel is used to blur in the remaining direction. The resulting effect is the same as convolving with a two-dimensional kernel in a single pass, but requires fewer calculations.\n\nDiscretization is typically achieved by sampling the Gaussian filter kernel at discrete points, normally at positions corresponding to the midpoints of each pixel. This reduces the computational cost but, for very small filter kernels, point sampling the Gaussian function with very few samples leads to a large error. In these cases, accuracy is maintained (at a slight computational cost) by integration of the Gaussian function over each pixel's area.<ref name=\"Reinhard\">Erik Reinhard. ''High dynamic range imaging: Acquisition, Display, and Image-Based Lighting''. Morgan Kaufmann, 2006, pp. 233–234.</ref>\n\nWhen converting the Gaussian’s continuous values into the discrete values needed for a kernel, the sum of the values will be different from 1. This will cause a darkening or brightening of the image. To remedy this, the values can be normalized by dividing each term in the kernel by the sum of all terms in the kernel.\n\n==Common uses==\n[[File:Edge Image.gif|thumb|right|This shows how smoothing affects edge detection. With more smoothing, fewer edges are detected]]\n\nGaussian smoothing is commonly used with [[edge detection]]. Most edge-detection algorithms are sensitive to noise; the 2-D Laplacian filter, built from a discretization of the [[Laplace operator]], is highly sensitive to noisy environments. Using a Gaussian Blur filter before edge detection aims to reduce the level of noise in the image, which improves the result of the following edge-detection algorithm. This approach is commonly referred to as [[Laplacian of Gaussian]], or LoG filtering.<ref>{{cite web|url=http://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm|title=Spatial Filters - Laplacian of Gaussian|last=Fisher, Perkins, Walker & Wolfart|year=2003|accessdate =2010-09-13}}</ref>\n\n==See also==\n*[[Image noise]]\n*[[Gaussian filter]]\n*[[Gaussian pyramid]]\n*[[Infinite impulse response]] (IIR)\n*[[Scale space implementation]]\n*[[Median filter]]\n*[[Weierstrass transform]]\n\n==Notes and references==\n{{reflist}}\n\n== External links ==\n*[http://www.gamerendering.com/2008/10/11/gaussian-blur-filter-shader/ GLSL implementation of a separable gaussian blur filter].\n*Example for [https://web.archive.org/web/20100310103512/http://holiday.snrk.de/SnarkSearch.cgi Gaussian blur (low pass filtering) applied to a wood-block print and an etching] in order to remove details for picture comparison.\n*Mathematica [http://reference.wolfram.com/mathematica/ref/GaussianFilter.html GaussianFilter] function\n\n* OpenCV (C++) [https://docs.opencv.org/3.3.1/d4/d86/group__imgproc__filter.html#gaabe8c836e97159a9193fb0b11ac52cf1 GaussianBlur] function\n\n{{Noise}}\n{{DEFAULTSORT:Gaussian Blur}}\n[[Category:Image processing]]\n[[Category:Gaussian function]]\n[[Category:Image noise reduction techniques]]"
    },
    {
      "title": "Gaussian correlation inequality",
      "url": "https://en.wikipedia.org/wiki/Gaussian_correlation_inequality",
      "text": "[[File:Gaussian distribution darts demonstration.svg|thumb|Gaussian correlation inequality states that probability of hitting both circle and rectangle with a dart is greater than or equal to the product of the individual probabilities of hitting circle or rectangle.]]\nThe '''Gaussian correlation inequality''' ('''GCI'''), formerly known as the '''Gaussian correlation conjecture''' ('''GCC'''),  is a [[mathematical theorem]] in the fields of [[mathematical statistics]] and [[convex geometry]]. A special case of the inequality was published as a conjecture in a paper from 1955;<ref>Dunnett, C. W.; Sobel, M. Approximations to the probability integral and certain percentage points of a multivariate analogue of Student's t-distribution. Biometrika 42, (1955). 258–260.</ref> further development was given by [[Olive Jean Dunn]] in 1958;<ref name=wolchover /><ref name=annals1998>Schechtman, G.; Schlumprecht, T.; Zinn, J. [http://projecteuclid.org/download/pdf_1/euclid.aop/1022855422 On the Gaussian Measure of the Intersection]. The Annals of Probability, Vol. 26, No. 1, 346–357, 1998.</ref> and the general case was stated in 1972, also as a conjecture.<ref>Das Gupta, S.; Eaton, M. L.; Olkin, I.; Perlman, M.; Savage, L. J.; Sobel, M. Inequalitites on the probability content of convex regions for elliptically contoured distributions. Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability (Univ. California, Berkeley, Calif., 1970/1971), Vol. II: Probability theory, pp. 241–265. Univ. California Press, Berkeley, Calif., 1972.</ref>\n\nThe inequality remained unproved until 2014, when [[Thomas Royen]], a German statistician, proved it using relatively elementary tools. The proof was not generally known when it was published in 2014, due to Royen's relative anonymity and the fact that the proof was published in a [[predatory journal]].<ref>{{cite web|url=http://www.pphmj.com/abstract/8713.htm|title=Pushpa Publishing House|website=www.pphmj.com|accessdate=4 July 2017}}</ref><ref>{{cite arxiv|title=A simple proof of the Gaussian correlation conjecture extended to multivariate gamma distributions|first=T.|last=Royen|date=5 August 2014|publisher=|eprint=1408.1028|class=math.PR}}</ref> Another reason was multiple futile attempts to prove it, causing skepticism among mathematicians in the field.<ref name=wolchover>{{cite-web| first=Natalie | last=Wolchover | url=https://www.quantamagazine.org/20170328-statistician-proves-gaussian-correlation-inequality/ | title= A Long-Sought Proof, Found and Almost Lost| publisher=QUANTA magazine | date=March 28, 2017 |access-date= April 4, 2017 }}</ref>\n\nThe conjecture, and its solution, came to public attention in 2017, when reports of Royen's proof were published in mainstream media.<ref name=wolchover /><ref>{{Cite news|url=https://www.independent.co.uk/news/science/german-man-thomas-royen-solve-maths-problem-gaussian-correlation-inequality-retired-67-age-a7663936.html|title=Retired man solves one of hardest maths problems in the world and no one notices|date=2017-04-03|work=The Independent|access-date=2017-04-04|language=en-GB|first=Chloe|last=Farand}}</ref><ref>{{Cite web|url=http://www.spiegel.de/wissenschaft/mensch/mathematiker-thomas-royen-beweist-mit-67-jahren-statistikproblem-gci-a-1141709.html|title=Erfolg mit 67 Jahren: Der Wunderopa der Mathematik|last=Dambeck|first=Holger|date=2017-04-04|website=SPIEGEL ONLINE|archive-url=|archive-date=|dead-url=|access-date=2017-04-04}}</ref>\n\n==Formal statement==\n\nLet <math>\\mu_n </math> be an ''n''-dimensional [[Gaussian measure]] on <math> \\mathbb{R}^n </math> centered at the origin. Then for all [[convex set]]s <math> E,F \\subset \\mathbb{R}^n </math> that are [[symmetric set|symmetric about the origin]],\n\n: <math> \\mu_n(E \\cap F) \\geq \\mu_n(E) \\cdot \\mu_n(F).</math>\n\nRoyen's proof of the conjecture generalized it, and demonstrates the same statement for the [[gamma distribution]].\n\nAs a simple example, one can think of darts in the plane distributed according to a [[multivariate normal distribution]]. If we are considering a circle and a rectangle, both centered at the origin, then the proportion of the darts landing in the intersection of both shapes is no less than the product of the proportions of the darts landing in each shape.\n\n==References==\n{{Reflist|30em}}\n\n===General===\n* [[Thomas Royen]], \"A simple proof of the Gaussian correlation conjecture extended to multivariate gamma distributions\", {{arxiv|1408.1028}}\n* Rafał Latała, Dariusz Matlak, \"Royen's proof of the Gaussian correlation inequality\", {{arxiv|1512.08776}}\n\n==External links==\n* George Lowther, [https://almostsure.wordpress.com/2009/09/27/the-gaussian-correlation-conjecture/ The Gaussian Correlation Conjecture], \"Almost Sure\"\n\n[[Category:Gaussian function]]\n[[Category:Geometric inequalities]]\n[[Category:Probabilistic inequalities]]"
    },
    {
      "title": "Gaussian filter",
      "url": "https://en.wikipedia.org/wiki/Gaussian_filter",
      "text": "{{Linear analog electronic filter|filter2=hide|filter3=hide}}\n\n[[File:Gaussian Filter.svg|thumb|Shape of the impulse response of a typical Gaussian filter]]\nIn [[electronics]] and [[signal processing]], a '''Gaussian filter''' is a [[filter (signal processing)|filter]] whose [[impulse response]] is a [[Gaussian function]] (or an approximation to it, since a true Gaussian response is physically unrealizable). Gaussian filters have the properties of having no overshoot to a step function input while minimizing the rise and fall time. This behavior is closely connected to the fact that the Gaussian filter has the minimum possible [[group delay]]. It is considered the ideal [[time domain]] filter, just as the [[sinc filter|sinc]] is the ideal frequency domain filter.<ref>Filtering in the Time and Frequency Domains by Herman J. Blinchikoff, Anatol I. Zverev</ref>  These properties are important in areas such as [[Oscilloscope#The vertical amplifier|oscilloscopes]]<ref>http://www.radiomuseum.org/forumdata/users/4767/file/Tektronix_VerticalAmplifierCircuits_Part1.pdf</ref> and digital telecommunication systems.<ref>https://kh6htv.files.wordpress.com/2015/11/an-07a-risetime-filters.pdf</ref>\n\nMathematically, a Gaussian filter modifies the input signal by [[convolution]] with a Gaussian function; this transformation is also known as the [[Weierstrass transform]].\n\n==Definition==\nThe one-dimensional Gaussian filter has an impulse response given by\n:<math>g(x)= \\sqrt{\\frac{a}{\\pi}}\\cdot e^{-a \\cdot x^2}</math>\nand the frequency response is given by the [[Fourier transform#Square-integrable functions|Fourier transform]]\n:<math>\\hat g(f)= e^{-\\frac{\\pi^2f^2}{a}}</math>\nwith <math>f</math> the ordinary frequency. These equations can also be expressed with the [[standard deviation]] as parameter\n:<math>g(x) = \\frac{1}{\\sqrt{2\\pi}\\cdot\\sigma}\\cdot e^{-\\frac{x^2}{2\\sigma^2}}</math>\nand the frequency response is given by\n:<math>\\hat g(f) = e^{-\\frac{f^2}{2\\sigma_f^2}}</math>\nBy writing <math>a</math> as a function of <math>\\sigma</math> with the two equations for <math>g(x)</math> and as a function of <math>\\sigma_f</math> with the two equations for <math>\\hat g(f)</math> it can be shown that the product of the standard deviation and the standard deviation in the frequency domain is given by\n:<math>\\sigma\\cdot\\sigma_f=\\frac{1}{2\\pi}</math>,\nwhere the standard deviations are expressed in their physical units, e.g. in the case of time and frequency in seconds and Hertz.\n\nIn two dimensions, it is the product of two such Gaussians, one per direction:\n\n:<math>g(x,y) = \\frac{1}{2\\pi \\sigma^2} \\cdot e^{-\\frac{x^2 + y^2}{2 \\sigma^2}}</math> <ref name=\"HaddadAkansu\">R.A. Haddad and A.N. Akansu, \"A Class of Fast Gaussian Binomial Filters for Speech and Image Processing,\" IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 39, pp 723-727, March 1991.</ref><ref name=\"ShapiroStockman\">[[Linda Shapiro|Shapiro, L. G.]] &amp; Stockman, G. C: \"Computer Vision\", page 137, 150. Prentence Hall, 2001</ref><ref name=\"NixonAguado\">Mark S. Nixon and Alberto S. Aguado. ''Feature Extraction and Image Processing''. Academic Press, 2008, p. 88.</ref>\n\nwhere ''x'' is the distance from the origin in the horizontal axis, ''y'' is the distance from the origin in the vertical axis, and ''σ'' is the [[standard deviation]] of the Gaussian distribution.\n\n== Digital implementation ==\n{{Refimprove section|date=September 2013}}\nThe Gaussian function is for <math> x \\in (-\\infty,\\infty) </math> and would theoretically require an infinite window length. However, since it decays rapidly, it is often reasonable to truncate the filter window and implement the filter directly for narrow windows, in effect by using a simple rectangular window function. In other cases, the truncation may introduce significant errors. Better results can be achieved by instead using a different [[window function]]; see [[scale space implementation]] for details.\n\nFiltering involves [[convolution]]. The filter function is said to be the kernel of an integral transform. The Gaussian kernel is continuous. Most commonly, the discrete equivalent is the [[sampled Gaussian kernel]] that is produced by sampling points from the continuous Gaussian. An alternate method is to use the [[discrete Gaussian kernel]] which has superior characteristics for some purposes. Unlike the sampled Gaussian kernel, the discrete Gaussian kernel is the solution to the discrete [[diffusion equation]].\n\nSince the [[Fourier transform]] of the Gaussian function yields a Gaussian function, the signal (preferably after being divided into overlapping windowed blocks) can be transformed with a [[Fast Fourier transform]], multiplied with a Gaussian function and transformed back. This is the standard procedure of applying an arbitrary [[finite impulse response]] filter, with the only difference that the Fourier transform of the filter window is explicitly known.\n\nDue to the [[central limit theorem]], the Gaussian can be approximated by several runs of a very simple filter such as the [[moving average]]. The simple moving average corresponds to [[convolution]] with the constant [[B-spline]] (a rectangular pulse), and, for example, four iterations of a moving average yields a cubic B-spline as filter window which approximates the Gaussian quite well.\n\nIn the discrete case the standard deviations are related by\n:<math>\\sigma\\cdot\\sigma_f=\\frac{N}{2\\pi}</math>\nwhere the standard deviations are expressed in number of samples and ''N'' is the total number of samples.\nBorrowing the terms from statistics, the [[standard deviation]] of a filter can be interpreted as a measure of its size. The cut-off frequency of a Gaussian filter might be defined by the standard deviation in the frequency domain yielding\n: <math>f_c = \\sigma_f = \\frac{1}{2\\pi\\sigma}</math>\nwhere all quantities are expressed in their physical units. If <math>\\sigma</math> is measured in samples the cut-off frequency (in physical units) can be calculated with\n: <math>f_c = \\frac{F_s}{2\\pi\\sigma}</math>\nwhere <math>F_s</math> is the sample rate.\nThe response value of the Gaussian filter at this cut-off frequency equals exp(-0.5)≈0.607.\n\nHowever, it is more common to define the cut-off frequency as the half power point: where the filter response is reduced to 0.5 (-3&nbsp;dB) in the power spectrum, or 1/{{sqrt|2}}&nbsp;≈ 0.707 in the amplitude spectrum (see e.g. [[Butterworth filter#Original paper|Butterworth filter]]).\nFor an arbitrary cut-off value 1/''c'' for the response of the filter the cut-off frequency is given by\n:<math>f_c = \\sqrt{2\\ln(c)}\\cdot\\sigma_f </math>\nFor ''c''=2 the constant before the standard deviation in the frequency domain in the last equation equals approximately 1.1774, which is half the Full Width at Half Maximum (FWHM) (see [[Gaussian function#Properties|Gaussian function]]). For ''c''={{sqrt|2}} this constant equals approximately 0.8326. These values are quite close to 1.\n\nA simple moving average corresponds to a [[uniform distribution (discrete)|uniform probability distribution]] and thus its filter width of size <math>n</math> has standard deviation <math>\\sqrt{({n}^2-1)/12}</math>. Thus the application of successive <math>m</math> moving averages with sizes <math>{n}_1,\\dots,{n}_m</math> yield a standard deviation of \n: <math>\\sigma = \\sqrt{\\frac{{n}_1^2+\\cdots+{n}_m^2-m}{12}}</math>\n(Note that standard deviations do not sum up, but [[variance]]s do.)\n\nA gaussian kernel requires <math>6{\\sigma}-1</math> values, e.g. for a <math>{\\sigma}</math> of 3 it needs a kernel of length 17. A running mean filter of 5 points will have a sigma of <math>{\\sqrt{2}}</math>. Running it three times will give a <math>{\\sigma}</math> of 2.42. It remains to be seen where the advantage is over using a gaussian rather than a poor approximation.\n\nWhen applied in two dimensions, this formula produces a Gaussian surface that has a maximum at the origin, whose [[:wikt:contour|contour]]s are [[concentric circles]] with the origin as center.  A two dimensional [[convolution]] [[Matrix (mathematics)|matrix]] is precomputed from the formula and convolved with two dimensional data. Each element in the resultant matrix new value is set to a [[weighted average]] of that elements neighborhood. The focal element receives the heaviest weight (having the highest Gaussian value) and neighboring elements receive smaller weights as their distance to the focal element increases. In Image processing, each element in the matrix represents a pixel attribute such as brightness or a color intensity, and the overall effect is called [[Gaussian blur]].\n\nThe Gaussian filter is non-causal which means the filter window is symmetric about the origin in the time-domain. This makes the Gaussian filter physically unrealizable. This is usually of no consequence for applications where the filter bandwidth is much larger than the signal. In real-time systems, a delay is incurred because incoming samples need to fill the filter window before the filter can be applied to the signal. While no amount of delay can make a theoretical Gaussian filter causal (because the Gaussian function is non-zero everywhere), the Gaussian function converges to zero so rapidly that a causal approximation can achieve any required tolerance with a modest delay, even to the accuracy of [[IEEE floating point|floating point representation]].\n\n==Applications==\n{{Expand section|date=May 2012}}\n* [[GSM]] since it applies [[GMSK]] modulation\n* the Gaussian filter is also used in [[GFSK]].\n* [[Canny edge detector|Canny Edge Detector]] used in image processing.\n\n== See also ==\n{{colbegin}}\n* [[Butterworth filter]]\n* [[Comb filter]]\n* [[Chebyshev filter]]\n* [[Discrete Gaussian kernel]]\n* [[Elliptic filter]]\n* [[Gaussian blur]]\n* [[Gaussian Pyramid]]\n* [[Scale space]]\n* [[Scale space implementation]]\n{{colend}}\n\n== References ==\n<references />\n\n{{DEFAULTSORT:Gaussian Filter}}\n[[Category:Linear filters]]\n[[Category:Gaussian function]]"
    },
    {
      "title": "Gaussian integral",
      "url": "https://en.wikipedia.org/wiki/Gaussian_integral",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Integral of the Gaussian function, equal to sqrt(&pi;)}}\n{{about|the Euler–Poisson integral|Gaussian quadrature|Gaussian integration}}\n[[Image:E^(-x^2).svg|thumb|right|A graph of ''f''(''x'') =&nbsp;''e''<sup>−''x''<sup>2</sup></sup> and the area between the function and the ''x''-axis, which is equal to {{math|{{sqrt|''&pi;''}}}}.]]\n\nThe '''Gaussian integral''', also known as the '''Euler–Poisson integral''', is the integral of the [[Gaussian function]] ''e''<sup>−''x''<sup>2</sup></sup> over the entire real line. It is named after the German mathematician [[Carl Friedrich Gauss]].  The integral is:\n\n:<math>\\int_{-\\infty}^\\infty e^{-x^2}\\,dx = \\sqrt{\\pi}.</math>\n\n[[Abraham de Moivre]] originally discovered this type of integral in 1733, while Gauss published the precise integral in 1809.<ref name=\"History of normal distribution\">{{cite web |url=https://www.maa.org/sites/default/files/pdf/upload_library/22/Allendoerfer/stahl96.pdf |title=The History of the Normal Distribution |work=MAA.org |first=Saul|last=Stahl|date=April 2006|accessdate=May 25, 2018}}</ref> The integral has a wide range of applications.  For example, with a slight change of variables it is used to compute the [[normalizing constant]] of the [[normal distribution]].  The same integral with finite limits is closely related to both the [[error function]] and the [[cumulative distribution function]] of the [[normal distribution]]. In physics this type of integral appears frequently, for example, in [[quantum mechanics]], to find the probability density of the ground state of the harmonic oscillator. This integral is also used in the path integral formulation, to find the propagator of the harmonic oscillator, and in [[statistical mechanics]], to find its [[partition function (statistical mechanics)|partition function]].\n\nAlthough no [[elementary function]] exists for the error function, as can be proven by the [[Risch algorithm]],<ref>{{cite journal |first=G. W. |last=Cherry |title=Integration in Finite Terms with Special Functions: the Error Function |journal=Journal of Symbolic Computation |volume=1 |issue=3 |year=1985 |pages=283–302 |doi=10.1016/S0747-7171(85)80037-7 }}</ref> the Gaussian integral can be solved analytically through the methods of [[multivariable calculus]]. That is, there is no elementary ''[[indefinite integral]]'' for\n:<math>\\int e^{-x^2}\\,dx,</math>\nbut the [[definite integral]]\n:<math>\\int_{-\\infty}^\\infty e^{-x^2}\\,dx</math>\ncan be evaluated. The definite integral of an arbitrary [[Gaussian function]] is\n\n:<math>\\int_{-\\infty}^{\\infty}  e^{-a(x+b)^2}\\,dx= \\sqrt{\\frac{\\pi}{a}}.</math>\n\nThe Gaussian integral is encountered very often in physics and numerous generalizations of the integral are encountered in [[quantum field theory]].\n\n==Computation==\n\n===By polar coordinates===\nA standard way to compute the Gaussian integral, the idea of which goes back to Poisson,<ref name=\"york.ac.uk\">{{cite web |title=The Probability Integral |url=https://www.york.ac.uk/depts/maths/histstat/normal_history.pdf }}</ref> is to make use of the property that:\n\n:<math>\\left(\\int_{-\\infty}^{\\infty} e^{-x^2}\\,dx\\right)^2 = \\int_{-\\infty}^{\\infty} e^{-x^2}\\,dx \\int_{-\\infty}^{\\infty} e^{-y^2}\\,dy = \\int_{-\\infty}^{\\infty}  \\int_{-\\infty}^{\\infty} e^{-(x^2+y^2)}\\, dx\\,dy. </math>\n\nConsider the function ''e''<sup>−(''x''<sup>2</sup>&nbsp;+&nbsp;''y''<sup>2</sup>)</sup>&nbsp;=&nbsp;''e''<sup>−''r''<sup>2</sup></sup> on the plane '''R'''<sup>2</sup>, and compute its integral two ways:\n# on the one hand, by [[double integration]] in the [[Cartesian coordinate system]], its integral is a square:\n#: <math>\\left(\\int e^{-x^2}\\,dx\\right)^2;</math>\n# on the other hand, by [[shell integration]] (a case of double integration in [[polar coordinates]]), its integral is computed to be {{mvar|π}}.\n\nComparing these two computations yields the integral, though one should take care about the improper integrals involved.\n\n:<math>\\begin{align}\n   \\iint_{\\mathbf{R}^2} e^{-(x^2+y^2)}\\,d(x,y)\n   &= \\int_0^{2\\pi} \\int_0^{\\infty} e^{-r^2}r\\,dr\\,d\\theta\\\\\n   &= 2\\pi \\int_0^\\infty re^{-r^2}\\,dr\\\\\n   &= 2\\pi \\int_{-\\infty}^0 \\tfrac{1}{2} e^s\\,ds && s = -r^2\\\\\n   &= \\pi \\int_{-\\infty}^0 e^s\\,ds \\\\\n   &= \\pi (e^0 - e^{-\\infty}) \\\\\n   &=\\pi,\n \\end{align}</math>\n\nwhere the factor of ''r'' is the [[Jacobian determinant]] which appears because of the [[list of canonical coordinate transformations|transform to polar coordinates]] (''r''&nbsp;''dr''&nbsp;''dθ'' is the standard measure on the plane, expressed in polar coordinates [[Wikibooks:Calculus/Polar Integration#Generalization]]), and the substitution involves taking ''s''&nbsp;=&nbsp;−''r''<sup>2</sup>, so ''ds''&nbsp;=&nbsp;−2''r''&nbsp;''dr''.\n\nCombining these yields\n\n: <math>\\left ( \\int_{-\\infty}^\\infty e^{-x^2}\\,dx \\right )^2=\\pi,</math>\n\nso\n\n: <math>\\int_{-\\infty}^\\infty e^{-x^2}\\,dx=\\sqrt{\\pi}</math>.\n\n====Complete proof====\nTo justify the improper double integrals and equating the two expressions, we begin with an approximating function:\n\n:<math>I(a)=\\int_{-a}^a e^{-x^2}dx.</math>\n\nIf the integral\n:<math>\\int_{-\\infty}^\\infty e^{-x^2}\\,dx</math>\nwere [[absolutely convergent]] we would have that its [[Cauchy principal value]], that is, the limit\n\n:<math>\\lim_{a\\to\\infty} I(a) </math>\n\nwould coincide with\n:<math>\\int_{-\\infty}^\\infty e^{-x^2}\\,dx.</math>\nTo see that this is the case, consider that\n\n:<math>\\int_{-\\infty}^\\infty |e^{-x^2}|\\, dx < \\int_{-\\infty}^{-1} -x e^{-x^2}\\, dx + \\int_{-1}^1 e^{-x^2}\\, dx+ \\int_{1}^{\\infty} x e^{-x^2}\\, dx<\\infty.</math>\n\nso we can compute\n:<math>\\int_{-\\infty}^\\infty e^{-x^2}\\,dx</math>\nby just taking the limit\n:<math>\\lim_{a\\to\\infty} I(a)</math>.\n\nTaking the square of <math>I(a)</math> yields\n\n:<math>\\begin{align}\nI(a)^2 & = \\left ( \\int_{-a}^a e^{-x^2}\\, dx \\right ) \\left ( \\int_{-a}^a e^{-y^2}\\, dy \\right ) \\\\\n& = \\int_{-a}^a \\left ( \\int_{-a}^a e^{-y^2}\\, dy \\right )\\,e^{-x^2}\\, dx \\\\\n&  = \\int_{-a}^a \\int_{-a}^a e^{-(x^2+y^2)}\\,dy\\,dx.\n\\end{align}</math>\n\nUsing [[Fubini's theorem]], the above double integral can be seen as an area integral\n\n: <math>\\iint_{[-a, a] \\times [-a, a]} e^{-(x^2+y^2)}\\,d(x,y),</math>\n\ntaken over a square with vertices {(−''a'',&nbsp;''a''), (''a'',&nbsp;''a''), (''a'',&nbsp;−''a''), (−''a'',&nbsp;−''a'')} on the ''xy''-[[Cartesian plane|plane]].\n\nSince the exponential function is greater than 0 for all real numbers, it then follows that the integral taken over the square's [[incircle]] must be less than <math>I(a)^2</math>, and similarly the integral taken over the square's [[circumcircle]] must be greater than <math>I(a)^2</math>. The integrals over the two disks can easily be computed by switching from cartesian coordinates to [[list of canonical coordinate transformations|polar coordinates]]:\n\n: <math>\n\\begin{align}\nx & = r \\cos \\theta \\\\\ny & = r \\sin\\theta\n\\end{align}\n</math>\n\n:<math>\n\\mathbf J(r, \\theta) = \n\\begin{bmatrix}\n  \\dfrac{\\partial x}{\\partial r} & \\dfrac{\\partial x}{\\partial\\theta}\\\\[1em]\n  \\dfrac{\\partial y}{\\partial r} & \\dfrac{\\partial y}{\\partial\\theta} \\end{bmatrix}\n= \\begin{bmatrix}\n  \\cos\\theta& - r\\sin \\theta \\\\\n  \\sin\\theta&   r\\cos \\theta\n\\end{bmatrix}\n</math>\n\n: <math>\nd(x,y) = |J(r, \\theta)|d(r,\\theta) = r\\, d(r,\\theta).\n</math>\n\n:<math>\\int_0^{2\\pi}\\int_0^a re^{-r^2}\\,dr\\,d\\theta < I^2(a) < \\int_0^{2\\pi}\\int_0^{a\\sqrt{2}} re^{-r^2}\\,dr\\,d\\theta.</math>\n\n(See [[list of canonical coordinate transformations|to polar coordinates from Cartesian coordinates]] for help with polar transformation.)\n\nIntegrating,\n\n:<math>\\pi (1-e^{-a^2}) <  I^2(a) < \\pi (1 - e^{-2a^2}). </math>\n\nBy the [[squeeze theorem]], this gives the Gaussian integral\n\n:<math>\\int_{-\\infty}^\\infty e^{-x^2}\\, dx = \\sqrt{\\pi}.</math>\n\n===By Cartesian coordinates===\nA different technique, which goes back to Laplace (1812),<ref name=\"york.ac.uk\" /> is the following. Let\n\n: <math>\\begin{align}\ny & = xs \\\\\ndy & = x\\,ds.\n\\end{align}</math>\n\nSince the limits on ''s'' as ''y'' → ±∞ depend on the sign of ''x'', it simplifies the calculation to use the fact that ''e''<sup>−''x''<sup>2</sup></sup> is an [[even function]], and, therefore, the integral over all real numbers is just twice the integral from zero to infinity. That is,\n\n:<math>\\int_{-\\infty}^{\\infty} e^{-x^2}\\,dx = 2\\int_{0}^{\\infty} e^{-x^2}\\,dx.</math>\n\nThus, over the range of integration, ''x'' ≥ 0, and the variables ''y'' and ''s'' have the same limits. This yields:\n\n:<math>\\begin{align}\nI^2 &= 4 \\int_0^\\infty \\int_0^\\infty e^{-(x^2 + y^2)} dy\\,dx \\\\\n&= 4 \\int_0^\\infty \\left( \\int_0^\\infty e^{-(x^2 + y^2)} \\, dy \\right) \\, dx \\\\\n&= 4 \\int_0^\\infty \\left( \\int_0^\\infty e^{-x^2(1+s^2)} x\\,ds \\right) \\, dx \\\\\n&= 4 \\int_0^\\infty \\left( \\int_0^\\infty e^{-x^2(1 + s^2)} x \\, dx \\right) \\, ds \\\\\n&= 4 \\int_0^\\infty \\left[ \\frac{1}{-2(1+s^2)} e^{-x^2(1+s^2)} \\right]_{x=0}^{x=\\infty} \\, ds \\\\\n&= 4 \\left (\\frac{1}{2} \\int_0^\\infty \\frac{ds}{1+s^2}  \\right ) \\\\\n&= 2 \\Big [ \\arctan s \\Big ]_0^\\infty \\\\\n&= \\pi.\n\\end{align}</math>\n\nTherefore, <math>I = \\sqrt{\\pi}</math>, as expected.\n\n==Relation to the gamma function==\n\nThe integrand is an [[even function]],\n\n:<math>\\int_{-\\infty}^{\\infty} e^{-x^2} dx = 2 \\int_0^\\infty e^{-x^2} dx</math>\n\nThus, after the change of variable <math>x=\\sqrt{t}</math>, this turns into the Euler integral\n\n:<math>2 \\int_0^\\infty e^{-x^2} dx=2\\int_0^\\infty \\frac{1}{2}\\ e^{-t} \\ t^{-\\frac{1}{2}} dt = \\Gamma\\left(\\frac{1}{2}\\right) = \\sqrt{\\pi}</math>\n\nwhere Γ is the [[gamma function]]. This shows why the [[factorial]] of a half-integer is a rational multiple of <math>\\sqrt \\pi</math>. More generally,\n\n:<math>\\int_0^\\infty e^{-ax^b} dx = \\frac{\\Gamma\\left(\\frac{1}{b}\\right)}{ba^{\\frac{1}{b}}} </math>\n\n==Generalizations==\n\n===The integral of a Gaussian function===\n{{Main|Integral of a Gaussian function}}\nThe integral of an arbitrary [[Gaussian function]] is\n\n:<math>\\int_{-\\infty}^{\\infty}  e^{-a(x+b)^2}\\,dx= \\sqrt{\\frac{\\pi}{a}}.</math>\n\nAn alternative form is\n\n:<math>\\int_{-\\infty}^{\\infty}e^{- a x^2 + b x + c}\\,dx=\\sqrt{\\frac{\\pi}{a}}\\,e^{\\frac{b^2}{4a}+c}.</math>\n\nThis form is useful for calculating expectations of some continuous probability distributions related to the normal distribution, such as the [[log-normal distribution]], for example.\n\n===''n''-dimensional and functional generalization===\n{{main|multivariate normal distribution}}\nSuppose ''A'' is a symmetric positive-definite (hence invertible) {{math|''n'' × ''n''}} [[precision matrix]], which is the matrix inverse of the [[covariance matrix]]. Then,\n\n:<math>\\int_{-\\infty}^\\infty \\exp{\\left(-\\frac 1 2 \\sum\\limits_{i,j=1}^{n}A_{ij} x_i x_j \\right)} \\, d^nx =\\int_{-\\infty}^\\infty \\exp{\\left(-\\frac 1 2 x^{T} A x \\right)} \\, d^nx=\\sqrt{\\frac{(2\\pi)^n}{\\det A}} =\\sqrt{\\frac{1}{\\det (A / 2\\pi)}} =\\sqrt{\\det (2 \\pi A^{-1})}</math>\n\nwhere the integral is understood to be over '''R'''<sup>''n''</sup>.  This fact is applied in the study of the [[multivariate normal distribution]].\n\nAlso,\n\n:<math>\\int x_{k_1}\\cdots x_{k_{2N}} \\, \\exp{\\left( -\\frac{1}{2} \\sum\\limits_{i,j=1}^{n}A_{ij} x_i x_j \\right)} \\, d^nx =\\sqrt{\\frac{(2\\pi)^n}{\\det A}} \\, \\frac{1}{2^N N!} \\, \\sum_{\\sigma \\in S_{2N}}(A^{-1})_{k_{\\sigma(1)}k_{\\sigma(2)}} \\cdots (A^{-1})_{k_{\\sigma(2N-1)}k_{\\sigma(2N)}}</math>\n\nwhere σ is a [[permutation]] of {1, ..., 2''N''} and the extra factor on the right-hand side is the sum over all combinatorial pairings of {1, ..., 2''N''} of ''N'' copies of ''A''<sup>−1</sup>.\n\nAlternatively, <ref name=\"Central identity explaination\">{{cite web |title=Reference for Multidimensional Gaussian Integral |date=March 30, 2012 |work=[[Stack Exchange]] |url=https://math.stackexchange.com/q/126227 }}</ref>\n\n:<math>\\int f(\\vec x) \\exp{\\left( - \\frac 1 2 \\sum\\limits_{i,j=1}^{n}A_{ij} x_i x_j \\right)} d^nx=\\sqrt{(2\\pi)^n\\over \\det A} \\, \\left. \\exp{\\left({1\\over 2}\\sum\\limits_{i,j=1}^{n}(A^{-1})_{ij}{\\partial \\over \\partial x_i}{\\partial \\over \\partial x_j}\\right)}f(\\vec{x})\\right|_{\\vec{x}=0}</math>\n\nfor some [[analytic function]] ''f'', provided it satisfies some appropriate bounds on its growth and some other technical criteria. (It works for some functions and fails for others. Polynomials are fine.) The exponential over a differential operator is understood as a [[power series]].\n\nWhile [[functional integral]]s have no rigorous definition (or even a nonrigorous computational one in most cases), we can ''define'' a Gaussian functional integral in analogy to the finite-dimensional case. {{Citation needed|date=June 2011}} There is still the problem, though, that <math>(2\\pi)^\\infty</math> is infinite and also, the [[functional determinant]] would also be infinite in general. This can be taken care of if we only consider ratios:\n\n:<math>\\frac{\\int f(x_1)\\cdots f(x_{2N}) \\exp\\left[{-\\iint \\frac{1}{2}A(x_{2N+1},x_{2N+2}) f(x_{2N+1}) f(x_{2N+2}) d^dx_{2N+1} d^dx_{2N+2}}\\right] \\mathcal{D}f}{\\int \\exp\\left[{-\\iint \\frac{1}{2} A(x_{2N+1}, x_{2N+2}) f(x_{2N+1}) f(x_{2N+2}) d^dx_{2N+1} d^dx_{2N+2}}\\right] \\mathcal{D}f} =\\frac{1}{2^N N!}\\sum_{\\sigma \\in S_{2N}}A^{-1}(x_{\\sigma(1)},x_{\\sigma(2)})\\cdots A^{-1}(x_{\\sigma(2N-1)},x_{\\sigma(2N)}).</math>\n\nIn the [[DeWitt notation]], the equation looks identical to the finite-dimensional case.\n\n===''n''-dimensional with linear term===\nIf A is again a symmetric positive-definite matrix, then (assuming all are column vectors)\n:<math>\\int e^{-\\frac{1}{2}\\sum\\limits_{i,j=1}^{n}A_{ij} x_i x_j+\\sum\\limits_{i=1}^{n}B_i x_i} d^nx=\\int e^{-\\frac{1}{2}\\vec{x}^T \\mathbf{A} \\vec{x}+\\vec{B}^T \\vec{x}} d^nx= \\sqrt{ \\frac{(2\\pi)^n}{\\det{A}} }e^{\\frac{1}{2}\\vec{B}^{T}\\mathbf{A}^{-1}\\vec{B}}.</math>\n\n===Integrals of similar form===\n:<math>\\int_0^\\infty x^{2n}  e^{-\\frac{x^2}{a^2}}\\,dx = \\sqrt{\\pi}\\frac{a^{2n+1} (2n-1)!!}{2^{n+1}}</math>\n:<math>\\int_0^\\infty x^{2n+1} e^{-\\frac{x^2}{a^2}}\\,dx = \\frac{n!}{2} a^{2n+2}</math>\n:<math>\\int_0^\\infty x^{2n}e^{-ax^2}\\,dx = \\frac{(2n-1)!!}{a^n 2^{n+1}} \\sqrt{\\frac{\\pi}{a}}</math>\n:<math>\\int_0^\\infty x^{2n+1}e^{-ax^2}\\,dx = \\frac{n!}{2a^{n+1}}</math>\n:<math>\\int_0^\\infty x^{n}e^{-ax^2}\\,dx = \\frac{\\Gamma(\\frac{n+1}{2})}{2a^{\\frac{n+1}{2}}}</math>\nwhere ''n'' is a positive integer and !! denotes the [[double factorial]].\n\nAn easy way to derive these is by [[differentiation under the integral sign|parameter differentiation]].\n\n:<math>\\begin{align}\n\\int_{-\\infty}^\\infty x^{2n} e^{-\\alpha x^2}\\,dx &= \\left(-1\\right)^n\\int_{-\\infty}^\\infty \\frac{\\partial^n}{\\partial \\alpha^n} e^{-\\alpha x^2}\\,dx ~= \\left(-1\\right)^n\\frac{\\partial^n}{\\partial \\alpha^n} \\int_{-\\infty}^\\infty e^{-\\alpha x^2}\\,dx\\\\\n&= \\sqrt{\\pi} \\left(-1\\right)^n\\frac{\\partial^n}{\\partial \\alpha^n}\\alpha^{-\\frac{1}{2}} ~= \\sqrt{\\frac{\\pi}{\\alpha}}\\frac{(2n-1)!!}{\\left(2\\alpha\\right)^n}\n\\end{align}</math>\n\nOne could also integrate by parts and find a [[recurrence relation]] to solve this.\n\n===Higher-order polynomials===\n\nApplying a linear change of basis shows that the integral of the exponential of a homogeneous polynomial in ''n'' variables may depend only on [[SL(n)|SL(''n'')]]-invariants of the polynomial. One such invariant is the [[discriminant]],\nzeros of which mark the singularities of the integral. However, the integral may also depend on other invariants.<ref name=\"morozov2009\">{{cite journal\n | last = Morozov | first = A.\n | last2 = Shakirove | first2= Sh.\n | journal = Journal of High Energy Physics\n | pages = 002\n | title = Introduction to integral discriminants\n | doi = 10.1088/1126-6708/2009/12/002\n | volume = 12\n | year = 2009 | arxiv = 0903.2595\n }}</ref>\n\nExponentials of other even polynomials can numerically be solved using series. These may be interpreted as [[formal calculation]]s when there is no convergence. For example, the solution to the integral of the exponential of a quartic polynomial is{{citation needed|date=August 2015}}\n\n: <math>\\int_{-\\infty}^{\\infty} e^{a x^4+b x^3+c x^2+d x+f}\\,dx = \\frac{1}{2} e^f \\sum_{\\begin{smallmatrix}n,m,p=0 \\\\ n+p=0 \\mod 2\\end{smallmatrix}}^{\\infty} \\frac{b^n}{n!} \\frac{c^m}{m!} \\frac{d^p}{p!} \\frac{\\Gamma \\left (\\frac{3n+2m+p+1}{4} \\right)}{(-a)^{\\frac{3n+2m+p+1}4}}.</math>\n\nThe ''n''&nbsp;+&nbsp;''p'' = 0 mod 2 requirement is because the integral from −∞ to 0 contributes a factor of (−1)<sup>''n''+''p''</sup>/2 to each term, while the integral from 0 to +∞ contributes a factor of 1/2 to each term. These integrals turn up in subjects such as [[quantum field theory]].\n\n==See also==\n* [[List of integrals of Gaussian functions]]\n* [[Common integrals in quantum field theory]]\n* [[Normal distribution]]\n* [[List of integrals of exponential functions]]\n* [[Error function]]\n* [[Grassmann integral]]\n\n== References ==\n=== Citations ===\n{{Reflist}}\n\n=== Sources ===\n{{refbegin}}\n* {{MathWorld |title = Gaussian Integral |urlname = GaussianIntegral }}\n* {{cite book |first=David |last=Griffiths |title=Introduction to Quantum Mechanics |edition=2nd }}\n* {{cite book |last=Abramowitz |first=M. |last2=Stegun |first2=I. A. |title = Handbook of Mathematical Functions |publisher=Dover Publications |location=New York }}\n{{refend}}\n\n{{-}}\n{{integral}}\n\n[[Category:Integrals]]\n[[Category:Articles containing proofs]]\n[[Category:Gaussian function]]\n[[Category:Theorems in analysis]]"
    },
    {
      "title": "List of integrals of Gaussian functions",
      "url": "https://en.wikipedia.org/wiki/List_of_integrals_of_Gaussian_functions",
      "text": "In these expressions,\n\n:<math>\\phi(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2} x^2}</math>\n\nis the [[standard normal]] probability density function,\n\n:<math>\\Phi(x) = \\int_{-\\infty}^x \\phi(t) \\, dt = \\frac{1}{2}\\left(1 + \\operatorname{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right)</math>\n\nis the corresponding [[cumulative distribution function]] (where '''erf''' is the [[error function]]) and\n\n:<math> T(h,a) = \\phi(h)\\int_0^a \\frac{\\phi(hx)}{1+x^2} \\, dx</math>\n\nis [[Owen's T function]].\n\nOwen <ref group=nb>{{harvtxt|Owen|1980}}</ref> has an extensive list of Gaussian-type integrals; only a subset is given below.\n\n== Indefinite integrals ==\n:<math>\\int \\phi(x) \\, dx = \\Phi(x) + C</math>\n:<math>\\int x \\phi(x) \\, dx = -\\phi(x) + C</math>\n:<math>\\int x^2 \\phi(x) \\, dx  = \\Phi(x) - x\\phi(x) + C</math>\n:<math>\\int x^{2k+1} \\phi(x) \\, dx = -\\phi(x) \\sum_{j=0}^k \\frac{(2k)!!}{(2j)!!}x^{2j} + C</math><ref group=\"nb\">{{harvtxt|Patel|Read|1996}} lists this integral above without the minus sign, which is an error. See calculation by [http://www.wolframalpha.com/input/?fp=1&i=D(-e^(-x^2/2)/sqrt(2pi)*Sum((2k)!!/(2j)!!*x^(2j),{j,0,k}),x)&s=40&incTime=true WolframAlpha]</ref>\n:<math>\\int x^{2k+2} \\phi(x) \\, dx = -\\phi(x)\\sum_{j=0}^k\\frac{(2k+1)!!}{(2j+1)!!}x^{2j+1} + (2k+1)!!\\,\\Phi(x) + C</math>\n\nIn these integrals, ''n''!! is the [[double factorial]]: for even ''n'' it is equal to the product of all even numbers from 2 to ''n'', and for odd ''n'' it is the product of all odd numbers from 1 to ''n'' ; additionally it is assumed that {{nowrap|1=0!! = (−1)!! = 1}}.\n\n: <math> \\int \\phi(x)^2 \\, dx           = \\frac{1}{2\\sqrt{\\pi}} \\Phi\\left(x\\sqrt{2}\\right) + C </math>\n: <math> \\int \\phi(x)\\phi(a + bx) \\, dx = \\frac{1}{t}\\phi\\left(\\frac{a}{t}\\right)\\Phi\\left(tx + \\frac{ab}{t}\\right) + C, \\qquad t = \\sqrt{1+b^2}</math><ref group=nb>{{harvtxt|Patel|Read|1996}} report this integral with error, see [http://www.wolframalpha.com/input/?i=Integrate(1/sqrt(2pi)*e^(-x^2/2)*1/sqrt(2pi)*e^(-(a%2Bb*x)^2/2),x) WolframAlpha]</ref>\n: <math> \\int x\\phi(a+bx) \\, dx         = -\\frac{1}{b^2}\\left (\\phi(a+bx) + a\\Phi(a+bx)\\right) + C </math>\n: <math> \\int x^2\\phi(a+bx) \\, dx       = \\frac{1}{b^3} \\left ((a^2+1)\\Phi(a+bx) + (a-bx)\\phi(a+bx) \\right ) + C </math>\n: <math> \\int \\phi(a+bx)^n \\, dx        = \\frac{1}{b\\sqrt{n(2\\pi)^{n-1}}} \\Phi\\left(\\sqrt{n}(a+bx)\\right) + C </math>\n: <math> \\int \\Phi(a+bx) \\, dx          = \\frac{1}{b} \\left ((a+bx)\\Phi(a+bx) + \\phi(a+bx)\\right) + C </math>\n: <math> \\int x\\Phi(a+bx) \\, dx         = \\frac{1}{2b^2}\\left((b^2x^2 - a^2 - 1)\\Phi(a+bx) + (bx-a)\\phi(a+bx)\\right) + C </math>\n: <math> \\int x^2\\Phi(a+bx) \\, dx       = \\frac{1}{3b^3}\\left((b^3x^3 + a^3 + 3a)\\Phi(a+bx) + (b^2x^2-abx+a^2+2)\\phi(a+bx)\\right) + C </math>\n: <math> \\int x^n \\Phi(x) \\, dx         = \\frac{1}{n+1}\\left( \\left (x^{n+1}-nx^{n-1} \\right )\\Phi(x) + x^n\\phi(x) + n(n-1)\\int x^{n-2}\\Phi(x)\\,dx \\right) + C </math>\n: <math> \\int x\\phi(x)\\Phi(a+bx) \\, dx  = \\frac{b}{t}\\phi\\left(\\frac{a}{t}\\right)\\Phi\\left(xt + \\frac{ab}{t}\\right) - \\phi(x)\\Phi(a+bx) + C, \\qquad t = \\sqrt{1+b^2} </math>\n: <math> \\int \\Phi(x)^2 \\, dx           = x \\Phi(x)^2 + 2\\Phi(x)\\phi(x) - \\frac{1}{\\sqrt{\\pi}}\\Phi\\left(x\\sqrt{2}\\right) + C </math>\n: <math> \\int e^{cx}\\phi(bx)^n \\, dx = \\frac{e^{\\frac{c^2}{2nb^2}}}{b\\sqrt{n(2\\pi)^{n-1}}}\\Phi \\left (\\frac{b^2xn-c }{b\\sqrt{n}} \\right ) + C, \\qquad b\\ne 0, n>0 </math>\n\n== Definite integrals ==\n\n: <math> \\int_{-\\infty}^\\infty x^2\\phi(x)^n \\, dx = \\frac{1}{\\sqrt{n^3(2\\pi)^{n-1}}} </math>\n: <math>\\int_{-\\infty}^0 \\phi(ax)\\Phi(bx)dx = \\frac{1}{2\\pi |a|}\\left(\\frac{\\pi}{2}-\\arctan\\left(\\frac{b}{|a|}\\right)\\right) </math> \n: <math>\\int_0^{\\infty} \\phi(ax)\\Phi(bx) \\, dx = \\frac{1}{2\\pi |a|}\\left(\\frac{\\pi}{2} + \\arctan\\left(\\frac{b}{|a|}\\right)\\right) </math> \n: <math> \\int_0^\\infty x\\phi(x)\\Phi(bx) \\, dx = \\frac{1}{2\\sqrt{2\\pi}} \\left( 1 + \\frac{b}{\\sqrt{1+b^2}} \\right) </math> \n: <math> \\int_0^\\infty x^2\\phi(x)\\Phi(bx) \\, dx = \\frac{1}{4} + \\frac{1}{2\\pi} \\left(\\frac{b}{1+b^2} + \\arctan(b) \\right) </math> \n: <math> \\int_0^\\infty x \\phi(x)^2\\Phi(x) \\, dx = \\frac{1}{4\\pi\\sqrt{3}} </math>\n: <math> \\int_0^\\infty \\Phi(bx)^2 \\phi(x) \\, dx = \\frac{1}{2\\pi}\\left( \\arctan(b) + \\arctan \\sqrt{1+2b^2} \\right) </math> \n: <math> \\int_{-\\infty}^\\infty \\Phi(a+bx)^2 \\phi(x) \\,dx = \\Phi\\left( \\frac{a}{\\sqrt{1+b^2}} \\right)-2T\\left( \\frac{a}{\\sqrt{1+b^2}}, \\frac{1}{\\sqrt{1+2b^2}} \\right) </math> \n: <math> \\int_{-\\infty}^{\\infty} x \\Phi(a+bx)^2 \\phi(x) \\,dx = \\frac{2b}{\\sqrt{1+b^2}} \\phi\\left(\\frac{a}{t}\\right) \\Phi\\left(\\frac{a}{\\sqrt{1+b^2}\\sqrt{1+2b^2}}\\right)</math><ref group=nb>{{harvtxt|Patel|Read|1996}} report this integral incorrectly by omitting ''x'' from the integrand</ref>\n: <math> \\int_{-\\infty}^\\infty \\Phi(bx)^2 \\phi(x) \\, dx = \\frac{1}{\\pi}\\arctan \\sqrt{1+2b^2} </math> \n: <math> \\int_{-\\infty}^\\infty x\\phi(x)\\Phi(bx) \\, dx = \\int_{-\\infty}^\\infty x\\phi(x)\\Phi(bx)^2 \\, dx = \\frac{b}{\\sqrt{2\\pi(1+b^2)}} </math> \n: <math> \\int_{-\\infty}^\\infty \\Phi(a+bx)\\phi(x) \\, dx = \\Phi\\left(\\frac{a}{\\sqrt{1+b^2}}\\right) </math> \n: <math> \\int_{-\\infty}^\\infty x\\Phi(a+bx)\\phi(x) \\, dx = \\frac{b}{t}\\phi\\left(\\frac{a}{t}\\right), \\qquad t = \\sqrt{1+b^2} </math>\n: <math> \\int_0^\\infty x\\Phi(a+bx)\\phi(x) \\, dx =\\frac{b}{t}\\phi\\left(\\frac{a}{t}\\right)\\Phi\\left(-\\frac{ab}{t}\\right) + \\frac{1}{\\sqrt{2\\pi}}\\Phi(a), \\qquad t = \\sqrt{1+b^2}  </math> \n: <math> \\int_{-\\infty}^\\infty \\ln(x^2) \\frac{1}{\\sigma}\\phi\\left(\\frac{x}{\\sigma}\\right) \\, dx = \\ln(\\sigma^2) - \\gamma - \\ln 2 \\approx \\ln(\\sigma^2) - 1.27036 </math>\n\n== References ==\n{{reflist|group=nb}}\n\n* {{cite book\n  | last1 = Patel | first1 = Jagdish K.\n  | last2 = Read  | first2 = Campbell B.\n  | year = 1996\n  | title = Handbook of the normal distribution | edition = 2nd\n  | publisher = CRC Press\n  | isbn = 0-8247-9342-0\n  | ref = harv\n  }}\n\n* {{cite article\n  | last1 = Owen | first1 = D.\n  | year = 1980\n  | title = A table of normal integrals\n  | journal = Communications in Statistics: Simulation and Computation\n  | pages = 389–419\n  | volume = B9\n  | ref = harv\n  }}\n\n{{Lists of integrals}}\n\n{{DEFAULTSORT:Integrals of Gaussian functions}}\n[[Category:Integrals|Gaussian functions]]\n[[Category:Mathematics-related lists]]\n[[Category:Gaussian function]]"
    },
    {
      "title": "Inverted bell curve",
      "url": "https://en.wikipedia.org/wiki/Inverted_bell_curve",
      "text": "In statistics, an '''inverted bell curve''' is a term used loosely or metaphorically to refer to a [[Multimodal distribution|bimodal distribution]] that falls to a trough between two peaks, rather than (as in a standard [[Normal distribution|bell curve]]) rising to a single peak and then falling off on both sides.<ref>E.g., see {{citation|journal=Journal of College Student Retention: Research, Theory and Practice|volume=16|issue=3|date=2014–2015|pages=307–324|title=Bimodal Inverted Bell Grade Distribution: Implications for Instruction and Student Retention|first1=Leah K.|last1=Gensheimer|first2=Charles T.|last2=Diebold|doi=10.2190/CS.16.3.a}}.</ref>\n\n==References==\n{{reflist}}\n\n\n{{stats-stub}}\n\n[[Category:Continuous distributions]]\n[[Category:Gaussian function]]"
    },
    {
      "title": "Scale space implementation",
      "url": "https://en.wikipedia.org/wiki/Scale_space_implementation",
      "text": "{{ScaleSpaceNavbox}}\n\n{{context|date=August 2011}}\n\nThe '''linear [[scale space|scale-space representation]]''' of an ''N''-dimensional continuous signal,\n\n:<math>f_C \\left (x_1, \\cdots, x_N, t \\right),</math>\n\nis obtained by [[convolution|convolving]] ''f<sub>C</sub>'' with an ''N''-dimensional Gaussian kernel:\n\n:<math>g_N \\left (x_1, \\cdots, x_N, t \\right).</math>\n\nIn other words:\n\n:<math>L \\left (x_1, \\cdots, x_N, t \\right ) =\\int_{u_1=-\\infty}^{\\infty} \\cdots \\int_{u_N=-\\infty}^{\\infty} f_C \\left (x_1-u_1, \\cdots, x_N-u_N, t \\right ) \\cdot g_N \\left(u_1, \\cdots, u_N, t \\right) \\, du_1 \\cdots du_N.</math>\n\nHowever, for '''implementation''', this definition is impractical, since it is continuous.  When applying the scale space concept to a discrete signal ''f<sub>D</sub>'', different approaches can be taken. This article is a brief summary of some of the most frequently used methods.\n\n==Separability==\nUsing the ''separability property'' of the Gaussian kernel\n\n:<math>g_N \\left (x_1,\\dots, x_N, t \\right) = G\\left(x_1, t \\right) \\cdots G\\left(x_N, t\\right) </math>\n\nthe ''N''-dimensional [[convolution]] operation can be decomposed into a set of separable smoothing steps with a one-dimensional Gaussian kernel ''G'' along each dimension\n\n:<math>L(x_1, \\cdots, x_N, t) = \\int_{u_1=-\\infty}^{\\infty} \\cdots \\int_{u_N=-\\infty}^{\\infty} f_C(x_1-u_1, \\cdots, x_N-u_N, t) G(u_1, t) \\, du_1 \\cdots G(u_N, t) \\, du_N,</math>\n\nwhere\n\n:<math>G(x, t) = \\frac {1}{\\sqrt{2\\pi t}} e^{-\\frac{x^2}{2t}}</math>\n\nand the standard deviation of the Gaussian σ is related to the scale parameter ''t'' according to ''t'' = σ<sup>2</sup>.\n\nSeparability will be assumed in all that follows, even when the kernel is not exactly Gaussian, since separation of the dimensions is the most practical way to implement multidimensional smoothing, especially at larger scales.  Therefore, '''the rest of the article focuses on the one-dimensional case.'''\n\n==The sampled Gaussian kernel==\nWhen implementing the one-dimensional smoothing step in practice, the presumably simplest approach is to convolve the discrete signal ''f<sub>D</sub>'' with a ''sampled Gaussian kernel'':\n\n:<math>L(x, t) = \\sum_{n=-\\infty}^{\\infty} f(x-n) \\, G(n, t)</math>\n\nwhere\n\n:<math>G(n, t) = \\frac {1}{\\sqrt{2\\pi t}} e^{-\\frac{n^2}{2t}}</math>\n\n(with ''t'' = σ<sup>2</sup>) which in turn is truncated at the ends to give a filter with finite impulse response\n\n:<math>L(x, t) = \\sum_{n=-M}^{M} f(x-n) \\, G(n, t)</math>\n\nfor ''M'' chosen sufficiently large (see [[error function]]) such that\n\n:<math>2 \\int_M^{\\infty} G(u, t) \\, du = 2 \\int_{\\frac{M}{\\sqrt{t}}}^{\\infty} G(v, 1) \\, dv < \\varepsilon.</math>\n\nA common choice is to set ''M'' to a constant ''C'' times the standard deviation of the Gaussian kernel\n\n:<math>M = C \\sigma + 1 = C \\sqrt{t} + 1</math>\n\nwhere ''C'' is often chosen somewhere between 3 and 6.\n\nUsing the sampled Gaussian kernel can, however, lead to implementation problems, in particular when computing higher-order derivatives at finer scales by applying sampled derivatives of Gaussian kernels. When accuracy and robustness are primary design criteria, alternative implementation approaches should therefore be considered.\n\nFor small values of ε (10<sup>−6</sup> to 10<sup>−8</sup>) the errors introduced by truncating the Gaussian are usually negligible. For larger values of ε, however, there are many better alternatives to a rectangular [[window function]].  For example, for a given number of points, a [[Hamming window]], [[Blackman window]], or [[Kaiser window]] will do less damage to the spectral and other properties of the Gaussian than a simple truncation will. Notwithstanding this, since the Gaussian kernel decreases rapidly at the tails, the main recommendation is still to use a sufficiently small value of ε such that the truncation effects are no longer important.\n\n==The discrete Gaussian kernel==\n[[File:Discrete Gaussian kernel.svg|thumb|358x358px|The ideal discrete Gaussian kernel (solid) compared with sampled ordinary Gaussian (dashed), for scales ''t'' = [0.5, 1, 2, 4]]]\nA more refined approach is to convolve the original signal by the ''discrete Gaussian kernel'' ''T''(''n'', ''t'')<ref name=\"tpl90\">[http://www.nada.kth.se/~tony/abstracts/Lin90-PAMI.html Lindeberg, T., \"Scale-space for discrete signals,\" PAMI(12), No. 3, March 1990, pp. 234-254.]</ref><ref name=lin94>[http://www.csc.kth.se/~tony/book.html Lindeberg, T., Scale-Space Theory in Computer Vision, Kluwer Academic Publishers, 1994], {{ISBN|0-7923-9418-6}}</ref><ref name=\"HaddadAkansu\">R.A. Haddad and A.N. Akansu, \"A Class of Fast Gaussian Binomial Filters for Speech and Image Processing,\" IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 39, pp 723-727, March 1991.</ref>\n\n:<math>L(x, t) = \\sum_{n=-\\infty}^{\\infty} f(x-n) \\, T(n, t)</math>\n\nwhere\n\n:<math>T(n, t) = e^{-t} I_n(t)</math>\n\nand <math>I_n(t)</math> denotes the [[modified Bessel function]]s of integer order, ''n''. This is the discrete counterpart of the continuous Gaussian in that it is the solution to the discrete [[diffusion equation]] (discrete space, continuous time), just as the continuous Gaussian is the solution to the continuous diffusion equation.<ref name=\"tpl90\"/><ref name=lin94/><ref>Campbell, J, 2007, ''[https://dx.doi.org/10.1016/j.tpb.2007.08.001 The SMM model as a boundary value problem using the discrete diffusion equation]'', Theor Popul Biol. 2007 Dec;72(4):539-46.</ref>\n\nThis filter can be truncated in the spatial domain as for the sampled Gaussian\n\n:<math>L(x, t) = \\sum_{n=-M}^{M} f(x-n) \\, T(n, t)</math>\n\nor can be implemented in the Fourier domain using a closed-form expression for its [[discrete-time Fourier transform]]:\n\n:<math>\\widehat{T}(\\theta, t) = \\sum_{n=-\\infty}^{\\infty} T(n, t) \\, e^{-i \\theta n} = e^{t(\\cos \\theta - 1)}. </math>\n\nWith this frequency-domain approach, the scale-space properties transfer ''exactly'' to the discrete domain, or with excellent approximation using periodic extension and a suitably long [[discrete Fourier transform]] to approximate the [[discrete-time Fourier transform]] of the signal being smoothed. Moreover, higher-order derivative approximations can be computed in a straightforward manner (and preserving scale-space properties) by applying small support central difference operators to the discrete [[scale space representation]].<ref>[http://www.nada.kth.se/~tony/abstracts/Lin93-JMIV.html Lindeberg, T. Discrete derivative approximations with scale-space properties: A basis for low-level feature extraction, J. of Mathematical Imaging and Vision, 3(4), pp. 349--376, 1993.]</ref>\n\nAs with the sampled Gaussian, a plain truncation of the infinite impulse response will in most cases be a sufficient approximation for small values of ε, while for larger values of ε it is better to use either a decomposition of the discrete Gaussian into a cascade of generalized binomial filters or alternatively to construct a finite approximate kernel by multiplying by a [[window function]]. If ε has been chosen too large such that effects of the truncation error begin to appear (for example as spurious extrema or spurious responses to higher-order derivative operators), then the options are to decrease the value of ε such that a larger finite kernel is used, with cutoff where the support is very small, or to use a tapered window.\n\n==Recursive filters==\n[[Image:ScaleSpaceKernels.png|frame|right|Scale-space kernels. Ideal discrete gaussian based on bessel functions (red), and two-pole-pair forward/backward recursive smoothing filters (blue) with poles as described in the text. Top shows individual kernels, and bottom is their cumulative convolution with each other; ''t'' = [0.5, 1, 2, 4].]]\n\nSince computational efficiency is often important, low-order ''[[recursive filter]]s'' are often used for scale-space smoothing.  For example, Young and van Vliet<ref name=young>{{cite journal | url = http://citeseer.ist.psu.edu/young95recursive.html |author1=Ian T. Young  |author2=Lucas J. van Vliet  |lastauthoramp=yes | title = Recursive implementation of the Gaussian filter | journal = Signal Processing | volume = 44 | year = 1995 | pages =139–151 | doi = 10.1016/0165-1684(95)00020-E | issue = 2 |citeseerx=10.1.1.12.2826  }}</ref> use a third-order recursive filter with one real pole and a pair of complex poles, applied forward and backward to make a sixth-order symmetric approximation to the Gaussian with low computational complexity for any smoothing scale.\n\nBy relaxing a few of the axioms, Lindeberg<ref name=\"tpl90\"/> concluded that good smoothing filters would be \"normalized [[Pólya]] frequency sequences\", a family of discrete kernels that includes all filters with real poles at 0 < ''Z'' < 1  and/or ''Z'' > 1, as well as with real zeros at ''Z'' < 0.  For symmetry, which leads to approximate directional homogeneity, these filters must be further restricted to pairs of poles and zeros that lead to zero-phase filters.\n\nTo match the transfer function curvature at zero frequency of the discrete Gaussian, which ensures an approximate [[semi-group]] property of additive ''t'', two poles at\n\n:<math>Z = 1 + \\frac{2}{t} - \\sqrt{\\left (1 + \\frac{2}{t} \\right)^2 - 1}</math>\n\ncan be applied forward and backwards, for symmetry and stability.  This filter is the simplest implementation of a normalized Pólya frequency sequence kernel that works for any smoothing scale, but it is not as excellent an approximation to the Gaussian as Young and van Vliet's filter, which is ''not'' normalized Pólya frequency sequence, due to its complex poles.\n\nThe transfer function, ''H''<sub>1</sub>, of a symmetric pole-pair recursive filter is closely related to the [[discrete-time Fourier transform]] of the discrete Gaussian kernel via first-order approximation of the exponential:\n\n:<math>\\widehat{T}(\\theta, t) = \\frac{1}{e^{t(1 - \\cos \\theta)}} \\approx \\frac{1}{{1 +t(1 - \\cos \\theta)}} = H_1(\\theta, t),</math>\n\nwhere the ''t'' parameter here is related to the stable pole position ''Z'' = ''p'' via:\n\n:<math>t = \\frac{2p}{(1-p)^2}.</math>\n\nFurthermore, such filters with ''N'' pairs of poles, such as the two pole pairs illustrated in this section, are an even better approximation to the exponential:\n\n:<math>\\frac{1}{\\left (1 +\\frac{t}{N}(1 - \\cos \\theta)\\right)^N} = H_N(\\theta, t),</math>\n\nwhere the stable pole positions are adjusted by solving:\n\n:<math>\\frac{t}{N} = \\frac{2p}{(1-p)^2}.</math>\n\nThe impulse responses of these filters are not very close to gaussian unless more than two pole pairs are used.  However, even with only one or two pole pairs per scale, a signal successively smoothed at increasing scales will be very close to a gaussian-smoothed signal.  The semi-group property is poorly approximated when too few pole pairs are used.\n\n[[Scale-space axioms]] that are still satisfied by these filters are:\n\n*''linearity''\n*''shift invariance'' (integer shifts)\n*''non-creation of local extrema'' (zero-crossings) in one dimension\n*''non-enhancement of local extrema'' in any number of dimensions\n*''positivity''\n*''normalization''\n\nThe following are only approximately satisfied, the approximation being better for larger numbers of pole pairs:\n\n*existence of an ''infinitesimal generator'' ''A'' (the infinitesimal generator of the discrete Gaussian, or a filter approximating it, approximately maps a recursive filter response to one of infinitesimally larger ''t'')\n*the ''semi-group structure'' with the associated ''cascade smoothing property'' (this property is approximated by considering kernels to be equivalent when they have the same ''t'' value, even if they are not quite equal)\n*''rotational symmetry''\n*''scale invariance''\n\nThis recursive filter method and variations to compute both the Gaussian smoothing as well as Gaussian derivatives has been described by several authors.<ref name=young/><ref>[http://citeseer.ist.psu.edu/deriche93recursively.html Deriche, R: Recursively implementing the Gaussian and its derivatives, INRIA Research Report 1893, 1993.]</ref><ref>[http://www.dicklyon.com/tech/Scans/ICASSP87_ScaleSpace-Lyon.pdf Richard F. Lyon. \"Speech recognition in scale space,\" Proc. of 1987 ICASSP. San Diego, March, pp. 29.3.14, 1987.]</ref><ref>Jin, JS, Gao Y. \"Recursive implementation of LoG Filtering\". ''Real-Time Imaging'' 1997;3:59–65.</ref>  Tan ''et al.'' have analyzed and compared some of these approaches, and have pointed out that the Young and van Vliet filters are a cascade (multiplication) of forward and backward filters, while the Deriche and the Jin ''et al.'' filters are sums of forward and backward filters.<ref>[http://www.pspc.dibe.unige.it/ecovision/pubs/papers/tan-dale-johnston.pdf .] {{cite magazine | title = Performance of three recursive algorithms for fast  space-variant Gaussian filtering |author1=Sovira Tan |author2=Jason L. Dale |author3=Alan Johnston  |last-author-amp=yes |magazine= Real-Time Imaging | volume = 9 | year = 2003 | pages = 215–228 | doi = 10.1016/S1077-2014(03)00040-8 | issue = 3 }}</ref>\n\nAt fine scales, the recursive filtering approach as well as other separable approaches are not guaranteed to give the best possible approximation to rotational symmetry, so non-separable implementations for 2D images may be considered as an alternative.\n\nWhen computing several derivatives in the [[N-jet]] simultaneously, discrete scale-space smoothing with the discrete analogue of the Gaussian kernel, or with a recursive filter approximation, followed by small support difference operators, may be both faster and more accurate than computing recursive approximations of each derivative operator.\n\n==Finite-impulse-response (FIR) smoothers==\nFor small scales, a low-order FIR filter may be a better smoothing filter than a recursive filter.  The symmetric 3-kernel {{nowrap|[''t''/2, 1-''t'', ''t''/2]}}, for ''t''&nbsp;&le; 0.5 smooths to a scale of ''t'' using a pair of real zeros at ''Z''&nbsp;< 0, and approaches the discrete Gaussian in the limit of small ''t''.  In fact, with infinitesimal ''t'', either this two-zero filter or the two-pole filter with poles at ''Z''&nbsp;= ''t''/2 and ''Z''&nbsp;= 2/''t'' can be used as the infinitesimal generator for the discrete Gaussian kernels described above.\n\nThe FIR filter's zeros can be combined with the recursive filter's poles to make a general high-quality smoothing filter.  For example, if the smoothing process is to always apply a biquadratic (two-pole, two-zero) filter forward then backwards on each row of data (and on each column in the 2D case), the poles and zeros can each do a part of the smoothing.  The zeros limit out at ''t'' = 0.5 per pair (zeros at ''Z'' = –1), so for large scales the poles do most of the work.  At finer scales, the combination makes an excellent approximation to the discrete Gaussian if the poles and zeros each do about half the smoothing.  The ''t'' values for each portion of the smoothing (poles, zeros, forward and backward multiple applications, etc.) are additive, in accordance with the approximate semi-group property.\n\n[[Image:Smoothing Filter Pole Zero Plot.svg|center|thumb|380px|''Z''-plane locations of four poles (X) and four zeros (circles) for a smoothing filter using forward/backward biquad to smooth to a scale ''t'' = 2, with half the smoothing from the poles and half from the zeros. The zeros are all at ''Z'' = –1; the poles are at ''Z'' = 0.172 and ''Z'' = 5.83.  The poles outside the unit circle are implemented by filtering backwards with the stable poles.]]\n\nThe FIR filter transfer function is closely related to the discrete Gaussian's DTFT, just as was the recursive filter's.  For a single pair of zeros, the transfer function is\n\n:<math>\\widehat{T}(\\theta, t) = e^{-t(1 - \\cos \\theta)} \\approx {1 -t(1 - \\cos \\theta)} = F_1(\\theta, t),</math>\n\nwhere the ''t'' parameter here is related to the zero positions ''Z'' = ''z'' via:\n\n:<math>t = -\\frac{2z}{(1-z)^2},</math>\n\nand we require ''t''&nbsp;&le; 0.5 to keep the transfer function non-negative.\n\nFurthermore, such filters with ''N'' pairs of zeros, are an even better approximation to the exponential and extend to higher values of ''t'' :\n\n:<math>\\left(1 -\\frac{t}{N}(1 - \\cos \\theta) \\right)^N = F_N(\\theta, t),</math>\n\nwhere the stable zero positions are adjusted by solving:\n\n:<math>\\frac{t}{N} = -\\frac{2z}{(1-z)^2}.</math>\n\nThese FIR and pole-zero filters are valid scale-space kernels, satisfying the same axioms as the all-pole recursive filters.\n\n==Real-time implementation within pyramids and discrete approximation of scale-normalized derivatives==\nRegarding the topic of automatic scale selection based on normalized derivatives, [[pyramid (image processing)|pyramid approximations]] are frequently used to obtain real-time performance.<ref>{{cite book |author1=Lindeberg, Tony  |author2=Bretzner, Lars  |lastauthoramp=yes | year = 2003 | title = Real-time scale selection in hybrid multi-scale representations | journal = Proc. Scale-Space'03, Springer Lecture Notes in Computer Science\n | volume = 2695 | pages = 148–163 | doi = 10.1007/3-540-44935-3_11 | isbn = 978-3-540-40368-5 | url = http://www.nada.kth.se/cvap/abstracts/cvap279.html | series = Lecture Notes in Computer Science }}</ref><ref>[http://www-prima.inrialpes.fr/Prima/Homepages/jlc/papers/Crowley-ScaleSpace03.pdf Crowley, J, Riff O: Fast computation of scale  normalised Gaussian receptive fields, Proc. Scale-Space'03, Isle of Skye, Scotland, Springer Lecture Notes in Computer Science, volume 2695, 2003.]</ref><ref>[http://citeseer.ist.psu.edu/lowe04distinctive.html Lowe, D. G., “Distinctive image features from scale-invariant keypoints”, International Journal of Computer Vision, 60, 2, pp. 91-110, 2004.]</ref> The appropriateness of approximating scale-space operations within a pyramid originates from the fact that repeated cascade smoothing with generalized binomial kernels leads to equivalent smoothing kernels that under reasonable conditions approach the Gaussian. Furthermore, the binomial kernels (or more generally the class of generalized binomial kernels) can be shown to constitute the unique class of finite-support kernels that guarantee non-creation of local extrema or zero-crossings with increasing scale (see the article on [[multi-scale approaches]] for details). Special care may, however, need to be taken to avoid discretization artifacts.\n\n==Other multi-scale approaches==\nFor one-dimensional kernels, there is a well-developed theory of [[multi-scale approaches]], concerning filters that do not create new local extrema or new zero-crossings with increasing scales. For continuous signals, filters with real poles in the ''s''-plane are within this class, while for discrete signals the above-described recursive and FIR filters satisfy these criteria. Combined with the strict requirement of a continuous semi-group structure, the continuous Gaussian and the discrete Gaussian constitute the unique choice for continuous and discrete signals.\n\nThere are many other multi-scale signal processing, image processing and data compression techniques, using [[wavelets]] and a variety of other kernels, that do not exploit or require the [[scale-space axioms|same requirements]] as [[scale space]] descriptions do; that is, they do not depend on a coarser scale not generating a new extremum that was not present at a finer scale (in 1D) or non-enhancement of local extrema between adjacent scale levels (in any number of dimensions).\n\n==See also==\n\n*[[scale space]]\n*[[pyramid (image processing)]]\n*[[multi-scale approaches]]\n*[[Gaussian filter]]\n\n==References==\n\n<references/>\n\n[[Category:Image processing]]\n[[Category:Computer vision]]\n[[Category:Gaussian function]]"
    },
    {
      "title": "Sheppard's correction",
      "url": "https://en.wikipedia.org/wiki/Sheppard%27s_correction",
      "text": "In statistics, '''Sheppard's corrections''' are approximate corrections to [[estimation theory|estimates]] of [[Moment (mathematics)|moment]]s computed from [[Data binning|bin]]ned data.  The concept is named after [[William Fleetwood Sheppard]].\n\nLet <math>m_k</math> be the measured ''k''<sup>th</sup> moment, <math>\\hat{\\mu}_k</math> the corresponding corrected moment, and <math>c</math> the [[class interval]] (bin width). No correction is necessary for the mean (first moment about zero). The first few measured and corrected moments about the mean are then related as follows:   \n\n: <math>\n\\begin{align}\n\\hat{\\mu}_2 &= m_2 - \\frac{1}{12} c^2 \\\\\n\\hat{\\mu}_3 &= m_3 \\\\\n\\hat{\\mu}_4 &= m_4 - \\frac{1}{2} m_2c^2 + \\frac{7}{240} c^4.\n\\end{align}\n</math>\n\nWhen the data come from a normally distributed population, then binning and using the midpoint of the bin as the observed value results in an overestimate of the variance.  That is why the correction to the variance is negative.  The reason why the uncorrected estimate of the variance is an overestimate is that the [[errors and residuals|error]] is negatively correlated with the observation.  For the uniform distribution, the error is uncorrelated with the observation, so a correction should be&nbsp;+''c''<sup>2</sup>/12, which is the variance of the error itself rather than&nbsp;−''c''<sup>2</sup>/12.  Thus Sheppard's correction is biased in favor of population distributions in which the error is negatively correlated with the observation.\n\nThe [[cumulant]]s of the sum of the grouped variable and the uniform variable are the sums of the cumulants. As odd cumulants of a uniform distribution are zero; only even moments are affected.\n\nThe second and fourth cumulants of the uniform distribution on (−0.5''c'',&nbsp;0.5''c'') are respectively, ''c''<sup>2</sup>/12 and&nbsp;−''c''<sup>4</sup>/120.\n\nThe correction to moments can be derived from the relation between cumulants and moments.\n\n== References ==\n* {{cite web |url=http://mathworld.wolfram.com/SheppardsCorrection.html |title=Sheppard's Correction|last=Weisstein |first=Eric W. |date= |website=[[MathWorld]]—A Wolfram Web Resource |publisher= |accessdate=March 2, 2014}}\n* {{cite |author=Weatherburn, C.E.|title=A first course in mathematical statistics|publisher=Cambridge University Press|year=1949}}\n{{statistics-stub}}\n\n[[Category:Gaussian function]]\n[[Category:Estimation theory]]"
    },
    {
      "title": "Generalized normal distribution",
      "url": "https://en.wikipedia.org/wiki/Generalized_normal_distribution",
      "text": "The '''generalized normal distribution''' or '''generalized Gaussian distribution (GGD)''' is either of two families of [[parametric statistics|parametric]] [[continuous probability distribution]]s on the [[real number|real]] line.  Both families add a [[shape parameter]] to the [[normal distribution]].  To distinguish the two families, they are referred to below as \"version 1\" and \"version 2\".  However this is not a standard nomenclature.\n\n==Version 1==\n\n{{Probability distribution |\n  name       =Generalized Normal (version 1)|\n  type       =density|\n  pdf_image  =[[File:Generalized normal densities.svg|325px|Probability density plots of generalized normal distributions]]|\n  cdf_image  =[[File:Generalized normal cdfs.svg|325px|Cumulative distribution function plots of generalized normal distributions]]|\n  parameters =<math> \\mu \\,</math> [[location parameter|location]] ([[real number|real]])<br/><math> \\alpha \\,</math> [[scale parameter|scale]] (positive, [[real number|real]])<br/><math> \\beta \\,</math> [[shape parameter|shape]] (positive, [[real number|real]])|\n  support    =<math>x \\in (-\\infty; +\\infty)\\!</math>|\n  pdf        =<math>\\frac{\\beta}{2\\alpha\\Gamma(1/\\beta)} \\; e^{-(|x-\\mu|/\\alpha)^\\beta}</math> <br/><br/><math>\\Gamma</math> denotes the [[gamma function]]|\n  cdf        =<math>\\frac{1}{2} + \\sgn(x-\\mu)\\frac{\\gamma\\left[1/\\beta, \\left( \\frac{|x-\\mu|}{\\alpha} \\right)^\\beta\\right]}{2\\Gamma(1/\\beta)}  </math> <br/>\n<math>\\gamma</math> denotes the lower [[incomplete gamma function]]|\n  mean       =<math> \\mu \\,</math>|\n  median     =<math> \\mu \\,</math>|\n  mode       =<math> \\mu \\,</math>|\n  variance   =<math>\\frac{\\alpha^2\\Gamma(3/\\beta)}{\\Gamma(1/\\beta)}</math>|\n  skewness   =0|\n  kurtosis   =<math>\\frac{\\Gamma(5/\\beta)\\Gamma(1/\\beta)}{\\Gamma(3/\\beta)^2}-3</math>|\n  entropy    =<math>\\frac{1}{\\beta}-\\log\\left[\\frac{\\beta}{2\\alpha\\Gamma(1/\\beta)}\\right]</math><ref>{{cite journal |last= Nadarajah|first= Saralees|authorlink= |date=September 2005|title= A generalized normal distribution|journal= Journal of Applied Statistics|volume= 32 |issue= 7|pages= 685&ndash;694|doi= 10.1080/02664760500079464 |url=  |quote= }}</ref>|\n  mgf        =<!-- to do -->|\n  char       =<!-- to do -->|\n}}\n\nKnown also as the '''exponential power distribution''', or the '''generalized error distribution''', this is a parametric family of symmetric distributions.  It includes all [[normal distribution|normal]] and [[Laplace distribution|Laplace]] distributions, and as limiting cases it includes all [[continuous uniform distribution]]s on bounded intervals of the real line.\n\nThis family includes the [[normal distribution]] when <math>\\textstyle\\beta=2</math> (with mean <math>\\textstyle\\mu</math> and variance <math>\\textstyle \\frac{\\alpha^2}{2}</math>) and it includes the [[Laplace distribution]] when <math>\\textstyle\\beta=1</math>. As <math>\\textstyle\\beta\\rightarrow\\infty</math>, the density [[pointwise convergence|converges pointwise]] to a uniform density on <math>\\textstyle (\\mu-\\alpha,\\mu+\\alpha)</math>.\n\nThis family allows for tails that are either heavier than normal (when <math>\\beta<2</math>) or lighter than normal (when <math>\\beta>2</math>).  It is a useful way to parametrize a continuum of symmetric, [[Platykurtic#Terminology and examples|platykurtic]] densities spanning from the normal (<math>\\textstyle\\beta=2</math>) to the uniform density (<math>\\textstyle\\beta=\\infty</math>), and a continuum of symmetric, [[Platykurtic#Terminology and examples|leptokurtic]] densities spanning from the Laplace (<math>\\textstyle\\beta=1</math>) to the normal density (<math>\\textstyle\\beta=2</math>).\n\n===Parameter estimation===\n\nParameter estimation via [[maximum likelihood estimation|maximum likelihood]] and the [[method of moments (statistics)|method of moments]] has been studied.<ref>{{cite journal |last= Varanasi |first= M.K. |authorlink= |author2=Aazhang, B. |date=October 1989|title= Parametric generalized Gaussian density estimation|journal= Journal of the Acoustical Society of America|volume= 86|issue= 4|pages= 1404&ndash;1415|id= |url=  |doi= 10.1121/1.398700}}</ref>  The estimates do not have a closed form and must be obtained numerically.  Estimators that do not require numerical calculation have also been proposed.<ref>\n{{cite journal |last= Domínguez-Molina |first= J. Armando|authorlink= |author2=González-Farías, Graciela |author3=Rodríguez-Dagnino, Ramón M. | title= A practical procedure to estimate the shape parameter in the generalized Gaussian distribution | url= http://www.cimat.mx/reportes/enlinea/I-01-18_eng.pdf |accessdate=2009-03-03 }}</ref>\n\nThe generalized normal log-likelihood function has infinitely many continuous derivates (i.e. it belongs to the class C<sup>∞</sup> of [[smooth function]]s) only if <math>\\textstyle\\beta</math> is a positive, even integer.  Otherwise, the function has <math>\\textstyle\\lfloor \\beta \\rfloor</math> continuous derivatives.  As a result, the standard results for consistency and asymptotic normality of [[maximum likelihood]] estimates of <math>\\beta</math> only apply when <math>\\textstyle\\beta\\ge 2</math>.\n\n==== Maximum likelihood estimator ====\nIt is possible to fit the generalized normal distribution adopting an approximate [[maximum likelihood]] method.<ref>{{cite journal |last= Varanasi|first= M.K.|author2=Aazhang B. |year= 1989|title= Parametric generalized Gaussian density estimation|journal= [[J. Acoust. Soc. Am.]] |volume= 86|issue= 4|pages= 1404&ndash;1415|id= |url=  |doi= 10.1121/1.398700}}</ref><ref>{{cite journal |last= Do |first= M.N.|author2=Vetterli, M. |date=February 2002|title= Wavelet-based Texture Retrieval Using Generalised Gaussian Density and Kullback-Leibler Distance|journal= Transaction on Image Processing|volume= 11|issue= 2|pages= 146&ndash;158|id= |url=  |doi= 10.1109/83.982822|pmid= 18244620}}</ref> With <math>\\mu</math> initially set to the sample first moment <math>m_1</math>, \n<math>\\textstyle\\beta</math> is estimated by using a [[Newton's method|Newton–Raphson]] iterative procedure, starting from an initial guess of <math>\\textstyle\\beta=\\textstyle\\beta_0</math>,\n:<math>\\beta _0 = \\frac{m_1}{\\sqrt{m_2}},</math>\nwhere\n:<math>m_1={1 \\over N} \\sum_{i=1}^N |x_i|,</math>\nis the first statistical [[Moment (mathematics)|moment]] of the absolute values and <math>m_2</math> is the second statistical [[Moment (mathematics)|moment]]. The iteration is\n\n:<math>\\beta _{i+1} = \\beta _{i} - \\frac{g(\\beta _{i})}{g'(\\beta _{i})} ,</math>\n\nwhere\n\n:<math>g(\\beta)= 1 + \\frac{\\psi(1/\\beta)}{\\beta} - \\frac{\\sum_{i=1}^{N} |x_i-\\mu|^\\beta \\log|x_i-\\mu| }{\\sum_{i=1}^{N} |x_i-\\mu|^\\beta} +  \\frac{\\log( \\frac{\\beta}{N} \\sum_{i=1}^{N} |x_i-\\mu|^\\beta)}{\\beta} ,</math>\nand\n\n: <math>\n\\begin{align}\ng'(\\beta) = {} & -\\frac{\\psi(1/\\beta)}{\\beta^2} - \\frac{\\psi'(1/\\beta)}{\\beta^3} + \\frac{1}{\\beta^2} - \\frac{\\sum_{i=1}^N |x_i-\\mu|^\\beta (\\log|x_i-\\mu|)^2}{\\sum_{i=1}^N |x_i-\\mu|^\\beta} \\\\[6pt]\n& {} + \\frac{\\left(\\sum_{i=1}^N |x_i-\\mu|^\\beta \\log|x_i-\\mu|\\right)^2}{\\left(\\sum_{i=1}^N |x_i-\\mu|^\\beta \\right)^2} + \\frac{\\sum_{i=1}^N |x_i-\\mu|^\\beta \\log|x_i-\\mu|}{\\beta \\sum_{i=1}^N |x_i-\\mu|^\\beta} \\\\[6pt]\n& {} - \\frac{\\log\\left(\\frac{\\beta}{N} \\sum_{i=1}^N |x_i-\\mu|^\\beta \\right)}{\\beta^2},\n\\end{align}\n</math>\n\nand where <math>\\psi</math> and <math>\\psi'</math> are the [[digamma function]] and [[trigamma function]].\n\nGiven a value for <math>\\textstyle\\beta</math>, it is possible to estimate <math>\\mu</math> by finding the minimum of:\n\n:<math> \\min_\\mu = \\sum_{i=1}^{N} |x_i-\\mu|^\\beta</math>\n\nFinally <math>\\textstyle\\alpha</math> is evaluated as\n\n:<math>\\alpha = \\left( \\frac{\\beta}{N} \\sum_{i=1}^N|x_i-\\mu|^\\beta\\right)^{1/\\beta} .</math>\n\n===Applications===\n\n\n\nThis version of the generalized normal distribution has been used in modeling when the concentration of values around the mean and the tail behavior are of particular interest.<ref>\n{{cite journal\n |last        = Liang\n |first       = Faming\n |authorlink  = \n |author2     = Liu, Chuanhai\n |author3     = Wang, Naisyin | author3-link = Naisyin Wang\n |date        = April 2007\n |title       = A robust sequential Bayesian method for identification of differentially expressed genes\n |journal     = Statistica Sinica\n |volume      = 17\n |issue       = 2\n |pages       = 571&ndash;597\n |id          = \n |url         = http://www3.stat.sinica.edu.tw/statistica/password.asp?vol=17&num=2&art=8\n |accessdate  = 2009-03-03\n |quote       = \n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20071009233343/http://www3.stat.sinica.edu.tw/statistica/password.asp?vol=17&num=2&art=8\n |archivedate = 2007-10-09\n |df          = \n}}</ref><ref>\n{{cite book |title= Bayesian Inference in Statistical Analysis |last= Box |first= George E. P.|authorlink= George E. P. Box |author2=Tiao, George C. |year= 1992 |publisher= Wiley|location= New York|isbn= 978-0-471-57428-6|page= |pages= |url= }}</ref>  Other families of distributions can be used if the focus is on other deviations from normality.  If the [[Symmetric distribution|symmetry]] of the distribution is the main interest, the [[skew normal distribution|skew normal]] family or version 2 of the generalized normal family discussed below can be used.  If the tail behavior is the main interest, the [[student t distribution|student t]] family can be used, which approximates the normal distribution as the degrees of freedom grows to infinity.  The t distribution, unlike this generalized normal distribution, obtains heavier than normal tails without acquiring a [[cusp (singularity)|cusp]] at the origin.\n\n===Properties===\n\n==== Moments ====\n\nLet <math> X_\\beta </math> be zero mean generalized Gaussian distribution of order <math> \\beta </math> and scaling paramater <math> \\alpha </math> .  The  moments of <math> X_\\beta </math>  exist and are finite for any k  greater than −1.  For any non-negative integer  k, the plain central moments are\n\n: <math>\n    \\operatorname{E}\\left[X^k_\\beta\\right] =\n      \\begin{cases}\n        0 & \\text{if }k\\text{ is odd,} \\\\\n         \\frac{2^{  \\frac{k}{\\beta}  } \\alpha^{k}  \\Gamma  \\left( \\frac{k+1}{\\beta}  \\right)  }{\\Gamma \\left( \\frac{1}{\\beta} \\right)} & \\text{if }k\\text{ is even.}\n      \\end{cases}\n  </math>\n\n====Connection to Positive-Definite Functions ====\n\nThe probability density function of this version of the generalized normal distribution is a [[positive-definite function]]  for  <math>\\beta \\in (0,2]</math>.<ref>\n{{cite journal\n |last        = Dytso\n |first       = Alex\n |authorlink  = \n |author2     = Bustin, Ronit\n |author3     = Poor, H. Vincent \n |author4     = Shamai, Shlomo\n |date        = 2018\n |title       = Analytical properties of generalized Gaussian distributions\n |journal     = Journal of Statistical Distributions and Applications\n |volume      = 5\n |issue       = 1\n |pages       = 6\n |id          = \n |url         = https://jsdajournal.springeropen.com/articles/10.1186/s40488-018-0088-5\n}}</ref><ref>\n{{cite journal\n |last        = Bochner\n |first       = Salomon\n |date        = 1937\n |title       = Stable laws of probability and completely monotone functions\n |journal     = Duke Mathematical Journal\n |volume      = 3\n |issue       = 4\n |pages       = 726--728\n |id          = \n |url         = https://jsdajournal.springeropen.com/articles/10.1186/s40488-018-0088-5\n}}</ref>\n\n\n==== Infinite divisibility  ====\nThis version of generalized Gaussian distribution is an [[infinitely divisible distribution]] if and only if <math> \\beta \\in (0,1] \\cup \\{ 2\\} </math>.<ref>\n{{cite journal\n |last        = Dytso\n |first       = Alex\n |authorlink  = \n |author2     = Bustin, Ronit\n |author3     = Poor, H. Vincent \n |author4     = Shamai, Shlomo\n |date        = 2018\n |title       = Analytical properties of generalized Gaussian distributions\n |journal     = Journal of Statistical Distributions and Applications\n |volume      = 5\n |issue       = 1\n |pages       = 6\n |id          = \n |url         = https://jsdajournal.springeropen.com/articles/10.1186/s40488-018-0088-5\n}}</ref>\n\n===Generalizations ===\nThe multivariate generalized normal distribution, i.e. the product of <math>n</math> exponential power distributions with the same <math>\\beta</math> and <math>\\alpha</math> parameters, is the only probability density that can be written in the form <math>p(\\mathbf x)=g(\\|\\mathbf x\\|_\\beta)</math> and has independent marginals.<ref>{{cite journal |last= Sinz|first= Fabian|authorlink= |author2=Gerwinn, Sebastian |author3=Bethge, Matthias\n|date=May 2009|title=Characterization of the p-Generalized Normal Distribution. |journal=Journal of Multivariate Analysis|volume= 100|issue= 5|pages= 817&ndash;820|id= |accessdate= |quote= |doi=10.1016/j.jmva.2008.07.006}}</ref> The results for the special case of the [[Multivariate normal distribution]] is originally attributed to [[James Clerk Maxwell|Maxwell]].<ref>{{cite journal |last= Kac|first= M.|authorlink= |year= 1939|title=On a characterization of the normal distribution|journal=American Journal of Mathematics|volume= 61|issue= 3|pages= 726&ndash;728|id= |url= |accessdate= |quote= |doi= 10.2307/2371328 |jstor= 2371328}}</ref>\n\n==Version 2==\n\n{{Probability distribution |\n  name       =Generalized Normal (version 2)|\n  type       =density|\n  pdf_image  =[[File:Generalized normal densities 2.svg|325px|Probability density plots of generalized normal distributions]]|\n  cdf_image  =[[File:Generalized normal cdfs 2.svg|325px|Cumulative distribution function plots of generalized normal distributions]]|\n  parameters =<math> \\xi \\,</math> [[location parameter|location]] ([[real number|real]])<br/><math> \\alpha \\,</math> [[scale parameter|scale]] (positive, [[real number|real]])<br/><math> \\kappa \\,</math> [[shape parameter|shape]] ([[real number|real]])|\n  support    =<math>x \\in (-\\infty,\\xi+\\alpha/\\kappa) \\text{ if } \\kappa>0</math><br/><math>x \\in (-\\infty,\\infty) \\text{ if } \\kappa=0</math><br/><math>x \\in (\\xi+\\alpha/\\kappa; +\\infty) \\text{ if } \\kappa<0</math>|\n  pdf        =<math> \\frac{\\phi(y)}{\\alpha-\\kappa(x-\\xi)}</math>, where <br/><math>y = \\begin{cases} - \\frac{1}{\\kappa} \\log \\left[ 1- \\frac{\\kappa(x-\\xi)}{\\alpha} \\right] & \\text{if } \\kappa \\neq 0 \\\\ \\frac{x-\\xi}{\\alpha} & \\text{if } \\kappa=0 \\end{cases} </math><br><math>\\phi</math> is the standard [[normal distribution|normal]] [[probability distribution function|pdf]]|\n  cdf        =<math> \\Phi(y) </math>, where <br/><math> y = \\begin{cases} - \\frac{1}{\\kappa} \\log \\left[ 1- \\frac{\\kappa(x-\\xi)}{\\alpha} \\right] & \\text{if } \\kappa \\neq 0 \\\\ \\frac{x-\\xi}{\\alpha} & \\text{if } \\kappa=0 \\end{cases} </math><br><math>\\Phi</math> is the standard [[normal distribution|normal]] [[cumulative distribution function|CDF]]|\n  mean       =<math>\\xi - \\frac{\\alpha}{\\kappa} \\left( e^{\\kappa^2/2} - 1 \\right)</math>|\n  median     =<math>\\xi  \\,</math>|\n  mode       =<!-- to do -->|\n  variance   =<math>\\frac{\\alpha^2}{\\kappa^2} e^{\\kappa^2} \\left( e^{\\kappa^2} - 1 \\right)</math>|\n  skewness   =<math>\\frac{3 e^{\\kappa^2} - e^{3 \\kappa^2} - 2}{(e^{\\kappa^2} - 1)^{3/2}} \\text{ sign}(\\kappa) </math>|\n  kurtosis   =<math>e^{4 \\kappa^2} + 2 e^{3 \\kappa^2} + 3 e^{2 \\kappa^2} - 6 </math>|\n  entropy    =<!-- to do -->|\n  mgf        =<!-- to do -->|\n  char       =<!-- to do -->|\n}}\n\nThis is a family of continuous probability distributions in which the shape parameter can be used to introduce skew.<ref>Hosking, J.R.M., Wallis, J.R. (1997) ''Regional frequency analysis: an approach based on L-moments'', Cambridge University Press. {{ISBN|0-521-43045-3}}. Section A.8</ref><ref>[http://www.cran.r-project.org/web/packages/lmomco/lmomco.pdf Documentation for the lmomco R package]</ref>  When the shape parameter is zero, the normal distribution results.  Positive values of the shape parameter yield left-skewed distributions bounded to the right, and negative values of the shape parameter yield right-skewed distributions bounded to the left. Only when the shape parameter is zero is the density function for this distribution positive over the whole real line: in this case the distribution is a [[normal distribution]], otherwise the distributions are shifted and possibly reversed [[log-normal distribution]]s.\n\n===Parameter estimation===\n\nParameters can be estimated via [[maximum likelihood estimation]] or the method of moments.  The parameter estimates do not have a closed form, so numerical calculations must be used to compute the estimates.  Since the sample space (the set of real numbers where the density is non-zero) depends on the true value of the parameter, some standard results about the performance of parameter estimates will not automatically apply when working with this family.\n\n===Applications===\n\nThis family of distributions can be used to model values that may be normally distributed, or that may be either right-skewed or left-skewed relative to the normal distribution.  The [[skew normal distribution]] is another distribution that is useful for modeling deviations from normality due to skew.  Other distributions used to model skewed data include the [[gamma distribution|gamma]], [[lognormal distribution|lognormal]], and [[Weibull distribution|Weibull]] distributions, but these do not include the normal distributions as special cases.\n\n==Other distributions related to the normal==\n\nThe two generalized normal families described here, like the [[skew normal distribution|skew normal]] family, are parametric families that extends the normal distribution by adding a shape parameter.  Due to the central role of the normal distribution in probability and statistics, many distributions can be characterized in terms of their relationship to the normal distribution.  For example, the [[lognormal distribution|lognormal]], [[folded normal distribution|folded normal]], and [[inverse normal distribution|inverse normal]] distributions are defined as transformations of a normally-distributed value, but unlike the generalized normal and skew-normal families, these do not include the normal distributions as special cases.<br />\nActually all distributions with finite variance are in the limit highly related to the normal distribution. The Student-t distribution, the [[Irwin–Hall distribution]] and the [[Bates distribution]] also extend the normal distribution, and ''include'' in the limit the normal distribution. So there is no strong reason to prefer the \"generalized\" normal distribution of type 1, e.g. over a combination of Student-t and a normalized extended Irwin–Hall – this would include e.g. the triangular distribution (which cannot be modeled by the generalized Gaussian type 1).\n<br />\nA symmetric distribution which can model both tail (long and short) ''and'' center behavior (like flat, triangular or Gaussian) completely independently could be derived e.g. by using&nbsp;''X''&nbsp;=&nbsp;IH/chi.\n\n==See also==\n* [[Skew normal distribution]]\n\n==References==\n{{reflist}}\n\n{{ProbDistributions|continuous}}\n{{Statistics|hide}}\n\n{{DEFAULTSORT:Generalized Normal Distribution}}\n[[Category:Continuous distributions]]\n[[Category:Normal distribution| ]]"
    },
    {
      "title": "Normal distribution",
      "url": "https://en.wikipedia.org/wiki/Normal_distribution",
      "text": "{{About|the univariate normal distribution|normally distributed vectors|Multivariate normal distribution|normally distributed matrices|Matrix normal distribution}}\n{{redirect|Bell curve}}\n\n{{use mdy dates|date=August 2012}}\n\n{{Probability distribution\n  | name       = Normal Distribution\n  | type       = density\n  | pdf_image  = [[File:Normal Distribution PDF.svg|340px|Probability density function for the normal distribution]]<br /><small>The red curve is the ''standard normal distribution''</small>\n  | cdf_image  = [[File:Normal Distribution CDF.svg|340px|Cumulative distribution function for the normal distribution]]\n  | notation   = <math>\\mathcal{N}(\\mu,\\sigma^2)</math>\n  | parameters = <math>\\mu\\in\\R</math> = mean ([[location parameter|location]])<br /><math>\\sigma^2>0</math> = variance (squared [[scale parameter|scale]])\n  | support    = <math>x\\in\\R</math>\n  | pdf        = <math>\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}}</math>\n  | cdf        = <math>\\frac{1}{2}\\left[1 + \\operatorname{erf}\\left( \\frac{x-\\mu}{\\sigma\\sqrt{2}}\\right)\\right] </math>\n  | quantile   = <math>\\mu+\\sigma\\sqrt{2} \\operatorname{erf}^{-1}(2F-1)</math>\n  | mean       = <math>\\mu</math>\n  | median     = <math>\\mu</math>\n  | mode       = <math>\\mu</math>\n  | variance   = <math>\\sigma^2</math>\n  | skewness   = <math>0</math>\n  | kurtosis   = <math>0</math> <!-- DO NOT REPLACE THIS WITH THE OLD-STYLE KURTOSIS WHICH IS 3. -->\n  | entropy    = <math>\\frac{1}{2} \\log(2\\pi e\\sigma^2)</math>\n  | mgf        = <math>\\exp(\\mu t + \\sigma^2t^2/2)</math>\n  | char       = <math>\\exp(i\\mu t - \\sigma^2 t^2/2)</math>\n  | fisher     = <math>\\mathcal{I}(\\mu,\\sigma) =\\begin {pmatrix} 1/\\sigma^2 & 0 \\\\ 0 & 2/\\sigma^2\\end{pmatrix}</math> \n<math>\\mathcal{I}(\\mu,\\sigma^2) =\\begin {pmatrix} 1/\\sigma^2 & 0 \\\\ 0 & 1/(2\\sigma^4)\\end{pmatrix}</math>\n  | KLDiv = <math>D_\\text{KL}(\\mathcal{N}_0 \\| \\mathcal{N}_1) = { 1 \\over 2 } \\{ (\\sigma_0/\\sigma_1)^2 + \\frac{(\\mu_1 - \\mu_0)^2}{\\sigma_1^2} - 1 + 2 \\ln {\\sigma_1 \\over \\sigma_0} \\}</math>\n  }}\n\nIn [[probability theory]], the '''normal''' (or '''Gaussian''' or '''Gauss''' or '''Laplace–Gauss''') '''distribution''' is a very common [[continuous probability distribution]]. Normal distributions are important in [[statistics]] and are often used in the [[natural science|natural]] and [[social science]]s to represent real-valued [[random variable]]s whose distributions are not known.<ref>[http://www.encyclopedia.com/topic/Normal_Distribution.aspx#3 ''Normal Distribution''], Gale Encyclopedia of Psychology</ref><ref>{{harvtxt |Casella |Berger |2001 |p=102 }}</ref> A [[random variable]] with a Gaussian distribution is said to be '''normally distributed''' and is called a '''normal [[Deviate (statistics)|deviate]]'''.\n\nThe normal distribution is useful because of the [[central limit theorem]]. In its most general form, under some conditions (which include finite [[variance]]), it states that averages of samples of observations of [[random variables]] independently drawn from independent distributions [[Convergence in distribution|converge in distribution]] to the normal, that is, they become normally distributed when the number of observations is sufficiently large. Physical quantities that are expected to be the sum of many independent processes (such as [[measurement error]]s) often have distributions that are nearly normal.<ref>Lyon, A. (2014). [https://aidanlyon.com/normal_distributions.pdf Why are Normal Distributions Normal?], The British Journal for the Philosophy of Science.</ref> Moreover, many results and methods (such as [[propagation of uncertainty]] and [[least squares]] parameter fitting) can be derived analytically in explicit form when the relevant variables are normally distributed.\n\nThe normal distribution is sometimes informally called the '''bell curve'''. However, many other distributions are bell-shaped (such as the [[Cauchy distribution|Cauchy]], [[Student's t-distribution|Student's ''t''-]], and [[logistic distribution|logistic]] distributions).\n\nThe [[probability density]] of the normal distribution is\n\n:<math>\nf(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2} } e^{ -\\frac{(x-\\mu)^2}{2\\sigma^2} }\n</math>\n\nwhere\n* <math>\\mu</math> is the [[mean]] or [[expected value|expectation]] of the distribution (and also its [[median]] and [[mode (statistics)|mode]]),\n* <math>\\sigma</math> is the [[standard deviation]], and\n* <math>\\sigma^2</math> is the [[variance]].\n\n== Definition ==\n\n=== Standard normal distribution ===\nThe simplest case of a normal distribution is known as the ''standard normal distribution''. This is a special case when <math>\\mu=0</math> and <math>\\sigma =1</math>, and it is described by this [[probability density function]]:\n\n:<math>\n\\varphi(x) = \\frac 1{\\sqrt{2\\pi}}e^{- \\frac 12 x^2}\n</math>\n\nThe factor <math>1/\\sqrt{2\\pi}</math> in this expression ensures that the total area under the curve <math>\\varphi(x)</math> is equal to one.{{NoteTag|For the proof see [[Gaussian integral]].}} The factor <math>1/2</math> in the exponent ensures that the distribution has unit variance (i.e. the variance is equal to one), and therefore also unit standard deviation. This function is symmetric around <math>x=0</math>, where it attains its maximum value <math>1/\\sqrt{2\\pi}</math> and has [[inflection point]]s at <math>x=+1</math> and <math>x=-1</math>.\n\nAuthors may differ also on which normal distribution should be called the \"standard\" one. [[Carl Friedrich Gauss|Gauss]] defined the standard normal as having variance <math>\\sigma^2 = 1/2</math>, that is\n\n:<math>\\varphi(x) = \\frac{e^{-x^2}}{\\sqrt\\pi}</math>\n\n[[Stephen Stigler|Stigler]]<ref>{{harvtxt |Stigler |1982 }}</ref> goes even further, defining the standard normal with variance <math>\\sigma^2 = 1/(2\\pi)</math> :\n\n:<math> \\varphi(x) = e^{-\\pi x^2} </math>\n\n=== General normal distribution ===\nEvery normal distribution is a version of the standard normal distribution whose domain has been stretched by a factor <math>\\sigma</math> (the standard deviation) and then translated by <math>\\mu</math> (the mean value):\n\n:<math>\nf(x \\mid \\mu, \\sigma^2) =\\frac 1 \\sigma \\varphi\\left(\\frac{x-\\mu} \\sigma \\right).\n</math>\n\nThe probability density must be scaled by <math>1/\\sigma</math> so that the integral is still&nbsp;1.\n\nIf <math>Z</math> is a [[standard normal deviate]], then <math>X=\\sigma Z + \\mu</math> will have a normal distribution with expected value <math>\\mu</math> and standard deviation <math>\\sigma</math>. Conversely, if <math>X</math> is a normal deviate with parameters <math>\\mu</math> and <math>\\sigma^2</math>, then <math>Z=(X-\\mu)/\\sigma</math> will have a standard normal distribution. This variate is called the standardized form of <math>X</math>\n\nEvery normal distribution is the exponential of a [[quadratic function]]:\n\n:<math>f(x) = e^{a x^2 + b x + c}</math>\n\nwhere <math>a<0</math> and <math>c=b^2/(4a)+\\ln(-a/\\pi)/2</math>. In this form, the mean value is <math>\\mu = -b/(2a)</math>, and the variance is <math>\\sigma^2=-1/(2a)</math>. For the standard normal distribution, <math>a=-1/2</math>, <math>b=0</math>, and <math>c=-\\ln(2\\pi)/2</math>.\n\n=== Notation ===\nThe probability density of the standard Gaussian distribution (standard normal distribution) (with zero mean and unit variance) is often denoted with the Greek letter <math>\\phi</math> ([[phi (letter)|phi]]).<ref>{{harvtxt |Halperin |Hartley |Hoel |1965 |loc=item 7 }}</ref> The alternative form of the Greek letter phi, <math>\\varphi</math>, is also used quite often.\n\nThe normal distribution is often referred to as <math>N(\\mu,\\sigma^2)</math> or <math>\\mathcal{N}(\\mu,\\sigma^2)</math>.<ref>{{harvtxt |McPherson |1990 |p=110 }}</ref> Thus when a random variable <math>X</math> is distributed normally with mean <math>\\mu</math> and variance <math>\\sigma^2</math>, one may write\n\n:<math>X \\sim \\mathcal{N}(\\mu,\\sigma^2).</math>\n\n===Alternative parameterizations ===\nSome authors advocate using the [[precision (statistics)|precision]] <math>\\tau</math> as the parameter defining the width of the distribution, instead of the deviation <math>\\sigma</math> or the variance <math>\\sigma^2</math>. The precision is normally defined as the reciprocal of the variance, <math>1/\\sigma^2</math>.<ref>{{harvtxt |Bernardo |Smith |2000 |page=121 }}</ref> The formula for the distribution then becomes\n\n:<math>f(x) = \\sqrt{\\frac\\tau{2\\pi}} e^{-\\tau(x-\\mu)^2/2}.</math>\n\nThis choice is claimed to have advantages in numerical computations when <math>\\sigma</math> is very close to zero and simplify formulas in some contexts, such as in the [[Bayesian statistics|Bayesian inference]] of variables with [[multivariate normal distribution]].\n\nAlso the reciprocal of the standard deviation <math>\\tau^\\prime=1/\\sigma</math> might be defined as the ''precision'' and the expression of the normal distribution becomes\n\n: <math>f(x) = \\frac{\\tau^\\prime}{\\sqrt{2\\pi}} e^{-(\\tau^\\prime)^2(x-\\mu)^2/2}.</math>\n\nAccording to Stigler, this formulation is advantageous because of a much simpler and easier-to-remember formula, and simple approximate formulas for the [[quantile]]s of the distribution.\n\nNormal distributions form an [[exponential family]] with [[natural parameter]]s <math> \\textstyle\\theta_1=\\frac{\\mu}{\\sigma^2}</math> and <math>\\textstyle\\theta_2=\\frac{-1}{2\\sigma^2}</math>, and natural statistics ''x'' and ''x''<sup>2</sup>. The dual, expectation parameters for normal distribution are {{nowrap|1=''η''<sub>1</sub> = ''μ''}} and {{nowrap|1=''η''<sub>2</sub> = ''μ''<sup>2</sup> + ''σ''<sup>2</sup>}}.\n\n== Properties ==\nThe normal distribution is the only [[absolute continuity|absolutely continuous]] distribution whose [[cumulant]]s beyond the first two (i.e., other than the mean and [[variance]]) are zero. It is also the continuous distribution with the [[maximum entropy probability distribution|maximum entropy]] for a specified mean and variance.<ref>{{cite book|last=Cover|first=Thomas M.|author2=Thomas, Joy A.|year=2006|title=Elements of Information Theory|publisher=John Wiley and Sons|page=254}}</ref><ref>{{cite journal|last1=Park|first1=Sung Y.|last2=Bera|first2=Anil K.|year=2009|title=Maximum Entropy Autoregressive Conditional Heteroskedasticity Model|journal=Journal of Econometrics|pages=219–230|url=http://www.wise.xmu.edu.cn/Master/Download/..%5C..%5CUploadFiles%5Cpaper-masterdownload%5C2009519932327055475115776.pdf|accessdate=2011-06-02|doi=10.1016/j.jeconom.2008.12.014|volume=150|issue=2|citeseerx=10.1.1.511.9750}}</ref> Geary has shown, assuming that the mean and variance are finite, that the normal distribution is the only distribution where the mean and variance calculated from a set of independent draws are independent of each other.<ref name=Geary1936>Geary RC(1936) The distribution of the \"Student's\" ratio for the non-normal samples\". Supplement to the Journal of the Royal Statistical Society 3 (2): 178–184</ref><ref name=Lukas1942>Lukas E (1942) A characterization of the normal distribution. Annals of Mathematical Statistics 13: 91–93</ref>\n\nThe normal distribution is a subclass of the [[elliptical distribution]]s. The normal distribution is [[Symmetric distribution|symmetric]] about its mean, and is non-zero over the entire real line. As such it may not be a suitable model for variables that are inherently positive or strongly skewed, such as the [[weight]] of a person or the price of a [[share (finance)|share]]. Such variables may be better described by other distributions, such as the [[log-normal distribution]] or the [[Pareto distribution]].\n\nThe value of the normal distribution is practically zero when the value <math>x</math> lies more than a few [[standard deviation]]s away from the mean (e.g., a spread of three standard deviations covers all but 0.27% of the total distribution). Therefore, it may not be an appropriate model when one expects a significant fraction of [[outlier]]s—values that lie many standard deviations away from the mean—and least squares and other [[statistical inference]] methods that are optimal for normally distributed variables often become highly unreliable when applied to such data. In those cases, a more [[heavy-tailed]] distribution should be assumed and the appropriate [[robust statistics|robust statistical inference]] methods applied.\n\nThe Gaussian distribution belongs to the family of [[stable distribution]]s which are the attractors of sums of [[Independent and identically distributed random variables|independent, identically distributed]] distributions whether or not the mean or variance is finite. Except for the Gaussian which is a limiting case, all stable distributions have heavy tails and infinite variance. It is one of the few distributions that are stable and that have probability density functions that can be expressed analytically, the others being the [[Cauchy distribution]] and the [[Lévy distribution]].\n\n=== Symmetries and derivatives ===\nThe normal distribution with density <math>f(x)</math> (mean <math>\\mu</math> and standard deviation <math>\\sigma > 0</math>) has the following properties:\n* It is symmetric around the point <math>x=\\mu,</math> which is at the same time the [[mode (statistics)|mode]], the [[median]] and the [[mean]] of the distribution.<ref name=\"PR2.1.4\">{{harvtxt |Patel |Read |1996 |loc=[2.1.4] }}</ref>\n* It is [[unimodal]]: its first [[derivative]] is positive for <math>x<\\mu,</math> negative for <math>x>\\mu,</math> and zero only at <math>x=\\mu.</math>\n* The area under the curve and over the <math>x</math>-axis is unity (i.e. equal to one).\n* Its density has two [[inflection point]]s (where the second derivative of <math>f</math> is zero and changes sign), located one standard deviation away from the mean, namely at <math>x=\\mu-\\sigma</math> and <math>x=\\mu+\\sigma.</math><ref name=\"PR2.1.4\" />\n* Its density is [[logarithmically concave function|log-concave]].<ref name=\"PR2.1.4\" />\n* Its density is infinitely [[differentiable function|differentiable]], indeed [[supersmooth]] of order 2.<ref>{{harvtxt |Fan |1991 |p=1258 }}</ref>\n\nFurthermore, the density <math>\\varphi</math> of the standard normal distribution (i.e. <math>\\mu=0</math> and <math>\\sigma=1</math>) also has the following properties:\n* Its first derivative is <math>\\varphi^\\prime(x)=-x\\varphi(x).</math>\n* Its second derivative is <math>\\varphi^{\\prime\\prime}(x)=(x^2-1)\\varphi(x)</math>\n* More generally, its {{mvar|n}}th derivative is <math>\\varphi^{(n)}(x) = (-1)^n\\operatorname{He}_n(x)\\varphi(x),</math> where <math>\\operatorname{He}_n(x)</math> is the {{mvar|n}}th (probabilist) [[Hermite polynomial]].<ref>{{harvtxt |Patel |Read |1996 |loc=[2.1.8] }}</ref>\n* The probability that a normally distributed variable <math>X</math> with known <math>\\mu</math> and <math>\\sigma</math> is in a particular set, can be calculated by using the fact that the fraction <math>Z = (X-\\mu)/\\sigma</math> has a standard normal distribution.\n\n=== Moments ===\n{{See also|List of integrals of Gaussian functions}}\nThe plain and absolute [[moment (mathematics)|moments]] of a variable <math>X</math> are the expected values of <math>X^p</math> and <math>|X|^p</math>, respectively. If the expected value <math>\\mu</math> of <math>X</math> is zero, these parameters are called ''central moments''.  Usually we are interested only in moments with integer order <math>\\ p</math>.\n\nIf <math>X</math> has a normal distribution, these moments exist and are finite for any <math>p</math> whose real part is greater than&nbsp;−1. For any non-negative integer <math>p</math>, the plain central moments are:<ref>{{cite book|last1=Papoulis|first1=Athanasios|title=Probability, Random Variables and Stochastic Processes (4th Edition)|page=148}}</ref>\n: <math>\n    \\operatorname{E}\\left[X^p\\right] =\n      \\begin{cases}\n        0 & \\text{if }p\\text{ is odd,} \\\\\n        \\sigma^p (p-1)!! & \\text{if }p\\text{ is even.}\n      \\end{cases}\n  </math>\nHere <math>n!!</math> denotes the [[double factorial]], that is, the product of all numbers from <math>n</math> to&nbsp;1 that have the same parity as <math>n.</math>\n\nThe central absolute moments coincide with plain moments for all even orders, but are nonzero for odd orders. For any non-negative integer <math>p,</math>\n\n:<math>\n    \\operatorname{E}\\left[|X|^p\\right] =\n      \\sigma^p (p-1)!! \\cdot \\left.\\begin{cases}\n        \\sqrt{\\frac{2}{\\pi}} & \\text{if }p\\text{ is odd} \\\\\n        1 & \\text{if }p\\text{ is even}\n      \\end{cases}\\right\\}\n    = \\sigma^p \\cdot \\frac{2^{p/2}\\Gamma\\left(\\frac{p+1} 2 \\right)}{\\sqrt\\pi}\n</math>\nThe last formula is valid also for any non-integer <math>p>-1.</math>  When the mean <math>\\mu \\ne 0,</math> the plain and absolute moments can be expressed in terms of [[confluent hypergeometric function]]s <math>{}_1F_1</math> and <math>U.</math>{{Citation needed|date=June 2010}}\n\n:<math>\n \\operatorname{E}\\left[X^p\\right]=\\sigma^p\\cdot (-i\\sqrt 2)^p U\\left(-\\frac{p}{2}, \\frac{1}{2}, -\\frac{1}{2} \\left( \\frac \\mu \\sigma \\right)^2 \\right),\n</math>\n:<math>\n \\operatorname{E}\\left[|X|^p \\right] =\\sigma^p \\cdot 2^{p/2} \\frac {\\Gamma\\left(\\frac{1+p} 2\\right)}{\\sqrt\\pi} {}_1F_1\\left( -\\frac{p}{2}, \\frac{1}{2}, -\\frac{1}{2} \\left( \\frac \\mu \\sigma \\right)^2 \\right).\n</math>\n\nThese expressions remain valid even if <math>p</math> is not integer. See also [[Hermite polynomials#\"Negative variance\"|generalized Hermite polynomials]].\n\n{| class=\"wikitable\" style=\"background:#fff; margin: auto;\"\n|-\n! Order !! Non-central moment !! Central moment\n|-\n| 1\n| <math>\\mu</math>\n| <math>0</math>\n|-\n| 2\n| <math>\\mu^2+\\sigma^2</math>\n| <math>\\sigma^2</math>\n|-\n| 3\n| <math>\\mu^3+3\\mu\\sigma^2</math>\n| <math>0</math>\n|-\n| 4\n| <math>\\mu^4+6\\mu^2\\sigma^2+3\\sigma^4</math>\n| <math>3\\sigma^4</math>\n|-\n| 5\n| <math>\\mu^5+10\\mu^3\\sigma^2+15\\mu\\sigma^4</math>\n| <math>0</math>\n|-\n| 6\n| <math>\\mu^6+15\\mu^4\\sigma^2+45\\mu^2\\sigma^4+15\\sigma^6</math>\n| <math>15\\sigma^6</math>\n|-\n| 7\n| <math>\\mu^7+21\\mu^5\\sigma^2+105\\mu^3\\sigma^4+105\\mu\\sigma^6</math>\n| <math>0</math>\n|-\n| 8\n| <math>\\mu^8+28\\mu^6\\sigma^2+210\\mu^4\\sigma^4+420\\mu^2\\sigma^6+105\\sigma^8</math>\n| <math>105\\sigma^8</math>\n|}\n\nThe expectation of <math>X</math> conditioned on the event that <math>X</math> lies in an interval <math>[a,b]</math> is given by\n:<math>\n\\operatorname{E}\\left[X \\mid a<X<b \\right] = \\mu - \\sigma^2\\frac{f(b)-f(a)}{F(b)-F(a)} \n</math>\nwhere <math>f</math> and <math>F</math> respectively are the density and the cumulative distribution function of <math>X</math>. For <math>b=\\infty</math> this is known as the [[inverse Mills ratio]]. Note that above, density <math>f</math> of <math>X</math> is used instead of standard normal density as in inverse Mills ratio, so here we have <math>\\sigma^2</math> instead of <math>\\sigma</math>.\n\n=== Fourier transform and characteristic function ===\nThe [[Fourier transform]] of a normal density <math>f</math> with mean <math>\\mu</math> and standard deviation <math>\\sigma</math> is<ref>{{harvtxt |Bryc |1995 |p=23 }}</ref>\n\n:<math>\n\\hat f(t) = \\int_{-\\infty}^\\infty f(x)e^{-itx} \\, dx = e^{ -i\\mu t} e^{- \\frac12 (\\sigma t)^2}\n</math>\n\nwhere <math>i</math> is the [[imaginary unit]]. If the mean <math>\\mu=0</math>, the first factor is 1, and the Fourier transform is, apart from a constant factor, a normal density on the [[frequency domain]], with mean 0 and standard deviation <math>1/\\sigma</math>. In particular, the standard normal distribution <math>\\varphi</math> is an [[eigenfunction]] of the Fourier transform.\n\nIn probability theory, the Fourier transform of the probability distribution of a real-valued random variable <math>X</math> is closely connected to the [[characteristic function (probability theory)|characteristic function]] <math>\\varphi_X(t)</math> of that variable, which is defined as the [[expected value]] of <math>e^{itX}</math>, as a function of the real variable <math>t</math> (the [[frequency]] parameter of the Fourier transform). This definition can be analytically extended to a complex-value variable <math>t</math>.<ref>{{harvtxt |Bryc |1995 |p=24 }}</ref> The relation between both is:\n:<math>\\varphi_X(t) = \\hat f(-t)</math>\n\n=== Moment and cumulant generating functions ===\nThe [[moment generating function]] of a real random variable <math>X</math> is the expected value of <math>e^{tX}</math>, as a function of the real parameter <math>t</math>. For a normal distribution with density <math>f</math>, mean <math>\\mu</math> and deviation <math>\\sigma</math>, the moment generating function exists and is equal to\n\n:<math>M(t) = \\operatorname{E}[e^{tX}] = \\hat f(it) = e^{\\mu t} e^{\\tfrac12 \\sigma^2 t^2}</math>\n\nThe [[cumulant generating function]] is the logarithm of the moment generating function, namely\n\n:<math>g(t) = \\ln M(t) = \\mu t + \\tfrac 12 \\sigma^2 t^2</math>\n\nSince this is a quadratic polynomial in <math>t</math>, only the first two [[cumulant]]s are nonzero, namely the mean&nbsp;<math>\\mu</math> and the variance&nbsp;<math>\\sigma^2</math>.\n\n== Cumulative distribution function ==\nThe [[cumulative distribution function]] (CDF) of the standard normal distribution, usually denoted with the capital Greek letter <math>\\Phi</math> ([[phi (letter)|phi]]), is the integral\n\n:<math>\\Phi(x) = \\frac 1 {\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-t^2/2} \\, dt</math>\n\nThe related [[error function]] <math>\\operatorname{erf}(x)</math> gives the probability of a random variable with normal distribution of mean 0 and variance 1/2 falling in the range <math>[-x, x]</math>;<!-- SIC! The interval for erf is [−x,+x], NOT [0,x]. --> that is\n\n:<math>\\operatorname{erf}(x) = \\frac 2 {\\sqrt\\pi} \\int_0^x e^{-t^2} \\, dt</math>\n\nThese integrals cannot be expressed in terms of elementary functions, and are often said to be [[special function]]s. However, many numerical approximations are known; see [[#Numerical approximations for the normal CDF|below]].\n\nThe two functions are closely related, namely\n\n: <math> \\Phi(x) = \\frac{1}{2} \\left[1 + \\operatorname{erf}\\left( \\frac x {\\sqrt 2} \\right) \\right]</math>\n\nFor a generic normal distribution with density <math>f</math>, mean <math>\\mu</math> and deviation <math>\\sigma</math>, the cumulative distribution function is\n\n:<math>\nF(x) = \\Phi\\left(\\frac{x-\\mu} \\sigma \\right) = \\frac{1}{2} \\left[1 + \\operatorname{erf}\\left(\\frac{x-\\mu}{\\sigma \\sqrt 2 }\\right)\\right]\n</math>\n\nThe complement of the standard normal CDF, <math>Q(x) = 1 - \\Phi(x)</math>, is often called the [[Q-function]], especially in engineering texts.<ref>{{cite web|url=http://cnx.org/content/m11537/1.2/|last=Scott|first=Clayton|first2=Robert|last2=Nowak|title=The Q-function|work=Connexions|date=August 7, 2003}}</ref><ref>{{cite web|url=http://www.eng.tau.ac.il/~jo/academic/Q.pdf|last=Barak|first=Ohad|title=Q Function and Error Function|publisher=Tel Aviv University|date=April 6, 2006|deadurl=yes|archiveurl=https://web.archive.org/web/20090325160012/http://www.eng.tau.ac.il/~jo/academic/Q.pdf|archivedate=March 25, 2009|df=mdy-all}}</ref> It gives the probability that the value of a standard normal random variable <math>X</math> will exceed <math>x</math>: <math>P(X>x)</math>. Other definitions of the <math>Q</math>-function, all of which are simple transformations of <math>\\Phi</math>, are also used occasionally.<ref>{{MathWorld |urlname=NormalDistributionFunction |title=Normal Distribution Function }}</ref>\n\nThe [[graph of a function|graph]] of the standard normal CDF <math>\\Phi</math> has 2-fold [[rotational symmetry]] around the point (0,1/2); that is, <math>\\Phi(-x) = 1 - \\Phi(x)</math>. Its [[antiderivative]] (indefinite integral) is \n:<math>\\int \\Phi(x)\\, dx = x\\Phi(x) + \\varphi(x) + C.</math>\n\nThe CDF of the standard normal distribution can be expanded by [[Integration by parts]] into a series:\n\n:<math>\\Phi(x)=\\frac{1}{2} + \\frac{1}{\\sqrt{2\\pi}}\\cdot e^{-x^2/2} \\left[x + \\frac{x^3}{3} + \\frac{x^5}{3\\cdot 5} + \\cdots + \\frac{x^{2n+1}}{(2n+1)!!} + \\cdots\\right]</math>\n\nwhere <math>!!</math> denotes the [[double factorial]].\n\nAn [[asymptotic expansion]] of the CDF for large ''x'' can also be derived using integration by parts; see [[Error function#Asymptotic expansion]].<ref>{{AS ref|26, eqn 26.2.12|932}}</ref>\n\n=== Standard deviation and coverage ===\n{{further|Interval estimation|Coverage probability}}\n[[File:Empirical Rule.PNG|thumb|350px|For the normal distribution, the values less than one standard deviation away from the mean account for 68.27% of the set; while two standard deviations from the mean account for 95.45%; and three standard deviations account for 99.73%.]]\nAbout 68% of values drawn from a normal distribution are within one standard deviation ''σ'' away from the mean; about 95% of the values lie within two standard deviations; and about 99.7% are within three standard deviations. This fact is known as the [[68–95–99.7 rule|68-95-99.7 (empirical) rule]], or the ''3-sigma rule''.\n\nMore precisely, the probability that a normal deviate lies in the range between <math>\\mu-n\\sigma</math> and <math>\\mu+n\\sigma</math> is given by\n:<math>\nF(\\mu+n\\sigma) - F(\\mu-n\\sigma) = \\Phi(n)-\\Phi(-n) = \\operatorname{erf} \\left(\\frac{n}{\\sqrt{2}}\\right).\n</math>\nTo 12 significant figures, the values for <math>n=1,2,\\ldots , 6</math> are:<ref>{{cite web|url=http://www.wolframalpha.com/input/?i=Table%5B{N(Erf(n/Sqrt(2)),+12),+N(1-Erf(n/Sqrt(2)),+12),+N(1/(1-Erf(n/Sqrt(2))),+12)},+{n,1,6}%5D |title=Wolfram&#124;Alpha: Computational Knowledge Engine |website=Wolframalpha.com |date= |accessdate=2017-03-03}}</ref>\n\n{| class=\"wikitable\" style=\"text-align:center;margin-left:24pt\"\n|- \"\n! <math>n</math> !! <math>p= F(\\mu+n\\sigma) - F(\\mu-n\\sigma)</math> !! <math>\\text{i.e. }1-p</math>!! <math>\\text{or }1\\text{ in }p</math> !! [[OEIS]]\n|-\n|1 || {{val|0.682689492137}} || {{val|0.317310507863}} ||\n{| cellpadding=\"0\" cellspacing=\"0\" style=\"width: 16em;\"\n| style=\"text-align: right; width: 7em;\" | {{val|3}} || style=\"text-align: left; width: 9em;\" | {{#invoke:Gapnum|main|.15148718753}}\n|}\n|| {{OEIS2C|A178647}}\n|-\n|2 || {{val|0.954499736104}} || {{val|0.045500263896}} ||\n{| cellpadding=\"0\" cellspacing=\"0\" style=\"width: 16em;\"\n| style=\"text-align: right; width: 7em;\" | {{val|21}} || style=\"text-align: left; width: 9em;\" | {{#invoke:Gapnum|main|.9778945080}}\n|}\n|| {{OEIS2C|A110894}}\n|-\n|3 || {{val|0.997300203937}} || {{val|0.002699796063}} ||\n{| cellpadding=\"0\" cellspacing=\"0\" style=\"width: 16em;\"\n| style=\"text-align: right; width: 7em;\" | {{val|370}} || style=\"text-align: left; width: 9em;\" | {{#invoke:Gapnum|main|.398347345}}\n|}\n|| {{OEIS2C|A270712}}\n|-\n|4 || {{val|0.999936657516}} || {{val|0.000063342484}} ||\n{| cellpadding=\"0\" cellspacing=\"0\" style=\"width: 16em;\"\n| style=\"text-align: right; width: 7em;\" | {{val|15787}} || style=\"text-align: left; width: 9em;\" | {{#invoke:Gapnum|main|.1927673}}\n|}\n|-\n|5 || {{val|0.999999426697}} || {{val|0.000000573303}} ||\n{| cellpadding=\"0\" cellspacing=\"0\" style=\"width: 16em;\"\n| style=\"text-align: right; width: 7em;\" | {{val|1744277}} || style=\"text-align: left; width: 9em;\" | {{#invoke:Gapnum|main|.89362}}\n|}\n|-\n|6 || {{val|0.999999998027}} || {{val|0.000000001973}} ||\n{| cellpadding=\"0\" cellspacing=\"0\" style=\"width: 16em;\"\n| style=\"text-align: right; width: 7em;\" | {{val|506797345}} || style=\"text-align: left; width: 9em;\" | {{#invoke:Gapnum|main|.897}}\n|}\n|}\n\n=== Quantile function ===\n{{further|Quantile function#Normal distribution}}\n\nThe [[quantile function]] of a distribution is the inverse of the cumulative distribution function.  The quantile function of the standard normal distribution is called the [[probit function]], and can be expressed in terms of the inverse [[error function]]:\n:<math>\n\\Phi^{-1}(p) = \\sqrt2\\operatorname{erf}^{-1}(2p - 1), \\quad p\\in(0,1).\n</math>\nFor a normal random variable with mean <math>\\mu</math> and variance <math>\\sigma^2</math>, the quantile function is\n:<math>\nF^{-1}(p) = \\mu + \\sigma\\Phi^{-1}(p)\n= \\mu + \\sigma\\sqrt 2 \\operatorname{erf}^{-1}(2p - 1), \\quad p\\in(0,1).\n</math>\nThe [[quantile]] <math>\\Phi^{-1}(p)</math> of the standard normal distribution is commonly denoted as <math>z_p</math>. These values are used in [[hypothesis testing]], construction of [[confidence interval]]s and [[Q-Q plot]]s. A normal random variable <math>X</math> will exceed <math>\\mu + z_p\\sigma</math> with probability <math>1-p</math>, and will lie outside the interval <math>\\mu \\pm z_p\\sigma</math> with probability <math>2(1-p)</math>. In particular, the quantile <math>z_{0.975}</math> is [[1.96]]; therefore a normal random variable will lie outside the interval <math>\\mu \\pm 1.96\\sigma</math> in only 5% of cases.\n\nThe following table gives the quantile <math>z_p</math> such that <math>X</math> will lie in the range <math>\\mu \\pm z_p\\sigma</math> with a specified probability <math>p</math>. These values are useful to determine [[tolerance interval]] for [[Sample mean and sample covariance#Sample mean|sample averages]] and other statistical [[estimator]]s with normal (or [[asymptotic]]ally normal) distributions:.<ref>{{cite web |url=http://www.wolframalpha.com/input/?i=Table%5BSqrt%282%29*InverseErf%28x%29%2C+{x%2C+N%28{8%2F10%2C+9%2F10%2C+19%2F20%2C+49%2F50%2C+99%2F100%2C+995%2F1000%2C+998%2F1000}%2C+13%29}%5D|title=Wolfram&#124;Alpha: Computational Knowledge Engine|website=Wolframalpha.com|date=|accessdate=}}</ref><ref>{{cite web|url=http://www.wolframalpha.com/input/?i=Table%5B%7BN(1-10%5E(-x),9),N(Sqrt(2)*InverseErf(1-10%5E(-x)),13)%7D,%7Bx,3,9%7D%5D|title=Wolfram&#124;Alpha: Computational Knowledge Engine|website=Wolframalpha.com|date=|accessdate=2017-03-03}}</ref> NOTE: the following table shows <math>\\sqrt 2 \\operatorname{erf}^{-1}(p)=\\Phi^{-1}\\left(\\frac{p+1}{2}\\right)</math>, not <math>\\Phi^{-1}(p)</math> as defined above.\n\n{| class=\"wikitable\" style=\"text-align:left;margin-left:24pt;border:none;background:none;\"\n! <math>p</math> !! <math>z_p</math>\n| rowspan=\"8\" style=\"border:none;background:none;\"|&nbsp;\n! <math>p</math> !! <math>z_p</math>\n|-\n| 0.80  || {{val|1.281551565545}} || 0.999       || {{val|3.290526731492}}\n|-\n| 0.90  || {{val|1.644853626951}} || 0.9999      || {{val|3.890591886413}}\n|-\n| 0.95  || {{val|1.959963984540}} || 0.99999     || {{val|4.417173413469}}\n|-\n| 0.98  || {{val|2.326347874041}} || 0.999999    || {{val|4.891638475699}}\n|-\n| 0.99  || {{val|2.575829303549}} || 0.9999999   || {{val|5.326723886384}}\n|-\n| 0.995 || {{val|2.807033768344}} || 0.99999999  || {{val|5.730728868236}}\n|-\n| 0.998 || {{val|3.090232306168}} || 0.999999999 || {{val|6.109410204869}}\n|}\n\nFor small <math>p</math>, the quantile function has the useful asymptotic expansion \n<math>\\Phi^{-1}(p)=-\\sqrt{\\ln\\frac{1}{p^2}-\\ln\\ln\\frac{1}{p^2}-\\ln(2\\pi)}+\\mathcal{o}(1).</math>\n\n== Zero-variance limit ==\nIn the [[limit (mathematics)|limit]] when <math>\\sigma</math> tends to zero, the probability density <math>f(x)</math> eventually tends to zero at any <math>x\\ne \\mu</math>, but grows without limit if <math>x = \\mu</math>, while its integral remains equal to 1. Therefore, the normal distribution cannot be defined as an ordinary [[function (mathematics)|function]] when <math>\\sigma = 0</math>.\n\nHowever, one can define the normal distribution with zero variance as a [[generalized function]]; specifically, as [[Dirac delta function|Dirac's \"delta function\"]] <math>\\delta</math> translated by the mean <math>\\mu</math>, that is <math>f(x)=\\delta(x-\\mu).</math>\nIts CDF is then the [[Heaviside step function]] translated by the mean <math>\\mu</math>, namely\n:<math>F(x) = \n\\begin{cases}\n  0 & \\text{if }x < \\mu \\\\\n  1 & \\text{if }x \\geq \\mu\n\\end{cases}\n</math>\n\n== Central limit theorem ==\n[[File:De moivre-laplace.gif|right|thumb|250px|As the number of discrete events increases, the function begins to resemble a normal distribution]]\n[[File:Dice sum central limit theorem.svg|thumb|250px|Comparison of probability density functions, <math>p(k)</math> for the sum of <math>n</math> fair 6-sided dice to show their convergence to a normal distribution with increasing <math>na</math>, in accordance to the central limit theorem. In the bottom-right graph, smoothed profiles of the previous graphs are rescaled, superimposed and compared with a normal distribution (black curve).]]\n{{Main|Central limit theorem}}\n\nThe central limit theorem states that under certain (fairly common) conditions, the sum of many random variables will have an approximately normal distribution. More specifically, where <math>X_1,\\ldots ,X_n</math> are [[independent and identically distributed]] random variables with the same arbitrary distribution, zero mean, and variance <math>\\sigma^2</math> and <math>Z</math> is their\nmean scaled by <math>\\sqrt{n}</math>\n:<math> Z = \\sqrt{n}\\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right) </math>\nThen, as <math>n</math> increases, the probability distribution of <math>Z</math> will tend to the normal distribution with zero mean and variance <math>\\sigma^2</math>.\n\nThe theorem can be extended to variables <math>(X_i)</math> that are not independent and/or not identically distributed if certain constraints are placed on the degree of dependence and the moments of the distributions.\n\nMany [[test statistic]]s, [[score (statistics)|scores]], and [[estimator]]s encountered in practice contain sums of certain random variables in them, and even more estimators can be represented as sums of random variables through the use of [[influence function (statistics)|influence functions]].  The central limit theorem implies that those statistical parameters will have asymptotically normal distributions.\n\nThe central limit theorem also implies that certain distributions can be approximated by the normal distribution, for example:\n* The [[binomial distribution]] <math>B(n,p)</math> is [[De Moivre–Laplace theorem|approximately normal]] with mean <math>np</math> and variance <math>np(1-p)</math> for large <math>n</math> and for <math>p</math> not too close to 0 or 1.\n* The [[Poisson distribution]] with parameter <math>\\lambda</math> is approximately normal with mean <math>\\lambda</math> and variance <math>\\lambda</math>, for large values of <math>\\lambda</math>.<ref>{{cite web|url=http://www.stat.ucla.edu/~dinov/courses_students.dir/Applets.dir/NormalApprox2PoissonApplet.html|title=Normal Approximation to Poisson Distribution|website=Stat.ucla.edu|date=|accessdate=2017-03-03}}</ref>\n* The [[chi-squared distribution]] <math>\\chi^2(k)</math> is approximately normal with mean <math>k</math> and variance <math>2k</math>, for large <math>k</math>.\n* The [[Student's t-distribution]] <math>t(\\nu)</math> is approximately normal with mean 0 and variance 1 when <math>\\nu</math> is large.\n\nWhether these approximations are sufficiently accurate depends on the purpose for which they are needed, and the rate of convergence to the normal distribution. It is typically the case that such approximations are less accurate in the tails of the distribution.\n\nA general upper bound for the approximation error in the central limit theorem is given by the [[Berry–Esseen theorem]], improvements of the approximation are given by the [[Edgeworth expansion]]s.\n\n== Maximum entropy ==\nOf all probability distributions over the reals with a specified mean <math>\\mu</math> and variance&nbsp;<math>\\sigma^2</math>, the normal distribution <math>N(\\mu,\\sigma^2)</math> is the one with [[Maximum entropy probability distribution|maximum entropy]].<ref>{{harvtxt |Cover |Thomas |2006 |p=254 }}</ref> If <math>X</math> is a [[continuous random variable]] with [[probability density function|probability density]] <math>f(x)</math>, then the entropy of <math>X</math> is defined as<ref>{{cite book|last1=Williams|first1=David|title=Weighing the odds : a course in probability and statistics|date=2001|publisher=Cambridge Univ. Press|location=Cambridge [u.a.]|isbn=978-0-521-00618-7|pages=197–199|edition=Reprinted.}}</ref><ref>{{cite book|last1=Smith|first1=José M. Bernardo; Adrian F. M.|title=Bayesian theory|date=2000|publisher=Wiley|location=Chichester [u.a.]|isbn=978-0-471-49464-5|pages=209, 366|edition=Reprint}}</ref><ref>O'Hagan, A. (1994) ''Kendall's Advanced Theory of statistics, Vol 2B, Bayesian Inference'', Edward Arnold. {{isbn|0-340-52922-9}} (Section 5.40)</ref>\n:<math>\nH(X) = - \\int_{-\\infty}^\\infty f(x)\\log f(x)\\, dx=\\tfrac{1}{2}(1+\\log(2\\sigma^2\\pi))\n</math>\n\nwhere <math>f(x)\\log f(x)</math> is understood to be zero whenever <math>f(x)=0</math>. This functional can be maximized, subject to the constraints that the distribution is properly normalized and has a specified variance, by using [[variational calculus]]. A function with two [[Lagrange multipliers]] is defined:\n\n:<math>\nL=\\int_{-\\infty}^\\infty f(x)\\ln(f(x))\\,dx-\\lambda_0\\left(1-\\int_{-\\infty}^\\infty f(x)\\,dx\\right)-\\lambda\\left(\\sigma^2-\\int_{-\\infty}^\\infty f(x)(x-\\mu)^2\\,dx\\right)\n</math>\n\nwhere <math>f(x)</math> is, for now, regarded as some density function with mean <math>\\mu</math> and standard deviation <math>\\sigma</math>.\n\nAt maximum entropy, a small variation <math>\\delta f(x)</math> about <math>f(x)</math> will produce a variation <math>\\delta L</math> about <math>L</math> which is equal to 0:\n\n:<math>\n0=\\delta L=\\int_{-\\infty}^\\infty \\delta f(x)\\left (\\ln(f(x))+1+\\lambda_0+\\lambda(x-\\mu)^2\\right )\\,dx\n</math>\n\nSince this must hold for any small <math>\\delta f(x)</math>, the term in brackets must be zero, and solving for <math>f(x)</math> yields:\n\n:<math>f(x)=e^{-\\lambda_0-1-\\lambda(x-\\mu)^2}</math>\n\nUsing the constraint equations to solve for <math>\\lambda_0</math> and <math>\\lambda</math> yields the density of the normal distribution:\n\n:<math>\nf(x, \\mu, \\sigma)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n</math>\n\n== Operations on normal deviates ==\nThe family of normal distributions is closed under linear transformations: if ''X'' is normally distributed with mean ''μ'' and standard deviation ''σ'', then the variable {{nowrap|''Y'' {{=}}  ''aX'' + ''b''}}, for any real numbers ''a'' and ''b'', is also normally distributed, with\nmean ''aμ'' + ''b'' and standard deviation |''a''|''σ''.\n\nAlso if ''X''<sub>1</sub> and ''X''<sub>2</sub> are two [[independence (probability theory)|independent]] normal random variables, with means ''μ''<sub>1</sub>, ''μ''<sub>2</sub> and standard deviations ''σ''<sub>1</sub>, ''σ''<sub>2</sub>, then their sum {{nowrap|''X''<sub>1</sub> + ''X''<sub>2</sub>}} will also be normally distributed,<sup>[[sum of normally distributed random variables|[proof]]]</sup> with mean ''μ''<sub>1</sub> + ''μ''<sub>2</sub> and variance <math>\\sigma_1^2 + \\sigma_2^2</math>.\n\nIn particular, if ''X'' and ''Y'' are independent normal deviates with zero mean and variance ''σ''<sup>2</sup>, then {{nowrap|''X + Y''}} and {{nowrap|''X − Y''}} are also independent and normally distributed, with zero mean and variance 2''σ''<sup>2</sup>. This is a special case of the [[polarization identity]].<ref>{{harvtxt |Bryc |1995 |p=27 }}</ref>\n\nAlso, if ''X''<sub>1</sub>, ''X''<sub>2</sub> are two independent normal deviates with mean ''μ'' and deviation ''σ'', and ''a'', ''b'' are arbitrary real numbers, then the variable\n: <math>\n    X_3 = \\frac{aX_1 + bX_2 - (a+b)\\mu}{\\sqrt{a^2+b^2}} + \\mu\n  </math>\nis also normally distributed with mean ''μ'' and deviation ''σ''. It follows that the normal distribution is [[stable distribution|stable]] (with exponent ''α'' = 2).\n\nMore generally, any [[linear combination]] of independent normal deviates is a normal deviate.\n\n=== Infinite divisibility and Cramér's theorem ===\nFor any positive integer ''n'', any normal distribution with mean ''μ'' and variance ''σ''<sup>2</sup> is the distribution of the sum of ''n'' independent normal deviates, each with mean ''μ''/''n'' and variance ''σ''<sup>2</sup>/''n''.  This property is called [[infinite divisibility (probability)|infinite divisibility]].<ref>{{harvtxt |Patel |Read |1996 |loc=[2.3.6] }}</ref>\n\nConversely, if ''X''<sub>1</sub> and ''X''<sub>2</sub> are independent random variables and their sum {{nowrap|''X''<sub>1</sub> + ''X''<sub>2</sub>}} has a normal distribution, then both ''X''<sub>1</sub> and ''X''<sub>2</sub> must be normal deviates.<ref>{{harvtxt |Galambos |Simonelli |2004 |loc=Theorem&nbsp;3.5 }}</ref>\n\nThis result is known as [[Cramér’s decomposition theorem]], and is equivalent to saying that the [[convolution]] of two distributions is normal if and only if both are normal. Cramér's theorem implies that a linear combination of independent non-Gaussian variables will never have an exactly normal distribution, although it may approach it arbitrarily closely.<ref name=\"Bryc 1995 35\">{{harvtxt |Bryc |1995 |p=35 }}</ref>\n\n=== Bernstein's theorem ===\nBernstein's theorem states that if ''X'' and ''Y'' are independent and {{nowrap|''X'' + ''Y''}} and {{nowrap|''X'' − ''Y''}} are also independent, then both ''X'' and ''Y'' must necessarily have normal distributions.<ref name=LK>{{harvtxt |Lukacs |King |1954 }}</ref><ref>{{cite journal|last1=Quine|first1=M.P.|year=1993|title=On three characterisations of the normal distribution|url=http://www.math.uni.wroc.pl/~pms/publicationsArticle.php?nr=14.2&nrA=8&ppB=257&ppE=263|journal=Probability and Mathematical Statistics|volume=14|issue=2|pages=257–263}}</ref>\n\nMore generally, if ''X''<sub>1</sub>, ..., ''X<sub>n</sub>'' are independent random variables, then two distinct linear combinations ∑''a<sub>k</sub>X<sub>k</sub>'' and ∑''b<sub>k</sub>X<sub>k</sub>'' will be independent if and only if all ''X<sub>k</sub>''{{'}}s are normal and {{nowrap|∑''a<sub>k</sub>b<sub>k</sub>''{{SubSup|σ|''k''|2}} {{=}} 0}}, where {{SubSup|σ|''k''|2}} denotes the variance of ''X<sub>k</sub>''.<ref name=LK />\n\n== Other properties ==\n{{ordered list\n|1= If the characteristic function ''φ<sub>X</sub>'' of some random variable ''X'' is of the form {{nowrap|''φ<sub>X</sub>''(''t'') {{=}} ''e''<sup>''Q''(''t'')</sup>}}, where ''Q''(''t'') is a [[polynomial]], then the '''Marcinkiewicz theorem''' (named after [[Józef Marcinkiewicz]]) asserts that ''Q'' can be at most a quadratic polynomial, and therefore ''X'' is a normal random variable.<ref name=\"Bryc 1995 35\" /> The consequence of this result is that the normal distribution is the only distribution with a finite number (two) of non-zero [[cumulant]]s.\n\n|2= If ''X'' and ''Y'' are [[multivariate normal distribution|jointly normal]] and [[uncorrelated]], then they are [[independence (probability theory)|independent]]. The requirement that ''X'' and ''Y'' should be ''jointly'' normal is essential; without it the property does not hold.<ref>[http://www.math.uiuc.edu/~r-ash/Stat/StatLec21-25.pdf UIUC, Lecture 21. ''The Multivariate Normal Distribution''], 21.6:\"Individually Gaussian Versus Jointly Gaussian\".</ref><ref>Edward L. Melnick and Aaron Tenenbein, \"Misspecifications of the Normal Distribution\", ''[[The American Statistician]]'', volume 36, number 4 November 1982, pages 372–373</ref><sup>[[Normally distributed and uncorrelated does not imply independent|[proof]]]</sup> For non-normal random variables uncorrelatedness does not imply independence.\n\n|3= The [[Kullback–Leibler divergence]] of one normal distribution {{nowrap|1=''X''<sub>1</sub> ∼ ''N''(''μ''<sub>1</sub>, ''σ''<sup>2</sup><span style=\"position:relative;left:-.6em;top:.1em\"><sub>1</sub> )</span>}}from another {{nowrap|1=''X''<sub>2</sub> ∼ ''N''(''μ''<sub>2</sub>, ''σ''<sup>2</sup><span style=\"position:relative;left:-.6em;top:.1em\"><sub>2</sub> )</span>}}is given by:<ref>{{cite web|url=http://www.allisons.org/ll/MML/KL/Normal/|title=Kullback Leibler (KL) Distance of Two Normal (Gaussian) Probability Distributions|website=Allisons.org|date=2007-12-05|accessdate=2017-03-03}}</ref>\n: <math>\n    D_\\mathrm{KL}( X_1 \\,\\|\\, X_2 ) = \\frac{(\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} + \\frac{1}{2}\\left( \\frac{\\sigma_1^2}{\\sigma_2^2} - 1 - \\ln\\frac{\\sigma_1^2}{\\sigma_2^2} \\right).\n  </math>\nThe [[Hellinger distance]] between the same distributions is equal to\n: <math>\n    H^2(X_1,X_2) = 1 - \\sqrt{\\frac{2\\sigma_1\\sigma_2}{\\sigma_1^2+\\sigma_2^2}}\n                       e^{-\\frac{1}{4}\\frac{(\\mu_1-\\mu_2)^2}{\\sigma_1^2+\\sigma_2^2}}.\n  </math>\n\n|4= The [[Fisher information matrix]] for a normal distribution is diagonal and takes the form\n: <math>\n    \\mathcal I = \\begin{pmatrix} \\frac{1}{\\sigma^2} & 0 \\\\ 0 & \\frac{1}{2\\sigma^4} \\end{pmatrix}\n  </math>\n\n|5= The [[conjugate prior]] of the mean of a normal distribution is another normal distribution.<ref>{{cite web|url=http://www.cs.berkeley.edu/~jordan/courses/260-spring10/lectures/lecture5.pdf|title=Stat260: Bayesian Modeling and Inference: The Conjugate Prior for the Normal Distribution|first=Michael I.|last=Jordan|date=February 8, 2010}}</ref> Specifically, if ''x''<sub>1</sub>, …, ''x<sub>n</sub>'' are iid {{nowrap|''N''(''μ'', ''σ''<sup>2</sup>)}} and the prior is {{nowrap|''μ'' ~ ''N''(''μ''<sub>0</sub>, ''σ''{{su|p=2|b=0}})}}, then the posterior distribution for the estimator of ''μ'' will be\n: <math>\n    \\mu \\mid x_1,\\ldots,x_n \\sim \\mathcal{N}\\left( \\frac{\\frac{\\sigma^2}{n}\\mu_0 + \\sigma_0^2\\bar{x}}{\\frac{\\sigma^2}{n}+\\sigma_0^2},\\left( \\frac{n}{\\sigma^2} + \\frac{1}{\\sigma_0^2} \\right)^{-1} \\right)\n  </math>\n\n|6= The family of normal distributions not only forms an [[exponential family]] (EF), but in fact forms a [[natural exponential family]] (NEF) with quadratic [[variance function]] ([[NEF-QVF]]). Many properties of normal distributions generalize to properties of NEF-QVF distributions, NEF distributions, or EF distributions generally. NEF-QVF distributions comprises 6 families, including Poisson, Gamma, binomial, and negative binomial distributions, while many of the common families studied in probability and statistics are NEF or EF.\n\n|7= In [[information geometry]], the family of normal distributions forms a [[statistical manifold]] with [[constant curvature]] −1. The same family is [[flat manifold|flat]] with respect to the (±1)-connections ∇<sup>(''e'')</sup> and ∇<sup>(''m'')</sup>.<ref>{{harvtxt |Amari |Nagaoka |2000 }}</ref>\n}}\n\n== Related distributions ==\n\n=== Operations on a single random variable ===\nIf ''X'' is distributed normally with mean ''μ'' and variance ''σ''<sup>2</sup>, then\n* The exponential of ''X'' is distributed [[Log-normal distribution|log-normally]]: {{nowrap|''e<sup>X</sup>'' ~ ln(''N'' (''μ'', ''σ''<sup>2</sup>))}}.\n* The absolute value of ''X'' has [[folded normal distribution]]: {{nowrap|&#124;''X''&#124; ~ ''N<sub>f</sub>'' (''μ'', ''σ''<sup>2</sup>)}}. If {{nowrap|''μ'' {{=}} 0}} this is known as the [[half-normal distribution]].\n* The absolute value of normalized residuals, |''X'' − ''μ''|/''σ'', has [[chi distribution]] with one degree of freedom: |''X'' − ''μ''|/''σ'' ~ ''χ''<sub>1</sub>(|''X'' − ''μ''|/''σ'').\n* The square of ''X''/''σ'' has the [[noncentral chi-squared distribution]] with one degree of freedom: {{nowrap|1= ''X''<sup>2</sup>/''σ''<sup>2</sup> ~ ''χ''<sup>2</sup><sub style=\"position:relative;top:.2em;left:-.6em\">1</sub><span style=\"position:relative;left:-.4em\">(''μ''<sup>2</sup>/''σ''<sup>2</sup>)</span>}}. If ''μ'' = 0, the distribution is called simply [[chi-squared distribution|chi-squared]].\n* The distribution of the variable ''X'' restricted to an interval [''a'', ''b''] is called the [[truncated normal distribution]].\n* (''X'' − ''μ'')<sup>−2</sup> has a [[Lévy distribution]] with location 0 and scale ''σ''<sup>−2</sup>.\n\n=== Combination of two independent random variables ===\nIf ''X''<sub>1</sub> and ''X''<sub>2</sub> are two independent standard normal random variables with mean 0 and variance 1, then\n* Their sum and difference is distributed normally with mean zero and variance two: {{nowrap|''X''<sub>1</sub> ± ''X''<sub>2</sub> ∼ ''N''(0, 2)}}.\n* Their product {{nowrap|''Z'' {{=}} ''X''<sub>1</sub>·''X''<sub>2</sub>}} follows the \"product-normal\" distribution<ref>{{cite web|url = http://mathworld.wolfram.com/NormalProductDistribution.html |title = Normal Product Distribution|work = MathWorld|publisher =wolfram.com| first = Eric W. |last = Weisstein}}</ref> with density function {{nowrap|''f<sub>Z</sub>''(''z'') {{=}} {{pi}}<sup>−1</sup>''K''<sub>0</sub>({{!}}''z''{{!}}),}} where ''K''<sub>0</sub> is the [[Macdonald function|modified Bessel function of the second kind]]. This distribution is symmetric around zero, unbounded at ''z'' = 0, and has the [[characteristic function (probability theory)|characteristic function]] {{nowrap|1=''φ<sub>Z</sub>''(''t'') = (1 + ''t''<sup> 2</sup>)<sup>−1/2</sup>}}.\n* Their ratio follows the standard [[Cauchy distribution]]: {{nowrap|''X''<sub>1</sub> / ''X''<sub>2</sub> ∼ Cauchy(0, 1)}}.\n* Their Euclidean norm <math>\\sqrt{X_1^2 + X_2^2}</math> has the [[Rayleigh distribution]].\n\n=== Combination of two or more independent random variables ===\n* If ''X''<sub>1</sub>, ''X''<sub>2</sub>, ..., ''X<sub>n</sub>'' are independent standard normal random variables, then the sum of their squares has the [[chi-squared distribution]] with ''n'' degrees of freedom\n::<math>X_1^2 + \\cdots + X_n^2 \\sim \\chi_n^2.</math>\n* If ''X''<sub>1</sub>, ''X''<sub>2</sub>, ..., ''X<sub>n</sub>'' are independent normally distributed random variables with means ''μ'' and variances ''σ''<sup>2</sup>, then their [[sample mean]] is independent from the sample [[standard deviation]],<ref>{{cite journal|title=A Characterization of the Normal Distribution|last=Lukacs|first=Eugene|journal=[[The Annals of Mathematical Statistics]]|issn=0003-4851|volume=13|issue=1|year=1942|pages=91–3|jstor=2236166|doi=10.1214/aoms/1177731647}}</ref> which can be demonstrated using [[Basu's theorem]] or [[Cochran's theorem]].<ref>{{cite journal|title=On Some Characterizations of the Normal Distribution|last1=Basu|first1=D.|last2=Laha|first2=R. G.|journal=[[Sankhyā (journal)|Sankhyā]]|issn=0036-4452|volume=13|issue=4|year=1954|pages=359–62|jstor=25048183}}</ref> The ratio of these two quantities will have the [[Student's t-distribution]] with ''n'' − 1 degrees of freedom:\n\n::<math>t = \\frac{\\overline X - \\mu}{S/\\sqrt{n}} = \\frac{\\frac{1}{n}(X_1+\\cdots+X_n) - \\mu}{\\sqrt{\\frac{1}{n(n-1)}\\left[(X_1-\\overline X)^2+\\cdots+(X_n-\\overline X)^2\\right]}} \\sim t_{n-1}.</math>\n*' If ''X''<sub>1</sub>, ..., ''X<sub>n</sub>'', ''Y''<sub>1</sub>, ..., ''Y<sub>m</sub>'' are independent standard normal random variables, then the ratio of their normalized sums of squares will have the {{nowrap|[[F-distribution]]}} with (''n'', ''m'') degrees of freedom:<ref>{{cite book|title=Testing Statistical Hypotheses|edition=2nd|first=E. L.|last=Lehmann|publisher=Springer|year=1997|isbn=978-0-387-94919-2|page=199}}</ref>\n\n::<math>F = \\frac{\\left(X_1^2+X_2^2+\\cdots+X_n^2\\right)/n}{\\left(Y_1^2+Y_2^2+\\cdots+Y_m^2\\right)/m} \\sim F_{n,m}.</math>\n\n=== Operations on the density function ===\nThe [[split normal distribution]] is most directly defined in terms of joining scaled sections of the density functions of different normal distributions and rescaling the density to integrate to one.  The [[truncated normal distribution]] results from rescaling a section of a single density function.\n\n=== Extensions ===\nThe notion of normal distribution, being one of the most important distributions in probability theory, has been extended far beyond the standard framework of the univariate (that is one-dimensional) case (Case 1). All these extensions are also called ''normal'' or ''Gaussian'' laws, so a certain ambiguity in names exists.\n* The [[multivariate normal distribution]] describes the Gaussian law in the ''k''-dimensional [[Euclidean space]]. A vector {{nowrap|''X'' ∈ '''R'''<sup>''k''</sup>}} is multivariate-normally distributed if any linear combination of its components {{nowrap|∑{{su|p=''k''|b=''j''=1}}''a<sub>j</sub> X<sub>j</sub>''}} has a (univariate) normal distribution. The variance of ''X'' is a ''k×k'' symmetric positive-definite matrix&nbsp;''V''. The multivariate normal distribution is a special case of the [[elliptical distribution]]s. As such, its iso-density loci in the ''k'' = 2 case are [[ellipse]]s and in the case of arbitrary ''k'' are [[ellipsoid]]s.\n* [[Rectified Gaussian distribution]] a rectified version of normal distribution with all the negative elements reset to 0\n* [[Complex normal distribution]] deals with the complex normal vectors. A complex vector {{nowrap|''X'' ∈ '''C'''<sup>''k''</sup>}} is said to be normal if both its real and imaginary components jointly possess a 2''k''-dimensional multivariate normal distribution. The variance-covariance structure of ''X'' is described by two matrices: the ''variance'' matrix&nbsp;Γ, and the ''relation'' matrix&nbsp;''C''.\n* [[Matrix normal distribution]] describes the case of normally distributed matrices.\n* [[Gaussian process]]es are the normally distributed [[stochastic process]]es. These can be viewed as elements of some infinite-dimensional [[Hilbert space]]&nbsp;''H'', and thus are the analogues of multivariate normal vectors for the case {{nowrap|''k'' {{=}} ∞}}. A random element {{nowrap|''h'' ∈ ''H''}} is said to be normal if for any constant {{nowrap|''a'' ∈ ''H''}} the [[scalar product]] {{nowrap|(''a'', ''h'')}} has a (univariate) normal distribution. The variance structure of such Gaussian random element can be described in terms of the linear ''covariance {{nowrap|operator K: H → H}}''. Several Gaussian processes became popular enough to have their own names:\n** [[Wiener process|Brownian motion]],\n** [[Brownian bridge]],\n** [[Ornstein–Uhlenbeck process]].\n* [[Gaussian q-distribution]] is an abstract mathematical construction that represents a \"[[q-analogue]]\" of the normal distribution.\n* the [[q-Gaussian]] is an analogue of the Gaussian distribution, in the sense that it maximises the [[Tsallis entropy]], and is one type of [[Tsallis distribution]]. Note that this distribution is different from the [[Gaussian q-distribution]] above.\n\nA random variable ''X'' has a two-piece normal distribution if it has a distribution\n\n: <math> f_X( x ) = N( \\mu, \\sigma_1^2 )  \\text{ if } x \\le \\mu</math>\n: <math> f_X( x ) = N( \\mu, \\sigma_2^2 )  \\text{ if } x \\ge \\mu</math>\n\nwhere ''μ'' is the mean and ''σ''<sub>1</sub> and ''σ''<sub>2</sub> are the standard deviations of the distribution to the left and right of the mean respectively.\n\nThe mean, variance and third central moment of this distribution have been determined<ref name=John1982>{{cite journal|last1=John|first1=S|year=1982|title=The three parameter two-piece normal family of distributions and its fitting|url=|journal=Communications in Statistics - Theory and Methods|volume=11|issue=8|pages=879–885|doi=10.1080/03610928208828279}}</ref>\n\n:<math> \\operatorname{E}( X ) = \\mu +  \\sqrt{\\frac 2 \\pi } ( \\sigma_2 - \\sigma_1 ) </math>\n:<math> \\operatorname{V}( X ) = \\left( 1 - \\frac 2 \\pi\\right)( \\sigma_2 - \\sigma_1 )^2 + \\sigma_1 \\sigma_2 </math>\n: <math> \\operatorname{T}( X ) = \\sqrt{ \\frac 2 \\pi}( \\sigma_2 - \\sigma_1 ) \\left[ \\left( \\frac 4 \\pi  - 1 \\right) ( \\sigma_2 - \\sigma_1)^2 + \\sigma_1 \\sigma_2 \\right]</math>\n\nwhere E(''X''), V(''X'') and T(''X'') are the mean, variance, and third central moment respectively.\n\nOne of the main practical uses of the Gaussian law is to model the empirical distributions of many different random variables encountered in practice. In such case a possible extension would be a richer family of distributions, having more than two parameters and therefore being able to fit the empirical distribution more accurately. The examples of such extensions are:\n* [[Pearson distribution]] — a four-parameter family of probability distributions that extend the normal law to include different skewness and kurtosis values.\n* The [[generalized normal distribution]], also known as the exponential power distribution, allows for distribution tails with thicker or thinner asymptotic behaviors.\n\n== Normality tests ==\n{{Main|Normality tests}}\n\nNormality tests assess the likelihood that the given data set {''x''<sub>1</sub>, ..., ''x<sub>n</sub>''} comes from a normal distribution. Typically the [[null hypothesis]] ''H''<sub>0</sub> is that the observations are distributed normally with unspecified mean ''μ'' and variance ''σ''<sup>2</sup>, versus the alternative ''H<sub>a</sub>'' that the distribution is arbitrary. Many tests (over 40) have been devised for this problem, the more prominent of them are outlined below:\n* '''\"Visual\" tests''' are more intuitively appealing but subjective at the same time, as they rely on informal human judgement to accept or reject the null hypothesis.\n** [[Q-Q plot]]— is a plot of the sorted values from the data set against the expected values of the corresponding quantiles from the standard normal distribution. That is, it's a plot of point of the form (Φ<sup>−1</sup>(''p<sub>k</sub>''), ''x''<sub>(''k'')</sub>), where plotting points ''p<sub>k</sub>'' are equal to ''p<sub>k</sub>''&nbsp;=&nbsp;(''k''&nbsp;−&nbsp;''α'')/(''n''&nbsp;+&nbsp;1&nbsp;−&nbsp;2''α'') and ''α'' is an adjustment constant, which can be anything between 0 and&nbsp;1. If the null hypothesis is true, the plotted points should approximately lie on a straight line.\n** [[P-P plot]]— similar to the Q-Q plot, but used much less frequently. This method consists of plotting the points (Φ(''z''<sub>(''k'')</sub>), ''p<sub>k</sub>''), where <math>\\textstyle z_{(k)} = (x_{(k)}-\\hat\\mu)/\\hat\\sigma</math>. For normally distributed data this plot should lie on a 45° line between (0,&nbsp;0) and&nbsp;(1,&nbsp;1).\n** [[Shapiro-Wilk test]] employs the fact that the line in the Q-Q plot has the slope of ''σ''. The test compares the least squares estimate of that slope with the value of the sample variance, and rejects the null hypothesis if these two quantities differ significantly.\n** [[Normal probability plot]] ([[rankit]] plot)\n* '''Moment tests''':\n** [[D'Agostino's K-squared test]]\n** [[Jarque–Bera test]]\n* '''Empirical distribution function tests''':\n** [[Lilliefors test]] (an adaptation of the [[Kolmogorov–Smirnov test]])\n** [[Anderson–Darling test]]\n\n== Estimation of parameters ==\n{{See also|Maximum likelihood#Continuous distribution, continuous parameter space}}\n\nIt is often the case that we don't know the parameters of the normal distribution, but instead want to [[Estimation theory|estimate]] them. That is, having a sample (''x''<sub>1</sub>, ..., ''x<sub>n</sub>'') from a normal {{nowrap|''N''(''μ'', ''σ''<sup>2</sup>)}} population we would like to learn the approximate values of parameters ''μ'' and ''σ''<sup>2</sup>. The standard approach to this problem is the [[maximum likelihood]] method, which requires maximization of the ''log-likelihood function'':\n: <math>\n   \\ln\\mathcal{L}(\\mu,\\sigma^2)\n     = \\sum_{i=1}^n \\ln f(x_i\\mid\\mu,\\sigma^2)\n     = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\sigma^2 - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i-\\mu)^2.\n  </math>\nTaking derivatives with respect to ''μ'' and ''σ''<sup>2</sup> and solving the resulting system of first order conditions yields the ''maximum likelihood estimates'':\n: <math>\n    \\hat{\\mu} = \\overline{x} \\equiv \\frac{1}{n}\\sum_{i=1}^n x_i, \\qquad\n    \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\overline{x})^2.\n  </math>\n\n=== Sample mean ===\n{{See also|Standard error of the mean}}\n\nEstimator <math style=\"vertical-align:-.3em\">\\textstyle\\hat\\mu</math> is called the ''[[sample mean]]'', since it is the arithmetic mean of all observations. The statistic <math style=\"vertical-align:0\">\\textstyle\\overline{x}</math> is [[complete statistic|complete]] and [[sufficient statistic|sufficient]] for ''μ'', and therefore by the [[Lehmann–Scheffé theorem]], <math style=\"vertical-align:-.3em\">\\textstyle\\hat\\mu</math> is the [[uniformly minimum variance unbiased]] (UMVU) estimator.<ref name=\"Kri127\">{{harvtxt |Krishnamoorthy |2006 |p=127 }}</ref> In finite samples it is distributed normally:\n: <math>\n    \\hat\\mu \\sim \\mathcal{N}(\\mu,\\sigma^2/n).\n  </math>\nThe variance of this estimator is equal to the ''μμ''-element of the inverse [[Fisher information matrix]] <math style=\"vertical-align:0\">\\textstyle\\mathcal{I}^{-1}</math>. This implies that the estimator is [[efficient estimator|finite-sample efficient]]. Of practical importance is the fact that the [[standard error (statistics)|standard error]] of <math style=\"vertical-align:-.3em\">\\textstyle\\hat\\mu</math> is proportional to <math style=\"vertical-align:-.3em\">\\textstyle1/\\sqrt{n}</math>, that is, if one wishes to decrease the standard error by a factor of 10, one must increase the number of points in the sample by a factor of 100. This fact is widely used in determining sample sizes for opinion polls and the number of trials in [[Monte Carlo simulation]]s.\n\nFrom the standpoint of the [[asymptotic theory (statistics)|asymptotic theory]], <math style=\"vertical-align:-.3em\">\\textstyle\\hat\\mu</math> is [[consistent estimator|consistent]], that is, it [[convergence in probability|converges in probability]] to ''μ'' as ''n'' → ∞. The estimator is also [[asymptotic normality|asymptotically normal]], which is a simple corollary of the fact that it is normal in finite samples:\n: <math>\n    \\sqrt{n}(\\hat\\mu-\\mu) \\,\\xrightarrow{d}\\, \\mathcal{N}(0,\\sigma^2).\n  </math>\n\n=== Sample variance ===\n{{See also|Standard deviation#Estimation|Variance#Estimation}}\n\nThe estimator <math style=\"vertical-align:0\">\\textstyle\\hat\\sigma^2</math> is called the ''[[sample variance]]'', since it is the variance of the sample (''x''<sub>1</sub>, ..., ''x<sub>n</sub>''). In practice, another estimator is often used instead of the <math style=\"vertical-align:0\">\\textstyle\\hat\\sigma^2</math>. This other estimator is denoted ''s''<sup>2</sup>, and is also called the ''sample variance'', which represents a certain ambiguity in terminology; its square root ''s'' is called the ''sample standard deviation''. The estimator ''s''<sup>2</sup> differs from <math style=\"vertical-align:0\">\\textstyle\\hat\\sigma^2</math> by having {{nowrap|(''n'' − 1)}} instead of&nbsp;''n'' in the denominator (the so-called [[Bessel's correction]]):\n: <math>\n    s^2 = \\frac{n}{n-1} \\hat\\sigma^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\overline{x})^2.\n  </math>\nThe difference between ''s''<sup>2</sup> and <math style=\"vertical-align:0\">\\textstyle\\hat\\sigma^2</math> becomes negligibly small for large ''n''{{'}}s. In finite samples however, the motivation behind the use of ''s''<sup>2</sup> is that it is an [[unbiased estimator]] of the underlying parameter ''σ''<sup>2</sup>, whereas <math style=\"vertical-align:0\">\\textstyle\\hat\\sigma^2</math> is biased. Also, by the Lehmann–Scheffé theorem the estimator ''s''<sup>2</sup> is uniformly minimum variance unbiased (UMVU),<ref name=\"Kri127\" /> which makes it the \"best\" estimator among all unbiased ones. However it can be shown that the biased estimator <math style=\"vertical-align:0\">\\textstyle\\hat\\sigma^2</math> is \"better\" than the ''s''<sup>2</sup> in terms of the [[mean squared error]] (MSE) criterion. In finite samples both ''s''<sup>2</sup> and <math style=\"vertical-align:0\">\\textstyle\\hat\\sigma^2</math> have scaled [[chi-squared distribution]] with {{nowrap|(''n'' − 1)}} degrees of freedom:\n: <math>\n    s^2 \\sim \\frac{\\sigma^2}{n-1} \\cdot \\chi^2_{n-1}, \\qquad\n    \\hat\\sigma^2 \\sim \\frac{\\sigma^2}{n} \\cdot \\chi^2_{n-1}.\n  </math>\nThe first of these expressions shows that the variance of ''s''<sup>2</sup> is equal to {{nowrap|2''σ''<sup>4</sup>/(''n''−1)}}, which is slightly greater than the ''σσ''-element of the inverse Fisher information matrix <math style=\"vertical-align:0\">\\textstyle\\mathcal{I}^{-1}</math>. Thus, ''s''<sup>2</sup> is not an efficient estimator for ''σ''<sup>2</sup>, and moreover, since ''s''<sup>2</sup> is UMVU, we can conclude that the finite-sample efficient estimator for ''σ''<sup>2</sup> does not exist.\n\nApplying the asymptotic theory, both estimators ''s''<sup>2</sup> and <math style=\"vertical-align:0\">\\textstyle\\hat\\sigma^2</math> are consistent, that is they converge in probability to ''σ''<sup>2</sup> as the sample size {{nowrap|''n'' → ∞}}. The two estimators are also both asymptotically normal:\n: <math>\n    \\sqrt{n}(\\hat\\sigma^2 - \\sigma^2) \\simeq\n    \\sqrt{n}(s^2-\\sigma^2) \\,\\xrightarrow{d}\\, \\mathcal{N}(0,2\\sigma^4).\n  </math>\nIn particular, both estimators are asymptotically efficient for ''σ''<sup>2</sup>.\n\n=== Confidence intervals ===\n{{see also|Studentization}}\n\nBy [[Cochran's theorem]], for normal distributions the sample mean <math style=\"vertical-align:-.3em\">\\textstyle\\hat\\mu</math> and the sample variance ''s''<sup>2</sup> are [[independence (probability theory)|independent]], which means there can be no gain in considering their [[joint distribution]]. There is also a converse theorem: if in a sample the sample mean and sample variance are independent, then the sample must have come from the normal distribution. The independence between <math style=\"vertical-align:-.3em\">\\textstyle\\hat\\mu</math> and ''s'' can be employed to construct the so-called ''t-statistic'':\n: <math>\n    t = \\frac{\\hat\\mu-\\mu}{s/\\sqrt{n}} = \\frac{\\overline{x}-\\mu}{\\sqrt{\\frac{1}{n(n-1)}\\sum(x_i-\\overline{x})^2}} \\sim t_{n-1}\n  </math>\nThis quantity ''t'' has the [[Student's t-distribution]] with {{nowrap|(''n'' − 1)}} degrees of freedom, and it is an [[ancillary statistic]] (independent of the value of the parameters). Inverting the distribution of this ''t''-statistics will allow us to construct the [[confidence interval]] for ''μ'';<ref>{{harvtxt |Krishnamoorthy |2006 |p=130 }}</ref> similarly, inverting the ''χ''<sup>2</sup> distribution of the statistic ''s''<sup>2</sup> will give us the confidence interval for ''σ''<sup>2</sup>:<ref>{{harvtxt |Krishnamoorthy |2006 |p=133 }}</ref>\n\n:<math>\\mu \\in \\left[ \\hat\\mu - t_{n-1,1-\\alpha/2} \\frac{1}{\\sqrt{n}}s,\n                      \\hat\\mu + t_{n-1,1-\\alpha/2} \\frac{1}{\\sqrt{n}}s \\right] \\approx\n              \\left[ \\hat\\mu - |z_{\\alpha/2}|\\frac{1}{\\sqrt n}s,\n                      \\hat\\mu + |z_{\\alpha/2}|\\frac{1}{\\sqrt n}s \\right],</math>\n:<math>\\sigma^2 \\in \\left[ \\frac{(n-1)s^2}{\\chi^2_{n-1,1-\\alpha/2}},\n                            \\frac{(n-1)s^2}{\\chi^2_{n-1,\\alpha/2}} \\right] \\approx\n                   \\left[ s^2 - |z_{\\alpha/2}|\\frac{\\sqrt{2}}{\\sqrt{n}}s^2,\n                           s^2 + |z_{\\alpha/2}|\\frac{\\sqrt{2}}{\\sqrt{n}}s^2 \\right],</math>\n\nwhere ''t<sub>k,p</sub>'' and {{SubSup|χ|''k,p''|2}} are the ''p''th [[quantile]]s of the ''t''- and ''χ''<sup>2</sup>-distributions respectively. These confidence intervals are of the ''[[confidence level]]'' {{nowrap|1 − ''α''}}, meaning that the true values ''μ'' and ''σ''<sup>2</sup> fall outside of these intervals with probability (or [[significance level]]) ''α''. In practice people usually take {{nowrap|''α'' {{=}} 5%}}, resulting in the 95% confidence intervals. The approximate formulas in the display above were derived from the asymptotic distributions of <math style=\"vertical-align:-.3em\">\\textstyle\\hat\\mu</math> and ''s''<sup>2</sup>. The approximate formulas become valid for large values of ''n'', and are more convenient for the manual calculation since the standard normal quantiles ''z''<sub>''α''/2</sub> do not depend on ''n''. In particular, the most popular value of {{nowrap|''α'' {{=}} 5%}}, results in {{nowrap|{{!}}''z''<sub>0.025</sub>{{!}} {{=}} [[1.96]]}}.\n\n== Bayesian analysis of the normal distribution ==\nBayesian analysis of normally distributed data is complicated by the many different possibilities that may be considered:\n* Either the mean, or the variance, or neither, may be considered a fixed quantity.\n* When the variance is unknown, analysis may be done directly in terms of the variance, or in terms of the [[precision (statistics)|precision]], the reciprocal of the variance.  The reason for expressing the formulas in terms of precision is that the analysis of most cases is simplified.\n* Both univariate and [[multivariate normal distribution|multivariate]] cases need to be considered.\n* Either [[conjugate prior|conjugate]] or [[improper prior|improper]] [[prior distribution]]s may be placed on the unknown variables.\n* An additional set of cases occurs in [[Bayesian linear regression]], where in the basic model the data is assumed to be normally distributed, and normal priors are placed on the [[regression coefficient]]s. The resulting analysis is similar to the basic cases of [[independent identically distributed]] data, but more complex.\n\nThe formulas for the non-linear-regression cases are summarized in the [[conjugate prior]] article.\n\n=== Sum of two quadratics ===\n\n==== Scalar form ====\nThe following auxiliary formula is useful for simplifying the [[posterior distribution|posterior]] update equations, which otherwise become fairly tedious.\n\n:<math>a(x-y)^2 + b(x-z)^2 = (a + b)\\left(x - \\frac{ay+bz}{a+b}\\right)^2 + \\frac{ab}{a+b}(y-z)^2</math>\n\nThis equation rewrites the sum of two quadratics in ''x'' by expanding the squares, grouping the terms in ''x'', and [[completing the square]].  Note the following about the complex constant factors attached to some of the terms:\n# The factor <math>\\frac{ay+bz}{a+b}</math> has the form of a [[weighted average]] of ''y'' and ''z''.\n# <math>\\frac{ab}{a+b} = \\frac{1}{\\frac{1}{a}+\\frac{1}{b}} = (a^{-1} + b^{-1})^{-1}.</math> This shows that this factor can be thought of as resulting from a situation where the [[Multiplicative inverse|reciprocals]] of quantities ''a'' and ''b'' add directly, so to combine ''a'' and ''b'' themselves, it's necessary to reciprocate, add, and reciprocate the result again to get back into the original units.  This is exactly the sort of operation performed by the [[harmonic mean]], so it is not surprising that <math>\\frac{ab}{a+b}</math> is one-half the [[harmonic mean]] of ''a'' and ''b''.\n\n==== Vector form ====\nA similar formula can be written for the sum of two vector quadratics: If '''x''', '''y''', '''z''' are vectors of length ''k'', and '''A''' and '''B''' are [[symmetric matrix|symmetric]], [[invertible matrices]] of size <math>k\\times k</math>, then\n\n:<math>\n\\begin{align}\n& (\\mathbf{y}-\\mathbf{x})'\\mathbf{A}(\\mathbf{y}-\\mathbf{x}) + (\\mathbf{x}-\\mathbf{z})' \\mathbf{B}(\\mathbf{x}-\\mathbf{z}) \\\\\n= {} & (\\mathbf{x} - \\mathbf{c})'(\\mathbf{A}+\\mathbf{B})(\\mathbf{x} - \\mathbf{c}) + (\\mathbf{y} - \\mathbf{z})'(\\mathbf{A}^{-1} + \\mathbf{B}^{-1})^{-1}(\\mathbf{y} - \\mathbf{z})\n\\end{align}\n</math>\n\nwhere\n\n:<math>\\mathbf{c} = (\\mathbf{A} + \\mathbf{B})^{-1}(\\mathbf{A}\\mathbf{y} + \\mathbf{B} \\mathbf{z}) </math>\n\nNote that the form '''x'''′ '''A''' '''x''' is called a [[quadratic form]] and is a [[scalar (mathematics)|scalar]]:\n:<math>\\mathbf{x}'\\mathbf{A}\\mathbf{x} = \\sum_{i,j}a_{ij} x_i x_j</math>\nIn other words, it sums up all possible combinations of products of pairs of elements from '''x''', with a separate coefficient for each.  In addition, since <math>x_i x_j = x_j x_i</math>, only the sum <math>a_{ij} + a_{ji}</math> matters for any off-diagonal elements of '''A''', and there is no loss of generality in assuming that '''A''' is [[symmetric matrix|symmetric]].  Furthermore, if '''A''' is symmetric, then the form <math>\\mathbf{x}'\\mathbf{A}\\mathbf{y} = \\mathbf{y}'\\mathbf{A}\\mathbf{x}.</math>\n\n=== Sum of differences from the mean ===\nAnother useful formula is as follows:\n\n:<math>\\sum_{i=1}^n (x_i-\\mu)^2 = \\sum_{i=1}^n(x_i-\\bar{x})^2 + n(\\bar{x} -\\mu)^2</math>\n\nwhere <math>\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i.</math>\n\n=== With known variance ===\nFor a set of [[i.i.d.]] normally distributed data points '''X''' of size ''n'' where each individual point ''x'' follows <math>x \\sim \\mathcal{N}(\\mu, \\sigma^2)</math> with known [[variance]] σ<sup>2</sup>, the [[conjugate prior]] distribution is also normally distributed.\n\nThis can be shown more easily by rewriting the variance as the [[precision (statistics)|precision]], i.e. using τ = 1/σ<sup>2</sup>. Then if <math>x \\sim \\mathcal{N}(\\mu, 1/\\tau)</math> and <math>\\mu \\sim \\mathcal{N}(\\mu_0, 1/\\tau_0),</math> we proceed as follows.\n\nFirst, the [[likelihood function]] is (using the formula above for the sum of differences from the mean):\n\n:<math>\\begin{align}\np(\\mathbf{X}\\mid\\mu,\\tau) &= \\prod_{i=1}^n \\sqrt{\\frac{\\tau}{2\\pi}} \\exp\\left(-\\frac{1}{2}\\tau(x_i-\\mu)^2\\right) \\\\\n&= \\left(\\frac{\\tau}{2\\pi}\\right)^{n/2} \\exp\\left(-\\frac{1}{2}\\tau \\sum_{i=1}^n (x_i-\\mu)^2\\right) \\\\\n&= \\left(\\frac{\\tau}{2\\pi}\\right)^{n/2} \\exp\\left[-\\frac{1}{2}\\tau \\left(\\sum_{i=1}^n(x_i-\\bar{x})^2 + n(\\bar{x} -\\mu)^2\\right)\\right].\n\\end{align}</math>\n\nThen, we proceed as follows:\n\n:<math>\\begin{align}\np(\\mu\\mid\\mathbf{X}) &\\propto p(\\mathbf{X}\\mid\\mu) p(\\mu) \\\\\n& = \\left(\\frac{\\tau}{2\\pi}\\right)^{n/2} \\exp\\left[-\\frac{1}{2}\\tau \\left(\\sum_{i=1}^n(x_i-\\bar{x})^2 + n(\\bar{x} -\\mu)^2\\right)\\right] \\sqrt{\\frac{\\tau_0}{2\\pi}} \\exp\\left(-\\frac{1}{2}\\tau_0(\\mu-\\mu_0)^2\\right) \\\\\n&\\propto \\exp\\left(-\\frac{1}{2}\\left(\\tau\\left(\\sum_{i=1}^n(x_i-\\bar{x})^2 + n(\\bar{x} -\\mu)^2\\right) + \\tau_0(\\mu-\\mu_0)^2\\right)\\right) \\\\\n&\\propto \\exp\\left(-\\frac{1}{2} \\left(n\\tau(\\bar{x}-\\mu)^2 + \\tau_0(\\mu-\\mu_0)^2 \\right)\\right) \\\\\n&= \\exp\\left(-\\frac{1}{2}(n\\tau + \\tau_0)\\left(\\mu - \\dfrac{n\\tau \\bar{x} + \\tau_0\\mu_0}{n\\tau + \\tau_0}\\right)^2 + \\frac{n\\tau\\tau_0}{n\\tau+\\tau_0}(\\bar{x} - \\mu_0)^2\\right) \\\\\n&\\propto \\exp\\left(-\\frac{1}{2}(n\\tau + \\tau_0)\\left(\\mu - \\dfrac{n\\tau \\bar{x} + \\tau_0\\mu_0}{n\\tau + \\tau_0}\\right)^2\\right)\n\\end{align}</math>\n\nIn the above derivation, we used the formula above for the sum of two quadratics and eliminated all constant factors not involving&nbsp;''μ''.  The result is the [[kernel (statistics)|kernel]] of a normal distribution, with mean <math>\\frac{n\\tau \\bar{x} + \\tau_0\\mu_0}{n\\tau + \\tau_0}</math> and precision <math>n\\tau + \\tau_0</math>, i.e.\n\n:<math>p(\\mu\\mid\\mathbf{X}) \\sim \\mathcal{N}\\left(\\frac{n\\tau \\bar{x} + \\tau_0\\mu_0}{n\\tau + \\tau_0}, \\frac{1}{n\\tau + \\tau_0}\\right)</math>\n\nThis can be written as a set of Bayesian update equations for the posterior parameters in terms of the prior parameters:\n\n:<math>\\begin{align}\n\\tau_0' &= \\tau_0 + n\\tau \\\\\n\\mu_0' &= \\frac{n\\tau \\bar{x} + \\tau_0\\mu_0}{n\\tau + \\tau_0} \\\\\n\\bar{x} &= \\frac{1}{n}\\sum_{i=1}^n x_i\n\\end{align}</math>\n\nThat is, to combine ''n'' data points with total precision of ''nτ'' (or equivalently, total variance of ''n''/''σ''<sup>2</sup>) and mean of values <math>\\bar{x}</math>, derive a new total precision simply by adding the total precision of the data to the prior total precision, and form a new mean through a ''precision-weighted average'', i.e. a [[weighted average]] of the data mean and the prior mean, each weighted by the associated total precision. This makes logical sense if the precision is thought of as indicating the certainty of the observations: In the distribution of the posterior mean, each of the input components is weighted by its certainty, and the certainty of this distribution is the sum of the individual certainties. (For the intuition of this, compare the expression \"the whole is (or is not) greater than the sum of its parts\".  In addition, consider that the knowledge of the posterior comes from a combination of the knowledge of the prior and likelihood, so it makes sense that we are more certain of it than of either of its components.)\n\nThe above formula reveals why it is more convenient to do [[Bayesian analysis]] of [[conjugate prior]]s for the normal distribution in terms of the precision.  The posterior precision is simply the sum of the prior and likelihood precisions, and the posterior mean is computed through a precision-weighted average, as described above.  The same formulas can be written in terms of variance by reciprocating all the precisions, yielding the more ugly formulas\n\n:<math>\\begin{align}\n{\\sigma^2_0}' &= \\frac{1}{\\frac{n}{\\sigma^2} + \\frac{1}{\\sigma_0^2}} \\\\\n\\mu_0' &= \\frac{\\frac{n\\bar{x}}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}}{\\frac{n}{\\sigma^2} + \\frac{1}{\\sigma_0^2}} \\\\\n\\bar{x} &= \\frac{1}{n}\\sum_{i=1}^n x_i\n\\end{align}</math>\n\n=== With known mean ===\nFor a set of [[i.i.d.]] normally distributed data points '''X''' of size ''n'' where each individual point ''x'' follows <math>x \\sim \\mathcal{N}(\\mu, \\sigma^2)</math> with known mean μ, the [[conjugate prior]] of the [[variance]] has an [[inverse gamma distribution]] or a [[scaled inverse chi-squared distribution]].  The two are equivalent except for having different [[parameterization]]s.  Although the inverse gamma is more commonly used, we use the scaled inverse chi-squared for the sake of convenience.  The prior for σ<sup>2</sup> is as follows:\n\n:<math>p(\\sigma^2\\mid\\nu_0,\\sigma_0^2) = \\frac{(\\sigma_0^2\\frac{\\nu_0}{2})^{\\nu_0/2}}{\\Gamma\\left(\\frac{\\nu_0}{2} \\right)}~\\frac{\\exp\\left[ \\frac{-\\nu_0 \\sigma_0^2}{2 \\sigma^2}\\right]}{(\\sigma^2)^{1+\\frac{\\nu_0}{2}}} \\propto \\frac{\\exp\\left[ \\frac{-\\nu_0 \\sigma_0^2}{2 \\sigma^2}\\right]}{(\\sigma^2)^{1+\\frac{\\nu_0}{2}}}</math>\n\nThe [[likelihood function]] from above, written in terms of the variance, is:\n\n:<math>\\begin{align}\np(\\mathbf{X}\\mid\\mu,\\sigma^2) &= \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left[-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i-\\mu)^2\\right] \\\\\n&= \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left[-\\frac{S}{2\\sigma^2}\\right]\n\\end{align}</math>\n\nwhere\n\n:<math>S = \\sum_{i=1}^n (x_i-\\mu)^2.</math>\n\nThen:\n\n:<math>\\begin{align}\np(\\sigma^2\\mid\\mathbf{X}) &\\propto p(\\mathbf{X}\\mid\\sigma^2) p(\\sigma^2) \\\\\n&= \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left[-\\frac{S}{2\\sigma^2}\\right] \\frac{(\\sigma_0^2\\frac{\\nu_0}{2})^{\\frac{\\nu_0}{2}}}{\\Gamma\\left(\\frac{\\nu_0}{2} \\right)}~\\frac{\\exp\\left[ \\frac{-\\nu_0 \\sigma_0^2}{2 \\sigma^2}\\right]}{(\\sigma^2)^{1+\\frac{\\nu_0}{2}}} \\\\\n&\\propto \\left(\\frac{1}{\\sigma^2}\\right)^{n/2} \\frac{1}{(\\sigma^2)^{1+\\frac{\\nu_0}{2}}} \\exp\\left[-\\frac{S}{2\\sigma^2} + \\frac{-\\nu_0 \\sigma_0^2}{2 \\sigma^2}\\right] \\\\\n&= \\frac{1}{(\\sigma^2)^{1+\\frac{\\nu_0+n}{2}}} \\exp\\left[-\\frac{\\nu_0 \\sigma_0^2 + S}{2\\sigma^2}\\right]\n\\end{align}</math>\n\nThe above is also a scaled inverse chi-squared distribution where\n\n:<math>\\begin{align}\n\\nu_0' &= \\nu_0 + n \\\\\n\\nu_0'{\\sigma_0^2}' &= \\nu_0 \\sigma_0^2 + \\sum_{i=1}^n (x_i-\\mu)^2\n\\end{align}</math>\n\nor equivalently\n\n:<math>\\begin{align}\n\\nu_0' &= \\nu_0 + n \\\\\n{\\sigma_0^2}' &= \\frac{\\nu_0 \\sigma_0^2 + \\sum_{i=1}^n (x_i-\\mu)^2}{\\nu_0+n}\n\\end{align}</math>\n\nReparameterizing in terms of an [[inverse gamma distribution]], the result is:\n\n:<math>\\begin{align}\n\\alpha' &= \\alpha + \\frac{n}{2} \\\\\n\\beta' &= \\beta + \\frac{\\sum_{i=1}^n (x_i-\\mu)^2}{2}\n\\end{align}</math>\n\n=== With unknown mean and unknown variance ===\nFor a set of [[i.i.d.]] normally distributed data points '''X''' of size ''n'' where each individual point ''x'' follows <math>x \\sim \\mathcal{N}(\\mu, \\sigma^2)</math> with unknown mean μ and unknown [[variance]] σ<sup>2</sup>, a combined (multivariate) [[conjugate prior]] is placed over the mean and variance, consisting of a [[normal-inverse-gamma distribution]].\nLogically, this originates as follows:\n# From the analysis of the case with unknown mean but known variance, we see that the update equations involve [[sufficient statistic]]s computed from the data consisting of the mean of the data points and the total variance of the data points, computed in turn from the known variance divided by the number of data points.\n# From the analysis of the case with unknown variance but known mean, we see that the update equations involve sufficient statistics over the data consisting of the number of data points and [[sum of squared deviations]].\n# Keep in mind that the posterior update values serve as the prior distribution when further data is handled.  Thus, we should logically think of our priors in terms of the sufficient statistics just described, with the same semantics kept in mind as much as possible.\n# To handle the case where both mean and variance are unknown, we could place independent priors over the mean and variance, with fixed estimates of the average mean, total variance, number of data points used to compute the variance prior, and sum of squared deviations.  Note however that in reality, the total variance of the mean depends on the unknown variance, and the sum of squared deviations that goes into the variance prior (appears to) depend on the unknown mean.  In practice, the latter dependence is relatively unimportant: Shifting the actual mean shifts the generated points by an equal amount, and on average the squared deviations will remain the same.  This is not the case, however, with the total variance of the mean: As the unknown variance increases, the total variance of the mean will increase proportionately, and we would like to capture this dependence.\n# This suggests that we create a ''conditional prior'' of the mean on the unknown variance, with a hyperparameter specifying the mean of the [[pseudo-observation]]s associated with the prior, and another parameter specifying the number of pseudo-observations.  This number serves as a scaling parameter on the variance, making it possible to control the overall variance of the mean relative to the actual variance parameter.  The prior for the variance also has two hyperparameters, one specifying the sum of squared deviations of the pseudo-observations associated with the prior, and another specifying once again the number of pseudo-observations.  Note that each of the priors has a hyperparameter specifying the number of pseudo-observations, and in each case this controls the relative variance of that prior.  These are given as two separate hyperparameters so that the variance (aka the confidence) of the two priors can be controlled separately.\n# This leads immediately to the [[normal-inverse-gamma distribution]], which is the product of the two distributions just defined, with [[conjugate prior]]s used (an [[inverse gamma distribution]] over the variance, and a normal distribution over the mean, ''conditional'' on the variance) and with the same four parameters just defined.\n\nThe priors are normally defined as follows:\n\n:<math>\\begin{align}\np(\\mu\\mid\\sigma^2; \\mu_0, n_0) &\\sim \\mathcal{N}(\\mu_0,\\sigma^2/n_0) \\\\\np(\\sigma^2; \\nu_0,\\sigma_0^2) &\\sim I\\chi^2(\\nu_0,\\sigma_0^2) = IG(\\nu_0/2, \\nu_0\\sigma_0^2/2)\n\\end{align}</math>\n<!-- \\\\\n & =\\frac{(\\sigma_0^2\\nu_0/2)^{\\nu_0/2}}{\\Gamma(\\nu_0/2)}~\\frac{\\exp\\left[ \\frac{-\\nu_0 \\sigma_0^2}{2 \\sigma^2}\\right]}{(\\sigma^2)^{1+\\nu_0/2}} \\propto \\frac{\\exp\\left[ \\frac{-\\nu_0 \\sigma_0^2}{2 \\sigma^2}\\right]}{(\\sigma^2)^{1+\\nu_0/2}}\n-->\n\nThe update equations can be derived, and look as follows:\n\n:<math>\\begin{align}\n\\bar{x} &= \\frac 1 n \\sum_{i=1}^n x_i \\\\\n\\mu_0' &= \\frac{n_0\\mu_0 + n\\bar{x}}{n_0 + n} \\\\\nn_0' &= n_0 + n \\\\\n\\nu_0' &= \\nu_0 + n \\\\\n\\nu_0'{\\sigma_0^2}' &= \\nu_0 \\sigma_0^2 + \\sum_{i=1}^n (x_i-\\bar{x})^2 + \\frac{n_0 n}{n_0 + n}(\\mu_0 - \\bar{x})^2\n\\end{align}</math>\n\nThe respective numbers of pseudo-observations add the number of actual observations to them.  The new mean hyperparameter is once again a weighted average, this time weighted by the relative numbers of observations.  Finally, the update for <math>\\nu_0'{\\sigma_0^2}'</math> is similar to the case with known mean, but in this case the sum of squared deviations is taken with respect to the observed data mean rather than the true mean, and as a result a new \"interaction term\" needs to be added to take care of the additional error source stemming from the deviation between prior and data mean.\n\n{{hidden begin|header=[Proof]|toggle=left}}\nThe prior distributions are\n\n:<math>\\begin{align}\np(\\mu\\mid\\sigma^2; \\mu_0, n_0) &\\sim \\mathcal{N}(\\mu_0,\\sigma^2/n_0) = \\frac{1}{\\sqrt{2\\pi\\frac{\\sigma^2}{n_0}}} \\exp\\left(-\\frac{n_0}{2\\sigma^2}(\\mu-\\mu_0)^2\\right) \\\\\n&\\propto (\\sigma^2)^{-1/2} \\exp\\left(-\\frac{n_0}{2\\sigma^2}(\\mu-\\mu_0)^2\\right) \\\\\np(\\sigma^2; \\nu_0,\\sigma_0^2) &\\sim I\\chi^2(\\nu_0,\\sigma_0^2) = IG(\\nu_0/2, \\nu_0\\sigma_0^2/2) \\\\\n&= \\frac{(\\sigma_0^2\\nu_0/2)^{\\nu_0/2}}{\\Gamma(\\nu_0/2)}~\\frac{\\exp\\left[ \\frac{-\\nu_0 \\sigma_0^2}{2 \\sigma^2}\\right]}{(\\sigma^2)^{1+\\nu_0/2}} \\\\\n&\\propto {(\\sigma^2)^{-(1+\\nu_0/2)}} \\exp\\left[ \\frac{-\\nu_0 \\sigma_0^2}{2 \\sigma^2}\\right].\n\\end{align}</math>\n\nTherefore, the joint prior is\n\n:<math>\\begin{align}\np(\\mu,\\sigma^2; \\mu_0, n_0, \\nu_0,\\sigma_0^2) &= p(\\mu\\mid\\sigma^2; \\mu_0, n_0)\\,p(\\sigma^2; \\nu_0,\\sigma_0^2) \\\\\n&\\propto (\\sigma^2)^{-(\\nu_0+3)/2} \\exp\\left[-\\frac 1 {2\\sigma^2}\\left(\\nu_0\\sigma_0^2 + n_0(\\mu-\\mu_0)^2\\right)\\right].\n\\end{align}</math>\n\nThe [[likelihood function]] from the section above with known variance is:\n\n:<math>\\begin{align}\np(\\mathbf{X}\\mid\\mu,\\sigma^2) &= \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left[-\\frac{1}{2\\sigma^2} \\left(\\sum_{i=1}^n(x_i -\\mu)^2\\right)\\right]\n\\end{align}</math>\n\nWriting it in terms of variance rather than precision, we get:\n:<math>\\begin{align}\np(\\mathbf{X}\\mid\\mu,\\sigma^2) &= \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left[-\\frac{1}{2\\sigma^2} \\left(\\sum_{i=1}^n(x_i-\\bar{x})^2 + n(\\bar{x} -\\mu)^2\\right)\\right] \\\\\n&\\propto {\\sigma^2}^{-n/2} \\exp\\left[-\\frac{1}{2\\sigma^2} \\left(S + n(\\bar{x} -\\mu)^2\\right)\\right]\n\\end{align}</math>\n\nwhere <math>S = \\sum_{i=1}^n(x_i-\\bar{x})^2.</math>\n\nTherefore, the posterior is (dropping the hyperparameters as conditioning factors):\n\n:<math>\\begin{align}\np(\\mu,\\sigma^2\\mid\\mathbf{X}) & \\propto p(\\mu,\\sigma^2) \\, p(\\mathbf{X}\\mid\\mu,\\sigma^2) \\\\\n& \\propto (\\sigma^2)^{-(\\nu_0+3)/2} \\exp\\left[-\\frac{1}{2\\sigma^2}\\left(\\nu_0\\sigma_0^2 + n_0(\\mu-\\mu_0)^2\\right)\\right] {\\sigma^2}^{-n/2} \\exp\\left[-\\frac{1}{2\\sigma^2} \\left(S + n(\\bar{x} -\\mu)^2\\right)\\right] \\\\\n&= (\\sigma^2)^{-(\\nu_0+n+3)/2} \\exp\\left[-\\frac{1}{2\\sigma^2}\\left(\\nu_0\\sigma_0^2 + S + n_0(\\mu-\\mu_0)^2 + n(\\bar{x} -\\mu)^2\\right)\\right] \\\\\n&= (\\sigma^2)^{-(\\nu_0+n+3)/2} \\exp\\left[-\\frac{1}{2\\sigma^2}\\left(\\nu_0\\sigma_0^2 + S + \\frac{n_0 n}{n_0+n}(\\mu_0-\\bar{x})^2 + (n_0+n)\\left(\\mu-\\frac{n_0\\mu_0 + n\\bar{x}}{n_0 + n}\\right)^2\\right)\\right] \\\\\n& \\propto (\\sigma^2)^{-1/2} \\exp\\left[-\\frac{n_0+n}{2\\sigma^2}\\left(\\mu-\\frac{n_0\\mu_0 + n\\bar{x}}{n_0 + n}\\right)^2\\right] \\\\\n& \\quad\\times (\\sigma^2)^{-(\\nu_0/2+n/2+1)} \\exp\\left[-\\frac{1}{2\\sigma^2}\\left(\\nu_0\\sigma_0^2 + S + \\frac{n_0 n}{n_0+n}(\\mu_0-\\bar{x})^2\\right)\\right] \\\\\n& = \\mathcal{N}_{\\mu\\mid\\sigma^2}\\left(\\frac{n_0\\mu_0 + n\\bar{x}}{n_0 + n}, \\frac{\\sigma^2}{n_0+n}\\right) \\cdot {\\rm IG}_{\\sigma^2}\\left(\\frac12(\\nu_0+n), \\frac12\\left(\\nu_0\\sigma_0^2 + S + \\frac{n_0 n}{n_0+n}(\\mu_0-\\bar{x})^2\\right)\\right).\n\\end{align}</math>\n\nIn other words, the posterior distribution has the form of a product of a normal distribution over ''p''(''μ''&nbsp;|&nbsp;''σ''<sup>2</sup>) times an inverse gamma distribution over ''p''(σ<sup>2</sup>), with parameters that are the same as the update equations above.\n{{hidden end}}\n\n==Occurrence and applications==\nThe occurrence of normal distribution in practical problems can be loosely classified into four categories:\n# Exactly normal distributions;\n# Approximately normal laws, for example when such approximation is justified by the [[central limit theorem]]; and\n# Distributions modeled as normal – the normal distribution being the distribution with [[Principle of maximum entropy|maximum entropy]] for a given mean and variance.\n# Regression problems – the normal distribution being found after systematic effects have been modeled sufficiently well.\n\n=== Exact normality ===\n[[File:QHarmonicOscillator.png|thumb|The ground state of a [[quantum harmonic oscillator]] has the [[Gaussian distribution]].]]\nCertain quantities in [[physics]] are distributed normally, as was first demonstrated by [[James Clerk Maxwell]]. Examples of such quantities are:\n* Probability density function of a ground state in a [[quantum harmonic oscillator]].\n* The position of a particle that experiences [[diffusion]]. If initially the particle is located at a specific point (that is its probability distribution is the [[dirac delta function]]), then after time ''t'' its location is described by a normal distribution with variance ''t'', which satisfies the [[diffusion equation]]&nbsp;<math>\\frac{\\partial}{\\partial t} f(x,t) = \\frac{1}{2} \\frac{\\partial^2}{\\partial x^2} f(x,t)</math>. If the initial location is given by a certain density function <math>g(x)</math>, then the density at time ''t'' is the [[convolution]] of ''g'' and the normal PDF.\n\n=== Approximate normality ===\n''Approximately'' normal distributions occur in many situations, as explained by the [[central limit theorem]]. When the outcome is produced by many small effects acting ''additively and independently'', its distribution will be close to normal. The normal approximation will not be valid if the effects act multiplicatively (instead of additively), or if there is a single external influence that has a considerably larger magnitude than the rest of the effects.\n* In counting problems, where the central limit theorem includes a discrete-to-continuum approximation and where [[Infinite divisibility|infinitely divisible]] and [[Indecomposable distribution|decomposable]] distributions are involved, such as\n** [[binomial distribution|Binomial random variables]], associated with binary response variables;\n** [[Poisson distribution|Poisson random variables]], associated with rare events;\n* [[Thermal radiation]] has a [[Bose–Einstein statistics|Bose–Einstein]] distribution on very short time scales, and a normal distribution on longer timescales due to the central limit theorem.\n\n=== Assumed normality ===\n[[File:Fisher iris versicolor sepalwidth.svg|thumb|right|Histogram of sepal widths for ''Iris versicolor'' from Fisher's [[Iris flower data set]], with superimposed best-fitting normal distribution.]]\n{{quote|I can only recognize the occurrence of the normal curve – the Laplacian curve of errors – as a very abnormal phenomenon. It is roughly approximated to in certain distributions; for this reason, and on account for its beautiful simplicity, we may, perhaps, use it as a first approximation, particularly in theoretical investigations.|{{harvtxt |Pearson |1901 }}}}\nThere are statistical methods to empirically test that assumption, see the above [[Normal distribution#Normality tests|Normality tests]] section.\n* In [[biology]], the ''logarithm'' of various variables tend to have a normal distribution, that is, they tend to have a [[log-normal distribution]] (after separation on male/female subpopulations), with examples including:\n** Measures of size of living tissue (length, height, skin area, weight);<ref>{{harvtxt |Huxley |1932 }}</ref>\n** The ''length'' of ''inert'' appendages (hair, claws, nails, teeth) of biological specimens, ''in the direction of growth''; presumably the thickness of tree bark also falls under this category;\n** Certain physiological measurements, such as blood pressure of adult humans.\n* In finance, in particular the [[Black–Scholes model]], changes in the ''logarithm'' of exchange rates, price indices, and stock market indices are assumed normal (these variables behave like [[compound interest]], not like simple interest, and so are multiplicative). Some mathematicians such as [[Benoit Mandelbrot]] have argued that [[Levy skew alpha-stable distribution|log-Levy distributions]], which possesses [[heavy tails]] would be a more appropriate model, in particular for the analysis for [[stock market crash]]es. The use of the assumption of normal distribution occurring in financial models has also been criticized by [[Nassim Nicholas Taleb]] in his works.\n* [[Propagation of uncertainty|Measurement errors]] in physical experiments are often modeled by a normal distribution. This use of a normal distribution does not imply that one is assuming the measurement errors are normally distributed, rather using the normal distribution produces the most conservative predictions possible given only knowledge about the mean and variance of the errors.<ref>{{cite book|last=Jaynes|first=Edwin T.|year=2003|title=Probability Theory: The Logic of Science|publisher=Cambridge University Press|pages=592–593|url=https://books.google.com/books?id=tTN4HuUNXjgC&pg=PA592|isbn=9780521592710}}</ref>\n* In [[Standardized testing (statistics)|standardized testing]], results can be made to have a normal distribution by either selecting the number and difficulty of questions (as in the [[Intelligence quotient|IQ test]]) or transforming the raw test scores into \"output\" scores by fitting them to the normal distribution. For example, the [[SAT]]'s traditional range of 200–800 is based on a normal distribution with a mean of 500 and a standard deviation of 100.\n[[File:FitNormDistr.tif|thumb|220px|Fitted cumulative normal distribution to October rainfalls, see [[distribution fitting]] ]]\n* Many scores are derived from the normal distribution, including [[percentile rank]]s (\"percentiles\" or \"quantiles\"), [[normal curve equivalent]]s, [[stanine]]s, [[Standard score|z-scores]], and T-scores. Additionally, some behavioral statistical procedures assume that scores are normally distributed; for example, [[Student's t-test|t-tests]] and [[Analysis of variance|ANOVAs]]. [[Bell curve grading]] assigns relative grades based on a normal distribution of scores.\n* In [[hydrology]] the distribution of long duration river discharge or rainfall, e.g. monthly and yearly totals, is often thought to be practically normal according to the [[central limit theorem]].<ref>{{cite book|last=Oosterbaan|first=Roland J. |editor-last=Ritzema |editor-first=Henk P.|chapter=Chapter 6: Frequency and Regression Analysis of Hydrologic Data|year=1994|edition=second revised|title=Drainage Principles and Applications, Publication 16|publisher=International Institute for Land Reclamation and Improvement (ILRI)|location=Wageningen, The Netherlands|pages=175–224|chapter-url=http://www.waterlog.info/pdf/freqtxt.pdf|isbn=978-90-70754-33-4}}</ref> The blue picture, made with [[CumFreq]], illustrates an example of fitting the normal distribution to ranked October rainfalls showing the 90% [[confidence belt]] based on the [[binomial distribution]]. The rainfall data are represented by [[plotting position]]s as part of the [[cumulative frequency analysis]].\n\n=== Produced normality ===\nIn [[regression analysis]], lack of normality in [[Errors and residuals in statistics|residuals]] simply indicates that the model postulated is inadequate in accounting for the tendency in the data and needs to be augmented; in other words, normality in residuals can always be achieved given a properly constructed model.\n\n== Generating values from normal distribution ==\n[[File:Planche de Galton.jpg|thumb|250px|right|The [[bean machine]], a device invented by [[Francis Galton]], can be called the first generator of normal random variables. This machine consists of a vertical board with interleaved rows of pins. Small balls are dropped from the top and then bounce randomly left or right as they hit the pins. The balls are collected into bins at the bottom and settle down into a pattern resembling the Gaussian curve.]]\n\nIn computer simulations, especially in applications of the [[Monte-Carlo method]], it is often desirable to generate values that are normally distributed. The algorithms listed below all generate the standard normal deviates, since a {{nowrap|''N''(''μ, σ''{{su|p=2}})}} can be generated as {{nowrap|''X {{=}} μ + σZ''}}, where ''Z'' is standard normal. All these algorithms rely on the availability of a [[random number generator]] ''U'' capable of producing [[Uniform distribution (continuous)|uniform]] random variates.\n* The most straightforward method is based on the [[probability integral transform]] property: if ''U'' is distributed uniformly on (0,1), then Φ<sup>−1</sup>(''U'') will have the standard normal distribution. The drawback of this method is that it relies on calculation of the [[probit function]] Φ<sup>−1</sup>, which cannot be done analytically. Some approximate methods are described in {{harvtxt |Hart |1968 }} and in the [[error function|erf]] article. Wichura gives a fast algorithm for computing this function to 16 decimal places,<ref>{{cite journal|last=Wichura|first=Michael J.|year=1988|title=Algorithm AS241: The Percentage Points of the Normal Distribution|journal=Applied Statistics|volume=37|pages=477–84|doi=10.2307/2347330|jstor=2347330|issue=3}}</ref> which is used by [[R programming language|R]] to compute random variates of the normal distribution.\n* An easy to program approximate approach, that relies on the [[central limit theorem]], is as follows: generate 12 uniform ''U''(0,1) deviates, add them all up, and subtract 6 – the resulting random variable will have approximately standard normal distribution. In truth, the distribution will be [[Irwin–Hall distribution|Irwin–Hall]], which is a 12-section eleventh-order polynomial approximation to the normal distribution. This random deviate will have a limited range of (−6,&nbsp;6).<ref>{{harvtxt |Johnson |Kotz  |Balakrishnan |1995 |loc=Equation (26.48) }}</ref>\n* The [[Box–Muller transform|Box–Muller method]] uses two independent random numbers ''U'' and ''V'' distributed [[uniform distribution (continuous)|uniformly]] on (0,1). Then the two random variables ''X'' and ''Y''\n:: <math>\n    X = \\sqrt{- 2 \\ln U} \\, \\cos(2 \\pi V) , \\qquad\n    Y = \\sqrt{- 2 \\ln U} \\, \\sin(2 \\pi V) .\n  </math>\n:will both have the standard normal distribution, and will be [[independence (probability theory)|independent]]. This formulation arises because for a [[bivariate normal]] random vector (''X'', ''Y'') the squared norm {{nowrap|''X''<sup>2</sup> + ''Y''<sup>2</sup>}} will have the [[chi-squared distribution]] with two degrees of freedom, which is an easily generated [[exponential distribution|exponential random variable]] corresponding to the quantity −2ln(''U'') in these equations; and the angle is distributed uniformly around the circle, chosen by the random variable ''V''.\n* The [[Marsaglia polar method]] is a modification of the Box–Muller method which does not require computation of the sine and cosine functions. In this method, ''U'' and ''V'' are drawn from the uniform (−1,1) distribution, and then {{nowrap|1=''S'' = ''U''<sup>2</sup> + ''V''<sup>2</sup>}} is computed. If ''S'' is greater or equal to 1, then the method starts over, otherwise the two quantities\n:: <math>X = U\\sqrt{\\frac{-2\\ln S}{S}}, \\qquad  Y = V\\sqrt{\\frac{-2\\ln S}{S}}</math>\n:are returned. Again, ''X'' and ''Y'' are independent, standard normal random variables.\n* The Ratio method<ref>{{harvtxt |Kinderman |Monahan |1977 }}</ref> is a rejection method. The algorithm proceeds as follows:\n** Generate two independent uniform deviates ''U'' and ''V'';\n** Compute ''X'' = {{sqrt|8/''e''}} (''V'' − 0.5)/''U'';\n** Optional: if ''X''<sup>2</sup> ≤ 5 − 4''e''<sup>1/4</sup>''U'' then accept ''X'' and terminate algorithm;\n** Optional: if ''X''<sup>2</sup> ≥ 4''e''<sup>−1.35</sup>/''U'' + 1.4 then reject ''X'' and start over from step 1;\n** If ''X''<sup>2</sup> ≤ −4 ln''U'' then accept ''X'', otherwise start over the algorithm.\n:The two optional steps allow the evaluation of the logarithm in the last step to be avoided in most cases.  These steps can be greatly improved<ref>{{harvtxt|Leva|1992}}</ref> so that the logarithm is rarely evaluated.\n* The [[ziggurat algorithm]]<ref>{{harvtxt |Marsaglia |Tsang |2000 }}</ref> is faster than the Box–Muller transform and still exact. In about 97% of all cases it uses only two random numbers, one random integer and one random uniform, one multiplication and an if-test. Only in 3% of the cases, where the combination of those two falls outside the \"core of the ziggurat\" (a kind of rejection sampling using logarithms), do exponentials and more uniform random numbers have to be employed.\n* Integer arithmetic can be used to sample from the standard normal distribution.<ref>{{harvtxt|Karney|2016}}</ref>  This method is exact in the sense that it satisfies the conditions of ''ideal approximation'';<ref>{{harvtxt|Monahan|1985|loc=section 2}}</ref> i.e., it is equivalent to sampling a real number from the standard normal distribution and rounding this to the nearest representable floating point number.\n* There is also some investigation<ref>{{harvtxt |Wallace |1996}}</ref> into the connection between the fast [[Hadamard transform]] and the normal distribution, since the transform employs just addition and subtraction and by the central limit theorem random numbers from almost any distribution will be transformed into the normal distribution. In this regard a series of Hadamard transforms can be combined with random permutations to turn arbitrary data sets into a normally distributed data.\n\n== Numerical approximations for the normal CDF ==\nThe standard normal [[cumulative distribution function|CDF]] is widely used in scientific and statistical computing.\n\nThe values Φ(''x'') may be approximated very accurately by a variety of methods, such as [[numerical integration]], [[Taylor series]], [[asymptotic series]] and [[Gauss's continued fraction#Of Kummer's confluent hypergeometric function|continued fractions]]. Different approximations are used depending on the desired level of accuracy.\n\n{{unordered list\n\n|1= {{harvtxt |Zelen |Severo |1964 }} give the approximation for Φ(''x'') for ''x > 0'' with the absolute error {{abs|''ε''(''x'')}}&nbsp;<&nbsp;7.5·10<sup>−8</sup> (algorithm [http://www.math.sfu.ca/~cbm/aands/page_932.htm 26.2.17]):\n: <math>\n    \\Phi(x) = 1 - \\varphi(x)\\left(b_1t + b_2t^2 + b_3t^3 + b_4t^4 + b_5t^5\\right) + \\varepsilon(x), \\qquad t = \\frac{1}{1+b_0x},\n  </math>\nwhere ''ϕ''(''x'') is the standard normal PDF, and ''b''<sub>0</sub> = 0.2316419, ''b''<sub>1</sub> = 0.319381530, ''b''<sub>2</sub> = −0.356563782, ''b''<sub>3</sub> =  1.781477937, ''b''<sub>4</sub> = −1.821255978, ''b''<sub>5</sub> = 1.330274429.\n\n|2= {{harvtxt |Hart |1968 }} lists some dozens of approximations – by means of rational functions, with or without exponentials – for the {{mono|erfc()}} function. His algorithms vary in the degree of complexity and the resulting precision, with maximum absolute precision of 24 digits. An algorithm by {{harvtxt |West |2009 }} combines Hart's algorithm 5666 with a [[continued fraction]] approximation in the tail to provide a fast computation algorithm with a 16-digit precision.\n\n|3= {{harvtxt |Cody |1969 }} after recalling Hart68 solution is not suited for erf, gives a solution for both erf and erfc, with maximal relative error bound, via [[rational function|Rational Chebyshev Approximation]].\n\n|4= {{harvtxt |Marsaglia |2004 }} suggested a simple algorithm{{NoteTag|For example, this algorithm is given in the article [[Bc programming language#A translated C function|Bc programming language]].}} based on the Taylor series expansion\n\n: <math>\n    \\Phi(x) = \\frac12 + \\varphi(x)\\left( x + \\frac{x^3} 3 + \\frac{x^5}{3\\cdot5} + \\frac{x^7}{3\\cdot5\\cdot7} + \\frac{x^9}{3\\cdot5\\cdot7\\cdot9} + \\cdots \\right)\n  </math>\n\nfor calculating Φ(''x'') with arbitrary precision. The drawback of this algorithm is comparatively slow calculation time (for example it takes over 300 iterations to calculate the function with 16 digits of precision when {{nowrap|1=''x'' = 10}}).\n\n|5= The [[GNU Scientific Library]] calculates values of the standard normal CDF using Hart's algorithms and approximations with [[Chebyshev polynomial]]s.\n}}\n\nShore (1982) introduced simple approximations that may be incorporated in stochastic optimization models of engineering and operations research, like reliability engineering and inventory analysis. Denoting p=Φ(z), the simplest approximation for the quantile function is:\n\n: <math>z=\\Phi^{-1}(p)=5.5556\\left[1- \\left( \\frac{1-p} p \\right)^{0.1186}\\right],\\qquad p\\ge 1/2 </math>\n\nThis approximation delivers for ''z'' a maximum absolute error of 0.026 (for 0.5&nbsp;≤&nbsp;''p''&nbsp;≤&nbsp;0.9999, corresponding to 0&nbsp;≤&nbsp;''z''&nbsp;≤&nbsp;3.719). For ''p''&nbsp;<&nbsp;1/2 replace ''p'' by 1&nbsp;−&nbsp;''p'' and change sign. Another approximation, somewhat less accurate, is the single-parameter approximation:\n\n: <math> z=-0.4115\\left\\{ \\frac{1-p} p + \\log \\left[ \\frac{1-p} p \\right] - 1 \\right\\}, \\qquad p\\ge 1/2</math>\n\nThe latter had served to derive a simple approximation for the loss integral of the normal distribution, defined by\n\n: <math>\n\\begin{align}\nL(z) & =\\int_z^\\infty (u-z)\\varphi(u) \\, du=\\int_z^\\infty [1-\\Phi (u)] \\, du \\\\[5pt]\nL(z) & \\approx \\begin{cases}\n   0.4115\\left(\\dfrac p {1-p} \\right) - z, & p<1/2,  \\\\  \\\\\n   0.4115\\left( \\dfrac {1-p} p \\right), & p\\ge 1/2.\n\\end{cases} \\\\[5pt]\n\\text{or, equivalently,} \\\\\nL(z) & \\approx \\begin{cases}\n   0.4115\\left\\{ 1-\\log \\left[ \\frac p {1-p} \\right] \\right\\}, & p<1/2, \\\\  \\\\\n   0.4115 \\dfrac{1-p} p, & p\\ge 1/2.\n\\end{cases}\n\\end{align}\n</math>\n\nThis approximation is particularly accurate for the right far-tail (maximum error of 10<sup>−3</sup> for z≥1.4). Highly accurate approximations for the CDF, based on [[Response modeling methodology|Response Modeling Methodology]] (RMM, Shore, 2011, 2012), are shown in Shore (2005).\n\nSome more approximations can be found at: [[Error function#Approximation with elementary functions]]. In particular, small ''relative'' error on the whole domain for the CDF <math>\\Phi</math> and the quantile function <math>\\Phi^{-1}</math> as well, is achieved via an explicitly invertible formula by Sergei Winitzki in 2008.\n\n== History ==\n\n=== Development ===\nSome authors<ref>{{harvtxt |Johnson |Kotz |Balakrishnan |1994 |p=85 }}</ref><ref>{{harvtxt |Le Cam | Lo Yang |2000 |p=74 }}</ref> attribute the credit for the discovery of the normal distribution to [[Abraham de Moivre|de Moivre]], who in 1738{{NoteTag|De Moivre first published his findings in 1733, in a pamphlet \"Approximatio ad Summam Terminorum Binomii {{nowrap|(''a'' + ''b'')<sup>''n''</sup>}} in Seriem Expansi\" that was designated for private circulation only. But it was not until the year 1738 that he made his results publicly available. The original pamphlet was reprinted several times, see for example {{harvtxt |Walker |1985 }}.}} published in the second edition of his \"''[[The Doctrine of Chances]]''\" the study of the coefficients in the [[binomial expansion]] of {{nowrap|(''a'' + ''b'')<sup>''n''</sup>}}. De Moivre proved that the middle term in this expansion has the approximate magnitude of <math>  2/\\sqrt{2\\pi n}</math>, and that \"If ''m'' or ½''n'' be a Quantity infinitely great, then the Logarithm of the Ratio, which a Term distant from the middle by the Interval ''ℓ'', has to the middle Term, is <math > -\\frac{2\\ell\\ell}{n}</math>.\"<ref>De Moivre, Abraham (1733), Corollary I – see {{harvtxt |Walker |1985 |p=77 }}</ref> Although this theorem can be interpreted as the first obscure expression for the normal probability law, [[Stephen Stigler|Stigler]] points out that de Moivre himself did not interpret his results as anything more than the approximate rule for the binomial coefficients, and in particular de Moivre lacked the concept of the probability density function.<ref>{{harvtxt |Stigler |1986 |p=76 }}</ref>\n\n[[File:Carl Friedrich Gauss.jpg|thumb|180px|left|[[Carl Friedrich Gauss]] discovered the normal distribution in 1809 as a way to rationalize the [[method of least squares]].]]\n\nIn 1809 [[Carl Friedrich Gauss|Gauss]] published his monograph <span title=\"Theory of the motion of the heavenly bodies moving about the Sun in conic sections\">\"''Theoria motus corporum coelestium in sectionibus conicis solem ambientium''\"</span> where among other things he introduces several important statistical concepts, such as the [[method of least squares]], the [[method of maximum likelihood]], and the ''normal distribution''. Gauss used ''M'', {{nobr|''M''′}}, {{nobr|''M''′′, …}} to denote the measurements of some unknown quantity&nbsp;''V'', and sought the \"most probable\" estimator of that quantity: the one that maximizes the probability {{nobr|''φ''(''M''&nbsp;−&nbsp;''V'') · ''φ''(''M′''&nbsp;−&nbsp;''V'') · ''φ''(''M''′′&nbsp;−&nbsp;''V'') · …}} of obtaining the observed experimental results. In his notation φΔ is the probability law of the measurement errors of magnitude Δ. Not knowing what the function ''φ'' is, Gauss requires that his method should reduce to the well-known answer: the arithmetic mean of the measured values.{{NoteTag|\"It has been customary certainly to regard as an axiom the hypothesis that if any quantity has been determined by several direct observations, made under the same circumstances and with equal care, the arithmetical mean of the observed values affords the most probable value, if not rigorously, yet very nearly at least, so that it is always most safe to adhere to it.\" — {{harvtxt |Gauss |1809 |loc=section 177 }} }} Starting from these principles, Gauss demonstrates that the only law that rationalizes the choice of arithmetic mean as an estimator of the location parameter, is the normal law of errors:<ref>{{harvtxt |Gauss |1809 |loc=section 177 }}</ref>\n\n: <math>\n    \\varphi\\mathit{\\Delta} = \\frac h {\\surd\\pi} \\, e^{-\\mathrm{hh}\\Delta\\Delta},\n</math> <!-- please do not modify this formula; its spacing and style follow the original as closely as possible -->\n\nwhere ''h'' is \"the measure of the precision of the observations\". Using this normal law as a generic model for errors in the experiments, Gauss formulates what is now known as the non-linear weighted least squares (NWLS) method.<ref>{{harvtxt |Gauss |1809 |loc=section 179 }}</ref>\n\n[[File:Pierre-Simon Laplace.jpg|thumb|180px|right|[[Pierre-Simon Laplace|Marquis de Laplace]] proved the [[central limit theorem]] in 1810, consolidating the importance of the normal distribution in statistics.]]\n\nAlthough Gauss was the first to suggest the normal distribution law, [[Pierre Simon de Laplace|Laplace]] made significant contributions.{{NoteTag|\"My custom of terming the curve the Gauss–Laplacian or ''normal'' curve saves us from proportioning the merit of discovery between the two great astronomer mathematicians.\" quote from {{harvtxt |Pearson |1905 |p=189 }} }} It was Laplace who first posed the problem of aggregating several observations in 1774,<ref>{{harvtxt |Laplace |1774 |loc = Problem III }}</ref> although his own solution led to the [[Laplacian distribution]]. It was Laplace who first calculated the value of the [[Gaussian integral|integral {{nowrap|∫ ''e''<sup>−''t''<sup>2</sup></sup>&nbsp;''dt'' {{=}} {{sqrt|{{pi}}}}}}]] in 1782, providing the normalization constant for the normal distribution.<ref>{{harvtxt |Pearson |1905 |p=189 }}</ref> Finally, it was Laplace who in 1810 proved and presented to the Academy the fundamental [[central limit theorem]], which emphasized the theoretical importance of the normal distribution.<ref>{{harvtxt |Stigler |1986 |p=144 }}</ref>\n\nIt is of interest to note that in 1809 an American mathematician [[Robert Adrain|Adrain]] published two derivations of the normal probability law, simultaneously and independently from Gauss.<ref>{{harvtxt |Stigler |1978 |p=243 }}</ref> His works remained largely unnoticed by the scientific community, until in 1871 they were \"rediscovered\" by [[Cleveland Abbe|Abbe]].<ref>{{harvtxt |Stigler |1978 |p=244 }}</ref>\n\nIn the middle of the 19th century [[James Clerk Maxwell|Maxwell]] demonstrated that the normal distribution is not just a convenient mathematical tool, but may also occur in natural phenomena:<ref>{{harvtxt |Maxwell |1860 |p=23 }}</ref> \"The number of particles whose velocity, resolved in a certain direction, lies between ''x'' and ''x''&nbsp;+&nbsp;''dx'' is\n: <math>\n    \\operatorname{N} \\frac{1}{\\alpha\\;\\sqrt\\pi}\\; e^{-\\frac{x^2}{\\alpha^2}} \\, dx\n  </math> <!-- please do not modify this formula; its spacing and style follow the original as close as possible -->\n\n=== Naming ===\nSince its introduction, the normal distribution has been known by many different names: the law of error, the law of facility of errors, Laplace's second law, Gaussian law, etc. Gauss himself apparently coined the term with reference to the \"normal equations\" involved in its applications, with normal having its technical meaning of orthogonal rather than \"usual\".<ref>Jaynes, Edwin J.; [http://www-biba.inrialpes.fr/Jaynes/cc07s.pdf ''Probability Theory: The Logic of Science'', Ch 7]</ref> However, by the end of the 19th century some authors{{NoteTag|Besides those specifically referenced here, such use is encountered in the works of [[Charles Sanders Peirce|Peirce]], [[Francis Galton|Galton]] ({{harvtxt |Galton |1889 |loc = chapter V }}) and [[Wilhelm Lexis|Lexis]] ({{harvtxt | Lexis |1878 }}, {{harvtxt |Rohrbasser |Véron |2003 }}) c. 1875.{{Citation needed |date=June 2011 }} }} had started using the name ''normal distribution'', where the word \"normal\" was used as an adjective&nbsp;– the term now being seen as a reflection of the fact that this distribution was seen as typical, common&nbsp;– and thus \"normal\". Peirce (one of those authors) once defined \"normal\" thus: \"...the 'normal' is not the average (or any other kind of mean) of what actually occurs, but of what ''would'', in the long run, occur under certain circumstances.\"<ref>Peirce, Charles S. (c. 1909 MS), ''[[Charles Sanders Peirce bibliography#CP|Collected Papers]]'' v. 6, paragraph 327</ref> Around the turn of the 20th century [[Karl Pearson|Pearson]] popularized the term ''normal'' as a designation for this distribution.<ref>{{harvtxt |Kruskal |Stigler |1997 }}</ref>\n{{quote|Many years ago I called the Laplace–Gaussian curve the ''normal'' curve, which name, while it avoids an international question of priority, has the disadvantage of leading people to believe that all other distributions of frequency are in one sense or another 'abnormal'. |{{harvtxt |Pearson |1920 }}}}\n\nAlso, it was Pearson who first wrote the distribution in terms of the standard deviation ''σ'' as in modern notation. Soon after this, in year 1915, [[Ronald Fisher|Fisher]] added the location parameter to the formula for normal distribution, expressing it in the way it is written nowadays:\n: <math> df = \\frac{1}{\\sqrt{2\\sigma^2\\pi}}e^{-(x-m)^2/(2\\sigma^2)} \\, dx </math>\n\nThe term \"standard normal\", which denotes the normal distribution with zero mean and unit variance came into general use around the 1950s, appearing in the popular textbooks by P.G. Hoel (1947) \"''Introduction to mathematical statistics''\" and A.M. Mood (1950) \"''Introduction to the theory of statistics''\".<ref>{{cite web|title=Earliest uses… (entry STANDARD NORMAL CURVE)|url=http://jeff560.tripod.com/s.html}}</ref>\n\nWhen the name is used, the \"Gaussian distribution\" was [[List of topics named after Carl Friedrich Gauss|named after]] [[Carl Friedrich Gauss]], who introduced the distribution in 1809 as a way of rationalizing the [[method of least squares]] as outlined above. Among English speakers, both \"normal distribution\" and \"Gaussian distribution\" are in common use, with different terms preferred by different communities.\n\n== See also ==\n{{Portal|Statistics}}\n* [[Wrapped normal distribution]] — the Normal distribution applied to a circular domain\n* [[Bates distribution]] — similar to the Irwin–Hall distribution, but rescaled back into the 0 to 1 range\n* [[Behrens–Fisher problem]] — the long-standing problem of testing whether two normal samples with different variances have same means;\n* [[Bhattacharyya distance]] – method used to separate mixtures of normal distributions\n* [[Erdős–Kac theorem]]—on the occurrence of the normal distribution in [[number theory]]\n* [[Gaussian blur]]—[[convolution]], which uses the normal distribution as a kernel\n* [[Normally distributed and uncorrelated does not imply independent]]\n* [[Standard normal table]]\n* [[Sub-Gaussian distribution]]\n* [[Sum of normally distributed random variables]]\n* [[Tweedie distribution]] — The normal distribution is a member of the family of Tweedie [[exponential dispersion model]]s\n* [[Z-test]]— using the normal distribution\n*[[Stein's lemma]]\n\n== Notes ==\n{{NoteFoot}}\n\n== References ==\n=== Citations ===\n{{Reflist}}\n\n=== Sources ===\n{{refbegin}}\n* {{cite web |url = http://jeff560.tripod.com/stat.html|title=Earliest Uses of Symbols in Probability and Statistics|last2=Miller|first2=Jeff |date=|website=|access-date=|ref=harv|last1=Aldrich|first1=John}}\n* {{cite web |url = http://jeff560.tripod.com/mathword.html|title=Earliest Known Uses of Some of the Words of Mathematics |last2=Miller|first2=Jeff|date=|website=|access-date=|ref=harv|last1=Aldrich |first1=John }} In particular, the entries for [http://jeff560.tripod.com/b.html \"bell-shaped and bell curve\"], [http://jeff560.tripod.com/n.html \"normal (distribution)\"], [http://jeff560.tripod.com/g.html \"Gaussian\"], and [http://jeff560.tripod.com/e.html \"Error, law of error, theory of errors, etc.\"].\n* {{cite book |title=Methods of Information Geometry|last2=Nagaoka|first2=Hiroshi|publisher=Oxford University Press|year=2000|isbn=978-0-8218-0531-2|location=|pages=|ref=harv|last1=Amari|first1=Shun-ichi}}\n* {{cite book |title=Bayesian Theory|last2=Smith|first2=Adrian F. M.|publisher=Wiley|year=2000|isbn=978-0-471-49464-5 |location=|pages=|ref=harv|last1=Bernardo|first1=José M.}}\n* {{cite book |title=The Normal Distribution: Characterizations with Applications|last=Bryc|first=Wlodzimierz|publisher=Springer-Verlag|year=1995|isbn=978-0-387-97990-8|location=|pages=|ref=harv}}\n* {{cite book |title=Statistical Inference|last2=Berger|first2=Roger L.|publisher=Duxbury|year=2001|isbn=978-0-534-24312-8|edition=2nd|location=|pages=|ref=harv|last1=Casella|first1=George}}\n* {{cite journal |last=Cody|first=William J.|year=1969|title=Rational Chebyshev Approximations for the Error Function|url=|journal=Mathematics of Computation|volume=23|issue=107|pages=631–638|doi=10.1090/S0025-5718-1969-0247736-4|ref=harv|title-link=Error function#cite note-5}}\n* {{cite book |title=Elements of Information Theory|last2=Thomas|first2=Joy A.|publisher=John Wiley and Sons|year=2006|isbn=|location=|pages=|ref=harv|last1=Cover|first1=Thomas M.}}\n* {{cite book |title=The Doctrine of Chances|last=de Moivre|first=Abraham|publisher=|year=1738|isbn=978-0-8218-2103-9|location=|pages=|ref=harv|authorlink=Abraham de Moivre|title-link=The Doctrine of Chances}}\n* {{cite journal |last=Fan|first=Jianqing|year=1991|title=On the optimal rates of convergence for nonparametric deconvolution problems|url=|journal=The Annals of Statistics|volume=19|issue=3|pages=1257–1272|doi=10.1214/aos/1176348248|jstor=2241949|ref=harv}}\n* {{cite book |url=http://galton.org/books/natural-inheritance/pdf/galton-nat-inh-1up-clean.pdf|title=Natural Inheritance|last=Galton|first=Francis|publisher=Richard Clay and Sons|year=1889|isbn=|location=London, UK|pages=|ref=harv}}\n* {{cite book |title=Products of Random Variables: Applications to Problems of Physics and to Arithmetical Functions|last2=Simonelli|first2=Italo|publisher=Marcel Dekker, Inc.|year=2004|isbn=978-0-8247-5402-0|location=|pages=|ref=harv|last1=Galambos|first1=Janos}}\n* {{cite book |title=Theoria motvs corporvm coelestivm in sectionibvs conicis Solem ambientivm|last=Gauss|first=Carolo Friderico|publisher=|year=1809|isbn=|location=|pages=|language=Latin|id=[https://books.google.com/books?id=1TIAAAAAQAAJ English translation]|ref=harv|authorlink=Carl Friedrich Gauss|trans-title=Theory of the Motion of the Heavenly Bodies Moving about the Sun in Conic Sections}}\n* {{cite book |title=The Mismeasure of Man|last=Gould|first=Stephen Jay|publisher=W. W. Norton|year=1981|isbn=978-0-393-01489-1|edition=first|location=|pages=|ref=harv|authorlink=Stephen Jay Gould|title-link=The Mismeasure of Man}}\n* {{cite journal |last2=Hartley|first2=Herman O.|last3=Hoel|first3=Paul G.|year=1965|title=Recommended Standards for Statistical Symbols and Notation. COPSS Committee on Symbols and Notation|url=|journal=The American Statistician|volume=19|issue=3|pages=12–14|doi=10.2307/2681417|jstor=2681417|ref=harv|last1=Halperin|first1=Max}}\n* {{cite book |title=Computer Approximations|last=Hart|first=John F.|publisher=John Wiley & Sons, Inc.|year=1968|isbn=978-0-88275-642-4|location=New York, NY|pages=|ref=harv|display-authors=etal}}\n* {{Springer\n  | title = Normal Distribution\n  | id = p/n067460\n  | ref = harv\n  }}\n* {{cite book |title=The Bell Curve: Intelligence and Class Structure in American Life|last2=Murray|first2=Charles|publisher=[[Free Press (publisher)|Free Press]]|year=1994|isbn=978-0-02-914673-6|location=|pages=|ref=harv|last1=Herrnstein|first1=Richard J.|authorlink2=Charles Murray (political scientist)|title-link=The Bell Curve}}\n* {{cite book|title=Problems of Relative Growth|last=Huxley|first=Julian S.|publisher=London|year=1932|isbn=978-0-486-61114-3|location=|pages=|oclc=476909537|ref=harv}}\n* {{cite book|title=Continuous Univariate Distributions, Volume 1|last2=Kotz|first2=Samuel|last3=Balakrishnan|first3=Narayanaswamy|publisher=Wiley|year=1994|isbn=978-0-471-58495-7|location=|pages=|ref=harv|last1=Johnson|first1=Norman L.}}\n* {{cite book|title=Continuous Univariate Distributions, Volume 2|last2=Kotz|first2=Samuel|last3=Balakrishnan|first3=Narayanaswamy|publisher=Wiley|year=1995|isbn=978-0-471-58494-0|location=|pages=|ref=harv|last1=Johnson|first1=Norman L.}}\n* {{cite journal|last=Karney|first=C. F. F.|year=2016|title=Sampling exactly from the normal distribution|url=|journal=ACM Transactions on Mathematical Software|volume=42|issue=1|pages=3:1–14|arxiv=1303.6257|doi=10.1145/2710016|ref=harv}}\n* {{cite journal|last2=Monahan|first2=John F.|year=1977|title=Computer Generation of Random Variables Using the Ratio of Uniform Deviates|url=|journal=ACM Transactions on Mathematical Software|volume=3|issue=3|pages=257–260|doi=10.1145/355744.355750|ref=harv|first1=Albert J.|last1=Kinderman}}\n* {{cite book|title=Handbook of Statistical Distributions with Applications|last=Krishnamoorthy|first=Kalimuthu|publisher=Chapman & Hall/CRC|year=2006|isbn=978-1-58488-635-8|location=|pages=|ref=harv}}\n* {{cite book|title=Normative Terminology: 'Normal' in Statistics and Elsewhere|last2=Stigler|first2=Stephen M.|publisher=Oxford University Press|year=1997|isbn=978-0-19-852341-3|editor-last = Spencer|editor-first = Bruce D.|series=Statistics and Public Policy|location=|pages=|ref=harv|last1=Kruskal|first1=William H.}}\n* {{cite journal|last=Laplace|first=Pierre-Simon de|year=1774|title=Mémoire sur la probabilité des causes par les événements|url=http://gallica.bnf.fr/ark:/12148/bpt6k77596b/f32|journal=Mémoires de l'Académie Royale des Sciences de Paris (Savants étrangers), Tome 6|volume=|pages=621–656|ref=harv|authorlink=Pierre-Simon Laplace}} Translated by Stephen M. Stigler in ''Statistical Science'' '''1''' (3), 1986: {{jstor|2245476}}.\n* {{cite book|title=Théorie analytique des probabilités|last=Laplace|first=Pierre-Simon|publisher=|year=1812|isbn=|location=|pages=|ref=harv|trans-title=[[Analytical theory of probabilities]]}}\n* {{cite book|title=Asymptotics in Statistics: Some Basic Concepts|last=Le Cam|first=Lucien|last2=Lo Yang|first2=Grace|publisher=Springer|year=2000|isbn=978-0-387-95036-5|edition=second|location=|pages=|ref=harv}}\n* {{cite journal|last1=Leva|first1=Joseph L.|year=1992|title=A fast normal random number generator|url=http://saluc.engr.uconn.edu/refs/crypto/rng/leva92afast.pdf|journal=ACM Transactions on Mathematical Software|volume=18|issue=4|pages=449–453|doi=10.1145/138351.138364|archive-url=https://web.archive.org/web/20100716035328/http://saluc.engr.uconn.edu/refs/crypto/rng/leva92afast.pdf|archive-date=16 July 2010|ref=harv|via=|citeseerx=10.1.1.544.5806}}\n* {{cite journal|last=Lexis|first=Wilhelm|year=1878|title=Sur la durée normale de la vie humaine et sur la théorie de la stabilité des rapports statistiques|url=|journal=Annales de Démographie Internationale|location=Paris|volume=II|pages=447–462|ref=harv}}\n* {{cite journal|last2=King|first2=Edgar P.|year=1954|title=A Property of Normal Distribution|url=|journal=The Annals of Mathematical Statistics|volume=25|issue=2|pages=389–394|doi=10.1214/aoms/1177728796|jstor=2236741|ref=harv|last1=Lukacs|first1=Eugene}}\n* {{cite book|title=Statistics in Scientific Investigation: Its Basis, Application and Interpretation|last=McPherson|first=Glen|publisher=Springer-Verlag|year=1990|isbn=978-0-387-97137-7|location=|pages=|ref=harv}}\n* {{cite journal|last2=Tsang|first2=Wai Wan|year=2000|title=The Ziggurat Method for Generating Random Variables|journal=Journal of Statistical Software|volume=5|issue=8|pages=|doi=10.18637/jss.v005.i08|ref=harv|last1=Marsaglia|first1=George|authorlink1=George Marsaglia}}\n* {{cite journal|last=Marsaglia|first=George|year=2004|title=Evaluating the Normal Distribution|journal=Journal of Statistical Software|volume=11|issue=4|pages=|doi=10.18637/jss.v011.i04|ref=harv}}\n* {{cite journal|last=Maxwell|first=James Clerk|year=1860|title=V. Illustrations of the dynamical theory of gases. — Part I: On the motions and collisions of perfectly elastic spheres|url=|journal=Philosophical Magazine |series=Series 4|volume=19|issue=124|pages=19–32|doi=10.1080/14786446008642818|ref=harv|authorlink=James Clerk Maxwell}}\n* {{cite journal|last=Monahan|first=J. F.|year=1985|title=Accuracy in random number generation|url=|journal=Mathematics of Computation|volume=45|issue=172|pages=559–568|doi=10.1090/S0025-5718-1985-0804945-X|ref=harv}}\n* {{cite book|title=Handbook of the Normal Distribution|last2=Read|first2=Campbell B.|publisher=CRC Press|year=1996|isbn=978-0-8247-9342-5|edition=2nd|location=|pages=|ref=harv|last1=Patel|first1=Jagdish K.}}\n* {{cite journal|last=Pearson|first=Karl|year=1901|title=On Lines and Planes of Closest Fit to Systems of Points in Space|url=http://stat.smmu.edu.cn/history/pearson1901.pdf|journal=[[Philosophical Magazine]]|series=6|volume=2|issue=11|pages=559–572|ref=harv|authorlink=Karl Pearson|doi=10.1080/14786440109462720}}\n* {{cite journal|last=Pearson|first=Karl|year=1905|title='Das Fehlergesetz und seine Verallgemeinerungen durch Fechner und Pearson'. A rejoinder|url=|journal=Biometrika|volume=4|issue=1|pages=169–212|doi=10.2307/2331536|jstor=2331536|ref=harv|authorlink=Karl Pearson}}\n* {{cite journal|last=Pearson|first=Karl|year=1920|title=Notes on the History of Correlation|url=|journal=Biometrika|volume=13|issue=1|pages=25–45|doi=10.1093/biomet/13.1.25|jstor=2331722|ref=harv}}\n* {{cite journal|last2=Véron|first2=Jacques|year=2003|title=Wilhelm Lexis: The Normal Length of Life as an Expression of the \"Nature of Things\"|url=http://www.persee.fr/web/revues/home/prescript/article/pop_1634-2941_2003_num_58_3_18444|journal=Population|volume=58|issue=3|pages=303–322|doi=10.3917/pope.303.0303|ref=harv|first1=Jean-Marc|last1=Rohrbasser}}\n* {{cite journal | last1 = Shore | first1 = H | year = 1982 | title = Simple Approximations for the Inverse Cumulative Function, the Density Function and the Loss Integral of the Normal Distribution | url = | journal = Journal of the Royal Statistical Society. Series C (Applied Statistics) | volume = 31 | issue = 2| pages = 108–114 | doi = 10.2307/2347972 | jstor = 2347972 }}\n* {{cite journal | last1 = Shore | first1 = H | year = 2005 | title = Accurate RMM-Based Approximations for the CDF of the Normal Distribution | url = | journal = Communications in Statistics – Theory and Methods | volume = 34 | issue = 3| pages = 507–513 | doi = 10.1081/sta-200052102 }}\n* {{cite journal | last1 = Shore | first1 = H | year = 2011 | title = Response Modeling Methodology | url = | journal = WIREs Comput Stat | volume = 3 | issue = 4| pages = 357–372 | doi = 10.1002/wics.151 }}\n* {{cite journal | last1 = Shore | first1 = H | year = 2012 | title = Estimating Response Modeling Methodology Models | url = | journal = WIREs Comput Stat | volume = 4 | issue = 3| pages = 323–333 | doi = 10.1002/wics.1199 }}\n* {{cite journal |last=Stigler|first=Stephen M.|year=1978|title=Mathematical Statistics in the Early States|url=|journal=The Annals of Statistics|volume=6|issue=2|pages=239–265|doi=10.1214/aos/1176344123|jstor=2958876|ref=harv|authorlink=Stephen Stigler}}\n* {{cite journal|last=Stigler|first=Stephen M.|year=1982|title=A Modest Proposal: A New Standard for the Normal|url=|journal=The American Statistician|volume=36|issue=2|pages=137–138|doi=10.2307/2684031|jstor=2684031|ref=harv}}\n* {{cite book |title=The History of Statistics: The Measurement of Uncertainty before 1900|last=Stigler|first=Stephen M.|publisher=Harvard University Press|year=1986|isbn=978-0-674-40340-6|location=|pages=|ref=harv}}\n* {{cite book |title=Statistics on the Table|last=Stigler|first=Stephen M.|publisher=Harvard University Press|year=1999|isbn=978-0-674-83601-3|location=|pages=|ref=harv}}\n* {{cite book |title=A Source Book in Mathematics|last=Walker|first=Helen M.|publisher=Dover|year=1985|isbn=978-0-486-64690-9|editor-last = Smith|editor-first = David Eugene|location=|pages=|chapter=De Moivre on the Law of Normal Probability|ref=harv|chapterurl=http://www.york.ac.uk/depts/maths/histstat/demoivre.pdf}}\n* {{cite journal |year=1996|title=Fast pseudo-random generators for normal and exponential variates|url=|journal=ACM Transactions on Mathematical Software|volume=22|issue=1|pages=119–127|doi=10.1145/225545.225554|ref=harv|last1=Wallace|first1=C. S.|authorlink1=Chris Wallace (computer scientist)}}\n* {{cite web|url=http://mathworld.wolfram.com/NormalDistribution.html|title=Normal Distribution|last=Weisstein|first=Eric W.|authorlink=Eric W. Weisstein|date=|website=|publisher=[[MathWorld]]|access-date=|ref=harv}}\n* {{cite journal|last=West|first=Graeme|year=2009|title=Better Approximations to Cumulative Normal Functions|url=http://www.wilmott.com/pdfs/090721_west.pdf|journal=Wilmott Magazine|volume=|pages=70–76|ref=harv}}\n* {{cite book|url=http://www.math.sfu.ca/~cbm/aands/page_931.htm|title=Probability Functions (chapter 26)|last2=Severo|first2=Norman C.|publisher=Dover|year=1964|isbn=978-0-486-61272-0|series=''[[Abramowitz and Stegun|Handbook of mathematical functions with formulas, graphs, and mathematical tables]]'', by [[Milton Abramowitz|Abramowitz, M.]]; and [[Irene A. Stegun|Stegun, I. A.]]: National Bureau of Standards|location=New York, NY|pages=|ref=harv|last1=Zelen|first1=Marvin}}\n{{refend}}\n\n== External links ==\n{{Commons category|Normal distribution}}\n* {{springer|title=Normal distribution|id=p/n067460}}\n* [https://www.hackmath.net/en/calculator/normal-distribution Normal distribution calculator]\n\n{{-}}\n{{ProbDistributions|continuous-infinite}}\n\n[[Category:Continuous distributions]]\n[[Category:Conjugate prior distributions]]\n[[Category:Normal distribution| ]]\n[[Category:Exponential family distributions]]\n[[Category:Stable distributions]]\n[[Category:Location-scale family probability distributions]]"
    },
    {
      "title": "1.96",
      "url": "https://en.wikipedia.org/wiki/1.96",
      "text": "{{Use dmy dates|date=July 2013}}\n[[File:NormalDist1.96.png|250px|thumb|95% of the area under the [[normal distribution]] lies within 1.96 standard deviations of the mean.]]\n\nIn [[probability]] and [[statistics]], '''1.96''' is the approximate value of the 97.5 [[percentile]] point of the [[normal distribution]]. 95% of the area under a [[normal curve]] lies within roughly 1.96 [[standard deviation]]s of the [[mean]], and due to the [[central limit theorem]], this number is therefore used in the construction of approximate 95% [[confidence interval]]s. Its ubiquity is due to the arbitrary but common convention of using confidence intervals with 95% coverage rather than other coverages (such as 90% or 99%).<ref>\n{{Citation| last=Rees | first=DG |title=Foundations of Statistics| page=246 |publisher=CRC Press |isbn=0-412-28560-6| quote=Why 95% confidence? Why not some other ''confidence level''? The use of 95% is partly convention, but levels such as 90%, 98% and sometimes 99.9% are also used.| year=1987}}</ref><ref>{{cite web\n |url         = http://www.itl.nist.gov/div898/handbook/eda/section3/eda352.htm\n |title       = Engineering Statistics Handbook: Confidence Limits for the Mean\n |accessdate  = 2008-02-04\n |publisher   = National Institute of Standards and Technology\n |quote       = Although the choice of confidence coefficient is somewhat arbitrary, in practice 90%, 95%, and 99% intervals are often used, with 95% being the most commonly used.\n |archiveurl  = https://web.archive.org/web/20080205120031/http://www.itl.nist.gov/div898/handbook/eda/section3/eda352.htm\n |archivedate = 5 February 2008\n |deadurl     = yes\n |df          = dmy-all\n}}</ref><ref>\n{{Citation| title=Real-Life Math: Statistics |first1=Eric T | last1=Olson | first2=Tammy Perry |last2=Olson|page=66| publisher=Walch Publishing | year=2000| isbn=0-8251-3863-9 | quote=While other stricter, or looser, limits may be chosen, the 95 percent interval is very often preferred by statisticians.}}</ref><ref>{{cite news|doi=10.1080/03610920802255856|last=Swift| first=MB| title=Comparison of Confidence Intervals for a Poisson Mean - Further Considerations|journal=[[Communications in Statistics]] - Theory and Methods| volume=38| issue=5| pages=748–759| quote=In modern applied practice, almost all confidence intervals are stated at the 95% level.}}</ref> This convention seems particularly common in medical statistics,<ref>{{Citation\n |title       = Why 95% confidence limits?\n |first       = Steve\n |last        = Simon\n |url         = http://www.childrens-mercy.org/stats/ask/why95.asp\n |year        = 2002\n |accessdate  = 2008-02-01\n |archiveurl  = https://web.archive.org/web/20080128190903/http://www.childrens-mercy.org/stats/ask/why95.asp\n |archivedate = 28 January 2008\n |deadurl     = yes\n |df          = dmy-all\n}}</ref><ref>\n{{Citation| url=http://www.consort-statement.org/index.aspx?o=1320| last1=Moher | first1=D | last2= Schulz |first2=KF | last3= Altman |first3=DG | title= The CONSORT statement: revised recommendations for improving the quality of reports of parallel-group randomised trials. | journal =Lancet | year=2001 |volume= 357| issue=9263 | pages=1191–1194| doi=10.1016/S0140-6736(00)04337-3| pmid=11323066| accessdate= 4 February 2008 <!--Added by DASHBot-->}}\n</ref><ref>\n{{Cite web\n |title=Resources for Authors: Research \n |url=http://resources.bmj.com/bmj/authors/types-of-article/research \n |publisher=BMJ Publishing Group Ltd \n |accessdate=2008-02-04 \n |quote=For standard original research articles please provide the following headings and information: [...] results - main results with (for quantitative studies) 95% confidence intervals and, where appropriate, the exact level of statistical significance and the number need to treat/harm \n |archiveurl=http://arquivo.pt/wayback/20090718220240/http://resources.bmj.com/bmj/authors/types-of-article/research\n |archivedate=18 July 2009\n |deadurl=no \n |df=dmy \n}}\n</ref> but is also common in other areas of application, such as earth sciences,<ref>\n{{Citation|quote=For simplicity, we adopt the common earth sciences convention of a 95% confidence interval.| page=79| title=Statistics of Earth Science Data|first=Graham J. |last=Borradaile |publisher=Springer| year=2003| isbn=3-540-43603-0}}</ref> social sciences and business research.<ref>{{Citation|title=Measuring Customer Service Effectiveness|first=Sarah |last= Cook| page=24|quote=Most researchers use a 95 per cent confidence interval | publisher=Gower Publishing| year=2004|isbn=0-566-08538-0}}</ref>\n\nThere is no single accepted name for this number; it is also commonly referred to as the \"standard normal [[Deviation (statistics)|deviate]]\", \"[[normal score]]\" or \"[[Z score]]\" for the 97.5 percentile point, or .975 point.\n\nIf ''X'' has a standard normal distribution, i.e. ''X'' ~ N(0,1),\n\n:<math> \\mathrm{P}(X > 1.96) \\approx 0.025, \\,</math>\n\n:<math> \\mathrm{P}(X < 1.96) \\approx 0.975, \\,</math>\n\nand as the normal distribution is symmetric,\n\n:<math> \\mathrm{P}(-1.96 < X < 1.96) \\approx 0.95. \\,</math>\n\nOne notation for this number is ''z''<sub>.975</sub>.<ref>\n{{Citation| first=J.| last=Gosling | title=Introductory Statistics| publisher=Pascal Press |year=1995 | isbn=1-86441-015-9|pages=78–9}}</ref> \nFrom the [[probability density function]] of the standard normal distribution, the exact value of ''z''<sub>.975</sub> is determined by\n\n:<math> \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{z_{.975}} e^{-x^2/2} \\, \\mathrm{d}x = 0.975.</math>\n\n== History ==\n[[File:Youngronaldfisher2.JPG|thumb|right|200px|Ronald Fisher]]\nThe use of this number in applied statistics can be traced to the influence of [[Ronald Fisher]]'s classic textbook, [[Statistical Methods for Research Workers]], first published in 1925:\n{{quote|text=\n\"The value for which P = .05, or 1 in 20, is 1.96 or nearly 2 ; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not.\"<ref>\n{{Citation| first=Ronald | last=Fisher |author-link=Ronald Fisher |title=[[Statistical Methods for Research Workers]] |year=1925 |isbn=0-05-002170-2|page=47| publisher=Oliver and Boyd| location=Edinburgh}}</ref>\n}}\nIn Table 1 of the same work, he gave the more precise value 1.959964.<ref>\n{{Citation| first=Ronald | last=Fisher |author-link=Ronald Fisher |title=[[Statistical Methods for Research Workers]] |year=1925 |isbn=0-05-002170-2| publisher=Oliver and Boyd| location=Edinburgh}}, [http://psychclassics.yorku.ca/Fisher/Methods/tabI-II.gif Table 1]</ref>\nIn 1970, the value truncated to 20 [[decimal places]] was calculated to be\n:1.95996 39845 40054 23552...<ref>\n{{Citation | title=Tables of Normal Percentile Points | first=John S. | last=White | journal =Journal of the American Statistical Association | volume=65 | number=330   | pages=635–638 | doi=10.2307/2284575 | jstor=2284575 | publisher=American Statistical Association |date=June  1970}}</ref>\n\nThe commonly used approximate value of 1.96 is therefore accurate to better than one part in 50,000, which is more than adequate for applied work.\n\nSome people even use the value of 2 in the place of 1.96, reporting a 95.4% confidence interval as a 95% confidence interval. This isn't recommended, but you might see it occasionally.<ref>{{cite web |title=Estimating the Population Mean Using Intervals |url=http://www.stat.wmich.edu/s216/book/node78.html |website=www.stat.wmich.edu |publisher=Statistical Computation Lab |accessdate=7 August 2018 |archive-url=https://web.archive.org/web/20180704195514/http://www.stat.wmich.edu/s216/book/node78.html |archive-date=4 July 2018 |dead-url=yes |df=dmy-all }}</ref>\n\n== Software functions ==<!-- This section is of practical use in knowing how to compute the value. -->\nThe inverse of the standard normal [[Cumulative distribution function|CDF]] can be used to compute the value. The following is a table of function calls that return 1.96 in some commonly used applications:\n\n{| class=\"wikitable sortable\"\n|-\n! Application\n! Function call\n|-\n| [[Microsoft Excel|Excel]]\n| [https://support.office.com/en-gb/article/NORM-S-INV-function-d6d556b4-ab7f-49cd-b526-5a20918452b1 NORM.S.INV](0.975)\n|-\n| [[MATLAB]]\n| [http://www.mathworks.com/access/helpdesk/help/toolbox/stats/index.html?/access/helpdesk/help/toolbox/stats/norminv.html norminv](0.975)\n|-\n| [[R (programming language)|R]]\n| [http://stat.ethz.ch/R-manual/R-devel/library/stats/html/Normal.html qnorm](0.975)\n|-\n| [[scipy]]\n| [http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html#scipy.stats.norm scipy.stats.norm.ppf](0.975)\n|-\n| [[SAS Institute|SAS]]\n| [http://support.sas.com/documentation/cdl/en/lefunctionsref/69762/HTML/default/viewer.htm#n0gygnmlse8n5in1sp1u2n97lpej.htm probit](0.025);\n|-\n| [[SPSS]]\n| x = COMPUTE [http://www-01.ibm.com/support/knowledgecenter/SSLVMB_21.0.0/com.ibm.spss.statistics.help/invdistfunctionlist.htm IDF.NORMAL](0.975,0,1).\n|-\n| [[Stata]]\n| [https://www.stata.com/help.cgi?density_functions#normal invnormal](0.975)\n|-\n| [[Wolfram Language]] ([[Mathematica]])\n| InverseCDF[NormalDistribution[0, 1], 0.975]<ref>[http://reference.wolfram.com/mathematica/ref/InverseCDF.html InverseCDF], Wolfram Language Documentation Center.</ref><ref>[http://reference.wolfram.com/mathematica/ref/NormalDistribution.html?q=NormalDistribution&lang=en NormalDistribution], Wolfram Language Documentation Center.</ref>\n|}\n\n==See also==\n*[[Margin of error]]\n*[[Probit]]\n*[[Reference range]]\n*[[Standard error (statistics)]]\n*[[68-95-99.7 rule]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n*{{Citation |editor-last=Gardner |editor-first=Martin J |editor2-last=Altman| editor2-first=Douglas G | editor2-link=Doug Altman |title= Statistics with confidence | publisher= BMJ Books|year= 1989 | isbn=978-0-7279-0222-1}}\n\n[[Category:Estimation theory]]\n[[Category:Normal distribution]]\n[[Category:Articles with example code]]"
    },
    {
      "title": "68–95–99.7 rule",
      "url": "https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule",
      "text": "[[File:Empirical rule histogram.svg|thumb|450px|For an approximately [[normal distribution|normal data set]], the values within one standard deviation of the mean account for about 68% of the set; while within two standard deviations account for about 95%; and within three standard deviations account for about 99.7%. Shown percentages are rounded theoretical probabilities intended only to approximate the empirical data derived from a normal population. This rule is generally taught within common core mathematics.]]\n[[File:Standard score and prediction interval.png|thumb|250px|right|Prediction interval (on the [[y-axis]]) given from the [[standard score]] (on the [[x-axis]]). The y-axis is logarithmically scaled (but the values on it are not modified).]]\n\nIn [[statistics]], the '''68–95–99.7 rule''', also known as the '''empirical rule''', is a shorthand used to remember the percentage of values that lie within\na band around the [[Arithmetic mean|mean]] in a [[normal distribution]] with a width of two, four and six [[standard deviation]]s, respectively; more accurately,  68.27%, 95.45%  and 99.73% of the values lie within one, two and three standard deviations of the mean, respectively.\n\nIn mathematical notation, these facts can be expressed as follows, where <span class=\"texhtml\">[[&Chi;]]</span> is an observation from a normally distributed [[random variable]], <span class=\"texhtml\">[[&mu;]]</span> is the mean of the distribution, and <span class=\"texhtml\">[[&sigma;]]</span> is its standard deviation:\n\n:<math>\\begin{align}\n  \\Pr(\\mu-1\\sigma \\le X \\le \\mu+1\\sigma) &\\approx 0.6827 \\\\\n  \\Pr(\\mu-2\\sigma \\le X \\le \\mu+2\\sigma)       &\\approx 0.9545 \\\\\n  \\Pr(\\mu-3\\sigma \\le X \\le \\mu+3\\sigma)       &\\approx 0.9973\n\\end{align}</math>\n\nIn the [[empirical science]]s the so-called  '''three-sigma rule of thumb ''' expresses a conventional heuristic that nearly all values are taken to lie within three standard deviations of the mean, and thus it is empirically useful to treat 99.7% probability as near certainty.<ref>this usage of \"three-sigma rule\" entered common usage in the 2000s, e.g. cited in  {{cite book |title=Schaum's Outline of Business Statistics |publisher=McGraw Hill Professional |year=2003 |page=359 |postscript=none }}, and in {{cite book |first=Erik W. |last=Grafarend |title=Linear and Nonlinear Models: Fixed Effects, Random Effects, and Mixed Models |publisher=Walter de Gruyter |year=2006 |page=553 }}</ref>\nThe usefulness of this heuristic depends significantly on the question under consideration. In the social sciences, a result may be considered \"significant\" if its [[confidence level]] is of the order of a two-sigma effect (95%), while in [[particle physics]], there is a convention of a five-sigma effect (99.99994% confidence) being required to qualify as a [[Discovery (observation)|discovery]].\n\nThe \"three-sigma rule of thumb\" is related to a result also known as the '''three-sigma rule,''' which states that even for non-normally distributed variables, at least 88.8% of cases should fall within properly calculated three-sigma intervals. It follows from [[Chebyshev's Inequality]]. For [[Unimodality#Unimodal probability distribution|unimodal distributions]] the probability of being within the interval is at least 95%. There may be certain assumptions for a distribution that force this probability to be at least 98%.<ref>See:\n* {{cite book |first=D. J. |last=Wheeler |first2=D. S. |last2=Chambers |title=Understanding Statistical Process Control |publisher=SPC Press |year=1992 |url=https://books.google.com/books?id=XvMJAQAAMAAJ }}\n* {{cite book |first=Veronica |last=Czitrom |first2=Patrick D. |last2=Spagon |title=Statistical Case Studies for Industrial Process Improvement |publisher=SIAM |year=1997 |page=342 |url=https://books.google.com/books?id=gEpkYxwDbvgC&pg=PA342 }}\n* {{cite journal |first=F. |last=Pukelsheim |title=The Three Sigma Rule |journal=American Statistician |volume=48 |year=1994 |issue= |pages=88–91|jstor=2684253}}</ref>\n<!--Czitrom has \"more than 98.2%\", but the article still lacks presentation of what the exact assumptions are, and what exactly is a \"properly constructed sigma interval\" for a non-normal distribution.-->\n\n==Cumulative distribution function==\n[[File:Cumulative distribution function for normal distribution, mean 0 and sd 1.png|270px|thumb|Diagram showing the [[cumulative distribution function]] for the normal distribution with mean (''&mu;'') 0 and variance (''&sigma;''<sup>2</sup>)&nbsp;1. ]]\n\nThese numerical values \"68%, 95%, 99.7%\" come from the [[Normal distribution#Cumulative distribution function|cumulative distribution function of the normal distribution]].\n\nThe prediction interval for any [[standard score]] ''z'' corresponds numerically to (1−(1−<span style=\"font-size:100%;\">Φ</span><sub>''&mu;'',''&sigma;''<sup>2</sup></sub>(z))·2).\n\nFor example, <span class=\"texhtml\">Φ(2) ≈ 0.9772</span>, or <span class=\"texhtml\">Pr(''X'' ≤ ''&mu;'' + 2''&sigma;'') ≈ 0.9772</span>, corresponding to a prediction interval of (1&nbsp;−&nbsp;(1&nbsp;−&nbsp;0.97725)·2) =&nbsp;0.9545 =&nbsp;95.45%.\nNote that this is not a symmetrical interval – this is merely the probability that an observation is less than <span class=\"texhtml\">''&mu;'' + 2''&sigma;''</span>.  To compute the probability that an observation is within two standard deviations of the mean (small differences due to rounding):\n:<math>\\Pr(\\mu-2\\sigma \\le X \\le \\mu+2\\sigma)\n = \\Phi(2) - \\Phi(-2)\n \\approx 0.9772 - (1 - 0.9772)\n \\approx 0.9545\n</math>\n\nThis is related to [[confidence interval]] as used in statistics: <math>\\bar{X} \\pm 2\\frac{\\sigma}{\\sqrt{n}}</math> is approximately a 95% confidence interval when <math>\\bar{X}</math> is the average of a sample of size <math>n</math>.\n\n==Normality tests==\n{{main|Normality test}}\nThe \"68–95–99.7 rule\" is often used to quickly get a rough probability estimate of something, given its standard deviation, if the population is assumed to be normal.  It is also used as a simple test for [[outliers]] if the population is assumed normal, and as a [[normality test]] if the population is potentially not normal.\n\nTo pass from a sample to a number of standard deviations, one first computes the [[deviation (statistics)|deviation]], either the [[Errors and residuals in statistics|error or residual]] depending on whether one knows the population mean or only estimates it.  The next step is [[standardizing]] (dividing by the population standard deviation), if the population parameters are known, or [[studentizing]] (dividing by an estimate of the standard deviation), if the parameters are unknown and only estimated.\n\nTo use as a test for outliers or a normality test, one computes the size of deviations in terms of standard deviations, and compares this to expected frequency. Given a sample set, one can compute the [[studentized residual]]s and compare these to the expected frequency: points that fall more than 3 standard deviations from the norm are likely outliers (unless the [[sample size]] is significantly large, by which point one expects a sample this extreme), and if there are many points more than 3 standard deviations from the norm, one likely has reason to question the assumed normality of the distribution. This holds ever more strongly for moves of 4 or more standard deviations.\n\nOne can compute more precisely, approximating the number of extreme moves of a given magnitude or greater by a [[Poisson distribution]], but simply, if one has multiple 4 standard deviation moves in a sample of size 1,000, one has strong reason to consider these outliers or question the assumed normality of the distribution.\n\nFor example, a 6''&sigma;'' event corresponds to a chance of about two [[parts per billion]]. For illustration, if events are taken to occur daily, this would correspond to an event expected every 1.4 million years.  This gives a [[normality test#Back of the envelope test|simple normality test]]: if one witnesses a 6''&sigma;'' in daily data and significantly fewer than 1 million years have passed, then a normal distribution most likely does not provide a good model for the magnitude or frequency of large deviations in this respect.\n\nIn ''[[The Black Swan (Taleb book)|The Black Swan]]'', [[Nassim Nicholas Taleb]] gives the example of risk models according to which the [[Black Monday (1987)|Black Monday]] crash would correspond to a 36-''&sigma;'' event:<!--\naccording to [http://www.wolframalpha.com/input/?i=erf%2836%2Fsqrt%282%29%29 wolphramalpha.com], a \"36-sigma event\" corresponds to an expected frequency of the order of 10<sup>−215</sup>, i.e. beyond astronomically small, perhaps one expected \"event\" in the history of the universe if events take place every planck time in every planck volume, or something insane like that.\nthis is far beyond the applicability of the \"gambler's fallacy\", citing gambler's fallacy here would be like calling \"gambler's fallacy\" if somebody complained about a certain roulette table coming up zero every time during a whole week or so.-->\nthe occurrence of such an event should instantly suggest that the model is flawed, i.e. that the process under consideration is not satisfactorily modelled by a normal distribution. Refined models should then be considered, e.g. by the introduction of [[stochastic volatility]].  In such discussions it is important to be aware of problem of the [[gambler's fallacy]], which states that a single observation of a rare event does not contradict that the event is in fact rare{{Citation needed|reason=It is not obvious that this is also part of the gambler's fallacy|date=November 2016}}. It is the observation of a plurality of purportedly rare events that increasingly [[Belief revision|undermines the hypothesis]] that they are rare, i.e. the validity of the assumed model. A proper modelling of this process of gradual loss of confidence in a hypothesis would involve the designation of [[prior probability]] not just to the hypothesis itself but to all possible alternative hypotheses. For this reason, [[statistical hypothesis testing]] works not so much by confirming a hypothesis considered to be likely, but by [[null hypothesis|refuting hypotheses considered unlikely]].\n\n==Table of numerical values==\nBecause of the exponential tails of the normal distribution, odds of higher deviations decrease very quickly. From the [[Standard deviation#Rules for normally distributed data|rules for normally distributed data]] for a daily event:\n{|class=\"wikitable\" style=\"text-align:center\"\n|-bgcolor=\"#CCCCCC\"\n! Range !! Expected fraction of population inside range\n! Approximate expected frequency outside range\n! Approximate frequency for daily event\n|-\n|{{nobr|&mu; ± 0.5&sigma;}} || {{gaps|0.382|924|922|548|026}} || 2 in 3 || Four or five times a week\n|-\n|&mu; ± &sigma; || {{gaps|0.682|689|492|137|086}} || 1 in 3 || Twice a week\n|-\n|&mu; ± 1.5&sigma; || {{gaps|0.866|385|597|462|284}} || 1 in 7 || Weekly\n|-\n|&mu; ± 2&sigma; || {{gaps|0.954|499|736|103|642}} || 1 in 22 || Every three weeks\n|-\n|&mu; ± 2.5&sigma; || {{gaps|0.987|580|669|348|448}} || 1 in 81 || Quarterly\n|-\n|&mu; ± 3&sigma; || {{gaps|0.997|300|203|936|740}} || 1 in 370 || Yearly\n|-\n|&mu; ± 3.5&sigma; || {{gaps|0.999|534|741|841|929}} || 1 in 2149 || Every six years\n|-\n|&mu; ± 4&sigma; || {{gaps|0.999|936|657|516|334}} || 1 in {{val|15787}} || Every 43 years (twice in a lifetime)\n|-\n|&mu; ± 4.5&sigma; || {{gaps|0.999|993|204|653|751}} || 1 in {{val|147160}} || Every 403 years (once in the modern era)\n|-\n|&mu; ± 5&sigma; || {{gaps|0.999|999|426|696|856}} || 1 in {{val|1744278}} || Every {{val|4776}} years (once in recorded history)\n|-\n|&mu; ± 5.5&sigma; || {{gaps|0.999|999|962|020|875}} || 1 in {{val|26330254}} || Every {{val|72090}} years (thrice in history of [[Homo sapiens|modern humankind]])\n|-\n|&mu; ± 6&sigma; || {{gaps|0.999|999|998|026|825}} || 1 in {{val|506797346}} || Every 1.38 million years (twice in history of [[Homo (genus)|humankind]])\n|-\n|&mu; ± 6.5&sigma; || {{gaps|0.999|999|999|919|680}} || 1 in {{val|12450197393}} || Every 34 million years (twice since the [[extinction of dinosaurs]])\n|-\n|&mu; ± 7&sigma; || {{gaps|0.999|999|999|997|440}} || 1 in {{val|390682215445}} || Every 1.07 billion years (four times in [[history of Earth]])\n|-\n|&mu; ± {{math|<var>x</var>}}&sigma; || [[Error function|<math>\\operatorname{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)</math>]] || 1 in <math>\\tfrac{1}{1-\\operatorname{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)}</math> || Every <math>\\tfrac{1}{1-\\operatorname{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)}</math> days\n|}\n\n==See also==\n* [[p-value|''p''-value]]\n* [[Six Sigma#Sigma levels]]\n* [[Standard score]]\n* [[t-statistic|''t''-statistic]]\n\n==References==\n{{reflist}}\n\n==External links==\n* \"[http://www-stat.stanford.edu/~naras/jsm/NormalDensity/NormalDensity.html The Normal Distribution]\" by Balasubramanian Narasimhan\n* \"[http://www.wolframalpha.com/input/?i=erf%28x%2Fsqrt%282%29%29 Calculate percentage proportion within ''x'' sigmas] at WolframAlpha\n\n{{ProbDistributions|Normal distribution}}\n\n{{DEFAULTSORT:68-95-99.7 rule}}\n[[Category:Normal distribution]]\n[[Category:Statistical approximations]]\n[[Category:Rules of thumb]]\n\n[[pl:Odchylenie standardowe#Dla rozkładu normalnego]]"
    },
    {
      "title": "Anscombe transform",
      "url": "https://en.wikipedia.org/wiki/Anscombe_transform",
      "text": "[[File:Anscombe stabilized stdev.svg|thumb|350px|right|Standard deviation of the transformed Poisson random variable as a function of the mean <math>m</math>.]]\n\nIn [[statistics]], the '''Anscombe transform''', named after [[Francis Anscombe]], is a [[variance-stabilizing transformation]] that transforms a [[random variable]] with a [[Poisson distribution]] into one with an approximately standard [[Normal distribution|Gaussian distribution]]. The Anscombe transform is widely used in photon-limited imaging (astronomy, X-ray) where images naturally follow the Poisson law. The Anscombe transform is usually used to pre-process the data in order to make the [[standard deviation]] approximately constant.  Then [[Noise reduction|denoising]] algorithms designed for the framework of [[additive white Gaussian noise]] are used; the final estimate is then obtained by applying an inverse Anscombe transformation to the denoised data.\n\n==Definition==\nFor the [[Poisson distribution]] the mean <math>m</math> and variance <math>v</math> are not independent: <math>m = v</math>. The Anscombe transform<ref name=\"Anscombe1948\">\n{{Citation\n | last = Anscombe\n | first = F. J.\n | authorlink = Frank Anscombe\n | year = 1948\n | title = The transformation of Poisson, binomial and negative-binomial data\n | periodical = Biometrika\n | volume = 35\n | issue = 3–4\n | pages = 246–254\n | doi = 10.1093/biomet/35.3-4.246\n | jstor = 2332343\n| publisher = [Oxford University Press, Biometrika Trust]\n }}</ref>\n\n: <math>A:x \\mapsto 2 \\sqrt{x + \\tfrac{3}{8}} \\, </math>\n\naims at transforming the data so that the variance is set approximately 1 for large enough mean; for mean zero, the variance is still zero.\n\nIt transforms Poissonian data <math>x</math> (with mean <math>m</math>) to approximately Gaussian data of mean <math>2\\sqrt{m + \\tfrac{3}{8}} - \\tfrac{1}{4 \\, m^{1/2}} + O\\left(\\tfrac{1}{m^{3/2}}\\right)</math> \nand standard deviation <math> 1 + O\\left(\\tfrac{1}{m^2}\\right)</math>. \nThis approximation is good provided that <math>m</math> is larger than 4.{{Citation needed|reason=The claimed lower bound on m (4) is not from the Anscombe paper.|date=March 2013}}\nFor a transformed variable of the form <math>2 \\sqrt{x + c}</math>, the expression for the variance has an additional term <math>\\frac{\\tfrac{3}{8} -c}{m}</math>; it is reduced to zero at <math>c = \\tfrac{3}{8}</math>, which is exactly the reason why this value was picked.\n\n==Inversion==\nWhen the Anscombe transform is used in denoising (i.e. when the goal is to obtain from <math>x</math> an estimate of <math>m</math>), its inverse transform is also needed\nin order to return the variance-stabilized and denoised data <math>y</math> to the original range.\nApplying the [[Inverse function|algebraic inverse]]\n\n: <math>A^{-1}:y \\mapsto \\left( \\frac{y}{2} \\right)^2 - \\frac{3}{8} </math>\n\nusually introduces undesired [[Bias of an estimator|bias]] to the estimate of the mean <math>m</math>, because the forward square-root\ntransform is not [[Linear map|linear]]. Sometimes using the asymptotically unbiased inverse<ref name=\"Anscombe1948\" />\n\n: <math>y \\mapsto \\left( \\frac{y}{2} \\right)^2 - \\frac{1}{8} </math>\n\nmitigates the issue of bias, but this is not the case in photon-limited imaging, for which\nthe exact unbiased inverse given by the implicit mapping<ref>{{Citation\n | last1 = Mäkitalo\n | first1 = M.\n | last2 = Foi\n | first2 = A.\n | year = 2011\n | title = Optimal inversion of the Anscombe transformation in low-count Poisson image denoising\n | periodical = IEEE Transactions on Image Processing\n | volume = 20\n | issue = 1\n | pages = 99–109 \n | doi = 10.1109/TIP.2010.2056693\n| pmid = 20615809\n | bibcode = 2011ITIP...20...99M\n | citeseerx = 10.1.1.219.6735\n }}</ref>\n\n: <math> \\operatorname{E} \\left[ 2\\sqrt{x+\\tfrac{3}{8}} \\mid m \\right] = 2  \\sum_{x=0}^{+\\infty} \\left( \\sqrt{x+\\tfrac{3}{8}} \\cdot \\frac{m^x e^{-m}}{x!} \\right) \\mapsto m </math>\n\nshould be used. A [[Closed-form expression|closed-form]] approximation of this exact unbiased inverse is<ref>{{Citation |last1=Mäkitalo |first1=M. |last2=Foi |first2=A. |year=2011 |title=A closed-form approximation of the exact unbiased inverse of the Anscombe variance-stabilizing transformation |periodical=IEEE Transactions on Image Processing |volume=20 |issue=9 |pages=2697–2698 |doi=10.1109/TIP.2011.2121085|bibcode=2011ITIP...20.2697M }}</ref>\n\n: <math>y \\mapsto \\frac{1}{4} y^2 - \\frac{1}{8} + \\frac{1}{4} \\sqrt{\\frac{3}{2}} y^{-1} - \\frac{11}{8} y^{-2} + \\frac{5}{8} \\sqrt{\\frac{3}{2}} y^{-3}.</math>\n\n==Alternatives==\nThere are many other possible variance-stabilizing transformations for the Poisson distribution. Bar-Lev and Enis report<ref>{{Citation |last1=Bar-Lev |first1=S. K. |last2=Enis |first2=P. |year=1988 |title=On the classical choice of variance stabilizing transformations and an application for a Poisson variate |periodical=Biometrika |volume=75 |issue=4 |pages=803–804 |doi=10.1093/biomet/75.4.803\n}}</ref> a family of such transformations which includes the Anscombe transform. Another member of the family is the Freeman-Tukey transformation<ref>{{Citation |last1=Freeman |first1=M. F. |last2=Tukey |first2=J. W. |authorlink2=John Tukey |year=1950 |title=Transformations related to the angular and the square root |periodical=The Annals of Mathematical Statistics |volume=21 |issue=4 |pages=607–611 |jstor=2236611 |doi=10.1214/aoms/1177729756}}</ref>\n\n: <math>A:x \\mapsto \\sqrt{x+1}+\\sqrt{x}. \\, </math>\n\nA simplified transformation, obtained as the [[Variance-stabilizing transformation|primitive of the reciprocal of the standard deviation of the data]], is\n\n: <math>A:x \\mapsto 2\\sqrt{x} \\, </math>\n\nwhich, while it is not quite so good at stabilizing the variance, has the advantage of being more easily understood.\nIndeed, from the delta method,\n\n<math> V[2\\sqrt{x}] \\approx \\left(\\frac{d (2\\sqrt{m})}{d m} \\right)^2 V[x] = \\left(\\frac{1}{\\sqrt{m}} \\right)^2 m = 1 </math>.\n\n==Generalization==\nWhile the Anscombe transform is appropriate for pure Poisson data, in many applications the data presents also an additive Gaussian component. These cases are treated by a Generalized Anscombe transform<ref>\n{{cite book\n | last1 = Starck\n | first1 = J.L.\n | last2 = Murtagh\n | first2 = F.\n | last3 = Bijaoui\n | first3 = A.\n | year = 1998\n | title =  Image Processing and Data Analysis\n | isbn = 9780521599146\n | publisher = Cambridge University Press\n}}</ref> and its asymptotically unbiased or exact unbiased inverses.<ref>{{Citation\n | last1 = Mäkitalo\n | first1 = M.\n | last2 = Foi\n | first2 = A.\n | year = 2013\n | title = Optimal inversion of the generalized Anscombe transformation for Poisson-Gaussian noise\n | periodical = IEEE Transactions on Image Processing\n | volume = 22\n | issue = 1\n | pages = 91–103 \n | doi = 10.1109/TIP.2012.2202675\n| pmid = 22692910\n | bibcode = 2013ITIP...22...91M\n }}</ref>\n\n==See also==\n*[[Variance-stabilizing transformation]]\n*[[Box–Cox transformation]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n*{{Citation |last1=Starck |first1=J.-L. |last2=Murtagh |first2=F. |year=2001 |title=Astronomical image and signal processing: looking at noise, information and scale |periodical=Signal Processing Magazine, IEEE |volume=18 |issue=2 |pages=30–40 |doi=10.1109/79.916319|bibcode=2001ISPM...18...30S }}\n\n[[Category:Poisson distribution]]\n[[Category:Normal distribution]]\n[[Category:Statistical data transformation]]"
    },
    {
      "title": "Bean machine",
      "url": "https://en.wikipedia.org/wiki/Bean_machine",
      "text": "{{more references|date=March 2018}}\n[[File:Galton box.jpg|thumb|180px|Bean machine]]\n[[File:Galton box.webm|thumb|Galton box in movement]]\nThe '''bean machine''', also known as the '''Galton Board''' or '''quincunx''', is a device invented by Sir [[Francis Galton]]<ref name=\"Galton\">{{cite book|last=Galton|first=Sir Francis|title=Natural Inheritance|date=1894|publisher=Macmillan}}</ref>{{rp|63f}} to demonstrate the [[central limit theorem]], in particular that the [[normal distribution]] is approximate to the [[binomial distribution]]. Among its applications, it afforded insight into [[Regression toward the mean|regression to the mean]] or \"regression to mediocrity\".\n\n==Description==\nThe Galton Board consists of a vertical board with interleaved rows of pegs. Beads are dropped from the top and, when the device is level, bounce either left or right as they hit the pegs. Eventually they are collected into bins at the bottom, where the height of bead columns accumulated in the bins will eventually approximate a [[normal distribution|bell curve]]. Overlaying [[Pascal's triangle]] onto the pins shows the number of different paths that can be taken to get to each bin.<ref name=\"GBweb\">{{cite web|title=The Galton Board|url=http://www.galtonboard.com|website=www.galtonboard.com|publisher=Four Pines Publishing, Inc.|accessdate=2018-03-06|language=en}}</ref>\n\nLarge-scale working models of this device created by [[Charles and Ray Eames]] can be seen in the ''[[Mathematica: A World of Numbers... and Beyond]]'' exhibits permanently on view at the [[Boston Museum of Science]], the [[New York Hall of Science]], or the [[Henry Ford Museum]]<ref name=\"Ford\">{{cite web|title=Henry Ford museum acquires Eames' Mathematica exhibit|url=https://www.liveauctioneers.com/news/top-news/museums/henry-ford-museum-acquires-eames-mathematica-exhibit/|website=Auction Central News|publisher=LiveAuctioneers|accessdate=2018-03-06|date=20 March 2015}}</ref>. Another large-scale version is displayed in the lobby of [[Index Fund Advisors]] in Irvine, California.<ref name=\"IFA\">{{cite web|title=IFA.tv - From Chaos to Order on the Galton Board -- A Random Walker|url=https://www.youtube.com/watch?v=9xUBhhM4vbM&t=10s|accessdate=2018-03-06|date=23 December 2009}}</ref>\n\nBean machines can be constructed for other distributions by changing the shape of the pins or biasing them towards one direction (even bimodal bean machines are possible<ref>Brehmer et al 2018, [https://arxiv.org/abs/1805.12244 \"Mining gold from implicit models to improve likelihood-free inference\"]: [https://github.com/johannbrehmer/simulator-mining-example \"Simulator Mining Example\"]</ref>. A bean machine for the [[log-normal distribution]] (common in [[Log-normal distribution#Occurrence and applications|many natural processes]], particularly biological ones), which uses isoceles triangles of varying widths to 'multiply' the distance the bead travels instead of fixed sizes steps which would 'sum', was constructed by [[Jacobus Kapteyn]] while studying & popularizing the statistics of the log-normal in order to help visualize it & demonstrate its plausibility<ref>Kapteyn 1903, [https://babel.hathitrust.org/cgi/pt?id=wu.89098679251;view=image ''Skew frequency curves in biology and statistics'' v1]; Kapteyn & van Uven 1916, [https://babel.hathitrust.org/cgi/pt?id=mdp.39015017409502;view=1up;seq=9 ''Skew frequency curves in biology and statistics'' v2]</ref>. (As of 1963, it was preserved in the [[University of Groningen]]<ref>[[John Aitchison|Aitchison]] & Brown 1963, [https://www.gwern.net/docs/statistics/1963-aitchison-thelognormaldistribution.pdf#page=24 ''The Lognormal Distribution, with Special Reference to its Uses in Economics'']</ref>) An improved log-normal bean machine, using skewed triangles, which avoids shifting the median of the beads to the left, has been introduced by Eckhard Limpert<ref>Limpert et al 2001, [https://stat.ethz.ch/~stahel/lognormal/bioscience2001.pdf#page=3 \"Log-normal Distributions across the Sciences: Keys and Clues\"]</ref>.\n\n==Distribution of the beads==\nIf a bead bounces to the right ''k'' times on its way down (and to the left on the remaining pegs) it ends up in the ''k''th bin counting from the left. Denoting the number of rows of pegs in a Galton Board by ''n'', the number of paths to the ''k''th bin on the bottom is given by the [[binomial coefficient]] <math>{n\\choose k}</math>. If the probability of bouncing right on a peg is ''p'' (which equals 0.5 on an unbiased level machine) the probability that the ball ends up in the ''k''th bin equals <math>{n\\choose k} p^k (1-p)^{n-k}</math>. This is the probability mass function of a [[binomial distribution]]. The number of rows correspond to the size of a binomial distribution in number of trials, while the probability ''p'' of each pin is the binomial's ''p''.\n\nAccording to the [[central limit theorem]] (more specifically, the [[de Moivre–Laplace theorem]]), the binomial distribution approximates the normal distribution provided that both the number of rows and balls is large. Varying the rows will result in different [[standard deviations]] or widths of the bell-shaped curve or the [[normal distribution]] in the bins.\n\n== Examples ==\n<gallery mode=packed heights=180>\nFile:GaltonBoard.png|Galton Board (7.5 in by 4.5 in)\nFile:Tabuleiros de Galton (antes e depois).jpg|Before and after the spin\nFile:Planche de Galton.jpg|A working replica of the machine (following a slightly modified design)\nFile:Quincunx (Galton Box) - Galton 1889 diagram.png|The bean machine, as drawn by [[Francis Galton|Sir Francis Galton]]\n</gallery>\n\n==History==\n[[Sir Francis Galton]] was fascinated with the order of the bell curve that emerges from the apparent chaos of beads bouncing off of pegs in the Galton Board. He eloquently described this relationship in his book ''Natural Inheritance'' (1889):\n\n<blockquote>Order in Apparent Chaos: I know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the Law of Frequency of Error. The law would have been personified by the Greeks and deified, if they had known of it. It reigns with serenity and in complete self-effacement amidst the wildest confusion. The huger the mob, and the greater the apparent anarchy, the more perfect is its sway. It is the supreme law of Unreason. Whenever a large sample of chaotic elements are taken in hand and marshalled in the order of their magnitude, an unsuspected and most beautiful form of regularity proves to have been latent all along.<ref name=\"Galton\" />{{rp|66}}</blockquote>\n\n==Games==\nSeveral games have been developed utilizing the idea of pins changing the route of balls or other objects: \n*[[Pachinko]]\n*[[Payazzo]]\n*[[Peggle]]\n*[[Pinball]]\n*[[Plinko]]\n*[[The Wall (game show)|The Wall]]\n\n== References ==\n{{lacking ISBN|date=March 2018}}\n{{reflist}}\n\n== External links ==\n{{Commons category|Galton box}} \n* [http://www.galtonboard.com Galton Board informational website with resource links]\n* [https://www.youtube.com/watch?v=AUSKTk9ENzg An {{convert|8|ft|m|adj=mid|-tall}} Probability Machine (named Sir Francis) comparing stock market returns to the randomness of the beans dropping through the quincunx pattern.] from Index Fund Advisors [http://www.ifa.com IFA.com] \n* [http://www.mathsisfun.com/probability/quincunx-explained.html Quincunx and its relationship to normal distribution] from [[Math Is Fun]]\n* [https://www.gwern.net/docs/statistics/order/beanmachine-multistage/index.html A multi-stage bean machine simulation (JS)]\n* [http://www.karlsims.com/marbles/  Pascal's Marble Run: a deterministic Galton board]\n* [https://github.com/babeheim/multiplicative-galton Log-normal bean machine] ([https://twitter.com/babeheim/status/1130463937302917120 animation])\n\n[[Category:Central limit theorem]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Chi distribution",
      "url": "https://en.wikipedia.org/wiki/Chi_distribution",
      "text": "{{broader|Chi-squared distribution}}\n{{refimprove|date=October 2009}}\n{{Probability distribution|\n  name       =chi|\n  type       =density|\n  pdf_image =[[Image:Chi distribution PDF.svg|325px|Plot of the Chi PMF]]<br /><small></small>|\n  cdf_image  =[[Image:Chi distribution CDF.svg|325px|Plot of the Chi CMF]]<br /><small></small>|\n  parameters =<math>k>0\\,</math> (degrees of freedom)|\n  support    =<math>x\\in [0,\\infty)</math>|\n  pdf        =<math>\\frac{1}{2^{(k/2)-1}\\Gamma(k/2)}\\;x^{k-1}e^{-x^2/2}</math>|\n  cdf        =<math>P(k/2,x^2/2)\\,</math>|\n  mean       =<math>\\mu=\\sqrt{2}\\,\\frac{\\Gamma((k+1)/2)}{\\Gamma(k/2)}</math>|\n  median     = <math>\\approx \\sqrt{k\\bigg(1-\\frac{2}{9k}\\bigg)^3}</math>|\n  mode       =<math>\\sqrt{k-1}\\,</math> for <math>k\\ge 1</math>|\n  variance   =<math>\\sigma^2=k-\\mu^2\\,</math>|\n  skewness   =<math>\\gamma_1=\\frac{\\mu}{\\sigma^3}\\,(1-2\\sigma^2)</math>|\n  kurtosis   =<math>\\frac{2}{\\sigma^2}(1-\\mu\\sigma\\gamma_1-\\sigma^2)</math>|\n  entropy    =<math>\\ln(\\Gamma(k/2))+\\,</math><br /><math>\\frac{1}{2}(k\\!-\\!\\ln(2)\\!-\\!(k\\!-\\!1)\\psi_0(k/2))</math>|\n  mgf        =Complicated (see text)|\n  char       =Complicated (see text)|\n}}\nIn [[probability theory]] and [[statistics]], the '''chi distribution''' is a continuous [[probability distribution]].  It is the distribution of the positive square root of the sum of squares of a set of independent random variables each following a standard [[normal distribution]], or equivalently, the distribution of the [[Euclidean distance]] of the random variables from the origin. It is thus related to the [[chi-squared distribution]] by describing the distribution of the positive square roots of a variable obeying a chi-squared distribution.\n\nThe most familiar examples are the [[Rayleigh distribution]] (chi distribution with two [[degrees of freedom]]) and the [[Maxwell–Boltzmann distribution]] of the molecular speeds in an [[ideal gas]] (chi distribution with three degrees of freedom). \n\nIf <math>Z_i</math> are {{mvar|k}} independent, [[normal distribution|normally distributed]] random variables with mean 0 and [[standard deviation]]&nbsp;1, then the statistic\n:<math>Y = \\sqrt{\\sum_{i=1}^k Z_i^2}</math>\n\nis distributed according to the chi distribution. Accordingly, dividing by the mean of the chi distribution (scaled by the square root of ''n''&nbsp;&minus;&nbsp;1) yields the correction factor in the [[Unbiased estimation of standard deviation#Results for the normal distribution|unbiased estimation of the standard deviation of the normal distribution]]. The chi distribution has one parameter: <math>k</math> which specifies the number of [[Degrees of freedom (statistics)|degrees of freedom]] (i.e. the number of <math>X_i</math>).\n\n==Characterization==\n\n=== Probability density function ===\nThe [[probability density function]] (pdf) of the chi-distribution is\n:<math>f(x;k) = \\begin{cases}\n\\dfrac{x^{k-1}e^{-x^2/2}}{2^{k/2-1}\\Gamma\\left(\\frac{k}{2}\\right)}, & x\\geq 0; \\\\ 0, & \\text{otherwise}.\n\\end{cases}\n</math>\n\nwhere <math>\\Gamma(z)</math> is the [[gamma function]].\n\n===Cumulative distribution function===\nThe cumulative distribution function is given by:\n\n:<math>F(x;k)=P(k/2,x^2/2)\\,</math>\n\nwhere <math>P(k,x)</math> is the [[regularized gamma function]].\n\n===Generating functions===\n\nThe [[moment-generating function]] is given by:\n\n:<math>M(t)=M\\left(\\frac{k}{2},\\frac{1}{2},\\frac{t^2}{2}\\right)+t\\sqrt{2}\\,\\frac{\\Gamma((k+1)/2)}{\\Gamma(k/2)} M\\left(\\frac{k+1}{2},\\frac{3}{2},\\frac{t^2}{2}\\right),</math>\n\nwhere <math>M(a,b,z)</math> is Kummer's [[confluent hypergeometric function]]. The [[Characteristic function (probability theory)|characteristic function]] is given by:\n\n:<math>\\varphi(t;k)=M\\left(\\frac{k}{2},\\frac{1}{2},\\frac{-t^2}{2}\\right) + it\\sqrt{2}\\,\\frac{\\Gamma((k+1)/2)}{\\Gamma(k/2)} M\\left(\\frac{k+1}{2},\\frac{3}{2},\\frac{-t^2}{2}\\right).</math>\n\n==Properties==\n\n=== Moments ===\nThe raw [[moment (mathematics)|moments]] are then given by:\n\n:<math>\\mu_j=2^{j/2}\\frac{\\Gamma((k+j)/2)}{\\Gamma(k/2)}</math>\n\nwhere <math>\\Gamma(z)</math> is the [[gamma function]]. The first few raw moments are:\n\n:<math>\\mu_1=\\sqrt{2}\\,\\,\\frac{\\Gamma((k\\!+\\!1)/2)}{\\Gamma(k/2)}</math>\n:<math>\\mu_2=k\\,</math>\n:<math>\\mu_3=2\\sqrt{2}\\,\\,\\frac{\\Gamma((k\\!+\\!3)/2)}{\\Gamma(k/2)}=(k+1)\\mu_1</math>\n:<math>\\mu_4=(k)(k+2)\\,</math>\n:<math>\\mu_5=4\\sqrt{2}\\,\\,\\frac{\\Gamma((k\\!+\\!5)/2)}{\\Gamma(k/2)}=(k+1)(k+3)\\mu_1</math>\n:<math>\\mu_6=(k)(k+2)(k+4)\\,</math>\n\nwhere the rightmost expressions are derived using the recurrence relationship for the gamma function:\n\n:<math>\\Gamma(x+1)=x\\Gamma(x)\\,</math>\n\nFrom these expressions we may derive the following relationships:\n\nMean: <math>\\mu=\\sqrt{2}\\,\\,\\frac{\\Gamma((k+1)/2)}{\\Gamma(k/2)}</math>\n\nVariance: <math>\\sigma^2=k-\\mu^2\\,</math>\n\nSkewness: <math>\\gamma_1=\\frac{\\mu}{\\sigma^3}\\,(1-2\\sigma^2)</math>\n\nKurtosis excess: <math>\\gamma_2=\\frac{2}{\\sigma^2}(1-\\mu\\sigma\\gamma_1-\\sigma^2)</math>\n\n===Entropy===\nThe entropy is given by:\n\n:<math>S=\\ln(\\Gamma(k/2))+\\frac{1}{2}(k\\!-\\!\\ln(2)\\!-\\!(k\\!-\\!1)\\psi^0(k/2))</math>\n\nwhere <math>\\psi^0(z)</math> is the [[polygamma function]].\n\n==Related distributions==\n*If <math>X \\sim \\chi_k(x)</math> then <math>X^2 \\sim \\chi^2_k</math> ([[chi-squared distribution]])\n*<math> \\lim_{k \\to \\infty}\\tfrac{\\chi_k(x)-\\mu_k}{\\sigma_k}  \\xrightarrow{d}\\ N(0,1) \\,</math> ([[Normal distribution]])\n*If <math> X \\sim N(0,1)\\,</math> then <math>| X | \\sim \\chi_1(x) \\,</math>\n*If <math>X \\sim \\chi_1(x) \\,</math> then <math>\\sigma X \\sim HN(\\sigma)\\,</math> ([[half-normal distribution]]) for any <math> \\sigma > 0 \\, </math>\n*<math> \\chi_2(x) \\sim \\mathrm{Rayleigh}(1)\\,</math> ([[Rayleigh distribution]]) \n*<math> \\chi_3(x) \\sim \\mathrm{Maxwell}(1)\\,</math> ([[Maxwell distribution]]) \n*<math> \\|\\boldsymbol{N}_{i=1,\\ldots,k}{(0,1)}\\|_2 \\sim \\chi_k(x) </math> (The [[Norm (mathematics)#Euclidean norm|2-norm]] of <math> k </math> standard normally distributed variables is a chi distribution with <math> k </math> [[Degrees of freedom (statistics)|degrees of freedom]])\n*chi distribution is a special case of the [[generalized gamma distribution]] or the [[Nakagami distribution]] or the [[noncentral chi distribution]]\n\n<center>\n{| class=\"wikitable\"\n|+ '''Various chi and chi-squared distributions'''\n|-\n! Name !! Statistic\n|-\n| [[chi-squared distribution]] || <math>\\sum_{i=1}^k \\left(\\frac{X_i-\\mu_i}{\\sigma_i}\\right)^2</math>\n|-\n| [[noncentral chi-squared distribution]] || <math>\\sum_{i=1}^k \\left(\\frac{X_i}{\\sigma_i}\\right)^2</math>\n|-\n| chi distribution || <math>\\sqrt{\\sum_{i=1}^k \\left(\\frac{X_i-\\mu_i}{\\sigma_i}\\right)^2}</math>\n|-\n| [[noncentral chi distribution]] || <math>\\sqrt{\\sum_{i=1}^k \\left(\\frac{X_i}{\\sigma_i}\\right)^2}</math>\n|}\n</center>\n\n==See also==\n*[[Nakagami distribution]]\n\n==References==\n*Martha L. Abell, James P. Braselton, John Arthur Rafter, John A. Rafter, ''Statistics with Mathematica'' (1999), [https://books.google.co.uk/books?id=k3rkxOURuOMC&pg=PA237 237f.]\n*Jan W. Gooch, ''Encyclopedic Dictionary of Polymers'' vol. 1 (2010), Appendix E,  [https://books.google.co.uk/books?id=HRgy8iHQtdwC&pg=PA972 p. 972].\n\n==External links==\n* http://mathworld.wolfram.com/ChiDistribution.html\n{{ProbDistributions|continuous-semi-infinite}}\n\n{{DEFAULTSORT:Chi Distribution}}\n[[Category:Continuous distributions]]\n[[Category:Normal distribution]]\n[[Category:Exponential family distributions]]"
    },
    {
      "title": "Chi-squared distribution",
      "url": "https://en.wikipedia.org/wiki/Chi-squared_distribution",
      "text": "{{About|the mathematics of the chi-squared distribution|its uses in statistics|chi-squared test|the music group|Chi2 (band)}}\n\n{{Probability distribution\n  | name       = chi-squared\n  | type       = density\n  | pdf_image  = [[File:Chi-square pdf.svg|321px]]\n  | cdf_image  = [[File:Chi-square cdf.svg|321px]]\n  | notation   = <math>\\chi^2(k)\\;</math> or <math>\\chi^2_k\\!</math>\n  | parameters = <math>k \\in \\mathbb{N}_{>0}~~</math>   (known as \"degrees of freedom\")\n  | support    = <math>x \\in (0, +\\infty)\\;</math> if <math>k = 1</math>, otherwise <math>x \\in [0, +\\infty)\\;</math>\n  | pdf        = <math>\\frac{1}{2^{k/2}\\Gamma(k/2)}\\; x^{k/2-1} e^{-x/2}\\; </math>\n  | cdf        = <math>\\frac{1}{\\Gamma(k/2 )} \\; \\gamma\\left(\\frac{k}{2},\\,\\frac{x}{2}\\right)\\;</math>\n  | mean       = <math>k</math>\n  | median     = <math>\\approx k\\bigg(1-\\frac{2}{9k}\\bigg)^3\\;</math>\n  | mode       = <math>\\max(k-2,0)\\;</math>\n  | variance   = <math>2k\\;</math>\n  | skewness   = <math>\\scriptstyle\\sqrt{8/k}\\,</math>\n  | kurtosis   = <math>\\frac{12}{k}</math>\n  | entropy    = <math>\\begin{align}\\tfrac{k}{2}&+\\log(2\\Gamma(k/2)) \\\\ &\\!+(1-k/2)\\psi(k/2) \\,{\\scriptstyle\\text{(nats)}}   \\end{align}</math>\n  | mgf        = <math>(1-2t)^{-k/2} \\text{ for } t < \\frac{1}{2}\\;</math>\n  | char       = <math>(1-2it)^{-k/2}</math>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<ref>{{cite web | url=http://www.planetmathematics.com/CentralChiDistr.pdf | title=Characteristic function of the central chi-squared distribution | author=M.A. Sanders | accessdate=2009-03-06 | archive-url=https://web.archive.org/web/20110715091705/http://www.planetmathematics.com/CentralChiDistr.pdf# | archive-date=2011-07-15 | dead-url=yes | df= }}</ref>\n|pgf=<math>(1-2\\ln t)^{-k/2} \\text{ for } 0<t<\\sqrt{e}\\;</math>}}\n\nIn [[probability theory]] and [[statistics]], the '''chi-squared distribution''' (also '''chi-square''' or {{nowrap|1='''<span style=\"font-family:serif\">''χ''</span><sup>2</sup>-distribution'''}}) with {{mvar|k}} [[Degrees of freedom (statistics)|degrees of freedom]] is the distribution of a sum of the squares of {{mvar|k}} [[Independence (probability theory)|independent]] [[standard normal]] random variables. The chi-squared distribution is a special case of the [[gamma distribution]] and is one of the most widely used [[probability distribution]]s in [[inferential statistics]], notably in [[hypothesis testing]] or in construction of [[confidence interval]]s.<ref name=abramowitz>{{Abramowitz_Stegun_ref|26|940}}</ref><ref>NIST (2006).  [http://www.itl.nist.gov/div898/handbook/eda/section3/eda3666.htm Engineering Statistics Handbook – Chi-Squared Distribution]</ref><ref name=\"Johnson_et_al\">{{cite book\n  | last = Johnson\n  | first = N. L.\n  | first2 = S. |last2=Kotz |first3=N. |last3=Balakrishnan\n  | title = Continuous Univariate Distributions |edition=Second |volume=1 |chapter=Chi-Squared Distributions including Chi and Rayleigh |pages=415–493\n  | publisher = John Wiley and Sons\n  | year = 1994\n  | isbn = 978-0-471-58495-7\n}}</ref><ref>{{cite book\n  | last = Mood\n  | first = Alexander\n  | first2=Franklin A. |last2=Graybill |first3=Duane C. |last3=Boes\n  | title = Introduction to the Theory of Statistics |edition=Third |pages=241–246\n  | publisher = McGraw-Hill\n  | year = 1974\n  | isbn = 978-0-07-042864-5\n}}</ref> When it is being distinguished from the more general [[noncentral chi-squared distribution]], this distribution is sometimes called the '''central chi-squared distribution'''.\n\nThe chi-squared distribution is used in the common [[chi-squared test]]s for [[goodness of fit]] of an observed distribution to a theoretical one, the [[statistical independence|independence]] of two criteria of classification of [[data analysis|qualitative data]], and in confidence interval estimation for a population [[standard deviation]] of a normal distribution from a sample standard deviation.  Many other statistical tests also use this distribution, such as [[Friedman test|Friedman's analysis of variance by ranks]].\n\n==Definition==\nIf ''Z''<sub>1</sub>, ..., ''Z''<sub>''k''</sub> are [[independence (probability theory)|independent]], [[standard normal]] random variables, then the sum of their squares,\n: <math>\n    Q\\ = \\sum_{i=1}^k Z_i^2 ,\n  </math>\nis distributed according to the chi-squared distribution with ''k'' degrees of freedom. This is usually denoted as\n: <math>\n    Q\\ \\sim\\ \\chi^2(k)\\ \\ \\text{or}\\ \\ Q\\ \\sim\\ \\chi^2_k .\n  </math>\n\nThe chi-squared distribution has one parameter: ''k'', a positive integer that specifies the number of [[degrees of freedom (statistics)|degrees of freedom]] (the number of ''Z''<sub>''i''</sub>’s).\n\n==Introduction==\n\nThe chi-squared distribution is used primarily in hypothesis testing, and to a lesser extent for confidence intervals for population variance when the underlying distribution is normal. Unlike more widely known distributions such as the [[normal distribution]] and the [[exponential distribution]], the chi-squared distribution is not as often applied in the direct modeling of natural phenomena. It arises in the following hypothesis tests, among others:\n\n*[[Pearson's chi-squared test|Chi-squared test]] of independence in [[contingency tables]]\n*[[Pearson's chi-squared test|Chi-squared test]] of goodness of fit of observed data to hypothetical distributions\n*[[Likelihood-ratio test]] for nested models\n*[[Log-rank test]] in survival analysis\n*[[Cochran–Mantel–Haenszel statistics|Cochran–Mantel–Haenszel test]] for stratified contingency tables\n\nIt is also a component of the definition of the [[Student's t-distribution|t-distribution]] and the [[F-distribution]] used in t-tests, analysis of variance, and regression analysis.\n\nThe primary reason that the chi-squared distribution is used extensively in hypothesis testing is its relationship to the normal distribution. Many hypothesis tests use a test statistic, such as the [[t-statistic]] in a t-test. For these hypothesis tests, as the sample size, n, increases, the [[sampling distribution]] of the test statistic approaches the normal distribution ([[central limit theorem]]). Because the test statistic (such as t) is asymptotically normally distributed, provided the sample size is sufficiently large, the distribution used for hypothesis testing may be approximated by a normal distribution. Testing hypotheses using a normal distribution is well understood and relatively easy. The simplest chi-squared distribution is the square of a standard normal distribution. So wherever a normal distribution could be used for a hypothesis test, a chi-squared distribution could be used.\n\nSuppose that ''Z'' is a random variable sampled from the standard normal distribution, where the mean equals to 0 and the variance equals to 1: ''Z'' ~ N(0,1). Now, consider the random variable Q = Z<sup>2</sup>. The distribution of the random variable Q is an example of a chi-squared distribution: <math>\n     \\ Q\\ \\sim\\ \\chi^2_1 .\n  </math> The subscript 1 indicates that this particular chi-squared distribution is constructed from only 1 standard normal distribution. A chi-squared distribution constructed by squaring a single standard normal distribution is said to have 1 degree of freedom. Thus, as the sample size for a hypothesis test increases, the distribution of the test statistic approaches a normal distribution, and the distribution of the ''square'' of the test statistic approaches a chi-squared distribution. Just as extreme values of the normal distribution have low probability (and give small p-values), extreme values of the chi-squared distribution have low probability.\n\nAn additional reason that the chi-squared distribution is widely used is that it is a member of the class of [[Likelihood-ratio test|likelihood ratio tests]] (LRT).<ref name=Westfall2013>{{cite book|last1=Westfall|first1=Peter H.|title=Understanding Advanced Statistical Methods|date=2013|publisher=CRC Press|location=Boca Raton, FL|isbn=978-1-4665-1210-8}}</ref> LRT's have several desirable properties; in particular, LRT's commonly provide the highest power to reject the null hypothesis ([[Neyman–Pearson lemma]]). However, the normal and chi-squared approximations are only valid asymptotically. For this reason, it is preferable to use the t distribution rather than the normal approximation or the chi-squared approximation for a small sample size. Similarly, in analyses of contingency tables, the chi-squared approximation will be poor for a small sample size, and it is preferable to use [[Fisher's exact test]]. Ramsey shows that the exact [[binomial test]] is always more powerful than the normal approximation.<ref name=Ramsey1988>{{cite journal|last1=Ramsey|first1=PH|title=Evaluating the Normal Approximation to the Binomial Test|journal=Journal of Educational Statistics|date=1988|volume=13|issue=2|pages=173–82|doi=10.2307/1164752|jstor=1164752}}</ref>\n\nLancaster shows the connections among the binomial, normal, and chi-squared distributions, as follows.<ref name=\"Lancaster1969\">{{Citation\n|last=Lancaster\n|first=H.O.\n|title=The Chi-squared Distribution\n|year=1969\n|publisher=Wiley\n}}</ref> De Moivre and Laplace established that a binomial distribution could be approximated by a normal distribution. Specifically they showed the asymptotic normality of the random variable\n\n:<math> \\chi = {m - Np \\over \\sqrt{Npq}} </math>\n\nwhere ''m'' is the observed number of successes in ''N'' trials, where the probability of success is ''p'', and ''q'' = 1 − ''p''.\n\nSquaring both sides of the equation gives\n\n:<math> \\chi^2 = {(m - Np)^2\\over Npq} </math>\n\nUsing ''N'' = ''Np'' + ''N''(1 − ''p''), ''N'' = ''m'' + (''N'' − ''m''), and ''q'' = 1 − ''p'', this equation simplifies to\n\n:<math> \\chi^2 = {(m - Np)^2\\over Np} + {(N - m - Nq)^2\\over Nq} </math>\n\nThe expression on the right is of the form that Pearson would generalize to the form:\n\n:<math> \\chi^2 = \\sum_{i=1}^n \\frac{(O_i - E_i)^2}{E_i}  </math>\n\nwhere\n\n:<math> \\chi^2</math> = Pearson's cumulative test statistic, which asymptotically approaches a <math>\\chi^2</math> distribution.\n:<math>O_i</math> = the number of observations of type ''i''.\n:<math>E_i = N p_i</math> = the expected (theoretical) frequency of type ''i'', asserted by the null hypothesis that the fraction of type ''i'' in the population is <math> p_i</math>\n:<math>n</math>  = the number of cells in the table.\n\nIn the case of a binomial outcome (flipping a coin), the binomial distribution may be approximated by a normal distribution (for sufficiently large n). Because the square of a standard normal distribution is the chi-squared distribution with one degree of freedom, the probability of a result such as 1 heads in 10 trials can be approximated either by the normal or the chi-squared distribution. However, many problems involve more than the two possible outcomes of a binomial, and instead require 3 or more categories, which leads to the multinomial distribution. Just as de Moivre and Laplace sought for and found the normal approximation to the binomial, Pearson sought for and found a multivariate normal approximation to the multinomial distribution. Pearson showed that the chi-squared distribution, the sum of multiple normal distributions, was such an approximation to the multinomial distribution <ref name=\"Lancaster1969\" />\n\n==Characteristics==\nFurther properties of the chi-squared distribution can be found in the box at the upper right corner of this article.\n\n===Probability density function===\nThe [[probability density function]] (pdf) of the chi-square distribution is\n:<math>\nf(x;\\,k) =\n\\begin{cases}\n  \\dfrac{x^{\\frac k 2 -1} e^{-\\frac x 2}}{2^{\\frac k 2} \\Gamma\\left(\\frac k 2 \\right)},  & x > 0; \\\\ 0, & \\text{otherwise}.\n\\end{cases}\n</math>\nwhere <math display=\"inline\">\\Gamma(k/2)</math> denotes the [[gamma function]], which has [[particular values of the gamma function|closed-form values for integer ''k'']].\n\nFor derivations of the pdf in the cases of one, two and ''k'' degrees of freedom, see [[Proofs related to chi-squared distribution]].\n\n===Cumulative distribution function===\n\n[[File:Chernoff-bound.svg|thumb|right|400px|Chernoff bound for the [[Cumulative distribution function|CDF]] and tail (1-CDF) of a chi-squared random variable with ten degrees of freedom (''k'' = 10) ]]\n\nIts [[cumulative distribution function]] is:\n: <math>\n    F(x;\\,k) = \\frac{\\gamma(\\frac{k}{2},\\,\\frac{x}{2})}{\\Gamma(\\frac{k}{2})} = P\\left(\\frac{k}{2},\\,\\frac{x}{2}\\right),\n  </math>\nwhere <math>\\gamma(s,t)</math> is the [[lower incomplete gamma function]] and <math display=\"inline\">P(s,t)</math> is the [[Regularized gamma function#Regularized Gamma functions and Poisson random variables|regularized gamma function]].\n\nIn a special case of ''k'' = 2 this function has a simple form:{{citation needed|reason=Deriving this equation is not obvious and would provide simpler implementation in many cases|date=January 2016}}\n: <math>\n    F(x;\\,2) = 1 - e^{-x/2}\n  </math>\nand the integer recurrence of the gamma function makes it easy to compute for other small even ''k''.\n\nTables of the chi-squared cumulative distribution function are widely available and the function is included in many [[spreadsheet]]s and all [[List of statistical packages|statistical packages]].\n\nLetting <math>z \\equiv x/k</math>, [[Chernoff bound#The first step in the proof of Chernoff bounds|Chernoff bounds]] on the lower and upper tails of the CDF may be obtained.<ref>{{cite journal |last1=Dasgupta |first1=Sanjoy D. A. |last2=Gupta |first2=Anupam K. |date=January 2003 |title=An Elementary Proof of a Theorem of Johnson and Lindenstrauss |journal=Random Structures and Algorithms |volume=22 |issue=1 |pages=60–65 |doi=10.1002/rsa.10073 |url=http://cseweb.ucsd.edu/~dasgupta/papers/jl.pdf |accessdate=2012-05-01 }}</ref>  For the cases when <math>0 < z < 1</math> (which include all of the cases when this CDF is less than half):\n: <math>\n    F(z k;\\,k) \\leq (z e^{1-z})^{k/2}.\n  </math>\n\nThe tail bound for the cases when <math>z > 1</math>, similarly, is\n: <math>\n    1-F(z k;\\,k) \\leq (z e^{1-z})^{k/2}.\n  </math>\n\nFor another [[approximation]] for the CDF modeled after the cube of a Gaussian, see [[Noncentral chi-squared distribution#Approximation|under Noncentral chi-squared distribution]].\n\n===Additivity===\nIt follows from the definition of the chi-squared distribution that the sum of independent chi-squared variables is also chi-squared distributed. Specifically, if {''X<sub>i</sub>''}<sub>''i''=1</sub><sup>''n''</sup> are independent chi-squared variables with {''k<sub>i</sub>''}<sub>''i''=1</sub><sup>''n''</sup> degrees of freedom, respectively, then {{nowrap|''Y {{=}} X''<sub>1</sub> + ⋯ + ''X<sub>n</sub>''}} is chi-squared distributed with {{nowrap|''k''<sub>1</sub> + ⋯ + ''k<sub>n</sub>''}} degrees of freedom.\n\n===Sample mean===\nThe sample mean of <math>n</math> [[Independent and identically distributed random variables|i.i.d.]] chi-squared variables of degree <math>k</math> is distributed according to a gamma distribution with shape <math>\\alpha</math> and scale <math>\\theta</math> parameters:\n:<math> \\bar X = \\frac{1}{n} \\sum_{i=1}^n X_i \\sim \\operatorname{Gamma}\\left(\\alpha=n\\, k /2, \\theta= 2/n \\right)  \\qquad \\text{where } X_i \\sim \\chi^2(k)</math>\n\n[[#Asymptotic properties|Asymptotically]], given that for a scale parameter <math> \\alpha </math> going to infinity, a Gamma distribution converges towards a normal distribution with expectation <math> \\mu = \\alpha\\cdot \\theta </math> and variance <math> \\sigma^2 = \\alpha\\, \\theta^2 </math>, the sample mean converges towards:\n\n:<math> \\bar X  \\xrightarrow{n \\to \\infty} N(\\mu = k, \\sigma^2 = 2\\, k /n ) </math>\n\nNote that we would have obtained the same result invoking instead the [[central limit theorem]], noting that for each chi-squared variable of degree <math> k </math> the expectation is <math> k </math> , and its variance <math> 2\\,k </math> (and hence the variance of the sample mean <math> \\bar X </math> being  <math> \\sigma^2 = 2\\,k/n  </math>).\n\n===Entropy===\nThe [[differential entropy]] is given by\n: <math>\n    h = \\int_{0}^\\infty f(x;\\,k)\\ln f(x;\\,k) \\, dx\n      = \\frac k 2 + \\ln \\left[2\\,\\Gamma \\left(\\frac k 2 \\right)\\right] + \\left(1-\\frac k 2 \\right)\\, \\psi\\!\\left[\\frac k 2 \\right],\n  </math>\nwhere ''ψ''(''x'') is the [[Digamma function]].\n\nThe chi-squared distribution is the [[maximum entropy probability distribution]] for a random variate ''X'' for which <math>\\operatorname{E}(X)=k</math> and <math>\\operatorname{E}(\\ln(X))=\\psi(k/2)+\\ln(2)</math> are fixed. Since the chi-squared is in the family of gamma distributions, this can be derived by substituting appropriate values in the [[gamma distribution#Logarithmic expectation|Expectation of the log moment of gamma]]. For derivation from more basic principles, see the derivation in [[exponential family#Moment-generating function of the sufficient statistic|moment-generating function of the sufficient statistic]].\n\n===Noncentral moments===\nThe moments about zero of a chi-squared distribution with ''k'' degrees of freedom are given by<ref>[http://mathworld.wolfram.com/Chi-SquaredDistribution.html Chi-squared distribution], from [[MathWorld]], retrieved Feb. 11, 2009</ref><ref>M. K. Simon, ''Probability Distributions Involving Gaussian Random Variables'', New York: Springer, 2002, eq. (2.35), {{ISBN|978-0-387-34657-1}}</ref>\n: <math>\n    \\operatorname{E}(X^m) = k (k+2) (k+4) \\cdots (k+2m-2) = 2^m \\frac{\\Gamma\\left(m+\\frac{k}{2}\\right)}{\\Gamma\\left(\\frac{k}{2}\\right)}.\n  </math>\n\n===Cumulants===\nThe [[cumulant]]s are readily obtained by a (formal) power series expansion of the logarithm of the characteristic function:\n: <math>\n    \\kappa_n = 2^{n-1}(n-1)!\\,k\n  </math>\n\n===Asymptotic properties===\nBy the [[central limit theorem]], because the chi-squared distribution is the sum of ''k'' independent random variables with finite mean and variance, it converges to a normal distribution for large ''k''. For many practical purposes, for ''k''&nbsp;>&nbsp;50 the distribution is sufficiently close to a [[normal distribution]] for the difference to be ignored.<ref>{{cite book|title=Statistics for experimenters|author=Box, Hunter and Hunter|publisher=Wiley|year=1978|isbn=978-0471093152|page=118}}</ref> Specifically, if ''X''&nbsp;~&nbsp;''χ''<sup>2</sup>(''k''), then as ''k'' tends to infinity, the distribution of <math>(X-k)/\\sqrt{2k}</math> [[convergence of random variables#Convergence in distribution|tends]] to a standard normal distribution. However, convergence is slow as the [[skewness]] is <math>\\sqrt{8/k}</math> and the [[excess kurtosis]] is&nbsp;12/''k''.\n\nThe sampling distribution of ln(''χ''<sup>2</sup>) converges to normality much faster than the sampling distribution of ''χ''<sup>2</sup>,<ref>{{cite journal |first=M. S. |last=Bartlett |first2=D. G. |last2=Kendall |title=The Statistical Analysis of Variance-Heterogeneity and the Logarithmic Transformation |journal=Supplement to the Journal of the Royal Statistical Society |volume=8 |issue=1 |year=1946 |pages=128–138 |jstor=2983618 |doi=10.2307/2983618 }}</ref> as the logarithm removes much of the asymmetry.<ref name=\":0\">{{Cite journal|last=Pillai|first=Natesh S.|year=2016|title=An unexpected encounter with Cauchy and Lévy|journal=[[Annals of Statistics]]|volume=44|issue=5|pages=2089–2097|doi=10.1214/15-aos1407|jstor=|arxiv=1505.01957}}</ref> Other functions of the chi-squared distribution converge more rapidly to a normal distribution. Some examples are:\n* If ''X'' ~ ''χ''<sup>2</sup>(''k'') then <math>\\scriptstyle\\sqrt{2X}</math> is approximately normally distributed with mean <math>\\scriptstyle\\sqrt{2k-1}</math> and unit variance (1922, by [[R. A. Fisher]], see (18.23), p.&nbsp;426 of.<ref name=\"Johnson_et_al\" />\n* If ''X'' ~ ''χ''<sup>2</sup>(''k'') then <math>\\scriptstyle\\sqrt[3]{X/k}</math> is approximately normally distributed with mean <math>\\scriptstyle 1-2/(9k)</math> and variance <math>\\scriptstyle 2/(9k) .</math><ref>{{cite journal |last=Wilson |first=E. B. |last2=Hilferty |first2=M. M. |year=1931 |title=The distribution of chi-squared |journal=[[Proceedings of the National Academy of Sciences of the United States of America|Proc. Natl. Acad. Sci. USA]] |volume=17 |issue=12 |pages=684–688 |bibcode=1931PNAS...17..684W |doi=10.1073/pnas.17.12.684 |pmid=16577411 |pmc=1076144 }}</ref> This is known as the Wilson–Hilferty transformation, see (18.24), p.&nbsp;426 of.<ref name=\"Johnson_et_al\" />\n\n==Relation to other distributions==\n{{Refimprove section|date=September 2011}}\n\n[[File:Chi on SAS.png|thumb|right|400px|Approximate formula for median compared with numerical quantile (top). Difference between numerical quantile and approximate formula (bottom).]]\n* As <math>k\\to\\infty</math>, <math> (\\chi^2_k-k)/\\sqrt{2k} ~ \\xrightarrow{d}\\ N(0,1) \\,</math> ([[normal distribution]])\n*<math> \\chi_k^2 \\sim  {\\chi'}^2_k(0)</math> ([[noncentral chi-squared distribution]] with non-centrality parameter <math> \\lambda = 0 </math>)\n*If <math>Y \\sim \\mathrm{F}(\\nu_1, \\nu_2)</math> then <math>X = \\lim_{\\nu_2 \\to \\infty} \\nu_1 Y</math> has the chi-squared distribution <math>\\chi^2_{\\nu_{1}}</math>\n:*As a special case, if <math>Y \\sim \\mathrm{F}(1, \\nu_2)\\,</math> then <math>X = \\lim_{\\nu_2 \\to \\infty} Y\\,</math> has the chi-squared distribution <math>\\chi^2_{1}</math>\n*<math> \\|\\boldsymbol{N}_{i=1,\\ldots,k} (0,1) \\|^2 \\sim \\chi^2_k </math> (The squared [[Norm (mathematics)|norm]] of ''k'' standard normally distributed variables is a chi-squared distribution with ''k'' [[degrees of freedom (statistics)|degrees of freedom]])\n*If <math>X \\sim \\chi^2(\\nu)\\,</math> and <math>c>0 \\,</math>, then <math>cX \\sim \\Gamma(k=\\nu/2, \\theta=2c)\\,</math>. ([[gamma distribution]])\n*If <math>X \\sim \\chi^2_k</math> then <math>\\sqrt{X} \\sim \\chi_k</math> ([[chi distribution]])\n*If <math>X \\sim \\chi^2(2)</math>, then <math>X \\sim \\operatorname{Exp}(1/2)</math> is an [[exponential distribution]].  (See [[gamma distribution]] for more.)\n*If <math>X \\sim \\chi^2(2k)</math>, then <math>X \\sim \\operatorname{Erlang}(k, 1/2)</math> is an [[Erlang distribution]].  \n*If <math> X \\sim \\operatorname{Erlang}(k,\\lambda)</math>, then <math> 2\\lambda X\\sim \\chi^2_{2k}</math>\n*If <math>X \\sim \\operatorname{Rayleigh}(1)\\,</math> ([[Rayleigh distribution]]) then <math>X^2 \\sim \\chi^2(2)\\,</math>\n*If <math>X \\sim \\operatorname{Maxwell}(1)\\,</math> ([[Maxwell distribution]])  then <math>X^2 \\sim \\chi^2(3)\\,</math>\n*If <math>X \\sim \\chi^2(\\nu)</math> then <math>\\tfrac{1}{X} \\sim \\operatorname{Inv-}\\chi^2(\\nu)\\, </math> ([[Inverse-chi-squared distribution]])\n*The chi-squared distribution is a special case of type 3 [[Pearson distribution]]\n* If <math>X \\sim \\chi^2(\\nu_1)\\,</math> and <math>Y \\sim \\chi^2(\\nu_2)\\,</math> are independent then <math>\\tfrac{X}{X+Y} \\sim \\operatorname{Beta}(\\tfrac{\\nu_1}{2}, \\tfrac{\\nu_2}{2})\\,</math> ([[beta distribution]])\n*If <math> X \\sim \\operatorname{U}(0,1)\\, </math> ([[Uniform distribution (continuous)|uniform distribution]]) then <math> -2\\log(X) \\sim \\chi^2(2)\\,</math>\n* <math>\\chi^2(6)\\,</math> is a transformation of [[Laplace distribution]]\n*If <math>X_i \\sim \\operatorname{Laplace}(\\mu,\\beta)\\,</math> then <math>\\sum_{i=1}^n \\frac{2 |X_i-\\mu|}{\\beta} \\sim \\chi^2(2n)\\,</math>\n* If <math>X_i</math> follows the [[generalized normal distribution]] (version 1) with parameters <math>\\mu,\\alpha,\\beta</math> then <math>\\sum_{i=1}^n \\frac{2 |X_i-\\mu|^\\beta}{\\alpha} \\sim \\chi^2\\left(\\frac{2n}{\\beta}\\right)\\,</math> <ref>{{cite journal |last= Bäckström |first= T. |authorlink= |author2=Fischer, J. |date=January 2018|title= Fast Randomization for Distributed Low-Bitrate Coding of Speech and Audio|journal= IEEE/ACM Transactions on Audio, Speech, and Language Processing |volume= 26|issue= 1|pages= 19&ndash;30|id= |doi=  10.1109/TASLP.2017.2757601|url= https://aaltodoc.aalto.fi/handle/123456789/33466 }}</ref> \n* chi-squared distribution is a transformation of [[Pareto distribution]]\n* [[Student's t-distribution]] is a transformation of chi-squared distribution\n* [[Student's t-distribution]] can be obtained from chi-squared distribution and [[normal distribution]]\n* [[Noncentral beta distribution]] can be obtained as a transformation of chi-squared distribution and [[Noncentral chi-squared distribution]]\n* [[Noncentral t-distribution]] can be obtained from normal distribution and chi-squared distribution\n\nA chi-squared variable with ''k'' degrees of freedom is defined as the sum of the squares of ''k'' independent [[standard normal distribution|standard normal]] random variables.\n\nIf ''Y'' is a ''k''-dimensional Gaussian random vector with mean vector ''μ'' and rank ''k'' covariance matrix ''C'', then ''X''&nbsp;=&nbsp;(''Y''−''μ'')<sup>T</sup>''C''<sup>−1</sup>(''Y''&nbsp;−&nbsp;''μ'') is chi-squared distributed with ''k'' degrees of freedom.\n\nThe sum of squares of [[statistically independent]] unit-variance Gaussian variables which do ''not'' have mean zero yields a generalization of the chi-squared distribution called the [[noncentral chi-squared distribution]].\n\nIf ''Y'' is a vector of ''k'' [[i.i.d.]] standard normal random variables and ''A'' is a ''k×k'' [[symmetric matrix|symmetric]], [[idempotent matrix]] with [[rank (linear algebra)|rank]] ''k−n'' then the [[quadratic form]] ''Y<sup>T</sup>AY'' is chi-squared distributed with ''k−n'' degrees of freedom.\n\nIf <math>\\Sigma</math> is a <math>p\\times p</math> positive-semidefinite covariance matrix with strictly positive diagonal entries, then for <math>X\\sim N(0,\\Sigma)</math> and <math>w</math> a random <math>p</math>-vector independent of <math>X</math> such that <math>w_1+\\cdots+w_p=1</math> and <math>w_i\\geq 0, i=1,\\cdots,p,</math> it holds that\n\n<math>\\frac{1}{\\left(\\frac{w_1}{X_1},\\cdots,\\frac{w_p}{X_p}\\right)\\Sigma\\left(\\frac{w_1}{X_1},\\cdots,\\frac{w_p}{X_p}\\right)^{\\top}}\\sim\\chi_1^2.</math><ref name=\":0\" />\n\nThe chi-squared distribution is also naturally related to other distributions arising from the Gaussian. In particular,\n\n* ''Y'' is [[F-distribution|F-distributed]], ''Y''&nbsp;~&nbsp;''F''(''k''<sub>1</sub>,''k''<sub>2</sub>) if <math>\\scriptstyle Y = \\frac{X_1 / k_1}{X_2 / k_2}</math> where ''X''<sub>1</sub>&nbsp;~&nbsp;''χ''²(''k''<sub>1</sub>) and ''X''<sub>2</sub> &nbsp;~&nbsp;''χ''²(''k''<sub>2</sub>) are statistically independent.\n* If {{nowrap|''X''<sub>1</sub> &nbsp;~&nbsp; ''χ''<sup>2</sup><sub>''k''<sub>1</sub></sub>}} and {{nowrap|''X''<sub>2</sub> &nbsp;~&nbsp; ''χ''<sup>2</sup><sub>''k''<sub>2</sub></sub>}} are statistically independent, then {{nowrap|''X''<sub>1</sub> + ''X''<sub>2</sub> &nbsp;~&nbsp;''χ''<sup>2</sup><sub>''k''<sub>1</sub>+''k''<sub>2</sub></sub>}}. If ''X''<sub>1</sub> and ''X''<sub>2</sub> are not independent, then {{nowrap|''X''<sub>1</sub> + ''X''<sub>2</sub>}} is not chi-squared distributed.\n\n==Generalizations==\nThe chi-squared distribution is obtained as the sum of the squares of ''k'' independent, zero-mean, unit-variance Gaussian random variables. Generalizations of this distribution can be obtained by summing the squares of other types of Gaussian random variables. Several such distributions are described below.\n\n===Linear combination===\nIf <math>X_1,\\ldots,X_n</math> are chi square random variables and <math>a_1,\\ldots,a_n\\in\\mathbb{R}_{>0}</math>, then a closed expression for the distribution of <math>X=\\sum_{i=1}^n a_i X_i</math> is not known. It may be, however, approximated efficiently using the [[Characteristic function (probability theory)#Properties|property of characteristic functions]] of chi-squared random variables.<ref>{{cite journal\n|first=J.\n|last=Bausch\n|title=On the Efficient Calculation of a Linear Combination of Chi-Square Random Variables with an Application in Counting String Vacua\n|journal=J. Phys. A: Math. Theor.\n|volume=46\n|issue=50\n|year=2013\n|pages=505202\n|doi=10.1088/1751-8113/46/50/505202 |bibcode=2013JPhA...46X5202B\n|arxiv=1208.2691\n}}</ref>\n\n===Chi-squared distributions===\n\n====Noncentral chi-squared distribution====\n{{Main|Noncentral chi-squared distribution}}\nThe noncentral chi-squared distribution is obtained from the sum of the squares of independent Gaussian random variables having unit variance and ''nonzero'' means.\n\n====Generalized chi-squared distribution====\n{{Main|Generalized chi-squared distribution}}\nThe generalized chi-squared distribution is obtained from the quadratic form ''z′Az'' where ''z'' is a zero-mean Gaussian vector having an arbitrary covariance matrix, and ''A'' is an arbitrary matrix.\n\n===Gamma, exponential, and related distributions===\nThe chi-squared distribution <math>X \\sim \\chi_k^2</math> is a special case of the [[gamma distribution]], in that <math>X \\sim \\Gamma \\left(\\frac{k}2,\\frac{1}2\\right)</math> using the rate parameterization of the gamma distribution (or\n<math>X \\sim \\Gamma \\left(\\frac{k}2,2 \\right)</math> using the scale parameterization of the gamma distribution)\nwhere ''k'' is an integer.\n\nBecause the [[exponential distribution]] is also a special case of the gamma distribution, we also have that if <math>X \\sim \\chi_2^2</math>, then <math>X\\sim \\operatorname{Exp}\\left(\\frac 1 2\\right)</math> is an [[exponential distribution]].\n\nThe [[Erlang distribution]] is also a special case of the gamma distribution and thus we also have that if <math>X \\sim\\chi_k^2</math> with even ''k'', then ''X'' is Erlang distributed with shape parameter ''k''/2 and scale parameter&nbsp;1/2.\n\n==Occurrence and applications{{anchor|Applications}}==\nThe chi-squared distribution has numerous applications in inferential [[statistics]], for instance in [[chi-squared test]]s and in estimating [[variance]]s. It enters the problem of estimating the mean of a normally distributed population and the problem of estimating the slope of a [[linear regression|regression]] line via its role in [[Student's t-distribution]]. It enters all [[analysis of variance]] problems via its role in the [[F-distribution]], which is the distribution of the ratio of two independent chi-squared [[random variable]]s, each divided by their respective degrees of freedom.\n\nFollowing are some of the most common situations in which the chi-squared distribution arises from a Gaussian-distributed sample.\n\n*if ''X''<sub>1</sub>, ..., ''X<sub>n</sub>'' are [[Independent and identically distributed random variables|i.i.d.]] ''N''(''μ'', ''σ''<sup>2</sup>) [[random variable]]s, then <math>\\sum_{i=1}^n(X_i - \\bar X)^2 \\sim \\sigma^2 \\chi^2_{n-1}</math> where <math>\\bar X = \\frac{1}{n} \\sum_{i=1}^n X_i</math>.\n*The box below shows some [[statistics]] based on {{nowrap|''X<sub>i</sub>'' ∼ Normal(''μ<sub>i</sub>'', ''σ''<sup>2</sup><sub>''i''</sub>), ''i'' {{=}} 1, ⋯, ''k'', }} independent random variables that have probability distributions related to the chi-squared distribution:\n<center>\n{| class=\"wikitable\" align=\"center\"\n|-\n! Name !! Statistic\n|-\n| chi-squared distribution || <math>\\sum_{i=1}^k \\left(\\frac{X_i-\\mu_i}{\\sigma_i}\\right)^2</math>\n|-\n| [[noncentral chi-squared distribution]] || <math>\\sum_{i=1}^k \\left(\\frac{X_i}{\\sigma_i}\\right)^2</math>\n|-\n| [[chi distribution]] || <math>\\sqrt{\\sum_{i=1}^k \\left(\\frac{X_i-\\mu_i}{\\sigma_i}\\right)^2}</math>\n|-\n| [[noncentral chi distribution]] || <math>\\sqrt{\\sum_{i=1}^k \\left(\\frac{X_i}{\\sigma_i}\\right)^2}</math>\n|}\n</center>\nThe chi-squared distribution is also often encountered in [[magnetic resonance imaging]].<ref>den Dekker A. J., Sijbers J., (2014) \"Data distributions in magnetic resonance images: a review\", ''Physica Medica'', [https://dx.doi.org/10.1016/j.ejmp.2014.05.002]</ref>\n\n==Table of ''χ''<sup>2</sup> values vs ''p''-values==\nThe [[p-value|''p''-value]] is the probability of observing a test statistic ''at least'' as extreme in a chi-squared distribution.  Accordingly, since the [[cumulative distribution function]] (CDF) for the appropriate degrees of freedom ''(df)'' gives the probability of having obtained a value ''less extreme'' than this point, subtracting the CDF value from 1 gives the ''p''-value.  A low ''p''-value, below the chosen significance level, indicates  [[statistical significance]], i.e., sufficient evidence to reject the null hypothesis. A significance level of 0.05 is often used as the cutoff between significant and not-significant results.\n\nThe table below gives a number of ''p''-values matching to ''χ''<sup>2</sup> for the first 10 degrees of freedom.\n{| class=\"wikitable\"\n|-\n! Degrees of freedom (df)\n!colspan=11| ''χ''<sup>2</sup> value<ref>[http://www2.lv.psu.edu/jxm57/irp/chisquar.html Chi-Squared Test] Table B.2. Dr. Jacqueline S. McLaughlin at The Pennsylvania State University. In turn citing: R. A. Fisher and F. Yates, Statistical Tables for Biological Agricultural and Medical Research, 6th ed., Table IV. Two values have been corrected, 7.82 with 7.81 and 4.60 with 4.61</ref>\n|-\n| style=\"text-align:center;\" | 1\n| 0.004\n| 0.02\n| 0.06\n| 0.15\n| 0.46\n| 1.07\n| 1.64\n| 2.71\n| 3.84\n| 6.63\n| 10.83\n|-\n| style=\"text-align:center;\" | 2\n| 0.10\n| 0.21\n| 0.45\n| 0.71\n| 1.39\n| 2.41\n| 3.22\n| 4.61\n| 5.99\n| 9.21\n| 13.82\n|-\n| style=\"text-align:center;\" | 3\n| 0.35\n| 0.58\n| 1.01\n| 1.42\n| 2.37\n| 3.66\n| 4.64\n| 6.25\n| 7.81\n| 11.34\n| 16.27\n|-\n| style=\"text-align:center;\" | 4\n| 0.71\n| 1.06\n| 1.65\n| 2.20\n| 3.36\n| 4.88\n| 5.99\n| 7.78\n| 9.49\n| 13.28\n| 18.47\n|-\n| style=\"text-align:center;\" | 5\n| 1.14\n| 1.61\n| 2.34\n| 3.00\n| 4.35\n| 6.06\n| 7.29\n| 9.24\n| 11.07\n| 15.09\n| 20.52\n|-\n| style=\"text-align:center;\" | 6\n| 1.63\n| 2.20\n| 3.07\n| 3.83\n| 5.35\n| 7.23\n| 8.56\n| 10.64\n| 12.59\n| 16.81\n| 22.46\n|-\n| style=\"text-align:center;\" | 7\n| 2.17\n| 2.83\n| 3.82\n| 4.67\n| 6.35\n| 8.38\n| 9.80\n| 12.02\n| 14.07\n| 18.48\n| 24.32\n|-\n| style=\"text-align:center;\" | 8\n| 2.73\n| 3.49\n| 4.59\n| 5.53\n| 7.34\n| 9.52\n| 11.03\n| 13.36\n| 15.51\n| 20.09\n| 26.12\n|-\n| style=\"text-align:center;\" | 9\n| 3.32\n| 4.17\n| 5.38\n| 6.39\n| 8.34\n| 10.66\n| 12.24\n| 14.68\n| 16.92\n| 21.67\n| 27.88\n|-\n| style=\"text-align:center;\" | 10\n| 3.94\n| 4.87\n| 6.18\n| 7.27\n| 9.34\n| 11.78\n| 13.44\n| 15.99\n| 18.31\n| 23.21\n| 29.59\n|-\n! scope=\"row\" style=\"text-align:right;\" | P value (Probability)\n| style=\"background: #ffa2aa\" | 0.95\n| style=\"background: #efaaaa\" | 0.90\n| style=\"background: #e8b2aa\" | 0.80\n| style=\"background: #dfbaaa\" | 0.70\n| style=\"background: #d8c2aa\" | 0.50\n| style=\"background: #cfcaaa\" | 0.30\n| style=\"background: #c8d2aa\" | 0.20\n| style=\"background: #bfdaaa\" | 0.10\n| style=\"background: #b8e2aa\" | 0.05\n| style=\"background: #afeaaa\" | 0.01\n| style=\"background: #a8faaa\" | 0.001\n|-\n|}\n\nThese values can be calculated evaluating the [[quantile function]] (also known as “inverse CDF” or “ICDF”) of the chi-squared distribution;<ref>[http://www.r-tutor.com/elementary-statistics/probability-distributions/chi-squared-distribution R Tutorial: Chi-squared Distribution]</ref> e. g., the {{math|''χ''<sup>2</sup>}} ICDF for {{math|1=''p'' = 0.05}} and {{math|1=df = 7}} yields {{math|14.06714 ≈ 14.07}} as in the table above.\n\n==History and name==\nThis distribution was first described by the German statistician [[Friedrich Robert Helmert]] in papers of 1875–6,{{sfn|Hald|1998|pp=633–692|loc=27. Sampling Distributions under Normality}}<ref>[[F. R. Helmert]], \"[http://gdz.sub.uni-goettingen.de/dms/load/img/?PPN=PPN599415665_0021&DMDID=DMDLOG_0018 Ueber die Wahrscheinlichkeit der Potenzsummen der Beobachtungsfehler und über einige damit im Zusammenhange stehende Fragen]\", ''Zeitschrift für Mathematik und Physik'' [http://gdz.sub.uni-goettingen.de/dms/load/toc/?PPN=PPN599415665_0021 21], 1876, pp. 102–219</ref> where he computed the sampling distribution of the sample variance of a normal population. Thus in German this was traditionally known as the ''Helmert'sche'' (\"Helmertian\") or \"Helmert distribution\".\n\nThe distribution was independently rediscovered by the English mathematician [[Karl Pearson]] in the context of [[goodness of fit]], for which he developed his [[Pearson's chi-squared test]], published in 1900, with computed table of values published in {{Harv|Elderton|1902}}, collected in {{Harv|Pearson|1914|pp=xxxi–xxxiii, 26–28|loc=Table XII}}.\nThe name \"chi-squared\" ultimately derives from Pearson's shorthand for the exponent in a [[multivariate normal distribution]] with the Greek letter [[Chi (letter)|Chi]], writing\n−½χ<sup>2</sup> for what would appear in modern notation as −½'''x'''<sup>T</sup>Σ<sup>−1</sup>'''x''' (Σ being the [[covariance matrix]]).<ref>\nR. L. Plackett, ''Karl Pearson and the Chi-Squared Test'', International Statistical Review, 1983,  [https://www.jstor.org/stable/1402731?seq=3 61f.]\nSee also Jeff Miller,  [http://jeff560.tripod.com/c.html Earliest Known Uses of Some of the Words of Mathematics].\n</ref> The idea of a family of \"chi-squared distributions\", however, is not due to Pearson but arose as a further development due to Fisher in the 1920s.{{sfn|Hald|1998|pp=633–692|loc=27. Sampling Distributions under Normality}}\n\n==See also==\n{{Portal|Statistics}}\n{{Colbegin}}\n* [[Chi distribution]]\n* [[Cochran's theorem]]\n* [[F-distribution|''F''-distribution]]\n* [[Fisher's method]] for combining [[Statistical independence|independent]] tests of significance\n* [[Gamma distribution]]\n* [[Generalized chi-squared distribution]]\n* [[Hotelling's T-squared distribution|Hotelling's ''T''-squared distribution]]\n* [[Noncentral chi-squared distribution]]\n* [[Pearson's chi-squared test]]\n* [[Reduced chi-squared statistic]]\n* [[Student's t-distribution|Student's ''t''-distribution]]\n* [[Wilks's lambda distribution]]\n* [[Wishart distribution]]\n{{Colend}}\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n{{refbegin}}\n* {{cite book |title=A history of mathematical statistics from 1750 to 1930 |last=Hald |first=Anders |authorlink=Anders Hald |year=1998 |publisher=Wiley |location=New York |isbn=978-0-471-17912-2 |ref=harv }}\n* {{Cite journal | last = Elderton | first = William Palin | authorlink = William Palin Elderton| title = Tables for Testing the Goodness of Fit of Theory to Observation | doi = 10.1093/biomet/1.2.155 | journal = Biometrika | volume = 1 | issue = 2 | pages = 155–163 | year = 1902 | pmid =  | pmc = |ref=harv| url = https://zenodo.org/record/1431595 }}\n* {{springer|title=Chi-squared distribution|id=Chi-squared_distribution}}\n{{refend}}\n\n==External links==\n*[http://jeff560.tripod.com/c.html Earliest Uses of Some of the Words of Mathematics: entry on Chi squared has a brief history]\n*[http://www.stat.yale.edu/Courses/1997-98/101/chigf.htm Course notes on Chi-Squared Goodness of Fit Testing] from Yale University Stats 101 class.\n*[http://demonstrations.wolfram.com/StatisticsAssociatedWithNormalSamples/ ''Mathematica'' demonstration showing the chi-squared sampling distribution of various statistics, e. g. Σ''x''², for a normal population]\n*[https://www.jstor.org/stable/2348373 Simple algorithm for approximating cdf and inverse cdf for the chi-squared distribution with a pocket calculator]\n* [https://www.medcalc.org/manual/chi-square-table.php Values of the Chi-squared distribution]\n\n{{ProbDistributions|continuous-semi-infinite}}\n\n{{DEFAULTSORT:Chi-Squared Distribution}}\n[[Category:Normal distribution]]\n[[Category:Infinitely divisible probability distributions]]"
    },
    {
      "title": "Elliptical distribution",
      "url": "https://en.wikipedia.org/wiki/Elliptical_distribution",
      "text": "In [[probability]] and [[statistics]], an '''elliptical distribution''' is any member of a broad family of [[probability distribution]]s that generalize the [[multivariate normal distribution]]. Intuitively, in the simplified two and three dimensional case, the joint distribution forms an ellipse and an ellipsoid, respectively, in iso-density plots.\n\nIn statistics, the normal distribution is used in ''classical'' [[multivariate analysis]], while elliptical distributions are used in ''generalized'' multivariate analysis, for the study of symmetric distributions with tails that are [[heavy tail|heavy]], like the [[multivariate t-distribution]], or light (in comparison with the normal distribution). Some statistical methods that were originally motivated by the study of the normal distribution have good performance for general elliptical distributions (with finite variance), particularly for spherical distributions (which are defined below). Elliptical distributions are also used in [[robust statistics]] to evaluate proposed multivariate-statistical procedures.\n\n==Definition==\n\nElliptical distributions are defined in terms of the [[characteristic function (probability theory)|characteristic function]] of probability theory. A random vector <math>X</math> on a [[Euclidean space]] has an ''elliptical distribution'' if its characteristic function <math>\\phi</math> satisfies the following [[functional equation]] (for every column-vector <math>t</math>) \n:<math>\\phi_{X-\\mu}(t)\n=\n\\psi(t' \\Sigma t)\n</math> \nfor some  [[location parameter]] <math>\\mu</math>, some [[nonnegative-definite matrix]] <math>\\Sigma</math> and some scalar function  <math>\\psi</math>.<ref name=\"chs\">{{harvtxt|Cambanis|Huang|Simons|1981|p=368}}</ref> The definition of elliptical distributions for ''real'' random-vectors has been extended to accommodate random vectors in Euclidean spaces over the [[complex number#field structure|field]] of [[complex number]]s, so facilitating applications in [[time-series analysis]].<ref>{{harvtxt|Fang|Kotz|Ng|1990|loc=Chapter 2.9 \"Complex elliptically symmetric distributions\", pp. 64-66}}</ref> Computational methods are available for generating [[pseudo-random number|pseudo-random]] vectors from elliptical distributions, for use in [[Monte Carlo method|Monte Carlo]] [[computer simulation|simulation]]s for example.<ref>{{harvtxt|Johnson|1987|loc=Chapter 6, \"Elliptically contoured distributions, pp. 106-124}}:  {{cite book|last=Johnson|first=Mark E.|title=Multivariate statistical simulation: A guide to selecting and generating continuous multivariate distributions|publisher=John Wiley and Sons|year=1987|ref=harv}}, \"an admirably lucid discussion\" according to {{harvtxt|Fang|Kotz|Ng|1990|p=27}}.</ref>\n\nSome elliptical distributions are alternatively defined in terms of their [[density function]]s.  An elliptical distribution with a density function ''f'' has the form:\n:<math>f(x)= k \\cdot g((x-\\mu)'\\Sigma^{-1}(x-\\mu))</math>\nwhere <math>k</math> is the [[normalizing constant]], <math>x</math> is an <math>n</math>-dimensional [[random vector]] with [[median#Multivariate median|median vector]] <math>\\mu</math> (which is also the mean vector if the latter exists), and  <math>\\Sigma</math> is a [[positive definite matrix]] which is proportional to the [[covariance matrix]] if the latter exists.<ref>Frahm, G., Junker, M., & Szimayer, A. (2003). Elliptical copulas: Applicability and limitations. ''Statistics & Probability Letters'', 63(3), 275–286.</ref>\n\n===Examples===\nExamples include the following multivariate probability distributions: \n* [[Multivariate normal distribution]]\n* [[Multivariate t-distribution|Multivariate ''t''-distribution]]\n* [[Multivariate stable distribution|Symmetric multivariate stable distribution]]<ref>{{cite web|title=Multivariate stable densities and distribution functions: general and elliptical case|author=Nolan, John|url=https://www.researchgate.net/publication/246910601_Multivariate_stable_densities_and_distribution_functions_general_and_elliptical_case|publisher=ResearchGate|accessdate=2017-05-26|date=September 29, 2014}}</ref>\n* [[Multivariate Laplace distribution|Symmetric multivariate Laplace distribution]]<ref>{{cite web|title=Parameter Estimation For Multivariate Generalized Gaussian Distributions|author=Pascal, F.|display-authors=etal|url=https://arxiv.org/pdf/1302.6498.pdf|accessdate=2017-05-26}}</ref>\n* [[Multivariate logistic distribution]]<ref name=schmidt>{{cite book|title=Credit Risk: Measurement, Evaluation and Management|page=274|author=Schmidt, Rafael|chapter=Credit Risk Modeling and Estimation via Elliptical Copulae|editor=Bol, George|display-editors=etal|year=2012|publisher=Springer|isbn=9783642593659}}</ref>\n* Multivariate symmetric general [[hyperbolic distribution]]<ref name=schmidt/>\n\n==Properties==\nIn the 2-dimensional case, if the density exists, each iso-density locus (the set of ''x''<sub>1</sub>,''x''<sub>2</sub> pairs all giving a particular value of <math>f(x)</math>) is an [[ellipse]] or a union of ellipses (hence the name elliptical distribution).  More generally, for arbitrary ''n'', the iso-density loci are unions of [[ellipsoid]]s. All these ellipsoids or ellipses have the common center &mu; and are scaled copies (homothets) of each other.\n\nThe [[multivariate normal distribution]] is the special case in which <math>g(z)=e^{-z/2}</math>.  While the multivariate normal is unbounded (each element of <math>x</math> can take on arbitrarily large positive or negative values with non-zero probability, because <math>e^{-z/2}>0</math> for all non-negative <math>z</math>), in general elliptical distributions can be bounded or unbounded—such a distribution is bounded if <math>g(z)=0</math> for all <math>z</math> greater than some value.\n\nThere exist elliptical distributions that have undefined [[mean]], such as the [[Cauchy distribution]] (even in the univariate case). Because the variable ''x'' enters the density function quadratically, all elliptical distributions are [[Symmetric distribution|symmetric]] about <math>\\mu.</math>\n\nIf two subsets of a jointly elliptical random vector are [[uncorrelated]], then if their means exist they are [[mean independent]] of each other (the mean of each subvector conditional on the value of the other subvector equals the unconditional mean).<ref name=\"Owen 1983\">{{harvtxt|Owen|Rabinovitch|1983}}</ref>{{rp|p. 748}}\n\nIf random vector ''X'' is elliptically distributed, then so is ''DX'' for any matrix ''D'' with full [[row rank]]. Thus any linear combination of the components of ''X'' is elliptical (though not necessarily with the same elliptical distribution), and any subset of ''X'' is elliptical.<ref name=\"Owen 1983\"/>{{rp|p. 748}}\n\n==Applications==\nElliptical distributions are used in statistics and in economics.\n\nIn mathematical economics, elliptical distributions have been used to describe [[portfolio theory|portfolio]]s in [[mathematical finance]].<ref>{{harv|Gupta|Varga|Bodnar|2013}}</ref><ref>(Chamberlain 1983; Owen and Rabinovitch 1983)</ref>\n\n===Statistics: Generalized multivariate analysis===\n{{anchor|Generalized multivariate analysis}}\n\nIn statistics, the [[multivariate normal distribution|multivariate ''normal'' distribution]] (of Gauss) is used in ''classical'' [[multivariate analysis]], in which most methods for estimation and hypothesis-testing are motivated for the normal distribution. In contrast to classical multivariate analysis, ''generalized'' multivariate analysis refers to research on elliptical distributions without the restriction of normality.\n\nFor suitable elliptical distributions, some classical methods  continue to have good  properties.<ref name=\"AndersonExtensions\">{{harvtxt|Anderson|2004|loc=The final section of the text (before \"Problems\") that are always entitled \"Elliptically contoured distributions\", of the following chapters: Chapters \n3 (\"Estimation of the mean vector and the covariance matrix\", Section 3.6, pp. 101-108), \n4 (\"The distributions and uses of sample correlation coefficients\", Section 4.5, pp. 158-163), \n5 (\"The generalized ''T<sup>2</sup>''-statistic\", Section 5.7, pp. 199-201), \n7 (\"The distribution of the sample covariance matrix and the sample generalized variance\", Section 7.9, pp. 242-248), \n8 (\"Testing the general linear hypothesis; multivariate analysis of variance\", Section 8.11, pp. 370-374), \n9 (\"Testing independence of sets of variates\", Section 9.11, pp. 404-408), \n10 (\"Testing hypotheses of equality of covariance matrices and equality of mean vectors and covariance vectors\", Section 10.11, pp. 449-454), \n11 (\"Principal components\", Section 11.8, pp. 482-483), \n13 (\"The distribution of characteristic roots and vectors\", Section 13.8, pp. 563-567)}}</ref><ref name=\"FangZhang\">{{harvtxt|Fang|Zhang|1990}}</ref> Under finite-variance assumptions, an extension of [[Cochran's theorem]] (on the distribution of quadratic forms) holds.<ref name=\"FangZhangCochran\">{{harvtxt|Fang|Zhang|1990|loc=Chapter 2.8 \"Distribution of quadratic forms and Cochran's theorem\", pp. 74-81}}</ref>\n\n====Spherical distribution====\n\nAn elliptical distribution with a zero mean and variance in the form <math>\\alpha I</math> where <math>I</math> is the identity-matrix is called a ''spherical distribution''.<ref name=\"FangZhangSpherical\">{{harvtxt|Fang|Zhang|1990|loc=Chapter 2.5 \"Spherical distributions\", pp. 53-64}}</ref>  For spherical distributions, classical results on parameter-estimation and hypothesis-testing hold have been extended.<ref name=\"FangZhangEstimation\">{{harvtxt|Fang|Zhang|1990|loc=Chapter IV \"Estimation of parameters\", pp. 127-153}}</ref><ref name=\"FangZhangTesting\">{{harvtxt|Fang|Zhang|1990|loc=Chapter V \"Testing hypotheses\", pp. 154-187}}</ref> Similar results hold for [[general linear model|linear models]],<ref name=\"FangZhangModels\">{{harvtxt|Fang|Zhang|1990|loc=Chapter VII \"Linear models\", pp. 188-211}}</ref> and indeed also for complicated models ( especially for the [[growth curve (statistics)|growth curve]] model). The analysis of multivariate models uses [[multilinear algebra]] (particularly [[Kronecker product]]s and [[vectorization (mathematics)|vectorization]]) and [[matrix calculus]].<ref name=\"FangZhang\" /><ref name=\"PanFang\">{{harvtxt|Pan|Fang|2007|p=ii}}</ref><ref name=\"KolloVonRosenXIII\">{{harvtxt|Kollo|von Rosen|2005|p=xiii}}</ref>\n\n====Robust statistics: Asymptotics====\n\nAnother use of elliptical distributions is in [[robust statistics]], in which researchers examine how statistical procedures perform on the class of elliptical distributions, to gain insight into the procedures' performance on even more general problems,<ref name=\"KariyaSinha\">{{cite book|last1=Kariya|first1=Takeaki|first2=Bimal K.|last2=Sinha|title=Robustness of statistical tests|publisher=Academic Press|year=1989|isbn=0123982308|ref=harv}}\n</ref> for example by using the [[asymptotic theory (statistics)|limiting theory of statistics]] (\"asymptotics\").<ref name=\"KolloVonRosen221\">{{harvtxt|Kollo|von Rosen|2005|p=221}}</ref>\n\n===Economics and finance===\n\nElliptical distributions are important in [[portfolio theory]] because, if the returns on all assets available for portfolio formation are jointly elliptically distributed, then all portfolios can be characterized completely by their location and scale &ndash; that is, any two portfolios with identical location and scale of portfolio return have identical distributions of portfolio return.<ref>{{harvtxt|Chamberlain|1983}}</ref><ref name=\"Owen 1983\"/> Various features of portfolio analysis, including [[mutual fund separation theorem]]s and the [[Capital Asset Pricing Model]], hold for all elliptical distributions.<ref name=\"Owen 1983\"/>{{rp|p. 748}}\n\n== Citations==\n{{Reflist}}\n{{refbegin}}\n\n==References==\n\n* {{cite book | last = Anderson | first = T. W. |authorlink=Theodore W. Anderson| title = An introduction to multivariate statistical analysis | publisher = John Wiley and Sons | location = New York | year = 2004 | edition = 3rd | isbn = 9789812530967|ref=harv }}\n* {{cite journal|first1=Stamatis|last1=Cambanis|first2=Steel|last2=Huang |first3=Gordon|last3=Simons \n| title=On the theory of elliptically contoured distributions\n|journal= Journal of Multivariate Analysis| volume=11| year=1981| pages=368–385| doi=10.1016/0047-259x(81)90082-8|ref=harv}}\n* Chamberlain, G. (1983). \"A characterization of the distributions that imply mean-variance utility functions\", ''Journal of Economic Theory'' 29, 185–201. {{doi|10.1016/0022-0531(83)90129-1}}\n* {{cite book|title=Generalized multivariate analysis \n|first1=Kai-Tai |last1=Fang |first2=Yao-Ting|last2=Zhang |publisher=Science Press (Beijing) and Springer-Verlag (Berlin)|year=1990|isbn=3540176519 \n|oclc=622932253 <!-- OCLC names \"K'ai-T'ai\" (sic.) --> |ref=harv}}\n* {{cite book |title=Symmetric multivariate and related distributions |last1=Fang|first1=Kai-Tai|authorlink1=Kai-Tai Fang|last2=Kotz|first2=Samuel|authorlink2=Samuel Kotz|last3=Ng|first3=Kai Wang (\"Kai-Wang\" on front cover)|year=1990|series=Monographs on statistics and applied probability|volume=36|publisher=Chapman and Hall|location=London|isbn=0 412 314 304|oclc=123206055|ref=harv}}\n* {{cite book|title=Elliptically contoured models in statistics and portfolio theory |first1=Arjun K.|last1=Gupta |first2=Tamas|last2=Varga |first3=Taras|last3=Bodnar \n|year=2013|publisher=Springer-Verlag |location=New York |doi=10.1007/978-1-4614-8154-6 |isbn=978-1-4614-8153-9|edition=2nd|ref=harv}}\n*:Originally {{cite book|title=Elliptically contoured models in statistics|first1=Arjun K.|last1=Gupta |first2=Tamas|last2=Varga \n|year=1993|publisher=Kluwer Academic Publishers |location=Dordrecht |doi= |isbn=0792326083|edition=1st|series=Mathematics and Its Applications|ref=harv}} \n* {{cite book|last1=Kollo|first1=Tõnu|last2=von Rosen|first2=Dietrich |title=Advanced multivariate statistics with matrices |location=Dordrecht |publisher=Springer |year=2005 |isbn=978-1-4020-3418-3 |ref=harv}}\n* Owen, J., and Rabinovitch, R. (1983). \"On the class of elliptical distributions and their applications to the theory of portfolio choice\", ''Journal of Finance'' 38, 745–752. {{jstor|2328079}}\n*{{cite book\n|last1=Pan\n|first1=Jianxin\n|last2=Fang\n|first2=Kaitai   <!-- In English, inconsistent with other publications --> \n|authorlink2=Kaitai Fang\n|title=Growth curve models and statistical diagnostics\n|publisher=Science Press (Beijing) and Springer-Verlag (New York)\n|series=Springer series in statistics\n|year=2007\n|oclc=44162563\n|doi=10.1007/978-0-387-21812-0\n|isbn=9780387950532|ref=harv\n}}\n\n==Further reading==\n* {{cite book|editor1-last=Fang|editor1-first=Kai-Tai|editorlink1=Kai-Tai Fang|editor2-last=Anderson|editor2-first=T. W.|editorlink2=Theodore W. Anderson|title=Statistical inference in elliptically contoured and related distributions|publisher=Allerton Press|location=New York|year=1990|isbn=0898640482|oclc=20490516|ref=harv}} A collection of papers.\n\n{{refend}}\n\n{{ProbDistributions|families|state=collapsed}}\n{{statistics|analysis|state=collapsed}}\n\n{{DEFAULTSORT:Elliptical Distribution}}\n[[Category:Types of probability distributions]]\n[[Category:Location-scale family probability distributions]]\n[[Category:Multivariate statistics]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Erdős–Kac theorem",
      "url": "https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93Kac_theorem",
      "text": "In [[number theory]], the '''Erdős–Kac theorem''', named after [[Paul Erdős]] and [[Mark Kac]], and also known as the fundamental theorem of [[probabilistic number theory]], states that if ω(''n'') is the number of distinct [[prime factor]]s of ''n'' {{OEIS|id=A001221}}, then, loosely speaking, the [[probability distribution]] of\n\n: <math> \\frac{\\omega(n) - \\log\\log n}{\\sqrt{\\log\\log n}} </math>\n\nis the standard [[normal distribution]]. This is an extension of the [[Hardy–Ramanujan theorem]], which states that the [[Normal order of an arithmetic function|normal order]] of ω(''n'') is log log ''n'' with a typical error of size <math>\\sqrt{\\log\\log n}</math>.\n\n==Precise statement==\nFor any fixed ''a''&nbsp;<&nbsp;''b'',\n\n:<math>\\lim_{x \\rightarrow \\infty}  \\left ( \\frac {1}{x} \\cdot \\#\\left\\{ n \\leq x : a \\le \\frac{\\omega(n) - \\log \\log n}{\\sqrt{\\log \\log n}} \\le b \\right\\} \\right ) = \\Phi(a,b) </math>\n\nwhere <math>\\Phi(a,b)</math> is the normal (or \"Gaussian\") distribution, defined as\n\n: <math>\\Phi(a,b)= \\frac{1}{\\sqrt{2\\pi}}\\int_a^b e^{-t^2/2} \\, dt. </math>\n\nMore generally, if f(''n'') is a strongly [[additive function]] (<math>\\scriptstyle f(p_1^{a_1}\\cdots p_k^{a_k})=f(p_1)+\\cdots+f(p_k)</math>) with <math>\\scriptstyle |f(p)|\\le1</math> for all prime ''p'', then\n:<math>\\lim_{x \\rightarrow \\infty}  \\left ( \\frac {1}{x} \\cdot \\#\\left\\{ n \\leq x : a \\le \\frac{f(n) - A(n)}{B(n)} \\le b \\right\\} \\right ) = \\Phi(a,b) </math>\nwith\n:<math>A(n)=\\sum_{p\\le n}\\frac{f(p)}{p},\\qquad B(n)=\\sqrt{\\sum_{p\\le n}\\frac{f(p)^2}{p}}.</math>\n\n==Kac's original heuristic==\n\nIntuitively, Kac's heuristic for the result says that if ''n'' is a randomly chosen large integer, then the number of distinct prime factors of ''n'' is approximately normally distributed with mean and variance&nbsp;log&nbsp;log&nbsp;''n''. This comes from the fact that given a random natural number ''n'', the events \"the number ''n'' is divisible by some prime ''p''\"  for each ''p'' are mutually independent.\n\nNow, denoting the event \"the number ''n'' is divisible by ''p''\" by <math>n_{p}</math>, consider the following sum of indicator random variables:\n\n:<math>I_{n_{2}} + I_{n_{3}} + I_{n_{5}} + I_{n_{7}} + \\ldots </math>\n\nThis sum counts how many distinct prime factors our random natural number ''n'' has. It can be shown that this sum satisfies the [[Lindeberg condition]], and therefore the [[Central Limit Theorem#Lindeberg CLT|Lindeberg central limit theorem]] guarantees that after appropriate rescaling, the above expression will be Gaussian.\n\nThe actual proof of the theorem, due to Erdős, uses [[sieve theory]] to make rigorous the above intuition.\n\n==Numerical examples==\nThe Erdős–Kac theorem means that the construction of a number around one billion requires on average three primes.\n\nFor example, 1,000,000,003 = 23&nbsp;×&nbsp;307&nbsp;×&nbsp;141623. The following table provides a numerical summary of the growth of the average number of distinct prime factors of a natural number <math>n</math> with increasing <math>n</math>.\n\n{| class=\"wikitable\" border=\"2\" style=\"text-align:center\"\n|-\n!  ''n''\n!Number of\ndigits in ''n''\n! Average number\nof distinct primes\n\n! Standard\ndeviation\n|-\n|1,000\n|4\n|2\n|1.4\n|-\n|1,000,000,000\n|10\n|3\n|1.7\n|-\n|1,000,000,000,000,000,000,000,000\n|25\n|4\n|2\n|-\n|10<sup>65</sup>\n|66\n|5\n|2.2\n|-\n|10<sup>9,566</sup>\n|9,567\n|10\n|3.2\n|-\n|10<sup>210,704,568</sup>\n|210,704,569\n|20\n|4.5\n|-\n|10<sup>10<sup>22</sup></sup>\n|10<sup>22</sup>+1\n|50\n|7.1\n|-\n|10<sup>10<sup>44</sup></sup>\n|10<sup>44</sup>+1\n|100\n|10\n|-\n|10<sup>10<sup>434</sup></sup>\n|10<sup>434</sup>+1\n|1000\n|31.6\n|}\n\n[[Image:EKT plot.svg|thumb|300px|right|A spreading Gaussian distribution of distinct primes illustrating the Erdos-Kac theorem]]\n\nAround 12.6% of 10,000 digit numbers are constructed from 10 distinct prime numbers and around 68% are constructed from between 7 and 13 primes.\n\nA hollow sphere the size of the planet Earth filled with fine sand would have around 10<sup>33</sup> grains. A volume the size of the observable universe would have around 10<sup>93</sup> grains of sand. There might be room for 10<sup>185</sup> quantum strings in such a universe.\n\nNumbers of this magnitude—with 186 digits—would require on average only 6 primes for construction.\n\nIt is very difficult if not impossible to discover this result empirically, as the Gaussian only shows up when <math>n</math> starts getting to be around <math>10^{100}</math>. More precisely, Rényi and Turán showed that the best possible uniform asymptotic bound on the error in the approximation to a Gaussian is <math>O\\left(\\frac{1}{\\sqrt{\\log \\log n}}\\right)</math>.<ref>{{cite journal|last1=Rényi|first1=A.|last2=Turán|first2=P.|title=On a theorem of Erdös-Kac|journal=Acta Arithmetica|volume=4|number=1|year=1958|pages=71–84|url=http://matwbn.icm.edu.pl/ksiazki/aa/aa4/aa417.pdf}}</ref>\n\n==References==\n{{Reflist}}\n* {{cite journal | last1=Erdős | first1=Paul | author1-link=Paul Erdős | last2=Kac | first2=Mark | author2-link=Mark Kac | title=The Gaussian Law of Errors in the Theory of Additive Number Theoretic Functions | journal=[[American Journal of Mathematics]] | volume=62 | number=1/4 | year=1940 | pages=738–742 | zbl=0024.10203 | issn=0002-9327 | doi=10.2307/2371483}}\n* {{cite book | last1=Kuo | first1=Wentang | last2=Liu | first2=Yu-Ru | chapter=The Erdős–Kac theorem and its generalizations | pages=209–216 | editor1-last=De Koninck | editor1-first=Jean-Marie | editor2-last=Granville | editor2-first=Andrew | editor2-link=Andrew Granville | editor3-last=Luca | editor3-first=Florian | title=Anatomy of integers. Based on the CRM workshop, Montreal, Canada, March 13--17, 2006 | location=Providence, RI | publisher=[[American Mathematical Society]] | series=CRM Proceedings and Lecture Notes | volume=46 | year=2008 | isbn=978-0-8218-4406-9 | zbl=1187.11024 }}\n* {{cite book | last=Kac | first=Mark | title=Statistical Independence in Probability, Analysis and Number Theory |year=1959 | publisher=John Wiley and Sons, Inc. }}\n\n==External links==\n* {{MathWorld|urlname=Erdos-KacTheorem|title=Erdős–Kac Theorem}}\n* [https://www.youtube.com/watch?v=4ivoaFLQ4vM#generator Timothy Gowers: The Importance of Mathematics (part 6, 4 mins in) and (part 7)]\n\n{{DEFAULTSORT:Erdos-Kac theorem}}\n[[Category:Paul Erdős|Kac theorem]]\n[[Category:Normal distribution]]\n[[Category:Theorems about prime numbers]]"
    },
    {
      "title": "Fieller's theorem",
      "url": "https://en.wikipedia.org/wiki/Fieller%27s_theorem",
      "text": "In [[statistics]], '''Fieller's theorem''' allows the calculation of a [[confidence interval]] for the ratio of two [[arithmetic mean|means]].\n\n==Approximate confidence interval==\nVariables ''a'' and ''b'' may be measured in different units, so there is no way to directly combine the [[standard error]]s as they may also be in different units. The most complete discussion of this is given by Fieller (1954).<ref>{{cite journal |author=Fieller, EC. |title=Some problems in interval estimation. |journal=[[Journal of the Royal Statistical Society, Series B]] |volume=16 |issue= 2|pages=175–185 |year=1954 |jstor=2984043 }}</ref>\n\nFieller showed that if ''a'' and ''b'' are (possibly [[correlation|correlated]]) [[sample mean|means of two samples]] with [[expected value|expectations]] <math>\\mu_a</math> and <math>\\mu_b</math>, and variances <math>\\nu_{11}\\sigma^2</math> and <math>\\nu_{22}\\sigma^2</math> and covariance <math>\\nu_{12}\\sigma^2</math>, and if <math>\\nu_{11}, \\nu_{12}, \\nu_{22} </math> are all known, then a (1&nbsp;&minus;&nbsp;''α'') confidence interval (''m''<sub>L</sub>,&nbsp;''m''<sub>U</sub>) for <math>\\mu_a/\\mu_b</math> is given by\n\n: <math>(m_L, m_{U}) = \\frac{1}{(1-g)} \\left[\\frac{a}{b} - \\frac{g\\nu_{12}}{\\nu_{22}} \\mp \\frac{t_{r,\\alpha}s}{b} \\sqrt{\\nu_{11} - 2\\frac{a}{b}\\nu_{12} + \\frac{a^2}{b^2} \\nu_{22} - g\\left(\\nu_{11} - \\frac{\\nu_{12}^2}{\\nu_{22}}\\right)} \\right]</math>\n\nwhere\n:<math>g=\\frac{t^{2}_{r,\\alpha}s^2\\nu_{22}}{b^2}.</math>\nHere <math>s^2</math> is an [[bias of an estimator|unbiased estimator]] of <math>\\sigma^2</math> based on r degrees of freedom, and <math>t_{r,\\alpha}</math> is the <math>\\alpha</math>-level deviate from the [[Student's t-distribution]] based on ''r'' degrees of freedom.\n\nThree features of this formula are important in this context:\n\na) The expression inside the square root has to be positive, or else the resulting interval will be imaginary.\n\nb) When ''g'' is very close to 1, the confidence interval is infinite.\n\nc) When ''g'' is greater than 1, the overall divisor outside the square brackets is negative and the confidence interval is exclusive.\n\n== Other methods ==\n\nOne problem is that, when ''g'' is not small, the confidence interval can blow up when using Fieller's theorem. Andy Grieve has provided a Bayesian solution where the CIs are still sensible, albeit wide.<ref>{{cite journal |vauthors=O'Hagan A, Stevens JW, Montmartin J | title=Inference for the cost-effectiveness acceptability curve and cost-effectiveness ratio. |journal=[[PharmacoEconomics (journal)|Pharmacoeconomics]] |volume=17|issue=4 |pages=339–49 |year=2000 |doi=10.2165/00019053-200017040-00004 |pmid=10947489}}</ref> [[Bootstrapping (statistics)|Bootstrapping]] provides another alternative that does not require the assumption of normality.<ref>{{cite journal|last=Campbell|first=M. K.|author2=Torgerson, D. J.|journal=[[QJM: An International Journal of Medicine]]|year=1999|volume=92|issue=3|pages=177–182|doi=10.1093/qjmed/92.3.177|title=Bootstrapping: estimating confidence intervals for cost-effectiveness ratios}}</ref>\n\n== History ==\nEdgar C. Fieller (1907&ndash;1960) first started working on this problem while in [[Karl Pearson]]'s group at [[University College London]], where he was employed for five years after graduating in Mathematics from [[King's College, Cambridge]]. He then worked for the [[Boots UK|Boots Pure Drug Company]] as a statistician and [[operational research]]er before becoming deputy head of operational research at [[RAF Fighter Command]] during the [[Second World War]], after which he was appointed the first head of the Statistics Section at the [[National Physical Laboratory (United Kingdom)|National Physical Laboratory]].<ref>{{cite journal |author1=Irwin, J. O.  |author2=Rest, E. D. Van | title=Edgar Charles Fieller, 1907-1960 |journal=Journal of the Royal Statistical Society, Series A |volume=124 |issue=2 |pages=275–277 |year=1961 | jstor=2984155 |publisher=Blackwell Publishing}}</ref>\n\n==See also==\n*[[Ratio distribution#Gaussian ratio distribution|Gaussian ratio distribution]]\n\n== Notes ==\n{{reflist}}\n\n== Further reading ==\n* Iris Pigeot, Juliane Schafer, Joachim Rohmel and Dieter Hauschke (2003) \"Assessing non-inferiority of a new treatment in a three-arm clinical trial including a placebo\". ''[[Statistics in Medicine (journal)|Statistics in Medicine]]'', 22:883–899, {{doi| 10.1002/sim.1450}}\n* Fieller, EC. (1932) \"The distribution of the index in a bivariate Normal distribution\". ''[[Biometrika]]'', 24(3&ndash;4):428&ndash;440. {{doi| 10.1093/biomet/24.3-4.428}}\n* Fieller, EC. (1940) \"The biological standardisation of insulin\". ''[[Journal of the Royal Statistical Society]] (Supplement)''. 1:1&ndash;54. {{jstor|2983630}}\n* Fieller, EC. (1944) \"A fundamental formula in the statistics of biological assay, and some applications\". ''Quarterly Journal of Pharmacy and Pharmacology''. 17: 117-123.\n* Motulsky, Harvey (1995) ''Intuitive Biostatistics''. Oxford University Press. {{ISBN|0-19-508607-4}}\n* Senn, Steven (2007) ''Statistical Issues in Drug Development''. Second Edition. Wiley. {{ISBN|0-471-97488-9}}\n*{{Cite journal| volume = 64 | pages = 234–241 | year = 2010 | doi = 10.1198/tast.2010.08130 | last1 = Hirschberg | journal = [[The American Statistician]] | first1 = J.| title = A Geometric Comparison of the Delta and Fieller Confidence Intervals| last2 = Lye |first2 = J. | issue = 3}}\n\n{{DEFAULTSORT:Fieller's Theorem}}\n[[Category:Statistical theorems]]\n[[Category:Statistical approximations]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Folded normal distribution",
      "url": "https://en.wikipedia.org/wiki/Folded_normal_distribution",
      "text": "{{Probability distribution\n  | name       =\n  | type       = density\n  | pdf_image  = [[Image:Folded normal pdf.svg|350px|Probability density function for the folded-normal distribution]]<br>{{math|''μ''{{=}}1, ''σ''{{=}}1}}\n  | cdf_image  = [[Image:Folded normal cdf.svg|350px|Cumulative distribution function for the normal distribution]]<br>{{math|''μ''{{=}}1, ''σ''{{=}}1}}\n  | notation   = \n  | parameters = {{math|''μ'' ∈ '''R'''}}&nbsp;&nbsp;&nbsp;([[location parameter|location]])<br />{{math|''σ''<sup>2</sup> > 0}}&nbsp;&nbsp;&nbsp;([[scale parameter|scale]])\n  | support    = {{math|''x'' ∈ [0,∞)}}\n  | pdf        = <math>\\frac{1}{\\sigma\\sqrt{2\\pi}} \\, e^{ -\\frac{(x-\\mu)^2}{2\\sigma^2} }\n+ \\frac{1}{\\sigma\\sqrt{2\\pi}} \\, e^{ -\\frac{(x+\\mu)^2}{2\\sigma^2} }</math>\n  | cdf        = <math>\\frac{1}{2}\\left[ \\mbox{erf}\\left(\\frac{x+\\mu}{\\sigma\\sqrt{2}}\\right) + \\mbox{erf}\\left(\\frac{x-\\mu}{\\sigma\\sqrt{2}}\\right)\\right]</math>\n  | mean       = <math>\\mu_Y = \\sigma \\sqrt{\\tfrac{2}{\\pi}} \\, e^{(-\\mu^2/2\\sigma^2)} + \\mu \\left(1 - 2\\,\\Phi(\\tfrac{-\\mu}{\\sigma}) \\right) </math>\n  | median     = \n  | mode       = \n  | variance   = <math>\\sigma_Y^2 = \\mu^2 + \\sigma^2 - \\mu_Y^2 </math>\n  | skewness   = \n  | kurtosis   = <!-- DO NOT REPLACE THIS WITH THE OLD-STYLE KURTOSIS -->\n  | entropy    = \n  | mgf        = \n  | char       = \n  | fisher     = \n  }}\n\nThe '''folded normal distribution''' is a [[probability distribution]] related to the [[normal distribution]]. Given a normally distributed random variable ''X'' with [[mean]] ''μ'' and [[variance]] ''σ''<sup>2</sup>, the [[random variable]] ''Y'' = |''X''| has a folded normal distribution.  Such a case may be encountered if only the magnitude of some variable is recorded, but not its sign. The distribution is called \"folded\" because probability mass to the left of the ''x'' = 0 is folded over by taking the [[absolute value]]. In the physics of [[heat conduction]], the folded normal distribution is a fundamental solution of the [[heat equation]] on the upper plane (i.e. a [[heat kernel]]).\n\nThe [[probability density function]] (PDF) is given by\n\n:<math>f_Y(x;\\mu,\\sigma^2)=\n\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\, e^{ -\\frac{(x-\\mu)^2}{2\\sigma^2} }\n+ \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\, e^{ -\\frac{(x+\\mu)^2}{2\\sigma^2} } </math>\n\nfor ''x''≥0, and 0 everywhere else. An alternative formulation is given by\n\n<math>f\\left(x \\right)=\\sqrt{\\frac{2}{\\pi\\sigma^2}}e^{-\\frac{\\left(x^2+\\mu^2 \\right)}{2\\sigma^2}}\\cosh{\\left(\\frac{\\mu x}{\\sigma^2}\\right)}</math>,\n\nwhere cosh is the cosine [[Hyperbolic function]]. It follows that the [[cumulative distribution function]] (CDF) is given by:\n\n: <math>\nF_Y(x; \\mu, \\sigma^2) = \\frac{1}{2}\\left[ \\mbox{erf}\\left(\\frac{x+\\mu}{\\sqrt{2\\sigma^2}}\\right) + \\mbox{erf}\\left(\\frac{x-\\mu}{\\sqrt{2\\sigma^2}}\\right)\\right] </math>\n\nfor ''x''≥0, where erf() is the [[error function]]. This expression reduces to the CDF of the [[half-normal distribution]] when ''μ'' = 0.\n\nThe mean of the folded distribution is then\n\n: <math>\\mu_Y = \\sigma \\sqrt{\\frac{2}{\\pi}} \\,\\, \\exp\\left(\\frac{-\\mu^2}{2\\sigma^2}\\right) + \\mu \\, \\mbox{erf}\\left(\\frac{\\mu}{\\sqrt{2\\sigma^2}}\\right)</math>\n\nor\n\n: <math> \\mu_Y = \\sqrt{\\frac{2}{\\pi}}\\sigma e^{-\\frac{\\mu^2}{2\\sigma^2}}+\\mu\\left[1-2\\Phi\\left(-\\frac{\\mu}{\\sigma}\\right) \\right]</math>\n\nwhere <math>\\Phi</math> is the [[normal cumulative distribution function]]:\n\n:<math> \\Phi(x)\\; =\\; \\frac12\\left[1 + \\operatorname{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right].</math>\n\nThe variance then is expressed easily in terms of the mean:\n\n: <math>\\sigma_Y^2 = \\mu^2 + \\sigma^2 - \\mu_Y^2. </math>\n\nBoth the mean (''μ'') and variance (''σ''<sup>2</sup>) of ''X'' in the original normal distribution can be interpreted as the location and scale parameters of ''Y'' in the folded distribution.\n\n== Mode of the distribution ==\nThe mode of the distribution is the value of <math>x</math> for which the density is maximised. In order to find this value, we take the first derivative of the density with respect to <math>x</math> and set it equal to zero. Unfortunately, there is no closed form. We can, however, write the derivative in a better way and end up with a non-linear equation\n\n<math>\\frac{df(x)}{dx}=0 \\Rightarrow  -\\frac{\\left(x-\\mu\\right)}{\\sigma^2}e^{-\\frac{1}{2}\\frac{\\left(x-\\mu\\right)^2}{\\sigma^2}}-\n\\frac{\\left(x+\\mu\\right)}{\\sigma^2}e^{-\\frac{1}{2}\\frac{\\left(x+\\mu\\right)^2}{\\sigma^2}}=0\n\n</math>\n\n<math> x\\left[e^{-\\frac{1}{2}\\frac{\\left(x-\\mu\\right)^2}{\\sigma^2}}+e^{-\\frac{1}{2}\\frac{\\left(x+\\mu\\right)^2}{\\sigma^2}}\\right]-\n\\mu \\left[e^{-\\frac{1}{2}\\frac{\\left(x-\\mu\\right)^2}{\\sigma^2}}-e^{-\\frac{1}{2}\\frac{\\left(x+\\mu\\right)^2}{\\sigma^2}}\\right]=0 </math>\n\n<math>x\\left(1+e^{-\\frac{2\\mu x}{\\sigma^2}}\\right)-\\mu\\left(1-e^{-\\frac{2\\mu x}{\\sigma^2}}\\right)=0 </math>\n\n<math>\\left(\\mu+x\\right)e^{-\\frac{2\\mu x}{\\sigma^2}}=\\mu-x </math>\n\n<math>x=-\\frac{\\sigma^2}{2\\mu}\\log{\\frac{\\mu-x}{\\mu+x}} </math>.\n\nTsagris et al. (2014) saw from numerical investigation that when <math>\\mu<\\sigma\n\n</math>, the maximum is met when <math>x=0\n\n</math>, and when <math>\\mu\n\n</math> becomes greater than <math>3\\sigma\n\n</math>, the maximum approaches <math>\\mu\n\n</math>. This is of course something to be expected, since, in this case, the folded normal converges to the normal distribution. In order to avoid any trouble with negative variances, the exponentiation of the parameter is suggested. Alternatively, you can add a constraint, such as if the optimiser goes for a negative variance the value of the log-likelihood is NA or something very small. <syntaxhighlight lang=\"r\">\n\n</syntaxhighlight>\n\n== Characteristic function and other related functions ==\n* The characteristic function is given by\n\n<math>\\varphi_x\\left(t\\right)=e^{\\frac{-\\sigma^2 t^2}{2}+i\\mu t}\\left[1-\\Phi\\left(-\\frac{\\mu}{\\sigma}+i\\sigma t \\right) \\right]+\ne^{-\\frac{\\sigma^2 t^2}{2}-i\\mu t}\\left[1-\\Phi\\left(\\frac{\\mu}{\\sigma}+i\\sigma t \\right) \\right]</math>.\n* The moment generating function is given by\n\n<math>M_x\\left(t\\right)=\\varphi_x\\left(-it\\right)=e^{\\frac{\\sigma^2 t^2}{2}+\\mu t}\\left[1-\\Phi\\left(-\\frac{\\mu}{\\sigma}-\\sigma t \\right) \\right]+\ne^{\\frac{\\sigma^2 t^2}{2}-\\mu t}\\left[1-\\Phi\\left(\\frac{\\mu}{\\sigma}-\\sigma t \\right) \\right]</math>.\n* The cumulant generating function is given by\n<math>K_x\\left(t\\right)=\\log{M_x\\left(t\\right)}=\n\\left(\\frac{\\sigma^2t^2}{2}+\\mu t\\right) + \\log{\\left\\lbrace 1-\\Phi\\left(-\\frac{\\mu}{\\sigma}-\\sigma t \\right) +\ne^{\\frac{\\sigma^2 t^2}{2}-\\mu t}\\left[1-\\Phi\\left(\\frac{\\mu}{\\sigma}-\\sigma t \\right) \\right] \\right\\rbrace}</math>.\n* The Laplace transformation is given by\n\n<math>E\\left(e^{-tx}\\right)=e^{\\frac{\\sigma^2t^2}{2}-\\mu t}\\left[1-\\Phi\\left(-\\frac{\\mu}{\\sigma}+\\sigma t \\right) \\right]+\ne^{\\frac{\\sigma^2 t^2}{2}+\\mu t}\\left[1-\\Phi\\left(\\frac{\\mu}{\\sigma}+\\sigma t \\right) \\right]</math>.\n* The Fourier transform is given by\n<math>\\hat{f}\\left(t\\right)=\\phi_x\\left(-2\\pi t\\right)=  e^{\\frac{-4\\pi^2\\sigma^2 t^2}{2}- i2\\pi \\mu t}\\left[1-\\Phi\\left(-\\frac{\\mu}{\\sigma}-i2\\pi \\sigma t \\right) \\right]+ e^{-\\frac{4\\pi^2 \\sigma^2 t^2}{2}+i2\\pi\\mu t}\\left[1-\\Phi\\left(\\frac{\\mu}{\\sigma}-i2\\pi \\sigma t \\right) \\right]\n</math>.\n\n== Parameter estimation ==\nThere are a few ways of estimating the parameters of the folded normal. All of them are essentially the maximum likelihood estimation procedure, but in the some cases, a numerical maximization is performed, whereas in other cases, the root of an equation is being searched. The log-likelihood of the folded normal when a sample <math>x_i</math> of size <math>n</math> is available can be written in the following way\n\n<math>l = -\\frac{n}{2}\\log{2\\pi\\sigma^2}+\\sum_{i=1}^n\\log{\\left[e^{-\\frac{\\left(x_i-\\mu\\right)^2}{2\\sigma^2}}+\ne^{-\\frac{\\left(x_i+\\mu\\right)^2}{2\\sigma^2}} \\right] } </math>\n\n<math>l = -\\frac{n}{2}\\log{2\\pi\\sigma^2}+\\sum_{i=1}^n\\log{\\left[e^{-\\frac{\\left(x_i-\\mu\\right)^2}{2\\sigma^2}}\n\\left(1+e^{-\\frac{\\left(x_i+\\mu\\right)^2}{2\\sigma^2}}e^{\\frac{\\left(x_i-\\mu\\right)^2}{2\\sigma^2}}\\right)\\right]}</math>\n\n<math>l = -\\frac{n}{2}\\log{2\\pi\\sigma^2}-\\sum_{i=1}^n\\frac{\\left(x_i-\\mu\\right)^2}{2\\sigma^2}+\\sum_{i=1}^n\\log{\\left(1+e^{-\\frac{2\\mu x_i}{\\sigma^2}} \\right)}</math>\n\nIn [[R (programming language)]], using the package '''Rfast''' one can obtain the MLE really fast (command <code>foldnorm.mle</code>). Alternatively, the command [[b:R Programming/Optimization|optim]] or [[b:R Programming/Optimization|nlm]] will fit this distribution. The maximisation is easy, since two parameters (<math>\\mu</math> and <math>\\sigma^2</math>) are involved. Note, that both positive and negative values for <math>\\mu</math> are acceptable, since <math>\\mu</math> belongs to the real line of numbers, hence, the sign is not important because the distribution is symmetric with respect to it. The next code is written in R<syntaxhighlight lang=\"r\">\nfolded <- function(y) {\n\n  ## y is a vector with positive data\n  n <- length(y)  ## sample size\n  sy2 <- sum(y^2)\n\n    sam <- function(para, n, sy2) {\n      me <- para[1]   ;   se <- exp( para[2] )\n      f <-  - n/2 * log(2/pi/se) + n * me^2 / 2 / se +\n            sy2 / 2 / se - sum( log( cosh( me * y/se ) ) )\n      f\n    }\n\n  mod <- optim( c( mean(y), sd(y) ), n = n, sy2 = sy2, sam, control = list(maxit = 2000) )\n  mod <- optim( mod$par, sam, n = n, sy2 = sy2, control = list(maxit = 20000) )\n  result <- c( -mod$value, mod$par[1], exp(mod$par[2]) )\n  names(result) <- c(\"log-likelihood\", \"mu\", \"sigma squared\")\n  result\n\n}\n</syntaxhighlight>The partial derivatives of the log-likelihood are written as\n\n<math>\\frac{\\partial l}{\\partial \\mu} = \\frac{\\sum_{i=1}^n\\left(x_i-\\mu \\right)}{\\sigma^2}-\n\\frac{2}{\\sigma^2}\\sum_{i=1}^n\\frac{x_ie^{\\frac{-2\\mu x_i}{\\sigma^2}}}{1+e^{\\frac{-2\\mu x_i}{\\sigma^2}}}\n</math>\n\n<math>\\frac{\\partial l}{\\partial \\mu} = \\frac{\\sum_{i=1}^n\\left(x_i-\\mu \\right)}{\\sigma^2}-\\frac{2}{\\sigma^2}\\sum_{i=1}^n\\frac{x_i}{1+e^{\\frac{2\\mu x_i}{\\sigma^2}}} \\ \\ \\text{and}</math>\n\n<math>\\frac{\\partial l}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2}+\\frac{\\sum_{i=1}^n\\left(x_i-\\mu \\right)^2}{2\\sigma^4}+\n\\frac{2\\mu}{\\sigma^4}\\sum_{i=1}^n\\frac{x_ie^{-\\frac{2\\mu x_i}{\\sigma^2}}}{1+e^{-\\frac{2\\mu x_i}{\\sigma^2}}} </math>\n\n<math>\\frac{\\partial l}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2}+\\frac{\\sum_{i=1}^n\\left(x_i-\\mu \\right)^2}{2\\sigma^4}+\n\\frac{2\\mu}{\\sigma^4}\\sum_{i=1}^n\\frac{x_i}{1+e^{\\frac{2\\mu x_i}{\\sigma^2}}}</math>.\n\nBy equating the first partial derivative of the log-likelihood to zero, we obtain a nice relationship\n\n<math>\n\n\\sum_{i=1}^n\\frac{x_i}{1+e^{\\frac{2\\mu x_i}{\\sigma^2}}}=\\frac{\\sum_{i=1}^n\\left(x_i-\\mu \\right)}{2}\n\n</math>.\n\nNote that the above equation has three solutions, one at zero and two more with the opposite sign. By substituting the above equation, to the partial derivative of the log-likelihood w.r.t <math> \\sigma^2</math> and equating it to zero, we get the following expression for the variance\n\n<math>\\sigma^2=\\frac{\\sum_{i=1}^n\\left(x_i-\\mu\\right)^2}{n}+\\frac{2\\mu\\sum_{i=1}^n\\left(x_i-\\mu\\right)}{n}=\\frac{\\sum_{i=1}^n\\left(x_i^2-\\mu^2\\right)}{n}=\\frac{\\sum_{i=1}^nx_i^2}{n}-\\mu^2</math>,\n\nwhich is the same formula as in the [[normal distribution]]. A main difference here is that <math>\\mu</math> and <math>\\sigma^2</math> are not statistically independent. The above relationships can be used to obtain maximum likelihood estimates in an efficient recursive way. We start with an initial value for <math>\\sigma^2</math> and find the positive root (<math>\\mu</math>) of the last equation. Then, we get an updated value of <math>\\sigma^2</math>. The procedure is being repeated until the change in the log-likelihood value is negligible. Another easier and more efficient way is to perform a search algorithm. Let us write the last equation in a more elegant way\n\n<math>2\\sum_{i=1}^n\\frac{x_i}{1+e^{\\frac{2\\mu x_i}{\\sigma^2}}}-\n\\sum_{i=1}^n\\frac{x_i\\left(1+e^{\\frac{2\\mu x_i}{\\sigma^2}}\\right)}{1+e^{\\frac{2\\mu x_i}{\\sigma^2}}}+n\\mu = 0</math>\n\n<math>\\sum_{i=1}^n\\frac{x_i\\left(1-e^{\\frac{2\\mu x_i}{\\sigma^2}}\\right)}{1+e^{\\frac{2\\mu x_i}{\\sigma^2}}}+n\\mu = 0\n</math>.\n\nIt becomes clear that the optimization the log-likelihood with respect to the two parameters has turned into a root search of a function. This of course is identical to the previous root search. Tsagris et al. (2014) spotted that there are three roots to this equation for <math>\\mu</math>, i.e. there are three possible values of <math>\\mu</math> that satisfy this equation. The <math>-\\mu</math> and <math>+\\mu</math>, which are the maximum likelihood estimates and 0, which corresponds to the minimum log-likelihood.\n\n== Related distributions ==\n* When {{math|''μ'' {{=}} 0}}, the distribution of {{math|''Y''}} is a [[half-normal distribution]].\n* The random variable {{math|(''Y''/''σ'')<sup>2</sup>}} has a [[noncentral chi-squared distribution]] with 1 degree of freedom and noncentrality equal to {{math|(''μ''/''σ'')<sup>2</sup>}}.\n* The folded normal distribution can also be seen as the limit of the [[Folded-t distribution|folded non-standardized t distribution]] as the degrees of freedom go to infinity.\n* There is a bivariate version developed by Psarakis and Panaretos (2001) as well as a multivariate version developed by Chakraborty and Moutushi (2013).\n\n== See also ==\n* [[Folded cumulative distribution]]\n\n== References ==\n* {{cite journal |last=Tsagris |first=M. |last2=Beneki |first2=C. |last3=Hassani |first3=H. |year=2014 |title=On the folded normal distribution |journal=Mathematics |volume=2 |issue=1 |pages=12–28 }}\n* {{cite journal |vauthors=Leone FC, Nottingham RB, Nelson LS | year = 1961\n | title = The Folded Normal Distribution\n | journal = Technometrics | volume = 3 | issue = 4 | pages = 543–550\n | doi = 10.2307/1266560 | jstor=1266560\n}}\n* {{cite journal | author=Johnson NL | year = 1962\n | title = The folded normal distribution: accuracy of the estimation by maximum likelihood\n | journal = Technometrics | volume = 4 | issue = 2 | pages = 249–256\n | doi = 10.2307/1266622 | jstor=1266622\n}}\n* {{cite journal | author=Nelson LS | year = 1980\n | title = The Folded Normal Distribution\n | journal = J Qual Technol | volume = 12 | issue = 4 | pages = 236–238\n | doi = \n}}\n* {{cite journal | author=Elandt RC | year = 1961\n | title = The folded normal distribution: two methods of estimating parameters from moments\n | journal = Technometrics | volume = 3 | issue = 4 | pages = 551–562\n | doi = 10.2307/1266561 | jstor=1266561\n}}\n* {{cite journal | author=Lin PC | year = 2005\n | title = Application of the generalized folded-normal distribution to the process capability measures\n | journal = Int J Adv Manuf Technol | volume = 26 | pages = 825–830\n | doi = 10.1007/s00170-003-2043-x | issue=7–8\n}}\n* {{cite journal |last=Psarakis |first=S. |last2=Panaretos |first2=J. |year=1990 |title=The folded t distribution |journal=Communications in Statistics-Theory and Methods |volume=19 |issue=7 |pages=2717–2734 }}\n* {{cite journal |last=Psarakis |first=S. |last2=Panaretos |first2=J. |year=2001 |title=On some bivariate extensions of the folded normal and the folded-t distributions |journal=Journal of Applied Statistical Science |volume=10 |issue=2 |pages=119–136 }}\n* {{cite journal |last=Chakraborty |first=A. K. |last2=Moutushi |first2=C. |year=2013 |title=On multivariate folded normal distribution |journal=Sankhya B |volume=75 |issue=1 |pages=1–15 }}\n\n== External links ==\n* [http://www.randomservices.org/random/special/FoldedNormal.html Random (formerly Virtual Laboratories): The Folded Normal Distribution]\n\n{{ProbDistributions|continuous-semi-infinite}}\n\n[[Category:Continuous distributions]]\n[[Category:Normal distribution]]"
    },
    {
      "title": "Gaussian noise",
      "url": "https://en.wikipedia.org/wiki/Gaussian_noise",
      "text": "{{multiple image\n| direction = vertical\n| width = 256\n| footer =\n| image1 = 512x512-No-Noise.jpg\n| alt1 = Without noise\n| caption1 = Without noise\n| image2 = 512x512-Gaussian-Noise.jpg\n| alt2 = With Gaussian noise\n| caption2 = With Gaussian noise\n}}\n\n'''Gaussian noise''', named after [[Carl Friedrich Gauss]], is [[statistical noise]] having a [[probability density function]] (PDF) equal to that of the [[normal distribution]], which is also known as the [[Gaussian distribution]].<ref name=\"Barbu\" /><ref name=\"Handbook\"/> In other words, the values that the noise can take on are Gaussian-distributed.\n\nThe probability density function <math>p</math> of a Gaussian random variable <math>z</math> is given by:\n\n: <math>p_G(z) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{(z-\\mu)^2}{2\\sigma^2} }</math>\n\nwhere <math>z</math> represents the grey level, <math>\\mu</math> the [[mean]] value and <math>\\sigma</math> the [[standard deviation]].<ref name=\"Basel\" />\n\nA special case is ''white Gaussian noise'', in which the values at any pair of times are [[iid|identically distributed]] and [[statistically independent]] (and hence [[uncorrelated]]). In [[communication channel]] testing and modelling, Gaussian noise is used as additive [[white noise]] to generate [[additive white Gaussian noise]].\n\nIn [[telecommunications]] and [[computer networking]], communication channels can be affected by [[wideband]] Gaussian noise coming from many natural sources, such as the thermal vibrations of atoms in conductors (referred to as thermal noise or [[Johnson–Nyquist noise]]), [[shot noise]], [[black-body radiation]] from the earth and other warm objects, and from celestial sources such as the Sun.\n\n=== Gaussian noise in digital images ===  \n\nPrincipal sources of Gaussian noise in [[digital image]]s arise during acquisition e.g. [[sensor noise]] caused by poor illumination and/or high temperature, and/or transmission e.g. [[Circuit noise level|electronic circuit noise]].<ref name=\"Basel\" /> In [[digital image processing]] Gaussian noise can be reduced using a [[spatial filter]], though when smoothing an image, an undesirable outcome may result in the blurring of fine-scaled image edges and details because they also correspond to blocked high frequencies. Conventional spatial filtering techniques for [[Noise reduction|noise removal]] include: mean ([[convolution]]) filtering, [[median filter]]ing and [[Gaussian smoothing]].<ref name=\"Barbu\" /><ref name=\"HIPR2\" />\n\n== See also ==\n\n* [[Gaussian process]]\n* [[Gaussian smoothing]]\n\n== References ==\n\n{{Reflist|refs=\n<ref name=\"Handbook\">{{cite web |url=https://www.sfu.ca/sonic-studio/handbook/Gaussian_Noise.html |title=Handbook for Acoustic Ecology |edition=Second |year=1999 |publisher=Cambridge Street Publishing |editor=Barry Truax |accessdate=2012-08-05}}</ref>\n<ref name=\"HIPR2\">{{cite web | url=http://homepages.inf.ed.ac.uk/rbf/HIPR2/noise.htm | title=Image Synthesis — Noise Generation | accessdate=11 October 2013 |author1=Robert Fisher |author2=Simon Perkins |author3=Ashley Walker |author4=Erik Wolfart }}</ref>\n<ref name=\"Basel\">{{cite web | url=http://miac.unibas.ch/SIP/06-Restoration.html | title=Image Restoration: Introduction to Signal and Image Processing | publisher=MIAC, University of Basel | date=2012-04-24 | accessdate=11 October 2013 | author= Philippe Cattin}}</ref>\n<ref name=\"Barbu\">{{cite journal | url=http://www.hindawi.com/journals/aaa/2013/856876/ | title=Variational Image Denoising Approach with Diffusion Porous Media Flow | author=Tudor Barbu | journal=Abstract and Applied Analysis | year=2013 | volume=2013 | pages=8 | doi=10.1155/2013/856876}}</ref>\n}}\n\n{{DEFAULTSORT:Gaussian Noise}}\n[[Category:Stochastic processes]]\n[[Category:Normal distribution]]"
    }
  ]
}